<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 18 Sep 2025 14:53:48 +0800</lastBuildDate>
    <item>
      <title>Beyond Averages: Open-Vocabulary 3D Scene Understanding with Gaussian Splatting and Bag of Embeddings</title>
      <link>http://arxiv.org/abs/2509.12938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种绕过可微分渲染的新方法，通过预分解的对象级高斯和多视图CLIP特征聚合来克服3D高斯溅射在场景理解方面的局限性，实现了准确的开放词汇对象检索和无缝任务适应。&lt;h4&gt;背景&lt;/h4&gt;3D高斯溅射在新型视图合成方面取得了显著进展，能够实现实时照片级真实感渲染，但其内在模糊性对3D场景理解提出了挑战，限制了在AR/VR和机器人技术中的应用。现有方法通过2D基础模型蒸馏学习语义存在根本性限制，alpha blending会使不同对象的语义平均化，导致无法实现3D级别的理解。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服3D高斯溅射场景理解限制的方法，实现准确的3D开放词汇对象提取和语义理解，扩展其在AR/VR和机器人技术中的应用范围。&lt;h4&gt;方法&lt;/h4&gt;提出范式转变的替代方案，完全绕过可微分渲染处理语义。利用预分解的对象级高斯，通过多视图CLIP特征聚合表示每个对象，创建全面的'嵌入袋'来全面描述对象。实现两种功能：1) 通过比较文本查询与对象级嵌入进行开放词汇对象检索；2) 将对象ID传播到像素用于2D分割或传播到高斯用于3D提取。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进性能相当，确保了最小程度的性能妥协。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过改变处理语义的方式，成功解决了3D高斯溅射在场景理解方面的局限性，为AR/VR和机器人技术等领域的应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;新型视图合成在3D高斯溅射(3DGS)的推动下取得了显著进展，实现了实时照片级真实感渲染。然而，高斯溅射的内在模糊性对3D场景理解提出了挑战，限制了其在AR/VR和机器人技术中的更广泛应用。虽然最近的工作尝试通过2D基础模型蒸馏来学习语义，但它们继承了根本性限制：alpha blending使不同对象的语义平均化，导致无法实现3D级别的理解。我们提出了一种范式转变的替代方案，完全绕过可微分渲染来处理语义。我们的关键见解是利用预分解的对象级高斯，并通过多视图CLIP特征聚合来表示每个对象，创建全面的'嵌入袋'，全面描述对象。这实现了：(1) 通过将文本查询与对象级(而非高斯级)嵌入进行比较，实现准确的开放词汇对象检索，以及(2) 无缝任务适应：将对象ID传播到像素用于2D分割或传播到高斯用于3D提取。实验证明，我们的方法有效克服了3D开放词汇对象提取的挑战，同时在2D开放词汇分割方面与最先进性能相当，确保最小程度的妥协。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D高斯溅射(3DGS)在场景理解方面的局限性。虽然3DGS能实现实时照片级渲染，但其内在模糊性导致难以准确识别和分割场景中的个体对象，这限制了在AR/VR、机器人技术等需要精确场景理解的应用中的使用。这个问题很重要，因为当前3D渲染技术虽然视觉效果逼真，但缺乏对场景内容的语义理解能力，无法支持需要精确对象识别和操作的高级应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法(如LangSplat、LeGaussians等)的局限性出发思考，这些方法通过可微分渲染将2D基础模型语义注入3D高斯，但存在噪声和不一致问题。作者借鉴了Gaussian Grouping方法，该方法能将3D高斯分组为不同对象并分配身份编码。作者还利用了CLIP模型强大的视觉-语言关联能力。基于这些观察，作者设计了一种绕过可微分渲染的新方法，转而使用预分解的对象级高斯，并通过多视图CLIP特征聚合创建'嵌入包'来表示每个对象。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：不再通过可微分渲染将语义注入单个高斯，而是将3D高斯预先分组为不同对象，为每个对象创建'嵌入包'，在对象级别执行开放词汇检索，并将对象ID传播到下游任务。整体流程：1)使用SAM模型从多视图图像生成2D掩码并确保跨视图一致性；2)使用Gaussian Grouping将3D高斯分组为不同对象并分配唯一ID；3)为每个对象提取多视图CLIP特征并聚合形成'嵌入包'；4)根据文本查询计算对象相关性分数并识别最相关对象ID；5)将对象ID用于3D对象选择(过滤高斯)或2D分割(生成像素级掩码)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)对象级语义编码，将3D高斯场景表示为每个对象的多元CLIP嵌入'包'，避免有问题的语义可微分渲染；2)开放词汇3D搜索，通过预分组场景实现直接对象级检索，解决高斯级语义不一致问题；3)任务无关的语义传播，将对象ID无缝传播到2D分割和3D提取，无需重新训练。与之前工作的不同：1)完全绕过可微分渲染进行语义学习，避免alpha混合导致的语义稀释；2)在对象级别而非高斯级别操作，提供更清洁一致的表示；3)保留多视图嵌入细节而非简单平均，捕获特定视角的独特特征；4)同时处理2D和3D任务，提供更全面场景理解。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出通过高斯分组和多视图CLIP嵌入包实现开放词汇3D场景理解，能准确进行3D对象提取和2D分割，同时克服了现有方法中的语义平均化和噪声问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Novel view synthesis has seen significant advancements with 3D GaussianSplatting (3DGS), enabling real-time photorealistic rendering. However, theinherent fuzziness of Gaussian Splatting presents challenges for 3D sceneunderstanding, restricting its broader applications in AR/VR and robotics.While recent works attempt to learn semantics via 2D foundation modeldistillation, they inherit fundamental limitations: alpha blending averagessemantics across objects, making 3D-level understanding impossible. We proposea paradigm-shifting alternative that bypasses differentiable rendering forsemantics entirely. Our key insight is to leverage predecomposed object-levelGaussians and represent each object through multiview CLIP feature aggregation,creating comprehensive "bags of embeddings" that holistically describe objects.This allows: (1) accurate open-vocabulary object retrieval by comparing textqueries to object-level (not Gaussian-level) embeddings, and (2) seamless taskadaptation: propagating object IDs to pixels for 2D segmentation or toGaussians for 3D extraction. Experiments demonstrate that our methodeffectively overcomes the challenges of 3D open-vocabulary object extractionwhile remaining comparable to state-of-the-art performance in 2Dopen-vocabulary segmentation, ensuring minimal compromise.</description>
      <author>example@mail.com (Abdalla Arafa, Didier Stricker)</author>
      <guid isPermaLink="false">2509.12938v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
  <item>
      <title>Semantic 3D Reconstructions with SLAM for Central Airway Obstruction</title>
      <link>http://arxiv.org/abs/2509.13541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的流程，通过结合DROID-SLAM与分割模型，利用单目内窥镜视频实现中央气道的实时、语义感知三维重建，为自主机器人干预提供了有希望的进展。&lt;h4&gt;背景&lt;/h4&gt;中央气道阻塞(CAO)是一种发病率不断增加的危及生命的疾病，由气道内外的肿瘤引起。传统治疗方法如支气管镜和电凝术虽可完全移除肿瘤，但并发症风险高。最近的机器人干预技术降低了风险，结合场景理解和映射还可能实现自动化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够使用单目内窥镜视频进行中央气道实时、语义感知三维重建的新颖流程，为机器人干预提供支持。&lt;h4&gt;方法&lt;/h4&gt;结合DROID-SLAM与一个经过训练用于识别阻塞组织的分割模型。SLAM模块实时重建气道的三维几何结构，分割掩码指导重建点云内阻塞区域的标注。使用离体模型评估重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;真实CT扫描与3D重建之间具有高度相似性(0.62 mm Chamfer距离)。系统通过集成分割到SLAM工作流中，能实时生成突出显示临床相关区域的标注3D地图。该流程比以往工作更快完成重建，更准确地反映手术场景。&lt;h4&gt;结论&lt;/h4&gt;据所知，这是第一个将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。该框架是模块化的，可泛化到其他解剖结构或程序，只需少量改动，为自主机器人干预提供了有希望的进展。&lt;h4&gt;翻译&lt;/h4&gt;中央气道阻塞(CAO)是一种发病率不断增加的危及生命的疾病，由气道内外的肿瘤引起。传统治疗方法如支气管镜和电凝术可用于完全移除肿瘤；然而，这些方法存在高并发症风险。最近的进展允许风险更低的机器人干预。机器人干预与场景理解和映射的结合也为自动化开辟了可能性。我们提出了一种新颖的流程，能够使用单目内窥镜视频进行中央气道的实时、语义感知三维重建。我们的方法将DROID-SLAM与一个用于识别阻塞组织的分割模型相结合。SLAM模块实时重建气道的三维几何结构，而分割掩码指导重建点云内阻塞区域的标注。为了验证我们的流程，我们使用离体模型评估重建质量。定性和定量结果显示真实CT扫描与3D重建之间具有高度相似性(0.62 mm Chamfer距离)。通过将分割直接集成到SLAM工作流中，我们的系统能实时生成突出显示临床相关区域的标注3D地图。该流程的高速能力比以往工作能更快完成重建，更准确地反映手术场景。据我们所知，这是第一个将语义分割与实时单目SLAM集成用于内窥镜CAO场景的工作。我们的框架是模块化的，可以泛化到其他解剖结构或程序，只需少量改动，为自主机器人干预提供了有希望的进展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决中心气道阻塞（CAO）的实时3D重建和场景理解问题。CAO是一种危及生命的疾病，发病率正在增加，由气道内外的肿瘤引起。传统治疗方法风险高，而机器人干预需要结合场景理解才能实现自动化。现有的3D重建方法耗时太长，无法在手术过程中实时更新，因此需要一种能够快速重建气道3D结构并识别阻塞区域的方法，以支持更安全、更精确的自动化手术操作。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CAO治疗的挑战性和机器人干预的潜力。他们之前的工作已实现实时分割但3D重建使用耗时的SfM方法，因此转向SLAM算法以实现实时重建。他们借鉴了DROID-SLAM作为3D重建框架，采用U-Net和SAM2构建分割模型，使用类似前人的phantom制作方法模拟临床场景。整体设计思路是将实时3D重建与语义分割结合，创建带有标注的3D气道地图，以支持下游自动化任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将实时3D重建（通过SLAM）与语义分割相结合，创建带有临床相关信息标注的3D气道地图。整体流程包括：1)使用羊心和鸡胸肉制作的phantom模拟CAO并收集内窥镜视频；2)训练U-Net/SAM2分割模型识别阻塞组织；3)使用DROID-SLAM处理单目内窥镜视频流，实时创建3D点云；4)将分割掩码集成到重建流程中，识别和标注阻塞区域；5)通过配准CT扫描评估重建质量和分割精度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将语义分割与实时单目SLAM结合应用于内窥镜CAO场景；创建实时、语义丰富的3D气道重建管道；实现比之前工作更快的重建速度；开发模块化框架可扩展到其他场景。相比之前工作，新方法用SLAM替代了耗时的SfM，将分割直接集成到SLAM工作流中，产生的3D重建带有语义标注，处理速度更快（每帧0.31秒），能更准确地反映手术场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的管道，通过结合实时单目SLAM与语义分割，首次实现了中心气道阻塞场景下的实时、语义丰富的3D重建，为自动化机器人手术干预提供了关键技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Central airway obstruction (CAO) is a life-threatening condition withincreasing incidence, caused by tumors in and outside of the airway.Traditional treatment methods such as bronchoscopy and electrocautery can beused to remove the tumor completely; however, these methods carry a high riskof complications. Recent advances allow robotic interventions with lesser risk.The combination of robot interventions with scene understanding and mappingalso opens up the possibilities for automation. We present a novel pipelinethat enables real-time, semantically informed 3D reconstructions of the centralairway using monocular endoscopic video.  Our approach combines DROID-SLAM with a segmentation model trained toidentify obstructive tissues. The SLAM module reconstructs the 3D geometry ofthe airway in real time, while the segmentation masks guide the annotation ofobstruction regions within the reconstructed point cloud. To validate ourpipeline, we evaluate the reconstruction quality using ex vivo models.  Qualitative and quantitative results show high similarity between groundtruth CT scans and the 3D reconstructions (0.62 mm Chamfer distance). Byintegrating segmentation directly into the SLAM workflow, our system producesannotated 3D maps that highlight clinically relevant regions in real time.High-speed capabilities of the pipeline allows quicker reconstructions comparedto previous work, reflecting the surgical scene more accurately.  To the best of our knowledge, this is the first work to integrate semanticsegmentation with real-time monocular SLAM for endoscopic CAO scenarios. Ourframework is modular and can generalize to other anatomies or procedures withminimal changes, offering a promising step toward autonomous roboticinterventions.</description>
      <author>example@mail.com (Ayberk Acar, Fangjie Li, Hao Li, Lidia Al-Zogbi, Kanyifeechukwu Jane Oguine, Susheela Sharma Stern, Jesse F. d'Almeida, Robert J. Webster III, Ipek Oguz, Jie Ying Wu)</author>
      <guid isPermaLink="false">2509.13541v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>ColonCrafter: A Depth Estimation Model for Colonoscopy Videos Using Diffusion Priors</title>
      <link>http://arxiv.org/abs/2509.13525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ColonCrafter，一个基于扩散的深度估计模型，用于从单目结肠镜视频中生成时间上一致的深度图，解决了现有内窥镜深度估计模型在视频序列中时间一致性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;结肠镜检查中的三维场景理解面临重大挑战，需要自动化的方法进行准确的深度估计。然而，现有的内窥镜深度估计模型在视频序列中的时间一致性方面存在问题，限制了它们在三维重建中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单目结肠镜视频中生成时间上一致的深度图的模型，以克服现有方法的局限性，并支持三维重建等临床应用。&lt;h4&gt;方法&lt;/h4&gt;作者提出了ColonCrafter，一个基于扩散的深度估计模型。该方法从合成的结肠镜序列中学习强大的几何先验，生成时间上一致的深度图。同时引入了一种风格转换技术，在保持几何结构的同时，将真实的临床视频调整为匹配合成的训练域。&lt;h4&gt;主要发现&lt;/h4&gt;ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用方法和专门针对内窥镜的方法。该模型能够生成时间上一致的深度图，并支持临床相关的应用，包括三维点云生成和表面覆盖率评估。&lt;h4&gt;结论&lt;/h4&gt;尽管完整的轨迹三维重建仍然是一个挑战，但ColonCrafter在临床应用方面展示了潜力，特别是在三维点云生成和表面覆盖率评估方面。&lt;h4&gt;翻译&lt;/h4&gt;结肠镜检查中的三维场景理解面临着重大挑战，需要自动化的方法进行准确的深度估计。然而，现有的内窥镜深度估计模型在视频序列中的时间一致性方面存在问题，限制了它们在三维重建中的应用。我们提出了ColonCrafter，一个基于扩散的深度估计模型，可以从单目结肠镜视频中生成时间上一致的深度图。我们的方法从合成的结肠镜序列中学习强大的几何先验，以生成时间上一致的深度图。我们还引入了一种风格转换技术，在保持几何结构的同时，将真实的临床视频调整为匹配我们的合成训练域。ColonCrafter在C3VD数据集上实现了最先进的零样本性能，优于通用方法和专门针对内窥镜的方法。尽管完整的轨迹三维重建仍然是一个挑战，但我们展示了ColonCrafter的临床相关应用，包括三维点云生成和表面覆盖率评估。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结肠镜视频中的深度估计问题，特别是实现时间一致的深度估计以支持结肠三维重建。这个问题非常重要，因为结直肠癌是美国癌症相关死亡的主要原因，结肠镜检查是筛查金标准，但临床实践中存在结肠皱褶后可视化不良导致病变漏诊率高、难以重新定位病变、以及难以准确测量息肉大小等挑战。结肠本质上是三维结构，而医生只能获得二维视觉信息，这种不匹配限制了临床效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了结肠镜环境的特殊挑战，包括黏膜缺乏明显视觉特征、非朗伯反射、严重高光等。他们借鉴了DepthCrafter架构和EDM框架，使用变分自编码器进行深度表示，并采用低秩适应(LoRA)进行模型微调而非从头训练。作者还参考了Chung等人的艺术风格转换工作，但创新性地将其应用于医学领域。他们使用合成结肠镜序列进行训练，并开发了一种风格转换技术来弥合合成数据与真实临床视频之间的域差距，同时引入了多种数据增强技术提高模型泛化能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型进行结肠镜视频的深度估计，并通过风格转换技术解决合成训练数据与真实临床视频之间的域差距问题。整体流程包括：1)从CT扫描构建合成结肠镜数据集；2)使用LoRA微调DepthCrafter模型；3)开发真实到合成的风格转换技术，保留几何结构同时改变外观；4)使用ColonCrafter从输入视频生成时间一致的深度图；5)将深度图应用于下游任务如3D点云生成和表面覆盖评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对结肠镜的扩散模型深度估计框架；2)新颖的风格转换技术，解决域差距同时保留几何结构；3)在C3VD基准上实现最先进的零样本性能。相比之前工作，通用深度估计模型在结肠镜数据上表现中等，而ColonCrafter通过特定微调将δ1准确性提高17%以上；其他内窥镜特定模型难以保持长期时间一致性；基于合成数据的方法通常在真实临床视频上表现不佳，而ColonCrafter通过风格转换成功弥合了域差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ColonCrafter通过结合扩散模型和风格转换技术，首次实现了从单目结肠镜视频中生成时间一致的高精度深度图，为结肠三维重建和临床应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional (3D) scene understanding in colonoscopy presentssignificant challenges that necessitate automated methods for accurate depthestimation. However, existing depth estimation models for endoscopy strugglewith temporal consistency across video sequences, limiting their applicabilityfor 3D reconstruction. We present ColonCrafter, a diffusion-based depthestimation model that generates temporally consistent depth maps from monocularcolonoscopy videos. Our approach learns robust geometric priors from syntheticcolonoscopy sequences to generate temporally consistent depth maps. We alsointroduce a style transfer technique that preserves geometric structure whileadapting real clinical videos to match our synthetic training domain.ColonCrafter achieves state-of-the-art zero-shot performance on the C3VDdataset, outperforming both general-purpose and endoscopy-specific approaches.Although full trajectory 3D reconstruction remains a challenge, we demonstrateclinically relevant applications of ColonCrafter, including 3D point cloudgeneration and surface coverage assessment.</description>
      <author>example@mail.com (Romain Hardy, Tyler Berzin, Pranav Rajpurkar)</author>
      <guid isPermaLink="false">2509.13525v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Cinéaste: A Fine-grained Contextual Movie Question Answering Benchmark</title>
      <link>http://arxiv.org/abs/2509.14227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Cinéaste基准，用于评估视觉语言模型对长篇电影的理解能力，现有模型在该基准上表现不佳，表明长程时间推理是主要瓶颈。&lt;h4&gt;背景&lt;/h4&gt;最近的视觉语言模型在视频理解方面取得了进步，但诊断它们对深层叙事理解的能力仍然是一个挑战。现有基准测试通常测试短片段识别或使用基于模板的问题，在评估对长篇叙事内容的细粒度推理方面存在关键空白。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的基准测试(Cinéaste)来评估视觉语言模型对长篇电影的细粒度推理和叙事理解能力。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含来自200部不同电影的1,805个场景的3,119个多项选择题-答案对的数据集，涵盖五个细粒度上下文推理类别。使用GPT-4o生成多样化、丰富的上下文问题，整合视觉描述、字幕、场景标题和摘要。采用两阶段过滤流程确保问题质量：上下文独立性过滤和上下文真实性过滤。&lt;h4&gt;主要发现&lt;/h4&gt;现有的多模态大语言模型在Cinéaste基准上表现不佳；分析显示长程时间推理是主要瓶颈，最好的开源模型仅达到63.15%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这强调了细粒度上下文理解面临的重大挑战以及对长篇电影理解进步的需求。&lt;h4&gt;翻译&lt;/h4&gt;尽管最近视觉语言模型的进步改善了视频理解，但诊断它们对深层叙事理解的能力仍然是一个挑战。现有的基准测试通常测试短片段识别或使用基于模板的问题，在评估对长篇叙事内容的细粒度推理方面留下了关键空白。为了解决这些空白，我们引入了Cinéaste，一个用于长篇电影理解的全面基准。我们的数据集包含来自200部不同电影的1,805个场景的3,119个多项选择题-答案对，涵盖了五个新颖的细粒度上下文推理类别。我们使用GPT-4o通过整合视觉描述、字幕、场景标题和摘要来生成多样化、丰富的上下文问题，这些问题需要深层的叙事理解。为确保高质量评估，我们的流程包含两阶段过滤：上下文独立性过滤确保问题需要视频上下文，而上下文真实性过滤验证与电影内容的事实一致性，减轻幻觉。实验表明，现有的多模态大语言模型在Cinéaste上表现不佳；我们的分析显示长程时间推理是主要瓶颈，最好的开源模型仅达到63.15%的准确率。这强调了细粒度上下文理解面临的重大挑战以及对长篇电影理解进步的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While recent advancements in vision-language models have improved videounderstanding, diagnosing their capacity for deep, narrative comprehensionremains a challenge. Existing benchmarks often test short-clip recognition oruse template-based questions, leaving a critical gap in evaluating fine-grainedreasoning over long-form narrative content. To address these gaps, we introduce$\mathsf{Cin\acute{e}aste}$, a comprehensive benchmark for long-form movieunderstanding. Our dataset comprises 3,119 multiple-choice question-answerpairs derived from 1,805 scenes across 200 diverse movies, spanning five novelfine-grained contextual reasoning categories. We use GPT-4o to generatediverse, context-rich questions by integrating visual descriptions, captions,scene titles, and summaries, which require deep narrative understanding. Toensure high-quality evaluation, our pipeline incorporates a two-stage filteringprocess: Context-Independence filtering ensures questions require videocontext, while Contextual Veracity filtering validates factual consistencyagainst the movie content, mitigating hallucinations. Experiments show thatexisting MLLMs struggle on $\mathsf{Cin\acute{e}aste}$; our analysis revealsthat long-range temporal reasoning is a primary bottleneck, with the topopen-source model achieving only 63.15\% accuracy. This underscores significantchallenges in fine-grained contextual understanding and the need foradvancements in long-form movie comprehension.</description>
      <author>example@mail.com (Nisarg A. Shah, Amir Ziai, Chaitanya Ekanadham, Vishal M. Patel)</author>
      <guid isPermaLink="false">2509.14227v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Dense Video Understanding with Gated Residual Tokenization</title>
      <link>http://arxiv.org/abs/2509.14199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Dense Video Understanding (DVU)框架，通过Gated Residual Tokenization (GRT)技术实现高帧率视频理解，并创建了DIVE基准测试评估密集时间推理能力。&lt;h4&gt;背景&lt;/h4&gt;当前视频大语言模型主要依赖低帧率采样，丢弃了密集时间信息，导致对需要精确时间对齐的任务（如讲座理解）表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发能够高效处理高FPS视频理解的方法，减少标记时间和标记开销，并创建新的基准测试评估密集时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出GRT两阶段框架：1)运动补偿间门控标记化，使用像素级运动估计跳过静态区域；2)语义场景内标记化合并，融合静态区域标记减少冗余。&lt;h4&gt;主要发现&lt;/h4&gt;在DIVE基准测试上，GRT表现优于更大规模的VLLM基线模型，并能随FPS正向扩展，证明了密集时间信息的重要性。&lt;h4&gt;结论&lt;/h4&gt;密集时间信息对视频理解至关重要，GRT方法能够实现高效、可扩展的高FPS视频理解。&lt;h4&gt;翻译&lt;/h4&gt;高时间分辨率对于捕捉视频理解中的精细细节至关重要。然而，当前视频大语言模型和基准测试主要依赖低帧率采样，如均匀采样或关键帧选择，丢弃了密集的时间信息。这种折衷避免了标记每一帧的高成本，否则会导致冗余计算和随着视频长度增加的线性标记增长。虽然这种权衡对缓慢变化的内容有效，但对于像讲座理解这样的任务则失败，其中信息几乎出现在每一帧中，需要精确的时间对齐。为解决这一差距，我们引入了Dense Video Understanding (DVU)，通过减少标记时间和标记开销来实现高FPS视频理解。现有基准测试也有局限，因为它们的问答对关注粗略的内容变化。因此，我们提出了DIVE (Dense Information Video Evaluation)，这是第一个专为密集时间推理设计的基准测试。为使DVU实用，我们提出了Gated Residual Tokenization (GRT)，一个两阶段框架：(1)运动补偿间门控标记化使用像素级运动估计在标记化过程中跳过静态区域，实现标记数和计算量的次线性增长。(2)语义场景内标记化合并融合场景内静态区域的标记，进一步减少冗余，同时保留动态语义。在DIVE上的实验表明，GRT优于更大的VLLM基线，并能随FPS正向扩展。这些结果突出了密集时间信息的重要性，并证明GRT能够实现高效、可扩展的高FPS视频理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High temporal resolution is essential for capturing fine-grained details invideo understanding. However, current video large language models (VLLMs) andbenchmarks mostly rely on low-frame-rate sampling, such as uniform sampling orkeyframe selection, discarding dense temporal information. This compromiseavoids the high cost of tokenizing every frame, which otherwise leads toredundant computation and linear token growth as video length increases. Whilethis trade-off works for slowly changing content, it fails for tasks likelecture comprehension, where information appears in nearly every frame andrequires precise temporal alignment. To address this gap, we introduce DenseVideo Understanding (DVU), which enables high-FPS video comprehension byreducing both tokenization time and token overhead. Existing benchmarks arealso limited, as their QA pairs focus on coarse content changes. We thereforepropose DIVE (Dense Information Video Evaluation), the first benchmark designedfor dense temporal reasoning. To make DVU practical, we present Gated ResidualTokenization (GRT), a two-stage framework: (1) Motion-Compensated Inter-GatedTokenization uses pixel-level motion estimation to skip static regions duringtokenization, achieving sub-linear growth in token count and compute. (2)Semantic-Scene Intra-Tokenization Merging fuses tokens across static regionswithin a scene, further reducing redundancy while preserving dynamic semantics.Experiments on DIVE show that GRT outperforms larger VLLM baselines and scalespositively with FPS. These results highlight the importance of dense temporalinformation and demonstrate that GRT enables efficient, scalable high-FPS videounderstanding.</description>
      <author>example@mail.com (Haichao Zhang, Wenhao Chai, Shwai He, Ang Li, Yun Fu)</author>
      <guid isPermaLink="false">2509.14199v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>CETUS: Causal Event-Driven Temporal Modeling With Unified Variable-Rate Scheduling</title>
      <link>http://arxiv.org/abs/2509.13784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Variable-Rate Spatial Event Mamba的新型架构，直接处理原始事件流而无需中间表示，通过轻量级编码器和Mamba状态空间模型实现高效处理，并采用自适应控制器优化延迟平衡。&lt;h4&gt;背景&lt;/h4&gt;事件相机以微秒级时间分辨率捕捉异步像素级亮度变化，为高速视觉任务提供独特优势。现有方法常将事件流转换为帧、体素网格或点云等中间表示，需要预定义时间窗口，引入窗口延迟；点检测方法则因计算成本高而无法实现实时效率。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，开发能够直接处理原始事件流、无需中间表示的架构，同时保持实时效率，实现窗口延迟和推理延迟之间的最佳平衡。&lt;h4&gt;方法&lt;/h4&gt;提出Variable-Rate Spatial Event Mamba架构，包含：1)轻量级因果空间邻域编码器，高效捕获局部几何关系；2)基于Mamba的状态空间模型，实现线性复杂度的可扩展时间建模；3)自适应控制器，根据事件率调整处理速度优化延迟。&lt;h4&gt;主要发现&lt;/h4&gt;直接处理原始事件流可避免中间表示的延迟；轻量级编码器能有效捕获局部几何关系；Mamba状态空间模型提供线性复杂度的时间建模能力；自适应处理可实现延迟优化。&lt;h4&gt;结论&lt;/h4&gt;Variable-Rate Spatial Event Mamba架构成功克服了现有方法的局限性，能够直接处理原始事件流并保持实时效率，为高速视觉任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;事件相机以微秒级时间分辨率捕捉异步像素级亮度变化，为高速视觉任务提供了独特优势。现有方法通常将事件流转换为帧、体素网格或点云等中间表示，这不可避免地需要预定义时间窗口，从而引入窗口延迟。同时，点检测方法由于计算成本高，面临计算挑战，无法实现实时效率。为克服这些局限性，我们提出了Variable-Rate Spatial Event Mamba，一种新型架构，可直接处理原始事件流而无需中间表示。我们的方法引入了轻量级因果空间邻域编码器，以高效捕获局部几何关系，然后使用基于Mamba的状态空间模型进行具有线性复杂度的可扩展时间建模。在推理过程中，控制器根据事件率自适应调整处理速度，实现窗口延迟和推理延迟之间的最佳平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras capture asynchronous pixel-level brightness changes withmicrosecond temporal resolution, offering unique advantages for high-speedvision tasks. Existing methods often convert event streams into intermediaterepresentations such as frames, voxel grids, or point clouds, which inevitablyrequire predefined time windows and thus introduce window latency. Meanwhile,pointwise detection methods face computational challenges that preventreal-time efficiency due to their high computational cost. To overcome theselimitations, we propose the Variable-Rate Spatial Event Mamba, a novelarchitecture that directly processes raw event streams without intermediaterepresentations. Our method introduces a lightweight causal spatialneighborhood encoder to efficiently capture local geometric relations, followedby Mamba-based state space models for scalable temporal modeling with linearcomplexity. During inference, a controller adaptively adjusts the processingspeed according to the event rate, achieving an optimal balance between windowlatency and inference latency.</description>
      <author>example@mail.com (Hanfang Liang, Bing Wang, Shizhen Zhang, Wen Jiang, Yizhuo Yang, Weixiang Guo, Shenghai Yuan)</author>
      <guid isPermaLink="false">2509.13784v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</title>
      <link>http://arxiv.org/abs/2509.10426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为DECAMP的去耦合上下文感知预训练框架，用于解决自动驾驶中轨迹预测面临的标记数据稀缺和多智能体预测场景表现不佳的问题。该框架通过将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，并结合上下文感知表示学习和协作空间-运动预训练任务，有效提升了多智能体运动预测的性能。&lt;h4&gt;背景&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法面临标记数据稀缺的问题，在多智能体预测场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，作者提出了一个名为DECAMP的去耦合上下文感知预训练框架，专门用于多智能体运动预测。&lt;h4&gt;方法&lt;/h4&gt;该框架将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，从而增强下游预测的场景表示。框架结合了上下文感知表示学习和协作空间-运动预训练任务，能够同时优化结构和意图推理，同时捕捉潜在的动态意图。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上的实验展示了该方法优越的性能，结果证明了其在多智能体运动预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是自动驾驶中首个用于多智能体运动预测的上下文自编码器框架，代码和模型将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并在多智能体预测场景中表现不佳。为了解决这些挑战，我们引入了一个用于多智能体运动预测的去耦合上下文感知预训练框架，名为DECAMP。与现有方法将表示学习与预训练任务纠缠在一起不同，我们的框架将行为模式学习与潜在特征重建去耦合，优先考虑可解释的动态，从而增强下游预测的场景表示。此外，我们的框架结合了上下文感知表示学习和协作空间-运动预训练任务，这能够在捕捉潜在动态意图的同时，对结构和意图推理进行联合优化。我们在Argoverse 2基准测试上的实验展示了我们方法的优越性能，取得的结果强调了其在多智能体运动预测中的有效性。据我们所知，这是自动驾驶中首个用于多智能体运动预测的上下文自编码器框架。代码和模型将公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体运动预测中的两个核心挑战：标记数据稀缺问题和多智能体预测场景中的次优性能。这个问题在自动驾驶领域至关重要，因为准确预测交通参与者的运动对确保道路安全和效率至关重要，特别是在复杂的多智能体交互场景中，不准确的预测可能导致生成与整体场景不一致的轨迹，阻碍自动驾驶车辆做出可靠决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前轨迹预测的主流方法（监督学习和自监督学习）及其局限性，特别是自监督学习方法存在的两个核心问题：编码器与预训练任务紧密耦合、难以扩展到多智能体预测。基于这些分析，作者借鉴了计算机视觉中自监督学习的成功经验，特别是掩码图像建模中的回归器设计，将其扩展到自动驾驶中的行为预测。同时，作者参考了DenseTNT和HiVT等方法的向量化表示技术，设计了'编码器-回归器-解码器'的级联范式来解耦行为模式学习与特征重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是解耦行为模式学习与潜在特征重建，优先考虑可解释的动力学，并引入协作空间-运动预训练任务来同时优化结构和推理。整体实现分为两个阶段：1)预训练阶段，模型接收历史和未来状态以及地图信息，通过编码器提取场景特征，回归器预测被遮盖令牌的表示，双解码器执行空间重建和运动识别任务；2)微调阶段，模型仅使用历史状态和地图信息，利用预训练的编码器生成K个场景一致的轨迹预测，每个场景包含所有目标代理的完整轨迹组合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)解耦的自监督学习框架，分离行为模式学习与特征重建；2)协作空间-运动预训练任务，联合优化空间线索和运动信号识别；3)'编码器-回归器-解码器'级联范式，增强场景元素间语义关系建模；4)多世界预测微调，生成场景一致的轨迹组合。相比之前工作，不同之处在于：不将表示学习与预训练任务紧密耦合，支持真正的多智能体联合预测而非单智能体预测，无需复杂后处理来确保场景一致性，并首次揭示协作预训练任务可以增强驾驶行为模式的表示学习。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DECAMP提出了一种解耦的上下文感知预训练框架，通过分离行为模式学习与特征重建并引入协作空间-运动预训练任务，显著提高了多智能体运动预测中场景一致性轨迹预测的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction is a critical component of autonomous driving,essential for ensuring both safety and efficiency on the road. However,traditional approaches often struggle with the scarcity of labeled data andexhibit suboptimal performance in multi-agent prediction scenarios. To addressthese challenges, we introduce a disentangled context-aware pre-trainingframework for multi-agent motion prediction, named DECAMP. Unlike existingmethods that entangle representation learning with pretext tasks, our frameworkdecouples behavior pattern learning from latent feature reconstruction,prioritizing interpretable dynamics and thereby enhancing scene representationfor downstream prediction. Additionally, our framework incorporatescontext-aware representation learning alongside collaborative spatial-motionpretext tasks, which enables joint optimization of structural and intentionalreasoning while capturing the underlying dynamic intentions. Our experiments onthe Argoverse 2 benchmark showcase the superior performance of our method, andthe results attained underscore its effectiveness in multi-agent motionforecasting. To the best of our knowledge, this is the first contextautoencoder framework for multi-agent motion forecasting in autonomous driving.The code and models will be made publicly available.</description>
      <author>example@mail.com (Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma)</author>
      <guid isPermaLink="false">2509.10426v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>An End-to-End Differentiable, Graph Neural Network-Embedded Pore Network Model for Permeability Prediction</title>
      <link>http://arxiv.org/abs/2509.13841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This preprint is also available at ESS Open Archive:  https://essopenarchive.org/users/960205/articles/1329010&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中，实现了高精度且跨尺度泛化的多孔介质渗透率预测，同时保持物理基础并提高模型可解释性。&lt;h4&gt;背景&lt;/h4&gt;准确预测多孔介质中的渗透率对地下流动建模至关重要。纯数据驱动模型计算效率高但缺乏跨尺度泛化能力和物理约束；传统孔隙网络模型基于物理且高效，但依赖理想化几何假设，限制了在复杂结构中的准确性。&lt;h4&gt;目的&lt;/h4&gt;克服纯数据驱动模型和传统孔隙网络模型的局限性，开发一种既能避免理想化几何假设又能保持物理基础流动计算的渗透率预测方法。&lt;h4&gt;方法&lt;/h4&gt;构建一个端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中，用GNN预测替代传统解析公式进行传导率计算，并通过反向传播梯度实现完全耦合的端到端训练，仅需单一标量渗透率作为训练目标。&lt;h4&gt;主要发现&lt;/h4&gt;所得模型具有高精度，能很好地跨不同尺度泛化，性能优于纯数据驱动和传统孔隙网络模型；基于梯度的敏感性分析揭示了物理上一致的特征影响，提高了模型可解释性。&lt;h4&gt;结论&lt;/h4&gt;该方法为复杂多孔介质中的渗透率预测提供了可扩展且物理信息丰富的框架，有效减少了模型不确定性并提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;多孔介质中渗透率的准确预测对地下流动建模至关重要。虽然纯数据驱动模型提供了计算效率，但它们通常缺乏跨尺度的泛化能力，且不包含明确的物理约束。另一方面，孔隙网络模型基于物理原理且高效，但依赖于理想化的几何假设来估计孔隙尺度水力传导率，限制了其在复杂结构中的准确性。为了克服这些局限，我们提出了一种端到端可区分的混合框架，将图神经网络嵌入到孔隙网络模型中。在此框架中，用于传导率计算的解析公式被基于GNN的预测所替代，这些预测由孔隙和喉部特征推导得出。预测的传导率随后被传递给PNM求解器进行渗透率计算。这样，模型避免了PNM的理想化几何假设，同时保留了基于物理的流动计算。GNN的训练不需要标记的传导率数据（每个孔隙网络可能有数千个），而是使用单一的标量渗透率作为训练目标。这通过反向传播梯度（通过GNN的自动微分和PNM求解器的离散伴随方法）成为可能，实现了完全耦合的端到端训练。所得模型实现高精度并能很好地跨不同尺度泛化，性能优于纯数据驱动和传统PNM方法。基于梯度的敏感性分析进一步揭示了物理上一致的特征影响，增强了模型可解释性。这种方法为复杂多孔介质中的渗透率预测提供了可扩展且物理信息丰富的框架，减少了模型不确定性并提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of permeability in porous media is essential for modelingsubsurface flow. While pure data-driven models offer computational efficiency,they often lack generalization across scales and do not incorporate explicitphysical constraints. Pore network models (PNMs), on the other hand, arephysics-based and efficient but rely on idealized geometric assumptions toestimate pore-scale hydraulic conductance, limiting their accuracy in complexstructures. To overcome these limitations, we present an end-to-enddifferentiable hybrid framework that embeds a graph neural network (GNN) into aPNM. In this framework, the analytical formulas used for conductancecalculations are replaced by GNN-based predictions derived from pore and throatfeatures. The predicted conductances are then passed to the PNM solver forpermeability computation. In this way, the model avoids the idealized geometricassumptions of PNM while preserving the physics-based flow calculations. TheGNN is trained without requiring labeled conductance data, which can number inthe thousands per pore network; instead, it learns conductance values by usinga single scalar permeability as the training target. This is made possible bybackpropagating gradients through both the GNN (via automatic differentiation)and the PNM solver (via a discrete adjoint method), enabling fully coupled,end-to-end training. The resulting model achieves high accuracy and generalizeswell across different scales, outperforming both pure data-driven andtraditional PNM approaches. Gradient-based sensitivity analysis further revealsphysically consistent feature influences, enhancing model interpretability.This approach offers a scalable and physically informed framework forpermeability prediction in complex porous media, reducing model uncertainty andimproving accuracy.</description>
      <author>example@mail.com (Qingqi Zhao, Heng Xiao)</author>
      <guid isPermaLink="false">2509.13841v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>State Space Models over Directed Graphs</title>
      <link>http://arxiv.org/abs/2509.13735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  currently undergoing review by IEEE Transactions on Big Data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新方法DirEgo2Token，通过k-hop自我图将有向图序列化，并开发了DirGraphSSM架构，首次将状态空间模型系统性地扩展到有向图学习领域。实验表明该方法在多个任务上取得了最先进性能，且训练速度提升1.5至2倍。&lt;h4&gt;背景&lt;/h4&gt;有向图在众多领域普遍存在，边的方向性编码关键因果关系。现有针对有向图的图神经网络和图变换器面临两大挑战：有效捕获长程因果关系，以及在大规模图数据集上平衡准确性和训练效率。状态空间模型在因果序列任务中表现出色，但其图变体仅适用于无向图，限制了在有向图学习中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有图状态空间模型仅适用于无向图的限制，将有向图学习与状态空间模型相结合，提高有向图学习的性能和效率。&lt;h4&gt;方法&lt;/h4&gt;提出两种方法：1) DirEgo2Token，通过k-hop自我图将有向图序列化；2) DirGraphSSM，一种新的有向图神经网络架构，通过消息传递机制在有向图上实现状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，DirGraphSSM在三个代表性的有向图学习任务上取得了最先进的性能，同时在另外两个任务上取得了具有竞争力的性能，并且比现有的最先进模型快1.5到2倍。&lt;h4&gt;结论&lt;/h4&gt;DirGraphSSM成功地将状态空间模型应用于有向图学习，解决了现有方法面临的挑战，并在多个任务上取得了优异的性能和训练效率。&lt;h4&gt;翻译&lt;/h4&gt;有向图在众多领域中普遍存在，其中边的方向性编码了关键的因果关系。然而，现有的针对有向图设计的图神经网络和图变换器面临两大挑战：(1)有效捕获来自有向边的长程因果关系；(2)在处理大规模图数据集时平衡准确性和训练效率。近年来，状态空间模型在因果序列任务中取得了实质性进展，其针对图的变体在各种图学习基准测试中展示了最先进的准确性，同时保持了高效率。然而，现有的图状态空间模型仅设计用于无向图，这限制了它们在有向图学习中的性能。为此，我们提出了一种创新方法DirEgo2Token，通过k-hop自我图将有向图序列化。这是首次将状态空间模型系统性地扩展到有向图学习领域。基于此，我们开发了DirGraphSSM，一种新的有向图神经网络架构，通过消息传递机制在有向图上实现状态空间模型。实验结果表明，DirGraphSSM在三个代表性的有向图学习任务上取得了最先进的性能，同时在另外两个任务上取得了具有竞争力的性能，并且比现有的最先进模型快1.5到2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Directed graphs are ubiquitous across numerous domains, where thedirectionality of edges encodes critical causal dependencies. However, existingGNNs and graph Transformers tailored for directed graphs face two majorchallenges: (1) effectively capturing long-range causal dependencies derivedfrom directed edges; (2) balancing accuracy and training efficiency whenprocessing large-scale graph datasets. In recent years, state space models(SSMs) have achieved substantial progress in causal sequence tasks, and theirvariants designed for graphs have demonstrated state-of-the-art accuracy whilemaintaining high efficiency across various graph learning benchmarks. However,existing graph state space models are exclusively designed for undirectedgraphs, which limits their performance in directed graph learning. To this end,we propose an innovative approach DirEgo2Token which sequentializes directedgraphs via k-hop ego graphs. This marks the first systematic extension of statespace models to the field of directed graph learning. Building upon this, wedevelop DirGraphSSM, a novel directed graph neural network architecture thatimplements state space models on directed graphs via the message-passingmechanism. Experimental results demonstrate that DirGraphSSM achievesstate-of-the-art performance on three representative directed graph learningtasks while attaining competitive performance on two additional tasks with1.5$\times $ to 2$\times $ training speed improvements compared to existingstate-of-the-art models.</description>
      <author>example@mail.com (Junzhi She, Xunkai Li, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2509.13735v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Hate Detection Using Dual-Stream Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.13515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的多模态双流图神经网络模型用于仇恨视频检测，通过构建实例图和互补权重图来突出仇恨内容，系统化建模视频中的结构化关系，实现了最先进的分类性能并具有强可解释性。&lt;h4&gt;背景&lt;/h4&gt;仇恨视频对在线安全和现实福祉构成严重风险，需要有效的检测方法。虽然多模态分类方法优于单模态方法，但存在忽视仇恨内容定义视频类别、统一处理所有内容而非强调仇恨组件，以及无法系统捕获视频结构化信息等局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态仇恨视频检测方法的局限性，开发能够突出仇恨内容、系统化建模视频结构化关系的高效检测模型。&lt;h4&gt;方法&lt;/h4&gt;提出一种多模态双流图神经网络模型，通过将视频分离为多个实例构建实例图提取实例级特征，使用互补权重图分配重要性权重突出仇恨实例，结合权重和特征生成视频标签，采用基于图的框架系统化建模模态内和跨模态的结构化关系。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上的广泛实验表明，该模型在仇恨视频分类方面达到了最先进的水平，具有强可解释性，代码已在GitHub上公开。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型有效解决了现有多模态仇恨视频检测方法的局限性，通过强调仇恨内容和系统化建模结构化关系，显著提升了分类性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;仇恨视频对在线安全和现实福祉构成严重风险，需要有效的检测方法。尽管整合多种模态信息的多模态分类方法优于单模态方法，但它们通常忽视了即使是最少量的仇恨内容也能定义视频类别。具体而言，它们通常统一处理所有内容，而非强调仇恨组件。此外，现有的多模态方法无法系统捕获视频中的结构化信息，限制了多模态融合的有效性。为解决这些局限性，我们提出了一种新颖的多模态双流图神经网络模型。它通过将给定视频分离为多个实例来构建实例图，以提取实例级特征。然后，互补权重图为这些特征分配重要性权重，突出仇恨实例。结合重要性权重和实例特征生成视频标签。我们的模型采用基于图的框架系统化建模模态内和跨模态的结构化关系。在公共数据集上的广泛实验表明，我们的模型在仇恨视频分类方面达到了最先进的水平，并具有强可解释性。代码可在以下网址获取：https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hateful videos present serious risks to online safety and real-worldwell-being, necessitating effective detection methods. Although multimodalclassification approaches integrating information from several modalitiesoutperform unimodal ones, they typically neglect that even minimal hatefulcontent defines a video's category. Specifically, they generally treat allcontent uniformly, instead of emphasizing the hateful components. Additionally,existing multimodal methods cannot systematically capture structuredinformation in videos, limiting the effectiveness of multimodal fusion. Toaddress these limitations, we propose a novel multimodal dual-stream graphneural network model. It constructs an instance graph by separating the givenvideo into several instances to extract instance-level features. Then, acomplementary weight graph assigns importance weights to these features,highlighting hateful instances. Importance weights and instance features arecombined to generate video labels. Our model employs a graph-based framework tosystematically model structured relationships within and across modalities.Extensive experiments on public datasets show that our model isstate-of-the-art in hateful video classification and has strong explainability.Code is available:https://github.com/Multimodal-Intelligence-Lab-MIL/MultiHateGNN.</description>
      <author>example@mail.com (Jiangbei Yue, Shuonan Yang, Tailin Chen, Jianbo Jiao, Zeyu Fu)</author>
      <guid isPermaLink="false">2509.13515v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Context-Aware Inputs: Physics-Inspired Improvements in Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10684v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了物理信息策略在改进NuGraph2架构语义分割方面的应用，特别关注提升对米歇尔电子等代表性不足粒子类别的识别性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在液态氩时间投影室的事件重建任务中显示出强大潜力，但对米歇尔电子等代表性不足的粒子类别性能仍然有限。&lt;h4&gt;目的&lt;/h4&gt;探索物理信息策略以改进NuGraph2架构中的语义分割，提高对米歇尔电子等粒子类别的识别能力。&lt;h4&gt;方法&lt;/h4&gt;研究三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器捕获类别级相关性；(iii)整合基于米歇尔电子能量分布的能量正则化项。&lt;h4&gt;主要发现&lt;/h4&gt;物理启发特征增强带来最大提升，特别显著提高了米歇尔电子的精确率和召回率；辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2缺乏明确的粒子或事件级表示。&lt;h4&gt;结论&lt;/h4&gt;将物理上下文直接嵌入节点级输入比施加任务特定辅助损失更有效；建议未来具有明确粒子和事件级推理的分层架构(如NuGraph3)将为高级解码器和基于物理的正则化提供更自然的环境。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近在液态氩时间投影室的事件重建任务中显示出强大的前景，但对于代表性不足的粒子类别(如米歇尔电子)其性能仍然有限。在这项工作中，我们研究了物理信息策略以改进NuGraph2架构中的语义分割。我们探索了三种互补方法：(i)通过从探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示，(ii)引入辅助解码器以捕获类别级相关性，以及(iii)整合受米歇尔电子能量分布启发的基于能量的正则化项。在MicroBooNE公共数据集上的实验表明，物理启发的特征增强带来了最大的收益，特别是通过解纠缠重叠的潜在空间区域，显著提高了米歇尔电子的精确率和召回率。相比之下，辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2的命中级别特性，缺乏明确的粒子或事件级表示。我们的发现强调，将物理上下文直接嵌入节点级输入比施加任务特定的辅助损失更有效，并表明未来的分层架构(如NuGraph3)具有明确的粒子和事件级推理，将为高级解码器和基于物理的正则化提供更自然的环境。这项工作的代码已在Github上公开可用：https://github.com/vitorgrizzi/nugraph_phys/tree/main_phys。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图神经网络在液态氩时间投影室(LArTPC)中对代表性不足粒子类别(特别是米歇尔电子)识别性能不佳的问题。这个问题很重要，因为米歇尔电子是粒子物理研究中的关键信号，准确识别不同类型粒子对于理解中微子相互作用和粒子物理现象至关重要，提高对少数类别的识别能力能改善整体粒子分类的可靠性，对高能物理实验的数据分析和物理发现具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于物理直觉探索了三种互补策略来注入物理领域知识：丰富输入表示、引入辅助解码器和结合基于能量的正则化项。他们借鉴了现有工作：参考了Drielsma的GrapPA研究引入几何描述符，借鉴了DUNE协作的CVN网络进行多任务学习，以及参考了Sharma等人的物理信息GNN将守恒定律作为损失项。作者系统性地评估了这些策略在NuGraph2架构中的适用性和有效性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将物理领域知识直接嵌入到图神经网络的输入表示中，而非仅通过辅助损失函数约束模型。整体实现流程包括：1)特征扩展，添加节点度、最短边长、导线差值和时间差值四个新特征；2)添加额外解码器，实验了二进制米歇尔解码器、图级米歇尔计数器和完整类别分布解码器；3)米歇尔能量正则化，在损失函数中添加基于物理的约束项，使用波形积分作为沉积能量的代理指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计特定的物理启发上下文感知输入特征；2)系统性评估三种物理知识注入策略；3)针对代表性不足类别优化识别性能。相比之前工作，不同之处在于：与GrapPA相比，专注于更广泛的语义分割而非仅电磁簇射组装；与CVN相比，通过特征增强而非多任务学习提高性能；与PINNs相比，将物理知识编码到输入特征而非直接施加物理方程；与原始NuGraph2相比，扩展了输入特征空间并评估了多种物理知识注入策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过在NuGraph2中引入物理启发的上下文感知输入特征，特别是节点度、几何距离和轨迹连续性度量，显著提高对代表性不足粒子类别(如米歇尔电子)的识别性能，同时证明了将领域知识直接嵌入输入表示比通过辅助损失函数约束模型更为有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have recently shown strong promise for eventreconstruction tasks in Liquid Argon Time Projection Chambers, yet theirperformance remains limited for underrepresented classes of particles, such asMichel electrons. In this work, we investigate physics-informed strategies toimprove semantic segmentation within the NuGraph2 architecture. We explorethree complementary approaches: (i) enriching the input representation withcontext-aware features derived from detector geometry and track continuity,(ii) introducing auxiliary decoders to capture class-level correlations, and(iii) incorporating energy-based regularization terms motivated by Michelelectron energy distributions. Experiments on MicroBooNE public datasets showthat physics-inspired feature augmentation yields the largest gains,particularly boosting Michel electron precision and recall by disentanglingoverlapping latent space regions. In contrast, auxiliary decoders andenergy-regularization terms provided limited improvements, partly due to thehit-level nature of NuGraph2, which lacks explicit particle- or event-levelrepresentations. Our findings highlight that embedding physics context directlyinto node-level inputs is more effective than imposing task-specific auxiliarylosses, and suggest that future hierarchical architectures such as NuGraph3,with explicit particle- and event-level reasoning, will provide a more naturalsetting for advanced decoders and physics-based regularization. The code forthis work is publicly available on Github athttps://github.com/vitorgrizzi/nugraph_phys/tree/main_phys.</description>
      <author>example@mail.com (Vitor F. Grizzi, Margaret Voetberg, Giuseppe Cerati, Hadi Meidani, V Hewes)</author>
      <guid isPermaLink="false">2509.10684v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Explainability: Post-hoc Explanations for Geometric Neural Network Predictions</title>
      <link>http://arxiv.org/abs/2509.10676v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究为科学应用中的AI方法引入了后验可解释性技术，通过多种解释方法共同分析图神经网络NuGraph2的决策过程，提高了AI在科学应用中的透明度和可靠性。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能在科学应用中的普及，需要能够将结果归因于网络推理过程的能力，以保持稳健的科学泛化。这促使了对AI模型可解释性的研究需求。&lt;h4&gt;目的&lt;/h4&gt;激励和展示后验可解释性方法在科学应用中AI方法的使用，特别是在中微子标记的图神经网络NuGraph2中的应用。&lt;h4&gt;方法&lt;/h4&gt;为现有的图神经网络NuGraph2引入可解释性附加组件，包括检查网络输出（节点分类）和边连接的技术，以及使用新通用工具探测潜在空间的方法。&lt;h4&gt;主要发现&lt;/h4&gt;没有单一的解释方法足以展示网络'理解'，但多种方法结合使用可以提供对分类过程中使用的见解。&lt;h4&gt;结论&lt;/h4&gt;这些可解释性方法虽然在NuGraph2上测试，但具有广泛适用性，可以应用于各种类型的网络，不仅限于图神经网络，为科学应用中的AI模型提供了更好的透明度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能在科学应用中的日益普及，将结果归因于网络推理过程的能力对于保持稳健的科学泛化至关重要。在这项工作中，我们旨在激励和展示后验可解释性方法在科学应用中AI方法的使用。为此，我们为现有的用于中微子标记的图神经网络NuGraph2引入了可解释性附加组件。解释形式包括一系列技术，检查网络的输出（节点分类）和它们之间的边连接，以及使用应用于该网络的新通用工具探测潜在空间。我们展示了没有这些方法中的任何一种足以单独展示网络'理解'，但它们共同可以提供对分类过程中使用的见解。虽然这些方法在NuGraph2应用上进行了测试，但它们可以应用于广泛的网络，不仅限于GNN。这项工作的代码已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing popularity of artificial intelligence used for scientificapplications, the ability of attribute a result to a reasoning process from thenetwork is in high demand for robust scientific generalizations to hold. Inthis work we aim to motivate the need for and demonstrate the use of post-hocexplainability methods when applied to AI methods used in scientificapplications. To this end, we introduce explainability add-ons to the existinggraph neural network (GNN) for neutrino tagging, NuGraph2. The explanationstake the form of a suite of techniques examining the output of the network(node classifications) and the edge connections between them, and probing ofthe latent space using novel general-purpose tools applied to this network. Weshow how none of these methods are singularly sufficient to show network"understanding", but together can give insights into the processes used inclassification. While these methods are tested on the NuGraph2 application,they can be applied to a broad range of networks, not limited to GNNs. The codefor this work is publicly available on GitHub athttps://github.com/voetberg/XNuGraph.</description>
      <author>example@mail.com (Margaret Voetberg, Vitor F. Grizzi, Giuseppe Cerati, Hadi Meidani, V Hewes)</author>
      <guid isPermaLink="false">2509.10676v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Past and Future: Distribution-Aware Alignment for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2509.14181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TimeAlign框架，通过表示对齐技术解决时间序列预测中历史输入与未来目标间的分布差距问题，提升预测性能，且与基础预测器架构无关，计算开销小。&lt;h4&gt;背景&lt;/h4&gt;表示学习技术如对比学习在计算机视觉和自然语言处理领域已取得成功，并在时间序列预测中也有探索。然而，最近最先进的预测器很少采用这些表示方法，因为它们显示出很小的性能优势。&lt;h4&gt;目的&lt;/h4&gt;挑战当前观点，证明显式表示对齐可以提供关键信息，弥合历史输入与未来目标之间的分布差距，从而提升时间序列预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出TimeAlign，一个轻量级即插即用框架，通过简单的重建任务学习辅助特征，并将其反馈给任何基础预测器。该方法与架构无关，计算开销小。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在八个基准测试上的广泛实验验证了TimeAlign的优越性能。2. 收益主要来自于修正历史输入和未来输出之间的频率不匹配。3. 研究提供了TimeAlign有效性的理论依据，证明它能增加学习表示与预测目标之间的互信息。&lt;h4&gt;结论&lt;/h4&gt;TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块，因为它与架构无关且计算开销小。&lt;h4&gt;翻译&lt;/h4&gt;表示学习技术如对比学习长期以来一直在时间序列预测中得到探索，反映了它们在计算机视觉和自然语言处理领域的成功。然而，最近的最先进预测器很少采用这些表示方法，因为它们显示出很小的性能优势。我们挑战这一观点，并证明显式表示对齐可以提供关键信息，弥合历史输入和未来目标之间的分布差距。为此，我们引入了TimeAlign，一个轻量级即插即用框架，通过简单的重建任务学习辅助特征，并将它们反馈给任何基础预测器。在八个基准测试上的广泛实验验证了其优越性能。进一步的研究表明，收益主要来自于修正历史输入和未来输出之间的频率不匹配。我们还提供了TimeAlign在增加学习表示与预测目标之间互信息方面的有效性的理论依据。由于它与架构无关且计算开销小，TimeAlign可作为现代深度学习时间序列预测系统的通用对齐模块。代码可在https://github.com/TROUBADOUR000/TimeAlign获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning techniques like contrastive learning have long beenexplored in time series forecasting, mirroring their success in computer visionand natural language processing. Yet recent state-of-the-art (SOTA) forecastersseldom adopt these representation approaches because they have shown littleperformance advantage. We challenge this view and demonstrate that explicitrepresentation alignment can supply critical information that bridges thedistributional gap between input histories and future targets. To this end, weintroduce TimeAlign, a lightweight, plug-and-play framework that learnsauxiliary features via a simple reconstruction task and feeds them back to anybase forecaster. Extensive experiments across eight benchmarks verify itssuperior performance. Further studies indicate that the gains arises primarilyfrom correcting frequency mismatches between historical inputs and futureoutputs. We also provide a theoretical justification for the effectiveness ofTimeAlign in increasing the mutual information between learned representationsand predicted targets. As it is architecture-agnostic and incurs negligibleoverhead, TimeAlign can serve as a general alignment module for modern deeplearning time-series forecasting systems. The code is available athttps://github.com/TROUBADOUR000/TimeAlign.</description>
      <author>example@mail.com (Yifan Hu, Jie Yang, Tian Zhou, Peiyuan Liu, Yujin Tang, Rong Jin, Liang Sun)</author>
      <guid isPermaLink="false">2509.14181v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>TopoSizing: An LLM-aided Framework of Topology-based Understanding and Sizing for AMS Circuits</title>
      <link>http://arxiv.org/abs/2509.14169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoSizing是一个端到端框架，能直接从原始网表进行稳健的电路理解，并将这种知识转化为优化收益，解决了模拟和混合信号电路设计中的数据质量和领域知识嵌入挑战。&lt;h4&gt;背景&lt;/h4&gt;模拟和混合信号电路设计面临高质量数据短缺和领域知识难以嵌入自动化流程的挑战。传统黑盒优化采样效率高但缺乏电路理解，基于学习的方法嵌入结构知识但特定于案例且重新训练成本高，而大语言模型方法常需要人工干预，限制了通用性和透明度。&lt;h4&gt;目的&lt;/h4&gt;提出TopoSizing框架，实现从原始网表直接进行稳健的电路理解，并将这种知识转化为优化收益，提高设计效率同时减少人工干预。&lt;h4&gt;方法&lt;/h4&gt;应用图算法将电路组织成层次化的器件-模块-阶段表示；LLM代理执行迭代假设-验证-细化循环，内置一致性检查产生明确注释；将验证过的见解集成到贝叶斯优化中，通过LLM引导的初始采样和停滞触发的信任区域更新提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;TopoSizing能够直接从原始网表理解电路并将这种理解转化为优化收益；通过LLM代理的迭代循环产生明确注释；验证过的见解可以集成到贝叶斯优化中，提高效率同时保持可行性。&lt;h4&gt;结论&lt;/h4&gt;TopoSizing是一个有前途的端到端框架，结合了电路理解和优化效率，解决了模拟和混合信号电路设计中的关键挑战，无需大量人工干预。&lt;h4&gt;翻译&lt;/h4&gt;模拟和混合信号电路设计仍然具有挑战性，这是由于高质量数据的缺乏以及将领域知识嵌入自动化流程的困难。传统的黑盒优化实现了采样效率，但缺乏电路理解，这常常导致在设计空间中的低价值区域浪费评估。相比之下，基于学习的方法嵌入结构知识，但特定于案例且重新训练成本高。最近使用大语言模型的尝试显示出潜力，但它们通常依赖人工干预，限制了通用性和透明度。我们提出了TopoSizing，一个端到端框架，直接从原始网表执行稳健的电路理解，并将这种知识转化为优化收益。我们的方法首先应用图算法将电路组织成层次化的器件-模块-阶段表示。然后，LLM代理执行迭代假设-验证-细化循环，内置一致性检查，产生明确的注释。验证过的见解通过LLM引导的初始采样和停滞触发的信任区域更新集成到贝叶斯优化中，提高了效率同时保持了可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog and mixed-signal circuit design remains challenging due to theshortage of high-quality data and the difficulty of embedding domain knowledgeinto automated flows. Traditional black-box optimization achieves samplingefficiency but lacks circuit understanding, which often causes evaluations tobe wasted in low-value regions of the design space. In contrast, learning-basedmethods embed structural knowledge but are case-specific and costly to retrain.Recent attempts with large language models show potential, yet they often relyon manual intervention, limiting generality and transparency. We proposeTopoSizing, an end-to-end framework that performs robust circuit understandingdirectly from raw netlists and translates this knowledge into optimizationgains. Our approach first applies graph algorithms to organize circuits into ahierarchical device-module-stage representation. LLM agents then execute aniterative hypothesis-verification-refinement loop with built-in consistencychecks, producing explicit annotations. Verified insights are integrated intoBayesian optimization through LLM-guided initial sampling andstagnation-triggered trust-region updates, improving efficiency whilepreserving feasibility.</description>
      <author>example@mail.com (Ziming Wei, Zichen Kong, Yuan Wang, David Z. Pan, Xiyuan Tang)</author>
      <guid isPermaLink="false">2509.14169v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>AD-DINOv3: Enhancing DINOv3 for Zero-Shot Anomaly Detection with Anomaly-Aware Calibration</title>
      <link>http://arxiv.org/abs/2509.14084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AD-DINOv3的新型视觉语言多模态框架，用于零样本异常检测(ZSAD)，通过结合DINOv3和CLIP模型的优势，解决了特征不对齐和异常区域识别困难的问题。&lt;h4&gt;背景&lt;/h4&gt;零样本异常检测(ZSAD)旨在识别任意新类别的异常，提供可扩展且标注高效的解决方案。传统方法大多基于CLIP模型，通过计算视觉和文本嵌入之间的相似性进行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力，但将其应用于ZSAD面临领域偏差和异常识别困难等挑战。&lt;h4&gt;目的&lt;/h4&gt;将DINOv3模型适应于零样本异常检测任务，解决特征不对齐和细微异常被误认为正常前景的问题，开发一个高效且通用的异常检测框架。&lt;h4&gt;方法&lt;/h4&gt;提出AD-DINOv3框架，将异常检测形式化为多模态对比学习问题。使用DINOv3作为视觉主干提取patch tokens和CLS token，CLIP文本编码器提供正常和异常提示的嵌入。引入轻量级适配器弥合领域差距，并设计异常感知校准模块(AACM)引导CLS token关注异常区域而非通用前景语义。&lt;h4&gt;主要发现&lt;/h4&gt;在八个工业和医疗基准上的大量实验表明，AD-DINOv3一致匹配或超越最先进的方法，验证了其作为通用零样本异常检测框架的优越性。轻量级适配器有效解决了领域偏差问题，AACM模块显著提高了对细微异常的识别能力。&lt;h4&gt;结论&lt;/h4&gt;AD-DINOv3框架成功将DINOv3模型应用于零样本异常检测，通过多模态对比学习和异常感知校准有效解决了传统方法面临的挑战，为异常检测领域提供了一个高效且通用的解决方案，具有广泛的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;零样本异常检测(ZSAD)旨在识别任意新类别的异常，提供可扩展且标注高效的解决方案。传统上，大多数ZSAD工作基于CLIP模型，通过计算视觉和文本嵌入之间的相似性来执行异常检测。最近，DINOv3等视觉基础模型展示了强大的可迁移表示能力。在这项工作中，我们首次将DINOv3适应于ZSAD。然而，这种适应带来了两个关键挑战：(i)大规模预训练数据与异常检测任务之间的领域偏差导致特征不对齐；(ii)预训练表示中对全局语义的固有倾向往往导致细微异常被误认为是正常前景对象的一部分，而不是被识别为异常区域。为了克服这些挑战，我们引入了AD-DINOv3，一个专为ZSAD设计的新型视觉语言多模态框架。具体来说，我们将异常检测形式化为多模态对比学习问题，其中DINOv3用作视觉主干提取patch tokens和CLS token，CLIP文本编码器为正常和异常提示提供嵌入。为了弥合领域差距，我们在两种模态中都引入了轻量级适配器，使它们的表示能够重新校准以适应异常检测任务。除了这种基线对齐外，我们还设计了一个异常感知校准模块(AACM)，明确引导CLS token关注异常区域而非通用前景语义，从而增强可区分性。在八个工业和医疗基准上的大量实验表明，AD-DINOv3一致匹配或超越最先进的方法，验证了其作为通用零样本异常检测框架的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-Shot Anomaly Detection (ZSAD) seeks to identify anomalies from arbitrarynovel categories, offering a scalable and annotation-efficient solution.Traditionally, most ZSAD works have been based on the CLIP model, whichperforms anomaly detection by calculating the similarity between visual andtext embeddings. Recently, vision foundation models such as DINOv3 havedemonstrated strong transferable representation capabilities. In this work, weare the first to adapt DINOv3 for ZSAD. However, this adaptation presents twokey challenges: (i) the domain bias between large-scale pretraining data andanomaly detection tasks leads to feature misalignment; and (ii) the inherentbias toward global semantics in pretrained representations often leads tosubtle anomalies being misinterpreted as part of the normal foreground objects,rather than being distinguished as abnormal regions. To overcome thesechallenges, we introduce AD-DINOv3, a novel vision-language multimodalframework designed for ZSAD. Specifically, we formulate anomaly detection as amultimodal contrastive learning problem, where DINOv3 is employed as the visualbackbone to extract patch tokens and a CLS token, and the CLIP text encoderprovides embeddings for both normal and abnormal prompts. To bridge the domaingap, lightweight adapters are introduced in both modalities, enabling theirrepresentations to be recalibrated for the anomaly detection task. Beyond thisbaseline alignment, we further design an Anomaly-Aware Calibration Module(AACM), which explicitly guides the CLS token to attend to anomalous regionsrather than generic foreground semantics, thereby enhancing discriminability.Extensive experiments on eight industrial and medical benchmarks demonstratethat AD-DINOv3 consistently matches or surpasses state-of-the-art methods,verifying its superiority as a general zero-shot anomaly detection framework.</description>
      <author>example@mail.com (Jingyi Yuan, Jianxiong Ye, Wenkang Chen, Chenqiang Gao)</author>
      <guid isPermaLink="false">2509.14084v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>PhenoGnet: A Graph-Based Contrastive Learning Framework for Disease Similarity Prediction</title>
      <link>http://arxiv.org/abs/2509.14037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PhenoGnet的新型基于图的对比学习框架，通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性，并在基准测试中表现出色，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;理解疾病相似性对于推进诊断、药物发现和个性化治疗策略至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发PhenoGnet框架，通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性。&lt;h4&gt;方法&lt;/h4&gt;PhenoGnet包含两个关键组件：内视图模型使用图卷积网络和图注意力网络分别编码基因和表型图；跨视图模型作为共享权重的多层感知机，通过对比学习对齐基因和表型嵌入。模型使用已知的基因-表型关联作为正对，随机采样的无关对作为负对进行训练，疾病通过其关联基因和/或表型的平均嵌入表示，通过余弦相似度计算成对相似度。&lt;h4&gt;主要发现&lt;/h4&gt;在包含1100个相似和866个不相似疾病对的基准测试中，基于基因的嵌入实现了0.9012的AUCPR和0.8764的AUROC，优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;PhenoGnet能够捕捉超越直接重叠的潜在生物学关系，为疾病相似性预测提供了可扩展且可解释的解决方案，具有在罕见疾病研究和精准医学中应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;理解疾病相似性对于推进诊断、药物发现和个性化治疗策略至关重要。我们提出了PhenoGnet，一种新颖的基于图的对比学习框架，旨在通过整合基因功能相互作用网络与人类表型本体来预测疾病相似性。PhenoGnet包含两个关键组件：一个内视图模型，分别使用图卷积网络和图注意力网络编码基因和表型图；以及一个跨视图模型，作为共享权重的多层感知机实现，通过对比学习对齐基因和表型嵌入。模型使用已知的基因-表型关联作为正对，随机采样的无关对作为负对进行训练。疾病通过其关联基因和/或表型的平均嵌入表示，成对相似度通过余弦相似度计算。在包含1100个相似和866个不相似疾病对的精选基准上的评估展示了强劲的性能，基于基因的嵌入实现了0.9012的AUCPR和0.8764的AUROC，优于现有的最先进方法。值得注意的是，PhenoGnet能够捕捉超越直接重叠的潜在生物学关系，为疾病相似性预测提供了可扩展且可解释的解决方案。这些结果强调了其在罕见疾病研究和精准医学中下游应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding disease similarity is critical for advancing diagnostics, drugdiscovery, and personalized treatment strategies. We present PhenoGnet, a novelgraph-based contrastive learning framework designed to predict diseasesimilarity by integrating gene functional interaction networks with the HumanPhenotype Ontology (HPO). PhenoGnet comprises two key components: an intra-viewmodel that separately encodes gene and phenotype graphs using GraphConvolutional Networks (GCNs) and Graph Attention Networks (GATs), and a crossview model implemented as a shared weight multilayer perceptron (MLP) thataligns gene and phenotype embeddings through contrastive learning. The model istrained using known gene phenotype associations as positive pairs and randomlysampled unrelated pairs as negatives. Diseases are represented by the meanembeddings of their associated genes and/or phenotypes, and pairwise similarityis computed via cosine similarity. Evaluation on a curated benchmark of 1,100similar and 866 dissimilar disease pairs demonstrates strong performance, withgene based embeddings achieving an AUCPR of 0.9012 and AUROC of 0.8764,outperforming existing state of the art methods. Notably, PhenoGnet captureslatent biological relationships beyond direct overlap, offering a scalable andinterpretable solution for disease similarity prediction. These resultsunderscore its potential for enabling downstream applications in rare diseaseresearch and precision medicine.</description>
      <author>example@mail.com (Ranga Baminiwatte, Kazi Jewel Rana, Aaron J. Masino)</author>
      <guid isPermaLink="false">2509.14037v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation</title>
      <link>http://arxiv.org/abs/2509.14036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出基于问题的手语翻译（QB-SLT）任务，探索对话上下文的高效集成，并开发了SSL-SSAF方法，利用问题文本和手语序列进行特征对齐和自适应提取，在两个新数据集上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;手语翻译旨在弥合聋人与健听人之间的沟通鸿沟，对话提供重要上下文线索。传统方法依赖手语标注（gloss annotations），但标注过程较为困难，而对话自然存在于交流中且更易标注。&lt;h4&gt;目的&lt;/h4&gt;探索对话在手语翻译中的高效集成，解决多模态特征对齐挑战，利用问题上下文提高翻译质量，开发一种利用易获取问题文本替代难度较高手语标注的方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨模态自监督学习与Sigmoid自注意力加权融合方法（SSL-SSAF），使用对比学习对齐多模态特征，引入Sigmoid自注意力加权模块进行自适应特征提取，通过自监督学习利用问题文本增强表示和翻译能力。&lt;h4&gt;主要发现&lt;/h4&gt;SSL-SSAF在CSL-Daily-QA和PHOENIX-2014T-QA数据集上取得最先进性能；易获取的问题文本辅助可达到甚至超过手语标注的性能；可视化结果表明整合对话有助于提高翻译质量。&lt;h4&gt;结论&lt;/h4&gt;通过引入对话上下文特别是问题文本，可以显著改进手语翻译效果，无需依赖难度较高的手语标注，同时达到或超过传统方法性能，为手语翻译领域提供新方向。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译（SLT）弥合了聋人与健听人之间的沟通差距，其中对话为翻译提供了关键的上下文线索。基于这一基础概念，本文提出了基于问题的手语翻译（QB-SLT），这是一个探索对话高效集成的新任务。与手语标注（手语转录）不同，对话自然出现在交流中，且更容易标注。关键挑战在于对齐多模态特征，同时利用问题上下文提高翻译质量。为解决这个问题，我们提出了用于手语翻译的跨模态自监督学习与Sigmoid自注意力加权融合方法（SSL-SSAF）。具体来说，我们在QB-SLT中使用对比学习对齐多模态特征，然后引入Sigmoid自注意力加权（SSAW）模块，从问题和手语序列中进行自适应特征提取。此外，我们通过自监督学习利用可用的问题文本来增强表示和翻译能力。我们在新构建的CSL-Daily-QA和PHOENIX-2014T-QA数据集上评估了我们的方法，SSL-SSAF取得了最先进的性能。值得注意的是，易于获取的问题文本辅助可以达到甚至超过手语标注的性能。此外，可视化结果表明，整合对话有助于提高翻译质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign Language Translation (SLT) bridges the communication gap between deafpeople and hearing people, where dialogue provides crucial contextual cues toaid in translation. Building on this foundational concept, this paper proposesQuestion-based Sign Language Translation (QB-SLT), a novel task that exploresthe efficient integration of dialogue. Unlike gloss (sign languagetranscription) annotations, dialogue naturally occurs in communication and iseasier to annotate. The key challenge lies in aligning multimodality featureswhile leveraging the context of the question to improve translation. To addressthis issue, we propose a cross-modality Self-supervised Learning with SigmoidSelf-attention Weighting (SSL-SSAW) fusion method for sign languagetranslation. Specifically, we employ contrastive learning to alignmultimodality features in QB-SLT, then introduce a Sigmoid Self-attentionWeighting (SSAW) module for adaptive feature extraction from question and signlanguage sequences. Additionally, we leverage available question text throughself-supervised learning to enhance representation and translationcapabilities. We evaluated our approach on newly constructed CSL-Daily-QA andPHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably,easily accessible question assistance can achieve or even surpass theperformance of gloss assistance. Furthermore, visualization results demonstratethe effectiveness of incorporating dialogue in improving translation quality.</description>
      <author>example@mail.com (Zekang Liu, Wei Feng, Fanhua Shang, Lianyu Hu, Jichao Feng, Liqing Gao)</author>
      <guid isPermaLink="false">2509.14036v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Noise Supervised Contrastive Learning and Feature-Perturbed for Anomalous Sound Detection</title>
      <link>http://arxiv.org/abs/2509.13853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种单阶段监督对比学习方法（OS-SCL）和TFgram特征，有效解决了无监督异常声音检测中的误报问题，并在DCASE 2020挑战赛上取得了优异的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进步，但在处理来自不同机器的相同类型样本时，仍然存在频繁误报的问题。&lt;h4&gt;目的&lt;/h4&gt;解决无监督异常声音检测中处理来自不同机器的相同类型样本时出现的频繁误报问题。&lt;h4&gt;方法&lt;/h4&gt;提出单阶段监督对比学习（OS-SCL）技术，通过扰动嵌入空间中的特征并采用单阶段噪声监督对比学习方法；同时提出一种名为TFgram的时间-频率特征，从原始音频中提取以捕获异常声音检测的关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64%的AUC、88.42%的pAUC和89.24%的mAUC；使用TFgram特征后，性能提升到95.71%的AUC、90.23%的pAUC和91.23%的mAUC。&lt;h4&gt;结论&lt;/h4&gt;所提出的OS-SCL方法和TFgram特征能有效提高无监督异常声音检测的性能，显著减少了误报情况，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;无监督异常声音检测旨在仅使用正常音频数据训练模型来检测未知异常声音。尽管自监督方法有所进步，但在处理来自不同机器的相同类型样本时，仍然存在频繁误报的问题。本文介绍了一种名为单阶段监督对比学习（OS-SCL）的新型训练技术，通过扰动嵌入空间中的特征并采用单阶段噪声监督对比学习方法，显著解决了这一问题。在DCASE 2020挑战赛任务2上，仅使用Log-Mel特征就达到了94.64%的AUC、88.42%的pAUC和89.24%的mAUC。此外，还提出了一种名为TFgram的时间-频率特征，从原始音频中提取。该特征能有效捕获异常声音检测的关键信息，最终达到95.71%的AUC、90.23%的pAUC和91.23%的mAUC。源代码可在www.github.com/huangswt/OS-SCL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised anomalous sound detection aims to detect unknown anomaloussounds by training a model using only normal audio data. Despite advancementsin self-supervised methods, the issue of frequent false alarms when handlingsamples of the same type from different machines remains unresolved. This paperintroduces a novel training technique called one-stage supervised contrastivelearning (OS-SCL), which significantly addresses this problem by perturbingfeatures in the embedding space and employing a one-stage noisy supervisedcontrastive learning approach. On the DCASE 2020 Challenge Task 2, it achieved94.64\% AUC, 88.42\% pAUC, and 89.24\% mAUC using only Log-Mel features.Additionally, a time-frequency feature named TFgram is proposed, which isextracted from raw audio. This feature effectively captures criticalinformation for anomalous sound detection, ultimately achieving 95.71\% AUC,90.23\% pAUC, and 91.23\% mAUC. The source code is available at:\underline{www.github.com/huangswt/OS-SCL}.</description>
      <author>example@mail.com (Shun Huang, Zhihua Fang, Liang He)</author>
      <guid isPermaLink="false">2509.13853v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Masked Feature Modeling Enhances Adaptive Segmentation</title>
      <link>http://arxiv.org/abs/2509.13801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种掩码特征建模方法，用于无监督域适应语义分割任务，通过在特征空间进行掩码和重建，提高了分割性能且不增加计算开销。&lt;h4&gt;背景&lt;/h4&gt;无监督域适应用于语义分割旨在将有标签源域的模型迁移到无标签目标域。虽然对比学习等辅助自监督任务已提高特征区分性，但掩码建模方法在此领域探索不足，主要因架构不兼容和优化目标不一致。&lt;h4&gt;目的&lt;/h4&gt;提出掩码特征建模作为辅助任务，直接在特征空间进行掩码和重建，使其学习目标与主要分割任务对齐，确保与标准架构兼容。&lt;h4&gt;方法&lt;/h4&gt;提出掩码特征建模方法，引入轻量级辅助模块Rebuilder进行联合训练但在推理时丢弃，利用分割解码器对重建特征进行分类，将辅助目标与逐像素预测任务紧密耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在各种架构和UDA基准上的大量实验表明，MFM一致提高分割性能，是一种简单、高效且可推广的策略。&lt;h4&gt;结论&lt;/h4&gt;MFM为无监督域适应语义分割提供了简单、高效且通用的解决方案，通过将辅助目标与主要任务对齐，避免了与主要任务的干扰。&lt;h4&gt;翻译&lt;/h4&gt;无监督域适应用于语义分割旨在将有标签源域的模型迁移到无标签目标域。虽然辅助自监督任务，特别是对比学习，已提高特征区分性，但掩码建模方法在此设置中探索不足，主要因架构不兼容和优化目标不一致。我们提出掩码特征建模，一种在特征空间直接执行特征掩码和重建的新型辅助任务。与现有掩码建模方法不同，MFM将其学习目标与主要分割任务对齐，确保与DeepLab和DAFormer等标准架构兼容，无需修改推理流程。为有效重建，我们引入轻量级辅助模块Rebuilder，该模块联合训练但在推理时丢弃，测试时零计算开销。关键是，MFM利用分割解码器对重建特征进行分类，将辅助目标与逐像素预测任务紧密耦合，避免干扰主要任务。在各种架构和UDA基准上的大量实验表明，MFM一致提高分割性能，为无监督域适应语义分割提供了简单、高效且可推广的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised domain adaptation (UDA) for semantic segmentation aims totransfer models from a labeled source domain to an unlabeled target domain.While auxiliary self-supervised tasks-particularly contrastive learning-haveimproved feature discriminability, masked modeling approaches remainunderexplored in this setting, largely due to architectural incompatibility andmisaligned optimization objectives. We propose Masked Feature Modeling (MFM), anovel auxiliary task that performs feature masking and reconstruction directlyin the feature space. Unlike existing masked modeling methods that reconstructlow-level inputs or perceptual features (e.g., HOG or visual tokens), MFMaligns its learning target with the main segmentation task, ensuringcompatibility with standard architectures like DeepLab and DAFormer withoutmodifying the inference pipeline. To facilitate effective reconstruction, weintroduce a lightweight auxiliary module, Rebuilder, which is trained jointlybut discarded during inference, adding zero computational overhead at testtime. Crucially, MFM leverages the segmentation decoder to classify thereconstructed features, tightly coupling the auxiliary objective with thepixel-wise prediction task to avoid interference with the primary task.Extensive experiments across various architectures and UDA benchmarksdemonstrate that MFM consistently enhances segmentation performance, offering asimple, efficient, and generalizable strategy for unsupervised domain-adaptivesemantic segmentation.</description>
      <author>example@mail.com (Wenlve Zhou, Zhiheng Zhou, Tiantao Xian, Yikui Zhai, Weibin Wu, Biyun Ma)</author>
      <guid isPermaLink="false">2509.13801v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>VocSegMRI: Multimodal Learning for Precise Vocal Tract Segmentation in Real-time MRI</title>
      <link>http://arxiv.org/abs/2509.13767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to ICASSP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VocSegMRI的多模态框架，通过交叉注意力融合整合视频、音频和音韵输入，实现了在实时磁共振成像中更准确的发音结构分割。&lt;h4&gt;背景&lt;/h4&gt;在实时磁共振成像(rtMRI)中准确分割发音结构具有挑战性，因为大多数现有方法几乎完全依赖于视觉线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够整合声学和音韵信号的多模态框架，以提高发音结构分割的精度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出VocSegMRI多模态框架，通过交叉注意力融合整合视频、音频和音韵输入实现动态特征对齐，并引入对比学习目标以增强跨模态表示，使模型在推理时音频模态不可用的情况下仍能保持良好性能。&lt;h4&gt;主要发现&lt;/h4&gt;在USC-75 rtMRI数据集子集上评估，VocSegMRI实现了最先进的性能，Dice得分为0.95，第95百分位Hausdorff距离为4.20毫米，优于单模态和多模态基线；消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。&lt;h4&gt;结论&lt;/h4&gt;集成多模态建模对于准确的声道分析具有重要价值，VocSegMRI框架能够有效利用多种信号源提高分割性能。&lt;h4&gt;翻译&lt;/h4&gt;在实时磁共振成像中准确分割发音结构仍然具有挑战性，因为大多数现有方法几乎完全依赖于视觉线索。然而，同步的声学和音韵信号提供了补充的上下文，可以丰富视觉信息并提高精度。在本文中，我们引入了VocSegMRI，一个通过交叉注意力融合整合视频、音频和音韵输入的多模态框架，用于动态特征对齐。为了进一步增强跨模态表示，我们纳入了对比学习目标，即使在推理时音频模态不可用，也能提高分割性能。在USC-75 rtMRI数据集的一个子集上评估，我们的方法实现了最先进的性能，Dice得分为0.95，第95百分位Hausdorff距离为4.20毫米，优于单模态和多模态基线。消融研究证实了交叉注意力和对比学习对分割精度和鲁棒性的贡献。这些结果强调了集成多模态建模对准确声道分析的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately segmenting articulatory structures in real-time magnetic resonanceimaging (rtMRI) remains challenging, as most existing methods rely almostentirely on visual cues. Yet synchronized acoustic and phonological signalsprovide complementary context that can enrich visual information and improveprecision. In this paper, we introduce VocSegMRI, a multimodal framework thatintegrates video, audio, and phonological inputs through cross-attention fusionfor dynamic feature alignment. To further enhance cross-modal representation,we incorporate a contrastive learning objective that improves segmentationperformance even when the audio modality is unavailable at inference. Evaluatedon a sub-set of USC-75 rtMRI dataset, our approach achieves state-of-the-artperformance, with a Dice score of 0.95 and a 95th percentile Hausdorff Distance(HD_95) of 4.20 mm, outperforming both unimodal and multimodal baselines.Ablation studies confirm the contributions of cross-attention and contrastivelearning to segmentation precision and robustness. These results highlight thevalue of integrative multimodal modeling for accurate vocal tract analysis.</description>
      <author>example@mail.com (Daiqi Liu, Tomás Arias-Vergara, Johannes Enk, Fangxu Xing, Maureen Stone, Jerry L. Prince, Jana Hutter, Andreas Maier, Jonghye Woo, Paula Andrea Pérez-Toro)</author>
      <guid isPermaLink="false">2509.13767v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Enhanced Cross-Modal Place Recognition for Robust Robot Localization</title>
      <link>http://arxiv.org/abs/2509.13474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为语义增强跨模态地点识别（SCM-PR）的框架，通过结合RGB图像的高层语义与LiDAR地图的几何信息，提高了在无GPS环境中的机器人定位精度，特别是在复杂场景和视点变化的情况下。&lt;h4&gt;背景&lt;/h4&gt;在没有GPS能力的环境中确保机器人精确定位具有挑战性。现有基于RGB的视觉地点识别技术对光照、天气和季节性变化敏感，而现有的跨模态定位方法虽利用RGB图像和3D LiDAR地图的几何特性来减少这些敏感性问题，但在复杂场景、细粒度匹配和视点变化情况下仍表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合高层语义的框架，利用RGB图像在LiDAR地图中实现更鲁棒的定位，解决当前方法在复杂场景、细粒度或高分辨率匹配以及视点变化情况下的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了SCM-PR框架，包含：1) 使用VMamba骨干网络进行RGB图像特征提取；2) 语义感知特征融合（SAFF）模块，同时使用地点描述符和分割掩码；3) 结合语义和几何的LiDAR描述符；4) 在NetVLAD中引入跨模态语义注意力机制提高匹配性能；5) 在对比学习框架中设计了多视图语义几何匹配和语义一致性损失。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和KITTI-360数据集上的实验表明，SCM-PR与其他跨模态地点识别方法相比实现了最先进的性能，特别是在复杂场景和视点变化的情况下表现更优。&lt;h4&gt;结论&lt;/h4&gt;语义信息的整合对于在复杂环境中实现鲁棒的机器人定位至关重要。SCM-PR框架有效解决了现有方法在复杂场景、细粒度匹配和视点变化情况下的局限性，提高了无GPS环境中的定位精度。&lt;h4&gt;翻译&lt;/h4&gt;确保机器人在没有GPS能力的环境中精确定位是一项具有挑战性的任务。视觉地点识别技术有可能实现这一目标，但现有的基于RGB的方法对光照、天气和其他季节性变化敏感。现有的跨模态定位方法利用RGB图像和3D LiDAR地图的几何特性来减少上述敏感性问题。目前，最先进的方法在复杂场景、细粒度或高分辨率匹配以及视点变化的情况下表现不佳。在这项工作中，我们提出了一个名为语义增强跨模态地点识别的框架，该框架结合利用RGB图像的高层语义，在LiDAR地图中实现鲁棒的定位。我们提出的方法引入：用于RGB图像特征提取的VMamba骨干网络；用于同时使用地点描述符和分割掩码的语义感知特征融合模块；结合语义和几何的LiDAR描述符；以及在NetVLAD中引入跨模态语义注意力机制以提高匹配性能。整合语义信息对于设计多视图语义几何匹配和语义一致性损失也至关重要，两者都在对比学习框架中。我们在KITTI和KITTI-360数据集上的实验表明，与其他跨模态地点识别方法相比，SCM-PR实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人在没有GPS信号的环境中准确定位的挑战。传统基于RGB图像的定位方法对光照、天气和季节变化敏感，而现有跨模态方法在复杂场景、细粒度匹配和视角变化时表现不佳。这个问题在现实世界中非常重要，因为随着自动驾驶和机器人技术的发展，特别是在GPS信号不可靠或不可用的环境（如城市峡谷、室内、地下空间等），机器人需要可靠的定位能力来安全导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有跨模态方法主要依赖低级几何特征，在几何相似但语义不同的场景中会产生歧义。他们思考通过整合高级语义信息可以提供更丰富的上下文理解，增强定位鲁棒性。作者借鉴了多项现有工作：使用VMamba作为RGB图像特征提取的骨干网络，采用预训练3D语义分割模型处理激光雷达数据，利用NetVLAD结构生成全局描述符，以及应用对比学习框架对齐不同模态特征。这些借鉴被创新性地组合并扩展，形成了独特的语义增强跨模态定位框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过深度融合高级语义信息来增强跨模态定位的鲁棒性和准确性，使系统能理解场景中的物体和结构而不仅仅是匹配外观或形状。整体流程分为三部分：1) RGB图像分支使用VMamba骨干网络提取特征，通过SAFF模块生成位置描述符和语义掩码，并应用跨模态语义注意力机制；2) 激光雷达分支将点云转换为距离图像，应用语义分割生成语义标签，构建语义-几何混合描述符，并生成多视角描述符；3) 全局描述符生成和匹配阶段使用NetVLAD结构，结合多视角语义-几何匹配方法，在对比学习框架中使用语义一致性损失进行训练，确保相同语义类别的特征在嵌入空间中聚集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 语义增强跨模态定位框架(SCM-PR)；2) 语义感知特征融合(SAFF)模块，同时生成位置描述符和语义掩码；3) 语义-几何混合描述符，结合几何和语义信息；4) 跨模态语义注意力机制，使RGB描述符关注激光雷达中对应语义区域；5) 多视角语义-几何匹配方法，同时考虑几何重叠和语义一致性；6) 语义一致性损失，确保相同语义类别的特征在嵌入空间中接近。相比之前工作，SCM-PR深度整合了高级语义信息而非仅依赖低级几何特征，实现了双向语义利用和细粒度语义对齐，在复杂环境和视角变化下表现更佳。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了语义增强跨模态定位框架(SCM-PR)，通过深度整合RGB图像的高级语义信息和激光雷达地图的几何信息，显著提高了在复杂环境和视角变化下的机器人定位准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring accurate localization of robots in environments without GPScapability is a challenging task. Visual Place Recognition (VPR) techniques canpotentially achieve this goal, but existing RGB-based methods are sensitive tochanges in illumination, weather, and other seasonal changes. Existingcross-modal localization methods leverage the geometric properties of RGBimages and 3D LiDAR maps to reduce the sensitivity issues highlighted above.Currently, state-of-the-art methods struggle in complex scenes, fine-grained orhigh-resolution matching, and situations where changes can occur in viewpoint.In this work, we introduce a framework we call Semantic-Enhanced Cross-ModalPlace Recognition (SCM-PR) that combines high-level semantics utilizing RGBimages for robust localization in LiDAR maps. Our proposed method introduces: aVMamba backbone for feature extraction of RGB images; a Semantic-Aware FeatureFusion (SAFF) module for using both place descriptors and segmentation masks;LiDAR descriptors that incorporate both semantics and geometry; and across-modal semantic attention mechanism in NetVLAD to improve matching.Incorporating the semantic information also was instrumental in designing aMulti-View Semantic-Geometric Matching and a Semantic Consistency Loss, both ina contrastive learning framework. Our experimental work on the KITTI andKITTI-360 datasets show that SCM-PR achieves state-of-the-art performancecompared to other cross-modal place recognition methods.</description>
      <author>example@mail.com (Yujia Lin, Nicholas Evans)</author>
      <guid isPermaLink="false">2509.13474v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Supervised and Unsupervised Deep Learning Applied to the Majority Vote Model</title>
      <link>http://arxiv.org/abs/2509.14155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习技术研究多数投票模型中的连续相变临界性质，结合主成分分析方法，通过监督学习和无监督学习准确识别临界点并估计临界指数。&lt;h4&gt;背景&lt;/h4&gt;多数投票模型的连续相变临界性质研究传统上依赖蒙特卡洛方法，本研究探索使用深度学习技术替代或补充传统方法。&lt;h4&gt;目的&lt;/h4&gt;利用深度学习技术探究多数投票模型的相变临界性质，并验证深度学习方法在相变研究中的有效性和准确性。&lt;h4&gt;方法&lt;/h4&gt;结合深度学习和主成分分析，通过监督学习在动力学蒙特卡洛生成的自旋构型数据上训练密集神经网络；使用主成分分析重现磁化；应用变分自编码器进行深度无监督学习来重构和生成自旋构型。&lt;h4&gt;主要发现&lt;/h4&gt;神经网络能够准确识别正方形和三角形晶格上的临界点；变分自编码器通过损失函数检测相变；真实数据与重构数据之间的关联函数在临界点具有普适性；变分自编码器可作为生成模型产生人工自旋构型。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法能够有效研究相变临界性质，变分自编码器不仅能检测相变，还能作为生成模型产生符合物理规律的人工自旋构型。&lt;h4&gt;翻译&lt;/h4&gt;我们采用深度学习技术研究多数投票模型中连续相变的临界性质。除了深度学习外，还利用主成分分析来分析相变。对于监督学习，我们在通过动力学蒙特卡洛方法生成的自旋构型数据上训练密集神经网络。使用独立模拟的构型数据，神经网络能够准确识别正方形和三角形晶格上的临界点。使用主成分分析的经典无监督学习重现了磁化，并能够估计通常通过蒙特卡洛重要性采样获得的临界指数。此外，使用变分自编码器进行深度无监督学习，它们重构输入自旋构型并生成人工输出。自编码器通过损失函数检测相变，量化了基本数据特征的保留。我们定义了真实数据与重构数据之间的关联函数，发现该关联函数在临界点是普适的。变分自编码器还可作为生成模型，产生人工自旋构型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We employ deep learning techniques to investigate the critical properties ofthe continuous phase transition in the majority vote model. In addition to deeplearning, principal component analysis is utilized to analyze the transition.For supervised learning, dense neural networks are trained on spinconfiguration data generated via the kinetic Monte Carlo method. Usingindependently simulated configuration data, the neural network accuratelyidentifies the critical point on both square and triangular lattices. Classicalunsupervised learning with principal component analysis reproduces themagnetization and enables estimation of critical exponents, typically obtainedvia Monte Carlo importance sampling. Furthermore, deep unsupervised learning isperformed using variational autoencoders, which reconstruct input spinconfigurations and generate artificial outputs. The autoencoders detect thephase transition through the loss function, quantifying the preservation ofessential data features. We define a correlation function between the real andreconstructed data, and find that this correlation function is universal at thecritical point. Variational autoencoders also serve as generative models,producing artificial spin configurations.</description>
      <author>example@mail.com (J. F. Silva Neto, D. S. M. Alencar, L. T. Brito, G. A. Alves, F. W. S. Lima, A. Macedo-Filho, R. S. Ferreira, T. F. A. Alves)</author>
      <guid isPermaLink="false">2509.14155v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>WatchAnxiety: A Transfer Learning Approach for State Anxiety Prediction from Smartwatch Data</title>
      <link>http://arxiv.org/abs/2509.13725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于智能手表的系统，用于测量和预测社交焦虑大学生的日常焦虑波动，实现了约60%的准确率，为实时个性化干预提供了可能。&lt;h4&gt;背景&lt;/h4&gt;社交焦虑是一种常见心理健康问题，与学业、社交和职业功能显著相关。其核心特征是在社交情境中的即时焦虑升高，但之前很少有研究测量或预测这种焦虑的日常波动。&lt;h4&gt;目的&lt;/h4&gt;捕捉社交焦虑的日内动态，为设计实时、个性化的干预措施(如JITAIs)提供依据。&lt;h4&gt;方法&lt;/h4&gt;对91名社交焦虑大学生(排除后72名)进行研究，使用定制智能手表系统，平均持续9.03天；每天进行7次生态瞬时评估(EMA)报告状态焦虑；基于外部心率数据开发基础模型，迁移表示并进行微调以生成概率预测；将这些预测与特质水平测量结合到元学习器中。&lt;h4&gt;主要发现&lt;/h4&gt;在数据集中，管道在状态焦虑检测中达到60.4%的平衡准确率；在TILES-18数据集的独立保留集上(10,095次每日EMA)，该方法达到59.1%的平衡准确率，优于先前工作至少7%。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效预测社交焦虑的日常波动，为个性化实时干预提供了可能性，有助于改善社交焦虑人群的日常功能。&lt;h4&gt;翻译&lt;/h4&gt;社交焦虑是一种常见心理健康问题，与学业、社交和职业功能方面的重大挑战相关。一个核心特征是在社交情境中即时(状态)焦虑升高，但之前很少有研究测量或预测这种焦虑在一天中的波动。捕捉这些日内动态对于设计实时、个性化的干预措施(如JITAIs)至关重要。为解决这一空白，我们对社交焦虑大学生(N=91；排除后72人)进行了研究，使用我们定制的基于智能手表的系统，平均9.03天(SD = 2.95)。参与者每天接受七次生态瞬时评估(EMA)以报告状态焦虑。我们在超过10,000天的外部心率数据上开发了基础模型，将其表示迁移到我们的数据集，并进行微调以生成概率预测。这些预测与特质水平测量在元学习器中结合。我们的管道在我们的数据集中实现了60.4%的状态焦虑检测平衡准确率。为了评估泛化能力，我们将训练方法应用于TILES-18数据集的独立保留集-与预训练相同的 dataset。在10,095次每日EMA上，我们的方法实现了59.1%的平衡准确率，优于先前工作至少7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social anxiety is a common mental health condition linked to significantchallenges in academic, social, and occupational functioning. A core feature iselevated momentary (state) anxiety in social situations, yet little prior workhas measured or predicted fluctuations in this anxiety throughout the day.Capturing these intra-day dynamics is critical for designing real-time,personalized interventions such as Just-In-Time Adaptive Interventions(JITAIs). To address this gap, we conducted a study with socially anxiouscollege students (N=91; 72 after exclusions) using our custom smartwatch-basedsystem over an average of 9.03 days (SD = 2.95). Participants received sevenecological momentary assessments (EMAs) per day to report state anxiety. Wedeveloped a base model on over 10,000 days of external heart rate data,transferred its representations to our dataset, and fine-tuned it to generateprobabilistic predictions. These were combined with trait-level measures in ameta-learner. Our pipeline achieved 60.4% balanced accuracy in state anxietydetection in our dataset. To evaluate generalizability, we applied the trainingapproach to a separate hold-out set from the TILES-18 dataset-the same datasetused for pretraining. On 10,095 once-daily EMAs, our method achieved 59.1%balanced accuracy, outperforming prior work by at least 7%.</description>
      <author>example@mail.com (Md Sabbir Ahmed, Noah French, Mark Rucker, Zhiyuan Wang, Taylor Myers-Brower, Kaitlyn Petz, Mehdi Boukhechba, Bethany A. Teachman, Laura E. Barnes)</author>
      <guid isPermaLink="false">2509.13725v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models</title>
      <link>http://arxiv.org/abs/2509.13706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种自然语言处理工具，用于自动检测放射肿瘤学中的高严重程度事件报告，解决了人工审查耗时且需要专业知识的问题。&lt;h4&gt;背景&lt;/h4&gt;事件报告是医疗保健中安全和质量改进的重要工具，但人工审查耗时且需要专业知识。&lt;h4&gt;目的&lt;/h4&gt;展示一个自然语言处理（NLP）筛查工具，用于在两个医疗机构中检测放射肿瘤学中的高严重程度事件报告。&lt;h4&gt;方法&lt;/h4&gt;使用两个文本来训练和评估NLP模型：7,094份来自我们机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告都由临床内容专家标记了严重程度分数。训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT（一种在PubMed摘要和住院患者数据上预训练的大型语言模型）。通过两种方式评估了模型的泛化能力：使用在Inst.-train上训练的模型在SF-test上进行评估；训练了一个BlueBERT_TRANSFER模型，先在Inst-train上微调，然后在SF-train上再次微调，最后在SF-test集上测试。为了进一步分析模型性能，还检查了我们机构数据集中的59份报告子集，这些报告经过人工编辑以提高清晰度。&lt;h4&gt;主要发现&lt;/h4&gt;在Inst.测试上的分类性能，SVM达到AUROC 0.82，BlueBERT达到0.81。在没有跨机构迁移学习的情况下，在SF测试上的性能有限，SVM仅为0.42，BlueBERT为0.56。在两个数据集上均进行微调的BlueBERT_TRANSFER将SF测试的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在人工编辑的Inst.报告上的性能（AUROC 0.85和0.74）与人类性能（AUROC 0.81）相似。&lt;h4&gt;结论&lt;/h4&gt;成功地在放射肿瘤学中心的事件报告文本上开发了跨机构NLP模型。这些模型能够在经过整理的数据集上像人类一样检测高严重程度报告。&lt;h4&gt;翻译&lt;/h4&gt;目的：事件报告是医疗保健中安全和质量改进的重要工具，但人工审查耗时且需要专业知识。在此，我们展示了一个自然语言处理（NLP）筛查工具，用于在两个医疗机构中检测放射肿瘤学中的高严重程度事件报告。方法与材料：我们使用两个文本来训练和评估我们的NLP模型：7,094份来自我们机构（Inst.）的报告和571份来自IAEA SAFRON（SF）的报告，所有报告都由临床内容专家标记了严重程度分数。我们训练和评估了两种类型的模型：基线支持向量机（SVM）和BlueBERT（一种在PubMed摘要和住院患者数据上预训练的大型语言模型）。我们通过两种方式评估了模型的泛化能力。首先，我们评估了使用Inst.-train训练后在SF-test上测试的模型。其次，我们训练了一个BlueBERT_TRANSFER模型，先在Inst.-train上微调，然后在SF-train上再次微调，最后在SF-test集上测试。为了进一步分析模型性能，我们还检查了我们机构数据集中的59份报告子集，这些报告经过人工编辑以提高清晰度。结果：在Inst.测试上的分类性能，SVM达到AUROC 0.82，BlueBERT达到0.81。在没有跨机构迁移学习的情况下，在SF测试上的性能有限，SVM仅为0.42，BlueBERT为0.56。在两个数据集上均进行微调的BlueBERT_TRANSFER将SF测试的性能提高到AUROC 0.78。SVM和BlueBERT_TRANSFER模型在人工编辑的Inst.报告上的性能（AUROC 0.85和0.74）与人类性能（AUROC 0.81）相似。结论：总之，我们成功地在放射肿瘤学中心的事件报告文本上开发了跨机构NLP模型。这些模型能够在经过整理的数据集上像人类一样检测高严重程度报告。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PURPOSE: Incident reports are an important tool for safety and qualityimprovement in healthcare, but manual review is time-consuming and requiressubject matter expertise. Here we present a natural language processing (NLP)screening tool to detect high-severity incident reports in radiation oncologyacross two institutions.  METHODS AND MATERIALS: We used two text datasets to train and evaluate ourNLP models: 7,094 reports from our institution (Inst.), and 571 from IAEASAFRON (SF), all of which had severity scores labeled by clinical contentexperts. We trained and evaluated two types of models: baseline support vectormachines (SVM) and BlueBERT which is a large language model pretrained onPubMed abstracts and hospitalized patient data. We assessed forgeneralizability of our model in two ways. First, we evaluated models trainedusing Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model thatwas first fine-tuned on Inst.-train then on SF-train before testing on SF-testset. To further analyze model performance, we also examined a subset of 59reports from our Inst. dataset, which were manually edited for clarity.  RESULTS Classification performance on the Inst. test achieved AUROC 0.82using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning,performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets,improved the performance on SF test to AUROC 0.78. Performance of SVM, andBlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and0.74) was similar to human performance (AUROC 0.81).  CONCLUSION: In summary, we successfully developed cross-institution NLPmodels on incident report text from radiation oncology centers. These modelswere able to detect high-severity reports similarly to humans on a curateddataset.</description>
      <author>example@mail.com (Peter Beidler, Mark Nguyen, Kevin Lybarger, Ola Holmberg, Eric Ford, John Kang)</author>
      <guid isPermaLink="false">2509.13706v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning</title>
      <link>http://arxiv.org/abs/2509.13624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Camera-ready version. Accepted to appear in the proceedings of the  14th Joint Conference on Lexical and Computational Semantics (*SEM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个分析框架，通过构建迁移学习矩阵和降维技术，研究了大型语言模型在跨任务迁移学习中的交互作用，揭示了影响模型性能的关键因素。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛应用于各种任务，但这些任务通常是模型在训练过程中未遇到的。为所有任务枚举和获取高质量训练数据是不可行的，因此需要依赖迁移学习，使用具有不同特性的数据集，并处理分布外请求。&lt;h4&gt;目的&lt;/h4&gt;提出一个分析框架，用于研究跨任务交互，理解迁移学习的效果和副作用。&lt;h4&gt;方法&lt;/h4&gt;构建迁移学习矩阵，使用降维技术，训练并分析10个模型，识别潜在能力（如推理、情感分类、自然语言理解、算术等），发现迁移学习的副作用。&lt;h4&gt;主要发现&lt;/h4&gt;性能提升通常无法基于表面层面的数据集相似性或源数据质量来解释，源数据集的隐藏统计因素（如类别分布和生成长度倾向）更有影响力，特定的语言特征也起着重要作用。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了对迁移学习复杂动态的见解，为更可预测和有效的LLM适应铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型正被越来越多地部署到各种应用中。这通常包括模型在训练过程中未遇到的任务。这意味着枚举和获取所有任务的高质量训练数据是不可行的。因此，我们通常需要依赖使用具有不同特性的数据集进行迁移学习，并预期分布外的请求。受这一实际需求的启发，我们提出了一个分析框架，通过构建迁移学习矩阵和降维技术，来剖析这些跨任务交互。我们训练并分析了10个模型，以识别潜在能力（如推理、情感分类、自然语言理解、算术），并发现了迁移学习的副作用。我们的研究揭示，性能提升通常无法基于表面层面的数据集相似性或源数据质量来解释。相反，源数据集的隐藏统计因素，如类别分布和生成长度倾向，以及特定的语言特征，实际上更有影响力。这项工作为迁移学习的复杂动态提供了见解，为更可预测和有效的LLM适应铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models are increasingly deployed across diverse applications.This often includes tasks LLMs have not encountered during training. Thisimplies that enumerating and obtaining the high-quality training data for alltasks is infeasible. Thus, we often need to rely on transfer learning usingdatasets with different characteristics, and anticipate out-of-distributionrequests. Motivated by this practical need, we propose an analysis framework,building a transfer learning matrix and dimensionality reduction, to dissectthese cross-task interactions. We train and analyze 10 models to identifylatent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic)and discover the side effects of the transfer learning. Our findings revealthat performance improvements often defy explanations based on surface-leveldataset similarity or source data quality. Instead, hidden statistical factorsof the source dataset, such as class distribution and generation lengthproclivities, alongside specific linguistic features, are actually moreinfluential. This work offers insights into the complex dynamics of transferlearning, paving the way for more predictable and effective LLM adaptation.</description>
      <author>example@mail.com (Shambhavi Krishna, Atharva Naik, Chaitali Agarwal, Sudharshan Govindan, Taesung Lee, Haw-Shiuan Chang)</author>
      <guid isPermaLink="false">2509.13624v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Speaker-Independent Dysarthric Speech Severity Classification with DSSCNet and Cross-Corpus Adaptation</title>
      <link>http://arxiv.org/abs/2509.13442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Speaker-independent experiments on classification of dysarthric  speech severity&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DSSCNet，一种新型深度神经网络架构，用于构音障碍言语的严重程度分类。该模型结合了卷积、挤压激励(SE)和残差网络，能够从梅尔频谱图中提取判别性表示。研究还提出了跨语料微调框架，并在两个基准语料库上进行了评估，结果显示该方法优于现有最先进技术。&lt;h4&gt;背景&lt;/h4&gt;构音障碍言语严重程度分类对于运动性言语障碍患者的客观临床评估和进展监测至关重要。尽管先前方法已解决此任务，但在说话人独立(SID)场景中实现强大泛化能力仍具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效对构音障碍言语严重程度进行分类的方法，特别是在说话人独立场景中实现更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1) 提出DSSCNet架构，结合卷积、Squeeze-Excitation和残差网络；2) 使用SE块选择性关注重要特征；3) 提出基于检测的迁移学习方法改编的跨语料微调框架；4) 在TORGO和UA-Speech语料库上评估；5) 使用OSPS和LOSO评估协议。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在OSPS协议下，DSSCNet在TORGO和UA-Speech上分别达到56.84%和62.62%的准确率；2) 在LOSO设置下，分别达到63.47%和64.18%的准确率；3) 微调后性能显著提高：OSPS下分别达到75.80%和68.25%，LOSO下分别达到77.76%和79.44%；4) 所有结果均优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了DSSCNet在跨不同构音障碍言语数据集进行细粒度严重程度分类方面的有效性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;构音障碍言语严重程度分类对于运动性言语障碍患者的客观临床评估和进展监测至关重要。尽管先前的方法已经解决了这一任务，但在说话人独立(SID)场景中实现强大的泛化能力仍然具有挑战性。这项工作介绍了DSSCNet，一种新型深度神经网络架构，它结合了卷积、挤压激励(SE)和残差网络，帮助它从梅尔频谱图中提取构音障碍言语的判别性表示。SE块的添加选择性关注构音障碍言语的重要特征，从而减少损失并提高整体模型性能。我们还提出了一个用于严重程度分类的跨语料微调框架，该框架基于检测的迁移学习方法改编而成。DSSCNet在两个基准构音障碍言语语料库(TORGO和UA-Speech)上进行了评估，使用说话人独立评估协议：每严重程度一位说话人(OSPS)和留一说话人法(LOSO)。在OSPS和LOSO设置下，DSSCNet在TORGO上分别达到56.84%和63.47%的准确率，在UA-Speech上分别达到62.62%和64.18%的准确率，优于现有的最先进方法。微调后，性能显著提高，在OSPS下，DSSCNet在TORGO和UA-Speech上分别达到75.80%和68.25%的准确率，在LOSO下分别达到77.76%和79.44%。这些结果证明了DSSCNet在跨不同构音障碍言语数据集进行细粒度严重程度分类方面的有效性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dysarthric speech severity classification is crucial for objective clinicalassessment and progress monitoring in individuals with motor speech disorders.Although prior methods have addressed this task, achieving robustgeneralization in speaker-independent (SID) scenarios remains challenging. Thiswork introduces DSSCNet, a novel deep neural architecture that combinesConvolutional, Squeeze-Excitation (SE), and Residual network, helping itextract discriminative representations of dysarthric speech from melspectrograms. The addition of SE block selectively focuses on the importantfeatures of the dysarthric speech, thereby minimizing loss and enhancingoverall model performance. We also propose a cross-corpus fine-tuning frameworkfor severity classification, adapted from detection-based transfer learningapproaches. DSSCNet is evaluated on two benchmark dysarthric speech corpora:TORGO and UA-Speech under speaker-independent evaluation protocols:One-Speaker-Per-Severity (OSPS) and Leave-One-Speaker-Out (LOSO) protocols.DSSCNet achieves accuracies of 56.84% and 62.62% under OSPS and 63.47% and64.18% under LOSO setting on TORGO and UA-Speech respectively outperformingexisting state-of-the-art methods. Upon fine-tuning, the performance improvessubstantially, with DSSCNet achieving up to 75.80% accuracy on TORGO and 68.25%on UA-Speech in OSPS, and up to 77.76% and 79.44%, respectively, in LOSO. Theseresults demonstrate the effectiveness and generalizability of DSSCNet forfine-grained severity classification across diverse dysarthric speech datasets.</description>
      <author>example@mail.com (Arnab Kumar Roy, Hemant Kumar Kathania, Paban Sapkota)</author>
      <guid isPermaLink="false">2509.13442v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HAM是一种新的持续学习框架，通过动态合并不同任务的适配器来解决灾难性遗忘问题，在处理大量任务时表现优异。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的重要能力，但对当前深度学习模型构成挑战。主要问题是新知识会干扰已学习信息，导致灾难性遗忘。大型预训练模型部分缓解此问题，但在面对新数据分布时表现不佳。参数高效微调方法如LoRA能有效适应新知识，但在扩展到动态学习场景和长任务序列时仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效扩展到更多任务的方法，提高持续学习效率，减少每个任务使用一个适配器带来的复杂性和干扰可能性。&lt;h4&gt;方法&lt;/h4&gt;提出分层适配器合并（HAM）框架，在训练期间动态组合来自不同任务的适配器。HAM维护一组固定的组，分层次整合新适配器。对于每个任务，训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务间的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;HAM能够有效扩展，管理比竞争基线更多的任务，同时提高效率。在三个视觉基准上的广泛实验表明，HAM显著优于最先进的方法，随着任务数量的增加，其优势尤其明显。&lt;h4&gt;结论&lt;/h4&gt;HAM框架为持续学习提供了一种有效解决方案，特别是在处理大量任务时。通过动态合并适配器，HAM能够更好地管理任务间的知识迁移，减少灾难性遗忘。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型提出了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型为了新知识而遗忘早期知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘问题，但当面对新的数据分布时，它们往往表现不佳。参数高效微调（PEFT）方法，如LoRA，能够有效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍然面临挑战，因为为每个任务维护一个适配器会引入复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并（HAM），这是一个新颖的框架，在训练期间动态组合来自不同任务的适配器。这种方法使HAM能够有效扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定的组，这些组分层次地整合新适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态分组任务。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准上的广泛实验表明，HAM显著优于最先进的方法，特别是随着任务数量的增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v2</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>A Domain Knowledge Informed Approach for Anomaly Detection of Electric Vehicle Interior Sounds</title>
      <link>http://arxiv.org/abs/2509.13390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to: Mechanical Systems and Signal Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于领域知识的模型选择方法，通过使用代理异常来改进汽车舱内声音异常检测模型的性能评估。&lt;h4&gt;背景&lt;/h4&gt;在汽车舱内声音异常检测中，由于标记的故障数据稀缺或完全缺失，这项任务更适合作为无监督学习问题而非监督学习问题。然而，在无监督设置中，由于缺乏标记的故障样本进行验证，以及常用指标的可靠性有限，有效的模型选择仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为克服无监督学习环境下模型选择的挑战，提出一种基于领域知识的模型选择方法，通过使用代理异常来支持模型选择。&lt;h4&gt;方法&lt;/h4&gt;通过健康频谱图的结构化扰动构建代理异常，将其用于验证集以支持模型选择。在一个包含健康和故障舱内声音的高保真电动汽车数据集上评估该方法，该数据集涵盖了五种代表性故障类型：不平衡、调制、啸叫、风声和脉冲宽度调制。&lt;h4&gt;主要发现&lt;/h4&gt;在五种故障案例上的实验评估表明，使用代理异常选择最优模型显著优于传统的模型选择策略。&lt;h4&gt;结论&lt;/h4&gt;基于领域知识的模型选择方法，使用代理异常，能够有效解决无监督学习环境下汽车舱内声音异常检测中的模型选择挑战，显著提高了模型选择的性能。&lt;h4&gt;翻译&lt;/h4&gt;汽车舱内声音异常的检测对于确保车辆质量和维护乘客舒适度至关重要。在许多现实场景中，由于标记的故障数据稀缺或完全缺失，这项任务更适合作为无监督学习问题而非监督学习案例。在这种无监督设置中，模型仅在健康样本上训练，并将异常检测为正常行为的偏差。然而，由于缺乏标记的故障样本进行验证以及常用指标（如验证重建误差）的可靠性有限，有效的模型选择仍然是一个重大挑战。为克服这些限制，提出了一种基于领域知识的模型选择方法，其中通过健康频谱图的结构化扰动构建的代理异常用于验证集以支持模型选择。该方法在一个包含健康和故障舱内声音的高保真电动汽车数据集上进行了评估，该数据集涵盖了五种代表性故障类型：不平衡、调制、啸叫、风声和脉冲宽度调制。这个数据集使用先进的声音合成技术生成，并通过专家评审评估验证，已公开以促进进一步研究。在五种故障案例上的实验评估表明，使用代理异常选择最优模型显著优于传统的模型选择策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in automotive cabin sounds is critical forensuring vehicle quality and maintaining passenger comfort. In many real-worldsettings, this task is more appropriately framed as an unsupervised learningproblem rather than the supervised case due to the scarcity or complete absenceof labeled faulty data. In such an unsupervised setting, the model is trainedexclusively on healthy samples and detects anomalies as deviations from normalbehavior. However, in the absence of labeled faulty samples for validation andthe limited reliability of commonly used metrics, such as validationreconstruction error, effective model selection remains a significantchallenge. To overcome these limitations, a domain-knowledge-informed approachfor model selection is proposed, in which proxy-anomalies engineered throughstructured perturbations of healthy spectrograms are used in the validation setto support model selection. The proposed methodology is evaluated on ahigh-fidelity electric vehicle dataset comprising healthy and faulty cabinsounds across five representative fault types viz., Imbalance, Modulation,Whine, Wind, and Pulse Width Modulation. This dataset, generated using advancedsound synthesis techniques, and validated via expert jury assessments, has beenmade publicly available to facilitate further research. Experimentalevaluations on the five fault cases demonstrate the selection of optimal modelsusing proxy-anomalies, significantly outperform conventional model selectionstrategies.</description>
      <author>example@mail.com (Deepti Kunte, Bram Cornelis, Claudio Colangeli, Karl Janssens, Brecht Van Baelen, Konstantinos Gryllias)</author>
      <guid isPermaLink="false">2509.13390v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>White Aggregation and Restoration for Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.13907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为White Aggregation and Restoration Module (WARM)的新型原型生成方法，用于解决少样本3D点云分割问题。该方法通过白化和彩色化变换之间的交叉注意力机制，解决了支持特征与原型令牌之间的分布不对齐问题，从而生成更具代表性的原型，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;少样本3D点云分割(FS-PCS)旨在仅有少量标记样本的情况下预测未标记点云的每个点标签。现有方法使用传统算法构建原型，但初始随机性显著影响性能，且原型生成过程缺乏深入探索。&lt;h4&gt;目的&lt;/h4&gt;调查一种基于注意力机制的先进原型生成方法，以提高FS-PCS性能。&lt;h4&gt;方法&lt;/h4&gt;提出White Aggregation and Restoration Module (WARM)，通过在白化和彩色化变换之间夹入交叉注意力来解决分布不对齐问题。白化在注意力过程前将支持特征与原型令牌对齐，彩色化随后恢复原始分布到已处理令牌，使注意力更加鲁棒，捕获支持特征间的语义关系生成代表性原型。&lt;h4&gt;主要发现&lt;/h4&gt;现有简单模块在可学习的原型令牌和支持特征之间存在分布差距；WARM方法能有效解决这种分布不对齐问题，生成更具代表性的原型。&lt;h4&gt;结论&lt;/h4&gt;在多个FS-PCS基准测试上，该方法以显著优势实现了最先进的性能，通过大量实验证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;少样本3D点云分割(FS-PCS)旨在仅有少量标记样本的情况下预测未标记点云的每个点标签。为从有限的支持集中提取判别性表示，现有方法使用最远点采样等传统算法构建原型。然而，我们指出其初始随机性显著影响FS-PCS性能，且尽管原型生成过程被广泛使用，它仍然缺乏深入探索。这促使我们调查一种基于注意力机制的先进原型生成方法。尽管有潜力，我们发现简单模块在可学习的原型令牌和支持特征之间存在分布差距。为克服这一问题，我们提出了白化聚合与恢复模块(WARM)，通过在白化和彩色化变换之间夹入交叉注意力来解决不对齐问题。具体而言，白化在注意力过程前将支持特征与原型令牌对齐，随后彩色化将原始分布恢复到已处理令牌。这种简单而有效的设计使注意力更加鲁棒，从而通过捕获支持特征之间的语义关系生成代表性原型。我们的方法在多个FS-PCS基准测试上以显著优势实现了最先进的性能，通过大量实验证明了其有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决少样本3D点云语义分割(FS-PCS)中的原型生成问题。传统方法使用最远点采样(FPS)等算法构建原型，但这些方法存在初始随机性大、性能不稳定的问题。这个问题很重要，因为3D点云语义分割在自动驾驶、机器人等领域有广泛应用，而标记3D点云数据需要大量人工劳动，成本高昂。少样本学习可以减少对大量标记数据的依赖，原型生成作为FS-PCS的关键步骤，其质量直接影响分割性能，现有方法的不稳定性限制了FS-PCS的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先指出传统FPS方法的局限性，特别是其初始随机性导致的性能不稳定。然后考虑使用注意力机制作为替代方案，因为注意力在其他少样本下游任务中表现出色。研究发现vanilla交叉注意力存在分布差距问题：可学习的原型令牌与支持特征之间分布不匹配。通过分析特征空间的分布差异，作者设计了白化和着色变换来解决这一问题。该方法借鉴了图像领域DETR和Mask2Former的注意力机制，以及标准的ZCA白化技术，并受到原型网络的启发，但将这些技术创新性地应用于3D点云FS-PCS场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'白化-交叉注意力-着色'三阶段流程解决原型令牌与支持特征之间的分布不匹配问题。白化阶段暂时移除支持特征的分布统计信息，使其与原型令牌对齐；交叉注意力阶段在共享空间中，原型令牌能够基于语义关系而非空间距离来聚合特征；着色阶段恢复原始分布统计信息，保留支持特征的固有特性。整体流程包括：使用主干网络提取特征；将支持特征分为前景和背景；对每类应用ZCA白化；使用白化后的特征通过交叉注意力生成临时原型；应用着色变换恢复原始分布；最后将查询点分配给最近原型进行分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出WARM模块解决FS-PCS中原型生成问题；2) 首次将白化和着色变换引入注意力机制，解决分布不匹配；3) 通过白化移除特征通道间相关性，使优化更稳定；4) 通过着色恢复原始特征特性。相比之前的工作：不同于FPS方法的随机性和不稳定性，WARM是确定性的；不同于简单注意力无法处理分布不匹配问题，WARM通过白化-着色变换解决了这一问题；不同于其他FS-PCS方法主要关注查询适应，WARM专注于改进原型生成本身；不同于3D领域通常使用非可学习令牌，WARM实现了可学习令牌的应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了WARM模块，通过白化和着色变换解决了少样本3D点云语义分割中原型生成的不稳定性和分布不匹配问题，显著提升了分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-Shot 3D Point Cloud Segmentation (FS-PCS) aims to predict per-pointlabels for an unlabeled point cloud, given only a few labeled examples. Toextract discriminative representations from the limited support set, existingmethods have constructed prototypes using conventional algorithms such asfarthest point sampling. However, we point out that its initial randomnesssignificantly affects FS-PCS performance and that the prototype generationprocess remains underexplored despite its prevalence. This motivates us toinvestigate an advanced prototype generation method based on attentionmechanism. Despite its potential, we found that vanilla module suffers from thedistributional gap between learnable prototypical tokens and support features.To overcome this, we propose White Aggregation and Restoration Module (WARM),which resolves the misalignment by sandwiching cross-attention betweenwhitening and coloring transformations. Specifically, whitening aligns thesupport features to prototypical tokens before attention process, andsubsequently coloring restores the original distribution to the attendedtokens. This simple yet effective design enables robust attention, therebygenerating representative prototypes by capturing the semantic relationshipsamong support features. Our method achieves state-of-the-art performance with asignificant margin on multiple FS-PCS benchmarks, demonstrating itseffectiveness through extensive experiments.</description>
      <author>example@mail.com (Jiyun Im, SuBeen Lee, Miso Lee, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2509.13907v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Algorithm of the GLMY Homology on Digraphs</title>
      <link>http://arxiv.org/abs/2509.13862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对GLMY同调的量子算法，在拓扑数据分析领域比最佳经典算法具有显著优势，在一般情况下提供二次加速，在特定情况下提供指数级加速。&lt;h4&gt;背景&lt;/h4&gt;量子算法在拓扑数据分析中比最佳经典算法有明显优势。GLMY同调是由Alexander Grigor'yan等人引入的，不同于传统的点云上的单纯复形，它是在有向图上定义的拓扑数据分析新兴领域，最近受到越来越多的关注。&lt;h4&gt;目的&lt;/h4&gt;提出一个针对GLMY同调的量子算法，使其比最佳经典算法具有显著优势。&lt;h4&gt;方法&lt;/h4&gt;设计了一个通用编码协议，用于GLMY同调的量子态和边界算子；同时证明了GLMY同调的一个性质，为量子算法提供了理论保证。&lt;h4&gt;主要发现&lt;/h4&gt;GLMY同调的量子算法在一般情况下提供了二次加速，在输入数据以路径规范形式给出时提供了指数级的量子优势。&lt;h4&gt;结论&lt;/h4&gt;量子算法在GLMY同调计算中具有显著优势，为拓扑数据分析领域提供了新的计算方法。&lt;h4&gt;翻译&lt;/h4&gt;拓扑数据分析的量子算法比最佳经典算法具有显著优势。由Alexander Grigor'yan、Yong Lin、Yuri Muranov和Shing-Tung Yau引入的GLMY同调，与之前的点云上的单纯复形不同，它是在有向图上定义的，是拓扑数据分析中的一个新兴领域，最近吸引了越来越多的关注。我们提出了一个针对GLMY同调的量子算法，比最佳经典算法有显著优势。我们设计了GLMY同调在有向图上的量子态和边界算子的通用编码协议。并且证明了GLMY同调的一个性质，为量子算法提供了理论保证。GLMY同调的量子算法在一般情况下给出了二次加速，在输入数据以路径规范形式给出时给出了指数级的量子优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum algorithms for topological data analysis provide significantadvantage over the best classical algorithm. Different from the previoussimplical complex on points cloud, the GLMY homology introduced by AlexanderGrigor'yan, Yong Lin, Yuri Muranov and Shing-Tung Yau, is defined on digraphand is a arising realm in Topological Data Analysis (TDA), which attracts moreand more attention recently. We propose a quantum algorithm for the GLMYhomology with significant advantage over the best classical algorithm. Wedesign a universal encoding protocol for the quantum states and boundaryoperators of GLMY homology on digraphs. And a property of the GLMY homology isproved for the theoretical guarantee of the quantum algorithm. The quantumalgorithm for GLMY homology gives a quadratic speedup in general cases, and itgives an exponential quantum advantage in the case of the input data is givenas a specification of paths.</description>
      <author>example@mail.com (Yunpeng Zi, Muchun Yang, D. L. Zhou)</author>
      <guid isPermaLink="false">2509.13862v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>InterKey: Cross-modal Intersection Keypoints for Global Localization on OpenStreetMap</title>
      <link>http://arxiv.org/abs/2509.13857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出InterKey，一种利用道路交叉路口作为独特地标的跨模态框架，实现自动驾驶车辆在GNSS受限环境中的全球定位。&lt;h4&gt;背景&lt;/h4&gt;可靠的全球定位对自动驾驶车辆至关重要，特别是在GNSS信号减弱或不可用的环境中。高清地图提供准确信息但成本高，限制了可扩展性；OpenStreetMap免费可用但抽象粗糙，与传感器数据匹配困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用道路交叉路口作为独特地标的全球定位方法，解决高清地图成本高和OSM抽象粗糙的问题。&lt;h4&gt;方法&lt;/h4&gt;提出InterKey跨模态框架，通过联合编码点云和OSM的道路和建筑物印记构建紧凑二进制描述符，并引入差异缓解、方向确定和面积均衡采样策略实现鲁棒跨模态匹配。&lt;h4&gt;主要发现&lt;/h4&gt;KITTI数据集实验表明，InterKey实现最先进准确性，大幅优于最近基线方法，框架可扩展至能生成密集结构点云的传感器。&lt;h4&gt;结论&lt;/h4&gt;InterKey为车辆定位提供了一种可扩展、经济高效的鲁棒解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可靠的全球定位对自动驾驶车辆至关重要，特别是在GNSS信号减弱或不可用的环境中，如城市峡谷和隧道。虽然高清地图提供准确的先验信息，但数据收集、地图构建和维护的成本限制了其可扩展性。OpenStreetMap提供了免费且全球可用的替代方案，但其粗略的抽象表示给与传感器数据的匹配带来挑战。我们提出InterKey，一种利用道路交叉路口作为独特地标的跨模态框架。我们的方法通过联合编码来自点云和OSM的道路和建筑物印记来构建紧凑的二进制描述符。为了弥合模态差距，我们引入了差异缓解、方向确定和面积均衡采样策略，实现了鲁棒的跨模态匹配。KITTI数据集上的实验表明，InterKey实现了最先进的准确性，以较大优势优于最近的基线方法。该框架可以扩展到能够生成密集结构点云的传感器，为车辆定位提供了一种可扩展且经济高效的鲁棒解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在GNSS信号弱或不可用环境（如城市峡谷、隧道）中，自动驾驶汽车的全局定位问题。这个问题很重要，因为GNSS在这些环境中容易受到多路径效应和信号遮挡的影响导致定位不可靠，而高精度地图虽准确但成本高昂限制了可扩展性。准确的全局定位对自动驾驶汽车的初始化、故障恢复、路径规划等关键任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到GNSS在复杂环境中的局限性和高精度地图的成本问题，选择道路交叉口作为关键特征，因其具有独特几何配置且可从LiDAR数据可靠提取。他们结合道路和建筑物信息提高描述符区分度，并通过差异缓解、方向确定和面积等采样策略解决模态差距。该方法借鉴了作者之前关于交叉口检测的研究，参考了Scan Context等点云识别方法，以及OSM Context等点云到OSM的匹配方法，但创新性地专注于交叉口特征的跨模态匹配。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用道路交叉口作为点云和OSM之间的跨模态关键点，联合编码道路和建筑物印记生成紧凑二进制描述符，并通过专门策略解决模态差距。整体流程分为四步：1)从OSM提取交叉口节点并生成道路和建筑物印记；2)处理点云数据生成俯视图道路和建筑物印记并检测交叉口；3)通过差异缓解、方向确定和形状编码生成二进制描述符；4)匹配描述符检索最佳OSM交叉口并计算车辆全局姿态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首个专注于跨模态交叉口匹配的框架、结合道路和建筑物的描述符、模态差异缓解方案、方向确定策略和面积等采样方法。相比之前工作，不同之处在于：结合道路和建筑物信息而非仅依赖建筑物轮廓，提高区分度；采用面积等采样模式而非线性增长，能均匀捕捉周围形状；专注于交叉口这一独特特征减少搜索空间；专门设计解决点云和OSM间的模态差距，而非仅处理点云到点云匹配。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InterKey通过利用道路交叉口作为跨模态关键点，结合道路和建筑物信息生成紧凑描述符，并采用创新的采样策略，实现了在GNSS受限环境中基于免费OpenStreetMap的高效、准确的全车定位。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable global localization is critical for autonomous vehicles, especiallyin environments where GNSS is degraded or unavailable, such as urban canyonsand tunnels. Although high-definition (HD) maps provide accurate priors, thecost of data collection, map construction, and maintenance limits scalability.OpenStreetMap (OSM) offers a free and globally available alternative, but itscoarse abstraction poses challenges for matching with sensor data. We proposeInterKey, a cross-modal framework that leverages road intersections asdistinctive landmarks for global localization. Our method constructs compactbinary descriptors by jointly encoding road and building imprints from pointclouds and OSM. To bridge modality gaps, we introduce discrepancy mitigation,orientation determination, and area-equalized sampling strategies, enablingrobust cross-modal matching. Experiments on the KITTI dataset demonstrate thatInterKey achieves state-of-the-art accuracy, outperforming recent baselines bya large margin. The framework generalizes to sensors that can produce densestructural point clouds, offering a scalable and cost-effective solution forrobust vehicle localization.</description>
      <author>example@mail.com (Nguyen Hoang Khoi Tran, Julie Stephany Berrio, Mao Shan, Stewart Worrall)</author>
      <guid isPermaLink="false">2509.13857v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HGACNet: Hierarchical Graph Attention Network for Cross-Modal Point Cloud Completion</title>
      <link>http://arxiv.org/abs/2509.13692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为HGACNet的新型框架，用于解决点云补全问题，通过分层编码3D几何特征并与RGB图像引导的先验信息融合，重建完整点云。&lt;h4&gt;背景&lt;/h4&gt;点云补全对机器人感知、物体重建及下游任务（如抓取规划、避障和操作）至关重要，但自遮挡和传感器限制导致的不完整几何结构会显著降低下游推理和交互质量。&lt;h4&gt;目的&lt;/h4&gt;解决由自遮挡和传感器限制导致的点云不完整问题，提高点云补全的准确性和适用性。&lt;h4&gt;方法&lt;/h4&gt;提出HGACNet框架，包含分层图注意力(HGA)编码器进行关键点选择和特征细化，多尺度跨模态融合(MSCF)模块实现几何特征与视觉特征的对齐，以及对比损失(C-Loss)对齐跨模态特征分布。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet-ViPC基准和YCB-Complete数据集上的实验证实HGACNet的有效性，展示了最先进的性能和强适用性。&lt;h4&gt;结论&lt;/h4&gt;HGACNet能够有效解决点云补全问题，在多个数据集上表现优异，并在实际机器人操作任务中具有很好的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;点云补全对机器人感知、物体重建和支持抓取规划、避障和操作等下游任务至关重要。然而，由自遮挡和传感器限制引起的不完整几何结构会显著降低下游推理和交互质量。为解决这些挑战，我们提出HGACNet，一种新型框架，通过分层编码3D几何特征并将其与单视图RGB图像引导的先验信息融合，重建单个物体的完整点云。我们方法的核心是分层图注意力(HGA)编码器，它通过基于图注意力的下采样自适应选择关键局部点，并逐步细化分层几何特征，以更好地捕捉结构连续性和空间关系。为加强跨模态交互，我们进一步设计了多尺度跨模态融合(MSCF)模块，在分层几何特征和结构化视觉表示之间进行基于注意力的特征对齐，实现补全的细粒度语义指导。此外，我们提出了对比损失(C-Loss)，明确对齐跨模态特征分布，提高模态差异下的补全保真度。最后，在ShapeNet-ViPC基准和YCB-Complete数据集上进行的广泛实验证实了HGACNet的有效性，展示了最先进的性能以及在现实世界机器人操作任务中的强适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云补全问题，即从部分点云和对应RGB图像重建完整的3D点云。这个问题在现实中非常重要，因为实际获取的点云数据常因传感器限制和物体自遮挡而不完整，这会严重影响机器人抓取、避障、物体识别等下游任务的可靠性。准确的点云补全对机器人感知、物体重建和交互操作至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：单模态方法难以处理遮挡和细粒度结构重建，而跨模态方法未能充分利用模态间关系。作者借鉴了图注意力机制、Transformer架构和对比学习等现有技术，但创新性地设计了分层图注意力编码器（HGA）和多尺度跨模态融合模块（MSCF），通过分层特征提取和跨模态注意力对齐，实现了更有效的点云补全。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分层编码3D几何特征并融合图像引导的先验知识，实现高质量点云补全。整体流程为：1) 使用HGA编码器从部分点云提取全局和局部特征；2) 用Swin Transformer从RGB图像提取视觉特征；3) 通过MSCF模块融合几何和视觉特征；4) 应用对比损失(C-Loss)对齐不同模态特征；5) 将融合特征输入解码器重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 分层图注意力编码器(HGA)，自适应选择关键点并保留结构细节；2) 多尺度跨模态融合模块(MSCF)，实现几何与视觉特征的实例级对齐；3) 对比损失函数(C-Loss)，减少模态差异提高重建准确性。相比之前工作，HGACNet不是通过简单连接进行早期融合，而是通过分层注意力实现深度特征交互；在不同几何抽象层次上选择性应用注意力，平衡计算效率和重建质量；同时关注全局结构和细粒度细节，并引入对比学习明确对齐不同模态特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HGACNet通过分层图注意力编码和多尺度跨模态融合，有效结合点云几何特征与图像语义先验，实现了高质量、结构一致的三维点云补全，在机器人感知任务中取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is essential for robotic perception, objectreconstruction and supporting downstream tasks like grasp planning, obstacleavoidance, and manipulation. However, incomplete geometry caused byself-occlusion and sensor limitations can significantly degrade downstreamreasoning and interaction. To address these challenges, we propose HGACNet, anovel framework that reconstructs complete point clouds of individual objectsby hierarchically encoding 3D geometric features and fusing them withimage-guided priors from a single-view RGB image. At the core of our approach,the Hierarchical Graph Attention (HGA) encoder adaptively selects criticallocal points through graph attention-based downsampling and progressivelyrefines hierarchical geometric features to better capture structural continuityand spatial relationships. To strengthen cross-modal interaction, we furtherdesign a Multi-Scale Cross-Modal Fusion (MSCF) module that performsattention-based feature alignment between hierarchical geometric features andstructured visual representations, enabling fine-grained semantic guidance forcompletion. In addition, we proposed the contrastive loss (C-Loss) toexplicitly align the feature distributions across modalities, improvingcompletion fidelity under modality discrepancy. Finally, extensive experimentsconducted on both the ShapeNet-ViPC benchmark and the YCB-Complete datasetconfirm the effectiveness of HGACNet, demonstrating state-of-the-artperformance as well as strong applicability in real-world robotic manipulationtasks.</description>
      <author>example@mail.com (Yadan Zeng, Jiadong Zhou, Xiaohan Li, I-Ming Chen)</author>
      <guid isPermaLink="false">2509.13692v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>RF-LSCM: Pushing Radiance Fields to Multi-Domain Localized Statistical Channel Modeling for Cellular Network Optimization</title>
      <link>http://arxiv.org/abs/2509.13686v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RF-LSCM的新型无线信道建模框架，通过辐射场联合表示大尺度信号衰减和多径分量，解决了传统LSCM方法的局限性，实现了多小区、多网格和多频率信道建模，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;精确的局部无线信道建模是蜂窝网络优化的基石，局部统计信道建模(LSCM)是当前最先进的信道建模框架。然而，传统LSCM方法仅限于单小区、单网格和单载波频率分析，无法捕捉复杂的跨域交互。&lt;h4&gt;目的&lt;/h4&gt;克服传统LSCM方法的局限性，开发一个能够捕捉复杂跨域交互的新型信道建模框架，实现多小区、多网格和多频率信道建模。&lt;h4&gt;方法&lt;/h4&gt;提出RF-LSCM框架，通过辐射场联合表示大尺度信号衰减和多径分量；引入多域LSCM公式和频率依赖衰减模型(FDAM)促进跨频率泛化；使用点云辅助的环境增强方法实现多小区和多网格信道建模；利用低秩张量表示和分层张量角度建模(HiTAM)算法提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;RF-LSCM在真实世界多小区数据集上显著优于最先进方法，覆盖预测的平均绝对误差减少高达30%，通过有效融合多频率数据，MAE提高了22%。&lt;h4&gt;结论&lt;/h4&gt;RF-LSCM是一种高效的信道建模框架，能够处理多小区、多网格和多频率数据，在保持细粒度准确性的同时，显著降低了GPU内存需求和训练时间。&lt;h4&gt;翻译&lt;/h4&gt;精确的局部无线信道建模是蜂窝网络优化的基石，能够在参数调整过程中可靠预测网络性能。局部统计信道建模(LSCM)是专为蜂窝网络优化定制的最先进信道建模框架。然而，传统LSCM方法从参考信号接收功率(RSRP)测量中推断信道的角度功率谱(APS)，存在关键局限性：它们通常仅限于单小区、单网格和单载波频率分析，无法捕捉复杂的跨域交互。为克服这些挑战，我们提出了RF-LSCM，一种新框架，通过辐射场联合表示大尺度信号衰减和多径分量来建模信道APS。RF-LSCM引入了多域LSCM公式，采用物理信息相关的频率依赖衰减模型(FDAM)促进跨频率泛化，以及点云辅助的环境增强方法实现多小区和多网格信道建模。此外，为解决典型神经辐射场的计算效率低下问题，RF-LSCM利用低秩张量表示，并辅以新型的分层张量角度建模(HiTAM)算法。这种高效设计显著降低了GPU内存需求和训练时间，同时保持了细粒度准确性。在真实世界多小区数据集上的广泛实验表明，RF-LSCM显著优于最先进的方法，在覆盖预测中平均绝对误差(MAE)减少高达30%，并通过有效融合多频率数据，MAE提高了22%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localized wireless channel modeling is a cornerstone of cellularnetwork optimization, enabling reliable prediction of network performanceduring parameter tuning. Localized statistical channel modeling (LSCM) is thestate-of-the-art channel modeling framework tailored for cellular networkoptimization. However, traditional LSCM methods, which infer the channel'sAngular Power Spectrum (APS) from Reference Signal Received Power (RSRP)measurements, suffer from critical limitations: they are typically confined tosingle-cell, single-grid and single-carrier frequency analysis and fail tocapture complex cross-domain interactions. To overcome these challenges, wepropose RF-LSCM, a novel framework that models the channel APS by jointlyrepresenting large-scale signal attenuation and multipath components within aradiance field. RF-LSCM introduces a multi-domain LSCM formulation with aphysics-informed frequency-dependent Attenuation Model (FDAM) to facilitate thecross frequency generalization as well as a point-cloud-aided environmentenhanced method to enable multi-cell and multi-grid channel modeling.Furthermore, to address the computational inefficiency of typical neuralradiance fields, RF-LSCM leverages a low-rank tensor representation,complemented by a novel Hierarchical Tensor Angular Modeling (HiTAM) algorithm.This efficient design significantly reduces GPU memory requirements andtraining time while preserving fine-grained accuracy. Extensive experiments onreal-world multi-cell datasets demonstrate that RF-LSCM significantlyoutperforms state-of-the-art methods, achieving up to a 30% reduction in meanabsolute error (MAE) for coverage prediction and a 22% MAE improvement byeffectively fusing multi-frequency data.</description>
      <author>example@mail.com (Bingsheng Peng, Shutao Zhang, Xi Zheng, Ye Xue, Xinyu Qin, Tsung-Hui Chang)</author>
      <guid isPermaLink="false">2509.13686v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Deep Lookup Network</title>
      <link>http://arxiv.org/abs/2509.13662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的查找表操作替代卷积神经网络中的乘法操作，以提高计算效率和降低能耗，同时保持与标准卷积网络相当的性能。作者构建了可微分的查找表并提出了训练策略，在多种任务上验证了查找网络的高效性。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络由大量不同类型的操作构成，计算密集度高。其中乘法操作计算复杂度高，通常比其他操作消耗更多能量且需要更长的推理时间，阻碍了卷积神经网络在移动设备上的部署。在资源受限的边缘设备上，可通过查找表计算复杂操作以降低计算成本。&lt;h4&gt;目的&lt;/h4&gt;引入一种通用的、高效的查找操作作为构建神经网络的基本操作，用简单的查找操作替代权重与激活值之间的乘法计算，提高神经网络的效率。&lt;h4&gt;方法&lt;/h4&gt;构建可微分的查找表，提出几种训练策略促进收敛，用查找操作替代计算密集的乘法操作，开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。&lt;h4&gt;主要发现&lt;/h4&gt;查找网络能够在能耗和推理速度方面实现更高的效率，同时保持与标准卷积网络相竞争的性能。在不同任务（分类和回归）和数据类型（图像和点云）上都取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过用查找操作替代乘法操作，成功开发了高效的查找网络，在保持竞争力的同时显著提高了计算效率和降低了能耗，适用于资源受限的边缘设备。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络由大量不同类型的操作构成，计算密集度高。在这些操作中，乘法操作的计算复杂度较高，通常比其他操作消耗更多能量且需要更长的推理时间，这阻碍了卷积神经网络在移动设备上的部署。在许多资源受限的边缘设备上，可以通过查找表来计算复杂操作以降低计算成本。受此启发，在本文中，我们引入了一种通用的、高效的查找操作，可作为构建神经网络的基本操作。我们采用简单而高效的查找操作来计算权重和激活值的响应，而不是直接计算它们的乘积。为了使查找操作能够端到端优化，我们以可微分的方式构建查找表，并提出了几种训练策略来促进其收敛。通过用我们的查找操作替代计算密集的乘法操作，我们开发了用于图像分类、图像超分辨率和点云分类任务的查找网络。实验证明，我们的查找网络能够受益于查找操作，在能耗和推理速度方面实现更高的效率，同时保持与标准卷积网络相竞争的性能。大量实验表明，我们的查找网络在不同任务（分类和回归任务）和数据类型（图像和点云）上都取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional neural networks are constructed with massive operations withdifferent types and are highly computationally intensive. Among theseoperations, multiplication operation is higher in computational complexity andusually requires {more} energy consumption with longer inference time thanother operations, which hinders the deployment of convolutional neural networkson mobile devices. In many resource-limited edge devices, complicatedoperations can be calculated via lookup tables to reduce computational cost.Motivated by this, in this paper, we introduce a generic and efficient lookupoperation which can be used as a basic operation for the construction of neuralnetworks. Instead of calculating the multiplication of weights and activationvalues, simple yet efficient lookup operations are adopted to compute theirresponses. To enable end-to-end optimization of the lookup operation, weconstruct the lookup tables in a differentiable manner and propose severaltraining strategies to promote their convergence. By replacing computationallyexpensive multiplication operations with our lookup operations, we developlookup networks for the image classification, image super-resolution, and pointcloud classification tasks. It is demonstrated that our lookup networks canbenefit from the lookup operations to achieve higher efficiency in terms ofenergy consumption and inference speed while maintaining competitiveperformance to vanilla convolutional networks. Extensive experiments show thatour lookup networks produce state-of-the-art performance on different tasks(both classification and regression tasks) and different data types (bothimages and point clouds).</description>
      <author>example@mail.com (Yulan Guo, Longguang Wang, Wendong Mao, Xiaoyu Dong, Yingqian Wang, Li Liu, Wei An)</author>
      <guid isPermaLink="false">2509.13662v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Object Pose Estimation through Dexterous Touch</title>
      <link>http://arxiv.org/abs/2509.13591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;结合触觉传感和强化学习的方法，能够主动探索物体表面以识别关键位姿特征，无需先验几何知识。&lt;h4&gt;背景&lt;/h4&gt;机器人在抓取和交互任务中需要稳健的目标位姿估计，特别是在视觉数据有限或对光照、遮挡和外观敏感的场景中。触觉传感器通常提供有限和局部的接触信息，这使得从部分数据重建位姿具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，通过主动控制机器人手与物体交互，来稳健地估计物体位姿。&lt;h4&gt;方法&lt;/h4&gt;使用强化学习进行训练以探索和收集触觉数据。收集到的3D点云用于迭代优化物体的形状和位姿。在一个设置中，一只手稳定地握住物体，另一只手执行主动探索。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够主动探索物体表面以识别关键的位姿特征，无需事先了解物体的几何形状。&lt;h4&gt;结论&lt;/h4&gt;通过触觉传感和强化学习训练的主动探索，可以有效地从有限数据中估计物体位姿。&lt;h4&gt;翻译&lt;/h4&gt;稳健的目标位姿估计对于机器人中的抓取和交互任务至关重要，特别是在视觉数据有限或对光照、遮挡和外观敏感的场景中。触觉传感器通常提供有限和局部的接触信息，这使得从部分数据重建位姿具有挑战性。我们的方法使用传感运动探索来主动控制机器人手与物体交互。我们使用强化学习进行训练以探索和收集触觉数据。收集到的3D点云用于迭代优化物体的形状和位姿。在我们的设置中，一只手稳定地握住物体，而另一只手执行主动探索。我们表明，我们的方法能够主动探索物体表面以识别关键的位姿特征，无需事先了解物体的几何形状。补充材料和更多演示将在https://amirshahid.github.io/BimanualTactilePose提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是通过触觉来估计物体姿态的问题。这个问题在现实中很重要，因为视觉方法在光照变化、遮挡或物体外观等因素影响下表现不稳定，而触觉传感器提供了一种紧凑、低成本的替代方案，可以直接嵌入到机器人手指尖。准确的物体姿态估计对机器人的操作和交互任务至关重要，特别是在视觉受限的环境中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类通过触摸和交互重建物体形状的能力启发，设计了双臂协作系统：一个手稳定握住物体，另一个手进行主动探索。作者设计了丰富的状态表示（包括手指配置、手部旋转、触摸状态等）和多目标奖励函数（平衡姿态估计和探索）。该方法借鉴了强化学习、基于好奇心的奖励函数、FoundationPose姿态估计和点云重建等现有工作，但创新性地将它们整合到一个完整的触觉探索框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过双臂协作，一个手稳定握住物体，另一个手使用触觉传感器主动探索物体表面，收集接触点云数据，并利用这些数据迭代重建物体形状和估计物体姿态。整体流程包括：1)初始化：双手向物体移动；2)握持调整：一只手找到稳定抓取姿态；3)探索阶段：探索手收集触觉数据；4)姿态估计：通过点云重建、深度图像渲染和FoundationPose进行姿态优化；5)强化学习训练：使用PPO算法训练探索策略，结合多目标奖励函数引导机器人收集有价值的信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双臂触态探索框架，使用简单FSR传感器实现仅触觉的姿态估计；2)新颖的状态表示和奖励函数，促进在有限动作内的探索；3)将姿态估计反馈整合到奖励函数中，引导探索关键特征；4)无需物体模板即可准确估计姿态。相比之前的工作，本文方法不依赖被动接触或预定义交互，解决了物体固定假设的限制，不仅关注表面覆盖(IoU)更关注姿态估计准确性(ADD-S)，在100步内达到87%的准确率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于强化学习的双臂触觉探索框架，通过主动收集触觉数据并利用姿态估计反馈，实现了仅使用简单触觉传感器就能高效准确地估计未知物体姿态。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust object pose estimation is essential for manipulation and interactiontasks in robotics, particularly in scenarios where visual data is limited orsensitive to lighting, occlusions, and appearances. Tactile sensors often offerlimited and local contact information, making it challenging to reconstruct thepose from partial data. Our approach uses sensorimotor exploration to activelycontrol a robot hand to interact with the object. We train with ReinforcementLearning (RL) to explore and collect tactile data. The collected 3D pointclouds are used to iteratively refine the object's shape and pose. In oursetup, one hand holds the object steady while the other performs activeexploration. We show that our method can actively explore an object's surfaceto identify critical pose features without prior knowledge of the object'sgeometry. Supplementary material and more demonstrations will be provided athttps://amirshahid.github.io/BimanualTactilePose .</description>
      <author>example@mail.com (Amir-Hossein Shahidzadeh, Jiyue Zhu, Kezhou Chen, Sha Yi, Cornelia Fermüller, Yiannis Aloimonos, Xiaolong Wang)</author>
      <guid isPermaLink="false">2509.13591v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>An Exploratory Study on Abstract Images and Visual Representations Learned from Them</title>
      <link>http://arxiv.org/abs/2509.14149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了由基本形状构成的抽象图像在传达视觉语义信息方面的能力，以及其与传统栅格图像表示之间的性能差异。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，由基本形状构建的抽象图像确实能够向深度学习模型传达视觉语义信息，但这类图像获得的表示通常不如从传统栅格图像获得的表示。&lt;h4&gt;目的&lt;/h4&gt;研究抽象图像与传统栅格图像之间性能差距的原因，并调查在不同抽象级别能捕获多少高级语义内容。&lt;h4&gt;方法&lt;/h4&gt;引入了分层抽象图像数据集(HAID)，这是一个包含从正常栅格图像在多个抽象级别生成的抽象图像的新数据集。在各种任务(包括分类、分割和目标检测)上训练和评估传统视觉系统，提供栅格化和抽象图像表示之间的综合研究。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的研究结果，主要介绍了研究方法和数据集的构建。&lt;h4&gt;结论&lt;/h4&gt;探讨抽象图像是否可以被视为传达视觉语义信息的潜在有效格式，以及其是否有助于视觉任务。&lt;h4&gt;翻译&lt;/h4&gt;想象一下生活在一个完全由基本形状构成的世界中，你还能认出熟悉的物体吗？最近的研究表明，由基本形状构建的抽象图像确实能够向深度学习模型传达视觉语义信息。然而，从这类图像获得的表示通常不如从传统栅格图像获得的表示。在本文中，我们研究这种性能差距背后的原因，并调查在不同抽象级别能捕获多少高级语义内容。为此，我们引入了分层抽象图像数据集(HAID)，这是一个新颖的数据收集，包含从正常栅格图像在多个抽象级别生成的抽象图像。然后我们在各种任务(包括分类、分割和目标检测)上训练和评估传统视觉系统，提供了栅格化和抽象图像表示之间的综合研究。我们还讨论了抽象图像是否可以被视为传达视觉语义信息的潜在有效格式，以及是否有助于视觉任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imagine living in a world composed solely of primitive shapes, could youstill recognise familiar objects? Recent studies have shown that abstractimages-constructed by primitive shapes-can indeed convey visual semanticinformation to deep learning models. However, representations obtained fromsuch images often fall short compared to those derived from traditional rasterimages. In this paper, we study the reasons behind this performance gap andinvestigate how much high-level semantic content can be captured at differentabstraction levels. To this end, we introduce the Hierarchical AbstractionImage Dataset (HAID), a novel data collection that comprises abstract imagesgenerated from normal raster images at multiple levels of abstraction. We thentrain and evaluate conventional vision systems on HAID across various tasksincluding classification, segmentation, and object detection, providing acomprehensive study between rasterised and abstract image representations. Wealso discuss if the abstract image can be considered as a potentially effectiveformat for conveying visual semantic information and contributing to visiontasks.</description>
      <author>example@mail.com (Haotian Li, Jianbo Jiao)</author>
      <guid isPermaLink="false">2509.14149v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>CSMoE: An Efficient Remote Sensing Foundation Model with Soft Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2509.14104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将Soft mixture-of-experts (MoE)机制集成到遥感基础模型中的方法，创建了CSMoE模型，并引入主题-气候描述符采样策略构建训练集。实验表明该方法在降低计算需求的同时保持或提高了表示性能，实现了现有模型两倍以上的计算效率。&lt;h4&gt;背景&lt;/h4&gt;自监督学习通过掩码自编码器在遥感基础模型发展中受到广泛关注，能够促进不同传感器和下游任务中的表示学习。然而，现有遥感基础模型通常在训练和推理过程中存在巨大的计算复杂性，或者表示能力有限，限制了它们在遥感领域的实际应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有遥感基础模型计算复杂度高和表示能力有限的问题，提高模型的效率和实用性。&lt;h4&gt;方法&lt;/h4&gt;1. 将Soft mixture-of-experts (MoE)机制集成到基础模型中；2. 将此方法应用于Cross-Sensor Masked Autoencoder (CSMAE)模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE)模型；3. 引入基于主题-气候描述符的采样策略构建具有代表性和多样性的训练集。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在场景分类、语义分割和基于内容的图像检索等任务上，该方法在降低计算需求的同时保持或提高了表示性能；2. 与最先进的遥感基础模型相比，CSMoE在表示能力、准确性和计算效率之间取得了更好的平衡；3. 平均而言，CSMoE实现了现有遥感基础模型两倍以上的计算效率，同时在所有实验中保持了有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的适应方法对于创建计算高效的遥感基础模型是有效的，通过集成Soft MoE机制，实现了特定模态的专家专业化与共享的跨传感器表示学习之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;通过掩码自编码器的自监督学习在遥感基础模型发展中引起了广泛关注，促进了不同传感器和下游任务中的改进表示学习。然而，现有的遥感基础模型通常在训练和推理过程中存在巨大的计算复杂性，或者表示能力有限。这些问题限制了它们在遥感领域的实际应用。为解决这一限制，我们提出了一种适应方法，通过将Soft mixture-of-experts (MoE)机制集成到基础模型中来提高遥感基础模型的效率。将Soft MoEs集成到基础模型中，允许特定模态的专家专业化与共享的跨传感器表示学习相结合。为了证明我们适应方法的有效性，我们将其应用于Cross-Sensor Masked Autoencoder (CSMAE)模型，创建了Cross-Sensor Mixture-of-Experts (CSMoE)模型。此外，我们引入了一种基于主题-气候描述符的采样策略，用于构建具有代表性和多样性的训练集来训练我们的CSMoE模型。在场景分类、语义分割和基于内容的图像检索方面的广泛实验表明，我们的适应方法在降低计算需求的同时保持了或提高了表示性能。与最先进的遥感基础模型相比，CSMoE在表示能力、准确性和计算效率之间取得了更好的平衡。平均而言，CSMoE实现了现有遥感基础模型两倍以上的计算效率，同时在所有实验中保持了有竞争力的性能。这些结果表明，所提出的适应方法对于创建计算高效的遥感基础模型是有效的。该模型的代码、训练集创建和模型权重将在https://git.tu-berlin.de/rsim/csmoe上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning through masked autoencoders has attracted greatattention for remote sensing (RS) foundation model (FM) development, enablingimproved representation learning across diverse sensors and downstream tasks.However, existing RS FMs often either suffer from substantial computationalcomplexity during both training and inference or exhibit limitedrepresentational capacity. These issues restrict their practical applicabilityin RS. To address this limitation, we propose an adaptation for enhancing theefficiency of RS FMs by integrating the Soft mixture-of-experts (MoE) mechanisminto the FM. The integration of Soft MoEs into the FM allows modality-specificexpert specialization alongside shared cross-sensor representation learning. Todemonstrate the effectiveness of our adaptation, we apply it on theCross-Sensor Masked Autoencoder (CSMAE) model, resulting in the Cross-SensorMixture-of-Experts (CSMoE) model. In addition, we introduce a thematic-climaticdescriptor-driven sampling strategy for the construction of a representativeand diverse training set to train our CSMoE model. Extensive experiments onscene classification, semantic segmentation, and content-based image retrievaldemonstrate that our adaptation yields a reduction in computationalrequirements while maintaining or improving representational performance.Compared to state-of-the-art RS FMs, CSMoE achieves a superior trade-offbetween representational capacity, accuracy, and computational efficiency. Onaverage, CSMoE achieves more than twice the computational efficiency ofexisting RS FMs, while maintaining competitive performance across allexperiments. These results show the effectiveness of the proposed adaptationfor creating computationally efficient RS FMs. The code for the model, thetraining set creation, and the model weights will be available athttps://git.tu-berlin.de/rsim/csmoe.</description>
      <author>example@mail.com (Leonard Hackel, Tom Burgert, Begüm Demir)</author>
      <guid isPermaLink="false">2509.14104v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Low-Rank Adapter Experts in Generalizable Audio Deepfake Detection</title>
      <link>http://arxiv.org/abs/2509.13878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种混合LoRA专家方法，通过集成多个低秩适配器和路由机制，提高了音频深度伪造检测模型的泛化能力，特别是在面对新型深度伪造方法时表现更佳&lt;h4&gt;背景&lt;/h4&gt;Wav2Vec2等基础模型在语音任务（包括音频深度伪造检测）的表示学习中表现出色。然而，在固定的一组真实和欺骗性音频片段上进行微调后，它们通常无法泛化到训练中未包含的新型深度伪造方法&lt;h4&gt;目的&lt;/h4&gt;解决基础模型在音频深度伪造检测中泛化能力不足的问题，提高模型对不断演变的深度伪造攻击的适应性&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层中，并使用路由机制选择性地激活专门的专家&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在领域内和领域外场景中都优于标准微调，相比基线模型降低了等错误率；最好的MoE-LoRA模型将平均领域外EER从8.55%降低到6.08%&lt;h4&gt;结论&lt;/h4&gt;该混合LoRA专家方法在实现可泛化的音频深度伪造检测方面有效&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Wav2Vec2在语音任务（包括音频深度伪造检测）的表示学习中表现出色。然而，在固定的一组真实和欺骗性音频片段上进行微调后，它们通常无法泛化到训练中未包含的新型深度伪造方法。为此，我们提出了一种混合LoRA专家方法，将多个低秩适配器（LoRA）集成到模型的注意力层中。路由机制选择性地激活专门的专家，增强了对不断演变的深度伪造攻击的适应性。实验结果表明，我们的方法在领域内和领域外场景中都优于标准微调，降低了相对于基线模型的等错误率。值得注意的是，我们最好的MoE-LoRA模型将平均领域外EER从8.55%降低到6.08%，证明了其在实现可泛化的音频深度伪造检测方面的有效性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models such as Wav2Vec2 excel at representation learning in speechtasks, including audio deepfake detection. However, after being fine-tuned on afixed set of bonafide and spoofed audio clips, they often fail to generalize tonovel deepfake methods not represented in training. To address this, we proposea mixture-of-LoRA-experts approach that integrates multiple low-rank adapters(LoRA) into the model's attention layers. A routing mechanism selectivelyactivates specialized experts, enhancing adaptability to evolving deepfakeattacks. Experimental results show that our method outperforms standardfine-tuning in both in-domain and out-of-domain scenarios, reducing equal errorrates relative to baseline models. Notably, our best MoE-LoRA model lowers theaverage out-of-domain EER from 8.55\% to 6.08\%, demonstrating itseffectiveness in achieving generalizable audio deepfake detection.</description>
      <author>example@mail.com (Janne Laakkonen, Ivan Kukanov, Ville Hautamäki)</author>
      <guid isPermaLink="false">2509.13878v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>Consistent View Alignment Improves Foundation Models for 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.13846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2025: 1st Place in Transformer track and 2nd Place in  Convolution track of SSL3D-OpenMind challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究挑战了表征学习中关于非相关视图足以学习有意义表征的假设，提出了一种名为'Consistent View Alignment'的自监督学习方法，通过结构化视图对齐提高下游任务性能，并在MICCAI 2025 SSL3D挑战赛中取得优异成绩。&lt;h4&gt;背景&lt;/h4&gt;近年来，表征学习方法隐含地假设数据点的非相关视图足以学习对各种下游任务有意义的表征。&lt;h4&gt;目的&lt;/h4&gt;挑战这一假设，证明潜在空间中的有意义结构不会自然出现，必须被明确诱导。&lt;h4&gt;方法&lt;/h4&gt;提出一种方法，对齐来自数据不同视图的表征，以对齐互补信息而不产生假阳性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的自监督学习方法'Consistent View Alignment'提高了下游任务的性能，突显了结构化视图对齐在学习有效表征中的关键作用。&lt;h4&gt;结论&lt;/h4&gt;结构化视图对齐对于学习有效表征至关重要。&lt;h4&gt;翻译&lt;/h4&gt;表征学习中的许多近期方法隐含地假设，数据点的非相关视图足以学习对各种下游任务有意义的表征。在本工作中，我们挑战这一假设，证明潜在空间中的有意义结构不会自然出现，而必须被明确诱导。我们提出了一种方法，对齐来自数据不同视图的表征，以对齐互补信息而不产生假阳性。我们的实验表明，我们提出的自监督学习方法'Consistent View Alignment'提高了下游任务的性能，突显了结构化视图对齐在学习有效表征中的关键作用。当使用Primus视觉变换器和ResEnc卷积神经网络时，我们的方法在MICCAI 2025 SSL3D挑战赛中分别获得第一名和第二名。代码和预训练模型权重已在https://github.com/Tenbatsu24/LatentCampus发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自监督学习中当数据视图之间相关性较弱时的'假阳性'问题。在医学图像分析中，简单的随机裁剪可能导致两个视图代表完全不同的语义信息，而现有对比学习方法仍会强制模型对这些不相关特征进行对齐，导致虚假关联并降低表示质量。这个问题很重要，因为它直接影响医学AI中下游任务（如分割、分类）的性能，进而关系到诊断的准确性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有对比学习方法（如SimCLR、SwAV）假设正样本对完美相关，但在医学图像等复杂场景中这一假设常被违反。他们注意到现有方法很少直接约束特征空间中对齐应该发生的位置。因此，作者设计了'一致视图对齐'方法，通过生成具有重叠区域的视图对，并在特征空间中对齐这些重叠区域，只在这些区域应用一致性损失。作者借鉴了对比学习中的学生-教师架构、掩码自编码器的重建目标、SwAV的对称化损失计算以及VoCo的体积特定增强方法，但创新性地将它们组合以解决假阳性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过确保只有真正语义对应的区域被对齐来避免假阳性问题，而不是简单地假设两个随机视图应该相似。整体流程包括：1)从原始3D医学图像生成两个具有重叠区域(40%-80%)的随机裁剪；2)使用学生和教师网络提取特征；3)应用掩码自编码器目标重建被遮蔽区域；4)使用ROIAlign对齐重叠区域的特征；5)计算对齐区域间的特征相似度；6)在两个视图方向上对称化计算损失；7)结合重建损失、一致性损失和可选的对比损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)通过一致视图构造缓解假阳性问题；2)通过强制局部特征一致性改善下游分割任务；3)在不同空间上下文和语义内容间强制特征一致性。相比之前工作的不同：与传统对比学习不同，CVA不假设整个视图相似，只对齐已知重叠区域；与掩码自编码器不同，CVA添加了显式特征对齐步骤；与对比掩码自编码器不同，CVA主动识别并纠正假阳性；与其他3D医学图像预训练方法不同，CVA更关注减少假阳性而非假阴性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CVA通过显式对齐不同视图中的语义对应区域而非强制对齐整个视图，有效缓解了自监督学习中的假阳性问题，显著提高了3D医学图像分割任务的表现，同时展示了局部特征一致性与全局判别特征学习之间的权衡关系。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many recent approaches in representation learning implicitly assume thatuncorrelated views of a data point are sufficient to learn meaningfulrepresentations for various downstream tasks. In this work, we challenge thisassumption and demonstrate that meaningful structure in the latent space doesnot emerge naturally. Instead, it must be explicitly induced. We propose amethod that aligns representations from different views of the data to aligncomplementary information without inducing false positives. Our experimentsshow that our proposed self-supervised learning method, Consistent ViewAlignment, improves performance for downstream tasks, highlighting the criticalrole of structured view alignment in learning effective representations. Ourmethod achieved first and second place in the MICCAI 2025 SSL3D challenge whenusing a Primus vision transformer and ResEnc convolutional neural network,respectively. The code and pretrained model weights are released athttps://github.com/Tenbatsu24/LatentCampus.</description>
      <author>example@mail.com (Puru Vaish, Felix Meister, Tobias Heimann, Christoph Brune, Jelmer M. Wolterink)</author>
      <guid isPermaLink="false">2509.13846v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>SAMIR, an efficient registration framework via robust feature learning from SAM</title>
      <link>http://arxiv.org/abs/2509.13629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SAMIR的高效医学图像配准框架，利用Segment Anything Model (SAM)增强特征提取，显著提升了配准性能。&lt;h4&gt;背景&lt;/h4&gt;医学图像配准是医学图像分析的基础任务，变形通常与组织的形态特征密切相关，准确的特征提取至关重要。然而，现有的弱监督方法需要分割掩码或地标等解剖先验，这些标签往往难以获取，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;利用视觉基础模型的强大表示学习能力，开发一种不依赖弱标签的高效医学图像配准方法，提高配准精度和实用性。&lt;h4&gt;方法&lt;/h4&gt;使用SAM的图像编码器提取结构感知的特征嵌入，而非直接使用原始输入图像；设计特定任务的自适应管道和轻量级3D头在嵌入空间内细化特征，适应医学图像中的局部变形；引入分层特征一致性损失来引导粗到细的特征匹配，改善解剖对齐。&lt;h4&gt;主要发现&lt;/h4&gt;SAMIR在受试者内心脏图像配准和受试者间腹部CT图像配准任务上显著优于现有方法，在ACDC数据集上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;SAMIR是一种有效的医学图像配准框架，利用SAM可以增强特征提取，提高配准性能，且代码将在论文接受后公开在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;图像配准是医学图像分析中的基础任务。变形通常与组织的形态特征密切相关，使得准确的特征提取至关重要。近期的弱监督方法通过整合分割掩码或地标等解剖先验来改进配准，这些先验可以作为输入或损失函数的一部分。然而，这类弱标签通常不易获取，限制了其实际应用。受视觉基础模型强大表示学习能力的启发，本文引入了SAMIR，一种利用Segment Anything Model (SAM)增强特征提取的高效医学图像配准框架。SAM在大型自然图像数据集上预训练，可以学习强大且通用的视觉表示。我们设计了一个特定任务的自适应管道，使用SAM的图像编码器提取结构感知的特征嵌入，而非使用原始输入图像，从而能够更准确地建模解剖一致性和变形模式。我们还设计了一个轻量级3D头在嵌入空间内细化特征，适应医学图像中的局部变形。此外，我们引入了分层特征一致性损失来引导粗到细的特征匹配，改善解剖对齐。大量实验表明，在受试者内心脏图像配准和受试者间腹部CT图像配准的基准数据集上，SAMIR显著优于最先进的方法，在ACDC上实现了2.68%的性能提升，在腹部数据集上实现了6.44%的性能提升。论文接受后，源代码将在GitHub上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学图像配准中特征提取的准确性和鲁棒性问题。医学图像配准对诊断、手术规划和运动分析至关重要，但现有方法难以处理图像质量差异（如对比度变化和噪声），且依赖难以获取的分割标签。提高配准准确性和鲁棒性可以辅助医生进行更精准的诊疗决策，扩大方法在实际临床环境中的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统配准方法的局限性（耗时、对图像质量敏感）和弱监督方法的不足（标签获取困难）。受SAM模型在图像分割中强大特征提取能力的启发，作者设计了一种新的配准框架，利用SAM的结构感知特征而非原始图像。作者借鉴了金字塔策略处理大变形问题，参考了弱监督方法的思想但避免了直接依赖分割标签，同时结合了视觉基础模型在其他任务中的成功应用经验。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用SAM的预训练图像编码器提取结构感知的特征嵌入，增强医学图像配准的准确性和鲁棒性。整体流程分为三部分：1) 结构感知特征嵌入模块：将3D医学数据转为2D切片处理，通过SAM编码器提取特征，再用轻量级3D卷积模块增强特征；2) 金字塔变形场预测模块：采用粗到细策略，在不同尺度上预测变形场，逐步优化；3) 分层特征一致性损失：在多尺度空间中对齐特征，提高解剖结构一致性。整个流程避免了依赖难以获取的弱标签，同时提高了对图像质量变化的鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 创新性地将SAM模型用于医学图像配准，设计了特定的3D适应管道；2) 提出结构感知特征嵌入模块，利用SAM的强大特征提取能力；3) 设计分层特征一致性损失，在多尺度空间中对齐特征；4) 结合金字塔策略处理大变形问题。相比之前工作，SAMIR避免了依赖难以获取的分割标签，直接利用SAM的特征提取能力而非原始图像，在ACDC和腹部CT数据集上分别实现了2.68%和6.44%的性能提升，且对图像质量变化表现出更强的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAMIR创新性地利用SAM的强大特征提取能力，通过结构感知特征嵌入和分层特征一致性损失，实现了更准确、更鲁棒的医学图像配准，显著优于现有方法且无需依赖难以获取的弱标签。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image registration is a fundamental task in medical image analysis.Deformations are often closely related to the morphological characteristics oftissues, making accurate feature extraction crucial. Recent weakly supervisedmethods improve registration by incorporating anatomical priors such assegmentation masks or landmarks, either as inputs or in the loss function.However, such weak labels are often not readily available, limiting theirpractical use. Motivated by the strong representation learning ability ofvisual foundation models, this paper introduces SAMIR, an efficient medicalimage registration framework that utilizes the Segment Anything Model (SAM) toenhance feature extraction. SAM is pretrained on large-scale natural imagedatasets and can learn robust, general-purpose visual representations. Ratherthan using raw input images, we design a task-specific adaptation pipelineusing SAM's image encoder to extract structure-aware feature embeddings,enabling more accurate modeling of anatomical consistency and deformationpatterns. We further design a lightweight 3D head to refine features within theembedding space, adapting to local deformations in medical images.Additionally, we introduce a Hierarchical Feature Consistency Loss to guidecoarse-to-fine feature matching and improve anatomical alignment. Extensiveexperiments demonstrate that SAMIR significantly outperforms state-of-the-artmethods on benchmark datasets for both intra-subject cardiac image registrationand inter-subject abdomen CT image registration, achieving performanceimprovements of 2.68% on ACDC and 6.44% on the abdomen dataset. The source codewill be publicly available on GitHub following the acceptance of this paper.</description>
      <author>example@mail.com (Yue He, Min Liu, Qinghao Liu, Jiazheng Wang, Yaonan Wang, Hang Zhang, Xiang Chen)</author>
      <guid isPermaLink="false">2509.13629v1</guid>
      <pubDate>Thu, 18 Sep 2025 14:53:48 +0800</pubDate>
    </item>
    <item>
      <title>HAM: Hierarchical Adapter Merging for Scalable Continual Learning</title>
      <link>http://arxiv.org/abs/2509.13211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为分层适配器合并(HAM)的新框架，用于解决持续学习中的灾难性遗忘问题，通过动态组合不同任务的适配器，有效管理多个任务并提高效率。&lt;h4&gt;背景&lt;/h4&gt;持续学习是人类认知的重要能力，但对当前深度学习模型构成重大挑战。新知识会干扰已学习的信息，导致模型遗忘旧知识，这种现象称为灾难性遗忘。大型预训练模型可通过利用现有知识和过参数化部分缓解遗忘，但在面对新数据分布时仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效扩展到动态学习场景和长任务序列的方法，解决参数高效微调(PEFT)方法在管理多个任务时面临的复杂性和干扰问题。&lt;h4&gt;方法&lt;/h4&gt;提出HAM框架，在训练过程中动态组合来自不同任务的适配器。维护一组固定组，分层合并新的适配器；对每个任务训练低秩适配器和重要性标量；基于适配器相似性动态分组任务；在每个组内对适配器进行修剪、缩放和合并，促进相关任务间的迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是在任务数量增加时优势更加明显。HAM能够有效扩展，比竞争基线方法管理更多任务同时提高效率。&lt;h4&gt;结论&lt;/h4&gt;HAM框架提供了一种有效解决持续学习中灾难性遗忘问题的方法，通过分层合并适配器，能够在处理多个任务时保持性能并提高效率。&lt;h4&gt;翻译&lt;/h4&gt;持续学习是人类认知的基本能力，然而它对当前的深度学习模型构成了重大挑战。主要问题是新知识可能会干扰已学习的信息，导致模型遗忘旧知识而倾向于新知识，这种现象被称为灾难性遗忘。尽管大型预训练模型可以通过利用其现有知识和过参数化部分缓解遗忘，但当面对新的数据分布时，它们仍然存在困难。参数高效微调(PEFT)方法（如LoRA）使模型能够高效地适应新知识。然而，在扩展到动态学习场景和长任务序列时，它们仍面临挑战，因为每个任务维护一个适配器会增加复杂性并增加干扰的可能性。在本文中，我们引入了分层适配器合并(HAM)，一个在训练过程中动态组合来自不同任务的适配器的新框架。这种方法使HAM能够有效扩展，允许它以更高的效率管理比竞争基线更多的任务。为此，HAM维护一组固定组，这些组分层合并新的适配器。对于每个任务，HAM训练一个低秩适配器和一个重要性标量，然后基于适配器相似性动态对任务进行分组。在每个组内，适配器被修剪、缩放和合并，促进相关任务之间的迁移学习。在三个视觉基准测试上的大量实验表明，HAM显著优于最先进的方法，特别是随着任务数量的增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual learning is an essential capability of human cognition, yet itposes significant challenges for current deep learning models. The primaryissue is that new knowledge can interfere with previously learned information,causing the model to forget earlier knowledge in favor of the new, a phenomenonknown as catastrophic forgetting. Although large pre-trained models canpartially mitigate forgetting by leveraging their existing knowledge andover-parameterization, they often struggle when confronted with novel datadistributions. Parameter-Efficient Fine-Tuning (PEFT) methods, such as LoRA,enable efficient adaptation to new knowledge. However, they still facechallenges in scaling to dynamic learning scenarios and long sequences oftasks, as maintaining one adapter per task introduces complexity and increasesthe potential for interference. In this paper, we introduce HierarchicalAdapters Merging (HAM), a novel framework that dynamically combines adaptersfrom different tasks during training. This approach enables HAM to scaleeffectively, allowing it to manage more tasks than competing baselines withimproved efficiency. To achieve this, HAM maintains a fixed set of groups thathierarchically consolidate new adapters. For each task, HAM trains a low-rankadapter along with an importance scalar, then dynamically groups tasks based onadapter similarity. Within each group, adapters are pruned, scaled and merge,facilitating transfer learning between related tasks. Extensive experiments onthree vision benchmarks show that HAM significantly outperformsstate-of-the-art methods, particularly as the number of tasks increases.</description>
      <author>example@mail.com (Eric Nuertey Coleman, Luigi Quarantiello, Samrat Mukherjee, Julio Hurtado, Vincenzo Lomonaco)</author>
      <guid isPermaLink="false">2509.13211v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
  <item>
      <title>Ensemble Visualization With Variational Autoencoder</title>
      <link>http://arxiv.org/abs/2509.13000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE Workshop on Uncertainty Visualization&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一种新的数据集可视化方法，通过在潜在空间中构建结构化概率表示，实现数据集的有效可视化和分析。&lt;h4&gt;背景&lt;/h4&gt;数据集的可视化是理解和分析复杂空间数据的重要挑战，特别是在处理高维数据集合时。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来可视化数据集，通过构建潜在空间中的结构化概率表示，实现数据集的特征提取和概率分布分析。&lt;h4&gt;方法&lt;/h4&gt;通过特征空间转换和变分自编码器的无监督学习，将空间数据集合的特征转换为潜在空间，使潜在空间遵循多元标准高斯分布，从而支持置信区间计算和概率分布密度估计。&lt;h4&gt;主要发现&lt;/h4&gt;在天气预报集合上的初步结果表明，所提出的方法在数据集可视化方面是有效且多功能的，能够准确表示数据集合的概率分布特性。&lt;h4&gt;结论&lt;/h4&gt;该方法为数据集可视化提供了一种新的有效途径，通过结构化概率表示在潜在空间中，能够更好地理解和分析复杂的数据集合。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过在潜在空间中构建结构化概率表示来可视化数据集的新方法，即空间数据特征的低维表示。我们的方法通过特征空间转换和使用变分自编码器的无监督学习，将集合的空间特征转换为潜在空间。由此产生的潜在空间遵循多元标准高斯分布，使得能够计算置信区间和生成数据集的概率分布的密度估计。在天气预报集合上的初步结果证明了我们方法的有效性和多功能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to visualize data ensembles by constructingstructured probabilistic representations in latent spaces, i.e.,lower-dimensional representations of spatial data features. Our approachtransforms the spatial features of an ensemble into a latent space throughfeature space conversion and unsupervised learning using a variationalautoencoder (VAE). The resulting latent spaces follow multivariate standardGaussian distributions, enabling analytical computation of confidence intervalsand density estimation of the probabilistic distribution that generates thedata ensemble. Preliminary results on a weather forecasting ensembledemonstrate the effectiveness and versatility of our method.</description>
      <author>example@mail.com (Cenyang Wu, Qinhan Yu, Liang Zhou)</author>
      <guid isPermaLink="false">2509.13000v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Rapid Adaptation of SpO2 Estimation to Wearable Devices via Transfer Learning on Low-Sampling-Rate PPG</title>
      <link>http://arxiv.org/abs/2509.12515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In the proceedings of IEEE-EMBS International Conference on Body  Sensor Networks 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的框架，利用低采样率双通道光电容积脉搏波实现可穿戴设备上的低功耗血氧饱和度监测，无需复杂临床校准。&lt;h4&gt;背景&lt;/h4&gt;血氧饱和度(SpO2)是医疗保健监测的重要指标，但传统估计方法依赖复杂临床校准，不适合低功耗可穿戴应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速适应于能量高效可穿戴设备的SpO2估计方法，使用低采样率(25Hz)双通道光电容积脉搏波(PPG)。&lt;h4&gt;方法&lt;/h4&gt;首先在公共临床数据集上预训练带有自注意力的双向长短期记忆(BiLSTM)模型，然后使用从可穿戴We-Be带和FDA批准的参考脉搏血氧仪收集的数据进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集上平均绝对误差为2.967%，在私有数据集上为2.624%，显著优于传统方法；使用25Hz PPG相比100Hz减少40%功耗；瞬时SpO2预测误差为3.284%，能有效捕捉快速波动。&lt;h4&gt;结论&lt;/h4&gt;证明了准确、低功耗SpO2监测在可穿戴设备上的快速适应性，无需临床校准即可实现。&lt;h4&gt;翻译&lt;/h4&gt;血氧饱和度(SpO2)是医疗保健监测的重要指标。传统的SpO2估计方法通常依赖复杂的临床校准，使其不适合低功耗、可穿戴应用。在本文中，我们提出了一种基于迁移学习的框架，利用低采样率(25Hz)双通道光电容积脉搏波(PPG)实现SpO2估计向节能可穿戴设备的快速适应。我们首先在公共临床数据集上预训练一个带有自注意力的双向长短期记忆(BiLSTM)模型，然后使用从我们的可穿戴We-Be带和FDA批准的参考脉搏血氧仪收集的数据对其进行微调。实验结果表明，我们的方法在公共数据集上达到2.967%的平均绝对误差(MAE)，在私有数据集上达到2.624%的MAE，显著优于传统校准和非迁移学习基线。此外，与100Hz相比，使用25Hz PPG可减少40%的功耗(不包括基线功耗)。我们的方法在瞬时SpO2预测中也达到了3.284%的MAE，有效捕捉了快速波动。这些结果证明了无需临床校准即可在可穿戴设备上实现准确、低功耗SpO2监测的快速适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Blood oxygen saturation (SpO2) is a vital marker for healthcare monitoring.Traditional SpO2 estimation methods often rely on complex clinical calibration,making them unsuitable for low-power, wearable applications. In this paper, wepropose a transfer learning-based framework for the rapid adaptation of SpO2estimation to energy-efficient wearable devices using low-sampling-rate (25Hz)dual-channel photoplethysmography (PPG). We first pretrain a bidirectional LongShort-Term Memory (BiLSTM) model with self-attention on a public clinicaldataset, then fine-tune it using data collected from our wearable We-Be bandand an FDA-approved reference pulse oximeter. Experimental results show thatour approach achieves a mean absolute error (MAE) of 2.967% on the publicdataset and 2.624% on the private dataset, significantly outperformingtraditional calibration and non-transferred machine learning baselines.Moreover, using 25Hz PPG reduces power consumption by 40% compared to 100Hz,excluding baseline draw. Our method also attains an MAE of 3.284% ininstantaneous SpO2 prediction, effectively capturing rapid fluctuations. Theseresults demonstrate the rapid adaptation of accurate, low-power SpO2 monitoringon wearable devices without the need for clinical calibration.</description>
      <author>example@mail.com (Zequan Liang, Ruoyu Zhang, Wei Shao, krishna Karthik, Ehsan Kourkchi, Setareh Rafatirad, Houman Homayoun)</author>
      <guid isPermaLink="false">2509.12515v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Smart Farming Through Federated Learning: A Secure, Scalable, and Efficient Approach for AI-Driven Agriculture</title>
      <link>http://arxiv.org/abs/2509.12363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种联邦学习框架用于智能农业，旨在为明尼苏达州农场开发可扩展、高效且安全的作物疾病检测解决方案，同时保护农场数据隐私。&lt;h4&gt;背景&lt;/h4&gt;农业部门正经历转型，先进技术特别是数据驱动决策的整合日益重要。然而，农场对共享运营数据存在隐私顾虑，导致数据驱动的农业解释需求与农场隐私担忧之间存在矛盾。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、高效且安全的作物疾病检测解决方案，该方案能适应明尼苏达州农场的环境和运营条件，同时保持敏感农场数据的本地性，实现高准确率的作物疾病分类而不损害数据隐私。&lt;h4&gt;方法&lt;/h4&gt;研究涉及从明尼苏达州农场收集数据，应用本地深度学习算法，使用迁移学习，以及通过中央聚合服务器进行模型优化。联邦学习框架使敏感数据保持本地，同时实现协作模型更新。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有望实现疾病检测准确率的提高，在农业场景中具有良好的泛化能力，降低通信和训练成本，并在未来实现疾病的早期识别和干预。&lt;h4&gt;结论&lt;/h4&gt;这项工作弥合了先进机器学习技术与明尼苏达州及其他地区农民的实际、隐私敏感需求之间的差距，利用联邦学习的优势，为农业提供了安全高效的疾病检测方法，有望彻底改变智能农业系统并解决当地农业问题，同时确保数据机密性。&lt;h4&gt;翻译&lt;/h4&gt;农业部门正经历转型，先进技术特别是数据驱动决策的整合日益重要。本研究提出了一种用于智能农业的联邦学习框架，旨在为明尼苏达州农场开发可扩展、高效且安全的作物疾病检测解决方案，适应其环境和运营条件。通过将敏感农场数据保存在本地并实现协作模型更新，我们提出的框架旨在实现高准确率的作物疾病分类而不损害数据隐私。我们概述了一种涉及从明尼苏达州农场收集数据、应用本地深度学习算法、迁移学习以及通过中央聚合服务器进行模型优化的方法，旨在提高疾病检测的准确率，在农业场景中实现良好的泛化能力，降低通信和训练成本，并在未来实施中实现疾病的早期识别和干预。我们概述了方法和预期成果，为后续研究中的实证验证奠定了基础。这项工作出现在越来越多的农业数据驱动解释需求必须与对农场不愿共享其运营数据的隐私担忧进行权衡的背景下。这将提供一种安全高效的疾病检测方法，最终能够彻底改变智能农业系统，并以数据保密性解决当地农业问题。通过这样做，本文弥合了先进机器学习技术与明尼苏达州及其他地区农民的实际、隐私敏感需求之间的差距，利用了联邦学习的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.47852/bonviewAIA52025089&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The agricultural sector is undergoing a transformation with the integrationof advanced technologies, particularly in data-driven decision-making. Thiswork proposes a federated learning framework for smart farming, aiming todevelop a scalable, efficient, and secure solution for crop disease detectiontailored to the environmental and operational conditions of Minnesota farms. Bymaintaining sensitive farm data locally and enabling collaborative modelupdates, our proposed framework seeks to achieve high accuracy in crop diseaseclassification without compromising data privacy. We outline a methodologyinvolving data collection from Minnesota farms, application of local deeplearning algorithms, transfer learning, and a central aggregation server formodel refinement, aiming to achieve improved accuracy in disease detection,good generalization across agricultural scenarios, lower costs in communicationand training time, and earlier identification and intervention against diseasesin future implementations. We outline a methodology and anticipated outcomes,setting the stage for empirical validation in subsequent studies. This workcomes in a context where more and more demand for data-driven interpretationsin agriculture has to be weighed with concerns about privacy from farms thatare hesitant to share their operational data. This will be important to providea secure and efficient disease detection method that can finally revolutionizesmart farming systems and solve local agricultural problems with dataconfidentiality. In doing so, this paper bridges the gap between advancedmachine learning techniques and the practical, privacy-sensitive needs offarmers in Minnesota and beyond, leveraging the benefits of federated learning.</description>
      <author>example@mail.com (Ritesh Janga, Rushit Dave)</author>
      <guid isPermaLink="false">2509.12363v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ROOM: A Physics-Based Continuum Robot Simulator for Photorealistic Medical Datasets Generation</title>
      <link>http://arxiv.org/abs/2509.13177v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ROOM是一个综合模拟框架，用于生成逼真的支气管镜检查训练数据，解决连续体机器人发展中缺乏真实训练环境的问题。&lt;h4&gt;背景&lt;/h4&gt;连续体机器人正在推进支气管镜检查程序，能够进入复杂的肺部气道并实现靶向干预。然而，其发展受到缺乏真实训练和测试环境的限制。真实数据难以收集，因为受到伦理约束和患者安全问题的限制，开发自主算法需要真实的成像和物理反馈。&lt;h4&gt;目的&lt;/h4&gt;提出ROOM（医学中真实的光学观察）模拟框架，用于生成逼真的支气管镜检查训练数据。&lt;h4&gt;方法&lt;/h4&gt;利用患者CT扫描，管道渲染多模态传感器数据，包括具有真实噪声和镜面反射的RGB图像、度量深度图、表面法线、光流和点云，在医学相关尺度上。&lt;h4&gt;主要发现&lt;/h4&gt;在多视图姿态估计和单目深度估计两个医疗机器人标准任务中验证了ROOM生成的数据，展示了最先进方法必须克服的多样化挑战。证明ROOM产生的数据可用于微调现有的深度估计模型，使其他下游应用（如导航）成为可能。&lt;h4&gt;结论&lt;/h4&gt;预期ROOM将能够在多样化的患者解剖结构和临床环境中难以捕捉的程序场景中实现大规模数据生成。&lt;h4&gt;翻译&lt;/h4&gt;连续体机器人通过进入复杂的肺部气道和实现靶向干预，正在推进支气管镜检查程序。然而，其发展受到缺乏真实训练和测试环境的限制：由于伦理约束和患者安全问题，真实数据难以收集，并且开发自主算法需要真实的成像和物理反馈。我们提出了ROOM（医学中真实的光学观察），这是一个综合模拟框架，专为生成逼真的支气管镜检查训练数据而设计。通过利用患者CT扫描，我们的管道渲染多模态传感器数据，包括具有真实噪声和镜面反射的RGB图像、度量深度图、表面法线、光流和点云，在医学相关尺度上。我们在医疗机器人的两个标准任务——多视图姿态估计和单目深度估计中验证了ROOM生成的数据，展示了最先进方法必须克服的多样化挑战，才能转移到这些医疗环境中。此外，我们证明ROOM产生的数据可用于微调现有的深度估计模型以克服这些挑战，同时使其他下游应用（如导航）成为可能。我们期望ROOM将能够在多样化的患者解剖结构和临床环境中难以捕捉的程序场景中实现大规模数据生成。代码和数据：https://github.com/iamsalvatore/room。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决支气管镜检查中连续体机器人开发面临的训练数据稀缺问题。由于伦理限制、患者安全问题和临床数据收集的高成本，真实数据难以获取；同时，人体解剖结构的个体化特性要求算法必须适应不同气道几何形状并保持毫米级精度。这个问题限制了自主导航算法的发展，而合成数据生成能提供大规模、多样化的训练数据，促进支气管镜机器人的技术进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了支气管镜检查的特殊挑战：需要解剖保真度、特定光照条件以及临床尺度校准。他们借鉴了Cosserat杆理论建模连续体机器人运动学，使用PyBullet作为物理引擎，Blender的路径追踪和BSDF着色器实现光保真渲染。同时参考了现有深度估计和姿态估计方法进行验证，并利用医学图像分割和3D重建技术处理CT数据。作者整合了这些现有技术，创建了一个专门针对支气管镜检查的统一模拟框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用患者CT扫描数据，通过物理模拟和光保真渲染，生成多样化的多模态传感器数据，解决支气管镜训练数据稀缺问题。整体流程分为四步：1)中轴提取：从CT扫描重建3D肺部模型并提取中轴轨迹；2)自动采样：沿骨骼结构采样，在关键区域增加密度；3)数据合成：生成RGB图像、深度图、表面法线等多模态数据；4)传感器噪声建模：通过频域分析添加真实噪声特征。整个过程从CT扫描开始，最终生成同步的多模态传感器数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ROOM框架：首个完全自动化将CT数据转换为合成训练数据的管道；2)光保真渲染：考虑内窥镜光照和组织表面特性；3)多模态数据生成：提供RGB、深度、法线等多种传感器数据；4)频域噪声建模：准确复制真实支气管镜图像噪声特性。相比传统模拟器，ROOM提供光保真度和多模态数据；相比结肠镜数据生成，ROOM应对支气管镜特有的几何和外观挑战；相比现有连续体机器人系统，ROOM统一了物理模拟和光保真渲染，弥合了两者间的差距。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ROOM是一个创新的物理模拟框架，通过整合患者特异性解剖重建、连续体机器人物理和光保真渲染，生成多样化的支气管镜训练数据集，解决了真实数据稀缺的关键挑战，并提高了姿态估计和深度估计模型在医疗环境中的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuum robots are advancing bronchoscopy procedures by accessing complexlung airways and enabling targeted interventions. However, their development islimited by the lack of realistic training and test environments: Real data isdifficult to collect due to ethical constraints and patient safety concerns,and developing autonomy algorithms requires realistic imaging and physicalfeedback. We present ROOM (Realistic Optical Observation in Medicine), acomprehensive simulation framework designed for generating photorealisticbronchoscopy training data. By leveraging patient CT scans, our pipelinerenders multi-modal sensor data including RGB images with realistic noise andlight specularities, metric depth maps, surface normals, optical flow and pointclouds at medically relevant scales. We validate the data generated by ROOM intwo canonical tasks for medical robotics -- multi-view pose estimation andmonocular depth estimation, demonstrating diverse challenges thatstate-of-the-art methods must overcome to transfer to these medical settings.Furthermore, we show that the data produced by ROOM can be used to fine-tuneexisting depth estimation models to overcome these challenges, also enablingother downstream applications such as navigation. We expect that ROOM willenable large-scale data generation across diverse patient anatomies andprocedural scenarios that are challenging to capture in clinical settings. Codeand data: https://github.com/iamsalvatore/room.</description>
      <author>example@mail.com (Salvatore Esposito, Matías Mattamala, Daniel Rebain, Francis Xiatian Zhang, Kevin Dhaliwal, Mohsen Khadem, Subramanian Ramamoorthy)</author>
      <guid isPermaLink="false">2509.13177v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>WHU-STree: A Multi-modal Benchmark Dataset for Street Tree Inventory</title>
      <link>http://arxiv.org/abs/2509.13172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WHU-STree是一个跨城市、丰富标注和多模态的城市行道树数据集，包含两个不同城市收集的同步点云和高分辨率图像，涵盖21,007个标注的树实例，涉及50个物种和2个形态参数。该数据集支持超过10个与行道树清单相关的任务，并针对树种分类和单木分割两个关键任务进行了基准测试。&lt;h4&gt;背景&lt;/h4&gt;行道树对城市宜居性至关重要，提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产需要建立详细、准确且动态更新的行道树清单。传统的地面调查耗时且劳动密集，而利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有的MMS获取的树数据集受限于小规模场景、有限的标注或单一模态，限制了它们用于综合分析的能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有MMS获取的树数据集的局限性，研究引入了WHU-STree，这是一个跨城市、丰富标注和多模态的城市行道树数据集。&lt;h4&gt;方法&lt;/h4&gt;WHU-STree数据集是在两个不同城市收集的，集成了同步点云和高分辨率图像，包含21,007个标注的树实例，涵盖50个物种和2个形态参数。研究利用WHU-STree的独特特性，同时支持超过10个与行道树清单相关的任务。研究对两个关键任务(树种分类和单木分割)的代表性基线进行了基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验和深入的分析证明了多模态数据融合的显著潜力，并强调了跨域适用性是实际算法部署的关键前提。研究确定了关键挑战，并概述了未来可能的工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。&lt;h4&gt;结论&lt;/h4&gt;WHU-STree数据集为城市行道树研究提供了一个全面、多模态的资源，支持多种任务，并有助于推动相关算法的发展和应用。&lt;h4&gt;翻译&lt;/h4&gt;行道树对城市宜居性至关重要，提供生态和社会效益。在空间受限的城市环境中优化这些多功能资产需要建立详细、准确且动态更新的行道树清单。鉴于传统的地面调查耗时且劳动密集，利用移动测量系统(MMS)的自动化调查提供了更高效的解决方案。然而，现有的MMS获取的树数据集受限于小规模场景、有限的标注或单一模态，限制了它们用于综合分析的能力。为了解决这些局限性，我们引入了WHU-STree，这是一个跨城市、丰富标注和多模态的城市行道树数据集。WHU-STree在两个不同城市收集，集成了同步点云和高分辨率图像，包含21,007个标注的树实例，涵盖50个物种和2个形态参数。利用WHU-STree的独特特性，它可以同时支持超过10个与行道树清单相关的任务。我们针对两个关键任务(树种分类和单木分割)对代表性基线进行了基准测试。广泛的实验和深入的分析证明了多模态数据融合的显著潜力，并强调了跨域适用性是实际算法部署的关键前提。特别是，我们确定了关键挑战，并概述了未来可能的工作方向，包括多模态融合、多任务协作、跨域泛化、空间模式学习以及用于行道树资产管理的多模态大语言模型。WHU-STree数据集可通过以下网址获取：https://github.com/WHU-USI3DV/WHU-STree。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有城市街道树木数据集的三个限制：小规模场景、有限标注和单一模态。这个问题很重要，因为街道树木对城市宜居性至关重要，提供生态和社会效益，建立准确、动态更新的树木清单对优化城市环境中的这些多功能资产至关重要，而传统实地调查耗时耗力，现有数据集的限制又阻碍了自动化调查解决方案的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过识别现有数据集的三个主要限制（小规模场景、有限标注和单一模态）来思考问题，然后设计了一个跨越城市、丰富标注和多模态的数据集。作者在两个气候区显著不同的城市（南京和沈阳）收集数据，并对数据进行预处理和详细标注。作者借鉴了现有的树木分割和分类方法（如MinkNet、PointMLP、SegmentAnyTree等）以及多模态融合方法（如TSCMDL和LCPS）作为基准进行评估。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个跨越城市、丰富标注和多模态的城市街道树木数据集，整合点云和图像数据以支持多种树木清单相关任务。整体流程包括：1)在南京和沈阳使用移动测量系统收集数据；2)对数据进行预处理（降采样、自适应分区、异常点去除）；3)进行详细标注（树木实例、物种信息和形态参数）；4)采用两种数据分割策略（类别平衡分割和跨城市分割）；5)最终形成一个包含21,007个树木实例、50个物种和2个形态参数的大规模数据集。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨城市覆盖，涵盖两个气候区不同的城市；2)丰富标注，包含精确的个体树木定位、物种信息和形态参数；3)多模态数据，整合点云和全景图像；4)多任务支持，支持多种配置和任务。相比之前的工作，WHU-STree规模更大（21,007个树木实例），标注更丰富（包含物种和形态参数），是多模态的，且跨城市覆盖，而现有数据集通常是单模态、小规模、单一区域的，且标注有限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个大规模、多模态、跨城市的街道树木基准数据集WHU-STree，通过整合精确的3D点云和高分辨率图像数据，支持多种与街道树木清单相关的任务，为城市树木自动化管理提供了宝贵资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Street trees are vital to urban livability, providing ecological and socialbenefits. Establishing a detailed, accurate, and dynamically updated streettree inventory has become essential for optimizing these multifunctional assetswithin space-constrained urban environments. Given that traditional fieldsurveys are time-consuming and labor-intensive, automated surveys utilizingMobile Mapping Systems (MMS) offer a more efficient solution. However, existingMMS-acquired tree datasets are limited by small-scale scene, limitedannotation, or single modality, restricting their utility for comprehensiveanalysis. To address these limitations, we introduce WHU-STree, a cross-city,richly annotated, and multi-modal urban street tree dataset. Collected acrosstwo distinct cities, WHU-STree integrates synchronized point clouds andhigh-resolution images, encompassing 21,007 annotated tree instances across 50species and 2 morphological parameters. Leveraging the unique characteristics,WHU-STree concurrently supports over 10 tasks related to street tree inventory.We benchmark representative baselines for two key tasks--tree speciesclassification and individual tree segmentation. Extensive experiments andin-depth analysis demonstrate the significant potential of multi-modal datafusion and underscore cross-domain applicability as a critical prerequisite forpractical algorithm deployment. In particular, we identify key challenges andoutline potential future works for fully exploiting WHU-STree, encompassingmulti-modal fusion, multi-task collaboration, cross-domain generalization,spatial pattern learning, and Multi-modal Large Language Model for street treeasset management. The WHU-STree dataset is accessible at:https://github.com/WHU-USI3DV/WHU-STree.</description>
      <author>example@mail.com (Ruifei Ding, Zhe Chen, Wen Fan, Chen Long, Huijuan Xiao, Yelu Zeng, Zhen Dong, Bisheng Yang)</author>
      <guid isPermaLink="false">2509.13172v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MSDNet: Efficient 4D Radar Super-Resolution via Multi-Stage Distillation</title>
      <link>http://arxiv.org/abs/2509.13149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MSDNet是一种多阶段蒸馏框架，用于4D雷达点云超分辨率，通过将密集LiDAR先验知识转移到雷达特征上，实现高质量重建和计算效率。&lt;h4&gt;背景&lt;/h4&gt;4D雷达超分辨率是自动驾驶感知中的基础问题，旨在将稀疏且带噪的点云重建为密集且几何一致的表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法训练成本高、推理延迟高和泛化能力差的问题，实现准确性和效率的平衡。&lt;h4&gt;方法&lt;/h4&gt;提出MSDNet框架，包括两个阶段：1)重建引导的特征蒸馏，通过特征重建对齐和加密学生特征；2)扩散引导的特征蒸馏，通过轻量级扩散网络优化特征。此外引入噪声适配器，自适应对齐噪声级别与扩散时间步。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和内部数据集上的实验表明，MSDNet实现了高保真重建和低延迟推理，并在下游任务中持续提高性能。&lt;h4&gt;结论&lt;/h4&gt;MSDNet成功平衡了准确性和效率，代码将在发表后公开。&lt;h4&gt;翻译&lt;/h4&gt;4D雷达超分辨率旨在将稀疏且带噪的点云重建为密集且几何一致的表示，是自动驾驶感知中的一个基础问题。然而，现有方法往往存在训练成本高或依赖复杂的基于扩散的采样，导致推理延迟高和泛化能力差，难以平衡准确性和效率。为解决这些限制，我们提出MSDNet，一个多阶段蒸馏框架，有效将密集LiDAR先验知识转移到4D雷达特征上，实现高质量重建和计算效率。第一阶段执行重建引导的特征蒸馏，通过特征重建对齐和加密学生特征。在第二阶段，我们提出扩散引导的特征蒸馏，将第一阶段蒸馏的特征视为教师表示的噪声版本，并通过轻量级扩散网络进行优化。此外，我们引入噪声适配器，自适应地将特征的噪声级别与预定义的扩散时间步对齐，实现更精确的降噪。在VoD和内部数据集上的大量实验表明，MSDNet在4D雷达点云超分辨率任务中实现了高保真重建和低延迟推理，并在下游任务中持续提高性能。代码将在发表后公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D雷达点云超分辨率问题，即如何将稀疏、有噪声的4D雷达点云重建为密集、几何一致的表示。这个问题在自动驾驶领域非常重要，因为4D雷达虽然能在恶劣天气条件下稳定工作，但其点云质量有限，严重影响了物体检测、场景理解和定位等精细感知任务的性能。当前方法要么训练成本高，要么推理延迟大且泛化能力差，难以平衡准确性和效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：信号级方法数据收集成本高且泛化能力有限，而基于扩散的方法（如R2LDM）虽然效果好但推理延迟高。作者借鉴了知识蒸馏的思想，将其应用于4D雷达超分辨率任务，设计了两个渐进阶段的蒸馏框架：第一阶段通过重建引导特征蒸馏（RGFD）进行初步知识转移，第二阶段通过扩散引导特征蒸馏（DGFD）进行细化。作者还引入了噪声适配器来精确对齐噪声水平，提高扩散效率。这种方法结合了知识蒸馏和扩散模型的优点，避免了各自的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多阶段知识蒸馏，将高分辨率LiDAR的几何先验高效转移到稀疏的4D雷达特征中，实现高质量且高效的点云超分辨率。整体流程包括：1) 使用VoxelNet提取LiDAR和4D雷达的BEV特征；2) 构建教师网络，通过S2D模块增强LiDAR特征并重建点云；3) 第一阶段RGFD通过特征重建网络将稀疏4D雷达特征转换为密集表示，并最小化与LiDAR特征的差异；4) 第二阶段DGFD将第一阶段结果视为噪声，通过噪声适配器和轻量级扩散网络进行去噪；5) 将去噪特征输入点云重建模块生成密集4D雷达点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次将知识蒸馏应用于4D雷达超分辨率，提出多阶段蒸馏框架；2) 设计重建引导特征蒸馏（RGFD）解决跨模态对齐挑战；3) 提出扩散引导特征蒸馏（DGFD）使用轻量级扩散网络进行特征细化；4) 引入噪声适配器自适应对齐噪声水平。相比之前工作，MSDNet避免了信号级方法的高数据成本和基于扩散方法的高推理延迟，通过两阶段蒸馏实现了高质量和效率的平衡。实验表明，MSDNet在保持高重建质量的同时，推理速度比R2LDM快89.6%，参数量减少一半。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MSDNet通过多阶段知识蒸馏框架，高效地将LiDAR的密集几何先验转移到4D雷达特征中，实现了高质量且低延迟的4D雷达点云超分辨率，显著提升了下游任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 4D radar super-resolution, which aims to reconstruct sparse and noisy pointclouds into dense and geometrically consistent representations, is afoundational problem in autonomous perception. However, existing methods oftensuffer from high training cost or rely on complex diffusion-based sampling,resulting in high inference latency and poor generalization, making itdifficult to balance accuracy and efficiency. To address these limitations, wepropose MSDNet, a multi-stage distillation framework that efficiently transfersdense LiDAR priors to 4D radar features to achieve both high reconstructionquality and computational efficiency. The first stage performsreconstruction-guided feature distillation, aligning and densifying thestudent's features through feature reconstruction. In the second stage, wepropose diffusion-guided feature distillation, which treats the stage-onedistilled features as a noisy version of the teacher's representations andrefines them via a lightweight diffusion network. Furthermore, we introduce anoise adapter that adaptively aligns the noise level of the feature with apredefined diffusion timestep, enabling a more precise denoising. Extensiveexperiments on the VoD and in-house datasets demonstrate that MSDNet achievesboth high-fidelity reconstruction and low-latency inference in the task of 4Dradar point cloud super-resolution, and consistently improves performance ondownstream tasks. The code will be publicly available upon publication.</description>
      <author>example@mail.com (Minqing Huang, Shouyi Lu, Boyuan Zheng, Ziyao Li, Xiao Tang, Guirong Zhuo)</author>
      <guid isPermaLink="false">2509.13149v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Weakly and Self-Supervised Class-Agnostic Motion Prediction for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.13116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  An extension of our CVPR 2023 paper, "Weakly Supervised  Class-Agnostic Motion Prediction for Autonomous Driving," accepted for  publication in TPAMI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从激光雷达点云中进行弱自监督类别无关运动预测的新方法，通过利用场景中的前景/背景或非地面/地面线索作为监督信号，有效减少了标注需求，同时保持了良好的预测性能。&lt;h4&gt;背景&lt;/h4&gt;动态环境中的运动理解对自动驾驶至关重要，户外场景通常包含移动前景和静态背景，使运动理解与场景解析相关联。&lt;h4&gt;目的&lt;/h4&gt;开发一种弱监督和自监督的类别无关运动预测方法，减少对运动标注的依赖，同时保持或提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;1) 提出用前景/背景掩码替代运动标注进行监督；2) 利用非地面/地面掩码作为前景/背景掩码的替代，进一步减少标注；3) 设计需要更少标注的弱监督方法和完全无标注的自监督方法；4) 开发鲁棒一致性感知Chamfer距离损失函数，结合多帧信息和鲁棒惩罚函数抑制异常值。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的弱监督和自监督模型优于现有的自监督对应模型，弱监督模型的性能甚至可以与一些监督模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该方法有效地平衡了标注工作量和预测性能，为自动驾驶中的运动预测提供了一种实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;理解动态环境中的运动对自动驾驶至关重要，从而推动了类别无关运动预测的研究。在本工作中，我们研究了从激光雷达点云中进行弱自监督类别无关运动预测。户外场景通常包含移动前景和静态背景，使运动理解能够与场景解析相关联。基于这一观察，我们提出了一种新的弱监督范式，用完全或部分标注的前景/背景掩码（1%、0.1%）替代运动标注进行监督。为此，我们开发了一种利用前景/背景线索指导运动预测模型自监督学习的弱监督方法。由于前景运动通常发生在非地面区域，非地面/地面掩码可以作为前景/背景掩码的替代，进一步减少标注工作量。利用非地面/地面线索，我们提出了两种额外方法：一种需要更少（0.01%）前景/背景标注的弱监督方法，以及一种无需标注的自监督方法。此外，我们设计了一种鲁棒一致性感知Chamfer距离损失，结合多帧信息和鲁棒惩罚函数，以抑制自监督学习中的异常值。实验表明，我们的弱监督和自监督模型优于现有的自监督对应模型，我们的弱监督模型甚至可以与一些监督模型相媲美。这证明了我们的方法有效地平衡了标注工作量和性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶中运动预测对昂贵运动标注数据的依赖问题。在现实中，运动数据的获取成本高且困难，而准确理解动态环境中的运动对确保自动驾驶安全至关重要。传统方法需要大量标注数据，限制了其在开放场景和未知物体类别上的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到户外场景可分解为移动前景和静态背景，从而将运动理解与场景解析联系起来。他们借鉴了现有类别无关运动预测(如MotionNet、BE-STI)和弱监督/自监督方法(如SSMP、SelfMotion)的工作，创新性地提出用前景/背景掩码替代运动标注作为弱监督信号。此外，他们利用前景运动通常发生在非地面区域的特性，进一步用非地面/地面掩码减少标注需求，并设计了鲁棒一致性感知Chamfer Distance损失函数提高自监督学习效果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用场景解析信息(前景/背景或非地面/地面掩码)替代昂贵的运动标注作为监督信号。整体实现流程包括：1)预训练前景/背景分割网络(PreSegNet)或通过平面拟合生成非地面/地面点；2)训练运动预测网络(WeakMotionNet或SelfMotionNet)，包含运动预测头和辅助分割头；3)使用鲁棒一致性感知Chamfer Distance损失函数在前景/非地面点上进行自监督学习；4)同时用少量前景/背景掩码训练辅助分割头。最终实现从弱监督(0.01%标注)到完全无监督的高效运动预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出使用前景/背景掩码作为弱监督进行类别无关运动预测；2)创新性地用非地面/地面掩码替代前景/背景掩码，进一步减少标注需求；3)设计鲁棒一致性感知Chamfer Distance损失函数，利用多帧信息和鲁棒惩罚函数提高自监督学习的鲁棒性；4)提出三种方法：WeakMotion-FB(使用前景/背景掩码)、WeakMotion-NG(使用非地面/地面掩码，需极少标注)和SelfMotion-NG(完全无监督)。相比之前工作，本文方法显著降低了标注依赖(从1%运动标注降至0.01%前景/背景掩码)，同时性能优于或媲美全监督方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的弱监督和无监督学习范式，通过利用场景解析信息替代昂贵的运动标注，实现了高效的类别无关运动预测，显著降低了自动驾驶系统中运动预测模型的标注依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3604036&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding motion in dynamic environments is critical for autonomousdriving, thereby motivating research on class-agnostic motion prediction. Inthis work, we investigate weakly and self-supervised class-agnostic motionprediction from LiDAR point clouds. Outdoor scenes typically consist of mobileforegrounds and static backgrounds, allowing motion understanding to beassociated with scene parsing. Based on this observation, we propose a novelweakly supervised paradigm that replaces motion annotations with fully orpartially annotated (1%, 0.1%) foreground/background masks for supervision. Tothis end, we develop a weakly supervised approach utilizingforeground/background cues to guide the self-supervised learning of motionprediction models. Since foreground motion generally occurs in non-groundregions, non-ground/ground masks can serve as an alternative toforeground/background masks, further reducing annotation effort. Leveragingnon-ground/ground cues, we propose two additional approaches: a weaklysupervised method requiring fewer (0.01%) foreground/background annotations,and a self-supervised method without annotations. Furthermore, we design aRobust Consistency-aware Chamfer Distance loss that incorporates multi-frameinformation and robust penalty functions to suppress outliers inself-supervised learning. Experiments show that our weakly and self-supervisedmodels outperform existing self-supervised counterparts, and our weaklysupervised models even rival some supervised ones. This demonstrates that ourapproaches effectively balance annotation effort and performance.</description>
      <author>example@mail.com (Ruibo Li, Hanyu Shi, Zhe Wang, Guosheng Lin)</author>
      <guid isPermaLink="false">2509.13116v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MATTER: Multiscale Attention for Registration Error Regression</title>
      <link>http://arxiv.org/abs/2509.12924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于回归而非分类的点云配准质量验证方法，通过多尺度特征提取和注意力聚合机制，实现了对配准质量的更细粒度量化，并在多样化数据集上表现出色，特别是在处理具有异构空间密度的点云时。&lt;h4&gt;背景&lt;/h4&gt;点云配准(PCR)对于同时定位与地图构建(SLAM)和目标跟踪等许多下游任务至关重要，因此检测和量化配准错位(即PCR质量验证)是一个重要任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于回归的PCR质量验证方法，替代现有的基于分类的方法，实现更精细的配准质量量化；并通过多尺度特征提取和注意力聚合扩展特征表示，提高方法在异构密度点云上的表现。&lt;h4&gt;方法&lt;/h4&gt;使用回归而非分类进行PCR质量验证；采用多尺度提取和基于注意力的聚合来扩展与错位相关的特征；在多样化数据集上评估方法的准确性和鲁棒性；将方法应用于指导地图构建下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在多样化数据集上提供了准确且稳健的配准误差估计；对于具有异构空间密度的点云尤其有效；当用于指导地图构建下游任务时，与最先进的基于分类的方法相比，可以在给定数量的重新配准帧数量下显著提高地图质量。&lt;h4&gt;结论&lt;/h4&gt;基于回归的PCR质量验证方法比现有的基于分类的方法更有效；通过多尺度特征提取和注意力聚合实现了更精细的质量量化；在实际应用中，该方法可以优化地图构建过程，提高地图质量。&lt;h4&gt;翻译&lt;/h4&gt;点云配准(PCR)对于许多下游任务至关重要，例如同时定位与地图构建(SLAM)和目标跟踪。这使得检测和量化配准错位，即PCR质量验证，成为一个重要任务。所有现有方法都将验证视为分类任务，旨在将PCR质量分配到几个类别中。在本工作中，我们使用回归进行PCR验证，允许对配准质量进行更细粒度的量化。我们还通过使用多尺度提取和基于注意力的聚合来扩展先前使用的与错位相关的特征。这导致在多样化数据集上准确且稳健的配准误差估计，特别是对于具有异构空间密度的点云。此外，当用于指导地图构建下游任务时，与最先进的基于分类的方法相比，我们的方法在给定数量的重新配准帧数量下显著提高了地图质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云配准(PCR)质量评估的问题，特别是检测和量化配准误差。这个问题在现实中非常重要，因为点云配准是SLAM、3D重建和机器人导航等下游任务的基础。配准误差会传播到下游模块，导致地图扭曲或导航失败。在挑战性场景下，如优化陷入局部最小值、运动失真或测量噪声时，配准误差仍然显著，因此需要准确评估配准质量以便采取纠正措施。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即它们都是基于分类的，只能粗略地将错位分为几个级别。作者借鉴了FACT等工作的特征提取方法，但将其从分类改为回归任务。作者注意到单尺度特征的局限性，特别是小半径在初始对齐差时可能失败，而大半径包含非重叠区域使熵和Sinkhorn散度不可靠。因此，作者设计了多尺度注意力机制，在三个不同半径(7.5m、4.0m、2.5m)上提取特征，并通过注意力机制自适应地选择最适合的尺度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将点云错位评估从分类任务转变为回归任务，实现更细粒度的配准质量评估，并使用多尺度注意力机制来融合不同几何尺度的特征，提高对点云空间密度异质性的鲁棒性。整体实现流程包括：1)特征提取：为每个锚点计算局部特征(分别和联合的微分熵、Sinkhorn散度、覆盖率比)和全局特征(共视分数、距离、源标志)；2)多尺度特征提取：在三个不同半径尺度上提取特征；3)多尺度交叉注意力：使用查询学习MLP生成查询，通过多头注意力机制聚合多尺度特征；4)回归预测：使用PointTransformer和MLP预测对齐误差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)任务转换：将点云错位分类扩展为点云错位回归，实现更细粒度的评估；2)多尺度注意力机制：提取多尺度几何特征并自适应选择最适合的尺度；3)鲁棒性增强：特别处理点云中空间密度异质性的情况。相比之前的工作，MATTER提供连续的误差预测而非离散分类，使用多尺度注意力而非固定尺度特征，实验表明在多个数据集上均优于现有方法，特别是在下游任务中能更好地指导重配准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MATTER通过引入多尺度注意力机制将点云配准质量评估从分类转变为回归任务，实现了更精确、更鲁棒的配准误差估计，特别是在处理空间密度异质的点云时表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration (PCR) is crucial for many downstream tasks, such assimultaneous localization and mapping (SLAM) and object tracking. This makesdetecting and quantifying registration misalignment, i.e.,~{\it PCR qualityvalidation}, an important task. All existing methods treat validation as aclassification task, aiming to assign the PCR quality to a few classes. In thiswork, we instead use regression for PCR validation, allowing for a morefine-grained quantification of the registration quality. We also extendpreviously used misalignment-related features by using multiscale extractionand attention-based aggregation. This leads to accurate and robust registrationerror estimation on diverse datasets, especially for point clouds withheterogeneous spatial densities. Furthermore, when used to guide a mappingdownstream task, our method significantly improves the mapping quality for agiven amount of re-registered frames, compared to the state-of-the-artclassification-based method.</description>
      <author>example@mail.com (Shipeng Liu, Ziliang Xiong, Khac-Hoang Ngo, Per-Erik Forssén)</author>
      <guid isPermaLink="false">2509.12924v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Few to Big: Prototype Expansion Network via Diffusion Learner for Point Cloud Few-shot Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.12878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种原型扩展网络(PENet)，用于解决少样本3D点云语义分割中的类内多样性和集合间不一致性问题。该方法利用扩散模型的条件编码器提供可泛化特征，通过双流学习器架构构建大容量原型，并通过原型同化模块和原型校准机制优化原型表示。&lt;h4&gt;背景&lt;/h4&gt;少样本3D点云语义分割旨在使用少量标注的支持样本分割新类别。现有的基于原型的方法虽然有一定效果，但面临类内多样性不足和集合间不一致性两大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于原型的方法在少样本3D点云语义分割中面临的类内多样性和集合间不一致性问题，提高对新类别的分割性能。&lt;h4&gt;方法&lt;/h4&gt;提出原型扩展网络(PENet)，采用双流学习器架构：保留传统的全监督内在学习器提取代表性特征，同时引入扩散学习器提供丰富的可泛化特征。双原型通过原型同化模块处理，采用推拉交叉引导注意力块迭代对齐原型与查询空间，并通过原型校准机制防止语义漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在S3DIS和ScanNet数据集上的实验表明，PENet在各种少样本设置下显著优于最先进的方法。扩散模型提供的可泛化特征能有效扩展原型的表示能力，解决类内多样性问题；原型同化模块和校准机制能有效解决集合间不一致性问题。&lt;h4&gt;结论&lt;/h4&gt;原型扩展网络(PENet)通过结合传统监督学习和扩散模型的可泛化特征，有效解决了少样本3D点云语义分割中的关键挑战，显著提高了分割性能。&lt;h4&gt;翻译&lt;/h4&gt;少样本3D点云语义分割旨在使用少量标注的支持样本分割新类别。虽然现有的基于原型的方法显示出潜力，但它们受到两个关键挑战的限制：(1)类内多样性，原型的有限表示能力无法覆盖类的全部变化；(2)集合间不一致性，从支持集导出的原型与查询特征空间不匹配。受扩散模型强大生成能力的启发，我们重新利用其预训练条件编码器为原型提供一种新的可泛化特征来源，以扩展原型的表示范围。在此设置下，我们引入了原型扩展网络(PENet)，这是一个从两个互补特征源构建大容量原型的框架。PENet采用双流学习器架构：它保留传统的全监督内在学习器(IL)来提取代表性特征，同时引入新的扩散学习器(DL)来提供丰富的可泛化特征。然后，双原型通过原型同化模块(PAM)处理，该模块采用新颖的推拉交叉引导注意力块迭代地将原型与查询空间对齐。此外，原型校准机制(PCM)正则化最终的大容量原型以防止语义漂移。在S3DIS和ScanNet数据集上的大量实验表明，PENet在各种少样本设置下显著优于最先进的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决少样本3D点云语义分割中的两个关键挑战：类内多样性（原型的有限表示能力无法覆盖一个类的全部变化）和集合间不一致性（支持集原型与查询特征空间不对齐）。这个问题在现实中很重要，因为3D点云语义分割在自动驾驶、机器人和增强现实等领域有广泛应用，而完全监督学习需要大量昂贵的标注数据。少样本学习方法使模型仅通过少量标注样本就能泛化到新类别，大大减少了对标注数据的依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析认识到现有基于原型的方法面临相互关联的挑战：容量小的原型即使对齐也无法覆盖类的全部变化，而容量大的原型如果不对齐则无法在查询空间中有效工作。因此，他们提出通过'原型扩展'来增加原型的表示能力。他们借鉴了元学习框架，利用扩散模型的生成能力重新调整其预训练条件编码器，并采用双流架构结合传统监督学习和自监督学习。具体设计包括保留内在学习者提取代表性特征，引入扩散学习者提供可泛化特征，设计原型同化模块迭代对齐原型，以及引入原型校准机制防止语义漂移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建'大容量原型'来解决少样本3D点云语义分割中的挑战，从两个互补特征源构建原型：一个来自传统监督学习（提供代表性特征），另一个来自扩散模型（提供可泛化特征）。整体流程包括：1)双流特征提取（内在学习者和扩散学习者）；2)从支持集特征生成初始原型；3)通过原型同化模块迭代对齐双原型与查询空间；4)将对齐后的原型融合成最终的大容量原型；5)使用原型校准机制防止语义漂移；6)用最终原型对查询点云进行语义分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)原型扩展网络框架，首次将扩散模型应用于少样本3D点云语义分割；2)双流学习架构，结合传统监督学习和扩散模型特征；3)原型同化模块，采用推-拉交叉引导注意力机制迭代对齐原型；4)原型校准机制，防止语义漂移。相比之前工作的不同在于：大多数方法依赖单一特征源，而本文从两个互补特征源构建原型；传统方法采用单步对齐，本文采用迭代推-拉机制；本文首次利用扩散模型条件编码器作为特征提取器；现有方法通过增加原型数量而非扩展单个原型容量来解决多样性问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了原型扩展网络（PENet），通过融合来自传统监督学习和扩散模型的互补特征构建大容量原型，有效解决了少样本3D点云语义分割中的类内多样性和集合间不一致性问题，显著提升了模型在未见类别上的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud semantic segmentation aims to segment novelcategories using a minimal number of annotated support samples. While existingprototype-based methods have shown promise, they are constrained by twocritical challenges: (1) Intra-class Diversity, where a prototype's limitedrepresentational capacity fails to cover a class's full variations, and (2)Inter-set Inconsistency, where prototypes derived from the support set aremisaligned with the query feature space. Motivated by the powerful generativecapability of diffusion model, we re-purpose its pre-trained conditionalencoder to provide a novel source of generalizable features for expanding theprototype's representational range. Under this setup, we introduce thePrototype Expansion Network (PENet), a framework that constructs big-capacityprototypes from two complementary feature sources. PENet employs a dual-streamlearner architecture: it retains a conventional fully supervised IntrinsicLearner (IL) to distill representative features, while introducing a novelDiffusion Learner (DL) to provide rich generalizable features. The resultingdual prototypes are then processed by a Prototype Assimilation Module (PAM),which adopts a novel push-pull cross-guidance attention block to iterativelyalign the prototypes with the query space. Furthermore, a Prototype CalibrationMechanism (PCM) regularizes the final big capacity prototype to preventsemantic drift. Extensive experiments on the S3DIS and ScanNet datasetsdemonstrate that PENet significantly outperforms state-of-the-art methodsacross various few-shot settings.</description>
      <author>example@mail.com (Qianguang Zhao, Dongli Wang, Yan Zhou, Jianxun Li, Richard Irampa)</author>
      <guid isPermaLink="false">2509.12878v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>A-TDOM: Active TDOM via On-the-Fly 3DGS</title>
      <link>http://arxiv.org/abs/2509.12759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种称为A-TDOM的近实时真正数字正射地图生成方法，基于即时3D高斯优化技术，能够在获取每张图像后快速计算姿态和稀疏点云，并将新的高斯函数集成和优化到先前未看到或粗略重建的区域，实现近实时渲染。&lt;h4&gt;背景&lt;/h4&gt;真正数字正射地图(TDOM)是城市管理、城市规划、土地测量等领域的关键地理空间产品。然而，传统TDOM生成方法依赖复杂的离线摄影测量流程，导致延迟阻碍了实时应用，且因摄像机姿态不准确、数字表面模型和场景遮挡等问题，质量可能下降。&lt;h4&gt;目的&lt;/h4&gt;解决传统TDOM生成方法的延迟问题，实现近实时生成，同时保持可接受的渲染质量和几何精度。&lt;h4&gt;方法&lt;/h4&gt;提出A-TDOM方法，基于即时3D高斯优化：每获取一张图像后，通过即时SfM计算姿态和稀疏点云；将新的高斯函数集成并优化到先前未看到或粗略重建的区域；结合正交渲染，在每次更新3D高斯场后立即渲染。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上的实验表明，A-TDOM能够在近实时主动渲染TDOM，每张新图像的3D高斯优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;结论&lt;/h4&gt;A-TDOM方法有效解决了传统TDOM生成方法的延迟问题，通过即时3D高斯优化实现了近实时生成，同时保持了良好的质量和精度，为需要实时TDOM的应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;真正数字正射地图(TDOM)是城市管理、城市规划、土地测量等各个领域中的关键地理空间产品。然而，传统的TDOM生成方法通常依赖于复杂的离线摄影测量流程，导致延迟阻碍了实时应用。此外，由于摄像机姿态不准确、数字表面模型(DSM)和场景遮挡等挑战，TDOM的质量可能会下降。为了解决这些挑战，本文介绍了A-TDOM，一种基于即时3D高斯优化的近实时TDOM生成方法。每获取一张图像，其姿态和稀疏点云通过即时SfM计算。然后新的高斯函数被集成并优化到先前未看到或粗略重建的区域。通过与正交渲染结合，A-TDOM可以在每次更新3D高斯场后立即渲染。在多个基准测试上的初步实验表明，所提出的A-TDOM能够在近实时主动渲染TDOM，每张新图像的3D高斯优化仅需几秒钟，同时保持可接受的渲染质量和TDOM几何精度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统TDOM（真正的数字正射影像图）生成方法的实时性问题。传统方法依赖复杂的离线摄影测量流程，导致处理延迟，无法满足实时应用需求，同时还存在相机姿态不准确、数字表面模型和场景遮挡等问题影响质量。这个问题在现实中非常重要，因为TDOM是城市管理、城市规划、土地测量等领域的关键地理空间产品，实时生成TDOM可以大大提高工作效率，支持灾害监测、城市规划动态调整、土地资源调查等场景中的及时决策。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有TDOM生成方法的局限性，包括传统方法计算成本高和新兴方法仍采用离线处理。作者借鉴了多项现有工作：基于On-the-Fly SfM进行实时姿态估计、利用3D高斯投影技术进行场景表示、参考Tortho-Gaussian和Ortho-3DGS等3DGS-based方法，以及Gaussian On-the-Fly Splatting的优化策略。作者的创新思路是将这些技术整合到一个在线框架中，通过即时更新和优化3DGS场，实现近实时TDOM生成，并设计了高斯采样和集成方法替代原始3DGS中的密集化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; A-TDOM方法的核心思想是通过即时更新和优化3D高斯投影场(3DGS)，实现近实时的TDOM生成，无需依赖数字表面模型(DSM)和显式遮挡检测。整体流程是：1)初始化训练一个初始3DGS场；2)对于每张新图像：a)使用On-the-Fly SfM计算姿态并更新稀疏点云；b)基于稀疏点云重投影进行Delaunay三角剖分，创建掩码确定关键优化区域；c)比较当前3DGS场渲染与输入图像的梯度图，识别需要额外高斯集成的区域；d)在选定区域内采样点，生成新高斯函数并集成；e)优化更新后的3DGS场；f)通过正交投影生成更新的TDOM。这一流程允许系统在图像采集过程中持续更新3D表示并实时生成高质量TDOM。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)近实时TDOM生成框架，首次实现近实时生成；2)高斯采样和集成方法，替代原始3DGS密集化策略；3)关键区域确定机制，通过Delaunay三角掩码确定优化区域；4)自适应优化策略，包括自适应学习率更新和局部优化；5)基于投影矩阵修改的正交投影方法。相比之前的工作，A-TDOM的主要不同在于：将离线处理转变为在线近实时处理；无需依赖DSM和显式遮挡检测；通过高斯采样和集成更有效地扩展3DGS场；采用自适应优化加速处理；实现了图像采集与TDOM生成的同步进行。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; A-TDOM通过创新的即时3D高斯投影优化和正交投影技术，首次实现了无需数字表面模型和遮挡检测的近实时真正的数字正射影像图生成，显著提高了地理空间信息获取的效率和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True Digital Orthophoto Map (TDOM) serves as a crucial geospatial product invarious fields such as urban management, city planning, land surveying, etc.However, traditional TDOM generation methods generally rely on a complexoffline photogrammetric pipeline, resulting in delays that hinder real-timeapplications. Moreover, the quality of TDOM may degrade due to variouschallenges, such as inaccurate camera poses or Digital Surface Model (DSM) andscene occlusions. To address these challenges, this work introduces A-TDOM, anear real-time TDOM generation method based on On-the-Fly 3DGS optimization. Aseach image is acquired, its pose and sparse point cloud are computed viaOn-the-Fly SfM. Then new Gaussians are integrated and optimized into previouslyunseen or coarsely reconstructed regions. By integrating with orthogonalsplatting, A-TDOM can render just after each update of a new 3DGS field.Initial experiments on multiple benchmarks show that the proposed A-TDOM iscapable of actively rendering TDOM in near real-time, with 3DGS optimizationfor each new image in seconds while maintaining acceptable rendering qualityand TDOM geometric accuracy.</description>
      <author>example@mail.com (Yiwei Xu, Xiang Wang, Yifei Yu, Wentian Gan, Luca Morelli, Giulio Perda, Xiongwu Xiao, Zongqian Zhan, Xin Wang, Fabio Remondino)</author>
      <guid isPermaLink="false">2509.12759v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DisorientLiDAR: Physical Attacks on LiDAR-based Localization</title>
      <link>http://arxiv.org/abs/2509.12595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DisorientLiDAR的新型对抗攻击框架，针对基于激光雷达的定位系统。通过逆向工程定位模型，攻击者可以识别并移除关键点，从而破坏自动驾驶汽车的定位能力。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型容易受到视觉上难以察觉的扰动的对抗攻击，这对自动驾驶汽车的定位构成严重安全挑战。然而，针对定位的攻击探索很少，因为大多数对抗攻击应用于3D感知而非定位系统。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对基于激光雷达(LiDAR)的定位系统的对抗攻击方法，以揭示自动驾驶定位系统的安全漏洞。&lt;h4&gt;方法&lt;/h4&gt;提出DisorientLiDAR攻击框架，通过逆向工程定位模型(如特征提取网络)识别关键点，策略性地移除这些关键点区域来破坏基于激光雷达的定位。实验在KITTI数据集上使用三种点云配准模型(HRegNet、D3Feat和GeoTransformer)进行评估，并在Autoware自动驾驶平台和物理世界中验证了攻击效果。&lt;h4&gt;主要发现&lt;/h4&gt;移除包含Top-K关键点的区域显著降低了点云配准模型的精度；在Autoware平台上，隐藏几个关键区域会导致明显的定位漂移；使用近红外吸收材料在物理世界中隐藏关键区域可以复制数字攻击效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的DisorientLiDAR攻击框架成功展示了自动驾驶定位系统的脆弱性，通过物理世界的攻击验证了方法的真实性和通用性，为提高自动驾驶系统的安全性提供了重要参考。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型已被证明容易受到具有视觉上难以察觉的扰动的对抗攻击。即使这对自动驾驶汽车的定位构成了严重的安全挑战，但针对它的攻击探索很少，因为大多数对抗攻击已被应用于3D感知。在这项工作中，我们提出了一种名为DisorientLiDAR的新型对抗攻击框架，针对基于激光雷达的定位。通过逆向工程定位模型(例如特征提取网络)，攻击者可以识别关键点并策略性地移除它们，从而破坏基于激光雷达的定位。我们的提案首先使用KITTI数据集在三种最先进的点云配准模型(HRegNet、D3Feat和GeoTransformer)上进行了评估。实验结果表明，移除包含Top-K关键点的区域显著降低了它们的配准精度。我们进一步验证了攻击对Autoware自动驾驶平台的影响，在那里隐藏仅几个关键区域就会引起明显的定位漂移。最后，我们通过使用近红外吸收材料隐藏关键区域，将攻击扩展到物理世界，从而成功复制了在KITTI数据中观察到的攻击效果。这一步骤更接近真实的物理世界攻击，证明了我们提案的真实性和通用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究如何对基于LiDAR的自动驾驶汽车定位系统进行物理攻击。这个问题非常重要，因为自动驾驶汽车的安全高度依赖准确定位，而定位错误可能导致严重事故。尽管已有研究证明深度学习模型对对抗攻击很脆弱，但对定位系统的攻击探索很少，且即使有GPS和IMU支持，这种攻击仍然有效，对自动驾驶安全构成重大威胁。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有的点云注册方法，发现它们都依赖检测和匹配关键几何特征区域。基于这一观察，作者设计了一种攻击策略：通过逆向工程定位模型识别关键点，然后使用红外吸收材料物理覆盖这些区域，使它们从LiDAR扫描中'消失'。作者借鉴了现有的LiDAR对抗攻击工作，但与之前需要物理访问LiDAR传感器的方法不同，这种方法只需在道路旁放置材料即可实施，更加隐蔽且易于部署。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是识别并移除对LiDAR定位系统最关键的几何特征区域，干扰点云匹配过程导致定位错误。整体流程包括：1)使用与受害者相同的定位模型提取高置信度关键点；2)选择Top-K最显著的关键点；3)用红外吸收材料覆盖这些关键区域对应的物理位置；4)当受害车辆经过时，其LiDAR无法获取这些区域数据，导致定位错误；5)评估攻击效果。为确保攻击可行性和隐蔽性，作者还提出了高度过滤、轨迹接近检查、重叠处理和埋伏工具放置等筛选步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出首个通过物理隐藏关键区域干扰定位系统的攻击框架；无需物理访问LiDAR传感器；在多种点云注册模型上验证了攻击通用性；在真实世界车辆和商业平台上进行了物理攻击验证；系统化评估了攻击参数影响。相比之前工作，本文专注于定位系统而非感知模块；通过物理遮挡而非激光注入实施攻击；在多种环境和LiDAR配置上评估；同时提出了防御策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出并验证了一种通过物理遮挡关键区域来干扰基于LiDAR的自动驾驶定位系统的攻击方法，揭示了定位系统的重要安全漏洞，并为开发更安全的定位系统提供了方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have been shown to be susceptible to adversarial attackswith visually imperceptible perturbations. Even this poses a serious securitychallenge for the localization of self-driving cars, there has been very littleexploration of attack on it, as most of adversarial attacks have been appliedto 3D perception. In this work, we propose a novel adversarial attack frameworkcalled DisorientLiDAR targeting LiDAR-based localization. Byreverse-engineering localization models (e.g., feature extraction networks),adversaries can identify critical keypoints and strategically remove them,thereby disrupting LiDAR-based localization. Our proposal is first evaluated onthree state-of-the-art point-cloud registration models (HRegNet, D3Feat, andGeoTransformer) using the KITTI dataset. Experimental results demonstrate thatremoving regions containing Top-K keypoints significantly degrades theirregistration accuracy. We further validate the attack's impact on the Autowareautonomous driving platform, where hiding merely a few critical regions inducesnoticeable localization drift. Finally, we extended our attacks to the physicalworld by hiding critical regions with near-infrared absorptive materials,thereby successfully replicate the attack effects observed in KITTI data. Thisstep has been closer toward the realistic physical-world attack thatdemonstrate the veracity and generality of our proposal.</description>
      <author>example@mail.com (Yizhen Lao, Yu Zhang, Ziting Wang, Chengbo Wang, Yifei Xue, Wanpeng Shao)</author>
      <guid isPermaLink="false">2509.12595v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Axis-Aligned 3D Stalk Diameter Estimation from RGB-D Imagery</title>
      <link>http://arxiv.org/abs/2509.12511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于几何感知的计算机视觉流水线，用于从RGB-D图像中估计茎直径，旨在解决传统测量方法劳动强度大、易出错的问题，为作物育种提供高通量表型分析解决方案。&lt;h4&gt;背景&lt;/h4&gt;准确的高通量表型分析是现代作物育种项目的关键组成部分，特别是对于改善机械稳定性、生物量生产和抗病性等性状。茎直径是一个关键的结构性状，但传统测量方法劳动密集、容易出错，不适合可扩展的表型分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确、高效测量茎直径的方法，支持作物育种和农学研究中的高通量表型分析。&lt;h4&gt;方法&lt;/h4&gt;提出一种几何感知的计算机视觉流水线，结合基于深度学习的实例分割、3D点云重建和通过主成分分析(PCA)的轴对齐切片，以执行鲁棒的直径估计。&lt;h4&gt;主要发现&lt;/h4&gt;通过减轻曲率、遮挡和图像噪声的影响，该方法提供了可扩展且可靠的解决方案，支持育种和农学研究中的高通量表型分析。&lt;h4&gt;结论&lt;/h4&gt;该几何感知的计算机视觉方法为茎直径测量提供了一种高效、准确的解决方案，克服了传统方法的局限性，为作物育种提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;准确的高通量表型分析是现代作物育种项目的关键组成部分，特别是对于改善机械稳定性、生物量生产和抗病性等性状。茎直径是一个关键的结构性状，但传统测量方法劳动密集、容易出错，不适合可扩展的表型分析。在本文中，我们提出了一种几何感知的计算机视觉流水线，用于从RGB-D图像中估计茎直径。我们的方法结合了基于深度学习的实例分割、3D点云重建和通过主成分分析(PCA)的轴对齐切片，以执行鲁棒的直径估计。通过减轻曲率、遮挡和图像噪声的影响，这种方法为育种和农学研究中的高通量表型分析提供了可扩展且可靠的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从RGB-D图像中自动估计植物茎秆直径的问题。这个问题很重要，因为茎秆直径影响作物的抗倒伏能力、生物量分配、水力功能和疾病易感性，是作物表型分析的关键指标。传统测量方法劳动密集且不适合高通量表型分析，而随着2050年全球人口预计达到97亿，自动化测量对确保粮食安全和提高农业生产力至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统测量方法的局限性，以及RGB-D传感器在自动化测量中的潜力。他们注意到现有方法（如颜色阈值、边缘检测）假设茎秆垂直对齐，限制了可靠性；更先进的方法虽改善了检测，但缺乏稳健的轴对齐。作者借鉴了深度学习在农业环境中的应用，结合了基于深度学习的实例隔离、3D点云重建和PCA轴估计，设计出能处理茎杆曲率和任意方向的轴对齐测量方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过确定茎杆的真实主轴方向，然后垂直于此轴进行横截面切片来提高测量准确性。整体流程包括：1)使用RGB-D相机采集数据；2)用YOLOv11x-seg模型分割茎杆区域；3)构建3D点云并进行过滤；4)应用PCA确定茎杆主轴；5)垂直于主轴将点云分为100个切片；6)对每个切片应用DBSCAN过滤并使用95百分位数估计半径；7)聚合切片结果得到最终直径估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)几何感知的计算机视觉管道，结合深度学习、3D重建和PCA轴估计；2)轴对齐的横截面测量，提高准确性和鲁棒性；3)能有效处理茎杆曲率和任意方向；4)使用95百分位数圆拟合和1-标准差聚合处理噪声；5)发现初始统计异常值移除过滤器冗余。相比之前工作，不同之处在于传统方法假设垂直对齐，而先进方法缺乏稳健轴对齐；本文方法能处理各种方向，并通过统计方法有效处理传感器噪声。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发了一种基于几何感知的计算机视觉管道，通过结合深度学习实例分割、3D点云重建和轴对齐的横截面切片，实现了从RGB-D图像中高精度、自动化地估计植物茎杆直径，为作物表型分析和农业管理提供了强大的工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, high-throughput phenotyping is a critical component of modern cropbreeding programs, especially for improving traits such as mechanicalstability, biomass production, and disease resistance. Stalk diameter is a keystructural trait, but traditional measurement methods are labor-intensive,error-prone, and unsuitable for scalable phenotyping. In this paper, we presenta geometry-aware computer vision pipeline for estimating stalk diameter fromRGB-D imagery. Our method integrates deep learning-based instance segmentation,3D point cloud reconstruction, and axis-aligned slicing via Principal ComponentAnalysis (PCA) to perform robust diameter estimation. By mitigating the effectsof curvature, occlusion, and image noise, this approach offers a scalable andreliable solution to support high-throughput phenotyping in breeding andagronomic research.</description>
      <author>example@mail.com (Benjamin Vail, Rahul Harsha Cheppally, Ajay Sharda, Sidharth Rai)</author>
      <guid isPermaLink="false">2509.12511v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Artist-Created Mesh Generation from Raw Observation</title>
      <link>http://arxiv.org/abs/2509.12501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种端到端框架，能够从嘈杂或不完整的点云生成高质量艺术家风格的网格，通过将3D点云精炼重新表述为2D修复任务，利用生成模型实现。&lt;h4&gt;背景&lt;/h4&gt;艺术家创建的网格对商业图形管道至关重要，因为它们与动画和纹理工具兼容且渲染效率高。然而，现有方法通常假设输入干净完整或依赖复杂的多阶段管道，限制了它们在现实场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端方法，能够精炼输入点云并直接生成高质量、艺术家风格的网格，适用于现实世界传感器获取的嘈杂或不完整点云数据。&lt;h4&gt;方法&lt;/h4&gt;提出了一种将3D点云精炼重新表述为2D修复任务的方法，从而能够利用强大的生成模型。该方法是一个端到端的框架，直接从原始点云生成高质量网格。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上的初步结果表明，该框架能够生成干净、完整的网格，证明了其在处理现实世界传感器数据方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的端到端框架为从嘈杂或不完整的点云生成高质量艺术家风格网格提供了有效解决方案，适用于商业图形管道和现实应用场景。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种端到端框架，用于从嘈杂或不完整的点云生成艺术家风格的网格，例如由现实世界传感器如LiDAR或移动RGB-D相机捕获的点云。艺术家创建的网格对商业图形管道至关重要，因为它们与动画和纹理工具兼容，并且在渲染中效率高。然而，现有方法通常假设输入干净完整或依赖复杂的多阶段管道，限制了它们在现实场景中的应用。为此，我们提出了一种端到端方法，可以精炼输入点云并直接生成高质量、艺术家风格的网格。我们方法的核心是将3D点云精炼重新表述为2D修复任务，从而能够使用强大的生成模型。在ShapeNet数据集上的初步结果证明了我们的框架在生成干净、完整网格方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从真实世界传感器（如LiDAR或移动RGB-D相机）捕获的嘈杂或不完整点云中生成高质量艺术风格网格的问题。这个问题在现实中很重要，因为艺术风格网格与商业图形管线兼容，适合动画和纹理处理，且渲染效率高，而现有方法往往需要干净完整的输入或复杂的多阶段处理流程，难以应用于真实世界场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，认识到艺术风格网格生成需要干净输入，而真实传感器数据往往嘈杂不完整。他们创新性地将3D点云精炼重新表述为2D修复问题，利用成熟的2D生成模型能力。方法设计包括将3D点云投影到球形地图集、应用扩散模型修复、再映射回3D并生成网格。作者借鉴了Stable Diffusion作为2D生成模型，MeshAnything V2作为网格生成模型，以及将3D转化为2D处理的高斯地图集方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云精炼问题转化为2D修复任务，利用强大的2D生成模型处理3D数据。整体流程分为四步：1)将3D点云通过最优传输映射到球面，再通过等距矩形投影转换为2D地图集；2)使用微调的扩散模型修复不完整的地图集；3)将修复后的地图集映射回3D，生成干净完整的点云；4)将点云输入MeshAnything V2模型生成最终艺术风格网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)端到端框架直接从原始点云生成艺术风格网格；2)首次将3D点云精炼重新表述为2D修复任务；3)使用球形地图集表示保留3D几何信息；4)利用扩散模型进行地图集修复。相比之前工作，不同之处在于：可直接处理嘈杂不完整输入而非假设干净数据；采用统一端到端框架而非多阶段处理；利用成熟的2D生成模型而非专门开发3D模型；直接生成适合商业管线的拓扑结构网格。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新方法，通过将3D点云精炼转化为2D修复任务，实现了从嘈杂或不完整的真实世界传感器数据直接生成高质量艺术风格网格的端到端框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an end-to-end framework for generating artist-style meshes fromnoisy or incomplete point clouds, such as those captured by real-world sensorslike LiDAR or mobile RGB-D cameras. Artist-created meshes are crucial forcommercial graphics pipelines due to their compatibility with animation andtexturing tools and their efficiency in rendering. However, existing approachesoften assume clean, complete inputs or rely on complex multi-stage pipelines,limiting their applicability in real-world scenarios. To address this, wepropose an end-to-end method that refines the input point cloud and directlyproduces high-quality, artist-style meshes. At the core of our approach is anovel reformulation of 3D point cloud refinement as a 2D inpainting task,enabling the use of powerful generative models. Preliminary results on theShapeNet dataset demonstrate the promise of our framework in producing clean,complete meshes.</description>
      <author>example@mail.com (Yao He, Youngjoong Kwon, Wenxiao Cai, Ehsan Adeli)</author>
      <guid isPermaLink="false">2509.12501v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning for 3D point cloud processing - from approaches, tasks to its implications on urban and environmental applications</title>
      <link>http://arxiv.org/abs/2509.12452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  57 Pages, 4 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对点云处理的深度学习方法和数据集进行了元综述，重点关注实际应用价值而非仅关注网络架构。&lt;h4&gt;背景&lt;/h4&gt;点云处理是大地测量学和计算机视觉的基础任务，支持从空中到地面的多种应用，包括测绘、环境监测、城市建模、自动驾驶等。深度学习的发展使基于学习的方法主导了点云处理算法，但这些方法大多尚未转化为实际应用。&lt;h4&gt;目的&lt;/h4&gt;提供点云处理关键任务的深度学习方法和数据集的元综述，关注这些方法在实际应用中的价值和挑战。&lt;h4&gt;方法&lt;/h4&gt;回顾了涵盖场景补全、配准、语义分割和建模等关键任务的深度学习方法和数据集，并通过分析这些任务支持的城市和环境应用，识别方法转化为应用时需要填补的差距。&lt;h4&gt;主要发现&lt;/h4&gt;现有调查主要集中在适应无序点云的网络架构更新上，忽略了典型点云处理应用中的实际价值，如大量数据处理、多样化场景内容、变化的点密度和数据模态等实际考量。&lt;h4&gt;结论&lt;/h4&gt;在算法和实践两方面对所调查的方法得出了结论，强调了点云处理从理论研究向实际应用转化的重要性。&lt;h4&gt;翻译&lt;/h4&gt;点云处理作为大地测量学和计算机视觉领域的基础任务，一直支持着从空中到地面不同规模的任务和应用，包括测绘、环境监测、城市/树木结构建模、自动驾驶、机器人技术、灾害响应等。由于深度学习的快速发展，点云处理算法现在几乎明确由基于学习的方法主导，但其中大多数尚未转化为实际应用实践。现有调查主要集中在不断更新的网络架构上，以适应无序点云，很大程度上忽略了它们在典型点云处理应用中的实际价值，在这些应用中需要考虑大量数据、多样化场景内容、变化的点密度和数据模态。在本文中，我们对点云处理中使用的深度学习方法和数据集进行了元综述，涵盖了场景补全、配准、语义分割和建模等关键任务。通过回顾这些任务可以支持的广泛城市和环境应用，我们确定了方法转化为应用时需要填补的差距，并对所调查方法的算法和实践两方面得出了结论。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云处理算法在实际应用中的价值和挑战问题。现实中，点云处理支持从空中到地面的多种应用如自动驾驶、导航、测绘等，但现有算法大多尚未过渡到实际应用。这个问题很重要，因为实际应用中需要考虑大量数据、多样化场景内容、不同点密度和数据模态等因素，而这些在现有综述中被忽视，导致理论与实践之间存在差距。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察现有综述主要关注网络架构而非实际应用价值，思考需要一种更全面的元综述方法。他们设计了涵盖点云处理关键任务（场景补全、配准、分割、建模）的综述框架，并关注这些任务在城市和环境应用中的支持作用。作者借鉴了现有的点云处理任务分类、深度学习方法（基于体素、多视图、基于点的方法）以及应用领域（城市建模、林业等），但将这些元素整合到一个关注实际应用的元综述框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过元综述深度学习在点云处理中的应用，从任务到实际应用，填补算法理论与实际应用之间的差距。整体流程包括：1)概述点云数据生成方式；2)详细讨论四个主要任务（场景补全、点云配准、分割、建模）和相关算法；3)回顾这些任务在城市建模、林业、农业等下游应用中的支持作用；4)讨论方法和应用，总结深度模型在点云处理中的展望。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)关注算法的实际应用价值而非仅关注网络架构；2)考虑实际应用中的额外挑战如大数据量、多样化场景内容等；3)提供从任务到应用的元综述；4)识别算法转化为应用时需要填补的差距。相比之前的工作，这篇论文不再局限于网络架构的枚举，而是建立了算法与实际应用之间的连接，并比较了不同学习模型在不同任务中的角色与传统方法的差异。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过元综述深度学习在点云处理中的应用，从任务到实际城市和环境应用，填补了算法理论与实际应用之间的差距，为点云处理技术的实际部署提供了指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud processing as a fundamental task in the field of geomatics andcomputer vision, has been supporting tasks and applications at different scalesfrom air to ground, including mapping, environmental monitoring, urban/treestructure modeling, automated driving, robotics, disaster responses etc. Due tothe rapid development of deep learning, point cloud processing algorithms havenowadays been almost explicitly dominated by learning-based approaches, most ofwhich are yet transitioned into real-world practices. Existing surveysprimarily focus on the ever-updating network architecture to accommodateunordered point clouds, largely ignoring their practical values in typicalpoint cloud processing applications, in which extra-large volume of data,diverse scene contents, varying point density, data modality need to beconsidered. In this paper, we provide a meta review on deep learning approachesand datasets that cover a selection of critical tasks of point cloud processingin use such as scene completion, registration, semantic segmentation, andmodeling. By reviewing a broad range of urban and environmental applicationsthese tasks can support, we identify gaps to be closed as these methodstransformed into applications and draw concluding remarks in both thealgorithmic and practical aspects of the surveyed methods.</description>
      <author>example@mail.com (Zhenxin Zhang, Zhihua Xu, Yuwei Cao, Ningli Xu, Shuye Wang, Shen'ao Cui, Zhen Li, Rongjun Qin)</author>
      <guid isPermaLink="false">2509.12452v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DYNAMO: Dependency-Aware Deep Learning Framework for Articulated Assembly Motion Prediction</title>
      <link>http://arxiv.org/abs/2509.12430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MechBench基准数据集和DYNAMO模型，用于解决从静态几何结构理解耦合机械运动的问题。MechBench包含693个合成齿轮组件，DYNAMO是一个依赖感知的神经模型，可直接从CAD点云预测零件运动轨迹，实验表明该方法优于基线，实现了准确且时间一致性的预测。&lt;h4&gt;背景&lt;/h4&gt;从静态几何结构理解铰接机械组件的运动是3D感知和设计自动化中的核心挑战。现有方法通常针对日常铰接物体（如门和笔记本电脑），假设简化的运动结构或依赖关节标注。但在机械组件（如齿轮）中，运动源于几何耦合（通过啮合齿或对齐轴），使得仅从几何结构推断关系运动变得困难。&lt;h4&gt;目的&lt;/h4&gt;解决从静态几何结构理解机械组件中耦合运动的问题，特别是那些通过几何耦合而非预定义关节产生运动的组件。为此，作者引入了MechBench数据集和DYNAMO模型，建立了一个系统性框架用于数据驱动的耦合机械运动学习。&lt;h4&gt;方法&lt;/h4&gt;作者提出了两个主要贡献：1) MechBench：一个包含693个多样化合成齿轮组件的基准数据集，具有零件级真实运动轨迹，提供研究耦合运动的结构化环境；2) DYNAMO：一个依赖感知的神经模型，可直接从分割的CAD点云预测每个零件的运动轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DYNAMO模型在各种齿轮配置下实现了准确且时间一致性的运动轨迹预测，性能优于现有的强基线方法。这证明了从几何结构推断耦合机械运动的可行性。&lt;h4&gt;结论&lt;/h4&gt;MechBench和DYNAMO共同建立了一个新颖的系统性框架，用于CAD组件中耦合机械运动的数据驱动学习，解决了从静态几何理解机械运动的核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;理解铰接机械组件从静态几何的运动是3D感知和设计自动化中的核心挑战。先前关于日常铰接物体（如门和笔记本电脑）的工作通常假设简化的运动结构或依赖关节标注。然而，在机械组件（如齿轮）中，运动源于几何耦合，通过啮合齿或对齐轴产生，这使得现有方法难以仅从几何结构推断关系运动。为解决这一差距，我们引入了MechBench，一个包含693个多样化合成齿轮组件的基准数据集，具有零件级真实运动轨迹。MechBench提供了一个研究耦合运动的结构化环境，其中零件动力学由接触和传动而非预定义关节引起。基于此，我们提出了DYNAMO，一个依赖感知的神经模型，可直接从分割的CAD点云预测每个零件的运动轨迹。实验表明，DYNAMO优于强基线方法，在各种齿轮配置下实现了准确且时间一致性的预测。MechBench和DYNAMO共同建立了一个新颖的系统性框架，用于CAD组件中耦合机械运动的数据驱动学习。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决铰接式机械装配体（特别是齿轮系统）从静态几何形状预测运动的问题。在现有方法中，运动预测通常假设简化的运动学结构或依赖关节标注，但在机械装配体中，运动源于零件间的几何耦合（如齿轮啮合或对齐轴），这使得从几何推理关系运动变得困难。这个问题在3D感知和设计自动化领域至关重要，因为现代CAD工具虽然提供详细3D几何，但很少包含可执行的运动规范，限制了它们在机器人应用（如装配自动化、运动规划和仿真）中的实用性。机器人要与、操作甚至设计这样的系统，理解其组成部分如何移动是必不可少的。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：大多数方法通过估计单个零件的机动性来建模运动学，然后应用固定方程组进行运动生成，这种方法在零件运动独立时有效，但在机械装配体中，零件运动相互依赖。作者选择基于齿轮的机制作为研究重点，因为它们的运动通过接触、啮合和传动传播。由于现有数据集很少包括耦合运动，作者创建了MechBench数据集。DYNAMO方法借鉴了PointNet++进行特征提取、图神经网络建模关系和Transformer进行时间解码等现有技术，但进行了改进以处理耦合运动，通过建模零件间的机械耦合关系来捕获运动传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是'依赖感知'，即认识到机械装配体中零件的运动不是独立的，而是通过接触和耦合相互依赖的。DYNAMO直接从分割的CAD点云预测每个零件的SE(3)运动轨迹，而不依赖于关节标注或预定义的运动学结构。整体实现流程包括：1) 使用PointNet++提取每个零件的几何特征；2) 构建零件图并使用接触启发式方法估计耦合矩阵，然后应用耦合感知的图神经网络建模零件间关系；3) 将零件特征在时间帧上复制并与位置编码结合，使用Transformer编码器处理时间依赖关系，并通过MLP预测6D扭转向量；4) 使用多项损失函数（平移L2损失、旋转测地损失和时间一致性损失）训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) MechBench数据集，包含693个合成齿轮装配体，专门用于评估耦合机械运动预测；2) DYNAMO依赖感知神经模型，能联合推理零件间关系；3) 通过图神经网络建模零件间的机械耦合关系；4) 实现从静态CAD几何端到端学习耦合刚体运动。相比之前工作，DYNAMO明确处理零件间的耦合关系而非假设独立运动；专注于机械装配体而非日常铰接物体；使用6D Lie代数向量确保预测的运动有效且连续；不依赖预定义关节信息而是从几何中学习；提供了专门针对耦合机械运动预测的数据集。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DYNAMO和MechBench共同建立了一个新颖的系统性框架，用于从CAD装配体的静态几何形状中学习耦合机械运动，通过依赖感知的神经网络准确预测相互连接零件的运动轨迹。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the motion of articulated mechanical assemblies from staticgeometry remains a core challenge in 3D perception and design automation. Priorwork on everyday articulated objects such as doors and laptops typicallyassumes simplified kinematic structures or relies on joint annotations.However, in mechanical assemblies like gears, motion arises from geometriccoupling, through meshing teeth or aligned axes, making it difficult forexisting methods to reason about relational motion from geometry alone. Toaddress this gap, we introduce MechBench, a benchmark dataset of 693 diversesynthetic gear assemblies with part-wise ground-truth motion trajectories.MechBench provides a structured setting to study coupled motion, where partdynamics are induced by contact and transmission rather than predefined joints.Building on this, we propose DYNAMO, a dependency-aware neural model thatpredicts per-part SE(3) motion trajectories directly from segmented CAD pointclouds. Experiments show that DYNAMO outperforms strong baselines, achievingaccurate and temporally consistent predictions across varied gearconfigurations. Together, MechBench and DYNAMO establish a novel systematicframework for data-driven learning of coupled mechanical motion in CADassemblies.</description>
      <author>example@mail.com (Mayank Patel, Rahul Jain, Asim Unmesh, Karthik Ramani)</author>
      <guid isPermaLink="false">2509.12430v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning</title>
      <link>http://arxiv.org/abs/2509.11594v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper needs major revision&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GBPP是一种基于快速学习的评分器，可以从单个RGB-D快照中选择机器人抓取的基础姿态。它使用两阶段课程学习方法：第一阶段使用简单的距离-可见性规则低成本自动标记大型数据集；第二阶段使用较小的高保真模拟试验集优化模型以匹配真实抓取结果。PointNet++风格的点云编码器与多层感知机一起对候选姿态进行评分，实现快速在线选择，无需完整的任务和运动优化。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取任务中需要选择合适的基础姿态，传统方法可能需要复杂的计算或大量数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速学习评分器(GBPP)，能够从单个RGB-D图像中选择机器人抓取的基础姿态。&lt;h4&gt;方法&lt;/h4&gt;两阶段课程学习：第一阶段使用简单的距离-可见性规则低成本自动标记大型数据集；第二阶段使用高保真模拟试验集优化模型。使用PointNet++风格的点云编码器与多层感知机对候选姿态进行评分，实现快速在线选择，无需完整的任务和运动优化。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何形状的基线方法；GBPP能选择更安全、更可达的姿态；在错误情况下表现良好(优雅降级)。&lt;h4&gt;结论&lt;/h4&gt;GBPP为数据高效、几何感知的基础放置提供了实用方法：先使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;h4&gt;翻译&lt;/h4&gt;GBPP是一种基于快速学习的评分器，它从单个RGB-D快照中选择机器人抓取的基础姿态。该方法使用两阶段课程学习：(1)简单的距离-可见性规则低成本自动标记大型数据集；(2)较小的高保真模拟试验集优化模型以匹配真实抓取结果。带有MLP的PointNet++风格点云编码器对密集网格候选姿态进行评分，实现无需完整任务和运动优化的快速在线选择。在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法，选择更安全、更可达的姿态，并在错误情况下优雅降级。这些结果为数据高效、几何感知的基础放置提供了实用方法：使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GBPP is a fast learning based scorer that selects a robot base pose forgrasping from a single RGB-D snapshot. The method uses a two stage curriculum:(1) a simple distance-visibility rule auto-labels a large dataset at low cost;and (2) a smaller set of high fidelity simulation trials refines the model tomatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLPscores dense grids of candidate poses, enabling rapid online selection withoutfull task-and-motion optimization. In simulation and on a real mobilemanipulator, GBPP outperforms proximity and geometry only baselines, choosingsafer and more reachable stances and degrading gracefully when wrong. Theresults offer a practical recipe for data efficient, geometry aware baseplacement: use inexpensive heuristics for coverage, then calibrate withtargeted simulation.</description>
      <author>example@mail.com (Jizhuo Chen, Diwen Liu, Jiaming Wang, Harold Soh)</author>
      <guid isPermaLink="false">2509.11594v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>3D Aware Region Prompted Vision Language Model</title>
      <link>http://arxiv.org/abs/2509.13317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Website: https://www.anjiecheng.me/sr3d&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SR-3D是一种视觉语言模型，通过共享视觉令牌空间连接2D图像和3D数据，支持灵活的区域标注，无需繁琐的多帧标注。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉模型在处理2D和3D数据时存在局限性，特别是在跨帧空间推理方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够统一2D和3D表示空间的模型，提高场景理解的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过将3D位置嵌入增强2D视觉特征，使3D模型能够利用2D先验知识进行跨帧空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;SR-3D在2D视觉语言和3D空间基准测试中达到了最先进的性能，即使在野外视频中也表现出色。&lt;h4&gt;结论&lt;/h4&gt;SR-3D有效地统一了2D和3D表示空间，提高了场景理解的准确性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了空间区域3D(SR-3D)感知视觉语言模型，通过共享视觉令牌空间连接单视图2D图像和多视图3D数据。SR-3D支持灵活的区域提示，允许用户在任何帧上使用边界框、分割掩码标注区域，或直接在3D中标注，无需进行繁琐的多帧标注。我们通过将3D位置嵌入增强2D视觉特征来实现这一点，使3D模型能够利用强大的2D先验知识进行更准确的跨帧空间推理，即使感兴趣的对象不出现在同一视图中。在通用2D视觉语言和专业3D空间基准上的大量实验表明，SR-3D达到了最先进的性能，证明了其在统一2D和3D表示空间以实现场景理解方面的有效性。此外，我们观察到在没有3D输入或真实3D标注的野外视频中，SR-3D也能准确推断空间关系和度量测量，显示出其适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉语言模型在3D空间推理方面的局限性。现有2D模型缺乏理解复杂3D结构的能力，而3D模型又难以利用2D模型的强大先验知识。这个问题很重要，因为准确的空间推理对机器人导航、自动驾驶、增强现实等应用至关重要，能让AI系统更好地理解和物理世界互动。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是：首先认识到2D视觉语言模型有强大先验但缺乏3D能力；然后发现多视图图像可作为3D表示与2D模型对齐；接着意识到区域提示在单视图有效但扩展到多视图有挑战；最后设计统一架构连接2D和3D表示。他们借鉴了DepthAnythingV2进行深度估计、动态瓦片机制处理高分辨率图像，以及RegionGPT等工作的区域提示概念，并整合了点云估计器处理视频输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建统一的3D感知视觉语言模型，通过共享视觉标记空间连接单视图2D图像和多视图3D数据，并在2D特征中加入3D位置嵌入，使模型能利用2D先验进行跨帧空间推理。实现流程包括：1)单视图表示：预训练基础模型，估计深度，计算3D位置，编码并融合位置嵌入；2)多视图表示：采样视频帧，对齐并规范化点图；3)动态瓦片区域提取：处理高分辨率图像和区域特征；4)训练：先单视图预训练，再多视图微调；5)推理：支持灵活的区域标注方式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一架构实现单视图和多视图任务的3D感知；2)动态瓦片区域提取器处理高分辨率图像；3)统一的嵌入空间使2D训练的区域能推广到多视图；4)灵活的区域标注支持；5)规范化的3D位置空间。相比之前工作，SR-3D不使用单独路径处理不同视图数据，而是直接在基础模型中整合位置嵌入，支持更灵活的区域标注，且在单视图训练后就能在多视图场景中表现出强大的零样本空间推理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SR-3D通过统一的3D感知视觉语言模型架构，实现了单视图2D图像和多视图3D数据的有效连接，支持灵活的区域提示，并在各种空间推理任务上达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Spatial Region 3D (SR-3D) aware vision-language model thatconnects single-view 2D images and multi-view 3D data through a shared visualtoken space. SR-3D supports flexible region prompting, allowing users toannotate regions with bounding boxes, segmentation masks on any frame, ordirectly in 3D, without the need for exhaustive multi-frame labeling. Weachieve this by enriching 2D visual features with 3D positional embeddings,which allows the 3D model to draw upon strong 2D priors for more accuratespatial reasoning across frames, even when objects of interest do not co-occurwithin the same view. Extensive experiments on both general 2D vision languageand specialized 3D spatial benchmarks demonstrate that SR-3D achievesstate-of-the-art performance, underscoring its effectiveness for unifying 2Dand 3D representation space on scene understanding. Moreover, we observeapplicability to in-the-wild videos without sensory 3D inputs or ground-truth3D annotations, where SR-3D accurately infers spatial relationships and metricmeasurements.</description>
      <author>example@mail.com (An-Chieh Cheng, Yang Fu, Yukang Chen, Zhijian Liu, Xiaolong Li, Subhashree Radhakrishnan, Song Han, Yao Lu, Jan Kautz, Pavlo Molchanov, Hongxu Yin, Xiaolong Wang, Sifei Liu)</author>
      <guid isPermaLink="false">2509.13317v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>MEJO: MLLM-Engaged Surgical Triplet Recognition via Inter- and Intra-Task Joint Optimization</title>
      <link>http://arxiv.org/abs/2509.12893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MLLM-Engaged Joint Optimization (MEJO)的框架，用于解决外科手术三元组识别中的跨任务和任务内优化冲突问题。该框架通过Shared-Specific-Disentangled学习方案和协调梯度学习策略，有效处理了长尾数据分布和类别不平衡带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;外科手术三元组识别涉及识别手术器械、动词、目标及其组合，是一个复杂的外科场景理解挑战，面临长尾数据分布问题。&lt;h4&gt;目的&lt;/h4&gt;克服外科手术三元组识别中的两个关键挑战：跨任务优化冲突（由任务通用和任务特定表示纠缠引起）和任务内优化冲突（由类别不平衡的训练数据引起）。&lt;h4&gt;方法&lt;/h4&gt;提出MEJO框架，包括：1) 对于跨任务优化，引入Shared-Specific-Disentangled学习方案，构建多模态大语言模型驱动的概率提示池；2) 对于任务内优化，开发协调梯度学习策略，解析和重新平衡来自头部和尾部类别的正负梯度。&lt;h4&gt;主要发现&lt;/h4&gt;在CholecT45和CholecT50数据集上的广泛实验表明，所提出的MEJO框架在处理外科手术三元组识别的优化冲突方面具有优越性，能有效解决跨任务和任务内优化冲突问题。&lt;h4&gt;结论&lt;/h4&gt;MEJO框架通过结合多模态大语言模型和精心设计的优化策略，能够有效解决外科手术三元组识别中的优化冲突问题，提高了识别性能。&lt;h4&gt;翻译&lt;/h4&gt;外科手术三元组识别涉及识别器械、动词、目标及其组合，是一个复杂的外科场景理解挑战，受长尾数据分布困扰。受益于跨任务协作促进的主流多任务学习范式在识别三元组方面显示出有前景的性能，但两个关键挑战仍然存在：1) 由任务通用和任务特定表示纠缠导致的跨任务优化冲突；2) 由于类别不平衡的训练数据导致的任务内优化冲突。为了克服这些困难，我们提出了MLLM-Engaged Joint Optimization (MEJO)框架，它为外科手术三元组识别赋能了跨任务和任务内优化。对于跨任务优化，我们引入了Shared-Specific-Disentangled学习方案，将表示分解为任务共享和任务特定组件。为了增强任务共享表示，我们构建了一个多模态大语言模型驱动的概率提示池，以专家级语义线索动态增强视觉特征。此外，通过覆盖时空维度的不同任务提示，全面建模了任务特定线索，有效减轻了跨任务模糊性。为了解决任务内优化冲突，我们开发了协调梯度学习策略，它解析和重新平衡来自头部和尾部类别的正负梯度，以实现更协调的学习行为。在CholecT45和CholecT50数据集上的广泛实验证明了我们提出框架的优越性，验证了其在处理优化冲突方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical triplet recognition, which involves identifying instrument, verb,target, and their combinations, is a complex surgical scene understandingchallenge plagued by long-tailed data distribution. The mainstream multi-tasklearning paradigm benefiting from cross-task collaborative promotion has shownpromising performance in identifying triples, but two key challenges remain: 1)inter-task optimization conflicts caused by entangling task-generic andtask-specific representations; 2) intra-task optimization conflicts due toclass-imbalanced training data. To overcome these difficulties, we propose theMLLM-Engaged Joint Optimization (MEJO) framework that empowers both inter- andintra-task optimization for surgical triplet recognition. For inter-taskoptimization, we introduce the Shared-Specific-Disentangled (S$^2$D) learningscheme that decomposes representations into task-shared and task-specificcomponents. To enhance task-shared representations, we construct a MultimodalLarge Language Model (MLLM) powered probabilistic prompt pool to dynamicallyaugment visual features with expert-level semantic cues. Additionally,comprehensive task-specific cues are modeled via distinct task prompts coveringthe temporal-spatial dimensions, effectively mitigating inter-task ambiguities.To tackle intra-task optimization conflicts, we develop a Coordinated GradientLearning (CGL) strategy, which dissects and rebalances the positive-negativegradients originating from head and tail classes for more coordinated learningbehaviors. Extensive experiments on the CholecT45 and CholecT50 datasetsdemonstrate the superiority of our proposed framework, validating itseffectiveness in handling optimization conflicts.</description>
      <author>example@mail.com (Yiyi Zhang, Yuchen Yuan, Ying Zheng, Jialun Pei, Jinpeng Li, Zheng Li, Pheng-Ann Heng)</author>
      <guid isPermaLink="false">2509.12893v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>EvoEmpirBench: Dynamic Spatial Reasoning with Agent-ExpVer</title>
      <link>http://arxiv.org/abs/2509.12718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Ongoing Work, 29 pages, 3 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究引入了两个动态空间基准测试，评估模型在动态环境下的空间理解和自适应规划能力，揭示了主流模型在动态空间推理和长期记忆方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;现有空间推理基准测试主要关注静态或全局可观察环境，无法捕捉部分可观察性和动态变化下的长期推理和内存利用挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍两个动态空间基准测试，系统评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。&lt;h4&gt;方法&lt;/h4&gt;提出局部可观察迷宫导航和match-2消除游戏两个基准测试，每个动作触发环境结构变化，需要持续更新认知和策略。同时提出基于主观体验的跨任务经验转移和验证的记忆机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这些基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性。&lt;h4&gt;结论&lt;/h4&gt;提供了一个全面的平台，用于未来的方法论改进。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有的空间推理基准测试关注静态或全局可观察环境，无法捕捉部分可观察性和动态变化下的长期推理和内存利用挑战。我们引入了两个动态空间基准测试——局部可观察迷宫导航和match-2消除游戏，系统评估模型在局部感知、环境反馈和全局目标紧密耦合情况下的空间理解和自适应规划能力。每个动作都会触发环境结构变化，需要持续更新认知和策略。我们进一步提出了基于主观体验的跨任务经验转移和验证的记忆机制。实验表明，我们的基准测试揭示了主流模型在动态空间推理和长期记忆方面的关键局限性，为未来方法论改进提供了全面平台。我们的代码和数据可在https://anonymous.4open.science/r/EvoEmpirBench-143C/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing spatial reasoning benchmarks focus on static or globallyobservable environments, failing to capture the challenges of long-horizonreasoning and memory utilization under partial observability and dynamicchanges. We introduce two dynamic spatial benchmarks, locally observable mazenavigation and match-2 elimination that systematically evaluate models'abilities in spatial understanding and adaptive planning when local perception,environment feedback, and global objectives are tightly coupled. Each actiontriggers structural changes in the environment, requiring continuous update ofcognition and strategy. We further propose a subjective experience-based memorymechanism for cross-task experience transfer and validation. Experiments showthat our benchmarks reveal key limitations of mainstream models in dynamicspatial reasoning and long-term memory, providing a comprehensive platform forfuture methodological advances. Our code and data are available athttps://anonymous.4open.science/r/EvoEmpirBench-143C/.</description>
      <author>example@mail.com (Pukun Zhao, Longxiang Wang, Miaowei Wang, Chen Chen, Fanqing Zhou, Haojian Huang)</author>
      <guid isPermaLink="false">2509.12718v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
      <link>http://arxiv.org/abs/2509.12125v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, EPIA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为RailSafeNet的实时框架，利用数字图像处理、深度学习和人工智能技术提高有轨电车与人类交互的安全性，通过语义分割、目标检测和距离评估来识别轨道入侵风险。&lt;h4&gt;背景&lt;/h4&gt;有轨电车经常在人口密集地区运行，与人类（行人、驾驶员、骑自行车的人、宠物等）的交互安全是一个重要挑战，碰撞可能导致从轻微伤害到致命后果的各种事故。&lt;h4&gt;目的&lt;/h4&gt;设计一个利用数字图像处理、深度学习和人工智能的解决方案，提高行人、驾驶员、骑自行车的人、宠物和有轨电车乘客的安全。&lt;h4&gt;方法&lt;/h4&gt;提出RailSafeNet实时框架，融合语义分割、目标检测和基于规则的距离评估器，使用单目视频识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨距比较来分类风险。&lt;h4&gt;主要发现&lt;/h4&gt;在RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，微调后的YOLOv8在交并比阈值为0.50的情况下达到了75.6%的平均精度均值。&lt;h4&gt;结论&lt;/h4&gt;RailSafeNet提供准确的、标注较少的场景理解，可以在危险情况升级之前警告驾驶员，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;有轨电车与人类交互安全是一个重要挑战，因为有轨电车经常在人口密集地区运行，碰撞可能导致轻微伤害甚至致命后果。本文从设计解决方案的角度解决这一问题，利用数字图像处理、深度学习和人工智能来提高行人、驾驶员、骑自行车的人、宠物和有轨电车乘客的安全。我们提出了RailSafeNet，一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器，以突出显示轨道入侵。仅使用单目视频，系统识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨距进行比较来分类风险。在多样化的RailSem19数据集上的实验表明，经过类别过滤的SegFormer B3模型实现了65%的交并比，而微调后的YOLOv8在交并比阈值为0.50的情况下达到了75.6%的平均精度均值。因此，RailSafeNet提供了准确的、标注较少的场景理解，可以在危险情况升级之前警告驾驶员。代码可在https://github.com/oValach/RailSafeNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tram-human interaction safety is an important challenge, given that tramsfrequently operate in densely populated areas, where collisions can range fromminor injuries to fatal outcomes. This paper addresses the issue from theperspective of designing a solution leveraging digital image processing, deeplearning, and artificial intelligence to improve the safety of pedestrians,drivers, cyclists, pets, and tram passengers. We present RailSafeNet, areal-time framework that fuses semantic segmentation, object detection and arule-based Distance Assessor to highlight track intrusions. Using onlymonocular video, the system identifies rails, localises nearby objects andclassifies their risk by comparing projected distances with the standard 1435mmrail gauge. Experiments on the diverse RailSem19 dataset show that aclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculatedat an intersection over union (IoU) threshold of 0.50. RailSafeNet thereforedelivers accurate, annotation-light scene understanding that can warn driversbefore dangerous situations escalate. Code available athttps://github.com/oValach/RailSafeNet.</description>
      <author>example@mail.com (Ondřej Valach, Ivan Gruber)</author>
      <guid isPermaLink="false">2509.12125v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
      <link>http://arxiv.org/abs/2509.11292v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入几何先验解决非对齐场景变化检测的核心挑战，提出无需训练的框架，结合视觉基础模型表征，实现视角不对齐情况下的可靠变化检测。&lt;h4&gt;背景&lt;/h4&gt;非对齐场景变化检测旨在检测不同时间拍摄且无视角对齐的图像对间的场景变化。当前方法仅依赖2D视觉线索建立对应关系，但大视角变化会导致基于外观的匹配失败。小规模数据集的2D变化掩码监督限制了多视图知识学习，缺乏显式几何推理是关键但被忽视的局限性。&lt;h4&gt;目的&lt;/h4&gt;引入几何先验解决非对齐SCD的核心挑战，实现可靠的视觉重叠识别、稳健的对应关系建立和显式的遮挡检测。&lt;h4&gt;方法&lt;/h4&gt;提出一种无需训练的框架，将几何先验与视觉基础模型的强大表征相结合，使模型能够在视角不对齐的情况下实现可靠的变化检测。&lt;h4&gt;主要发现&lt;/h4&gt;在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何先验，解决了非对齐SCD中的核心挑战，提高了变化检测的可靠性。&lt;h4&gt;翻译&lt;/h4&gt;非对齐场景变化检测旨在检测在不同时间拍摄且没有假设视角对齐的图像对之间的场景变化。为处理视角变化，当前方法仅依赖2D视觉线索建立跨图像对应关系以辅助变化检测。然而，大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。此外，仅限于小规模SCD数据集的2D变化掩码监督限制了可泛化的多视图知识学习，使得难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理代表了关键但被忽视的局限性。在这项工作中，我们首次引入几何先验来解决非对齐SCD的核心挑战，实现可靠的视觉重叠识别、稳健的对应关系建立和显式的遮挡检测。基于这些先验，我们提出了一种无需训练的框架，将其与视觉基础模型的强大表征相结合，使模型能够在视角不对齐的情况下实现可靠的变化检测。通过在PSCD、ChangeSim和PASLCD数据集上的广泛评估，我们证明了我们的方法实现了优越且稳健的性能。我们的代码将在https://github.com/ZilingLiu/GeoSCD上发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决不对齐场景变化检测问题，即在相机视角不同的情况下检测场景变化。这个问题在现实中非常重要，因为自动驾驶、无人机和移动机器人等应用中经常遇到不同时间拍摄的图像存在视角差异的情况。传统方法假设图像视角对齐，限制了它们在真实世界场景中的实用性。缺乏显式几何推理导致这些方法在大视角变化下表现不佳，难以可靠识别视觉重叠和处理遮挡问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到当前方法仅依赖2D视觉线索建立图像对应关系，在大视角变化下容易失败。他们发现2D监督限制了多视图知识的泛化学习，难以处理遮挡问题。受人类能轻松从不同视角建立关联和感知遮挡的启发，作者认为3D信息可以解决这些挑战。他们借鉴了几何基础模型(GFMs)可以从多视角图像中恢复3D几何的能力，并利用视觉基础模型SAM的强大表示能力，设计了一个两阶段框架：先利用GFM建立几何理解，再指导变化掩码预测。这种方法还借鉴了零样本思想来实现跨数据集泛化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入几何先验知识来解决不对齐场景变化检测的挑战，包括识别视觉重叠、建立稳健对应关系和检测遮挡。整体流程分为两个主要模块：1)几何先验生成模块：使用GFM重建深度图和相机参数，建立像素级对应关系，识别视觉重叠区域，并检测遮挡区域；2)几何引导变化掩码预测模块：将几何线索与SAM模型集成，在重叠区域内通过特征相似性生成初始变化提案，用遮挡掩码精炼提案，再与SAM的分割掩码匹配融合产生最终结果。此外，还包含光照变化处理步骤，使用Retinex或Color Transfer技术减少光照差异对GFM重建的影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次引入几何先验知识来识别视觉重叠、建立稳健对应关系和检测遮挡；2)提出无需训练的框架，将几何先验与视觉基础模型集成，减少对标注数据的依赖；3)在各种视角变化和场景类型上实现领先且稳健的性能。相比之前工作，本文超越了仅依赖2D视觉线索的传统方法，解决了遮挡问题，无需大规模训练数据，具有更强的跨数据集泛化能力，并能处理更复杂的场景和更广泛的视角变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次将几何先验知识引入不对齐场景变化检测，通过结合几何基础模型与视觉基础模型，实现了无需训练、视角变化鲁棒且跨数据集泛化的场景变化检测方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unaligned Scene Change Detection aims to detect scene changes between imagepairs captured at different times without assuming viewpoint alignment. Tohandle viewpoint variations, current methods rely solely on 2D visual cues toestablish cross-image correspondence to assist change detection. However, largeviewpoint changes can alter visual observations, causing appearance-basedmatching to drift or fail. Additionally, supervision limited to 2D change masksfrom small-scale SCD datasets restricts the learning of generalizablemulti-view knowledge, making it difficult to reliably identify visual overlapsand handle occlusions. This lack of explicit geometric reasoning represents acritical yet overlooked limitation. In this work, we introduce geometric priorsfor the first time to address the core challenges of unaligned SCD, forreliable identification of visual overlaps, robust correspondenceestablishment, and explicit occlusion detection. Building on these priors, wepropose a training-free framework that integrates them with the powerfulrepresentations of a visual foundation model to enable reliable changedetection under viewpoint misalignment. Through extensive evaluation on thePSCD, ChangeSim, and PASLCD datasets, we demonstrate that our approach achievessuperior and robust performance. Our code will be released athttps://github.com/ZilingLiu/GeoSCD.</description>
      <author>example@mail.com (Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.11292v2</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Curriculum Multi-Task Self-Supervision Improves Lightweight Architectures for Onboard Satellite Hyperspectral Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.13229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CMTSSL的课程多任务自监督学习框架，专为高光谱成像分析设计的轻量级架构，有效结合了掩模图像建模与空间光谱拼图求解。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像能捕获每个像素的数百个连续波段的光谱特征，对土地覆盖分类、变化检测和环境监测等遥感应用至关重要。但由于数据高维度和卫星系统传输速率慢，需要高效模型支持星上处理。&lt;h4&gt;目的&lt;/h4&gt;开发轻量级模型支持星上处理，最小化冗余或低价值数据(如云覆盖区域)的传输，同时保持HSI分析的准确性。&lt;h4&gt;方法&lt;/h4&gt;CMTSSL框架结合掩模图像建模与解耦的空间和光谱拼图求解，采用课程学习策略逐步增加数据复杂度，使编码器能同时捕获光谱连续性、空间结构和全局语义特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上验证，CMTSSL在下游分割任务中表现一致提升，使用的架构比某些最先进模型轻16000多倍。&lt;h4&gt;结论&lt;/h4&gt;CMTSSL在轻量级架构上的可推广表示学习具有潜力，适用于真实世界的HSI应用，代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)捕获每个像素的数百个连续波段的详细光谱特征，对于土地覆盖分类、变化检测和环境监测等遥感应用不可或缺。由于HSI数据的高维性和卫星系统数据传输速率慢，需要紧凑高效的模型支持星上处理并最小化冗余或低价值数据(如云覆盖区域)的传输。为此，我们引入了一种专为HSI分析轻量架构设计的课程多任务自监督学习(CMTSSL)框架。CMTSSL将掩模图像建模与解耦的空间和光谱拼图求解相结合，通过课程学习策略在自监督过程中逐步增加数据复杂度，使编码器能够同时捕获细粒度光谱连续性、空间结构和全局语义特征。与先前的双任务SSL方法不同，CMTSSL在统一且计算高效的设计中同时解决空间和光谱推理，特别适合用于轻量级模型的星上卫星部署。我们在四个公共基准数据集上验证了我们的方法，在使用比某些最先进模型轻16000多倍的架构时，在下游分割任务中表现出一致的改进。这些结果突显了CMTSSL在轻量级架构上可推广表示学习的潜力，适用于真实世界的HSI应用。我们的代码已在https://github.com/hugocarlesso/CMTSSL公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) captures detailed spectral signatures acrosshundreds of contiguous bands per pixel, being indispensable for remote sensingapplications such as land-cover classification, change detection, andenvironmental monitoring. Due to the high dimensionality of HSI data and theslow rate of data transfer in satellite-based systems, compact and efficientmodels are required to support onboard processing and minimize the transmissionof redundant or low-value data, e.g. cloud-covered areas. To this end, weintroduce a novel curriculum multi-task self-supervised learning (CMTSSL)framework designed for lightweight architectures for HSI analysis. CMTSSLintegrates masked image modeling with decoupled spatial and spectral jigsawpuzzle solving, guided by a curriculum learning strategy that progressivelyincreases data complexity during self-supervision. This enables the encoder tojointly capture fine-grained spectral continuity, spatial structure, and globalsemantic features. Unlike prior dual-task SSL methods, CMTSSL simultaneouslyaddresses spatial and spectral reasoning within a unified and computationallyefficient design, being particularly suitable for training lightweight modelsfor onboard satellite deployment. We validate our approach on four publicbenchmark datasets, demonstrating consistent gains in downstream segmentationtasks, using architectures that are over 16,000x lighter than somestate-of-the-art models. These results highlight the potential of CMTSSL ingeneralizable representation learning with lightweight architectures forreal-world HSI applications. Our code is publicly available athttps://github.com/hugocarlesso/CMTSSL.</description>
      <author>example@mail.com (Hugo Carlesso, Josiane Mothe, Radu Tudor Ionescu)</author>
      <guid isPermaLink="false">2509.13229v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Representation Learning for Robust Sim-to-Real Transfer of Adaptive Humanoid Locomotion</title>
      <link>http://arxiv.org/abs/2509.12858v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新范式，通过对比学习框架使纯本体感觉策略获得感知的前瞻性能力，解决了人形机器人运动中反应性控制和感知驱动系统之间的权衡问题，实现了在不增加部署成本的情况下主动适应复杂地形的能力。&lt;h4&gt;背景&lt;/h4&gt;强化学习在人形机器人运动方面取得了显著进展，但在实际部署中存在一个基本困境：策略必须在反应性本体感觉控制的鲁棒性和复杂的、脆弱的感知驱动系统的主动性之间做出选择。&lt;h4&gt;目的&lt;/h4&gt;解决这一困境，引入一种范式，使纯粹的本体感觉策略具有主动性，获得感知的前瞻性能力，同时避免其部署时的成本。&lt;h4&gt;方法&lt;/h4&gt;核心贡献是一个对比学习框架，它强制执行者的潜在状态从仿真中编码特权的环境信息。这种'蒸馏意识'使自适应节拍时钟能够根据对地形的推断理解主动调整节奏。&lt;h4&gt;主要发现&lt;/h4&gt;这种协同作用解决了刚性时钟步态和不稳定的无时钟策略之间的经典权衡。通过零样本仿真到真实世界的转移，验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法使全尺寸人形机器人在具有挑战性的地形上（包括30厘米高的台阶和26.5度的斜坡）实现了高度鲁棒的运动。&lt;h4&gt;翻译&lt;/h4&gt;强化学习在人形机器人运动方面已经取得了显著进展，但实际部署中仍然存在一个基本困境：策略必须在反应性本体感觉控制的鲁棒性和复杂的、脆弱的感知驱动系统的主动性之间做出选择。本文通过引入一种范式解决了这一困境，该范式赋予纯本体感觉策略以主动能力，实现了感知的前瞻性，同时避免了其部署时的成本。我们的核心贡献是一个对比学习框架，它强制执行者的潜在状态从仿真中编码特权的环境信息。重要的是，这种'蒸馏意识'使自适应节拍时钟能够基于对地形的推断理解主动调整节奏。这种协同作用解决了刚性时钟步态和不稳定的无时钟策略之间的经典权衡。我们通过零样本仿真到真实世界的转移验证了我们的方法，证明全尺寸人形机器人在具有挑战性的地形上（包括30厘米高的台阶和26.5度的斜坡）实现了高度鲁棒的运动。网站：https://lu-yidan.github.io/cra-loco。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning has produced remarkable advances in humanoidlocomotion, yet a fundamental dilemma persists for real-world deployment:policies must choose between the robustness of reactive proprioceptive controlor the proactivity of complex, fragile perception-driven systems. This paperresolves this dilemma by introducing a paradigm that imbues a purelyproprioceptive policy with proactive capabilities, achieving the foresight ofperception without its deployment-time costs. Our core contribution is acontrastive learning framework that compels the actor's latent state to encodeprivileged environmental information from simulation. Crucially, this``distilled awareness" empowers an adaptive gait clock, allowing the policy toproactively adjust its rhythm based on an inferred understanding of theterrain. This synergy resolves the classic trade-off between rigid, clockedgaits and unstable clock-free policies. We validate our approach with zero-shotsim-to-real transfer to a full-sized humanoid, demonstrating highly robustlocomotion over challenging terrains, including 30 cm high steps and 26.5{\deg}slopes, proving the effectiveness of our method. Website:https://lu-yidan.github.io/cra-loco.</description>
      <author>example@mail.com (Yidan Lu, Rurui Yang, Qiran Kou, Mengting Chen, Tao Fan, Peter Cui, Yinzhao Dong, Peng Lu)</author>
      <guid isPermaLink="false">2509.12858v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Modeling the Multivariate Relationship with Contextualized Representations for Effective Human-Object Interaction Detection</title>
      <link>http://arxiv.org/abs/2509.12784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种上下文化表征学习网络，通过结合功能引导推理和上下文提示，改进了人机交互(HOI)检测方法，能够更好地捕捉复杂交互关系。&lt;h4&gt;背景&lt;/h4&gt;人机交互(HOI)检测旨在同时定位人-物体对并识别它们的交互。最近的两阶段方法虽然取得了显著进展，但由于上下文建模不完整，仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍一种上下文表征学习网络，结合功能引导推理和上下文提示，以更好地捕捉复杂交互。&lt;h4&gt;方法&lt;/h4&gt;通过三元结构&lt;人，工具，物体&gt;明确建模辅助对象的功能角色；将可学习提示与实例类别丰富化，并使用注意力机制与上下文视觉特征集成；在全局和区域级别将语言与图像内容对齐。&lt;h4&gt;主要发现&lt;/h4&gt;提出的上下文化表征为模型提供了丰富的关系线索，使其能够对复杂、上下文依赖的交互进行更可靠的推理。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在HICO-Det和V-COCO数据集的大多数场景中表现出优越的性能。&lt;h4&gt;翻译&lt;/h4&gt;人机交互(HOI)检测旨在同时定位人-物体对并识别它们的交互。虽然最近的两阶段方法已经取得了显著进展，但由于上下文建模不完整，它们仍然面临挑战。在这项工作中，我们引入了一种上下文化表征学习网络，结合功能引导推理和上下文提示以及视觉线索，以更好地捕捉复杂交互。我们通过将常规HOI检测框架扩展到简单的物体对之外，包括涉及工具等辅助实体的多元关系，来增强它。具体来说，我们通过三元结构&lt;人，工具，物体&gt;明确建模这些辅助对象的功能角色(功能)。这使得我们的模型能够识别依赖于工具的交互，如'填充'。此外，可学习提示被实例类别丰富化，随后使用注意力机制与上下文视觉特征集成。这个过程在全局和区域级别上将语言与图像内容对齐。这些上下文化表征为模型提供了丰富的关系线索，以便对复杂、上下文依赖的交互进行更可靠的推理。我们提出的方法在HICO-Det和V-COCO数据集的大多数场景中表现出优越的性能。代码将在接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-Object Interaction (HOI) detection aims to simultaneously localizehuman-object pairs and recognize their interactions. While recent two-stageapproaches have made significant progress, they still face challenges due toincomplete context modeling. In this work, we introduce a ContextualizedRepresentation Learning Network that integrates both affordance-guidedreasoning and contextual prompts with visual cues to better capture complexinteractions. We enhance the conventional HOI detection framework by expandingit beyond simple human-object pairs to include multivariate relationshipsinvolving auxiliary entities like tools. Specifically, we explicitly model thefunctional role (affordance) of these auxiliary objects through tripletstructures &lt;human, tool, object&gt;. This enables our model to identifytool-dependent interactions such as 'filling'. Furthermore, the learnableprompt is enriched with instance categories and subsequently integrated withcontextual visual features using an attention mechanism. This process alignslanguage with image content at both global and regional levels. Thesecontextualized representations equip the model with enriched relational cuesfor more reliable reasoning over complex, context-dependent interactions. Ourproposed method demonstrates superior performance on both the HICO-Det andV-COCO datasets in most scenarios. Codes will be released upon acceptance.</description>
      <author>example@mail.com (Zhehao Li, Yucheng Qian, Chong Wang, Yinghao Lu, Zhihao Yang, Jiafei Wu)</author>
      <guid isPermaLink="false">2509.12784v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>NORA: A Nephrology-Oriented Representation Learning Approach Towards Chronic Kidney Disease Classification</title>
      <link>http://arxiv.org/abs/2509.12704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, accepted to the International Conference on  Machine Learning and Applications (ICMLA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为NORA的肾脏病导向表示学习方法，利用常规收集的非肾脏临床变量进行慢性肾脏病分类，在缺乏专门肾脏生物标志物的环境中表现出良好的性能。&lt;h4&gt;背景&lt;/h4&gt;慢性肾脏病影响全球数百万人，但其早期检测仍具有挑战性，特别是在门诊环境中，基于实验室的肾脏生物标志物通常不可用。&lt;h4&gt;目的&lt;/h4&gt;研究常规收集的非肾脏临床变量（包括社会人口学因素、合并症和尿检结果）在慢性肾脏病分类中的预测潜力。&lt;h4&gt;方法&lt;/h4&gt;引入Nephrology-Oriented Representation leArning (NORA)方法，结合监督对比学习和非线性随机森林分类器，从表格型电子健康记录数据中推导区分性患者表示，用于下游CKD分类。在Riverside肾脏病学医师的基于诊所的EHR数据集上评估该方法，并在UCI CKD数据集上测试其可推广性。&lt;h4&gt;主要发现&lt;/h4&gt;NORA提高了类间可分性和整体分类性能，特别增强了早期阶段CKD的F1分数；在不同患者队列中证明了NORA进行CKD风险分层的有效性。&lt;h4&gt;结论&lt;/h4&gt;NORA方法能够有效利用常规临床数据进行CKD检测和风险分层，特别是在缺乏专门肾脏生物标志物的环境中具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;慢性肾脏病(CKD)影响全球数百万人，但其早期检测仍然具有挑战性，特别是在门诊环境中，基于实验室的肾脏生物标志物通常不可用。在本研究中，我们调查了常规收集的非肾脏临床变量（包括社会人口学因素、合并症和尿检结果）在CKD分类中的预测潜力。我们引入了肾脏病导向表示学习(NORA)方法，结合监督对比学习和非线性随机森林分类器。NORA首先从表格型电子健康记录数据中推导出区分性患者表示，然后用于下游CKD分类。我们在Riverside肾脏病学医师的基于诊所的EHR数据集上评估了NORA。结果表明，NORA提高了类间可分性和整体分类性能，特别增强了早期阶段CKD的F1分数。此外，我们在UCI CKD数据集上评估了NORA的可推广性，证明了其在不同患者队列中进行CKD风险分层的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chronic Kidney Disease (CKD) affects millions of people worldwide, yet itsearly detection remains challenging, especially in outpatient settings wherelaboratory-based renal biomarkers are often unavailable. In this work, weinvestigate the predictive potential of routinely collected non-renal clinicalvariables for CKD classification, including sociodemographic factors, comorbidconditions, and urinalysis findings. We introduce the Nephrology-OrientedRepresentation leArning (NORA) approach, which combines supervised contrastivelearning with a nonlinear Random Forest classifier. NORA first derivesdiscriminative patient representations from tabular EHR data, which are thenused for downstream CKD classification. We evaluated NORA on a clinic-based EHRdataset from Riverside Nephrology Physicians. Our results demonstrated thatNORA improves class separability and overall classification performance,particularly enhancing the F1-score for early-stage CKD. Additionally, weassessed the generalizability of NORA on the UCI CKD dataset, demonstrating itseffectiveness for CKD risk stratification across distinct patient cohorts.</description>
      <author>example@mail.com (Mohammad Abdul Hafeez Khan, Twisha Bhattacharyya, Omar Khan, Noorah Khan, Alina Aziz Fatima Khan, Mohammed Qutub Khan, Sujoy Ghosh Hajra)</author>
      <guid isPermaLink="false">2509.12704v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Atomic Data Mining via Multi-Kernel Graph Autoencoders for Machine Learning Force Fields</title>
      <link>http://arxiv.org/abs/2509.12358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为MEAGraph的无监督学习方法，用于分析原子数据集，有效去除采样偏差，优化数据集。&lt;h4&gt;背景&lt;/h4&gt;在计算化学和材料科学中，构建化学多样性的数据集并避免采样偏差对于训练高效且可推广的力场至关重要。然而，许多常见的数据集生成技术容易过度采样势能表面的某些区域，这些区域难以识别和区分，或者与人类直觉不符，使得系统性地去除数据集中的偏差具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别和去除数据集中采样偏差的方法，同时避免信息损失，并能正确识别势能表面的不同区域。&lt;h4&gt;方法&lt;/h4&gt;作者提出了MEAGraph模型，这是一种无监督的原子数据分析方法。该方法结合了多种线性核变换和基于注意力的消息传递，能够捕获几何敏感性，并在不依赖标签或大量训练的情况下实现有效的数据集剪枝。&lt;h4&gt;主要发现&lt;/h4&gt;在铌、钽和铁数据集上的应用表明，MEAGraph能够高效地分组相似的原子环境，使得可以使用基本的剪枝技术来去除采样偏差。&lt;h4&gt;结论&lt;/h4&gt;这种方法为表示学习和聚类提供了有效的方法，可用于数据分析、异常检测和数据集优化。&lt;h4&gt;翻译&lt;/h4&gt;在构建化学多样性数据集的同时避免采样偏差对于训练高效且可推广的力场至关重要。然而，在计算化学和材料科学中，许多常见的数据集生成技术容易过度采样势能表面的某些区域。此外，这些区域可能难以相互识别和分离，或者与人类直觉不太吻合，这使得系统性地去除数据集中的偏差具有挑战性。虽然传统的聚类和修剪方法对此有用，但由于原子描述符的高维度相关困难，它们往往会导致信息损失或无法正确识别势能表面的不同区域。在本工作中，我们引入了多核边缘注意力的图自编码器模型，这是一种用于分析原子数据集的无监督方法。MEAGraph将多种线性核变换与基于注意力的消息传递相结合，以捕获几何敏感性并实现有效的数据集修剪，而不依赖于标签或大量训练。在铌、钽和铁数据集上的应用表明，MEAGraph能够高效地分组相似的原子环境，从而可以使用基本的修剪技术来去除采样偏差。这种方法为表示学习和聚类提供了有效的方法，可用于数据分析、异常检测和数据集优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructing a chemically diverse dataset while avoiding sampling bias iscritical to training efficient and generalizable force fields. However, incomputational chemistry and materials science, many common dataset generationtechniques are prone to oversampling regions of the potential energy surface.Furthermore, these regions can be difficult to identify and isolate from eachother or may not align well with human intuition, making it challenging tosystematically remove bias in the dataset. While traditional clustering andpruning (down-sampling) approaches can be useful for this, they can often leadto information loss or a failure to properly identify distinct regions of thepotential energy surface due to difficulties associated with the highdimensionality of atomic descriptors. In this work, we introduce theMulti-kernel Edge Attention-based Graph Autoencoder (MEAGraph) model, anunsupervised approach for analyzing atomic datasets. MEAGraph combines multiplelinear kernel transformations with attention-based message passing to capturegeometric sensitivity and enable effective dataset pruning without relying onlabels or extensive training. Demonstrated applications on niobium, tantalum,and iron datasets show that MEAGraph efficiently groups similar atomicenvironments, allowing for the use of basic pruning techniques for removingsampling bias. This approach provides an effective method for representationlearning and clustering that can be used for data analysis, outlier detection,and dataset optimization.</description>
      <author>example@mail.com (Hong Sun, Joshua A. Vita, Amit Samanta, Vincenzo Lordi)</author>
      <guid isPermaLink="false">2509.12358v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Disentanglement of Biological and Technical Factors via Latent Space Rotation in Clinical Imaging Improves Disease Pattern Discovery</title>
      <link>http://arxiv.org/abs/2509.11436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Fourth Workshop on Applications of Medical Artificial  Intelligence, AMAI 2025, Held in Conjunction with MICCAI 2025, Daejeon,  Republic of Korea, September 23, 2025, Proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通过机器学习主动学习域偏移的方法，通过数据潜在空间的后旋转实现生物和技术因素的解耦，提高了医学影像数据中疾病相关模式的识别能力。&lt;h4&gt;背景&lt;/h4&gt;在医学影像数据中借助机器学习识别新的疾病相关模式可以扩展可识别发现的词汇，支持诊断和预后评估。然而，图像外观的变化不仅源于生物学差异，还与供应商相关的成像技术、扫描或重建参数有关。这些导致的域偏移阻碍了数据表示学习策略和有意义的生物聚类外观的发现。&lt;h4&gt;目的&lt;/h4&gt;解决医学影像数据中的域偏移问题，使机器学习能够更好地识别疾病相关模式，提高诊断和预后评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入一种通过数据潜在空间的后旋转来主动学习域偏移的方法，实现生物和技术因素的解耦，从而在不同采集设置下实现代表组织类型的稳定聚类。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界异构临床数据上，学习的解耦表示导致跨不同采集设置的稳定聚类；与纠缠表示相比，聚类一致性提高了+19.01%（ARI）、+16.85%（NMI）和+12.39%（Dice），优于四种最先进的调和方法；当使用聚类对特发性肺纤维化患者进行组织成分量化时，学习的特征增强了Cox生存预测。&lt;h4&gt;结论&lt;/h4&gt;所提出的无标签框架通过解耦生物和技术因素，能够有效处理医学影像数据中的域偏移问题，提高聚类一致性，并促进多中心常规成像数据中的生物标志物发现。&lt;h4&gt;翻译&lt;/h4&gt;借助机器学习在医学影像数据中识别新的疾病相关模式扩展了可识别发现的词汇，支持诊断和预后评估。然而，图像外观的变化不仅源于生物学差异，还与供应商相关的成像技术、扫描或重建参数有关。由此产生的域偏移阻碍了数据表示学习策略和有意义的生物聚类外观的发现。为应对这些挑战，我们引入了一种通过数据潜在空间的后旋转来主动学习域偏移的方法，实现生物和技术因素的解耦。真实世界异构临床数据的结果表明，学习的解耦表示导致跨不同采集设置的稳定聚类。与纠缠表示相比，聚类一致性提高了+19.01%（ARI）、+16.85%（NMI）和+12.39%（Dice），优于四种最先进的调和方法。当使用聚类对特发性肺纤维化患者进行组织成分量化时，学习的特征增强了Cox生存预测。这表明所提出的无标签框架促进了多中心常规成像数据中的生物标志物发现。代码可在GitHub https://github.com/cirmuw/latent-space-rotation-disentanglement获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying new disease-related patterns in medical imaging data with thehelp of machine learning enlarges the vocabulary of recognizable findings. Thissupports diagnostic and prognostic assessment. However, image appearance variesnot only due to biological differences, but also due to imaging technologylinked to vendors, scanning- or re- construction parameters. The resultingdomain shifts impedes data representation learning strategies and the discoveryof biologically meaningful cluster appearances. To address these challenges, weintroduce an approach to actively learn the domain shift via post-hoc rotationof the data latent space, enabling disentanglement of biological and technicalfactors. Results on real-world heterogeneous clinical data showcase that thelearned disentangled representation leads to stable clusters representingtissue-types across different acquisition settings. Cluster consistency isimproved by +19.01% (ARI), +16.85% (NMI), and +12.39% (Dice) compared to theentangled representation, outperforming four state-of-the-art harmonizationmethods. When using the clusters to quantify tissue composition on idiopathicpulmonary fibrosis patients, the learned profiles enhance Cox survivalprediction. This indicates that the proposed label-free framework facilitatesbiomarker discovery in multi-center routine imaging data. Code is available onGitHub https://github.com/cirmuw/latent-space-rotation-disentanglement.</description>
      <author>example@mail.com (Jeanny Pan, Philipp Seeböck, Christoph Fürböck, Svitlana Pochepnia, Jennifer Straub, Lucian Beer, Helmut Prosch, Georg Langs)</author>
      <guid isPermaLink="false">2509.11436v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Representation Learning on Large Non-Bipartite Transaction Networks using GraphSAGE</title>
      <link>http://arxiv.org/abs/2509.12255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了GraphSAGE图神经网络框架在银行交易网络中的实际应用，证明了其在处理动态银行数据方面的优势，并展示了其在洗钱检测等实际应用中的价值。&lt;h4&gt;背景&lt;/h4&gt;金融机构日益需要可扩展的工具来分析复杂的交易网络，但传统的图嵌入方法难以处理动态、真实的银行数据。&lt;h4&gt;目的&lt;/h4&gt;展示GraphSAGE（一种归纳图神经网络框架）在银行非二分异构交易网络中的实际应用，并证明其在处理随时间演变的交易数据时的优势。&lt;h4&gt;方法&lt;/h4&gt;构建使用匿名客户和商家交易数据的交易网络，并训练GraphSAGE模型生成节点嵌入，然后将这些嵌入应用于下游分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;对嵌入的探索性工作揭示了与地理和人口统计属性一致的、可解释的聚类；在洗钱检测模型中使用这些嵌入可以改进高风险账户的优先级排序。&lt;h4&gt;结论&lt;/h4&gt;这项研究为金融机构利用图机器学习在交易生态系统中获取可操作的见解提供了蓝图，强调了该框架的归纳能力、可扩展性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;金融机构日益需要可扩展的工具来分析复杂的交易网络，然而传统的图嵌入方法难以处理动态、真实的银行数据。本文展示了GraphSAGE（一种归纳图神经网络框架）在银行非二分异构交易网络中的实际应用。与归纳方法不同，GraphSAGE能够很好地扩展到大型网络，并且可以推广到未见过的节点，这对于处理随时间演变的交易数据的机构至关重要。我们使用匿名客户和商家交易数据构建了一个交易网络，并训练了一个GraphSAGE模型来生成节点嵌入。我们对嵌入的探索性工作揭示了与地理和人口统计属性一致的、可解释的聚类。此外，我们通过将它们应用于洗钱检测模型，说明了它们在下游分类任务中的实用性，使用这些嵌入可以改进高风险账户的优先级排序。除了欺诈检测外，我们的工作强调了该框架对银行规模网络的适应性，强调了其归纳能力、可扩展性和可解释性。本研究为金融机构利用图机器学习在交易生态系统中获取可操作的见解提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-94139-9_17&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial institutions increasingly require scalable tools to analyse complextransactional networks, yet traditional graph embedding methods struggle withdynamic, real-world banking data. This paper demonstrates the practicalapplication of GraphSAGE, an inductive Graph Neural Network framework, tonon-bipartite heterogeneous transaction networks within a banking context.Unlike transductive approaches, GraphSAGE scales well to large networks and cangeneralise to unseen nodes which is critical for institutions working withtemporally evolving transactional data. We construct a transaction networkusing anonymised customer and merchant transactions and train a GraphSAGE modelto generate node embeddings. Our exploratory work on the embeddings revealsinterpretable clusters aligned with geographic and demographic attributes.Additionally, we illustrate their utility in downstream classification tasks byapplying them to a money mule detection model where using these embeddingsimproves the prioritisation of high-risk accounts. Beyond fraud detection, ourwork highlights the adaptability of this framework to banking-scale networks,emphasising its inductive capability, scalability, and interpretability. Thisstudy provides a blueprint for financial organisations to harness graph machinelearning for actionable insights in transactional ecosystems.</description>
      <author>example@mail.com (Mihir Tare, Clemens Rattasits, Yiming Wu, Euan Wielewski)</author>
      <guid isPermaLink="false">2509.12255v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Why and How Auxiliary Tasks Improve JEPA Representations</title>
      <link>http://arxiv.org/abs/2509.12249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了JEPA架构的理论特性，证明了一个'无有害表征崩溃'定理，并通过实验表明联合训练JEPA模型与辅助头能生成更丰富的表征。&lt;h4&gt;背景&lt;/h4&gt;JEPA（Joint-Embedding Predictive Architecture）越来越多地用于视觉表征学习和基于模型的强化学习组件中，但其行为仍然不被充分理解。&lt;h4&gt;目的&lt;/h4&gt;提供一个简单实用的JEPA变体的理论表征，该变体有一个与潜在动力学联合训练的辅助回归头。&lt;h4&gt;方法&lt;/h4&gt;证明了一个'无有害表征崩溃'定理：在确定性MDPs中，如果训练使潜在转换一致性损失和辅助回归损失都趋近于零，那么任何非等价观测（即那些不具有相同转换动力学或辅助标签的观测）必须映射到不同的潜在表征。&lt;h4&gt;主要发现&lt;/h4&gt;辅助任务锚定了表征必须保留的区分。在计数环境中的受控消融实验证实了这一理论，并表明与辅助头联合训练JEPA模型比分别训练它们能生成更丰富的表征。&lt;h4&gt;结论&lt;/h4&gt;我们的工作指出了改进JEPA编码器的一条路径：使用一个辅助函数与它们一起训练，该辅助函数与转换动力学一起编码正确的等价关系。&lt;h4&gt;翻译&lt;/h4&gt;联合嵌入预测架构（JEPA）越来越多地用于视觉表征学习和作为基于模型的强化学习组件，但其行为仍然不被充分理解。我们提供了一个简单实用的JEPA变体的理论表征，该变体有一个与潜在动力学联合训练的辅助回归头。我们证明了一个'无有害表征崩溃'定理：在确定性MDPs中，如果训练使潜在转换一致性损失和辅助回归损失都趋近于零，那么任何非等价观测（即那些不具有相同转换动力学或辅助标签的观测）必须映射到不同的潜在表征。因此，辅助任务锚定了表征必须保留的区分。在计数环境中的受控消融实验证实了这一理论，并表明与辅助头联合训练JEPA模型比分别训练它们能生成更丰富的表征。我们的工作指出了改进JEPA编码器的一条路径：使用一个辅助函数与它们一起训练，该辅助函数与转换动力学一起编码正确的等价关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint-Embedding Predictive Architecture (JEPA) is increasingly used forvisual representation learning and as a component in model-based RL, but itsbehavior remains poorly understood. We provide a theoretical characterizationof a simple, practical JEPA variant that has an auxiliary regression headtrained jointly with latent dynamics. We prove a No Unhealthy RepresentationCollapse theorem: in deterministic MDPs, if training drives both thelatent-transition consistency loss and the auxiliary regression loss to zero,then any pair of non-equivalent observations, i.e., those that do not have thesame transition dynamics or auxiliary label, must map to distinct latentrepresentations. Thus, the auxiliary task anchors which distinctions therepresentation must preserve. Controlled ablations in a counting environmentcorroborate the theory and show that training the JEPA model jointly with theauxiliary head generates a richer representation than training them separately.Our work indicates a path to improve JEPA encoders: training them with anauxiliary function that, together with the transition dynamics, encodes theright equivalence relations.</description>
      <author>example@mail.com (Jiacan Yu, Siyi Chen, Mingrui Liu, Nono Horiuchi, Vladimir Braverman, Zicheng Xu, Dan Haramati, Randall Balestriero)</author>
      <guid isPermaLink="false">2509.12249v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Agents via Continual Pre-training</title>
      <link>http://arxiv.org/abs/2509.13310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为智能体持续预训练(Agentic CPT)的新方法，用于构建强大的智能体基础模型，解决了后训练过程中同时学习多样智能体行为与对齐专家演示之间的优化冲突问题。基于此方法开发的AgentFounder-30B模型在10个基准测试上取得了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大语言模型已发展为能够自主使用工具和多步推理的智能体系统，用于解决复杂问题。然而，基于通用基础模型的后训练方法在智能体任务中表现不佳，特别是在开源实现中。&lt;h4&gt;目的&lt;/h4&gt;构建强大的智能体基础模型，解决后训练过程中同时学习多样智能体行为与对齐专家演示之间的优化冲突问题。&lt;h4&gt;方法&lt;/h4&gt;首次将智能体持续预训练(Agentic CPT)纳入深度研究智能体训练流程，并基于此方法开发了名为AgentFounder的深度研究智能体模型。&lt;h4&gt;主要发现&lt;/h4&gt;AgentFounder-30B在10个基准测试上取得了最先进性能，同时保持强大的工具使用能力，具体表现为：在BrowseComp-en上达到39.9%，在BrowseComp-zh上达到43.3%，在HLE上达到31.5%的Pass@1。&lt;h4&gt;结论&lt;/h4&gt;通过引入Agentic CPT方法，成功构建了强大的智能体基础模型，解决了后训练过程中的优化冲突问题，显著提升了智能体任务性能。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型已经发展为能够自主使用工具和多步推理的智能体系统，用于解决复杂问题。然而，建立在通用基础模型之上的后训练方法在智能体任务中持续表现不佳，特别是在开源实现中。我们确定了根本原因：缺乏强大的智能体基础模型迫使模型在后训练过程中同时学习多样的智能体行为，同时将这些行为与专家演示对齐，从而产生根本的优化冲突。为此，我们首次提出将智能体持续预训练(Agentic CPT)纳入深度研究智能体训练流程，以构建强大的智能体基础模型。基于这种方法，我们开发了一个名为AgentFounder的深度研究智能体模型。我们在10个基准测试上评估了AgentFounder-30B，并取得了最先进的性能，同时保持了强大的工具使用能力，特别是在BrowseComp-en上达到39.9%，在BrowseComp-zh上达到43.3%，以及在HLE上达到31.5%的Pass@1。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have evolved into agentic systems capable ofautonomous tool use and multi-step reasoning for complex problem-solving.However, post-training approaches building upon general-purpose foundationmodels consistently underperform in agentic tasks, particularly in open-sourceimplementations. We identify the root cause: the absence of robust agenticfoundation models forces models during post-training to simultaneously learndiverse agentic behaviors while aligning them to expert demonstrations, therebycreating fundamental optimization tensions. To this end, we are the first topropose incorporating Agentic Continual Pre-training (Agentic CPT) into thedeep research agents training pipeline to build powerful agentic foundationalmodels. Based on this approach, we develop a deep research agent model namedAgentFounder. We evaluate our AgentFounder-30B on 10 benchmarks and achievestate-of-the-art performance while retains strong tool-use ability, notably39.9% on BrowseComp-en, 43.3% on BrowseComp-zh, and 31.5% Pass@1 on HLE.</description>
      <author>example@mail.com (Liangcai Su, Zhen Zhang, Guangyu Li, Zhuo Chen, Chenxi Wang, Maojia Song, Xinyu Wang, Kuan Li, Jialong Wu, Xuanzhong Chen, Zile Qiao, Zhongwang Zhang, Huifeng Yin, Shihao Cai, Runnan Fang, Zhengwei Tao, Wenbiao Yin, Chenxiong Qian, Yong Jiang, Pengjun Xie, Fei Huang, Jingren Zhou)</author>
      <guid isPermaLink="false">2509.13310v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ResidualViT for Efficient Temporally Dense Video Encoding</title>
      <link>http://arxiv.org/abs/2509.13255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作通过创新的架构设计和蒸馏策略，显著提高了时间密集型视频理解任务的计算效率，同时保持了准确性。&lt;h4&gt;背景&lt;/h4&gt;多个视频理解任务需要'时间密集型'推理，如自然语言视频时序定位、时序活动定位和音频描述生成。这些任务需要在高时间分辨率下对帧进行采样，而计算帧级特征对于这些任务来说计算成本很高。&lt;h4&gt;目的&lt;/h4&gt;降低时间密集型任务中特征计算的成本，提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为ResidualViT的视觉Transformer架构，利用视频中的大时间冗余来高效计算时间密集的帧级特征。该架构包含可学习的残差连接确保时间一致性，以及标记减少模块提高处理速度。同时提出了一种轻量级蒸馏策略来近似原始基础模型的帧级特征。&lt;h4&gt;主要发现&lt;/h4&gt;在四个任务和五个数据集上进行了评估，在零样本和全监督设置下都表现出显著效果。计算成本降低了高达60%，推理速度提高了高达2.5倍，同时保持了与原始基础模型相近的准确性。&lt;h4&gt;结论&lt;/h4&gt;ResidualViT架构和蒸馏策略有效地降低了时间密集型视频理解任务的计算成本，同时保持了性能，为视频理解任务提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多个视频理解任务，如自然语言视频时序定位、时序活动定位和音频描述生成，需要在高时间分辨率下采样的帧上进行'时间密集型'推理。然而，考虑到时间分辨率的要求，为这些任务计算帧级特征的计算成本很高。在本文中，我们做出了三项贡献以降低时间密集型任务的特征计算成本。首先，我们引入了一种视觉Transformer架构，称为ResidualViT，它利用视频中的大时间冗余来高效计算时间密集的帧级特征。我们的架构包含(i)确保连续帧之间时间一致性的可学习残差连接，以及(ii)一个标记减少模块，通过选择性丢弃时间冗余信息同时重用预训练基础模型的权重来提高处理速度。其次，我们提出了一种轻量级蒸馏策略来近似原始基础模型的帧级特征。最后，我们在四个任务和五个数据集上评估了我们的方法，在零样本和全监督设置下，展示了计算成本的显著降低（高达60%）和推理速度的改进（高达2.5倍），同时紧密近似了原始基础模型的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several video understanding tasks, such as natural language temporal videogrounding, temporal activity localization, and audio description generation,require "temporally dense" reasoning over frames sampled at high temporalresolution. However, computing frame-level features for these tasks iscomputationally expensive given the temporal resolution requirements. In thispaper, we make three contributions to reduce the cost of computing features fortemporally dense tasks. First, we introduce a vision transformer (ViT)architecture, dubbed ResidualViT, that leverages the large temporal redundancyin videos to efficiently compute temporally dense frame-level features. Ourarchitecture incorporates (i) learnable residual connections that ensuretemporal consistency across consecutive frames and (ii) a token reductionmodule that enhances processing speed by selectively discarding temporallyredundant information while reusing weights of a pretrained foundation model.Second, we propose a lightweight distillation strategy to approximate theframe-level features of the original foundation model. Finally, we evaluate ourapproach across four tasks and five datasets, in both zero-shot and fullysupervised settings, demonstrating significant reductions in computational cost(up to 60%) and improvements in inference speed (up to 2.5x faster), all whileclosely approximating the accuracy of the original foundation model.</description>
      <author>example@mail.com (Mattia Soldan, Fabian Caba Heilbron, Bernard Ghanem, Josef Sivic, Bryan Russell)</author>
      <guid isPermaLink="false">2509.13255v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Don't Forget the Nonlinearity: Unlocking Activation Functions in Efficient Fine-Tuning</title>
      <link>http://arxiv.org/abs/2509.13240v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NoRA是一种创新的参数高效微调(PEFT)框架，首次直接适应预训练Transformer模型中的非线性激活函数，而非仅调整权重矩阵。&lt;h4&gt;背景&lt;/h4&gt;现有的PEFT方法主要适应权重矩阵而保持激活函数固定，限制了模型适应的灵活性。&lt;h4&gt;目的&lt;/h4&gt;引入NoRA框架，探索激活函数作为模型适应一级对象的潜力，提供一种新的参数高效微调方法。&lt;h4&gt;方法&lt;/h4&gt;NoRA用可学习的有理函数替换固定的激活函数，对分子和分母系数应用结构化低秩更新，并采用分组设计实现本地化适应和提高稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;NoRA在视觉Transformer上仅更新0.4%参数即可匹配或超越完全微调；与LoRA结合的NoRA++在多种任务上优于其他方法；NoRA能将适应限制在低维函数子空间，实现隐式正则化。&lt;h4&gt;结论&lt;/h4&gt;激活空间调优是基于权重的PEFT的互补且高度参数高效的替代方案，激活函数应被视为模型适应的一级对象。&lt;h4&gt;翻译&lt;/h4&gt;现有的参数高效微调(PEFT)方法主要适应权重矩阵，同时保持激活函数固定。我们引入NoRA，这是第一个直接适应预训练基于Transformer模型的非线性激活函数的PEFT框架。NoRA用可学习的有理函数替换固定的激活函数，并对分子和分母系数应用结构化低秩更新，采用分组设计使适应本地化并提高稳定性，同时成本最小。在CIFAR-10和CIFAR-100上训练的视觉Transformer上，NoRA在仅更新0.4%参数(0.02M)的情况下匹配或完全微调，实现了+0.17%和+0.27%的准确率提升。与LoRA结合(NoRA++)时，在匹配的训练预算下，通过添加更少的可训练参数，优于LoRA和DoRA。在LLaMA3-8B指令调优上，NoRA++持续提高生成质量，平均MMLU提升+0.3%--0.8%，包括在STEM(Alpaca)上+1.6%和OpenOrca上+1.3%。我们进一步证明NoRA将适应限制在低维函数子空间，隐式正则化更新的幅度和方向。这些结果确立了激活空间调优作为基于权重的PEFT的互补且高度参数高效的替代方案，将激活函数定位为模型适应的一级对象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing parameter-efficient fine-tuning (PEFT) methods primarily adaptweight matrices while keeping activation functions fixed. We introduce\textbf{NoRA}, the first PEFT framework that directly adapts nonlinearactivation functions in pretrained transformer-based models. NoRA replacesfixed activations with learnable rational functions and applies structuredlow-rank updates to numerator and denominator coefficients, with a group-wisedesign that localizes adaptation and improves stability at minimal cost. Onvision transformers trained on CIFAR-10 and CIFAR-100, NoRA matches or exceedsfull fine-tuning while updating only 0.4\% of parameters (0.02M), achievingaccuracy gains of +0.17\% and +0.27\%. When combined with LoRA(\textbf{NoRA++}), it outperforms LoRA and DoRA under matched training budgetsby adding fewer trainable parameters. On LLaMA3-8B instruction tuning, NoRA++consistently improves generation quality, yielding average MMLU gains of+0.3\%--0.8\%, including +1.6\% on STEM (Alpaca) and +1.3\% on OpenOrca. Wefurther show that NoRA constrains adaptation to a low-dimensional functionalsubspace, implicitly regularizing update magnitude and direction. These resultsestablish activation-space tuning as a complementary and highlyparameter-efficient alternative to weight-based PEFT, positioning activationfunctions as first-class objects for model adaptation.</description>
      <author>example@mail.com (Bo Yin, Xingyi Yang, Xinchao Wang)</author>
      <guid isPermaLink="false">2509.13240v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Road Obstacle Video Segmentation</title>
      <link>http://arxiv.org/abs/2509.13181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  GCPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对自动驾驶中道路障碍物分割问题，认识到该任务具有时间相关性，提出了基于视觉基础模型的新方法，在长序列视频分割中取得了最先进的效果。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶代理的广泛部署，道路障碍物的检测与分割对于确保安全导航变得至关重要。然而，现有的道路障碍物分割方法应用于单帧图像，忽略了问题的时间特性，导致连续帧之间的预测图不一致。&lt;h4&gt;目的&lt;/h4&gt;解决现有道路障碍物分割方法在处理连续帧时存在的不一致问题，提高道路障碍物视频分割的准确性，为未来研究提供有价值的见解和方向。&lt;h4&gt;方法&lt;/h4&gt;1. 构建并调整了四个用于道路障碍物视频分割的评估基准；2. 在这些基准上评估了11种最先进的图像和视频分割方法；3. 引入了两种基于视觉基础模型的新型基线方法。&lt;h4&gt;主要发现&lt;/h4&gt;道路障碍物分割任务本质上具有时间特性，因为连续帧的分割图之间存在强相关性。作者提出的方法在长序列视频分割中建立了新的技术水平。&lt;h4&gt;结论&lt;/h4&gt;通过考虑时间相关性，改进的道路障碍物视频分割方法能够提供更一致和准确的预测，这对自动驾驶系统的安全性和可靠性具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;随着自动驾驶代理的广泛部署，道路障碍物的检测与分割已成为确保安全自动驾驶的关键。然而，现有的道路障碍物分割方法应用于单个帧，忽略了问题的时间特性，导致连续帧之间的预测图不一致。在本工作中，我们证明了道路障碍物分割任务本质上具有时间性，因为连续帧的分割图之间存在强相关性。为此，我们整理并调整了四个用于道路障碍物视频分割的评估基准，并评估了11种最先进的基于图像和视频的分割方法。此外，我们引入了两种基于视觉基础模型的有效基线方法。我们的方法在长序列视频的道路障碍物视频分割中建立了新的技术水平，为未来研究提供了宝贵的见解和方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing deployment of autonomous driving agents, the detection andsegmentation of road obstacles have become critical to ensure safe autonomousnavigation. However, existing road-obstacle segmentation methods are applied onindividual frames, overlooking the temporal nature of the problem, leading toinconsistent prediction maps between consecutive frames. In this work, wedemonstrate that the road-obstacle segmentation task is inherently temporal,since the segmentation maps for consecutive frames are strongly correlated. Toaddress this, we curate and adapt four evaluation benchmarks for road-obstaclevideo segmentation and evaluate 11 state-of-the-art image- and video-basedsegmentation methods on these benchmarks. Moreover, we introduce two strongbaseline methods based on vision foundation models. Our approach establishes anew state-of-the-art in road-obstacle video segmentation for long-range videosequences, providing valuable insights and direction for future research.</description>
      <author>example@mail.com (Shyam Nandan Rai, Shyamgopal Karthik, Mariana-Iuliana Georgescu, Barbara Caputo, Carlo Masone, Zeynep Akata)</author>
      <guid isPermaLink="false">2509.13181v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Brought a Gun to a Knife Fight: Modern VFM Baselines Outgun Specialized Detectors on In-the-Wild AI Image Detection</title>
      <link>http://arxiv.org/abs/2509.12995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于现代视觉基础模型的简单线性分类器方法，用于检测AI生成的图像，相比专门设计的检测器在真实场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;专门的AI生成图像检测器在精心设计的基准测试中表现出色，但在真实场景中表现很差，表现为在'野外'基准测试中有极高的假阴性率。&lt;h4&gt;目的&lt;/h4&gt;开发一种更有效的AI生成图像检测方法，能够在真实世界场景中取得更好的性能。&lt;h4&gt;方法&lt;/h4&gt;在现代视觉基础模型(VFM)上使用简单的线性分类器，而不是设计专门的检测器，在相同数据上训练。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基于VFM的方法在'野外'准确率上比专门设计的检测器提高了超过20%；2) 最近的VLM已经学会将合成图像与伪造相关概念对齐；3) 这种对齐能力可能源于模型在预训练期间接触到的数据。&lt;h4&gt;结论&lt;/h4&gt;1) 对于AI生成图像检测，更新后的VFM的原始能力比静态检测器的专门设计更有效；2) 真正的泛化评估需要测试数据独立于模型的整个训练历史，包括预训练。&lt;h4&gt;翻译&lt;/h4&gt;虽然针对AI生成图像的专业检测器在精心设计的基准测试中表现出色，但它们在真实场景中的表现灾难性地差，正如它们在'野外'基准测试中极高的假阴性率所证明的那样。我们不是为这个问题再打造一把专门的'刀'，而是带来一把'枪'：在现代视觉基础模型(VFM)上使用一个简单的线性分类器。在相同数据上训练后，这个基线方法明显优于专门设计的检测器，在'野外'准确率上提高了超过20%。我们的分析确定了VFM'火力'的来源：首先，通过探测文本-图像相似性，我们发现最近的VLM(如Perception Encoder、Meta CLIP2)已经学会将合成图像与伪造相关概念(如'AI生成')对齐，而之前的版本则没有。其次，我们推测这是由于数据暴露，因为在一个在VFM预训练截止日期后抓取的新数据集上，这种对齐和整体准确率都大幅下降，确保了在预训练期间未见。我们的研究得出两个关键结论：1) 对于AI生成图像检测的真实'枪战'，更新后的VFM的原始'火力'比静态检测器的'工艺'更有效。2) 真正的泛化评估需要测试数据独立于模型的整个训练历史，包括预训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While specialized detectors for AI-generated images excel on curatedbenchmarks, they fail catastrophically in real-world scenarios, as evidenced bytheir critically high false-negative rates on `in-the-wild' benchmarks. Insteadof crafting another specialized `knife' for this problem, we bring a `gun' tothe fight: a simple linear classifier on a modern Vision Foundation Model(VFM). Trained on identical data, this baseline decisively `outguns' bespokedetectors, boosting in-the-wild accuracy by a striking margin of over 20\%.  Our analysis pinpoints the source of the VFM's `firepower': First, by probingtext-image similarities, we find that recent VLMs (e.g., Perception Encoder,Meta CLIP2) have learned to align synthetic images with forgery-relatedconcepts (e.g., `AI-generated'), unlike previous versions. Second, we speculatethat this is due to data exposure, as both this alignment and overall accuracyplummet on a novel dataset scraped after the VFM's pre-training cut-off date,ensuring it was unseen during pre-training. Our findings yield two criticalconclusions: 1) For the real-world `gunfight' of AI-generated image detection,the raw `firepower' of an updated VFM is far more effective than the`craftsmanship' of a static detector. 2) True generalization evaluationrequires test data to be independent of the model's entire training history,including pre-training.</description>
      <author>example@mail.com (Yue Zhou, Xinan He, Kaiqing Lin, Bing Fan, Feng Ding, Jinhua Zeng, Bin Li)</author>
      <guid isPermaLink="false">2509.12995v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Performance Gaps for Foundation Models: A Post-Training Strategy for ECGFounder</title>
      <link>http://arxiv.org/abs/2509.12991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A simple yet effective strategy for ECG foundation models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单而有效的后训练方法，用于增强ECG基础模型，显著提升了其在临床应用中的性能，特别是在有限数据情况下表现优异。&lt;h4&gt;背景&lt;/h4&gt;ECG基础模型因其跨任务适应性而日益流行，但与特定任务模型相比存在性能差距，即使在大型数据集预训练和目标数据微调后仍然存在，这可能是由于缺乏有效的后训练策略。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而有效的后训练方法来增强ECG Founder模型（一种在超过700万个ECG记录上预训练的最先进基础模型），提高其临床适用性。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单而有效的后训练方法应用于ECG Founder模型，并在PTB-XL基准测试上进行评估，同时进行了消融研究以确定关键性能贡献组件。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在宏观AUROC上比基线微调策略提高1.2%-3.3%，在宏观AUPRC上提高5.3%-20.9%；优于多种先进方法；仅使用10%训练数据时，宏观AUROC提高9.1%，宏观AUPRC提高34.9%；随机深度和预览线性探测是关键性能提升组件。&lt;h4&gt;结论&lt;/h4&gt;后训练策略在改进ECG基础模型方面具有显著潜力，这项工作有望促进ECG领域基础模型的持续发展。&lt;h4&gt;翻译&lt;/h4&gt;ECG基础模型因其跨任务的适应性而日益流行。然而，与特定任务模型相比，它们的临床适用性通常受到性能差距的限制，即使在大型ECG数据集上进行预训练并在目标数据上进行微调后也是如此。这种局限性可能是由于缺乏有效的后训练策略。在本文中，我们提出了一种简单而有效的后训练方法来增强ECG Founder模型，这是一种在超过700万个ECG记录上预训练的最先进ECG基础模型。在PTB-XL基准测试上的实验表明，我们的方法在宏观AUROC上比基线微调策略提高了1.2%-3.3%，在宏观AUPRC上提高了5.3%-20.9%。此外，我们的方法优于几种最近的先进方法，包括特定任务模型和高级架构。进一步评估显示，与基线相比，我们的方法更稳定且样本效率更高，仅使用10%的训练数据即可实现9.1%的宏观AUROC和34.9%的宏观AUPRC改进。消融研究确定了随机深度和预览线性探测等关键组件，这些组件有助于提高性能。这些发现强调了后训练策略在改进ECG基础模型方面的潜力，我们希望这项工作将促进ECG领域基础模型的持续发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; ECG foundation models are increasingly popular due to their adaptabilityacross various tasks. However, their clinical applicability is often limited byperformance gaps compared to task-specific models, even after pre-training onlarge ECG datasets and fine-tuning on target data. This limitation is likelydue to the lack of an effective post-training strategy. In this paper, wepropose a simple yet effective post-training approach to enhance ECGFounder, astate-of-the-art ECG foundation model pre-trained on over 7 million ECGrecordings. Experiments on the PTB-XL benchmark show that our approach improvesthe baseline fine-tuning strategy by 1.2%-3.3% in macro AUROC and 5.3%-20.9% inmacro AUPRC. Additionally, our method outperforms several recentstate-of-the-art approaches, including task-specific and advancedarchitectures. Further evaluation reveals that our method is more stable andsample-efficient compared to the baseline, achieving a 9.1% improvement inmacro AUROC and a 34.9% improvement in macro AUPRC using just 10% of thetraining data. Ablation studies identify key components, such as stochasticdepth and preview linear probing, that contribute to the enhanced performance.These findings underscore the potential of post-training strategies to improveECG foundation models, and we hope this work will contribute to the continueddevelopment of foundation models in the ECG domain.</description>
      <author>example@mail.com (Ya Zhou, Yujie Yang, Xiaohan Fan, Wei Zhao)</author>
      <guid isPermaLink="false">2509.12991v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Data Scaling Laws for Radiology Foundation Models</title>
      <link>http://arxiv.org/abs/2509.12818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统性地研究了两种医学影像视觉编码器MI2和RAD-DINO在大规模胸部X光片数据上的持续预训练效果，评估了模型在分类、分割和放射报告生成任务上的表现，发现不同模型在不同任务上有各自优势，并证明了特定中心持续预训练的实用价值。&lt;h4&gt;背景&lt;/h4&gt;基础视觉编码器如CLIP和DINOv2经过网络规模数据训练后展现出强大的跨任务性能，但医学影像基础模型受限于较小的数据集，限制了我们对数据规模和预训练范式如何影响性能的理解。&lt;h4&gt;目的&lt;/h4&gt;系统研究两种代表CLIP和DINOv2范式的视觉编码器MI2和RAD-DINO在350万张胸部X光片上的持续预训练效果，同时保持计算和评估协议不变。&lt;h4&gt;方法&lt;/h4&gt;在350万张胸部X光片上持续预训练两种视觉编码器；评估模型在分类、分割和放射报告生成任务上的表现；包含线条和导管任务以平衡偏倚；使用UniCL结合报告和结构化标签预训练MI2；测试仅需少量领域内样本即可超越开放权重基础模型的场景。&lt;h4&gt;主要发现&lt;/h4&gt;MI2在发现相关任务上扩展更有效，RAD-DINO在导管相关任务上更强；使用报告和结构化标签通过UniCL持续预训练MI2可提升性能，突显大规模结构化监督的价值；对于某些任务，仅需30k领域内样本就足以超越开放权重基础模型。&lt;h4&gt;结论&lt;/h4&gt;特定中心持续预训练具有实用价值，医疗机构可利用领域内数据获得显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;基础视觉编码器如CLIP和DINOv2经过网络规模数据训练后，在任务和数据集间展现出强大的转移性能。然而，医学影像基础模型仍受限于较小的数据集，限制了我们对数据规模和预训练范式在此环境中如何影响性能的理解。在本工作中，我们系统地研究了两种视觉编码器（代表CLIP和DINOv2两种主要编码器范式的MedImageInsight (MI2)和RAD-DINO）在多达350万张来自单一机构的胸部X光片上的持续预训练，同时保持计算和评估协议不变。我们在分类（放射学发现、线条和导管）、分割（线条和导管）和放射报告生成任务上进行了评估。尽管先前工作主要关注与放射学发现相关的任务，但我们包含了线条和导管任务，以平衡这种偏倚，并评估模型提取沿 elongated 结构保持连续性的特征的能力。我们的实验表明，MI2在发现相关任务上扩展更有效，而RAD-DINO在导管相关任务上更强。令人惊讶的是，使用报告和结构化标签通过UniCL持续预训练MI2可以提高性能，突显了大规模结构化监督的价值。我们进一步表明，对于某些任务，仅需30k领域内样本就足以超越开放权重基础模型。这些结果突显了特定中心持续预训练的实用性，使医疗机构能够利用领域内数据获得显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation vision encoders such as CLIP and DINOv2, trained on web-scaledata, exhibit strong transfer performance across tasks and datasets. However,medical imaging foundation models remain constrained by smaller datasets,limiting our understanding of how data scale and pretraining paradigms affectperformance in this setting. In this work, we systematically study continualpretraining of two vision encoders, MedImageInsight (MI2) and RAD-DINOrepresenting the two major encoder paradigms CLIP and DINOv2, on up to 3.5Mchest x-rays from a single institution, holding compute and evaluationprotocols constant. We evaluate on classification (radiology findings, linesand tubes), segmentation (lines and tubes), and radiology report generation.While prior work has primarily focused on tasks related to radiology findings,we include lines and tubes tasks to counterbalance this bias and evaluate amodel's ability to extract features that preserve continuity along elongatedstructures. Our experiments show that MI2 scales more effectively forfinding-related tasks, while RAD-DINO is stronger on tube-related tasks.Surprisingly, continually pretraining MI2 with both reports and structuredlabels using UniCL improves performance, underscoring the value of structuredsupervision at scale. We further show that for some tasks, as few as 30kin-domain samples are sufficient to surpass open-weights foundation models.These results highlight the utility of center-specific continual pretraining,enabling medical institutions to derive significant performance gains byutilizing in-domain data.</description>
      <author>example@mail.com (Maximilian Ilse, Harshita Sharma, Anton Schwaighofer, Sam Bond-Taylor, Fernando Pérez-García, Olesya Melnichenko, Anne-Marie G. Sykes, Kelly K. Horst, Ashish Khandelwal, Maxwell Reynolds, Maria T. Wetscherek, Noel C. F. Codella, Javier Alvarez-Valle, Korfiatis Panagiotis, Valentina Salvatelli)</author>
      <guid isPermaLink="false">2509.12818v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Bi-level Personalization for Federated Foundation Models: A Task-vector Aggregation Approach</title>
      <link>http://arxiv.org/abs/2509.12697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种双层个性化框架，用于联邦基础模型的微调，以解决在有限数据情况下个性化与联邦之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;联邦基础模型代表了一种新的范式，可以在客户端之间联合微调预训练的基础模型。然而，为少数新用户或专门场景微调基础模型仍然是一个挑战，因为这些场景涉及的数据规模通常远小于预训练时使用的大规模数据。&lt;h4&gt;目的&lt;/h4&gt;解决在有限数据情况下个性化与联邦之间的权衡问题，特别是在为少数新用户或专门场景微调基础模型时。&lt;h4&gt;方法&lt;/h4&gt;提出了一种双层个性化框架：在客户端层面，使用私有数据进行个性化微调；在服务器层面，使用基于客户端特定任务向量测量的相似用户进行个性化聚合。&lt;h4&gt;主要发现&lt;/h4&gt;通过客户端微调获得的个性化信息，服务器层面的个性化聚合可以获得群体级别的个性化信息，同时减轻非独立同分布数据中不相关或利益冲突客户端的干扰。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法在基准数据集上的广泛实验分析中证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;联邦基础模型代表了一种新的范式，可以在客户端之间联合微调预训练的基础模型。为少数新用户或专门场景微调基础模型仍然是一个挑战，这些场景涉及的数据规模通常远小于预训练时使用的大规模数据。在这种情况下，个性化与联邦之间的权衡变得更加敏感。为了解决这些问题，我们提出了一种用于联邦基础模型微调的双层个性化框架。具体来说，我们在客户端层面使用私有数据进行个性化微调，然后在服务器层面使用基于客户端特定任务向量测量的相似用户进行个性化聚合。鉴于从客户端微调中获得的个性化信息，服务器层面的个性化聚合可以获得群体级别的个性化信息，同时减轻非独立同分布数据中不相关或利益冲突客户端的干扰。所提出的算法在基准数据集上的广泛实验分析中证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated foundation models represent a new paradigm to jointly fine-tunepre-trained foundation models across clients. It is still a challenge tofine-tune foundation models for a small group of new users or specializedscenarios, which typically involve limited data compared to the large-scaledata used in pre-training. In this context, the trade-off betweenpersonalization and federation becomes more sensitive. To tackle these, weproposed a bi-level personalization framework for federated fine-tuning onfoundation models. Specifically, we conduct personalized fine-tuning on theclient-level using its private data, and then conduct a personalizedaggregation on the server-level using similar users measured by client-specifictask vectors. Given the personalization information gained from client-levelfine-tuning, the server-level personalized aggregation can gain group-wisepersonalization information while mitigating the disturbance of irrelevant orinterest-conflict clients with non-IID data. The effectiveness of the proposedalgorithm has been demonstrated by extensive experimental analysis in benchmarkdatasets.</description>
      <author>example@mail.com (Yiyuan Yang, Guodong Long, Qinghua Lu, Liming Zhu, Jing Jiang)</author>
      <guid isPermaLink="false">2509.12697v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Intermediate Representations of Time Series Foundation Models for Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.12650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages,8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TimeRep是一种利用时间序列基础模型中间层表示进行异常检测的新方法，通过计算表示之间的距离作为异常分数，并采用核心集策略和适应机制处理概念漂移。&lt;h4&gt;背景&lt;/h4&gt;时间序列数据中的异常检测对许多现实系统的可靠运行至关重要，时间序列基础模型已成为异常检测的有力工具。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖于TSFMs最终层表示，而是利用中间层表示进行异常检测的新方法。&lt;h4&gt;方法&lt;/h4&gt;TimeRep选择最具信息量的中间层和patch-token位置，从训练数据形成中间表示参考集合，应用核心集策略减小集合大小，在推理时通过测量距离计算异常分数，并集成适应机制处理概念漂移。&lt;h4&gt;主要发现&lt;/h4&gt;在包含250个单变量时间序列的UCR异常档案上进行的实验表明，TimeRep始终优于各种最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;TimeRep是一种有效的时间序列异常检测方法，通过利用中间层表示而非最终层表示，结合核心集策略和适应机制，在各种基线方法上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;检测时间序列数据中的异常对于许多现实世界系统的可靠运行至关重要。最近，时间序列基础模型(TSFMs)已成为异常检测的有力工具。然而，现有方法通常依赖于TSFMs的最终层表示，通过特定任务头来计算异常分数作为重建或预测误差。相反，我们提出了TimeRep，一种新颖的异常检测方法，利用TSFMs的中间层表示，将这些表示之间的距离计算为异常分数。给定预训练的TSFM，TimeRep选择产生最具信息量表示的中间层和patch-token位置。TimeRep从训练数据形成中间表示的参考集合，并应用core-set策略来减小其大小，同时保持分布覆盖。在推理期间，TimeRep通过测量其中间表示与集合中中间表示之间的距离来计算传入数据的异常分数。为了处理概念漂移，TimeRep集成了一个适应机制，在推理时，仅用来自传入数据的非冗余中间表示来增强集合。我们在包含250个单变量时间序列的UCR异常档案上进行了广泛的实验。TimeRep始终优于各种最先进的基线方法，包括非深度学习、深度学习和基于基础模型的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in time series data is essential for the reliableoperation of many real-world systems. Recently, time series foundation models(TSFMs) have emerged as a powerful tool for anomaly detection. However,existing methods typically rely on the final layer's representations of TSFMs,computing the anomaly score as a reconstruction or forecasting error via atask-specific head. Instead, we propose TimeRep, a novel anomaly detectionapproach that leverages the intermediate layer's representations of TSFMs,computing the anomaly score as the distance between these representations.Given a pre-trained TSFM, TimeRep selects the intermediate layer andpatch-token position that yield the most informative representation. TimeRepforms a reference collection of intermediate representations from the trainingdata and applies a core-set strategy to reduce its size while maintainingdistributional coverage. During inference, TimeRep computes the anomaly scorefor incoming data by measuring the distance between its intermediaterepresentations and those of the collection. To address concept drift, TimeRepintegrates an adaptation mechanism that, at inference time, augments thecollection exclusively with non-redundant intermediate representations fromincoming data. We conducted extensive experiments on the UCR Anomaly Archive,which contains 250 univariate time series. TimeRep consistently outperforms abroad spectrum of state-of-the-art baselines, including non-DL, DL, andfoundation model-based methods.</description>
      <author>example@mail.com (Chan Sik Han, Keon Myung Lee)</author>
      <guid isPermaLink="false">2509.12650v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Foundation Model to Enhance Generalizability and Data Efficiency for Pan-cancer Prognosis Prediction</title>
      <link>http://arxiv.org/abs/2509.12600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MICE多模态基础模型，通过整合病理图像、临床报告和基因组数据，实现了精确的泛癌预后预测，显著提升了模型的泛化性和数据效率。&lt;h4&gt;背景&lt;/h4&gt;多模态数据为全面理解肿瘤微环境提供了异构信息，但现有AI模型往往难以有效利用多模态数据中的丰富信息，提取的泛化性较差。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合病理图像、临床报告和基因组数据的多模态基础模型，用于精确的泛癌预后预测。&lt;h4&gt;方法&lt;/h4&gt;提出了MICE（Multimodal data Integration via Collaborative Experts）多模态基础模型，采用多种功能不同的专家全面捕捉跨癌种和癌种特定洞察，利用来自30种癌症类型、11,799名患者的数据，通过对比学习和监督学习相结合增强MICE的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;MICE优于单模态和最先进的基于多专家的多模态模型，在内部队列中C-index提高了3.8%至11.2%，在独立队列中提高了5.8%至8.8%，同时在多样化的临床场景中表现出卓越的数据效率。&lt;h4&gt;结论&lt;/h4&gt;MICE具有增强的泛化性和数据效率，为泛癌预后预测提供了有效且可扩展的基础，有潜力个性化定制治疗并改善治疗效果。&lt;h4&gt;翻译&lt;/h4&gt;多模态数据为全面理解肿瘤微环境提供了异构信息。然而，现有的人工智能模型往往难以利用多模态数据中的丰富信息，提取的泛化性较差。在此，我们提出了MICE（通过协作专家进行多模态数据集成），这是一个多模态基础模型，能够有效整合病理图像、临床报告和基因组数据，用于精确的泛癌预后预测。MICE采用多种功能不同的专家，全面捕捉跨癌种和癌种特定的洞察，而非传统的多专家模块。利用来自30种癌症类型、11,799名患者的数据，我们通过结合对比学习和监督学习增强了MICE的泛化能力。MICE优于单模态和最先进的基于多专家的多模态模型，在内部队列中C-index显著提高了3.8%至11.2%，在独立队列中提高了5.8%至8.8%。此外，它在多样化的临床场景中表现出卓越的数据效率。凭借其增强的泛化性和数据效率，MICE为泛癌预后预测建立了有效且可扩展的基础，具有为个性化定制治疗和改善治疗效果提供强大潜力的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal data provides heterogeneous information for a holisticunderstanding of the tumor microenvironment. However, existing AI models oftenstruggle to harness the rich information within multimodal data and extractpoorly generalizable representations. Here we present MICE (Multimodal dataIntegration via Collaborative Experts), a multimodal foundation model thateffectively integrates pathology images, clinical reports, and genomics datafor precise pan-cancer prognosis prediction. Instead of conventionalmulti-expert modules, MICE employs multiple functionally diverse experts tocomprehensively capture both cross-cancer and cancer-specific insights.Leveraging data from 11,799 patients across 30 cancer types, we enhanced MICE'sgeneralizability by coupling contrastive and supervised learning. MICEoutperformed both unimodal and state-of-the-art multi-expert-based multimodalmodels, demonstrating substantial improvements in C-index ranging from 3.8% to11.2% on internal cohorts and 5.8% to 8.8% on independent cohorts,respectively. Moreover, it exhibited remarkable data efficiency across diverseclinical scenarios. With its enhanced generalizability and data efficiency,MICE establishes an effective and scalable foundation for pan-cancer prognosisprediction, holding strong potential to personalize tailored therapies andimprove treatment outcomes.</description>
      <author>example@mail.com (Huajun Zhou, Fengtao Zhou, Jiabo Ma, Yingxue Xu, Xi Wang, Xiuming Zhang, Li Liang, Zhenhui Li, Hao Chen)</author>
      <guid isPermaLink="false">2509.12600v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>DinoAtten3D: Slice-Level Attention Aggregation of DinoV2 for 3D Brain MRI Anomaly Classification</title>
      <link>http://arxiv.org/abs/2509.12512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACCEPTED at the ICCV 2025 Workshop on Anomaly Detection with  Foundation Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于注意力的全局聚合框架，用于3D医学图像异常分类，利用预训练的DINOv2模型和软注意机制处理脑部MRI切片，并通过复合损失函数解决数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;医学影像中的异常检测和分类对早期诊断至关重要，但由于标注数据有限、类别不平衡和专家标注的高成本，这仍然是一个挑战。新兴的视觉基础模型如DINOv2在大量无标注数据上预训练，可提供通用表示来缓解这些限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门用于3D医学图像异常分类的基于注意力的全局聚合框架，利用自监督DINOv2模型作为预训练特征提取器。&lt;h4&gt;方法&lt;/h4&gt;处理脑部MRI的2D轴向切片，通过软注意机制分配自适应的切片级重要性权重，并采用复合损失函数结合监督对比学习和类别方差正则化来增强类间分离性和类内一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在ADNI数据集和机构多类别头痛队列上的验证表明，尽管数据有限且类别不平衡显著，该方法仍表现出强大的异常分类性能。&lt;h4&gt;结论&lt;/h4&gt;利用预训练的2D基础模型结合基于注意力的切片聚合，对于实现医学影像中鲁棒的体积异常检测是有效的，该方法已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;医学影像中的异常检测和分类对早期诊断至关重要，但由于标注数据有限、类别不平衡和专家标注的高成本，这仍然具有挑战性。新兴的视觉基础模型（如DINOv2）在大量无标注数据上预训练，可提供通用表示， potentially缓解这些限制。在本研究中，我们提出了一种专门用于3D医学图像异常分类的基于注意力的全局聚合框架。利用自监督DINOv2模型作为预训练特征提取器，我们的方法处理脑部MRI的2D轴向切片，通过软注意机制分配自适应的切片级重要性权重。为进一步解决数据稀缺问题，我们采用结合监督对比学习和类别方差正则化的复合损失函数，增强类间分离性和类内一致性。我们在ADNI数据集和机构多类别头痛队列上验证了我们的框架，尽管数据有限且类别不平衡显著，但仍表现出强大的异常分类性能。我们的结果突显了利用预训练的2D基础模型结合基于注意力的切片聚合进行医学影像中鲁棒体积异常检测的有效性。我们的实现已在https://github.com/Rafsani/DinoAtten3D.git上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学影像（特别是3D脑部MRI）中异常检测和分类的挑战，包括标注数据有限、类别不平衡和专家标注成本高的问题。这个问题在现实中非常重要，因为早期疾病诊断对治疗和预后至关重要，而医学数据获取和标注成本高昂限制了监督学习方法的应用，类别不平衡问题也影响模型泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统监督学习方法需要大量标注数据且易过拟合，无监督方法（如基于GAN或扩散模型）仅依赖健康数据重建，可能因健康数据有限而泛化能力不足。作者注意到基础模型（如DINOv2）在大规模无标签数据上预训练的潜力，但识别到这些模型本质上是2D的，无法直接处理3D医学影像。作者借鉴了注意力池化在高维医学图像分析中的有效性、监督对比学习在处理不平衡分类问题上的进展（如SC-MIL）以及DINOv2作为特征提取器的泛化能力，设计了结合注意力机制和基础模型的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的2D基础模型（DINOv2）处理3D医学影像的各个2D切片，通过软注意力机制自适应地聚合不同切片的特征，为诊断相关的切片分配更高权重，并结合监督对比学习和类方差正则化处理数据稀缺和类别不平衡问题。整体流程包括：1)将3D MRI分解为2D切片，用DINOv2提取每个切片的特征嵌入；2)使用两层MLN学习注意力分数，通过softmax转换为权重，计算加权聚合特征；3)将聚合特征通过MLP映射后进行分类；4)结合交叉熵损失、监督对比损失和类内方差损失进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全局注意力驱动的3D医学影像聚合框架，自适应融合切片级嵌入；2)针对数据稀缺和类别不平衡的组合损失函数；3)在两个真实临床队列（ADNI和头痛队列）上验证。相比之前的工作，不同之处在于：不同于传统MIL方法可能忽略跨切片病理特征，DinoAtten3D使用全局注意力聚合切片级特征；不同于直接应用2D基础模型到3D影像，通过注意力机制桥接2D能力和体积特性；不同于单一监督对比学习，结合类方差正则化专门针对医学影像数据稀缺和类别不平衡问题设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DinoAtten3D通过结合DINOv2基础模型的特征提取能力和切片级软注意力机制，为3D脑部MRI图像异常检测提供了一个高效、数据友好的解决方案，在数据有限和类别不平衡的情况下仍能保持强大的分类性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection and classification in medical imaging are critical forearly diagnosis but remain challenging due to limited annotated data, classimbalance, and the high cost of expert labeling. Emerging vision foundationmodels such as DINOv2, pretrained on extensive, unlabeled datasets, offergeneralized representations that can potentially alleviate these limitations.In this study, we propose an attention-based global aggregation frameworktailored specifically for 3D medical image anomaly classification. Leveragingthe self-supervised DINOv2 model as a pretrained feature extractor, our methodprocesses individual 2D axial slices of brain MRIs, assigning adaptiveslice-level importance weights through a soft attention mechanism. To furtheraddress data scarcity, we employ a composite loss function combining supervisedcontrastive learning with class-variance regularization, enhancing inter-classseparability and intra-class consistency. We validate our framework on the ADNIdataset and an institutional multi-class headache cohort, demonstrating stronganomaly classification performance despite limited data availability andsignificant class imbalance. Our results highlight the efficacy of utilizingpretrained 2D foundation models combined with attention-based slice aggregationfor robust volumetric anomaly detection in medical imaging. Our implementationis publicly available at https://github.com/Rafsani/DinoAtten3D.git.</description>
      <author>example@mail.com (Fazle Rafsani, Jay Shah, Catherine D. Chong, Todd J. Schwedt, Teresa Wu)</author>
      <guid isPermaLink="false">2509.12512v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Prediction and Causality of functional MRI and synthetic signal using a Zero-Shot Time-Series Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12497v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作比较了基础模型与传统方法在神经科学中的时间序列预测和因果发现能力，发现基础模型在零样本设置下表现良好，能够更精确地检测因果相互作用。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测和因果发现在神经科学中处于核心地位，因为预测大脑活动并识别神经群体和回路之间的因果关系可以阐明认知和疾病背后的机制。随着基础模型的兴起，一个开放性问题是如何它们与传统方法在脑信号预测和因果分析方面的比较，以及它们是否可以在零样本设置中应用。&lt;h4&gt;目的&lt;/h4&gt;评估一个基础模型与传统方法在推断人类功能性磁共振成像（fMRI）测量的自发性大脑活动中的方向性相互作用方面的比较。&lt;h4&gt;方法&lt;/h4&gt;研究人员测试了基础模型在零样本和微调设置中的预测能力，通过将模型中的类Granger估计与标准Granger因果性进行比较来评估因果性，并使用从真实因果模型生成的合成时间序列（包括逻辑映射耦合和Ornstein-Uhlenbeck过程）验证了该方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在零样本预测fMRI时间序列方面具有竞争力（对照组平均绝对百分误差为0.55，患者组为0.27）；尽管标准Granger因果性没有显示出模型之间的明显定量差异，但基础模型提供了更精确的因果相互作用检测。&lt;h4&gt;结论&lt;/h4&gt;总的来说，这些发现表明基础模型提供了多功能性、强大的零样本性能，以及在时间序列数据中进行预测和因果发现的潜在效用。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测和因果发现在神经科学中处于核心地位，因为预测大脑活动并识别神经群体和回路之间的因果关系可以阐明认知和疾病背后的机制。随着基础模型的兴起，一个开放性问题是如何它们与传统方法在脑信号预测和因果分析方面的比较，以及它们是否可以在零样本设置中应用。在这项工作中，我们评估了一个基础模型与传统方法在推断人类功能性磁共振成像（fMRI）测量的自发性大脑活动中的方向性相互作用方面的比较。传统方法通常依赖于Wiener-Granger因果性。我们测试了基础模型在零样本和微调设置中的预测能力，并通过将模型中的类Granger估计与标准Granger因果性比较来评估因果性。我们使用从真实因果模型生成的合成时间序列（包括逻辑映射耦合和Ornstein-Uhlenbeck过程）验证了该方法。基础模型在零样本预测fMRI时间序列方面具有竞争力（对照组平均绝对百分误差为0.55，患者组为0.27）。尽管标准Granger因果性没有显示出模型之间的明显定量差异，但基础模型提供了更精确的因果相互作用检测。总的来说，这些发现表明基础模型提供了多功能性、强大的零样本性能，以及在时间序列数据中进行预测和因果发现的潜在效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series forecasting and causal discovery are central in neuroscience, aspredicting brain activity and identifying causal relationships between neuralpopulations and circuits can shed light on the mechanisms underlying cognitionand disease. With the rise of foundation models, an open question is how theycompare to traditional methods for brain signal forecasting and causalityanalysis, and whether they can be applied in a zero-shot setting. In this work,we evaluate a foundation model against classical methods for inferringdirectional interactions from spontaneous brain activity measured withfunctional magnetic resonance imaging (fMRI) in humans. Traditional approachesoften rely on Wiener-Granger causality. We tested the forecasting ability ofthe foundation model in both zero-shot and fine-tuned settings, and assessedcausality by comparing Granger-like estimates from the model with standardGranger causality. We validated the approach using synthetic time seriesgenerated from ground-truth causal models, including logistic map coupling andOrnstein-Uhlenbeck processes. The foundation model achieved competitivezero-shot forecasting fMRI time series (mean absolute percentage error of 0.55in controls and 0.27 in patients). Although standard Granger causality did notshow clear quantitative differences between models, the foundation modelprovided a more precise detection of causal interactions.  Overall, these findings suggest that foundation models offer versatility,strong zero-shot performance, and potential utility for forecasting and causaldiscovery in time-series data.</description>
      <author>example@mail.com (Alessandro Crimi, Andrea Brovelli)</author>
      <guid isPermaLink="false">2509.12497v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Towards Foundational Models for Single-Chip Radar</title>
      <link>http://arxiv.org/abs/2509.12482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文收集了最大的原始雷达数据集(100万个样本，29小时)，训练了通用雷达变换器(GRT)基础模型，用于4D单芯片雷达的3D占用预测和语义分割任务，实现了通常需要更高分辨率传感器才能达到的质量水平。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达是紧凑、廉价、耐用的传感器，对遮挡和环境条件(如天气和黑暗)具有鲁棒性，但角度分辨率较差，特别是廉价的单芯片雷达。虽然已有基于学习的方法来缓解这一弱点，但毫米波雷达缺乏标准的基础模型或大型数据集，实践者通常使用较小的数据集从头训练特定任务模型。&lt;h4&gt;目的&lt;/h4&gt;收集大型原始雷达数据集，训练适用于4D单芯片雷达的基础模型，实现高质量的3D占用预测和语义分割。&lt;h4&gt;方法&lt;/h4&gt;收集100万个样本(29小时)的原始雷达数据集，开发Generalizable Radar Transformer (GRT)模型，进行广泛的消融实验，评估原始雷达数据与有损表示的性能差异。&lt;h4&gt;主要发现&lt;/h4&gt;GRT能在不同环境中泛化，可针对不同任务微调，数据呈对数缩放比例(每增加10倍数据，性能提高20%)；使用原始雷达数据显著优于有损表示，相当于增加10倍训练数据的效果；估计需要约1亿个样本(3000小时)的数据才能完全发挥GRT潜力。&lt;h4&gt;结论&lt;/h4&gt;通过大型数据集和GRT模型解决了毫米波雷达角度分辨率差的问题，原始数据比有损表示更有效，GRT具有良好的泛化能力和可微调性，未来需要更多数据进一步发挥模型潜力。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达是紧凑、廉价且耐用的传感器，对遮挡具有鲁棒性，并且在天气和黑暗等环境条件下仍能正常工作。然而，这以较差的角度分辨率为代价，特别是对于通常用于汽车和室内感应应用的廉价单芯片雷达。尽管许多人提出了基于学习的方法来缓解这一弱点，但毫米波雷达尚未出现标准的基础模型或大型数据集，实践者主要使用相对较小的数据集从头开始训练特定任务模型。在本文中，我们收集了(据我们所知)最大的可用原始雷达数据集，包含100万个样本(29小时)，并训练了一个用于4D单芯片雷达的基础模型，该模型可以预测3D占用和进行语义分割，其质量通常只有在使用更高分辨率传感器时才能实现。我们证明了我们的通用雷达变换器(GRT)能够在不同环境中泛化，可以针对不同任务进行微调，并显示出每增加10倍数据性能提升20%的对数数据缩放比例。我们还对常见设计决策进行了广泛的消融实验，发现使用原始雷达数据显著优于广泛使用的有损表示，相当于增加10倍训练数据的效果。最后，我们大致估计需要约1亿个样本(3000小时)的数据才能完全发挥GRT的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单芯片毫米波雷达缺乏基础模型和大规模数据集的问题。毫米波雷达虽然体积小、成本低、耐用且不受环境条件影响，但角度分辨率差，限制了其在自动驾驶、室内感知等领域的应用。目前实践者大多使用小数据集从头训练特定任务模型，效率低下。缺乏基础模型阻碍了雷达感知技术的发展，因此这个问题对推动雷达技术在自动驾驶和机器人领域的应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到毫米波雷达处理流程中过度依赖有损点云表示和缺乏大规模数据集的瓶颈。他们设计了一个便携式的多模态数据收集系统'red-rover'，在不同场景收集原始雷达数据。在模型设计上，借鉴了计算机视觉中的Transformer架构和Perceiver I/O的查询机制，将其适配到4D雷达数据处理。作者参考了雷达处理领域的4D FFT技术，以及现有雷达感知任务定义，但创新性地将其应用于基础模型训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模原始雷达数据训练通用Transformer模型，直接从4D雷达数据立方体中学习特征，而非依赖传统的有损点云表示。整体流程包括：1)使用便携式系统收集多模态数据(雷达、激光雷达、相机)；2)将原始I/Q数据通过4D FFT转换为4D数据立方体；3)将数据分割成补丁并添加位置编码；4)使用Transformer编码器-解码器架构处理数据；5)训练基础模型(3D占用分类)并扩展到其他任务；6)评估模型在不同场景的泛化能力和微调性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了目前最大的原始毫米波雷达数据集I/Q-1M(29小时/100万帧)；2)证明使用原始4D数据比传统点云表示效果更好，相当于10倍以上训练数据量；3)设计了通用雷达Transformer(GRT)架构，能生成高质量3D占用图和语义分割；4)系统研究了雷达Transformer的扩展规律。相比之前工作，本研究数据规模大得多(比最大公开数据集大8倍)，直接使用原始数据而非点云，首次系统将Transformer应用于雷达基础模型，并实现了3D而非仅2D的复杂任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过构建大规模原始毫米波雷达数据集并训练通用雷达Transformer，证明了数据规模对提升单芯片雷达感知能力的关键作用，为雷达领域的基础模型发展奠定了重要基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; mmWave radars are compact, inexpensive, and durable sensors that are robustto occlusions and work regardless of environmental conditions, such as weatherand darkness. However, this comes at the cost of poor angular resolution,especially for inexpensive single-chip radars, which are typically used inautomotive and indoor sensing applications. Although many have proposedlearning-based methods to mitigate this weakness, no standardized foundationalmodels or large datasets for the mmWave radar have emerged, and practitionershave largely trained task-specific models from scratch using relatively smalldatasets.  In this paper, we collect (to our knowledge) the largest available raw radardataset with 1M samples (29 hours) and train a foundational model for 4Dsingle-chip radar, which can predict 3D occupancy and semantic segmentationwith quality that is typically only possible with much higher resolutionsensors. We demonstrate that our Generalizable Radar Transformer (GRT)generalizes across diverse settings, can be fine-tuned for different tasks, andshows logarithmic data scaling of 20\% per $10\times$ data. We also runextensive ablations on common design decisions, and find that using raw radardata significantly outperforms widely-used lossy representations, equivalent toa $10\times$ increase in training data. Finally, we roughly estimate that$\approx$100M samples (3000 hours) of data are required to fully exploit thepotential of GRT.</description>
      <author>example@mail.com (Tianshu Huang, Akarsh Prabhakara, Chuhan Chen, Jay Karhade, Deva Ramanan, Matthew O'Toole, Anthony Rowe)</author>
      <guid isPermaLink="false">2509.12482v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Prompt Management in GitHub Repositories: A Call for Best Practices</title>
      <link>http://arxiv.org/abs/2509.12421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究对开源提示进行了实证分析，发现了管理挑战并提供了改进建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型（如大型语言模型）的快速采用引发了promptware（使用自然语言提示构建的软件）的发展，有效管理提示变得至关重要但具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;调查提示管理实践和质量属性，以识别关键挑战并提供改进建议。&lt;h4&gt;方法&lt;/h4&gt;对来自92个GitHub仓库的24,800个开源提示进行实证分析。&lt;h4&gt;主要发现&lt;/h4&gt;提示格式化存在相当大的不一致性、内部和外部提示大量重复，以及频繁出现的可读性和拼写问题。&lt;h4&gt;结论&lt;/h4&gt;开发者需要采取措施提高开源提示的可用性和可维护性，以应对promptware生态系统中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基础模型（如大型语言模型）的快速采用引发了promptware的发展，即使用自然语言提示构建的软件。有效管理提示，如组织和质量保证，虽然至关重要但具有挑战性。在本研究中，我们对来自92个GitHub仓库的24,800个开源提示进行了实证分析，以调查提示管理实践和质量属性。我们的发现揭示了关键挑战，如提示格式化存在相当大的不一致性、内部和外部提示大量重复，以及频繁出现的可读性和拼写问题。基于这些发现，我们为开发者提供了可操作的建议，以在不断发展的promptware生态系统中提高开源提示的可用性和可维护性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid adoption of foundation models (e.g., large language models) hasgiven rise to promptware, i.e., software built using natural language prompts.Effective management of prompts, such as organization and quality assurance, isessential yet challenging. In this study, we perform an empirical analysis of24,800 open-source prompts from 92 GitHub repositories to investigate promptmanagement practices and quality attributes. Our findings reveal criticalchallenges such as considerable inconsistencies in prompt formatting,substantial internal and external prompt duplication, and frequent readabilityand spelling issues. Based on these findings, we provide actionablerecommendations for developers to enhance the usability and maintainability ofopen-source prompts within the rapidly evolving promptware ecosystem.</description>
      <author>example@mail.com (Hao Li, Hicham Masri, Filipe R. Cogo, Abdul Ali Bangash, Bram Adams, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2509.12421v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>T-SiamTPN: Temporal Siamese Transformer Pyramid Networks for Robust and Efficient UAV Tracking</title>
      <link>http://arxiv.org/abs/2509.12913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为T-SiamTPN的新型空中目标跟踪方法，通过引入时间感知的Siamese框架解决了现有跟踪器在处理尺度变化、动态背景、杂乱和频繁遮挡等问题时的局限性。该方法通过时间特征融合和基于注意力的交互显著提高了跟踪性能，同时保持计算效率，适合在资源受限的嵌入式设备上实时运行。&lt;h4&gt;背景&lt;/h4&gt;空中目标跟踪具有挑战性，因为存在尺度变化、动态背景、杂乱和频繁遮挡等问题。现有跟踪器大多强调空间线索，但忽略了时间依赖性，导致长期跟踪和遮挡情况下的鲁棒性有限。此外，基于相关的Siamese跟踪器受限于相关操作的线性本质，对复杂、非线性的外观变化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;解决现有跟踪器的局限性，特别是时间依赖性建模不足的问题；提高跟踪器在长期跟踪和遮挡情况下的鲁棒性；处理复杂、非线性的外观变化。&lt;h4&gt;方法&lt;/h4&gt;提出T-SiamTPN，一个具有时间感知的Siamese跟踪框架；通过显式的时间建模扩展了SiamTPN架构；结合时间特征融合和基于注意力的交互，增强时间一致性并实现更丰富的特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;T-SiamTPN相比基线有显著改进，性能与最先进的跟踪器相当；尽管增加了时间模块，T-SiamTPN仍保持计算效率；在资源受限的Jetson Nano上，跟踪器实时运行，达到7.1 FPS，适合嵌入式应用；与基线相比，T-SiamTPN将成功率提高了13.7%，精度提高了14.7%。&lt;h4&gt;结论&lt;/h4&gt;时间建模在Siamese跟踪框架中很重要；T-SiamTPN是空中目标跟踪的一个强大且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空中目标跟踪由于尺度变化、动态背景、杂乱和频繁遮挡而仍然是一项具有挑战性的任务。虽然大多数现有跟踪器强调空间线索，但它们经常忽略时间依赖性，导致在长期跟踪和遮挡情况下鲁棒性有限。此外，基于相关的Siamese跟踪器本质上受限于相关操作的线性特性，使其对复杂、非线性的外观变化无效。为解决这些局限性，我们引入了T-SiamTPN，这是一个具有时间感知的Siamese跟踪框架，通过显式的时间建模扩展了SiamTPN架构。我们的方法结合了时间特征融合和基于注意力的交互，增强了时间一致性并实现了更丰富的特征表示。这些增强相比基线带来了显著改进，并实现了与最先进跟踪器相当的性能。关键的是，尽管增加了时间模块，T-SiamTPN仍保持了计算效率。在资源受限的Jetson Nano上部署时，跟踪器以7.1 FPS的帧率实时运行，展示了其适合实际嵌入式应用的适用性，而没有明显的运行时开销。实验结果突出了显著的增益：与基线相比，T-SiamTPN将成功率提高了13.7%，精度提高了14.7%。这些发现强调了时间建模在Siamese跟踪框架中的重要性，并将T-SiamTPN确立为空中目标跟踪的一个强大且高效的解决方案。代码可在以下网址获取：https://github.com/to/be/released&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerial object tracking remains a challenging task due to scale variations,dynamic backgrounds, clutter, and frequent occlusions. While most existingtrackers emphasize spatial cues, they often overlook temporal dependencies,resulting in limited robustness in long-term tracking and under occlusion.Furthermore, correlation-based Siamese trackers are inherently constrained bythe linear nature of correlation operations, making them ineffective againstcomplex, non-linear appearance changes. To address these limitations, weintroduce T-SiamTPN, a temporal-aware Siamese tracking framework that extendsthe SiamTPN architecture with explicit temporal modeling. Our approachincorporates temporal feature fusion and attention-based interactions,strengthening temporal consistency and enabling richer feature representations.These enhancements yield significant improvements over the baseline and achieveperformance competitive with state-of-the-art trackers. Crucially, despite theadded temporal modules, T-SiamTPN preserves computational efficiency. Deployedon the resource-constrained Jetson Nano, the tracker runs in real time at 7.1FPS, demonstrating its suitability for real-world embedded applications withoutnotable runtime overhead. Experimental results highlight substantial gains:compared to the baseline, T-SiamTPN improves success rate by 13.7% andprecision by 14.7%. These findings underscore the importance of temporalmodeling in Siamese tracking frameworks and establish T-SiamTPN as a strong andefficient solution for aerial object tracking. Code is available at:https://github.com/to/be/released</description>
      <author>example@mail.com (Hojat Ardi, Amir Jahanshahi, Ali Diba)</author>
      <guid isPermaLink="false">2509.12913v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>HistoryBankQA: Multilingual Temporal Question Answering on Historical Events</title>
      <link>http://arxiv.org/abs/2509.12720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HistoryBank，一个包含1000多万个历史事件的多语言数据库，以及一个全面的问答基准，用于评估大型语言模型在历史事件时间推理方面的能力。&lt;h4&gt;背景&lt;/h4&gt;时间推理对于自然语言处理任务至关重要，但现有的大型语言模型在时间推理方面的基准测试有限。现有的时间推理数据集规模有限，缺乏多语言覆盖，并且更关注当代事件而非历史事件。&lt;h4&gt;目的&lt;/h4&gt;解决现有时间推理数据集的局限性，提供一个大规模、多语言的历史事件数据库，并构建一个全面的问答基准来评估语言模型在时间推理任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;从维基百科的时间线页面和文章信息框中提取了1000多万个历史事件，构建了一个包含10种语言的数据库。此外，还构建了一个包含6种时间问答推理任务的全面基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;评估了多种流行的语言模型（LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B、GPT4o）在这些任务上的性能。GPT4o在所有答案类型和语言中表现最佳；Gemma-2-9b优于其他小型语言模型。&lt;h4&gt;结论&lt;/h4&gt;这项工作旨在为推进对历史事件的多语言和时间感知自然语言理解提供全面资源。为了促进进一步研究，将在论文接受后公开代码和数据集。&lt;h4&gt;翻译&lt;/h4&gt;关于历史事件的时间推理是自然语言处理任务的关键技能，如事件提取、历史实体链接、时间问答、时间线总结、时间事件聚类和时间自然语言推理。然而，对大型语言模型时间推理能力的基准测试工作相当有限。现有的时间推理数据集在规模上有限，缺乏多语言覆盖，并且更关注当代事件。为了解决这些局限性，我们提出了HistoryBank，一个从维基百科时间线页面和文章信息框中提取的1000多万个历史事件的多语言数据库。我们的数据库在历史深度和语言广度上提供了前所未有的覆盖，包含10种语言。此外，我们构建了一个全面的跨所有语言的问答基准，用于时间推理。该基准涵盖了多样化的6种时间问答推理任务集，我们评估了一系列流行的语言模型（LLaMA-3-8B、Mistral-7B、Gemma-2-9b、Qwen3-8B、GPT4o）以评估它们在这些任务上的性能。正如预期的那样，GPT4o在所有答案类型和语言中表现最佳；Gemma-2优于其他小型语言模型。我们的工作旨在为推进对历史事件的多语言和时间感知自然语言理解提供全面资源。为了促进进一步研究，我们将在论文接受后公开我们的代码和数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning about historical events is a critical skill for NLP taskslike event extraction, historical entity linking, temporal question answering,timeline summarization, temporal event clustering and temporal natural languageinference. Yet efforts on benchmarking temporal reasoning capabilities of largelanguage models (LLMs) are rather limited. Existing temporal reasoning datasetsare limited in scale, lack multilingual coverage and focus more on contemporaryevents. To address these limitations, we present HistoryBank, a multilingualdatabase of 10M+ historical events extracted from Wikipedia timeline pages andarticle infoboxes. Our database provides unprecedented coverage in bothhistorical depth and linguistic breadth with 10 languages. Additionally, weconstruct a comprehensive question answering benchmark for temporal reasoningacross all languages. This benchmark covers a diverse set of 6 temporal QAreasoning tasks, and we evaluate a suite of popular language models(LLaMA-3-8B, Mistral-7B, Gemma-2-9b, Qwen3-8B, GPT4o) to assess theirperformance on these tasks. As expected GPT4o performs best across all answertypes and languages; Gemma-2 outperforms the other small language models. Ourwork aims to provide a comprehensive resource for advancing multilingual andtemporally-aware natural language understanding of historical events. Tofacilitate further research, we will make our code and datasets publiclyavailable upon acceptance of this paper.</description>
      <author>example@mail.com (Biswadip Mandal, Anant Khandelwal, Manish Gupta)</author>
      <guid isPermaLink="false">2509.12720v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Research on Short-Video Platform User Decision-Making via Multimodal Temporal Modeling and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.12269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MT-DQN模型，结合了Transformer、时间图神经网络和深度Q网络，用于解决短视频环境中的用户行为预测和推荐策略优化问题。实验证明该模型在多个评估指标上均优于传统模型。&lt;h4&gt;背景&lt;/h4&gt;短视频环境中存在用户行为预测困难和推荐策略优化挑战的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效预测用户行为并优化推荐策略的模型，以应对短视频环境的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出MT-DQN模型，该模型集成了Transformer、时间图神经网络(TGNN)和深度Q网络(DQN)。&lt;h4&gt;主要发现&lt;/h4&gt;1) MT-DQN比传统连接模型(如Concat-Modal)表现更好，平均F1分数提高10.97%，平均NDCG@5提高8.3%；2) 与经典强化学习模型Vanilla-DQN相比，MT-DQN的MSE降低34.8%，MAE降低26.5%。&lt;h4&gt;结论&lt;/h4&gt;MT-DQN模型在短视频推荐领域具有显著优势，但面临计算成本高和在线推理延迟敏感等现实部署挑战，这些挑战将通过未来的架构优化来解决。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了MT-DQN模型，该模型集成了Transformer、时间图神经网络(TGNN)和深度Q网络(DQN)，以解决短视频环境中预测用户行为和优化推荐策略的挑战。实验证明，MT-DQN持续优于传统的连接模型，如Concat-Modal，平均F1分数提高了10.97%，平均NDCG@5提高了8.3%。与经典的强化学习模型Vanilla-DQN相比，MT-DQN的MSE降低了34.8%，MAE降低了26.5%。尽管如此，我们也认识到在现实场景中部署MT-DQN面临的挑战，如计算成本和在线推理时的延迟敏感性，这些问题将通过未来的架构优化来解决。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes the MT-DQN model, which integrates a Transformer,Temporal Graph Neural Network (TGNN), and Deep Q-Network (DQN) to address thechallenges of predicting user behavior and optimizing recommendation strategiesin short-video environments. Experiments demonstrated that MT-DQN consistentlyoutperforms traditional concatenated models, such as Concat-Modal, achieving anaverage F1-score improvement of 10.97% and an average NDCG@5 improvement of8.3%. Compared to the classic reinforcement learning model Vanilla-DQN, MT-DQNreduces MSE by 34.8% and MAE by 26.5%. Nonetheless, we also recognizechallenges in deploying MT-DQN in real-world scenarios, such as itscomputational cost and latency sensitivity during online inference, which willbe addressed through future architectural optimization.</description>
      <author>example@mail.com (Jinmeiyang Wang, Jing Dong, Li Zhou)</author>
      <guid isPermaLink="false">2509.12269v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>JANUS: A Dual-Constraint Generative Framework for Stealthy Node Injection Attacks</title>
      <link>http://arxiv.org/abs/2509.13266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为JANUS的双重约束隐蔽节点注入框架，通过局部和全局两个层面的优化，显著提高了图神经网络对抗攻击的隐蔽性和有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在各种应用中表现出色，但它们容易受到复杂的对抗攻击，特别是节点注入攻击。这些攻击的成功很大程度上依赖于它们的隐蔽性，即能够融入原始图并逃避检测的能力。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，这些方法往往依赖间接代理指标、忽略注入内容的基本特征或仅关注模仿局部结构，导致局部近视问题。&lt;h4&gt;方法&lt;/h4&gt;提出名为'Joint Alignment of Nodal and Universal Structures'(JANUS)的框架。在局部层面，引入局部特征流形对齐策略实现特征空间中的几何一致性；在全局层面，结合结构化潜在变量并最大化与生成结构的互信息，确保注入结构与原始图的语义模式一致。将注入攻击建模为顺序决策过程，通过强化学习代理进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;JANUS框架通过同时考虑局部和全局结构，解决了现有节点注入攻击方法中的局部近视问题，实现了更隐蔽、更有效的攻击。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已在各种应用中展现出卓越的性能，但它们容易受到复杂的对抗攻击，特别是节点注入攻击。这类攻击的成功很大程度上依赖于其隐蔽性，即能够融入原始图并逃避检测的能力。然而，现有方法通常通过依赖间接代理指标来实现隐蔽性，缺乏对注入内容基本特征的考虑，或仅专注于模仿局部结构，这导致了局部近视问题。为了克服这些限制，我们提出了一种双重约束的隐蔽节点注入框架，称为'节点与通用结构联合对齐'(JANUS)。在局部层面，我们引入了局部特征流形对齐策略，以实现特征空间中的几何一致性。在全局层面，我们结合了结构化潜在变量，并最大化与生成结构的互信息，确保注入的结构与原始图的语义模式一致。我们将注入攻击建模为顺序决策过程，并通过强化学习代理进行优化。在多个标准数据集上的实验表明，JANUS框架在攻击有效性和隐蔽性方面显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossvarious applications, yet they are vulnerable to sophisticated adversarialattacks, particularly node injection attacks. The success of such attacksheavily relies on their stealthiness, the ability to blend in with the originalgraph and evade detection. However, existing methods often achieve stealthinessby relying on indirect proxy metrics, lacking consideration for the fundamentalcharacteristics of the injected content, or focusing only on imitating localstructures, which leads to the problem of local myopia. To overcome theselimitations, we propose a dual-constraint stealthy node injection framework,called Joint Alignment of Nodal and Universal Structures (JANUS). At the locallevel, we introduce a local feature manifold alignment strategy to achievegeometric consistency in the feature space. At the global level, we incorporatestructured latent variables and maximize the mutual information with thegenerated structures, ensuring the injected structures are consistent with thesemantic patterns of the original graph. We model the injection attack as asequential decision process, which is optimized by a reinforcement learningagent. Experiments on multiple standard datasets demonstrate that the JANUSframework significantly outperforms existing methods in terms of both attackeffectiveness and stealthiness.</description>
      <author>example@mail.com (Jiahao Zhang, Xiaobing Pei, Zhaokun Zhong, Wenqiang Hao, Zhenghao Tang)</author>
      <guid isPermaLink="false">2509.13266v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Learning from Heterophilic Graphs: A Spectral Theory Perspective on the Impact of Self-Loops and Parallel Edges</title>
      <link>http://arxiv.org/abs/2509.13139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图异质性对消息传递图神经网络性能的影响，特别是图卷积网络等低通滤波器在异构图上的表现。通过添加自环和平行边修改图结构，观察图拉普拉斯矩阵特征值变化及GCN性能变化，建立了图频谱与低通滤波器性能之间的联系，提出了一种无需昂贵特征值分解即可评估图特性的方法。&lt;h4&gt;背景&lt;/h4&gt;图异质性对消息传递图神经网络性能构成了重大挑战。像图卷积网络这样的常见低通滤波器面临性能下降，这可以归因于来自不相似邻居节点的消息混合。低通滤波器在异构图上的性能仍需要深入分析。&lt;h4&gt;目的&lt;/h4&gt;研究图异质性对低通滤波器性能的影响，特别是GCN在异构图上的表现，并探索通过修改图结构（添加自环和平行边）来改善性能的可能性。&lt;h4&gt;方法&lt;/h4&gt;通过添加自环和平行边来更新异构图。观察图拉普拉斯矩阵的特征值如何随着自环和平行边数量的变化而变化。在添加自环或平行边的情况下，在各种基准异构网络上进行关于GCN性能的研究。&lt;h4&gt;主要发现&lt;/h4&gt;1) 随着自环数量的增加，图拉普拉斯矩阵的特征值减小；2) 随着平行边数量的增加，图拉普拉斯矩阵的特征值增大；3) GCN在添加自环和平行边后表现出性能增加或减少的趋势；4) 建立了图频谱与低通滤波器在异构图上性能趋势之间的联系；5) 图频谱描述了输入图的基本内在特性，如连通分量的存在、稀疏性、平均度、聚类结构等；6) 可以通过观察低通滤波器的性能趋势来评估图频谱和特性，而无需进行昂贵的特征值分解。&lt;h4&gt;结论&lt;/h4&gt;本研究工作能够通过观察低通滤波器的性能趋势来评估图频谱和特性，而不需要进行昂贵的特征值分解。理论基础也验证了添加自环和平行边对图频谱的影响。&lt;h4&gt;翻译&lt;/h4&gt;图异质性对消息传递图神经网络性能构成了严峻挑战。像图卷积网络这样的常见低通滤波器面临性能下降，这可以归因于来自不相似邻居节点的消息混合。低通滤波器在异构图上的性能仍需要深入分析。在此背景下，我们通过添加自环和平行边来更新异构图。我们观察到，随着自环数量的增加，图拉普拉斯矩阵的特征值减小；随着平行边数量的增加，图拉普拉斯矩阵的特征值增大。我们通过添加自环或平行边，在各种基准异构网络上进行了关于GCN性能的几项研究。研究表明，GCN在添加自环和平行边后表现出性能增加或减少的趋势。基于这些研究，我们建立了图频谱与低通滤波器在异构图上性能趋势之间的联系。图频谱描述了输入图的基本内在特性，如连通分量的存在、稀疏性、平均度、聚类结构等。我们的工作能够通过观察低通滤波器的性能趋势来无缝评估图频谱和特性，而无需进行昂贵的特征值分解。还讨论了理论基础，以验证添加自环和平行边对图频谱的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph heterophily poses a formidable challenge to the performance ofMessage-passing Graph Neural Networks (MP-GNNs). The familiar low-pass filterslike Graph Convolutional Networks (GCNs) face performance degradation, whichcan be attributed to the blending of the messages from dissimilar neighboringnodes. The performance of the low-pass filters on heterophilic graphs stillrequires an in-depth analysis. In this context, we update the heterophilicgraphs by adding a number of self-loops and parallel edges. We observe thateigenvalues of the graph Laplacian decrease and increase respectively byincreasing the number of self-loops and parallel edges. We conduct severalstudies regarding the performance of GCN on various benchmark heterophilicnetworks by adding either self-loops or parallel edges. The studies reveal thatthe GCN exhibited either increasing or decreasing performance trends on addingself-loops and parallel edges. In light of the studies, we establishedconnections between the graph spectra and the performance trends of thelow-pass filters on the heterophilic graphs. The graph spectra characterize theessential intrinsic properties of the input graph like the presence ofconnected components, sparsity, average degree, cluster structures, etc. Ourwork is adept at seamlessly evaluating graph spectrum and properties byobserving the performance trends of the low-pass filters without pursuing thecostly eigenvalue decomposition. The theoretical foundations are also discussedto validate the impact of adding self-loops and parallel edges on the graphspectrum.</description>
      <author>example@mail.com (Kushal Bose, Swagatam Das)</author>
      <guid isPermaLink="false">2509.13139v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Curriculum Learning for Mesh-based simulations</title>
      <link>http://arxiv.org/abs/2509.13138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了一种从粗到细的课程学习方法，加速图神经网络在高分辨率非结构化网格上的训练过程，同时保持相当的准确性并减少训练时间。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为基于网格的计算流体动力学的强大替代模型，但在高分辨率非结构化网格上训练这些网络（包含数十万个节点）仍然极其昂贵。&lt;h4&gt;目的&lt;/h4&gt;研究一种从粗到细的课程学习方法，以加速图神经网络在高分辨率非结构化网格上的训练过程。&lt;h4&gt;方法&lt;/h4&gt;首先在非常粗糙的网格上训练，然后逐步引入中等和高分辨率（最高可达30万个节点）的网格。与多尺度GNN架构不同，模型本身保持不变，只有训练数据的保真度随时间变化。&lt;h4&gt;主要发现&lt;/h4&gt;使用这种方法可以实现相当的泛化准确性，同时将总墙上时钟时间减少高达50%；在模型缺乏学习底层物理能力的情况下，课程学习可以帮助模型突破性能瓶颈。&lt;h4&gt;结论&lt;/h4&gt;从粗到细的课程学习可以显著加速图神经网络在高分辨率非结构化网格上的训练，同时保持相当的准确性，并且在某些情况下还能帮助模型突破性能瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为基于网格的计算流体动力学的强大替代模型，但在高分辨率非结构化网格上训练它们（包含数十万个节点）仍然极其昂贵。我们研究了一种从粗到细的课程学习，通过首先在非常粗糙的网格上训练，然后逐步引入中等和高分辨率（最高达30万个节点）来加速收敛。与多尺度GNN架构不同，模型本身保持不变；只有训练数据的保真度随时间变化。我们在减少高达50%的总墙上时钟时间的同时，实现了相当的泛化准确性。此外，在我们的模型缺乏学习底层物理能力的数据集上，使用课程学习能够使其突破性能瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as powerful surrogates formesh-based computational fluid dynamics (CFD), but training them onhigh-resolution unstructured meshes with hundreds of thousands of nodes remainsprohibitively expensive. We study a \emph{coarse-to-fine curriculum} thataccelerates convergence by first training on very coarse meshes and thenprogressively introducing medium and high resolutions (up to \(3\times10^5\)nodes). Unlike multiscale GNN architectures, the model itself is unchanged;only the fidelity of the training data varies over time. We achieve comparablegeneralization accuracy while reducing total wall-clock time by up to 50\%.Furthermore, on datasets where our model lacks the capacity to learn theunderlying physics, using curriculum learning enables it to break throughplateaus.</description>
      <author>example@mail.com (Paul Garnier, Vincent Lannelongue, Elie Hachem)</author>
      <guid isPermaLink="false">2509.13138v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Spatiotemporal graph neural process for reconstruction, extrapolation, and classification of cardiac trajectories</title>
      <link>http://arxiv.org/abs/2509.12953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种结合神经常微分方程、图神经网络和神经过程的概率框架，用于从稀疏观测中建模心脏运动的时空动力学特性。&lt;h4&gt;背景&lt;/h4&gt;心脏运动的建模需要考虑时空结构、稀疏观测和不确定性，传统方法可能难以捕捉这些复杂特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从稀疏观测中重建和预测心脏运动的统一模型，同时捕捉不确定性、时间连续性和解剖结构。&lt;h4&gt;方法&lt;/h4&gt;将动态系统表示为时空多路复用图，使用GNN参数化的向量场建模潜在轨迹，并通过神经过程推断初始状态和控制变量的分布。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够在合成动态系统和真实心脏成像数据集上准确重建轨迹并外推未来周期，在ACDC分类任务上达到99%准确率，在UK Biobank中检测心房颤动达到67%准确率。&lt;h4&gt;结论&lt;/h4&gt;该框架为心脏运动分析提供了灵活的方法，并为结构化生物医学时空时间序列数据中的基于图的学习奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于从稀疏观测中建模结构化时空动力学的概率框架，专注于心脏运动。我们的方法将神经常微分方程、图神经网络和神经过程整合到一个统一模型中，该模型捕捉不确定性、时间连续性和解剖结构。我们将动态系统表示为时空多路复用图，并使用GNN参数化的向量场建模其潜在轨迹。给定节点和边缘级别的稀疏上下文观测，模型推断潜在初始状态和控制变量的分布，从而实现轨迹的内插和外推。我们在三个合成动态系统和两个真实世界的心脏成像数据集上验证了该方法，展示了精确重建、外推和疾病分类能力。模型能从单个观测周期准确重建轨迹并外推未来心脏周期。在ACDC分类任务上达到最先进的结果，并在UK Biobank受试者中检测心房颤动，具有竞争力的性能。这项工作为分析心脏运动提供了灵活的方法，并为结构化生物医学时空时间序列数据中的基于图的学习奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a probabilistic framework for modeling structured spatiotemporaldynamics from sparse observations, focusing on cardiac motion. Our approachintegrates neural ordinary differential equations (NODEs), graph neuralnetworks (GNNs), and neural processes into a unified model that capturesuncertainty, temporal continuity, and anatomical structure. We representdynamic systems as spatiotemporal multiplex graphs and model their latenttrajectories using a GNN-parameterized vector field. Given the sparse contextobservations at node and edge levels, the model infers a distribution overlatent initial states and control variables, enabling both interpolation andextrapolation of trajectories. We validate the method on three syntheticdynamical systems (coupled pendulum, Lorenz attractor, and Kuramotooscillators) and two real-world cardiac imaging datasets - ACDC (N=150) and UKBiobank (N=526) - demonstrating accurate reconstruction, extrapolation, anddisease classification capabilities. The model accurately reconstructstrajectories and extrapolates future cardiac cycles from a single observedcycle. It achieves state-of-the-art results on the ACDC classification task (upto 99% accuracy), and detects atrial fibrillation in UK Biobank subjects withcompetitive performance (up to 67% accuracy). This work introduces a flexibleapproach for analyzing cardiac motion and offers a foundation for graph-basedlearning in structured biomedical spatiotemporal time-series data.</description>
      <author>example@mail.com (Jaume Banus, Augustin C. Ogier, Roger Hullin, Philippe Meyer, Ruud B. van Heeswijk, Jonas Richiardi)</author>
      <guid isPermaLink="false">2509.12953v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Explicit Multimodal Graph Modeling for Human-Object Interaction Detection</title>
      <link>http://arxiv.org/abs/2509.12554v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MGNM方法，利用图神经网络的关系结构增强人类-物体交互检测性能&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的方法已成为人类-物体交互检测的主流方法，但该架构没有明确建模HOI检测中固有的关系结构，阻碍了交互识别&lt;h4&gt;目的&lt;/h4&gt;提出MGNM方法，利用基于GNN的关系结构增强HOI检测&lt;h4&gt;方法&lt;/h4&gt;设计多模态图网络框架，在四阶段图结构中明确建模HOI任务；引入多级特征交互机制，利用多级视觉和语言特征增强人类-物体对之间的信息传播&lt;h4&gt;主要发现&lt;/h4&gt;在HICO-DET和V-COCO基准测试上取得最先进性能；与先进物体检测器集成时展示显著性能提升，并在稀有和非稀有类别间保持平衡&lt;h4&gt;结论&lt;/h4&gt;MGNM方法能有效提升人类-物体交互检测的性能&lt;h4&gt;翻译&lt;/h4&gt;基于Transformer的方法最近已成为人类-物体交互检测的主流方法。然而，Transformer架构没有明确建模HOI检测中固有的关系结构，这阻碍了交互的识别。相比之下，图神经网络(GNNs)本质上更适合这项任务，因为它们明确建模了人类-物体对之间的关系。因此，在本文中，我们提出了MGNM(多模态图网络建模)，利用基于GNN的关系结构来增强HOI检测。具体来说，我们设计了一个多模态图网络框架，在四阶段图结构中明确建模HOI任务。此外，我们在图网络中引入了多级特征交互机制。该机制利用多级视觉和语言特征来增强人类-物体对之间的信息传播。因此，我们提出的MGNM在两个广泛使用的基准测试上取得了最先进的性能：HICO-DET和V-COCO。此外，当与更先进的物体检测器集成时，我们的方法展示了显著的性能提升，并在稀有和非稀有类别之间保持了有效的平衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformer-based methods have recently become the prevailing approach forHuman-Object Interaction (HOI) detection. However, the Transformer architecturedoes not explicitly model the relational structures inherent in HOI detection,which impedes the recognition of interactions. In contrast, Graph NeuralNetworks (GNNs) are inherently better suited for this task, as they explicitlymodel the relationships between human-object pairs. Therefore, in this paper,we propose \textbf{M}ultimodal \textbf{G}raph \textbf{N}etwork\textbf{M}odeling (MGNM) that leverages GNN-based relational structures toenhance HOI detection. Specifically, we design a multimodal graph networkframework that explicitly models the HOI task in a four-stage graph structure.Furthermore, we introduce a multi-level feature interaction mechanism withinour graph network. This mechanism leverages multi-level vision and languagefeatures to enhance information propagation across human-object pairs.Consequently, our proposed MGNM achieves state-of-the-art performance on twowidely used benchmarks: HICO-DET and V-COCO. Moreover, when integrated with amore advanced object detector, our method demonstrates a significantperformance gain and maintains an effective balance between rare and non-rareclasses.</description>
      <author>example@mail.com (Wenxuan Ji, Haichao Shi, Xiao-Yu zhang)</author>
      <guid isPermaLink="false">2509.12554v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Graph Homophily Booster: Rethinking the Role of Discrete Features on Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2509.12530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GRAPHITE的新框架，通过图转换直接增加图的同质性来解决图神经网络在异质性图上的表现不佳问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是建模图结构数据的强大工具，但现有GNN在处理异质性图(连接节点具有不同特征或标签)时表现不佳。现有方法主要集中在架构设计上，没有直接针对异质性问题的根本原因，甚至在某些异质性数据集上表现不如简单的多层感知机。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新方法来解决图异质性问题，超越架构设计的限制，直接通过图转换增加图的同质性。&lt;h4&gt;方法&lt;/h4&gt;提出名为GRAPHITE的框架，基于同质性的精确定义，创建特征节点来促进具有相似特征的节点之间的同质性消息传递，直接将异质性图转换为更具同质性的图。&lt;h4&gt;主要发现&lt;/h4&gt;理论和实证表明GRAPHITE显著增加了原始异质性图的同质性，同时仅略微增加图的大小；在具有挑战性的数据集上，GRAPHITE在异质性图上显著优于最先进方法，在同质性图上实现了与最先进方法相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;GRAPHITE是首个明确转换图以直接提高图同质性的方法，为解决图异质性问题提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为建模图结构数据的强大工具。然而，现有GNN通常难以处理异质性图，其中连接的节点往往具有不同的特征或标签。虽然已提出多种方法来解决这一挑战，但它们主要关注架构设计，而没有直接针对异质性问题的根本原因。这些方法在具有挑战性的异质性数据集上的表现甚至比最简单的多层感知机还差。例如，我们的实验显示，21种最新的GNN在Actor数据集上仍然落后于MLP。这一关键挑战需要一种超越架构设计的创新方法来解决图异质性。为了弥合这一差距，我们提出并研究了一种新的、未被探索的范式：通过精心设计的图转换直接增加图的同质性。在这项工作中，我们提出了一个简单而有效的框架GRAPHITE来解决图异质性问题。据我们所知，这项工作是首个明确转换图以直接提高图同质性的方法。源于同质性的精确定义，我们提出的GRAPHITE创建特征节点，以促进具有相似特征的节点之间的同质性消息传递。此外，我们从理论和实证上都证明，我们提出的GRAPHITE显著增加了原始异质性图的同质性，同时仅略微增加了图的大小。在具有挑战性的数据集上的大量实验表明，我们提出的GRAPHITE在异质性图上显著优于最先进的方法，同时在同质性图上实现了与最先进方法相当的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as a powerful tool for modelinggraph-structured data. However, existing GNNs often struggle with heterophilicgraphs, where connected nodes tend to have dissimilar features or labels. Whilenumerous methods have been proposed to address this challenge, they primarilyfocus on architectural designs without directly targeting the root cause of theheterophily problem. These approaches still perform even worse than thesimplest MLPs on challenging heterophilic datasets. For instance, ourexperiments show that 21 latest GNNs still fall behind the MLP on the Actordataset. This critical challenge calls for an innovative approach to addressinggraph heterophily beyond architectural designs. To bridge this gap, we proposeand study a new and unexplored paradigm: directly increasing the graphhomophily via a carefully designed graph transformation. In this work, wepresent a simple yet effective framework called GRAPHITE to address graphheterophily. To the best of our knowledge, this work is the first method thatexplicitly transforms the graph to directly improve the graph homophily.Stemmed from the exact definition of homophily, our proposed GRAPHITE createsfeature nodes to facilitate homophilic message passing between nodes that sharesimilar features. Furthermore, we both theoretically and empirically show thatour proposed GRAPHITE significantly increases the homophily of originallyheterophilic graphs, with only a slight increase in the graph size. Extensiveexperiments on challenging datasets demonstrate that our proposed GRAPHITEsignificantly outperforms state-of-the-art methods on heterophilic graphs whileachieving comparable accuracy with state-of-the-art methods on homophilicgraphs.</description>
      <author>example@mail.com (Ruizhong Qiu, Ting-Wei Li, Gaotang Li, Hanghang Tong)</author>
      <guid isPermaLink="false">2509.12530v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>GraphDerm: Fusing Imaging, Physical Scale, and Metadata in a Population-Graph Classifier for Dermoscopic Lesions</title>
      <link>http://arxiv.org/abs/2509.12277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GraphDerm是一种融合图像、毫米级校准和患者元数据的群体图框架，用于多类皮肤镜分类，显著提高了黑色素瘤分诊的准确性。&lt;h4&gt;背景&lt;/h4&gt;皮肤镜辅助黑色素瘤分诊，但仅基于图像的AI通常忽略患者元数据（年龄、性别、部位）和进行几何分析所需的物理尺度信息。&lt;h4&gt;目的&lt;/h4&gt;开发GraphDerm，一个融合图像、毫米级校准和元数据的多类皮肤镜分类的群体图框架，首次将图神经网络(GNNs)应用于ISIC规模的皮肤镜分析。&lt;h4&gt;方法&lt;/h4&gt;整合ISIC 2018/2019数据集，合成带有精确掩膜的标尺嵌入图像，训练U-Nets进行病变和标尺分割，通过1D-CNN回归每毫米像素数，计算病变的真实尺度描述符，使用EfficientNet-B3作为节点特征，编码元数据/几何相似性作为边，采用谱GNN进行半监督分类。&lt;h4&gt;主要发现&lt;/h4&gt;标尺和病变分割达到Dice 0.904和0.908，尺度回归达到MAE 1.5像素，图分类达到AUC 0.9812，使用约25%边的阈值变体保留AUC 0.9788，远高于图像基线的0.9440，每类AUC通常在0.97-0.99范围内。&lt;h4&gt;结论&lt;/h4&gt;将校准尺度、病变几何形状和元数据统一到群体图中，比仅基于图像的流程有显著提升；更稀疏的图保留了接近最优的准确性，表明高效部署的可能性；具有尺度感知、基于图的AI是皮肤镜决策支持的 promising 方向。&lt;h4&gt;翻译&lt;/h4&gt;引言。皮肤镜辅助黑色素瘤分诊，但仅基于图像的AI通常忽略患者元数据（年龄、性别、部位）和进行几何分析所需的物理尺度。我们提出了GraphDerm，一个融合图像、毫米级校准和元数据的多类皮肤镜分类的群体图框架，据我们所知，这是首次将图神经网络应用于ISIC规模的皮肤镜。方法。我们整理了ISIC 2018/2019，合成了带有精确掩膜的标尺嵌入图像，并训练U-Nets(SE-ResNet-18)进行病变和标尺分割。通过轻量级1D-CNN从标尺掩膜两点相关性回归每毫米像素数。从病变掩膜我们计算真实尺度描述符（面积、周长、回转半径）。节点特征使用EfficientNet-B3；边编码元数据/几何相似性（全权重或阈值化）。谱GNN执行半监督节点分类；图像-only ANN作为基线。结果。标尺和病变分割达到Dice 0.904和0.908；尺度回归达到MAE 1.5像素（RMSE 6.6）。图达到AUC 0.9812，阈值变体使用约25%的边保留AUC 0.9788（对比图像基线的0.9440）；每类AUC通常在0.97-0.99范围内。结论。在ISIC-2019上，将校准尺度、病变几何形状和元数据统一到群体图中，比仅基于图像的流程有显著提升。更稀疏的图保留了接近最优的准确性，表明高效部署的可能性。具有尺度感知、基于图的AI是皮肤镜决策支持的 promising 方向；未来工作将改进学习的边语义并在更广泛的精选基准上评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction. Dermoscopy aids melanoma triage, yet image-only AI oftenignores patient metadata (age, sex, site) and the physical scale needed forgeometric analysis. We present GraphDerm, a population-graph framework thatfuses imaging, millimeter-scale calibration, and metadata for multiclassdermoscopic classification, to the best of our knowledge the first ISIC-scaleapplication of GNNs to dermoscopy. Methods. We curate ISIC 2018/2019,synthesize ruler-embedded images with exact masks, and train U-Nets(SE-ResNet-18) for lesion and ruler segmentation. Pixels-per-millimeter areregressed from the ruler-mask two-point correlation via a lightweight 1D-CNN.From lesion masks we compute real-scale descriptors (area, perimeter, radius ofgyration). Node features use EfficientNet-B3; edges encode metadata/geometrysimilarity (fully weighted or thresholded). A spectral GNN performssemi-supervised node classification; an image-only ANN is the baseline.Results. Ruler and lesion segmentation reach Dice 0.904 and 0.908; scaleregression attains MAE 1.5 px (RMSE 6.6). The graph attains AUC 0.9812, with athresholded variant using about 25% of edges preserving AUC 0.9788 (vs. 0.9440for the image-only baseline); per-class AUCs typically fall in the 0.97-0.99range. Conclusion. Unifying calibrated scale, lesion geometry, and metadata ina population graph yields substantial gains over image-only pipelines onISIC-2019. Sparser graphs retain near-optimal accuracy, suggesting efficientdeployment. Scale-aware, graph-based AI is a promising direction fordermoscopic decision support; future work will refine learned edge semanticsand evaluate on broader curated benchmarks.</description>
      <author>example@mail.com (Mehdi Yousefzadeh, Parsa Esfahanian, Sara Rashidifar, Hossein Salahshoor Gavalan, Negar Sadat Rafiee Tabatabaee, Saeid Gorgin, Dara Rahmati, Maryam Daneshpazhooh)</author>
      <guid isPermaLink="false">2509.12277v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge Graph Tokenization for Behavior-Aware Generative Next POI Recommendation</title>
      <link>http://arxiv.org/abs/2509.12350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了KGTB方法，通过知识图谱标记化和多行为学习来解决现有POI推荐方法中信息损失和对用户移动理解不足的问题。&lt;h4&gt;背景&lt;/h4&gt;生成范式，特别是由大型语言模型(LLMs)驱动的，已成为解决下一个兴趣点(POI)推荐的新方案。现有研究通常采用两阶段流程，首先使用标记化器将POI转换为可由LLM处理的离散标识符，然后进行POI行为预测任务来指令微调LLM以实现下一个POI推荐。&lt;h4&gt;目的&lt;/h4&gt;解决现有POI推荐方法的两个局限性：(1)现有标记化器难以编码推荐数据中的异构信号，存在信息损失问题；(2)之前的指令微调任务只关注用户的POI访问行为，而忽略其他行为类型，导致对移动性的理解不足。&lt;h4&gt;方法&lt;/h4&gt;提出KGTB方法，具体包括：(1)将推荐数据组织为知识图谱(KG)格式，保留异构信息；(2)开发基于KG的标记化器，将每个节点量化为独立的结构ID，由KG结构监督，减少异构信息损失；(3)提出多行为学习，引入多种特定行为的预测任务（如POI、类别和区域访问行为）来微调LLM。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实城市数据集上的实验表明，KGTB具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;KGTB通过知识图谱标记化和多行为学习有效解决了现有POI推荐方法中的信息损失和对用户移动理解不足的问题，提升了推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;生成范式，特别是由大型语言模型(LLMs)驱动的，已成为解决下一个兴趣点(POI)推荐的新方案。开创性研究通常采用两阶段流程，首先使用标记化器将POI转换为可由LLM处理的离散标识符，然后进行POI行为预测任务来指令微调LLM以实现下一个POI推荐。尽管取得了显著进展，它们仍面临两个局限性：(1)现有标记化器难以编码推荐数据中的异构信号，存在信息损失问题；(2)之前的指令微调任务只关注用户的POI访问行为而忽略其他行为类型，导致对移动性的理解不足。为解决这些局限性，我们提出了KGTB（用于行为感知生成式下一个POI推荐的知识图谱标记化）。具体而言，KGTB将推荐数据组织为知识图谱(KG)格式，其结构可以无缝保留异构信息。然后，开发了一个基于KG的标记化器，将每个节点量化为独立的结构ID。此过程由KG结构监督，从而减少异构信息的损失。使用生成的ID，KGTB提出了多行为学习，为LLM微调引入多种特定行为的预测任务，例如POI、类别和区域访问行为。在这些行为任务上的学习为LLMs提供了对目标POI访问行为的全面洞察。在四个真实城市数据集上的实验证明了KGTB的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative paradigm, especially powered by Large Language Models (LLMs), hasemerged as a new solution to the next point-of-interest (POI) recommendation.Pioneering studies usually adopt a two-stage pipeline, starting with atokenizer converting POIs into discrete identifiers that can be processed byLLMs, followed by POI behavior prediction tasks to instruction-tune LLM fornext POI recommendation. Despite of remarkable progress, they still face twolimitations: (1) existing tokenizers struggle to encode heterogeneous signalsin the recommendation data, suffering from information loss issue, and (2)previous instruction-tuning tasks only focus on users' POI visit behavior whileignore other behavior types, resulting in insufficient understanding ofmobility. To address these limitations, we propose KGTB (Knowledge GraphTokenization for Behavior-aware generative next POI recommendation).Specifically, KGTB organizes the recommendation data in a knowledge graph (KG)format, of which the structure can seamlessly preserve the heterogeneousinformation. Then, a KG-based tokenizer is developed to quantize each node intoan individual structural ID. This process is supervised by the KG's structure,thus reducing the loss of heterogeneous information. Using generated IDs, KGTBproposes multi-behavior learning that introduces multiple behavior-specificprediction tasks for LLM fine-tuning, e.g., POI, category, and region visitbehaviors. Learning on these behavior tasks provides LLMs with comprehensiveinsights on the target POI visit behavior. Experiments on four real-world citydatasets demonstrate the superior performance of KGTB.</description>
      <author>example@mail.com (Ke Sun, Mayi Xu)</author>
      <guid isPermaLink="false">2509.12350v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive timbre representations for musical instrument and synthesizer retrieval</title>
      <link>http://arxiv.org/abs/2509.13285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于乐器检索的对比学习框架，能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音。&lt;h4&gt;背景&lt;/h4&gt;从音频混合物中高效检索特定乐器音色在数字音乐制作中仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理单乐器和多乐器声音的统一检索模型，解决现有音频数据增强方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出技术来为虚拟乐器（如采样器和合成器）生成逼真的正/负声音对，构建对比学习框架。&lt;h4&gt;主要发现&lt;/h4&gt;在单乐器检索实验中，对比学习方法与基于分类预训练的先前工作具有竞争力；在多乐器检索实验中，所提出的框架表现更优，对三种乐器混合物实现了81.7%的top-1准确率和95.7%的top-5准确率。&lt;h4&gt;结论&lt;/h4&gt;该对比学习框架在乐器检索任务中表现出色，特别是在多乐器检索方面优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;从音频混合物中高效检索特定乐器音色在数字音乐制作中仍然是一个挑战。本文介绍了一种用于乐器检索的对比学习框架，能够使用单一模型直接查询乐器数据库，适用于单乐器和多乐器声音。我们提出了为虚拟乐器（如采样器和合成器）生成逼真的正/负声音对的技术，解决了常见音频数据增强方法的局限性。第一个实验专注于从3,884种乐器的数据集中检索乐器，使用单乐器音频作为输入。对比方法与基于分类预训练的先前工作具有竞争力。第二个实验考虑使用乐器混合作为音频输入进行多乐器检索。在这种情况下，所提出的对比框架优于相关工作，对三种乐器混合物实现了81.7%的top-1准确率和95.7%的top-5准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently retrieving specific instrument timbres from audio mixturesremains a challenge in digital music production. This paper introduces acontrastive learning framework for musical instrument retrieval, enablingdirect querying of instrument databases using a single model for both single-and multi-instrument sounds. We propose techniques to generate realisticpositive/negative pairs of sounds for virtual musical instruments, such assamplers and synthesizers, addressing limitations in common audio dataaugmentation methods.  The first experiment focuses on instrument retrieval from a dataset of 3,884instruments, using single-instrument audio as input. Contrastive approaches arecompetitive with previous works based on classification pre-training. Thesecond experiment considers multi-instrument retrieval with a mixture ofinstruments as audio input. In this case, the proposed contrastive frameworkoutperforms related works, achieving 81.7\% top-1 and 95.7\% top-5 accuraciesfor three-instrument mixtures.</description>
      <author>example@mail.com (Gwendal Le Vaillant, Yannick Molle)</author>
      <guid isPermaLink="false">2509.13285v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Flow-Based Fragment Identification via Binding Site-Specific Latent Representations</title>
      <link>http://arxiv.org/abs/2509.13216v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种蛋白质片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享潜在空间，并提出LatentFrag方法用于虚拟筛选和生成设计，有效提高了基于片段的药物设计效率。&lt;h4&gt;背景&lt;/h4&gt;基于片段的药物设计是一种有前景的策略，利用小化学分子的结合来有效指导药物发现。然而，片段识别的初始步骤具有挑战性，因为片段通常结合较弱且非特异性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效识别和设计蛋白质-片段相互作用的方法，克服片段结合弱和非特异性的挑战，提高药物发现的效率。&lt;h4&gt;方法&lt;/h4&gt;研究人员开发了一个蛋白质片段编码器，采用对比学习方法将分子片段和蛋白质表面映射到共享的潜在空间。该方法能够捕获相互作用相关特征，并允许进行虚拟筛选以及生成设计。提出的LatentFrag方法根据蛋白质表面生成片段嵌入和位置，并在构造上保持化学真实性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 表达性片段和蛋白质表示能够以高灵敏度定位蛋白质-片段相互作用位点；2. 从学习的潜在片段嵌入分布中采样时，观察到最先进的片段回收率；3. 生成方法在计算成本仅为常用方法的一小部分的情况下，表现优于这些方法；4. 该方法为片段命中发现提供了有价值的起点。&lt;h4&gt;结论&lt;/h4&gt;这些方法共同推进了片段识别的发展，并为基于片段的药物发现提供了有价值的工具。研究人员进一步展示了LatentFrag的实际效用，并将工作流程扩展到完整的配体设计任务。&lt;h4&gt;翻译&lt;/h4&gt;基于片段的药物设计是一种有前景的策略，利用小化学分子的结合来有效指导药物发现。片段识别的初始步骤仍然具有挑战性，因为片段通常结合较弱且非特异性。我们开发了一个蛋白质片段编码器，它依赖于对比学习方法，将分子片段和蛋白质表面映射到共享的潜在空间。该编码器捕获相互作用相关特征，并允许使用我们的新方法LatentFrag进行虚拟筛选和生成设计。在LatentFrag中，根据蛋白质表面生成片段嵌入和位置，并在构造上保持化学真实性。我们表达性的片段和蛋白质表示能够以高灵敏度定位蛋白质-片段相互作用位点，并且当我们从学习的潜在片段嵌入分布中采样时，观察到最先进的片段回收率。我们的生成方法在计算成本仅为常用方法（如虚拟筛选）的一小部分的情况下，表现优于这些方法，为片段命中发现提供了有价值的起点。我们进一步展示了LatentFrag的实际效用，并将工作流程扩展到完整的配体设计任务。这些方法共同推进了片段识别的发展，并为基于片段的药物发现提供了有价值的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fragment-based drug design is a promising strategy leveraging the binding ofsmall chemical moieties that can efficiently guide drug discovery. The initialstep of fragment identification remains challenging, as fragments often bindweakly and non-specifically. We developed a protein-fragment encoder thatrelies on a contrastive learning approach to map both molecular fragments andprotein surfaces in a shared latent space. The encoder capturesinteraction-relevant features and allows to perform virtual screening as wellas generative design with our new method LatentFrag. In LatentFrag, fragmentembeddings and positions are generated conditioned on the protein surface whilebeing chemically realistic by construction. Our expressive fragment and proteinrepresentations allow location of protein-fragment interaction sites with highsensitivity and we observe state-of-the-art fragment recovery rates whensampling from the learned distribution of latent fragment embeddings. Ourgenerative method outperforms common methods such as virtual screening at afraction of its computational cost providing a valuable starting point forfragment hit discovery. We further show the practical utility of LatentFrag andextend the workflow to full ligand design tasks. Together, these approachescontribute to advancing fragment identification and provide valuable tools forfragment-based drug discovery.</description>
      <author>example@mail.com (Rebecca Manuela Neeser, Ilia Igashov, Arne Schneuing, Michael Bronstein, Philippe Schwaller, Bruno Correia)</author>
      <guid isPermaLink="false">2509.13216v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Dual Network Based Semi-Supervised Medical Image Segmentation with Uncertainty-Guided Pseudo-Labeling</title>
      <link>http://arxiv.org/abs/2509.13084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted in Knowledge-Based Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架，通过交叉一致性增强模块和动态加权策略解决伪标签噪声问题，并利用自监督对比学习减少预测不确定性。实验表明该方法在多个数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;监督医学图像分割模型虽然性能优异，但在实际应用中依赖大量标注数据不切实际。半监督学习方法通过伪标签生成利用未标注数据来缓解这一挑战，但现有方法仍受伪标签噪声和特征空间监督不足的影响。&lt;h4&gt;目的&lt;/h4&gt;解决现有半监督分割方法中存在的伪标签噪声和特征空间监督不足的问题，提出一种新颖的基于双网络架构的半监督3D医学图像分割框架。&lt;h4&gt;方法&lt;/h4&gt;提出交叉一致性增强模块使用交叉伪标签和熵过滤监督减少噪声伪标签；设计动态加权策略通过不确定性感知机制调整伪标签贡献；使用自监督对比学习机制区分可信和不确定预测，将不确定体素特征与可靠类别原型对齐以减少预测不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在Left Atrial、NIH Pancreas和BraTS-2019三个3D分割数据集上进行实验，结果表明所提方法在各种设置下均优于最先进方法（如在Left Atrial数据集上使用10%标注数据时达到89.95%的Dice分数），消融实验验证了各模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的新型半监督3D医学图像分割框架有效解决了伪标签噪声和特征空间监督不足的问题，实验结果证实了其在多种数据集和设置下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;尽管监督医学图像分割模型表现出色，但在实际情况中依赖大量标注数据是不切实际的。半监督学习方法旨在通过伪标签生成利用未标注数据来缓解这一挑战。然而，现有的半监督分割方法仍然受到噪声伪标签和特征空间内监督不足的影响。为了解决这些挑战，本文提出了一种基于双网络架构的新型半监督3D医学图像分割框架。具体来说，我们研究了一个交叉一致性增强模块，使用交叉伪标签和熵过滤监督来减少噪声伪标签，同时我们设计了一种动态加权策略，使用不确定性感知机制（即Kullback-Leibler散度）来调整伪标签的贡献。此外，我们使用自监督对比学习机制，通过有效区分可信和不确定的预测，将不确定的体素特征与可靠的类别原型对齐，从而减少预测不确定性。在三个3D分割数据集Left Atrial、NIH Pancreas和BraTS-2019上进行了大量实验。与最先进的方法相比，所提出的方法在各种设置下都表现出优越的性能（例如，在Left Atrial数据集上使用10%标注数据时达到89.95%的Dice分数）。此外，通过消融实验进一步验证了所提出模块的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决半监督医学图像分割中的两个关键挑战：噪声伪标签问题和特征空间监督不足。在现实中，医学图像分割需要大量像素级标注数据，而这些数据需要专家临床知识，获取成本高昂且耗时。半监督学习可以利用大量未标注数据缓解这一问题，但噪声伪标签会降低分割性能，而特征空间监督不足则限制了模型学习判别性表示的能力，这些问题限制了半监督方法在医学图像分割中的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有半监督医学图像分割方法的局限性进行设计：发现交叉伪监督方法计算开销大，不确定性估计方法成本高，基于原型的方法往往单独应用效果有限。因此，作者借鉴了知识蒸馏框架但创新性地使用两个并行学生网络而非传统教师-学生架构。他们结合了交叉伪监督(CPS)和熵过滤监督(EFS)形成CCE模块，并引入不确定性感知机制和原型引导的对比学习，从而有效解决了现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用双网络架构相互验证和改进伪标签质量，通过熵过滤减少噪声影响，利用不确定性机制动态调整伪标签权重，并通过对比学习增强特征空间判别性。整体流程包括：1)构建两个并行的3D编码器-解码器子网络；2)对标记数据使用标准损失函数并加入一致性正则化；3)对未标记数据实施交叉伪监督、熵过滤、不确定性加权调整和原型引导的对比学习；4)整合监督损失、对比损失和半监督损失进行模型优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出交叉一致性增强(CCE)模块协同整合交叉伪监督和熵过滤监督；2)设计不确定性感知的加权机制动态调整伪标签贡献；3)整合基于原型的对比学习策略对齐不确定特征与可靠类原型。相比之前工作，该方法使用双并行学生网络而非传统教师-学生架构，将不确定性估计与原型引导相结合实现高效噪声识别，通过CCE模块有机结合多种策略，并使用自适应熵阈值而非固定阈值，在不同数据集上表现出更强的适应性和性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于双网络架构和不确定性引导伪标签的半监督医学图像分割方法，通过交叉一致性增强模块和原型引导的对比学习有效解决了噪声伪标签和特征空间监督不足的问题，在有限标注数据条件下实现了最先进的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.knosys.2025.114454&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the remarkable performance of supervised medical image segmentationmodels, relying on a large amount of labeled data is impractical in real-worldsituations. Semi-supervised learning approaches aim to alleviate this challengeusing unlabeled data through pseudo-label generation. Yet, existingsemi-supervised segmentation methods still suffer from noisy pseudo-labels andinsufficient supervision within the feature space. To solve these challenges,this paper proposes a novel semi-supervised 3D medical image segmentationframework based on a dual-network architecture. Specifically, we investigate aCross Consistency Enhancement module using both cross pseudo andentropy-filtered supervision to reduce the noisy pseudo-labels, while we designa dynamic weighting strategy to adjust the contributions of pseudo-labels usingan uncertainty-aware mechanism (i.e., Kullback-Leibler divergence). Inaddition, we use a self-supervised contrastive learning mechanism to alignuncertain voxel features with reliable class prototypes by effectivelydifferentiating between trustworthy and uncertain predictions, thus reducingprediction uncertainty. Extensive experiments are conducted on three 3Dsegmentation datasets, Left Atrial, NIH Pancreas and BraTS-2019. The proposedapproach consistently exhibits superior performance across various settings(e.g., 89.95\% Dice score on left Atrial with 10\% labeled data) compared tothe state-of-the-art methods. Furthermore, the usefulness of the proposedmodules is further validated via ablation experiments.</description>
      <author>example@mail.com (Yunyao Lu, Yihang Wu, Ahmad Chaddad, Tareef Daqqaq, Reem Kateb)</author>
      <guid isPermaLink="false">2509.13084v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>LTA-thinker: Latent Thought-Augmented Training Framework for Large Language Models on Complex Reasoning</title>
      <link>http://arxiv.org/abs/2509.12875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LTA-Thinker是一种新型训练框架，通过增加潜在思想分布的方差和引入分布方向性优化，解决了大型语言模型中复杂推理的瓶颈问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型中的复杂推理可以通过测试时扩展(TTS)进行动态优化，以减轻过度思考问题。然而，在连续潜在空间推理中，高质量潜在思想的生成和利用仍然是核心瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出一个潜在思想增强训练框架——LTA-Thinker，以提高分布方差并从两个方面增强推理性能。&lt;h4&gt;方法&lt;/h4&gt;构建基于可学习先验的潜在思想生成架构，以增加生成潜在思想向量的方差分布，简化整体结构并提高性能上限；引入基于分布的方向性优化范式，联合约束分布局部性和分布规模，通过多目标共同训练策略结合标准监督微调(SFT)损失与两种新损失：语义对齐损失和推理焦点损失。&lt;h4&gt;主要发现&lt;/h4&gt;LTA-Thinker在各种基线中取得了最先进的(SOTA)性能，表现出更高的性能上限和更好的扩展效果。&lt;h4&gt;结论&lt;/h4&gt;LTA-Thinker通过提高分布方差和优化潜在思想的生成与利用，有效提升了大型语言模型的复杂推理能力。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型中的复杂推理可以通过测试时扩展(TTS)进行动态优化以减轻过度思考。诸如Coconut、SoftCoT及其变体等方法在连续潜在空间推理中有效，但核心瓶颈仍在于高质量潜在思想的生成和利用。借鉴SoftCoT++理论——生成的潜在思想分布的更大方差更接近黄金真实分布，我们提出了一个潜在思想增强训练框架——LTA-Thinker，它从两个方面提高分布方差并增强推理性能。首先，LTA-Thinker构建了一个基于可学习先验的潜在思想生成架构，该架构旨在增加生成潜在思想向量的方差分布，以简化整体结构并提高性能上限。其次，LTA-Thinker引入了一种基于分布的方向性优化范式，联合约束分布局部性和分布规模。这种机制通过多目标共同训练策略提高信息效率和计算成本，该策略结合了标准监督微调(SFT)损失与两种新损失：语义对齐损失，利用KL散度确保潜在思想与问题语义高度相关；推理焦点损失，利用对比学习机制引导模型关注最关键的推理步骤。实验表明，LTA-Thinker在各种基线中实现了最先进的(SOTA)性能，表现出更高的性能上限和更好的扩展效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex Reasoning in Large Language Models can be dynamically optimized usingTest-Time Scaling (TTS) to mitigate Overthinking. Methods such as Coconut,SoftCoT and its variant are effective in continuous latent space inference, thecore bottleneck still lies in the efficient generation and utilization ofhigh-quality Latent Thought. Drawing from the theory of SoftCoT++ that a largervariance in the generated Latent Thought distribution more closely approximatesthe golden truth distribution, we propose a Latent Thought-Augmented TrainingFramework--LTA-Thinker, which improves distributional variance and enhancesreasoning performance from two perspectives. First, LTA-Thinker constructs aLatent Thought generation architecture based on a learnable prior. Thisarchitecture aims to increase the variance distribution of generated LatentThought Vectors in order to simplify the overall structure and raise theperformance ceiling. Second, LTA-Thinker introduces a distribution-baseddirectional optimization paradigm that jointly constrains both distributionlocality and distribution scale. This mechanism improves information efficiencyand computational cost through a multi-objective co-training strategy, whichcombines standard Supervised Fine-Tuning (SFT) loss with two novel losses:Semantic Alignment Loss, which utilizes KL divergence to ensure that the LatentThought is highly relevant to the semantics of the question; Reasoning FocusLoss, which utilizes a contrastive learning mechanism to guide the model tofocus on the most critical reasoning steps. Experiments show that LTA-thinkerachieves state-of-the-art (SOTA) performance among various baselines anddemonstrates a higher performance ceiling and better scaling effects.</description>
      <author>example@mail.com (Jiaqi Wang, Binquan Ji, Haibo Luo, Yiyang Qi, Ruiting Li, Huiyan Wang, Yuantao Han, Cangyi Yang, jiaxu Zhang, Feiliang Ren)</author>
      <guid isPermaLink="false">2509.12875v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Enhanced Abstract Representations using Grouped Loss of Abstract Semantic Supervision</title>
      <link>http://arxiv.org/abs/2509.12771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型(VLMs)的概念抽象能力，提出了一种新的训练方法使模型能够识别图像作为一般概念的实例，而不仅仅是识别对象及其关系。&lt;h4&gt;背景&lt;/h4&gt;人类能够将图像识别为一般概念的实例，而不仅仅是识别其中的对象及其关系。研究者调查VLMs是否具有这种概念抽象能力。&lt;h4&gt;目的&lt;/h4&gt;1) 调查VLMs在多大程度上具有概念抽象能力；2) 研究策略使VLM模型能够更大程度地具备这种能力。&lt;h4&gt;方法&lt;/h4&gt;引入分组图像-标题数据集(MAGIC)，使用新颖的对比损失技术编码每组图像(标题)的共有信息，提出基于文本-图像对比组的分组对比损失函数(外部对比损失)和衡量组内图像-标题实例距离的内部损失，训练模型创建语义表示使其接近高层概念的语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种训练方法产生的CLEAR GLASS模型在抽象概念识别方面比最先进(SOTA)模型有所改进。&lt;h4&gt;结论&lt;/h4&gt;CLEAR GLASS模型通过这种训练方法获得了概念抽象能力，能够更好地识别抽象概念，尽管训练过程中模型并未直接接触高层概念标签。&lt;h4&gt;翻译&lt;/h4&gt;人类能够将图像识别为一般概念的实例，而不仅仅是识别其中的对象及其关系。在本文中，我们研究了1) VLMs在多大程度上具有这种概念抽象能力，以及2)如何在图像中编码那种高层概念信息，使产生的VLM模型(CLEAR GLASS模型)更大程度地具备这种能力的策略。为此，我们引入了一个分组图像-标题数据集(MAGIC)，它包含多组图像标题、相关图像和高层概念标签。我们使用新颖的对比损失技术，使模型编码每组图像(标题)中所有成员共有的信息。我们的主要贡献是基于文本-图像对比组的分组对比损失函数(外部对比损失)以及衡量组内图像-标题实例之间距离的内部损失。我们的训练方法使CLEAR GLASS模型获得了概念抽象能力，因为模型没有暴露给与每组相关的高层概念。相反，训练迫使模型为每组图像-标题创建一个语义表示，使其在潜在语义空间中更接近高层概念的语义表示。我们的实验表明，这种训练方法产生的模型在抽象概念识别方面比SOTA模型有所改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can recognize an image as an instance of a general concept, beyondsimply identifying its objects and their relationships. In this paper, weinvestigate 1. The extent to which VLMs have this concept abstraction capacity,and 2. Strategies for encoding the sort of higher-concept information in imagesthat would enable the resulting VLM model (CLEAR GLASS model) to have thiscapability to a greater degree. To this end, we introduce a groupedimage-caption dataset (MAGIC), which consists of several groups of imagecaptions and for each group a set of associated images and higher-levelconceptual labels. We use a novel contrastive loss technique to induce themodel to encode in the representation of each image (caption) in a group theinformation that is common to all members of the image-caption group. Our maincontribution is a grouped contrastive loss function based on text-imagecontrastive groups (outer contrastive loss) as well as an inner loss whichmeasures the distances between image-caption instances in the group. Ourtraining methodology results in the CLEAR GLASS model having the conceptabstraction capacity as an emergent capacity because the model is not exposedto the higher-level concepts associated with each group. Instead, the trainingforces the model to create for each image-caption group a semanticrepresentation that brings it closer to the semantic representation of thehigher-level concepts in the latent semantic space. Our experiments show thatthis training methodology results in a model which shows improvement inabstract concept recognition compared to SOTA models.</description>
      <author>example@mail.com (Omri Suissa, Muhiim Ali, Shengmai Chen, Yinuo Cai, Shekhar Pradhan)</author>
      <guid isPermaLink="false">2509.12771v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>ScaleDoc: Scaling LLM-based Predicates over Large Document Collections</title>
      <link>http://arxiv.org/abs/2509.12610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ScaleDoc是一种新型系统，通过解耦谓词执行为离线表示阶段和在线过滤阶段，结合大型语言模型和轻量级代理模型，显著提高了大规模语义分析的效率。&lt;h4&gt;背景&lt;/h4&gt;谓词是数据分析系统的基础组件，但现代工作负载越来越多地涉及需要语义理解的非结构化文档，传统基于值的谓词已无法满足需求。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在处理海量文档和临时查询时的高推理成本问题，使大规模语义分析变得实用和高效。&lt;h4&gt;方法&lt;/h4&gt;ScaleDoc系统将谓词执行分为离线表示阶段和在线过滤阶段。离线阶段利用LLM为文档生成语义表示；在线阶段训练轻量级代理模型过滤文档，仅将模糊案例转发给LLM。系统还包含两个核心创新：基于对比学习的框架训练代理模型生成可靠决策分数，以及自适应级联机制确定有效过滤策略。&lt;h4&gt;主要发现&lt;/h4&gt;ScaleDoc在三个数据集上实现了超过2倍的端到端加速，并将昂贵的LLM调用减少了高达85%。&lt;h4&gt;结论&lt;/h4&gt;ScaleDoc使大规模语义分析变得实用和高效，通过减少对大型语言模型的依赖，显著提高了处理性能。&lt;h4&gt;翻译&lt;/h4&gt;谓词是数据分析系统的基础组件。然而，现代工作负载越来越多地涉及非结构化文档，这需要超越传统基于值的谓词的语义理解。面对海量文档和临时查询，虽然大型语言模型展示了强大的零样本能力，但其高推理成本导致不可接受的额外开销。因此，我们提出了ScaleDoc，一种新型系统，通过将谓词执行解耦为离线表示阶段和优化后的在线过滤阶段来解决这一问题。在离线阶段，ScaleDoc利用LLM为每个文档生成语义表示。在线，对于每个查询，它在这些表示上训练一个轻量级代理模型来过滤大部分文档，仅将模糊案例转发给LLM进行最终决策。此外，ScaleDoc提出了两个核心创新以实现显著效率：(1)基于对比学习的框架，训练代理模型生成可靠的预测决策分数；(2)自适应级联机制，在满足特定准确度目标的同时确定有效的过滤策略。我们在三个数据集上的评估表明，ScaleDoc实现了超过2倍的端到端加速，并将昂贵的LLM调用减少了高达85%，使大规模语义分析变得实用和高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicates are foundational components in data analysis systems. However,modern workloads increasingly involve unstructured documents, which demandssemantic understanding, beyond traditional value-based predicates. Givenenormous documents and ad-hoc queries, while Large Language Models (LLMs)demonstrate powerful zero-shot capabilities, their high inference cost leads tounacceptable overhead. Therefore, we introduce \textsc{ScaleDoc}, a novelsystem that addresses this by decoupling predicate execution into an offlinerepresentation phase and an optimized online filtering phase. In the offlinephase, \textsc{ScaleDoc} leverages a LLM to generate semantic representationsfor each document. Online, for each query, it trains a lightweight proxy modelon these representations to filter the majority of documents, forwarding onlythe ambiguous cases to the LLM for final decision. Furthermore,\textsc{ScaleDoc} proposes two core innovations to achieve significantefficiency: (1) a contrastive-learning-based framework that trains the proxymodel to generate reliable predicating decision scores; (2) an adaptive cascademechanism that determines the effective filtering policy while meeting specificaccuracy targets. Our evaluations across three datasets demonstrate that\textsc{ScaleDoc} achieves over a 2$\times$ end-to-end speedup and reducesexpensive LLM invocations by up to 85\%, making large-scale semantic analysispractical and efficient.</description>
      <author>example@mail.com (Hengrui Zhang, Yulong Hui, Yihao Liu, Huanchen Zhang)</author>
      <guid isPermaLink="false">2509.12610v1</guid>
      <pubDate>Wed, 17 Sep 2025 15:11:39 +0800</pubDate>
    </item>
    <item>
      <title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
      <link>http://arxiv.org/abs/2509.10156v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerLock是一种简单而有效的自监督视觉表征学习方法，通过渐进式层冻结技术，从像素预测逐步过渡到潜在空间预测。&lt;h4&gt;背景&lt;/h4&gt;在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层先收敛，深层后收敛。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法加速标准MAE训练，并实现简单且可扩展的潜在空间预测，避免'表征崩溃'问题。&lt;h4&gt;方法&lt;/h4&gt;LayerLock利用ViT层按深度顺序收敛的观察，在整个训练过程中根据显式调度逐步冻结模型，既可用于加速标准MAE，也可用于潜在空间预测。&lt;h4&gt;主要发现&lt;/h4&gt;通过逐步冻结模型可以加速标准MAE训练；使用相同调度可以实现简单且可扩展的潜在空间预测，避免'表征崩溃'问题。&lt;h4&gt;结论&lt;/h4&gt;LayerLock方法成功应用于高达40亿参数的大模型，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LayerLock，一种简单而有效的自监督视觉表征学习方法，它通过渐进式层冻结技术，从像素预测逐步过渡到潜在空间预测。首先，我们观察到在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层先收敛，深层后收敛。然后，我们证明这一观察可以用来加速标准MAE，通过在整个训练过程中根据显式调度逐步冻结模型。此外，相同的调度可用于一种简单且可扩展的潜在空间预测方法，不会遭受'表征崩溃'问题。我们将提出的LayerLock方法应用于高达40亿参数的大模型，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LayerLock, a simple yet effective approach for self-supervisedvisual representation learning, that gradually transitions from pixel to latentprediction through progressive layer freezing. First, we make the observationthat during training of video masked-autoencoding (MAE) models, ViT layersconverge in the order of their depth: shallower layers converge early, deeperlayers converge late. We then show that this observation can be exploited toaccelerate standard MAE by progressively freezing the model according to anexplicit schedule, throughout training. Furthermore, this same schedule can beused in a simple and scalable approach to latent prediction that does notsuffer from "representation collapse". We apply our proposed approach,LayerLock, to large models of up to 4B parameters with results surpassing thoseof non-latent masked prediction on the 4DS perception suite.</description>
      <author>example@mail.com (Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira)</author>
      <guid isPermaLink="false">2509.10156v2</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
  <item>
      <title>Open-ended Hierarchical Streaming Video Understanding with Vision Language Models</title>
      <link>http://arxiv.org/abs/2509.12145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了分层流式视频理解任务，结合了在线时序动作定位和自由形式描述生成。作者提出使用大语言模型丰富现有数据集的方法，并开发了名为OpenHOUSE的系统，该系统具有专门的流式模块，能够准确检测相邻动作边界，性能显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前缺乏具有分层和细粒度时间标注的视频数据集，限制了视频理解技术的发展。同时，流式动作感知目前主要局限于动作分类，无法满足更复杂的视频理解需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够进行分层流式视频理解的方法，扩展流式动作感知的能力，使其超越简单的动作分类，能够进行更精细的动作边界检测和事件描述生成。&lt;h4&gt;方法&lt;/h4&gt;1. 利用大语言模型将原子动作分组为更高级别的事件，以丰富现有数据集；2. 提出OpenHOUSE系统，包含专门的流式模块用于检测紧密相邻动作之间的边界；3. 扩展流式动作感知，使其不仅限于动作分类，还包括自由形式的描述生成。&lt;h4&gt;主要发现&lt;/h4&gt;1. 大语言模型能够有效地将原子动作分组为更高级别的事件，从而丰富现有数据集；2. OpenHOUSE的专门流式模块能够准确检测紧密相邻动作之间的边界；3. OpenHOUSE的性能几乎是现有方法直接扩展的两倍。&lt;h4&gt;结论&lt;/h4&gt;流式动作感知的未来发展方向是集成强大的生成模型，OpenHOUSE代表了朝着这一方向迈出的关键一步，为更高级的视频理解任务提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了分层流式视频理解，这是一个结合在线时序动作定位与自由形式描述生成的任务。鉴于具有分层和细粒度时间标注的数据集稀缺，我们证明了大型语言模型能够有效地将原子动作分组为更高级别的事件，从而丰富现有数据集。随后，我们提出了OpenHOUSE(面向事件的开分层式在线理解系统)，它将流式动作感知扩展超越了动作分类的范围。OpenHOUSE具有专门的流式模块，能够准确检测紧密相邻动作之间的边界，性能几乎是现有方法直接扩展的两倍。我们展望流式动作感知的未来在于集成强大的生成模型，而OpenHOUSE代表了朝着这一方向迈出的关键一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Hierarchical Streaming Video Understanding, a task that combinesonline temporal action localization with free-form description generation.Given the scarcity of datasets with hierarchical and fine-grained temporalannotations, we demonstrate that LLMs can effectively group atomic actions intohigher-level events, enriching existing datasets. We then propose OpenHOUSE(Open-ended Hierarchical Online Understanding System for Events), which extendsstreaming action perception beyond action classification. OpenHOUSE features aspecialized streaming module that accurately detects boundaries between closelyadjacent actions, nearly doubling the performance of direct extensions ofexisting methods. We envision the future of streaming action perception in theintegration of powerful generative models, with OpenHOUSE representing a keystep in that direction.</description>
      <author>example@mail.com (Hyolim Kang, Yunsu Park, Youngbeom Yoo, Yeeun Choi, Seon Joo Kim)</author>
      <guid isPermaLink="false">2509.12145v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Computer Vision Pipeline for Individual-Level Behavior Analysis: Benchmarking on the Edinburgh Pig Dataset</title>
      <link>http://arxiv.org/abs/2509.12047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 figures, Submitted to Computers and Electronics in Agriculture&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于计算机视觉的模块化流程，用于自动分析群体饲养环境中的动物行为，显著提高了分析准确性和效率。&lt;h4&gt;背景&lt;/h4&gt;动物行为分析对理解农业环境中的动物福利、健康和生产率至关重要，但传统人工观察方法耗时、主观且可扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个利用开源最先进计算机视觉技术的模块化流程，实现群体饲养环境中动物行为分析的自动化。&lt;h4&gt;方法&lt;/h4&gt;结合零样本目标检测模型、感知运动跟踪和分割技术，以及使用视觉转换器进行高级特征提取，解决动物遮挡和群体饲养场景等挑战，并在室内猪监测中进行了演示。&lt;h4&gt;主要发现&lt;/h4&gt;在爱丁堡猪行为视频数据集上验证，时间模型总体准确率达94.2%，比现有方法提高21.2个百分点；跟踪能力身份保持得分为93.3%，目标检测精度达89.3%。&lt;h4&gt;结论&lt;/h4&gt;模块化设计有潜力适应其他环境，但需对不同物种进一步验证；开源实现为行为监测提供了可扩展解决方案，通过自动化、客观和持续的分析促进精准养猪和福利评估。&lt;h4&gt;翻译&lt;/h4&gt;动物行为分析在理解农业环境中的动物福利、健康状况和生产率方面起着至关重要的作用。然而，传统的人工观察方法耗时、主观且可扩展性有限。我们提出了一种模块化流程，利用开源的最先进计算机视觉技术来自动化群体饲养环境中的动物行为分析。我们的方法结合了用于零样本目标检测的最先进模型、感知运动跟踪和分割技术，以及使用视觉转换器进行高级特征提取，以实现稳健的行为识别。该流程解决了包括动物遮挡和群体饲养场景在内的挑战，如在室内猪监测中所展示的那样。我们在爱丁堡猪行为视频数据集上对多个行为任务验证了我们的系统。我们的时间模型实现了94.2%的总体准确率，比现有方法提高了21.2个百分点。该流程显示出强大的跟踪能力，身份保持得分为93.3%，目标检测精度为89.3%。模块化设计表明有潜力适应其他环境，但需要对不同物种进行进一步验证。开源实现为行为监测提供了可扩展的解决方案，通过自动化、客观和持续的分析为精准养猪和福利评估做出贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Animal behavior analysis plays a crucial role in understanding animalwelfare, health status, and productivity in agricultural settings. However,traditional manual observation methods are time-consuming, subjective, andlimited in scalability. We present a modular pipeline that leveragesopen-sourced state-of-the-art computer vision techniques to automate animalbehavior analysis in a group housing environment. Our approach combinesstate-of-the-art models for zero-shot object detection, motion-aware trackingand segmentation, and advanced feature extraction using vision transformers forrobust behavior recognition. The pipeline addresses challenges including animalocclusions and group housing scenarios as demonstrated in indoor pigmonitoring. We validated our system on the Edinburgh Pig Behavior Video Datasetfor multiple behavioral tasks. Our temporal model achieved 94.2% overallaccuracy, representing a 21.2 percentage point improvement over existingmethods. The pipeline demonstrated robust tracking capabilities with 93.3%identity preservation score and 89.3% object detection precision. The modulardesign suggests potential for adaptation to other contexts, though furthervalidation across species would be required. The open-source implementationprovides a scalable solution for behavior monitoring, contributing to precisionpig farming and welfare assessment through automated, objective, and continuousanalysis.</description>
      <author>example@mail.com (Haiyu Yang, Enhong Liu, Jennifer Sun, Sumit Sharma, Meike van Leerdam, Sebastien Franceschini, Puchun Niu, Miel Hostens)</author>
      <guid isPermaLink="false">2509.12047v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Agentic Temporal Graph of Reasoning with Multimodal Language Models: A Potential AI Aid to Healthcare</title>
      <link>http://arxiv.org/abs/2509.11944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于时间图的多模态医疗推理模型，通过有向图建模，能够适应动态变化，完善推理内容，并考虑时间因素跟踪患者健康变化。多代理时间推理框架进一步提高了推理准确性，实验验证了该方法的新颖性和实用性。&lt;h4&gt;背景&lt;/h4&gt;医疗和医学是多模态学科，需要处理多模态数据进行推理和诊断多种疾病。尽管已经出现了一些用于科学领域复杂任务的多模态推理模型，但它们在医疗领域的应用仍然有限，并且在诊断推理方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决多模态医疗推理在正确诊断方面的挑战，协助医疗专业人员。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于时间图的推理过程模型，通过有向图建模。该模型能够通过回溯来适应原因的动态变化，完善推理内容，创建新原因或删除现有原因，以达到最佳推荐或答案。考虑不同时间点的多模态数据，能够跟踪和分析患者的健康状况和疾病进展。提出了多代理时间推理框架，提供任务分配和交叉验证机制，进一步提高推理输出的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;基本实验和分析结果证明了所提出的初步方法的新颖性和实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的时间图推理框架和多代理时间推理框架能够有效解决医疗领域多模态推理的挑战，提高诊断准确性。&lt;h4&gt;翻译&lt;/h4&gt;医疗和医学是处理多模态数据以推理和诊断多种疾病的多模态学科。尽管已经出现了一些用于科学领域复杂任务的多模态推理模型，但它们在医疗领域的应用仍然有限，并且在诊断推理方面表现不佳。为解决多模态医疗推理在正确诊断方面的挑战并协助医疗专业人员，本文提出了一种新颖的基于时间图的推理过程，通过有向图建模。该模型能够通过回溯来适应原因的动态变化，完善推理内容，创建新原因或删除现有原因，以达到最佳推荐或答案。此外，考虑不同时间点的多模态数据能够跟踪和分析患者的健康状况和疾病进展。此外，所提出的多代理时间推理框架提供任务分配和交叉验证机制，进一步提高推理输出的准确性。一些基本实验和分析结果证明了所提出的初步方法的新颖性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare and medicine are multimodal disciplines that deal with multimodaldata for reasoning and diagnosing multiple diseases. Although some multimodalreasoning models have emerged for reasoning complex tasks in scientificdomains, their applications in the healthcare domain remain limited and fallshort in correct reasoning for diagnosis. To address the challenges ofmultimodal medical reasoning for correct diagnosis and assist the healthcareprofessionals, a novel temporal graph-based reasoning process modelled througha directed graph has been proposed in the current work. It helps inaccommodating dynamic changes in reasons through backtracking, refining thereasoning content, and creating new or deleting existing reasons to reach thebest recommendation or answer. Again, consideration of multimodal data atdifferent time points can enable tracking and analysis of patient health anddisease progression. Moreover, the proposed multi-agent temporal reasoningframework provides task distributions and a cross-validation mechanism tofurther enhance the accuracy of reasoning outputs. A few basic experiments andanalysis results justify the novelty and practical utility of the proposedpreliminary approach.</description>
      <author>example@mail.com (Susanta Mitra)</author>
      <guid isPermaLink="false">2509.11944v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Dr.V: A Hierarchical Perception-Temporal-Cognition Framework to Diagnose Video Hallucination by Fine-grained Spatial-Temporal Grounding</title>
      <link>http://arxiv.org/abs/2509.11866v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了Dr.V分层框架，用于诊断大型视频模型中的幻觉问题，包括基准数据集Dr.V-Bench和卫星视频代理Dr.V-Agent，通过细粒度时空定位提高视频理解的可靠性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)的最新进展显著提升了视频理解能力，但这些模型仍然存在幻觉问题，即产生与输入视频内容相冲突的内容。&lt;h4&gt;目的&lt;/h4&gt;解决大型视频模型中的幻觉问题，建立一个能够诊断视频幻觉的框架。&lt;h4&gt;方法&lt;/h4&gt;提出Dr.V分层框架，包括：1) Dr.V-Bench基准数据集：包含来自4,974个视频的10k实例，涵盖多样化任务，每个实例都有详细的时空标注；2) Dr.V-Agent卫星视频代理：通过在感知和时空层面系统应用细粒度时空定位，然后进行认知层面推理，来检测LVMs中的幻觉。&lt;h4&gt;主要发现&lt;/h4&gt;Dr.V-Agent能够有效诊断幻觉，同时提高模型的可解释性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;Dr.V为现实世界场景中的鲁棒视频理解提供了实用的蓝图，所有数据和代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)的最新进展显著提升了视频理解能力。然而，这些模型仍然存在幻觉问题，产生与输入视频内容相冲突的内容。为解决这一问题，我们提出了Dr.V，一个覆盖感知、时空和认知三个层面的分层框架，通过细粒度的时空定位来诊断视频幻觉。Dr.V包含两个关键组件：基准数据集Dr.V-Bench和卫星视频代理Dr.V-Agent。Dr.V-Bench包含来自4,974个视频的10k实例，涵盖多样化任务，每个实例都配有详细的时空标注。Dr.V-Agent通过在感知和时空层面系统应用细粒度时空定位，然后进行认知层面推理，来检测LVMs中的幻觉。这一逐步流程模拟了类人视频理解过程，能有效识别幻觉。大量实验证明，Dr.V-Agent在诊断幻觉方面有效，同时提高了可解释性和可靠性，为现实世界场景中的鲁棒视频理解提供了实用蓝图。我们所有的数据和代码可在https://github.com/Eurekaleo/Dr.V获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video models (LVMs) have significantly enhancevideo understanding. However, these models continue to suffer fromhallucinations, producing content that conflicts with input videos. To addressthis issue, we propose Dr.V, a hierarchical framework covering perceptive,temporal, and cognitive levels to diagnose video hallucination by fine-grainedspatial-temporal grounding. Dr.V comprises of two key components: a benchmarkdataset Dr.V-Bench and a satellite video agent Dr.V-Agent. Dr.V-Bench includes10k instances drawn from 4,974 videos spanning diverse tasks, each enrichedwith detailed spatial-temporal annotation. Dr.V-Agent detects hallucinations inLVMs by systematically applying fine-grained spatial-temporal grounding at theperceptive and temporal levels, followed by cognitive level reasoning. Thisstep-by-step pipeline mirrors human-like video comprehension and effectivelyidentifies hallucinations. Extensive experiments demonstrate that Dr.V-Agent iseffective in diagnosing hallucination while enhancing interpretability andreliability, offering a practical blueprint for robust video understanding inreal-world scenarios. All our data and code are available athttps://github.com/Eurekaleo/Dr.V.</description>
      <author>example@mail.com (Meng Luo, Shengqiong Wu, Liqiang Jing, Tianjie Ju, Li Zheng, Jinxiang Lai, Tianlong Wu, Xinya Du, Jian Li, Siyuan Yan, Jiebo Luo, William Yang Wang, Hao Fei, Mong-Li Lee, Wynne Hsu)</author>
      <guid isPermaLink="false">2509.11866v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Vision Language Models and Symbolic Grounding for Video Question Answering</title>
      <link>http://arxiv.org/abs/2509.11862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合符号场景图与视觉语言模型的新框架SG-VLM，用于提升视频问答中的因果和时间推理能力，尽管相对于强大的VLMs改进有限。&lt;h4&gt;背景&lt;/h4&gt;视频问答(VQA)需要模型对视频中的空间、时间和因果线索进行推理。近期的视觉语言模型虽取得良好结果，但常依赖浅层关联，导致时间定位能力弱且可解释性有限。&lt;h4&gt;目的&lt;/h4&gt;研究符号场景图(SGs)作为视频问答的中间定位信号，探索如何将SGs与VLMs结合以提升模型表现。&lt;h4&gt;方法&lt;/h4&gt;提出SG-VLM模块化框架，通过提示和视觉定位将冻结的VLMs与场景图定位相结合，利用SGs提供的结构化对象-关系表示来补充VLMs的整体推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试(NExT-QA, iVQA, ActivityNet-QA)和多个VLMs(QwenVL, InternVL)上测试，SG-VLM提高了因果和时间推理能力，超越了先前的基线方法，但相对于强大的VLMs的改进有限。&lt;h4&gt;结论&lt;/h4&gt;符号定位在视频问答中具有前景但也存在当前局限性，为未来混合VLM-符号方法在视频理解中的应用提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;视频问答(VQA)需要模型对视频中的空间、时间和因果线索进行推理。近期的视觉语言模型(VLMs)取得了很好的结果，但通常依赖于浅层关联，导致时间定位能力弱且可解释性有限。我们研究符号场景图(SGs)作为VQA的中间定位信号。SGs提供结构化的对象-关系表示，补充VLMs的整体推理能力。我们提出SG-VLM，一个通过提示和视觉定位将冻结的VLMs与场景图定位相结合的模块化框架。在三个基准测试(NExT-QA, iVQA, ActivityNet-QA)和多个VLMs(QwenVL, InternVL)上，SG-VLM提高了因果和时间推理能力，并超越了先前的基线方法，但相对于强大的VLMs的改进有限。这些发现突显了符号定位的前景和当前局限性，为未来混合VLM-符号方法在视频理解中的应用提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VQA) requires models to reason over spatial,temporal, and causal cues in videos. Recent vision language models (VLMs)achieve strong results but often rely on shallow correlations, leading to weaktemporal grounding and limited interpretability. We study symbolic scene graphs(SGs) as intermediate grounding signals for VQA. SGs provide structuredobject-relation representations that complement VLMs holistic reasoning. Weintroduce SG-VLM, a modular framework that integrates frozen VLMs with scenegraph grounding via prompting and visual localization. Across three benchmarks(NExT-QA, iVQA, ActivityNet-QA) and multiple VLMs (QwenVL, InternVL), SG-VLMimproves causal and temporal reasoning and outperforms prior baselines, thoughgains over strong VLMs are limited. These findings highlight both the promiseand current limitations of symbolic grounding, and offer guidance for futurehybrid VLM-symbolic approaches in video understanding.</description>
      <author>example@mail.com (Haodi Ma, Vyom Pathak, Daisy Zhe Wang)</author>
      <guid isPermaLink="false">2509.11862v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FineQuest: Adaptive Knowledge-Assisted Sports Video Understanding via Agent-of-Thoughts Reasoning</title>
      <link>http://arxiv.org/abs/2509.11796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出FineQuest，一个基于双模式推理的无需训练框架，专门用于解决体育视频问答中的复杂理解问题。&lt;h4&gt;背景&lt;/h4&gt;基于大型语言模型(LLMs)的视频问答在一般视频理解方面显示出潜力，但在应用于体育视频这一固有复杂领域时面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决通用模型与特定体育领域理解之间的知识差距，提高体育视频问答的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出FineQuest框架，采用双模式推理（反应式推理用于简单问题，深思熟虑式推理用于复杂问题）；集成SSGraph，一个跨越九种运动的多模态体育知识场景图；引入两个新的体育VideoQA基准：Gym-QA和Diving-QA。&lt;h4&gt;主要发现&lt;/h4&gt;FineQuest在新的基准测试以及现有的SPORTU数据集上取得了最先进的性能，同时保持了强大的通用VideoQA能力。&lt;h4&gt;结论&lt;/h4&gt;FineQuest通过双模式推理和领域特定知识整合，有效解决了体育视频问答的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基于大型语言模型(LLMs)的视频问答在一般视频理解方面显示出潜力，但在应用于体育视频这一固有复杂领域时面临重大挑战。在这项工作中，我们提出了FineQuest，这是第一个利用认知科学启发的双模式推理的无训练框架：i)用于简单体育查询的反应式推理；ii)用于更复杂查询的深思熟虑式推理。为了弥合通用模型与特定体育领域理解之间的知识差距，FineQuest集成了SSGraph，一个跨越九种运动的多模态体育知识场景图，它编码视觉实例和领域特定术语以提高推理准确性。此外，我们引入了两个新的体育VideoQA基准，Gym-QA和Diving-QA，它们源自FineGym和FineDiving数据集，实现了多样化和全面的评估。FineQuest在这些基准以及现有的SPORTU数据集上取得了最先进的性能，同时保持了强大的通用VideoQA能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VideoQA) based on Large Language Models (LLMs) hasshown potential in general video understanding but faces significant challengeswhen applied to the inherently complex domain of sports videos. In this work,we propose FineQuest, the first training-free framework that leveragesdual-mode reasoning inspired by cognitive science: i) Reactive Reasoning forstraightforward sports queries and ii) Deliberative Reasoning for more complexones. To bridge the knowledge gap between general-purpose models anddomain-specific sports understanding, FineQuest incorporates SSGraph, amultimodal sports knowledge scene graph spanning nine sports, which encodesboth visual instances and domain-specific terminology to enhance reasoningaccuracy. Furthermore, we introduce two new sports VideoQA benchmarks, Gym-QAand Diving-QA, derived from the FineGym and FineDiving datasets, enablingdiverse and comprehensive evaluation. FineQuest achieves state-of-the-artperformance on these benchmarks as well as the existing SPORTU dataset, whilemaintains strong general VideoQA capabilities.</description>
      <author>example@mail.com (Haodong Chen, Haojian Huang, XinXiang Yin, Dian Shao)</author>
      <guid isPermaLink="false">2509.11796v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GLaVE-Cap: Global-Local Aligned Video Captioning with Vision Expert Integration</title>
      <link>http://arxiv.org/abs/2509.11360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了GLaVE-Cap框架，通过整合视觉专家和双流结构解决视频详细描述中的上下文一致性和细节性问题，同时构建了GLaVE-Bench基准测试和GLaVE-1.2M数据集，在多个基准上实现了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;视频详细描述旨在生成全面视频描述以促进视频理解。目前大多数研究采用从局部到全局的范式，先生成视频片段的局部描述，再将其汇总为全局描述。&lt;h4&gt;目的&lt;/h4&gt;解决现有从局部到全局的视频描述范式导致的描述不够详细且上下文不一致的问题，提高视频描述的质量和连贯性。&lt;h4&gt;方法&lt;/h4&gt;提出GLaVE-Cap框架，包含两个核心模块：1) TrackFusion模块利用视觉专家获取跨帧视觉提示，结合双流结构实现全面的局部描述生成；2) CaptionBridge模块使用全局上下文指导局部描述，并将局部描述自适应地汇总为连贯的全局描述。同时构建GLaVE-Bench基准测试和GLaVE-1.2M训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;现有范式导致描述不够详细且上下文不一致的原因是：(1)没有确保细粒度描述的机制，(2)局部和全局描述之间的交互较弱。GLaVE-Cap通过整合视觉专家和双流结构有效解决了这些问题。&lt;h4&gt;结论&lt;/h4&gt;在四个基准测试上的大量实验表明，GLaVE-Cap达到了最先进的性能。消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M对视频理解社区的贡献。源代码、模型权重、基准和数据集将开源。&lt;h4&gt;翻译&lt;/h4&gt;视频详细描述旨在生成全面的视频描述以促进视频理解。最近，视频详细描述领域的大多数努力都集中在从局部到全局的范式上，该范式首先从视频片段生成局部描述，然后将其汇总为全局描述。然而，我们发现这种范式导致描述不够详细且上下文不一致，这可以归因于：(1)没有确保细粒度描述的机制，以及(2)局部和全局描述之间的交互较弱。为了解决上述两个问题，我们提出了GLaVE-Cap，这是一种具有视觉专家集成的全局-局部对齐框架，包含两个核心模块：TrackFusion通过利用视觉专家获取跨帧视觉提示，结合双流结构实现全面的局部描述生成；而CaptionBridge通过使用全局上下文指导局部描述，并将局部描述自适应地汇总为连贯的全局描述来建立局部-全局交互。此外，我们构建了GLaVE-Bench，这是一个全面的视频描述基准测试，每个视频包含比现有基准多5倍的查询，覆盖多样化的视觉维度，以促进可靠的评估。我们还提供了包含16K高质量细粒度视频描述和1.2M相关问答对的训练数据集GLaVE-1.2M。在四个基准上的大量实验表明，我们的GLaVE-Cap实现了最先进的性能。此外，消融研究和学生模型分析进一步验证了所提出模块的有效性以及GLaVE-1.2M对视频理解社区的贡献。源代码、模型权重、基准和数据集将开源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video detailed captioning aims to generate comprehensive video descriptionsto facilitate video understanding. Recently, most efforts in the video detailedcaptioning community have been made towards a local-to-global paradigm, whichfirst generates local captions from video clips and then summarizes them into aglobal caption. However, we find this paradigm leads to less detailed andcontextual-inconsistent captions, which can be attributed to (1) no mechanismto ensure fine-grained captions, and (2) weak interaction between local andglobal captions. To remedy the above two issues, we propose GLaVE-Cap, aGlobal-Local aligned framework with Vision Expert integration for Captioning,which consists of two core modules: TrackFusion enables comprehensive localcaption generation, by leveraging vision experts to acquire cross-frame visualprompts, coupled with a dual-stream structure; while CaptionBridge establishesa local-global interaction, by using global context to guide local captioning,and adaptively summarizing local captions into a coherent global caption.Besides, we construct GLaVE-Bench, a comprehensive video captioning benchmarkfeaturing 5X more queries per video than existing benchmarks, covering diversevisual dimensions to facilitate reliable evaluation. We further provide atraining dataset GLaVE-1.2M containing 16K high-quality fine-grained videocaptions and 1.2M related question-answer pairs. Extensive experiments on fourbenchmarks show that our GLaVE-Cap achieves state-of-the-art performance.Besides, the ablation studies and student model analyses further validate theeffectiveness of the proposed modules and the contribution of GLaVE-1.2M to thevideo understanding community. The source code, model weights, benchmark, anddataset will be open-sourced.</description>
      <author>example@mail.com (Wan Xu, Feng Zhu, Yihan Zeng, Yuanfan Guo, Ming Liu, Hang Xu, Wangmeng Zuo)</author>
      <guid isPermaLink="false">2509.11360v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Equalize: Data-Driven Frequency-Domain Signal Recovery in Molecular Communications</title>
      <link>http://arxiv.org/abs/2509.11327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于长短期记忆(LSTM)神经网络的频域均衡(FDE)技术，用于分子通信系统中的符号间干扰和噪声抑制，无需预先知道信道模型信息，同时保持高计算效率。&lt;h4&gt;背景&lt;/h4&gt;在分子通信中，符号间干扰(ISI)和噪声是影响通信可靠性的关键因素。时域均衡可有效减轻这些影响但计算复杂度高，而频域均衡计算效率更高却通常需要预先了解信道模型。&lt;h4&gt;目的&lt;/h4&gt;解决传统FDE方法对先验信道信息的依赖问题，提出基于LSTM神经网络的FDE技术，能够在分子通信信道中建模时间相关性，提高ISI和噪声抑制能力。&lt;h4&gt;方法&lt;/h4&gt;采用基于长短期记忆(LSTM)神经网络的频域均衡技术，通过监督训练策略实现信道自适应均衡，消除对信道先验信息的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提出的LSTM-FDE相比传统FDE和基于前馈神经网络的均衡器显著降低了误码率，这种性能提升归因于LSTM的时间建模能力增强了噪声抑制并加速了模型收敛。&lt;h4&gt;结论&lt;/h4&gt;基于LSTM神经网络的频域均衡技术能有效解决分子通信中的符号间干扰和噪声问题，无需预先知道信道模型信息，同时保持高计算效率，是一种有前景的分子通信均衡方案。&lt;h4&gt;翻译&lt;/h4&gt;在分子通信(MC)中，符号间干扰(ISI)和噪声是降低通信可靠性的关键因素。虽然时域均衡可以有效减轻这些影响，但它通常涉及与信道内存相关的高计算复杂度。相比之下，频域均衡(FDE)提供了更高的计算效率，但通常需要预先了解信道模型。为了解决这一限制，本文提出了一种基于长短期记忆(LSTM)神经网络的FDE技术，能够在分子通信信道中建模时间相关性，以提高ISI和噪声抑制能力。为了消除传统FDE方法对信道先验信息的依赖，采用监督训练策略进行信道自适应均衡。仿真结果表明，与传统的FDE和基于前馈神经网络的均衡器相比，所提出的LSTM-FDE显著降低了误码率。这一性能提升归因于LSTM的时间建模能力，它增强了噪声抑制并加速了模型收敛，同时保持了相当的计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In molecular communications (MC), inter-symbol interference (ISI) and noiseare key factors that degrade communication reliability. Although time-domainequalization can effectively mitigate these effects, it often entails highcomputational complexity concerning the channel memory. In contrast,frequency-domain equalization (FDE) offers greater computational efficiency buttypically requires prior knowledge of the channel model. To address thislimitation, this letter proposes FDE techniques based on long short-term memory(LSTM) neural networks, enabling temporal correlation modeling in MC channelsto improve ISI and noise suppression. To eliminate the reliance on priorchannel information in conventional FDE methods, a supervised training strategyis employed for channel-adaptive equalization. Simulation results demonstratethat the proposed LSTM-FDE significantly reduces the bit error rate compared totraditional FDE and feedforward neural network-based equalizers. Thisperformance gain is attributed to the LSTM's temporal modeling capabilities,which enhance noise suppression and accelerate model convergence, whilemaintaining comparable computational efficiency.</description>
      <author>example@mail.com (Cheng Xiang, Yu Huang, Miaowen Wen, Weiqiang Tan, Chan-Byoung Chae)</author>
      <guid isPermaLink="false">2509.11327v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Traffic-MLLM: A Spatio-Temporal MLLM with Retrieval-Augmented Generation for Causal Inference in Traffic</title>
      <link>http://arxiv.org/abs/2509.11165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Traffic-MLLM是一个基于Qwen2.5-VL的多模态大语言模型，通过LoRA微调和创新的知识提示模块，实现了在交通视频理解任务上的最先进性能，并具备优秀的零样本推理和跨场景泛化能力。&lt;h4&gt;背景&lt;/h4&gt;随着智能交通系统的发展，交通视频理解在全面场景感知和因果分析中扮演着越来越重要的角色。然而，现有方法在准确建模时空因果关系和集成领域特定知识方面面临显著挑战，限制了它们在复杂场景中的有效性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者提出了Traffic-MLLM，一个专门用于细粒度交通分析的多模态大语言模型。&lt;h4&gt;方法&lt;/h4&gt;基于Qwen2.5-VL主干构建，利用高质量交通特定多模态数据集，使用低秩自适应(LoRA)进行轻量级微调，增强建模视频序列中连续时空特征的能力。同时引入创新的知识提示模块，将思维链(CoT)推理与检索增强生成(RAG)融合，使详细的交通规则和领域知识能够精确注入推理过程。&lt;h4&gt;主要发现&lt;/h4&gt;在TrafficQA和DriveQA基准测试上的实验结果表明，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出显著的零样本推理和跨场景泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Traffic-MLLM模型能够有效解决交通视频理解中的时空因果关系建模和领域知识集成问题，在复杂场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;随着智能交通系统的发展，交通视频理解在全面场景感知和因果分析中扮演着越来越重要的角色。然而，现有方法在准确建模时空因果关系和集成领域特定知识方面面临显著挑战，限制了它们在复杂场景中的有效性。为了解决这些局限性，我们提出了Traffic-MLLM，一个专门用于细粒度交通分析的多模态大语言模型。基于Qwen2.5-VL主干构建，我们的模型利用高质量交通特定多模态数据集，并使用低秩自适应(LoRA)进行轻量级微调，显著增强了其建模视频序列中连续时空特征的能力。此外，我们引入了一个创新的知识提示模块，将思维链(CoT)推理与检索增强生成(RAG)融合，使详细的交通规则和领域知识能够精确注入推理过程。这种设计显著提升了模型的逻辑推理和知识适应能力。在TrafficQA和DriveQA基准测试上的实验结果表明，Traffic-MLLM取得了最先进的性能，验证了其处理多模态交通数据的卓越能力。它还表现出显著的零样本推理和跨场景泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As intelligent transportation systems advance, traffic video understandingplays an increasingly pivotal role in comprehensive scene perception and causalanalysis. Yet, existing approaches face notable challenges in accuratelymodeling spatiotemporal causality and integrating domain-specific knowledge,limiting their effectiveness in complex scenarios. To address theselimitations, we propose Traffic-MLLM, a multimodal large language modeltailored for fine-grained traffic analysis. Built on the Qwen2.5-VL backbone,our model leverages high-quality traffic-specific multimodal datasets and usesLow-Rank Adaptation (LoRA) for lightweight fine-tuning, significantly enhancingits capacity to model continuous spatiotemporal features in video sequences.Furthermore, we introduce an innovative knowledge prompting module fusingChain-of-Thought (CoT) reasoning with Retrieval-Augmented Generation (RAG),enabling precise injection of detailed traffic regulations and domain knowledgeinto the inference process. This design markedly boosts the model's logicalreasoning and knowledge adaptation capabilities. Experimental results onTrafficQA and DriveQA benchmarks show Traffic-MLLM achieves state-of-the-artperformance, validating its superior ability to process multimodal trafficdata. It also exhibits remarkable zero-shot reasoning and cross-scenariogeneralization capabilities.</description>
      <author>example@mail.com (Waikit Xiu, Qiang Lu, Xiying Li, Chen Hu, Shengbo Sun)</author>
      <guid isPermaLink="false">2509.11165v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning Representations in Video Game Agents with Supervised Contrastive Imitation Learning</title>
      <link>http://arxiv.org/abs/2509.11880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将监督对比学习(SupCon)应用于模仿学习(IL)的新方法，专注于为视频游戏环境中的智能体学习更有效的状态表示。&lt;h4&gt;背景&lt;/h4&gt;模仿学习在视频游戏环境中面临状态表示学习的挑战，需要捕捉观察结果与行动之间的因果关系。&lt;h4&gt;目的&lt;/h4&gt;获取观察结果的潜在表示，更好地捕捉与行动相关的因素，从而更好地建模观察结果映射到演示者执行动作的因果关系。&lt;h4&gt;方法&lt;/h4&gt;提出一种将SupCon损失与连续输出空间集成的方案，使SupCon能够在不限制环境动作类型的情况下运行。&lt;h4&gt;主要发现&lt;/h4&gt;在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，改进了表示质量，加快了学习收敛速度，并提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;将SupCon应用于IL可以有效地改善智能体在视频游戏环境中的学习性能，使其能够更好地捕捉观察结果与行动之间的因果关系。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了监督对比学习(SupCon)在模仿学习(IL)中的新应用，专注于为视频游戏环境中的智能体学习更有效的状态表示。目标是获取观察结果的潜在表示，更好地捕捉与行动相关的因素，从而更好地建模观察结果映射到演示者执行动作的因果关系，例如当前方出现障碍物时玩家会跳跃。我们提出了一种将SupCon损失与连续输出空间集成的方案，使SupCon能够在不限制环境动作类型的情况下运行。在3D游戏Astro Bot和Returnal以及多个2D Atari游戏上的实验表明，与仅使用监督动作预测损失函数训练的基线模型相比，改进了表示质量，加快了学习收敛速度，并提高了泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是在视频游戏环境中，如何让智能体通过高维视觉输入学习更有效的状态表示问题。这个问题在现实中很重要，因为当测试已发布游戏时，往往无法访问游戏的内部状态数据，只能通过视觉输入端到端训练智能体。这种方法需要更大的数据集和更长的训练时间，增加了过拟合风险，因此学习能识别与动作相关关键因素并泛化到新状态的有效表示至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到在视频游戏中，精确的空间配置对决策至关重要，而传统状态表示学习方法虽引入有用归纳偏置，但未能完全实现由智能体动作明确塑造表示的目标。他们借鉴了监督对比学习(SupCon)的思想，但发现其依赖的数据增强会扭曲游戏中的关键空间信息。因此，作者设计出使用动作标签而非数据增强来确定正负样本对的方法，并针对视频游戏中常见的连续和离散混合动作空间提出了具体处理策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在潜在嵌入空间中，根据动作对观察结果进行分组：导致相同动作的观察结果应具有相似表示，而与不同动作相关的观察结果应被很好分离。整体流程包括：1)构建包含特征提取器和策略网络的结构；2)组合预测任务损失和监督对比损失；3)将连续动作空间离散化为多个区间；4)使用混合基数位置编码将多维动作转换为单一分类标签；5)计算SupCon损失，同时优化两种损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将监督对比学习应用于模仿学习领域；2)提出不需要人工视图或正样本对增强的对比学习方法；3)针对连续和离散混合动作空间提出有效处理策略；4)解决小批量中可能不存在正样本对的问题。相比之前工作，这种方法不依赖数据增强避免空间信息扭曲，明确由智能体动作塑造表示而非仅受时间或物理动力学影响，专门针对模仿学习任务和视频游戏环境进行了改进，通过添加SupCon损失作为正则化项改善了表示质量、学习收敛速度和泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过将监督对比学习引入模仿学习领域并针对视频游戏环境特点进行改进，提出了一种能学习更有效状态表示的方法，显著提高了智能体的学习效率、收敛速度和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CoG64752.2025.11114174&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel application of Supervised Contrastive Learning(SupCon) to Imitation Learning (IL), with a focus on learning more effectivestate representations for agents in video game environments. The goal is toobtain latent representations of the observations that capture better theaction-relevant factors, thereby modeling better the cause-effect relationshipfrom the observations that are mapped to the actions performed by thedemonstrator, for example, the player jumps whenever an obstacle appears ahead.We propose an approach to integrate the SupCon loss with continuous outputspaces, enabling SupCon to operate without constraints regarding the type ofactions of the environment. Experiments on the 3D games Astro Bot and Returnal,and multiple 2D Atari games show improved representation quality, fasterlearning convergence, and better generalization compared to baseline modelstrained only with supervised action prediction loss functions.</description>
      <author>example@mail.com (Carlos Celemin, Joseph Brennan, Pierluigi Vito Amadori, Tim Bradley)</author>
      <guid isPermaLink="false">2509.11880v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Identity Learning for Unsupervised Visible-Infrared Person Re-Identification</title>
      <link>http://arxiv.org/abs/2509.11587v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层身份学习(HIL)框架，用于无监督可见光-红外行人重识别任务，通过多中心对比学习和双向反向选择传输机制解决了现有方法忽略细粒度差异的问题，在SYSU-MM01和RegDB数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;无监督可见光-红外行人重识别(USVI-ReID)旨在从无标注的跨模态行人数据集中学习模态不变图像特征，同时减少模态差距并最小化对昂贵人工标注的依赖。现有方法通常使用基于聚类的对比学习，将一个人表示为单一聚类中心，主要关注每个聚类内图像的共同性，而忽略了它们之间的细粒度差异。&lt;h4&gt;目的&lt;/h4&gt;解决现有无监督可见光-红外行人重识别方法中只关注聚类内图像共同性而忽略细粒度差异的问题，提高跨模态匹配质量。&lt;h4&gt;方法&lt;/h4&gt;提出分层身份学习(HIL)框架，包括：(1)通过二次聚类为每个现有的粗粒度聚类生成多个记忆，反映图像之间的细粒度变化；(2)提出多中心对比学习(MCCL)来优化表示，增强模内聚类并最小化跨模态差异；(3)设计双向反向选择传输(BRST)机制，通过执行伪标签的双向匹配建立可靠的跨模态对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;在SYSU-MM01和RegDB数据集上的大量实验表明，提出的方法优于现有方法，证明了HIL框架在无监督可见光-红外行人重识别任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;分层身份学习框架通过考虑细粒度差异和多中心表示，有效解决了现有无监督可见光-红外行人重识别方法的局限性，提高了跨模态匹配质量，为该领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;无监督可见光-红外行人重识别(USVI-ReID)旨在通过减少模态差距同时最小化对昂贵人工标注的依赖，从未标记的跨模态行人数据集中学习模态不变的图像特征。现有方法通常使用基于聚类的对比学习来解决USVI-ReID，将一个人表示为单一聚类中心。然而，它们主要关注每个聚类内图像的共同性，而忽略了它们之间的细粒度差异。为了解决这一局限性，我们提出了分层身份学习(HIL)框架。由于每个聚类可能包含几个反映图像间细粒度变化的小子聚类，我们通过二次聚类为每个现有的粗粒度聚类生成多个记忆。此外，我们提出了多中心对比学习(MCCL)来优化表示，以增强模内聚类并最小化跨模态差异。为了进一步提高跨模态匹配质量，我们设计了一种双向反向选择传输(BRST)机制，通过执行伪标签的双向匹配建立可靠的跨模态对应关系。在SYSU-MM01和RegDB数据集上进行的大量实验表明，所提出的方法优于现有方法。源代码可在以下网址获取：https://github.com/haonanshi0125/HIL。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised visible-infrared person re-identification (USVI-ReID) aims tolearn modality-invariant image features from unlabeled cross-modal persondatasets by reducing the modality gap while minimizing reliance on costlymanual annotations. Existing methods typically address USVI-ReID usingcluster-based contrastive learning, which represents a person by a singlecluster center. However, they primarily focus on the commonality of imageswithin each cluster while neglecting the finer-grained differences among them.To address the limitation, we propose a Hierarchical Identity Learning (HIL)framework. Since each cluster may contain several smaller sub-clusters thatreflect fine-grained variations among images, we generate multiple memories foreach existing coarse-grained cluster via a secondary clustering. Additionally,we propose Multi-Center Contrastive Learning (MCCL) to refine representationsfor enhancing intra-modal clustering and minimizing cross-modal discrepancies.To further improve cross-modal matching quality, we design a BidirectionalReverse Selection Transmission (BRST) mechanism, which establishes reliablecross-modal correspondences by performing bidirectional matching ofpseudo-labels. Extensive experiments conducted on the SYSU-MM01 and RegDBdatasets demonstrate that the proposed method outperforms existing approaches.The source code is available at: https://github.com/haonanshi0125/HIL.</description>
      <author>example@mail.com (Haonan Shi, Yubin Wang, De Cheng, Lingfeng He, Nannan Wang, Xinbo Gao)</author>
      <guid isPermaLink="false">2509.11587v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Promoting Shape Bias in CNNs: Frequency-Based and Contrastive Regularization for Corruption Robustness</title>
      <link>http://arxiv.org/abs/2509.11355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种互补的正则化策略，通过减少CNN对高频纹理的依赖和促进形状感知表示，提高模型对图像损坏的鲁棒性，同时保持原始分类性能。&lt;h4&gt;背景&lt;/h4&gt;卷积神经网络在图像分类方面表现出色，但对人类容易处理的常见损坏仍然脆弱。这种脆弱性的一个关键原因是CNN依赖局部纹理线索而非全局物体形状，这与人类感知形成鲜明对比。&lt;h4&gt;目的&lt;/h4&gt;提出两种互补的正则化策略，旨在促进形状偏向的表示并增强CNN对图像损坏的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;第一种方法引入辅助损失，强制原始输入和低频滤波输入之间的特征一致性，减少对高频纹理的依赖；第二种方法结合监督对比学习，围绕类一致、形状相关的表示来构建特征空间。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10-C基准测试上评估，两种方法都提高了对图像损坏的鲁棒性，同时没有降低干净数据的准确性。&lt;h4&gt;结论&lt;/h4&gt;损失级别的正则化可以有效引导CNN朝向更形状感知、更有韧性的表示。&lt;h4&gt;翻译&lt;/h4&gt;卷积神经网络在图像分类方面表现出色，但对人类容易处理的常见损坏仍然脆弱。这种脆弱性的一个关键原因是它们依赖局部纹理线索而非全局物体形状——这与人类感知形成鲜明对比。为此，我们提出两种互补的正则化策略，旨在促进形状偏向的表示并增强鲁棒性。第一种引入辅助损失，强制原始输入和低频滤波输入之间的特征一致性，减少对高频纹理的依赖。第二种结合监督对比学习，围绕类一致、形状相关的表示来构建特征空间。在CIFAR-10-C基准测试上评估，两种方法都提高了对损坏的鲁棒性，同时没有降低干净数据的准确性。我们的结果表明，损失级别的正则化可以有效引导CNN朝向更形状感知、更有韧性的表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Convolutional Neural Networks (CNNs) excel at image classification but remainvulnerable to common corruptions that humans handle with ease. A key reason forthis fragility is their reliance on local texture cues rather than globalobject shapes -- a stark contrast to human perception. To address this, wepropose two complementary regularization strategies designed to encourageshape-biased representations and enhance robustness. The first introduces anauxiliary loss that enforces feature consistency between original andlow-frequency filtered inputs, discouraging dependence on high-frequencytextures. The second incorporates supervised contrastive learning to structurethe feature space around class-consistent, shape-relevant representations.Evaluated on the CIFAR-10-C benchmark, both methods improve corruptionrobustness without degrading clean accuracy. Our results suggest thatloss-level regularization can effectively steer CNNs toward more shape-aware,resilient representations.</description>
      <author>example@mail.com (Robin Narsingh Ranabhat, Longwei Wang, Amit Kumar Patel, KC santosh)</author>
      <guid isPermaLink="false">2509.11355v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Network Representation Learning</title>
      <link>http://arxiv.org/abs/2509.11316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应对比边表示学习(ACERL)的新方法，用于处理脑连接数据分析中的挑战，包括个体特异性、高维性和稀疏性网络。该方法基于对比学习和自适应随机掩码机制，在边表示学习中达到最小最优收敛率，并在多种下游任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;网络表示学习旨在将网络嵌入低维空间同时保留结构和语义属性，但脑连接数据分析面临个体特异性、高维性和稀疏性网络的挑战，且缺乏节点或边协变量。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于对比学习的统计方法用于网络边嵌入，解决脑连接数据分析中的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出自适应对比边表示学习(ACERL)，包含两个关键组件：增强网络对的对比学习和数据驱动的自适应随机掩码机制，并建立非渐近误差边界。&lt;h4&gt;主要发现&lt;/h4&gt;ACERL在边表示学习中达到最小最优收敛率，学习到的表示可用于网络分类、重要边检测和社区检测等下游任务，且对这些任务有理论保证。通过合成数据和真实脑连接研究验证了该方法，与稀疏主成分分析相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;ACERL方法能有效处理脑连接数据分析中的挑战，在多种任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;网络表示学习寻求将网络嵌入到低维空间，同时保留结构和语义属性，从而促进下游任务，如分类、特征预测、边识别和社区检测。受脑连接数据分析挑战的启发，这些数据具有个体特异性、高维性和稀疏性网络的特点，且缺乏节点或边协变量，我们提出了一种新颖的基于对比学习的统计方法用于网络边嵌入，命名为自适应对比边表示学习(ACERL)。它基于两个关键组件：增强网络对的对比学习和数据驱动的自适应随机掩码机制。我们建立了非渐近误差边界，并证明我们的方法在边表示学习中达到了最小最优收敛率。我们进一步展示了学习到的表示在多种下游任务中的适用性，包括网络分类、重要边检测和社区检测，并建立了相应的理论保证。我们通过合成数据和真实脑连接研究验证了我们的方法，并显示出与稀疏主成分分析基线方法相比具有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network representation learning seeks to embed networks into alow-dimensional space while preserving the structural and semantic properties,thereby facilitating downstream tasks such as classification, trait prediction,edge identification, and community detection. Motivated by challenges in brainconnectivity data analysis that is characterized by subject-specific,high-dimensional, and sparse networks that lack node or edge covariates, wepropose a novel contrastive learning-based statistical approach for networkedge embedding, which we name as Adaptive Contrastive Edge RepresentationLearning (ACERL). It builds on two key components: contrastive learning ofaugmented network pairs, and a data-driven adaptive random masking mechanism.We establish the non-asymptotic error bounds, and show that our method achievesthe minimax optimal convergence rate for edge representation learning. Wefurther demonstrate the applicability of the learned representation in multipledownstream tasks, including network classification, important edge detection,and community detection, and establish the corresponding theoreticalguarantees. We validate our method through both synthetic data and real brainconnectivities studies, and show its competitive performance compared to thebaseline method of sparse principal components analysis.</description>
      <author>example@mail.com (Zihan Dong, Xin Zhou, Ryumei Nakada, Lexin Li, Linjun Zhang)</author>
      <guid isPermaLink="false">2509.11316v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>AlignKT: Explicitly Modeling Knowledge State for Knowledge Tracing with Ideal State Alignment</title>
      <link>http://arxiv.org/abs/2509.11135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AlignKT的新方法，用于知识追踪(KT)，采用前端到后端架构明确建模稳定知识状态，通过对比学习增强对齐鲁棒性，实验证明在三个数据集上优于七种基线方法。&lt;h4&gt;背景&lt;/h4&gt;知识追踪(KT)是智能教学系统(ITS)的基本组件，通过建模学习者知识状态监控学习进度。然而现有KT模型主要关注拟合交互序列，忽视知识状态本身，导致可解释性降低和教学支持不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有KT模型可解释性不足和教学支持不够的问题，通过明确建模稳定知识状态提高KT模型性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AlignKT采用前端到后端架构，定义基于教学理论的理想知识状态作为对齐标准，使用五个编码器实现，并加入对比学习模块增强对齐过程的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验，AlignKT在三个真实数据集上优于七种KT基线方法，在两个数据集上达到最先进结果，在第三个数据集上也表现具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;AlignKT通过明确建模知识状态并引入对齐标准，成功提高了KT模型的性能和可解释性，为智能教学系统提供了更好的教学支持。&lt;h4&gt;翻译&lt;/h4&gt;知识追踪(KT)是智能教学系统(ITS)的基本组成部分，通过建模学习者的知识状态，使这些系统能够监控和理解学习者的进步。然而，许多现有的KT模型主要关注拟合学习者交互序列，而常常忽视知识状态本身。这一限制降低了可解释性，并为ITS提供了不足的教学支持。为了应对这一挑战，我们提出了AlignKT，它采用前端到后端的架构来明确建模稳定的知识状态。在此方法中，初步知识状态与额外的标准对齐。具体来说，我们基于教学理论定义了一个理想知识状态作为对齐标准，为可解释性提供了基础。我们使用五个编码器来实现此设置，并采用对比学习模块来增强对齐过程的鲁棒性。通过大量实验，AlignKT表现出优越的性能，在三个真实数据集上优于七种KT基线方法。它在其中两个数据集上取得了最先进的结果，在第三个数据集上也表现出具有竞争力的性能。本工作的代码可在https://github.com/SCNU203/AlignKT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Tracing (KT) serves as a fundamental component of IntelligentTutoring Systems (ITS), enabling these systems to monitor and understandlearners' progress by modeling their knowledge state. However, many existing KTmodels primarily focus on fitting the sequences of learners' interactions, andoften overlook the knowledge state itself. This limitation leads to reducedinterpretability and insufficient instructional support from the ITS. Toaddress this challenge, we propose AlignKT, which employs a frontend-to-backendarchitecture to explicitly model a stable knowledge state. In this approach,the preliminary knowledge state is aligned with an additional criterion.Specifically, we define an ideal knowledge state based on pedagogical theoriesas the alignment criterion, providing a foundation for interpretability. Weutilize five encoders to implement this set-up, and incorporate a contrastivelearning module to enhance the robustness of the alignment process. Throughextensive experiments, AlignKT demonstrates superior performance, outperformingseven KT baselines on three real-world datasets. It achieves state-of-the-artresults on two of these datasets and exhibits competitive performance on thethird. The code of this work is available athttps://github.com/SCNU203/AlignKT.</description>
      <author>example@mail.com (Jing Xiao, Chang You, Zhiyu Chen)</author>
      <guid isPermaLink="false">2509.11135v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SPARK: Adaptive Low-Rank Knowledge Graph Modeling in Hybrid Geometric Spaces for Recommendation</title>
      <link>http://arxiv.org/abs/2509.11094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM' 25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SPARK，一个多阶段框架，用于解决知识图谱增强推荐系统中的噪声、稀疏性和几何表示问题，特别关注长尾项目的推荐效果。&lt;h4&gt;背景&lt;/h4&gt;知识图谱增强推荐系统面临固有噪声、数据稀疏性和欧几里得几何不适合复杂关系结构的挑战，这些问题损害了表示学习，特别是对于长尾实体。现有方法通常缺乏针对项目流行度的自适应多源信号融合。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效处理知识图谱噪声和稀疏性，同时为长尾项目提供更好表示的推荐系统框架，特别关注主流和长尾项目的精确建模。&lt;h4&gt;方法&lt;/h4&gt;SPARK框架采用多阶段方法：首先使用Tucker低秩分解去噪知识图谱并生成鲁棒实体表示；然后采用SVD初始化的混合几何图神经网络同时在欧几里得和双曲空间中学习表示，利用双曲空间建模层次结构的能力；引入项目流行度感知的自适应融合策略，动态加权来自协作过滤、精炼知识图谱嵌入和不同几何空间的信号；最后使用对比学习对齐多源表示。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，SPARK在最新的方法上表现出显著的优越性，特别是在改善长尾项目推荐方面，为知识增强推荐提供了一种稳健且原则性的方法。&lt;h4&gt;结论&lt;/h4&gt;SPARK框架通过系统性地解决知识图谱增强推荐系统中的关键挑战，特别是长尾项目的表示问题，提供了一个创新且有效的解决方案，实现了主流和长尾项目的精确建模。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱增强推荐系统，但面临固有噪声、稀疏性和欧几里得几何不适合复杂关系结构的挑战，严重损害了表示学习，特别是对于长尾实体。现有方法通常也缺乏针对项目流行度的自适应多源信号融合。本文介绍了SPARK，一个新颖的多阶段框架，系统性地解决这些问题。SPARK首先采用Tucker低秩分解对知识图谱去噪并生成鲁棒实体表示。随后，一个SVD初始化的混合几何图神经网络同时在欧几里得和双曲空间中学习表示；后者被战略性地利用，因为它擅长建模层次结构，有效捕获稀疏长尾项目的语义特征。核心贡献是一个项目流行度感知的自适应融合策略，动态加权来自协作过滤、精炼知识图谱嵌入和不同几何空间的信号，精确建模主流和长尾项目。最后，对比学习对齐这些多源表示。大量实验证明了SPARK在最新方法上的显著优越性，特别是在改善长尾项目推荐方面，为知识增强推荐提供了一种稳健且原则性的方法。实现代码可在https://github.com/Applied-Machine-Learning-Lab/SPARK获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge Graphs (KGs) enhance recommender systems but face challenges frominherent noise, sparsity, and Euclidean geometry's inadequacy for complexrelational structures, critically impairing representation learning, especiallyfor long-tail entities. Existing methods also often lack adaptive multi-sourcesignal fusion tailored to item popularity. This paper introduces SPARK, a novelmulti-stage framework systematically tackling these issues. SPARK first employsTucker low-rank decomposition to denoise KGs and generate robust entityrepresentations. Subsequently, an SVD-initialized hybrid geometric GNNconcurrently learns representations in Euclidean and Hyperbolic spaces; thelatter is strategically leveraged for its aptitude in modeling hierarchicalstructures, effectively capturing semantic features of sparse, long-tail items.A core contribution is an item popularity-aware adaptive fusion strategy thatdynamically weights signals from collaborative filtering, refined KGembeddings, and diverse geometric spaces for precise modeling of bothmainstream and long-tail items. Finally, contrastive learning aligns thesemulti-source representations. Extensive experiments demonstrate SPARK'ssignificant superiority over state-of-the-art methods, particularly inimproving long-tail item recommendation, offering a robust, principled approachto knowledge-enhanced recommendation. Implementation code is available athttps://github.com/Applied-Machine-Learning-Lab/SPARK.</description>
      <author>example@mail.com (Binhao Wang, Yutian Xiao, Maolin Wang, Zhiqi Li, Tianshuo Wei, Ruocheng Guo, Xiangyu Zhao)</author>
      <guid isPermaLink="false">2509.11094v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>An Advanced Convolutional Neural Network for Bearing Fault Diagnosis under Limited Data</title>
      <link>http://arxiv.org/abs/2509.11053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对有限数据条件下轴承故障诊断的先进数据增强和对比傅里叶卷积框架(DAC-FCF)，通过结合新型生成对抗网络、对比学习和傅里叶卷积神经网络，有效解决了数据稀缺、特征提取不充分和样本关系建模不足等问题。&lt;h4&gt;背景&lt;/h4&gt;在轴承故障诊断领域，深度学习方法被广泛应用，但由于高成本或隐私问题，实际场景中高质量的标记数据稀缺。虽然少样本学习在解决数据稀缺问题上显示出潜力，但现有方法仍面临显著限制。&lt;h4&gt;目的&lt;/h4&gt;解决传统数据增强技术生成低质量样本、传统卷积神经网络难以提取全局特征以及现有方法无法建模有限训练样本间复杂关系等问题，为有限数据条件下的轴承故障诊断提供有效解决方案。&lt;h4&gt;方法&lt;/h4&gt;提出DAC-FCF框架，包含三个核心组件：1) 条件一致潜在表示和重建生成对抗网络(CCLR-GAN)用于生成更多样化的数据；2) 基于对比学习的联合优化机制用于建模训练数据间的关系；3) 一维傅里叶卷积神经网络(1D-FCNN)用于实现输入数据的全局感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明DAC-FCF取得了显著改进，在CWRU数据集上比基线方法提高最多32%，在自收集测试台上提高10%。大量的消融实验证明了所提出组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的DAC-FCF为有限数据条件下的轴承故障诊断提供了有前景的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在轴承故障诊断领域，深度学习方法最近已被广泛应用。然而，由于高成本或隐私问题，实际场景中高质量的标记数据稀缺。虽然少样本学习在解决数据稀缺问题上显示出潜力，但现有方法在此领域仍面临显著限制。传统数据增强技术常遭受模式崩溃，生成无法捕捉轴承故障模式多样性的低质量样本。此外，具有局部感受野的传统卷积神经网络不足以从复杂振动信号中提取全局特征。另外，现有方法无法建模有限训练样本之间的复杂关系。为解决这些问题，我们提出了一种针对有限数据条件下轴承故障诊断的先进数据增强和对比傅里叶卷积框架(DAC-FCF)。首先，提出了一种新型的条件一致潜在表示和重建生成对抗网络(CCLR-GAN)来生成更多样化的数据。其次，利用基于对比学习的联合优化机制来更好地建模可用训练数据之间的关系。最后，我们提出了一维傅里叶卷积神经网络(1D-FCNN)来实现对输入数据的全局感知。实验证明DAC-FCF取得了显著改进，在凯斯西储大学(CWRU)数据集上比基线方法提高最多32%，在自收集测试台上提高10%。大量的消融实验证明了所提出组件的有效性。因此，提出的DAC-FCF为有限数据条件下的轴承故障诊断提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the area of bearing fault diagnosis, deep learning (DL) methods have beenwidely used recently. However, due to the high cost or privacy concerns,high-quality labeled data are scarce in real world scenarios. While few-shotlearning has shown promise in addressing data scarcity, existing methods stillface significant limitations in this domain. Traditional data augmentationtechniques often suffer from mode collapse and generate low-quality samplesthat fail to capture the diversity of bearing fault patterns. Moreover,conventional convolutional neural networks (CNNs) with local receptive fieldsmakes them inadequate for extracting global features from complex vibrationsignals. Additionally, existing methods fail to model the intricaterelationships between limited training samples. To solve these problems, wepropose an advanced data augmentation and contrastive fourier convolutionframework (DAC-FCF) for bearing fault diagnosis under limited data. Firstly, anovel conditional consistent latent representation and reconstructiongenerative adversarial network (CCLR-GAN) is proposed to generate more diversedata. Secondly, a contrastive learning based joint optimization mechanism isutilized to better model the relations between the available training data.Finally, we propose a 1D fourier convolution neural network (1D-FCNN) toachieve a global-aware of the input data. Experiments demonstrate that DAC-FCFachieves significant improvements, outperforming baselines by up to 32\% oncase western reserve university (CWRU) dataset and 10\% on a self-collectedtest bench. Extensive ablation experiments prove the effectiveness of theproposed components. Thus, the proposed DAC-FCF offers a promising solution forbearing fault diagnosis under limited data.</description>
      <author>example@mail.com (Shengke Sun, Shuzhen Han, Ziqian Luan, Xinghao Qin, Jiao Yin, Zhanshan Zhao, Jinli Cao, Hua Wang)</author>
      <guid isPermaLink="false">2509.11053v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>OpenUrban3D: Annotation-Free Open-Vocabulary Semantic Segmentation of Large-Scale Urban Point Clouds</title>
      <link>http://arxiv.org/abs/2509.10842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpenUrban3D是首个不依赖对齐多视图图像、预训练点云分割网络或手动标注的3D开放词汇语义分割框架，可直接从原始点云生成语义特征，实现任意文本查询的零样本分割。&lt;h4&gt;背景&lt;/h4&gt;开放词汇语义分割能识别和分割来自任意自然语言描述的对象，对大规模城市点云应用至关重要，但该领域研究不足，主要受限于高质量多视图图像的缺乏和现有3D分割方法的泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;解决大规模城市点云数据集中缺乏高质量多视图图像和现有3D分割方法泛化能力差的问题，开发一个不依赖对齐多视图图像、预训练网络或手动标注的3D开放词汇语义分割框架。&lt;h4&gt;方法&lt;/h4&gt;通过多视图多粒度渲染、掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成语义特征，然后蒸馏到3D骨干模型，实现零样本分割并保留语义和几何信息。&lt;h4&gt;主要发现&lt;/h4&gt;在SensatUrban和SUM等大规模城市基准测试上，OpenUrban3D在分割精度和跨场景泛化能力上显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;OpenUrban3D为大规模城市场景理解提供了灵活且可扩展的解决方案，无需依赖对齐多视图图像或预训练模型。&lt;h4&gt;翻译&lt;/h4&gt;开放词汇语义分割使模型能够识别和分割来自任意自然语言描述的对象，提供了处理固定标签集之外的新的、细粒度的或功能定义的类别的灵活性。虽然这种能力对于支持数字孪生、智能城市管理和城市分析的大规模城市点云至关重要，但在这个领域仍 largely unexplored。主要障碍是大规模城市点云数据集中经常缺乏高质量、对齐良好的多视图图像，以及现有的三维分割管道在几何、尺度和外观差异大的多样化城市环境中泛化能力差。为了解决这些挑战，我们提出了OpenUrban3D，这是第一个大规模城市场景的3D开放词汇语义分割框架，它可以在没有对齐多视图图像、预训练点云分割网络或手动标注的情况下运行。我们的方法通过多视图、多粒度渲染、掩码级视觉-语言特征提取和样本平衡融合，直接从原始点云生成强大的语义特征，然后将其蒸馏到3D骨干模型中。这种设计能够实现任意文本查询的零样本分割，同时捕获语义丰富性和几何先验。在SensatUrban和SUM等大规模城市基准测试上的广泛实验表明，OpenUrban3D在分割精度和跨场景泛化方面都显著优于现有方法，证明了其作为3D城市场景理解的灵活且可扩展解决方案的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决大规模城市点云的开集词汇语义分割问题，具体包括两个关键挑战：1) 大规模城市点云数据集常缺乏高质量对齐的多视图图像；2) 现有3D分割管道在不同城市环境中的泛化能力差。这个问题在现实中非常重要，因为城市点云支持数字孪生、智能城市管理等应用，而传统方法依赖预定义封闭类别标签，导致标注成本高昂且无法处理未见过的类别或功能定义区域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：室内场景的开集词汇方法依赖对齐的2D图像，而城市点云数据缺乏这种图像；'先分割后识别'策略在城市场景中表现不佳。因此设计了无需对齐图像、预训练网络或手动标注的框架。方法借鉴了OpenScene的2D到3D知识蒸馏策略，利用2D视觉-语言模型(如CLIP)进行特征提取，并参考了'先分割后识别'的范式，但针对城市场景进行了改进，设计了专门的多视图多粒度投影方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从原始点云生成鲁棒语义特征，不依赖对齐图像或预训练3D分割网络，通过多视图渲染捕获不同尺度对象，利用掩码级视觉-语言特征提取和样本平衡融合，然后将知识蒸馏到3D模型。整体流程包括：1)多视图多粒度投影生成虚拟图像；2)提取2D掩码特征并反投影到点云；3)样本平衡特征融合构建2D特征库；4)2D到3D知识蒸馏训练3D主干；5)推理时融合2D-3D特征，通过余弦相似度实现任意文本查询的分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个仅需要原始点云输入的大规模城市3D开集词汇语义分割框架；2)多视图多粒度投影、掩码级特征提取和2D-3D知识蒸馏的定制化技术流程；3)样本平衡特征融合(SBFF)解决数据不平衡问题。相比之前工作，OpenUrban3D不依赖对齐RGB图像或预训练3D分割网络，能处理大规模物体尺度变化，在跨场景泛化方面表现更好，并通过2D-3D特征融合结合了语义识别能力和几何先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenUrban3D首次实现了无需对齐图像、预训练分割网络或手动标注的大规模城市点云开集词汇语义分割，通过多视图多粒度渲染、掩码级特征提取和2D-3D知识蒸馏，显著提升了分割精度和跨场景泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary semantic segmentation enables models to recognize and segmentobjects from arbitrary natural language descriptions, offering the flexibilityto handle novel, fine-grained, or functionally defined categories beyond fixedlabel sets. While this capability is crucial for large-scale urban point cloudsthat support applications such as digital twins, smart city management, andurban analytics, it remains largely unexplored in this domain. The mainobstacles are the frequent absence of high-quality, well-aligned multi-viewimagery in large-scale urban point cloud datasets and the poor generalizationof existing three-dimensional (3D) segmentation pipelines across diverse urbanenvironments with substantial variation in geometry, scale, and appearance. Toaddress these challenges, we present OpenUrban3D, the first 3D open-vocabularysemantic segmentation framework for large-scale urban scenes that operateswithout aligned multi-view images, pre-trained point cloud segmentationnetworks, or manual annotations. Our approach generates robust semanticfeatures directly from raw point clouds through multi-view, multi-granularityrendering, mask-level vision-language feature extraction, and sample-balancedfusion, followed by distillation into a 3D backbone model. This design enableszero-shot segmentation for arbitrary text queries while capturing both semanticrichness and geometric priors. Extensive experiments on large-scale urbanbenchmarks, including SensatUrban and SUM, show that OpenUrban3D achievessignificant improvements in both segmentation accuracy and cross-scenegeneralization over existing methods, demonstrating its potential as a flexibleand scalable solution for 3D urban scene understanding.</description>
      <author>example@mail.com (Chongyu Wang, Kunlei Jing, Jihua Zhu, Di Wang)</author>
      <guid isPermaLink="false">2509.10842v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Learning Contact Dynamics for Control with Action-conditioned Face Interaction Graph Networks</title>
      <link>http://arxiv.org/abs/2509.12151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可学习的物理模拟器，能够准确预测机器人在接触丰富操作中末端执行器的运动和力矩。该模型扩展了现有的GNN-based模拟器，实现了条件动作预测，并在模拟和真实世界实验中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的物理模拟器在处理接触丰富的操作任务时存在局限性，特别是在预测机器人末端执行器的运动和力矩方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测机器人末端执行器在接触丰富操作中的运动和力矩的可学习物理模拟器，提高预测精度和性能。&lt;h4&gt;方法&lt;/h4&gt;扩展了基于图神经网络(GNN)的最先进模拟器FIGNET，引入了新的节点和边类型，实现了对控制任务和状态估计任务的条件动作预测。在模拟中使用MPC代理进行测试，并在真实世界实验中验证性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在模拟中，使用该模型的MPC代理在孔插入任务中与使用真实动力学模型的控制器性能相当。2. 在真实世界实验中，该模型相比基线物理模拟器，运动预测准确率提高了50%，力矩预测精度提高了3倍。3. 源代码和数据已公开可用。&lt;h4&gt;结论&lt;/h4&gt;所提出的可学习物理模拟器在机器人末端执行器的运动和力矩预测方面表现出色，能够有效支持控制任务和状态估计任务，且源代码和数据已公开，便于进一步研究和应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种可学习的物理模拟器，能够准确预测机器人在接触丰富操作中末端执行器的运动和力矩。所提出的模型通过引入新的节点和边类型扩展了最先进的基于图神经网络的模拟器(FIGNet)，实现了对控制任务和状态估计任务的条件动作预测。在模拟中，使用我们模型的MPC代理在具有挑战性的孔插入任务中与使用真实动力学模型的相同控制器性能相匹配；而在真实世界实验中，我们的模型相比基线物理模拟器，运动预测准确率提高了50%，力矩预测精度提高了3倍。源代码和数据已公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a learnable physics simulator that provides accurate motion andforce-torque prediction of robot end effectors in contact-rich manipulation.The proposed model extends the state-of-the-art GNN-based simulator (FIGNet)with novel node and edge types, enabling action-conditional predictions forcontrol and state estimation tasks. In simulation, the MPC agent using ourmodel matches the performance of the same controller with the ground truthdynamics model in a challenging peg-in-hole task, while in the real-worldexperiment, our model achieves a 50% improvement in motion prediction accuracyand 3$\times$ increase in force-torque prediction precision over the baselinephysics simulator. Source code and data are publicly available.</description>
      <author>example@mail.com (Zongyao Yi, Joachim Hertzberg, Martin Atzmueller)</author>
      <guid isPermaLink="false">2509.12151v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>End-to-End Visual Autonomous Parking via Control-Aided Attention</title>
      <link>http://arxiv.org/abs/2509.11090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CAA-Policy的端到端模仿学习系统，通过控制辅助注意力机制(CAA)解决了精确停车中感知和控制协同不足的问题，实现了更准确、鲁棒且可解释的停车决策。&lt;h4&gt;背景&lt;/h4&gt;精确停车需要一个端到端系统，感知系统需自适应提供与策略相关的细节，特别是在需要精细控制决策的关键区域。现有的端到端学习方法在感知和控制之间缺乏有效的协同作用，且transformer-based自注意力机制单独使用时往往产生不稳定和不一致的空间注意力。&lt;h4&gt;目的&lt;/h4&gt;解决现有端到端学习方法中感知和控制协同不足的问题，提高停车系统的准确性、鲁棒性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出CAA-Policy系统，引入新颖的Control-Aided Attention (CAA)机制，允许控制信号指导视觉注意力的学习；首次以自监督方式训练注意力模块，使用来自控制输出的反向传播梯度而非训练损失；集成短时程航点预测作为辅助任务；引入单独训练的运动预测模块以稳健跟踪目标位置。&lt;h4&gt;主要发现&lt;/h4&gt;仅基于transformer的自注意力机制会产生不稳定和时间上不一致的空间注意力，削弱下游策略决策的可靠性；使用控制输出的梯度训练注意力可促使注意力集中在导致动作输出高方差的视觉特征上，而非仅最小化训练损失；这种转变带来了更稳健和可推广的策略。&lt;h4&gt;结论&lt;/h4&gt;在CARLA模拟器中的大量实验表明，CAA-Policy持续优于端到端学习基线和模块化的BEV分割+混合A*流水线，实现了更高的准确性、鲁棒性和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;精确停车需要一个端到端系统，其中感知系统自适应地提供与策略相关的细节-特别是在需要精细控制决策的关键区域。端到端学习通过直接将传感器输入映射到控制动作提供了一个统一的框架，但现有方法在感知和控制之间缺乏有效的协同作用。我们发现，单独使用基于transformer的自注意力机制往往会产生不稳定且时间上不一致的空间注意力，这随时间削弱了下游策略决策的可靠性。相反，我们提出了CAA-Policy，一个端到端的模仿学习系统，通过新颖的控制辅助注意力(CAA)机制允许控制信号指导视觉注意力的学习。我们首次以自监督方式训练这样的注意力模块，使用从控制输出反向传播的梯度而非来自训练损失的梯度。这种策略鼓励注意力集中在导致动作输出高方差的视觉特征上，而不仅仅是最小化训练损失-我们证明这种转变带来了更稳健和可推广的策略。为进一步增强稳定性，CAA-Policy集成了短时程航点预测作为辅助任务，并引入了单独训练的运动预测模块以随时间稳健地跟踪目标位置。在CARLA模拟器中的大量实验表明，CAA-Policy持续优于端到端学习基线和模块化的BEV分割+混合A*流水线，实现了更高的准确性、鲁棒性和可解释性。代码已在https://github.com/Joechencc/CAAPolicy发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise parking requires an end-to-end system where perception adaptivelyprovides policy-relevant details-especially in critical areas where finecontrol decisions are essential. End-to-end learning offers a unified frameworkby directly mapping sensor inputs to control actions, but existing approacheslack effective synergy between perception and control. We find thattransformer-based self-attention, when used alone, tends to produce unstableand temporally inconsistent spatial attention, which undermines the reliabilityof downstream policy decisions over time. Instead, we propose CAA-Policy, anend-to-end imitation learning system that allows control signal to guide thelearning of visual attention via a novel Control-Aided Attention (CAA)mechanism. For the first time, we train such an attention module in aself-supervised manner, using backpropagated gradients from the control outputsinstead of from the training loss. This strategy encourages the attention tofocus on visual features that induce high variance in action outputs, ratherthan merely minimizing the training loss-a shift we demonstrate leads to a morerobust and generalizable policy. To further enhance stability, CAA-Policyintegrates short-horizon waypoint prediction as an auxiliary task, andintroduces a separately trained motion prediction module to robustly track thetarget spot over time. Extensive experiments in the CARLA simulator show that\titlevariable~consistently surpasses both the end-to-end learning baseline andthe modular BEV segmentation + hybrid A* pipeline, achieving superior accuracy,robustness, and interpretability. Code is released athttps://github.com/Joechencc/CAAPolicy.</description>
      <author>example@mail.com (Chao Chen, Shunyu Yao, Yuanwu He, Tao Feng, Ruojing Song, Yuliang Guo, Xinyu Huang, Chenxu Wu, Ren Liu, Chen Feng)</author>
      <guid isPermaLink="false">2509.11090v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3D Human Pose and Shape Estimation from LiDAR Point Clouds: A Review</title>
      <link>http://arxiv.org/abs/2509.12197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对从野外LiDAR点云进行3D人体姿态估计和人体网格恢复的研究进行了全面综述，提出了结构化分类方法，并分析了各种方法的优缺点。&lt;h4&gt;背景&lt;/h4&gt;从野外LiDAR点云进行3D人体姿态估计和人体网格恢复是计算机视觉领域的重要研究方向。&lt;h4&gt;目的&lt;/h4&gt;对现有方法进行系统比较，提出分类框架，分析方法的优缺点，并建立评估基准以促进该领域的发展。&lt;h4&gt;方法&lt;/h4&gt;对三个常用数据集进行定量比较，编写评估指标的统一定义，在数据集上建立两个任务的基准表格，维护一个配套网页持续更新研究。&lt;h4&gt;主要发现&lt;/h4&gt;提出了结构化分类方法，分析了各种方法的优缺点和设计选择，建立了评估基准，明确了开放挑战和研究方向。&lt;h4&gt;结论&lt;/h4&gt;基于LiDAR的3D人体理解仍面临开放挑战，需要进一步研究，作者提供了持续更新的资源平台。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们对从野外LiDAR点云进行3D人体姿态估计和人体网格恢复的研究进行了全面综述。我们在多个关键维度上比较了现有方法，并提出了一个结构化的分类方法来对这些方法进行分类。按照这一分类方法，我们分析了每种方法的优点、局限性和设计选择。此外，(i)我们对三个最常用的数据集进行了定量比较，详细描述了它们的特征；(ii)我们编写了所有评估指标的统一定义；(iii)我们在这些数据集上为两个任务建立了基准表格，以实现公平比较并促进该领域的进步。我们还概述了推动基于LiDAR的3D人体理解的关键开放挑战和研究方向。此外，我们维护了一个配套网页，根据我们的分类组织论文并持续更新新研究：https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决对从LiDAR点云进行3D人体姿态估计和人体网格恢复的全面综述问题。这个问题在现实中非常重要，因为这些技术在自动驾驶、虚拟现实、人机交互、医疗保健等领域有广泛应用，能帮助系统准确理解和预测人类行为，提高安全性和交互体验。LiDAR传感器提供精确的3D几何信息，不受光照影响，且保护隐私，是这些应用的关键技术。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析现有文献，提出一个结构化的分类法来组织各种方法，并分析它们的优缺点。作者借鉴了先前关于3D人体姿态估计和人体网格恢复的综述工作，但发现现有综述主要关注图像或视频方法，缺乏对LiDAR传感器的专门关注。因此，作者填补了这一空白，专注于从LiDAR点云进行3D人体理解的方法，并整合了多传感器融合、弱监督学习等先进技术思路。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为综述论文，其核心思想是提供一个全面的框架来理解和分类从LiDAR点云进行3D人体姿态估计和人体网格恢复的方法。整体流程包括：介绍问题背景和重要性；分析传感器技术特别是LiDAR的特点；提出分类法组织现有方法；按监督类型（监督、弱监督、无监督）分析3D人体姿态估计方法；讨论人体网格恢复方法；评估常用数据集；统一评估指标定义；建立基准测试；指出未来挑战和方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对LiDAR点云的3D人体理解进行全面综述；提出结构化分类法；对三个主要数据集进行定量比较；统一评估指标定义；建立基准测试表；维护持续更新的配套网页。相比之前工作，这篇综述专注于LiDAR而非图像/视频；针对户外'野外'场景而非受控环境；包含最新研究成果（至2025年）；提供更全面的方法分类和实际应用基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性的综述、分类和基准测试，显著推进了从LiDAR点云进行3D人体姿态估计和人体网格恢复领域的研究与应用，为该领域提供了清晰的发展路线图。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a comprehensive review of 3D human pose estimationand human mesh recovery from in-the-wild LiDAR point clouds. We compareexisting approaches across several key dimensions, and propose a structuredtaxonomy to classify these methods. Following this taxonomy, we analyze eachmethod's strengths, limitations, and design choices. In addition, (i) weperform a quantitative comparison of the three most widely used datasets,detailing their characteristics; (ii) we compile unified definitions of allevaluation metrics; and (iii) we establish benchmark tables for both tasks onthese datasets to enable fair comparisons and promote progress in the field. Wealso outline open challenges and research directions critical for advancingLiDAR-based 3D human understanding. Moreover, we maintain an accompanyingwebpage that organizes papers according to our taxonomy and continuously updateit with new studies:https://github.com/valeoai/3D-Human-Pose-Shape-Estimation-from-LiDAR</description>
      <author>example@mail.com (Salma Galaaoui, Eduardo Valle, David Picard, Nermin Samet)</author>
      <guid isPermaLink="false">2509.12197v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Segmentation-Driven Initialization for Sparse-view 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2509.11853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SDI-GS的新方法，通过区域分割识别和保留结构上重要的区域，减少3D高斯点数量，同时保持场景保真度，提高稀疏视图合成的效率和实用性。&lt;h4&gt;背景&lt;/h4&gt;稀疏视图合成具有挑战性，因为从有限观测中恢复准确几何和外观困难。现有3D高斯散射方法通常依赖运动结构(SfM)进行相机姿态估计，这在真正稀疏视图设置中表现不佳。无SfM方法虽用多视图立体视觉替代，但通过反向投影所有像素生成大量3D高斯点，导致高内存成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，在稀疏视图条件下高效生成高质量3D场景表示，同时减少内存使用和提高训练速度。&lt;h4&gt;方法&lt;/h4&gt;提出分割驱动的Gaussian Splatting初始化(SDI-GS)，利用基于区域的分割识别和保留结构上重要的区域，实现对密集点云的选择性下采样，保持场景保真度的同时显著减少高斯点数量。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SDI-GS可减少高达50%的高斯点数量，在PSNR和SSIM指标上实现相当或更优的渲染质量，LPIPS指标仅有轻微下降。该方法还实现了更快的训练速度和更低的内存占用。&lt;h4&gt;结论&lt;/h4&gt;SDI-GS推进了3D高斯散射在受限视图场景中的实用性，通过减少内存需求和提高训练效率，使3DGS更适合实际应用。&lt;h4&gt;翻译&lt;/h4&gt;稀疏视图合成仍然是一个具有挑战性的问题，因为从有限的观测中恢复准确的几何和外观很困难。虽然3D高斯散射(3DGS)的最新进展实现了具有竞争力的质量的实时渲染，但现有流程通常依赖于运动结构(SfM)进行相机姿态估计，这种方法在真正稀疏视图设置中表现不佳。此外，一些无SfM的方法用多视图立体视觉(MVS)模型替代SfM，但通过将每个像素反向投影到3D空间来生成大量3D高斯点，导致高内存成本。我们提出了分割驱动的Gaussian Splatting初始化(SDI-GS)，一种通过利用基于区域的分割来识别和保留结构上重要的区域，从而缓解低效率问题的方法。这实现了对密集点云的选择性下采样，在保持场景保真度的同时显著减少高斯点数量。跨不同基准的实验表明，SDI-GS可减少高达50%的高斯点数量，并在PSNR和SSIM上实现相当或更优的渲染质量，同时LPIPS指标仅有轻微下降。它还实现了更快的训练速度和更低的内存占用，推进了3DGS在受限视图场景中的实用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏视角3D高斯溅射（3DGS）中的初始化效率问题。传统方法依赖运动恢复结构（SfM）在稀疏视角下不稳定，而无SfM方法则生成过多高斯分布导致高内存成本。这个问题在机器人、增强现实和医疗成像等实际应用中至关重要，因为在这些场景中获取密集视图不切实际，限制了3DGS在资源受限场景中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有SfM方法在稀疏视角下不可靠，而无SfM方法效率低下。他们借鉴了自己在2D高斯回归方面的先前工作，证明基于区域的分割可有效减少冗余同时保留重要结构。他们将这种分割驱动范式从2D扩展到3D高斯溅射领域，利用跨视图区域一致性指导选择性下采样。具体借鉴了MASt3R进行姿态估计和点云生成，以及使用修改的DBSCAN算法（MDBSCAN）进行高效区域分割。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基于区域的分割识别并保留结构上重要的区域，对密集点云进行选择性下采样，在保持场景保真度的同时减少高斯数量。整体流程包括：1)使用MASt3R估计相机姿态和生成密集点云；2)对每个视图执行基于区域的分割；3)构建感知3D标记向量来识别跨视图一致区域；4)在每个结构聚类内进行分层采样；5)用下采样点初始化3D高斯并联合优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：分割驱动的初始化策略减少高斯数量达50%；高效的表示学习保持高质量渲染；更快的训练速度和更低内存占用；利用跨视图一致性识别有意义区域。相比SfM方法，避免了稀疏视角下的不稳定性；相比其他SfM-free方法，使用智能分割指导下采样而非简单置信度过滤；相比训练期使用分割的方法，在初始化阶段使用轻量级区域分割，不依赖语义标签或逐步密集化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种分割驱动的初始化方法，通过基于区域的分割和智能下采样，显著减少了稀疏视角3D高斯溅射中的高斯数量和内存需求，同时保持了高质量的渲染效果和快速的训练速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparse-view synthesis remains a challenging problem due to the difficulty ofrecovering accurate geometry and appearance from limited observations. Whilerecent advances in 3D Gaussian Splatting (3DGS) have enabled real-timerendering with competitive quality, existing pipelines often rely onStructure-from-Motion (SfM) for camera pose estimation, an approach thatstruggles in genuinely sparse-view settings. Moreover, several SfM-free methodsreplace SfM with multi-view stereo (MVS) models, but generate massive numbersof 3D Gaussians by back-projecting every pixel into 3D space, leading to highmemory costs. We propose Segmentation-Driven Initialization for GaussianSplatting (SDI-GS), a method that mitigates inefficiency by leveragingregion-based segmentation to identify and retain only structurally significantregions. This enables selective downsampling of the dense point cloud,preserving scene fidelity while substantially reducing Gaussian count.Experiments across diverse benchmarks show that SDI-GS reduces Gaussian countby up to 50% and achieves comparable or superior rendering quality in PSNR andSSIM, with only marginal degradation in LPIPS. It further enables fastertraining and lower memory footprint, advancing the practicality of 3DGS forconstrained-view scenarios.</description>
      <author>example@mail.com (Yi-Hsin Li, Thomas Sikora, Sebastian Knorr, Måarten Sjöström)</author>
      <guid isPermaLink="false">2509.11853v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Augmented Reality-Enhanced Robot Teleoperation for Collecting User Demonstrations</title>
      <link>http://arxiv.org/abs/2509.11783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 8th International Conference on Robotics, Control  and Automation Engineering (RCAE 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种增强现实(AR)增强的机器人远程操作系统，通过结合AR控制和空间点云渲染，实现了直观、无接触的机器人演示收集。该系统已在ABB机器人平台上验证，研究表明增强感知显著提高了任务性能和用户体验。&lt;h4&gt;背景&lt;/h4&gt;传统工业机器人编程复杂且耗时，需要专家程序员花费数周甚至数月时间。虽然通过演示编程(PbD)提供了一种更易选择的替代方案，但机器人控制和演示收集的直观界面仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种增强现实(AR)增强的机器人远程操作系统，结合基于AR的控制和空间点云渲染，实现直观、无接触的演示，使操作员能够远程控制机器人而无需进入工作空间或使用传统工具。&lt;h4&gt;方法&lt;/h4&gt;提出并实现了一个结合AR控制和空间点云渲染的系统，使操作员能够远程控制机器人。该系统在ABB机器人平台上进行了演示，特别是在IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。通过用户研究评估了实时环境感知（有点云渲染和无点云渲染）对任务完成的影响。&lt;h4&gt;主要发现&lt;/h4&gt;增强感知显著提高了28%的任务性能，并改善了用户体验，系统可用性量表(SUS)得分提高了12%。实时环境感知对任务完成准确性、效率和用户信心有积极影响。&lt;h4&gt;结论&lt;/h4&gt;这项工作有助于推进工业环境中用于演示收集的直观机器人远程操作、AR界面设计、环境感知和远程操作安全机制。收集的演示可作为机器学习应用的有价值训练数据。&lt;h4&gt;翻译&lt;/h4&gt;传统工业机器人编程通常复杂且耗时，通常需要专家程序员花费数周甚至数月的努力。虽然通过演示编程(PbD)提供了更易选择的替代方案，但机器人控制和演示收集的直观界面仍然具有挑战性。为此，我们提出了一种增强现实(AR)增强的机器人远程操作系统，将基于AR的控制与空间点云渲染相结合，实现直观、无接触的演示。这种方法允许操作员远程控制机器人，无需进入工作空间或使用示教器等传统工具。所提出的系统具有通用性，已在ABB机器人平台上进行了演示，特别是在IRB 1200工业机器人和GoFa 5协作机器人上进行了验证。用户研究评估了实时环境感知的影响，具体是有点云渲染和无点云渲染两种情况，对任务完成准确性、效率和用户信心的影响。结果表明，增强感知显著提高了28%的任务性能，并通过系统可用性量表(SUS)得分提高12%改善了用户体验。这项工作有助于推进工业环境中用于演示收集的直观机器人远程操作、AR界面设计、环境感知和远程操作安全机制。收集的演示可作为机器学习应用的有价值训练数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决工业机器人编程复杂、耗时的问题，以及通过演示编程(PbD)时缺乏直观界面的问题。这个问题很重要，因为传统机器人编程需要专家花费数周甚至数月，而现有方法要么局限于昂贵的协作机器人，要么存在安全风险，限制了工业机器人的广泛应用和人机协作的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统编程和现有PbD方法的局限性，然后探索了AR/VR技术在机器人控制中的应用潜力。他们借鉴了多项现有研究，如Rivera-Pinto的全息球体轨迹定义、Thormann的手势控制、Smith和Van Haastregt的数字末端执行器代理控制、Pizzagalli的数字孪生环境路径规划等，但针对这些方法的不足进行了改进，设计了一个模块化的AR遥操作系统，结合实时点云渲染增强环境感知。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用增强现实技术创建直观界面，使用户能远程控制机器人而不需进入工作空间或使用传统工具。系统由四个模块组成：机器人实时控制模块(EGM接口)、Unity AR系统模块(可视化与交互)、空间点云渲染模块(深度传感器)和机器人监控模块(RWS协议)。工作流程是用户通过HMD在AR界面演示动作，Unity处理数据并传输给物理机器人，同时点云渲染提供环境感知，机器人状态实时反馈给用户。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)适用于工业机器人和协作机器人的通用AR遥操作系统；2)集成AR控制与实时点云渲染增强视觉反馈；3)用户研究评估环境感知影响。相比之前工作，不同之处在于：确保轨迹可行性(不同于Rivera-Pinto的不验证可达性)、支持更广泛运动(不同于Thormann的手势限制)、减轻距离和物体大小限制(不同于Smith和Van Haastregt的直接视觉依赖)、无需预先建模适应动态环境(不同于Pizzagalli的数字孪生)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发并验证了一个基于增强现实的机器人遥操作系统，通过集成实时点云渲染，使用户能够直观、安全地远程控制工业机器人进行任务演示，显著提高了任务效率和用户体验。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional industrial robot programming is often complex and time-consuming,typically requiring weeks or even months of effort from expert programmers.Although Programming by Demonstration (PbD) offers a more accessiblealternative, intuitive interfaces for robot control and demonstrationcollection remain challenging. To address this, we propose an Augmented Reality(AR)-enhanced robot teleoperation system that integrates AR-based control withspatial point cloud rendering, enabling intuitive, contact-free demonstrations.This approach allows operators to control robots remotely without entering theworkspace or using conventional tools like the teach pendant. The proposedsystem is generally applicable and has been demonstrated on ABB robotplatforms, specifically validated with the IRB 1200 industrial robot and theGoFa 5 collaborative robot. A user study evaluates the impact of real-timeenvironmental perception, specifically with and without point cloud rendering,on task completion accuracy, efficiency, and user confidence. Results indicatethat enhanced perception significantly improves task performance by 28% andenhances user experience, as reflected by a 12% increase in the SystemUsability Scale (SUS) score. This work contributes to the advancement ofintuitive robot teleoperation, AR interface design, environmental perception,and teleoperation safety mechanisms in industrial settings for demonstrationcollection. The collected demonstrations may serve as valuable training datafor machine learning applications.</description>
      <author>example@mail.com (Shiqi Gong, Sebastian Zudaire, Chi Zhang, Zhen Li)</author>
      <guid isPermaLink="false">2509.11783v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GBPP: Grasp-Aware Base Placement Prediction for Robots via Two-Stage Learning</title>
      <link>http://arxiv.org/abs/2509.11594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Jizhuo Chen and Diwen Liu contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GBPP是一种基于快速学习的评分器，用于从单个RGB-D快照中选择机器人抓取的基础姿态。该方法采用两阶段课程学习，结合简单启发式方法和高保真模拟优化，实现快速、安全的基础姿态选择。&lt;h4&gt;背景&lt;/h4&gt;机器人抓取任务中，从单个RGB-D图像选择合适的基础姿态具有挑战性。传统方法可能需要复杂的任务和运动优化，或仅依赖简单几何信息，效果有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的方法，从单个RGB-D图像选择机器人抓取基础姿态，使其能够安全、可靠到达目标位置，并在出错时优雅降级。&lt;h4&gt;方法&lt;/h4&gt;1. 两阶段课程学习：第一阶段使用距离-可见性规则低成本自动标记大型数据集；第二阶段使用高保真模拟试验优化模型。2. 采用PointNet++风格点云编码器与多层感知机对候选姿态密集网格评分。3. 实现快速在线选择，无需完整任务和运动优化。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在模拟和真实移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法。2. GBPP选择的基础姿态更安全、更易到达。3. 出错时能优雅降级而非完全失败。4. 实现了数据高效、几何感知的基础定位。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了一种实用的基础定位方法：使用廉价启发式方法获得广泛覆盖，然后通过有针对性的模拟进行校准，结合了简单高效和精确优化的优点。&lt;h4&gt;翻译&lt;/h4&gt;GBPP是一种基于快速学习的评分器，用于从单个RGB-D快照中选择机器人抓取的基础姿态。该方法采用两阶段课程：(1)使用简单的距离-可见性规则低成本自动标记大型数据集；(2)使用更小规模的高保真模拟试验来优化模型以匹配真实的抓取结果。PointNet++风格的点云编码器与多层感知机一起对候选姿态的密集网格进行评分，实现了快速在线选择，无需完整的任务和运动优化。在模拟和真实的移动机械臂上，GBPP优于仅依赖接近度和几何的基线方法，选择更安全、更易到达的姿态，并且在出错时能优雅降级。研究结果为数据高效、几何感知的基础定位提供了实用的方法：使用廉价启发式方法覆盖，然后通过有针对性的模拟进行校准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决移动机器人在杂乱环境中如何适当放置基座以成功抓取目标物体的问题。这个问题在现实中很重要，因为当前模块化系统往往忽略基座放置与机械臂可达性之间的协调，导致抓取失败；而几何方法计算成本高，任务和运动规划难以实时部署，限制了机器人在家庭、仓库等实际环境中的抓取效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：模块化系统忽略基座放置挑战，几何方法计算成本高，TAMP难以实时部署。然后他们将问题重新定义为候选姿态的二分类问题，考虑到直接使用大规模仿真训练成本极高，设计了结合廉价启发式数据和高保真仿真的两阶段方法。他们借鉴了PointNet++作为点云编码器，使用ProcTHOR和ManiSkill等现有仿真环境，但通过创新的两阶段课程学习解决了数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合廉价启发式数据的大规模覆盖和高保真仿真数据的高精度，通过两阶段学习实现数据高效的几何感知基座放置。整体流程包括：1)任务定义，输入RGB-D观测和机器人参数，输出最优基座位置；2)第一阶段使用距离-可见性启发式自动标记180k训练样本；3)第二阶段使用约12k高保真仿真样本微调模型；4)推理时评估所有候选位置，选择最佳位置执行；5)在仿真和真实环境中评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段课程学习框架，结合启发式和仿真数据优势；2)启发式自标记方法，大幅降低数据收集成本；3)数据高效的学习方法，在更少高质量数据下实现更好性能；4)实时性能，0.3秒内评估600个候选姿态。相比之前工作，不同于模块化系统的分离设计，直接从RGB-D预测基座位置；不同于几何方法的高计算成本，学习处理复杂环境；不同于TAMP的实时部署困难，支持快速在线规划；不同于纯学习的高数据需求，通过两阶段解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种两阶段课程学习方法，结合廉价启发式数据的大规模覆盖和高保真仿真数据的高精度，实现了数据高效的几何感知基座放置，显著提高了机器人在复杂环境中的抓取成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GBPP is a fast learning based scorer that selects a robot base pose forgrasping from a single RGB-D snapshot. The method uses a two stage curriculum:(1) a simple distance-visibility rule auto-labels a large dataset at low cost;and (2) a smaller set of high fidelity simulation trials refines the model tomatch true grasp outcomes. A PointNet++ style point cloud encoder with an MLPscores dense grids of candidate poses, enabling rapid online selection withoutfull task-and-motion optimization. In simulation and on a real mobilemanipulator, GBPP outperforms proximity and geometry only baselines, choosingsafer and more reachable stances and degrading gracefully when wrong. Theresults offer a practical recipe for data efficient, geometry aware baseplacement: use inexpensive heuristics for coverage, then calibrate withtargeted simulation.</description>
      <author>example@mail.com (Jizhuo Chen, Diwen Liu, Jiaming Wang, Harold Soh)</author>
      <guid isPermaLink="false">2509.11594v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Frame-wise Tracking: A Trajectory-based Paradigm for Efficient Point Cloud Tracking</title>
      <link>http://arxiv.org/abs/2509.11453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TrajTrack的新型基于轨迹的LiDAR 3D单目标跟踪框架，通过从历史边界框轨迹中隐式学习运动连续性，在保持高效率的同时显著提高了跟踪精度。&lt;h4&gt;背景&lt;/h4&gt;LiDAR-based 3D单目标跟踪是机器人和自主系统中的关键任务。现有方法中，两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中容易失效；而基于序列的方法处理多个点云，计算成本高但鲁棒性强。&lt;h4&gt;目的&lt;/h4&gt;解决两帧方法和序列方法之间的权衡问题，提出一种新的基于轨迹的范式，在保持高效率的同时增强跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;TrajTrack是一个轻量级框架，通过从历史边界框轨迹中隐式学习运动连续性来增强基础两帧跟踪器，无需额外点云输入。它首先生成快速显式的运动建议，然后使用隐式运动建模模块预测未来轨迹，完善和纠正初始建议。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes基准测试上，TrajTrack实现了新的最先进性能，相比强大基线提高跟踪精度4.48%，同时保持56 FPS的运行速度，并展示了在不同基础跟踪器上的强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;TrajTrack成功解决了两帧方法和序列方法之间的权衡问题，在保持高效率的同时显著提高了跟踪精度，为LiDAR 3D单目标跟踪提供了新范式。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的3D单目标跟踪是机器人和自主系统中的关键任务。现有方法通常遵循逐帧运动估计或基于序列的范式。然而，两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中容易失效，而处理多个点云的序列方法则以显著的计算成本获得鲁棒性。为解决这一困境，我们提出了一种新的基于轨迹的范式及其实现TrajTrack。TrajTrack是一个轻量级框架，通过仅从历史边界框轨迹中隐式学习运动连续性来增强基础两帧跟踪器，不需要额外昂贵的点云输入。它首先生成快速、显式的运动建议，然后使用隐式运动建模模块预测未来轨迹，进而完善和纠正初始建议。在大型NuScenes基准上的大量实验表明，TrajTrack实现了新的最先进性能，相比强大基线显著提高4.48%的跟踪精度，同时以56 FPS的速度运行。此外，我们还展示了TrajTrack在不同基础跟踪器上的强大泛化能力。视频可在https://www.bilibili.com/video/BV1ahYgzmEWP观看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是点云跟踪中效率与鲁棒性之间的权衡问题。现有的两帧方法效率高但缺乏长期时间上下文，在稀疏或遮挡场景中表现不佳；而序列方法虽能增强鲁棒性但计算成本高，不适合实时应用。这个问题在自动驾驶和机器人系统中至关重要，因为LiDAR点云跟踪是这些系统的核心任务，需要在保持实时性的同时应对各种复杂环境挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后从轨迹预测领域获得灵感，意识到可以从物体历史轨迹中学习运动连续性而不需要处理高带宽点云数据。他们设计了一个结合短期显式运动和长期隐式连续性的框架。方法借鉴了现有工作：显式运动部分参考了P2P等两帧跟踪方法；隐式轨迹预测部分借鉴了VectorNet、AgentFormer等使用Transformer建模序列数据的方法；整体框架受到SeqTrack3D等序列方法的启发，但通过只使用历史边界框而非完整点云序列来降低计算成本。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过利用历史边界框轨迹学习物体长期运动连续性，在不增加大量计算成本的情况下提高跟踪鲁棒性。整体实现采用两阶段'提出-预测-细化'流程：1)显式运动提案：使用两帧模型处理点云，生成初始跟踪提案；2)隐式轨迹预测：创新性地使用仅历史边界框序列的隐式运动建模(IMM)模块，用TrajFormer架构预测未来轨迹；3)轨迹引导的提案细化：根据两个提案间的IoU动态选择，智能校正初始提案，输出最终跟踪结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于轨迹的范式，利用历史边界框融入长期运动连续性；2)隐式运动建模(IMM)模块，仅用轻量级历史边界框序列学习长期运动模式；3)两阶段处理流程，结合短期显式和长期隐式运动线索。相比之前工作：与两帧方法不同，它考虑长期连续性，在稀疏场景表现更好；与序列方法不同，它不处理多帧点云，计算成本低且能实时运行(56 FPS)；与其他轨迹预测方法不同，它专为3D跟踪设计，并提出了轨迹引导的提案细化机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TrajTrack通过引入基于轨迹的范式和隐式运动建模模块，有效结合了短期显式运动和长期隐式连续性的优势，在保持实时性能的同时显著提升了点云跟踪的鲁棒性，实现了前所未有的性能和效率平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D single object tracking (3D SOT) is a critical task in roboticsand autonomous systems. Existing methods typically follow frame-wise motionestimation or a sequence-based paradigm. However, the two-frame methods areefficient but lack long-term temporal context, making them vulnerable in sparseor occluded scenes, while sequence-based methods that process multiple pointclouds gain robustness at a significant computational cost. To resolve thisdilemma, we propose a novel trajectory-based paradigm and its instantiation,TrajTrack. TrajTrack is a lightweight framework that enhances a base two-frametracker by implicitly learning motion continuity from historical bounding boxtrajectories alone-without requiring additional, costly point cloud inputs. Itfirst generates a fast, explicit motion proposal and then uses an implicitmotion modeling module to predict the future trajectory, which in turn refinesand corrects the initial proposal. Extensive experiments on the large-scaleNuScenes benchmark show that TrajTrack achieves new state-of-the-artperformance, dramatically improving tracking precision by 4.48% over a strongbaseline while running at 56 FPS. Besides, we also demonstrate the stronggeneralizability of TrajTrack across different base trackers. Video isavailable at https://www.bilibili.com/video/BV1ahYgzmEWP.</description>
      <author>example@mail.com (BaiChen Fan, Sifan Zhou, Jian Li, Shibo Zhao, Muqing Cao, Qin Wang)</author>
      <guid isPermaLink="false">2509.11453v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3D Gaussian Modeling and Ray Marching of OpenVDB datasets for Scientific Visualization</title>
      <link>http://arxiv.org/abs/2509.11377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了3D高斯模型在科学可视化领域的应用，提出使用OpenVDB格式作为3D高斯粒子的建模框架，实现高效体积数据压缩，并开发基于光线积分的渲染算法进行光学深度累积计算。&lt;h4&gt;背景&lt;/h4&gt;3D高斯模型正被广泛研究用于场景建模和压缩。在科学可视化领域，大多数流行格式是密集网格数据结构，存储每个网格单元而忽略其贡献。OpenVDB库和数据格式专为稀疏体积数据设计，通过屏蔽空单元格避免存储它们，最初用于视觉效果如云、火和流体等。&lt;h4&gt;目的&lt;/h4&gt;探索在科学可视化中使用OpenVDB作为建模框架，转换为3D高斯粒子以实现进一步压缩，为不同科学体积类型提供统一建模方法，利用OpenVDB的压缩优势作为3D高斯模型的起点。&lt;h4&gt;方法&lt;/h4&gt;在OptiX 8.1中实现基于光线积分的渲染算法，计算3D高斯沿光线的贡献用于光学深度累积；实现SciVis风格的主光线仅NanoVDB HDDA基于光线步进器用于OpenVDB体素网格，以比较渲染结果；探索将此高斯模型应用于非规则网格体积格式如AMR体积和点云，使用OpenVDB网格类类型的内部表示用于数据层次结构。&lt;h4&gt;主要发现&lt;/h4&gt;使用OpenVDB作为3D高斯模型的起点提供了非平凡的压缩优势；成功实现了基于光线积分的渲染算法，能够计算3D高斯沿光线的贡献；开发了SciVis风格的主光线仅NanoVDB HDDA基于光线步进器用于性能比较。&lt;h4&gt;结论&lt;/h4&gt;OpenVDB在科学可视化场景中的应用具有可行性；3D高斯模型为体积数据压缩和渲染提供了新途径；高斯模型可扩展应用于非规则网格体积格式。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯模型目前因其场景建模和压缩能力而被广泛研究。在3D体积中，其使用正被探索用于以尽可能稀疏的方式表示密集体积。然而，这些方法大多开始于内存效率低下的数据格式。特别是在科学可视化(SciVis)领域，大多数流行格式是密集网格数据结构，存储每个网格单元，无论其贡献如何。OpenVDB库和数据格式被引入用于专门表示稀疏体积数据，用于视觉效果用例如云、火、流体等。它通过在存储过程中屏蔽空单元格来避免存储它们。这为在SciVis中使用提供了机会，特别是作为转换为3D高斯粒子的建模框架以实现进一步压缩，以及为不同科学体积类型提供统一建模方法。这种压缩起点是非平凡的，本文希望提出一种基于光线积分的渲染算法，在OptiX 8.1中实现，用于计算沿光线的3D高斯贡献以进行光学深度累积。为了比较我们光线步进高斯渲染器的渲染结果，我们还实现了SciVis风格的主光线仅NanoVDB HDDA基于光线步进器，用于OpenVDB体素网格。最后，本文还探索了将此高斯模型应用于非规则网格的体积格式，如AMR体积和点云，使用OpenVDB网格类类型的内部表示用于数据层次结构和细分结构。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何将OpenVDB科学可视化数据集高效地转换为3D高斯模型并进行渲染的问题。这个问题很重要，因为科学可视化中常用的密集网格数据结构存储效率低下，而OpenVDB虽然能高效存储稀疏体积数据，但传统渲染方法仍有局限。3D高斯模型提供了一种潜在的压缩和统一表示方法，能够支持不同类型的科学体积数据，提高渲染效率同时保持视觉质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了科学可视化中体积数据处理的挑战，特别是稀疏数据的存储和渲染效率问题。他们注意到OpenVDB在处理稀疏体积数据方面的优势，以及3D高斯模型在场景表示和压缩方面的潜力。作者设计方法时借鉴了多项现有工作，包括OpenVDB库本身、3D高斯建模研究(如3DGS)、光线行进技术、线积分方法以及OptiX 8.1框架。作者的创新在于将这些技术结合起来，专门针对科学可视化的需求，提出了从OpenVDB到3D高斯的转换和渲染方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将OpenVDB中的稀疏体积数据转换为3D高斯表示，每个高斯代表一个体素或一组体素，然后使用光线行进技术穿过这些高斯，通过线积分计算光学深度，最后应用传递函数将累积的光学深度转换为最终的颜色和透明度。整体流程包括：1)加载OpenVDB数据集；2)遍历OpenVDB叶节点，根据密集或稀疏特性使用不同策略生成3D高斯；3)为每个高斯创建AABB并构建BVH加速结构；4)使用OptiX框架进行光线追踪，计算光学深度贡献；5)累积光学深度并应用传递函数获得最终颜色。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出将OpenVDB科学可视化数据格式转换为3D高斯模型；2)传递函数无关的建模方法，允许运行时动态调整传递函数；3)提供多层次细节控制，平衡渲染质量和性能；4)支持多种科学数据格式(规则网格、AMR体积、点云)的统一高斯表示；5)使用线积分方法计算3D高斯沿光线的贡献。相比之前工作，本文不依赖训练循环或图像监督，直接从原始体积数据生成高斯；使用3D高斯而非体素作为基本渲染单元；提供完整的体积覆盖而非仅表示表面；为多种科学数据格式提供统一的渲染框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种将OpenVDB科学可视化数据转换为3D高斯模型的方法，通过光线行进和线积分实现高效渲染，支持多种数据格式和动态传递函数调整，为科学可视化提供了统一的体积数据表示和渲染框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussians are currently being heavily investigated for their scenemodeling and compression abilities. In 3D volumes, their use is being exploredfor representing dense volumes as sparsely as possible. However, most of thesemethods begin with a memory inefficient data format. Specially in ScientificVisualization(SciVis), where most popular formats are dense-grid datastructures that store every grid cell, irrespective of its contribution.OpenVDB library and data format were introduced for representing sparsevolumetric data specifically for visual effects use cases such as clouds, fire,fluids etc. It avoids storing empty cells by masking them during storage. Itpresents an opportunity for use in SciVis, specifically as a modeling frameworkfor conversion to 3D Gaussian particles for further compression and for aunified modeling approach for different scientific volume types. Thiscompression head-start is non-trivial and this paper would like to present thiswith a rendering algorithm based on line integration implemented in OptiX8.1for calculating 3D Gaussians contribution along a ray for optical-depthaccumulation. For comparing the rendering results of our ray marching Gaussiansrenderer, we also implement a SciVis style primary-ray only NanoVDB HDDA basedray marcher for OpenVDB voxel grids. Finally, this paper also exploresapplication of this Gaussian model to formats of volumes other than regulargrids, such as AMR volumes and point clouds, using internal representation ofOpenVDB grid class types for data hierarchy and subdivision structure.</description>
      <author>example@mail.com (Isha Sharma, Dieter Schmalstieg)</author>
      <guid isPermaLink="false">2509.11377v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Limit Theorems for Verbose Persistence Diagrams</title>
      <link>http://arxiv.org/abs/2509.11256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作启动了对随机冗长图的研究，建立了随机冗长图的大数定律，证明了极限冗长图的存在性，刻画了其支撑并计算总质量，同时扩展了持久贝蒂数的概念并揭示了其与冗长图的关系。&lt;h4&gt;背景&lt;/h4&gt;持久图是持久同调研究的中心对象，也在随机拓扑背景下被研究。冗长图是对持久图的精化，通过沿对角线添加额外点来包含瞬时持久特征。&lt;h4&gt;目的&lt;/h4&gt;研究随机冗长图，建立其大数定律，证明极限冗长图的存在性，刻画其支撑并计算总质量，扩展持久贝蒂数概念并研究其与冗长图的关系。&lt;h4&gt;方法&lt;/h4&gt;通过分析随机点云的增长，研究冗长图的极限行为，将持久贝蒂数概念扩展，并研究其渐进行为。&lt;h4&gt;主要发现&lt;/h4&gt;随着随机点云规模扩大，存在极限冗长图，可视为对上半平面(对角线及其上方)上的一个测度；刻画了极限冗长图的支撑并计算了其总质量；揭示了扩展持久贝蒂数概念与冗长图之间的关系；建立了关于扩展持久贝蒂数渐进行为的结果。&lt;h4&gt;结论&lt;/h4&gt;这项工作成功地将Hiraoka、Shirai和Trinh及其后续Shirai和Suzaki的主要结果扩展到了冗长图的设定中，为随机冗长图研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;持久图是持久同调研究中的中心对象，也在随机拓扑的背景下被研究。新近提出的冗长图(也称为冗长条形码)是对持久图的精化，通过沿对角线添加额外点来包含瞬时持久特征。在这项工作中，我们启动了对随机冗长图的研究。我们建立了随机冗长图的大数定律，证明随着随机点云规模的扩大，存在一个极限冗长图，可以看作是对上半平面(对角线及其上方)上的一个测度。同时，我们刻画了其支撑并计算了其总质量。在此过程中，我们扩展了持久贝蒂数的概念，揭示了这种扩展概念与冗长图之间的关系，并建立了关于扩展持久贝蒂数渐进行为的结果。这项工作将Hiraoka、Shirai和Trinh及其后续Shirai和Suzaki的主要结果扩展到了冗长图的设定中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The persistence diagram is a central object in the study of persistenthomology and has also been investigated in the context of random topology. Themore recent notion of the verbose diagram (a.k.a. verbose barcode) is arefinement of the persistence diagram that is obtained by incorporatingephemeral persistence features as extra points along the diagonal. In thiswork, we initiate the study of random verbose diagrams. We establish a stronglaw of large numbers for verbose diagrams as a random point cloud grows in size-- that is, we prove the existence of a limiting verbose diagram, viewed as ameasure on the half-plane on and above the diagonal. Also, we characterize itssupport and compute its total mass. Along the way, we extend the notion of thepersistent Betti number, reveal the relation between this extended notion andthe verbose diagram (which is an extension of the fundamental lemma ofpersistent homology), and establish results on the asymptotic behavior of theextended persistent Betti numbers.  This work extends the main results of the work by Hiraoka, Shirai, and Trinhand its sequel by Shirai and Suzaki to the setting of verbose diagrams.</description>
      <author>example@mail.com (Jeong-hwi Joe, Woojin Kim, Cheolwoo Park)</author>
      <guid isPermaLink="false">2509.11256v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>No Mesh, No Problem: Estimating Coral Volume and Surface from Sparse Multi-View Images</title>
      <link>http://arxiv.org/abs/2509.11164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的轻量级可扩展学习框架，通过分析珊瑚的2D多视图RGB图像来预测其3D体积和表面积，用于珊瑚礁监测。&lt;h4&gt;背景&lt;/h4&gt;珊瑚礁监测需要量化珊瑚生长，这需要准确估算珊瑚的体积和表面积。然而，由于珊瑚形态复杂，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从二维多视图RGB图像预测珊瑚状物体三维体积和表面积的轻量级可扩展学习框架。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模块VGGT从每个视图中提取密集点图，合并为统一点云并添加置信度分数，通过两个并行的DGCNN解码器头部输出体积和表面积及其置信度估计，并引入基于高斯负对数似然的复合损失函数以增强预测稳定性和提供不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;该方法实现了具有竞争力的准确性，并对未见过的珊瑚形态具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架为直接从稀疏图像集合中高效可扩展的珊瑚几何估算铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;有效的珊瑚礁监测需要通过精确的体积和表面积估算来量化珊瑚生长，但由于珊瑚形态复杂，这是一项具有挑战性的任务。我们提出了一种新颖的轻量级可扩展学习框架，通过从二维多视图RGB图像预测珊瑚状物体的三维体积和表面积来解决这一挑战。我们的方法使用预训练模块从每个视图中提取密集点图；这些点图被合并为统一的点云，并添加了每个视图的置信度分数。将结果点云输入到两个并行的DGCNN解码器头部，它们共同输出珊瑚的体积和表面积，以及它们相应的置信度估计。为了增强预测稳定性并提供不确定性估计，我们在实数和对数域中引入了基于高斯负对数似然的复合损失函数。我们的方法实现了具有竞争力的准确性，并对未见过的形态具有良好的泛化能力。该框架为直接从稀疏图像集合中高效可扩展的珊瑚几何估算铺平了道路，在珊瑚生长分析和珊瑚礁监测方面具有潜在应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏的多视角2D RGB图像中准确估算珊瑚体积和表面积的问题。这个问题很重要，因为珊瑚礁是海洋生态系统健康的重要指标，跟踪珊瑚体积和表面积对理解生长趋势、检测退化及指导保护工作至关重要。传统方法需要昂贵设备、大量人工处理且不可扩展，而现有方法在稀疏视角或遮挡情况下会产生不准确结果，严重影响监测可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了珊瑚形态复杂性和传统3D重建方法的局限性，然后借鉴了多视角技术在无需完整3D监督下估计几何形状的成功经验。他们利用VGGT视觉几何基础Transformer框架提取密集点图，并采用DGCNN动态图卷积网络作为解码器。方法设计考虑了珊瑚形态的极端变异性，通过混合损失函数处理不确定性，并针对稀疏视角情况进行了优化，体现了对现有计算机视觉和深度学习技术的创造性应用。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从稀疏多视角2D图像预测珊瑚体积和表面积，无需传统3D网格重建。整体流程分为三部分：1) 珊瑚数据集生成：合成watertight珊瑚网格并渲染多视角图像；2) 特征提取：使用VGGT从掩码图像提取密集点图并转换为带置信度的3D点云；3) 解码器处理：两个并行的DGCNN解码器分别预测体积和表面积及其置信度。训练时采用混合损失函数，结合高斯负对数似然和确定性误差指标，提高预测稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 轻量级可扩展的自动化管道，无需3D网格监督；2) 利用VGGT提取特征和DGCNN解码器预测几何属性；3) 设计混合损失函数处理不同珊瑚尺度的不确定性；4) 解决传统网格重建的伪影问题。相比Trellis方法，论文在体积估计上MAPE从60.81%降至10.27%，表面积从55.41%降至7.56%。与现有珊瑚监测方法不同，论文直接针对体积和表面积估计，而非仅关注覆盖率或地形指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种轻量级、可扩展的深度学习框架，能够从稀疏的多视角2D RGB图像中直接、准确地估计珊瑚的体积和表面积，克服了传统3D重建方法的局限性，为大尺度珊瑚监测和保护提供了高效工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective reef monitoring requires the quantification of coral growth viaaccurate volumetric and surface area estimates, which is a challenging task dueto the complex morphology of corals. We propose a novel, lightweight, andscalable learning framework that addresses this challenge by predicting the 3Dvolume and surface area of coral-like objects from 2D multi-view RGB images.Our approach utilizes a pre-trained module (VGGT) to extract dense point mapsfrom each view; these maps are merged into a unified point cloud and enrichedwith per-view confidence scores. The resulting cloud is fed to two parallelDGCNN decoder heads, which jointly output the volume and the surface area ofthe coral, as well as their corresponding confidence estimate. To enhanceprediction stability and provide uncertainty estimates, we introduce acomposite loss function based on Gaussian negative log-likelihood in both realand log domains. Our method achieves competitive accuracy and generalizes wellto unseen morphologies. This framework paves the way for efficient and scalablecoral geometry estimation directly from a sparse set of images, with potentialapplications in coral growth analysis and reef monitoring.</description>
      <author>example@mail.com (Diego Eustachio Farchione, Ramzi Idoughi, Peter Wonka)</author>
      <guid isPermaLink="false">2509.11164v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Maximum diversity, weighting and invariants of time series</title>
      <link>http://arxiv.org/abs/2509.11146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了magnitude（量级）的连续性及其在数据分析中的应用，特别是提出了一种用于周期时间序列分析的新不变量，并通过机器学习实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;Magnitude作为富范畴欧拉特征的特殊情况，代表了度量空间的大小感，与基数、维度和体积等经典概念相关。虽然已有研究从多角度解释了magnitude的意义，但连续性为理解magnitude提供了新视角。&lt;h4&gt;目的&lt;/h4&gt;基于magnitude和最大多样性的连续性已有结果，本文重点关注加权的连续性及其与最大多样性的关系，并将magnitude理论应用于时间序列分析。&lt;h4&gt;方法&lt;/h4&gt;应用magnitude理论到表示数据或模型参数的点云上，利用连续性结果推导周期时间序列的新不变量，并进行机器学习实验验证其性能。&lt;h4&gt;主要发现&lt;/h4&gt;加权具有连续性，其变化与最大多样性相关；magnitude理论与数据分析有密切联系；提出的周期时间序列不变量直接基于连续性结果，并在实验中改善了性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入基于magnitude连续性的新不变量，有效提升了时间序列分析的性能，展示了magnitude理论在数据科学中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;Magnitude作为富范畴欧拉特征的特例，代表了度量空间的大小感，并与基数、维度和体积等经典概念相关。虽然研究已从多角度解释了magnitude的意义，但连续性也为magnitude提供了有价值的视角。基于关于magnitude和最大多样性的连续性已有结果，本文重点关注加权的连续性，其总和为magnitude，及其与最大多样性的变化。同时，近期研究通过将magnitude理论应用于表示数据或模型参数的点云，阐明了magnitude与数据分析之间的联系。本文还通过引入周期时间序列的新不变量为时间序列分析提供应用，其中不变性直接来自于连续性结果。作为案例，使用真实数据进行了简单的机器学习实验，结果表明建议的不变量提高了性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Magnitude, obtained as a special case of Euler characteristic of enrichedcategory, represents a sense of the size of metric spaces and is related toclassical notions such as cardinality, dimension, and volume. While the studieshave explained the meaning of magnitude from various perspectives, continuityalso gives a valuable view of magnitude. Based on established results aboutcontinuity of magnitude and maximum diversity, this article focuses oncontinuity of weighting, a distribution whose totality is magnitude, and itsvariation corresponding to maximum diversity. Meanwhile, recent studies alsoilluminated the connection between magnitude and data analysis by applyingmagnitude theory to point clouds representing the data or the set of modelparameters. This article will also provide an application for time seriesanalysis by introducing a new kind of invariants of periodic time series, wherethe invariance follows directly from the continuity results. As a use-case, asimple machine learning experiment is conducted with real-world data, in whichthe suggested invariants improved the performance.</description>
      <author>example@mail.com (Byungchang So)</author>
      <guid isPermaLink="false">2509.11146v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>ManiVID-3D: Generalizable View-Invariant Reinforcement Learning for Robotic Manipulation via Disentangled 3D Representations</title>
      <link>http://arxiv.org/abs/2509.11125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为ManiVID-3D的新型3D强化学习架构，专门用于机器人操作，通过自监督解耦特征学习实现视角不变的表示。该框架包含ViewNet模块和高效的GPU加速批量渲染模块，能够在不依赖精确相机校准的情况下处理任意视角的点云观测，并实现了前所未有的训练速度。实验表明，该方法在视角变化下的表现优于现有技术，且参数效率更高。&lt;h4&gt;背景&lt;/h4&gt;在真实世界的机器人操作中部署视觉强化学习策略常常受到摄像头视角变化的阻碍。从固定前置摄像头训练的策略在摄像头位置改变时可能会失效，这是真实环境中难以避免的情况，因为传感器位置的适当管理很困难。现有方法通常依赖精确的相机校准或在处理大的透视变化时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决视觉强化学习在真实世界机器人操作中因摄像头视角变化导致的问题，开发一种不依赖精确相机校准且能有效处理大视角变化的3D强化学习架构。&lt;h4&gt;方法&lt;/h4&gt;提出ManiVID-3D，一种用于机器人操作的新型3D强化学习架构，通过自监督解耦特征学习来学习视角不变的表示。框架包含ViewNet模块，这是一个轻量级但有效的模块，能够自动将任意视角的点云观测对齐到统一的空间坐标系，无需外部校准。此外，还开发了一种高效的GPU加速批量渲染模块，每秒可处理超过5000帧，支持前所未有的3D视觉强化学习大规模训练。&lt;h4&gt;主要发现&lt;/h4&gt;在10个模拟任务和5个现实世界任务上的广泛评估表明，该方法在视角变化下的成功率比最先进的方法高44.7%，同时参数减少了80%。系统对严重透视变化的鲁棒性和强大的模拟到现实性能证明了学习几何一致表示对非结构化环境中可扩展机器人操作的有效性。&lt;h4&gt;结论&lt;/h4&gt;ManiVID-3D通过自监督解耦特征学习和ViewNet模块，有效解决了视觉强化学习在机器人操作中因视角变化导致的问题，无需精确的相机校准即可实现高性能。该方法的参数效率高、训练速度快，且在模拟到现实迁移中表现出色，为非结构化环境中的可扩展机器人操作提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在真实世界的机器人操作中部署视觉强化学习策略常常受到摄像头视角变化的阻碍。从固定前置摄像头训练的策略在摄像头位置改变时可能会失效——这是真实环境中难以避免的情况，因为传感器位置的适当管理很困难。现有方法通常依赖精确的相机校准或在处理大的透视变化时表现不佳。为了解决这些局限性，我们提出了ManiVID-3D，一种用于机器人操作的新型3D强化学习架构，通过自监督解耦特征学习来学习视角不变的表示。该框架包含ViewNet模块，这是一个轻量级但有效的模块，能够自动将任意视角的点云观测对齐到统一的空间坐标系，无需外部校准。此外，我们还开发了一种高效的GPU加速批量渲染模块，每秒可处理超过5000帧，支持前所未有的3D视觉强化学习大规模训练。在10个模拟任务和5个现实世界任务上的广泛评估表明，该方法在视角变化下的成功率比最先进的方法高44.7%，同时参数减少了80%。系统对严重透视变化的鲁棒性和强大的模拟到现实性能证明了学习几何一致表示对非结构化环境中可扩展机器人操作的有效性。我们的项目网站可以在https://zheng-joe-lee.github.io/manivid3d/找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉强化学习策略在摄像头视角变化时泛化能力不足的问题。当机器人从一个固定视角训练后，如果摄像头位置发生变化（真实世界中不可避免），训练好的策略可能会失效。这个问题在现实中非常重要，因为真实环境中传感器位置很难保持固定，特别是杂乱的家庭或动态工业环境中；视角变化会导致视觉观察中剧烈的几何畸变，使策略泛化变得特别困难；现有方法要么依赖精确相机校准，要么在大视角变化下表现不佳，限制了视觉强化学习在真实世界中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D视觉输入方法难以捕捉3D结构先验知识；深度感知方法依赖易受遮挡的深度图；3D点云方法需要精确相机校准。基于这些分析，作者设计了结合监督视图对齐和自监督特征解耦的方法。他们借鉴了DP3的轻量级MLP架构，对比学习表示解耦的思想，以及PointNet++用于点云变换。作者的创新在于将这些技术有机结合，特别是消除了对相机校准的依赖，实现了视角不变的强化学习策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦的3D表示学习视角不变的视觉强化学习策略，同时学习视角不变特征（包含任务关键信息）和视角相关特征（不影响任务执行）。整体流程包括：1)训练阶段：使用ViewNet对齐不同视角点云；处理点云数据；通过双头编码器提取视角不变和视角相关特征；使用对比学习目标实现特征解耦；2)部署阶段：处理真实世界点云；使用预训练ViewNet和策略网络执行动作；3)高效批量模拟与渲染：GPU加速系统实现大规模训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ManiVID-3D架构：使用解耦的视角不变表示实现大视角变化下的鲁棒操作；2)ViewNet模块：无需校准的多视图对齐，统一不同视角点云坐标；3)高效批量渲染系统：GPU加速实现大规模3D视觉RL训练；4)强大的零样本模拟到现实迁移能力。相比之前工作，不同之处在于：不依赖相机校准；使用真正的3D点云表示而非2D图像；参数减少80%同时提高性能；端到端训练而非两阶段预训练；在±75°极端视角变化下仍保持稳定性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ManiVID-3D通过解耦的3D表示和无需校准的多视图对齐，实现了在极端视角变化下具有强大泛化能力的机器人操作策略，同时显著提高了训练效率和模拟到现实的迁移能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying visual reinforcement learning (RL) policies in real-worldmanipulation is often hindered by camera viewpoint changes. A policy trainedfrom a fixed front-facing camera may fail when the camera is shifted--anunavoidable situation in real-world settings where sensor placement is hard tomanage appropriately. Existing methods often rely on precise camera calibrationor struggle with large perspective changes. To address these limitations, wepropose ManiVID-3D, a novel 3D RL architecture designed for roboticmanipulation, which learns view-invariant representations throughself-supervised disentangled feature learning. The framework incorporatesViewNet, a lightweight yet effective module that automatically aligns pointcloud observations from arbitrary viewpoints into a unified spatial coordinatesystem without the need for extrinsic calibration. Additionally, we develop anefficient GPU-accelerated batch rendering module capable of processing over5000 frames per second, enabling large-scale training for 3D visual RL atunprecedented speeds. Extensive evaluation across 10 simulated and 5 real-worldtasks demonstrates that our approach achieves a 44.7% higher success rate thanstate-of-the-art methods under viewpoint variations while using 80% fewerparameters. The system's robustness to severe perspective changes and strongsim-to-real performance highlight the effectiveness of learning geometricallyconsistent representations for scalable robotic manipulation in unstructuredenvironments. Our project website can be found inhttps://zheng-joe-lee.github.io/manivid3d/.</description>
      <author>example@mail.com (Zheng Li, Pei Qu, Yufei Jia, Shihui Zhou, Haizhou Ge, Jiahang Cao, Jinni Zhou, Guyue Zhou, Jun Ma)</author>
      <guid isPermaLink="false">2509.11125v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3DAeroRelief: The first 3D Benchmark UAV Dataset for Post-Disaster Assessment</title>
      <link>http://arxiv.org/abs/2509.11097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3DAeroRelief，首个专门用于灾后评估的三维基准数据集，使用低成本无人机收集飓风破坏区域的密集三维点云，并通过语义标注提供细粒度结构损伤信息。&lt;h4&gt;背景&lt;/h4&gt;及时评估结构损伤对灾害响应和恢复至关重要，但现有自然灾害分析主要依赖二维图像，存在缺乏深度、遮挡和空间上下文有限等问题；虽然三维语义分割提供了更丰富的替代方案，但现有三维基准数据集主要关注城市或室内场景，很少关注灾害影响区域。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集在灾害场景中的不足，创建首个专门针对灾后评估的三维基准数据集，为灾害响应中的三维场景理解提供资源。&lt;h4&gt;方法&lt;/h4&gt;使用低成本无人机在飓风破坏区域收集数据，通过运动结构(Motion Structure)和多视图立体技术(Multi-View Stereo)重建密集三维点云，并通过手动二维标注并将结果投影到三维空间来生成语义标注。&lt;h4&gt;主要发现&lt;/h4&gt;3DAeroRelief捕捉了具有细粒度结构损伤的大规模户外环境，处于真实世界灾害背景中；无人机能够在危险区域实现经济、灵活和安全的数据收集，特别适合紧急情况。&lt;h4&gt;结论&lt;/h4&gt;通过评估多个最先进的三维分割模型，展示了3DAeroRelief的效用，突出了灾害响应中三维场景理解的挑战和机会；该数据集作为推进灾后场景中稳健三维视觉系统在真实世界应用中有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;及时评估结构损伤对灾害响应和恢复至关重要。然而，之前大多数自然灾害分析工作依赖于二维图像，这些图像缺乏深度、存在遮挡问题，且提供有限的空间上下文。三维语义分割提供了更丰富的替代方案，但现有的三维基准数据集主要关注城市或室内场景，很少关注灾害影响区域。为解决这一差距，我们提出了3DAeroRelief——首个专门用于灾后评估的三维基准数据集。该数据集使用低成本无人机在飓风破坏区域收集，通过运动结构(Motion Structure)和多视图立体技术(Multi-View Stereo)重建密集三维点云。语义标注通过手动二维标注并投影到三维空间生成。与现有数据集不同，3DAeroRelief在真实世界灾害背景下捕捉了具有细粒度结构损伤的大规模户外环境。无人机能够在危险区域实现经济、灵活和安全的数据收集，使其特别适合紧急情况。为展示3DAeroRelief的效用，我们评估了数据集上的几个最先进的三维分割模型，以突显灾害响应中三维场景理解的挑战和机会。我们的数据集作为推进灾后场景中稳健三维视觉系统在真实世界应用中有价值的资源。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决灾后评估中缺乏高质量3D数据集的问题。目前大多数灾害分析依赖2D图像，这些图像缺乏深度信息、容易受遮挡且空间上下文有限。这个问题在现实中非常重要，因为及时准确评估灾后结构损坏对灾害响应、资源分配和恢复规划至关重要，直接影响救援效率和灾后重建质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有3D数据集主要集中在城市或室内场景，缺乏灾后特定场景的问题。他们借鉴了先前2D灾害评估数据集(如FloodNet和RescueNet)的思路，但将其扩展到3D领域。技术上，他们采用标准的运动恢复结构(SfM)和多视图立体(MVS)技术进行3D重建，参考了现有的语义标注方法，并使用CloudCompare等工具进行3D编辑，形成了一套完整的从数据采集到标注的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用无人机收集灾后区域的航拍影像，重建高分辨率3D点云，并进行精细语义标注，创建专门的灾后评估3D基准数据集。整体流程包括：1)数据收集与预处理(无人机拍摄、视频清理和帧提取)；2)3D重建(SfM建立稀疏结构，MVS生成密集点云)；3)后处理(绝对重缩放和区域清理)；4)标注(2D图像标注、标签投影到3D空间和3D精细化调整)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门用于灾后评估的3D基准数据集；2)使用低成本无人机收集数据，提高经济性和安全性；3)提供高分辨率3D点云覆盖大规模灾后区域；4)包含精细的结构损坏标注。相比之前工作，室内数据集(如S3DIS)只覆盖小规模环境，室外数据集(如SemanticKITTI)使用LiDAR生成稀疏点云且主要针对自动驾驶场景，而3DAeroRelief填补了大规模高分辨率灾后3D数据的空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3DAeroRelief是首个专门用于灾后评估的3D基准数据集，通过无人机收集高分辨率航拍影像并重建为密集3D点云，为开发更准确、全面的灾后评估3D视觉系统提供了宝贵的资源。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely assessment of structural damage is critical for disaster response andrecovery. However, most prior work in natural disaster analysis relies on 2Dimagery, which lacks depth, suffers from occlusions, and provides limitedspatial context. 3D semantic segmentation offers a richer alternative, butexisting 3D benchmarks focus mainly on urban or indoor scenes, with littleattention to disaster-affected areas. To address this gap, we present3DAeroRelief--the first 3D benchmark dataset specifically designed forpost-disaster assessment. Collected using low-cost unmanned aerial vehicles(UAVs) over hurricane-damaged regions, the dataset features dense 3D pointclouds reconstructed via Structure-from-Motion and Multi-View Stereotechniques. Semantic annotations were produced through manual 2D labeling andprojected into 3D space. Unlike existing datasets, 3DAeroRelief captures 3Dlarge-scale outdoor environments with fine-grained structural damage inreal-world disaster contexts. UAVs enable affordable, flexible, and safe datacollection in hazardous areas, making them particularly well-suited foremergency scenarios. To demonstrate the utility of 3DAeroRelief, we evaluateseveral state-of-the-art 3D segmentation models on the dataset to highlightboth the challenges and opportunities of 3D scene understanding in disasterresponse. Our dataset serves as a valuable resource for advancing robust 3Dvision systems in real-world applications for post-disaster scenarios.</description>
      <author>example@mail.com (Nhut Le, Ehsan Karimi, Maryam Rahnemoonfar)</author>
      <guid isPermaLink="false">2509.11097v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Point-Plane Projections for Accurate LiDAR Semantic Segmentation in Small Data Scenarios</title>
      <link>http://arxiv.org/abs/2509.10841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Computer Vision and Image Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的LiDAR点云语义分割方法，通过点平面投影从2D表示中学习特征，并引入几何感知数据增强技术，在仅依赖LiDAR数据的情况下提高了性能，特别是在数据有限场景中表现优异。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云语义分割对于自动驾驶和机器人等应用中解释3D环境至关重要。现有方法虽然性能强大，但通常计算复杂度高，需要大量训练数据，限制了它们在数据稀少场景中的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;改进基于点的方法在数据有限场景中的性能，通过有效学习2D表示的特征来提取互补信息，同时仅依赖LiDAR数据。&lt;h4&gt;方法&lt;/h4&gt;通过点平面投影从2D表示中有效学习特征，引入符合LiDAR传感器特性的几何感知数据增强技术来缓解类别不平衡问题，并将点平面投影应用于点云的多个信息丰富的2D表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，这种方法在数据有限的情况下取得了显著改进，同时在两个公开可用的标准数据集（SemanticKITTI和PandaSet）上也取得了具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在仅依赖LiDAR数据的情况下，通过点平面投影和几何感知数据增强技术，有效提高了点云语义分割的性能，特别是在数据有限场景中表现优异，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;激光雷达点云语义分割对于自动驾驶和机器人等应用中解释三维环境至关重要。最近的方法通过利用不同的点云表示或整合来自其他传感器（如摄像头）或外部数据集的数据来实现强大的性能。然而，这些方法通常计算复杂度高，需要大量训练数据，限制了它们在数据稀少场景中的泛化能力。本文通过点平面投影从二维表示中有效学习特征，改进了基于点的方法的性能，能够在仅依赖激光雷达数据的同时提取互补信息。此外，我们引入了一种符合激光雷达传感器特性的几何感知数据增强技术，可以缓解类别不平衡问题。我们实现了并评估了将点平面投影应用于点云的多个信息丰富的二维表示的方法。实验表明，这种方法在数据有限的情况下取得了显著改进，同时在两个公开可用的标准数据集（如SemanticKITTI和PandaSet）上也取得了具有竞争力的结果。我们方法的代码可在https://github.com/SiMoM0/3PNet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云语义分割在数据稀缺场景下的性能提升问题。这个问题在现实世界中非常重要，因为自动驾驶和机器人等应用需要高效的3D环境理解，但获取大量标注数据成本高昂且耗时。许多现有方法依赖大量训练数据或多传感器融合，限制了它们在资源受限环境或特殊场景中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的优缺点，发现点云直接处理方法在数据稀缺场景下表现更好但仍有提升空间，而投影方法虽然高效但存在信息损失。他们设计了一种混合方法，保留点云直接处理的优点同时借鉴投影方法的高效性。该方法基于之前的工作(Fusaro et al., 2024)进行扩展，该工作又受Puy et al. (2023)启发。作者采用了Instance CutMix技术来缓解类别不平衡，并引入了两种新的2D点云表示和层跳跃连接机制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过点平面投影技术将3D点云投影到多个2D平面上，从不同视角提取互补的几何和空间信息，利用高效的2D卷积处理这些投影，同时结合简单的点级操作保持点云原始结构。整体流程包括：1)点云嵌入模块提取点级和邻域特征；2)主干网络使用SpatialMix模块将点云投影到5种2D平面并处理，ChannelMix模块在3D空间细化特征；3)分割头结合初始特征生成预测；4)几何感知的Instance CutMix数据增强；5)交叉熵和Lovasz-Softmax损失的加权和作为最终损失函数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多点平面投影架构，使用5种2D点云表示并引入极坐标网格和距离图像两种新表示；2)几何感知的Instance CutMix数据增强，根据LiDAR束配置调整点密度；3)层跳跃连接保留重要特征；4)轻量级高效设计，使用2D卷积替代3D卷积。相比之前工作，该方法仅使用LiDAR数据不需要多传感器融合，比纯投影方法保留了更多原始点云信息，比纯点云方法计算效率更高，并在小数据场景下表现显著优于作者之前的工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于点平面投影的轻量级LiDAR语义分割方法，通过多视角2D表示和几何感知数据增强，显著提升了数据稀缺场景下的分割性能，同时保持了计算效率和标准数据集上的竞争力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud semantic segmentation is essential for interpreting 3Denvironments in applications such as autonomous driving and robotics. Recentmethods achieve strong performance by exploiting different point cloudrepresentations or incorporating data from other sensors, such as cameras orexternal datasets. However, these approaches often suffer from highcomputational complexity and require large amounts of training data, limitingtheir generalization in data-scarce scenarios. In this paper, we improve theperformance of point-based methods by effectively learning features from 2Drepresentations through point-plane projections, enabling the extraction ofcomplementary information while relying solely on LiDAR data. Additionally, weintroduce a geometry-aware technique for data augmentation that aligns withLiDAR sensor properties and mitigates class imbalance. We implemented andevaluated our method that applies point-plane projections onto multipleinformative 2D representations of the point cloud. Experiments demonstrate thatthis approach leads to significant improvements in limited-data scenarios,while also achieving competitive results on two publicly available standarddatasets, as SemanticKITTI and PandaSet. The code of our method is available athttps://github.com/SiMoM0/3PNet</description>
      <author>example@mail.com (Simone Mosco, Daniel Fusaro, Wanmeng Li, Emanuele Menegatti, Alberto Pretto)</author>
      <guid isPermaLink="false">2509.10841v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deceptive Risk Minimization: Out-of-Distribution Generalization by Deceiving Distribution Shift Detectors</title>
      <link>http://arxiv.org/abs/2509.12081v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为欺骗风险最小化(DRM)的新方法，通过学习使训练数据看起来独立同分布的数据表示来实现分布外泛化，从而识别稳定特征并消除虚假相关性。&lt;h4&gt;背景&lt;/h4&gt;传统的领域适应或不变表示学习方法需要访问测试数据或将训练数据划分为有限数量的数据生成域，限制了它们在实际应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;提出一种不需要测试数据访问或训练数据预划分的分布外泛化方法。&lt;h4&gt;方法&lt;/h4&gt;提出欺骗风险最小化(DRM)原则，通过一个可微分目标函数实现，该函数同时学习消除分布偏移的特征并最小化任务特定损失。&lt;h4&gt;主要发现&lt;/h4&gt;DRM在概念转换的数值实验和具有协变量偏移的模拟模仿学习环境中表现有效，特别是在机器人部署的环境中。&lt;h4&gt;结论&lt;/h4&gt;DRM提供了一种新的分布外泛化机制，通过学习使训练数据看起来独立同分布的表示，可以推广到未见过的领域，且不需要传统方法所需的测试数据访问或数据预划分。&lt;h4&gt;翻译&lt;/h4&gt;本文提出将欺骗作为分布外(OOD)泛化的机制：通过学习使训练数据对观察者看起来独立同分布(iid)的数据表示，我们可以识别出消除虚假相关性的稳定特征，并推广到未见过的领域。我们将这一原则称为欺骗风险最小化(DRM)，并通过一个实用的可微分目标函数实例化，该函数同时学习从基于保鞅的检测器角度看消除分布偏移的特征，同时最小化特定任务的损失。与领域适应或先前的不变表示学习方法相比，DRM不需要访问测试数据或将训练数据划分为有限数量的数据生成域。我们在具有概念转换的数值实验和机器人部署环境中具有协变量偏移的模拟模仿学习设置上证明了DRM的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes deception as a mechanism for out-of-distribution (OOD)generalization: by learning data representations that make training data appearindependent and identically distributed (iid) to an observer, we can identifystable features that eliminate spurious correlations and generalize to unseendomains. We refer to this principle as deceptive risk minimization (DRM) andinstantiate it with a practical differentiable objective that simultaneouslylearns features that eliminate distribution shifts from the perspective of adetector based on conformal martingales while minimizing a task-specific loss.In contrast to domain adaptation or prior invariant representation learningmethods, DRM does not require access to test data or a partitioning of trainingdata into a finite number of data-generating domains. We demonstrate theefficacy of DRM on numerical experiments with concept shift and a simulatedimitation learning setting with covariate shift in environments that a robot isdeployed in.</description>
      <author>example@mail.com (Anirudha Majumdar)</author>
      <guid isPermaLink="false">2509.12081v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>RAM++: Robust Representation Learning via Adaptive Mask for All-in-One Image Restoration</title>
      <link>http://arxiv.org/abs/2509.12039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 22 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了RAM++（Robust Representation Learning via Adaptive Mask），一个用于全面图像恢复的两阶段框架，结合高级语义理解和低级纹理生成，实现面向内容的鲁棒恢复。&lt;h4&gt;背景&lt;/h4&gt;现有基于退化导向的方法在极端场景（如与图像结构强耦合的退化）中存在局限性，同时面临跨任务性能不均衡、对已见退化过拟合以及对未见过退化泛化能力弱等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理各种退化场景（包括已见、未见、极端和混合退化）的鲁棒图像恢复框架，解决现有方法在极端场景下的局限性，并提高跨任务性能的均衡性。&lt;h4&gt;方法&lt;/h4&gt;RAM++包含三个关键设计：1) 自适应语义感知掩码（AdaSAM）：对语义丰富和纹理区域应用像素级掩码的预训练策略；2) 掩码属性电导（MAC）：调整贡献度更高的层的选择性微调策略；3) 鲁棒特征正则化（RFR）：利用DINOv2的语义一致且退化不变的表示，结合高效特征融合的策略。&lt;h4&gt;主要发现&lt;/h4&gt;RAM++通过三个关键设计，在已见、未见、极端和混合退化场景下实现了鲁棒、均衡和最先进的性能，有效解决了现有方法在极端场景下的局限性。&lt;h4&gt;结论&lt;/h4&gt;RAM++是一个有效的两阶段框架，通过结合高级语义理解和低级纹理生成，实现了面向内容的鲁棒图像恢复，在各种退化场景下表现出色。&lt;h4&gt;翻译&lt;/h4&gt;这项工作提出了通过自适应掩码进行鲁棒表示学习（RAM++），这是一个用于全面图像恢复的两阶段框架。RAM++将高级语义理解与低级纹理生成相结合，以实现面向内容的鲁棒恢复。它解决了现有基于退化导向的方法在极端场景（例如与图像结构强耦合的退化）中的局限性。RAM++还通过三个关键设计缓解了常见挑战，如跨任务性能不均衡、对已见退化的过拟合以及对未见过退化的弱泛化能力：1）自适应语义感知掩码（AdaSAM）：一种预训练策略，对语义丰富和纹理区域应用像素级掩码。这种设计使网络能够从各种退化中学习生成先验和图像内容先验。2）掩码属性电导（MAC）：一种选择性微调策略，调整贡献度更高的层，以弥合掩码预训练和完整图像微调之间的完整性差距，同时保留已学习的先验。3）鲁棒特征正则化（RFR）：一种策略，利用DINOv2的语义一致且退化不变的表示，结合高效特征融合，实现忠实且语义连贯的恢复。通过这些设计，RAM++在已见、未见、极端和混合退化场景下实现了鲁棒、均衡和最先进的性能。我们的代码和模型将在https://github.com/DragonisCV/RAM发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents Robust Representation Learning via Adaptive Mask (RAM++),a two-stage framework for all-in-one image restoration. RAM++ integrateshigh-level semantic understanding with low-level texture generation to achievecontent-oriented robust restoration. It addresses the limitations of existingdegradation-oriented methods in extreme scenarios (e.g., degradations stronglycoupled with image structures). RAM++ also mitigates common challenges such asunbalanced performance across tasks, overfitting to seen degradations, and weakgeneralization to unseen ones through three key designs: 1) AdaptiveSemantic-Aware Mask (AdaSAM): a pretraining strategy that applies pixel-levelmasks to semantically rich and textured regions. This design enables thenetwork to learn both generative priors and image content priors from variousdegradations. 2) Mask Attribute Conductance (MAC): a selective fine-tuningstrategy that adjusts the layers with higher contributions to bridge theintegrity gap between masked pretraining and full-image fine-tuning whileretaining learned priors. 3) Robust Feature Regularization (RFR): a strategythat leverages DINOv2's semantically consistent and degradation-invariantrepresentations, together with efficient feature fusion, to achieve faithfuland semantically coherent restoration. With these designs, RAM++ achievesrobust, well-balanced, and state-of-the-art performance across seen, unseen,extreme, and mixed degradations. Our code and model will be released athttps://github.com/DragonisCV/RAM</description>
      <author>example@mail.com (Zilong Zhang, Chujie Qin, Chunle Guo, Yong Zhang, Chao Xue, Ming-Ming Cheng, Chongyi Li)</author>
      <guid isPermaLink="false">2509.12039v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Captions for Open-Vocabulary Zero-Shot Segmentation</title>
      <link>http://arxiv.org/abs/2509.11840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 CDEL Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。本研究通过将图像与VLMs生成的合成描述进行密集对齐，结合了生成式VLMs和视觉语言对齐表示学习两个研究方向。合成字幕成本低、可扩展且易于生成，为密集对齐方法提供了高级语义理解的优质来源。实验表明，该方法在标准零样本开放词汇分割基准上优于先前工作，且数据效率更高。&lt;h4&gt;背景&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐。另一研究方向专注于视觉语言对齐的表示学习，针对的是像分割这样的密集任务的零样本推理。&lt;h4&gt;目的&lt;/h4&gt;结合生成式VLMs和视觉语言对齐表示学习两个研究方向，通过将图像与VLMs生成的合成描述进行密集对齐。&lt;h4&gt;方法&lt;/h4&gt;使用VLMs生成合成字幕，这些字幕成本低、可扩展且易于生成，作为密集对齐方法的高级语义理解来源。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在标准的零样本开放词汇分割基准/数据集上优于先前的工作，同时具有更高的数据效率。&lt;h4&gt;结论&lt;/h4&gt;通过VLMs生成的合成描述与图像进行密集对齐是一种有效的方法，可以在零样本开放词汇分割任务中取得更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;生成式视觉语言模型在高层图像理解方面表现出色，但缺乏视觉和语言模态之间的空间密集对齐，正如我们的研究结果所示。与生成式VLMs的进展相辅相成，另一研究方向专注于视觉语言对齐的表示学习，针对的是像分割这样的密集任务的零样本推理。在这项工作中，我们通过将图像与VLMs生成的合成描述进行密集对齐，结合了这两个方向。合成字幕成本低、可扩展且易于生成，使它们成为密集对齐方法的高级语义理解的优质来源。实验上，我们的方法在标准的零样本开放词汇分割基准/数据集上优于先前的工作，同时具有更高的数据效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative vision-language models (VLMs) exhibit strong high-level imageunderstanding but lack spatially dense alignment between vision and languagemodalities, as our findings indicate. Orthogonal to advancements in generativeVLMs, another line of research has focused on representation learning forvision-language alignment, targeting zero-shot inference for dense tasks likesegmentation. In this work, we bridge these two directions by densely aligningimages with synthetic descriptions generated by VLMs. Synthetic captions areinexpensive, scalable, and easy to generate, making them an excellent source ofhigh-level semantic understanding for dense alignment methods. Empirically, ourapproach outperforms prior work on standard zero-shot open-vocabularysegmentation benchmarks/datasets, while also being more data-efficient.</description>
      <author>example@mail.com (Tim Lebailly, Vijay Veerabadran, Satwik Kottur, Karl Ridgeway, Michael Louis Iuzzolino)</author>
      <guid isPermaLink="false">2509.11840v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly Detection in Industrial Control Systems Based on Cross-Domain Representation Learning</title>
      <link>http://arxiv.org/abs/2509.11786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于跨域表示学习的工业控制系统异常检测方法，能够学习多领域行为的联合特征并在不同领域内检测异常，实验结果表明该方法性能优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;工业控制系统广泛应用于工业领域，其安全性和稳定性至关重要。ICS通过网络通信远程监控和管理物理设备，而现有异常检测方法主要关注网络流量或传感器数据的安全分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于跨域表示学习的ICS异常检测方法，学习多领域行为的联合特征，在不同领域内检测异常，克服仅分析单一领域难以全面识别异常的问题。&lt;h4&gt;方法&lt;/h4&gt;构建能够表示ICS多领域行为的跨域图，利用图神经网络学习这些行为的联合特征，采用多任务学习方法在不同领域分别识别异常并进行联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;由于异常在不同领域表现出不同行为，通过多任务学习分别识别不同领域的异常并进行联合训练，能够提高异常检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，所提出的基于跨域表示学习的异常检测方法在识别ICS异常方面的性能优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;工业控制系统(ICS)广泛应用于工业领域，其安全性和稳定性非常重要。一旦ICS受到攻击，可能会造成严重损害。因此，检测ICS中的异常非常重要。ICS可以使用通信网络远程监控和管理物理设备。现有的异常检测方法主要关注分析网络流量或传感器数据的安全性。然而，ICS不同领域（如网络流量和传感器物理状态）的行为是相互关联的，因此仅通过分析单一领域难以全面识别异常。本文提出了一种基于ICS跨域表示学习的异常检测方法，可以学习多领域行为的联合特征，并在不同领域内检测异常。在构建能够表示ICS多领域行为的跨域图后，我们的方法可以利用图神经网络学习它们的联合特征。由于异常在不同领域表现出不同的行为，我们利用多任务学习方法分别识别不同领域的异常并进行联合训练。实验结果表明，我们的方法在识别ICS异常方面的性能优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial control systems (ICSs) are widely used in industry, and theirsecurity and stability are very important. Once the ICS is attacked, it maycause serious damage. Therefore, it is very important to detect anomalies inICSs. ICS can monitor and manage physical devices remotely using communicationnetworks. The existing anomaly detection approaches mainly focus on analyzingthe security of network traffic or sensor data. However, the behaviors ofdifferent domains (e.g., network traffic and sensor physical status) of ICSsare correlated, so it is difficult to comprehensively identify anomalies byanalyzing only a single domain. In this paper, an anomaly detection approachbased on cross-domain representation learning in ICSs is proposed, which canlearn the joint features of multi-domain behaviors and detect anomalies withindifferent domains. After constructing a cross-domain graph that can representthe behaviors of multiple domains in ICSs, our approach can learn the jointfeatures of them by leveraging graph neural networks. Since anomalies behavedifferently in different domains, we leverage a multi-task learning approach toidentify anomalies in different domains separately and perform joint training.The experimental results show that the performance of our approach is betterthan existing approaches for identifying anomalies in ICSs.</description>
      <author>example@mail.com (Dongyang Zhan, Wenqi Zhang, Lin Ye, Xiangzhan Yu, Hongli Zhang, Zheng He)</author>
      <guid isPermaLink="false">2509.11786v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FuseCodec: Semantic-Contextual Fusion and Supervision for Neural Codecs</title>
      <link>http://arxiv.org/abs/2509.11425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FuseCodec通过强大的跨模态对齐和全局监督信息统一了声学、语义和上下文表示，在语音标记化任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的神经编解码器捕获低级声学特征，忽略了人类语音中固有的语义和上下文线索。虽然最近的工作引入了语义表示或上下文表示，但在对齐和统一这些表示方面仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;引入FuseCodec，通过强大的跨模态对齐和全局监督信息来统一声学、语义和上下文表示。&lt;h4&gt;方法&lt;/h4&gt;FuseCodec采用三种互补技术：(i)潜在表示融合：将语义和上下文特征直接集成到编码器潜在空间中；(ii)全局语义-上下文监督：使用全局池化和广播表示监督离散令牌；(iii)时间对齐的上下文监督：在局部窗口内动态匹配上下文和语音令牌。同时介绍了FuseCodec-TTS，展示其在零样本语音合成中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;FuseCodec在LibriSpeech上取得了最先进的性能，在转录准确性、感知质量、可懂度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。&lt;h4&gt;结论&lt;/h4&gt;上下文和语义引导的标记化对语音标记化和下游任务有效。代码和预训练模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;语音标记化能够实现离散表示并促进语音语言建模。然而，现有的神经编解码器捕获低级声学特征，忽略了人类语音中固有的语义和上下文线索。虽然最近的工作引入了来自自监督语音模型的语义表示或集成了来自预训练语言模型的上下文表示，但在对齐和统一这些语义和上下文表示方面仍存在挑战。我们引入了FuseCodec，它通过强大的跨模态对齐和全局监督信息来统一声学、语义和上下文表示。我们提出了三种互补技术：(i)潜在表示融合，将语义和上下文特征直接集成到编码器潜在空间中，以实现强大和统一的表示学习；(ii)全局语义-上下文监督，使用全局池化和广播表示监督离散令牌，以增强时间一致性和跨模态对齐；(iii)时间对齐的上下文监督，通过在局部窗口内动态匹配上下文和语音令牌来加强对齐，实现细粒度的令牌级监督。我们进一步介绍了FuseCodec-TTS，展示了我们的方法在零样本语音合成中的应用。实验表明，FuseCodec在LibriSpeech上取得了最先进的性能，在转录准确性、感知质量、可懂度和说话人相似性方面超越了EnCodec、SpeechTokenizer和DAC。结果强调了上下文和语义引导的标记化对语音标记化和下游任务的有效性。代码和预训练模型可在https://github.com/mubtasimahasan/FuseCodec获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech tokenization enables discrete representation and facilitates speechlanguage modeling. However, existing neural codecs capture low-level acousticfeatures, overlooking the semantic and contextual cues inherent to humanspeech. While recent efforts introduced semantic representations fromself-supervised speech models or incorporated contextual representations frompre-trained language models, challenges remain in aligning and unifying thesemantic and contextual representations. We introduce FuseCodec, which unifiesacoustic, semantic, and contextual representations through strong cross-modalalignment and globally informed supervision. We propose three complementarytechniques: (i) Latent Representation Fusion, integrating semantic andcontextual features directly into the encoder latent space for robust andunified representation learning; (ii) Global Semantic-Contextual Supervision,supervising discrete tokens with globally pooled and broadcastedrepresentations to enhance temporal consistency and cross-modal alignment; and(iii) Temporally Aligned Contextual Supervision, strengthening alignment bydynamically matching contextual and speech tokens within a local window forfine-grained token-level supervision. We further introduce FuseCodec-TTS,demonstrating our methodology's applicability to zero-shot speech synthesis.Empirically, FuseCodec achieves state-of-the-art performance in LibriSpeech,surpassing EnCodec, SpeechTokenizer, and DAC in transcription accuracy,perceptual quality, intelligibility, and speaker similarity. Results highlightthe effectiveness of contextually and semantically guided tokenization forspeech tokenization and downstream tasks. Code and pretrained models areavailable at https://github.com/mubtasimahasan/FuseCodec.</description>
      <author>example@mail.com (Md Mubtasim Ahasan, Rafat Hasan Khan, Tasnim Mohiuddin, Aman Chadha, Tariq Iqbal, M Ashraful Amin, Amin Ahsan Ali, Md Mofijul Islam, A K M Mahbubur Rahman)</author>
      <guid isPermaLink="false">2509.11425v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>PersonaX: Multimodal Datasets with LLM-Inferred Behavior Traits</title>
      <link>http://arxiv.org/abs/2509.11362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了PersonaX，一个多模态数据集集合，用于全面分析公共人物的行为特征，包含CelebPersona（9444名公众人物）和AthlePersona（4181名运动员）两个子数据集，每个数据集都包含行为特征评估、面部图像和传记信息。&lt;h4&gt;背景&lt;/h4&gt;理解人类行为特征在人机交互、计算社会科学和个性化AI系统中至关重要，需要整合多种模态捕捉细微模式，但现有资源很少提供结合行为描述符与面部属性、传记信息等互补模态的数据集。&lt;h4&gt;目的&lt;/h4&gt;解决现有资源不足，提供一个结合行为描述符与互补模态的数据集，支持跨模态的公共特征全面分析。&lt;h4&gt;方法&lt;/h4&gt;创建PersonaX数据集集合，并在两个层次上分析：1)从文本描述提取高级特征分数，应用五种统计独立性检验检查与其他模态的关系；2)引入专为多模态和多测量数据设计的因果表示学习框架，提供理论可识别性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实数据上的实验证明了该方法的有效性，PersonaX为研究LLM推断的行为特征与视觉和传记属性的结合奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;PersonaX通过整合结构化和非结构化分析，建立了研究LLM推断行为特征与视觉和传记属性相结合的基础，促进了多模态特征分析和因果推理的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解人类行为特征是人机交互、计算社会科学和个性化AI系统应用的核心。这种理解通常需要整合多种模态来捕捉细微的模式和关系。然而，现有资源很少提供将行为描述符与面部属性和传记信息等互补模态结合的数据集。为了解决这一差距，我们提出了PersonaX，这是一个精心策划的多模态数据集集合，旨在实现跨模态的公共特征全面分析。PersonaX包括(1)CelebPersona，包含来自不同职业的9444名公众人物，和(2)AthlePersona，涵盖7个主要体育联赛的4181名专业运动员。每个数据集都包含由三个高性能大型语言模型推断的行为特征评估，以及面部图像和结构化的传记特征。我们在两个互补层次上分析PersonaX。首先，我们从文本描述中抽象出高级特征分数，并应用五种统计独立性检验来检查它们与其他模态的关系。其次，我们引入了一种新的因果表示学习框架，专为多模态和多测量数据设计，提供理论可识别性保证。在合成和真实数据上的实验证明了我们方法的有效性。通过统一结构化和非结构化分析，PersonaX为研究LLM推断的行为特征与视觉和传记属性相结合奠定了基础，推进了多模态特征分析和因果推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding human behavior traits is central to applications inhuman-computer interaction, computational social science, and personalized AIsystems. Such understanding often requires integrating multiple modalities tocapture nuanced patterns and relationships. However, existing resources rarelyprovide datasets that combine behavioral descriptors with complementarymodalities such as facial attributes and biographical information. To addressthis gap, we present PersonaX, a curated collection of multimodal datasetsdesigned to enable comprehensive analysis of public traits across modalities.PersonaX consists of (1) CelebPersona, featuring 9444 public figures fromdiverse occupations, and (2) AthlePersona, covering 4181 professional athletesacross 7 major sports leagues. Each dataset includes behavioral traitassessments inferred by three high-performing large language models, alongsidefacial imagery and structured biographical features. We analyze PersonaX at twocomplementary levels. First, we abstract high-level trait scores from textdescriptions and apply five statistical independence tests to examine theirrelationships with other modalities. Second, we introduce a novel causalrepresentation learning (CRL) framework tailored to multimodal andmulti-measurement data, providing theoretical identifiability guarantees.Experiments on both synthetic and real-world data demonstrate the effectivenessof our approach. By unifying structured and unstructured analysis, PersonaXestablishes a foundation for studying LLM-inferred behavioral traits inconjunction with visual and biographical attributes, advancing multimodal traitanalysis and causal reasoning.</description>
      <author>example@mail.com (Loka Li, Wong Yu Kang, Minghao Fu, Guangyi Chen, Zhenhao Chen, Gongxu Luo, Yuewen Sun, Salman Khan, Peter Spirtes, Kun Zhang)</author>
      <guid isPermaLink="false">2509.11362v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Towards Automated Error Discovery: A Study in Conversational AI</title>
      <link>http://arxiv.org/abs/2509.10833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to EMNLP 2025 main conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基于大语言模型的对话代理虽然流畅连贯，但仍会产生难以预防的不良行为。本研究提出了一种自动错误发现框架和SEEED方法，用于检测对话AI中的错误，特别是指令中未明确指定的错误。&lt;h4&gt;背景&lt;/h4&gt;基于大语言模型的对话代理表现出强大的流畅性和连贯性，但仍会产生不良行为（错误），这些错误很难在部署过程中阻止其到达用户。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够检测和定义对话AI中错误的框架，特别是那些由于响应生成模型更新或用户行为变化而产生的新错误。&lt;h4&gt;方法&lt;/h4&gt;引入'自动错误发现'框架，提出SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为基于编码器的实现方法。增强软最近邻损失中负样本的距离加权，并引入基于标签的样本排序来选择高度对比的示例，以实现更好的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;SEEED在多个错误标注的对话数据集上优于适应的基线方法（包括GPT-4o和Phi-4），将检测未知错误的准确性提高了最多8个百分点，并且在对未知意图检测方面表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SEEED方法能有效检测对话AI中的未知错误，提高错误检测的准确性，并在未知意图检测方面表现出良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;尽管基于大语言模型的对话代理表现出强大的流畅性和连贯性，但它们仍然会产生不良行为（错误），这些错误很难在部署过程中阻止其到达用户。最近的研究利用大语言模型来检测错误并指导响应生成模型的改进。然而，当前的LLM难以识别指令中未明确指定的错误，例如由于响应生成模型更新或用户行为变化而产生的错误。在这项工作中，我们引入了'自动错误发现'，这是一个用于检测和定义对话AI中错误的框架，并提出了SEEED（Soft Clustering Extended Encoder-Based Error Detection）作为其基于编码器的实现方法。我们通过增强负样本的距离加权来改进软最近邻损失，并引入基于标签的样本排序来选择高度对比的示例，以实现更好的表示学习。SEEED在多个错误标注的对话数据集上优于适应的基线方法（包括GPT-4o和Phi-4），将检测未知错误的准确性提高了最多8个百分点，并表现出对未知意图检测的强大泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although LLM-based conversational agents demonstrate strong fluency andcoherence, they still produce undesirable behaviors (errors) that arechallenging to prevent from reaching users during deployment. Recent researchleverages large language models (LLMs) to detect errors and guideresponse-generation models toward improvement. However, current LLMs struggleto identify errors not explicitly specified in their instructions, such asthose arising from updates to the response-generation model or shifts in userbehavior. In this work, we introduce Automated Error Discovery, a framework fordetecting and defining errors in conversational AI, and propose SEEED (SoftClustering Extended Encoder-Based Error Detection), as an encoder-basedapproach to its implementation. We enhance the Soft Nearest Neighbor Loss byamplifying distance weighting for negative samples and introduce Label-BasedSample Ranking to select highly contrastive examples for better representationlearning. SEEED outperforms adapted baselines -- including GPT-4o and Phi-4 --across multiple error-annotated dialogue datasets, improving the accuracy fordetecting unknown errors by up to 8 points and demonstrating stronggeneralization to unknown intent detection.</description>
      <author>example@mail.com (Dominic Petrak, Thy Thy Tran, Iryna Gurevych)</author>
      <guid isPermaLink="false">2509.10833v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Socially-Informed Content Analysis of Online Human Behavior</title>
      <link>http://arxiv.org/abs/2509.10807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Doctoral dissertation, University of Southern California, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文采用计算社会科学技术研究社交媒体带来的挑战，如政治极化、虚假信息、仇恨言论和回音室，并提出数据驱动的解决方案促进更健康的数字互动。&lt;h4&gt;背景&lt;/h4&gt;社交媒体的爆炸性增长不仅改变了通信方式，还带来了政治极化、虚假信息、仇恨言论和回音室等挑战。&lt;h4&gt;目的&lt;/h4&gt;研究社交媒体带来的问题，理解推动负面在线行为的社会动态，并提出数据驱动的解决方案以促进更健康的数字互动。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种可扩展的社会网络表示学习方法，将用户生成的内容与社会连接相结合，创建统一的用户嵌入，从而准确预测和可视化用户属性、社区和行为倾向。&lt;h4&gt;主要发现&lt;/h4&gt;1) Twitter上关于COVID-19的讨论揭示了政治极化和不对称政治回音室；2) 在线仇恨言论表明追求社会认可推动了有毒行为；3) COVID-19讨论的道德基础揭示了道德同质性和回音室的模式，同时表明道德多样性和多元性可以提高跨意识形态分歧的信息传播和接受度。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于计算社会科学的发展，并为通过社会互动和网络同质性视角理解人类行为提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体的爆炸性增长不仅彻底改变了通信方式，也带来了政治极化、虚假信息、仇恨言论和回音室等挑战。本文采用计算社会科学技术研究这些问题，理解推动负面在线行为的社会动态，并提出数据驱动的解决方案以促进更健康的数字互动。首先，我介绍了一种可扩展的社会网络表示学习方法，将用户生成的内容与社会连接相结合，创建统一的用户嵌入，从而能够准确预测和可视化用户属性、社区和行为倾向。使用这一工具，我探索了三个相互关联的问题：1) Twitter上关于COVID-19的讨论，揭示了政治极化和不对称政治回音室；2) 在线仇恨言论，表明追求社会认可推动了有毒行为；3) COVID-19讨论的道德基础，揭示了道德同质性和回音室的模式，同时表明道德多样性和多元性可以提高跨意识形态分歧的信息传播和接受度。这些发现有助于计算社会科学的发展，并为通过社会互动和网络同质性视角理解人类行为提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of social media has not only revolutionizedcommunication but also brought challenges such as political polarization,misinformation, hate speech, and echo chambers. This dissertation employscomputational social science techniques to investigate these issues, understandthe social dynamics driving negative online behaviors, and propose data-drivensolutions for healthier digital interactions. I begin by introducing a scalablesocial network representation learning method that integrates user-generatedcontent with social connections to create unified user embeddings, enablingaccurate prediction and visualization of user attributes, communities, andbehavioral propensities. Using this tool, I explore three interrelatedproblems: 1) COVID-19 discourse on Twitter, revealing polarization andasymmetric political echo chambers; 2) online hate speech, suggesting thepursuit of social approval motivates toxic behavior; and 3) moral underpinningsof COVID-19 discussions, uncovering patterns of moral homophily and echochambers, while also indicating moral diversity and plurality can improvemessage reach and acceptance across ideological divides. These findingscontribute to the advancement of computational social science and provide afoundation for understanding human behavior through the lens of socialinteractions and network homophily.</description>
      <author>example@mail.com (Julie Jiang)</author>
      <guid isPermaLink="false">2509.10807v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Outlier-Resistant Heterogeneous Treatment Effect Estimation in HDLSS Settings via GAT--CVAE Framework</title>
      <link>http://arxiv.org/abs/2509.10787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对高维小样本设置的异质性治疗效应估计的稳健框架，结合图注意力网络和条件变分自编码器，通过扩展样本空间和聚类分析，实现稳定且可推广的因果效应估计。&lt;h4&gt;背景&lt;/h4&gt;在高维小样本环境下进行异质性治疗效应估计面临挑战，现有方法可能无法有效处理结构依赖性和异常值问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健的HTE估计框架，能够在高维小样本设置中准确捕捉混杂因素间的结构依赖性，并有效整合异常值数据，从而获得稳定且可推广的因果效应估计结果。&lt;h4&gt;方法&lt;/h4&gt;结合图注意力网络捕捉混杂因素间的结构依赖性，以及条件变分自编码器进行潜在表示学习，扩展样本空间并进行聚类分析，将异常值集合整合为连贯的子组，然后使用双重稳健抗异常值估计器估计子组因果效应。&lt;h4&gt;主要发现&lt;/h4&gt;模拟和实际应用表明，该方法相比现有HTE方法具有优越性能，能够有效处理高维小样本数据中的复杂结构和异常值问题。&lt;h4&gt;结论&lt;/h4&gt;该框架为精准医疗和政策评估等领域提供了有效的异质性治疗效应估计工具，具有实际应用价值和推广潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种针对高维小样本设置的异质性治疗效应估计的稳健框架。通过结合图注意力网络来捕捉混杂因素之间的结构依赖性，以及使用条件变分自编码器进行潜在表示学习，我们的方法扩展了样本空间并执行聚类，将异常值集合整合为连贯的子组。然后使用双重稳健抗异常值估计器估计子组因果效应，产生稳定且可推广的结果。模拟和实际应用证实了与现有HTE方法相比的优越性能，突显了该框架在精准医疗和政策评估方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a robust framework for heterogeneous treatment effect (HTE)estimation tailored to high-dimensional low sample size (HDLSS) settings. Bycombining Graph Attention Networks (GAT) to capture structural dependenciesamong confounders with a Conditional Variational Autoencoder (CVAE) for latentrepresentation learning, our method expands the sample space and performsclustering that integrates even outlier sets into coherent subgroups.Clusterwise causal effects are then estimated using a doubly robustoutlier-resistant estimator, yielding stable and generalizable results.Simulations and real-world applications confirm superior performance comparedwith existing HTE methods, highlighting the framework's potential for precisionmedicine and policy evaluation.</description>
      <author>example@mail.com (Byeonghee Lee, Joonsung Kang)</author>
      <guid isPermaLink="false">2509.10787v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SurgLaVi: Large-Scale Hierarchical Dataset for Surgical Vision-Language Representation Learning</title>
      <link>http://arxiv.org/abs/2509.10555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgLaVi，迄今为止最大且多样化的手术视觉语言数据集，包含近24万剪辑-字幕对，涵盖200多种手术程序，并具有层次化结构。研究还开发了SurgCLIP模型，在多项任务上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;视觉语言预训练(VLP)在手术领域具有独特优势，能够将语言与手术视频对齐，实现工作流程理解和跨任务迁移，无需依赖专家标注数据集。然而，现有手术VLP数据集受限于规模小、程序多样性不足、语义质量不高和层次结构不完善等问题。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、多样化、语义质量高且具有层次结构的手术视觉语言数据集，以促进手术基础模型的发展。&lt;h4&gt;方法&lt;/h4&gt;开发了一个全自动管道系统，系统生成手术视频的精细转录并将其分割为连贯的程序单元；应用双模态过滤去除不相关和噪声样本，确保高质量注释；推出开源版本SurgLaVi-eta，包含11.3万剪辑-字幕对，完全基于公共数据构建；引入SurgCLIP模型，一种具有双编码器的CLIP风格视频文本对比框架。&lt;h4&gt;主要发现&lt;/h4&gt;SurgCLIP在阶段、步骤、动作和工具识别任务上实现了持续改进，显著超越了先前最先进的方法；大规模、语义丰富和层次结构化的数据集直接转化为更强和更可泛化的表示能力。&lt;h4&gt;结论&lt;/h4&gt;SurgLaVi作为开发手术基础模型的关键资源，证明了高质量、大规模数据集对提升手术AI模型性能的重要性，为手术领域的基础模型研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言预训练(VLP)通过将语言与手术视频对齐，为手术领域提供了独特优势，使工作流程理解和跨任务迁移成为可能，无需依赖专家标注的数据集。然而，现有手术VLP的进展受限于现有数据集的规模有限、程序多样性不足、语义质量和层次结构等问题。在本工作中，我们提出了SurgLaVi，迄今为止最大且最多样化的手术视觉语言数据集，包含来自200多种程序的近24万剪辑-字幕对，并在阶段、步骤和任务级别具有层次结构。SurgLaVi的核心是一个全自动管道，系统生成手术视频的精细转录并将其分割为连贯的程序单元。为确保高质量注释，它应用双模态过滤去除不相关和噪声样本。在此框架内，生成的字幕通过上下文细节进行丰富，产生语义丰富且易于解释的注释。为确保可访问性，我们发布了SurgLaVi-eta，这是一个完全基于公共数据构建的开源衍生版本，包含11.3万剪辑-字幕对，比现有手术VLP数据集大四倍以上。为证明SurgLaVi数据集的价值，我们引入了SurgCLIP，一种具有双编码器的CLIP风格视频文本对比框架，作为代表性基础模型。SurgCLIP在阶段、步骤、动作和工具识别方面取得了一致的改进，显著超越了先前最先进的方法。这些结果验证了大规模、语义丰富和层次结构化的数据集直接转化为更强和更可泛化的表示能力，确立了SurgLaVi作为开发手术基础模型的关键资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language pre-training (VLP) offers unique advantages for surgery byaligning language with surgical videos, enabling workflow understanding andtransfer across tasks without relying on expert-labeled datasets. However,progress in surgical VLP remains constrained by the limited scale, proceduraldiversity, semantic quality, and hierarchical structure of existing datasets.In this work, we present SurgLaVi, the largest and most diverse surgicalvision-language dataset to date, comprising nearly 240k clip-caption pairs frommore than 200 procedures, and comprising hierarchical levels at phase-, step-,and task-level. At the core of SurgLaVi lies a fully automated pipeline thatsystematically generates fine-grained transcriptions of surgical videos andsegments them into coherent procedural units. To ensure high-qualityannotations, it applies dual-modality filtering to remove irrelevant and noisysamples. Within this framework, the resulting captions are enriched withcontextual detail, producing annotations that are both semantically rich andeasy to interpret. To ensure accessibility, we release SurgLaVi-\b{eta}, anopen-source derivative of 113k clip-caption pairs constructed entirely frompublic data, which is over four times larger than existing surgical VLPdatasets. To demonstrate the value of SurgLaVi datasets, we introduce SurgCLIP,a CLIP-style video-text contrastive framework with dual encoders, as arepresentative base model. SurgCLIP achieves consistent improvements acrossphase, step, action, and tool recognition, surpassing prior state-of-the-artmethods, often by large margins. These results validate that large-scale,semantically rich, and hierarchically structured datasets directly translateinto stronger and more generalizable representations, establishing SurgLaVi asa key resource for developing surgical foundation models.</description>
      <author>example@mail.com (Alejandra Perez, Chinedu Nwoye, Ramtin Raji Kermani, Omid Mohareri, Muhammad Abdullah Jamal)</author>
      <guid isPermaLink="false">2509.10555v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deriving accurate galaxy cluster masses using X-ray thermodynamic profiles and graph neural networks</title>
      <link>http://arxiv.org/abs/2509.12199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 15 figures, 6 tables, resubmitted to A&amp;A after revision,  comments welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用图神经网络(GNNs)从X射线观测数据估计星系团质量，实现了比传统方法更精确的结果，并发现了SZ方法中存在的质量依赖性偏差。&lt;h4&gt;背景&lt;/h4&gt;精确测定星系团质量对于建立星系团宇宙学中可靠的质量-观测标度关系至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来精确估计星系团质量，并验证其准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNNs)处理X射线观测推断的星系团内介质(ICM)径向采样剖面，将每个ICM剖面表示为图，以处理可变长度和分辨率的输入。使用The Three Hundred Project的星系团流体动力学模拟来训练和测试模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 与真实质量相比没有系统偏差；2) 质量估计散射约为6%，比传统方法小6倍；3) 算法对数据质量和星系团形态具有鲁棒性；4) 发现SZ推导的质量存在质量依赖性偏差，质量越高的星系团偏差越大；5) 中位偏差为(1-b)=0.85_{-14}^{+34}。&lt;h4&gt;结论&lt;/h4&gt;通过整合X射线、SZ和光学数据集使用深度学习技术，在建立无偏观测质量标度关系方面迈出了重要一步，增强了星系团在精密宇宙学中的作用。&lt;h4&gt;翻译&lt;/h4&gt;精确测定星系团质量对于建立星系团宇宙学中可靠的质量-观测标度关系至关重要。我们采用图神经网络(GNNs)从X射线观测推断的星系团内介质(ICM)径向采样剖面来估计星系团质量。GNNs通过将每个ICM剖面表示为图，自然处理可变长度和分辨率的输入，能够准确灵活地建模各种观测条件。我们使用The Three Hundred Project的最先进星系团流体动力学模拟来训练和测试GNN模型。与模拟中的真实星系团质量相比，我们方法的质量估计没有系统偏差。此外，我们实现恢复质量与真实质量约为6%的散射，比标准流体静力学平衡方法获得的散射小6倍。我们的算法对数据质量和星系团形态具有鲁棒性，能够同时结合模型不确定性和观测不确定性。最后，我们将该技术应用于XMM-Newton观测的星系团样本，并将GNN推导的质量估计与通过Y_SZ-M_500标度关系获得的质量估计进行比较。我们的结果提供了强有力的证据(5σ水平)表明SZ推导的质量存在质量依赖性偏差，质量更高的星系团表现出更大的偏差程度。此外，我们发现中位偏差为(1-b)=0.85_{-14}^{+34}，但由于其质量依赖性而具有显著的弥散。这项工作通过整合X射线、SZ和光学数据集使用深度学习技术，在建立无偏观测质量标度关系方面迈出了重要一步，从而增强了星系团在精密宇宙学中的作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Precise determination of galaxy cluster masses is crucial for establishingreliable mass-observable scaling relations in cluster cosmology. We employgraph neural networks (GNNs) to estimate galaxy cluster masses from radiallysampled profiles of the intra-cluster medium (ICM) inferred from X-rayobservations. GNNs naturally handle inputs of variable length and resolution byrepresenting each ICM profile as a graph, enabling accurate and flexiblemodeling across diverse observational conditions. We trained and tested GNNmodel using state-of-the-art hydrodynamical simulations of galaxy clusters fromThe Three Hundred Project. The mass estimates using our method exhibit nosystematic bias compared to the true cluster masses in the simulations.Additionally, we achieve a scatter in recovered mass versus true mass of about6\%, which is a factor of six smaller than obtained from a standard hydrostaticequilibrium approach. Our algorithm is robust to both data quality and clustermorphology and it is capable of incorporating model uncertainties alongsideobservational uncertainties. Finally, we apply our technique to XMM-Newtonobserved galaxy cluster samples and compare the GNN derived mass estimates withthose obtained with $Y_{\rm SZ}$-M$_{500}$ scaling relations. Our resultsprovide strong evidence, at 5$\sigma$ level, for a mass-dependent bias in SZderived masses, with higher mass clusters exhibiting a greater degree ofdeviation. Furthermore, we find the median bias to be $(1-b)=0.85_{-14}^{+34}$,albeit with significant dispersion due to its mass dependence. This work takesa significant step towards establishing unbiased observable mass scalingrelations by integrating X-ray, SZ and optical datasets using deep learningtechniques, thereby enhancing the role of galaxy clusters in precisioncosmology.</description>
      <author>example@mail.com (Asif Iqbal, Subhabrata Majumdar, Elena Rasia, Gabriel W. Pratt, Daniel de Andres, Jean-Baptiste Melin, Weiguang Cui)</author>
      <guid isPermaLink="false">2509.12199v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>3DViT-GAT: A Unified Atlas-Based 3D Vision Transformer and Graph Learning Framework for Major Depressive Disorder Detection Using Structural MRI Data</title>
      <link>http://arxiv.org/abs/2509.12143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 1 figure, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合视觉Transformer和图神经网络的统一方法，用于通过结构磁共振成像自动检测重度抑郁症，有效提高了诊断准确性。&lt;h4&gt;背景&lt;/h4&gt;重度抑郁症是一种常见的精神健康问题，影响个人幸福感和全球公共卫生。使用结构磁共振成像和深度学习方法自动检测MDD有望提高诊断准确性和实现早期干预。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉复杂脑部模式的方法，超越现有的仅使用体素级特征或手工制作区域表征的方法。&lt;h4&gt;方法&lt;/h4&gt;开发了统一流程，使用视觉Transformer从sMRI数据中提取3D区域嵌入，使用图神经网络进行分类。探索了两种定义区域的方法：基于图谱的方法使用预定义的脑图谱，基于立方体的方法直接从3D补丁中识别区域。生成余弦相似度图建模区域间关系，指导GNN分类。使用REST-meta-MDD数据集进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;最佳模型在分层10折交叉验证中获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。基于图谱的模型始终优于基于立方体的方法。&lt;h4&gt;结论&lt;/h4&gt;使用特定领域的解剖先验对于MDD检测至关重要，基于预定义脑图谱的方法表现更好。&lt;h4&gt;翻译&lt;/h4&gt;重度抑郁症是一种常见的精神健康问题，对个人幸福感和全球公共卫生产生负面影响。使用结构磁共振成像和深度学习方法自动检测MDD有望提高诊断准确性和实现早期干预。大多数现有方法使用体素级特征或基于预定义脑图谱构建的手工制作区域表征，限制了它们捕捉复杂脑部模式的能力。本文开发了一个统一流程，利用视觉Transformer从sMRI数据中提取3D区域嵌入，并使用图神经网络进行分类。我们探索了两种定义区域的方法：(1)使用预定义的结构性和功能性脑图谱的基于图谱方法，(2)基于立方体的方法，ViTs直接训练以从均匀提取的3D补丁中识别区域。此外，生成余弦相似度图来建模区域间关系，并指导基于GNN的分类。使用REST-meta-MDD数据集进行了广泛的实验以证明我们模型的有效性。通过分层10折交叉验证，最佳模型获得了78.98%的准确率、76.54%的敏感性、81.58%的特异性、81.58%的精确度和78.98%的F1分数。此外，基于图谱的模型始终优于基于立方体的方法，突显了使用特定领域的解剖先验进行MDD检测的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Major depressive disorder (MDD) is a prevalent mental health condition thatnegatively impacts both individual well-being and global public health.Automated detection of MDD using structural magnetic resonance imaging (sMRI)and deep learning (DL) methods holds increasing promise for improvingdiagnostic accuracy and enabling early intervention. Most existing methodsemploy either voxel-level features or handcrafted regional representationsbuilt from predefined brain atlases, limiting their ability to capture complexbrain patterns. This paper develops a unified pipeline that utilizes VisionTransformers (ViTs) for extracting 3D region embeddings from sMRI data andGraph Neural Network (GNN) for classification. We explore two strategies fordefining regions: (1) an atlas-based approach using predefined structural andfunctional brain atlases, and (2) an cube-based method by which ViTs aretrained directly to identify regions from uniformly extracted 3D patches.Further, cosine similarity graphs are generated to model interregionalrelationships, and guide GNN-based classification. Extensive experiments wereconducted using the REST-meta-MDD dataset to demonstrate the effectiveness ofour model. With stratified 10-fold cross-validation, the best model obtained78.98% accuracy, 76.54% sensitivity, 81.58% specificity, 81.58% precision, and78.98% F1-score. Further, atlas-based models consistently outperformed thecube-based approach, highlighting the importance of using domain-specificanatomical priors for MDD detection.</description>
      <author>example@mail.com (Nojod M. Alotaibi, Areej M. Alhothali, Manar S. Ali)</author>
      <guid isPermaLink="false">2509.12143v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Travel Time and Weather-Aware Traffic Forecasting in a Conformal Graph Neural Network Framework</title>
      <link>http://arxiv.org/abs/2509.12043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript has been accepted as a REGULAR PAPER in the  Transactions on Intelligent Transportation Systems 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的交通流预测框架，通过自适应邻接矩阵和天气因素调整来处理交通随机性，并使用自适应一致性预测进行不确定性量化，实验证明该方法具有更好的预测精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;交通流预测对管理拥堵、提高安全和优化交通系统至关重要，但由于城市交通的随机性和环境因素，这仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够适应由多个动态和复杂相互依赖因素影响的交通变异性的模型，以实现更好的预测。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络框架，使用对数正态分布和变异系数值的自适应邻接矩阵反映真实世界的行程时间变异性；通过温度、风速和降水等天气因素调整边权重，使GNN能够捕获交通站点之间不断变化的时空依赖关系；使用自适应一致性预测框架提供可靠的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与基线方法相比，所提出的模型显示出更好的预测精度和不确定性边界；在SUMO中构建交通场景并应用蒙特卡洛模拟推导测试车辆的行程时间分布，模拟的平均行程时间落在INRIX历史数据定义的区间内。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型能够有效适应交通随机性和变化的环境条件，验证了模型的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;交通流预测对于管理拥堵、提高安全和优化各种交通系统至关重要。然而，由于城市交通的随机性和环境因素，这仍然是一个普遍存在的挑战。更好的预测需要能够适应由多个动态和复杂相互依赖因素影响的交通变异性的模型。在这项工作中，我们提出了一种图神经网络框架，通过使用对数正态分布和变异系数值的自适应邻接矩阵来处理随机性，以反映真实世界的行程时间变异性。此外，温度、风速和降水等天气因素调整边权重，使GNN能够捕获交通站点之间不断变化的时空依赖关系。这种对静态邻接矩阵的增强使模型能够有效适应交通随机性和变化的环境条件。此外，我们使用自适应一致性预测框架提供可靠的不确定性量化，在实现目标覆盖率的同时保持可接受的预测区间。实验结果表明，与基线方法相比，所提出的模型显示出更好的预测精度和不确定性边界。然后，我们通过在SUMO中构建交通场景并应用蒙特卡洛模拟来推导测试车辆的行程时间分布，以反映真实世界的变异性，从而验证了该方法。测试车辆的模拟平均行程时间落在INRIX历史数据定义的区间内，验证了模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic flow forecasting is essential for managing congestion, improvingsafety, and optimizing various transportation systems. However, it remains aprevailing challenge due to the stochastic nature of urban traffic andenvironmental factors. Better predictions require models capable ofaccommodating the traffic variability influenced by multiple dynamic andcomplex interdependent factors. In this work, we propose a Graph Neural Network(GNN) framework to address the stochasticity by leveraging adaptive adjacencymatrices using log-normal distributions and Coefficient of Variation (CV)values to reflect real-world travel time variability. Additionally, weatherfactors such as temperature, wind speed, and precipitation adjust edge weightsand enable GNN to capture evolving spatio-temporal dependencies across trafficstations. This enhancement over the static adjacency matrix allows the model toadapt effectively to traffic stochasticity and changing environmentalconditions. Furthermore, we utilize the Adaptive Conformal Prediction (ACP)framework to provide reliable uncertainty quantification, achieving targetcoverage while maintaining acceptable prediction intervals. Experimentalresults demonstrate that the proposed model, in comparison with baselinemethods, showed better prediction accuracy and uncertainty bounds. We, then,validate this method by constructing traffic scenarios in SUMO and applyingMonte-Carlo simulation to derive a travel time distribution for a Vehicle UnderTest (VUT) to reflect real-world variability. The simulated mean travel time ofthe VUT falls within the intervals defined by INRIX historical data, verifyingthe model's robustness.</description>
      <author>example@mail.com (Mayur Patil, Qadeer Ahmed, Shawn Midlam-Mohler)</author>
      <guid isPermaLink="false">2509.12043v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Prior Observations for Incremental 3D Scene Graph Prediction</title>
      <link>http://arxiv.org/abs/2509.11895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at 24th International Conference on Machine Learning and  Applications (ICMLA'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于增量3D语义场景图预测的新型异构图模型，该模型通过消息传递过程整合多模态信息，无需完整场景重建即可灵活融合全局和局部场景表示。&lt;h4&gt;背景&lt;/h4&gt;3D语义场景图(3DSSG)通过显式建模对象、属性和关系为环境提供紧凑结构化表示，但现有方法主要依赖传感器数据，未充分整合语义丰富环境中的信息，且大多假设可访问完整场景重建，限制了在现实世界增量设置中的适用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型异构图模型，用于增量3DSSG预测，整合多模态信息到消息传递过程中，使其能在不依赖完整场景重建的情况下工作。&lt;h4&gt;方法&lt;/h4&gt;提出使用多层异构图模型，通过消息传递过程整合多模态信息如语义嵌入和先验观测，灵活融合全局和局部场景表示，无需专门模块或完整场景重建。&lt;h4&gt;主要发现&lt;/h4&gt;在3DSSG数据集上评估表明，通过多模态信息增强的图神经网络为复杂现实世界环境提供了可扩展且可推广的解决方案。&lt;h4&gt;结论&lt;/h4&gt;所提出的架构通过整合多模态信息，为增量3D语义场景图预测提供了有效方法，适用于现实世界的增量设置。&lt;h4&gt;翻译&lt;/h4&gt;3D语义场景图(3DSSG)通过显式建模对象、属性和关系，为环境提供紧凑的结构化表示。尽管3DSSG在机器人和具身AI中显示出潜力，但现有方法主要依赖传感器数据，没有进一步整合语义丰富环境中的信息。此外，大多数方法假设可以访问完整的场景重建，限制了它们在现实世界增量设置中的适用性。本文介绍了一种用于增量3DSSG预测的新型异构图模型，该模型通过消息传递过程直接整合了额外的多模态信息，如先验观测。利用多层，该模型灵活地融合全局和局部场景表示，而无需专门模块或完整场景重建。我们在3DSSG数据集上评估了我们的方法，表明通过多模态信息（如语义嵌入（例如CLIP）和先验观测）增强的图神经网络为复杂现实世界环境提供了一种可扩展且可推广的解决方案。所提出架构的完整源代码将在https://github.com/m4renz/incremental-scene-graph-prediction上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D语义场景图(3DSSG)预测中两个关键问题：一是现有方法主要依赖传感器数据，没有充分利用环境中丰富的语义信息；二是大多数方法假设可以访问完整的场景重建，这在现实世界中是不切实际的。这个问题很重要，因为3DSSG在机器人和具身AI领域潜力巨大，能提供环境的紧凑结构化表示，而实际应用中场景通常是从传感器数据流逐步捕获的，需要模型能够利用先前观察的信息来预测新输入。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，发现它们缺乏通用的信息整合机制且不适用于增量场景生成。作者借鉴了SceneGraphFusion的增量生成思想和Feng等人的历史预测整合方法，但改进了这些方法。作者设计了一个多层异构图模型，融合全局和局部信息，将先前观察直接整合到消息传递过程中，而不是像现有方法那样只使用更新的几何信息或显式编码全局信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个两层异构图架构：全局场景图积累先前帧的观察和关系，局部场景图处理当前帧数据，并通过连接匹配的对象实例使信息在两层间流动。整体流程是：1)预处理RGB-D帧数据；2)构建图模型，包括对象分割、点云转换和边连接；3)提取节点特征，包括点云采样和几何描述符计算；4)使用异构图神经网络进行消息传递；5)通过多层感知机进行节点和边分类预测；6)将预测结果合并到全局图中供后续使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新型异构图模型整合多模态信息用于增量预测；2)多层架构灵活整合全局和局部场景表示；3)将先前观察直接嵌入消息传递过程；4)展示对错误预测的鲁棒性；5)能无缝整合额外信息而无需修改核心架构。相比之前工作，不同之处在于：与SceneGraphFusion相比，直接整合先前的预测；与Feng等人的方法相比，不显式编码全局信息；与其他多模态方法相比，首次应用于增量3DSSG生成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的异构图模型，通过整合先前观察的多模态信息，实现了高效的增量3D场景图预测，显著提升了机器人在复杂环境中的感知和理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D semantic scene graphs (3DSSG) provide compact structured representationsof environments by explicitly modeling objects, attributes, and relationships.While 3DSSGs have shown promise in robotics and embodied AI, many existingmethods rely mainly on sensor data, not integrating further information fromsemantically rich environments. Additionally, most methods assume access tocomplete scene reconstructions, limiting their applicability in real-world,incremental settings. This paper introduces a novel heterogeneous graph modelfor incremental 3DSSG prediction that integrates additional, multi-modalinformation, such as prior observations, directly into the message-passingprocess. Utilizing multiple layers, the model flexibly incorporates global andlocal scene representations without requiring specialized modules or full scenereconstructions. We evaluate our approach on the 3DSSG dataset, showing thatGNNs enriched with multi-modal information such as semantic embeddings (e.g.,CLIP) and prior observations offer a scalable and generalizable solution forcomplex, real-world environments. The full source code of the presentedarchitecture will be made available athttps://github.com/m4renz/incremental-scene-graph-prediction.</description>
      <author>example@mail.com (Marian Renz, Felix Igelbrink, Martin Atzmueller)</author>
      <guid isPermaLink="false">2509.11895v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Descriptor and Graph-based Molecular Representations in Prediction of Copolymer Properties Using Machine Learning</title>
      <link>http://arxiv.org/abs/2509.11874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究使用机器学习方法预测共聚物的物理性质，通过两种不同的分子表示方法建立了预测模型，并发现不同表示方法对不同类型的物理性质预测效果不同。&lt;h4&gt;背景&lt;/h4&gt;共聚物是高度通用的材料，具有广泛的化学成分可能性。通过计算方法预测性质可以加速共聚物设计，优先选择具有有利性质的候选材料。&lt;h4&gt;目的&lt;/h4&gt;利用两种不同的分子集合表示方法，通过机器学习预测共聚物的七种不同物理性质，并比较不同表示方法的预测效果。&lt;h4&gt;方法&lt;/h4&gt;1. 使用随机森林模型从分子描述符预测聚合物性质；2. 使用图神经网络从2D聚合物图预测相同性质，包括单任务和多任务设置；3. 构建了一个包含140种具有不同单体组成和构型的二元共聚物的分子动力学模拟数据集；4. 训练和评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基于描述符的随机森林模型在预测密度和定压/定容比热容方面表现出色，因为这些性质与分子描述符捕获的特定分子特征密切相关；2. 图表示方法更好地预测膨胀系数和体积模量，因为这些性质更依赖于图模型更好地捕获的复杂结构相互作用。&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了选择适当表示方法预测分子性质的重要性。研究结果展示了机器学习模型如何通过可学习的结构-性质关系加速共聚物发现，简化聚合物设计，促进高性能材料在多种应用中的开发。&lt;h4&gt;翻译&lt;/h4&gt;共聚物是高度通用的材料，具有广泛的化学成分可能性。通过使用计算方法进行性质预测，可以加速共聚物设计，优先选择具有有利性质的候选材料。在本研究中，我们利用分子集合的两种不同表示方法，通过机器学习预测共聚物的七种不同物理性质：我们使用随机森林模型从分子描述符预测聚合物性质，使用图神经网络从2D聚合物图预测相同性质，包括单任务和多任务设置。为了训练和评估模型，我们构建了一个数据集，包含来自分子动力学模拟的140种具有不同单体组成和构型的二元共聚物。我们的结果表明，基于描述符的随机森林在预测密度和定压/定容比热容方面表现出色，因为这些性质与分子描述符捕获的特定分子特征密切相关。相比之下，图表示方法更好地预测膨胀系数和体积模量，因为这些性质更依赖于图模型更好地捕获的复杂结构相互作用。这项研究强调了选择适当表示方法预测分子性质的重要性。我们的研究展示了机器学习模型如何通过可学习的结构-性质关系加速共聚物发现，简化聚合物设计，促进高性能材料在多种应用中的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Copolymers are highly versatile materials with a vast range of possiblechemical compositions. By using computational methods for property prediction,the design of copolymers can be accelerated, allowing for the prioritization ofcandidates with favorable properties. In this study, we utilized two distinctrepresentations of molecular ensembles to predict the seven different physicalpolymer properties copolymers using machine learning: we used a random forest(RF) model to predict polymer properties from molecular descriptors, and agraph neural network (GNN) to predict the same properties from 2D polymergraphs under both a single- and multi-task setting. To train and evaluate themodels, we constructed a data set from molecular dynamic simulations for 140binary copolymers with varying monomer compositions and configurations. Ourresults demonstrate that descriptors-based RFs excel at predicting density andspecific heat capacities at constant pressure (Cp) and volume (Cv) becausethese properties are strongly tied to specific molecular features captured bymolecular descriptors. In contrast, graph representations better predictexpansion coefficients ({\gamma}, {\alpha}) and bulk modulus (K), which dependmore on complex structural interactions better captured by graph-based models.This study underscores the importance of choosing appropriate representationsfor predicting molecular properties. Our findings demonstrate how machinelearning models can expedite copolymer discovery with learnablestructure-property relationships, streamlining polymer design and advancing thedevelopment of high-performance materials for diverse applications.</description>
      <author>example@mail.com (Elaheh Kazemi-Khasragh, Rocío Mercado, Carlos Gonzalez, Maciej Haranczyk)</author>
      <guid isPermaLink="false">2509.11874v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Visualization and Analysis of the Loss Landscape in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入可学习的降维方法来可视化GNN损失景观，并分析了多种因素对GNN优化的影响，为开发更高效的GNN架构和训练策略提供了见解。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是处理图结构数据的强大模型，有广泛应用，但其参数优化、表达能力和泛化能力之间的相互作用仍理解不充分。&lt;h4&gt;目的&lt;/h4&gt;引入一种高效的、可学习的降维方法来可视化GNN损失景观，并分析过平滑、量化、稀疏化和预调节器对GNN优化的影响。&lt;h4&gt;方法&lt;/h4&gt;提出一种可学习的投影方法，优于基于PCA的最先进方法，能够以更低的内存使用量准确重建高维参数。&lt;h4&gt;主要发现&lt;/h4&gt;架构、稀疏化和优化器的预调节显著影响GNN优化景观，进而影响训练过程和最终预测性能。&lt;h4&gt;结论&lt;/h4&gt;这些发现有助于开发更高效的GNN架构设计和训练策略。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络是处理图结构数据的强大模型，有广泛应用。然而，GNN参数优化、表达能力和泛化能力之间的相互作用仍理解不充分。我们通过引入一种高效的、可学习的降维方法来可视化GNN损失景观，并分析了过平滑、跳跃知识、量化、稀疏化和预调节器对GNN优化的影响。我们的可学习投影方法优于基于PCA的最先进方法，能够以更低的内存使用量准确重建高维参数。我们进一步表明，架构、稀疏化和优化器的预调节显著影响GNN优化景观及其训练过程和最终预测性能。这些见解有助于开发更高效的GNN架构设计和训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-032-04552-2_9&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are powerful models for graph-structured data,with broad applications. However, the interplay between GNN parameteroptimization, expressivity, and generalization remains poorly understood. Weaddress this by introducing an efficient learnable dimensionality reductionmethod for visualizing GNN loss landscapes, and by analyzing the effects ofover-smoothing, jumping knowledge, quantization, sparsification, andpreconditioner on GNN optimization. Our learnable projection method surpassesthe state-of-the-art PCA-based approach, enabling accurate reconstruction ofhigh-dimensional parameters with lower memory usage. We further show thatarchitecture, sparsification, and optimizer's preconditioning significantlyimpact the GNN optimization landscape and their training process and finalprediction performance. These insights contribute to developing more efficientdesigns of GNN architectures and training strategies.</description>
      <author>example@mail.com (Samir Moustafa, Lorenz Kummer, Simon Fetzel, Nils M. Kriege, Wilfried N. Gansterer)</author>
      <guid isPermaLink="false">2509.11792v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Regression for Enzyme Turnover Rates Prediction</title>
      <link>http://arxiv.org/abs/2509.11782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures. This paper was withdrawn from the IJCAI 2025  proceedings due to the lack of participation in the conference and  presentation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态框架，用于预测酶转换率，整合了酶序列、底物结构和环境因素，实现了可解释且准确的预测。&lt;h4&gt;背景&lt;/h4&gt;酶转换率是酶动力学中的基本参数，反映了酶的催化效率。然而，由于实验测量的高成本和复杂性，大多数生物体中的酶转换率数据仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;解决酶转换率数据稀缺的问题，开发一种能够准确预测酶转换率的工具，为酶动力学研究和相关应用提供支持。&lt;h4&gt;方法&lt;/h4&gt;研究提出的多模态框架结合了预训练语言模型和卷积神经网络提取蛋白质序列特征，使用图神经网络捕获底物分子表示，并加入注意力机制增强酶和底物间的相互作用。同时利用Kolmogorov-Arnold网络的符号回归学习控制酶转换率的数学公式。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，该框架优于传统的和最先进的深度学习方法，为研究酶动力学提供了强大的工具。&lt;h4&gt;结论&lt;/h4&gt;这项工作为研究酶动力学提供了强大工具，在酶工程、生物技术和工业生物催化应用方面具有广阔前景。&lt;h4&gt;翻译&lt;/h4&gt;酶转换率是酶动力学中的一个基本参数，反映了酶的催化效率。然而，由于实验测量的高成本和复杂性，大多数生物体中的酶转换率数据仍然稀缺。为解决这一差距，我们提出了一种多模态框架来预测酶转换率，该框架整合了酶序列、底物结构和环境因素。我们的模型结合了预训练语言模型和卷积神经网络来提取蛋白质序列特征，同时图神经网络捕获底物分子的信息表示。加入了注意力机制以增强酶和底物表示之间的相互作用。此外，我们利用Kolmogorov-Arnold网络的符号回归来明确学习控制酶转换率的数学公式，实现了可解释且准确的预测。大量实验表明，我们的框架优于传统的和最先进的深度学习方法。这项工作为研究酶动力学提供了强大的工具，并在酶工程、生物技术和工业生物催化应用方面具有广阔前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The enzyme turnover rate is a fundamental parameter in enzyme kinetics,reflecting the catalytic efficiency of enzymes. However, enzyme turnover ratesremain scarce across most organisms due to the high cost and complexity ofexperimental measurements. To address this gap, we propose a multimodalframework for predicting the enzyme turnover rate by integrating enzymesequences, substrate structures, and environmental factors. Our model combinesa pre-trained language model and a convolutional neural network to extractfeatures from protein sequences, while a graph neural network capturesinformative representations from substrate molecules. An attention mechanism isincorporated to enhance interactions between enzyme and substraterepresentations. Furthermore, we leverage symbolic regression viaKolmogorov-Arnold Networks to explicitly learn mathematical formulas thatgovern the enzyme turnover rate, enabling interpretable and accuratepredictions. Extensive experiments demonstrate that our framework outperformsboth traditional and state-of-the-art deep learning approaches. This workprovides a robust tool for studying enzyme kinetics and holds promise forapplications in enzyme engineering, biotechnology, and industrial biocatalysis.</description>
      <author>example@mail.com (Bozhen Hu, Cheng Tan, Siyuan Li, Jiangbin Zheng, Sizhe Qiu, Jun Xia, Stan Z. Li)</author>
      <guid isPermaLink="false">2509.11782v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SpaPool: Soft Partition Assignment Pooling for__Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为SpaPool的新型池化方法，结合了密集和稀疏技术的优点，用于图神经网络处理。&lt;h4&gt;背景&lt;/h4&gt;图神经网络处理需要有效的池化方法来减少图的大小同时保持结构完整性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效减少图大小同时保持结构完整性的池化方法。&lt;h4&gt;方法&lt;/h4&gt;SpaPool通过将顶点分组为自适应数量的簇，结合密集和稀疏技术的优势来实现图的池化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的实验表明，SpaPool与现有池化技术相比具有竞争力，特别是在小规模图上表现优异。&lt;h4&gt;结论&lt;/h4&gt;SpaPool是一种有前途的方法，适用于需要高效有效图处理的应用。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SpaPool，一种新颖的池化方法，它结合了密集和稀疏技术的优点，用于图神经网络。SpaPool将顶点分组为自适应数量的簇，利用密集和稀疏方法的优势。它旨在在保持图结构完整性的同时高效减少其大小。在多个数据集上的实验结果表明，与现有的池化技术相比，SpaPool具有竞争力的性能，并特别在小规模图上表现出色。这使得SpaPool成为需要高效有效图处理的应用的一种有前途的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SpaPool, a novel pooling method that combines thestrengths of both dense and sparse techniques for a graph neural network.SpaPool groups vertices into an adaptive number of clusters, leveraging thebenefits of both dense and sparse approaches. It aims to maintain thestructural integrity of the graph while reducing its size efficiently.Experimental results on several datasets demonstrate that SpaPool achievescompetitive performance compared to existing pooling techniques and excelsparticularly on small-scale graphs. This makes SpaPool a promising method forapplications requiring efficient and effective graph processing.</description>
      <author>example@mail.com (Rodrigue Govan, Romane Scherrer, Philippe Fournier-Viger, Nazha Selmaoui-Folcher)</author>
      <guid isPermaLink="false">2509.11675v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Drug Repurposing Using Deep Embedded Clustering and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.11493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 2025 International Conference on Machine Learning and  Applications (ICMLA)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合无监督深度嵌入聚类和监督图神经网络的机器学习流程，用于从多组学数据中识别新的药物-疾病链接。&lt;h4&gt;背景&lt;/h4&gt;药物再利用在历史上是一个经济上不可行的过程，用于识别废弃药物的新用途。现代机器学习能够识别候选药物中的复杂生化细节，但许多研究依赖于具有已知药物-疾病相似性的简化数据集。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习流程，使用无监督深度嵌入聚类与监督图神经网络链接预测相结合，从多组学数据中识别新的药物-疾病链接。&lt;h4&gt;方法&lt;/h4&gt;使用无监督自编码器和聚类训练将组学数据的维度压缩为潜在嵌入。将9022种独特药物划分为35个簇，并使用图神经网络进行链接预测。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络实现了强大的统计性能，预测准确率为0.901，ROC曲线下面积为0.960，F1得分为0.901。生成了一个包含477个每个簇链接概率超过99%的排序列表。&lt;h4&gt;结论&lt;/h4&gt;这项研究可能在不同疾病领域提供新的药物-疾病链接前景，同时推进对药物再利用研究中机器学习的理解。&lt;h4&gt;翻译&lt;/h4&gt;药物再利用在历史上一直是识别废弃药物新用途的经济上不可行的过程。现代机器学习使得识别候选药物中的复杂生化细节成为可能；然而，许多研究依赖于具有已知药物-疾病相似性的简化数据集。我们提出了一种机器学习流程，使用无监督深度嵌入聚类，结合监督图神经网络链接预测，从多组学数据中识别新的药物-疾病链接。无监督自编码器和聚类训练将组学数据的维度压缩为压缩的潜在嵌入。总共9022种独特药物被划分为35个簇，平均轮廓得分为0.8550。图神经网络实现了强大的统计性能，预测准确率为0.901，受试者工作特征曲线下面积为0.960，F1得分为0.901。生成了一个包含477个每个簇链接概率超过99%的排序列表。这项研究可能在不同疾病领域提供新的药物-疾病链接前景，同时推进对药物再利用研究中机器学习的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug repurposing has historically been an economically infeasible process foridentifying novel uses for abandoned drugs. Modern machine learning has enabledthe identification of complex biochemical intricacies in candidate drugs;however, many studies rely on simplified datasets with known drug-diseasesimilarities. We propose a machine learning pipeline that uses unsuperviseddeep embedded clustering, combined with supervised graph neural network linkprediction to identify new drug-disease links from multi-omic data.Unsupervised autoencoder and cluster training reduced the dimensionality ofomic data into a compressed latent embedding. A total of 9,022 unique drugswere partitioned into 35 clusters with a mean silhouette score of 0.8550. Graphneural networks achieved strong statistical performance, with a predictionaccuracy of 0.901, receiver operating characteristic area under the curve of0.960, and F1-Score of 0.901. A ranked list comprised of 477 per-cluster linkprobabilities exceeding 99 percent was generated. This study could provide newdrug-disease link prospects across unrelated disease domains, while advancingthe understanding of machine learning in drug repurposing studies.</description>
      <author>example@mail.com (Luke Delzer, Robert Kroleski, Ali K. AlShami, Jugal Kalita)</author>
      <guid isPermaLink="false">2509.11493v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Networks: Trainable Quantum Encoders for Inductive Graph Learning</title>
      <link>http://arxiv.org/abs/2509.11390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了量子图注意力网络（QGATs）作为图上进行归纳学习的可训练量子编码器，扩展了量子图神经网络框架。QGATs通过量子注意力机制动态调节邻居节点的贡献，实现了具有局部感知能力的量子表示。&lt;h4&gt;背景&lt;/h4&gt;量子图神经网络（QGNN）框架为图数据提供了量子编码方法，但在处理复杂图结构时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效编码节点特征和邻域结构的量子神经网络，提高在图数据上的归纳学习能力，特别是在化学性质预测任务中。&lt;h4&gt;方法&lt;/h4&gt;利用参数化量子电路编码节点特征和邻域结构，通过动态学习的酉算子实现量子注意力机制，调节每个邻居节点的贡献。在QM9数据集上评估该方法，预测各种化学性质，并与经典和量子图神经网络进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;注意力机制在经典和量子图神经网络中都能提高性能；量子注意力随着图大小的增长而带来更大的益处；在较大分子图上，QGATs显著优于无注意力的量子对应模型；对于较小图，QGATs实现了与经典GAT模型相当的预测精度。&lt;h4&gt;结论&lt;/h4&gt;量子注意力机制有潜力增强量子图神经网络在化学及其他领域的归纳能力，QGATs作为表达性量子编码器具有可行性和优势。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了量子图注意力网络（QGATs）作为图上进行归纳学习的可训练量子编码器，扩展了量子图神经网络（QGNN）框架。QGATs利用参数化量子电路编码节点特征和邻域结构，通过量子注意力机制经由动态学习的酉算子调节每个邻居的贡献。这使得能够表达具有局部感知能力的量子表示，并能推广到未见过的图实例。我们在QM9数据集上评估了我们的方法，目标是预测各种化学性质。我们的实验比较了经典和量子图神经网络（有和没有注意力层），表明注意力在两种范式中都能提高性能。值得注意的是，我们观察到量子注意力随着图大小的增长而带来更大的益处，在较大的分子图上，QGATs显著优于无注意力的量子对应模型。此外，对于较小的图，QGATs实现了与经典GAT模型相当的预测精度，突显了它们作为表达性量子编码器的可行性。这些结果表明量子注意力机制有潜力增强QGNN在化学及其他领域的归纳能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Quantum Graph Attention Networks (QGATs) as trainable quantumencoders for inductive learning on graphs, extending the Quantum Graph NeuralNetworks (QGNN) framework. QGATs leverage parameterized quantum circuits toencode node features and neighborhood structures, with quantum attentionmechanisms modulating the contribution of each neighbor via dynamically learnedunitaries. This allows for expressive, locality-aware quantum representationsthat can generalize across unseen graph instances. We evaluate our approach onthe QM9 dataset, targeting the prediction of various chemical properties. Ourexperiments compare classical and quantum graph neural networks-with andwithout attention layers-demonstrating that attention consistently improvesperformance in both paradigms. Notably, we observe that quantum attentionyields increasing benefits as graph size grows, with QGATs significantlyoutperforming their non-attentive quantum counterparts on larger moleculargraphs. Furthermore, for smaller graphs, QGATs achieve predictive accuracycomparable to classical GAT models, highlighting their viability as expressivequantum encoders. These results show the potential of quantum attentionmechanisms to enhance the inductive capacity of QGNN in chemistry and beyond.</description>
      <author>example@mail.com (Arthur M. Faria, Mehdi Djellabi, Igor O. Sokolov, Savvas Varsamopoulos)</author>
      <guid isPermaLink="false">2509.11390v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>BIGNet: Pretrained Graph Neural Network for Embedding Semantic, Spatial, and Topological Data in BIM Models</title>
      <link>http://arxiv.org/abs/2509.11104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了首个大规模图神经网络BIGNet，用于学习和重用BIM模型中的多维设计特征，解决了现有模型忽略BIM中语义、空间和拓扑特征的问题。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型在土木工程中显示出显著优势，但主要关注文本和视觉数据，忽略了BIM模型中丰富的语义、空间和拓扑特征。&lt;h4&gt;目的&lt;/h4&gt;开发第一个大规模图神经网络（GNN）BIGNet，以学习和重用BIM模型中嵌入的多维设计特征。&lt;h4&gt;方法&lt;/h4&gt;引入可扩展的图表示编码BIM组件的'语义-空间-拓扑'特征，创建包含近100万节点和350万条边的数据集；通过向GraphMAE2引入新消息传递机制提出BIGNet，并使用节点掩蔽策略进行预训练；在基于BIM的设计检查任务中评估BIGNet。&lt;h4&gt;主要发现&lt;/h4&gt;1) 同构图在学习设计特征方面优于异构图；2) 考虑30厘米半径内的局部空间关系可以提高性能；3) 基于GAT的特征提取的BIGNet实现了最佳迁移学习结果。&lt;h4&gt;结论&lt;/h4&gt;BIGNet使平均F1分数比未预训练模型提高72.7%，证明了其在学习和转移BIM设计特征方面的有效性，促进了这些特征在设计和生命周期管理中的自动化应用。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型在土木工程中显示出显著优势，但它们主要关注文本和视觉数据，忽略了BIM（建筑信息建模）模型中丰富的语义、空间和拓扑特征。因此，本研究开发了第一个大规模图神经网络（GNN）BIGNet，以学习和重用BIM模型中嵌入的多维设计特征。首先，引入了一种可扩展的图表示来编码BIM组件的'语义-空间-拓扑'特征，并创建了一个包含近100万个节点和350万条边的数据集。随后，通过向GraphMAE2引入新的消息传递机制提出了BIGNet，并使用节点掩蔽策略进行了进一步预训练。最后，在基于BIM的设计检查的各种迁移学习任务中评估了BIGNet。结果表明：1）同构图在学习设计特征方面优于异构图；2）考虑30厘米半径内的局部空间关系可以提高性能；3）基于GAT（图注意力网络）的特征提取的BIGNet实现了最佳的迁移学习结果。这一创新使平均F1分数比未预训练的模型提高了72.7%，证明了它在学习和转移BIM设计特征方面的有效性，并促进了它们在未来的设计和生命周期管理中的自动化应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有大型基础模型忽略BIM模型中丰富的语义、空间和拓扑特征的问题，以及BIM数据表示方法缺乏通用性和标注数据稀缺的问题。这个问题在现实中非常重要，因为BIM是建筑行业数字化的重要工具，包含大量设计知识和经验；有效提取和重用这些知识可以提高设计质量，减少施工错误，降低成本，并提升建筑全生命周期管理效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了BIM模型特点和现有方法局限性，指出BIM包含复杂几何数据、非几何信息和关系数据，而深度学习需要结构化输入。作者借鉴了图神经网络处理非欧几里得数据的能力，基于GraphMAE2架构进行改进，引入新的消息传递机制。同时利用预训练和迁移学习技术解决标注数据稀缺问题，并扩展了统一的基于网络的表示方法，以支持更全面的特征编码。整个设计过程体现了对现有工作的批判性继承和创新性发展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络表示和处理BIM模型中的语义、空间和拓扑特征，通过预训练学习隐式设计知识，并利用迁移学习应用于多种下游任务。整体流程分为三步：1) 开发BIM特定的可扩展图表示方法，将BIM模型转换为图结构，提取并编码组件的多维特征；2) 基于改进的GraphMAE2架构，使用节点掩码策略(50%掩码率)进行自监督预训练，引入新的消息传递机制；3) 在三种BIM设计检查任务(语义冲突、数据范围错误和拓扑错误)中评估模型，使用混淆矩阵和F1分数等指标衡量性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个大规模预训练图神经网络(BIGNet)用于学习BIM中的多维特征；2) BIM特定的可扩展图表示方法，统一编码语义、空间和拓扑特征；3) 改进的预训练策略，引入新的消息传递机制；4) 多任务设计检查框架。相比之前的工作，不同之处在于：现有方法主要针对特定任务设计特征提取，缺乏通用性；需要大量标注数据，而BIGNet通过预训练减少标注需求；通常只处理单一错误类型，而BIGNet支持多任务检查；图结构设计更灵活，可根据任务需求调整。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BIGNet通过创新的BIM图表示方法和预训练图神经网络，有效提取和重用了建筑信息模型中的语义、空间和拓扑设计知识，实现了多任务自动化设计检查，显著提高了建筑设计的质量和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Foundation Models (LFMs) have demonstrated significant advantages incivil engineering, but they primarily focus on textual and visual data,overlooking the rich semantic, spatial, and topological features in BIM(Building Information Modelling) models. Therefore, this study develops thefirst large-scale graph neural network (GNN), BIGNet, to learn, and reusemultidimensional design features embedded in BIM models. Firstly, a scalablegraph representation is introduced to encode the "semantic-spatial-topological"features of BIM components, and a dataset with nearly 1 million nodes and 3.5million edges is created. Subsequently, BIGNet is proposed by introducing a newmessage-passing mechanism to GraphMAE2 and further pretrained with a nodemasking strategy. Finally, BIGNet is evaluated in various transfer learningtasks for BIM-based design checking. Results show that: 1) homogeneous graphrepresentation outperforms heterogeneous graph in learning design features, 2)considering local spatial relationships in a 30 cm radius enhances performance,and 3) BIGNet with GAT (Graph Attention Network)-based feature extractionachieves the best transfer learning results. This innovation leads to a 72.7%improvement in Average F1-score over non-pretrained models, demonstrating itseffectiveness in learning and transferring BIM design features and facilitatingtheir automated application in future design and lifecycle management.</description>
      <author>example@mail.com (Jin Han, Xin-Zheng Lu, Jia-Rui Lin)</author>
      <guid isPermaLink="false">2509.11104v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>CogGNN: Cognitive Graph Neural Networks in Generative Connectomics</title>
      <link>http://arxiv.org/abs/2509.10864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CogGNN，第一个认知生成模型，使图神经网络具有认知能力，能够生成保留认知特征的脑网络，并在连接性脑模板学习任务中表现优异。&lt;h4&gt;背景&lt;/h4&gt;生成式学习已推进网络神经科学，支持图超分辨率、时间图预测和多模态脑图融合等任务，但当前基于图神经网络的方法仅关注结构和拓扑特性，忽略了认知特征。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够整合认知特征的生成模型，特别是视觉输入，以生成在认知和结构上都有意义的脑网络和连接性脑模板。&lt;h4&gt;方法&lt;/h4&gt;提出CogGNN，一种认知感知的生成模型，具有基于视觉记忆的损失函数，以及一种连接性脑模板学习框架，采用共同优化策略产生居中良好、可区分且认知增强的模板。&lt;h4&gt;主要发现&lt;/h4&gt;CogGNN生成的连接性脑模板在认知和结构上都有意义，且大量实验表明CogGNN优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CogGNN为认知基础的脑网络建模奠定了坚实基础，是第一个将认知能力整合到图神经网络中的生成模型。&lt;h4&gt;翻译&lt;/h4&gt;生成式学习已推进网络神经科学，使图超分辨率、时间图预测和多模态脑图融合等任务成为可能。然而，当前主要基于图神经网络(GNNs)的方法仅关注结构和拓扑特性，忽略了认知特征。为此，我们引入了第一个认知生成模型CogGNN，它赋予GNN认知能力（如视觉记忆），以生成保留认知特征的脑网络。虽然具有广泛适用性，我们提出了CogGNN，这是一种特殊变体，旨在整合视觉输入，这是大脑功能（如图案识别和记忆回忆）的关键因素。作为概念验证，我们使用该模型学习连接性脑模板(CBTs)，这是来自多视图脑网络的群体级指纹。与之前忽略认知属性的工作不同，CogGNN生成的CBT在认知和结构上都有意义。我们的贡献是：(i) 一种新颖的认知感知生成模型，具有基于视觉记忆的损失函数；(ii) 一种CBT学习框架，采用共同优化策略，产生居中良好、可区分且认知增强的模板。大量实验表明，CogGNN优于最先进的方法，为认知基础的脑网络建模奠定了坚实基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative learning has advanced network neuroscience, enabling tasks likegraph super-resolution, temporal graph prediction, and multimodal brain graphfusion. However, current methods, mainly based on graph neural networks (GNNs),focus solely on structural and topological properties, neglecting cognitivetraits. To address this, we introduce the first cognified generative model,CogGNN, which endows GNNs with cognitive capabilities (e.g., visual memory) togenerate brain networks that preserve cognitive features. While broadlyapplicable, we present CogGNN, a specific variant designed to integrate visualinput, a key factor in brain functions like pattern recognition and memoryrecall. As a proof of concept, we use our model to learn connectional braintemplates (CBTs), population-level fingerprints from multi-view brain networks.Unlike prior work that overlooks cognitive properties, CogGNN generates CBTsthat are both cognitively and structurally meaningful. Our contributions are:(i) a novel cognition-aware generative model with a visual-memory-based loss;(ii) a CBT-learning framework with a co-optimization strategy to yieldwell-centered, discriminative, cognitively enhanced templates. Extensiveexperiments show that CogGNN outperforms state-of-the-art methods, establishinga strong foundation for cognitively grounded brain network modeling.</description>
      <author>example@mail.com (Mayssa Soussia, Yijun Lin, Mohamed Ali Mahjoub, Islem Rekik)</author>
      <guid isPermaLink="false">2509.10864v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Context--Aware Inputs: Physics--Inspired Improvements in Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了物理信息策略以提高图神经网络在液氩时间投影室事件重建任务中的性能，特别是针对代表性不足的粒子类别如米歇尔电子。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在液氩时间投影室的事件重建任务中显示出强大潜力，但对于米歇尔电子等代表性不足的粒子类别性能仍然有限。&lt;h4&gt;目的&lt;/h4&gt;研究基于物理信息的策略，以改进NuGraph2架构中的语义分割性能。&lt;h4&gt;方法&lt;/h4&gt;探索三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器捕获类级别相关性；(iii)整合基于能量的正则化项，受米歇尔电子能量分布启发。&lt;h4&gt;主要发现&lt;/h4&gt;物理启发的特征增强带来最大收益，特别是通过解纠缠重叠潜在空间区域提高米歇尔电子精确率和召回率；辅助解码器和能量正则化项改进有限，部分原因是NuGraph2缺乏明确的粒子或事件级别表示。&lt;h4&gt;结论&lt;/h4&gt;将物理上下文直接嵌入到节点级输入中比施加特定任务的辅助损失更有效；具有明确粒子级和事件级推理的分层架构如NuGraph3将为高级解码器和基于物理的正则化提供更自然的环境。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近在液氩时间投影室的事件重建任务中显示出强大的前景，然而对于代表性不足的粒子类别，如米歇尔电子，其性能仍然有限。在这项工作中，我们研究基于物理信息的策略以改进NuGraph2架构中的语义分割。我们探索了三种互补方法：(i)通过探测器几何和轨迹连续性派生的上下文感知特征丰富输入表示；(ii)引入辅助解码器以捕获类级别相关性；(iii)整合受米歇尔电子能量分布启发的基于能量的正则化项。在MicroBooNE公共数据集上的实验表明，物理启发的特征增强带来最大收益，特别是在通过解纠缠重叠的潜在空间区域显著提高米歇尔电子的精确率和召回率。相比之下，辅助解码器和能量正则化项提供的改进有限，部分原因是NuGraph2的命中级别性质，缺乏明确的粒子或事件级别表示。我们的发现强调将物理上下文直接嵌入到节点级输入中比施加特定任务的辅助损失更有效，并表明未来的分层架构如NuGraph3，具有明确的粒子级和事件级推理，将为高级解码器和基于物理的正则化提供更自然的环境。本工作的代码已在Github上公开：https://github.com/vitorgrizzi/nugraph_phys/tree/main_phys。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决图神经网络在液态氩时间投影室(LArTPC)粒子事件重建中对代表性不足的粒子类别(如Michel电子)分类性能不佳的问题。这个问题很重要，因为Michel电子等稀有粒子的准确识别对粒子物理实验至关重要，它们提供了关于粒子行为和相互作用的关键信息，而当前模型对这些稀有粒子的识别能力有限，影响了科学家对基本粒子行为的理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了图神经网络在粒子物理任务中的局限性，特别是对稀有类别识别不足的问题。他们借鉴了三种主要策略：通过输入特征注入物理知识、使用辅助解码器捕获类别间相关性、在损失函数中加入基于物理的约束项。作者参考了多项现有工作，如Drielsma等人的GrapPA方法引入几何描述符，Kiesler等人结合CNN与物理特征，以及DUNE协作的CVN多任务学习框架，这些工作启发了作者将物理学知识深度融入神经网络的不同方式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在图神经网络的输入表示中直接嵌入物理上下文来增强模型表达能力，特别是改善对稀有粒子的识别，而不是仅依赖辅助损失函数来强制执行物理约束。整体实现包括三个主要步骤：1)特征扩展：在原始四个特征基础上添加节点度、最近邻距离和双差分特征，编码结构和关系上下文；2)添加额外解码器：引入预测事件中粒子类别分布的解码器，利用类别间相关性；3)Michel能量正则化：在损失函数中加入基于物理的约束项，惩罚偏离预期能量分布的分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)设计特定的物理启发特征，直接编码节点结构和关系上下文；2)使用全局注意力机制实现有效的图级表示；3)基于Michel电子能量分布特性设计特定正则化项。相比之前的工作，本研究强调在输入表示中嵌入物理上下文的重要性，而非仅通过损失函数强制执行物理约束；专注于语义分割而非粒子聚类；针对稀有类别识别而非多任务学习；将物理知识注入图神经网络的特定组件而非直接强制执行物理方程。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过在图神经网络的输入表示中融入物理学启发的上下文感知特征，显著提高了对稀有粒子类别(如Michel电子)的语义分割性能，证明了直接在节点级嵌入物理知识比通过辅助损失函数强制执行物理约束更有效。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have recently shown strong promise for eventreconstruction tasks in Liquid Argon Time Projection Chambers, yet theirperformance remains limited for underrepresented classes of particles, such asMichel electrons. In this work, we investigate physics-informed strategies toimprove semantic segmentation within the NuGraph2 architecture. We explorethree complementary approaches: (i) enriching the input representation withcontext-aware features derived from detector geometry and track continuity,(ii) introducing auxiliary decoders to capture class-level correlations, and(iii) incorporating energy-based regularization terms motivated by Michelelectron energy distributions. Experiments on MicroBooNE public datasets showthat physics-inspired feature augmentation yields the largest gains,particularly boosting Michel electron precision and recall by disentanglingoverlapping latent space regions. In contrast, auxiliary decoders andenergy-regularization terms provided limited improvements, partly due to thehit-level nature of NuGraph2, which lacks explicit particle- or event-levelrepresentations. Our findings highlight that embedding physics context directlyinto node-level inputs is more effective than imposing task-specific auxiliarylosses, and suggest that future hierarchical architectures such as NuGraph3,with explicit particle- and event-level reasoning, will provide a more naturalsetting for advanced decoders and physics-based regularization. The code forthis work is publicly available on Github athttps://github.com/vitorgrizzi/nugraph_phys/tree/main_phys.</description>
      <author>example@mail.com (Vitor F. Grizzi, Margaret Voetberg, Giuseppe Cerati, Hadi Meidani)</author>
      <guid isPermaLink="false">2509.10684v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>NuGraph2 with Explainability: Post-hoc Explanations for Geometric Neural Network Predictions</title>
      <link>http://arxiv.org/abs/2509.10676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了人工智能在科学应用中的可解释性问题，为现有的中微子标记图神经网络NuGraph2引入了解释性附加组件，展示了多种解释性技术如何结合使用以揭示AI模型的推理过程。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能在科学应用中的日益普及，将结果归因于网络的推理过程变得至关重要，这对于支持稳健的科学泛化具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;强调并展示事后解释性方法在科学应用中AI方法上的需求，并通过具体案例演示其应用价值。&lt;h4&gt;方法&lt;/h4&gt;为NuGraph2图神经网络引入一系列解释性技术，包括检查网络输出（节点分类）、分析节点间的边连接关系，以及使用新开发的通用工具探测潜在空间。这些方法不仅限于图神经网络，可应用于广泛的网络类型。&lt;h4&gt;主要发现&lt;/h4&gt;单独使用任何一种解释方法都不足以展示网络的'理解'能力，但将这些方法结合使用可以提供对分类过程中所使用方法的深入见解。&lt;h4&gt;结论&lt;/h4&gt;虽然这些解释性方法在NuGraph2应用上进行了测试，但它们具有广泛的适用性，可应用于各类神经网络。相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;随着人工智能在科学应用中日益普及，将结果归因于网络推理过程对于支持稳健的科学泛化来说需求很高。在本工作中，我们旨在强调并展示事后解释性方法在科学应用中AI方法上的应用需求。为此，我们为现有的用于中微子标记的图神经网络NuGraph2引入了解释性附加组件。这些解释采用一系列技术形式，包括检查网络输出（节点分类）和它们之间的边连接关系，以及使用应用于该网络的新开发通用工具探测潜在空间。我们展示了这些方法中的任何单独一种都不足以展示网络的'理解'能力，但结合起来可以提供对分类过程中所使用方法的见解。虽然这些方法在NuGraph2应用上进行了测试，但它们可以应用于广泛的网络，不仅限于图神经网络。本工作的代码已在GitHub上公开，网址为https://github.com/voetberg/XNuGraph。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the growing popularity of artificial intelligence used for scientificapplications, the ability of attribute a result to a reasoning process from thenetwork is in high demand for robust scientific generalizations to hold. Inthis work we aim to motivate the need for and demonstrate the use of post-hocexplainability methods when applied to AI methods used in scientificapplications. To this end, we introduce explainability add-ons to the existinggraph neural network (GNN) for neutrino tagging, NuGraph2. The explanationstake the form of a suite of techniques examining the output of the network(node classifications) and the edge connections between them, and probing ofthe latent space using novel general-purpose tools applied to this network. Weshow how none of these methods are singularly sufficient to show network"understanding", but together can give insights into the processes used inclassification. While these methods are tested on the NuGraph2 application,they can be applied to a broad range of networks, not limited to GNNs. The codefor this work is publicly available on GitHub athttps://github.com/voetberg/XNuGraph.</description>
      <author>example@mail.com (Margaret Voetberg, Vitor F. Grizzi, Giuseppe Cerati, Hadi Meidani)</author>
      <guid isPermaLink="false">2509.10676v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>M4GN: Mesh-based Multi-segment Hierarchical Graph Network for Dynamic Simulations</title>
      <link>http://arxiv.org/abs/2509.10659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and published in Transactions on Machine Learning Research  (TMLR), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为M4GN的三层段中心分层网络，用于解决基于网格的图神经网络在PDE模拟中面临的高成本、过度平滑以及构建粗图和保持细粒度精度的问题。&lt;h4&gt;背景&lt;/h4&gt;基于网格的图神经网络已成为PDE模拟的有效替代方法，但深度消息传递在大规模、长距离网格上成本高且存在过度平滑问题；分层GNN虽缩短了传播路径，但仍面临构建尊重网格拓扑、几何和物理不连续性的粗图，以及保持细粒度精度而不牺牲粗化带来的速度优势两大障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡精度和效率的图神经网络架构，解决现有方法在处理大规模网格时的高计算成本和过度平滑问题，同时保持细粒度精度。&lt;h4&gt;方法&lt;/h4&gt;提出M4GN，采用混合分割策略结合快速图分割器和超像素式细化，产生动态一致节点的连续段；使用排列不变聚合器编码这些段，避免顺序敏感性和二次成本；通过微观级GNN捕获局部动力学，宏观级transformer跨段高效推理，实现精度与效率的平衡。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性基准数据集上评估，M4GN将预测准确性提高了高达56%，同时比最先进的基线快达22%。&lt;h4&gt;结论&lt;/h4&gt;M4GN通过创新的分层架构和混合分割策略，有效解决了现有图神经网络在PDE模拟中的关键挑战，实现了精度和效率的显著提升。&lt;h4&gt;翻译&lt;/h4&gt;基于网格的图神经网络已成为PDE模拟的有效替代方法，但其深度消息传递在大规模、长距离网格上成本高且过度平滑；分层GNN缩短了传播路径但仍面临两个关键障碍：(i)构建尊重网格拓扑、几何和物理不连续性的粗图，以及(ii)保持细粒度精度而不牺牲粗化带来的速度优势。我们通过M4GN应对这些挑战，这是一种三层、段中心的分层网络。M4GN采用混合分割策略，将快速图分割器与由模态分解特征引导的超像素式细化配对，产生动态一致节点的连续段。这些段通过排列不变聚合器进行编码，避免了先前工作中使用的聚合方法的顺序敏感性和二次成本。得到的信息连接了捕获局部动力学的微观级GNN和跨段高效推理的宏观级transformer，实现了精度和效率之间的原则性平衡。在多个代表性基准数据集上评估，M4GN将预测准确性提高了高达56%，同时比最先进的基线快达22%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mesh-based graph neural networks (GNNs) have become effective surrogates forPDE simulations, yet their deep message passing incurs high cost andover-smoothing on large, long-range meshes; hierarchical GNNs shortenpropagation paths but still face two key obstacles: (i) building coarse graphsthat respect mesh topology, geometry, and physical discontinuities, and (ii)maintaining fine-scale accuracy without sacrificing the speed gained fromcoarsening. We tackle these challenges with M4GN, a three-tier, segment-centrichierarchical network. M4GN begins with a hybrid segmentation strategy thatpairs a fast graph partitioner with a superpixel-style refinement guided bymodal-decomposition features, producing contiguous segments of dynamicallyconsistent nodes. These segments are encoded by a permutation-invariantaggregator, avoiding the order sensitivity and quadratic cost of aggregationapproaches used in prior works. The resulting information bridges a micro-levelGNN, which captures local dynamics, and a macro-level transformer that reasonsefficiently across segments, achieving a principled balance between accuracyand efficiency. Evaluated on multiple representative benchmark datasets, M4GNimproves prediction accuracy by up to 56% while achieving up to 22% fasterinference than state-of-the-art baselines.</description>
      <author>example@mail.com (Bo Lei, Victor M. Castillo, Yeping Hu)</author>
      <guid isPermaLink="false">2509.10659v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Hetero-EUCLID: Interpretable model discovery for heterogeneous hyperelastic materials using stress-unsupervised learning</title>
      <link>http://arxiv.org/abs/2509.11784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Hetero-EUCLID的计算框架，用于异质材料的分割和参数识别，以表征所有组分的完整超弹性行为。&lt;h4&gt;背景&lt;/h4&gt;异质材料的完整力学行为表征是一个挑战性问题，需要能够识别不同材料组分及其力学特性的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够分割异质材料域并识别各组分材料本构参数的计算框架，以实现完整的超弹性行为表征。&lt;h4&gt;方法&lt;/h4&gt;基于Bayesian-EUCLID框架，使用稀疏促进先验和马尔可夫链蒙特卡罗采样解决异构化公式。框架包括两个主要步骤：基于残余力的分割和本构参数识别。使用有限元模拟生成的三维表面位移和边界平均力数据作为输入。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够成功分割各种类型的薄方形异质域，并表征不同组分的材料特性。即使在存在位移噪声和非原生网格离散化的情况下，框架也能保持有效。基于单次实验数据即可完成分割和材料表征。&lt;h4&gt;结论&lt;/h4&gt;Hetero-EUCLID框架在基于数字图像/体积相关的实验场景中具有适用性，为航空航天和国防复合材料等领域的快速、可解释的模型发现提供了工具，同时也适用于医疗领域如纤维动脉粥样硬化、动脉粥样硬化或癌症中的选择性组织硬化表征。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种计算框架Hetero-EUCLID，用于分割和参数识别，以表征异质材料所有组分的完整超弹性行为。在这项工作中，我们利用Bayesian-EUCLID（高效无监督本构定律识别和发现）框架，通过使用稀疏促进先验和马尔可夫链蒙特卡罗采样进行简约模型选择，有效地解决了异构化公式。我们使用了从异质试样非等双轴拉伸试验的有限元模拟中生成的实验可观测的三维表面位移和边界平均力数据。该框架 broadly包括两个步骤——基于残余力的分割和本构参数识别。我们验证并展示了所提出框架分割域和表征组分材料在各种类型薄方形异质域上的能力。我们验证了该框架在不同级别位移噪声和非原生网格离散化情况下的分割和表征材料的能力，即使用不同的网格进行前向FE模拟和逆EUCLID问题。这证明了Hetero-EUCLID框架在基于数字图像/体积相关的实验场景中的适用性。此外，所提出的框架基于单次实验数据成功进行了分割和材料表征，使其在航空航天和国防复合材料等领域的快速、可解释的模型发现，以及在纤维动脉粥样硬化、动脉粥样硬化或癌症等医疗状况中选择性组织硬化表征方面具有可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a computational framework, Hetero-EUCLID, for segmentation andparameter identification to characterize the full hyperelastic behavior of allconstituents of a heterogeneous material. In this work, we leverage theBayesian-EUCLID (Efficient Unsupervised Constitutive Law Identification andDiscovery) framework to efficiently solve the heterogenized formulation throughparsimonious model selection using sparsity-promoting priors and Monte CarloMarkov Chain sampling. We utilize experimentally observable 3D surfacedisplacement and boundary-averaged force data generated from Finite Elementsimulations of non-equi-biaxial tension tests on heterogeneous specimens. Theframework broadly consists of two steps -- residual force-based segmentation,and constitutive parameter identification. We validate and demonstrate theability of the proposed framework to segment the domain, and characterize theconstituent materials on various types of thin square heterogeneous domains. Wevalidate of the framework's ability to segment and characterize materials withvarious levels of displacement noises and non-native mesh discretizations, i.e,using different meshes for the forward FE simulations and the inverse EUCLIDproblem. This demonstrates Hetero-EUCLID framework's applicability in DigitalImage/Volume Correlation-based experimental scenarios. Furthermore, theproposed framework performs successful segmentation and materialcharacterizations based on data from a single experiment, thereby making itviable for rapid, interpretable model discovery in domains such as aerospaceand defense composites and for characterization of selective tissue stiffeningin medical conditions such as fibroatheroma, atherosclerosis, or cancer.</description>
      <author>example@mail.com (Kanhaiya Lal Chaurasiya, Saurav Dutta, Siddhant Kumar, Akshay Joshi)</author>
      <guid isPermaLink="false">2509.11784v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>The Quest for Universal Master Key Filters in DS-CNNs</title>
      <link>http://arxiv.org/abs/2509.11711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文扩展了卷积神经网络的'主键过滤器假设'，发现深度可分离卷积网络固有收敛到一组仅8个通用过滤器，这些过滤器在图像处理中具有基础性作用。&lt;h4&gt;背景&lt;/h4&gt;最近的研究提出了卷积神经网络过滤器的'主键过滤器假设'，本文进一步扩展了这一假设。&lt;h4&gt;目的&lt;/h4&gt;将假设范围严格限制到一组仅8个通用过滤器，探究深度可分离卷积网络(DS-CNN)的固有收敛特性。&lt;h4&gt;方法&lt;/h4&gt;通过系统性的无监督搜索，从不同架构和数据集中提取这些基本模式。&lt;h4&gt;主要发现&lt;/h4&gt;传统DS-CNN使用的数千个过滤器主要是这8个通用集合的线性移位；用这8个冻结过滤器初始化的网络在ImageNet上达到超过80%的准确率；这些过滤器与高斯差、高斯及其导数相似，与哺乳动物视觉系统感受野相似；深度卷积层自然倾向于这组基本空间算子。&lt;h4&gt;结论&lt;/h4&gt;深度卷积层自然地倾向于这组基本的空间算子，无论任务或架构如何。这些主键过滤器为理解泛化和迁移学习提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究提出了卷积神经网络过滤器的'主键过滤器假设'。本文通过将假设范围严格限制到一组仅8个通用过滤器来扩展这一假设，这些是深度可分离卷积网络固有收敛到的。虽然传统DS-CNN使用数千个不同的训练过滤器，但我们的分析显示这些过滤器主要是我们发现的通用集合的线性移位。通过系统性的无监督搜索，我们从不同架构和数据集中提取了这些基本模式。值得注意的是，用这8个独特的冻结过滤器初始化的网络在ImageNet上获得超过80%的准确率，甚至在应用于较小数据集时优于具有数千个可训练参数的模型。识别出的主键过滤器与高斯差、高斯及其导数非常匹配，这些结构不仅是经典图像处理的基础，而且与哺乳动物视觉系统中的感受野惊人地相似。我们的发现提供了有力证据，表明深度卷积层自然地倾向于这组基本的空间算子，无论任务或架构如何。这项工作通过这些主键过滤器的通用语言，为理解泛化和迁移学习提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A recent study has proposed the "Master Key Filters Hypothesis" forconvolutional neural network filters. This paper extends this hypothesis byradically constraining its scope to a single set of just 8 universal filtersthat depthwise separable convolutional networks inherently converge to. Whileconventional DS-CNNs employ thousands of distinct trained filters, our analysisreveals these filters are predominantly linear shifts (ax+b) of our discovereduniversal set. Through systematic unsupervised search, we extracted thesefundamental patterns across different architectures and datasets. Remarkably,networks initialized with these 8 unique frozen filters achieve over 80%ImageNet accuracy, and even outperform models with thousands of trainableparameters when applied to smaller datasets. The identified master key filtersclosely match Difference of Gaussians (DoGs), Gaussians, and their derivatives,structures that are not only fundamental to classical image processing but alsostrikingly similar to receptive fields in mammalian visual systems. Ourfindings provide compelling evidence that depthwise convolutional layersnaturally gravitate toward this fundamental set of spatial operators regardlessof task or architecture. This work offers new insights for understandinggeneralization and transfer learning through the universal language of thesemaster key filters.</description>
      <author>example@mail.com (Zahra Babaiee, Peyman M. Kiassari, Daniela Rus, Radu Grosu)</author>
      <guid isPermaLink="false">2509.11711v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>An Unsupervised Learning Approach For A Reliable Profiling Of Cyber Threat Actors Reported Globally Based On Complete Contextual Information Of Cyber Attacks</title>
      <link>http://arxiv.org/abs/2509.11683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于无监督的高效凝聚层次聚类技术，用于对网络犯罪团伙进行全面画像，解决了现有监督机器学习方法在攻击者画像方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着技术进步，网络攻击迅速增加，而信息保护不足。现有监督机器学习方法只考虑了文本威胁报告中的少量特征，且依赖结构化数据集，需要先建立数据集再进行分析，效率低下。&lt;h4&gt;目的&lt;/h4&gt;识别网络威胁行为者之间的共同特征关系，对其进行聚合，并对网络犯罪团伙进行画像，以创建更有效的防御机制。&lt;h4&gt;方法&lt;/h4&gt;采用无监督的高效凝聚层次聚类技术，基于全面的上下文威胁信息对网络犯罪团伙进行画像，避免了对结构化数据集的依赖。&lt;h4&gt;主要发现&lt;/h4&gt;通过基于网络威胁行为者的共同特征识别关系并聚合，可以更有效地创建防御机制，提高威胁分析的效率。&lt;h4&gt;结论&lt;/h4&gt;基于全面上下文信息的无监督方法可以更有效地识别网络威胁行为者之间的关系，为提前创建防御机制提供支持。&lt;h4&gt;翻译&lt;/h4&gt;网络攻击随着技术进步而迅速增加，我们的信息没有保护。为防止未来的网络攻击，及时识别网络攻击并建立强大的防御机制至关重要。为立即应对网络安全威胁，必须检查攻击者的技能、知识和行为，目的是评估他们对系统的影响并理解与这些攻击相关的特征。基于网络威胁行为者的特征或行为模式创建画像，可以帮助提前创建有效的防御机制。在现有文献中，多种基于监督机器学习的方法只考虑了文本网络威胁事件报告中报告的少量特征用于攻击者画像，尽管这些画像是基于安全专家自身的认知开发的，但我们不能依赖它们。监督机器学习方法严格依赖于结构化数据集。这通常导致一个两步过程，即我们首先必须建立结构化数据集，然后才能分析它并用于构建防御机制，这需要时间。在本文中，提出了一种无监督的高效凝聚层次聚类技术，用于基于全面的上下文威胁信息对网络犯罪团伙进行画像，以解决上述问题。本报告的主要目标是识别基于共同特征的网络威胁行为者之间的关系，对其进行聚合，并对网络犯罪团伙进行画像。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cyber attacks are rapidly increasing with the advancement of technology andthere is no protection for our information. To prevent future cyberattacks itis critical to promptly recognize cyberattacks and establish strong defensemechanisms against them. To respond to cybersecurity threats immediately, it isessential to examine the attackers skills, knowledge, and behaviors with thegoal of evaluating their impact on the system and comprehending the traitsassociated with these attacks. Creating a profile of cyber threat actors basedon their traits or patterns of behavior can help to create effective defensesagainst cyberattacks in advance. In the current literature, multiple supervisedmachine learning based approaches considered a smaller number of features forattacker profiling that are reported in textual cyber threat incident documentsalthough these profiles have been developed based on the security experts ownperception, we cannot rely on them. Supervised machine learning approachesstrictly depend upon the structure data set. This usually leads to a two stepprocess where we first have to establish a structured data set before we cananalyze it and then employ it to construct defense mechanisms, which takestime. In this paper, an unsupervised efficient agglomerative hierarchalclustering technique is proposed for profiling cybercriminal groups based ontheir comprehensive contextual threat information in order to address theaforementioned issues. The main objective of this report is to identify therelationship between cyber threat actors based on their common features,aggregate them, and also profile cyber criminal groups.</description>
      <author>example@mail.com (Sawera Shahid, Umara Noor, Zahid Rashid)</author>
      <guid isPermaLink="false">2509.11683v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Stacked Intelligent Metasurface for End-to-End OFDM System</title>
      <link>http://arxiv.org/abs/2509.11551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于SIM（DPSIM）辅助的端到端OFDM系统，将传统通信任务在电磁前向传播中同时完成，并引入电磁神经网络（EMNN）进行系统控制，通过迁移学习进行模型训练，实现了在复杂信道条件下的稳健比特流传输。&lt;h4&gt;背景&lt;/h4&gt;堆叠智能超表面（SIM）和双极化SIM（DPSIM）的波域信号处理是卸载基带数字处理和简化收发器设计的有前途的研究方向，但现有架构仅限于将SIM（DPSIM）用于单一通信功能。&lt;h4&gt;目的&lt;/h4&gt;提高SIM（DPSIM）辅助系统的整体性能，实现从发送比特流到接收比特流的端到端（E2E）联合优化。&lt;h4&gt;方法&lt;/h4&gt;提出SIM（DPSIM）辅助的端到端OFDM系统，在电磁前向传播过程中同时完成调制、预编码、合并和解调任务；受神经网络启发提出电磁神经网络（EMNN）控制系统；引入迁移学习进行模型训练；设计EMNN的训练和部署框架。&lt;h4&gt;主要发现&lt;/h4&gt;SIM辅助和DPSIM辅助的端到端OFDM系统都能在复杂信道条件下实现稳健的比特流传输。&lt;h4&gt;结论&lt;/h4&gt;EMNN和SIM（DPSIM）辅助的端到端OFDM系统在下一代收发器设计中具有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;堆叠智能超表面（SIM）和双极化SIM（DPSIM）的波域信号处理已成为卸载基带数字处理任务和高效简化收发器设计的研究方向。然而，现有架构仅限于将SIM（DPSIM）用于单一通信功能，如预编码或合并。为了进一步提高SIM（DPSIM）辅助系统的整体性能，并实现从发送比特流到接收比特流的端到端（E2E）联合优化，我们提出了一种SIM（DPSIM）辅助的端到正交频分复用（OFDM）系统，其中传统的通信任务如调制、预编码、合并和解调在电磁（EM）前向传播过程中同时完成。此外，受将真实超表面抽象为神经网络隐藏层的启发，我们提出了电磁神经网络（EMNN）来控制端到端OFDM通信系统。另外，将迁移学习引入模型训练，并设计了EMNN的训练和部署框架。仿真结果表明，SIM辅助的端到端OFDM系统和DPSIM辅助的端到端OFDM系统都能在复杂信道条件下实现稳健的比特流传输。我们的研究突显了EMNN和SIM（DPSIM）辅助的端到端OFDM系统在下一代收发器设计中的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stacked intelligent metasurface (SIM) and dual-polarized SIM (DPSIM) enabledwave-domain signal processing have emerged as promising research directions foroffloading baseband digital processing tasks and efficiently simplifyingtransceiver design. However, existing architectures are limited to employingSIM (DPSIM) for a single communication function, such as precoding orcombining. To further enhance the overall performance of SIM (DPSIM)-assistedsystems and achieve end-to-end (E2E) joint optimization from the transmittedbitstream to the received bitstream, we propose an SIM (DPSIM)- assisted E2Eorthogonal frequency-division multiplexing (OFDM) system, where traditionalcommunication tasks such as modulation, precoding, combining, and demodulationare performed simultaneously during electromagnetic (EM) forward propagation.Furthermore, inspired by the idea of abstracting real metasurfaces as hiddenlayers of a neural network, we propose the electromagnetic neural network(EMNN) to enable the control of the E2E OFDM communication system. In addition,transfer learning is introduced into the model training, and a training anddeployment framework for the EMNN is designed. Simulation results demonstratethat both SIM-assisted E2E OFDM systems and DPSIM-assisted E2E OFDM systems canachieve robust bitstream transmission under complex channel conditions. Ourstudy highlights the application potential of EMNN and SIM (DPSIM)-assisted E2EOFDM systems in the design of next-generation transceivers.</description>
      <author>example@mail.com (Yida Zhang, Qiuyan Liu, Hongtao Luo, Yuqi Xia, Qiang Wang)</author>
      <guid isPermaLink="false">2509.11551v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>MAUI: Reconstructing Private Client Data in Federated Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.11451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAUI，一种隐蔽的数据重建攻击方法，在联邦学习中仅利用分类头梯度即可重建原始输入数据，显著提高了重建质量。&lt;h4&gt;背景&lt;/h4&gt;联邦学习中，服务器首先在公共数据集上预训练全局模型，然后在客户端微调模型的最后几个线性层（分类头）。现有数据重建攻击存在两个弱点：初始模型层的梯度信息不共享，以及服务器对模型的手工操作易被检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种隐蔽的数据重建攻击方法，不需要对模型架构或权重进行明显操作，仅依靠分类头梯度实现数据重建。&lt;h4&gt;方法&lt;/h4&gt;MAUI从分类头的梯度中提取输入批次的'鲁棒'特征表示，然后将这些特征表示反转回原始输入数据。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR10和ImageNet数据集上实现了高度准确的重建，适用于多种模型架构（CNN、VGG11、ResNets、ShuffleNet-V2和ViT B-32），无论批量大小如何都能工作，重建质量比先前方法高40-120%的PSNR分数。&lt;h4&gt;结论&lt;/h4&gt;MAUI是一种隐蔽高效的数据重建攻击方法，仅利用分类头梯度即可实现高质量的数据重建，显著优于现有攻击方法。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)的最新研究表明，利用迁移学习可以平衡联邦学习和集中式学习的好处。在这种设置中，联邦训练在通过传统训练达到稳定点后进行。全局模型权重首先由服务器在公共数据集上进行集中预训练，之后只有模型的最后几个线性层（分类头）在客户端间进行微调。在这种情况下，现有的联邦学习数据重建攻击(DRAs)表现出两个关键弱点：首先，与输入强相关的初始模型层梯度信息从不共享，显著降低了重建准确性；其次，服务器对模型结构或参数进行高度特定、手工操作的攻击方法（如全零权重层、恒等映射和具有相同权重模式的行）容易被活跃客户端检测。针对这些弱点，我们提出了MAUI，一种隐蔽的数据重建攻击，不需要对模型架构或权重进行任何明显操作，仅依赖于分类头的梯度。MAUI首先从分类头的梯度中提取输入批次的'鲁棒'特征表示，然后将这些表示反转回原始输入。我们在CIFAR10和ImageNet数据集上报告了高度准确的重建，适用于多种模型架构，包括卷积网络(CNN, VGG11)、ResNets(18, 50)、ShuffleNet-V2和Vision Transformer(ViT B-32)，无论批量大小如何。MAUI在重建质量上显著优于先前的DRAs，实现了40-120%更高的PSNR分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works in federated learning (FL) have shown the utility of leveragingtransfer learning for balancing the benefits of FL and centralized learning. Inthis setting, federated training happens after a stable point has been reachedthrough conventional training. Global model weights are first centrallypretrained by the server on a public dataset following which only the last fewlinear layers (the classification head) of the model are finetuned acrossclients. In this scenario, existing data reconstruction attacks (DRAs) in FLshow two key weaknesses. First, strongly input-correlated gradient informationfrom the initial model layers is never shared, significantly degradingreconstruction accuracy. Second, DRAs in which the server makes highlyspecific, handcrafted manipulations to the model structure or parameters (fore.g., layers with all zero weights, identity mappings and rows with identicalweight patterns) are easily detectable by an active client.  Improving on these, we propose MAUI, a stealthy DRA that does not require anyovert manipulations to the model architecture or weights, and relies solely onthe gradients of the classification head. MAUI first extracts "robust" featurerepresentations of the input batch from the gradients of the classificationhead and subsequently inverts these representations to the original inputs. Wereport highly accurate reconstructions on the CIFAR10 and ImageNet datasets ona variety of model architectures including convolution networks (CNN, VGG11),ResNets (18, 50), ShuffleNet-V2 and Vision Transformer (ViT B-32), regardlessof the batch size. MAUI significantly outperforms prior DRAs in reconstructionquality, achieving 40-120% higher PSNR scores.</description>
      <author>example@mail.com (Ahaan Dabholkar, Atul Sharma, Z. Berkay Celik, Saurabh Bagchi)</author>
      <guid isPermaLink="false">2509.11451v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic Dataset Evaluation Based on Generalized Cross Validation</title>
      <link>http://arxiv.org/abs/2509.11273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IST 2025. Official IEEE Xplore entry will  be available once published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合广义交叉验证和领域迁移学习原则的合成数据集质量评估框架，通过构建交叉性能矩阵和GCV矩阵来量化领域可迁移性，并引入两个关键指标分别评估模拟质量和迁移质量，实验证明该框架能有效评估合成数据保真度。&lt;h4&gt;背景&lt;/h4&gt;随着合成数据集生成技术的快速发展，评估合成数据质量已成为关键研究焦点。当前评估研究有限，缺乏普遍接受的标准框架。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的评估框架，对合成数据集质量进行可推广和可比较的评估，指导合成数据集优化。&lt;h4&gt;方法&lt;/h4&gt;在合成数据集和真实世界基准数据集上训练特定任务模型，形成交叉性能矩阵；构建广义交叉验证矩阵量化领域可迁移性；引入两个关键指标：一个衡量合成数据与真实数据的相似性，一个评估合成数据在不同真实世界场景中的多样性和覆盖范围。&lt;h4&gt;主要发现&lt;/h4&gt;在Virtual KITTI上的实验验证了所提框架和指标在评估合成数据保真度方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;该可扩展和可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了有原则的方法。&lt;h4&gt;翻译&lt;/h4&gt;随着合成数据集生成技术的快速发展，评估合成数据的质量已成为关键研究焦点。稳健的评估不仅推动数据生成方法的创新，还能指导研究人员优化这些合成资源的利用。然而，当前对合成数据集的评估研究仍然有限，缺乏普遍接受的标准框架。为解决这一问题，本文提出了一种结合广义交叉验证实验和领域迁移学习原则的新评估框架，能够对合成数据集质量进行可推广和可比较的评估。该框架涉及在合成数据集和多个真实世界基准（如KITTI、BDD100K）上训练特定任务模型（如YOLOv5s），形成一个交叉性能矩阵。经过归一化后，构建广义交叉验证（GCV）矩阵来量化领域可迁移性。该框架引入两个关键指标：一个通过量化合成数据与真实数据集之间的相似性来衡量模拟质量，另一个通过评估合成数据在各种真实世界场景中的多样性和覆盖范围来评估迁移质量。在Virtual KITTI上的实验验证了所提框架和指标在评估合成数据保真度方面的有效性。这种可扩展和可量化的评估解决方案克服了传统局限性，为人工智能研究中合成数据集的优化提供了有原则的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of synthetic dataset generation techniques,evaluating the quality of synthetic data has become a critical research focus.Robust evaluation not only drives innovations in data generation methods butalso guides researchers in optimizing the utilization of these syntheticresources. However, current evaluation studies for synthetic datasets remainlimited, lacking a universally accepted standard framework. To address this,this paper proposes a novel evaluation framework integrating generalizedcross-validation experiments and domain transfer learning principles, enablinggeneralizable and comparable assessments of synthetic dataset quality. Theframework involves training task-specific models (e.g., YOLOv5s) on bothsynthetic datasets and multiple real-world benchmarks (e.g., KITTI, BDD100K),forming a cross-performance matrix. Following normalization, a GeneralizedCross-Validation (GCV) Matrix is constructed to quantify domaintransferability. The framework introduces two key metrics. One measures thesimulation quality by quantifying the similarity between synthetic data andreal-world datasets, while another evaluates the transfer quality by assessingthe diversity and coverage of synthetic data across various real-worldscenarios. Experimental validation on Virtual KITTI demonstrates theeffectiveness of our proposed framework and metrics in assessing synthetic datafidelity. This scalable and quantifiable evaluation solution overcomestraditional limitations, providing a principled approach to guide syntheticdataset optimization in artificial intelligence research.</description>
      <author>example@mail.com (Zhihang Song, Dingyi Yao, Ruibo Ming, Lihui Peng, Danya Yao, Yi Zhang)</author>
      <guid isPermaLink="false">2509.11273v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Meter Tracking in Carnatic Music using Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2509.11241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了两种深度学习模型在卡纳提克音乐节拍跟踪任务上的性能，并探索了通过迁移学习和音乐信息参数调整来提高模型适应性的方法。&lt;h4&gt;背景&lt;/h4&gt;节拍和强拍跟踪是音乐信息检索的基础任务。深度学习模型在西方音乐流派中表现优异，但在代表性不足的音乐传统上表现不佳。卡纳提克音乐以其节奏复杂性和独特节拍结构著称，先前研究主要采用概率动态贝叶斯网络(DBN)方法。&lt;h4&gt;目的&lt;/h4&gt;评估两种深度学习模型在卡纳提克音乐节拍跟踪任务上的性能，研究适应策略，包括数据微调和音乐信息参数调整，探索最先进深度学习模型在非西方音乐传统中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;在卡纳提克音乐节奏(CMR_f)数据集上评估时间卷积网络(TCN)和基于Transformer的Beat This!模型，复制DBN基线实验设置，并通过迁移学习和音乐信息参数调整来优化模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;现成的深度学习模型并不总是优于DBN基线，但通过迁移学习，模型性能显著提高，能够匹配或超过基线性能，表明深度学习模型可以有效地适应卡纳提克音乐等非西方音乐传统。&lt;h4&gt;结论&lt;/h4&gt;最先进的深度学习模型可以有效地适应代表性不足的音乐传统，为开发更具包容性和广泛适用性的节拍跟踪系统开辟了道路。&lt;h4&gt;翻译&lt;/h4&gt;节拍和强拍跟踪，统称为节拍跟踪，是音乐信息检索(MIR)中的基础任务。深度学习模型在这一领域已经远远超越了传统的信号处理和经典机器学习方法，特别是在西方(欧洲起源)音乐流派中，因为有大量标注数据集可用。然而，这些系统在代表性不足的音乐传统上表现不太可靠。卡纳提克音乐是来自印度次大陆的丰富传统，以其节奏复杂性和独特的节拍结构(talas)而闻名。在此背景下，关于节拍跟踪的最著名先前工作采用了概率动态贝叶斯网络(DBNs)。然而，最先进的深度学习模型在卡纳提克音乐上的表现仍然 largely未被探索。在本研究中，我们评估了两种用于卡纳提克音乐节拍跟踪的模型：时间卷积网络(TCN)，这是一种已成功适应拉丁节奏的轻量级架构，以及Beat This!，一种不需要后处理的基于Transformer的模型，旨在广泛风格覆盖。我们在卡纳提克音乐节奏(CMR_f)数据集上复制DBN基线的实验设置，在直接可比的环境中系统地评估这些模型的性能。我们进一步研究了适应策略，包括在卡纳提克数据上微调模型和使用音乐信息参数。结果表明，虽然现成的模型并不总是优于DBN，但通过迁移学习，它们的性能显著提高，匹配或超过了基线。这些发现表明，最先进的深度学习模型可以有效地适应代表性不足的传统，为更具包容性和更广泛适用的节拍跟踪系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Beat and downbeat tracking, jointly referred to as Meter Tracking, is afundamental task in Music Information Retrieval (MIR). Deep learning modelshave far surpassed traditional signal processing and classical machine learningapproaches in this domain, particularly for Western (Eurogenetic) genres, wherelarge annotated datasets are widely available. These systems, however, performless reliably on underrepresented musical traditions. Carnatic music, a richtradition from the Indian subcontinent, is renowned for its rhythmic intricacyand unique metrical structures (t\=alas). The most notable prior work on metertracking in this context employed probabilistic Dynamic Bayesian Networks(DBNs). The performance of state-of-the-art (SOTA) deep learning models onCarnatic music, however, remains largely unexplored.  In this study, we evaluate two models for meter tracking in Carnatic music:the Temporal Convolutional Network (TCN), a lightweight architecture that hasbeen successfully adapted for Latin rhythms, and Beat This!, atransformer-based model designed for broad stylistic coverage without the needfor post-processing. Replicating the experimental setup of the DBN baseline onthe Carnatic Music Rhythm (CMR$_f$) dataset, we systematically assess theperformance of these models in a directly comparable setting. We furtherinvestigate adaptation strategies, including fine-tuning the models on Carnaticdata and the use of musically informed parameters. Results show that whileoff-the-shelf models do not always outperform the DBN, their performanceimproves substantially with transfer learning, matching or surpassing thebaseline. These findings indicate that SOTA deep learning models can beeffectively adapted to underrepresented traditions, paving the way for moreinclusive and broadly applicable meter tracking systems.</description>
      <author>example@mail.com (Satyajeet Prabhu)</author>
      <guid isPermaLink="false">2509.11241v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Feature Space Topology Control via Hopkins Loss</title>
      <link>http://arxiv.org/abs/2509.11154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Proc. IEEE ICTAI 2025, Athens, Greece&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为'Hopkins loss'的新型损失函数，它利用Hopkins统计量强制实现所需的特征空间拓扑结构，与现有方法不同，现有方法旨在保留输入特征拓扑。研究者在语音、文本和图像数据上评估了该方法在分类和降维场景下的有效性，结果表明该方法对分类性能影响较小，同时能修改特征拓扑。&lt;h4&gt;背景&lt;/h4&gt;特征空间拓扑指的是特征空间中样本的组织方式。修改这种拓扑在机器学习应用中是有益的，包括降维、生成建模、迁移学习和对抗攻击的鲁棒性。现有的拓扑相关方法主要关注保留输入特征拓扑。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的损失函数(Hopkins loss)，能够强制实现所需的特征空间拓扑，从而在保持分类性能的同时修改特征拓扑结构。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'Hopkins loss'的新型损失函数，它利用Hopkins统计量来强制实现所需的特征空间拓扑。研究者在语音、文本和图像数据上评估了该方法，在两个场景中进行测试：分类和非线性瓶颈自编码器的降维。&lt;h4&gt;主要发现&lt;/h4&gt;将Hopkins loss集成到分类或降维任务中，对分类性能只有很小的影响，同时能够提供修改特征拓扑的好处。&lt;h4&gt;结论&lt;/h4&gt;Hopkins loss是一种有效的工具，可以在保持模型性能的同时，修改特征空间的拓扑结构，这有助于提高机器学习应用中的多种任务表现。&lt;h4&gt;翻译&lt;/h4&gt;特征空间拓扑指的是特征空间中样本的组织方式。修改这种拓扑在机器学习应用中是有益的，包括降维、生成建模、迁移学习和对抗攻击的鲁棒性。本文介绍了一种新型的损失函数Hopkins loss，它利用Hopkins统计量来强制实现所需的特征空间拓扑，这与现有的拓扑相关方法形成对比，后者旨在保留输入特征拓扑。我们在语音、文本和图像数据上评估了Hopkins loss在两种场景下的有效性：使用非线性瓶颈自编码器的分类和降维。我们的实验表明，将Hopkins loss集成到分类或降维中，对分类性能只有很小的影响，同时能够提供修改特征拓扑的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature space topology refers to the organization of samples within thefeature space. Modifying this topology can be beneficial in machine learningapplications, including dimensionality reduction, generative modeling, transferlearning, and robustness to adversarial attacks. This paper introduces a novelloss function, Hopkins loss, which leverages the Hopkins statistic to enforce adesired feature space topology, which is in contrast to existingtopology-related methods that aim to preserve input feature topology. Weevaluate the effectiveness of Hopkins loss on speech, text, and image data intwo scenarios: classification and dimensionality reduction using nonlinearbottleneck autoencoders. Our experiments show that integrating Hopkins lossinto classification or dimensionality reduction has only a small impact onclassification performance while providing the benefit of modifying featuretopology.</description>
      <author>example@mail.com (Einari Vaaras, Manu Airaksinen)</author>
      <guid isPermaLink="false">2509.11154v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Policy-Driven Transfer Learning in Resource-Limited Animal Monitoring</title>
      <link>http://arxiv.org/abs/2509.10995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 3 algorithms, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于强化学习的迁移学习框架，使用上置信界算法自动选择最适合动物检测任务的预训练模型，实现了更高的检测率并减少了计算时间。&lt;h4&gt;背景&lt;/h4&gt;动物健康监测和种群管理是野生动物保护和畜牧业管理的关键方面，这些领域越来越依赖自动化检测和跟踪系统。基于无人机和计算机视觉的系统提供了有前景的解决方案，但标记训练数据的有限性是开发有效深度学习模型的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于强化学习的迁移学习框架，使用上置信界算法自动选择最适合动物检测任务的预训练模型，简化模型选择过程。&lt;h4&gt;方法&lt;/h4&gt;采用强化学习方法进行迁移学习，使用上置信界算法系统评估和排序候选模型，基于性能选择最优模型。&lt;h4&gt;主要发现&lt;/h4&gt;该框架实现了比传统方法更高的检测率，同时需要显著更少的计算时间。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了预训练神经网络架构众多导致的模型选择难题，特别对领域新手研究者有帮助，能够简化模型选择过程。&lt;h4&gt;翻译&lt;/h4&gt;动物健康监测和种群管理是野生动物保护和畜牧业管理的关键方面，这些方面越来越依赖于自动化检测和跟踪系统。虽然基于无人机(UAV)的系统结合计算机视觉为具有挑战性地形上的非侵入式动物监测提供了有前景的解决方案，但标记训练数据的有限性仍然是开发这些应用的有效深度学习(DL)模型的障碍。迁移学习已成为一种潜在的解决方案，允许在大型数据集上训练的模型适应资源有限的情况，例如数据有限的情况。然而，预训练神经网络架构的广阔格局使得选择最佳模型具有挑战性，特别是对领域新手研究人员。在本文中，我们提出了一种基于强化学习(RL)的迁移学习框架，该框架采用上置信界(UCB)算法自动选择最适合动物检测任务的预训练模型。我们的方法基于性能系统评估和排名候选模型，简化了模型选择过程。实验结果表明，与传统方法相比，我们的框架实现了更高的检测率，同时需要显著更少的计算时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Animal health monitoring and population management are critical aspects ofwildlife conservation and livestock management that increasingly rely onautomated detection and tracking systems. While Unmanned Aerial Vehicle (UAV)based systems combined with computer vision offer promising solutions fornon-invasive animal monitoring across challenging terrains, limitedavailability of labeled training data remains an obstacle in developingeffective deep learning (DL) models for these applications. Transfer learninghas emerged as a potential solution, allowing models trained on large datasetsto be adapted for resource-limited scenarios such as those with limited data.However, the vast landscape of pre-trained neural network architectures makesit challenging to select optimal models, particularly for researchers new tothe field. In this paper, we propose a reinforcement learning (RL)-basedtransfer learning framework that employs an upper confidence bound (UCB)algorithm to automatically select the most suitable pre-trained model foranimal detection tasks. Our approach systematically evaluates and rankscandidate models based on their performance, streamlining the model selectionprocess. Experimental results demonstrate that our framework achieves a higherdetection rate while requiring significantly less computational time comparedto traditional methods.</description>
      <author>example@mail.com (Nisha Pillai, Aditi Virupakshaiah, Harrison W. Smith, Amanda J. Ashworth, Prasanna Gowda, Phillip R. Owens, Adam R. Rivers, Bindu Nanduri, Mahalingam Ramkumar)</author>
      <guid isPermaLink="false">2509.10995v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A novel IR-SRGAN assisted super-resolution evaluation of photothermal coherence tomography for impact damage in toughened thermoplastic CFRP laminates under room temperature and low temperature</title>
      <link>http://arxiv.org/abs/2509.10894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种新型红外超分辨率生成对抗网络（IR-SRGAN），用于提高复合材料在低温条件下冲击损伤检测的准确性，解决了红外热成像技术存在的空间分辨率限制问题。&lt;h4&gt;背景&lt;/h4&gt;评估复合材料在温度变化条件下的冲击损伤对航空航天、极地等极端环境应用中的结构完整性和可靠性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;精确检测和量化复合材料亚表面损伤特征，如分层面积、裂纹形态和界面分离，以进行后续机械表征和寿命预测。&lt;h4&gt;方法&lt;/h4&gt;结合红外热成像（IRT）与新开发的频分复用光热相关层析成像（FM-PCT）技术，并应用基于迁移学习的红外超分辨率生成对抗网络（IR-SRGAN）来增强成像质量。&lt;h4&gt;主要发现&lt;/h4&gt;低温条件下，基体脆性增加，导致损伤机制发生变化，常温下轻微的冲击可能引发严重的基体开裂、纤维/基体脱粘或界面失效；IRT技术存在帧率受限和横向热扩散等固有局限性，影响损伤尺寸测量的准确性。&lt;h4&gt;结论&lt;/h4&gt;通过开发的IR-SRGAN技术，可以在有限的热成像数据基础上，提高横向和深度分辨成像的保真度，从而更准确地评估复合材料在极端环境下的损伤情况。&lt;h4&gt;翻译&lt;/h4&gt;评估复合材料在变化温度条件下的冲击损伤对于确保航空航天、极地和其他极端环境应用中的结构完整性和可靠性能至关重要。随着低温下基体脆性的增加，损伤机制发生变化：在常温条件下只产生轻微分层的冲击事件在严重冷载荷下可能引发广泛的基体开裂、纤维/基体脱粘或界面失效，从而降低剩余强度和疲劳寿命。精确检测和量化亚表面损伤特征（如分层面积、裂纹形态、界面分离）对后续机械表征和寿命预测至关重要。在本研究中，采用红外热成像（IRT）与新开发的频分复用光热相关层析成像（FM-PCT）相结合，捕捉三维亚表面损伤特征，其深度分辨率接近X射线显微CT的分辨率。然而，IRT的固有局限性，包括帧率受限和横向热扩散，降低了空间分辨率，从而影响损伤尺寸测量的准确性。为解决此问题，我们开发了一种基于迁移学习的红外超分辨率生成对抗网络（IR-SRGAN），基于有限的热成像数据集增强横向和深度分辨成像保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating impact-induced damage in composite materials under varyingtemperature conditions is essential for ensuring structural integrity andreliable performance in aerospace, polar, and other extreme-environmentapplications. As matrix brittleness increases at low temperatures, damagemechanisms shift: impact events that produce only minor delaminations atambient conditions can trigger extensive matrix cracking, fiber/matrixdebonding, or interfacial failure under severe cold loads, thereby degradingresidual strength and fatigue life. Precision detection and quantification ofsubsurface damage features (e.g., delamination area, crack morphology,interface separation) are critical for subsequent mechanical characterizationand life prediction. In this study, infrared thermography (IRT) coupled with anewly developed frequency multiplexed photothermal correlation tomography(FM-PCT) is employed to capture three-dimensional subsurface damage signatureswith depth resolution approaching that of X-ray micro-computed tomography.However, the inherent limitations of IRT, including restricted frame rate andlateral thermal diffusion, reduce spatial resolution and thus the accuracy ofdamage size measurement. To address this, we develop a new transferlearning-based infrared super-resolution generative adversarial network(IR-SRGAN) that enhances both lateral and depth-resolved imaging fidelity basedon limited thermographic datasets.</description>
      <author>example@mail.com (Pengfei Zhu, Hai Zhang, Stefano Sfarra, Fabrizio Sarasini, Zijing Ding, Clemente Ibarra-Castanedo, Xavier Maldague)</author>
      <guid isPermaLink="false">2509.10894v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Neurosymbolic AI Transfer Learning Improves Network Intrusion Detection</title>
      <link>http://arxiv.org/abs/2509.10850v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的神经符号AI框架，用于网络入侵检测系统，展示了迁移学习在网络安全领域的潜力。&lt;h4&gt;背景&lt;/h4&gt;迁移学习在计算机视觉、自然语言处理和医学影像等领域被广泛应用，但在网络安全领域的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;设计一个创新的神经符号AI框架应用于网络入侵检测系统，以应对网络安全中的恶意活动。&lt;h4&gt;方法&lt;/h4&gt;利用迁移学习和不确定性量化的神经符号AI框架进行网络入侵检测。&lt;h4&gt;主要发现&lt;/h4&gt;在大型和结构良好的数据集上训练的迁移学习模型比依赖较小数据集的基于神经的模型表现更好。&lt;h4&gt;结论&lt;/h4&gt;迁移学习模型在网络安全领域的应用为网络安全解决方案的新时代铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习因其解决子任务和处理不同数据集的出色能力，常被应用于计算机视觉、自然语言处理和医学影像等多个领域。然而，其在网络安全领域的应用尚未得到充分探索。在本文中，我们提出了一种创新的神经符号AI框架，专门用于网络入侵检测系统，该系统在应对网络安全中的恶意活动方面起着关键作用。我们的框架利用了迁移学习和不确定性量化。研究结果表明，在大型和结构良好的数据集上训练的迁移学习模型，比依赖较小数据集的基于神经的模型表现更优，为网络安全解决方案的新时代铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is commonly utilized in various fields such as computervision, natural language processing, and medical imaging due to its impressivecapability to address subtasks and work with different datasets. However, itsapplication in cybersecurity has not been thoroughly explored. In this paper,we present an innovative neurosymbolic AI framework designed for networkintrusion detection systems, which play a crucial role in combating maliciousactivities in cybersecurity. Our framework leverages transfer learning anduncertainty quantification. The findings indicate that transfer learningmodels, trained on large and well-structured datasets, outperform neural-basedmodels that rely on smaller datasets, paving the way for a new era incybersecurity solutions.</description>
      <author>example@mail.com (Huynh T. T. Tran, Jacob Sander, Achraf Cohen, Brian Jalaian, Nathaniel D. Bastian)</author>
      <guid isPermaLink="false">2509.10850v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
      <link>http://arxiv.org/abs/2509.07766v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, International Quantum Engineering conference and  exhibition (QUEST-IS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图的联盟结构生成算法(GCS-Q)，用于金融资产的相关性聚类，该方法能够直接处理带符号的加权图，无需传统转换方法，并通过量子退火技术高效探索解空间。实验证明该方法在聚类质量和动态确定聚类数量方面优于现有经典算法。&lt;h4&gt;背景&lt;/h4&gt;基于收益率相关性的金融资产聚类是投资组合优化和统计套利的基础任务，但传统聚类方法在处理带符号的相关性结构时表现不佳，通常需要损失性转换和固定聚类数量等启发式假设。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接处理带符号、加权图的聚类方法，避免传统方法的局限性，并利用量子计算技术提高聚类效率和质量。&lt;h4&gt;方法&lt;/h4&gt;应用基于图的联盟结构生成算法(GCS-Q)，将每个分区步骤表述为QUBO问题，利用量子退火技术高效探索解空间。在合成和真实金融数据上验证该方法，并与SPONGE和k-Medoids等经典算法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GCS-Q在调整兰德指数和结构平衡惩罚等指标上始终实现更高的聚类质量，同时能够动态确定聚类数量，无需预先指定聚类数。&lt;h4&gt;结论&lt;/h4&gt;近期量子计算在金融应用的基于图的无监督学习中具有实用价值，为处理复杂金融数据提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;基于收益率相关性对金融资产进行聚类是投资组合优化和统计套利中的基础任务。然而，传统的聚类方法在处理带符号的相关性结构时往往表现不足，通常需要损失性转换和启发式假设，如固定数量的聚类。在本工作中，我们应用基于图的联盟结构生成算法(GCS-Q)直接聚类带符号的加权图，而不依赖这些转换。GCS-Q将每个分区步骤表述为QUBO问题，使其能够利用量子退火高效探索指数级大的解空间。我们在合成和真实金融数据上验证了我们的方法，并与SPONGE和k-Medoids等最先进的经典算法进行基准测试。我们的实验表明，GCS-Q在调整兰德指数和结构平衡惩罚等指标上始终实现更高的聚类质量，同时动态确定聚类数量。这些结果突显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering financial assets based on return correlations is a fundamentaltask in portfolio optimization and statistical arbitrage. However, classicalclustering methods often fall short when dealing with signed correlationstructures, typically requiring lossy transformations and heuristic assumptionssuch as a fixed number of clusters. In this work, we apply the Graph-basedCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,weighted graphs without relying on such transformations. GCS-Q formulates eachpartitioning step as a QUBO problem, enabling it to leverage quantum annealingfor efficient exploration of exponentially large solution spaces. We validateour approach on both synthetic and real-world financial data, benchmarkingagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Ourexperiments demonstrate that GCS-Q consistently achieves higher clusteringquality, as measured by Adjusted Rand Index and structural balance penalties,while dynamically determining the number of clusters. These results highlightthe practical utility of near-term quantum computing for graph-basedunsupervised learning in financial applications.</description>
      <author>example@mail.com (Shivam Sharma, Supreeth Mysore Venkatesh, Pushkin Kachroo)</author>
      <guid isPermaLink="false">2509.07766v2</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>LoRA-fine-tuned Large Vision Models for Automated Assessment of Post-SBRT Lung Injury</title>
      <link>http://arxiv.org/abs/2509.12155v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了使用低秩适应（LoRA）技术微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向体放射治疗（SBRT）后X射线CT扫描中的放射性肺损伤（RILI）的有效性。&lt;h4&gt;背景&lt;/h4&gt;需要开发有效的方法来诊断放射治疗后的放射性肺损伤，这可能对患者监测和治疗调整至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估LoRA方法在微调大型视觉模型以诊断RILI方面的稳健性和效率，并与传统的完全微调和仅推理方法进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用两种尺寸（50 mm³和75 mm³）的裁剪图像（以治疗等中心为中心），以及不同的适应技术将2D大型视觉模型适应为3D数据处理，评估模型对空间上下文的敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;LoRA实现了与传统微调相当或更好的性能，同时显著降低了计算成本和训练时间，因为需要更少的可训练参数。&lt;h4&gt;结论&lt;/h4&gt;LoRA是一种有效的方法，可用于微调大型视觉模型以诊断放射性肺损伤，同时保持高性能并减少计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了低秩适应（LoRA）技术用于微调大型视觉模型（DinoV2和SwinV2）以诊断立体定向体放射治疗（SBRT）后X射线CT扫描中放射性肺损伤（RILI）的有效性。为了评估这种方法的稳健性和效率，我们将LoRA与传统完全微调和仅推理（不微调）方法进行比较。除了使用以治疗等为中心的两种尺寸（50 mm³和75 mm³）的裁剪图像外，我们还使用了不同的适应技术将2D大型视觉模型适应为3D数据处理，以确定模型对空间上下文的敏感性。实验结果表明，LoRA实现了与传统微调相当或更好的性能，同时通过需要更少的可训练参数显著降低了计算成本和训练时间。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何使用人工智能技术自动诊断肺癌患者接受立体定向体部放射治疗（SBRT）后可能出现的辐射诱导肺损伤（RILI）。这个问题很重要，因为RILI是肺癌放疗后的常见并发症（发生率5-25%），早期诊断困难但至关重要，能帮助医生及时干预，改善患者治疗效果。传统诊断方法面临症状与其他肺部疾病重叠、影像特征随时间变化等挑战，而现有AI方法（如CNN）性能有限。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CNN在RILI诊断中的局限性，转而探索视觉变换器（ViT）等先进模型，因为它们能更好地捕捉全局图像信息。他们借鉴了LoRA（低秩自适应）技术，这是一种高效微调大型模型的方法。作者还参考了自己之前使用CNN诊断RILI的工作，以及其他研究使用放射组学预测RILI的方法。设计上，他们比较了三种微调策略（不微调、完全微调、LoRA微调），并测试了不同输入方式（2D切片和3D正交信息）和图像大小对模型性能的影响。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用LoRA技术微调大型视觉模型（DinoV2和SwinV2），只更新少量参数就能有效适应医学影像诊断任务，同时保留模型从自然图像中学到的丰富特征。整体流程包括：1）收集并预处理CT扫描数据，包括标准化分辨率、对齐、裁剪治疗区域等；2）设计两种输入方式（2D轴向切片和3D正交切片）；3）应用三种微调策略（不微调、完全微调、LoRA微调）；4）使用五折交叉验证训练和评估模型；5）在独立测试集上评估性能，使用ROC-AUC、F1分数等指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将大型视觉模型应用于SBRT后RILI分类；引入LoRA微调技术大幅降低计算成本；评估不同输入方式和图像大小对性能的影响；在具有挑战性的子集（如治疗后早期小病灶）上验证模型。相比之前工作，本研究使用更先进的视觉变换器代替CNN，直接处理原始影像而非手工特征，使用更大数据集，并通过LoRA实现了参数高效的微调，在保持性能的同时显著减少了训练时间和计算资源需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了使用LoRA微调的大型视觉模型能够高效准确地诊断SBRT后的辐射诱导肺损伤，显著减少了计算成本和训练时间，同时保持了与完全微调相当或更好的性能，为临床决策提供了AI支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the efficacy of Low-Rank Adaptation (LoRA) forfine-tuning large Vision Models, DinoV2 and SwinV2, to diagnoseRadiation-Induced Lung Injury (RILI) from X-ray CT scans following StereotacticBody Radiation Therapy (SBRT). To evaluate the robustness and efficiency ofthis approach, we compare LoRA with traditional full fine-tuning andinference-only (no fine-tuning) methods. Cropped images of two sizes (50 mm3and 75 mm3), centered at the treatment isocenter, in addition to differentadaptation techniques for adapting the 2D LVMs for 3D data were used todetermine the sensitivity of the models to spatial context. Experimentalresults show that LoRA achieves comparable or superior performance totraditional fine-tuning while significantly reducing computational costs andtraining times by requiring fewer trainable parameters.</description>
      <author>example@mail.com (M. Bolhassani, B. Veasey, E. Daugherty, S. Keltner, N. Kumar, N. Dunlap, A. Amini)</author>
      <guid isPermaLink="false">2509.12155v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multi Anatomy X-Ray Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了XR-0，这是一个多解剖部位的X射线基础模型，通过在大规模私有数据集上进行自监督学习训练，在多种临床任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;X射线影像在放射学中无处不在，但现有的AI基础模型大多局限于胸部解剖结构，无法在更广泛的临床任务中泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多种解剖区域和临床任务泛化的X射线基础模型。&lt;h4&gt;方法&lt;/h4&gt;介绍XR-0模型，使用自监督学习方法，在包含115万张图像的大规模私有数据集上进行训练，这些图像涵盖不同的解剖区域。该模型在12个数据集和20个下游任务上进行了评估，包括分类、检索、分割、定位、视觉定位和报告生成。&lt;h4&gt;主要发现&lt;/h4&gt;XR-0在大多数多解剖部位任务上达到了最先进的性能，并且在胸部特定基准测试中保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;解剖多样性和监督对于构建健壮的通用医疗视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;X射线影像在放射学中无处不在，然而大多数现有的AI基础模型仅限于胸部解剖结构，无法在更广泛的临床任务中泛化。在这项工作中，我们介绍了XR-0，这是一个多解剖部位的X射线基础模型，它通过在包含115万张图像的大规模私有数据集上进行自监督学习训练，这些图像涵盖多种解剖区域，并在12个数据集和20个下游任务上进行了评估，包括分类、检索、分割、定位、视觉定位和报告生成。XR-0在大多数多解剖部位任务上达到了最先进的性能，并在胸部特定基准测试中保持竞争力。我们的结果表明，解剖多样性和监督对于构建健壮的通用医疗视觉模型至关重要，为放射学中可扩展和适应性强的AI系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; X-ray imaging is a ubiquitous in radiology, yet most existing AI foundationmodels are limited to chest anatomy and fail to generalize across broaderclinical tasks. In this work, we introduce XR-0, the multi-anatomy X-rayfoundation model using self-supervised learning on a large, private dataset of1.15 million images spanning diverse anatomical regions and evaluated across 12datasets and 20 downstream tasks, including classification, retrieval,segmentation, localization, visual grounding, and report generation. XR-0achieves state-of-the-art performance on most multi-anatomy tasks and remainscompetitive on chest-specific benchmarks. Our results demonstrate thatanatomical diversity and supervision are critical for building robust,general-purpose medical vision models, paving the way for scalable andadaptable AI systems in radiology.</description>
      <author>example@mail.com (Nishank Singla, Krisztian Koos, Farzin Haddadpour, Amin Honarmandi Shandiz, Lovish Chum, Xiaojian Xu, Qing Jin, Erhan Bas)</author>
      <guid isPermaLink="false">2509.12146v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Navigation Foundation Model</title>
      <link>http://arxiv.org/abs/2509.12129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://pku-epic.github.io/NavFoM-Web/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一个跨具身和跨任务的导航基础模型(NavFoM)，该模型在多种具身(四足动物、无人机、轮式机器人和车辆)和多种导航任务(视觉语言导航、目标搜索、目标跟踪和自动驾驶)上表现出强大的泛化能力，无需任务特定的微调即可达到最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;尽管大型视觉-语言模型(VLMs)在通用视觉-语言任务上表现出显著的零样本性能，但它们在具身导航中的泛化能力仍然主要局限于狭窄的任务设置和特定的具身架构。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨不同具身架构和多种导航任务泛化的导航基础模型，解决当前具身AI导航系统泛化能力有限的问题。&lt;h4&gt;方法&lt;/h4&gt;NavFoM采用统一架构处理多模态导航输入，集成了标识符令牌来嵌入具身的摄像头视图信息和任务的时间上下文，并在有限的令牌长度预算下使用动态调整的采样策略控制所有观测令牌。该模型在八百万个导航样本上进行了训练，涵盖了四足动物、无人机、轮式机器人和车辆等多种具身。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的广泛评估表明，NavFoM在多个导航任务和具身上实现了最先进或极具竞争力的性能，无需任务特定的微调。额外的真实世界实验进一步证实了该模型具有强大的泛化能力和实际适用性。&lt;h4&gt;结论&lt;/h4&gt;NavFoM代表了具身AI导航领域的重要进展，通过统一的架构和创新的令牌处理策略，实现了跨具身和跨任务的强大泛化能力，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;导航是具身AI中的基本能力，代表遵循语言指令在物理环境中感知和互动所需的智能。尽管大型视觉-语言模型(VLMs)在通用视觉-语言任务上表现出显著的零样本性能，但它们在具身导航中的泛化能力仍然主要局限于狭窄的任务设置和特定的具身架构。在这项工作中，我们引入了一个跨具身和跨任务的导航基础模型(NavFoM)，该模型在八百万个导航样本上进行了训练，这些样本涵盖了四足动物、无人机、轮式机器人和车辆，并跨越了视觉语言导航、目标搜索、目标跟踪和自动驾驶等多种任务。NavFoM采用统一架构，处理来自不同摄像头配置和导航范围的多模态导航输入。为了适应不同的摄像头设置和时间范围，NavFoM集成了标识符令牌，这些令牌嵌入具身的摄像头视图信息和任务的时间上下文。此外，为了满足实际部署的需求，NavFoM在有限的令牌长度预算下，使用动态调整的采样策略控制所有观测令牌。在公共基准上的广泛评估表明，我们的模型在多个导航任务和具身上实现了最先进或极具竞争力的性能，而无需任务特定的微调。额外的真实世界实验进一步证实了我们方法的强大泛化能力和实际适用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决具身导航模型在跨具身形态和跨任务上的泛化能力不足问题。现有导航模型要么局限于特定具身形态，要么只能处理特定任务，缺乏通用性。这个问题很重要，因为导航是具身AI的基础能力，一个通用的导航模型可以适应各种机器人和应用场景，减少定制化需求，降低开发和部署成本，并更好地适应真实世界的复杂性和多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类主要通过视觉完成导航任务的启发，以及最近仅视觉导航方法的成功，将通用导航任务表述为处理第一视角视频和语言指令并预测轨迹。他们借鉴了视觉语言模型的基本架构、Transformer-based在大规模跨具身数据上的训练方法、视觉特征缓存机制和位置编码等技术。创新性地设计了时间-视角指示器(TVI)tokens来标识摄像头视角和时序信息，以及基于预算的时序采样(BATS)策略来处理大量视频帧，平衡性能和推理速度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个统一的导航基础模型，能够处理不同具身形态和不同任务的导航问题。整体流程包括：1)使用预训练视觉编码器提取视觉特征并通过网格池化生成紧凑表示；2)使用TVI tokens编码摄像头视角和时序信息；3)采用BATS策略动态采样历史帧；4)组织视觉和语言令牌并通过LLM和规划模型预测轨迹；5)在大规模数据(800万导航样本和476万开放世界知识样本)上训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨具身和跨任务的统一导航框架；2)时间-视角指示器(TVI)tokens显式编码摄像头视角和时序信息；3)基于预算的时序采样(BATS)策略平衡性能和推理速度；4)大规模多样化数据集训练。相比之前工作，NavFoM不仅能跨不同具身形态泛化，还能处理多种导航任务；显式编码了多视角信息，解决了输入歧义；优化了长序列处理；直接预测轨迹而非生成文本描述；处理范围更广，包括自动驾驶和UAV导航等复杂场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; NavFoM通过引入时间-视角指示器tokens和基于预算的时序采样策略，创建了一个能够跨具身形态和跨任务泛化的统一导航基础模型，在多种导航任务和具身形态上实现了最先进或极具竞争力的性能，无需任务特定的微调。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigation is a fundamental capability in embodied AI, representing theintelligence required to perceive and interact within physical environmentsfollowing language instructions. Despite significant progress in largeVision-Language Models (VLMs), which exhibit remarkable zero-shot performanceon general vision-language tasks, their generalization ability in embodiednavigation remains largely confined to narrow task settings andembodiment-specific architectures. In this work, we introduce across-embodiment and cross-task Navigation Foundation Model (NavFoM), trainedon eight million navigation samples that encompass quadrupeds, drones, wheeledrobots, and vehicles, and spanning diverse tasks such as vision-and-languagenavigation, object searching, target tracking, and autonomous driving. NavFoMemploys a unified architecture that processes multimodal navigation inputs fromvarying camera configurations and navigation horizons. To accommodate diversecamera setups and temporal horizons, NavFoM incorporates identifier tokens thatembed camera view information of embodiments and the temporal context of tasks.Furthermore, to meet the demands of real-world deployment, NavFoM controls allobservation tokens using a dynamically adjusted sampling strategy under alimited token length budget. Extensive evaluations on public benchmarksdemonstrate that our model achieves state-of-the-art or highly competitiveperformance across multiple navigation tasks and embodiments without requiringtask-specific fine-tuning. Additional real-world experiments further confirmthe strong generalization capability and practical applicability of ourapproach.</description>
      <author>example@mail.com (Jiazhao Zhang, Anqi Li, Yunpeng Qi, Minghan Li, Jiahang Liu, Shaoan Wang, Haoran Liu, Gengze Zhou, Yuze Wu, Xingxing Li, Yuxin Fan, Wenjun Li, Zhibo Chen, Fei Gao, Qi Wu, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2509.12129v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>FS-SAM2: Adapting Segment Anything Model 2 for Few-Shot Semantic Segmentation via Low-Rank Adaptation</title>
      <link>http://arxiv.org/abs/2509.12105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICIAP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于SAM2的少样本分割方法(FS-SAM2)，该方法利用SAM2的视频能力直接用于少样本任务，并通过低秩适应(LoRA)处理标准数据集中的多样化图像，只需少量参数进行元训练，在多个数据集上取得了显著结果且推理效率高。&lt;h4&gt;背景&lt;/h4&gt;少样本语义分割最近受到广泛关注，目标是用少量标注样本分割未见类别。现有方法通常需要从头训练额外模块并在大型数据集上大量训练才能达到最佳性能。SAM2是一个用于零样本图像和视频分割的基础模型，采用模块化设计。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于SAM2的少样本分割方法，将SAM2的视频能力直接应用于少样本任务，通过LoRA处理标准数据集中的多样化图像，仅用少量参数进行元训练，支持任何K-shot配置。&lt;h4&gt;方法&lt;/h4&gt;提出FS-SAM2方法，直接利用SAM2的视频能力处理少样本任务，应用低秩适应(LoRA)到原始模块以处理标准数据集中的多样化图像，只对少量参数进行元训练以适应SAM2。&lt;h4&gt;主要发现&lt;/h4&gt;在PASCAL-5^i、COCO-20^i和FSS-1000数据集上评估FS-SAM2取得了显著结果，在推理过程中表现出优秀的计算效率，代码已开源。&lt;h4&gt;结论&lt;/h4&gt;FS-SAM2是一种有效的少样本语义分割方法，能够有效利用SAM2的基础模型能力，在多个数据集上取得了良好性能且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;少样本语义分割最近受到了广泛关注。其目标是开发一个仅使用少量标注样本就能分割未见类别的模型。大多数现有方法通过从头训练一个额外模块来调整预训练模型。这些方法需要在大型数据集上进行大量训练才能达到最佳性能。Segment Anything Model 2 (SAM2) 是一个用于零样本图像和视频分割的基础模型，采用模块化设计。在本文中，我们提出了一种基于SAM2的少样本分割方法(FS-SAM2)，其中SAM2的视频能力被直接用于少样本任务。此外，我们对原始模块应用了低秩适应(Low-Rank Adaptation, LoRA)，以处理标准数据集中常见的多样化图像，这与SAM2预训练中使用的时间连接帧不同。通过这种方法，只有少量参数进行元训练，从而有效适应SAM2，同时受益于其出色的分割性能。我们的方法支持任何K-shot配置。我们在PASCAL-5^i、COCO-20^i和FSS-1000数据集上评估了FS-SAM2，取得了显著结果，并在推理过程中表现出优秀的计算效率。代码可在https://github.com/fornib/FS-SAM2获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot semantic segmentation has recently attracted great attention. Thegoal is to develop a model capable of segmenting unseen classes using only afew annotated samples. Most existing approaches adapt a pre-trained model bytraining from scratch an additional module. Achieving optimal performance withthese approaches requires extensive training on large-scale datasets. TheSegment Anything Model 2 (SAM2) is a foundational model for zero-shot image andvideo segmentation with a modular design. In this paper, we propose a Few-Shotsegmentation method based on SAM2 (FS-SAM2), where SAM2's video capabilitiesare directly repurposed for the few-shot task. Moreover, we apply a Low-RankAdaptation (LoRA) to the original modules in order to handle the diverse imagestypically found in standard datasets, unlike the temporally connected framesused in SAM2's pre-training. With this approach, only a small number ofparameters is meta-trained, which effectively adapts SAM2 while benefiting fromits impressive segmentation performance. Our method supports any K-shotconfiguration. We evaluate FS-SAM2 on the PASCAL-5$^i$, COCO-20$^i$ andFSS-1000 datasets, achieving remarkable results and demonstrating excellentcomputational efficiency during inference. Code is available athttps://github.com/fornib/FS-SAM2</description>
      <author>example@mail.com (Bernardo Forni, Gabriele Lombardi, Federico Pozzi, Mirco Planamente)</author>
      <guid isPermaLink="false">2509.12105v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Time-Series Foundation Model by Universal Delay Embedding</title>
      <link>http://arxiv.org/abs/2509.12080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为'通用延迟嵌入'(UDE)的预训练基础模型，通过整合延迟嵌入表示和Koopman算子预测来革新时间序列预测，在各种基准和真实世界数据集上表现出色，具有可扩展性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在科学和工业领域具有重要应用价值，但传统方法在处理非线性时间序列时面临挑战，需要更有效的方法来捕捉动力系统的动态和拓扑特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测非线性时间序列的框架，同时保持良好的可解释性和泛化能力，适用于广泛的科学和工业应用。&lt;h4&gt;方法&lt;/h4&gt;利用Takens嵌入定理，将观测数据构造为动力系统表示，从Hankel矩阵构建二维子空间补丁，将这些补丁视为图像并通过先进深度学习技术处理，使用自注意力编码器学习这些补丁，在潜在空间中以线性方式学习有限维Koopman算子进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;在各类基准和气候数据集上评估显示，UDE比最先进的基础模型平均降低20%以上的均方误差，在微调场景中表现出更好的泛化能力；学习到的动力表示和Koopman算子预测具有卓越的可解释性，能够一致识别拓扑信息丰富的子空间并稳健编码领域不变动力。&lt;h4&gt;结论&lt;/h4&gt;UDE确立了一种可扩展、可解释的通用时间序列建模和预测框架，具有广泛的科学和工业应用价值，为时间序列分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;本研究引入了通用延迟嵌入(UDE)，这是一种预训练基础模型，旨在通过有原则地整合延迟嵌入表示和Koopman算子预测来革新时间序列预测。利用Takens嵌入定理，UDE作为观测数据的动力表示，从Hankel矩阵构建二维子空间补丁，理论上保留了底层动力系统的动力和拓扑特性。这些补丁被视为图像，可以通过利用先进的深度学习技术高效处理。计算上，这些补丁进一步作为令牌用于学习自注意力编码器，从而在潜在空间中以线性方式通过有限维Koopman算子实现非线性时间序列的准确预测。在各种基准和真实世界气候数据集上的广泛评估表明，与最先进的基础模型相比，平均均方误差降低了20%以上，同时在微调场景中表现出更好的泛化能力。特别是，学习到的动力表示和从补丁中形成的Koopman算子预测表现出卓越的可解释性，能够一致识别拓扑信息丰富的子空间并稳健编码领域不变动力，确立了UDE作为可扩展、可解释的通用时间序列建模和预测框架，具有广泛的科学和工业应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces Universal Delay Embedding (UDE), a pretrainedfoundation model designed to revolutionize time-series forecasting throughprincipled integration of delay embedding representation and Koopman operatorprediction. Leveraging Takens' embedding theorem, UDE as a dynamicalrepresentation of observed data constructs two-dimensional subspace patchesfrom Hankel matrices, theoretically preserving dynamical and topologicalproperties of underlying dynamical systems. Such patches are viewed as images,which can be efficiently processed by exploiting advanced deep learningtechnologies. Computationally, these patches further serve as tokens forlearning a self-attention encoder, thus enabling accurate prediction ofnonlinear time-series by a finite-dimensional Koopman operator in a linearmanner in a latent space. Extensive evaluations across various benchmarks andreal-world climate datasets demonstrate over 20% average reduction in meansquared error versus state-of-the-art foundation models, alongside superiorgeneralization in fine-tuning scenarios. In particular, the learned dynamicalrepresentations and Koopman operator prediction forms from the patches exhibitexceptional interpretability, with consistent identification of topologicallyinformative subspaces and robust encoding of domain-invariant dynamics,establishing UDE as a scalable, interpretable framework for universaltime-series modeling and forecasting with broad scientific and industrialapplicability.</description>
      <author>example@mail.com (Zijian Wang, Peng Tao, Jifan Shi, Rui Bao, Rui Liu, Luonan Chen)</author>
      <guid isPermaLink="false">2509.12080v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>LEGO: Spatial Accelerator Generation and Optimization for Tensor Applications</title>
      <link>http://arxiv.org/abs/2509.12053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The first two authors have equal contributions; Published as a  conference paper in HPCA 2025; 13 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LEGO框架是一种针对张量应用的新方法，能够自动生成空间架构设计和可综合的RTL代码，无需手工RTL设计模板，解决了现有框架在设计灵活性和RTL生成生产力之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;现代张量应用，特别是基础模型和生成式AI应用需要多种输入模态（视觉和语言），这增加对灵活加速器架构的需求。现有框架要么仅限于少数手工编写的模板，要么无法自动生成RTL。&lt;h4&gt;目的&lt;/h4&gt;提出LEGO框架，针对张量应用，自动生成空间架构设计并输出可综合的RTL代码，无需手工RTL设计模板，解决设计灵活性和RTL生成生产力之间的权衡问题。&lt;h4&gt;方法&lt;/h4&gt;利用基于仿射变换的架构表示，LEGO前端找到功能单元之间的互连，合成存储系统，并根据数据重用分析融合不同的空间数据流设计；LEGO后端在原始级别图上翻译硬件以执行低级优化，应用线性规划算法最优插入流水线寄存器，减少切换空间数据流时的未使用逻辑开销。&lt;h4&gt;主要发现&lt;/h4&gt;LEGO相比之前的工作Gemmini可以实现3.2倍的速度提升和2.4倍的能效提升，能够为生成式AI应用中的各种现代基础模型生成一个架构。&lt;h4&gt;结论&lt;/h4&gt;LEGO框架有效地解决了设计灵活性和RTL生成生产力之间的权衡问题，能够高效地为现代张量应用生成优化的硬件架构。&lt;h4&gt;翻译&lt;/h4&gt;现代张量应用，特别是基础模型和生成式AI应用需要多种输入模态（视觉和语言），这增加对灵活加速器架构的需求。现有框架在设计灵活性和RTL生成生产力之间存在权衡：要么仅限于少数手工编写的模板，要么无法自动生成RTL。为解决这一挑战，我们提出了LEGO框架，该框架针对张量应用，自动生成空间架构设计并输出可综合的RTL代码，无需手工RTL设计模板。利用基于仿射变换的架构表示，LEGO前端找到功能单元之间的互连，合成存储系统，并根据数据重用分析融合不同的空间数据流设计。LEGO后端然后在原始级别图上翻译硬件以执行低级优化，并应用一组线性规划算法以最优方式插入流水线寄存器，减少切换空间数据流时未使用逻辑的开销。我们的评估表明，与之前的工作Gemmini相比，LEGO可以实现3.2倍的速度提升和2.4倍的能效提升，并且可以为生成式AI应用中的各种现代基础模型生成一个架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/HPCA61900.2025.00101&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern tensor applications, especially foundation models and generative AIapplications require multiple input modalities (both vision and language),which increases the demand for flexible accelerator architecture. Existingframeworks suffer from the trade-off between design flexibility andproductivity of RTL generation: either limited to very few hand-writtentemplates or cannot automatically generate the RTL. To address this challenge,we propose the LEGO framework, which targets tensor applications andautomatically generates spatial architecture design and outputs synthesizableRTL code without handwritten RTL design templates. Leveraging theaffine-transformation-based architecture representation, LEGO front end findsinterconnections between function units, synthesizes the memory system, andfuses different spatial dataflow designs based on data reuse analysis. LEGOback end then translates the hardware in a primitive-level graph to performlower-level optimizations, and applies a set of linear-programming algorithmsto optimally insert pipeline registers and reduce the overhead of unused logicwhen switching spatial dataflows. Our evaluation demonstrates that LEGO canachieve 3.2x speedup and 2.4x energy efficiency compared to previous workGemmini, and can generate one architecture for diverse modern foundation modelsin generative AI applications.</description>
      <author>example@mail.com (Yujun Lin, Zhekai Zhang, Song Han)</author>
      <guid isPermaLink="false">2509.12053v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Radio Galaxy Zoo: Morphological classification by Fanaroff-Riley designation using self-supervised pre-training</title>
      <link>http://arxiv.org/abs/2509.11988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MNRAS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用RGZ项目中的14,000多个射电星系，对约5,900个FRI型和8,100个FRII型射电星系进行了分类。研究分析了使用预训练并微调的射电星系基础模型预测的形态，发现了FRI和FRII光度-大小分布的重叠区域，模型在这些区域的置信度较低。研究还探讨了预训练和微调数据选择对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;Radio Galaxy Zoo (RGZ)项目提供了大量射电星系数据，射电星系的形态分类（特别是Fanaroff-Riley分类）是天文学研究中的重要课题。随着自动化分类方法的发展，了解模型训练数据选择对结果的影响变得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;分析RGZ目录中预测的射电星系形态，评估预训练并微调的射电星系基础模型在FR形态分类任务上的表现，并探讨训练数据选择对模型输出的影响。&lt;h4&gt;方法&lt;/h4&gt;使用预训练的射电星系基础模型，并针对预测Fanaroff-Riley形态的任务进行了微调。对RGZ项目中的14,000多个射电星系进行分类，并对约5,900个FRI型和8,100个FRII型射电星系进行分析。研究还考察了预训练和微调数据选择对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. FRI和FRII形态分类的光度-大小分布存在重叠；2. 模型在重叠区域的预测置信度最低，表明源形态在这些区域更为模糊；3. 发现了低光度FRII源，其比例与之前的研究一致；4. 本研究发现的低光度FRII源与之前研究识别的源存在差异，可能受分类方法影响；5. 不同的预训练数据选择会影响模型置信度，但不会引起系统性的泛化偏差；6. 微调数据的选择可能对模型有不同影响。&lt;h4&gt;结论&lt;/h4&gt;随着天体源识别和分类的自动化方法日益普及，训练数据的选择会显著影响模型输出并可能传播到下游分析中，因此需要谨慎选择训练数据。&lt;h4&gt;翻译&lt;/h4&gt;在本研究中，我们检查了从Radio Galaxy Zoo (RGZ)项目中精心挑选的14,000多个射电星系，并为约5,900个FRI型和8,100个FRII型射电星系提供了分类。我们展示了使用经过微调以预测Fanaroff-Riley (FR)形态的预训练射电星系基础模型对RGZ目录中预测的射电星系形态的分析。如先前研究所示，我们的结果表明形态分类的FRI和FRII光度-大小分布存在重叠，我们发现模型对其预测的置信度在这一重叠区域最低，表明源形态更加模糊。我们确定了低光度FRII源的存在，其相对于FRII总数的比例与先前研究一致。然而，将本研究发现的低光度FRII源与先前研究确定的源进行比较，揭示了差异，这可能表明它们的选择受到分类方法选择的影响。我们研究了预训练和微调数据选择对下游分类任务模型性能的影响，表明虽然不同的预训练数据选择会影响模型置信度，但它们似乎不会引起所考虑的物理和观测特性范围内的系统性泛化偏差；然而，我们注意到对于微调情况可能并非如此。随着天体源识别和分类的自动化方法日益普及，我们强调了可能影响模型输出并传播到下游分析的训练数据选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we examine over 14,000 radio galaxies finely selected fromRadio Galaxy Zoo (RGZ) project and provide classifications for approximately5,900 FRIs and 8,100 FRIIs. We present an analysis of these predicted radiogalaxy morphologies for the RGZ catalogue, classified using a pre-trained radiogalaxy foundation model that has been fine-tuned to predict Fanaroff-Riley (FR)morphology. As seen in previous studies, our results show overlap betweenmorphologically classified FRI and FRII luminosity-size distributions and wefind that the model's confidence in its predictions is lowest in this overlapregion, suggesting that source morphologies are more ambiguous. We identify thepresence of low-luminosity FRII sources, the proportion of which, with respectto the total number of FRIIs, is consistent with previous studies. However, acomparison of the low-luminosity FRII sources found in this work with thoseidentified by previous studies reveals differences that may indicate theirselection is influenced by the choice of classification methodology. Weinvestigate the impacts of both pre-training and fine-tuning data selection onmodel performance for the downstream classification task, and show that whiledifferent pre-training data choices affect model confidence they do not appearto cause systematic generalisation biases for the range of physical andobservational characteristics considered in this work; however, we note thatthe same is not necessarily true for fine-tuning. As automated approaches toastronomical source identification and classification become increasinglyprevalent, we highlight training data choices that can affect the model outputsand propagate into downstream analyses.</description>
      <author>example@mail.com (Nutthawara Buatthaisong, Inigo Val Slijepcevic, Anna M. M. Scaife, Micah Bowles, Andrew Hopkins, Devina Mohan, Stanislav S Shabala, O. Ivy Wong)</author>
      <guid isPermaLink="false">2509.11988v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Low-rank Orthogonalization for Large-scale Matrix Optimization with Applications to Foundation Model Training</title>
      <link>http://arxiv.org/abs/2509.11983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了低秩正交化方法，用于改进神经网络训练中的矩阵优化问题，并通过实验和理论分析证明了其优越性。&lt;h4&gt;背景&lt;/h4&gt;神经网络训练本质上是一个大规模矩阵优化问题，但长期以来神经网络参数的矩阵结构被忽视了。Muon优化器因其显式利用这种结构而在基础模型训练中表现出色。&lt;h4&gt;目的&lt;/h4&gt;提出低秩正交化方法，利用神经网络训练过程中梯度的低秩特性，改进Muon优化器的性能。&lt;h4&gt;方法&lt;/h4&gt;提出低秩正交化，基于此提出低秩矩阵符号梯度下降和Muon的低秩变体。&lt;h4&gt;主要发现&lt;/h4&gt;低秩正交化具有优越的性能，低秩Muon在GPT-2和LLaMA预训练中超过了基础Muon的性能。理论上，建立了低秩矩阵符号梯度下降和低秩Muon的迭代复杂性。&lt;h4&gt;结论&lt;/h4&gt;低秩正交化是一种有效的神经网络训练方法，能够显著提升优化性能。&lt;h4&gt;翻译&lt;/h4&gt;神经网络训练本质上是一个大规模矩阵优化问题，但长期以来神经网络参数的矩阵结构被忽视了。最近，名为Muon的优化器因其显式利用这种结构而在基础模型训练中表现出色，引起了广泛关注。Muon成功的一个关键组成部分是矩阵正交化。在这篇论文中，作者提出了'低秩正交化'，它明确利用了神经网络训练过程中梯度的低秩特性。基于此，他们提出了低秩矩阵符号梯度下降和Muon的低秩变体。数值实验表明，低秩正交化具有优越的性能，低秩Muon在GPT-2和LLaMA预训练中取得了有希望的结果，超过了精心调整的基础Muon的性能。理论上，他们建立了低秩矩阵符号梯度下降寻找近似平稳解的迭代复杂性，以及低秩Muon在重尾噪声下寻找近似随机平稳解的迭代复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural network (NN) training is inherently a large-scale matrix optimizationproblem, yet the matrix structure of NN parameters has long been overlooked.Recently, the optimizer Muon \cite{jordanmuon}, which explicitly exploits thisstructure, has gained significant attention for its strong performance infoundation model training. A key component contributing to Muon's success ismatrix orthogonalization. In this paper, we propose {\it low-rankorthogonalization}, which explicitly leverages the low-rank nature of gradientsduring NN training. Building on this, we propose low-rank matrix-signedgradient descent and a low-rank variant of Muon. Our numerical experimentsdemonstrate the superior performance of low-rank orthogonalization, with thelow-rank Muon achieving promising results in GPT-2 and LLaMA pretraining --surpassing the performance of the carefully tuned vanilla Muon. Theoretically,we establish the iteration complexity of the low-rank matrix-signed gradientdescent for finding an approximate stationary solution, as well as that oflow-rank Muon for finding an approximate stochastic stationary solution underheavy-tailed noise.</description>
      <author>example@mail.com (Chuan He, Zhanwang Deng, Zhaosong Lu)</author>
      <guid isPermaLink="false">2509.11983v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>MusicSwarm: Biologically Inspired Intelligence for Music Composition</title>
      <link>http://arxiv.org/abs/2509.11973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了一种去中心化的音乐创作系统，由相同的冻结基础模型组成，通过基于信息素的点对点信号协调，无需权重更新即可生成连贯的长篇音乐作品。&lt;h4&gt;背景&lt;/h4&gt;传统音乐创作系统通常采用集中式架构，而本研究探索了去中心化群体在音乐创作中的应用潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需权重更新的去中心化音乐创作系统，通过信息素协调机制生成连贯的长篇音乐作品，并探索其在创意生成中的优势。&lt;h4&gt;方法&lt;/h4&gt;使用相同的冻结基础模型组成群体，通过基于信息素的点对点信号协调；比较集中式多智能体系统与完全去中心化群体；进行符号、音频和图论分析评估结果。&lt;h4&gt;主要发现&lt;/h4&gt;去中心化群体系统产生了更高质量、更大多样性和结构变化的作品；系统动态趋于稳定的互补角色配置；自相似性网络揭示了小世界架构；局部新颖性能够整合为全球音乐形式。&lt;h4&gt;结论&lt;/h4&gt;MusicSwarm通过将专业化从参数更新转向交互规则、共享记忆和动态共识，为长时程创造性结构提供了计算和数据高效的途径，可直接应用于音乐之外的协作写作、设计和科学发现。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了一种去中心化的群体，由相同的冻结基础模型组成，通过基于信息素的点对点信号协调，无需任何权重更新即可产生连贯的长篇音乐创作。我们将带有全局评论者的集中式多智能体系统与完全去中心化的群体进行比较，在后者中，小节级智能体感知并存储和声、节奏和结构线索，适应短期记忆，并达成共识。通过符号、音频和图论分析，群体系统产生了更高质量的作品，同时提供了更大的多样性和结构变化，并在创意指标上表现更优。系统动态趋于稳定的互补角色配置，自相似性网络揭示了具有高效远程连接和专门桥接模体的小世界架构，阐明了局部新颖性如何整合为全球音乐形式。通过将专业化从参数更新转向交互规则、共享记忆和动态共识，MusicSwarm为长时程创造性结构提供了一种计算和数据高效的途径，可直接应用于音乐之外的协作写作、设计和科学发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We show that coherent, long-form musical composition can emerge from adecentralized swarm of identical, frozen foundation models that coordinate viastigmergic, peer-to-peer signals, without any weight updates. We compare acentralized multi-agent system with a global critic to a fully decentralizedswarm in which bar-wise agents sense and deposit harmonic, rhythmic, andstructural cues, adapt short-term memory, and reach consensus. Across symbolic,audio, and graph-theoretic analyses, the swarm yields superior quality whiledelivering greater diversity and structural variety and leads across creativitymetrics. The dynamics contract toward a stable configuration of complementaryroles, and self-similarity networks reveal a small-world architecture withefficient long-range connectivity and specialized bridging motifs, clarifyinghow local novelties consolidate into global musical form. By shiftingspecialization from parameter updates to interaction rules, shared memory, anddynamic consensus, MusicSwarm provides a compute- and data-efficient route tolong-horizon creative structure that is immediately transferable beyond musicto collaborative writing, design, and scientific discovery.</description>
      <author>example@mail.com (Markus J. Buehler)</author>
      <guid isPermaLink="false">2509.11973v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>BREA-Depth: Bronchoscopy Realistic Airway-geometric Depth Estimation</title>
      <link>http://arxiv.org/abs/2509.11885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper has been accepted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Brea-Depth的新框架，用于支气管镜检查中的单目深度估计，通过整合气道特定的几何先验，提高了实时导航精度和手术安全性。&lt;h4&gt;背景&lt;/h4&gt;单目深度估计在支气管镜检查中可以显著提高复杂分支气道中的实时导航精度和手术安全性。然而，现有的深度基础模型在支气管镜场景中缺乏解剖学意识，容易过度拟合局部纹理而非捕获全局气道结构，特别是在深度线索模糊和光照条件差的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计支气管镜检查中深度的方法，特别是提高模型对气道结构的理解和处理能力，从而产生更准确、鲁棒的3D气道重建。&lt;h4&gt;方法&lt;/h4&gt;提出Brea-Depth框架，整合气道特定几何先验到基础模型适应中。方法包括：1) 引入深度感知的CycleGAN，优化真实支气管镜图像与解剖数据中气道几何之间的转换；2) 引入气道结构感知损失，强制气道腔内的深度一致性，同时保持平滑过渡和结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;Brea-Depth通过整合解剖先验，增强了模型泛化能力，产生了更鲁棒、准确的3D气道重建。在收集的离体人肺数据集和公开的支气管镜数据集上，该方法在解剖深度保存方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;Brea-Depth通过整合气道特定的几何先验，有效解决了现有深度基础模型在支气管镜场景中缺乏解剖学意识的问题，提高了深度估计的准确性和鲁棒性，有望改善支气管镜检查中的实时导航精度和手术安全性。&lt;h4&gt;翻译&lt;/h4&gt;支气管镜检查中的单目深度估计可以显著提高复杂分支气道中的实时导航精度和手术安全性。最近的深度基础模型进展在内镜场景中显示出前景，但这些模型通常缺乏支气管镜中的解剖学意识，过度拟合局部纹理而非捕获全局气道结构，特别是在深度线索模糊和光照条件差的情况下。为解决这一问题，我们提出了Brea-Depth，一种新颖的框架，将气道特定的几何先验整合到基础模型适应中，用于支气管镜深度估计。我们的方法引入了深度感知的CycleGAN，优化真实支气管镜图像与解剖数据中气道几何之间的转换，有效弥合了域差距。此外，我们引入了气道结构感知损失，强制气道腔内的深度一致性，同时保持平滑过渡和结构完整性。通过整合解剖先验，Brea-Depth增强了模型泛化能力，产生了更鲁棒、准确的3D气道重建。为评估解剖学真实性，我们引入了气道深度结构评估，一种新的结构一致性指标。我们在收集的离体人肺数据集和公开的支气管镜数据集上验证了Brea-Depth，它在解剖深度保存方面优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决支气管镜检查中的单目深度估计问题。现有方法缺乏解剖学意识，过度拟合局部纹理而非捕获全局气道结构，尤其在模糊深度线索和不良光照条件下表现不佳。这个问题很重要，因为准确的深度估计能显著提高实时导航精度，增强复杂分支气道中干预措施的安全性，对靶向活检或局部药物输送等需要结构精确深度的任务至关重要，同时能改进3D气道重建质量。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有深度基础模型在支气管镜场景中的局限性，认识到需要针对特定解剖结构进行优化。他们借鉴了CycleGAN进行图像转换、U-Net架构进行编码解码、以及基础模型提供伪深度监督等现有技术。在此基础上，他们创新性地开发了深度感知的CycleGAN专门针对支气管镜场景优化，并引入气道结构感知损失函数确保深度预测符合解剖学先验。这种设计既利用了现有方法的优势，又针对性地解决了支气管镜特有的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将气道特定的几何先验知识整合到基础深度估计模型中，创建专门针对支气管镜场景的深度估计框架，确保生成的深度图不仅在像素级准确，而且在解剖学上一致。整体流程包括：1)创建几何准确的3D气道模型；2)实现深度感知CycleGAN，包含合成到真实和真实到合成两个转换分支；3)引入气道结构感知损失函数强制气道管腔内深度一致性；4)使用合成和真实数据混合训练，最终实现60FPS的实时深度估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)BREA-Depth框架，首个将气道几何整合到基础深度估计中的专门框架；2)深度感知CycleGAN，直接将深度整合到域转换中；3)气道结构感知损失，强制气道管腔内深度一致性；4)气道深度结构评估指标，关注解剖一致性而非仅像素级准确性。相比之前工作，不同之处在于：不依赖CT生成的简化数据；具有解剖学意识关注全局结构；在模糊条件下表现更好；保留气道几何结构；引入专门评估指标更全面评估实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BREA-Depth通过整合气道特定的几何先验知识到深度估计基础模型中，首次实现了在支气管镜场景中既保持像素级准确性又确保解剖学一致性的深度预测，显著提升了复杂气道导航和3D重建的可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular depth estimation in bronchoscopy can significantly improvereal-time navigation accuracy and enhance the safety of interventions incomplex, branching airways. Recent advances in depth foundation models haveshown promise for endoscopic scenarios, yet these models often lack anatomicalawareness in bronchoscopy, overfitting to local textures rather than capturingthe global airway structure, particularly under ambiguous depth cues and poorlighting. To address this, we propose Brea-Depth, a novel framework thatintegrates airway-specific geometric priors into foundation model adaptationfor bronchoscopic depth estimation. Our method introduces a depth-awareCycleGAN, refining the translation between real bronchoscopic images and airwaygeometries from anatomical data, effectively bridging the domain gap. Inaddition, we introduce an airway structure awareness loss to enforce depthconsistency within the airway lumen while preserving smooth transitions andstructural integrity. By incorporating anatomical priors, Brea-Depth enhancesmodel generalization and yields more robust, accurate 3D airwayreconstructions. To assess anatomical realism, we introduce Airway DepthStructure Evaluation, a new metric for structural consistency. We validateBREA-Depth on a collected ex vivo human lung dataset and an open bronchoscopicdataset, where it outperforms existing methods in anatomical depthpreservation.</description>
      <author>example@mail.com (Francis Xiatian Zhang, Emile Mackute, Mohammadreza Kasaei, Kevin Dhaliwal, Robert Thomson, Mohsen Khadem)</author>
      <guid isPermaLink="false">2509.11885v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Seg2Track-SAM2: SAM2-based Multi-object Tracking and Segmentation for Zero-shot Generalization</title>
      <link>http://arxiv.org/abs/2509.11772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Seg2Track-SAM2框架，结合预训练目标检测器、SAM2和新型Seg2Track模块，实现了无需微调且与检测器无关的多目标跟踪与分割(MOTS)系统，在基准测试中达到最先进性能并显著降低内存使用。&lt;h4&gt;背景&lt;/h4&gt;自主系统需要在动态环境中可靠运行，这要求具备强大的多目标跟踪(MOT)能力。虽然基础模型如SAM2在视频分割方面表现出强大的零样本泛化能力，但其直接应用于MOTS仍受身份管理和内存效率不足的限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解决MOTS问题的框架，结合强大的零样本跟踪能力、增强的身份保持和高效的内存利用，同时不需要微调且与检测器无关。&lt;h4&gt;方法&lt;/h4&gt;Seg2Track-SAM2框架整合了预训练的目标检测器、SAM2和一个新的Seg2Track模块，用于处理轨道初始化、轨道管理和强化。该方法采用滑动窗口内存策略，可将内存使用减少高达75%，且性能下降可忽略不计。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI MOT和KITTI MOTS基准测试上，Seg2Track-SAM2实现了最先进(SOTA)的性能，在KITTI MOTS上汽车和行人类别总体排名第四，并在关联准确率(AssA)方面建立了新基准。滑动窗口内存策略可在资源受限条件下支持部署。&lt;h4&gt;结论&lt;/h4&gt;Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。&lt;h4&gt;翻译&lt;/h4&gt;自主系统需要在动态环境中可靠运行，这要求具备强大的多目标跟踪(MOT)能力。MOT确保了对象身份分配的一致性和空间 delineation 的精确性。基础模型(如SAM2)的最新进展已在视频分割方面表现出强大的零样本泛化能力，但它们直接应用于MOTS(多目标跟踪+分割)仍受身份管理和内存效率不足的限制。本研究介绍了Seg2Track-SAM2，一个结合了预训练目标检测器、SAM2和新型Seg2Track模块的框架，用于解决轨道初始化、轨道管理和强化问题。所提出的方法无需微调，且与检测器无关。在KITTI MOT和KITTI MOTS基准测试上的实验结果表明，Seg2Track-SAM2实现了最先进(SOTA)的性能，在KITTI MOTS上汽车和行人类别总体排名第四，同时在关联准确率(AssA)方面建立了新基准。此外，滑动窗口内存策略可将内存使用减少高达75%，且性能下降可忽略不计，支持在资源受限条件下的部署。这些结果证实，Seg2Track-SAM2通过结合强大的零样本跟踪、增强的身份保持和高效的内存利用，推动了MOTS的发展。代码可在https://github.com/hcmr-lab/Seg2Track-SAM2获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous systems require robust Multi-Object Tracking (MOT) capabilities tooperate reliably in dynamic environments. MOT ensures consistent objectidentity assignment and precise spatial delineation. Recent advances infoundation models, such as SAM2, have demonstrated strong zero-shotgeneralization for video segmentation, but their direct application to MOTS(MOT+Segmentation) remains limited by insufficient identity management andmemory efficiency. This work introduces Seg2Track-SAM2, a framework thatintegrates pre-trained object detectors with SAM2 and a novel Seg2Track moduleto address track initialization, track management, and reinforcement. Theproposed approach requires no fine-tuning and remains detector-agnostic.Experimental results on KITTI MOT and KITTI MOTS benchmarks show thatSeg2Track-SAM2 achieves state-of-the-art (SOTA) performance, ranking fourthoverall in both car and pedestrian classes on KITTI MOTS, while establishing anew benchmark in association accuracy (AssA). Furthermore, a sliding-windowmemory strategy reduces memory usage by up to 75% with negligible performancedegradation, supporting deployment under resource constraints. These resultsconfirm that Seg2Track-SAM2 advances MOTS by combining robust zero-shottracking, enhanced identity preservation, and efficient memory utilization. Thecode is available at https://github.com/hcmr-lab/Seg2Track-SAM2</description>
      <author>example@mail.com (Diogo Mendonça, Tiago Barros, Cristiano Premebida, Urbano J. Nunes)</author>
      <guid isPermaLink="false">2509.11772v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Igniting VLMs toward the Embodied Space</title>
      <link>http://arxiv.org/abs/2509.11766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WALL-OSS是一个端到端的具身基础模型，通过大规模多模态预训练实现了具身感知的视觉语言理解、强大的语言-动作关联和稳健的操作能力，解决了现有视觉语言模型在空间和具身理解方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在语言和视觉方面取得了显著进展，但现有视觉语言模型在空间和具身理解方面仍然有限。将视觉语言模型转移到具身领域揭示了模态、预训练分布和训练目标之间的根本不匹配，使得动作理解和生成成为通往通用人工智能道路上的主要瓶颈。&lt;h4&gt;目的&lt;/h4&gt;介绍WALL-OSS模型，利用大规模多模态预训练实现具身感知的视觉语言理解、强大的语言-动作关联和稳健的操作能力，解决视觉语言模型到具身领域的转移问题。&lt;h4&gt;方法&lt;/h4&gt;采用紧密耦合的架构和多策略训练课程，实现统一跨层级思维链，在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。&lt;h4&gt;主要发现&lt;/h4&gt;WALL-OSS在复杂长期操作上取得高成功率，展示强大的指令遵循能力和复杂理解推理能力，性能优于强大的基线模型。&lt;h4&gt;结论&lt;/h4&gt;WALL-OSS为从视觉语言模型到具身基础模型提供了一条可靠且可扩展的路径，解决了模态间不匹配和训练目标不一致的问题。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型在语言和视觉方面显示出显著进展，但现有的视觉语言模型在空间和具身理解方面仍然有限。将视觉语言模型转移到具身领域揭示了模态、预训练分布和训练目标之间的根本不匹配，使得动作理解和生成成为通往通用人工智能道路上的主要瓶颈。我们引入了WALL-OSS，一个端到端的具身基础模型，它利用大规模多模态预训练实现(1)具身感知的视觉语言理解，(2)强大的语言-动作关联，和(3)稳健的操作能力。我们的方法采用紧密耦合的架构和多策略训练课程，实现了统一跨层级思维链——在单一可微分框架内无缝统一指令推理、子目标分解和细粒度动作合成。我们的结果表明，WALL-OSS在复杂的长期操作上取得高成功率，展示了强大的指令遵循能力、复杂理解和推理能力，并优于强大的基线模型，从而为从视觉语言模型到具身基础模型提供了一条可靠且可扩展的路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models show remarkable progress in language and vision,existing vision-language models (VLMs) still have limited spatial andembodiment understanding. Transferring VLMs to embodied domains revealsfundamental mismatches between modalities, pretraining distributions, andtraining objectives, leaving action comprehension and generation as a centralbottleneck on the path to AGI.  We introduce WALL-OSS, an end-to-end embodied foundation model that leverageslarge-scale multimodal pretraining to achieve (1) embodiment-awarevision-language understanding, (2) strong language-action association, and (3)robust manipulation capability.  Our approach employs a tightly coupled architecture and multi-strategiestraining curriculum that enables Unified Cross-Level CoT-seamlessly unifyinginstruction reasoning, subgoal decomposition, and fine-grained action synthesiswithin a single differentiable framework.  Our results show that WALL-OSS attains high success on complex long-horizonmanipulations, demonstrates strong instruction-following capabilities, complexunderstanding and reasoning, and outperforms strong baselines, therebyproviding a reliable and scalable path from VLMs to embodied foundation models.</description>
      <author>example@mail.com (Andy Zhai, Brae Liu, Bruno Fang, Chalse Cai, Ellie Ma, Ethan Yin, Hao Wang, Hugo Zhou, James Wang, Lights Shi, Lucy Liang, Make Wang, Qian Wang, Roy Gan, Ryan Yu, Shalfun Li, Starrick Liu, Sylas Chen, Vincent Chen, Zach Xu)</author>
      <guid isPermaLink="false">2509.11766v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Uniqueness Theorem for Distributed Computation under Physical Constraint</title>
      <link>http://arxiv.org/abs/2509.11754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文在网络内计算等极端环境中，针对通信效率、有限内存和健壮可扩展性之间的三难困境，提出了一种基于逻辑必然性的解决方案。作者建立了严格的公理系统，证明了对于具有幂等合并操作符的计算，存在一个最优范式——自描述并行流（SDPF），并证明了该范式的收敛性、图灵完备性和最小性。这项工作类似于CAP定理，但提供了分布式计算流中不可避免性的唯一性定理。&lt;h4&gt;背景&lt;/h4&gt;传统的计算模型通常抽象了物理硬件限制，但在网络内计算等极端环境中，这些限制变得不可违反，导致在通信效率、有限内存和健壮可扩展性之间形成尖锐的三难困境。现有的分布式范式虽然在其预期领域很强大，但并非为这种严格的体系设计，因此面临根本性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决网络内计算等极端环境中面临的三难困境，即通信效率、有限内存和健壮可扩展性之间的冲突，寻找最优的分布式计算范式。&lt;h4&gt;方法&lt;/h4&gt;建立了一个严格的公理系统来形式化物理约束，并通过数学证明推导出最优范式。作者证明了对于具有幂等合并操作符的广泛计算类别，存在一个独特的最优范式，并分析了该范式的性质。&lt;h4&gt;主要发现&lt;/h4&gt;1) 对于具有幂等合并操作符的计算，存在一个独特的最优范式；2) 任何满足所建立公理的系统都必须收敛到自描述并行流（SDPF）这一单一标准形式；3) SDPF范式是收敛的、图灵完备的且是最小的；4) 类似于CAP定理，本文提供了一个唯一性定理，揭示了在物理定律下分布式计算流中什么是不可避免的。&lt;h4&gt;结论&lt;/h4&gt;解决极端环境中的计算三难困境需要转变视角，从寻求工程权衡到从逻辑必然性推导解决方案。自描述并行流（SDPF）是一个基于物理约束推导出的最优范式，它为分布式计算流提供了不可避免的设计原则。&lt;h4&gt;翻译&lt;/h4&gt;计算的基础模型通常抽象了物理硬件限制。然而，在网络内计算等极端环境中，这些限制变得不可违反的定律，在通信效率、有限内存和健壮可扩展性之间形成尖锐的三难困境。现有的分布式范式虽然在其预期领域很强大，但并非为这种严格的体系设计，因此面临根本性挑战。本文表明，解决这一三难困境需要转变视角——从寻求工程权衡到从逻辑必然性推导解决方案。我们建立了一个严格的公理系统，形式化了这些物理约束，并证明对于具有幂等合并操作符的广泛计算类别，存在一个独特的最优范式。任何满足这些公理的系统都必须收敛到一个单一的标准形式：自描述并行流（SDPF），这是一种纯数据中心的模型，其中无状态执行器处理携带自身控制逻辑的流。我们进一步证明这种独特范式是收敛的、图灵完备的且是最小的。就像CAP定理为分布式状态管理中不可能实现的事情建立了边界一样，我们的工作提供了一个建设性的对偶：一个唯一性定理，揭示了在物理定律下分布式计算流中什么是不可避免的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models of computation often abstract away physical hardwarelimitations. However, in extreme environments like In-Network Computing (INC),these limitations become inviolable laws, creating an acute trilemma amongcommunication efficiency, bounded memory, and robust scalability. Prevailingdistributed paradigms, while powerful in their intended domains, were notdesigned for this stringent regime and thus face fundamental challenges. Thispaper demonstrates that resolving this trilemma requires a shift in perspective- from seeking engineering trade-offs to deriving solutions from logicalnecessity. We establish a rigorous axiomatic system that formalizes thesephysical constraints and prove that for the broad class of computationsadmitting an idempotent merge operator, there exists a unique, optimalparadigm. Any system satisfying these axioms must converge to a single normalform: Self-Describing Parallel Flows (SDPF), a purely data-centric model wherestateless executors process flows that carry their own control logic. Wefurther prove this unique paradigm is convergent, Turing-complete, and minimal.In the same way that the CAP theorem established a boundary for what isimpossible in distributed state management, our work provides a constructivedual: a uniqueness theorem that reveals what is \textit{inevitable} fordistributed computation flows under physical law.</description>
      <author>example@mail.com (Zhiyuan Ren, Mingxuan Lu, Wenchi Cheng)</author>
      <guid isPermaLink="false">2509.11754v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Fully Open and Generalizable Foundation Model for Ultrasound Clinical Applications</title>
      <link>http://arxiv.org/abs/2509.11752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EchoCare是一个新型的超声基础模型，通过自监督学习在大型、多样化数据集EchoCareData上开发，能够有效整合多源数据学习超声表征，并在多种超声应用中表现出色。&lt;h4&gt;背景&lt;/h4&gt;真实临床环境中缺乏大型标记数据集，特定任务模型的泛化能力有限，这些因素阻碍了超声应用中可泛化临床AI模型的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的超声基础模型，能够有效整合多源数据学习超声表征，提高临床护理水平。&lt;h4&gt;方法&lt;/h4&gt;创建了EchoCareData数据集，包含450万张超声图像，来自5大洲23个国家，使用多种不同成像设备获取；开发了EchoCare模型，采用分层分类器，能够同时学习像素级和表征级特征；通过自监督学习方法训练模型；在10个代表性超声基准测试上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;EchoCare在10个不同诊断难度的代表性超声基准测试中优于最先进的比较模型，这些基准测试涵盖了疾病诊断、病灶分割、器官检测、标志点预测、定量回归、图像增强和报告生成；EchoCare仅需少量训练就能表现优异。&lt;h4&gt;结论&lt;/h4&gt;EchoCare提供了一个完全开放且可泛化的基础模型，可以促进各种临床超声应用中AI技术的发展，代码和预训练模型已公开发布，支持微调和本地适应，可扩展到更多应用。&lt;h4&gt;翻译&lt;/h4&gt;能够通过整合多源数据有效学习超声表征的人工智能在推进临床护理方面具有巨大潜力。然而，真实临床环境中大型标记数据集的稀缺以及特定任务模型有限的泛化能力，阻碍了超声应用中可泛化临床AI模型的发展。本研究提出了EchoCare，一个用于通用临床的新型超声基础模型，通过在策划的、公开可用的、大规模数据集EchoCareData上进行自监督学习开发。EchoCareData包含450万张超声图像，源自5大洲23个国家，通过多种不同的成像设备获取，因此包含了多中心、多设备和多种族的全队列人群。与采用现成视觉基础模型架构的先前研究不同，我们在EchoCare中引入了分层分类器，以实现像素级和表征级特征的联合学习，同时捕捉全局解剖上下文和局部超声特征。经过最少训练，EchoCare在10个具有不同诊断难度的代表性超声基准测试中优于最先进的比较模型，涵盖了疾病诊断、病灶分割、器官检测、标志点预测、定量回归、图像增强和报告生成。代码和预训练模型已公开发布，使EchoCare可用于微调和本地适应，支持扩展到更多应用。EchoCare提供了一个完全开放且可泛化的基础模型，以促进各种临床超声应用中AI技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) that can effectively learn ultrasoundrepresentations by integrating multi-source data holds significant promise foradvancing clinical care. However, the scarcity of large labeled datasets inreal-world clinical environments and the limited generalizability oftask-specific models have hindered the development of generalizable clinical AImodels for ultrasound applications. In this study, we present EchoCare, a novelultrasound foundation model for generalist clinical use, developed viaself-supervised learning on our curated, publicly available, large-scaledataset EchoCareData. EchoCareData comprises 4.5 million ultrasound images,sourced from over 23 countries across 5 continents and acquired via a diverserange of distinct imaging devices, thus encompassing global cohorts that aremulti-center, multi-device, and multi-ethnic. Unlike prior studies that adoptoff-the-shelf vision foundation model architectures, we introduce ahierarchical classifier into EchoCare to enable joint learning of pixel-leveland representation-level features, capturing both global anatomical contextsand local ultrasound characteristics. With minimal training, EchoCareoutperforms state-of-the-art comparison models across 10 representativeultrasound benchmarks of varying diagnostic difficulties, spanning diseasediagnosis, lesion segmentation, organ detection, landmark prediction,quantitative regression, imaging enhancement and report generation. The codeand pretrained model are publicly released, rendering EchoCare accessible forfine-tuning and local adaptation, supporting extensibility to additionalapplications. EchoCare provides a fully open and generalizable foundation modelto boost the development of AI technologies for diverse clinical ultrasoundapplications.</description>
      <author>example@mail.com (Hongyuan Zhang, Yuheng Wu, Mingyang Zhao, Zhiwei Chen, Rebecca Li, Fei Zhu, Haohan Zhao, Xiaohua Yuan, Meng Yang, Chunli Qiu, Xiang Cong, Haiyan Chen, Lina Luan, Randolph H. L. Wong, Huai Liao, Colin A Graham, Shi Chang, Guowei Tao, Dong Yi, Zhen Lei, Nassir Navab, Sebastien Ourselin, Jiebo Luo, Hongbin Liu, Gaofeng Meng)</author>
      <guid isPermaLink="false">2509.11752v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>DRAG: Data Reconstruction Attack using Guided Diffusion</title>
      <link>http://arxiv.org/abs/2509.11724v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICML 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于引导扩散的新型数据重建攻击方法，用于评估大型基础模型在分割推理环境下的隐私风险。&lt;h4&gt;背景&lt;/h4&gt;随着大型基础模型的兴起，分割推理已成为一种流行的计算范式，用于在轻量级边缘设备和云服务器之间部署模型，以解决数据隐私和计算成本问题。然而，大多数现有的数据重建攻击都集中在较小的CNN分类模型上，而基础模型在SI环境下的隐私风险在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一研究空白，论文旨在提出一种新型的数据重建攻击方法，以评估大型基础模型在SI环境下的隐私风险。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一种基于引导扩散的数据重建攻击方法，该方法利用在大型数据集上预训练的潜在扩散模型(LDM)中嵌入的丰富先验知识。该方法在LDM学习到的图像先验上进行迭代重建，能够从中间表示(IR)有效地生成高保真度图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在从视觉基础模型的深层中间表示重建数据方面，该方法在定性和定量上都显著优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在SI场景中需要为大型模型开发更强大的隐私保护机制。&lt;h4&gt;翻译&lt;/h4&gt;随着大型基础模型的兴起，分割推理已成为一种流行的计算范式，用于在轻量级边缘设备和云服务器之间部署模型，以解决数据隐私和计算成本问题。然而，大多数现有的数据重建攻击都集中在较小的CNN分类模型上，而基础模型在SI环境下的隐私风险在很大程度上尚未被探索。为了解决这一研究空白，我们提出了一种基于引导扩散的新型数据重建攻击方法，该方法利用在大型数据集上预训练的潜在扩散模型(LDM)中嵌入的丰富先验知识。我们的方法在LDM学习到的图像先验上进行迭代重建，能够有效地从中间表示(IR)生成高保真度图像，这些图像类似于原始数据。大量实验表明，在从视觉基础模型的深层中间表示重建数据方面，我们的方法在定性和定量上都显著优于最先进的方法。研究结果强调了在SI场景中需要为大型模型开发更强大的隐私保护机制。代码可在以下网址获取：https://github.com/ntuaislab/DRAG。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of large foundation models, split inference (SI) has emerged asa popular computational paradigm for deploying models across lightweight edgedevices and cloud servers, addressing data privacy and computational costconcerns. However, most existing data reconstruction attacks have focused onsmaller CNN classification models, leaving the privacy risks of foundationmodels in SI settings largely unexplored. To address this gap, we propose anovel data reconstruction attack based on guided diffusion, which leverages therich prior knowledge embedded in a latent diffusion model (LDM) pre-trained ona large-scale dataset. Our method performs iterative reconstruction on theLDM's learned image prior, effectively generating high-fidelity imagesresembling the original data from their intermediate representations (IR).Extensive experiments demonstrate that our approach significantly outperformsstate-of-the-art methods, both qualitatively and quantitatively, inreconstructing data from deep-layer IRs of the vision foundation model. Theresults highlight the urgent need for more robust privacy protection mechanismsfor large models in SI scenarios. Code is available at:https://github.com/ntuaislab/DRAG.</description>
      <author>example@mail.com (Wa-Kin Lei, Jun-Cheng Chen, Shang-Tse Chen)</author>
      <guid isPermaLink="false">2509.11724v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Tabular Data with Class Imbalance: Predicting Electric Vehicle Crash Severity with Pretrained Transformers (TabPFN) and Mamba-Based Models</title>
      <link>http://arxiv.org/abs/2509.11449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the author's preprint version of a paper accepted for  presentation at the 24th International Conference on Machine Learning and  Applications (ICMLA 2025), December 3-5, 2025, Florida, USA. The final  published version will appear in the official IEEE proceedings. Conference  site: https://www.icmla-conference.org/icmla25/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种深度表格学习框架，使用德州2017-2023年的真实碰撞数据预测电动汽车碰撞的严重程度。&lt;h4&gt;背景&lt;/h4&gt;研究使用德州(2017-2023)的真实碰撞数据，筛选后分析了23,301起电动汽车碰撞记录。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度表格学习框架来预测电动汽车碰撞的严重程度，并评估不同模型的性能。&lt;h4&gt;方法&lt;/h4&gt;使用XGBoost和Random Forest进行特征重要性分析，识别关键预测因素；应用SMOTEENN重新采样技术解决类别不平衡问题；比较TabPFN、MambaNet和MambaAttention三种深度表格模型。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN表现出强大的泛化能力，而MambaAttention在严重伤害案例分类中表现更优，归因于其基于注意力的特征重加权机制。&lt;h4&gt;结论&lt;/h4&gt;深度表格架构在改善碰撞严重程度预测方面具有潜力，能够在电动汽车碰撞背景下实现数据驱动的安全干预。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种深度表格学习框架，使用德州(2017-2023)的真实碰撞数据预测电动汽车碰撞的严重程度。在筛选出纯电动汽车后，分析了23,301起电动汽车碰撞记录。使用XGBoost和随机森林的特征重要性技术确定了交叉路口关系、首次有害事件、人员年龄、碰撞限速和星期几为主要预测因素，以及自动紧急制动等先进安全功能。为解决类别不平衡问题，应用了合成少数过采样技术和编辑最近邻(SMOTEENN)重新采样。对三种最先进的深度表格模型TabPFN、MambaNet和MambaAttention进行了严重程度预测的基准测试。虽然TabPFN表现出强大的泛化能力，但MambaAttention由于其基于注意力的特征重加权，在严重伤害案例分类中取得了优越的性能。这些发现强调了深度表格架构在改善碰撞严重程度预测方面的潜力，并能够在电动汽车碰撞背景下实现数据驱动的安全干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a deep tabular learning framework for predicting crashseverity in electric vehicle (EV) collisions using real-world crash data fromTexas (2017-2023). After filtering for electric-only vehicles, 23,301EV-involved crash records were analyzed. Feature importance techniques usingXGBoost and Random Forest identified intersection relation, first harmfulevent, person age, crash speed limit, and day of week as the top predictors,along with advanced safety features like automatic emergency braking. Toaddress class imbalance, Synthetic Minority Over-sampling Technique and EditedNearest Neighbors (SMOTEENN) resampling was applied. Three state-of-the-artdeep tabular models, TabPFN, MambaNet, and MambaAttention, were benchmarked forseverity prediction. While TabPFN demonstrated strong generalization,MambaAttention achieved superior performance in classifying severe injury casesdue to its attention-based feature reweighting. The findings highlight thepotential of deep tabular architectures for improving crash severity predictionand enabling data-driven safety interventions in EV crash contexts.</description>
      <author>example@mail.com (Shriyank Somvanshi, Pavan Hebli, Gaurab Chhetri, Subasish Das)</author>
      <guid isPermaLink="false">2509.11449v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Geometric Priors for Unaligned Scene Change Detection</title>
      <link>http://arxiv.org/abs/2509.11292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于几何先验的未对齐场景变化检测方法，解决了当前方法在大视角变化下匹配漂移或失败的问题。&lt;h4&gt;背景&lt;/h4&gt;未对齐场景变化检测旨在检测不同时间拍摄且没有假设视点对齐的图像对之间的场景变化。当前方法仅依赖2D视觉线索建立跨图像对应关系来辅助变化检测，但大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。&lt;h4&gt;目的&lt;/h4&gt;利用几何基础模型的几何先验来解决未对齐场景变化检测的核心挑战，包括可靠识别视觉重叠、稳健建立对应关系和明确遮挡检测。&lt;h4&gt;方法&lt;/h4&gt;提出一个无需训练的框架，将几何先验与视觉基础模型的强大表示相结合，实现视点不对齐情况下的可靠变化检测。&lt;h4&gt;主要发现&lt;/h4&gt;在PSCD、ChangeSim和PASLCD数据集上的广泛评估表明，该方法实现了优越且稳健的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入几何先验，解决了未对齐场景变化检测中缺乏显式几何推理的关键且被忽视的限制。&lt;h4&gt;翻译&lt;/h4&gt;未对齐场景变化检测旨在检测在不同时间拍摄且不假设视点对齐的图像对之间的场景变化。为处理视角变化，当前方法仅依赖2D视觉线索建立跨图像对应关系以辅助变化检测。然而，大视角变化会改变视觉观测，导致基于外观的匹配漂移或失败。此外，监督仅限于来自小规模SCD数据集的2D变化掩码，限制了可泛化的多视图知识的学习，使得难以可靠识别视觉重叠和处理遮挡。这种缺乏显式几何推理代表了关键但被忽视的限制。在这项工作中，我们首次利用几何基础模型的几何先验来解决未对齐SCD的核心挑战，包括可靠识别视觉重叠、稳健建立对应关系和明确遮挡检测。基于这些先验，我们提出一个无需训练的框架，将其与视觉基础模型的强大表示相结合，实现视点不对齐情况下的可靠变化检测。通过在PSCD、ChangeSim和PASLCD数据集上的广泛评估，我们证明了我们的方法实现了优越且稳健的性能。我们的代码将在https://github.com/ZilingLiu/GeoSCD发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决'未对齐场景变化检测'问题，即在不同时间、不同视点拍摄的图像对之间检测场景变化。这个问题在现实中很重要，因为自动驾驶、无人机和移动机器人等应用中，图像往往因传感器位置、运动或环境因素而存在视点差异。现有方法大多假设视点对齐，限制了实际应用；同时，大视点变化下仅依赖2D视觉线索的方法难以建立可靠对应关系，导致性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析现有方法的局限性：它们仅依赖2D视觉线索，在大视点变化下易出现匹配漂移或失败；且受限于小规模数据集，难以学习多视图可泛化知识。作者借鉴了几何基础模型(GFM)从多视图图像恢复3D几何的能力，以及视觉基础模型SAM的零样本特性。基于这些，作者设计了两阶段框架：先用GFM建立几何理解，再引导变化掩码预测，无需训练即可实现跨数据集泛化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用几何基础模型提供的3D几何先验来处理视点未对齐问题。整体流程分为两阶段：1)几何先验生成模块：用GFM恢复深度图和相机参数，建立像素级对应关系，识别视觉重叠区域并检测遮挡；2)几何引导变化掩码预测模块：将图像输入SAM提取特征，基于几何对应关系生成初始变化提案，用遮挡掩码精炼，与SAM分割掩码匹配后融合得到最终结果。此外，还包含预处理步骤减轻光照变化影响。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次引入GFM几何先验解决未对齐SCD的核心挑战(识别重叠、建立对应、检测遮挡)；2)提出无需训练的框架，集成几何先验与视觉基础模型，消除对大规模标注数据的需求；3)在多个数据集上验证了方法的优越性和鲁棒性。相比之前工作：不同于传统视点对齐方法和仅依赖2D线索的未对齐方法，本文利用3D几何信息建立更可靠的对应关系；不同于其他未对齐方法(如光流、特征相关)，本文方法能明确检测遮挡且无需训练；相比零样本方法，本文能有效处理大视点变化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次利用几何基础模型的几何先验，提出了一种无需训练的框架，有效解决了视点未对齐场景变化检测中的核心挑战，实现了在各种视点变化和场景类型下的鲁棒变化检测。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unaligned Scene Change Detection aims to detect scene changes between imagepairs captured at different times without assuming viewpoint alignment. Tohandle viewpoint variations, current methods rely solely on 2D visual cues toestablish cross-image correspondence to assist change detection. However, largeviewpoint changes can alter visual observations, causing appearance-basedmatching to drift or fail. Additionally, supervision limited to 2D change masksfrom small-scale SCD datasets restricts the learning of generalizablemulti-view knowledge, making it difficult to reliably identify visual overlapsand handle occlusions. This lack of explicit geometric reasoning represents acritical yet overlooked limitation. In this work, we are the first to leveragegeometric priors from a Geometric Foundation Model to address the corechallenges of unaligned SCD, including reliable identification of visualoverlaps, robust correspondence establishment, and explicit occlusiondetection. Building on these priors, we propose a training-free framework thatintegrates them with the powerful representations of a visual foundation modelto enable reliable change detection under viewpoint misalignment. Throughextensive evaluation on the PSCD, ChangeSim, and PASLCD datasets, wedemonstrate that our approach achieves superior and robust performance. Ourcode will be released at https://github.com/ZilingLiu/GeoSCD.</description>
      <author>example@mail.com (Ziling Liu, Ziwei Chen, Mingqi Gao, Jinyu Yang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.11292v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>DreamNav: A Trajectory-Based Imaginative Framework for Zero-Shot Vision-and-Language Navigation</title>
      <link>http://arxiv.org/abs/2509.11197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DreamNav是一种新的零样本视觉语言导航方法，通过改进感知、规划和预测能力，解决了现有方法依赖昂贵感知和被动理解的问题，实现了更高效、更语义一致的导航。&lt;h4&gt;背景&lt;/h4&gt;Vision-and-Language Navigation in Continuous Environments (VLN-CE)是具身机器人的核心能力，将语言指令与现实世界的感知和控制联系起来。大规模预训练基础模型被用作感知、推理和动作的共享先验，实现了无需任务特定训练的零样本VLN。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本VLN方法依赖昂贵感知和被动场景理解、控制简化为点级选择、部署成本高、动作语义不一致、规划缺乏前瞻性的问题。&lt;h4&gt;方法&lt;/h4&gt;DreamNav专注于三个方面：1) EgoView Corrector减少感知成本，对齐视角并稳定以自我为中心的感知；2) Trajectory Predictor替代点级动作，倾向于全局轨迹级规划，更好地与指令语义对齐；3) Imagination Predictor实现前瞻性和长期规划，赋予代理主动思考能力。&lt;h4&gt;主要发现&lt;/h4&gt;在VLN-CE和真实世界测试中，DreamNav建立了新的零样本最先进水平，在SR和SPL指标上分别比最强的以自我为中心的基线高出7.49%和18.15%。&lt;h4&gt;结论&lt;/h4&gt;DreamNav是第一个统一轨迹级规划和主动想象并仅使用以自我为中心输入的零样本VLN方法，显著提升了性能并降低了部署成本。&lt;h4&gt;翻译&lt;/h4&gt;Vision-and-Language Navigation in Continuous Environments (VLN-CE)将语言指令与现实世界中的感知和控制联系起来，是具身机器人的核心能力。最近，大规模预训练基础模型被用作感知、推理和动作的共享先验，实现了无需任务特定训练的零样本VLN。然而，现有零样本VLN方法依赖于昂贵的感知和被动场景理解，导致控制简化为点级选择。因此，这些方法部署成本高，动作语义不一致，且规划缺乏前瞻性。为解决这些问题，我们提出了DreamNav，专注于以下三个方面：(1)为减少感知成本，我们的EgoView Corrector对齐视角并稳定以自我为中心的感知；(2)替代点级动作，我们的Trajectory Predictor倾向于全局轨迹级规划，更好地与指令语义对齐；(3)为实现前瞻性和长期规划，我们提出了Imagination Predictor，赋予代理主动思考能力。在VLN-CE和真实世界测试中，DreamNav建立了新的零样本最先进水平，在SR和SPL指标上分别比最强的以自我为中心的基线高出7.49%和18.15%。据我们所知，这是第一个统一轨迹级规划和主动想象并仅使用以自我为中心输入的零样本VLN方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是零样本视觉语言导航(VLN-CE)中的三个核心问题：高成本感知、短视规划和语义不匹配。这些问题很重要，因为VLN-CE是具身机器人的关键能力，能让机器人根据语言指令在真实环境中导航。高成本感知限制了实际应用，短视规划导致机器人缺乏长远视野，语义不匹配则使导航决策与人类意图不符，这些都阻碍了机器人在复杂现实环境中的有效导航。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从人类导航行为中获得灵感，人类会构建连贯轨迹并主动想象未来场景。他们将问题分解为感知成本、规划前瞻性和语义对齐三个方面，针对性地设计解决方案。作者借鉴了扩散策略框架在机器人操作中的应用，利用预训练多模态模型作为认知核心，采用可控世界模型进行视觉预测，并借鉴FastSAM等技术用于可行走区域检测。整体设计是一个分层框架，包含视角校正、轨迹预测、想象预测和导航管理四个主要组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轨迹级规划和主动想象模仿人类导航行为，构建连贯轨迹并预见未来场景，而非被动理解和反应。整体流程：1)接收第一人称RGB-D观察和语言指令；2)EgoView Corrector校正视角方向；3)Trajectory Predictor生成候选轨迹并过滤；4)Imagination Predictor将候选轨迹转换为未来场景的语义描述；5)Navigation Manager选择最佳轨迹并监控执行；6)系统持续调整和优化导航策略，形成闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)EgoView Corrector解决第一人称视角的方向错误，仅用低成本输入；2)Trajectory Predictor采用轨迹级而非点级决策；3)Imagination Predictor将被动理解转为主动想象；4)统一框架整合轨迹规划和主动想象。相比之前工作，DreamNav摒弃了高成本的全景感知，避免了点级决策的短视性，通过文本而非像素级输出降低成本，并在真实测试中实现了SR和SPL指标分别提升7.49%和18.15%的性能突破。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DreamNav首次统一了轨迹级规划和主动想象，仅使用第一人称输入就在零样本视觉语言导航中实现了最先进性能，解决了现有方法的高成本、短视和语义不匹配问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation in Continuous Environments (VLN-CE), whichlinks language instructions to perception and control in the real world, is acore capability of embodied robots. Recently, large-scale pretrained foundationmodels have been leveraged as shared priors for perception, reasoning, andaction, enabling zero-shot VLN without task-specific training. However,existing zero-shot VLN methods depend on costly perception and passive sceneunderstanding, collapsing control to point-level choices. As a result, they areexpensive to deploy, misaligned in action semantics, and short-sighted inplanning. To address these issues, we present DreamNav that focuses on thefollowing three aspects: (1) for reducing sensory cost, our EgoView Correctoraligns viewpoints and stabilizes egocentric perception; (2) instead ofpoint-level actions, our Trajectory Predictor favors global trajectory-levelplanning to better align with instruction semantics; and (3) to enableanticipatory and long-horizon planning, we propose an Imagination Predictor toendow the agent with proactive thinking capability. On VLN-CE and real-worldtests, DreamNav sets a new zero-shot state-of-the-art (SOTA), outperforming thestrongest egocentric baseline with extra information by up to 7.49\% and18.15\% in terms of SR and SPL metrics. To our knowledge, this is the firstzero-shot VLN method to unify trajectory-level planning and active imaginationwhile using only egocentric inputs.</description>
      <author>example@mail.com (Yunheng Wang, Yuetong Fang, Taowen Wang, Yixiao Feng, Yawen Tan, Shuning Zhang, Peiran Liu, Yiding Ji, Renjing Xu)</author>
      <guid isPermaLink="false">2509.11197v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Organoid Tracker: A SAM2-Powered Platform for Zero-shot Cyst Analysis in Human Kidney Organoid Videos</title>
      <link>http://arxiv.org/abs/2509.11063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了Organoid Tracker平台，基于SAM2模型实现肾类器官时空显微镜视频的自动分析和定量评估，无需编程专业知识即可获取详细的定量指标，为肾脏疾病研究和药物发现提供强大工具。&lt;h4&gt;背景&lt;/h4&gt;类器官模型的最新进展改变了人类肾脏疾病机制研究和药物发现的方式，使研究可扩展、经济高效且无需牺牲动物。&lt;h4&gt;目的&lt;/h4&gt;开发一种优化的肾脏类器官平台，用于多囊肾病(PKD)的高效筛选，并解决现有手动分析方法仅限于粗略分类的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了Organoid Tracker，一个基于图形用户界面(GUI)的平台，采用模块化插件架构，基于Segment Anything Model 2 (SAM2)实现零样本分割和时空显微镜视频的自动分析。&lt;h4&gt;主要发现&lt;/h4&gt;Organoid Tracker可以量化关键指标，如囊肿形成率、生长速度和形态变化，同时生成综合报告，提供有价值的像素级和纵向信息。&lt;h4&gt;结论&lt;/h4&gt;Organoid Tracker提供了一个可扩展的开源框架，为改善和加速肾脏发育、PKD建模和治疗发现研究提供了强大解决方案，平台已作为开源软件公开可用。&lt;h4&gt;翻译&lt;/h4&gt;类器官模型的最新进展通过实现可扩展、经济高效且无需牺牲动物的研究，彻底改变了人类肾脏疾病机制研究和药物发现。我们在此介绍了一种为多囊肾病(PKD)高效筛选而优化的肾脏类器官平台。虽然这些系统生成丰富的时空显微镜视频数据集，但当前的手工分析方法仍仅限于粗略分类（如命中与非命中），常常遗漏有价值的像素级和纵向信息。为帮助克服这一瓶颈，我们开发了Organoid Tracker，一个采用模块化插件架构设计的图形用户界面(GUI)平台，使研究人员无需编程专业知识即可提取详细的定量指标。基于前沿视觉基础模型Segment Anything Model 2 (SAM2)构建，Organoid Tracker实现了时空显微镜视频的零样本分割和自动分析。它量化了囊肿形成率、生长速度和形态变化等关键指标，同时生成综合报告。通过提供可扩展的开源框架，Organoid Tracker为改善和加速肾脏发育、PKD建模和治疗发现研究提供了强大解决方案。该平台作为开源软件可在https://github.com/hrlblab/OrganoidTracker公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in organoid models have revolutionized the study of humankidney disease mechanisms and drug discovery by enabling scalable,cost-effective research without the need for animal sacrifice. Here, we presenta kidney organoid platform optimized for efficient screening in polycystickidney disease (PKD). While these systems generate rich spatial-temporalmicroscopy video datasets, current manual approaches to analysis remain limitedto coarse classifications (e.g., hit vs. non-hit), often missing valuablepixel-level and longitudinal information. To help overcome this bottleneck, wedeveloped Organoid Tracker, a graphical user interface (GUI) platform designedwith a modular plugin architecture, which empowers researchers to extractdetailed, quantitative metrics without programming expertise. Built on thecutting-edge vision foundation model Segment Anything Model 2 (SAM2), OrganoidTracker enables zero-shot segmentation and automated analysis ofspatial-temporal microscopy videos. It quantifies key metrics such as cystformation rate, growth velocity, and morphological changes, while generatingcomprehensive reports. By providing an extensible, open-source framework,Organoid Tracker offers a powerful solution for improving and acceleratingresearch in kidney development, PKD modeling, and therapeutic discovery. Theplatform is publicly available as open-source software athttps://github.com/hrlblab/OrganoidTracker.</description>
      <author>example@mail.com (Xiaoyu Huang, Lauren M Maxson, Trang Nguyen, Cheng Jack Song, Yuankai Huo)</author>
      <guid isPermaLink="false">2509.11063v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reinforcement Learning-Assisted Component Auto-Configuration of Differential Evolution Algorithm for Constrained Optimization: A Foundation Model</title>
      <link>http://arxiv.org/abs/2509.11016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SuperDE的新型框架，利用深度强化学习自动配置差分进化算法的组件，以解决约束优化问题。SuperDE作为一个基础模型，能够根据实时进化动态调整算法配置，并通过元学习实现零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;尽管人们努力设计高性能的进化算法，但由于现实问题的动态性和不断演变的特性，它们的适应性仍然有限。'无免费午餐'定理表明没有单一算法能在所有问题上都表现最佳。现有的在线适应方法通常存在效率低下、收敛性弱以及在约束优化问题上的泛化能力有限的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些挑战，作者引入了一个新的框架，用于在差分进化算法中自动化组件配置，以处理约束优化问题，该框架由深度强化学习提供支持。&lt;h4&gt;方法&lt;/h4&gt;作者提出了SuperDE，这是一个基础模型，可以根据实时进化动态配置差分进化算法的进化组件。通过元学习在各种约束优化问题上进行离线训练，SuperDE能够以零样本的方式为未见问题推荐每代的最优配置。利用双深度Q网络，SuperDE能够根据优化过程中不断变化的种群状态调整其配置策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SuperDE在基准测试套件上显著优于现有的最先进算法，实现了卓越的泛化和优化性能。&lt;h4&gt;结论&lt;/h4&gt;SuperDE通过结合深度强化学习和差分进化算法，为解决约束优化问题提供了一种有效的方法，能够动态调整算法参数以适应不同的问题特性。&lt;h4&gt;翻译&lt;/h4&gt;尽管人们付出了巨大努力来手动设计高性能的进化算法，但由于现实问题的动态和不断演变的特性，它们的适应性仍然有限。'无免费午餐'定理强调，没有单一算法能在所有问题上都表现最优。虽然已经提出了在线适应方法，但它们通常效率低下、收敛性弱，并且在约束优化问题上泛化能力有限。为了解决这些挑战，我们引入了一个新的框架，用于在差分进化算法中自动化组件配置以解决约束优化问题，该框架由深度强化驱动。具体来说，我们提出了SuperDE，这是一个基础模型，能够根据实时进化动态配置差分进化的组件。通过在各种约束优化问题上进行元学习离线训练，SuperDE能够以零样本的方式为未见问题推荐每代的最优配置。利用双深度Q网络，SuperDE能够根据优化过程中不断变化的种群状态调整其配置策略。实验结果表明，SuperDE在基准测试套件上显著优于现有的最先进算法，实现了卓越的泛化和优化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant efforts to manually design high-performance evolutionaryalgorithms, their adaptability remains limited due to the dynamic andever-evolving nature of real-world problems. The "no free lunch" theoremhighlights that no single algorithm performs optimally across all problems.While online adaptation methods have been proposed, they often suffer frominefficiency, weak convergence, and limited generalization on constrainedoptimization problems (COPs).  To address these challenges, we introduce a novel framework for automatedcomponent configuration in Differential Evolution (DE) algorithm to addressCOPs, powered by Deep Reinforcement Learning (DRL). Specifically, we proposeSuperDE, a foundation model that dynamically configures DE's evolutionarycomponents based on real-time evolution. Trained offline through meta-learningacross a wide variety of COPs, SuperDE is capable of recommending optimalper-generation configurations for unseen problems in a zero-shot manner.Utilizing a Double Deep Q-Network (DDQN), SuperDE adapts its configurationstrategies in response to the evolving population states during optimization.Experimental results demonstrate that SuperDE significantly outperformsexisting state-of-the-art algorithms on benchmark test suites, achievingsuperior generalization and optimization performance.</description>
      <author>example@mail.com (Xu Yang, Rui Wang, Kaiwen Li, Wenhua Li, Ling Wang)</author>
      <guid isPermaLink="false">2509.11016v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Lightweight Metadata-Aware Mixture-of-Experts Masked Autoencoder for Earth Observation</title>
      <link>http://arxiv.org/abs/2509.10919v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种紧凑的元数据感知专家混合掩码自编码器（MoE-MAE），该模型仅具有250万参数，但性能可与更大规模模型相媲美，证明了元数据感知预训练在地球观测领域的有效性。&lt;h4&gt;背景&lt;/h4&gt;地球观测领域最近的发展集中在大型基础模型上，但这些模型计算成本高，限制了其在下游任务中的可访问性和重用性。&lt;h4&gt;目的&lt;/h4&gt;研究紧凑架构作为实现小型通用地球观测模型的实用途径。&lt;h4&gt;方法&lt;/h4&gt;提出了一个只有250万参数的元数据感知专家混合掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理时间条件，同时结合了图像数据以及纬度/经度和季节/日循环编码。研究者在BigEarthNet-Landsat数据集上预训练MoE-MAE，并使用线性探针评估其冻结编码器的嵌入结果。&lt;h4&gt;主要发现&lt;/h4&gt;尽管模型规模小，但它能够与更大的架构竞争，这表明元数据感知预训可以提高迁移效率和标签效率。在缺乏显式元数据的EuroSAT-Landsat数据集上的评估显示，与拥有数亿参数的模型相比，仍然观察到具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型的高效且可扩展的一步。&lt;h4&gt;翻译&lt;/h4&gt;最近的地球观测进展主要集中在大型基础模型上。然而，这些模型计算成本高昂，限制了它们在下游任务中的可访问性和重用。在这项工作中，我们研究紧凑架构作为迈向小型通用地球观测模型的实用途径。我们提出了一个只有250万参数的元数据感知专家混合掩码自编码器（MoE-MAE）。该模型结合了稀疏专家路由与地理时间条件，整合了图像数据以及纬度/经度和季节/日循环编码。我们在BigEarthNet-Landsat数据集上预训练MoE-MAE，并使用线性探针评估其冻结编码器的嵌入结果。尽管规模小，该模型能与更大的架构竞争，这表明元数据感知预训练提高了迁移效率和标签效率。为了进一步评估泛化能力，我们在缺乏显式元数据的EuroSAT-Landsat数据集上进行了评估，与拥有数亿参数的模型相比，仍然观察到具有竞争力的性能。这些结果表明，紧凑的、元数据感知的MoE-MAE是未来地球观测基础模型的高效且可扩展的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Earth Observation have focused on large-scale foundationmodels. However, these models are computationally expensive, limiting theiraccessibility and reuse for downstream tasks. In this work, we investigatecompact architectures as a practical pathway toward smaller general-purpose EOmodels. We propose a Metadata-aware Mixture-of-Experts Masked Autoencoder(MoE-MAE) with only 2.5M parameters. The model combines sparse expert routingwith geo-temporal conditioning, incorporating imagery alongsidelatitude/longitude and seasonal/daily cyclic encodings. We pretrain the MoE-MAEon the BigEarthNet-Landsat dataset and evaluate embeddings from its frozenencoder using linear probes. Despite its small size, the model competes withmuch larger architectures, demonstrating that metadata-aware pretrainingimproves transfer and label efficiency. To further assess generalization, weevaluate on the EuroSAT-Landsat dataset, which lacks explicit metadata, andstill observe competitive performance compared to models with hundreds ofmillions of parameters. These results suggest that compact, metadata-awareMoE-MAEs are an efficient and scalable step toward future EO foundation models.</description>
      <author>example@mail.com (Mohanad Albughdadi)</author>
      <guid isPermaLink="false">2509.10919v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Nav-R1: Reasoning and Navigation in Embodied Scenes</title>
      <link>http://arxiv.org/abs/2509.10884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Nav-R1，一个用于具身导航的基础模型，通过统一推理框架解决了现有方法在推理轨迹不连贯、不稳定以及难以平衡长时程语义推理与低延迟控制方面的问题。&lt;h4&gt;背景&lt;/h4&gt;具身导航需要智能体在复杂3D环境中整合感知、推理和行动，但现有方法常因推理轨迹不连贯和不稳定而影响泛化能力，且难以平衡长时程语义推理与低延迟控制以实现实时导航。&lt;h4&gt;目的&lt;/h4&gt;解决具身导航中推理不连贯、不稳定以及语义推理与控制平衡困难的问题，提高智能体在不同环境中的导航性能。&lt;h4&gt;方法&lt;/h4&gt;1) 构建Nav-CoT-110K大规模分步思维链数据集实现冷启动初始化；2) 设计基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励；3) 引入'Fast-in-Slow'推理范式，将 deliberative 语义推理与低延迟反应控制解耦。&lt;h4&gt;主要发现&lt;/h4&gt;Nav-R1在具身AI基准测试中显著优于基线模型，推理和导航性能平均提升超过8%；在移动机器人上的实际部署验证了其在有限机载资源下的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Nav-R1通过统一推理框架和创新的训练方法，有效解决了具身导航中的关键挑战，实现了高效且连贯的导航性能。&lt;h4&gt;翻译&lt;/h4&gt;具身导航需要智能体在复杂3D环境中整合感知、推理和行动以实现稳健的交互。现有方法常因推理轨迹不连贯和不稳定而阻碍了跨不同环境的泛化能力，并且难以平衡长时程语义推理与低延迟控制以实现实时导航。为应对这些挑战，我们提出Nav-R1，一个统一具身环境中推理的基础模型。我们首先构建Nav-CoT-110K，一个用于具身任务的大规模分步思维链数据集，实现了具有结构化推理的冷启动初始化。在此基础上，我们设计了一个基于GRPO的强化学习框架，包含格式、理解和导航三种互补奖励，以提高结构遵循性、语义基础和路径保真度。此外，我们引入了'Fast-in-Slow'推理范式，将 deliberative 语义推理与低延迟反应控制解耦，实现高效且连贯的导航。在具身AI基准上的广泛评估表明，Nav-R1始终优于强大的基线模型，推理和导航性能平均提升超过8%。在移动机器人上的实际部署进一步验证了其在有限机载资源下的鲁棒性。代码：https://github.com/AIGeeksGroup/Nav-R1。网站：https://aigeeksgroup.github.io/Nav-R1。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身导航中的两个关键问题：一是现有方法存在推理轨迹不连贯和不稳定的问题，影响模型在不同环境中的泛化能力；二是难以平衡长距离语义推理与低延迟控制以实现实时导航。这些问题在现实中非常重要，因为具身导航是服务机器人、增强现实助手等智能系统在复杂3D环境中执行物体搜索、指令跟随等任务的核心能力，直接关系到这些系统在现实世界中的实用性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，然后设计了一个分阶段的解决方案：首先构建Nav-CoT-110K数据集进行冷启动初始化，然后使用基于GRPO的强化学习框架进行优化，最后提出Fast-in-Slow双系统推理范式。该方法借鉴了人类认知科学中的双系统理论（快速直觉系统1和慢速理性系统2），以及大型语言模型中常用的Chain-of-Thought推理技术和GRPO强化学习方法，同时整合了多个现有3D视觉语言数据集来构建新的高质量数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义推理与实时导航控制解耦，通过双系统设计实现高效且连贯的导航，同时使用结构化的思维链数据确保推理质量。整体实现流程分为四个阶段：1) 使用CoT数据引擎构建Nav-CoT-110K数据集，通过视觉语言模型生成结构化推理轨迹并过滤；2) 冷启动阶段，使用该数据集进行监督微调，初始化模型生成结构化推理-行动序列；3) 强化学习阶段，应用三种互补奖励函数（格式、理解和导航奖励）进行GRPO优化；4) 实现Fast-in-Slow双系统，慢系统处理长期语义推理，快系统执行短期反应控制，两者异步协调工作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) Nav-CoT-110K大型数据集，提供结构化思维链轨迹；2) 三种互补奖励机制，平衡结构一致性、语义理解和导航准确性；3) Fast-in-Slow双系统推理范式，解耦长期推理与短期控制；4) 结合模拟训练与真实世界部署。相比之前工作，Nav-R1不将语义推理和导航视为分离问题，而是在统一框架中解决；双系统设计比单一系统更好地平衡了语义连贯性和实时响应；多奖励机制提供了更全面的训练信号；结合真实世界数据提高了模型泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Nav-R1通过结构化思维链数据、多奖励强化学习和双系统推理范式，显著提升了具身导航中的推理连贯性和实时导航性能，实现了语义理解与行动执行的有效平衡。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied navigation requires agents to integrate perception, reasoning, andaction for robust interaction in complex 3D environments. Existing approachesoften suffer from incoherent and unstable reasoning traces that hindergeneralization across diverse environments, and difficulty balancinglong-horizon semantic reasoning with low-latency control for real-timenavigation. To address these challenges, we propose Nav-R1, an embodiedfoundation model that unifies reasoning in embodied environments. We firstconstruct Nav-CoT-110K, a large-scale dataset of step-by-step Chains-of-Thought(CoT) for embodied tasks, which enables cold-start initialization withstructured reasoning. Building on this foundation, we design a GRPO-basedreinforcement learning framework with three complementary rewards: format,understanding, and navigation, to improve structural adherence, semanticgrounding, and path fidelity. Furthermore, we introduce a Fast-in-Slowreasoning paradigm, decoupling deliberate semantic reasoning from low-latencyreactive control for efficient yet coherent navigation. Extensive evaluationson embodied AI benchmarks demonstrate that Nav-R1 consistently outperformsstrong baselines, with over 8% average improvement in reasoning and navigationperformance. Real-world deployment on a mobile robot further validates itsrobustness under limited onboard resources. Code:https://github.com/AIGeeksGroup/Nav-R1. Website:https://aigeeksgroup.github.io/Nav-R1.</description>
      <author>example@mail.com (Qingxiang Liu, Ting Huang, Zeyu Zhang, Hao Tang)</author>
      <guid isPermaLink="false">2509.10884v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>GoldenTransformer: A Modular Fault Injection Framework for Transformer Robustness Research</title>
      <link>http://arxiv.org/abs/2509.10790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Transformer已成为多种先进模型的基础，但其故障条件下的鲁棒性研究不足。GoldenTransformer是一个模块化且可扩展的故障注入框架，用于评估大型语言模型对硬件故障的恢复能力。&lt;h4&gt;背景&lt;/h4&gt;Transformers在自然语言处理、计算机视觉和机器学习领域广泛应用，但这些模型在故障条件下的鲁棒性研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;开发GoldenTransformer框架，用于评估大型语言模型对诱导硬件故障的恢复能力，并提供一个统一的平台来测试不同类型的故障影响。&lt;h4&gt;方法&lt;/h4&gt;GoldenTransformer是一个基于Python的统一平台，可以向预训练的transformer模型注入多种类型的故障，如权重损坏、激活注入和注意力级别干扰。该框架基于PyTorch和HuggingFace Transformers构建，支持实验可重复性、指标记录和可视化功能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在transformer的多个逻辑和结构点进行受控故障注入，GoldenTransformer能够帮助研究人员和从业者分析模型鲁棒性，指导现实世界LLM应用中的可靠系统设计。&lt;h4&gt;结论&lt;/h4&gt;GoldenTransformer解决了大型transformer架构的独特挑战，包括结构复杂性、潜在依赖性和非均匀层定义等问题，为模型鲁棒性分析提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;Transformers已成为自然语言处理、计算机视觉和其他机器学习领域中各种先进模型的基础。尽管它们被广泛部署，但这些模型在故障条件下的鲁棒性仍然研究不足。我们提出了GoldenTransformer，这是一个模块化和可扩展的故障注入框架，旨在评估大型语言模型对诱导硬件故障的恢复能力。GoldenTransformer提供了一个基于Python的统一平台，可以向预训练的基于transformer的模型注入多种类型的故障，如权重损坏、激活注入和注意力级别干扰。受DNN的GoldenEye模拟器启发，我们的框架专注于处理大型transformer架构的独特挑战，包括结构复杂性、潜在依赖性和非均匀层定义等考虑因素。GoldenTransformer基于PyTorch和HuggingFace Transformers构建，开箱即用地支持实验可重复性、指标记录和可视化。我们详细介绍了GoldenTransformer的技术设计和使用方法，并通过分类和生成任务上的几个示例实验进行了演示。通过能够在transformer的多个逻辑和结构点进行受控故障注入，GoldenTransformer为研究人员和从业者提供了有价值的工具，用于模型鲁棒性分析和指导现实世界LLM应用中的可靠系统设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have become the foundation for a wide range ofstate--of--the--art models across natural language processing, computer vision,and other machine learning domains. Despite their widespread deployment, therobustness of these models under fault conditions remains underexplored. Wepresent GoldenTransformer, a modular and extensible fault injection frameworkdesigned to evaluate the resiliency of Large Language Models to inducedhardware faults. GoldenTransformer offers a unified Python-based platform forinjecting diverse classes of faults--such as weight corruption, activationinjections, and attention--level disruptions--into pretrainedtransformer--based models. Inspired by the GoldenEye simulator for DNNs, ourframework focuses on the unique challenges of working with large transformerarchitectures, including considerations such as structural complexity, latentdependencies, and nonuniform layer definitions. GoldenTransformer is built atopPyTorch and HuggingFace Transformers, and it supports experimentreproducibility, metric logging, and visualization out of the box. We detailthe technical design and use of GoldenTransformer and demonstrate throughseveral example experiments on classification and generation tasks. By enablingcontrolled injection of faults at multiple logical and structural points in atransformer, GoldenTransformer offers researchers and practitioners a valuabletool for model robustness analysis and for guiding dependable system design inreal-world LLM applications.</description>
      <author>example@mail.com (Luke Howard)</author>
      <guid isPermaLink="false">2509.10790v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Medical Vision Foundation Models for Volumetric Medical Image Segmentation via Active Learning and Selective Semi-supervised Fine-tuning</title>
      <link>http://arxiv.org/abs/2509.10784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种主动无源域适应(ASFDA)方法，用于高效地将医学视觉基础模型(Med-VFMs)适应到目标域，用于体积医学图像分割任务。该方法通过主动学习选择最有信息量的样本进行微调，无需访问源预训练样本，从而以最小的选择预算最大化模型性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉基础模型(Med-VFMs)通过大量未标注图像的自监督预训练学习，具有解释医学图像的优越能力。目前，为了提高它们在适应性下游评估（特别是分割任务）中的性能，通常从目标域随机选择少量样本进行微调。然而，缺乏探索如何高效适应Med-VFMs以达到目标域最佳性能的研究。&lt;h4&gt;目的&lt;/h4&gt;设计一种高效的微调方法，通过选择信息量最大的样本来最大化Med-VFMs在目标域上的适应性能。&lt;h4&gt;方法&lt;/h4&gt;提出主动无源域适应(ASFDA)方法，采用新颖的主动学习策略，通过两个查询指标选择最有信息量的样本：多样化的知识差异(DKD)测量源-目标知识差距和域内多样性，利用预训练知识指导查询源不相似和语义多样的样本；解剖分割难度(ASD)通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，采用选择性半监督微调，通过从未查询的样本中识别高可靠性样本来提高微调的性能和效率。&lt;h4&gt;主要发现&lt;/h4&gt;DKD和ASD两个查询指标能够有效选择最有价值的样本进行微调，提高模型在目标域上的性能，无需访问源预训练样本即可实现最优适应。&lt;h4&gt;结论&lt;/h4&gt;ASFDA方法能够高效地将Med-VFMs适应到目标域，用于体积医学图像分割，通过主动学习选择最有信息量的样本，实现最优性能。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉基础模型(Med-VFMs)由于通过大量未标注图像进行自监督预训练所获得的知识，具有解释医学图像的优越能力。为了提高它们在适应性下游评估中的性能，特别是分割任务，通常从目标域随机选择少量样本进行微调。然而，缺乏探索如何高效适应Med-VFMs以达到目标域最佳性能的研究。因此，迫切需要设计一种高效的微调方法，通过选择信息量最大的样本来最大化Med-VFMs在目标域上的适应性能。为此，我们提出了一种主动无源域适应(ASFDA)方法，用于高效地将Med-VFMs适应到目标域，用于体积医学图像分割。该ASFDA采用一种新颖的主动学习(AL)方法，从目标域中选择信息量最大的样本进行微调，无需访问源预训练样本，从而以最小的选择预算最大化其性能。在该AL方法中，我们设计了一种主动测试时间样本查询策略，通过两个查询指标（包括多样化的知识差异DKD和解剖分割难度ASD）从目标域中选择样本。DKD旨在测量源-目标知识差距和域内多样性，它利用预训练知识指导从目标域中查询源不相似和语义多样的样本。ASD旨在通过自适应测量前景区域的预测熵来评估解剖结构分割的难度。此外，我们的ASFDA方法采用选择性半监督微调，通过从未查询的样本中识别高可靠性样本来提高微调的性能和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-13&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical Vision Foundation Models (Med-VFMs) have superior capabilities ofinterpreting medical images due to the knowledge learned from self-supervisedpre-training with extensive unannotated images. To improve their performance onadaptive downstream evaluations, especially segmentation, a few samples fromtarget domains are selected randomly for fine-tuning them. However, there lacksworks to explore the way of adapting Med-VFMs to achieve the optimalperformance on target domains efficiently. Thus, it is highly demanded todesign an efficient way of fine-tuning Med-VFMs by selecting informativesamples to maximize their adaptation performance on target domains. To achievethis, we propose an Active Source-Free Domain Adaptation (ASFDA) method toefficiently adapt Med-VFMs to target domains for volumetric medical imagesegmentation. This ASFDA employs a novel Active Learning (AL) method to selectthe most informative samples from target domains for fine-tuning Med-VFMswithout the access to source pre-training samples, thus maximizing theirperformance with the minimal selection budget. In this AL method, we design anActive Test Time Sample Query strategy to select samples from the targetdomains via two query metrics, including Diversified Knowledge Divergence (DKD)and Anatomical Segmentation Difficulty (ASD). DKD is designed to measure thesource-target knowledge gap and intra-domain diversity. It utilizes theknowledge of pre-training to guide the querying of source-dissimilar andsemantic-diverse samples from the target domains. ASD is designed to evaluatethe difficulty in segmentation of anatomical structures by measuring predictiveentropy from foreground regions adaptively. Additionally, our ASFDA methodemploys a Selective Semi-supervised Fine-tuning to improve the performance andefficiency of fine-tuning by identifying samples with high reliability fromunqueried ones.</description>
      <author>example@mail.com (Jin Yang, Daniel S. Marcus, Aristeidis Sotiras)</author>
      <guid isPermaLink="false">2509.10784v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>SCOPE: Speech-guided COllaborative PErception Framework for Surgical Scene Segmentation</title>
      <link>http://arxiv.org/abs/2509.10748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种语音引导的协同感知(SCOPE)框架，将大型语言模型与开放式视觉基础模型相结合，支持在手术视频中对手术器械和解剖结构进行即时分割、标记和跟踪，实现人机协作的手术场景理解。&lt;h4&gt;背景&lt;/h4&gt;手术场景中准确分割和跟踪相关元素对提供术中辅助和决策支持至关重要。当前解决方案依赖于特定领域的监督模型，需要标记数据，且难以适应新场景和预定义标签类别外的元素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在术中环境中实现开放式、零样本分割和跟踪的框架，无需依赖手动视觉或文本提示，支持人机协作的手术场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出语音引导的协同感知(SCOPE)框架，整合大型语言模型的推理能力与开放式视觉基础模型的感知能力。框架包含协同感知代理组件，生成分割候选并整合临床医生的语音反馈，实现自然的人机协作。手术器械本身作为交互式指针标记其他手术场景元素。&lt;h4&gt;主要发现&lt;/h4&gt;在Cataract1k数据集子集和自体离体颅底数据集上的评估表明，该框架能够生成手术场景的即时分割和跟踪。实时离体模拟实验进一步验证了其动态能力。&lt;h4&gt;结论&lt;/h4&gt;这种人-AI协作范式展示了开发适应性、免手持、以外科医生为中心的工具用于动态手术室环境的潜力。&lt;h4&gt;翻译&lt;/h4&gt;准确的手术场景相关元素分割和跟踪对于实现情境感知的术中辅助和决策制定至关重要。当前解决方案仍然依赖于特定领域的监督模型，这些模型依赖标记数据，并且需要特定领域的数据来适应新的手术场景和预定义标签类别之外的元素。提示驱动视觉基础模型(VFM)的最新进展使得在异构医学图像上进行开放式、零样本分割成为可能。然而，这些模型对手动视觉或文本提示的依赖限制了其在术中手术环境中的部署。我们引入了一个语音引导的协同感知(SCOPE)框架，该框架将大型语言模型(LLM)的推理能力与开放式VFM的感知能力相结合，支持在术中视频流中对手术器械和解剖结构进行即时分割、标记和跟踪。该框架的一个关键组件是一个协同感知代理，它生成VFM生成的分割候选，并整合临床医生直观的语音反馈，以自然的人机协作方式指导手术器械的分割。之后，手术器械本身作为交互式指针来标记手术场景的其他元素。我们在公开可用的Cataract1k数据集的子集和一个自体离体颅底数据集上评估了我们提出的框架，以证明其在生成手术场景即时分割和跟踪方面的潜力。此外，我们通过一个实时的离体模拟实验展示了其动态能力。这种人-AI协作范式展示了开发适应性、免手持、以外科医生为中心的工具用于动态手术室环境的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术场景中实时、准确分割和跟踪手术器械及解剖结构的问题。传统方法依赖标记数据和特定领域模型，难以适应新场景；现有视觉模型虽强大但需手动操作，不适合无菌手术环境。此问题对实现术中情境感知辅助和决策制定至关重要，能提高手术安全性和效率，增强医生在动态手术环境中的应变能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有监督学习难以泛化、手动交互不适合手术环境的问题。借鉴了视觉基础模型(SAM, GSAM)的分割能力和大型语言模型(GPT-4)的理解能力，参考了Visual ChatGPT的多模态交互范式但针对手术环境优化。设计了模块化交互流程，结合语音交互、协作感知代理和虚拟指针技术，实现免提操作和自然人机协作，无需针对特定手术数据进行微调。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合大型语言模型的自然语言理解能力和视觉基础模型的感知能力，通过语音引导实现人机协作，实时分割和跟踪手术场景元素。流程包括：1)语音输入处理；2)查询扩展与掩码生成；3)医生语音选择掩码并分配标签；4)使用视频分割模型跟踪器械；5)通过器械尖端作为虚拟指针分割解剖结构；6)持续交互优化结果，形成完整闭环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语音引导的免提交互方式；2)人机协作感知框架；3)器械尖端作为虚拟指针的技术；4)模块化交互设计；5)零样本泛化能力。相比之前工作，SCOPE摒弃了手动交互方式，专为动态手术视频设计，无需领域特定数据微调，强调人机协作而非自动化，更注重实际术中应用而非数据集标注。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCOPE通过整合语言模型推理能力和视觉模型感知能力，创造了一种语音引导的人机协作框架，使外科医生能通过自然语音命令实时分割和跟踪手术场景元素，从而提高手术情境感知和决策能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation and tracking of relevant elements of the surgical sceneis crucial to enable context-aware intraoperative assistance and decisionmaking. Current solutions remain tethered to domain-specific, supervised modelsthat rely on labeled data and required domain-specific data to adapt to newsurgical scenarios and beyond predefined label categories. Recent advances inprompt-driven vision foundation models (VFM) have enabled open-set, zero-shotsegmentation across heterogeneous medical images. However, dependence of thesemodels on manual visual or textual cues restricts their deployment inintroperative surgical settings. We introduce a speech-guided collaborativeperception (SCOPE) framework that integrates reasoning capabilities of largelanguage model (LLM) with perception capabilities of open-set VFMs to supporton-the-fly segmentation, labeling and tracking of surgical instruments andanatomy in intraoperative video streams. A key component of this framework is acollaborative perception agent, which generates top candidates of VFM-generatedsegmentation and incorporates intuitive speech feedback from clinicians toguide the segmentation of surgical instruments in a natural human-machinecollaboration paradigm. Afterwards, instruments themselves serve as interactivepointers to label additional elements of the surgical scene. We evaluated ourproposed framework on a subset of publicly available Cataract1k dataset and anin-house ex-vivo skull-base dataset to demonstrate its potential to generateon-the-fly segmentation and tracking of surgical scene. Furthermore, wedemonstrate its dynamic capabilities through a live mock ex-vivo experiment.This human-AI collaboration paradigm showcase the potential of developingadaptable, hands-free, surgeon-centric tools for dynamic operating-roomenvironments.</description>
      <author>example@mail.com (Jecia Z. Y. Mao, Francis X Creighton, Russell H Taylor, Manish Sahu)</author>
      <guid isPermaLink="false">2509.10748v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>CrunchLLM: Multitask LLMs for Structured Business Reasoning and Outcome Prediction</title>
      <link>http://arxiv.org/abs/2509.10698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CrunchLLM，一个专门用于创业公司成功预测的领域自适应大型语言模型框架，结合结构化数据和非结构化文本，通过参数高效微调和提示优化提高预测准确率，同时提供可解释的推理。&lt;h4&gt;背景&lt;/h4&gt;预测创业公司成功（通过收购或IPO退出）是创业研究的关键问题；Crunchbase等数据集提供结构化信息和非结构化文本，但传统机器学习方法仅依赖结构化特征且准确率一般，而大型语言模型虽具推理能力但难以直接适应商业领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于创业公司成功预测的领域自适应大型语言模型框架，整合结构化公司属性与非结构化文本叙述，应用参数高效微调策略和提示优化，使基础模型专门用于创业数据。&lt;h4&gt;方法&lt;/h4&gt;提出CrunchLLM框架，整合结构化公司属性与非结构化文本叙述，应用参数高效微调策略和提示优化，使基础模型专门适应创业领域数据。&lt;h4&gt;主要发现&lt;/h4&gt;CrunchLLM在Crunchbase创业公司成功预测上实现超过80%的准确率，显著优于传统分类器和基线LLMs；提供可解释的推理轨迹，增强金融和政策决策者的透明度和可信度。&lt;h4&gt;结论&lt;/h4&gt;通过领域感知微调和结构化-非结构化数据融合改进LLMs可推进创业成果预测建模；CrunchLLM为风险资本和创新政策中的数据驱动决策提供了方法论框架和实用工具。&lt;h4&gt;翻译&lt;/h4&gt;预测创业公司的成功，定义为通过收购或首次公开募股实现退出，是创业和创新研究中的关键问题。Crunchbase等数据集提供了结构化信息（如融资轮次、行业、投资者网络）和非结构化文本（如公司描述），但有效利用这种异构数据进行预测仍然具有挑战性。传统机器学习方法通常只依赖结构化特征并取得中等准确率，而大型语言模型虽提供丰富的推理能力却难以直接适应特定领域的商业数据。我们提出了CrunchLLM，一个用于创业公司成功预测的领域自适应LLM框架。CrunchLLM整合了结构化公司属性和非结构化文本叙述，并应用参数高效微调策略和提示优化，使基础模型专门用于创业数据。我们的方法在Crunchbase创业公司成功预测上实现了超过80%的准确率，显著优于传统分类器和基线LLMs。除了预测性能外，CrunchLLM提供可解释的推理轨迹，为其预测提供合理性解释，增强了金融和政策决策者的透明度和可信度。这项工作展示了如何通过领域感知微调和结构化-非结构化数据融合来改进LLMs，以推进创业成果的预测建模。CrunchLLM为风险资本和创新政策中的数据驱动决策提供了方法论框架和实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the success of start-up companies, defined as achieving an exitthrough acquisition or IPO, is a critical problem in entrepreneurship andinnovation research. Datasets such as Crunchbase provide both structuredinformation (e.g., funding rounds, industries, investor networks) andunstructured text (e.g., company descriptions), but effectively leveraging thisheterogeneous data for prediction remains challenging. Traditional machinelearning approaches often rely only on structured features and achieve moderateaccuracy, while large language models (LLMs) offer rich reasoning abilities butstruggle to adapt directly to domain-specific business data. We present\textbf{CrunchLLM}, a domain-adapted LLM framework for startup successprediction. CrunchLLM integrates structured company attributes withunstructured textual narratives and applies parameter-efficient fine-tuningstrategies alongside prompt optimization to specialize foundation models forentrepreneurship data. Our approach achieves accuracy exceeding 80\% onCrunchbase startup success prediction, significantly outperforming traditionalclassifiers and baseline LLMs. Beyond predictive performance, CrunchLLMprovides interpretable reasoning traces that justify its predictions, enhancingtransparency and trustworthiness for financial and policy decision makers. Thiswork demonstrates how adapting LLMs with domain-aware fine-tuning andstructured--unstructured data fusion can advance predictive modeling ofentrepreneurial outcomes. CrunchLLM contributes a methodological framework anda practical tool for data-driven decision making in venture capital andinnovation policy.</description>
      <author>example@mail.com (Rabeya Tus Sadia, Qiang Cheng)</author>
      <guid isPermaLink="false">2509.10698v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Building a General SimCLR Self-Supervised Foundation Model Across Neurological Diseases to Advance 3D Brain MRI Diagnoses</title>
      <link>http://arxiv.org/abs/2509.10620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025 Workshop CVAMD&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一个基于SimCLR的自监督学习基础模型，用于3D脑结构MRI分析，该模型在多种神经系统疾病的预测任务中表现出色，即使使用少量标记数据也能保持高性能。&lt;h4&gt;背景&lt;/h4&gt;3D结构MRI常用于监测多种神经系统疾病，但现有深度学习模型针对特定任务设计，泛化能力有限；自监督学习在2D医学成像中已取得成功，但3D脑MRI基础模型在分辨率、范围或可访问性方面仍有不足。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用、高分辨率的自监督学习基础模型，用于3D脑结构MRI分析，提高模型在不同任务和人群中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于SimCLR架构构建自监督学习模型，使用来自11个公开数据集的18,759名患者（44,958次扫描）进行预训练，涵盖多种神经系统疾病；与掩码自编码器及两个监督基线模型在四种下游预测任务上进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的SimCLR模型在所有下游任务上均优于其他模型；即使仅使用20%的标记训练样本预测阿尔茨海默病，模型仍能保持卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一个广泛适用且可访问的基础模型，显著提升了3D脑MRI分析的性能，特别适用于标记数据有限的情况。&lt;h4&gt;翻译&lt;/h4&gt;3D结构磁共振成像（MRI）脑扫描通常在临床环境中获取，用于监测广泛的神经系统疾病，包括神经退行性疾病和中风。虽然深度学习模型在分析3D MRI的多个脑成像任务中显示出有希望的结果，但大多数模型都是针对特定任务高度定制化的，标记数据有限，且无法跨任务和/或人群泛化。自监督学习（SSL）的发展使得能够利用从健康到疾病多样化的未标记数据集创建大型医学基础模型，在2D医学成像应用中显示出显著成功。然而，即使是少数已开发的3D脑MRI基础模型，在分辨率、范围或可访问性方面仍然有限。在这项工作中，我们提出了一个通用的高分辨率基于SimCLR的SSL基础模型，用于3D脑结构MRI，在11个公开可用的数据集上进行了预训练，这些数据集涵盖了18,759名患者（44,958次扫描），涉及多种神经系统疾病。我们在分布内和分布外设置中的四个不同下游预测任务上，将我们的模型与掩码自编码器（MAE）以及两个监督基线模型进行了比较。我们的微调SimCLR模型在所有任务上都优于所有其他模型。值得注意的是，即使在使用仅20%的标记训练样本来预测阿尔茨海默病时，我们的模型仍能实现卓越的性能。我们使用公开可用的代码和数据，并在https://github.com/emilykaczmarek/3D-Neuro-SimCLR上发布了我们的训练模型，为临床脑MRI分析贡献了一个广泛适用且可访问的基础模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D脑部MRI扫描分析中深度学习模型难以泛化的问题。现有模型通常针对特定任务定制，依赖有限标记数据，无法在不同任务和人群中泛化。这个问题很重要，因为脑部疾病诊断常使用3D MRI，但医学影像标记数据获取成本高、耗时长，且模型在遇到分布变化时可能失效。不同人群的脑部MRI具有高度视觉和解剖相似性，为利用多样化数据集训练通用模型提供了机会。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了当前3D脑部MRI基础模型的三个主要局限：数据集范围有限、分辨率低、难以复制。他们借鉴了现有的自监督学习方法，特别是基于对比学习的SimCLR和基于掩码的MAE架构。选择SimCLR是因为其强大的归纳偏差，适合相对较小的医学影像数据集。作者还实现了MAE作为互补基线，并设计了快速可复现的预处理流程，使用TurboPrep包处理高分辨率3D数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用自监督学习从大量未标记的多样化3D脑部MRI数据中学习通用表示，使模型能泛化到各种下游任务。整体流程包括：1)收集11个数据集的18,759名患者(44,958次扫描)的3D脑部MRI；2)使用TurboPrep进行预处理，包括偏置场校正、颅骨剥离、配准等，保持1×1×1mm³高分辨率；3)采用SimCLR或MAE进行自监督预训练，学习对变换不变的语义特征；4)在四个下游任务(中风量表回归、阿尔茨海默病分类、性别分类、年龄回归)上评估模型性能，包括分布内和分布外测试；5)研究在有限标记数据下的模型表现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用大规模多样化数据集(11个数据集，近4.5万次扫描)，涵盖多种神经系统疾病；2)保持高分辨率(1×1×1mm³)处理能力，保留临床相关小特征；3)完全公开代码和训练模型，提高可访问性和可复现性；4)在所有下游任务上表现优于现有模型，即使仅使用20%标记数据；5)使用快速预处理流程(TurboPrep)，每个扫描约需一分钟。相比之前工作，本文模型数据规模更大、更多样化、分辨率更高、更易获取，且在更广泛的任务和数据分布上进行了全面评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于SimCLR的高分辨率、可泛化的3D脑部MRI自监督基础模型，通过整合多源数据实现了在各种神经系统疾病诊断任务上的卓越性能，并在有限标记数据条件下保持高准确率，同时公开了代码和模型以促进医学影像分析领域的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D structural Magnetic Resonance Imaging (MRI) brain scans are commonlyacquired in clinical settings to monitor a wide range of neurologicalconditions, including neurodegenerative disorders and stroke. While deeplearning models have shown promising results analyzing 3D MRI across a numberof brain imaging tasks, most are highly tailored for specific tasks withlimited labeled data, and are not able to generalize across tasks and/orpopulations. The development of self-supervised learning (SSL) has enabled thecreation of large medical foundation models that leverage diverse, unlabeleddatasets ranging from healthy to diseased data, showing significant success in2D medical imaging applications. However, even the very few foundation modelsfor 3D brain MRI that have been developed remain limited in resolution, scope,or accessibility. In this work, we present a general, high-resolutionSimCLR-based SSL foundation model for 3D brain structural MRI, pre-trained on18,759 patients (44,958 scans) from 11 publicly available datasets spanningdiverse neurological diseases. We compare our model to Masked Autoencoders(MAE), as well as two supervised baselines, on four diverse downstreamprediction tasks in both in-distribution and out-of-distribution settings. Ourfine-tuned SimCLR model outperforms all other models across all tasks. Notably,our model still achieves superior performance when fine-tuned using only 20% oflabeled training samples for predicting Alzheimer's disease. We use publiclyavailable code and data, and release our trained model athttps://github.com/emilykaczmarek/3D-Neuro-SimCLR, contributing a broadlyapplicable and accessible foundation model for clinical brain MRI analysis.</description>
      <author>example@mail.com (Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel)</author>
      <guid isPermaLink="false">2509.10620v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>RailSafeNet: Visual Scene Understanding for Tram Safety</title>
      <link>http://arxiv.org/abs/2509.12125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, EPIA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为RailSafeNet的实时框架，利用数字图像处理、深度学习和人工智能技术来提高有轨电车与人交互的安全性。&lt;h4&gt;背景&lt;/h4&gt;有轨电车经常在人口密集区域运行，与人交互的安全是一个重要挑战，碰撞可能导致从轻微伤害到致命结果。&lt;h4&gt;目的&lt;/h4&gt;设计一个解决方案，利用数字图像处理、深度学习和人工智能来提高行人、司机、骑行者、宠物和有轨电车乘客的安全。&lt;h4&gt;方法&lt;/h4&gt;RailSafeNet是一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器来突出显示轨道入侵。系统仅使用单目视频，可以识别轨道、定位附近物体，并通过将投影距离与标准1435毫米轨道轨距比较来分类风险。&lt;h4&gt;主要发现&lt;/h4&gt;在RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，经过微调的YOLOv8在交并比阈值为0.50时达到了75.6%的平均精度均值。&lt;h4&gt;结论&lt;/h4&gt;RailSafeNet提供了准确、标注轻量的场景理解能力，可以在危险情况升级前警告司机。&lt;h4&gt;翻译&lt;/h4&gt;有轨电车与人交互安全是一个重要挑战，因为有轨电车经常在人口密集区域运行，碰撞可能导致从轻微伤害到致命结果。本文从利用数字图像处理、深度学习和人工智能设计解决方案的角度出发，以提高行人、司机、骑行者、宠物和有轨电车乘客的安全性。我们提出了RailSafeNet，一个实时框架，融合了语义分割、目标检测和基于规则的距离评估器来突出显示轨道入侵。仅使用单目视频，系统可以识别轨道，定位附近物体，并通过将投影距离与标准1435毫米轨道轨距比较来分类风险。在多样化的RailSem19数据集上的实验显示，经过类别过滤的SegFormer B3模型实现了65%的交并比，而经过微调的YOLOv8在交并比阈值为0.50时达到了75.6%的平均精度均值。因此，RailSafeNet提供了准确、标注轻量的场景理解，可以在危险情况升级前警告司机。代码可在https://github.com/oValach/RailSafeNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tram-human interaction safety is an important challenge, given that tramsfrequently operate in densely populated areas, where collisions can range fromminor injuries to fatal outcomes. This paper addresses the issue from theperspective of designing a solution leveraging digital image processing, deeplearning, and artificial intelligence to improve the safety of pedestrians,drivers, cyclists, pets, and tram passengers. We present RailSafeNet, areal-time framework that fuses semantic segmentation, object detection and arule-based Distance Assessor to highlight track intrusions. Using onlymonocular video, the system identifies rails, localises nearby objects andclassifies their risk by comparing projected distances with the standard 1435mmrail gauge. Experiments on the diverse RailSem19 dataset show that aclass-filtered SegFormer B3 model achieves 65% intersection-over-union (IoU),while a fine-tuned YOLOv8 attains 75.6% mean average precision (mAP) calculatedat an intersection over union (IoU) threshold of 0.50. RailSafeNet thereforedelivers accurate, annotation-light scene understanding that can warn driversbefore dangerous situations escalate. Code available athttps://github.com/oValach/RailSafeNet.</description>
      <author>example@mail.com (Ing. Ondrej Valach, Ing. Ivan Gruber)</author>
      <guid isPermaLink="false">2509.12125v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Microsurgical Instrument Segmentation for Robot-Assisted Surgery</title>
      <link>http://arxiv.org/abs/2509.11727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MISRA框架，通过增强RGB输入、集成跳跃注意力和迭代反馈模块来解决显微手术中薄结构分割的挑战，并引入了一个专门的显微手术数据集。&lt;h4&gt;背景&lt;/h4&gt;准确的薄结构分割对于显微手术场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个分割框架来解决显微手术中薄结构分割的挑战，提高分割的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;提出了MISRA分割框架，通过增加亮度通道增强RGB输入，集成跳跃注意力保留细长特征，采用迭代反馈模块在多次传递中恢复连续性，并引入了一个专门的显微手术数据集。&lt;h4&gt;主要发现&lt;/h4&gt;MISRA实现了有竞争力的性能，与竞争方法相比，平均类别IoU提高了5.37%，同时在器械接触和重叠处提供了更稳定的预测。&lt;h4&gt;结论&lt;/h4&gt;MISRA是迈向计算机辅助和机器人显微手术可靠场景解析的有希望的一步。&lt;h4&gt;翻译&lt;/h4&gt;准确的薄结构分割对于显微手术场景理解至关重要，但由于分辨率损失、低对比度和类别不平衡，这仍然具有挑战性。我们提出了MISRA（机器人辅助显微手术器械分割），一个分割框架，通过增加亮度通道来增强RGB输入，集成跳跃注意力以保留细长特征，并采用迭代反馈模块在多次传递中恢复连续性。此外，我们引入了一个专门的显微手术数据集，包含精细标注的外科器械包括薄物体，为稳健评估提供了基准。数据集可在https://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg获取。实验证明MISRA实现了有竞争力的性能，与竞争方法相比，平均类别IoU提高了5.37%，同时在器械接触和重叠处提供了更稳定的预测。这些结果表明MISRA是迈向计算机辅助和机器人显微手术可靠场景解析的有希望的一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of thin structures is critical for microsurgical sceneunderstanding but remains challenging due to resolution loss, low contrast, andclass imbalance. We propose Microsurgery Instrument Segmentation for RoboticAssistance(MISRA), a segmentation framework that augments RGB input withluminance channels, integrates skip attention to preserve elongated features,and employs an Iterative Feedback Module(IFM) for continuity restoration acrossmultiple passes. In addition, we introduce a dedicated microsurgical datasetwith fine-grained annotations of surgical instruments including thin objects,providing a benchmark for robust evaluation Dataset available athttps://huggingface.co/datasets/KIST-HARILAB/MISAW-Seg. Experiments demonstratethat MISRA achieves competitive performance, improving the mean class IoU by5.37% over competing methods, while delivering more stable predictions atinstrument contacts and overlaps. These results position MISRA as a promisingstep toward reliable scene parsing for computer-assisted and roboticmicrosurgery.</description>
      <author>example@mail.com (Tae Kyeong Jeong, Garam Kim, Juyoun Park)</author>
      <guid isPermaLink="false">2509.11727v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>See What I Mean? Mobile Eye-Perspective Rendering for Optical See-through Head-mounted Displays</title>
      <link>http://arxiv.org/abs/2509.11653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对光学透视头戴显示器中的视角错位问题，提出并评估了三种基于软件的眼睛视角渲染技术，通过用户研究证明了凝视代理方法的优越性，并开源了相关框架。&lt;h4&gt;背景&lt;/h4&gt;基于图像的场景理解技术可在未准备的真实世界环境中为增强现实系统提供上下文视觉指导，但在光学透视头戴显示器上存在世界-facing相机与用户眼睛视角之间的错位问题，影响效果。&lt;h4&gt;目的&lt;/h4&gt;开发并评估软件眼睛视角渲染技术，以近似用户的真实视角，解决光学透视头戴显示器中的错位问题。&lt;h4&gt;方法&lt;/h4&gt;在Microsoft HoloLens 2上实现并评估了三种眼睛视角渲染技术：平面代理EPR（投影到固定距离平面）、网格代理EPR（使用SLAM重建）和凝视代理EPR（基于眼动追踪的对齐方法）。&lt;h4&gt;主要发现&lt;/h4&gt;真实世界任务的用户研究强调了准确眼睛视角渲染的重要性，并发现凝视代理方法是一种轻量级的有效替代方案，优于基于几何的方法。&lt;h4&gt;结论&lt;/h4&gt;软件眼睛视角渲染技术可有效解决光学透视头戴显示器中的视角错位问题，凝视代理方法因其轻量级特性而特别有应用价值。&lt;h4&gt;翻译&lt;/h4&gt;基于图像的场景理解使增强系统能够在未准备的真实世界环境中提供上下文视觉指导。虽然在视频透视头戴显示器上有效，但这类方法在光学透视头戴显示器上存在问题，因为前置相机与用户眼睛视角之间存在错位。为了近似用户的真实视角，我们在商业化的无绳光学透视头戴显示器（Microsoft HoloLens 2）上实现了并评估了三种基于软件的眼睛视角渲染技术：（1）平面代理EPR，投影到固定距离的平面上；（2）网格代理EPR，使用基于SLAM的重建进行投影；（3）凝视代理EPR，一种新颖的基于眼动追踪的方法，将投影与用户的凝视深度对齐。真实世界任务的用户研究强调了准确眼睛视角渲染的重要性，并证明了凝视代理作为基于几何方法的轻量级替代方案。我们以开源形式发布了眼睛视角渲染框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决光学穿透式头戴显示器中世界相机视角与用户眼睛视角不匹配导致的视觉错位问题。这个问题很重要，因为当系统分析世界相机图像并叠加信息到用户视野时，视角差异会导致虚拟内容无法准确对齐真实物体，影响用户体验，在低视力辅助、颜色视觉缺陷补偿和任务指导等应用中尤其明显，还可能引起恶心等生理不适。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有工作，发现有两种主要方法：硬件修改方法（如使用半透镜）和软件方法（如固定平面投影）。硬件方法无法应用于现有商业设备，软件方法则存在明显错位问题。作者借鉴了现有的纹理投影技术，设计了三种不同的代理几何体方法：固定平面投影、网格投影和一种新的基于凝视的动态平面投影方法，以解决视角不匹配问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过纹理投影技术，从用户视角重新渲染世界相机捕获的视图，使用不同的代理几何体确保虚拟内容与真实世界场景准确对齐。流程包括：1)使用HoloLens 2获取传感器数据；2)在Unity中进行渲染；3)处理世界相机图像；4)根据选择的EPR方法将图像投影到不同代理几何体上；5)从用户眼睛位置重新渲染生成EPR视图；6)在显示器上显示结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Gaze-Proxy EPR方法，使用眼动跟踪动态调整投影平面；2)在移动式商业OST HMD上实现和比较三种EPR方法；3)进行真实场景下的用户研究；4)开源所有实现。相比之前工作，本研究关注移动、无连接线场景，提出的Gaze-Proxy方法在保持高准确性的同时降低了计算复杂度，解决了之前方法要么计算复杂要么准确性差的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这项研究提出并评估了一种基于眼动跟踪的新型轻量级EPR方法，为光学穿透式头戴显示器提供了准确的图像分析结果对齐解决方案，并通过开源框架促进了移动式增强现实应用的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image-based scene understanding allows Augmented Reality systems to providecontextual visual guidance in unprepared, real-world environments. Whileeffective on video see-through (VST) head-mounted displays (HMDs), such methodssuffer on optical see-through (OST) HMDs due to misregistration between theworld-facing camera and the user's eye perspective. To approximate the user'strue eye view, we implement and evaluate three software-based eye-perspectiverendering (EPR) techniques on a commercially available, untethered OST HMD(Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distanceplane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and(3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns theprojection with the user's gaze depth. A user study on real-world tasksunderscores the importance of accurate EPR and demonstrates gaze-proxy as alightweight alternative to geometry-based methods. We release our EPR frameworkas open source.</description>
      <author>example@mail.com (Gerlinde Emsenhuber, Tobias Langlotz, Denis Kalkofen, Markus Tatzgern)</author>
      <guid isPermaLink="false">2509.11653v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>How Auxiliary Reasoning Unleashes GUI Grounding in VLMs</title>
      <link>http://arxiv.org/abs/2509.11548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图形用户界面(GUI)基础任务提出三种零样本辅助推理方法，通过提供明确的空间线索使视觉语言模型能够表达其隐含的空间理解能力，显著提高了GUI基础性能。&lt;h4&gt;背景&lt;/h4&gt;图形用户界面(GUI)基础是构建GUI代理的基本任务，但通用的视觉语言模型(VLMs)在这个任务上表现不佳，因为缺乏特定优化。&lt;h4&gt;目的&lt;/h4&gt;解决VLMs在Pointing Game测量中显示出潜在定位能力但在输出明确坐标任务上表现不佳的问题，同时避免当前微调方法的高数据和高标注成本。&lt;h4&gt;方法&lt;/h4&gt;提出三种零样本辅助推理方法，通过在输入图像中提供明确的空间线索(如坐标轴、网格和标记的交叉点)，使VLMs能够表达其隐含的空间理解能力。&lt;h4&gt;主要发现&lt;/h4&gt;在四个GUI基础基准上对七个开源和专有VLMs的评估结果表明，提出的方法显著提高了GUI基础性能。&lt;h4&gt;结论&lt;/h4&gt;通过提供空间线索的零样本辅助推理方法可以有效提升VLMs在GUI基础任务上的表现，避免了传统微调方法的高成本问题。&lt;h4&gt;翻译&lt;/h4&gt;图形用户界面(GUI)基础是构建GUI代理的基本任务。然而，通用的视觉语言模型(VLMs)由于缺乏特定优化而难以完成此任务。本文确定了一个关键差距：虽然VLMs在通过Pointing Game测量的性能中显示出显著的潜在定位能力，但在输出明确坐标的任务上表现不佳。为解决这种差异，并绕过当前微调方法的高数据和标注成本，我们提出了三种零样本辅助推理方法。通过在输入图像中提供明确的空间线索，如坐标轴、网格和标记的交叉点，这些方法使VLMs能够表达其隐含的空间理解能力。我们在四个GUI基础基准上对七个开源和专有VLMs评估了这些方法。评估结果表明，提出的方法显著提高了GUI基础的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphical user interface (GUI) grounding is a fundamental task for buildingGUI agents. However, general vision-language models (VLMs) struggle with thistask due to a lack of specific optimization. We identify a key gap in thispaper: while VLMs exhibit significant latent grounding potential, asdemonstrated by their performance measured by Pointing Game, they underperformwhen tasked with outputting explicit coordinates. To address this discrepancy,and bypass the high data and annotation costs of current fine-tuningapproaches, we propose three zero-shot auxiliary reasoning methods. Byproviding explicit spatial cues such as axes, grids and labeled intersectionsas part of the input image, these methods enable VLMs to articulate theirimplicit spatial understanding capabilities. We evaluate these methods on fourGUI grounding benchmarks across seven open-source and proprietary VLMs. Theevaluation results demonstrate that the proposed methods substantially improvethe performance of GUI grounding.</description>
      <author>example@mail.com (Weiming Li, Yan Shao, Jing Yang, Yujing Lu, Ling Zhong, Yuhan Wang, Manni Duan)</author>
      <guid isPermaLink="false">2509.11548v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Aware Infrared and Visible Image Fusion with Target-Aware Supervision</title>
      <link>http://arxiv.org/abs/2509.11476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 6th International Conference on Computer Vision and  Data Mining (ICCVDM 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FusionNet的新型端到端红外和可见图像融合框架，通过模态感知注意力机制和像素级alpha混合模块实现细粒度融合，并利用目标感知损失函数保留重要对象区域的语义一致性。&lt;h4&gt;背景&lt;/h4&gt;红外和可见图像融合(IVIF)是多模态感知中的基本任务，旨在整合来自不同光谱域的互补结构和纹理线索。&lt;h4&gt;目的&lt;/h4&gt;提出一个新型端到端融合框架，明确建模模态间交互并增强任务关键区域。&lt;h4&gt;方法&lt;/h4&gt;引入模态感知注意力机制动态调整红外和可见特征贡献；集成像素级alpha混合模块自适应学习空间变化的融合权重；提出目标感知损失函数利用弱ROI监督保留重要对象区域的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留性、高感知质量和清晰的可解释性。&lt;h4&gt;结论&lt;/h4&gt;该框架为语义感知的多模态图像融合提供了通用且可扩展的解决方案，有利于下游任务如目标检测和场景理解。&lt;h4&gt;翻译&lt;/h4&gt;红外和可见图像融合(IVIF)是多模态感知中的一个基本任务，旨在整合来自不同光谱域的互补结构和纹理线索。在本文中，我们提出FusionNet，一种新型端到端融合框架，明确建模模态间交互并增强任务关键区域。FusionNet引入了一种模态感知注意力机制，根据红外和可见特征的判别能力动态调整其贡献。为实现细粒度、可解释的融合，我们进一步集成了像素级alpha混合模块，该模块以自适应和内容感知的方式学习空间变化的融合权重。此外，我们制定了一种目标感知损失函数，利用弱ROI监督来保留包含重要对象（如行人、车辆）区域的语义一致性。在公共M3FD数据集上的实验表明，FusionNet生成的融合图像具有增强的语义保留性、高感知质量和清晰的可解释性。我们的框架为语义感知的多模态图像融合提供了通用且可扩展的解决方案，有利于目标检测和场景理解等下游任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrared and visible image fusion (IVIF) is a fundamental task in multi-modalperception that aims to integrate complementary structural and textural cuesfrom different spectral domains. In this paper, we propose FusionNet, a novelend-to-end fusion framework that explicitly models inter-modality interactionand enhances task-critical regions. FusionNet introduces a modality-awareattention mechanism that dynamically adjusts the contribution of infrared andvisible features based on their discriminative capacity. To achievefine-grained, interpretable fusion, we further incorporate a pixel-wise alphablending module, which learns spatially-varying fusion weights in an adaptiveand content-aware manner. Moreover, we formulate a target-aware loss thatleverages weak ROI supervision to preserve semantic consistency in regionscontaining important objects (e.g., pedestrians, vehicles). Experiments on thepublic M3FD dataset demonstrate that FusionNet generates fused images withenhanced semantic preservation, high perceptual quality, and clearinterpretability. Our framework provides a general and extensible solution forsemantic-aware multi-modal image fusion, with benefits for downstream taskssuch as object detection and scene understanding.</description>
      <author>example@mail.com (Tianyao Sun, Dawei Xiang, Tianqi Ding, Xiang Fang, Yijiashun Qi, Zunduo Zhao)</author>
      <guid isPermaLink="false">2509.11476v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>A Comparison and Evaluation of Fine-tuned Convolutional Neural Networks to Large Language Models for Image Classification and Segmentation of Brain Tumors on MRI</title>
      <link>http://arxiv.org/abs/2509.10683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了大型语言模型(LLMs)和传统卷积神经网络(CNNs)在医学影像任务（胶质瘤分类和分割）上的性能，结果表明CNN在准确性和空间理解方面优于LLMs，LLMs在当前形式下不太适合基于图像的医疗任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)在基于文本的医疗任务中表现出色，但它们在基于图像的应用中的效用尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;研究LLMs在医学影像任务（特别是胶质瘤分类和分割）中的有效性，并将其性能与传统卷积神经网络(CNNs)进行比较。&lt;h4&gt;方法&lt;/h4&gt;使用BraTS 2020多模态脑部MRI数据集，评估通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其与自定义3D CNN进行基准测试。对于分类任务，比较低级别与高级别胶质瘤的分类效果；对于分割任务，实现中心点、边界框和多边形提取三种方法。&lt;h4&gt;主要发现&lt;/h4&gt;分类任务：CNN达到80%准确率，平衡了精确率和召回率；通用LLM达到76%准确率但特异性仅18%，经常错误分类低级别肿瘤；微调后特异性提高到55%但准确率降至72%。分割任务：CNNs能准确定位胶质瘤但有时遗漏小肿瘤；LLMs将预测聚集在图像中心，无法区分胶质瘤大小、位置；微调改善了输出格式但未提高空间准确性；边界多边形方法产生随机无结构输出。&lt;h4&gt;结论&lt;/h4&gt;CNNs在两项任务中都优于LLMs。LLMs表现出有限的空间理解能力，微调带来的改进很小，表明在当前形式下它们不适合基于图像的任务。需要更严格的微调或替代训练策略，才能使LLMs在医疗领域实现更好的性能、鲁棒性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在基于文本的医疗任务中表现出色。然而，它们在基于图像的应用中的效用尚未被探索。我们研究了LLMs在医学影像任务（特别是胶质瘤分类和分割）中的有效性，并将其性能与传统卷积神经网络(CNNs)进行了比较。使用BraTS 2020多模态脑部MRI数据集，我们评估了通用视觉-语言LLM（LLaMA 3.2 Instruct）在微调前后的性能，并将其性能与自定义3D CNN进行了基准测试。对于胶质瘤分类（低级别vs高级别），CNN实现了80%的准确率，并平衡了精确率和召回率。通用LLM达到76%的准确率，但特异性仅为18%，经常错误分类低级别肿瘤。微调将特异性提高到55%，但整体性能下降（例如，准确率降至72%）。对于分割任务，实现了三种方法：中心点、边界框和多边形提取。CNNs能够准确定位胶质瘤，但有时会遗漏小肿瘤。相比之下，LLMs将预测一致地聚集在图像中心，无法区分胶质瘤的大小、位置或放置。微调改善了输出格式，但未能有意义地提高空间准确性。边界多边形方法产生随机、无结构的输出。总体而言，CNNs在两项任务中都优于LLMs。LLMs表现出有限的空间理解能力，微调带来的改进也很小，表明在当前形式下，它们不适合基于图像的任务。可能需要更严格的微调或替代训练策略，才能使LLMs在医疗领域实现更好的性能、鲁棒性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have shown strong performance in text-basedhealthcare tasks. However, their utility in image-based applications remainsunexplored. We investigate the effectiveness of LLMs for medical imaging tasks,specifically glioma classification and segmentation, and compare theirperformance to that of traditional convolutional neural networks (CNNs). Usingthe BraTS 2020 dataset of multi-modal brain MRIs, we evaluated ageneral-purpose vision-language LLM (LLaMA 3.2 Instruct) both before and afterfine-tuning, and benchmarked its performance against custom 3D CNNs. For gliomaclassification (Low-Grade vs. High-Grade), the CNN achieved 80% accuracy andbalanced precision and recall. The general LLM reached 76% accuracy butsuffered from a specificity of only 18%, often misclassifying Low-Grade tumors.Fine-tuning improved specificity to 55%, but overall performance declined(e.g., accuracy dropped to 72%). For segmentation, three methods - centerpoint, bounding box, and polygon extraction, were implemented. CNNs accuratelylocalized gliomas, though small tumors were sometimes missed. In contrast, LLMsconsistently clustered predictions near the image center, with no distinctionof glioma size, location, or placement. Fine-tuning improved output formattingbut failed to meaningfully enhance spatial accuracy. The bounding polygonmethod yielded random, unstructured outputs. Overall, CNNs outperformed LLMs inboth tasks. LLMs showed limited spatial understanding and minimal improvementfrom fine-tuning, indicating that, in their current form, they are notwell-suited for image-based tasks. More rigorous fine-tuning or alternativetraining strategies may be needed for LLMs to achieve better performance,robustness, and utility in the medical space.</description>
      <author>example@mail.com (Felicia Liu, Jay J. Yoo, Farzad Khalvati)</author>
      <guid isPermaLink="false">2509.10683v1</guid>
      <pubDate>Tue, 16 Sep 2025 16:29:16 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal SAM-adapter for Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.10408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MM SAM-adapter的新型多模态语义分割框架，通过适配器网络将融合的多模态特征注入到SAM的RGB特征中，实现了在具有挑战性条件下的高效场景理解。&lt;h4&gt;背景&lt;/h4&gt;语义分割作为计算机视觉的关键任务，在自动驾驶、医学成像和机器人技术等领域有广泛应用。尽管深度学习推动了该领域的显著进步，但现有方法在光照不足、遮挡和恶劣天气等挑战条件下仍表现脆弱。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些局限性，作者提出MM SAM-adapter框架，扩展Segment Anything Model（SAM）的能力以实现多模态语义分割，实现多模态信息的平衡和高效使用。&lt;h4&gt;方法&lt;/h4&gt;所提出的方法采用适配器网络，将融合的多模态特征（如LiDAR、红外数据）注入到SAM丰富的RGB特征中。这种设计使模型能够保留RGB特征的强泛化能力，同时仅在辅助模态提供额外有用信息时选择性地融入它们。&lt;h4&gt;主要发现&lt;/h4&gt;在三个具有挑战性的基准测试（DeLiVER、FMB和MUSES）上评估，MM SAM-adapter实现了最先进的性能。通过将数据集划分为RGB-easy和RGB-hard子集的分析表明，该框架在有利和不利条件下均优于现有方法，证明了多模态适应对鲁棒场景理解的有效性。&lt;h4&gt;结论&lt;/h4&gt;MM SAM-adapter实现了多模态信息的平衡和高效使用，显著提升了在挑战性条件下的语义分割性能，为多模态场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;语义分割是计算机视觉中的一个关键任务，在自动驾驶、医学成像和机器人技术中有广泛应用，随着深度学习的发展取得了显著进步。然而，当前方法在光照不足、遮挡和恶劣天气等具有挑战性的条件下仍然脆弱。为了解决这些局限性，最近出现了整合辅助传感器数据（如LiDAR、红外）的多模态方法，提供互补信息以增强鲁棒性。在这项工作中，我们提出了MM SAM-adapter，一种扩展Segment Anything Model（SAM）能力以实现多模态语义分割的新框架。所提出的方法采用适配器网络，将融合的多模态特征注入到SAM丰富的RGB特征中。这种设计使模型能够保留RGB特征的强泛化能力，同时仅在辅助模态提供额外线索时选择性地融入它们。因此，MM SAM-adapter实现了多模态信息的平衡和高效使用。我们在三个具有挑战性的基准测试（DeLiVER、FMB和MUSES）上评估了我们的方法，MM SAM-adapter在这些测试中实现了最先进的性能。为了进一步分析模态贡献，我们将DeLiVER和FMB划分为RGB-easy和RGB-hard子集。结果一致表明，我们的框架在有利和不利条件下都优于竞争方法，突显了多模态适应对鲁棒场景理解的有效性。代码可在以下链接获取：https://github.com/iacopo97/Multimodal-SAM-Adapter。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation, a key task in computer vision with broad applicationsin autonomous driving, medical imaging, and robotics, has advancedsubstantially with deep learning. Nevertheless, current approaches remainvulnerable to challenging conditions such as poor lighting, occlusions, andadverse weather. To address these limitations, multimodal methods thatintegrate auxiliary sensor data (e.g., LiDAR, infrared) have recently emerged,providing complementary information that enhances robustness. In this work, wepresent MM SAM-adapter, a novel framework that extends the capabilities of theSegment Anything Model (SAM) for multimodal semantic segmentation. The proposedmethod employs an adapter network that injects fused multimodal features intoSAM's rich RGB features. This design enables the model to retain the stronggeneralization ability of RGB features while selectively incorporatingauxiliary modalities only when they contribute additional cues. As a result, MMSAM-adapter achieves a balanced and efficient use of multimodal information. Weevaluate our approach on three challenging benchmarks, DeLiVER, FMB, and MUSES,where MM SAM-adapter delivers state-of-the-art performance. To further analyzemodality contributions, we partition DeLiVER and FMB into RGB-easy and RGB-hardsubsets. Results consistently demonstrate that our framework outperformscompeting methods in both favorable and adverse conditions, highlighting theeffectiveness of multimodal adaptation for robust scene understanding. The codeis available at the following link:https://github.com/iacopo97/Multimodal-SAM-Adapter.</description>
      <author>example@mail.com (Iacopo Curti, Pierluigi Zama Ramirez, Alioscia Petrelli, Luigi Di Stefano)</author>
      <guid isPermaLink="false">2509.10408v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
  <item>
      <title>LayerLock: Non-collapsing Representation Learning with Progressive Freezing</title>
      <link>http://arxiv.org/abs/2509.10156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LayerLock是一种简单有效的自监督视觉表征学习方法，通过渐进式层冻结实现从像素到潜在预测的过渡。&lt;h4&gt;背景&lt;/h4&gt;在视频掩码自编码(MAE)模型训练中，ViT层按深度顺序收敛：浅层先收敛，深层后收敛。&lt;h4&gt;目的&lt;/h4&gt;加速标准MAE训练，并提供一种简单可扩展的潜在预测方法，避免'表征崩溃'问题。&lt;h4&gt;方法&lt;/h4&gt;LayerLock通过显式计划逐步冻结模型，从像素预测过渡到潜在预测。&lt;h4&gt;主要发现&lt;/h4&gt;ViT层收敛顺序与其深度相关，浅层先收敛，深层后收敛；这种观察可用于加速MAE训练。&lt;h4&gt;结论&lt;/h4&gt;LayerLock在大模型上表现优异，在4DS感知套件上的结果超越了非掩码潜在预测方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LayerLock，一种简单而有效的自监督视觉表征学习方法，通过渐进式层冻结从像素预测逐步过渡到潜在预测。首先，我们观察到在视频掩码自编码(MAE)模型训练过程中，ViT层按照其深度顺序收敛：浅层收敛早，深层收敛晚。然后我们证明，这一观察可以通过在整个训练过程中按照显式计划逐步冻结模型来加速标准MAE。此外，同样的计划可用于一种简单且可扩展的潜在预测方法，不会遭受'表征崩溃'。我们将提出的LayerLock方法应用于高达40亿参数的大模型，在4DS感知套件上的结果超过了非掩码潜在预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LayerLock, a simple yet effective approach for self-supervisedvisual representation learning, that gradually transitions from pixel to latentprediction through progressive layer freezing. First, we make the observationthat during training of video masked-autoencoding (MAE) models, ViT layersconverge in the order of their depth: shallower layers converge early, deeperlayers converge late. We then show that this observation can be exploited toaccelerate standard MAE by progressively freezing the model according to anexplicit schedule, throughout training. Furthermore, this same schedule can beused in a simple and scalable approach to latent prediction that does notsuffer from "representation collapse". We apply our proposed approach,LayerLock, to large models of up to 4B parameters with results surpassing thoseof non-latent masked prediction on the 4DS perception suite.</description>
      <author>example@mail.com (Goker Erdogan, Nikhil Parthasarathy, Catalin Ionescu, Drew Hudson, Alexander Lerchner, Andrew Zisserman, Mehdi Sajjadi, Joao Carreira)</author>
      <guid isPermaLink="false">2509.10156v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Merging Physics-Based Synthetic Data and Machine Learning for Thermal Monitoring of Lithium-ion Batteries: The Role of Data Fidelity</title>
      <link>http://arxiv.org/abs/2509.10380v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合物理建模与机器学习的新型框架，用于开发资源高效且可扩展的内部温度估计算法，解决了传统方法中数据收集、模型参数化和估计器设计的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;内部温度比表面温度更难获取，需要开发准确且实时的估计算法以实现更好的热管理和安全。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效且可扩展的框架，用于构建准确、鲁棒和自适应的内部温度估计算法。&lt;h4&gt;方法&lt;/h4&gt;结合物理建模和机器学习：利用物理模型生成包含不同操作场景的仿真数据；使用这些数据预训练机器学习算法；应用迁移学习和无监督领域适应弥合仿真到现实的差距；使用有限的目标电池运行数据微调预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;框架在多种圆柱形电池和不同对流空气冷却条件下得到验证；仅依赖电池热特性的先验知识时，均方根误差为0.5摄氏度；使用接近真实值的热参数时，误差小于0.1摄氏度；全面研究了仿真数据质量在框架中的作用。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架能够有效解决传统方法中数据收集、模型参数化和估计器设计方面的关键挑战，实现准确、鲁棒和自适应的内部温度估计。&lt;h4&gt;翻译&lt;/h4&gt;由于内部温度比表面温度更难获取，迫切需要开发准确且实时的估计算法以实现更好的热管理和安全。这项工作通过结合物理建模与机器学习，提出了一种新型框架，用于资源高效且可扩展地开发准确、鲁棒和自适应的内部温度估计算法，以解决传统方法中数据收集、模型参数化和估计器设计方面的关键挑战。在该框架中，利用物理模型生成包含不同操作场景的仿真数据，通过扫描模型参数和输入曲线。这种廉价的仿真数据集可用于预训练机器学习算法以捕获底层映射关系。为了弥合由不完美建模导致的仿真到现实的差距，应用了带有无监督领域适应的迁移学习，使用来自目标电池的有限运行数据（无内部温度值）来微调预训练的机器学习模型。该框架在不同操作条件和多个采用对流空气冷却的圆柱形电池上得到验证，仅依赖电池热特性的先验知识时，均方根误差为0.5摄氏度，而使用接近真实值的热参数时，误差小于0.1摄氏度。此外，全面研究了仿真数据质量在所提框架中的作用，以识别有前途的合成数据生成方法，保证机器学习模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Since the internal temperature is less accessible than surface temperature,there is an urgent need to develop accurate and real-time estimation algorithmsfor better thermal management and safety. This work presents a novel frameworkfor resource-efficient and scalable development of accurate, robust, andadaptive internal temperature estimation algorithms by blending physics-basedmodeling with machine learning, in order to address the key challenges in datacollection, model parameterization, and estimator design that traditionallyhinder both approaches. In this framework, a physics-based model is leveragedto generate simulation data that includes different operating scenarios bysweeping the model parameters and input profiles. Such a cheap simulationdataset can be used to pre-train the machine learning algorithm to capture theunderlying mapping relationship. To bridge the simulation-to-reality gapresulting from imperfect modeling, transfer learning with unsupervised domainadaptation is applied to fine-tune the pre-trained machine learning model, byusing limited operational data (without internal temperature values) fromtarget batteries. The proposed framework is validated under different operatingconditions and across multiple cylindrical batteries with convective aircooling, achieving a root mean square error of 0.5 {\deg}C when relying solelyon prior knowledge of battery thermal properties, and less than 0.1 {\deg}Cwhen using thermal parameters close to the ground truth. Furthermore, the roleof the simulation data quality in the proposed framework has beencomprehensively investigated to identify promising ways of synthetic datageneration to guarantee the performance of the machine learning model.</description>
      <author>example@mail.com (Yusheng Zheng, Wenxue Liu, Yunhong Che, Ferdinand Grimm, Jingyuan Zhao, Xiaosong Hu, Simona Onori, Remus Teodorescu, Gregory J. Offer)</author>
      <guid isPermaLink="false">2509.10380v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Property prediction for ionic liquids without prior structural knowledge using limited experimental data: A data-driven neural recommender system leveraging transfer learning</title>
      <link>http://arxiv.org/abs/2509.10273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种数据驱动的迁移学习框架，利用神经推荐系统在稀疏实验数据集上实现离子液体性质的可靠预测。&lt;h4&gt;背景&lt;/h4&gt;离子液体因其物理化学性质可精确调节而成为传统溶剂的多功能替代品，但准确预测其关键热物理性质具有挑战性，因为化学设计空间大且实验数据有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用神经推荐系统的数据驱动迁移学习框架，能够在稀疏实验数据情况下可靠预测离子液体的热物理性质。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段过程：首先在固定温度压力下使用COSMO-RS模拟数据预训练NRS模型学习阳离子和阴离子的结构嵌入；然后使用这些嵌入和不同温度压力下的实验数据微调前馈神经网络。研究考虑了密度、粘度、表面张力、热容和熔点五种性质，并支持同性质和跨性质知识转移。&lt;h4&gt;主要发现&lt;/h4&gt;使用密度、粘度和热容的预训练模型进行微调后，五种目标性质中有四种的性能显著提高；模型对未见过的离子液体具有强大的外推能力；最终训练的模型可预测超过700,000种离子液体组合的性质。&lt;h4&gt;结论&lt;/h4&gt;结合模拟数据和迁移学习是克服实验数据稀疏性的有效方法，为离子液体筛选和工艺设计提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;离子液体（ILs）因其物理化学性质可精确调节以适应各种应用而成为传统溶剂的多功能替代品。然而，由于巨大的化学设计空间和实验数据的有限性，准确预测关键热物理性质仍然具有挑战性。在本研究中，我们提出了一个数据驱动的迁移学习框架，利用神经推荐系统（NRS）使用稀疏实验数据集实现离子液体性质的可靠预测。该方法涉及一个两阶段过程：首先在固定温度和压力下使用基于COSMO-RS的模拟数据预训练NRS模型，学习阳离子和阴离子的特定性质结构嵌入；其次使用这些嵌入和不同温度压力下的实验数据微调简单的前馈神经网络。在本工作中，考虑了五种基本的离子液体性质：密度、粘度、表面张力、热容和熔点。该框架支持同性质和跨性质知识转移。值得注意的是，密度、粘度和热容的预训练模型被用来微调所有五种目标性质的模型，其中四种性质的性能显著提高。该模型对未见过的离子液体表现出强大的外推能力。此外，最终训练的模型能够预测超过700,000种离子液体组合的性质，为工艺设计中的离子液体筛选提供了可扩展的解决方案。这项工作强调了结合模拟数据和迁移学习克服实验数据稀疏性的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ionic liquids (ILs) have emerged as versatile replacements for traditionalsolvents because their physicochemical properties can be precisely tailored tovarious applications. However, accurately predicting key thermophysicalproperties remains challenging due to the vast chemical design space and thelimited availability of experimental data. In this study, we present adata-driven transfer learning framework that leverages a neural recommendersystem (NRS) to enable reliable property prediction for ILs using sparseexperimental datasets. The approach involves a two-stage process: first,pre-training NRS models on COSMO-RS-based simulated data at fixed temperatureand pressure to learn property-specific structural embeddings for cations andanions; and second, fine-tuning simple feedforward neural networks using theseembeddings with experimental data at varying temperatures and pressures. Inthis work, five essential IL properties are considered: density, viscosity,surface tension, heat capacity, and melting point. The framework supports bothwithin-property and cross-property knowledge transfer. Notably, pre-trainedmodels for density, viscosity, and heat capacity are used to fine-tune modelsfor all five target properties, achieving improved performance by a substantialmargin for four of them. The model exhibits robust extrapolation to previouslyunseen ILs. Moreover, the final trained models enable property prediction forover 700,000 IL combinations, offering a scalable solution for IL screening inprocess design. This work highlights the effectiveness of combining simulateddata and transfer learning to overcome sparsity in the experimental data.</description>
      <author>example@mail.com (Sahil Sethi, Kai Sundmacher, Caroline Ganzer)</author>
      <guid isPermaLink="false">2509.10273v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Supervised and unsupervised learning with numerical computation for the Wolfram cellular automata</title>
      <link>http://arxiv.org/abs/2509.10209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了Wolfram细胞自动机的渐近密度和动态演化机制，通过结合数值模拟与机器学习方法分析了不同规则下的系统行为。&lt;h4&gt;背景&lt;/h4&gt;Wolfram细胞自动机使用八位二进制编码的一维三细胞邻域确定性更新规则，被广泛应用于研究自组织现象和复杂系统动力学。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探究Wolfram自动机的渐近密度和动态演化机制，识别不同规则下的配置特征，并探索初始条件对系统行为的影响。&lt;h4&gt;方法&lt;/h4&gt;研究采用数值模拟和计算方法，结合监督学习和无监督学习方法（如主成分分析和自编码器）来分析不同Wolfram规则的配置特征和系统行为。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了所选规则的渐近密度与初始密度之间的关系；某些Wolfram规则即使在从单个活动位点开始的情况下也能随时间生成相似的分形图案；监督学习方法有效识别各种Wolfram规则的配置，而无监督方法可以将不同规则的配置聚类到不同组中。&lt;h4&gt;结论&lt;/h4&gt;通过结合数值模拟与机器学习方法，研究成功揭示了Wolfram自动机的动态特性，为理解复杂系统行为提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;Wolfram细胞自动机具有一维三细胞邻域的局部规则由八位二进制表示，这些二进制编码确定性更新规则。这些自动机被广泛用于研究自组织现象和复杂系统的动力学。在这项工作中，我们采用数值模拟和计算方法来研究Wolfram自动机的渐近密度和动态演化机制。我们应用监督和无监督学习方法来识别与不同Wolfram规则相关的配置。此外，我们探索了替代初始条件，在这些条件下，某些Wolfram规则即使从单个活动位点开始也能随时间生成相似的分形图案。我们的结果揭示了所选规则的渐近密度与初始密度之间的关系。监督学习方法有效识别了各种Wolfram规则的配置，而无监督方法如主成分分析和自编码器可以将不同Wolfram规则的配置近似聚类到不同的组中，产生与模拟密度输出一致的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The local rules of Wolfram cellular automata with one-dimensional three-cellneighborhoods are represented by eight-bit binary that encode deterministicupdate rules. These automata are widely utilized to investigateself-organization phenomena and the dynamics of complex systems. In this work,we employ numerical simulations and computational methods to investigate theasymptotic density and dynamical evolution mechanisms in Wolfram automata. Weapply both supervised and unsupervised learning methods to identify theconfigurations associated with different Wolfram rules. Furthermore, we explorealternative initial conditions under which certain Wolfram rules generatesimilar fractal patterns over time, even when starting from a single activesite. Our results reveal the relationship between the asymptotic density andthe initial density of selected rules. The supervised learning methodseffectively identify the configurations of various Wolfram rules, whileunsupervised methods like principal component analysis and autoencoders canapproximately cluster configurations of different Wolfram rules into distinctgroups, yielding results that align well with simulated density outputs.</description>
      <author>example@mail.com (Kui Tuo, Shengfeng Deng, Yuxiang Yang, Yanyang Wang, Qiuping A. Wang, Wei Li, Wenjun Zhang)</author>
      <guid isPermaLink="false">2509.10209v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>FetalSleepNet: A Transfer Learning Framework with Spectral Equalisation Domain Adaptation for Fetal Sleep Stage Classification</title>
      <link>http://arxiv.org/abs/2509.10082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 4 tables, 5 figures, submitted to IEEE Journal of  Biomedical and Health Informatics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FetalSleepNet是一个创新的深度学习方法，首次用于从羊胎儿脑电图(EEG)分类睡眠状态。通过迁移学习和频谱均衡域适应策略，实现了高准确率的睡眠阶段分类，准确率达86.6%，宏F1分数为62.5%，优于基线模型。该方法有潜力应用于临床胎儿监测，为早期发现妊娠并发症相关的脑发育异常提供工具。&lt;h4&gt;背景&lt;/h4&gt;羊胎儿脑电图(EEG)的获取复杂，解释起来困难且繁琐。然而，精确的睡眠阶段分类可能有助于早期发现与妊娠并发症（如缺氧或宫内生长受限）相关的异常脑发育。&lt;h4&gt;目的&lt;/h4&gt;开发一个深度学习方法来分类羊胎儿的睡眠状态，并应用于妊娠并发症的早期检测。&lt;h4&gt;方法&lt;/h4&gt;研究人员将EEG电极固定在24只晚期妊娠胎羊的顶叶皮层硬脑膜上。使用一种为成人EEG睡眠阶段分类开发的轻量级深度神经网络，通过从成人EEG进行迁移学习来训练该网络处理羊EEG，并采用基于频谱均衡的域适应策略来减少跨域不匹配。&lt;h4&gt;主要发现&lt;/h4&gt;直接迁移表现不佳，但完全微调结合频谱均衡取得了最佳整体性能（准确率：86.6%，宏F1分数：62.5%），优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;FetalSleepNet是第一个专门为从胎儿EEG自动睡眠阶段分类开发的深度学习框架。它可以作为标记引擎，支持大规模弱/半监督标记和蒸馏，促进在临床中获取的侵入性较小信号（如多普勒超声或心电图数据）的训练。其轻量级设计使其非常适合部署在低功耗、实时和可穿戴的胎儿监测系统中。&lt;h4&gt;翻译&lt;/h4&gt;引言：本研究介绍了FetalSleepNet，这是首个已发表的使用深度学习方法从羊胎儿脑电图(EEG)分类睡眠状态的方法。胎儿EEG的获取复杂，解释起来困难且繁琐。然而，精确的睡眠阶段分类可能有助于早期发现与妊娠并发症相关的异常脑发育（如缺氧或宫内生长受限）。方法：将EEG电极固定在24只晚期妊娠胎羊的顶叶皮层硬脑膜上。使用一种为成人EEG睡眠阶段分类开发的轻量级深度神经网络，通过从成人EEG进行迁移学习来训练该网络处理羊EEG。采用基于频谱均衡的域适应策略来减少跨域不匹配。结果：我们证明了虽然直接迁移表现不佳，但完全微调结合频谱均衡取得了最佳整体性能（准确率：86.6%，宏F1分数：62.5%），优于基线模型。结论：据我们所知，FetalSleepNet是首个专门为从胎儿EEG自动睡眠阶段分类开发的深度学习框架。在实验室之外，基于EEG的睡眠阶段分类器可以作为标记引擎，支持大规模弱/半监督标记和蒸馏，促进在临床中获取的侵入性较小信号（如多普勒超声或心电图数据）的训练。FetalSleepNet的轻量级设计使其非常适合部署在低功耗、实时和可穿戴的胎儿监测系统中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Introduction: This study presents FetalSleepNet, the first published deeplearning approach to classifying sleep states from the ovineelectroencephalogram (EEG). Fetal EEG is complex to acquire and difficult andlaborious to interpret consistently. However, accurate sleep stageclassification may aid in the early detection of abnormal brain maturationassociated with pregnancy complications (e.g. hypoxia or intrauterine growthrestriction).  Methods: EEG electrodes were secured onto the ovine dura over the parietalcortices of 24 late gestation fetal sheep. A lightweight deep neural networkoriginally developed for adult EEG sleep staging was trained on the ovine EEGusing transfer learning from adult EEG. A spectral equalisation-based domainadaptation strategy was used to reduce cross-domain mismatch.  Results: We demonstrated that while direct transfer performed poorly, fullfine tuning combined with spectral equalisation achieved the best overallperformance (accuracy: 86.6 percent, macro F1-score: 62.5), outperformingbaseline models.  Conclusions: To the best of our knowledge, FetalSleepNet is the first deeplearning framework specifically developed for automated sleep staging from thefetal EEG. Beyond the laboratory, the EEG-based sleep stage classifierfunctions as a label engine, enabling large scale weak/semi supervised labelingand distillation to facilitate training on less invasive signals that can beacquired in the clinic, such as Doppler Ultrasound or electrocardiogram data.FetalSleepNet's lightweight design makes it well suited for deployment in lowpower, real time, and wearable fetal monitoring systems.</description>
      <author>example@mail.com (Weitao Tang, Johann Vargas-Calixto, Nasim Katebi, Nhi Tran, Sharmony B. Kelly, Gari D. Clifford, Robert Galinsky, Faezeh Marzbanrad)</author>
      <guid isPermaLink="false">2509.10082v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>D-CAT: Decoupled Cross-Attention Transfer between Sensor Modalities for Unimodal Inference</title>
      <link>http://arxiv.org/abs/2509.09747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了D-CAT（解耦跨注意力迁移）框架，用于改进多模态分类模型，特别是在人机协作中的人类活动识别，解决了资源受限环境中部署的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的跨模态迁移学习方法需要在训练和推理阶段都需要成对的传感器数据，限制了在资源受限环境中的应用，因为在这些环境中完整传感器套件在经济和技术上都不实用。&lt;h4&gt;目的&lt;/h4&gt;提出一个不需要在推理阶段联合传感器模态的框架，解决资源受限环境中的多模态分类问题，同时保持准确性并减少硬件冗余。&lt;h4&gt;方法&lt;/h4&gt;D-CAT框架结合了一个用于特征提取的自注意力模块和一个新颖的跨注意力对齐损失，强制对齐传感器特征空间，而不需要两个模态的分类管道耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在分布内场景中，从高性能模态迁移可获得高达10%的F1分数提升；在分布外场景中，即使较弱的源模态也能改善目标性能，只要目标模型没有在训练数据上过拟合；D-CAT通过跨模态知识实现单传感器推理，减少了感知系统的硬件冗余。&lt;h4&gt;结论&lt;/h4&gt;D-CAT框架能够在保持准确性的同时减少感知系统的硬件冗余，这对于成本敏感或适应性部署（如传感器可用性变化的家庭辅助机器人）至关重要。&lt;h4&gt;翻译&lt;/h4&gt;跨模态迁移学习用于改进多模态分类模型（例如人机协作中的人类活动识别）。然而，现有方法在训练和推理阶段都需要成对的传感器数据，限制了在资源受限环境中的部署，在这些环境中完整传感器套件在经济和技术上都不实用。为解决这一问题，我们提出了D-CAT（解耦跨注意力迁移）框架，该框架对齐模态特定表示，不需要在推理阶段联合传感器模态。我们的方法将自注意力模块用于特征提取，结合了一个新颖的跨注意力对齐损失，强制对齐传感器特征空间，而不需要两个模态分类管道的耦合。我们在三个多模态人类活动数据集（IMU、视频和音频）上评估了D-CAT，包括分布内和分布外场景，并与单模态模型进行比较。结果表明，在分布内场景中，从高性能模态（如视频到IMU）迁移可获得高达10%的F1分数提升。在分布外场景中，即使较弱的源模态（如IMU到视频）也能改善目标性能，只要目标模型没有在训练数据上过拟合。通过实现单传感器推理与跨模态知识，D-CAT减少了感知系统的硬件冗余，同时保持准确性，这对于成本敏感或适应性部署（如传感器可用性变化的家庭辅助机器人）至关重要。代码可在https://github.com/Schindler-EPFL-Lab/D-CAT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-modal transfer learning is used to improve multi-modal classificationmodels (e.g., for human activity recognition in human-robot collaboration).However, existing methods require paired sensor data at both training andinference, limiting deployment in resource-constrained environments where fullsensor suites are not economically and technically usable. To address this, wepropose Decoupled Cross-Attention Transfer (D-CAT), a framework that alignsmodality-specific representations without requiring joint sensor modalityduring inference. Our approach combines a self-attention module for featureextraction with a novel cross-attention alignment loss, which enforces thealignment of sensors' feature spaces without requiring the coupling of theclassification pipelines of both modalities. We evaluate D-CAT on threemulti-modal human activity datasets (IMU, video, and audio) under bothin-distribution and out-of-distribution scenarios, comparing against uni-modalmodels. Results show that in in-distribution scenarios, transferring fromhigh-performing modalities (e.g., video to IMU) yields up to 10% F1-score gainsover uni-modal training. In out-of-distribution scenarios, even weaker sourcemodalities (e.g., IMU to video) improve target performance, as long as thetarget model isn't overfitted on the training data. By enabling single-sensorinference with cross-modal knowledge, D-CAT reduces hardware redundancy forperception systems while maintaining accuracy, which is critical forcost-sensitive or adaptive deployments (e.g., assistive robots in homes withvariable sensor availability). Code is available athttps://github.com/Schindler-EPFL-Lab/D-CAT.</description>
      <author>example@mail.com (Leen Daher, Zhaobo Wang, Malcolm Mielle)</author>
      <guid isPermaLink="false">2509.09747v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Pipeline for Aortic Segmentation and Shape Analysis</title>
      <link>http://arxiv.org/abs/2509.09718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  STACOM 2025 with MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种稳健、全自动的主动脉形状分析流程，结合深度学习和统计技术，从心脏MRI中提取主动脉形状信息。&lt;h4&gt;背景&lt;/h4&gt;主动脉形状分析在心血管诊断、治疗规划和理解疾病进展中起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健、全自动的主动脉形状分析流程，提高分析精度和效率。&lt;h4&gt;方法&lt;/h4&gt;结合深度学习和统计技术，包括分割、3D表面重建和网格配准；对比nnUNet、TotalSegmentator和MedSAM2等分割模型；重建高质量3D网格；引入基于深度学习的网格配准方法，直接优化顶点位移。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在几何精度和解剖一致性上显著优于传统方法；主成分分析揭示了主动脉形状变化的主导模式，捕获了全局形态和局部结构差异。&lt;h4&gt;结论&lt;/h4&gt;整合传统几何处理与基于学习模型的优势，为解剖精确且可扩展的主动脉分析提供基础，支持心血管医学个性化诊断的发展。&lt;h4&gt;翻译&lt;/h4&gt;主动脉形状分析在心血管诊断、治疗规划和理解疾病进展中起着关键作用。我们提出了一种稳健的、全自动的主动脉形状分析流程，从心脏MRI中结合深度学习和统计技术，包括分割、3D表面重建和网格配准。我们基准测试了领先的分割模型，包括nnUNet、TotalSegmentator和MedSAM2，强调了在特定数据集上进行领域特定训练和迁移学习的有效性。分割后，我们重建高质量的3D网格，并引入一种基于深度学习的网格配准方法，直接优化顶点位移。这种方法在几何精度和解剖一致性上显著优于传统的刚性和非刚性方法。使用配准后的网格，我们对599名健康受试者进行统计形状分析。主成分分析揭示了主动脉形状变化的主导模式，捕获了刚性和相似变换下的全局形态和局部结构差异。我们的研究结果证明了整合传统几何处理与基于学习模型的优势，用于解剖精确且可扩展的主动脉分析。这项工作为未来研究病理形状偏差和心血管医学个性化诊断的发展奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从心脏MRI图像中进行主动脉分割和形状分析的自动化流程问题。由于MRI图像中主动脉与周围组织对比度低、分辨率低，导致分割困难，且传统表面重建和配准方法存在诸多挑战。这个问题在现实中很重要，因为准确的主动脉形状分析对心血管疾病诊断、治疗规划和疾病进展监测至关重要，能帮助建立健康参考模型以检测病理性偏差，支持个性化医疗发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为三个主要步骤：分割、3D表面重建和网格配准，针对每个步骤设计解决方案。他们系统评估了现有技术，包括nn-UNet、TotalSegmentator等分割模型，以及传统表面重建和配准方法。在此基础上，作者借鉴了现有的深度学习分割模型和经典几何处理算法，并在网格配准方面提出了创新，设计了一个结合统计方法和深度学习技术的集成管道。这种设计思路体现了对现有技术的批判性评估和有针对性的改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个全自动化的管道，结合深度学习和统计技术，通过领域特定训练和迁移学习提高分割性能，使用基于深度学习的网格配准方法直接优化顶点位移以实现更高精度，并利用统计形状分析揭示主动脉形状变化的主导模式。整体流程包括：数据准备与预处理；多种分割模型评估与选择；3D表面重建（等值面提取、顶点标准化、法线处理、网格重建）；模板选择与对齐；网格配准（比较传统方法并提出深度学习方法）；最后对599名健康受试者进行主成分分析以识别形状变化模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出了完整的端到端自动化管道；2) 系统评估并优化了分割方法，发现nn-UNet表现最佳；3) 创新性地提出基于深度学习的网格配准方法，直接优化顶点位移；4) 在大型健康队列上进行统计分析，识别主动脉形状变化模式。相比之前工作，本文提供了更全面的解决方案，在配准精度上显著优于传统方法，使用了更大的数据集进行统计分析，并成功整合了传统几何处理与基于学习的方法，实现了既解剖精确又可扩展的主动脉分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合深度学习和统计技术的全自动管道，实现了从心脏MRI中精确分割、重建和配准主动脉，并揭示了健康人群主动脉形状变化的主要模式，为心血管疾病的个性化诊断提供了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aortic shape analysis plays a key role in cardiovascular diagnostics,treatment planning, and understanding disease progression. We present a robust,fully automated pipeline for aortic shape analysis from cardiac MRI, combiningdeep learning and statistical techniques across segmentation, 3D surfacereconstruction, and mesh registration. We benchmark leading segmentation modelsincluding nnUNet, TotalSegmentator, and MedSAM2 highlighting the effectivenessof domain specific training and transfer learning on a curated dataset.Following segmentation, we reconstruct high quality 3D meshes and introduce aDL based mesh registration method that directly optimises vertex displacements.This approach significantly outperforms classical rigid and nonrigid methods ingeometric accuracy and anatomical consistency. Using the registered meshes, weperform statistical shape analysis on a cohort of 599 healthy subjects.Principal Component Analysis reveals dominant modes of aortic shape variation,capturing both global morphology and local structural differences under rigidand similarity transformations. Our findings demonstrate the advantages ofintegrating traditional geometry processing with learning based models foranatomically precise and scalable aortic analysis. This work lays thegroundwork for future studies into pathological shape deviations and supportsthe development of personalised diagnostics in cardiovascular medicine.</description>
      <author>example@mail.com (Nairouz Shehata, Amr Elsawy, Mohamed Nagy, Muhammad ElMahdy, Mariam Ali, Soha Romeih, Heba Aguib, Magdi Yacoub, Ben Glocker)</author>
      <guid isPermaLink="false">2509.09718v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SSL-AD: Spatiotemporal Self-Supervised Learning for Generalizability and Adaptability Across Alzheimer's Prediction Tasks and Datasets</title>
      <link>http://arxiv.org/abs/2509.10453v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过采用三种先进的时序自监督学习方法处理3D脑部MRI数据，解决了阿尔茨海默病预测中标记数据缺乏、跨数据集泛化能力差以及输入灵活性不足的问题。模型在四个数据集的3161名患者上进行了预训练，并在多种预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病是一种导致记忆丧失和认知能力下降的进行性神经退行性疾病。现有的深度学习模型应用于阿尔茨海默病预测时受限于标记数据缺乏、跨数据集泛化能力差，以及无法处理不同数量的输入扫描和扫描间时间间隔。&lt;h4&gt;目的&lt;/h4&gt;适应三种先进的时序自监督学习方法用于3D脑部MRI分析，并添加新型扩展以处理可变长度输入和鲁棒空间特征学习。&lt;h4&gt;方法&lt;/h4&gt;聚合四个公开数据集，包含3161名患者进行预训练，评估模型在阿尔茨海默病多种预测任务中的性能，包括诊断分类、转化检测和未来转化预测。&lt;h4&gt;主要发现&lt;/h4&gt;采用时序顺序预测和对比学习的自监督学习模型在七个下游任务中的六个表现优于监督学习。该模型展示了跨任务和不同数量输入图像（具有不同时间间隔）的适应性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该自监督学习模型在临床应用中表现出强大的适应性和鲁棒性能，作者已公开代码和模型供进一步研究使用。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病是一种进行性神经退行性疾病，会导致记忆丧失和认知能力下降。尽管已有大量研究将深度学习模型应用于阿尔茨海默病预测任务，但这些模型仍受限于可用标记数据的缺乏、跨数据集的泛化能力差，以及无法处理不同数量的输入扫描和扫描间时间间隔。在本研究中，我们调整了三种最先进的时序自学习方法用于3D脑部MRI分析，并添加了新型扩展以处理可变长度输入和鲁棒空间特征学习。我们聚合了四个包含3161名患者的公开数据集进行预训练，展示了我们的模型在多种阿尔茨海默病预测任务中的性能，包括诊断分类、转化检测和未来转化预测。重要的是，我们采用时序顺序预测和对比学习的自监督模型在七个下游任务中的六个上优于监督学习。它展示了跨任务和不同数量输入图像（具有不同时间间隔）的适应性和泛化能力，突显了其在临床应用中保持鲁棒性能的能力。我们在https://github.com/emilykaczmarek/SSL-AD公开了我们的代码和模型。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决阿尔茨海默病预测中深度学习模型的三大局限性：缺乏足够标记数据、模型在不同数据集间泛化能力差、无法灵活处理不同数量的输入扫描图像和时间间隔。这个问题在现实中非常重要，因为阿尔茨海默病是一种进行性疾病，早期准确预测对治疗干预至关重要，而现有模型受限于数据不足和临床场景的多样性，难以充分利用患者多次就诊的完整信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到阿尔茨海默病作为随时间发展的疾病，其预测需要模型能捕捉时间变化特征。他们发现自监督学习可解决标记数据不足问题，但现有SSL方法在阿尔茨海默病应用中存在三大局限。因此，他们借鉴了计算机视觉领域的时间顺序验证和预测方法，结合对比学习技术，设计了三种模型：时间顺序验证(SSL-TOV)、时间顺序预测(SSL-TOP)和结合对比学习的SSL-TOPC。他们创新性地扩展了这些方法以处理医学影像的特殊性，如可变长度输入和3D脑MRI分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自监督学习从大量未标记脑MRI数据中学习通用表示，利用时间顺序预测任务使模型关注疾病进展的微妙变化，并结合对比学习增强空间特征鲁棒性。整体流程包括：1)聚合四个公开数据集共3,161名患者进行预训练；2)对数据进行标准化预处理；3)设计三种模型分别进行时间顺序验证、预测和结合对比学习；4)在七个下游任务(如分类、转换检测、未来转换预测)上评估模型性能。模型能处理1-4个输入图像，适应不同时间间隔，并在大多数任务上优于监督学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)整合四个大规模数据集进行预训练；2)创新性地将时间顺序预测应用于3D脑MRI分析；3)设计专门方法处理可变长度输入，无需填充；4)结合时间顺序预测和对比学习增强时空特征；5)在七个不同任务上全面评估模型性能。相比之前工作，本研究首次提出完全可适应、可泛化的自监督框架，解决了现有SSL模型缺乏大型数据集、忽略时间学习和无法处理可变输入的问题，特别是SSL-TOPC模型在六个 out of seven 的下游任务上超越了监督学习基线。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的时空自监督学习框架，通过整合大规模脑MRI数据和时间顺序预测任务，显著提高了阿尔茨海默病预测模型的泛化能力和适应性，特别是在标记数据有限和临床应用场景多变的情况下。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Alzheimer's disease is a progressive, neurodegenerative disorder that causesmemory loss and cognitive decline. While there has been extensive research inapplying deep learning models to Alzheimer's prediction tasks, these modelsremain limited by lack of available labeled data, poor generalization acrossdatasets, and inflexibility to varying numbers of input scans and timeintervals between scans. In this study, we adapt three state-of-the-arttemporal self-supervised learning (SSL) approaches for 3D brain MRI analysis,and add novel extensions designed to handle variable-length inputs and learnrobust spatial features. We aggregate four publicly available datasetscomprising 3,161 patients for pre-training, and show the performance of ourmodel across multiple Alzheimer's prediction tasks including diagnosisclassification, conversion detection, and future conversion prediction.Importantly, our SSL model implemented with temporal order prediction andcontrastive learning outperforms supervised learning on six out of sevendownstream tasks. It demonstrates adaptability and generalizability acrosstasks and number of input images with varying time intervals, highlighting itscapacity for robust performance across clinical applications. We release ourcode and model publicly at https://github.com/emilykaczmarek/SSL-AD.</description>
      <author>example@mail.com (Emily Kaczmarek, Justin Szeto, Brennan Nichyporuk, Tal Arbel)</author>
      <guid isPermaLink="false">2509.10453v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Data distribution impacts the performance and generalisability of contrastive learning-based foundation models of electrocardiograms</title>
      <link>http://arxiv.org/abs/2509.10369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review at npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;对比学习对队列组成的依赖性是一个重要但未被充分探索的问题。CAPE基础模型和IDB策略解决了多中心、多样化队列预训练中的挑战，队列组成对模型性能有显著影响，需要考虑临床公平性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;对比学习是一种广泛采用的自我监督预训练策略，但其对队列组成的依赖性尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;系统评估队列人口统计、健康状态和人口多样性如何影响下游预测性能，开发一种临床公平且可推广的基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出了基于患者增强心电图(CAPE)的基础模型，在四个队列(n = 5,203,352)上进行预训练，这些队列来自三个大洲(北美、南美、亚洲)的多样化人群；评估了两个来自欧洲的额外队列的预测任务；提出了In-Distribution Batch (IDB)策略，在预训练期间保留队列内部一致性并增强OOD鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;下游性能取决于预训练队列的分布特性，包括人口统计和健康状态；使用多中心、人口统计多样化的队列进行预训练可以提高分布内准确率，但通过编码队列特定的人工制品，降低了对比方法的分布外(OOD)泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作为开发临床公平且可推广的基础模型提供了重要见解。&lt;h4&gt;翻译&lt;/h4&gt;对比学习是一种广泛采用的自我监督预训练策略，但其对队列组成的依赖性仍未得到充分探索。我们提出了基于患者增强心电图(CAPE)的基础模型，并在来自三个大洲(北美、南美、亚洲)的多样化人群中，在四个队列(n = 5,203,352)上进行预训练。我们系统评估了队列人口统计、健康状态和人口多样性如何影响预测任务的下游性能，这些任务还包括来自另一个大陆(欧洲)的两个额外队列。我们发现下游性能取决于预训练队列的分布特性，包括人口统计和健康状态。此外，虽然使用多中心、人口统计多样化的队列进行预训练可以提高分布内准确率，但它通过编码队列特定的人工制品，降低了我们对比方法的分布外(OOD)泛化能力。为解决这一问题，我们提出了In-Distribution Batch (IDB)策略，该策略在预训练期间保留队列内部一致性并增强OOD鲁棒性。这项工作为开发临床公平且可推广的基础模型提供了重要见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning is a widely adopted self-supervised pretrainingstrategy, yet its dependence on cohort composition remains underexplored. Wepresent Contrasting by Patient Augmented Electrocardiograms (CAPE) foundationmodel and pretrain on four cohorts (n = 5,203,352), from diverse populationsacross three continents (North America, South America, Asia). We systematicallyassess how cohort demographics, health status, and population diversityinfluence the downstream performance for prediction tasks also including twoadditional cohorts from another continent (Europe). We find that downstreamperformance depends on the distributional properties of the pretraining cohort,including demographics and health status. Moreover, while pretraining with amulti-centre, demographically diverse cohort improves in-distribution accuracy,it reduces out-of-distribution (OOD) generalisation of our contrastive approachby encoding cohort-specific artifacts. To address this, we propose theIn-Distribution Batch (IDB) strategy, which preserves intra-cohort consistencyduring pretraining and enhances OOD robustness. This work provides importantinsights for developing clinically fair and generalisable foundation models.</description>
      <author>example@mail.com (Gul Rukh Khattak, Konstantinos Patlatzoglou, Joseph Barker, Libor Pastika, Boroumand Zeidaabadi, Ahmed El-Medany, Hesham Aggour, Yixiu Liang, Antonio H. Ribeiro, Jeffrey Annis, Antonio Luiz Pinho Ribeiro, Junbo Ge, Daniel B. Kramer, Jonathan W. Waks, Evan Brittain, Nicholas Peters, Fu Siong Ng, Arunashis Sau)</author>
      <guid isPermaLink="false">2509.10369v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>GLAM: Geometry-Guided Local Alignment for Multi-View VLP in Mammography</title>
      <link>http://arxiv.org/abs/2509.10344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLAM的新型深度学习方法，用于改进乳腺X光筛查中的视觉语言模型预训练，通过利用多视图成像过程的先验知识，学习局部跨视图对齐和细粒度特征，在多个数据集上超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;乳腺X光筛查是早期发现乳腺癌的重要工具，深度学习可提高其解释速度和准确性。然而，视觉语言模型的发展受限于数据有限和自然图像与医学图像间的领域差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模乳腺X光多视图关系的视觉语言模型，解决现有方法忽略领域特定特征和多视图关系的问题。&lt;h4&gt;方法&lt;/h4&gt;提出GLAM模型：全局和局部对齐的多视图乳腺X光模型，利用乳腺X光多视图成像过程的先验知识，通过联合全局和局部、视觉-视觉以及视觉语言对比学习来学习局部跨视图对齐和细粒度局部特征，并在EMBED数据集上预训练。&lt;h4&gt;主要发现&lt;/h4&gt;GLAM模型在不同设置下的多个数据集上均优于现有基线方法，表明其能有效捕捉乳腺X光的多视图关系和领域特定特征。&lt;h4&gt;结论&lt;/h4&gt;通过利用乳腺X光成像过程的先验知识和多视图关系，GLAM模型改进了视觉语言模型在乳腺X光筛查中的应用效果，为乳腺癌早期检测提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;乳腺X光筛查是早期发现乳腺癌的重要工具。深度学习方法有望提高乳腺X光解释的速度和准确性。然而，基础视觉语言模型(VLM)的发展受到数据有限和自然图像与医学图像之间领域差异的阻碍。现有的乳腺X光VLM通常从自然图像改编而来，忽略了多视图等特定领域特征。与放射科医生不同，当前方法将两个视图视为独立图像或未正确建模多视图对应关系学习，导致失去关键几何上下文和次优预测。我们提出GLAM：利用几何引导进行VLM预训练的全局和局部对齐多视图乳腺X光模型。通过利用乳腺X光多视图成像过程的先验知识，我们的模型通过联合全局和局部、视觉-视觉以及视觉语言对比学习，学习局部跨视图对齐和细粒度局部特征。在最大的开放乳腺X光数据集之一EMBED上预训练后，我们的模型在不同设置下的多个数据集上都优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mammography screening is an essential tool for early detection of breastcancer. The speed and accuracy of mammography interpretation have the potentialto be improved with deep learning methods. However, the development of afoundation visual language model (VLM) is hindered by limited data and domaindifferences between natural and medical images. Existing mammography VLMs,adapted from natural images, often ignore domain-specific characteristics, suchas multi-view relationships in mammography. Unlike radiologists who analyzeboth views together to process ipsilateral correspondence, current methodstreat them as independent images or do not properly model the multi-viewcorrespondence learning, losing critical geometric context and resulting insuboptimal prediction. We propose GLAM: Global and Local Alignment forMulti-view mammography for VLM pretraining using geometry guidance. Byleveraging the prior knowledge about the multi-view imaging process ofmammograms, our model learns local cross-view alignments and fine-grained localfeatures through joint global and local, visual-visual, and visual-languagecontrastive learning. Pretrained on EMBED [14], one of the largest openmammography datasets, our model outperforms baselines across multiple datasetsunder different settings.</description>
      <author>example@mail.com (Yuexi Du, Lihui Chen, Nicha C. Dvornek)</author>
      <guid isPermaLink="false">2509.10344v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SignClip: Leveraging Mouthing Cues for Sign Language Translation by Multimodal Contrastive Fusion</title>
      <link>http://arxiv.org/abs/2509.10266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SignClip是一种新的手语翻译框架，通过融合手动和非手动线索（空间手势和嘴唇运动特征），并采用层次对比学习框架，显著提高了手语翻译的准确性。&lt;h4&gt;背景&lt;/h4&gt;手语翻译旨在从手语视频中翻译自然语言，作为包容性交流的重要桥梁。最近的进展利用了强大的视觉骨干网络和大语言模型，但大多数方法主要关注手动信号（手部动作）而往往忽略非手动线索如口型。&lt;h4&gt;目的&lt;/h4&gt;提出SignClip框架，提高手语翻译的准确性。&lt;h4&gt;方法&lt;/h4&gt;SignClip融合手动和非手动线索，特别是空间手势和嘴唇运动特征。此外，SignClip引入了具有多级对齐目标的层次对比学习框架，确保手语-嘴唇和视觉-文本模态之间的语义一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了该方法的优势。例如，在PHOENIX14T上，Gloss-free设置中，SignClip超越了之前的最先进模型SpaMo，将BLEU-4从24.32提高到24.71，ROUGE从46.57提高到48.38。&lt;h4&gt;结论&lt;/h4&gt;SignClip通过融合手势和口型特征，并采用层次对比学习框架，显著提高了手语翻译的准确性。&lt;h4&gt;翻译&lt;/h4&gt;手语翻译旨在从手语视频中翻译自然语言，作为包容性交流的重要桥梁。尽管最近的进展利用了强大的视觉骨干网络和大语言模型，但大多数方法主要关注手动信号（手部动作）而往往忽略非手动线索如口型。事实上，口型在手语中传达重要的语言信息，并在区分视觉相似的手势中起关键作用。本文提出SignClip，一种新的手语翻译框架，通过融合手动和非手动线索（特别是空间手势和嘴唇运动特征）来提高手语翻译的准确性。此外，SignClip引入了具有多级对齐目标的层次对比学习框架，确保手语-嘴唇和视觉-文本模态之间的语义一致性。在PHOENIX14T和How2Sign两个基准数据集上的大量实验证明了该方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language translation (SLT) aims to translate natural language from signlanguage videos, serving as a vital bridge for inclusive communication. Whilerecent advances leverage powerful visual backbones and large language models,most approaches mainly focus on manual signals (hand gestures) and tend tooverlook non-manual cues like mouthing. In fact, mouthing conveys essentiallinguistic information in sign languages and plays a crucial role indisambiguating visually similar signs. In this paper, we propose SignClip, anovel framework to improve the accuracy of sign language translation. It fusesmanual and non-manual cues, specifically spatial gesture and lip movementfeatures. Besides, SignClip introduces a hierarchical contrastive learningframework with multi-level alignment objectives, ensuring semantic consistencyacross sign-lip and visual-text modalities. Extensive experiments on twobenchmark datasets, PHOENIX14T and How2Sign, demonstrate the superiority of ourapproach. For example, on PHOENIX14T, in the Gloss-free setting, SignClipsurpasses the previous state-of-the-art model SpaMo, improving BLEU-4 from24.32 to 24.71, and ROUGE from 46.57 to 48.38.</description>
      <author>example@mail.com (Wenfang Wu, Tingting Yuan, Yupeng Li, Daling Wang, Xiaoming Fu)</author>
      <guid isPermaLink="false">2509.10266v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning</title>
      <link>http://arxiv.org/abs/2509.10208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SI FACT的自我改进框架，用于解决大型语言模型在知识密集型任务中生成不忠实响应的问题。该框架通过自我指导机制生成对比学习数据，并应用对比学习训练模型，显著提高了模型的上下文忠实性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在知识密集型任务中经常生成不忠实的响应，这是因为知识冲突，即模型倾向于依赖内部参数知识而非提供的上下文。&lt;h4&gt;目的&lt;/h4&gt;解决大型语言模型在知识密集型任务中由于知识冲突而产生的不忠实响应问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Self Improving Faithfulness Aware Contrastive Tuning(SI FACT)的自我改进框架，该框架使用自我指导机制让基础LLM自动生成高质量的、结构化的对比学习数据，包括锚样本、语义等价正样本和模拟不忠实场景的负样本，然后应用对比学习训练模型，使其在表示空间中拉近忠实响应，推远不忠实响应。&lt;h4&gt;主要发现&lt;/h4&gt;在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。&lt;h4&gt;结论&lt;/h4&gt;SI FACT在增强LLM的上下文忠实性方面提供了有效性和高数据效率，为构建更加主动和可信赖的语言模型提供了实用途径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在知识密集型任务中经常因知识冲突而产生不忠实响应，即倾向于依赖内部参数知识而非提供的上下文。为解决这一问题，我们提出了一种新颖的自我改进框架——自我提高忠实性感知对比调优(SI FACT)。该框架使用自我指导机制，使基础LLM能够自动生成高质量、结构化的对比学习数据，包括锚样本、语义等价正样本和模拟不忠实场景的负样本。这种方法显著降低了手动标注成本。随后，应用对比学习训练模型，使其在表示空间中拉近忠实响应，推远不忠实响应。在知识冲突评估基准ECARE KRE和COSE KRE上的实验表明，基于Llama3 8B Instruct的SI FACT模型比最佳基线方法提高了6.2%的上下文召回率，同时显著减少了对内部记忆的依赖。结果表明，SI FACT在增强LLM的上下文忠实性方面提供了强大的有效性和高数据效率，为构建更加主动和可信赖的语言模型提供了实用途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models often generate unfaithful responses in knowledgeintensive tasks due to knowledge conflict,that is,a preference for relying oninternal parametric knowledge rather than the provided context.To address thisissue,we propose a novel self improving framework,Self Improving FaithfulnessAware Contrastive Tuning.The framework uses a self instruct mechanism thatallows the base LLM to automatically generate high quality,structuredcontrastive learning data,including anchor samples,semantically equivalentpositive samples,and negative samples simulating unfaithful scenarios.Thisapproach significantly reduces the cost of manualannotation.Subsequently,contrastive learning is applied to train themodel,enabling it to pull faithful responses closer and push unfaithfulresponses farther apart in the representation space.Experiments on knowledgeconflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACTmodel based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2%over the best baseline method,while significantly reducing dependence oninternal memory.The results indicate that SI FACT provides strong effectivenessand high data efficiency in enhancing the contextual faithfulness ofLLMs,offering a practical pathway toward building more proactive andtrustworthy language models.</description>
      <author>example@mail.com (Shengqiang Fu)</author>
      <guid isPermaLink="false">2509.10208v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Grad-CL: Source Free Domain Adaptation with Gradient Guided Feature Disalignment</title>
      <link>http://arxiv.org/abs/2509.10134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Grad-CL的新型无源域适应框架，用于解决视盘和视杯分割在不同成像条件下的适应性问题。该框架结合了梯度引导的伪标签细化和基于余弦相似度的对比学习策略，无需访问原始源数据即可实现稳健的分割性能。&lt;h4&gt;背景&lt;/h4&gt;视盘和视杯的准确分割对于眼科疾病（如青光眼）的早期诊断和管理至关重要。然而，在一个数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会经历显著的性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决分割模型在不同成像条件下性能下降的问题，提出一种无需访问原始源数据的无源域适应框架，实现视盘和视杯分割的稳健适应。&lt;h4&gt;方法&lt;/h4&gt;Grad-CL框架结合两个主要阶段：第一阶段通过基于梯度的机制提取显著的类别特定特征，实现更准确的不确定性量化和稳健的原型估计，用于细化有噪声的伪标签；第二阶段采用基于余弦相似度的对比损失，明确强制执行视杯和视盘的梯度引导特征之间的类间可分离性。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的跨域眼底成像数据集上进行的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了 superior 的分割精度和改进的边界描绘。&lt;h4&gt;结论&lt;/h4&gt;Grad-CL是一种有效的无源域适应方法，可以解决视盘和视杯分割在不同成像条件下的适应性问题，无需原始源数据。&lt;h4&gt;翻译&lt;/h4&gt;准确的视盘和视杯分割对于眼科疾病（如青光眼）的早期诊断和管理至关重要。然而，在一个数据集上训练的分割模型在应用于不同成像协议或条件下获取的目标数据时，往往会经历显著的性能下降。为应对这一挑战，我们提出Grad-CL，一种新颖的无源域适应框架，它利用预训练的源模型和无标签的目标数据，无需访问原始源数据即可稳健地适应分割性能。Grad-CL结合了基于梯度引导的伪标签细化模块和基于余弦相似度的对比学习策略。在第一阶段，通过基于梯度的机制提取显著的类别特定特征，实现更准确的不确定性量化和稳健的原型估计，用于细化有噪声的伪标签。在第二阶段，采用基于余弦相似度的对比损失，明确强制执行视杯和视盘的梯度引导特征之间的类间可分离性。在具有挑战性的跨域眼底成像数据集上进行的大量实验表明，Grad-CL优于最先进的无监督和无源域适应方法，实现了 superior 的分割精度和改进的边界描绘。项目和代码可在https://visdomlab.github.io/GCL/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of the optic disc and cup is critical for the earlydiagnosis and management of ocular diseases such as glaucoma. However,segmentation models trained on one dataset often suffer significant performancedegradation when applied to target data acquired under different imagingprotocols or conditions. To address this challenge, we propose\textbf{Grad-CL}, a novel source-free domain adaptation framework thatleverages a pre-trained source model and unlabeled target data to robustlyadapt segmentation performance without requiring access to the original sourcedata. Grad-CL combines a gradient-guided pseudolabel refinement module with acosine similarity-based contrastive learning strategy. In the first stage,salient class-specific features are extracted via a gradient-based mechanism,enabling more accurate uncertainty quantification and robust prototypeestimation for refining noisy pseudolabels. In the second stage, a contrastiveloss based on cosine similarity is employed to explicitly enforce inter-classseparability between the gradient-informed features of the optic cup and disc.Extensive experiments on challenging cross-domain fundus imaging datasetsdemonstrate that Grad-CL outperforms state-of-the-art unsupervised andsource-free domain adaptation methods, achieving superior segmentation accuracyand improved boundary delineation. Project and code are available athttps://visdomlab.github.io/GCL/.</description>
      <author>example@mail.com (Rini Smita Thakur, Rajeev Ranjan Dwivedi, Vinod K Kurmi)</author>
      <guid isPermaLink="false">2509.10134v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Prototypical Contrastive Learning For Improved Few-Shot Audio Classification</title>
      <link>http://arxiv.org/abs/2509.10074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and Presented at IEEE International Workshop on Machine  Learning for Signal Processing, Aug.\ 31-- Sep.\ 3, 2025, Istanbul, Turkey ,  6 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探索了在音频分类中将监督对比损失整合到原型小样本训练中的方法，通过结合SpecAugment和自注意力机制，实现了在5路5 shot设置下的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;小样本学习已成为处理标记数据有限场景的有效方法，但在图像领域已有大量研究，而音频分类中的小样本学习仍然相对未被充分探索。大规模数据标注在某些场景下不切实际，因此需要开发小样本学习方法。&lt;h4&gt;目的&lt;/h4&gt;研究旨在探索监督对比损失在音频分类小样本学习中的应用，并比较不同对比损失（标准对比损失与角度损失）的性能，同时开发一种结合SpecAugment和自注意力机制的新方法。&lt;h4&gt;方法&lt;/h4&gt;研究将监督对比损失整合到原型小样本训练框架中，特别采用了角度损失而非标准对比损失。方法利用SpecAugment进行数据增强，然后通过自注意力机制将增强输入版本的多样化信息封装到一个统一的嵌入中。实验在MetaAudio基准数据集上进行，该数据集包含五个预定义分割的标准化数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，与标准对比损失相比，角度损失能进一步提高性能。提出的结合SpecAugment和自注意力机制的方法在5路5 shot设置下实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;将监督对比损失（特别是角度损失）整合到原型小样本训练中，结合SpecAugment和自注意力机制，是音频分类小样本学习的有效方法，能够达到当前最先进的性能水平。&lt;h4&gt;翻译&lt;/h4&gt;小样本学习已成为一种强大的范式，用于训练标记数据有限的模型，解决了大规模标注不切实际场景中的挑战。尽管图像领域已有大量研究，但音频分类中的小样本学习仍然相对未被探索。在这项工作中，我们研究了将监督对比损失整合到原型小样本训练中对音频分类的影响。具体而言，我们证明与标准对比损失相比，角度损失能进一步提高性能。我们的方法利用SpecAugment，然后通过自注意力机制，将增强输入版本的多样化信息封装到一个统一的嵌入中。我们在MetaAudio上评估了我们的方法，这是一个包含五个数据集的基准，具有预定义的分割、标准化的预处理和一套全面的用于比较的小样本学习模型。在5路、5 shot设置下，我们提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot learning has emerged as a powerful paradigm for training models withlimited labeled data, addressing challenges in scenarios where large-scaleannotation is impractical. While extensive research has been conducted in theimage domain, few-shot learning in audio classification remains relativelyunderexplored. In this work, we investigate the effect of integratingsupervised contrastive loss into prototypical few shot training for audioclassification. In detail, we demonstrate that angular loss further improvesthe performance compared to the standard contrastive loss. Our method leveragesSpecAugment followed by a self-attention mechanism to encapsulate diverseinformation of augmented input versions into one unified embedding. We evaluateour approach on MetaAudio, a benchmark including five datasets with predefinedsplits, standardized preprocessing, and a comprehensive set of few-shotlearning models for comparison. The proposed approach achieves state-of-the-artperformance in a 5-way, 5-shot setting.</description>
      <author>example@mail.com (Christos Sgouropoulos, Christos Nikou, Stefanos Vlachos, Vasileios Theiou, Christos Foukanelis, Theodoros Giannakopoulos)</author>
      <guid isPermaLink="false">2509.10074v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>On Syntactical Simplification of Temporal Operators in Negation-free MTL</title>
      <link>http://arxiv.org/abs/2509.10146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了无否定MTL时态逻辑框架的表达能力，发现可以仅使用'until'和'since'算子构建强大的时态逻辑片段，挑战了否定对于表达普遍时态约束的必要性假设。&lt;h4&gt;背景&lt;/h4&gt;在动态、数据密集型环境中进行时态推理需要表达性强且可处理的逻辑框架。传统方法依赖否定表达缺失或矛盾，但在开放分布式系统如物联网网络或语义网中，由于数据不完整和异步性，否定失败语义变得不可靠。&lt;h4&gt;目的&lt;/h4&gt;研究无否定MTL（时态逻辑框架）的表达能力，该框架用于基于规则的时态推理。&lt;h4&gt;方法&lt;/h4&gt;展示如何使用'once'、'since'和'until'算子消除MTL中的'always'算子，并进一步展示如何移除'once'算子，仅基于'until'和'since'构建片段。&lt;h4&gt;主要发现&lt;/h4&gt;'always'算子和'once'算子都可以被消除，可以仅使用'until'和'since'算子构建一个能够捕获存在性和不变时态模式的强大片段。&lt;h4&gt;结论&lt;/h4&gt;挑战了否定对于表达普遍时态约束的必要性假设，揭示了能够捕获存在性和不变时态模式的强大片段，结果导致MTL语法的简化，为理论研究和实现工作带来好处。&lt;h4&gt;翻译&lt;/h4&gt;在动态、数据密集型环境中的时态推理越来越需要表达性强且可处理的逻辑框架。传统方法通常依赖否定来表达缺失或矛盾。在这种情况下，否定失败常用于从缺乏积极证据中推断负面信息。然而，在物联网网络或语义网等开放和分布式系统中，由于数据不完整和异步性，否定失败语义变得不可靠。这导致了对无否定时态规则系统片段的日益关注，这些片段保持单调性并支持可扩展的推理。本文研究了无否定MTL的表达能力，这是一种为时间上的基于规则推理而设计的时态逻辑框架。我们展示了MTL的'always'算子通常被视为其他时态结构的语法糖，可以使用'once'、'since'和'until'算子消除。值得注意的是，即使'once'算子也可以被移除，产生一个仅基于'until'和'since'的片段。这些结果挑战了否定对于表达普遍时态约束的必要性假设，并揭示了一个能够捕获存在性和不变时态模式的强大片段。此外，这些结果导致了MTL语法的简化，这可以为理论研究以及实现工作带来好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning in dynamic, data-intensive environments increasinglydemands expressive yet tractable logical frameworks. Traditional approachesoften rely on negation to express absence or contradiction. In such contexts,Negation-as-Failure is commonly used to infer negative information from thelack of positive evidence. However, open and distributed systems such as IoTnetworks or the Semantic Web Negation-as-Failure semantics become unreliabledue to incomplete and asynchronous data. This has led to a growing interest innegation-free fragments of temporal rule-based systems, which preservemonotonicity and enable scalable reasoning.  This paper investigates the expressive power of negation-free MTL, a temporallogic framework designed for rule-based reasoning over time. We show that the"always" operators of MTL, often treated as syntactic sugar for combinations ofother temporal constructs, can be eliminated using "once", "since" and "until"operators. Remarkably, even the "once" operators can be removed, yielding afragment based solely on "until" and "since". These results challenge theassumption that negation is necessary for expressing universal temporalconstraints, and reveal a robust fragment capable of capturing both existentialand invariant temporal patterns. Furthermore, the results induce a reduction inthe syntax of MTL, which in turn can provide benefits for both theoreticalstudy as well as implementation efforts.</description>
      <author>example@mail.com (Mathijs van Noort, Femke Ongenae, Pieter Bonte)</author>
      <guid isPermaLink="false">2509.10146v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Multi-modal Adaptive Estimation for Temporal Respiratory Disease Outbreak</title>
      <link>http://arxiv.org/abs/2509.08578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MAESTRO框架，一种用于流感发病率预测的新型统一方法，通过整合多模态数据和先进的谱时建模实现了高准确率的预测。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测流感发病率的方法，为公共卫生决策提供支持。&lt;h4&gt;方法&lt;/h4&gt;提出MAESTRO框架，整合先进的谱时建模与多模态数据融合（包括监测数据、网络搜索趋势和气象数据），通过自适应加权异构数据源和分解复杂时间序列模式实现预测。&lt;h4&gt;主要发现&lt;/h4&gt;在香港11年以上的流感数据评估中，MAESTRO达到了0.956的R平方值，展示了最先进的预测性能；消融实验证实了多模态和谱时成分的重要贡献。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO是一种模块化、可复现的预测工具，已公开可用，可部署到其他地区和病原体，为流行病学预测提供了强大工具。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。本文提出了MAESTRO（多模态时间呼吸道疾病爆发自适应估计），一种新颖的统一框架，该框架协同整合了先进的谱时建模与多模态数据融合，包括监测数据、网络搜索趋势和气象数据。通过自适应加权异构数据源和分解复杂时间序列模式，该模型实现了稳健且准确的预测。在香港11年以上的流感数据评估中（排除COVID-19期间），MAESTRO展示了最先进的性能，实现了0.956的R平方值的优越模型拟合。大量的消融实验证实了其多模态和谱时成分的重要贡献。模块化和可复现的管道已公开可用，便于部署和扩展到其他地区和病原体，为流行病学预测提供了强大工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. This paper presents MAESTRO (Multi-modal AdaptiveEstimation for Temporal Respiratory Disease Outbreak), a novel, unifiedframework that synergistically integrates advanced spectro-temporal modelingwith multi-modal data fusion, including surveillance, web search trends, andmeteorological data. By adaptively weighting heterogeneous data sources anddecomposing complex time series patterns, the model achieves robust andaccurate forecasts. Evaluated on over 11 years of Hong Kong influenza data(excluding the COVID-19 period), MAESTRO demonstrates state-of-the-artperformance, achieving a superior model fit with an R-square of 0.956.Extensive ablations confirm the significant contributions of its multi-modaland spectro-temporal components. The modular and reproducible pipeline is madepublicly available to facilitate deployment and extension to other regions andpathogens, presenting a powerful tool for epidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu, Kerui Cen, Yanxing Chen, Zige Liu, Dong Chen, Zifeng Yang, Chitin Hon)</author>
      <guid isPermaLink="false">2509.08578v2</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Multiscaling in Wasserstein Spaces</title>
      <link>http://arxiv.org/abs/2509.10415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的多尺度框架，用于分析欧几里得域上Wasserstein空间中的概率测度序列。该框架利用最优传输的内在几何结构，构建了适用于绝对连续和离散测度的多尺度变换，并引入了最优性数量来量化序列与Wasserstein测地线的偏差，从而检测不规则动态和异常。&lt;h4&gt;背景&lt;/h4&gt;在欧几里得域上的Wasserstein空间中分析概率测度序列是当前研究的一个挑战领域，特别是需要同时处理绝对连续和离散测度的情况。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效分析概率测度序列的多尺度框架，检测不规则动态和异常，并提供稳健且可解释的多尺度表示。&lt;h4&gt;方法&lt;/h4&gt;研究基于最优传输的内在几何结构构建多尺度变换，核心是基于McCann插值的细化算子，该算子保留了测度流的测地线结构并作为上采样机制。此外，引入了最优性数量这一标量，用于量化序列与Wasserstein测地线的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;研究建立了关键的理论保证，包括变换的稳定性和系数的几何衰减，确保了多尺度表示的稳健性和可解释性。通过数值实验展示了该方法在高斯流去噪和异常检测、向量场下点云动态分析以及神经网络学习轨迹多尺度表征方面的多功能性。&lt;h4&gt;结论&lt;/h4&gt;该多尺度框架为分析概率测度序列提供了一种强大而灵活的工具，能够有效检测不规则动态和异常，并在多种应用场景中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的多尺度框架，用于分析欧几里得域上Wasserstein空间中的概率测度序列。利用最优传输的内在几何结构，我们构建了适用于绝对连续和离散测度的多尺度变换。我们方法的核心是基于McCann插值的细化算子，它保留了测度流的测地线结构并作为上采样机制。在此基础上，我们引入了最优性数量，这是一个标量，用于量化序列跨尺度偏离Wasserstein测地线的程度，从而能够检测不规则动态和异常。我们建立了关键的理论保证，包括变换的稳定性和系数的几何衰减，确保了多尺度表示的稳健性和可解释性。最后，我们通过数值实验展示了我们方法的多功能性：高斯流中的去噪和异常检测、向量场下点云动态的分析，以及神经网络学习轨迹的多尺度表征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel multiscale framework for analyzing sequences ofprobability measures in Wasserstein spaces over Euclidean domains. Exploitingthe intrinsic geometry of optimal transport, we construct a multiscaletransform applicable to both absolutely continuous and discrete measures.Central to our approach is a refinement operator based on McCann'sinterpolants, which preserves the geodesic structure of measure flows andserves as an upsampling mechanism. Building on this, we introduce theoptimality number, a scalar that quantifies deviations of a sequence fromWasserstein geodesicity across scales, enabling the detection of irregulardynamics and anomalies. We establish key theoretical guarantees, includingstability of the transform and geometric decay of coefficients, ensuringrobustness and interpretability of the multiscale representation. Finally, wedemonstrate the versatility of our methodology through numerical experiments:denoising and anomaly detection in Gaussian flows, analysis of point clouddynamics under vector fields, and the multiscale characterization of neuralnetwork learning trajectories.</description>
      <author>example@mail.com (Wael Mattar, Nir Sharon)</author>
      <guid isPermaLink="false">2509.10415v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Acetrans: An Autonomous Corridor-Based and Efficient UAV Suspended Transport System</title>
      <link>http://arxiv.org/abs/2509.10349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Acetrans系统，一种自主的、基于走廊的、高效的无人机悬吊运输系统，通过统一的感知、规划和控制框架解决现有系统在感知、规划和控制方面的关键限制。&lt;h4&gt;背景&lt;/h4&gt;无人机悬吊载荷在复杂和杂乱环境中的空中运输具有显著优势，但现有系统面临感知不可靠、大规模环境中规划效率低、无法保证在电缆弯曲和外部干扰下的全身安全等关键限制。&lt;h4&gt;目的&lt;/h4&gt;提出Acetrans系统，解决现有无人机悬吊运输系统在感知、规划和控制方面的关键限制，实现更安全、高效的空中货物运输。&lt;h4&gt;方法&lt;/h4&gt;通过统一的感知、规划和控制框架解决问题：1)提出LiDAR-IMU融合模块，在张紧和弯曲模式下估计载荷姿态和电缆形状；2)引入MACIRI算法，考虑不同无人机和载荷几何形状，生成安全飞行走廊；3)开发时空受限的轨迹优化方案；4)使用带有电缆弯曲约束的非线性模型预测控制器确保执行过程中的全身安全。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真和实验验证了Acetrans的有效性，与最先进的方法相比，在感知准确性、规划效率和控制安全性方面有显著改进。&lt;h4&gt;结论&lt;/h4&gt;Acetrans系统能够有效解决现有无人机悬吊运输系统的关键限制，实现更安全、高效的空中货物运输，在复杂和杂乱环境中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;带有悬吊载荷的无人机在复杂和杂乱环境中的空中运输具有显著优势。然而，现有系统面临关键限制，包括对电缆-载荷动力学的感知不可靠、大规模环境中的规划效率低下，以及在电缆弯曲和外部干扰下无法保证全身安全。本文提出了Acetrans，一种自主的、基于走廊的、高效的无人机悬吊运输系统，通过统一的感知、规划和控制框架解决这些挑战。提出了LiDAR-IMU融合模块，在张紧和弯曲模式下联合估计载荷姿态和电缆形状，实现鲁棒的全身状态估计和电缆点云的实时滤波。为了提高规划的可扩展性，我们引入了多尺寸感知配置空间迭代区域膨胀算法，该算法在考虑不同无人机和载荷几何形状的同时生成安全飞行走廊。然后，开发了一种时空受限的轨迹优化方案，以确保动态可行且无碰撞的轨迹。最后，通过增加电缆弯曲约束的非线性模型预测控制器在执行期间提供鲁棒的全身安全。仿真和实验结果验证了Acetrans的有效性，表明与最先进的方法相比，在感知准确性、规划效率和控制安全性方面有显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机悬挂运输系统中的三个关键问题：对电缆-载荷动力学的感知不可靠、在大规模环境中规划效率低下、以及无法保证电缆弯曲和外部干扰下的全身安全。这个问题在现实中非常重要，因为无人机悬挂运输系统在物流、农业和救灾等领域具有巨大潜力，特别是在复杂环境（如森林、城市峡谷或室内）中能够提供传统固定翼无人机和带机械臂的多旋翼无人机无法比拟的灵活性和适应性。解决这些问题将使无人机能够在这些复杂环境中实现自主、安全、高效的货物运输，拓展其应用范围。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性来设计Acetrans系统。他们首先识别了悬挂运输系统面临的三个主要挑战：感知不可靠、规划效率低下和安全保障不足。在感知方面，作者借鉴了LiDAR-IMU融合技术，但进行了创新以适应电缆弯曲情况；在规划方面，作者基于现有的迭代区域膨胀算法（如IRIS、RILS、FIRI和CIRI）思想，扩展了它们以适应悬挂载荷系统的复杂几何形状；在控制方面，作者借鉴了非线性模型预测控制技术，但增加了电缆弯曲约束。总的来说，作者不是从零开始设计，而是基于现有工作的基础上进行创新和扩展，解决了悬挂运输系统特有的挑战，并统一了感知、规划和控制三个模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; Acetrans方法的核心思想是通过一个统一的框架同时解决感知、规划和控制三个关键问题，实现无人机在复杂环境中的安全、高效悬挂运输。核心思想包括：1)全身感知：使用LiDAR-IMU融合技术同时估计无人机、载荷和电缆的状态；2)多尺寸感知的走廊生成：提出MACIRI算法根据系统中不同部分的尺寸动态调整障碍物表示；3)走廊约束的轨迹优化：在生成的安全走廊内进行轨迹优化；4)电缆弯曲安全的控制：使用增强的非线性模型预测控制器确保全身安全。整体流程为：首先通过双LiDAR和LiDAR-IMU融合进行全身感知和状态估计；然后使用动力学A*算法和MACIRI算法生成安全飞行走廊；接着在走廊内进行时空轨迹优化；最后使用带有电缆弯曲约束的NMPC控制器跟踪轨迹，形成一个完整的自主运输系统。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1)全身感知框架：能够处理电缆弯曲情况，独立于光照条件，而现有视觉方法在低光条件下表现不佳且大多假设电缆总是张紧的；2)MACIRI算法：根据系统中不同部分的尺寸动态调整障碍物表示，扩大了可用走廊体积，而现有算法只考虑单一机器人尺寸；3)走廊约束的轨迹优化：专门针对悬挂载荷系统设计，优化速度比现有基线快1-3个数量级；4)电缆弯曲安全的控制：是第一个实现电缆弯曲时障碍避免的框架，而现有方法大多忽略电缆或未保证其安全。Acetrans的创新之处在于它首次将感知、规划和控制统一到一个完整的自主框架中，解决了悬挂载荷系统特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Acetrans首次提出了一个统一的自主悬挂运输系统框架，通过创新的全身感知、多尺寸感知的走廊生成和电缆弯曲安全的控制方法，实现了无人机在复杂环境中安全、高效的悬挂载荷运输。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned aerial vehicles (UAVs) with suspended payloads offer significantadvantages for aerial transportation in complex and cluttered environments.However, existing systems face critical limitations, including unreliableperception of the cable-payload dynamics, inefficient planning in large-scaleenvironments, and the inability to guarantee whole-body safety under cablebending and external disturbances. This paper presents Acetrans, an Autonomous,Corridor-based, and Efficient UAV suspended transport system that addressesthese challenges through a unified perception, planning, and control framework.A LiDAR-IMU fusion module is proposed to jointly estimate both payload pose andcable shape under taut and bent modes, enabling robust whole-body stateestimation and real-time filtering of cable point clouds. To enhance planningscalability, we introduce the Multi-size-Aware Configuration-space IterativeRegional Inflation (MACIRI) algorithm, which generates safe flight corridorswhile accounting for varying UAV and payload geometries. A spatio-temporal,corridor-constrained trajectory optimization scheme is then developed to ensuredynamically feasible and collision-free trajectories. Finally, a nonlinearmodel predictive controller (NMPC) augmented with cable-bending constraintsprovides robust whole-body safety during execution. Simulation and experimentalresults validate the effectiveness of Acetrans, demonstrating substantialimprovements in perception accuracy, planning efficiency, and control safetycompared to state-of-the-art methods.</description>
      <author>example@mail.com (Weiyan Lu, Huizhe Li, Yuhao Fang, Zhexuan Zhou, Junda Wu, Yude Li, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2509.10349v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MCL-AD: Multimodal Collaboration Learning for Zero-Shot 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.10282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Page 14, 5 pictures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了MCL-AD，一个新颖的多模态协作学习框架，用于零样本3D异常检测。该框架结合点云、RGB图像和文本语义信息，通过多模态提示学习机制和协作调制机制，实现了最先进的检测性能。&lt;h4&gt;背景&lt;/h4&gt;Zero-shot 3D异常检测旨在无需标记训练数据的情况下识别3D物体缺陷，在数据稀缺、隐私限制或标注成本高的场景中特别有价值。然而，现有方法主要专注于点云，忽视了来自RGB图像和文本先验等互补模态的丰富语义线索。&lt;h4&gt;目的&lt;/h4&gt;引入MCL-AD框架，利用点云、RGB图像和文本语义之间的多模态协作学习，实现更优的零样本3D异常检测。&lt;h4&gt;方法&lt;/h4&gt;提出多模态提示学习机制（MPLM），引入与对象无关的解耦文本提示和多模态对比损失，增强模内表示能力和模间协作学习；同时提出协作调制机制（CMM），通过联合调制RGB图像引导和点云引导的分支，充分利用点云和RGB图像的互补表示。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，提出的MCL-AD框架在零样本3D异常检测任务中达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;MCL-AD框架通过多模态协作学习有效提升了零样本3D异常检测的性能，为该领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;零样本3D异常检测旨在无需依赖标记的训练数据来识别3D物体中的缺陷，在数据稀缺、隐私限制或标注成本高的场景中特别有价值。然而，大多数现有方法仅专注于点云，忽视了来自RGB图像和文本先验等互补模态的丰富语义线索。本文介绍了MCL-AD，一个新颖的框架，它利用点云、RGB图像和文本语义之间的多模态协作学习来实现卓越的零样本3D异常检测。具体而言，我们提出了多模态提示学习机制（MPLM），通过引入与对象无关的解耦文本提示和多模态对比损失，增强模内表示能力和模间协作学习。此外，还提出了协作调制机制（CMM），通过联合调制RGB图像引导和点云引导的分支，充分利用点云和RGB图像的互补表示。大量实验证明，所提出的MCL-AD框架在零样本3D异常检测中达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D异常检测问题，即在没有针对特定对象标记的训练数据的情况下检测3D物体中的缺陷。这个问题在现实中很重要，因为它能解决数据稀缺、隐私限制和高标注成本等挑战，使异常检测系统能够快速适应新对象类别，在工业检测、质量控制等领域具有广泛应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法仅关注点云数据而忽略RGB图像和文本提示等互补模态的局限性，提出多模态协作学习思路。设计过程中借鉴了CLIP模型作为基础编码器、多视图渲染技术处理点云、提示学习和对比学习等现有技术，但进行了创新改进，设计了多模态提示学习机制和协作调制机制来有效融合不同模态信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态协作学习，融合点云、RGB图像和文本语义信息，增强模型对异常的感知能力，使其能在无特定类别训练数据的情况下准确检测3D异常。整体流程包括：1)特征提取阶段，将点云转换为多视图深度图像并提取特征；2)训练阶段使用多模态提示学习机制构建解耦文本提示并计算多模态对比损失；3)测试阶段通过协作调制机制动态调整RGB和点云分支的输出权重，融合生成最终结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多模态协作学习框架MCL-AD，首次融合点云、RGB图像和文本语义三种模态；2)多模态提示学习机制MPLM，包含对象无关的解耦文本提示和多模态对比损失；3)协作调制机制CMM，动态调整不同模态贡献权重。相比之前工作，MCL-AD不仅利用了更多模态信息，还通过解耦设计和动态融合解决了模态不平衡问题，显著提升了检测性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCL-AD通过多模态协作学习框架，有效融合点云、RGB图像和文本语义信息，显著提升了在没有特定对象类别训练数据的情况下3D异常检测的准确性和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot 3D (ZS-3D) anomaly detection aims to identify defects in 3D objectswithout relying on labeled training data, making it especially valuable inscenarios constrained by data scarcity, privacy, or high annotation cost.However, most existing methods focus exclusively on point clouds, neglectingthe rich semantic cues available from complementary modalities such as RGBimages and texts priors. This paper introduces MCL-AD, a novel framework thatleverages multimodal collaboration learning across point clouds, RGB images,and texts semantics to achieve superior zero-shot 3D anomaly detection.Specifically, we propose a Multimodal Prompt Learning Mechanism (MPLM) thatenhances the intra-modal representation capability and inter-modalcollaborative learning by introducing an object-agnostic decoupled text promptand a multimodal contrastive loss. In addition, a collaborative modulationmechanism (CMM) is proposed to fully leverage the complementary representationsof point clouds and RGB images by jointly modulating the RGB image-guided andpoint cloud-guided branches. Extensive experiments demonstrate that theproposed MCL-AD framework achieves state-of-the-art performance in ZS-3Danomaly detection.</description>
      <author>example@mail.com (Gang Li, Tianjiao Chen, Mingle Zhou, Min Li, Delong Han, Jin Wan)</author>
      <guid isPermaLink="false">2509.10282v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>CaR1: A Multi-Modal Baseline for BEV Vehicle Segmentation via Camera-Radar Fusion</title>
      <link>http://arxiv.org/abs/2509.10139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CaR1的新型相机-雷达融合架构，用于BEV车辆分割，通过结合相机和雷达的互补优势，实现了与最先进方法相当的分割性能。&lt;h4&gt;背景&lt;/h4&gt;相机-雷达融合为基于激光雷达的自动驾驶系统提供了一种稳健且具有成本效益的替代方案，相机提供丰富的语义线索但深度信息不可靠，而雷达提供稀疏但可靠的位置和运动信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型相机-雷达融合架构CaR1，专门用于BEV（鸟瞰图）车辆分割任务。&lt;h4&gt;方法&lt;/h4&gt;基于BEVFusion构建，包含网格化的雷达编码，将点云离散化为结构化的BEV特征，以及自适应融合机制，动态平衡传感器的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验显示，CaR1实现了57.6的IoU分割性能，与当前最先进的方法相当。&lt;h4&gt;结论&lt;/h4&gt;CaR1架构通过有效融合相机和雷达数据，在BEV车辆分割任务上取得了优异性能，相关代码已在GitHub公开可用。&lt;h4&gt;翻译&lt;/h4&gt;相机-雷达融合通过结合互补的感知能力，为基于激光雷达的自动驾驶系统提供了一种稳健且具有成本效益的替代方案：相机提供丰富的语义线索但深度信息不可靠，而雷达提供稀疏但可靠的位置和运动信息。我们介绍了CaR1，一种用于BEV车辆分割的新型相机-雷达融合架构。基于BEVFusion构建，我们的方法包含一种网格化的雷达编码，将点云离散化为结构化的BEV特征，以及一种自适应融合机制，能够动态平衡传感器的贡献。在nuScenes上的实验表明，该方法具有竞争力的分割性能（57.6 IoU），与最先进的方法相当。代码已在GitHub上公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效融合相机和雷达两种传感器的数据，实现鸟瞰视角下的车辆分割问题。这个问题很重要，因为自动驾驶系统需要准确感知周围环境，而单一传感器有局限性：相机提供丰富语义信息但深度估计不可靠，雷达提供可靠位置和运动信息且在恶劣天气下稳定但数据稀疏。相机-雷达融合可以提供一种成本更低、更鲁棒的替代方案，比基于LiDAR的系统更适合大规模部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了相机和雷达的互补特性及融合挑战，然后基于BEVFusion架构进行改进。借鉴了现有工作中的BEV表示空间、EfficientViT-L2图像编码器、Point Transformer V3点云编码网络和Attention U-Net解码器。作者的创新设计包括网格化雷达特征编码方法将稀疏点云转换为结构化特征，以及自适应融合机制动态平衡各传感器贡献。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用相机和雷达的互补优势，通过鸟瞰视角表示空间统一两种传感器数据，采用网格化方法处理雷达点云，并设计自适应融合机制动态调整传感器贡献。整体流程包括：1)图像特征提取：使用EfficientViT-L2处理多视角图像并投影到BEV空间；2)雷达特征提取：使用PTv3处理点云并通过金字塔聚合网络增强特征；3)自适应融合：通过注意力机制动态调整传感器贡献；4)BEV解码：使用Attention U-Net refine特征；5)分割头：生成车辆分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)网格化雷达特征编码方法，将稀疏点云转换为结构化BEV特征；2)自适应融合机制，动态平衡各传感器贡献；3)整体架构设计，使用轻量级编码器和注意力解码器。相比之前工作，不同之处在于：雷达处理方式更先进，结合了点网络优势和BEV框架；融合策略更灵活，能动态调整传感器贡献；性能表现更好，在nuScenes上实现57.6% IoU，比纯相机方法提升10.2%，与最先进方法相当。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CaR1通过创新的网格化雷达特征编码和自适应融合机制，实现了相机和雷达数据在鸟瞰视角下的有效融合，为自动驾驶车辆分割提供了一种成本更低、更鲁棒的感知方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Camera-radar fusion offers a robust and cost-effective alternative toLiDAR-based autonomous driving systems by combining complementary sensingcapabilities: cameras provide rich semantic cues but unreliable depth, whileradar delivers sparse yet reliable position and motion information. Weintroduce CaR1, a novel camera-radar fusion architecture for BEV vehiclesegmentation. Built upon BEVFusion, our approach incorporates a grid-wise radarencoding that discretizes point clouds into structured BEV features and anadaptive fusion mechanism that dynamically balances sensor contributions.Experiments on nuScenes demonstrate competitive segmentation performance (57.6IoU), on par with state-of-the-art methods. Code is publicly available\href{https://www.github.com/santimontiel/car1}{online}.</description>
      <author>example@mail.com (Santiago Montiel-Marín, Angel Llamazares, Miguel Antunes-García, Fabio Sánchez-García, Luis M. Bergasa)</author>
      <guid isPermaLink="false">2509.10139v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Soft Tissue Simulation and Force Estimation from Heterogeneous Structures using Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.10125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的软组织变形模拟方法，能够从稀疏点云预测组织表面变形和施加的力，相比传统有限元法更高效且具有较好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;软组织变形模拟对手术训练、术前规划和实时触觉反馈系统至关重要。基于物理的模型如有限元法能提供高保真结果，但计算成本高且需要大量预处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的数据驱动方法，能够准确模拟软组织变形，适用于实时应用，并具有良好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络架构，通过每个点下方的二值组织轮廓整合内部解剖信息，并利用E(n)-等变消息传递提高鲁棒性。实验数据包括真实硅胶和类骨 phantom，以及使用FEM生成的合成模拟数据。&lt;h4&gt;主要发现&lt;/h4&gt;模型在标准测试案例中与基线GNN性能相当，在旋转和跨分辨率场景中显著优于基线，显示出对未见方向和点密度的强泛化能力。模型也实现了显著的速度提升，为实时应用提供了解决方案。在实验数据上微调后，模型在有限样本量和测量噪声下仍保持亚毫米级变形精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为传统模拟提供了高效的数据驱动替代方案，能够泛化到不同的解剖结构配置，并支持交互式手术环境。&lt;h4&gt;翻译&lt;/h4&gt;准确模拟软组织变形对手术培训、术前规划和实时触觉反馈系统至关重要。虽然基于物理的模型如有限元法(FEM)能提供高保真结果，但它们通常计算量大且需要大量预处理。我们提出了一种图神经网络(GNN)架构，可以从稀疏点云预测组织表面变形和施加的力。该模型通过每个点下方的二值组织轮廓整合内部解剖信息，并利用E(n)-等变消息传递提高鲁棒性。我们收集了包含真实硅胶和类骨 phantom 的实验数据，并辅以使用FEM生成的合成模拟数据。我们的模型在标准测试案例中与基线GNN性能相当，在旋转和跨分辨率场景中显著优于基线，显示出对未见方向和点密度的强泛化能力。它还实现了显著的速度提升，为实时应用提供了解决方案。在实验数据上微调后，尽管样本量有限且存在测量噪声，模型仍保持亚毫米级变形精度。结果表明，我们的方法为传统模拟提供了高效的数据驱动替代方案，能够泛化到不同的解剖结构配置，并支持交互式手术环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决软组织变形模拟的计算效率问题。传统物理模拟方法(如有限元法)虽然准确但计算量大，难以满足实时应用需求。这一问题在现实中至关重要，因为准确的软组织模拟对手术培训、术前规划和实时手术引导系统都至关重要，能够提高外科医生技能、预见手术并发症、改善临床决策精度，并支持医疗机器人的精确控制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：物理模拟计算昂贵，而现有学习方法无法提供触觉反馈所需的力估计或仅能处理均匀组织。他们借鉴了多种现有工作，包括多层感知机(MLP)、U-Mesh架构、图神经网络(GNN)如MagNet框架、条件图神经网络以及E(n)-等变图神经网络概念。基于这些工作，作者设计了一种新方法，通过引入异构组织建模、条件等变图卷积层(cEGCL)和多任务预测(变形和力)来解决现有方法的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络处理点云数据，同时预测软组织变形和施加力，并利用组织内部结构信息提高预测准确性。整体流程包括：1)收集实验数据(硅胶和骨状phantom)和FEM模拟数据；2)为每个表面点提取128维二元组织特征向量，表示垂直材料分布；3)设计条件等变图卷积层(cEGCL)更新节点坐标和特征；4)构建包含位移预测和力预测两个分支的模型架构；5)使用加权损失函数进行训练，并在实验数据上微调；6)评估模型在旋转、点密度变化和迁移学习场景下的性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)异构对象建模，引入内部解剖结构信息；2)条件等变图卷积层(cEGCL)，提高对旋转的鲁棒性；3)多任务预测，同时输出变形和力；4)数据融合策略，结合实验和FEM模拟数据；5)强大的泛化能力，在旋转和不同点密度场景下表现优异。相比之前的工作，本方法不仅能处理异质组织结构，还提供力估计支持触觉反馈，计算速度更快，且对旋转和点密度变化具有更强的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合组织内部结构信息和等变图神经网络的创新方法，实现了高效、准确的软组织变形和力估计，显著提高了在异质结构、旋转和不同点密度场景下的泛化能力，为实时手术模拟和触觉反馈提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately simulating soft tissue deformation is crucial for surgicaltraining, pre-operative planning, and real-time haptic feedback systems. Whilephysics-based models such as the finite element method (FEM) providehigh-fidelity results, they are often computationally expensive and requireextensive preprocessing. We propose a graph neural network (GNN) architecturethat predicts both tissue surface deformation and applied force from sparsepoint clouds. The model incorporates internal anatomical information throughbinary tissue profiles beneath each point and leverages E(n)-equivariantmessage passing to improve robustness. We collected experimental data thatcomprises a real silicone and bone-like phantom, and complemented it withsynthetic simulations generated using FEM. Our model achieves a comparableperformance to a baseline GNN on standard test cases and significantlyoutperforms it in rotated and cross-resolution scenarios, showing a stronggeneralization to unseen orientations and point densities. It also achieves asignificant speed improvement, offering a solution for real-time applications.When fine-tuned on experimental data, the model maintains sub-millimeterdeformation accuracy despite limited sample size and measurement noise. Theresults demonstrate that our approach offers an efficient, data-drivenalternative to traditional simulations, capable of generalizing acrossanatomical configurations and supporting interactive surgical environments.</description>
      <author>example@mail.com (Madina Kojanazarova, Sidady El Hadramy, Jack Wilkie, Georg Rauter, Philippe C. Cattin)</author>
      <guid isPermaLink="false">2509.10125v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Design and Evaluation of Two Spherical Systems for Mobile 3D Mapping</title>
      <link>http://arxiv.org/abs/2509.10032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages, 9 figures, International Workshop 3D-AdViCE in conjunction  with 12th ECMR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了两种互补的球形测绘系统，评估了它们在资源受限硬件上的激光雷达-惯性里程计性能，并发现球形运动的高动态特性导致传统算法性能下降。&lt;h4&gt;背景&lt;/h4&gt;球形机器人因其保护壳和全向移动能力，在危险或受限环境的测绘应用中具有独特优势。&lt;h4&gt;目的&lt;/h4&gt;开发两种互补的球形测绘系统（轻量级无驱动设计和内部摆锤驱动变体），并评估其测绘精度。&lt;h4&gt;方法&lt;/h4&gt;两种系统均配备Livox Mid-360固态激光雷达传感器，在资源受限硬件上运行激光雷达-惯性里程计算法，通过与真实地图比较点云来评估精度。&lt;h4&gt;主要发现&lt;/h4&gt;球形运动引入的高动态运动导致最先进的LIO算法性能下降，产生全局不一致的地图和有时不可恢复的漂移。&lt;h4&gt;结论&lt;/h4&gt;球形机器人的特殊运动特性对传统LIO算法构成挑战，需要改进算法以适应这种高动态运动环境。&lt;h4&gt;翻译&lt;/h4&gt;球形机器人因其保护壳和全向移动能力，在危险或受限环境的测绘应用中具有独特优势。这项工作提出了两种互补的球形测绘系统：一种轻量级无驱动设计和一种内部摆锤驱动运动的变体。两种系统都配备了Livox Mid-360固态激光雷达传感器，并在资源受限的硬件上运行激光雷达-惯性里程计算法。我们通过将LIO算法生成的3D点云与真实地图进行比较来评估这些系统的测绘精度。结果表明，由于球形运动引入的高动态运动，最先进的LIO算法性能下降，导致全局不一致的地图和有时不可恢复的漂移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spherical robots offer unique advantages for mapping applications inhazardous or confined environments, thanks to their protective shells andomnidirectional mobility. This work presents two complementary sphericalmapping systems: a lightweight, non-actuated design and an actuated variantfeaturing internal pendulum-driven locomotion. Both systems are equipped with aLivox Mid-360 solid-state LiDAR sensor and run LiDAR-Inertial Odometry (LIO)algorithms on resource-constrained hardware. We assess the mapping accuracy ofthese systems by comparing the resulting 3D point-clouds from the LIOalgorithms to a ground truth map. The results indicate that the performance ofstate-of-the-art LIO algorithms deteriorates due to the high dynamic movementintroduced by the spherical locomotion, leading to globally inconsistent mapsand sometimes unrecoverable drift.</description>
      <author>example@mail.com (Marawan Khalil, Fabian Arzberger, Andreas Nüchter)</author>
      <guid isPermaLink="false">2509.10032v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Online 3D Multi-Camera Perception through Robust 2D Tracking and Depth-based Late Aggregation</title>
      <link>http://arxiv.org/abs/2509.09946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCVW 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种将现有2D多摄像头跟踪系统扩展到3D空间的方法，通过利用深度信息重建目标点云并恢复3D边界框，同时引入增强的数据关联机制，在AI City Challenge的3D MTMC数据集上获得第三名。&lt;h4&gt;背景&lt;/h4&gt;多目标多摄像头跟踪(MTMC)是自动化大规模监控的重要计算机视觉任务。通过摄像头校准和深度信息，可以将场景中的目标投影到3D空间，提供对3D环境的自动感知。然而，在3D空间中进行跟踪需要从头替换所有2D跟踪组件，这对现有的MTMC系统可能不可行。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在开发一种方法，能够将任何现有的在线2D多摄像头跟踪系统扩展到3D空间，而无需完全重新设计系统架构。&lt;h4&gt;方法&lt;/h4&gt;研究提出的方法包括：1)利用深度信息重建目标的点云空间；2)通过聚类和偏航角细化恢复目标的3D边界框；3)引入增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，并在排行榜上获得第三名的成绩，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;本研究提供了一种实用的方法，可以将现有的2D多摄像头跟踪系统扩展到3D空间，而无需完全重新设计系统，为大规模3D监控应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多目标多摄像头跟踪(MTMC)是自动化大规模监控的重要计算机视觉任务。通过摄像头校准和深度信息，场景中的目标可以被投影到3D空间，为3D环境提供前所未有的自动感知水平。然而，在3D空间中进行跟踪需要从头替换所有2D跟踪组件，这对于现有的MTMC系统可能不可行。在本文中，我们提出了一种方法，通过利用深度信息将任何在线2D多摄像头跟踪系统扩展到3D空间，重建目标在点云空间中的表示，并在跟踪后通过聚类和偏航角细化恢复其3D边界框。我们还引入了一种增强的在线数据关联机制，利用目标的局部ID一致性来跨帧分配全局ID。所提出的框架在2025年AI City Challenge的3D MTMC数据集上进行了评估，在排行榜上获得第三名。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将现有的在线2D多摄像头跟踪系统扩展到3D空间的问题。这个问题在现实中非常重要，因为大规模监控系统（如智慧城市、智能交通、安防等）需要处理来自多个摄像头的视频数据，而3D空间中的目标跟踪能提供更丰富的场景信息，如目标的高度、位置和朝向等。直接替换2D系统的所有组件到3D空间成本高昂且复杂，而本文提供了一种更高效、经济的解决方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的3D多摄像头跟踪方法需要在跟踪前融合多视图数据，这需要替换所有2D组件，对大规模监控系统不切实际。因此，他们提出了'后期聚合'的思路，即在完成2D跟踪后，再利用深度信息将2D结果扩展到3D空间。作者借鉴了多项现有工作，包括使用Co-DETR进行目标检测、CLIP-ReID提取外观特征、RTMPose进行姿态估计、Deep OC-SORT进行单摄像头跟踪、SAM2进行实例分割、DBSCAN进行点云聚类以及分层聚类进行空间数据关联。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'后期聚合'的方式，在完成2D多摄像头跟踪后，再利用深度信息将2D结果扩展到3D空间，同时利用目标局部ID的一致性来改进跨摄像头跟踪的ID分配机制。整体流程分为两个阶段：第一阶段是2D多摄像头跟踪，包括单摄像头跟踪、空间数据关联和时间数据关联；第二阶段是后期3D边界框聚合，包括深度到点云转换、点云到3D边界框转换、3D边界框融合和偏航角优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 后期聚合框架，可将2D系统扩展为3D系统而无需修改现有组件；2) 增强的在线关联机制，利用局部ID一致性改进跨摄像头跟踪；3) 3D边界框恢复机制，通过分割和点云聚类将2D边界框扩展到3D空间；4) 偏航角优化，通过运动轨迹分析提高定位精度。相比之前的工作，本文方法不需要在跟踪前融合多视图数据，而是利用现有的2D跟踪结果，降低了系统复杂度，同时通过局部ID一致性改进了跨摄像头跟踪性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的后期聚合框架，能够将现有的在线2D多摄像头跟踪系统无缝扩展为3D跟踪系统，同时引入了基于局部ID一致性的增强关联机制和基于深度信息的3D边界框恢复方法，在2025年AI City Challenge的3D MTMC任务中取得了第三名的好成绩。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-Target Multi-Camera Tracking (MTMC) is an essential computer visiontask for automating large-scale surveillance. With camera calibration and depthinformation, the targets in the scene can be projected into 3D space, offeringunparalleled levels of automatic perception of a 3D environment. However,tracking in the 3D space requires replacing all 2D tracking components from theground up, which may be infeasible for existing MTMC systems. In this paper, wepresent an approach for extending any online 2D multi-camera tracking systeminto 3D space by utilizing depth information to reconstruct a target inpoint-cloud space, and recovering its 3D box through clustering and yawrefinement following tracking. We also introduced an enhanced online dataassociation mechanism that leverages the target's local ID consistency toassign global IDs across frames. The proposed framework is evaluated on the2025 AI City Challenge's 3D MTMC dataset, achieving 3rd place on theleaderboard.</description>
      <author>example@mail.com (Vu-Minh Le, Thao-Anh Tran, Duc Huy Do, Xuan Canh Do, Huong Ninh, Hai Tran)</author>
      <guid isPermaLink="false">2509.09946v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Purge-Gate: Backpropagation-Free Test-Time Adaptation for Point Clouds Classification via Token Purging</title>
      <link>http://arxiv.org/abs/2509.09785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Token Purging (PG)的新型测试时适应方法，用于解决3D点云分类中的分布偏移问题。该方法通过在令牌到达注意力层之前移除受域偏移高度影响的令牌，实现了无需反向传播的稳健适应。实验表明，该方法在准确率和效率方面均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;测试时适应(TTA)对于缓解3D点云分类中分布偏移引起的性能下降至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的TTA方法，能够在不进行反向传播的情况下有效适应分布偏移，同时提高计算效率和内存利用率。&lt;h4&gt;方法&lt;/h4&gt;引入Token Purging (PG)，一种新颖的无反向传播方法，在令牌到达注意力层之前移除受域偏移高度影响的令牌。提出了两种变体：PG-SP（利用源统计信息）和PG-SF（完全无源版本，依赖CLS令牌驱动的适应）。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP比最先进的无反向传播方法平均高出10.3%的准确率，而PG-SF为无源适应设定了新基准。此外，PG比基线快12.4倍，内存效率高5.5倍，适合实际部署。&lt;h4&gt;结论&lt;/h4&gt;Token Purging是一种有效的TTA方法，在性能和效率方面都有显著优势，适合实际部署。&lt;h4&gt;翻译&lt;/h4&gt;测试时适应(TTA)对于缓解3D点云分类中分布偏移引起的性能下降至关重要。在这项工作中，我们引入了Token Purging (PG)，一种新颖的无反向传播方法，在令牌到达注意力层之前移除受域偏移高度影响的令牌。与现有的TTA方法不同，PG在令牌级别操作，确保无需迭代更新的稳健适应。我们提出了两种变体：PG-SP，它利用源统计信息，以及PG-SF，一种完全无源的版本，依赖于CLS令牌驱动的适应。在ModelNet40-C、ShapeNet-C和ScanObjectNN-C上的广泛评估表明，PG-SP比最先进的无反向传播方法平均高出10.3%的准确率，而PG-SF为无源适应设定了新基准。此外，PG比我们的基线快12.4倍，内存效率高5.5倍，使其适合实际部署。代码可在https://github.com/MosyMosy/Purge-Gate获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类任务中的测试时适应问题，即在测试数据分布与训练数据分布不同（分布偏移）的情况下，如何保持模型性能。这个问题在现实中非常重要，因为3D点云应用广泛（如自动驾驶、机器人、AR/VR等），这些应用中数据分布经常变化，而预训练一个模型应对所有场景是不现实的。现有方法要么计算成本高，要么需要额外源数据，限制了实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了点云数据中分布偏移的特殊性，指出噪声可能表现为新增点或不均匀分布的扰动。他们观察到噪声会破坏transformer架构中的注意力机制，影响特征聚合。受Token Pruning（用于提高transformer效率）的启发，作者转而设计删除受分布偏移影响最大的token的方法。同时，他们利用了transformer中CLS token的特性，发现CLS token在预训练过程中会吸收领域信息，可以作为原型用于测试时适应。整体设计思路是创建一个轻量级的、无反向传播的门控机制，在注意力层输入前过滤掉噪声token。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在测试时识别并移除受分布偏移影响最大的token，防止这些噪声token到达注意力层，从而保护模型的注意力机制不被破坏。整体流程包括：1) 将点云样本表示为token集合；2) 计算每个token的偏离程度（PG-SP使用源数据统计信息计算马氏距离，PG-SF使用CLS token作为原型计算余弦距离）；3) 移除偏离最大的Lpg个token；4) 将净化后的token输入transformer网络进行分类；5) 使用熵最小化策略动态选择最优的Lpg值。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次在token级别进行测试时适应；2) 提出无反向传播的轻量级方法；3) 设计两种变体（PG-SP利用源数据统计，PG-SF完全无源）；4) 提供分布偏移如何影响transformer注意力机制的理论分析。相比之前工作，不同之处在于：相比反向传播方法（如TENT），不需要梯度计算和参数更新，效率更高；相比无反向传播方法（如BFTT3D），不依赖源数据类别原型；相比测试时训练方法（如MATE），不需要辅助训练任务，可应用于任意预训练模型；相比Token Pruning，专注于解决分布偏移而非提高计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Purge-Gate通过在测试时动态移除受分布偏移影响最大的token，提出了一种高效的无反向传播的点云分类测试时适应方法，显著提高了模型在分布偏移场景下的性能，同时大幅降低了计算和内存需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Test-time adaptation (TTA) is crucial for mitigating performance degradationcaused by distribution shifts in 3D point cloud classification. In this work,we introduce Token Purging (PG), a novel backpropagation-free approach thatremoves tokens highly affected by domain shifts before they reach attentionlayers. Unlike existing TTA methods, PG operates at the token level, ensuringrobust adaptation without iterative updates. We propose two variants: PG-SP,which leverages source statistics, and PG-SF, a fully source-free versionrelying on CLS-token-driven adaptation. Extensive evaluations on ModelNet40-C,ShapeNet-C, and ScanObjectNN-C demonstrate that PG-SP achieves an average of+10.3\% higher accuracy than state-of-the-art backpropagation-free methods,while PG-SF sets new benchmarks for source-free adaptation. Moreover, PG is12.4 times faster and 5.5 times more memory efficient than our baseline, makingit suitable for real-world deployment. Code is available at\hyperlink{https://github.com/MosyMosy/Purge-Gate}{https://github.com/MosyMosy/Purge-Gate}</description>
      <author>example@mail.com (Moslem Yazdanpanah, Ali Bahri, Mehrdad Noori, Sahar Dastani, Gustavo Adolfo Vargas Hakim, David Osowiechi, Ismail Ben Ayed, Christian Desrosiers)</author>
      <guid isPermaLink="false">2509.09785v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>DECAMP: Towards Scene-Consistent Multi-Agent Motion Prediction with Disentangled Context-Aware Pre-Training</title>
      <link>http://arxiv.org/abs/2509.10426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DECAMP的解耦上下文感知预训练框架，用于解决自动驾驶中多智能体运动预测的挑战，特别是标记数据稀缺和多智能体场景预测效果不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并且在多智能体预测场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法面临的标记数据稀缺和多智能体预测场景中表现不佳的挑战，通过引入一个解耦的上下文感知预训练框架来提高多智能体运动预测的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为DECAMP的解耦上下文感知预训练框架，将行为模式学习与潜在特征重建解耦，优先考虑可解释的动力学，从而增强下游预测的场景表示。同时，框架集成了上下文感知表示学习和协作空间-运动预训练任务，能够同时优化结构推理和意图推理，同时捕获潜在的动态意图。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上的实验展示了该方法优越的性能，结果证明了其在多智能体运动预测中的有效性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是自动驾驶领域中首个用于多智能体运动预测的上下文自编码器框架，代码和模型将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;轨迹预测是自动驾驶的关键组成部分，对确保道路安全和效率至关重要。然而，传统方法往往面临标记数据稀缺的问题，并且在多智能体预测场景中表现不佳。为了解决这些挑战，我们引入了一个用于多智能体运动预测的解耦上下文感知预训练框架，名为DECAMP。与现有将表示学习与预训练任务纠缠在一起的方法不同，我们的框架将行为模式学习与潜在特征重建解耦，优先考虑可解释的动力学，从而增强下游预测的场景表示。此外，我们的框架集成了上下文感知表示学习和协作空间-运动预训练任务，这能够在捕获潜在动态意图的同时，优化结构推理和意图推理。我们在Argoverse 2基准测试上的实验展示了我们方法的优越性能，取得的结果强调了其在多智能体运动预测中的有效性。据我们所知，这是自动驾驶领域中首个用于多智能体运动预测的上下文自编码器框架。代码和模型将公开提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多智能体运动预测中的两个核心挑战：标记数据稀缺问题和场景一致性不足。在自动驾驶领域，准确预测多个交通参与者的互动行为对确保道路安全和效率至关重要，而场景不一致的预测会导致不合理的轨迹组合，影响自动驾驶汽车的可靠决策。解决这些问题不仅能提高预测准确性，还能降低系统开发和部署成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有监督学习和自监督方法的局限性，特别是编码器与预训练任务纠缠的问题以及单智能体方法扩展到多智能体场景的困难。受计算机视觉中掩码图像建模的启发，作者设计了'编码器-回归器-解码器'级联范式，引入回归器来减少编码器与特定任务的耦合。同时借鉴了Transformer架构、FPN和PointNet等现有技术，并创新性地设计了协作的空间-运动预训练任务，使模型能够同时捕获空间结构和动态意图。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦行为模式学习与潜在特征重建，使编码器专注于学习驾驶行为表示，而解码器专注于完成预训练任务。整体流程分为两个阶段：1)预训练阶段，输入历史、未来状态和地图，通过掩码策略和双解码器（空间解码器和运动解码器）分别重建空间线索和识别运动信号；2)微调阶段，仅使用历史状态和地图，通过预训练编码器生成K个场景一致的联合轨迹集合，确保预测结果在场景级别保持一致。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)解耦的'编码器-回归器-解码器'预训练框架，减少编码器与特定任务的纠缠；2)协作的空间-运动预训练任务，同时优化空间重建和运动识别；3)直接生成场景一致的多智能体联合轨迹，避免复杂的后处理；4)上下文感知表示学习，增强场景理解能力。相比之前工作，DECAMP解决了现有自监督方法中编码器与预训练任务耦合的问题，从单智能体预测扩展到多智能体联合预测，并通过解耦学习更鲁棒的行为先验，显著提高了预测准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DECAMP通过解耦行为模式学习与特征重建，并引入协作的空间-运动预训练任务，显著提高了多智能体运动预测的场景一致性和准确性，同时减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory prediction is a critical component of autonomous driving,essential for ensuring both safety and efficiency on the road. However,traditional approaches often struggle with the scarcity of labeled data andexhibit suboptimal performance in multi-agent prediction scenarios. To addressthese challenges, we introduce a disentangled context-aware pre-trainingframework for multi-agent motion prediction, named DECAMP. Unlike existingmethods that entangle representation learning with pretext tasks, our frameworkdecouples behavior pattern learning from latent feature reconstruction,prioritizing interpretable dynamics and thereby enhancing scene representationfor downstream prediction. Additionally, our framework incorporatescontext-aware representation learning alongside collaborative spatial-motionpretext tasks, which enables joint optimization of structural and intentionalreasoning while capturing the underlying dynamic intentions. Our experiments onthe Argoverse 2 benchmark showcase the superior performance of our method, andthe results attained underscore its effectiveness in multi-agent motionforecasting. To the best of our knowledge, this is the first contextautoencoder framework for multi-agent motion forecasting in autonomous driving.The code and models will be made publicly available.</description>
      <author>example@mail.com (Jianxin Shi, Zengqi Peng, Xiaolong Chen, Tianyu Wo, Jun Ma)</author>
      <guid isPermaLink="false">2509.10426v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>GundamQ: Multi-Scale Spatio-Temporal Representation Learning for Robust Robot Path Planning</title>
      <link>http://arxiv.org/abs/2509.10305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为GundamQ的多尺度时空Q网络，用于解决动态环境中机器人路径规划的挑战。该框架包含时空感知模块和自适应策略优化模块，能够有效处理多尺度时间依赖性和平衡探索-利用关系。实验结果表明，该方法在成功率和路径质量方面显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在动态和不确定的环境中，机器人路径规划需要准确的空间时间环境理解以及在部分可观测情况下的鲁棒决策能力。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于深度强化学习的路径规划方法在多尺度时间依赖性建模不足和探索-利用平衡效率低下方面的局限，提高机器人路径规划的成功率和质量。&lt;h4&gt;方法&lt;/h4&gt;提出GundamQ框架，包含两个关键模块：(1)时空感知模块，分层提取多粒度空间特征和多尺度时间依赖性；(2)自适应策略优化模块，在训练中平衡探索和利用，并通过约束策略更新优化路径平滑度和碰撞概率。&lt;h4&gt;主要发现&lt;/h4&gt;在动态环境中的实验表明，GundamQ成功率达到15.3%的提升，整体路径质量提高21.7%，显著优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;GundamQ通过有效建模多尺度时空特征和优化探索-利用平衡，显著提高了动态环境中机器人路径规划的适应性和路径质量。&lt;h4&gt;翻译&lt;/h4&gt;在动态和不确定的环境中，机器人路径规划需要准确的空间时间环境理解以及在部分可观测情况下的鲁棒决策能力。然而，当前基于深度强化学习的路径规划方法面临两个基本局限：(1)对多尺度时间依赖性建模不足，导致在动态场景中适应性次优；(2)探索-利用平衡效率低下，导致路径质量下降。为解决这些挑战，我们提出GundamQ：一种用于机器人路径规划的多尺度时空Q网络。该框架包含两个关键模块：(i)时空感知模块，分层提取从瞬时到延长时间范围的多粒度空间特征和多尺度时间依赖性，从而提高动态环境中的感知准确性；(ii)自适应策略优化模块，在训练过程中平衡探索和利用，同时通过约束策略更新优化平滑度和碰撞概率。动态环境中的实验表明，GundamQ成功率达到15.3%的提升，整体路径质量提高21.7%，显著优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In dynamic and uncertain environments, robotic path planning demands accuratespatiotemporal environment understanding combined with robust decision-makingunder partial observability. However, current deep reinforcement learning-basedpath planning methods face two fundamental limitations: (1) insufficientmodeling of multi-scale temporal dependencies, resulting in suboptimaladaptability in dynamic scenarios, and (2) inefficient exploration-exploitationbalance, leading to degraded path quality. To address these challenges, wepropose GundamQ: A Multi-Scale Spatiotemporal Q-Network for Robotic PathPlanning. The framework comprises two key modules: (i) the SpatiotemporalPerception module, which hierarchically extracts multi-granularity spatialfeatures and multi-scale temporal dependencies ranging from instantaneous toextended time horizons, thereby improving perception accuracy in dynamicenvironments; and (ii) the Adaptive Policy Optimization module, which balancesexploration and exploitation during training while optimizing for smoothnessand collision probability through constrained policy updates. Experiments indynamic environments demonstrate that GundamQ achieves a 15.3\% improvement insuccess rate and a 21.7\% increase in overall path quality, significantlyoutperforming existing state-of-the-art methods.</description>
      <author>example@mail.com (Yutong Shen, Ruizhe Xia, Bokai Yan, Shunqi zhang, Pengrui Xiang, Sicheng He, Yixin Xu)</author>
      <guid isPermaLink="false">2509.10305v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>BenchECG and xECG: a benchmark and baseline for ECG foundation models</title>
      <link>http://arxiv.org/abs/2509.10151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 4 figures, 22 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了BenchECG作为心电图(ECG)基础模型的标准化基准，并提出了xECG模型，该模型在所有评估任务中表现最佳，为ECG表示学习设定了新的基线。&lt;h4&gt;背景&lt;/h4&gt;心电图(ECG)是一种低成本、广泛使用且适合深度学习的医疗数据。最近，人们对开发能够在各种下游任务中泛化的ECG基础模型的兴趣日益增长。然而，先前的研究缺乏一致的评估方法，常使用狭窄的任务选择和不一致的数据集，妨碍了公平比较。&lt;h4&gt;目的&lt;/h4&gt;引入BenchECG作为标准化的基准，包含全面的公开可用ECG数据集和多样化任务；同时提出并评估xECG模型，这是一种基于xLSTM的循环模型。&lt;h4&gt;方法&lt;/h4&gt;使用SimDINOv2自监督学习训练基于xLSTM的循环模型(xECG)，并在BenchECG基准上与公开的最先进模型进行比较评估。&lt;h4&gt;主要发现&lt;/h4&gt;与公开的最先进模型相比，xECG在BenchECG评分上取得了最佳成绩。特别地，xECG是唯一在所有数据集和任务上都表现出色的公开可用模型。&lt;h4&gt;结论&lt;/h4&gt;通过标准化评估，BenchECG能够进行严格的模型比较，并旨在加速ECG表示学习的进展。xECG比早期方法表现更优，为未来的ECG基础模型设定了新的基线。&lt;h4&gt;翻译&lt;/h4&gt;心电图(ECG)是一种低成本、广泛使用且适合深度学习的工具。最近，人们对开发能够跨多样化下游任务泛化的ECG基础模型的兴趣日益增长。然而，一直缺乏一致的评估：先前的工作通常使用狭窄的任务选择和不一致的数据集，妨碍了公平比较。在此，我们引入了BenchECG，一个包含全面公开可用ECG数据集和多样化任务的标准基准。我们还提出了xECG，这是一种基于xLSTM的循环模型，使用SimDINOv2自监督学习进行训练，与公开的最先进模型相比，它在BenchECG评分上取得了最佳成绩。特别是，xECG是唯一在所有数据集和任务上都表现强劲的公开可用模型。通过标准化评估，BenchECG能够进行严格的比较，并旨在加速ECG表示学习的进展。xECG比早期方法表现更优，为未来的ECG基础模型设定了新的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electrocardiograms (ECGs) are inexpensive, widely used, and well-suited todeep learning. Recently, interest has grown in developing foundation models forECGs - models that generalise across diverse downstream tasks. However,consistent evaluation has been lacking: prior work often uses narrow taskselections and inconsistent datasets, hindering fair comparison. Here, weintroduce BenchECG, a standardised benchmark comprising a comprehensive suiteof publicly available ECG datasets and versatile tasks. We also propose xECG,an xLSTM-based recurrent model trained with SimDINOv2 self-supervised learning,which achieves the best BenchECG score compared to publicly availablestate-of-the-art models. In particular, xECG is the only publicly availablemodel to perform strongly on all datasets and tasks. By standardisingevaluation, BenchECG enables rigorous comparison and aims to accelerateprogress in ECG representation learning. xECG achieves superior performanceover earlier approaches, defining a new baseline for future ECG foundationmodels.</description>
      <author>example@mail.com (Riccardo Lunelli, Angus Nicolson, Samuel Martin Pröll, Sebastian Johannes Reinstadler, Axel Bauer, Clemens Dlaska)</author>
      <guid isPermaLink="false">2509.10151v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty-Aware Tabular Prediction: Evaluating VBLL-Enhanced TabPFN in Safety-Critical Medical Data</title>
      <link>http://arxiv.org/abs/2509.10048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究评估了将变分贝叶斯最后一层（VBLL）集成到表格先验数据拟合网络（TabPFN）中后对不确定性校准性能的影响，并在三个医疗数据集上进行了比较。&lt;h4&gt;背景&lt;/h4&gt;预测模型正被广泛应用于包括医疗诊断和刑事司法等安全关键领域，在这些领域中可靠的不确定性估计至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估VBLL集成到TabPFN中后对不确定性校准性能的影响。&lt;h4&gt;方法&lt;/h4&gt;在三个基准医疗表格数据集上比较原始TabPFN和VBLL集成版本的性能。&lt;h4&gt;主要发现&lt;/h4&gt;与预期相反，原始TabPFN在所有数据集上的不确定性校准性能均优于VBLL集成的TabPFN。&lt;h4&gt;结论&lt;/h4&gt;尽管VBLL在其他应用中能有效提高不确定性估计，但在与TabPFN集成时，原始TabPFN的不确定性校准表现更好。&lt;h4&gt;翻译&lt;/h4&gt;预测模型正被广泛应用于各种领域，包括医疗诊断和刑事司法等安全关键应用。在这样的场景中，可靠的不确定性估计是一项关键任务。表格先验数据拟合网络（TabPFN）是最近提出的一种用于表格数据集的机器学习基础模型，它采用生成式Transformer架构。变分贝叶斯最后一层（VBLL）是最先进的轻量级变分公式，能够以最小的计算开销有效提高不确定性估计。在本研究中，我们旨在评估VBLL与最近提出的TabPFN集成后在不确定性校准方面的性能。我们在三个基准医疗表格数据集上进行了实验，比较了原始TabPFN和VBLL集成版本的性能。与预期相反，我们在所有数据集上都观察到原始TabPFN的不确定性校准性能始终优于VBLL集成的TabPFN。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive models are being increasingly used across a wide range of domains,including safety-critical applications such as medical diagnosis and criminaljustice. Reliable uncertainty estimation is a crucial task in such settings.Tabular Prior-data Fitted Network (TabPFN) is a recently proposed machinelearning foundation model for tabular dataset, which uses a generativetransformer architecture. Variational Bayesian Last Layers (VBLL) is astate-of-the-art lightweight variational formulation that effectively improvesuncertainty estimation with minimal computational overhead. In this work we aimto evaluate the performance of VBLL integrated with the recently proposedTabPFN in uncertainty calibration. Our experiments, conducted on threebenchmark medical tabular datasets, compare the performance of the originalTabPFN and the VBLL-integrated version. Contrary to expectations, we observedthat original TabPFN consistently outperforms VBLL integrated TabPFN inuncertainty calibration across all datasets.</description>
      <author>example@mail.com (Madhushan Ramalingam)</author>
      <guid isPermaLink="false">2509.10048v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Chord: Chain of Rendering Decomposition for PBR Material Estimation from Generated Texture Images</title>
      <link>http://arxiv.org/abs/2509.09952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to SIGGRAPH Asia 2025. Project page:  https://ubisoft-laforge.github.io/world/chord&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的两阶段生成和估计框架用于PBR材质生成，解决了传统方法耗时耗力以及现有方法在质量、灵活性和用户控制方面的不足。&lt;h4&gt;背景&lt;/h4&gt;材质创建和重建对外观建模至关重要，但传统方法需要艺术家投入大量时间和专业知识。近期利用视觉基础模型合成PBR材质的方法在质量、灵活性和用户控制方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效、高质量且能提供灵活用户控制的PBR材质生成方法。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段框架：1)生成阶段：使用微调的扩散模型合成符合用户输入的、可平铺的纹理图像；2)估计阶段：引入链式分解方案，通过将先前提取的表示作为输入传递到单步图像条件扩散模型，顺序预测SVBRDF通道。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在材质生成和估计方面表现出优越性能，材质估计方法在生成的纹理和野外照片上都显示出强大的鲁棒性，且框架具有广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;该方法高效、高质量且能提供灵活的用户控制，在多种应用场景中表现优异，包括文本到材质、图像到材质、结构引导的生成和材质编辑。&lt;h4&gt;翻译&lt;/h4&gt;材质创建和重建对外观建模至关重要，但传统上需要艺术家投入大量时间和专业知识。虽然最近的方法利用视觉基础模型从用户提供的输入合成PBR材质，但它们在质量、灵活性和用户控制方面常常不尽如人意。我们提出了一种新颖的两阶段生成和估计框架用于PBR材质生成。在生成阶段，微调的扩散模型合成符合用户输入的、可平铺的纹理图像。在估计阶段，我们引入了一种链式分解方案，通过将先前提取的表示作为输入传递到单步图像条件扩散模型，顺序预测SVBRDF通道。我们的方法高效、高质量且能提供灵活的用户控制。我们将我们的方法与现有的材质生成和估计方法进行了比较，展示了优越的性能。我们的材质估计方法在生成的纹理和野外照片上都显示出强大的鲁棒性。此外，我们展示了我们的框架在多种应用中的灵活性，包括文本到材质、图像到材质、结构引导的生成和材质编辑。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决PBR材质生成和估计的问题。传统方法需要专业艺术家花费大量时间，而现有自动生成方法在质量、灵活性和用户控制方面表现不佳。这个问题在现实中很重要，因为材质建模是游戏、电影、虚拟现实等领域的核心需求，影响着数字内容的真实感和视觉效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：多通道压缩限制了灵活性，逆向渲染问题具有不确定性。因此设计了两阶段框架：先生成纹理图像，再分解为材质通道。借鉴了扩散模型（如SDXL）、图像条件扩散模型（如RGB→X）、内在分解方法和材质生成统一框架（如MatFusion）的思想，但进行了创新改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两阶段框架和链式分解（Chord）处理材质生成和估计。第一阶段使用微调的扩散模型生成可平铺的纹理RGB图像；第二阶段按特定顺序（基础颜色→法线→粗糙度和金属度）预测SVBRDF通道，利用LEGO-conditioning为不同模态提供特定权重。整体流程包括纹理生成、材质估计两个主要阶段，训练过程包含预训练和单步两个阶段。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）两阶段生成和估计框架；2）链式分解方案（Chord）；3）LEGO-conditioning机制；4）单步训练方法。相比之前的工作，该方法先生成纹理再分解而非直接预测通道，更好地处理了模态间关系，利用扩散模型先验知识提高了质量和效率，并更明确地建模了SVBRDF模态间的内在关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种两阶段生成和估计框架，通过链式分解和LEGO-conditioning实现了高质量、高效率且用户可控的PBR材质生成和估计，显著提升了材质生成质量和用户控制能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Material creation and reconstruction are crucial for appearance modeling buttraditionally require significant time and expertise from artists. While recentmethods leverage visual foundation models to synthesize PBR materials fromuser-provided inputs, they often fall short in quality, flexibility, and usercontrol. We propose a novel two-stage generate-and-estimate framework for PBRmaterial generation. In the generation stage, a fine-tuned diffusion modelsynthesizes shaded, tileable texture images aligned with user input. In theestimation stage, we introduce a chained decomposition scheme that sequentiallypredicts SVBRDF channels by passing previously extracted representation asinput into a single-step image-conditional diffusion model. Our method isefficient, high quality, and enables flexible user control. We evaluate ourapproach against existing material generation and estimation methods,demonstrating superior performance. Our material estimation method shows strongrobustness on both generated textures and in-the-wild photographs. Furthermore,we highlight the flexibility of our framework across diverse applications,including text-to-material, image-to-material, structure-guided generation, andmaterial editing.</description>
      <author>example@mail.com (Zhi Ying, Boxiang Rong, Jingyu Wang, Maoyuan Xu)</author>
      <guid isPermaLink="false">2509.09952v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Segment Anything for Cell Tracking</title>
      <link>http://arxiv.org/abs/2509.09943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于SAM2的零样本细胞追踪框架，解决了现有方法对标记数据集依赖性强且泛化能力有限的问题，实现了在2D和3D显微镜视频中的高精度细胞追踪，无需数据集特定的微调。&lt;h4&gt;背景&lt;/h4&gt;在时间推移显微镜图像序列中追踪细胞和检测有丝分裂事件是生物医学研究中的关键任务，但由于分裂物体、低信噪比、不明确的边界、密集簇和单个细胞的视觉相似性，这仍然极具挑战性。现有的基于深度学习的方法依赖于手动标记的数据集进行训练，这既昂贵又耗时，且由于显微镜数据的巨大多样性，它们对未见过的数据集的泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;克服现有细胞追踪方法对标记数据集的依赖性和泛化能力有限的限制，开发一种无需微调即可跨多样化显微镜数据集泛化的细胞追踪方法。&lt;h4&gt;方法&lt;/h4&gt;通过将Segment Anything 2（SAM2）这种为通用图像和视频分割设计的大型基础模型整合到细胞追踪流程中，构建了一种完全无监督的零样本细胞追踪框架。该方法不依赖于任何特定的训练数据集，也不继承任何特定训练数据集的偏差。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在二维和大规模三维时间推移显微镜视频中都取得了具有竞争力的准确性，同时消除了对数据集特定适应的需求。&lt;h4&gt;结论&lt;/h4&gt;通过整合SAM2大型基础模型，作者提出了一种完全无监督的零样本细胞追踪方法，能够在多样化显微镜数据集上泛化，无需数据集特定的微调，同时保持高准确性。&lt;h4&gt;翻译&lt;/h4&gt;在时间推移显微镜图像序列中追踪细胞和检测有丝分裂事件是生物医学研究中的关键任务。然而，由于分裂物体、低信噪比、不明确的边界、密集簇以及单个细胞的视觉相似性，这仍然极具挑战性。现有的基于深度学习的方法依赖于手动标记的数据集进行训练，这既昂贵又耗时。此外，由于显微镜数据的巨大多样性，它们对未见过的数据集的泛化能力仍然有限。为了克服这些限制，我们通过将Segment Anything 2（SAM2）——一种为通用图像和视频分割设计的大型基础模型——整合到追踪流程中，提出了一种零样本细胞追踪框架。作为一种完全无监督的方法，我们的方法不依赖于或继承任何特定训练数据集的偏差，使其能够在无需微调的情况下跨多样化显微镜数据集进行泛化。我们的方法在二维和大规模三维时间推移显微镜视频中都取得了具有竞争力的准确性，同时消除了对数据集特定适应的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking cells and detecting mitotic events in time-lapse microscopy imagesequences is a crucial task in biomedical research. However, it remains highlychallenging due to dividing objects, low signal-tonoise ratios, indistinctboundaries, dense clusters, and the visually similar appearance of individualcells. Existing deep learning-based methods rely on manually labeled datasetsfor training, which is both costly and time-consuming. Moreover, theirgeneralizability to unseen datasets remains limited due to the vast diversityof microscopy data. To overcome these limitations, we propose a zero-shot celltracking framework by integrating Segment Anything 2 (SAM2), a large foundationmodel designed for general image and video segmentation, into the trackingpipeline. As a fully-unsupervised approach, our method does not depend on orinherit biases from any specific training dataset, allowing it to generalizeacross diverse microscopy datasets without finetuning. Our approach achievescompetitive accuracy in both 2D and large-scale 3D time-lapse microscopy videoswhile eliminating the need for dataset-specific adaptation.</description>
      <author>example@mail.com (Zhu Chen, Mert Edgü, Er Jin, Johannes Stegmaier)</author>
      <guid isPermaLink="false">2509.09943v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>LoFT: Parameter-Efficient Fine-Tuning for Long-tailed Semi-Supervised Learning in Open-World Scenarios</title>
      <link>http://arxiv.org/abs/2509.09926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为LoFT的长尾半监督学习框架，通过基础模型微调解决传统方法中的过度自信和低质量伪标签问题，并进一步提出LoFT-OW处理开放世界条件下的半监督学习。实验表明，即使只使用以往方法1%的未标记数据，新方法也能取得优越性能。&lt;h4&gt;背景&lt;/h4&gt;长尾学习在现实场景中应用广泛。长尾半监督学习(LTSSL)通过整合大量未标记数据到不平衡标记数据集中成为有效解决方案，但现有方法多从零开始训练，导致过度自信和低质量伪标签问题。&lt;h4&gt;目的&lt;/h4&gt;扩展LTSSL到基础模型微调范式，解决从头训练导致的问题；探索更实际的开放世界条件下的半监督学习场景。&lt;h4&gt;方法&lt;/h4&gt;提出LoFT(Long-tailed semi-supervised learning via parameter-efficient Fine-Tuning)框架，利用微调基础模型生成更可靠伪标签；提出LoFT-OW处理开放世界条件下的半监督学习问题，提高模型判别能力。&lt;h4&gt;主要发现&lt;/h4&gt;微调的基础模型能生成更可靠的伪标签；在多个基准测试上，新方法性能优于以前方法；即使只使用以往方法1%的未标记数据，也能获得更好性能。&lt;h4&gt;结论&lt;/h4&gt;LoFT框架有效解决了LTSSL中的过度自信和低质量伪标签问题；LoFT-OW能处理开放世界条件下的半监督学习；基础模型微调是解决长尾半监督学习问题的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;长尾学习由于其现实场景中的广泛适用性而获得越来越多的关注。在现有方法中，长尾半监督学习(LTSSL)通过将大量未标记数据整合到不平衡标记数据集中，已成为一种有效的解决方案。然而，大多数先前的LTSSL方法设计为从头开始训练模型，这常常导致过度自信和低质量伪标签等问题。为了解决这些挑战，我们将LTSSL扩展到基础模型微调范式，并提出了一种新框架：LoFT(通过参数高效微调的长尾半监督学习)。我们证明微调的基础模型可以生成更可靠的伪标签，从而有利于不平衡学习。此外，我们通过研究开放世界条件下的半监督学习来探索更实际的设置，其中未标记数据可能包括分布外(OOD)样本。为了解决这个问题，我们提出LoFT-OW(开放世界场景下的LoFT)来提高判别能力。在多个基准测试上的实验结果表明，与以前的方法相比，我们的方法取得了优越的性能，即使与以前的工作相比只使用1%的未标记数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-tailed learning has garnered increasing attention due to its wideapplicability in real-world scenarios. Among existing approaches, Long-TailedSemi-Supervised Learning (LTSSL) has emerged as an effective solution byincorporating a large amount of unlabeled data into the imbalanced labeleddataset. However, most prior LTSSL methods are designed to train models fromscratch, which often leads to issues such as overconfidence and low-qualitypseudo-labels. To address these challenges, we extend LTSSL into the foundationmodel fine-tuning paradigm and propose a novel framework: LoFT (Long-tailedsemi-supervised learning via parameter-efficient Fine-Tuning). We demonstratethat fine-tuned foundation models can generate more reliable pseudolabels,thereby benefiting imbalanced learning. Furthermore, we explore a morepractical setting by investigating semi-supervised learning under open-worldconditions, where the unlabeled data may include out-of-distribution (OOD)samples. To handle this problem, we propose LoFT-OW (LoFT under Open-Worldscenarios) to improve the discriminative ability. Experimental results onmultiple benchmarks demonstrate that our method achieves superior performancecompared to previous approaches, even when utilizing only 1\% of the unlabeleddata compared with previous works.</description>
      <author>example@mail.com (Jiahao Chen, Zhiyuan Huang, Yurou Liu, Bing Su)</author>
      <guid isPermaLink="false">2509.09926v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>How well can LLMs provide planning feedback in grounded environments?</title>
      <link>http://arxiv.org/abs/2509.09790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练基础模型（如大型语言模型和视觉语言模型）在具身环境规划中提供反馈的能力，评估了不同类型反馈和推理方法的效果，发现基础模型能提供高质量反馈，且更大规模的模型表现更好。&lt;h4&gt;背景&lt;/h4&gt;在具身环境中进行规划学习通常需要精心设计的奖励函数或高质量标注的演示数据，这增加了学习成本和难度。&lt;h4&gt;目的&lt;/h4&gt;评估基础模型在不同类型环境中提供反馈的能力，研究不同反馈类型和推理方法对规划性能的影响，探索基础模型减少对奖励设计和演示数据依赖的可能性。&lt;h4&gt;方法&lt;/h4&gt;研究者在符号、语言和连续控制环境中评估了LLMs和VLMs的反馈能力，考虑了二元反馈、偏好反馈、行动建议、目标建议和增量动作反馈等反馈类型，以及上下文学习、思维链和访问环境动态信息等推理方法。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型能够在不同领域提供多样化的高质量反馈；更大规模的推理模型提供更准确的反馈，表现出更少的偏见，并且从增强的推理方法中获益更多；反馈质量在具有复杂动态或连续状态空间和动作空间的环境中会下降。&lt;h4&gt;结论&lt;/h4&gt;基础模型可以作为规划学习的有效反馈来源，减少对奖励设计和演示数据的依赖，但在复杂动态环境中可能需要额外的改进或辅助方法。&lt;h4&gt;翻译&lt;/h4&gt;在具身环境中学习规划通常需要精心设计的奖励函数或高质量的标注演示。最近的研究表明，预训练的基础模型（如大型语言模型和视觉语言模型）捕捉了对规划有用的背景知识，这减少了策略学习所需的奖励设计和演示数据的数量。我们评估了LLMs和VLMs在符号、语言和连续控制环境中提供反馈的能力。我们考虑了规划的主要反馈类型，包括二元反馈、偏好反馈、行动建议、目标建议和增量动作反馈。我们还研究了影响反馈性能的推理方法，包括上下文学习、思维链和访问环境动态信息。我们发现基础模型能够在不同领域提供多样化的高质量反馈。此外，更大规模的推理模型始终提供更准确的反馈，表现出更少的偏见，并且从增强的推理方法中获益更多。最后，对于具有复杂动态或连续状态空间和动作空间的环境，反馈质量会下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to plan in grounded environments typically requires carefullydesigned reward functions or high-quality annotated demonstrations. Recentworks show that pretrained foundation models, such as large language models(LLMs) and vision language models (VLMs), capture background knowledge helpfulfor planning, which reduces the amount of reward design and demonstrationsneeded for policy learning. We evaluate how well LLMs and VLMs provide feedbackacross symbolic, language, and continuous control environments. We considerprominent types of feedback for planning including binary feedback, preferencefeedback, action advising, goal advising, and delta action feedback. We alsoconsider inference methods that impact feedback performance, includingin-context learning, chain-of-thought, and access to environment dynamics. Wefind that foundation models can provide diverse high-quality feedback acrossdomains. Moreover, larger and reasoning models consistently provide moreaccurate feedback, exhibit less bias, and benefit more from enhanced inferencemethods. Finally, feedback quality degrades for environments with complexdynamics or continuous state spaces and action spaces.</description>
      <author>example@mail.com (Yuxuan Li, Victor Zhong)</author>
      <guid isPermaLink="false">2509.09790v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>AI-enabled tuberculosis screening in a high-burden setting using cough sound analysis and speech foundation models</title>
      <link>http://arxiv.org/abs/2509.09746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  submitted to The Lancet Digital Health&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究利用基于语音基础模型的深度学习技术分析咳嗽声音，以检测结核病。研究在赞比亚招募了500名参与者，包括TB患者、其他呼吸道疾病患者和健康人。仅使用音频的分类器表现良好，而结合人口统计和临床特征的多模态模型性能更佳，达到WHO目标产品特征基准。&lt;h4&gt;背景&lt;/h4&gt;人工智能可用于检测与疾病相关的咳嗽声音模式，为高负担、低资源环境中的结核病筛查提供可扩展方法。以往研究存在数据集小、症状性非TB患者代表性不足、依赖简单模型、在理想条件下收集录音等局限性。&lt;h4&gt;目的&lt;/h4&gt;开发并评估基于深度学习的咳嗽声音分析模型，用于结核病的筛查和分诊，特别是在资源有限的环境中。&lt;h4&gt;方法&lt;/h4&gt;在赞比亚两家医院招募512名参与者，分为TB患者组、其他呼吸道疾病患者组和健康对照组。从500名参与者获取咳嗽录音及人口统计和临床数据。使用基于语音基础模型的深度学习分类器训练模型，并在3秒片段上表现最佳的模型进一步评估人口统计和临床特征的影响。&lt;h4&gt;主要发现&lt;/h4&gt;仅音频分类器区分TB+与其他人的AUROC为85.2%，区分TB+与其他呼吸道疾病的AUROC为80.1%。加入人口统计和临床特征后，性能提升至92.1%和84.2%。在0.38阈值下，多模态模型对TB+/Rest达到90.3%敏感性和73.1%特异性，对TB+/OR达到80.6%和73.1%特异性。&lt;h4&gt;结论&lt;/h4&gt;基于语音基础模型的咳嗽分析，特别是结合人口统计和临床数据时，作为结核病分诊工具显示出强大潜力，达到WHO目标产品特征基准。该模型对背景噪声、录音时间和设备变异性等混杂因素具有鲁棒性，表明检测到真实的疾病相关声学模式。临床应用前需在不同地区和病例定义中进一步验证。&lt;h4&gt;翻译&lt;/h4&gt;背景：人工智能可检测与疾病相关的咳嗽声音模式，为高负担、低资源环境中的结核病筛查提供可扩展方法。以往研究受限于小数据集、症状性非结核病患者代表性不足、依赖简单模型及在理想条件下收集录音。方法：我们在赞比亚两家医院招募512名参与者，分为细菌学确认的结核病患者、有其他呼吸道疾病的症状患者和健康对照组。从500名参与者获取可用的咳嗽录音及人口统计和临床数据。基于语音基础模型的深度学习分类器在咳嗽录音上进行训练。表现最佳的模型在3秒片段上训练，并进一步使用人口统计和临床特征进行评估。结果：仅音频的最佳分类器区分结核病患者与其他人的AUROC为85.2%，区分结核病患者与其他呼吸道疾病患者的AUROC为80.1%。加入人口统计和临床特征后，性能提升至92.1%和84.2%。在0.38阈值下，多模态模型对结核病患者与其他人达到90.3%敏感性和73.1%特异性，对结核病患者与其他呼吸道疾病患者达到80.6%和73.1%特异性。结论：使用语音基础模型进行咳嗽分析，特别是结合人口统计和临床数据时，显示出作为结核病分诊工具的强大潜力，达到WHO目标产品特征基准。该模型对背景噪声、录音时间和设备变异性等混杂因素具有鲁棒性，表明检测到真实的疾病相关声学模式。临床应用前需在不同地区和病例定义中进一步验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background  Artificial intelligence (AI) can detect disease-related acoustic patterns incough sounds, offering a scalable approach to tuberculosis (TB) screening inhigh-burden, low-resource settings. Previous studies have been limited by smalldatasets, under-representation of symptomatic non-TB patients, reliance onsimple models, and recordings collected under idealised conditions.  Methods  We enrolled 512 participants at two hospitals in Zambia, grouped asbacteriologically confirmed TB (TB+), symptomatic patients with otherrespiratory diseases (OR), and healthy controls (HC). Usable cough recordingsplus demographic and clinical data were obtained from 500 participants. Deeplearning classifiers based on speech foundation models were trained on coughrecordings. The best-performing model, trained on 3-second segments, wasfurther evaluated with demographic and clinical features.  Findings  The best audio-only classifier achieved an AUROC of 85.2% for distinguishingTB+ from all others (TB+/Rest) and 80.1% for TB+ versus OR. Adding demographicand clinical features improved performance to 92.1% (TB+/Rest) and 84.2%(TB+/OR). At a threshold of 0.38, the multimodal model reached 90.3%sensitivity and 73.1% specificity for TB+/Rest, and 80.6% and 73.1% for TB+/OR.  Interpretation  Cough analysis using speech foundation models, especially when combined withdemographic and clinical data, showed strong potential as a TB triage tool,meeting WHO target product profile benchmarks. The model was robust toconfounding factors including background noise, recording time, and devicevariability, indicating detection of genuine disease-related acoustic patterns.Further validation across diverse regions and case definitions, includingsubclinical TB, is required before clinical use.</description>
      <author>example@mail.com (Ning Ma, Bahman Mirheidari, Guy J. Brown, Minyoi M. Maimbolwa, Nsala Sanjase, Solomon Chifwamba, Seke Muzazu, Monde Muyoyeta, Mary Kagujje)</author>
      <guid isPermaLink="false">2509.09746v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>MatSKRAFT: A framework for large-scale materials knowledge extraction from scientific tables</title>
      <link>http://arxiv.org/abs/2509.10448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatSKRAFT是一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识，从表格数据中构建包含超过535,000个条目的综合数据库，显著优于现有模型，并促进材料发现。&lt;h4&gt;背景&lt;/h4&gt;科学进步越来越需要综合大量文献中的知识，但大多数实验数据仍然被困在半结构化格式中，难以进行系统提取和分析。&lt;h4&gt;目的&lt;/h4&gt;开发一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识。&lt;h4&gt;方法&lt;/h4&gt;MatSKRAFT将表格转换为基于图的表示，通过约束驱动的图神经网络处理，并将科学原理直接编码到模型架构中。&lt;h4&gt;主要发现&lt;/h4&gt;MatSKRAFT在属性提取方面达到88.68的F1分数，在成分提取方面达到71.35的F1分数，处理速度比其他模型快19-496倍，硬件要求适中；应用于47,000多篇研究论文的近69,000个表格，构建了包含535,000多个条目和104,000种成分的综合数据库，扩展了现有数据库覆盖范围，揭示了具有独特属性组合的材料，并实现了成分-属性关系的发现。&lt;h4&gt;结论&lt;/h4&gt;这种系统化的方法为材料和科学发现奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;科学进步越来越依赖于综合大量文献中的知识，但大多数实验数据仍然被困在半结构化格式中，难以进行系统提取和分析。在此，我们提出了MatSKRAFT，一个计算框架，能够前所未有地大规模自动提取和整合材料科学知识。我们的方法将表格转换为基于图的表示，通过约束驱动的图神经网络处理，将科学原理直接编码到模型架构中。MatSKRAFT显著优于最先进的大型语言模型，在属性提取方面达到88.68的F1分数，在成分提取方面达到71.35的F1分数，同时处理速度比这些模型快19-496倍（与最慢和最快模型相比），且硬件要求适中。应用于来自47,000多篇研究论文的近69,000个表格，我们构建了一个包含超过535,000个条目的综合数据库，其中包括104,000种成分，扩展了现有数据库的覆盖范围，有待人工验证。这种系统化的方法揭示了以前被忽视的具有独特属性组合的材料，并实现了构成材料和科学发现基石的成分-属性关系的发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific progress increasingly depends on synthesizing knowledge acrossvast literature, yet most experimental data remains trapped in semi-structuredformats that resist systematic extraction and analysis. Here, we presentMatSKRAFT, a computational framework that automatically extracts and integratesmaterials science knowledge from tabular data at unprecedented scale. Ourapproach transforms tables into graph-based representations processed byconstraint-driven GNNs that encode scientific principles directly into modelarchitecture. MatSKRAFT significantly outperforms state-of-the-art largelanguage models, achieving F1 scores of 88.68 for property extraction and 71.35for composition extraction, while processing data $19$-$496\times$ faster thanthem (compared to the slowest and the fastest models, respectively) with modesthardware requirements. Applied to nearly 69,000 tables from more than 47,000research publications, we construct a comprehensive database containing over535,000 entries, including 104,000 compositions that expand coverage beyondmajor existing databases, pending manual validation. This systematic approachreveals previously overlooked materials with distinct property combinations andenables data-driven discovery of composition-property relationships forming thecornerstone of materials and scientific discovery.</description>
      <author>example@mail.com (Kausik Hira, Mohd Zaki, Mausam, N. M. Anoop Krishnan)</author>
      <guid isPermaLink="false">2509.10448v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Modeling and Risk Zoning of Global Extreme Precipitation via Graph Neural Networks and r-Pareto Processes</title>
      <link>http://arxiv.org/abs/2509.10362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合图神经网络与r-Pareto过程(GNN-rP)的混合框架，用于空间极端降水建模和风险分区，能够学习非线性、非平稳依赖结构，并应用于高风险区域识别和气候变化下的热点检测。&lt;h4&gt;背景&lt;/h4&gt;极端降水事件对人类社会构成重大威胁，可引发大范围复合洪水、山体滑坡和基础设施故障，传统统计空间极值模型存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合框架用于空间极端降水建模和风险分区，能够学习非线性、非平稳依赖结构，并识别气候变化下的高风险区域。&lt;h4&gt;方法&lt;/h4&gt;提出结合图神经网络与r-Pareto过程(GNN-rP)的混合框架，从降水衍生的空间图中学习依赖结构，应用数据驱动的尾部函数建模低维嵌入空间中的联合超越概率，使用NASA的IMERG观测数据和CMIP6 SSP5-8.5投影进行分析。&lt;h4&gt;主要发现&lt;/h4&gt;框架成功描绘了连贯的高风险区域，量化了其时序持续性，检测到气候变化下的新兴热点；与基线方法相比显著提高了高风险网格单元的检测能力；结果显示热带带特别是季风和对流区存在持续高风险区域，并揭示十年尺度持续性在高排放情景下被偶发性重组中断。&lt;h4&gt;结论&lt;/h4&gt;通过结合机器学习与极值理论，GNN-rP提供了一种可扩展、可解释的自适应气候风险分区工具，可直接应用于基础设施规划、防灾准备和气候韧性政策设计。&lt;h4&gt;翻译&lt;/h4&gt;发生在广大空间区域的极端降水事件对社会构成重大威胁，因为它们可能在大范围内引发复合洪水、山体滑坡和基础设施故障。本文提出了一种用于空间极端降水建模和风险分区的混合框架，该框架结合了图神经网络与r-Pareto过程(GNN-rP)。与传统统计空间极值模型不同，这种方法从降水衍生的空间图中学习非线性、非平稳的依赖结构，并应用数据驱动的尾部函数来建模低维嵌入空间中的联合超越概率。利用NASA的IMERG观测数据(2000-2021)和CMIP6 SSP5-8.5投影，该框架描绘了连贯的高风险区域，量化了它们的时序持续性，并检测了气候变化下的新兴热点。与两种基线方法相比，GNN-rP管道显著提高对高风险网格单元的逐点检测能力，同时保持了相当的聚类稳定性。结果突显了热带带特别是季风和对流区存在持续高风险区域，并揭示了在高排放情景下被偶发性重组所中断的十年尺度持续性。通过将机器学习与极值理论相结合，GNN-rP为自适应气候风险分区提供了一种可扩展、可解释的工具，可直接应用于基础设施规划、防灾准备和气候韧性政策设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extreme precipitation events occurring over large spatial domains posesubstantial threats to societies because they can trigger compound flooding,landslides, and infrastructure failures across wide areas. A hybrid frameworkfor spatial extreme precipitation modeling and risk zoning is proposed thatintegrates graph neural networks with r-Pareto processes (GNN-rP). Unliketraditional statistical spatial extremes models, this approach learnsnonlinear, nonstationary dependence structures from precipitation-derivedspatial graphs and applies a data-driven tail functional to model jointexceedances in a low-dimensional embedding space. Using NASA's IMERGobservations (2000-2021) and CMIP6 SSP5-8.5 projections, the frameworkdelineates coherent high-risk zones, quantifies their temporal persistence, anddetects emerging hotspots under climate change. Compared with two baselineapproaches, the GNN-rP pipeline substantially improves pointwise detection ofhigh-risk grid cells while yielding comparable clustering stability. Resultshighlight persistent high-risk regions in the tropical belt, especially monsoonand convective zones, and reveal decadal-scale persistence that is punctuatedby episodic reconfigurations under high-emission scenarios. By coupling machinelearning with extreme value theory, GNN-rP offers a scalable, interpretabletool for adaptive climate risk zoning, with direct applications ininfrastructure planning, disaster preparedness, and climate-resilient policydesign.</description>
      <author>example@mail.com (Zimu Wang, Yifan Wu, Daning Bi)</author>
      <guid isPermaLink="false">2509.10362v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Why does your graph neural network fail on some graphs? Insights from exact generalisation error</title>
      <link>http://arxiv.org/abs/2509.10337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过信号处理的视角，推导了图神经网络(GNNs)在转导固定设计设置下的确切泛化误差，解释了GNNs成功或失败的原因，并为模型选择提供了实践指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络被广泛用于图结构数据学习，但对它们成功或失败的原因缺乏原则性理解。先前研究关注架构限制如过平滑和过挤压，但不能解释GNNs如何提取有意义表示或为什么相似架构间性能差异大。这些问题与泛化能力相关，而现有GNN泛化误差界限通常宽松、仅限于单一架构，且对实践中泛化机制的洞察有限。&lt;h4&gt;目的&lt;/h4&gt;通过信号处理方法推导GNNs的确切泛化误差，解释GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供实践指导。&lt;h4&gt;方法&lt;/h4&gt;采用信号处理视角，将GNNs解释为通过图结构作用于节点特征的图滤波器算子。专注于线性GNNs同时允许图滤波器中的非线性，推导出包括卷积、基于PageRank和基于注意力模型在内的广泛GNNs的第一个确切泛化误差。&lt;h4&gt;主要发现&lt;/h4&gt;泛化误差的确切表征揭示出只有节点特征与图结构之间对齐的信息才对泛化有贡献；量化了同质性(homophily)对泛化的影响。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一个框架，解释了GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供了实践指导。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)被广泛应用于图结构数据的学习，然而对它们成功或失败的原因仍缺乏原则性理解。虽然先前研究已经考察了架构限制如过平滑和过挤压，但这些并不能解释GNNs如何提取有意义表示或为什么相似架构间性能差异巨大。这些问题与泛化能力相关：模型对未标记数据做出准确预测的能力。尽管已有研究推导了GNNs的泛化误差界限，但这些界限通常宽松、仅限于单一架构，且对实践中控制泛化的因素提供有限洞察。在本工作中，我们通过信号处理的视角，采用不同方法推导了GNNs在转导固定设计设置下的确切泛化误差。从这一观点看，GNNs可被解释为通过图结构作用于节点特征的图滤波器算子。在专注于线性GNNs的同时允许图滤波器中的非线性，我们推导出广泛GNNs（包括卷积、基于PageRank和基于注意力的模型）的第一个确切泛化误差。泛化误差的确切表征揭示出只有节点特征与图结构之间对齐的信息才对泛化有贡献。此外，我们量化了同质性对泛化的影响。我们的工作提供了一个框架，解释了GNNs何时以及为何能有效利用结构和特征信息，为模型选择提供了实践指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are widely used in learning on graph-structureddata, yet a principled understanding of why they succeed or fail remainselusive. While prior works have examined architectural limitations such asover-smoothing and over-squashing, these do not explain what enables GNNs toextract meaningful representations or why performance varies drasticallybetween similar architectures. These questions are related to the role ofgeneralisation: the ability of a model to make accurate predictions onunlabelled data. Although several works have derived generalisation errorbounds for GNNs, these are typically loose, restricted to a singlearchitecture, and offer limited insight into what governs generalisation inpractice. In this work, we take a different approach by deriving the exactgeneralisation error for GNNs in a transductive fixed-design setting throughthe lens of signal processing. From this viewpoint, GNNs can be interpreted asgraph filter operators that act on node features via the graph structure. Byfocusing on linear GNNs while allowing non-linearity in the graph filters, wederive the first exact generalisation error for a broad range of GNNs,including convolutional, PageRank-based, and attention-based models. The exactcharacterisation of the generalisation error reveals that only the alignedinformation between node features and graph structure contributes togeneralisation. Furthermore, we quantify the effect of homophily ongeneralisation. Our work provides a framework that explains when and why GNNscan effectively leverage structural and feature information, offering practicalguidance for model selection.</description>
      <author>example@mail.com (Nil Ayday, Mahalakshmi Sabanayagam, Debarghya Ghoshdastidar)</author>
      <guid isPermaLink="false">2509.10337v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Approximate Graph Propagation Revisited: Dynamic Parameterized Queries, Tighter Bounds and Dynamic Updates</title>
      <link>http://arxiv.org/abs/2509.10036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新研究了近似图传播(AGP)框架，针对动态图和动态参数化查询场景提出了改进算法AGP-Static++和AGP-Dynamic，显著提高了查询效率和更新性能。&lt;h4&gt;背景&lt;/h4&gt;AGP是一个统一框架，可以捕获各种图传播任务，如PageRank、图神经网络中的特征传播和基于图的检索增强生成(RAG)。在动态图和动态参数化查询场景下，底层图随时间演变，查询参数根据应用需求即时指定。&lt;h4&gt;目的&lt;/h4&gt;解决现有AGP-Static算法在动态参数化查询和动态图场景下的局限性，包括查询时间复杂度高和动态图更新效率低的问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种新算法：1) AGP-Static++：简化算法设计，将查询复杂度降低约对数平方n因子，同时保留近似保证；2) AGP-Dynamic：实现每次更新常数时间的摊销时间，显著提高动态图更新效率，同时保持查询复杂度和近似保证。&lt;h4&gt;主要发现&lt;/h4&gt;1) AGP-Static可适应支持动态参数化查询，但存在查询复杂度高和动态图更新效率低的问题；2) AGP-Static++能显著降低查询复杂度；3) AGP-Dynamic能大幅提高动态图更新效率，实现常数时间摊销每次更新。&lt;h4&gt;结论&lt;/h4&gt;提出的AGP-Static++和AGP-Dynamic算法在理论和实验上都显著优于现有方法，与基线相比，在更新时间上实现了高达177倍的加速，在查询效率上实现了10倍的加速。&lt;h4&gt;翻译&lt;/h4&gt;我们重新研究了近似图传播(AGP)，这是一个统一框架，可以捕获各种图传播任务，如PageRank、图神经网络(GNN)中的特征传播和基于图的检索增强生成(RAG)。我们的工作重点是动态图和动态参数化查询的场景，其中底层图随时间演变（通过边插入或删除更新），输入查询参数根据应用需求即时指定。我们的第一个贡献是一个有趣的观察：SOTA解决方案AGP-Static可以适应支持动态参数化查询；然而，仍有几个挑战未解决。首先，AGP-Static的查询时间复杂度基于一个假设：在其查询算法中使用子集采样的最优算法。不幸的是，当时不存在这样的算法；没有这种最优算法，查询复杂度需要额外的对数平方n因子，其中n是图中的顶点数。其次，AGP-Static在动态图上表现不佳，每次更新需要n乘以对数n的时间。为了解决这些挑战，我们提出了一种新算法AGP-Static++，它更简单，同时将查询复杂度大致降低了对数平方n因子，同时保留了AGP-Static的近似保证。然而，AGP-Static++仍然需要n时间来处理每个更新。为了更好地支持动态图，我们进一步提出了AGP-Dynamic，它实现了每次更新常数时间的摊销时间，显著改善了前面提到的每次更新n的界限，同时仍然保留了查询复杂度和近似保证。最后，我们的全面实验验证了理论改进：与基线相比，我们的算法在更新时间上实现了高达177倍的加速，在查询效率上实现了10倍的加速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit Approximate Graph Propagation (AGP), a unified framework whichcaptures various graph propagation tasks, such as PageRank, feature propagationin Graph Neural Networks (GNNs), and graph-based Retrieval-Augmented Generation(RAG). Our work focuses on the settings of dynamic graphs and dynamicparameterized queries, where the underlying graphs evolve over time (updated byedge insertions or deletions) and the input query parameters are specified onthe fly to fit application needs. Our first contribution is an interestingobservation that the SOTA solution, AGP-Static, can be adapted to supportdynamic parameterized queries; however several challenges remain unresolved.Firstly, the query time complexity of AGP-Static is based on an assumption ofusing an optimal algorithm for subset sampling in its query algorithm.Unfortunately, back to that time, such an algorithm did not exist; without suchan optimal algorithm, an extra $O(\log^2 n)$ factor is required in the querycomplexity, where $n$ is the number of vertices in the graphs. Secondly,AGP-Static performs poorly on dynamic graphs, taking $O(n\log n)$ time toprocess each update. To address these challenges, we propose a new algorithm,AGP-Static++, which is simpler yet reduces roughly a factor of $O(\log^2 n)$ inthe query complexity while preserving the approximation guarantees ofAGP-Static. However, AGP-Static++ still requires $O(n)$ time to process eachupdate. To better support dynamic graphs, we further propose AGP-Dynamic, whichachieves $O(1)$ amortized time per update, significantly improving theaforementioned $O(n)$ per-update bound, while still preserving the querycomplexity and approximation guarantees. Last, our comprehensive experimentsvalidate the theoretical improvements: compared to the baselines, our algorithmachieves speedups of up to $177\times$ on update time and $10\times$ on queryefficiency.</description>
      <author>example@mail.com (Zhuowei Zhao, Zhuo Zhang, Hanzhi Wang, Junhao Gan, Zhifeng Bao, Jianzhong Qi)</author>
      <guid isPermaLink="false">2509.10036v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>HGEN: Heterogeneous Graph Ensemble Networks</title>
      <link>http://arxiv.org/abs/2509.09843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper is in proceedings of the 34th IJCAI Conference, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HGEN是一种创新的异构图集成学习方法，通过元路径和转换优化流程集成多个学习器，提高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;异构图中的节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来挑战，特别是在适应多样化的图学习器方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理异构图的集成学习框架，提高分类准确性，同时确保学习器之间的多样性。&lt;h4&gt;方法&lt;/h4&gt;HGEN使用元路径结合随机丢弃创建等位图神经网络(Allele GNNs)，训练基础图学习器并进行对齐。框架包含两个关键组件：残差注意力机制校准不同元路径的GNNs，使节点嵌入更关注信息量大的图；相关性正则化项扩大不同元路径生成的嵌入矩阵差异，增强学习器多样性。&lt;h4&gt;主要发现&lt;/h4&gt;HGEN通过残差注意力机制和相关正则化项有效提高了基础学习器的准确性和多样性。实验表明HGEN的正则化程度高于简单投票，在五个异构网络上的表现显著优于最先进竞争对手。&lt;h4&gt;结论&lt;/h4&gt;HGEN为异构图的集成学习提供了有效解决方案，通过创新的元路径和优化流程，显著提升了分类性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了HGEN，这是一种开创性的异构图集成学习方法。我们认为，节点类型、节点特征和局部邻域拓扑的异质性给集成学习带来了重大挑战，特别是在适应多样化的图学习器方面。我们的HGEN框架通过基于元路径和转换的优化流程集成多个学习器，以提高分类准确性。具体来说，HGEN使用元路径结合随机丢弃创建等位图神经网络(GNNs)，基础图学习器在这些网络上进行训练和对齐以便后续集成。为确保有效的集成学习，HGEN提出了两个关键组件：1)残差注意力机制用于校准不同元路径的等位GNNs，使节点嵌入更专注于信息量更大的图，提高基础学习器准确性；2)相关性正则化项扩大不同元路径生成的嵌入矩阵之间的差异，从而丰富基础学习器多样性。我们分析了HGEN的收敛性，并证明其正则化程度高于简单投票。在五个异构网络上的实验验证了HGEN以显著优势持续优于最先进的竞争对手。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents HGEN that pioneers ensemble learning for heterogeneousgraphs. We argue that the heterogeneity in node types, nodal features, andlocal neighborhood topology poses significant challenges for ensemble learning,particularly in accommodating diverse graph learners. Our HGEN frameworkensembles multiple learners through a meta-path and transformation-basedoptimization pipeline to uplift classification accuracy. Specifically, HGENuses meta-path combined with random dropping to create Allele Graph NeuralNetworks (GNNs), whereby the base graph learners are trained and aligned forlater ensembling. To ensure effective ensemble learning, HGEN presents two keycomponents: 1) a residual-attention mechanism to calibrate allele GNNs ofdifferent meta-paths, thereby enforcing node embeddings to focus on moreinformative graphs to improve base learner accuracy, and 2) acorrelation-regularization term to enlarge the disparity among embeddingmatrices generated from different meta-paths, thereby enriching base learnerdiversity. We analyze the convergence of HGEN and attest its higherregularization magnitude over simple voting. Experiments on five heterogeneousnetworks validate that HGEN consistently outperforms its state-of-the-artcompetitors by substantial margin.</description>
      <author>example@mail.com (Jiajun Shen, Yufei Jin, Yi He, Xingquan Zhu)</author>
      <guid isPermaLink="false">2509.09843v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Commissioning and Testing of IceAct Telescopes at the IceCube Neutrino Observatory</title>
      <link>http://arxiv.org/abs/2509.09778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;IceAct是一个位于IceCube中微子天文台上方的成像切伦科夫望远镜阵列，专为南极恶劣环境设计。该系统由七个望远镜组成'苍蝇眼'配置和一个额外的望远镜用于立体观测。研究团队进行了严格的测试和校准程序，包括现场几何对齐和图神经网络重建初级粒子方向。&lt;h4&gt;背景&lt;/h4&gt;IceAct是一个部署在南极冰面上的成像切伦科夫望远镜阵列，位于著名的IceCube中微子天文台上方。每个望远镜配备硅光电倍增器相机和菲涅尔透镜，具有12度视场。&lt;h4&gt;目的&lt;/h4&gt;研究旨在介绍IceAct望远镜的测试程序、现场对齐校准方法，以及使用图神经网络重建初级粒子方向的技术，并验证其在实际数据中的应用。&lt;h4&gt;方法&lt;/h4&gt;研究团队通过比较IceCube测量的μ子方向重建与IceAct重建的初级粒子方向，推导出每个IceAct望远镜的几何对齐。此外，还采用了图神经网络技术重建初级粒子方向，并通过蒙特卡洛模拟进行验证，最后应用于调试数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究展示了IceAct望远镜的测试程序，包括夜空观测和低温测试；开发了现场对齐校准方法；实现了基于图神经网络的初级粒子方向重建；验证了该方法在蒙特卡洛模拟中的有效性；并将该方法成功应用于调试数据集。&lt;h4&gt;结论&lt;/h4&gt;IceAct望远镜阵列已成功部署并进行了全面测试和校准，图神经网络重建方法表现良好，为未来中微子和宇宙射线观测提供了可靠的工具。&lt;h4&gt;翻译&lt;/h4&gt;IceAct是一个位于IceCube中微子天文台冰面上方的成像切伦科夫望远镜阵列。每个望远镜配备基于硅光电倍增器的61像素相机和菲涅尔透镜作为成像光学元件，产生12度的视场。设计针对恶劣环境进行了优化，特别是在南极。该装置将由七个望远镜组成在所谓的'苍蝇眼'配置中，将视场扩大到36度，另外还有一个相距200米的望远镜用于立体观测。在部署前进行了严格的测试程序，确保在这些条件下能够运行，例如夜空观测和低温测试。此外，现场校准用于验证安装的准确性和可靠性。我们通过比较使用IceCube测量的μ子的方向重建与IceAct对应初级粒子方向重建，推导出每个IceAct望远镜的几何对齐。本贡献展示了这些测试程序。此外，我们展示了现场对齐校准，包括IceAct中初级粒子方向的图神经网络重建、蒙特卡洛模拟验证以及应用于调试数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; IceAct is an array of imaging air Cherenkov telescopes located at the icesurface above the IceCube Neutrino Observatory. Each telescope features asilicon photomultiplier based 61 pixel camera and a Fresnel-lens as imagingoptic, resulting in a 12-degree field of view. The design is optimized to beoperated in harsh environments, particularly at the South Pole. The setup willconsist of seven telescopes in a so-called fly's eye configuration, increasingthe field of view to 36^\circ, and an additional telescope 200m apart forstereoscopic observations. Rigorous testing procedures have been performedbefore deployment to ensure that operation under these conditions is possible,e.g. night sky observations and cold temperature tests. Furthermore, on-sitecalibrations are used to verify the accuracy and reliability of theinstallation. We derive the geometric alignment of each IceAct telescope bycomparing the directional reconstruction of muons measured with IceCube to thecorresponding primary particle direction reconstruction from IceAct. Thiscontribution presents these testing procedures. Additionally, we present theon-site alignment calibration, including a Graph Neural Network reconstructionfor the primary particle direction in IceAct, verification on Monte Carlosimulation, and the application to a commissioning dataset.</description>
      <author>example@mail.com (Arun Vaidyanathan, Lars Heuermann)</author>
      <guid isPermaLink="false">2509.09778v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>HHI-Assist: A Dataset and Benchmark of Human-Human Interaction in Physical Assistance Scenario</title>
      <link>http://arxiv.org/abs/2509.10096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to RA-L 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于辅助机器人的交互感知运动预测方法，通过创建新数据集和开发基于Transformer的条件去噪扩散模型，有效捕捉了护工与护理接受者之间的耦合动力学，显著提升了机器人在物理交互场景中的辅助能力。&lt;h4&gt;背景&lt;/h4&gt;劳动力短缺和人口老龄化问题日益严重，需要辅助机器人来支持人类护理接受者。在物理交互场景中，机器人需要准确预测人类动作以确保安全和响应性帮助，然而这仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;解决辅助环境中可变性和物理交互中耦合动力学复杂性的挑战，实现安全、响应性辅助。&lt;h4&gt;方法&lt;/h4&gt;创建HHI-Assist数据集，包含辅助任务中人类-人类交互的运动捕捉片段；开发基于Transformer的条件去噪扩散模型，用于预测交互代理的姿态。&lt;h4&gt;主要发现&lt;/h4&gt;模型能有效捕捉护工和护理接受者之间的耦合动力学，相比基线方法有所改进，对未见过的场景有很强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过推进交互感知的运动预测和引入新数据集，显著增强机器人辅助策略。数据集和代码已在https://sites.google.com/view/hhi-assist/home公开可用。&lt;h4&gt;翻译&lt;/h4&gt;日益严重的劳动力短缺和人口老龄化凸显了对辅助机器人的需求，以支持人类护理接受者。为了实现安全且响应迅速的辅助，机器人在物理交互场景中需要准确预测人类动作。然而，由于辅助环境的多变性和物理交互中耦合动力学的复杂性，这仍然是一项具有挑战性的任务。在本工作中，我们通过两个关键贡献解决了这些挑战：(1) HHI-Assist，一个包含辅助任务中人类-人类交互动作捕捉片段的数据集；(2) 一种基于Transformer的条件去噪扩散模型，用于预测交互代理的姿态。我们的模型有效地捕捉了护工和护理接受者之间的耦合动力学，展示了相比基线方法的改进，并对未见场景表现出强大的泛化能力。通过推进交互感知的运动预测和引入新数据集，我们的工作有潜力显著增强机器人辅助策略。数据集和代码可在以下网址获取：https://sites.google.com/view/hhi-assist/home&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3586011&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing labor shortage and aging population underline the need forassistive robots to support human care recipients. To enable safe andresponsive assistance, robots require accurate human motion prediction inphysical interaction scenarios. However, this remains a challenging task due tothe variability of assistive settings and the complexity of coupled dynamics inphysical interactions. In this work, we address these challenges through twokey contributions: (1) HHI-Assist, a dataset comprising motion capture clips ofhuman-human interactions in assistive tasks; and (2) a conditionalTransformer-based denoising diffusion model for predicting the poses ofinteracting agents. Our model effectively captures the coupled dynamics betweencaregivers and care receivers, demonstrating improvements over baselines andstrong generalization to unseen scenarios. By advancing interaction-awaremotion prediction and introducing a new dataset, our work has the potential tosignificantly enhance robotic assistance policies. The dataset and code areavailable at: https://sites.google.com/view/hhi-assist/home</description>
      <author>example@mail.com (Saeed Saadatnejad, Reyhaneh Hosseininejad, Jose Barreiros, Katherine M. Tsui, Alexandre Alahi)</author>
      <guid isPermaLink="false">2509.10096v1</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>http://arxiv.org/abs/2509.06426v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了首个果蝇腿部的三维数据驱动肌肉骨骼模型，填补了现有研究空白。该模型连接了神经活动和运动表现，为理解运动控制提供了新工具。通过结合实验数据和仿真，模型预测了肌肉协同作用，并测试了关节特性对学习的影响。该模型不仅有助于基础科学研究，还可应用于开发更自然的机器人运动控制。&lt;h4&gt;背景&lt;/h4&gt;计算模型对于理解神经、生物力学和物理系统如何相互作用以协调动物行为至关重要。尽管已有果蝇中枢神经系统、肌肉和骨骼的完整重建，但基于解剖学和物理学的果蝇腿肌模型仍然缺失。这些模型是运动神经元活动和关节运动之间不可或缺的桥梁。&lt;h4&gt;目的&lt;/h4&gt;引入第一个果蝇腿部的三维、数据驱动的肌肉骨骼模型，并在OpenSim和MuJoCo仿真环境中实现该模型。&lt;h4&gt;方法&lt;/h4&gt;基于来自多个固定标本的高分辨率X射线扫描，采用Hill型肌肉表示。提出了一个使用形态学成像数据构建肌肉模型的流程，并优化了果蝇特有的未知肌肉参数。将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据相结合，在OpenSim中实现肌肉驱动的行为重放。在MuJoCo中训练模仿学习策略，测试不同被动关节特性对学习速度的影响。&lt;h4&gt;主要发现&lt;/h4&gt;模拟多种行走和梳理行为中的肌肉活动预测了协调的肌肉协同作用，这些协同作用可以通过实验进行测试。阻尼和刚度有助于学习。&lt;h4&gt;结论&lt;/h4&gt;该模型使得在实验上易于处理的模式生物中研究运动控制成为可能，深入了解生物力学如何促进复杂肢体运动的产生。该模型还可用于控制具身人工智能体，在模拟环境中生成自然且柔顺的运动。&lt;h4&gt;翻译&lt;/h4&gt;计算模型对于推进我们理解神经、生物力学和物理系统如何相互作用以协调动物行为至关重要。尽管已有果蝇(Drosophila melanogaster)中枢神经系统、肌肉和骨骼的近乎完整重建，但基于解剖学和物理学的果蝇腿肌模型仍然缺失。这些模型是运动神经元活动和关节运动之间不可或缺的桥梁。在此，我们介绍了首个果蝇腿部的三维、数据驱动的肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现。我们的模型纳入了基于来自多个固定标本的高分辨率X射线扫描的Hill型肌肉表示。我们提出了一个使用形态学成像数据构建肌肉模型的流程，并优化了果蝇特有的未知肌肉参数。然后，我们将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据相结合，在OpenSim中实现肌肉驱动的行为重放。对多种行走和梳理行为中肌肉活动的模拟预测了可通过实验测试的协调肌肉协同作用。此外，通过在MuJoCo中训练模仿学习策略，我们测试了不同被动关节特性对学习速度的影响，发现阻尼和刚度有助于学习。总体而言，我们的模型使得在实验上易于处理的模式生物中研究运动控制成为可能，深入了解生物力学如何促进复杂肢体运动的产生。此外，我们的模型可用于控制具身人工智能体，在模拟环境中生成自然且柔顺的运动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models are critical to advance our understanding of how neural,biomechanical, and physical systems interact to orchestrate animal behaviors.Despite the availability of near-complete reconstructions of the Drosophilamelanogaster central nervous system, musculature, and exoskeleton, anatomicallyand physically grounded models of fly leg muscles are still missing. Thesemodels provide an indispensable bridge between motor neuron activity and jointmovements. Here, we introduce the first 3D, data-driven musculoskeletal modelof Drosophila legs, implemented in both OpenSim and MuJoCo simulationenvironments. Our model incorporates a Hill-type muscle representation based onhigh-resolution X-ray scans from multiple fixed specimens. We present apipeline for constructing muscle models using morphological imaging data andfor optimizing unknown muscle parameters specific to the fly. We then combineour musculoskeletal models with detailed 3D pose estimation data from behavingflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations ofmuscle activity across diverse walking and grooming behaviors predictcoordinated muscle synergies that can be tested experimentally. Furthermore, bytraining imitation learning policies in MuJoCo, we test the effect of differentpassive joint properties on learning speed and find that damping and stiffnessfacilitate learning. Overall, our model enables the investigation of motorcontrol in an experimentally tractable model organism, providing insights intohow biomechanics contribute to generation of complex limb movements. Moreover,our model can be used to control embodied artificial agents to generatenaturalistic and compliant locomotion in simulated environments.</description>
      <author>example@mail.com (Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya)</author>
      <guid isPermaLink="false">2509.06426v2</guid>
      <pubDate>Mon, 15 Sep 2025 14:57:30 +0800</pubDate>
    </item>
    <item>
      <title>Resource-Efficient Glioma Segmentation on Sub-Saharan MRI</title>
      <link>http://arxiv.org/abs/2509.09469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种针对资源受限环境的深度学习框架，用于撒哈拉以南非洲地区的胶质瘤MRI分割，克服了数据稀缺的挑战，取得了良好的分割效果，模型紧凑且高效。&lt;h4&gt;背景&lt;/h4&gt;胶质瘤是最常见的原发性脑肿瘤类型，从MRI图像中准确分割它们对诊断、治疗计划和纵向监测至关重要。然而，撒哈拉以南非洲地区高质量标注影像数据的稀缺是部署高级分割模型在临床工作流程中的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍一种为资源受限环境量身定制的稳健且计算效率高的深度学习框架，以支持低资源环境中的临床决策。&lt;h4&gt;方法&lt;/h4&gt;使用3D注意力UNet架构，增加残差块，并通过从BraTS 2021数据集上的预训练权重进行迁移学习来增强模型。在95个来自BraTS-Africa数据集的MRI病例上评估模型。&lt;h4&gt;主要发现&lt;/h4&gt;尽管数据质量和数量有限，模型仍取得了良好的分割效果：增强肿瘤的Dice分数为0.76，坏死和非增强肿瘤核心的Dice分数为0.80，周围非功能半球的Dice分数为0.85。模型具有约90MB的紧凑架构，在消费级硬件上的每卷推理时间不到一分钟。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明了所提出模型的泛化能力及其在资源有限环境中支持临床决策的潜力。这项工作通过为服务不足的地区提供高性能且可及的医学影像解决方案，为缩小全球健康领域公平AI的差距做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;胶质瘤是最常见的原发性脑肿瘤类型，从MRI图像中准确分割它们对诊断、治疗计划和纵向监测至关重要。然而，撒哈拉以南非洲地区高质量标注影像数据的稀缺是部署高级分割模型在临床工作流程中的重大挑战。本研究介绍了一种为资源受限环境量身定制的稳健且计算效率高的深度学习框架。我们利用了3D注意力UNet架构，增加了残差块，并通过从BraTS 2021数据集上的预训练权重进行迁移学习来增强模型。我们的模型在来自BraTS-Africa数据集的95个MRI病例上进行了评估，该数据集是撒哈拉以南非洲MRI数据胶质瘤分割的基准。尽管数据质量和数量有限，我们的方法仍取得了良好的效果：增强肿瘤的Dice分数为0.76，坏死和非增强肿瘤核心的Dice分数为0.80，周围非功能半球的Dice分数为0.85。这些结果表明了所提出模型的泛化能力及其在资源有限环境中支持临床决策的潜力。模型约90MB的紧凑架构以及在消费级硬件上每卷不到一分钟的推理时间进一步证明了其在撒哈拉以南非洲卫生系统中部署的实用性。这项工作通过为服务不足的地区提供高性能且可及的医学影像解决方案，为缩小全球健康领域公平AI的差距做出了贡献。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在撒哈拉以南非洲地区资源有限的情况下，如何高效地对胶质瘤MRI图像进行分割的问题。这个问题很重要，因为胶质瘤是最常见的原发性脑肿瘤，准确分割对诊断和治疗至关重要；而该地区放射科医生稀缺，现有深度学习方法又需要大量计算资源，无法在资源有限的环境中部署；此外，该地区胶质瘤患者预后很差，近80%患者在诊断后两年内死亡，亟需更好的诊断工具。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法在撒哈拉以南非洲地区的局限性，包括计算资源需求高、数据质量差异大等。他们借鉴了3D U-Net架构、残差块和注意力机制等现有技术，同时针对资源受限环境进行了优化。作者采用两阶段训练策略：先在大型BraTS 2021数据集上预训练，再在BraTS-Africa数据集上微调，以提高模型在特定地区数据上的表现。此外，他们还设计了轻量级模型和优化预处理流程，以适应低质量MRI图像和有限计算资源的环境。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个轻量级的3D注意力U-Net架构，结合残差块，并通过迁移学习策略提高在撒哈拉以南非洲特定数据上的分割性能。整体流程包括：1)使用BraTS-Africa数据集，包含95名胶质瘤患者的多参数MRI扫描；2)预处理阶段进行图像裁剪、模态堆叠和强度归一化；3)应用数据增强提高模型泛化能力；4)构建3D注意力U-Net与残差块的模型架构；5)采用两阶段训练策略进行预训练和微调；6)使用5折交叉验证和多种指标评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)针对资源受限环境优化的轻量级模型(约91.4MB)，在消费级硬件上实现快速推理；2)专门针对撒哈拉以南非洲MRI图像的低分辨率和模态差异进行优化；3)两阶段训练策略，结合大规模预训练和特定领域微调；4)结合残差块和注意力机制提高分割精度。相比之前工作，该方法更轻量、更适合资源有限环境，且专门针对撒哈拉以南非洲地区的特定挑战进行了优化，而大多数现有方法没有考虑这些因素。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种轻量级的3D注意力U-Net模型，通过迁移学习优化了在撒哈拉以南非洲资源受限环境下对胶质瘤MRI图像的高效分割，为医疗资源不足地区提供了可及的医学影像解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gliomas are the most prevalent type of primary brain tumors, and theiraccurate segmentation from MRI is critical for diagnosis, treatment planning,and longitudinal monitoring. However, the scarcity of high-quality annotatedimaging data in Sub-Saharan Africa (SSA) poses a significant challenge fordeploying advanced segmentation models in clinical workflows. This studyintroduces a robust and computationally efficient deep learning frameworktailored for resource-constrained settings. We leveraged a 3D Attention UNetarchitecture augmented with residual blocks and enhanced through transferlearning from pre-trained weights on the BraTS 2021 dataset. Our model wasevaluated on 95 MRI cases from the BraTS-Africa dataset, a benchmark for gliomasegmentation in SSA MRI data. Despite the limited data quality and quantity,our approach achieved Dice scores of 0.76 for the Enhancing Tumor (ET), 0.80for Necrotic and Non-Enhancing Tumor Core (NETC), and 0.85 for SurroundingNon-Functional Hemisphere (SNFH). These results demonstrate thegeneralizability of the proposed model and its potential to support clinicaldecision making in low-resource settings. The compact architecture,approximately 90 MB, and sub-minute per-volume inference time on consumer-gradehardware further underscore its practicality for deployment in SSA healthsystems. This work contributes toward closing the gap in equitable AI forglobal health by empowering underserved regions with high-performing andaccessible medical imaging solutions.</description>
      <author>example@mail.com (Freedmore Sidume, Oumayma Soula, Joseph Muthui Wacira, YunFei Zhu, Abbas Rabiu Muhammad, Abderrazek Zeraii, Oluwaseun Kalejaye, Hajer Ibrahim, Olfa Gaddour, Brain Halubanza, Dong Zhang, Udunna C Anazodo, Confidence Raymond)</author>
      <guid isPermaLink="false">2509.09469v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
  <item>
      <title>From scratch to silver: Creating trustworthy training data for patent-SDG classification using Large Language Models</title>
      <link>http://arxiv.org/abs/2509.09303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于弱监督的专利-SDG分类方法，利用专利引用到SDG标记的科学出版物作为初始信号，结合大型语言模型提取结构化概念，并通过跨域相似度计算和复合标记函数进行校准，最终生成高质量的专利-SDG映射数据集。&lt;h4&gt;背景&lt;/h4&gt;将专利按照联合国可持续发展目标(SDGs)分类对于追踪创新如何应对全球挑战至关重要。然而，缺乏大型标记数据集限制了监督学习的应用。现有方法如关键词搜索、迁移学习和基于引用的启发式方法缺乏可扩展性和通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效将专利映射到SDGs的方法，克服数据稀缺问题，同时保证分类的准确性和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;将专利-SDG分类视为弱监督问题，使用专利引用到SDG标记科学出版物(NPL引用)作为初始信号；开发复合标记函数(LF)，利用大型语言模型从专利和SDG论文中提取功能、解决方案和应用等结构化概念；基于专利本体计算跨域相似度分数；通过仅正损失函数校准标记函数；生成银标准软多标签数据集。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在内部验证中优于多个基线模型(包括基于transformer的模型和零样本LLM)；在外部验证中，与传统技术分类相比，生成的标签在专利引用、共同发明人和共同申请人网络中显示出更大的主题、认知和组织一致性。&lt;h4&gt;结论&lt;/h4&gt;弱监督和语义对齐能够增强大规模SDG分类的效果，为追踪创新如何应对全球挑战提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;根据联合国可持续发展目标(SDGs)对专利进行分类对于追踪创新如何应对全球挑战至关重要。然而，缺乏大型标记数据集限制了监督学习的使用。现有方法如关键词搜索、迁移学习和基于引用的启发式方法缺乏可扩展性和通用性。本文将专利-SDG分类视为弱监督问题，使用专利引用到SDG标记的科学出版物(NPL引用)作为初始信号。为了解决其稀疏性和噪声问题，我们开发了一个复合标记函数(LF)，利用大型语言模型(LLMs)基于专利本体从专利和SDG论文中提取结构化概念，即功能、解决方案和应用。使用基于检索的方法计算和组合跨域相似度分数。通过定制的仅正损失函数对LF进行校准，与已知的NPL-SDG链接保持一致，同时不惩罚新SDG关联的发现。结果是一个银标准、软多标签数据集，将专利映射到SDGs，能够训练有效的多标签回归模型。我们通过两种互补策略验证该方法：(1)针对保留的NPL标签进行内部验证，我们的方法优于多个基线模型，包括基于transformer的模型和零样本LLM；(2)使用专利引用、共同发明人和共同申请人图中的网络模块度进行外部验证，我们的标签显示出比传统技术分类更大的主题、认知和组织一致性。这些结果表明，弱监督和语义对齐能够增强大规模SDG分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classifying patents by their relevance to the UN Sustainable DevelopmentGoals (SDGs) is crucial for tracking how innovation addresses globalchallenges. However, the absence of a large, labeled dataset limits the use ofsupervised learning. Existing methods, such as keyword searches, transferlearning, and citation-based heuristics, lack scalability and generalizability.This paper frames patent-to-SDG classification as a weak supervision problem,using citations from patents to SDG-tagged scientific publications (NPLcitations) as a noisy initial signal. To address its sparsity and noise, wedevelop a composite labeling function (LF) that uses large language models(LLMs) to extract structured concepts, namely functions, solutions, andapplications, from patents and SDG papers based on a patent ontology.Cross-domain similarity scores are computed and combined using a rank-basedretrieval approach. The LF is calibrated via a custom positive-only loss thataligns with known NPL-SDG links without penalizing discovery of new SDGassociations. The result is a silver-standard, soft multi-label dataset mappingpatents to SDGs, enabling the training of effective multi-label regressionmodels. We validate our approach through two complementary strategies: (1)internal validation against held-out NPL-based labels, where our methodoutperforms several baselines including transformer-based models, and zero-shotLLM; and (2) external validation using network modularity in patent citation,co-inventor, and co-applicant graphs, where our labels reveal greater thematic,cognitive, and organizational coherence than traditional technologicalclassifications. These results show that weak supervision and semanticalignment can enhance SDG classification at scale.</description>
      <author>example@mail.com (Grazia Sveva Ascione, Nicolò Tamagnone)</author>
      <guid isPermaLink="false">2509.09303v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Boosting Data Utilization for Multilingual Dense Retrieval</title>
      <link>http://arxiv.org/abs/2509.09459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP 2025 (main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种提高多语言密集检索中数据利用率的方法，通过获取高质量的难负样本和有效的小批量数据来提升检索性能&lt;h4&gt;背景&lt;/h4&gt;多语言密集检索旨在基于统一的检索器模型在不同语言中检索相关文档，其挑战在于将不同语言的表示在共享向量空间中对齐&lt;h4&gt;目的&lt;/h4&gt;提高多语言密集检索中的数据利用率，解决对比学习中负样本质量和小批量数据效率的问题&lt;h4&gt;方法&lt;/h4&gt;不同于专注于开发复杂模型架构的现有研究，提出通过获取高质量的难负样本和有效的小批量数据来提升多语言密集检索性能的方法&lt;h4&gt;主要发现&lt;/h4&gt;在包含16种语言的多语言检索基准测试MIRACL上的广泛实验结果表明，该方法有效且性能优于多个现有的强基线模型&lt;h4&gt;结论&lt;/h4&gt;通过提高数据利用率，特别是获取高质量的难负样本和有效的小批量数据，可以有效提升多语言密集检索的性能&lt;h4&gt;翻译&lt;/h4&gt;多语言密集检索旨在基于统一的检索器模型在不同语言中检索相关文档。挑战在于将不同语言的表示在共享向量空间中对齐。常见做法是通过对比学习微调密集检索器，其有效性高度依赖于负样本的质量和小批量数据的效率。与专注于开发复杂模型架构的现有研究不同，我们提出了一种通过获取高质量的难负样本和有效的小批量数据来提高多语言密集检索数据利用率的方法。在包含16种语言的多语言检索基准MIRACL上的广泛实验结果证明了我们方法的有效性，性能优于几个现有的强基线模型&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multilingual dense retrieval aims to retrieve relevant documents acrossdifferent languages based on a unified retriever model. The challenge lies inaligning representations of different languages in a shared vector space. Thecommon practice is to fine-tune the dense retriever via contrastive learning,whose effectiveness highly relies on the quality of the negative sample and theefficacy of mini-batch data. Different from the existing studies that focus ondeveloping sophisticated model architecture, we propose a method to boost datautilization for multilingual dense retrieval by obtaining high-quality hardnegative samples and effective mini-batch data. The extensive experimentalresults on a multilingual retrieval benchmark, MIRACL, with 16 languagesdemonstrate the effectiveness of our method by outperforming several existingstrong baselines.</description>
      <author>example@mail.com (Chao Huang, Fengran Mo, Yufeng Chen, Changhao Guan, Zhenrui Yue, Xinyu Wang, Jinan Xu, Kaiyu Huang)</author>
      <guid isPermaLink="false">2509.09459v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Listening for "You": Enhancing Speech Image Retrieval via Target Speaker Extraction</title>
      <link>http://arxiv.org/abs/2509.09306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于目标说话人语音的图像检索方法，通过对比学习整合音频和视觉模型，在多说话人场景中实现了显著优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;使用语音提示进行图像检索是多模态感知领域的一个有前景的方向，但在多说话人场景中有效利用语音信号仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的'目标说话人语音-图像检索'任务和框架，学习在有目标说话人存在的情况下图像与多说话人语音信号之间的关系。&lt;h4&gt;方法&lt;/h4&gt;将预训练的自监督音频编码器与视觉模型通过目标说话人感知对比学习进行整合，基于目标说话人提取和检索模块，使系统能够从目标说话人处提取语音命令并将其与相应图像对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在SpokenCOCO2Mix和SpokenCOCO3Mix上的实验表明，TSRE方法在2人和3人场景中分别实现了36.3%和29.9%的Recall@1，显著优于单说话人基线和最先进模型。&lt;h4&gt;结论&lt;/h4&gt;该方法在辅助机器人和多模态交互系统的实际部署中显示出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;使用语音提示进行图像检索已成为多模态感知领域的一个有前景的方向，但在多说话人场景中利用语音仍然具有挑战性。我们提出了一种新的目标说话人语音-图像检索任务和框架，学习在有目标说话人存在的情况下图像与多说话人语音信号之间的关系。我们的方法通过目标说话人感知对比学习，将预训练的自监督音频编码器与视觉模型整合，基于目标说话人提取和检索模块。这使系统能够从目标说话人处提取语音命令并将其与相应图像对齐。在SpokenCOCO2Mix和SpokenCOCO3Mix上的实验表明，TSRE显著优于现有方法，在2人和3人场景中分别实现了36.3%和29.9%的Recall@1，比单说话人基线和最先进模型有显著提升。我们的方法在辅助机器人和多模态交互系统的实际部署中显示出潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image retrieval using spoken language cues has emerged as a promisingdirection in multimodal perception, yet leveraging speech in multi-speakerscenarios remains challenging. We propose a novel Target Speaker Speech-ImageRetrieval task and a framework that learns the relationship between images andmulti-speaker speech signals in the presence of a target speaker. Our methodintegrates pre-trained self-supervised audio encoders with vision models viatarget speaker-aware contrastive learning, conditioned on a Target SpeakerExtraction and Retrieval module. This enables the system to extract spokencommands from the target speaker and align them with corresponding images.Experiments on SpokenCOCO2Mix and SpokenCOCO3Mix show that TSRE significantlyoutperforms existing methods, achieving 36.3% and 29.9% Recall@1 in 2 and 3speaker scenarios, respectively - substantial improvements over single speakerbaselines and state-of-the-art models. Our approach demonstrates potential forreal-world deployment in assistive robotics and multimodal interaction systems.</description>
      <author>example@mail.com (Wenhao Yang, Jianguo Wei, Wenhuan Lu, Xinyue Song, Xianghu Yue)</author>
      <guid isPermaLink="false">2509.09306v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Over-the-Air Adversarial Attack Detection: from Datasets to Defenses</title>
      <link>http://arxiv.org/abs/2509.09296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了AdvSV 2.0数据集，提出了基于神经回放模拟器的对抗攻击方法和CODA-OCC防御方法，有效提高了自动说话人验证系统的安全性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;自动说话人验证(ASV)系统可用于语音应用的身份验证，但容易受到线上(OTL)和空中(OTA)对抗性攻击的威胁。现有的检测方法由于缺乏全面的数据集而未得到充分测试。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的数据集来解决对抗性攻击检测方法缺乏充分测试的问题，并提高ASV系统对对抗攻击的防御能力。&lt;h4&gt;方法&lt;/h4&gt;开发了AdvSV 2.0数据集，包含628k个样本和800小时音频数据，涵盖经典对抗攻击算法和ASV系统，包括OTL和OTA场景。引入了基于神经回放模拟器(NRS)的新型对抗攻击方法，并提出了CODA-OCC防御方法，这是一种在一类分类框架中的对比学习方法。&lt;h4&gt;主要发现&lt;/h4&gt;CODA-OCC在AdvSV 2.0数据集上实现了11.2%的等错误率(EER)和0.95的AUC，优于几种最先进的检测方法。基于神经回放模拟器的攻击方法增强了OTA攻击的效力，对ASV系统构成更大威胁。&lt;h4&gt;结论&lt;/h4&gt;AdvSV 2.0数据集为对抗性攻击研究提供了重要资源，CODA-OCC方法能有效防御ASV系统面临的威胁，提高了系统的安全性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;自动说话人验证(ASV)系统可用于语音应用的身份验证。然而，最近的研究揭示了这些系统容易受到线上(OTL)和空中(OTA)对抗性攻击的漏洞。尽管已经提出了各种检测方法来应对这些威胁，但由于缺乏全面的数据集，这些方法没有得到充分测试。为了解决这一差距，我们开发了AdvSV 2.0数据集，包含628k个样本，总时长800小时。该数据集包含经典对抗攻击算法、ASV系统，并涵盖OTL和OTA场景。此外，我们引入了一种基于神经回放模拟器(NRS)的新型对抗攻击方法，增强了OTA对抗攻击的效力，从而对ASV系统构成更大威胁。为了防御这些攻击，我们提出了CODA-OCC，一种在一类分类框架中的对比学习方法。实验结果表明，在AdvSV 2.0数据集上，CODA-OCC的EER为11.2%，AUC为0.95，优于几种最先进的检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatic Speaker Verification (ASV) systems can be used for voice-enabledapplications for identity verification. However, recent studies have exposedthese systems' vulnerabilities to both over-the-line (OTL) and over-the-air(OTA) adversarial attacks. Although various detection methods have beenproposed to counter these threats, they have not been thoroughly tested due tothe lack of a comprehensive data set. To address this gap, we developed theAdvSV 2.0 dataset, which contains 628k samples with a total duration of 800hours. This dataset incorporates classical adversarial attack algorithms, ASVsystems, and encompasses both OTL and OTA scenarios. Furthermore, we introducea novel adversarial attack method based on a Neural Replay Simulator (NRS),which enhances the potency of adversarial OTA attacks, thereby presenting agreater threat to ASV systems. To defend against these attacks, we proposeCODA-OCC, a contrastive learning approach within the one-class classificationframework. Experimental results show that CODA-OCC achieves an EER of 11.2% andan AUC of 0.95 on the AdvSV 2.0 dataset, outperforming several state-of-the-artdetection methods.</description>
      <author>example@mail.com (Li Wang, Xiaoyan Lei, Haorui He, Lei Wang, Jie Shi, Zhizheng Wu)</author>
      <guid isPermaLink="false">2509.09296v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Multi-Attention Meta Transformer for Rotating Machinery Fault Diagnosis</title>
      <link>http://arxiv.org/abs/2509.09251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多注意力元Transformer方法（MMT-FD）用于少样本无监督旋转机械故障诊断，从未标记数据中提取潜在故障表示，具有强大泛化能力，适用于各种类型机械设备的故障诊断。&lt;h4&gt;背景&lt;/h4&gt;旋转机械设备的智能故障诊断通常需要大量标记样本数据，但在实际工业应用中获取足够数据在时间和成本方面具有挑战性。此外，不同类型的旋转机械设备具有独特机械特性，需要为每种情况单独训练诊断模型。&lt;h4&gt;目的&lt;/h4&gt;解决有限故障样本和预测模型在实际工程应用中缺乏泛化能力的问题，提出适用于少样本无监督旋转机械故障诊断的方法。&lt;h4&gt;方法&lt;/h4&gt;MMT-FD框架集成了时频域编码器和元学习泛化模型。时频域编码器预测通过时频域随机增强生成的状态表示，这些增强数据被输入到元学习网络中进行分类和泛化训练，然后使用少量标记数据进行微调。模型通过少量对比学习迭代进行优化，实现高效率。&lt;h4&gt;主要发现&lt;/h4&gt;在轴承故障数据集和转子试验台数据上的实验表明，MMT-FD模型仅使用1%的标记样本数据就能实现99%的故障诊断准确率，表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MMT-FD框架能够有效解决有限故障样本和模型泛化能力的问题，适用于各种类型机械设备的故障诊断。&lt;h4&gt;翻译&lt;/h4&gt;旋转机械设备的智能故障诊断通常需要大量标记样本数据。然而，在实际工业应用中，获取足够的数据在时间和成本方面都具有挑战性和高成本。此外，不同类型的旋转机械设备具有不同的独特机械特性，需要为每种情况单独训练诊断模型。为了解决有限故障样本和预测模型在实际工程应用中缺乏泛化能力的问题，我们提出了一种用于少样本无监督旋转机械故障诊断的多注意力元Transformer方法（MMT-FD）。该框架从未标记数据中提取潜在故障表示，并表现出强大的泛化能力，使其适用于诊断各种类型机械设备的故障。MMT-FD框架集成了一个时频域编码器和一个元学习泛化模型。时频域编码器预测通过时频域随机增强生成的状态表示。这些增强数据随后被输入到元学习网络中进行分类和泛化训练，然后使用少量标记数据进行微调。模型通过少量对比学习迭代进行优化，从而实现高效率。为了验证该框架，我们在轴承故障数据集和转子试验台数据上进行了实验。结果表明，MMT-FD模型仅使用1%的标记样本数据就能实现99%的故障诊断准确率，表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The intelligent fault diagnosis of rotating mechanical equipment usuallyrequires a large amount of labeled sample data. However, in practicalindustrial applications, acquiring enough data is both challenging andexpensive in terms of time and cost. Moreover, different types of rotatingmechanical equipment with different unique mechanical properties, requireseparate training of diagnostic models for each case. To address the challengesof limited fault samples and the lack of generalizability in prediction modelsfor practical engineering applications, we propose a Multi-Attention MetaTransformer method for few-shot unsupervised rotating machinery fault diagnosis(MMT-FD). This framework extracts potential fault representations fromunlabeled data and demonstrates strong generalization capabilities, making itsuitable for diagnosing faults across various types of mechanical equipment.The MMT-FD framework integrates a time-frequency domain encoder and ameta-learning generalization model. The time-frequency domain encoder predictsstatus representations generated through random augmentations in thetime-frequency domain. These enhanced data are then fed into a meta-learningnetwork for classification and generalization training, followed by fine-tuningusing a limited amount of labeled data. The model is iteratively optimizedusing a small number of contrastive learning iterations, resulting in highefficiency. To validate the framework, we conducted experiments on a bearingfault dataset and rotor test bench data. The results demonstrate that theMMT-FD model achieves 99\% fault diagnosis accuracy with only 1\% of labeledsample data, exhibiting robust generalization capabilities.</description>
      <author>example@mail.com (Hanyang Wang, Yuxuan Yang, Hongjun Wang, Lihui Wang)</author>
      <guid isPermaLink="false">2509.09251v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Target-oriented Multimodal Sentiment Classification with Counterfactual-enhanced Debiasing</title>
      <link>http://arxiv.org/abs/2509.09160v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE International Conference on Multimedia and Expo  (ICME 2025). \copyright\ 2025 IEEE. Personal use of this material is  permitted. Permission from IEEE must be obtained for all other uses&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种反事实增强去偏框架，用于减少目标导向多模态情感分类中的虚假相关性，提高分类准确性。&lt;h4&gt;背景&lt;/h4&gt;目标导向多模态情感分类旨在预测图像-文本对中特定目标的情感极性。现有方法虽取得有竞争力性能，但过度依赖文本内容，忽视数据集偏差特别是词级上下文偏差，导致文本特征与输出标签间存在虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;减少文本特征与输出标签间的虚假相关性，提高目标导向多模态情感分类的准确性。&lt;h4&gt;方法&lt;/h4&gt;引入反事实增强去偏框架，包含：1)反事实数据增强策略，最小程度改变情感相关因果特征，生成细节匹配的图像-文本来引导模型关注情感相关内容；2)自适应去偏对比学习机制，从反事实数据中学习鲁棒特征并提示模型决策，有效减轻偏差词影响。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验结果表明，提出的方法优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;通过反事实增强去偏框架可以有效减少虚假相关性，提高目标导向多模态情感分类的准确性。&lt;h4&gt;翻译&lt;/h4&gt;目标导向多模态情感分类旨在预测图像-文本对中特定目标的情感极性。虽然现有工作取得了有竞争力的性能，但它们往往过度依赖文本内容，而未能考虑数据集偏差，特别是词级上下文偏差。这导致文本特征与输出标签之间存在虚假相关性，影响了分类准确性。在本文中，我们引入了一种新颖的反事实增强去偏框架来减少此类虚假相关性。我们的框架结合了一种反事实数据增强策略，该策略最小程度地改变情感相关的因果特征，生成细节匹配的图像-文本来引导模型关注与情感相关的内容。此外，为了从反事实数据中学习鲁棒特征并提示模型决策，我们引入了一种自适应去偏对比学习机制，有效减轻了偏差词的影响。在多个基准数据集上的实验结果表明，我们提出的方法优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Target-oriented multimodal sentiment classification seeks to predictsentiment polarity for specific targets from image-text pairs. While existingworks achieve competitive performance, they often over-rely on textual contentand fail to consider dataset biases, in particular word-level contextualbiases. This leads to spurious correlations between text features and outputlabels, impairing classification accuracy. In this paper, we introduce a novelcounterfactual-enhanced debiasing framework to reduce such spuriouscorrelations. Our framework incorporates a counterfactual data augmentationstrategy that minimally alters sentiment-related causal features, generatingdetail-matched image-text samples to guide the model's attention toward contenttied to sentiment. Furthermore, for learning robust features fromcounterfactual data and prompting model decisions, we introduce an adaptivedebiasing contrastive learning mechanism, which effectively mitigates theinfluence of biased words. Experimental results on several benchmark datasetsshow that our proposed method outperforms state-of-the-art baselines.</description>
      <author>example@mail.com (Zhiyue Liu, Fanrong Ma, Xin Ling)</author>
      <guid isPermaLink="false">2509.09160v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Gradient-Attention Guided Dual-Masking Synergetic Framework for Robust Text-based Person Retrieval</title>
      <link>http://arxiv.org/abs/2509.09118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种改进的CLIP模型（GA-DMS）用于人物表征学习，通过构建大规模高质量数据集和改进模型架构解决了CLIP在人物表征中的两个关键挑战。&lt;h4&gt;背景&lt;/h4&gt;CLIP在多种视觉任务中表现出色，但在人物表征学习方面面临两个关键挑战：缺乏大规模标注的人物中心化视觉-语言数据；全局对比学习的固有局限性难以保持细粒度匹配所需的判别性局部特征，同时容易受到噪声文本标记的影响。&lt;h4&gt;目的&lt;/h4&gt;通过数据整理和模型架构的协同改进，推进CLIP在人物表征学习中的应用。&lt;h4&gt;方法&lt;/h4&gt;1) 开发抗噪声数据构建流程，利用MLLMs自动过滤和标注网络图像，创建WebPerson数据集（500万高质量人物中心化图像-文本对）；2) 提出GA-DMS框架，通过梯度-注意力相似度分数自适应掩码噪声文本标记；3) 结合掩码标记预测目标增强细粒度语义表征学习。&lt;h4&gt;主要发现&lt;/h4&gt;GA-DMS在多个基准测试中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过数据构建和模型架构的改进，成功解决了CLIP在人物表征学习中的两个关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;尽管对比语言-图像预训练（CLIP）在多种视觉任务中表现出色，但其应用于人物表征学习面临两个关键挑战：（i）缺乏大规模标注的人物中心化视觉-语言数据，以及（ii）全局对比学习的固有局限性，难以保持细粒度匹配所需的判别性局部特征，同时容易受到噪声文本标记的影响。本研究通过数据整理和模型架构的协同改进，推进了CLIP在人物表征学习中的应用。首先，我们开发了一种抗噪声数据构建流程，利用MLLMs的上下文学习能力自动过滤和标注网络图像，创建了WebPerson数据集，包含500万高质量人物中心化图像-文本对。其次，我们提出了GA-DMS（梯度-注意力引导的双掩码协同）框架，通过基于梯度-注意力相似度分数自适应地掩码噪声文本标记来改进跨模态对齐。此外，我们结合了掩码标记预测目标，强制模型预测信息丰富的文本标记，增强细粒度语义表征学习。大量实验表明，GA-DMS在多个基准测试中取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Contrastive Language-Image Pre-training (CLIP) exhibits strongperformance across diverse vision tasks, its application to personrepresentation learning faces two critical challenges: (i) the scarcity oflarge-scale annotated vision-language data focused on person-centric images,and (ii) the inherent limitations of global contrastive learning, whichstruggles to maintain discriminative local features crucial for fine-grainedmatching while remaining vulnerable to noisy text tokens. This work advancesCLIP for person representation learning through synergistic improvements indata curation and model architecture. First, we develop a noise-resistant dataconstruction pipeline that leverages the in-context learning capabilities ofMLLMs to automatically filter and caption web-sourced images. This yieldsWebPerson, a large-scale dataset of 5M high-quality person-centric image-textpairs. Second, we introduce the GA-DMS (Gradient-Attention Guided Dual-MaskingSynergetic) framework, which improves cross-modal alignment by adaptivelymasking noisy textual tokens based on the gradient-attention similarity score.Additionally, we incorporate masked token prediction objectives that compel themodel to predict informative text tokens, enhancing fine-grained semanticrepresentation learning. Extensive experiments show that GA-DMS achievesstate-of-the-art performance across multiple benchmarks.</description>
      <author>example@mail.com (Tianlu Zheng, Yifan Zhang, Xiang An, Ziyong Feng, Kaicheng Yang, Qichuan Ding)</author>
      <guid isPermaLink="false">2509.09118v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>PromptGuard: An Orchestrated Prompting Framework for Principled Synthetic Text Generation for Vulnerable Populations using LLMs with Enhanced Safety, Fairness, and Controllability</title>
      <link>http://arxiv.org/abs/2509.08910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PromptGuard，一个新颖的模块化提示框架，特别是其中的VulnGuard Prompt技术，旨在主动防止大型语言模型生成有害信息，保护弱势群体。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在现实世界应用中的激增给LGBTQ+个人、单亲父母和边缘化社区等弱势群体带来了生成有害、有偏见或误导性信息的风险。现有安全方法依赖事后过滤或通用对齐技术，无法在生成源头主动防止有害输出。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够主动防止有害信息生成的框架，特别是在生成源头阻止有害内容，保护弱势群体免受潜在伤害。&lt;h4&gt;方法&lt;/h4&gt;PromptGuard框架包含六个核心模块：输入分类、VulnGuard提示、道德原则集成、外部工具交互、输出验证和用户系统交互。VulnGuard Prompt是一种混合技术，整合了来自精选GitHub存储库的少样本示例、道德思维链推理和自适应角色提示，使用真实世界数据驱动的对比学习来防止有害信息生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论多目标优化和形式化证明，实现了25-30%的分析性危害减少。框架能够为特定人群创建保护屏障，并提供完整的数学形式化，包括收敛证明、使用信息论的脆弱性分析以及使用GitHub源数据集的理论验证框架。&lt;h4&gt;结论&lt;/h4&gt;PromptGuard建立了一个实时危害预防的智能专家系统，为系统实证研究建立了数学基础，能够主动防止大型语言模型生成有害内容。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在现实世界应用中的激增给LGBTQ+个人、单亲父母和边缘化社区等弱势群体带来了前所未有的风险，可能导致生成有害、有偏见或误导性信息。虽然现有的安全方法依赖于事后过滤或通用对齐技术，但它们无法在生成源头主动防止有害输出。本文介绍了PromptGuard，一个新颖的模块化提示框架，以及我们的突破性贡献：VulnGuard Prompt，这是一种使用真实世界数据驱动的对比学习来防止有害信息生成的混合技术。VulnGuard整合了来自精选GitHub存储库的少样本示例、道德思维链推理和自适应角色提示，创建特定人群的保护屏障。我们的框架采用理论多目标优化，并通过形式化证明展示了通过熵边界和帕累托最优性实现25-30%的分析性危害减少。PromptGuard协调六个核心模块：输入分类、VulnGuard提示、道德原则集成、外部工具交互、输出验证和用户系统交互，创建了一个实时危害预防的智能专家系统。我们提供了完整的数学形式化，包括收敛证明、使用信息论的脆弱性分析以及使用GitHub源数据集的理论验证框架，为系统实证研究建立了数学基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Large Language Models (LLMs) in real-world applicationsposes unprecedented risks of generating harmful, biased, or misleadinginformation to vulnerable populations including LGBTQ+ individuals, singleparents, and marginalized communities. While existing safety approaches rely onpost-hoc filtering or generic alignment techniques, they fail to proactivelyprevent harmful outputs at the generation source. This paper introducesPromptGuard, a novel modular prompting framework with our breakthroughcontribution: VulnGuard Prompt, a hybrid technique that prevents harmfulinformation generation using real-world data-driven contrastive learning.VulnGuard integrates few-shot examples from curated GitHub repositories,ethical chain-of-thought reasoning, and adaptive role-prompting to createpopulation-specific protective barriers. Our framework employs theoreticalmulti-objective optimization with formal proofs demonstrating 25-30% analyticalharm reduction through entropy bounds and Pareto optimality. PromptGuardorchestrates six core modules: Input Classification, VulnGuard Prompting,Ethical Principles Integration, External Tool Interaction, Output Validation,and User-System Interaction, creating an intelligent expert system forreal-time harm prevention. We provide comprehensive mathematical formalizationincluding convergence proofs, vulnerability analysis using information theory,and theoretical validation framework using GitHub-sourced datasets,establishing mathematical foundations for systematic empirical research.</description>
      <author>example@mail.com (Tung Vu, Lam Nguyen, Quynh Dao)</author>
      <guid isPermaLink="false">2509.08910v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为CAME-AB的新型跨模态注意力框架，基于专家混合（MoE）主干架构，用于稳健的抗体结合位点预测。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB整合了五种生物学基础模态：原始氨基酸编码、BLOSUM替换特征、预训练语言模型嵌入、结构感知特征和GCN精化的生化图。提出自适应模态融合模块，根据全局相关性和输入特定贡献动态加权每种模态。使用Transformer编码器结合MoE模块促进特征专业化和容量扩展。加入监督对比学习目标，塑造潜在空间几何结构，并应用随机权重平均提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究验证了每个架构组件的有效性和多模态特征集成的益处。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过整合多种生物模态和创新的架构设计，显著提高了抗体结合位点预测的性能。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出CAME-AB，一种基于专家混合（MoE）主干架构的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB整合了五种生物学基础模态，包括原始氨基酸编码、BLOSUM替换特征、预训练语言模型嵌入、结构感知特征和GCN精化的生化图，形成统一的多模态表示。为增强自适应跨模态推理，我们提出自适应模态融合模块，学习根据全局相关性和输入特定贡献动态加权每种模态。Transformer编码器结合MoE模块进一步促进特征专业化和容量扩展。我们还加入监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分离性。为提高优化稳定性和泛化能力，我们在训练过程中应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的益处。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Hangjun Che, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v4</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>AEOS: Active Environment-aware Optimal Scanning Control for UAV LiDAR-Inertial Odometry in Complex Scenes</title>
      <link>http://arxiv.org/abs/2509.09141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AEOS是一种受猫头鹰主动感知行为启发的生物启发框架，通过融合模型预测控制和强化学习，实现了无人机激光雷达的自适应扫描，显著提高了复杂环境中的里程计精度，同时保持实时性能。&lt;h4&gt;背景&lt;/h4&gt;小型激光雷达传感器视场角有限，无人机有效载荷限制排除了多传感器配置的可能性。传统电机扫描系统以固定速度旋转，缺乏场景感知和任务级适应性，导致在复杂、遮挡环境中里程计和地图绘制性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为AEOS（Active Environment-aware Optimal Scanning）的生物启发框架，用于无人机激光雷达-惯性里程计（LIO）的自适应激光雷达控制。&lt;h4&gt;方法&lt;/h4&gt;AEOS采用混合架构，结合模型预测控制（MPC）和强化学习（RL）。分析不确定性模型预测未来姿态可观测性以进行利用，轻量级神经网络从全景深度表示中学习隐式成本图以指导探索。开发基于点云的仿真环境，支持可扩展训练和泛化，并使用真实世界激光雷达地图实现仿真到现实的迁移。&lt;h4&gt;主要发现&lt;/h4&gt;在仿真和真实环境中的大量实验表明，AEOS显著提高了里程计准确性，与固定速率、仅优化和完全学习的基线相比表现更好，同时在机载计算约束下保持实时性能。&lt;h4&gt;结论&lt;/h4&gt;AEOS是一种生物启发且计算高效的框架，有效解决了无人机激光雷达3D感知和定位的局限性，显著提升了复杂环境中的感知性能。&lt;h4&gt;翻译&lt;/h4&gt;基于激光雷达的无人机3D感知和定位受到小型激光雷达传感器窄视场角和有效载荷限制（这些限制排除了多传感器配置）的根本性限制。传统固定速度旋转的电机扫描系统缺乏场景感知和任务级适应性，导致在复杂、遮挡环境中里程计和地图绘制性能下降。受猫头鹰主动感知行为的启发，我们提出了AEOS（Active Environment-aware Optimal Scanning），这是一种生物启发且计算高效的框架，用于无人机激光雷达-惯性里程计（LIO）的自适应激光雷达控制。AEOS在混合架构中结合了模型预测控制（MPC）和强化学习（RL）：分析不确定性模型预测未来姿态可观测性以进行利用，而轻量级神经网络从全景深度表示中学习隐式成本图以指导探索。为了支持可扩展训练和泛化，我们开发了一个基于点云的仿真环境，包含不同场景的真实世界激光雷达地图，实现了仿真到现实的迁移。在仿真和真实环境中的大量实验表明，与固定速率、仅优化和完全学习的基线相比，AEOS显著提高了里程计准确性，同时在机载计算约束下保持实时性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无人机在复杂场景中使用激光雷达-惯性里程计(LIO)时面临的两个关键限制：紧凑型激光雷达传感器固有的窄视场(FoV)以及无人机有效载荷限制排除了多传感器配置的可能性。这个问题在现实世界中非常重要，因为在地下基础设施、建筑工地或森林等非结构化和遮挡环境中，自主3D感知对于机器人在检查、测绘、监控和搜索救援等应用中至关重要。激光雷达作为这些复杂场景中高精度3D感知的可靠选择，但在无人机平台上部署时面临上述限制，导致定位和建图性能严重下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从猫头鹰的主动感知行为中获得灵感，猫头鹰不断重新定向头部以在物理约束下最大化视觉感知。作者分析了现有工作的局限性：早期电机扫描系统缺乏场景适应性；优化方法需要手工设计的成本函数且对局部最小值敏感；纯学习策略计算密集且难以在资源受限的无人机上部署。作者设计了一个混合RL-MPC框架，融合显式和隐式建模以平衡利用和探索：使用分析模型预测位姿不确定性指导MPC优化控制，同时使用轻量级神经网络从点云中学习成本图促进探索。作者借鉴了生物启发感知、模型预测控制和强化学习等方法，但将它们结合在一个专门针对无人机平台优化的混合框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是受猫头鹰主动感知行为启发，提出一种生物启发且计算高效的框架，用于无人机激光雷达-惯性里程计中的自适应激光雷达控制。该方法融合了显式和隐式建模，以平衡主动SLAM中的利用(将感知资源集中在特征丰富区域)和探索(获取观察不足区域信息)两个竞争性目标。整体实现流程包括：1)定义系统状态为激光雷达扫描角度，控制输入为角速度；2)接收包含局部点云和位姿不确定性的观察；3)将点云投影为全景深度图；4)使用差分混合RL-MPC架构：分析不确定性模型指导利用，神经网络成本图促进探索；5)在基于点云的模拟环境中训练和验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)生物启发的紧凑型电机激光雷达控制策略，使无人机具有全景和自适应扫描能力；2)混合RL-MPC框架，融合显式和隐式建模平衡利用和探索，保留可解释性的同时利用学习能力；3)高保真基于点云的强化学习模拟环境，支持物理一致的无人机动力学和多样化场景；4)在模拟和真实世界环境中的全面实验验证。相比之前的工作，AEOS不是简单的固定速度扫描、纯优化或纯学习方法，而是结合了生物启发的感知机制，平衡了计算效率和准确性，专门针对无人机平台约束进行了优化，提供了可解释的决策过程同时保持了学习能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AEOS提出了一种受猫头鹰启发的混合强化学习-模型预测控制框架，通过动态调整激光雷达扫描方向和速度，显著提高了无人机在复杂场景中的激光雷达-惯性里程计定位和建图性能，同时保持了实时计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based 3D perception and localization on unmanned aerial vehicles (UAVs)are fundamentally limited by the narrow field of view (FoV) of compact LiDARsensors and the payload constraints that preclude multi-sensor configurations.Traditional motorized scanning systems with fixed-speed rotations lack sceneawareness and task-level adaptability, leading to degraded odometry and mappingperformance in complex, occluded environments. Inspired by the active sensingbehavior of owls, we propose AEOS (Active Environment-aware Optimal Scanning),a biologically inspired and computationally efficient framework for adaptiveLiDAR control in UAV-based LiDAR-Inertial Odometry (LIO). AEOS combines modelpredictive control (MPC) and reinforcement learning (RL) in a hybridarchitecture: an analytical uncertainty model predicts future poseobservability for exploitation, while a lightweight neural network learns animplicit cost map from panoramic depth representations to guide exploration. Tosupport scalable training and generalization, we develop a point cloud-basedsimulation environment with real-world LiDAR maps across diverse scenes,enabling sim-to-real transfer. Extensive experiments in both simulation andreal-world environments demonstrate that AEOS significantly improves odometryaccuracy compared to fixed-rate, optimization-only, and fully learnedbaselines, while maintaining real-time performance under onboard computationalconstraints. The project page can be found athttps://kafeiyin00.github.io/AEOS/.</description>
      <author>example@mail.com (Jianping Li, Xinhang Xu, Zhongyuan Liu, Shenghai Yuan, Muqing Cao, Lihua Xie)</author>
      <guid isPermaLink="false">2509.09141v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>iMatcher: Improve matching in point cloud registration via local-to-global geometric consistency learning</title>
      <link>http://arxiv.org/abs/2509.08982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了iMatcher，一个用于点云配准中特征匹配的完全可微分框架。该方法通过学习特征预测几何一致性置信矩阵，结合局部和全局一致性，显著提高了刚性配准性能，在多个数据集上达到了最先进的内点比率。&lt;h4&gt;背景&lt;/h4&gt;点云配准是计算机视觉和三维重建中的关键任务，特征匹配是其中的重要环节，需要提高配准的准确性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一个完全可微分的框架，用于点云配准中的特征匹配，通过结合局部和全局一致性来提高配准性能。&lt;h4&gt;方法&lt;/h4&gt;iMatcher方法包括：1) 局部图嵌入模块初始化分数矩阵；2) 重新定位步骤通过双向最近邻搜索优化矩阵；3) 通过全局几何一致性学习预测点对点匹配概率。该方法利用学习到的特征同时考虑局部和全局一致性。&lt;h4&gt;主要发现&lt;/h4&gt;iMatcher在多个数据集上实现了最先进的内点比率：在KITTI上达到95%-97%，在KITTI-360上达到94%-97%，在3DMatch上最高达到81.1%。&lt;h4&gt;结论&lt;/h4&gt;iMatcher是一个有效的点云配准特征匹配框架，通过结合局部和全局一致性学习，在各种场景下都表现出色，具有很好的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了iMatcher，一个用于点云配准中特征匹配的完全可微分框架。该方法利用学习到的特征预测几何一致性置信矩阵，同时考虑局部和全局一致性。首先，局部图嵌入模块导致分数矩阵的初始化。随后的重新定位步骤通过考虑双向源到目标和目标到源匹配，通过在3D空间中的最近邻搜索来优化该矩阵。然后，配对的点特征被堆叠在一起，通过全局几何一致性学习进行优化，以预测点对点匹配概率。在真实世界的户外(KITTI, KITTI-360)和室内(3DMatch)数据集上，以及在6-DoF姿态估计(TUD-L)和部分到部分匹配(MVP-RG)上的大量实验表明，iMatcher显著提高了刚性配准性能。该方法在数据集上实现了最先进的内点比率，在KITTI上得分为95%-97%，在KITTI-360上得分为94%-97%，在3DMatch上最高达到81.1%，突显了其在各种设置下的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准(point cloud registration)中的特征匹配问题，提高匹配的准确性和鲁棒性。这个问题在现实中非常重要，因为点云配准是自动驾驶、机器人抓取、3D重建、医学图像对齐等多个领域的关键技术，帮助计算机理解不同视角下的3D场景。然而，点云的固有无序性、实际数据获取中的遮挡和传感器噪声等因素使得精确匹配变得困难，限制了这些应用的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考了现有方法的局限性：传统优化方法如ICP依赖初始化且对噪声敏感；基于深度学习的方法中Transformer计算复杂度高；Sinkhorn算法虽广泛使用但依赖迭代优化。作者设计iMatcher时借鉴了多项现有工作：受LightGlue启发估计点的可匹配性分数；使用图卷积网络(Graph CNN)捕获局部拓扑关系；采用一阶空间兼容性(FOSC)评估全局一致性；使用可微加权SVD进行预对齐。作者的核心思路是结合局部和全局几何一致性，创建一个支持渐进式特征精化的全可微分框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合局部图结构和全局几何一致性来提高点云匹配的准确性。整体流程分为四个步骤：1) 使用图卷积网络构建局部图，提取每个点的邻域特征，计算初始分数矩阵；2) 通过可微SVD进行预对齐，将源点云变换到目标空间，并通过最近邻搜索建立双向匹配；3) 利用一阶空间兼容性(FOSC)措施评估全局一致性，计算每个点的可匹配性分数；4) 融合局部和全局信息，通过可匹配性矩阵调整初始分数矩阵，生成最终的高质量匹配结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 局部到全局的几何一致性学习，同时考虑点云的局部结构和全局空间关系；2) 可微分的重新定位步骤，通过SVD预对齐和最近邻搜索提高匹配精度；3) 基于FOSC的可匹配性评估，学习温度参数提高对遮挡和噪声的鲁棒性；4) 高效的架构设计，避免Transformer的高计算复杂度。相比之前的工作，iMatcher不依赖迭代优化过程，提供了更好的可解释性和计算效率；同时考虑局部和全局一致性，引入可匹配性评估机制，在各种场景下实现了更高的内点比率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; iMatcher通过结合局部图卷积和全局几何一致性学习，提出了一种高效、可微的点云匹配框架，显著提高了点云配准的内点比率和鲁棒性，在各种场景下实现了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents iMatcher, a fully differentiable framework for featurematching in point cloud registration. The proposed method leverages learnedfeatures to predict a geometrically consistent confidence matrix, incorporatingboth local and global consistency. First, a local graph embedding module leadsto an initialization of the score matrix. A subsequent repositioning steprefines this matrix by considering bilateral source-to-target andtarget-to-source matching via nearest neighbor search in 3D space. The pairedpoint features are then stacked together to be refined through global geometricconsistency learning to predict a point-wise matching probability. Extensiveexperiments on real-world outdoor (KITTI, KITTI-360) and indoor (3DMatch)datasets, as well as on 6-DoF pose estimation (TUD-L) and partial-to-partialmatching (MVP-RG), demonstrate that iMatcher significantly improves rigidregistration performance. The method achieves state-of-the-art inlier ratios,scoring 95% - 97% on KITTI, 94% - 97% on KITTI-360, and up to 81.1% on 3DMatch,highlighting its robustness across diverse settings.</description>
      <author>example@mail.com (Karim Slimani, Catherine Achard, Brahim Tamadazte)</author>
      <guid isPermaLink="false">2509.08982v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>3D and 4D World Modeling: A Survey</title>
      <link>http://arxiv.org/abs/2509.07996v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at  https://github.com/worldbench/survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了3D和4D世界建模与生成领域的研究，解决了该领域缺乏标准化定义和分类法的问题，建立了精确的定义和结构化分类法，总结了相关数据集和评估指标，并讨论了实际应用、开放挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;世界建模已成为AI研究的基石，使代理能够理解、表示和预测动态环境。然而，先前工作主要关注2D图像和视频数据的生成方法，忽略了利用3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的研究。&lt;h4&gt;目的&lt;/h4&gt;通过首次专门针对3D和4D世界建模和生成的全面综述，解决该领域缺乏标准化定义和分类法的问题，提供连贯且基础的参考。&lt;h4&gt;方法&lt;/h4&gt;建立精确的定义，引入涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，系统地总结针对3D/4D设置的数据集和评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了3D/4D世界建模的实际应用，确定了开放性挑战，指出了有前景的研究方向。&lt;h4&gt;结论&lt;/h4&gt;为推进3D和4D世界建模领域提供连贯且基础的参考，相关文献总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;翻译&lt;/h4&gt;世界建模已成为AI研究的基石，使代理能够理解、表示和预测它们所居住的动态环境。虽然先前的工作主要强调2D图像和视频数据的生成方法，但它们忽略了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的快速增长的研究。同时，缺乏对'世界模型'的标准化定义和分类法，导致文献中的观点碎片化且有时不一致。本综述通过首次专门针对3D和4D世界建模和生成的全面综述来解决这些差距。我们建立了精确的定义，引入了涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，并系统地总结了针对3D/4D设置的数据集和评估指标。我们进一步讨论了实际应用，确定了开放性挑战，并指出了有前景的研究方向，旨在为推进该领域提供连贯且基础的参考。现有文献的系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决世界建模领域缺乏标准化定义和分类体系的问题，以及现有研究过度关注2D图像和视频而忽视原生3D和4D表示方法的问题。这个问题很重要，因为世界建模是AI研究的基础，使智能体能理解、表示和预测动态环境；同时，原生3D/4D表示方法在自动驾驶、机器人和安全关键系统中至关重要，它们提供明确的几何和物理基础，对构建可靠的世界模型非常必要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这篇论文是一篇综述文章，作者通过识别世界建模领域的碎片化状态和缺乏统一框架的问题，首先建立了精确的'世界模型'和'3D/4D世界建模'定义，然后提出一个分层的分类方法，基于表示模态（视频生成、占用生成和LiDAR生成）和功能类型（数据引擎、动作解释器、神经模拟器和场景重建器）进行组织。作者借鉴了大量现有工作，通过引用和总结这些工作来构建他们的分类框架和评估体系，为3D/4D世界建模领域提供了系统化的概述。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文的核心思想是建立统一的分类框架，将3D/4D世界建模方法按照表示模态和功能类型进行组织，强调原生3D/4D表示方法的重要性，并区分条件输入和功能输出。作为综述论文，它没有具体的实现流程，而是系统地组织现有工作，介绍3D/4D世界建模的基本概念、方法分类、数据集和评估指标，为研究社区提供清晰的概念框架和参考方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对3D和4D世界建模进行全面综述；建立精确的'世界模型'和'3D/4D世界建模'定义；提出基于表示模态和功能类型的分层分类方法；系统总结专门针对3D/4D场景的数据集和评估协议；强调原生3D/4D表示方法的重要性。相比之前的工作，这篇论文专门聚焦于3D/4D表示方法，而大多数现有综述集中在2D或仅视觉模态上；同时，其分类框架更细致，区分了条件输入和功能输出，为复杂的研究领域提供了更清晰的视角。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统综述了3D和4D世界建模领域，建立了统一的定义和分类框架，为理解和推进这一快速发展的重要研究方向提供了基础性参考。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World modeling has become a cornerstone in AI research, enabling agents tounderstand, represent, and predict the dynamic environments they inhabit. Whileprior work largely emphasizes generative methods for 2D image and video data,they overlook the rapidly growing body of work that leverages native 3D and 4Drepresentations such as RGB-D imagery, occupancy grids, and LiDAR point cloudsfor large-scale scene modeling. At the same time, the absence of a standardizeddefinition and taxonomy for ``world models'' has led to fragmented andsometimes inconsistent claims in the literature. This survey addresses thesegaps by presenting the first comprehensive review explicitly dedicated to 3Dand 4D world modeling and generation. We establish precise definitions,introduce a structured taxonomy spanning video-based (VideoGen),occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, andsystematically summarize datasets and evaluation metrics tailored to 3D/4Dsettings. We further discuss practical applications, identify open challenges,and highlight promising research directions, aiming to provide a coherent andfoundational reference for advancing the field. A systematic summary ofexisting literature is available at https://github.com/worldbench/survey</description>
      <author>example@mail.com (Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu)</author>
      <guid isPermaLink="false">2509.07996v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Concentration for Self-Supervised Dense Representations Learning</title>
      <link>http://arxiv.org/abs/2509.09429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了图像级自监督学习在密集表示学习方面的挑战，提出了一种解决图像块过度分散现象的方法，通过显式语义集中来改善密集任务性能。&lt;h4&gt;背景&lt;/h4&gt;图像级自监督学习已取得显著进展，但学习图像块的密集表示仍然具有挑战性。主流方法存在过度分散现象，即来自同一实例/类别的图像块分散，影响密集任务性能。&lt;h4&gt;目的&lt;/h4&gt;探索显式语义集中方法来解决密集自监督学习中的过度分散问题，提高下游密集任务性能。&lt;h4&gt;方法&lt;/h4&gt;1) 提出蒸馏块对应关系打破严格空间对齐，并设计噪声容忍的排序损失处理嘈杂不平衡的伪标签；2) 提出基于对象的过滤器，通过交叉注意力将输出空间映射到基于对象的空间，使块由可学习的对象原型表示。&lt;h4&gt;主要发现&lt;/h4&gt;图像级SSL通过隐式语义集中避免过度分散，非严格空间对齐保证实例内一致性，共享模式保证图像间一致性。但这些方法对密集SSL不可行，需要探索显式语义集中方法。&lt;h4&gt;结论&lt;/h4&gt;提出的显式语义集中方法有效解决了密集自监督学习中的过度分散问题，各种任务的经验研究验证了方法的有效性，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;最近的图像级自监督学习(SSL)进展显著，但学习图像块的密集表示仍然具有挑战性。主流方法遇到过度分散现象，即来自同一实例/类别的图像块分散，损害了密集任务的下游性能。这项工作揭示图像级SSL通过隐式语义集中避免过度分散。具体来说，非严格空间对齐确保实例内一致性，而共享模式，即输入空间内同类实例的相似部分，确保图像间一致性。不幸的是，由于这些方法对空间敏感且场景复杂的数据，它们对密集SSL不可行。这些观察促使我们探索密集SSL的显式语义集中。首先，为打破严格空间对齐，我们提出蒸馏块对应关系。面对嘈杂和不平衡的伪标签，我们提出了一种噪声容忍的排序损失。核心思想是将平均精度(AP)损失扩展到连续目标，使其决策无关和自适应聚焦特性防止学生模型被误导。其次，为从复杂场景中区分共享模式，我们提出基于对象的过滤器将输出空间映射到基于对象的空间。具体来说，通过交叉注意力，块由对象的可学习原型表示。最后，各种任务的经验研究有力支持了我们方法的有效性。代码可在https://github.com/KID-7391/CoTAP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in image-level self-supervised learning (SSL) have madesignificant progress, yet learning dense representations for patches remainschallenging. Mainstream methods encounter an over-dispersion phenomenon thatpatches from the same instance/category scatter, harming downstream performanceon dense tasks. This work reveals that image-level SSL avoids over-dispersionby involving implicit semantic concentration. Specifically, the non-strictspatial alignment ensures intra-instance consistency, while shared patterns,i.e., similar parts of within-class instances in the input space, ensureinter-image consistency. Unfortunately, these approaches are infeasible fordense SSL due to their spatial sensitivity and complicated scene-centric data.These observations motivate us to explore explicit semantic concentration fordense SSL. First, to break the strict spatial alignment, we propose to distillthe patch correspondences. Facing noisy and imbalanced pseudo labels, wepropose a noise-tolerant ranking loss. The core idea is extending the AveragePrecision (AP) loss to continuous targets, such that its decision-agnostic andadaptive focusing properties prevent the student model from being misled.Second, to discriminate the shared patterns from complicated scenes, we proposethe object-aware filter to map the output space to an object-based space.Specifically, patches are represented by learnable prototypes of objects viacross-attention. Last but not least, empirical studies across various taskssoundly support the effectiveness of our method. Code is available inhttps://github.com/KID-7391/CoTAP.</description>
      <author>example@mail.com (Peisong Wen, Qianqian Xu, Siran Dai, Runmin Cong, Qingming Huang)</author>
      <guid isPermaLink="false">2509.09429v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>MoSE: Unveiling Structural Patterns in Graphs via Mixture of Subgraph Experts</title>
      <link>http://arxiv.org/abs/2509.09337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个新颖的子图专家混合(MoSE)框架，用于在不同图任务中进行灵活且具有表达力的子图表示学习，解决了传统图神经网络在捕捉复杂高阶子图模式方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在处理图结构数据方面取得了很大成功，但它们依赖于局部的、成对的消息传递，限制了捕捉复杂高阶子图模式的能力，导致结构表达能力不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提出一个能够在不同图任务中进行灵活且具有表达力的子图表示学习的框架。&lt;h4&gt;方法&lt;/h4&gt;MoSE通过匿名游走提取信息子图，并根据结构语义将它们动态路由到专门的专家，使模型能够以改进的灵活性和可解释性捕捉多样化的子图模式。&lt;h4&gt;主要发现&lt;/h4&gt;MoSE在子图Weisfeiler-Lehman测试中的理论分析证明其比SWL更强大；实验和可视化表明MoSE优于竞争性基线，并为模型学习的结构模式提供了可解释的见解。&lt;h4&gt;结论&lt;/h4&gt;MoSE框架能够有效捕捉复杂的高阶子图模式，在不同图任务上表现优异，并且提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;虽然图神经网络(GNNs)在从图结构数据中学习方面取得了巨大成功，但它们对局部、成对消息传递的依赖限制了它们捕捉复杂高阶子图模式的能力，导致结构表达能力不足。最近的工作尝试通过将随机游走核集成到GNNs中来增强结构表达能力。然而，这些方法本质上是针对图级任务设计的，限制了它们在其他下游任务（如节点分类）中的应用。此外，它们的固定核配置阻碍了模型捕捉多样化子图结构的灵活性。为了解决这些限制，本文提出了一个新颖的子图专家混合(MoSE)框架，用于在不同图任务中进行灵活且具有表达力的子图表示学习。具体来说，MoSE通过匿名游走提取信息子图，并根据结构语义将它们动态路由到专门的专家，使模型能够以改进的灵活性和可解释性捕捉多样化的子图模式。我们进一步提供了MoSE在子图Weisfeiler-Lehman测试中的表达能力理论分析，证明它比SWL更强大。广泛的实验以及学习到的子图专家的可视化表明，MoSE不仅优于竞争性基线，还为模型学习的结构模式提供了可解释的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While graph neural networks (GNNs) have achieved great success in learningfrom graph-structured data, their reliance on local, pairwise message passingrestricts their ability to capture complex, high-order subgraph patterns.leading to insufficient structural expressiveness. Recent efforts haveattempted to enhance structural expressiveness by integrating random walkkernels into GNNs. However, these methods are inherently designed forgraph-level tasks, which limits their applicability to other downstream taskssuch as node classification. Moreover, their fixed kernel configurations hinderthe model's flexibility in capturing diverse subgraph structures. To addressthese limitations, this paper proposes a novel Mixture of Subgraph Experts(MoSE) framework for flexible and expressive subgraph-based representationlearning across diverse graph tasks. Specifically, MoSE extracts informativesubgraphs via anonymous walks and dynamically routes them to specializedexperts based on structural semantics, enabling the model to capture diversesubgraph patterns with improved flexibility and interpretability. We furtherprovide a theoretical analysis of MoSE's expressivity within the SubgraphWeisfeiler-Lehman (SWL) Test, proving that it is more powerful than SWL.Extensive experiments, together with visualizations of learned subgraphexperts, demonstrate that MoSE not only outperforms competitive baselines butalso provides interpretable insights into structural patterns learned by themodel.</description>
      <author>example@mail.com (Junda Ye, Zhongbao Zhang, Li Sun, Siqiang Luo)</author>
      <guid isPermaLink="false">2509.09337v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing 3D Medical Image Understanding with Pretraining Aided by 2D Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.09064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Journal of Biomedical and Health Informatics (JBHI)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Med3DInsight，一种新颖的预训练框架，通过集成3D图像编码器与2D多模态大语言模型(MLLMs)，解决了3D医学图像理解中缺乏深层次语义理解的问题，无需人工标注即可实现高性能的3D医学图像分割和分类任务。&lt;h4&gt;背景&lt;/h4&gt;理解3D医学图像体积在医学领域至关重要，但现有的基于3D医学卷积和Transformer的自监督学习方法往往缺乏深层次语义理解，而多模态大语言模型为增强图像理解提供了有前景的途径。&lt;h4&gt;目的&lt;/h4&gt;利用2D多模态大语言模型(MLLMs)来增强3D医学图像理解，开发一种无需人工标注的可扩展多模态3D医学表征学习新范式。&lt;h4&gt;方法&lt;/h4&gt;提出了Med3DInsight框架，通过专门设计的平面切片感知transformer模块将3D图像编码器与2D MLLMs集成，并采用基于部分最优传输的对齐方法，对LLM生成内容中可能引入的噪声具有更好的容忍度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公共数据集上的大量实验表明，Med3DInsight在分割和分类两个下游任务上展示了最先进的性能，优于当前的SSL方法，且可以无缝集成到现有的3D医学图像理解网络中。&lt;h4&gt;结论&lt;/h4&gt;Med3DInsight为可扩展的多模态3D医学表征学习引入了新范式，无需人工标注即可实现高性能，并且可以无缝集成到现有的3D医学图像理解网络中，增强它们的性能。相关源代码、生成的数据集和预训练模型将在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;理解3D医学图像体积在医学领域至关重要，然而现有的3D医学卷积和基于Transformer的自监督学习方法往往缺乏深层次的语义理解。多模态大语言模型(MLLMs)的最新进展提供了一种通过文本描述增强图像理解的有前景的方法。为了利用这些2D MLLMs来改进3D医学图像理解，我们提出了Med3DInsight，一种新颖的预训练框架，通过专门设计的平面切片感知transformer模块将3D图像编码器与2D MLLMs集成。此外，我们的模型采用基于部分最优传输的对齐方法，展示了对于LLM生成内容中可能引入的噪声具有更好的容忍度。Med3DInsight为可扩展的多模态3D医学表征学习引入了一种无需人工标注的新范式。大量实验证明了我们在两个下游任务，即分割和分类上的最先进性能，在各种使用CT和MRI模态的公共数据集上，超越了当前的SSL方法。Med3DInsight可以无缝集成到现有的3D医学图像理解网络中，潜在地增强它们的性能。我们的源代码、生成的数据集和预训练模型将在https://github.com/Qybc/Med3DInsight上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D医学图像理解缺乏深层次语义理解的问题。现有3D医学卷积和transformer自学习方法主要关注低级别图像理解（像素或块级别），无法捕获高级语义特征，限制了模型充分利用3D医学扫描中复杂信息的能力。这个问题很重要，因为3D医学图像（如MRI、CT）包含比2D图像更全面的信息，对诊断、治疗规划和医疗研究至关重要，而缺乏语义理解会阻碍医疗AI系统的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到2D多模态大语言模型(MLLMs)在图像理解方面的强大能力，但直接应用于3D医学图像存在挑战：缺乏大规模3D医学图像-文本配对数据集，以及2D MLLMs与3D医学图像理解需求之间的差距。作者借鉴了自监督学习(SSL)的思想、多模态大语言模型(如CLIP、GPT-4V)的架构、最优传输(OT)理论，以及类似MAE的重建策略。通过设计PSAT模块连接3D和2D特征空间，并采用POT技术处理MLLMs可能产生的噪声，作者构建了一个同时增强高级语义和低级细节理解的新框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D MLLMs的语义理解能力增强3D医学图像理解，通过特殊设计的PSAT模块集成3D图像编码器和2D MLLMs，结合重建技术和图像-文本对齐，同时增强高级语义和低级细节理解，并使用POT技术处理噪声。整体流程包括：1)使用GPT-4V为3D体积的2D切片生成文本描述创建三元组数据集；2)提取3D体积、2D切片和文本的特征表示；3)通过PSAT模块将3D特征投影到2D图像和文本嵌入空间；4)使用POT对齐3D体积特征与2D图像和文本特征；5)通过重建3D体积增强低级视觉语义学习；6)结合对齐损失和重建损失训练模型；7)在下游任务上应用预训练的3D编码器。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出Med3DInsight框架，利用2D MLLMs和重建技术增强3D图像编码器的高级和低级理解能力；2)设计平面切片感知Transformer(PSAT)模块连接3D和2D特征空间，考虑视觉特征的空间方向；3)提出部分最优传输(POT)对齐技术，减少模态间差异，提高对MLLMs噪声的容忍度。相比之前工作，本文无需大规模3D医学图像-文本配对数据集，同时兼顾高级语义和低级细节理解，使用POT而非对比学习进行模态对齐，在多个数据集的分割和分类任务上超越了现有SSL方法，并在数据有限情况下表现出更好的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Med3DInsight框架，通过结合2D多模态大语言模型和重建技术，显著提升了3D医学图像的理解能力，无需人工标注即可在分割和分类任务上达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D medical image volumes is critical in the medical field, yetexisting 3D medical convolution and transformer-based self-supervised learning(SSL) methods often lack deep semantic comprehension. Recent advancements inmultimodal large language models (MLLMs) provide a promising approach toenhance image understanding through text descriptions. To leverage these 2DMLLMs for improved 3D medical image understanding, we propose Med3DInsight, anovel pretraining framework that integrates 3D image encoders with 2D MLLMs viaa specially designed plane-slice-aware transformer module. Additionally, ourmodel employs a partial optimal transport based alignment, demonstratinggreater tolerance to noise introduced by potential noises in LLM-generatedcontent. Med3DInsight introduces a new paradigm for scalable multimodal 3Dmedical representation learning without requiring human annotations. Extensiveexperiments demonstrate our state-of-the-art performance on two downstreamtasks, i.e., segmentation and classification, across various public datasetswith CT and MRI modalities, outperforming current SSL methods. Med3DInsight canbe seamlessly integrated into existing 3D medical image understanding networks,potentially enhancing their performance. Our source code, generated datasets,and pre-trained models will be available athttps://github.com/Qybc/Med3DInsight.</description>
      <author>example@mail.com (Qiuhui Chen, Xuancheng Yao, Huping Ye, Yi Hong)</author>
      <guid isPermaLink="false">2509.09064v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Divergent Representations between Text-to-Image Models</title>
      <link>http://arxiv.org/abs/2509.08940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICCV 2025. Code available at  https://github.com/adobe-research/CompCon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了两个不同生成模型学习的视觉表示何时以及如何出现差异，提出了CompCon算法来发现不同模型间的视觉属性差异及其触发条件&lt;h4&gt;背景&lt;/h4&gt;现有两个文本到图像模型可能在相同提示下生成不同视觉属性的图像，但缺乏系统方法来发现这些差异&lt;h4&gt;目的&lt;/h4&gt;发现一个模型生成而另一个模型不生成的视觉属性，以及触发这些差异的提示词类型&lt;h4&gt;方法&lt;/h4&gt;提出CompCon进化搜索算法，创建自动化数据生成管道构建ID2数据集（包含60个输入相关差异），并与LLM和VLM基线方法比较&lt;h4&gt;主要发现&lt;/h4&gt;PixArt在描述孤独提示时倾向于展示湿漉街道，Stable Diffusion 3.5在描述非裔美国人媒体职业时表现出特定视觉特征&lt;h4&gt;结论&lt;/h4&gt;CompCon能有效发现不同文本到图像模型间的视觉表示差异，为理解模型行为提供新视角&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们研究两个不同生成模型学习的视觉表示何时以及如何出现差异。给定两个文本到图像模型，我们的目标是发现一个模型生成图像中出现而另一个模型不出现的视觉属性，以及触发这些属性差异的提示词类型。例如，当给定表达强烈情感的提示时，一个模型的输出中可能出现'火焰'，而另一个模型在相同提示下不会产生此属性。我们引入CompCon（比较概念），一种进化搜索算法，发现一个模型输出中比另一个模型更普遍的视觉属性，并揭示与这些视觉差异相关的提示概念。为了评估CompCon发现不同表示的能力，我们创建了一个自动化数据生成管道来生成ID2数据集（包含60个输入相关差异），并将我们的方法与几种基于LLM和VLM的基线方法进行了比较。最后，我们使用CompCon比较流行的文本到图像模型，发现不同的表示，例如PixArt如何描绘提及孤独的提示词与湿漉街道，以及Stable Diffusion 3.5如何描绘非裔美国人在媒体职业中的形象。代码地址：https://github.com/adobe-research/CompCon&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we investigate when and how visual representations learned bytwo different generative models diverge. Given two text-to-image models, ourgoal is to discover visual attributes that appear in images generated by onemodel but not the other, along with the types of prompts that trigger theseattribute differences. For example, "flames" might appear in one model'soutputs when given prompts expressing strong emotions, while the other modeldoes not produce this attribute given the same prompts. We introduce CompCon(Comparing Concepts), an evolutionary search algorithm that discovers visualattributes more prevalent in one model's output than the other, and uncoversthe prompt concepts linked to these visual differences. To evaluate CompCon'sability to find diverging representations, we create an automated datageneration pipeline to produce ID2, a dataset of 60 input-dependentdifferences, and compare our approach to several LLM- and VLM-poweredbaselines. Finally, we use CompCon to compare popular text-to-image models,finding divergent representations such as how PixArt depicts prompts mentioningloneliness with wet streets and Stable Diffusion 3.5 depicts African Americanpeople in media professions. Code at: https://github.com/adobe-research/CompCon</description>
      <author>example@mail.com (Lisa Dunlap, Joseph E. Gonzalez, Trevor Darrell, Fabian Caba Heilbron, Josef Sivic, Bryan Russell)</author>
      <guid isPermaLink="false">2509.08940v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
      <link>http://arxiv.org/abs/2509.08392v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对交通监控图像增强任务的垂直残差自编码器（VRAE）架构，通过在每个编码阶段注入输入感知特征来指导表示学习，相比传统方法在车牌识别的图像预处理中表现更优。&lt;h4&gt;背景&lt;/h4&gt;实际交通监控中，车辆图像常受到恶劣天气、光照不足或高速运动的影响，导致严重的噪声和模糊。这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。&lt;h4&gt;目的&lt;/h4&gt;快速实时恢复这些退化图像，是提高识别性能的关键预处理步骤。设计一种图像增强架构，用于交通监控中的图像增强任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种垂直残差自编码器（Vertical Residual Autoencoder, VRAE）架构。该方法采用了一种增强策略，包含一个辅助块，在每个编码阶段注入输入感知特征，以指导表示学习过程，与自编码器相比能够在整个网络中更好地保留一般信息。&lt;h4&gt;主要发现&lt;/h4&gt;在带有可见车牌的车辆图像数据集上的实验表明，该方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的自编码器相比，PSNR提高了约20%，NMSE减少了约50%，SSIM提高了1%，参数仅需要大约1%的边际增加。&lt;h4&gt;结论&lt;/h4&gt;VRAE架构在交通监控图像增强任务中表现优异，通过辅助块注入输入感知特征的方法有效地改善了图像恢复质量。&lt;h4&gt;翻译&lt;/h4&gt;在实际交通监控中，车辆图像在恶劣天气、光照不足或高速运动条件下捕获时，常常受到严重的噪声和模糊影响。此类退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。因此，快速实时地恢复这些退化图像是提高识别性能的关键预处理步骤。在这项工作中，我们提出了一种专为交通监控图像增强任务设计的垂直残差自编码器（VRAE）架构。该方法采用了一种增强策略，包含一个辅助块，在每个编码阶段注入输入感知特征，以指导表示学习过程，与传统自编码器相比，能够在整个网络中更好地保留一般信息。在有可见车牌的车辆图像数据集上的实验表明，我们的方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流（FB）的方法。与相同深度的自编码器相比，它将PSNR提高了约20%，NMSE减少了约50%，SSIM提高了1%，而参数仅需要大约1%的边际增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world traffic surveillance, vehicle images captured under adverseweather, poor lighting, or high-speed motion often suffer from severe noise andblur. Such degradations significantly reduce the accuracy of license platerecognition systems, especially when the plate occupies only a small regionwithin the full vehicle image. Restoring these degraded images a fast realtimemanner is thus a crucial pre-processing step to enhance recognitionperformance. In this work, we propose a Vertical Residual Autoencoder (VRAE)architecture designed for the image enhancement task in traffic surveillance.The method incorporates an enhancement strategy that employs an auxiliaryblock, which injects input-aware features at each encoding stage to guide therepresentation learning process, enabling better general informationpreservation throughout the network compared to conventional autoencoders.Experiments on a vehicle image dataset with visible license plates demonstratethat our method consistently outperforms Autoencoder (AE), GenerativeAdversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE atthe same depth, it improves PSNR by about 20%, reduces NMSE by around 50%, andenhances SSIM by 1%, while requiring only a marginal increase of roughly 1% inparameters.</description>
      <author>example@mail.com (Cuong Nguyen, Dung T. Tran, Hong Nguyen, Xuan-Vu Phan, Nam-Phong Nguyen)</author>
      <guid isPermaLink="false">2509.08392v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>PeftCD: Leveraging Vision Foundation Models with Parameter-Efficient Fine-Tuning for Remote Sensing Change Detection</title>
      <link>http://arxiv.org/abs/2509.09572v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PeftCD框架，一种基于视觉基础模型(VFMs)和参数高效微调(PEFT)的变化检测方法，解决了多时相多源遥感影像中的伪变化、标记样本稀缺和跨域泛化困难问题。&lt;h4&gt;背景&lt;/h4&gt;多时相多源遥感影像中存在伪变化普遍、标记样本稀缺、跨域泛化困难等挑战，影响了变化检测的准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理伪变化、减少对标记数据依赖并提高跨域泛化能力的变化检测框架。&lt;h4&gt;方法&lt;/h4&gt;构建了PeftCD框架，核心是使用源自VFM的权重共享Siamese编码器，无缝集成LoRA和Adapter模块，只需训练少量额外参数即可实现高效任务适应。研究了SAM2和DINOv3两种骨干网络，并配备轻量级解码器以保留骨干网络的强大特征表示能力。&lt;h4&gt;主要发现&lt;/h4&gt;PeftCD在多个公共数据集上实现了最先进性能：SYSU-CD (IoU 73.81%)、WHUCD (92.05%)、MSRSCD (64.07%)、MLCD (76.89%)、CDD (97.01%)、S2Looking (52.25%)和LEVIR-CD (85.62%)，具有精确的边界描绘能力和对伪变化的强抑制能力。&lt;h4&gt;结论&lt;/h4&gt;PeftCD在准确性、效率和泛化能力方面实现了最佳平衡，为适应大规模VFMs到真实世界遥感变化检测应用提供了强大且可扩展的范式。&lt;h4&gt;翻译&lt;/h4&gt;为了解决多时相和多源遥感影像中伪变化的普遍性、标记样本的稀缺性以及跨域泛化的困难，我们提出了PeftCD，这是一种基于视觉基础模型(VFMs)并采用参数高效微调(PEFT)的变化检测框架。其核心是使用源自VFM的权重共享Siamese编码器，无缝集成了LoRA和Adapter模块。这种设计通过仅训练少量额外参数，实现了高度高效的任务适应。为了充分释放VFMs的潜力，我们研究了两种主流骨干网络：以强大分割先验而闻名的Segment Anything Model v2 (SAM2)，以及最先进的自监督表征学习器DINOv3。该框架配备了精心设计的轻量级解码器，确保重点保持在骨干网络的强大特征表示上。大量实验表明，PeftCD在多个公共数据集上实现了最先进的性能，包括SYSU-CD (IoU 73.81%)、WHUCD (92.05%)、MSRSCD (64.07%)、MLCD (76.89%)、CDD (97.01%)、S2Looking (52.25%)和LEVIR-CD (85.62%)，具有精确的边界描绘能力和对伪变化的强抑制能力。总之，PeftCD在准确性、效率和泛化能力方面实现了最佳平衡。它为将大规模VFMs适应到真实世界遥感变化检测应用中提供了强大且可扩展的范式。代码和预训练模型将在https://github.com/dyzy41/PeftCD发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To tackle the prevalence of pseudo changes, the scarcity of labeled samples,and the difficulty of cross-domain generalization in multi-temporal andmulti-source remote sensing imagery, we propose PeftCD, a change detectionframework built upon Vision Foundation Models (VFMs) with Parameter-EfficientFine-Tuning (PEFT). At its core, PeftCD employs a weight-sharing Siameseencoder derived from a VFM, into which LoRA and Adapter modules are seamlesslyintegrated. This design enables highly efficient task adaptation by trainingonly a minimal set of additional parameters. To fully unlock the potential ofVFMs, we investigate two leading backbones: the Segment Anything Model v2(SAM2), renowned for its strong segmentation priors, and DINOv3, astate-of-the-art self-supervised representation learner. The framework iscomplemented by a deliberately lightweight decoder, ensuring the focus remainson the powerful feature representations from the backbones. Extensiveexperiments demonstrate that PeftCD achieves state-of-the-art performanceacross multiple public datasets, including SYSU-CD (IoU 73.81%), WHUCD(92.05%), MSRSCD (64.07%), MLCD (76.89%), CDD (97.01%), S2Looking (52.25%) andLEVIR-CD (85.62%), with notably precise boundary delineation and strongsuppression of pseudo-changes. In summary, PeftCD presents an optimal balanceof accuracy, efficiency, and generalization. It offers a powerful and scalableparadigm for adapting large-scale VFMs to real-world remote sensing changedetection applications. The code and pretrained models will be released athttps://github.com/dyzy41/PeftCD.</description>
      <author>example@mail.com (Sijun Dong, Yuxuan Hu, LiBo Wang, Geng Chen, Xiaoliang Meng)</author>
      <guid isPermaLink="false">2509.09572v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>DualTrack: Sensorless 3D Ultrasound needs Local and Global Context</title>
      <link>http://arxiv.org/abs/2509.09530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为DualTrack的新型双编码器架构，用于无传感器的三维超声成像，通过分离处理局部和全局特征，实现了高精度和全局一致的三维重建。&lt;h4&gt;背景&lt;/h4&gt;三维超声相比传统二维成像具有临床优势，但传统3D系统的高成本和复杂性限制了其广泛应用。无传感器3D US是一种有前景的替代方案，使用深度学习从2D超声图像序列中估计3D探头轨迹。局部特征帮助预测帧间运动，全局特征帮助扫描定位，但先前的 approaches 无法有效分离处理这两类特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够稳健地建模局部和全局特征的方法，以提高无传感器3D US的准确性，实现高精度和全局一致的三维重建。&lt;h4&gt;方法&lt;/h4&gt;提出DualTrack双编码器架构，利用分离的局部和全局编码器分别处理不同尺度的特征。局部编码器使用密集时空卷积捕获细粒度特征，全局编码器利用图像主干和时序注意力层嵌入高级解剖特征和长程依赖关系，通过轻量级融合模块结合这些特征来估计轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在大型公共基准上的实验结果表明，DualTrack达到了最先进的准确性，实现了全局一致的三维重建，优于先前的方法，平均重建误差低于5毫米。&lt;h4&gt;结论&lt;/h4&gt;DualTrack通过分离处理局部和全局特征，有效解决了无传感器3D US中的关键挑战，在三维超声成像领域具有重要的临床应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;三维超声(US)相比传统二维成像具有许多临床优势，但其广泛应用受到传统3D系统成本和复杂性的限制。无传感器3D US使用深度学习从2D US图像序列中估计3D探头轨迹，是一种有前景的替代方案。局部特征（如斑点模式）可以帮助预测帧间运动，而全局特征（如粗略形状和解剖结构）可以帮助将扫描相对于解剖定位并预测其一般形状。在先前的 approaches 中，全局特征要么被忽略，要么与局部特征提取紧密耦合，限制了稳健地建模这两个互补方面的能力。我们提出DualTrack，一种新颖的双编码器架构，利用分离的局部和全局编码器，专门用于各自尺度的特征提取。局部编码器使用密集时空卷积来捕获细粒度特征，而全局编码器利用图像主干（如2D CNN或基础模型）和时序注意力层来嵌入高级解剖特征和长程依赖关系。然后，轻量级融合模块结合这些特征来估计轨迹。在大型公共基准上的实验结果表明，DualTrack达到了最先进的准确性，并实现了全局一致的三维重建，优于先前的方法，平均重建误差低于5毫米。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无传感器3D超声成像中的轨迹估计问题，即如何仅从2D超声图像序列中估计探头的3D运动轨迹，从而重建出3D超声图像。这个问题很重要，因为传统3D超声系统需要昂贵的矩阵探头或外部追踪设备，限制了3D超声的广泛应用。而无传感器3D超声可以降低成本、简化操作，使3D超声在临床实践中更易于推广，同时3D超声相比2D成像具有增强可视化、改善解剖导航和精确体积测量的优势。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到局部特征（如斑点模式）能帮助预测帧间运动，全局特征（如解剖结构）能帮助定位扫描位置。先前方法要么忽略全局特征，要么将局部和全局特征紧密耦合，限制了建模能力。作者借鉴了双编码器范式，设计了两个专门编码器：一个处理局部特征，一个处理全局特征。他们还借鉴了CNN用于局部特征提取、序列模型用于长期依赖建模，以及医学图像处理中的双编码器应用。通过分离处理，作者能针对不同特征类型使用专门的设计和训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用双编码器架构分别处理局部和全局特征，然后融合它们以估计探头轨迹，从而提高轨迹估计的准确性和全局一致性。整体流程：1) 局部编码器使用3D CNN捕获细粒度特征，通过短连续子序列和全分辨率输入保持局部性；2) 全局编码器使用2D CNN和时序注意力捕获高级解剖特征，通过非连续子序列和下采样强调全局性；3) 融合模块使用transformer架构结合两种特征，通过交叉注意力让局部特征受益于全局上下文；4) 训练时先独立编码器预训练，再训练融合模块。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 首个用于无传感器3D超声的双编码器网络架构；2) 专门的局部和全局编码器设计，针对不同特征类型优化；3) 探索医学基础模型作为全局编码器的潜力；4) 专门的融合机制结合两种特征。相比之前工作，不同之处在于：DualTrack明确分离局部和全局建模，而先前方法要么忽略全局特征，要么使用相同编码器处理两种特征；局部编码器专注于斑点模式和短时运动，全局编码器专注于解剖结构和长期依赖；实验证明DualTrack显著优于先前方法，平均重建误差低于5毫米。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DualTrack通过创新的双编码器架构分离和融合局部与全局特征，显著提高了无传感器3D超声重建的准确性和一致性，实现了低于5毫米的平均重建误差。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional ultrasound (US) offers many clinical advantages overconventional 2D imaging, yet its widespread adoption is limited by the cost andcomplexity of traditional 3D systems. Sensorless 3D US, which uses deeplearning to estimate a 3D probe trajectory from a sequence of 2D US images, isa promising alternative. Local features, such as speckle patterns, can helppredict frame-to-frame motion, while global features, such as coarse shapes andanatomical structures, can situate the scan relative to anatomy and helppredict its general shape. In prior approaches, global features are eitherignored or tightly coupled with local feature extraction, restricting theability to robustly model these two complementary aspects. We proposeDualTrack, a novel dual-encoder architecture that leverages decoupled local andglobal encoders specialized for their respective scales of feature extraction.The local encoder uses dense spatiotemporal convolutions to capturefine-grained features, while the global encoder utilizes an image backbone(e.g., a 2D CNN or foundation model) and temporal attention layers to embedhigh-level anatomical features and long-range dependencies. A lightweightfusion module then combines these features to estimate the trajectory.Experimental results on a large public benchmark show that DualTrack achievesstate-of-the-art accuracy and globally consistent 3D reconstructions,outperforming previous methods and yielding an average reconstruction errorbelow 5 mm.</description>
      <author>example@mail.com (Paul F. R. Wilson, Matteo Ronchetti, Rüdiger Göbl, Viktoria Markova, Sebastian Rosenzweig, Raphael Prevost, Parvin Mousavi, Oliver Zettinig)</author>
      <guid isPermaLink="false">2509.09530v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Video Understanding by Design: How Datasets Shape Architectures and Insights</title>
      <link>http://arxiv.org/abs/2509.09151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Research report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述首次采用数据驱动的视角，分析数据集特性如何引导模型架构发展，将运动复杂性、时间跨度、层次结构和多模态丰富性视为施加归纳偏差的关键因素，并为视频理解领域提供统一框架和指导路线图。&lt;h4&gt;背景&lt;/h4&gt;视频理解因日益复杂的数据集和强大的架构而快速发展，但现有调查多按任务或家族分类模型，忽视了数据集引导架构演化的结构性压力。&lt;h4&gt;目的&lt;/h4&gt;采用数据驱动视角揭示数据集特性与模型架构间的关联，重新解释视频理解领域的发展里程碑，并提供将模型设计与数据集特性对齐的实用指导。&lt;h4&gt;方法&lt;/h4&gt;通过统一数据集、归纳偏差和架构到一个连贯框架中，重新解释从双流网络和3D CNN到顺序模型、Transformer和多模态基础模型的发展历程。&lt;h4&gt;主要发现&lt;/h4&gt;运动复杂性、时间跨度、层次结构和多模态丰富性对模型施加归纳偏差；模型发展里程碑是对这些数据驱动压力的具体响应；将模型设计与数据集不变性对齐同时平衡可扩展性和任务需求是有效的实践方法。&lt;h4&gt;结论&lt;/h4&gt;通过将数据集、归纳偏差和架构统一为一个连贯框架，该综述为推进通用视频理解提供了全面的回顾和指导性路线图。&lt;h4&gt;翻译&lt;/h4&gt;视频理解因日益复杂的数据集和强大的架构而迅速发展。然而，现有的调查大多按任务或家族对模型进行分类，忽视了数据集引导架构演化的结构性压力。这篇综述首次采用数据驱动的视角，展示了运动复杂性、时间跨度、层次结构和多模态丰富性如何施加模型应编码的归纳偏差。我们将从双流网络和3D CNN到顺序模型、Transformer和多模态基础模型的里程碑重新解释为对这些数据驱动压力的具体响应。基于这一综合，我们提供了将模型设计与数据集不变性对齐的实用指导，同时平衡可扩展性和任务需求。通过将数据集、归纳偏差和架构统一为一个连贯的框架，本综述为推进通用视频理解提供了全面的回顾和指导性路线图。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是：现有视频理解研究大多按任务或模型家族分类，忽视了数据集如何通过其结构特性引导架构演化的作用。这个问题在研究中很重要，因为缺乏这种理解会导致研究缺乏概念地图来解释架构为何如此演变，没有集成框架来背景化先前进展和预测未来趋势，也无法指导更有效模型的设计、训练策略的选择和更好的数据集构建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用数据驱动的视角，将数据集视为'结构透镜'而非静态基准。他们通过分析过去20年中最流行的数据集，识别出四个主要类别的数据集特性：运动复杂性、时间跨度、层次结构和多模态丰富性。然后分析这些特性如何引导架构演变。作者借鉴了现有工作，但提供了一个新视角，将数据集、架构和范式统一在一个框架下，填补了现有调查缺乏对数据集如何影响架构设计理解的空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：数据集通过其结构特性（运动复杂性、时间跨度、层次结构和多模态丰富性）施加'结构压力'，这些压力引导归纳偏差，最终影响架构设计。作者提出了一个'数据集-偏差-架构'框架。整体流程包括：收集分析过去20年流行数据集；根据结构特性对数据集分类；分析这些特性如何引导架构演变；重新解释从双流CNN到transformer等里程碑作为对数据集驱动压力的响应；基于此提供任务导向路线图，用于设计在时间、关系和多模态推理与可扩展性和部署约束间取得平衡的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次采用数据驱动视角审视视频理解；提出'数据集-偏差-架构'框架统一数据集、架构和归纳偏差；系统分析数据集特性如何引导架构演变；提供任务导向路线图。相比之前工作，这篇论文的不同之处在于：之前的调查大多按任务或模型家族分类，忽视数据集的结构引导作用；缺乏能背景化先前进展和预测未来趋势的概念框架；没有将数据集、架构和范式统一在一个框架下。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过建立数据集特性与架构设计之间的明确联系，提供了一个理解视频理解领域演进的统一框架，并指导未来研究如何根据数据集的结构压力来设计更有效的模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding has advanced rapidly, fueled by increasingly complexdatasets and powerful architectures. Yet existing surveys largely classifymodels by task or family, overlooking the structural pressures through whichdatasets guide architectural evolution. This survey is the first to adopt adataset-driven perspective, showing how motion complexity, temporal span,hierarchical composition, and multimodal richness impose inductive biases thatmodels should encode. We reinterpret milestones, from two-stream and 3D CNNs tosequential, transformer, and multimodal foundation models, as concreteresponses to these dataset-driven pressures. Building on this synthesis, weoffer practical guidance for aligning model design with dataset invarianceswhile balancing scalability and task demands. By unifying datasets, inductivebiases, and architectures into a coherent framework, this survey provides botha comprehensive retrospective and a prescriptive roadmap for advancinggeneral-purpose video understanding.</description>
      <author>example@mail.com (Lei Wang, Piotr Koniusz, Yongsheng Gao)</author>
      <guid isPermaLink="false">2509.09151v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>ALL-PET: A Low-resource and Low-shot PET Foundation Model in the Projection Domain</title>
      <link>http://arxiv.org/abs/2509.09130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ALL-PET是一种创新的低资源、少样本PET基础模型，通过在投影域直接操作，克服了数据稀缺和计算资源限制问题，仅需500个样本即可实现高质量正弦图生成，性能媲美大规模数据集训练的模型。&lt;h4&gt;背景&lt;/h4&gt;构建大规模PET成像基础模型受到标记数据获取有限和计算资源不足的阻碍，限制了PET成像领域的发展。&lt;h4&gt;目的&lt;/h4&gt;克服数据稀缺和效率限制，提出一种在投影域直接运行的低资源、少样本PET基础模型。&lt;h4&gt;方法&lt;/h4&gt;ALL-PET利用潜在扩散模型(LDM)并包含三个关键创新：1) Radon掩码增强策略(RMAS)，通过投影随机图像域掩码生成20多万个结构多样训练样本；2) 动态多掩码(DMM)机制，改变掩码数量和分布增强数据多样性；3) 正/负掩码约束嵌入几何一致性；4) 透明医学注意力(TMA)增强病变相关区域，支持临床ROI调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明ALL-PET仅使用500个样本即可实现高质量正弦图生成，性能可与更大数据集训练的模型相媲美；该模型在低剂量重建、衰减校正、延迟帧预测和示踪剂分离等多种任务中表现出良好的泛化能力；内存使用低于24GB，运行效率高。&lt;h4&gt;结论&lt;/h4&gt;ALL-PET是一种有效的解决方案，能够在数据有限的情况下实现高质量PET成像，为资源受限环境下的PET成像应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;构建用于PET成像的大规模基础模型受到标记数据获取有限和计算资源不足的阻碍。为了克服数据稀缺和效率限制，我们提出了ALL-PET，一种在投影域直接运行的低资源、少样本PET基础模型。ALL-PET利用潜在扩散模型(LDM)并具有三个关键创新。首先，我们设计了一种Radon掩码增强策略(RMAS)，通过将随机图像域掩码投影到正弦图空间，生成超过20万个结构多样的训练样本，以最小数据量显著提高泛化能力。通过动态多掩码(DMM)机制扩展该方法，改变掩码数量和分布，增加数据多样性而不增加模型复杂度。其次，我们实现了正/负掩码约束，嵌入严格的几何一致性，减少参数负担同时保持生成质量。第三，我们引入透明医学注意力(TMA)，一种无参数、几何驱动的机制，增强原始投影数据中的病变相关区域。病变焦点注意力图从粗略分割导出，覆盖高代谢和低代谢区域，并投影到正弦图空间以保持物理一致性引导。系统支持临床医生定义的ROI调整，确保灵活、可解释且任务自适应的强调，与PET采集物理原理一致。实验结果表明，ALL-PET仅使用500个样本即可实现高质量正弦图生成，性能可与在更大数据集上训练的模型相媲美。ALL-PET在低剂量重建、衰减校正、延迟帧预测和示踪剂分离等任务中具有泛化能力，内存使用低于24GB，运行效率高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决PET成像基础模型构建中的两个核心挑战：数据资源稀缺和计算资源限制。在临床环境中，获取大量标记的PET数据困难且成本高，同时现有基础模型通常需要数亿参数和大量计算资源，难以在资源有限的医院或便携式设备中部署。这个问题很重要，因为PET成像在肿瘤学、神经科学等领域至关重要，解决这些问题可以促进先进AI模型在资源受限医疗环境中的实际应用，提高诊断效率和可及性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了PET基础模型面临的数据稀缺和计算资源限制问题，指出大多数现有医学基础模型在图像域操作，忽略了投影域中包含的重要物理先验信息。他们选择在投影域操作，采用潜在扩散模型(LDM)作为基础框架，并针对数据稀缺问题设计创新的数据增强策略。作者借鉴了扩散模型在医学图像生成中的应用、Radon变换在PET成像中的理论基础、注意力机制在医学图像分析中的应用，以及低资源学习方法。他们的创新性设计包括RMAS策略、DMM机制、TMA机制和正负掩码约束，这些都是对现有方法的改进和扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; ALL-PET的核心思想是通过智能的数据设计和物理引导的学习，在投影域构建一个低资源、低剂量的PET基础模型。整体实现流程包括：1)收集有限数量的PET正弦图数据；2)使用RMAS策略将图像域掩码投影到正弦图空间生成多样化训练样本；3)应用DMM机制动态变化掩码增加数据多样性；4)基于潜在扩散模型构建框架，应用正负掩码约束嵌入几何一致性；5)使用TMA机制增强病变相关区域；6)将训练好的模型应用于各种下游任务如低剂量重建、衰减校正、延迟帧预测等；7)使用扩散采样方法进行推理和生成，支持文本提示和参数控制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Radon Mask增强策略(RMAS)，通过投影生成多样化训练样本；2)动态多掩码(DMM)机制，增加数据多样性而不增加模型复杂度；3)正负掩码约束，作为零成本正则化器嵌入几何一致性；4)透明医疗注意力(TMA)，无参数的几何驱动注意力机制。相比之前工作，ALL-PET直接在投影域操作而非图像域，仅使用500个样本而非数万至数十万样本，可在单个GPU而非多节点集群上运行，通过物理约束确保物理一致性，并通过TMA机制提供临床灵活性，允许医生注入领域知识无需重新训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALL-PET通过创新的投影域数据增强、几何约束和可解释注意力机制，首次实现了在资源受限条件下仅使用500个样本即可生成高质量PET投影数据的基础模型，为临床环境中的低资源PET成像应用提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building large-scale foundation model for PET imaging is hindered by limitedaccess to labeled data and insufficient computational resources. To overcomedata scarcity and efficiency limitations, we propose ALL-PET, a low-resource,low-shot PET foundation model operating directly in the projection domain.ALL-PET leverages a latent diffusion model (LDM) with three key innovations.First, we design a Radon mask augmentation strategy (RMAS) that generates over200,000 structurally diverse training samples by projecting randomizedimage-domain masks into sinogram space, significantly improving generalizationwith minimal data. This is extended by a dynamic multi-mask (DMM) mechanismthat varies mask quantity and distribution, enhancing data diversity withoutadded model complexity. Second, we implement positive/negative mask constraintsto embed strict geometric consistency, reducing parameter burden whilepreserving generation quality. Third, we introduce transparent medicalattention (TMA), a parameter-free, geometry-driven mechanism that enhanceslesion-related regions in raw projection data. Lesion-focused attention mapsare derived from coarse segmentation, covering both hypermetabolic andhypometabolic areas, and projected into sinogram space for physicallyconsistent guidance. The system supports clinician-defined ROI adjustments,ensuring flexible, interpretable, and task-adaptive emphasis aligned with PETacquisition physics. Experimental results show ALL-PET achieves high-qualitysinogram generation using only 500 samples, with performance comparable tomodels trained on larger datasets. ALL-PET generalizes across tasks includinglow-dose reconstruction, attenuation correction, delayed-frame prediction, andtracer separation, operating efficiently with memory use under 24GB.</description>
      <author>example@mail.com (Bin Huang, Kang Chen, Bingxuan Li, Huafeng Liu, Qiegen Liu)</author>
      <guid isPermaLink="false">2509.09130v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>FoundationalECGNet: A Lightweight Foundational Model for ECG-based Multitask Cardiac Analysis</title>
      <link>http://arxiv.org/abs/2509.08961v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FoundationalECGNet的基础框架，用于自动心电图分类，解决了当前ECG分析中面临的噪声、类别不平衡和数据集异质性等挑战。&lt;h4&gt;背景&lt;/h4&gt;心血管疾病是全球主要的死亡原因，心电图分析是检测心脏异常的关键技术，但当前方法面临多种技术挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种准确且可扩展的心电图自动诊断系统，提高心脏异常检测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;FoundationalECGNet模型集成双阶段去噪技术（Morlet和Daubechies小波变换）、卷积块注意力模块、图注意力网络和时间序列变换器，共同捕获多通道ECG信号中的空间和时间依赖关系，采用两阶段分类策略：先区分正常与异常ECG信号，再将异常信号细分为五种心脏疾病。&lt;h4&gt;主要发现&lt;/h4&gt;模型在多个数据集上表现优异，正常与异常分类达到99%的F1分数；在多类疾病检测中达到最先进性能，包括传导障碍和肥厚症99%的F1分数，心律失常98.9%的F1分数；同时提供风险水平估计以辅助临床决策。&lt;h4&gt;结论&lt;/h4&gt;FoundationalECGNet是一种可扩展、可解释和通用的自动ECG分析解决方案，有潜力提高医疗环境中的诊断精度和患者结局。&lt;h4&gt;翻译&lt;/h4&gt;心血管疾病(CVDs)仍然是全球主要的死亡原因，这凸显了准确且可扩展的诊断系统的重要性。心电图(ECG)分析是检测心脏异常的核心，然而噪声、类别不平衡和数据集异质性等挑战限制了当前方法。为解决这些问题，我们提出了FoundationalECGNet，一种用于自动ECG分类的基础框架。该模型集成了Morlet和Daubechies小波变换的双阶段去噪、卷积块注意力模块(CBAM)、图注意力网络(GAT)和时间序列变换器(TST)，共同捕获多通道ECG信号中的空间和时间依赖关系。FoundationalECGNet首先区分正常和异常ECG信号，然后将异常信号分类为五种心脏疾病之一：心律失常、传导障碍、心肌梗死、QT异常或肥厚症。在多个数据集上，模型在正常与异常分类中达到99%的F1分数，并在多类疾病检测中显示出最先进的性能，包括传导障碍和肥厚症99%的F1分数，以及心律失常98.9%的F1分数。此外，该模型提供风险水平估计以促进临床决策。总之，FoundationalECGNet代表了自动ECG分析的一种可扩展、可解释和通用的解决方案，有可能提高医疗环境中的诊断精度和患者结局。我们将在接受后分享代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiovascular diseases (CVDs) remain a leading cause of mortality worldwide,underscoring the importance of accurate and scalable diagnostic systems.Electrocardiogram (ECG) analysis is central to detecting cardiac abnormalities,yet challenges such as noise, class imbalance, and dataset heterogeneity limitcurrent methods. To address these issues, we propose FoundationalECGNet, afoundational framework for automated ECG classification. The model integrates adual-stage denoising by Morlet and Daubechies wavelets transformation,Convolutional Block Attention Module (CBAM), Graph Attention Networks (GAT),and Time Series Transformers (TST) to jointly capture spatial and temporaldependencies in multi-channel ECG signals. FoundationalECGNet firstdistinguishes between Normal and Abnormal ECG signals, and then classifies theAbnormal signals into one of five cardiac conditions: Arrhythmias, ConductionDisorders, Myocardial Infarction, QT Abnormalities, or Hypertrophy. Acrossmultiple datasets, the model achieves a 99% F1-score for Normal vs. Abnormalclassification and shows state-of-the-art performance in multi-class diseasedetection, including a 99% F1-score for Conduction Disorders and Hypertrophy,as well as a 98.9% F1-score for Arrhythmias. Additionally, the model providesrisk level estimations to facilitate clinical decision-making. In conclusion,FoundationalECGNet represents a scalable, interpretable, and generalizablesolution for automated ECG analysis, with the potential to improve diagnosticprecision and patient outcomes in healthcare settings. We'll share the codeafter acceptance.</description>
      <author>example@mail.com (Md. Sajeebul Islam Sk., Md Jobayer, Md Mehedi Hasan Shawon, Md. Golam Raibul Alam)</author>
      <guid isPermaLink="false">2509.08961v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Live(r) Die: Predicting Survival in Colorectal Liver Metastasis</title>
      <link>http://arxiv.org/abs/2509.08935v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Thesis at Erasmus Mundus Joint Master's Degree in Medical Imaging and  Applications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个全自动框架，利用术前MRI图像预测结直肠癌肝转移(CRLM)手术患者的生存结果，结合自动分割算法和基于放射组学的生存分析，显著提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;结直肠癌经常转移到肝脏，显著降低长期生存率。虽然手术切除是唯一可能治愈CRLM的治疗方法，但患者结果因肿瘤特征以及临床和基因组因素而异。当前的预后模型基于有限的临床或分子特征，预测能力不足，特别是在多发性CRLM病例中。&lt;h4&gt;目的&lt;/h4&gt;开发一个全自动框架，利用术前获取的对比增强前和对比增强后MRI图像来预测CRLM手术患者的预后结果，提高预测准确性并减少对标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;研究提出了包含分割流程和放射组学流程的框架。分割流程利用可提示的基础模型从未标注数据中学习分割肝脏、肿瘤和脾脏。研究者还提出了SAMONAI，一种零样本3D提示传播算法，利用分割任何模型从单点提示分割3D感兴趣区域。分割结果随后输入放射组学流程，提取每个肿瘤的特征并使用SurvAMINN(一种基于自编码器的多实例神经网络)预测生存。&lt;h4&gt;主要发现&lt;/h4&gt;在包含227名患者的机构数据集上进行的大量评估表明，该框架超越了现有的临床和生物标志物，C-index提高了超过10%。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了整合自动分割算法和基于放射组学的生存分析能够为CRLM提供准确、标注效率高且可解释的结果预测。&lt;h4&gt;翻译&lt;/h4&gt;结直肠癌经常转移到肝脏，显著降低长期生存率。虽然手术切除是唯一可能治愈结直肠癌肝转移(CRLM)的治疗方法，但患者结果因肿瘤特征以及临床和基因组因素而异。当前的预后模型通常基于有限的临床或分子特征，预测能力不足，特别是在多发性CRLM病例中。我们提出了一个全自动框架，用于从术前获取的对比增强前和对比增强后MRI预测手术结果。我们的框架包含一个分割流程和一个放射组学流程。分割流程利用可提示的基础模型从未标注数据中学习分割肝脏、肿瘤和脾脏，完成缺失的标签。此外，我们提出了SAMONAI，一种新颖的零样本3D提示传播算法，利用分割任何模型从单点提示分割3D感兴趣区域，显著提高了我们分割流程的准确性和效率。然后将预测的对比增强前和对比增强后的分割结果输入我们的放射组学流程，该流程从每个肿瘤中提取特征并使用SurvAMINN(一种基于自编码器的多实例神经网络用于生存分析)预测生存。SurvAMINN从右删失生存数据中联合学习降维和风险预测，专注于最具侵袭性的肿瘤。在包含227名患者的机构数据集上进行的大量评估表明，我们的框架超越了现有的临床和基因组生物标志物，C-index提高了超过10%。我们的结果表明，整合自动分割算法和基于放射组学的生存分析能够为CRLM提供准确、标注效率高且可解释的结果预测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结直肠癌肝转移(CRLM)患者手术预后预测不准确的问题。结直肠癌是全球第三大常见癌症和第二大癌症相关死亡原因，肝脏是其最常见的转移部位。手术切除是唯一可能治愈CRLM的方法，但并非所有患者都适合，且患者预后差异很大。准确预测手术结果对于避免无益手术、允许个性化治疗并最终提高患者生存结果至关重要。现有的预后模型（基于有限的临床或分子特征）预测能力不足，特别是在多发性CRLM病例中，因此这一问题在临床实践中具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有预后方法的局限性（忽略多发性肿瘤、仅依赖后对比图像、依赖手动分割、需要完全标注数据）来设计方法。作者借鉴了现有工作：分割管道借鉴了UNet、nn-UNet和基于Transformer的架构，以及自监督学习和基础模型（特别是Segment Anything Model, SAM）；放射组学管道借鉴了AMINN（基于自编码器的多实例网络）和多实例学习(MIL)概念。作者创新性地将SAM扩展到3D医学图像分割（提出SAMONAI），并改进了生存预测网络（SurvAMINN）以更好地处理删失数据和多发性肿瘤，实现了全自动、多病灶、标注高效的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用医学影像（MRI）的全自动分析来预测CRLM患者的生存情况，结合对比前后的MRI图像提高准确性，使用基础模型和少量学习技术解决标注数据稀缺问题，并聚焦最具侵袭性的肿瘤进行预后预测。整体流程分为两个主要管道：1）分割管道：使用SAMONAI（3D提示传播算法）从部分标注数据中学习肝脏、肿瘤和脾脏的分割；2）放射组学管道：从每个肿瘤中提取放射组学特征，使用SurvAMINN（自编码器多实例神经网络）进行生存预测，通过LogSumExp池化操作关注最具侵袭性的肿瘤。评估使用Dice系数（分割质量）、F1分数（肿瘤检测）和C-index（预后预测）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）SAMONAI - 一种3D提示传播算法，从单个点提示分割整个3D对象；2）SurvAMINN - 用于生存分析的自编码器多实例神经网络，能从删失数据中学习并关注最具侵袭性的肿瘤；3）全自动、多病灶、标注高效的框架，能从部分标注数据中学习并整合对比前后的MRI图像。相比之前的工作，不同之处在于：不再仅依赖临床或分子特征，而是整合医学影像信息；考虑多发性肿瘤而不仅是最大肿瘤；使用全自动分割而非手动分割；结合对比前后的图像信息；能够处理删失数据；联合学习降维和预测避免信息损失。这些创新使预测准确性显著提高（C-index提高超过10%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种全自动框架，通过创新的3D图像分割技术和基于放射组学的生存预测方法，显著提高了结直肠癌肝转移患者的手术预后预测准确性，同时减少了对大量标注数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colorectal cancer frequently metastasizes to the liver, significantlyreducing long-term survival. While surgical resection is the only potentiallycurative treatment for colorectal liver metastasis (CRLM), patient outcomesvary widely depending on tumor characteristics along with clinical and genomicfactors. Current prognostic models, often based on limited clinical ormolecular features, lack sufficient predictive power, especially in multifocalCRLM cases. We present a fully automated framework for surgical outcomeprediction from pre- and post-contrast MRI acquired before surgery. Ourframework consists of a segmentation pipeline and a radiomics pipeline. Thesegmentation pipeline learns to segment the liver, tumors, and spleen frompartially annotated data by leveraging promptable foundation models to completemissing labels. Also, we propose SAMONAI, a novel zero-shot 3D promptpropagation algorithm that leverages the Segment Anything Model to segment 3Dregions of interest from a single point prompt, significantly improving oursegmentation pipeline's accuracy and efficiency. The predicted pre- andpost-contrast segmentations are then fed into our radiomics pipeline, whichextracts features from each tumor and predicts survival using SurvAMINN, anovel autoencoder-based multiple instance neural network for survival analysis.SurvAMINN jointly learns dimensionality reduction and hazard prediction fromright-censored survival data, focusing on the most aggressive tumors. Extensiveevaluation on an institutional dataset comprising 227 patients demonstratesthat our framework surpasses existing clinical and genomic biomarkers,delivering a C-index improvement exceeding 10%. Our results demonstrate thepotential of integrating automated segmentation algorithms and radiomics-basedsurvival analysis to deliver accurate, annotation-efficient, and interpretableoutcome prediction in CRLM.</description>
      <author>example@mail.com (Muhammad Alberb, Helen Cheung, Anne Martel)</author>
      <guid isPermaLink="false">2509.08935v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Calib3R: A 3D Foundation Model for Multi-Camera to Robot Calibration and 3D Metric-Scaled Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Calib3R的新方法，能够同时完成相机到机器人的校准和度量缩放的3D重建，无需传统校准所需的图案，只需少量RGB图像即可实现高精度校准。&lt;h4&gt;背景&lt;/h4&gt;机器人通常依赖RGB图像完成操作和导航任务，但可靠的交互需要度量缩放并与机器人参考框架对齐的3D场景表示。传统方法中，相机校准和3D重建被视为独立任务，多摄像头设置增加了数据整合的复杂性。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需图案的方法，通过统一优化同时执行相机到机器人的校准和度量缩放的3D重建，适用于单摄像头和多摄像头设置。&lt;h4&gt;方法&lt;/h4&gt;提出Calib3R方法，基于3D基础模型MASt3R从RGB图像中提取点图，结合机器人姿态信息，重建与机器人参考框架对齐的缩放3D场景，可应用于机器人手臂或移动机器人。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，Calib3R在各种数据集上使用少于10张图像即可实现精确校准，性能优于现有的无目标和基于标记的方法。&lt;h4&gt;结论&lt;/h4&gt;Calib3R提供了一种高效解决方案，能够同时解决相机校准和3D重建问题，简化了机器人系统的3D场景获取流程。&lt;h4&gt;翻译&lt;/h4&gt;机器人通常依赖RGB图像完成操作和导航等任务。然而，可靠的交互通常需要3D场景表示，该表示应该是度量缩放的并与机器人参考框架对齐。这依赖于精确的相机到机器人的校准和密集的3D重建，这些任务通常被分别处理，尽管两者都依赖于RGB数据中的几何对应关系。传统校准需要图案，而基于RGB的重建在任意框架中产生未知尺度的几何形状。多摄像头设置增加了额外的复杂性，因为数据必须在共享参考框架中表示。我们提出了Calib3R，一种无需图案的方法，通过统一优化同时执行相机到机器人的校准和度量缩放的3D重建。Calib3R处理机器人手臂或移动机器人上的单摄像头和多摄像头设置。它基于3D基础模型MASt3R从RGB图像中提取点图，这些点图与机器人姿态结合，重建与机器人对齐的缩放3D场景。在各种数据集上的实验表明，Calib3R使用少于10张图像即可实现精确校准，性能优于无目标和基于标记的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人系统中相机到机器人的标定和3D场景重建的统一问题。传统方法通常将这两个任务分开处理，且需要专门的校准模式或标记物。这个问题在现实中很重要，因为机器人需要准确理解周围环境的3D结构才能安全有效地与环境交互，而现有的标定方法往往需要专用设备、手动操作或特定环境条件，限制了机器人在复杂、动态环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括传统标定需要模式、多相机标定只关注相机间相对姿态而忽略与机器人参考帧的关系、以及标定和重建作为独立任务处理导致效率低下。作者注意到3D基础模型MASt3R能直接从RGB图像估计局部3D几何，但缺乏度量比例和与机器人参考帧的对齐。方法设计借鉴了MASt3R-SfM框架和手眼标定的经典公式AX=XB，但进行了改进以处理比例问题。作者设计了一个统一的优化过程，将相机到机器人标定和3D重建结合在一起，利用机器人姿态信息同时估计相机到机器人的变换和比例因子。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将相机到机器人标定和3D场景重建视为一个统一的优化问题，而不是两个独立任务。方法利用MASt3R模型从RGB图像中提取局部点图，结合机器人姿态信息，通过联合优化实现标定和重建，并引入比例因子λ解决重建的尺度模糊问题。在多相机情况下，添加跨相机一致性约束保持相机间相对姿态。整体流程：1)输入RGB图像和机器人姿态；2)用MASt3R生成局部点图；3)构建共可见性图；4)计算规范点图；5)定义包含场景几何、标定和跨相机一致性的联合损失函数；6)通过梯度下降优化；7)输出相机到机器人变换和对齐的3D重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一框架，联合执行标定和重建；2)无需校准模式；3)单一优化过程同时处理两个任务；4)引入比例因子解决尺度模糊；5)多相机情况下的跨相机一致性约束；6)适用于多种机器人平台和相机配置。相比之前的工作，不同之处在于：不需要传统校准设备；直接与机器人参考帧对齐；避免了多阶段处理和误差累积；将重建和标定集成在同一个优化循环中；能估计完整的6自由度变换，不受平面运动限制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Calib3R是一种创新的统一方法，通过结合3D基础模型和机器人姿态信息，实现了无需校准模式的相机到机器人标定和与机器人参考帧对齐的度量比例3D场景重建，适用于各种机器人平台和相机配置。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots often rely on RGB images for tasks like manipulation and navigation.However, reliable interaction typically requires a 3D scene representation thatis metric-scaled and aligned with the robot reference frame. This depends onaccurate camera-to-robot calibration and dense 3D reconstruction, tasks usuallytreated separately, despite both relying on geometric correspondences from RGBdata. Traditional calibration needs patterns, while RGB-based reconstructionyields geometry with an unknown scale in an arbitrary frame. Multi-camerasetups add further complexity, as data must be expressed in a shared referenceframe. We present Calib3R, a patternless method that jointly performscamera-to-robot calibration and metric-scaled 3D reconstruction via unifiedoptimization. Calib3R handles single- and multi-camera setups on robot arms ormobile robots. It builds on the 3D foundation model MASt3R to extract pointmapsfrom RGB images, which are combined with robot poses to reconstruct a scaled 3Dscene aligned with the robot. Experiments on diverse datasets show that Calib3Rachieves accurate calibration with less than 10 images, outperformingtarget-less and marker-based methods.</description>
      <author>example@mail.com (Davide Allegro, Matteo Terreran, Stefano Ghidoni)</author>
      <guid isPermaLink="false">2509.08813v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals</title>
      <link>http://arxiv.org/abs/2509.08809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的智能体标注范式和CAI比率指标，用于在动态、无监督环境中评估大型语言模型的标注质量，解决了oracle反馈稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型与基于提示的任务结合显著降低了数据标注成本和对人工标注员的依赖。然而，在动态、无监督环境中评估这些模型标注质量具有挑战性，因为oracle反馈稀缺且传统方法失效。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖oracle反馈的新方法，用于评估和改进大型语言模型在动态、无监督环境中的标注质量。&lt;h4&gt;方法&lt;/h4&gt;提出了一种智能体标注范式，其中学生模型与嘈杂的教师模型(即LLM)协作，采用基于用户偏好的多数投票策略评估LLM输出的一致性。同时引入了一致性与不一致性(CAI)比率作为无监督评估指标，系统测量LLM生成标注的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;CAI比率不仅能在有限用户偏好下量化嘈杂教师的标注质量，还在模型选择中发挥关键作用。在四个LLM和十个开放域NLP数据集上的应用表明，CAI比率与LLM准确性呈强正相关。&lt;h4&gt;结论&lt;/h4&gt;CAI比率是真实世界环境中无监督评估和模型选择的重要工具，能够帮助识别动态、无监督环境中的稳健大型语言模型。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型与基于提示的任务结合时，显著降低了数据标注成本和对人工标注员的依赖。然而，在oracle反馈稀缺且传统方法失效的动态、无监督环境中，评估其标注质量仍然具有挑战性。为应对这一挑战，我们提出了一种新颖的智能体标注范式，其中学生模型与嘈杂的教师模型(即LLM)协作，评估和改进标注质量而不依赖oracle反馈。作为无监督反馈机制的学生模型，采用基于用户偏好的多数投票策略来评估LLM输出的一致性。为了系统测量LLM生成标注的可靠性，我们引入了一致性与不一致性(CAI)比率这一新颖的无监督评估指标。CAI比率不仅能在有限用户偏好下量化嘈杂教师的标注质量，还在模型选择中发挥关键作用，使能够在动态、无监督环境中识别稳健的LLM。在四个LLM和十个开放域NLP数据集上的应用表明，CAI比率与LLM准确性呈强正相关，使其成为真实世界环境中无监督评估和模型选择的重要工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs), when paired with prompt-based tasks, havesignificantly reduced data annotation costs and reliance on human annotators.However, evaluating the quality of their annotations remains challenging indynamic, unsupervised environments where oracle feedback is scarce andconventional methods fail. To address this challenge, we propose a novelagentic annotation paradigm, where a student model collaborates with a noisyteacher (the LLM) to assess and refine annotation quality without relying onoracle feedback. The student model, acting as an unsupervised feedbackmechanism, employs a user preference-based majority voting strategy to evaluatethe consistency of the LLM outputs. To systematically measure the reliabilityof LLM-generated annotations, we introduce the Consistent and Inconsistent(CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not onlyquantifies the annotation quality of the noisy teacher under limited userpreferences but also plays a critical role in model selection, enabling theidentification of robust LLMs in dynamic, unsupervised environments. Applied toten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates astrong positive correlation with LLM accuracy, establishing it as an essentialtool for unsupervised evaluation and model selection in real-world settings.</description>
      <author>example@mail.com (Cheng Chen, Haiyan Yin, Ivor Tsang)</author>
      <guid isPermaLink="false">2509.08809v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>TANGO: Traversability-Aware Navigation with Local Metric Control for Topological Goals</title>
      <link>http://arxiv.org/abs/2509.08699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于RGB视觉输入、对象级别的拓扑导航方法，无需3D地图或预训练控制器即可实现零样本、长距离机器人导航。该方法结合全局拓扑路径规划和局部度量轨迹控制，使用单目深度和可 traversability 估计持续预测局部轨迹，并包含自动切换机制。研究使用基础模型确保开放集适用性，在模拟和真实环境中证明了其有效性和优越性。&lt;h4&gt;背景&lt;/h4&gt;传统的机器人视觉导航方法依赖于全局一致的3D地图或学习控制器，这些方法计算成本高且难以在不同环境中泛化。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的RGB-only、对象级别的拓扑导航管道，实现零样本、长距离机器人导航，无需3D地图或预训练控制器，同时确保开放集适用性和领域无关性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合全局拓扑路径规划和局部度量轨迹控制的导航方法。使用单目深度和可 traversability 估计持续预测局部轨迹，并包含自动切换机制，在必要时回退到基础控制器。系统使用基础模型确保开放集适用性，无需领域特定微调。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在模拟环境和真实世界测试中表现出色，证明了其鲁棒性和可部署性。与现有最先进方法相比，该方法在开放环境中的视觉导航提供了更适应和有效的解决方案。&lt;h4&gt;结论&lt;/h4&gt;提出的RGB-only、对象级别拓扑导航方法克服了传统方法的局限性，实现了无需3D地图或预训练控制器的零样本、长距离导航，为机器人视觉导航提供了一种更适应和有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;机器人视觉导航传统上依赖于全局一致的3D地图或学习控制器，这些方法计算成本高且难以在不同环境中泛化。在这项工作中，我们提出了一种新颖的仅使用RGB、基于对象级别的拓扑导航管道，能够在无需3D地图或预训练控制器的情况下实现零样本、长距离机器人导航。我们的方法将全局拓扑路径规划与局部度量轨迹控制相结合，使机器人能够导航至对象级别的子目标，同时避开障碍物。我们通过使用单目深度和可 traversability 估计持续预测局部轨迹，并纳入自动切换机制（必要时回退到基础控制器），解决了先前方法的关键局限性。该系统使用基础模型运行，确保开放集适用性，无需领域特定的微调。我们在模拟环境和真实世界测试中证明了该方法的有效性，突显了其鲁棒性和可部署性。我们的方法优于现有的最先进方法，为开放环境中的视觉导航提供了更适应和有效的解决方案。源代码已公开：https://github.com/podgorki/TANGO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA55743.2025.11127998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual navigation in robotics traditionally relies on globally-consistent 3Dmaps or learned controllers, which can be computationally expensive anddifficult to generalize across diverse environments. In this work, we present anovel RGB-only, object-level topometric navigation pipeline that enableszero-shot, long-horizon robot navigation without requiring 3D maps orpre-trained controllers. Our approach integrates global topological pathplanning with local metric trajectory control, allowing the robot to navigatetowards object-level sub-goals while avoiding obstacles. We address keylimitations of previous methods by continuously predicting local trajectoryusing monocular depth and traversability estimation, and incorporating anauto-switching mechanism that falls back to a baseline controller whennecessary. The system operates using foundational models, ensuring open-setapplicability without the need for domain-specific fine-tuning. We demonstratethe effectiveness of our method in both simulated environments and real-worldtests, highlighting its robustness and deployability. Our approach outperformsexisting state-of-the-art methods, offering a more adaptable and effectivesolution for visual navigation in open-set environments. The source code ismade publicly available: https://github.com/podgorki/TANGO.</description>
      <author>example@mail.com (Stefan Podgorski, Sourav Garg, Mehdi Hosseinzadeh, Lachlan Mares, Feras Dayoub, Ian Reid)</author>
      <guid isPermaLink="false">2509.08699v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>RoentMod: A Synthetic Chest X-Ray Modification Model to Identify and Correct Image Interpretation Model Shortcuts</title>
      <link>http://arxiv.org/abs/2509.08640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 + 8 pages, 4 + 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RoentMod是一种创新的反事实图像编辑框架，能够生成具有特定病理特征的解剖学上真实的胸部X光片，同时保留原始图像的其他解剖特征。研究表明，这种方法可以有效探测和纠正深度学习模型中的捷径学习问题，提高模型的性能和可靠性。&lt;h4&gt;背景&lt;/h4&gt;胸部X光片是医学中最常见的检查之一。自动图像解释可以减少放射科医生的工作量并扩大诊断专业知识的获取渠道。然而，深度学习多任务和基础模型在胸部X光片解释方面表现出色的同时，容易受到'捷径学习'的影响，即模型依赖虚假和非目标相关性而非临床相关特征来做决策。&lt;h4&gt;目的&lt;/h4&gt;引入RoentMod框架，生成具有用户指定的合成病理的解剖学上真实的胸部X光片，同时保留原始扫描中不相关的解剖特征，用于探测和纠正医学AI中的捷径学习问题。&lt;h4&gt;方法&lt;/h4&gt;RoentMod结合了开源医学图像生成器RoentGen和图像到图像的修改模型，无需重新训练。通过获得认证的放射科医生和放射科住院医师的读者研究评估其性能，并使用RoentMod生成的反事实图像进行训练以缓解模型对捷径学习的脆弱性。&lt;h4&gt;主要发现&lt;/h4&gt;RoentMod生成的图像在93%的情况下看起来很真实，在89-99%的情况下正确包含了指定的发现，并保留了与真实随访胸部X光片相当的原始解剖结构。研究显示，最先进的多任务和基础模型经常利用非目标病理作为捷径，限制了它们的特异性。在训练期间纳入RoentMod生成的反事实图像后，在内部验证中使多种病理的模型鉴别能力提高了3-19%的AUC，在外部测试中使6种测试病理中的5种提高了1-11%。&lt;h4&gt;结论&lt;/h4&gt;RoentMod是一个广泛适用的工具，用于探测和纠正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了胸部X光片解释模型的鲁棒性和可解释性，为改进医学成像中的基础模型提供了可推广的策略。&lt;h4&gt;翻译&lt;/h4&gt;胸部X光片是医学中最常见的检查之一。自动图像解释可以减少放射科医生的工作量并扩大诊断专业知识的获取渠道。深度学习多任务和基础模型在胸部X光片解释方面表现出色，但容易受到捷径学习的影响，即模型依赖虚假和非目标相关性而非临床相关特征来做决策。我们引入了RoentMod，这是一个反事实图像编辑框架，能够生成具有用户指定的合成病理的解剖学上真实的胸部X光片，同时保留原始扫描中不相关的解剖特征。RoentMod结合了一个开源医学图像生成器（RoentGen）和一个图像到图像的修改模型，不需要重新训练。在获得认证的放射科医生和放射科住院医师的读者研究中，RoentMod生成的图像在93%的情况下看起来很真实，在89-99%的情况下正确包含了指定的发现，并保留了与真实随访胸部X光片相当的原始解剖结构。使用RoentMod，我们证明了最先进的多任务和基础模型经常利用非目标病理作为捷径，限制了它们的特异性。在训练期间纳入RoentMod生成的反事实图像可以缓解这种脆弱性，在内部验证中使多种病理的模型鉴别能力提高了3-19%的AUC，在外部测试中使6种测试病理中的5种提高了1-11%。这些研究结果表明RoentMod是一个广泛适用的工具，用于探测和纠正医学AI中的捷径学习。通过实现可控的反事实干预，RoentMod增强了胸部X光片解释模型的鲁棒性和可解释性，为改进医学成像中的基础模型提供了可推广的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chest radiographs (CXRs) are among the most common tests in medicine.Automated image interpretation may reduce radiologists\' workload and expandaccess to diagnostic expertise. Deep learning multi-task and foundation modelshave shown strong performance for CXR interpretation but are vulnerable toshortcut learning, where models rely on spurious and off-target correlationsrather than clinically relevant features to make decisions. We introduceRoentMod, a counterfactual image editing framework that generates anatomicallyrealistic CXRs with user-specified, synthetic pathology while preservingunrelated anatomical features of the original scan. RoentMod combines anopen-source medical image generator (RoentGen) with an image-to-imagemodification model without requiring retraining. In reader studies withboard-certified radiologists and radiology residents, RoentMod-produced imagesappeared realistic in 93\% of cases, correctly incorporated the specifiedfinding in 89-99\% of cases, and preserved native anatomy comparable to realfollow-up CXRs. Using RoentMod, we demonstrate that state-of-the-art multi-taskand foundation models frequently exploit off-target pathology as shortcuts,limiting their specificity. Incorporating RoentMod-generated counterfactualimages during training mitigated this vulnerability, improving modeldiscrimination across multiple pathologies by 3-19\% AUC in internal validationand by 1-11\% for 5 out of 6 tested pathologies in external testing. Thesefindings establish RoentMod as a broadly applicable tool for probing andcorrecting shortcut learning in medical AI. By enabling controlledcounterfactual interventions, RoentMod enhances the robustness andinterpretability of CXR interpretation models and provides a generalizablestrategy for improving foundation models in medical imaging.</description>
      <author>example@mail.com (Lauren H. Cooke, Matthias Jung, Jan M. Brendel, Nora M. Kerkovits, Borek Foldyna, Michael T. Lu, Vineet K. Raghu)</author>
      <guid isPermaLink="false">2509.08640v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>CLAPS: A CLIP-Unified Auto-Prompt Segmentation for Multi-Modal Retinal Imaging</title>
      <link>http://arxiv.org/abs/2509.08618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BIBM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了CLIP-unified Auto-Prompt Segmentation (CLAPS)方法，解决了视网膜图像分割中的模态模糊性、手动提示依赖和缺乏统一框架等挑战，实现了跨任务和模态的自动化精确分割。&lt;h4&gt;背景&lt;/h4&gt;基础模型如SAM在医学图像分割特别是视网膜图像分割中取得显著进展，精确分割对诊断至关重要。&lt;h4&gt;目的&lt;/h4&gt;克服当前视网膜图像分割方法面临的三大挑战：模态模糊性、手动提示依赖和缺乏统一框架，实现跨任务和模态的自动化分割。&lt;h4&gt;方法&lt;/h4&gt;CLAPS方法包括：1)在大型多模态视网膜数据集上预训练基于CLIP的图像编码器；2)利用GroundingDINO自动生成空间边界框提示；3)使用带有独特'模态签名'的文本提示统一任务；4)通过自动化提示引导SAM执行精确分割。&lt;h4&gt;主要发现&lt;/h4&gt;在12个不同数据集的11个关键分割类别上进行的实验表明，CLAPS性能与专业专家模型相当，并在大多数指标上超越现有基准，展示了广泛的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CLAPS作为一个统一框架，成功解决了视网膜图像分割中的关键挑战，实现了自动化和跨模态、跨任务的精确分割，作为基础模型具有强大的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;最近基础模型的进展，如Segment Anything Model(SAM)，显著影响了医学图像分割，特别是在视网膜成像中，精确分割对诊断至关重要。尽管如此，当前方法面临关键挑战：1)文本疾病描述中的模态模糊性，2)继续依赖SAM工作流程的手动提示，3)缺乏统一框架，大多数方法都是模态和任务特定的。为克服这些障碍，我们提出了CLIP-unified Auto-Prompt Segmentation (CLAPS)，一种用于视网膜成像中跨多样化和模态的统一分割的新方法。我们的方法首先在大型多模态视网膜数据集上预训练基于CLIP的图像编码器，以处理数据稀缺和分布不平衡。然后我们利用GroundingDINO通过检测局部病变自动生成空间边界框提示。为了统一任务并解决模糊性，我们使用带有独特'模态签名'的文本提示。最终，这些自动化的文本和空间提示指导SAM执行精确分割，创建完全自动化的统一流程。在11个关键分割类别的12个不同数据集上进行的大量实验表明，CLAPS的性能与专业专家模型相当，并且在大多数指标上超越现有基准，展示了其作为基础模型的广泛泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models, such as the Segment Anything Model(SAM), have significantly impacted medical image segmentation, especially inretinal imaging, where precise segmentation is vital for diagnosis. Despitethis progress, current methods face critical challenges: 1) modality ambiguityin textual disease descriptions, 2) a continued reliance on manual promptingfor SAM-based workflows, and 3) a lack of a unified framework, with mostmethods being modality- and task-specific. To overcome these hurdles, wepropose CLIP-unified Auto-Prompt Segmentation (\CLAPS), a novel method forunified segmentation across diverse tasks and modalities in retinal imaging.Our approach begins by pre-training a CLIP-based image encoder on a large,multi-modal retinal dataset to handle data scarcity and distribution imbalance.We then leverage GroundingDINO to automatically generate spatial bounding boxprompts by detecting local lesions. To unify tasks and resolve ambiguity, weuse text prompts enhanced with a unique "modality signature" for each imagingmodality. Ultimately, these automated textual and spatial prompts guide SAM toexecute precise segmentation, creating a fully automated and unified pipeline.Extensive experiments on 12 diverse datasets across 11 critical segmentationcategories show that CLAPS achieves performance on par with specialized expertmodels while surpassing existing benchmarks across most metrics, demonstratingits broad generalizability as a foundation model.</description>
      <author>example@mail.com (Zhihao Zhao, Yinzheng Zhao, Junjie Yang, Xiangtong Yao, Quanmin Liang, Shahrooz Faghihroohi, Kai Huang, Nassir Navab, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2509.08618v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Semantic Aggregation Leveraging Foundation Model for Generalizable Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.08570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages and 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进多模态模型在医疗图像分割领域性能的方法，通过解决文本提示与医学视觉特征之间的语义鸿沟和特征分散问题，显著提升了模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态模型在自然图像分割方面表现优异，但在医疗领域应用时性能不佳。研究发现这种性能差距主要源于多模态融合的挑战，特别是抽象文本提示与细粒度医学视觉特征之间的显著语义鸿沟，以及由此导致的特征分散问题。&lt;h4&gt;目的&lt;/h4&gt;从语义聚合的角度重新审视医疗图像分割问题，设计有效机制弥合语义鸿沟并缓解特征分散，提高多模态模型在医疗领域的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两种关键机制：1) 期望最大化（EM）聚合机制，通过动态将特征聚类为紧凑的语义中心来缓解特征分散，增强跨模态对应；2) 文本引导的像素解码器，利用领域不变的文本知识弥合语义鸿沟，有效指导深度视觉表示。这两种机制的协同作用显著提升了模型的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在公共心脏和眼底数据集上的大量实验表明，所提出的方法在多个领域泛化基准上持续优于现有的最先进方法，证明了其在医疗图像分割领域的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过EM聚合机制和文本引导的像素解码器的协同作用，成功解决了医疗图像分割中的多模态融合挑战，显著提高了模型的性能和泛化能力，为医疗图像分割领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态模型在自然图像分割方面取得了显著成功，但应用于医疗领域时往往表现不佳。通过广泛研究，我们将这种性能差距归因于多模态融合的挑战，主要是抽象文本提示与细粒度医学视觉特征之间的显著语义鸿沟，以及由此导致的特征分散问题。为解决这些问题，我们从语义聚合的角度重新审视问题。具体而言，我们提出了期望最大化（EM）聚合机制和文本引导的像素解码器。前者通过动态将特征聚类为紧凑的语义中心来缓解特征分散，增强跨模态对应。后者旨在利用领域不变的文本知识弥合语义鸿沟，有效指导深度视觉表示。这两种机制之间的协同作用显著提高了模型的泛化能力。在公共心脏和眼底数据集上的大量实验表明，我们的方法在多个领域泛化基准上持续优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal models have achieved remarkable success in natural imagesegmentation, yet they often underperform when applied to the medical domain.Through extensive study, we attribute this performance gap to the challenges ofmultimodal fusion, primarily the significant semantic gap between abstracttextual prompts and fine-grained medical visual features, as well as theresulting feature dispersion. To address these issues, we revisit the problemfrom the perspective of semantic aggregation. Specifically, we propose anExpectation-Maximization (EM) Aggregation mechanism and a Text-Guided PixelDecoder. The former mitigates feature dispersion by dynamically clusteringfeatures into compact semantic centers to enhance cross-modal correspondence.The latter is designed to bridge the semantic gap by leveragingdomain-invariant textual knowledge to effectively guide deep visualrepresentations. The synergy between these two mechanisms significantlyimproves the model's generalization ability. Extensive experiments on publiccardiac and fundus datasets demonstrate that our method consistentlyoutperforms existing SOTA approaches across multiple domain generalizationbenchmarks.</description>
      <author>example@mail.com (Wenjun Yu, Yinchen Zhou, Jia-Xuan Jiang, Shubin Zeng, Yuee Li, Zhong Wang)</author>
      <guid isPermaLink="false">2509.08570v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>HuMo: Human-Centric Video Generation via Collaborative Multi-Modal Conditioning</title>
      <link>http://arxiv.org/abs/2509.08519v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了HuMo，一个用于协作多模态控制的人为中心视频生成统一框架，解决了现有方法在协调多模态输入方面的挑战，包括数据稀缺和子任务协作困难。&lt;h4&gt;背景&lt;/h4&gt;现有的人为中心视频生成方法在协调文本、图像和音频等多模态输入时面临两大挑战：缺乏具有配对三元条件的训练数据，以及难以协调主体保持和音视频同步这两个子任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的HCVG框架，有效解决多模态输入的协调问题，实现高质量的人视频合成。&lt;h4&gt;方法&lt;/h4&gt;1. 构建高质量数据集，包含多样且配对的文本、参考图像和音频；2. 提出两阶段渐进式多模态训练范式；3. 主体保持任务采用最小侵入性图像注入策略；4. 音视频同步任务结合音频交叉注意力层和预测焦点策略；5. 推理阶段使用时间自适应的无分类器引导策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，HuMo在子任务中超越了专门的最先进方法，证明了其作为协作多模态条件HCVG统一框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;HuMo成功解决了HCVG领域中的多模态协调挑战，为高质量人视频生成提供了一个统一的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;以人为中心的视频生成方法旨在从多模态输入（包括文本、图像和音频）合成人类视频。由于两个挑战，现有方法难以有效协调这些异构模态：缺乏具有配对三元条件的训练数据，以及难以协调多模态输入下的主体保持和音视频同步子任务。在这项工作中，我们提出了HuMo，一个用于协作多模态控制的统一HCVG框架。针对第一个挑战，我们构建了一个高质量的数据集，包含多样且配对的文本、参考图像和音频。针对第二个挑战，我们提出了一种两阶段渐进式多模态训练范式，带有任务特定策略。对于主体保持任务，为了保持基础模型的提示跟随和视觉生成能力，我们采用最小侵入性图像注入策略。对于音视频同步任务，除了常用的音频交叉注意力层外，我们还提出了一种预测焦点策略，隐式引导模型将音频与面部区域关联。为了实现跨多模态输入的可控性联合学习，在已获取能力的基础上，我们逐步融入音视频同步任务。在推理阶段，为了灵活和细粒度的多模态控制，我们设计了一种时间自适应的无分类器引导策略，在去噪步骤中动态调整引导权重。大量实验结果表明，HuMo在子任务中超越了专门的最先进方法，建立了协作多模态条件HCVG的统一框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human-Centric Video Generation (HCVG) methods seek to synthesize human videosfrom multimodal inputs, including text, image, and audio. Existing methodsstruggle to effectively coordinate these heterogeneous modalities due to twochallenges: the scarcity of training data with paired triplet conditions andthe difficulty of collaborating the sub-tasks of subject preservation andaudio-visual sync with multimodal inputs. In this work, we present HuMo, aunified HCVG framework for collaborative multimodal control. For the firstchallenge, we construct a high-quality dataset with diverse and paired text,reference images, and audio. For the second challenge, we propose a two-stageprogressive multimodal training paradigm with task-specific strategies. For thesubject preservation task, to maintain the prompt following and visualgeneration abilities of the foundation model, we adopt the minimal-invasiveimage injection strategy. For the audio-visual sync task, besides the commonlyadopted audio cross-attention layer, we propose a focus-by-predicting strategythat implicitly guides the model to associate audio with facial regions. Forjoint learning of controllabilities across multimodal inputs, building onpreviously acquired capabilities, we progressively incorporate the audio-visualsync task. During inference, for flexible and fine-grained multimodal control,we design a time-adaptive Classifier-Free Guidance strategy that dynamicallyadjusts guidance weights across denoising steps. Extensive experimental resultsdemonstrate that HuMo surpasses specialized state-of-the-art methods insub-tasks, establishing a unified framework for collaborativemultimodal-conditioned HCVG. Project Page:https://phantom-video.github.io/HuMo.</description>
      <author>example@mail.com (Liyang Chen, Tianxiang Ma, Jiawei Liu, Bingchuan Li, Zhuowei Chen, Lijie Liu, Xu He, Gen Li, Qian He, Zhiyong Wu)</author>
      <guid isPermaLink="false">2509.08519v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Two Sides of the Same Optimization Coin: Model Degradation and Representation Collapse in Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2509.08401v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对图基础模型中的两个关键问题（模型退化和表示崩溃）提出MoT解决方案，通过信息修补和正则化修补显著提高了模型在多领域任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;图基础模型受大型语言模型成功启发，旨在从多领域图(TAGs)中学习最优嵌入以实现跨任务泛化。图VQ-MAE因其能将多领域拓扑和文本属性编码到具有清晰语义边界的离散嵌入空间而表现突出。&lt;h4&gt;目的&lt;/h4&gt;解决图基础模型中由领域泛化冲突导致的模型退化和表示崩溃问题，提高模型在预训练过程中的优化效果和下游任务性能。&lt;h4&gt;方法&lt;/h4&gt;提出MoT(Mixture-of-Tinkers)解决方案，包含：(1)信息修补：利用边缘语义融合策略和域感知路由的混合代码本提高信息容量；(2)正则化修补：使用两个额外正则化改进梯度监督。MoT作为灵活架构遵循图基础模型扩展规律，提供可控制模型规模。&lt;h4&gt;主要发现&lt;/h4&gt;图基础模型存在两个相互关联的问题：模型退化（编码器和代码本无法捕获输入多样性）和表示崩溃（隐藏嵌入和代码本向量无法保持语义可分性），这些问题共同导致解码器生成低质量重建监督，引发预训练优化困境。这些问题归因于信息瓶颈和正则化不足。&lt;h4&gt;结论&lt;/h4&gt;MoT在6个领域的22个数据集上实验表明，其在监督、少样本和零样本场景中显著优于现有最先进方法，为图基础模型优化提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型，受大型语言模型成功的启发，旨在从多领域图(TAGs)中学习最优嵌入，以实现下游跨任务泛化能力。在我们的研究中，图VQ-MAE在日益多样化的图基础模型架构中脱颖而出。这归功于其能够将多领域的拓扑和文本属性联合编码到具有清晰语义边界的离散嵌入空间的能力。尽管其潜力巨大，领域泛化冲突导致了难以察觉的陷阱。在本文中，我们实例化了其中的两个陷阱，它们就像同一枚图基础模型优化硬币的两面 - 第一面：模型退化：编码器和代码本无法捕获输入的多样性；第二面：表示崩溃：由于来自窄表示子空间的约束，隐藏嵌入和代码本向量无法保持语义可分性。这两个陷阱（面）共同损害了解码器并生成低质量的重建监督，导致预训练过程中的图基础模型优化困境（硬币）。通过实证研究，我们将上述挑战归因于信息瓶颈和正则化不足。为解决这些问题，我们提出了MoT(Mixture-of-Tinkers)-(1)针对两个问题的信息修补，利用边缘语义融合策略和具有域感知路由的混合代码本来提高信息容量。(2)针对优化硬币的正则化修补，使用两个额外的正则化来进一步改进我们提出的信息修补中的梯度监督。值得注意的是，作为一种灵活的架构，MoT遵循图基础模型的扩展规律，提供可控制的模型规模。与现有最先进基线相比，在6个领域的22个数据集上的实验表明，MoT在监督、少样本和零样本场景中实现了显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models, inspired by the success of LLMs, are designed tolearn the optimal embedding from multi-domain TAGs for the downstreamcross-task generalization capability. During our investigation, graph VQ-MAEstands out among the increasingly diverse landscape of GFM architectures. Thisis attributed to its ability to jointly encode topology and textual attributesfrom multiple domains into discrete embedding spaces with clear semanticboundaries. Despite its potential, domain generalization conflicts causeimperceptible pitfalls. In this paper, we instantiate two of them, and they arejust like two sides of the same GFM optimization coin - Side 1 ModelDegradation: The encoder and codebook fail to capture the diversity of inputs;Side 2 Representation Collapse: The hidden embedding and codebook vector failto preserve semantic separability due to constraints from narrow representationsubspaces. These two pitfalls (sides) collectively impair the decoder andgenerate the low-quality reconstructed supervision, causing the GFMoptimization dilemma during pre-training (coin). Through empiricalinvestigation, we attribute the above challenges to Information Bottleneck andRegularization Deficit. To address them, we propose MoT (Mixture-of-Tinkers) -(1) Information Tinker for Two Pitfalls, which utilizes an edge-wise semanticfusion strategy and a mixture-of-codebooks with domain-aware routing to improveinformation capacity. (2) Regularization Tinker for Optimization Coin, whichutilizes two additional regularizations to further improve gradient supervisionin our proposed Information Tinker. Notably, as a flexible architecture, MoTadheres to the scaling laws of GFM, offering a controllable model scale.Compared to SOTA baselines, experiments on 22 datasets across 6 domainsdemonstrate that MoT achieves significant improvements in supervised, few-shot,and zero-shot scenarios.</description>
      <author>example@mail.com (Xunkai Li, Daohan Su, Sicheng Liu, Ru Zhang, Zhenjun Li, Bing Zhou, Rong-Hua Li, Guoren Wang)</author>
      <guid isPermaLink="false">2509.08401v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking the Backbone in Class Imbalanced Federated Source Free Domain Adaptation: The Utility of Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2509.08372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE ICIP 2025 Satellite Workshop 1: Edge  Intelligence: Smart, Efficient, and Scalable Solutions for IoT, Wearables,  and Embedded Devices (SEEDS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了类不平衡联邦无源域适应(CI-FFREEDA)问题，提出使用冻结的视觉基础模型(VFM)替换FFREEDA主干网络，以提高整体准确性并减少计算和通信成本。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(FL)是一种在保护数据隐私的同时协作训练模型的框架。最近的研究集中在联邦无源域适应(FFREEDA)上，其中客户端持有的目标域数据保持未标记状态，服务器只能在预训练期间访问源域数据。&lt;h4&gt;目的&lt;/h4&gt;将FFREEDA框架扩展到更复杂的类不平衡联邦无源域适应(CI-FFREEDA)场景，考虑源域和目标域中的类不平衡问题，以及源域和目标域之间以及目标客户端之间的标签偏移。&lt;h4&gt;方法&lt;/h4&gt;提出用冻结的视觉基础模型(VFM)替换FFREEDA主干网络，改进网络内部的特征提取器，而不是增强聚合和域适应方法，从而在不进行大量参数调整的情况下提高整体准确性，并减少联邦学习中的计算和通信成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，VFM有效缓解了域差距、类不平衡甚至目标客户端之间的非IID性问题。&lt;h4&gt;结论&lt;/h4&gt;强大的特征提取器，而不是复杂的适应或FL方法，是现实世界中联邦学习成功的关键。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)提供了一种在保护每个客户端数据隐私的同时协作训练模型的框架。最近，研究集中在联邦无源域适应(FFREEDA)上，这是一个更现实的场景，其中客户端持有的目标域数据保持未标记状态，服务器只能在预训练期间访问源域数据。我们将这个框架扩展到一个更复杂和现实的设置：类不平衡联邦无源域适应(CI-FFREEDA)，它考虑了源域和目标域中的类不平衡问题，以及源域和目标域之间以及目标客户端之间的标签偏移。在我们的实验设置中复制现有方法使我们重新思考重点，从增强聚合和域适应方法转向改进网络本身的特征提取器。我们提出用冻结的视觉基础模型(VFM)替换FFREEDA主干网络，从而在不进行大量参数调整的情况下提高整体准确性，并减少联邦学习中的计算和通信成本。我们的实验结果表明，VFM有效缓解了域差距、类不平衡甚至目标客户端之间的非IID性问题，这表明强大的特征提取器，而不是复杂的适应或FL方法，是现实世界中FL成功的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) offers a framework for training modelscollaboratively while preserving data privacy of each client. Recently,research has focused on Federated Source-Free Domain Adaptation (FFREEDA), amore realistic scenario wherein client-held target domain data remainsunlabeled, and the server can access source domain data only duringpre-training. We extend this framework to a more complex and realistic setting:Class Imbalanced FFREEDA (CI-FFREEDA), which takes into account classimbalances in both the source and target domains, as well as label shiftsbetween source and target and among target clients. The replication of existingmethods in our experimental setup lead us to rethink the focus from enhancingaggregation and domain adaptation methods to improving the feature extractorswithin the network itself. We propose replacing the FFREEDA backbone with afrozen vision foundation model (VFM), thereby improving overall accuracywithout extensive parameter tuning and reducing computational and communicationcosts in federated learning. Our experimental results demonstrate that VFMseffectively mitigate the effects of domain gaps, class imbalances, and evennon-IID-ness among target clients, suggesting that strong feature extractors,not complex adaptation or FL methods, are key to success in the real-world FL.</description>
      <author>example@mail.com (Kosuke Kihara, Junki Mori, Taiki Miyagawa, Akinori F. Ebihara)</author>
      <guid isPermaLink="false">2509.08372v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>An Open Benchmark Dataset for GeoAI Foundation Models for Oil Palm Mapping in Indonesia</title>
      <link>http://arxiv.org/abs/2509.08303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究创建了一个印度尼西亚油棕种植园和相关土地覆盖类型的开放获取地理空间数据集，通过专家对2020-2024年高分辨率卫星图像进行标注，支持可持续发展和森林砍伐监测。&lt;h4&gt;背景&lt;/h4&gt;油棕种植是印度尼西亚森林砍伐的主要原因之一，需要详细可靠的地图来跟踪和解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;创建一个详细的油棕种植地图，支持可持续发展努力和新兴监管框架，提高土地覆盖类型绘图的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过专家对2020年至2024年高分辨率卫星图像进行标注，创建基于多边形的全面标注数据集，使用多解释者共识和实地验证确保质量，采用大网格全面数字化方法，适合训练传统卷积神经网络和新型地理空间基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集填补了遥感训练数据的关键空白，提高了土地覆盖类型绘图的准确性，支持油棕扩张的透明监测。&lt;h4&gt;结论&lt;/h4&gt;这一资源通过支持油棕扩张的透明监测，有助于实现全球减少森林砍伐的目标，并遵循FAIR数据原则。&lt;h4&gt;翻译&lt;/h4&gt;油棕种植仍然是印度尼西亚森林砍伐的主要原因之一。为了更好地跟踪和解决这一挑战，需要详细可靠的地图来支持可持续发展努力和新兴监管框架。我们提出了一个印度尼西亚油棕种植园和相关土地覆盖类型的开放获取地理空间数据集，通过专家对2020年至2024年高分辨率卫星图像进行标注制作而成。该数据集提供了基于多边形的全面标注，涵盖各种农业生态区，并包含分层分类法，区分油棕种植阶段以及类似的多年生作物。通过多解释者共识和实地验证确保质量。该数据集使用大网格的全面数字化创建，适合训练和测试传统卷积神经网络以及新型地理空间基础模型。在CC-BY许可下发布，它填补了遥感训练数据的关键空白，旨在提高土地覆盖类型绘图的准确性。通过支持油棕扩张的透明监测，这一资源有助于实现全球减少森林砍伐的目标，并遵循FAIR数据原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oil palm cultivation remains one of the leading causes of deforestation inIndonesia. To better track and address this challenge, detailed and reliablemapping is needed to support sustainability efforts and emerging regulatoryframeworks. We present an open-access geospatial dataset of oil palmplantations and related land cover types in Indonesia, produced through expertlabeling of high-resolution satellite imagery from 2020 to 2024. The datasetprovides polygon-based, wall-to-wall annotations across a range ofagro-ecological zones and includes a hierarchical typology that distinguishesoil palm planting stages as well as similar perennial crops. Quality wasensured through multi-interpreter consensus and field validation. The datasetwas created using wall-to-wall digitization over large grids, making itsuitable for training and benchmarking both conventional convolutional neuralnetworks and newer geospatial foundation models. Released under a CC-BYlicense, it fills a key gap in training data for remote sensing and aims toimprove the accuracy of land cover types mapping. By supporting transparentmonitoring of oil palm expansion, the resource contributes to globaldeforestation reduction goals and follows FAIR data principles.</description>
      <author>example@mail.com (M. Warizmi Wafiq, Peter Cutter, Ate Poortinga, Daniel Marc G. dela Torre, Karis Tenneson, Vanna Teck, Enikoe Bihari, Chanarun Saisaward, Weraphong Suaruang, Andrea McMahon, Andi Vika Faradiba Muin, Karno B. Batiran, Chairil A, Nurul Qomar, Arya Arismaya Metananda, David Ganz, David Saah)</author>
      <guid isPermaLink="false">2509.08303v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery</title>
      <link>http://arxiv.org/abs/2509.08032v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SciGPT（科学领域专用基础模型）和ScienceBench（科学LLMs评估基准），解决了科学文献增长带来的知识综合挑战，以及通用LLMs在科学领域的局限性。&lt;h4&gt;背景&lt;/h4&gt;科学文献呈指数级增长，研究人员难以高效综合知识。通用大型语言模型在文本处理方面有潜力，但往往无法捕捉科学领域的特定细节（如技术术语、方法严谨性），难以处理复杂的科学任务，限制了其在跨学科研究中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对科学文献理解的基础模型和相应的评估基准，以解决通用LLMs在科学领域的不足，促进AI增强的科学发现。&lt;h4&gt;方法&lt;/h4&gt;SciGPT基于Qwen3架构构建，包含三个关键创新：(1)通过两阶段流程进行低成本领域蒸馏，平衡性能和效率；(2)稀疏专家混合注意力机制，可将32,000个长文档推理的内存消耗减少55%；(3)知识感知适应，整合领域本体论以弥合跨学科知识差距。&lt;h4&gt;主要发现&lt;/h4&gt;在ScienceBench上的实验结果表明，SciGPT在序列标注、生成和推理等核心科学任务上优于GPT-4o。它在未见过的科学任务中也表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SciGPT具有促进AI增强科学发现的潜力，能够有效处理科学文献并支持跨学科研究。&lt;h4&gt;翻译&lt;/h4&gt;科学文献呈指数级增长，为研究人员高效综合知识造成了关键瓶颈。虽然通用大型语言模型在文本处理方面显示出潜力，但它们往往无法捕捉科学领域的特定细微差别（如技术术语、方法严谨性），并且在处理复杂的科学任务时遇到困难，限制了它们在跨学科研究中的实用性。为解决这些不足，本文提出了SciGPT（一个针对科学文献理解的基础模型）和ScienceBench（一个专门用于评估科学LLMs的开源基准）。SciGPT基于Qwen3架构构建，包含三个关键创新：(1)通过两阶段流程进行低成本领域蒸馏，平衡性能和效率；(2)稀疏专家混合注意力机制，可将32,000个长文档推理的内存消耗减少55%；(3)知识感知适应，整合领域本体论以弥合跨学科知识差距。在ScienceBench上的实验结果表明，SciGPT在序列标注、生成和推理等核心科学任务上优于GPT-4o。它在未见过的科学任务中也表现出强大的鲁棒性，验证了其促进AI增强科学发现的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific literature is growing exponentially, creating a criticalbottleneck for researchers to efficiently synthesize knowledge. Whilegeneral-purpose Large Language Models (LLMs) show potential in text processing,they often fail to capture scientific domain-specific nuances (e.g., technicaljargon, methodological rigor) and struggle with complex scientific tasks,limiting their utility for interdisciplinary research. To address these gaps,this paper presents SciGPT, a domain-adapted foundation model for scientificliterature understanding and ScienceBench, an open source benchmark tailored toevaluate scientific LLMs.  Built on the Qwen3 architecture, SciGPT incorporates three key innovations:(1) low-cost domain distillation via a two-stage pipeline to balanceperformance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attentionmechanism that cuts memory consumption by 55\% for 32,000-token long-documentreasoning; and (3) knowledge-aware adaptation integrating domain ontologies tobridge interdisciplinary knowledge gaps.  Experimental results on ScienceBench show that SciGPT outperforms GPT-4o incore scientific tasks including sequence labeling, generation, and inference.It also exhibits strong robustness in unseen scientific tasks, validating itspotential to facilitate AI-augmented scientific discovery.</description>
      <author>example@mail.com (Fengyu She, Nan Wang, Hongfei Wu, Ziyi Wan, Jingmian Wang, Chang Wang)</author>
      <guid isPermaLink="false">2509.08032v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>MCTED: A Machine-Learning-Ready Dataset for Digital Elevation Model Generation From Mars Imagery</title>
      <link>http://arxiv.org/abs/2509.08027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个名为MCTED的新数据集，专门用于火星数字高程模型预测任务，包含80,898个处理后的样本，并开源了相关代码。研究团队还比较了专门训练的小型U-Net模型与通用深度估计基础模型在该任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;火星勘测轨道器使用CTX仪器收集了高分辨率的火星正射影像和数字高程模型数据，但原始数据中存在伪影和缺失数据点问题，影响了大规模DEM处理的质量。&lt;h4&gt;目的&lt;/h4&gt;创建一个适用于机器学习应用的火星数字高程模型预测数据集，解决原始数据中的质量问题，并提供一个基准来评估专门训练模型与通用深度估计模型的性能差异。&lt;h4&gt;方法&lt;/h4&gt;1. 使用全面管道处理高分辨率火星正射影像和DEM对；2. 开发工具解决原始数据中的伪影和缺失数据点问题；3. 将处理后的样本分为不重叠的训练集和验证集；4. 每个样本包含光学图像块、DEM块和两个掩码块；5. 提供数据集的统计分析；6. 训练小型U-Net架构并与DepthAnythingV2模型比较&lt;h4&gt;主要发现&lt;/h4&gt;专门在MCTED数据集上训练的小型U-Net模型在火星高程预测任务上的性能优于通用深度估计基础模型DepthAnythingV2的零样本性能。&lt;h4&gt;结论&lt;/h4&gt;MCTED数据集为火星数字高程模型预测任务提供了高质量、多样化的训练资源，专门训练的模型即使在规模较小的情况下也能超越通用深度估计模型的表现。数据集和代码已完全开源，供研究社区使用。&lt;h4&gt;翻译&lt;/h4&gt;这项工作为火星数字高程模型预测任务提出了一个名为MCTED的新数据集，适用于机器学习应用。该数据集是使用一个全面的管道生成的，用于处理来自Day等人提供的高分辨率火星正射影像和DEM对，最终得到包含80,898个数据样本的数据集。源图像是由火星勘测轨道器使用CTX仪器收集的数据，提供了火星表面非常多样和全面的覆盖。鉴于大规模DEM处理管道的复杂性，原始数据中经常存在伪影和缺失数据点，为此开发了工具来解决或减轻其影响。我们将处理后的样本分为训练集和验证集，确保两个集合中的样本不覆盖相互区域，避免数据泄露。数据集中的每个样本由光学图像块、DEM块和两个掩码块表示，指示原始缺失或被修改的值。我们提供了生成数据集的统计见解，包括样本的空间分布、高程值、坡度等的分布。最后，我们在MCTED数据集上训练了一个小型U-Net架构，并将其在高程预测任务上的性能与单目深度估计基础模型DepthAnythingV2进行了比较。我们发现即使是一个专门在此数据集上训练的非常小的架构，也能击败像DepthAnythingV2这样的深度估计基础模型的零样本性能。我们将数据集及其生成代码完全开源在公共仓库中。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决火星高分辨率数字高程模型(DEM)生成缺乏专用机器学习数据集的问题。这个问题很重要，因为DEM对火星科学研究和探索任务至关重要，包括着陆点映射、古代水文过程建模等。传统DEM生成方法资源密集，需要立体图像对，且延迟时间长，而能从单张图像生成DEM的方法可以解决这些缺点，但目前缺乏适合此任务的公开数据集。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有火星高程图数据集的局限性，发现它们存在碎片化、空值多、噪声大等问题。他们注意到CTX仪器获取的图像具有全球覆盖和高分辨率特性，适合单目深度估计。作者借鉴了NASA Ames立体管道生成的DEM数据，但针对其质量问题开发了专门的处理管道。他们还参考了现有深度估计数据集，但指出这些数据集主要包含地面视角，而MCTED提供了纯粹空中视角的数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个机器学习就绪的数据集，使研究人员能够训练模型从单张火星图像生成DEM。整体流程包括：1)从Day等人的仓库获取原始数据；2)数据质量控制(样本选择、初步地图填充、垂直化)；3)处理缺失值和异常值；4)数据分块和选择；5)确保训练和验证集之间无数据泄漏；6)提供数据集统计分析；7)训练基线模型并与现有模型比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建MCTED数据集，包含80,8个清理策划的图像-DEM对；2)开发全面的数据处理管道解决原始数据质量问题；3)提供数据集的详细统计分析；4)展示小型U-Net架构能超越最先进的MDE基础模型；5)完全开源数据集和代码。相比之前工作，MCTED专注于单图像DEM生成，解决了数据集碎片化问题，提供纯空中视角数据，并证明专门训练的小模型能优于通用的大模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MCTED数据集填补了深度估计和遥感数据集之间的空白，通过创建高质量的策划数据和专门的处理管道，使研究人员能够从单张火星图像生成更准确的数字高程模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents a new dataset for the Martian digital elevation modelprediction task, ready for machine learning applications called MCTED. Thedataset has been generated using a comprehensive pipeline designed to processhigh-resolution Mars orthoimage and DEM pairs from Day et al., yielding adataset consisting of 80,898 data samples. The source images are data gatheredby the Mars Reconnaissance Orbiter using the CTX instrument, providing a verydiverse and comprehensive coverage of the Martian surface. Given the complexityof the processing pipelines used in large-scale DEMs, there are often artefactsand missing data points in the original data, for which we developed tools tosolve or mitigate their impact. We divide the processed samples into trainingand validation splits, ensuring samples in both splits cover no mutual areas toavoid data leakage. Every sample in the dataset is represented by the opticalimage patch, DEM patch, and two mask patches, indicating values that wereoriginally missing or were altered by us. This allows future users of thedataset to handle altered elevation regions as they please. We providestatistical insights of the generated dataset, including the spatialdistribution of samples, the distributions of elevation values, slopes andmore. Finally, we train a small U-Net architecture on the MCTED dataset andcompare its performance to a monocular depth estimation foundation model,DepthAnythingV2, on the task of elevation prediction. We find that even a verysmall architecture trained on this dataset specifically, beats a zero-shotperformance of a depth estimation foundation model like DepthAnythingV2. Wemake the dataset and code used for its generation completely open source inpublic repositories.</description>
      <author>example@mail.com (Rafał Osadnik, Pablo Gómez, Eleni Bohacek, Rickbir Bahia)</author>
      <guid isPermaLink="false">2509.08027v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Towards Post-mortem Data Management Principles for Generative AI</title>
      <link>http://arxiv.org/abs/2509.07375v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了逝者数据所有权这一较少被关注的维度，分析了当前身后数据管理和隐私权利的现状，提出了三条保护逝者数据权利的管理原则，并为政策制定者和隐私从业者提供了实践建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型、大型语言模型和AI代理系统严重依赖大量用户数据，这引发了关于所有权、版权和潜在损害的持续担忧。&lt;h4&gt;目的&lt;/h4&gt;探索逝者数据所有权这一相关但较少被研究的维度，并提出保护逝者数据权利的原则。&lt;h4&gt;方法&lt;/h4&gt;检查当前身后数据管理和隐私权利的现状，分析主要科技公司的隐私政策和欧盟AI法案等法规。&lt;h4&gt;主要发现&lt;/h4&gt;提出了三条身后数据管理原则，以指导保护逝者数据权利。&lt;h4&gt;结论&lt;/h4&gt;讨论了未来工作方向，并为政策制定者和隐私从业者提供了关于如何部署这些原则，以及如何通过技术和解决方案在实践中落实和审计它们的建议。&lt;h4&gt;翻译&lt;/h4&gt;基础模型、大型语言模型和AI代理系统严重依赖大量用户数据。使用此类数据进行训练引发了关于所有权、版权和潜在损害的持续担忧。在本工作中，我们探索了一个相关但较少被研究的维度：逝者数据的所有权权利。我们检查了当前身后数据管理和隐私权利的现状，这由主要科技公司的隐私政策和欧盟AI法案等法规定义。基于此分析，我们提出了三条身后数据管理原则，以指导保护逝者数据权利。最后，我们讨论了未来工作方向，并为政策制定者和隐私从业者提供了建议，关于如何部署这些原则以及如何通过技术和解决方案在实践中落实和审计它们。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, large language models (LLMs), and agentic AI systems relyheavily on vast corpora of user data. The use of such data for training hasraised persistent concerns around ownership, copyright, and potential harms. Inthis work, we explore a related but less examined dimension: the ownershiprights of data belonging to deceased individuals. We examine the currentlandscape of post-mortem data management and privacy rights as defined by theprivacy policies of major technology companies and regulations such as the EUAI Act. Based on this analysis, we propose three post-mortem data managementprinciples to guide the protection of deceased individuals data rights.Finally, we discuss directions for future work and offer recommendations forpolicymakers and privacy practitioners on deploying these principles alongsidetechnological solutions to operationalize and audit them in practice.</description>
      <author>example@mail.com (Elina Van Kempen, Ismat Jarin, Chloe Georgiou)</author>
      <guid isPermaLink="false">2509.07375v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Graph Alignment via Dual-Pass Spectral Encoding and Latent Space Communication</title>
      <link>http://arxiv.org/abs/2509.09597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图对齐框架，解决了现有无监督方法中的两个关键局限性：节点区分度下降和潜在空间不对齐问题。该方法通过双通道编码器和几何感知功能映射模块，显著提升了图对齐的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;图对齐是识别多个图中对应节点的基础问题，对许多应用至关重要。大多数现有无监督方法将节点特征嵌入到潜在表示中，以实现无需真实对应关系的跨图比较。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图对齐框架，同时增强节点区分度并强制执行潜在空间之间的几何一致性，解决现有方法的两个关键局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 双通道编码器：结合低通和高通谱滤波器生成既具有结构感知又高度区分性的嵌入；2) 几何感知功能映射模块：学习图嵌入之间的双射和等距变换，确保不同表示之间一致的几何关系。&lt;h4&gt;主要发现&lt;/h4&gt;1) 在图基准测试上始终优于现有的无监督对齐基线；2) 对结构不一致性和具有挑战性的对齐场景表现出更强的鲁棒性；3) 在视觉-语言表示的无监督对齐中有效泛化到图域之外。&lt;h4&gt;结论&lt;/h4&gt;所提出的图对齐框架通过增强节点区分度和强制执行潜在空间之间的几何一致性，有效解决了现有无监督方法的局限性，在各种应用场景中表现出优越的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;图对齐-识别多个图中对应节点的问题-是许多应用的基础问题。大多数现有的无监督方法将节点特征嵌入到潜在表示中，以实现无需真实对应关系的跨图比较。然而，这些方法存在两个关键局限性：由于基于GNN的嵌入中的过平滑导致的节点区分度下降，以及由于结构噪声、特征异质性和训练不稳定性导致的潜在空间不对齐，最终导致不可靠的节点对应关系。我们提出了一种新的图对齐框架，同时增强节点区分度并强制执行潜在空间之间的几何一致性。我们的方法引入了一种双通道编码器，结合低通和高通谱滤波器来生成既具有结构感知又高度区分性的嵌入。为了解决潜在空间不对齐问题，我们纳入了一个感知几何的功能映射模块，该模块学习图嵌入之间的双射和等距变换，确保不同表示之间一致的几何关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph alignment-the problem of identifying corresponding nodes acrossmultiple graphs-is fundamental to numerous applications. Most existingunsupervised methods embed node features into latent representations to enablecross-graph comparison without ground-truth correspondences. However, thesemethods suffer from two critical limitations: the degradation of nodedistinctiveness due to oversmoothing in GNN-based embeddings, and themisalignment of latent spaces across graphs caused by structural noise, featureheterogeneity, and training instability, ultimately leading to unreliable nodecorrespondences. We propose a novel graph alignment framework thatsimultaneously enhances node distinctiveness and enforces geometric consistencyacross latent spaces. Our approach introduces a dual-pass encoder that combineslow-pass and high-pass spectral filters to generate embeddings that are bothstructure-aware and highly discriminative. To address latent spacemisalignment, we incorporate a geometry-aware functional map module that learnsbijective and isometric transformations between graph embeddings, ensuringconsistent geometric relationships across different representations. Extensiveexperiments on graph benchmarks demonstrate that our method consistentlyoutperforms existing unsupervised alignment baselines, exhibiting superiorrobustness to structural inconsistencies and challenging alignment scenarios.Additionally, comprehensive evaluation on vision-language benchmarks usingdiverse pretrained models shows that our framework effectively generalizesbeyond graph domains, enabling unsupervised alignment of vision and languagerepresentations.</description>
      <author>example@mail.com (Maysam Behmanesh, Erkan Turan, Maks Ovsjanikov)</author>
      <guid isPermaLink="false">2509.09597v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Towards Explainable Job Title Matching: Leveraging Semantic Textual Relatedness and Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2509.09522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了语义文本相关性在职位标题匹配中的应用，提出了一种结合密集句子嵌入和领域特定知识图谱的自监督混合架构，通过分层评估方法发现知识图谱增强的模型在高语义相关性区域表现显著提升。&lt;h4&gt;背景&lt;/h4&gt;语义文本相关性(STR)能够捕捉文本间超越表面词汇相似性的细微关系。在简历推荐系统中，职位标题匹配是一个关键挑战，其中重叠术语往往有限或具有误导性。&lt;h4&gt;目的&lt;/h4&gt;研究STR在职位标题匹配中的应用，提高语义对齐和可解释性，并通过分层评估方法更细致地分析模型在不同语义相关性区域的表现。&lt;h4&gt;方法&lt;/h4&gt;引入一种自监督混合架构，结合密集句子嵌入与领域特定知识图谱(KGs)；将STR分数连续体划分为低、中、高三个语义相关性区域进行分层评估；评估多种嵌入模型，包括和不通过图神经网络整合KGs的模型。&lt;h4&gt;主要发现&lt;/h4&gt;使用知识图谱增强的微调SBERT模型在高STR区域表现显著提升，RMSE比强基线模型降低25%；结合KGs与文本嵌入的方法有益；区域性能分析能揭示全局指标隐藏的优势和弱点，支持更针对性的模型选择。&lt;h4&gt;结论&lt;/h4&gt;结合知识图谱与文本嵌入的方法能有效提升语义文本相关性任务的表现，特别是在高语义相关性区域；区域性能分析对于理解模型行为和选择适合特定应用场景的模型至关重要。&lt;h4&gt;翻译&lt;/h4&gt;语义文本相关性(STR)捕捉了超越表面词汇相似性的文本间细微关系。在本研究中，我们在职位标题匹配的背景下探讨STR——这是简历推荐系统中的一个关键挑战，其中重叠术语通常有限或具有误导性。我们引入了一种自监督混合架构，结合密集句子嵌入和领域特定的知识图谱(KGs)，以提高语义对齐和可解释性。与之前在整体性能上评估模型的研究不同，我们的方法通过将STR分数连续体划分为不同的区域：低、中、高语义相关性，强调数据分层。这种分层评估能够在语义上有意义的子空间中对模型性能进行细粒度分析。我们评估了多种嵌入模型，包括通过图神经网络整合和不整合KGs的模型。结果显示，使用KGs增强的微调SBERT模型在高STR区域产生了一致的改进，RMSE比强基线模型降低了25%。我们的发现不仅突出了结合KGs与文本嵌入的好处，还强调了区域性能分析在理解模型行为中的重要性。这种细粒度方法揭示了全局指标隐藏的优势和弱点，并支持在人力资源(HR)系统和应用中进行更有针对性的模型选择，这些应用中公平性、可解释性和上下文匹配至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic Textual Relatedness (STR) captures nuanced relationships betweentexts that extend beyond superficial lexical similarity. In this study, weinvestigate STR in the context of job title matching - a key challenge inresume recommendation systems, where overlapping terms are often limited ormisleading. We introduce a self-supervised hybrid architecture that combinesdense sentence embeddings with domain-specific Knowledge Graphs (KGs) toimprove both semantic alignment and explainability. Unlike previous work thatevaluated models on aggregate performance, our approach emphasizes datastratification by partitioning the STR score continuum into distinct regions:low, medium, and high semantic relatedness. This stratified evaluation enablesa fine-grained analysis of model performance across semantically meaningfulsubspaces. We evaluate several embedding models, both with and without KGintegration via graph neural networks. The results show that fine-tuned SBERTmodels augmented with KGs produce consistent improvements in the high-STRregion, where the RMSE is reduced by 25% over strong baselines. Our findingshighlight not only the benefits of combining KGs with text embeddings, but alsothe importance of regional performance analysis in understanding modelbehavior. This granular approach reveals strengths and weaknesses hidden byglobal metrics, and supports more targeted model selection for use in HumanResources (HR) systems and applications where fairness, explainability, andcontextual matching are essential.</description>
      <author>example@mail.com (Vadim Zadykian, Bruno Andrade, Haithem Afli)</author>
      <guid isPermaLink="false">2509.09522v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Database Views as Explanations for Relational Deep Learning</title>
      <link>http://arxiv.org/abs/2509.09482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，用于解释关系数据库上的机器学习模型，特别是基于异构图神经网络(hetero-GNNs)的架构。该框架通过视图定义生成解释，突出显示对模型预测贡献最大的数据库部分。&lt;h4&gt;背景&lt;/h4&gt;近年来，在关系数据库上开发深度学习模型取得了显著进展，包括基于异构图神经网络和异构Transformer的架构。这些架构将数据库记录和链接转化为包含大量可学习参数的复杂数值表达式，导致模型难以解释。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够以人类可理解的方式解释关系数据库上机器学习模型如何做出预测的框架，特别关注异构GNNs模型。&lt;h4&gt;方法&lt;/h4&gt;通过适应Nash、Segoufin和Vianu提出的确定性概念建立全局abduction解释；开发避免穷举搜索的启发式算法；提出模型无关技术和针对异构GNNs的可学习掩码技术；允许调整确定性与简洁性之间的权衡并控制解释粒度。&lt;h4&gt;主要发现&lt;/h4&gt;在RelBench集合上的广泛实证研究表明，所提出的解释是有用的，且它们的生成过程是高效的。该框架能够突出显示对模型预测贡献最大的数据库部分，包括整个列、表间外键和相关元组组等。&lt;h4&gt;结论&lt;/h4&gt;该框架有效地解决了关系数据库上深度学习模型的可解释性问题，特别是对于异构GNNs模型，提供了既可解释又高效的解释方法。&lt;h4&gt;翻译&lt;/h4&gt;近年来，在关系数据库上开发深度学习模型取得了显著进展，包括基于异构图神经网络和异构Transformer的架构。实际上，这些架构描述了数据库记录和链接（如外键引用）如何转化为包含大量可学习参数的大型复杂数值表达式。这种复杂性使得很难用人类可理解的方式解释模型如何利用可用数据得出特定预测。我们提出了一种新颖的框架，用于解释关系数据库上的机器学习模型，其中解释是视图定义，突出显示对模型预测贡献最大的数据库部分。我们通过适应Nash、Segoufin和Vianu（2010）提出的确定性经典概念来建立这种全局abduction解释。除了调整确定性与简洁性之间的权衡外，该框架还允许通过采用不同的视图定义片段（如突出显示整个列、表之间的外键、相关元组组等）来控制粒度级别。我们研究了该框架在异构GNNs情况下的实现。我们开发了避免对所有数据库空间进行穷举搜索的启发式算法。我们提出了模型无关的技术，以及通过可学习掩码概念专门针对异构GNNs定制的技术。我们通过在RelBench集合上进行广泛的实证研究来评估该方法，涵盖各种领域和不同的记录级任务。结果表明所提出的解释是有用的，并且它们的生成是高效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been significant progress in the development ofdeep learning models over relational databases, including architectures basedon heterogeneous graph neural networks (hetero-GNNs) and heterogeneous graphtransformers. In effect, such architectures state how the database records andlinks (e.g., foreign-key references) translate into a large, complex numericalexpression, involving numerous learnable parameters. This complexity makes ithard to explain, in human-understandable terms, how a model uses the availabledata to arrive at a given prediction. We present a novel framework forexplaining machine-learning models over relational databases, whereexplanations are view definitions that highlight focused parts of the databasethat mostly contribute to the model's prediction. We establish such globalabductive explanations by adapting the classic notion of determinacy by Nash,Segoufin, and Vianu (2010). In addition to tuning the tradeoff betweendeterminacy and conciseness, the framework allows controlling the level ofgranularity by adopting different fragments of view definitions, such as oneshighlighting whole columns, foreign keys between tables, relevant groups oftuples, and so on. We investigate the realization of the framework in the caseof hetero-GNNs. We develop heuristic algorithms that avoid the exhaustivesearch over the space of all databases. We propose techniques that aremodel-agnostic, and others that are tailored to hetero-GNNs via the notion oflearnable masking. Our approach is evaluated through an extensive empiricalstudy on the RelBench collection, covering a variety of domains and differentrecord-level tasks. The results demonstrate the usefulness of the proposedexplanations, as well as the efficiency of their generation.</description>
      <author>example@mail.com (Agapi Rissaki, Ilias Fountalis, Wolfgang Gatterbauer, Benny Kimelfeld)</author>
      <guid isPermaLink="false">2509.09482v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Vejde: A Framework for Inductive Deep Reinforcement Learning Based on Factor Graph Color Refinement</title>
      <link>http://arxiv.org/abs/2509.09219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍并评估了Vejde框架，该框架结合数据抽象、图神经网络和强化学习，用于为具有丰富结构化状态的决策问题生成归纳策略函数。&lt;h4&gt;背景&lt;/h4&gt;在处理具有丰富结构化状态（如对象类别和关系）的决策问题时，需要有效的方法来表示和处理这些复杂结构。MDP状态被表示为关于实体的事实数据库，需要能够泛化到不同大小和结构问题的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理具有丰富结构化状态的决策问题的框架，生成能够在不同大小和结构问题上泛化的归纳策略函数。&lt;h4&gt;方法&lt;/h4&gt;Vejde框架结合三种技术：1)数据抽象将MDP状态表示为事实数据库；2)图神经网络将状态转换为二部图并通过神经消息传递映射到潜在状态；3)强化学习训练策略函数。状态和动作的因子化表示使代理能够处理不同大小和结构的问题。&lt;h4&gt;主要发现&lt;/h4&gt;在八个RDDL问题域（每个域十个实例）上测试显示，Vejde策略平均能够泛化到未见实例而不会显著降低分数，其表现接近于特定于实例的MLP代理。&lt;h4&gt;结论&lt;/h4&gt;Vejde框架成功结合了数据抽象、图神经网络和强化学习，为具有丰富结构化状态的决策问题提供了有效的归纳策略函数，能够在未见实例上表现良好。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍并评估了Vejde；一个结合数据抽象、图神经网络和强化学习的框架，用于为具有丰富结构化状态（如对象类别和关系）的决策问题生成归纳策略函数。MDP状态被表示为关于实体的事实数据库，Vejde将每个状态转换为二部图，并通过神经消息传递映射到潜在状态。状态和动作的因子化表示使Vejde代理能够处理不同大小和结构的问题。我们在RDDL定义的八个问题域上测试了Vejde代理，每个域有十个问题实例，其中策略使用监督学习和强化学习进行训练。为测试策略泛化，我们将问题实例分为两个集合，一个用于训练，另一个仅用于测试。Vejde代理在未见实例上的测试结果与在每个问题实例上训练的MLP代理以及在线规划算法Prost进行了比较。我们的结果表明，Vejde策略平均能够泛化到测试实例而不会显著降低分数。此外，归纳代理在未见测试实例上的平均分数接近于特定于实例的MLP代理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present and evaluate Vejde; a framework which combines data abstraction,graph neural networks and reinforcement learning to produce inductive policyfunctions for decision problems with richly structured states, such as objectclasses and relations. MDP states are represented as data bases of facts aboutentities, and Vejde converts each state to a bipartite graph, which is mappedto latent states through neural message passing. The factored representation ofboth states and actions allows Vejde agents to handle problems of varying sizeand structure. We tested Vejde agents on eight problem domains defined in RDDL,with ten problem instances each, where policies were trained using bothsupervised and reinforcement learning. To test policy generalization, weseparate problem instances in two sets, one for training and the other solelyfor testing. Test results on unseen instances for the Vejde agents werecompared to MLP agents trained on each problem instance, as well as the onlineplanning algorithm Prost. Our results show that Vejde policies in averagegeneralize to the test instances without a significant loss in score.Additionally, the inductive agents received scores on unseen test instancesthat on average were close to the instance-specific MLP agents.</description>
      <author>example@mail.com (Jakob Nyberg, Pontus Johnson)</author>
      <guid isPermaLink="false">2509.09219v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>CryptGNN: Enabling Secure Inference for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.09107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CryptGNN是一种安全有效的云端第三方图神经网络模型推理解决方案，通过分布式安全多方计算技术保护数据隐私和模型安全。&lt;h4&gt;背景&lt;/h4&gt;第三方图神经网络模型通过机器学习即服务(MLaaS)在云端被客户使用，存在数据隐私和模型安全风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种保护客户输入数据、图结构和模型参数安全的云端GNN推理解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用分布式安全多方计算(SMPC)技术实现安全消息传递和特征转换层，支持任意数量的SMPC方协作，无需可信服务器。&lt;h4&gt;主要发现&lt;/h4&gt;即使云中P个方中有P-1方合谋，CryptGNN也能保证安全性，理论分析和实验验证了其安全性和效率。&lt;h4&gt;结论&lt;/h4&gt;CryptGNN为云端第三方GNN模型推理提供了安全有效的解决方案，解决了数据隐私和模型安全问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了CryptGNN，一种用于云端第三方图神经网络(GNN)模型的安全有效推理解决方案，客户通过机器学习即服务(MLaaS)访问这些模型。CryptGNN的主要创新点是使用分布式安全多方计算(SMPC)技术实现安全消息传递和特征转换层。CryptGNN保护客户的输入数据和图结构，防止云服务提供商和第三方模型所有者获取；同时保护模型参数，防止云服务提供商和客户获取。CryptGNN可与任意数量的SMPC方协作，不需要可信服务器，并且即使云中P个方中有P-1方合谋，也能证明其安全性。理论分析和实证实验证明了CryptGNN的安全性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present CryptGNN, a secure and effective inference solution forthird-party graph neural network (GNN) models in the cloud, which are accessedby clients as ML as a service (MLaaS). The main novelty of CryptGNN is itssecure message passing and feature transformation layers using distributedsecure multi-party computation (SMPC) techniques. CryptGNN protects theclient's input data and graph structure from the cloud provider and thethird-party model owner, and it protects the model parameters from the cloudprovider and the clients. CryptGNN works with any number of SMPC parties, doesnot require a trusted server, and is provably secure even if P-1 out of Pparties in the cloud collude. Theoretical analysis and empirical experimentsdemonstrate the security and efficiency of CryptGNN.</description>
      <author>example@mail.com (Pritam Sen, Yao Ma, Cristian Borcea)</author>
      <guid isPermaLink="false">2509.09107v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>A Zero-Inflated Spatio-Temporal Model for Integrating Fishery-Dependent and Independent Data under Preferential Sampling</title>
      <link>http://arxiv.org/abs/2509.09336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一个新的时空模型，用于整合渔业独立数据(FID)和渔业依赖数据(FDD)，解决了生态数据中常见的零膨胀和优先抽样问题，并在欧洲沙丁鱼分布研究中得到验证，为渔业管理提供了实用见解。&lt;h4&gt;背景&lt;/h4&gt;海洋生态系统的可持续管理对维持健康渔业资源至关重要，需要准确评估物种分布模式。渔业科学中主要使用两种数据源：通过系统调查收集的渔业独立数据(FID)和从商业捕捞获得的渔业依赖数据(FDD)。虽然两者互补，但不同的抽样方案（系统抽样vs优先抽样）带来了整合挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的时空模型，有效整合FID和FDD数据，解决生态数据中常见的零膨胀和优先抽样(PS)问题，从而准确评估物种分布模式，为渔业管理提供科学依据。&lt;h4&gt;方法&lt;/h4&gt;研究采用六层结构的时空模型，区分存在-不存在数据和生物量观测，处理优先抽样偏差问题。通过模拟测试模型在不同PS场景下的参数估计准确性和优先信号检测能力，并将模型应用于葡萄牙大陆架南部欧洲沙丁鱼种群分布研究，整合了多种数据源及环境与船舶特定协变量。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明模型能在不同优先抽样场景下准确估计参数并检测优先信号。在沙丁鱼研究中，模型成功整合了不同数据源，揭示了沙丁鱼存在和生物量的时空变异性，为渔业管理提供了可操作的见解。&lt;h4&gt;结论&lt;/h4&gt;该时空模型为整合渔业独立数据和渔业依赖数据提供了有效框架，能够处理生态数据中的零膨胀和优先抽样问题，不仅在生态学研究中具有应用价值，还可广泛应用于其他学科的数据整合挑战。&lt;h4&gt;翻译&lt;/h4&gt;海洋生态系统的可持续管理对于维持健康的渔业资源至关重要，并受益于先进的科学工具来准确评估物种分布模式。在渔业科学中，使用两种主要数据源：通过系统调查收集的渔业独立数据(FID)，以及从商业捕捞活动中获得的渔业依赖数据(FDD)。虽然这些数据源提供互补信息，但它们不同的抽样方案——FID的系统抽样和FDD的优先抽样——带来了重大的整合挑战。本研究引入了一种新的时空模型，整合FID和FDD，解决了生态数据中常见的与零膨胀和优先抽样(PS)相关的挑战。该模型采用六层结构来区分存在-不存在数据和生物量观测，为受优先抽样偏差影响的生态学研究提供了稳健的框架。模拟结果证明了该模型在不同优先抽样场景下参数估计的准确性及其检测优先信号的能力。应用于研究葡萄牙大陆架南部欧洲沙丁鱼种群分布模式的案例，说明了该模型在整合不同数据源和纳入环境及船舶特定协变量方面的有效性。该模型揭示了沙丁鱼存在和生物量的时空变异性，为渔业管理提供了可操作的见解。除了生态学应用外，该框架在解决其他学科的数据整合挑战方面具有广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sustainable management of marine ecosystems is vital for maintaining healthyfishery resources, and benefits from advanced scientific tools to accuratelyassess species distribution patterns. In fisheries science, two primary datasources are used: fishery-independent data (FID), collected through systematicsurveys, and fishery-dependent data (FDD), obtained from commercial fishingactivities. While these sources provide complementary information, theirdistinct sampling schemes - systematic for FID and preferential for FDD - posesignificant integration challenges. This study introduces a novelspatio-temporal model that integrates FID and FDD, addressing challengesassociated with zero-inflation and preferential sampling (PS) common inecological data. The model employs a six-layer structure to differentiatebetween presence-absence and biomass observations, offering a robust frameworkfor ecological studies affected by PS biases. Simulation results demonstratethe model's accuracy in parameter estimation across diverse PS scenarios andits ability to detect preferential signals. Application to the study of thedistribution patterns of the European sardine populations along the southernPortuguese continental shelf illustrates the model's effectiveness inintegrating diverse data sources and incorporating environmental andvessel-specific covariates. The model reveals spatio-temporal variability insardine presence and biomass, providing actionable insights for fisheriesmanagement. Beyond ecology, this framework offers broad applicability to dataintegration challenges in other disciplines.</description>
      <author>example@mail.com (Daniela Silva, Raquel Menezes, Gonçalo Araújo, Ana Machado, Renato Rosa, Ana Moreno, Alexandra Silva, Susana Garrido)</author>
      <guid isPermaLink="false">2509.09336v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>DATE: Dynamic Absolute Time Enhancement for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2509.09263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了动态绝对时间增强(DATE)方法，通过时间戳注入机制(TIM)和语义引导的时间感知相似度采样(TASS)策略，有效提高了多模态大语言模型对长时间视频的理解能力，特别是在绝对时间理解和关键事件定位方面取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)仍然是一个基本挑战，特别是在需要精确时间推理和事件定位的任务中。现有方法通常采用统一帧采样和隐式位置编码来建模时间顺序，但这些方法难以处理长距离依赖关系，导致关键信息丢失和时间理解能力下降。&lt;h4&gt;目的&lt;/h4&gt;提高MLLMs的时间感知能力，改善对长时间视频的理解和事件定位性能，解决现有方法在处理长视频时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出动态绝对时间增强(DATE)方法，包括：1)时间戳注入机制(TIM)，将视频帧嵌入与文本时间戳令交错，构建连续时间参考系统；2)语义引导的时间感知相似度采样(TASS)策略，将视频采样问题重新表述为视觉-语言检索任务，并引入两阶段算法确保语义相关性和时间覆盖。&lt;h4&gt;主要发现&lt;/h4&gt;在绝对时间理解和关键事件定位方面取得了显著改进，在7B和72B模型的小时级视频基准测试中达到了最先进的性能。特别值得注意的是，7B模型在某些基准测试中甚至超过了许多72B模型的表现。&lt;h4&gt;结论&lt;/h4&gt;DATE方法有效解决了长视频理解中的时间感知问题，为提高多模态大语言模型对长时间视频的理解能力提供了新的有效解决方案，在保持模型规模较小的情况下也能获得优异性能。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解对多模态大语言模型(MLLMs)来说仍然是一个基本挑战，特别是在需要精确时间推理和事件定位的任务中。现有方法通常采用统一帧采样和依赖隐式位置编码来建模时间顺序。然而，这些方法难以处理长距离依赖关系，导致关键信息丢失和时间理解能力下降。在本文中，我们提出了动态绝对时间增强(DATE)，通过时间戳注入机制(TIM)和语义引导的时间感知相似度采样(TASS)策略来增强MLLMs的时间感知能力。具体而言，我们将视频帧嵌入与文本时间戳令交错，构建连续时间参考系统。我们进一步将视频采样问题重新表述为视觉-语言检索任务，并引入两阶段算法确保语义相关性和时间覆盖：将每个查询丰富为描述性字幕以更好地与视觉特征对齐，并采用相似度驱动的时间正则化贪婪策略采样关键事件。我们的方法在绝对时间理解和关键事件定位方面取得了显著改进，在7B和72B模型的小时级视频基准测试中达到了最先进的性能。特别是，我们的7B模型在某些基准测试中甚至超过了许多72B模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding remains a fundamental challenge for multimodal largelanguage models (MLLMs), particularly in tasks requiring precise temporalreasoning and event localization. Existing approaches typically adopt uniformframe sampling and rely on implicit position encodings to model temporal order.However, these methods struggle with long-range dependencies, leading tocritical information loss and degraded temporal comprehension. In this paper,we propose Dynamic Absolute Time Enhancement (DATE) that enhances temporalawareness in MLLMs through the Timestamp Injection Mechanism (TIM) and asemantically guided Temporal-Aware Similarity Sampling (TASS) strategy.Specifically, we interleave video frame embeddings with textual timestamptokens to construct a continuous temporal reference system. We furtherreformulate the video sampling problem as a vision-language retrieval task andintroduce a two-stage algorithm to ensure both semantic relevance and temporalcoverage: enriching each query into a descriptive caption to better align withthe vision feature, and sampling key event with a similarity-driven temporallyregularized greedy strategy. Our method achieves remarkable improvements w.r.t.absolute time understanding and key event localization, resulting instate-of-the-art performance among 7B and 72B models on hour-long videobenchmarks. Particularly, our 7B model even exceeds many 72B models on somebenchmarks.</description>
      <author>example@mail.com (Chao Yuan, Yang Yang, Yehui Yang, Zach Cheng)</author>
      <guid isPermaLink="false">2509.09263v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
      <link>http://arxiv.org/abs/2509.08538v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了MESH，一个专门用于系统评估大型视频模型(LVMs)中幻觉问题的新基准。研究指出，尽管LVMs在理解视频内容方面有所进步，但它们容易产生不准确或无关的描述。MESH基准采用问答框架，从基础到复杂评估LVMs的能力，并与人类理解视频的方式保持一致。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型(LLMs)和视觉模块的语义能力基础上，通过整合时间信息来更好地理解动态视频内容。然而，现有的视频幻觉评估基准严重依赖对视频内容的手动分类，忽略了人类自然解释视频时的感知过程。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一个系统化的基准来评估LVMs中的幻觉问题，该基准能够反映人类理解视频的自然过程，并全面检测模型在视频描述中的不准确或无关内容。&lt;h4&gt;方法&lt;/h4&gt;MESH基准采用问答框架，包含二进制和多选格式，并融入目标实例和陷阱实例。它采用自下而上的方法，评估基础物体、从粗到细的主体特征以及主体-动作对，与人类对视频的理解过程保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，虽然LVMs在识别基础物体和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，特别是在较长的视频中，它们产生幻觉的倾向显著增加。&lt;h4&gt;结论&lt;/h4&gt;MESH为识别视频中的幻觉问题提供了一种有效且全面的方法，能够系统化地评估LVMs在不同复杂度视频内容上的表现，并揭示其在处理更复杂视频场景时的局限性。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)通过整合时间信息，建立在大型语言模型(LLMs)和视觉模块的语义能力基础上，以更好地理解动态视频内容。尽管取得了进展，LVMs仍然容易产生幻觉——产生不准确或无关的描述。当前的视频幻觉评估基准严重依赖对视频内容的手动分类，忽略了人类自然解释视频时的感知过程。我们引入了MESH，一个旨在系统评估LVMs中幻觉问题的基准。MESH采用问答框架，包含二进制和多选格式，并融入目标实例和陷阱实例。它遵循自下而上的方法，评估基础物体、从粗到细的主体特征以及主体-动作对，与人类对视频的理解保持一致。我们证明MESH为识别视频中的幻觉问题提供了一种有效且全面的方法。我们的评估显示，尽管LVMs在识别基础物体和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，特别是在较长的视频中，它们产生幻觉的倾向显著增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Video Models (LVMs) build on the semantic capabilities of LargeLanguage Models (LLMs) and vision modules by integrating temporal informationto better understand dynamic video content. Despite their progress, LVMs areprone to hallucinations-producing inaccurate or irrelevant descriptions.Current benchmarks for video hallucination depend heavily on manualcategorization of video content, neglecting the perception-based processesthrough which humans naturally interpret videos. We introduce MESH, a benchmarkdesigned to evaluate hallucinations in LVMs systematically. MESH uses aQuestion-Answering framework with binary and multi-choice formats incorporatingtarget and trap instances. It follows a bottom-up approach, evaluating basicobjects, coarse-to-fine subject features, and subject-action pairs, aligningwith human video understanding. We demonstrate that MESH offers an effectiveand comprehensive approach for identifying hallucinations in videos. Ourevaluations show that while LVMs excel at recognizing basic objects andfeatures, their susceptibility to hallucinations increases markedly whenhandling fine details or aligning multiple actions involving various subjectsin longer videos.</description>
      <author>example@mail.com (Garry Yang, Zizhe Chen, Man Hon Wong, Haoyu Lei, Yongqiang Chen, Zhenguo Li, Kaiwen Zhou, James Cheng)</author>
      <guid isPermaLink="false">2509.08538v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>ProgD: Progressive Multi-scale Decoding with Dynamic Graphs for Joint Multi-agent Motion Forecasting</title>
      <link>http://arxiv.org/abs/2509.09210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为ProgD的渐进式多尺度解码策略，结合动态异构图场景建模，用于解决自动驾驶车辆中多代理运动预测的挑战，特别关注处理代理间交互的演变性质，有效降低了预测不确定性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆的安全规划依赖于对周围代理的准确运动预测。最近的进展已将预测技术从单个代理扩展到多个交互代理的联合预测，但现有方法忽略了这些交互的演变性质。&lt;h4&gt;目的&lt;/h4&gt;解决现有多代理运动预测方法忽略交互演变性质的问题，提出一种能够捕捉动态交互并降低预测不确定性的新方法。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种名为ProgD的渐进式多尺度解码策略，结合动态异构图场景建模。具体包括：1)使用动态异构图对场景进行渐进建模，捕捉未来场景中演变的社会交互；2)设计分解架构处理时空依赖关系，逐步消除多代理未来运动的不确定性；3)集成多尺度解码过程，改进场景建模和运动预测的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ProgD在INTERACTION多代理预测基准测试中取得了最先进的性能，排名第一，并在Argoverse 2多世界预测基准测试中表现优异。&lt;h4&gt;结论&lt;/h4&gt;通过结合动态异构图场景建模和多尺度解码过程，ProgD有效解决了多代理运动预测中交互演变性质被忽略的问题，显著提高了预测准确性，为自动驾驶车辆的安全规划提供了更可靠的决策依据。&lt;h4&gt;翻译&lt;/h4&gt;准确的周围代理运动预测对自动驾驶车辆的安全规划至关重要。最近的进展已经将预测技术从单个代理扩展到多个交互代理的联合预测，采用了各种策略来处理代理未来运动中的复杂交互。然而，这些方法忽略了这些交互的演变性质。为了解决这一局限性，我们提出了一种新颖的渐进式多尺度解码策略，称为ProgD，借助动态异构图场景建模。特别是，为了明确和全面地捕捉未来场景中演变的社会交互（考虑到其固有的不确定性），我们设计了使用动态异构图的场景渐进建模。随着这些动态异构图的展开，设计了一个分解架构来处理未来场景中的时空依赖关系，并逐步消除多个代理未来运动的不确定性。此外，还集成了多尺度解码过程，以改进未来场景建模和代理未来运动的一致性预测。所提出的ProgD在INTERACTION多代理预测基准测试中取得了最先进的性能，排名第一，并在Argoverse 2多世界预测基准测试中表现优异。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多代理联合运动预测问题，特别是在自动驾驶场景中准确预测多个交互代理（如车辆、行人）的未来运动轨迹。这个问题在现实中至关重要，因为不准确的预测可能导致代理之间的碰撞或冲突轨迹，威胁自动驾驶系统的安全性和可靠性，影响实际交通效率。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，即它们主要关注观察到的交互而忽略了未来交互的动态演化。基于这一洞察，作者设计了ProgD方法，利用动态异构图来显式建模未来场景的演化。该方法借鉴了图神经网络和Transformer架构处理时空依赖，异构图表示不同类型交互，以及编码器-解码器框架等现有技术，但创新性地将这些技术与动态图建模和多尺度解码相结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用动态异构图显式建模未来交互的演化，并通过多尺度解码策略逐步消除不确定性。整体流程包括：1) 场景编码：使用静态异构图编码观察到的代理历史状态和道路网络；2) 联合运动预测：通过时间模块捕获时间依赖，动态构建异构图表示未来交互，采用多尺度解码（粗略预测→快照更新→联合预测）逐步细化预测；3) 多模态预测：生成多个可能的联合预测，每个预测内部保持一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 渐进式多尺度解码策略，通过动态异构图显式建模未来交互的演化；2) 动态异构图建模，图结构随时间动态更新反映交互变化；3) 多模态一致性预测，确保多个代理预测之间的一致性。相比之前的工作，ProgD不再使用静态图而是动态图来处理未来交互，不再直接预测完整轨迹而是采用多尺度解码，并且更注重预测的一致性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProgD通过动态异构图和多尺度解码策略实现了对多个交互代理未来运动的准确、一致预测，在INTERACTION和Argoverse 2等基准测试中达到了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion prediction of surrounding agents is crucial for the safeplanning of autonomous vehicles. Recent advancements have extended predictiontechniques from individual agents to joint predictions of multiple interactingagents, with various strategies to address complex interactions within futuremotions of agents. However, these methods overlook the evolving nature of theseinteractions. To address this limitation, we propose a novel progressivemulti-scale decoding strategy, termed ProgD, with the help of dynamicheterogeneous graph-based scenario modeling. In particular, to explicitly andcomprehensively capture the evolving social interactions in future scenarios,given their inherent uncertainty, we design a progressive modeling of scenarioswith dynamic heterogeneous graphs. With the unfolding of such dynamicheterogeneous graphs, a factorized architecture is designed to process thespatio-temporal dependencies within future scenarios and progressivelyeliminate uncertainty in future motions of multiple agents. Furthermore, amulti-scale decoding procedure is incorporated to improve on the futurescenario modeling and consistent prediction of agents' future motion. Theproposed ProgD achieves state-of-the-art performance on the INTERACTIONmulti-agent prediction benchmark, ranking $1^{st}$, and the Argoverse 2multi-world forecasting benchmark.</description>
      <author>example@mail.com (Xing Gao, Zherui Huang, Weiyao Lin, Xiao Sun)</author>
      <guid isPermaLink="false">2509.09210v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>ObjectReact: Learning Object-Relative Control for Visual Navigation</title>
      <link>http://arxiv.org/abs/2509.09594v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CoRL 2025; 23 pages including appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种基于物体的视觉导航新范式，通过使用'物体相对'控制而非传统的'图像相对'控制，解决了现有方法在姿态依赖和泛化能力方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;视觉导航通常需要额外的传感器和3D地图。目前主流方法是'图像相对'的方法，通过当前观察图像和子目标图像对来估计控制。然而，图像表示存在局限性，因为它们严格依赖于机器人的姿态和形态。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的'物体相对'控制学习范式，解决图像级表示的局限性，实现跨形态部署的高不变性，并使控制预测问题与图像匹配问题解耦。&lt;h4&gt;方法&lt;/h4&gt;提出一种'相对'3D场景图形式的拓扑地图表示，用于获取更有信息量的物体级全局路径规划成本。训练一个名为'ObjectReact'的局部控制器，直接基于高级'WayObject Costmap'表示进行条件化，不需要显式的RGB输入。&lt;h4&gt;主要发现&lt;/h4&gt;'物体相对'控制相比'图像相对'控制在传感器高度变化和多个导航任务中表现更好；能够处理反向导航等挑战空间理解能力的任务；仅在模拟环境中训练的策略能够很好地泛化到真实世界的室内环境。&lt;h4&gt;结论&lt;/h4&gt;物体作为地图的属性，提供了与形态和轨迹无关的世界表示；新方法能够实现新路径的遍历，无需严格模仿先验经验；在跨形态部署中具有高不变性。&lt;h4&gt;翻译&lt;/h4&gt;仅使用单个相机和拓扑地图的视觉导航最近已成为需要额外传感器和3D地图方法的吸引人的替代方案。这通常通过一种'图像相对'的方法来实现，从给定的当前观察图像和子目标图像对中估计控制。然而，世界的图像级表示存在局限性，因为图像严格绑定到机器人的姿态和形态。相比之下，物体作为地图的属性，提供了一种与形态和轨迹无关的世界表示。在这项工作中，我们提出了学习'物体相对'控制的新范式，展现出几个理想特性：a) 可以遍历新路径而无需严格模仿先验经验，b) 控制预测问题可以与图像匹配问题解耦，c) 在训练-测试和映射-执行环境变化的跨形态部署中可以实现高不变性。我们提出了一种'相对'3D场景图形式的拓扑地图表示，用于获取更有信息量的物体级全局路径规划成本。我们训练了一个名为'ObjectReact'的局部控制器，直接基于高级'WayObject Costmap'表示进行条件化，消除了对显式RGB输入的需求。我们展示了在传感器高度变化和多个导航任务中，学习物体相对控制相比其图像相对对手的优势，这些任务挑战了底层空间理解能力，例如在地图轨迹中反向导航。我们进一步表明，我们的仅模拟策略能够很好地泛化到真实世界的室内环境。代码和补充材料可通过项目页面访问：https://object-react.github.io/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决视觉导航中基于图像的方法的局限性，特别是它们对机器人姿态和形态的依赖问题。这个问题很重要，因为视觉导航仅使用单个相机和拓扑图已成为一种有吸引力的替代方案，不需要额外的传感器和3D地图，但传统图像级表示与机器人姿态紧密绑定，限制了其在不同条件下的鲁棒性和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有图像相对方法的局限性，提出使用物体作为地图属性提供与形态无关的世界表示。他们借鉴了RoboHop的对象级拓扑映射，但改进了其2D连通性，使用更信息丰富的相对3D连通性；同时借鉴了SAM等基础模型进行物体分割，以及SuperPoint和LightGlue进行特征匹配；在控制器设计上参考了GNM架构但进行了修改以适应物体相对控制范式。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用物体级别的连接性而非图像级别来表示世界，提出物体相对控制范式，直接基于当前图像中可见的物体子目标进行条件化。整体流程分为三阶段：1)映射阶段：构建相对3D场景图，提取物体节点并建立图像内外的连接；2)执行阶段：进行物体定位和全局路径规划，生成WayObject Costmap；3)训练阶段：训练ObjectReact控制器，基于WayObject Costmap预测轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：物体相对控制新范式、基于相对3D场景图的地图表示、WayObject Costmap表示方法、ObjectReact控制器设计。相比之前的工作，不同之处在于：不依赖机器人姿态和形态；使用3D而非2D连通性；将控制学习与图像匹配问题解耦；在跨形态部署方面表现出色；能够处理传统图像相对方法难以解决的导航任务如反向导航和捷径寻找。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种物体相对控制的视觉导航新范式，通过基于物体级别的世界表示和WayObject Costmap控制器，实现了对机器人姿态和环境变化的鲁棒性，并能解决传统图像相对方法难以处理的复杂导航任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual navigation using only a single camera and a topological map hasrecently become an appealing alternative to methods that require additionalsensors and 3D maps. This is typically achieved through an "image-relative"approach to estimating control from a given pair of current observation andsubgoal image. However, image-level representations of the world havelimitations because images are strictly tied to the agent's pose andembodiment. In contrast, objects, being a property of the map, offer anembodiment- and trajectory-invariant world representation. In this work, wepresent a new paradigm of learning "object-relative" control that exhibitsseveral desirable characteristics: a) new routes can be traversed withoutstrictly requiring to imitate prior experience, b) the control predictionproblem can be decoupled from solving the image matching problem, and c) highinvariance can be achieved in cross-embodiment deployment for variations acrossboth training-testing and mapping-execution settings. We propose a topometricmap representation in the form of a "relative" 3D scene graph, which is used toobtain more informative object-level global path planning costs. We train alocal controller, dubbed "ObjectReact", conditioned directly on a high-level"WayObject Costmap" representation that eliminates the need for an explicit RGBinput. We demonstrate the advantages of learning object-relative control overits image-relative counterpart across sensor height variations and multiplenavigation tasks that challenge the underlying spatial understandingcapability, e.g., navigating a map trajectory in the reverse direction. Wefurther show that our sim-only policy is able to generalize well to real-worldindoor environments. Code and supplementary material are accessible via projectpage: https://object-react.github.io/</description>
      <author>example@mail.com (Sourav Garg, Dustin Craggs, Vineeth Bhat, Lachlan Mares, Stefan Podgorski, Madhava Krishna, Feras Dayoub, Ian Reid)</author>
      <guid isPermaLink="false">2509.09594v1</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
      <link>http://arxiv.org/abs/2509.01106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://robix-seed.github.io/robix/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Robix是一个统一的模型，集成了机器人推理、任务规划和自然语言交互，采用视觉-语言架构。作为分层机器人系统的高级认知层，Robix动态生成低级控制器的原子命令和人类交互的口头响应，使机器人能够遵循复杂指令、规划长期任务并在端到端框架内与人类自然交互。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要能够理解人类指令、规划任务并进行自然交互的能力。现有的系统可能缺乏统一架构来整合这些能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型，整合机器人推理、任务规划和自然语言交互，使机器人能够遵循复杂指令、规划长期任务并与人类自然交互。&lt;h4&gt;方法&lt;/h4&gt;采用链式思维推理和三阶段训练策略：(1)持续预训练以增强基础具身推理能力，包括3D空间理解、视觉定位和任务中心推理；(2)监督微调将人机交互和任务建模为统一的推理-动作序列；(3)强化学习提高推理-动作一致性和长期任务连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;Robix在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），在各种指令类型（开放式、多阶段、受约束的、无效的和中断的）上表现出强大的泛化能力，并在各种用户参与的任务（如餐桌清理、杂货购物和饮食过滤）上表现良好。&lt;h4&gt;结论&lt;/h4&gt;Robix是一个统一的模型，能够有效整合机器人推理、任务规划和自然语言交互，在各种任务和指令类型上表现出强大的泛化能力和性能。&lt;h4&gt;翻译&lt;/h4&gt;我们引入Robix，一个统一的模型，在单一视觉-语言架构中集成了机器人推理、任务规划和自然语言交互。作为分层机器人系统中的高级认知层，Robix动态生成低级控制器的原子命令和人类交互的口头响应，使机器人能够在端到端框架内遵循复杂指令、规划长期任务并与人类自然交互。Robix进一步引入了新颖功能，如主动对话、实时中断处理和任务执行中的上下文常识推理。其核心是，Robix利用链式思维推理并采用三阶段训练策略：(1)持续预训练以增强基础具身推理能力，包括3D空间理解、视觉定位和任务中心推理；(2)监督微调将人机交互和任务规划建模为统一的推理-动作序列；(3)强化学习提高推理-动作一致性和长期任务连贯性。大量实验表明，Robix在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），展示了在各种指令类型（如开放式、多阶段、受约束的、无效的和中断的）和各种用户参与任务（如餐桌清理、杂货购物和饮食过滤）上的强大泛化能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让机器人理解复杂指令、规划长期任务并与人类自然交互的问题。这个问题在现实中非常重要，因为机器人需要在开放、动态环境中协助人类完成多样化日常任务，这不仅是执行孤立命令，还需要自然交互和复杂推理能力。现有方法要么只关注任务分解而忽视交互和推理，要么采用模块化框架但缺乏灵活性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有分层方法的局限性，即大型语言模型主要关注任务分解，忽视了人类交互和具身推理；模块化系统虽然易于开发但缺乏灵活性。基于这些观察，作者借鉴了链式思维推理方法，设计了统一模型Robix。作者使用了三阶段训练策略，并从多个现有数据集（如AgiBot、BridgeData V2等）获取数据构建大规模训练集，同时参考了UI-TARS等工作的思想来合成高质量推理轨迹。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是构建一个统一的视觉-语言模型，将机器人推理、任务规划和人类交互整合在单一架构中，使机器人能以端到端方式处理复杂任务。整体流程分为三阶段：1)持续预训练：使用2000亿token数据集增强3D空间理解、视觉定位等基础能力；2)监督微调：通过数据合成将交互任务建模为统一推理-行动序列；3)强化学习：提高推理-行动一致性，特别是长期任务中的连贯性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一模型架构而非模块化设计；2)支持主动对话、实时中断处理和上下文常识推理的灵活交互能力；3)三阶段训练策略；4)大规模多样化数据集。相比之前工作，Robix不同之处在于：采用端到端架构而非模块化；同时关注任务规划和交互；引入链式思维推理；提供实时中断处理等新能力，使机器人能更好适应动态环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Robix提出了一种统一的视觉-语言模型，通过三阶段训练策略将机器人推理、任务规划和自然语言交互整合在一个端到端框架中，显著提升了机器人在复杂环境中执行长期任务的能力并实现了与人类自然灵活的交互。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Robix, a unified model that integrates robot reasoning, taskplanning, and natural language interaction within a single vision-languagearchitecture. Acting as the high-level cognitive layer in a hierarchical robotsystem, Robix dynamically generates atomic commands for the low-levelcontroller and verbal responses for human interaction, enabling robots tofollow complex instructions, plan long-horizon tasks, and interact naturallywith human within an end-to-end framework. Robix further introduces novelcapabilities such as proactive dialogue, real-time interruption handling, andcontext-aware commonsense reasoning during task execution. At its core, Robixleverages chain-of-thought reasoning and adopts a three-stage trainingstrategy: (1) continued pretraining to enhance foundational embodied reasoningabilities including 3D spatial understanding, visual grounding, andtask-centric reasoning; (2) supervised finetuning to model human-robotinteraction and task planning as a unified reasoning-action sequence; and (3)reinforcement learning to improve reasoning-action consistency and long-horizontask coherence. Extensive experiments demonstrate that Robix outperforms bothopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) ininteractive task execution, demonstrating strong generalization across diverseinstruction types (e.g., open-ended, multi-stage, constrained, invalid, andinterrupted) and various user-involved tasks such as table bussing, groceryshopping, and dietary filtering.</description>
      <author>example@mail.com (Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li)</author>
      <guid isPermaLink="false">2509.01106v2</guid>
      <pubDate>Fri, 12 Sep 2025 15:05:52 +0800</pubDate>
    </item>
    <item>
      <title>SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</title>
      <link>http://arxiv.org/abs/2509.08757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025 Project site:  https://larg.github.io/socialnav-sub&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SocialNav-SUB基准，用于评估视觉语言模型(VLMs)在社会机器人导航场景中的场景理解能力，研究发现当前VLMs在社会场景理解方面仍存在关键差距。&lt;h4&gt;背景&lt;/h4&gt;机器人导航在动态、以人为中心的环境中需要基于鲁棒场景理解的社会合规决策。视觉语言模型(VLMs)显示出物体识别、常识推理和上下文理解等有希望的能力，但这些能力是否能准确理解复杂的社会导航场景(如推断代理间的时空关系和人类意图)尚不明确。&lt;h4&gt;目的&lt;/h4&gt;引入SocialNav-SUB(Social Navigation Scene Understanding Benchmark)，一个视觉问答(VQA)数据集和基准，旨在评估VLMs在真实世界社会机器人导航场景中的场景理解能力，并提供统一框架比较VLMs与人类和基于规则的基线。&lt;h4&gt;方法&lt;/h4&gt;设计并创建了SocialNav-SUB基准，包含需要空间、时空和社会推理的VQA任务，通过实验评估了最先进的VLMs，并将其表现与人类和基于规则的基线方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管表现最好的VLMs与人类答案达成一致的几率令人鼓舞，但它们仍然表现不如简单的基于规则的方法和人类共识基线，表明当前VLMs在社会场景理解方面存在关键差距。&lt;h4&gt;结论&lt;/h4&gt;该基准为社交机器人导航的基础模型研究奠定了基础，提供了一个探索如何调整VLMs以满足真实世界社交机器人导航需求的框架。&lt;h4&gt;翻译&lt;/h4&gt;在动态的、以人为中心的环境中，机器人导航需要基于鲁棒场景理解的社会合规决策。最近的视觉语言模型(VLMs)展现出有希望的能力，如物体识别、常识推理和上下文理解——这些能力与社会机器人导航的微妙需求相符合。然而，目前尚不清楚VLMs是否能准确理解复杂的社会导航场景(例如推断代理之间的时空关系和人类意图)，这对于安全和社会合规的机器人导航至关重要。虽然最近的一些工作已经探索了VLMs在社会机器人导航中的应用，但没有现有工作系统性地评估它们满足这些必要条件的能力。在本文中，我们引入了SocialNav-SUB(社会导航场景理解基准)，这是一个视觉问答(VQA)数据集和基准，旨在评估VLMs在真实世界社会机器人导航场景中的场景理解能力。SocialNav-SUB提供了一个统一框架，用于评估VLMs在需要空间、时空和社会推理的社会机器人导航VQA任务中与人类和基于规则的基线的对比。通过对最先进的VLMs进行实验，我们发现尽管表现最好的VLMs与人类答案达成一致的几率令人鼓舞，但它仍然表现不如简单的基于规则的方法和人类共识基线，这表明当前VLMs在社会场景理解方面存在关键差距。我们的基准为社交机器人导航的基础模型研究奠定了基础，提供了一个探索如何调整VLMs以满足真实世界社交机器人导航需求的框架。本文概述以及代码和数据可在https://larg.github.io/socialnav-sub找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot navigation in dynamic, human-centered environments requiressocially-compliant decisions grounded in robust scene understanding. RecentVision-Language Models (VLMs) exhibit promising capabilities such as objectrecognition, common-sense reasoning, and contextual understanding-capabilitiesthat align with the nuanced requirements of social robot navigation. However,it remains unclear whether VLMs can accurately understand complex socialnavigation scenes (e.g., inferring the spatial-temporal relations among agentsand human intentions), which is essential for safe and socially compliant robotnavigation. While some recent works have explored the use of VLMs in socialrobot navigation, no existing work systematically evaluates their ability tomeet these necessary conditions. In this paper, we introduce the SocialNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual QuestionAnswering (VQA) dataset and benchmark designed to evaluate VLMs for sceneunderstanding in real-world social robot navigation scenarios. SocialNav-SUBprovides a unified framework for evaluating VLMs against human and rule-basedbaselines across VQA tasks requiring spatial, spatiotemporal, and socialreasoning in social robot navigation. Through experiments with state-of-the-artVLMs, we find that while the best-performing VLM achieves an encouragingprobability of agreeing with human answers, it still underperforms simplerrule-based approach and human consensus baselines, indicating critical gaps insocial scene understanding of current VLMs. Our benchmark sets the stage forfurther research on foundation models for social robot navigation, offering aframework to explore how VLMs can be tailored to meet real-world social robotnavigation needs. An overview of this paper along with the code and data can befound at https://larg.github.io/socialnav-sub .</description>
      <author>example@mail.com (Michael J. Munje, Chen Tang, Shuijing Liu, Zichao Hu, Yifeng Zhu, Jiaxun Cui, Garrett Warnell, Joydeep Biswas, Peter Stone)</author>
      <guid isPermaLink="false">2509.08757v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
  <item>
      <title>Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</title>
      <link>http://arxiv.org/abs/2509.08302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 14 figures, accepted at IEEE Open Journal of Vehicular  Technology (OJVT)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇调查论文探讨了基础模型如何革新自动驾驶感知领域，从特定任务模型转向通用架构，解决泛化、可扩展性和鲁棒性等挑战。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变自动驾驶感知领域，从狭隘的、特定任务的深度学习模型转向多功能、通用架构，这些架构在庞大且多样化的数据集上进行训练。&lt;h4&gt;目的&lt;/h4&gt;探讨这些模型如何解决自动驾驶感知中的关键挑战，包括泛化能力、可扩展性和对分布变化的鲁棒性限制。&lt;h4&gt;方法&lt;/h4&gt;提出一个围绕四种基本能力的分类法：泛化知识、空间理解、多传感器鲁棒性和时序推理，并对每种能力进行全面的前沿方法审查。&lt;h4&gt;主要发现&lt;/h4&gt;与传统以方法为中心的调查不同，他们的框架优先考虑概念设计原则，为模型开发提供能力驱动的指导，并对基础方面提供更清晰的见解。&lt;h4&gt;结论&lt;/h4&gt;讨论了将这些能力集成到实时系统中的挑战，以及与计算需求和模型可靠性相关的部署挑战，并提出了未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在革新自动驾驶感知，将领域从狭隘的、特定任务的深度学习模型转变为在庞大、多样化数据集上训练的多功能、通用架构。本调查探讨了这些模型如何解决自动驾驶感知中的关键挑战，包括泛化、可扩展性和对分布变化的鲁棒性限制。调查提出了一个围绕四种基本能力构建的新颖分类法，以在动态驾驶环境中实现稳健性能：泛化知识、空间理解、多传感器鲁棒性和时序推理。对于每种能力，调查阐明了其意义并全面审查了最前沿的方法。与传统以方法为中心的调查不同，我们的独特框架优先考虑概念设计原则，为模型开发提供了能力驱动的指导，并对基础方面提供了更清晰的见解。我们最后讨论了关键挑战，特别是将这些能力集成到实时、可扩展系统中的挑战，以及与计算需求和确保模型可靠性（如幻觉问题和分布外故障）相关的更广泛的部署挑战。调查还概述了关键的未来研究方向，以促进基础模型在自动驾驶系统中的安全有效部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/OJVT.2025.3604823 10.1109/OJVT.2025.3604823  10.1109/OJVT.2025.3604823&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are revolutionizing autonomous driving perception,transitioning the field from narrow, task-specific deep learning models toversatile, general-purpose architectures trained on vast, diverse datasets.This survey examines how these models address critical challenges in autonomousperception, including limitations in generalization, scalability, androbustness to distributional shifts. The survey introduces a novel taxonomystructured around four essential capabilities for robust performance in dynamicdriving environments: generalized knowledge, spatial understanding,multi-sensor robustness, and temporal reasoning. For each capability, thesurvey elucidates its significance and comprehensively reviews cutting-edgeapproaches. Diverging from traditional method-centric surveys, our uniqueframework prioritizes conceptual design principles, providing acapability-driven guide for model development and clearer insights intofoundational aspects. We conclude by discussing key challenges, particularlythose associated with the integration of these capabilities into real-time,scalable systems, and broader deployment challenges related to computationaldemands and ensuring model reliability against issues like hallucinations andout-of-distribution failures. The survey also outlines crucial future researchdirections to enable the safe and effective deployment of foundation models inautonomous driving systems.</description>
      <author>example@mail.com (Rajendramayavan Sathyam, Yueqi Li)</author>
      <guid isPermaLink="false">2509.08302v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2509.08126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid  Robots&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为OGRG的框架，能够通过自然语言理解抓取目标物体，即使在有重复物体的情况下也能进行空间推理和抓取预测。&lt;h4&gt;背景&lt;/h4&gt;让机器人通过自然语言抓取物体对有效人机交互至关重要，但现有方法难以处理开放形式语言表达，通常假设无重复物体，且依赖密集像素级标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解释开放形式语言表达并进行空间推理的框架，以定位目标物体并预测平面抓取姿态，即使在包含重复物体实例的场景中也能工作。&lt;h4&gt;方法&lt;/h4&gt;提出基于属性的目标定位和机器人抓取(OGRG)框架，在两种设置下测试：像素级完全监督的指称抓取合成(RGS)和使用单像素标注的弱监督指称抓取可能性(RGA)。关键贡献包括双向视觉-语言融合模块和深度信息集成以增强几何推理。&lt;h4&gt;主要发现&lt;/h4&gt;OGRG在具有多样化空间语言指令的桌面场景中优于基线方法，在RGS设置下以17.59 FPS运行，提供更好的定位和抓取预测准确性；在弱监督RGA设置下，模拟和真实机器人试验中均优于基线抓取成功率。&lt;h4&gt;结论&lt;/h4&gt;OGRG成功解决了通过自然语言抓取物体的问题，特别是在处理开放形式语言表达和重复物体实例方面，在完全监督和弱监督设置下都表现出色，具有实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;使机器人能够通过自然语言抓取指定物体对于有效的人机交互至关重要，但仍然是一个重大挑战。现有方法通常难以处理开放形式的语言表达，通常假设没有重复的目标物体。此外，它们通常依赖于密集的像素级标注来进行物体定位和抓取配置。我们提出了基于属性的目标定位和机器人抓取(OGRG)，这是一个新框架，能够解释开放形式的语言表达并进行空间推理，以定位目标物体并预测平面抓取姿态，即使在包含重复物体实例的场景中也是如此。我们在两种设置下研究了OGRG：(1)在像素级完全监督下的指称抓取合成(RGS)，以及(2)使用仅单像素抓取标注的弱监督学习进行指称抓取可能性(RGA)。关键贡献包括双向视觉-语言融合模块和深度信息的集成，以增强几何推理，提高定位和抓取性能。实验结果表明，在具有多样化空间语言指令的桌面场景中，OGRG优于强大的基线方法。在RGS中，它在单个NVIDIA RTX 2080 Ti GPU上以17.59 FPS的速度运行，能够在闭环或多物体顺序抓取中使用，并提供比所有考虑的基线更好的定位和抓取预测准确性。在弱监督的RGA设置下，OGRG在模拟和真实机器人试验中都优于基线的抓取成功率，突显了其空间推理设计的有效性。项目页面：https://z.umn.edu/ogrg&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人通过自然语言描述来抓取指定物体的问题。具体来说，现有方法难以处理开放形式的语言表达、无法处理场景中重复出现的物体，且依赖密集的像素级标注。这个问题在现实中很重要，因为它能使机器人更好地理解人类指令，实现更自然的人机交互，同时减少对昂贵标注数据的依赖，使技术更容易在实际场景中应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括处理开放形式语言能力不足、无法处理重复物体、依赖密集标注等。他们注意到虽然多模态大语言模型性能强大，但计算需求大，难以在资源受限的机器人平台上部署。因此，作者设计了一个紧凑且计算高效的融合模块作为替代。他们借鉴了ETRG的CLIP模型和下采样-上采样策略，以及LAVT的单向融合模块，但在此基础上提出了改进的双向融合模块。同时，他们使用了Swin Transformer作为视觉主干网络和BERT作为语言特征提取器，并整合了深度信息来增强几何推理能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个名为OGRG的框架，能够解释开放形式的语言表达并进行空间推理，以定位目标物体并预测平面抓取姿态。整体流程包括：1)接受RGB图像、深度图像和语言描述作为输入；2)使用Swin Transformer提取视觉特征，BERT提取语言特征，ResNet-18提取深度特征；3)通过四阶段多模态融合过程，使用双向对齐器进行视觉、语言和深度特征的交互；4)对于RGS任务，使用FCN头生成物体掩码和抓取相关图；对于RGA任务，先生成物体掩码，再使用掩码条件抓取网络预测抓取能力图；5)输出物体定位结果和抓取姿态参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向视觉-语言融合模块(Bi-Aligner)，同时进行视觉-语言和语言-视觉的交叉注意力计算；2)深度信息集成，增强几何推理能力；3)支持两种抓取检测设置：完全监督的RGS和弱监督的RGA；4)掩码条件抓取网络(MGN)用于RGA任务。相比之前工作，本文的双向融合比ETRG的双向适配器和LAVT的单向融合更有效地对齐特征；深度融合方式比ETRG更有效，避免信息损失；计算效率比多模态大语言模型更高，适合资源受限的机器人平台；支持弱监督学习，减少标注需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于双向视觉-语言融合和深度信息集成的OGRG框架，使机器人能够通过开放形式的自然语言描述准确识别并抓取目标物体，即使在有重复物体实例的场景中也能高效工作，同时支持完全监督和弱监督两种训练方式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to grasp objects specified through natural language isessential for effective human-robot interaction, yet it remains a significantchallenge. Existing approaches often struggle with open-form languageexpressions and typically assume unambiguous target objects without duplicates.Moreover, they frequently rely on costly, dense pixel-wise annotations for bothobject grounding and grasp configuration. We present Attribute-based ObjectGrounding and Robotic Grasping (OGRG), a novel framework that interpretsopen-form language expressions and performs spatial reasoning to ground targetobjects and predict planar grasp poses, even in scenes containing duplicatedobject instances. We investigate OGRG in two settings: (1) Referring GraspSynthesis (RGS) under pixel-wise full supervision, and (2) Referring GraspAffordance (RGA) using weakly supervised learning with only single-pixel graspannotations. Key contributions include a bi-directional vision-language fusionmodule and the integration of depth information to enhance geometric reasoning,improving both grounding and grasping performance. Experiment results show thatOGRG outperforms strong baselines in tabletop scenes with diverse spatiallanguage instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX2080 Ti GPU, enabling potential use in closed-loop or multi-object sequentialgrasping, while delivering superior grounding and grasp prediction accuracycompared to all the baselines considered. Under the weakly supervised RGAsetting, OGRG also surpasses baseline grasp-success rates in both simulationand real-robot trials, underscoring the effectiveness of its spatial reasoningdesign. Project page: https://z.umn.edu/ogrg</description>
      <author>example@mail.com (Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi)</author>
      <guid isPermaLink="false">2509.08126v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Tokenizing Loops of Antibodies</title>
      <link>http://arxiv.org/abs/2509.08707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures, 10 tables, code available at  https://github.com/prescient-design/igloo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Igloo的多模态抗体环标记器，能够编码主链二面角和序列信息，通过对比学习训练，高效检索匹配的环结构，解决传统方法覆盖有限的问题，并提高蛋白质基础模型在抗体设计中的应用。&lt;h4&gt;背景&lt;/h4&gt;抗体互补决定区是环状结构，对与抗原的相互作用至关重要。自1980年代以来，将CDR结构分类为规范簇有助于识别关键结构基序，但现有方法覆盖范围有限且难以整合到蛋白质基础模型中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够编码抗体环结构信息的多模态标记器，解决现有方法覆盖有限的问题，并提高蛋白质基础模型在抗体设计中的应用。&lt;h4&gt;方法&lt;/h4&gt;引入ImmunoGlobulin LOOp Tokenizer（Igloo），一种多模态抗体环标记器，编码主链二面角和序列信息。使用对比学习目标训练，将相似主链二面角的环在潜在空间中映射得更近。开发了IglooLM和IglooALM，将Igloo标记整合到蛋白质语言模型中。&lt;h4&gt;主要发现&lt;/h4&gt;1. Igloo在识别相似H3环方面比现有方法高5.9%；2. 为所有环分配标记，解决规范簇覆盖有限问题；3. IglooLM在8/10抗体-抗原靶点上优于基础模型；4. IglooLM性能与参数量多7倍的模型相当；5. IglooALM采样的抗体环序列更多样化，结构更一致。&lt;h4&gt;结论&lt;/h4&gt;Igloo证明了为抗体环引入多模态标记的益处，能够编码抗体环的多样化景观，改进蛋白质基础模型，并用于抗体CDR设计。&lt;h4&gt;翻译&lt;/h4&gt;抗体的互补决定区是与抗原相互作用的关键环状结构，对设计新型生物制剂非常重要。自1980年代以来，将CDR结构的多样性分类为规范簇有助于识别抗体的关键结构基序。然而，现有方法覆盖范围有限，且难以直接整合到蛋白质基础模型中。我们在此介绍ImmunoGlobulin LOOp Tokenizer（Igloo），这是一种多模态抗体环标记器，编码主链二面角和序列信息。Igloo使用对比学习目标进行训练，将具有相似主链二面角的环在潜在空间中映射得更近。Igloo能够从结构抗体数据库中高效检索最匹配的环结构，在识别相似的H3环方面比现有方法高出5.9%。Igloo为所有环分配标记，解决了规范簇覆盖有限的问题，同时保留了恢复规范环构象的能力。为了展示Igloo标记的多功能性，我们展示了它们可以整合到蛋白质语言模型中，形成IglooLM和IglooALM。在预测重链变体的结合亲和力方面，IglooLM在10个抗体-抗原靶点中有8个表现优于基础蛋白质语言模型。此外，它与现有的最先进的序列和 multimodal 蛋白质语言模型相当，性能与参数量多7倍的模型相当。IglooALM采样的抗体环在序列上更多样化，结构上比最先进的抗体逆向折叠模型更一致。Igloo证明了为抗体环引入多模态标记的益处，能够编码抗体环的多样化景观，改进蛋白质基础模型，并用于抗体CDR设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The complementarity-determining regions of antibodies are loop structuresthat are key to their interactions with antigens, and of high importance to thedesign of novel biologics. Since the 1980s, categorizing the diversity of CDRstructures into canonical clusters has enabled the identification of keystructural motifs of antibodies. However, existing approaches have limitedcoverage and cannot be readily incorporated into protein foundation models.Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibodyloop tokenizer that encodes backbone dihedral angles and sequence. Igloo istrained using a contrastive learning objective to map loops with similarbackbone dihedral angles closer together in latent space. Igloo can efficientlyretrieve the closest matching loop structures from a structural antibodydatabase, outperforming existing methods on identifying similar H3 loops by5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issueof canonical clusters, while retaining the ability to recover canonical loopconformations. To demonstrate the versatility of Igloo tokens, we show thatthey can be incorporated into protein language models with IglooLM andIglooALM. On predicting binding affinity of heavy chain variants, IglooLMoutperforms the base protein language model on 8 out of 10 antibody-antigentargets. Additionally, it is on par with existing state-of-the-artsequence-based and multimodal protein language models, performing comparably tomodels with $7\times$ more parameters. IglooALM samples antibody loops whichare diverse in sequence and more consistent in structure than state-of-the-artantibody inverse folding models. Igloo demonstrates the benefit of introducingmultimodal tokens for antibody loops for encoding the diverse landscape ofantibody loops, improving protein foundation models, and for antibody CDRdesign.</description>
      <author>example@mail.com (Ada Fang, Robert G. Alberstein, Simon Kelow, Frédéric A. Dreyer)</author>
      <guid isPermaLink="false">2509.08707v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation</title>
      <link>http://arxiv.org/abs/2509.08624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BIBM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为UOPSL的新型无配对多模态框架，利用OCT衍生的空间先验知识增强基于眼底图像的疾病识别，解决了多模态眼科图像获取成本高和模态不平衡的问题。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的多模态医学图像诊断在眼科疾病识别方面取得了显著进展，但获取配对的多模态眼科图像成本过高。眼底摄影简单且经济高效，但OCT数据有限且存在模态不平衡问题。传统仅依赖眼底或文本特征的方法无法捕捉细粒度空间信息，因为每种成像模态对病变偏好部位提供不同的线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用OCT衍生的空间先验知识来增强眼底图像疾病识别的无配对多模态框架，解决配对数据获取困难和模态不平衡的问题。&lt;h4&gt;方法&lt;/h4&gt;提出UOPSL框架，通过扩展疾病文本描述连接无配对的眼底图像和OCT。首先，在大量无配对的OCT和眼底图像上使用对比学习，同时在OCT潜在空间中学习偏好部位矩阵。该矩阵通过大量优化捕捉OCT特征空间内的病变定位模式。在仅基于眼底图像的下游分类任务中，消除OCT输入，利用偏好部位矩阵辅助眼底图像分类学习。&lt;h4&gt;主要发现&lt;/h4&gt;在9个不同数据集、28个关键类别上进行的广泛实验表明，该框架优于现有基准方法。&lt;h4&gt;结论&lt;/h4&gt;UOPSL框架成功利用OCT衍生的空间先验知识增强基于眼底图像的疾病识别，无需配对数据，有效解决了多模态眼科图像获取成本高和模态不平衡的问题。&lt;h4&gt;翻译&lt;/h4&gt;近年来，AI驱动的多模态医学图像诊断的显著进展极大地促进了眼科疾病的识别。然而，获取配对的多模态眼科图像仍然成本过高。虽然眼底摄影简单且经济高效，但OCT数据的有限性和固有的模态不平衡阻碍了进一步发展。传统仅依赖眼底或文本特征的方法往往无法捕捉细粒度空间信息，因为每种成像模态对病变偏好部位提供不同的线索。在本研究中，我们提出了一种名为UOPSL的新型无配对多模态框架，利用大量的OCT衍生空间先验知识动态识别偏好部位，增强基于眼底图像的疾病识别。我们的方法通过扩展疾病文本描述连接无配对的眼底图像和OCT。最初，我们在大量无配对的OCT和眼底图像上使用对比学习，同时在OCT潜在空间中学习偏好部位矩阵。通过大量优化，该矩阵捕捉了OCT特征空间内的病变定位模式。在仅基于眼底图像的下游分类任务的微调或推理阶段，当无法获取配对的OCT数据时，我们消除OCT输入，利用偏好部位矩阵辅助眼底图像分类学习。在9个不同数据集、28个关键类别上进行的广泛实验表明，我们的框架优于现有基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant advancements in AI-driven multimodal medical image diagnosis haveled to substantial improvements in ophthalmic disease identification in recentyears. However, acquiring paired multimodal ophthalmic images remainsprohibitively expensive. While fundus photography is simple and cost-effective,the limited availability of OCT data and inherent modality imbalance hinderfurther progress. Conventional approaches that rely solely on fundus or textualfeatures often fail to capture fine-grained spatial information, as eachimaging modality provides distinct cues about lesion predilection sites. Inthis study, we propose a novel unpaired multimodal framework \UOPSL thatutilizes extensive OCT-derived spatial priors to dynamically identifypredilection sites, enhancing fundus image-based disease recognition. Ourapproach bridges unpaired fundus and OCTs via extended disease textdescriptions. Initially, we employ contrastive learning on a large corpus ofunpaired OCT and fundus images while simultaneously learning the predilectionsites matrix in the OCT latent space. Through extensive optimization, thismatrix captures lesion localization patterns within the OCT feature space.During the fine-tuning or inference phase of the downstream classification taskbased solely on fundus images, where paired OCT data is unavailable, weeliminate OCT input and utilize the predilection sites matrix to assist infundus image classification learning. Extensive experiments conducted on 9diverse datasets across 28 critical categories demonstrate that our frameworkoutperforms existing benchmarks.</description>
      <author>example@mail.com (Zhihao Zhao, Yinzheng Zhao, Junjie Yang, Xiangtong Yao, Quanmin Liang, Daniel Zapp, Kai Huang, Nassir Navab, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2509.08624v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography</title>
      <link>http://arxiv.org/abs/2509.08116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为PhysioCLR的心电图分析框架，它是一种生理感知的对比学习框架，通过整合领域特定先验知识增强心电图心律失常分类的可泛化性和临床相关性，解决了标记数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;心电图在诊断心脏疾病中起着关键作用，但基于人工智能的心电图分析往往受限于标记数据的可用性。自监督学习可以通过利用大规模无标签数据来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为PhysioCLR的生理感知对比学习框架，整合领域特定的先验知识，以增强心电图心律失常分类的可泛化性和临床相关性。&lt;h4&gt;方法&lt;/h4&gt;在预训练过程中，PhysioCLR学习将具有相似临床相关特征的样本嵌入拉近，同时将不相似的推开。该方法整合心电图生理相似性线索到对比学习中，引入特定于心电图的增强方法保持类别不变，并提出混合损失函数优化学习表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;在Chapman、Georgia和私人ICU数据集上，PhysioCLR比最强基线模型平均提升了12%的AUROC，展示了强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过将生理知识嵌入对比学习，PhysioCLR使模型能够学习临床上有意义且可迁移的心电图特征。&lt;h4&gt;翻译&lt;/h4&gt;目标：心电图在诊断心脏疾病中起着关键作用；然而，基于人工智能的心电图分析的有效性常常受限于标记数据的可用性。自监督学习可以通过利用大规模无标签数据来解决这个问题。我们引入了PhysioCLR（心电图生理感知对比学习表示），这是一种生理感知的对比学习框架，整合了领域特定的先验知识，以增强基于心电图的心律失常分类的可泛化性和临床相关性。方法：在预训练过程中，PhysioCLR学习将具有相似临床相关特征的样本嵌入拉近，同时将不相似的推开。与现有方法不同，我们的方法将心电图生理相似性线索整合到对比学习中，促进学习临床上有意义的表示。此外，我们引入了特定于心电图的增强方法，这些方法在增强后保持心电图类别不变，并提出了一种混合损失函数来进一步优化学习到的表示质量。结果：我们在两个公开的心电图数据集（Chapman和Georgia）上对PhysioCLR进行多标签心电图诊断评估，以及一个标记为二元分类的私人ICU数据集。在Chapman、Georgia和私人队列中，PhysioCLR比最强基线模型平均提升了12%的AUROC，强调了其强大的跨数据集泛化能力。结论：通过将生理知识嵌入对比学习，PhysioCLR使模型能够学习临床上有意义且可迁移的心电图特征。意义：PhysioCLR展示了生理信息感知的自监督学习在提供更有效且标记高效的心电图诊断方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heartconditions; however, the effectiveness of artificial intelligence (AI)-basedECG analysis is often hindered by the limited availability of labeled data.Self-supervised learning (SSL) can address this by leveraging large-scaleunlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive LearningRepresentation for ECG), a physiology-aware contrastive learning framework thatincorporates domain-specific priors to enhance the generalizability andclinical relevance of ECG-based arrhythmia classification. Methods: Duringpretraining, PhysioCLR learns to bring together embeddings of samples thatshare similar clinically relevant features while pushing apart those that aredissimilar. Unlike existing methods, our method integrates ECG physiologicalsimilarity cues into contrastive learning, promoting the learning of clinicallymeaningful representations. Additionally, we introduce ECG- specificaugmentations that preserve the ECG category post augmentation and propose ahybrid loss function to further refine the quality of learned representations.Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,for multilabel ECG diagnoses, as well as a private ICU dataset labeled forbinary classification. Across the Chapman, Georgia, and private cohorts,PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,underscoring its robust cross-dataset generalization. Conclusion: By embeddingphysiological knowledge into contrastive learning, PhysioCLR enables the modelto learn clinically meaningful and transferable ECG eatures. Significance:PhysioCLR demonstrates the potential of physiology-informed SSL to offer apromising path toward more effective and label-efficient ECG diagnostics.</description>
      <author>example@mail.com (Nooshin Maghsoodi, Sarah Nassar, Paul F R Wilson, Minh Nguyen Nhat To, Sophia Mannina, Shamel Addas, Stephanie Sibley, David Maslove, Purang Abolmaesumi, Parvin Mousavi)</author>
      <guid isPermaLink="false">2509.08116v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAME-AB是一种创新的跨模态注意力框架，结合专家混合主干，用于抗体结合位点预测。通过整合五种生物学模态并采用自适应融合策略，有效解决了传统方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态注意力框架CAME-AB，用于稳健的抗体结合位点预测，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB集成了五种生物学基础的模态：原始氨基酸编码、BLOSUM置换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图。提出了自适应模态融合模块，使用Transformer编码器结合MoE模块促进特征专业化和容量扩展，并集成了监督对比学习目标以提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的大量实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上始终优于强大的基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过多模态特征融合和自适应跨模态推理实现了抗体结合位点预测的性能提升，为计算免疫学和治疗性抗体设计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB将五种生物学基础的模态整合为统一的多模态表示，包括原始氨基酸编码、BLOSUM置换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图。为了增强自适应跨模态推理，我们提出了自适应模态融合模块，学习根据全局相关性和输入特定贡献动态加权每个模态。Transformer编码器结合MoE模块进一步促进特征专业化和容量扩展。我们还集成了监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分离性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的大量实验表明，CAME-AB在多个指标上始终优于强大的基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v3</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
      <link>http://arxiv.org/abs/2509.05230v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference on Empirical Methods in Natural Language  Processing (EMNLP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURE是一个新颖的轻量级框架，通过分离和抑制概念性捷径同时保留内容信息，有效提高了预训练语言模型的鲁棒性和公平性，在多个数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;预训练语言模型在各种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性影响，这损害了模型的鲁棒性和公平性。&lt;h4&gt;目的&lt;/h4&gt;引入CURE框架，系统性地分离和抑制概念性捷径，同时保留必要的内容信息，以提高模型的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;首先通过一个由反转网络强化的专门内容提取器提取概念无关的表示，确保任务相关信息的最小损失；随后采用对比学习进行可控去偏，微调剩余概念线索的影响，使模型能根据目标任务适当减少有害偏见或利用有益相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在IMDB和Yelp数据集上使用三种预训练架构评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;CURE方法为对抗概念偏见提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;预训练语言模型在各种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性影响，这损害了模型的鲁棒性和公平性。在这项工作中，我们引入了CURE，一个新颖且轻量级的框架，可以系统性地分离和抑制概念性捷径，同时保留必要的内容信息。我们的方法首先通过一个由反转网络强化的专门内容提取器提取概念无关的表示，确保任务相关信息的最小损失。随后的可控去偏模块采用对比学习来微调剩余概念线索的影响，使模型能够根据目标任务适当减少有害偏见或利用有益相关性。在IMDB和Yelp数据集上使用三种预训练架构评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。我们的方法为对抗概念偏见提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained language models have achieved remarkable success across diverseapplications but remain susceptible to spurious, concept-driven correlationsthat impair robustness and fairness. In this work, we introduce CURE, a noveland lightweight framework that systematically disentangles and suppressesconceptual shortcuts while preserving essential content information. Our methodfirst extracts concept-irrelevant representations via a dedicated contentextractor reinforced by a reversal network, ensuring minimal loss oftask-relevant information. A subsequent controllable debiasing module employscontrastive learning to finely adjust the influence of residual conceptualcues, enabling the model to either diminish harmful biases or harnessbeneficial correlations as appropriate for the target task. Evaluated on theIMDB and Yelp datasets using three pre-trained architectures, CURE achieves anabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,while introducing minimal computational overhead. Our approach establishes aflexible, unsupervised blueprint for combating conceptual biases, paving theway for more reliable and fair language understanding systems.</description>
      <author>example@mail.com (Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci)</author>
      <guid isPermaLink="false">2509.05230v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions</title>
      <link>http://arxiv.org/abs/2509.08654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于特征的量子网络路由部分可观察马尔可夫决策过程(POMDP)框架，结合了信念状态规划和图神经网络(GNNs)，以解决动态量子系统中的部分可观察性、退相干和可扩展性挑战。&lt;h4&gt;背景&lt;/h4&gt;在动态量子系统中，部分可观察性、退相干和可扩展性是量子网络路由面临的主要挑战，需要新的方法来处理这些复杂问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理量子网络中部分可观察性、退相干和可扩展性挑战的路由框架，提高路由保真度和纠缠传输率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合GNN-POMDP架构，将复杂的量子网络动态（包括纠缠退相干和时间变化的信道噪声）编码到低维特征空间，并使用噪声自适应机制融合POMDP信念更新和GNN输出进行决策。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟量子网络实验中，该方法在多达100个节点的网络上显著提高了路由保真度和纠缠传输率，特别是在高退相干和非平稳条件下优于现有最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架为量子网络路由提供了有效的解决方案，通过结合图神经网络和POMDP框架，能够处理复杂的量子网络动态和噪声环境，实现了更好的路由性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于特征的量子网络路由部分可观察马尔可夫决策过程(POMDP)框架，结合了信念状态规划和图神经网络(GNNs)，以解决动态量子系统中的部分可观察性、退相干和可扩展性挑战。我们的方法将复杂的量子网络动态（包括纠缠退相干和时间变化的信道噪声）编码到低维特征空间，从而实现高效的信念更新和可扩展的策略学习。我们框架的核心是一个混合GNN-POMDP架构，它处理纠缠链的图结构表示以学习路由策略，并结合一个噪声自适应机制，该机制融合POMDP信念更新和GNN输出以实现鲁棒的决策制定。我们提供了理论分析，确立了信念收敛、策略改进和对噪声鲁棒性的保证。在多达100个节点的模拟量子网络上的实验表明，与最先进的基线相比，路由保真度和纠缠传输率有显著提高，特别是在高退相干和非平稳条件下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a feature-based Partially Observable Markov DecisionProcess (POMDP) framework for quantum network routing, combining belief-stateplanning with Graph Neural Networks (GNNs) to address partial observability,decoherence, and scalability challenges in dynamic quantum systems. Ourapproach encodes complex quantum network dynamics, including entanglementdegradation and time-varying channel noise, into a low-dimensional featurespace, enabling efficient belief updates and scalable policy learning. The coreof our framework is a hybrid GNN-POMDP architecture that processesgraph-structured representations of entangled links to learn routing policies,coupled with a noise-adaptive mechanism that fuses POMDP belief updates withGNN outputs for robust decision making. We provide a theoretical analysisestablishing guarantees for belief convergence, policy improvement, androbustness to noise. Experiments on simulated quantum networks with up to 100nodes demonstrate significant improvements in routing fidelity and entanglementdelivery rates compared to state-of-the-art baselines, particularly under highdecoherence and nonstationary conditions.</description>
      <author>example@mail.com (Amirhossein Taherpour, Abbas Taherpour, Tamer Khattab)</author>
      <guid isPermaLink="false">2509.08654v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Facet: highly efficient E(3)-equivariant networks for interatomic potentials</title>
      <link>http://arxiv.org/abs/2509.08418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Facet，一种用于高效机器学习势能面的图神经网络架构，通过系统分析可导向GNNs开发，解决了计算材料发现中的计算瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;计算材料发现受限于第一性原理计算的高成本。现有的机器学习势能面方法虽然前景广阔，但面临计算瓶颈。可导向图神经网络(GNNs)利用球谐函数编码几何结构，尊重原子对称性，但维持等变性困难且计算复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的机器学习势能面的GNN架构，通过系统分析可导向GNNs来创建性能更优、计算效率更高的模型。&lt;h4&gt;方法&lt;/h4&gt;用样条替换用于原子间距离的多层感知器(MLPs)，减少计算和内存需求；引入一种通用的等变层，通过球面网格投影混合节点信息，然后使用标准MLPs，这种方法比张量积更快且比线性或门控层更具表现力。&lt;h4&gt;主要发现&lt;/h4&gt;在MPTrj数据集上，Facet与领先模型匹配，但参数少得多，训练计算量不到10%；在晶体弛豫任务中，它比MACE模型运行速度快两倍；SevenNet-0的参数可减少25%以上而不会损失准确性；这些技术使大型基础模型的训练速度提高10倍以上。&lt;h4&gt;结论&lt;/h4&gt;这些技术使大型基础模型的训练速度提高10倍以上，有可能改变计算材料发现的格局。&lt;h4&gt;翻译&lt;/h4&gt;计算材料发现受限于第一性原理计算的高成本。预测晶体结构能量的机器学习(ML)势能面很有前景，但现有方法面临计算瓶颈。可导向图神经网络(GNNs)利用球谐函数编码几何结构，尊重原子对称性——排列、旋转和平移——进行物理上真实的预测。然而，维持等变性很困难：激活函数必须修改，每层必须处理不同谐波阶数的多种数据类型。我们提出了Facet，一种用于高效ML势能面的GNN架构，通过系统分析可导向GNNs开发。我们的创新包括用样条替换用于原子间距离的昂贵多层感知器(MLPs)，在匹配性能的同时减少计算和内存需求。我们还引入了一种通用等变层，通过球面网格投影混合节点信息，然后使用标准MLPs——比张量积更快且比线性或门控层更具表现力。在MPTrj数据集上，Facet与领先模型匹配，但参数少得多，训练计算量不到10%。在晶体弛豫任务中，它比MACE模型运行速度快两倍。我们进一步表明SevenNet-0的参数可以减少25%以上而不会损失准确性。这些技术使ML势能面的大型基础模型的训练速度提高10倍以上，可能改变计算材料发现的格局。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器学习势能函数(MLIPs)训练过程中的计算效率低下问题。当前基于第一性原理的计算方法(如密度泛函理论)虽然准确但计算成本极高，而现有的机器学习势能函数训练大型基础模型需要大量计算资源和时间(如MACE-MP-0模型训练310天，SevenNet-0训练90天)。这种高计算成本阻碍了研究人员快速更新模型、探索不同训练数据和调整参数，限制了材料科学领域的研究进展和创新。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者系统分析了现有的可导向GNN设计面临的挑战，包括保持E(3)等变性的困难和标准网络组件需要修改的问题。他们识别出计算瓶颈在于等变性卷积中的消息过滤器，并考虑了网络中节点信息混合部分的计算复杂性和表达力之间的权衡。作者借鉴了SevenNet的架构基础，但采用了EquiformerV2中的等变层归一化方法，并受到计算机视觉中MLP-Mixer架构的启发，将其应用于球面处理。整体设计思路是简化计算密集组件，同时保持物理对称性和模型表达能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用高效的样条函数替代计算密集的多层感知器处理原子间距离，并提出S2-MLP-Mixer层通过球面网格投影混合节点信息。整体流程包括：1)将晶体编码为周期轨道图，原子为节点，最近邻为边；2)交互块中分解边向量为方向和距离分量，使用球谐函数和贝塞尔基编码，通过Clebsch-Gordan张量积组合特征；3)节点自交互中应用S2-MLP-Mixer处理信息；4)每层后使用残差连接和等变层归一化；5)最后通过读取层预测节点能量并求和得到总能量。整个流程保持E(3)等变性，同时优化计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)用简单线性层和样条函数替代计算密集的多层感知器处理距离信息，减少参数和计算需求；2)提出S2-MLP-Mixer层，通过球面网格投影混合节点信息，在速度和表达力上优于现有方法；3)灵活的消息归一化策略，根据估计的平均邻居数归一化；4)从零开始学习元素嵌入，不依赖预训练。相比MACE，Facet避免了其计算极其昂贵的对称张量积层；相比SevenNet/GNoMe/NequIP，Facet的非线性门控函数能混合非标量信息，而不仅是缩放它们。整体上，Facet在保持相当性能的同时实现了更高的训练和推理效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Facet通过系统优化E(3)-等变图神经网络架构，用高效样条替代计算密集的多层感知器，并创新的S2-MLP-Mixer层，实现了在保持高性能的同时将训练计算需求减少90%以上的机器学习势能函数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational materials discovery is limited by the high cost offirst-principles calculations. Machine learning (ML) potentials that predictenergies from crystal structures are promising, but existing methods facecomputational bottlenecks. Steerable graph neural networks (GNNs) encodegeometry with spherical harmonics, respecting atomic symmetries -- permutation,rotation, and translation -- for physically realistic predictions. Yetmaintaining equivariance is difficult: activation functions must be modified,and each layer must handle multiple data types for different harmonic orders.We present Facet, a GNN architecture for efficient ML potentials, developedthrough systematic analysis of steerable GNNs. Our innovations includereplacing expensive multi-layer perceptrons (MLPs) for interatomic distanceswith splines, which match performance while cutting computational and memorydemands. We also introduce a general-purpose equivariant layer that mixes nodeinformation via spherical grid projection followed by standard MLPs -- fasterthan tensor products and more expressive than linear or gate layers. On theMPTrj dataset, Facet matches leading models with far fewer parameters and under10% of their training compute. On a crystal relaxation task, it runs twice asfast as MACE models. We further show SevenNet-0's parameters can be reduced byover 25% with no accuracy loss. These techniques enable more than 10x fastertraining of large-scale foundation models for ML potentials, potentiallyreshaping computational materials discovery.</description>
      <author>example@mail.com (Nicholas Miklaucic, Lai Wei, Rongzhi Dong, Nihang Fu, Sadman Sadeed Omee, Qingyang Li, Sourin Dey, Victor Fung, Jianjun Hu)</author>
      <guid isPermaLink="false">2509.08418v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research</title>
      <link>http://arxiv.org/abs/2509.08553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PEHRT，一个标准化的电子健康记录(EHR)数据整合流程，通过多机构数据整合提高转化研究的可靠性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多机构EHR数据的整合分析可以借助更大更多样化的患者群体和多种数据模态来提高转化研究的可靠性和泛化能力，但跨机构整合EHR数据面临数据异质性、语义差异和隐私问题等主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个标准化的流程来解决跨机构EHR数据整合的挑战，实现高效的数据整合而不需要个体级数据共享。&lt;h4&gt;方法&lt;/h4&gt;PEHRT是一个包含两个核心模块的标准化流程：(1)数据预处理和(2)表示学习。该流程将EHR数据映射到标准编码系统，使用先进的机器学习技术生成研究就绪的数据集，且与数据模型无关，可在各机构间简化执行。&lt;h4&gt;主要发现&lt;/h4&gt;PEHRT能够在不共享个体级数据的情况下有效整合多机构EHR数据，生成研究就绪的数据集，并在各种任务中展示了其效用。&lt;h4&gt;结论&lt;/h4&gt;PEHRT是一个有效的EHR数据整合解决方案，研究人员提供了完整的开源软件套件和用户友好的教程，使其能够被广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;多机构电子健康记录(EHR)数据的整合分析通过利用更大、更多样化的患者群体和整合多种数据模态，提高了转化研究的可靠性和泛化能力。然而，由于数据异质性、语义差异和隐私问题，跨机构整合EHR数据面临重大挑战。为解决这些挑战，我们引入了PEHRT，这是一个用于高效EHR数据整合的标准化流程，包含两个核心模块：(1)数据预处理和(2)表示学习。PEHRT将EHR数据映射到标准编码系统，并使用先进的机器学习技术生成研究就绪的数据集，无需个体级数据共享。我们的流程也与数据模型无关，基于我们丰富的实际经验设计，可在各机构间简化执行。我们提供了完整的开源软件套件，配有用户友好的教程，并使用来自不同医疗系统的数据在各种任务中证明了PEHRT的效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrative analysis of multi-institutional Electronic Health Record (EHR)data enhances the reliability and generalizability of translational research byleveraging larger, more diverse patient cohorts and incorporating multiple datamodalities. However, harmonizing EHR data across institutions poses majorchallenges due to data heterogeneity, semantic differences, and privacyconcerns. To address these challenges, we introduce $\textit{PEHRT}$, astandardized pipeline for efficient EHR data harmonization consisting of twocore modules: (1) data pre-processing and (2) representation learning. PEHRTmaps EHR data to standard coding systems and uses advanced machine learning togenerate research-ready datasets without requiring individual-level datasharing. Our pipeline is also data model agnostic and designed for streamlinedexecution across institutions based on our extensive real-world experience. Weprovide a complete suite of open source software, accompanied by auser-friendly tutorial, and demonstrate the utility of PEHRT in a variety oftasks using data from diverse healthcare systems.</description>
      <author>example@mail.com (Jessica Gronsbell, Vidul Ayakulangara Panickan, Chris Lin, Thomas Charlon, Chuan Hong, Doudou Zhou, Linshanshan Wang, Jianhui Gao, Shirley Zhou, Yuan Tian, Yaqi Shi, Ziming Gan, Tianxi Cai)</author>
      <guid isPermaLink="false">2509.08553v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening</title>
      <link>http://arxiv.org/abs/2509.08502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了手性动作识别这一新任务，并提出了一种自监督方法来构建时间敏感的紧凑视频表示，该方法在多个数据集上表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有视频嵌入模型在表示随时间变化的简单视觉变化方面表现不佳，而日常生活中存在大量需要理解时间变化的手性动作对。&lt;h4&gt;目的&lt;/h4&gt;开发能够敏感感知视觉随时间变化的紧凑视频表示，并构建时间感知的视频表示，使手性动作对具有线性可分性。&lt;h4&gt;方法&lt;/h4&gt;提出一种自监督适应方案，将时间敏感性注入到冻结图像特征序列中；基于自编码器构建模型，并引入受感知拉直启发的潜在空间归纳偏置。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在三个数据集上提供了紧凑但时间敏感的视频表示；优于在大型视频数据集上预训练的更大的视频模型；与现有模型结合时，在标准基准测试中提高了分类性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功构建了时间敏感的紧凑视频表示，在手性动作识别任务上表现优异，并能提升现有模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;我们的目标是开发对随时间变化的视觉变化敏感的紧凑视频表示。为了衡量这种时间敏感性，我们引入了一个新任务：手性动作识别，其中需要区分一对时间上相反的动作，如'开门vs关门'、'接近vs远离某物'、'折叠vs展开纸张'等。这类动作(i)在日常生活中频繁出现，(ii)需要理解随时间变化的简单视觉变化(物体状态、大小、空间位置、计数等)，(iii)已知许多视频嵌入表示效果不佳。我们的目标是构建时间感知的视频表示，使这些手性动作对具有线性可分性。为此，我们提出了一种自监督适应方案，将时间敏感性注入到冻结图像特征序列中。我们的模型基于自编码器，其潜在空间具有受感知拉直启发的归纳偏置。我们证明，在三个数据集上，这为所提出的任务提供了紧凑但时间敏感的视频表示。我们的方法(i)优于在大型视频数据集上预训练的更大的视频模型，(ii)与这些现有模型结合时，在标准基准测试中提高了分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our objective is to develop compact video representations that are sensitiveto visual change over time. To measure such time-sensitivity, we introduce anew task: chiral action recognition, where one needs to distinguish between apair of temporally opposite actions, such as "opening vs. closing a door","approaching vs. moving away from something", "folding vs. unfolding paper",etc. Such actions (i) occur frequently in everyday life, (ii) requireunderstanding of simple visual change over time (in object state, size, spatialposition, count . . . ), and (iii) are known to be poorly represented by manyvideo embeddings. Our goal is to build time aware video representations whichoffer linear separability between these chiral pairs. To that end, we propose aself-supervised adaptation recipe to inject time-sensitivity into a sequence offrozen image features. Our model is based on an auto-encoder with a latentspace with inductive bias inspired by perceptual straightening. We show thatthis results in a compact but time-sensitive video representation for theproposed task across three datasets: Something-Something, EPIC-Kitchens, andCharade. Our method (i) outperforms much larger video models pre-trained onlarge-scale video datasets, and (ii) leads to an improvement in classificationperformance on standard benchmarks when combined with these existing models.</description>
      <author>example@mail.com (Piyush Bagad, Andrew Zisserman)</author>
      <guid isPermaLink="false">2509.08502v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.08470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Sparse MERIT框架，通过多任务学习同时优化语音情感识别和语音增强任务，解决了传统方法在噪声条件下的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;语音情感识别(SER)在构建情感感知语音系统中起关键作用，但在噪声条件下性能显著下降。语音增强(SE)可以提高鲁棒性，但会引入模糊情感线索的伪影，并增加计算开销。传统多任务学习模型在SE和SER任务中存在梯度干扰和表示冲突问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统多任务学习在语音增强和情感识别任务中存在的梯度干扰和表示冲突问题，提高在噪声条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT)，一种灵活的多任务学习框架，在自监督语音表示上应用帧级专家路由。该框架包含任务特定的门控网络，动态地从共享专家池中为每帧选择专家，实现参数高效和任务自适应的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上都优于基线模型。在-5 dB信噪比条件下，相比SE预处理基线，SER F1-macro平均提高12.0%；相比简单MTL基线提高3.4%，并且在未见噪声条件下具有统计显著性。对于SE任务，分段信噪比(SSNR)比SE预处理基线提高28.2%，比简单MTL基线提高20.0%。&lt;h4&gt;结论&lt;/h4&gt;Sparse MERIT在嘈杂环境中为情感识别和增强任务提供了稳健且可泛化的性能。&lt;h4&gt;翻译&lt;/h4&gt;语音情感识别(SER)在构建情感感知语音系统中起着关键作用，但在噪声条件下其性能会显著下降。尽管语音增强(SE)可以提高鲁棒性，但它常常会引入模糊情感线索的伪影，并为管道增加计算开销。多任务学习(MTL)通过联合优化SE和SER任务提供了一种替代方案。然而，传统的共享主干模型经常在任务间遭受梯度干扰和表示冲突。为解决这些挑战，我们提出了稀疏专家表示集成技术(Sparse MERIT)，一种灵活的MTL框架，该框架在自监督语音表示上应用帧级专家路由。Sparse MERIT集成了任务特定的门控网络，动态地从共享专家池中为每帧选择专家，实现了参数高效和任务自适应的表示学习。在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上都一致优于基线模型。在最具挑战性的-5 dB信噪比条件下，Sparse MERIT比依赖SE预处理策略的基线平均提高SER F1-macro 12.0%，比简单MTL基线提高3.4%，在未见噪声条件下具有统计显著性。对于SE任务，Sparse MERIT将分段信噪比(SSNR)比SE预处理基线提高28.2%，比简单MTL基线提高20.0%。这些结果表明，Sparse MERIT在嘈杂环境中为情感识别和增强任务提供了稳健且可泛化的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech emotion recognition (SER) plays a critical role in buildingemotion-aware speech systems, but its performance degrades significantly undernoisy conditions. Although speech enhancement (SE) can improve robustness, itoften introduces artifacts that obscure emotional cues and adds computationaloverhead to the pipeline. Multi-task learning (MTL) offers an alternative byjointly optimizing SE and SER tasks. However, conventional shared-backbonemodels frequently suffer from gradient interference and representationalconflicts between tasks. To address these challenges, we propose the SparseMixture-of-Experts Representation Integration Technique (Sparse MERIT), aflexible MTL framework that applies frame-wise expert routing overself-supervised speech representations. Sparse MERIT incorporates task-specificgating networks that dynamically select from a shared pool of experts for eachframe, enabling parameter-efficient and task-adaptive representation learning.Experiments on the MSP-Podcast corpus show that Sparse MERIT consistentlyoutperforms baseline models on both SER and SE tasks. Under the mostchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERITimproves SER F1-macro by an average of 12.0% over a baseline relying on a SEpre-processing strategy, and by 3.4% over a naive MTL baseline, withstatistical significance on unseen noise conditions. For SE, Sparse MERITimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline andby 20.0% over the naive MTL baseline. These results demonstrate that SparseMERIT provides robust and generalizable performance for both emotionrecognition and enhancement tasks in noisy environments.</description>
      <author>example@mail.com (Jing-Tong Tzeng, Carlos Busso, Chi-Chun Lee)</author>
      <guid isPermaLink="false">2509.08470v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
      <link>http://arxiv.org/abs/2509.08392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于交通监控图像增强任务的垂直残差自编码器（VRAE）架构，通过在每个编码阶段注入感知输入的特征，有效提升了车牌识别系统在恶劣条件下的性能。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的交通监控中，车辆图像在恶劣天气、光线不足或高速运动的情况下常常受到严重的噪声和模糊影响，这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。&lt;h4&gt;目的&lt;/h4&gt;快速实时地恢复这些退化图像是增强识别性能的关键预处理步骤。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种垂直残差自编码器（VRAE）架构，采用了一种增强策略，使用辅助块在每个编码阶段注入感知输入的特征，以引导表示学习过程，与传统的自编码器相比，能够在整个网络中更好地保留一般信息。&lt;h4&gt;主要发现&lt;/h4&gt;在带有可见车牌的车辆图像数据集上的实验表明，VRAE方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的AE相比，它将PSNR提高了约20%，将NMSE降低了约50%，将SSIM提高了1%，而参数仅增加了约1%。&lt;h4&gt;结论&lt;/h4&gt;VRAE架构在交通监控图像增强任务中表现优异，能够在保持参数量增加很小的情况下显著提高图像质量指标。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的交通监控中，车辆图像在恶劣天气、光线不足或高速运动的情况下常常受到严重的噪声和模糊影响。这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。快速实时地恢复这些退化图像是增强识别性能的关键预处理步骤。在这项工作中，我们提出了一种用于交通监控图像增强任务的垂直残差自编码器（VRAE）架构。该方法采用了一种增强策略，该策略使用一个辅助块，在每个编码阶段注入感知输入的特征，以引导表示学习过程，与传统的自编码器相比，能够在整个网络中更好地保留一般信息。在带有可见车牌的车辆图像数据集上的实验表明，我们的方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的AE相比，它将PSNR提高了约20%，将NMSE降低了约50%，将SSIM提高了1%，而参数仅增加了约1%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world traffic surveillance, vehicle images captured under adverseweather, poor lighting, or high-speed motion often suffer from severe noise andblur. Such degradations significantly reduce the accuracy of license platerecognition systems, especially when the plate occupies only a small regionwithin the full vehicle image. Restoring these degraded images a fast realtimemanner is thus a crucial pre-processing step to enhance recognitionperformance. In this work, we propose a Vertical Residual Autoencoder (VRAE)architecture designed for the image enhancement task in traffic surveillance.The method incorporates an enhancement strategy that employs an auxiliaryblock, which injects input-aware features at each encoding stage to guide therepresentation learning process, enabling better general informationpreservation throughout the network compared to conventional autoencoders.Experiments on a vehicle image dataset with visible license plates demonstratethat our method consistently outperforms Autoencoder (AE), GenerativeAdversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE atthe same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%,and enhances SSIM by 1\%, while requiring only a marginal increase of roughly1\% in parameters.</description>
      <author>example@mail.com (Cuong Nguyen, Dung T. Tran, Hong Nguyen, Xuan-Vu Phan, Nam-Phong Nguyen)</author>
      <guid isPermaLink="false">2509.08392v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</title>
      <link>http://arxiv.org/abs/2509.08376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的通用框架，用于将视频数据分解为动态运动和静态内容两个组成部分。该方法采用自监督流水线，假设更少，归纳偏差更小，能够有效学习视频的解缠结表示。&lt;h4&gt;背景&lt;/h4&gt;视频数据包含动态运动和静态内容两个重要组成部分，如何有效解缠结这两个成分是视频理解和生成领域的挑战，现有方法通常有较多假设和归纳偏差，限制了通用性。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖且通用的框架，将视频数据分解为动态运动和静态内容两个组成部分，同时减少假设和归纳偏差，实现更通用的视频表示学习。&lt;h4&gt;方法&lt;/h4&gt;使用基于Transformer的架构联合生成帧级运动和片段级内容的隐式特征；引入低比特率矢量量化作为信息瓶颈促进解缠结；将比特率控制的潜在运动和内容作为条件输入用于去噪扩散模型；通过自监督方式进行表示学习；在多种视频类型上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在说话头视频上有效进行运动迁移和自回归运动生成；能够推广到2D卡通角色像素精灵等其他视频类型；通过低比特率矢量量化成功促进运动和内容解缠结；基于Transformer的架构能有效生成灵活的隐式特征表示。&lt;h4&gt;结论&lt;/h4&gt;该工作为自监督学习解缠结视频表示提供了新视角，通过减少假设和归纳偏差，实现了更通用的视频表示学习，有助于视频分析和生成领域的更广泛发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖且通用的框架，将视频数据分解为其动态运动和静态内容组成部分。我们提出的方法是一个自监督流水线，比之前的工作假设更少，归纳偏差更小：它利用基于Transformer的架构联合生成帧级运动和片段级内容的灵活隐式特征，并引入低比特率矢量量化作为信息瓶颈，促进解缠结并形成有意义的离散运动空间。比特率控制的潜在运动和内容被用作去噪扩散模型的条件输入，以促进自监督表示学习。我们在真实世界的说话头视频上验证了我们的解缠结表示学习框架，并进行运动迁移和自回归运动生成任务。此外，我们还展示了我们的方法可以推广到其他类型的视频数据，如2D卡通角色的像素精灵。我们的工作为自监督学习解缠结视频表示提供了新视角，有助于视频分析和生成领域的更广泛发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel and general framework to disentangle video data into itsdynamic motion and static content components. Our proposed method is aself-supervised pipeline with less assumptions and inductive biases thanprevious works: it utilizes a transformer-based architecture to jointlygenerate flexible implicit features for frame-wise motion and clip-wisecontent, and incorporates a low-bitrate vector quantization as an informationbottleneck to promote disentanglement and form a meaningful discrete motionspace. The bitrate-controlled latent motion and content are used as conditionalinputs to a denoising diffusion model to facilitate self-supervisedrepresentation learning. We validate our disentangled representation learningframework on real-world talking head videos with motion transfer andauto-regressive motion generation tasks. Furthermore, we also show that ourmethod can generalize to other types of video data, such as pixel sprites of 2Dcartoon characters. Our work presents a new perspective on self-supervisedlearning of disentangled video representations, contributing to the broaderfield of video analysis and generation.</description>
      <author>example@mail.com (Xiao Li, Qi Chen, Xiulian Peng, Kai Yu, Xie Chen, Yan Lu)</author>
      <guid isPermaLink="false">2509.08376v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training</title>
      <link>http://arxiv.org/abs/2509.08311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种胸部CT上的相似性驱动跨粒度预训练（SimCroP）框架，结合相似性驱动对齐和跨粒度融合技术，有效解决了CT影像中病变分布的空间稀疏性问题，提高了放射影像解释能力，在多尺度下游任务中表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉语言预训练在从大量配对的放射影像和报告中学习代表性特征方面显示出巨大潜力。然而，在CT扫描中，包含复杂结构的病变分布具有空间稀疏性，且报告中不同病理描述与其在放射影像中对应区域之间的复杂关系带来了额外挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个胸部CT上的相似性驱动跨粒度预训练框架，结合相似性驱动对齐和跨粒度融合，以提高放射影像解释能力，更好地捕捉稀疏放射影像中的关键病理结构。&lt;h4&gt;方法&lt;/h4&gt;利用多模态掩码建模优化编码器以理解放射影像的低级语义；设计相似性驱动对齐机制使编码器自适应选择并与报告句子对齐；通过跨粒度融合模块整合实例级别和单词-块级别的多模态信息；在大型配对CT-报告数据集上预训练，并在五个公共数据集上的图像分类和分割任务中验证。&lt;h4&gt;主要发现&lt;/h4&gt;SimCroP框架在实验中超越了最先进的医学自监督学习方法和医学视觉语言预训练方法，有效解决了CT影像中病变分布的空间稀疏性问题，提高了多尺度下游任务的性能。&lt;h4&gt;结论&lt;/h4&gt;SimCroP通过相似性驱动对齐和跨粒度融合技术，能够更好地捕捉稀疏放射影像中的关键病理结构，在多尺度下游任务中表现出改进的性能，为医学影像分析提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉语言预训练在从大量配对的放射影像和报告中学习代表性特征方面显示出巨大潜力。然而，在计算机断层扫描（CT）扫描中，包含复杂结构的病变分布具有空间稀疏性。此外，报告中每个句子内不同病理描述与其在放射影像中对应子区域之间的复杂且隐含的关系带来了额外挑战。在本文中，我们在胸部CT上提出了一个相似性驱动跨粒度预训练（SimCroP）框架，它结合相似性驱动对齐和跨粒度融合来提高放射影像解释能力。我们首先利用多模态掩码建模来优化编码器，以便从放射影像中理解精确的低级语义。然后设计相似性驱动对齐来预训练编码器，以自适应选择并与报告中每个句子相对应的正确块进行对齐。跨粒度融合模块整合实例级别和单词-块级别的多模态信息，这有助于模型更好地捕捉稀疏放射影像中的关键病理结构，从而提高多尺度下游任务的性能。SimCroP在大型配对CT-报告数据集上进行预训练，并在五个公共数据集上的图像分类和分割任务中进行了验证。实验结果表明，SimCroP优于最先进的医学自监督学习方法和医学视觉语言预训练方法。代码和模型可在 https://github.com/ToniChopp/SimCroP 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学影像（特别是CT扫描）中两个关键问题：一是病变的空间分布稀疏性问题，病变包含复杂结构但分布稀疏，导致特征提取困难；二是医学影像报告中不同病理描述与影像中相应子区域之间的复杂隐含关系难以建立对应。这些问题在现实中非常重要，因为医学影像标注需要专业医生投入大量时间和精力，资源密集且负担重，这限制了深度学习在医学影像中的应用；同时，医学影像报告包含丰富的语义信息，能有效辅助影像理解，但如何有效利用这些信息一直是挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有医学视觉语言预训练方法在处理3D胸部CT时的局限性，特别是对空间稀疏性和报告结构复杂性的处理不足。作者借鉴了多模态掩码自编码器架构，使用Vision Transformer作为视觉编码器，BERT作为文本编码器。针对CT的空间稀疏性，设计了相似性驱动对齐机制，使模型能自动选择和与报告句子对齐正确的影像块；针对报告结构复杂性，引入跨粒度融合模块，整合不同粒度的多模态信息。作者还借鉴了对比学习思想，但将其应用于细粒度的句子-影像块对齐，以及掩码建模方法用于图像和文本重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; SimCroP的核心思想是通过相似性驱动对齐和跨粒度融合来改进医学影像表征学习，让模型自动学习将医学报告中的描述性句子与影像中对应的子区域对齐，同时整合全局和局部信息以更好地理解和稀疏的医学影像。整体流程包括：1)多模态掩码建模：将CT影像分割并掩码75%的块，对报告文本分词并掩码部分词，通过编码器处理未掩码内容，解码器重建被掩码内容；2)相似性驱动对齐：计算句子与影像块相似度，为每个句子选择最相似的K个块，通过对比学习损失对齐特征；3)跨粒度融合：通过全局平均池化获取实例级别特征，通过交叉注意力计算词块级别特征，融合后重建被掩码文本；4)整体优化：结合影像重建、相似性对齐和文本重建损失优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)相似性驱动对齐机制：无需人工标注自动对齐句子与影像子区域，不同于之前方法依赖全局对比学习；2)跨粒度融合：同时整合实例级别和词块级别多模态信息，优于仅使用交叉注意力的方法；3)针对医学影像特殊性的设计：专门解决CT空间稀疏性和报告结构复杂性；4)多目标学习框架：结合掩码图像建模、句子-子区域对齐和跨粒度掩码报告建模。相比之前工作，SimCroP能更好地处理医学影像的特殊性，自动建立精确的文本-影像对应关系，整合多粒度信息，从而获得更全面的表征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimCroP通过相似性驱动对齐和跨粒度融合的创新方法，有效解决了医学影像中的空间稀疏性问题，显著提升了医学影像表征学习能力，在多个下游任务上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical vision-language pre-training shows great potential in learningrepresentative features from massive paired radiographs and reports. However,in computed tomography (CT) scans, the distribution of lesions which containintricate structures is characterized by spatial sparsity. Besides, the complexand implicit relationships between different pathological descriptions in eachsentence of the report and their corresponding sub-regions in radiographs poseadditional challenges. In this paper, we propose a Similarity-DrivenCross-Granularity Pre-training (SimCroP) framework on chest CTs, which combinessimilarity-driven alignment and cross-granularity fusion to improve radiographinterpretation. We first leverage multi-modal masked modeling to optimize theencoder for understanding precise low-level semantics from radiographs. Then,similarity-driven alignment is designed to pre-train the encoder to adaptivelyselect and align the correct patches corresponding to each sentence in reports.The cross-granularity fusion module integrates multimodal information acrossinstance level and word-patch level, which helps the model better capture keypathology structures in sparse radiographs, resulting in improved performancefor multi-scale downstream tasks. SimCroP is pre-trained on a large-scalepaired CT-reports dataset and validated on image classification andsegmentation tasks across five public datasets. Experimental resultsdemonstrate that SimCroP outperforms both cutting-edge medical self-supervisedlearning methods and medical vision-language pre-training methods. Codes andmodels are available at https://github.com/ToniChopp/SimCroP.</description>
      <author>example@mail.com (Rongsheng Wang, Fenghe Tang, Qingsong Yao, Rui Yan, Xu Zhang, Zhen Huang, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, Shaohua Kevin Zhou)</author>
      <guid isPermaLink="false">2509.08311v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>How Far Are We from True Unlearnability?</title>
      <link>http://arxiv.org/abs/2509.08058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了现有不可学习方法在多任务场景下的局限性，从模型优化角度分析了不可学习性的本质，并提出了新的评估指标。&lt;h4&gt;背景&lt;/h4&gt;高质量数据在大模型时代至关重要，但使用未经授权的数据进行模型训练损害数据所有者利益。为解决此问题，提出了不可学习方法(UEs)，通过破坏数据训练可用性生成不可学习的示例。&lt;h4&gt;目的&lt;/h4&gt;探究如何实现真正跨任务的不可学习示例，并评估现有不可学习方法的能力边界。&lt;h4&gt;方法&lt;/h4&gt;使用简单模型架构观察干净与中毒模型的收敛差异；从损失景观分析关键参数优化路径；提出锐度感知可学习性(SAL)量化参数不可学习性；提出不可学习距离(UD)衡量数据不可学习性；使用UD对主流不可学习方法进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;在Taskonomy多任务数据集上，现有UEs在语义分割等任务上仍表现良好，未能实现真正的跨任务不可学习性；损失景观与不可学习性有密切关系。&lt;h4&gt;结论&lt;/h4&gt;现有不可学习方法在多任务场景下存在局限性，提出的UD指标有助于促进社区对不可学习方法能力边界的认识。&lt;h4&gt;翻译&lt;/h4&gt;高质量数据在大模型时代扮演着不可或缺的角色，但使用未经授权的数据进行模型训练严重损害了数据所有者的利益。为克服这一威胁，已提出几种不可学习方法，它们通过破坏数据的训练可用性来生成不可学习的示例(UEs)。显然，由于训练目的未知和现有模型的强大表征学习能力，这些数据预计对多任务模型都不可学习，即不会帮助提高模型性能。然而，出乎意料的是，我们在Taskonomy多任务数据集上发现，UEs在语义分割等任务上仍然表现良好，未能表现出跨任务的不可学习性。这种现象让我们质疑：我们距离实现真正不可学习的示例还有多远？我们尝试从模型优化的角度回答这个问题。为此，我们使用简单模型架构观察了干净模型和中毒模型的收敛过程差异。随后，从损失景观中我们发现只有部分关键参数优化路径显示出显著差异，这表明损失景观与不可学习性密切相关。因此，我们利用损失景观解释了UEs的潜在原因，并提出了锐度感知可学习性(SAL)来基于此解释量化参数的不可学习性。此外，我们提出了不可学习距离(UD)来基于干净和中毒模型中参数的SAL分布来衡量数据的不可学习性。最后，我们使用提出的UD对主流不可学习方法进行了基准测试，旨在促进社区对现有不可学习方法能力边界的认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality data plays an indispensable role in the era of large models, butthe use of unauthorized data for model training greatly damages the interestsof data owners. To overcome this threat, several unlearnable methods have beenproposed, which generate unlearnable examples (UEs) by compromising thetraining availability of data. Clearly, due to unknown training purposes andthe powerful representation learning capabilities of existing models, thesedata are expected to be unlearnable for models across multiple tasks, i.e.,they will not help improve the model's performance. However, unexpectedly, wefind that on the multi-task dataset Taskonomy, UEs still perform well in taskssuch as semantic segmentation, failing to exhibit cross-task unlearnability.This phenomenon leads us to question: How far are we from attaining trulyunlearnable examples? We attempt to answer this question from the perspectiveof model optimization. To this end, we observe the difference in theconvergence process between clean and poisoned models using a simple modelarchitecture. Subsequently, from the loss landscape we find that only a part ofthe critical parameter optimization paths show significant differences,implying a close relationship between the loss landscape and unlearnability.Consequently, we employ the loss landscape to explain the underlying reasonsfor UEs and propose Sharpness-Aware Learnability (SAL) to quantify theunlearnability of parameters based on this explanation. Furthermore, we proposean Unlearnable Distance (UD) to measure the unlearnability of data based on theSAL distribution of parameters in clean and poisoned models. Finally, weconduct benchmark tests on mainstream unlearnable methods using the proposedUD, aiming to promote community awareness of the capability boundaries ofexisting unlearnable methods.</description>
      <author>example@mail.com (Kai Ye, Liangcai Su, Chenxiong Qian)</author>
      <guid isPermaLink="false">2509.08058v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Unidimensional semi-discrete partial optimal transport</title>
      <link>http://arxiv.org/abs/2509.08799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种解决一维部分最优传输问题的半离散形式，通过正则化方法克服了对偶函数正则性不足的困难，并验证了二次收敛率，同时展示了该方法在稳定性和计算效率方面的优势。&lt;h4&gt;背景&lt;/h4&gt;一维部分最优传输问题在风险管理、人群运动建模和点云配准等领域有广泛应用。与高维情况不同，一维情况下的对偶函数正则性较差，这给问题的求解带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究一维二次成本部分最优传输的半离散形式，解决对偶函数正则性不足的问题，并设计有效的数值方法。&lt;h4&gt;方法&lt;/h4&gt;引入基于沿辅助维度加厚密度的正则化过程，证明正则化对偶问题最大值以二次速率收敛到原始对偶问题最大值，并开发利用正则化函数的数值方案。&lt;h4&gt;主要发现&lt;/h4&gt;正则化对偶问题的最大值以二次速率收敛到原始对偶问题的最大值；所提出的数值方案能够有效解决一维部分传输问题；与全离散设置相比，该方法具有更好的稳定性和计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的正则化方法有效解决了一维部分最优传输问题中的正则性不足挑战，提供了具有二次收敛率的数值方案，并在实际应用中表现出良好的稳定性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了一维二次成本部分最优传输的半离散形式，其中概率密度被部分传输到总质量更小的有限狄拉克质量点之和。这个问题在风险管理、人群运动建模以及用于点云配准的切片部分传输算法中自然出现。与高维设置不同，一维情况下的对偶函数正则性较差。为了克服这一困难，我们引入了一种基于沿辅助维度加厚密度的正则化过程。我们证明了正则化对偶问题的最大值收敛到原始对偶问题的最大值，收敛速度与引入的厚度成二次方关系。我们进一步提供了一个利用正则化函数的数值方案，并通过模拟验证了我们的分析，确认了二次收敛率。最后，我们比较了半离散和全离散设置，证明我们的方法在一维部分传输问题上既提高了稳定性又提高了计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the semi-discrete formulation of one-dimensional partial optimaltransport with quadratic cost, where a probability density is partiallytransported to a finite sum of Dirac masses of smaller total mass. This problemarises naturally in applications such as risk management, the modeling of crowdmotion, and sliced partial transport algorithms for point cloud registration.Unlike higher-dimensional settings, the dual functional in the unidimensionalcase exhibits reduced regularity. To overcome this difficulty, we introduce aregularization procedure based on thickening the density along an auxiliarydimension. We prove that the maximizers of the regularized dual problemconverge to those of the original dual problem, with quadratic rate in theintroduced thickness. We further provide a numerical scheme that leverages theregularized functional, and we validate our analysis with simulations thatconfirm the quadratic convergence rate. Finally, we compare the semi-discreteand fully discrete settings, demonstrating that our approach offers bothimproved stability and computational efficiency for unidimensional partialtransport problems.</description>
      <author>example@mail.com (Adrien Cances, Hugo Leclerc)</author>
      <guid isPermaLink="false">2509.08799v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding</title>
      <link>http://arxiv.org/abs/2509.08685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于多分辨率B样条投影框架的3D点云有损属性压缩方法，通过端到端可微分的投影操作和数据驱动的优化技术实现了高效的点云属性压缩。&lt;h4&gt;背景&lt;/h4&gt;研究基于已编码的3D点云几何信息，在解码器端进行属性压缩处理。&lt;h4&gt;目的&lt;/h4&gt;解决在多分辨率B样条投影框架下的有损属性压缩问题，提高压缩效率和质量。&lt;h4&gt;方法&lt;/h4&gt;将目标连续3D属性函数投影到嵌套的B样条函数子空间中，通过变复杂度展开的率失真优化算法计算低通系数，使用ℓ1范数作为率项，并实现端到端可微分；同时采用从粗到细的预测器，以数据驱动方式优化系数调整。&lt;h4&gt;主要发现&lt;/h4&gt;通过率失真优化算法的前馈网络展开和ℓ1范数的使用，投影操作可以实现端到端可微分；从低分辨率到高分辨率的预测调整也可以通过数据驱动方式优化，提高压缩效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的多分辨率B样条投影框架结合数据驱动的优化方法，能够有效实现3D点云属性的高效压缩。&lt;h4&gt;翻译&lt;/h4&gt;在解码器端可用的已编码3D点云几何信息基础上，我们研究了多分辨率B样条投影框架下的有损属性压缩问题。首先将目标连续3D属性函数投影到一系列嵌套的子空间中，其中每个子空间由选定尺度的B样条基函数及其整数平移张成。通过将率失真优化算法展开为变复杂度的前馈网络来计算投影的低通系数，其中率项是促进稀疏的ℓ1范数。因此，投影操作是端到端可微分的。对于选定的从粗到细的预测器，然后调整系数以考虑从低分辨率到高分辨率的预测，这种预测也是以数据驱动方式优化的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given encoded 3D point cloud geometry available at the decoder, we study theproblem of lossy attribute compression in a multi-resolution B-splineprojection framework. A target continuous 3D attribute function is firstprojected onto a sequence of nested subspaces $\mathcal{F}^{(p)}_{l_0}\subseteq \cdots \subseteq \mathcal{F}^{(p)}_{L}$, where$\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basisfunction of order $p$ at a chosen scale and its integer shifts. The projectedlow-pass coefficients $F_l^*$ are computed by variable-complexity unrolling ofa rate-distortion (RD) optimization algorithm into a feed-forward network,where the rate term is the sparsity-promoting $\ell_1$-norm. Thus, theprojection operation is end-to-end differentiable. For a chosen coarse-to-finepredictor, the coefficients are then adjusted to account for the predictionfrom a lower-resolution to a higher-resolution, which is also optimized in adata-driven manner.</description>
      <author>example@mail.com (Tam Thuc Do, Philip A. Chou, Gene Cheung)</author>
      <guid isPermaLink="false">2509.08685v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title>
      <link>http://arxiv.org/abs/2509.08280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 12 figures, AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为E3DPC-GZSL的新方法，用于解决3D点云广义零样本语义分割中的过度自信预测问题，通过基于证据的不确定性估计器和改进的训练策略实现了对已见和未见类别的有效分类。&lt;h4&gt;背景&lt;/h4&gt;3D点云的广义零样本语义分割旨在将每个点分类为已见和未见类别，但这些模型存在显著挑战：它们倾向于做出有偏见的预测，偏向于训练过程中遇到的类别。在3D应用中，这一问题更为突出，因为训练数据规模通常小于基于图像的任务。&lt;h4&gt;目的&lt;/h4&gt;解决模型对已见类别的过度自信预测问题，且不依赖单独的已见和未见数据分类器。&lt;h4&gt;方法&lt;/h4&gt;提出E3DPC-GZSL方法，将基于证据的不确定性估计器集成到分类器中，使用动态校准堆叠因子调整预测概率，并引入新的训练策略，通过合并可学习参数和文本派生特征来改进语义空间，提高不确定性估计和模型对未见数据的优化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在ScanNet v2和S3DIS等广义零样本语义分割数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;E3DPC-GZSL方法有效解决了3D点云广义零样本语义分割中的过度自信问题，通过基于证据的不确定性估计和改进的训练策略，显著提高了对未见类别的分割性能。&lt;h4&gt;翻译&lt;/h4&gt;3D点云的广义零样本语义分割旨在将每个点分类为已见和未见类别。这些模型的一个显著挑战是它们倾向于做出有偏见的预测，通常偏向于训练过程中遇到的类别。在3D应用中，这一问题更为突出，因为训练数据的规模通常小于基于图像的任务。为解决这个问题，我们提出了一种名为E3DPC-GZSL的新方法，它减少了对已见类别的过度自信预测，而不依赖单独的已见和未见数据分类器。E3DPC-GZSL通过将基于证据的不确定性估计器集成到分类器中来解决过度自信问题。然后使用考虑逐点预测不确定性的动态校准堆叠因子来调整预测概率。此外，E3DPC-GZSL引入了一种新的训练策略，通过合并可学习参数和文本派生特征来改进语义空间，从而提高对未见数据的不确定性估计和模型优化。大量实验证明，所提出的方法在ScanNet v2和S3DIS等广义零样本语义分割数据集上达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决三维点云分割中的广义零样本学习(GZSL)问题，特别是模型对已见类别(训练时见过的类别)的过度自信预测偏差问题。这个问题在现实中很重要，因为自动驾驶、医疗成像等高风险应用需要精确的分割来确保安全和可靠性；现实世界场景可能包含训练时未遇到的类别，模型需要能泛化到这些新类别；三维点云数据标注成本高，难以覆盖所有可能的类别；现有方法过度依赖超参数且难以一致地应用于所有输入数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有GZSL方法的局限性，特别是对已见类别的过度自信预测问题，设计了基于证据理论的方法。他们评估了二元分类和校准堆叠两种主流方法，发现它们都严重依赖超参数且难以一致应用。作者借鉴了证据理论和主观逻辑中的不确定性估计方法，改进了校准堆叠的基本思想，使其能够动态调整而非使用固定超参数。此外，他们借鉴了语义空间对齐的思路，但引入了场景语义调优来增强特征表示。整体设计思路是利用不确定性估计实现动态校准，并通过语义调优解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用证据理论估计预测的不确定性，并基于此动态调整已见类别的预测概率；同时通过场景语义调优改善语义空间表示，解决三维零样本学习中的数据稀缺问题。整体实现流程分为三个阶段：1)训练编码器：从头开始训练编码器E提取特征向量；2)训练解码器：训练解码器D生成合成特征，通过融合可学习的场景语义向量与文本特征增强特征表示；3)训练基于证据的分类器：训练分类器C进行分割，同时训练不确定性估计器U评估预测置信度，使用三种损失函数优化。推理阶段使用编码器提取特征，分类器预测概率，不确定性估计器计算动态校准因子，应用校准调整已见类别的预测概率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)证据驱动的动态校准：基于证据理论实现动态校准因子，而非使用预定义的固定超参数；2)语义空间调优：将可学习的场景语义向量与文本特征融合，增强语义表示；3)多阶段训练策略：分阶段训练编码器、解码器和分类器；4)不确定性感知的损失函数：设计三种互补的损失函数，同时优化分割性能和不确定性估计；5)统一的处理框架：无需显式区分已见和未见类别，通过不确定性估计自动处理。相比之前工作，本文方法解决了超参数依赖问题，提高了模型在未见类别上的性能，并增强了特征表示能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于证据理论的三维点云广义零样本分割方法，通过动态校准预测概率和语义空间调优，有效解决了模型对已见类别的过度自信问题，显著提升了在未见类别上的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v39i4.32446&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized zero-shot semantic segmentation of 3D point clouds aims toclassify each point into both seen and unseen classes. A significant challengewith these models is their tendency to make biased predictions, often favoringthe classes encountered during training. This problem is more pronounced in 3Dapplications, where the scale of the training data is typically smaller than inimage-based tasks. To address this problem, we propose a novel method calledE3DPC-GZSL, which reduces overconfident predictions towards seen classeswithout relying on separate classifiers for seen and unseen data. E3DPC-GZSLtackles the overconfidence problem by integrating an evidence-based uncertaintyestimator into a classifier. This estimator is then used to adjust predictionprobabilities using a dynamic calibrated stacking factor that accounts forpointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a noveltraining strategy that improves uncertainty estimation by refining the semanticspace. This is achieved by merging learnable parameters with text-derivedfeatures, thereby improving model optimization for unseen data. Extensiveexperiments demonstrate that the proposed approach achieves state-of-the-artperformance on generalized zero-shot semantic segmentation datasets, includingScanNet v2 and S3DIS.</description>
      <author>example@mail.com (Hyeonseok Kim, Byeongkeun Kang, Yeejin Lee)</author>
      <guid isPermaLink="false">2509.08280v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 6 figures, conference, 7 tables, 15 formulas&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应概率匹配损失(APML)的新方法，用于点云预测任务中的点集比较，解决了传统损失函数的局限性，同时保持了计算效率。&lt;h4&gt;背景&lt;/h4&gt;在点云预测任务中，常用的损失函数如Chamfer距离(CD)、HyperCD和InfoCD依赖于最近邻分配，导致多对一对应关系，引起密集区域点拥塞和稀疏区域覆盖不足，且涉及非可微分操作。虽然地球移动距离(EMD)能更好地捕获结构相似性，但其三次计算复杂度限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的损失函数，避免最近邻分配问题，解决点拥塞和覆盖不足，避免非可微分操作，保持计算效率，且不需要手动调整超参数。&lt;h4&gt;方法&lt;/h4&gt;提出自适应概率匹配损失(APML)，这是一种一对一匹配的完全可微分近似方法，利用基于成对距离导出的温度缩放相似性矩阵上的Sinkhorn迭代。通过分析计算温度来保证最小分配概率，消除手动调整的需要。&lt;h4&gt;主要发现&lt;/h4&gt;APML集成到PoinTr、PCN、FoldingNet等架构中，在ShapeNet基准测试和从WiFi CSI测量生成3D人体点云的CSI2PC模型上表现优异；实现了更快的收敛速度，特别是在低密度区域有优越的空间分布，定量性能有所提升或相当，无需额外超参数搜索。&lt;h4&gt;结论&lt;/h4&gt;APML是一种有效的点云预测任务损失函数，解决了传统方法的局限性，同时保持了计算效率，在各种架构和任务上都表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;用于点云预测任务（如形状补全和生成）的深度学习模型的训练，严重依赖于衡量预测点集与真实点集之间差异的损失函数。常用的函数如Chamfer距离(CD)、HyperCD和InfoCD依赖于最近邻分配，这通常导致多对一对应关系，引起密集区域的点拥塞和稀疏区域的覆盖不足。这些损失还涉及由于索引选择导致的非可微分操作，可能影响基于梯度的优化。地球移动距离(EMD)强制一对一对应并能更有效地捕获结构相似性，但其三次计算复杂度限制了其实际应用。我们提出了自适应概率匹配损失（APML），这是一种一对一匹配的完全可微分近似方法，它利用基于成对距离导出的温度缩放相似性矩阵上的Sinkhorn迭代。我们通过分析计算温度来保证最小分配概率，消除了手动调整的需要。APML实现了接近二次的运行时间，与基于Chamfer的损失相当，并避免了非可微分操作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云重建任务中损失函数的设计问题。现有的常用损失函数如Chamfer Distance存在点聚集、稀疏区域覆盖不佳和非可微等问题，而Earth Mover's Distance虽然几何保真度高但计算复杂度太高。这个问题很重要，因为点云是三维数据的主要表示形式，广泛应用于自动驾驶、机器人、AR/VR等领域，高质量的点云重建对这些应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有损失函数的局限性：Chamfer Distance效率高但几何细节捕捉不足，EMD几何保真度高但计算成本太高。他们借鉴了最优传输理论，特别是熵正则化的Sinkhorn算法，该算法提供了可微的软对应关系。关键创新是解决了Sinkhorn算法需要手动调整正则化参数的问题，通过分析计算自适应温度来保证最小分配概率，从而消除了手动调整的需要。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是应用最优传输原理建立点集间的软概率对应关系，通过自适应温度机制控制分配锐度，无需手动调整。整体流程包括：1)计算预测点集与真实点集间的欧氏距离矩阵；2)应用自适应Softmax生成初始概率分配；3)双向匹配并对称化得到初始软分配矩阵；4)通过Sinkhorn归一化使矩阵近似双随机；5)计算期望匹配成本作为最终损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应概率匹配机制，完全可微且近似一对一匹配；2)自适应温度选择机制，通过闭式调度自动调整分配锐度；3)平衡计算效率与几何保真度，以近二次复杂度实现EMD级别的质量。相比之前的工作，APML避免了Chamfer Distance的点聚集问题，解决了EMD的高计算复杂度问题，并且不需要像其他CD变体那样手动调整多个参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; APML通过结合最优传输理论和自适应温度机制，提供了一种高效、可微的损失函数，显著提升了3D点云重建任务中的几何保真度，同时保持了与Chamfer Distance相当的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training deep learning models for point cloud prediction tasks such as shapecompletion and generation depends critically on loss functions that measurediscrepancies between predicted and ground-truth point sets. Commonly usedfunctions such as Chamfer Distance (CD), HyperCD, and InfoCD rely onnearest-neighbor assignments, which often induce many-to-one correspondences,leading to point congestion in dense regions and poor coverage in sparseregions. These losses also involve non-differentiable operations due to indexselection, which may affect gradient-based optimization. Earth Mover Distance(EMD) enforces one-to-one correspondences and captures structural similaritymore effectively, but its cubic computational complexity limits its practicaluse. We propose the Adaptive Probabilistic Matching Loss (APML), a fullydifferentiable approximation of one-to-one matching that leverages Sinkhorniterations on a temperature-scaled similarity matrix derived from pairwisedistances. We analytically compute the temperature to guarantee a minimumassignment probability, eliminating manual tuning. APML achieves near-quadraticruntime, comparable to Chamfer-based losses, and avoids non-differentiableoperations. When integrated into state-of-the-art architectures (PoinTr, PCN,FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)that generates 3D human point clouds from WiFi CSI measurements, APM lossyields faster convergence, superior spatial distribution, especially inlow-density regions, and improved or on-par quantitative performance withoutadditional hyperparameter search. The code is available at:https://github.com/apm-loss/apml.</description>
      <author>example@mail.com (Sasan Sharifipour, Constantino Álvarez Casado, Mohammad Sabokrou, Miguel Bordallo López)</author>
      <guid isPermaLink="false">2509.08104v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
      <link>http://arxiv.org/abs/2509.07209v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ASME IDETC/CIE 2025 (DETC2025-168977). Dataset  availability: BlendedNet dataset is openly available at Harvard Dataverse  (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VJT9EP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlendedNet是一个包含999个混合翼身(BWB)几何形状的公开空气动力学数据集，每个几何形状在约9种飞行条件下进行模拟，生成8830个RANS案例。研究团队还提出了一个端到端代理框架用于点对点空气动力学预测，实验显示在不同BWB上的表面预测误差较低。&lt;h4&gt;背景&lt;/h4&gt;非常规空气动力学配置面临数据稀缺问题，传统方法难以有效处理。&lt;h4&gt;目的&lt;/h4&gt;创建一个大型公开数据集解决非常规配置的数据稀缺问题，并开发一个数据驱动的代理模型框架用于空气动力学设计。&lt;h4&gt;方法&lt;/h4&gt;通过采样几何设计参数和飞行条件生成数据集；使用排列不变的PointNet回归器从表面点云预测几何参数，然后在预测参数和飞行条件下调节特征线性调制(FiLM)网络，预测点对点系数Cp、Cfx和Cfz。&lt;h4&gt;主要发现&lt;/h4&gt;BlendedNet数据集包含丰富的表面量信息，可用于研究升力和阻力；提出的端到端代理框架在不同BWB上的表面预测表现出低误差。&lt;h4&gt;结论&lt;/h4&gt;BlendedNet解决了非常规空气动力学配置的数据稀缺问题，并促进了数据驱动代理模型在空气动力学设计研究中的应用。&lt;h4&gt;翻译&lt;/h4&gt;BlendedNet是一个公开可用的空气动力学数据集，包含999个混合翼身(BWB)几何形状。每个几何形状在约九种飞行条件下进行模拟，产生了8830个使用Spalart-Allmaras模型的收敛RANS案例，每个案例有900万至1400万个单元。该数据集通过采样几何设计参数和飞行条件生成，包括研究升力和阻力所需的详细点对点表面量。我们还介绍了一个用于点对点空气动力学预测的端到端代理框架。该流程首先使用排列不变的PointNet回归器从采样表面点云预测几何参数，然后在预测的参数和飞行条件下调节特征线性调制(FiLM)网络，以预测点对点系数Cp、Cfx和Cfz。实验表明，在不同BWB上的表面预测误差较低。BlendedNet解决了非常规配置的数据稀缺问题，并促进了空气动力学设计的数据驱动代理模型研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; BlendedNet is a publicly available aerodynamic dataset of 999 blended wingbody (BWB) geometries. Each geometry is simulated across about nine flightconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras modeland 9 to 14 million cells per case. The dataset is generated by samplinggeometric design parameters and flight conditions, and includes detailedpointwise surface quantities needed to study lift and drag. We also introducean end-to-end surrogate framework for pointwise aerodynamic prediction. Thepipeline first uses a permutation-invariant PointNet regressor to predictgeometric parameters from sampled surface point clouds, then conditions aFeature-wise Linear Modulation (FiLM) network on the predicted parameters andflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.Experiments show low errors in surface predictions across diverse BWBs.BlendedNet addresses data scarcity for unconventional configurations andenables research on data-driven surrogate modeling for aerodynamic design.</description>
      <author>example@mail.com (Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.07209v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>3D and 4D World Modeling: A Survey</title>
      <link>http://arxiv.org/abs/2509.07996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at  https://github.com/worldbench/survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是首个专门针对3D和4D世界建模与生成的全面综述，建立了精确的定义和结构化分类法，总结了相关数据集和评估指标，并讨论了应用、挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;世界建模已成为AI研究的基石，但先前研究主要关注2D图像和视频数据的生成方法，忽视了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的工作。同时，'世界模型'缺乏标准化定义和分类法，导致文献中碎片化且有时不一致的主张。&lt;h4&gt;目的&lt;/h4&gt;提出首个专门针对3D和4D世界建模与生成的全面综述，建立精确的定义，引入结构化分类法，系统总结适用于3D/4D设置的数据集和评估指标，讨论实际应用，确定开放挑战，突出有希望的研究方向，为推进该领域提供连贯且基础的参考。&lt;h4&gt;方法&lt;/h4&gt;提出基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）的方法的分类法，并系统总结现有文献，提供可访问的文献汇编资源。&lt;h4&gt;主要发现&lt;/h4&gt;需要更多关注3D和4D表示方法；建立了世界模型的精确定义和分类框架；总结了适用于3D/4D设置的数据集和评估指标；识别了实际应用、开放挑战和有希望的研究方向。&lt;h4&gt;结论&lt;/h4&gt;该综述为3D和4D世界建模领域提供了连贯且基础的参考，有助于推进该领域的发展，相关文献系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;翻译&lt;/h4&gt;世界建模已成为AI研究的基石，使智能体能够理解、表示和预测他们所处的动态环境。虽然先前的工作主要强调2D图像和视频数据的生成方法，但他们忽视了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的快速增长的研究。同时，'世界模型'缺乏标准化定义和分类法，导致文献中碎片化且有时不一致的主张。本综述通过提出首个专门针对3D和4D世界建模与生成的全面综述来解决这些差距。我们建立了精确的定义，引入了涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，并系统总结了适用于3D/4D设置的数据集和评估指标。我们进一步讨论了实际应用，确定了开放挑战，并突出了有希望的研究方向，旨在为推进该领域提供连贯且基础的参考。现有文献的系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决世界建模领域缺乏标准化定义和分类体系的问题，以及现有工作过度关注2D数据而忽视原生3D/4D表示的局限性。这个问题很重要，因为世界建模是AI研究的基石，能帮助智能体理解、表示和预测动态环境；原生3D/4D表示（如RGB-D、占用网格、LiDAR点云）提供显式几何和物理基础，对自动驾驶、机器人等安全关键系统至关重要；缺乏统一框架导致文献碎片化，阻碍领域发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述文章，作者首先明确'世界模型'和'3D/4D世界建模'的精确定义，然后提出分层分类法，根据表示模态（VideoGen、OccGen和LiDARGen）对现有方法进行分类，并系统总结专门针对3D/4D场景的数据集和评估指标。作者借鉴了多种生成模型（VAEs、GANs、扩散模型、自回归模型）作为算法基础，整合了计算机视觉、机器人和人工智能领域的相关工作，但专注于3D/4D表示而非2D数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立精确的'世界模型'定义，提出结构化分类法涵盖VideoGen、OccGen和LiDARGen方法，系统总结3D/4D场景的数据集和评估指标。整体流程是：第2节介绍基本概念和定义；第3节详细介绍分层分类法；第4节总结数据集和评估指标；第5节回顾实际应用；第6节讨论挑战和未来方向。这种结构化组织方式帮助读者系统理解3D/4D世界建模的全貌。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对3D/4D世界建模进行全面综述；建立精确的'世界模型'定义；提出涵盖VideoGen、OccGen和LiDARGen的结构化分类法；提供3D/4D场景数据集和评估指标的全面覆盖。相比之前工作，这篇综述明确聚焦原生3D/4D表示而非2D数据，提供更系统化的分类和更全面的文献覆盖，填补了现有文献的知识空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过建立精确的定义、提出结构化分类法、系统总结数据集和评估指标，为3D和4D世界建模领域提供了首个全面且基础的参考框架，填补了现有文献的空白并指明了未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World modeling has become a cornerstone in AI research, enabling agents tounderstand, represent, and predict the dynamic environments they inhabit. Whileprior work largely emphasizes generative methods for 2D image and video data,they overlook the rapidly growing body of work that leverages native 3D and 4Drepresentations such as RGB-D imagery, occupancy grids, and LiDAR point cloudsfor large-scale scene modeling. At the same time, the absence of a standardizeddefinition and taxonomy for ``world models'' has led to fragmented andsometimes inconsistent claims in the literature. This survey addresses thesegaps by presenting the first comprehensive review explicitly dedicated to 3Dand 4D world modeling and generation. We establish precise definitions,introduce a structured taxonomy spanning video-based (VideoGen),occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, andsystematically summarize datasets and evaluation metrics tailored to 3D/4Dsettings. We further discuss practical applications, identify open challenges,and highlight promising research directions, aiming to provide a coherent andfoundational reference for advancing the field. A systematic summary ofexisting literature is available at https://github.com/worldbench/survey</description>
      <author>example@mail.com (Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu)</author>
      <guid isPermaLink="false">2509.07996v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference</title>
      <link>http://arxiv.org/abs/2509.08542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ASP-DAC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BitROM是一种基于CiROM的新型加速器，通过与BitNet的1.58位量化模型协同设计，解决了传统CiROM加速器在扩展到大型语言模型时面临的硅面积限制问题，实现了边缘设备上的高效LLM推理。&lt;h4&gt;背景&lt;/h4&gt;CiROM加速器通过消除运行时权重更新为CNN提供卓越能效，但在扩展到大型语言模型时受到参数量巨大的根本限制。即使是最小的LLaMA-7B模型在先进CMOS节点中也需要超过1,000 cm²的硅面积。&lt;h4&gt;目的&lt;/h4&gt;开发首个基于CiROM的加速器BitROM，通过协同设计克服LLM部署的面积限制，实现边缘设备上实用高效的LLM推理。&lt;h4&gt;方法&lt;/h4&gt;BitROM引入三种创新：1)双向ROM阵列，每个晶体管存储两个三进制权重；2)针对三进制权重计算优化的三模式局部累加器；3)集成解码刷新(DR) eDRAM支持片上KV缓存管理。此外还集成了LoRA适配器实现高效迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在65nm CMOS中评估，BitROM实现20.8 TOPS/W能效和4,967 kB/mm²比特密度，比先前的数字CiROM设计在面积效率上提高10倍。DR eDRAM减少43.6%的外部DRAM访问。&lt;h4&gt;结论&lt;/h4&gt;BitROM成功解决了CiROM加速器扩展到LLMs的硅面积限制问题，通过创新设计实现了边缘设备上的高效LLM推理，为边缘计算中的LLM部署提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;只读存储器计算(CiROM)加速器通过消除运行时权重更新为CNN提供了卓越的能效。然而，它们扩展到大型语言模型(LLMs)受到参数量巨大的根本限制。值得注意的是，即使是在先进的CMOS节点中，LLaMA系列中最小的模型LLaMA-7B也需要超过1,000 cm²的硅面积。本文提出了BitROM，这是首个基于CiROM的加速器，通过与BitNet的1.58位量化模型协同设计克服了这一限制，实现了边缘设备上实用高效的LLM推理。BitROM引入了三个关键创新：1)每个晶体管存储两个三进制权重的双向ROM阵列；2)针对三进制权重计算优化的三模式局部累加器；3)支持片上KV缓存管理的集成解码刷新(DR) eDRAM，显著减少了解码期间的外部内存访问。此外，BitROM集成了基于LoRA的适配器，实现各种下游任务的高效迁移学习。在65nm CMOS中评估，BitROM实现了20.8 TOPS/W和4,967 kB/mm²的比特密度，比先前的数字CiROM设计在面积效率上提高了10倍。此外，DR eDRAM减少了43.6%的外部DRAM访问，进一步增强了边缘应用中LLM的部署效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energyefficiency for CNNs by eliminating runtime weight updates. However, theirscalability to Large Language Models (LLMs) is fundamentally constrained bytheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMAseries - demands more than 1,000 cm2 of silicon area even in advanced CMOSnodes. This paper presents BitROM, the first CiROM-based accelerator thatovercomes this limitation through co-design with BitNet's 1.58-bit quantizationmodel, enabling practical and efficient LLM inference at the edge. BitROMintroduces three key innovations: 1) a novel Bidirectional ROM Array thatstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulatoroptimized for ternary-weight computations; and 3) an integrated Decode-Refresh(DR) eDRAM that supports on-die KV-cache management, significantly reducingexternal memory access during decoding. In addition, BitROM integratesLoRA-based adapters to enable efficient transfer learning across variousdownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bitdensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency overprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%reduction in external DRAM access, further enhancing deployment efficiency forLLMs in edge applications.</description>
      <author>example@mail.com (Wenlun Zhang, Xinyu Li, Shimpei Ando, Kentaro Yoshioka)</author>
      <guid isPermaLink="false">2509.08542v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Label Transfer Learning in Non-Stationary Data Streams</title>
      <link>http://arxiv.org/abs/2509.08181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE International Conference on Data Mining (ICDM) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对多标签数据流中的概念漂移问题，提出了两种新的迁移学习方法，通过标签间知识转移提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;在非平稳环境中，多标签数据流中的标签概念可能会发生漂移，这种漂移可能是独立发生的，也可能是与其他标签相关的。尽管在相关标签之间转移知识可以加速适应，但针对数据流的多标签迁移学习研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出两种新的迁移学习方法，解决多标签数据流中的概念漂移问题，通过标签间知识转移提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;BR-MARLENE：利用源数据流和目标数据流中不同标签的知识进行多标签分类；BRPW-MARLENE：在此基础上，通过显式建模和转移成对的标签依赖关系来增强学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;两种方法在非平稳环境中都优于最先进的多标签流方法，标签间知识转移对于提高预测性能是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的方法通过标签间知识转移有效解决了多标签数据流中的概念漂移问题，在非平稳环境中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;在非平稳环境中，多标签数据流中的标签概念往往会发生漂移，无论是独立发生还是与其他标签相关。在相关标签之间转移知识可以加速适应，然而针对数据流的多标签迁移学习研究仍然有限。为此，我们提出了两种新颖的迁移学习方法：BR-MARLENE利用源数据流和目标数据流中不同标签的知识进行多标签分类；BRPW-MARLENE在此基础上通过显式建模和转移成对的标签依赖关系来增强学习性能。全面的实验表明，两种方法在非平稳环境中都优于最先进的多标签流方法，证明了标签间知识转移对于提高预测性能的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label concepts in multi-label data streams often experience drift innon-stationary environments, either independently or in relation to otherlabels. Transferring knowledge between related labels can accelerateadaptation, yet research on multi-label transfer learning for data streamsremains limited. To address this, we propose two novel transfer learningmethods: BR-MARLENE leverages knowledge from different labels in both sourceand target streams for multi-label classification; BRPW-MARLENE builds on thisby explicitly modelling and transferring pairwise label dependencies to enhancelearning performance. Comprehensive experiments show that both methodsoutperform state-of-the-art multi-label stream approaches in non-stationaryenvironments, demonstrating the effectiveness of inter-label knowledge transferfor improved predictive performance.</description>
      <author>example@mail.com (Honghui Du, Leandro Minku, Aonghus Lawlor, Huiyu Zhou)</author>
      <guid isPermaLink="false">2509.08181v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments</title>
      <link>http://arxiv.org/abs/2509.08176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the 2020 IEEE International Conference on Data Mining  (ICDM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MARLINE的新方法，用于处理非平稳环境中的概念漂移问题，即使在源概念和目标概念不匹配的情况下也能从多个数据源中获益。&lt;h4&gt;背景&lt;/h4&gt;概念漂移是影响数据流挖掘系统预测性能的主要问题。现有方法假设至少有一个源模型与目标概念相似，这在许多现实场景中不成立。&lt;h4&gt;目的&lt;/h4&gt;开发一种即使源概念和目标概念不匹配也能从多个数据源知识中受益的方法。&lt;h4&gt;方法&lt;/h4&gt;提出MARLINE（Multi-source mApping with tRansfer LearnIng for Non-stationary Environments）方法，通过将目标概念投影到每个源概念空间，使多个源子分类器作为集成的一部分为目标概念预测做出贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在多个合成和真实世界数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。&lt;h4&gt;结论&lt;/h4&gt;MARLINE能够有效处理非平稳环境中的概念漂移问题，即使源概念和目标概念不匹配也能提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;概念漂移是在线学习中的一个主要问题，因为它影响数据流挖掘系统的预测性能。最近的研究开始探索来自不同数据源的数据流作为处理目标领域中概念漂移的策略。这些方法假设至少有一个源模型代表与目标概念相似的概念，这在许多现实场景中可能不成立。在本文中，我们提出了一种名为MARLINE（Multi-source mApping with tRansfer LearnIng for Non-stationary Environments）的新方法。MARLINE即使在源概念和目标概念不匹配的情况下，也能从非平稳环境中的多个数据源知识中受益。这是通过将目标概念投影到每个源概念的空间实现的，使多个源子分类器能够作为集成的一部分为目标概念的预测做出贡献。在几个合成和真实世界数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDM50108.2020.00021&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept drift is a major problem in online learning due to its impact on thepredictive performance of data stream mining systems. Recent studies havestarted exploring data streams from different sources as a strategy to tackleconcept drift in a given target domain. These approaches make the assumptionthat at least one of the source models represents a concept similar to thetarget concept, which may not hold in many real-world scenarios. In this paper,we propose a novel approach called Multi-source mApping with tRansfer LearnIngfor Non-stationary Environments (MARLINE). MARLINE can benefit from knowledgefrom multiple data sources in non-stationary environments even when source andtarget concepts do not match. This is achieved by projecting the target conceptto the space of each source concept, enabling multiple source sub-classifiersto contribute towards the prediction of the target concept as part of anensemble. Experiments on several synthetic and real-world datasets show thatMARLINE was more accurate than several state-of-the-art data stream learningapproaches.</description>
      <author>example@mail.com (Honghui Du, Leandro Minku, Huiyu Zhou)</author>
      <guid isPermaLink="false">2509.08176v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2509.08026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SI-EDTL的两阶段群体智能集成深度迁移学习模型，用于无人机图像中的多车辆检测。该模型结合了多种预训练特征提取器和分类器，并通过加权平均进行聚合，在AU-AIR无人机数据集上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;随着无人机技术的发展，无人机图像中的车辆检测变得越来越重要。然而，在无人机图像中准确检测多种类型的车辆仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效准确的车辆检测模型，能够在无人机图像中识别多种类型的车辆（汽车、面包车、卡车、公共汽车等）。&lt;h4&gt;方法&lt;/h4&gt;1. 采用两阶段群体智能集成深度迁移学习框架；2. 结合三个预训练的Faster R-CNN特征提取器模型（InceptionV3、ResNet50、GoogLeNet）；3. 集成五个迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯），形成15个基础学习器；4. 使用加权平均方法聚合这些学习器的结果；5. 应用鲸鱼优化算法对超参数进行优化，以平衡准确性、精确度和召回率；6. 在MATLAB R2020b中实现，并采用并行处理技术。&lt;h4&gt;主要发现&lt;/h4&gt;SI-EDTL模型在AU-AIR无人机数据集上表现优于现有的车辆检测方法，能够准确识别多种类型的车辆。&lt;h4&gt;结论&lt;/h4&gt;SI-EDTL模型通过结合多种深度学习技术和集成学习方法，有效地提高了无人机图像中多车辆检测的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SI-EDTL，一种用于检测无人机图像中多辆车辆的两阶段群体智能集成深度迁移学习模型。它将三个预训练的Faster R-CNN特征提取器模型（InceptionV3、ResNet50、GoogLeNet）与五个迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯）相结合，形成15个不同的基础学习器。这些学习器通过加权平均进行聚合，将区域分类为汽车、面包车、卡车、公共汽车或背景。使用鲸鱼优化算法对超参数进行优化，以平衡准确性、精确度和召回率。该模型在MATLAB R2020b中实现并使用并行处理，在AU-AIR无人机数据集上优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/cpe.6726&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deeptransfer learning model for detecting multiple vehicles in UAV images. Itcombines three pre-trained Faster R-CNN feature extractor models (InceptionV3,ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,Na\"ive Bayes), resulting in 15 different base learners. These are aggregatedvia weighted averaging to classify regions as Car, Van, Truck, Bus, orbackground. Hyperparameters are optimized with the whale optimization algorithmto balance accuracy, precision, and recall. Implemented in MATLAB R2020b withparallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAVdataset.</description>
      <author>example@mail.com (Zeinab Ghasemi Darehnaei, Mohammad Shokouhifar, Hossein Yazdanjouei, S. M. J. Rastegar Fatemi)</author>
      <guid isPermaLink="false">2509.08026v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>http://arxiv.org/abs/2509.07627v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为LSMTCR的新型框架，用于从头生成全长、表位特异性的T细胞受体（TCR）αβ链。该框架通过分离特异性和约束学习，实现了基于表位条件的配对全长TCR生成。&lt;h4&gt;背景&lt;/h4&gt;设计全长、表位特异性的TCR αβ链具有挑战性，原因包括巨大的序列空间、数据偏差以及免疫遗传约束的不完整建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的多架构框架，能够从头生成基于表位条件的配对全长TCR。&lt;h4&gt;方法&lt;/h4&gt;LSMTCR采用三个主要组件：1) 扩散增强的BERT编码器学习时间条件的表位表示；2) 条件GPT解码器在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性CDR3，并通过温度控制多样性；3) 基因感知的Transformer通过预测V/J使用来组装完整的αβ序列，确保免疫遗传保真度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上，LSMTCR实现了比基线更高的预测结合能力，更忠实地恢复了位置和长度语法，提供了优越且可通过温度调节的多样性。迁移学习显著改善了α链生成的预测结合、长度真实性和多样性。从已知或从头生成的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低。在配对αβ共建模中获得了更高的pTM/ipTM值。&lt;h4&gt;结论&lt;/h4&gt;LSMTCR仅从表位输入就能输出多样化、基因背景化的全长TCR设计，支持高通量筛选和迭代优化。&lt;h4&gt;翻译&lt;/h4&gt;设计全长、表位特异性的TCR αβ链仍然具有挑战性，这是由于巨大的序列空间、数据偏差以及免疫遗传约束的不完整建模。我们提出了LSMTCR，一种可扩展的多架构框架，该框架分离了特异性和约束学习，从而能够从头、基于表位条件地生成配对的全长TCR。扩散增强的BERT编码器学习时间条件的表位表示；条件GPT解码器在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性的CDR3，并通过温度控制的多样性；基因感知的Transformer通过预测V/J使用来组装完整的αβ序列，确保免疫遗传保真度。在GLIPH、TEP、MIRA、McPAS和我们整理的数据集上，LSMTCR在大多数数据集上实现了比基线更高的预测结合，更忠实地恢复了位置和长度语法，并提供了优越的、可通过温度调节的多样性。对于α链生成，迁移学习在预测结合、长度真实性和多样性方面优于代表性方法。从已知或从头生成的CDR3进行全长组装保留了k-mer谱，产生了与参考序列的低编辑距离，并且在与表位的配对αβ共建模中，比单链设置获得了更高的pTM/ipTM。LSMTCR仅从表位输入就能输出多样化、基因背景化的全长TCR设计， enabling高通量筛选和迭代优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing full-length, epitope-specific TCR $\alpha\beta$ remains challengingdue to vast sequence space, data biases and incomplete modeling ofimmunogenetic constraints. We present LSMTCR, a scalable multi-architectureframework that separates specificity from constraint learning to enable denovo, epitope-conditioned generation of paired, full-length TCRs. Adiffusion-enhanced BERT encoder learns time-conditioned epitoperepresentations; conditional GPT decoders, pretrained on CDR3$\beta$ andtransferred to CDR3$\alpha$, generate chain-specific CDR3s under cross-modalconditioning with temperature-controlled diversity; and a gene-awareTransformer assembles complete $\alpha\beta$ sequences by predicting V/J usageto ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and ourcurated dataset, LSMTCR achieves higher predicted binding than baselines onmost datasets, more faithfully recovers positional and length grammars, anddelivers superior, temperature-tunable diversity. For $\alpha$-chaingeneration, transfer learning improves predicted binding, length realism anddiversity over representative methods. Full-length assembly from known or denovo CDR3s preserves k-mer spectra, yields low edit distances to references,and, in paired $\alpha\beta$ co-modelling with epitope, attains higher pTM/ipTMthan single-chain settings. LSMTCR outputs diverse, gene-contextualized,full-length TCR designs from epitope input alone, enabling high-throughputscreening and iterative optimization.</description>
      <author>example@mail.com (Ruihao Zhang, Xiao Liu)</author>
      <guid isPermaLink="false">2509.07627v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</title>
      <link>http://arxiv.org/abs/2509.08018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为联邦迁移学习（FTL）的新方法，结合数字孪生技术和联邦学习应用于CT扫描分析。FTL利用预训练模型和节点间知识传输，解决了数据隐私、计算资源有限和数据异构性问题，在非独立同分布数据环境下表现优于传统联邦学习方法。&lt;h4&gt;背景&lt;/h4&gt;数字孪生（DT）技术和联邦学习（FL）在生物医学图像分析领域具有巨大潜力，特别是在CT扫描方面。然而，该领域面临数据隐私保护、计算资源有限以及数据异质性等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于数字孪生的CT扫描分析范式，即联邦迁移学习（FTL），以解决数据隐私、计算资源限制和数据异构性问题，同时实现云服务器与数字孪生CT扫描仪之间的实时协作。&lt;h4&gt;方法&lt;/h4&gt;联邦迁移学习（FTL）方法利用预训练模型和节点间的知识传输。研究团队将FTL应用于异构CT扫描数据集，并使用收敛时间、模型准确率、精确率、召回率、F1分数和混淆矩阵来评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;FTL方法在非独立同分布（non-IID）数据环境下表现优于传统联邦学习（FL）和聚类联邦学习（CFL）方法，具有更高的精确度、准确率、召回率和F1分数。&lt;h4&gt;结论&lt;/h4&gt;FTL能够改善基于数字孪生的CT扫描分析中的决策制定，为安全高效的医疗图像分析提供新可能，促进隐私保护，并为精准医疗和智能医疗系统的应用开辟新途径。&lt;h4&gt;翻译&lt;/h4&gt;数字孪生（DT）技术和联邦学习（FL）的应用对改变生物医学图像分析领域具有巨大潜力，特别是在计算机断层扫描（CT）扫描方面。本文提出了联邦迁移学习（FTL）作为一种新的基于数字孪生的CT扫描分析范式。FTL利用预训练模型和节点间的知识传输来解决数据隐私、有限计算资源和数据异构性问题。所提出的框架允许云服务器和数字孪生CT扫描仪之间的实时协作，同时保护患者身份。我们将FTL方法应用于异构CT扫描数据集，并使用收敛时间、模型准确率、精确率、召回率、F1分数和混淆矩阵评估模型性能。实验表明，与传统的联邦学习和聚类联邦学习（CFL）方法相比，FTL具有更好的精确度、准确率、召回率和F1分数。该技术在数据非独立同分布（non-IID）的环境中特别有益，为医疗诊断提供了可靠、高效和安全的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of Digital Twin (DT) technology and Federated Learning (FL)has great potential to change the field of biomedical image analysis,particularly for Computed Tomography (CT) scans. This paper presents FederatedTransfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.FTL uses pre-trained models and knowledge transfer between peer nodes to solveproblems such as data privacy, limited computing resources, and dataheterogeneity. The proposed framework allows real-time collaboration betweencloud servers and Digital Twin-enabled CT scanners while protecting patientidentity. We apply the FTL method to a heterogeneous CT scan dataset and assessmodel performance using convergence time, model accuracy, precision, recall, F1score, and confusion matrix. It has been shown to perform better thanconventional FL and Clustered Federated Learning (CFL) methods with betterprecision, accuracy, recall, and F1-score. The technique is beneficial insettings where the data is not independently and identically distributed(non-IID), and it offers reliable, efficient, and secure solutions for medicaldiagnosis. These findings highlight the possibility of using FTL to improvedecision-making in digital twin-based CT scan analysis, secure and efficientmedical image analysis, promote privacy, and open new possibilities forapplying precision medicine and smart healthcare systems.</description>
      <author>example@mail.com (Avais Jan, Qasim Zia, Murray Patterson)</author>
      <guid isPermaLink="false">2509.08018v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>AdsQA: Towards Advertisement Video Understanding</title>
      <link>http://arxiv.org/abs/2509.08621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出使用广告视频作为测试平台来评估大型语言模型对视觉内容之外的理解能力，创建了AdsQA基准数据集，提出了ReAd-R模型，并在测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在通用人工智能方面取得重大进展，领域特定问题促使这些模型不断学习更深层次的专业知识。然而，收集具有高质量和意外性的数据具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;扩展知识型大型语言模型的多样化专业应用，通过广告视频这一具有挑战性的测试平台，探测模型对视觉领域客观物理内容之外的理解能力。&lt;h4&gt;方法&lt;/h4&gt;使用广告视频作为测试平台，提出ReAd-R模型(一种Deepseek-R1风格的强化学习模型)，该模型通过反思问题并通过奖励驱动的优化生成答案。创建了AdsQA基准数据集，包含1,544个广告视频、10,962个片段，总计22.7小时，提供5个挑战性任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AdsQA基准上对14个顶级大型语言模型进行测试，ReAd-R模型以明显优势超越具有长链推理能力的强大竞争对手，达到最先进水平。&lt;h4&gt;结论&lt;/h4&gt;使用广告视频作为测试平台能够有效评估大型语言模型对视觉内容之外的理解能力，ReAd-R模型在这一任务上表现出色，为未来模型在专业应用方面的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在通用人工智能方面取得了重大进展。同时，越来越多的领域特定问题，如数学和编程，促使这些通用模型通过学习更深层次的专业知识不断进化。因此，现在是时候进一步扩展知识型大型语言模型的多样化专业应用，尽管收集具有意外性和信息量高的高质量数据具有挑战性。在本文中，我们提出使用广告视频作为具有挑战性的测试平台，来探测大型语言模型对视觉领域客观物理内容之外的理解能力。我们的动机是充分利用广告视频线索丰富和信息密集的特点，例如营销逻辑、说服策略和受众参与度。我们的贡献有三方面：(1)据我们所知，这是首次使用广告视频设计任务来评估大型语言模型。我们贡献了AdsQA，这是一个具有挑战性的广告视频问答基准，源自1,544个广告视频和10,962个片段，总计22.7小时，提供5个挑战性任务。(2)我们提出了ReAd-R，一种Deepseek-R1风格的强化学习模型，该模型反思问题并通过奖励驱动的优化生成答案。(3)我们在AdsQA上对14个顶级大型语言模型进行了基准测试，我们的ReAd-R以明显优势超越了配备长链推理能力的强大竞争对手，达到了最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have taken a great step towards AGI. Meanwhile,an increasing number of domain-specific problems such as math and programmingboost these general-purpose models to continuously evolve via learning deeperexpertise. Now is thus the time further to extend the diversity of specializedapplications for knowledgeable LLMs, though collecting high quality data withunexpected and informative tasks is challenging. In this paper, we propose touse advertisement (ad) videos as a challenging test-bed to probe the ability ofLLMs in perceiving beyond the objective physical content of common visualdomain. Our motivation is to take full advantage of the clue-rich andinformation-dense ad videos' traits, e.g., marketing logic, persuasivestrategies, and audience engagement. Our contribution is three-fold: (1) To ourknowledge, this is the first attempt to use ad videos with well-designed tasksto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmarkderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model thatreflects on questions, and generates answers via reward-driven optimization.(3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achievesthe state-of-the-art outperforming strong competitors equipped with long-chainreasoning capabilities by a clear margin.</description>
      <author>example@mail.com (Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, Bowen Zhou)</author>
      <guid isPermaLink="false">2509.08621v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization</title>
      <link>http://arxiv.org/abs/2509.08578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MAESTRO模型，一种多模态自适应集成方法，用于流感发病率的稳健预测，通过融合监测数据、网络搜索趋势和气象数据，结合先进的频谱-时间架构，在流感预测任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要，但传统方法在处理多源数据和时间序列复杂性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够融合多模态数据并利用频谱-时间建模技术的流感预测模型，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出MAESTRO模型，包含：1)多模态数据自适应融合；2)时间序列分解为季节性和趋势成分；3)混合特征增强管道(Transformer编码器、Mamba状态空间模型、多尺度时间卷积、频域分析模块)；4)跨通道注意力机制；5)时间投影头进行序列预测；6)可选预测不确定性量化器。&lt;h4&gt;主要发现&lt;/h4&gt;在11年香港流感数据上评估，MAESTRO实现了0.956的最先进R平方值，展示了优越的模型拟合和准确性，消融实验证实了多模态融合和频谱-时间组件的关键贡献。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO提供了一个模块化、可重现的统一框架，展示了先进的频谱-时间建模与多模态数据融合对稳健流行病学预测的重要协同作用，已公开可用以促进部署和扩展。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。为此，我们提出了MAESTRO，一种用于频谱-时间稳健优化的多模态自适应集成。MAESTRO通过自适应融合多模态输入（包括监测数据、网络搜索趋势和气象数据）并利用全面的频谱-时间架构来实现稳健性。该模型首先将时间序列分解为季节性和趋势成分。然后，这些成分通过混合特征增强管道进行处理，该管道结合了基于Transformer的编码器、用于长程依赖关系的Mamba状态空间模型、多尺度时间卷积和频域分析模块。跨通道注意力机制进一步整合了不同数据模态的信息。最后，时间投影头执行序列到序列的预测，并带有可选估计器来量化预测不确定性。在超过11年的香港流感数据（排除COVID-19期间）上评估，MAESTRO显示出强大的竞争性能，展示了优越的模型拟合和相对准确性，实现了最先进的0.956的R平方值。大量消融实验证实了多模态融合和频谱-时间成分的重要贡献。我们的模块化和可重现的管道已公开可用，以促进部署和扩展到其他地区和病原体。我们公开的管道提供了一个强大、统一的框架，展示了先进的频谱-时间建模和多模态数据融合对稳健流行病学预测的关键协同作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. To address this, we present MAESTRO, a Multi-modalAdaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achievesrobustness by adaptively fusing multi-modal inputs-including surveillance, websearch trends, and meteorological data-and leveraging a comprehensivespectro-temporal architecture. The model first decomposes time series intoseasonal and trend components. These are then processed through a hybridfeature enhancement pipeline combining Transformer-based encoders, a Mambastate-space model for long-range dependencies, multi-scale temporalconvolutions, and a frequency-domain analysis module. A cross-channel attentionmechanism further integrates information across the different data modalities.Finally, a temporal projection head performs sequence-to-sequence forecasting,with an optional estimator to quantify prediction uncertainty. Evaluated onover 11 years of Hong Kong influenza data (excluding the COVID-19 period),MAESTRO shows strong competitive performance, demonstrating a superior modelfit and relative accuracy, achieving a state-of-the-art R-square of 0.956.Extensive ablations confirm the significant contributions of both multi-modalfusion and the spectro-temporal components. Our modular and reproduciblepipeline is made publicly available to facilitate deployment and extension toother regions and pathogens.Our publicly available pipeline presents apowerful, unified framework, demonstrating the critical synergy of advancedspectro-temporal modeling and multi-modal data fusion for robustepidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu)</author>
      <guid isPermaLink="false">2509.08578v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
      <link>http://arxiv.org/abs/2509.08538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MESH是一个新基准，用于系统评估视频大型模型(LVMs)中的幻觉问题。它采用问答框架和自下而上的评估方法，与人类视频理解过程一致。研究发现LVMs在基本识别方面表现良好，但在处理复杂场景时容易产生幻觉。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型和视觉模块的语义能力基础上，通过整合时间信息来更好地理解动态视频内容。尽管取得了进展，但LVMs容易出现幻觉问题，产生不准确或无关的描述。&lt;h4&gt;目的&lt;/h4&gt;当前视频幻觉评估基准严重依赖视频内容的手动分类，忽略了人类自然解释视频时所依赖的感知过程。需要开发一个系统评估LVMs中幻觉的基准。&lt;h4&gt;方法&lt;/h4&gt;引入MESH基准，使用问答框架，包含二进制和多选格式，并包含目标和陷阱实例。采用自下而上的方法，评估基本对象、从粗到细的主体特征以及主体-动作对，这种方法与人类视频理解过程保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;MESH为识别视频中的幻觉提供了有效且全面的方法。评估显示，LVMs在识别基本对象和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，在较长视频中，LVMs的幻觉易感性显著增加。&lt;h4&gt;结论&lt;/h4&gt;MESH基准提供了一个系统评估LVMs幻觉的方法，表明LVMs在处理复杂视频内容时仍有改进空间。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型的语义能力和视觉模块的基础上，通过整合时间信息来更好地理解动态视频内容。尽管取得了进展，LVMs仍然容易出现幻觉，产生不准确或无关的描述。当前用于视频幻觉评估的基准严重依赖视频内容的手动分类，忽略了人类自然解释视频时所依赖的感知过程。我们引入了MESH，这是一个旨在系统评估LVMs中幻觉的基准。MESH使用问答框架，包含二进制和多选格式，并融入目标和陷阱实例。它采用自下而上的方法，评估基本对象、从粗到细的主体特征以及主体-动作对，与人类视频理解过程保持一致。我们证明MESH为识别视频中的幻觉提供了有效且全面的方法。我们的评估显示，虽然LVMs在识别基本对象和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，在较长视频中，它们对幻觉的易感性显著增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Video Models (LVMs) build on the semantic capabilities of LargeLanguage Models (LLMs) and vision modules by integrating temporal informationto better understand dynamic video content. Despite their progress, LVMs areprone to hallucinations-producing inaccurate or irrelevant descriptions.Current benchmarks for video hallucination depend heavily on manualcategorization of video content, neglecting the perception-based processesthrough which humans naturally interpret videos. We introduce MESH, a benchmarkdesigned to evaluate hallucinations in LVMs systematically. MESH uses aQuestion-Answering framework with binary and multi-choice formats incorporatingtarget and trap instances. It follows a bottom-up approach, evaluating basicobjects, coarse-to-fine subject features, and subject-action pairs, aligningwith human video understanding. We demonstrate that MESH offers an effectiveand comprehensive approach for identifying hallucinations in videos. Ourevaluations show that while LVMs excel at recognizing basic objects andfeatures, their susceptibility to hallucinations increases markedly whenhandling fine details or aligning multiple actions involving various subjectsin longer videos.</description>
      <author>example@mail.com (Garry Yang, Zizhe Chen, Man Hon Wong, Haoyu Lei, Yongqiang Chen, Zhenguo Li, Kaiwen Zhou, James Cheng)</author>
      <guid isPermaLink="false">2509.08538v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Mamba for Hyperspectral Object Tracking</title>
      <link>http://arxiv.org/abs/2509.08265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyMamba的新型高光谱目标跟踪网络，通过状态空间模块统一光谱、跨深度和时间建模，实现了高光谱目标跟踪的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;高光谱目标跟踪因高光谱图像丰富的光谱信息和细粒度材料区分能力而具有很大潜力，尤其在具有挑战性的场景中。现有方法通过将高光谱数据转换为伪彩色图像或结合模态融合策略取得进展，但往往无法捕捉内在的光谱信息、时间依赖性和跨深度交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有高光谱跟踪器的局限性，提出一种能够有效捕捉光谱信息、时间依赖性和跨深度交互的新型高光谱目标跟踪网络。&lt;h4&gt;方法&lt;/h4&gt;通过状态空间模块(SSMs)统一光谱、跨深度和时间建模；核心是光谱状态集成(SSI)模块，实现光谱特征的渐进式细化和传播；在每个SSI中嵌入高光谱Mamba(HSM)模块，通过三个方向扫描的SSM同步学习和空间信息；基于SSI和HSM构建联合特征，并通过与原始光谱特征交互来增强这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准数据集上的广泛实验表明，HyMamba实现了最先进的性能，在HOTC2020数据集上达到73.0%的AUC分数和96.3%的DP@20分数。&lt;h4&gt;结论&lt;/h4&gt;HyMamba是一种有效的高光谱目标跟踪方法，能够有效捕捉光谱信息、时间依赖性和跨深度交互，代码将在https://github.com/lgao001/HyMamba发布。&lt;h4&gt;翻译&lt;/h4&gt;高光谱目标跟踪由于高光谱图像中丰富的光谱信息和细粒度的材料区分能力而具有巨大潜力，在具有挑战性的场景中特别有益。虽然现有的高光谱跟踪器通过将高光谱数据转换为伪彩色图像或结合模态融合策略已经取得进展，但它们往往无法捕捉内在的光谱信息、时间依赖性和跨深度交互。为解决这些局限性，本文提出了一种配备Mamba的新型高光谱目标跟踪网络(HyMamba)。它通过状态空间模块统一了光谱、跨深度和时间建模。HyMamba的核心在于光谱状态集成(SSI)模块，该模块能够通过跨深度和时间光谱信息实现光谱特征的渐进式细化和传播。嵌入在每个SSI中的是高光谱Mamba(HSM)模块，它通过三个方向扫描的SSM同步学习空间和光谱信息。基于SSI和HSM，HyMamba从伪彩色和高光谱输入构建联合特征，并通过与从原始高光谱图像提取的光谱特征交互来增强它们。在七个基准数据集上进行的广泛实验表明，HyMamba实现了最先进的性能。例如，它在HOTC2020数据集上达到了73.0%的AUC分数和96.3%的DP@20分数。代码将在https://github.com/lgao001/HyMamba发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral object tracking holds great promise due to the rich spectralinformation and fine-grained material distinctions in hyperspectral images,which are beneficial in challenging scenarios. While existing hyperspectraltrackers have made progress by either transforming hyperspectral data intofalse-color images or incorporating modality fusion strategies, they often failto capture the intrinsic spectral information, temporal dependencies, andcross-depth interactions. To address these limitations, a new hyperspectralobject tracking network equipped with Mamba (HyMamba), is proposed. It unifiesspectral, cross-depth, and temporal modeling through state space modules(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)module, which enables progressive refinement and propagation of spectralfeatures with cross-depth and temporal spectral information. Embedded withineach SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatialand spectral information synchronously via three directional scanning SSMs.Based on SSI and HSM, HyMamba constructs joint features from false-color andhyperspectral inputs, and enhances them through interaction with originalspectral features extracted from raw hyperspectral images. Extensiveexperiments conducted on seven benchmark datasets demonstrate that HyMambaachieves state-of-the-art performance. For instance, it achieves 73.0\% of theAUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code willbe released at https://github.com/lgao001/HyMamba.</description>
      <author>example@mail.com (Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li)</author>
      <guid isPermaLink="false">2509.08265v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs</title>
      <link>http://arxiv.org/abs/2509.08016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/hyungjin-chung/VPS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VPS是一种创新的推理时方法，通过并行处理视频帧的不同子集并聚合结果，解决了VideoLLMs在处理长视频序列时面临的计算成本和性能下降问题，无需额外训练即可提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;Video Large Language Models (VideoLLMs)面临一个关键瓶颈：增加输入帧数以捕捉细粒度时间细节会导致不可接受的计算成本和长上下文长度引起的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Video Parallel Scaling (VPS)的推理时方法，旨在扩展模型的感知带宽而不增加其上下文窗口。&lt;h4&gt;方法&lt;/h4&gt;VPS通过运行多个并行推理流来实现，每个流处理视频帧的独特、不重叠子集。通过聚合这些互补流的输出概率，VPS集成了比单次处理更丰富的视觉信息集合。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，这种方法通过利用不相关的视觉证据，有效地收缩了Chinchilla缩放定律，从而在不增加额外训练的情况下提高了性能。在各种模型架构和规模(2B-32B)上，以及在Video-MME和EventHallusion等基准测试中的大量实验表明，VPS持续且显著地提高了性能。&lt;h4&gt;结论&lt;/h4&gt;VPS比其他并行替代方案(如Self-consistency)扩展性更好，并且与其他解码策略互补，为增强VideoLLMs的时间推理能力提供了一种内存高效且强大的框架。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(VideoLLMs)面临一个关键瓶颈：增加输入帧数以捕捉细粒度时间细节会导致不可接受的计算成本和长上下文长度引起的性能下降。我们引入了视频并行缩放(VPS)，一种推理时方法，可以在不增加模型上下文窗口的情况下扩展其感知带宽。VPS通过运行多个并行推理流来实现，每个流处理视频帧的独特、不重叠子集。通过聚合这些互补流的输出概率，VPS集成了比单次处理更丰富的视觉信息集合。我们从理论上证明，这种方法通过利用不相关的视觉证据，有效地收缩了Chinchilla缩放定律，从而在不增加额外训练的情况下提高了性能。在各种模型架构和规模(2B-32B)以及在Video-MME和EventHallusion等基准测试上的大量实验表明，VPS持续且显著地提高了性能。它的扩展性比其他并行替代方案(如Self-consistency)更好，并且与其他解码策略互补，为增强VideoLLMs的时间推理能力提供了一种内存高效且强大的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) face a critical bottleneck:increasing the number of input frames to capture fine-grained temporal detailleads to prohibitive computational costs and performance degradation from longcontext lengths. We introduce Video Parallel Scaling (VPS), an inference-timemethod that expands a model's perceptual bandwidth without increasing itscontext window. VPS operates by running multiple parallel inference streams,each processing a unique, disjoint subset of the video's frames. By aggregatingthe output probabilities from these complementary streams, VPS integrates aricher set of visual information than is possible with a single pass. Wetheoretically show that this approach effectively contracts the Chinchillascaling law by leveraging uncorrelated visual evidence, thereby improvingperformance without additional training. Extensive experiments across variousmodel architectures and scales (2B-32B) on benchmarks such as Video-MME andEventHallusion demonstrate that VPS consistently and significantly improvesperformance. It scales more favorably than other parallel alternatives (e.g.Self-consistency) and is complementary to other decoding strategies, offering amemory-efficient and robust framework for enhancing the temporal reasoningcapabilities of VideoLLMs.</description>
      <author>example@mail.com (Hyungjin Chung, Hyelin Nam, Jiyeon Kim, Hyojun Go, Byeongjun Park, Junho Kim, Joonseok Lee, Seongsu Ha, Byung-Hoon Kim)</author>
      <guid isPermaLink="false">2509.08016v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究结合神经算子与生成模型，解决了传统神经算子在处理湍流结构时过度平滑的问题，实现了在三个湍流挑战任务中的高性能：时空超分辨率、预测和稀疏流重构。&lt;h4&gt;背景&lt;/h4&gt;神经算子作为动力系统的替代品很有前景，但当使用标准L2损失训练时，往往会过度平滑小尺度的湍流结构。&lt;h4&gt;目的&lt;/h4&gt;结合算子学习和生成建模来克服神经算子过度平滑湍流结构的限制，解决三个传统神经算子失败的湍流挑战：时空超分辨率、预测和稀疏流重构。&lt;h4&gt;方法&lt;/h4&gt;使用对抗训练的神经算子(adv-NO)处理Schlieren射流超分辨率和3D均匀各向同性湍流预测；使用条件生成模型从稀疏粒子跟踪测速样输入重构圆柱尾流。&lt;h4&gt;主要发现&lt;/h4&gt;对于Schlieren射流超分辨率，adv-NO将能量谱误差降低15倍，同时保留尖锐梯度；对于3D均匀各向同性湍流，adv-NO仅用160个时间步训练能准确预测五个涡旋翻转时间，推理速度比基线方法快114倍；条件生成模型能从高度稀疏输入推断出相位对齐和统计正确的完整3D速度和压力场。&lt;h4&gt;结论&lt;/h4&gt;这些进展实现了低成本下的准确重构和预测，使实验和计算流体力学中的近实时分析和控制成为可能。&lt;h4&gt;翻译&lt;/h4&gt;神经算子作为动力系统的替代品很有前景，但当使用标准L2损失训练时，往往会过度平滑小尺度的湍流结构。在这里，我们表明将算子学习与生成建模相结合可以克服这一限制。我们考虑了三个传统神经算子失败的湍流流体的实际挑战：时空超分辨率、预测和稀疏流重构。对于Schlieren射流超分辨率，对抗训练的神经算子(adv-NO)将能量谱误差降低了15倍，同时保留了尖锐梯度，且以神经算子级的推理成本。对于3D均匀各向同性湍流，adv-NO仅使用单个轨迹的160个时间步进行训练，能够准确预测五个涡旋翻转时间，比基线基于扩散的预测器快114倍，实现近实时滚动。对于从高度稀疏的粒子跟踪测速样输入重构圆柱尾流，条件生成模型推断出相位对齐和统计正确的完整3D速度和压力场。这些进展实现了低成本下的准确重构和预测，使实验和计算流体力学中的近实时分析和控制成为可能。请访问我们的项目页面：https://vivekoommen.github.io/Gen4Turb/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决神经算子在处理湍流时的频谱偏差问题，即传统神经算子倾向于过度平滑精细尺度的湍流结构。这个问题在湍流研究中非常重要，因为湍流具有多尺度特性，精细结构对理解湍流动力学至关重要。高精度湍流模拟计算成本极高，而实验中通常只能获得稀疏测量数据，因此需要高效的方法来准确模拟、预测和重建湍流场，这对实验和计算流体力学中的实时分析和控制具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有神经算子的局限性，指出它们存在频谱偏差问题，即使能表示高频的架构也可能无法学习到这些内容。作者借鉴了生成模型的工作，如生成对抗网络、扩散模型和流匹配技术，这些方法通过学习数据的完整概率结构，自然减轻了确定性算子的低频偏差。基于这些观察，作者设计了对抗性训练的神经算子（adv-NO），结合了算子学习和生成建模的优点，以克服传统方法在处理湍流时的频谱偏差问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将生成建模与神经算子学习相结合，以克服传统神经算子在处理湍流时的频谱偏差问题。整体实现流程包括：1) 使用时间条件UNet作为基础架构，结合L1损失、感知损失和对抗性损失进行训练；2) 对于超分辨率任务，从低分辨率低帧率输入映射到高分辨率高帧率输出；3) 对于预测任务，使用历史窗口预测未来状态，然后自回归地预测更远的未来；4) 对于重建任务，使用条件生成模型从稀疏观测重建完整流场，确保相位对齐和统计匹配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 对抗性训练的神经算子（adv-NO），在保持高效计算的同时提高湍流精细结构的保真度；2) 条件生成模型用于零样本流场重建，从稀疏PTV样式的输入重建完整的3D速度和压力场；3) 在非常有限的数据下有效工作（预测仅用160个快照，重建仅用150个快照）。相比之前的工作，本文的不同之处在于：1) 提供了对神经算子频谱偏差的理论解释；2) 在计算效率与保真度之间取得了更好的平衡；3) 提出了统一的框架解决湍流中的三个不同挑战；4) 通过傅里叶分析显示adv-NO自然发展出高通滤波行为，减轻了频谱偏差。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将生成建模与神经算子学习相结合，开发出对抗性训练的神经算子，在保持高效计算的同时显著提高了湍流模拟、预测和重建中精细尺度结构的保真度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators are promising surrogates for dynamical systems but whentrained with standard L2 losses they tend to oversmooth fine-scale turbulentstructures. Here, we show that combining operator learning with generativemodeling overcomes this limitation. We consider three practical turbulent-flowchallenges where conventional neural operators fail: spatio-temporalsuper-resolution, forecasting, and sparse flow reconstruction. For Schlierenjet super-resolution, an adversarially trained neural operator (adv-NO) reducesthe energy-spectrum error by 15x while preserving sharp gradients at neuraloperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NOtrained on only 160 timesteps from a single trajectory forecasts accurately forfive eddy-turnover times and offers 114x wall-clock speed-up at inference thanthe baseline diffusion-based forecasters, enabling near-real-time rollouts. Forreconstructing cylinder wake flows from highly sparse Particle TrackingVelocimetry-like inputs, a conditional generative model infers full 3D velocityand pressure fields with correct phase alignment and statistics. These advancesenable accurate reconstruction and forecasting at low compute cost, bringingnear-real-time analysis and control within reach in experimental andcomputational fluid mechanics. See our project page:https://vivekoommen.github.io/Gen4Turb/</description>
      <author>example@mail.com (Vivek Oommen, Siavash Khodakarami, Aniruddha Bora, Zhicheng Wang, George Em Karniadakis)</author>
      <guid isPermaLink="false">2509.08752v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.08374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了InsFusion方法，用于解决多视角相机和激光雷达三维物体检测中的误差累积问题，通过从原始和融合特征中提取提案并查询原始特征，结合注意力机制，有效减轻了误差累积的影响，并在nuScenes数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;三维物体检测是自动驾驶和智能交通的关键组成部分，但在基本特征提取、透视变换和特征融合过程中，噪声和误差会逐渐累积。&lt;h4&gt;目的&lt;/h4&gt;解决三维物体检测过程中噪声和误差逐渐累积的问题，提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出InsFusion方法，从原始和融合特征中提取提案，利用这些提案查询原始特征减轻累积误差影响，并结合注意力机制应用于原始特征进一步减轻误差累积。&lt;h4&gt;主要发现&lt;/h4&gt;InsFusion与各种先进的基线方法兼容，在nuScenes数据集上实现了三维物体检测的新最先进性能。&lt;h4&gt;结论&lt;/h4&gt;InsFusion通过有效的特征提取和查询机制，成功减轻了三维物体检测中的误差累积问题，提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;多视角相机和激光雷达的三维物体检测是自动驾驶和智能交通的关键组成部分。然而，在基本特征提取、透视变换和特征融合过程中，噪声和误差会逐渐累积。为解决这一问题，我们提出了InsFusion，它可以从原始特征和融合特征中提取提案，并利用这些提案查询原始特征，从而减轻累积误差的影响。此外，通过将注意力机制应用于原始特征，进一步减轻了累积误差的影响。在nuScenes数据集上的实验表明，InsFusion与各种先进的基线方法兼容，并在三维物体检测方面取得了新的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D目标检测中多传感器（LiDAR和相机）融合过程中噪声和误差累积的问题。这个问题在自动驾驶和智能交通系统中至关重要，因为误差累积会导致检测精度下降，影响系统的可靠性和安全性。在特征提取、透视变换和融合的每个步骤都可能引入误差，如深度估计错误、坐标变换不准确等，这些误差会累积并最终影响整体性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有BEV（鸟瞰图）LiDAR-相机融合模型的局限性，发现误差在处理管道中逐渐累积。他们借鉴了BEVFormer系列的可变形注意力机制、IS-Fusion的实例引导融合方法以及FocalFormer3D的多阶段热图技术。但不同于这些方法专注于改进单个组件，作者设计了一种新范式，同时利用原始特征和融合特征，通过从两者中提取提案并查询原始特征来减轻误差累积，而不是试图优化管道中的特定环节。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过从原始特征和融合特征中提取提案，并利用这些提案查询原始特征，从而减轻多传感器融合过程中累积的误差。实现流程分为三步：1) 从相机特征、LiDAR特征和融合特征中提取查询；2) 通过模态特定的线性变换器将三组查询对齐到共享空间；3) 使用可变形变压器解码器，将三个核心特征源（原始相机、原始LiDAR和融合特征）作为键值对，通过注意力机制精炼实例特征。这一过程重复2次（实验确定的最佳层数），确保充分利用多源信息同时避免过度拟合局部噪声。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出通用的实例级融合范式InsFusion；2) 同时从原始特征和融合特征中提取提案；3) 使用多源查询设计而非单一特征源；4) 应用于原始特征的注意力机制；5) 与现有方法兼容且只需微调。相比之前工作，不同之处在于：传统方法主要关注改进管道中的单个组件（如减少深度估计误差），而InsFusion利用原始特征减轻噪声；传统方法依赖单一特征源，而InsFusion同时使用原始和融合特征；InsFusion具有更好的通用性，可集成到各种现有模型中，且计算开销小（FPS仅下降6.6%-9.3%）而性能提升显著（mAP提高1.0%-1.1%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InsFusion提出了一种创新的实例级LiDAR-相机融合方法，通过同时利用原始特征和融合特征中的提案来查询原始特征，有效解决了多传感器融合过程中的误差累积问题，显著提升了3D目标检测性能，同时保持了与现有方法的兼容性和较低的计算开销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional Object Detection from multi-view cameras and LiDAR is acrucial component for autonomous driving and smart transportation. However, inthe process of basic feature extraction, perspective transformation, andfeature fusion, noise and error will gradually accumulate. To address thisissue, we propose InsFusion, which can extract proposals from both raw andfused features and utilizes these proposals to query the raw features, therebymitigating the impact of accumulated errors. Additionally, by incorporatingattention mechanisms applied to the raw features, it thereby mitigates theimpact of accumulated errors. Experiments on the nuScenes dataset demonstratethat InsFusion is compatible with various advanced baseline methods anddelivers new state-of-the-art performance for 3D object detection.</description>
      <author>example@mail.com (Zhongyu Xia, Hansong Yang, Yongtao Wang)</author>
      <guid isPermaLink="false">2509.08374v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models</title>
      <link>http://arxiv.org/abs/2509.06609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICKG 2025. 8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对图异常检测(GAD)中的泛化能力进行了全面综述，追溯了GAD中泛化能力的演变，建立了系统性分类，并对现有方法进行了评述，最后指出了开放性挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;图异常检测近年来在社交媒体、电子商务等基于图的应用中受到广泛关注，但大多数方法假设训练和测试分布相同且针对特定任务定制，导致在真实场景中适应性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有GAD方法在数据分布变化和新应用中训练样本稀少等场景下的局限性，提高GAD模型的泛化能力，并提供对GAD中泛化能力的系统性理解。&lt;h4&gt;方法&lt;/h4&gt;通过迁移学习利用相关领域知识增强检测性能，或开发'一专多能'的GAD基础模型实现跨应用泛化；追溯GAD中泛化演变，形式化问题设置，建立系统性分类，并对现有广义GAD方法进行全面回顾。&lt;h4&gt;主要发现&lt;/h4&gt;目前对GAD中泛化的系统性理解仍然缺乏；现有方法主要通过迁移学习和基础模型来提高泛化能力，以应对真实世界场景中的挑战。&lt;h4&gt;结论&lt;/h4&gt;识别了当前GAD泛化领域的开放性挑战，提出了未来研究方向，以启发这一新兴领域的后续研究。&lt;h4&gt;翻译&lt;/h4&gt;图异常检测(GAD)近年来在识别社交媒体和电子商务等多种基于图的应用中的恶意样本方面引起了越来越多的关注。然而，大多数GAD方法假设训练和测试分布相同，并且针对特定任务定制，导致在真实场景中的适应性有限，如数据分布变化和新应用中训练样本稀少。为了解决这些局限性，最近的工作专注于通过利用相关领域的知识来增强检测性能的迁移学习来提高GAD模型的泛化能力，或者开发能够跨多个应用泛化的'一专多能'GAD基础模型。由于对GAD中泛化的系统性理解仍然缺乏，在本文中，我们提供了对GAD中泛化的全面回顾。我们首先追溯了GAD中泛化的演变，并形式化了问题设置，这进一步导致了我们的系统性分类。基于这种精细分类法，我们对现有的广义GAD方法进行了最新和全面的回顾。最后，我们确定了当前的开放性挑战，并提出了未来方向，以启发这一新兴领域的未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection (GAD) has attracted increasing attention in recentyears for identifying malicious samples in a wide range of graph-basedapplications, such as social media and e-commerce. However, most GAD methodsassume identical training and testing distributions and are tailored tospecific tasks, resulting in limited adaptability to real-world scenarios suchas shifting data distributions and scarce training samples in new applications.To address the limitations, recent work has focused on improving thegeneralization capability of GAD models through transfer learning thatleverages knowledge from related domains to enhance detection performance, ordeveloping "one-for-all" GAD foundation models that generalize across multipleapplications. Since a systematic understanding of generalization in GAD isstill lacking, in this paper, we provide a comprehensive review ofgeneralization in GAD. We first trace the evolution of generalization in GADand formalize the problem settings, which further leads to our systematictaxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensivereview is conducted for the existing generalized GAD methods. Finally, weidentify current open challenges and suggest future directions to inspirefuture research in this emerging field.</description>
      <author>example@mail.com (Junjun Pan, Yu Zheng, Yue Tan, Yixin Liu)</author>
      <guid isPermaLink="false">2509.06609v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
  <item>
      <title>Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study</title>
      <link>http://arxiv.org/abs/2509.05553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型是否真正理解概念或仅识别模式的问题，提出双向推理作为测试真正理解的标准，并开发了对比微调方法来解决认知专业化问题。&lt;h4&gt;背景&lt;/h4&gt;人工智能领域的基本问题：大型语言模型是否真正理解概念，还是仅仅识别模式。&lt;h4&gt;目的&lt;/h4&gt;提出双向推理作为测试真正理解能力的标准，真正的理解应该允许自然可逆性。&lt;h4&gt;方法&lt;/h4&gt;开发对比微调方法，使用三种类型的示例进行训练：保持语义意义的正面示例、具有不同语义的负面示例、前向方向混淆示例。&lt;h4&gt;主要发现&lt;/h4&gt;当前语言模型存在'认知专业化'现象：在前向任务上微调会提高任务表现但降低双向推理能力；对比微调方法成功实现了双向推理，同时保持前向任务能力。&lt;h4&gt;结论&lt;/h4&gt;双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法。&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了人工智能中的一个基本问题：大型语言模型是否真正理解概念或仅仅识别模式。作者提出双向推理（能够在没有明确反向训练的情况下双向应用转换的能力）作为真正理解的测试。他们认为真正的理解应该自然允许可逆性。例如，能够将变量名如userIndex更改为i的模型也应该能够推断出i代表用户索引，而无需反向训练。研究人员测试了当前的语言模型，并发现了他们所称的认知专业化：当模型在前向任务上微调时，这些任务的表现有所提高，但它们的双向推理能力显著下降。为解决此问题，他们开发了对比微调方法，使用三种类型的示例训练模型：保持语义意义的正面示例、具有不同语义的负面示例和前向方向混淆示例。这种方法旨在培养更深层次的理解，而非表面模式识别，并允许反向能力在没有明确反向训练的情况下自然发展。他们的实验证明对比微调成功实现了双向推理，能够在保持前向任务能力的同时实现强大的反向性能。作者得出结论，双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research addresses a fundamental question in AI: whether large languagemodels truly understand concepts or simply recognize patterns. The authorspropose bidirectional reasoning,the ability to apply transformations in bothdirections without being explicitly trained on the reverse direction, as a testfor genuine understanding. They argue that true comprehension should naturallyallow reversibility. For example, a model that can change a variable name likeuserIndex to i should also be able to infer that i represents a user indexwithout reverse training. The researchers tested current language models anddiscovered what they term cognitive specialization: when models are fine-tunedon forward tasks, their performance on those tasks improves, but their abilityto reason bidirectionally becomes significantly worse. To address this issue,they developed Contrastive Fine-Tuning (CFT), which trains models using threetypes of examples: positive examples that maintain semantic meaning, negativeexamples with different semantics, and forward-direction obfuscation examples.This approach aims to develop deeper understanding rather than surface-levelpattern recognition and allows reverse capabilities to develop naturallywithout explicit reverse training. Their experiments demonstrated that CFTsuccessfully achieved bidirectional reasoning, enabling strong reverseperformance while maintaining forward task capabilities. The authors concludethat bidirectional reasoning serves both as a theoretical framework forassessing genuine understanding and as a practical training approach fordeveloping more capable AI systems.</description>
      <author>example@mail.com (Serge Lionel Nikiema, Jordan Samhi, Micheline Bénédicte Moumoula, Albérick Euraste Djiré, Abdoul Kader Kaboré, Jacques Klein, Tegawendé F. Bissyandé)</author>
      <guid isPermaLink="false">2509.05553v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</title>
      <link>http://arxiv.org/abs/2509.07825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Point Linguist Model (PLM)，解决了大型语言模型与3D点云表示不一致的问题，通过引入Object-centric Discriminative Representation和Geometric Reactivation Decoder显著提升了3D目标分割性能。&lt;h4&gt;背景&lt;/h4&gt;3D目标分割与大型语言模型的结合已成为主流范式，具有广泛的语义、任务灵活性和强大的泛化能力，但存在LLMs处理高级语义标记而3D点云仅传达密集几何结构的表示不一致问题。&lt;h4&gt;目的&lt;/h4&gt;解决LLMs与3D点云之间的表示不一致问题，搭建两者之间的表示差距桥梁，无需3D-文本或3D-图像之间的大规模预对齐。&lt;h4&gt;方法&lt;/h4&gt;提出Point Linguist Model通用框架，引入Object-centric Discriminative Representation学习以目标为中心的标记，捕获目标语义和场景关系；引入Geometric Reactivation Decoder结合OcDR标记和密集特征预测掩码，保留全面的密集特征。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNetv2上实现+7.3 mIoU改进，在Multi3DRefer的3D引用分割上实现+6.0 mIoU改进，在跨越4个不同任务的7个基准测试中保持一致的性能提升。&lt;h4&gt;结论&lt;/h4&gt;全面目标级推理对鲁棒的3D理解是有效的，PLM框架成功解决了LLMs和3D点云之间的表示不一致问题。&lt;h4&gt;翻译&lt;/h4&gt;使用大型语言模型进行3D目标分割已成为一种主流范式，因为它具有广泛的语义、任务灵活性和强大的泛化能力。然而，这种范式受到表示不一致的阻碍：大型语言模型处理高级语义标记，而3D点云仅传达密集的几何结构。在先前的方法中，这种不一致限制了输入和输出。在输入阶段，密集点块需要大量预对齐，弱化了目标级语义并混淆了相似的干扰项。在输出阶段，预测仅依赖于密集特征而没有明确的几何线索，导致精细粒度准确性的损失。为解决这些局限性，我们提出了Point Linguist Model，一个通用框架，它搭建了大型语言模型和密集3D点云之间的表示差距，而不需要3D-文本或3D-图像之间的大规模预对齐。具体而言，我们引入了Object-centric Discriminative Representation，它在硬负感知训练目标下学习以目标为中心的标记，捕获目标语义和场景关系。这减轻了大型语言模型标记和3D点之间的不一致，增强了对干扰项的鲁棒性，并促进了大型语言模型内的语义级推理。为了准确分割，我们引入了Geometric Reactivation Decoder，它通过结合携带大型语言模型推断几何的OcDR标记和相应的密集特征来预测掩码，在整个流程中保留全面的密集特征。大量实验表明，PLM在ScanNetv2的3D引用分割上实现了+7.3 mIoU的显著改进，在Multi3DRefer上实现了+6.0 mIoU的改进，在跨越4个不同任务的7个基准测试中保持一致的性能提升，证明了全面目标级推理对鲁棒3D理解的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云分割中的表示不匹配问题：大型语言模型处理高级语义标记，而3D点云只包含密集几何结构。这种不匹配限制了输入和输出效果，削弱了对象级语义识别能力，并导致细粒度分割精度下降。这个问题在3D场景理解、机器人导航和人机交互等领域至关重要，因为解决它可以显著提升AI系统对复杂3D环境的理解和分割能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析现有方法的局限性，然后借鉴了2D多模态语言模型(如LISA)的成功经验，结合3D点云特性进行创新。他们使用了预训练的Mask3D生成对象提案，采用LLaMA2作为基础语言模型，并应用LoRA进行高效微调。设计思路是创建一个桥梁连接LLM和3D点云，不依赖大规模预对齐数据，通过对象中心化表示和干扰物感知训练解决表示不匹配问题，最终形成PLM框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个关键组件桥接LLM和3D点云：1)对象中心化表示(OcDR)使用对象标记作为LLM输入，捕获目标语义和场景关系；2)几何重新激活解码器(GRD)结合OcDR标记和密集特征预测掩码。流程包括：1)输入处理-生成OcDR表示；2)多模态推理-LLM处理视觉和语言输入；3)解码输出-GRD重新激活几何特征生成最终分割掩码；4)训练-使用干扰物感知机制增强对象区分能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)OcDR使用对象中心化标记替代传统点块，自然保持对象边界和语义；2)干扰物感知机制通过语义相似的硬负样本增强对象区分；3)GRD在整个流程中保留密集特征，提高细粒度精度。不同之处：传统方法依赖密集点块和大量预对齐，PLM使用对象中心化表示无需预对齐；传统方法输出仅依赖密集特征，PLM结合几何线索；传统方法任务受限，PLM支持多种3D分割任务；PLM训练效率更高，数据需求更少。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point Linguist Model通过对象中心化表示和几何重新激活解码器，有效桥接了大型语言模型与3D点云之间的表示差距，显著提升了多种3D分割任务的性能，实现了开放词汇和语言引导的高效3D对象分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object segmentation with Large Language Models (LLMs) has become aprevailing paradigm due to its broad semantics, task flexibility, and stronggeneralization. However, this paradigm is hindered by representationmisalignment: LLMs process high-level semantic tokens, whereas 3D point cloudsconvey only dense geometric structures. In prior methods, misalignment limitsboth input and output. At the input stage, dense point patches require heavypre-alignment, weakening object-level semantics and confusing similardistractors. At the output stage, predictions depend only on dense featureswithout explicit geometric cues, leading to a loss of fine-grained accuracy. Toaddress these limitations, we present the Point Linguist Model (PLM), a generalframework that bridges the representation gap between LLMs and dense 3D pointclouds without requiring large-scale pre-alignment between 3D-text or3D-images. Specifically, we introduce Object-centric DiscriminativeRepresentation (OcDR), which learns object-centric tokens that capture targetsemantics and scene relations under a hard negative-aware training objective.This mitigates the misalignment between LLM tokens and 3D points, enhancesresilience to distractors, and facilitates semantic-level reasoning withinLLMs. For accurate segmentation, we introduce the Geometric ReactivationDecoder (GRD), which predicts masks by combining OcDR tokens carryingLLM-inferred geometry with corresponding dense features, preservingcomprehensive dense features throughout the pipeline. Extensive experimentsshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gainsacross 7 benchmarks spanning 4 different tasks, demonstrating the effectivenessof comprehensive object-centric reasoning for robust 3D understanding.</description>
      <author>example@mail.com (Zhuoxu Huang, Mingqi Gao, Jungong Han)</author>
      <guid isPermaLink="false">2509.07825v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Physics-informed low-rank neural operators with application to parametric elliptic PDEs</title>
      <link>http://arxiv.org/abs/2509.07687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PILNO是一种用于在点云数据上高效逼近偏微分方程解算子的神经算子框架，结合低秩核逼近与编码器-解码器架构，能够快速、连续地进行一次性预测，同时保持对特定离散化的独立性。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程的求解在科学计算和工程应用中非常重要，但传统方法可能计算成本高且难以处理复杂几何形状或高维参数空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的神经算子框架，能够在点云数据上逼近偏微分方程的解算子，同时保持计算效率和对特定离散化的独立性。&lt;h4&gt;方法&lt;/h4&gt;提出PILNO框架，结合低秩核逼近与编码器-解码器架构，使用物理信息惩罚框架进行训练，确保PDE约束和边界条件得到满足。&lt;h4&gt;主要发现&lt;/h4&gt;PILNO在函数拟合、泊松方程、具有可变系数的屏蔽泊松方程和参数化达西流动等多种问题上表现出色；低秩结构在高维参数空间中提供了计算效率。&lt;h4&gt;结论&lt;/h4&gt;PILNO是一种可扩展且灵活的偏微分方程代理建模工具，能够高效处理各种偏微分方程问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出物理信息低秩神经算子，这是一种神经算子框架，用于在点云数据上高效逼近偏微分方程的解算子。PILNO将低秩核逼近与编码器-解码器架构相结合，能够快速、连续地进行一次性预测，同时保持对特定离散化的独立性。该模型使用物理信息惩罚框架进行训练，确保在监督和非监督设置下都满足PDE约束和边界条件。我们在各种问题上证明了其有效性，包括函数拟合、泊松方程、具有可变系数的屏蔽泊松方程和参数化达西流动。低秩结构在高维参数空间中提供了计算效率，使PILNO成为偏微分方程的可扩展且灵活的代理建模工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neuraloperator framework for efficiently approximating solution operators of partialdifferential equations (PDEs) on point cloud data. PILNO combines low-rankkernel approximations with an encoder--decoder architecture, enabling fast,continuous one-shot predictions while remaining independent of specificdiscretizations. The model is trained using a physics-informed penaltyframework, ensuring that PDE constraints and boundary conditions are satisfiedin both supervised and unsupervised settings. We demonstrate its effectivenesson diverse problems, including function fitting, the Poisson equation, thescreened Poisson equation with variable coefficients, and parameterized Darcyflow. The low-rank structure provides computational efficiency inhigh-dimensional parameter spaces, establishing PILNO as a scalable andflexible surrogate modeling tool for PDEs.</description>
      <author>example@mail.com (Sebastian Schaffer, Lukas Exl)</author>
      <guid isPermaLink="false">2509.07687v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.07507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MVAT框架，利用时间多视图信息解决3D目标检测中的弱监督标注问题，通过教师-学生蒸馏范式和多视图2D投影损失，实现了接近完全监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;3D数据标注是3D目标检测中的昂贵瓶颈，促使研究人员开发依赖更易获取的2D边界框标注的弱监督标注方法。&lt;h4&gt;目的&lt;/h4&gt;解决仅依赖2D边界框标注时产生的投影歧义问题，以及单一视角下部分物体可见性导致的3D边界框估计困难问题。&lt;h4&gt;方法&lt;/h4&gt;提出MVAT框架，通过跨时间聚合物体中心点云构建3D物体表示，采用教师-学生蒸馏范式让教师网络从单一视角学习并生成高质量伪标签，学生网络从单一视角预测这些伪标签，同时使用多视图2D投影损失确保3D预测与2D标注的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo Open数据集上的实验表明，MVAT在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与完全监督方法的差距。&lt;h4&gt;结论&lt;/h4&gt;MVAT框架无需任何3D边界框标注即可实现接近完全监督方法的性能，为3D目标检测提供了一种有效的弱监督解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为MVAT的新颖框架，该框架利用序列数据中存在的时间多视图信息来解决这些挑战。我们的方法跨时间聚合以物体为中心的点云，以构建尽可能密集和完整的3D物体表示。采用教师-学生蒸馏范式：教师网络从单一视角学习，但目标是从时间聚合的静态物体中推导出来的。然后，教师生成高质量的伪标签，学生从单一视角学习预测这些伪标签，用于静态和移动物体。整个框架集成了多视图2D投影损失，以强制预测的3D边界框与所有可用的2D标注之间的一致性。在nuScenes和Waymo Open数据集上的实验表明，MVAT在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与完全监督方法的差距，而无需任何3D边界框标注。我们的代码可在公共仓库中获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决弱监督3D目标检测中的投影歧义问题，即仅使用2D框标注来推断3D物体位置和大小时的不确定性。这个问题在现实中非常重要，因为3D数据标注成本高昂（平均114秒），而2D标注只需7-35秒，有3-16倍的效率提升。3D检测是自动驾驶和机器人的基础感知任务，降低标注成本能促进3D检测技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考到现有弱监督方法仅依赖单帧数据，忽略了连续数据采集中自然存在的多视角信息。他们设计了一个教师-学生知识蒸馏框架，教师网络利用多帧信息学习3D几何，学生网络学习从单帧预测3D信息。方法借鉴了SAM 2图像分割、DBSCAN点云聚类、PCA 3D框估计等现有技术，以及全监督领域的时间融合方法，但创新性地将这些技术组合应用于弱监督3D检测场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时间多视角信息解决投影歧义问题，通过聚合时序点云构建更完整的3D物体表示，采用教师-学生知识蒸馏框架。流程包括：1)使用SAM 2提取对象中心点云；2)聚合静态物体的多帧点云；3)使用PCA估计粗略3D框；4)教师网络用多帧信息训练；5)教师生成高质量伪标签；6)学生网络从单帧学习预测这些伪标签；7)使用多视角2D投影损失确保一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次有效利用时间多视角数据解决投影歧义；2)提出通过聚合点云生成高质量3D表示和伪标签的方法；3)采用多视角2D投影损失强制几何一致性；4)实现弱监督3D检测的最先进性能。不同之处：不依赖类别特定先验，而是直接解决歧义；不需要任何3D框标注或弱3D标注；首次将时间融合引入弱监督3D检测；教师用多帧训练，学生用单帧学习，实现有效知识迁移。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MVAT通过利用时间多视角一致性和教师-学生知识蒸馏框架，首次有效解决了弱监督3D目标检测中的投影歧义问题，显著缩小了与全监督方法的性能差距，同时仅需2D框标注。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotating 3D data remains a costly bottleneck for 3D object detection,motivating the development of weakly supervised annotation methods that rely onmore accessible 2D box annotations. However, relying solely on 2D boxesintroduces projection ambiguities since a single 2D box can correspond tomultiple valid 3D poses. Furthermore, partial object visibility under a singleviewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,a novel framework that leverages temporal multi-view present in sequential datato address these challenges. Our approach aggregates object-centric pointclouds across time to build 3D object representations as dense and complete aspossible. A Teacher-Student distillation paradigm is employed: The Teachernetwork learns from single viewpoints but targets are derived from temporallyaggregated static objects. Then the Teacher generates high qualitypseudo-labels that the Student learns to predict from a single viewpoint forboth static and moving objects. The whole framework incorporates a multi-view2D projection loss to enforce consistency between predicted 3D boxes and allavailable 2D annotations. Experiments on the nuScenes and Waymo Open datasetsdemonstrate that MVAT achieves state-of-the-art performance for weaklysupervised 3D object detection, significantly narrowing the gap with fullysupervised methods without requiring any 3D box annotations. % \footnote{Codeavailable upon acceptance} Our code is available in our public repository(\href{https://github.com/CEA-LIST/MVAT}{code}).</description>
      <author>example@mail.com (Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, Hervé Le Borgne, Quoc Cuong Pham)</author>
      <guid isPermaLink="false">2509.07507v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis</title>
      <link>http://arxiv.org/abs/2509.07463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepthVision是一个多模态场景理解框架，通过结合激光雷达点云生成的RGB图像与真实RGB数据，解决了视觉输入退化或不充分时机器人可靠操作的问题，特别是在低光条件下表现优异。&lt;h4&gt;背景&lt;/h4&gt;当视觉输入退化或不充分时，确保机器人可靠运行仍然是一个核心挑战。现有的视觉语言模型(VLMs)仅依赖基于相机的视觉输入和语言，在环境条件差(如黑暗或运动模糊)时性能受限。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，解决视觉输入退化情况下机器人操作可靠性问题，特别是在低光条件下的表现，同时保持与现有冻结VLMs的兼容性。&lt;h4&gt;方法&lt;/h4&gt;DepthVision使用条件生成对抗网络(GAN)结合集成精炼网络，从稀疏激光雷达点云合成RGB图像。然后，使用亮度感知模态适应(LAMA)将这些合成视图与真实RGB数据动态结合，基于环境光照条件混合两种类型的数据。这种方法无需对下游视觉语言模型进行任何微调即可补偿传感器退化。&lt;h4&gt;主要发现&lt;/h4&gt;在真实和模拟数据集上对各种模型和任务(特别是安全关键任务)的评估表明，该方法在低光条件下提高了性能，比仅使用RGB的基线方法获得显著提升，同时保持与冻结VLMs的兼容性。&lt;h4&gt;结论&lt;/h4&gt;激光雷达引导的RGB合成在实现真实环境中机器人操作的鲁棒性方面具有巨大潜力，为解决视觉输入退化问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;当视觉输入退化或不充分时，确保机器人可靠运行仍然是机器人技术中的一个核心挑战。本文介绍了DepthVision，这是一个为解决此问题而设计的多模态场景理解框架。与仅使用基于相机的视觉输入和语言的现有视觉语言模型(VLMs)不同，DepthVision使用带有集成精炼网络的条件生成对抗网络(GAN)，从稀疏激光雷达点云合成RGB图像。然后，使用亮度感知模态适应(LAMA)将这些合成视图与真实RGB数据结合，根据环境光照条件动态混合这两种类型的数据。这种方法补偿了传感器退化，如黑暗或运动模糊，而无需对下游视觉语言模型进行任何微调。我们在真实和模拟数据集上对各种模型和任务评估了DepthVision，特别关注安全关键任务。结果表明，我们的方法在低光条件下提高了性能，比仅使用RGB的基线方法获得显著提升，同时保持与冻结VLMs的兼容性。这项工作强调了激光雷达引导的RGB合成在实现真实环境中机器人操作鲁棒性方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人在视觉输入退化或不足时（如低光、运动模糊等条件）如何保持可靠运行的问题。这个问题在现实中非常重要，因为机器人任务依赖对环境的准确感知，而现有的视觉语言模型主要依赖相机图像，在低光等条件下表现不佳。虽然LiDAR等传感器能在这些条件下提供更好的空间感知，但LiDAR数据稀缺且难以大规模获取，限制了三维空间推理能力，这对自动驾驶等安全关键领域尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到视觉语言模型在低光条件下的局限性，以及LiDAR数据在提供空间感知方面的优势但存在数据稀缺问题。他们借鉴了生成对抗网络(GAN)特别是pix2pix框架来从LiDAR数据生成RGB图像，并采用U-Net架构保留空间细节。此外，作者还参考了refiner网络概念提高生成图像质量，并利用现有的视觉语言模型架构如Vision Transformer。关键创新在于设计了光照感知的模态适应机制，根据环境光照条件动态调整不同模态的权重，无需修改下游模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LiDAR点云数据生成RGB图像，以补充或替代相机在低光条件下的输入，并根据环境光照条件动态调整真实RGB图像和生成RGB图像的融合权重。整体实现流程包括：1) LiDAR预处理：将3D点云投影到2D图像平面并进行插值；2) GAN和Refiner设置：使用基于pix2pix的GAN从单通道LiDAR投影生成RGB图像，并通过refiner迭代提高质量；3) 光照感知模态适应(LAMA)：计算图像亮度并动态调整融合权重；4) VLM集成：将融合后的图像送入冻结的视觉语言模型进行推理，无需微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于GAN的LiDAR到RGB合成架构，集成refiner网络提高生成质量；2) 光照感知模态适应(LAMA)机制，根据环境光照动态调整不同模态权重；3) 无需对下游视觉语言模型进行微调，保持与现有模型的兼容性。相比之前工作的不同：不同于现有模型仅使用相机图像，DepthVision结合LiDAR数据；不同于传统传感器融合直接处理原始数据，DepthVision生成RGB样式的表示；不同于需要大量LiDAR数据训练的方法，DepthVision通过GAN从稀疏点云生成图像；不同于静态融合策略，DepthVision根据光照条件动态调整；不同于需要修改下游模型的方法，保持视觉语言模型冻结状态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DepthVision通过基于GAN的LiDAR到RGB合成和光照感知模态适应，显著提高了机器人在低光条件下的视觉语言理解能力，同时无需修改下游模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring reliable robot operation when visual input is degraded orinsufficient remains a central challenge in robotics. This letter introducesDepthVision, a framework for multimodal scene understanding designed to addressthis problem. Unlike existing Vision-Language Models (VLMs), which use onlycamera-based visual input alongside language, DepthVision synthesizes RGBimages from sparse LiDAR point clouds using a conditional generativeadversarial network (GAN) with an integrated refiner network. These syntheticviews are then combined with real RGB data using a Luminance-Aware ModalityAdaptation (LAMA), which blends the two types of data dynamically based onambient lighting conditions. This approach compensates for sensor degradation,such as darkness or motion blur, without requiring any fine-tuning ofdownstream vision-language models. We evaluate DepthVision on real andsimulated datasets across various models and tasks, with particular attentionto safety-critical tasks. The results demonstrate that our approach improvesperformance in low-light conditions, achieving substantial gains over RGB-onlybaselines while preserving compatibility with frozen VLMs. This work highlightsthe potential of LiDAR-guided RGB synthesis for achieving robust robotoperation in real-world environments.</description>
      <author>example@mail.com (Sven Kirchner, Nils Purschke, Ross Greer, Alois C. Knoll)</author>
      <guid isPermaLink="false">2509.07463v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark</title>
      <link>http://arxiv.org/abs/2509.07362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对密集城市环境中的视觉定位问题，提出了一种利用机载激光扫描(ALS)数据作为先验地图的新方法，并克服了现有ALS-based定位的三个主要限制。&lt;h4&gt;背景&lt;/h4&gt;在密集城市环境中进行准确的视觉定位是摄影测量、地理信息科学和机器人学中的基础任务。图像作为一种低成本且广泛可用的传感方式，其有效性常受限于无纹理表面、视角变化大和长期漂移问题。ALS数据的公开可用性为可扩展且精确的视觉定位开辟了新途径。&lt;h4&gt;目的&lt;/h4&gt;克服ALS-based定位面临的三个主要限制：缺乏平台多样化的数据集、缺乏适用于大规模城市环境的可靠真实值生成方法、以及现有I2P算法在空中-地面跨平台设置下的验证有限。&lt;h4&gt;方法&lt;/h4&gt;引入了一个新的数据集，该数据集集成了来自移动测绘系统的地面级图像与在中国武汉、香港和旧金山收集的ALS点云。&lt;h4&gt;主要发现&lt;/h4&gt;基于ALS的定位潜力尚未得到充分探索，主要受限于数据集多样性、真实值生成方法和算法验证的不足。&lt;h4&gt;结论&lt;/h4&gt;通过整合地面图像和ALS点云的新数据集为解决城市环境中的视觉定位问题提供了新的可能性，有助于推动ALS-based定位技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;在密集城市环境中进行准确的视觉定位是摄影测量、地理信息科学和机器人学中的基础任务。虽然图像是一种低成本且广泛可用的传感方式，但其在视觉里程计中的有效性常受限于无纹理表面、严重的视角变化和长期漂移。机载激光扫描(ALS)数据的日益公开可用性为可扩展且精确的视觉定位开辟了新途径，利用ALS作为先验地图。然而，由于三个关键限制，ALS-based定位的潜力尚未得到充分探索：(1)缺乏平台多样化的数据集，(2)缺乏适用于大规模城市环境的可靠真实值生成方法，(3)现有图像到点云(I2P)算法在空中-地面跨平台设置下的验证有限。为克服这些挑战，我们引入了一个新的大规模数据集，该数据集集成了来自移动测绘系统的地面级图像与在中国武汉、香港和旧金山收集的ALS点云。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空中-地面跨模态定位问题，即如何利用地面图像和机载激光扫描(ALS)点云数据进行准确定位。这个问题在现实中非常重要，因为在高密度城市环境中，卫星导航信号常被遮挡或失效，而纯视觉定位又受限于无纹理表面、视角变化和长期漂移等问题。结合ALS点云作为先验地图可以提供更精确、更稳定的定位解决方案，对自动驾驶、机器人导航和地理测绘等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有工作的三大局限性：缺乏平台多样化的数据集、缺乏适用于大规模城市环境的可靠地面真实值生成方法、以及现有图像到点云(I2P)算法在跨平台场景下的验证有限。基于这些认识，作者设计了结合地面图像和ALS点云的新数据集，并借鉴了现有的点云配准、姿态图优化和特征提取技术，但进行了改进以适应跨模态场景。在地面真实值生成方面，作者创新地采用间接方法，通过将移动激光扫描(MLS)数据与ALS点云对齐，然后将优化后的轨迹转移到图像流中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用ALS点云作为先验地图，结合地面图像实现跨模态定位，并通过间接方法生成高精度的地面真实值轨迹。整体流程分为三步：1)数据收集与预处理，收集三个城市的地面图像和ALS点云，并进行下采样和分割；2)地面真实值生成，通过地面分割和立面重建将MLS数据与ALS点云对齐，利用多传感器姿态图优化获得精确的图像姿态；3)定位算法评估，评估多种I2P算法在全局和精细定位任务上的性能，并采用SfM和ICP作为替代方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了首个整合地面图像和ALS点云的大规模跨平台数据集，覆盖三个不同特点的城市；2)提出创新的间接地面真实值生成方法，通过MLS-ALS对齐和姿态图优化，避免了跨模态直接配准的困难；3)建立了统一的基准测试套件，系统评估了现有方法在跨模态场景下的性能。相比之前的工作，这个数据集提供了更全面的跨模态场景，地面真实值生成方法不依赖重访区域或高端设备，且评估工作首次系统性地分析了I2P算法在空中-地面跨平台设置下的表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建大规模跨模态数据集、创新的地面真实值生成方法和统一的基准测试，为空中-地面视觉定位研究提供了重要资源和评估标准，推动了城市环境中高精度定位技术的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate visual localization in dense urban environments poses a fundamentaltask in photogrammetry, geospatial information science, and robotics. Whileimagery is a low-cost and widely accessible sensing modality, its effectivenesson visual odometry is often limited by textureless surfaces, severe viewpointchanges, and long-term drift. The growing public availability of airborne laserscanning (ALS) data opens new avenues for scalable and precise visuallocalization by leveraging ALS as a prior map. However, the potential ofALS-based localization remains underexplored due to three key limitations: (1)the lack of platform-diverse datasets, (2) the absence of reliable ground-truthgeneration methods applicable to large-scale urban environments, and (3)limited validation of existing Image-to-Point Cloud (I2P) algorithms underaerial-ground cross-platform settings. To overcome these challenges, weintroduce a new large-scale dataset that integrates ground-level imagery frommobile mapping systems with ALS point clouds collected in Wuhan, Hong Kong, andSan Francisco.</description>
      <author>example@mail.com (Yandi Yang, Jianping Li, Youqi Liao, Yuhao Li, Yizhe Zhang, Zhen Dong, Bisheng Yang, Naser El-Sheimy)</author>
      <guid isPermaLink="false">2509.07362v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Performance Characterization of a Point-Cloud-Based Path Planner in Off-Road Terrain</title>
      <link>http://arxiv.org/abs/2509.07321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been published in the Journal of Field Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对名为MUONS的点云导航系统进行了全面评估，用于非结构化道路的自主导航。研究通过模拟和实地测试进行了30,000次规划和导航试验，分析了七个路径规划参数的二十种组合在不同地形上的表现。&lt;h4&gt;背景&lt;/h4&gt;非结构化道路环境下的自主导航是一个具有挑战性的问题，需要高效可靠的导航系统来处理复杂地形。&lt;h4&gt;目的&lt;/h4&gt;评估MUONS导航系统在非结构化道路环境中的性能，并确定影响性能的关键参数。&lt;h4&gt;方法&lt;/h4&gt;研究者在模拟环境中测试了三个具有运动学挑战的地形图，并进行了实地验证。总共进行了30,000次规划和导航试验，通过统计和相关性分析评估不同参数组合对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. 配备MUONS的自动导引车在模拟中取得了0.98的成功率；2. 在实地测试中未出现故障；3. 初始规划阶段使用的双扩展半径与规划时间和路径长度性能最相关；4. 调整参数引起的变化比例与实地测试性能高度相关。&lt;h4&gt;结论&lt;/h4&gt;蒙特卡洛模拟活动可有效用于导航系统的性能评估和参数调整，模拟结果与实际表现具有良好的一致性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个基于点云的导航堆栈MUONS的全面评估，用于非结构化道路的自主导航。通过分析模拟中30,000次规划和导航试验的结果，并通过实地测试验证，来表征其性能。我们的模拟活动考虑了三个具有运动学挑战的地形图和七种路径规划参数的二十种组合。在模拟中，配备MUONS的自动导引车取得了0.98的成功率，并且在实地测试中没有出现故障。通过统计和相关性分析，我们确定在初始规划阶段使用的双扩展半径与规划时间和路径长度性能最相关。最后，我们观察到调整参数引起的变化比例与实地测试性能高度相关。这一发现支持使用蒙特卡洛模拟活动进行性能评估和参数调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/rob.70059&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a comprehensive evaluation of a point-cloud-based navigationstack, MUONS, for autonomous off-road navigation. Performance is characterizedby analyzing the results of 30,000 planning and navigation trials in simulationand validated through field testing. Our simulation campaign considers threekinematically challenging terrain maps and twenty combinations of sevenpath-planning parameters. In simulation, our MUONS-equipped AGV achieved a 0.98success rate and experienced no failures in the field. By statistical andcorrelation analysis we determined that the Bi-RRT expansion radius used in theinitial planning stages is most correlated with performance in terms ofplanning time and traversed path length. Finally, we observed that theproportional variation due to changes in the tuning parameters is remarkablywell correlated to performance in field testing. This finding supports the useof Monte-Carlo simulation campaigns for performance assessment and parametertuning.</description>
      <author>example@mail.com (Casey D. Majhor, Jeremy P. Bos)</author>
      <guid isPermaLink="false">2509.07321v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
      <link>http://arxiv.org/abs/2509.07209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ASME IDETC/CIE 2025 (DETC2025-168977). Dataset  availability: BlendedNet dataset is openly available at Harvard Dataverse  (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VJT9EP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlendedNet是一个包含999个混合翼身几何形状的公开气动数据集，在约9种飞行条件下模拟产生8830个RANS案例，并引入了一个端到端的代理框架用于点状气动预测。&lt;h4&gt;背景&lt;/h4&gt;非常规配置（如混合翼身BWB）的气动设计面临数据稀缺问题，限制了数据驱动代理模型的研究。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模的混合翼身气动数据集，并提供一个端到端的代理框架用于点状气动预测，以解决数据稀缺问题并促进气动设计研究。&lt;h4&gt;方法&lt;/h4&gt;1. 采样几何设计参数和飞行条件生成数据集；2. 使用Spalart-Allmaras模型进行RANS模拟；3. 开发端到端代理框架，包括使用PointNet回归器从表面点云预测几何参数，以及使用特征线性调制(FiLM)网络预测点状系数Cp、Cfx和Cfz。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在多样化的混合翼身上，表面预测具有较低的误差，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;BlendedNet数据集解决了非常规配置的数据稀缺问题，并使基于数据驱动的代理模型研究成为可能，有助于气动设计的发展。&lt;h4&gt;翻译&lt;/h4&gt;BlendedNet是一个公开的气动数据集，包含999个混合翼身(BWB)几何形状。每个几何形状在约9种飞行条件下进行模拟，产生了8830个使用Spalart-Allmaras模型的收敛RANS案例，每个案例有900万至1400万个单元。该数据集通过采样几何设计参数和飞行条件生成，包括研究升力和阻力所需的详细点状表面量。我们还引入了一个用于点状气动预测的端到端代理框架。该流程首先使用排列不变的PointNet回归器从采样的表面点云预测几何参数，然后在预测的参数和飞行条件下调节特征线性调制(FiLM)网络，以预测点状系数Cp、Cfx和Cfz。实验显示，在多样化的BWB上表面预测误差较低。BlendedNet解决了非常规配置的数据稀缺问题，并支持基于数据驱动的代理模型研究，用于气动设计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决混合翼身飞机(BWB)气动分析中的数据稀缺问题。这个问题很重要，因为BWB飞机具有更高的升阻比、更轻的结构重量和更低的燃油消耗(比传统飞机减少30%)，但其复杂几何形状导致流场分析困难，现有方法计算成本高、设计迭代慢，且缺乏详细的表面级气动数据，限制了数据驱动设计方法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到BWB设计中的数据稀缺和计算效率问题，然后借鉴了多项现有工作：使用Zhang等人的几何参数化方法定义BWB形状；采用PointNet架构处理点云数据并预测几何参数；受Catalani等人启发使用Feature-wise Linear Modulation (FiLM)网络；利用Latin Hypercube Sampling系统性地采样几何和飞行条件参数。作者将这些方法整合成一个两阶段深度学习框架，从点云直接预测气动特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个高质量的BWB气动数据集，并开发一个端到端的深度学习框架，从点云数据直接预测气动表面特性。整体流程分为三部分：1)数据生成：使用OpenVSP生成999种几何形状，Pointwise生成网格，FUN3D进行高精度RANS模拟，处理得到8830个成功案例；2)代理模型：第一阶段用PointNet从点云预测9个几何参数，第二阶段用FiLM网络结合几何参数和飞行条件预测表面气动系数(Cp, Cfx, Cfz)；3)训练评估：按几何分组分割数据，使用Adam优化器训练，在独立测试集上评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个公开的BWB高分辨率表面气动数据集，包含999种几何和8830种飞行条件；2)端到点云到气动预测的完整框架；3)结合PointNet和FiLM的两阶段模型；4)提供点级压力和摩擦系数而非仅整体系数。相比之前工作，BlendedNet提供了更丰富的几何变体和更详细的表面数据，预测方法从整体系数扩展到表面分布，实现了从点云输入到气动预测的端到端流程，并作为开源数据集发布，促进了领域研究。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个高质量的混合翼身飞机气动数据集和一个创新的端到端深度学习框架，能够从点云数据直接预测表面气动特性，解决了BWB设计中的数据稀缺和计算效率问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; BlendedNet is a publicly available aerodynamic dataset of 999 blended wingbody (BWB) geometries. Each geometry is simulated across about nine flightconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras modeland 9 to 14 million cells per case. The dataset is generated by samplinggeometric design parameters and flight conditions, and includes detailedpointwise surface quantities needed to study lift and drag. We also introducean end-to-end surrogate framework for pointwise aerodynamic prediction. Thepipeline first uses a permutation-invariant PointNet regressor to predictgeometric parameters from sampled surface point clouds, then conditions aFeature-wise Linear Modulation (FiLM) network on the predicted parameters andflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.Experiments show low errors in surface predictions across diverse BWBs.BlendedNet addresses data scarcity for unconventional configurations andenables research on data-driven surrogate modeling for aerodynamic design.</description>
      <author>example@mail.com (Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.07209v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Visual Representation Alignment for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.07979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://cvlab-kaist.github.io/VIRAL/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VIRAL方法，通过将多模态大语言模型的内部视觉表示与预训练视觉基础模型对齐，增强模型在视觉中心任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型通过视觉指令调优在各种任务上表现强大，但在以视觉为中心的任务（如物体计数或空间推理）中仍然有限。这种差距归因于当前仅文本的监督范式，它仅为视觉通路提供间接指导，导致模型在训练过程中丢弃细粒度的视觉细节。&lt;h4&gt;目的&lt;/h4&gt;提出VIRAL方法，通过显式对齐多模态大语言模型与预训练视觉基础模型的内部表示，使模型能够保留关键视觉细节并补充额外视觉知识，增强对复杂视觉输入的推理能力。&lt;h4&gt;方法&lt;/h4&gt;VIRAL是一种简单而有效的正则化策略，它将多模态大语言模型的内部视觉表示与预训练视觉基础模型(VFMs)的表示进行对齐。通过显式强制执行这种对齐，使模型能够保留输入视觉编码器中的关键视觉细节，同时补充VFMs的额外视觉知识。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，在广泛采用的多模态基准测试的所有任务中，VIRAL都带来了一致的性能提升。全面的消融研究验证了框架背后的关键设计选择。&lt;h4&gt;结论&lt;/h4&gt;这种简单的发现为在多模态大语言模型训练中有效整合视觉信息开辟了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通过视觉指令调优训练后已在各种任务上取得强大性能，但在以视觉为中心的任务（如物体计数或空间推理）中仍然存在局限。我们将这一差距归因于当前盛行的仅文本监督范式，该范式仅为视觉通路提供间接指导，并常导致MLLMs在训练过程中丢弃细粒度的视觉细节。在本文中，我们提出了VIRAL（Visual Representation Alignment，视觉表示对齐），这是一种简单而有效的正则化策略，它将MLLMs的内部视觉表示与预训练视觉基础模型(VFMs)的表示进行对齐。通过显式强制执行这种对齐，VIRAL使模型不仅能够保留来自输入视觉编码器的关键视觉细节，还能补充VFMs的额外视觉知识，从而增强其对复杂视觉输入的推理能力。我们的实验在广泛采用的多模态基准测试的所有任务中展示了一致的性能提升。此外，我们进行了全面的消融研究，以验证我们框架背后的关键设计选择。我们相信这一简单发现为在MLLMs训练中有效整合视觉信息开辟了重要方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) trained with visual instructiontuning have achieved strong performance across diverse tasks, yet they remainlimited in vision-centric tasks such as object counting or spatial reasoning.We attribute this gap to the prevailing text-only supervision paradigm, whichprovides only indirect guidance for the visual pathway and often leads MLLMs todiscard fine-grained visual details during training. In this paper, we presentVIsual Representation ALignment (VIRAL), a simple yet effective regularizationstrategy that aligns the internal visual representations of MLLMs with those ofpre-trained vision foundation models (VFMs). By explicitly enforcing thisalignment, VIRAL enables the model not only to retain critical visual detailsfrom the input vision encoder but also to complement additional visualknowledge from VFMs, thereby enhancing its ability to reason over complexvisual inputs. Our experiments demonstrate consistent improvements across alltasks on widely adopted multimodal benchmarks. Furthermore, we conductcomprehensive ablation studies to validate the key design choices underlyingour framework. We believe this simple finding opens up an important directionfor the effective integration of visual information in training MLLMs.</description>
      <author>example@mail.com (Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim)</author>
      <guid isPermaLink="false">2509.07979v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title>
      <link>http://arxiv.org/abs/2509.07946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为M3T FedFMs（多模态多任务联邦基础模型）的新范式，将联邦学习与多模态多任务基础模型相结合，用于教育领域，实现在保护隐私前提下的跨机构协作训练。&lt;h4&gt;背景&lt;/h4&gt;多模态多任务基础模型在人工智能领域展现出变革性潜力，特别是在教育应用方面。然而，这些模型在实际教育环境中的部署受到隐私法规、数据孤岛和领域特定数据有限性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;向教育界揭示M3T FedFMs作为一种有前景但尚未被充分探索的方法，探索其潜力，并揭示相关的未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;提出M3T FedFMs框架，整合联邦学习与多模态多任务基础模型，使分散机构能够进行协作且隐私保护的训练，同时适应多样化的模态和任务。&lt;h4&gt;主要发现&lt;/h4&gt;M3T FedFMs能够推进下一代智能教育系统的三个关键支柱：(1)隐私保护，通过将敏感数据保留在本地；(2)个性化，通过模块化架构定制模型；(3)公平性和包容性，促进资源有限实体的参与。&lt;h4&gt;结论&lt;/h4&gt;M3T FedFMs面临多个开放研究挑战，包括机构间异构隐私法规研究、数据模态特性不均匀性、遗忘方法、持续学习框架以及模型可解释性，这些挑战需集体解决才能实现实际部署。&lt;h4&gt;翻译&lt;/h4&gt;多模态多任务基础模型最近在人工智能领域显示出变革性潜力，在教育领域有新兴应用。然而，其在现实教育环境中的部署受到隐私法规、数据孤岛和领域特定数据可用性有限的阻碍。我们为教育领域引入了多模态多任务联邦基础模型：一种将联邦学习与多模态多任务基础模型相结合的范式，使分散机构能够进行协作、隐私保护的训练，同时适应多样的模态和任务。随后，这篇立场论文旨在向教育界揭示M3T FedFMs作为一种有前景但尚未被充分探索的方法，探索其潜力，并揭示相关的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal multi-task (M3T) foundation models (FMs) have recently showntransformative potential in artificial intelligence, with emerging applicationsin education. However, their deployment in real-world educational settings ishindered by privacy regulations, data silos, and limited domain-specific dataavailability. We introduce M3T Federated Foundation Models (FedFMs) foreducation: a paradigm that integrates federated learning (FL) with M3T FMs toenable collaborative, privacy-preserving training across decentralizedinstitutions while accommodating diverse modalities and tasks. Subsequently,this position paper aims to unveil M3T FedFMs as a promising yet underexploredapproach to the education community, explore its potentials, and reveal itsrelated future research directions. We outline how M3T FedFMs can advance threecritical pillars of next-generation intelligent education systems: (i) privacypreservation, by keeping sensitive multi-modal student and institutional datalocal; (ii) personalization, through modular architectures enabling tailoredmodels for students, instructors, and institutions; and (iii) equity andinclusivity, by facilitating participation from underrepresented andresource-constrained entities. We finally identify various open researchchallenges, including studying of (i) inter-institution heterogeneous privacyregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)the unlearning approaches for M3T FedFMs, (iv) the continual learningframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which mustbe collectively addressed for practical deployment.</description>
      <author>example@mail.com (Kasra Borazjani, Naji Khosravan, Rajeev Sahay, Bita Akram, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2509.07946v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Long-Document Retrieval in the PLM and LLM Era</title>
      <link>http://arxiv.org/abs/2509.07759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述提供了长文档检索(LDR)的首次全面处理，系统化了从经典词汇和早期神经模型到现代预训练和大型语言模型的演变，涵盖了关键范式如段落聚合、层次编码和高效注意力技术。&lt;h4&gt;背景&lt;/h4&gt;长文档的激增对信息检索(IR)提出了根本性挑战，因为长文档的长度、分散证据和复杂结构需要超越标准段落级技术的专门方法。&lt;h4&gt;目的&lt;/h4&gt;提供长文档检索的综合参考和前瞻性议程，整合三个主要时代的方法、挑战和应用，推动基础模型时代的长文档检索发展。&lt;h4&gt;方法&lt;/h4&gt;系统化长文档检索模型的演变过程，包括经典词汇模型、早期神经模型、现代预训练模型(PLM)和大型语言模型(LLMs)，涵盖段落聚合、层次编码、高效注意力和LLM驱动的重新排序和检索技术。&lt;h4&gt;主要发现&lt;/h4&gt;长文档检索需要专门方法；领域存在从早期模型到现代预训练和大型语言模型的演变；存在领域特定应用和专门的评估资源。&lt;h4&gt;结论&lt;/h4&gt;长文档检索是一个重要且快速发展的领域，但仍面临效率权衡、多模态对齐和忠实性等关键开放挑战。&lt;h4&gt;翻译&lt;/h4&gt;长文档的激增对信息检索(IR)提出了根本性挑战，因为它们的长度、分散的证据和复杂的结构需要超越标准段落级技术的专门方法。本综述首次全面处理了长文档检索(LDR)，整合了三个主要时代的方法、挑战和应用。我们将从经典词汇和早期神经模型到现代预训练(PLM)和大型语言模型(LLMs)的演变系统化，涵盖了段落聚合、层次编码、高效注意力和最新的LLM驱动的重新排序和检索技术等关键范式。除了模型，我们还回顾了领域特定应用、专门的评估资源，并概述了关键的开放挑战，如效率权衡、多模态对齐和忠实性。本综述旨在为推进基础模型时代的长文档检索提供综合参考和前瞻性议程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of long-form documents presents a fundamental challenge toinformation retrieval (IR), as their length, dispersed evidence, and complexstructures demand specialized methods beyond standard passage-level techniques.This survey provides the first comprehensive treatment of long-documentretrieval (LDR), consolidating methods, challenges, and applications acrossthree major eras. We systematize the evolution from classical lexical and earlyneural models to modern pre-trained (PLM) and large language models (LLMs),covering key paradigms like passage aggregation, hierarchical encoding,efficient attention, and the latest LLM-driven re-ranking and retrievaltechniques. Beyond the models, we review domain-specific applications,specialized evaluation resources, and outline critical open challenges such asefficiency trade-offs, multimodal alignment, and faithfulness. This survey aimsto provide both a consolidated reference and a forward-looking agenda foradvancing long-document retrieval in the era of foundation models.</description>
      <author>example@mail.com (Minghan Li, Miyang Luo, Tianrui Lv, Yishuai Zhang, Siqi Zhao, Ercong Nie, Guodong Zhou)</author>
      <guid isPermaLink="false">2509.07759v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</title>
      <link>http://arxiv.org/abs/2509.07596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究视觉语言基础模型中的性别偏见评估问题，发现现有基准中的虚假特征会显著影响偏见评估结果&lt;h4&gt;背景&lt;/h4&gt;视觉语言基础模型(VLMs)存在性别偏见问题，通常使用带有性别标注的真实图像基准进行评估，但这些基准中常存在性别特征与非性别特征之间的虚假相关性&lt;h4&gt;目的&lt;/h4&gt;探究非性别特征的虚假相关性是否会影响性别偏见的评估结果&lt;h4&gt;方法&lt;/h4&gt;在四个广泛使用的基准(COCO-gender, FACET, MIAP, 和 PHASE)和各种VLMs中系统地改变非性别特征，量化它们对偏见评估的影响&lt;h4&gt;主要发现&lt;/h4&gt;即使是最小的扰动（如仅遮蔽10%的物体或弱模糊背景）也会显著改变偏见分数，在生成式VLMs中使指标变化高达175%，在CLIP变体中变化43%&lt;h4&gt;结论&lt;/h4&gt;当前偏见评估往往反映的是模型对虚假特征的响应而非真正的性别偏见，建议同时报告偏见指标和特征敏感度测量以提高评估可靠性&lt;h4&gt;翻译&lt;/h4&gt;视觉语言基础模型中的性别偏见引发了对其安全部署的担忧，通常使用带有真实图像性别标注的基准进行评估。然而，由于这些基准中常存在性别特征与非性别特征（如物体和背景）之间的虚假相关性，我们识别出性别偏见评估中的一个关键疏忽：这些虚假特征是否会扭曲性别偏见评估？为解决这一问题，我们在四个广泛使用的基准和各种VLMs中系统地改变非性别特征，以量化它们对偏见评估的影响。我们的发现表明，即使是最小的扰动，如仅遮蔽10%的物体或弱模糊背景，也会显著改变偏见分数，在生成式VLMs中使指标变化高达175%，在CLIP变体中变化43%。这表明当前的偏见评估往往反映的是模型对虚假特征的响应，而非性别偏见，这降低了它们的可靠性。由于创建无虚假特征的基准具有根本性挑战，我们建议在报告偏见指标的同时报告特征敏感度测量，以实现更可靠的偏见评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gender bias in vision-language foundation models (VLMs) raises concerns abouttheir safe deployment and is typically evaluated using benchmarks with genderannotations on real-world images. However, as these benchmarks often containspurious correlations between gender and non-gender features, such as objectsand backgrounds, we identify a critical oversight in gender bias evaluation: Dospurious features distort gender bias evaluation? To address this question, wesystematically perturb non-gender features across four widely used benchmarks(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impacton bias evaluation. Our findings reveal that even minimal perturbations, suchas masking just 10% of objects or weakly blurring backgrounds, can dramaticallyalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% inCLIP variants. This suggests that current bias evaluations often reflect modelresponses to spurious features rather than gender bias, undermining theirreliability. Since creating spurious feature-free benchmarks is fundamentallychallenging, we recommend reporting bias metrics alongside feature-sensitivitymeasurements to enable a more reliable bias assessment.</description>
      <author>example@mail.com (Yusuke Hirota, Ryo Hachiuma, Boyi Li, Ximing Lu, Michael Ross Boone, Boris Ivanovic, Yejin Choi, Marco Pavone, Yu-Chiang Frank Wang, Noa Garcia, Yuta Nakashima, Chao-Han Huck Yang)</author>
      <guid isPermaLink="false">2509.07596v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Towards Postmortem Data Management Principles for Generative AI</title>
      <link>http://arxiv.org/abs/2509.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了AI系统使用用户数据的问题，特别关注已故个人数据的所有权，分析了当前数据管理现状，提出了三条死后数据管理原则，并给出了政策建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型、大型语言模型和智能AI系统严重依赖大量用户数据进行训练，这引发了关于所有权、版权和潜在危害的持续关注。然而，已故个人数据的所有权问题是一个相关但较少被研究的方面。&lt;h4&gt;目的&lt;/h4&gt;探索已故个人数据的所有权权利，分析当前死后数据管理和隐私权状况，提出死后数据管理原则，并为政策制定者和隐私从业者提供建议。&lt;h4&gt;方法&lt;/h4&gt;分析主要科技公司的隐私政策和欧盟AI法案等法规对死后数据管理和隐私权的定义，基于此分析提出死后数据管理原则。&lt;h4&gt;主要发现&lt;/h4&gt;当前在死后数据管理和隐私权方面存在空白，需要明确的原则来保护已故个人的数据权利。&lt;h4&gt;结论&lt;/h4&gt;提出了三条死后数据管理原则来指导保护已故个人数据权利，并建议政策制定者和隐私从业者与技术解决方案一起部署这些原则，使其在实践中可操作和可审计。&lt;h4&gt;翻译&lt;/h4&gt;基础模型、大型语言模型（LLMs）和智能AI系统严重依赖大量用户数据。使用此类数据进行训练引发了关于所有权、版权和潜在危害的持续关注。在这项工作中，我们探讨了一个相关但较少被研究的方面：已故个人数据的所有权权利。我们检查了主要科技公司的隐私政策和欧盟AI法案等法规所定义的死后数据管理和隐私权现状。基于此分析，我们提出了三条死后数据管理原则，以指导保护已故个人数据权利。最后，我们讨论了未来工作的方向，并为政策制定者和隐私从业者提供了关于如何与技术解决方案一起部署这些原则，使其在实践中可操作和可审计的建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, large language models (LLMs), and agentic AI systems relyheavily on vast corpora of user data. The use of such data for training hasraised persistent concerns around ownership, copyright, and potential harms. Inthis work, we explore a related but less examined dimension: the ownershiprights of data belonging to deceased individuals. We examine the currentlandscape of post-mortem data management and privacy rights as defined by theprivacy policies of major technology companies and regulations such as the EUAI Act. Based on this analysis, we propose three post-mortem data managementprinciples to guide the protection of deceased individuals data rights.Finally, we discuss directions for future work and offer recommendations forpolicymakers and privacy practitioners on deploying these principles alongsidetechnological solutions to operationalize and audit them in practice.</description>
      <author>example@mail.com (Ismat Jarin, Elina Van Kempen, Chloe Georgiou)</author>
      <guid isPermaLink="false">2509.07375v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases</title>
      <link>http://arxiv.org/abs/2509.07330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种通用人口统计预训练(GDP)模型，作为针对年龄和性别的基础表示框架，通过探索排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入，提高了模型的区分度、校准度和信息增益，增强了人口统计属性在预测模型中的重要性。&lt;h4&gt;背景&lt;/h4&gt;人口统计属性普遍存在于电子健康记录中，是临床风险分层和治疗决策的重要预测因素，但在模型设计中通常只起辅助作用，很少关注它们的表示学习。&lt;h4&gt;目的&lt;/h4&gt;开发一种基础表示框架，专门用于学习人口统计属性(尤其是年龄和性别)的有效表示，以提高医疗预测模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了通用人口统计预训练(GDP)模型，探索不同排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入，并使用来自不同地理区域的多样化疾病和人口组成的数据集进行预训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;顺序排序显著提高了模型的区分度、校准度和每个决策树分割的信息增益，特别是在年龄和性别对风险分层有显著贡献的疾病中；即使在人口统计属性预测价值相对较低的数据集中，GDP也能增强表示的重要性，增加它们在下游梯度提升模型中的影响。&lt;h4&gt;结论&lt;/h4&gt;表格化人口统计属性的基础模型可以跨任务和人群泛化，为提高医疗保健应用的预测性能提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;人口统计属性普遍存在于电子健康记录中，并在临床风险分层和治疗决策中作为重要的预测因素。尽管它们具有重要意义，但这些属性在模型设计中通常只被赋予辅助角色，很少给予关注来学习它们的表示。本研究提出了一个通用人口统计预训练(GDP)模型作为基础表示框架，专门针对年龄和性别。该模型使用来自不同地理区域的多样化疾病和人口组成的数据集进行预训练和评估。GDP架构探索了排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入。实验结果表明，顺序排序显著提高了模型在区分度、校准度以及每个决策树分割的相应信息增益方面的性能，特别是在年龄和性别对风险分层有显著贡献的疾病中。即使在人口统计属性具有相对较低预测价值的数据集中，GDP也能增强表示的重要性，增加它们在下游梯度提升模型中的影响。研究结果表明，表格化人口统计属性的基础模型可以跨任务和人群泛化，为提高医疗保健应用的预测性能提供了有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Demographic attributes are universally present in electronic health recordsand serve as vital predictors in clinical risk stratification and treatmentdecisions. Despite their significance, these attributes are often relegated toauxiliary roles in model design, with limited attention has been given tolearning their representations. This study proposes a General DemographicPre-trained (GDP) model as a foundational representation framework tailored toage and gender. The model is pre-trained and evaluated using datasets withdiverse diseases and population compositions from different geographic regions.The GDP architecture explores combinations of ordering strategies and encodingmethods to transform tabular demographic inputs into latent embeddings.Experimental results demonstrate that sequential ordering substantiallyimproves model performance in discrimination, calibration, and thecorresponding information gain at each decision tree split, particularly indiseases where age and gender contribute significantly to risk stratification.Even in datasets where demographic attributes hold relatively low predictivevalue, GDP enhances the representational importance, increasing their influencein downstream gradient boosting models. The findings suggest that foundationalmodels for tabular demographic attributes can generalize across tasks andpopulations, offering a promising direction for improving predictiveperformance in healthcare applications.</description>
      <author>example@mail.com (Li-Chin Chen, Ji-Tian Sheu, Yuh-Jue Chuang)</author>
      <guid isPermaLink="false">2509.07330v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2509.07143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TabGFM的图基础模型框架，通过将图数据转换为表格格式，利用表格基础模型进行节点分类，在28个真实世界数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;图基础模型(GFMs)虽然在各种图数据上展现出泛化能力，但通常在不能很好代表真实世界图的数据集上训练，限制了其泛化性能。相比之下，表格基础模型(TFMs)不仅在表格预测任务上表现出色，还在时间序列、自然语言处理和计算机视觉等领域显示出强大适用性。&lt;h4&gt;目的&lt;/h4&gt;受表格基础模型的启发，作者重新审视图基础模型的标准视角，将节点分类问题重新表述为表格问题，使TFMs能够通过上下文学习直接执行零样本节点分类。&lt;h4&gt;方法&lt;/h4&gt;TabGFM框架首先通过特征和结构编码器将图转换为表格，然后应用多个TFMs到多样本采样的表格上，最后通过集成选择聚合它们的输出。&lt;h4&gt;主要发现&lt;/h4&gt;在28个真实世界数据集上的实验表明，TabGFM在特定任务的图神经网络(GNNs)和最先进的图基础模型上取得了一致的改进。&lt;h4&gt;结论&lt;/h4&gt;表格重新表述方法为可扩展和可泛化的图学习提供了新的可能性，展示了将图数据转换为表格格式以利用表格基础模型优势的潜力。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型(GFMs)最近出现，作为一种很有前景的范式，可以在各种图数据上实现广泛的泛化。然而，现有的GFMs通常在不能很好地表示真实世界图的数据集上进行训练，这限制了它们的泛化性能。相比之下，表格基础模型(TFMs)不仅在经典的表格预测任务上表现出色，还在其他领域如时间序列预测、自然语言处理和计算机视觉中显示出强大的适用性。受此启发，我们对GFMs的标准观点采取了另一种视角，并将节点分类重新表述为表格问题。每个节点可以表示为行，特征、结构和标签信息作为列，使TFMs能够通过上下文学习直接执行零样本节点分类。在这项工作中，我们介绍了TabGFM，这是一种图基础模型框架，它首先通过特征和结构编码器将图转换为表格，应用多个TFMs到多样本采样的表格上，然后通过集成选择聚合它们的输出。在28个真实世界数据集上的实验中，TabGFM在特定任务的GNNs和最先进的GFMs上取得了一致的改进，突显了表格重新表述对于可扩展和可泛化的图学习的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models (GFMs) have recently emerged as a promising paradigmfor achieving broad generalization across various graph data. However, existingGFMs are often trained on datasets that were shown to poorly representreal-world graphs, limiting their generalization performance. In contrast,tabular foundation models (TFMs) not only excel at classical tabular predictiontasks but have also shown strong applicability in other domains such as timeseries forecasting, natural language processing, and computer vision. Motivatedby this, we take an alternative view to the standard perspective of GFMs andreformulate node classification as a tabular problem. Each node can berepresented as a row with feature, structure, and label information as columns,enabling TFMs to directly perform zero-shot node classification via in-contextlearning. In this work, we introduce TabGFM, a graph foundation model frameworkthat first converts a graph into a table via feature and structural encoders,applies multiple TFMs to diversely subsampled tables, and then aggregates theiroutputs through ensemble selection. Through experiments on 28 real-worlddatasets, TabGFM achieves consistent improvements over task-specific GNNs andstate-of-the-art GFMs, highlighting the potential of tabular reformulation forscalable and generalizable graph learning.</description>
      <author>example@mail.com (Adrian Hayler, Xingyue Huang, İsmail İlkan Ceylan, Michael Bronstein, Ben Finkelshtein)</author>
      <guid isPermaLink="false">2509.07143v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Interleaving Reasoning for Better Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2509.06945v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为交错推理生成(IRG)的新框架，通过在文本思考和图像合成之间交替进行来改进文本到图像(T2I)生成能力。研究还提出了交错推理生成学习(IRGL)方法和IRGL-300K数据集，通过两阶段训练实现了在多个评估指标上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;统一的多模态理解和生成模型在图像生成能力方面取得了显著进步，但在遵循指令和细节保留方面与紧密耦合理解与生成的系统(如GPT-4o)相比仍有较大差距。受最近交错推理进展的启发，研究者探索了这种推理是否能进一步改进文本到图像的生成。&lt;h4&gt;目的&lt;/h4&gt;探索交错推理是否能改进文本到图像(T2I)生成，以缩小统一多模态模型与紧密耦合理解与生成的系统之间的差距，特别是在遵循指令和细节保留方面。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了交错推理生成(IRG)框架，模型交替进行文本思考和图像合成；2. 提出了交错推理生成学习(IRGL)方法，包含两个子目标：强化初始思考和生成阶段，以及实现高质量的文本反思和后续图像中的忠实实现；3. 构建了IRGL-300K数据集，包含六种分解的学习模式；4. 采用两阶段训练：首先构建强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效调整IRG管道。&lt;h4&gt;主要发现&lt;/h4&gt;在多个评估指标(GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN)上取得了5-10个百分点的绝对提升，在视觉质量和细粒度保真度方面有显著改进，实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;交错推理框架能够有效提升文本到图像生成能力，特别是在遵循指令和细节保留方面，缩小了统一多模态模型与紧密耦合系统之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;统一多模态理解和生成模型最近在图像生成能力方面取得了显著改进，然而与紧密耦合理解与生成的系统(如GPT-4o)相比，在遵循指令和细节保留方面仍存在较大差距。受最近交错推理进展的启发，我们探索了这种推理是否能进一步改进文本到图像(T2I)生成。我们引入了交错推理生成(IRG)，这是一个在文本思考和图像合成之间交替的框架：模型首先产生基于文本的思考来引导初始图像，然后反思结果以细化细粒度细节、视觉质量和美学，同时保持语义。为了有效训练IRG，我们提出了交错推理生成学习(IRGL)，它针对两个子目标：(1)强化初始思考和生成阶段以建立核心内容和基础质量，(2)在后续图像中实现高质量的文本反思和对这些调整的忠实执行。我们整理了IRGL-300K，这是一个组织成六种分解学习模式的数据集，共同涵盖了基于文本的思考和完整的思考-图像轨迹学习。从一个原生输出交错文本-图像输出的统一基础模型开始，我们的两阶段训练首先构建强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效调整IRG管道。大量实验显示了最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN上获得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也有显著改进。代码、模型权重和数据集将在以下地址发布：https://github.com/Osilly/Interleaving-Reasoning-Generation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal understanding and generation models recently have achievesignificant improvement in image generation capability, yet a large gap remainsin instruction following and detail preservation compared to systems thattightly couple comprehension with generation such as GPT-4o. Motivated byrecent advances in interleaving reasoning, we explore whether such reasoningcan further improve Text-to-Image (T2I) generation. We introduce InterleavingReasoning Generation (IRG), a framework that alternates between text-basedthinking and image synthesis: the model first produces a text-based thinking toguide an initial image, then reflects on the result to refine fine-graineddetails, visual quality, and aesthetics while preserving semantics. To trainIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),which targets two sub-goals: (1) strengthening the initial think-and-generatestage to establish core content and base quality, and (2) enabling high-qualitytextual reflection and faithful implementation of those refinements in asubsequent image. We curate IRGL-300K, a dataset organized into six decomposedlearning modes that jointly cover learning text-based thinking, and fullthinking-image trajectories. Starting from a unified foundation model thatnatively emits interleaved text-image outputs, our two-stage training firstbuilds robust thinking and reflection, then efficiently tunes the IRG pipelinein the full thinking-image trajectory data. Extensive experiments show SoTAperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual qualityand fine-grained fidelity. The code, model weights and datasets will bereleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description>
      <author>example@mail.com (Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin)</author>
      <guid isPermaLink="false">2509.06945v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</title>
      <link>http://arxiv.org/abs/2509.07047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于奖励函数的优化方法，用于微调基础模型（以SAM为例），解决了基础模型参数不透明且需要大量手动优化的问题，提高了模型在实时流数据分割中的性能。&lt;h4&gt;背景&lt;/h4&gt;图像分割是显微分析中的关键任务，可通过自定义模型、迁移学习或基础模型实现。然而，基础模型通常包含大量不透明的调优参数，需要广泛的手动优化，限制了其在实时流数据分析中的应用。&lt;h4&gt;目的&lt;/h4&gt;引入基于奖励函数的优化方法来微调基础模型，以Meta的SAM框架为例，提高其在显微图像分割中的适应性和性能，特别是实现实时流数据分割。&lt;h4&gt;方法&lt;/h4&gt;构建代表成像系统物理特性的奖励函数，包括粒子尺寸分布、几何形状和其他标准，并将奖励驱动的优化框架整合到SAM模型中，创建优化变体SAM*。&lt;h4&gt;主要发现&lt;/h4&gt;通过奖励函数优化，开发了SAM*模型，该模型更好地满足多样化分割任务的需求，并特别支持实时流数据分割，在显微成像中表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于奖励函数的优化方法有效提升了基础模型在显微图像分割中的性能，精确分割对分析细胞结构、材料界面和纳米级特征至关重要。&lt;h4&gt;翻译&lt;/h4&gt;图像分割是显微学中的关键任务，对于准确分析和解释复杂的视觉数据至关重要。这项任务可以使用在特定领域数据集上训练的自定义模型、从预训练模型迁移学习，或提供广泛适用性的基础模型来完成。然而，基础模型通常呈现大量不透明的调优参数，需要广泛的手动优化，限制了它们对实时流数据分析的可用性。在此，我们引入了基于奖励函数的优化方法来微调基础模型，并通过Meta的SAM（Segment Anything Model）框架说明这种方法。奖励函数可以被构建来表示成像系统的物理特性，包括粒子尺寸分布、几何形状和其他标准。通过整合奖励驱动的优化框架，我们提高了SAM的适应性和性能，从而产生了一个优化变体SAM*，它更好地满足了多样化分割任务的需求，并特别允许实时流数据分割。我们在显微成像中证明了这种方法的有效性，其中精确分割对于分析细胞结构、材料界面和纳米级特征至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image segmentation is a critical task in microscopy, essential for accuratelyanalyzing and interpreting complex visual data. This task can be performedusing custom models trained on domain-specific datasets, transfer learning frompre-trained models, or foundational models that offer broad applicability.However, foundational models often present a considerable number ofnon-transparent tuning parameters that require extensive manual optimization,limiting their usability for real-time streaming data analysis. Here, weintroduce a reward function-based optimization to fine-tune foundational modelsand illustrate this approach for SAM (Segment Anything Model) framework byMeta. The reward functions can be constructed to represent the physics of theimaged system, including particle size distributions, geometries, and othercriteria. By integrating a reward-driven optimization framework, we enhanceSAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,that better aligns with the requirements of diverse segmentation tasks andparticularly allows for real-time streaming data segmentation. We demonstratethe effectiveness of this approach in microscopy imaging, where precisesegmentation is crucial for analyzing cellular structures, material interfaces,and nanoscale features.</description>
      <author>example@mail.com (Kamyar Barakati, Utkarsh Pratiush, Sheryl L. Sanchez, Aditya Raghavan, Delia J. Milliron, Mahshid Ahmadi, Philip D. Rack, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2509.07047v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
      <link>http://arxiv.org/abs/2509.06685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Withdrawn due to an error in the author list &amp; incomplete  experimental results&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架，通过结合视觉惯性运动恢复结构的精确稀疏深度和大型基础模型的密集粗糙深度，实现了高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统高斯溅射技术需要精确的深度信息来初始化高斯椭球体，但RGB-D/立体相机的深度传感范围有限，难以在大场景中应用。单目图像缺乏深度信息指导，导致新视角合成结果不佳。虽然大型基础模型可用于单目深度估计，但存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索的歧义等问题。&lt;h4&gt;目的&lt;/h4&gt;从单目RGB输入生成密集、精确的深度图像，用于高清晰度高斯溅射渲染。&lt;h4&gt;方法&lt;/h4&gt;利用视觉惯性运动恢复结构(SfM)提供的精确但稀疏的深度信息来优化大型基础模型(LFMs)提供的密集但粗糙的深度信息。提出对象分割深度传播算法渲染结构化对象像素的深度，并开发动态深度优化模块处理动态对象的损坏SfM深度，优化粗糙的LFM深度。&lt;h4&gt;主要发现&lt;/h4&gt;结合精确稀疏深度和密集粗糙深度的方法能够有效解决大场景中新视角合成的问题，提高渲染质量。&lt;h4&gt;结论&lt;/h4&gt;VIM-GS框架在大场景新视角合成中表现出优越的渲染质量，证明了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架。高斯溅射通常需要精确的深度来使用RGB-D/立体相机初始化高斯椭球体。它们有限的深度传感范围使得高斯溅射难以在大场景中工作。然而，单目图像缺乏深度来指导学习，导致新视角合成结果不佳。尽管有可用于单目深度估计的大型基础模型，但它们存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索的歧义等问题。本文旨在从单目RGB输入生成密集、精确的深度图像，用于高清晰度高斯溅射渲染。关键思路是利用视觉惯性运动恢复结构(SfM)的精确但稀疏的深度来优化大型基础模型(LFMs)的密集但粗糙的深度。为了连接稀疏输入和密集输出，我们提出了一种对象分割深度传播算法，该算法渲染结构化对象像素的深度。然后我们开发了一个动态深度优化模块来处理动态对象的损坏SfM深度，并优化粗糙的LFM深度。使用公共和定制数据集的实验证明了VIM-GS在大场景中的优越渲染质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; VIM-GS is a Gaussian Splatting (GS) framework using monocular images fornovel-view synthesis (NVS) in large scenes. GS typically requires accuratedepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limiteddepth sensing range makes it difficult for GS to work in large scenes.Monocular images, however, lack depth to guide the learning and lead toinferior NVS results. Although large foundation models (LFMs) for monoculardepth estimation are available, they suffer from cross-frame inconsistency,inaccuracy for distant scenes, and ambiguity in deceptive texture cues. Thispaper aims to generate dense, accurate depth images from monocular RGB inputsfor high-definite GS rendering. The key idea is to leverage the accurate butsparse depth from visual-inertial Structure-from-Motion (SfM) to refine thedense but coarse depth from LFMs. To bridge the sparse input and dense output,we propose an object-segmented depth propagation algorithm that renders thedepth of pixels of structured objects. Then we develop a dynamic depthrefinement module to handle the crippled SfM depth of dynamic objects andrefine the coarse LFM depth. Experiments using public and customized datasetsdemonstrate the superior rendering quality of VIM-GS in large scenes.</description>
      <author>example@mail.com (Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu)</author>
      <guid isPermaLink="false">2509.06685v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>HU-based Foreground Masking for 3D Medical Masked Image Modeling</title>
      <link>http://arxiv.org/abs/2509.07534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI AMAI Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于HU的前景掩码方法，改进了掩码图像建模在3D医学图像计算中的应用，通过关注内脏器官的强度分布而非随机掩码，显著提高了医学图像分割的性能。&lt;h4&gt;背景&lt;/h4&gt;虽然掩码图像建模(MIM)已经彻底改变了计算机视觉领域，但在3D医学图像计算中的应用一直受到随机掩码使用的限制，这种方法忽略了解剖物体的密度。&lt;h4&gt;目的&lt;/h4&gt;通过一个简单而有效的掩码策略来增强预训练任务，解决随机掩码在医学图像处理中的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用HU(亨氏单位)测量，实现基于HU的前景掩码，专注于内脏器官的强度分布，排除缺乏诊断意义特征的区域，如空气和流体等非组织区域。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共3D医学影像数据集上的广泛实验表明，提出的掩码策略在分割质量和Dice分数方面持续提高了性能(BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%)。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了以领域为中心的MIM的重要性，并为医学图像分割的表示学习指出了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;虽然掩码图像建模(MIM)已经彻底改变了计算机视觉领域，但在3D医学图像计算中的应用一直受到随机掩码使用的限制，这种方法忽略了解剖物体的密度。为了解决这一限制，我们通过一个简单而有效的掩码策略来增强预训练任务。利用HU(亨氏单位)测量，我们实现了基于HU的前景掩码，它专注于内脏器官的强度分布，排除了缺乏诊断意义特征的区域，如空气和流体等非组织区域。在五个公共3D医学影像数据集上的广泛实验表明，我们的掩码策略在分割质量和Dice分数方面持续提高了性能(BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%)。这些结果强调了以领域为中心的MIM的重要性，并为医学图像分割的表示学习指出了一个有前景的方向。实现可在github.com/AISeedHub/SubFore/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决将掩码图像建模方法有效应用于3D医学图像的问题。传统方法使用随机掩码策略，忽略了医学图像中解剖结构的密度特点。这个问题很重要，因为医学图像（如CT扫描）包含大量缺乏诊断意义的背景区域（如空气和流体），随机掩码会导致模型在预训练时关注不相关信息，而不是包含诊断价值的解剖结构，影响最终分割效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析医学图像与自然图像的差异，发现CT扫描的HU值分布特点：低HU值（&lt;0）通常对应背景区域，而高HU值对应有诊断意义的解剖结构。他们比较了前景和背景区域的信息量，发现前景区域包含更多诊断相关信息。作者借鉴了现有的掩码图像建模框架（如MAE和SimMIM）和3D体积处理方法（如MAE3D），但没有直接照搬，而是根据医学图像特点进行了创新性改进，设计了基于HU值的前景掩码策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用医学图像中HU值的分布特点，只掩码包含诊断信息的区域（HU值在[0.1,1]范围内），排除低HU值背景区域（如空气和流体），使模型在预训练时专注于有意义的解剖结构。整体实现流程包括：1)子体积划分：将原始3D医学图像体积划分为多个16×16×16的小子体积；2)前景掩码：计算每个子体积的平均HU值，只掩码平均HU值≥0.1的子体积；3)预训练任务：使用掩码后的体积作为输入，训练模型重建原始体积；4)下游任务：将预训练好的模型应用于医学图像分割任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将HU值信息整合到掩码图像建模框架中；2)提出简单而有效的前景掩码方法，专注于解剖结构而非随机区域；3)通过大量实验验证了方法在多个医学图像数据集上的有效性。相比之前的工作，不同之处在于：传统MIM方法使用随机掩码，而本文基于HU值进行语义感知的掩码；现有医学图像MIM方法通常直接从自然图像领域迁移，没有考虑医学图像的特殊性；本文方法通过分析医学图像的强度分布，有选择地掩码信息丰富的区域，在多个数据集上表现优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于HU值的前景掩码策略，通过专注于医学图像中有诊断意义的解剖结构而非随机掩码，显著提高了3D医学图像分割任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Masked Image Modeling (MIM) has revolutionized fields of computervision, its adoption in 3D medical image computing has been limited by the useof random masking, which overlooks the density of anatomical objects. Toaddress this limitation, we enhance the pretext task with a simple yeteffective masking strategy. Leveraging Hounsfield Unit (HU) measurements, weimplement an HU-based Foreground Masking, which focuses on the intensitydistribution of visceral organs and excludes non-tissue regions, such as airand fluid, that lack diagnostically meaningful features. Extensive experimentson five public 3D medical imaging datasets demonstrate that our maskingconsistently improves performance, both in quality of segmentation and Dicescore (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,BraTS:~78.55\%). These results underscore the importance of domain-centric MIMand suggest a promising direction for representation learning in medical imagesegmentation. Implementation is available at github.com/AISeedHub/SubFore/.</description>
      <author>example@mail.com (Jin Lee, Vu Dang, Gwang-Hyun Yu, Anh Le, Zahid Rahman, Jin-Ho Jang, Heonzoo Lee, Kun-Yung Kim, Jin-Sul Kim, Jin-Young Kim)</author>
      <guid isPermaLink="false">2509.07534v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents</title>
      <link>http://arxiv.org/abs/2509.07531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;科学文档表示学习当前面临三个主要挑战：基于引用结构的对比训练方法未能充分利用引用信息；细粒度表示学习方法需要昂贵的集成且缺乏领域泛化；任务感知学习方法依赖于手动预定义的任务分类且需要额外训练数据。为解决这些问题，作者提出了FLeW方法，统一了三种方法，通过引入三元组采样方法增强引用结构信号，利用引用意图进行细粒度表示学习，并通过简单权重搜索自适应集成嵌入。实验表明FLeW在多个科学任务和领域中具有适用性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前科学文档表示学习方法面临三个主要挑战：1)基于引用结构的对比训练方法未能充分利用引用信息，仍然生成单一向量表示；2)细粒度表示学习方法在句子或方面级别生成多个向量，需要昂贵的集成且缺乏领域泛化能力；3)任务感知学习方法依赖于手动预定义的任务分类，忽略了细微的任务区别，并为任务特定模块需要额外的训练数据。&lt;h4&gt;目的&lt;/h4&gt;解决科学文档表示学习中的三个挑战，提出一种统一三种方法的新方法FLeW，以获得更好的文档表示。&lt;h4&gt;方法&lt;/h4&gt;FLeW方法引入了一种新的三元组采样方法，利用引用意图和频率增强引用结构信号进行训练；利用引用意图(背景、方法、结果)与科学写作结构相一致的特点，促进细粒度表示学习的领域通用方面划分；采用简单的权重搜索来自适应地将三个方面级别的嵌入集成到任务特定的文档嵌入中，无需任务感知的微调。&lt;h4&gt;主要发现&lt;/h4&gt;FLeW方法在多个科学任务和领域中表现出适用性和鲁棒性，优于先前的模型。&lt;h4&gt;结论&lt;/h4&gt;FLeW方法通过统一三种表示学习方法的优点，解决了现有方法的局限性，为科学文档表示学习提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;科学文档表示学习为各种任务提供了强大的嵌入，而当前方法在三种方法中面临挑战。1)基于引用结构的对比训练未能充分利用引用信息，仍然生成单一向量表示。2)细粒度表示学习在句子或方面级别生成多个向量，需要昂贵的集成且缺乏领域泛化能力。3)任务感知学习依赖于手动预定义的任务分类，忽略了细微的任务区别，并为任务特定模块需要额外的训练数据。为解决这些问题，我们提出了一种统一三种方法以获得更好表示的新方法，即FLeW。具体来说，我们引入了一种新的三元组采样方法，利用引用意图和频率来增强用于训练的引用结构信号。引用意图(背景、方法、结果)与科学写作的一般结构相一致，促进细粒度表示学习的领域通用方面划分。然后，我们采用简单的权重搜索来自适应地将三个方面级别的嵌入集成到任务特定的文档嵌入中，无需任务感知的微调。实验表明，与先前的模型相比，FLeW在多个科学任务和领域中具有适用性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific document representation learning provides powerful embeddings forvarious tasks, while current methods face challenges across three approaches.1) Contrastive training with citation-structural signals underutilizes citationinformation and still generates single-vector representations. 2) Fine-grainedrepresentation learning, which generates multiple vectors at the sentence oraspect level, requires costly integration and lacks domain generalization. 3)Task-aware learning depends on manually predefined task categorization,overlooking nuanced task distinctions and requiring extra training data fortask-specific modules. To address these problems, we propose a new method thatunifies the three approaches for better representations, namely FLeW.Specifically, we introduce a novel triplet sampling method that leveragescitation intent and frequency to enhance citation-structural signals fortraining. Citation intents (background, method, result), aligned with thegeneral structure of scientific writing, facilitate a domain-generalized facetpartition for fine-grained representation learning. Then, we adopt a simpleweight search to adaptively integrate three facet-level embeddings into atask-specific document embedding without task-aware fine-tuning. Experimentsshow the applicability and robustness of FLeW across multiple scientific tasksand fields, compared to prior models.</description>
      <author>example@mail.com (Zheng Dou, Deqing Wang, Fuzhen Zhuang, Jian Ren, Yanlin Hu)</author>
      <guid isPermaLink="false">2509.07531v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space</title>
      <link>http://arxiv.org/abs/2509.07289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出Kernel VICReg，一种将VICReg目标提升到再生核希尔伯特空间的自监督学习框架，通过核化损失函数实现非线性特征学习，在多个数据集上展示了一致的性能提升。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为强大的表征学习范式，通过优化几何目标而无需标签，但现有方法主要在欧几里得空间中操作，限制了捕捉非线性依赖和几何结构的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在非线性空间中操作的自监督学习框架，以捕捉数据中的非线性依赖和几何结构，提高在复杂数据或小规模数据上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Kernel VICReg框架，将VICReg目标提升到再生核希尔伯特空间，通过核化损失函数的各个项（方差、不变性和协方差），实现在双中心核矩阵和希尔伯特-施密特范数上操作，进行非线性特征学习而无需显式映射。&lt;h4&gt;主要发现&lt;/h4&gt;Kernel VICReg避免了表示崩溃；在复杂或小规模数据任务上提高了性能；在MNIST、CIFAR-10、STL-10、TinyImageNet和ImageNetNet上展示了一致的性能提升；在非线性结构突出的数据集上改进尤为显著；UMAP可视化证实基于核的嵌入表现出更好的等距性和类分离。&lt;h4&gt;结论&lt;/h4&gt;将自监督学习目标核化是将经典核方法与现代表征学习相结合的有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为一种强大的表征学习范式，通过优化几何目标（如对增强的不变性、方差保持和特征解相关）而无需标签。然而，大多数现有方法在欧几里得空间中操作，限制了它们捕捉非线性依赖和几何结构的能力。在这项工作中，我们提出了Kernel VICReg，一种新颖的自监督学习框架，将VICReg目标提升到再生核希尔伯特空间中。通过核化损失函数的各个项（方差、不变性和协方差），我们获得了一种在双中心核矩阵和希尔伯特-施密特范数上操作的一般公式，能够进行非线性特征学习而无需显式映射。我们证明，Kernel VICReg不仅避免了表示崩溃，还在具有复杂或小规模数据的任务上提高了性能。在多个数据集上的经验评估显示，相比欧几里得VICReg有一致的性能提升，特别是在非线性结构突出的数据集上改进尤为显著。UMAP可视化进一步证实，基于核的嵌入表现出更好的等距性和类分离。我们的结果表明，将自监督学习目标核化是将经典核方法与现代表征学习相结合的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm forrepresentation learning by optimizing geometric objectives--such as invarianceto augmentations, variance preservation, and feature decorrelation--withoutrequiring labels. However, most existing methods operate in Euclidean space,limiting their ability to capture nonlinear dependencies and geometricstructures. In this work, we propose Kernel VICReg, a novel self-supervisedlearning framework that lifts the VICReg objective into a Reproducing KernelHilbert Space (RKHS). By kernelizing each term of the loss-variance,invariance, and covariance--we obtain a general formulation that operates ondouble-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinearfeature learning without explicit mappings.  We demonstrate that Kernel VICReg not only avoids representational collapsebut also improves performance on tasks with complex or small-scale data.Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, andImageNet100 show consistent gains over Euclidean VICReg, with particularlystrong improvements on datasets where nonlinear structures are prominent. UMAPvisualizations further confirm that kernel-based embeddings exhibit betterisometry and class separation. Our results suggest that kernelizing SSLobjectives is a promising direction for bridging classical kernel methods withmodern representation learning.</description>
      <author>example@mail.com (M. Hadi Sepanj, Benyamin Ghojogh, Paul Fieguth)</author>
      <guid isPermaLink="false">2509.07289v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data</title>
      <link>http://arxiv.org/abs/2509.07198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fed-REACT是一种针对异构且演变客户端数据的联邦学习框架，通过结合表示学习和进化聚类的两阶段过程，提高了模型的准确性和鲁棒性&lt;h4&gt;背景&lt;/h4&gt;集中式机器学习存在高资源成本和隐私问题，联邦学习(FL)作为替代方案允许客户端协作训练全局模型同时保持数据本地化。然而，实际部署中客户端数据分布随时间演变且在不同客户端间差异显著，这种异质性降低了标准FL算法的性能&lt;h4&gt;目的&lt;/h4&gt;提出一种针对异构且演变的客户端数据的联邦学习框架&lt;h4&gt;方法&lt;/h4&gt;引入Fed-REACT框架，结合表示学习和进化聚类的两阶段过程：(1)第一阶段：每个客户端学习局部模型从其数据中提取特征表示；(2)第二阶段：服务器基于这些表示动态地将客户端分组为集群，并协调集群特定任务的模型训练&lt;h4&gt;主要发现&lt;/h4&gt;对表示学习阶段提供了理论分析，实验证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性&lt;h4&gt;结论&lt;/h4&gt;Fed-REACT是处理异构且演变客户端数据的有效联邦学习框架&lt;h4&gt;翻译&lt;/h4&gt;受集中式机器学习相关的高资源成本和隐私问题启发，联邦学习(FL)已成为一种高效的替代方案，使客户端能够在保持数据本地化的同时协作训练全局模型。然而，在实际部署中，客户端数据分布通常随时间演变且在不同客户端间存在显著差异，引入了异质性，降低了标准FL算法的性能。在这项工作中，我们引入了Fed-REACT，一种为异构且演变的客户端数据设计的联邦学习框架。Fed-REACT将表示学习与进化聚类结合在一个两阶段过程中：(1)在第一阶段，每个客户端学习一个局部模型，从其数据中提取特征表示；(2)在第二阶段，服务器基于这些表示动态地将客户端分组为集群，并协调针对下游目标（如分类或回归）的集群特定任务模型训练。我们对表示学习阶段提供了理论分析，并通过实证证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivated by the high resource costs and privacy concerns associated withcentralized machine learning, federated learning (FL) has emerged as anefficient alternative that enables clients to collaboratively train a globalmodel while keeping their data local. However, in real-world deployments,client data distributions often evolve over time and differ significantlyacross clients, introducing heterogeneity that degrades the performance ofstandard FL algorithms. In this work, we introduce Fed-REACT, a federatedlearning framework designed for heterogeneous and evolving client data.Fed-REACT combines representation learning with evolutionary clustering in atwo-stage process: (1) in the first stage, each client learns a local model toextracts feature representations from its data; (2) in the second stage, theserver dynamically groups clients into clusters based on these representationsand coordinates cluster-wise training of task-specific models for downstreamobjectives such as classification or regression. We provide a theoreticalanalysis of the representation learning stage, and empirically demonstrate thatFed-REACT achieves superior accuracy and robustness on real-world datasets.</description>
      <author>example@mail.com (Yiyue Chen, Usman Akram, Chianing Wang, Haris Vikalo)</author>
      <guid isPermaLink="false">2509.07198v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
      <link>http://arxiv.org/abs/2509.06570v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 2025 IEEE/CVF International Conference on  Computer Vision Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为保留性角度表示学习（RARL）的方法用于增量开放集识别（IOSR），解决了现有方法在动态场景中难以维持决策边界区分能力的问题。通过在角度空间中对齐未知表示并采用虚拟内在交互训练策略和分层校正策略，该方法有效减轻了表示漂移和特征空间扭曲，在CIFAR100和TinyImageNet数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的开放集识别方法通常为静态场景设计，模型分类已知类别并识别固定范围内的未知类别，这与从连续数据流中逐步识别新出现的未知类别的期望不符。在动态场景中，由于无法访问之前的训练数据，决策边界的区分能力难以维持，导致严重的类别间混淆。&lt;h4&gt;目的&lt;/h4&gt;解决增量开放集识别中由于无法访问历史训练数据导致的决策边界区分能力下降和类别间混淆问题，提出一种能够有效处理连续数据流中新出现未知类别的方法。&lt;h4&gt;方法&lt;/h4&gt;提出保留性角度表示学习（RARL）方法，包括：1) 在等角紧框架构建的角度空间内，让未知表示围绕非活跃原型对齐，减轻表示漂移；2) 采用虚拟内在交互（VII）训练策略，通过边界接近的虚拟类强制清晰的类别间边界；3) 设计分层校正策略优化决策边界，减轻样本不平衡导致的表示偏差和特征空间扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR100和TinyImageNet数据集上的实验结果表明，所提出的RARL方法在各种任务设置下都达到了最先进的性能，为增量开放集识别建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;RARL方法通过有效的表示学习和边界优化策略，成功解决了增量开放集识别中的关键挑战，为处理连续数据流中的未知类别识别提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的开放集识别方法通常为静态场景设计，模型旨在分类已知类别并识别固定范围内的未知类别。这与模型应该从连续数据流中逐步识别新出现的未知类别并获取相应知识的期望不符。在这样不断变化的场景中，由于无法访问之前的训练数据，开放集识别决策边界的区分能力难以维持，导致严重的类别间混淆。为解决此问题，我们提出了用于增量开放集识别的保留性角度表示学习（RARL）。在RARL中，未知表示被鼓励在等角紧框架构建的角度空间内围绕非活跃原型对齐，从而减轻知识更新过程中的表示漂移。具体来说，我们采用虚拟内在交互（VII）训练策略，通过边界接近的虚拟类强制清晰的类别间边界，从而压缩已知表示。此外，还设计了一种分层校正策略来优化决策边界，减轻由于新旧类别和正负类别样本不平衡导致的表示偏差和特征空间扭曲。我们在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open set recognition (OSR) methods are typically designed for staticscenarios, where models aim to classify known classes and identify unknown oneswithin fixed scopes. This deviates from the expectation that the model shouldincrementally identify newly emerging unknown classes from continuous datastreams and acquire corresponding knowledge. In such evolving scenarios, thediscriminability of OSR decision boundaries is hard to maintain due torestricted access to former training data, causing severe inter-classconfusion. To solve this problem, we propose retentive angular representationlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknownrepresentations are encouraged to align around inactive prototypes within anangular space constructed under the equiangular tight frame, thereby mitigatingexcessive representation drift during knowledge updates. Specifically, we adopta virtual-intrinsic interactive (VII) training strategy, which compacts knownrepresentations by enforcing clear inter-class margins throughboundary-proximal virtual classes. Furthermore, a stratified rectificationstrategy is designed to refine decision boundaries, mitigating representationbias and feature space distortion caused by imbalances between old/new andpositive/negative class samples. We conduct thorough evaluations on CIFAR100and TinyImageNet datasets and establish a new benchmark for IOSR. Experimentalresults across various task setups demonstrate that the proposed methodachieves state-of-the-art performance.</description>
      <author>example@mail.com (Runqing Yang, Yimin Fu, Changyuan Wu, Zhunga Liu)</author>
      <guid isPermaLink="false">2509.06570v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Guided Diffusion Transformer with Spherical Harmonic Posterior Sampling for High-Fidelity Angular Super-Resolution in Diffusion MRI</title>
      <link>http://arxiv.org/abs/2509.07020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Physics-Guided Diffusion Transformer (PGDiT)方法，用于从低角分辨率dMRI数据重建高角分辨率信号，通过整合物理先验知识提高重建质量。&lt;h4&gt;背景&lt;/h4&gt;现有dMRI角超分辨率方法在恢复细粒度角细节和保持高保真度方面存在局限，主要由于q空间几何建模不足和物理约束整合不够充分。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够在训练和推理阶段探索物理先验的模型，实现高质量的高角分辨率dMRI重建。&lt;h4&gt;方法&lt;/h4&gt;训练阶段采用Q-space Geometry-Aware Module (QGAM)结合b向量调制和随机角掩码促进方向感知学习；推理阶段使用两阶段Spherical Harmonics-Guided Posterior Sampling (SHPS)进行从粗到细的细化，确保物理合理重建。&lt;h4&gt;主要发现&lt;/h4&gt;在ASR任务、DTI和NODDI应用上的实验表明，PGDiT在细节恢复和数据保真度方面优于现有深度学习模型。&lt;h4&gt;结论&lt;/h4&gt;PGDiT提供了一种新的生成式ASR框架，能实现高保真度的高角分辨率dMRI重建，在神经科学和临床研究中有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;扩散磁共振成像(dMRI)角超分辨率(ASR)旨在从有限的低角分辨率(LAR)数据中重建高角分辨率(HAR)信号而不延长扫描时间。然而，现有方法在恢复细粒度角细节或保持高保真度方面存在局限，这是由于对q空间几何建模不足以及物理约束整合不够充分。本文引入了一个Physics-Guided Diffusion Transformer (PGDiT)，旨在探索训练和推理阶段的物理先验知识。在训练阶段，Q-space Geometry-Aware Module (QGAM)结合b向量调制和随机角掩码促进方向感知表示学习，使网络能够从稀疏和嘈杂数据中生成方向一致且具有精细角细节的重建。在推理阶段，两阶段Spherical Harmonics-Guided Posterior Sampling (SHPS)强制与获取数据对齐，然后基于热扩散的SH正则化确保物理合理的重建。这种从粗到细的细化策略减轻了纯数据驱动或生成模型中常见的过度平滑和伪影问题。在ASR任务、DTI和NODDI应用上的大量实验表明，PGDiT在细节恢复和数据保真度方面优于现有深度学习模型。我们的方法提出了一个新的生成式ASR框架，能够提供高保真度的高角分辨率dMRI重建，在神经科学和临床研究中有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion MRI (dMRI) angular super-resolution (ASR) aims to reconstructhigh-angular-resolution (HAR) signals from limited low-angular-resolution (LAR)data without prolonging scan time. However, existing methods are limited inrecovering fine-grained angular details or preserving high fidelity due toinadequate modeling of q-space geometry and insufficient incorporation ofphysical constraints. In this paper, we introduce a Physics-Guided DiffusionTransformer (PGDiT) designed to explore physical priors throughout bothtraining and inference stages. During training, a Q-space Geometry-Aware Module(QGAM) with b-vector modulation and random angular masking facilitatesdirection-aware representation learning, enabling the network to generatedirectionally consistent reconstructions with fine angular details from sparseand noisy data. In inference, a two-stage Spherical Harmonics-Guided PosteriorSampling (SHPS) enforces alignment with the acquired data, followed byheat-diffusion-based SH regularization to ensure physically plausiblereconstructions. This coarse-to-fine refinement strategy mitigatesoversmoothing and artifacts commonly observed in purely data-driven orgenerative models. Extensive experiments on general ASR tasks and twodownstream applications, Diffusion Tensor Imaging (DTI) and Neurite OrientationDispersion and Density Imaging (NODDI), demonstrate that PGDiT outperformsexisting deep learning models in detail recovery and data fidelity. Ourapproach presents a novel generative ASR framework that offers high-fidelityHAR dMRI reconstructions, with potential applications in neuroscience andclinical research.</description>
      <author>example@mail.com (Mu Nan, Taohui Xiao, Ruoyou Wu, Shoujun Yu, Ye Li, Hairong Zheng, Shanshan Wang)</author>
      <guid isPermaLink="false">2509.07020v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
      <link>http://arxiv.org/abs/2509.05685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MSRFormer，一种新的道路网络表示学习框架，通过整合多尺度空间交互解决城市道路网络的异构性和层次性问题。该框架利用空间流卷积提取小尺度特征，识别尺度依赖的空间交互区域，并通过图Transformer捕获多尺度空间依赖关系。实验表明MSRFormer在道路网络分析任务中优于基线方法，复杂道路网络结构中性能提升可达16%。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异构性和层次性特点对准确表示学习构成了挑战。图神经网络在聚合邻接节点特征时，由于其同质性假设和对单一结构尺度的关注而面临困难。&lt;h4&gt;目的&lt;/h4&gt;为了解决图神经网络在城市道路网络表示学习中的局限性，本文提出了MSRFormer框架，旨在通过整合多尺度空间交互来处理流异质性和长距离依赖问题，从而提高道路网络表示学习的准确性。&lt;h4&gt;方法&lt;/h4&gt;MSRFormer使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度依赖的空间交互区域以捕获道路网络的空间结构和流异质性。采用图Transformer捕获多尺度复杂空间依赖关系，通过残差连接融合空间交互特征，并将融合后的特征输入对比学习算法以获得最终的道路网络表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。与最先进的基线方法相比，在复杂道路网络结构中性能提升可达16%。交通相关任务从整合轨迹数据中获益更多，且规模效应与空间交互流异质性之间存在明显的关联模式。&lt;h4&gt;结论&lt;/h4&gt;该研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互中规模效应与流异质性相互作用之间的不同关联模式。MSRFormer通过多尺度方法有效解决了城市道路网络的异构性和层次性挑战。&lt;h4&gt;翻译&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异构性和层次性特点对准确表示学习构成了挑战。图神经网络在聚合邻接节点特征时，由于其同质性假设和对单一结构尺度的关注而面临困难。为解决这些问题，本文提出了MSRFormer，一种新颖的道路网络表示学习框架，通过解决流异质性和长距离依赖问题整合多尺度空间交互。它使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度依赖的空间交互区域以捕获道路网络的空间结构和流异质性。通过采用图Transformer，MSRFormer有效捕获了多尺度复杂空间依赖关系。空间交互特征通过残差连接融合，然后输入对比学习算法以获得最终的道路网络表示。在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，交通相关任务从整合轨迹数据中获益更多，在复杂道路网络结构中与最具竞争力的基线方法相比性能提升可达16%。该研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互中规模效应与流异质性相互作用之间的不同关联模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description>
      <author>example@mail.com (Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You)</author>
      <guid isPermaLink="false">2509.05685v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges</title>
      <link>http://arxiv.org/abs/2509.07887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络在药物发现领域得到广泛应用，能够处理药物分子模型等图结构数据，本文全面涵盖了多个研究类别并提供了未来工作指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络因其处理图结构数据的能力，在药物发现这一复杂领域受到关注。&lt;h4&gt;目的&lt;/h4&gt;为图神经网络在药物发现领域的未来工作提供指导。&lt;h4&gt;方法&lt;/h4&gt;全面回顾和总结图神经网络在药物发现各类研究中的应用，包括分子属性预测、药物-药物相互作用研究等。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络已应用于药物发现研究的多个类别，包括分子属性预测、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计等。&lt;h4&gt;结论&lt;/h4&gt;图神经网络在药物发现领域具有广泛应用前景，本文为相关研究提供了全面概述和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在药物发现这一复杂领域受到关注，因为它们能够处理药物分子模型等图结构数据。这一方法已在已发表文献中产生了众多方法和模型，涵盖药物发现研究的多个类别。本文全面涵盖了这些研究类别，包括最近的论文，即分子属性预测（包括药物-靶点结合亲和力预测）、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计，并为图神经网络在药物发现领域的未来工作提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have gained traction in the complex domain ofdrug discovery because of their ability to process graph-structured data suchas drug molecule models. This approach has resulted in a myriad of methods andmodels in published literature across several categories of drug discoveryresearch. This paper covers the research categories comprehensively with recentpapers, namely molecular property prediction, including drug-target bindingaffinity prediction, drug-drug interaction study, microbiome interactionprediction, drug repositioning, retrosynthesis, and new drug design, andprovides guidance for future work on GNNs for drug discovery.</description>
      <author>example@mail.com (Katherine Berry, Liang Cheng)</author>
      <guid isPermaLink="false">2509.07887v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing</title>
      <link>http://arxiv.org/abs/2509.07725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为IBN的可解释双向建模网络，用于解决多变量时间序列预测中的缺失变量问题，结合不确定性感知插值和高斯核图卷积技术，在各种缺失率场景下实现了最先进的预测性能。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测面临缺失变量的挑战，这些缺失变量阻碍了传统时空图神经网络对变量间相关性的建模。现有方法GinAR虽然首次使用基于注意力的插补和自适应图学习处理变量缺失，但缺乏可解释性且无法捕获更多潜在的时间模式。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提供一个更可靠且可解释的框架来处理缺失变量的多变量时间序列预测问题。&lt;h4&gt;方法&lt;/h4&gt;提出可解释的双向建模网络（IBN），集成不确定性感知插值（UAI）和高斯核图卷积（GGCN）。IBN使用MC Dropout估计重建值的不确定性，并应用不确定性加权策略降低高风险重建。GGCN明确建模变量间的空间相关性，双向RU增强时间依赖建模。&lt;h4&gt;主要发现&lt;/h4&gt;在各种缺失率场景下，IBN实现了最先进的预测性能，为处理缺失变量的多变量时间序列预测提供了更可靠且可解释的框架。&lt;h4&gt;结论&lt;/h4&gt;IBN成功解决了多变量时间序列预测中缺失变量带来的挑战，通过结合不确定性感知插值和高斯核图卷积技术，提供了更可靠和可解释的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列预测（MTSF）通常面临来自缺失变量的挑战，这阻碍了传统的时空图神经网络对变量间相关性的建模。虽然GinAR首次使用基于注意力的插补和自适应图学习来处理变量缺失，但它缺乏可解释性，并且由于其简单的递归单元（RU）而无法捕获更多潜在的时间模式。为了克服这些局限性，我们提出了可解释的双向建模网络（IBN），集成了不确定性感知插值（UAI）和高斯核图卷积（GGCN）。IBN使用MC Dropout估计重建值的不确定性，并应用不确定性加权策略来降低高风险重建。GGCN明确建模变量间的空间相关性，而双向RU增强了时间依赖建模。大量实验表明，在各种缺失率场景下，IBN实现了最先进的预测性能，为具有缺失变量的MTSF提供了更可靠和可解释的框架。代码可在https://github.com/zhangth1211/NICLab-IBN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting (MTSF) often faces challenges frommissing variables, which hinder conventional spatial-temporal graph neuralnetworks in modeling inter-variable correlations. While GinAR addressesvariable missing using attention-based imputation and adaptive graph learningfor the first time, it lacks interpretability and fails to capture more latenttemporal patterns due to its simple recursive units (RUs). To overcome theselimitations, we propose the Interpretable Bidirectional-modeling Network (IBN),integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-basedGraph Convolution (GGCN). IBN estimates the uncertainty of reconstructed valuesusing MC Dropout and applies an uncertainty-weighted strategy to mitigatehigh-risk reconstructions. GGCN explicitly models spatial correlations amongvariables, while a bidirectional RU enhances temporal dependency modeling.Extensive experiments show that IBN achieves state-of-the-art forecastingperformance under various missing-rate scenarios, providing a more reliable andinterpretable framework for MTSF with missing variables. Code is available at:https://github.com/zhangth1211/NICLab-IBN.</description>
      <author>example@mail.com (Shusen Ma, Tianhao Zhang, Qijiu Xia, Yun-Bo Zhao)</author>
      <guid isPermaLink="false">2509.07725v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>DeepGraphLog for Layered Neurosymbolic AI</title>
      <link>http://arxiv.org/abs/2509.07665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepGraphLog的新型神经符号AI框架，解决了现有框架在处理图结构数据时的局限性，实现了更灵活的神经-符号集成。&lt;h4&gt;背景&lt;/h4&gt;神经符号AI旨在结合神经网络的统计优势和符号推理的可解释性，但当前框架如DeepProbLog强制符号推理必须跟随神经处理，限制了它们对复杂依赖关系的建模能力，特别是在处理图等不规则数据结构时。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理复杂依赖关系，特别是图结构数据的NeSy框架，突破现有框架的限制，实现更灵活的神经-符号集成。&lt;h4&gt;方法&lt;/h4&gt;提出DeepGraphLog，将ProbLog扩展为具有图神经谓词，支持多层神经-符号推理，允许神经和符号组件以任意顺序分层，并将符号表示视为图，通过图神经网络处理。&lt;h4&gt;主要发现&lt;/h4&gt;DeepGraphLog在规划、知识图谱补全（远程监督）和GNN表达能力等任务上展示了其能力，能够有效捕获复杂的关系依赖关系，克服了现有NeSy系统的关键局限性。&lt;h4&gt;结论&lt;/h4&gt;DeepGraphLog拓宽了神经符号AI在图结构领域的适用性，提供了一个更具表现力和灵活的神经-符号集成框架。&lt;h4&gt;翻译&lt;/h4&gt;神经符号AI（NeSy）旨在结合神经网络的统计优势与符号推理的可解释性和结构性。然而，当前NeSy框架如DeepProbLog强制符号推理必须跟随神经处理，这限制了它们对复杂依赖关系的建模能力，特别是在处理图等不规则数据结构时。本文介绍了DeepGraphLog，一种新型NeSy框架，将ProbLog扩展为具有图神经谓词。DeepGraphLog支持多层神经-符号推理，允许神经和符号组件以任意顺序分层。与无法通过神经方法处理符号推理的DeepProbLog不同，DeepGraphLog将符号表示视为图，可通过图神经网络（GNN）处理。我们在规划、知识图谱补全（远程监督）和GNN表达能力等任务上展示了DeepGraphLog的能力。结果表明，DeepGraphLog能有效捕获复杂的关系依赖关系，克服了现有NeSy系统的关键局限性。通过拓宽神经符号AI在图结构领域的适用性，DeepGraphLog为神经-符号集成提供了更具表现力和灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neuralnetworks with the interpretability and structure of symbolic reasoning.However, current NeSy frameworks like DeepProbLog enforce a fixed flow wheresymbolic reasoning always follows neural processing. This restricts theirability to model complex dependencies, especially in irregular data structuressuch as graphs. In this work, we introduce DeepGraphLog, a novel NeSy frameworkthat extends ProbLog with Graph Neural Predicates. DeepGraphLog enablesmulti-layer neural-symbolic reasoning, allowing neural and symbolic componentsto be layered in arbitrary order. In contrast to DeepProbLog, which cannothandle symbolic reasoning via neural methods, DeepGraphLog treats symbolicrepresentations as graphs, enabling them to be processed by Graph NeuralNetworks (GNNs). We showcase the capabilities of DeepGraphLog on tasks inplanning, knowledge graph completion with distant supervision, and GNNexpressivity. Our results demonstrate that DeepGraphLog effectively capturescomplex relational dependencies, overcoming key limitations of existing NeSysystems. By broadening the applicability of neurosymbolic AI tograph-structured domains, DeepGraphLog offers a more expressive and flexibleframework for neural-symbolic integration.</description>
      <author>example@mail.com (Adem Kikaj, Giuseppe Marra, Floris Geerts, Robin Manhaeve, Luc De Raedt)</author>
      <guid isPermaLink="false">2509.07665v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Integrated Gradients for Explaining Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.07648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Australasian Joint Conference on Artificial  Intelligence (AJCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的集成梯度(GB-IG)方法，将传统的集成梯度(IG)可解释性技术扩展到图结构数据上，解决了IG不适用于离散图结构的问题。&lt;h4&gt;背景&lt;/h4&gt;集成梯度(IG)是一种常见的可解释性技术，用于解决神经网络的黑盒问题，但它假设数据是连续的，而图是离散结构，这使得IG不适用于图数据。&lt;h4&gt;目的&lt;/h4&gt;将集成梯度(IG)方法扩展到图结构数据上，开发一种专门针对图数据的可解释性技术。&lt;h4&gt;方法&lt;/h4&gt;提出基于图的集成梯度(GB-IG)，作为IG在图数据上的扩展方法。&lt;h4&gt;主要发现&lt;/h4&gt;在四个合成数据集上，GB-IG能够准确识别分类任务中使用的图的关键结构组件；在三个 prevalent 现实世界图数据集上，GB-IG在突出显示节点分类任务中的重要特征方面优于传统的IG方法。&lt;h4&gt;结论&lt;/h4&gt;GB-IG是IG在图数据上的有效扩展，能够更好地处理图结构的离散特性，为图神经网络提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;集成梯度(IG)是一种常见的可解释性技术，用于解决神经网络的黑盒问题。集成梯度假设数据是连续的。图是离散结构，这使得IG不适用于图。在这项工作中，我们引入了基于图的集成梯度(GB-IG)；作为IG在图上的扩展。我们在四个合成数据集上证明，GB-IG能够准确识别分类任务中使用的图的关键结构组件。我们进一步在三个 prevalent 现实世界图数据集上证明，GB-IG在突出显示节点分类任务中的重要特征方面优于IG。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated Gradients (IG) is a common explainability technique to address theblack-box problem of neural networks. Integrated gradients assumes continuousdata. Graphs are discrete structures making IG ill-suited to graphs. In thiswork, we introduce graph-based integrated gradients (GB-IG); an extension of IGto graphs. We demonstrate on four synthetic datasets that GB-IG accuratelyidentifies crucial structural components of the graph used in classificationtasks. We further demonstrate on three prevalent real-world graph datasets thatGB-IG outperforms IG in highlighting important features for node classificationtasks.</description>
      <author>example@mail.com (Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew)</author>
      <guid isPermaLink="false">2509.07648v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice</title>
      <link>http://arxiv.org/abs/2509.07123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的嵌套效用图神经网络(NestGNN)，用于改进离散选择分析，特别是交通方式选择问题。该方法结合了经典嵌套logit模型和深度神经网络的优势，通过引入替代方案图的概念来表示交通方式间的关系。&lt;h4&gt;背景&lt;/h4&gt;嵌套logit模型(NL)已被广泛用于离散选择分析，包括交通方式选择、汽车拥有或位置决策等多种应用。然而，传统NL模型受到其表示能力有限和手工指定效用的限制。虽然研究者引入了深度神经网络(DNNs)来解决这些挑战，但现有DNN无法明确捕捉离散选择情境中的替代方案间的相关性。&lt;h4&gt;目的&lt;/h4&gt;解决传统NL模型的表示能力有限和手工指定效用的问题，以及现有DNN无法明确捕捉离散选择情境中替代方案间相关性的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的概念——替代方案图，来表示交通方式替代方案之间的关系；使用嵌套替代方案图，设计了嵌套效用图神经网络(NestGNN)，作为神经网络家族中经典NL模型的泛化。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，NestGNN在模型表示方面泛化了经典NL模型和现有DNN，同时保留了NL模型关键的二层替代模式：巢内比例替代但巢外非比例替代。经验上，NestGNN显著优于基准模型，特别是比相应的NL模型高出9.2%。从弹性表和替代可视化可以看出，NestGNN保留了NL模型的二层替代模式，但在模型设计空间上表现出更大的灵活性。&lt;h4&gt;结论&lt;/h4&gt;NestGNN在预测、解释方面表现出强大的能力，并且能够灵活地泛化经典NL模型用于分析交通方式选择问题。&lt;h4&gt;翻译&lt;/h4&gt;嵌套logit(NL)已被广泛用于离散选择分析，包括交通方式选择、汽车拥有或位置决策等多种应用。然而，传统NL模型受到其表示能力有限和手工指定效用的限制。虽然研究者引入了深度神经网络(DNNs)来解决这些挑战，但现有DNN无法明确捕捉离散选择情境中的替代方案间的相关性。为应对这些挑战，本研究提出了一个新概念——替代方案图，来表示交通方式替代方案之间的关系。使用嵌套替代方案图，本研究进一步设计了嵌套效用图神经网络(NestGNN)，作为神经网络家族中经典NL模型的泛化。理论上，NestGNN在模型表示方面泛化了经典NL模型和现有DNN，同时保留了NL模型关键的二层替代模式：巢内比例替代但巢外非比例替代。经验上，我们发现NestGNN显著优于基准模型，特别是比相应的NL模型高出9.2%。从弹性表和替代可视化可以看出，NestGNN保留了NL模型的二层替代模式，但在模型设计空间上表现出更大的灵活性。总体而言，我们的研究展示了NestGNN在预测、解释方面的能力，以及其泛化经典NL模型分析交通方式选择的灵活性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nested logit (NL) has been commonly used for discrete choice analysis,including a wide range of applications such as travel mode choice, automobileownership, or location decisions. However, the classical NL models arerestricted by their limited representation capability and handcrafted utilityspecification. While researchers introduced deep neural networks (DNNs) totackle such challenges, the existing DNNs cannot explicitly captureinter-alternative correlations in the discrete choice context. To address thechallenges, this study proposes a novel concept - alternative graph - torepresent the relationships among travel mode alternatives. Using a nestedalternative graph, this study further designs a nested-utility graph neuralnetwork (NestGNN) as a generalization of the classical NL model in the neuralnetwork family. Theoretically, NestGNNs generalize the classical NL models andexisting DNNs in terms of model representation, while retaining the crucialtwo-layer substitution patterns of the NL models: proportional substitutionwithin a nest but non-proportional substitution beyond a nest. Empirically, wefind that the NestGNNs significantly outperform the benchmark models,particularly the corresponding NL models by 9.2\%. As shown by elasticitytables and substitution visualization, NestGNNs retain the two-layersubstitution patterns as the NL model, and yet presents more flexibility in itsmodel design space. Overall, our study demonstrates the power of NestGNN inprediction, interpretation, and its flexibility of generalizing the classicalNL model for analyzing travel mode choice.</description>
      <author>example@mail.com (Yuqi Zhou, Zhanhong Cheng, Lingqian Hu, Yuheng Bu, Shenhao Wang)</author>
      <guid isPermaLink="false">2509.07123v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.06763v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VariSAC的图神经网络增强深度强化学习框架，用于解决RIS辅助ISAC使能的V2X系统中的统一可靠建模和资源优化问题，实现了时间连续的保证连接。&lt;h4&gt;背景&lt;/h4&gt;可重构智能表面与集成感知和通信在车辆网络中的集成实现了动态空间资源管理和环境实时适应，但V2I和V2V连接要求的共存以及高度动态异构的网络拓扑对统一可靠建模和资源优化提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决V2I和V2V连接要求共存问题，应对高度动态异构网络拓扑，实现统一可靠建模和资源优化，提供时间连续的保证连接。&lt;h4&gt;方法&lt;/h4&gt;提出VariSAC框架，引入连续连接比率作为统一指标，使用带有残差适配器的图神经网络编码复杂高维系统状态，并通过软演员-评论家代理联合优化信道分配、功率控制和RIS配置。&lt;h4&gt;主要发现&lt;/h4&gt;在真实城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V交付可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;h4&gt;结论&lt;/h4&gt;VariSAC有效解决了RIS辅助ISAC使能的V2X系统中的统一可靠建模和资源优化问题，实现了时间连续的保证连接。&lt;h4&gt;翻译&lt;/h4&gt;可重构智能表面与集成感知和通信在车辆网络中的集成实现了动态空间资源管理和对环境变化的实时适应。然而，车辆到基础设施和车辆到车辆连接要求的共存，以及高度动态和异构的网络拓扑，给统一可靠建模和资源优化带来了显著挑战。为解决这些问题，我们提出VariSAC，一种用于RIS辅助、ISAC使能的车辆到万物系统中保证时间连续连接的图神经网络增强深度强化学习框架。具体而言，我们引入了连续连接比率作为统一指标，表征V2I连接的持续时间可靠性和V2V链路概率交付保证，从而统一了它们的连续可靠性语义。接下来，我们采用带有残差适配器的图神经网络来编码复杂高维系统状态，捕捉车辆、基站和RIS节点之间的空间依赖关系。这些表示随后由软演员-评论家代理处理，该代理联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。在真实城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V交付可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of Reconfigurable Intelligent Surfaces (RIS) and IntegratedSensing and Communication (ISAC) in vehicular networks enables dynamic spatialresource management and real-time adaptation to environmental changes. However,the coexistence of distinct vehicle-to-infrastructure (V2I) andvehicle-to-vehicle (V2V) connectivity requirements, together with highlydynamic and heterogeneous network topologies, presents significant challengesfor unified reliability modeling and resource optimization. To address theseissues, we propose VariSAC, a graph neural network (GNN)-augmented deepreinforcement learning framework for assured, time-continuous connectivity inRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,we introduce the Continuous Connectivity Ratio (CCR), a unified metric thatcharacterizes the sustained temporal reliability of V2I connections and theprobabilistic delivery guarantees of V2V links, thus unifying their continuousreliability semantics. Next, we employ a GNN with residual adapters to encodecomplex, high-dimensional system states, capturing spatial dependencies amongvehicles, base stations (BS), and RIS nodes. These representations are thenprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channelallocation, power control, and RIS configurations to maximize CCR-drivenlong-term rewards. Extensive experiments on real-world urban datasetsdemonstrate that VariSAC consistently outperforms existing baselines in termsof continuous V2I ISAC connectivity and V2V delivery reliability, enablingpersistent connectivity in highly dynamic vehicular environments.</description>
      <author>example@mail.com (Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun)</author>
      <guid isPermaLink="false">2509.06763v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
      <link>http://arxiv.org/abs/2509.05429v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acctepted by ACM CCS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络(GNNs)中的拓扑隐私风险，提出了一种名为私有图重构(PGR)的新型防御框架，能够在保护图结构隐私的同时保持模型准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为处理图结构数据的有力模型，但其广泛应用引发了严重的隐私问题。先前研究主要关注边缘级别的隐私保护，而忽略了拓扑隐私（即图整体结构的保密性）这一关键但尚未充分探索的威胁。&lt;h4&gt;目的&lt;/h4&gt;全面探讨GNN中的拓扑隐私风险，揭示其对图级推理攻击的脆弱性，并提出有效的防御机制。&lt;h4&gt;方法&lt;/h4&gt;提出了一套拓扑推理攻击(TIAs)方法，仅通过黑盒访问GNN模型就能重建目标训练图的结构；同时引入了私有图重构(PGR)防御框架，这是一个双层优化问题，通过元梯度迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;GNN极易受到拓扑推理攻击，现有的边缘级别差分隐私机制不足以缓解风险，要么无法减轻风险，要么严重损害模型准确性。&lt;h4&gt;结论&lt;/h4&gt;PGR框架能够显著减少拓扑信息泄露，同时对模型准确性的影响最小。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的有力模型。然而，它们的广泛应用引发了严重的隐私问题。虽然先前研究主要关注边缘级别的隐私保护，但一个关键 yet 尚未充分探索的威胁在于拓扑隐私 - 图整体结构的保密性。在这项工作中，我们对GNN中的拓扑隐私风险进行了全面研究，揭示了它们对图级推理攻击的脆弱性。为此，我们提出了一套拓扑推理攻击(TIAs)，仅通过黑盒访问GNN模型就能重建目标训练图的结构。我们的发现表明，GNN极易受到这些攻击，现有的边缘级别差分隐私机制不足以缓解风险，因为它们要么无法减轻风险，要么严重损害模型准确性。为应对这一挑战，我们引入了私有图重构(PGR)，这是一种新型防御框架，旨在保护拓扑隐私同时保持模型准确性。PGR被表述为一个双层优化问题，其中使用元梯度迭代生成合成训练图，GNN模型则基于不断演化的图同时更新。大量实验证明，PGR显著减少了拓扑信息泄露，同时对模型准确性的影响最小。我们的代码可在 https://github.com/JeffffffFu/PGR 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for learningfrom graph-structured data. However, their widespread adoption has raisedserious privacy concerns. While prior research has primarily focused onedge-level privacy, a critical yet underexplored threat lies in topologyprivacy - the confidentiality of the graph's overall structure. In this work,we present a comprehensive study on topology privacy risks in GNNs, revealingtheir vulnerability to graph-level inference attacks. To this end, we propose asuite of Topology Inference Attacks (TIAs) that can reconstruct the structureof a target training graph using only black-box access to a GNN model. Ourfindings show that GNNs are highly susceptible to these attacks, and thatexisting edge-level differential privacy mechanisms are insufficient as theyeither fail to mitigate the risk or severely compromise model accuracy. Toaddress this challenge, we introduce Private Graph Reconstruction (PGR), anovel defense framework designed to protect topology privacy while maintainingmodel accuracy. PGR is formulated as a bi-level optimization problem, where asynthetic training graph is iteratively generated using meta-gradients, and theGNN model is concurrently updated based on the evolving graph. Extensiveexperiments demonstrate that PGR significantly reduces topology leakage withminimal impact on model accuracy. Our code is available athttps://github.com/JeffffffFu/PGR.</description>
      <author>example@mail.com (Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang)</author>
      <guid isPermaLink="false">2509.05429v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</title>
      <link>http://arxiv.org/abs/2509.07923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作介绍了一种名为ToothMCL的多模态预训练框架，用于牙齿分割，整合了CBCT和IOS两种数据模态，通过多模态对比学习提高了分割精度，并在多个独立数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;数字牙科代表了现代牙科实践的重大转变，其基础是准确获取患者牙齿的数字表示。然而，尽管数字牙科技术日益受到关注，现有的分割方法通常缺乏严格的验证，性能有限且临床应用性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态预训练框架，用于提高牙齿分割的准确性和临床适用性，实现精确的多类分割和准确的FDI牙齿编号识别。&lt;h4&gt;方法&lt;/h4&gt;提出ToothMCL（Tooth Multimodal Contrastive Learning）框架，整合体积型CBCT和基于表面的IOS数据模态，通过多模态对比学习捕获模态不变表示。同时构建了CBCT-IOS3.8K数据集，包含3,867名患者的配对CBCT和IOS数据，并在一系列独立数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;ToothMCL在内部和外部测试中都达到了最先进的性能，Dice相似系数在CBCT分割上提高了12%，在IOS分割上提高了8%。该方法在牙齿组分割方面持续超越现有方法，并在不同的成像条件和临床场景中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ToothMCL通过多模态对比学习有效建模了细粒度解剖特征，显著提高了牙齿分割的准确性和临床适用性，为数字牙科实践提供了强有力的工具。&lt;h4&gt;翻译&lt;/h4&gt;数字牙科代表了现代牙科实践的一次转变性变革。这一转变的基础是准确获取患者牙齿的数字表示，这通常通过分割锥形束计算机断层扫描和口腔内扫描获得。尽管数字牙科技术日益受到关注，现有的分割方法通常缺乏严格的验证，性能有限且临床应用性差。据我们所知，这是首次引入用于牙齿分割的多模态预训练框架的工作。我们提出了ToothMCL，这是一种用于预训练的牙齿多模态对比学习，整合了体积型和基于表面的模态。通过多模态对比学习捕获模态不变表示，我们的方法有效地建模了细粒度解剖特征，实现了精确的多类分割和准确的Fédération Dentaire Internationale牙齿编号识别。除了该框架外，我们还构建了CBCT-IOS3.8K，这是迄今为止最大的配对CBCT和IOS数据集，包含3,867名患者。随后，我们在一系列独立数据集上评估了ToothMCL，这是迄今为止最大且最多样化的评估。我们的方法在内部和外部测试中都达到了最先进的性能，Dice相似系数在CBCT分割上提高了12%，在IOS分割上提高了8%。此外，ToothMCL在牙齿组方面持续超越现有方法，并在不同的成像条件和临床场景中表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital dentistry represents a transformative shift in modern dentalpractice. The foundational step in this transformation is the accurate digitalrepresentation of the patient's dentition, which is obtained from segmentedCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite thegrowing interest in digital dental technologies, existing segmentationmethodologies frequently lack rigorous validation and demonstrate limitedperformance and clinical applicability. To the best of our knowledge, this isthe first work to introduce a multimodal pretraining framework for toothsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning forpretraining that integrates volumetric (CBCT) and surface-based (IOS)modalities. By capturing modality-invariant representations through multimodalcontrastive learning, our approach effectively models fine-grained anatomicalfeatures, enabling precise multi-class segmentation and accurate identificationof F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with theframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset todate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensivecollection of independent datasets, representing the largest and most diverseevaluation to date. Our method achieves state-of-the-art performance in bothinternal and external testing, with an increase of 12\% for CBCT segmentationand 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).Furthermore, ToothMCL consistently surpasses existing approaches in toothgroups and demonstrates robust generalizability across varying imagingconditions and clinical scenarios.</description>
      <author>example@mail.com (Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen)</author>
      <guid isPermaLink="false">2509.07923v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2509.07623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的自监督交叉编码器框架，用于解决神经退行性疾病诊断中依赖大量标记数据和缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习在从MRI数据诊断神经退行性疾病方面显示出巨大潜力，但现有方法主要依赖大量标记数据且往往产生缺乏可解释性的表示。&lt;h4&gt;目的&lt;/h4&gt;解决神经退行性疾病诊断中依赖大量标记数据和缺乏可解释性的双重挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种自监督交叉编码器框架，利用纵向MRI扫描中的时间连续性进行监督。该框架将学习到的表示解耦为静态表示（通过对比学习约束，捕获稳定的解剖特征）和动态表示（由输入梯度正则化引导，反映时间变化，可微调用于下游分类任务）。&lt;h4&gt;主要发现&lt;/h4&gt;在阿尔茨海默病神经影像学倡议(ADNI)数据集上，该方法实现了 superior 的分类准确性和改进的可解释性。学习到的表示在开放获取影像学研究系列(OASIS)数据集上表现出强大的零样本泛化能力，并在帕金森病进展标志物倡议(PPMI)数据集上表现出跨任务泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法在神经退行性疾病诊断中表现出色，具有良好的可解释性和泛化能力，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在从MRI数据诊断神经退行性疾病方面显示出巨大潜力。然而，大多数现有方法严重依赖大量标记数据，并且往往产生缺乏可解释性的表示。为了解决这两个挑战，我们提出了一种新颖的自监督交叉编码器框架，利用纵向MRI扫描中的时间连续性进行监督。该框架将学习到的表示解耦为两个组成部分：静态表示，通过对比学习约束，捕获稳定的解剖特征；动态表示，由输入梯度正则化引导，反映时间变化，可有效微调用于下游分类任务。在阿尔茨海默病神经影像学倡议(ADNI)数据集上的实验结果表明，我们的方法实现了 superior 的分类准确性和改进的可解释性。此外，学习到的表示在开放获取影像学研究系列(OASIS)数据集上表现出强大的零样本泛化能力，并在帕金森病进展标志物倡议(PPMI)数据集上表现出跨任务泛化能力。所提出方法的代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has shown significant potential in diagnosing neurodegenerativediseases from MRI data. However, most existing methods rely heavily on largevolumes of labeled data and often yield representations that lackinterpretability. To address both challenges, we propose a novelself-supervised cross-encoder framework that leverages the temporal continuityin longitudinal MRI scans for supervision. This framework disentangles learnedrepresentations into two components: a static representation, constrained bycontrastive learning, which captures stable anatomical features; and a dynamicrepresentation, guided by input-gradient regularization, which reflectstemporal changes and can be effectively fine-tuned for downstreamclassification tasks. Experimental results on the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) dataset demonstrate that our method achievessuperior classification accuracy and improved interpretability. Furthermore,the learned representations exhibit strong zero-shot generalization on the OpenAccess Series of Imaging Studies (OASIS) dataset and cross-task generalizationon the Parkinson Progression Marker Initiative (PPMI) dataset. The code for theproposed method will be made publicly available.</description>
      <author>example@mail.com (Fangqi Cheng, Yingying Zhao, Xiaochen Yang)</author>
      <guid isPermaLink="false">2509.07623v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Water Demand Forecasting of District Metered Areas through Learned Consumer Representations</title>
      <link>http://arxiv.org/abs/2509.07515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at European Conference for Signal Procesing - EUSIPCO 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新型的短期水需求预测方法，通过无监督对比学习对用户进行分类，并使用小波变换卷积网络进行预测，在真实DMA区域测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;智能水表技术的进步显著提升了对水务设施的监控和管理能力。在气候变化带来不确定性的背景下，保障水资源供应已成为具有广泛社会经济影响的全球性紧迫问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对分区计量区域(DMAs)的短期水需求预测新方法，该方法包含商业、农业和住宅消费者。&lt;h4&gt;方法&lt;/h4&gt;首先应用无监督对比学习对DMA内具有不同消费行为的终端用户进行分类；然后将这些不同的消费行为作为特征，使用结合历史数据和衍生表示的小波变换卷积网络进行后续的需求预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在真实DMA区域进行了六个月的测试，在不同DMA区域的MAPE预测性能上有所提高，最大改进为4.9%。此外，该方法还能识别出其行为受社会经济因素影响的消费者，增强了关于影响需求的确定性模式的前期认识。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过结合用户行为分类和先进的预测模型，有效提高了水需求预测的准确性，并为理解影响需求的社会经济因素提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;智能水表技术的进步显著提高监控和管理水务设施的能力。在气候变化带来不确定性的背景下，保障水资源供应已成为具有广泛社会经济影响的全球性紧迫问题。来自终端用户的每小时消费数据为具有不同消费模式区域的需求预测提供了重要见解。然而，由于气象条件等非确定性影响因素，水需求预测仍然具有挑战性。本文介绍了一种针对包含商业、农业和住宅消费者的分区计量区域(DMAs)的短期水需求预测新方法。应用无监督对比学习对DMA内存在的不同消费行为的终端用户进行分类。随后，将不同的消费行为作为特征，在后续的需求预测任务中使用结合历史数据和衍生表示的小波变换卷积网络。所提出的方法在真实DMA区域进行了六个月的测试，展示了在不同DMA区域MAEP方面的改进预测性能，最大改进为4.9%。此外，它还识别出其行为受社会经济因素影响的消费者，增强了关于影响需求的确定性模式的前期认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in smart metering technologies have significantly improved theability to monitor and manage water utilities. In the context of increasinguncertainty due to climate change, securing water resources and supply hasemerged as an urgent global issue with extensive socioeconomic ramifications.Hourly consumption data from end-users have yielded substantial insights forprojecting demand across regions characterized by diverse consumption patterns.Nevertheless, the prediction of water demand remains challenging due toinfluencing non-deterministic factors, such as meteorological conditions. Thiswork introduces a novel method for short-term water demand forecasting forDistrict Metered Areas (DMAs) which encompass commercial, agricultural, andresidential consumers. Unsupervised contrastive learning is applied tocategorize end-users according to distinct consumption behaviors present withina DMA. Subsequently, the distinct consumption behaviors are utilized asfeatures in the ensuing demand forecasting task using wavelet-transformedconvolutional networks that incorporate a cross-attention mechanism combiningboth historical data and the derived representations. The proposed approach isevaluated on real-world DMAs over a six-month period, demonstrating improvedforecasting performance in terms of MAPE across different DMAs, with a maximumimprovement of 4.9%. Additionally, it identifies consumers whose behavior isshaped by socioeconomic factors, enhancing prior knowledge about thedeterministic patterns that influence demand.</description>
      <author>example@mail.com (Adithya Ramachandran, Thorkil Flensmark B. Neergaard, Tomás Arias-Vergara, Andreas Maier, Siming Bayer)</author>
      <guid isPermaLink="false">2509.07515v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAME-AB是一种用于抗体结合位点预测的新型跨模态注意力框架，结合了多种生物学模态和自适应融合机制，在多个评估指标上表现出色。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态注意力框架CAME-AB，用于稳健的抗体结合位点预测，解决现有方法无法识别抗体特异性结合位点的问题。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB集成了五种生物学基础模态：原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化的生化图；提出了自适应模态融合模块，根据全局相关性和输入特定贡献动态加权每种模态；结合了Transformer编码器和MoE模块促进特征专业化和能力扩展；引入了监督对比学习目标塑造潜在空间几何；在训练期间应用随机权重平均提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上一致优于强基线；消融研究验证了每个架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过结合多种生物学基础模态和自适应融合机制，有效提高了抗体结合位点预测的准确性，为计算免疫学和治疗性抗体设计提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出了CAME-AB，一种具有专家混合（MoE）主干的新颖跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB将五种基于生物学的模态整合到统一的multimodal表示中，包括原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化的生化图。为了增强自适应跨模态推理，我们提出了一个自适应模态融合模块，根据其全局相关性和输入特定贡献动态学习对每种模态进行加权。结合MoE模块的Transformer编码器进一步促进了特征专业化和能力扩展。我们还结合了监督对比学习目标，明确塑造潜在空间几何，鼓励类内紧凑性和类间可分离性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在多个指标上始终优于强基线，包括精确度、召回率、F1分数、AUC-ROC和MCC。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
      <link>http://arxiv.org/abs/2509.06995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Protocol Genome，一种自监督学习系统，通过学习DICOM头部信息的相关性，在完全保留的外部验证中实现了AUROC 0.901（对比基线0.847）和ECE 0.036（对比0.056）的优异性能。&lt;h4&gt;背景&lt;/h4&gt;临床影像通过PACS/DICOM系统传输，程序选择（如扫描仪型号、序列参数等）影响影像质量，这些潜在的混杂因素阻碍了仅基于图像的网络在不同站点间的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;将结构化的DICOM头部信息作为标签，学习协议感知但临床稳健的图像表示，提高模型在不同模态和供应商间的校准能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;获取去标识化DICOM头部的标记化嵌入，结合图像特征使用三种方法建模：(1)协议-图像对比学习，(2)掩蔽协议预测，(3)协议-协议翻译。研究使用了126万项临床数据（7个医疗系统，31台扫描仪，3个供应商）。&lt;h4&gt;主要发现&lt;/h4&gt;相比强大的SSL基线和ImageNet迁移，Protocol Genome在外部AUROC上显著提高（肺栓塞:+0.046，胶质瘤:+0.058，心肌肥大:+0.041），并实现25-37%的校准改进（p &lt; 0.01）。即使仅使用10-20%的标记数据，这些增益仍能保持。&lt;h4&gt;结论&lt;/h4&gt;该技术减少了协议边界处的假阳性，可直接集成到PACS系统中，并已发布包含去标识化和偏见审计的模型卡和部署指南。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为协议基因组（Protocol Genome）的自监督学习系统，该系统从DICOM头部信息中学习相关性，并在完全保留的外部验证中实现了AUROC 0.901（对比基线0.847）和ECE 0.036（对比0.058）。我们的方法还提高了CT、MRI、CXR等不同模态和供应商间的校准能力和鲁棒性。临床影像通过PACS/DICOM系统传输，程序选择（扫描仪型号/型号、序列、内核、kVp、TR/TE和切片厚度）会影响对比度、噪声和伪影。这些潜在的混杂因素阻碍了仅基于图像的网络在不同站点间的泛化。我们将结构化的DICOM头部信息作为标签，学习协议感知但临床稳健的图像表示。协议基因组获取去标识化头部字段的标记化嵌入，并使用以下方法与图像特征一起建模：(1)协议-图像对比学习，(2)掩蔽协议预测，(3)协议-协议翻译。基于126万项研究（7个医疗系统，31台扫描仪，3个供应商；CT、MR、CR/DR），我们在以下任务上进行了实验：(A)胸部CT肺栓塞分诊，(B)脑部MRI胶质瘤分级，(C)胸部放射心肌肥大检测。与强大的SSL基线（SimCLR、MAE）以及ImageNet迁移相比，协议基因组在外部AUROC上表现更优（肺栓塞:+0.046，胶质瘤:+0.058，心肌肥大:+0.041），并获得了25-37%的校准改进（p &lt; 0.01，DeLong测试）。虽然增益可能依赖于任务，但在仅使用10-20%的标记数据时仍能保持。从临床角度来看，该技术减少了协议边界处的假阳性，并可应用于PACS系统（DICOM C-FIND/C-MOVE，DICOMweb QIDO/WADO）。我们发布了包含去标识化和偏见审计的模型卡和部署指南。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学影像中的域偏移、扫描仪异质性和隐藏混杂因素问题。这些问题在现实中很重要，因为不同医院和设备的协议差异导致AI模型难以跨站点部署，限制了医学影像AI的临床应用；标签稀缺问题也阻碍了模型训练；而模型可能学习到与临床决策无关的协议特征，导致预测偏差。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将DICOM头文件视为'基因组'代码，借鉴了基因组学的概念，设计了一个多模态自监督学习框架。他们参考了现有的自监督学习方法（如SimCLR、MoCo、MAE）、医学影像深度学习架构（ResNet、ViT等）和域适应技术。创新点在于将协议信息融入自监督学习，通过标记化DICOM头文件、混合注意力融合和对抗性头部来分离协议身份和临床特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将DICOM头文件从'负担'转变为'第一类自监督信号'，将域偏移从'对手'变为'教师'。实现流程包括：1)数据预处理（选择、标记化、去标识化DICOM头文件）；2)自监督预训练（协议-图像对比学习、掩码协议建模、协议-协议翻译）；3)架构设计（图像和协议编码器、混合注意力融合、对抗性头部）；4)偏见感知微调（梯度反转学习、重要性重加权、采集感知增强）；5)多站点外部评估和子群体分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将DICOM头文件作为基因组代码处理；2)多组件自监督目标（对比学习、掩码建模、协议翻译）；3)混合注意力融合机制；4)对抗性头部分离协议和临床特征；5)与PACS/DICOM工作流集成。相比之前工作，传统方法只考虑像素信息，将协议视为障碍；而本文将协议视为有价值信号，首次系统利用DICOM头文件作为自监督目标，并提供完整的模型卡和部署建议。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Protocol Genome通过将DICOM头文件转化为自监督学习信号，解决了医学影像中的域偏移和标签稀缺问题，实现了跨站点、跨厂商的鲁棒临床预测，同时提供了可审计的协议感知表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce the Protocol Genome, a self-supervised learningsystem that learns correlations from DICOM headers and achieves AUROC 0.901 (vs0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.Our method also improves calibration and robustness across modalities (CT, MRI,CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, whereprocedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slicethickness) have consequences for contrast, noise, and artifact. These latentconfounders impede the generalization of image-only networks across sites. Weconsider structured DICOM headers as a label and learn protocol-aware butclinically robust image representations. Protocol Genome obtains tokenizedembeddings of de-identified header fields and models them along with imagefeatures using: (1) protocol-image contrastive learning, (2) masked protocolprediction, and (3) protocol-protocol translation. With 1.26M studies (7 healthsystems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CTtriage for PE, (B) brain MRI glioma grading, and (C) chest radiographcardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as wellas ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:cardiomegaly) is associated with higher external AUROC; 25-37% calibrationimprovements are obtained (p &lt; 0.01, DeLong tests). While the gains may betask-dependent, they are preserved with 10-20% of labeled data. From a clinicalpoint of view, the technique reduces false positives at protocol borders and isapplicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish amodel card and deployment guide, complete with both de-identification and biasaudits.</description>
      <author>example@mail.com (Jimmy Joseph)</author>
      <guid isPermaLink="false">2509.06995v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
      <link>http://arxiv.org/abs/2509.07766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, International Quantum Engineering conference and  exhibition (QUEST-IS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图的联盟结构生成算法(GCS-Q)来处理金融资产的聚类问题，特别是在处理带符号的相关性结构时。这种方法避免了传统聚类方法中的有损转换和启发式假设，如固定聚类数量，并利用量子退火技术高效探索解决方案空间。实验证明，该方法在合成和真实金融数据上均优于最先进的经典算法，能够动态确定聚类数量，并提高聚类质量。&lt;h4&gt;背景&lt;/h4&gt;基于收益相关性的金融资产聚类是投资组合优化和统计套利中的基本任务。然而，传统的聚类方法在处理带符号的相关性结构时往往效果不佳，通常需要依赖有损转换和启发式假设，如固定聚类数量。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接处理带符号、加权图的聚类方法，无需依赖有损转换和固定聚类数量的假设，并利用量子计算技术提高聚类效率和质量。&lt;h4&gt;方法&lt;/h4&gt;应用基于图的联盟结构生成算法(GCS-Q)，该方法将每个分区步骤表述为QUBO问题，从而能够利用量子退火技术高效探索指数级大的解决方案空间。在合成和真实金融数据上验证该方法，并与最先进的经典算法(如SPONGE和k-Medoids)进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GCS-Q方法在调整兰德指数和结构平衡惩罚等指标上持续获得更高的聚类质量，同时能够动态确定聚类数量。这些结果凸显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;h4&gt;结论&lt;/h4&gt;GCS-Q算法为金融资产聚类提供了一种有效的新方法，特别适用于处理带符号的相关性结构。量子计算技术在解决金融领域的复杂聚类问题时具有实用价值，能够克服传统方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;基于收益相关性的金融资产聚类是投资组合优化和统计套利中的基本任务。然而，当处理带符号的相关性结构时，传统聚类方法往往表现不佳，通常需要依赖有损转换和启发式假设，如固定聚类数量。在本工作中，我们应用基于图的联盟结构生成算法(GCS-Q)来直接聚类带符号、加权图，而不依赖这些转换。GCS-Q将每个分区步骤表述为QUBO问题，使其能够利用量子退火技术高效探索指数级大的解决方案空间。我们在合成和真实金融数据上验证了我们的方法，并与最先进的经典算法(如SPONGE和k-Medoids)进行了基准测试。我们的实验证明，GCS-Q在调整兰德指数和结构平衡惩罚等指标上持续获得更高的聚类质量，同时动态确定聚类数量。这些结果凸显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering financial assets based on return correlations is a fundamentaltask in portfolio optimization and statistical arbitrage. However, classicalclustering methods often fall short when dealing with signed correlationstructures, typically requiring lossy transformations and heuristic assumptionssuch as a fixed number of clusters. In this work, we apply the Graph-basedCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,weighted graphs without relying on such transformations. GCS-Q formulates eachpartitioning step as a QUBO problem, enabling it to leverage quantum annealingfor efficient exploration of exponentially large solution spaces. We validateour approach on both synthetic and real-world financial data, benchmarkingagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Ourexperiments demonstrate that GCS-Q consistently achieves higher clusteringquality, as measured by Adjusted Rand Index and structural balance penalties,while dynamically determining the number of clusters. These results highlightthe practical utility of near-term quantum computing for graph-basedunsupervised learning in financial applications.</description>
      <author>example@mail.com (Shivam Sharma, Supreeth Mysore Venkatesh, Pushkin Kachroo)</author>
      <guid isPermaLink="false">2509.07766v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>http://arxiv.org/abs/2509.07627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LSMTCR是一个可扩展的多架构框架，能够从头开始、基于表位条件生成配对的全长T细胞受体(TCR)，在多个数据集上表现出优于基线的预测结合能力，并提供了温度可调的多样性和免疫遗传保真度。&lt;h4&gt;背景&lt;/h4&gt;设计全长、表位特异性的T细胞受体(TCR)具有挑战性，主要源于序列空间庞大、数据偏差和免疫遗传约束建模不完整等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的多架构框架LSMTCR，将特异性学习与约束学习分离，实现从头开始、基于表位条件生成配对的全长TCRs。&lt;h4&gt;方法&lt;/h4&gt;LSMTCR采用三个关键组件：1) 扩散增强的BERT编码器学习时间条件的表位表示；2) 条件GPT解码器在跨模态条件下生成链特异性CDR3，具有温度控制的多样性；3) 基因感知的Transformer通过预测V/J使用来组装完整的α/β序列，确保免疫遗传保真度。&lt;h4&gt;主要发现&lt;/h4&gt;LSMTCR在GLIPH、TEP、MIRA、McPAS和策展数据集上，在大多数数据集上的预测结合能力优于基线；更忠实地恢复了位置和长度语法；提供了优越的、温度可调的多样性；迁移学习提高了α链生成的预测结合能力、长度真实性和多样性；从已知或从头开始的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低；在与表位的配对α/β共建模中，比单链设置获得更高的pTM/ipTM。&lt;h4&gt;结论&lt;/h4&gt;LSMTCR仅从表位输入就能输出多样化、基因上下文化的全长TCR设计，使高通量筛选和迭代优化成为可能。&lt;h4&gt;翻译&lt;/h4&gt;设计全长、表位特异性的T细胞受体(TCR)αβ由于序列空间庞大、数据偏差和免疫遗传约束建模不完整而具有挑战性。我们提出了LSMTCR，一个可扩展的多架构框架，它将特异性学习与约束学习分离，能够从头开始、基于表位条件生成配对的全长TCR。扩散增强的BERT编码器学习时间条件的表位表示；条件GPT解码器，在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性CDR3，具有温度控制的多样性；基因感知的Transformer通过预测V/J使用来组装完整的α/β序列，确保免疫遗传保真度。在GLIPH、TEP、MIRA、McPAS和我们策展的数据集上，LSMTCR在大多数数据集上的预测结合能力优于基线，更忠实地恢复了位置和长度语法，并提供了优越的、温度可调的多样性。对于α链生成，迁移学习提高了预测结合能力、长度真实性和多样性。从已知或从头开始的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低，并且在与表位的配对α/β共建模中，比单链设置获得更高的pTM/ipTM。LSMTCR仅从表位输入就能输出多样化、基因上下文化的全长TCR设计，使高通量筛选和迭代优化成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing full-length, epitope-specific TCR {\alpha}\b{eta} remainschallenging due to vast sequence space, data biases and incomplete modeling ofimmunogenetic constraints. We present LSMTCR, a scalable multi-architectureframework that separates specificity from constraint learning to enable denovo, epitope-conditioned generation of paired, full-length TCRs. Adiffusion-enhanced BERT encoder learns time-conditioned epitoperepresentations; conditional GPT decoders, pretrained on CDR3\b{eta} andtransferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modalconditioning with temperature-controlled diversity; and a gene-awareTransformer assembles complete {\alpha}/\b{eta} sequences by predicting V/Jusage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and ourcurated dataset, LSMTCR achieves higher predicted binding than baselines onmost datasets, more faithfully recovers positional and length grammars, anddelivers superior, temperature-tunable diversity. For {\alpha}-chaingeneration, transfer learning improves predicted binding, length realism anddiversity over representative methods. Full-length assembly from known or denovo CDR3s preserves k-mer spectra, yields low edit distances to references,and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higherpTM/ipTM than single-chain settings. LSMTCR outputs diverse,gene-contextualized, full-length TCR designs from epitope input alone, enablinghigh-throughput screening and iterative optimization.</description>
      <author>example@mail.com (Ruihao Zhang, Xiao Liu)</author>
      <guid isPermaLink="false">2509.07627v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data</title>
      <link>http://arxiv.org/abs/2509.07202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了将大型语言模型与脑电图解码相结合以增强辅助技术的潜力，特别是对有严重运动障碍人士的独立性和沟通能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的出现使文本生成能力发生了巨大变革，但基于脑电图的文本生成仍面临挑战，因为它需要大量数据和计算能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，减少EEG文本生成所需的数据和计算资源，同时保持接近最先进方法的性能。&lt;h4&gt;方法&lt;/h4&gt;结合使用Gemma 2B LLM和分类器-LLM架构，并引入循环神经网络(RNN)编码器。&lt;h4&gt;主要发现&lt;/h4&gt;新方法显著降低了所需的数据和计算能力，性能接近最先进方法，整体性能提高10%；所提出的架构展示了EEG文本生成有效迁移学习的可能性，即使在数据有限的情况下也能保持强大和功能性。&lt;h4&gt;结论&lt;/h4&gt;通过有效利用预训练语言模型的优势，该方法推动了当前能力的极限，为脑机接口的研究和应用开辟了新途径，使基于EEG的文本生成更加易于访问和高效。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型(LLMs)的引入，文本生成能力已经发生了巨大变革。然而，基于脑电图(EEG)的文本生成仍然困难，因为它需要大量数据和计算能力。本文介绍了一种新方法，它结合使用Gemma 2B LLM和分类器-LLM架构，并引入循环神经网络(RNN)编码器。我们的方法显著降低了所需的数据和计算能力，同时实现了接近最先进方法的性能。值得注意的是，与当前方法相比，我们的方法整体性能提高了10%。所提出的架构展示了EEG文本生成有效迁移学习的可能性，即使在数据有限的情况下也能保持强大和功能性。这项工作强调了将LLMs与EEG解码相结合以增强辅助技术的潜力，提高有严重运动限制人士的独立性和沟通能力。我们的方法通过有效利用预训练语言模型的优势，推动了当前能力的极限，为脑机接口的研究和应用开辟了新途径，使基于EEG的文本生成更加易于访问和高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text generating capabilities have undergone a substantial transformation withthe introduction of large language models (LLMs). Electroencephalography(EEG)-based text production is still difficult, though, because it requires alot of data and processing power. This paper introduces a new method thatcombines the use of the Gemma 2B LLM with a classifier-LLM architecture toincorporate a Recurrent Neural Network (RNN) encoder. Our approach drasticallylowers the amount of data and compute power needed while achieving performanceclose to that of cutting-edge methods. Notably, compared to currentmethodologies, our methodology delivers an overall performance improvement of10%. The suggested architecture demonstrates the possibility of effectivetransfer learning for EEG-based text production, remaining strong andfunctional even in the face of data limits. This work highlights the potentialof integrating LLMs with EEG decoding to improve assistive technologies andimprove independence and communication for those with severe motor limitations.Our method pushes the limits of present capabilities and opens new paths forresearch and application in brain-computer interfaces by efficiently using thestrengths of pre-trained language models. This makes EEG-based text productionmore accessible and efficient.</description>
      <author>example@mail.com (Khushiyant)</author>
      <guid isPermaLink="false">2509.07202v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Comparing unsupervised learning methods for local structural identification in colloidal systems</title>
      <link>http://arxiv.org/abs/2509.07186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 20 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了三种降维技术在自组装系统局部环境分类中的表现，发现UMAP在捕捉复杂结构特征方面表现最佳，为无监督结构分类提供了有效工具。&lt;h4&gt;背景&lt;/h4&gt;量化自组装系统中的局部结构是软物质和材料科学的核心挑战。当没有相关结构的先验知识可用时，传统的序参数往往不够用。&lt;h4&gt;目的&lt;/h4&gt;系统地比较三种流行的降维技术（主成分分析PCA、自编码器AE和统一流形近似和投影UMAP）用于分类自组装系统中的局部环境，探索无监督机器学习在自主发现结构基序方面的应用。&lt;h4&gt;方法&lt;/h4&gt;应用三种降维技术分别于硬球和带电球的流体和晶体构型，以及球形约束中自组装的二十面体球排列（包括模拟和实验数据）。&lt;h4&gt;主要发现&lt;/h4&gt;UMAP在捕捉复杂结构特征方面始终优于其他方法，为无监督结构分类提供了强大的工具。&lt;h4&gt;结论&lt;/h4&gt;无监督机器学习为从粒子构型中自主发现结构基序提供了便捷的途径，UMAP是一种鲁棒的工具，可用于自组装系统的结构分类，无需监督。&lt;h4&gt;翻译&lt;/h4&gt;量化自组装系统中的局部结构是软物质和材料科学的核心挑战。当没有相关结构的先验知识可用时，传统的序参数往往不够用。无监督机器学习为从粒子构型中自主发现结构基序提供了便捷的途径。在这项工作中，我们系统地比较了三种流行的降维技术：主成分分析（PCA）、自编码器（AE）和统一流形近似和投影（UMAP），用于分类自组装系统中的局部环境。我们首先将这些方法应用于硬球和带电球的流体和晶体构型。之后，我们将它们应用于球形约束中自组装的二十面体球排列，包括模拟和实验数据。我们证明UMAP在捕捉复杂结构特征方面始终优于其他方法，为无监督的结构分类提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantifying local structures in self-assembled systems is a central challengein soft matter and materials science. When no a priori knowledge of therelevant structures is available, traditional order parameters often fallshort. Unsupervised machine learning provides a convenient route toautonomously uncover structural motifs directly from particle configurations.In this work, we systematically compare three popular dimensionality reductiontechniques; Principal Component Analysis (PCA), Autoencoders (AE), and UniformManifold Approximation and Projection (UMAP), for classifying localenvironments in self-assembled systems. We first apply these methods to fluidand crystal configurations of hard and charged spheres. Thereafter, we apply itto an icosahedral arrangement of spheres that self-assembled in sphericalconfinement, both from simulations as well as from experiments. We demonstratethat UMAP consistently outperforms the other methods in capturing complexstructural features, offering a robust tool for structural classificationwithout supervision.</description>
      <author>example@mail.com (Alptuğ Ulugöl, Jessi Bückmann, Ruizhi Yang, Roy Hoitink, Alfons van Blaaderen, Frank Smallenburg, Laura Filion)</author>
      <guid isPermaLink="false">2509.07186v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Fourier Neural Operators for Time-Periodic Quantum Systems: Learning Floquet Hamiltonians, Observable Dynamics, and Operator Growth</title>
      <link>http://arxiv.org/abs/2509.07084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了傅里叶神经算子(FNO)作为非平衡量子动力学的有效替代模型，通过三种学习范式展示了其多功能性，包括重建Floquet哈密顿量、预测可观测量和学习量子信息传播，并证明了其准确性和计算效率优势。&lt;h4&gt;背景&lt;/h4&gt;时间周期性量子系统表现出丰富多样的非平衡现象，是量子工程和控制的理想平台。然而，由于希尔伯特空间维度指数增长和纠缠快速传播，传统数值方法难以模拟其动力学。&lt;h4&gt;目的&lt;/h4&gt;引入傅里叶神经算子(FNO)作为非平衡量子动力学的有效、准确和可扩展的替代模型，解决传统数值方法面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用在傅里叶空间参数化的神经算子，通过三种互补学习范式：重建有效Floquet哈密顿量、预测局部可观测量期望值和学习量子信息传播，评估FNO的性能。&lt;h4&gt;主要发现&lt;/h4&gt;FNO在每个学习任务上都取得显著准确性，同时实现与精确数值方法相比的显著加速；具有在不同时间离散化和系统驱动频率间迁移学习的能力；可在训练数据时间窗口外进行外推；计算成本随系统大小呈多项式缩放。&lt;h4&gt;结论&lt;/h4&gt;FNO是预测非平衡量子动力学的多功能和可扩展替代模型，在处理近期量子计算机数据方面具有潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;时间周期性量子系统表现出丰富多样的非平衡现象，并成为量子工程和控制的理想平台。然而，由于希尔伯特空间维度的指数增长和纠缠的快速传播，使用传统数值方法模拟它们的动力学仍然具有挑战性。在这项工作中，我们引入傅里叶神经算子(FNO)作为非平衡量子动力学的有效、准确和可扩展的替代模型。在傅里叶空间中参数化，FNO自然地捕捉时间相关性，并且对时间离散化的依赖性最小。我们通过三种互补的学习范式展示了FNO的多功能性：重建有效的Floquet哈密顿量、预测局部可观测量期望值和学习量子信息传播。对于每个学习任务，与精确数值方法相比，FNO都取得了显著的准确性，同时实现了显著的加速。此外，FNO还具有在不同时间离散化和系统驱动频率之间迁移学习的显著能力。我们还表明，FNO可以在训练数据提供的时间窗口之外进行外推，使原本难以获得的可观测量和算子传播动力学变得可访问。通过采用适当的局部基，我们认为FNO的计算成本仅随系统大小呈多项式缩放。我们的研究结果表明，FNO是预测非平衡量子动力学的多功能和可扩展的替代模型，在处理近期量子计算机数据方面具有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-periodic quantum systems exhibit a rich variety of far-from-equilibriumphenomena and serve as ideal platforms for quantum engineering and control.However, simulating their dynamics with conventional numerical methods remainschallenging due to the exponential growth of Hilbert space dimension and rapidspreading of entanglement. In this work, we introduce Fourier Neural Operators(FNO) as an efficient, accurate, and scalable surrogate for non-equilibriumquantum dynamics. Parameterized in Fourier space, FNO naturally capturestemporal correlations and remains minimally dependent on discretization oftime. We demonstrate the versatility of FNO through three complementarylearning paradigms: reconstructing effective Floquet Hamiltonians, predictingexpectation values of local observables, and learning quantum informationspreading. For each learning task, FNO achieves remarkable accuracy, whileattaining a significant speedup, compared to exact numerical methods. Moreover,FNO possesses a remarkable capacity to transfer learning across differenttemporal discretizations and system driving frequencies. We also show that FNOcan extrapolate beyond the time window provided by training data, enablingaccess to observables and operator-spreading dynamics that might otherwise bedifficult to obtain. By employing an appropriate local basis, we argue that thecomputational cost of FNOs scales only polynomially with the system size. Ourresults establish FNO as a versatile and scalable surrogate for predictingnon-equilibrium quantum dynamics, with potential applications to processingdata from near-term quantum computers.</description>
      <author>example@mail.com (Zihao Qi, Yang Peng, Christopher Earls)</author>
      <guid isPermaLink="false">2509.07084v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
      <link>http://arxiv.org/abs/2509.05657v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LM-Searcher框架，利用大语言模型进行跨领域神经架构优化，无需领域特定调优。核心是NCode表示方法和基于剪枝的子空间采样策略，使LLMs能从候选池中选择高性能架构。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在解决复杂优化问题(包括神经网络架构搜索)方面取得进展，但现有方法严重依赖提示工程和领域特定调优，限制了实用性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出一个无需大量领域特定适应的框架，利用LLMs进行跨领域神经架构优化。&lt;h4&gt;方法&lt;/h4&gt;开发NCode作为神经架构的通用数字字符串表示；将NAS问题重新表述为排序任务；使用基于剪枝的子空间采样策略训练LLMs；构建包含各种架构-性能对的数据集促进稳健学习。&lt;h4&gt;主要发现&lt;/h4&gt;LM-Searcher在领域内(如图像分类的CNNs)和领域外(如分割和生成的LoRA配置)任务中都能取得有竞争力的性能，建立了灵活且可推广的基于LLM的架构搜索新范式。&lt;h4&gt;结论&lt;/h4&gt;LM-Searcher为跨领域神经架构优化提供了有效且通用的方法，数据集和模型将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;近期大语言模型(LLMs)的进展为解决复杂优化问题开辟了新途径，包括神经网络架构搜索(NAS)。然而，现有的LLM驱动的NAS方法严重依赖提示工程和领域特定调优，限制了它们在不同任务上的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，一个新颖的框架，利用LLMs进行跨领域神经架构优化，无需大量领域特定的适应。我们方法的核心是NCode，一种神经架构的通用数字字符串表示，它实现了跨领域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLMs使用基于剪枝的子空间采样策略衍生的指令调整样本从候选池中选择高性能架构。我们策划的数据集包含各种架构-性能对，促进了稳健和可迁移的学习。全面的实验表明，LM-Searcher在领域内(如图像分类的CNNs)和领域外(如分割和生成的LoRA配置)任务中都取得了有竞争力的性能，建立了灵活且可推广的基于LLM的架构搜索新范式。数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Large Language Models (LLMs) has opened new avenues forsolving complex optimization problems, including Neural Architecture Search(NAS). However, existing LLM-driven NAS approaches rely heavily on promptengineering and domain-specific tuning, limiting their practicality andscalability across diverse tasks. In this work, we propose LM-Searcher, a novelframework that leverages LLMs for cross-domain neural architecture optimizationwithout the need for extensive domain-specific adaptation. Central to ourapproach is NCode, a universal numerical string representation for neuralarchitectures, which enables cross-domain architecture encoding and search. Wealso reformulate the NAS problem as a ranking task, training LLMs to selecthigh-performing architectures from candidate pools using instruction-tuningsamples derived from a novel pruning-based subspace sampling strategy. Ourcurated dataset, encompassing a wide range of architecture-performance pairs,encourages robust and transferable learning. Comprehensive experimentsdemonstrate that LM-Searcher achieves competitive performance in both in-domain(e.g., CNNs for image classification) and out-of-domain (e.g., LoRAconfigurations for segmentation and generation) tasks, establishing a newparadigm for flexible and generalizable LLM-based architecture search. Thedatasets and models will be released at https://github.com/Ashone3/LM-Searcher.</description>
      <author>example@mail.com (Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li)</author>
      <guid isPermaLink="false">2509.05657v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>OmniMap: A General Mapping Framework Integrating Optics, Geometry, and Semantics</title>
      <link>http://arxiv.org/abs/2509.07500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Robotics (TRO), project website:  https://omni-map.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniMap是一个创新的在线映射框架，首次同时捕捉光学、几何和语义场景属性，保持实时性能和模型紧凑性，在多个方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要准确和全面的3D环境感知，要求同时捕捉逼真的外观、精确的布局形状和开放词汇的场景理解。现有方法通常只能部分满足这些要求，并存在光学模糊、几何不规则和语义模糊等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D环境感知方法中的光学模糊、几何不规则和语义模糊问题，开发一个能够同时捕捉光学、几何和语义场景属性的框架，同时保持实时性能和模型紧凑性。&lt;h4&gt;方法&lt;/h4&gt;OmniMap采用紧密耦合的3DGS-Voxel混合表示方法，结合细粒度建模和结构稳定性。引入了几项创新：自适应相机建模（用于运动模糊和曝光补偿）、具有法线约束的混合增量表示，以及用于鲁棒实例级理解的概率融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的方法相比，OmniMap在渲染保真度、几何准确性和零样本语义分割方面具有优越性能。该框架的通用性通过各种下游应用得到进一步证明。&lt;h4&gt;结论&lt;/h4&gt;OmniMap代表了第一个能够同时捕捉光学、几何和语义场景属性的在线映射框架，在多个方面优于现有方法，并具有广泛的下游应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;机器人系统需要准确和全面的3D环境感知，要求同时捕捉逼真的外观（光学）、精确的布局形状（几何）和开放词汇的场景理解（语义）。现有方法通常只能部分满足这些要求，并存在光学模糊、几何不规则和语义模糊等问题。为应对这些挑战，我们提出了OmniMap。总体而言，OmniMap代表了第一个在线映射框架，能够同时捕捉光学、几何和语义场景属性，同时保持实时性能和模型紧凑性。在架构层面，OmniMap采用紧密耦合的3DGS-Voxel混合表示方法，结合细粒度建模和结构稳定性。在实现层面，OmniMap确定了不同模态的关键挑战，并引入了几项创新：用于运动模糊和曝光补偿的自适应相机建模、具有法线约束的混合增量表示，以及用于鲁棒实例级理解的概率融合。大量实验表明，与最先进的方法相比，OmniMap在渲染保真度、几何准确性和零样本语义分割方面具有优越性能。该框架的通用性通过各种下游应用得到进一步证明，包括多领域场景问答、交互式编辑、感知引导操作和地图辅助导航。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决机器人系统中3D环境感知的全面性问题，即如何同时实现高保真的光学渲染、精确的几何重建和开放词汇的语义理解。这个问题很重要，因为随着具身人工智能的发展，机器人需要高质量的多维环境表示来支持各种任务，如交互式虚拟操作（场景问答和编辑）和多粒度物理交互（桌面级操作和房间级导航），这对提升机器人在现实世界中的操作能力至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，提出了四个关键子问题：1)如何集成多样化场景属性；2)如何处理相机运动模糊和曝光不一致；3)如何提高几何稳定性；4)如何实现开放环境中的实例识别与融合。作者借鉴了多项现有工作：使用TSDF-Fusion作为体素映射基础，利用3DGS实现高保真渲染，采用YOLO-World进行开放词汇检测，使用TAP模型进行分割和标题生成，并通过SBERT进行嵌入编码。基于这些借鉴，作者创新性地设计了混合3DGS-体素表示框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合的3DGS-体素表示，将光学、几何和语义三种场景属性集成到一个统一的在线映射框架中，体素作为增量映射的基本单元封装概率语义，而新分配的体素则引导支持精确光学和几何重建的高斯原语初始化。整体流程包含三个主要模块：1)2D语言嵌入提取器，通过检测、分割、标题生成和编码获取实例级语义信息；2)概率体素重建器，执行3D域中的增量开放集实例融合；3)运动鲁棒的3DGS增量重建器，从新体素初始化高斯并在多种模态下进行渲染和监督。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的表示框架，首次连接光学、几何和语义；2)高保真光学处理，通过四个可微分参数建模相机模糊和曝光；3)详细几何重建，使用增量3DGS策略和法线监督；4)开放语义理解，设计高效实例理解管道；5)实现最先进的性能指标。相比之前工作，OmniMap突破了传统方法的局限：不同于传统语义体积映射的离散表示，不同于表面重建方法缺乏语义，不同于NeRF/3DGS方法缺乏语义理解，不同于开放词汇映射的稀疏表示，是首个将开放词汇理解直接集成到3DGS增量映射中的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniMap提出了首个集成光学、几何和语义的通用在线映射框架，通过创新的混合3DGS-体素表示和概率建模方法，实现了实时、高保真、高精度的3D环境感知，支持多样化的下游应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic systems demand accurate and comprehensive 3D environment perception,requiring simultaneous capture of photo-realistic appearance (optical), preciselayout shape (geometric), and open-vocabulary scene understanding (semantic).Existing methods typically achieve only partial fulfillment of theserequirements while exhibiting optical blurring, geometric irregularities, andsemantic ambiguities. To address these challenges, we propose OmniMap. Overall,OmniMap represents the first online mapping framework that simultaneouslycaptures optical, geometric, and semantic scene attributes while maintainingreal-time performance and model compactness. At the architectural level,OmniMap employs a tightly coupled 3DGS-Voxel hybrid representation thatcombines fine-grained modeling with structural stability. At the implementationlevel, OmniMap identifies key challenges across different modalities andintroduces several innovations: adaptive camera modeling for motion blur andexposure compensation, hybrid incremental representation with normalconstraints, and probabilistic fusion for robust instance-level understanding.Extensive experiments show OmniMap's superior performance in renderingfidelity, geometric accuracy, and zero-shot semantic segmentation compared tostate-of-the-art methods across diverse scenes. The framework's versatility isfurther evidenced through a variety of downstream applications, includingmulti-domain scene Q&amp;A, interactive editing, perception-guided manipulation,and map-assisted navigation.</description>
      <author>example@mail.com (Yinan Deng, Yufeng Yue, Jianyu Dou, Jingyu Zhao, Jiahui Wang, Yujie Tang, Yi Yang, Mengyin Fu)</author>
      <guid isPermaLink="false">2509.07500v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>CAViAR: Critic-Augmented Video Agentic Reasoning</title>
      <link>http://arxiv.org/abs/2509.07680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合大型语言模型代理和视频感知模块的方法来解决复杂视频推理任务，并引入评判机制来优化推理过程。&lt;h4&gt;背景&lt;/h4&gt;视频理解近年来取得显著进展，模型在短片段感知任务上性能不断提升，但在需要复杂推理的任务中，随着查询变得更复杂、视频变长，性能有所下降。多个基准测试如LVBench、Neptune和ActivityNet-RTL证实了这一现象。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以利用现有的视频感知能力来成功执行更复杂的视频推理任务。&lt;h4&gt;方法&lt;/h4&gt;开发了一个大型语言模型代理，可以访问视频模块作为子代理或工具。与之前遵循固定程序的方法不同，该代理使用每次调用模块的结果来确定后续步骤。同时引入评判机制来区分成功和不成功的推理序列。&lt;h4&gt;主要发现&lt;/h4&gt;代理和评判机制的组合在LVBench、Neptune和ActivityNet-RTL等数据集上取得了强大的性能表现。&lt;h4&gt;结论&lt;/h4&gt;通过将大型语言模型与视频感知模块结合，并引入评判机制，可以有效解决复杂的视频推理任务，突破了现有方法在复杂查询和长视频上的性能瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;视频理解近年来取得了显著进展，模型在短片段感知任务上的性能持续提升。然而，多个最近的基准测试，如LVBench、Neptune和ActivityNet-RTL表明，对于需要复杂推理的任务，随着查询变得更加复杂和视频变得更长，性能有所下降。在这项工作中，我们问：现有的感知能力是否可以被利用来成功执行更复杂的视频推理？特别是，我们开发了一个大型语言模型代理，可以访问视频模块作为子代理或工具。与之前的工作如Visual Programming、ViperGPT和MoReVQA遵循固定程序来解决查询不同，该代理使用每次调用模块的结果来确定后续步骤。受到文本推理领域工作的启发，我们引入了一个评判机制来区分代理成功和不成功的序列。我们展示了代理和评判机制的组合在上述提到的数据集上取得了强大的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding has seen significant progress in recent years, withmodels' performance on perception from short clips continuing to rise. Yet,multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, showperformance wanes for tasks requiring complex reasoning on videos as queriesgrow more complex and videos grow longer. In this work, we ask: can existingperception capabilities be leveraged to successfully perform more complex videoreasoning? In particular, we develop a large language model agent given accessto video modules as subagents or tools. Rather than following a fixed procedureto solve queries as in previous work such as Visual Programming, ViperGPT, andMoReVQA, the agent uses the results of each call to a module to determinesubsequent steps. Inspired by work in the textual reasoning domain, weintroduce a critic to distinguish between instances of successful andunsuccessful sequences from the agent. We show that the combination of ouragent and critic achieve strong performance on the previously-mentioneddatasets.</description>
      <author>example@mail.com (Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid)</author>
      <guid isPermaLink="false">2509.07680v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors</title>
      <link>http://arxiv.org/abs/2509.00969v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 8 figures, EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LangDC的语言感知动态标记压缩器，解决了大型视频语言模型处理高容量视觉标记时的效率问题，通过动态调整压缩比来适应不同视频片段的语义密度。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型的最新进展彻底改变了视频理解任务，但其效率受到处理大量视觉标记的限制。现有的标记压缩策略采用固定压缩比，忽略了不同视频片段间语义密度的变化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据视频片段语义密度动态调整压缩比的方法，以优化计算资源分配并提高模型效率。&lt;h4&gt;方法&lt;/h4&gt;LangDC利用轻量级语言模型描述视频片段，将其转换为软标题标记作为视觉表示，并通过语义密度感知监督进行训练，以覆盖关键视觉线索并基于场景丰富性动态调整压缩比。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与VideoGPT+相比，LangDC减少了49%的FLOPs同时保持竞争性性能，且能根据视频片段丰富性自适应调整标记压缩比。&lt;h4&gt;结论&lt;/h4&gt;LangDC通过模仿人类描述视觉内容的方式（复杂场景用更详细语言，简单场景用简洁语言），实现了高效的视频标记压缩，为视频语言模型提供了更高效的计算框架。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型的最新进展彻底改变了视频理解任务。然而，它们的效率受到处理大量视觉标记的显著限制。现有的标记压缩策略采用固定压缩比，忽略了不同视频片段间语义密度的变化。因此，这导致信息丰富的片段因标记不足而表示不充分，同时对静态或内容贫乏的片段进行不必要的计算。为解决此问题，我们提出了LangDC，一种语言感知的动态标记压缩器。LangDC利用轻量级语言模型描述视频片段，将其转换为软标题标记作为视觉表示。通过我们提出的语义密度感知监督进行训练，LangDC旨在1)覆盖下游任务推理所需的关键视觉线索；2)基于场景丰富性（由描述长度反映）动态调整压缩比。我们的设计模仿了人类如何动态表达他们所看到的内容：复杂场景（看到更多）引发更详细的语言来传达细微差别（说更多），而简单的场景用更少的词描述。实验结果表明，与VideoGPT+相比，我们的方法减少了49%的FLOPs，同时保持竞争性性能。此外，定性结果表明我们的方法根据视频片段丰富性自适应地调整标记压缩比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video-language models have revolutionized videounderstanding tasks. However, their efficiency is significantly constrained byprocessing high volumes of visual tokens. Existing token compression strategiesapply a fixed compression ratio, ignoring the variability in semantic densityamong different video clips. Consequently, this lead to inadequaterepresentation of information-rich clips due to insufficient tokens andunnecessary computation on static or content-poor ones. To address this, wepropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages alightweight language model to describe video clips, converting them into softcaption tokens as visual representations. Trained with our proposed semanticdensity-aware supervision, LangDC aims to 1) cover key visual cues necessaryfor downstream task reasoning and 2) dynamically adjust compression ratiosbased on scene richness, reflected by descriptions length. Our design mimicshow humans dynamically express what they see: complex scenes (seeing more)elicit more detailed language to convey nuances (saying more), whereas simplerscenes are described with fewer words. Experimental results show that ourmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitiveperformance. Furthermore, qualitative results demonstrate our approachadaptively adjusts the token compression ratio based on video segment richness.</description>
      <author>example@mail.com (Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.00969v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Interleaving Reasoning for Better Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2509.06945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了交错推理生成(IRG)框架，通过在文本思考和图像合成之间交替进行，提高文本到图像生成质量。作者还提出了交错推理生成学习(IRGL)方法和IRGL-300K数据集，通过两阶段训练实现了在多个评估指标上的显著提升。&lt;h4&gt;背景&lt;/h4&gt;统一多模态理解和生成模型在图像生成能力上取得了显著进步，但在遵循指令和保持细节方面与GPT-4o等系统相比仍有较大差距。&lt;h4&gt;目的&lt;/h4&gt;探索交错推理是否能进一步提高文本到图像(T2I)的生成质量，缩小与紧密耦合理解与生成的系统之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出交错推理生成(IRG)框架，模型先产生文本思考指导初始图像，再反思结果以细化细节和质量；提出交错推理生成学习(IRGL)方法和IRGL-300K数据集；采用两阶段训练：首先建立思考和反思能力，然后调整IRG管道。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验显示最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等指标上取得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;交错推理框架能有效提高文本到图像生成质量，特别是在细节保持和视觉质量方面。代码、模型权重和数据集将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近的统一多模态理解和生成模型在图像生成能力方面取得了显著进步，但在遵循指令和保持细节方面与紧密耦合理解与生成的系统(如GPT-4o)相比仍有较大差距。受最近交错推理进展的启发，我们探索了这种推理是否能进一步提高文本到图像(T2I)生成。我们引入了交错推理生成(IRG)，一个在基于文本的思考和图像合成之间交替的框架：模型首先产生基于文本的思考来指导初始图像，然后反思结果以细化细粒度细节、视觉质量和美学，同时保持语义。为了有效训练IRG，我们提出了交错推理生成学习(IRGL)，它针对两个子目标：(1)加强初始思考和生成阶段以建立核心内容和基础质量，以及(2)实现高质量的文本反思并在后续图像中忠实地实施这些改进。我们整理了IRGL-300K数据集，组织成六个分解的学习模式，共同覆盖基于文本的思考和完整的思考-图像轨迹学习。从一个原生发出交错文本-图像输出的统一基础模型开始，我们的两阶段训练首先建立强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效地调整IRG管道。大量实验显示了最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN上取得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也取得了显著改进。代码、模型权重和数据集将在以下地址发布：https://github.com/Osilly/Interleaving-Reasoning-Generation 。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal understanding and generation models recently have achievesignificant improvement in image generation capability, yet a large gap remainsin instruction following and detail preservation compared to systems thattightly couple comprehension with generation such as GPT-4o. Motivated byrecent advances in interleaving reasoning, we explore whether such reasoningcan further improve Text-to-Image (T2I) generation. We introduce InterleavingReasoning Generation (IRG), a framework that alternates between text-basedthinking and image synthesis: the model first produces a text-based thinking toguide an initial image, then reflects on the result to refine fine-graineddetails, visual quality, and aesthetics while preserving semantics. To trainIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),which targets two sub-goals: (1) strengthening the initial think-and-generatestage to establish core content and base quality, and (2) enabling high-qualitytextual reflection and faithful implementation of those refinements in asubsequent image. We curate IRGL-300K, a dataset organized into six decomposedlearning modes that jointly cover learning text-based thinking, and fullthinking-image trajectories. Starting from a unified foundation model thatnatively emits interleaved text-image outputs, our two-stage training firstbuilds robust thinking and reflection, then efficiently tunes the IRG pipelinein the full thinking-image trajectory data. Extensive experiments show SoTAperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual qualityand fine-grained fidelity. The code, model weights and datasets will bereleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description>
      <author>example@mail.com (Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin)</author>
      <guid isPermaLink="false">2509.06945v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
  <item>
      <title>FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</title>
      <link>http://arxiv.org/abs/2509.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FoMo4Wheat是一种针对小麦的视觉基础模型，使用大规模、多样化的小麦图像数据集进行自监督预训练，在各种农业视觉任务中表现优异，并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉驱动的田间监测是数字农业的核心，但基于通用域预训练骨干的模型难以在不同任务中泛化，这是由于精细变化的冠层结构与田间波动条件的相互作用导致的。&lt;h4&gt;目的&lt;/h4&gt;开发一种作物特定的视觉基础模型，解决通用域预训练模型在农业视觉任务中的泛化问题，提高田间监测的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出FoMo4Wheat模型，在ImAg4Wheat数据集上进行自监督预训练。该数据集包含250万张高分辨率图像，收集自30个全球站点，跨越十年，涵盖超过2000个基因型和500种环境条件。&lt;h4&gt;主要发现&lt;/h4&gt;小麦特定的预训练产生了稳健且可迁移到其他作物和杂草的表示。在十个田间视觉任务（冠层和器官层面）上，FoMo4Wheat模型持续优于最先进的通用域预训练模型。&lt;h4&gt;结论&lt;/h4&gt;作物特定的基础模型对可靠的田间感知具有重要价值，为开发具有跨物种和跨任务能力的通用作物基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;视觉驱动的田间监测是数字农业的核心，然而基于通用域预训练骨干构建的模型往往无法跨任务泛化，这是由于精细变化的冠层结构与田间波动条件的相互作用所致。我们提出了FoMo4Wheat，这是首批针对作物领域的视觉基础模型之一，使用自监督方法在ImAg4Wheat上进行预训练，这是迄今为止最大且最多样化的小麦图像数据集（包含250万张高分辨率图像，在十年间从30个全球站点收集，涵盖&gt;2000个基因型和&gt;500种环境条件）。这种小麦特定的预训练产生了对小麦稳健且可迁移到其他作物和杂草的表示。在冠层和器官层面的十个田间视觉任务中，FoMo4Wheat模型持续优于在通用域数据集上预训练的最先进模型。这些结果证明了作物特定基础模型对可靠田间感知的价值，并为具有跨物种和跨任务能力的通用作物基础模型铺平了道路。FoMo4Wheat模型和ImAg4Wheat数据集已在线公开：https://github.com/PheniX-Lab/FoMo4Wheat和https://huggingface.co/PheniX-Lab/FoMo4Wheat。演示网站为：https://fomo4wheat.phenix-lab.com/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-driven field monitoring is central to digital agriculture, yet modelsbuilt on general-domain pretrained backbones often fail to generalize acrosstasks, owing to the interaction of fine, variable canopy structures withfluctuating field conditions. We present FoMo4Wheat, one of the firstcrop-domain vision foundation model pretrained with self-supervision onImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5million high-resolution images collected over a decade at 30 global sites,spanning &gt;2,000 genotypes and &gt;500 environmental conditions). Thiswheat-specific pretraining yields representations that are robust for wheat andtransferable to other crops and weeds. Across ten in-field vision tasks atcanopy and organ levels, FoMo4Wheat models consistently outperformstate-of-the-art models pretrained on general-domain dataset. These resultsdemonstrate the value of crop-specific foundation models for reliable in-fieldperception and chart a path toward a universal crop foundation model withcross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheatdataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheatand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:https://fomo4wheat.phenix-lab.com/.</description>
      <author>example@mail.com (Bing Han, Chen Zhu, Dong Han, Rui Yu, Songliang Cao, Jianhui Wu, Scott Chapman, Zijian Wang, Bangyou Zheng, Wei Guo, Marie Weiss, Benoit de Solan, Andreas Hund, Lukas Roth, Kirchgessner Norbert, Andrea Visioni, Yufeng Ge, Wenjuan Li, Alexis Comar, Dong Jiang, Dejun Han, Fred Baret, Yanfeng Ding, Hao Lu, Shouyang Liu)</author>
      <guid isPermaLink="false">2509.06907v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis</title>
      <link>http://arxiv.org/abs/2509.06831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures; accepted at ML-CDS @ MICCAI 2025, Daejeon,  Republic of Korea&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何通过迁移学习适应通用基础模型以及整合手术室的互补模态来支持外科数据科学。使用V-JEPA作为多模态模型的基础，研究了在未标记手术视频数据上微调以及整合手术室其他时间解析数据流对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;外科数据科学需要有效利用手术数据，而通用基础模型和手术室的多种数据模态提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;研究单模态基础模型(V-JEPA)如何作为多模态模型的基础用于微创手术支持，以及迁移学习和多模态整合如何提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;使用V-JEPA作为基础模型，在内部肝脏手术视频数据集和公共HeiCo数据集上测试预测住院时间、术后并发症和手术阶段识别任务。通过在未标记视频上微调模型以及整合手术室的其他时间解析数据流来评估性能变化。&lt;h4&gt;主要发现&lt;/h4&gt;在领域特定数据上微调提高了模型性能；整合其他时间解析数据也有益于模型；预训练的视频单模态基线与最佳提交相当，领域特定微调进一步提高了准确性。&lt;h4&gt;结论&lt;/h4&gt;外科数据科学可以有效地利用公共通用基础模型；领域适应和整合手术室合适的互补数据流具有很大潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们研究如何通过迁移学习适应通用基础模型以及整合手术室(OR)的互补模态来支持外科数据科学。为此，我们使用V-JEPA作为多模态模型的基础，用于微创手术支持。我们分析了模型的下游性能如何受益于(a)在未标记的手术视频数据上进行微调，以及(b)在多模态设置中提供来自手术室的其他时间解析数据流。在内部肝脏手术视频数据集中，我们分析预测住院时间和术后并发症的任务。在公共HeiCo数据集的视频中，我们分析手术阶段识别任务。作为基线，我们将预训练的V-JEPA应用于所有任务。然后我们在未标记的保留视频上进行微调，以研究领域适应后的性能变化。遵循模块化决策支持网络的理念，我们通过训练单独的编码器整合来自手术室的其他数据流，与V-JEPA的嵌入形成共享表示空间。我们的实验表明，在领域特定数据上微调可以提高模型性能。在内部数据中，整合其他时间解析数据同样有利于模型。在HeiCo数据上，仅使用预训练视频的单模态基线设置与EndoVis2017挑战的最佳提交相当，而在领域特定数据上微调可以进一步提高准确性。我们的结果因此证明了外科数据科学如何可以利用公共通用基础模型。同样，它们表明了领域适应和整合来自手术室合适的互补数据流的潜力。为支持进一步研究，我们在https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025发布了我们的代码和模型权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate how both the adaptation of a generic foundation model viatransfer learning and the integration of complementary modalities from theoperating room (OR) can support surgical data science. To this end, we useV-JEPA as the single-modality foundation of a multimodal model for minimallyinvasive surgery support. We analyze how the model's downstream performance canbenefit (a) from finetuning on unlabeled surgical video data and (b) fromproviding additional time-resolved data streams from the OR in a multimodalsetup.  In an in-house dataset of liver surgery videos, we analyze the tasks ofpredicting hospital length of stay and postoperative complications. In videosof the public HeiCo dataset, we analyze the task of surgical phase recognition.As a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it onunlabeled, held-out videos to investigate its change in performance afterdomain adaptation. Following the idea of modular decision support networks, weintegrate additional data streams from the OR by training a separate encoder toform a shared representation space with V-JEPA's embeddings.  Our experiments show that finetuning on domain-specific data increases modelperformance. On the in-house data, integrating additional time-resolved datalikewise benefits the model. On the HeiCo data, accuracy of the pretrainedvideo-only, single-modality baseline setup is on par with the top-performingsubmissions of the EndoVis2017 challenge, while finetuning on domain-specificdata increases accuracy further. Our results thus demonstrate how surgical datascience can leverage public, generic foundation models. Likewise, they indicatethe potential of domain adaptation and of integrating suitable complementarydata streams from the OR. To support further research, we release our code andmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.</description>
      <author>example@mail.com (Simon Pezold, Jérôme A. Kurylec, Jan S. Liechti, Beat P. Müller, Joël L. Lavanchy)</author>
      <guid isPermaLink="false">2509.06831v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Curia: A Multi-Modal Foundation Model for Radiology</title>
      <link>http://arxiv.org/abs/2509.06830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了Curia，一个在大型医院多年横断面影像数据上训练的基础模型，用于AI辅助放射学解释，该模型在多模态和低数据设置下展现出临床意义的重要涌现特性。&lt;h4&gt;背景&lt;/h4&gt;当前的AI辅助放射学解释主要基于狭窄的、单任务模型，这种方法难以覆盖广泛的成像方式、疾病和放射学发现。基础模型(FMs)有望在多种模态和低数据设置下实现广泛泛化，但这一潜力在放射学领域尚未实现。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在多种成像模态和低数据设置下有效工作的基础模型，以克服当前单任务AI模型的局限性，并实现更广泛的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;研究团队训练了一个名为Curia的基础模型，该模型在一个主要医院多年的横断面影像输出上进行训练，包含150,000次检查(130 TB)。他们在一个新创建的19项任务外部验证基准上评估了该模型。&lt;h4&gt;主要发现&lt;/h4&gt;Curia能够准确识别器官，检测脑出血和心肌梗死等疾病，在肿瘤分期中预测结果，其表现达到或超过了放射科医生和最近的基础模型，并且在跨模态和低数据设置下展现出临床意义的重要涌现特性。&lt;h4&gt;结论&lt;/h4&gt;Curia代表了放射学基础模型的重要进展，通过在大型真实世界数据集上训练，实现了在多种任务和条件下的高性能。研究团队发布了基础模型的权重，以加速该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;AI-assisted radiological interpretation：AI辅助放射学解释；Foundation models：基础模型；Curia：Curia（模型名称）；cross-sectional imaging：横断面影像；brain hemorrhages：脑出血；myocardial infarctions：心肌梗死；tumor staging：肿瘤分期&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决放射学AI领域过度依赖狭窄单任务模型的问题。现实中，这种'一个任务一个模型'的方法资源密集且难以覆盖广泛的成像方式、疾病和放射学发现，成为AI模型整合到临床工作流程的瓶颈。基础模型(FMs)有潜力实现跨模态和低数据设置下的广泛泛化，但这一潜力在放射学领域尚未实现。放射学作为医学诊断的核心，AI辅助有潜力提高诊断效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前放射学AI的局限性，认识到基础模型在自然图像领域的成功(如DINOv2和MAE)可迁移到医学影像。他们借鉴了Vision Transformer架构和DINOv2自监督学习算法，同时参考了现有放射学基础模型如BiomedCLIP和MedImageInsight。设计上，他们专注于使用大规模真实世界临床数据而非混合专业数据，并创建了全面的评估基准。在下游任务适配上，他们借鉴了SAM和RadSAM的提示分割框架，但进行了医学领域特定的改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模未标记医学影像数据进行自监督学习，学习通用的视觉特征表示，创建能适应多种下游任务的基础模型，实现跨模态泛化。流程包括：1)收集并预处理一家医院15万次检查的130TB真实世界数据；2)使用Vision Transformer架构和DINOv2算法在2亿图像上预训练；3)为不同下游任务设计适配方法(图像级、对象级、体积级预测等)；4)在包含19个任务的CuriaBench基准上全面评估，并与放射科医师和其他模型比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用最大规模真实世界医学影像语料库(2亿图像，130TB)；2)实现卓越的跨模态泛化能力(CT到MRI性能下降仅9.17%)；3)在少样本学习中表现优异；4)创建全面的CuriaBench基准(19个任务)；5)性能达到或超过放射科医师水平；6)开源模型权重。相比之前工作，Curia使用更大规模且来源单一的放射学数据(而非多领域混合数据)，采用DINOv2自监督方法，在跨模态任务和少样本学习中表现显著更优，评估更全面且包含与放射科医师的比较。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Curia通过在超大规模真实世界医学影像数据上的自监督学习，实现了跨模态和低数据设置下的强大泛化能力，在广泛放射学任务中达到或超过放射科医师性能，为放射学AI提供了新的基础模型范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-assisted radiological interpretation is based on predominantly narrow,single-task models. This approach is impractical for covering the vast spectrumof imaging modalities, diseases, and radiological findings. Foundation models(FMs) hold the promise of broad generalization across modalities and inlow-data settings. However, this potential has remained largely unrealized inradiology. We introduce Curia, a foundation model trained on the entirecross-sectional imaging output of a major hospital over several years, which toour knowledge is the largest such corpus of real-world data-encompassing150,000 exams (130 TB). On a newly curated 19-task external validationbenchmark, Curia accurately identifies organs, detects conditions like brainhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.Curia meets or surpasses the performance of radiologists and recent foundationmodels, and exhibits clinically significant emergent properties incross-modality, and low-data regimes. To accelerate progress, we release ourbase model's weights at https://huggingface.co/raidium/curia.</description>
      <author>example@mail.com (Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, Théo Danielou, Léo Alberge, Léo Machado, Daniel Tordjman, Julie Dupuis, Korentin Le Floch, Jean Du Terrail, Mariam Moshiri, Laurent Dercle, Tom Boeken, Jules Gregory, Maxime Ronot, François Legou, Pascal Roux, Marc Sapoval, Pierre Manceron, Paul Hérent)</author>
      <guid isPermaLink="false">2509.06830v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
      <link>http://arxiv.org/abs/2509.06685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架，通过结合视觉惯性运动恢复结构的精确稀疏深度和大型基础模型的密集粗糙深度，实现了高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统高斯溅射技术需要精确的深度信息来初始化高斯椭球体，通常依赖RGB-D/立体相机，但这些传感器的有限范围使其难以在大场景中应用。单目图像虽然易于获取，但缺乏深度信息指导学习，导致新视角合成结果质量较差。现有的单目深度估计大型基础模型存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索模糊不清等问题。&lt;h4&gt;目的&lt;/h4&gt;从单目RGB输入生成密集、精确的深度图像，用于高确定性的高斯溅射渲染，以解决大场景中的新视角合成问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种结合视觉惯性运动恢复结构(SfM)和大型基础模型(LFMs)的深度生成方法。具体包括：1)利用SfM提供的精确但稀疏的深度信息来 refine LFM提供的密集但粗糙的深度信息；2)提出对象分割的深度传播算法，连接稀疏输入和密集输出；3)开发动态深度细化模块，处理动态对象的受损SfM深度，并细化粗糙的LFM深度。&lt;h4&gt;主要发现&lt;/h4&gt;通过公共和定制数据集的实验证明，VIM-GS在大场景的新视角合成中具有优越的渲染质量，能够有效解决传统方法在大场景中面临的挑战。&lt;h4&gt;结论&lt;/h4&gt;VIM-GS框架成功结合了视觉惯性运动恢复结构和大型基础模型的优势，通过创新的深度传播和细化算法，实现了大场景中高质量的新视角合成，为单目图像驱动的三维场景重建提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架。高斯溅射通常需要精确的深度信息来使用RGB-D/立体相机初始化高斯椭球体。它们的有限深度传感范围使得高斯溅射难以在大场景中工作。然而，单目图像缺乏引导学习的深度信息，导致新视角合成结果质量较差。尽管有可用于单目深度估计的大型基础模型，但它们存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索模糊不清等问题。本文旨在从单目RGB输入生成密集、精确的深度图像，用于高确定性的高斯溅射渲染。关键思想是利用视觉惯性运动恢复结构提供的精确但稀疏的深度信息来 refine 大型基础模型提供的密集但粗糙的深度信息。为了连接稀疏输入和密集输出，我们提出了一种对象分割的深度传播算法，用于渲染结构化对象像素的深度。然后我们开发了一个动态深度细化模块，用于处理动态对象的受损SfM深度，并细化粗糙的LFM深度。使用公共和定制数据集进行的实验证明了VIM-GS在大场景中的优越渲染质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在大场景下使用单目图像进行高质量新视角合成的问题。传统高斯溅射方法需要RGB-D或立体相机提供深度信息，但这些设备深度感知范围有限，难以应用于大场景。单目图像虽然成本低，但缺乏深度信息导致渲染质量差。这个问题很重要，因为高成本激光雷达限制了GS技术在消费级应用中的普及，而低成本单目相机实现大场景高质量渲染对自动驾驶、虚拟现实等领域有重要价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：RGB-D/立体相机深度范围有限，单目SfM只提供稀疏深度，而大规模基础模型深度估计存在跨帧不一致和远处不准确等问题。设计上借鉴了视觉惯性SLAM的准确稀疏深度和大规模基础模型的密集粗糙深度，结合分割模型进行物体识别。整体思路是优势互补，将不同方法的优点结合，弥补各自的不足，通过物体分割深度传播和动态深度细化两个模块实现深度信息的优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉惯性SfM提供的准确但稀疏的深度信息来 refine大规模基础模型提供的密集但粗糙的深度信息，生成密集准确的深度图像。整体流程：1)输入RGB和IMU数据，通过VI前端获得稀疏特征3D坐标，并用分割模型生成物体掩码；2)物体分割深度传播模块识别结构化物体，将稀疏深度传播到物体所有像素；3)动态深度细化模块处理动态物体错误深度，通过新损失函数细化LFMs深度；4)输出密集准确深度图像，为高斯溅射提供良好初始化，实现大场景高质量渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)物体分割深度传播算法，识别结构化物体并将稀疏深度传播到所有像素；2)动态深度细化模块，处理动态物体错误深度并细化LFMs深度；3)结合视觉惯性SfM和大规模基础模型优势。相比之前工作：传统GS依赖RGB-D/立体相机，深度范围有限；单目支持方法因随机或稀疏深度初始化，大场景效果差；SfM只提供稀疏深度；LFMs存在跨帧不一致和远处不准确问题。VIM-GS首次有效结合两种深度信息，通过物体分割和动态处理解决了各自局限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VIM-GS通过结合视觉惯性SfM的准确稀疏深度和大规模基础模型的密集粗糙深度，并引入物体分割和动态处理机制，实现了在大场景下使用单目相机进行高质量实时新视角合成的突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; VIM-GS is a Gaussian Splatting (GS) framework using monocular images fornovel-view synthesis (NVS) in large scenes. GS typically requires accuratedepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limiteddepth sensing range makes it difficult for GS to work in large scenes.Monocular images, however, lack depth to guide the learning and lead toinferior NVS results. Although large foundation models (LFMs) for monoculardepth estimation are available, they suffer from cross-frame inconsistency,inaccuracy for distant scenes, and ambiguity in deceptive texture cues. Thispaper aims to generate dense, accurate depth images from monocular RGB inputsfor high-definite GS rendering. The key idea is to leverage the accurate butsparse depth from visual-inertial Structure-from-Motion (SfM) to refine thedense but coarse depth from LFMs. To bridge the sparse input and dense output,we propose an object-segmented depth propagation algorithm that renders thedepth of pixels of structured objects. Then we develop a dynamic depthrefinement module to handle the crippled SfM depth of dynamic objects andrefine the coarse LFM depth. Experiments using public and customized datasetsdemonstrate the superior rendering quality of VIM-GS in large scenes.</description>
      <author>example@mail.com (Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu)</author>
      <guid isPermaLink="false">2509.06685v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2509.06617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为MM-DINOv2的新框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像分析。该方法通过多模态块嵌入和全模态掩码技术解决了多模态医学影像分析中的挑战，并利用半监督学习提高预测准确性和可靠性。在胶质瘤亚型分类任务中，该方法取得了优于现有监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型如DINOv2在医学影像领域展现出巨大潜力，尽管它们最初是为自然图像设计的。然而，这些模型的设计主要用于单模态图像分析，限制了它们在神经学、肿瘤学等医学领域中常见的多模态成像任务中的有效性。虽然监督模型在这种情况下表现良好，但它们无法利用未标记的数据集，并且在处理缺失模态方面存在困难，这是临床环境中常见的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理多模态医学影像数据的框架，解决现有监督模型无法利用未标记数据和处理缺失模态的问题，从而提高医学预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了MM-DINOv2框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像。该方法包括：1) 使用多模态块嵌入，使视觉基础模型能够有效处理多模态影像数据；2) 采用全模态掩码技术，鼓励模型学习稳健的跨模态关系，以解决缺失模态的问题；3) 利用半监督学习来利用大型未标记数据集，提高医学预测的准确性和可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;在胶质瘤亚型分类任务中（基于多序列脑部MRI），该方法在外部测试集上获得了0.6的马修斯相关系数(MCC)，比最先进的监督方法高出11.1%。&lt;h4&gt;结论&lt;/h4&gt;该研究建立了一种可扩展且稳健的多模态医学影像任务解决方案，它利用了在自然图像上预训练的强大视觉基础模型，同时解决了缺失数据和有限标注等现实临床挑战。&lt;h4&gt;翻译&lt;/h4&gt;DINOv2等视觉基础模型尽管源自自然图像领域，但在医学影像中展现出巨大潜力。然而，其设计本质上是针对单模态图像分析优化的，限制了它们在神经学和肿瘤学等许多医学领域常见的多模态成像任务中的有效性。虽然监督模型在这种情况下表现良好，但它们无法利用未标记的数据集，并且在处理缺失模态方面存在困难，这是临床环境中常见的挑战。为了弥补这些差距，我们引入了MM-DINOv2，这是一种新颖且高效的框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像。我们的方法结合了多模态块嵌入，使视觉基础模型能够有效处理多模态影像数据。为解决缺失模态问题，我们采用全模态掩码，这鼓励模型学习稳健的跨模态关系。此外，我们利用半监督学习来利用大型未标记数据集，提高医学预测的准确性和可靠性。在应用于基于多序列脑部MRI的胶质瘤亚型分类时，我们的方法在外部测试集上获得了0.6的马修斯相关系数(MCC)，比最先进的监督方法高出11.1%。我们的工作为多模态医学影像任务建立了可扩展且稳健的解决方案，利用了在自然图像上预训练的强大视觉基础模型，同时解决了缺失数据和有限标注等现实临床挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models like DINOv2 demonstrate remarkable potential inmedical imaging despite their origin in natural image domains. However, theirdesign inherently works best for uni-modal image analysis, limiting theireffectiveness for multi-modal imaging tasks that are common in many medicalfields, such as neurology and oncology. While supervised models perform well inthis setting, they fail to leverage unlabeled datasets and struggle withmissing modalities, a frequent challenge in clinical settings. To bridge thesegaps, we introduce MM-DINOv2, a novel and efficient framework that adapts thepre-trained vision foundation model DINOv2 for multi-modal medical imaging. Ourapproach incorporates multi-modal patch embeddings, enabling vision foundationmodels to effectively process multi-modal imaging data. To address missingmodalities, we employ full-modality masking, which encourages the model tolearn robust cross-modality relationships. Furthermore, we leveragesemi-supervised learning to harness large unlabeled datasets, enhancing boththe accuracy and reliability of medical predictions. Applied to glioma subtypeclassification from multi-sequence brain MRI, our method achieves a MatthewsCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassingstate-of-the-art supervised approaches by +11.1%. Our work establishes ascalable and robust solution for multi-modal medical imaging tasks, leveragingpowerful vision foundation models pre-trained on natural images whileaddressing real-world clinical challenges such as missing data and limitedannotations.</description>
      <author>example@mail.com (Daniel Scholz, Ayhan Can Erdur, Viktoria Ehm, Anke Meyer-Baese, Jan C. Peeken, Daniel Rueckert, Benedikt Wiestler)</author>
      <guid isPermaLink="false">2509.06617v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking EfficientTAM on FMO datasets</title>
      <link>http://arxiv.org/abs/2509.06536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文解决了快速和小型物体跟踪的挑战，通过引入JSON元数据文件和扩展数据集描述，并展示了EfficientTAM模型在这些数据集上的良好性能。&lt;h4&gt;背景&lt;/h4&gt;快速和小型物体跟踪在计算机视觉领域仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;作者旨在引入与四个开源的快速移动物体(FMOs)图像序列相关的JSON元数据文件，并扩展这些数据集的描述。&lt;h4&gt;方法&lt;/h4&gt;引入一个JSON元数据文件与四个开源的快速移动物体图像序列相关联；扩展FMOs数据集的描述，添加以JSON格式(称为FMOX)的额外真实信息，包括物体大小信息；使用FMOX文件测试最近提出的基础跟踪模型EfficientTAM。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientTAM模型在FMOX数据集上的性能与专门为这些FMO数据集设计的管道相比表现良好，使用轨迹交并比(TIoU)分数进行评估。&lt;h4&gt;结论&lt;/h4&gt;代码和JSON文件已开源共享，使FMOX能够被其他处理FMO数据集的机器学习管道访问和使用，促进了该领域的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;快速且小型物体的跟踪在计算机视觉中仍然是一个挑战。在本文中，我们首先介绍了与四个开源的快速移动物体(FMOs)图像序列相关的JSON元数据文件。此外，我们以JSON格式(称为FMOX)扩展了FMOs数据集的描述，添加了包括物体大小信息的额外真实信息。最后，我们使用FMOX文件测试了最近提出的基础跟踪模型(称为EfficientTAM)，显示其性能与专门为这些FMO数据集设计的管道相比表现良好。我们在FMOX上使用轨迹交并比(TIoU)分数提供了这些最先进技术的比较。代码和JSON已开源共享，使FMOX能够被其他旨在处理FMO数据集的机器学习管道访问和使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and tiny object tracking remains a challenge in computer vision and inthis paper we first introduce a JSON metadata file associated with four opensource datasets of Fast Moving Objects (FMOs) image sequences. In addition, weextend the description of the FMOs datasets with additional ground truthinformation in JSON format (called FMOX) with object size information. Finallywe use our FMOX file to test a recently proposed foundational model fortracking (called EfficientTAM) showing that its performance compares well withthe pipelines originally taylored for these FMO datasets. Our comparison ofthese state-of-the-art techniques on FMOX is provided with TrajectoryIntersection of Union (TIoU) scores. The code and JSON is shared open sourceallowing FMOX to be accessible and usable for other machine learning pipelinesaiming to process FMO datasets.</description>
      <author>example@mail.com (Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot)</author>
      <guid isPermaLink="false">2509.06536v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</title>
      <link>http://arxiv.org/abs/2509.06516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为QualityFM的新型多模态基础模型，用于评估和处理光电容积脉搏波(PPG)和心电图(ECG)信号的质量问题，解决了现有方法在泛化能力、数据依赖性和跨任务迁移性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;光电容积脉搏波(PPG)和心电图(ECG)信号常在重症监护室和手术室中记录，但这些信号经常出现质量差、不完整和不一致的情况，可能导致误报或诊断不准确。现有方法存在泛化能力有限、依赖大量标记数据、跨任务迁移性差等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够获取生理信号质量通用理解的新型多模态基础模型，克服现有方法的局限性，提高信号质量评估的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建双轨架构处理不同质量的成对生理信号；2) 采用自蒸馏策略，让高质量信号编码器指导低质量信号编码器训练；3) 在基于Transformer的模型中集成窗口稀疏注意力机制处理长序列信号；4) 设计复合损失函数结合直接蒸馏损失和间接重建损失；5) 在大规模数据集上预训练三个参数数量不同的模型(9.6M至319M)。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的三个模型在三个临床任务上表现出有效性和实用价值，包括：室性心动过速检测的误报减少、心房颤动的准确识别以及从PPG和ECG信号精确估算动脉血压。&lt;h4&gt;结论&lt;/h4&gt;QualityFM模型通过大规模预训练和创新的架构设计，有效解决了生理信号质量评估的挑战，为临床监护提供了更可靠的信号质量评估工具，有助于减少误报和提高诊断准确性。&lt;h4&gt;翻译&lt;/h4&gt;光电容积脉搏波(PPG)和心电图(ECG)通常在重症监护室(ICU)和手术室(OR)中记录。然而，信号质量差、不完整和不一致的高发生率，可能导致误报或诊断不准确。迄今为止探索的方法存在泛化能力有限、依赖大量标记数据和跨任务迁移性差等问题。为了克服这些挑战，我们引入了QualityFM，这是一种用于这些生理信号的新型多模态基础模型，旨在获取对信号质量的通用理解。我们的模型在一个包含超过2100万个30秒波形和179,757小时数据的大规模数据集上进行预训练。我们的方法涉及一个双轨架构，处理不同质量的成对生理信号，利用自蒸馏策略，其中高质量信号编码器用于指导低质量信号编码器的训练。为了有效处理长序列信号并捕获基本局部准周期性模式，我们在基于Transformer的模型中集成了窗口稀疏注意力机制。此外，结合编码器输出的直接蒸馏损失和基于功率谱及相位谱的间接重建损失的复合损失函数，确保了信号频域特征的保留。我们预训练了三个参数数量不同的模型(9.6M至319M)，并通过在三个不同的临床任务上的迁移学习证明了它们的有效性和实用价值：室性心动过速检测的误报、心房颤动的识别以及从PPG和ECG信号估算动脉血压(ABP)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded inintesive care unit (ICU) and operating room (OR). However, the high incidenceof poor, incomplete, and inconsistent signal quality, can lead to false alarmsor diagnostic inaccuracies. The methods explored so far suffer from limitedgeneralizability, reliance on extensive labeled data, and poor cross-tasktransferability. To overcome these challenges, we introduce QualityFM, a novelmultimodal foundation model for these physiological signals, designed toacquire a general-purpose understanding of signal quality. Our model ispre-trained on an large-scale dataset comprising over 21 million 30-secondwaveforms and 179,757 hours of data. Our approach involves a dual-trackarchitecture that processes paired physiological signals of differing quality,leveraging a self-distillation strategy where an encoder for high-qualitysignals is used to guide the training of an encoder for low-quality signals. Toefficiently handle long sequential signals and capture essential localquasi-periodic patterns, we integrate a windowed sparse attention mechanismwithin our Transformer-based model. Furthermore, a composite loss function,which combines direct distillation loss on encoder outputs with indirectreconstruction loss based on power and phase spectra, ensures the preservationof frequency-domain characteristics of the signals. We pre-train three modelswith varying parameter counts (9.6 M to 319 M) and demonstrate their efficacyand practical value through transfer learning on three distinct clinical tasks:false alarm of ventricular tachycardia detection, the identification of atrialfibrillation and the estimation of arterial blood pressure (ABP) from PPG andECG signals.</description>
      <author>example@mail.com (Zongheng Guo, Tao Chen, Manuela Ferrario)</author>
      <guid isPermaLink="false">2509.06516v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Does DINOv3 Set a New Medical Vision Standard?</title>
      <link>http://arxiv.org/abs/2509.06467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了DINOv3视觉基础模型在医学视觉任务上的表现，发现尽管仅用自然图像训练，它仍能成为医学任务的强大基线，甚至在某些任务上优于医学特定模型，但在高度专业化的医学领域表现有限。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉基础模型在自然图像上的预训练已改变了计算机视觉领域，但这些前沿模型在专业领域（如医学影像）的效能转移仍是一个开放性问题。&lt;h4&gt;目的&lt;/h4&gt;调查DINOv3（一种先进的自监督视觉Transformer）是否可以直接作为医学视觉任务的统一编码器，无需领域特定的预训练。&lt;h4&gt;方法&lt;/h4&gt;在多种医学成像模态的2D/3D分类和分割任务上对DINOv3进行基准测试，并通过改变模型大小和输入图像分辨率来分析其可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv3在医学视觉任务上表现出色，建立了强大的新基线，在某些任务上甚至超过医学特定模型；但在需要深度领域专业化的场景（如病理图像、电子显微镜和PET扫描）中性能下降；在医学领域不始终遵循缩放定律，性能不会随模型大小或分辨率增加而可靠提高。&lt;h4&gt;结论&lt;/h4&gt;DINOv3可作为医学视觉任务的强大基线模型，其视觉特征可作为复杂医学任务的稳健先验，为未来研究（如3D重建中的多视图一致性）开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉基础模型的出现，这些模型在各种自然图像上进行了预训练，标志着计算机视觉的范式转变。然而，前沿视觉基础模型的效能如何转移到专业领域（如医学影像）仍然是一个悬而未决的问题。本报告研究了DINOv3（一种在密集预测任务中具有强大能力的最先进自监督视觉Transformer）是否可以直接作为医学视觉任务的强大统一编码器，无需领域特定的预训练。为了回答这个问题，我们在常见的医学视觉任务上对DINOv3进行了基准测试，包括在多种医学成像模态上的2D/3D分类和分割。我们通过改变模型大小和输入图像分辨率，系统地分析了其可扩展性。我们的研究结果显示，DINOv3表现出令人印象深刻的性能，并建立了一个强大的新基线。值得注意的是，尽管仅在自然图像上训练，它在几个任务上甚至可以超越医学特定的基础模型，如BiomedCLIP和CT-Net。然而，我们确定了明显的局限性：在需要深度领域专业化的场景中，如全载病理图像(WSIs)、电子显微镜(EM)和正电子发射断层扫描(PET)，模型的特征会退化。此外，我们观察到DINOv3在医学领域并不始终遵循缩放定律；性能不会随着更大模型或更精细的特征分辨率而可靠地增加，在不同任务上表现出多样的缩放行为。最终，我们的工作将DINOv3确立为一个强大的基线模型，其强大的视觉特征可以作为多种复杂医学任务的稳健先验。这为未来开辟了有希望的方向，例如利用其特征在3D重建中强制多视图一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of large-scale vision foundation models, pre-trained on diversenatural images, has marked a paradigm shift in computer vision. However, howthe frontier vision foundation models' efficacies transfer to specializeddomains remains such as medical imaging remains an open question. This reportinvestigates whether DINOv3, a state-of-the-art self-supervised visiontransformer (ViT) that features strong capability in dense prediction tasks,can directly serve as a powerful, unified encoder for medical vision taskswithout domain-specific pre-training. To answer this, we benchmark DINOv3across common medical vision tasks, including 2D/3D classification andsegmentation on a wide range of medical imaging modalities. We systematicallyanalyze its scalability by varying model sizes and input image resolutions. Ourfindings reveal that DINOv3 shows impressive performance and establishes aformidable new baseline. Remarkably, it can even outperform medical-specificfoundation models like BiomedCLIP and CT-Net on several tasks, despite beingtrained solely on natural images. However, we identify clear limitations: Themodel's features degrade in scenarios requiring deep domain specialization,such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3does not consistently obey scaling law in the medical domain; performance doesnot reliably increase with larger models or finer feature resolutions, showingdiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3as a strong baseline, whose powerful visual features can serve as a robustprior for multiple complex medical tasks. This opens promising futuredirections, such as leveraging its features to enforce multiview consistency in3D reconstruction.</description>
      <author>example@mail.com (Che Liu, Yinda Chen, Haoyuan Shi, Jinpeng Lu, Bailiang Jian, Jiazhen Pan, Linghan Cai, Jiayi Wang, Yundi Zhang, Jun Li, Cosmin I. Bercea, Cheng Ouyang, Chen Chen, Zhiwei Xiong, Benedikt Wiestler, Christian Wachinger, Daniel Rueckert, Wenjia Bai, Rossella Arcucci)</author>
      <guid isPermaLink="false">2509.06467v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics</title>
      <link>http://arxiv.org/abs/2509.06322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了文本训练的基础模型能够在不微调或自然语言提示的情况下，从离散的偏微分方程解中准确推断时空动态，并揭示了预测质量与上下文长度和输出长度之间的可预测关系。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已在各种任务中表现出上下文学习能力，包括零样本时间序列预测。&lt;h4&gt;目的&lt;/h4&gt;探究文本训练的基础模型是否可以在无需额外训练的情况下，从偏微分方程的离散解中准确推断时空动态。&lt;h4&gt;方法&lt;/h4&gt;分析标记级输出分布，研究大型语言模型如何内部处理偏微分方程解以实现准确预测。&lt;h4&gt;主要发现&lt;/h4&gt;预测准确性随时间上下文长度增加而提高，但在更精细的空间离散化时会下降；多步预测中误差随时间范围代数增长，类似于经典有限差分解算子的全局误差累积；这些趋势符合上下文神经缩放定律。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型通过一致的上下文学习进展处理偏微分方程解：从语法模式模仿开始，过渡到探索性高熵阶段，最终以自信的、基于数字的预测结束。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已在一系列任务中表现出上下文学习能力，包括零样本时间序列预测。我们证明，文本训练的基础模型可以在不微调或自然语言提示的情况下，从离散的偏微分方程解中准确推断时空动态。预测准确性随时间上下文长度增加而提高，但在更精细的空间离散化时会下降。在多步展开中，模型递归预测多个时间步的未来空间状态，误差随时间范围代数增长，类似于经典有限差分解算子中的全局误差累积。我们将这些趋势解释为上下文神经缩放定律，其中预测质量随上下文长度和输出长度可预测地变化。为了更好地理解大型语言模型如何能够内部处理偏微分方程解以准确展开预测，我们分析了标记级输出分布，并发现了一致的上下文学习进展：从语法模式模仿开始，过渡到探索性高熵阶段，最终以自信的、基于数字的预测结束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated emergent in-context learning(ICL) capabilities across a range of tasks, including zero-shot time-seriesforecasting. We show that text-trained foundation models can accuratelyextrapolate spatiotemporal dynamics from discretized partial differentialequation (PDE) solutions without fine-tuning or natural language prompting.Predictive accuracy improves with longer temporal contexts but degrades atfiner spatial discretizations. In multi-step rollouts, where the modelrecursively predicts future spatial states over multiple time steps, errorsgrow algebraically with the time horizon, reminiscent of global erroraccumulation in classical finite-difference solvers. We interpret these trendsas in-context neural scaling laws, where prediction quality varies predictablywith both context length and output length. To better understand how LLMs areable to internally process PDE solutions so as to accurately roll them out, weanalyze token-level output distributions and uncover a consistent ICLprogression: beginning with syntactic pattern imitation, transitioning throughan exploratory high-entropy phase, and culminating in confident, numericallygrounded predictions.</description>
      <author>example@mail.com (Jiajun Bao, Nicolas Boullé, Toni J. B. Liu, Raphaël Sarfati, Christopher J. Earls)</author>
      <guid isPermaLink="false">2509.06322v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting</title>
      <link>http://arxiv.org/abs/2509.06311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WindFM是一个专门为概率性风力发电预测设计的轻量级生成式基础模型，采用离散化和生成框架，能够实现高质量的零样本预测性能，无需微调即可优于专业模型和更大的基础模型。&lt;h4&gt;背景&lt;/h4&gt;高质量的风力发电预测对现代电网运行至关重要，但现有的数据驱动方法存在局限性，要么无法泛化到其他地点，要么难以融入能源领域的特定领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于概率性风力发电预测的轻量级生成式基础模型，解决现有方法的泛化和领域适应性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;采用离散化和生成框架，使用专门的时间序列标记器将连续多元观测值转换为离散的分层标记，然后通过自回归预训练学习风力发电动力学的通用表示。&lt;h4&gt;主要发现&lt;/h4&gt;810万参数的WindFM模型在确定性和概率性任务上实现了最先进的零样本性能，无需微调即可优于专业模型和更大的基础模型，并且在分布外数据上表现出强适应性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;WindFM通过大规模数据预训练学习到的表示具有良好的泛化能力和可转移性，为风力发电预测提供了一个有效的基础模型解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高质量的风力发电预测对现代电网运行至关重要。然而，现有的数据驱动范式要么训练特定地点的模型无法泛化到其他位置，要么依赖通用时间序列基础模型的微调，难以融入能源领域的特定领域数据。本文介绍了WindFM，一个专门为概率性风力发电预测设计的轻量级生成式基础模型。WindFM采用离散化和生成框架，首先使用专门的时间序列标记器将连续多元观测值转换为离散的分层标记，然后通过自回归预训练学习风力发电动力学的通用表示。使用包含约1500亿个时间步和超过126,000个地点的WIND Toolkit数据集，WindFM开发了对大气条件与发电输出之间复杂相互作用的基础理解。大量实验表明，我们紧凑的810万参数模型在确定性和概率性任务上都实现了最先进的零样本性能，优于专业模型和更大的基础模型，无需任何微调。特别是，WindFM在不同大陆的分布外数据上表现出强适应性，展示了其学习表示的鲁棒性和可转移性。我们的预训练模型已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality wind power forecasting is crucial for the operation of modernpower grids. However, prevailing data-driven paradigms either train asite-specific model which cannot generalize to other locations or rely onfine-tuning of general-purpose time series foundation models which aredifficult to incorporate domain-specific data in the energy sector. This paperintroduces WindFM, a lightweight and generative Foundation Model designedspecifically for probabilistic wind power forecasting. WindFM employs adiscretize-and-generate framework. A specialized time-series tokenizer firstconverts continuous multivariate observations into discrete, hierarchicaltokens. Subsequently, a decoder-only Transformer learns a universalrepresentation of wind generation dynamics by autoregressively pre-training onthese token sequences. Using the comprehensive WIND Toolkit dataset comprisingapproximately 150 billion time steps from more than 126,000 sites, WindFMdevelops a foundational understanding of the complex interplay betweenatmospheric conditions and power output. Extensive experiments demonstrate thatour compact 8.1M parameter model achieves state-of-the-art zero-shotperformance on both deterministic and probabilistic tasks, outperformingspecialized models and larger foundation models without any fine-tuning. Inparticular, WindFM exhibits strong adaptiveness under out-of-distribution datafrom a different continent, demonstrating the robustness and transferability ofits learned representations. Our pre-trained model is publicly available athttps://github.com/shiyu-coder/WindFM.</description>
      <author>example@mail.com (Hang Fan, Yu Shi, Zongliang Fu, Shuo Chen, Wei Wei, Wei Xu, Jian Li)</author>
      <guid isPermaLink="false">2509.06311v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2509.06233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025. Project website:  https://o3afford.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的一次性3D物体到物体可供性学习方法(O³Afford)，用于机器人操作中的物体可供性 grounding，解决了现有研究主要关注单物体可供性而忽略物体对交互关系的问题。&lt;h4&gt;背景&lt;/h4&gt;物体可供性(grounding object affordance)对机器人操作至关重要，它建立了感知与行动之间的关键联系。然而，现有研究主要关注单物体可供性预测，忽略了现实世界中大多数交互涉及物体对之间的关系。&lt;h4&gt;目的&lt;/h4&gt;解决在有限数据约束下的物体到物体可供性 grounding 挑战，提出一种新颖的一次性3D物体到物体可供性学习方法，用于机器人操作。&lt;h4&gt;方法&lt;/h4&gt;受近期2D视觉基础模型的小样本学习进展启发，结合视觉基础模型的语义特征和点云表示以实现几何理解，构建一次性学习管道。此外，将3D可供性表示与大型语言模型(LLMs)集成，显著增强LLMs在生成特定任务约束函数时理解物体交互的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在3D物体到物体可供性 grounding 和机器人操作实验中，提出的O³Afford方法在准确性和泛化能力方面显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;O³Afford方法在解决物体到物体可供性 grounding 问题上表现优异，在有限数据条件下能够有效学习并推广到新场景。&lt;h4&gt;翻译&lt;/h4&gt;物体可供性 grounding 是机器人操作的基础，因为它在相互作用的物体之间建立了感知与行动的关键联系。然而，先前的工作主要集中预测单物体可供性，忽略了大多数现实世界交互涉及物体对之间关系的事实。在这项工作中，我们解决了在有限数据约束下物体到物体可供性 grounding 的挑战。受近期2D视觉基础模型小样本学习进展的启发，我们提出了一种新颖的一次性3D物体到物体可供性学习方法，用于机器人操作。视觉基础模型的语义特征结合点云表示以实现几何理解，使我们的一次性学习管道能够有效推广到新物体和类别。我们进一步将3D可供性表示与大型语言模型(LLMs)集成用于机器人操作，显著增强了LLMs在生成特定任务约束函数时理解和推理物体交互的能力。我们在3D物体到物体可供性 grounding 和机器人操作上的实验表明，我们的O³Afford在准确性和泛化能力方面显著优于现有基线。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是3D物体间可操作性的单样本学习，即如何让机器人仅通过一个示例就能理解两个物体之间的交互关系并泛化到新物体上。这个问题在现实中非常重要，因为日常生活中的许多任务（如倒水、切割、悬挂等）都涉及两个物体之间的交互，而收集大量标注数据非常困难。现有方法主要关注单个物体的可操作性，忽略了物体间的关系，且泛化能力有限，难以适应新物体和新场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现实世界交互大多涉及物体对而非单个物体的特点，认识到收集大量物体间交互数据的困难，因此提出单样本学习思路。他们借鉴了视觉基础模型（如DINOv2）在少样本学习中的成功经验，结合3D点云表示来获取几何信息，弥补2D图像的局限性。同时，他们利用大型语言模型来生成任务特定的约束函数。方法设计上，他们将语义特征投影到3D点云上，设计了双向注意力变换器解码器处理物体间交互，并将可操作性表示与LLM集成用于机器人操作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 将视觉基础模型的语义特征与3D点云的几何信息结合，同时理解物体的功能属性和空间结构；2) 通过双向注意力机制，让模型同时考虑源物体和目标物体的上下文信息；3) 利用单样本学习实现泛化到新物体的能力；4) 将可操作性表示与大型语言模型结合，使机器人能根据自然语言指令执行复杂任务。整体流程包括：1) 构建语义点云，从多视角RGB-D中提取DINOv2特征并投影到3D点云；2) 使用双向注意力变换器解码器预测可操作性图；3) 将可操作性图与LLM结合，LLM生成约束函数，通过优化算法找到最佳物体姿态实现机器人操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现单样本3D物体间可操作性学习，解决数据稀缺问题；2) 融合语义特征与几何信息，提高泛化能力；3) 设计双向注意力机制，理解物体间交互关系；4) 集成LLM自动生成任务特定约束函数。相比之前工作，不同之处在于：从关注单物体转向物体对交互，从2D图像空间转向3D点云空间，从需要大量数据转向仅需单样本，从手动设计约束转向LLM自动生成约束。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出O3Afford方法，通过融合视觉基础模型语义特征与3D点云几何信息，并集成大型语言模型，实现了仅通过一个示例就能理解两个物体间的交互关系并泛化到新物体的能力，显著提升了机器人在复杂操作任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grounding object affordance is fundamental to robotic manipulation as itestablishes the critical link between perception and action among interactingobjects. However, prior works predominantly focus on predicting single-objectaffordance, overlooking the fact that most real-world interactions involverelationships between pairs of objects. In this work, we address the challengeof object-to-object affordance grounding under limited data contraints.Inspired by recent advances in few-shot learning with 2D vision foundationmodels, we propose a novel one-shot 3D object-to-object affordance learningapproach for robotic manipulation. Semantic features from vision foundationmodels combined with point cloud representation for geometric understandingenable our one-shot learning pipeline to generalize effectively to novelobjects and categories. We further integrate our 3D affordance representationwith large language models (LLMs) for robotics manipulation, significantlyenhancing LLMs' capability to comprehend and reason about object interactionswhen generating task-specific constraint functions. Our experiments on 3Dobject-to-object affordance grounding and robotic manipulation demonstrate thatour O$^3$Afford significantly outperforms existing baselines in terms of bothaccuracy and generalization capability.</description>
      <author>example@mail.com (Tongxuan Tian, Xuhui Kang, Yen-Ling Kuo)</author>
      <guid isPermaLink="false">2509.06233v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving</title>
      <link>http://arxiv.org/abs/2509.06142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为RetinaGuard的隐私增强框架，用于保护眼底图像中的生物特征数据，特别是视网膜年龄信息，同时保持图像质量和诊断效用。&lt;h4&gt;背景&lt;/h4&gt;AI与医学图像的结合可以从图像中提取隐性的生物标志物用于健康评估。视网膜年龄是从眼底图像预测出的生物标志物，可以预测全身疾病风险、行为模式、衰老轨迹和死亡率。然而，这种技术带来了隐私风险，未经授权使用眼底图像可能导致生物信息泄露。&lt;h4&gt;目的&lt;/h4&gt;提出与医学图像相关的生物特征隐私这一新研究问题，并开发一种隐私增强框架来保护眼底图像中的敏感生物特征数据。&lt;h4&gt;方法&lt;/h4&gt;提出RetinaGuard框架，采用特征级别生成对抗性掩蔽机制来模糊视网膜年龄，同时保持图像视觉质量和疾病诊断效用。框架还利用多对一知识蒸馏策略，结合视网膜基础模型和多样化的代理年龄编码器，以实现对黑盒年龄预测模型的通用防御。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估证实，RetinaGuard成功模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。&lt;h4&gt;结论&lt;/h4&gt;RetinaGuard是一种有效的隐私保护框架，可以灵活扩展到其他医学图像衍生的生物标志物。&lt;h4&gt;翻译&lt;/h4&gt;将AI与医学图像相结合可以从图像中提取隐性的图像衍生生物标志物，用于精确的健康评估。最近，从眼底图像预测出的视网膜年龄是一种已证实可以预测全身疾病风险、行为模式、衰老轨迹甚至死亡率的生物标志物。然而，推断此类敏感生物特征数据的能力带来了重大的隐私风险，未经授权使用眼底图像可能导致生物信息泄露，侵犯个人隐私。为此，我们提出了与医学图像相关的生物特征隐私这一新研究问题，并提出了RetinaGuard，一种新颖的隐私增强框架，该框架采用特征级别生成对抗性掩蔽机制来模糊视网膜年龄，同时保持图像视觉质量和疾病诊断效用。该框架进一步利用了一种新颖的多对一知识蒸馏策略，结合了视网膜基础模型和多样化的代理年龄编码器，以实现对黑盒年龄预测模型的通用防御。全面评估证实，RetinaGuard成功模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。RetinaGuard还可以灵活扩展到其他医学图像衍生的生物标志物。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of AI with medical images enables the extraction of implicitimage-derived biomarkers for a precise health assessment. Recently, retinalage, a biomarker predicted from fundus images, is a proven predictor ofsystemic disease risks, behavioral patterns, aging trajectory and evenmortality. However, the capability to infer such sensitive biometric dataraises significant privacy risks, where unauthorized use of fundus images couldlead to bioinformation leakage, breaching individual privacy. In response, weformulate a new research problem of biometric privacy associated with medicalimages and propose RetinaGuard, a novel privacy-enhancing framework thatemploys a feature-level generative adversarial masking mechanism to obscureretinal age while preserving image visual quality and disease diagnosticutility. The framework further utilizes a novel multiple-to-one knowledgedistillation strategy incorporating a retinal foundation model and diversesurrogate age encoders to enable a universal defense against black-box ageprediction models. Comprehensive evaluations confirm that RetinaGuardsuccessfully obfuscates retinal age prediction with minimal impact on imagequality and pathological feature representation. RetinaGuard is also flexiblefor extension to other medical image derived biomarkers. RetinaGuard is alsoflexible for extension to other medical image biomarkers.</description>
      <author>example@mail.com (Zhengquan Luo, Chi Liu, Dongfu Xiao, Zhen Yu, Yueye Wang, Tianqing Zhu)</author>
      <guid isPermaLink="false">2509.06142v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.06096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MedSeqFT是一种顺序微调框架，通过最大数据相似性选择和基于LoRA的知识蒸馏方案，解决了基础模型在医学图像分割任务中微调的局限性，显著提升了性能和可迁移性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分析中显示出巨大潜力，特别是在分割任务方面。然而，现有的微调策略存在局限性：并行微调隔离任务无法利用共享知识，而多任务微调需要同时访问所有数据集，难以集成增量任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够逐步将预训练模型适应到新任务同时改进其表示能力的微调框架，解决现有微调策略的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出MedSeqFT，包含两个核心组件：(1)最大数据相似性选择，识别最能代表原始预训练分布的下游样本以保留通用知识；(2)知识和泛化保留微调，一种基于LoRA的知识蒸馏方案，平衡特定任务适应与预训练知识保留。&lt;h4&gt;主要发现&lt;/h4&gt;在两个多任务数据集上覆盖10个3D分割任务的实验表明，MedSeqFT始终优于最先进的微调策略，带来显著的性能提升（平均Dice提高3.0%）。在两个未见过的任务上验证了MedSeqFT提高了可迁移性，特别是肿瘤分割。损失景观和参数变化的视觉分析进一步证明了MedSeqFT的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;顺序微调是一种有效的、保留知识的范式，用于将基础模型适应不断发展的临床任务。代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已成为推进医学图像分析的一种有前景的范式，特别是在下游应用通常按顺序出现的分割任务中。然而，现有的微调策略仍然有限：并行微调隔离任务且无法利用共享知识，而多任务微调需要同时访问所有数据集且难以集成增量任务。为解决这些挑战，我们提出了MedSeqFT，一种顺序微调框架，能够逐步将预训练模型适应到新任务，同时改进其表示能力。MedSeqFT引入了两个核心组件：(1)最大数据相似性选择，识别最能代表原始预训练分布的下游样本以保留通用知识；(2)知识和泛化保留微调，一种基于LoRA的知识蒸馏方案，平衡特定任务适应与预训练知识保留。在覆盖10个3D分割任务的两个多任务数据集上的广泛实验表明，MedSeqFT始终优于最先进的微调策略，带来显著的性能提升（例如，平均Dice提高3.0%）。此外，在两个未见过的任务（COVID-19-20和肾脏）上的评估验证了MedSeqFT提高了可迁移性，特别是对于肿瘤分割。损失景观和参数变化的视觉分析进一步突显了MedSeqFT的鲁棒性。这些结果确立了顺序微调作为一种有效的、保留知识的范式，用于将基础模型适应不断发展的临床任务。代码将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学图像分割中基础模型的微调策略问题。现有方法中，并行微调无法利用任务间的共享知识，而多任务微调需要同时访问所有数据集，难以灵活增量整合新任务。这个问题在医学图像分析中非常重要，因为临床实践中新任务往往随时间推移逐渐出现，而非一次性全部出现；同时医学图像分割需要专家标注，耗时耗力，基础模型通过自监督学习可减少对标注数据的依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有微调策略的局限性，提出更符合临床工作流的顺序微调范式。他们识别出顺序微调中的关键挑战是避免灾难性遗忘。设计方法时借鉴了自监督学习(SSL)的预训练-微调范式、知识蒸馏(KD)技术、LoRA参数高效微调方法，以及nnU-Net等医学图像分析领域的现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出MedSeqFT框架，采用顺序微调策略逐步适应预训练模型到多个3D医学图像分割任务，并通过两个关键组件平衡任务特定适应与保留预训练知识：1)最大数据相似性(MDS)选择，保留最能代表原始预训练分布的样本；2)知识与泛化保留微调(K&amp;G RFT)，基于LoRA的知识蒸馏方案。整体流程包括：初始化第一个任务，使用MDS选择代表性样本，对后续任务顺序微调，迭代更新模型和缓冲区。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出MedSeqFT顺序微调框架；2)引入MDS策略保留泛化知识；3)设计K&amp;G RFT策略平衡适应与知识保留；4)实验证明在多任务上性能优越且增强可迁移性。相比之前工作不同：能利用任务间共享知识而非独立处理；不需要同时访问所有数据集，可灵活增量整合新任务；通过MDS和K&amp;G RFT显式减轻灾难性遗忘；整体性能优于传统FFT和PEFT方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MedSeqFT通过创新的顺序微调框架解决了医学图像分割中任务间知识共享与灵活适应新任务的矛盾，在保持模型泛化能力的同时显著提升了分割性能和可迁移性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have become a promising paradigm for advancing medicalimage analysis, particularly for segmentation tasks where downstreamapplications often emerge sequentially. Existing fine-tuning strategies,however, remain limited: parallel fine-tuning isolates tasks and fails toexploit shared knowledge, while multi-task fine-tuning requires simultaneousaccess to all datasets and struggles with incremental task integration. Toaddress these challenges, we propose MedSeqFT, a sequential fine-tuningframework that progressively adapts pre-trained models to new tasks whilerefining their representational capacity. MedSeqFT introduces two corecomponents: (1) Maximum Data Similarity (MDS) selection, which identifiesdownstream samples most representative of the original pre-trainingdistribution to preserve general knowledge, and (2) Knowledge andGeneralization Retention Fine-Tuning (K&amp;G RFT), a LoRA-based knowledgedistillation scheme that balances task-specific adaptation with the retentionof pre-trained knowledge. Extensive experiments on two multi-task datasetscovering ten 3D segmentation tasks demonstrate that MedSeqFT consistentlyoutperforms state-of-the-art fine-tuning strategies, yielding substantialperformance gains (e.g., an average Dice improvement of 3.0%). Furthermore,evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFTenhances transferability, particularly for tumor segmentation. Visual analysesof loss landscapes and parameter variations further highlight the robustness ofMedSeqFT. These results establish sequential fine-tuning as an effective,knowledge-retentive paradigm for adapting foundation models to evolvingclinical tasks. Code will be released.</description>
      <author>example@mail.com (Yiwen Ye, Yicheng Wu, Xiangde Luo, He Zhang, Ziyang Chen, Ting Dang, Yanning Zhang, Yong Xia)</author>
      <guid isPermaLink="false">2509.06096v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior</title>
      <link>http://arxiv.org/abs/2509.06025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了统一交互基础模型（UIFM），这是一种为真正行为理解而设计的基础模型，通过复合标记化原则将多属性事件视为语义连贯的单位，从而能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型是为自然语言设计的，但无法理解电信、电子商务和金融等领域中复杂的、不断发展的序列事件的整体性质。&lt;h4&gt;目的&lt;/h4&gt;构建能够理解和预测复杂、不断发展的序列事件的人工智能系统。&lt;h4&gt;方法&lt;/h4&gt;引入统一交互基础模型（UIFM），采用复合标记化原则，将每个多属性事件视为单个语义连贯的单位。&lt;h4&gt;主要发现&lt;/h4&gt;UIFM架构不仅更准确，而且代表了创建更适应性和更智能的预测系统的根本性进展。&lt;h4&gt;结论&lt;/h4&gt;UIFM能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流，是创建更适应性和智能预测系统的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的一个核心目标是构建能够理解和预测复杂、不断发展的序列事件的系统。然而，当前为自然语言设计的基础模型无法理解电信、电子商务和金融等领域中结构化交互的整体性质。通过将事件序列化为文本，这些模型将它们分解为语义碎片化的部分，失去了关键上下文。在这项工作中，我们介绍了统一交互基础模型（UIFM），这是一个为真正的行为理解而设计的基础模型。其核心是复合标记化原则，其中每个多属性事件被视为单个语义连贯的单位。这使得UIFM能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流。我们证明，这种架构不仅更准确，而且代表了创建更适应性和更智能的预测系统的根本性进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central goal of artificial intelligence is to build systems that canunderstand and predict complex, evolving sequences of events. However, currentfoundation models, designed for natural language, fail to grasp the holisticnature of structured interactions found in domains like telecommunications,e-commerce and finance. By serializing events into text, they disassemble theminto semantically fragmented parts, losing critical context. In this work, weintroduce the Unified Interaction Foundation Model (UIFM), a foundation modelengineered for genuine behavioral understanding. At its core is the principleof composite tokenization, where each multi-attribute event is treated as asingle, semantically coherent unit. This allows UIFM to learn the underlying"grammar" of user behavior, perceiving entire interactions rather than adisconnected stream of data points. We demonstrate that this architecture isnot just more accurate, but represents a fundamental step towards creating moreadaptable and intelligent predictive systems.</description>
      <author>example@mail.com (Vignesh Ethiraj, Subhash Talluri)</author>
      <guid isPermaLink="false">2509.06025v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</title>
      <link>http://arxiv.org/abs/2509.05978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种基于语言提示的框架，能够生成高分辨率3D反事实医学图像，解决了3D领域缺乏预训练基础模型的问题，首次将语言引导的原生3D扩散模型应用于神经影像数据。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在2D图像生成方面表现出色，但这依赖于大量可用的预训练基础模型。然而，3D领域缺乏类似的预训练基础模型，严重限制了该领域的进展。视觉语言模型在仅基于自然语言描述生成高分辨率3D反事实医学图像方面的潜力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;解决3D医学影像生成的这一差距，开发一个能够根据自由形式语言提示生成高分辨率3D反事实医学图像的框架，用于临床和研究应用，如个性化反事实解释、疾病进展场景模拟和增强医疗培训。&lt;h4&gt;方法&lt;/h4&gt;采用最先进的3D扩散模型，结合Simple Diffusion的增强功能，并加入增强的条件设置以提高文本对齐和图像质量。这是首次将语言引导的原生3D扩散模型应用于神经影像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。&lt;h4&gt;主要发现&lt;/h4&gt;在两个不同的神经MRI数据集上，该框架成功模拟了多发性硬化症的不同反事实病变负荷和阿尔茨海默病的认知状态，生成了高质量图像，同时在合成的医学图像中保持了主体保真度。&lt;h4&gt;结论&lt;/h4&gt;研究结果为3D医学影像中的提示驱动疾病进展分析奠定了基础，为个性化医疗和医学研究提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在各种条件下生成2D图像方面表现出令人印象深刻的能力；然而，这些模型在2D方面的出色表现很大程度上依赖于大量可用的预训练基础模型。关键的是，3D领域没有类似的预训练基础模型，这严重限制了该领域的进展。因此，视觉语言模型仅根据自然语言描述生成高分辨率3D反事实医学图像的潜力仍未被探索。解决这一差距将 enable强大的临床和研究应用，如个性化反事实解释、疾病进展场景模拟，以及通过以真实细节可视化假设性医疗条件来增强医疗培训。我们的工作通过引入一个能够根据自由形式语言提示生成高分辨率3D反事实医学图像的框架，朝着解决这一挑战迈出了有意义的一步。我们采用了最先进的3D扩散模型，结合了Simple Diffusion的增强功能，并加入了增强的条件设置以提高文本对齐和图像质量。据我们所知，这是首次将语言引导的原生3D扩散模型专门应用于神经影像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。通过对两个不同的神经MRI数据集的实验，我们的框架成功模拟了多发性硬化症中的不同反事实病变负荷和阿尔茨海默病中的认知状态，生成了高质量图像，同时在合成的医学图像中保持了主体保真度。我们的结果为3D医学影像中的提示驱动疾病进展分析奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过自然语言描述生成高分辨率的3D反事实医学图像问题。这个问题在现实中很重要，因为它可以实现个性化反事实解释、模拟疾病进展场景、增强医学培训，让医生能够可视化假设的医疗状况，从而提高临床决策能力和医学教育效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作：采用了Simple Diffusion的三种架构增强（加深U-Net瓶颈、应用目标dropout、多尺度交叉注意力层）；整合了MAISI 3D潜在扩散框架并引入Rectified Flow噪声调度；使用BiomedCLIP获取医学语义嵌入。作者设计思路是构建一个能根据语言提示生成高分辨率3D反事实医学图像的框架，直接在体素空间操作以支持精细编辑。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用语言引导的扩散模型生成3D医学图像的反事实版本，保持患者解剖结构不变，同时根据文本提示改变临床状态。流程包括：1)创建与MRI特征对应的文本提示；2)构建语言引导的3D扩散模型，采用架构增强和交叉注意力层；3)从相同噪声源使用不同文本提示生成反事实图像，共享患者身份但反映不同临床状态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：首次实现语言引导的3D反事实医学图像生成；首次将原生3D扩散模型应用于神经成像；结合Simple Diffusion增强和BiomedCLIP语义嵌入；引入Rectified Flow噪声调度提高效率；提出使用无分类器引导平衡文本对齐和解剖保真度。不同之处：现有方法主要生成新合成扫描而非修改现有图像；受限于简单变量而非自由文本；主要关注物体表面而非内部体积结构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的语言引导3D扩散模型框架，能够根据自然语言描述生成高保真度的反事实医学图像，为疾病进展模拟、个性化临床解释和医学教育提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have demonstrated impressive capabilities ingenerating 2D images under various conditions; however the impressiveperformance of these models in 2D is largely enabled by extensive, readilyavailable pretrained foundation models. Critically, comparable pretrainedfoundation models do not exist for 3D, significantly limiting progress in thisdomain. As a result, the potential of vision-language models to producehigh-resolution 3D counterfactual medical images conditioned solely on naturallanguage descriptions remains completely unexplored. Addressing this gap wouldenable powerful clinical and research applications, such as personalizedcounterfactual explanations, simulation of disease progression scenarios, andenhanced medical training by visualizing hypothetical medical conditions inrealistic detail. Our work takes a meaningful step toward addressing thischallenge by introducing a framework capable of generating high-resolution 3Dcounterfactual medical images of synthesized patients guided by free-formlanguage prompts. We adapt state-of-the-art 3D diffusion models withenhancements from Simple Diffusion and incorporate augmented conditioning toimprove text alignment and image quality. To our knowledge, this represents thefirst demonstration of a language-guided native-3D diffusion model appliedspecifically to neurological imaging data, where faithful three-dimensionalmodeling is essential to represent the brain's three-dimensional structure.Through results on two distinct neurological MRI datasets, our frameworksuccessfully simulates varying counterfactual lesion loads in MultipleSclerosis (MS), and cognitive states in Alzheimer's disease, generatinghigh-quality images while preserving subject fidelity in syntheticallygenerated medical images. Our results lay the groundwork for prompt-drivendisease progression analysis within 3D medical imaging.</description>
      <author>example@mail.com (Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel)</author>
      <guid isPermaLink="false">2509.05978v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at IEEE 35th Workshop on Machine  Learning for Signal Processing (MLSP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的语义压缩方法，利用CLIP模型的能力，专注于保留图像的语义信息而非像素级细节。通过压缩CLIP特征嵌入而非原始图像，实现了极低的比特率，同时保持跨任务的语义完整性。&lt;h4&gt;背景&lt;/h4&gt;现有基于深度学习的有损图像压缩方法通过端到端训练和先进架构实现了具有竞争力的率失真性能。然而，新兴应用越来越重视语义保持而非像素级重建，并且需要在不同数据分布和下游任务中保持稳健性能。&lt;h4&gt;目的&lt;/h4&gt;开发先进的语义压缩范式，以满足新兴应用对语义保持和跨数据分布稳健性的需求。&lt;h4&gt;方法&lt;/h4&gt;基于对比语言-图像预训练(CLIP)模型提出新型语义压缩方法。不压缩图像用于重建，而是将CLIP特征嵌入压缩到最少比特数，同时保留不同任务中的语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在基准数据集中保持语义完整性，实现了每像素约2-3×10^(-3)比特的平均比特率，不到主流图像压缩方法在可比性能下所需比特率的5%。即使在极端压缩下，该方法也在不同数据分布和下游任务中表现出零样本稳健性。&lt;h4&gt;结论&lt;/h4&gt;基于CLIP模型的语义压缩方法能够在极低比特率下有效保留图像语义信息，相比传统像素级重建方法在语义保持方面具有显著优势，且在不同数据分布中表现出强大的零样本稳健性。&lt;h4&gt;翻译&lt;/h4&gt;最近基于深度学习的有损图像压缩方法通过广泛的端到端训练和先进架构实现了具有竞争力的率失真性能。然而，新兴应用越来越优先考虑语义保持而非像素级重建，并要求在不同数据分布和下游任务中保持稳健性能。这些挑战需要先进的语义压缩范式。受多模态基础模型的零样本和表征能力启发，我们提出了一种基于对比语言-图像预训练(CLIP)模型的新型语义压缩方法。我们不是为重建而压缩图像，而是将CLIP特征嵌入压缩到最少的比特数，同时保留不同任务中的语义信息。实验表明，我们的方法在基准数据集中保持语义完整性，实现了每像素约2-3×10^(-3)比特的平均比特率。这不到主流图像压缩方法在可比性能下所需比特率的5%。值得注意的是，即使在极端压缩下，所提出的方法也在不同数据分布和下游任务中表现出零样本稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep learning-based methods for lossy image compression achievecompetitive rate-distortion performance through extensive end-to-end trainingand advanced architectures. However, emerging applications increasinglyprioritize semantic preservation over pixel-level reconstruction and demandrobust performance across diverse data distributions and downstream tasks.These challenges call for advanced semantic compression paradigms. Motivated bythe zero-shot and representational capabilities of multimodal foundationmodels, we propose a novel semantic compression method based on the contrastivelanguage-image pretraining (CLIP) model. Rather than compressing images forreconstruction, we propose compressing the CLIP feature embeddings into minimalbits while preserving semantic information across different tasks. Experimentsshow that our method maintains semantic integrity across benchmark datasets,achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. Thisis less than 5% of the bitrate required by mainstream image compressionapproaches for comparable performance. Remarkably, even under extremecompression, the proposed approach exhibits zero-shot robustness across diversedata distributions and downstream tasks.</description>
      <author>example@mail.com (Ruiqi Shen, Haotian Wu, Wenjing Zhang, Jiangjing Hu, Deniz Gunduz)</author>
      <guid isPermaLink="false">2509.05925v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</title>
      <link>http://arxiv.org/abs/2509.05892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多种先进的深度学习分割模型在心血管组织病理图像中颈动脉结构分割任务上的性能，发现模型性能对数据分割高度敏感，且性能差异更多由统计噪声而非真正的算法优势驱动。&lt;h4&gt;背景&lt;/h4&gt;准确分割心血管组织病理图像中的颈动脉结构对推进心血管疾病研究和诊断至关重要。然而，该领域深度学习模型的发展受到已标注的心血管组织病理数据稀缺性的限制。&lt;h4&gt;目的&lt;/h4&gt;系统评估最先进的深度学习分割模型在有限的心血管组织病理数据集上的表现，并探讨标准基准测试实践在低数据临床环境中的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究评估了多种深度学习分割模型，包括卷积神经网络（U-Net、DeepLabV3+）、视觉Transformer（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+UNet）。研究采用了贝叶斯搜索的广泛超参数优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;尽管进行了超参数优化，研究发现模型性能对数据分割高度敏感，微小的性能差异更多是由统计噪声而非真正的算法优势驱动的。&lt;h4&gt;结论&lt;/h4&gt;这种不稳定性暴露了标准基准测试实践在低数据临床环境中的局限性，并挑战了性能排名反映有意义临床效用的假设。&lt;h4&gt;翻译&lt;/h4&gt;准确分割心血管组织病理图像中的颈动脉结构对推进心血管疾病研究和诊断至关重要。然而，该领域深度学习模型的发展受到已标注的心血管组织病理数据稀缺性的限制。本研究评估了最先进的深度学习分割模型，包括卷积神经网络（U-Net、DeepLabV3+）、视觉Transformer（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+UNet）在有限的心血管组织病理图像数据集上的表现。尽管采用了贝叶斯搜索的广泛超参数优化策略，我们的研究结果显示模型性能对数据分割高度敏感，微小的性能差异更多是由统计噪声而非真正的算法优势驱动的。这种不稳定性暴露了标准基准测试实践在低数据临床环境中的局限性，并挑战了性能排名反映有意义临床效用的假设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of carotid artery structures in histopathologicalimages is vital for advancing cardiovascular disease research and diagnosis.However, deep learning model development in this domain is constrained by thescarcity of annotated cardiovascular histopathological data. This studyinvestigates a systematic evaluation of state-of-the-art deep learningsegmentation models, including convolutional neural networks (U-Net,DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histologyimages. Despite employing an extensive hyperparameter optimization strategywith Bayesian search, our findings reveal that model performance is highlysensitive to data splits, with minor differences driven more by statisticalnoise than by true algorithmic superiority. This instability exposes thelimitations of standard benchmarking practices in low-data clinical settingsand challenges the assumption that performance rankings reflect meaningfulclinical utility.</description>
      <author>example@mail.com (Phongsakon Mark Konrad, Andrei-Alexandru Popa, Yaser Sabzehmeidani, Liang Zhong, Elisa A. Liehn, Serkan Ayvaz)</author>
      <guid isPermaLink="false">2509.05892v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明大型时间序列Transformer模型确实内部化了语义概念，而不仅仅是拟合曲线。通过激活移植技术，可以操纵模型内部表示来模拟不同市场状况，为理解和控制模型预测提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的基础模型在预测常规模式方面表现出色，但有两个关键问题：这些模型是否内部化了市场状况等语义概念，还是仅仅拟合曲线？它们的内部表示是否可以被利用来模拟罕见的高风险事件如市场崩盘？&lt;h4&gt;目的&lt;/h4&gt;研究Transformer模型是否真正理解了时间序列数据中的语义概念，以及是否可以通过模型的内部表示来模拟罕见的高风险事件。&lt;h4&gt;方法&lt;/h4&gt;引入了激活移植方法，这是一种因果干预技术，通过在前向传播过程中将一个事件（如历史崩盘）的统计强加到另一个事件（如平静期）上来操纵隐藏状态，从而确定性地引导预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;注入崩盘语义会诱导下跌预测，注入平静语义会抑制崩盘并恢复稳定性；模型编码了事件严重性的分级概念，潜在向量范数与系统性冲击的幅度直接相关；这种可引导的、语义基础的表示是大型时间序列Transformer的稳健特性。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持存在一个潜在的概念空间来控制模型预测，将可解释性从事后归因转向直接因果干预，并支持战略压力测试的语义化'假设分析'。&lt;h4&gt;翻译&lt;/h4&gt;虽然基于Transformer的基础模型在预测常规模式方面表现出色，但仍有两个问题：它们是否内部化了市场状况等语义概念，还是仅仅拟合曲线？它们的内部表示是否可以被利用来模拟罕见的高风险事件如市场崩盘？为了研究这一点，我们引入了激活移植，这是一种因果干预，通过在前向传播过程中将一个事件（如历史崩盘）的统计强加到另一个事件（如平静期）上来操纵隐藏状态。这个过程确定性地引导预测：注入崩盘语义会诱导下跌预测，而注入平静语义会抑制崩盘并恢复稳定性。除了二元控制外，我们发现模型编码了事件严重性的分级概念，其中潜在向量范数与系统性冲击的幅度直接相关。在两种架构不同的时间序列Transformer模型（Toto和Chronos）上得到验证，我们的结果表明可引导的、语义基础的表示是大型时间序列Transformer的稳健特性。我们的研究为控制模型预测的潜在概念空间提供了证据，将可解释性从事后归因转向直接因果干预，并支持战略压力测试的语义化'假设分析'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transformer-based foundation models excel at forecasting routinepatterns, two questions remain: do they internalize semantic concepts such asmarket regimes, or merely fit curves? And can their internal representations beleveraged to simulate rare, high-stakes events such as market crashes? Toinvestigate this, we introduce activation transplantation, a causalintervention that manipulates hidden states by imposing the statistical momentsof one event (e.g., a historical crash) onto another (e.g., a calm period)during the forward pass. This procedure deterministically steers forecasts:injecting crash semantics induces downturn predictions, while injecting calmsemantics suppresses crashes and restores stability. Beyond binary control, wefind that models encode a graded notion of event severity, with the latentvector norm directly correlating with the magnitude of systemic shocks.Validated across two architecturally distinct TSFMs, Toto (decoder only) andChronos (encoder-decoder), our results demonstrate that steerable, semanticallygrounded representations are a robust property of large time seriestransformers. Our findings provide evidence for a latent concept space thatgoverns model predictions, shifting interpretability from post-hoc attributionto direct causal intervention, and enabling semantic "what-if" analysis forstrategic stress-testing.</description>
      <author>example@mail.com (Debdeep Sanyal, Aaryan Nagpal, Dhruv Kumar, Murari Mandal, Saurabh Deshpande)</author>
      <guid isPermaLink="false">2509.05801v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>TPCpp-10M: Simulated proton-proton collisions in a Time Projection Chamber for AI Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个包含1000万次模拟质子-质子碰撞的大型开放数据集，支持基础模型的自我监督训练，包含70,000个标记示例用于三个下游任务的评估，旨在促进核物理和粒子物理领域的基础模型发展。&lt;h4&gt;背景&lt;/h4&gt;科学基础模型在核物理和粒子物理领域有很大潜力，但进展受限于缺乏大规模开放数据集、标准化评估任务和指标。此外，处理粒子物理学数据所需的专业知识和软件阻碍了与机器学习社区的跨学科合作。&lt;h4&gt;目的&lt;/h4&gt;引入一个大型开放数据集支持基础模型的自我监督训练，提供易于使用的数据集格式，并包含标记示例以评估基础模型的适应性。&lt;h4&gt;方法&lt;/h4&gt;使用Pythia蒙特卡洛事件生成器在200 GeV中心质心能量下模拟质子-质子碰撞数据，并用Geant4处理以包含真实的探测器条件和信号模拟，数据针对sPHENIX时间投影室设计，使用sPHENIX软件栈实现完整的模拟和重建链。&lt;h4&gt;主要发现&lt;/h4&gt;创建了包含1000万次模拟质子-质子碰撞的大型开放数据集，提供NumPy格式的数据集，包含70,000个标记示例涵盖三个下游任务：轨迹寻找、粒子识别和噪声标记。&lt;h4&gt;结论&lt;/h4&gt;该数据集资源为跨学科研究建立了共同基础，使机器学习科学家和物理学家能够探索扩展行为、评估可转移性，加速核物理和高能物理领域基础模型的进展。&lt;h4&gt;翻译&lt;/h4&gt;科学基础模型在推进核物理和粒子物理方面具有巨大潜力，通过提高分析精度和加速发现。然而，这一领域的进展通常受到缺乏公开可用的大规模数据集以及标准化评估任务和指标的限制。此外，处理粒子物理数据通常需要的专业知识和软件对与更广泛的机器学习社区进行跨学科合作构成了重大障碍。这项工作引入了一个包含1000万次模拟质子-质子碰撞的大型开放数据集，旨在支持基础模型的自我监督训练。为便于使用，数据集以通用的NumPy格式提供。此外，它包含70,000个标记示例，涵盖三个明确定义的下游任务：轨迹寻找、粒子识别和噪声标记，以实现对基础模型适应性的系统评估。模拟数据使用Pythia蒙特卡洛事件生成器在质心能量为200 GeV的条件下生成，并使用Geant4进行处理，以包含在布鲁克黑文国家实验室的相对论重离子对撞机上的sPHENIX时间投影室中的真实探测条件和信号模拟。该数据集资源为跨学科研究建立了共同基础，使机器学习科学家和物理学家都能够探索扩展行为、评估可转移性，并加速核物理和高能物理领域基础模型的进展。完整的模拟和重建链可以使用sPHENIX软件栈重现。所有数据和代码位置在数据可访问性部分提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific foundation models hold great promise for advancing nuclear andparticle physics by improving analysis precision and accelerating discovery.Yet, progress in this field is often limited by the lack of openly availablelarge scale datasets, as well as standardized evaluation tasks and metrics.Furthermore, the specialized knowledge and software typically required toprocess particle physics data pose significant barriers to interdisciplinarycollaboration with the broader machine learning community.  This work introduces a large, openly accessible dataset of 10 millionsimulated proton-proton collisions, designed to support self-supervisedtraining of foundation models. To facilitate ease of use, the dataset isprovided in a common NumPy format. In addition, it includes 70,000 labeledexamples spanning three well defined downstream tasks: track finding, particleidentification, and noise tagging, to enable systematic evaluation of thefoundation model's adaptability.  The simulated data are generated using the Pythia Monte Carlo event generatorat a center of mass energy of sqrt(s) = 200 GeV and processed with Geant4 toinclude realistic detector conditions and signal emulation in the sPHENIX TimeProjection Chamber at the Relativistic Heavy Ion Collider, located atBrookhaven National Laboratory.  This dataset resource establishes a common ground for interdisciplinaryresearch, enabling machine learning scientists and physicists alike to explorescaling behaviors, assess transferability, and accelerate progress towardfoundation models in nuclear and high energy physics. The complete simulationand reconstruction chain is reproducible with the sPHENIX software stack. Alldata and code locations are provided under Data Accessibility.</description>
      <author>example@mail.com (Shuhang Li, Yi Huang, David Park, Xihaier Luo, Haiwang Yu, Yeonju Go, Christopher Pinkenburg, Yuewei Lin, Shinjae Yoo, Joseph Osborn, Christof Roland, Jin Huang, Yihui Ren)</author>
      <guid isPermaLink="false">2509.05792v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian</title>
      <link>http://arxiv.org/abs/2509.05668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Michael Hoffmann and Jophin John contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Llama-GENBA-10B是一个三语种基础模型，旨在解决大型语言模型中的英语中心主义偏见。该模型基于Llama 3.1-8B构建并扩展至100亿参数，在平衡资源分配的同时防止英语主导，针对德语NLP社区，同时促进巴伐利亚语作为低资源语言的发展。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型普遍存在英语中心主义偏见，大多数模型主要在英语数据上进行训练，导致对其他语言的支持不足。特别是对于像巴伐利亚语这样的低资源语言，缺乏有效的模型支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个平衡英语、德语和巴伐利亚语的三语种基础模型，解决英语中心主义偏见，促进低资源语言巴伐利亚语的发展，并为德语NLP社区提供有力工具。&lt;h4&gt;方法&lt;/h4&gt;基于Llama 3.1-8B构建并扩展至100亿参数；在164B个标记上进行持续预训练（820亿英语，820亿德语和800万巴伐利亚语）；解决四个主要挑战：策划多语言语料库、创建统一分词器、优化架构和语言比例超参数、建立标准化三语种评估套件；使用Cerebras CS-2进行训练展示大规模多语言预训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;Llama-GENBA-10B实现了强大的跨语言性能；微调后的变体在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言同类中的最佳模型；在英语上表现优于EuroLLM，在德语上与EuroLLM结果相当；在Cerebras CS-2上的训练展示了大规模多语言预训练的高效性。&lt;h4&gt;结论&lt;/h4&gt;Llama-GENBA-10B为整合低资源语言的包容性基础模型提供了蓝图，证明了在平衡多语言资源分配的同时防止英语主导的可行性，为促进语言多样性和包容性AI发展做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Llama-GENBA-10B，一个三语种基础模型，旨在解决大型语言模型中的英语中心主义偏见。该模型基于Llama 3.1-8B构建并扩展至100亿参数，在164B个标记上持续预训练（820亿英语，820亿德语和800万巴伐利亚语），平衡资源分配同时防止英语主导。针对德语NLP社区，该模型还促进巴伐利亚语作为低资源语言的发展。开发过程中解决了四个挑战：(1) 尽管巴伐利亚语资源稀缺，仍策划多语言语料库，(2) 为英语、德语和巴伐利亚语创建统一的分词器，(3) 优化架构和语言比例超参数以实现跨语言迁移，(4) 通过将德语基准测试翻译成巴伐利亚语，建立首个标准化三语种评估套件。评估显示，Llama-GENBA-10B实现了强大的跨语言性能，微调后的变体在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言同类中的最佳模型，同时在英语上表现优于EuroLLM，在德语上与EuroLLM结果相当。在Cerebras CS-2上的训练展示了大规模多语言预训练的高效性，并记录了能源使用情况，为整合低资源语言的包容性基础模型提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Llama-GENBA-10B, a trilingual foundation model addressingEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaledto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens(82B English, 82B German, and 80M Bavarian), balancing resources whilepreventing English dominance. Targeted at the German NLP community, the modelalso promotes Bavarian as a low-resource language. Development tackled fourchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)creating a unified tokenizer for English, German, and Bavarian, (3) optimizingarchitecture and language-ratio hyperparameters for cross-lingual transfer, and(4) establishing the first standardized trilingual evaluation suite bytranslating German benchmarks into Bavarian. Evaluations show thatLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tunedvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishingitself as the best model in its class for this language, while alsooutperforming EuroLLM in English and matching its results in German. Trainingon the Cerebras CS-2 demonstrated efficient large-scale multilingualpretraining with documented energy use, offering a blueprint for inclusivefoundation models that integrate low-resource languages.</description>
      <author>example@mail.com (Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, Nicolay J. Hammer)</author>
      <guid isPermaLink="false">2509.05668v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</title>
      <link>http://arxiv.org/abs/2509.05584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, 5 tables, 1 algorithm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ProfilingAgent的智能体方法，利用大型语言模型和性能分析技术来自动化模型压缩过程，解决了基础模型在资源受限平台上部署的计算和内存瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型面临日益严重的计算和内存瓶颈，限制了在资源有限平台上的部署。现有的压缩技术如剪枝和量化大多依赖均匀启发式方法，忽略了架构和运行时异质性。性能分析工具虽能揭示各层性能数据，但很少集成到自动化流程中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于性能分析、智能体引导的自动化压缩方法，利用大型语言模型实现结构化剪枝和训练后动态量化，为不同架构模型定制优化策略。&lt;h4&gt;方法&lt;/h4&gt;提出ProfilingAgent，一个模块化多智能体系统，该系统结合静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，为特定架构设计优化策略，针对瓶颈进行逐层决策，而非使用均匀启发式方法。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet-1K、CIFAR-10和CIFAR-100数据集上使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16的实验表明：剪枝保持了竞争性或改进的精度（ImageNet-1K上约1%下降，ViT-B/16在较小数据集上+2%增益）；量化实现了高达74%的内存节省，精度损失&lt;0.5%；量化还带来了高达1.74倍的推理加速。与GPT-4o和GPT-4-Turbo的比较研究强调了LLM推理质量对迭代剪枝的重要性。&lt;h4&gt;结论&lt;/h4&gt;研究结果确立了智能体系统作为性能分析引导模型优化的可扩展解决方案，能够有效解决模型在资源受限环境中的部署问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型面临日益增长的计算和内存瓶颈，阻碍了在资源有限平台上的部署。虽然剪枝和量化等压缩技术被广泛使用，但大多数依赖均匀启发式方法，忽略了架构和运行时异质性。性能分析工具可以揭示各层的延迟、内存和计算成本，但很少集成到自动化流程中。我们提出ProfilingAgent，这是一种基于性能分析、智能体引导的方法，使用大型语言模型通过结构化剪枝和训练后动态量化来自动化压缩。我们的模块化多智能体系统基于静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，以设计特定架构的策略。与启发式基线不同，ProfilingAgent针对瓶颈进行逐层决策。在ImageNet-1K、CIFAR-10和CIFAR-100上使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16进行的实验表明，剪枝保持了竞争性或改进的精度（ImageNet-1上约1%的下降，ViT-B/16在较小数据集上+2%的增益），而量化实现了高达74%的内存节省，精度损失&lt;0.5%。我们的量化还带来了高达1.74倍的持续推理加速。与GPT-4o和GPT-4-Turbo的比较研究突显了LLM推理质量对迭代剪枝的重要性。这些结果确立了智能体系统作为性能分析引导模型优化的可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models face growing compute and memory bottlenecks, hinderingdeployment on resource-limited platforms. While compression techniques such aspruning and quantization are widely used, most rely on uniform heuristics thatignore architectural and runtime heterogeneity. Profiling tools exposeper-layer latency, memory, and compute cost, yet are rarely integrated intoautomated pipelines. We propose ProfilingAgent, a profiling-guided, agenticapproach that uses large language models (LLMs) to automate compression viastructured pruning and post-training dynamic quantization. Our modularmulti-agent system reasons over static metrics (MACs, parameter counts) anddynamic signals (latency, memory) to design architecture-specific strategies.Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions tobottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 withResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitiveor improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 onsmaller datasets), while quantization achieves up to 74% memory savings with&lt;0.5% accuracy loss. Our quantization also yields consistent inference speedupsof up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbohighlight the importance of LLM reasoning quality for iterative pruning. Theseresults establish agentic systems as scalable solutions for profiling-guidedmodel optimization.</description>
      <author>example@mail.com (Sadegh Jafari, Aishwarya Sarkar, Mohiuddin Bilwal, Ali Jannesari)</author>
      <guid isPermaLink="false">2509.05584v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations</title>
      <link>http://arxiv.org/abs/2509.05186v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出概率框架揭示ICON隐式执行贝叶斯推理，并扩展其到生成式设置(GenICON)，实现解预测中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;ICON是一类基于基础模型新架构的算子学习方法，在多样化的初始和边界条件数据集（与常微分方程和偏微分方程的解配对）上进行训练。&lt;h4&gt;目的&lt;/h4&gt;揭示ICON作为隐式贝叶斯推理方法的本质，并扩展其到生成式设置以实现不确定性量化。&lt;h4&gt;方法&lt;/h4&gt;通过随机微分方程形式提供概率框架，将ICON扩展为生成式形式(GenICON)，使其能够从解算子的后验预测分布中进行采样。&lt;h4&gt;主要发现&lt;/h4&gt;1. ICON隐式执行贝叶斯推理，计算基于上下文的解算子后验预测分布均值；2. 随机微分方程形式为描述ICON任务提供了概率框架；3. GenICON能够捕获解算子的潜在不确定性。&lt;h4&gt;结论&lt;/h4&gt;ICON的概率视角为算子学习中的解预测提供了有原则的不确定性量化基础，提高了其可靠性并扩展了应用范围。&lt;h4&gt;翻译&lt;/h4&gt;上下文算子网络(ICON)是一类基于基础模型新架构的算子学习方法。在多样化的初始和边界条件数据集（与常微分方程和偏微分方程的解配对）上进行训练，ICON学习将给定微分方程的条件-解示例对映射到其解算子的近似。在此，我们提出一个概率框架，揭示ICON隐式执行贝叶斯推理，计算基于提供上下文（即条件-解示例对）的解算子后验预测分布的均值。随机微分方程的形式为描述ICON完成的任务提供了概率框架，也为理解其他多算子学习方法提供了基础。这种概率视角为ICON扩展到生成式设置提供了基础，可以从解算子的后验预测分布中进行采样。ICON的生成式形式(GenICON)捕获了解算子的潜在不确定性，使算子学习中的解预测能够进行有原则的不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context operator networks (ICON) are a class of operator learning methodsbased on the novel architectures of foundation models. Trained on a diverse setof datasets of initial and boundary conditions paired with correspondingsolutions to ordinary and partial differential equations (ODEs and PDEs), ICONlearns to map example condition-solution pairs of a given differential equationto an approximation of its solution operator. Here, we present a probabilisticframework that reveals ICON as implicitly performing Bayesian inference, whereit computes the mean of the posterior predictive distribution over solutionoperators conditioned on the provided context, i.e., example condition-solutionpairs. The formalism of random differential equations provides theprobabilistic framework for describing the tasks ICON accomplishes while alsoproviding a basis for understanding other multi-operator learning methods. Thisprobabilistic perspective provides a basis for extending ICON to\emph{generative} settings, where one can sample from the posterior predictivedistribution of solution operators. The generative formulation of ICON(GenICON) captures the underlying uncertainty in the solution operator, whichenables principled uncertainty quantification in the solution predictions inoperator learning.</description>
      <author>example@mail.com (Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2509.05186v2</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.06826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于对比学习的视频分类方法，用于MPAA评级系统的自动分类，通过结合CNN、LSTM和注意力机制，实现了88%的准确率和0.8815的F1分数。&lt;h4&gt;背景&lt;/h4&gt;各平台视觉内容消费快速增长，需要自动化视频分类来满足MPAA分级系统(G, PG, PG-13, R)等年龄适宜性标准，而传统方法存在大量标记数据需求、泛化能力差和特征学习效率低等问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统视频分类方法的局限性，提高视频分类的准确性和效率，实现对MPAA等级的自动分类。&lt;h4&gt;方法&lt;/h4&gt;采用对比学习提高区分能力和适应性，探索实例判别、上下文对比学习和多视图对比学习三种框架；使用混合架构，结合LRCN(CNN+LSTM)主干和Bahdanau注意力机制；评估了NT-Xent、NT-logistic和Margin Triplet等多种对比损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在上下文对比学习框架下实现了最先进的性能，准确率达到88%，F1分数为0.8815；结合CNN的空间特征、LSTM的时间建模和注意力机制的动态帧优先级选择，模型在细粒度边界区分方面表现出色；架构在各种对比损失函数下表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;模型已部署为Web应用程序，用于实时MPAA评级分类，为流媒体平台自动化内容合规性提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;各平台视觉内容消费的快速增长需要对MPAA评级系统(如G, PG, PG-13, R等级)的年龄适宜性标准进行自动化视频分类。传统方法难以满足大量标记数据需求、泛化能力差和特征学习效率低等挑战。为解决这些问题，我们采用对比学习来提高区分能力和适应性，探索了三种框架：实例判别、上下文对比学习和多视图对比学习。我们的混合架构集成了LRCN(CNN+LSTM)主干和Bahdanau注意力机制，在上下文对比学习框架下实现了最先进的性能，准确率达88%，F1分数为0.8815。通过结合CNN的空间特征、LSTM的时间建模和注意力机制的动态帧优先级选择，模型在细粒度边界区分方面表现出色，例如区分PG-13和R级内容。我们评估了模型在各种对比损失函数(包括NT-Xent、NT-logistic和Margin Triplet)上的性能，证明了我们提出架构的鲁棒性。为确保实际应用，该模型已部署为Web应用程序，用于实时MPAA评级分类，为流媒体平台自动化内容合规性提供了高效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of visual content consumption across platforms necessitatesautomated video classification for age-suitability standards like the MPAArating system (G, PG, PG-13, R). Traditional methods struggle with largelabeled data requirements, poor generalization, and inefficient featurelearning. To address these challenges, we employ contrastive learning forimproved discrimination and adaptability, exploring three frameworks: InstanceDiscrimination, Contextual Contrastive Learning, and Multi-View ContrastiveLearning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with aBahdanau attention mechanism, achieving state-of-the-art performance in theContextual Contrastive Learning framework, with 88% accuracy and an F1 score of0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,and attention mechanisms for dynamic frame prioritization, the model excels infine-grained borderline distinctions, such as differentiating PG-13 and R-ratedcontent. We evaluate the model's performance across various contrastive lossfunctions, including NT-Xent, NT-logistic, and Margin Triplet, demonstratingthe robustness of our proposed architecture. To ensure practical application,the model is deployed as a web application for real-time MPAA ratingclassification, offering an efficient solution for automated content complianceacross streaming platforms.</description>
      <author>example@mail.com (Dipta Neogi, Nourash Azmine Chowdhury, Muhammad Rafsan Kabir, Mohammad Ashrafuzzaman Khan)</author>
      <guid isPermaLink="false">2509.06826v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>BEAM: Brainwave Empathy Assessment Model for Early Childhood</title>
      <link>http://arxiv.org/abs/2509.06620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为BEAM的新型深度学习框架，用于客观评估4-6岁儿童的共情能力，通过多视图EEG信号捕捉共情的认知和情感维度。&lt;h4&gt;背景&lt;/h4&gt;儿童共情能力对其社交和情感发展至关重要，但预测共情能力具有挑战性。传统方法依赖自我报告或观察者标记，易受偏见影响，无法客观捕捉共情形成过程。EEG虽提供了客观替代方案，但当前方法主要提取静态模式，忽略了时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，即脑波共情评估模型（BEAM），用于预测4-6岁儿童的共情水平。&lt;h4&gt;方法&lt;/h4&gt;BEAM利用多视图EEG信号捕捉共情的认知和情感维度，包含三个关键组件：1) 基于LaBraM的编码器用于时空特征提取，2) 特征融合模块整合多视图信号的互补信息，3) 对比学习模块增强类间分离。&lt;h4&gt;主要发现&lt;/h4&gt;在CBCP数据集上验证，BEAM在多个指标上优于最先进的方法，展示了客观共情评估的潜力。&lt;h4&gt;结论&lt;/h4&gt;BEAM为儿童亲社会发展的早期干预提供了初步见解，有望实现客观共情评估。&lt;h4&gt;翻译&lt;/h4&gt;儿童共情能力对其社交和情感发展至关重要，但预测共情能力仍然具有挑战性。传统方法通常仅依赖自我报告或观察者标记，这些方法容易受到偏见影响，无法客观捕捉共情形成过程。EEG提供了客观替代方案；然而，当前方法主要提取静态模式，忽略了时间动态性。为克服这些局限，我们提出了一种新型深度学习框架——脑波共情评估模型（BEAM），用于预测4-6岁儿童的共情水平。BEAM利用多视图EEG信号捕捉共情的认知和情感维度。该框架包含三个关键组件：1) 基于LaBraM的编码器，用于有效的时空特征提取；2) 特征融合模块，用于整合多视图信号的互补信息；3) 对比学习模块，用于增强类间分离。在CBCP数据集上验证，BEAM在多个指标上优于最先进的方法，展示了其客观共情评估的潜力，并为儿童亲社会发展的早期干预提供了初步见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Empathy in young children is crucial for their social and emotionaldevelopment, yet predicting it remains challenging. Traditional methods oftenonly rely on self-reports or observer-based labeling, which are susceptible tobias and fail to objectively capture the process of empathy formation. EEGoffers an objective alternative; however, current approaches primarily extractstatic patterns, neglecting temporal dynamics. To overcome these limitations,we propose a novel deep learning framework, the Brainwave Empathy AssessmentModel (BEAM), to predict empathy levels in children aged 4-6 years. BEAMleverages multi-view EEG signals to capture both cognitive and emotionaldimensions of empathy. The framework comprises three key components: 1) aLaBraM-based encoder for effective spatio-temporal feature extraction, 2) afeature fusion module to integrate complementary information from multi-viewsignals, and 3) a contrastive learning module to enhance class separation.Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods acrossmultiple metrics, demonstrating its potential for objective empathy assessmentand providing a preliminary insight into early interventions in children'sprosocial development.</description>
      <author>example@mail.com (Chen Xie, Gaofeng Wu, Kaidong Wang, Zihao Zhu, Xiaoshu Luo, Yan Liang, Feiyu Quan, Ruoxi Wu, Xianghui Huang, Han Zhang)</author>
      <guid isPermaLink="false">2509.06620v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
      <link>http://arxiv.org/abs/2509.06550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in: Proceedings of IEEE Conference on Cyber Security and  Resilience (CSR), 2025. Official version:  https://doi.org/10.1109/CSR64739.2025.11129979 Code:  https://github.com/jackwilkie/CLAN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLAN的新型网络入侵检测方法，通过对比学习使用增强负样本对，在良性流量预训练后提高了分类精度和推理效率，在二元和多类分类任务中均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;网络入侵检测是网络安全的关键挑战。监督式机器学习虽性能优异但依赖大型标记数据集，不切实际；异常检测仅用良性流量训练但误报率高；现有自监督方法通过学习良性流量表示改进性能但仍有限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型网络入侵检测方法，解决现有自监督和异常检测方法的局限性，提高检测精度并降低误报率。&lt;h4&gt;方法&lt;/h4&gt;提出CLAN（使用增强负样本对的对比学习）范式，将增强样本视为负视图（代表潜在恶意分布），其他良性样本作为正视图，通过对比学习学习良性流量的判别性表示。&lt;h4&gt;主要发现&lt;/h4&gt;在Lycos2017数据集上，CLAN在二元分类任务中优于现有自监督和异常检测技术；在有限标记数据集上微调后，多类分类性能也优于现有自监督模型。&lt;h4&gt;结论&lt;/h4&gt;CLAN方法通过创新的负样本处理方式，有效提升了网络入侵检测的性能和效率，是一种实用且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;网络入侵检测仍然是网络安全中的一个关键挑战。虽然监督式机器学习模型实现了最先进的性能，但它们对大型标记数据集的依赖使得它们在许多实际应用中不切实际。仅对良性流量进行训练以识别恶意活动的异常检测方法，误报率高，限制了其可用性。最近，自监督学习技术通过学习良性流量的判别性潜在表示，展示了改进的性能和更低的误报率。特别是，对比自监督模型通过最小化良性流量相似（正）视图之间的距离，同时最大化不同（负）视图之间的距离来实现这一点。现有方法通过数据增强生成正视图，并将其他样本视为负视图。相比之下，这项工作引入了使用增强负样本对的对比学习（CLAN），这是一种用于网络入侵检测的新范式，其中增强样本被视为负视图 - 代表潜在的恶意分布 - 而其他良性样本则作为正视图。这种方法在良性流量上进行预训练后，提高了分类精度和推理效率。在Lycos2017数据集上的实验评估表明，所提出的方法在二元分类任务中优于现有的自监督和异常检测技术。此外，当在有限的标记数据集上进行微调时，所提出的方法在多类分类任务中实现了比现有自监督模型更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CSR64739.2025.11129979&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network intrusion detection remains a critical challenge in cybersecurity.While supervised machine learning models achieve state-of-the-art performance,their reliance on large labelled datasets makes them impractical for manyreal-world applications. Anomaly detection methods, which train exclusively onbenign traffic to identify malicious activity, suffer from high false positiverates, limiting their usability. Recently, self-supervised learning techniqueshave demonstrated improved performance with lower false positive rates bylearning discriminative latent representations of benign traffic. Inparticular, contrastive self-supervised models achieve this by minimizing thedistance between similar (positive) views of benign traffic while maximizing itbetween dissimilar (negative) views. Existing approaches generate positiveviews through data augmentation and treat other samples as negative. Incontrast, this work introduces Contrastive Learning using Augmented Negativepairs (CLAN), a novel paradigm for network intrusion detection where augmentedsamples are treated as negative views - representing potentially maliciousdistributions - while other benign samples serve as positive views. Thisapproach enhances both classification accuracy and inference efficiency afterpretraining on benign traffic. Experimental evaluation on the Lycos2017 datasetdemonstrates that the proposed method surpasses existing self-supervised andanomaly detection techniques in a binary classification task. Furthermore, whenfine-tuned on a limited labelled dataset, the proposed approach achievessuperior multi-class classification performance compared to existingself-supervised models.</description>
      <author>example@mail.com (Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson)</author>
      <guid isPermaLink="false">2509.06550v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</title>
      <link>http://arxiv.org/abs/2509.06531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP Findings 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SLiNT框架，通过将知识图谱结构信息注入到冻结的大型语言模型中，解决了LLM在知识图谱链接预测中结构信号利用不足的问题，实现了在不完整或零样本设置下的稳健预测。&lt;h4&gt;背景&lt;/h4&gt;知识图谱链接预测需要整合结构信息和语义上下文来推断缺失实体。大型语言模型虽有强大生成推理能力，但对结构信号利用有限，导致结构稀疏性和语义模糊性问题，尤其在不完整或少样本场景下更为突出。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用结构信息的框架，解决大型语言模型在知识图谱链接预测中的局限性，提高在不完整或零样本设置下的预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出SLiNT框架，将知识图谱衍生的结构上下文注入冻结的LLM主干中，使用LoRA进行轻量级适应。包含三个核心组件：结构引导的邻域增强(SGNE)检索伪邻居丰富稀疏实体；动态难对比学习(DHCL)通过插值难正负样本引入细粒度监督；梯度解耦双注入(GDDI)执行标记级别结构感知干预同时保留核心LLM参数。&lt;h4&gt;主要发现&lt;/h4&gt;在WN18RR和FB15k-237数据集上的实验表明，SLiNT与基于嵌入和基于生成的基线相比实现了优越或具有竞争力的性能，验证了结构感知表示学习对可扩展知识图谱补全的有效性。&lt;h4&gt;结论&lt;/h4&gt;SLiNT框架通过结构感知的学习方法，成功解决了大型语言模型在知识图谱链接预测中的局限性，提高了在不完整或零样本设置下的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱中的链接预测需要整合结构信息和语义上下文来推断缺失的实体。虽然大型语言模型提供了强大的生成推理能力，但它们对结构信号的有限利用通常会导致结构稀疏性和语义模糊性，特别是在不完整或少样本设置下。为解决这些挑战，我们提出了SLiNT（结构感知语言模型，具有注入和对比训练），这是一个模块化框架，它将知识图谱衍生的结构上下文注入到冻结的LLM主干中，并使用基于LoRA的轻量级适应进行稳健的链接预测。具体而言，结构引导的邻域增强(SGNE)检索伪邻居来丰富稀疏实体并缓解缺失上下文；动态难对比学习(DHCL)通过插值难正样本和难负样本引入细粒度监督，以解决实体级别的模糊性；梯度解耦双注入(GDDI)在保留核心LLM参数的同时执行标记级别的结构感知干预。在WN18RR和FB15k-237上的实验表明，与基于嵌入和基于生成的基线相比，SLiNT实现了优越或具有竞争力的性能，证明了结构感知表示学习对可扩展知识图谱补全的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction in knowledge graphs requires integrating structuralinformation and semantic context to infer missing entities. While largelanguage models offer strong generative reasoning capabilities, their limitedexploitation of structural signals often results in structural sparsity andsemantic ambiguity, especially under incomplete or zero-shot settings. Toaddress these challenges, we propose SLiNT (Structure-aware Language model withInjection and coNtrastive Training), a modular framework that injectsknowledge-graph-derived structural context into a frozen LLM backbone withlightweight LoRA-based adaptation for robust link prediction. Specifically,Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors toenrich sparse entities and mitigate missing context; Dynamic Hard ContrastiveLearning (DHCL) introduces fine-grained supervision by interpolating hardpositives and negatives to resolve entity-level ambiguity; andGradient-Decoupled Dual Injection (GDDI) performs token-level structure-awareintervention while preserving the core LLM parameters. Experiments on WN18RRand FB15k-237 show that SLiNT achieves superior or competitive performancecompared with both embedding-based and generation-based baselines,demonstrating the effectiveness of structure-aware representation learning forscalable knowledge graph completion.</description>
      <author>example@mail.com (Mengxue Yang, Chun Yang, Jiaqi Zhu, Jiafan Li, Jingqi Zhang, Yuyang Li, Ying Li)</author>
      <guid isPermaLink="false">2509.06531v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于抗体结合位点预测。该方法整合了五种生物学基础模态，并通过自适应模态融合和监督对比学习提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的基于序列或结构的方法依赖于单一视图特征，无法识别抗原上的抗体特异性结合位点，这在表示和预测方面存在双重限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服现有方法局限性的新型抗体结合位点预测方法，通过多模态特征融合和自适应推理提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CAME-AB框架，整合五种生物学基础模态（原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图），采用自适应模态融合模块动态加权每种模态，结合Transformer编码器和MoE模块促进特征专业化，并通过监督对比学习塑造潜在空间几何结构。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的实验表明，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究验证了各架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过多模态特征融合和自适应推理成功克服了现有抗体结合位点预测方法的局限性，实现了更高的预测准确性。模型实现细节和代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单一视图特征，无法识别抗原上的抗体特异性结合位点——这是表示和预测方面的双重限制。在本文中，我们提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB整合了五种生物学基础模态，包括原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图，形成统一的 multimodal 表示。为了增强自适应跨模态推理，我们提出了一个自适应模态融合模块，该模块学习根据全局相关性和输入特定贡献动态加权每种模态。Transformer编码器与MoE模块相结合进一步促进了特征专业化和能力扩展。我们还结合了监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在多个指标上持续优于强基线方法，包括精确率、召回率、F1分数、AUC-ROC和MCC。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在 https://anonymous.4open.science/r/CAME-AB-C525 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens-a dual limitation in representation andprediction. In this paper, we propose CAME-AB, a novel Cross-modality Attentionframework with a Mixture-of-Experts (MoE) backbone for robust antibody bindingsite prediction. CAME-AB integrates five biologically grounded modalities,including raw amino acid encodings, BLOSUM substitution profiles, pretrainedlanguage model embeddings, structure-aware features, and GCN-refinedbiochemical graphs-into a unified multimodal representation. To enhanceadaptive cross-modal reasoning, we propose an adaptive modality fusion modulethat learns to dynamically weight each modality based on its global relevanceand input-specific contribution. A Transformer encoder combined with an MoEmodule further promotes feature specialization and capacity expansion. Weadditionally incorporate a supervised contrastive learning objective toexplicitly shape the latent space geometry, encouraging intra-class compactnessand inter-class separability. To improve optimization stability andgeneralization, we apply stochastic weight averaging during training. Extensiveexperiments on benchmark antibody-antigen datasets demonstrate that CAME-ABconsistently outperforms strong baselines on multiple metrics, includingPrecision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies furthervalidate the effectiveness of each architectural component and the benefit ofmultimodal feature integration. The model implementation details and the codesare available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.06306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的Memory-guided Consistency-aware Contrastive Learning (MCCL)框架，用于解决视频领域的广义类别发现(Video-GCD)问题。该方法通过整合时空信息，显著提升了在视频中发现新类别的性能。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现(GCD)是一个新兴且具有挑战性的开放世界问题。现有方法主要关注静态图像中的类别发现，但仅依靠静态视觉内容往往不足以可靠地发现新类别。&lt;h4&gt;目的&lt;/h4&gt;将GCD问题扩展到视频领域，引入Video-GCD新设置，有效整合时间上的多视角信息，以准确发现视频中的新类别。&lt;h4&gt;方法&lt;/h4&gt;提出MCCL框架，包含两个核心组件：1)一致性感知对比学习(CACL)，利用多视角时间特征估计未标记实例间的一致性分数并加权对比损失；2)记忆引导的表示增强(MGRE)，通过双重级别内存缓冲区提供全局上下文，增强类内紧凑性和类间可分离性。两者形成相互强化的反馈循环。&lt;h4&gt;主要发现&lt;/h4&gt;构建了新的Video-GCD基准数据集，包括动作识别和鸟类分类视频数据集。实验表明，该方法显著优于从基于图像设置调整的竞争性GCD方法，证明了时间信息对发现视频新类别的重要性。&lt;h4&gt;结论&lt;/h4&gt;通过整合时间信息，有效解决了视频中类别发现的挑战。研究强调了时间信息在视频新类别发现中的关键作用，并将公开代码。&lt;h4&gt;翻译&lt;/h4&gt;广义类别发现(GCD)是一个新兴的、具有挑战性的开放世界问题，近年来受到越来越多的关注。大多数现有的GCD方法专注于在静态图像中发现类别。然而，仅依靠静态视觉内容通常不足以可靠地发现新类别。为了弥补这一差距，我们将GCD问题扩展到视频领域，引入了一种新的设置，称为Video-GCD。因此，有效整合时间上的多视角信息对于准确的Video-GCD至关重要。为了应对这一挑战，我们提出了一种新颖的记忆引导的一致性感知对比学习(MCCL)框架，该框架明确捕获时空线索，并通过一致性引导的投票机制将其整合到对比学习中。MCCL包含两个核心组件：一致性感知对比学习(CACL)和记忆引导的表示增强(MGRE)。CACL利用多视角时间特征来估计未标记实例之间的一致性分数，然后相应地加权对比损失。MGRE引入了一个双重级别的内存缓冲区，保持特征级别和logit级别的表示，提供全局上下文以增强类内紧凑性和类间可分离性。这反过来又完善了CACL中的一致性估计，形成了表示学习和一致性建模之间的相互强化的反馈循环。为了促进全面评估，我们构建了一个新的、具有挑战性的Video-GCD基准，包括动作识别和鸟类分类视频数据集。大量实验表明，我们的方法显著优于从基于图像设置调整的竞争性GCD方法，强调了时间信息对于在视频中发现新类别的重要性。代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) is an emerging and challengingopen-world problem that has garnered increasing attention in recent years. Mostexisting GCD methods focus on discovering categories in static images. However,relying solely on static visual content is often insufficient to reliablydiscover novel categories. To bridge this gap, we extend the GCD problem to thevideo domain and introduce a new setting, termed Video-GCD. Thus, effectivelyintegrating multi-perspective information across time is crucial for accurateVideo-GCD. To tackle this challenge, we propose a novel Memory-guidedConsistency-aware Contrastive Learning (MCCL) framework, which explicitlycaptures temporal-spatial cues and incorporates them into contrastive learningthrough a consistency-guided voting mechanism. MCCL consists of two corecomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-GuidedRepresentation Enhancement (MGRE). CACL exploits multiperspective temporalfeatures to estimate consistency scores between unlabeled instances, which arethen used to weight the contrastive loss accordingly. MGRE introduces adual-level memory buffer that maintains both feature-level and logit-levelrepresentations, providing global context to enhance intra-class compactnessand inter-class separability. This in turn refines the consistency estimationin CACL, forming a mutually reinforcing feedback loop between representationlearning and consistency modeling. To facilitate a comprehensive evaluation, weconstruct a new and challenging Video-GCD benchmark, which includes actionrecognition and bird classification video datasets. Extensive experimentsdemonstrate that our method significantly outperforms competitive GCDapproaches adapted from image-based settings, highlighting the importance oftemporal information for discovering novel categories in videos. The code willbe publicly available.</description>
      <author>example@mail.com (Zhang Jing, Pu Nan, Xie Yu Xiang, Guo Yanming, Lu Qianqi, Zou Shiwei, Yan Jie, Chen Yan)</author>
      <guid isPermaLink="false">2509.06306v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology</title>
      <link>http://arxiv.org/abs/2509.06105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept by EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的基准测试PathoHR-Bench和病理特定的视觉语言训练方案，解决了现有模型在病理图像分析中的局限性，提高了层次语义理解和组合推理能力。&lt;h4&gt;背景&lt;/h4&gt;病理图像的准确分析对自动化肿瘤诊断至关重要，但由于组织图像中高度的结构相似性和细微的形态学变化，这仍然具有挑战性。当前的视觉语言模型往往难以捕捉解释结构化病理报告所需的复杂推理能力。&lt;h4&gt;目的&lt;/h4&gt;提出PathoHR-Bench，一个新的基准测试，用于评估视觉语言模型在病理领域中的层次语义理解和组合推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入了一种病理特定的视觉语言训练方案，该方案生成增强和扰动的样本用于多模态对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;现有的视觉语言模型无法有效建模复杂的跨模态关系，限制了它们在临床环境中的应用；该方法在PathoHR-Bench和另外六个病理数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在细粒度病理表示方面是有效的，能够提高视觉语言模型在病理图像分析中的表现。&lt;h4&gt;翻译&lt;/h4&gt;病理图像的准确分析对自动化肿瘤诊断至关重要，但由于组织图像中高度的结构相似性和细微的形态学变化，这仍然具有挑战性。当前的视觉语言模型往往难以捕捉解释结构化病理报告所需的复杂推理能力。为了解决这些局限性，我们提出了PathoHR-Bench，这是一个新的基准测试，旨在评估视觉语言模型在病理领域中的层次语义理解和组合推理能力。该基准测试的结果表明，现有的视觉语言模型无法有效建模复杂的跨模态关系，从而限制了它们在临床环境中的应用。为了克服这一点，我们进一步引入了一种病理特定的视觉语言训练方案，该方案生成增强和扰动的样本用于多模态对比学习。实验评估表明，我们的方法在PathoHR-Bench和另外六个病理数据集上取得了最先进的性能，突显了它在细粒度病理表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate analysis of pathological images is essential for automated tumordiagnosis but remains challenging due to high structural similarity and subtlemorphological variations in tissue images. Current vision-language (VL) modelsoften struggle to capture the complex reasoning required for interpretingstructured pathological reports. To address these limitations, we proposePathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities inhierarchical semantic understanding and compositional reasoning within thepathology domain. Results of this benchmark reveal that existing VL models failto effectively model intricate cross-modal relationships, hence limiting theirapplicability in clinical setting. To overcome this, we further introduce apathology-specific VL training scheme that generates enhanced and perturbedsamples for multimodal contrastive learning. Experimental evaluationsdemonstrate that our approach achieves state-of-the-art performance onPathoHR-Bench and six additional pathology datasets, highlighting itseffectiveness in fine-grained pathology representation.</description>
      <author>example@mail.com (Yating Huang, Ziyan Huang, Lintao Xiang, Qijun Yang, Hujun Yin)</author>
      <guid isPermaLink="false">2509.06105v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AttriPrompt: Dynamic Prompt Composition Learning for CLIP</title>
      <link>http://arxiv.org/abs/2509.05949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AttriPrompt框架，解决了当前深度文本提示方法的两个关键限制：过度依赖对比学习目标而忽略细粒度特征优化，以及使用静态提示无法实现内容自适应。该框架通过利用CLIP视觉编码器的中间层特征增强文本语义表示，并引入属性检索模块、双流对比学习和自正则化机制，在三个基准测试上实现了高达7.37%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;提示学习方法的发展推动了更深层次的提示设计以增强模型性能，但当前深度文本提示方法存在两个关键限制：过度依赖对比学习目标优先考虑高层语义对齐而忽略细粒度特征优化；在所有输入类别中使用静态提示无法实现内容自适应。&lt;h4&gt;目的&lt;/h4&gt;解决当前深度文本提示方法的两个关键限制，提出AttriPrompt框架，通过利用CLIP视觉编码器的中间层特征来增强和 refine 文本语义表示，实现细粒度特征优化和内容自适应。&lt;h4&gt;方法&lt;/h4&gt;设计属性检索模块对每一层的视觉特征进行聚类，聚合视觉特征从提示池中检索语义相似的提示并连接到文本编码器各层输入；引入双流对比学习实现细粒度对齐；通过自正则化机制在提示文本特征和非提示文本特征间应用显式正则化约束防止过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试上的大量实验表明AttriPrompt优于最先进方法，在基础到新颖设置中实现了高达7.37%的改进；该方法在跨领域知识转移方面表现出色，使视觉语言预训练模型成为更可行的现实世界解决方案。&lt;h4&gt;结论&lt;/h4&gt;AttriPrompt框架有效解决了当前深度文本提示方法的两个关键限制，通过利用CLIP视觉编码器的中间层特征和设计属性检索模块实现了细粒度特征优化和内容自适应，自正则化机制有效防止了在有限训练数据上的过拟合，在多个基准测试上表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;提示学习方法的发展推动了更深层次的提示设计以增强模型性能。然而，当前深度文本提示方法存在两个关键限制：过度依赖对比学习目标，优先考虑高层语义对齐，而忽略细粒度特征优化；在所有输入类别中使用静态提示，无法实现内容自适应。为解决这些限制，我们提出了AttriPrompt-一个新颖的框架，通过利用CLIP视觉编码器的中间层特征来增强和 refine 文本语义表示。我们设计了一个属性检索模块，首先对每一层的视觉特征进行聚类。聚合的视觉特征从提示池中检索语义相似的提示，然后将这些提示连接到文本编码器每一层的输入中。利用提示文本特征中嵌入的分层视觉信息，我们引入双流对比学习来实现细粒度对齐。此外，我们通过在提示文本特征和非提示文本特征之间应用显式的正则化约束，引入了自正则化机制，以防止在有限训练数据上的过拟合。在三个基准测试上的大量实验证明了AttriPrompt优于最先进的方法，在基础到新颖设置中实现了高达7.37%的改进。我们方法在跨领域知识转移方面的优势，使视觉语言预训练模型成为更可行的现实世界解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The evolution of prompt learning methodologies has driven exploration ofdeeper prompt designs to enhance model performance. However, current deep textprompting approaches suffer from two critical limitations: Over-reliance onconstrastive learning objectives that prioritize high-level semantic alignment,neglecting fine-grained feature optimization; Static prompts across all inputcategories, preventing content-aware adaptation. To address these limitations,we propose AttriPrompt-a novel framework that enhances and refines textualsemantic representations by leveraging the intermediate-layer features ofCLIP's vision encoder. We designed an Attribute Retrieval module that firstclusters visual features from each layer. The aggregated visual featuresretrieve semantically similar prompts from a prompt pool, which are thenconcatenated to the input of every layer in the text encoder. Leveraginghierarchical visual information embedded in prompted text features, weintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.Furthermore, we introduce a Self-Regularization mechanism by applying explicitregularization constraints between the prompted and non-prompted text featuresto prevent overfitting on limited training data. Extensive experiments acrossthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-artmethods, achieving up to 7.37\% improvement in the base-to-novel setting. Theobserved strength of our method in cross-domain knowledge transfer positionsvision-language pre-trained models as more viable solutions for real-worldimplementation.</description>
      <author>example@mail.com (Qiqi Zhan, Shiwei Li, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2509.05949v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of the State-of-the-Art in Conversational Question Answering Systems</title>
      <link>http://arxiv.org/abs/2509.05716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 12 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对话式问答系统(ConvQA)领域的综述，提供了该领域的全面分析，包括核心组件、先进机器学习技术、大型语言模型的应用、关键数据集以及未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;对话式问答系统已成为自然语言处理(NLP)的关键领域，使机器能够进行动态和具有上下文感知能力的对话。这些能力正被应用于客户支持、教育、法律和医疗保健等多个领域，在这些领域中保持连贯且相关的对话至关重要。&lt;h4&gt;目的&lt;/h4&gt;提供对话式问答系统最先进技术的综合分析，全面概述ConvQA领域，并为该领域的未来发展提供有价值的见解。&lt;h4&gt;方法&lt;/h4&gt;论文分析了ConvQA系统的核心组件（历史选择、问题理解和答案预测），调查了先进的机器学习技术（包括强化学习、对比学习和迁移学习）以提高ConvQA的准确性和效率，并探讨了大型语言模型（如RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3）的关键作用。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型在ConvQA领域发挥着关键作用，通过数据规模扩展和架构进步展示了其影响力。论文还分析了关键的ConvQA数据集。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了ConvQA领域的全面概述，并为指导该领域的未来发展提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;对话式问答(ConvQA)系统已成为自然语言处理(NLP)领域的关键领域，推动了机器能够进行动态和具有上下文感知能力的对话的进步。这些能力正被应用于各个领域，即客户支持、教育、法律和医疗保健，在这些领域中保持连贯且相关的对话至关重要。基于最近的进展，本综述对ConvQA的最先进技术进行了全面分析。本综述首先考察了ConvQA系统的核心组件，即历史选择、问题理解和答案预测，强调了它们在确保多轮对话中的连贯性和相关性方面的相互作用。它进一步研究了先进机器学习技术的应用，包括但不限于强化学习、对比学习和迁移学习，以提高ConvQA的准确性和效率。还探讨了大型语言模型的关键作用，即RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3，从而展示了它们通过数据规模扩展和架构进步所产生的影响。此外，本综述还对关键的ConvQA数据集进行了全面分析，最后概述了开放的研究方向。总体而言，这项工作提供了对ConvQA领域的全面概述，并为指导该领域的未来进展提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conversational Question Answering (ConvQA) systems have emerged as a pivotalarea within Natural Language Processing (NLP) by driving advancements thatenable machines to engage in dynamic and context-aware conversations. Thesecapabilities are increasingly being applied across various domains, i.e.,customer support, education, legal, and healthcare where maintaining a coherentand relevant conversation is essential. Building on recent advancements, thissurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.This survey begins by examining the core components of ConvQA systems, i.e.,history selection, question understanding, and answer prediction, highlightingtheir interplay in ensuring coherence and relevance in multi-turnconversations. It further investigates the use of advanced machine learningtechniques, including but not limited to, reinforcement learning, contrastivelearning, and transfer learning to improve ConvQA accuracy and efficiency. Thepivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impactthrough data scalability and architectural advancements. Additionally, thissurvey presents a comprehensive analysis of key ConvQA datasets and concludesby outlining open research directions. Overall, this work offers acomprehensive overview of the ConvQA landscape and provides valuable insightsto guide future advancements in the field.</description>
      <author>example@mail.com (Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng)</author>
      <guid isPermaLink="false">2509.05716v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
      <link>http://arxiv.org/abs/2509.05685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MSRFormer，一个新颖的道路网络表示学习框架，通过整合多尺度空间交互来解决道路网络分析的挑战。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异质性和层次性给准确的表示学习带来了挑战。图神经网络由于同质性假设和只关注单一结构尺度，在聚合邻居节点特征时往往表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决城市道路网络异质性和层次性带来的表示学习挑战，特别是处理空间交互的流量异质性和长距离依赖问题。&lt;h4&gt;方法&lt;/h4&gt;MSRFormer框架使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度相关的空间交互区域来捕获道路网络的空间结构和流量异质性。通过采用图Transformer，MSRFormer有效地捕获了多尺度上的复杂空间依赖关系。空间交互特征通过残差连接融合，然后输入到对比学习算法中，以获得最终的道路网络表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与交通相关的任务从整合轨迹数据中获益更多，在复杂的道路网络结构中，与最具竞争力的基线方法相比，实现了高达16%的改进。&lt;h4&gt;结论&lt;/h4&gt;这项研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互的尺度效应与流量异质性之间相互作用的独特关联模式。&lt;h4&gt;翻译&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异质性和层次性给准确的表示学习带来了挑战。图神经网络，其聚合来自邻居节点的特征，常常由于其同质性假设和对单一结构尺度的关注而表现不佳。为解决这些问题，本文提出了MSRFormer，一种新颖的道路网络表示学习框架，通过解决其流量异质性和长距离依赖来整合多尺度空间交互。它使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度相关的空间交互区域以捕获道路网络的空间结构和流量异质性。通过采用图Transformer，MSRFormer有效地捕获了多尺度上的复杂空间依赖关系。空间交互特征使用残差连接融合，然后输入到对比学习算法中以推导最终的道路网络表示。在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与交通相关的任务从整合轨迹数据中获益更多，在复杂的道路网络结构中与最具竞争力的基线方法相比实现了高达16%的改进。这项研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互的尺度效应与流量异质性之间相互作用的独特关联模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description>
      <author>example@mail.com (Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You)</author>
      <guid isPermaLink="false">2509.05685v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation</title>
      <link>http://arxiv.org/abs/2509.05543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 accepted paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对比表示学习框架，通过使用修剪过的骨架序列进行预训练来增强人体动作分割。该框架利用多尺度表示和跨序列变化，并提出新颖的'Shuffle and Warp'数据增强策略，引入跨排列对比和相对顺序推理两个代理任务，构建双代理对比学习网络，显著提升动作分割性能。&lt;h4&gt;背景&lt;/h4&gt;以往的动作表示学习方法都是针对动作识别设计的，基于孤立的序列级表示，没有充分利用多尺度表示和跨序列变化的信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比表示学习框架，通过预训练增强人体动作分割性能，专注于利用多尺度表示和跨序列变化。&lt;h4&gt;方法&lt;/h4&gt;提出'Shuffle and Warp'数据增强策略，利用多动作排列；引入跨排列对比学习类内相似性和相对顺序推理学习类间上下文；构建双代理对比学习网络；在修剪过的骨架数据集上预训练，在未修剪数据集上评估。&lt;h4&gt;主要发现&lt;/h4&gt;DuoCLR在多类和多标签动作分割任务中显著优于最先进的比较方法；消融研究验证了所提出方法的每个组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过利用多尺度表示、跨序列变化、新颖的数据增强策略和双代理学习任务，有效增强了人体动作分割性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种对比表示学习框架，通过使用修剪过的（单一动作）骨架序列进行预训练来增强人体动作分割。与以往针对动作识别设计的、基于孤立序列级表示的表示学习工作不同，所提出的框架专注于利用多尺度表示和跨序列变化。更具体地说，它提出了一种新颖的数据增强策略'Shuffle and Warp'，利用多样的多动作排列。后者有效辅助了对比学习中引入的两个代理任务：跨排列对比和相对顺序推理。在优化过程中，CPC通过对比不同排列中相同动作类别的表示来学习类内相似性，而ROR通过预测两个排列之间的相对映射来推理类间上下文。这些任务共同使双代理对比学习网络能够学习针对动作分割优化的多尺度特征表示。在实验中，DuoCLR在修剪过的骨架数据集上预训练，然后在未修剪的数据集上进行评估，在多类和多标签动作分割任务中均显示出比最先进的比较方法显著的优势。最后，进行了消融研究以评估所提出方法的每个组件的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, a contrastive representation learning framework is proposed toenhance human action segmentation via pre-training using trimmed (singleaction) skeleton sequences. Unlike previous representation learning works thatare tailored for action recognition and that build upon isolated sequence-wiserepresentations, the proposed framework focuses on exploiting multi-scalerepresentations in conjunction with cross-sequence variations. Morespecifically, it proposes a novel data augmentation strategy, 'Shuffle andWarp', which exploits diverse multi-action permutations. The latter effectivelyassists two surrogate tasks that are introduced in contrastive learning: CrossPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). Inoptimization, CPC learns intra-class similarities by contrastingrepresentations of the same action class across different permutations, whileROR reasons about inter-class contexts by predicting relative mapping betweentwo permutations. Together, these tasks enable a Dual-Surrogate ContrastiveLearning (DuoCLR) network to learn multi-scale feature representationsoptimized for action segmentation. In experiments, DuoCLR is pre-trained on atrimmed skeleton dataset and evaluated on an untrimmed dataset where itdemonstrates a significant boost over state-the-art comparatives in bothmulti-class and multi-label action segmentation tasks. Lastly, ablation studiesare conducted to evaluate the effectiveness of each component of the proposedapproach.</description>
      <author>example@mail.com (Haitao Tian, Pierre Payeur)</author>
      <guid isPermaLink="false">2509.05543v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.06777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的模型无关框架，通过异步更新节点特征来解决图神经网络中的过度压缩问题，在多个数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)存在过度压缩问题，当任务需要长程交互时会出现，这是由于瓶颈限制了远距离节点间的消息传播。现有的图重连方法虽然可能表现良好，但会损害归纳偏差，导致下游任务中信息损失显著增加；而增加通道容量虽然可以克服信息瓶颈，但会增加模型的参数复杂度。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，作者提出了一种高效的模型无关框架，通过异步更新节点特征来避免传统同步消息传递GNNs的局限性。&lt;h4&gt;方法&lt;/h4&gt;该框架基于节点中心度值在每一层创建节点批次，只有属于这些批次的节点特征才会被更新。异步消息更新跨层顺序处理信息，避免了同时压缩到固定容量通道中。作者还从理论上证明了该框架比标准同步方法保持更高的特征敏感度界限。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在六个标准图数据集和两个长程数据集上应用于图分类任务，在REDDIT-BINARY和Peptides-struct上分别取得了5%和4%的显著性能提升。&lt;h4&gt;结论&lt;/h4&gt;提出的异步更新框架能够有效解决GNN中的过度压缩问题，同时保持或提高模型性能，为处理需要长程交互的图任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)存在过度压缩问题，当任务需要长程交互时会出现。这个问题源于瓶颈限制了远距离节点间的消息传播。最近，图重连方法修改边连接性，并期望在长程任务上表现良好。然而，图重连损害了归纳偏差，导致在解决下游任务时出现显著的信息损失。此外，增加通道容量虽然可以克服信息瓶颈，但会增加模型的参数复杂度。为了缓解这些缺点，我们提出了一种高效的模型无关框架，该框架异步更新节点特征，不同于传统的同步消息传递GNNs。我们的框架基于节点中心度值在每一层创建节点批次，只有属于这些批次的节点特征才会被更新。异步消息更新跨层顺序处理信息，避免了同时压缩到固定容量通道中。我们还从理论上证明，与标准同步方法相比，我们提出的框架保持了更高的特征敏感度界限。我们的框架应用于六个标准图数据集和两个长程数据集进行图分类，在REDDIT-BINARY和Peptides-struct上分别取得了5%和4%的显著性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs whentasks require long-range interactions. The problem arises from the presence ofbottlenecks that limit the propagation of messages among distant nodes.Recently, graph rewiring methods modify edge connectivity and are expected toperform well on long-range tasks. Yet, graph rewiring compromises the inductivebias, incurring significant information loss in solving the downstream task.Furthermore, increasing channel capacity may overcome information bottlenecksbut enhance the parameter complexity of the model. To alleviate theseshortcomings, we propose an efficient model-agnostic framework thatasynchronously updates node features, unlike traditional synchronous messagepassing GNNs. Our framework creates node batches in every layer based on thenode centrality values. The features of the nodes belonging to these batcheswill only get updated. Asynchronous message updates process informationsequentially across layers, avoiding simultaneous compression intofixed-capacity channels. We also theoretically establish that our proposedframework maintains higher feature sensitivity bounds compared to standardsynchronous approaches. Our framework is applied to six standard graph datasetsand two long-range datasets to perform graph classification and achievesimpressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARYand Peptides-struct, respectively.</description>
      <author>example@mail.com (Kushal Bose, Swagatam Das)</author>
      <guid isPermaLink="false">2509.06777v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.06763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VariSAC的图神经网络增强深度强化学习框架，用于解决RIS辅助的ISAC使能V2X系统中的统一可靠性和资源优化问题。&lt;h4&gt;背景&lt;/h4&gt;在车辆网络中集成可重构智能表面(RIS)与集成感知和通信(ISAC)技术可实现动态空间资源管理和实时环境适应，但车辆到基础设施(V2I)和车辆到车辆(V2V)连接要求共存，加上高度动态和异构的网络拓扑，给统一的可靠性建模和资源优化带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来确保在RIS辅助的、ISAC使能的车辆到万物(V2X)系统中实现时间连续的连接，解决V2I和V2V连接的统一可靠性问题。&lt;h4&gt;方法&lt;/h4&gt;引入连续连接比(CCR)作为统一指标；采用带有残差适配器的图神经网络编码复杂高维系统状态；使用软演员-评论家(SAC)代理联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界城市数据集上的实验表明，VariSAC在连续V2I ISAC连接和V2V传递可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;h4&gt;结论&lt;/h4&gt;VariSAC框架有效解决了V2X系统中统一可靠性和资源优化的问题，实现了在动态环境中的持久连接。&lt;h4&gt;翻译&lt;/h4&gt;在车辆网络中集成可重构智能表面(RIS)和集成感知与通信(ISAC)技术能够实现动态空间资源管理和实时环境适应。然而，车辆到基础设施(V2I)和车辆到车辆(V2V)连接要求共存，加上高度动态和异构的网络拓扑，给统一的可靠性建模和资源优化带来了显著挑战。为解决这些问题，我们提出了VariSAC，一种用于在RIS辅助的、ISAC使能的车辆到万物(V2X)系统中确保时间连续连接的图神经网络(GNN)增强深度强化学习框架。具体而言，我们引入了连续连接比(CCR)，这是一个统一指标，表征V2I连接的持续时间可靠性和V2V链路的概率传递保证，从而统一了它们的连续可靠性语义。接下来，我们采用带有残差适配器的GNN来编码复杂的高维系统状态，捕获车辆、基站(BS)和RIS节点之间的空间依赖关系。这些表示随后由软演员-评论家(SAC)代理处理，联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。在真实世界城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V传递可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of Reconfigurable Intelligent Surfaces (RIS) and IntegratedSensing and Communication (ISAC) in vehicular networks enables dynamic spatialresource management and real-time adaptation to environmental changes. However,the coexistence of distinct vehicle-to-infrastructure (V2I) andvehicle-to-vehicle (V2V) connectivity requirements, together with highlydynamic and heterogeneous network topologies, presents significant challengesfor unified reliability modeling and resource optimization. To address theseissues, we propose VariSAC, a graph neural network (GNN)-augmented deepreinforcement learning framework for assured, time-continuous connectivity inRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,we introduce the Continuous Connectivity Ratio (CCR), a unified metric thatcharacterizes the sustained temporal reliability of V2I connections and theprobabilistic delivery guarantees of V2V links, thus unifying their continuousreliability semantics. Next, we employ a GNN with residual adapters to encodecomplex, high-dimensional system states, capturing spatial dependencies amongvehicles, base stations (BS), and RIS nodes. These representations are thenprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channelallocation, power control, and RIS configurations to maximize CCR-drivenlong-term rewards. Extensive experiments on real-world urban datasetsdemonstrate that VariSAC consistently outperforms existing baselines in termsof continuous V2I ISAC connectivity and V2V delivery reliability, enablingpersistent connectivity in highly dynamic vehicular environments.</description>
      <author>example@mail.com (Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun)</author>
      <guid isPermaLink="false">2509.06763v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Long-Range Graph Wavelet Networks</title>
      <link>http://arxiv.org/abs/2509.06743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Long-Range Graph Wavelet Networks (LR-GWN)的新型图神经网络，能够有效建模图中的长程交互，解决图机器学习中的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;在图机器学习中，建模长程交互（信息跨越图中遥远部分传播）是一个核心挑战。图小波受多分辨率信号处理的启发，为捕获局部和全局结构提供了原则性的方法。然而，现有的基于小波的图神经网络依赖于有限阶多项式近似，限制了感受野并阻碍了长程传播。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获图中长程交互的图神经网络，克服现有基于小波的图神经网络的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出LR-GWN，将小波滤波器分解为互补的局部和全局组件。局部聚合通过高效的低阶多项式处理，长程交互则通过灵活的频域参数化捕获，在原则性小波框架内统一短距离和长距离信息流。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LR-GWN在长程基准测试中实现了基于小波方法的最先进性能，同时在短距离数据集上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;LR-GWN成功解决了现有基于小波的图神经网络在长程交互建模方面的局限性，通过结合局部和全局组件，实现了更有效的信息传播，同时保持了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;建模长程交互，即信息跨越图中遥远部分的传播，是图机器学习中的一个核心挑战。受多分辨率信号处理启发的图小波，为捕获局部和全局结构提供了原则性的方法。然而，现有的基于小波的图神经网络依赖于有限阶多项式近似，这限制了它们的感受野并阻碍了长程传播。我们提出了长程图小波网络(LR-GWN)，它将小波滤波器分解为互补的局部和全局组件。局部聚合通过高效的低阶多项式处理，而长程交互则通过灵活的频域参数化来捕获。这种混合设计在原则性的小波框架内统一了短距离和长距离信息流。实验表明，LR-GWN在长程基准测试中实现了基于小波方法的最先进性能，同时在短距离数据集上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling long-range interactions, the propagation of information acrossdistant parts of a graph, is a central challenge in graph machine learning.Graph wavelets, inspired by multi-resolution signal processing, provide aprincipled way to capture both local and global structures. However, existingwavelet-based graph neural networks rely on finite-order polynomialapproximations, which limit their receptive fields and hinder long-rangepropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), whichdecompose wavelet filters into complementary local and global components. Localaggregation is handled with efficient low-order polynomials, while long-rangeinteractions are captured through a flexible spectral domain parameterization.This hybrid design unifies short- and long-distance information flow within aprincipled wavelet framework. Experiments show that LR-GWN achievesstate-of-the-art performance among wavelet-based methods on long-rangebenchmarks, while remaining competitive on short-range datasets.</description>
      <author>example@mail.com (Filippo Guerranti, Fabrizio Forte, Simon Geisler, Stephan Günnemann)</author>
      <guid isPermaLink="false">2509.06743v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AnalysisGNN: Unified Music Analysis with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.06654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 17th International Symposium on Computer Music  Multidisciplinary Research (CMMR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了AnalysisGNN，一种新的图神经网络框架，用于整合异构注释的符号数据集进行音乐分析。通过数据洗牌策略、加权多任务损失和logit融合技术，结合非和弦音预测模块，实现了与现有方法相当的性能，同时提高了对域偏移和注释不一致的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;近年来计算音乐分析方法蓬勃发展，但每种方法通常针对特定的分析领域，在处理异构注释的符号数据集时存在挑战。&lt;h4&gt;目的&lt;/h4&gt;引入AnalysisGNN框架，整合异构注释的符号数据集，用于全面乐谱分析。&lt;h4&gt;方法&lt;/h4&gt;使用数据洗牌策略，采用自定义加权多任务损失，在特定任务分类器之间进行logit融合，集成非和弦音预测模块识别并排除经过音和非功能性音符，以提高标签信号的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;AnalysisGNN实现了与传统静态数据集方法相当的性能，在多个异构语料库上对域偏移和注释不一致性表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;AnalysisGNN是一种有效的框架，能够整合异构注释的数据集进行综合音乐分析，在处理域偏移和注释不一致方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;近年来，计算音乐分析方法蓬勃发展，但每种方法通常针对特定的分析领域。在这项工作中，我们引入了AnalysisGNN，一种新颖的图神经网络框架，它利用数据洗牌策略和自定义加权多任务损失，以及在特定任务分类器之间的logit融合，来整合异构注释的符号数据集，用于全面的乐谱分析。我们进一步集成了一个非和弦音预测模块，该模块识别并排除所有任务中的经过音和非功能性音符，从而提高标签信号的一致性。实验评估表明，AnalysisGNN实现了与传统静态数据集方法相当的性能，同时在多个异构语料库上对域偏移和注释不一致表现出更强的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have seen a boom in computational approaches to music analysis,yet each one is typically tailored to a specific analytical domain. In thiswork, we introduce AnalysisGNN, a novel graph neural network framework thatleverages a data-shuffling strategy with a custom weighted multi-task loss andlogit fusion between task-specific classifiers to integrate heterogeneouslyannotated symbolic datasets for comprehensive score analysis. We furtherintegrate a Non-Chord-Tone prediction module, which identifies and excludespassing and non-functional notes from all tasks, thereby improving theconsistency of label signals. Experimental evaluations demonstrate thatAnalysisGNN achieves performance comparable to traditional static-datasetapproaches, while showing increased resilience to domain shifts and annotationinconsistencies across multiple heterogeneous corpora.</description>
      <author>example@mail.com (Emmanouil Karystinaios, Johannes Hentschel, Markus Neuwirth, Gerhard Widmer)</author>
      <guid isPermaLink="false">2509.06654v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification</title>
      <link>http://arxiv.org/abs/2509.06600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了图神经网络(GCNs)用于归纳节点分类的PAC-Bayesian理论分析，考虑了节点作为相关且非同分布的数据点，推导了一层和两层GCNs的泛化边界，并建立了泛化差距收敛的条件。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理图结构数据方面取得了显著成功，但真实世界的图具有动态性，新节点不断添加，现有连接可能随时间变化。&lt;h4&gt;目的&lt;/h4&gt;建立图神经网络在动态图环境中的理论分析，特别是针对归纳节点分类任务，解决以往理论研究中无法充分建模时间演化和结构动态性的问题。&lt;h4&gt;方法&lt;/h4&gt;采用PAC-Bayesian理论框架分析图卷积网络(GCNs)，将节点视为相关且非同分布的数据点，推导泛化边界并建立收敛条件。&lt;h4&gt;主要发现&lt;/h4&gt;推导了一层GCNs的新泛化边界，明确包含数据依赖性和非平稳性的影响；建立泛化差距随节点数量增加而收敛到零的充分条件；对于两层GCNs，需要更强的图拓扑假设来保证收敛。&lt;h4&gt;结论&lt;/h4&gt;本研究为理解和改进动态图环境中图神经网络的泛化能力建立了理论基础，对处理动态图数据具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种应用中处理图结构数据方面取得了显著成功。真实世界图的一个关键方面是其动态性，新节点不断被添加，现有连接可能随时间变化。以往的理论研究主要基于归纳学习框架，无法充分建模这种时间演化和结构动态性。本文提出了图卷积网络(GCNs)用于归纳节点分类的PAC-Bayesian理论分析，将节点视为相关且非同分布的数据点。我们推导了一层GCNs的新泛化边界，明确包含了数据依赖性和非平稳性的影响，并建立了充分条件，使得泛化差距随着节点数量的增加而收敛到零。此外，我们将分析扩展到两层GCNs，并发现需要更强的图拓扑假设来保证收敛。这项工作为理解和改进动态图环境中GNN的泛化能力建立了理论基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in processinggraph-structured data across various applications. A critical aspect ofreal-world graphs is their dynamic nature, where new nodes are continuallyadded and existing connections may change over time. Previous theoreticalstudies, largely based on the transductive learning framework, fail toadequately model such temporal evolution and structural dynamics. In thispaper, we presents a PAC-Bayesian theoretical analysis of graph convolutionalnetworks (GCNs) for inductive node classification, treating nodes as dependentand non-identically distributed data points. We derive novel generalizationbounds for one-layer GCNs that explicitly incorporate the effects of datadependency and non-stationarity, and establish sufficient conditions underwhich the generalization gap converges to zero as the number of nodesincreases. Furthermore, we extend our analysis to two-layer GCNs, and revealthat it requires stronger assumptions on graph topology to guaranteeconvergence. This work establishes a theoretical foundation for understandingand improving GNN generalization in dynamic graph environments.</description>
      <author>example@mail.com (Huayi Tang, Yong Liu)</author>
      <guid isPermaLink="false">2509.06600v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology</title>
      <link>http://arxiv.org/abs/2509.06574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种多尺度框架，用于捕捉主动粒子在流体中的动力学行为，结合三种学习驱动工具协同工作。&lt;h4&gt;背景&lt;/h4&gt;主动粒子（小型自驱动粒子）在流体中移动时既能变形又能改变流体形态，这种动力学行为的模拟具有挑战性，因为它需要耦合精细尺度的流体动力学与大尺度的集体效应。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够准确预测主动粒子动力学行为的多尺度框架，解决流体动力学与集体效应耦合的复杂问题。&lt;h4&gt;方法&lt;/h4&gt;结合三种学习驱动工具：1)使用高分辨率格子玻尔兹曼快照作为输入；2)采用E(2)-等变图神经网络预测粒子间相互作用力；3)使用物理信息神经网络结合傅里叶特征映射和拓扑正则化更新力预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够提供整体的高度数据驱动的完整力网络预测，同时强调物理基础和主动物质典型的多尺度结构。&lt;h4&gt;结论&lt;/h4&gt;该多尺度框架成功捕捉了主动粒子的动力学行为，解决了流体动力学与集体效应耦合的挑战性问题。&lt;h4&gt;翻译&lt;/h4&gt;捕捉主动粒子的动力学，即小型自驱动粒子在流体中既能变形又能改变流体形态，是一个艰巨的问题，因为它需要将精细尺度的流体动力学与大尺度的集体效应耦合起来。因此，我们提出了一个多尺度框架，结合了三种学习驱动工具在一个管道中协同学习。我们使用周期性盒子中流体速度和粒子应力的高分辨率格子玻尔兹曼快照作为学习管道的输入。第二步使用粒子的形态、位置和方向来预测它们之间的成对相互作用力，采用E(2)-等变图神经网络，必须满足平面对称性。然后，物理信息神经网络通过使用傅里叶特征映射和残差块，结合应力数据对这些局部估计进行进一步更新，同时使用持久同调引入的拓扑项进行正则化，以惩罚不切实际的缠结或虚假连接。这些阶段共同提供了一个整体的高度数据驱动的完整力网络预测，强调物理基础以及主动物质典型的多尺度结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Capturing the dynamics of active particles, i.e., small self-propelled agentsthat both deform and are deformed by a fluid in which they move is a formidableproblem as it requires coupling fine scale hydrodynamics with large scalecollective effects. So we present a multi-scale framework that combines thethree learning-driven tools to learn in concert within one pipeline. We usehigh-resolution Lattice Boltzmann snapshots of fluid velocity and particlestresses in a periodic box as input to the learning pipeline. the second steptakes the morphology and positions orientations of particles to predictpairwise interaction forces between them with a E(2)-equivariant graph neuralnetwork that necessarily respect flat symmetries. Then, a physics-informedneural network further updates these local estimates by summing over them witha stress data using Fourier feature mappings and residual blocks that isadditionally regularized with a topological term (introduced by persistenthomology) to penalize unrealistically tangled or spurious connections. Inconcert, these stages deliver an holistic highly-data driven full force networkprediction empathizing on the physical underpinnings together with emergingmulti-scale structure typical for active matter.</description>
      <author>example@mail.com (Sadra Saremi, Amirhossein Ahmadkhan Kordbacheh)</author>
      <guid isPermaLink="false">2509.06574v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints</title>
      <link>http://arxiv.org/abs/2509.06395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络(GNN)的算法JCPGNN-M，用于解决无线通信系统中满足最小数据速率约束的挑战，同时满足服务质量要求。&lt;h4&gt;背景&lt;/h4&gt;无线通信系统满足最小数据速率要求是一个重大挑战，尤其是随着网络复杂性增加时。传统深度学习方法通过在损失函数中引入惩罚项和经验性调整超参数来处理这些约束，但这种方法没有理论收敛保证，且在实际场景中经常无法满足服务质量要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展且理论上有保证的解决方案，用于满足无线通信系统中的服务质量约束，同时降低计算复杂度并提高泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于WMMSE算法结构，扩展到具有QoS约束的多信道环境，得到增强的WMMSE(eWMMSE)算法；开发基于GNN的JCPGNN-M算法支持每个用户同时进行多信道分配；提出将GNN与基于拉格朗日的原始-对偶优化方法相结合的原则性框架，在拉格朗日框架内训练GNN以确保满足QoS约束并收敛到平稳点。&lt;h4&gt;主要发现&lt;/h4&gt;JCPGNN-M算法与eWMMSE性能相匹配，同时在推理速度、对更大网络泛化和在信道状态信息不完善情况下的鲁棒性方面具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来无线网络中的约束资源分配提供了一种可扩展且有理论依据的解决方案，克服了传统深度学习方法的理论局限性。&lt;h4&gt;翻译&lt;/h4&gt;在无线通信系统中满足最小数据速率要求是一个重大挑战，尤其是随着网络复杂性增长时。传统深度学习方法通常通过在损失函数中引入惩罚项和经验性调整超参数来处理这些约束。然而，这种启发式处理方法没有理论收敛保证，并且在实际场景中经常无法满足服务质量要求。基于WMMSE算法结构，我们首先将其扩展到具有QoS约束的多信道环境，得到了增强的WMMSE(eWMMSE)算法，当问题可行时，该算法可收敛到局部最优解。为了进一步降低计算复杂度并提高可扩展性，我们开发了一种基于GNN的算法JCPGNN-M，能够支持每个用户同时进行多信道分配。为了克服传统深度学习方法的局限性，我们提出了一个原则性框架，将GNN与基于拉格朗日的原始-对偶优化方法相结合。通过在拉格朗日框架内训练GNN，我们确保满足QoS约束并收敛到平稳点。广泛的模拟表明，JCPGNN-M与eWMMSE性能相匹配，同时在推理速度、对更大网络泛化和在信道状态信息不完善情况下的鲁棒性方面具有显著优势。这项工作为未来无线网络中的约束资源分配提供了一种可扩展且有理论依据的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meeting minimum data rate constraints is a significant challenge in wirelesscommunication systems, particularly as network complexity grows. Traditionaldeep learning approaches often address these constraints by incorporatingpenalty terms into the loss function and tuning hyperparameters empirically.However, this heuristic treatment offers no theoretical convergence guaranteesand frequently fails to satisfy QoS requirements in practical scenarios.Building upon the structure of the WMMSE algorithm, we first extend it to amulti-channel setting with QoS constraints, resulting in the enhanced WMMSE(eWMMSE) algorithm, which is provably convergent to a locally optimal solutionwhen the problem is feasible. To further reduce computational complexity andimprove scalability, we develop a GNN-based algorithm, JCPGNN-M, capable ofsupporting simultaneous multi-channel allocation per user. To overcome thelimitations of traditional deep learning methods, we propose a principledframework that integrates GNN with a Lagrangian-based primal-dual optimizationmethod. By training the GNN within the Lagrangian framework, we ensuresatisfaction of QoS constraints and convergence to a stationary point.Extensive simulations demonstrate that JCPGNN-M matches the performance ofeWMMSE while offering significant gains in inference speed, generalization tolarger networks, and robustness under imperfect channel state information. Thiswork presents a scalable and theoretically grounded solution for constrainedresource allocation in future wireless networks.</description>
      <author>example@mail.com (Lili Chen, Changyang She, Jingge Zhu, Jamie Evans)</author>
      <guid isPermaLink="false">2509.06395v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults</title>
      <link>http://arxiv.org/abs/2509.06289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 9 figures, plan to submit to ACM TODAES&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种统一的时空图卷积网络（ST-GCN），用于快速、准确地预测大型顺序电路中的长周期故障影响概率（FIPs），支持风险评估。该方法将门级网表建模为时空图，使用专门的时空编码器高效预测多周期FIPs，在ISCAS-89基准测试中，将仿真时间减少了10倍以上，同时保持高精度。&lt;h4&gt;背景&lt;/h4&gt;静态数据错误（SDEs）源于初始缺陷和老化，会降低安全关键系统的可靠性。功能测试可以检测与SDE相关的故障，但模拟成本高昂。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、准确的方法来预测大型顺序电路中的长周期故障影响概率，以支持定量风险评估，并减少仿真时间。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的时空图卷积网络（ST-GCN），将门级网表建模为时空图以捕获拓扑结构和信号时序，使用专门的时空编码器高效预测多周期FIPs。该方法接受可测试性指标或故障仿真的特征，允许在效率和精度之间进行权衡。&lt;h4&gt;主要发现&lt;/h4&gt;在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024）。通过预测的FIPs选择观测点可以改进对长周期、难以检测的故障的检测。该方法可扩展到SoC级测试策略优化，并适合下游电子设计自动化流程。&lt;h4&gt;结论&lt;/h4&gt;所提出的时空图卷积网络框架能够有效预测长周期故障影响概率，显著减少仿真时间，同时保持高精度，为安全关键系统的风险评估提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;静态数据错误（SDEs）源于初始缺陷和老化，会降低安全关键系统的可靠性。功能测试可以检测与SDE相关的故障，但模拟成本高昂。我们提出了一种统一的时空图卷积网络（ST-GCN），用于快速、准确地预测大型顺序电路中的长周期故障影响概率（FIPs），支持定量风险评估。门级网表被建模为时空图以捕获拓扑结构和信号时序；专门的时空编码器高效地预测多周期FIPs。在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024）。该框架接受可测试性指标或故障仿真的特征，允许在效率和精度之间进行权衡。测试点选择研究表明，通过预测的FIPs选择观测点可以改进对长周期、难以检测的故障的检测。该方法可扩展到SoC级测试策略优化，并适合下游电子设计自动化流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Silent Data Errors (SDEs) from time-zero defects and aging degradesafety-critical systems. Functional testing detects SDE-related faults but isexpensive to simulate. We present a unified spatio-temporal graph convolutionalnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impactprobabilities (FIPs) in large sequential circuits, supporting quantitative riskassessment. Gate-level netlists are modeled as spatio-temporal graphs tocapture topology and signal timing; dedicated spatial and temporal encoderspredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the methodreduces simulation time by more than 10x while maintaining high accuracy (meanabsolute error 0.024 for 5-cycle predictions). The framework accepts featuresfrom testability metrics or fault simulation, allowing efficiency-accuracytrade-offs. A test-point selection study shows that choosing observation pointsby predicted FIPs improves detection of long-cycle, hard-to-detect faults. Theapproach scales to SoC-level test strategy optimization and fits downstreamelectronic design automation flows.</description>
      <author>example@mail.com (Shaoqi Wei, Senling Wang, Hiroshi Kai, Yoshinobu Higami, Ruijun Ma, Tianming Ni, Xiaoqing Wen, Hiroshi Takahashi)</author>
      <guid isPermaLink="false">2509.06289v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations</title>
      <link>http://arxiv.org/abs/2509.06286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RecMind是一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器，通过文本条件化和图学习两种方式生成嵌入，并通过门控机制融合，在推荐任务上取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;个性化技术在消费技术、流媒体、购物、可穿戴设备和语音等领域是核心能力，但仍面临交互稀疏、内容快速更新和异构文本信号等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出RecMind，一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器，以解决个性化推荐中的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用配备轻量适配器的冻结LLM从标题、属性和评论中生成文本条件化的用户/物品嵌入；使用LightGCN主干从用户-物品图中学习协作嵌入；通过对称对比目标对齐两种视图；通过层内门控融合它们，使语言在冷启动/长尾场景中占主导，图结构在其他地方稳定排序。&lt;h4&gt;主要发现&lt;/h4&gt;在Yelp和Amazon-Electronics数据集上，RecMind在所有八个报告指标上取得了最佳结果，相对于强大的基线，相对改进最高达+4.53%（Recall@40）和+4.01%（NDCG@40）；消融研究证实了跨视图对齐的必要性以及门控相比晚期融合和仅LLM变体的优势。&lt;h4&gt;结论&lt;/h4&gt;RecMind成功地将语言模型和图神经网络结合，解决了个性化推荐中的稀疏交互、内容快速更新和异构文本信号等挑战，在多个指标上超越了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;个性化是消费技术、流媒体、购物、可穿戴设备和语音等领域的核心能力，但仍受到交互稀疏、内容快速周转和异构文本信号的挑战。我们提出了RecMind，一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器。配备轻量适配器的冻结LLM从标题、属性和评论中生成文本条件化的用户/物品嵌入；LightGCN主干从用户-物品图中学习协作嵌入。我们通过对称对比目标对齐两种视图，并通过层内门控融合它们，使语言在冷启动/长尾场景中占主导，图结构在其他地方稳定排序。在Yelp和Amazon-Electronics上，RecMind在所有八个报告指标上取得了最佳结果，相对于强大的基线，相对改进最高达+4.53%（Recall@40）和+4.01%（NDCG@40）。消融研究证实了跨视图对齐的必要性以及门控相比晚期融合和仅LLM变体的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalization is a core capability across consumer technologies, streaming,shopping, wearables, and voice, yet it remains challenged by sparseinteractions, fast content churn, and heterogeneous textual signals. We presentRecMind, an LLM-enhanced graph recommender that treats the language model as apreference prior rather than a monolithic ranker. A frozen LLM equipped withlightweight adapters produces text-conditioned user/item embeddings fromtitles, attributes, and reviews; a LightGCN backbone learns collaborativeembeddings from the user-item graph. We align the two views with a symmetriccontrastive objective and fuse them via intra-layer gating, allowing languageto dominate in cold/long-tail regimes and graph structure to stabilize rankingselsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results onall eight reported metrics, with relative improvements up to +4.53\%(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm boththe necessity of cross-view alignment and the advantage of gating over latefusion and LLM-only variants.</description>
      <author>example@mail.com (Chang Xue, Youwei Lu, Chen Yang, Jinming Xing)</author>
      <guid isPermaLink="false">2509.06286v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators</title>
      <link>http://arxiv.org/abs/2509.06154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages including references. Supplementary Information provided&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种图神经模拟器(GNS)方法，结合消息传递图神经网络和显式数值时间步进方案，通过建模瞬时时间导数来学习时间依赖的偏微分方程(PDE)解，显著提高了数据效率和预测精度。&lt;h4&gt;背景&lt;/h4&gt;神经算子(NOs)可以近似无限维函数空间之间的映射，但需要大型数据集且在训练数据稀疏时表现不佳。许多神经算子公式没有明确编码物理演化的因果性和时间局部结构。自回归模型虽然通过预测下一个时间步保持因果性，但存在误差快速累积的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效学习时间依赖PDE解的方法，解决神经算子在数据稀疏情况下的局限性，同时减少自回归模型中的误差累积问题。&lt;h4&gt;方法&lt;/h4&gt;使用图神经模拟器(GNS)，这是一种消息传递图神经网络框架，结合显式数值时间步进方案，通过建模瞬时时间导数来学习PDE解。引入了PCA+KMeans轨迹选择策略以提高低数据性能。&lt;h4&gt;主要发现&lt;/h4&gt;GNS在三个典型PDE系统(2D Burgers标量方程、2D耦合Burgers矢量方程和2D Allen-Cahn方程)上表现优异。仅使用30个训练样本(可用数据的3%)，GNS在所有系统上都实现了低于1%的相对L2误差。与基线模型相比，GNS显著减少了长时间范围内的误差累积：相对于FNO AR减少82.48%的误差，相对于DON AR减少99.86%的误差。&lt;h4&gt;结论&lt;/h4&gt;将基于图的局部归纳偏差与传统时间积分器相结合，可以为时间依赖的PDEs生成准确、物理一致且可扩展的代理模型，显著提高了数据效率和预测精度。&lt;h4&gt;翻译&lt;/h4&gt;神经算子(NOs)近似无限维函数空间之间的映射，但需要大型数据集且在训练数据稀疏时表现不佳。许多神经算子公式没有明确编码物理演化的因果性和时间局部结构。虽然自回归模型通过预测下一个时间步来保持因果性，但存在误差快速累积的问题。我们采用图神经模拟器(GNS)——一种消息传递图神经网络框架——结合显式数值时间步进方案，构建精确的前向模型，通过建模瞬时时间导数来学习PDE解。我们在三个典型的PDE系统上评估我们的框架：(1) 2D Burgers标量方程，(2) 2D耦合Burgers矢量方程，(3) 2D Allen-Cahn方程。严格的评估表明，GNS显著提高了数据效率，与DeepONet和FNO等神经算子基线相比，使用更少的训练轨迹实现了更高的泛化精度。仅使用30个训练样本(可用数据的3%)，GNS在所有三个PDE系统上都实现了低于1%的相对L2误差。在长时间范围内，GNS显著减少了误差累积：平均而言，GNS相对于FNO AR减少了82.48%的误差，相对于DON AR减少了99.86%的误差。我们引入了PCA+KMeans轨迹选择策略，提高了低数据性能。结果表明，将基于图的局部归纳偏差与传统时间积分器相结合，可以为时间依赖的PDEs生成准确、物理一致且可扩展的代理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators (NOs) approximate mappings between infinite-dimensionalfunction spaces but require large datasets and struggle with scarce trainingdata. Many NO formulations don't explicitly encode causal, local-in-timestructure of physical evolution. While autoregressive models preserve causalityby predicting next time-steps, they suffer from rapid error accumulation. Weemploy Graph Neural Simulators (GNS) - a message-passing graph neural networkframework - with explicit numerical time-stepping schemes to construct accurateforward models that learn PDE solutions by modeling instantaneous timederivatives. We evaluate our framework on three canonical PDE systems: (1) 2DBurgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2DAllen-Cahn equation. Rigorous evaluations demonstrate GNS significantlyimproves data efficiency, achieving higher generalization accuracy withsubstantially fewer training trajectories compared to neural operator baselineslike DeepONet and FNO. GNS consistently achieves under 1% relative L2 errorswith only 30 training samples out of 1000 (3% of available data) across allthree PDE systems. It substantially reduces error accumulation over extendedtemporal horizons: averaged across all cases, GNS reduces autoregressive errorby 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce aPCA+KMeans trajectory selection strategy enhancing low-data performance.Results indicate combining graph-based local inductive biases with conventionaltime integrators yields accurate, physically consistent, and scalable surrogatemodels for time-dependent PDEs.</description>
      <author>example@mail.com (Dibyajyoti Nayak, Somdatta Goswami)</author>
      <guid isPermaLink="false">2509.06154v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities</title>
      <link>http://arxiv.org/abs/2509.06041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型多阶段图神经网络架构，用于改进封闭空腔中浮力驱动热传递的模拟，解决了传统GNN在高分辨率图结构中捕获长程依赖关系的困难。&lt;h4&gt;背景&lt;/h4&gt;封闭空腔中的浮力驱动热传递是热设计的典型测试平台，高精度CFD建模虽能提供精确温度场解，但依赖专家设计的物理模型、精细网格和密集计算，限制了快速迭代。数据驱动建模，特别是图神经网络，为从模拟数据中学习热流体行为提供了新选择。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN难以在高分辨率图结构中捕获长程依赖关系的问题，开发一种能够跨多个空间尺度建模全局到局部相互作用的架构，提高热传递模拟的预测准确性和训练效率。&lt;h4&gt;方法&lt;/h4&gt;提出利用分层池化和上池化操作的多阶段GNN架构，在新开发的CFD数据集上评估该模型，该数据集模拟了不同宽高比矩形空腔中的自然对流，条件为底部壁面等温热，顶部壁面等温冷，两个垂直壁面绝热。&lt;h4&gt;主要发现&lt;/h4&gt;与传统最先进的GNN基线相比，所提出的模型实现了更高的预测准确性，提高了训练效率，并减少了长期误差累积。&lt;h4&gt;结论&lt;/h4&gt;多阶段GNN方法在基于网格的流体动力学模拟中模拟复杂热传递具有显著潜力，为热设计提供了更高效的计算工具。&lt;h4&gt;翻译&lt;/h4&gt;封闭空腔中的浮力驱动热传递作为热设计的典型测试平台，高精度CFD建模能提供精确的温度场解，但其对专家设计的物理模型、精细网格和密集计算的依赖限制了快速迭代。数据驱动建模的最新发展，特别是图神经网络，为从模拟数据中直接学习热流体行为提供了新选择，特别是在不规则网格结构上。然而，传统GNN往往难以在高分辨率图结构中捕获长程依赖关系。为克服这一局限，我们提出了一种新型多阶段GNN架构，利用分层池化和上池化操作逐步跨多个空间尺度建模全局到局部的相互作用。我们在新开发的CFD数据集上评估了所提出的模型，该数据集模拟了不同宽高比矩形空腔中的自然对流，其中底部壁面等温热，顶部壁面等温冷，两个垂直壁面绝热。实验结果表明，与最先进的GNN基线相比，所提出的模型实现了更高的预测准确性，提高了训练效率，并减少了长期误差累积。这些发现强调了所提出的多阶段GNN方法在基于网格的流体动力学模拟中模拟复杂热传递的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Buoyancy-driven heat transfer in closed cavities serves as a canonicaltestbed for thermal design High-fidelity CFD modelling yields accurate thermalfield solutions, yet its reliance on expert-crafted physics models, finemeshes, and intensive computation limits rapid iteration. Recent developmentsin data-driven modeling, especially Graph Neural Networks (GNNs), offer newalternatives for learning thermal-fluid behavior directly from simulation data,particularly on irregular mesh structures. However, conventional GNNs oftenstruggle to capture long-range dependencies in high-resolution graphstructures. To overcome this limitation, we propose a novel multi-stage GNNarchitecture that leverages hierarchical pooling and unpooling operations toprogressively model global-to-local interactions across multiple spatialscales. We evaluate the proposed model on our newly developed CFD datasetsimulating natural convection within a rectangular cavities with varying aspectratios where the bottom wall is isothermal hot, the top wall is isothermalcold, and the two vertical walls are adiabatic. Experimental resultsdemonstrate that the proposed model achieves higher predictive accuracy,improved training efficiency, and reduced long-term error accumulation comparedto state-of-the-art (SOTA) GNN baselines. These findings underscore thepotential of the proposed multi-stage GNN approach for modeling complex heattransfer in mesh-based fluid dynamics simulations.</description>
      <author>example@mail.com (Mohammad Ahangarkiasari, Hassan Pouraria)</author>
      <guid isPermaLink="false">2509.06041v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion</title>
      <link>http://arxiv.org/abs/2509.05980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一种创新的检索增强生成方法，通过构建多级别、多语义的代码图和混合图检索器，解决了LLMs在存储库级代码任务中面临的上下文限制和结构依赖问题，显著提升了代码检索和生成的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在局部代码补全方面表现出色，但在处理存储库级别的任务时存在困难，主要原因是上下文窗口有限，以及代码库之间复杂的语义和结构依赖关系。虽然检索增强生成通过检索相关代码片段缓解了上下文稀缺问题，但当前方法存在显著限制。&lt;h4&gt;目的&lt;/h4&gt;解决当前代码检索方法过度依赖文本相似性而忽略结构关系，以及通过简单连接代码片段丢失关键结构信息的问题，提高LLMs在存储库级代码任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;GRACE构建了一个多级别、多语义的代码图，统一了文件结构、抽象语法树、函数调用图、类层次结构和数据流图，以捕获静态和动态代码语义。对于检索，GRACE采用混合图检索器，结合基于图神经网络的结构相似性和文本检索，并通过基于图注意力网络的重排序器来优化拓扑相关的子图。GRACE还引入了结构融合机制，将检索到的子图与本地代码上下文合并，并保留函数调用和继承等关键依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;在公共存储库级基准上的大量实验表明，GRACE在所有指标上都显著优于最先进的方法。使用DeepSeek-V3作为骨干LLM，GRACE在每个数据集上都比最强的基于图的RAG基线高出8.19%的EM和7.51%的ES点。&lt;h4&gt;结论&lt;/h4&gt;GRACE通过综合考虑代码的文本内容和结构关系，有效解决了LLMs在存储库级代码任务中的局限性，显著提升了代码检索和生成的性能，为代码理解与生成领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在局部代码补全方面表现出色，但在处理存储库级任务时由于上下文窗口有限以及代码库间复杂的语义和结构依赖关系而表现不佳。虽然检索增强生成通过检索相关代码片段缓解了上下文稀缺问题，但当前方法存在显著限制。它们过度依赖文本相似性进行检索，忽略了调用链和继承层次结构等结构关系，并通过简单地将检索到的代码片段连接成文本序列作为LLM输入，丢失了关键的结构信息。为解决这些不足，GRACE构建了一个多级别、多语义的代码图，统一了文件结构、抽象语法树、函数调用图、类层次结构和数据流图，以捕获静态和动态代码语义。对于检索，GRACE采用混合图检索器，结合基于图神经网络的结构相似性和文本检索，并通过基于图注意力网络的重排序器来优先考虑拓扑相关的子图。为增强上下文，GRACE引入了结构融合机制，将检索到的子图与本地代码上下文合并，并保留函数调用和继承等关键依赖关系。在公共存储库级基准上的大量实验表明，GRACE在所有指标上都显著优于最先进的方法。使用DeepSeek-V3作为骨干LLM，GRACE在每个数据集上都比最强的基于图的RAG基线高出8.19%的EM和7.51%的ES点。代码可在https://anonymous.4open.science/r/grace_icse-C3D5获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLMs excel in localized code completion but struggle with repository-leveltasks due to limited context windows and complex semantic and structuraldependencies across codebases. While Retrieval-Augmented Generation (RAG)mitigates context scarcity by retrieving relevant code snippets, currentapproaches face significant limitations. They overly rely on textual similarityfor retrieval, neglecting structural relationships such as call chains andinheritance hierarchies, and lose critical structural information by naivelyconcatenating retrieved snippets into text sequences for LLM input. To addressthese shortcomings, GRACE constructs a multi-level, multi-semantic code graphthat unifies file structures, abstract syntax trees, function call graphs,class hierarchies, and data flow graphs to capture both static and dynamic codesemantics. For retrieval, GRACE employs a Hybrid Graph Retriever thatintegrates graph neural network-based structural similarity with textualretrieval, refined by a graph attention network-based re-ranker to prioritizetopologically relevant subgraphs. To enhance context, GRACE introduces astructural fusion mechanism that merges retrieved subgraphs with the local codecontext and preserves essential dependencies like function calls andinheritance. Extensive experiments on public repository-level benchmarksdemonstrate that GRACE significantly outperforms state-of-the-art methodsacross all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses thestrongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on everydataset. The code is available athttps://anonymous.4open.science/r/grace_icse-C3D5.</description>
      <author>example@mail.com (Xingliang Wang, Baoyi Wang, Chen Zhi, Junxiao Han, Xinkui Zhao, Jianwei Yin, Shuiguang Deng)</author>
      <guid isPermaLink="false">2509.05980v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification</title>
      <link>http://arxiv.org/abs/2509.05943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submit to IEEE Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DRDCAE-STGNN的新型端到端深度学习框架，用于增强运动想象脑机接口的特征学习和分类，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;运动想象脑机接口在辅助技术和神经康复方面有很大潜力，但由于其非平稳特性和低信噪比，精确高效解码运动想象仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效提升运动想象特征学习和分类性能的深度学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出判别性残差密集卷积自编码器与时空图神经网络(DRDCAE-STGNN)框架，其中DRDCAE模块利用残差密集连接通过联合重建和分类学习判别性潜在表示，STGNN模块通过可学习图邻接矩阵捕获动态空间依赖性，并使用双向长短期记忆建模时间动态。&lt;h4&gt;主要发现&lt;/h4&gt;在BCI竞赛IV 2a、2b和PhysioNet数据集上分别实现了95.42%、97.51%和90.15%的平均准确率，消融研究确认了各组件的贡献，可解释性分析揭示了神经生理学上有意义的连接模式，模型每样本推理时间为0.32毫秒。&lt;h4&gt;结论&lt;/h4&gt;该方法为MI-EEG解码提供了稳健、准确和可解释的解决方案，在受试者和任务间具有强大的泛化能力，满足潜在实时BCI应用的要求。&lt;h4&gt;翻译&lt;/h4&gt;运动想象脑机接口在辅助技术和神经康复方面具有巨大潜力。然而，由于其非平稳特性和低信噪比，运动想象的精确高效解码仍然具有挑战性。本文引入了一种新颖的端到端深度学习框架——判别性残差密集卷积自编码器与时空图神经网络，以增强运动想象特征学习和分类。具体而言，判别性残差密集卷积自编码器模块利用残差密集连接通过联合重建和分类学习判别性潜在表示，而时空图神经网络模块通过可学习的图邻接矩阵捕获动态空间依赖性，并使用双向长短期记忆建模时间动态。在BCI竞赛IV 2a、2b和PhysioNet数据集上的广泛评估展示了最先进的性能，平均准确率分别为95.42%、97.51%和90.15%。消融研究确认了每个组件的贡献，可解释性分析揭示了神经生理学上有意义的连接模式。此外，尽管模型复杂，但它保持了可行的参数计数和每样本0.32毫秒的推理时间。这些结果表明，我们的方法为运动想象脑电图解码提供了稳健、准确和可解释的解决方案，在受试者和任务间具有强大的泛化能力，并满足潜在实时脑机接口应用的要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motor imagery (MI) based brain-computer interfaces (BCIs) hold significantpotential for assistive technologies and neurorehabilitation. However, theprecise and efficient decoding of MI remains challenging due to theirnon-stationary nature and low signal-to-noise ratio. This paper introduces anovel end-to-end deep learning framework of Discriminative Residual DenseConvolutional Autoencoder with Spatio-Temporal Graph Neural Network(DRDCAE-STGNN) to enhance the MI feature learning and classification.Specifically, the DRDCAE module leverages residual-dense connections to learndiscriminative latent representations through joint reconstruction andclassifica-tion, while the STGNN module captures dynamic spatial dependenciesvia a learnable graph adjacency matrix and models temporal dynamics usingbidirectional long short-term memory (LSTM). Extensive evaluations on BCICompetition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-artperformance, with average accuracies of 95.42%, 97.51%, and 90.15%,respectively. Ablation studies confirm the contribution of each component, andinterpreta-bility analysis reveals neurophysiologically meaningful connectivitypatterns. Moreover, despite its complexity, the model maintains a feasibleparameter count and an inference time of 0.32 ms per sample. These resultsindicate that our method offers a robust, accurate, and interpretable solutionfor MI-EEG decoding, with strong generalizability across subjects and tasks andmeeting the requirements for potential real-time BCI applications.</description>
      <author>example@mail.com (Yi Wang, Haodong Zhang, Hongqi Li)</author>
      <guid isPermaLink="false">2509.05943v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the critical factors in crystal structure graph representation: a comparative analysis using streamlined MLPSets frameworks</title>
      <link>http://arxiv.org/abs/2509.05712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;数据驱动的图表示方法在材料科学和化学领域表现优异，特别是结合电子结构生成模型和多任务学习的方法，能够更全面地表示材料的复杂特性并达到接近DFT计算的精度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在材料科学和化学领域迅速发展，其性能依赖于对晶体或分子结构在五个维度上的全面表示：元素信息、几何拓扑、电子相互作用、对称性和长程相互作用。然而，现有模型在表示电子相互作用、对称性和长程信息方面仍存在局限性。&lt;h4&gt;目的&lt;/h4&gt;比较基于物理的位点特征计算器与数据驱动的图表示策略，探索更有效的材料结构表示方法。&lt;h4&gt;方法&lt;/h4&gt;比较物理方法和数据驱动方法；数据驱动方法结合电子结构生成模型（如变分自编码器VAEs，用于压缩Kohn-Sham波函数）；利用多任务学习；将CHGNet-V1/V2策略集成到DenseGNN模型中；采用预训练和微调策略。&lt;h4&gt;主要发现&lt;/h4&gt;数据驱动方法在表示完整性、收敛速度和外推能力方面表现更优越；CHGNet-V1/V2策略集成到DenseGNN模型后，在35个数据集上超越了最先进模型，预测精度接近DFT计算；预训练和微调策略显著降低了复杂无序材料带隙的预测误差。&lt;h4&gt;结论&lt;/h4&gt;数据驱动的图表示在加速材料发现方面具有优越性和潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在材料科学和化学领域迅速发展，其性能严重依赖于对晶体或分子结构在五个维度上的全面表示：元素信息、几何拓扑、电子相互作用、对称性和长程相互作用。现有模型在表示电子相互作用、对称性和长程信息方面仍存在局限性。本研究比较了基于物理的位点特征计算器与数据驱动的图表示策略。我们发现，后者通过结合电子结构生成模型（如压缩Kohn-Sham波函数的变分自编码器VAEs）和利用多任务学习，在表示完整性、收敛速度和外推能力方面实现了更优越的性能。值得注意的是，当CHGNet-V1/V2策略集成到DenseGNN模型中时，在Matbench和JARVIS-DFT的35个数据集上显著优于最先进模型，产生了接近DFT计算精度的预测结果。此外，采用预训练和微调策略显著降低了复杂无序材料带隙的预测误差，证明了数据驱动图表示在加速材料发现方面的优越性和潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决晶体结构图表示中的关键因素分析问题，特别是如何有效表示晶体结构的五个关键维度：元素信息、几何拓扑信息、电子相互作用信息、对称性信息和长程相互作用信息。这个问题重要是因为图神经网络已成为材料科学和化学领域预测材料特性的关键工具，但现有模型在电子相互作用表示方面存在不足，缺乏系统性比较研究，限制了材料发现和性质预测的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到晶体结构表示需要涵盖多个维度，发现现有方法在电子相互作用表示上有不足。他们设计了两种晶体结构图表示策略进行比较：基于物理的位点特征计算器和数据驱动的结构图表示策略。借鉴了现有工作如AGNIFingerprints、OPSiteFingerprint、CrystalNNFingerprint等物理方法，以及MEGNet、M3GNet等数据驱动模型，并创新性地整合了变分自编码器(VAE)来处理电子结构信息，开发出CHGNet系列模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是数据驱动的晶体结构图表示策略比基于物理的方法更能全面表示晶体结构，特别是通过VAE压缩Kohn-Sham波函数可以有效表示电子结构。实现流程包括：1)设计两种表示策略并进行比较；2)引入简化的MLPSets框架以最小化GNN操作干扰；3)在多个基准数据集上评估性能；4)将最优的CHGNet-V1策略应用于DenseGNN模型并进行实际应用测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统比较物理和数据驱动的两种表示策略；2)开发出CHGNet-V1/V2数据驱动模型，整合VAE处理电子结构；3)提出简化的MLPSets框架便于公平比较；4)将CHGNet-V1应用于DenseGNN显著提升性能；5)证明数据驱动方法在外推任务上的优势。相比之前工作，本文更全面地考虑了电子相互作用信息，并通过系统比较揭示了数据驱动方法的优势，特别是在小数据集和外推场景下表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过系统比较和优化晶体结构图表示策略，开发出基于数据驱动的CHGNet-V1表示方法，显著提升了材料性质预测模型的性能和泛化能力，特别是在外推任务和小数据集场景下表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks have rapidly advanced in materials science andchemistry,with their performance critically dependent on comprehensiverepresentations of crystal or molecular structures across five dimensions:elemental information, geometric topology, electronic interactions, symmetry,and long-range interactions. Existing models still exhibit limitations inrepresenting electronic interactions, symmetry, and long-range information.This study compares physics-based site feature calculators with data-drivengraph representation strategies. We find that the latter achieve superiorperformance in representation completeness, convergence speed, andextrapolation capability by incorporating electronic structure generationmodels-such as variational autoencoders (VAEs) that compress Kohn-Sham wavefunctions and leveraging multi-task learning. Notably, the CHGNet-V1/V2strategies, when integrated into the DenseGNN model,significantly outperformstate-of-the-art models across 35 datasets from Matbench and JARVIS-DFT,yielding predictions with accuracy close to that of DFT calculations.Furthermore, applying a pre-training and fine-tuning strategy substantiallyreduces the prediction error for band gaps of complex disordered materials,demonstrating the superiority and potential of data-driven graphrepresentations in accelerating materials discovery.</description>
      <author>example@mail.com (Hongwei Du, Hong Wang)</author>
      <guid isPermaLink="false">2509.05712v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</title>
      <link>http://arxiv.org/abs/2509.05671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraMFedDHAR的基于图的多模态联邦学习框架，用于人类活动识别任务。该框架将不同传感器数据建模为模态特定图，通过残差图卷积神经网络处理，并通过基于注意力的权重融合。实验结果表明，在差分隐私约束下，所提出的MultiModalGCN模型显著优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;使用多模态传感器数据进行人类活动识别面临测量噪声或不完整、标记样本稀缺以及隐私问题等挑战。传统集中式深度学习方法受限于基础设施可用性、网络延迟和数据共享限制。联邦学习虽通过本地训练解决了隐私问题，但仍需处理异构多模态数据和差分隐私要求带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的多模态联邦学习框架GraMFedDHAR，用于解决人类活动识别任务中的隐私保护和多模态数据融合问题。&lt;h4&gt;方法&lt;/h4&gt;将不同传感器流（如压力垫、深度相机和多个加速度计）建模为模态特定图，通过残差图卷积神经网络处理，使用基于注意力的权重而非简单连接来融合多模态数据，并在联邦聚合过程中应用差分隐私保护数据安全。&lt;h4&gt;主要发现&lt;/h4&gt;MultiModalGCN模型在非差分隐私设置下比基线模型高约2%的准确率；在差分隐私约束下，性能差距达7%到13%；基于图的建模在多模态学习中表现出鲁棒性，图神经网络更能抵抗差分隐私噪声带来的性能下降。&lt;h4&gt;结论&lt;/h4&gt;基于图的建模方法在多模态学习中表现出鲁棒性，特别是在差分隐私约束下。图神经网络比传统方法更能抵抗差分隐私噪声带来的性能下降，为隐私保护下的人类活动识别提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用多模态传感器数据进行人类活动识别(HAR)仍然具有挑战性，因为测量存在噪声或不完整，标记样本稀缺，以及存在隐私问题。传统的集中式深度学习方法通常受限于基础设施可用性、网络延迟和数据共享限制。虽然联邦学习(FL)通过本地训练模型并仅共享模型参数解决了隐私问题，但仍需处理使用异构多模态数据和差分隐私要求所带来的问题。在本文中，提出了一种基于图的多模态联邦学习框架GraMFedDHAR用于HAR任务。将压力垫、深度相机和多个加速度计等不同传感器流建模为模态特定图，通过残差图卷积神经网络(GCN)处理，并通过基于注意力的权重而非简单连接进行融合。融合后的嵌入使活动分类更加鲁棒，同时差分隐私在联邦聚合过程中保护数据。实验结果表明，所提出的MultiModalGCN模型在集中式和联邦范式的非差分隐私设置下，比基线MultiModalFFN模型高高达2%的准确率。更重要的是，在差分隐私约束下观察到显著改进：MultiModalGCN模型持续超越MultiModalFFN模型，性能差距根据隐私预算和设置不同，在7%到13%之间。这些结果突显了基于图的建模在多模态学习中的鲁棒性，其中图神经网络(GNNs)被证明更能抵抗差分隐私噪声引入的性能下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) using multimodal sensor data remainschallenging due to noisy or incomplete measurements, scarcity of labeledexamples, and privacy concerns. Traditional centralized deep learningapproaches are often constrained by infrastructure availability, networklatency, and data sharing restrictions. While federated learning (FL) addressesprivacy by training models locally and sharing only model parameters, it stillhas to tackle issues arising from the use of heterogeneous multimodal data anddifferential privacy requirements. In this article, a Graph-based MultimodalFederated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diversesensor streams such as a pressure mat, depth camera, and multipleaccelerometers are modeled as modality-specific graphs, processed throughresidual Graph Convolutional Neural Networks (GCNs), and fused viaattention-based weighting rather than simple concatenation. The fusedembeddings enable robust activity classification, while differential privacysafeguards data during federated aggregation. Experimental results show thatthe proposed MultiModalGCN model outperforms the baseline MultiModalFFN, withup to 2 percent higher accuracy in non-DP settings in both centralized andfederated paradigms. More importantly, significant improvements are observedunder differential privacy constraints: MultiModalGCN consistently surpassesMultiModalFFN, with performance gaps ranging from 7 to 13 percent depending onthe privacy budget and setting. These results highlight the robustness ofgraph-based modeling in multimodal learning, where GNNs prove more resilient tothe performance degradation introduced by DP noise.</description>
      <author>example@mail.com (Labani Halder, Tanmay Sen, Sarbani Palit)</author>
      <guid isPermaLink="false">2509.05671v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation</title>
      <link>http://arxiv.org/abs/2509.05550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://github.com/lizixi-0x2F/TreeGPT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了TreeGPT，一种创新的神经网络架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树。在视觉推理任务中实现了96%的准确率，显著优于其他模型，同时仅使用1.5M参数。&lt;h4&gt;背景&lt;/h4&gt;传统神经程序合成方法主要依赖顺序处理或图神经网络，在处理抽象语法树时存在局限性。ARC Prize 2025是一个需要抽象模式识别和规则推理的视觉推理基准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效处理抽象语法树的新神经网络架构，提高神经程序合成任务中的性能，特别是在视觉推理任务中。&lt;h4&gt;方法&lt;/h4&gt;TreeGPT采用混合设计，结合自注意力机制捕获局部依赖，以及TreeFeed-Forward Network通过迭代消息传递建模树结构。核心是全局父子聚合机制，使节点能通过多次迭代聚合整个树的信息。还包括门控聚合、残差连接和双向传播等增强功能。&lt;h4&gt;主要发现&lt;/h4&gt;在ARC Prize 2025数据集上，TreeGPT实现了96%的准确率，显著优于transformer基线模型(1.3%)、Grok-4(15.9%)和SOAR(52%)。消融研究表明边投影是最关键组件，边投影和门控组合实现最佳性能。&lt;h4&gt;结论&lt;/h4&gt;TreeGPT通过结合transformer注意力机制和全局父子聚合，为处理抽象语法树提供了有效方法，在视觉推理任务中取得显著成果，同时保持模型高效性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了TreeGPT，一种新颖的神经网络架构，它结合了基于transformer的注意力机制和全局父子聚合，用于处理神经程序合成任务中的抽象语法树。与仅依赖顺序处理或图神经网络的传统方法不同，TreeGPT采用混合设计，利用自注意力机制捕获局部依赖关系，并通过专门的TreeFeed-Forward Network通过迭代消息传递建模层次化树结构。核心创新在于全局父子聚合机制，使每个节点能够通过多次迭代逐步聚合整个树结构的信息。我们的架构集成了门控聚合、残差连接和双向传播等增强功能。在ARC Prize 2025数据集上的评估表明，TreeGPT实现了96%的准确率，显著优于其他基准模型，同时仅使用1.5M参数。消融研究表明边投影是最关键组件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TreeGPT, a novel neural architecture that combinestransformer-based attention mechanisms with global parent-child aggregation forprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.Unlike traditional approaches that rely solely on sequential processing orgraph neural networks, TreeGPT employs a hybrid design that leverages bothself-attention for capturing local dependencies and a specialized TreeFeed-Forward Network (TreeFFN) for modeling hierarchical tree structuresthrough iterative message passing.  The core innovation lies in our Global Parent-Child Aggregation mechanism,formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \inE_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents thehidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edgesinvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. Thisformulation enables each node to progressively aggregate information from theentire tree structure through $T$ iterations.  Our architecture integrates optional enhancements including gated aggregationwith learnable edge weights, residual connections for gradient stability, andbidirectional propagation for capturing both bottom-up and top-downdependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challengingvisual reasoning benchmark requiring abstract pattern recognition and ruleinference. Experimental results demonstrate that TreeGPT achieves 96\%accuracy, significantly outperforming transformer baselines (1.3\%),large-scale models like Grok-4 (15.9\%), and specialized program synthesismethods like SOAR (52\%) while using only 1.5M parameters. Our comprehensiveablation study reveals that edge projection is the most critical component,with the combination of edge projection and gating achieving optimalperformance.</description>
      <author>example@mail.com (Zixi Li)</author>
      <guid isPermaLink="false">2509.05550v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)</title>
      <link>http://arxiv.org/abs/2509.05447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 18 figures, accepted to IEEE Transactions on Wireless  Communications. This is the extended journal version of the conference paper  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, "Distributed Link  Sparsification for Scalable Scheduling using Graph Neural Networks," IEEE  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络(GNN)的分布式链路稀疏化方案，用于减少无线网络中的调度开销，同时保持网络容量。&lt;h4&gt;背景&lt;/h4&gt;在具有密集连接特性的无线网络中，分布式链路调度算法产生的显著信令开销会加剧拥塞、能源消耗和无线电足迹扩展等问题。&lt;h4&gt;目的&lt;/h4&gt;缓解无线网络中的信令开销问题，提出一种分布式链路稀疏化方案，减少对延迟容忍型流量的调度开销，同时保持网络容量。&lt;h4&gt;方法&lt;/h4&gt;训练一个图神经网络模块，根据流量统计和网络拓扑为单个链路调整竞争阈值，使不太可能成功的链路退出调度竞争。通过离线约束无监督学习算法平衡最小化调度开销和确保总效用达到所需水平这两个目标。&lt;h4&gt;主要发现&lt;/h4&gt;在包含多达500条链路的模拟无线多跳网络中，链路稀疏化技术有效缓解了网络拥塞，并减少了四种不同分布式链路调度协议的无线电足迹。&lt;h4&gt;结论&lt;/h4&gt;基于GNN的链路稀疏化方案是一种有效的方法，可以在保持网络容量的同时减少无线网络中的调度开销。&lt;h4&gt;翻译&lt;/h4&gt;在具有密集连接特性的无线网络中，分布式链路调度算法产生的显著信令开销会加剧拥塞、能源消耗和无线电足迹扩展等问题。为了缓解这些挑战，我们提出了一种分布式链路稀疏化方案，该方案使用图神经网络(GNNs)来减少对延迟容忍型流量的调度开销，同时保持网络容量。一个GNN模块被训练来根据流量统计和网络拓扑为单个链路调整竞争阈值，使链路在不太可能成功的情况下退出调度竞争。我们的方法通过一种新颖的离线约束无监督学习算法实现，该算法能够平衡两个相互竞争的目标：最小化调度开销，同时确保总效用达到所需水平。在包含多达500条链路的模拟无线多跳网络中，我们的链路稀疏化技术有效缓解了网络拥塞，并减少了四种不同分布式链路调度协议的无线电足迹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TWC.2025.3606741&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In wireless networks characterized by dense connectivity, the significantsignaling overhead generated by distributed link scheduling algorithms canexacerbate issues like congestion, energy consumption, and radio footprintexpansion. To mitigate these challenges, we propose a distributed linksparsification scheme employing graph neural networks (GNNs) to reducescheduling overhead for delay-tolerant traffic while maintaining networkcapacity. A GNN module is trained to adjust contention thresholds forindividual links based on traffic statistics and network topology, enablinglinks to withdraw from scheduling contention when they are unlikely to succeed.Our approach is facilitated by a novel offline constrained {unsupervised}learning algorithm capable of balancing two competing objectives: minimizingscheduling overhead while ensuring that total utility meets the required level.In simulated wireless multi-hop networks with up to 500 links, our linksparsification technique effectively alleviates network congestion and reducesradio footprints across four distinct distributed link scheduling protocols.</description>
      <author>example@mail.com (Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra)</author>
      <guid isPermaLink="false">2509.05447v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
      <link>http://arxiv.org/abs/2509.05429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acctepted by ACM CCS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)中的拓扑隐私问题，提出了一种新型的拓扑推理攻击方法，并设计了私有图重构(PGR)防御框架来保护拓扑隐私同时保持模型准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图结构数据的有力模型，但其广泛应用引发了严重的隐私问题。先前的研究主要集中在边级隐私上，而图的整体结构拓扑隐私这一关键但未被充分探索的威胁却很少受到关注。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs中的拓扑隐私风险，揭示其对图级推理攻击的脆弱性，并提出一种能够保护拓扑隐私同时保持模型准确性的防御框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一套拓扑推理攻击(TIAs)，这些攻击仅通过黑盒访问GNN模型就能重构目标训练图的结构；引入私有图重构(PGR)防御框架，将其表述为双层优化问题，使用元迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs极易受到拓扑推理攻击，现有的边级差分隐私机制要么无法缓解风险，要么严重损害模型准确性；PGR能显著减少拓扑泄露，同时对模型准确性影响最小。&lt;h4&gt;结论&lt;/h4&gt;拓扑隐私是GNNs中一个重要但被忽视的隐私问题，需要专门的防御机制来保护，而PGR框架为此提供了一个有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的有力模型。然而，它们的广泛应用引发了严重的隐私问题。尽管先前的研究主要集中在边级隐私上，但一个关键却未被充分探索的威胁在于拓扑隐私——图整体结构的保密性。在这项工作中，我们对GNNs中的拓扑隐私风险进行了全面研究，揭示了它们对图级推理攻击的脆弱性。为此，我们提出了一套拓扑推理攻击(TIAs)，这些攻击仅通过黑盒访问GNN模型就能重构目标训练图的结构。我们的研究表明，GNNs极易受到这些攻击，现有的边级差分隐私机制不足以缓解风险，因为它们要么无法减轻风险，要么严重损害模型准确性。为了应对这一挑战，我们引入了私有图重构(PGR)，这是一个新的防御框架，旨在保护拓扑隐私同时保持模型准确性。PGR被表述为一个双层优化问题，其中使用元梯度迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。大量实验证明，PGR显著减少了拓扑泄露，同时对模型准确性的影响最小。我们的代码已在 https://github.com/JeffffffFu/PGR 上匿名提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for learningfrom graph-structured data. However, their widespread adoption has raisedserious privacy concerns. While prior research has primarily focused onedge-level privacy, a critical yet underexplored threat lies in topologyprivacy - the confidentiality of the graph's overall structure. In this work,we present a comprehensive study on topology privacy risks in GNNs, revealingtheir vulnerability to graph-level inference attacks. To this end, we propose asuite of Topology Inference Attacks (TIAs) that can reconstruct the structureof a target training graph using only black-box access to a GNN model. Ourfindings show that GNNs are highly susceptible to these attacks, and thatexisting edge-level differential privacy mechanisms are insufficient as theyeither fail to mitigate the risk or severely compromise model accuracy. Toaddress this challenge, we introduce Private Graph Reconstruction (PGR), anovel defense framework designed to protect topology privacy while maintainingmodel accuracy. PGR is formulated as a bi-level optimization problem, where asynthetic training graph is iteratively generated using meta-gradients, and theGNN model is concurrently updated based on the evolving graph. Extensiveexperiments demonstrate that PGR significantly reduces topology leakage withminimal impact on model accuracy. Our code is anonymously available athttps://github.com/JeffffffFu/PGR.</description>
      <author>example@mail.com (Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang)</author>
      <guid isPermaLink="false">2509.05429v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.05397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Science Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于强化学习的框架，用于解决多机器人在共享障碍物丰富环境中的自动化任务分配、调度和运动规划问题，实现了八个机器人执行40个抓取任务的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现代机器人制造需要多个机器人在共享、障碍物丰富的环境中无碰撞地协调完成多项任务。虽然单个任务可能简单，但在时空约束下自动进行联合任务分配、调度和运动规划对于经典方法在现实规模下计算上难以处理。现有工业多臂系统依赖人工经验手动设计轨迹，过程劳动密集。&lt;h4&gt;目的&lt;/h4&gt;解决多机器人系统在复杂环境中的自动化任务和运动规划挑战，减少对人工干预的依赖，提高效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出基于强化学习的框架，使用图神经网络(GNN)策略，在程序生成的多样化环境中进行训练。该方法采用场景的图表示和图策略神经网络，通过强化学习生成多个机器人的轨迹，联合解决任务分配、调度和运动规划的子问题。&lt;h4&gt;主要发现&lt;/h4&gt;训练好的策略可以直接泛化到未见过的设置，适应不同的机器人位置、障碍物几何形状和任务姿态。解决方案的高速能力使其可用于工作单元布局优化，提高解决方案时间。&lt;h4&gt;结论&lt;/h4&gt;该规划器的速度和可扩展性为容错规划和基于在线感知的重新规划等新功能开辟了可能，特别是在需要快速适应动态任务集的场景中。&lt;h4&gt;翻译&lt;/h4&gt;现代机器人制造需要多个机器人在共享、障碍物丰富的环境中无碰撞地协调完成多项任务。虽然单个任务可能简单，但在时空约束下自动进行联合任务分配、调度和运动规划对于经典方法在现实规模下计算上难以处理。现有工业中部署的多臂系统依赖人类直觉和经验手动设计可行轨迹，这是一个劳动密集型过程。为解决这一挑战，我们提出了一种强化学习(RL)框架来实现自动化任务和运动规划，在障碍物丰富的环境中进行了测试，八个机器人在共享工作空间中执行40个抓取任务，任何机器人可以以任何顺序执行任何任务。我们的方法建立在通过强化学习在程序生成的环境中训练的图神经网络(GNN)策略基础上，这些环境具有多样化的障碍物布局、机器人配置和任务分布。它采用场景的图表示和通过强化学习训练的图策略神经网络来生成多个机器人的轨迹，联合解决任务分配、调度和运动规划的子问题。在模拟中使用大型随机生成的任务集进行训练后，我们的策略可以直接泛化到具有不同机器人位置、障碍物几何形状和任务姿态的未见设置。我们进一步证明了该解决方案的高速能力使其可用于工作单元布局优化，提高解决方案时间。该规划器的速度和可扩展性也为容错规划和基于在线感知的重新规划等新功能打开了大门，在这些场景中需要快速适应动态任务集。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在障碍物丰富的共享工作空间中，多个机器人协同完成多个任务时的自动化任务分配、调度和运动规划问题。这个问题在现实中非常重要，因为现代机器人制造需要高效协调多个机器人，而传统方法在真实世界规模上计算上不可行，现有工业系统依赖人工手动设计轨迹，耗时耗力且难以适应变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了经典方法的局限性：采样方法在复杂环境中计算复杂度呈指数增长，任务调度类似旅行商问题但更复杂，任务分配类似背包问题但成本相互依赖。然后作者提出基于深度强化学习的框架，使用图神经网络表示场景关系。他们借鉴了图神经网络处理关系信息的能力，强化学习中的TD3算法，以及后视经验回放(HER)解决稀疏奖励问题。关键创新在于将这三个子问题联合解决，而不是传统方法的分步解决。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用图神经网络表示多机器人场景，其中节点代表机器人、任务和障碍物，边代表它们之间的关系。通过强化学习训练策略网络直接控制所有机器人的关节速度，将复杂计算从在线规划转移到离线训练。整体流程包括：1)将场景表示为图；2)图神经网络接收状态输入；3)策略网络输出关节速度命令；4)模拟器执行动作并防止碰撞；5)基于任务完成和碰撞情况计算奖励；6)使用改进的TD3算法和HER训练网络；7)训练后的模型可快速生成规划并支持工作单元优化等应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图神经网络与强化学习的结合解决多机器人协调；2)联合解决任务分配、调度和运动规划三个子问题；3)模型复杂度随问题规模线性或二次增长而非指数增长；4)在随机训练环境上训练后能零样本泛化到新环境；5)规划速度极快，最大设置下每步仅需0.3毫秒。相比之前工作，传统方法需分步解决子问题且牺牲最优性，最多只能处理5个机器人和10个任务，依赖人工设计耗时轨迹，难以适应动态环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过结合图神经网络和强化学习，RoboBallet实现了在障碍物丰富的环境中高效协调多达八个机器人完成四十个任务的自动化任务和运动规划，解决了传统方法在真实世界规模上面临的计算复杂性问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1126/scirobotics.ads1204&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern robotic manufacturing requires collision-free coordination of multiplerobots to complete numerous tasks in shared, obstacle-rich workspaces. Althoughindividual tasks may be simple in isolation, automated joint task allocation,scheduling, and motion planning under spatio-temporal constraints remaincomputationally intractable for classical methods at real-world scales.Existing multi-arm systems deployed in the industry rely on human intuition andexperience to design feasible trajectories manually in a labor-intensiveprocess. To address this challenge, we propose a reinforcement learning (RL)framework to achieve automated task and motion planning, tested in anobstacle-rich environment with eight robots performing 40 reaching tasks in ashared workspace, where any robot can perform any task in any order. Ourapproach builds on a graph neural network (GNN) policy trained via RL onprocedurally-generated environments with diverse obstacle layouts, robotconfigurations, and task distributions. It employs a graph representation ofscenes and a graph policy neural network trained through reinforcement learningto generate trajectories of multiple robots, jointly solving the sub-problemsof task allocation, scheduling, and motion planning. Trained on large randomlygenerated task sets in simulation, our policy generalizes zero-shot to unseensettings with varying robot placements, obstacle geometries, and task poses. Wefurther demonstrate that the high-speed capability of our solution enables itsuse in workcell layout optimization, improving solution times. The speed andscalability of our planner also open the door to new capabilities such asfault-tolerant planning and online perception-based re-planning, where rapidadaptation to dynamic task sets is required.</description>
      <author>example@mail.com (Matthew Lai, Keegan Go, Zhibin Li, Torsten Kroger, Stefan Schaal, Kelsey Allen, Jonathan Scholz)</author>
      <guid isPermaLink="false">2509.05397v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Pothole Detection and Recognition based on Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.06750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型，用于自动检测和识别道路坑洞。该模型通过预处理技术和持续优化，在测试集上实现了97.78%至98.89%的高分类准确率，性能优于随机森林、MLP、SVM和LightGBM等其他模型。&lt;h4&gt;背景&lt;/h4&gt;随着计算机视觉和机器学习的快速发展，基于图像和视频数据的坑洞检测和识别自动化方法受到广泛关注。对道路图像进行深入分析对社会发展具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;通过特征提取对道路图像进行深入分析，实现对新图像中坑穴状况的自动识别。&lt;h4&gt;方法&lt;/h4&gt;研究团队对收集的原始数据集应用了标准化、归一化和数据增强等预处理技术，基于实验结果持续改进网络模型，构建了基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型。采用比较评估方法，将提出的模型与随机森林、MLP、SVM和LightGBM等模型进行性能比较，基于准确率、召回率、精确率、F1分数和FPS等指标进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;提出的迁移学习模型在识别速度和准确性方面表现出高性能，超越了其他模型的性能。通过仔细的参数选择和模型优化，在90个初始测试样本上实现了97.78%的分类准确率（88/90），在扩展测试集上实现了98.89%的分类准确率（890/900）。&lt;h4&gt;结论&lt;/h4&gt;构建的ResNet50-EfficientNet-RegNet模型具有高分类精度和计算效率，在坑洞检测任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;随着计算机视觉和机器学习的迅速发展，基于图像和视频数据的坑洞检测和识别自动化方法已受到显著关注。通过特征提取对道路图像进行深入分析，从而实现对新图像中坑洞状况的自动识别，对社会发展具有重要意义。因此，这是本研究解决的主要问题。基于对收集的原始数据集应用标准化、归一化和数据增强等预处理技术，我们根据实验结果持续改进网络模型。最终，我们构建了一个基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型。该模型具有高分类精度和计算效率。在模型评估方面，本研究采用比较评估方法，将提出的迁移学习模型与随机森林、MLP、SVM和LightGBM等其他模型进行比较。比较分析基于准确率、召回率、精确率、F1分数和FPS等指标，以评估本文提出的迁移学习模型的分类性能。结果表明，我们的模型在识别速度和准确性方面表现出高性能，超越了其他模型的性能。通过仔细的参数选择和模型优化，我们的迁移学习模型在90个初始测试样本集上实现了97.78%（88/90）的分类准确率，在扩展测试集上实现了98.89%（890/900）的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of computer vision and machine learning, automatedmethods for pothole detection and recognition based on image and video datahave received significant attention. It is of great significance for socialdevelopment to conduct an in-depth analysis of road images through featureextraction, thereby achieving automatic identification of the pothole conditionin new images. Consequently, this is the main issue addressed in this study.Based on preprocessing techniques such as standardization, normalization, anddata augmentation applied to the collected raw dataset, we continuouslyimproved the network model based on experimental results. Ultimately, weconstructed a deep learning feature extraction networkResNet50-EfficientNet-RegNet model based on transfer learning. This modelexhibits high classification accuracy and computational efficiency. In terms ofmodel evaluation, this study employed a comparative evaluation approach bycomparing the performance of the proposed transfer learning model with othermodels, including Random Forest, MLP, SVM, and LightGBM. The comparisonanalysis was conducted based on metrics such as Accuracy, Recall, Precision,F1-score, and FPS, to assess the classification performance of the transferlearning model proposed in this paper. The results demonstrate that our modelexhibits high performance in terms of recognition speed and accuracy,surpassing the performance of other models. Through careful parameter selectionand model optimization, our transfer learning model achieved a classificationaccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%(890/900) on the expanded test set.</description>
      <author>example@mail.com (Mang Hu, Qianqian Xia)</author>
      <guid isPermaLink="false">2509.06750v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination</title>
      <link>http://arxiv.org/abs/2509.06575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种鲁棒且自适应的光谱方法(RAS)，用于处理具有未知且可能大量污染比例的表示多任务学习，允许任务间存在异质性，无需预先了解污染水平或真实表示维度。&lt;h4&gt;背景&lt;/h4&gt;基于表示的多任务学习(MTL)通过学习任务间的共享结构提高效率，但实际应用常受污染、异常值或对抗任务阻碍。现有方法和理论假设清洁环境，在污染严重时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;处理具有未知且可能大量污染比例的表示多任务学习，同时允许内聚任务之间存在异质性。&lt;h4&gt;方法&lt;/h4&gt;提出鲁棒且自适应的光谱方法(RAS)，可以有效且高效地提炼共享的内聚表示，无需预先了解污染水平或真实表示维度。&lt;h4&gt;主要发现&lt;/h4&gt;为学习到的表示和每个任务的参数提供了非渐近误差界，这些界适应于内聚任务相似性和异常值结构，保证RAS至少与单任务学习一样好，防止负迁移；将框架扩展到迁移学习，为目标任务提供理论保证。&lt;h4&gt;结论&lt;/h4&gt;大量实验证实了理论，展示了RAS的鲁棒性和自适应性，以及在高达80%任务污染情况下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;基于表示的多任务学习(MTL)通过学习任务间的共享结构提高效率，但其实际应用常受到污染、异常值或对抗任务的阻碍。大多数现有方法和理论假设清洁或接近清洁的环境，在污染严重时表现不佳。本文处理具有未知且可能大量污染比例的表示多任务学习，同时允许内聚任务之间存在异质性。我们提出了一种鲁棒且自适应的光谱方法(RAS)，可以有效且高效地提炼共享的内聚表示，无需预先了解污染水平或真实表示的维度。理论上，我们为学习到的表示和每个任务的参数提供了非渐近误差界。这些界适应于内聚任务相似性和异常值结构，并保证RAS至少与单任务学习一样好，从而防止负迁移。我们将框架扩展到迁移学习，为目标任务提供了相应的理论保证。大量实验证实了我们的理论，展示了RAS的鲁棒性和自适应性，以及在高达80%任务污染情况下的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation-based multi-task learning (MTL) improves efficiency bylearning a shared structure across tasks, but its practical application isoften hindered by contamination, outliers, or adversarial tasks. Most existingmethods and theories assume a clean or near-clean setting, failing whencontamination is significant. This paper tackles representation MTL with anunknown and potentially large contamination proportion, while also allowing forheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectralmethod (RAS) that can distill the shared inlier representation effectively andefficiently, while requiring no prior knowledge of the contamination level orthe true representation dimension. Theoretically, we provide non-asymptoticerror bounds for both the learned representation and the per-task parameters.These bounds adapt to inlier task similarity and outlier structure, andguarantee that RAS performs at least as well as single-task learning, thuspreventing negative transfer. We also extend our framework to transfer learningwith corresponding theoretical guarantees for the target task. Extensiveexperiments confirm our theory, showcasing the robustness and adaptivity ofRAS, and its superior performance in regimes with up to 80\% taskcontamination.</description>
      <author>example@mail.com (Yian Huang, Yang Feng, Zhiliang Ying)</author>
      <guid isPermaLink="false">2509.06575v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2509.06506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种名为LPC-FT的激光雷达点云特征传输系统，通过机器联觉实现高效、鲁棒且适用的点云传输，支持多智能体协作感知&lt;h4&gt;背景&lt;/h4&gt;协作感知通过学习智能体之间如何共享信息来实现更准确和全面的场景理解，激光雷达点云提供了必要的精确空间数据。由于激光雷达传感器产生的大量数据，高效的点云传输对于低延迟多智能体协作至关重要&lt;h4&gt;目的&lt;/h4&gt;提出一个高效、鲁棒且适用的激光雷达点云传输系统，通过机器联觉(SoM)实现，称为激光雷达点云特征传输(LPC-FT)，用于支持多智能体之间的协作感知&lt;h4&gt;方法&lt;/h4&gt;采用一种保持密度的深度点云压缩方法，将完整点云编码为下采样高效表示；设计基于自注意力的信道编码模块，以增强激光雷达点云特征；设计基于交叉注意力的特征融合模块，集成收发器特征；利用非线性激活层和迁移学习来改善存在数字信道噪声时的深度神经网络训练&lt;h4&gt;主要发现&lt;/h4&gt;所提出的LPC-FT比基于八叉树的传统压缩后跟信道编码更鲁棒和有效；优于最先进的基于深度学习的压缩技术和现有的语义通信方法；将Chamfer距离减少了30%，平均提高了PSNR 1.9 dB&lt;h4&gt;结论&lt;/h4&gt;由于其优越的重构性能和对信道变化的鲁棒性，LPC-FT有望支持协作感知任务&lt;h4&gt;翻译&lt;/h4&gt;协作感知通过学习智能体之间如何共享信息来实现更准确和全面的场景理解，激光雷达点云提供了必要的精确空间数据。由于激光雷达传感器产生的大量数据，高效的点云传输对于低延迟多智能体协作至关重要。在这项工作中，我们通过机器联觉(SoM)提出了一种高效、鲁棒且适用的激光雷达点云传输系统，称为激光雷达点云特征传输(LPC-FT)，以支持多智能体之间的协作感知。具体来说，我们采用一种保持密度的深度点云压缩方法，将完整点云编码为下采样高效表示。为了减轻无线信道的影响，我们设计了一个基于自注意力的信道编码模块来增强激光雷达点云特征，以及一个基于交叉注意力的特征融合模块来集成收发器的特征。此外，我们利用非线性激活层和迁移学习来改善存在数字信道噪声时的深度神经网络训练。实验结果表明，所提出的LPC-FT比传统的基于八叉树的压缩后跟信道编码更鲁棒和有效，并优于最先进的基于深度学习的压缩技术和现有的语义通信方法，平均将Chamfer距离减少了30%，将PSNR提高了1.9 dB。由于其优越的重构性能和对信道变化的鲁棒性，LPC-FT有望支持协作感知任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云在多智能体协作感知中的高效传输问题。由于LiDAR传感器产生大量数据，且无线信道噪声会影响传输质量，现有方法难以高效传输点云数据。这个问题在自动驾驶和智能机器人系统中非常重要，因为协作感知能克服单一智能体的视野限制，而LiDAR点云提供了精确的空间数据，对环境理解至关重要。传输效率直接影响实时协作感知的性能，点云重建质量又直接影响感知准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了协作感知中点云传输面临的三大挑战：高效表示非均匀点云、抵抗无线信道干扰、处理数字通信系统中的非可微操作。他们借鉴了机器联觉(SoM)概念，设计任务感知的特征提取和传输方法。具体借鉴了密度保持的点云压缩方法作为特征编码器基础，利用注意力机制设计通道编码器和特征融合模块，并采用直通估计器(STE)处理数字通信系统中的非可微操作。作者采用两阶段训练策略，先训练点云压缩网络，再训练整个传输系统，以提高训练效率和性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过语义通信方式只传输点云的关键特征而非原始数据，设计端到端的深度学习框架实现点云特征的高效提取、传输和重建，利用注意力机制增强特征表示抵抗无线信道噪声，并采用数字通信框架确保与现有通信系统的兼容性。整体流程为：1)特征编码器将原始点云压缩为紧凑特征；2)通道编码器基于自注意力机制增强特征；3)调制将特征量化为数字符号；4)无线传输通过物理信道；5)解调和通道解码；6)特征融合模块基于交叉注意力机制融合接收特征和接收端自感知特征；7)特征解码器重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出LiDAR点云特征传输系统(LPC-FT)；2)采用密度保持的点云压缩网络并添加绝对坐标位置编码；3)设计基于自注意力的通道编码器增强特征；4)提出基于交叉注意力的特征融合模块；5)采用非线性激活层处理数字通信系统中的非可微操作；6)使用迁移学习和两阶段训练策略。相比之前工作的不同：专注于LiDAR点云而非普通物体点云；采用数字通信框架而非离散时间模拟传输；结合特征增强和特征融合；考虑大规模点云的绝对坐标信息；针对无线信道噪声设计专门的增强机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于机器联觉的LiDAR点云特征传输系统，通过密度保持压缩、自注意力增强和交叉注意力融合，实现了高效、鲁棒的点云传输，显著提升了多智能体协作感知的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception enables more accurate and comprehensive sceneunderstanding by learning how to share information between agents, with LiDARpoint clouds providing essential precise spatial data. Due to the substantialdata volume generated by LiDAR sensors, efficient point cloud transmission isessential for low-latency multi-agent collaboration. In this work, we proposean efficient, robust and applicable LiDAR point cloud transmission system viathe Synesthesia of Machines (SoM), termed LiDAR Point Cloud FeatureTransmission (LPC-FT), to support collaborative perception among multipleagents. Specifically, we employ a density-preserving deep point cloudcompression method that encodes the complete point cloud into a downsampledefficient representation. To mitigate the effects of the wireless channel, wedesign a channel encoder module based on self-attention to enhance LiDAR pointcloud features and a feature fusion module based on cross-attention tointegrate features from transceivers. Furthermore, we utilize the nonlinearactivation layer and transfer learning to improve the training of deep neuralnetworks in the presence the digital channel noise. Experimental resultsdemonstrate that the proposed LPC-FT is more robust and effective thantraditional octree-based compression followed by channel coding, andoutperforms state-of-the-art deep learning-based compression techniques andexisting semantic communication methods, reducing the Chamfer Distance by 30%and improving the PSNR by 1.9 dB on average. Owing to its superiorreconstruction performance and robustness against channel variations, LPC-FT isexpected to support collaborative perception tasks.</description>
      <author>example@mail.com (Ensong Liu, Rongqing Zhang, Xiang Cheng, Jian Tang)</author>
      <guid isPermaLink="false">2509.06506v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Minimax optimal transfer learning for high-dimensional additive regression</title>
      <link>http://arxiv.org/abs/2509.06308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a draft version of the paper. All responsibilities are  assigned to the first author&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了迁移学习框架下的高维加性回归问题，通过结合目标样本和来自相关回归模型的辅助样本，提出了一种新的估计方法，并在理论和实证上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;研究者观察到来自目标群体的样本，以及来自不同但可能相关的回归模型的辅助样本，需要利用这些信息进行高维加性回归分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种在迁移学习框架下处理高维加性回归的方法，建立通用误差边界，并实现最小最优速率。&lt;h4&gt;方法&lt;/h4&gt;首先引入基于局部线性平滑的光滑反向拟合估计器的目标估计程序；然后开发一种两阶段估计方法，在迁移学习框架内提供理论和经验水平的保证。&lt;h4&gt;主要发现&lt;/h4&gt;在次魏布噪声下建立了通用误差边界，能够处理重尾误差分布；在次指数情况下达到最小最大下界；当辅助和目标分布足够接近时，实现了最小最优速率。&lt;h4&gt;结论&lt;/h4&gt;所有理论结果都得到了模拟研究和实际数据分析的支持，证明了所提出方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了迁移学习框架下高维加性回归问题，其中研究者观察到来自目标群体的样本以及来自不同但可能相关的回归模型的辅助样本。我们首先引入了基于局部线性平滑的光滑反向拟合估计器的目标估计程序。与以往工作不同，我们在次魏布噪声下建立了通用误差边界，从而能够处理重尾误差分布。在次指数情况下，我们证明估计器在正则性条件下达到最小最大下界，这需要从现有证明策略中做出重大调整。随后，我们在迁移学习框架内开发了一种新颖的两阶段估计方法，并在总体和经验水平上提供了理论保证。在通用尾部条件下推导了每个阶段的误差边界，并且我们进一步证明当辅助和目标分布足够接近时，可以实现最小最优速率。所有理论结果都得到了模拟研究和实际数据分析的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies high-dimensional additive regression under the transferlearning framework, where one observes samples from a target populationtogether with auxiliary samples from different but potentially relatedregression models. We first introduce a target-only estimation procedure basedon the smooth backfitting estimator with local linear smoothing. In contrast toprevious work, we establish general error bounds under sub-Weibull($\alpha$)noise, thereby accommodating heavy-tailed error distributions. In thesub-exponential case ($\alpha=1$), we show that the estimator attains theminimax lower bound under regularity conditions, which requires a substantialdeparture from existing proof strategies. We then develop a novel two-stageestimation method within a transfer learning framework, and provide theoreticalguarantees at both the population and empirical levels. Error bounds arederived for each stage under general tail conditions, and we furtherdemonstrate that the minimax optimal rate is achieved when the auxiliary andtarget distributions are sufficiently close. All theoretical results aresupported by simulation studies and real data analysis.</description>
      <author>example@mail.com (Seung Hyun Moon)</author>
      <guid isPermaLink="false">2509.06308v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models</title>
      <link>http://arxiv.org/abs/2509.06228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Amna-Hassan04/Fracture-Detection-Using-X-Rays-with-CNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于AI的骨折检测解决方案，使用自定义卷积神经网络(CNN)从X光图像中自动检测骨折，并在公开数据集上进行了测试和基准比较。&lt;h4&gt;背景&lt;/h4&gt;骨折是全球性的重大健康挑战，尤其在资源有限环境中，专家放射科服务有限。传统成像方法存在成本高、辐射暴露和依赖专业解读等问题。&lt;h4&gt;目的&lt;/h4&gt;开发基于AI的解决方案，用于从X光图像中自动检测骨折，并与迁移学习模型进行比较评估。&lt;h4&gt;方法&lt;/h4&gt;使用自定义卷积神经网络(CNN)，并与迁移学习模型(EfficientNetB0, MobileNetV2, ResNet50)进行基准测试。训练使用公开可用的FracAtlas数据集，包含4,083个匿名化的肌肉骨骼X光片。&lt;h4&gt;主要发现&lt;/h4&gt;自定义CNN在FracAtlas数据集上达到95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。迁移学习模型在此特定设置中表现不佳，这些结果应考虑类别不平衡和数据集限制来解释。&lt;h4&gt;结论&lt;/h4&gt;轻量级CNN在X光骨折检测方面具有前景，强调了公平基准测试、多样化数据集和临床转化的外部验证的重要性。&lt;h4&gt;翻译&lt;/h4&gt;骨折是全球面临的主要健康挑战，常导致疼痛、活动能力下降和生产力损失，特别是在资源有限的环境中，专家放射科服务有限。传统成像方法存在成本高、辐射暴露和依赖专业解读等问题。为此，我们开发了一种基于AI的解决方案，使用自定义卷积神经网络(CNN)从X光图像中自动检测骨折，并将其与迁移学习模型(包括EfficientNetB0、MobileNetV2和ResNet50)进行了基准测试。训练在公开可用的FracAtlas数据集上进行，该数据集包含4,083个匿名化的肌肉骨骼X光片。在FracAtlas数据集上，自定义CNN达到了95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。尽管迁移学习模型(EfficientNetB0、MobileNetV2、ResNet50)在此特定设置中表现不佳，但这些结果应结合类别不平衡和数据集限制来解释。这项工作突出了轻量级CNN在X光骨折检测方面的前景，并强调了公平基准测试、多样化数据集和临床转化的外部验证的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bone fractures present a major global health challenge, often resulting inpain, reduced mobility, and productivity loss, particularly in low-resourcesettings where access to expert radiology services is limited. Conventionalimaging methods suffer from high costs, radiation exposure, and dependency onspecialized interpretation. To address this, we developed an AI-based solutionfor automated fracture detection from X-ray images using a custom ConvolutionalNeural Network (CNN) and benchmarked it against transfer learning modelsincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted onthe publicly available FracAtlas dataset, comprising 4,083 anonymizedmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)performed poorly in this specific setup, these results should be interpreted inlight of class imbalance and data set limitations. This work highlights thepromise of lightweight CNNs for detecting fractures in X-rays and underscoresthe importance of fair benchmarking, diverse datasets, and external validationfor clinical translation</description>
      <author>example@mail.com (Amna Hassan, Ilsa Afzaal, Nouman Muneeb, Aneeqa Batool, Hamail Noor)</author>
      <guid isPermaLink="false">2509.06228v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Repeating vs. Non-Repeating FRBs: A Deep Learning Approach To Morphological Characterization</title>
      <link>http://arxiv.org/abs/2509.06208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 17 figures, submitted to ApJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于形态学的深度学习方法来分类快速射电暴(FRB)，通过迁移学习和预训练的ConvNext架构实现了高效准确的分类，能够区分重复暴和非重复暴。&lt;h4&gt;背景&lt;/h4&gt;基于CHIME/FRB目录2中记录的动态频谱对快速射电暴进行形态学分类，快速射电暴分为重复暴和非重复暴两类。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅基于FRB形态学特征的深度学习方法，用于区分重复暴和非重复暴。&lt;h4&gt;方法&lt;/h4&gt;使用迁移学习技术，采用预训练的ConvNext架构，将去色散动态频谱视为图像，基于时间和频谱特性及子脉冲结构关系进行分类，并使用数学模型表示来解释深度学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型实现了高分类指标，显著减少了训练时间和计算需求；CHIME重复和非重复事件间的形态学差异在目录2中仍然存在，深度学习模型利用了这些差异进行分类。&lt;h4&gt;结论&lt;/h4&gt;微调后的深度学习模型可用于推理，预测FRB形态是否类似于重复暴或非重复暴，当在更大数据集上训练时，这类推断将变得更加重要。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种深度学习方法，仅基于形态学对快速射电暴(FRB)进行分类，形态学编码在CHIME/FRB目录2中记录的动态频谱上。我们实现了迁移学习，使用预训练的ConvNext架构，利用其强大的特征提取能力。ConvNext被调整用于分类FRB的去色散动态频谱（我们将其视为图像），根据各种时间和频谱特性以及子脉冲结构之间的关系，将FRB分为两个子类之一，即重复暴和非重复暴。此外，我们还使用总强度数据的数学模型表示来解释深度学习模型。在FRB频谱图上微调预训练的ConvNet后，我们能够实现高分类指标，与从头开始训练深度学习模型（随机权重和偏差，没有任何特征提取能力）相比，显著减少了训练时间和计算能力。重要的是，我们的结果表明，CHIME重复事件和非重复事件之间的形态学差异在目录2中仍然存在，深度学习模型利用了这些差异进行分类。微调后的深度学习模型可用于推理，使我们能够预测FRB的形态是否类似于重复暴或非重复暴。当在即将出现的更大数据集上训练时，此类推断可能会变得越来越重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a deep learning approach to classify fast radio bursts (FRBs)based purely on morphology as encoded on recorded dynamic spectrum fromCHIME/FRB Catalog 2. We implemented transfer learning with a pretrainedConvNext architecture, exploiting its powerful feature extraction ability.ConvNext was adapted to classify dedispersed dynamic spectra (which we treat asimages) of the FRBs into one of the two sub-classes, i.e., repeater andnon-repeater, based on their various temporal and spectral properties andrelation between the sub-pulse structures. Additionally, we also usedmathematical model representation of the total intensity data to interpret thedeep learning model. Upon fine-tuning the pretrained ConvNext on the FRBspectrograms, we were able to achieve high classification metrics whilesubstantially reducing training time and computing power as compared totraining a deep learning model from scratch with random weights and biaseswithout any feature extraction ability. Importantly, our results suggest thatthe morphological differences between CHIME repeating and non-repeating eventspersist in Catalog 2 and the deep learning model leveraged these differencesfor classification. The fine-tuned deep learning model can be used forinference, which enables us to predict whether an FRB's morphology resemblesthat of repeaters or non-repeaters. Such inferences may become increasinglysignificant when trained on larger data sets that will exist in the nearfuture.</description>
      <author>example@mail.com (Bikash Kharel, Emmanuel Fonseca, Charanjot Brar, Afrokk Khan, Lluis Mas-Ribas, Swarali Shivraj Patil, Paul Scholz, Seth Robert Siegel, David C. Stenning)</author>
      <guid isPermaLink="false">2509.06208v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features</title>
      <link>http://arxiv.org/abs/2509.05913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ViSK-GAT的新型多模态深度学习框架，用于运动员肌肉骨骼风险评估，通过结合视觉和骨骼坐标特征实现了高准确率的风险分类。&lt;h4&gt;背景&lt;/h4&gt;肌肉骨骼障碍对运动员构成重大风险，早期风险评估对预防很重要，但现有方法大多针对受控环境设计，无法在复杂环境中可靠评估风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂环境中可靠评估运动员肌肉骨骼风险的方法，通过多模态数据融合提高风险评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出ViSK-GAT框架，结合残差块和轻量级Transformer块学习时空依赖关系，包含细粒度注意力模块(FGAM)和多模态几何对应模块(MGCM)；构建自定义多模态数据集，结合视觉数据和骨骼坐标，将样本标记为八个风险类别。&lt;h4&gt;主要发现&lt;/h4&gt;ViSK-GAT在验证和测试中分别达到93.55%和93.89%的准确率，精确度93.86%，F1得分93.85%，Cohen's Kappa和Matthews相关系数93%；回归结果显示预测概率分布的均方根误差为0.1205，平均绝对误差为0.0156；优于九种流行的迁移学习骨干方法。&lt;h4&gt;结论&lt;/h4&gt;ViSK-GAT模型推进了人工智能在肌肉骨骼风险分类领域的应用，能够在体育领域实现有影响力的早期干预。&lt;h4&gt;翻译&lt;/h4&gt;肌肉骨骼障碍对运动员构成重大风险，早期风险评估对预防很重要。然而，大多数现有方法是为受控环境设计的，由于依赖单一类型数据，无法在复杂环境中可靠评估风险。本研究提出了ViSK-GAT（视觉-骨骼几何注意力Transformer），一种新型多模态深度学习框架，使用视觉和基于骨骼坐标的特征来分类肌肉骨骼风险。此外，还构建了一个自定义多模态数据集，结合视觉数据和骨骼坐标进行风险评估。每个样本根据全身快速评估系统标记为八个风险类别。ViSK-GAT结合了残差块和轻量级Transformer块，共同学习时空依赖关系。它包含两个新颖模块：细粒度注意力模块(FGAM)，通过视觉和骨骼输入之间的交叉注意力实现精确的跨模态特征细化；以及多模态几何对应模块(MGCM)，通过将图像特征与基于坐标的表示对齐来增强跨模态一致性。ViSK-GAT在验证和测试中分别实现了93.55%和93.89%的强性能，精确度为93.86%，F1得分为93.85%，Cohen's Kappa和Matthews相关系数为93%。回归结果也表明预测概率分布的均方根误差较低，为0.1205，相应的平均绝对误差为0.0156。与九种流行的迁移学习骨干相比，ViSK-GAT始终优于先前的方法。ViSK-GAT模型推进了人工智能的实施和应用，改变了肌肉骨骼风险分类，并能够在体育领域实现有影响力的早期干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Musculoskeletal disorders pose significant risks to athletes, and assessingrisk early is important for prevention. However, most existing methods aredesigned for controlled settings and fail to reliably assess risk in complexenvironments due to their reliance on a single type of data. This researchproposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novelmultimodal deep learning framework designed to classify musculoskeletal riskusing visual and skeletal coordinate-based features. In addition, a custommultimodal dataset is constructed by combining visual data and skeletalcoordinates for risk assessment. Each sample is labeled into eight riskcategories based on the Rapid Entire Body Assessment system. ViSK-GAT combinesa Residual Block with a Lightweight Transformer Block to learn spatial andtemporal dependencies jointly. It incorporates two novel modules: theFine-Grained Attention Module (FGAM), which enables precise inter-modal featurerefinement through cross-attention between visual and skeletal inputs, and theMultimodal Geometric Correspondence Module (MGCM), which enhances cross-modalcoherence by aligning image features with coordinate-based representations.ViSK-GAT achieved strong performance with validation and test accuracies of93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. Theregression results also indicated a low Root Mean Square Error of the predictedprobability distribution of 0.1205 and a corresponding Mean Absolute Error of0.0156. Compared to nine popular transfer learning backbones, ViSK-GATconsistently outperformed previous methods. The ViSK-GAT model advancesartificial intelligence implementation and application, transformingmusculoskeletal risk classification and enabling impactful early interventionsin sports.</description>
      <author>example@mail.com (Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Tamanna Shermin, Md Rafiqul Islam, Mukhtar Hussain, Sami Azam)</author>
      <guid isPermaLink="false">2509.05913v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework</title>
      <link>http://arxiv.org/abs/2509.05886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发代理模型预测矩形微型散热器中液态钠流动的对流传热系数&lt;h4&gt;背景&lt;/h4&gt;使用计算流体动力学(CFD)对液态金属的湍流强制对流进行高保真建模既耗时又计算成本高&lt;h4&gt;目的&lt;/h4&gt;开发一种替代工具，用于液金属冷却的微型散热器的设计和优化&lt;h4&gt;方法&lt;/h4&gt;首先应用基于核的机器学习技术和浅层神经网络处理87个努塞尔数数据集，然后使用自监督物理信息神经网络和迁移学习方法提高估计性能。自监督物理信息神经网络中增加一层确定物理在损失函数中的权重，基于不确定性平衡数据和物理。迁移学习将针对水训练的浅层神经网络调整为用于钠&lt;h4&gt;主要发现&lt;/h4&gt;自监督物理信息神经网络成功估计了钠的传热率，误差约为+8%；仅使用物理进行回归时，误差保持在5%到10%之间；其他机器学习方法预测误差主要在+8%以内&lt;h4&gt;结论&lt;/h4&gt;基于机器学习的模型为液金属冷却的微型散热器的设计和优化提供了强大的替代工具&lt;h4&gt;翻译&lt;/h4&gt;开发了一种代理模型来预测矩形微型散热器中液态钠流动的对流传热系数。最初，将基于核的机器学习技术和浅层神经网络应用于包含87个矩形微型散热器中液态钠努塞尔数的数据集。随后，使用自监督物理信息神经网络和迁移学习方法提高估计性能。在自监督物理信息神经网络中，增加了一层来确定物理在损失函数中的权重，基于不确定性平衡数据和物理以获得更好的估计。对于迁移学习，将针对水训练的浅层神经网络调整为用于钠。验证结果表明，自监督物理信息神经网络成功估计了钠的传热率，误差约为+8%。仅使用物理进行回归时，误差保持在5%到10%之间。其他机器学习方法预测误差主要在+8%以内。使用计算流体动力学(CFD)对液态金属的湍流强制对流进行高保真建模既耗时又计算成本高。因此，基于机器学习的模型为液金属冷却的微型散热器的设计和优化提供了强大的替代工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A surrogate model is developed to predict the convective heat transfercoefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.Initially, kernel-based machine learning techniques and shallow neural networkare applied to a dataset with 87 Nusselt numbers for liquid sodium inrectangular miniature heat sinks. Subsequently, a self-supervisedphysics-informed neural network and transfer learning approach are used toincrease the estimation performance. In the self-supervised physics-informedneural network, an additional layer determines the weight the of physics in theloss function to balance data and physics based on their uncertainty for abetter estimation. For transfer learning, a shallow neural network trained onwater is adapted for use with Na. Validation results show that theself-supervised physics-informed neural network successfully estimate the heattransfer rates of Na with an error margin of approximately +8%. Using onlyphysics for regression, the error remains between 5% to 10%. Other machinelearning methods specify the prediction mostly within +8%. High-fidelitymodeling of turbulent forced convection of liquid metals using computationalfluid dynamics (CFD) is both time-consuming and computationally expensive.Therefore, machine learning based models offer a powerful alternative tool forthe design and optimization of liquid-metal-cooled miniature heat sinks.</description>
      <author>example@mail.com (Reza Pirayeshshirazinezhad)</author>
      <guid isPermaLink="false">2509.05886v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Brain Tumor Detection Through Diverse CNN Architectures in IoT Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and Fully Connected CNN</title>
      <link>http://arxiv.org/abs/2509.05821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用人工智能深度学习技术在物联网医疗系统中进行脑肿瘤诊断，通过多种卷积神经网络架构对MRI图像进行分析，实现了高准确率的脑肿瘤分类。&lt;h4&gt;背景&lt;/h4&gt;人工智能驱动的深度学习在物联网医疗系统中推动了脑肿瘤诊断的发展。大脑健康对人类生活至关重要，而磁共振成像(MRI)为脑肿瘤检测提供了关键数据，是人工智能驱动图像分类的主要大数据来源。&lt;h4&gt;目的&lt;/h4&gt;使用MRI图像对胶质瘤、脑膜瘤和垂体肿瘤进行分类，并评估不同人工智能模型在脑肿瘤诊断中的性能。&lt;h4&gt;方法&lt;/h4&gt;使用基于区域的卷积神经网络(R-CNN)和UNet架构进行分类，同时应用卷积神经网络(CNN)和基于CNN的迁移学习模型如Inception-V3、EfficientNetB4和VGG19。模型性能通过F-score、召回率、精确度和准确率进行评估，并进行外部队列跨数据集验证。&lt;h4&gt;主要发现&lt;/h4&gt;Fast R-CNN取得了最佳结果，准确率达到99%，F得分为98.5%，曲线下面积为99.5%，召回率为99.4%，精确度为98.5%。在外部队列跨数据集验证中，EfficientNetB2表现最强，精确率达到92.11%，召回率为92.11%，特异性为95.96%，F1得分为92.02%，准确率为92.23%。&lt;h4&gt;结论&lt;/h4&gt;结合R-CNN、UNet和迁移学习可以在物联网医疗系统中实现更早的诊断和更有效的治疗。这些发现强调了人工智能模型在处理多样化数据时的稳健性和可靠性，有潜力增强物联网医疗环境中的脑肿瘤分类和患者护理。&lt;h4&gt;翻译&lt;/h4&gt;人工智能驱动的深度学习在物联网医疗系统中推动了脑肿瘤诊断的发展，在大数据集上实现了高准确性。大脑健康对人类生活至关重要，准确诊断对有效治疗必不可少。磁共振成像为脑肿瘤检测提供了关键数据，是人工智能驱动图像分类的主要大数据来源。在本研究中，我们使用基于区域的卷积神经网络和UNet架构从MRI图像中对胶质瘤、脑膜瘤和垂体肿瘤进行分类。我们还应用了卷积神经网络和基于CNN的迁移学习模型，如Inception-V3、EfficientNetB4和VGG19。使用F-score、召回率、精确度和准确率评估模型性能。Fast R-CNN取得了最佳结果，准确率为99%，F得分为98.5%，曲线下面积为99.5%，召回率为99.4%，精确度为98.5%。结合R-CNN、UNet和迁移学习可以在物联网医疗系统中实现更早的诊断和更有效的治疗，改善患者预后。物联网设备如可穿戴监控设备和智能成像系统持续收集实时数据，人工智能算法分析这些数据以提供即时见解，实现及时干预和个性化护理。在外部队列跨数据集验证中，在微调的EfficientNet模型中，EfficientNetB2表现最强，精确度为92.11%，召回率/敏感度为92.11%，特异性为95.96%，F1得分为92.02%，准确率为92.23%。这些发现强调了人工智能模型在处理多样化数据时的稳健性和可靠性，强化了它们在增强物联网医疗环境中的脑肿瘤分类和患者护理方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI)-powered deep learning has advanced brain tumordiagnosis in Internet of Things (IoT)-healthcare systems, achieving highaccuracy with large datasets. Brain health is critical to human life, andaccurate diagnosis is essential for effective treatment. Magnetic ResonanceImaging (MRI) provides key data for brain tumor detection, serving as a majorsource of big data for AI-driven image classification. In this study, weclassified glioma, meningioma, and pituitary tumors from MRI images usingRegion-based Convolutional Neural Network (R-CNN) and UNet architectures. Wealso applied Convolutional Neural Networks (CNN) and CNN-based transferlearning models such as Inception-V3, EfficientNetB4, and VGG19. Modelperformance was assessed using F-score, recall, precision, and accuracy. TheFast R-CNN achieved the best results with 99% accuracy, 98.5% F-score, 99.5%Area Under the Curve (AUC), 99.4% recall, and 98.5% precision. Combining R-CNN,UNet, and transfer learning enables earlier diagnosis and more effectivetreatment in IoT-healthcare systems, improving patient outcomes. IoT devicessuch as wearable monitors and smart imaging systems continuously collectreal-time data, which AI algorithms analyze to provide immediate insights fortimely interventions and personalized care. For external cohort cross-datasetvalidation, EfficientNetB2 achieved the strongest performance among fine-tunedEfficientNet models, with 92.11% precision, 92.11% recall/sensitivity, 95.96%specificity, 92.02% F1-score, and 92.23% accuracy. These findings underscorethe robustness and reliability of AI models in handling diverse datasets,reinforcing their potential to enhance brain tumor classification and patientcare in IoT healthcare environments.</description>
      <author>example@mail.com (Mohsen Asghari Ilani, Yaser M. Banad)</author>
      <guid isPermaLink="false">2509.05821v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-based Topology Optimization</title>
      <link>http://arxiv.org/abs/2509.05800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer的机器学习模型用于拓扑优化，通过类令牌机制将边界和加载条件直接嵌入域表示，实现了高效无迭代的拓扑生成。&lt;h4&gt;背景&lt;/h4&gt;拓扑优化可设计高效复杂结构，但传统迭代方法计算成本高且对初始条件敏感；现有机器学习方法要么仍为迭代式，要么难以达到真实性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种无迭代、高保真度的拓扑生成模型，通过类令牌机制将关键条件嵌入域表示，实现实时拓扑优化。&lt;h4&gt;方法&lt;/h4&gt;提出基于Transformer的机器学习模型，使用类令牌机制嵌入边界和加载条件，在静态和动态数据集上实现，采用迁移学习和FFT编码改进性能，引入辅助损失函数提高设计真实性和可制造性。&lt;h4&gt;主要发现&lt;/h4&gt;模型在合规性误差、体积分数误差、浮动材料百分比和负载差异误差等评估指标上表现良好，接近基于扩散模型的保真度。&lt;h4&gt;结论&lt;/h4&gt;所提模型实现了无迭代的高保真拓扑生成，为实时、高质量拓扑优化提供了重要进展。&lt;h4&gt;翻译&lt;/h4&gt;拓扑优化能够设计高效复杂的结构，但传统的迭代方法（如基于SIMP的方法）通常面临高计算成本和对初始条件敏感的问题。尽管机器学习方法最近在加速拓扑生成方面显示出潜力，但现有模型要么仍然是迭代的，要么难以匹配真实性能。在这项工作中，我们提出了一种基于Transformer的机器学习模型用于拓扑优化，通过类令牌机制将关键的边界和加载条件直接嵌入到令牌化的域表示中。我们在静态和动态数据集上实现了该模型，使用迁移学习和动态负载的FFT编码来改进在动态数据集上的性能。引入了辅助损失函数以提高生成设计的真实性和可制造性。我们对模型的性能进行了全面评估，包括合规性误差、体积分数误差、浮动材料百分比和负载差异误差，并将其与最先进的非迭代和迭代生成模型进行了基准测试。我们的结果表明，所提出的模型接近基于扩散模型的保真度，同时保持无迭代特性，为实时、高保真拓扑生成迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Topology optimization enables the design of highly efficient and complexstructures, but conventional iterative methods, such as SIMP-based approaches,often suffer from high computational costs and sensitivity to initialconditions. Although machine learning methods have recently shown promise foraccelerating topology generation, existing models either remain iterative orstruggle to match ground-truth performance. In this work, we propose atransformer-based machine learning model for topology optimization that embedscritical boundary and loading conditions directly into the tokenized domainrepresentation via a class token mechanism. We implement this model on staticand dynamic datasets, using transfer learning and FFT encoding of dynamic loadsto improve our performance on the dynamic dataset. Auxiliary loss functions areintroduced to promote the realism and manufacturability of the generateddesigns. We conduct a comprehensive evaluation of the model's performance,including compliance error, volume fraction error, floating materialpercentage, and load discrepancy error, and benchmark it againststate-of-the-art non-iterative and iterative generative models. Our resultsdemonstrate that the proposed model approaches the fidelity of diffusion-basedmodels while remaining iteration-free, offering a significant step towardreal-time, high-fidelity topology generation.</description>
      <author>example@mail.com (Aaron Lutheran, Srijan Das, Alireza Tabarraei)</author>
      <guid isPermaLink="false">2509.05800v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery</title>
      <link>http://arxiv.org/abs/2509.05775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print for camera ready version for IEEE EMBS BHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，通过基于因果森林学习的核函数对个体进行聚类，以识别对干预措施有不同反应的亚群体，从而估计异质性治疗效果。&lt;h4&gt;背景&lt;/h4&gt;估计异质性治疗效果在个性化医疗、资源分配和政策评估等领域至关重要，但传统聚类方法与因果推断的结合有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够识别对干预措施反应不同的亚群体，从而实现更有针对性和有效决策的方法。&lt;h4&gt;方法&lt;/h4&gt;提出两步法框架：首先使用Robinson分解的正交化学习器估计去偏的条件平均治疗效果，生成编码样本间治疗反应相似性的核矩阵；然后应用核化聚类发现不同的、对治疗敏感的亚群体，并计算聚类级别的平均CATEs。&lt;h4&gt;主要发现&lt;/h4&gt;通过半合成和真实世界数据集的实验证明，该方法能有效捕捉有意义的治疗效果异质性。&lt;h4&gt;结论&lt;/h4&gt;将核化聚类作为残差-残差回归框架中的正则化形式，为异质性治疗效果估计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;估计异质性治疗效果在个性化医疗、资源分配和政策评估等领域至关重要。核心挑战在于识别对不同干预措施有不同反应的亚群体，从而实现更有针对性和有效的决策。虽然聚类方法在无监督学习中已有深入研究，但它们与因果推断的结合仍然有限。我们提出了一种新颖的框架，该框架使用从因果森林中学习到的核函数来基于估计的治疗效果对个体进行聚类，从而揭示潜在的亚群结构。我们的方法包含两个主要步骤：首先，使用Robinson分解的正交化学习器估计去偏的条件平均治疗效果，产生一个编码样本间治疗反应相似性的核矩阵；其次，对该矩阵应用核化聚类来发现不同的、对治疗敏感的亚群体，并计算聚类级别的平均CATEs。我们将这个核化聚类步骤表述为残差-残差回归框架中的一种正则化形式。通过在半合成和真实世界数据集上的大量实验，以及消融研究和探索性分析，我们证明了我们的方法在捕捉有意义的治疗效果异质性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating heterogeneous treatment effects is critical in domains such aspersonalized medicine, resource allocation, and policy evaluation. A centralchallenge lies in identifying subpopulations that respond differently tointerventions, thereby enabling more targeted and effective decision-making.While clustering methods are well-studied in unsupervised learning, theirintegration with causal inference remains limited. We propose a novel frameworkthat clusters individuals based on estimated treatment effects using a learnedkernel derived from causal forests, revealing latent subgroup structures. Ourapproach consists of two main steps. First, we estimate debiased ConditionalAverage Treatment Effects (CATEs) using orthogonalized learners via theRobinson decomposition, yielding a kernel matrix that encodes sample-levelsimilarities in treatment responsiveness. Second, we apply kernelizedclustering to this matrix to uncover distinct, treatment-sensitivesubpopulations and compute cluster-level average CATEs. We present thiskernelized clustering step as a form of regularization within theresidual-on-residual regression framework. Through extensive experiments onsemi-synthetic and real-world datasets, supported by ablation studies andexploratory analyses, we demonstrate the effectiveness of our method incapturing meaningful treatment effect heterogeneity.</description>
      <author>example@mail.com (Zilong Wang, Turgay Ayer, Shihao Yang)</author>
      <guid isPermaLink="false">2509.05775v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
      <link>http://arxiv.org/abs/2509.05657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LM-Searcher框架，利用大语言模型进行跨领域神经架构优化，无需大量领域特定适应。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在解决复杂优化问题（包括神经架构搜索）方面取得进展，但现有方法严重依赖提示工程和领域特定调优，限制了实用性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需大量领域特定适应的框架，利用LLMs进行跨领域神经架构优化。&lt;h4&gt;方法&lt;/h4&gt;提出NCode作为神经架构的通用数值字符串表示；将NAS问题重新表述为排序任务；使用基于修剪的子空间采样策略；构建包含广泛架构-性能对的数据集促进稳健学习。&lt;h4&gt;主要发现&lt;/h4&gt;LM-Searcher在领域内（如图像分类的CNN）和领域外（如分割和生成的LoRA配置）任务中都取得有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;LM-Searcher建立了灵活且可推广的基于LLM的架构搜索新范式，数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;h4&gt;翻译&lt;/h4&gt;最近大语言模型的进展为解决复杂优化问题（包括神经架构搜索NAS）开辟了新途径。然而，现有的LLM驱动的NAS方法严重依赖提示工程和领域特定调优，限制了它们在不同任务中的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，一个新框架，它利用LLMs进行跨领域神经架构优化，无需大量领域特定适应。我们方法的核心是NCode，一种神经架构的通用数值字符串表示，它支持跨领域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLMs使用基于修剪的子空间采样策略衍生的指令调优样本，从候选池中选择高性能架构。我们策划的数据集包含广泛的架构-性能对，促进了稳健和可转移的学习。全面的实验证明，LM-Searcher在领域内（例如，图像分类的CNN）和领域外（例如，分割和生成的LoRA配置）任务中都取得了有竞争力的性能，为灵活且可推广的基于LLM的架构搜索建立了新范式。数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Large Language Models (LLMs) has opened new avenues forsolving complex optimization problems, including Neural Architecture Search(NAS). However, existing LLM-driven NAS approaches rely heavily on promptengineering and domain-specific tuning, limiting their practicality andscalability across diverse tasks. In this work, we propose LM-Searcher, a novelframework that leverages LLMs for cross-domain neural architecture optimizationwithout the need for extensive domain-specific adaptation. Central to ourapproach is NCode, a universal numerical string representation for neuralarchitectures, which enables cross-domain architecture encoding and search. Wealso reformulate the NAS problem as a ranking task, training LLMs to selecthigh-performing architectures from candidate pools using instruction-tuningsamples derived from a novel pruning-based subspace sampling strategy. Ourcurated dataset, encompassing a wide range of architecture-performance pairs,encourages robust and transferable learning. Comprehensive experimentsdemonstrate that LM-Searcher achieves competitive performance in both in-domain(e.g., CNNs for image classification) and out-of-domain (e.g., LoRAconfigurations for segmentation and generation) tasks, establishing a newparadigm for flexible and generalizable LLM-based architecture search. Thedatasets and models will be released at https://github.com/Ashone3/LM-Searcher.</description>
      <author>example@mail.com (Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li)</author>
      <guid isPermaLink="false">2509.05657v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures</title>
      <link>http://arxiv.org/abs/2509.05490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了YOLOv8和YOLOv10架构在不同层冻结策略下的性能，发现在资源受限环境中，最优冻结策略取决于数据特性而非通用规则。适当冻结可减少高达28%的GPU内存消耗，并在某些情况下超越全微调性能。&lt;h4&gt;背景&lt;/h4&gt;YOLO架构对实时目标检测至关重要，但在资源受限环境（如无人机）部署需要高效迁移学习。层冻结是常见技术，但其对当代YOLOv8和YOLOv10架构的具体影响尚未充分探索，特别是冻结深度、数据集特性和训练动态间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;填补层冻结策略对YOLOv8和YOLOv10架构影响的知识空白，提供冻结深度、数据集特性和训练动态间相互作用的详细分析，并为资源有限场景下的目标检测提供平衡迁移学习的实用、循证方法。&lt;h4&gt;方法&lt;/h4&gt;系统性地研究YOLOv8和YOLOv10变体在四个代表关键基础设施监控的挑战性数据集上的多种冻结配置，整合梯度行为分析（L2范数）和视觉解释（Grad-CAM）来提供不同冻结策略下训练动态的深入见解。&lt;h4&gt;主要发现&lt;/h4&gt;1. 没有通用最优冻结策略，最优策略取决于数据特性；2. 冻结骨干网络保留通用特征有效，较浅冻结更适合处理极端类别不平衡；3. 这些配置减少高达28%的GPU内存消耗；4. 在某些情况下实现超越全微调的mAP@50分数；5. 梯度分析显示适度冻结模型具有不同收敛模式。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了实证发现和选择冻结策略的实际指导，为资源有限场景下的目标检测提供了平衡迁移学习的实用、循证方法。&lt;h4&gt;翻译&lt;/h4&gt;YOLO架构对于实时目标检测至关重要。然而，在资源受限环境（如无人机）中部署它需要高效的迁移学习。尽管层冻结是一种常见技术，但各种冻结配置对当代YOLOv8和YOLOv10架构的具体影响仍未被探索，特别是关于冻结深度、数据集特性和训练动态之间的相互作用。本研究通过呈现层冻结策略的详细分析来填补这一知识空白。我们使用四个代表关键基础设施监控的具有挑战性的数据集，系统性地研究了YOLOv8和YOLOv10变体上的多种冻结配置。我们的方法整合了梯度行为分析（L2范数）和视觉解释（Grad-CAM），以提供不同冻结策略下训练动态的深入见解。我们的研究结果表明，没有通用的最优冻结策略，而是存在一种取决于数据特性的策略。例如，冻结骨干网络对于保留通用特征有效，而较浅的冻结更适合处理极端类别不平衡。与全微调相比，这些配置可减少高达28%的GPU内存消耗，并且在某些情况下实现了超越全微调的mAP@50分数。梯度分析证实了这些发现，显示适度冻结的模型具有不同的收敛模式。最终，这项工作提供了实证发现和选择冻结策略的实际指导。它为资源有限场景下的目标检测提供了平衡迁移学习的实用、循证方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3390/math13152539&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The You Only Look Once (YOLO) architecture is crucial for real-time objectdetection. However, deploying it in resource-constrained environments such asunmanned aerial vehicles (UAVs) requires efficient transfer learning. Althoughlayer freezing is a common technique, the specific impact of various freezingconfigurations on contemporary YOLOv8 and YOLOv10 architectures remainsunexplored, particularly with regard to the interplay between freezing depth,dataset characteristics, and training dynamics. This research addresses thisgap by presenting a detailed analysis of layer-freezing strategies. Wesystematically investigate multiple freezing configurations across YOLOv8 andYOLOv10 variants using four challenging datasets that represent criticalinfrastructure monitoring. Our methodology integrates a gradient behavioranalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeperinsights into training dynamics under different freezing strategies. Ourresults reveal that there is no universal optimal freezing strategy but,rather, one that depends on the properties of the data. For example, freezingthe backbone is effective for preserving general-purpose features, while ashallower freeze is better suited to handling extreme class imbalance. Theseconfigurations reduce graphics processing unit (GPU) memory consumption by upto 28% compared to full fine-tuning and, in some cases, achieve mean averageprecision (mAP@50) scores that surpass those of full fine-tuning. Gradientanalysis corroborates these findings, showing distinct convergence patterns formoderately frozen models. Ultimately, this work provides empirical findings andpractical guidelines for selecting freezing strategies. It offers a practical,evidence-based approach to balanced transfer learning for object detection inscenarios with limited resources.</description>
      <author>example@mail.com (Andrzej D. Dobrzycki, Ana M. Bernardos, José R. Casar)</author>
      <guid isPermaLink="false">2509.05490v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
      <link>http://arxiv.org/abs/2509.06953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website at \url{deep-reactive-policy.com}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Deep Reactive Policy (DRP)的视觉-运动神经运动政策，用于在动态环境中实现无碰撞运动生成。DRP直接在点云感官输入上运行，结合了基于transformer的神经运动政策IMPACT和DCP-RMP模块，实现了强大的泛化能力，在模拟和现实世界环境中都表现出色。&lt;h4&gt;背景&lt;/h4&gt;在动态、部分可观察环境中生成无碰撞运动是机械臂的基本挑战。经典运动规划器可以计算全局最优轨迹但需要完整环境知识且速度慢，神经运动政策直接在原始感官输入上运行但在复杂或动态环境中难以泛化。&lt;h4&gt;目的&lt;/h4&gt;设计一种反应式神经运动政策，能够在多种动态环境中直接从点云感官输入生成无碰撞运动。&lt;h4&gt;方法&lt;/h4&gt;DRP的核心是基于transformer的神经运动政策IMPACT，在10百万条专家轨迹上预训练，并通过迭代式学生-教师微调改进静态障碍物避免能力。此外，使用DCP-RMP模块在推理时增强动态障碍物避免能力。&lt;h4&gt;主要发现&lt;/h4&gt;DRP在具有杂乱场景、动态移动障碍物和目标阻塞的挑战性任务上实现了强大的泛化能力，在模拟和现实世界环境中，成功率都超过了先前的经典方法和神经方法。&lt;h4&gt;结论&lt;/h4&gt;DRP是一种有效的解决方案，能够在动态环境中实现无碰撞运动生成，并且具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在动态、部分可观察的环境中生成无碰撞运动是机械臂的基本挑战。经典运动规划器可以计算全局最优轨迹，但需要完整的环境知识，并且在动态场景中通常太慢。神经运动政策通过直接在原始感官输入上闭环运行提供了一种有前途的替代方案，但在复杂或动态环境中往往难以泛化。我们提出了Deep Reactive Policy (DRP)，这是一种视觉-运动神经运动政策，专为在多种动态环境中进行反应式运动生成而设计，直接在点云感官输入上运行。其核心是IMPACT，一种基于transformer的神经运动政策，在10百万条跨多种模拟场景生成的专家轨迹上进行了预训练。我们通过迭代式学生-教师微调进一步改进了IMPACT的静态障碍物避免能力。此外，我们使用DCP-RMP（一种局部反应式目标提议模块）在推理时增强了政策的动态障碍物避免能力。我们在具有杂乱场景、动态移动障碍物和目标阻塞的具有挑战性的任务上评估了DRP。DRP实现了强大的泛化能力，在模拟和现实世界环境中，成功率都超过了先前的经典方法和神经方法。视频结果和代码可在https://deep-reactive-policy.com获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态、部分可观测环境中为机械臂生成无碰撞运动轨迹的问题。这个问题在现实中很重要，因为机器人需要在人类自然环境中（如家庭和厨房）安全导航，而传统运动规划方法需要完整环境知识且反应太慢，神经运动策略又难以在复杂或动态环境中泛化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统规划方法需要完整环境知识且速度慢；神经策略难以泛化；混合方法需要测试时间优化，牺牲了反应性。然后设计Deep Reactive Policy (DRP)，结合了大规模预训练、迭代学生-教师微调和动态障碍避免模块。作者借鉴了cuRobo作为专家规划器、Geometric Fabrics作为教师策略、RMP设计了DCP-RMP模块，并使用PointNet++处理点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合全局规划和局部反应控制的优势，通过大规模预训练学习通用运动规划能力，使用微调改进局部障碍避免，并在推理时添加轻量级反应模块处理动态障碍。整体流程包括：1) 大规模运动预训练：使用cuRobo生成1000万个专家轨迹训练IMPACT；2) 迭代学生-教师微调：结合IMPACT和Geometric Fabrics改进静态障碍避免；3) 动态障碍避免：添加DCP-RMP模块处理动态障碍；4) 执行：DCP-RMP先处理动态障碍，IMPACT生成动作序列供低级控制器实时执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 大规模运动预训练：使用1000万个专家轨迹，提高了泛化能力；2) 迭代学生-教师微调：结合全局规划和局部控制，改进静态障碍避免；3) DCP-RMP模块：直接从点云操作的反应性目标提议，处理动态障碍；4) 端到端的视觉运动策略：直接从点云输入生成动作。相比之前的工作，DRP不需要完整环境知识，能处理动态场景；比纯神经方法有更好的泛化能力；比混合方法不需要测试时间优化，保持实时反应性；比纯反应方法具有全局场景意识，不易陷入局部最优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Deep Reactive Policy通过结合大规模预训练的Transformer策略、迭代学生-教师微调和动态障碍避免模块，实现了在复杂动态环境中实时生成无碰撞轨迹的能力，显著优于传统和现有的神经运动规划方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating collision-free motion in dynamic, partially observableenvironments is a fundamental challenge for robotic manipulators. Classicalmotion planners can compute globally optimal trajectories but require fullenvironment knowledge and are typically too slow for dynamic scenes. Neuralmotion policies offer a promising alternative by operating in closed-loopdirectly on raw sensory inputs but often struggle to generalize in complex ordynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neuralmotion policy designed for reactive motion generation in diverse dynamicenvironments, operating directly on point cloud sensory input. At its core isIMPACT, a transformer-based neural motion policy pretrained on 10 milliongenerated expert trajectories across diverse simulation scenarios. We furtherimprove IMPACT's static obstacle avoidance through iterative student-teacherfinetuning. We additionally enhance the policy's dynamic obstacle avoidance atinference time using DCP-RMP, a locally reactive goal-proposal module. Weevaluate DRP on challenging tasks featuring cluttered scenes, dynamic movingobstacles, and goal obstructions. DRP achieves strong generalization,outperforming prior classical and neural methods in success rate across bothsimulated and real-world settings. Video results and code available athttps://deep-reactive-policy.com</description>
      <author>example@mail.com (Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak)</author>
      <guid isPermaLink="false">2509.06953v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention</title>
      <link>http://arxiv.org/abs/2509.06705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Cortex Synth是一个从单张2D图像联合合成3D骨架几何和拓扑的端到端可微分框架，具有三个关键创新点和四个协同模块，在多个指标上达到最先进结果，并具有广泛的应用前景。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉和3D重建领域，从2D图像生成3D骨架几何和拓扑是一个具有挑战性的任务，需要同时考虑几何形状和拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个端到端可微分框架，能够从单张2D图像联合合成3D骨架的几何和拓扑结构，提高合成精度并减少拓扑错误。&lt;h4&gt;方法&lt;/h4&gt;提出Cortex Synth框架，包含三个关键创新：(1)具有多尺度骨架细化的分层图注意力机制；(2)通过拉普拉斯特征分解的可微分谱拓扑优化；(3)用于姿态结构对抗性几何一致性训练。框架集成了四个协同模块：伪3D点云生成器、增强的PointNet编码器、骨架坐标解码器和新型可微分图构建网络(DGCN)。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上，该方法实现了最先进的结果，MPJPE提高了18.7%，图编辑距离提高了27.3%，与之前的方法相比拓扑错误减少了42%。&lt;h4&gt;结论&lt;/h4&gt;Cortex Synth框架通过端到端可微分设计，实现了从2D图像到3D骨架几何和拓扑的高质量合成，为机器人操作、医学成像和自动角色绑定等领域提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出Cortex Synth，一种用于从单张2D图像联合合成3D骨架几何和拓扑的新型端到端可微分框架。我们的架构引入了三个关键创新：(1)具有多尺度骨架细化的分层图注意力机制，(2)通过拉普拉斯特征分解的可微分谱拓扑优化，(3)用于姿态结构对抗性几何一致性训练。该框架集成了四个协同模块：伪3D点云生成器、增强的PointNet编码器、骨架坐标解码器以及新型可微分图构建网络(DGCN)。我们的实验表明，在ShapeNet上实现了最先进的结果，MPJPE提高了18.7%，图编辑距离提高了27.3%，与之前的方法相比拓扑错误减少了42%。该模型的端到端可微分性使其在机器人操作、医学成像和自动角色绑定中有应用前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从2D图像生成3D骨架时，传统方法存在的三个关键限制：非可微分骨架化管道阻止端到端优化、固定拓扑先验限制跨类别泛化能力、几何和拓扑分离优化导致结构不一致。这个问题在现实中非常重要，因为准确的3D骨架表示对机器人操作、医学成像和计算机图形学等领域至关重要，能帮助机器人理解物体结构进行抓取、辅助医生进行手术规划、以及为动画角色提供自动绑定等。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者针对传统方法的局限性，设计了一个端到端可微分框架，整合了多个现有工作的优点：借鉴了图注意力网络(GAT)用于多尺度骨架细化，采用拉普拉斯特征分解实现可微分谱拓扑优化，使用对抗训练确保几何一致性，并基于PointNet++进行点云编码。作者将这些技术整合到四个协同模块中：伪3D点云生成器、增强的PointNet++编码器、骨架坐标解码器和可微分图构造网络(DGCN)，形成一个完整的从2D图像到3D骨架的生成流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可微分框架联合优化3D骨架的几何和拓扑结构，使用分层图注意力机制捕捉多尺度特征，并通过谱拓扑优化保持结构一致性。整体流程是：首先通过图像处理模块将2D RGB图像转换为伪3D点云；然后使用增强的PointNet++编码器提取分层特征；接着通过骨架解码器预测初始关节位置；最后通过DGCN模块使用图注意力和光谱约束优化骨架拓扑，生成最终的3D骨架表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)分层图注意力机制实现多尺度骨架细化；(2)可微分谱拓扑优化通过拉普拉斯特征分解确保结构一致性；(3)对抗几何一致性训练使用双判别器实现姿态-结构对齐；(4)自适应骨架复杂性基于结构熵动态分配节点。相比之前工作，Cortex-Synth的主要不同在于实现了完全端到端可微分框架，不依赖固定拓扑先验，能够联合优化几何和拓扑，实验显示在ShapeNet上MPJPE提高18.7%，图编辑距离降低27.3%，拓扑错误减少42%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Cortex-Synth通过引入分层图注意力和可微分谱拓扑优化，实现了从单张2D图像到3D骨架的高质量端到端合成，显著提高了骨架几何和拓扑的准确性，为机器人操作、医学成像和计算机图形学等领域提供了更强大的3D理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Cortex Synth, a novel end-to-end differentiable framework forjoint 3D skeleton geometry and topology synthesis from single 2D images. Ourarchitecture introduces three key innovations: (1) A hierarchical graphattention mechanism with multi-scale skeletal refinement, (2) Differentiablespectral topology optimization via Laplacian eigen decomposition, and (3)Adversarial geometric consistency training for pose structure alignment. Theframework integrates four synergistic modules: a pseudo 3D point cloudgenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and anovel Differentiable Graph Construction Network (DGCN). Our experimentsdemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and27.3 percent in Graph Edit Distance on ShapeNet, while reducing topologicalerrors by 42 percent compared to previous approaches. The model's end-to-enddifferentiability enables applications in robotic manipulation, medicalimaging, and automated character rigging.</description>
      <author>example@mail.com (Mohamed Zayaan S)</author>
      <guid isPermaLink="false">2509.06705v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G</title>
      <link>http://arxiv.org/abs/2509.06672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于信道状态信息的综合感知与通信成像框架，利用多径分量的角度和延迟信息实现三维物体重建，并通过两段反射点优化算法精确估计路径长度。&lt;h4&gt;背景&lt;/h4&gt;综合感知与通信作为第六代无线系统的基石技术，通过共享硬件、频谱和波形实现连接与环境映射的统一。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用CSI路径分量、发射器和接收器位置的ISAC成像框架，实现物体表面的精确几何重建。&lt;h4&gt;方法&lt;/h4&gt;从6.75 GHz频段的NYURay射线追踪器获取数据，提取可分辨多径分量并转换为三维反射点，采用两段反射点优化算法独立估计路径长度，最后聚合多对收发位置的反射点生成密集三维点云。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ISAC成像框架能够准确重建物体表面、边缘和曲线特征，实现了多跳ISAC成像。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次使用6.75 GHz无线射线追踪实现多跳ISAC成像的演示。&lt;h4&gt;翻译&lt;/h4&gt;综合感知与通信正成为第六代无线系统的基石技术，通过共享硬件、频谱和波形统一连接和环境映射。本文提出了一种利用从校准的NYURay射线追踪器在6.75 GHz上频段获取的信道状态信息路径分量、发射器位置和接收器位置的ISAC成像框架。我们的研究展示了如何从CSI估计中提取每个可分辨的多径分量，并通过融合其角度和延迟信息将其转换为等效的三维反射点，这对于多跳反射是有用且具有挑战性的。论文的主要贡献是两段反射点优化算法，该算法独立估计从发射器位置和接收器位置到物体表面上等效反射点的路径长度，从而实现精确的几何重建。随后，我们聚合从多对发射器和接收器位置导出的等效反射点，生成表示信道中物体的密集三维点云。实验结果验证了所提出的ISAC成像框架能够准确重建物体表面、边缘和曲线特征。据我们所知，本文首次展示了使用6.75 GHz无线射线追踪进行多跳ISAC成像。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用无线信道状态信息(CSI)进行高精度3D环境重建，特别是处理多跳反射场景下的成像问题。这个问题在6G时代至关重要，因为ISAC(集成感知与通信)是6G的核心技术，需要通过共享硬件、频谱和波形来实现通信与环境感知的统一。高精度环境地图对自动驾驶、数字孪生和XR服务等6G关键应用不可或缺，而无线信号能穿透障碍物、全天候工作，弥补光学成像的不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统CSI成像方法通常假设单次反射，难以处理复杂物体中的多跳反射问题。他们提出将多跳反射路径抽象为'等效反射点'(ERPs)的概念，避免追踪每次反射的具体位置。设计了独立估计发射段和接收段路径长度的两段式优化算法。借鉴了NYURay射线追踪引擎、通信系统中的CSI估计技术、SAR成像思路以及点云处理中的几何滤波技术。通过多视角融合(MVF)整合不同发射-接收位置的数据，形成完整的3D点云。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂环境中的多跳反射路径抽象为单个等效反射点(ERP)，利用无线信道的角度和延迟信息重建物体几何形状，并通过多视角融合提高成像质量和覆盖率。实现流程：1)使用NYURay生成不同TX-RX位置的CSI数据；2)提取每条路径的六元组参数(角度、延迟、增益)；3)应用两段式反射点优化算法计算ERP；4)几何滤波剔除异常值；5)多视角融合生成密集3D点云；6)后处理优化点云质量，生成最终RF图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次实现多跳反射的ISAC成像；2)提出两段式反射点优化算法；3)开发多视角融合框架；4)在6.75GHz中频段验证方法；5)利用校准的NYURay生成高质量数据。相比之前工作：突破单次反射假设限制；在更高频段验证；仅利用通信系统CSI；能处理更复杂物体形状；通过多视角融合提供更完整3D重建，减少单视角盲区。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于信道状态信息的多跳反射等效反射点优化和多视角融合方法，实现了在6.75GHz频段上对复杂物体的高精度无线射频成像，为6G通信与感知一体化系统提供了新的环境感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated sensing and communications (ISAC) is emerging as a cornerstonetechnology for sixth generation (6G) wireless systems, unifying connectivityand environmental mapping through shared hardware, spectrum, and waveforms. Thefollowing paper presents an ISAC imaging framework utilizing channel stateinformation (CSI) per-path components, transmitter (TX) positions, and receiver(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz inthe upper mid-band. Our work shows how each resolvable multipath component canbe extracted from CSI estimation and cast into an equivalent three-dimensionalreflection point by fusing its angle and delay information, which is useful andchallenging for multi-bounce reflections. The primary contribution of the paperis the two-segment reflection point optimization algorithm, which independentlyestimates the path lengths from the TX position and RX position to anequivalent reflection point (ERP) on the object surface, thus enabling precisegeometric reconstruction. Subsequently, we aggregate the ERPs derived frommultiple pairs of TX and RX positions, generating dense three dimensional pointclouds representing the objects in the channel. Experimental results validatethat the proposed ISAC imaging framework accurately reconstructs objectsurfaces, edges, and curved features. To the best of our knowledge, this paperprovides the first demonstration of multi bounce ISAC imaging using wirelessray tracing at 6.75 GHz.</description>
      <author>example@mail.com (Ahmad Bazzi, Mingjun Ying, Ojas Kanhere, Theodore S. Rappaport, Marwa Chafii)</author>
      <guid isPermaLink="false">2509.06672v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods</title>
      <link>http://arxiv.org/abs/2509.06597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint of final paper that will appear in the Proceedings of the  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LiHRA，一个用于促进人类-机器人交互场景中自动化、基于学习或经典风险监控方法开发的新型数据集。&lt;h4&gt;背景&lt;/h4&gt;协作机器人在工业环境中日益普及，增加了对可靠安全系统的需求。然而，缺乏捕捉真实人类-机器人交互（包括潜在危险事件）的高质量数据集，阻碍了相关方法的开发。&lt;h4&gt;目的&lt;/h4&gt;创建LiHRA数据集，为人类-机器人交互的风险监控研究提供全面、多模态的数据支持。&lt;h4&gt;方法&lt;/h4&gt;LiHRA是一个综合的多模态数据集，结合了3D LiDAR点云、人体关键点和机器人关节状态，捕捉人类-机器人协作的完整空间和动态上下文。该数据集涵盖六种代表性HRI场景，包括协作和共存任务、物体传递和表面抛光，每种场景都有安全和危险版本，共包含4,431个标记的点云，以10Hz频率记录。&lt;h4&gt;主要发现&lt;/h4&gt;LiHAR数据集通过结合多种模态，能够精确跟踪人体运动、机器人动作和环境条件，实现协作任务中的准确风险监控。作者还展示了一种利用机器人状态和机器人动态模型来量化每个场景风险水平的方法。&lt;h4&gt;结论&lt;/h4&gt;LiHAR凭借其高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件组合，为未来研究人类-机器人工作空间中的实时风险监控和自适应安全策略提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LiHRA，这是一个新型数据集，旨在促进人类-机器人交互场景中自动化、基于学习或经典风险监控方法的发展。工业环境中协作机器人的日益普及增加了对可靠安全系统的需求。然而，缺乏捕捉真实人类-机器人交互（包括潜在危险事件）的高质量数据集，减缓了开发进程。LiHAR通过提供结合3D LiDAR点云、人体关键点和机器人关节状态的全面多模态数据集，解决了这一挑战，捕捉了人类-机器人协作的完整空间和动态上下文。这种模态组合允许精确跟踪人体运动、机器人动作和环境条件，实现协作任务中的准确风险监控。LiHAR数据集涵盖六种代表性HRI场景，包括协作和共存任务、物体传递和表面抛光，每种场景都有安全和危险版本。该数据集共包含4,431个以10Hz频率记录的标记点云，为训练和评估经典和AI驱动的风险监控算法提供了丰富的资源。最后，为了展示LiHAR的实用性，我们介绍了一种量化每个场景随时间风险水平的方法。该方法利用上下文信息，包括机器人状态和机器人动态模型。凭借其高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件组合，LiHAR为未来研究人类-机器人工作空间中的实时风险监控和自适应安全策略提供了重要基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏高质量数据集来捕捉真实人机交互场景（包括潜在危险事件）的问题。这个问题很重要，因为随着协作机器人在工业环境中的普及，对可靠安全系统的需求增加，而现有风险评估方法依赖于专家主观评估，难以应对人机交互的复杂性、预测人类行为的挑战以及碰撞临界值估计的困难，阻碍了自动化安全系统的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集无法满足人机交互风险评估需求，然后选择3D LiDAR传感器作为核心感知方式（借鉴了自动驾驶领域的经验），结合HTC VIVE Tracker 3.0捕捉人体关键点和Franka Emika Robot记录机器人状态，构建了多模态数据集。设计过程中借鉴了ISO/TS 15066:2016安全标准，并利用现有文献中的经典方法估计外力，同时创新性地整合了多种传感器数据来捕捉人机交互的完整空间和动态上下文。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个结合3D LiDAR点云、人体关键点和机器人关节状态的多模态数据集，实现对人机协作场景的精确跟踪和风险监控。整体流程包括：1)使用专业硬件采集数据(LiDAR、运动捕捉系统和协作机器人)；2)对各传感器系统进行精确校准；3)记录六种不同场景(每种有安全/危险版本)；4)开发风险监控方法量化风险水平；5)通过外部力估计评估碰撞严重性；6)将数据导出为标准格式便于后续分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个集成LiDAR点云、机器人关节状态和人体关键点的数据集；2)包含六种代表性场景的安全和危险版本；3)提供精确标记的人机交互动态数据；4)提出基于安全标准的自动化风险监控方法。相比之前工作，LiHRA更全面(包含多种传感器数据)、更实用(包含危险场景)、更符合工业需求(使用3D LiDAR而非RGB相机)，解决了现有数据集要么缺乏机器人数据、要么缺乏危险场景、要么使用不合适传感器的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiHRA数据集通过整合多模态传感器数据和危险场景，为开发人机交互环境中的自动化风险评估和风险监控方法提供了全面且实用的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LiHRA, a novel dataset designed to facilitate the development ofautomated, learning-based, or classical risk monitoring (RM) methods forHuman-Robot Interaction (HRI) scenarios. The growing prevalence ofcollaborative robots in industrial environments has increased the need forreliable safety systems. However, the lack of high-quality datasets thatcapture realistic human-robot interactions, including potentially dangerousevents, slows development. LiHRA addresses this challenge by providing acomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human bodykeypoints, and robot joint states, capturing the complete spatial and dynamiccontext of human-robot collaboration. This combination of modalities allows forprecise tracking of human movement, robot actions, and environmentalconditions, enabling accurate RM during collaborative tasks. The LiHRA datasetcovers six representative HRI scenarios involving collaborative and coexistenttasks, object handovers, and surface polishing, with safe and hazardousversions of each scenario. In total, the data set includes 4,431 labeled pointclouds recorded at 10 Hz, providing a rich resource for training andbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrateLiHRA's utility, we introduce an RM method that quantifies the risk level ineach scenario over time. This method leverages contextual information,including robot states and the dynamic model of the robot. With its combinationof high-resolution LiDAR data, precise human tracking, robot state data, andrealistic collision events, LiHRA offers an essential foundation for futureresearch into real-time RM and adaptive safety strategies in human-robotworkspaces.</description>
      <author>example@mail.com (Frederik Plahl, Georgios Katranis, Ilshat Mamaev, Andrey Morozov)</author>
      <guid isPermaLink="false">2509.06597v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark</title>
      <link>http://arxiv.org/abs/2509.06456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种跨源点云配准方法，构建了大型多模态数据集并设计了基于重叠区域的配准框架，有效解决了跨源配准中的挑战，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;跨源点云配准是3D视觉中的基础任务，但与同源配准相比面临两大挑战：缺乏大规模真实世界数据集用于训练深度模型；不同传感器采集的点云存在固有差异，导致特征提取和匹配困难，影响配准精度。&lt;h4&gt;目的&lt;/h4&gt;推进跨源点云配准研究，通过构建大型数据集和创新框架解决现有挑战，实现准确且鲁棒的跨源点云配准。&lt;h4&gt;方法&lt;/h4&gt;构建了Cross3DReg数据集（目前最大真实世界多模态跨源点云配准数据集）；设计了基于重叠区域的跨源配准框架，利用未对齐图像预测点云重叠区域；提出了视觉-几何注意力引导的匹配模块，通过融合图像和几何信息建立可靠对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法实现了最先进的配准性能，相对旋转误差降低63.2%，相对平移误差降低40.2%，配准召回率提高5.4%，有效验证了跨源配准的准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在实现准确的跨源配准方面是有效的，能够有效处理跨源点云配准中的挑战，显著提升配准精度和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;跨源点云配准旨在对齐来自不同传感器的点云数据，是3D视觉中的基础任务。然而，与同源点云配准相比，跨源配准面临两个核心挑战：缺乏用于训练深度配准模型的大规模真实世界公开数据集，以及多传感器捕获的点云存在固有差异。传感器引起的多样模式对鲁棒且准确的点云特征提取和匹配构成巨大挑战，进而影响配准精度。为推进该领域研究，我们构建了Cross3DReg，目前最大且真实世界的多模态跨源点云配准数据集，分别通过旋转机械激光雷达和混合半固态激光雷达采集。此外，我们设计了基于重叠区域的跨源配准框架，利用未对齐的图像预测源点云和目标点云之间的重叠区域，有效过滤掉无关区域的冗余点，并显著减轻非重叠区域噪声引起的干扰。然后，提出视觉-几何注意力引导的匹配模块，通过融合图像和几何信息增强跨源点云特征一致性，建立可靠对应关系，最终实现准确且鲁棒的配准。大量实验表明，我们的方法实现了最先进的配准性能。我们的框架将相对旋转误差和相对平移误差分别降低了63.2%和40.2%，并将配准召回率提高了5.4%，这验证了其在实现准确跨源配准方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决跨源点云配准问题，即对齐来自不同传感器（如旋转机械激光雷达和混合半固态激光雷达）的点云数据。这个问题在现实中非常重要，因为它是3D视觉的基础任务，在机器人导航、遥感测绘和自动驾驶定位等领域有广泛应用。然而，与同源点云配准相比，跨源配准面临缺乏大规模真实世界数据集和传感器差异导致的特征不一致两大挑战，这些问题限制了该领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到跨源点云配准的两个核心挑战：缺乏大规模真实世界数据集和传感器差异导致的特征不一致。为解决数据集问题，作者构建了Cross3DReg数据集；为解决配准精度问题，作者设计了基于重叠区域的配准框架。方法设计借鉴了多项现有工作：参考ImLoveNet预测重叠区域，使用KPConv-FPN提取点云特征，采用U-Net处理图像，并利用多模态特征融合和注意力机制增强特征一致性。作者通过融合视觉和几何信息，建立了可靠的点对应关系，实现了跨源点云的准确配准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用未对齐的图像预测源点云和目标点云之间的重叠区域，有效过滤无关区域的冗余点，减轻非重叠区域噪声干扰；同时通过融合图像和几何信息，增强跨源点云特征一致性，建立可靠对应关系。整体流程分为四个阶段：1)特征提取，通过双分支网络提取图像和点云特征；2)重叠掩码预测(OMP)，融合图像和点云特征预测重叠区域；3)视觉-几何注意力引导的超点匹配(VGAM)，在重叠区域内建立对应关系；4)点匹配和配准，基于超点匹配获得点级对应关系并估计变换矩阵。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建了Cross3DReg，目前最大规模的真实世界跨源点云配准数据集，包含13,231个点云对；2)提出基于重叠区域的配准框架，利用未对齐图像预测点云重叠区域；3)设计视觉-几何注意力引导匹配模块，融合视觉和几何信息增强特征一致性。相比之前工作，不同之处在于：现有数据集规模小或主要使用合成数据，而Cross3DReg是真实世界大规模数据集；现有方法主要针对合成跨源设置设计，而作者方法专门针对真实世界跨源数据；实验显示作者方法在RRE和RTE上分别降低63.2%和40.2%，RR提高5.4%，性能显著提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Cross3DReg大规模真实世界跨源点云配准数据集和一种基于重叠区域的配准方法，通过融合视觉和几何信息实现了跨源点云的高精度对齐。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-source point cloud registration, which aims to align point cloud datafrom different sensors, is a fundamental task in 3D vision. However, comparedto the same-source point cloud registration, cross-source registration facestwo core challenges: the lack of publicly available large-scale real-worlddatasets for training the deep registration models, and the inherentdifferences in point clouds captured by multiple sensors. The diverse patternsinduced by the sensors pose great challenges in robust and accurate point cloudfeature extraction and matching, which negatively influence the registrationaccuracy. To advance research in this field, we construct Cross3DReg, thecurrently largest and real-world multi-modal cross-source point cloudregistration dataset, which is collected by a rotating mechanical lidar and ahybrid semi-solid-state lidar, respectively. Moreover, we design anoverlap-based cross-source registration framework, which utilizes unalignedimages to predict the overlapping region between source and target pointclouds, effectively filtering out redundant points in the irrelevant regionsand significantly mitigating the interference caused by noise innon-overlapping areas. Then, a visual-geometric attention guided matchingmodule is proposed to enhance the consistency of cross-source point cloudfeatures by fusing image and geometric information to establish reliablecorrespondences and ultimately achieve accurate and robust registration.Extensive experiments show that our method achieves state-of-the-artregistration performance. Our framework reduces the relative rotation error(RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$,respectively, and improves the registration recall (RR) by $5.4\%$, whichvalidates its effectiveness in achieving accurate cross-source registration.</description>
      <author>example@mail.com (Zongyi Xu, Zhongpeng Lang, Yilong Chen, Shanshan Zhao, Xiaoshui Huang, Yifan Zuo, Yan Zhang, Qianni Zhang, Xinbo Gao)</author>
      <guid isPermaLink="false">2509.06456v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Approximations of the mean curvature, and the Buet-Rumpf approximate mean curvature flow</title>
      <link>http://arxiv.org/abs/2509.06438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文推广了B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及点云相关平均曲率运动的工作，提出了两种推广方式，并将结果扩展到近似第二基本形式，证明了点云在平均曲率运动中满足的额外比较原理。&lt;h4&gt;背景&lt;/h4&gt;基于B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及点云相关平均曲率运动的研究。&lt;h4&gt;目的&lt;/h4&gt;推广变分流形的近似平均曲率向量定义及其相关平均曲率运动的工作。&lt;h4&gt;方法&lt;/h4&gt;通过线性算子和变分流形的正则性两种方式推广近似平均曲率向量定义，并将结果扩展到近似第二基本形式。&lt;h4&gt;主要发现&lt;/h4&gt;点云在平均曲率运动中满足一些额外的比较原理，包括离散和连续两种情况。&lt;h4&gt;结论&lt;/h4&gt;成功推广了近似平均曲率向量定义并扩展到近似第二基本形式，证明了点云运动的比较原理。&lt;h4&gt;翻译&lt;/h4&gt;本文的目的是推广B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及其相关点云平均曲率运动的工作。我们提出了近似平均曲率向量定义的两种推广方式：通过线性算子和通过变分流形的正则性。然后我们将结果扩展到近似第二基本形式。最后，我们证明了点云在平均曲率运动中满足的一些额外比较原理（包括离散和连续情况）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of this paper is to generalize the work of B. Buet and M. Rumpf onsome definition of the approximate mean curvature vector for varifolds, and itsassociated mean curvature motions for points clouds. We propose ageneralization of the definition of the approximate mean curvature vector intwo terms: in terms of linear operators and in terms of regularity of thevarifold. We then extend the results to the approximate second fundamentalform. Finally, we prove some additional comparison principles satisfied by themotion of points cloud by mean curvature (in the discrete and the continuouscases).</description>
      <author>example@mail.com (Abdelmouksit Sagueni)</author>
      <guid isPermaLink="false">2509.06438v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap</title>
      <link>http://arxiv.org/abs/2509.06329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文系统性地解决了植物3D分割领域的三大挑战，提供了现有数据集概述，总结了深度学习方法，开发了Plant Segmentation Studio开源框架，并通过实验验证了有效策略，为研究人员提供了实用工具和路线图。&lt;h4&gt;背景&lt;/h4&gt;植物形态学的精确表征为植物环境相互作用和遗传进化研究提供了有价值的见解。3D分割技术是从复杂点云中提取植物器官信息的关键技术，但在植物表型分析中的应用仍面临三大挑战：大规模标注数据集稀缺、难以将先进深度神经网络适应到植物点云、缺乏针对植物科学的标准基准和评估协议。&lt;h4&gt;目的&lt;/h4&gt;系统性地解决植物3D分割领域面临的三大挑战，提供现有3D植物数据集概述，总结基于深度学习的点云分割方法，介绍Plant Segmentation Studio开源框架，并评估代表性网络和模拟到现实的学习策略。&lt;h4&gt;方法&lt;/h4&gt;提供现有3D植物数据集的概述；系统总结基于深度学习的点云语义和实例分割方法；介绍Plant Segmentation Studio开源框架；进行广泛的定量实验评估代表性网络和模拟到现实的学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;稀疏卷积主干和基于Transformer的实例分割方法有效；基于建模和基于增强的合成数据生成在模拟到现实学习中具有互补作用，可以减少标注需求。&lt;h4&gt;结论&lt;/h4&gt;这项研究弥合了算法进步与实际部署之间的差距，为研究人员提供了即用工具，并为开发数据高效且可泛化的3D植物表型深度学习解决方案提供了路线图。&lt;h4&gt;翻译&lt;/h4&gt;植物形态学的精确表征为植物环境相互作用和遗传进化研究提供了有价值的见解。提取这一信息的关键技术是3D分割技术，它可以从复杂点云中勾勒出单个植物器官。尽管在一般3D计算机视觉领域取得了显著进展，但3D分割在植物表型分析中的应用仍然受到三大挑战的限制：大规模标注数据集的稀缺性；将先进的深度神经网络适应到植物点云的技术困难；缺乏针对植物科学的标准基准和评估协议。这篇论文通过以下方式系统性地解决了这些障碍：在一般3D分割领域的背景下提供现有3D植物数据集的概述；系统总结基于深度学习的点云语义和实例分割方法；介绍Plant Segmentation Studio开源框架；进行广泛的定量实验以评估代表性网络和模拟到现实的学习策略。我们的研究突出了稀疏卷积主干和基于Transformer的实例分割的有效性，同时强调基于建模和基于增强的合成数据生成在模拟到现实学习中的互补作用，可以减少标注需求。总体而言，这项研究弥合了算法进步与实际部署之间的差距，为研究人员提供了即用工具，并为开发数据高效且可泛化的3D植物表型深度学习解决方案提供了路线图。数据和代码可在https://github.com/perrydoremi/PlantSegStudio获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决植物器官级别3D分割规模化应用面临的三大挑战：大规模标注数据集稀缺、将先进深度神经网络适应植物点云的技术困难、以及缺乏针对植物科学定制的标准化基准和评估协议。这个问题在现实中非常重要，因为精确的植物形态表征对理解植物-环境相互作用和遗传进化至关重要，而器官级别的3D分割是提取这些形态信息的关键技术。解决这些问题可以促进植物表型分析的发展，支持农业可持续性和韧性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用'数据-算法-计算'三角框架系统性地解决问题：在数据层面，提供3D植物数据集概述并讨论真实数据采集和合成数据生成方法；在算法层面，总结深度学习点云分割方法并分析不同策略；在计算层面，开发Plant Segmentation Studio框架。作者确实借鉴了现有工作，特别是基于MMDetection3D框架进行定制，利用现有的深度学习架构如PointNet、3D CNN和Transformer，并结合程序建模和增强方法生成合成数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'数据-算法-计算'三角框架的系统整合，解决植物器官级别3D分割的三大挑战。整体流程包括：1)数据收集与处理，使用多种3D成像技术收集植物点云并进行标准化处理；2)算法设计与优化，设计适合植物点云特性的分割算法并优化神经网络架构；3)计算框架构建，开发PSS框架实现数据准备、算法集成和简化推理；4)评估与优化，在多个数据集上进行定量实验并优化算法参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'数据-算法-计算'三角框架系统性解决问题；2)全面分析3D植物数据集和合成数据生成方法；3)系统总结适合植物点云的深度学习方法，特别关注Transformer-based方法；4)开发首个针对3D植物表型分析的标准化框架PSS。相比之前工作，本文的不同之处在于全面性（整合三个层面而非单一层面）、针对性（专门针对植物器官级别需求）、实用性（提供可用工具框架）、定量评估（进行广泛实验）和开放性（提供开源代码）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性整合数据、算法和计算三个层面，开发了一个开源框架(PSS)来弥合植物器官级别3D分割中的数据-算法-计算差距，为研究人员提供了可扩展的植物表型分析工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The precise characterization of plant morphology provides valuable insightsinto plant environment interactions and genetic evolution. A key technology forextracting this information is 3D segmentation, which delineates individualplant organs from complex point clouds. Despite significant progress in general3D computer vision domains, the adoption of 3D segmentation for plantphenotyping remains limited by three major challenges: i) the scarcity oflarge-scale annotated datasets, ii) technical difficulties in adapting advanceddeep neural networks to plant point clouds, and iii) the lack of standardizedbenchmarks and evaluation protocols tailored to plant science. This reviewsystematically addresses these barriers by: i) providing an overview ofexisting 3D plant datasets in the context of general 3D segmentation domains,ii) systematically summarizing deep learning-based methods for point cloudsemantic and instance segmentation, iii) introducing Plant Segmentation Studio(PSS), an open-source framework for reproducible benchmarking, and iv)conducting extensive quantitative experiments to evaluate representativenetworks and sim-to-real learning strategies. Our findings highlight theefficacy of sparse convolutional backbones and transformer-based instancesegmentation, while also emphasizing the complementary role of modeling-basedand augmentation-based synthetic data generation for sim-to-real learning inreducing annotation demands. In general, this study bridges the gap betweenalgorithmic advances and practical deployment, providing immediate tools forresearchers and a roadmap for developing data-efficient and generalizable deeplearning solutions in 3D plant phenotyping. Data and code are available athttps://github.com/perrydoremi/PlantSegStudio.</description>
      <author>example@mail.com (Ruiming Du, Guangxun Zhai, Tian Qiu, Yu Jiang)</author>
      <guid isPermaLink="false">2509.06329v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration</title>
      <link>http://arxiv.org/abs/2509.06285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 19 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出DCReg框架，通过三个集成创新系统性解决LiDAR点云配准中的病态条件问题，显著提高定位精度和计算效率。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云配准是机器人感知和导航的基础，但在几何退化或狭窄环境中，配准问题会变成病态条件，导致解决方案不稳定和精度降低。现有方法未能准确检测、解释和解决这种病态条件问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性解决病态配准问题的框架，通过准确检测、解释和解决病态条件，提高配准的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;DCReg框架包含三个创新：1) 使用Hessian矩阵的Schur补分解实现可靠的病态条件检测，将配准问题解耦为干净的旋转和平移子空间；2) 开发定量表征技术，建立数学特征空间与物理运动方向之间的明确映射；3) 设计新型预处理器，仅稳定识别出的病态方向，同时保留所有良好约束信息。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，DCReg在各种环境中实现了比最先进方法高20%-50%的定位精度，并且速度提高了5-100倍。&lt;h4&gt;结论&lt;/h4&gt;DCReg提供了一个有效且高效的解决方案，能够处理LiDAR点云配准中的病态条件问题，显著提高了定位精度和计算效率，代码将在https://github.com/JokerJohn/DCReg上提供。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR点云配准是机器人感知和导航的基础。然而，在几何退化或狭窄环境中，配准问题会变成病态条件，导致解决方案不稳定和精度降低。虽然现有方法试图处理这些问题，但它们未能解决核心挑战：准确检测、解释和解决这种病态条件，导致漏检或损坏的解决方案。在本研究中，我们引入了DCReg，一个通过三个集成创新系统性解决病态配准问题的原则性框架。首先，DCReg通过将Schur补分解应用于Hessian矩阵，实现可靠的病态条件检测。该技术将配准问题解耦为干净的旋转和平移子空间，消除了传统分析中掩盖退化模式的耦合效应。其次，在这些干净的子空间中，我们开发了定量表征技术，建立了数学特征空间与物理运动方向之间的明确映射，提供了关于哪些特定运动缺乏约束的可操作见解。最后，利用这个干净的子空间，我们设计了一个有针对性的缓解策略：一种新型预处理器，选择性稳定仅识别出的病态方向，同时保留可观测空间中的所有良好约束信息。这通过具有单一可物理解释参数的预共轭梯度方法实现了高效且稳健的优化。广泛的实验表明，DCReg在各种环境中实现了比最先进方法高20%-50%的定位精度，并且速度提高了5-100倍。我们的实现在https://github.com/JokerJohn/DCReg上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达点云配准在几何退化或狭窄环境（如走廊、隧道）中的病态问题。在这些环境中，由于重复结构或稀疏特征，系统缺乏沿特定运动方向的充分几何约束，导致信息矩阵接近奇异，使优化问题变得病态。这个问题在现实中非常重要，因为它是现代自主导航系统（如自动驾驶车辆、机器人）可靠性的关键瓶颈，在这些环境中即使微小的传感器误差或初始估计偏差也可能导致定位完全失败，严重影响实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法的三个根本局限性出发：1) 检测时机问题：传统方法分析Hessian谱时，旋转和平移间的尺度差异和耦合效应掩盖了关键病态模式；2) 失败原因问题：即使检测到病态，也无法解释哪些物理运动方向是退化的；3) 缓解策略问题：现有方法（如正则化、截断）不加选择地修改优化问题。作者借鉴了Schur补分解处理块矩阵耦合效应的思想，以及预调节共轭梯度(PCG)优化框架，但创新性地将它们组合成一个统一框架，通过解耦表征来系统解决病态配准问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦表征（Decoupled Characterization）处理病态配准问题，使用Schur补分解将旋转和平移参数解耦到干净子空间中，消除耦合效应，然后进行定量表征和针对性缓解。整体流程分为三步：1) 病态检测：使用Schur补分解分析旋转和平移子空间，计算归一化特征值识别病态方向；2) 定量表征：通过内积匹配解决符号歧义，最大分量分析解决排序歧义，Gram-Schmidt正交化产生稳定正交基；3) 针对性缓解：设计基于PCG的求解器，使用特征值钳制策略只稳定病态方向，保持良好约束方向的自然收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 尺度鲁棒且耦合感知的病态检测，使用Schur补分解正确处理旋转-平移耦合；2) 物理可解释的方向性病态分析，建立数学特征空间和物理运动方向的明确映射；3) 针对性病态缓解求解器，使用单一参数控制条件数。与传统方法不同：检测上，传统方法忽略耦合效应，DCReg揭示隐藏退化；表征上，传统方法错误假设直接对应，DCReg提供精确映射；缓解上，传统方法不加选择修改问题，DCReg保持原始问题完整性；整体上，传统方法将检测和缓解视为分离步骤，DCReg提供统一框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DCReg通过解耦表征方法，实现了在退化环境中激光雷达点云配准的可靠病态检测、精确物理表征和高效稳定优化，显著提升了定位精度和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud registration is fundamental to robotic perception andnavigation. However, in geometrically degenerate or narrow environments,registration problems become ill-conditioned, leading to unstable solutions anddegraded accuracy. While existing approaches attempt to handle these issues,they fail to address the core challenge: accurately detection, interpret, andresolve this ill-conditioning, leading to missed detections or corruptedsolutions. In this study, we introduce DCReg, a principled framework thatsystematically addresses the ill-conditioned registration problems throughthree integrated innovations. First, DCReg achieves reliable ill-conditioningdetection by employing a Schur complement decomposition to the hessian matrix.This technique decouples the registration problem into clean rotational andtranslational subspaces, eliminating coupling effects that mask degeneracypatterns in conventional analyses. Second, within these cleanly subspaces, wedevelop quantitative characterization techniques that establish explicitmappings between mathematical eigenspaces and physical motion directions,providing actionable insights about which specific motions lack constraints.Finally, leveraging this clean subspace, we design a targeted mitigationstrategy: a novel preconditioner that selectively stabilizes only theidentified ill-conditioned directions while preserving all well-constrainedinformation in observable space. This enables efficient and robust optimizationvia the Preconditioned Conjugate Gradient method with a single physicalinterpretable parameter. Extensive experiments demonstrate DCReg achieves atleast 20% - 50% improvement in localization accuracy and 5-100 times speedupover state-of-the-art methods across diverse environments. Our implementationwill be available at https://github.com/JokerJohn/DCReg.</description>
      <author>example@mail.com (Xiangcheng Hu, Xieyuanli Chen, Mingkai Jia, Jin Wu, Ping Tan, Steven L. Waslander)</author>
      <guid isPermaLink="false">2509.06285v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud</title>
      <link>http://arxiv.org/abs/2509.05954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StripDet是一种轻量级3D目标检测框架，通过创新的条带注意力模块和分层骨干网络，有效解决了高精度3D目标检测模型在计算和内存需求方面的挑战，在保持高精度的同时大幅减少了参数量。&lt;h4&gt;背景&lt;/h4&gt;高精度3D目标检测模型的部署面临重大挑战，因为它们需要大量的计算和内存资源。&lt;h4&gt;目的&lt;/h4&gt;介绍StripDet，一个为设备端效率设计的新型轻量级框架。&lt;h4&gt;方法&lt;/h4&gt;提出了新的条带注意力模块（SAB），通过将标准2D卷积分解为非对称条带卷积，高效提取方向特征并降低计算复杂度；设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上，仅使用0.65M参数，StripDet在汽车检测上达到79.97%的mAP，超过了基准PointPillars，参数减少了7倍；优于最近的轻量级和基于知识蒸馏的方法，实现了更好的精度-效率权衡。&lt;h4&gt;结论&lt;/h4&gt;StripDet成为边缘设备上实际3D检测的可行解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将高精度3D目标检测模型从点云中部署仍然是一个重大挑战，因为它们需要大量的计算和内存资源。为此，我们引入了StripDet，一种专为设备端效率设计的新型轻量级框架。首先，我们提出了新型的条带注意力模块（SAB），这是一个为捕获长程空间依赖关系而设计的高效模块。通过将标准2D卷积分解为非对称条带卷积，SAB能够高效提取方向特征，同时将计算复杂度从二次方降低到线性。其次，我们设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合，实现了端到端的效率。在KITTI数据集上的大量实验验证了StripDet的优越性。仅使用0.65M参数，我们的模型在汽车检测上达到了79.97%的mAP，超过了基准PointPillars，参数减少了7倍。此外，StripDet优于最近的轻量级和基于知识蒸馏的方法，实现了更好的精度-效率权衡，同时成为边缘设备上实际3D检测的可行解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of high-accuracy 3D object detection models from point cloudremains a significant challenge due to their substantial computational andmemory requirements. To address this, we introduce StripDet, a novellightweight framework designed for on-device efficiency. First, we propose thenovel Strip Attention Block (SAB), a highly efficient module designed tocapture long-range spatial dependencies. By decomposing standard 2Dconvolutions into asymmetric strip convolutions, SAB efficiently extractsdirectional features while reducing computational complexity from quadratic tolinear. Second, we design a hardware-friendly hierarchical backbone thatintegrates SAB with depthwise separable convolutions and a simple multiscalefusion strategy, achieving end-to-end efficiency. Extensive experiments on theKITTI dataset validate StripDet's superiority. With only 0.65M parameters, ourmodel achieves a 79.97% mAP for car detection, surpassing the baselinePointPillars with a 7x parameter reduction. Furthermore, StripDet outperformsrecent lightweight and knowledge distillation-based methods, achieving asuperior accuracy-efficiency trade-off while establishing itself as a practicalsolution for real-world 3D detection on edge devices.</description>
      <author>example@mail.com (Weichao Wang, Wendong Mao, Zhongfeng Wang)</author>
      <guid isPermaLink="false">2509.05954v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets</title>
      <link>http://arxiv.org/abs/2509.06936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了基于MGPHot数据集的新音乐自动标注基准测试框架，包含专家音乐学注释、可检索音频数据集、标准化评估分割和预计算模型表示，并比较了专家与通用标签注释的差异。&lt;h4&gt;背景&lt;/h4&gt;音乐自动标注是自动为音频分配描述性标签的任务，因其挑战性、语义描述多样性和实际应用价值，已成为评估通用音乐表示性能的常见下游任务。MGPHot数据集包含专家音乐学注释，但原始数据集缺乏音频和标准化评估设置。&lt;h4&gt;目的&lt;/h4&gt;解决MGPHot数据集缺乏音频和标准化评估设置的问题，为音乐理解研究提供更先进的基准测试框架。&lt;h4&gt;方法&lt;/h4&gt;提供一组可检索的YouTube URL音频，提出train/val/test标准化评估分割，为七种最先进模型预计算表示，并在MGPHot和标准参考标签数据集上评估这些模型。&lt;h4&gt;主要发现&lt;/h4&gt;专家音乐学注释与通用标签注释之间存在关键差异，通过新基准测试框架揭示了这些差异，为音乐理解研究提供了更深入的见解。&lt;h4&gt;结论&lt;/h4&gt;所提出的新基准测试框架为未来音乐理解研究提供了更先进的评估工具，有助于推动音乐自动标注领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;音乐自动标注旨在为音频记录自动分配描述性标签，如流派、情绪或乐器等。由于其挑战性、语义描述的多样性以及在各种应用中的实际价值，它已成为评估从音频数据学习到的通用音乐表示性能的常见下游任务。我们介绍了一个基于最近发布的MGPHot数据集的新基准测试数据集，该数据集包含专家音乐学注释，允许与在通用标签数据集上获得的结果进行额外的见解和比较。虽然MGPHot注释已被证明对计算音乐学有用，但原始数据集既不包括音频，也没有提供将其用作标准化自动标注基准的评估设置。为了解决这个问题，我们提供了一组精心挑选的YouTube URL，可检索音频，并提出了用于标准化评估的train/val/test分割，以及为七种最先进模型预先计算的表示。利用这些资源，我们在MGPHot和标准参考标签数据集上评估了这些模型，突出了专家和通用标签注释之间的关键差异。总之，我们的贡献为未来音乐理解研究提供了一个更先进的基准测试框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Music autotagging aims to automatically assign descriptive tags, such asgenre, mood, or instrumentation, to audio recordings. Due to its challenges,diversity of semantic descriptions, and practical value in variousapplications, it has become a common downstream task for evaluating theperformance of general-purpose music representations learned from audio data.We introduce a new benchmarking dataset based on the recently published MGPHotdataset, which includes expert musicological annotations, allowing foradditional insights and comparisons with results obtained on common generic tagdatasets. While MGPHot annotations have been shown to be useful forcomputational musicology, the original dataset neither includes audio norprovides evaluation setups for its use as a standardized autotagging benchmark.To address this, we provide a curated set of YouTube URLs with retrievableaudio, and propose a train/val/test split for standardized evaluation, andprecomputed representations for seven state-of-the-art models. Using theseresources, we evaluated these models in MGPHot and standard reference tagdatasets, highlighting key differences between expert and generic tagannotations. Altogether, our contributions provide a more advanced benchmarkingframework for future research in music understanding.</description>
      <author>example@mail.com (Pedro Ramoneda, Pablo Alonso-Jimenez, Sergio Oramas, Xavier Serra, Dmitry Bogdanov)</author>
      <guid isPermaLink="false">2509.06936v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
      <link>http://arxiv.org/abs/2509.06570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 2025 IEEE/CVF International Conference on  Computer Vision Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为保留性角表示学习（RARL）的方法，用于解决增量开放集识别（IOSR）问题。该方法通过在角度空间中对齐未知表示和采用虚拟内在交互训练策略，有效减轻了知识更新过程中的表示漂移和类别间混淆问题。&lt;h4&gt;背景&lt;/h4&gt;现有的开放集识别方法通常针对静态场景，模型只能分类已知类别并识别固定范围内的未知类别，无法从连续数据流中增量识别新出现的未知类别并获取相应知识。&lt;h4&gt;目的&lt;/h4&gt;解决在动态场景中由于无法访问之前训练数据导致的开放集识别决策边界区分性难以维持、严重类别间混淆的问题。&lt;h4&gt;方法&lt;/h4&gt;提出保留性角表示学习（RARL）方法，包括：1) 在等角紧框架下构建的角度空间内，鼓励未知表示围绕非活跃原型对齐；2) 采用虚拟内在交互（VII）训练策略，通过边界接近虚拟类强制清晰的类间边界；3) 设计分层校正策略优化决策边界，减轻样本不平衡引起的表示偏差和特征空间扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR100和TinyImageNet数据集上的实验结果表明，所提出的RARL方法在各种任务设置下都达到了最先进的性能，为IOSR建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;RARL方法有效解决了增量开放集识别中的表示漂移和类别混淆问题，能够更好地适应动态场景下的开放集识别任务。&lt;h4&gt;翻译&lt;/h4&gt;现有的开放集识别方法通常设计用于静态场景，模型旨在分类已知类别并识别固定范围内的未知类别。这与模型应能从连续数据流中增量识别新出现的未知类别并获取相应知识的期望不符。在这种动态场景中，由于无法访问之前的训练数据，开放集识别决策边界的区分性难以维持，导致严重的类别间混淆。为解决这一问题，我们提出了用于增量开放集识别的保留性角表示学习（RARL）。在RARL中，未知表示被鼓励在等角紧框架下构建的角度空间内围绕非活跃原型对齐，从而减轻知识更新过程中的过度表示漂移。具体来说，我们采用虚拟内在交互（VII）训练策略，通过边界接近虚拟类强制清晰的类间边界，压缩已知表示。此外，还设计了一种分层校正策略来优化决策边界，减轻由旧/新类别和正/负类别样本不平衡引起的表示偏差和特征空间扭曲。我们在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open set recognition (OSR) methods are typically designed for staticscenarios, where models aim to classify known classes and identify unknown oneswithin fixed scopes. This deviates from the expectation that the model shouldincrementally identify newly emerging unknown classes from continuous datastreams and acquire corresponding knowledge. In such evolving scenarios, thediscriminability of OSR decision boundaries is hard to maintain due torestricted access to former training data, causing severe inter-classconfusion. To solve this problem, we propose retentive angular representationlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknownrepresentations are encouraged to align around inactive prototypes within anangular space constructed under the equiangular tight frame, thereby mitigatingexcessive representation drift during knowledge updates. Specifically, we adopta virtual-intrinsic interactive (VII) training strategy, which compacts knownrepresentations by enforcing clear inter-class margins throughboundary-proximal virtual classes. Furthermore, a stratified rectificationstrategy is designed to refine decision boundaries, mitigating representationbias and feature space distortion caused by imbalances between old/new andpositive/negative class samples. We conduct thorough evaluations on CIFAR100and TinyImageNet datasets and establish a new benchmark for IOSR. Experimentalresults across various task setups demonstrate that the proposed methodachieves state-of-the-art performance.</description>
      <author>example@mail.com (Runqing Yang, Yimin Fu, Changyuan Wu, Zhunga Liu)</author>
      <guid isPermaLink="false">2509.06570v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix</title>
      <link>http://arxiv.org/abs/2509.06314v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为rho(C)的冗余指数，用于直接量化表示学习中潜在嵌入的维度间依赖关系，帮助识别冗余并提高表示效率。&lt;h4&gt;背景&lt;/h4&gt;深度网络产生的潜在空间常包含冗余信息，多个坐标编码重叠内容，降低了模型容量和泛化能力，而现有标准指标无法直接检测这种冗余。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接量化潜在表示中冗余的方法，以评估和改进学习表示的效率。&lt;h4&gt;方法&lt;/h4&gt;引入冗余指数rho(C)，通过分析潜在表示的耦合矩阵，使用能量距离比较非对角线统计量与正态分布的差异，直接测量维度间依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;低rho(C)与高分类准确率和低重建误差相关；冗余增加与性能崩溃相关；估计器可靠性随潜在维度增加；TPE方法倾向于探索低冗余区域。&lt;h4&gt;结论&lt;/h4&gt;rho(C)作为冗余的通用指标，为评估和改进学习表示的效率提供了理论视角和实用工具，可用于指导神经架构搜索和作为正则化目标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：表示学习中的一个核心挑战是构建既具有表达力又高效的潜在嵌入。在实践中，深度网络常常产生冗余的潜在空间，其中多个坐标编码重叠信息，降低了有效容量并阻碍了泛化能力。标准指标如准确率或重建损失仅提供此类冗余的间接证据，无法将其隔离为一种失败模式。我们引入了一个冗余指数，表示为rho(C)，它通过分析从潜在表示推导的耦合矩阵，并通过能量距离将其非对角线统计量与正态分布进行比较，直接量化了维度间的依赖关系。结果是一个紧凑、可解释且具有统计基础的表示质量度量。我们在MNIST变体、Fashion-MNIST、CIFAR-10和CIFAR-100上的判别性和生成性设置中验证了rho(C)，涵盖了多种架构和超参数优化策略。经验表明，低rho(C)可靠地预测高分类准确率或低重建误差，而冗余增加与性能崩溃相关。估计器的可靠性随潜在维度增加而提高，为可靠分析提供了自然下限。我们还表明，树结构Parzen估计器（TPE）倾向于探索低rho区域，表明rho(C)可以指导神经架构搜索并作为冗余感知的正则化目标。通过将冗余暴露为跨模型和任务的普遍瓶颈，rho(C)为评估和改进学习表示的效率提供了理论视角和实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central challenge in representation learning is constructing latentembeddings that are both expressive and efficient. In practice, deep networksoften produce redundant latent spaces where multiple coordinates encodeoverlapping information, reducing effective capacity and hinderinggeneralization. Standard metrics such as accuracy or reconstruction lossprovide only indirect evidence of such redundancy and cannot isolate it as afailure mode. We introduce a redundancy index, denoted rho(C), that directlyquantifies inter-dimensional dependencies by analyzing coupling matricesderived from latent representations and comparing their off-diagonal statisticsagainst a normal distribution via energy distance. The result is a compact,interpretable, and statistically grounded measure of representational quality.We validate rho(C) across discriminative and generative settings on MNISTvariants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiplearchitectures and hyperparameter optimization strategies. Empirically, lowrho(C) reliably predicts high classification accuracy or low reconstructionerror, while elevated redundancy is associated with performance collapse.Estimator reliability grows with latent dimension, yielding natural lowerbounds for reliable analysis. We further show that Tree-structured ParzenEstimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)can guide neural architecture search and serve as a redundancy-awareregularization target. By exposing redundancy as a universal bottleneck acrossmodels and tasks, rho(C) offers both a theoretical lens and a practical toolfor evaluating and improving the efficiency of learned representations.</description>
      <author>example@mail.com (Mehmet Can Yavuz, Berrin Yanikoglu)</author>
      <guid isPermaLink="false">2509.06314v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
      <link>http://arxiv.org/abs/2509.06165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UNO（统一对象中心视频场景图生成），一个单阶段统一框架，能够同时处理粗粒度框级别和细粒度全景像素级别的视频场景图生成任务，通过扩展的slot注意力机制、对象时间一致性学习和动态三元组预测模块实现高效的对象时序交互建模。&lt;h4&gt;背景&lt;/h4&gt;现有视频场景图生成研究通常只针对粗粒度框级别或细粒度全景像素级别中的一种任务，需要特定的架构和多阶段训练流程，缺乏能够同时处理这两种粒度级别的统一方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个单阶段、统一的框架，在一个端到端架构中同时处理框级别和像素级别的VidSGG任务，最小化任务特定修改，最大化参数共享，实现不同视觉粒度级别上的泛化。&lt;h4&gt;方法&lt;/h4&gt;UNO框架采用扩展的slot注意力机制将视觉特征分解为对象和关系slot；引入对象时间一致性学习强制跨帧保持一致的对象表示；使用动态三元组预测模块将关系slot链接到相应的对象对，捕捉随时间变化的交互。&lt;h4&gt;主要发现&lt;/h4&gt;UNO在标准框级别和像素级别VidSGG基准测试上取得了具有竞争力的性能，同时通过统一的对象中心设计提供了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;UNO不仅能够在两种任务上实现竞争性性能，还通过统一的对象中心设计提高了效率，证明了统一框架处理不同视觉粒度级别视频场景图生成的可行性。&lt;h4&gt;翻译&lt;/h4&gt;视频场景图生成(VidSGG)旨在通过检测对象并将其时间交互建模为结构化图来表示动态视觉内容。先前的研究通常针对粗粒度框级别或细粒度全景像素级别的VidSGG，通常需要特定的架构和多阶段训练流程。在本文中，我们提出了UNO（统一对象中心VidSGG），这是一个单阶段统一框架，在一个端到端架构中共同解决这两个任务。UNO旨在最小化任务特定的修改并最大化参数共享，实现在不同视觉粒度级别上的泛化。UNO的核心是一个扩展的slot注意力机制，它将视觉特征分解为对象和关系slot。为确保鲁棒的时序建模，我们引入了对象时间一致性学习，强制跨帧保持一致的对象表示，而不依赖显式跟踪模块。此外，动态三元组预测模块将关系slot链接到相应的对象对，捕捉随时间变化的交互。我们在标准的框级别和像素级别VidSGG基准上评估了UNO。结果表明，UNO不仅在两个任务上都取得了具有竞争力的性能，还通过统一的对象中心设计提供了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Scene Graph Generation (VidSGG) aims to represent dynamic visualcontent by detecting objects and modeling their temporal interactions asstructured graphs. Prior studies typically target either coarse-grainedbox-level or fine-grained panoptic pixel-level VidSGG, often requiringtask-specific architectures and multi-stage training pipelines. In this paper,we present UNO (UNified Object-centric VidSGG), a single-stage, unifiedframework that jointly addresses both tasks within an end-to-end architecture.UNO is designed to minimize task-specific modifications and maximize parametersharing, enabling generalization across different levels of visual granularity.The core of UNO is an extended slot attention mechanism that decomposes visualfeatures into object and relation slots. To ensure robust temporal modeling, weintroduce object temporal consistency learning, which enforces consistentobject representations across frames without relying on explicit trackingmodules. Additionally, a dynamic triplet prediction module links relation slotsto corresponding object pairs, capturing evolving interactions over time. Weevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Resultsdemonstrate that UNO not only achieves competitive performance across bothtasks but also offers improved efficiency through a unified, object-centricdesign.</description>
      <author>example@mail.com (Huy Le, Nhat Chung, Tung Kieu, Jingkang Yang, Ngan Le)</author>
      <guid isPermaLink="false">2509.06165v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Micro-Expression Recognition via Fine-Grained Dynamic Perception</title>
      <link>http://arxiv.org/abs/2509.06015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新颖的细粒度动态感知(FDP)框架用于面部微表情识别，通过帧级特征排序和动态图像构建任务，有效捕捉微表情的动态信息，解决了现有方法在特征提取和数据限制方面的问题，并在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;面部微表情识别是一项具有挑战性的任务，因为微表情具有短暂、细微和动态的特性。现有方法大多依赖手工设计特征或深度网络，前者通常需要关键帧，后者则受限于小规模和低多样性的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的微表情识别框架，能够有效捕捉微表情的动态信息，解决现有方法在特征提取和数据限制方面的问题。&lt;h4&gt;方法&lt;/h4&gt;提出细粒度动态感知(FDP)框架，包括：按时间顺序对原始帧序列的帧级特征进行排序，编码微表情出现和运动的动态信息；提出局部-全局特征感知transformer进行帧表示学习；采用排序评分器计算每个帧级特征的排序分数；在时间维度上对排序特征进行池化，捕获动态表示；将动态表示共享给微表情识别模块和动态图像构建模块。&lt;h4&gt;主要发现&lt;/h4&gt;提出的FDP方法显著超越了最先进的微表情识别方法；在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，FDP的F1分数分别比之前最好的结果提高了4.05%、2.50%、7.71%和2.11%；动态图像构建任务表现良好。&lt;h4&gt;结论&lt;/h4&gt;细粒度动态感知框架有效解决了微表情识别中的挑战，通过帧级特征排序和动态图像构建任务，能够更好地捕捉微表情的动态信息，缓解数据稀缺问题，并在多个数据集上取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;面部微表情识别是一项具有挑战性的任务，因为微表情具有短暂、细微和动态的特性。大多数现有方法依赖于手工设计特征或深度网络，其中前者通常需要关键帧，而后者则受限于小规模和低多样性的训练数据。在本文中，我们为微表情识别开发了一种新颖的细粒度动态感知(FDP)框架。我们提出按时间顺序对原始帧序列的帧级特征进行排序，其中排序过程编码了微表情出现和运动的动态信息。具体来说，提出了一种新颖的局部-全局特征感知transformer用于帧表示学习。进一步采用排序评分器计算每个帧级特征的排序分数。之后，从排序评分器中提取的排序特征在时间维度上进行池化，以捕获动态表示。最后，动态表示被微表情识别模块和动态图像构建模块共享，前者预测微表情类别，后者使用编码器-解码器结构构建动态图像。动态图像构建任务的设计有助于捕捉与微表情相关的面部细微动作，并缓解数据稀缺问题。大量实验表明，我们的方法(i)显著超越了最先进的微表情识别方法，(ii)在动态图像构建方面表现良好。特别是在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，我们的FDP在F1分数上分别比之前最好的结果提高了4.05%、2.50%、7.71%和2.11%。代码可在https://github.com/CYF-cuber/FDP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial micro-expression recognition (MER) is a challenging task, due to thetransience, subtlety, and dynamics of micro-expressions (MEs). Most existingmethods resort to hand-crafted features or deep networks, in which the formeroften additionally requires key frames, and the latter suffers from small-scaleand low-diversity training data. In this paper, we develop a novel fine-graineddynamic perception (FDP) framework for MER. We propose to rank frame-levelfeatures of a sequence of raw frames in chronological order, in which the rankprocess encodes the dynamic information of both ME appearances and motions.Specifically, a novel local-global feature-aware transformer is proposed forframe representation learning. A rank scorer is further adopted to calculaterank scores of each frame-level feature. Afterwards, the rank features fromrank scorer are pooled in temporal dimension to capture dynamic representation.Finally, the dynamic representation is shared by a MER module and a dynamicimage construction module, in which the former predicts the ME category, andthe latter uses an encoder-decoder structure to construct the dynamic image.The design of dynamic image construction task is beneficial for capturingfacial subtle actions associated with MEs and alleviating the data scarcityissue. Extensive experiments show that our method (i) significantly outperformsthe state-of-the-art MER methods, and (ii) works well for dynamic imageconstruction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%over the previous best results in terms of F1-score on the CASME II, SAMM,CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available athttps://github.com/CYF-cuber/FDP.</description>
      <author>example@mail.com (Zhiwen Shao, Yifan Cheng, Fan Zhang, Xuehuai Shi, Canlin Li, Lizhuang Ma, Dit-yan Yeung)</author>
      <guid isPermaLink="false">2509.06015v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model</title>
      <link>http://arxiv.org/abs/2509.05849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 (Main Conference)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种通过自监督学习解决婴儿语音习得中声学到发音映射问题的计算模型，模型使用wav2vec 2.0的表示学习发音参数，产生可理解语音，研究支持感知学习指导发音发展的发育理论。&lt;h4&gt;背景&lt;/h4&gt;婴儿在语音习得方面面临巨大挑战，需要将极其可变的声学输入映射到适当的发音动作，而没有明确指导。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决声学到发音映射问题的计算模型，通过自监督学习来解决这个问题。&lt;h4&gt;方法&lt;/h4&gt;研究提出一个包含三个部分的计算模型：特征提取器将语音转换为潜在表示，逆向模型将这些表示映射到发音参数，合成器生成语音输出。在单说话人和多说话人环境中进行实验，使用预训练的wav2vec 2.0模型的中间层表示，并与MFCC特征进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的wav2vec 2.0模型的中间层表示为发音学习提供了最佳表示，显著优于MFCC特征。这些表示使模型能够学习与人类模式相关的发音轨迹，区分发音部位，产生可理解的语音。对成功的发音学习至关重要的是具有音素可区分性和说话人不变性的表示，这正是自监督表示学习模型的特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了与发育理论一致的计算证据，该理论提出音素类别的感知学习指导发音发展，为婴儿如何获得语音产生能力提供了见解，尽管他们面临复杂的映射问题。&lt;h4&gt;翻译&lt;/h4&gt;人类婴儿在语音习得方面面临一个巨大挑战：在没有明确指导的情况下，将极其可变的声学输入映射到适当的发音动作。我们提出了一个通过自监督学习解决声学到发音映射问题的计算模型。我们的模型包含一个将语音转换为潜在表示的特征提取器，一个将这些表示映射到发音参数的逆向模型，以及一个生成语音输出的合成器。在单说话人和多说话人环境中进行的实验显示，预训练的wav2vec 2.0模型的中间层为发音学习提供了最佳表示，显著优于MFCC特征。这些表示使我们的模型能够学习与人类模式相关的发音轨迹，区分发音部位，并产生可理解的语音。对成功的发音学习至关重要的是具有音素可区分性和说话人不变性的表示——这正是自监督表示学习模型的特征。我们的研究结果提供了与发育理论一致的计算证据，该理论提出音素类别的感知学习指导发音发展，为婴儿如何获得语音产生能力提供了见解，尽管他们面临复杂的映射问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human infants face a formidable challenge in speech acquisition: mappingextremely variable acoustic inputs into appropriate articulatory movementswithout explicit instruction. We present a computational model that addressesthe acoustic-to-articulatory mapping problem through self-supervised learning.Our model comprises a feature extractor that transforms speech into latentrepresentations, an inverse model that maps these representations toarticulatory parameters, and a synthesizer that generates speech outputs.Experiments conducted in both single- and multi-speaker settings reveal thatintermediate layers of a pre-trained wav2vec 2.0 model provide optimalrepresentations for articulatory learning, significantly outperforming MFCCfeatures. These representations enable our model to learn articulatorytrajectories that correlate with human patterns, discriminate between places ofarticulation, and produce intelligible speech. Critical to successfularticulatory learning are representations that balance phoneticdiscriminability with speaker invariance -- precisely the characteristics ofself-supervised representation learning models. Our findings providecomputational evidence consistent with developmental theories proposing thatperceptual learning of phonetic categories guides articulatory development,offering insights into how infants might acquire speech production capabilitiesdespite the complex mapping problem they face.</description>
      <author>example@mail.com (Marvin Lavechin, Thomas Hueber)</author>
      <guid isPermaLink="false">2509.05849v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Large Language Models</title>
      <link>http://arxiv.org/abs/2509.05757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了双曲几何在大型语言模型中的应用，提出了双曲大型语言模型(HypLLMs)的分类框架，并探讨了其在增强语义表示学习和多尺度推理方面的潜力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在各种任务中表现出色，但现实世界数据往往具有高度非欧几里得的层次结构，如蛋白质网络、交通网络、金融网络、大脑网络和语言结构。使用LLMs有效学习这些数据的语义蕴含和层次关系仍是一个探索不足的领域。&lt;h4&gt;目的&lt;/h4&gt;提供对利用双曲几何作为表示空间来增强语义表示学习和多尺度推理的最新进展进行全面阐述，并提出双曲大型语言模型的分类框架。&lt;h4&gt;方法&lt;/h4&gt;将双曲大型语言模型(HypLLMs)的技术分为四类：(1)通过exp/log映射的双曲LLMs；(2)双曲微调模型；(3)完全双曲LLMs；(4)双曲状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;双曲几何作为非欧几里得空间，能有效建模树状层次结构，特别适合处理具有层次结构的数据，与LLMs结合可增强语义表示学习和多尺度推理能力。&lt;h4&gt;结论&lt;/h4&gt;双曲几何在大型语言模型中的应用前景广阔，特别是在处理层次结构数据方面。论文提供了全面的分类框架，并探讨了潜在应用和未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在各种任务中取得了显著成功，展示了卓越的性能，包括自然语言处理(NLP)、天气预报、生物蛋白质折叠、文本生成和解决数学问题。然而，许多现实世界的数据表现出高度非欧几里得的潜在层次解剖结构，如蛋白质网络、交通网络、金融网络、大脑网络以及自然语言中的语言结构或句法树。使用LLMs从这些原始、非结构化的输入数据中有效学习内在语义蕴含和层次关系仍然是一个探索不足的领域。由于其在建模树状层次结构方面的有效性，双曲几何——一种非欧几里得空间——已成为跨领域复杂数据建模的表达性潜在表示空间。论文提出了双曲大型语言模型(HypLLMs)主要技术的分类，并探讨了关键应用和未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have achieved remarkable success anddemonstrated superior performance across various tasks, including naturallanguage processing (NLP), weather forecasting, biological protein folding,text generation, and solving mathematical problems. However, many real-worlddata exhibit highly non-Euclidean latent hierarchical anatomy, such as proteinnetworks, transportation networks, financial networks, brain networks, andlinguistic structures or syntactic trees in natural languages. Effectivelylearning intrinsic semantic entailment and hierarchical relationships fromthese raw, unstructured input data using LLMs remains an underexplored area.Due to its effectiveness in modeling tree-like hierarchical structures,hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularityas an expressive latent representation space for complex data modeling acrossdomains such as graphs, images, languages, and multi-modal data. Here, weprovide a comprehensive and contextual exposition of recent advancements inLLMs that leverage hyperbolic geometry as a representation space to enhancesemantic representation learning and multi-scale reasoning. Specifically, thepaper presents a taxonomy of the principal techniques of Hyperbolic LLMs(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/logmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)hyperbolic state-space models. We also explore crucial potential applicationsand outline future research directions. A repository of key papers, models,datasets, and code implementations is available athttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.</description>
      <author>example@mail.com (Sarang Patil, Zeyong Zhang, Yiran Huang, Tengfei Ma, Mengjia Xu)</author>
      <guid isPermaLink="false">2509.05757v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities</title>
      <link>http://arxiv.org/abs/2509.05615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE TKDE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医学多模态表示学习旨在将异构临床数据整合为统一的患者表示，以支持预测建模，这是医疗数据挖掘领域的一项重要且具有挑战性的任务。然而，现实世界中的医疗数据集常常因成本、协议或患者特定限制而存在模态缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的方法主要通过原始数据空间或特征空间中的可用观测数据进行学习，但通常忽略了数据采集过程本身引入的潜在偏差。现实世界中的医疗数据集经常面临模态缺失问题。&lt;h4&gt;目的&lt;/h4&gt;识别阻碍模型泛化的两种偏差：缺失偏差（由模态可用性的非随机模式导致）和分布偏差（由影响观测特征和结果的潜在混杂因素引起）。提出一个统一框架来解决这些挑战。&lt;h4&gt;方法&lt;/h4&gt;进行数据生成过程的结构因果分析，并提出一个与现有直接预测多模态学习方法兼容的统一框架。该方法包含两个关键组件：(1) 基于后门调整的缺失解混杂模块，用于近似因果干预；(2) 双分支神经网络，明确将因果特征与虚假相关性分离。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界的公共和院内数据集上评估了该方法，证明了其有效性和因果洞察力。&lt;h4&gt;结论&lt;/h4&gt;通过识别和处理缺失偏差和分布偏差，该方法能够提高医学多模态表示学习的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;医学多模态表示学习旨在将异构临床数据整合为统一的患者表示，以支持预测建模，这仍然是医疗数据挖掘领域一项基本且具有挑战性的任务。然而，现实世界中的医疗数据集常常因成本、协议或患者特定限制而面临模态缺失的问题。现有方法主要通过原始数据空间或特征空间中的可用观测数据进行学习，但通常忽略了数据采集过程本身引入的潜在偏差。在这项工作中，我们确定了阻碍模型泛化的两种偏差：缺失偏差，由模态可用性的非随机模式导致；以及分布偏差，由影响观测特征和结果的潜在混杂因素引起。为应对这些挑战，我们对数据生成过程进行了结构因果分析，并提出了一个与现有直接预测多模态学习方法兼容的统一框架。我们的方法包含两个关键组件：(1) 基于后门调整的缺失解混杂模块，用于近似因果干预；(2) 双分支神经网络，明确将因果特征与虚假相关性分离。我们在现实世界的公共和院内数据集上评估了我们的方法，证明了其有效性和因果洞察力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical multimodal representation learning aims to integrate heterogeneousclinical data into unified patient representations to support predictivemodeling, which remains an essential yet challenging task in the medical datamining community. However, real-world medical datasets often suffer frommissing modalities due to cost, protocol, or patient-specific constraints.Existing methods primarily address this issue by learning from the availableobservations in either the raw data space or feature space, but typicallyneglect the underlying bias introduced by the data acquisition process itself.In this work, we identify two types of biases that hinder model generalization:missingness bias, which results from non-random patterns in modalityavailability, and distribution bias, which arises from latent confounders thatinfluence both observed features and outcomes. To address these challenges, weperform a structural causal analysis of the data-generating process and proposea unified framework that is compatible with existing direct prediction-basedmultimodal learning methods. Our method consists of two key components: (1) amissingness deconfounding module that approximates causal intervention based onbackdoor adjustment and (2) a dual-branch neural network that explicitlydisentangles causal features from spurious correlations. We evaluated ourmethod in real-world public and in-hospital datasets, demonstrating itseffectiveness and causal insights.</description>
      <author>example@mail.com (Xiaoguang Zhu, Lianlong Sun, Yang Liu, Pengyi Jiang, Uma Srivatsa, Nipavan Chiamvimonvat, Vladimir Filkov)</author>
      <guid isPermaLink="false">2509.05615v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning</title>
      <link>http://arxiv.org/abs/2509.05606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过自监督学习构建密集表示的框架，解决了全局表示方法在密集预测任务中难以捕捉局部语义的问题。&lt;h4&gt;背景&lt;/h4&gt;密集表示对于需要空间精度和细粒度细节的视觉任务至关重要，而大多数自监督表示学习方法专注于全局表示，往往无法捕捉密集预测任务所需的局部语义信息。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，通过额外的自监督学习建立在预训练表示的基础上，旨在将现有的语义知识转移到密集特征空间中，以克服全局表示方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一种框架，对齐教师模型和学生模型之间的密集特征分布；引入Patch级核对齐（PaKA）作为简单有效的对齐目标，捕捉统计依赖关系，匹配两个模型中密集补丁的结构关系；研究专门为密集表示学习设计的增强策略。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在各种密集视觉基准测试中取得了最先进的结果，证明了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过额外的自监督学习和特定的对齐方法，可以有效地将语义知识转移到密集特征空间，Patch级核对齐（PaKA）是一种有效的方法，可以捕捉统计依赖关系并匹配结构关系。&lt;h4&gt;翻译&lt;/h4&gt;密集表示对于需要空间精度和细粒度细节的视觉任务至关重要。虽然大多数自监督表示学习方法专注于总结整个图像的全局表示，但这类方法通常难以捕捉密集预测任务所需的局部语义。为克服这些局限，我们提出了一种框架，通过额外的自监督学习建立在预训练表示的基础上，旨在将现有的语义知识转移到密集特征空间。我们的方法对齐了教师模型和学生模型之间的密集特征分布。具体而言，我们引入了Patch级核对齐（PaKA），这是一种简单而有效的对齐目标，可以捕捉统计依赖关系，从而匹配两个模型中密集补丁的结构关系。此外，我们还研究了专门为密集表示学习设计的增强策略。我们的框架在各种密集视觉基准测试中取得了最先进的结果，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense representations are essential for vision tasks that require spatialprecision and fine-grained detail. While most self-supervised representationlearning methods focus on global representations that summarize the image as awhole, such approaches often fall short in capturing the localized semanticsnecessary for dense prediction tasks. To overcome these limitations, we proposea framework that builds on pretrained representations through additionalself-supervised learning, aiming to transfer existing semantic knowledge intothe dense feature space. Our method aligns the distributions of dense featuresbetween a teacher and a student model. Specifically, we introduce Patch-levelKernel Alignment (PaKA), a simple yet effective alignment objective thatcaptures statistical dependencies, thereby matching the structuralrelationships of dense patches across the two models. In addition, weinvestigate augmentation strategies specifically designed for denserepresentation learning. Our framework achieves state-of-the-art results acrossa variety of dense vision benchmarks, demonstrating the effectiveness of ourapproach.</description>
      <author>example@mail.com (Juan Yeo, Ijun Jang, Taesup Kim)</author>
      <guid isPermaLink="false">2509.05606v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2509.05478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PLanTS，一个周期感知的自监督学习框架，用于处理多元时间序列数据。该框架能够显式建模不规则潜在状态及其转换，通过多粒度修补机制和对比损失保留时间分辨率上的相似性，并通过下一个转换预测任务捕获时间动态。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列在医疗保健、气候科学和工业监测等领域普遍存在，但它们的高维性、有限的标记数据和非平稳特性对传统机器学习方法构成了重大挑战。现有的自监督学习方法忽略了多元时间序列的内在周期结构，无法捕获潜在状态的动态演化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉多元时间序列周期结构并建模潜在状态动态演化的自监督学习框架，以提高在多种下游任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了PLanTS框架，包含：1)周期感知的多粒度修补机制；2)广义对比损失保留多个时间分辨率上的实例级和状态级相似性；3)下一个转换预测预训练任务，鼓励表示编码关于未来状态演化的预测信息。&lt;h4&gt;主要发现&lt;/h4&gt;PLanTS在多类和多标签分类、预测、轨迹跟踪和异常检测等多种下游任务中评估，在表示质量上持续优于现有的自监督学习方法，并且与基于DTW的方法相比展示了更优的运行时效率。&lt;h4&gt;结论&lt;/h4&gt;PLanTS是一个有效的自监督学习框架，能够处理多元时间序列数据的周期性和动态性，在各种任务中表现优异且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列在医疗保健、气候科学和工业监测等领域普遍存在，但它们的高维性、有限的标记数据和非平稳特性对传统机器学习方法构成了重大挑战。尽管最近的自学习方法通过数据增强或基于时间点的对比策略缓解了标签稀缺问题，但它们忽略了多元时间序列的内在周期结构，并且无法捕获潜在状态的动态演化。我们提出了PLanTS，一个周期感知的自监督学习框架，能够显式地建模不规则潜在状态及其转换。我们首先设计了一个周期感知的多粒度修补机制和一个广义对比损失，以在多个时间分辨率上保留实例级和状态级相似性。为了进一步捕获时间动态，我们设计了一个下一个转换预测预训练任务，鼓励表示编码关于未来状态演化的预测信息。我们在广泛的下游任务中评估了PLanTS，包括多类和多标签分类、预测、轨迹跟踪和异常检测。PLanTS在表示质量上持续优于现有的自监督学习方法，并且与基于DTW的方法相比展示了更优的运行时效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) are ubiquitous in domains such as healthcare,climate science, and industrial monitoring, but their high dimensionality,limited labeled data, and non-stationary nature pose significant challenges forconventional machine learning methods. While recent self-supervised learning(SSL) approaches mitigate label scarcity by data augmentations or timepoint-based contrastive strategy, they neglect the intrinsic periodic structureof MTS and fail to capture the dynamic evolution of latent states. We proposePLanTS, a periodicity-aware self-supervised learning framework that explicitlymodels irregular latent states and their transitions. We first designed aperiod-aware multi-granularity patching mechanism and a generalized contrastiveloss to preserve both instance-level and state-level similarities acrossmultiple temporal resolutions. To further capture temporal dynamics, we designa next-transition prediction pretext task that encourages representations toencode predictive information about future state evolution. We evaluate PLanTSacross a wide range of downstream tasks-including multi-class and multi-labelclassification, forecasting, trajectory tracking and anomaly detection. PLanTSconsistently improves the representation quality over existing SSL methods anddemonstrates superior runtime efficiency compared to DTW-based methods.</description>
      <author>example@mail.com (Jia Wang, Xiao Wang, Chi Zhang)</author>
      <guid isPermaLink="false">2509.05478v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</title>
      <link>http://arxiv.org/abs/2509.06266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个新的基准测试Ego3D-Bench和一种改进框架Ego3D-VLM，用于评估和提高视觉语言模型在3D空间关系理解方面的能力，特别是在以自我为中心的多视图户外场景中。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在理解3D空间关系方面存在明显局限。先前的工作基于单图像或室内视频创建了空间问答数据集，但现实世界中的具身AI代理(如机器人和自动驾驶汽车)通常依赖于以自我为中心的多视图观测。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的基准测试Ego3D-Bench来评估视觉语言模型在使用以自我为中心的多视图户外数据时的空间推理能力，并提出一种改进框架Ego3D-VLM来增强视觉语言模型的3D空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建了包含8,600多个问答对的Ego3D-Bench基准测试，由人工注释者大量参与以确保质量和多样性。提出了Ego3D-VLM后训练框架，该框架基于估计的全局3D坐标生成认知地图。&lt;h4&gt;主要发现&lt;/h4&gt;在16个最先进的视觉语言模型(包括GPT-4o、Gemini1.5-Pro、InternVL3和Qwen2.5-VL)的测试中，发现了人类水平分数与模型性能之间的显著差距，表明当前的视觉语言模型仍无法达到人类水平的空间理解。Ego3D-VLM框架在多选题问答上实现了12%的平均改进，在绝对距离估计上实现了56%的平均改进。&lt;h4&gt;结论&lt;/h4&gt;Ego3D-Bench和Ego3D-VLM共同为在现实世界多视环境中推进人类水平的空间理解提供了有价值的工具。Ego3D-VLM是模块化的，可以与任何现有的视觉语言模型集成。&lt;h4&gt;翻译&lt;/h4&gt;理解3D空间关系仍然是当前视觉语言模型的主要局限性。先前的工作通过创建基于单图像或室内视频的空间问答数据集来解决这一问题。然而，现实世界中的具身AI代理通常依赖于以自我为中心的多视图观测。为此，我们引入了Ego3D-Bench，这是一个新的基准测试，旨在使用以自我为中心的多视图户外数据评估视觉语言模型的空间推理能力。Ego3D-Bench包含8,600多个问答对，由人工注释者大量参与创建，以确保质量和多样性。我们对16个最先进的视觉语言模型进行了基准测试，结果显示人类水平分数与模型性能之间存在显著差距，突显出当前的视觉语言模型仍未达到人类水平的空间理解。为了弥合这一差距，我们提出了Ego3D-VLM，一种后训练框架，可以增强视觉语言模型的3D空间推理能力。Ego3D-VLM基于估计的全局3D坐标生成认知地图，在多选题问答上实现了12%的平均改进，在绝对距离估计上实现了56%的平均改进。Ego3D-Bench和Ego3D-VLM共同为在现实世界多视环境中推进人类水平的空间理解提供了有价值的工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视觉语言模型（VLMs）在3D空间推理方面的能力不足问题，特别是在以自我为中心的多视角场景中。这个问题很重要，因为现实世界中的具身AI代理（如机器人和自动驾驶汽车）需要理解3D空间关系来感知环境、估计物体距离和推理运动，而现有的空间理解基准测试主要基于单图像或室内视频，无法反映这些代理的真实感知体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有工作的局限性：现有空间基准测试基于单图像或室内静态视频，与真实世界多视角动态场景不符；点云和鸟瞰图方法虽提供丰富空间信息但难以在动态环境中重建且计算成本高。作者借鉴了人类自然整合多视角视觉信息形成统一空间表示的能力，设计了一个名为Ego3D-VLM的后训练框架，生成文本认知地图而非复杂3D表示。该方法整合了现有的2D目标检测和深度估计技术，但将其统一到更符合人类感知机制的框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个以自我为中心的文本认知地图，定义以代理为中心的坐标系，并将重要物体定位在3D坐标空间中。这种方法只关注提示中提到的物体，减少输入令牌数量，实现高效推理。实现流程包括：1)使用REC模型找到指代表达式的2D位置；2)使用深度估计器估计深度值；3)将2D点转换为3D点；4)将所有视角的3D点转换为全局坐标系；5)生成文本认知地图；6)将认知地图、多视角图像和查询输入VLM得到答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Ego3D-Bench基准测试，首个针对以自我为中心多视角场景的3D空间理解基准，包含8600个问答对；2)Ego3D-VLM框架，一种即插即用的后训练方法，生成文本认知地图而非复杂3D表示；3)关系缩放技术基于常识物体大小进行比例调整。相比之前工作，现有方法主要使用点云或鸟瞰图表示，在动态环境中难以重建且计算成本高；现有基准测试基于单图像或室内视频，不符合实际应用需求；Ego3D-VLM专注于多视角场景，更符合实际应用，且可集成到任何现有VLM中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出首个针对以自我为中心多视角场景的3D空间理解基准测试（Ego3D-Bench）和一种增强VLMs空间推理能力的后训练框架（Ego3D-VLM），显著缩小了当前VLMs与人类在3D空间理解能力之间的差距。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D spatial relationships remains a major limitation of currentVision-Language Models (VLMs). Prior work has addressed this issue by creatingspatial question-answering (QA) datasets based on single images or indoorvideos. However, real-world embodied AI agents such as robots and self-drivingcars typically rely on ego-centric, multi-view observations. To this end, weintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatialreasoning abilities of VLMs using ego-centric, multi-view outdoor data.Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvementfrom human annotators to ensure quality and diversity. We benchmark 16 SOTAVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our resultsreveal a notable performance gap between human level scores and VLMperformance, highlighting that current VLMs still fall short of human levelspatial understanding. To bridge this gap, we propose Ego3D-VLM, apost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLMgenerates cognitive map based on estimated global 3D coordinates, resulting in12% average improvement on multi-choice QA and 56% average improvement onabsolute distance estimation. Ego3D-VLM is modular and can be integrated withany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools foradvancing toward human level spatial understanding in real-world, multi-viewenvironments.</description>
      <author>example@mail.com (Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Yong Zhang, Mohammad Akbari)</author>
      <guid isPermaLink="false">2509.06266v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</title>
      <link>http://arxiv.org/abs/2509.06079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种标题辅助推理框架，有效解决了多模态推理中的挑战，在ICML 2025挑战赛中获得第一名，并在MathVerse基准测试上验证了其泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态推理是人工智能中的基本挑战，尽管基于文本的推理取得了实质性进展，但最先进的模型如GPT-o3在多模态场景中仍难以保持强大性能。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效桥接视觉和文本模态的推理框架，以解决多模态推理中的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种标题辅助推理框架，用于有效连接视觉和文本模态。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在ICML 2025 AI for Math Workshop &amp; Challenge 2: SeePhys中获得第一名，证明了其有效性和鲁棒性；在MathVerse基准测试上验证了其在几何推理方面的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;标题辅助推理框架能够有效解决多模态推理挑战，具有良好的性能和泛化能力，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;多模态推理仍然是人工智能中的一个基本挑战。尽管基于文本的推理取得了实质性进展，但即使是像GPT-o3这样的最先进模型也难以在多模态场景中保持强大的性能。为了解决这一差距，我们引入了一个标题辅助推理框架，有效地桥接了视觉和文本模态。我们的方法在ICML 2025 AI for Math Workshop &amp; Challenge 2: SeePhys中获得了第一名，突显了其有效性和鲁棒性。此外，我们在MathVerse基准测试上验证了其在几何推理方面的泛化能力，展示了我们方法的通用性。我们的代码已在https://github.com/OpenDCAI/SciReasoner上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal reasoning remains a fundamental challenge in artificialintelligence. Despite substantial advances in text-based reasoning, evenstate-of-the-art models such as GPT-o3 struggle to maintain strong performancein multimodal scenarios. To address this gap, we introduce a caption-assistedreasoning framework that effectively bridges visual and textual modalities. Ourapproach achieved 1st place in the ICML 2025 AI for Math Workshop \&amp; Challenge2: SeePhys, highlighting its effectiveness and robustness. Furthermore, wevalidate its generalization on the MathVerse benchmark for geometric reasoning,demonstrating the versatility of our method. Our code is publicly available athttps://github.com/OpenDCAI/SciReasoner.</description>
      <author>example@mail.com (Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong)</author>
      <guid isPermaLink="false">2509.06079v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation</title>
      <link>http://arxiv.org/abs/2509.05746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于空间自适应重建策略的单图像超分辨率方法，解决了传统方法中空间不变退化模型的局限性。通过引入几何场景理解和深度依赖效应，作者建立了一个严格的变分框架，并将其实现为一个具有深度条件卷积核的神经架构。该方法在多个基准数据集上取得了最先进的结果，特别是在深度变化场景中表现出显著改进。&lt;h4&gt;背景&lt;/h4&gt;传统单图像超分辨率方法假设空间不变的退化模型，然而真实世界的成像系统表现出复杂的空间依赖效应，包括大气散射、景深变化和透视失真等。这种基本局限性需要空间自适应重建策略，明确结合几何场景理解以获得最佳性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理空间依赖效应（特别是与距离相关的效应）的超分辨率方法，建立将超分辨率描述为空间变化逆问题的理论框架，并实现一个能够根据深度信息自适应调整的神经架构。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种严格的变分框架，将超分辨率描述为空间变化的逆问题，将退化算子建模为具有距离依赖谱特性的伪微分算子。神经架构通过级联残差块实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时包含学习到的距离自适应正则化项，根据局部几何结构动态调整平滑约束。基于大气散射理论的光谱约束防止远场区域中的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的全面评估展示了最先进的性能，在KITTI户外场景2倍和4倍放大时分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM值，分别比现有方法高出0.44dB和0.36dB。该方法在深度变化场景中显示出显著改进，同时在传统基准测试中保持了竞争性性能。&lt;h4&gt;结论&lt;/h4&gt;该工作建立了首个理论上合理的距离自适应超分辨率框架，在深度变化场景上展示了显著改进，同时在传统基准测试中保持了竞争性性能，为处理真实世界成像系统中的复杂空间依赖效应提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;单图像超分辨率传统上假设空间不变的退化模型，而真实世界的成像系统表现出复杂的空间依赖效应，包括大气散射、景深变化和透视失真。这种基本局限性需要明确结合几何场景理解的空间自适应重建策略以获得最佳性能。我们提出了一种严格的变分框架，将超分辨率描述为空间变化的逆问题，将退化算子建模为具有距离依赖谱特性的伪微分算子，从而能够跨深度范围分析重建极限的理论。我们的神经架构通过具有深度条件卷积核的级联残差块实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时包含学习到的距离自适应正则化项，这些项根据局部几何结构动态调整平滑约束。从大气散射理论导出的光谱约束防止远场区域中的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。在五个基准数据集上的全面评估展示了最先进的性能，在KITTI户外场景2倍和4倍放大时分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM值，分别比现有方法高出0.44dB和0.36dB。该工作建立了首个理论上合理的距离自适应超分辨率框架，并在深度变化场景上展示了显著改进，同时在传统基准测试中保持了竞争性性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决传统单图像超分辨率方法假设空间不变退化模型的问题，而现实世界成像系统存在复杂的距离相关效应（如大气散射、景深变化、透视失真）。这一问题重要是因为户外场景（如自动驾驶、卫星成像）中图像质量随距离变化而退化，传统方法无法处理这种空间变化的退化，导致在真实场景中效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析发现户外场景与室内场景有不同的退化机制，因此构建了基于伪微分算子的变分框架来建模距离相关的退化。他们借鉴了大气散射理论（如Mie散射）来推导光谱约束，同时利用深度估计网络获取场景几何信息。方法设计上借鉴了残差网络架构和梯度流优化技术，但将其扩展为深度条件卷积和距离自适应正则化，实现了理论框架与神经网络的结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将超分辨率建模为考虑距离相关退化的空间变化逆问题，利用深度信息指导重建过程，并根据大气散射理论限制重建带宽防止噪声放大。整体流程包括：1)提取深度图；2)生成距离自适应卷积核；3)初始化重建图像；4)迭代优化（计算数据保真度、应用距离自适应正则化、更新重建）；5)最终上采样输出高分辨率图像。整个过程中使用梯度流块逐步优化重建结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于伪微分算子的理论变分框架；2)深度自适应正则化项，根据距离动态调整平滑约束；3)基于大气散射理论的光谱约束，防止远场噪声放大；4)深度条件卷积核和自适应核生成网络。相比之前工作，不同之处在于：传统方法假设空间不变退化，而本文处理空间变化的退化；本文有严格的理论基础（传统方法多为启发式设计）；专门针对户外场景优化（传统方法在室内场景表现好但户外效果差）；显式利用深度信息指导重建（传统方法通常不利用深度信息）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于距离自适应变分公式的深度感知超分辨率方法，通过理论建模和神经网络实现了对户外场景中距离相关退化的有效处理，显著提升了超分辨率重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single image super-resolution traditionally assumes spatially-invariantdegradation models, yet real-world imaging systems exhibit complexdistance-dependent effects including atmospheric scattering, depth-of-fieldvariations, and perspective distortions. This fundamental limitationnecessitates spatially-adaptive reconstruction strategies that explicitlyincorporate geometric scene understanding for optimal performance. We propose arigorous variational framework that characterizes super-resolution as aspatially-varying inverse problem, formulating the degradation operator as apseudodifferential operator with distance-dependent spectral characteristicsthat enable theoretical analysis of reconstruction limits across depth ranges.Our neural architecture implements discrete gradient flow dynamics throughcascaded residual blocks with depth-conditional convolution kernels, ensuringconvergence to stationary points of the theoretical energy functional whileincorporating learned distance-adaptive regularization terms that dynamicallyadjust smoothness constraints based on local geometric structure. Spectralconstraints derived from atmospheric scattering theory prevent bandwidthviolations and noise amplification in far-field regions, while adaptive kernelgeneration networks learn continuous mappings from depth to reconstructionfilters. Comprehensive evaluation across five benchmark datasets demonstratesstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIMat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by0.44dB and 0.36dB respectively. This work establishes the firsttheoretically-grounded distance-adaptive super-resolution framework anddemonstrates significant improvements on depth-variant scenarios whilemaintaining competitive performance across traditional benchmarks.</description>
      <author>example@mail.com (Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu)</author>
      <guid isPermaLink="false">2509.05746v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</title>
      <link>http://arxiv.org/abs/2509.05578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为OccVLA的新框架，将3D占用表示集成到多模态推理过程中，解决了多模态大语言模型在3D空间理解方面的局限性，特别是在自动驾驶领域的应用。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)展现出强大的视觉-语言推理能力，但缺乏稳健的3D空间理解能力，这对自动驾驶至关重要。这种局限性源于两个关键挑战：(1)构建易于获取且有效的3D表示困难，需要昂贵的手动标注；(2)由于缺乏大规模3D视觉语言预训练，VLMs会丢失细粒度的空间细节。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在3D空间理解方面的局限性，特别是在自动驾驶领域的应用，通过创新方法提升模型的3D空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;OccVLA框架将密集的3D占用同时作为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。与依赖显式3D输入的先前方法不同，OccVLA将占用预测视为隐式推理过程，在推理过程中可以跳过而不会导致性能下降，因此不会增加额外的计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越的性能，为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。&lt;h4&gt;结论&lt;/h4&gt;OccVLA通过创新地将3D占用表示集成到多模态推理过程中，有效解决了MLLMs在3D空间理解方面的局限性，为自动驾驶领域提供了一个高效且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已经展现出强大的视觉-语言推理能力，但仍然缺乏稳健的3D空间理解能力，这对于自动驾驶至关重要。这种局限性源于两个关键挑战：(1)在没有昂贵手动标注的情况下构建易于获取且有效的3D表示存在困难，以及(2)由于缺乏大规模3D视觉语言预训练，VLMs会丢失细粒度的空间细节。为了解决这些挑战，我们提出了OccVLA，一种新颖的框架，将3D占用表示集成到统一的多模态推理过程中。与依赖显式3D输入的先前方法不同，OccVLA将密集的3D占用同时作为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。占用预测被视为隐式推理过程，在推理过程中可以跳过而不会导致性能下降，因此不会增加额外的计算开销。OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越的性能，为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)缺乏稳健3D空间理解能力的问题，这对自动驾驶至关重要。这个问题很重要，因为自动驾驶需要强大的3D感知能力进行定位和导航，而3D感知的保真度直接影响下游决策的安全性。现有方法要么依赖昂贵的人工3D标注数据，要么在处理3D输入时丢失细粒度空间细节，限制了自动驾驶系统的性能和可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：VLM-based方法依赖稀疏的文本描述3D标注，而整合3D输入的方法受限于缺乏大规模3D预训练。作者借鉴了自动化标注流程和Transformer模型在占用表示方面的进展，设计了OccVLA框架。该方法通过交叉注意力机制使占用令牌从视觉特征中学习，并在潜在空间中预测占用以解决空间稀疏性问题，同时允许在推理时跳过占用预测以提高效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将密集3D占用表示作为模型的预测输出和监督信号，从2D视觉输入中学习细粒度空间结构，且推理时可跳过占用预测过程。整体流程包括：1)占用预测阶段：占用令牌通过交叉注意力获取视觉特征，在潜在空间预测占用后映射回高分辨率空间；2)运动规划阶段：分解为元动作预测(速度和方向)和坐标生成；3)三阶段训练：自动驾驶场景预训练、占用-语言联合训练和规划头训练，最终实现从视觉输入到未来轨迹的端到端处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新颖的OccVLA框架，将3D占用作为预测输出和监督信号而非输入；2)占用预测可作为隐式推理过程，推理时可跳过而不增加计算开销；3)通过占用监督增强VLM的3D表示能力；4)输出可解释且可定量评估。相比之前工作，该方法不依赖昂贵3D标注或额外3D传感器，保留了VLM的泛化能力，在nuScenes上实现了最先进的轨迹规划性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OccVLA通过将3D占用预测作为视觉语言模型的隐式推理和监督信号，实现了无需额外3D传感器输入的高效3D空间理解，并在自动驾驶任务中取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong vision-languagereasoning abilities but still lack robust 3D spatial understanding, which iscritical for autonomous driving. This limitation stems from two key challenges:(1) the difficulty of constructing accessible yet effective 3D representationswithout expensive manual annotations, and (2) the loss of fine-grained spatialdetails in VLMs due to the absence of large-scale 3D vision-languagepretraining. To address these challenges, we propose OccVLA, a novel frameworkthat integrates 3D occupancy representations into a unified multimodalreasoning process. Unlike prior approaches that rely on explicit 3D inputs,OccVLA treats dense 3D occupancy as both a predictive output and a supervisorysignal, enabling the model to learn fine-grained spatial structures directlyfrom 2D visual inputs. The occupancy predictions are regarded as implicitreasoning processes and can be skipped during inference without performancedegradation, thereby adding no extra computational overhead. OccVLA achievesstate-of-the-art results on the nuScenes benchmark for trajectory planning anddemonstrates superior performance on 3D visual question-answering tasks,offering a scalable, interpretable, and fully vision-based solution forautonomous driving.</description>
      <author>example@mail.com (Ruixun Liu, Lingyu Kong, Derun Li, Hang Zhao)</author>
      <guid isPermaLink="false">2509.05578v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>http://arxiv.org/abs/2509.06426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了第一个果蝇腿部的3D数据驱动肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现，能够模拟果蝇行为并研究肌肉协同作用和关节特性对学习的影响。&lt;h4&gt;背景&lt;/h4&gt;计算模型对于理解神经、生物力学和物理系统如何协调动物行为至关重要。尽管已有果蝇神经系统、肌肉和外骨骼的完整重建，但仍缺乏基于解剖学和物理学的果蝇腿部肌肉模型，这种模型是连接运动神经元活动和关节运动的必要桥梁。&lt;h4&gt;目的&lt;/h4&gt;开发第一个果蝇腿部的3D数据驱动肌肉骨骼模型，研究运动控制，了解生物力学如何促进复杂肢体运动的产生，并用于控制人工智能体在模拟环境中生成自然柔顺的运动。&lt;h4&gt;方法&lt;/h4&gt;创建3D数据驱动的果蝇腿部肌肉骨骼模型，使用基于高分辨率X射线扫描的Hill型肌肉表示；提出构建肌肉模型的流程和优化未知肌肉参数的方法；将模型与果蝇行为数据结合实现行为重现；在MuJoCo中训练模仿学习策略测试关节特性对学习速度的影响。&lt;h4&gt;主要发现&lt;/h4&gt;模拟行走和梳理行为中的肌肉活动，预测了可实验测试的协调肌肉协同作用；阻尼和刚度特性有助于提高学习速度。&lt;h4&gt;结论&lt;/h4&gt;该模型使得在实验上易于处理的模式生物中研究运动控制成为可能，提供了关于生物力学如何促进复杂肢体运动产生的见解；同时可用于控制具身人工智能体在模拟环境中生成自然和柔顺的运动。&lt;h4&gt;翻译&lt;/h4&gt;计算模型对于推进我们对神经、生物力学和物理系统如何相互作用以协调动物行为的理解至关重要。尽管已有果蝇中枢神经系统、肌肉和外骨骼的近乎完整的重建，但仍然缺乏基于解剖学和物理学的果蝇腿部肌肉模型。这些模型提供了运动神经元活动和关节运动之间不可或缺的桥梁。在此，我们介绍了第一个果蝇腿部的3D数据驱动肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现。我们的模型纳入了基于来自多个固定标本的高分辨率X射线扫描的Hill型肌肉表示。我们提出了一个使用形态成像数据构建肌肉模型的流程，以及优化果蝇特定未知肌肉参数的方法。随后，我们将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据结合，在OpenSim中实现肌肉驱动的行为重现。对行走和梳理行为中肌肉活动的模拟预测了可实验测试的协调肌肉协同作用。此外，通过在MuJoCo中训练模仿学习策略，我们测试了不同被动关节特性对学习速度的影响，发现阻尼和刚度有助于学习。总体而言，我们的模型使得在实验上易于处理的模式生物中研究运动控制成为可能，提供了关于生物力学如何促进复杂肢体运动产生的见解。此外，我们的模型可用于控制具身人工智能体，在模拟环境中生成自然和柔顺的运动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models are critical to advance our understanding of how neural,biomechanical, and physical systems interact to orchestrate animal behaviors.Despite the availability of near-complete reconstructions of the Drosophilamelanogaster central nervous system, musculature, and exoskeleton, anatomicallyand physically grounded models of fly leg muscles are still missing. Thesemodels provide an indispensable bridge between motor neuron activity and jointmovements. Here, we introduce the first 3D, data-driven musculoskeletal modelof Drosophila legs, implemented in both OpenSim and MuJoCo simulationenvironments. Our model incorporates a Hill-type muscle representation based onhigh-resolution X-ray scans from multiple fixed specimens. We present apipeline for constructing muscle models using morphological imaging data andfor optimizing unknown muscle parameters specific to the fly. We then combineour musculoskeletal models with detailed 3D pose estimation data from behavingflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations ofmuscle activity across diverse walking and grooming behaviors predictcoordinated muscle synergies that can be tested experimentally. Furthermore, bytraining imitation learning policies in MuJoCo, we test the effect of differentpassive joint properties on learning speed and find that damping and stiffnessfacilitate learning. Overall, our model enables the investigation of motorcontrol in an experimentally tractable model organism, providing insights intohow biomechanics contribute to generation of complex limb movements. Moreover,our model can be used to control embodied artificial agents to generatenaturalistic and compliant locomotion in simulated environments.</description>
      <author>example@mail.com (Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya)</author>
      <guid isPermaLink="false">2509.06426v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</title>
      <link>http://arxiv.org/abs/2509.05830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何通过微调大型语言模型来提高社会科学实验模拟的准确性。研究者构建了SocSci210数据集，包含来自210个开源社会科学实验的290万份参与者回应。通过微调，模型在未见过的研究中表现优异，且能减少偏差。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型为模拟社会科学实验结果提供了强大机会，但如何提高这些模拟的准确性是一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;通过在过去的个体层面回应数据上直接微调大型语言模型，提高跨不同社会科学领域实验模拟的准确性。&lt;h4&gt;方法&lt;/h4&gt;构建SocSci210数据集，包含210个开源社会科学实验中400,491名参与者的290万份回应，并通过微调训练模型，特别是Socrates-Qwen-14B模型。&lt;h4&gt;主要发现&lt;/h4&gt;在完全未见过的研究中，Socrates-Qwen-14B模型预测与人类回应分布的 alignment 比基础模型高26%，比GPT-4o高13%；在研究条件子集上微调，对新未见条件的泛化能力提高71%；通过微调，人口统计公平性偏差降低10.6%。&lt;h4&gt;结论&lt;/h4&gt;由于社会科学经常生成丰富、主题特定的数据集，在这些数据上进行微调可以实现更准确的实验假设筛选模拟。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型为模拟社会科学实验结果提供了强大机会。在本工作中，我们证明直接在过往实验的个体层面回应上微调大型语言模型，能显著提高跨不同社会科学领域此类模拟的准确性。我们通过自动流程构建了SocSci210，这是一个包含来自210个开源社会科学实验的400,491名参与者的290万份回应的数据集。通过微调，我们实现了多个层次的泛化。在完全未见过的研究中，我们最强大的模型Socrates-Qwen-14B产生的预测与不同条件下对多样化结果问题的人类回应分布更加一致，相对于其基础模型(Qwen2.5-14B)提高了26%，优于GPT-4o达13%。通过在研究条件的子集上进行微调，对新的未见条件的泛化特别稳健，提高了71%。由于SocSci210包含丰富的人口统计信息，我们通过微调将人口统计公平性(一种偏差度量)降低了10.6%。由于社会科学常规生成丰富、主题特定的数据集，我们的研究结果表明，在这些数据上进行微调可以 enable 更准确的实验假设筛选模拟。我们在stanfordhci.github.io/socrates发布了我们的数据、模型和微调代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) offer a powerful opportunity to simulate theresults of social science experiments. In this work, we demonstrate thatfinetuning LLMs directly on individual-level responses from past experimentsmeaningfully improves the accuracy of such simulations across diverse socialscience domains. We construct SocSci210 via an automatic pipeline, a datasetcomprising 2.9 million responses from 400,491 participants in 210 open-sourcesocial science experiments. Through finetuning, we achieve multiple levels ofgeneralization. In completely unseen studies, our strongest model,Socrates-Qwen-14B, produces predictions that are 26% more aligned withdistributions of human responses to diverse outcome questions under varyingconditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by13%. By finetuning on a subset of conditions in a study, generalization to newunseen conditions is particularly robust, improving by 71%. Since SocSci210contains rich demographic information, we reduce demographic parity, a measureof bias, by 10.6% through finetuning. Because social sciences routinelygenerate rich, topic-specific datasets, our findings indicate that finetuningon such data could enable more accurate simulations for experimental hypothesisscreening. We release our data, models and finetuning code atstanfordhci.github.io/socrates.</description>
      <author>example@mail.com (Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein)</author>
      <guid isPermaLink="false">2509.05830v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory</title>
      <link>http://arxiv.org/abs/2509.05337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at IEEE RO-MAN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合动态图神经网络和长短期记忆网络的混合模型，用于预测人类跌倒。该模型将运动预测和步态分类任务解耦，能够高精度地预测跌倒并监控稳定到跌倒之间的过渡状态。&lt;h4&gt;背景&lt;/h4&gt;检测和预防人类跌倒是辅助机器人系统的关键组成部分。虽然跌倒检测已取得进展，但对跌倒发生前的预测以及稳定性与即将发生的跌倒之间过渡状态的分析尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种预期跌倒检测方法，能够高精度地预测跌倒，并分析稳定与即将跌倒之间的过渡状态。&lt;h4&gt;方法&lt;/h4&gt;提出一种混合模型，结合动态图神经网络（DGNN）和长短期记忆（LSTM）网络，将运动预测和步态分类任务解耦。使用从视频序列中提取的实时骨骼特征作为输入，DGNN作为分类器区分三种步态状态，LSTM网络预测后续时间步中的人体运动以实现早期跌倒检测。&lt;h4&gt;主要发现&lt;/h4&gt;模型使用OUMVLP-Pose和URFD数据集训练验证后，在预测误差和识别准确性方面优于仅依赖DGNN的模型和文献中的模型。解耦预测和分类比统一处理问题更有效，且能监控过渡状态，为辅助系统提供额外价值。&lt;h4&gt;结论&lt;/h4&gt;解耦预测和分类任务的方法可有效提高跌倒预测性能，且能够监控过渡状态，为增强先进辅助系统的功能提供有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;检测和预防人类跌倒是辅助机器人系统的关键组成部分。虽然跌倒检测方面已取得显著进展，但对跌倒发生前的预测以及稳定性与即将发生的跌倒之间过渡状态的分析尚未被探索。本文提出了一种预期跌倒检测方法，利用结合动态图神经网络和长短期记忆网络的混合模型，将运动预测和步态分类任务解耦，从而高精度地预测跌倒。我们的方法使用从视频序列中提取的实时骨骼特征作为模型输入。DGNN作为分类器，区分三种步态状态：稳定、过渡和跌倒。基于LSTM的网络随后预测后续时间步中的人体运动，实现跌倒的早期检测。所提模型使用OUMVLP-Pose和URFD数据集进行训练和验证，在预测误差和识别准确性方面表现出优于仅依赖DGNN的模型和文献中的模型。结果表明，与仅使用DGNN解决统一问题相比，解耦预测和分类可以提高性能。此外，我们的方法允许监控过渡状态，为增强先进辅助系统的功能提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting and preventing falls in humans is a critical component of assistiverobotic systems. While significant progress has been made in detecting falls,the prediction of falls before they happen, and analysis of the transient statebetween stability and an impending fall remain unexplored. In this paper, wepropose a anticipatory fall detection method that utilizes a hybrid modelcombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory(LSTM) networks that decoupled the motion prediction and gait classificationtasks to anticipate falls with high accuracy. Our approach employs real-timeskeletal features extracted from video sequences as input for the proposedmodel. The DGNN acts as a classifier, distinguishing between three gait states:stable, transient, and fall. The LSTM-based network then predicts humanmovement in subsequent time steps, enabling early detection of falls. Theproposed model was trained and validated using the OUMVLP-Pose and URFDdatasets, demonstrating superior performance in terms of prediction error andrecognition accuracy compared to models relying solely on DGNN and models fromliterature. The results indicate that decoupling prediction and classificationimproves performance compared to addressing the unified problem using only theDGNN. Furthermore, our method allows for the monitoring of the transient state,offering valuable insights that could enhance the functionality of advancedassistance systems.</description>
      <author>example@mail.com (Younggeol Cho, Gokhan Solak, Olivia Nocentini, Marta Lorenzini, Andrea Fortuna, Arash Ajoudani)</author>
      <guid isPermaLink="false">2509.05337v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</title>
      <link>http://arxiv.org/abs/2509.06781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真实时副本：LUMPI、V2X-Real-IC和TUMTraf-I。每个数据集包含10K个带注释的帧，包括3D边界框、实例分割标签、跟踪ID和语义分割标签。&lt;h4&gt;背景&lt;/h4&gt;激光雷达感知任务需要大量高质量数据，但真实数据的获取可能受到限制，且场景多样性有限。&lt;h4&gt;目的&lt;/h4&gt;创建高质量的合成数据集，能够替代真实数据用于激光雷达感知任务，并增强现有基准数据集。&lt;h4&gt;方法&lt;/h4&gt;通过仿真激光雷达传感器在精确建模的数字孪生中合成数据，基于实际位置的周围几何形状、道路对齐和车辆运动模式建模。通过统计和结构相似性分析评估合成数据与真实数据的对齐情况，并训练3D目标检测模型进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据与真实数据高度相似，仅使用合成数据训练的模型在真实、未见数据上的检测性能优于使用真实数据训练的模型。UrbanTwin数据集可通过修改模拟设计来适应测试自定义场景。&lt;h4&gt;结论&lt;/h4&gt;UrbanTwin数据集是首批能够替代领域内真实世界数据集用于激光雷达感知任务的数字合成数据集，通过增加样本量和场景多样性有效增强了现有基准数据集。&lt;h4&gt;翻译&lt;/h4&gt;这篇文章介绍了UrbanTwin数据集 - 三个公共路边激光雷达数据集的高保真真实副本：LUMPI、V2X-Real-IC和TUMTraf-I。每个UrbanTwin数据集包含10K个注释帧，对应于一个公共数据集。注释包括六个类别的3D边界框、实例分割标签和跟踪ID，以及九个类别的语义分割标签。这些数据集是在真实数字孪生中使用仿真的激光雷达传感器合成的，基于实际位置的周围几何形状、车道级别的道路对齐以及交叉口的车道拓扑和车辆运动模式建模。由于精确的数字孪生建模，合成数据集与真实数据很好地对齐，为训练深度学习模型提供了强大的独立和增强价值。我们通过统计和结构相似性分析评估了合成数据与真实数据的对齐情况，并通过仅使用合成数据训练3D目标检测模型并在真实数据上进行测试，进一步证明了它们的效用。与使用真实数据训练的模型相比，高相似度分数和改进的检测性能表明，UrbanTwin数据集通过增加样本量和场景多样性有效地增强了现有基准数据集。此外，数字孪生可以通过修改模拟的设计和动力学来适应测试自定义场景。据我们所知，这些是首批能够替代领域内真实世界数据集用于激光雷达感知任务的数字合成数据集。UrbanTwin数据集可在https://dataverse.harvard.edu/dataverse/ucf-ut公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决创建高质量路边激光雷达数据集的高成本和低效率问题。当前真实世界标注的激光雷达数据集需要大量人力、时间和资金投入，而现有模拟环境与真实世界存在明显差距，缺乏专门针对路边激光雷达应用的合成数据集。这个问题在研究中很重要，因为激光雷达技术对智能交通系统的感知算法发展至关重要，高质量数据集是训练和评估3D感知算法的基础，而现有数据集的局限性阻碍了相关研究的进展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有数据集创建的高成本和现有模拟器的局限性，然后提出使用数字孪生技术来精确建模真实世界场景的物理结构和行为动态。他们借鉴了现有的激光雷达模拟技术，如Manivasagam的配对场景方法学和Haider的精确光线追踪模型，以及数据驱动的生成方法如CoLiGen框架。但作者的方法从根本上不同于这些工作，不是通过事后数据驱动适应来解决sim-to-real差距，而是通过高保真数字孪生建模从源头提高模拟真实性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高保真数字孪生技术创建与真实世界路边激光雷达数据集高度匹配的合成数据集，同时模拟真实世界场景的物理结构和行为动态。整体流程包括：1)使用卫星图像和真实位置测量构建精确的3D环境模型；2)配置虚拟激光雷达传感器匹配真实传感器规格；3)随机生成符合交通规则的动态元素；4)使用CARLA模拟器在数字孪生环境中运行模拟；5)生成带标注的点云数据；6)通过统计和结构相似性分析验证数据质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对路边激光雷达应用的合成数据集；2)高保真数字孪生建模方法，整合静态几何和动态行为；3)通过精确建模实现最小化sim-to-real差距；4)证明完全在合成数据上训练的模型可匹配或超过真实数据训练的性能。相比之前工作，UrbanTwin更专注于路边激光雷达应用而非车辆-基础设施协作场景；从根本上提高模拟真实性而非事后数据驱动适应；实现了比之前合成数据集更接近真实世界的质量；不仅用于数据增强，还可作为独立训练数据集使用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanTwin通过高保真数字孪生技术创建了首个能够完全替代领域内真实世界数据集的合成激光雷达数据集，显著降低了sim-to-real差距，并证明了其在3D物体检测等任务中的卓越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents UrbanTwin datasets - high-fidelity, realistic replicasof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.Each UrbanTwin dataset contains 10K annotated frames corresponding to one ofthe public datasets. Annotations include 3D bounding boxes, instancesegmentation labels, and tracking IDs for six object classes, along withsemantic segmentation labels for nine classes. These datasets are synthesizedusing emulated lidar sensors within realistic digital twins, modeled based onsurrounding geometry, road alignment at lane level, and the lane topology andvehicle movement patterns at intersections of the actual locationscorresponding to each real dataset. Due to the precise digital twin modeling,the synthetic datasets are well aligned with their real counterparts, offeringstrong standalone and augmentative value for training deep learning models ontasks such as 3D object detection, tracking, and semantic and instancesegmentation. We evaluate the alignment of the synthetic replicas throughstatistical and structural similarity analysis with real data, and furtherdemonstrate their utility by training 3D object detection models solely onsynthetic data and testing them on real, unseen data. The high similarityscores and improved detection performance, compared to the models trained onreal data, indicate that the UrbanTwin datasets effectively enhance existingbenchmark datasets by increasing sample size and scene diversity. In addition,the digital twins can be adapted to test custom scenarios by modifying thedesign and dynamics of the simulations. To our knowledge, these are the firstdigitally synthesized datasets that can replace in-domain real-world datasetsfor lidar perception tasks. UrbanTwin datasets are publicly available athttps://dataverse.harvard.edu/dataverse/ucf-ut.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.06781v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</title>
      <link>http://arxiv.org/abs/2509.05999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages. Accepted to MMSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于预计算分割信息先验的解耦策略，用于单目3D目标检测，将分割信息直接融合到特征空间中指导检测，而不扩展模型或联合学习先验。&lt;h4&gt;背景&lt;/h4&gt;单目3D目标检测是一项具有挑战性的计算机视觉任务，因为输入是单个2D图像，缺乏深度线索，使得深度估计成为一个不适定的问题。&lt;h4&gt;目的&lt;/h4&gt;评估额外分割信息对现有检测管道的影响，而不添加额外的预测分支，并探索理解输入数据对减少对额外传感器或训练数据需求的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出一种解耦策略，基于注入预计算的分割信息先验，并将它们直接融合到特征空间中用于指导检测，不扩展检测模型或联合学习先验。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI 3D目标检测基准上的评估表明，所提出的方法对于场景中的小物体（行人和骑行者）的表现优于仅依赖RGB图像特征的等效架构。&lt;h4&gt;结论&lt;/h4&gt;理解输入数据可以平衡对额外传感器或训练数据的需求，证明了分割信息对单目3D目标检测的积极影响。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测是一项具有挑战性的计算机视觉任务，因为所使用的输入是单个2D图像，缺乏任何深度线索，并将深度估计问题作为一个不适定的问题。现有解决方案利用从输入中提取的信息，使用卷积神经网络或Transformer架构作为特征提取主干，然后使用特定的检测头来预测3D参数。在本文中，我们介绍了一种基于注入预计算分割信息先验的解耦策略，并将它们直接融合到特征空间中用于指导检测，而不扩展检测模型或联合学习先验。重点在于评估额外分割信息对现有检测管道的影响，而不添加额外的预测分支。所提出的方法在KITTI 3D目标检测基准上进行了评估，对于场景中的小物体（行人和骑行者）的表现优于仅依赖RGB图像特征的等效架构，证明了理解输入数据可以平衡对额外传感器或训练数据的需求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D物体检测的挑战问题。单目3D物体检测使用单个2D图像作为输入，缺乏深度线索，导致这是一个'不适定'的问题。这个问题在自动驾驶和机器人领域非常重要，因为准确检测3D物体（如汽车、行人、骑行者）的位置、方向和尺寸对于安全导航至关重要。单目相机比多传感器系统更便宜、更轻量，但缺乏深度信息使其成为最具挑战性的任务，尤其是对小物体的检测。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单目3D物体检测的挑战，认识到缺乏深度信息是主要问题。他们借鉴了'引导式单目3D物体检测'的概念，参考了使用深度图或分割图作为先验信息的工作。特别是受到了DetAny3D和MonoCInIS的启发，但选择使用分割图而非深度图，因为分割图能提供更丰富的语义信息。作者设计了一个'解耦'策略，使用Grounded SAM生成高质量分割先验，然后探索不同的融合策略和点，最终确定了元素级乘法融合和特征聚合后注入分割先验为最佳方案，整个过程不需要联合训练或添加额外预测分支。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过注入预计算的分割信息先验来增强单目3D物体检测性能，分割图作为一种注意力机制，能强调图像中与物体相关的区域，抑制不相关背景。整体流程：1)使用Grounded SAM生成分割图；2)RGB图像通过Transformer主干提取视觉特征；3)使用深度层聚合(DLA)多尺度特征；4)将分割图与特征进行空间对齐和标准化；5)通过元素级乘法融合将分割信息与视觉特征结合；6)最后通过卷积层将融合特征投影到64维空间用于3D检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)解耦的分割先验注入策略，无需联合训练；2)简单有效的元素级乘法融合方法；3)确定特征聚合后为最佳融合点；4)使用Grounded SAM生成高质量分割先验。相比之前工作的不同：与多模态方法相比，不需要额外传感器；与深度引导方法相比，提供更丰富的语义信息；与MonoCInIS相比，使用更强大的分割模型和探索了更多融合策略；与DetAny3D相比，更专注于分割先验且更轻量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S-LAM3D通过解耦的分割先验注入和轻量级特征融合方法，显著提升了单目3D物体检测中小物体（如行人和骑行者）的检测性能，同时保持模型轻量化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D Object Detection represents a challenging Computer Vision taskdue to the nature of the input used, which is a single 2D image, lacking in anydepth cues and placing the depth estimation problem as an ill-posed one.Existing solutions leverage the information extracted from the input by usingConvolutional Neural Networks or Transformer architectures as featureextraction backbones, followed by specific detection heads for 3D parametersprediction. In this paper, we introduce a decoupled strategy based on injectingprecomputed segmentation information priors and fusing them directly into thefeature space for guiding the detection, without expanding the detection modelor jointly learning the priors. The focus is on evaluating the impact ofadditional segmentation information on existing detection pipelines withoutadding additional prediction branches. The proposed method is evaluated on theKITTI 3D Object Detection Benchmark, outperforming the equivalent architecturethat relies only on RGB image features for small objects in the scene:pedestrians and cyclists, and proving that understanding the input data canbalance the need for additional sensors or training data.</description>
      <author>example@mail.com (Diana-Alexandra Sas, Florin Oniga)</author>
      <guid isPermaLink="false">2509.05999v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation</title>
      <link>http://arxiv.org/abs/2509.05785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CRAB的新型相机-雷达融合3D目标检测和分割模型，通过利用雷达信息缓解深度歧义问题，在基于反向投影的视图转换中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;鸟瞰图(BEV)中的相机-雷达融合3D目标检测方法因这两种传感器的互补特性和成本效益而受到关注。先前使用前向投影的方法在生成稀疏BEV特征方面存在困难，而采用反向投影的方法则忽略了深度歧义，导致误报。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述局限性，提出一种新的相机-雷达融合3D目标检测和分割模型，使用反向投影并利用雷达来缓解深度歧义问题。&lt;h4&gt;方法&lt;/h4&gt;CRAB在视图转换过程中将透视视图图像上下文特征聚合到BEV查询中，通过结合图像中密集但不可靠的深度分布与雷达占用图中稀疏但精确的深度信息，提高沿同一射线的查询之间的深度区分度。同时引入包含雷达上下文信息的特征图的空间交叉注意力机制，增强对3D场景的理解。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes开放数据集上评估时，所提出的方法在基于反向投影的相机-雷达融合方法中取得了最先进的性能，3D目标检测达到62.4%的NDS和54.0%的mAP。&lt;h4&gt;结论&lt;/h4&gt;CRAB模型通过利用雷达信息缓解深度歧义问题，有效提高了基于反向投影的相机-雷达融合3D目标检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于相机-雷达融合的鸟瞰图(BEV)3D目标检测方法因这些传感器的互补特性和成本效益而受到关注。先前使用前向投影的方法在生成稀疏BEV特征方面存在困难，而采用反向投影的方法则忽略了深度歧义，导致误报。在本文中，为了解决上述局限性，我们提出了一种名为CRAB的新型相机-雷达融合3D目标检测和分割模型，使用利用雷达缓解深度歧义的反向投影。在视图转换过程中，CRAB将透视视图图像上下文特征聚合到BEV查询中。它通过将图像中密集但不可靠的深度分布与雷达占用图中稀疏但精确的深度信息相结合，提高了沿同一射线的查询之间的深度区分度。我们还引入了包含雷达上下文信息的特征图的空间交叉注意力机制，以增强对3D场景的理解。在nuScenes开放数据集上进行评估时，我们提出的方法在基于反向投影的相机-雷达融合方法中取得了最先进的性能，3D目标检测达到62.4%的NDS和54.0%的mAP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于后向投影的视角变换方法中的深度模糊问题。这个问题在自动驾驶和移动机器人领域非常重要，因为它直接影响3D目标检测的准确性。深度模糊导致沿同一射线的查询获得相同特征，无法区分物体深度，产生假阳性检测结果，影响自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：前向投影方法生成稀疏BEV特征，后向投影方法存在深度模糊问题。他们注意到相机提供密集但不可靠的深度信息，而雷达提供稀疏但精确的深度信息，决定结合两者优势。方法设计借鉴了BEVFormer和DFA3D的后向投影框架，CRN中利用雷达占用的思想，以及变形注意力机制。作者设计了两个核心模块：雷达占用引导的空间交叉注意力(ROSCA)和雷达上下文感知的空间交叉注意力(RCSCA)，分阶段解决深度模糊并整合雷达上下文信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合相机和雷达的互补优势，解决后向投影中的深度模糊问题。整体流程包括：1)输入处理：使用图像主干网络提取图像特征并获取深度分布，处理雷达点云并提取雷达特征；2)视角变换：首先通过ROSCA模块结合相机和雷达的占用信息解决深度模糊问题，然后通过RCSCA模块整合雷达上下文信息增强3D场景理解；3)任务头：将融合后的BEV特征输入到特定任务的头部进行3D目标检测或分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出ROSCA模块，结合相机和雷达的占用信息解决深度模糊问题；2)提出RCSCA模块，利用雷达上下文信息增强3D场景理解；3)采用锥形视图而非鸟瞰视图的雷达特征图，确保特征一致性。相比前向投影方法，CRAB生成更密集的BEV特征；相比其他后向投影方法，CRAB专门解决深度模糊问题；相比仅使用图像深度分布的方法，CRAB利用雷达精确深度信息校准相机估计；相比传统融合方法，CRAB在统一BEV空间中融合，适用于多种下游任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CRAB通过创新的相机-雷达融合方法，有效解决了基于后向投影的视角变换中的深度模糊问题，显著提高了3D目标检测和分割的准确性，特别是在恶劣天气条件下展现了优越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, camera-radar fusion-based 3D object detection methods in bird's eyeview (BEV) have gained attention due to the complementary characteristics andcost-effectiveness of these sensors. Previous approaches using forwardprojection struggle with sparse BEV feature generation, while those employingbackward projection overlook depth ambiguity, leading to false positives. Inthis paper, to address the aforementioned limitations, we propose a novelcamera-radar fusion-based 3D object detection and segmentation model named CRAB(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-basedview transformation), using a backward projection that leverages radar tomitigate depth ambiguity. During the view transformation, CRAB aggregatesperspective view image context features into BEV queries. It improves depthdistinction among queries along the same ray by combining the dense butunreliable depth distribution from images with the sparse yet precise depthinformation from radar occupancy. We further introduce spatial cross-attentionwith a feature map containing radar context information to enhance thecomprehension of the 3D scene. When evaluated on the nuScenes open dataset, ourproposed approach achieves a state-of-the-art performance among backwardprojection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in3D object detection.</description>
      <author>example@mail.com (In-Jae Lee, Sihwan Hwang, Youngseok Kim, Wonjune Kim, Sanmin Kim, Dongsuk Kum)</author>
      <guid isPermaLink="false">2509.05785v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>3DPillars: Pillar-based two-stage 3D object detection</title>
      <link>http://arxiv.org/abs/2509.05780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的两阶段3D检测框架，利用伪图像表示缩小了PointPillars与最先进方法之间的性能差距，同时保持了其效率。&lt;h4&gt;背景&lt;/h4&gt;PointPillars是最快的3D目标检测器之一，利用伪图像表示编码场景中3D对象的特征，但通常被最先进的3D检测方法超越。&lt;h4&gt;目的&lt;/h4&gt;引入第一个利用伪图像表示的两阶段3D检测框架，缩小PointPillars与最先进方法之间的性能差距，同时保持其效率。&lt;h4&gt;方法&lt;/h4&gt;提出两个新颖组件：1) 3DPillars架构，通过可分离体素特征模块从伪图像表示中高效学习基于3D体素的特征；2) 带有稀疏场景上下文特征模块的RoI头，聚合多尺度特征以获得稀疏场景特征，有效采用两阶段管道并充分利用场景上下文信息。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和Waymo Open数据集上的实验证明，该方法在速度和准确性之间取得了良好的平衡。&lt;h4&gt;结论&lt;/h4&gt;该研究成功克服了PointPillars的两个主要局限性：无法保留精确的3D结构和难以采用两阶段检测管道，同时保持了PointPillars的效率。&lt;h4&gt;翻译&lt;/h4&gt;PointPillars是最快的3D目标检测器，它利用伪图像表示来编码场景中3D对象的特征。尽管高效，PointPillars通常由于以下限制而被最先进的3D检测方法超越：1) 伪图像表示无法保留精确的3D结构，2) 它们使得难以采用使用3D对象提议的两阶段检测管道，而这种方法通常比单阶段方法表现更好。我们在本文中引入了第一个利用伪图像表示的两阶段3D检测框架，缩小了PointPillars与最先进方法之间的性能差距，同时保留了其效率。我们的框架由两个新颖的组件组成，克服了PointPillars的上述局限性：首先，我们引入了一种新的CNN架构，称为3DPillars，它能够使用2D卷积从伪图像表示中高效学习基于3D体素的特征。3DPillars背后的基本思想是，来自体素的3D特征可以被视为伪图像的堆叠。为了实现这一想法，我们提出了一个可分离的体素特征模块，它不使用3D卷积来提取基于体素的特征。其次，我们引入了一个带有稀疏场景上下文特征模块的RoI头，它聚合来自3DPillars的多尺度特征以获得稀疏场景特征。这使得能够有效采用两阶段管道，并充分利用场景的上下文信息来改进3D对象提议。在KITTI和Waymo Open数据集上的实验结果证明了我们方法的有效性和效率，在速度和准确性方面取得了良好的平衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决PointPillars方法在3D物体检测中的两个局限性：1)伪图像表示无法保留精确的3D结构；2)难以采用通常性能更好的两阶段检测管道。这个问题在自动驾驶和机器人领域非常重要，因为3D物体检测是理解周围环境的关键组件，精确的3D结构信息对于准确识别和定位物体至关重要，而两阶段检测方法通常比单阶段方法有更好的检测性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了PointPillars的局限性，发现它无法保留精确的3D结构且难以采用两阶段检测框架。作者受到基于体素的方法能够保留精细3D结构的启发，但注意到3D卷积计算成本高。因此提出将3D体素特征视为伪图像堆栈的想法，这样可以用2D卷积高效地提取3D特征。作者借鉴了PointPillars的支柱表示方法、Zhou &amp; Tuzel的体素特征编码(VFE)、Deng et al.的体素RoI池化方法以及Miller et al.的键值记忆模块等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D体素特征视为沿X、Y、Z轴的伪图像堆栈，从而可以用2D卷积高效提取3D特征；同时引入稀疏场景上下文特征模块，利用全局场景上下文来改进物体建议。整体流程是：1)输入点云被分配到3D体素网格；2)使用VFE层提取初始特征；3)将3D体素特征分割为伪图像堆栈；4)使用SVFM模块提取多尺度视图特定特征；5)将多尺度特征转换为2D BEV特征图；6)使用RPN生成3D物体提议；7)在RoI头中，S2CFM模块聚合多尺度特征，提取RoI特征，结合全局上下文特征生成上下文感知的RoI表示；8)使用全连接层预测3D边界框和置信度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)3DPillars架构，第一个利用伪图像表示的两阶段3D检测框架；2)可分离体素特征模块(SVFM)，通过2D卷积提取视图特定特征；3)带有稀疏场景上下文特征模块(S2CFM)的RoI头，聚合多尺度特征并利用键值记忆模块获取全局场景上下文。相比之前的工作，它保留了更精细的3D结构并支持两阶段检测，使用2D卷积而非3D卷积提高了效率，不需要对点云下采样，并引入了全局场景上下文信息，特别有利于小物体检测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了3DPillars，一种创新的两阶段3D物体检测框架，通过将3D体素特征表示为伪图像堆栈并利用稀疏场景上下文信息，在保持高效的同时显著提高了检测精度，特别是在处理小物体和远距离物体时表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.eswa.2025.128349&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PointPillars is the fastest 3D object detector that exploits pseudo imagerepresentations to encode features for 3D objects in a scene. Albeit efficient,PointPillars is typically outperformed by state-of-the-art 3D detection methodsdue to the following limitations: 1) The pseudo image representations fail topreserve precise 3D structures, and 2) they make it difficult to adopt atwo-stage detection pipeline using 3D object proposals that typically showsbetter performance than a single-stage approach. We introduce in this paper thefirst two-stage 3D detection framework exploiting pseudo image representations,narrowing the performance gaps between PointPillars and state-of-the-artmethods, while retaining its efficiency. Our framework consists of two novelcomponents that overcome the aforementioned limitations of PointPillars: First,we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3Dvoxel-based features from the pseudo image representation efficiently using 2Dconvolutions. The basic idea behind 3DPillars is that 3D features from voxelscan be viewed as a stack of pseudo images. To implement this idea, we propose aseparable voxel feature module that extracts voxel-based features without using3D convolutions. Second, we introduce an RoI head with a sparse scene contextfeature module that aggregates multi-scale features from 3DPillars to obtain asparse scene feature. This enables adopting a two-stage pipeline effectively,and fully leveraging contextual information of a scene to refine 3D objectproposals. Experimental results on the KITTI and Waymo Open datasetsdemonstrate the effectiveness and efficiency of our approach, achieving a goodcompromise in terms of speed and accuracy.</description>
      <author>example@mail.com (Jongyoun Noh, Junghyup Lee, Hyekang Park, Bumsub Ham)</author>
      <guid isPermaLink="false">2509.05780v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2509.05332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的开源集成模拟框架，用于生成针对自动驾驶车辆感知和通信层的对抗性攻击，通过高保真建模和统一协调系统，有效测试自动驾驶系统在对抗环境下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆依赖复杂的感知和通信系统，使其容易受到对抗性攻击，危及安全。现有模拟框架通常缺乏对多领域对抗性场景的全面支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个开源集成模拟框架，能够生成针对自动驾驶车辆感知和通信层的对抗性攻击，并提供高保真度的物理环境、交通动态和V2X网络建模。&lt;h4&gt;方法&lt;/h4&gt;框架通过统一的核心协调物理环境、交通动态和V2X网络建模，基于单个配置文件同步多个模拟器，支持对LiDAR传感器数据的感知级别攻击和V2X消息操纵、GPS欺骗等通信级别威胁，并集成ROS 2确保与第三方自动驾驶软件栈的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估生成的对抗性场景对最先进3D目标检测器的影响，证明在现实条件下，自动驾驶系统的性能会显著下降，验证了框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;该模拟框架为自动驾驶系统对抗性攻击的研究提供了全面工具，有助于提高自动驾驶系统在面对复杂威胁时的安全性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆(AVs)依赖复杂的感知和通信系统，使其容易受到对抗性攻击，可能危及安全。虽然模拟为鲁棒性测试提供了可扩展且安全的环境，但现有框架通常缺乏对多领域对抗性场景的全面建模支持。本文介绍了一个新的开源集成模拟框架，旨在生成针对自动驾驶车辆感知和通信层的对抗性攻击。该框架提供物理环境、交通动态和V2X网络的高保真建模，通过基于单一配置文件同步多个模拟器的统一核心协调这些组件。我们的实现支持对LiDAR传感器数据的多种感知级别攻击，以及V2X消息操纵和GPS欺骗等通信级别威胁。此外，ROS 2集成确保与第三方自动驾驶软件栈的无缝兼容。我们通过评估生成的对抗性场景对最先进3D目标检测器的影响，证明了框架的有效性，显示在现实条件下性能显著下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles (AVs) rely on complex perception and communicationsystems, making them vulnerable to adversarial attacks that can compromisesafety. While simulation offers a scalable and safe environment for robustnesstesting, existing frameworks typically lack comprehensive supportfor modelingmulti-domain adversarial scenarios. This paper introduces a novel, open-sourceintegrated simulation framework designed to generate adversarial attackstargeting both perception and communication layers of AVs. The frameworkprovides high-fidelity modeling of physical environments, traffic dynamics, andV2X networking, orchestrating these components through a unified core thatsynchronizes multiple simulators based on a single configuration file. Ourimplementation supports diverse perception-level attacks on LiDAR sensor data,along with communication-level threats such as V2X message manipulation and GPSspoofing. Furthermore, ROS 2 integration ensures seamless compatibility withthird-party AV software stacks. We demonstrate the framework's effectiveness byevaluating the impact of generated adversarial scenarios on a state-of-the-art3D object detector, revealing significant performance degradation underrealistic conditions.</description>
      <author>example@mail.com (Christos Anagnostopoulos, Ioulia Kapsali, Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2509.05332v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Harnessing Object Grounding for Time-Sensitive Video Understanding</title>
      <link>http://arxiv.org/abs/2509.06335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GO-Tokenizer的轻量级模块，通过引入基础对象(Grounded Objects)来增强视频大语言模型(Video-LLMs)的时间敏感视频理解(TSV)能力，解决了传统方法中标记长度过长和对象信息噪声敏感的问题。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在时间敏感视频理解任务中面临挑战，现有方法虽然可以通过添加对象注释的文本描述来提高性能，但会引入额外的标记长度和对对象级别信息噪声的敏感性。&lt;h4&gt;目的&lt;/h4&gt;改进视频大语言模型的时间敏感视频理解能力，使其能够更有效地利用帧内基础对象信息，同时避免传统方法的缺点。&lt;h4&gt;方法&lt;/h4&gt;提出GO-Tokenizer，一个轻量级的Video-LLMs附加模块，利用现成的对象检测器即时编码紧凑的对象信息，而不是使用冗长的文本描述。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基础对象可以提升时间敏感视频理解任务性能；2) 使用GO-Tokenizer进行预训练的性能优于普通Video-LLM；3) GO-Tokenizer的性能优于在提示中使用对象文本描述的方法；4) 这种改进在不同模型、数据集和视频理解任务中具有普遍性。&lt;h4&gt;结论&lt;/h4&gt;GO-Tokenizer是一种有效的解决方案，能够增强Video-LLMs的时间敏感视频理解能力，同时避免了传统方法的局限性，在多种场景下表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出通过基础对象(GO)来提高视频大语言模型(Video-LLMs)的时间敏感视频理解(TSV)能力。我们假设TSV任务可以从帧内的基础对象中受益，这一点在我们的初步实验中得到了验证，实验对象是LITA——一种用于推理时间定位的最先进Video-LLM。虽然在提示中加入这些对象注释的文本描述可以提高LITA的性能，但它也引入了额外的标记长度和对对象级别信息噪声的敏感性。为解决这一问题，我们提出了GO-Tokenizer，这是一个轻量级的Video-LLMs附加模块，利用现成的对象检测器即时编码紧凑的对象信息。实验结果表明，使用GO-Tokenizer进行预训练的性能优于普通Video-LLM及其在提示中利用对象文本描述的对应模型。这种增益在不同模型、数据集和视频理解任务(如推理时间定位和密集字幕)中具有普遍性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose to improve the time-sensitive video understanding (TSV) capabilityof video large language models (Video-LLMs) with grounded objects (GO). Wehypothesize that TSV tasks can benefit from GO within frames, which issupported by our preliminary experiments on LITA, a state-of-the-art Video-LLMfor reasoning temporal localization. While augmenting prompts with textualdescription of these object annotations improves the performance of LITA, italso introduces extra token length and susceptibility to the noise in objectlevel information. To address this, we propose GO-Tokenizer, a lightweightadd-on module for Video-LLMs leveraging off-the-shelf object detectors toencode compact object information on the fly. Experimental results demonstratethat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and itscounterpart utilizing textual description of objects in the prompt. The gaingeneralizes across different models, datasets and video understanding taskssuch as reasoning temporal localization and dense captioning.</description>
      <author>example@mail.com (Tz-Ying Wu, Sharath Nittur Sridhar, Subarna Tripathi)</author>
      <guid isPermaLink="false">2509.06335v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</title>
      <link>http://arxiv.org/abs/2509.05604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IJCV, 29 pages, 14 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VideoGraph方法，将视频建模为语言引导的时空图网络，通过整合语义关系和语言查询来提高视频摘要质量，在多个基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有视频摘要方法主要关注帧之间的全局时间关系，忽视了细粒度视觉实体（如对象）与视频内容的关联，且语言引导的视频摘要需要全面理解复杂视频内容。&lt;h4&gt;目的&lt;/h4&gt;为了考虑所有对象之间的语义关系，将视频摘要视为一个语言引导的时空图建模问题，以提高摘要质量。&lt;h4&gt;方法&lt;/h4&gt;提出递归时空图网络VideoGraph，将对象和帧分别作为空间图和时间图的节点，通过图边连接表示语义关系，并引入语言查询增强节点表示，采用递归策略优化初始图并分类关键帧。&lt;h4&gt;主要发现&lt;/h4&gt;VideoGraph在通用和查询导向的视频摘要任务上，无论有监督还是无监督方式，均取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过将视频摘要建模为语言引导的时空图问题，并利用语言查询增强节点表示，能够有效提高视频摘要的性能，生成更具代表性的摘要。&lt;h4&gt;翻译&lt;/h4&gt;视频摘要旨在选择视觉上多样化且能代表给定视频整个故事的关键帧。先前的方法已通过时间建模关注了视频中帧之间的全局互联性。然而，细粒度的视觉实体（如对象）也与视频的主要内容高度相关。此外，最近研究的语言引导视频摘要需要对复杂的现实世界视频进行全面的语言理解。为了考虑所有对象之间的语义关系，本文将视频摘要视为一个语言引导的时空图建模问题。作者提出了递归时空图网络，称为VideoGraph，它将对象和帧分别作为空间图和时间图的节点。每个图中的节点通过图边连接和聚合，表示节点间的语义关系。为了防止边仅基于视觉相似性配置，作者将视频衍生的语言查询整合到图节点表示中，使节点包含语义知识。此外，作者采用递归策略来优化初始图并正确分类每个帧节点为关键帧。在实验中，VideoGraph在多个基准测试中，无论是有监督还是无监督方式，都在通用和查询导向的视频摘要任务上取得了最先进的性能。代码可在https://github.com/park-jungin/videograph获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s11263-025-02577-2&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video summarization aims to select keyframes that are visually diverse andcan represent the whole story of a given video. Previous approaches havefocused on global interlinkability between frames in a video by temporalmodeling. However, fine-grained visual entities, such as objects, are alsohighly related to the main content of the video. Moreover, language-guidedvideo summarization, which has recently been studied, requires a comprehensivelinguistic understanding of complex real-world videos. To consider how all theobjects are semantically related to each other, this paper regards videosummarization as a language-guided spatiotemporal graph modeling problem. Wepresent recursive spatiotemporal graph networks, called VideoGraph, whichformulate the objects and frames as nodes of the spatial and temporal graphs,respectively. The nodes in each graph are connected and aggregated with graphedges, representing the semantic relationships between the nodes. To preventthe edges from being configured with visual similarity, we incorporate languagequeries derived from the video into the graph node representations, enablingthem to contain semantic knowledge. In addition, we adopt a recursive strategyto refine initial graphs and correctly classify each frame node as a keyframe.In our experiments, VideoGraph achieves state-of-the-art performance on severalbenchmarks for generic and query-focused video summarization in both supervisedand unsupervised manners. The code is available athttps://github.com/park-jungin/videograph.</description>
      <author>example@mail.com (Jungin Park, Jiyoung Lee, Kwanghoon Sohn)</author>
      <guid isPermaLink="false">2509.05604v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Keye-VL-1.5模型，通过三种创新方法解决了视频理解中的挑战，在视频理解任务和通用多模态基准测试中展现出显著优势。&lt;h4&gt;背景&lt;/h4&gt;近年来大语言模型已扩展到多模态任务，但视频理解仍具挑战性，因为视频具有动态和信息密集的特点。现有模型在处理视频时难以平衡空间分辨率和时间覆盖范围。&lt;h4&gt;目的&lt;/h4&gt;解决视频理解中的基本挑战，开发一种能够更有效处理视频内容的新型模型。&lt;h4&gt;方法&lt;/h4&gt;1. 引入慢速-快速视频编码策略，根据帧间相似度动态分配计算资源；2. 实现渐进式四阶段预训练方法，将模型上下文长度从8K扩展到128K tokens；3. 开发全面的训练后流程，包括5步思维链数据构建、基于GSPO的迭代强化学习和对齐训练。&lt;h4&gt;主要发现&lt;/h4&gt;Keye-VL-1.5在公共基准测试和内部人工评估中显著优于现有模型，尤其在视频理解任务上表现突出，同时在通用多模态基准测试中保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三种关键创新成功解决了视频理解中的基本挑战，实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展取得了显著进步，其能力已通过多模态大语言模型扩展到多模态任务。然而，由于视频的动态性和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，难以在空间分辨率和时间覆盖范围之间取得平衡。我们提出了Keye-VL-1.5，通过三个关键创新解决了视频理解中的基本挑战。首先，我们引入了一种新颖的慢速-快速视频编码策略，根据帧间相似度动态分配计算资源，以更高分辨率处理具有显著视觉变化的关键帧（慢速路径），同时以更低分辨率但增加时间覆盖范围处理相对静态的帧（快速路径）。其次，我们实现了渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，使模型能够处理更长的视频和更复杂的视觉内容。第三，我们开发了一个全面的训练后流程，专注于推理增强和人类偏好对齐，包含5步思维链数据构建过程、基于GSPO的迭代强化学习以及对困难案例的渐进式提示提示，以及训练对齐。通过在公共基准测试上的广泛评估和严格的内部人工评估，Keye-VL-1.5显示出比现有模型的显著改进，尤其在视频理解任务上表现出色，同时在通用多模态基准测试中保持有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fan, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v3</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2509.04344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ISPA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MICACL的新型多实例学习框架，用于解决动态面部表情识别中的长尾类别分布和时空特征建模复杂性问题。该框架结合了时空依赖建模和长尾对比学习优化，通过特定模块增强特征提取，并采用多尺度类别感知对比学习策略平衡各类别训练。实验证明该方法在野外数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;动态面部表情识别(DFER)面临两个主要挑战：长尾类别分布和时空特征建模的复杂性。现有的基于深度学习的方法虽然提高了DFER性能，但未能有效解决这些问题，导致模型引入偏差。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，解决动态面部表情识别中的长尾类别分布问题和时空特征建模复杂性，减少模型诱导偏差，提高模型的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为MICACL的多实例学习框架，包括：1) 图增强实例交互模块(GEIIM)：通过自适应邻接矩阵和多尺度卷积捕获相邻实例间的复杂时空关系；2) 加权实例聚合网络(WIAN)：根据实例重要性动态分配权重，增强实例级特征聚合；3) 多尺度类别感知对比学习(MCCL)策略：平衡主要和次要类别的训练。&lt;h4&gt;主要发现&lt;/h4&gt;在野外数据集(DFEW和FERV39k)上的大量实验表明，MICACL实现了最先进的性能，具有优越的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MICACL框架有效解决了动态面部表情识别中的关键挑战，通过整合时空依赖建模和长尾对比学习优化，显著提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;动态面部表情识别(DFER)由于长尾类别分布和时空特征建模的复杂性而面临重大挑战。虽然现有的基于深度学习的方法已经提高了DFER性能，但它们通常无法解决这些问题，导致严重的模型诱导偏差。为了克服这些局限性，我们提出了一种名为MICACL的新型多实例学习框架，该框架整合了时空依赖建模和长尾对比学习优化。具体来说，我们设计了图增强实例交互模块(GEIIM)，通过自适应邻接矩阵和多尺度卷积来捕获相邻实例之间的复杂时空关系。为了增强实例级特征聚合，我们开发了加权实例聚合网络(WIAN)，它根据实例重要性动态分配权重。此外，我们引入了多尺度类别感知对比学习(MCCL)策略，以平衡主要和次要类别之间的训练。在野外数据集(即DFEW和FERV39k)上的大量实验表明，MICACL实现了最先进的性能，具有优越的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic facial expression recognition (DFER) faces significant challenges dueto long-tailed category distributions and complexity of spatio-temporal featuremodeling. While existing deep learning-based methods have improved DFERperformance, they often fail to address these issues, resulting in severe modelinduction bias. To overcome these limitations, we propose a novelmulti-instance learning framework called MICACL, which integratesspatio-temporal dependency modeling and long-tailed contrastive learningoptimization. Specifically, we design the Graph-Enhanced Instance InteractionModule (GEIIM) to capture intricate spatio-temporal between adjacent instancesrelationships through adaptive adjacency matrices and multiscale convolutions.To enhance instance-level feature aggregation, we develop the Weighted InstanceAggregation Network (WIAN), which dynamically assigns weights based on instanceimportance. Furthermore, we introduce a Multiscale Category-aware ContrastiveLearning (MCCL) strategy to balance training between major and minorcategories. Extensive experiments on in-the-wild datasets (i.e., DFEW andFERV39k) demonstrate that MICACL achieves state-of-the-art performance withsuperior robustness and generalization.</description>
      <author>example@mail.com (Feng-Qi Cui, Zhen Lin, Xinlong Rao, Anyang Tong, Shiyao Li, Fei Wang, Changlin Chen, Bin Liu)</author>
      <guid isPermaLink="false">2509.04344v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
  <item>
      <title>FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</title>
      <link>http://arxiv.org/abs/2509.05297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 - Project Page: https://flowseek25.github.io/ - Code:  https://github.com/mattpoggi/flowseek&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowSeek是一个新颖的光流计算框架，结合了最新的光流网络设计、单图像深度基础模型和低维运动参数化，实现了在有限硬件资源下的高效训练和高精度光流估计。&lt;h4&gt;背景&lt;/h4&gt;光流计算在计算机视觉领域具有重要意义，但现有的先进方法通常需要大量计算资源进行训练，限制了其在资源受限环境中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个资源高效的光流计算框架，能够在有限的硬件条件下实现高性能的光流估计，并保持良好的跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FlowSeek结合了光流网络设计空间的最新进展、单图像深度基础模型和经典低维运动参数化，实现了紧凑而准确的架构。该方法在单个消费级GPU上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;FlowSeek在硬件预算比最新方法低约8倍的情况下，在Sintel Final和KITTI数据集上实现了比之前最先进的SEA-RAFT方法高10%和15%的相对改进，同时在Spring和LayeredFlow数据集上也表现优异。&lt;h4&gt;结论&lt;/h4&gt;FlowSeek证明了通过结合最新的网络设计、基础模型和经典参数化方法，可以在显著降低硬件需求的同时，实现更优的光流估计性能和跨数据集泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了FlowSeek，这是一个用于光流计算的新颖框架，训练时只需最少的硬件资源。FlowSeek将光流网络设计空间的最新进展与前沿的单图像深度基础模型和经典低维运动参数化相结合，实现了紧凑而准确的架构。FlowSeek在单个消费级GPU上训练，硬件预算比最新方法低约8倍，但在Sintel Final和KITTI上仍实现了比之前最先进的SEA-RAFT高10%和15%的相对改进，同时在Spring和LayeredFlow数据集上也表现优异。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决光流估计领域中的两个关键问题：一是当前最先进的光流模型需要大量高端GPU进行训练，硬件成本过高；二是现有模型在不同数据集上的泛化能力有限。这个问题很重要，因为光流估计是计算机视觉的基础任务，应用于动作识别、视频插值和4D重建等高级任务；高昂的硬件成本限制了资源有限的研究团队参与前沿研究；模型泛化能力不足限制了实际应用场景，因为真实世界场景多样性远超训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察当前光流研究过度依赖硬件资源的现象，借鉴了三个不同领域的知识：最新的光流网络设计空间(特别是SEA-RAFT架构)、先进的单图像深度基础模型(如Depth Anything v2)和经典计算机视觉中的低维运动参数化方法。作者认为可以通过重用已经训练好的基础模型来减少硬件依赖，并将深度模型的光学先验知识整合到光流估计中，同时利用运动基来约束光流估计空间。这种将30年光流研究、最近的深度基础模型和经典方法结合的创新思路，形成了FlowSeek的设计基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; FlowSeek的核心思想是通过整合深度基础模型的几何先验知识、低维运动参数化和最新光流网络架构，实现高效且准确的光流估计。整体流程包括：1)输入一对图像；2)通过特征提取器生成图像特征，同时用深度基础模型估计深度图和提取深度特征；3)融合原始特征与深度特征构建增强特征；4)基于深度图计算运动基并提取其特征；5)通过上下文网络获取上下文特征和初始隐藏状态；6)通过迭代优化过程结合相关体积、隐藏状态和上下文特征逐步 refine 光流估计；7)使用混合拉普拉斯分布的负对数似然作为损失函数，在单个GPU上完成训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合深度基础模型的光流模型；2)可在单个消费级GPU上训练，硬件需求比最新方法低约8倍；3)在多个数据集上实现了零样本泛化，性能比之前最先进的SEA-RAFT高出10-15%；4)三重知识整合(光流网络、深度基础模型、经典运动参数化)；5)有效利用运动基提供的先验知识。相比之前工作，FlowSeek不从头训练大型网络而是重用深度模型知识，将深度信息深度整合到光流流程而非仅作为辅助输入，在保持低计算成本的同时实现更好性能，特别是在细节恢复和跨域泛化方面表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FlowSeek通过整合深度基础模型的几何先验知识和经典运动参数化方法，在显著降低硬件资源需求的同时，实现了更准确的光流估计和更强的跨域泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FlowSeek, a novel framework for optical flow requiring minimalhardware resources for training. FlowSeek marries the latest advances on thedesign space of optical flow networks with cutting-edge single-image depthfoundation models and classical low-dimensional motion parametrization,implementing a compact, yet accurate architecture. FlowSeek is trained on asingle consumer-grade GPU, a hardware budget about 8x lower compared to mostrecent methods, and still achieves superior cross-dataset generalization onSintel Final and KITTI, with a relative improvement of 10 and 15% over theprevious state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlowdatasets.</description>
      <author>example@mail.com (Matteo Poggi, Fabio Tosi)</author>
      <guid isPermaLink="false">2509.05297v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>BEDTime: A Unified Benchmark for Automatically Describing Time Series</title>
      <link>http://arxiv.org/abs/2509.05215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个标准化的评估框架，用于测试模型使用自然语言描述时间序列的能力，通过统一多个数据集和明确定义三个核心任务，实现了对不同模型的直接比较，并发现了当前模型在特定架构和鲁棒性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;最近许多研究提出了用于多种时间序列分析任务的通用基础模型。虽然已有一些评估这些模型的数据集，但先前的研究经常在介绍模型的同时引入新数据集，这限制了直接、独立的比较机会，模糊了不同方法相对优势的见解。此外，先前的评估通常同时涵盖众多任务，评估广泛的模型能力，而没有明确指出哪些能力对整体性能有贡献。&lt;h4&gt;目的&lt;/h4&gt;解决上述评估差距，正式评估3个测试模型使用通用自然语言描述时间系列能力的任务，并统一4个最近的数据集，使模型能够在每个任务上进行直接比较。&lt;h4&gt;方法&lt;/h4&gt;定义并评估3个任务：(1)识别（是/否问题回答），(2)区分（多项选择题回答），(3)生成（开放式自然语言描述）。统一4个最近的数据集，并评估13种最先进的语言、视觉-语言和时间序列-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;(1)流行的纯语言方法表现不佳，表明需要时间序列特定的架构；(2)视觉-语言模型非常成功，突显了视觉模型对这些任务的价值；(3)预训练的多模态时间序列-语言模型成功优于大型语言模型，但仍有显著改进空间；(4)所有方法在一系列鲁棒性测试中都表现出明显的脆弱性。&lt;h4&gt;结论&lt;/h4&gt;该基准为时间序列推理系统提供了标准化评估，有助于更清晰地了解不同模型的相对优势和不足。&lt;h4&gt;翻译&lt;/h4&gt;最近许多研究提出了用于各种时间序列分析任务的通用基础模型。虽然已经有一些既定的数据集用于评估这些模型，但先前的工作经常在介绍其模型的同时引入新数据集，限制了直接、独立比较的机会，并模糊了对不同方法相对优势的见解。此外，先前的评估通常同时涵盖众多任务，评估广泛的模型能力，而没有明确指出哪些能力对整体性能有贡献。为了解决这些差距，我们正式评估了3个任务，测试模型使用通用自然语言描述时间序列的能力：(1)识别（是/否问题回答），(2)区分（多项选择题回答），(3)生成（开放式自然语言描述）。然后我们统一了4个最近的数据集，使模型能够在每个任务上进行直接比较。实验中，在评估13种最先进的语言、视觉-语言和时间序列-语言模型时，我们发现(1)流行的纯语言方法表现不佳，表明需要时间序列特定的架构，(2)视觉-语言模型非常成功，正如预期的那样，突显了视觉模型对这些任务的价值，(3)预训练的多模态时间序列-语言模型成功优于大型语言模型，但仍有显著的改进空间。我们还发现所有方法在一系列鲁棒性测试中都表现出明显的脆弱性。总的来说，我们的基准为时间序列推理系统提供了必要的标准化评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many recent studies have proposed general-purpose foundation models designedfor a variety of time series analysis tasks. While several established datasetsalready exist for evaluating these models, previous works frequently introducetheir models in conjunction with new datasets, limiting opportunities fordirect, independent comparisons and obscuring insights into the relativestrengths of different methods. Additionally, prior evaluations often covernumerous tasks simultaneously, assessing a broad range of model abilitieswithout clearly pinpointing which capabilities contribute to overallperformance. To address these gaps, we formalize and evaluate 3 tasks that testa model's ability to describe time series using generic natural language: (1)recognition (True/False question-answering), (2) differentiation (multiplechoice question-answering), and (3) generation (open-ended natural languagedescription). We then unify 4 recent datasets to enable head-to-head modelcomparisons on each task. Experimentally, in evaluating 13 state-of-the-artlanguage, vision--language, and time series--language models, we find that (1)popular language-only methods largely underperform, indicating a need for timeseries-specific architectures, (2) VLMs are quite successful, as expected,identifying the value of vision models for these tasks and (3) pretrainedmultimodal time series--language models successfully outperform LLMs, but stillhave significant room for improvement. We also find that all approaches exhibitclear fragility in a range of robustness tests. Overall, our benchmark providesa standardized evaluation on a task necessary for time series reasoningsystems.</description>
      <author>example@mail.com (Medhasweta Sen, Zachary Gottesman, Jiaxing Qiu, C. Bayan Bruss, Nam Nguyen, Tom Hartvigsen)</author>
      <guid isPermaLink="false">2509.05215v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations</title>
      <link>http://arxiv.org/abs/2509.05186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种概率框架，揭示上下文算子网络(ICON)实际上是在执行贝叶斯推断，并扩展ICON到生成式设置，实现算子学习中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;ICON是一类基于基础模型新颖架构的算子学习方法，在多样化的初始和边界条件数据集上进行训练，这些数据集与常微分方程和偏微分方程的相应解配对。&lt;h4&gt;目的&lt;/h4&gt;揭示ICON作为隐式执行贝叶斯推断的本质，并扩展ICON到生成式设置，使其能够从解算子的后验预测分布中进行采样。&lt;h4&gt;方法&lt;/h4&gt;提出概率框架将ICON描述为计算给定上下文条件下解算子的后验预测分布均值的方法，利用随机微分方程的形式论为理解ICON和其他多算子学习方法提供基础。&lt;h4&gt;主要发现&lt;/h4&gt;ICON实际上执行贝叶斯推断，计算条件后验预测分布的均值；生成式ICON(GenICON)能够捕捉解算子的潜在不确定性，实现算子学习中的原则性不确定性量化。&lt;h4&gt;结论&lt;/h4&gt;通过概率视角，ICON可扩展到生成式设置，从解算子的后验预测分布中进行采样，GenICON捕捉了解算子的潜在不确定性，使算子学习中的解决方案预测能够进行原则性不确定性量化。&lt;h4&gt;翻译&lt;/h4&gt;上下文算子网络(ICON)是一类基于基础模型新颖架构的算子学习方法。在多样化的初始和边界条件数据集上进行训练，这些数据集与常微分方程和偏微分方程的相应解配对，ICON学习将给定微分方程的示例条件-解对映射到其解算子的近似值。在这里，我们提出了一个概率框架，揭示ICON作为隐式执行贝叶斯推断，其中它计算给定上下文(即示例条件-解对)条件下解算子的后验预测分布的均值。随机微分方程的形式论为描述ICON完成的任务提供了概率框架，同时也为理解其他多算子学习方法提供了基础。这种概率视角为将ICON扩展到生成式设置提供了基础，在这种设置中，可以从解算子的后验预测分布中进行采样。ICON的生成式公式(GenICON)捕捉了解算子的潜在不确定性，这使算子学习中的解决方案预测能够进行原则性不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context operator networks (ICON) are a class of operator learning methodsbased on the novel architectures of foundation models. Trained on a diverse setof datasets of initial and boundary conditions paired with correspondingsolutions to ordinary and partial differential equations (ODEs and PDEs), ICONlearns to map example condition-solution pairs of a given differential equationto an approximation of its solution operator. Here, we present a probabilisticframework that reveals ICON as implicitly performing Bayesian inference, whereit computes the mean of the posterior predictive distribution over solutionoperators conditioned on the provided context, i.e., example condition-solutionpairs. The formalism of random differential equations provides theprobabilistic framework for describing the tasks ICON accomplishes while alsoproviding a basis for understanding other multi-operator learning methods. Thisprobabilistic perspective provides a basis for extending ICON to\emph{generative} settings, where one can sample from the posterior predictivedistribution of solution operators. The generative formulation of ICON(GenICON) captures the underlying uncertainty in the solution operator, whichenables principled uncertainty quantification in the solution predictions inoperator learning.</description>
      <author>example@mail.com (Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2509.05186v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Autoregressive Vision Foundation Models for Image Compression</title>
      <link>http://arxiv.org/abs/2509.05169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究首次尝试将视觉基础模型(VFMs)重新用作图像编解码器，探索其在低速率图像压缩中的生成能力。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)在各种下游任务中广泛应用，包括条件生成和非条件生成场景，例如物理AI应用。许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归(AR)模型进行下一个令牌预测。&lt;h4&gt;目的&lt;/h4&gt;探索视觉基础模型(VFMs)在低速率图像压缩中的生成能力，并评估其与传统编解码器的性能比较。&lt;h4&gt;方法&lt;/h4&gt;重新利用VFM中的AR模型，基于已编码的令牌对下一个令牌进行熵编码，从而实现压缩。这种方法不同于早期仅依赖条件生成来重建输入图像的语义压缩工作。&lt;h4&gt;主要发现&lt;/h4&gt;某些预训练的通用VFMs在极低比特率下表现出比专业学习图像编解码器更好的感知质量。&lt;h4&gt;结论&lt;/h4&gt;这项发现为利用VFMs进行低速率、语义丰富的图像压缩这一有前景的研究方向铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;这项工作首次尝试将视觉基础模型(VFMs)重新用作图像编解码器，旨在探索它们在低速率图像压缩中的生成能力。VFMs在各种下游任务中被广泛用于条件生成和非条件生成场景，例如物理AI应用。许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归(AR)模型来执行下一个令牌预测。为了实现压缩，我们重新利用VFM中的AR模型，基于先前编码的令牌对下一个令牌进行熵编码。这种方法不同于早期依赖条件生成重建输入图像的语义压缩工作。进行了广泛的实验和分析，将基于VFM的编解码器与当前针对失真或感知质量优化的SOTA编解码器进行比较。值得注意的是，某些预训练的通用VFMs在极低比特率下表现出比专业学习图像编解码器更好的感知质量。这一发现为利用VFMs进行低速率、语义丰富的图像压缩这一有前景的研究方向铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents the first attempt to repurpose vision foundation models(VFMs) as image codecs, aiming to explore their generation capability forlow-rate image compression. VFMs are widely employed in both conditional andunconditional generation scenarios across diverse downstream tasks, e.g.,physical AI applications. Many VFMs employ an encoder-decoder architecturesimilar to that of end-to-end learned image codecs and learn an autoregressive(AR) model to perform next-token prediction. To enable compression, werepurpose the AR model in VFM for entropy coding the next token based onpreviously coded tokens. This approach deviates from early semantic compressionefforts that rely solely on conditional generation for reconstructing inputimages. Extensive experiments and analysis are conducted to compare VFM-basedcodec to current SOTA codecs optimized for distortion or perceptual quality.Notably, certain pre-trained, general-purpose VFMs demonstrate superiorperceptual quality at extremely low bitrates compared to specialized learnedimage codecs. This finding paves the way for a promising research directionthat leverages VFMs for low-rate, semantically rich image compression.</description>
      <author>example@mail.com (Huu-Tai Phung, Yu-Hsiang Lin, Yen-Kuan Ho, Wen-Hsiao Peng)</author>
      <guid isPermaLink="false">2509.05169v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights</title>
      <link>http://arxiv.org/abs/2509.05142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了联邦学习与基础模型的交叉领域，提供了一个结构化的文献综述和分类法，涵盖了42种独特方法，特别关注医疗保健领域。&lt;h4&gt;背景&lt;/h4&gt;联邦学习能够通过不共享私有数据的方式进行协作模型训练，从而解锁孤立的数据和分布式资源。随着更复杂的基础模型得到广泛应用，扩展训练资源和整合私有数据的需求也随之增长。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索联邦学习与基础模型的交叉领域，识别、分类和描述整合这两种范式的技术方法。由于目前缺乏统一的综述，作者提出了一个遵循开发生命周期阶段的新型分类法。&lt;h4&gt;方法&lt;/h4&gt;作者检索并审查了4,200多篇文章，通过纳入标准筛选出250多篇经过彻底审查的文章，其中包含42种独特方法。这些方法被用于构建分类法，并基于复杂性、效率和可扩展性进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;论文提供了实施和演变这些方法的实践见解和指导，特别关注医疗保健领域作为案例研究，并展示了联邦学习与基础模型结合的潜在影响。研究涵盖了多个交叉主题，包括联邦学习、自监督学习、微调、蒸馏和迁移学习等。&lt;h4&gt;结论&lt;/h4&gt;作者提供了一个自包含的概述，不仅总结了该领域的现状，还提供了关于采用、演变和整合基础模型与联邦学习的实践见解。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习有潜力在不共享私有数据的情况下，通过协作模型训练来解锁孤立的数据和分布式资源。随着更复杂的基础模型得到广泛使用，扩展训练资源和整合私有数据的需求也随之增长。在本文中，我们探讨了联邦学习与基础模型的交叉领域，旨在识别、分类和描述整合这两种范式的技术方法。由于目前缺乏统一的综述，我们提出了一个遵循开发生命周期阶段的文献综述和新型分类法，并对现有方法进行了技术比较。此外，我们提供了实施和演变这些方法的实践见解和指导，特别关注医疗保健领域作为案例研究，其中联邦学习与基础模型的潜在影响被认为非常重要。我们的综述涵盖了多个交叉主题，包括但不限于联邦学习、自监督学习、微调、蒸馏和迁移学习。最初，我们检索并审查了4,200多篇文章集合。通过纳入标准，这一集合被缩减到250多篇经过彻底审查的文章，其中包含42种独特方法。这些方法被用于构建分类法，并基于复杂性、效率和可扩展性进行了比较。我们呈现这些结果作为一个自包含的概述，不仅总结了该领域的现状，还提供了关于采用、演变和整合基础模型与联邦学习的实践见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7717/peerj-cs.2993&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning has the potential to unlock siloed data and distributedresources by enabling collaborative model training without sharing privatedata. As more complex foundational models gain widespread use, the need toexpand training resources and integrate privately owned data grows as well. Inthis article, we explore the intersection of federated learning andfoundational models, aiming to identify, categorize, and characterize technicalmethods that integrate the two paradigms. As a unified survey is currentlyunavailable, we present a literature survey structured around a novel taxonomythat follows the development life-cycle stages, along with a technicalcomparison of available methods. Additionally, we provide practical insightsand guidelines for implementing and evolving these methods, with a specificfocus on the healthcare domain as a case study, where the potential impact offederated learning and foundational models is considered significant. Oursurvey covers multiple intersecting topics, including but not limited tofederated learning, self-supervised learning, fine-tuning, distillation, andtransfer learning. Initially, we retrieved and reviewed a set of over 4,200articles. This collection was narrowed to more than 250 thoroughly reviewedarticles through inclusion criteria, featuring 42 unique methods. The methodswere used to construct the taxonomy and enabled their comparison based oncomplexity, efficiency, and scalability. We present these results as aself-contained overview that not only summarizes the state of the field butalso provides insights into the practical aspects of adopting, evolving, andintegrating foundational models with federated learning.</description>
      <author>example@mail.com (Cosmin-Andrei Hatfaludi, Alex Serban)</author>
      <guid isPermaLink="false">2509.05142v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>LEMURS dataset: Large-scale multi-detector ElectroMagnetic Universal Representation of Showers</title>
      <link>http://arxiv.org/abs/2509.05108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 24 figures + appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍LEMURS数据集，这是一个模拟量热器簇射的广泛数据集，用于支持高能物理中快速模拟方法的发展和基准测试，为基础模型的发展提供支持。&lt;h4&gt;背景&lt;/h4&gt;高能物理领域需要快速模拟方法，而基础模型的发展需要大规模、多样化的训练数据。现有的CaloChallenge数据集2存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建一个比现有数据集更稳健、规模更大、多样性更高的数据集，以支持快速模拟方法的开发和基准测试，促进基础模型在高能物理中的应用。&lt;h4&gt;方法&lt;/h4&gt;构建模拟量热器簇射数据集，包含多种探测器几何结构，以HDF5格式提供，文件结构受CaloChallenge启发但包含更多变量。&lt;h4&gt;主要发现&lt;/h4&gt;LEMURS数据集比CaloChallenge数据集2更稳健，具有更大的统计量、更广的入射角度范围，以及多种探测器几何结构（包括更现实的量热器）。&lt;h4&gt;结论&lt;/h4&gt;LEMURS数据集的规模和多样性使其特别适合基础模型的发展，已在CaloDiT-2模型中得到应用，该模型已集成到Geant4模拟工具包中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LEMURS：一个广泛的模拟量热器簇射数据集，旨在支持高能物理中快速模拟方法的发展和基准测试，主要为基础模型的发展提供一步。这个新数据集比已建立的CaloChallenge数据集2更稳健，具有更大的统计量、更广的探测器入射角度范围，并且最关键的是包含多种探测器几何结构（包括更现实的量热器）。数据集以HDF5格式提供，文件结构受CaloChallenge簇射表示的启发，同时包含更多变量。LEMURS的规模和多样性使其特别适合基础模型的发展，并已在CaloDiT-2模型中使用，这是一个在社区标准模拟工具包Geant4（版本11.4.beta）中发布的预训练模型。所有数据和生成及分析的代码都是公开可访问的，便于社区的可重复使用和重用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LEMURS: an extensive dataset of simulated calorimeter showersdesigned to support the development and benchmarking of fast simulation methodsin high-energy physics, most notably providing a step towards the developmentof foundation models. This new dataset is more robust than the well-establishedCaloChallenge dataset 2, featuring substantially greater statistics, a widerrange of incident angles in the detector, and most crucially multiple detectorgeometries (including more realistic calorimeters). The dataset is provided inHDF5 format, with a file structure inspired by the CaloChallenge showerrepresentation while also including more variables. LEMURS scale and diversitymake it particularly suitable for development of foundation models and has beenused in the CaloDiT-2 model, a pre-trained model released in the communitystandard simulation toolkit Geant4 (version 11.4.beta). All data and code forgeneration and analysis are openly accessible, facilitating reproducibility andreuse across the community.</description>
      <author>example@mail.com (Peter McKeown, Piyush Raikwar, Anna Zaborowska)</author>
      <guid isPermaLink="false">2509.05108v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</title>
      <link>http://arxiv.org/abs/2509.05066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (Main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ToM-SSI，一个新的多模态心智理论基准测试，用于测试在丰富社交互动和空间动态环境中的心智理论能力，突破了现有文本限制或二元互动的局限。&lt;h4&gt;背景&lt;/h4&gt;现有的大模型心智理论基准测试主要基于Sally-Anne测试的变体，视角非常有限，忽略了人类社交互动的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出新的基准测试ToM-SSI，专门设计用于测试在丰富社交互动和空间动态环境中的心智理论能力。&lt;h4&gt;方法&lt;/h4&gt;ToM-SSI是多模态的，包括多达四个代理的群体互动，这些代理在情境环境中进行交流和移动，允许研究混合合作-阻碍设置和并行推理多个代理的心理状态。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型的表现仍然严重受限，特别是在这些新任务中表现不佳，突显了未来研究的关键差距。&lt;h4&gt;结论&lt;/h4&gt;新的基准测试能够捕捉比现有基准更广泛的社会认知范围，为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有针对基础模型的心智理论(ToM)基准测试依赖于Sally-Anne测试的变体，仅提供了非常有限的心智理论视角，忽略了人类社交互动的复杂性。为解决这一差距，我们提出了ToM-SSI：一个新的基准测试，专门设计用于测试在丰富社交互动和空间动态环境中的心智理论能力。虽然当前的ToM基准测试仅限于文本或二元互动，但ToM-SSI是多模态的，包括多达四个代理的群体互动，这些代理在情境环境中进行交流和移动。这种独特的设计使我们首次能够研究混合合作-阻碍设置和并行推理多个代理的心理状态，从而捕捉比现有基准更广泛的社会认知范围。我们的评估显示，当前模型的表现仍然严重受限，特别是在这些新任务中，突显了未来研究的关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing Theory of Mind (ToM) benchmarks for foundation models rely onvariations of the Sally-Anne test, offering only a very limited perspective onToM and neglecting the complexity of human social interactions. To address thisgap, we propose ToM-SSI: a new benchmark specifically designed to test ToMcapabilities in environments rich with social interactions and spatialdynamics. While current ToM benchmarks are limited to text-only or dyadicinteractions, ToM-SSI is multimodal and includes group interactions of up tofour agents that communicate and move in situated environments. This uniquedesign allows us to study, for the first time, mixed cooperative-obstructivesettings and reasoning about multiple agents' mental state in parallel, thuscapturing a wider range of social cognition than existing benchmarks. Ourevaluations reveal that the current models' performance is still severelylimited, especially in these new tasks, highlighting critical gaps for futureresearch.</description>
      <author>example@mail.com (Matteo Bortoletto, Constantin Ruhdorfer, Andreas Bulling)</author>
      <guid isPermaLink="false">2509.05066v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>LUIVITON: Learned Universal Interoperable VIrtual Try-ON</title>
      <link>http://arxiv.org/abs/2509.05030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LUIVITON系统，一个端到端的完全自动化虚拟试衣系统，能够将复杂的多层服装覆盖在不同姿态的人形角色上，并支持服装尺寸的快速定制。&lt;h4&gt;背景&lt;/h4&gt;虚拟试衣面临的主要挑战是将复杂服装与各种不同姿态和形状的人体正确对齐，传统方法难以处理复杂的几何形状、非流形网格以及多样化的人形角色。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够完全自动化的虚拟试衣系统，能够处理复杂的多层服装，适应各种人形角色（包括人类、机器人、卡通角色、生物和外星人），并支持服装尺寸的快速定制，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;使用SMPL作为代理表示，将服装到人体的覆盖问题分解为服装到SMPL对应和身体到SMPL对应两个任务；对于服装到SMPL对应，使用基于几何学习的方法进行部分到完整形状对应预测；对于身体到SMPL对应，引入基于扩散模型的方法，使用多视角一致的外观特征和预训练的2D基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;系统能够产生高质量的3D服装搭配，无需任何人工劳动；即使在没有2D服装缝制模式的情况下也能工作；系统计算效率高，适合实际应用；支持服装尺寸和材质属性的快速定制。&lt;h4&gt;结论&lt;/h4&gt;LUIVITON系统提供了一个完全自动化的虚拟试衣解决方案，能够处理复杂服装和各种人形角色，同时保持计算效率，并支持服装尺寸的快速定制，为虚拟试衣领域提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LUIVITON，一个端到端的系统，用于完全自动化的虚拟试衣，能够将复杂的多层服装覆盖在不同姿态和多样化的人形角色上。为了解决将复杂服装与任意且高度多样化的人体形状对齐的挑战，我们使用SMPL作为代理表示，并将服装到人体的覆盖问题分解为两个对应任务：1)服装到SMPL和2)身体到SMPL对应，每个任务都有其独特的挑战。虽然我们使用基于几何学习的方法来解决服装到SMPL的贴合问题，以进行部分到完整形状对应的预测，但我们引入了基于扩散模型的方法来处理身体到SMPL的对应，使用多视角一致的外观特征和预训练的2D基础模型。我们的方法能够处理复杂的几何形状、非流形网格，并有效推广到广泛的人形角色--包括人类、机器人、卡通主题、生物和外星人，同时保持计算效率以便实际应用。除了提供完全自动化的贴合解决方案外，LUIVITON还支持服装尺寸的快速定制，允许用户在服装被覆盖后调整服装尺寸和材质属性。我们证明，我们的系统可以在没有任何人工劳动的情况下产生高质量的3D服装搭配，即使2D服装缝制模式不可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决虚拟试衣（virtual try-on）的自动化问题，即如何将任意3D服装模型自动、准确地穿在各种姿势和形状的人形3D角色身上。这个问题在现实和研究中的重要性在于：游戏、电影、社交媒体和AR/VR应用中，角色服装对身份认同和视觉连贯性至关重要；而现有的服装建模和模拟方法劳动密集型，难以自动化适配到不同形状、姿势或风格的角色；当前方法在处理复杂服装、多层结构或风格化角色时存在局限，且缺乏对服装尺寸的灵活控制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将复杂的服装到身体适配问题分解为两个更易管理的对应任务：服装到SMPL的对应和身体到SMPL的对应。针对服装到SMPL对应，作者使用几何学习方法处理部分到完整形状的对应；针对身体到SMPL对应，采用基于扩散模型的方法处理大形状和姿势变化。作者借鉴了多个现有技术：使用SMPL作为人体表示代理；采用DiffusionNet进行服装对应；利用SyncMVD和DINOv2进行特征提取；使用ContourCraft作为神经布料模拟器。整个系统设计为端到端的流程，分为对应预测、注册和服装适配三个主要阶段。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用SMPL作为中间表示，将复杂的服装到身体适配问题分解为两个对应问题，并针对不同问题采用不同方法：几何学习处理服装对应，扩散模型处理身体对应。整体流程分为三阶段：1）对应预测阶段，分别计算服装到SMPL和身体到SMPL的对应关系；2）注册阶段，优化SMPL和SMPL+D参数以对齐服装和身体；3）服装适配阶段，生成平滑过渡序列并使用神经布料模拟器将服装适配到目标身体，支持默认、自动调整和定制三种尺寸调整模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）全自动通用虚拟试-on框架，能处理复杂几何和多层服装；2）基于DiffusionNet的服装到SMPL对应预测模型，能处理非流形网格；3）结合多视角扩散特征和DINOv2先验的身体注册技术，能泛化到各种身体形状；4）快速定制功能，每次调整仅需15秒。相比之前工作，不同之处在于：不再局限于参数化身体和流形服装；使用专门设计的对应预测方法提高精度；能处理人类、机器人、卡通等多种角色；支持快速尺寸调整且无需2D缝纫模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LUIVITON是一个全自动的通用虚拟试-on系统，能够将任意3D服装精确地适配到各种姿势和形状的人形角色上，支持快速定制且无需人工干预。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LUIVITON, an end-to-end system for fully automated virtual try-on,capable of draping complex, multi-layer clothing onto diverse and arbitrarilyposed humanoid characters. To address the challenge of aligning complexgarments with arbitrary and highly diverse body shapes, we use SMPL as a proxyrepresentation and separate the clothing-to-body draping problem into twocorrespondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,where each has its unique challenges. While we address the clothing-to-SMPLfitting problem using a geometric learning-based approach forpartial-to-complete shape correspondence prediction, we introduce a diffusionmodel-based approach for body-to-SMPL correspondence using multi-viewconsistent appearance features and a pre-trained 2D foundation model. Ourmethod can handle complex geometries, non-manifold meshes, and generalizeseffectively to a wide range of humanoid characters -- including humans, robots,cartoon subjects, creatures, and aliens, while maintaining computationalefficiency for practical adoption. In addition to offering a fully automaticfitting solution, LUIVITON supports fast customization of clothing size,allowing users to adjust clothing sizes and material properties after they havebeen draped. We show that our system can produce high-quality 3D clothingfittings without any human labor, even when 2D clothing sewing patterns are notavailable.</description>
      <author>example@mail.com (Cong Cao, Xianhang Cheng, Jingyuan Liu, Yujian Zheng, Zhenhui Lin, Meriem Chkir, Hao Li)</author>
      <guid isPermaLink="false">2509.05030v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</title>
      <link>http://arxiv.org/abs/2509.04957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了多基础模型映射器(MFM-Mapper)，用于高效的视频到音频(V2A)生成，通过融合双视觉编码器特征和使用GPT-2改进特征对齐，仅需16%的训练量即可达到与大规模模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;近期的视频到音频生成依赖于从视频中提取语义和时间特征来调节生成模型，但从头训练这些模型资源消耗大。基础模型因其跨模态知识迁移和泛化能力而受到关注。&lt;h4&gt;目的&lt;/h4&gt;改进现有的视频到音频生成方法，提出一种更高效的映射器架构，能够在降低训练成本的同时保持或提高生成质量。&lt;h4&gt;方法&lt;/h4&gt;MFM-Mapper通过融合双视觉编码器的特征来获取更丰富的语义和时间信息，并用GPT-2替换线性映射器以改进特征对齐，将跨模态特征映射与自回归翻译任务联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;MFM-Mapper在语义和时间一致性方面表现更好，仅需先前映射器方法16%的训练量，就能与在更大规模上训练的模型实现相当的性能。&lt;h4&gt;结论&lt;/h4&gt;MFM-Mapper是一种高效的视频到音频生成方法，显著降低了训练成本，同时保持了高质量的音频生成能力。&lt;h4&gt;翻译&lt;/h4&gt;近期的视频到音频(V2A)生成依赖于从视频中提取语义和时间特征来调节生成模型。从头训练这些模型资源消耗大。因此，由于基础模型(FMs)具有跨模态知识迁移和泛化能力，利用它们已成为趋势。先前的一项工作探索了微调一个轻量级映射器网络，将预训练的视觉编码器与文本到音频生成模型连接起来用于V2A。受此启发，我们引入了多基础模型映射器(MFM-Mapper)。与之前的映射器方法相比，MFM-Mapper通过融合双视觉编码器的特征，受益于更丰富的语义和时间信息。此外，通过用GPT-2替换线性映射器，MFM-Mapper改进了特征对齐，将跨模态特征映射与自回归翻译任务相提并论。我们的MFM-Mapper表现出显著的训练效率。它在语义和时间一致性方面实现了更好的性能，训练消耗更少，仅需先前基于映射器工作的16%的训练规模，却实现了与在更大规模上训练的模型相竞争的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Video-to-Audio (V2A) generation relies on extracting semantic andtemporal features from video to condition generative models. Training thesemodels from scratch is resource intensive. Consequently, leveraging foundationmodels (FMs) has gained traction due to their cross-modal knowledge transferand generalization capabilities. One prior work has explored fine-tuning alightweight mapper network to connect a pre-trained visual encoder with atext-to-audio generation model for V2A. Inspired by this, we introduce theMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapperapproach, MFM-Mapper benefits from richer semantic and temporal information byfusing features from dual visual encoders. Furthermore, by replacing a linearmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallelsbetween cross-modal features mapping and autoregressive translation tasks. OurMFM-Mapper exhibits remarkable training efficiency. It achieves betterperformance in semantic and temporal consistency with fewer training consuming,requiring only 16\% of the training scale compared to previous mapper-basedwork, yet achieves competitive performance with models trained on a much largerscale.</description>
      <author>example@mail.com (Gehui Chen, Guan'an Wang, Xiaowen Huang, Jitao Sang)</author>
      <guid isPermaLink="false">2509.04957v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series</title>
      <link>http://arxiv.org/abs/2509.04921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Patent pending&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过生成人工混沌时间序列和应用重采样技术来模拟金融时间序列数据的方法，并进行大规模预训练。使用比特币交易数据进行零样本预测，结果表明该方法在交易策略盈利能力上显著优于自相关模型。研究还发现了类似缩放定律的现象，表明通过增加训练样本数量可以扩展混沌时间序列的预测范围。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在气象学、交通、电力、经济、金融等多个领域的决策过程中起着关键作用。预测金融工具回报是一个具有挑战性的问题。研究人员已提出适用于各种预测任务的时间序列基础模型，并基于现实世界时间序列的混沌特性开发了人工生成合成混沌时间序列的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种通过生成人工混沌时间序列并对金融时间序列数据进行建模的方法论。&lt;h4&gt;方法&lt;/h4&gt;应用重采样技术模拟金融时间序列数据作为训练样本；增加重采样间隔以扩展预测范围；使用100亿个训练样本进行大规模预训练；基于实际比特币交易数据创建多时间段测试数据集；进行零样本预测；评估基于预测的简单交易策略的盈利能力。&lt;h4&gt;主要发现&lt;/h4&gt;与自相关模型相比，预测结果表现出显著的性能改进；在预训练过程中观察到类似缩放定律的现象；通过指数增加训练样本数量，可以在混沌时间序列中扩展预测范围的同时达到一定水平的预测性能。&lt;h4&gt;结论&lt;/h4&gt;如果这种缩放定律在各种混沌模型中都成立，意味着通过投入大量计算资源有可能预测近期事件。未来研究应侧重于进一步的大规模训练，并验证此缩放定律对不同混沌模型的适用性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在气象学、交通、电力、经济、金融等多个领域的决策过程中起着关键作用。特别是，预测金融工具回报是一个具有挑战性的问题。一些研究人员提出了适用于各种预测任务的时间序列基础模型。同时，基于现实世界时间序列表现出混沌特性的认识，已经开发了人工生成合成混沌时间序列、构建多样化数据集和训练模型的方法。在本研究中，我们提出了一种通过生成人工混沌时间序列并应用重采样技术来模拟金融时间序列数据的方法，然后将其用作训练样本。通过增加重采样间隔来扩展预测范围，我们对每个案例使用了100亿个训练样本进行大规模预训练。随后，我们使用实际的比特币交易数据为多个时间段创建了测试数据集，并在不重新训练预训练模型的情况下进行了零样本预测。基于这些预测评估的简单交易策略的盈利能力结果表明，与自相关模型相比表现出显著的性能改进。在大规模预训练过程中，我们观察到了类似缩放定律的现象，即通过指数增加训练样本数量，我们可以在混沌时间序列中扩展预测范围的同时达到一定水平的预测性能。如果这种缩放定律被证明是稳健的，并且在各种混沌模型中都成立，它表明通过投入大量计算资源有可能预测近期事件。未来研究应侧重于进一步的大规模训练，并验证此缩放定律对不同混沌模型的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting plays a critical role in decision-making processesacross diverse fields including meteorology, traffic, electricity, economics,finance, and so on. Especially, predicting returns on financial instruments isa challenging problem. Some researchers have proposed time series foundationmodels applicable to various forecasting tasks. Simultaneously, based on therecognition that real-world time series exhibit chaotic properties, methodshave been developed to artificially generate synthetic chaotic time series,construct diverse datasets and train models. In this study, we propose amethodology for modeling financial time series by generating artificial chaotictime series and applying resampling techniques to simulate financial timeseries data, which we then use as training samples. Increasing the resamplinginterval to extend predictive horizons, we conducted large-scale pre-trainingusing 10 billion training samples for each case. We subsequently created testdatasets for multiple timeframes using actual Bitcoin trade data and performedzero-shot prediction without re-training the pre-trained model. The results ofevaluating the profitability of a simple trading strategy based on thesepredictions demonstrated significant performance improvements overautocorrelation models. During the large-scale pre-training process, weobserved a scaling law-like phenomenon that we can achieve predictiveperformance at a certain level with extended predictive horizons for chaotictime series by increasing the number of training samples exponentially. If thisscaling law proves robust and holds true across various chaotic models, itsuggests the potential to predict near-future events by investing substantialcomputational resources. Future research should focus on further large-scaletraining and verifying the applicability of this scaling law to diverse chaoticmodels.</description>
      <author>example@mail.com (Yuki Takemoto)</author>
      <guid isPermaLink="false">2509.04921v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms</title>
      <link>http://arxiv.org/abs/2509.04751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多模态基础模型的用户兴趣建模和行为分析框架，通过整合视频、文本和音频数据，结合行为序列分析，显著提高了推荐系统的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着短视频平台用户基数的快速增长，个性化推荐系统对提升用户体验和优化内容分发至关重要。然而，传统兴趣建模方法通常依赖单模态数据（如点击日志或文本标签），无法完全捕捉复杂多模态内容环境中的用户偏好。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法在复杂多模态内容环境中无法完全捕捉用户偏好的问题，提出基于多模态基础模型框架进行用户兴趣建模和行为分析，以提高推荐系统的及时性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出多模态基础模型框架，通过跨模态对齐策略将视频帧、文本描述和背景音乐集成到统一语义空间构建细粒度用户兴趣向量；引入行为驱动的特征嵌入机制，整合观看、点赞和评论序列来建模动态兴趣演化；在实验阶段使用公共和专有短视频数据集进行广泛评估；并使用注意力权重和特征可视化纳入可解释性机制。&lt;h4&gt;主要发现&lt;/h4&gt;行为预测准确性显著提高；对冷启动用户的兴趣建模效果改善；推荐点击率提高；模型决策基础可追溯，兴趣变化可追踪。&lt;h4&gt;结论&lt;/h4&gt;多模态基础模型框架能有效提升推荐系统的及时性和准确性，增强了推荐系统的透明度和可控性。&lt;h4&gt;翻译&lt;/h4&gt;随着短视频平台用户基数的快速增长，个性化推荐系统在提升用户体验和优化内容分发方面发挥着越来越关键的作用。传统的兴趣建模方法通常依赖单模态数据，如点击日志或文本标签，这限制了它们在复杂多模态内容环境中完全捕捉用户偏好的能力。为应对这一挑战，本文提出了一种基于多模态基础模型的用户兴趣建模和行为分析框架。通过跨模态对齐策略将视频帧、文本描述和背景音乐集成到统一语义空间，该框架构建了细粒度的用户兴趣向量。此外，我们引入了一种行为驱动的特征嵌入机制，结合观看、点赞和评论序列来建模动态兴趣演化，从而提高推荐的及时性和准确性。在实验阶段，我们使用公共和专有短视频数据集进行了广泛评估，将我们的方法与多种主流推荐算法和建模技术进行了比较。结果表明，在行为预测准确性、冷启动用户的兴趣建模和推荐点击率方面均有显著改善。此外，我们使用注意力权重和特征可视化纳入了可解释性机制，以揭示模型在多模态输入下的决策基础，并追踪兴趣变化，从而增强推荐系统的透明度和可控性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid expansion of user bases on short video platforms, personalizedrecommendation systems are playing an increasingly critical role in enhancinguser experience and optimizing content distribution. Traditional interestmodeling methods often rely on unimodal data, such as click logs or textlabels, which limits their ability to fully capture user preferences in acomplex multimodal content environment. To address this challenge, this paperproposes a multimodal foundation model-based framework for user interestmodeling and behavior analysis. By integrating video frames, textualdescriptions, and background music into a unified semantic space usingcross-modal alignment strategies, the framework constructs fine-grained userinterest vectors. Additionally, we introduce a behavior-driven featureembedding mechanism that incorporates viewing, liking, and commenting sequencesto model dynamic interest evolution, thereby improving both the timeliness andaccuracy of recommendations. In the experimental phase, we conduct extensiveevaluations using both public and proprietary short video datasets, comparingour approach against multiple mainstream recommendation algorithms and modelingtechniques. Results demonstrate significant improvements in behavior predictionaccuracy, interest modeling for cold-start users, and recommendationclick-through rates. Moreover, we incorporate interpretability mechanisms usingattention weights and feature visualization to reveal the model's decisionbasis under multimodal inputs and trace interest shifts, thereby enhancing thetransparency and controllability of the recommendation system.</description>
      <author>example@mail.com (Yushang Zhao, Yike Peng, Li Zhang, Qianyi Sun, Zhihui Zhang, Yingying Zhuang)</author>
      <guid isPermaLink="false">2509.04751v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
      <link>http://arxiv.org/abs/2509.04735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入不确定性量化方法，提高视觉基础模型在恶劣天气条件下自动驾驶场景的图像分割性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(如SAM和SAM2)在通用图像分割中表现优异，但在恶劣天气等视觉模糊度高的情况下表现不佳，主要原因是缺乏不确定性量化。医学影像领域通过不确定性感知训练提高了可靠性。&lt;h4&gt;目的&lt;/h4&gt;增强自动驾驶场景下图像分割模型的鲁棒性，解决模型在恶劣天气条件下的性能问题。&lt;h4&gt;方法&lt;/h4&gt;1) 引入多步微调程序，将不确定性指标直接纳入SAM2的损失函数；2) 将医学图像分割中的不确定性感知适配器(UAT)适应到驾驶场景中。&lt;h4&gt;主要发现&lt;/h4&gt;在CamVid、BDD100K和GTA数据集上评估显示，UAT-SAM在极端天气条件下优于标准SAM，而带有不确定性感知损失的SAM2在多样化驾驶场景中表现更好。&lt;h4&gt;结论&lt;/h4&gt;明确的不确定性建模对安全关键型自动驾驶在具有挑战性环境中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;近期视觉基础模型的进展，如Segment Anything Model (SAM)及其后续版本SAM2，在通用图像分割基准测试中取得了最先进的性能。然而，在视觉模糊度高的恶劣天气条件下，这些模型表现不佳，主要是因为它们缺乏不确定性量化。受医学影像领域进展的启发，不确定性感知训练提高了模糊情况下的可靠性，我们研究了两种增强自动驾驶分割鲁棒性的方法。首先，我们引入了SAM2的多步微调程序，将不确定性指标直接纳入损失函数，提高整体场景识别能力。其次，我们将专为医学图像分割设计的'不确定性感知适配器'(UAT)适应到驾驶上下文中。我们在CamVid、BDD100K和GTA驾驶数据集上评估了这两种方法。实验表明，UAT-SAM在极端天气条件下优于标准SAM，而具有不确定性感知损失的SAM2在多样化的驾驶场景中实现了更好的性能。这些发现强调了明确的不确定性建模对安全关键型自动驾驶在具有挑战性环境中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision foundation models, such as the Segment AnythingModel (SAM) and its successor SAM2, have achieved state-of-the-art performanceon general image segmentation benchmarks. However, these models struggle inadverse weather conditions where visual ambiguity is high, largely due to theirlack of uncertainty quantification. Inspired by progress in medical imaging,where uncertainty-aware training has improved reliability in ambiguous cases,we investigate two approaches to enhance segmentation robustness for autonomousdriving. First, we introduce a multi-step finetuning procedure for SAM2 thatincorporates uncertainty metrics directly into the loss function, improvingoverall scene recognition. Second, we adapt the Uncertainty-Aware Adapter(UAT), originally designed for medical image segmentation, to driving contexts.We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.Experiments show that UAT-SAM outperforms standard SAM in extreme weather,while SAM2 with uncertainty-aware loss achieves improved performance acrossdiverse driving scenes. These findings underscore the value of explicituncertainty modeling for safety-critical autonomous driving in challengingenvironments.</description>
      <author>example@mail.com (Dharsan Ravindran, Kevin Wang, Zhuoyuan Cao, Saleh Abdelrahman, Jeffery Wu)</author>
      <guid isPermaLink="false">2509.04735v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Sample-efficient Integration of New Modalities into Large Language Models</title>
      <link>http://arxiv.org/abs/2509.04606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SEMI的样本高效的模态集成方法，可以有效地将新模态集成到大型语言模型中，显著减少了所需的数据量。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型可处理多种模态，但由于模态空间大且不断演变，从头训练包含所有模态的模型不可行。此外，将模态集成到现有基础模型需要大量成对数据，而低资源模态通常缺乏这些数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种样本高效的模态集成方法，使大型语言模型能够适应新的模态，而无需大量成对训练数据。&lt;h4&gt;方法&lt;/h4&gt;设计一个超网络，该网络可以将共享投影仪(位于模态特定编码器和LLM之间)适应任何模态。超网络在高资源模态上训练，并在推理时基于少量样本生成适配器。通过等距变换增加编码器数量以提高训练模态多样性。&lt;h4&gt;主要发现&lt;/h4&gt;SEMI在集成新模态(卫星图像、天文图像、惯性测量和分子)时显著提高了样本效率，适用于任意嵌入维度的编码器。例如，要达到与32次样本SEMI相同的准确度，从头训练投影仪需要多64倍的数据。&lt;h4&gt;结论&lt;/h4&gt;SEMI方法有望扩展基础模型的模态覆盖范围，使模型能够更高效地适应新的模态。&lt;h4&gt;翻译&lt;/h4&gt;多模态基础模型可以处理多种模态。然而，由于可能的模态空间很大且随时间演变，从头开始训练一个包含所有模态的模型是不可行的。此外，将模态集成到现有基础模型中目前需要大量成对数据，而这些数据对于低资源模态通常不可用。在本文中，我们介绍了一种将模态高效集成到大型语言模型中的方法。为此，我们设计了一个超网络，可以将位于模态特定编码器和大型语言模型之间的共享投影仪适应任何模态。该超网络在高资源模态上训练，并在推理时基于任意模态的少量样本生成合适的适配器。为了增加训练模态的多样性，我们通过等距变换人为增加了编码器的数量。我们发现，SEMI在集成新模态时，在少量样本情况下显著提高了样本效率，适用于任意嵌入维度的编码器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal foundation models can process several modalities. However, sincethe space of possible modalities is large and evolving over time, training amodel from scratch to encompass all modalities is unfeasible. Moreover,integrating a modality into a pre-existing foundation model currently requiresa significant amount of paired data, which is often not available forlow-resource modalities. In this paper, we introduce a method forsample-efficient modality integration (SEMI) into Large Language Models (LLMs).To this end, we devise a hypernetwork that can adapt a shared projector --placed between modality-specific encoders and an LLM -- to any modality. Thehypernetwork, trained on high-resource modalities (i.e., text, speech, audio,video), is conditioned on a few samples from any arbitrary modality atinference time to generate a suitable adapter. To increase the diversity oftraining modalities, we artificially multiply the number of encoders throughisometric transformations. We find that SEMI achieves a significant boost insample efficiency during few-shot integration of new modalities (i.e.,satellite images, astronomical images, inertial measurements, and molecules)with encoders of arbitrary embedding dimensionality. For instance, to reach thesame accuracy as 32-shot SEMI, training the projector from scratch needs64$\times$ more data. As a result, SEMI holds promise to extend the modalitycoverage of foundation models.</description>
      <author>example@mail.com (Osman Batur İnce, André F. T. Martins, Oisin Mac Aodha, Edoardo M. Ponti)</author>
      <guid isPermaLink="false">2509.04606v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>CEHR-XGPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
      <link>http://arxiv.org/abs/2509.03643v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CEHR-XGPT是一种通用基础模型，专为电子健康记录数据设计，整合了特征表示、零样本预测和合成数据生成三种能力，通过时间令牌学习框架支持临床序列的时间推理，在各项任务中表现优异，能有效泛化到外部数据集。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录提供患者健康的丰富纵向视图，具有推进临床决策支持、风险预测和数据驱动医疗研究的潜力，但大多数现有人工智能模型为单一目的设计，限制了其在现实世界中的泛化能力和实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用基础模型，将EHR数据分析的三种基本能力（特征表示、零样本预测和合成数据生成）统一在单一架构中。&lt;h4&gt;方法&lt;/h4&gt;提出CEHR-XGPT模型，采用新颖的时间令牌学习框架，将患者的动态时间线明确编码到模型结构中，以支持对临床序列的时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;CEHR-XGPT在所有三项任务中均表现出强大性能，通过词汇扩展和微调能有效泛化到外部数据集。&lt;h4&gt;结论&lt;/h4&gt;CEHR-XGPT的多功能性使研究人员无需任务特定的重新训练即可实现快速模型开发、队列发现和患者结果预测。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录（EHRs）提供了患者健康的丰富纵向视图，在推进临床决策支持、风险预测和数据驱动的医疗研究方面具有巨大潜力。然而，大多数用于EHR的人工智能模型都是为狭窄的单一目的任务设计的，限制了它们在现实世界环境中的泛化能力和实用性。在这里，我们提出了CEHR-XGPT，这是一种用于EHR数据的通用基础模型，在单一架构中统一了三种基本能力——特征表示、零样本预测和合成数据生成。为了支持对临床序列的时间推理，CEHR-XGPT采用了一种新颖的时间令牌学习框架，明确将患者的动态时间线编码到模型结构中。CEHR-XGPT在所有三项任务中都表现出强大的性能，并通过词汇扩展和微调有效地泛化到外部数据集。它的多功能性使得无需任务特定的重新训练即可实现快速模型开发、队列发现和患者结果预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) provide a rich, longitudinal view of patienthealth and hold significant potential for advancing clinical decision support,risk prediction, and data-driven healthcare research. However, most artificialintelligence (AI) models for EHRs are designed for narrow, single-purposetasks, limiting their generalizability and utility in real-world settings.Here, we present CEHR-XGPT, a general-purpose foundation model for EHR datathat unifies three essential capabilities - feature representation, zero-shotprediction, and synthetic data generation - within a single architecture. Tosupport temporal reasoning over clinical sequences, CEHR-XGPT incorporates anovel time-token-based learning framework that explicitly encodes patients'dynamic timelines into the model structure. CEHR-XGPT demonstrates strongperformance across all three tasks and generalizes effectively to externaldatasets through vocabulary expansion and fine-tuning. Its versatility enablesrapid model development, cohort discovery, and patient outcome forecastingwithout the need for task-specific retraining.</description>
      <author>example@mail.com (Chao Pang, Jiheum Park, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Shalmali Joshi, Noémie Elhadad, Karthik Natarajan)</author>
      <guid isPermaLink="false">2509.03643v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning to accelerate distributed ADMM using graph neural networks</title>
      <link>http://arxiv.org/abs/2509.05288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review, the first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种将分布式ADMM算法与图神经网络相结合的方法，通过学习自适应超参数来提高ADMM的收敛速度和解决方案质量，同时保持算法的收敛特性。&lt;h4&gt;背景&lt;/h4&gt;分布式优化是大规模机器学习和控制应用的基础。ADMM方法因其强收敛保证和适合分布式计算而受到欢迎，但常面临收敛速度慢和对超参数选择敏感的问题。&lt;h4&gt;目的&lt;/h4&gt;解决ADMM收敛速度慢和对超参数选择敏感的问题，提高分布式优化算法的性能。&lt;h4&gt;方法&lt;/h4&gt;将分布式ADMM迭代表示在图神经网络的消息传递框架中，通过图神经网络学习自适应步长和通信权重，基于迭代预测超参数。通过展开固定次数的ADMM迭代，端到端训练网络参数，以最小化特定问题类的最终迭代误差。&lt;h4&gt;主要发现&lt;/h4&gt;学习到的ADMM变体在收敛速度和解决方案质量方面一致优于标准ADMM，数值实验验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;将图神经网络与ADMM结合可以有效改善ADMM的性能，在保持算法收敛特性的同时，显著提高收敛速度和解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;分布式优化是大规模机器学习和控制应用的基础。在现有方法中，交替方向乘子法（ADMM）因其强收敛保证和适合分布式计算而广受欢迎。然而，ADMM常面临收敛速度慢和对超参数选择敏感的问题。在这项工作中，我们展示了分布式ADMM迭代可以在图神经网络（GNNs）的消息传递框架中自然表示。基于这一联系，我们提出通过图神经网络学习自适应步长和通信权重，该网络基于迭代预测超参数。通过展开固定次数的ADMM迭代，我们端到端训练网络参数，以最小化特定问题类的最终迭代误差，同时保持算法的收敛特性。数值实验表明，我们学习到的变体在收敛速度和解决方案质量方面一致优于标准ADMM。代码可在https://github.com/paulhausner/learning-distributed-admm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed optimization is fundamental in large-scale machine learning andcontrol applications. Among existing methods, the Alternating Direction Methodof Multipliers (ADMM) has gained popularity due to its strong convergenceguarantees and suitability for decentralized computation. However, ADMM oftensuffers from slow convergence and sensitivity to hyperparameter choices. Inthis work, we show that distributed ADMM iterations can be naturallyrepresented within the message-passing framework of graph neural networks(GNNs). Building on this connection, we propose to learn adaptive step sizesand communication weights by a graph neural network that predicts thehyperparameters based on the iterates. By unrolling ADMM for a fixed number ofiterations, we train the network parameters end-to-end to minimize the finaliterates error for a given problem class, while preserving the algorithm'sconvergence properties. Numerical experiments demonstrate that our learnedvariant consistently improves convergence speed and solution quality comparedto standard ADMM. The code is available athttps://github.com/paulhausner/learning-distributed-admm.</description>
      <author>example@mail.com (Henri Doerks, Paul Häusner, Daniel Hernández Escobar, Jens Sjölund)</author>
      <guid isPermaLink="false">2509.05288v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.05207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2505.10806&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RapidGNN是一种创新的分布式图神经网络训练框架，通过确定性采样调度策略实现高效缓存构建和远程特征预取，显著提升了训练性能和能源效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在探索实体间结构关系的各种任务中变得流行。然而，由于数据集的高度连接结构，在大规模图上进行GNN的分布式训练面临重大挑战。传统的基于采样的方法虽然可以减轻计算负载，但通信开销仍然是一个主要问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种分布式GNN训练框架，解决大规模图上GNN训练的通信开销问题，提高训练效率和能源效率。&lt;h4&gt;方法&lt;/h4&gt;提出RapidGNN，一种具有确定性采样调度策略的分布式GNN训练框架，能够实现高效的缓存构建和远程特征预取。&lt;h4&gt;主要发现&lt;/h4&gt;在基准图数据集上，RapidGNN将端到端训练吞吐量平均提高了2.46倍到3.00倍，同时将远程特征获取减少了9.70倍到15.39倍。RapidGNN展示了接近线性的可扩展性，随着计算单元数量的增加而高效扩展。对于CPU和GPU，RapidGNN分别比基线方法提高了44%和32%的能源效率。&lt;h4&gt;结论&lt;/h4&gt;RapidGNN是一种有效的分布式GNN训练解决方案，能够显著提高训练吞吐量，减少远程特征获取，并提高能源效率，适用于不同规模和拓扑结构的图数据集。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为探索实体间结构关系的各种任务中的热门选择。然而，由于数据集的高度连接结构，在大规模图上进行GNN的分布式训练带来了重大挑战。传统的基于采样的方法减轻了计算负载，但通信开销仍然是一个挑战。本文提出了RapidGNN，一种具有确定性采样调度的分布式GNN训练框架，能够实现高效的缓存构建和远程特征预取。在基准图数据集上的评估表明，RapidGNN在不同规模和拓扑结构上都是有效的。RapidGNN在基准数据集上将端到端训练吞吐量平均提高了2.46倍到3.00倍，同时将远程特征获取减少了9.70倍到15.39倍。RapidGNN进一步展示了接近线性的可扩展性，随着计算单元数量的增加而高效扩展。此外，对于CPU和GPU，RapidGNN分别比基线方法提高了44%和32%的能源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become popular across a diverse set oftasks in exploring structural relationships between entities. However, due tothe highly connected structure of the datasets, distributed training of GNNs onlarge-scale graphs poses significant challenges. Traditional sampling-basedapproaches mitigate the computational loads, yet the communication overheadremains a challenge. This paper presents RapidGNN, a distributed GNN trainingframework with deterministic sampling-based scheduling to enable efficientcache construction and prefetching of remote features. Evaluation on benchmarkgraph datasets demonstrates RapidGNN's effectiveness across different scalesand topologies. RapidGNN improves end-to-end training throughput by 2.46x to3.00x on average over baseline methods across the benchmark datasets, whilecutting remote feature fetches by over 9.70x to 15.39x. RapidGNN furtherdemonstrates near-linear scalability with an increasing number of computingunits efficiently. Furthermore, it achieves increased energy efficiency overthe baseline methods for both CPU and GPU by 44% and 32%, respectively.</description>
      <author>example@mail.com (Arefin Niam, Tevfik Kosar, M S Q Zulkar Nine)</author>
      <guid isPermaLink="false">2509.05207v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Matrix Factorization Based Graph Contrastive Learning for Recommendation System</title>
      <link>http://arxiv.org/abs/2509.05115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HMFGCL的新方法，结合了低秩矩阵分解和奇异值分解两种技术，通过图对比学习解决推荐系统中的数据稀疏性问题，并在多个数据集上表现出色，尤其是在小规模数据集上。&lt;h4&gt;背景&lt;/h4&gt;近年来，结合对比学习和图神经网络的方法已出现用于解决推荐系统的挑战，在推荐领域展现出强大性能并发挥重要作用。&lt;h4&gt;目的&lt;/h4&gt;解决现有图对比学习方法中数据增强策略的局限性，更好地捕获用户-项目交互信息。&lt;h4&gt;方法&lt;/h4&gt;提出HMFGCL（基于混合矩阵分解的图对比学习）方法，整合低秩矩阵分解（MF）和奇异值分解（SVD）两种技术，互补获取全局协作信息，构建增强视图。&lt;h4&gt;主要发现&lt;/h4&gt;现有图对比学习方法主要基于扰动图结构和应用聚类两种数据增强策略，但这些策略获得的交互信息不能完全捕捉用户-项目交互；对比学习通过数据增强策略有效缓解了数据稀疏问题。&lt;h4&gt;结论&lt;/h4&gt;在多个公共数据集上的实验结果表明，HMFGCL模型优于现有基线方法，特别在小规模数据集上表现突出。&lt;h4&gt;翻译&lt;/h4&gt;近年来，结合对比学习与图神经网络的方法已出现，用于解决推荐系统的挑战，展现出强大的性能并在该领域发挥重要作用。对比学习主要通过采用数据增强策略解决数据稀疏问题，有效缓解了这一问题并显示出良好的结果。尽管现有研究已取得良好成果，但当前大多数图对比学习方法基于两种数据增强策略：第一种是扰动图结构，如随机添加或删除边；第二种是应用聚类技术。我们认为通过这两种策略获得的交互信息不能完全捕捉用户-项目交互。在本文中，我们提出了一种名为HMFGCL（基于混合矩阵分解的图对比学习）的新方法，该方法整合了两种不同的矩阵分解技术——低秩矩阵分解（MF）和奇异值分解（SVD）——互补地获取全局协作信息，从而构建增强视图。在多个公共数据集上的实验结果表明，我们的模型优于现有基线方法，特别是在小规模数据集上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, methods that combine contrastive learning with graph neuralnetworks have emerged to address the challenges of recommendation systems,demonstrating powerful performance and playing a significant role in thisdomain. Contrastive learning primarily tackles the issue of data sparsity byemploying data augmentation strategies, effectively alleviating this problemand showing promising results. Although existing research has achievedfavorable outcomes, most current graph contrastive learning methods are basedon two types of data augmentation strategies: the first involves perturbing thegraph structure, such as by randomly adding or removing edges; and the secondapplies clustering techniques. We believe that the interactive informationobtained through these two strategies does not fully capture the user-iteminteractions. In this paper, we propose a novel method called HMFGCL (HybridMatrix Factorization Based Graph Contrastive Learning), which integrates twodistinct matrix factorization techniques-low-rank matrix factorization (MF) andsingular value decomposition (SVD)-to complementarily acquire globalcollaborative information, thereby constructing enhanced views. Experimentalresults on multiple public datasets demonstrate that our model outperformsexisting baselines, particularly on small-scale datasets.</description>
      <author>example@mail.com (Hao Chen, Wenming Ma, Zihao Chu, Mingqi Li)</author>
      <guid isPermaLink="false">2509.05115v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.04785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图神经网络节点遗忘方法，旨在有效移除敏感训练节点信息并保护隐私。作者提出了三种创新方法，其中两种特别利用了图拓扑特征，在基准测试中表现出优越的性能和效率。&lt;h4&gt;背景&lt;/h4&gt;随着对隐私攻击和敏感信息泄露的担忧增加，研究人员积极探索从图神经网络模型中高效移除敏感训练数据的方法。节点遗忘作为一种有前景的技术，可以通过从GNN模型中高效移除特定训练节点信息来保护敏感节点的隐私。然而，现有的节点遗忘方法要么对GNN结构施加限制，要么没有有效利用图拓扑进行节点遗忘，甚至会损害图的拓扑结构，难以实现满意的性能-复杂度权衡。&lt;h4&gt;目的&lt;/h4&gt;解决现有节点遗忘方法的局限性，实现GNN中训练节点移除的高效遗忘，提出三种新颖的节点遗忘方法，并验证它们在保护GNN模型隐私方面的优越性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了三种新颖的节点遗忘方法：1) 基于类的标签替换(Class-based Label Replacement)；2) 基于拓扑引导的邻居平均后验概率(Topology-guided Neighbor Mean Posterior Probability)；3) 类一致的邻居节点过滤(Class-consistent Neighbor Node Filtering)。其中，后两种方法有效利用了图的拓扑特征，实现了更有效的节点遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上进行的实验结果表明，所提出的方法在模型效用、遗忘效用和遗忘效率方面均表现出优越性，证明了它们在节点遗忘方面的实用性和效率，以及与最先进的节点遗忘方法相比的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够高效移除敏感训练节点，保护GNN中敏感节点的隐私信息。这些发现有助于提高GNN模型的隐私和安全性，并为节点遗忘领域提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;随着对隐私攻击和潜在敏感信息泄露的担忧日益增加，研究人员积极探索高效移除敏感训练数据并降低图神经网络(GNN)模型隐私风险的方法。节点遗忘已成为一种有前景的技术，通过从GNN模型中高效移除特定训练节点信息来保护敏感节点的隐私。然而，现有的节点遗忘方法要么对GNN结构施加限制，要么没有有效利用图拓扑进行节点遗忘。一些方法甚至损害了图的拓扑结构，使得难以实现令人满意的性能-复杂度权衡。为了解决这些问题并实现GNN中训练节点移除的高效遗忘，我们提出了三种新颖的节点遗忘方法：基于类的标签替换、基于拓扑引导的邻居平均后验概率和类一致的邻居节点过滤。在这些方法中，基于拓扑引导的邻居平均后验概率和类一致的邻居节点过滤有效利用了图的拓扑特征，实现了更有效的节点遗忘。为了验证我们提出的节点遗忘方法的优越性，我们在三个基准数据集上进行了实验。评估标准包括模型效用、遗忘效用和遗忘效率。实验结果证明了所提出方法的实用性和效率，并展示了它们与最先进的节点遗忘方法相比的优势。总体而言，所提出的方法能够高效移除敏感训练节点，保护GNN中敏感节点的隐私信息。这些发现有助于提高GNN模型的隐私和安全性，并为节点遗忘领域提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With increasing concerns about privacy attacks and potential sensitiveinformation leakage, researchers have actively explored methods to efficientlyremove sensitive training data and reduce privacy risks in graph neural network(GNN) models. Node unlearning has emerged as a promising technique forprotecting the privacy of sensitive nodes by efficiently removing specifictraining node information from GNN models. However, existing node unlearningmethods either impose restrictions on the GNN structure or do not effectivelyutilize the graph topology for node unlearning. Some methods even compromisethe graph's topology, making it challenging to achieve a satisfactoryperformance-complexity trade-off. To address these issues and achieve efficientunlearning for training node removal in GNNs, we propose three novel nodeunlearning methods: Class-based Label Replacement, Topology-guided NeighborMean Posterior Probability, and Class-consistent Neighbor Node Filtering. Amongthese methods, Topology-guided Neighbor Mean Posterior Probability andClass-consistent Neighbor Node Filtering effectively leverage the topologicalfeatures of the graph, resulting in more effective node unlearning. To validatethe superiority of our proposed methods in node unlearning, we conductedexperiments on three benchmark datasets. The evaluation criteria included modelutility, unlearning utility, and unlearning efficiency. The experimentalresults demonstrate the utility and efficiency of the proposed methods andillustrate their superiority compared to state-of-the-art node unlearningmethods. Overall, the proposed methods efficiently remove sensitive trainingnodes and protect the privacy information of sensitive nodes in GNNs. Thefindings contribute to enhancing the privacy and security of GNN models andprovide valuable insights into the field of node unlearning.</description>
      <author>example@mail.com (Faqian Guan, Tianqing Zhu, Zhoutian Wang, Wei Ren, Wanlei Zhou)</author>
      <guid isPermaLink="false">2509.04785v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Inferring the Graph Structure of Images for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.04677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过寻找传统网格图和超像素方法的替代图表示来提高图神经网络在图像分类任务中的准确性，使用行相关图、列相关图和乘积图表示MNIST和Fashion-MNIST数据集中的图像，实验证明这些方法可以提高下游GNN模型的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;图像数据集如MNIST是测试图神经网络架构的关键基准。传统上，图像被表示为网格图，其中每个节点代表一个像素，边连接相邻像素(垂直和水平)，图信号是图像中每个像素的值(强度)。这些图通常用作图神经网络(如GraphCNNs、GAT、GatedGCN)的输入来对图像进行分类。&lt;h4&gt;目的&lt;/h4&gt;提高下游图神经网络任务的准确性，寻找替代传统网格图和超像素方法的图表示方法来表示数据集图像。&lt;h4&gt;方法&lt;/h4&gt;基于像素值之间的相关性，为MNIST和Fashion-MNIST中的每幅图像构建行相关图、列相关图和乘积图，延续了[5,6]中的方法，并将这些不同的图表示和特征作为输入提供给下游GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，使用这些不同的图表示和特征作为下游GNN模型的输入，可以提高准确性，与使用传统网格图和超像素方法相比，这些替代图表示方法表现更好。&lt;h4&gt;结论&lt;/h4&gt;使用基于相关性的图表示可以改进图神经网络在图像分类任务中的性能，为图像数据集的图表示提供了新的视角，超越了传统的网格图和超像素方法。&lt;h4&gt;翻译&lt;/h4&gt;图像数据集如MNIST是测试图神经网络架构的关键基准。图像传统上被表示为网格图，其中每个节点代表一个像素，边连接相邻像素(垂直和水平)。图信号是图像中每个像素的值(强度)。这些图通常用作图神经网络(例如图卷积神经网络(GraphCNNs)[1,2]、图注意力网络(GAT)[3]、GatedGCN[4])的输入来对图像进行分类。在本工作中，我们通过寻找网格图和超像素方法的替代图来表示数据集图像，改进了下游图神经网络任务的准确性，遵循[5,6]中的方法。我们使用像素值之间的相关性，基于[5,6]中的方法，为MNIST和Fashion-MNIST中的每幅图像找到行相关图、列相关图和乘积图。实验表明，将这些不同的图表示和特征作为下游GNN模型的输入，比使用文献中的传统网格图和超像素方法提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image datasets such as MNIST are a key benchmark for testing Graph NeuralNetwork (GNN) architectures. The images are traditionally represented as a gridgraph with each node representing a pixel and edges connecting neighboringpixels (vertically and horizontally). The graph signal is the values(intensities) of each pixel in the image. The graphs are commonly used as inputto graph neural networks (e.g., Graph Convolutional Neural Networks (GraphCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify theimages. In this work, we improve the accuracy of downstream graph neuralnetwork tasks by finding alternative graphs to the grid graph and superpixelmethods to represent the dataset images, following the approach in [5, 6]. Wefind row correlation, column correlation, and product graphs for each image inMNIST and Fashion-MNIST using correlations between the pixel values building onthe method in [5, 6]. Experiments show that using these different graphrepresentations and features as input into downstream GNN models improves theaccuracy over using the traditional grid graph and superpixel methods in theliterature.</description>
      <author>example@mail.com (Mayur S Gowda, John Shi, Augusto Santos, José M. F. Moura)</author>
      <guid isPermaLink="false">2509.04677v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability</title>
      <link>http://arxiv.org/abs/2509.03547v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatterVial是一个创新的混合框架，用于材料科学中的基于特征的机器学习，通过整合多种预训练图神经网络的潜在表示和新型特征，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;材料科学领域需要结合传统特征模型的透明性和深度学习模型的预测能力，以提高材料预测的准确性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个高性能、透明的机器学习框架，能够在保持化学透明度的同时，提供与最先进端到端图神经网络相媲美的预测能力。&lt;h4&gt;方法&lt;/h4&gt;整合多种预训练图神经网络（包括基于结构的MEGNet、基于组成的ROOST和等变的ORB图网络）的潜在表示，结合计算高效的GNN近似描述符和符号回归产生的新特征，并使用代理模型和符号回归进行可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench任务上，该方法显著降低了基于特征模型MODNet的误差，将其性能提升至与最先进的端到端GNN相当甚至在某些情况下超越它们，多个任务的准确率提高了40%以上。&lt;h4&gt;结论&lt;/h4&gt;MatterVial统一框架通过提供高性能、透明的工具，符合可解释AI原则，推动了材料信息学的发展，为更有针对性和自主性的材料发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了MatterVial，一个用于材料科学中基于特征机器学习的创新混合框架。MatterVial通过整合来自多样化预训练图神经网络模型的潜在表示来扩展特征空间，包括：基于结构的（MEGNet）、基于组成的（ROOST）和等变的（ORB）图网络，以及计算高效的GNN近似描述符和符号回归的新特征。我们的方法结合了传统基于特征模型的化学透明性和深度学习架构的预测能力。在Matbench任务上增强基于特征的模型MODNet时，这种方法带来了显著的误差降低，并将其性能提升至与最先进的端到端GNN相媲美，在多个情况下甚至超越它们，多个任务的准确率提高了40%以上。集成的可解释性模块使用代理模型和符号回归，将潜在的GNN衍生描述符解码为明确、具有物理意义的公式。这个统一框架通过提供符合可解释AI原则的高性能透明工具，推动了材料信息学的发展，为更有针对性和自主性的材料发现铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces MatterVial, an innovative hybrid framework forfeature-based machine learning in materials science. MatterVial expands thefeature space by integrating latent representations from a diverse suite ofpretrained graph neural network (GNN) models including: structure-based(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, withcomputationally efficient, GNN-approximated descriptors and novel features fromsymbolic regression. Our approach combines the chemical transparency oftraditional feature-based models with the predictive power of deep learningarchitectures. When augmenting the feature-based model MODNet on Matbenchtasks, this method yields significant error reductions and elevates itsperformance to be competitive with, and in several cases superior to,state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% formultiple tasks. An integrated interpretability module, employing surrogatemodels and symbolic regression, decodes the latent GNN-derived descriptors intoexplicit, physically meaningful formulas. This unified framework advancesmaterials informatics by providing a high-performance, transparent tool thataligns with the principles of explainable AI, paving the way for more targetedand autonomous materials discovery.</description>
      <author>example@mail.com (Rogério Almeida Gouvêa, Pierre-Paul De Breuck, Tatiane Pretto, Gian-Marco Rignanese, Marcos José Leite Santos)</author>
      <guid isPermaLink="false">2509.03547v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of Linguistic Representations Throughout LLM Pretraining</title>
      <link>http://arxiv.org/abs/2509.05291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出使用稀疏交叉编码器和新的相对间接效应(RelIE)指标来追踪大型语言模型预训练过程中语言特征的演变，填补了传统评估方法无法揭示模型如何获取概念和能力的空白。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在预训练过程中学习到复杂的抽象能力，如检测不规则复数名词主语，但传统评估方法无法揭示这些能力如何形成。&lt;h4&gt;目的&lt;/h4&gt;为了更好地理解模型在概念层面的训练过程，发现和校准不同模型检查点之间的特征演变。&lt;h4&gt;方法&lt;/h4&gt;使用稀疏交叉编码器在具有显著性能和表示变化的开放源代码检查点之间进行训练，并引入相对间接效应(RelIE)指标来追踪特征何时对任务性能变得因果重要。&lt;h4&gt;主要发现&lt;/h4&gt;交叉编码器可以检测预训练过程中语言特征的出现、维持和消失，并通过RelIE指标能够识别特征变得对任务性能有因果相关性的具体训练阶段。&lt;h4&gt;结论&lt;/h4&gt;该方法与架构无关且可扩展，为更可解释和细粒度地分析预训练过程中的表示学习提供了有希望的途径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在预训练过程中学习到复杂的抽象能力，比如检测不规则复数名词主语。然而，目前尚不清楚特定的语言能力何时以及如何出现，因为传统的评估方法无法揭示模型如何获取概念和能力。为了填补这一空白并更好地理解模型在概念层面的训练过程，我们使用稀疏交叉编码器来发现和校准不同模型检查点之间的特征。使用这种方法，我们跟踪了预训练过程中语言特征的演变。我们在具有显著性能和表示变化的开放源代码检查点之间训练交叉编码器，并引入了一种新的指标——相对间接效应(RelIE)，用于追踪单个特征对任务性能变得因果相关的时间点。我们表明，交叉编码器可以检测预训练过程中特征的出现、维持和消失。我们的方法与架构无关且可扩展，为更可解释和细粒度地分析预训练过程中的表示学习提供了有希望的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) learn non-trivial abstractions duringpretraining, like detecting irregular plural noun subjects. However, it is notwell understood when and how specific linguistic abilities emerge astraditional evaluation methods such as benchmarking fail to reveal how modelsacquire concepts and capabilities. To bridge this gap and better understandmodel training at the concept level, we use sparse crosscoders to discover andalign features across model checkpoints. Using this approach, we track theevolution of linguistic features during pretraining. We train crosscodersbetween open-sourced checkpoint triplets with significant performance andrepresentation shifts, and introduce a novel metric, Relative Indirect Effects(RelIE), to trace training stages at which individual features become causallyimportant for task performance. We show that crosscoders can detect featureemergence, maintenance, and discontinuation during pretraining. Our approach isarchitecture-agnostic and scalable, offering a promising path toward moreinterpretable and fine-grained analysis of representation learning throughoutpretraining.</description>
      <author>example@mail.com (Deniz Bayazit, Aaron Mueller, Antoine Bosselut)</author>
      <guid isPermaLink="false">2509.05291v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2509.05188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对手语识别的自监督学习框架，解决了传统对比学习方法在处理手语视频时面临的忽视视频部分相关性和负对相似性问题。&lt;h4&gt;背景&lt;/h4&gt;手语识别(SLR)是一个旨在识别视频中手语的机器学习任务。由于标注数据稀缺，无监督方法如对比学习在该领域变得很有前景，它们通过拉近正对(同一实例的两个增强版本)和推开负对(不同于正对)来学习有意义的表示。&lt;h4&gt;目的&lt;/h4&gt;设计一个自监督学习框架，学习对手语识别有意义的表现，解决传统对比学习方法在SLR中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个关键组件的自监督学习框架：(1)一种基于自由负对的新型自监督方法；(2)一种新的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;该方法与多种对比和自监督方法相比，在线性评估、半监督学习和手语间迁移性方面显示出显著的精度提升。&lt;h4&gt;结论&lt;/h4&gt;通过解决传统对比学习方法在SLR中的两个关键问题，本文提出的自监督框架能够学习更具区分性的特征，提高下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;手语识别(SLR)是一种机器学习任务，旨在识别视频中的手语。由于标注数据的稀缺，像对比学习这样的无监督方法已成为该领域有前景的方法。它们通过将正对(同一实例的两个增强版本)拉近并将负对(不同于正对)推开来学习有意义的表示。在SLR中，在手语视频中，只有某些部分提供对其识别真正有用的信息。将对比方法应用于SLR引发两个问题：(i)对比学习方法同等对待视频的所有部分，没有考虑某些部分的相关性；(ii)不同手势之间的共享动作使得负对高度相似，增加了手势区分的难度。这些问题导致学习到对手语识别缺乏区分性的特征，并在下游任务中表现不佳。为此，本文提出了一种专为学习SLR有意义表示而设计的自监督学习框架。该框架包含两个协同工作的关键组件：(i)一种基于自由负对的新型自监督方法；(ii)一种新的数据增强技术。与多种对比和自监督方法相比，该方法在线性评估、半监督学习和手语间的迁移性方面显示出显著的精度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language recognition (SLR) is a machine learning task aiming to identifysigns in videos. Due to the scarcity of annotated data, unsupervised methodslike contrastive learning have become promising in this field. They learnmeaningful representations by pulling positive pairs (two augmented versions ofthe same instance) closer and pushing negative pairs (different from thepositive pairs) apart. In SLR, in a sign video, only certain parts provideinformation that is truly useful for its recognition. Applying contrastivemethods to SLR raises two issues: (i) contrastive learning methods treat allparts of a video in the same way, without taking into account the relevance ofcertain parts over others; (ii) shared movements between different signs makenegative pairs highly similar, complicating sign discrimination. These issueslead to learning non-discriminative features for sign recognition and poorresults in downstream tasks. In response, this paper proposes a self-supervisedlearning framework designed to learn meaningful representations for SLR. Thisframework consists of two key components designed to work together: (i) a newself-supervised approach with free-negative pairs; (ii) a new data augmentationtechnique. This approach shows a considerable gain in accuracy compared toseveral contrastive and self-supervised methods, across linear evaluation,semi-supervised learning, and transferability between sign languages.</description>
      <author>example@mail.com (Ariel Basso Madjoukeng, Jérôme Fink, Pierre Poitier, Edith Belise Kenmogne, Benoit Frenay)</author>
      <guid isPermaLink="false">2509.05188v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Multidimensional Urban Poverty Representation with Satellite Imagery</title>
      <link>http://arxiv.org/abs/2509.04958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的表示学习框架，通过融合卫星图像中的可及性、形态学和经济特征，实现精确的城市贫困制图，解决了仅依赖城市化特征无法准确捕捉贫困的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习的进步使得从卫星图像推断城市社会经济特征成为可能，但仅依赖城市化特征的模型与贫困指标相关性较弱，因为无序的城市增长可能掩盖经济差异和空间不平等。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的表示学习框架，从超高分辨率卫星图像中捕获多维度的剥夺相关特征，用于精确的城市贫困制图。&lt;h4&gt;方法&lt;/h4&gt;该方法整合了三种互补特征：(1)通过对比学习编码接近基本基础设施的可及性特征；(2)从建筑足迹推导反映非正规定居点住房条件的形态学特征；(3)从夜间灯光强度推断经济活动的经济特征。同时，通过后门调整机制利用形态学特征减轻训练经济模块时的虚假相关性。&lt;h4&gt;主要发现&lt;/h4&gt;通过融合这些互补特征，该框架能够捕捉与经济发展趋势不同的贫困复杂性质。在开普敦、达卡和金边三个首都城市的评估中，该模型显著优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架为数据稀缺地区的贫困制图和政策支持提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的最新进展使得从卫星图像中推断城市社会经济特征成为可能。然而，仅依赖城市化特征的模型通常与贫困指标相关性较弱，因为无序的城市增长可能掩盖经济差异和空间不平等。为解决这一局限，我们引入了一种新的表示学习框架，从超高分辨率卫星图像中捕获多维度的剥夺相关特征，用于精确的城市贫困制图。我们的方法整合了三种互补特征：(1)通过对比学习编码接近基本基础设施的可及性特征；(2)从建筑足迹推导反映非正规定居点住房条件的形态学特征；(3)从夜间灯光强度推断作为经济活动代理的经济特征。为了减轻虚假相关性——例如那些不能代表贫困条件的非住宅夜间灯光源——我们在训练经济模块时纳入了一个利用形态学特征的后门调整机制。通过将这些互补特征融合到统一表示中，我们的框架捕捉了贫困的复杂性质，这些性质往往与经济发展趋势不同。在开普敦、达卡和金边三个首都城市的评估中，我们的模型显著优于现有基线，为数据稀缺地区的贫困制图和政策支持提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in deep learning have enabled the inference of urbansocioeconomic characteristics from satellite imagery. However, models relyingsolely on urbanization traits often show weak correlations with povertyindicators, as unplanned urban growth can obscure economic disparities andspatial inequalities. To address this limitation, we introduce a novelrepresentation learning framework that captures multidimensionaldeprivation-related traits from very high-resolution satellite imagery forprecise urban poverty mapping. Our approach integrates three complementarytraits: (1) accessibility traits, learned via contrastive learning to encodeproximity to essential infrastructure; (2) morphological traits, derived frombuilding footprints to reflect housing conditions in informal settlements; and(3) economic traits, inferred from nightlight intensity as a proxy for economicactivity. To mitigate spurious correlations - such as those fromnon-residential nightlight sources that misrepresent poverty conditions - weincorporate a backdoor adjustment mechanism that leverages morphological traitsduring training of the economic module. By fusing these complementary featuresinto a unified representation, our framework captures the complex nature ofpoverty, which often diverges from economic development trends. Evaluationsacross three capital cities - Cape Town, Dhaka, and Phnom Penh - show that ourmodel significantly outperforms existing baselines, offering a robust tool forpoverty mapping and policy support in data-scarce regions.</description>
      <author>example@mail.com (Sungwon Park, Sumin Lee, Jihee Kim, Jae-Gil Lee, Meeyoung Cha, Jeasurk Yang, Donghyun Ahn)</author>
      <guid isPermaLink="false">2509.04958v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization</title>
      <link>http://arxiv.org/abs/2509.04745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何通过语音学归纳偏差改进手语学习表示的推广性，特别是在处理未见手语符号时的表现。&lt;h4&gt;背景&lt;/h4&gt;手语数据集在词汇方面通常不具有代表性，这突显了需要能够推广到未见过的手语符号的模型的必要性。&lt;h4&gt;目的&lt;/h4&gt;调查两种语音学归纳偏差（参数解耦和语音学半监督）以提高已知手语符号的识别和未见手语符号的重构质量。&lt;h4&gt;方法&lt;/h4&gt;使用向量量化自编码器，结合参数解耦（架构偏差）和语音学半监督（正则化技术）来改进手语识别和重构。&lt;h4&gt;主要发现&lt;/h4&gt;与受控基线相比，所提出模型学习到的表示对于一次性重建未见手语符号更有效，并且对于手语识别更具区分性。&lt;h4&gt;结论&lt;/h4&gt;明确的、语言动机的偏差可以改进手语学习表示的推广性，本研究提供了相关的定量分析。&lt;h4&gt;翻译&lt;/h4&gt;手语数据集在词汇方面通常不具有代表性，突显了需要能够推广到未见手语的模型的必要性。向量量化是学习离散类令牌表示的一种有前景的方法，但目前尚未评估学习到的单元是否会捕获阻碍词汇外性能的虚假相关性。本研究调查了两种语音学归纳偏差：参数解耦（架构偏差）和语音学半监督（正则化技术），以使用向量量化自编码器改进已知手语符号的孤立手语识别和未见手语符号的重构质量。主要发现是，与受控基线相比，所提出模型学习到的表示对于一次性重建未见手语符号更有效，并且对于手语识别更具区分性。这项工作提供了关于明确的、语言动机的偏差如何改进手语学习表示的推广性的定量分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language datasets are often not representative in terms of vocabulary,underscoring the need for models that generalize to unseen signs. Vectorquantization is a promising approach for learning discrete, token-likerepresentations, but it has not been evaluated whether the learned unitscapture spurious correlations that hinder out-of-vocabulary performance. Thiswork investigates two phonological inductive biases: Parameter Disentanglement,an architectural bias, and Phonological Semi-Supervision, a regularizationtechnique, to improve isolated sign recognition of known signs andreconstruction quality of unseen signs with a vector-quantized autoencoder. Theprimary finding is that the learned representations from the proposed model aremore effective for one-shot reconstruction of unseen signs and morediscriminative for sign identification compared to a controlled baseline. Thiswork provides a quantitative analysis of how explicit, linguistically-motivatedbiases can improve the generalization of learned representations of signlanguage.</description>
      <author>example@mail.com (Lee Kezar, Zed Sehyr, Jesse Thomason)</author>
      <guid isPermaLink="false">2509.04745v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</title>
      <link>http://arxiv.org/abs/2509.04737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures, Accepted at CoRL2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种运动生成模型，使机器人能够根据人类指令中的修饰词调整动作，在擦拭和拾取放置任务中表现良好，能够在线响应修饰指令调整动作，优于传统的批量处理方法。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习领域，通过语言指令协调机器人动作正变得越来越可行，但使机器人动作适应人类指令仍具挑战性，因为这些指令通常是定性的，需要探索满足不同条件的行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种运动生成模型，使机器人能够根据人类指令中的修饰词在任务执行期间调整动作行为。&lt;h4&gt;方法&lt;/h4&gt;通过将演示分割成短序列，分配与特定修饰类型相对应的弱监督标签，学习从修饰指令到动作的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;在擦拭和拾取放置任务中评估显示，该方法能够在线响应修饰指令调整动作，而传统批量处理方法在执行期间无法适应。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型能够实时调整机器人动作以适应人类指令中的修饰条件，提高了机器人对语言指令的适应能力。&lt;h4&gt;翻译&lt;/h4&gt;在机器人学习领域，通过语言指令协调机器人动作正变得越来越可行。然而，使机器人动作适应人类指令仍然具有挑战性，因为这些指令通常是定性的，需要探索满足不同条件的行为。本文提出了一种运动生成模型，使机器人能够根据人类指令中的修饰词在任务执行期间调整动作行为。所提出的方法通过将演示分割成短序列，分配与特定修饰类型相对应的弱监督标签，学习从修饰指令到动作的映射。我们在擦拭和拾取放置任务中评估了该方法。结果表明，它能够在线响应修饰指令调整动作，而传统的批量处理方法在执行期间无法适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of robot learning, coordinating robot actions through languageinstructions is becoming increasingly feasible. However, adapting actions tohuman instructions remains challenging, as such instructions are oftenqualitative and require exploring behaviors that satisfy varying conditions.This paper proposes a motion generation model that adapts robot actions inresponse to modifier directives human instructions imposing behavioralconditions during task execution. The proposed method learns a mapping frommodifier directives to actions by segmenting demonstrations into shortsequences, assigning weakly supervised labels corresponding to specificmodifier types. We evaluated our method in wiping and pick and place tasks.Results show that it can adjust motions online in response to modifierdirectives, unlike conventional batch-based methods that cannot adapt duringexecution.</description>
      <author>example@mail.com (Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji)</author>
      <guid isPermaLink="false">2509.04737v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
      <link>http://arxiv.org/abs/2509.04734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Beyond I-Con框架，通过探索替代的统计散度和相似性核来系统地发现新的损失函数，解决了KL散度在表示学习中可能带来的优化挑战。&lt;h4&gt;背景&lt;/h4&gt;Information Contrastive (I-Con)框架揭示了超过23种表示学习方法隐式地最小化了数据分布和学习分布之间的KL散度，但KL散度存在不对称性和无界性等特性，可能导致优化挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性发现新型损失函数的框架，通过探索替代的统计散度和相似性核来超越现有的基于KL的方法。&lt;h4&gt;方法&lt;/h4&gt;探索替代的统计散度和相似性核；修改PMI算法使用总变差(TV)距离；在监督对比学习中使用TV和基于距离的相似性核替代KL和角度核；在降维中用有界的f-散度替代KL。&lt;h4&gt;主要发现&lt;/h4&gt;(1)在DINO-ViT嵌入的无监督聚类上，使用TV距离的修改版PMI算法实现了最先进结果；(2)在监督对比学习中，使用TV和基于距离的相似性核超越了标准方法；(3)在降维任务中，用有界的f-散度替代KL获得了比SNE更好的结果和下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在表示学习优化中，散度和相似性核的选择至关重要。&lt;h4&gt;翻译&lt;/h4&gt;信息对比(I-Con)框架揭示，超过23种表示学习方法隐式地最小化了数据分布和学习分布之间的KL散度，这些分布编码了数据点之间的相似性。然而，基于KL的损失函数可能与真实目标不一致，KL散度的特性如不对称性和无界性可能会带来优化挑战。我们提出了Beyond I-Con框架，通过探索替代的统计散度和相似性核，使系统能够发现新的损失函数。主要发现：(1)在DINO-ViT嵌入的无监督聚类中，我们通过修改PMI算法使用总变差(TV)距离实现了最先进的结果；(2)在监督对比学习中，我们使用TV和基于距离的相似性核而非KL和角度核，超越了标准方法；(3)在降维方面，我们通过用有界的f-散度替代KL，获得了比SNE更好的定性结果和下游任务性能。我们的结果突显了在表示学习优化中考虑散度和相似性核选择的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Information Contrastive (I-Con) framework revealed that over 23representation learning methods implicitly minimize KL divergence between dataand learned distributions that encode similarities between data points.However, a KL-based loss may be misaligned with the true objective, andproperties of KL divergence such as asymmetry and unboundedness may createoptimization challenges. We present Beyond I-Con, a framework that enablessystematic discovery of novel loss functions by exploring alternativestatistical divergences and similarity kernels. Key findings: (1) onunsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-artresults by modifying the PMI algorithm to use total variation (TV) distance;(2) on supervised contrastive learning, we outperform the standard approach byusing TV and a distance-based similarity kernel instead of KL and an angularkernel; (3) on dimensionality reduction, we achieve superior qualitativeresults and better performance on downstream tasks than SNE by replacing KLwith a bounded f-divergence. Our results highlight the importance ofconsidering divergence and similarity kernel choices in representation learningoptimization.</description>
      <author>example@mail.com (Jasmine Shone, Shaden Alshammari, Mark Hamilton, Zhening Li, William Freeman)</author>
      <guid isPermaLink="false">2509.04734v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems</title>
      <link>http://arxiv.org/abs/2509.04694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一表示学习框架，用于解决推荐系统中用户意图多样性和行为不确定性的联合建模问题。该框架包含多意图表示模块和不确定性建模机制，通过贝叶斯分布建模捕捉行为模糊性和偏好波动，结合长期意图和短期行为信号，提高推荐准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统推荐系统在处理复杂用户行为时面临建模瓶颈，特别是在捕捉用户意图多样性和行为不确定性方面存在挑战。用户行为序列中包含多粒度兴趣结构，且存在行为模糊性和偏好波动，这些因素都需要被有效建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够联合建模用户意图多样性和行为不确定性的推荐系统框架，通过提取用户行为序列中的多粒度兴趣结构，捕捉行为模糊性和偏好波动，提高推荐的准确性和鲁棒性，特别是在冷启动和行为干扰场景下的表现。&lt;h4&gt;方法&lt;/h4&gt;提出统一表示学习框架，包含：多意图表示模块（引入多个潜在意图向量，通过注意力机制进行加权融合，生成长期用户偏好语义丰富表示）和不确定性建模机制（通过高斯分布学习行为表示的均值和协方差，反映用户在不同行为上下文中的置信度），以及可学习融合策略（结合长期意图和短期行为信号，生成最终用户表示）。&lt;h4&gt;主要发现&lt;/h4&gt;在标准公共数据集上的实验表明，该方法在多个指标上优于现有代表性模型；在冷启动和行为干扰场景下表现出更高的稳定性和适应性；有效缓解了传统方法处理复杂用户行为时的建模瓶颈。&lt;h4&gt;结论&lt;/h4&gt;统一建模策略在现实世界推荐任务中具有有效性和实用价值，能够同时提高推荐准确性和鲁棒性，特别是在处理复杂用户行为方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了推荐系统中联合建模用户意图多样性和行为不确定性的挑战。提出了一种统一的表示学习框架。该框架构建了多意图表示模块和不确定性建模机制。它从用户行为序列中提取多粒度兴趣结构。使用贝叶斯分布建模捕捉行为模糊性和偏好波动。在多意图建模部分，模型引入了多个潜在意图向量。这些向量通过注意力机制进行加权融合，生成长期用户偏好的语义丰富表示。在不确定性建模部分，模型通过高斯分布学习行为表示的均值和协方差。这反映了用户在不同行为上下文中的置信度。接下来，使用可学习融合策略结合长期意图和短期行为信号。这产生了最终的用户表示，提高了推荐准确性和鲁棒性。该方法在标准公共数据集上进行了评估。实验结果表明，它在多个指标上优于现有代表性模型。在冷启动和行为干扰场景下，它还表现出更高的稳定性和适应性。该方法缓解了传统方法处理复杂用户行为时面临的建模瓶颈。这些发现证实了统一建模策略在现实世界推荐任务中的有效性和实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of jointly modeling user intent diversityand behavioral uncertainty in recommender systems. A unified representationlearning framework is proposed. The framework builds a multi-intentrepresentation module and an uncertainty modeling mechanism. It extractsmulti-granularity interest structures from user behavior sequences. Behavioralambiguity and preference fluctuation are captured using Bayesian distributionmodeling. In the multi-intent modeling part, the model introduces multiplelatent intent vectors. These vectors are weighted and fused using an attentionmechanism to generate semantically rich representations of long-term userpreferences. In the uncertainty modeling part, the model learns the mean andcovariance of behavior representations through Gaussian distributions. Thisreflects the user's confidence in different behavioral contexts. Next, alearnable fusion strategy is used to combine long-term intent and short-termbehavior signals. This produces the final user representation, improving bothrecommendation accuracy and robustness. The method is evaluated on standardpublic datasets. Experimental results show that it outperforms existingrepresentative models across multiple metrics. It also demonstrates greaterstability and adaptability under cold-start and behavioral disturbancescenarios. The approach alleviates modeling bottlenecks faced by traditionalmethods when dealing with complex user behavior. These findings confirm theeffectiveness and practical value of the unified modeling strategy inreal-world recommendation tasks.</description>
      <author>example@mail.com (Wei Xu, Jiasen Zheng, Junjiang Lin, Mingxuan Han, Junliang Du)</author>
      <guid isPermaLink="false">2509.04694v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</title>
      <link>http://arxiv.org/abs/2509.05144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SGS-3D的高保真3D实例分割方法，通过'先分割后生长'框架解决2D到3D提升过程中的累积误差问题，有效融合语义和几何信息，显著提高了分割准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确的3D实例分割对3D视觉领域的高质量场景理解至关重要，但基于2D到3D提升方法的现有技术难以产生精确的实例级分割，主要因为在提升过程中从模糊语义引导和深度约束不足引入了累积误差。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D实例分割方法的局限性，提出一种能够提高分割精度和鲁棒性的新方法，特别是在处理模糊语义和几何信息时。&lt;h4&gt;方法&lt;/h4&gt;提出SGS-3D框架，采用'先分割后生长'策略：首先使用几何基元净化和分割模糊的提升掩模，然后将它们生长为场景中的完整实例；引入掩模过滤策略利用3D几何基元共现识别可靠语义；利用空间连续性和高级特征进行几何精炼以构建细粒度对象实例。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet200、ScanNet++和KITTI-360数据集上的实验表明，SGS-3D显著提高了分割准确性和鲁棒性，能够抵抗来自预训练模型的不准确掩模，产生高保真对象实例，并在多样化的室内和室外环境中保持强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SGS-3D作为一种无需训练的精炼方法，通过联合融合语义和几何信息，有效解决了2D到3D提升过程中的累积误差问题，为高保真3D实例分割提供了新思路，代码已在补充材料中公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D实例分割对3D视觉领域的高质量场景理解至关重要。然而，基于2D到3D提升方法的3D实例分割难以产生精确的实例级分割，这是由于在从模糊语义引导和深度约束不足的提升过程中引入了累积误差。为解决这些挑战，我们提出了用于高保真3D实例分割的可靠语义掩模分割和生长方法（SGS-3D），一种新颖的'先分割后生长'框架，首先使用几何基元净化和分割模糊的提升掩模，然后将它们生长为场景中的完整实例。与直接依赖原始提升掩模并牺牲分割准确性的现有方法不同，SGS-3D作为一种无需训练的精炼方法，联合融合语义和几何信息，实现了两种表示级别之间的有效合作。具体而言，对于语义引导，我们引入了一种掩模过滤策略，利用3D几何基元的共现来识别和移除模糊掩模，从而确保与3D对象实例更可靠的语义一致性。对于几何精炼，我们利用空间连续性和高级特征构建细粒度对象实例，特别是在不同对象之间语义模糊的情况下。在ScanNet200、ScanNet++和KITTI-360上的实验结果表明，SGS-3D显著提高了分割准确性和对预训练模型不准确掩模的鲁棒性，产生高保真对象实例，同时在多样化的室内和室外环境中保持强大的泛化能力。代码可在补充材料中获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于2D到3D提升(2D-to-3D lifting)的3D实例分割方法中存在的精度问题，特别是在处理模糊语义引导、不足深度约束以及遮挡场景时导致的累积误差。这个问题在现实中非常重要，因为3D实例分割是自动驾驶、虚拟现实和多模态场景理解等领域的核心技术，而现有方法在开放世界环境中的泛化能力有限，且深度传感器在无纹理和高反射表面表现不佳，在野外场景中常常不可用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到基于2D预训练基础模型进行3D场景感知是一个有前景的方向，但现有方法存在局限性：基于特征的方法有训练效率和误差传播问题，基于掩码的方法则忽略了语义信息的稳健传播。作者借鉴了现有工作中的3D几何过度分割思想，使用SAM提取语义掩码，并采用HDBSCAN进行空间分割，但创新性地提出了'先分割后生长'策略，联合利用语义和几何线索来克服误差累积问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'先分割后生长'(split-then-grow)策略，联合利用语义和几何线索来克服2D到3D提升中的误差累积。整体流程分为三个主要阶段：1)点-图像映射：建立鲁棒映射并计算可见性；2)2D掩码提案：生成初始掩码并通过同时出现过滤消除模糊掩码；3)语义引导聚合：先通过空间连续性分割生成纯语义-几何种子，再利用特征引导生长将种子扩展为完整实例，最后通过多视图渐进合并形成最终对象实例。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SGS-3D训练-free框架，实现'先分割后生长'策略；2)遮挡感知的点-图像映射，无需真实深度图；3)基于同时出现的掩码过滤机制，提高计算效率和鲁棒性；4)语义引导聚合管道，结合空间连续性和特征引导。相比之前的工作，SGS-3D不直接依赖原始提升掩码，而是先净化和分割模糊掩码；联合融合语义和几何信息；在深度约束不足场景中表现更好；处理遮挡和缺乏深度信息时更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SGS-3D通过可靠的语义掩码分割和生长策略，解决了2D到3D提升方法中的累积误差问题，实现了高保真的3D实例分割，并在多种室内外场景中达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D instance segmentation is crucial for high-quality sceneunderstanding in the 3D vision domain. However, 3D instance segmentation basedon 2D-to-3D lifting approaches struggle to produce precise instance-levelsegmentation, due to accumulated errors introduced during the lifting processfrom ambiguous semantic guidance and insufficient depth constraints. To tacklethese challenges, we propose splitting and growing reliable semantic mask forhigh-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"framework that first purifies and splits ambiguous lifted masks using geometricprimitives, and then grows them into complete instances within the scene.Unlike existing approaches that directly rely on raw lifted masks and sacrificesegmentation accuracy, SGS-3D serves as a training-free refinement method thatjointly fuses semantic and geometric information, enabling effectivecooperation between the two levels of representation. Specifically, forsemantic guidance, we introduce a mask filtering strategy that leverages theco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,thereby ensuring more reliable semantic consistency with the 3D objectinstances. For the geometric refinement, we construct fine-grained objectinstances by exploiting both spatial continuity and high-level features,particularly in the case of semantic ambiguity between distinct objects.Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate thatSGS-3D substantially improves segmentation accuracy and robustness againstinaccurate masks from pre-trained models, yielding high-fidelity objectinstances while maintaining strong generalization across diverse indoor andoutdoor environments. Code is available in the supplementary materials.</description>
      <author>example@mail.com (Chaolei Wang, Yang Luo, Jing Du, Siyu Chen, Yiping Chen, Ting Han)</author>
      <guid isPermaLink="false">2509.05144v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
      <link>http://arxiv.org/abs/2509.05230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference on Empirical Methods in Natural Language  Processing (EMNLP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURE是一种新型轻量级框架，能够系统地分离和抑制概念性捷径，同时保留内容信息，在IMDB和Yelp数据集上显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;预训练语言模型在多种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性的影响，这些相关性损害了模型的鲁棒性和公平性。&lt;h4&gt;目的&lt;/h4&gt;引入CURE框架，系统地分离和抑制概念性捷径，同时保留基本的内容信息，以提高模型的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;通过专用内容提取器和反转网络提取概念无关表示，并使用可控去偏模块和对比学习来微调剩余概念线索的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在IMDB上的F1分数绝对提高+10分，在Yelp上提高+2分，同时引入最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;CURE为对抗概念偏差提供了一个灵活的无监督蓝图，有助于构建更可靠和公平的语言理解系统。&lt;h4&gt;翻译&lt;/h4&gt;预训练语言模型在多种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性的影响，这些相关性损害了模型的鲁棒性和公平性。在这项工作中，我们引入了CURE，一种新型轻量级框架，能够系统地分离和抑制概念性捷径，同时保留基本的内容信息。我们的方法首先通过一个由反转网络强化的专用内容提取器提取概念无关的表示，确保最小化任务相关信息的损失。随后的可控去偏模块采用对比学习来微调剩余概念线索的影响，使模型能够根据目标任务适当减少有害偏差或利用有益相关性。在IMDB和Yelp数据集上使用三种预训练架构进行评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。我们的方法为对抗概念偏差提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained language models have achieved remarkable success across diverseapplications but remain susceptible to spurious, concept-driven correlationsthat impair robustness and fairness. In this work, we introduce CURE, a noveland lightweight framework that systematically disentangles and suppressesconceptual shortcuts while preserving essential content information. Our methodfirst extracts concept-irrelevant representations via a dedicated contentextractor reinforced by a reversal network, ensuring minimal loss oftask-relevant information. A subsequent controllable debiasing module employscontrastive learning to finely adjust the influence of residual conceptualcues, enabling the model to either diminish harmful biases or harnessbeneficial correlations as appropriate for the target task. Evaluated on theIMDB and Yelp datasets using three pre-trained architectures, CURE achieves anabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,while introducing minimal computational overhead. Our approach establishes aflexible, unsupervised blueprint for combating conceptual biases, paving theway for more reliable and fair language understanding systems.</description>
      <author>example@mail.com (Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci)</author>
      <guid isPermaLink="false">2509.05230v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation</title>
      <link>http://arxiv.org/abs/2509.04970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DeGuV的强化学习框架，通过可学习的掩码网络和对比学习技术，提高强化学习代理的泛化能力和样本效率，同时保持训练稳定性。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理可以从视觉输入中学习解决复杂任务，但将学到的技能泛化到新环境仍然是强化学习应用的主要挑战，特别是在机器人领域。虽然数据增强可以提高泛化能力，但它通常会降低样本效率和训练稳定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种强化学习框架，既能提高泛化能力，又能保持样本效率和训练稳定性。&lt;h4&gt;方法&lt;/h4&gt;DeGuV框架包含：1)可学习的掩码网络从深度输入生成掩码，只保留关键视觉信息；2)结合对比学习增强模型对关键特征的识别能力；3)稳定Q值估计技术在数据增强条件下保持训练稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在RL-ViGen基准测试中使用Franka Emika机器人评估显示，DeGuV在零样本模拟到现实迁移任务中表现出色，在泛化能力和样本效率方面优于最先进方法，同时通过突出显示视觉输入中最相关区域提高了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;DeGuV框架成功地解决了强化学习中泛化能力和样本效率之间的权衡问题，通过选择性关注关键视觉特征，实现了更好的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;强化学习代理可以从视觉输入中学习解决复杂任务，但将这些学到的技能泛化到新环境仍然是强化学习应用的主要挑战，特别是在机器人领域。虽然数据增强可以提高泛化能力，但它通常会降低样本效率和训练稳定性。本文介绍了DeGuV，一种增强泛化能力和样本效率的强化学习框架。具体而言，我们利用一个可学习的掩码网络从深度输入生成掩码，只保留关键视觉信息，同时丢弃无关像素。通过这种方式，我们确保强化学习代理关注关键特征，提高数据增强下的鲁棒性。此外，我们结合对比学习，并在增强条件下稳定Q值估计，以进一步提高样本效率和训练稳定性。我们在RL-ViGen基准上使用Franka Emika机器人评估了我们的方法，并证明了其在零样本模拟到现实迁移中的有效性。我们的结果表明，DeGuV在泛化能力和样本效率方面都优于最先进的方法，同时还通过突出显示视觉输入中最相关的区域提高了可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) agents can learn to solve complex tasks fromvisual inputs, but generalizing these learned skills to new environmentsremains a major challenge in RL application, especially robotics. While dataaugmentation can improve generalization, it often compromises sample efficiencyand training stability. This paper introduces DeGuV, an RL framework thatenhances both generalization and sample efficiency. In specific, we leverage alearnable masker network that produces a mask from the depth input, preservingonly critical visual information while discarding irrelevant pixels. Throughthis, we ensure that our RL agents focus on essential features, improvingrobustness under data augmentation. In addition, we incorporate contrastivelearning and stabilize Q-value estimation under augmentation to further enhancesample efficiency and training stability. We evaluate our proposed method onthe RL-ViGen benchmark using the Franka Emika robot and demonstrate itseffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuVoutperforms state-of-the-art methods in both generalization and sampleefficiency while also improving interpretability by highlighting the mostrelevant regions in the visual input</description>
      <author>example@mail.com (Tien Pham, Xinyun Chi, Khang Nguyen, Manfred Huber, Angelo Cangelosi)</author>
      <guid isPermaLink="false">2509.04970v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
      <link>http://arxiv.org/abs/2509.04833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PropVG是一种端到端的基于提议的视觉定位框架，整合了前景物体提议生成与参考物体理解，无需额外检测器。它引入了基于对比的参考评分(CRS)模块和多粒度目标区分(MTD)模块，通过对比学习增强物体理解能力，并融合物体级和语义级信息改善缺失目标的识别。实验证明其在多个基准测试上的有效性。&lt;h4&gt;背景&lt;/h4&gt;视觉定位领域最近已从基于提议的传统两阶段框架转向端到端的直接参考范式，因为前者效率低下且计算复杂度高。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉定位方法完全依赖被引用目标进行监督而忽视潜在前瞻性目标的问题，以及未能融入多粒度区分导致在复杂场景中物体识别不稳健的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出PropVG框架，包括：(1)端到端的基于提议的框架，无缝集成前景物体提议生成与参考物体理解；(2)基于对比的参考评分(CRS)模块，在句子和词级别采用对比学习；(3)多粒度目标区分(MTD)模块，融合物体级和语义级信息。&lt;h4&gt;主要发现&lt;/h4&gt;PropVG在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES)等多个基准测试上证明了其有效性，表明所提出的模块设计和框架能够提升视觉定位性能。&lt;h4&gt;结论&lt;/h4&gt;PropVG成功解决了现有视觉定位方法的局限性，通过整合前景物体提议生成与参考物体理解，以及利用对比学习和多粒度区分，显著提升了物体识别能力，特别是在复杂场景中。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉定位进展在很大程度上已经从基于提议的传统两阶段框架转移开来，因为它们效率低下且计算复杂度高，倾向于采用端到端的直接参考范式。然而，这些方法完全依赖于被引用的目标进行监督，忽视了潜在的前瞻性目标的好处。此外，现有方法通常未能融入多粒度区分，这对于复杂场景中稳健的物体识别至关重要。为解决这些局限性，我们提出了PropVG，这是一个端到端的基于提议的框架，据我们所知，它是第一个无缝集成前景物体提议生成与参考物体理解而无需额外检测器的框架。此外，我们引入了基于对比的参考评分(CRS)模块，该模块在句子和词级别采用对比学习，以增强理解和区分被引用物体的能力。另外，我们设计了一个多粒度目标区分(MTD)模块，融合物体级和语义级信息，以改善缺失目标的识别。在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES)基准上的大量实验证明了PropVG的有效性。代码和模型可在https://github.com/Dmmm1997/PropVG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in visual grounding have largely shifted away fromtraditional proposal-based two-stage frameworks due to their inefficiency andhigh computational complexity, favoring end-to-end direct reference paradigms.However, these methods rely exclusively on the referred target for supervision,overlooking the potential benefits of prominent prospective targets. Moreover,existing approaches often fail to incorporate multi-granularity discrimination,which is crucial for robust object identification in complex scenarios. Toaddress these limitations, we propose PropVG, an end-to-end proposal-basedframework that, to the best of our knowledge, is the first to seamlesslyintegrate foreground object proposal generation with referential objectcomprehension without requiring additional detectors. Furthermore, we introducea Contrastive-based Refer Scoring (CRS) module, which employs contrastivelearning at both sentence and word levels to enhance the capability inunderstanding and distinguishing referred objects. Additionally, we design aMulti-granularity Target Discrimination (MTD) module that fuses object- andsemantic-level information to improve the recognition of absent targets.Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes andmodels are available at https://github.com/Dmmm1997/PropVG.</description>
      <author>example@mail.com (Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang)</author>
      <guid isPermaLink="false">2509.04833v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</title>
      <link>http://arxiv.org/abs/2509.01910v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种将全球地理定位与概念瓶颈相结合的新框架，通过概念感知对齐模块提高了地理定位模型的准确性和解释性，首次将解释性引入地理定位领域。&lt;h4&gt;背景&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的精确地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管GeoCLIP等地理定位模型有所进步，但这些模型的解释性仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于概念的解释性方法无法有效与地理定位图像-位置对齐目标保持一致的问题，提出一个将全球地理定位与概念瓶颈相结合的新框架。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库上（如热带气候、山脉、大教堂等），并最小化概念级损失，增强概念特定子空间中的对齐，从而实现强大的解释性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在地理定位准确性上超越了GeoCLIP，在多样化的地理空间预测任务中提高了性能，并揭示了地理决策过程更丰富的语义洞察。&lt;h4&gt;结论&lt;/h4&gt;这是首个将解释性引入地理定位的工作，通过概念感知对齐模块实现了更好的性能和解释性。&lt;h4&gt;翻译&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的精确地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管像GeoCLIP这样的地理定位模型有所进步，但这些模型的解释性仍未得到充分探索。当前基于概念的解释性方法无法有效地与地理定位图像-位置对齐目标保持一致，导致次优的解释性和性能。为解决这一差距，我们提出了一种将全球地理定位与概念瓶颈相结合的新框架。我们的方法插入了一个概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库（如热带气候、山脉、大教堂）上，并最小化概念级损失，增强概念特定子空间中的对齐，实现强大的解释性。据我们所知，这是首个将解释性引入地理定位的工作。大量实验证明，我们的方法在地理定位准确性上超越了GeoCLIP，并在多样化的地理空间预测任务中提高了性能，揭示了地理决策过程更丰富的语义洞察。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Worldwide geo-localization involves determining the exact geographic locationof images captured globally, typically guided by geographic cues such asclimate, landmarks, and architectural styles. Despite advancements ingeo-localization models like GeoCLIP, which leverages images and locationalignment via contrastive learning for accurate predictions, theinterpretability of these models remains insufficiently explored. Currentconcept-based interpretability methods fail to align effectively withGeo-alignment image-location embedding objectives, resulting in suboptimalinterpretability and performance. To address this gap, we propose a novelframework integrating global geo-localization with concept bottlenecks. Ourmethod inserts a Concept-Aware Alignment Module that jointly projects image andlocation embeddings onto a shared bank of geographic concepts (e.g., tropicalclimate, mountain, cathedral) and minimizes a concept-level loss, enhancingalignment in a concept-specific subspace and enabling robust interpretability.To our knowledge, this is the first work to introduce interpretability intogeo-localization. Extensive experiments demonstrate that our approach surpassesGeoCLIP in geo-localization accuracy and boosts performance across diversegeospatial prediction tasks, revealing richer semantic insights into geographicdecision-making processes.</description>
      <author>example@mail.com (Furong Jia, Lanxin Liu, Ce Hou, Fan Zhang, Xinyan Liu, Yu Liu)</author>
      <guid isPermaLink="false">2509.01910v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>An Emotion Recognition Framework via Cross-modal Alignment of EEG and Eye Movement Data</title>
      <link>http://arxiv.org/abs/2509.04938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于跨模态注意力机制混合架构的情感识别框架，实现了脑电图和眼动数据的精确多模态对齐，在SEED-IV数据集上达到90.62%的准确率。&lt;h4&gt;背景&lt;/h4&gt;情感识别对于情感计算和行为预测应用至关重要，但依赖单一模态数据的传统系统往往无法捕捉情感状态的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一个情感识别框架，解决单一模态数据的局限性，实现脑电图(EEG)和眼动数据的精确多模态对齐。&lt;h4&gt;方法&lt;/h4&gt;基于跨模态注意力机制的混合架构，实现EEG和眼动数据的多模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在SEED-IV数据集上的实验表明，该方法达到了90.62%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这项工作为在情感识别中利用多模态数据提供了有前景的基础。&lt;h4&gt;翻译&lt;/h4&gt;情感识别对于情感计算和行为预测应用至关重要，但依赖单一模态数据的传统系统往往无法捕捉情感状态的复杂性。为解决这一局限，我们提出了一种情感识别框架，通过基于跨模态注意力机制的混合架构，实现了脑电图(EEG)和眼动数据的精确多模态对齐。在SEED-IV数据集上的实验表明，我们的方法达到了90.62%的准确率。这项工作为在情感识别中利用多模态数据提供了有前景的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotion recognition is essential for applications in affective computing andbehavioral prediction, but conventional systems relying on single-modality dataoften fail to capture the complexity of affective states. To address thislimitation, we propose an emotion recognition framework that achieves accuratemultimodal alignment of Electroencephalogram (EEG) and eye movement datathrough a hybrid architecture based on cross-modal attention mechanism.Experiments on the SEED-IV dataset demonstrate that our method achieve 90.62%accuracy. This work provides a promising foundation for leveraging multimodaldata in emotion recognition</description>
      <author>example@mail.com (Jianlu Wang, Yanan Wang, Tong Liu)</author>
      <guid isPermaLink="false">2509.04938v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation for Different Sensor Configurations in 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.04711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了3D物体检测中不同传感器配置之间的领域适应问题，提出了两种技术：下游微调和部分层微调，实验证明联合使用这两种技术优于简单联合训练。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶的最新进展凸显了精确3D物体检测的重要性，LiDAR因其在不同能见度条件下的鲁棒性而发挥核心作用。然而，不同车辆平台采用不同传感器配置，导致模型从一个配置应用到另一个配置时性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决3D物体检测中不同传感器配置之间的领域适应问题，以提高模型在多样化车辆平台上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两种技术：1) 下游微调（多数据集训练后进行数据集特定的微调）；2) 部分层微调（仅更新部分层以改善跨配置泛化能力）。使用在相同地理区域收集的、具有多种传感器配置的配对数据集进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;联合使用下游微调和部分层微调的训练方法，一致性地优于每种配置的简单联合训练，显著提高了模型在不同传感器配置间的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;研究结果为适应3D物体检测模型到多样化车辆平台提供了实用且可扩展的解决方案，有效解决了不同传感器配置间的领域差距问题。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶的最新进展凸显了精确3D物体检测的重要性，LiDAR由于其在不同能见度条件下的鲁棒性而发挥核心作用。然而，不同车辆平台通常采用不同的传感器配置，导致在一个配置上训练的模型应用于另一个配置时，由于点云分布的变化，性能会下降。先前关于3D物体检测的多数据集训练和领域适应工作主要解决了单一LiDAR内的环境领域差距和密度变化；相比之下，不同传感器配置的领域差距在很大程度上尚未探索。在这项工作中，我们解决了3D物体检测中不同传感器配置之间的领域适应问题。我们提出了两种技术：下游微调（多数据集训练后进行数据集特定的微调）和部分层微调（仅更新部分层以改善跨配置泛化能力）。使用在相同地理区域收集的、具有多种传感器配置的配对数据集，我们证明联合使用下游微调和部分层微调的训练一致性地优于每种配置的简单联合训练。我们的研究结果为适应3D物体检测模型到多样化车辆平台提供了实用且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决不同传感器配置下的3D目标检测领域适应问题。当在一个传感器配置（如RoboTaxi）上训练的模型应用到另一个配置（如RoboBus）时，由于点云分布变化导致性能显著下降。这个问题很重要，因为自动驾驶平台多样化（汽车、公交车、卡车等），每种平台使用不同传感器配置，而现有研究主要关注环境变化，忽略了传感器配置差异导致的领域差异，影响了实际部署中针对特定平台的高性能模型开发。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同传感器配置导致点云分布差异，进而影响3D检测性能。发现现有领域适应研究主要关注环境变化，而忽略了传感器配置差异。认识到多数据集训练虽提高泛化能力，但会导致特定配置性能下降。设计方法时借鉴了无监督领域适应(UDA)、半监督领域适应(SSDA)和多数据集训练(MDT)的思想，但针对传感器配置差异进行了调整。还受到大型语言模型微调策略的启发，提出了下游微调(Downstream Fine-tuning)和部分层微调(Partial Layer Fine-tuning)的创新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用多阶段训练策略，先通过联合训练学习跨配置的通用特征，再针对每个特定配置进行专门优化，并在微调过程中只更新对传感器配置变化敏感的层。整体流程：1)构建包含RoboTaxi和RoboBus不同配置的数据集；2)在所有配置数据上进行联合训练学习通用特征；3)针对每个配置进行下游微调；4)在微调中应用部分层微调策略，保持编码器和头部固定，更新主干和颈部层；5)在测试集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次针对传感器配置差异引起的领域差异进行研究；2)构建统一标注格式的多传感器配置数据集；3)提出下游微调策略，解决多数据集训练导致的性能下降；4)提出部分层微调策略，只更新敏感层提高效率；5)提供全面的消融研究和无监督领域适应实验。相比之前工作，不同之处在于：专注于传感器配置差异而非环境变化；解决了多数据集训练的性能下降问题；提出更高效的部分层微调；提供专门的数据集和评估基准；证明传感器配置差异比其他领域差距更适合通过微调适应。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过结合下游微调和部分层微调的训练策略，有效解决了不同LiDAR传感器配置间的领域适应问题，使3D目标检测模型能够更高效地适应多样化的自动驾驶平台。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in autonomous driving have underscored the importance ofaccurate 3D object detection, with LiDAR playing a central role due to itsrobustness under diverse visibility conditions. However, different vehicleplatforms often deploy distinct sensor configurations, causing performancedegradation when models trained on one configuration are applied to anotherbecause of shifts in the point cloud distribution. Prior work on multi-datasettraining and domain adaptation for 3D object detection has largely addressedenvironmental domain gaps and density variation within a single LiDAR; incontrast, the domain gap for different sensor configurations remains largelyunexplored. In this work, we address domain adaptation across different sensorconfigurations in 3D object detection. We propose two techniques: DownstreamFine-tuning (dataset-specific fine-tuning after multi-dataset training) andPartial Layer Fine-tuning (updating only a subset of layers to improvecross-configuration generalization). Using paired datasets collected in thesame geographic region with multiple sensor configurations, we show that jointtraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistentlyoutperforms naive joint training for each configuration. Our findings provide apractical and scalable solution for adapting 3D object detection models to thediverse vehicle platforms.</description>
      <author>example@mail.com (Satoshi Tanaka, Kok Seang Tan, Isamu Yamashita)</author>
      <guid isPermaLink="false">2509.04711v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</title>
      <link>http://arxiv.org/abs/2507.23567v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3D-MOOD，第一个端到端的3D单目开放集目标检测器，解决了现实应用中新环境和新目标类别的挑战，在多个数据集上取得了最先进结果。&lt;h4&gt;背景&lt;/h4&gt;单目3D目标检测在机器人和AR/VR等领域具有重要价值，但现有方法局限于封闭集设置，即训练集和测试集包含相同场景和目标类别，无法应对现实世界中的新环境和新类别挑战。&lt;h4&gt;目的&lt;/h4&gt;解决开放环境下的单目3D目标检测问题，提出第一个端到端的3D单目开放集目标检测器（3D-MOOD）。&lt;h4&gt;方法&lt;/h4&gt;1) 设计3D边界框头部将开放集2D检测提升到3D空间；2) 实现2D和3D任务的端到端联合训练；3) 使用几何先验条件化目标查询，提高跨场景3D估计泛化能力；4) 设计规范图像空间实现更高效的跨数据集训练。&lt;h4&gt;主要发现&lt;/h4&gt;在封闭集设置（Omni3D）和开放集设置（从Omni3D到Argoverse 2、ScanNet）上评估3D-MOOD，均取得新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;3D-MOOD成功解决了开放环境下的单目3D目标检测问题，通过创新方法设计实现了端到端训练，并在多个数据集上取得突破性成果，代码和模型已公开。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测对于机器人和AR/VR等应用具有重要价值。现有方法局限于封闭集设置，其中训练集和测试集包含相同的场景和/或目标类别。然而，现实应用常常引入新环境和新的目标类别，对这些方法构成了挑战。在本文中，我们解决了开放环境下的单目3D目标检测问题，并引入了第一个端到端的3D单目开放集目标检测器（3D-MOOD）。我们通过设计的3D边界框头部将开放集2D检测提升到3D空间，使2D和3D任务能够进行端到端的联合训练，从而获得更好的整体性能。我们使用几何先验条件化目标查询，克服了跨不同场景的3D估计泛化问题。为了进一步提高性能，我们设计了规范图像空间以实现更高效的跨数据集训练。我们在封闭集设置（Omni3D）和开放集设置（从Omni3D到Argoverse 2、ScanNet）上评估了3D-MOOD，并取得了新的最先进结果。代码和模型可在royyang0714.github.io/3D-MOOD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目开放集3D物体检测问题，即让模型能够识别并定位训练时未见过的物体类别和在未见过的场景中的物体。这个问题很重要，因为现实世界的应用（如机器人、AR/VR）经常需要面对新环境和未知物体，而现有方法受限于封闭集设计，只能检测训练时见过的物体类别，无法适应真实世界的多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了开放集单目3D物体检测的两个主要障碍：跨模态学习困难和单目深度估计泛化能力差。他们借鉴了G-DINO作为2D开放集检测器的基础架构，利用了通用单目深度估计方法（如UniDepth）的泛化能力，并参考了Cube R-CNN的虚拟深度概念。作者通过'提升'机制将2D检测转换为3D检测，并设计了几何感知的3D查询生成和规范图像空间来解决跨场景泛化问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'提升'机制将开放集2D检测转换为3D检测，同时利用几何先验增强模型泛化能力。整体流程：1)接收单目图像和语言提示；2)提取图像和文本特征；3)进行早期视觉-语言特征融合；4)生成检测查询；5)通过跨模态解码结合多模态信息；6)进行2D检测；7)使用3D边界框头和几何感知查询生成将2D检测结果'提升'为3D检测；8)输出包含3D位置、尺寸和方向的检测结果；9)通过端到端训练优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个端到端开放集单目3D物体检测器；2)2D到3D的提升机制；3)几何感知的3D查询生成；4)规范图像空间设计；5)辅助度量深度估计。相比之前工作：不同于封闭集方法（如Cube R-CNN）只能检测已知类别，3D-MOOD能检测未知类别；不同于OVM3D-Det的伪GT方法，3D-MOOD是端到端训练的；不同于传统方法使用类别先验和仅估计偏航角，3D-MOOD直接预测尺寸并使用6D方向参数化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3D-MOOD首次实现了端到端的开放集单目3D物体检测，通过创新的2D到3D提升机制和几何感知查询生成，使模型能够识别并定位未见过的物体类别，在封闭集和开放集设置下均达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection is valuable for various applications such asrobotics and AR/VR. Existing methods are confined to closed-set settings, wherethe training and testing sets consist of the same scenes and/or objectcategories. However, real-world applications often introduce new environmentsand novel object categories, posing a challenge to these methods. In thispaper, we address monocular 3D object detection in an open-set setting andintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).We propose to lift the open-set 2D detection into 3D space through our designed3D bounding box head, enabling end-to-end joint training for both 2D and 3Dtasks to yield better overall performance. We condition the object queries withgeometry prior and overcome the generalization for 3D estimation across diversescenes. To further improve performance, we design the canonical image space formore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-setsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), andachieve new state-of-the-art results. Code and models are available atroyyang0714.github.io/3D-MOOD.</description>
      <author>example@mail.com (Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, Yuqian Fu, Marc Pollefeys, Hermann Blum, Zuria Bauer)</author>
      <guid isPermaLink="false">2507.23567v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition</title>
      <link>http://arxiv.org/abs/2509.05019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20pages, 9 figures and 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了迁移学习与移动端卷积神经网络在阿拉伯手写字符识别中的集成应用，通过评估三种迁移学习策略和四种轻量级网络模型，发现MobileNet表现最佳，完全微调策略效果最好，IFHCDB数据集上达到99%的准确率，为资源高效的阿拉伯手写字符识别提供了新思路。&lt;h4&gt;背景&lt;/h4&gt;阿拉伯手写字符识别面临计算资源需求大和数据集稀缺的挑战，需要更高效的识别方法。&lt;h4&gt;目的&lt;/h4&gt;评估迁移学习与轻量级移动端卷积神经网络的结合效果，提高阿拉伯手写字符识别的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;评估三种迁移学习策略（完全微调、部分微调和从头训练）使用四种轻量级MbNets（MobileNet、SqueezeNet、MnasNet和ShuffleNet），在三个基准数据集（AHCD、HIJJA和IFHCDB）上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;MobileNet是表现最好的模型，在准确性、鲁棒性和效率方面都表现优异；ShuffleNet在泛化能力方面表现突出；IFHCDB数据集上使用MnasNet在完全微调下达到99%的准确率；AHCD数据集上使用ShuffleNet实现97%的准确率；HIJJA数据集挑战大，达到92%的峰值准确率；完全微调在所有指标中表现最佳，平衡了准确性和收敛速度；部分微调在各项指标中表现不佳。&lt;h4&gt;结论&lt;/h4&gt;迁移学习与MbNets的结合为资源高效的阿拉伯手写字符识别提供了潜力，为进一步优化和更广泛的应用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了迁移学习与移动端卷积神经网络的集成，以增强阿拉伯手写字符识别。针对大量计算需求和数据集稀缺等挑战，本研究评估了三种迁移学习策略——完全微调、部分微调和从头训练——使用了四种轻量级MbNets：MobileNet、SqueezeNet、MnasNet和ShuffleNet。实验在三个基准数据集上进行：AHCD、HIJJA和IFHCDB。MobileNet成为表现最佳的模型，在准确性、鲁棒性和效率方面持续取得优异表现，而ShuffleNet在泛化能力方面表现出色，特别是在完全微调的情况下。IFHCDB数据集获得了最佳结果，使用MnasNet在完全微调下达到99%的准确率，突显了其在鲁棒字符识别中的适用性。AHCD数据集使用ShuffleNet实现了97%的竞争性准确率，而HIJJA由于其变化性大带来了显著挑战，使用ShuffleNet达到92%的峰值准确率。值得注意的是，完全微调展示了最佳的整体性能，平衡了准确性和收敛速度，而部分微调在各项指标中表现不佳。这些发现强调了结合迁移学习和MbNets进行资源高效阿拉伯手写字符识别的潜力，为进一步优化和更广泛的应用铺平了道路。未来工作将探索架构修改、深入数据集特征分析、数据增强和高级敏感性分析，以提高模型的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study explores the integration of transfer learning (TL) withmobile-enabled convolutional neural networks (MbNets) to enhance ArabicHandwritten Character Recognition (AHCR). Addressing challenges like extensivecomputational requirements and dataset scarcity, this research evaluates threeTL strategies--full fine-tuning, partial fine-tuning, and training fromscratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, andShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistentlyachieving superior accuracy, robustness, and efficiency, with ShuffleNetexcelling in generalization, particularly under full fine-tuning. The IFHCDBdataset yielded the highest results, with 99% accuracy using MnasNet under fullfine-tuning, highlighting its suitability for robust character recognition. TheAHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJAposed significant challenges due to its variability, achieving a peak accuracyof 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overallperformance, balancing accuracy and convergence speed, while partialfine-tuning underperformed across metrics. These findings underscore thepotential of combining TL and MbNets for resource-efficient AHCR, paving theway for further optimizations and broader applications. Future work willexplore architectural modifications, in-depth dataset feature analysis, dataaugmentation, and advanced sensitivity analysis to enhance model robustness andgeneralizability.</description>
      <author>example@mail.com (Mohsine El Khayati, Ayyad Maafiri, Yassine Himeur, Hamzah Ali Alkhazaleh, Shadi Atalla, Wathiq Mansoor)</author>
      <guid isPermaLink="false">2509.05019v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study</title>
      <link>http://arxiv.org/abs/2509.05004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多种机器学习和深度学习模型在乳腺癌超声图像分类中的应用，发现ResNet-18表现最佳，支持AI诊断工具在临床实践中的应用。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是全球女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益而被广泛使用，在早期检测中起着关键作用，特别是在致密乳腺组织患者中。&lt;h4&gt;目的&lt;/h4&gt;全面研究机器学习和深度学习技术在乳腺癌超声图像分类中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，评估经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet-18达到最高的准确率（99.7%）和完美的恶性肿瘤敏感性；经典机器学习模型结合深度特征提取时也能达到有竞争力的性能；Grad-CAM可视化通过突出显示诊断相关的图像区域提高了模型透明度。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持将基于人工智能的诊断工具整合到临床工作流程中，证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。&lt;h4&gt;翻译&lt;/h4&gt;乳腺癌仍然是全球女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益而被广泛使用，在早期检测中起着关键作用，特别是在致密乳腺组织患者中。本文全面研究了机器学习和深度学习技术在乳腺癌超声图像分类中的应用。使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，我们评估了经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。实验结果表明，ResNet-18达到了最高的准确率（99.7%）和完美的恶性肿瘤敏感性。经典机器学习模型虽然被CNN超越，但当结合深度特征提取时也能达到有竞争力的性能。Grad-CAM可视化通过突出显示诊断相关的图像区域进一步提高了模型透明度。这些研究结果支持将基于人工智能的诊断工具整合到临床工作流程中，并证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer remains a leading cause of cancer-related mortality among womenworldwide. Ultrasound imaging, widely used due to its safety andcost-effectiveness, plays a key role in early detection, especially in patientswith dense breast tissue. This paper presents a comprehensive study on theapplication of machine learning and deep learning techniques for breast cancerclassification using ultrasound images. Using datasets such as BUSI, BUS-BRA,and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,GoogLeNet). Experimental results show that ResNet-18 achieves the highestaccuracy (99.7%) and perfect sensitivity for malignant lesions. Classical MLmodels, though outperformed by CNNs, achieve competitive performance whenenhanced with deep feature extraction. Grad-CAM visualizations further improvemodel transparency by highlighting diagnostically relevant image regions. Thesefindings support the integration of AI-based diagnostic tools into clinicalworkflows and demonstrate the feasibility of deploying high-performing,interpretable systems for ultrasound-based breast cancer detection.</description>
      <author>example@mail.com (Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Wathiq Mansoor)</author>
      <guid isPermaLink="false">2509.05004v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
      <link>http://arxiv.org/abs/2509.04889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15  in appendix)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了计算机视觉模型在预测蜘蛛相关图像恐惧水平方面的应用。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉的进步为临床应用开辟了新途径，特别是在计算机暴露疗法中，可以根据患者反应动态调整视觉刺激。&lt;h4&gt;目的&lt;/h4&gt;研究预训练的计算机视觉模型是否能准确预测与蜘蛛相关图像的恐惧水平。&lt;h4&gt;方法&lt;/h4&gt;使用迁移学习调整三种不同的模型，从一个包含313张图像的标准数据集中预测人类的恐惧评分（0-100分制），并通过交叉验证进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;模型平均绝对误差在10.1到11.0之间；减少数据集规模会显著损害性能，但进一步增加数据量不会带来实质性收益；模型预测基于蜘蛛相关特征；远景和人工/绘制的蜘蛛与较高误差相关。&lt;h4&gt;结论&lt;/h4&gt;可解释计算机视觉模型在预测恐惧评分方面具有潜力，模型可解释性和足够的数据集规模对于开发有效的情感感知治疗技术至关重要。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉的进步为临床应用开辟了新途径，特别是在计算机暴露疗法中，可以根据患者反应动态调整视觉刺激。作为开发此类自适应系统的重要一步，我们研究了预训练的计算机视觉模型是否能准确预测与蜘蛛相关图像的恐惧水平。我们使用迁移学习调整了三种不同的模型，从一个包含313张图像的标准数据集中预测人类的恐惧评分（0-100分制）。模型通过交叉验证进行评估，平均绝对误差在10.1到11.0之间。我们的学习曲线分析显示，减少数据集规模会显著损害性能，但进一步增加数据量不会带来实质性收益。可解释性评估表明，模型的预测是基于蜘蛛相关特征的。分类错误分析进一步确定了与较高误差相关的视觉条件（例如，远景和人工/绘制的蜘蛛）。这些发现展示了可解释计算机视觉模型在预测恐惧评分方面的潜力，强调了模型可解释性和足够数据集规模对于开发有效的情感感知治疗技术的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in computer vision have opened new avenues for clinicalapplications, particularly in computerized exposure therapy where visualstimuli can be dynamically adjusted based on patient responses. As a criticalstep toward such adaptive systems, we investigated whether pretrained computervision models can accurately predict fear levels from spider-related images. Weadapted three diverse models using transfer learning to predict human fearratings (on a 0-100 scale) from a standardized dataset of 313 images. Themodels were evaluated using cross-validation, achieving an average meanabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysisrevealed that reducing the dataset size significantly harmed performance,though further increases yielded no substantial gains. Explainabilityassessments showed the models' predictions were based on spider-relatedfeatures. A category-wise error analysis further identified visual conditionsassociated with higher errors (e.g., distant views and artificial/paintedspiders). These findings demonstrate the potential of explainable computervision models in predicting fear ratings, highlighting the importance of bothmodel explainability and a sufficient dataset size for developing effectiveemotion-aware therapeutic technologies.</description>
      <author>example@mail.com (Dominik Pegler, David Steyrl, Mengfan Zhang, Alexander Karner, Jozsef Arato, Frank Scharnowski, Filip Melinscak)</author>
      <guid isPermaLink="false">2509.04889v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
      <link>http://arxiv.org/abs/2509.05198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for presentation at the 7th  International Conference on Pattern Recognition and Image Analysis (IPRIA  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了ModelNet-R（改进版ModelNet40数据集）和Point-SkipNet（轻量级基于图的神经网络），用于解决3D点云分类问题，强调了高质量数据集对模型效率的重要性。&lt;h4&gt;背景&lt;/h4&gt;3D点云分类对自动驾驶、机器人和增强现实等应用至关重要，但常用的ModelNet40数据集存在标签不一致、二维数据、尺寸不匹配和类别区分不足等局限性，限制了模型性能。&lt;h4&gt;目的&lt;/h4&gt;创建更可靠的基准数据集ModelNet-R以解决ModelNet40的局限性，并提出Point-SkipNet神经网络实现高分类精度同时减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;通过改进ModelNet40创建ModelNet-R数据集，并设计Point-SkipNet神经网络，该网络利用高效采样、邻域分组和跳跃连接技术。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet-R上训练的模型表现出显著性能提升；Point-SkipNet在ModelNet-R上实现最先进准确性，同时参数数量大幅减少；数据集质量对优化3D点云分类模型效率至关重要。&lt;h4&gt;结论&lt;/h4&gt;高质量数据集和精心设计的网络架构能显著提高3D点云分类的性能和效率，为相关领域提供更可靠的基准和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D点云分类对于自动驾驶、机器人和增强现实等应用至关重要。然而，常用的ModelNet40数据集存在标签不一致、二维数据、尺寸不匹配和类别区分不足等局限性，这些限制阻碍了模型性能。本文介绍了ModelNet-R，这是ModelNet40的精心改进版本，旨在解决这些问题并作为更可靠的基准。此外，本文提出了Point-SkipNet，一种轻量级基于图的神经网络，它利用高效采样、邻域分组和跳跃连接来实现高分类精度同时减少计算开销。大量实验证明，在ModelNet-R上训练的模型表现出显著的性能提升。值得注意的是，Point-SkipNet在ModelNet-R上实现了最先进的准确性，同时与当代模型相比参数数量大幅减少。这项研究强调了数据集质量在优化3D点云分类模型效率方面的关键作用。更多详情，请参阅代码：https://github.com/m-saeid/ModeNetR_PointSkipNet。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：一是常用的ModelNet40数据集存在标签不一致、包含2D数据、尺寸不匹配和类别区分度不足等质量问题；二是现有的点云分类模型计算量大，不适合资源受限环境。这些问题很重要，因为数据集质量直接影响模型性能和评估可靠性，而计算效率限制模型在实际应用中的部署，而3D点云分类在自动驾驶、机器人和增强现实等领域有广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出3D点云分类领域的两个主要问题：数据集质量和模型效率。针对数据集问题，作者决定改进ModelNet数据集创建ModelNet-R；针对效率问题，设计了轻量级的Point-SkipNet。作者借鉴了PointNet和PointNet++的分层特征提取策略，DGCNN的动态图构建方法，以及FPS采样和ball query邻域分组等现有技术，但进行了改进以提高效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1) 创建高质量的ModelNet-R数据集，通过修正标签、移除低质量样本和改进类别定义提高数据可靠性；2) 设计Point-SkipNet轻量级图神经网络，通过高效采样、邻域分组和跳跃连接实现高精度低计算开销。整体流程：ModelNet-R创建包括修正标签、移除2D数据、解决尺寸不匹配和改进类别区分；Point-SkipNet实现包括数据增强、采样分组、特征提取、跳跃连接、全局特征聚合和分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) ModelNet-R数据集解决原始数据集的质量问题；2) Point-SkipNet轻量级图神经网络架构；3) 实现高精度与低计算开销的平衡。不同之处：不同于以往仅扩展数据集，作者专注于改进现有数据集质量；Point-SkipNet结合高效采样和跳跃连接大幅减少计算量；在保持精度的同时参数数量远少于当代模型；强调了数据集质量对模型效率的重要性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建高质量的ModelNet-R数据集和设计轻量级的Point-SkipNet网络，显著提高了3D点云分类的准确性和计算效率，为资源受限环境中的应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The classification of 3D point clouds is crucial for applications such asautonomous driving, robotics, and augmented reality. However, the commonly usedModelNet40 dataset suffers from limitations such as inconsistent labeling, 2Ddata, size mismatches, and inadequate class differentiation, which hinder modelperformance. This paper introduces ModelNet-R, a meticulously refined versionof ModelNet40 designed to address these issues and serve as a more reliablebenchmark. Additionally, this paper proposes Point-SkipNet, a lightweightgraph-based neural network that leverages efficient sampling, neighborhoodgrouping, and skip connections to achieve high classification accuracy withreduced computational overhead. Extensive experiments demonstrate that modelstrained in ModelNet-R exhibit significant performance improvements. Notably,Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with asubstantially lower parameter count compared to contemporary models. Thisresearch highlights the crucial role of dataset quality in optimizing modelefficiency for 3D point cloud classification. For more details, see the codeat: https://github.com/m-saeid/ModeNetR_PointSkipNet.</description>
      <author>example@mail.com (Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari)</author>
      <guid isPermaLink="false">2509.05198v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement</title>
      <link>http://arxiv.org/abs/2509.04645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025  (https://planning-from-point-clouds.github.io/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SPOT的混合学习与规划方法，用于解决机器人操作中的长期规划问题，通过在点云空间中搜索变换序列，避免了传统方法中对连续状态和动作空间的离散化需求。&lt;h4&gt;背景&lt;/h4&gt;机器人操作的长期规划是一个具有挑战性的问题，需要推理一系列动作对物理3D场景的影响。传统任务规划方法虽然有效，但需要将连续的状态和动作空间离散化为对象、对象关系和动作的符号描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维连续动作空间的规划方法，避免对状态和动作空间进行离散化，同时保持规划的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出SPOT（Search over Point cloud Object Transformations）方法，通过搜索从初始场景点云到满足目标的点云的变换序列来进行规划。SPOT从在部分观测点云上运行的学习型建议器中采样候选动作，消除了离散化动作或对象关系的需要。&lt;h4&gt;主要发现&lt;/h4&gt;在多对象重排任务上评估SPOT，实验表明SPOT能够生成成功的计划，并且优于策略学习方法。消融实验强调了基于搜索的规划的重要性。&lt;h4&gt;结论&lt;/h4&gt;SPOT作为一种混合学习与规划的方法，有效解决了长期机器人操作规划问题，在模拟和真实环境中都表现出色，证明了结合学习模型与搜索规划的有效性。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作的长期规划是一个具有挑战性的问题，需要推理一系列动作对物理3D场景的影响。虽然传统的任务规划方法被证明对长期操作有效，但它们需要将连续的状态和动作空间离散化为对象、对象关系和动作的符号描述。相反，我们提出了一种混合学习和规划的方法，利用学习到的模型作为领域特定的先验知识，在高维连续动作空间中引导搜索。我们介绍了SPOT：点云对象变换搜索，它通过搜索从初始场景点云到满足目标的点云的变换序列来进行规划。SPOT从在部分观测点云上运行的学习型建议器中采样候选动作，消除了离散化动作或对象关系的需要。我们在多对象重排任务上评估了SPOT，报告了在模拟和真实环境中的任务规划成功率和任务执行成功率。我们的实验表明，SPOT能够生成成功的计划，并且优于一种策略学习方法。我们还进行了消融实验，强调了基于搜索的规划的重要性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作中的长期规划问题，特别是在多物体重新排列任务中，机器人需要推理一系列动作对3D物理场景的影响，将物体从初始配置移动到满足目标条件的配置。这个问题在现实中很重要，因为它涉及机器人如何处理复杂的物理操作任务，如餐桌收拾、物体装箱等；在研究中也很重要，因为它突破了传统规划方法需要离散化连续状态和动作空间的限制，使机器人能够更自然地处理现实世界中的连续操作问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统符号规划方法的局限性，即需要离散化连续状态和动作空间，以及对场景的完全知识需求。然后提出了一种混合学习和规划的方法，利用学习的模型作为领域特定先验来指导搜索。作者借鉴了A*搜索算法，但将其应用于连续状态和动作空间；利用了学习的物体建议器和放置建议器来指导搜索；使用了模型偏差估计器来避免不太可能的转换；并参考了TAXPose-D方法来实现相对放置任务。整体设计考虑了直接从点云观测进行规划，而不需要符号表示或潜在空间表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从点云观测进行规划，通过搜索物体变换序列来找到满足目标条件的点云配置，利用学习的模型作为领域特定先验来指导在高维连续动作空间中的搜索。整体流程包括：1)接收初始场景的部分观测点云、物体名称和目标函数；2)将输入点云分割为有限个物体；3)使用A*搜索算法在物体变换空间中搜索，通过学习的物体建议器和放置建议器采样候选动作；4)使用成本函数和启发式函数评估和引导搜索；5)生成物体变换序列的计划；6)机器人执行计划，使用抓取检测器确定抓取姿态；7)利用模型偏差估计器预测计划动作与实际执行之间的偏差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出新的规划范式，不需要离散化，通过A*搜索原始3D观测的3D变换进行规划；2)学习物体建议器和放置建议器从视频演示中学习；3)结合搜索基础规划和学习，利用学习到的领域特定先验；4)集成模型偏差估计器引导搜索。相比之前的工作，不同之处在于：不需要离散化连续状态和动作空间；不需要符号场景描述；不在潜在空间中规划；不假设访问技能库或谓词库；不假设离散的物体关系集合；专注于多物体重新排列的规划而非单物体姿态重新配置。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPOT是一种混合学习和规划方法，它通过直接从点云观测进行搜索，利用学习的领域特定先验来指导连续动作空间中的多物体重新排列规划，避免了传统方法中对状态和动作空间离散化的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon planning for robot manipulation is a challenging problem thatrequires reasoning about the effects of a sequence of actions on a physical 3Dscene. While traditional task planning methods are shown to be effective forlong-horizon manipulation, they require discretizing the continuous state andaction space into symbolic descriptions of objects, object relationships, andactions. Instead, we propose a hybrid learning-and-planning approach thatleverages learned models as domain-specific priors to guide search inhigh-dimensional continuous action spaces. We introduce SPOT: Search over Pointcloud Object Transformations, which plans by searching for a sequence oftransformations from an initial scene point cloud to a goal-satisfying pointcloud. SPOT samples candidate actions from learned suggesters that operate onpartially observed point clouds, eliminating the need to discretize actions orobject relationships. We evaluate SPOT on multi-object rearrangement tasks,reporting task planning success and task execution success in both simulationand real-world environments. Our experiments show that SPOT generatessuccessful plans and outperforms a policy-learning approach. We also performablations that highlight the importance of search-based planning.</description>
      <author>example@mail.com (Kallol Saha, Amber Li, Angela Rodriguez-Izquierdo, Lifan Yu, Ben Eisner, Maxim Likhachev, David Held)</author>
      <guid isPermaLink="false">2509.04645v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset</title>
      <link>http://arxiv.org/abs/2509.04449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChronoGraph是一个基于真实生产微服务构建的图结构多元时间序列预测数据集，包含服务节点和依赖边，提供系统性能指标和异常标签&lt;h4&gt;背景&lt;/h4&gt;现有时间序列预测基准多来自工业控制系统、交通或空气质量领域，缺乏结合多元时间序列、依赖图结构和真实异常标签的微服务数据集&lt;h4&gt;目的&lt;/h4&gt;提供一个真实的基准数据集，用于研究微服务系统中结构感知预测和事件感知评估&lt;h4&gt;方法&lt;/h4&gt;构建图结构数据集，节点代表微服务并发出多元性能指标流，边表示服务依赖，同时提供专家标注的事故窗口作为异常标签&lt;h4&gt;主要发现&lt;/h4&gt;ChronoGraph独特地结合了多元时间序列、明确的机器可读依赖图和与真实事件对齐的异常标签，为微服务系统研究提供了更全面的基准&lt;h4&gt;结论&lt;/h4&gt;ChronoGraph为研究结构感知预测和事件感知评估提供了现实基准，报告了预测模型、预训练时间序列基础模型和标准异常检测器的基线结果&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ChronoGraph，一个基于真实生产微服务构建的图结构多元时间序列预测数据集。每个节点是一个发出系统级性能指标多元流的服务，捕获CPU、内存和网络使用模式，而有向边编码服务之间的依赖关系。主要任务是预测这些信号在服务级别的未来值。此外，ChronoGraph提供专家注释的事故窗口作为异常标签，使能够评估异常检测方法并在操作中断期间评估预测鲁棒性。与来自工业控制系统或交通和空气质量领域的现有基准相比，ChronoGraph独特地结合了(i)多元时间序列，(ii)明确的、机器可读的依赖图，以及(iii)与真实事件对齐的异常标签。我们报告了涵盖预测模型、预训练时间序列基础模型和标准异常检测器的基线结果。ChronoGraph为研究微服务系统中的结构感知预测和事件感知评估提供了现实基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ChronoGraph, a graph-structured multivariate time seriesforecasting dataset built from real-world production microservices. Each nodeis a service that emits a multivariate stream of system-level performancemetrics, capturing CPU, memory, and network usage patterns, while directededges encode dependencies between services. The primary task is forecastingfuture values of these signals at the service level. In addition, ChronoGraphprovides expert-annotated incident windows as anomaly labels, enablingevaluation of anomaly detection methods and assessment of forecast robustnessduring operational disruptions. Compared to existing benchmarks from industrialcontrol systems or traffic and air-quality domains, ChronoGraph uniquelycombines (i) multivariate time series, (ii) an explicit, machine-readabledependency graph, and (iii) anomaly labels aligned with real incidents. Wereport baseline results spanning forecasting models, pretrained time-seriesfoundation models, and standard anomaly detectors. ChronoGraph offers arealistic benchmark for studying structure-aware forecasting and incident-awareevaluation in microservice systems.</description>
      <author>example@mail.com (Adrian Catalin Lutu, Ioana Pintilie, Elena Burceanu, Andrei Manolache)</author>
      <guid isPermaLink="false">2509.04449v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
  <item>
      <title>IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation</title>
      <link>http://arxiv.org/abs/2509.04398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IPA的特征感知投影框架，用于改进参数高效微调方法LoRA的性能，通过解决LoRA随机初始化下投影导致的信息丢失问题。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)方法如LoRA通过向预训练权重注入低秩更新来降低适应成本，但LoRA的下投影是随机初始化且与数据无关，丢弃了可能有用的信息。分析表明下投影在训练中变化很小，而上投影携带大部分适应信息，使随机输入压缩成为性能瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出IPA框架，明确保留简化隐藏空间中的有用信息，解决LoRA方法中随机下投影导致的信息丢失问题。&lt;h4&gt;方法&lt;/h4&gt;在线性情况下，使用近似主成分的算法实现IPA，使投影器预训练高效且推理开销可以忽略不计。&lt;h4&gt;主要发现&lt;/h4&gt;在语言和视觉基准测试中，IPA持续优于LoRA和DoRA，在常识推理上平均提高1.5个准确率点，在VTAB-1k上提高2.3个准确率点。当投影被冻结时，使用大约一半的可训练参数即可匹配完整LoRA的性能。&lt;h4&gt;结论&lt;/h4&gt;IPA是一种有效的改进方法，能够在保持或提高性能的同时减少可训练参数，解决了LoRA方法中的信息丢失瓶颈问题。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)方法，如LoRA，通过向预训练权重注入低秩更新来降低适应成本。然而，LoRA的下投影是随机初始化且与数据无关，丢弃了可能有用的信息。先前的分析表明，这个投影在训练过程中变化很小，而上投影携带了大部分适应信息，使随机输入压缩成为性能瓶颈。我们提出了IPA，一种特征感知的投影框架，明确保留简化隐藏空间中的信息。在线性情况下，我们使用近似主成分的算法实现IPA，使投影器预训练高效且推理开销可以忽略不计。在语言和视觉基准测试中，IPA持续优于LoRA和DoRA，在常识推理上平均提高1.5个准确率点，在VTAB-1k上提高2.3个准确率点，当投影被冻结时，使用大约一半的可训练参数即可匹配完整LoRA的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduceadaptation cost by injecting low-rank updates into pretrained weights. However,LoRA's down-projection is randomly initialized and data-agnostic, discardingpotentially useful information. Prior analyses show that this projectionchanges little during training, while the up-projection carries most of theadaptation, making the random input compression a performance bottleneck. Wepropose IPA, a feature-aware projection framework that explicitly preservesinformation in the reduced hidden space. In the linear case, we instantiate IPAwith algorithms approximating top principal components, enabling efficientprojector pretraining with negligible inference overhead. Across language andvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving onaverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points onVTAB-1k, while matching full LoRA performance with roughly half the trainableparameters when the projection is frozen.</description>
      <author>example@mail.com (Yuan Yin, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Matthieu Cord)</author>
      <guid isPermaLink="false">2509.04398v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</title>
      <link>http://arxiv.org/abs/2509.04324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OVGrasp是一个基于软外骨骼的抓取辅助分层控制框架，通过整合RGB-D视觉、开放词汇提示和语音命令实现多模态交互，使用视觉语言基础模型实现零样本检测未见物体，并通过多模态决策者融合空间和语言线索推断用户意图，实验表明其达到87.00%的抓取能力分数，优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;抓取辅助对于运动障碍人士恢复自主性至关重要，特别是在物体类别和用户意图多样且不可预测的非结构化环境中。&lt;h4&gt;目的&lt;/h4&gt;提出OVGrasp框架，用于基于软外骨骼的抓取辅助，以增强在开放环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;OVGrasp是一个分层控制框架，集成了RGB-D视觉、开放词汇提示和语音命令；采用具有开放词汇机制的视觉语言基础模型实现零样本检测；通过多模态决策者融合空间和语言线索来推断用户意图；在自为中心视角的可穿戴外骨骼上部署完整框架，并在三种抓取类型下的15个物体上进行系统评估。&lt;h4&gt;主要发现&lt;/h4&gt;10名参与者的实验表明，OVGrasp达到87.00%的抓取能力分数(GAS)，优于最先进的基线，并实现了与自然手部运动更好的运动对齐。&lt;h4&gt;结论&lt;/h4&gt;OVGrasp能够有效地帮助运动障碍人士在非结构化环境中进行抓取任务，恢复其自主性。&lt;h4&gt;翻译&lt;/h4&gt;抓取辅助对于恢复运动障碍人士的自主性至关重要，特别是在物体类别和用户意图多样且不可预测的非结构化环境中。我们提出了OVGrasp，一个基于软外骨骼的抓取辅助分层控制框架，整合了RGB-D视觉、开放词汇提示和语音命令，以实现强大的多模态交互。为了增强开放环境中的泛化能力，OVGrasp采用了具有开放词汇机制的视觉语言基础模型，允许在不重新训练的情况下对未见过的物体进行零样本检测。多模态决策者进一步融合空间和语言线索，在多物体场景中推断用户意图，如抓取或释放。我们在自定制为中心视角的可穿戴外骨骼上部署了完整框架，并在三种抓取类型下的15个物体上进行了系统评估。十名参与者的实验结果表明，OVGrasp实现了87.00%的抓取能力分数(GAS)，优于最先进的基线，并实现了与自然手部运动更好的运动对齐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasping assistance is essential for restoring autonomy in individuals withmotor impairments, particularly in unstructured environments where objectcategories and user intentions are diverse and unpredictable. We presentOVGrasp, a hierarchical control framework for soft exoskeleton-based graspassistance that integrates RGB-D vision, open-vocabulary prompts, and voicecommands to enable robust multimodal interaction. To enhance generalization inopen environments, OVGrasp incorporates a vision-language foundation model withan open-vocabulary mechanism, allowing zero-shot detection of previously unseenobjects without retraining. A multimodal decision-maker further fuses spatialand linguistic cues to infer user intent, such as grasp or release, inmulti-object scenarios. We deploy the complete framework on a customegocentric-view wearable exoskeleton and conduct systematic evaluations on 15objects across three grasp types. Experimental results with ten participantsdemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,outperforming state-of-the-art baselines and achieving improved kinematicalignment with natural hand motion.</description>
      <author>example@mail.com (Chen Hu, Shan Luo, Letizia Gionfrida)</author>
      <guid isPermaLink="false">2509.04324v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>RL's Razor: Why Online Reinforcement Learning Forgets Less</title>
      <link>http://arxiv.org/abs/2509.04259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了通过强化学习(RL)和监督微调(SFT)微调模型的差异，发现RL在保持先验知识方面优于SFT，尽管两者在新任务上表现相似。&lt;h4&gt;背景&lt;/h4&gt;模型微调过程中可能会遗忘原有知识，这种现象在机器学习中被称为'灾难性遗忘'。&lt;h4&gt;目的&lt;/h4&gt;探究不同微调方法(RL vs SFT)对模型保留先验知识能力的影响机制。&lt;h4&gt;方法&lt;/h4&gt;通过比较RL和SFT两种微调方法，分析KL散度作为分布偏移的度量标准，并通过大型语言模型和机器人基础模型进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;RL微调倾向于选择KL最小的解决方案，而SFT可能收敛到与基础模型相差很远的分布；遗忘程度由KL散度决定；在线RL更新导致KL变化更小。&lt;h4&gt;结论&lt;/h4&gt;RL在新任务性能相似的情况下能更好地保留先验知识，这种现象可以用'RL's Razor'原则解释：RL倾向于选择与原始模型KL距离最近的任务解决方法。&lt;h4&gt;翻译&lt;/h4&gt;通过将强化学习(RL)模型与监督微调(SFT)模型进行比较，我们发现，尽管在新任务上性能相似，但RL能显著更好地保留先验知识和能力。我们发现遗忘程度由分布偏移决定，通过在新任务上评估的微调后策略与基础策略之间的KL散度来衡量。我们的分析表明，在线RL隐式地偏向于选择新任务解决方案中KL最小的解，而SFT可能收敛到与基础模型任意距离的分布。我们通过大型语言模型和机器人基础模型的实验验证了这些发现，并进一步提供了理论解释，说明为什么在线RL更新会导致更小的KL变化。我们将这一原则称为'RL's Razor'：在解决新任务的所有方法中，RL倾向于选择与原始模型KL距离最近的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comparison of fine-tuning models with reinforcement learning (RL) andsupervised fine-tuning (SFT) reveals that, despite similar performance at a newtask, RL preserves prior knowledge and capabilities significantly better. Wefind that the degree of forgetting is determined by the distributional shift,measured as the KL-divergence between the fine-tuned and base policy evaluatedon the new task. Our analysis reveals that on-policy RL is implicitly biasedtowards KL-minimal solutions among the many that solve the new task, whereasSFT can converge to distributions arbitrarily far from the base model. Wevalidate these findings through experiments with large language models androbotic foundation models and further provide theoretical justification for whyon-policy RL updates lead to a smaller KL change. We term this principle$\textit{RL's Razor}$: among all ways to solve a new task, RL prefers thoseclosest in KL to the original model.</description>
      <author>example@mail.com (Idan Shenfeld, Jyothish Pari, Pulkit Agrawal)</author>
      <guid isPermaLink="false">2509.04259v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Sailing Towards Zero-Shot State Estimation using Foundation Models Combined with a UKF</title>
      <link>http://arxiv.org/abs/2509.04213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at CDC2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FM-UKF的基础模型无迹卡尔曼滤波器，结合基于transformer的系统动力学模型与分析已知的传感器模型，实现了零样本状态估计，能够在不重新训练的情况下适应不同传感器配置。&lt;h4&gt;背景&lt;/h4&gt;在控制和系统工程中，状态估计传统上需要大量手动系统识别或数据收集工作。基于transformer的基础模型在其他领域已通过预训练通用模型减少数据需求，但现有端到端方法仅限于训练期间见过的传感器模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种零样本状态估计方法，能够处理未见过的系统动力学，同时适应不同传感器配置而不需重新训练。&lt;h4&gt;方法&lt;/h4&gt;提出FM-UKF，将基于transformer的系统动力学模型与分析已知的传感器模型通过UKF结合，实现跨不同动力学系统的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在具有复杂动力学的集装箱船舶模型新基准上评估，FM-UKF与经典方法和端到端方法相比，在准确性、工作量和鲁棒性方面具有竞争优势。&lt;h4&gt;结论&lt;/h4&gt;FM-UKF有效结合基础模型和已知传感器模型实现零样本状态估计，相关基准和数据集已开源以支持未来研究。&lt;h4&gt;翻译&lt;/h4&gt;控制和系统工程中的状态估计传统上需要大量的手动系统识别或数据收集工作。然而，其他领域中基于transformer的基础模型通过利用预训练的通用模型减少了数据需求。最终，开发系统动力学的零样本基础模型可以显著减少手动部署工作。虽然最近的研究表明，基于transformer的端到端方法可以在未见过的系统上实现零样本性能，但它们仅限于训练期间见过的传感器模型。我们引入了基础模型无迹卡尔曼滤波器（FM-UKF），它将基于transformer的系统动力学模型与分析已知的传感器模型通过UKF结合，使得能够在不重新训练新传感器配置的情况下推广到变化的动力学系统。我们在具有复杂动力学的集装箱船舶模型新基准上评估了FM-UKF，与具有近似系统知识的经典方法和端到端方法相比，展示了竞争力的准确性、工作量和鲁棒性权衡。该基准和数据集已开源，以进一步支持通过基础模型进行零样本状态估计的未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State estimation in control and systems engineering traditionally requiresextensive manual system identification or data-collection effort. However,transformer-based foundation models in other domains have reduced datarequirements by leveraging pre-trained generalist models. Ultimately,developing zero-shot foundation models of system dynamics could drasticallyreduce manual deployment effort. While recent work shows that transformer-basedend-to-end approaches can achieve zero-shot performance on unseen systems, theyare limited to sensor models seen during training. We introduce the foundationmodel unscented Kalman filter (FM-UKF), which combines a transformer-basedmodel of system dynamics with analytically known sensor models via an UKF,enabling generalization across varying dynamics without retraining for newsensor configurations. We evaluate FM-UKF on a new benchmark of container shipmodels with complex dynamics, demonstrating a competitive accuracy, effort, androbustness trade-off compared to classical methods with approximate systemknowledge and to an end-to-end approach. The benchmark and dataset are opensourced to further support future research in zero-shot state estimation viafoundation models.</description>
      <author>example@mail.com (Tobin Holtmann, David Stenger, Andres Posada-Moreno, Friedrich Solowjow, Sebastian Trimpe)</author>
      <guid isPermaLink="false">2509.04213v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>One-Embedding-Fits-All: Efficient Zero-Shot Time Series Forecasting by a Model Zoo</title>
      <link>http://arxiv.org/abs/2509.04208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZooCast是一种智能组合时间序列基础模型(TSFMs)的框架，通过构建统一表示空间实现动态模型选择，在零样本预测任务中表现出色同时保持高效性。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)的快速发展显著推动了零样本预测能力，使模型能够在无需任务特定微调的情况下预测未见的时间序列。研究表明，没有单一TSFM能够普遍优于其他模型，不同模型表现出对不同时间模式的偏好。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用不同TSFM的互补能力，构建一个能够智能组合当前TSFMs并根据任务特点选择最优模型的框架。&lt;h4&gt;方法&lt;/h4&gt;提出ZooCast方法，表征每个模型的独特预测优势；采用'One-Embedding-Fits-All'范式构建统一表示空间，使每个模型由单个嵌入表示，实现高效相似性匹配；建立动态模型选择机制，根据不同预测任务选择最优模型。&lt;h4&gt;主要发现&lt;/h4&gt;ZooCast在GIFT-Eval零样本预测基准上表现出强大性能，同时保持了单个TSFM的效率；在连续模型发布的实际场景中，框架可以无缝添加新模型以实现渐进式精度提升，且开销可忽略不计。&lt;h4&gt;结论&lt;/h4&gt;ZooCast通过智能组合不同TSFMs的优势，有效提高了零样本预测的性能和效率，为时间序列预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)的激增显著推动了零样本预测的发展，使模型能够对未见的时间序列进行预测而无需任务特定的微调。大量研究证实，没有任何单一的TSFM能够普遍优于其他模型，因为不同模型表现出对不同时间模式的偏好。这种多样性表明存在一个机会：如何利用TSFM的互补能力。为此，我们提出了ZooCast，该方法表征每个模型的独特预测优势。ZooCast能够智能地将当前的TSFMs组装成一个模型库，动态选择不同预测任务的最优模型。我们的关键创新在于'一种嵌入适配所有'的范式，该范式构建了一个统一的表示空间，其中库中的每个模型由单个嵌入表示，从而能够为所有任务实现高效的相似性匹配。实验证明，ZooCast在GIFT-Eval零样本预测基准上表现出强大的性能，同时保持了单个TSFM的效率。在连续模型发布的实际场景中，该框架可以无缝添加新模型以实现渐进式精度提升，且开销可忽略不计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Time Series Foundation Models (TSFMs) has significantlyadvanced zero-shot forecasting, enabling predictions for unseen time serieswithout task-specific fine-tuning. Extensive research has confirmed that nosingle TSFM excels universally, as different models exhibit preferences fordistinct temporal patterns. This diversity suggests an opportunity: how to takeadvantage of the complementary abilities of TSFMs. To this end, we proposeZooCast, which characterizes each model's distinct forecasting strengths.ZooCast can intelligently assemble current TSFMs into a model zoo thatdynamically selects optimal models for different forecasting tasks. Our keyinnovation lies in the One-Embedding-Fits-All paradigm that constructs aunified representation space where each model in the zoo is represented by asingle embedding, enabling efficient similarity matching for all tasks.Experiments demonstrate ZooCast's strong performance on the GIFT-Eval zero-shotforecasting benchmark while maintaining the efficiency of a single TSFM. Inreal-world scenarios with sequential model releases, the framework seamlesslyadds new models for progressive accuracy gains with negligible overhead.</description>
      <author>example@mail.com (Hao-Nan Shi, Ting-Ji Huang, Lu Han, De-Chuan Zhan, Han-Jia Ye)</author>
      <guid isPermaLink="false">2509.04208v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</title>
      <link>http://arxiv.org/abs/2509.04180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VisioFirm是一个开源的Web应用程序，通过AI辅助自动化简化图像标注流程，可减少最多90%的手动工作量，同时保持高标注准确性。&lt;h4&gt;背景&lt;/h4&gt;AI模型依赖标注数据学习模式和执行预测，传统标注工具劳动密集且难以扩展到大型数据集，需要处理从简单分类标签到复杂任务如目标检测、方向边界框估计和实例分割等不同类型的标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个AI辅助的图像标注工具，减少人工输入需求，提高标注效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;VisioFirm集成基础模型到界面中，采用CLIP结合预训练检测器（如Ultralytics）处理常见类别，使用零样本模型（如GroundingDINO）处理自定义标签，通过低置信度阈值最大化召回率生成初始标注，并提供交互式工具进行完善，同时利用WebGPU加速Segment Anything实现实时分割。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO类型类别测试中，初始预测大多正确；支持多种导出格式（YOLO、COCO、Pascal VOC、CSV）；模型缓存后可离线运行；通过基于CLIP的连接组件聚类和IoU图进行冗余检测抑制。&lt;h4&gt;结论&lt;/h4&gt;VisioFirm显著降低了手动标注工作量，同时保持了高标注准确性，提高了标注流程的可扩展性和可访问性。&lt;h4&gt;翻译&lt;/h4&gt;AI模型依赖于标注数据来学习模式和执行预测。标注通常是一个劳动密集型的步骤，需要将标签从简单的分类标签到更复杂的任务（如目标检测、方向边界框估计和实例分割）相关联。传统工具通常需要大量手动输入，限制了大型数据集的可扩展性。为了解决这个问题，我们引入了VisioFirm，一个开源的Web应用程序，旨在通过AI辅助自动化简化图像标注流程。VisioFirm将最先进的基础模型集成到一个带有过滤管道的界面中，以减少人在环路中的工作量。这种混合方法采用CLIP结合预训练检测器（如Ultralytics模型）用于常见类别，以及零样本模型（如GroundingDINO）用于自定义标签，通过低置信度阈值最大化召回率来生成初始标注。通过这个框架，在COCO类型类别的测试中，初始预测已被证明大多是正确的，用户可以通过支持边界框、方向边界框和多边形的交互式工具进行完善。此外，VisioFirm还具有由Segment Anything驱动的实时分割功能，通过WebGPU加速以提高浏览器端的效率。该工具支持多种导出格式（YOLO、COCO、Pascal VOC、CSV），并在模型缓存后可离线运行，提高了可访问性。VisioFirm通过在不同数据集上的基准测试，展示了最多可减少90%的手动工作量，同时通过基于CLIP的连接组件聚类和IoU图进行冗余检测抑制，保持高标注准确性。VisioFirm可以从https://github.com/OschAI/VisioFirm访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI models rely on annotated data to learn pattern and perform prediction.Annotation is usually a labor-intensive step that require associating labelsranging from a simple classification label to more complex tasks such as objectdetection, oriented bounding box estimation, and instance segmentation.Traditional tools often require extensive manual input, limiting scalabilityfor large datasets. To address this, we introduce VisioFirm, an open-source webapplication designed to streamline image labeling through AI-assistedautomation. VisioFirm integrates state-of-the-art foundation models into aninterface with a filtering pipeline to reduce human-in-the-loop efforts. Thishybrid approach employs CLIP combined with pre-trained detectors likeUltralytics models for common classes and zero-shot models such as GroundingDINO for custom labels, generating initial annotations with low-confidencethresholding to maximize recall. Through this framework, when tested onCOCO-type of classes, initial prediction have been proven to be mostly correctthough the users can refine these via interactive tools supporting boundingboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm hason-the-fly segmentation powered by Segment Anything accelerated through WebGPUfor browser-side efficiency. The tool supports multiple export formats (YOLO,COCO, Pascal VOC, CSV) and operates offline after model caching, enhancingaccessibility. VisioFirm demonstrates up to 90\% reduction in manual effortthrough benchmarks on diverse datasets, while maintaining high annotationaccuracy via clustering of connected CLIP-based disambiguate components andIoU-graph for redundant detection suppression. VisioFirm can be accessed from\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.</description>
      <author>example@mail.com (Safouane El Ghazouali, Umberto Michelucci)</author>
      <guid isPermaLink="false">2509.04180v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.03906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepMedix-R1是一种用于胸部X光解释的整体医学基础模型，通过顺序训练流水线实现了透明的推理过程和本地化可解释性，在报告生成和视觉问答任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;医学基础模型在人工智能技术快速发展的背景下显示出巨大潜力，但当前医学基础模型通常以黑盒方式生成答案，缺乏透明的推理过程和本地化的可解释性，这阻碍了它们在临床实践中的实际部署。&lt;h4&gt;目的&lt;/h4&gt;介绍DeepMedix-R1，一个用于胸部X光解释的整体医学基础模型，解决医学基础模型缺乏透明推理和本地化可解释性的问题。&lt;h4&gt;方法&lt;/h4&gt;采用顺序训练流水线：首先在精选的胸部X光指令数据上进行微调，赋予基本的解释能力；然后接触高质量的合成推理样本，实现冷启动推理；最后通过在线强化学习进行优化，提高推理质量和生成性能。模型为每个查询生成答案和与图像局部区域相关的推理步骤。同时提出了Report Arena基准框架，使用先进语言模型评估答案质量。&lt;h4&gt;主要发现&lt;/h4&gt;在报告生成任务上比LLaVA-Rad和MedGemma分别提高14.54%和31.32%；在视觉问答任务上比MedGemma和CheXagent分别提高57.75%和23.06%；生成的推理步骤比Qwen2.5-VL-7B模型具有更高的可解释性和临床可行性(总体偏好0.7416 vs. 0.2584)。&lt;h4&gt;结论&lt;/h4&gt;该工作推进了医学基础模型的发展，朝着胸部X光解释的整体、透明和临床可行的建模方向发展。&lt;h4&gt;翻译&lt;/h4&gt;医学基础模型在人工智能技术快速发展的背景下显示出巨大潜力。然而，当前医学基础模型通常以黑盒方式生成答案，缺乏透明的推理过程和本地化的可解释性，这阻碍了它们在临床实践中的实际部署。为此，我们介绍了DeepMedix-R1，一个用于胸部X光解释的整体医学基础模型。它采用顺序训练流水线：首先在精选的胸部X光指令数据上进行微调，赋予基本的胸部X光解释能力；然后接触高质量的合成推理样本，实现冷启动推理；最后通过在线强化学习进行优化，提高基础推理质量和生成性能。因此，模型为每个查询生成答案和与图像局部区域相关的推理步骤。定量评估表明，在报告生成任务上(例如比LLaVA-Rad和MedGemma分别提高14.54%和31.32%)和视觉问答任务上(例如比MedGemma和CheXagent分别提高57.75%和23.06%)都有显著改进。为了促进稳健评估，我们提出了Report Arena，一个使用先进语言模型评估答案质量的基准框架，进一步凸显了DeepMedix-R1的优越性。专家对生成的推理步骤的审查显示，与已建立的Qwen2.5-VL-7B模型相比，具有更高的可解释性和临床可行性(总体偏好0.7416比0.2584)。总的来说，我们的工作推进了医学基础模型的发展，朝着胸部X光解释的整体、透明和临床可行的建模方向发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical foundation models (FMs) have shown tremendous promise amid the rapidadvancements in artificial intelligence (AI) technologies. However, currentmedical FMs typically generate answers in a black-box manner, lackingtransparent reasoning processes and locally grounded interpretability, whichhinders their practical clinical deployments. To this end, we introduceDeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. Itleverages a sequential training pipeline: initially fine-tuned on curated CXRinstruction data to equip with fundamental CXR interpretation capabilities,then exposed to high-quality synthetic reasoning samples to enable cold-startreasoning, and finally refined via online reinforcement learning to enhanceboth grounded reasoning quality and generation performance. Thus, the modelproduces both an answer and reasoning steps tied to the image's local regionsfor each query. Quantitative evaluation demonstrates substantial improvementsin report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) andvisual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)tasks. To facilitate robust assessment, we propose Report Arena, a benchmarkingframework using advanced language models to evaluate answer quality, furtherhighlighting the superiority of DeepMedix-R1. Expert review of generatedreasoning steps reveals greater interpretability and clinical plausibilitycompared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overallpreference). Collectively, our work advances medical FM development towardholistic, transparent, and clinically actionable modeling for CXRinterpretation.</description>
      <author>example@mail.com (Qika Lin, Yifan Zhu, Bin Pu, Ling Huang, Haoran Luo, Jingying Ma, Zhen Peng, Tianzhe Zhao, Fangzhi Xu, Jian Zhang, Kai He, Zhonghong Ou, Swapnil Mishra, Mengling Feng)</author>
      <guid isPermaLink="false">2509.03906v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>A Generative Foundation Model for Chest Radiography</title>
      <link>http://arxiv.org/abs/2509.03903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了ChexGen，一个专门用于胸部X光片合成的生成式视觉-语言基础模型。该模型通过文本、掩码和边界框引导的方式生成高质量的胸部X光片，并在大规模数据集上进行了预训练。研究展示了该模型在数据增强、模型训练和提升医疗AI系统公平性方面的显著效用。&lt;h4&gt;背景&lt;/h4&gt;医疗领域缺乏充分注释的多样化医学图像是开发可靠AI模型的主要障碍。尽管自然图像的生成式基础模型已取得重大技术进步，但在医疗影像领域仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的生成框架，用于胸部X光片的文本、掩码和边界框引导合成，以解决医疗AI领域数据稀缺问题，并提高模型的准确性、数据效率和公平性。&lt;h4&gt;方法&lt;/h4&gt;构建了基于潜在扩散变换器架构的ChexGen模型，在包含96万个X光片-报告对的最大精选胸部X光数据集上进行预训练。通过专家评估和定量指标验证合成图像的准确性，并利用该模型进行训练数据增强和监督预训练。&lt;h4&gt;主要发现&lt;/h4&gt;ChexGen能够生成准确的胸部X光片，通过使用少量训练数据进行数据增强和监督预训练，显著提高了疾病分类、检测和分割任务的性能。此外，该模型能够创建多样化的患者队列，通过检测和减轻人口统计学偏见来增强模型公平性。&lt;h4&gt;结论&lt;/h4&gt;生成式基础模型在构建更准确、数据效率更高和更公平的医疗AI系统中具有变革性作用，ChexGen展示了这种潜力。&lt;h4&gt;翻译&lt;/h4&gt;缺乏充分注释的多样化医学图像是开发医疗领域可靠AI模型的主要障碍。自然图像的生成式基础模型已取得重大技术进步。我们开发了ChexGen，一个生成式视觉-语言基础模型，引入了用于胸部X光片文本、掩码和边界框引导合成的统一框架。基于潜在扩散变换器架构，ChexGen在迄今为止最大的精选胸部X光数据集上进行了预训练，该数据集包含96万个X光片-报告对。ChexGen通过专家评估和定量指标实现了胸部X光片的准确合成。我们展示了ChexGen在训练数据增强和监督预训练方面的效用，这在使用少量训练数据的情况下提高了疾病分类、检测和分割任务的性能。此外，我们的模型能够创建多样化的患者队列，通过检测和减轻人口统计学偏见来增强模型公平性。我们的研究支持生成式基础模型在构建更准确、数据效率更高和更公平的医疗AI系统中的变革性作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of well-annotated diverse medical images is a major hurdle fordeveloping reliable AI models in healthcare. Substantial technical advanceshave been made in generative foundation models for natural images. Here wedevelop `ChexGen', a generative vision-language foundation model thatintroduces a unified framework for text-, mask-, and bounding box-guidedsynthesis of chest radiographs. Built upon the latent diffusion transformerarchitecture, ChexGen was pretrained on the largest curated chest X-ray datasetto date, consisting of 960,000 radiograph-report pairs. ChexGen achievesaccurate synthesis of radiographs through expert evaluations and quantitativemetrics. We demonstrate the utility of ChexGen for training data augmentationand supervised pretraining, which led to performance improvements acrossdisease classification, detection, and segmentation tasks using a smallfraction of training data. Further, our model enables the creation of diversepatient cohorts that enhance model fairness by detecting and mitigatingdemographic biases. Our study supports the transformative role of generativefoundation models in building more accurate, data-efficient, and equitablemedical AI systems.</description>
      <author>example@mail.com (Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li)</author>
      <guid isPermaLink="false">2509.03903v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning AI Foundation Models to Develop Subgrid-Scale Parameterizations: A Case Study on Atmospheric Gravity Waves</title>
      <link>http://arxiv.org/abs/2509.03816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过微调预训练AI基础模型(FM)来开发小尺度气候过程机器学习参数化的新方法，应用于大气重力波参数化，并展示了其优越的预测性能。&lt;h4&gt;背景&lt;/h4&gt;全球气候模型参数化了多种无法充分解析的大气-海洋过程（如重力波、云、湿对流和湍流），这些次网格尺度闭合方案是模型不确定性的主要来源。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的机器学习方法，通过微调预训练的AI基础模型来创建小尺度气候过程的参数化方案。&lt;h4&gt;方法&lt;/h4&gt;使用NASA和IBM研究的230亿参数Prithvi WxC FM的预训练编码器-解码器，通过微调创建大气重力波的深度学习参数化；学习比粗分辨率气候模型精细10倍的大气再分析数据中的通量；与注意力U-Net基线模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;FM参数化在整个大气层中表现出优越的预测性能，即使在排除在预训练之外的区域也是如此；Hellinger距离显示性能提升：基线为0.11，微调模型为0.06。&lt;h4&gt;结论&lt;/h4&gt;FM的多功能性和可重用性为完成多种大气和气候相关应用提供了可能，为创建更多地球系统过程的观测驱动且物理准确的参数化铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;全球气候模型参数化了多种无法充分解析的大气-海洋过程，如重力波、云、湿对流和湍流。这些针对未解析过程的次网格尺度闭合方案是模型不确定性的主要来源。在此，我们提出了一种新方法，通过微调预训练的AI基础模型(FM)来开发小尺度气候过程的机器学习参数化。FM在气候研究中 largely未被探索。我们从230亿参数的FM（NASA和IBM研究的Prithvi WxC）--包含大气演化的潜在概率表示--中预训练的编码器-解码器进行微调（或重用），以创建大气重力波(GW)的深度学习参数化。该参数化通过学习分辨率比粗分辨率气候模型精细10倍的大气再分析数据中的通量，为粗分辨率气候模型捕捉重力波效应。与机器学习基线模型（注意力U-Net）的月平均值和瞬时演化比较显示，FM参数化在整个大气层中表现出优越的预测性能，即使在排除在预训练之外的区域也是如此。使用Hellinger距离量化这一性能提升，基线为0.11，微调模型为0.06。我们的发现强调了FM的多功能性和可重用性，可用于完成多种大气和气候相关应用，为创建更多地球系统过程的观测驱动且物理准确的参数化铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global climate models parameterize a range of atmospheric-oceanic processeslike gravity waves, clouds, moist convection, and turbulence that cannot besufficiently resolved. These subgrid-scale closures for unresolved processesare a leading source of model uncertainty. Here, we present a new approach todeveloping machine learning parameterizations of small-scale climate processesby fine-tuning a pre-trained AI foundation model (FM). FMs are largelyunexplored in climate research. A pre-trained encoder-decoder from a 2.3billion parameter FM (NASA and IBM Research's Prithvi WxC) -- which contains alatent probabilistic representation of atmospheric evolution -- is fine-tuned(or reused) to create a deep learning parameterization for atmospheric gravitywaves (GWs). The parameterization captures GW effects for a coarse-resolutionclimate model by learning the fluxes from an atmospheric reanalysis with 10times finer resolution. A comparison of monthly averages and instantaneousevolution with a machine learning model baseline (an Attention U-Net) revealssuperior predictive performance of the FM parameterization throughout theatmosphere, even in regions excluded from pre-training. This performance boostis quantified using the Hellinger distance, which is 0.11 for the baseline and0.06 for the fine-tuned model. Our findings emphasize the versatility andreusability of FMs, which could be used to accomplish a range of atmosphere-and climate-related applications, leading the way for the creation ofobservations-driven and physically accurate parameterizations for moreearth-system processes.</description>
      <author>example@mail.com (Aman Gupta, Aditi Sheshadri, Sujit Roy, Johannes Schmude, Vishal Gaur, Wei Ji Leong, Manil Maskey, Rahul Ramachandran)</author>
      <guid isPermaLink="false">2509.03816v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures</title>
      <link>http://arxiv.org/abs/2509.03695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层联邦基础模型(HF-FMs)，作为多模态多任务联邦基础模型(M3T FFMs)的新变体，解决了雾/边缘网络中两个被忽视的异构性维度：模态异构性和任务异构性。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)的出现重塑了机器学习领域。随着模型规模增长，利用无线设备的地理分布式数据变得至关重要，催生了联邦基础模型(FFMs)。近期，FMs已发展为多模态多任务(M3T) FMs(如GPT-4)，能够处理跨多种任务的多样模态，这促使了M3T FFMs这一新范式的出现。&lt;h4&gt;目的&lt;/h4&gt;提出分层联邦基础模型(HF-FMs)，揭示雾/边缘网络中两个影响新兴模型的异构性维度：(i)收集模态的异构性；(ii)执行任务的异构性。&lt;h4&gt;方法&lt;/h4&gt;HF-FMs将M3T FMs的模块化结构(模态编码器、提示、专家混合、适配器和任务头)与雾/边缘基础设施的层次结构对齐。支持可选的设备到设备(D2D)通信，实现节点间的水平模块中继和本地协作训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入研究HF-FMs的架构设计，强调了其独特能力和一系列定制化的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;在无线网络环境中构建了HF-FMs原型，并发布了开源代码，旨在促进这一未被充分探索领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)的兴起重塑了机器学习格局。随着这些模型持续增长，利用无线设备中的地理分布式数据变得越来越重要，催生了联邦基础模型(FFMs)。最近，FMs已发展为多模态多任务(M3T) FMs(如GPT-4)，能够处理跨多个任务的多种模态，这促使了一种新的未被充分探索的范式：M3T FFMs。在本文中，我们通过提出分层联邦基础模型(HF-FMs)，揭示了M3T FFMs的一个未被探索的变体，这反过来又暴露了影响这些新兴模型的雾/边缘网络中被忽视的两个异构性维度：(i)收集模态的异构性；(ii)雾/边缘节点上执行任务的异构性。HF-FMs将M3T FMs的模块化结构(包括模态编码器、提示、专家混合、适配器和任务头)与雾/边缘基础设施的层次性质战略性对齐。此外，HF-FMs支持可选的设备到设备(D2D)通信，使节点间能够在可行时进行水平模块中继和本地协作训练。通过深入研究HF-FMs的架构设计，我们强调了它们的独特能力以及一系列定制化的未来研究方向。最后，为了展示它们的潜力，我们在无线网络环境中构建了HF-FMs的原型，并发布了用于开发HF-FMs的开源代码，旨在促进这一未被充分探索领域的研究(GitHub: https://github.com/payamsiabd/M3T-FFM)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of foundation models (FMs) has reshaped the landscape of machinelearning. As these models continued to grow, leveraging geo-distributed datafrom wireless devices has become increasingly critical, giving rise tofederated foundation models (FFMs). More recently, FMs have evolved intomulti-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diversemodalities across multiple tasks, which motivates a new underexplored paradigm:M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs byproposing hierarchical federated foundation models (HF-FMs), which in turnexpose two overlooked heterogeneity dimensions to fog/edge networks that have adirect impact on these emerging models: (i) heterogeneity in collectedmodalities and (ii) heterogeneity in executed tasks across fog/edge nodes.HF-FMs strategically align the modular structure of M3T FMs, comprisingmodality encoders, prompts, mixture-of-experts (MoEs), adapters, and taskheads, with the hierarchical nature of fog/edge infrastructures. Moreover,HF-FMs enable the optional usage of device-to-device (D2D) communications,enabling horizontal module relaying and localized cooperative training amongnodes when feasible. Through delving into the architectural design of HF-FMs,we highlight their unique capabilities along with a series of tailored futureresearch directions. Finally, to demonstrate their potential, we prototypeHF-FMs in a wireless network setting and release the open-source code for thedevelopment of HF-FMs with the goal of fostering exploration in this untappedfield (GitHub: https://github.com/payamsiabd/M3T-FFM).</description>
      <author>example@mail.com (Payam Abdisarabshali, Fardis Nadimi, Kasra Borazjani, Naji Khosravan, Minghui Liwang, Wei Ni, Dusit Niyato, Michael Langberg, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2509.03695v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
      <link>http://arxiv.org/abs/2509.03643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CEHR-GPT，一个针对电子健康记录(EHR)数据的通用基础模型，整合了特征表示、零样本预测和合成数据生成三种核心能力。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHRs)提供患者健康的丰富纵向视图，在临床决策支持、风险预测和数据驱动医疗研究方面具有巨大潜力。然而，大多数EHR人工智能模型为单一目的任务设计，限制了其在现实世界中的泛化能力和实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的EHR基础模型，同时具备多种能力，提高模型在现实世界中的泛化能力和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出CEHR-GPT模型，采用基于时间令牌的学习框架，将患者的时间线动态编码到模型结构中，支持对临床序列的时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;CEHR-GPT在所有三项任务上都表现出强大性能，通过词汇扩展和微调能有效泛化到外部数据集。&lt;h4&gt;结论&lt;/h4&gt;CEHR-GPT的多功能性使其能够快速进行模型开发、队列发现和患者结果预测，无需针对特定任务重新训练。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHRs)提供了患者健康的丰富纵向视图，并在临床决策支持、风险预测和数据驱动的医疗保健研究方面具有巨大潜力。然而，大多数针对EHR的人工智能(AI)模型是为狭窄的单一目的任务设计的，限制了它们在现实世界环境中的泛化能力和实用性。在这里，我们提出了CEHR-GPT，这是一种用于EHR数据的通用基础模型，它在单一架构中统一了三种基本能力——特征表示、零样本预测和合成数据生成。为了支持对临床序列的时间推理，CEHR-GPT采用了一种新颖的基于时间令牌的学习框架，将患者的时间线动态编码到模型结构中。CEHR-GPT在所有三项任务上都表现出强大的性能，并通过词汇扩展和微调能够有效泛化到外部数据集。它的多功能性使其能够快速进行模型开发、队列发现和患者结果预测，而无需针对特定任务进行重新训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) provide a rich, longitudinal view of patienthealth and hold significant potential for advancing clinical decision support,risk prediction, and data-driven healthcare research. However, most artificialintelligence (AI) models for EHRs are designed for narrow, single-purposetasks, limiting their generalizability and utility in real-world settings.Here, we present CEHR-GPT, a general-purpose foundation model for EHR data thatunifies three essential capabilities - feature representation, zero-shotprediction, and synthetic data generation - within a single architecture. Tosupport temporal reasoning over clinical sequences, \cehrgpt{} incorporates anovel time-token-based learning framework that explicitly encodes patients'dynamic timelines into the model structure. CEHR-GPT demonstrates strongperformance across all three tasks and generalizes effectively to externaldatasets through vocabulary expansion and fine-tuning. Its versatility enablesrapid model development, cohort discovery, and patient outcome forecastingwithout the need for task-specific retraining.</description>
      <author>example@mail.com (Chao Pang, Jiheum Park, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Shalmali Joshi, Noémie Elhadad, Karthik Natarajan)</author>
      <guid isPermaLink="false">2509.03643v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm</title>
      <link>http://arxiv.org/abs/2509.02846v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对偏微分方程的测试时计算策略，通过利用推理过程中的计算资源实现更准确的预测，同时减少训练样本量和模型大小。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程是现代计算科学和工程的基础，但计算成本很高。现有PDE基础模型虽在模拟复杂时空现象方面有前景，但仍受预训练数据集限制，自回归展开性能不佳，特别是在分布外情况下，且需要大量计算资源和训练数据。&lt;h4&gt;目的&lt;/h4&gt;受大型语言模型中'思考'策略启发，为PDEs引入第一个测试时计算策略，利用推理过程中的计算资源实现更准确预测，减少训练样本量和模型大小。&lt;h4&gt;方法&lt;/h4&gt;使用两种类型的奖励模型评估基于随机模型的时空一致性预测，并在PDEGym基准测试中的可压缩欧拉方程模拟上展示该方法。&lt;h4&gt;主要发现&lt;/h4&gt;TTC相对于标准的非自适应自回归推理能够捕捉到改进的预测。&lt;h4&gt;结论&lt;/h4&gt;TTC框架向更高级的推理算法或PDE建模迈出了基础性的一步，包括构建基于强化学习的方法，可能改变物理和工程中的计算工作流程。&lt;h4&gt;翻译&lt;/h4&gt;偏微分方程(PDEs)是现代计算科学和工程的基础，本质上计算成本很高。虽然PDE基础模型在模拟这类复杂的时空现象方面显示出很大 promise，但现有模型仍受预训练数据集的限制，在自回归展开性能方面表现不佳，特别是在分布外(OOD)情况下。此外，它们需要大量的计算资源和训练数据，这限制了它们在许多关键应用中的使用。受大型语言模型(LLMs)中最近发展的'思考'策略启发，我们为PDEs引入了第一个测试时计算(TTC)策略，该策略在推理过程中利用计算资源，以实现更准确的预测，同时使用更少的训练样本和更小的模型。我们通过两种类型的奖励模型实现这一点，这些模型评估基于随机模型的时空一致性预测。我们在PDEGym基准测试中的可压缩欧拉方程模拟上展示了这种方法，并表明TTC相对于标准的非自适应自回归推理捕捉到了改进的预测。这个TTC框架朝着更高级的推理算法或PDE建模迈出了基础性的一步，包括构建基于强化学习的方法，可能改变物理和工程中的计算工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial Differential Equations (PDEs) are the bedrock for moderncomputational sciences and engineering, and inherently computationallyexpensive. While PDE foundation models have shown much promise for simulatingsuch complex spatio-temporal phenomena, existing models remain constrained bythe pretraining datasets and struggle with auto-regressive rollout performance,especially in out-of-distribution (OOD) cases. Furthermore, they havesignificant compute and training data requirements which hamper their use inmany critical applications. Inspired by recent advances in ``thinking"strategies used in large language models (LLMs), we introduce the firsttest-time computing (TTC) strategy for PDEs that utilizes computationalresources during inference to achieve more accurate predictions with fewertraining samples and smaller models. We accomplish this with two types ofreward models that evaluate predictions of a stochastic based model forspatio-temporal consistency. We demonstrate this method on compressibleEuler-equation simulations from the PDEGym benchmark and show that TTC capturesimproved predictions relative to standard non-adaptive auto-regressiveinference. This TTC framework marks a foundational step towards more advancedreasoning algorithms or PDE modeling, inluding buildingreinforcement-learning-based approaches, potentially transforming computationalworkflows in physics and engineering.</description>
      <author>example@mail.com (Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debardeleben, Ayan Biswas, Diane Oyen, Earl Lawrence)</author>
      <guid isPermaLink="false">2509.02846v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot</title>
      <link>http://arxiv.org/abs/2509.04076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICANN 20255 Special Session on Neural Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种基于扩散的新型动作模型用于机器人运动规划，通过深度学习显著提高了规划速度，同时保持高达90%的无碰撞解决方案成功率。&lt;h4&gt;背景&lt;/h4&gt;现有机器人运动规划通常使用成熟的数值规划方法，但这些方法存在显著的运行时间要求，限制了实时应用的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够快速、高效解决机器人运动规划问题的方法，减少运行时间同时保持较高的成功率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于扩散的动作模型，通过深度学习从传统规划器生成的数据集中学习。初始模型使用点云嵌入作为输入，预测基于关键点的关节序列。通过消融研究发现网络难以基于点云嵌入进行条件化，因此识别并改进了数据集中的偏差，优化了数据集。&lt;h4&gt;主要发现&lt;/h4&gt;即使不使用点云编码，所提出的模型在运行时间上也比数值模型快一个数量级；在测试集上，模型能够达到高达90%的无碰撞解决方案成功率；通过改进数据集，模型性能得到了提升。&lt;h4&gt;结论&lt;/h4&gt;基于扩散的深度学习方法在机器人运动规划领域具有巨大潜力，能够在保持高成功率的同时显著减少计算时间，为实时机器人应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于机器人运动规划的新型基于扩散的动作模型。通常，成熟的数值规划方法被用于解决通用运动规划问题，但需要大量的运行时间。通过利用深度学习的力量，我们能够从这些规划器生成的数据集中学习，在更小的运行时间内获得良好的结果。虽然我们的初始模型在输入中使用点云嵌入来预测输出中的基于关键点的关节序列，但我们在消融研究中观察到，让网络基于点云嵌入进行条件化仍然具有挑战性。我们确定了数据集中的一些偏差并对其进行了改进，从而提高了模型的性能。我们的模型即使不使用点云编码，在运行时间上也比数值模型快一个数量级，同时在测试集上达到高达90%的无碰撞解决方案成功率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人运动规划的计算效率问题。传统数值规划方法虽然能保证规划的最优性和完整性，但计算成本高，无法实现近实时应用，这在需要机器人快速响应变化环境的现实场景中是一个重大限制。随着自主机器人在复杂环境中应用的增多，快速、可靠的运动规划变得至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从传统数值规划方法的高计算成本出发，考虑利用深度学习加速规划过程。他们最初尝试使用点云嵌入作为输入来预测基于关键点的关节序列，通过消融研究发现数据集存在偏差并进行了改进。设计上借鉴了行为克隆方法，使用扩散策略架构，并参考了运动规划网络(Motion Planning Networks)的工作，但用扩散模型替代了MLP动作预测器，同时利用了扩散模型在行为克隆领域的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型从数值规划器生成的数据集中学习，快速生成机器人运动规划。通过基于关键点的表示减少规划步骤数量，并利用批量推理稳定性能。整体流程包括：1)创建包含5000个场景和10万条规划的合成数据集；2)设计包含点云编码器和扩散动作预测器的神经网络架构；3)训练模型预测从起点到目标的16步关节序列；4)通过逆扩散过程生成轨迹；5)使用批量规划提高成功率和稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于关键点的扩散模型，使16步就能生成完整规划；2)批量规划方法，利用GPU并行推理同时生成多个规划；3)数据集改进，消除短轨迹偏见。相比之前工作，该方法运行时间比传统数值规划方法快一个数量级(3秒对比20秒)，同时保持90%成功率；与其他神经规划方法不同，它简化了架构，减少了对复杂点云嵌入的依赖，使部署更容易；与其他扩散模型方法相比，专注于关节空间中的时空扩散，结合关键点表示提高效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于关键点扩散的快速机器人运动规划方法，通过批量推理实现了比传统数值规划方法快一个数量级的运行速度，同时保持了90%的无碰撞规划成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel diffusion-based action model for robotic motion planning.Commonly, established numerical planning approaches are used to solve generalmotion planning problems, but have significant runtime requirements. Byleveraging the power of deep learning, we are able to achieve good results in amuch smaller runtime by learning from a dataset generated by these planners.While our initial model uses point cloud embeddings in the input to predictkeypoint-based joint sequences in its output, we observed in our ablation studythat it remained challenging to condition the network on the point cloudembeddings. We identified some biases in our dataset and refined it, whichimproved the model's performance. Our model, even without the use of the pointcloud encodings, outperforms numerical models by an order of magnituderegarding the runtime, while reaching a success rate of up to 90% of collisionfree solutions on the test set.</description>
      <author>example@mail.com (Lennart Clasmeier, Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, Stefan Wermter)</author>
      <guid isPermaLink="false">2509.04076v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</title>
      <link>http://arxiv.org/abs/2509.03633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了改进版的treeX算法，一种无监督的树木实例分割方法，适用于近距离激光扫描数据。该方法结合了基于聚类的树干检测和区域生长的树冠分割，为地面激光扫描和无人机激光扫描提供了不同参数预设。研究在六个公共数据集上进行了评估，结果显示改进算法运行时间更短，准确性更高。&lt;h4&gt;背景&lt;/h4&gt;近距离激光扫描能提供森林林分的详细三维捕捉，但需要高效软件处理三维点云数据并提取单棵树木。虽然深度学习方法已用于树木实例分割，但这些方法需要大量标注数据集和计算资源。&lt;h4&gt;目的&lt;/h4&gt;提出资源高效的替代方法，改进原始treeX算法，使其能处理地面激光扫描和无人机激光扫描数据，提高算法效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;改进版treeX算法结合基于聚类的树干检测和区域生长的树冠分割，为TLS/PLS和ULS提供不同参数预设。在六个公共数据集上评估，并与六种开源方法比较。&lt;h4&gt;主要发现&lt;/h4&gt;与原始算法相比，改进版本减少了运行时间并提高了准确性，地面数据实例检测F1分数提高0.11-0.49。ULS数据F1分数达0.58，而原始算法无法分割任何正确实例。TLS和PLS数据准确性与最新开源方法相当。&lt;h4&gt;结论&lt;/h4&gt;该方法可作为深度学习的高效替代方案，适用于数据特征相符的场景；也可用于深度学习模型的半自动标签生成。研究提供了开源Python实现以促进广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;近距离激光扫描能够提供森林林分的详细三维捕捉，但需要高效的软件来处理三维点云数据并提取单棵树木。虽然最近的研究引入了深度学习方法进行树木实例分割，但这些方法需要大量标注数据集和大量计算资源。作为资源高效的替代方案，我们提出了treeX算法的改进版本，这是一种无监督方法，结合了基于聚类的树干检测和区域生长的树冠分割。虽然原始treeX算法是为个人激光扫描数据开发的，但我们提供了两个参数预设，一个用于地面激光扫描，另一个用于无人机载激光扫描。我们在六个公共数据集上评估了该方法，并将其与六种开源方法进行了比较。与原始treeX算法相比，我们的改进版本减少了运行时间并提高了准确性。对于ULS数据，我们的预设实现了0.58的F1分数，而原始算法无法分割任何正确实例。对于TLS和PLS数据，我们的算法实现了与最近的开源方法相当的准确性。鉴于其算法设计，我们看到了该方法的两个主要应用：作为深度学习方法的资源高效替代方案，以及用于深度学习模型的半自动标签生成。为了促进更广泛的应用，我们在pointtree包中提供了开源的Python实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Close-range laser scanning provides detailed 3D captures of forest stands butrequires efficient software for processing 3D point cloud data and extractingindividual trees. Although recent studies have introduced deep learning methodsfor tree instance segmentation, these approaches require large annotateddatasets and substantial computational resources. As a resource-efficientalternative, we present a revised version of the treeX algorithm, anunsupervised method that combines clustering-based stem detection with regiongrowing for crown delineation. While the original treeX algorithm was developedfor personal laser scanning (PLS) data, we provide two parameter presets, onefor ground-based laser scanning (stationary terrestrial - TLS and PLS), and onefor UAV-borne laser scanning (ULS). We evaluated the method on six publicdatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, WythamWoods) and compared it to six open-source methods (original treeX, treeiso,RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the originaltreeX algorithm, our revision reduces runtime and improves accuracy, withinstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the originalalgorithm fails to segment any correct instances. For TLS and PLS data, ouralgorithm achieves accuracy similar to recent open-source methods, includingdeep learning. Given its algorithmic design, we see two main applications forour method: (1) as a resource-efficient alternative to deep learning approachesin scenarios where the data characteristics align with the method design(sufficient stem visibility and point density), and (2) for the semi-automaticgeneration of labels for deep learning models. To enable broader adoption, weprovide an open-source Python implementation in the pointtree package.</description>
      <author>example@mail.com (Josafat-Mattias Burmeister, Andreas Tockner, Stefan Reder, Markus Engel, Rico Richter, Jan-Peter Mund, Jürgen Döllner)</author>
      <guid isPermaLink="false">2509.03633v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Comment on "A Note on Over-Smoothing for Graph Neural Networks"</title>
      <link>http://arxiv.org/abs/2509.04178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Comment on arXiv:2006.13318 (Cai &amp; Wang, 2020). Revisits their  Dirichlet-energy analysis of over-smoothing and extends it to Leaky-ReLU and  spectral polynomial filters; includes Proposition 7.1 and a new proof of  Lemma 3.3 for Leaky-ReLU. 7 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对Cai和Wang (2020) 关于使用Dirichlet能量分析GNN过平滑的工作进行了评论和扩展，展示了节点嵌入的Dirichlet能量如何随网络深度变化，并探索了缓解过平滑的方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)中的过平滑问题是一个重要的研究课题，需要深入理解其机制和缓解方法。&lt;h4&gt;目的&lt;/h4&gt;展示节点嵌入的Dirichlet能量在GNN中的变化规律，特别是在不同深度和激活函数条件下的行为，以及如何利用这些规律缓解过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;在温和的谱条件下分析Dirichlet能量的变化，将结果扩展到谱多项式滤波器，并通过边删除和权重放大实验进行验证，同时为Leaky-ReLU情况提供简短证明。&lt;h4&gt;主要发现&lt;/h4&gt;节点嵌入的Dirichlet能量在温和的谱条件下(包括Leaky-ReLU)随网络深度呈指数级下降；当Dirichlet能量增加时，可能暗示了缓解过平滑的实际方法。&lt;h4&gt;结论&lt;/h4&gt;通过理解Dirichlet能量随深度的变化规律，可以为设计缓解GNN中过平滑问题的方法提供理论指导和实践启示。&lt;h4&gt;翻译&lt;/h4&gt;我们评论了Cai和Wang (2020, arXiv:2006.13318)，他们通过Dirichlet能量分析了GNN中的过平滑问题。我们表明，在温和的谱条件下(包括Leaky-ReLU)，节点嵌入的Dirichlet能量随深度呈指数级下降；我们进一步将这一结果扩展到谱多项式滤波器，并为Leaky-ReLU情况提供了简短证明。关于边删除和权重放大的实验说明了Dirichlet能量何时增加，暗示了缓解过平滑的实际方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We comment on Cai and Wang (2020, arXiv:2006.13318), who analyzeover-smoothing in GNNs via Dirichlet energy. We show that under mild spectralconditions (including with Leaky-ReLU), the Dirichlet energy of node embeddingsdecreases exponentially with depth; we further extend the result to spectralpolynomial filters and provide a short proof for the Leaky-ReLU case.Experiments on edge deletion and weight amplification illustrate when Dirichletenergy increases, hinting at practical ways to relieve over-smoothing.</description>
      <author>example@mail.com (Razi Hasson, Reuven Guetta)</author>
      <guid isPermaLink="false">2509.04178v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Topotein: Topological Deep Learning for Protein Representation Learning</title>
      <link>http://arxiv.org/abs/2509.03885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Topotein框架通过拓扑深度学习改进蛋白质表示学习，提出蛋白质组合复合物(PCC)和拓扑完全感知器网络(TCPNet)，在四个蛋白质表示学习任务中表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;蛋白质表示学习(PRL)对于理解蛋白质结构与功能关系至关重要，但当前的序列和图方法无法捕捉蛋白质结构中固有的层次组织。&lt;h4&gt;目的&lt;/h4&gt;引入Topotein框架，通过拓扑深度学习来改进蛋白质表示学习，更好地捕捉蛋白质结构的层次组织。&lt;h4&gt;方法&lt;/h4&gt;提出蛋白质组合复合物(PCC)表示蛋白质的多个层次结构(从残基到二级结构再到完整蛋白质)，并开发拓扑完全感知器网络(TCPNet)，通过层次结构之间的SE(3)-等变消息传递捕获多尺度结构模式。&lt;h4&gt;主要发现&lt;/h4&gt;在四个蛋白质表示学习任务上的广泛实验中，TCPNet始终优于最先进的几何图神经网络，在需要理解二级结构排列的任务(如折叠分类)中表现出特别的优势。&lt;h4&gt;结论&lt;/h4&gt;层次拓扑特征对蛋白质分析具有重要意义，Topotein框架有效捕捉了蛋白质结构的层次组织，提高了蛋白质表示学习的性能。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质表示学习(PRL)对于理解结构与功能关系至关重要，然而当前基于序列和图的方法无法捕捉蛋白质结构中固有的层次组织。我们引入Topotein，一个综合框架，通过新颖的蛋白质组合复合物(PCC)和拓扑完全感知器网络(TCPNet)将拓扑深度学习应用于PRL。我们的PCC在多个层次上表示蛋白质——从残基到二级结构再到完整蛋白质——同时在每个层次保留几何信息。TCPNet在这些层次结构之间采用SE(3)-等变消息传递，能够更有效地捕获多尺度结构模式。在四个PRL任务上的广泛实验中，TCPNet始终优于最先进的几何图神经网络。我们的方法在需要理解二级结构排列的任务(如折叠分类)中表现出特别的优势，验证了层次拓扑特征对蛋白质分析的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein representation learning (PRL) is crucial for understandingstructure-function relationships, yet current sequence- and graph-based methodsfail to capture the hierarchical organization inherent in protein structures.We introduce Topotein, a comprehensive framework that applies topological deeplearning to PRL through the novel Protein Combinatorial Complex (PCC) andTopology-Complete Perceptron Network (TCPNet). Our PCC represents proteins atmultiple hierarchical levels -- from residues to secondary structures tocomplete proteins -- while preserving geometric information at each level.TCPNet employs SE(3)-equivariant message passing across these hierarchicalstructures, enabling more effective capture of multi-scale structural patterns.Through extensive experiments on four PRL tasks, TCPNet consistentlyoutperforms state-of-the-art geometric graph neural networks. Our approachdemonstrates particular strength in tasks such as fold classification whichrequire understanding of secondary structure arrangements, validating theimportance of hierarchical topological features for protein analysis.</description>
      <author>example@mail.com (Zhiyu Wang, Arian Jamasb, Mustafa Hajij, Alex Morehead, Luke Braithwaite, Pietro Liò)</author>
      <guid isPermaLink="false">2509.03885v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability</title>
      <link>http://arxiv.org/abs/2509.03547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了MatterVial，一个用于材料科学中基于特征的机器学习的创新混合框架，通过整合多种预训练图神经网络模型的潜在表示来扩展特征空间，结合传统模型的化学透明度和深度学习的预测能力，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;材料科学中的机器学习面临特征选择和模型可解释性的挑战，传统基于特征模型具有化学透明性但预测能力有限，而深度学习模型预测能力强但往往缺乏可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个混合框架，结合传统基于特征模型的化学透明度和深度学习架构的预测能力，同时保持可解释性，提高材料科学中机器学习模型的性能和实用性。&lt;h4&gt;方法&lt;/h4&gt;MatterVial框架整合多种预训练图神经网络模型的潜在表示(包括基于结构的MEGNet、基于成分的ROOST和等变的ORB图网络)，结合计算高效的GNN近似描述符和符号回归的新特征，并集成可解释性模块将GNN特征解码为物理意义明确的公式。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench任务中增强MODNet模型时，显著降低了误差，将性能提升到与最先进端到端GNN相竞争甚至在某些情况下超越它们，多个任务的准确性提高了40%以上，成功将GNN的复杂特征转换为明确的物理意义公式。&lt;h4&gt;结论&lt;/h4&gt;MatterVial统一框架通过提供高性能、透明的工具来推进材料信息学，该工具符合可解释AI的原则，为更有针对性和自主性的材料发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了MatterVial，这是一个用于材料科学中基于特征的机器学习的创新混合框架。MatterVial通过整合来自多样化预训练图神经网络(GNN)模型的潜在表示来扩展特征空间，包括：基于结构的(MEGNet)、基于成分的(ROOST)和等变的(ORB)图网络，以及计算高效的GNN近似描述符和符号回归的新颖特征。我们的方法将传统基于特征模型的化学透明度与深度学习架构的预测能力相结合。在Matbench任务中增强基于特征的模型MODNet时，这种方法产生了显著的误差减少，并将其性能提升到与最先进的端到端GNN相竞争，并在几种情况下优于它们，多个任务的准确性提高了40%以上。一个集成的可解释性模块，采用代理模型和符号回归，将GNN衍生的潜在描述符解码为明确的、具有物理意义的公式。这个统一的框架通过提供符合可解释AI原则的高性能、透明工具来推进材料信息学，为更有针对性和自主性的材料发现铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces MatterVial, an innovative hybrid framework forfeature-based machine learning in materials science. MatterVial expands thefeature space by integrating latent representations from a diverse suite ofpretrained graph neural network (GNN) models including: structure-based(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, withcomputationally efficient, GNN-approximated descriptors and novel features fromsymbolic regression. Our approach combines the chemical transparency oftraditional feature-based models with the predictive power of deep learningarchitectures. When augmenting the feature-based model MODNet on Matbenchtasks, this method yields significant error reductions and elevates itsperformance to be competitive with, and in several cases superior to,state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% formultiple tasks. An integrated interpretability module, employing surrogatemodels and symbolic regression, decodes the latent GNN-derived descriptors intoexplicit, physically meaningful formulas. This unified framework advancesmaterials informatics by providing a high-performance, transparent tool thataligns with the principles of explainable AI, paving the way for more targetedand autonomous materials discovery.</description>
      <author>example@mail.com (Rogério Almeida Gouvêa, Pierre-Paul De Breuck, Tatiane Pretto, Gian-Marco Rignanese, Marcos José Leite dos Santos)</author>
      <guid isPermaLink="false">2509.03547v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection</title>
      <link>http://arxiv.org/abs/2509.01153v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络和锚定区间的框架，用于呼吸声音事件检测，能够处理可变长度音频并提供更精确的时间定位，实验证明结合呼吸位置信息可以提高异常声音的区分能力。&lt;h4&gt;背景&lt;/h4&gt;听诊是呼吸和肺部疾病早期诊断的关键方法，但过程主观且专家间存在变异性。现有深度学习方法多专注于呼吸声音分类，而声音事件检测研究有限。现有方法通常依赖帧级预测和后处理，难以直接学习区间边界，且多数只能处理固定长度音频，限制了在可变长度呼吸声音中的应用。呼吸声音位置信息对检测性能的影响也未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决现有呼吸声音事件检测方法的局限性，提出能够处理可变长度音频并提供更精确时间定位的方法，探索呼吸声音位置信息对检测性能的影响。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于图神经网络和锚定区间的框架，能够处理可变长度音频并提供异常呼吸声音事件更精确的时间定位，结合呼吸位置信息来提高异常声音的区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;在SPRSound 2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，结合呼吸位置信息可以增强异常声音之间的区分能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法提高了呼吸声音检测的灵活性和适用性，基于图神经网络和锚定区间的框架能够有效处理可变长度音频并提供更精确的时间定位。&lt;h4&gt;翻译&lt;/h4&gt;听诊是呼吸和肺部疾病早期诊断的关键方法，依赖于熟练的医疗专业人员。然而，这个过程往往是主观的，专家之间存在变异性。因此，许多基于深度学习的自动分类方法已经出现，大多数专注于呼吸声音分类。相比之下，呼吸声音事件检测的研究仍然有限。现有的声音事件检测方法通常依赖于帧级预测，然后通过后处理生成事件级输出，使得区间边界难以直接学习。此外，许多方法只能处理固定长度的音频，限制了它们在可变长度呼吸声音中的应用。此外，呼吸声音位置信息对检测性能的影响尚未得到充分探索。为了解决这些问题，我们提出了一种基于图神经网络的框架，具有锚定区间，能够处理可变长度的音频，并为异常呼吸声音事件提供更精确的时间定位。我们的方法提高了呼吸声音检测的灵活性和适用性。在SPRSound 2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，并且结合呼吸位置信息可以提高异常声音的区分能力。参考实现可在https://github.com/chumingqian/EzhouNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.bspc.2025.108491&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auscultation is a key method for early diagnosis of respiratory and pulmonarydiseases, relying on skilled healthcare professionals. However, the process isoften subjective, with variability between experts. As a result, numerous deeplearning-based automatic classification methods have emerged, most of whichfocus on respiratory sound classification. In contrast, research on respiratorysound event detection remains limited. Existing sound event detection methodstypically rely on frame-level predictions followed by post-processing togenerate event-level outputs, making interval boundaries challenging to learndirectly. Furthermore, many approaches can only handle fixed-length audio,limiting their applicability to variable-length respiratory sounds.Additionally, the impact of respiratory sound location information on detectionperformance has not been extensively explored. To address these issues, wepropose a graph neural network-based framework with anchor intervals, capableof handling variable-length audio and providing more precise temporallocalization for abnormal respiratory sound events. Our method improves boththe flexibility and applicability of respiratory sound detection. Experimentson the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness ofthe proposed approach, and incorporating respiratory position informationenhances the discrimination between abnormal sounds. The referenceimplementation is available at https://github.com/chumingqian/EzhouNet.</description>
      <author>example@mail.com (Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng)</author>
      <guid isPermaLink="false">2509.01153v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer</title>
      <link>http://arxiv.org/abs/2509.04362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 5 figures, under review for journal publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SST-iTransformer的新方法，用于预测城市停车场可用性。该方法通过K-means聚类建立停车聚类区域，整合多种交通模式的需求特征，并采用改进的双分支注意力机制进行时空建模。实验证明该方法优于现有基线模型，且不同数据源对预测性能的贡献度不同。&lt;h4&gt;背景&lt;/h4&gt;私家车数量的快速增长加剧了城市停车困境，因此需要准确有效的停车场可用性预测来支持城市规划和管理。&lt;h4&gt;目的&lt;/h4&gt;解决停车场可用性预测中建模时空依赖性和利用多源数据的关键局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 使用K-means聚类建立停车聚类区域(PCZs)；2) 提取并整合与目标停车场相关的多种交通模式(地铁、公交、网约车、出租车)的交通需求特征；3) 基于原始iTransformer进行升级，提出SST-iTransformer：整合基于掩码重建的自监督任务进行时空表示学习，并采用双分支注意力机制(序列注意力捕获长期时间依赖，通道注意力建模跨变量交互)。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用成都真实数据的实验表明，SST-iTransformer优于基线深度学习模型，达到最低的均方误差和具有竞争力的平均绝对误差；2) 不同数据源贡献度不同：网约车数据贡献最大，其次是出租车数据，公交和地铁数据贡献较小；3) 排除相关停车场的历史数据会导致性能大幅下降，强调了建模空间依赖性的重要性。&lt;h4&gt;结论&lt;/h4&gt;SST-iTransformer方法通过有效建模时空依赖性和整合多源数据，显著提高了停车场可用性预测的准确性，为城市停车管理提供了有力支持。&lt;h4&gt;翻译&lt;/h4&gt;私家车数量的快速增长加剧了城市停车困境，凸显了需要准确有效的停车场可用性预测来支持城市规划和管理的必要性。为解决停车场可用性预测中建模时空依赖性和利用多源数据的关键局限性，本研究提出了一种基于SST-iTransformer的新方法。该方法利用K-means聚类建立停车聚类区域(PCZs)，提取并整合与目标停车场相关的各种交通模式(即地铁、公交、网约车和出租车)的交通需求特征。在原始iTransformer基础上升级的SST-iTransformer，整合了基于掩码重建的自监督 pretext 任务进行时空表示学习，并具有创新的双分支注意力机制：序列注意力通过patch操作捕获长期时间依赖，而通道注意力通过反转维度建模跨变量交互。使用中国成都真实数据的广泛实验表明，SST-iTransformer优于基线深度学习模型(包括Informer、Autoformer、Crossformer和iTransformer)，以最低的均方误差(MSE)和具有竞争力的平均绝对误差(MAE)实现了最先进的性能。全面的消融研究定量揭示了不同数据源的相对重要性：整合网约车数据带来最大的性能提升，其次是出租车数据，而固定路线交通特征(公交/地铁)贡献较小。空间相关性分析进一步证实，排除PCZs内相关停车场的历史数据会导致性能大幅下降，强调了建模空间依赖性的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of private car ownership has worsened the urban parkingpredicament, underscoring the need for accurate and effective parkingavailability prediction to support urban planning and management. To addresskey limitations in modeling spatio-temporal dependencies and exploitingmulti-source data for parking availability prediction, this study proposes anovel approach with SST-iTransformer. The methodology leverages K-meansclustering to establish parking cluster zones (PCZs), extracting andintegrating traffic demand characteristics from various transportation modes(i.e., metro, bus, online ride-hailing, and taxi) associated with the targetedparking lots. Upgraded on vanilla iTransformer, SST-iTransformer integratesmasking-reconstruction-based pretext tasks for self-supervised spatio-temporalrepresentation learning, and features an innovative dual-branch attentionmechanism: Series Attention captures long-term temporal dependencies viapatching operations, while Channel Attention models cross-variate interactionsthrough inverted dimensions. Extensive experiments using real-world data fromChengdu, China, demonstrate that SST-iTransformer outperforms baseline deeplearning models (including Informer, Autoformer, Crossformer, andiTransformer), achieving state-of-the-art performance with the lowest meansquared error (MSE) and competitive mean absolute error (MAE). Comprehensiveablation studies quantitatively reveal the relative importance of differentdata sources: incorporating ride-hailing data provides the largest performancegains, followed by taxi, whereas fixed-route transit features (bus/metro)contribute marginally. Spatial correlation analysis further confirms thatexcluding historical data from correlated parking lots within PCZs leads tosubstantial performance degradation, underscoring the importance of modelingspatial dependencies.</description>
      <author>example@mail.com (Yin Huang, Yongqi Dong, Youhua Tang, Li Li)</author>
      <guid isPermaLink="false">2509.04362v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Entity Representation Learning for Pinterest Ads Ranking</title>
      <link>http://arxiv.org/abs/2509.04337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种Pinterest使用的上游-下游新框架，通过多种数据源构建用户和Pin的嵌入表示，用于个性化推荐和广告投放，并在实际系统中取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;Pinterest需要从多样化数据源构建用户和Pin的嵌入表示，以实现有效的个性化Pin和广告推荐。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的框架，能够从多样化数据源学习用户和Pin的嵌入表示，并支持多种下游推荐和广告任务。&lt;h4&gt;方法&lt;/h4&gt;采用上游-下游范式，上游模型在广泛数据源上训练使用复杂架构捕捉用户-Pin关系，通过学习并定期刷新实体嵌入而非实时计算确保可扩展性，然后将这些嵌入作为下游任务如广告检索和CTR/CVR预测排序模型的输入特征。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在离线和在线环境下，各种下游任务中取得了显著的性能提升，并在Pinterest生产广告排序系统中部署后带来在线指标的显著增长。&lt;h4&gt;结论&lt;/h4&gt;所提出的上游-下游框架是Pinterest个性化推荐和广告系统的有效解决方案，通过嵌入表示的异步交互实现了良好的可扩展性和性能提升。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种遵循上游-下游范式的新框架，从多样化数据源构建用户和物品(Pin)的嵌入表示，这些嵌入对Pinterest有效提供个性化Pin和广告至关重要。我们的上游模型在具有不同信号特征的大量数据源上训练，利用复杂架构捕捉Pinterest上用户和Pin之间的复杂关系。为确保上游模型的可扩展性，学习实体嵌入并定期刷新，而非实时计算，允许上游和下游模型之间异步交互。这些嵌入随后被整合为众多下游任务的输入特征，包括用于点击率和转化率预测的广告检索和排序模型。我们证明该框架在各种下游任务的离线和在线设置中均取得了显著的性能提升。该框架已在Pinterest的生产广告排序系统中部署，带来了在线指标的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a novel framework following anupstream-downstream paradigm to construct user and item (Pin) embeddings fromdiverse data sources, which are essential for Pinterest to deliver personalizedPins and ads effectively. Our upstream models are trained on extensive datasources featuring varied signals, utilizing complex architectures to captureintricate relationships between users and Pins on Pinterest. To ensurescalability of the upstream models, entity embeddings are learned, andregularly refreshed, rather than real-time computation, allowing forasynchronous interaction between the upstream and downstream models. Theseembeddings are then integrated as input features in numerous downstream tasks,including ad retrieval and ranking models for CTR and CVR predictions. Wedemonstrate that our framework achieves notable performance improvements inboth offline and online settings across various downstream tasks. Thisframework has been deployed in Pinterest's production ad ranking systems,resulting in significant gains in online metrics.</description>
      <author>example@mail.com (Jie Liu, Yinrui Li, Jiankai Sun, Kungang Li, Han Sun, Sihan Wang, Huasen Wu, Siyuan Gao, Paulo Soares, Nan Li, Zhifang Liu, Haoyang Li, Siping Ji, Ling Leng, Prathibha Deshikachar)</author>
      <guid isPermaLink="false">2509.04337v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis</title>
      <link>http://arxiv.org/abs/2509.04295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Excerpt from C. Jones' PhD thesis. Winner of the G-Research PhD prize  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习方法在现实世界部署时经常失败，特别是在高风险场景和社会敏感领域，限制了其在医疗诊断等领域的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;机器学习在现实世界部署时经常失败，尤其是在高风险场景和社会敏感领域，这对其在医疗诊断等领域的应用产生了抑制作用。&lt;h4&gt;目的&lt;/h4&gt;介绍导致图像分析中机器学习方法失败的因果和统计结构。&lt;h4&gt;方法&lt;/h4&gt;分析机器学习方法在图像分析中失败的根本原因，并评估现有公平表示学习方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两个之前被忽视的问题：'no fair lunch' problem（无公平午餐问题）和'subgroup separability' problem（子群可分离性问题）。&lt;h4&gt;结论&lt;/h4&gt;当今的公平表示学习方法无法充分解决这些问题，需要新的方法路径来应对这些挑战。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法在现实世界中部署时常常失败。更糟糕的是，它们在高风险情境和社会敏感领域都会失败。这些问题对机器学习方法在医疗诊断等领域的应用产生了抑制作用，尽管在这些领域，如果能够安全部署，机器学习方法本可以提供最大益处。在本入门指南中，我们介绍了导致图像分析中机器学习方法失败的因果和统计结构。我们强调了两个之前被忽视的问题，我们称之为'无公平午餐'问题和'子群可分离性'问题。我们阐明了为什么当今的公平表示学习方法无法充分解决这些问题，并为该领域提出了可能的解决路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning methods often fail when deployed in the real world. Worsestill, they fail in high-stakes situations and across socially sensitive lines.These issues have a chilling effect on the adoption of machine learning methodsin settings such as medical diagnosis, where they are arguably best-placed toprovide benefits if safely deployed. In this primer, we introduce the causaland statistical structures which induce failure in machine learning methods forimage analysis. We highlight two previously overlooked problems, which we callthe \textit{no fair lunch} problem and the \textit{subgroup separability}problem. We elucidate why today's fair representation learning methods fail toadequately solve them and propose potential paths forward for the field.</description>
      <author>example@mail.com (Charles Jones, Ben Glocker)</author>
      <guid isPermaLink="false">2509.04295v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music</title>
      <link>http://arxiv.org/abs/2509.04215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the 26th International Society for Music  Information Retrieval Conference (ISMIR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PianoBind，一个专用于钢琴音乐的多模态联合嵌入模型，解决了现有模型难以捕捉钢琴音乐细微语义差异和多模态特性的问题。&lt;h4&gt;背景&lt;/h4&gt;独奏钢琴音乐具有丰富的表现力和语义信息，但当前通用音乐表征模型难以捕捉同质化钢琴音乐中的细微语义差异，且现有钢琴专用模型多为单模态，无法捕捉钢琴音乐固有的多模态特性（音频、符号和文本）。&lt;h4&gt;目的&lt;/h4&gt;开发一个钢琴专用的多模态联合嵌入模型，以有效捕捉钢琴音乐的细微语义差异和多模态特性，并在小规模和同质化数据集上表现良好。&lt;h4&gt;方法&lt;/h4&gt;在联合嵌入框架内系统研究多源训练策略和模态利用方法，优化模型以在(1)小规模和(2)同质化钢琴数据集中捕捉细粒度语义差异，同时整合音频、符号和文本多种模态信息。&lt;h4&gt;主要发现&lt;/h4&gt;PianoBind学习到的多模态表征能有效捕捉钢琴音乐的细微差别，在领域内和领域外的钢琴数据集上，其文本到音乐检索性能优于通用音乐联合嵌入模型，且其设计选择为其他同质化数据集的多模态表征学习提供了可重用的见解。&lt;h4&gt;结论&lt;/h4&gt;PianoBind成功解决了现有钢琴音乐表征模型的局限性，不仅适用于钢琴音乐，其设计原则还可扩展到其他同质化数据集的多模态表征学习。&lt;h4&gt;翻译&lt;/h4&gt;独奏钢琴音乐，尽管是单一乐器媒介，却具有显著的表现力，能跨越流派、情绪和风格传达丰富的语义信息。然而，当前主要在大规模数据集上训练的通用音乐表征模型，往往难以在同质化的独奏钢琴音乐中捕捉细微的语义差异。此外，现有的钢琴专用表征模型通常是单模态的，无法通过音频、符号和文本模态捕捉钢琴音乐固有的多模态特性。为解决这些局限性，我们提出了PianoBind，一个钢琴专用的多模态联合嵌入模型。我们在联合嵌入框架内系统研究了多源训练和模态利用策略，针对在(1)小规模和(2)同质化钢琴数据集中捕捉细粒度语义差异进行了优化。我们的实验结果表明，PianoBind学习到的多模态表征能有效捕捉钢琴音乐的细微差别，在领域内和领域外的钢琴数据集上，其文本到音乐检索性能优于通用音乐联合嵌入模型。此外，我们的设计选择为其他钢琴音乐之外的同质化数据集的多模态表征学习提供了可重用的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solo piano music, despite being a single-instrument medium, possessessignificant expressive capabilities, conveying rich semantic information acrossgenres, moods, and styles. However, current general-purpose musicrepresentation models, predominantly trained on large-scale datasets, oftenstruggle to captures subtle semantic distinctions within homogeneous solo pianomusic. Furthermore, existing piano-specific representation models are typicallyunimodal, failing to capture the inherently multimodal nature of piano music,expressed through audio, symbolic, and textual modalities. To address theselimitations, we propose PianoBind, a piano-specific multimodal joint embeddingmodel. We systematically investigate strategies for multi-source training andmodality utilization within a joint embedding framework optimized for capturingfine-grained semantic distinctions in (1) small-scale and (2) homogeneous pianodatasets. Our experimental results demonstrate that PianoBind learns multimodalrepresentations that effectively capture subtle nuances of piano music,achieving superior text-to-music retrieval performance on in-domain andout-of-domain piano datasets compared to general-purpose music joint embeddingmodels. Moreover, our design choices offer reusable insights for multimodalrepresentation learning with homogeneous datasets beyond piano music.</description>
      <author>example@mail.com (Hayeon Bang, Eunjin Choi, Seungheon Doh, Juhan Nam)</author>
      <guid isPermaLink="false">2509.04215v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity</title>
      <link>http://arxiv.org/abs/2509.04107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 3rd IEEE International Conference on Federated Learning  Technologies and Applications (FLTA25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FedQuad的新方法，用于解决联邦学习中因数据异构性（特别是数据集规模小和类别不平衡）导致的全局模型泛化能力下降问题。通过优化客户端间较小的类内方差和较大的类间方差，FedQuad减少了模型聚合对全局模型的负面影响，在CIFAR-10和CIFAR-100数据集上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(FL)提供了一种去中心化的模型训练方法，能够有效处理分布式数据和隐私保护问题。然而，客户端间的数据异构性常常导致全局模型的泛化能力面临挑战，当数据集规模有限且存在类别不平衡时，这一挑战变得更加突出。&lt;h4&gt;目的&lt;/h4&gt;解决联邦学习中由于数据异构性（特别是数据集规模小和类别不平衡）导致的全局模型泛化能力下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为FedQuad的新方法，该方法明确优化客户端间较小的类内方差和较大的类间方差，从而减少模型聚合对全局模型的负面影响。通过最小化相似样本对的距离，同时最大化负样本对的距离，有效地在共享特征空间中分离客户端数据。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10和CIFAR-100数据集上，在各种数据分布和多个客户端的情况下，FedQuad方法相比现有方法表现出优越的性能。此外，研究还提供了基于度量的学习策略在监督学习和联邦学习范式中的详细分析，突显了它们在解决联邦环境中表示学习挑战方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;FedQuad方法通过解决数据异构性问题，有效提高了联邦学习中的全局模型泛化能力，特别是在数据集规模小和类别不平衡的情况下。该研究还强调了基于度量的学习策略在联邦学习中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)提供了一种去中心化的模型训练方法，能够有效处理分布式数据和隐私保护问题。然而，客户端间的数据异构性常常导致全局模型的泛化能力面临挑战。当数据集规模有限且存在类别不平衡时，这一挑战变得更加突出。为了解决数据异构性问题，我们提出了一种名为FedQuad的新方法，该方法明确优化客户端间较小的类内方差和较大的类间方差，从而减少模型聚合对全局模型的负面影响。我们的方法通过最小化相似样本对的距离，同时最大化负样本对的距离，有效地在共享特征空间中分离客户端数据。我们在各种数据分布和多个客户端的情况下，在CIFAR-10和CIFAR-100数据集上评估了我们的方法，展示了相比现有方法的优越性能。此外，我们还提供了基于度量的学习策略在监督学习和联邦学习范式中的详细分析，突显了它们在解决联邦环境中表示学习挑战方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) provides decentralised model training, whicheffectively tackles problems such as distributed data and privacy preservation.However, the generalisation of global models frequently faces challenges fromdata heterogeneity among clients. This challenge becomes even more pronouncedwhen datasets are limited in size and class imbalance. To address dataheterogeneity, we propose a novel method, \textit{FedQuad}, that explicitlyoptimises smaller intra-class variance and larger inter-class variance acrossclients, thereby decreasing the negative impact of model aggregation on theglobal model over client representations. Our approach minimises the distancebetween similar pairs while maximising the distance between negative pairs,effectively disentangling client data in the shared feature space. We evaluateour method on the CIFAR-10 and CIFAR-100 datasets under various datadistributions and with many clients, demonstrating superior performancecompared to existing approaches. Furthermore, we provide a detailed analysis ofmetric learning-based strategies within both supervised and federated learningparadigms, highlighting their efficacy in addressing representational learningchallenges in federated settings.</description>
      <author>example@mail.com (Ozgu Goksu, Nicolas Pugeault)</author>
      <guid isPermaLink="false">2509.04107v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting</title>
      <link>http://arxiv.org/abs/2509.03800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MedVista3D是一种用于3D CT分析的多尺度语义增强视觉-语言预训练框架，通过局部和全局图像-文本对齐以及语义感知对齐，实现了联合疾病检测和整体解释，在多项任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;放射诊断中的错误（如漏读、注意力盲点和沟通失败）在临床实践中普遍存在，这些问题源于漏掉的局部异常、有限的全球背景和报告语言的变异性。在3D成像中，这些挑战被放大，因为临床医生必须检查每个扫描的数百个切片。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时满足精确局部检测、全局体积级推理和语义一致自然语言报告需求的系统，以解决放射诊断中的错误问题。&lt;h4&gt;方法&lt;/h4&gt;提出MedVista3D，一个多尺度语义增强视觉-语言预训练框架，通过执行局部和全局图像-文本对齐进行细粒度表示学习，并应用语言模型重写和引入放射学语义匹配库来解决报告变异性问题。&lt;h4&gt;主要发现&lt;/h4&gt;MedVista3D在零样本疾病分类、报告检索和医疗视觉问答方面取得了最先进的性能，并且能够很好地迁移到器官分割和预后预测任务。&lt;h4&gt;结论&lt;/h4&gt;MedVista3D是一个有效的框架，能够解决放射诊断中的错误问题，通过结合局部和全局理解以及语义一致的自然语言报告，提高了诊断的准确性。&lt;h4&gt;翻译&lt;/h4&gt;放射诊断错误——包括漏读错误、注意力盲点和沟通失败——在临床实践中仍然普遍存在。这些问题通常源于漏掉的局部异常、有限的全球背景和报告语言的变异性。在3D成像中，这些挑战被放大，因为临床医生必须检查每个扫描的数百个切片。解决这些问题需要具有精确局部检测、全局体积级推理和语义一致自然语言报告的系统。然而，现有的3D视觉-语言模型无法同时满足这三个需求，它们缺乏空间推理的局部-全局理解，并且难以处理未整理的放射报告的变异性和噪声。我们提出了MedVista3D，这是一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架。为了实现联合疾病检测和整体解释，MedVista3D在完整体积上下文中执行局部和全局图像-文本对齐，以便进行细粒度表示学习。为了解决报告变异性问题，我们应用了语言模型重写，并引入了放射学语义匹配库，用于语义感知对齐。MedVista3D在零样本疾病分类、报告检索和医疗视觉问答方面取得了最先进的性能，并且能够很好地迁移到器官分割和预后预测。代码和数据集将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决放射学诊断中的三类错误：漏读错误（遗漏视野内的异常）、注意力盲点（忽略全局背景中的病变）和沟通失败（报告表述不一致或模糊）。这些问题在现实中非常重要，因为放射学诊断错误仍然普遍存在，会对患者健康造成持续威胁。特别是在3D成像中，医生需要检查数百个切片，这些挑战被放大，需要系统能够同时进行精确局部检测、全局理解和一致报告。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D视觉语言模型的局限性，发现它们无法同时处理局部和全局信息，且难以处理报告中的语言变异性。作者借鉴了现有工作但进行了改进：分析了CT-CLIP（全局模型）和fVLM（局部模型）的优缺点；借鉴了多尺度对齐思想但结合了更精细的器官分割掩码；改进了使用LLM重写报告的方法，并引入了放射学语义匹配银行来解决文本变异性问题。作者通过理论分析证明多尺度对齐能捕获更多互信息，从而设计出更有效的模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多尺度语义增强的视觉语言预训练框架，同时实现局部疾病检测和全局图像理解，并解决报告语言一致性问题。整体流程包括：1）多尺度对齐：使用双路径视觉编码器同时处理全局CT体积和局部器官，通过多尺度对比损失对齐图像和文本；2）语义增强：用LLM重写报告强调疾病存在/不存在，建立放射学语义匹配银行(RSMB)检索语义相似描述；3）最终目标结合多尺度对齐和语义对齐，使模型能同时关注细节和全局，并生成一致报告。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）多尺度对齐损失，同时捕获局部和全局信息；2）放射学语义匹配银行(RSMB)，解决报告语言变异性；3）双路径视觉编码器，处理全局和局部信息并保留空间关系；4）LLM重写报告，使表述更标准化。相比之前工作：不同于全局模型（如CT-CLIP）的漏读问题，MedVista3D能检测小异常；不同于局部模型（如fVLM）的注意力盲点问题，能理解全局背景；使用分割掩码而非粗略边界框实现更精确定位；不仅关注疾病查询，还考虑查询上下文和完整性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MedVista3D通过多尺度语义对齐的视觉语言模型，有效减少了放射学诊断中的漏读错误、注意力盲点和沟通失败，同时实现了精确的局部疾病检测、全局图像理解和一致的疾病报告。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radiologic diagnostic errors-under-reading errors, inattentional blindness,and communication failures-remain prevalent in clinical practice. These issuesoften stem from missed localized abnormalities, limited global context, andvariability in report language. These challenges are amplified in 3D imaging,where clinicians must examine hundreds of slices per scan. Addressing themrequires systems with precise localized detection, global volume-levelreasoning, and semantically consistent natural language reporting. However,existing 3D vision-language models are unable to meet all three needs jointly,lacking local-global understanding for spatial reasoning and struggling withthe variability and noise of uncurated radiology reports. We presentMedVista3D, a multi-scale semantic-enriched vision-language pretrainingframework for 3D CT analysis. To enable joint disease detection and holisticinterpretation, MedVista3D performs local and global image-text alignment forfine-grained representation learning within full-volume context. To addressreport variability, we apply language model rewrites and introduce a RadiologySemantic Matching Bank for semantics-aware alignment. MedVista3D achievesstate-of-the-art performance on zero-shot disease classification, reportretrieval, and medical visual question answering, while transferring well toorgan segmentation and prognosis prediction. Code and datasets will bereleased.</description>
      <author>example@mail.com (Yuheng Li, Yenho Chen, Yuxiang Lai, Jike Zhong, Vanessa Wildman, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2509.03800v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?</title>
      <link>http://arxiv.org/abs/2509.03790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了奖励函数的基本性质如何促进高效的稀疏奖励强化学习，通过奖励矩阵的低秩结构视角，揭示了这种结构能导致从指数级到多项式样本复杂性的显著转变。&lt;h4&gt;背景&lt;/h4&gt;稀疏奖励强化学习面临样本效率低的挑战，需要理解奖励函数的基本性质来提高学习效率。&lt;h4&gt;目的&lt;/h4&gt;研究奖励函数的基本性质如何促进高效的稀疏奖励强化学习，探索低秩结构对样本复杂性的影响。&lt;h4&gt;方法&lt;/h4&gt;引入策略感知矩阵补全(PAMC)，通过策略相关采样分析将矩阵补全理论与强化学习联系起来，提供四种框架应用：一般稀疏奖励观测的不可能性结果、无奖励表征学习、分布无关置信集和鲁棒补全保证。&lt;h4&gt;主要发现&lt;/h4&gt;低秩结构在奖励矩阵中会导致从指数级到多项式样本复杂性的急剧转变；在100个系统性采样的领域评估中，超过一半存在可利用的结构；PAMC相比基线方法将样本效率提高了1.6到2.1倍，同时仅增加约20%的计算开销。&lt;h4&gt;结论&lt;/h4&gt;结构化奖励学习是一个有前景的新范式，对机器人技术、医疗保健和其他安全关键、样本密集型应用有直接影响。&lt;h4&gt;翻译&lt;/h4&gt;奖励函数的哪些基本性质能够促进高效的稀疏奖励强化学习？我们通过奖励矩阵中的低秩结构视角来解决这个问题，表明这种结构会导致从指数级到多项式样本复杂性的急剧转变，这是稀疏奖励强化学习领域的首个此类结果。我们引入了策略感知矩阵补全(PAMC)，通过新的策略相关采样分析将矩阵补全理论与强化学习联系起来。我们的框架提供了：(i)一般稀疏奖励观测的不可能性结果，(ii)从动态中进行的无奖励表征学习，(iii)通过保形预测实现的分布无关置信集，以及(iv)当低秩结构仅为近似时的鲁棒补全保证。实验上，我们在100个系统性采样的领域进行了预注册评估，发现超过一半的领域存在可利用的结构。与强大的探索、结构和表征学习基线相比，PAMC将样本效率提高了1.6到2.1倍，同时仅增加了约20%的计算开销。这些结果将结构化奖励学习确立为一个有前景的新范式，对机器人技术、医疗保健和其他安全关键、样本密集型应用有直接影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; What fundamental properties of reward functions enable efficientsparse-reward reinforcement learning? We address this question through the lensof low-rank structure in reward matrices, showing that such structure induces asharp transition from exponential to polynomial sample complexity, the firstresult of this kind for sparse-reward RL. We introduce Policy-Aware MatrixCompletion (PAMC), which connects matrix completion theory with reinforcementlearning via a new analysis of policy-dependent sampling. Our frameworkprovides: (i) impossibility results for general sparse reward observation, (ii)reward-free representation learning from dynamics, (iii) distribution-freeconfidence sets via conformal prediction, and (iv) robust completion guaranteesthat degrade gracefully when low-rank structure is only approximate.Empirically, we conduct a pre-registered evaluation across 100 systematicallysampled domains, finding exploitable structure in over half. PAMC improvessample efficiency by factors between 1.6 and 2.1 compared to strongexploration, structured, and representation-learning baselines, while addingonly about 20 percent computational overhead.These results establish structuralreward learning as a promising new paradigm, with immediate implications forrobotics, healthcare, and other safety-critical, sample-expensive applications.</description>
      <author>example@mail.com (Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma)</author>
      <guid isPermaLink="false">2509.03790v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Multi Attribute Bias Mitigation via Representation Learning</title>
      <link>http://arxiv.org/abs/2509.03616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECAI 2025 (28th European Conference on Artificial Intelligence)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为广义多偏见缓解（GMBM）的框架，用于解决真实世界图像中多种重叠偏见的问题，提高视觉模型的鲁棒性和公平性。&lt;h4&gt;背景&lt;/h4&gt;真实世界的图像经常表现出多种重叠的偏见，包括纹理、水印、性别化妆、场景对象配对等。这些偏见共同损害了现代视觉模型的性能，影响了它们的鲁棒性和公平性。单独处理这些偏见是不够的，因为减轻一种偏见往往会允许或加剧其他偏见。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时处理多种重叠偏见的框架，提高视觉模型在复杂场景下的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;提出广义多偏见缓解（GMBM）框架，这是一个两阶段方法：1）自适应偏见集成学习（ABIL）通过训练特定属性的编码器并集成到主网络中，使分类器明确识别这些偏见；2）梯度抑制微调从骨干网络梯度中修剪这些偏见方向，形成单一紧凑网络。同时引入了缩放偏见放大（SBA）作为新的评估指标，将模型诱导的偏见放大与分布差异分离开来。&lt;h4&gt;主要发现&lt;/h4&gt;在FB CMNIST、CelebA和COCO数据集上验证显示，GMBM提高了最差组的准确性，将多属性偏见放大减半，并在偏见复杂性和分布偏移加剧的情况下设定了新的SBA低点。&lt;h4&gt;结论&lt;/h4&gt;GMBM是视觉识别中第一个实用的、端到端的多偏见解决方案，能够有效处理复杂场景中的多种重叠偏见问题。&lt;h4&gt;翻译&lt;/h4&gt;真实世界的图像经常表现出多种重叠的偏见，包括纹理、水印、性别化妆、场景对象配对等。这些偏见共同损害了现代视觉模型的性能，影响了它们的鲁棒性和公平性。单独解决这些偏见证明是不够的，因为减轻一种偏见往往会允许或加剧其他偏见。我们通过广义多偏见缓解（GMBM）解决这一多偏见问题，这是一个简洁的两阶段框架，只需要在训练时使用组标签，并在测试时最小化偏见。首先，自适应偏见集成学习（ABIL）通过为每个属性训练编码器并将它们与主骨干网络集成，故意识别已知捷径的影响，迫使分类器明确识别这些偏见。然后，梯度抑制微调从骨干网络的梯度中修剪这些偏见方向，留下一个单一的紧凑网络，忽略它刚刚学会识别的所有捷径。此外，我们发现现有的偏见指标在子组不平衡和训练-测试分布偏移的情况下会失效，因此我们引入了缩放偏见放大（SBA）：一种测试时度量，将模型诱导的偏见放大与分布差异分离开来。我们在FB CMNIST、CelebA和COCO上验证了GMBM，提高了最差组的准确性，将多属性偏见放大减半，并在偏见复杂性和分布偏移加剧的情况下设定了新的SBA低点，使GMBM成为视觉识别中第一个实用的、端到端的多偏见解决方案。项目页面：http://visdomlab.github.io/GMBM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real world images frequently exhibit multiple overlapping biases, includingtextures, watermarks, gendered makeup, scene object pairings, etc. These biasescollectively impair the performance of modern vision models, undermining boththeir robustness and fairness. Addressing these biases individually provesinadequate, as mitigating one bias often permits or intensifies others. Wetackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), alean two stage framework that needs group labels only while training andminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)deliberately identifies the influence of known shortcuts by training encodersfor each attribute and integrating them with the main backbone, compelling theclassifier to explicitly recognize these biases. Then Gradient Suppression FineTuning prunes those very bias directions from the backbone's gradients, leavinga single compact network that ignores all the shortcuts it just learned torecognize. Moreover we find that existing bias metrics break under subgroupimbalance and train test distribution shifts, so we introduce Scaled BiasAmplification (SBA): a test time measure that disentangles model induced biasamplification from distributional differences. We validate GMBM on FB CMNIST,CelebA, and COCO, where we boost worst group accuracy, halve multi attributebias amplification, and set a new low in SBA even as bias complexity anddistribution shifts intensify, making GMBM the first practical, end to endmultibias solution for visual recognition. Project page:http://visdomlab.github.io/GMBM/</description>
      <author>example@mail.com (Rajeev Ranjan Dwivedi, Ankur Kumar, Vinod K Kurmi)</author>
      <guid isPermaLink="false">2509.03616v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</title>
      <link>http://arxiv.org/abs/2509.03614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 1 figures, final submission for MIDOG 2025 challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于教师-学生模型的统一框架，通过像素级分割和分类相结合的方法，解决了有丝分裂检测中的领域迁移和数据不平衡问题，实现了较高的检测和分类性能。&lt;h4&gt;背景&lt;/h4&gt;计数有丝分裂图像对病理学家来说是耗时的，并且会导致观察者之间的变异性。人工智能(AI)有望通过自动检测有丝分裂同时保持决策一致性来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;将有丝分裂检测制定为像素级分割问题，并提出一种教师-学生模型，同时解决有丝分裂检测(赛道1)和非典型有丝分裂分类(赛道2)。&lt;h4&gt;方法&lt;/h4&gt;基于UNet分割主干网络，集成领域泛化模块(对比表示学习和领域对抗训练)。采用教师-学生策略生成像素级伪掩码，不仅用于注释的有丝分裂和困难负样本，也用于正常细胞核。引入多尺度CNN分类器，利用分割模型的特征图，在多任务学习范式中工作。&lt;h4&gt;主要发现&lt;/h4&gt;在初步测试集上，赛道1的算法F1得分为0.7660，赛道2的平衡准确率为0.8414，表明该方法在稳健有丝分裂分析中的有效性。&lt;h4&gt;结论&lt;/h4&gt;将基于分割的检测和分类整合到一个统一框架中，能够有效解决有丝分裂检测中的领域迁移和数据不平衡问题，提高检测和分类的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;计数有丝分裂图像对病理学家来说是耗时的，并且会导致观察者之间的变异性。人工智能(AI)有望通过自动检测有丝分裂同时保持决策一致性来解决这个问题。然而，AI工具容易受到领域迁移的影响，由于训练集和测试集之间的差异，包括器官之间的形态差异、物种差异和染色方案变化，可能导致性能显著下降。此外，有丝分裂数量远少于正常细胞核数量，这为检测任务引入了严重的数据不平衡。在本工作中，我们将有丝分裂检测制定为像素级分割问题，并提出了一种教师-学生模型，同时解决有丝分裂检测(赛道1)和非典型有丝分裂分类(赛道2)。我们的方法基于UNet分割主干网络，集成了领域泛化模块，即对比表示学习和领域对抗训练。采用教师-学生策略生成像素级伪掩码，不仅用于注释的有丝分裂和困难负样本，也用于正常细胞核，从而增强特征区分能力并提高对领域迁移的鲁棒性。对于分类任务，我们引入了一个多尺度CNN分类器，在多任务学习范式中利用分割模型的特征图。在初步测试集上，算法在赛道1上实现了0.7660的F1分数，在赛道2上实现了0.8414的平衡准确率，证明了将基于分割的检测和分类整合到统一框架中对稳健有丝分裂分析的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Counting mitotic figures is time-intensive for pathologists and leads tointer-observer variability. Artificial intelligence (AI) promises a solution byautomatically detecting mitotic figures while maintaining decision consistency.However, AI tools are susceptible to domain shift, where a significant drop inperformance can occur due to differences in the training and testing sets,including morphological diversity between organs, species, and variations instaining protocols. Furthermore, the number of mitoses is much less than thecount of normal nuclei, which introduces severely imbalanced data for thedetection task. In this work, we formulate mitosis detection as a pixel-levelsegmentation and propose a teacher-student model that simultaneously addressesmitosis detection (Track 1) and atypical mitosis classification (Track 2). Ourmethod is based on a UNet segmentation backbone that integrates domaingeneralization modules, namely contrastive representation learning anddomain-adversarial training. A teacher-student strategy is employed to generatepixel-level pseudo-masks not only for annotated mitoses and hard negatives butalso for normal nuclei, thereby enhancing feature discrimination and improvingrobustness against domain shift. For the classification task, we introduce amulti-scale CNN classifier that leverages feature maps from the segmentationmodel within a multi-task learning paradigm. On the preliminary test set, thealgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of0.8414 in Track 2, demonstrating the effectiveness of integratingsegmentation-based detection and classification into a unified framework forrobust mitosis analysis.</description>
      <author>example@mail.com (Seungho Choe, Xiaoli Qin, Abubakr Shafique, Amanda Dy, Dimitri Androutsos, Susan Done, April Khademi)</author>
      <guid isPermaLink="false">2509.03614v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.03837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE GLOBECOM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种轻量级的鸟瞰图(BEV)注入连接器，用于改善车对基础设施(V2I)系统中通信链路质量指标的预测准确性。通过结合多模态大语言模型与三维空间理解能力，该方法在多种场景下显著提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;准确预测通信链路质量指标对V2I系统至关重要，可支持无缝切换、高效波束管理和可靠低延迟通信。现代车辆传感器数据的可用性增加促使使用多模态大语言模型，但这些模型缺乏三维空间理解能力。&lt;h4&gt;目的&lt;/h4&gt;克服MLLMs缺乏三维空间理解的限制，提高V2I系统中通信链路质量预测的准确性，特别是在恶劣环境下的性能表现。&lt;h4&gt;方法&lt;/h4&gt;提出一种轻量级的即插即用BEV注入连接器，收集邻近车辆传感数据构建环境BEV，并将其与自车输入融合以提供空间上下文。开发结合CARLA仿真器和MATLAB射线追踪的协同仿真环境，生成多模态数据并提取指令和真实响应进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;BEV注入框架在三个V2I链路预测任务(LoS与非LoS分类、链路可用性和阻塞预测)中一致提高了性能。与仅自车的基线相比，准确度指标宏平均提高高达13.9%，在雨天和夜间等挑战性条件下性能增益提高高达32.7%。&lt;h4&gt;结论&lt;/h4&gt;所提出的BEV注入框架有效提升了V2I链路预测性能，特别是在恶劣条件下表现出更强的鲁棒性，证明其在实际应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;准确预测通信链路质量指标对车对基础设施(V2I)系统至关重要，可实现无缝切换、高效波束管理和可靠的低延迟通信。现代车辆传感器数据日益增多促使使用多模态大语言模型，因其跨任务适应性和推理能力。然而，MLLMs本质上缺乏三维空间理解能力。为克服这一局限，提出了一种轻量级即插即用的鸟瞰图(BEV)注入连接器。在该框架中，通过收集邻近车辆的传感数据构建环境鸟瞰图。然后将此BEV表示与自车输入融合，为大型语言模型提供空间上下文。为支持真实的多模态学习，开发了结合CARLA仿真器和基于MATLAB的射线追踪的协同仿真环境，在不同场景中生成RGB、LiDAR、GPS和无线信号数据。指令和真实响应从射线追踪输出中程序化提取。在三个V2I链路预测任务上进行了广泛实验：视距(LoS)与非视距(NLoS)分类、链路可用性和阻塞预测。仿真结果表明，所提出的BEV注入框架在所有任务中一致提高了性能。结果表明，与仅自车的基线相比，该方法将准确度指标的宏平均提高了高达13.9%。结果还显示，在具有挑战性的雨天和夜间条件下，这种性能增益提高了高达32.7%，证实了该框架在不利环境中的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决车辆到基础设施（V2I）系统中的通信链路质量预测问题。这个问题在现实中非常重要，因为准确的链路质量预测可以实现无缝切换、高效的波束管理和可靠的低延迟通信，对于自动驾驶和6G网络等应用至关重要。随着现代车辆传感器数据的日益丰富，多模态大型语言模型（MLLMs）因其适应性和推理能力而受到关注，但它们缺乏三维空间理解能力，这是车辆通信和环境感知的关键限制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性：特定任务的融合链难以扩展，LLMs缺乏空间感知能力，以及现有方法通常只关注单一任务。基于这些认识，作者设计了一个轻量级、即插即用的鸟瞰图（BEV）注入连接器，通过收集邻近车辆的传感数据构建环境BEV表示，并与自车输入融合。作者借鉴了多项现有工作：使用BEVFusion框架融合RGB和LiDAR数据，采用BEVFormer的时序自注意力机制处理时间序列，以及利用Q-Former蒸馏与指令相关的空间特征。同时，作者结合了CARLA模拟器和MATLAB射线追踪来生成训练数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多车辆协作感知扩展单个车辆的视野，解决盲点问题；使用鸟瞰图（BEV）表示统一不同车辆视角下的空间信息；将空间信息压缩为紧凑的令牌序列注入到冻结的大型语言模型中；同时保持大型语言模型和视觉编码器冻结，只训练轻量级的连接器。整体流程包括：1）各车辆收集RGB图像、LiDAR点云和GPS数据；2）自车使用BEVFusion生成BEV表示；3）协作者车辆生成局部BEV特征并转换到自车坐标系；4）融合所有协作者的BEV；5）使用Q-Former提取与指令相关的空间特征；6）将BEV令牌与视觉流融合后输入LLM生成预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）即插即用的BEV注入连接器，无需端到端重新训练；2）协作BEV融合用于链路质量预测，首次将多智能体协作纳入多模态LLM框架；3）专门的V2I MLLM数据集，结合CARLA模拟和MATLAB射线追踪。相比之前的工作，本文方法不依赖特定任务的融合链，解决了单车辆盲点问题，能够处理多个下游任务，并且在恶劣条件下表现出更强的鲁棒性。之前的解决方案难以扩展到新模态或任务，而本文方法通过轻量级连接器实现了更好的灵活性和性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于多模态大型语言模型的车辆到基础设施协同空间感知方法，通过轻量级的鸟瞰图注入连接器使冻结的大型语言模型获得三维空间理解能力，显著提高了通信链路质量预测的准确性和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of communication link quality metrics is essential forvehicle-to-infrastructure (V2I) systems, enabling smooth handovers, efficientbeam management, and reliable low-latency communication. The increasingavailability of sensor data from modern vehicles motivates the use ofmultimodal large language models (MLLMs) because of their adaptability acrosstasks and reasoning capabilities. However, MLLMs inherently lackthree-dimensional spatial understanding. To overcome this limitation, alightweight, plug-and-play bird's-eye view (BEV) injection connector isproposed. In this framework, a BEV of the environment is constructed bycollecting sensing data from neighboring vehicles. This BEV representation isthen fused with the ego vehicle's input to provide spatial context for thelarge language model. To support realistic multimodal learning, a co-simulationenvironment combining CARLA simulator and MATLAB-based ray tracing is developedto generate RGB, LiDAR, GPS, and wireless signal data across varied scenarios.Instructions and ground-truth responses are programmatically extracted from theray-tracing outputs. Extensive experiments are conducted across three V2I linkprediction tasks: line-of-sight (LoS) versus non-line-of-sight (NLoS)classification, link availability, and blockage prediction. Simulation resultsshow that the proposed BEV injection framework consistently improvedperformance across all tasks. The results indicate that, compared to anego-only baseline, the proposed approach improves the macro-average of theaccuracy metrics by up to 13.9%. The results also show that this performancegain increases by up to 32.7% under challenging rainy and nighttime conditions,confirming the robustness of the framework in adverse settings.</description>
      <author>example@mail.com (Kimia Ehsani, Walid Saad)</author>
      <guid isPermaLink="false">2509.03837v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2509.03635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Reg3D，一种新颖的重建几何指令调优框架，通过引入几何感知监督解决了大型多模态模型在3D场景理解方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型(LMMs)在2D视觉理解方面取得了显著进展，但将其能力扩展到3D场景理解仍面临重大挑战。现有方法主要依赖纯文本监督，无法提供学习鲁棒3D空间表示所需的几何约束。&lt;h4&gt;目的&lt;/h4&gt;引入Reg3D框架，通过在训练过程中直接融入几何感知监督，解决现有方法在3D场景理解中的局限性。&lt;h4&gt;方法&lt;/h4&gt;Reg3D采用双监督范式，同时利用3D几何信息作为输入和学习目标。在双编码器架构中设计互补的物体级和帧级重建任务，强制几何一致性以促进空间推理能力的发展。核心见解是有效的3D理解需要重建底层几何结构，而不仅仅是描述它们。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanQA、Scan2Cap、ScanRefer和SQA3D数据集上的广泛实验表明，Reg3D带来了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;Reg3D建立了具有空间感知能力的多模态模型的新训练范式，为3D场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型多模态模型(LMMs)的快速发展已在2D视觉理解方面取得了显著进展；然而，将这些能力扩展到3D场景理解仍然是一个重大挑战。现有方法主要依赖纯文本监督，无法提供学习鲁棒3D空间表示所需的几何约束。在本文中，我们介绍了Reg3D，一种新颖的重建几何指令调优框架，通过在训练过程中直接引入几何感知监督来解决这一局限性。我们的关键见解是，有效的3D理解需要重建底层几何结构，而不仅仅是描述它们。与仅在输入级别注入3D信息的方法不同，Reg3D采用双监督范式，同时利用3D几何信息作为输入和明确的学习目标。具体而言，我们在双编码器架构中设计了互补的物体级和帧级重建任务，强制几何一致性以促进空间推理能力的发展。在ScanQA、Scan2Cap、ScanRefer和SQA3D上的广泛实验表明，Reg3D带来了显著的性能提升，建立了具有空间感知能力的多模态模型的新训练范式。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将大型多模态模型(LMMs)在2D视觉理解方面的成功扩展到3D场景理解的问题。这个问题很重要，因为3D场景理解是人工智能的关键挑战，对于机器人导航、增强现实、自动驾驶等应用至关重要，而现有方法受限于2D归纳偏置，无法有效理解和推理3D空间关系。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，指出当前方法主要在输入层面注入3D信息但无法发展真正的3D空间感知能力。作者的关键见解是有效的3D理解需要重建底层几何结构而不仅仅是描述它们。作者借鉴了预训练的3D基础模型(如VGGT)和双编码器架构，但创新性地设计了双重监督范式，将3D几何信息同时作为输入和监督信号，并引入了对象级和帧级两种互补的重建任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是有效的3D理解需要重建底层几何结构，而非仅描述它们，通过将3D几何信息同时作为输入和监督信号来克服2D偏置。整体流程包括：1)双编码器架构(2D视觉编码器和3D几何编码器)；2)两种重建任务(对象级重建促进跨视图空间推理，帧级重建发展全面场景理解)；3)自适应帧采样策略；4)训练时遮蔽部分3D特征，要求模型基于2D视觉和未遮蔽3D信息重建遮蔽部分，结合文本损失和重建损失进行多任务训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出重建几何指令调准(Reg3D)框架；2)设计双重监督范式，将3D几何信息同时作为输入和监督信号；3)引入对象级和帧级两种互补重建任务；4)设计自适应帧采样策略。相比之前工作，Reg3D不仅将3D信息作为输入，还将其作为明确的监督信号；不仅依赖文本监督，还引入几何感知监督；不仅在输入层面注入3D信息，还在整个训练过程中整合3D几何约束；通过重建任务强制模型推理3D空间关系，而非仅描述它们。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Reg3D通过引入重建几何指令调准框架和双重监督范式，有效解决了大型多模态模型在3D场景理解中的2D偏置问题，显著提升了模型对3D空间关系的理解和推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Large Multimodal Models (LMMs) has led to remarkableprogress in 2D visual understanding; however, extending these capabilities to3D scene understanding remains a significant challenge. Existing approachespredominantly rely on text-only supervision, which fails to provide thegeometric constraints required for learning robust 3D spatial representations.In this paper, we introduce Reg3D, a novel Reconstructive Geometry InstructionTuning framework that addresses this limitation by incorporating geometry-awaresupervision directly into the training process. Our key insight is thateffective 3D understanding necessitates reconstructing underlying geometricstructures rather than merely describing them. Unlike existing methods thatinject 3D information solely at the input level, Reg3D adopts adual-supervision paradigm that leverages 3D geometric information both as inputand as explicit learning targets. Specifically, we design complementaryobject-level and frame-level reconstruction tasks within a dual-encoderarchitecture, enforcing geometric consistency to encourage the development ofspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performanceimprovements, establishing a new training paradigm for spatially awaremultimodal models.</description>
      <author>example@mail.com (Hongpei Zheng, Lintao Xiang, Qijun Yang, Qian Lin, Hujun Yin)</author>
      <guid isPermaLink="false">2509.03635v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks</title>
      <link>http://arxiv.org/abs/2509.02175v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RocketScience是一个开源的对比VLM基准测试，用于测试空间关系理解能力，由新的真实世界图像-文本对组成，主要涵盖相对空间理解和对象顺序。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在空间关系理解方面存在明显不足，需要一个专门的基准测试来评估这一能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个对人类容易但对当前VLMs具有挑战性的基准测试，专门评估空间关系理解能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个由全新真实世界图像-文本对组成的基准测试，专注于相对空间理解和对象顺序，并进行了解缠分析以分离对象定位和空间推理的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;开源和前沿商业VLMs在空间关系理解方面表现明显不足，而推理模型表现出令人惊讶的高性能；基准测试的性能瓶颈在于空间推理能力，而非对象定位能力。&lt;h4&gt;结论&lt;/h4&gt;当前VLMs在空间关系理解方面存在显著缺陷，需要进一步改进这一能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了RocketScience，一个开源的对比VLM基准测试，用于测试空间关系理解能力。它完全由新的真实世界图像-文本对组成，主要涵盖相对空间理解和对象顺序。该基准测试设计为对人类很容易，但对当前一代VLMs来说很难，这一点已得到经验验证。我们的结果表明，开源和前沿商业VLMs在空间关系理解方面明显缺乏，而推理模型表现出令人惊讶的高性能。此外，我们进行了解缠分析，以分离基于思维链模型中的对象定位和空间推理的贡献，发现基准测试的性能瓶颈在于空间推理，而不是对象定位能力。我们以CC-BY-4.0许可证发布数据集，并在https://github.com/nilshoehing/rocketscience提供评估代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose RocketScience, an open-source contrastive VLM benchmark that testsfor spatial relation understanding. It is comprised of entirely new real-worldimage-text pairs covering mostly relative spatial understanding and the orderof objects. The benchmark is designed to be very easy for humans and hard forthe current generation of VLMs, and this is empirically verified. Our resultsshow a striking lack of spatial relation understanding in open source andfrontier commercial VLMs and a surprisingly high performance of reasoningmodels. Additionally, we perform a disentanglement analysis to separate thecontributions of object localization and spatial reasoning inchain-of-thought-based models and find that the performance on the benchmark isbottlenecked by spatial reasoning and not object localization capabilities. Werelease the dataset with a CC-BY-4.0 license and make the evaluation codeavailable at: https://github.com/nilshoehing/rocketscience</description>
      <author>example@mail.com (Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque)</author>
      <guid isPermaLink="false">2509.02175v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</title>
      <link>http://arxiv.org/abs/2508.17832v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HLG是一种创新的细粒度3D室内场景生成方法，通过分层布局和优化网络解决了现有方法在细节生成方面的不足，提高了生成场景的真实性和实用性。&lt;h4&gt;背景&lt;/h4&gt;真实的3D室内场景生成对虚拟现实、室内设计、具身智能和场景理解至关重要。现有方法在粗粒度家具布局方面取得了进展，但难以捕捉细粒度的物体放置，限制了生成环境的真实性和实用性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在细粒度3D场景生成方面的不足，提出一种能够生成更真实、更详细的3D室内场景的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了分层布局生成（HLG）方法，采用粗到细的分层方法优化场景布局；细粒度布局对齐模块通过垂直和水平解耦构建分层布局；可训练布局优化网络解决了放置问题，确保结构连贯且物理上合理的场景生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明了HLG方法的有效性，与现有方法相比，在生成真实室内场景方面表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了场景生成领域的发展，为需要详细3D环境的应用开辟了新的可能性，作者将在发表后发布代码以鼓励未来研究。&lt;h4&gt;翻译&lt;/h4&gt;真实的3D室内场景生成对虚拟现实、室内设计、具身智能和场景理解至关重要。虽然现有方法在粗粒度家具布置方面取得了进展，但它们难以捕捉细粒度的物体放置，限制了生成环境的真实性和实用性。这一差距阻碍了沉浸式虚拟体验和具身AI应用的详细场景理解。为解决这些问题，我们提出了分层布局生成（HLG），一种用于细粒度3D场景生成的新方法。HLG首次采用粗到细的分层方法，从大规模家具布置到精细物体排列来优化场景布局。具体而言，我们的细粒度布局对齐模块通过垂直和水平解耦构建分层布局，有效将复杂的3D室内场景分解为多个粒度级别。此外，我们的可训练布局优化网络解决了放置问题，如错误定位、方向错误和物体相交，确保结构连贯且物理上合理的场景生成。我们通过大量实验证明了我们方法的有效性，显示出与现有方法相比在生成真实室内场景方面的优越性能。这项工作推进了场景生成领域的发展，并为需要详细3D环境的应用开辟了新的可能性。我们将在发表后发布代码以鼓励未来的研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D室内场景生成方法在精细粒度物体放置方面的不足问题。现有方法能够生成大尺度家具布局，但难以精确处理小物体（如餐具、装饰品）的摆放，这限制了生成环境的真实性和实用性。这个问题在现实中很重要，因为精细的3D场景对虚拟现实体验、室内设计应用和具身AI系统的场景理解都至关重要，高质量的3D环境能提供更沉浸式的虚拟体验和更准确的场景表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D室内场景生成方法的局限性，结合基于生成模型和大型语言模型(LLM)的优势，设计了层次化布局生成方法。作者借鉴了现有生成模型的空间特征先验和LLM的语言理解能力，但发现现有层次化方法缺乏对小型物体放置的精细控制。因此，作者设计了一个从粗粒度到细粒度的生成框架，包括场景信息提取、粗粒度房间生成和层次化布局生成三个阶段，通过垂直和水平解耦将复杂场景分解为多个层次，并引入可训练的布局优化网络来解决放置问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; HLG的核心思想是通过层次化方式从粗粒度家具布置逐步细化到精细物体放置，将复杂3D室内场景分解为多个层次的粒度。整体流程分为三阶段：1)场景信息提取：使用GPT-4o从多模态输入中提取房间类型、家具类型和空间关系；2)粗粒度房间生成：构建基本房间结构和家具的初始布局；3)层次化布局生成：通过垂直解耦沿Z轴分解场景为独立层，水平解耦利用LLM构建场景图，最后使用可训练布局优化网络(TLO-Net)优化物体位置，确保物理合理性和输入一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)层次化布局生成方法，实现从粗到细的逐步细化；2)精细布局对齐模块，通过垂直和水平解耦处理复杂场景；3)可训练布局优化网络(TLO-Net)，使用图注意力网络和复合损失函数优化物体位置。相比之前工作，HLG能处理精细小物体放置，支持非矩形房间，使用可微优化而非规则约束，能处理多模态输入，并通过碰撞和稳定性损失确保物理合理性，而不仅仅是视觉上的合理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HLG通过层次化布局生成方法，解决了3D室内场景生成中精细物体放置的挑战，实现了从粗粒度家具布置到细粒度物体定位的高质量场景构建，为虚拟现实、室内设计和具身智能应用提供了更加真实和实用的3D环境。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realistic 3D indoor scene generation is crucial for virtual reality, interiordesign, embodied intelligence, and scene understanding. While existing methodshave made progress in coarse-scale furniture arrangement, they struggle tocapture fine-grained object placements, limiting the realism and utility ofgenerated environments. This gap hinders immersive virtual experiences anddetailed scene comprehension for embodied AI applications. To address theseissues, we propose Hierarchical Layout Generation (HLG), a novel method forfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-finehierarchical approach, refining scene layouts from large-scale furnitureplacement to intricate object arrangements. Specifically, our fine-grainedlayout alignment module constructs a hierarchical layout through vertical andhorizontal decoupling, effectively decomposing complex 3D indoor scenes intomultiple levels of granularity. Additionally, our trainable layout optimizationnetwork addresses placement issues, such as incorrect positioning, orientationerrors, and object intersections, ensuring structurally coherent and physicallyplausible scene generation. We demonstrate the effectiveness of our approachthrough extensive experiments, showing superior performance in generatingrealistic indoor scenes compared to existing methods. This work advances thefield of scene generation and opens new possibilities for applicationsrequiring detailed 3D environments. We will release our code upon publicationto encourage future research.</description>
      <author>example@mail.com (Xiping Wang, Yuxi Wang, Mengqi Zhou, Junsong Fan, Zhaoxiang Zhang)</author>
      <guid isPermaLink="false">2508.17832v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding</title>
      <link>http://arxiv.org/abs/2509.03939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LMAE4Eth的多视图学习框架，用于以太坊欺诈检测，通过融合交易语义、掩码图嵌入和专家知识，解决了现有方法在捕捉交易语义、学习判别性账户嵌入以及处理大规模数据方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前以太坊欺诈检测方法依赖于上下文无关的数值交易序列，无法捕捉账户交易的语义信息；以太坊交易记录的普遍同质性使得学习判别性账户嵌入变得困难；现有自监督图学习方法主要通过图重构学习节点表示，在欺诈账户检测等节点级任务上表现不佳，同时面临可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有以太坊欺诈检测方法的局限性，提出能够捕捉交易语义的框架，学习更具表现力的账户表示，提高节点级账户检测性能，并解决可扩展性问题。&lt;h4&gt;方法&lt;/h4&gt;提出LMAE4Eth多视图学习框架，包含：1)交易-标记对比语言模型(TxCLM)，将数值交易记录转换为语言表示；2)使用标记感知对比学习和掩码交易模型预训练目标学习高表现力账户表示；3)掩码账户图自编码器(MAGAE)，通过生成式自监督学习专注于重构账户节点特征；4)集成层邻居采样提高可扩展性；5)使用交叉注意力融合网络统一TxCLM和MAGAE的嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上与21种基线方法进行评估，结果显示该方法在两个数据集上的F1分数比最佳基线方法高出10%以上。&lt;h4&gt;结论&lt;/h4&gt;LMAE4Eth框架通过融合交易语义和图学习方法，有效提高了以太坊欺诈检测的准确性，同时提出的层邻居采样方法解决了可扩展性问题，使该方法能够处理大规模数据。&lt;h4&gt;翻译&lt;/h4&gt;当前以太坊欺诈检测方法依赖于上下文无关的数值交易序列，无法捕捉账户交易的语义。此外，以太坊交易记录的普遍同质性使得学习判别性账户嵌入具有挑战性。而且，当前自监督图学习方法主要通过图重构来学习节点表示，导致在欺诈账户检测等节点级任务上表现不佳，同时这些方法还面临可扩展性挑战。为应对这些挑战，我们提出了LMAE4Eth，一个融合交易语义、掩码图嵌入和专家知识的多视图学习框架。我们首先提出交易-标记对比语言模型(TxCLM)，将上下文无关的数值交易记录转换为逻辑连贯的语言表示。为了清晰地区分账户间的语义差异，我们还使用标记感知的对比学习预训练目标结合掩码交易模型预训练目标，学习高表现力的账户表示。然后，我们提出使用生成式自监督学习的掩码账户图自编码器(MAGAE)，通过专注于重构账户节点特征实现卓越的节点级账户检测。为使MAGAE能够进行大规模训练，我们提出将层邻居采样集成到图中，在不降低训练质量的情况下将采样顶点数量减少数倍。最后，使用交叉注意力融合网络，我们统一了TxCLM和MAGAE的嵌入，以利用两者的优势。我们在三个数据集上对21种基线方法评估了我们的方法。实验结果表明，在两个数据集上，我们的方法的F1分数比最佳基线方法高出10%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Ethereum fraud detection methods rely on context-independent,numerical transaction sequences, failing to capture semantic of accounttransactions. Furthermore, the pervasive homogeneity in Ethereum transactionrecords renders it challenging to learn discriminative account embeddings.Moreover, current self-supervised graph learning methods primarily learn noderepresentations through graph reconstruction, resulting in suboptimalperformance for node-level tasks like fraud account detection, while thesemethods also encounter scalability challenges. To tackle these challenges, wepropose LMAE4Eth, a multi-view learning framework that fuses transactionsemantics, masked graph embedding, and expert knowledge. We first propose atransaction-token contrastive language model (TxCLM) that transformscontext-independent numerical transaction records into logically cohesivelinguistic representations. To clearly characterize the semantic differencesbetween accounts, we also use a token-aware contrastive learning pre-trainingobjective together with the masked transaction model pre-training objective,learns high-expressive account representations. We then propose a maskedaccount graph autoencoder (MAGAE) using generative self-supervised learning,which achieves superior node-level account detection by focusing onreconstructing account node features. To enable MAGAE to scale for large-scaletraining, we propose to integrate layer-neighbor sampling into the graph, whichreduces the number of sampled vertices by several times without compromisingtraining quality. Finally, using a cross-attention fusion network, we unify theembeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate ourmethod against 21 baseline approaches on three datasets. Experimental resultsshow that our method outperforms the best baseline by over 10% in F1-score ontwo of the datasets.</description>
      <author>example@mail.com (Yifan Jia, Yanbin Wang, Jianguo Sun, Ye Tian, Peng Qian)</author>
      <guid isPermaLink="false">2509.03939v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-Supervised Learning of Dense Functional Correspondences</title>
      <link>http://arxiv.org/abs/2509.03893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025. Project website:  https://dense-functional-correspondence.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于物体功能的密集对应关系建立方法，利用视觉-语言模型和密集对比学习来解决跨类别图像匹配问题。&lt;h4&gt;背景&lt;/h4&gt;建立图像对之间的密集对应关系对于形状重建和机器人操作等任务至关重要，但在不同类别之间匹配时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;推导密集功能对应的定义，并提出一种弱监督学习范式来解决跨类别图像匹配中的对应关系建立问题。&lt;h4&gt;方法&lt;/h4&gt;利用视觉-语言模型为多视图图像伪标记以获得功能部分，然后与像素对应的密集对比学习相结合，将功能和空间知识提炼到新模型中。&lt;h4&gt;主要发现&lt;/h4&gt;物体的功能可以指导对应关系的建立，因为实现特定功能的物体部分通常在形状和外观上具有相似性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成和真实评估数据集上表现优于基线解决方案，证明了其在密集功能对应任务上的优势。&lt;h4&gt;翻译&lt;/h4&gt;在图像对之间建立密集对应关系对于形状重建和机器人操作等任务至关重要。在跨不同类别匹配的挑战性场景下，物体的功能即物体对其他物体产生的影响，可以指导对应关系的建立。这是因为实现特定功能的物体部分通常在形状和外观上具有相似性。我们基于这一观察推导出密集功能对应的定义，并提出一种弱监督学习范式来解决预测任务。我们方法的主要见解是，可以利用视觉-语言模型为多视图图像伪标记以获得功能部分。然后将其与像素对应的密集对比学习相结合，将功能和空间知识提炼到能够建立密集功能对应的新模型中。此外，我们整理了合成和真实评估数据集作为任务基准。我们的结果表明，与包含现成自监督图像表示和基础视觉语言模型的基线解决方案相比，我们的方法具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing dense correspondences across image pairs is essential for taskssuch as shape reconstruction and robot manipulation. In the challenging settingof matching across different categories, the function of an object, i.e., theeffect that an object can cause on other objects, can guide how correspondencesshould be established. This is because object parts that enable specificfunctions often share similarities in shape and appearance. We derive thedefinition of dense functional correspondence based on this observation andpropose a weakly-supervised learning paradigm to tackle the prediction task.The main insight behind our approach is that we can leverage vision-languagemodels to pseudo-label multi-view images to obtain functional parts. We thenintegrate this with dense contrastive learning from pixel correspondences todistill both functional and spatial knowledge into a new model that canestablish dense functional correspondence. Further, we curate synthetic andreal evaluation datasets as task benchmarks. Our results demonstrate theadvantages of our approach over baseline solutions consisting of off-the-shelfself-supervised image representations and grounded vision language models.</description>
      <author>example@mail.com (Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu)</author>
      <guid isPermaLink="false">2509.03893v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Training LLMs to be Better Text Embedders through Bidirectional Reconstruction</title>
      <link>http://arxiv.org/abs/2509.03020v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种增强大型语言模型文本嵌入能力的新方法，通过在对比学习前添加专门训练阶段，显著提升了模型在检索和重排序任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被探索作为文本嵌入器，但现有方法通常依赖最终token（如[EOS]）的嵌入，这些token并未经过专门训练来捕获整个上下文的语义，限制了其作为文本嵌入的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的训练阶段以丰富最终token嵌入的语义，提升大型语言模型在文本嵌入任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;在对比学习之前添加一个新训练阶段，采用双向生成重建任务（EBQ2D和EBD2Q），这些任务交替进行，锚定[EOS]嵌入并重建查询-文档对中的任一侧。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，额外的训练阶段显著提高了大型语言模型在大规模文本嵌入基准（MTEB）上的性能，在不同的LLM基础模型和规模上实现了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;通过添加专门的训练阶段，可以有效提升大型语言模型作为文本嵌入器的性能，特别是在检索和重排序任务中。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）越来越多地被探索作为强大的文本嵌入器。现有的基于LLM的文本嵌入方法通常利用最终token的嵌入，通常是像[EOS]这样的保留特殊token。然而，这些token并没有经过专门训练来捕获整个上下文的语义，限制了它们作为文本嵌入的能力，特别是在检索和重排序任务中。我们提出在对比学习之前添加一个新的训练阶段，以丰富最终token嵌入的语义。这个阶段采用双向生成重建任务，即EBQ2D（基于嵌入的查询到文档）和EBD2Q（基于嵌入的文档到查询），这些任务交替进行，以锚定[EOS]嵌入并重建查询-文档对中的任一侧。实验结果表明，我们的额外训练阶段显著提高了LLM在大规模文本嵌入基准（MTEB）上的性能，在不同的LLM基础模型和规模上实现了新的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have increasingly been explored as powerful textembedders. Existing LLM-based text embedding approaches often leverage theembedding of the final token, typically a reserved special token such as [EOS].However, these tokens have not been intentionally trained to capture thesemantics of the whole context, limiting their capacity as text embeddings,especially for retrieval and re-ranking tasks. We propose to add a new trainingstage before contrastive learning to enrich the semantics of the final tokenembedding. This stage employs bidirectional generative reconstruction tasks,namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-BasedDocument-to-Query), which interleave to anchor the [EOS] embedding andreconstruct either side of Query-Document pairs. Experimental resultsdemonstrate that our additional training stage significantly improves LLMperformance on the Massive Text Embedding Benchmark (MTEB), achieving newstate-of-the-art results across different LLM base models and scales.</description>
      <author>example@mail.com (Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin)</author>
      <guid isPermaLink="false">2509.03020v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>An Interactive Framework for Finding the Optimal Trade-off in Differential Privacy</title>
      <link>http://arxiv.org/abs/2509.04290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的差分隐私(DP)多目标优化方法，通过直接建模Pareto前沿和使用更高效的交互方式来学习用户偏好，从而在隐私保护和模型性能之间找到最优平衡。&lt;h4&gt;背景&lt;/h4&gt;差分隐私(DP)是隐私保护分析的标准，但在隐私保证和模型性能之间存在基本权衡。选择这种最优平衡是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决标准多目标优化方法在差分隐私中的低效问题，提出一种能够直接利用问题独特结构的方法，以更少的计算成本和用户交互找到最优的隐私-准确性权衡。&lt;h4&gt;方法&lt;/h4&gt;首先理论上推导权衡的形状，直接高效地建模Pareto前沿；然后用更具信息量的交互替代成对比较，向用户展示假设的权衡曲线并让他们选择偏好的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;通过六个真实世界数据集上的实验，表明所提方法在差分隐私逻辑回归和深度迁移学习中，能够以显著更少的计算成本和用户交互收敛到最优的隐私-准确性权衡。&lt;h4&gt;结论&lt;/h4&gt;通过直接利用差分隐私问题的独特结构，所提出的方法能够更有效地解决隐私-准确性权衡问题，为实际应用提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;差分隐私(DP)是隐私保护分析的标准，它在隐私保证和模型性能之间引入了一种基本的权衡。选择最优平衡是一个关键挑战，可以将其表述为一个多目标优化(MOO)问题，即首先发现最优权衡的集合(Pareto前沿)，然后学习决策者对这些权衡的偏好。尽管已有大量关于交互式MOO的研究，但标准方法——使用通用代理建模目标函数并从简单的成对反馈中学习偏好——对于DP来说是低效的，因为它未能利用问题的独特结构：Pareto前沿上的一个点可以通过固定隐私水平最大化精度直接生成。受此特性启发，我们首先理论上推导了权衡的形状，这使我们能够直接且高效地建模Pareto前沿。为了解决偏好学习中的低效问题，我们用更具信息量的交互替代了成对比较。特别是，我们向用户展示假设的权衡曲线并要求他们选择偏好的权衡。我们在差分隐私逻辑回归和深度迁移学习上的实验，使用六个真实世界数据集，表明我们的方法以显著更少的计算成本和用户交互收敛到最优的隐私-准确性权衡，优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Differential privacy (DP) is the standard for privacy-preserving analysis,and introduces a fundamental trade-off between privacy guarantees and modelperformance. Selecting the optimal balance is a critical challenge that can beframed as a multi-objective optimization (MOO) problem where one firstdiscovers the set of optimal trade-offs (the Pareto front) and then learns adecision-maker's preference over them. While a rich body of work on interactiveMOO exists, the standard approach -- modeling the objective functions withgeneric surrogates and learning preferences from simple pairwise feedback -- isinefficient for DP because it fails to leverage the problem's unique structure:a point on the Pareto front can be generated directly by maximizing accuracyfor a fixed privacy level. Motivated by this property, we first derive theshape of the trade-off theoretically, which allows us to model the Pareto frontdirectly and efficiently. To address inefficiency in preference learning, wereplace pairwise comparisons with a more informative interaction. Inparticular, we present the user with hypothetical trade-off curves and ask themto pick their preferred trade-off. Our experiments on differentially privatelogistic regression and deep transfer learning across six real-world datasetsshow that our method converges to the optimal privacy-accuracy trade-off withsignificantly less computational cost and user interaction than baselines.</description>
      <author>example@mail.com (Yaohong Yang, Aki Rehn, Sammie Katt, Antti Honkela, Samuel Kaski)</author>
      <guid isPermaLink="false">2509.04290v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds</title>
      <link>http://arxiv.org/abs/2509.04166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, uses dcase2025.sty, submitted to DCASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了自监督语音模型在生物声学检测和分类任务中的迁移学习能力。研究表明，像HuBERT、WavLM和XEUS这样的模型能够生成跨物种动物声音的丰富潜在表示，其性能可与专门的生物声学预训练模型相媲美，突显了基于语音的自监督学习作为推进生物声学研究的有效框架的潜力。&lt;h4&gt;背景&lt;/h4&gt;自监督语音模型在语音处理方面已经展示了令人印象深刻的表现，但它们在非语音数据上的有效性尚未得到充分探索。生物声学领域需要有效的声音分析工具，但专门为此领域训练的模型可能需要大量标注数据。&lt;h4&gt;目的&lt;/h4&gt;研究自监督语音模型在生物声学检测和分类任务中的迁移学习能力，评估这些模型在动物声音表示生成、时间信息处理、频率范围和噪声影响方面的表现。&lt;h4&gt;方法&lt;/h4&gt;研究使用了HuBERT、WavLM和XEUS等自监督语音模型，通过线性探测分析时间平均表示的特性，并扩展到考虑时间信息的其他下游架构，同时研究了频率范围和噪声对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;自监督语音模型能够生成跨物种动物声音的丰富潜在表示；这些模型在生物声学任务上的结果与微调的生物声学预训练模型具有竞争力；噪声鲁棒预训练设置对性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;基于语音的自监督学习作为一种有效框架，在推进生物声学研究方面具有巨大潜力，为生物声学分析提供了不需要大量标注数据的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;自监督语音模型在语音处理方面已经展示了令人印象深刻的表现，但它们在非语音数据上的有效性尚未得到充分探索。我们研究了此类模型在生物声学检测和分类任务中的迁移学习能力。我们表明，如HuBERT、WavLM和XEUS等模型能够生成跨物种动物声音的丰富潜在表示。我们通过在线性探测上对时间平均表示分析模型的特性。然后，我们扩展这种方法，使用其他下游架构来考虑时间信息的影响。最后，我们研究了频率范围和噪声对性能的影响。值得注意的是，我们的结果与微调的生物声学预训练模型具有竞争力，并展示了噪声鲁棒预训练设置的影响。这些发现突显了基于语音的自监督学习作为推进生物声学研究的有效框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised speech models have demonstrated impressive performance inspeech processing, but their effectiveness on non-speech data remainsunderexplored. We study the transfer learning capabilities of such models onbioacoustic detection and classification tasks. We show that models such asHuBERT, WavLM, and XEUS can generate rich latent representations of animalsounds across taxa. We analyze the models properties with linear probing ontime-averaged representations. We then extend the approach to account for theeffect of time-wise information with other downstream architectures. Finally,we study the implication of frequency range and noise on performance. Notably,our results are competitive with fine-tuned bioacoustic pre-trained models andshow the impact of noise-robust pre-training setups. These findings highlightthe potential of speech-based self-supervised learning as an efficientframework for advancing bioacoustic research.</description>
      <author>example@mail.com (Jules Cauzinille, Marius Miron, Olivier Pietquin, Masato Hagiwara, Ricard Marxer, Arnaud Rey, Benoit Favre)</author>
      <guid isPermaLink="false">2509.04166v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture</title>
      <link>http://arxiv.org/abs/2509.03950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 page, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用深度学习的自动化气胸检测方法，通过U-Net架构与EfficientNet-B4编码器相结合，能够在胸部X光片上准确识别气胸区域。&lt;h4&gt;背景&lt;/h4&gt;气胸是胸膜腔内异常积聚空气的病症，若未及时发现可能危及生命。胸部X光片是首选诊断工具，但小型气胸病例可能表现不明显，容易漏诊。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化的深度学习管道，用于分割和识别胸部X光片中的气胸区域，辅助放射科医生进行诊断。&lt;h4&gt;方法&lt;/h4&gt;采用带有EfficientNet-B4编码器的U-Net架构，在SIIM-ACR数据集上进行训练，结合数据增强技术和二元交叉熵加Dice损失函数进行模型优化。&lt;h4&gt;主要发现&lt;/h4&gt;模型在独立的PTX-498数据集上取得了0.7008的交并比(IoU)和0.8241的Dice分数，显示出良好的气胸区域分割能力。&lt;h4&gt;结论&lt;/h4&gt;该深度学习模型能够准确定位气胸区域，可以为放射科医生提供有效的诊断支持，提高气胸检测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;气胸是胸膜腔内异常积聚空气的病症，若未被发现可能危及生命。胸部X光片是首选诊断工具，但小型病例可能表现不明显。我们提出了一种使用带有EfficientNet-B4编码器的U-Net的自动化深度学习管道，用于分割气胸区域。在SIIM-ACR数据集上训练，结合数据增强和二元交叉熵加Dice损失，该模型在独立的PTX-498数据集上达到了0.7008的IoU和0.8241的Dice分数。这些结果表明该模型可以准确定位气胸并为放射科医生提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pneumothorax, the abnormal accumulation of air in the pleural space, can belife-threatening if undetected. Chest X-rays are the first-line diagnostictool, but small cases may be subtle. We propose an automated deep-learningpipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothoraxregions. Trained on the SIIM-ACR dataset with data augmentation and a combinedbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 andDice score of 0.8241 on the independent PTX-498 dataset. These resultsdemonstrate that the model can accurately localize pneumothoraces and supportradiologists.</description>
      <author>example@mail.com (Alvaro Aranibar Roque, Helga Sebastian)</author>
      <guid isPermaLink="false">2509.03950v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns</title>
      <link>http://arxiv.org/abs/2509.03729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了三种深度学习架构（ResNet50、MobileNetV2和EfficientNetB0）基于叶片脉图模式进行植物物种自动分类的有效性，发现EfficientNetB0表现最佳，测试准确率达94.67%&lt;h4&gt;背景&lt;/h4&gt;叶片脉图是一种具有高度分类学相关性的关键形态学特征，可用于植物物种分类&lt;h4&gt;目的&lt;/h4&gt;评估三种深度学习架构在基于叶片脉图模式的植物物种自动分类中的性能表现&lt;h4&gt;方法&lt;/h4&gt;使用瑞典叶片数据集（包含15个不同物种的1,125张图像），在训练和测试阶段使用标准性能指标评估ResNet50、MobileNetV2和EfficientNetB0三种模型&lt;h4&gt;主要发现&lt;/h4&gt;ResNet50训练准确率94.11%但存在过拟合，测试准确率降至88.45%；MobileNetV2测试准确率93.34%，F1得分93.23%，适合轻量级应用；EfficientNetB0测试准确率94.67%，各项指标均超过94.6%，表现最佳&lt;h4&gt;结论&lt;/h4&gt;深度学习，特别是EfficientNetB0，在开发可扩展且准确的基于叶脉特征的自动化植物分类工具方面具有巨大潜力&lt;h4&gt;翻译&lt;/h4&gt;本研究评估了三种深度学习架构（ResNet50、MobileNetV2和EfficientNetB0）基于叶片脉图模式进行植物物种自动分类的有效性，这是一种具有高度分类学相关性的关键形态学特征。使用包含15个不同物种（每个物种75张图像，总计1,125张图像）的瑞典叶片数据集，在训练和测试阶段使用标准性能指标评估了这些模型。ResNet50达到94.11%的训练准确率，但表现出过拟合，测试准确率降至88.45%，F1得分为87.82%。MobileNetV2展示了更好的泛化能力，测试准确率达到93.34%，F1得分为93.23%，表明其适合轻量级实时应用。EfficientNetB0表现优于其他两个模型，测试准确率达到94.67%，精确率、召回率和F1分数均超过94.6%，突显了其在基于脉图的分类中的鲁棒性。这些发现强调了深度学习，特别是EfficientNetB0，在开发利用脉图特征进行自动化植物分类的可扩展且准确工具方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study evaluates the efficacy of three deep learning architectures:ResNet50, MobileNetV2, and EfficientNetB0 for automated plant speciesclassification based on leaf venation patterns, a critical morphologicalfeature with high taxonomic relevance. Using the Swedish Leaf Datasetcomprising images from 15 distinct species (75 images per species, totalling1,125 images), the models were demonstrated using standard performance metricsduring training and testing phases. ResNet50 achieved a training accuracy of94.11% but exhibited overfitting, reflected by a reduced testing accuracy of88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated bettergeneralization capabilities, attaining a testing accuracy of 93.34% and an F1score of 93.23%, indicating its suitability for lightweight, real-timeapplications. EfficientNetB0 outperformed both models, achieving a testingaccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,highlighting its robustness in venation-based classification. The findingsunderscore the potential of deep learning, particularly EfficientNetB0, indeveloping scalable and accurate tools for automated plant taxonomy usingvenation traits.</description>
      <author>example@mail.com (Bandita Bharadwaj, Ankur Mishra, Saurav Bharadwaj)</author>
      <guid isPermaLink="false">2509.03729v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-Assisted Correlation Clustering</title>
      <link>http://arxiv.org/abs/2509.03561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in IEEE QAI 2025 conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合量子-经典方法用于关联聚类，通过量子退火解决二次无约束二元优化问题，实现了在带符号图中进行高质量节点分区。&lt;h4&gt;背景&lt;/h4&gt;关联聚类是一种基于图的无监督学习任务，旨在根据节点间的一致性和不一致性对图中的节点进行分区。传统方法在处理具有负边或不平衡簇大小的图时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理任意相关结构图（包括负边）的聚类方法，不依赖于度量假设或预定义的簇数量，同时提高聚类质量和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将GCS-Q量子辅助求解器调整为关联聚类任务，通过递归分裂分区来最大化簇内一致性。每个二分区步骤被编码为二次无约束二元优化问题，并通过量子退火解决。&lt;h4&gt;主要发现&lt;/h4&gt;在合成带符号图和真实世界高光谱成像数据上的实证评估表明，调整后的GCS-Q在真实数据上的鲁棒性和聚类质量以及在簇大小不平衡的场景中优于经典算法。&lt;h4&gt;结论&lt;/h4&gt;混合量子-经典优化在推进基于图的无监督学习中可扩展和结构感知的聚类技术方面具有潜力，特别是在处理复杂图结构时。&lt;h4&gt;翻译&lt;/h4&gt;这项工作引入了一种混合量子-经典方法用于关联聚类，这是一种基于图的无监督学习任务，旨在根据节点间的一致性和不一致性对图中的节点进行分区。特别是，我们将GCS-Q（一种最初为联盟结构生成设计的量子辅助求解器）调整为通过递归分裂分区来最大化带符号图中的簇内一致性。所提出的方法将每个二分区步骤编码为二次无约束二元优化问题，通过量子退火解决。这种在分层聚类框架中集成量子优化的方法能够处理具有任意相关结构的图，包括负边，而不依赖于度量假设或预定义的簇数量。在合成带符号图和真实世界高光谱成像数据上的实证评估表明，当为关联聚类而调整时，GCS-Q在真实数据上的鲁棒性和聚类质量以及在簇大小不平衡的场景中优于经典算法。我们的研究结果强调了混合量子-经典优化在推进基于图的无监督学习中可扩展和结构感知的聚类技术方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces a hybrid quantum-classical method to correlationclustering, a graph-based unsupervised learning task that seeks to partitionthe nodes in a graph based on pairwise agreement and disagreement. Inparticular, we adapt GCS-Q, a quantum-assisted solver originally designed forcoalition structure generation, to maximize intra-cluster agreement in signedgraphs through recursive divisive partitioning. The proposed method encodeseach bipartitioning step as a quadratic unconstrained binary optimizationproblem, solved via quantum annealing. This integration of quantum optimizationwithin a hierarchical clustering framework enables handling of graphs witharbitrary correlation structures, including negative edges, without relying onmetric assumptions or a predefined number of clusters. Empirical evaluations onsynthetic signed graphs and real-world hyperspectral imaging data demonstratethat, when adapted for correlation clustering, GCS-Q outperforms classicalalgorithms in robustness and clustering quality on real-world data and inscenarios with cluster size imbalance. Our results highlight the promise ofhybrid quantum-classical optimization for advancing scalable andstructurally-aware clustering techniques in graph-based unsupervised learning.</description>
      <author>example@mail.com (Antonio Macaluso, Supreeth Mysore Venkatesh, Diego Arenas, Matthias Klusch, Andreas Dengel)</author>
      <guid isPermaLink="false">2509.03561v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Can Language Models Handle a Non-Gregorian Calendar?</title>
      <link>http://arxiv.org/abs/2509.04432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统评估了开源语言模型处理非公历日历系统(特别是日本日历)的能力，发现即使是专门针对日语的模型在日历算术和跨日历一致性方面也存在困难，强调了开发具有特定文化日历理解能力的语言模型的重要性。&lt;h4&gt;背景&lt;/h4&gt;时间推理和知识是语言模型的重要能力。虽然先前的工作已经分析和改进了语言模型中的时间推理，但大多数研究仅关注公历。然而，许多非公历系统(如日本历、伊斯兰历和希伯来历)仍在积极使用，并反映了基于文化的时间概念。目前尚未评估当前语言模型能否准确处理这类非公历日历。&lt;h4&gt;目的&lt;/h4&gt;评估开源语言模型处理非公历日历系统(特别是日本日历)的能力，确定它们在需要时间知识和时间推理的任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;作者创建了四个需要时间知识和时间推理的任务的数据集，评估了一系列以英语为中心和以日语为中心的语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;一些模型可以执行日历转换，但即使是专门针对日语的模型在处理日本日历算术和保持跨日历一致性方面也存在困难。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了开发具有更好特定文化日历理解能力的语言模型的重要性。&lt;h4&gt;翻译&lt;/h4&gt;时间推理和知识是语言模型的基本能力。虽然先前的工作已经分析和改进了语言模型中的时间推理，但大多数研究仅关注公历。然而，许多非公历系统，如日本历、伊斯兰历和希伯来历，仍在积极使用，并反映了基于文化的时间概念。迄今为止，尚未评估当前语言模型能否准确处理这类非公历日历。在此，我们系统评估了开源语言模型处理一种非公历系统(日本历)的能力。为了评估，我们创建了四个需要时间知识和时间推理的任务数据集。评估了一系列以英语为中心和以日语为中心的语言模型后，我们发现一些模型可以执行日历转换，但即使是专门针对日语的模型在处理日本日历算术和保持跨日历一致性方面也存在困难。我们的研究结果强调了开发具有更好特定文化日历理解能力的语言模型的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning and knowledge are essential capabilities for languagemodels (LMs). While much prior work has analyzed and improved temporalreasoning in LMs, most studies have focused solely on the Gregorian calendar.However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrewcalendars, are in active use and reflect culturally grounded conceptions oftime. If and how well current LMs can accurately handle such non-Gregoriancalendars has not been evaluated so far. Here, we present a systematicevaluation of how well open-source LMs handle one such non-Gregorian system:the Japanese calendar. For our evaluation, we create datasets for four tasksthat require both temporal knowledge and temporal reasoning. Evaluating a rangeof English-centric and Japanese-centric LMs, we find that some models canperform calendar conversions, but even Japanese-centric models struggle withJapanese-calendar arithmetic and with maintaining consistency across calendars.Our results highlight the importance of developing LMs that are better equippedfor culture-specific calendar understanding.</description>
      <author>example@mail.com (Mutsumi Sasaki, Go Kamoda, Ryosuke Takahashi, Kosuke Sato, Kentaro Inui, Keisuke Sakaguchi, Benjamin Heinzerling)</author>
      <guid isPermaLink="false">2509.04432v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph</title>
      <link>http://arxiv.org/abs/2509.04086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合Bi-Directional Text Fusion (BiT)模块和Category-Aware Temporal Graph (CATS)模块的方法，用于解决Audio-Visual Video Parsing (AVVP)任务中的噪声伪标签问题，实现了更精确的事件类别识别和时间定位。&lt;h4&gt;背景&lt;/h4&gt;Audio-Visual Video Parsing (AVVP)任务旨在识别给定视频中的事件类别及其发生时间，使用弱监督标签。现有方法通常分为两类：(i) 基于注意力机制设计增强架构以更好地进行时间建模；(ii) 生成更丰富的伪标签以补偿帧级注释的缺失。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中噪声伪标签被当作可靠监督信号以及初始错误在训练过程中被反复放大的问题。&lt;h4&gt;方法&lt;/h4&gt;结合Bi-Directional Text Fusion (BiT)模块和Category-Aware Temporal Graph (CATS)模块。通过BiT模块对音频和视觉模态特征进行语义注入和动态校准，定位和净化更清洁、更丰富的语义线索；利用CATS模块进行语义传播和连接，实现精确的语义信息在时间上的传播。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在两个基准数据集LLP和UnAV-100上的多个关键指标上实现了最先进的(SOTA)性能。&lt;h4&gt;结论&lt;/h4&gt;结合两种现有方法的优点，通过BiT模块净化语义线索，并通过CATS模块实现精确的语义传播，能够有效提高AVVP任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;视听视频解析任务旨在识别给定视频中的事件类别及其发生时间，并使用弱监督标签。现有方法通常分为两类：(i) 基于注意力机制设计增强架构以更好地进行时间建模，以及(ii) 生成更丰富的伪标签以补偿帧级注释的缺失。然而，第一类方法将噪声的段级伪标签视为可靠的监督信号，第二类方法让无差别的注意力扩散到所有帧上，导致初始错误在训练过程中被反复放大。为解决这个问题，我们提出了一种结合双向文本融合模块和类别感知时序图模块的方法。具体而言，我们整合了两个先前研究方向的优点和互补性。首先，通过BiT模块对音频和视觉模态特征进行语义注入和动态校准，以定位和净化更清洁、更丰富的语义线索。然后，我们利用CATS模块进行语义传播和连接，使精确的语义信息能够在时间上传播。实验结果表明，我们在两个基准数据集LLP和UnAV-100的多个关键指标上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Video Parsing (AVVP) task aims to identify event categories andtheir occurrence times in a given video with weakly supervised labels. Existingmethods typically fall into two categories: (i) designing enhancedarchitectures based on attention mechanism for better temporal modeling, and(ii) generating richer pseudo-labels to compensate for the absence offrame-level annotations. However, the first type methods treat noisysegment-level pseudo labels as reliable supervision and the second type methodslet indiscriminate attention spread them across all frames, the initial errorsare repeatedly amplified during training. To address this issue, we propose amethod that combines the Bi-Directional Text Fusion (BiT) module andCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate thestrengths and complementarity of the two previous research directions. We firstperform semantic injection and dynamic calibration on audio and visual modalityfeatures through the BiT module, to locate and purify cleaner and richersemantic cues. Then, we leverage the CATS module for semantic propagation andconnection to enable precise semantic information dissemination across time.Experimental results demonstrate that our proposed method achievesstate-of-the-art (SOTA) performance in multiple key indicators on two benchmarkdatasets, LLP and UnAV-100.</description>
      <author>example@mail.com (Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang)</author>
      <guid isPermaLink="false">2509.04086v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Keye-VL-1.5通过三项创新解决了视频理解中的挑战：Slow-Fast视频编码策略、渐进式四阶段预训练方法和全面的微调流程，在视频理解和一般多模态任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;大语言模型已扩展到多模态任务形成MLLMs，但视频理解仍具挑战性，因为视频动态且信息密集。现有模型在处理视频内容时，在空间分辨率和时间覆盖之间难以平衡。&lt;h4&gt;目的&lt;/h4&gt;提出Keye-VL-1.5模型，解决视频理解中的基本挑战，提高视频处理能力。&lt;h4&gt;方法&lt;/h4&gt;引入Slow-Fast视频编码策略动态分配计算资源；实现四阶段预训练将上下文长度从8K扩展到128K；开发包含5步思维链数据构建、GSPO强化学习和对齐训练的微调流程。&lt;h4&gt;主要发现&lt;/h4&gt;Keye-VL-1.5在公共基准测试和人类评估中显著优于现有模型，尤其在视频理解任务上表现出色，同时在一般多模态基准上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三项创新有效解决了视频理解中的基本挑战，在视频理解和一般多模态任务上均表现优异。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展显著推进，通过多模态大语言模型扩展到多模态任务。然而，由于视频的动态和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，在空间分辨率和时间覆盖之间难以权衡。我们提出了Keye-VL-1.5，通过三项关键创新解决视频理解中的基本挑战。首先，我们引入了一种新颖的Slow-Fast视频编码策略，根据帧间相似性动态分配计算资源，以更高分辨率处理有显著视觉变化的关键帧，同时以更低分辨率但增加时间覆盖处理相对静态的帧。其次，我们实现了渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，能够处理更长的视频和更复杂的视觉内容。第三，我们开发了专注于推理增强和人类偏好对齐的全面微调流程，包含5步思维链数据构建过程，针对困难案例的迭代GSPO强化学习与渐进式提示提示，以及对齐训练。通过在公共基准上的广泛评估和严格的人类评估，Keye-VL-1.5显示出比现有模型显著的改进，特别是在视频理解任务上表现出色，同时在一般多模态基准上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Safety-Critical Multi-Agent MCTS for Mixed Traffic Coordination at Unsignalized Roundabout</title>
      <link>http://arxiv.org/abs/2509.01856v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种安全关键的多智能体蒙特卡洛树搜索框架，用于解决自动驾驶车辆在无信号环岛的决策问题，特别是在混合交通环境中与人类驾驶车辆的协调问题。&lt;h4&gt;背景&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在混合交通环境中，自动驾驶车辆需要与人类驾驶车辆安全协调。&lt;h4&gt;目的&lt;/h4&gt;开发一个整合确定性和概率预测模型的安全关键多智能体蒙特卡洛树搜索框架，促进复杂环岛场景中的协作决策。&lt;h4&gt;方法&lt;/h4&gt;提出三个关键创新：(1)分层安全评估模块，通过动态安全阈值和时空风险评估处理AV-to-AV、AV-to-HDV和AV-to-Road交互；(2)自适应HDV行为预测方案，结合智能驾驶员模型和概率不确定性建模；(3)多目标奖励优化策略，联合考虑安全、效率和协作意图。&lt;h4&gt;主要发现&lt;/h4&gt;大量模拟验证了该方法在完全自主(100% AVs)和混合交通(50% AVs + 50% HDVs)条件下的有效性。与基准方法相比，框架减少了所有AV的轨迹偏差，显著降低了侵入后时间违规率，在完全自主场景中仅实现1.0%，在混合交通环境中实现3.2%。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在复杂环岛场景中能够有效实现安全、高效的协作决策。&lt;h4&gt;翻译&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在混合交通环境中，自动驾驶车辆必须与人类驾驶车辆安全协调。本文提出了一个安全关键的多智能体蒙特卡洛树搜索框架，整合了确定性和概率预测模型，以促进复杂环岛场景中的协作决策。所提出的框架引入了三个关键创新：(1)分层安全评估模块，通过动态安全阈值和时空风险评估系统处理AV-to-AV、AV-to-HDV和AV-to-Road交互；(2)自适应HDV行为预测方案，结合智能驾驶员模型和概率不确定性建模；(3)多目标奖励优化策略，联合考虑安全、效率和协作意图。大量模拟结果验证了所提出方法在完全自主(100% AVs)和混合交通(50% AVs + 50% HDVs)条件下的有效性。与基准方法相比，我们的框架减少了所有AV的轨迹偏差，显著降低了侵入后时间违规率，在完全自主场景中仅实现1.0%，在混合交通环境中实现3.2%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decision-making at unsignalized roundabouts poses substantial challenges forautonomous vehicles (AVs), particularly in mixed traffic environments where AVsmust coordinate safely with human-driven vehicles (HDVs). This paper presents asafety-critical multi-agent Monte Carlo Tree Search (MCTS) framework thatintegrates both deterministic and probabilistic prediction models to facilitatecooperative decision-making in complex roundabout scenarios. The proposedframework introduces three key innovations: (1) a hierarchical safetyassessment module that systematically addresses AV-to-AV (A2A), AV-to-HDV(A2H), and AV-to-Road (A2R) interactions through dynamic safety thresholds andspatiotemporal risk evaluation; (2) an adaptive HDV behavior prediction schemethat combines the Intelligent Driver Model (IDM) with probabilistic uncertaintymodeling; and (3) a multi-objective reward optimization strategy that jointlyconsiders safety, efficiency, and cooperative intent. Extensive simulationresults validate the effectiveness of the proposed approach under both fullyautonomous (100% AVs) and mixed traffic (50% AVs + 50% HDVs) conditions.Compared to benchmark methods, our framework consistently reduces trajectorydeviations across all AVs and significantly lowers the rate ofPost-Encroachment Time (PET) violations, achieving only 1.0\% in the fullyautonomous scenario and 3.2% in the mixed traffic setting.</description>
      <author>example@mail.com (Zhihao Lin, Shuo Liu, Zhen Tian, Dezong Zhao, Jianglin Lan)</author>
      <guid isPermaLink="false">2509.01856v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>AIVA: An AI-based Virtual Companion for Emotion-aware Interaction</title>
      <link>http://arxiv.org/abs/2509.03212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作通过整合多模态情感感知到大型语言模型中，开发了一个能够解读情感线索并做出情感一致回应的AI虚拟伴侣，为人机交互提供了更自然和有同理心的体验。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言理解和生成方面取得了显著进步，改善了人机交互。但这些模型仅限于单模态文本处理，缺乏解读非语言信号中情感线索的能力，限制了更沉浸和有同理心的交互体验。&lt;h4&gt;目的&lt;/h4&gt;探索将多模态情感感知整合到大型语言模型中，创建具有情感感知能力的智能体，实现更自然和有同理心的人机交互。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为'\ours'的基于AI的虚拟伴侣，引入了多模态情感感知网络(MSPN)，使用跨模态融合transformer和监督对比学习来提供情感线索；开发了情感感知的提示工程策略用于生成有同理心的回应；集成了文本到语音系统和动画头像模块以实现富有表现力的交互。&lt;h4&gt;主要发现&lt;/h4&gt;多模态情感感知可以增强大型语言模型的能力，使其能够更好地理解和回应人类情感，从而创建更自然、更有同理心的人机交互体验。&lt;h4&gt;结论&lt;/h4&gt;'\ours'为情感感知智能体提供了一个框架，在陪伴机器人、社会关怀、心理健康和以人为本的人工智能等领域有广泛应用前景。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的最新进展显著改善了自然语言理解和生成能力，增强了人机交互。然而，大型语言模型仅限于单模态文本处理，缺乏解读非语言信号中情感线索的能力，阻碍了更沉浸和有同理心的交互。这项工作探索将多模态情感感知整合到大型语言模型中，以创建具有情感感知能力的智能体。我们提出了'\ours'，一个基于AI的虚拟伴侣，能够捕获多模态情感线索，实现情感一致和生动的人机交互。'\ours'引入了一个多模态情感感知网络，使用跨模态融合transformer和监督对比学习来提供情感线索。此外，我们还开发了一种情感感知的提示工程策略，用于生成有同理心的回应，并集成了文本到语音系统和动画头像模块，以实现富有表现力的交互。'\ours'为情感感知智能体提供了一个框架，在陪伴机器人、社会关怀、心理健康和以人为本的人工智能等领域有应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Large Language Models (LLMs) have significantly improvednatural language understanding and generation, enhancing Human-ComputerInteraction (HCI). However, LLMs are limited to unimodal text processing andlack the ability to interpret emotional cues from non-verbal signals, hinderingmore immersive and empathetic interactions. This work explores integratingmultimodal sentiment perception into LLMs to create emotion-aware agents. Wepropose \ours, an AI-based virtual companion that captures multimodal sentimentcues, enabling emotionally aligned and animated HCI. \ours introduces aMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusiontransformer and supervised contrastive learning to provide emotional cues.Additionally, we develop an emotion-aware prompt engineering strategy forgenerating empathetic responses and integrate a Text-to-Speech (TTS) system andanimated avatar module for expressive interactions. \ours provides a frameworkfor emotion-aware agents with applications in companion robotics, social care,mental health, and human-centered AI.</description>
      <author>example@mail.com (Chenxi Li)</author>
      <guid isPermaLink="false">2509.03212v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
  <item>
      <title>Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback</title>
      <link>http://arxiv.org/abs/2509.03206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将对比学习原则整合到目标条件监督学习(GCSL)框架中的新模型，解决了GCSL仅从成功经验学习而忽略失败经验，以及加剧代理固有偏见的问题，使代理能够从成功和失败中学习，实现更优的探索性行为和性能。&lt;h4&gt;背景&lt;/h4&gt;强化学习在稀疏奖励结构的任务中面临挑战；模仿学习收敛快但依赖人工演示；目标条件监督学习(GCSL)使自主系统能够进行自我模仿学习。&lt;h4&gt;目的&lt;/h4&gt;解决GCSL框架的两个局限性：仅从自我生成的经验学习会加剧代理固有偏见；重新标记策略使代理只关注成功结果而无法从错误中学习。&lt;h4&gt;方法&lt;/h4&gt;提出一种新模型，将对比学习原则整合到GCSL框架中，使代理能够从成功和失败中学习。&lt;h4&gt;主要发现&lt;/h4&gt;该算法克服了代理初始偏见带来的限制，使代理能够进行更具探索性的行为，有助于识别和采用有效策略。&lt;h4&gt;结论&lt;/h4&gt;该模型在各种具有挑战性的环境中实现了更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;强化学习应用于具有稀疏奖励结构的任务时面临重大挑战。尽管模仿学习（监督学习领域内）提供了更快的收敛速度，但它严重依赖人工生成的演示。最近，目标条件监督学习(GCSL)作为一种潜在解决方案出现，使自主系统能够进行自我模仿学习。通过战略性地重新标记目标，代理可以从自身经验中获取策略见解。尽管该框架取得了成功，但它存在两个显著局限：(1)仅从自我生成的经验学习可能会加剧代理的固有偏见；(2)重新标记策略允许代理只关注成功结果，无法从错误中学习。为解决这些问题，我们提出了一种将对比学习原则整合到GCSL框架中的新模型，以从成功和失败中学习。通过经验评估，我们证明了该算法克服了代理初始偏见带来的限制，从而实现了更具探索性的行为。这有助于识别和采用有效策略，在各种具有挑战性的环境中实现更优的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning faces significant challenges when applied to taskscharacterized by sparse reward structures. Although imitation learning, withinthe domain of supervised learning, offers faster convergence, it relies heavilyon human-generated demonstrations. Recently, Goal-Conditioned SupervisedLearning (GCSL) has emerged as a potential solution by enabling self-imitationlearning for autonomous systems. By strategically relabelling goals, agents canderive policy insights from their own experiences. Despite the successes ofthis framework, it presents two notable limitations: (1) Learning exclusivelyfrom self-generated experiences can exacerbate the agents' inherent biases; (2)The relabelling strategy allows agents to focus solely on successful outcomes,precluding them from learning from their mistakes. To address these issues, wepropose a novel model that integrates contrastive learning principles into theGCSL framework to learn from both success and failure. Through empiricalevaluations, we demonstrate that our algorithm overcomes limitations imposed byagents' initial biases and thereby enables more exploratory behavior. Thisfacilitates the identification and adoption of effective policies, leading tosuperior performance across a variety of challenging environments.</description>
      <author>example@mail.com (Zeqiang Zhang, Fabian Wurzberger, Gerrit Schmid, Sebastian Gottwald, Daniel A. Braun)</author>
      <guid isPermaLink="false">2509.03206v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Interpretability and Effectiveness in Recommendation with Numerical Features via Learning to Contrast the Counterfactual samples</title>
      <link>http://arxiv.org/abs/2509.03187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TheWebConf2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCSS的通用模型无关对比学习框架，用于建模神经网络输出与数值特征之间的单调性，提高推荐系统的可解释性和有效性。&lt;h4&gt;背景&lt;/h4&gt;推荐系统的可解释性和有效性依赖于神经网络输出与数值特征之间的单调性关系，但现有方法缺乏对此特性的有效建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的、与模型无关的框架，能够准确建模神经网络输出与数值特征之间的单调性关系。&lt;h4&gt;方法&lt;/h4&gt;提出CCSS框架，通过两阶段过程建模单调性：1)合成反事实样本；2)对比反事实样本。这两种技术自然集成到一个与模型无关的框架中，形成端到端的训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集和真实工业数据集上的大量实证测试表明，CCSS框架能有效建模单调性关系。此外，CCSS已在真实大规模工业推荐系统中部署，为数亿用户提供服务。&lt;h4&gt;结论&lt;/h4&gt;CCSS框架成功解决了神经网络输出与数值特征单调性建模的问题，提高了推荐系统的可解释性和有效性，并在实际工业应用中证明了其价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出一个通用的、与模型无关的带有反事实样本合成的对比学习框架（CCSS），用于建模神经网络输出与数值特征之间的单调性，这对推荐系统的可解释性和有效性至关重要。CCSS通过两阶段过程建模单调性：合成反事实样本和对比反事实样本。这两种技术自然集成到一个与模型无关的框架中，形成端到端的训练过程。在公开可用数据集和真实工业数据集上进行了大量实证测试，结果很好地证明了我们提出的CCSS的有效性。此外，CCSS已在我们真实的大规模工业推荐系统中部署，成功为数亿用户提供服务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a general model-agnostic Contrastive learning framework withCounterfactual Samples Synthesizing (CCSS) for modeling the monotonicitybetween the neural network output and numerical features which is critical forinterpretability and effectiveness of recommender systems. CCSS models themonotonicity via a two-stage process: synthesizing counterfactual samples andcontrasting the counterfactual samples. The two techniques are naturallyintegrated into a model-agnostic framework, forming an end-to-end trainingprocess. Abundant empirical tests are conducted on a publicly available datasetand a real industrial dataset, and the results well demonstrate theeffectiveness of our proposed CCSS. Besides, CCSS has been deployed in our reallarge-scale industrial recommender, successfully serving over hundreds ofmillions users.</description>
      <author>example@mail.com (Xiaoxiao Xu, Hao Wu, Wenhui Yu, Lantao Hu, Peng Jiang, Kun Gai)</author>
      <guid isPermaLink="false">2509.03187v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised Radio Representation Learning: Can we Learn Multiple Tasks?</title>
      <link>http://arxiv.org/abs/2509.03077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, IEEE international conference on communication  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于动量对比的自监督学习方案，用于无线电信号表征学习。该方法利用对比学习从大型真实世界数据集中提取鲁棒且可迁移的表征，并在两个无线通信任务中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;人工智能预计将在6G中发挥关键作用，但开发AI解决方案面临的主要挑战是需要大量的数据收集和标注工作来训练监督深度学习模型。自监督学习通过利用大量未标记数据来实现接近监督学习的性能，已在各个领域显示出显著的成功。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的自监督学习方案，用于无线电信号表征学习，以减少对标记数据的依赖，提高模型泛化能力，为可扩展的基础6G AI模型和解决方案铺平道路。&lt;h4&gt;方法&lt;/h4&gt;使用动量对比的自监督学习方案，通过应用对比学习从大型真实世界数据集中提取鲁棒且可迁移的表征。评估这些学习到的表征在两个无线通信任务（到达角估计和自动调制分类）中的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;精心设计的增强和多样化数据使对比学习能够产生高质量、不变的潜在表征。即使冻结编码器权重，这些表征也是有效的，并且微调可以进一步提高性能，超越监督基线。据作者所知，这是第一个提出并证明自监督学习对多任务无线电信号有效性的工作。&lt;h4&gt;结论&lt;/h4&gt;自监督学习具有变革无线通信AI的潜力，通过减少对标记数据的依赖和提高模型泛化能力，为可扩展的基础6G AI模型和解决方案铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;人工智能(AI)预计将在6G中发挥关键作用。然而，开发AI解决方案的一个主要挑战是需要大量的数据收集和标注工作来训练监督深度学习模型。为了克服这一挑战，自监督学习(SSL)方法最近通过利用大量未标记数据来实现接近监督学习的性能，已在各个领域显示出显著的成功。在本文中，我们提出了一种使用动量对比的有效SSL方案，用于无线电信号表征学习。通过应用对比学习，我们的方法从大型真实世界数据集中提取鲁棒且可迁移的表征。我们评估了这些学习到的表征在两个无线通信任务中的泛化能力：到达角(AoA)估计和自动调制分类(AMC)。我们的研究结果表明，精心设计的增强和多样化数据使对比学习能够产生高质量、不变的潜在表征。即使冻结编码器权重，这些表征也是有效的，并且微调可以进一步提高性能，超越监督基线。据我们所知，这是第一个提出并证明自监督学习对多任务无线电信号有效性的工作。我们的研究结果强调了自监督学习通过减少对标记数据的依赖和提高模型泛化能力来变革无线通信AI的潜力，为可扩展的基础6G AI模型和解决方案铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) is anticipated to play a pivotal role in 6G.However, a key challenge in developing AI-powered solutions is the extensivedata collection and labeling efforts required to train supervised deep learningmodels. To overcome this, self-supervised learning (SSL) approaches haverecently demonstrated remarkable success across various domains by leveraginglarge volumes of unlabeled data to achieve near-supervised performance. In thispaper, we propose an effective SSL scheme for radio signal representationlearning using momentum contrast. By applying contrastive learning, our methodextracts robust, transferable representations from a large real-world dataset.We assess the generalizability of these learned representations across twowireless communications tasks: angle of arrival (AoA) estimation and automaticmodulation classification (AMC). Our results show that carefully designedaugmentations and diverse data enable contrastive learning to producehigh-quality, invariant latent representations. These representations areeffective even with frozen encoder weights, and fine-tuning further enhancesperformance, surpassing supervised baselines. To the best of our knowledge,this is the first work to propose and demonstrate the effectiveness ofself-supervised learning for radio signals across multiple tasks. Our findingshighlight the potential of self-supervised learning to transform AI forwireless communications by reducing dependence on labeled data and improvingmodel generalization - paving the way for scalable foundational 6G AI modelsand solutions.</description>
      <author>example@mail.com (Ogechukwu Kanu, Ashkan Eshaghbeigi, Hatem Abou-Zeid)</author>
      <guid isPermaLink="false">2509.03077v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Training LLMs to be Better Text Embedders through Bidirectional Reconstruction</title>
      <link>http://arxiv.org/abs/2509.03020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的训练方法，通过双向生成重建任务增强大型语言模型中最终令牌的语义表达能力，显著提升了文本嵌入性能&lt;h4&gt;背景&lt;/h4&gt;大型语言模型正被探索作为文本嵌入器，但现有方法依赖的最终令牌（如[EOS]）未经专门训练来捕获整个上下文语义，限制了其在检索和重排序任务中的表现&lt;h4&gt;目的&lt;/h4&gt;在对比学习前添加新的训练阶段，以丰富最终令牌嵌入的语义表达能力&lt;h4&gt;方法&lt;/h4&gt;采用双向生成重建任务EBQ2D（基于嵌入的查询到文档）和EBD2Q（基于嵌入的文档到查询），交替进行锚定[EOS]嵌入并重建查询-文档对&lt;h4&gt;主要发现&lt;/h4&gt;额外训练阶段显著提高了LLM在Massive Text Embedding Benchmark上的性能，在不同LLM基础模型和规模上实现了新的最先进结果&lt;h4&gt;结论&lt;/h4&gt;通过专门设计的训练阶段增强最终令牌语义表达能力，可有效提升大型语言模型在文本嵌入任务中的性能&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）越来越多地被探索作为强大的文本嵌入器。现有的基于LLM的文本嵌入方法通常利用最终令牌的嵌入，通常是保留的特殊令牌如[EOS]。然而，这些令牌没有经过专门训练来捕获整个上下文的语义，限制了它们作为文本嵌入的能力，特别是在检索和重排序任务中。我们提出在对比学习之前添加一个新的训练阶段来丰富最终令牌嵌入的语义。该阶段采用双向生成重建任务，即EBQ2D（基于嵌入的查询到文档）和EBD2Q（基于嵌入的文档到查询），这些任务交替进行，锚定[EOS]嵌入并重建查询-文档对中的任一侧。实验结果表明，我们额外的训练阶段显著提高了LLM在Massive Text Embedding Benchmark（MTEB）上的性能，在不同LLM基础模型和规模上实现了新的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have increasingly been explored as powerful textembedders. Existing LLM-based text embedding approaches often leverage theembedding of the final token, typically a reserved special token such as [EOS].However, these tokens have not been intentionally trained to capture thesemantics of the whole context, limiting their capacity as text embeddings,especially for retrieval and re-ranking tasks. We propose to add a new trainingstage before contrastive learning to enrich the semantics of the final tokenembedding. This stage employs bidirectional generative reconstruction tasks,namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-BasedDocument-to-Query), which interleave to anchor the [EOS] embedding andreconstruct either side of Query-Document pairs. Experimental resultsdemonstrate that our additional training stage significantly improves LLMperformance on the Massive Text Embedding Benchmark (MTEB), achieving newstate-of-the-art results across different LLM base models and scales.</description>
      <author>example@mail.com (Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin)</author>
      <guid isPermaLink="false">2509.03020v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability</title>
      <link>http://arxiv.org/abs/2509.02962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE/ASME Transactions on Mechatronics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种多模态工业表面缺陷检测方法，通过融合RGB和3D模态数据来识别和定位工业产品缺陷。针对模态缺失问题，作者提出跨模态提示学习和对称对比学习方法，实验证明该方法显著优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;多模态工业表面缺陷检测(MISDD)旨在通过融合RGB和3D模态数据来识别和定位工业产品缺陷。然而，在实际应用中，传感器的不确定性会导致模态缺失问题，给多模态融合带来困难，包括学习模式变换和信息空缺等问题。&lt;h4&gt;目的&lt;/h4&gt;解决MISDD中的模态缺失问题，提高在模态缺失情况下的缺陷检测性能，特别是在传感器可用性不确定的情况下。&lt;h4&gt;方法&lt;/h4&gt;1) 提出跨模态提示学习：包括跨模态一致性提示建立双视觉模态信息一致性；模态特定提示适应不同输入模式；缺失感知提示补偿动态模态缺失造成的信息空缺。2) 提出对称对比学习：利用文本模态作为双视觉模态融合的桥梁，设计成对反义文本提示生成二元文本语义，并提供三模态对比预训练实现多模态学习。&lt;h4&gt;主要发现&lt;/h4&gt;在RGB和3D模态总缺失率为0.7的情况下，所提方法达到73.83%的I-AUROC和93.05%的P-AUROC，分别超过现有最先进方法3.84%和5.58%。在不同缺失类型和率的情况下，该方法也以不同程度优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的跨模态提示学习和对称对比学习方法有效解决了MISDD中的模态缺失问题，显著提高了缺陷检测性能，特别是在模态缺失的情况下。&lt;h4&gt;翻译&lt;/h4&gt;多模态工业表面缺陷检测(MISDD)旨在通过融合RGB和3D模态数据来识别和定位工业产品缺陷。本文关注由传感器可用性不确定引起的MISDD中的模态缺失问题。在此背景下，多模态融合遇到几个困难，包括学习模式变换和信息空缺。为此，我们首先提出跨模态提示学习，包括：i)跨模态一致性提示用于建立双视觉模态的信息一致性；ii)插入模态特定提示以适应不同的输入模式；iii)附加缺失感知提示以补偿动态模态缺失造成的信息空缺。此外，我们提出对称对比学习，利用文本模态作为双视觉模态融合的桥梁。具体来说，设计了成对反义文本提示来生成二元文本语义，并提供三模态对比预训练来实现多模态学习。实验结果表明，我们提出的方法在RGB和3D模态总缺失率为0.7的情况下，实现了73.83%的I-AUROC和93.05%的P-AUROC（分别超过现有最先进方法3.84%和5.58%），并在不同缺失类型和率的情况下以不同程度优于现有方法。源代码将在https://github.com/SvyJ/MISDD-MM上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal industrial surface defect detection (MISDD) aims to identify andlocate defect in industrial products by fusing RGB and 3D modalities. Thisarticle focuses on modality-missing problems caused by uncertain sensorsavailability in MISDD. In this context, the fusion of multiple modalitiesencounters several troubles, including learning mode transformation andinformation vacancy. To this end, we first propose cross-modal prompt learning,which includes: i) the cross-modal consistency prompt serves the establishmentof information consistency of dual visual modalities; ii) the modality-specificprompt is inserted to adapt different input patterns; iii) the missing-awareprompt is attached to compensate for the information vacancy caused by dynamicmodalities-missing. In addition, we propose symmetric contrastive learning,which utilizes text modality as a bridge for fusion of dual vision modalities.Specifically, a paired antithetical text prompt is designed to generate binarytext semantics, and triple-modal contrastive pre-training is offered toaccomplish multimodal learning. Experiment results show that our proposedmethod achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%respectively), and outperforms existing approaches to varying degrees underdifferent missing types and rates. The source code will be available athttps://github.com/SvyJ/MISDD-MM.</description>
      <author>example@mail.com (Shuai Jiang, Yunfeng Ma, Jingyu Zhou, Yuan Bian, Yaonan Wang, Min Liu)</author>
      <guid isPermaLink="false">2509.02962v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>RankGraph: Unified Heterogeneous Graph Learning for Cross-Domain Recommendation</title>
      <link>http://arxiv.org/abs/2509.02942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  RecSys 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RankGraph是一个可扩展的图学习框架，作为推荐基础模型的核心组件，通过构建跨产品的异构节点和边图，整合用户、帖子、广告等实体间的复杂关系，有效提升了跨领域推荐系统的性能。&lt;h4&gt;背景&lt;/h4&gt;跨领域推荐系统面临挑战，需要整合不同产品领域中的细粒度用户和项目关系。&lt;h4&gt;目的&lt;/h4&gt;介绍RankGraph框架，作为推荐基础模型的核心组件，解决跨领域推荐中的关系整合问题。&lt;h4&gt;方法&lt;/h4&gt;构建多产品中异构节点和边组成的图，利用GPU加速的图神经网络和对比学习，动态提取子图如项目-项目图和用户-用户图，并将基于图的预训练表示作为上下文令牌整合到FM序列模型中。&lt;h4&gt;主要发现&lt;/h4&gt;通过在线A/B测试验证，RankGraph使点击率提高0.92%，转化率提高2.82%，有效支持基于相似性的检索和实时聚类。&lt;h4&gt;结论&lt;/h4&gt;RankGraph在跨领域推荐场景中展示了显著的有效性，能够成功整合复杂关系并提升推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;跨领域推荐系统面临着整合各种产品领域中细粒度用户和项目关系的挑战。为此，我们引入了RankGraph，这是一个可扩展的图学习框架，设计用作推荐基础模型的核心组件。通过构建和利用跨多个产品的异构节点和边组成的图，RankGraph能够整合用户、帖子、广告和其他实体之间的复杂关系。我们的框架采用GPU加速的图神经网络和对比学习，能够动态提取项目-项目和用户-用户等子图，以支持基于相似性的检索和实时聚类。此外，RankGraph将基于图的预训练表示作为上下文令牌整合到FM序列模型中，通过结构化关系知识丰富模型。RankGraph在在线A/B测试中展示了点击率（+0.92%）和转化率（+2.82%）的提升，展示了其在跨领域推荐场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain recommendation systems face the challenge of integratingfine-grained user and item relationships across various product domains. Toaddress this, we introduce RankGraph, a scalable graph learning frameworkdesigned to serve as a core component in recommendation foundation models(FMs). By constructing and leveraging graphs composed of heterogeneous nodesand edges across multiple products, RankGraph enables the integration ofcomplex relationships between users, posts, ads, and other entities. Ourframework employs a GPU-accelerated Graph Neural Network and contrastivelearning, allowing for dynamic extraction of subgraphs such as item-item anduser-user graphs to support similarity-based retrieval and real-timeclustering. Furthermore, RankGraph integrates graph-based pretrainedrepresentations as contextual tokens into FM sequence models, enriching themwith structured relational knowledge. RankGraph has demonstrated improvementsin click (+0.92%) and conversion rates (+2.82%) in online A/B tests, showcasingits effectiveness in cross-domain recommendation scenarios.</description>
      <author>example@mail.com (Renzhi Wu, Junjie Yang, Li Chen, Hong Li, Li Yu, Hong Yan)</author>
      <guid isPermaLink="false">2509.02942v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images</title>
      <link>http://arxiv.org/abs/2509.02287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SynthGenNet是一种自监督的学生-教师架构，通过ClassMix++算法、基于掩码的一致性损失和伪标签引导的对比学习，实现了在复杂城市环境中的强大域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;非结构化城市环境由于其复杂多样的布局，为场景理解和泛化带来了独特的挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍SynthGenNet，一种自监督的学生-教师架构，旨在使用合成多源图像实现强大的测试时域泛化。&lt;h4&gt;方法&lt;/h4&gt;提出了新颖的ClassMix++算法，混合来自各种合成源的标记数据同时保持语义完整性；采用基于掩码的一致性损失(GMC)，利用源真实数据提高跨域预测一致性和特征对齐；集成伪标签引导的对比学习(PLGCL)机制到学生网络中，通过从教师网络进行迭代知识蒸馏促进域不变特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;这种自监督策略提高了预测准确性，解决了现实世界的变化性，弥合了仿真到现实的域差距，减少了对标记目标数据的依赖，即使在复杂的城市区域也是如此。&lt;h4&gt;结论&lt;/h4&gt;该模型在印度驾驶数据集(IDD)等真实世界数据集上实现了50%的平均交并比(mIoU)值，超越了依赖单一源的最先进技术。&lt;h4&gt;翻译&lt;/h4&gt;非结构化的城市环境由于其复杂多样的布局，为场景理解和泛化带来了独特的挑战。我们介绍了SynthGenNet，一种自监督的学生-教师架构，旨在使用合成多源图像实现强大的测试时域泛化。我们的贡献包括新颖的ClassMix++算法，该算法混合来自各种合成源的标记数据，同时保持语义完整性，增强模型适应性。我们进一步采用基于掩码的一致性损失(GMC)，利用源真实数据提高跨域预测一致性和特征对齐。伪标签引导的对比学习(PLGCL)机制被集成到学生网络中，通过从教师网络进行迭代知识蒸馏，促进域不变特征学习。这种自监督策略提高了预测准确性，解决了现实世界的变化性，弥合了仿真到现实的域差距，减少了对标记目标数据的依赖，即使在复杂的城市区域也是如此。结果显示，我们的模型在印度驾驶数据集(IDD)等真实世界数据集上实现了50%的平均交并比(mIoU)值，超越了依赖单一源的最先进技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unstructured urban environments present unique challenges for sceneunderstanding and generalization due to their complex and diverse layouts. Weintroduce SynthGenNet, a self-supervised student-teacher architecture designedto enable robust test-time domain generalization using synthetic multi-sourceimagery. Our contributions include the novel ClassMix++ algorithm, which blendslabeled data from various synthetic sources while maintaining semanticintegrity, enhancing model adaptability. We further employ Grounded MaskConsistency Loss (GMC), which leverages source ground truth to improvecross-domain prediction consistency and feature alignment. The Pseudo-LabelGuided Contrastive Learning (PLGCL) mechanism is integrated into the studentnetwork to facilitate domain-invariant feature learning through iterativeknowledge distillation from the teacher network. This self-supervised strategyimproves prediction accuracy, addresses real-world variability, bridges thesim-to-real domain gap, and reliance on labeled target data, even in complexurban areas. Outcomes show our model outperforms the state-of-the-art (relyingon single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value onreal-world datasets like Indian Driving Dataset (IDD).</description>
      <author>example@mail.com (Pushpendra Dhakara, Prachi Chachodhia, Vaibhav Kumar)</author>
      <guid isPermaLink="false">2509.02287v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching</title>
      <link>http://arxiv.org/abs/2509.02033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by PRICAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了StructCoh，一个图增强对比学习框架，用于文本语义匹配，结合了结构推理和表示空间优化，通过双图编码器和层次化对比目标实现更好的性能。&lt;h4&gt;背景&lt;/h4&gt;文本语义匹配需要理解结构关系和细粒度语义差异。预训练语言模型虽然在捕捉令牌级交互方面表现出色，但往往忽略层次化结构模式，且在细微语义区分方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时考虑结构关系和细粒度语义差异的文本语义匹配框架，提高匹配性能，特别是在法律文档和学术抄袭检测领域。&lt;h4&gt;方法&lt;/h4&gt;提出StructCoh框架，包含两个关键创新：(1)双图编码器通过依存解析和主题建模构建语义图，利用图同构网络传播结构特征；(2)层次化对比目标在多个粒度上强制一致性，包括节点级对比正则化和图感知对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个法律文档匹配基准测试和学术抄袭检测数据集上，StructCoh比最先进方法有显著改进。在法律条文匹配上实现了86.7%的F1分数，绝对增益+6.2%，通过有效识别论证结构相似性。&lt;h4&gt;结论&lt;/h4&gt;StructCoh通过协同结合结构推理和表示空间优化，有效解决了文本语义匹配中的结构理解和细粒度语义区分问题，在法律文档匹配和学术抄袭检测任务中取得了显著成果。&lt;h4&gt;翻译&lt;/h4&gt;文本语义匹配需要对结构关系和细粒度语义差异有细致的理解。虽然预训练语言模型在捕捉令牌级别的交互方面表现出色，但它们往往忽略了层次化的结构模式，并且在处理细微的语义区分方面存在困难。在本文中，我们提出了StructCoh，一个图增强对比学习框架，协同结合了结构推理和表示空间优化。我们的方法有两个关键创新：(1)双图编码器通过依存解析和主题建模构建语义图，然后利用图同构网络在句法依赖和跨文档概念节点之间传播结构特征。(2)层次化对比目标在多个粒度上强制一致性：节点级对比正则化保留核心语义单元，而图感知对比学习通过显式和隐式负采样策略对齐跨文档结构语义。在三个法律文档匹配基准测试和学术抄袭检测数据集上的实验表明，与最先进的方法相比有显著改进。值得注意的是，StructCoh在法律条文匹配上实现了86.7%的F1分数（绝对增益+6.2%），通过有效识别论证结构相似性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text semantic matching requires nuanced understanding of both structuralrelationships and fine-grained semantic distinctions. While pre-trainedlanguage models excel at capturing token-level interactions, they oftenoverlook hierarchical structural patterns and struggle with subtle semanticdiscrimination. In this paper, we proposed StructCoh, a graph-enhancedcontrastive learning framework that synergistically combines structuralreasoning with representation space optimization. Our approach features two keyinnovations: (1) A dual-graph encoder constructs semantic graphs via dependencyparsing and topic modeling, then employs graph isomorphism networks topropagate structural features across syntactic dependencies and cross-documentconcept nodes. (2) A hierarchical contrastive objective enforces consistency atmultiple granularities: node-level contrastive regularization preserves coresemantic units, while graph-aware contrastive learning aligns inter-documentstructural semantics through both explicit and implicit negative samplingstrategies. Experiments on three legal document matching benchmarks andacademic plagiarism detection datasets demonstrate significant improvementsover state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score(+6.2% absolute gain) on legal statute matching by effectively identifyingargument structure similarities.</description>
      <author>example@mail.com (Chao Xue, Ziyuan Gao)</author>
      <guid isPermaLink="false">2509.02033v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs</title>
      <link>http://arxiv.org/abs/2509.02017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM 2025 Full Research Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MME-SID框架，一种基于大型语言模型的顺序推荐方法，通过整合多模态嵌入和量化嵌入解决了嵌入崩溃和灾难性遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐旨在捕捉用户的动态兴趣和顺序模式，大型语言模型的强大能力推动了其在顺序推荐中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于LLM的顺序推荐方法中存在的嵌入崩溃和灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;提出MME-SID框架，整合多模态嵌入和量化嵌入减轻嵌入崩溃；提出MM-RQ-VAE模型保留模态内距离信息和捕获模态间相关性；使用训练好的多模态码嵌入初始化模型减轻灾难性遗忘；使用LoRA在多模态频率感知融合方式下高效微调LLM。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的实验验证了MME-SID的优越性能，能够有效减轻嵌入崩溃和灾难性遗忘问题。&lt;h4&gt;结论&lt;/h4&gt;MME-SID框架通过解决嵌入崩溃和灾难性遗忘问题，提高了顺序推荐的性能，实现代码和数据集已公开可供复现。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐旨在基于用户的历史交互来捕捉用户的动态兴趣和顺序模式。最近，大型语言模型的强大能力推动了它们在顺序推荐中的应用。然而，我们确定了现有基于LLM的SR方法中的两个关键挑战：1)整合预训练协作嵌入时的嵌入崩溃和2)使用语义ID时的量化嵌入灾难性遗忘。这些问题削弱了模型的可扩展性并导致次优的推荐性能。因此，基于Llama3-8B-instruct等LLM，我们引入了一种名为MME-SID的新型SR框架，该框架整合了多模态嵌入和量化嵌入以减轻嵌入崩溃。此外，我们提出了具有最大均值差异作为重构损失和对比学习对齐的多模态残差量化变分自编码器(MM-RQ-VAE)，分别有效地保留模态内距离信息和捕获模态间相关性。为了进一步减轻灾难性遗忘，我们使用训练好的多模态码嵌入初始化模型。最后，我们使用LoRA在多模态频率感知融合方式下高效微调LLM。在三个公共数据集上的大量实验验证了MME-SID的优越性能，这得益于其减轻嵌入崩溃和灾难性遗忘的能力。实现代码和数据集已公开可供复现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation (SR) aims to capture users' dynamic interests andsequential patterns based on their historical interactions. Recently, thepowerful capabilities of large language models (LLMs) have driven theiradoption in SR. However, we identify two critical challenges in existingLLM-based SR methods: 1) embedding collapse when incorporating pre-trainedcollaborative embeddings and 2) catastrophic forgetting of quantized embeddingswhen utilizing semantic IDs. These issues dampen the model scalability and leadto suboptimal recommendation performance. Therefore, based on LLMs likeLlama3-8B-instruct, we introduce a novel SR framework named MME-SID, whichintegrates multimodal embeddings and quantized embeddings to mitigate embeddingcollapse. Additionally, we propose a Multimodal Residual Quantized VariationalAutoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstructionloss and contrastive learning for alignment, which effectively preserveintra-modal distance information and capture inter-modal correlations,respectively. To further alleviate catastrophic forgetting, we initialize themodel with the trained multimodal code embeddings. Finally, we fine-tune theLLM efficiently using LoRA in a multimodal frequency-aware fusion manner.Extensive experiments on three public datasets validate the superiorperformance of MME-SID thanks to its capability to mitigate embedding collapseand catastrophic forgetting. The implementation code and datasets are publiclyavailable for reproduction:https://github.com/Applied-Machine-Learning-Lab/MME-SID.</description>
      <author>example@mail.com (Yuhao Wang, Junwei Pan, Xinhang Li, Maolin Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang, Xiangyu Zhao)</author>
      <guid isPermaLink="false">2509.02017v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models</title>
      <link>http://arxiv.org/abs/2509.01959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的训练范式，专门用于增强视觉语言模型对图表的理解，通过利用困难样本和专门的损失函数，在图像-文本匹配和视觉问答任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;多模态模型（如CLIP）在视觉和语言表征对齐方面表现出色，但这些模型在应用于专业视觉领域（如图表）时存在局限性，因为图表编码的是结构化、符号化信息，与自然图像不同。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的训练范式，专门设计用于增强视觉语言模型对图表图像的理解能力。&lt;h4&gt;方法&lt;/h4&gt;使用'困难'样本进行对比学习，结合两种专门的损失函数，利用图表的固有结构特性，并将这些目标整合到模型训练中，使模型能够发展出对图表内容更结构化和语义连贯的理解。&lt;h4&gt;主要发现&lt;/h4&gt;在流程图基准数据集上进行了实证验证，与标准CLIP和传统的困难负例CLIP学习范式相比，在图像-文本匹配和视觉问答任务上都有显著改进。&lt;h4&gt;结论&lt;/h4&gt;强调了针对专业任务定制训练策略的重要性，为视觉语言集成领域中图表理解的进步做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;多模态模型，如对比语言-图像预训练模型，在视觉和语言表征对齐方面已经显示出显著的成功。然而，当应用于专业视觉领域（如图表）时，这些模型表现出局限性，因为图表编码的是与自然图像不同的结构化、符号化信息。在本文中，我们引入了一种新的训练范式，专门设计用于增强视觉语言模型对图表图像的理解。我们的方法使用'困难'样本进行对比学习，结合了两种专门的损失函数，这些函数利用图表的固有结构特性。通过将这些目标整合到模型训练中，我们的方法使模型能够发展出对图表内容更结构化和语义连贯的理解。我们在流程图的基准数据集上经验性地验证了我们的方法，流程图作为图表图像的一个代表性类别，证明了与标准CLIP和传统的困难负例CLIP学习范式相比，在图像-文本匹配和视觉问答任务上都有显著改进。我们的发现强调了针对专业任务定制训练策略的重要性，并为视觉语言集成领域中图表理解的进步做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)model, have demonstrated remarkable success in aligning visual and linguisticrepresentations. However, these models exhibit limitations when applied tospecialised visual domains, such as diagrams, which encode structured, symbolicinformation distinct from that of natural imagery.  In this paper, we introduce a novel training paradigm explicitly designed toenhance the comprehension of diagrammatic images within vision-language models.Our approach uses ``hard'' samples for our proposed contrastive learning thatincorporates two specialised loss functions that leverage the inherentstructural properties of diagrams. By integrating these objectives into modeltraining, our method enables models to develop a more structured andsemantically coherent understanding of diagrammatic content.  We empirically validate our approach on a benchmark dataset of flowcharts, asa representative class of diagrammatic imagery, demonstrating substantialimprovements over standard CLIP and conventional hard negative CLIP learningparadigms for both image-text matching and visual question answering tasks. Ourfindings underscore the significance of tailored training strategies forspecialised tasks and contribute to advancing diagrammatic understanding withinthe broader landscape of vision-language integration.</description>
      <author>example@mail.com (Hiroshi Sasaki)</author>
      <guid isPermaLink="false">2509.01959v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</title>
      <link>http://arxiv.org/abs/2509.01910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种将全球地理定位与概念瓶颈相结合的新框架，通过概念感知对齐模块增强模型解释性并提高定位准确性。&lt;h4&gt;背景&lt;/h4&gt;全球地理定位涉及确定全球图像的确切地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管GeoCLIP等模型通过对比学习实现了准确预测，但其解释性仍不足，且当前基于概念的解释性方法无法有效与地理对齐的图像-位置嵌入目标保持一致。&lt;h4&gt;目的&lt;/h4&gt;解决地理定位模型解释性不足的问题，提高模型的可解释性和定位准确性。&lt;h4&gt;方法&lt;/h4&gt;提出新框架集成全球地理定位与概念瓶颈，插入概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库（如热带气候、山脉、教堂等），并最小化概念级别损失，在概念特定子空间中增强对齐。&lt;h4&gt;主要发现&lt;/h4&gt;据所知，这是首次将解释性引入地理定位的研究。实验表明该方法在地理定位准确性上超过GeoCLIP，并在各种地理空间预测任务中提升性能，揭示了地理决策过程更丰富的语义见解。&lt;h4&gt;结论&lt;/h4&gt;通过概念感知对齐模块成功将解释性引入地理定位领域，同时提高了定位准确性和其他地理空间预测任务的性能，为地理决策提供了更丰富的语义见解。&lt;h4&gt;翻译&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的确切地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管像GeoCLIP这样的地理定位模型通过对比学习利用图像和位置对齐实现了准确预测，但这些模型的解释性仍未得到充分探索。当前基于概念的解释性方法无法有效与地理对齐的图像-位置嵌入目标保持一致，导致次优的解释性和性能。为了解决这一差距，我们提出了一种将全球地理定位与概念瓶颈相结合的新框架。我们的方法插入了一个概念感知对齐模块，将图像和位置嵌入共同投影到一个共享的地理概念库（如热带气候、山脉、教堂等），并最小化概念级别的损失，从而在概念特定的子空间中增强对齐，并实现强大的可解释性。据我们所知，这是第一项将解释性引入地理定位的研究。大量实验表明，该方法在地理定位准确性上超过了GeoCLIP，并在各种地理空间预测任务中提升了性能，揭示了地理决策过程更丰富的语义见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Worldwide geo-localization involves determining the exact geographic locationof images captured globally, typically guided by geographic cues such asclimate, landmarks, and architectural styles. Despite advancements ingeo-localization models like GeoCLIP, which leverages images and locationalignment via contrastive learning for accurate predictions, theinterpretability of these models remains insufficiently explored. Currentconcept-based interpretability methods fail to align effectively withGeo-alignment image-location embedding objectives, resulting in suboptimalinterpretability and performance. To address this gap, we propose a novelframework integrating global geo-localization with concept bottlenecks. Ourmethod inserts a Concept-Aware Alignment Module that jointly projects image andlocation embeddings onto a shared bank of geographic concepts (e.g., tropicalclimate, mountain, cathedral) and minimizes a concept-level loss, enhancingalignment in a concept-specific subspace and enabling robust interpretability.To our knowledge, this is the first work to introduce interpretability intogeo-localization. Extensive experiments demonstrate that our approach surpassesGeoCLIP in geo-localization accuracy and boosts performance across diversegeospatial prediction tasks, revealing richer semantic insights into geographicdecision-making processes.</description>
      <author>example@mail.com (Furong Jia, Lanxin Liu, Ce Hou, Fan Zhang, Xinyan Liu, Yu Liu)</author>
      <guid isPermaLink="false">2509.01910v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size</title>
      <link>http://arxiv.org/abs/2509.01541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图对比学习(GCL)在图自监督学习中表现突出，但其优势取决于数据集大小和任务难度。研究表明，在标准数据集上，未经训练的基线模型可与之媲美；在大型分子数据集上，GCL在数据量超过几千个图后表现更优，但最终收益趋于平稳。&lt;h4&gt;背景&lt;/h4&gt;图对比学习已成为图自监督学习的主要范式，在标准化数据集上表现出色，并已应用于从基因组学到药物发现等多个领域。&lt;h4&gt;目的&lt;/h4&gt;研究图对比学习是否真正优于未经训练的基线模型。&lt;h4&gt;方法&lt;/h4&gt;作者在多个数据集(包括标准数据集和大型分子数据集ogbg-molhiv)上评估了GCL与未经训练的基线模型(如图神经网络、多层感知机和手工统计方法)的性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;1. GCL的优势取决于数据集大小和任务难度；2. 在标准数据集上，未经训练的基线模型可与之媲美或超越；3. 在ogbg-molhiv上，GCL在小规模时落后，超过几千个图后超越，但最终收益平稳；4. 在合成数据集上，GCL准确度随图数量对数增长，与任务复杂度相关。&lt;h4&gt;结论&lt;/h4&gt;确定数据集大小在基准测试和应用中的作用至关重要，需要设计避免性能平稳期的GCL算法。&lt;h4&gt;翻译&lt;/h4&gt;图对比学习已成为图自监督学习的主要范式，在标准化数据集上表现出色，应用范围从基因组学到药物发现不断扩大。我们提出一个基本问题：GCL是否真正优于未经训练的基线？我们发现GCL的优势取决于数据集大小和任务难度。在标准数据集上，未经训练的图神经网络、简单的多层感知机甚至手工设计的统计方法都可以与GCL相媲美或超越。在大型分子数据集ogbg-molhiv上，我们观察到交叉点：GCL在小规模时落后，但在几千个图之后超越，尽管这一收益最终趋于平稳。在合成数据集上，GCL的准确度大致随图数量的对数缩放，与未经训练的GNN相比的性能差距随任务复杂度变化。未来，确定数据集大小在基准测试和应用中的作用至关重要，同时需要设计避免性能平稳期的GCL算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-supervised learning on graphs, with strong performance reported on standardizeddatasets and growing applications ranging from genomics to drug discovery. Weask a basic question: does GCL actually outperform untrained baselines? We findthat GCL's advantage depends strongly on dataset size and task difficulty. Onstandard datasets, untrained Graph Neural Networks (GNNs), simple multilayerperceptrons, and even handcrafted statistics can rival or exceed GCL. On thelarge molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at smallscales but pulls ahead beyond a few thousand graphs, though this gaineventually plateaus. On synthetic datasets, GCL accuracy approximately scaleswith the logarithm of the number of graphs and its performance gap (comparedwith untrained GNNs) varies with respect to task complexity. Moving forward, itis crucial to identify the role of dataset size in benchmarks and applications,as well as to design GCL algorithms that avoid performance plateaus.</description>
      <author>example@mail.com (Smayan Khanna, Doruk Efe Gökmen, Risi Kondor, Vincenzo Vitelli)</author>
      <guid isPermaLink="false">2509.01541v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Representation Learning for Real-Time Ultrasound Analysis</title>
      <link>http://arxiv.org/abs/2509.01433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICMl 2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从超声视频中学习有效时间表示的方法，特别关注基于超声心动图的射血分数估计。通过利用时间一致的掩码和对比学习来增强模型捕捉心脏运动模式的能力，在EchoNet-Dynamic数据集上取得了显著的EF预测准确性改进。&lt;h4&gt;背景&lt;/h4&gt;超声成像是医学诊断中的重要工具，能够实时可视化生理过程。其主要优势是能够捕捉时间动态性，这对于评估心脏监测、胎儿发育和血管成像等应用中的运动模式至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种从超声视频中学习有效时间表示的方法，重点关注基于超声心动图的射血分数估计，解决当前深度学习模型忽视超声序列时间连续性的问题。&lt;h4&gt;方法&lt;/h4&gt;利用时间一致的掩码和对比学习来强制视频帧之间的时间连贯性，增强模型表示运动模式的能力，特别针对心脏的节律性收缩和舒张过程。&lt;h4&gt;主要发现&lt;/h4&gt;在EchoNet-Dynamic数据集上评估，该方法在EF预测准确性方面取得了显著改进，证明了时间感知的表示学习对提高超声分析性能的重要性。&lt;h4&gt;结论&lt;/h4&gt;时间感知的表示学习对实时超声分析至关重要，特别是在需要捕捉动态过程的应用中，如心脏功能的评估。&lt;h4&gt;翻译&lt;/h4&gt;超声成像是医学诊断中的关键工具，能够实时可视化生理过程。其主要优势之一是能够捕捉时间动态性，这对于评估心脏监测、胎儿发育和血管成像等应用中的运动模式至关重要。尽管超声成像很重要，但当前的深度学习模型常常忽视超声序列的时间连续性，独立分析帧而错过了关键的时间依赖关系。为了解决这一差距，我们提出了一种从超声视频中学习有效时间表示的方法，重点关注基于超声心动图的射血分数估计。EF预测作为案例研究，很好地证明了时间学习的必要性，因为它需要捕捉心脏的节律性收缩和舒张。我们的方法利用时间一致的掩码和对比学习来强制视频帧之间的时间连贯性，增强模型表示运动模式的能力。在EchoNet-Dynamic数据集上评估，我们的方法在EF预测准确性方面取得了显著改进，强调了时间感知的表示学习对实时超声分析的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultrasound (US) imaging is a critical tool in medical diagnostics, offeringreal-time visualization of physiological processes. One of its major advantagesis its ability to capture temporal dynamics, which is essential for assessingmotion patterns in applications such as cardiac monitoring, fetal development,and vascular imaging. Despite its importance, current deep learning modelsoften overlook the temporal continuity of ultrasound sequences, analyzingframes independently and missing key temporal dependencies. To address thisgap, we propose a method for learning effective temporal representations fromultrasound videos, with a focus on echocardiography-based ejection fraction(EF) estimation. EF prediction serves as an ideal case study to demonstrate thenecessity of temporal learning, as it requires capturing the rhythmiccontraction and relaxation of the heart. Our approach leverages temporallyconsistent masking and contrastive learning to enforce temporal coherenceacross video frames, enhancing the model's ability to represent motionpatterns. Evaluated on the EchoNet-Dynamic dataset, our method achieves asubstantial improvement in EF prediction accuracy, highlighting the importanceof temporally-aware representation learning for real-time ultrasound analysis.</description>
      <author>example@mail.com (Yves Stebler, Thomas M. Sutter, Ece Ozkan, Julia E. Vogt)</author>
      <guid isPermaLink="false">2509.01433v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning</title>
      <link>http://arxiv.org/abs/2509.01166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025, Main, Long Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SAT框架，通过结构感知对齐调用来增强大语言模型在知识图谱补全任务中的应用，解决了现有方法中自然语言与图结构表示空间不一致以及任务特定指令设计繁琐的问题。&lt;h4&gt;背景&lt;/h4&gt;知识图谱补全(KGC)旨在从知识图谱中推断新知识和进行预测。最近，大语言模型(LLMs)展示了显著的推理能力，LLM增强的KGC方法主要通过设计特定任务指令取得进展。&lt;h4&gt;目的&lt;/h4&gt;解决现有LLM增强KGC方法中的两个关键挑战：自然语言和图结构之间不一致的表示空间，以及为不同KGC任务设计单独指令导致的重复工作和耗时过程。&lt;h4&gt;方法&lt;/h4&gt;提出了SAT框架，包含两个主要组件：1) 分层知识对齐，通过多任务对比学习将图嵌入与自然语言空间对齐；2) 结构指令调优，使用统一的图指令结合轻量级知识适配器，引导LLM在知识图谱上进行结构感知推理。&lt;h4&gt;主要发现&lt;/h4&gt;在两个KGC任务和四个基准数据集上的实验结果表明，SAT显著优于最先进的方法，特别是在链接预测任务上的改进范围从8.7%到29.8%。&lt;h4&gt;结论&lt;/h4&gt;SAT框架有效解决了现有LLM增强KGC方法中的关键挑战，提高了知识图谱补全任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱补全(KGC)旨在从知识图谱中推断新知识和进行预测。最近，大语言模型(LLMs)展示了显著的推理能力。LLM增强的KGC方法主要专注于设计特定任务的指令，取得了有希望的进展。然而，仍存在两个关键挑战：首先，现有方法常常忽略自然语言和图结构之间不一致的表示空间；其次，大多数方法为不同的KGC任务设计单独的指令，导致重复工作和耗时过程。为解决这些挑战，我们提出了SAT，一个通过结构感知对齐调用来增强LLM用于KGC的新框架。具体而言，我们首先引入分层知识对齐，通过多任务对比学习将图嵌入与自然语言空间对齐。然后，我们提出结构指令调优，使用统一的图指令结合轻量级知识适配器，引导LLM在知识图谱上进行结构感知推理。在四个基准数据集的两个KGC任务上的实验结果表明，SAT显著优于最先进的方法，特别是在链接预测任务上的改进范围从8.7%到29.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graph completion (KGC) aims to infer new knowledge and makepredictions from knowledge graphs. Recently, large language models (LLMs) haveexhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarilyfocus on designing task-specific instructions, achieving promisingadvancements. However, there are still two critical challenges. First, existingmethods often ignore the inconsistent representation spaces between naturallanguage and graph structures. Second, most approaches design separateinstructions for different KGC tasks, leading to duplicate works andtime-consuming processes. To address these challenges, we propose SAT, a novelframework that enhances LLMs for KGC via structure-aware alignment-tuning.Specifically, we first introduce hierarchical knowledge alignment to aligngraph embeddings with the natural language space through multi-task contrastivelearning. Then, we propose structural instruction tuning to guide LLMs inperforming structure-aware reasoning over KGs, using a unified graphinstruction combined with a lightweight knowledge adapter. Experimental resultson two KGC tasks across four benchmark datasets demonstrate that SATsignificantly outperforms state-of-the-art methods, especially in the linkprediction task with improvements ranging from 8.7% to 29.8%.</description>
      <author>example@mail.com (Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan)</author>
      <guid isPermaLink="false">2509.01166v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning</title>
      <link>http://arxiv.org/abs/2509.01119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, Accepted to IEEE Transactions on Mobile Computing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SC-GIR（基于不变表示的目标导向语义通信）的新框架，用于图像传输。该框架利用自监督学习提取不变表示，实现了高效的通信，同时保留了成功执行下游任务的关键特征。实验表明，SC-GIR比基线方案提高了约10%，并在不同信噪比条件下实现了超过85%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;目标导向语义通信（SC）旨在通过仅传输任务必需的信息来革新通信系统。然而，当前方法收发器联合训练面临挑战，导致冗余数据交换和依赖标记数据集，这限制了其任务无关的实用性。&lt;h4&gt;目的&lt;/h4&gt;解决当前目标导向语义通信方法中的问题，特别是收发器联合训练导致的冗余数据交换和依赖标记数据集的限制，提高任务无关的实用性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SC-GIR（基于不变表示的目标导向语义通信）的新框架。该框架利用自监督学习提取不变表示，该表示封装了来自源数据的关键信息，独立于特定的下游任务。使用基于协方差的对比学习技术获得有意义且语义密集的潜在表示。在各种图像数据集上应用该框架进行有损压缩，然后在目标导向AI任务中使用压缩表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的大量实验表明，SC-GIR比基线方案提高了约10%，并在不同信噪比条件下实现了超过85%的分类准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在学习紧凑和信息丰富的潜在表示方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;目标导向语义通信（SC）旨在通过仅传输任务必需的信息来革新通信系统。然而，当前方法在收发器联合训练方面面临挑战，导致冗余数据交换和依赖标记数据集，这限制了其任务无关的实用性。为了解决这些挑战，我们提出了一种名为基于不变表示的目标导向语义通信（SC-GIR）的新框架用于图像传输。我们的框架利用自监督学习提取不变表示，该表示封装了来自源数据的关键信息，独立于特定的下游任务。这种压缩表示促进了高效通信，同时保留了成功执行下游任务的关键特征。专注于机器间任务，我们使用基于协方差的对比学习技术获得有意义且语义密集的潜在表示。为了评估所提方案在下游任务上的有效性，我们在各种图像数据集上应用它进行有损压缩。然后，压缩表示用于目标导向AI任务。在多个数据集上的大量实验表明，SC-GIR比基线方案提高了约10%，并在不同信噪比条件下实现了超过85%的压缩数据分类准确率。这些结果强调了所提框架在学习紧凑和信息丰富的潜在表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TMC.2025.3600434&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Goal-oriented semantic communication (SC) aims to revolutionize communicationsystems by transmitting only task-essential information. However, currentapproaches face challenges such as joint training at transceivers, leading toredundant data exchange and reliance on labeled datasets, which limits theirtask-agnostic utility. To address these challenges, we propose a novelframework called Goal-oriented Invariant Representation-based SC (SC-GIR) forimage transmission. Our framework leverages self-supervised learning to extractan invariant representation that encapsulates crucial information from thesource data, independent of the specific downstream task. This compressedrepresentation facilitates efficient communication while retaining key featuresfor successful downstream task execution. Focusing on machine-to-machine tasks,we utilize covariance-based contrastive learning techniques to obtain a latentrepresentation that is both meaningful and semantically dense. To evaluate theeffectiveness of the proposed scheme on downstream tasks, we apply it tovarious image datasets for lossy compression. The compressed representationsare then used in a goal-oriented AI task. Extensive experiments on severaldatasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,and achieves over 85% classification accuracy for compressed data underdifferent SNR conditions. These results underscore the effectiveness of theproposed framework in learning compact and informative latent representations.</description>
      <author>example@mail.com (Senura Hansaja Wanasekara, Van-Dinh Nguyen, Kok-Seng, M. -Duong Nguyen, Symeon Chatzinotas, Octavia A. Dobre)</author>
      <guid isPermaLink="false">2509.01119v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings</title>
      <link>http://arxiv.org/abs/2509.00842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种多粒度困难负样本(MGH)合成框架和锚点标记感知(ATA)池化方法，用于改进文本嵌入模型，在MTEB基准测试上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入模型对各种自然语言处理任务至关重要，它们能有效将语义信息编码为密集向量表示。这些模型通常使用三元组(query, positive, negative)数据进行对比学习优化，其中负样本在增强模型辨别细微语义差异方面起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;引入一种多粒度困难负样本(MGH)合成框架，利用大型语言模型生成多样化负样本；提出一种锚点标记感知(ATA)池化方法，基于在LLMs中观察到的聚合模式为锚点标记分配更高权重，以提高文本嵌入准确性而不增加模型复杂性。&lt;h4&gt;方法&lt;/h4&gt;1. 多粒度困难负样本(MGH)合成框架：利用大型语言模型生成多样化负样本，实现从粗到细的课程学习策略；2. 锚点标记感知(ATA)池化方法：根据LLMs中的聚合模式为锚点标记分配更高权重，提高文本嵌入准确性而不增加模型复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;在MTEB基准测试上的全面实验表明，所提出的方法实现了最先进的性能，超越了现有的合成策略，无论是使用合成数据还是与公共检索数据集结合使用时。&lt;h4&gt;结论&lt;/h4&gt;通过MGH合成框架和ATA池化方法，文本嵌入模型能够更有效地学习语义表示，在各种自然语言处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;文本嵌入模型对各种自然语言处理任务至关重要，能够将语义信息有效编码为密集向量表示。这些模型通常使用三元组(query, positive, negative)数据进行对比学习优化，其中负样本在增强模型辨别细微语义差异方面起着关键作用。在本工作中，我们引入了一种多粒度困难负样本(MGH)合成框架，利用大型语言模型生成与查询具有不同相似度级别的多样化负样本。这种方法在监督训练过程中实现了从粗到细的课程学习策略，使嵌入模型能够逐步学习更细致的语义表示。同时，我们提出了一种锚点标记感知(ATA)池化方法，基于在LLMs中观察到的聚合模式为锚点标记分配更高权重，提高了文本嵌入准确性而不增加模型复杂性。在MTEB基准测试上的全面实验表明，我们的方法实现了最先进的性能，超越了现有的合成策略，无论是使用合成数据还是与公共检索数据集结合使用时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.18653/v1/2025.acl-long.1501&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text embedding models are essential for various natural language processingtasks, enabling the effective encoding of semantic information into densevector representations. These models are typically optimized using triplets of(query, positive, negative) data pairs for contrastive learning, where thenegative samples play a critical role in enhancing the model's ability todiscern subtle semantic distinctions. In this work, we introduce aMulti-Granularity Hard-negative (MGH) synthesis framework that leverages largelanguage models (LLMs) to generate diverse negative samples with varying levelsof similarity with the query. This approach facilitates a coarse-to-finecurriculum learning strategy during supervised training, allowing the embeddingmodel to progressively learn more nuanced semantic representations. Meanwhile,we propose an Anchor Token Aware (ATA) pooling method that assigns higherweights to anchor tokens based on aggregation patterns observed in LLMs,improving text embedding accuracy without increasing model complexity.Comprehensive experiments on the MTEB benchmark demonstrate that our methodsachieve state-of-the-art performance, surpassing existing synthesis strategiesboth with synthetic data and when combined with public retrieval datasets.</description>
      <author>example@mail.com (Tengyu Pan, Zhichao Duan, Zhenyu Li, Bowen Dong, Ning Liu, Xiuxing Li, Jianyong Wang)</author>
      <guid isPermaLink="false">2509.00842v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification</title>
      <link>http://arxiv.org/abs/2509.00752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对耳鼻喉科内窥镜图像分析的统一视觉-语言框架，同时处理图像分类、图像到图像检索和文本到图像检索三个临床相关任务。&lt;h4&gt;背景&lt;/h4&gt;传统基于CNN的管道难以捕捉跨模态语义，而医疗数据有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的视觉-语言框架，能够同时处理多个临床相关任务，并在有限医疗数据上实现高效微调。&lt;h4&gt;方法&lt;/h4&gt;使用CLIP ViT-B/16作为主干网络，通过低秩适配、多级CLS标记聚合和球形特征插值进行增强；引入特定类别的自然语言提示，结合监督分类和对比学习进行联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;在ACM MM'25 ENTRep Grand Challenge中取得优异成绩，包括95%的分类准确率和F1分数，0.93的图像到图像检索Recall@1，0.92的文本到图像检索Recall@1，以及0.97和0.96的MRR分数。&lt;h4&gt;结论&lt;/h4&gt;消融研究证明了每个架构组件的增量效益，验证了该设计在资源有限的临床环境中对鲁棒多模态医疗理解的有效性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种专为耳鼻喉科内窥镜图像分析设计的统一视觉-语言框架，同时解决三个临床相关任务：图像分类、图像到图像检索和文本到图像检索。与难以捕捉跨模态语义的传统基于CNN的管道不同，我们的方法利用CLIP ViT-B/16主干网络，并通过低秩适配、多级CLS标记聚合和球形特征插值进行增强。这些组件共同实现在有限医疗数据上的高效微调，同时提高跨模态的表示多样性和语义对齐。为了弥合视觉输入和文本诊断上下文之间的差距，我们引入了特定类别的自然语言提示，通过结合监督分类和对比学习的联合训练目标指导图像编码器。我们通过参加ACM MM'25 ENTRep Grand Challenge验证了我们的框架，在分类中达到95%的准确率和F1分数，图像到图像和文本到图像检索的Recall@1分别为0.93和0.92，MRS分数分别为0.97和0.96。消融研究证明了每个架构组件的增量效益，验证了我们的设计在资源有限的临床环境中对鲁棒多模态医疗理解的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3762093&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a unified vision-language framework tailored for ENT endoscopyimage analysis that simultaneously tackles three clinically-relevant tasks:image classification, image-to-image retrieval, and text-to-image retrieval.Unlike conventional CNN-based pipelines that struggle to capture cross-modalsemantics, our approach leverages the CLIP ViT-B/16 backbone and enhances itthrough Low-Rank Adaptation, multi-level CLS token aggregation, and sphericalfeature interpolation. These components collectively enable efficientfine-tuning on limited medical data while improving representation diversityand semantic alignment across modalities. To bridge the gap between visualinputs and textual diagnostic context, we introduce class-specific naturallanguage prompts that guide the image encoder through a joint trainingobjective combining supervised classification with contrastive learning. Wevalidated our framework through participation in the ACM MM'25 ENTRep GrandChallenge, achieving 95% accuracy and F1-score in classification, Recall@1 of0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, andMRR scores of 0.97 and 0.96. Ablation studies demonstrated the incrementalbenefits of each architectural component, validating the effectiveness of ourdesign for robust multimodal medical understanding in low-resource clinicalsettings.</description>
      <author>example@mail.com (Y Hop Nguyen, Doan Anh Phan Huu, Trung Thai Tran, Nhat Nam Mai, Van Toi Giap, Thao Thi Phuong Dao, Trung-Nghia Le)</author>
      <guid isPermaLink="false">2509.00752v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Valid Property-Enhanced Contrastive Learning for Targeted Optimization &amp; Resampling for Novel Drug Design</title>
      <link>http://arxiv.org/abs/2509.00684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/amartya21/vector-drug-design.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VECTOR+的新框架，用于在低数据条件下生成具有药理学相关特性的分子。该方法结合了属性引导的表示学习和可控的分子生成，适用于回归和分类任务，能够高效地探索功能性化学空间。&lt;h4&gt;背景&lt;/h4&gt;在分子药物发现中，特别是在数据有限的情况下，有效地引导生成模型朝向药理学相关的化学空间区域仍然是一个主要障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，能够在低数据条件下进行属性条件化的分子设计，结合对比学习和生成模型，实现可重现、AI加速的分子发现。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VECTOR+（Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling），这是一个将属性引导的表示学习与可控分子生成相结合的框架。该方法适用于回归和分类任务，能够对功能性化学空间进行可解释、数据高效的探索。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在两个数据集上进行了评估：PD-L1抑制剂集合和受体激酶抑制剂集合。2. 尽管训练数据有限，VECTOR+能够生成新颖且具有合成可行性的候选分子。3. 针对PD-L1，生成分子中有100个超过了-15.0千卡每摩尔的对接阈值，最佳得分达到-17.6千卡每摩尔。4. 性能最佳的分子保留了保守的联苯药效团，同时引入了新的基序。5. 分子动力学证实了结合稳定性。6. VECTOR+可推广到激酶抑制剂，产生比已确立药物具有更强对接得分的化合物。7. 与其他方法的基准测试突显了该方法的优越性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果将作者的工作确立为一种在低数据设置下进行属性条件化分子设计的稳健、可扩展方法，将对比学习和生成建模相结合，实现可重现、AI加速的分子发现。&lt;h4&gt;翻译&lt;/h4&gt;在低数据条件下，有效地引导生成模型朝向药理学相关的化学空间区域仍然是分子药物发现中的一个主要障碍。我们提出了VECTOR+（Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling），这是一个将属性引导的表示学习与可控分子生成相结合的框架。VECTOR+适用于回归和分类任务，能够对功能性化学空间进行可解释、数据高效的探索。我们在两个数据集上进行了评估：一个经过整理的PD-L1抑制剂集合（296个具有实验IC50值的化合物）和一个受体激酶抑制剂集合（2,056个按结合模式分类的分子）。尽管训练数据有限，VECTOR+能够生成新颖且具有合成可行性的候选分子。针对PD-L1（PDB 5J89），在8,374个生成分子中有100个超过了-15.0千卡每摩尔的对接阈值，最佳得分达到-17.6千卡每摩尔，而顶级参考抑制剂的得分为-15.4千卡每摩尔。性能最佳的分子保留了保守的联苯药效团，同时引入了新的基序。分子动力学（250纳秒）证实了结合稳定性（配体RMSD &lt; 2.5埃）。VECTOR+可推广到激酶抑制剂，产生比已确立药物（如brigatinib和sorafenib）具有更强对接得分的化合物。与JT-VAE和MolGPT在对接、新颖性、独特性和Tanimoto相似性方面的基准测试突显了该方法的优越性能。这些结果将作者的工作确立为一种在低数据设置下进行属性条件化分子设计的稳健、可扩展方法，将对比学习和生成建模相结合，实现可重现、AI加速的分子发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently steering generative models toward pharmacologically relevantregions of chemical space remains a major obstacle in molecular drug discoveryunder low-data regimes. We present VECTOR+: Valid-property-Enhanced ContrastiveLearning for Targeted Optimization and Resampling, a framework that couplesproperty-guided representation learning with controllable molecule generation.VECTOR+ applies to both regression and classification tasks and enablesinterpretable, data-efficient exploration of functional chemical space. Weevaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds withexperimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056molecules by binding mode). Despite limited training data, VECTOR+ generatesnovel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, withthe best scoring $-17.6$ kcal/mol compared to the top reference inhibitor($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenylpharmacophore while introducing novel motifs. Molecular dynamics (250 ns)confirm binding stability (ligand RMSD &lt; $2.5$ angstroms). VECTOR+ generalizesto kinase inhibitors, producing compounds with stronger docking scores thanestablished drugs such as brigatinib and sorafenib. Benchmarking against JT-VAEand MolGPT across docking, novelty, uniqueness, and Tanimoto similarityhighlights the superior performance of our method. These results position ourwork as a robust, extensible approach for property-conditioned molecular designin low-data settings, bridging contrastive learning and generative modeling forreproducible, AI-accelerated discovery.</description>
      <author>example@mail.com (Amartya Banerjee, Somnath Kar, Anirban Pal, Debabrata Maiti)</author>
      <guid isPermaLink="false">2509.00684v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive clustering based on regular equivalence for influential node identification in complex networks</title>
      <link>http://arxiv.org/abs/2509.02609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ReCC的深度无监督框架，用于识别复杂网络中的影响力节点，克服了现有监督方法对标记数据的依赖限制。&lt;h4&gt;背景&lt;/h4&gt;识别复杂网络中的影响力节点是网络分析的基本任务，有广泛的应用。现有监督方法受限于对标记数据的依赖，在实际标记数据稀缺的场景中适用性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记数据的影响力节点识别方法，解决现有监督方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出ReCC框架，将影响力节点识别重新定义为无标记深度聚类问题，开发基于正则等价相似性的对比学习机制，整合到图卷积网络中学习节点嵌入，使用网络重建损失预训练，结合对比和聚类损失微调，并通过结合结构指标和正则等价相似性增强节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;ReCC在多个基准测试中表现优于现有的最先进方法，证明了其在影响力节点识别任务上的有效性。&lt;h4&gt;结论&lt;/h4&gt;ReCC作为一种无监督学习方法，能够在不依赖标记数据的情况下有效识别复杂网络中的影响力节点，具有更广泛的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在复杂网络中识别影响力节点是网络分析中的一个基本任务，在各领域有广泛应用。虽然深度学习已推进了节点影响力检测，但现有监督方法仍受限于对标记数据的依赖，在标记数据稀缺或不可用的实际场景中适用性有限。虽然对比学习展示了显著的性能提升潜力，但现有方法主要依赖多嵌入生成来构建正/负样本对。为克服这些限制，我们提出了ReCC（基于正则等价的对比聚类），一种新颖的深度无监督框架，用于影响力节点识别。我们首先将影响力节点识别重新定义为无标记深度聚类问题，然后开发了利用基于正则等价相似性的对比学习机制，这种相似性捕获了节点超越局部邻域的结构相似性，用于生成正负样本。该机制被整合到图卷积网络中，用于学习用于区分影响力节点与非影响力节点的节点嵌入。ReCC使用网络重建损失进行预训练，并结合对比和聚类损失进行微调，两个阶段都独立于标记数据。此外，ReCC通过结合结构指标和基于正则等价的相似性来增强节点表示。大量实验表明，ReCC在多个基准测试中优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying influential nodes in complex networks is a fundamental task innetwork analysis with wide-ranging applications across domains. While deeplearning has advanced node influence detection, existing supervised approachesremain constrained by their reliance on labeled data, limiting theirapplicability in real-world scenarios where labels are scarce or unavailable.While contrastive learning demonstrates significant potential for performanceenhancement, existing approaches predominantly rely on multiple-embeddinggeneration to construct positive/negative sample pairs. To overcome theselimitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based\textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised frameworkfor influential node identification. We first reformalize influential nodeidentification as a label-free deep clustering problem, then develop acontrastive learning mechanism that leverages regular equivalence-basedsimilarity, which captures structural similarities between nodes beyond localneighborhoods, to generate positive and negative samples. This mechanism isintegrated into a graph convolutional network to learn node embeddings that areused to differentiate influential from non-influential nodes. ReCC ispre-trained using network reconstruction loss and fine-tuned with a combinedcontrastive and clustering loss, with both phases being independent of labeleddata. Additionally, ReCC enhances node representations by combining structuralmetrics with regular equivalence-based similarities. Extensive experimentsdemonstrate that ReCC outperforms state-of-the-art approaches across severalbenchmarks.</description>
      <author>example@mail.com (Yanmei Hu, Yihang Wu, Bing Sun, Xue Yue, Biao Cai, Xiangtao Li, Yang Chen)</author>
      <guid isPermaLink="false">2509.02609v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>CoMET: A Contrastive-Masked Brain Foundation Model for Universal EEG Representation</title>
      <link>http://arxiv.org/abs/2509.00314v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoMET是一种新型脑基础模型，通过重新设计的掩码自编码器和对比学习框架解决现有EEG模型过度关注局部特征而忽略全局判别模式的问题。该模型在3000多名受试者、超过100万个样本的混合EEG数据集上预训练，并在十个下游数据集上取得了最先进的结果，展示了提取通用EEG表示和临床应用的强大潜力。&lt;h4&gt;背景&lt;/h4&gt;EEG是一种非侵入性记录脑活动的技术，广泛应用于脑机接口、临床和医疗保健领域。传统EEG深度模型通常专注于特定数据集和任务，限制了模型大小和泛化能力。最近，自监督脑基础模型出现并应用于各种下游任务。&lt;h4&gt;目的&lt;/h4&gt;解决现有自监督EEG模型的局限性，特别是它们过度关注局部区域的低维信号相似性特征而忽略全局判别模式的问题，提出一种能够提取全局判别模式的脑基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出名为CoMET的脑基础模型，采用重新设计的分块和嵌入的掩码自编码器作为骨干网络，设计了一种新的对比学习框架，包含镜像尺度增强来加强全局判别能力。在超过3000个受试者、超过100万个样本的混合EEG数据集上进行预训练，并在十个不同的下游数据集上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;CoMET在十个不同的下游数据集上取得了最先进的实验结果，证明了其提取通用EEG表示的优越能力和强大的临床潜力。&lt;h4&gt;结论&lt;/h4&gt;CoMET模型有效解决了现有EEG基础模型的局限性，能够提取更全面的EEG特征，包括全局判别模式，在多种下游任务中表现优异，具有良好的临床应用前景。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)是一种记录脑活动的非侵入性技术，广泛应用于脑机接口、临床和医疗保健领域。传统的EEG深度模型通常专注于特定数据集和任务，限制了模型大小和泛化能力。最近，自监督脑基础模型已经出现并应用于各种下游任务。然而，这些模型仍然存在局限性：当前的最先进模型通常依赖掩码重建策略；但是，相邻通道的EEG特征高度相关，这导致预训练过度关注局部区域的低维信号相似性特征，而忽略了对下游任务至关重要的全局判别模式。为解决这些局限性，我们提出了一种名为CoMET的脑基础模型。具体来说，我们采用重新设计的分块和嵌入的掩码自编码器作为EEG的骨干网络，并设计了一种新颖的包含镜像尺度增强的对比学习框架，以加强全局判别能力。CoMET在超过3000个受试者、超过100万个样本的混合EEG数据集上进行预训练。它在十个不同的下游数据集上进行了评估，最先进的结果证明了CoMET提取通用EEG表示的优越能力和强大的临床潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) is a non-invasive technique for recording brainactivity, widely used in brain-computer interfaces, clinic, and healthcare.Traditional EEG deep models typically focus on specific dataset and task,limiting model size and generalization. Recently, self-supervised brainfoundation models have emerged and been applied to various downstream tasks.Nevertheless, these models still have limitations: current SOTA modelstypically rely on masked reconstruction strategy; however, EEG features ofadjacent channels are highly correlated, which causes the pre-training tooverly focus on low-dimensional signal-similarity features in local regions andneglect the global discriminative patterns vital for downstream tasks. Toaddress these limitations, we propose a brain foundation model called CoMET.Specifically, we employ the masked autoencoder with redesigned patching andembedding for EEG as backbone and devise a novel contrastive learning frameworkwith mirror-scale augmentation to strengthen the global discrimination ability.CoMET is pre-trained on mixed EEG datasets over 3000 subjects with over onemillion samples. It is evaluated on ten different downstream datasets, and theSOTA results demonstrate CoMET's superior ability in extracting universal EEGrepresentations and strong clinical potential.</description>
      <author>example@mail.com (Ang Li, Zikai Wang, Liuyin Yang, Zhenyu Wang, Tianheng Xu, Honglin Hu, Marc M. Van Hulle)</author>
      <guid isPermaLink="false">2509.00314v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</title>
      <link>http://arxiv.org/abs/2509.00311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出MorphGen方法，通过整合组织病理学图像、增强技术和细胞核分割掩膜，在监督对比学习框架中学习对领域变化具有鲁棒性的癌症表征，解决计算病理学中的领域泛化问题。&lt;h4&gt;背景&lt;/h4&gt;计算病理学中的领域泛化受到全幻灯片图像异质性的阻碍，这种异质性由不同机构间的组织制备、染色和成像条件差异造成。与机器学习系统不同，病理学家依靠跨不同环境保持诊断价值的领域不变形态学线索，如细胞核异型性、结构异型性和整体形态异型性。&lt;h4&gt;目的&lt;/h4&gt;假设通过明确建模生物学上稳健的细胞核形态和空间组织，能够学习到对领域变化具有韧性的癌症表征，提高模型在不同数据分布上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MorphGen是一种在监督对比学习框架中整合组织病理学图像、增强技术和细胞核分割掩膜的方法。通过对齐图像和细胞核掩膜的潜在表征，优先考虑诊断特征而非染色伪影。此外，引入随机权重平均（SWA）技术，以优化更平坦的最小值，进一步增强分布外鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;注意力图分析显示，MorphGen主要依赖于细胞核形态、细胞组成和肿瘤或正常区域内的空间细胞组织进行分类。学习到的表征对图像损坏和对抗性攻击具有韧性，不仅展示了分布外泛化能力，还解决了当前深度学习系统在数字病理学中的关键漏洞。&lt;h4&gt;结论&lt;/h4&gt;MorphGen通过模拟病理学家使用的领域不变形态学线索，成功提高了计算病理学模型在不同机构数据上的泛化能力，增强了模型对图像损坏和对抗性攻击的鲁棒性，对数字病理学领域的实际应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学中的领域泛化受到全幻灯片图像异质性的阻碍，这种异质性由不同机构间的组织制备、染色和成像条件差异造成。与机器学习系统不同，病理学家依靠跨不同环境保持诊断价值的领域不变形态学线索，如细胞核异型性（增大、不规则轮廓、深染、染色质纹理、空间无序）、结构异型性（异常结构和腺体形成）和整体形态异型性。受此启发，我们假设通过明确建模生物学上稳健的细胞核形态和空间组织，能够学习到对领域变化具有韧性的癌症表征。我们提出了MorphGen（形态引导泛化），一种在监督对比学习框架中整合组织病理学图像、增强技术和细胞核分割掩膜的方法。通过对齐图像和细胞核掩膜的潜在表征，MorphGen优先考虑细胞核和形态异型性以及空间组织等诊断特征，而非染色伪影和领域特定特征。为进一步增强分布外鲁棒性，我们纳入了随机权重平均（SWA），引导优化朝向更平坦的最小值。注意力图分析显示，MorphGen主要依赖于细胞核形态、细胞组成和肿瘤或正常区域内的空间细胞组织进行最终分类。最后，我们证明了学习到的表征对图像损坏（如染色伪影）和对抗性攻击具有韧性，不仅展示了分布外泛化能力，还解决了当前深度学习系统在数字病理学中的关键漏洞。代码、数据集和训练模型可在以下网址获取：https://github.com/hikmatkhan/MorphGen&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain generalization in computational histopathology is hindered byheterogeneity in whole slide images (WSIs), caused by variations in tissuepreparation, staining, and imaging conditions across institutions. Unlikemachine learning systems, pathologists rely on domain-invariant morphologicalcues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,chromatin texture, spatial disorganization), structural atypia (abnormalarchitecture and gland formation), and overall morphological atypia that remaindiagnostic across diverse settings. Motivated by this, we hypothesize thatexplicitly modeling biologically robust nuclear morphology and spatialorganization will enable the learning of cancer representations that areresilient to domain shifts. We propose MorphGen (Morphology-GuidedGeneralization), a method that integrates histopathology images, augmentations,and nuclear segmentation masks within a supervised contrastive learningframework. By aligning latent representations of images and nuclear masks,MorphGen prioritizes diagnostic features such as nuclear and morphologicalatypia and spatial organization over staining artifacts and domain-specificfeatures. To further enhance out-of-distribution robustness, we incorporatestochastic weight averaging (SWA), steering optimization toward flatter minima.Attention map analyses revealed that MorphGen primarily relies on nuclearmorphology, cellular composition, and spatial cell organization within tumorsor normal regions for final classification. Finally, we demonstrate resilienceof the learned representations to image corruptions (such as stainingartifacts) and adversarial attacks, showcasing not only OOD generalization butalso addressing critical vulnerabilities in current deep learning systems fordigital pathology. Code, datasets, and trained models are available at:https://github.com/hikmatkhan/MorphGen</description>
      <author>example@mail.com (Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, Jia Wu)</author>
      <guid isPermaLink="false">2509.00311v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network</title>
      <link>http://arxiv.org/abs/2509.03241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于神经网络的解决方案，用于可重构智能表面（RIS）的相位配置和资源分配优化。通过结合预处理技术和五层全连接神经网络，显著降低了计算复杂度，提高了系统吞吐量，并增强了可扩展性。&lt;h4&gt;背景&lt;/h4&gt;无线系统中对高速率和无缝连接的需求不断增长，激发了对可重构智能表面（RIS）和基于人工智能的无线应用的兴趣。RIS通常由被动反射天线元件组成，通过调节反射元件的相位来控制无线传播环境。将RIS元件分配给多个用户设备对于有效利用RIS至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决RIS相位配置和资源分配的联合优化问题，特别是在α公平调度框架下。同时，克服传统迭代优化方法随着RIS元件数量增加而导致的计算复杂度指数增长问题，以及监督学习中训练标签生成的复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合预处理技术的五层全连接神经网络（FNN）方法，以显著降低输入维度、降低计算复杂度并增强可扩展性。这种方法解决了传统迭代优化方法的局限性。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提出的基于神经网络的解决方案与现有的RIS元件分配方案相比，在减少计算开销的同时，显著提高了系统吞吐量6.8%。此外，所提出的系统在降低计算复杂度的同时实现了更好的性能，使其比迭代优化算法具有显著更高的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;基于神经网络的解决方案在RIS无线通信系统中表现出色，不仅提高了系统性能，还解决了传统方法的计算复杂度问题，为未来无线通信系统提供了更高效、可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无线系统中对高速数据和无缝连接日益增长的需求，激发了对可重构智能表面（RIS）和基于人工智能的无线应用的浓厚兴趣。RIS通常由被动反射天线元件组成，通过适当调节反射元件的相位来控制无线传播环境。将RIS元件分配给多个用户设备对于有效利用RIS至关重要。在这项工作中，我们制定了一个联合优化问题，在α公平调度框架下优化RIS相位配置和资源分配，并提出了一种高效的RIS元件分配方式。然而，传统的迭代优化方法随着RIS元件数量的增加而面临计算复杂度指数增长的问题，同时也增加了监督学习中训练标签生成的复杂性。为了克服这些挑战，我们提出了一种结合预处理技术的五层全连接神经网络（FNN），以显著降低输入维度、降低计算复杂度并增强可扩展性。仿真结果表明，我们提出的基于神经网络的解决方案在减少计算开销的同时，与现有的RIS元件分配方案相比，显著提高了系统吞吐量6.8%。此外，所提出的系统在降低计算复杂度的同时实现了更好的性能，使其比迭代优化算法具有显著更高的可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing demand for high data rates and seamless connectivity inwireless systems has sparked significant interest in reconfigurable intelligentsurfaces (RIS) and artificial intelligence-based wireless applications. RIStypically comprises passive reflective antenna elements that control thewireless propagation environment by adequately tuning the phase of thereflective elements. The allocation of RIS elements to multipleuser equipment(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate ajoint optimization problem that optimizes the RIS phase configuration andresource allocation under an $\alpha$-fair scheduling framework and propose anefficient way of allocating RIS elements. Conventional iterative optimizationmethods, however, suffer from exponentially increasing computational complexityas the number of RIS elements increases and also complicate the generation oftraining labels for supervised learning. To overcome these challenges, wepropose a five-layer fully connected neural network (FNN) combined with apreprocessing technique to significantly reduce input dimensionality, lowercomputational complexity, and enhance scalability. The simulation results showthat our proposed NN-based solution reduces computational overhead whilesignificantly improving system throughput by 6.8% compared to existing RISelement allocation schemes. Furthermore, the proposed system achieves betterperformance while reducing computational complexity, making it significantlymore scalable than the iterative optimization algorithms.</description>
      <author>example@mail.com (Pujitha Mamillapalli, Yoghitha Ramamoorthi, Abhinav Kumar, Tomoki Murakami, Tomoaki Ogawa, Yasushi Takatori)</author>
      <guid isPermaLink="false">2509.03241v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Isolated Bangla Handwritten Character Classification using Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.03061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Comments: 13 pages, 14 figures, published in the Proceedings of the  2nd International Conference on Computing Advancements (ICCA 2022), IEEE.  Strong experimental section with comparisons across models (3DCNN, ResNet50,  MobileNet)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用迁移学习的方法来分类孟加拉语手写字符，包括基本字符、独特字符和复合字符，解决了梯度消失问题。&lt;h4&gt;背景&lt;/h4&gt;孟加拉语由50个不同字符和许多复合字符组成。已有一些针对手写和光学孟加拉语字符识别的研究。&lt;h4&gt;目的&lt;/h4&gt;使用迁移学习分类基础、独特和复合的孟加拉语手写字符，同时避免梯度消失问题。&lt;h4&gt;方法&lt;/h4&gt;应用深度神经网络技术，包括3D卷积神经网络(3DCNN)、残差神经网络(ResNet)和MobileNet，实现孟加拉语手写字符所有可能标准形式的端到端分类。&lt;h4&gt;主要发现&lt;/h4&gt;模型在训练数据上达到99.82%的准确率，在测试数据上达到99.46%的准确率。&lt;h4&gt;结论&lt;/h4&gt;与各种孟加拉语手写字符分类的最先进基准相比，所提出的模型在分类数据方面取得了更好的准确性。&lt;h4&gt;翻译&lt;/h4&gt;孟加拉语由50个不同字符和许多复合字符组成。已经进行了多项研究来识别手写和光学孟加拉语字符。我们的方法使用迁移学习来分类基础、独特以及复合的孟加拉语手写字符，同时避免梯度消失问题。应用了深度神经网络技术，如3D卷积神经网络(3DCNN)、残差神经网络(ResNet)和MobileNet，生成孟加拉语手写字符所有可能标准形式的端到端分类。使用包含166,105个分为84个不同类别的孟加拉语字符图像样本的孟加拉语Lekha孤立数据集进行此分类模型。该模型在训练数据上达到99.82%的准确率，在测试数据上达到99.46%的准确率。与各种孟加拉语手写字符分类的最先进基准相比，所提出的模型在分类数据方面取得了更好的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bangla language consists of fifty distinct characters and many compoundcharacters. Several notable studies have been performed to recognize Banglacharacters, both handwritten and optical. Our approach uses transfer learningto classify the basic, distinct, as well as compound Bangla handwrittencharacters while avoiding the vanishing gradient problem. Deep Neural Networktechniques such as 3D Convolutional Neural Network (3DCNN), Residual NeuralNetwork (ResNet), and MobileNet are applied to generate an end-to-endclassification of all possible standard formations of handwritten characters inthe Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105Bangla character image samples categorized into 84 distinct classes, is usedfor this classification model. The model achieved 99.82% accuracy on trainingdata and 99.46% accuracy on test data. Comparisons with variousstate-of-the-art benchmarks of Bangla handwritten character classification showthat the proposed model achieves better accuracy in classifying the data.</description>
      <author>example@mail.com (Abdul Karim, S M Rafiuddin, Jahidul Islam Razin, Tahira Alam)</author>
      <guid isPermaLink="false">2509.03061v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal learning of melt pool dynamics in laser powder bed fusion</title>
      <link>http://arxiv.org/abs/2509.03029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 6 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态数据融合方法，结合高保真X射线数据和低保真吸收率数据来预测增材制造中的熔池动力学，通过迁移学习技术实现了仅使用低成本吸收率数据就能准确监测熔池特性的目标。&lt;h4&gt;背景&lt;/h4&gt;在增材制造中，虽然使用多种传感器进行实时监测，但并非所有传感器都能提供实用可靠的过程信息。高速X射线成像提供宝贵空间信息但成本高昂，不适合大多数工业环境；低成本光电二极管的吸收率数据与熔池动力学相关但单独使用时噪声过大，难以准确预测。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态数据融合方法，通过结合高保真X射线数据和低保真吸收率数据，实现激光粉末床熔融(LPBF)过程中熔池动力学的准确预测，并最终实现仅使用低成本吸收率数据的监测。&lt;h4&gt;方法&lt;/h4&gt;构建多模态学习框架，整合卷积神经网络(CNNs)从X射线数据中提取空间特征，以及循环神经网络(RNNs)从吸收率信号中提取时间特征，采用早期融合策略。将多模态模型作为迁移学习模型，微调仅使用吸收率数据的RNN模型以提高预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;与单独使用任何一种模态相比，同时使用两种模态进行训练显著提高了预测准确性。训练完成后，模型仅使用吸收率数据就可以推断熔池特性，无需昂贵的X射线成像设备。&lt;h4&gt;结论&lt;/h4&gt;这种多模态融合方法能够实现经济有效的实时监测，在增材制造领域具有广泛适用性，解决了高精度监测与成本效益之间的矛盾。&lt;h4&gt;翻译&lt;/h4&gt;虽然多个传感器用于增材制造的实时监测，但并非所有传感器都能提供实用或可靠的过程洞察。例如，高速X射线成像提供了关于亚表面熔池行为的宝贵空间信息，但成本高昂且对大多数工业环境不切实际。相比之下，低成本光电二极管的吸收率数据与熔池动力学相关，但单独使用时通常过于嘈杂而无法准确预测。在本文中，我们提出了一种多模态数据融合方法，通过结合激光粉末床熔融(LPBF)过程中的高保真X射线数据和低保真吸收率数据来预测熔池动力学。我们的多模态学习框架整合了卷积神经网络(CNNs)用于从X射线数据中提取空间特征，以及循环神经网络(RNNs)用于从吸收率信号中提取时间特征，采用早期融合策略。多模态模型进一步用作迁移学习模型，对仅使用吸收率数据就能预测熔池动力学的RNN模型进行微调，相比多模态模型具有更高的准确性。结果表明，与单独使用任何一种模态相比，使用两种模态进行训练显著提高了预测准确性。此外，一旦训练完成，该模型仅使用吸收率数据就可以推断熔池特性，消除了对昂贵X射线成像的需求。这种多模态融合方法能够实现经济有效的实时监测，并在增材制造领域具有广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While multiple sensors are used for real-time monitoring in additivemanufacturing, not all provide practical or reliable process insights. Forexample, high-speed X-ray imaging offers valuable spatial information aboutsubsurface melt pool behavior but is costly and impractical for most industrialsettings. In contrast, absorptivity data from low-cost photodiodes correlatewith melt pool dynamics but is often too noisy for accurate prediction whenused alone. In this paper, we propose a multimodal data fusion approach forpredicting melt pool dynamics by combining high-fidelity X-ray data withlow-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.Our multimodal learning framework integrates convolutional neural networks(CNNs) for spatial feature extraction from X-ray data with recurrent neuralnetworks (RNNs) for temporal feature extraction from absorptivity signals,using an early fusion strategy. The multimodal model is further used as atransfer learning model to fine-tune the RNN model that can predict melt pooldynamics only with absorptivity, with greater accuracy compared to themultimodal model. Results show that training with both modalities significantlyimproves prediction accuracy compared to using either modality alone.Furthermore, once trained, the model can infer melt pool characteristics usingonly absorptivity data, eliminating the need for expensive X-ray imaging. Thismultimodal fusion approach enables cost-effective, real-time monitoring and hasbroad applicability in additive manufacturing.</description>
      <author>example@mail.com (Satyajit Mojumder, Pallock Halder, Tiana Tonge)</author>
      <guid isPermaLink="false">2509.03029v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>RiverScope: High-Resolution River Masking Dataset</title>
      <link>http://arxiv.org/abs/2509.02451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RiverScope，一个高分辨率河流和地表水数据集，由计算机科学和水文学专家合作开发，包含1,145张高分辨率图像和专家标注的掩膜数据，与多种卫星数据配准，为精细尺度水文监测提供资源。&lt;h4&gt;背景&lt;/h4&gt;地表水动态在地球气候系统中扮演关键角色，影响生态系统、农业、灾害恢复力和可持续发展。然而，在精细时空尺度上监测河流和地表水具有挑战性，特别是对于窄河或富含沉积物的河流，低分辨率卫星数据难以捕捉。&lt;h4&gt;目的&lt;/h4&gt;开发一个高分辨率数据集来解决精细尺度河流和地表水监测的挑战，并支持多传感器水文监测，同时评估不同传感器间的成本-准确性权衡。&lt;h4&gt;方法&lt;/h4&gt;创建RiverScope数据集，包含1,145张高分辨率图像（覆盖2,577平方公里）和专家标记的河流和地表水掩膜（需100多小时手工标注）；将每张图像与Sentinel-2、SWOT和SWOT河流数据库配准；建立全球首个高分辨率河流宽度估计基准；评估多种深度网络架构、预训练策略和训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;建立了全球首个高分辨率河流宽度估计基准，中位误差为7.2米，显著优于现有卫星衍生方法；最佳性能模型结合了迁移学习的优势，并通过学习适配器使用所有多光谱PlanetScope通道。&lt;h4&gt;结论&lt;/h4&gt;RiverScope为精细尺度和多传感器水文建模提供了宝贵资源，支持气候适应和可持续水资源管理。&lt;h4&gt;翻译&lt;/h4&gt;地表水动态在地球气候系统中起着关键作用，影响着生态系统、农业、灾害恢复力和可持续发展。然而，在精细的时空尺度上监测河流和地表水仍然具有挑战性——特别是对于低分辨率卫星数据难以捕捉的窄河或富含沉积物的河流。为此，我们引入了RiverScope，这是一个由计算机科学和水文学专家合作开发的高分辨率数据集。RiverScope包含1,145张高分辨率图像（覆盖2,577平方公里），配有专家标记的河流和地表水掩膜，需要超过100小时的手工标注。每张图像都与Sentinel-2、SWOT和SWOT河流数据库（SWORD）配准，能够评估不同传感器之间的成本-准确性权衡——这是业务水资源监测的一个关键考虑因素。我们还建立了全球首个高分辨率河流宽度估计基准，实现了7.2米的中位误差——显著优于现有的卫星衍生方法。我们在多种架构（如CNN和transformers）、预训练策略（如监督和自监督）以及训练数据集（如ImageNet和卫星图像）上广泛评估了深度网络。我们表现最佳的模型结合了迁移学习的优势，并通过学习适配器使用了所有多光谱PlanetScope通道。RiverScope为精细尺度和多传感器水文建模提供了宝贵资源，支持气候适应和可持续水资源管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surface water dynamics play a critical role in Earth's climate system,influencing ecosystems, agriculture, disaster resilience, and sustainabledevelopment. Yet monitoring rivers and surface water at fine spatial andtemporal scales remains challenging -- especially for narrow or sediment-richrivers that are poorly captured by low-resolution satellite data. To addressthis, we introduce RiverScope, a high-resolution dataset developed throughcollaboration between computer science and hydrology experts. RiverScopecomprises 1,145 high-resolution images (covering 2,577 square kilometers) withexpert-labeled river and surface water masks, requiring over 100 hours ofmanual annotation. Each image is co-registered with Sentinel-2, SWOT, and theSWOT River Database (SWORD), enabling the evaluation of cost-accuracytrade-offs across sensors -- a key consideration for operational watermonitoring. We also establish the first global, high-resolution benchmark forriver width estimation, achieving a median error of 7.2 meters -- significantlyoutperforming existing satellite-derived methods. We extensively evaluate deepnetworks across multiple architectures (e.g., CNNs and transformers),pretraining strategies (e.g., supervised and self-supervised), and trainingdatasets (e.g., ImageNet and satellite imagery). Our best-performing modelscombine the benefits of transfer learning with the use of all the multispectralPlanetScope channels via learned adaptors. RiverScope provides a valuableresource for fine-scale and multi-sensor hydrological modeling, supportingclimate adaptation and sustainable water management.</description>
      <author>example@mail.com (Rangel Daroya, Taylor Rowley, Jonathan Flores, Elisa Friedmann, Fiona Bennitt, Heejin An, Travis Simmons, Marissa Jean Hughes, Camryn L Kluetmeier, Solomon Kica, J. Daniel Vélez, Sarah E. Esenther, Thomas E. Howard, Yanqi Ye, Audrey Turcotte, Colin Gleason, Subhransu Maji)</author>
      <guid isPermaLink="false">2509.02451v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Speech transformer models for extracting information from baby cries</title>
      <link>http://arxiv.org/abs/2509.02259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to WOCCI2025 (interspeech2025 workshop)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了预训练语音模型的潜在表示对非语音数据的适用性及其编码的声学特性&lt;h4&gt;背景&lt;/h4&gt;使用预训练语音模型的潜在表示进行迁移学习在标记数据稀少的任务中表现出色，但这些表示对非语音数据的适用性以及编码的具体声学特性很大程度上仍未探索&lt;h4&gt;目的&lt;/h4&gt;研究预训练语音模型的潜在表示对非语音数据的适用性及其编码的具体声学特性&lt;h4&gt;方法&lt;/h4&gt;评估五个预训练语音模型在八个婴儿哭声数据集上的表现，这些数据集包含来自960个婴儿的115小时音频。对于每个数据集，评估每个模型在所有可用分类任务中的潜在表示&lt;h4&gt;主要发现&lt;/h4&gt;这些模型的潜在表示可以有效分类人类婴儿哭声，并编码与发声源不稳定性和哭泣婴儿身份相关的关键信息&lt;h4&gt;结论&lt;/h4&gt;比较这些模型的架构和训练策略为未来针对类似任务（如情绪检测）的定制模型设计提供了有价值的见解&lt;h4&gt;翻译&lt;/h4&gt;使用预训练语音模型的潜在表示进行迁移学习在标记数据稀少的任务中取得了卓越的性能。然而，这些表示对非语音数据的适用性以及编码在这些表示中的特定声学特性很大程度上仍未被探索。在本研究中，我们调查了这两个方面。我们在八个婴儿哭声数据集上评估了五个预训练语音模型，这些数据集包含来自960个婴儿的115小时音频。对于每个数据集，我们评估了每个模型在所有可用分类任务中的潜在表示。我们的结果表明，这些模型的潜在表示可以有效分类人类婴儿哭声，并编码与发声源不稳定性和哭泣婴儿身份相关的关键信息。此外，比较这些模型的架构和训练策略为未来针对类似任务（如情绪检测）的定制模型设计提供了有价值的见解&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning using latent representations from pre-trained speech modelsachieves outstanding performance in tasks where labeled data is scarce.However, their applicability to non-speech data and the specific acousticproperties encoded in these representations remain largely unexplored. In thisstudy, we investigate both aspects. We evaluate five pre-trained speech modelson eight baby cries datasets, encompassing 115 hours of audio from 960 babies.For each dataset, we assess the latent representations of each model across allavailable classification tasks. Our results demonstrate that the latentrepresentations of these models can effectively classify human baby cries andencode key information related to vocal source instability and identity of thecrying baby. In addition, a comparison of the architectures and trainingstrategies of these models offers valuable insights for the design of futuremodels tailored to similar tasks, such as emotion detection.</description>
      <author>example@mail.com (Guillem Bonafos, Jéremy Rouch, Lény Lego, David Reby, Hugues Patural, Nicolas Mathevon, Rémy Emonet)</author>
      <guid isPermaLink="false">2509.02259v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision</title>
      <link>http://arxiv.org/abs/2509.01882v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is under peer review for IEEE Journal of Oceanic  Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了HydroVision，一个基于深度学习的场景分类框架，能够从标准RGB图像中估算多种水质参数，为环境监测提供了一种经济有效的替代方案。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉技术的进步，特别是在模式识别和场景分类方面，为环境监测开辟了新的应用可能性。深度学习现在提供了非接触式的水质评估和污染检测方法，这对灾害响应和公共卫生保护至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从RGB图像中估算光学活性水质参数的框架，支持污染趋势的早期检测，并加强监管机构在环境压力、工业活动和不可抗力事件期间的监测能力。&lt;h4&gt;方法&lt;/h4&gt;创建HydroVision深度学习框架，使用50多万张从美国地质调查局收集的季节变化图像进行训练，评估五种架构（VGG-16、ResNet50、MobileNetV2、DenseNet121和Vision Transformer），通过迁移学习确定最佳性能架构。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet121在验证性能上表现最佳，在预测有色溶解有机物(CDOM)时R²得分为0.89，证明了该框架在不同条件下进行实际水质监测的潜力。&lt;h4&gt;结论&lt;/h4&gt;HydroVision利用广泛可用的RGB图像作为传统多光谱和高光谱遥感的可扩展、经济有效的替代方案。未来工作将提高模型在低光和受阻场景下的鲁棒性，以扩大其实用性。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉技术的持续进步，特别是在模式识别和场景分类方面，为环境监测开辟了新的应用。深度学习现在为评估水质和检测污染提供了非接触式方法，这两者对于灾害响应和公共卫生保护都至关重要。这项工作介绍了HydroVision，一个基于深度学习的场景分类框架，可以从标准红绿蓝(RGB)图像中估算光学活性水质参数，包括叶绿素-Alpha、叶绿素、有色溶解有机物(CDOM)、藻蓝蛋白、悬浮沉积物和浊度。HydroVision支持污染趋势的早期检测，并在外部环境压力、工业活动和不可抗力事件期间，加强监管机构的监测能力。该模型是在2022年至2024年间从美国地质调查局水文图像可视化和信息系统收集的50多万张季节变化图像上训练的。这种方法利用广泛可用的RGB图像作为传统多光谱和高光谱遥感的可扩展、经济有效的替代方案。通过迁移学习评估了四种最先进的卷积神经网络(VGG-16、ResNet50、MobileNetV2、DenseNet121)和一个视觉变换器，以确定性能最佳的架构。DenseNet121在验证性能上表现最佳，在预测CDOM时R²得分为0.89，证明了该框架在不同条件下进行实际水质监测的潜力。虽然当前模型针对光照良好的图像进行了优化，但未来的工作将侧重于提高在低光和受阻场景下的鲁棒性，以扩大其操作实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ongoing advancements in computer vision, particularly in pattern recognitionand scene classification, have enabled new applications in environmentalmonitoring. Deep learning now offers non-contact methods for assessing waterquality and detecting contamination, both critical for disaster response andpublic health protection. This work introduces HydroVision, a deeplearning-based scene classification framework that estimates optically activewater quality parameters including Chlorophyll-Alpha, Chlorophylls, ColoredDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, andTurbidity from standard Red-Green-Blue (RGB) images of surface water.HydroVision supports early detection of contamination trends and strengthensmonitoring by regulatory agencies during external environmental stressors,industrial activities, and force majeure events. The model is trained on morethan 500,000 seasonally varied images collected from the United StatesGeological Survey Hydrologic Imagery Visualization and Information Systembetween 2022 and 2024. This approach leverages widely available RGB imagery asa scalable, cost-effective alternative to traditional multispectral andhyperspectral remote sensing. Four state-of-the-art convolutional neuralnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformerare evaluated through transfer learning to identify the best-performingarchitecture. DenseNet121 achieves the highest validation performance, with anR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise forreal-world water quality monitoring across diverse conditions. While thecurrent model is optimized for well-lit imagery, future work will focus onimproving robustness under low-light and obstructed scenarios to expand itsoperational utility.</description>
      <author>example@mail.com (Shubham Laxmikant Deshmukh, Matthew Wilchek, Feras A. Batarseh)</author>
      <guid isPermaLink="false">2509.01882v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing</title>
      <link>http://arxiv.org/abs/2509.01754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TransMatch框架，结合迁移学习和半监督少样本学习，解决增材制造中标记数据稀缺的表面缺陷检测问题&lt;h4&gt;背景&lt;/h4&gt;激光粉末床熔融(LPBF)中的表面缺陷会对增材制造组件的结构完整性构成重大风险&lt;h4&gt;目的&lt;/h4&gt;引入TransMatch框架，解决增材制造缺陷标记数据稀缺的问题&lt;h4&gt;方法&lt;/h4&gt;TransMatch结合迁移学习和半监督少样本学习，有效利用标记和新类别图像，克服了先前元学习方法的限制&lt;h4&gt;主要发现&lt;/h4&gt;在包含8,284张图像的表面缺陷数据集上，TransMatch实现了98.91%的准确率，最小损失，同时针对多种缺陷类别获得了高精度、召回率和F1分数&lt;h4&gt;结论&lt;/h4&gt;TransMatch在准确识别裂纹、针孔、孔洞和飞溅等多种缺陷方面表现出强大的鲁棒性，代表了增材制造缺陷检测的重要进步，为广泛工业应用中的质量保证和可靠性提供了实用且可扩展的解决方案&lt;h4&gt;翻译&lt;/h4&gt;激光粉末床熔融(LPBF)中的表面缺陷会对增材制造组件的结构完整性构成重大风险。本文引入TransMatch，一种新颖的框架，它结合迁移学习和半监督少样本学习来解决增材制造缺陷标记数据稀缺的问题。通过有效利用标记和新类别图像，TransMatch克服了先前元学习方法的局限性。在包含8,284张图像的表面缺陷数据集上的实验评估证明了TransMatch的有效性，实现了98.91%的准确率和最小损失，同时针对多种缺陷类别获得了高精度、召回率和F1分数。这些发现强调了其在准确识别裂纹、针孔、孔洞和飞溅等多样缺陷方面的鲁棒性。因此，TransMatch代表了增材制造缺陷检测的重要进步，为广泛工业应用中的质量保证和可靠性提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks tothe structural integrity of additively manufactured components. This paperintroduces TransMatch, a novel framework that merges transfer learning andsemi-supervised few-shot learning to address the scarcity of labeled AM defectdata. By effectively leveraging both labeled and unlabeled novel-class images,TransMatch circumvents the limitations of previous meta-learning approaches.Experimental evaluations on a Surface Defects dataset of 8,284 imagesdemonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimalloss, alongside high precision, recall, and F1-scores for multiple defectclasses. These findings underscore its robustness in accurately identifyingdiverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thusrepresents a significant leap forward in additive manufacturing defectdetection, offering a practical and scalable solution for quality assurance andreliability across a wide range of industrial applications.</description>
      <author>example@mail.com (Mohsen Asghari Ilani, Yaser Mike Banad)</author>
      <guid isPermaLink="false">2509.01754v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets</title>
      <link>http://arxiv.org/abs/2509.01566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种冷启动相关性匹配(CSRM)框架，利用多语言大型语言模型解决电子商务平台在新兴市场面临的冷启动挑战，通过激活跨语言迁移学习能力、基于检索的查询增强和多轮自蒸馏训练策略，实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;全球电子商务平台不断扩张，公司进入新市场时面临冷启动挑战，原因是有限的标签和用户行为数据。&lt;h4&gt;目的&lt;/h4&gt;分享Coupang公司的经验，为新兴电子商务市场提供具有竞争力的冷启动相关性匹配性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个冷启动相关性匹配(CSRM)框架，利用多语言大型语言模型(LLM)解决三个挑战：(1)通过机器翻译任务激活LLM的跨语言迁移学习能力；(2)通过基于检索的查询增强来提高查询理解并整合电子商务知识；(3)通过多轮自蒸馏训练策略减轻训练标签错误的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明CSRM-LLM和所提出技术的有效性，成功实现实际部署并获得显著的在线收益，缺陷比率降低了45.8%，会话购买率提升了0.866%。&lt;h4&gt;结论&lt;/h4&gt;CSRM框架能够有效解决电子商务平台在新兴市场面临的冷启动挑战。&lt;h4&gt;翻译&lt;/h4&gt;随着全球电子商务平台的持续扩张，公司正在进入新市场，由于有限的标签和用户行为而面临冷启动挑战。在本文中，我们分享了在Coupang的经验，为新兴电子商务市场提供具有竞争力的冷启动相关性匹配性能。具体来说，我们提出了一个冷启动相关性匹配(CSRM)框架，利用多语言大型语言模型(LLM)解决三个挑战：(1)通过机器翻译任务激活LLM的跨语言迁移学习能力；(2)通过基于检索的查询增强提高查询理解并整合电子商务知识；(3)通过多轮自蒸馏训练策略减轻训练标签错误的影响。我们的实验证明了CSRM-LLM和所提出技术的有效性，成功实现了实际部署并获得了显著的在线收益，缺陷比率降低了45.8%，会话购买率提升了0.866%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761545&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As global e-commerce platforms continue to expand, companies are entering newmarkets where they encounter cold-start challenges due to limited human labelsand user behaviors. In this paper, we share our experiences in Coupang toprovide a competitive cold-start performance of relevance matching for emerginge-commerce markets. Specifically, we present a Cold-Start Relevance Matching(CSRM) framework, utilizing a multilingual Large Language Model (LLM) toaddress three challenges: (1) activating cross-lingual transfer learningabilities of LLMs through machine translation tasks; (2) enhancing queryunderstanding and incorporating e-commerce knowledge by retrieval-based queryaugmentation; (3) mitigating the impact of training label errors through amulti-round self-distillation training strategy. Our experiments demonstratethe effectiveness of CSRM-LLM and the proposed techniques, resulting insuccessful real-world deployment and significant online gains, with a 45.8%reduction in defect ratio and a 0.866% uplift in session purchase rate.</description>
      <author>example@mail.com (Yujing Wang, Yiren Chen, Huoran Li, Chunxu Xu, Yuchong Luo, Xianghui Mao, Cong Li, Lun Du, Chunyang Ma, Qiqi Jiang, Yin Wang, Fan Gao, Wenting Mo, Pei Wen, Shantanu Kumar, Taejin Park, Yiwei Song, Vijay Rajaram, Tao Cheng, Sonu Durgia, Pranam Kolari)</author>
      <guid isPermaLink="false">2509.01566v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Characterization of Speech Similarity Between Australian Aboriginal and High-Resource Languages: A Case Study on Dharawal</title>
      <link>http://arxiv.org/abs/2509.01419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at APSIPA ASC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对澳大利亚原住民语言在语音AI系统中代表性不足的问题，通过收集整理Dharawal语言的语音数据集，并使用预训练多语言语音编码器分析其与107种高资源语言的语音相似性。研究发现Dharawal与多种语言有很强的语音相似性，为迁移学习和模型适应提供了指导。&lt;h4&gt;背景&lt;/h4&gt;澳大利亚原住民语言具有重要的文化和语言学价值，但在现代语音AI系统中严重缺乏代表性。最先进的语音基础模型和自动语音识别系统在高资源语言环境中表现出色，但在低资源语言（特别是缺乏干净、标注语音数据的语言）上泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;收集并整理Dharawal（一种低资源澳大利亚原住民语言）的语音数据集，并分析Dharawal与107种高资源语言之间的语音相似性。&lt;h4&gt;方法&lt;/h4&gt;仔细收集和处理公开可用的录音构建Dharawal语音数据集，使用预训练的多语言语音编码器，结合错误分类率分析评估语言混淆性，以及使用余弦相似度和Fréchet初始距离在嵌入空间中进行细粒度相似度测量。&lt;h4&gt;主要发现&lt;/h4&gt;Dharawal与拉丁语、毛利语、韩语、泰语和威尔士语等语言有很强的语音相似性。&lt;h4&gt;结论&lt;/h4&gt;这些发现为未来的迁移学习和模型适应工作提供了实用指导，强调了数据收集和基于嵌入的分析在支持濒危语言社区语音技术方面的重要性。&lt;h4&gt;翻译&lt;/h4&gt;澳大利亚原住民语言具有重要的文化和语言学价值，但在现代语音AI系统中仍然严重缺乏代表性。虽然最先进的语音基础模型和自动语音识别在高资源环境中表现出色，但它们往往难以泛化到低资源语言，特别是那些缺乏干净、标注语音数据的语言。在这项工作中，我们通过仔细收集和处理公开可用的录音，为Dharawal（一种低资源澳大利亚原住民语言）收集并整理了一个语音数据集。使用这个数据集，我们使用预训练的多语言语音编码器分析了Dharawal和107种高资源语言之间的语音相似性。我们的方法结合了（1）错误分类率分析以评估语言混淆性，以及（2）在嵌入空间中使用余弦相似度和Fréchet初始距离进行细粒度相似度测量。实验结果显示，Dharawal与拉丁语、毛利语、韩语、泰语和威尔士语等语言有很强的语音相似性。这些发现为未来的迁移学习和模型适应工作提供了实用指导，并强调了数据收集和基于嵌入的分析在支持濒危语言社区语音技术方面的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Australian Aboriginal languages are of significant cultural and linguisticvalue but remain severely underrepresented in modern speech AI systems. Whilestate-of-the-art speech foundation models and automatic speech recognitionexcel in high-resource settings, they often struggle to generalize tolow-resource languages, especially those lacking clean, annotated speech data.In this work, we collect and clean a speech dataset for Dharawal, alow-resource Australian Aboriginal language, by carefully sourcing andprocessing publicly available recordings. Using this dataset, we analyze thespeech similarity between Dharawal and 107 high-resource languages using apre-trained multilingual speech encoder. Our approach combines (1)misclassification rate analysis to assess language confusability, and (2)fine-grained similarity measurements using cosine similarity and Fr\'echetInception Distance (FID) in the embedding space. Experimental results revealthat Dharawal shares strong speech similarity with languages such as Latin,M\=aori, Korean, Thai, and Welsh. These findings offer practical guidance forfuture transfer learning and model adaptation efforts, and underscore theimportance of data collection and embedding-based analysis in supporting speechtechnologies for endangered language communities.</description>
      <author>example@mail.com (Ting Dang, Trini Manoj Jeyaseelan, Eliathamby Ambikairajah, Vidhyasaharan Sethu)</author>
      <guid isPermaLink="false">2509.01419v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Comparison between Supervised and Unsupervised Learning in Deep Unfolded Sparse Signal Recovery</title>
      <link>http://arxiv.org/abs/2509.01331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work will be submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了深度展开技术中损失函数选择对稀疏信号恢复算法的影响，发现损失函数的效果取决于优化问题的凸性。&lt;h4&gt;背景&lt;/h4&gt;深度展开技术通过将迭代优化算法的迭代展开为网络层，将其转换为可训练的轻量级神经网络，并根据应用场景使用不同的损失函数进行参数学习。&lt;h4&gt;目的&lt;/h4&gt;研究在深度展开的ISTA和IHT算法中，不同损失函数选择对算法性能的影响，为设计有效的深度展开网络提供指导。&lt;h4&gt;方法&lt;/h4&gt;比较了基本迭代软阈值算法(ISTA)和迭代硬阈值算法(IHT)的深度展开版本，分别使用均方误差的监督学习和使用原始优化问题目标函数的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;对于凸的ℓ₁正则化问题，监督式ISTA恢复精度更高但无法最小化原始目标函数，无监督式ISTA收敛速度更快；对于非凸的ℓ₀正则化问题，监督式和无监督式IHT都收敛到更好的局部最小值，性能相似。&lt;h4&gt;结论&lt;/h4&gt;损失函数的选择效果显著取决于优化问题的凸性，这一发现为稀疏信号恢复应用中设计有效的深度展开网络提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了深度展开技术中损失函数选择对稀疏信号恢复算法的影响。深度展开通过将迭代优化算法的迭代展开为网络层，将其转换为可训练的轻量级神经网络，并根据应用场景使用不同的损失函数进行参数学习。我们专注于基本迭代软阈值算法(ISTA)和迭代硬阈值算法(IHT)的深度展开版本，比较了使用均方误差的监督学习与使用原始优化问题目标函数的无监督学习。我们的模拟结果表明，损失函数的选择效果显著取决于优化问题的凸性。对于凸的ℓ₁正则化问题，监督式ISTA实现了更好的最终恢复精度，但未能最小化原始目标函数；而无监督式ISTA收敛到与常规ISTA几乎相同的解，但收敛速度更快。相反，对于非凸的ℓ₀正则化问题，监督式IHT和无监督式IHT都收敛到比原始IHT更好的局部最小值，无论使用哪种损失函数，性能都相似。这些发现为稀疏信号恢复应用中设计有效的深度展开网络提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the impact of loss function selection in deepunfolding techniques for sparse signal recovery algorithms. Deep unfoldingtransforms iterative optimization algorithms into trainable lightweight neuralnetworks by unfolding their iterations as network layers, with various lossfunctions employed for parameter learning depending on application contexts. Wefocus on deep unfolded versions of the fundamental iterative shrinkagethresholding algorithm (ISTA) and the iterative hard thresholding algorithm(IHT), comparing supervised learning using mean squared error with unsupervisedlearning using the objective function of the original optimization problem. Oursimulation results reveal that the effect of the choice of loss functionsignificantly depends on the convexity of the optimization problem. For convex$\ell_1$-regularized problems, supervised-ISTA achieves better final recoveryaccuracy but fails to minimize the original objective function, whereas weempirically observe that unsupervised-ISTA converges to a nearly identicalsolution as conventional ISTA but with accelerated convergence. Conversely, fornonconvex $\ell_0$-regularized problems, both supervised-IHT andunsupervised-IHT converge to better local minima than the original IHT, showingsimilar performance regardless of the loss function employed. These findingsprovide valuable insights into the design of effective deep unfolded networksfor sparse signal recovery applications.</description>
      <author>example@mail.com (Koshi Nagahisa, Ryo Hayakawa, Youji Iiguni)</author>
      <guid isPermaLink="false">2509.01331v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency</title>
      <link>http://arxiv.org/abs/2509.01204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DcMatch，一种新颖的无监督学习框架，用于非刚性多形状匹配。该方法利用形状图注意力网络捕获形状集合的底层流形结构，构建共享潜在空间，并通过双层次一致性实现更准确和连贯的形状对应关系映射。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉和图形学领域，建立多个3D形状之间的点对点对应关系是一个基本问题。现有方法通常从单个形状学习规范嵌入，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获整个形状集合底层结构的多形状匹配方法，实现更准确、一致和鲁棒的非刚性形状对应关系。&lt;h4&gt;方法&lt;/h4&gt;DcMatch采用形状图注意力网络捕获形状集合的流形结构，构建共享潜在空间，通过universe predictor实现形状到宇宙的对应关系，同时在空间和频谱域表示对应关系，并通过循环一致性损失强制对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在多个具有挑战性的基准测试上，DcMatch在不同多形状匹配场景中始终优于之前的最先进方法，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DcMatch通过创新的双层次一致性机制和形状图注意力网络，显著提高了多形状匹配的准确性和鲁棒性，为计算机视觉和图形学中的形状对应问题提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在计算机视觉和图形学中，建立多个3D形状之间的点对点对应关系是一个基本问题。在本文中，我们介绍了DcMatch，一种新颖的无监督学习框架，用于非刚性多形状匹配。与从单个形状学习规范嵌入的现有方法不同，我们的方法利用形状图注意力网络来捕获整个形状集合的底层流形结构。这使得能够构建更具表现力和鲁棒性的共享潜在空间，通过宇宙预测器实现更一致的形状到宇宙的对应关系。同时，我们在空间和频谱域表示这些对应关系，并通过新颖的循环一致性损失在共享宇宙空间中强制它们的对齐。这种双层次一致性促进了更准确和连贯的映射。在几个具有挑战性的基准测试上的广泛实验表明，我们的方法在不同多形状匹配场景中始终优于之前的最先进方法。代码可在https://github.com/YeTianwei/DcMatch获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是多形状匹配问题，即在多个3D形状之间建立准确且一致性的点对点对应关系。这个问题在计算机视觉和图形学中非常重要，因为它支持多种应用，如纹理转移、统计形状分析（医学成像中）和3D重建。随着3D扫描技术的发展，同时捕获多个形状变得越来越常见，但现有的成对形状匹配方法难以扩展到多形状场景，因为多形状匹配需要整个形状集合之间的循环一致性，这大大增加了问题的复杂性，同时随着形状集合大小的增加，计算开销也呈组合式增长。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了多形状匹配的挑战，特别是循环一致性问题。他们指出现有方法主要有两种范式：排列同步范式和基于宇宙的范式，但各有局限性。作者受到排列同步方法中成对匹配准确性的启发，同时希望改进基于宇宙的方法，使其能够捕获整个形状集合的底层流形结构。该方法借鉴了功能映射（Functional Maps）来表示形状对应关系，使用图注意力网络（GAT）捕获形状集合的结构关系，并采用宇宙预测器和循环一致性损失来确保匹配的一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; DcMatch的核心思想是通过双重级别的一致性（空间和频谱）和形状图注意力网络来捕获整个形状集合的底层流形结构，构建更丰富、鲁棒的共享宇宙空间。整体实现流程包括：1) 使用DiffusionNet提取每个形状的顶点特征；2) 功能映射模块计算形状对之间的双向功能映射并转换为点对点对应关系；3) 形状图注意力模块构建形状图并使用GAT提取流形感知特征；4) 宇宙预测器模块预测每个形状与共享宇宙之间的对应关系；5) 使用频谱损失和循环一致性损失进行训练，确保空间和频谱域对应关系的一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1) 双重级别的一致性约束，同时 enforcing 空间（点对点）和频谱（功能映射）两个级别的一致性；2) 形状图注意力模块，将整个形状集合建模为无向图并捕获底层流形结构；3) 增强的宇宙空间，通过形状图注意力网络构建更丰富、鲁棒的共享潜在空间；4) 完全无监督的学习框架。相比之前的工作，DcMatch不依赖于单个参考形状来学习宇宙嵌入，而是考虑整个形状集合的流形结构，同时利用功能映射和形状到宇宙匹配的双重一致性，提高了匹配的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DcMatch通过引入形状图注意力网络和双重级别的一致性约束，在无监督设置下实现了更准确、鲁棒和连贯的多形状匹配，同时捕获了整个形状集合的底层流形结构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing point-to-point correspondences across multiple 3D shapes is afundamental problem in computer vision and graphics. In this paper, weintroduce DcMatch, a novel unsupervised learning framework for non-rigidmulti-shape matching. Unlike existing methods that learn a canonical embeddingfrom a single shape, our approach leverages a shape graph attention network tocapture the underlying manifold structure of the entire shape collection. Thisenables the construction of a more expressive and robust shared latent space,leading to more consistent shape-to-universe correspondences via a universepredictor. Simultaneously, we represent these correspondences in both thespatial and spectral domains and enforce their alignment in the shared universespace through a novel cycle consistency loss. This dual-level consistencyfosters more accurate and coherent mappings. Extensive experiments on severalchallenging benchmarks demonstrate that our method consistently outperformsprevious state-of-the-art approaches across diverse multi-shape matchingscenarios. Code is available at https://github.com/YeTianwei/DcMatch.</description>
      <author>example@mail.com (Tianwei Ye, Yong Ma, Xiaoguang Mei)</author>
      <guid isPermaLink="false">2509.01204v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework</title>
      <link>http://arxiv.org/abs/2509.01198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种关系保持损失函数(RPL)，用于在降维过程中保留向量空间的关键属性，如正交性和线性独立性。&lt;h4&gt;背景&lt;/h4&gt;降维可能会扭曲向量空间属性如正交性和线性独立性，而这些属性对跨模态检索、聚类和分类等任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种损失函数来保留高维数据和低维嵌入之间的关系矩阵，从而维护关键的向量空间属性。&lt;h4&gt;方法&lt;/h4&gt;RPL通过最小化高维数据和其低维嵌入之间的关系矩阵（如Gram矩阵或余弦相似度）之间的差异来工作，用于训练神经网络的非线性投影，并由矩阵扰动理论推导出的误差边界支持。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，RPL可以在降低嵌入维度的同时，在很大程度上保持下游任务的性能，这可能是由于其保留了关键的向量空间属性。&lt;h4&gt;结论&lt;/h4&gt;RPL不仅可以用于降维，还可以更广泛地应用于跨领域对齐和迁移学习、知识蒸馏、公平性和不变性、去中心化、图和流形学习以及联邦学习等领域。&lt;h4&gt;翻译&lt;/h4&gt;降维可能会扭曲向量空间属性，如正交性和线性独立性，这些属性对于跨模态检索、聚类和分类等任务至关重要。我们提出了一种关系保持损失(RPL)，这是一种通过最小化高维数据及其低维嵌入之间的关系矩阵（如Gram矩阵或余弦矩阵）之间的差异来保留这些属性的损失函数。RPL训练神经网络的非线性投影，并由矩阵扰动理论推导出的误差边界支持。初步实验表明，RPL在降低嵌入维度的同时，很大程度上保留了下游任务的性能，这可能是由于其保留了关键的向量空间属性。虽然我们在本文中描述了RPL在降维中的应用，但这种损失也可以更广泛地应用，例如在跨领域对齐和迁移学习、知识蒸馏、公平性和不变性、去中心化、图和流形学习以及联邦学习中，其中分布式嵌入必须保持几何一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dimensionality reduction can distort vector space properties such asorthogonality and linear independence, which are critical for tasks includingcross-modal retrieval, clustering, and classification. We propose aRelationship Preserving Loss (RPL), a loss function that preserves theseproperties by minimizing discrepancies between relationship matrices (e.g.,Gram or cosine) of high-dimensional data and their low-dimensional embeddings.RPL trains neural networks for non-linear projections and is supported by errorbounds derived from matrix perturbation theory. Initial experiments suggestthat RPL reduces embedding dimensions while largely retaining performance ondownstream tasks, likely due to its preservation of key vector spaceproperties. While we describe here the use of RPL in dimensionality reduction,this loss can also be applied more broadly, for example to cross-domainalignment and transfer learning, knowledge distillation, fairness andinvariance, dehubbing, graph and manifold learning, and federated learning,where distributed embeddings must remain geometrically consistent.</description>
      <author>example@mail.com (Eddi Weinwurm, Alexander Kovalenko)</author>
      <guid isPermaLink="false">2509.01198v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</title>
      <link>http://arxiv.org/abs/2509.01135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于脑电图信号的情感识别方法，通过多域聚合迁移学习框架(MATL-DC)解决了传统迁移学习模型依赖源域和目标域数据的问题，实现了在没有目标域数据参与训练的情况下进行情感识别。&lt;h4&gt;背景&lt;/h4&gt;基于脑电图(EEG)的情感识别是情感脑机接口(aBCIs)的关键研究热点，但当前迁移学习模型严重依赖源域和目标域数据，限制了情感识别的实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个针对EEG情感识别的多域聚合迁移学习框架，带有未见目标下的领域-类别原型(MATL-DC)，以减少对目标域数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;设计特征解耦模块解耦类别不变的领域特征和领域不变的类别特征；通过多域聚合机制将领域特征空间聚合形成超域增强情感EEG信号特征；提取类别原型表示；采用成对学习策略将分类问题转化为样本对相似性问题；训练过程中目标域完全不可见；推理阶段使用训练好的领域-类别原型进行情感识别。&lt;h4&gt;主要发现&lt;/h4&gt;在SEED、SEED-IV和SEED-V公开数据库上验证，MATL-DC模型准确率分别达到84.70%、68.11%和61.08%，性能与依赖源域和目标域的方法相当或更好。&lt;h4&gt;结论&lt;/h4&gt;MATL-DC模型在EEG情感识别任务中表现良好，不需要目标域数据参与训练，具有实际应用价值。源代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;基于脑电图(EEG)信号的情感识别正逐渐成为情感脑机接口(aBCIs)的关键研究热点。然而，当前的迁移学习模型严重依赖源域和目标域数据，这阻碍了情感识别的实际应用。因此，我们提出了一种针对EEG情感识别的多域聚合迁移学习框架，带有未见目标下的领域-类别原型(MATL-DC)。我们设计了特征解耦模块，从浅层特征中解耦出类别不变的领域特征和领域不变的类别特征。在模型训练阶段，多域聚合机制将领域特征空间聚合形成超域，增强情感EEG信号的特征。在每个超域中，我们通过类别特征进一步提取类别原型表示。此外，我们采用成对学习策略将样本分类问题转化为样本对之间的相似性问题，有效缓解了标签噪声的影响。值得注意的是，在训练过程中目标域完全不可见。在推理阶段，我们使用训练好的领域-类别原型进行推理，实现情感识别。我们在公开数据库(SEED、SEED-IV和SEED-V)上对其进行了严格验证。结果表明，MATL-DC模型的准确率分别为84.70%、68.11%和61.08%。MATL-DC实现了与依赖源域和目标域的方法相当甚至更好的性能。源代码可在https://github.com/WuCB-BCI/MATL-DC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotion recognition based on electroencephalography (EEG) signals isincreasingly becoming a key research hotspot in affective Brain-ComputerInterfaces (aBCIs). However, the current transfer learning model greatlydepends on the source domain and target domain data, which hinder the practicalapplication of emotion recognition. Therefore, we propose a Multi-domainAggregation Transfer Learning framework for EEG emotion recognition withDomain-Class prototype under unseen targets (MATL-DC). We design the featuredecoupling module to decouple class-invariant domain features fromdomain-invariant class features from shallow features. In the model trainingstage, the multi-domain aggregation mechanism aggregates the domain featurespace to form a superdomain, which enhances the characteristics of emotionalEEG signals. In each superdomain, we further extract the class prototyperepresentation by class features. In addition, we adopt the pairwise learningstrategy to transform the sample classification problem into the similarityproblem between sample pairs, which effectively alleviates the influence oflabel noise. It is worth noting that the target domain is completely unseenduring the training process. In the inference stage, we use the traineddomain-class prototypes for inference, and then realize emotion recognition. Werigorously validate it on the publicly available databases (SEED, SEED-IV andSEED-V). The results show that the accuracy of MATL-DC model is 84.70\%,68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even betterperformance than methods that rely on both source and target domains. Thesource code is available at https://github.com/WuCB-BCI/MATL-DC.</description>
      <author>example@mail.com (Guangli Li, Canbiao Wu, Zhehao Zhou, Na Tian, Zhen Liang)</author>
      <guid isPermaLink="false">2509.01135v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery</title>
      <link>http://arxiv.org/abs/2509.00226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Our publicly available fine-tuned models provide a scalable transfer  learning solution for gravitational lens finding in LSST. Submitted to MNRAS.  Comments welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了GraViT，一个基于PyTorch的引力透镜检测流水线，利用了Vision Transformer和MLP-Mixer的预训练模型。研究评估了迁移学习对分类性能的影响，包括数据质量、模型架构、训练策略和集成预测等方面。&lt;h4&gt;背景&lt;/h4&gt;引力透镜是研究暗物质性质和推断宇宙学参数的有力工具。LSST（空间和时间遗产调查）预计在未来十年内会发现约10^5个引力透镜，需要开发自动分类器。&lt;h4&gt;目的&lt;/h4&gt;介绍GraViT流水线，评估迁移学习对引力透镜检测分类性能的影响，并提供对强引力透镜可检测性的见解。&lt;h4&gt;方法&lt;/h4&gt;通过研究数据质量（源和样本大小）、模型架构（选择和微调）、训练策略（增强、归一化和优化）和集成预测来评估迁移学习的影响。使用HOLISMOKES VI和SuGOHI X的数据集对十种架构进行微调，并与卷积基线进行比较，分析复杂性和推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;研究重现了先前神经网络系统性比较中的实验，并对通用测试样本上强引力透镜的可检测性提供了见解。不同架构在性能和效率上表现出差异。&lt;h4&gt;结论&lt;/h4&gt;GraViT是一个有效的引力透镜检测工具，系统评估了模型架构和训练策略对性能的影响，为大规模引力透镜检测提供了方法。&lt;h4&gt;翻译&lt;/h4&gt;引力透镜为研究暗物质特性提供了强大的探测手段，对于推断宇宙学参数至关重要。空间和时间遗产调查（LSST）预计在未来十年内将发现约10^5个引力透镜，这需要自动分类器的支持。在本研究中，我们介绍了GraViT，一个用于引力透镜检测的PyTorch流水线，它利用了最先进的Vision Transformer（ViT）模型和MLP-Mixer的广泛预训练。我们通过研究数据质量（源和样本大小）、模型架构（选择和微调）、训练策略（增强、归一化和优化）以及集成预测来评估迁移学习对分类性能的影响。该研究重现了先前神经网络系统性比较中的实验，并对那个通用测试样本上强引力透镜的可检测性提供了见解。我们使用HOLISMOKES VI和SuGOHI X的数据集对十种架构进行微调，并与卷积基线进行比较，讨论了复杂性和推理时间分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gravitational lensing offers a powerful probe into the properties of darkmatter and is crucial to infer cosmological parameters. The Legacy Survey ofSpace and Time (LSST) is predicted to find O(10^5) gravitational lenses overthe next decade, demanding automated classifiers. In this work, we introduceGraViT, a PyTorch pipeline for gravitational lens detection that leveragesextensive pretraining of state-of-the-art Vision Transformer (ViT) models andMLP-Mixer. We assess the impact of transfer learning on classificationperformance by examining data quality (source and sample size), modelarchitecture (selection and fine-tuning), training strategies (augmentation,normalization, and optimization), and ensemble predictions. This studyreproduces the experiments in a previous systematic comparison of neuralnetworks and provides insights into the detectability of strong gravitationallenses on that common test sample. We fine-tune ten architectures usingdatasets from HOLISMOKES VI and SuGOHI X, and benchmark them againstconvolutional baselines, discussing complexity and inference-time analysis.</description>
      <author>example@mail.com (René Parlange, Juan C. Cuevas-Tello, Octavio Valenzuela, Omar de J. Cabrera-Rosas, Tomás Verdugo, Anupreeta More, Anton T. Jaelani)</author>
      <guid isPermaLink="false">2509.00226v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.00077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过迁移学习和数据增强技术解决了有限数据集上的语音情感识别挑战，开发了多种机器学习模型，并在结合RAVDESS和SAVEE数据集上取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;语音情感识别是人机交互中的一个重要且持续的挑战，尽管深度学习已推进口语语言处理，但在有限数据集上实现高性能仍然是一个关键障碍。&lt;h4&gt;目的&lt;/h4&gt;开发和评估一系列机器学习模型，用于人类语音中的自动情感分类，解决有限数据集上的高性能问题。&lt;h4&gt;方法&lt;/h4&gt;开发了支持向量机、长短期记忆网络和卷积神经网络等多种机器学习模型，采用迁移学习策略和创新的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;通过策略性地应用迁移学习和数据增强技术，模型可以在相对较小的数据集限制下实现令人印象深刻的性能；最有效的ResNet34架构在结合RAVDESS和SAVEE数据集上达到66.7%的准确率和0.631的F1分数。&lt;h4&gt;结论&lt;/h4&gt;利用预训练模型和数据增强克服数据稀缺性具有显著优势，为更强大和可推广的语音情感识别系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;语音情感识别在人机交互中呈现一个重要且持续的挑战。虽然深度学习已推进口语语言处理，但在有限数据集上实现高性能仍然是一个关键障碍。本文通过开发和评估一系列机器学习模型（包括支持向量机、长短期记忆网络和卷积神经网络）来应对这个问题，用于人类语音中的自动情感分类。我们证明，通过策略性地采用迁移学习和创新的数据增强技术，我们的模型可以在相对较小的数据集限制下实现令人印象深刻的性能。我们最有效的模型，即ResNet34架构，在结合RAVDESS和SAVEE数据集上建立了新的性能基准，达到66.7%的准确率和0.631的F1分数。这些结果强调了利用预训练模型和数据增强克服数据稀缺性的显著好处，从而为更强大和可推广的语音情感识别系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech Emotion Recognition (SER) presents a significant yet persistentchallenge in human-computer interaction. While deep learning has advancedspoken language processing, achieving high performance on limited datasetsremains a critical hurdle. This paper confronts this issue by developing andevaluating a suite of machine learning models, including Support VectorMachines (SVMs), Long Short-Term Memory networks (LSTMs), and ConvolutionalNeural Networks (CNNs), for automated emotion classification in human speech.We demonstrate that by strategically employing transfer learning and innovativedata augmentation techniques, our models can achieve impressive performancedespite the constraints of a relatively small dataset. Our most effectivemodel, a ResNet34 architecture, establishes a new performance benchmark on thecombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1score of 0.631. These results underscore the substantial benefits of leveragingpre-trained models and data augmentation to overcome data scarcity, therebypaving the way for more robust and generalizable SER systems.</description>
      <author>example@mail.com (Tai Vu)</author>
      <guid isPermaLink="false">2509.00077v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Safety-Critical Multi-Agent MCTS for Mixed Traffic Coordination at Unsignalized Roundabout</title>
      <link>http://arxiv.org/abs/2509.01856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对自动驾驶车辆在无信号环岛环境中进行安全决策的多智能体蒙特卡洛树搜索框架，特别关注混合交通环境中的协调决策问题。&lt;h4&gt;背景&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在混合交通环境中，自动驾驶车辆必须与人类驾驶车辆安全协调。&lt;h4&gt;目的&lt;/h4&gt;开发一个安全关键的多智能体蒙特卡洛树搜索框架，整合确定性和概率预测模型，促进复杂环岛场景中的协作决策。&lt;h4&gt;方法&lt;/h4&gt;提出框架包含三个关键创新：1）分层安全评估模块，通过动态安全阈值和时空风险评估系统处理车辆间交互；2）自适应人类驾驶车辆行为预测方案，结合智能驾驶模型与概率不确定性建模；3）多目标奖励优化策略，联合考虑安全性、效率和协作意图。&lt;h4&gt;主要发现&lt;/h4&gt;大量仿真结果验证了所提出方法在完全自动驾驶和混合交通条件下的有效性。与基准方法相比，该框架一致降低了所有自动驾驶车辆的轨迹偏差，并显著降低了侵占后时间违规率，在完全自动驾驶场景中仅达到1.0%，在混合交通环境中为3.2%。&lt;h4&gt;结论&lt;/h4&gt;所提出的多智能体蒙特卡洛树搜索框架能够有效处理自动驾驶车辆在无信号环岛中的复杂决策问题，特别是在混合交通环境中，实现了安全高效的协调决策。&lt;h4&gt;翻译&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在自动驾驶车辆必须与人类驾驶车辆安全协调的混合交通环境中。本文提出了一个安全关键的多智能体蒙特卡洛树搜索框架，整合了确定性和概率预测模型，以促进复杂环岛场景中的协作决策。所提出的框架引入了三个关键创新：（1）分层安全评估模块，通过动态安全阈值和时空风险评估系统性地处理自动驾驶车辆间、自动驾驶车辆与人类驾驶车辆以及自动驾驶车辆与道路的交互；（2）自适应人类驾驶车辆行为预测方案，结合智能驾驶模型与概率不确定性建模；（3）多目标奖励优化策略，联合考虑安全性、效率和协作意图。大量仿真结果验证了所提出方法在完全自动驾驶和混合交通条件下的有效性。与基准方法相比，我们的框架一致降低了所有自动驾驶车辆的轨迹偏差，并显著降低了侵占后时间违规率，在完全自动驾驶场景中仅达到1.0%，在混合交通环境中为3.2%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decision-making at unsignalized roundabouts poses substantial challenges forautonomous vehicles (AVs), particularly in mixed traffic environments where AVsmust coordinate safely with human-driven vehicles (HDVs). This paper presents asafety-critical multi-agent Monte Carlo Tree Search (MCTS) framework thatintegrates both deterministic and probabilistic prediction models to facilitatecooperative decision-making in complex roundabout scenarios. The proposedframework introduces three key innovations: (1) a hierarchical safetyassessment module that systematically addresses AV-to-AV (A2A), AV-to-HDV(A2H), and AV-to-Road (A2R) interactions through dynamic safety thresholds andspatiotemporal risk evaluation; (2) an adaptive HDV behavior prediction schemethat combines the Intelligent Driver Model (IDM) with probabilistic uncertaintymodeling; and (3) a multi-objective reward optimization strategy that jointlyconsiders safety, efficiency, and cooperative intent. Extensive simulationresults validate the effectiveness of the proposed approach under both fullyautonomous (100% AVs) and mixed traffic (50% AVs + 50% HDVs) conditions.Compared to benchmark methods, our framework consistently reduces trajectorydeviations across all AVs and significantly lowers the rate ofPost-Encroachment Time (PET) violations, achieving only 1.0\% in the fullyautonomous scenario and 3.2% in the mixed traffic setting.</description>
      <author>example@mail.com (Zhihao Lin, Shuo Liu, Zhen Tian, Dezong Zhao, Jianglin Lan)</author>
      <guid isPermaLink="false">2509.01856v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning</title>
      <link>http://arxiv.org/abs/2509.01297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The Conference on Robot Learning (CoRL) 2025 Project  Page: seonsoo-p1.github.io/DMCM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种解纠缠的多上下文元学习框架，通过将任务因素分配给不同的上下文向量，提高了模型的鲁棒性和泛化能力。在正弦回归和四足机器人运动任务中，该方法在分布外条件下表现优于基线，并能够实现模拟到现实的策略转移，仅需少量真实数据。&lt;h4&gt;背景&lt;/h4&gt;在元学习及其下游任务中，许多方法依赖于对任务变化的隐式适应，多个因素在单一纠缠表示中混合在一起。这使得难以解释哪些因素驱动性能，并可能阻碍泛化能力。&lt;h4&gt;目的&lt;/h4&gt;引入一个解纠缠的多上下文元学习框架，明确将每个任务因素分配给不同的上下文向量，以提高对任务的深入理解和增强泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过解耦任务变化，提高对任务的深入理解，并通过在具有共享因素的任务间共享上下文向量来增强泛化能力。在正弦回归任务和四足机器人运动任务两个领域评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;在正弦回归任务中，模型在分布外任务上优于基线，并通过共享与共享振幅或相位偏移相关的上下文向量推广到未见过的正弦函数。在四足机器人运动任务中，解耦了机器人特定属性和地形特征，通过将解耦的上下文向量从动力学模型转移到强化学习，得到的策略在分布外条件下提高了鲁棒性。通过有效共享上下文，模型仅使用20秒的平坦地形真实数据，成功实现了具有分布外机器人特定属性的有挑战性地形的模拟到现实策略转移。&lt;h4&gt;结论&lt;/h4&gt;解纠缠的多上下文元学习框架能够提高对任务的理解和泛化能力。通过解耦任务因素和共享上下文，模型在分布外条件下表现更好，并实现了从模拟到现实的策略转移，这是单任务适应无法实现的。&lt;h4&gt;翻译&lt;/h4&gt;在元学习及其下游任务中，许多方法依赖于对任务变化的隐式适应，其中多个因素在单一纠缠表示中混合在一起。这使得难以解释哪些因素驱动性能，并可能阻碍泛化能力。在这项工作中，我们引入了一个解纠缠的多上下文元学习框架，明确将每个任务因素分配给不同的上下文向量。通过解耦这些变化，我们的方法通过更深入的任务理解提高了鲁棒性，并通过在具有共享因素的任务间共享上下文向量来增强泛化能力。我们在两个领域评估了我们的方法。首先，在正弦回归任务中，我们的模型在分布外任务上优于基线，并通过共享与共享振幅或相位偏移相关的上下文向量推广到未见过的正弦函数。其次，在四足机器人运动任务中，我们在机器人动力学模型中解耦了机器人特定属性和地形特征。通过将从动力学模型获取的解耦上下文向量转移到强化学习中，得到的策略在分布外条件下提高了鲁棒性，超过了依赖单一统一上下文的基线。此外，通过有效共享上下文，我们的模型仅使用20秒的平坦地形真实数据，成功实现了对具有分布外机器人特定属性的有挑战性地形的模拟到现实策略转移，这是单任务适应无法实现的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In meta-learning and its downstream tasks, many methods rely on implicitadaptation to task variations, where multiple factors are mixed together in asingle entangled representation. This makes it difficult to interpret whichfactors drive performance and can hinder generalization. In this work, weintroduce a disentangled multi-context meta-learning framework that explicitlyassigns each task factor to a distinct context vector. By decoupling thesevariations, our approach improves robustness through deeper task understandingand enhances generalization by enabling context vector sharing across taskswith shared factors. We evaluate our approach in two domains. First, on asinusoidal regression task, our model outperforms baselines onout-of-distribution tasks and generalizes to unseen sine functions by sharingcontext vectors associated with shared amplitudes or phase shifts. Second, in aquadruped robot locomotion task, we disentangle the robot-specific propertiesand the characteristics of the terrain in the robot dynamics model. Bytransferring disentangled context vectors acquired from the dynamics model intoreinforcement learning, the resulting policy achieves improved robustness underout-of-distribution conditions, surpassing the baselines that rely on a singleunified context. Furthermore, by effectively sharing context, our model enablessuccessful sim-to-real policy transfer to challenging terrains without-of-distribution robot-specific properties, using just 20 seconds of realdata from flat terrain, a result not achievable with single-task adaptation.</description>
      <author>example@mail.com (Seonsoo Kim, Jun-Gill Kang, Taehong Kim, Seongil Hong)</author>
      <guid isPermaLink="false">2509.01297v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting</title>
      <link>http://arxiv.org/abs/2509.01997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, DASFAA2025 conference full paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，与传统时空长序列方法相比实现了优越的性能，有效解决了物流需求-供应预测问题。&lt;h4&gt;背景&lt;/h4&gt;物流需求-供应预测对即时食品配送平台的效率和质量至关重要，是调度决策的关键指标。未来订单分布信息对物流需求-供应预测的性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决当前时空分析方法难以有效捕捉未来订单分布信息同时保持效率的问题，提高物流需求-供应预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，并采用自适应未来图学习和创新的交叉注意力机制(ACA-Net)的图学习网络框架。&lt;h4&gt;主要发现&lt;/h4&gt;引入进行中和全局图显著提高了预测性能；创新的图学习网络框架能有效学习强大的未来图，显著提高物流需求-供应压力预测结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在实际生产环境中被证明是有效的，相比传统时空长序列方法具有优越性能。&lt;h4&gt;翻译&lt;/h4&gt;评估预期供应与预期需求之间一致性的物流需求-供应预测，对于即时食品配送平台的效率和质量至关重要，并作为调度决策的关键指标。反映即时食品配送中订单分布的未来订单分布信息，对物流需求-供应预测的性能至关重要。当前研究使用时空分析方法从长时间序列中建模未来订单分布信息。然而，学习在线配送平台的未来订单分布是一个对时间序列不敏感且具有强随机性的问题。这些方法往往难以在保持效率的同时有效捕捉这些信息。本文提出了一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，与传统时空长序列方法相比实现了优越的性能。主要贡献如下：(1) 与传统长时间序列相比，在物流需求-供应压力预测中引入进行中和全局图显著提高了预测性能。(2) 提出了一种创新的图学习网络框架，使用自适应未来图学习和创新的交叉注意力机制(ACA-Net)来提取未来订单分布信息，有效学习了一个强大的未来图，显著提高了物流需求-供应压力预测结果。(3) 在实际生产环境中验证了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Logistical demand-supply forecasting that evaluates the alignment betweenprojected supply and anticipated demand, is essential for the efficiency andquality of on-demand food delivery platforms and serves as a key indicator forscheduling decisions. Future order distribution information, which reflects thedistribution of orders in on-demand food delivery, is crucial for theperformance of logistical demand-supply forecasting. Current studies utilizespatial-temporal analysis methods to model future order distributioninformation from serious time slices. However, learning future orderdistribution in online delivery platform is a time-series-insensitive problemwith strong randomness. These approaches often struggle to effectively capturethis information while remaining efficient. This paper proposes an innovativespatiotemporal learning model that utilizes only two graphs (ongoing andglobal) to learn future order distribution information, achieving superiorperformance compared to traditional spatial-temporal long-series methods. Themain contributions are as follows: (1) The introduction of ongoing and globalgraphs in logistical demand-supply pressure forecasting compared to traditionallong time series significantly enhances forecasting performance. (2) Aninnovative graph learning network framework using adaptive future graphlearning and innovative cross attention mechanism (ACA-Net) is proposed toextract future order distribution information, effectively learning a robustfuture graph that substantially improves logistical demand-supply pressureforecasting outcomes. (3) The effectiveness of the proposed method is validatedin real-world production environments.</description>
      <author>example@mail.com (Jiacheng Shi, Haibin Wei, Jiang Wang, Xiaowei Xu, Longzhi Du, Taixu Jiang)</author>
      <guid isPermaLink="false">2509.01997v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Rashomon in the Streets: Explanation Ambiguity in Scene Understanding</title>
      <link>http://arxiv.org/abs/2509.03169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2025 Fall Symposium: AI Trustworthiness and Risk Assessment for  Challenged Contexts (ATRACC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了可解释AI在自动驾驶等安全关键应用中的可靠性问题，特别关注了Rashomon效应对解释一致性的影响。&lt;h4&gt;背景&lt;/h4&gt;可解释AI对于验证和信任安全关键应用（如自动驾驶）中的模型至关重要。&lt;h4&gt;目的&lt;/h4&gt;首次对现实驾驶场景中动作预测任务的Rashomon效应进行经验量化。&lt;h4&gt;方法&lt;/h4&gt;使用定性可解释图(QXGs)作为符号场景表示，训练两种模型类的Rashomon集合（可解释的基于对梯度提升模型和复杂图神经网络GNNs），并通过特征归因方法测量解释一致性。&lt;h4&gt;主要发现&lt;/h4&gt;不同模型间存在显著的解释不一致。&lt;h4&gt;结论&lt;/h4&gt;解释模糊性是问题的固有属性，而不仅仅是建模的人为产物。&lt;h4&gt;翻译&lt;/h4&gt;可解释人工智能(XAI)对于验证和信任自动驾驶等安全关键应用中的模型至关重要。然而，XAI的可靠性受到Rashomon效应的挑战，即多个同样准确的模型可能对同一预测提供不同的解释。本文首次对现实驾驶场景中动作预测任务的Rashomon效应进行了经验量化。使用定性可解释图(QXGs)作为符号场景表示，我们训练了两种不同模型类的Rashomon集合：可解释的、基于对的梯度提升模型和复杂的、基于图的图神经网络(GNNs)。使用特征归因方法，我们测量了这些类别内部和之间的解释一致性。我们的结果显示存在显著的解释不一致。我们的研究结果表明，解释模糊性是问题的固有属性，而不仅仅是建模的人为产物。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable AI (XAI) is essential for validating and trusting models insafety-critical applications like autonomous driving. However, the reliabilityof XAI is challenged by the Rashomon effect, where multiple, equally accuratemodels can offer divergent explanations for the same prediction. This paperprovides the first empirical quantification of this effect for the task ofaction prediction in real-world driving scenes. Using Qualitative ExplainableGraphs (QXGs) as a symbolic scene representation, we train Rashomon sets of twodistinct model classes: interpretable, pair-based gradient boosting models andcomplex, graph-based Graph Neural Networks (GNNs). Using feature attributionmethods, we measure the agreement of explanations both within and between theseclasses. Our results reveal significant explanation disagreement. Our findingssuggest that explanation ambiguity is an inherent property of the problem, notjust a modeling artifact.</description>
      <author>example@mail.com (Helge Spieker, Jørn Eirik Betten, Arnaud Gotlieb, Nadjib Lazaar, Nassim Belmecheri)</author>
      <guid isPermaLink="false">2509.03169v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model</title>
      <link>http://arxiv.org/abs/2509.02876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review for ASCE OPEN: Multidisciplinary Journal of Civil  Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可泛化的学习架构，通过众包在线自然语言指令直接教会机器人多任务执行技能，解决了建筑机器人重新编程工作量大、通用性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;建筑工作的准重复性质导致编程建筑机器人的通用性不足，限制了机器人在建筑行业的广泛应用。机器人无法获得通用能力，因为在一个领域学到的技能无法轻松转移到另一个工作领域或直接用于执行不同的任务集合。&lt;h4&gt;目的&lt;/h4&gt;减少人类工人重新编程机器人场景理解、路径规划和操作组件的工作量，实现机器人多任务技能的高效转移。&lt;h4&gt;方法&lt;/h4&gt;开发了大型语言模型（LLM）、标准化和模块化的分层建模方法，以及建筑信息建模-机器人语义数据管道，以解决多任务技能转移问题。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的机器人任务学习方案实现了最小努力和高质量的多任务重新编程，通过长视野石膏板安装实验验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;基于LLM的分层技能学习框架和技能标准化方案能够显著降低机器人重新编程的工作量，提高任务质量和通用性。&lt;h4&gt;翻译&lt;/h4&gt;建筑工作的准重复性质以及由此导致的编程建筑机器人的通用性不足，持续阻碍着机器人在建筑行业的广泛应用。机器人无法获得通用能力，因为在一个领域学到的技能无法轻松转移到另一个工作领域或直接用于执行不同的任务集合。人类工人必须费力地重新编程机器人的场景理解、路径规划和操作组件，以使机器人能够执行替代的工作任务。本文提出的方法通过提出一种可泛化的学习架构解决了大部分重新编程工作量的问题，该架构通过众包在线自然语言指令直接教会机器人多任务执行技能。开发了大型语言模型（LLM）、标准化和模块化的分层建模方法以及建筑信息建模-机器人语义数据管道，以解决多任务技能转移问题。通过使用全尺寸工业机械臂进行长视野石膏板安装实验测试了所提出的技能标准化方案和基于LLM的分层技能学习框架。所得到的机器人任务学习方案实现了最小努力和高质量的多任务重新编程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决建筑机器人的技能学习和泛化问题。由于建筑工作具有准重复性，导致在一个领域学到的技能难以转移到其他工作领域或直接用于执行不同任务。这个问题很重要，因为建筑行业面临劳动力短缺、成本上升和生产率低等挑战，建筑机器人本可成为解决方案，但成功部署取决于机器人能否灵活执行各种工艺技能，而传统编程方法无法满足这种灵活性需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析建筑行业挑战和机器人应用局限性开始研究，发现模仿学习(IL)虽有效但样本效率低，因此引入分层模仿学习(HIL)来解决数据需求问题。作者认识到语言指令比演示更高效，因此利用大型语言模型(LLM)解决语言准确性问题。方法设计包括建立标准化微技能数据库、开发LLM信息提取系统和构建BIM-ROS信息管道。作者借鉴了IL、HIL、LLM在机器人学习中的应用，以及微技能数据库和BIM系统的现有研究，但针对建筑领域进行了创新整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立标准化的微技能数据库，利用LLM处理众包自然语言指令实现技能链，并通过BIM-ROS管道实现技能参数化。整体流程分为三阶段：1)微技能学习：构建标准化数据库，分析人类指令行为，建立BIM-Web App-ROS信息系统；2)宏技能学习：收集处理自然语言指令，比较不同技能链模型，实现参数化和链式微技能；3)实验验证：评估数据库质量，执行机器人实验，比较算法性能，验证系统可行性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出通用学习架构通过众包自然语言指令教机器人执行多样化任务；2)开发LLM处理自然语言指令；3)提出标准化分层建模方法；4)构建BIM-机器人语义数据管道；5)建立消除个人变异的微技能数据库；6)开发从自然语言指令中检索和连接技能的有效计算方案。相比之前工作，这篇论文专注于建筑机器人领域，结合众包语言、标准化技能和LLM形成完整解决方案，提出具体微技能定义解决语言-动作匹配问题，并通过实验在干墙安装等实际任务中验证了方法有效性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合众包自然语言指令、标准化技能组合和大型语言模型的通用学习架构，使建筑机器人能够高效学习和执行多样化任务技能，显著减少了多任务编程的工作量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The quasi-repetitive nature of construction work and the resulting lack ofgeneralizability in programming construction robots presents persistentchallenges to the broad adoption of robots in the construction industry. Robotscannot achieve generalist capabilities as skills learnt from one domain cannotreadily transfer to another work domain or be directly used to perform adifferent set of tasks. Human workers have to arduously reprogram theirscene-understanding, path-planning, and manipulation components to enable therobots to perform alternate work tasks. The methods presented in this paperresolve a significant proportion of such reprogramming workload by proposing ageneralizable learning architecture that directly teaches robots versatiletask-performance skills through crowdsourced online natural languageinstructions. A Large Language Model (LLM), a standardized and modularizedhierarchical modeling approach, and Building Information Modeling-Robot sematicdata pipeline are developed to address the multi-task skill transfer problem.The proposed skill standardization scheme and LLM-based hierarchical skilllearning framework were tested with a long-horizon drywall installationexperiment using a full-scale industrial robotic manipulator. The resultingrobot task learning scheme achieves multi-task reprogramming with minimaleffort and high quality.</description>
      <author>example@mail.com (Hongrui Yu, Vineet R. Kamat, Carol C. Menassa)</author>
      <guid isPermaLink="false">2509.02876v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture</title>
      <link>http://arxiv.org/abs/2509.02359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The benchmark MulSeT is available at  https://huggingface.co/datasets/WanyueZhang/MulSeT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统分析了多模态大语言模型(MLLMs)在空间理解方面的局限性，提出了MulSeT基准，并从数据和架构角度探索了改进空间推理能力的方法。&lt;h4&gt;背景&lt;/h4&gt;空间理解对MLLMs在具身环境中支持感知、推理和规划至关重要。尽管近期有所进展，现有研究表明MLLMs在空间理解方面仍存在困难，且现有研究缺乏对这些局限性的全面系统评估，通常局限于单一场景。&lt;h4&gt;目的&lt;/h4&gt;从数据和架构两个角度，在单视图、多视图和视频三个代表性场景中系统分析MLLMs的空间理解能力，并提出MulSeT基准。&lt;h4&gt;方法&lt;/h4&gt;设计一系列实验分析MLLMs的空间推理能力；研究训练数据增加对性能的影响；探索视觉编码器和语言模型中位置编码的作用；探索推理注入和通过架构设计优化空间理解的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;从数据角度看，随着训练数据增加，空间理解性能迅速收敛但上限较低，特别是需要空间想象的任务，表明仅扩展数据不足以获得满意性能；从架构角度看，空间理解更依赖于视觉编码器内的位置编码，而非语言模型内的位置编码。&lt;h4&gt;结论&lt;/h4&gt;这些发现揭示了当前MLLMs的局限性，并提出通过数据扩展和架构调优提高空间推理能力的新方向。&lt;h4&gt;翻译&lt;/h4&gt;空间理解对于多模态大语言模型(MLLMs)在具身环境中支持感知、推理和规划至关重要。尽管最近有所进展，现有研究表明MLLMs在空间理解方面仍然存在困难。然而，现有研究缺乏对这些局限性的全面和系统评估，通常局限于单一场景，如单视图或视频。在这项工作中，我们从数据和架构两个角度，在三个代表性场景（单视图、多视图和视频）中对空间理解进行了系统分析。我们提出了一个名为MulSeT（多视图空间理解任务）的基准，并设计了一系列实验来分析MLLMs的空间推理能力。从数据角度看，随着训练数据的增加，空间理解的性能迅速收敛，上限相对较低，特别是对于需要空间想象的任务。这表明仅扩展训练数据不足以获得满意性能。从架构角度看，我们发现无论是级联还是原生MLLMs，空间理解更依赖于视觉编码器内的位置编码，而非语言模型内的位置编码。此外，我们探索了推理注入，并展望通过架构设计优化空间理解的未来改进。这些见解揭示了当前MLLMs的局限性，并提出了通过数据扩展和架构调优提高空间推理能力的新方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型(MLLMs)在空间理解方面的局限性问题。空间理解对于MLLMs在具身环境中支持感知、推理和规划至关重要，在物体操作、路径规划和构建世界模型等认知任务中扮演核心角色。研究这个问题是因为现有MLLMs在单视图场景中能达到人类水平表现，但在多视图和视频场景中存在显著差距，且缺乏对这些局限性的全面系统性评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从数据(数据量)和架构(模型设计)两个维度系统分析MLLMs空间理解能力的局限性。他们首先创建了MulSeT基准，包含三个渐进式任务(遮蔽恢复、距离比较、方位转移)来评估多视图空间理解能力。在数据层面，分析不同训练数据量对性能的影响；在架构层面，研究位置编码在视觉编码器和语言模型中的作用。作者借鉴了现有空间理解任务(如What'sUp、COCO-QA)、位置编码研究(RoPE)、参数高效微调方法(LoRA)和推理注入技术(Chain-of-Thought)的思想，但进行了系统性的整合与创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是系统分析MLLMs空间理解能力的局限性，揭示单纯增加训练数据对空间理解的提升有限，特别是对于需要空间想象的任务，而视觉编码器中的位置编码对空间理解至关重要。整体实现流程包括：1)构建MulSeT基准，在模拟环境中创建多视图场景和三个渐进式任务；2)进行数据层面分析，评估不同训练数据量对性能的影响；3)进行架构层面分析，研究位置编码的作用；4)探索解决方案，包括推理注入(隐式和显式)和架构增强；5)通过实验评估和注意力可视化验证发现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次从数据到架构对MLLMs空间理解进行全面系统分析，覆盖单视图、多视图和视频三个场景；2)提出MulSeT基准，填补多视图空间理解评估的空白，包含三个渐进式任务形成能力梯度；3)揭示数据量增加对空间理解提升有限，特别是需要空间想象的任务；4)发现视觉编码器中的位置编码对空间理解至关重要，而非语言模型中的位置编码；5)提出多视图提示策略并验证其有效性。相比之前工作，本文更全面、系统，不仅评估了模型性能，还深入分析了背后的机制，并提出了针对性的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性分析揭示了多模态大语言模型在空间理解方面的主要瓶颈在于视觉编码器的位置编码而非数据量不足，并提出了针对性的改进策略。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial understanding is essential for Multimodal Large Language Models(MLLMs) to support perception, reasoning, and planning in embodiedenvironments. Despite recent progress, existing studies reveal that MLLMs stillstruggle with spatial understanding. However, existing research lacks acomprehensive and systematic evaluation of these limitations, often restrictedto isolated scenarios, such as single-view or video. In this work, we present asystematic analysis of spatial understanding from both data and architecturalperspectives across three representative scenarios: single-view, multi-view,and video. We propose a benchmark named MulSeT (Multi-view SpatialUnderstanding Tasks), and design a series of experiments to analyze the spatialreasoning capabilities of MLLMs. From the data perspective, the performance ofspatial understanding converges quickly as the training data increases, and theupper bound is relatively low, especially for tasks that require spatialimagination. This indicates that merely expanding training data is insufficientto achieve satisfactory performance. From the architectural perspective, wefind that spatial understanding relies more heavily on the positional encodingwithin the visual encoder than within the language model, in both cascaded andnative MLLMs. Moreover, we explore reasoning injection and envision futureimprovements through architectural design to optimize spatial understanding.These insights shed light on the limitations of current MLLMs and suggest newdirections for improving spatial reasoning capabilities through data scalingand architectural tuning.</description>
      <author>example@mail.com (Wanyue Zhang, Yibin Huang, Yangbin Xu, JingJing Huang, Helu Zhi, Shuo Ren, Wang Xu, Jiajun Zhang)</author>
      <guid isPermaLink="false">2509.02359v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks</title>
      <link>http://arxiv.org/abs/2509.02175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RocketScience，一个开源的对比视觉语言模型基准测试，专门用于测试空间关系理解能力。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在空间关系理解方面存在明显不足，需要一个专门的基准测试来评估这方面的能力。&lt;h4&gt;目的&lt;/h4&gt;创建一个对人类容易但对当前VLMs困难的基准测试，专门评估模型在空间关系理解方面的表现。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含全新真实世界图像-文本对的基准测试，主要关注相对空间理解和物体顺序。进行了解缠分析，分离了基于思维链模型中的物体定位和空间推理的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;开源和前沿商业VLMs在空间关系理解方面表现明显不足；推理模型表现出令人惊讶的高性能；基准测试的性能瓶颈在于空间推理能力，而非物体定位能力。&lt;h4&gt;结论&lt;/h4&gt;当前VLMs在空间关系理解方面存在显著不足，需要改进这方面的能力以提升整体表现。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了RocketScience，一个开源的对比视觉语言模型基准测试，用于测试空间关系理解能力。它完全由全新的真实世界图像-文本对组成，主要涵盖相对空间理解和物体顺序。该基准测试设计为对人类非常容易但对当前一代VLMs非常困难，这一点已通过经验验证。我们的结果显示，开源和前沿商业VLMs在空间关系理解方面明显缺乏，而推理模型表现出令人惊讶的高性能。此外，我们进行了解缠分析，分离了基于思维链的模型中物体定位和空间推理的贡献，发现基准测试的性能受限于空间推理能力，而非物体定位能力。我们以CC-BY-4.0许可证发布数据集，并在https://github.com/nilshoehing/rocketscience提供评估代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose RocketScience, an open-source contrastive VLM benchmark that testsfor spatial relation understanding. It is comprised of entirely new real-worldimage-text pairs covering mostly relative spatial understanding and the orderof objects. The benchmark is designed  to be very easy for humans and hard for the current generation of VLMs, andthis is empirically verified. Our results show a striking lack of spatialrelation understanding in open source and frontier commercial VLMs and asurprisingly high performance of reasoning models. Additionally, we perform adisentanglement analysis to separate the contributions of object localizationand spatial reasoning in chain-of-thought-based models and find that theperformance on the benchmark is bottlenecked by spatial reasoning and notobject localization capabilities.  We release the dataset with a CC-BY-4.0 license and make the evaluation codeavailable at: https://github.com/nilshoehing/rocketscience</description>
      <author>example@mail.com (Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque)</author>
      <guid isPermaLink="false">2509.02175v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Omnidirectional Spatial Modeling from Correlated Panoramas</title>
      <link>http://arxiv.org/abs/2509.02164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍CFpano数据集和方法名多模态大语言模型，用于处理跨帧相关全景图像的视觉问答任务。该数据集包含2700多张图像和8000多个问答对，方法使用GRPO优化和定制奖励函数进行微调，在实验中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;全方位场景理解对于具身AI、自动驾驶和沉浸式环境等多种下游应用至关重要，但由于360度图像中的几何失真和复杂空间关系，这仍然具有挑战性。现有的全方位方法仅处理单帧图像，忽略了跨帧相关的全景图。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究人员引入了CFpano，这是第一个专注于整体360度场景中跨帧相关全景图像视觉问答的基准数据集。基于此数据集，他们进一步提出了方法名，一个使用GRPO和定制奖励函数进行微调的多模态大语言模型，以实现跨帧相关全景图的稳健和一致的推理。&lt;h4&gt;方法&lt;/h4&gt;研究人员构建了CFpano数据集，包含2700多张图像和8000多个问答对，问题类型包括多项选择和开放式VQA。基于CFpano，他们提出了方法名，一个使用GRPO和定制奖励函数进行微调的多模态大语言模型。他们使用现有的多模态大语言模型在CFpano上进行了基准实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，方法名在多项选择和开放式VQA任务上都取得了最先进的性能，在所有主要推理类别上都优于强基线（整体性能提升5.37%）。分析验证了GRPO的有效性，并为全景场景理解建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;CFpano和方法名为跨帧相关全景图像的视觉问答任务提供了新的基准和解决方案，能够实现更稳健和一致的全景场景理解。&lt;h4&gt;翻译&lt;/h4&gt;全方位场景理解对于各种下游应用至关重要，例如具身AI、自动驾驶和沉浸式环境，但由于360度图像中的几何失真和复杂空间关系，这仍然具有挑战性。现有的全方位方法在单帧内实现场景理解，同时忽略了跨帧相关的全景图。为了填补这一空白，我们引入了CFpano，这是第一个专注于整体360度场景中跨帧相关全景图像视觉问答的基准数据集。CFpano包含2700多张图像和8000多个问答对，问题类型包括多项选择和开放式VQA。基于我们的CFpano，我们进一步提出了方法名，一个使用GRPO和定制奖励函数进行微调的多模态大语言模型，以实现跨帧相关全景图的稳健和一致的推理。我们使用现有的多模态大语言模型在CFpano上进行了基准实验。实验结果表明，方法名在多项选择和开放式VQA任务上都取得了最先进的性能，在所有主要推理类别上都优于强基线（整体性能提升5.37%）。我们的分析验证了GRPO的有效性，并为全景场景理解建立了新的基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让AI模型更好地理解和推理360度全景图像中的跨帧相关内容。这个问题重要是因为全景图像在具身AI、自动驾驶和虚拟现实等领域有广泛应用，但全景图像存在严重几何失真、高物体密度和复杂空间关系，使得视觉理解比传统窄视野图像更具挑战性。现有方法大多只关注单帧全景图像，忽略了跨帧之间的相关性，而跨帧理解对完整把握全景场景至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有全景数据集的局限性，特别是缺乏跨帧、多视图和细粒度推理的全面覆盖。他们基于ReplicaPano数据集构建了CFpano数据集，利用其丰富的3D注释信息。模型设计上，作者借鉴了Qwen2.5-VL作为基础模型，并采用DeepSeek-R1等模型中成功的GRPO强化学习方法。同时，作者设计了专门针对全景场景的奖励函数，包括格式奖励、准确度奖励和一致性奖励，以优化模型在跨帧全景推理中的表现。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用跨帧全景图像之间的相关性来增强场景理解，并通过专门的强化学习训练和任务特定的奖励函数提高模型在全景场景推理中的表现。整体流程包括：1) 构建CFpano数据集，利用3D注释生成细粒度描述和问题；2) 基于Qwen2.5-VL构建Pano-R1模型；3) 使用GRPO进行微调，并设计三种特定奖励函数；4) 评估模型在多选题和开放式问答任务上的性能。模型训练采用LoRA进行参数高效微调，评估时使用精确匹配准确度评估多选题，语义相似度评估开放式问答。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CFpano数据集：首个专门用于跨帧全景视觉问答的数据集，包含2700多张图像和8000多个问答对；2) Pano-R1模型：基于GRPO训练的多模态大语言模型，针对全景场景优化；3) 特定奖励函数设计：格式奖励、准确度奖励和一致性奖励。相比之前的工作，CFpano专注于跨帧相关全景图像，而不仅仅是单帧；Pano-R1专门针对全景场景进行了优化，使用GRPO而非传统强化学习方法；评估框架更全面，涵盖多种推理类型和问题格式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了首个专门用于跨帧全景视觉问答的数据集CFpano，并提出了基于强化学习的多模态大语言模型Pano-R1，显著提升了AI在360度全景场景中的理解和推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omnidirectional scene understanding is vital for various downstreamapplications, such as embodied AI, autonomous driving, and immersiveenvironments, yet remains challenging due to geometric distortion and complexspatial relations in 360{\deg} imagery. Existing omnidirectional methodsachieve scene understanding within a single frame while neglecting cross-framecorrelated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramasvisual question answering in the holistic 360{\deg} scenes. CFpano consists ofover 2700 images together with over 8000 question-answer pairs, and thequestion types include both multiple choice and open-ended VQA. Building uponour CFpano, we further present \methodname, a multi-modal large language model(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set oftailored reward functions for robust and consistent reasoning with cross-framecorrelated panoramas. Benchmark experiments with existing MLLMs are conductedwith our CFpano. The experimental results demonstrate that \methodname achievesstate-of-the-art performance across both multiple-choice and open-ended VQAtasks, outperforming strong baselines on all major reasoning categories(\textbf{+5.37\%} in overall performance). Our analyses validate theeffectiveness of GRPO and establish a new benchmark for panoramic sceneunderstanding.</description>
      <author>example@mail.com (Xinshen Zhang, Tongxi Fu, Xu Zheng)</author>
      <guid isPermaLink="false">2509.02164v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring</title>
      <link>http://arxiv.org/abs/2509.01878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了水下AI作为研究前沿的兴起，以及海洋感知从利基应用转变为AI创新催化剂的因素。研究分析了水下独特挑战如何推动AI学习方法的进步，以及这些创新如何超越海洋应用，对一般计算机视觉、机器人和环境监测产生积极影响。&lt;h4&gt;背景&lt;/h4&gt;海洋生态系统正面临气候变化带来的日益增长的压力，这促使需要可扩展的、由人工智能驱动的监测解决方案。&lt;h4&gt;目的&lt;/h4&gt;研究水下AI作为主要研究前沿的快速兴起，分析将海洋感知从利基应用转变为AI创新催化剂的因素，并探讨水下约束如何推动AI技术的边界。&lt;h4&gt;方法&lt;/h4&gt;识别三个趋同驱动因素，分析水下独特挑战如何推动AI学习方法的进步，调查数据集、场景理解和3D重建的新趋势，以及水下AI监测范式的转变。&lt;h4&gt;主要发现&lt;/h4&gt;1) 三个趋同驱动因素：生态系统规模监测的环境必要性、公民科学平台使水下数据集民主化、研究人员从饱和的陆地计算机视觉领域迁移；2) 水下独特挑战（浑浊度、隐匿物种检测、专家标注瓶颈、跨生态系统泛化）推动了弱监督学习、开放集识别和退化条件下鲁棒感知的基本进展；3) 水下约束正在推动基础模型、自监督学习和感知的边界，其方法创新远远超出海洋应用。&lt;h4&gt;结论&lt;/h4&gt;水下AI监测正在从被动观察向AI驱动的有针对性干预能力转变范式，其方法创新不仅对海洋科学有意义，还对一般计算机视觉、机器人和环境监测领域有广泛影响。&lt;h4&gt;翻译&lt;/h4&gt;海洋生态系统正面临气候变化带来的日益增长的压力，这促使需要可扩展的、由人工智能驱动的监测解决方案。本文研究了水下AI作为主要研究前沿的快速兴起，分析了将海洋感知从利基应用转变为AI创新催化剂的因素。我们确定了三个趋同驱动因素：生态系统规模监测的环境必要性、公民科学平台使水下数据集民主化、研究人员从饱和的陆地计算机视觉领域迁移。我们的分析揭示了水下独特挑战——浑浊度、隐匿物种检测、专家标注瓶颈和跨生态系统泛化——如何推动弱监督学习、开放集识别和退化条件下鲁棒感知的基本进展。我们调查了数据集、场景理解和3D重建的新趋势，突显了从被动观察向AI驱动的有针对性干预能力转变的范式转变。该论文展示了水下约束如何推动基础模型、自监督学习和感知的边界，其方法创新远远超出海洋应用，有利于一般的计算机视觉、机器人和环境监测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决海洋生态系统监测面临的挑战，特别是气候变化对珊瑚礁和海草甸等生态系统的威胁。这个问题重要是因为传统监测方法成本高、效率低且难以覆盖广阔的海洋区域，而AI驱动的自动化解决方案能够实现生态系统规模的监测，为海洋保护和恢复提供关键支持。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于三个汇聚的驱动因素设计方法：环境必要性（对大规模监测的需求）、数据民主化（通过公民科学平台获取数据）和研究机会（水下环境提供新的AI创新空间）。作者借鉴了现有的计算机视觉、机器学习和深度学习技术，包括自监督学习、基础模型、开集识别、弱监督学习、视觉-语言模型和3D重建技术，并将这些方法应用于水下环境，根据水下特有的挑战进行了调整和创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将水下环境的独特挑战转化为AI创新机会，推动技术进步的同时解决海洋生态监测需求。整体流程包括：1)数据收集与标注（利用公民科学平台、游戏化工具和半自动AI标注）；2)场景理解（应用弱监督学习、鲁棒感知技术和基础模型）；3)场景重建（使用水下特定3D重建技术）；4)应用部署（渔业管理、水产养殖、珊瑚礁恢复等）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)数据民主化创新（公民科学和游戏化平台）；2)弱监督学习方法（从密集监督到图像级标注）；3)鲁棒感知技术（针对水下视觉退化）；4)基础模型应用（如CoralSCOP）；5)水下3D重建（实时处理和数字孪生）。相比之前工作，这些创新实现了从小规模到大规模、从密集监督到弱监督、从通用技术到领域感知算法、从被动观测到主动干预、从单一任务到通用模型的转变。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了水下环境如何通过其独特挑战推动AI创新，同时为应对海洋生态系统监测的迫切需求提供解决方案，建立了环境需求与技术进步之间的双向关系。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine ecosystems face increasing pressure due to climate change, driving theneed for scalable, AI-powered monitoring solutions. This paper examines therapid emergence of underwater AI as a major research frontier and analyzes thefactors that have transformed marine perception from a niche application into acatalyst for AI innovation. We identify three convergent drivers: environmentalnecessity for ecosystem-scale monitoring, democratization of underwaterdatasets through citizen science platforms, and researcher migration fromsaturated terrestrial computer vision domains. Our analysis reveals how uniqueunderwater challenges - turbidity, cryptic species detection, expert annotationbottlenecks, and cross-ecosystem generalization - are driving fundamentaladvances in weakly supervised learning, open-set recognition, and robustperception under degraded conditions. We survey emerging trends in datasets,scene understanding and 3D reconstruction, highlighting the paradigm shift frompassive observation toward AI-driven, targeted intervention capabilities. Thepaper demonstrates how underwater constraints are pushing the boundaries offoundation models, self-supervised learning, and perception, withmethodological innovations that extend far beyond marine applications tobenefit general computer vision, robotics, and environmental monitoring.</description>
      <author>example@mail.com (Scarlett Raine, Tobias Fischer)</author>
      <guid isPermaLink="false">2509.01878v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Articulated Object Estimation in the Wild</title>
      <link>http://arxiv.org/abs/2509.01708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9th Conference on Robot Learning (CoRL), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为ArtiPoint的新框架，用于在动态相机运动和部分可观察性条件下推断关节式物体模型。作者还推出了Arti4D数据集，这是第一个以第一人称视角在自然环境中捕获关节式物体交互的数据集，并通过基准测试证明了ArtiPoint的优越性能。&lt;h4&gt;背景&lt;/h4&gt;理解关节式物体的三维运动在机器人场景理解、移动操作和运动规划中至关重要。先前的方法主要关注受控环境，假设固定的相机视角或直接观察各种物体状态，这些方法在更真实的无约束环境中往往失效。相比之下，人类可以轻松地通过观察他人操作物体来推断关节运动。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在动态相机运动和部分可观察性条件下推断关节式物体模型的新框架，并创建相应的数据集以促进该领域的研究。&lt;h4&gt;方法&lt;/h4&gt;作者提出了ArtiPoint框架，结合深度点跟踪和因子图优化框架，可以直接从原始RGB-D视频中稳健地估计关节部件轨迹和关节轴。同时创建了Arti4D数据集，包含关节标签和真实相机姿态。&lt;h4&gt;主要发现&lt;/h4&gt;ArtiPoint在Arti4D数据集上的性能优于一系列经典和基于学习的基线方法。&lt;h4&gt;结论&lt;/h4&gt;作者将ArtiPoint代码和Arti4D数据集公开，可供研究人员使用，网址为https://artipoint.cs.uni-freiburg.de。&lt;h4&gt;翻译&lt;/h4&gt;理解关节式物体的三维运动在机器人场景理解、移动操作和运动规划中至关重要。先前用于关节估计的方法主要关注受控环境，假设固定的相机视角或直接观察各种物体状态，这些方法在更真实的无约束环境中往往失效。相比之下，人类可以轻松地通过观察他人操作物体来推断关节运动。受此启发，我们引入了ArtiPoint，这是一个新颖的估计框架，可以在动态相机运动和部分可观察性的情况下推断关节式物体模型。通过结合深度点跟踪和因子图优化框架，ArtiPoint可以直接从原始RGB-D视频中稳健地估计关节部件轨迹和关节轴。为了促进该领域的未来研究，我们引入了Arti4D，这是第一个以第一人称视角在自然环境中捕获关节式物体交互的数据集，包含关节标签和真实相机姿态。我们在一系列经典和基于学习的基线上对ArtiPoint进行了基准测试，证明了其在Arti4D上的优越性能。我们在https://artipoint.cs.uni-freiburg.de上公开了代码和Arti4D。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在非受控环境中（动态相机视角、部分可观测条件下）估计可关节化物体三维运动模型的问题。这个问题在机器人领域很重要，因为机器人场景理解、移动操作和运动规划都需要理解可关节化物体的运动，而现有方法在受控环境中表现良好，但在真实世界场景中往往失败。人类能轻松通过观察他人操作物体推断关节活动，而机器人系统还缺乏这种能力，限制了从可关节化物体估计到日常机器人中人类到机器人模仿的见解转移。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人类通过观察他人操作物体推断关节活动的启发，设计了ArtiPoint方法。他们借鉴了多项现有工作：利用了深度点跟踪技术（特别是CoTracker3等any-point跟踪模型），采用了因子图优化框架（参考Buchanan等人的工作），使用手部检测作为触发信号（借鉴交互感知领域），并利用MobileSAM进行类无关的实例分割。设计过程包括四个阶段：提取交互片段、识别关键点、深度点跟踪与3D过滤、以及关节模型估计，形成一个完整的从输入到输出的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; ArtiPoint方法的核心思想是利用人类操作物体的手部作为先验知识，通过跟踪手部附近的点来捕获物体运动，然后利用这些点的轨迹估计可关节化物体的运动模型。整体实现流程分为四个阶段：1）交互间隔提取：使用手部分割模型识别包含物体交互的视频片段；2）深度点跟踪：在手部附近采样点，使用实例分割模型识别潜在物体，并跟踪关键点；3）3D轨迹估计和过滤：将2D轨迹提升到3D，补偿相机运动，过滤静态点和被遮挡点，并进行轨迹平滑；4）关节模型估计：使用因子图框架估计物体部件轨迹和关节运动模型参数，分类关节类型并在场景中注册模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1）ArtiPoint框架：一种能在非受控环境中基于深度点轨迹工作的鲁棒估计方法；2）Arti4D数据集：首个场景级的野外可关节化物体交互数据集，包含关节轴线标签和3D真实相机姿态；3）全面的性能比较：与经典和深度学习方法进行对比；4）公开资源：发布代码、数据和模型预测。相比之前工作，ArtiPoint能在动态相机运动和部分可观测条件下工作，处理场景级而非孤立物体，结合深度点跟踪和因子图优化而非依赖昂贵场景级优化或易过拟合的深度模型，并利用手部检测作为交互触发信号。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ArtiPoint通过结合深度点跟踪和因子图优化，首次在非受控环境中实现了对可关节化物体的鲁棒运动模型估计，并提供了首个场景级的野外可关节化物体交互数据集Arti4D，推动了机器人场景理解的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the 3D motion of articulated objects is essential in roboticscene understanding, mobile manipulation, and motion planning. Prior methodsfor articulation estimation have primarily focused on controlled settings,assuming either fixed camera viewpoints or direct observations of variousobject states, which tend to fail in more realistic unconstrained environments.In contrast, humans effortlessly infer articulation by watching othersmanipulate objects. Inspired by this, we introduce ArtiPoint, a novelestimation framework that can infer articulated object models under dynamiccamera motion and partial observability. By combining deep point tracking witha factor graph optimization framework, ArtiPoint robustly estimates articulatedpart trajectories and articulation axes directly from raw RGB-D videos. Tofoster future research in this domain, we introduce Arti4D, the firstego-centric in-the-wild dataset that captures articulated object interactionsat a scene level, accompanied by articulation labels and ground-truth cameraposes. We benchmark ArtiPoint against a range of classical and learning-basedbaselines, demonstrating its superior performance on Arti4D. We make code andArti4D publicly available at https://artipoint.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Abdelrhman Werby, Martin Büchner, Adrian Röfer, Chenguang Huang, Wolfram Burgard, Abhinav Valada)</author>
      <guid isPermaLink="false">2509.01708v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation</title>
      <link>http://arxiv.org/abs/2509.01364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoNav是一种新框架，通过利用拓扑结构作为空间记忆来解决ObjectNav中的内存管理挑战，特别是在长距离任务和动态场景中。它构建并更新捕捉场景连接、相邻关系和语义含义的拓扑图，帮助代理积累空间知识、检索关键信息并有效推理遥远目标。&lt;h4&gt;背景&lt;/h4&gt;ObjectNav在大语言模型(LLMs)的帮助下取得了很大进展，但在内存管理方面仍然面临挑战，特别是在长距离任务和动态场景中。&lt;h4&gt;目的&lt;/h4&gt;提出TopoNav新框架，解决ObjectNav中的内存管理挑战，特别是在长距离任务和动态场景中。&lt;h4&gt;方法&lt;/h4&gt;TopoNav利用拓扑结构作为空间记忆，构建并更新拓扑图，捕捉场景连接、相邻关系和语义含义，帮助代理随时间积累空间知识，检索关键信息，并有效推理遥远目标。&lt;h4&gt;主要发现&lt;/h4&gt;TopoNav在基准ObjectNav数据集上取得了最先进的性能，具有更高的成功率和更高效的路径。它在多样化和复杂环境中表现尤为出色，能够将临时视觉输入与持久的空间理解连接起来。&lt;h4&gt;结论&lt;/h4&gt;TopoNav通过拓扑结构有效解决了ObjectNav中的内存管理问题，特别是在长距离任务和动态场景中。&lt;h4&gt;翻译&lt;/h4&gt;物体导航(ObjectNav)在大语言模型(LLMs)的帮助下取得了很大进展，但在内存管理方面仍然面临挑战，特别是在长距离任务和动态场景中。为此，我们提出了TopoNav，一种利用拓扑结构作为空间记忆的新框架。通过构建和更新捕捉场景连接、相邻关系和语义含义的拓扑图，TopoNav帮助代理随时间积累空间知识，检索关键信息，并有效推理遥远目标。我们的实验显示，TopoNav在基准ObjectNav数据集上取得了最先进的性能，具有更高的成功率和更高效的路径。它在多样化和复杂环境中表现尤为出色，因为它将临时视觉输入与持久的空间理解连接起来。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决对象导航(ObjectNav)中，特别是长距离任务和动态场景下的内存管理挑战。当前方法大多依赖短暂的视觉观察，难以积累和保留环境的长期结构知识，导致目标混乱、冗余路径甚至导航失败。这个问题很重要，因为对象导航是具身AI和机器人交互的核心任务，缺乏稳健的空间记忆机制会阻碍导航系统在真实世界复杂环境中的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到拓扑结构本质上是一种紧凑、持久的空间记忆形式，不同于像素级视觉细节，拓扑信息能以稳定、支持决策的格式捕获环境结构本质。作者借鉴了现有工作如双记忆网络、CVLN和Mem4Nav等记忆机制，以及ETPNav和Revind等拓扑导航方法，但指出它们的局限性：大多存储数据级观察而非结构级关系，依赖预定义拓扑先验，缺乏实时适应环境变化的能力。基于此，作者设计了TopoNav框架，结合语义点云地图和拓扑记忆图，并利用VLM进行指导。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将拓扑结构作为空间记忆的核心，构建动态更新的拓扑图来捕捉场景连接、相邻性和语义含义，作为连接感知和行动的认知地图。整体实现流程包括：1) 构建语义点云地图(场景点云、物体点云、可导航点云、障碍物点云和前沿点云)；2) 构建拓扑记忆图(创建包含ID、位置、物体类别、房间类型和前沿计数的节点，并实现节点创建和合并)；3) 设计航点选择策略(基于VLM指导，分为探索阶段和目标获取阶段，使用多种能力值进行决策)；4) 系统集成(在每个时间步结合语义地图、拓扑记忆图和VLM指导选择最佳航点)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 建立了拓扑结构与对象导航中空间记忆的理论联系；2) 引入TopoNav框架实现动态拓扑记忆图，桥接短暂视觉输入和持久空间理解；3) 在基准数据集上实现了最先进的性能。相比之前工作，TopoNav将拓扑结构建模为可操作的空间记忆而非静态地图，动态更新图节点和边以反映环境变化；将拓扑图与LLM紧密集成实现语义感知的拓扑更新；通过视觉-语言交互不断细化空间知识，支持实时自适应偏差纠正和长距离目标推理；作为零样本方法，其性能匹配或超越了基于训练的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TopoNav通过引入基于拓扑结构的动态空间记忆机制，显著提升了对象导航在长距离任务和复杂环境中的性能，成功桥接了短暂视觉感知与持久空间理解之间的差距，实现了无需训练的最先进导航效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object Navigation (ObjectNav) has made great progress with large languagemodels (LLMs), but still faces challenges in memory management, especially inlong-horizon tasks and dynamic scenes. To address this, we propose TopoNav, anew framework that leverages topological structures as spatial memory. Bybuilding and updating a topological graph that captures scene connections,adjacency, and semantic meaning, TopoNav helps agents accumulate spatialknowledge over time, retrieve key information, and reason effectively towarddistant goals. Our experiments show that TopoNav achieves state-of-the-artperformance on benchmark ObjectNav datasets, with higher success rates and moreefficient paths. It particularly excels in diverse and complex environments, asit connects temporary visual inputs with lasting spatial understanding.</description>
      <author>example@mail.com (Peiran Liu, Qiang Zhang, Daojie Peng, Lingfeng Zhang, Yihao Qin, Hang Zhou, Jun Ma, Renjing Xu, Yiding Ji)</author>
      <guid isPermaLink="false">2509.01364v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
      <link>http://arxiv.org/abs/2509.01106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://robix-seed.github.io/robix/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Robix是一个统一的视觉-语言模型，整合了机器人推理、任务规划和自然语言交互，作为分层机器人系统的高级认知层，能够生成命令和响应，使机器人遵循复杂指令、规划长期任务并与人类自然交互。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要高级认知能力来处理复杂任务规划和人机交互，现有方法可能缺乏统一架构和高级推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型，整合机器人推理、任务规划和自然语言交互，实现机器人遵循复杂指令、规划长期任务并与人类自然交互。&lt;h4&gt;方法&lt;/h4&gt;采用三阶段训练策略：持续预训练增强基础具身推理能力；监督微调将人机交互和任务规划建模为统一序列；强化学习提高推理-动作一致性和任务连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;Robix在交互式任务执行中优于GPT-4o和Gemini 2.5 Pro等基线，在不同指令类型和用户参与任务中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Robix作为一个统一模型，成功整合了机器人的多种高级认知功能，提供了比现有基线更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了Robix，这是一个统一模型，在单一的视觉-语言架构中整合了机器人推理、任务规划和自然语言交互。作为分层机器人系统的高级认知层，Robix动态生成原子命令给低级控制器并为人类交互生成口头响应，使机器人能够在端到端框架内遵循复杂指令、规划长期任务并与人类自然交互。Robix进一步引入了新功能，如主动对话、实时中断处理和任务执行中的上下文常识推理。其核心是利用思维链推理，并采用三阶段训练策略：(1)持续预训练以增强基础具身推理能力，包括3D空间理解、视觉定位和任务中心推理；(2)监督微调将人机交互和任务规划建模为统一的推理-动作序列；(3)强化学习以提高推理-动作一致性和长期任务连贯性。大量实验表明，Robix在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），在多样化指令类型（如开放式、多阶段、约束式、无效式和中断式）和各种用户参与任务（如餐桌清理、杂货采购和饮食过滤）中表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Robix, a unified model that integrates robot reasoning, taskplanning, and natural language interaction within a single vision-languagearchitecture. Acting as the high-level cognitive layer in a hierarchical robotsystem, Robix dynamically generates atomic commands for the low-levelcontroller and verbal responses for human interaction, enabling robots tofollow complex instructions, plan long-horizon tasks, and interact naturallywith human within an end-to-end framework. Robix further introduces novelcapabilities such as proactive dialogue, real-time interruption handling, andcontext-aware commonsense reasoning during task execution. At its core, Robixleverages chain-of-thought reasoning and adopts a three-stage trainingstrategy: (1) continued pretraining to enhance foundational embodied reasoningabilities including 3D spatial understanding, visual grounding, andtask-centric reasoning; (2) supervised finetuning to model human-robotinteraction and task planning as a unified reasoning-action sequence; and (3)reinforcement learning to improve reasoning-action consistency and long-horizontask coherence. Extensive experiments demonstrate that Robix outperforms bothopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) ininteractive task execution, demonstrating strong generalization across diverseinstruction types (e.g., open-ended, multi-stage, constrained, invalid, andinterrupted) and various user-involved tasks such as table bussing, groceryshopping, and dietary filtering.</description>
      <author>example@mail.com (Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li)</author>
      <guid isPermaLink="false">2509.01106v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2509.00800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to SIGGRAPH Asia 2025 Technical Communications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合语义引导的3D高斯飞溅技术，用于水下3D重建。通过语义特征嵌入和分阶段训练策略，提高了重建质量和稳定性，在多个数据集上展示了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;水下环境中的准确3D重建是一个复杂挑战，主要问题包括光线扭曲、浑浊度和能见度有限。基于AI的技术已被用于解决这些问题，但现有方法尚未充分利用AI的潜力，特别是在将语言模型与视觉处理集成方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，利用多模态交叉知识创建语义引导的3D高斯飞溅，实现稳健且高保真的深海场景重建。&lt;h4&gt;方法&lt;/h4&gt;将额外的语义特征嵌入到每个高斯基元中，由CLIP提取的语义特征监督，通过专门的语义一致性损失确保与高级场景理解的对齐，并提出一种新的分阶段训练策略，结合从粗到细的学习和后期参数优化。&lt;h4&gt;主要发现&lt;/h4&gt;在SeaThru-NeRF和Submerged3D数据集上，该方法在三个指标上持续优于最先进的方法，在PSNR方面平均提高了高达3.09分贝。&lt;h4&gt;结论&lt;/h4&gt;该方法在水下探索和海洋感知应用中是一个强有力的候选方案。&lt;h4&gt;翻译&lt;/h4&gt;在水下环境中进行准确的3D重建仍然是一个复杂的挑战，由于光线扭曲、浑浊度和能见度有限等问题。基于AI的技术已被应用于解决这些问题，然而，现有方法尚未充分利用AI的潜力，特别是在将语言模型与视觉处理集成方面。在本文中，我们提出了一种新颖的框架，利用多模态交叉知识创建语义引导的3D高斯飞溅，用于稳健且高保真的深海场景重建。通过将额外的语义特征嵌入到每个高斯基元中，并由CLIP提取的语义特征监督，我们的方法在整个训练过程中强制执行语义和结构感知。专门的语义一致性损失确保与高级场景理解的对齐。此外，我们提出了一种新颖的分阶段训练策略，结合从粗到细的学习和后期参数优化，以进一步提高稳定性和重建质量。大量结果表明，我们的方法在SeaThru-NeRF和Submerged3D数据集上的三个指标中持续优于最先进的方法，在PSNR方面平均提高了高达3.09分贝，使其成为水下探索和海洋感知应用的有力候选。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决水下环境中准确3D重建的挑战，特别是由于光线扭曲、浑浊度和能见度有限等问题导致的重建困难。这个问题在现实中非常重要，因为水下探索支持海洋生态研究、考古学探索和机器人导航等多种应用，这些任务都依赖于高质量的3D重建技术。此外，水下环境的数据采集成本高且通常稀疏，使得重建工作更具挑战性，而高质量的3D视觉在VR头显等教育创意应用中也尤为关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法（如NeRF和3DGS）通常基于清晰介质的假设，在水下浑浊条件下性能严重下降。他们注意到先前方法对所有区域均匀处理，而水下环境中显著物体（通常在前景）应得到更多关注。作者借鉴了RUSplatting的工作，使用MLP估计水下介质参数，结合帧插值机制和边缘感知损失。此外，他们利用CLIP模型提取语义特征，使用BLIPo3生成场景描述，并通过Grounded-SAM捕获感兴趣区域。基于这些思考，作者设计了语义引导的高斯点和分阶段训练策略来解决水下重建问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为每个3D高斯点添加可学习的语义特征，通过CLIP提取的语义特征进行监督，强制模型在整个训练过程中保持语义和结构感知，并采用分阶段训练策略结合粗到细学习和后期参数细化。整体流程包括：1)为场景生成文本描述并使用Grounded-SAM捕获感兴趣区域；2)计算检测区域的CLIP嵌入作为参考语义特征；3)为每个3D高斯点添加语义特征向量；4)训练中通过语义损失函数使高点特征与参考特征对齐；5)采用分阶段优化，前60%关注粗结构和语义对齐，后40%冻结几何参数细化外观和颜色，同时调整损失函数权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语义引导的高斯点，为每个3D高斯添加可学习语义特征；2)语义一致性损失，确保与高级场景理解一致；3)分阶段训练策略，结合粗到细学习和后期参数细化。相比之前的工作，SWAGSplatting特别关注水下显著物体而非均匀处理所有区域，通过语义机制增强几何一致性和区域级对齐，并采用分阶段优化策略提高训练稳定性和重建质量，解决了传统方法在浑浊水下环境中性能下降的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SWAGSplatting通过引入语义引导机制和分阶段训练策略，显著提高了水下场景的3D重建质量和保真度，有效解决了水下环境中光线扭曲、浑浊度和能见度有限等挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D reconstruction in underwater environments remains a complexchallenge due to issues such as light distortion, turbidity, and limitedvisibility. AI-based techniques have been applied to address these issues,however, existing methods have yet to fully exploit the potential of AI,particularly in integrating language models with visual processing. In thispaper, we propose a novel framework that leverages multimodal cross-knowledgeto create semantic-guided 3D Gaussian Splatting for robust and high-fidelitydeep-sea scene reconstruction. By embedding an extra semantic feature into eachGaussian primitive and supervised by the CLIP extracted semantic feature, ourmethod enforces semantic and structural awareness throughout the training. Thededicated semantic consistency loss ensures alignment with high-level sceneunderstanding. Besides, we propose a novel stage-wise training strategy,combining coarse-to-fine learning with late-stage parameter refinement, tofurther enhance both stability and reconstruction quality. Extensive resultsshow that our approach consistently outperforms state-of-the-art methods onSeaThru-NeRF and Submerged3D datasets across three metrics, with an improvementof up to 3.09 dB on average in terms of PSNR, making it a strong candidate forapplications in underwater exploration and marine perception.</description>
      <author>example@mail.com (Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai)</author>
      <guid isPermaLink="false">2509.00800v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.00789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniReason框架，通过联合建模动态3D环境及其底层决策过程，建立稳健的时空推理能力，解决了现有视觉-语言模型在自动驾驶中忽略时间维度的问题。&lt;h4&gt;背景&lt;/h4&gt;现有视觉-语言模型(VLMs)在自动驾驶方面展示了令人印象深刻的空间推理能力，但主要关注静态场景理解，忽略了真实驾驶场景中重要的时间维度。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽略时间维度的关键限制，提出能够进行稳健时空推理的框架，以提高自动驾驶系统在复杂动态环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出OmniReason-Data两个大规模视觉-语言-行动(VLA)数据集，具有密集时空注释和自然语言解释；开发幻觉减轻自动标注管道确保物理合理性和时间一致性；设计OmniReason-Agent架构，包含稀疏时间记忆模块和解释生成器；提出时空知识蒸馏方法捕捉时空因果推理模式。&lt;h4&gt;主要发现&lt;/h4&gt;OmniReason-Agent在开环规划任务和视觉问答(VQA)基准测试中取得显著改进，为在复杂动态环境中运行的、可解释的、具有时间意识的自动驾驶车辆建立了新能力。&lt;h4&gt;结论&lt;/h4&gt;OmniReason框架成功解决了现有方法在时空推理方面的局限性，通过联合建模动态环境和决策过程，提高了自动驾驶系统的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉-语言模型(VLMs)进展展示了自动驾驶中令人印象深刻的空间推理能力，但现有方法主要关注静态场景理解，同时忽略了真实驾驶场景中本质的时间维度。为解决这一关键限制，我们提出了OmniReason框架，通过联合建模动态3D环境及其底层决策过程，建立稳健的时空推理。我们的工作有两个基本进展：(1)我们引入了OmniReason-Data，两个具有密集时空注释和自然语言解释的大规模视觉-语言-行动(VLA)数据集，通过一种新颖的幻觉减轻自动标注管道生成，确保物理合理性和时间一致性；(2)我们开发了OmniReason-Agent架构，集成了一个稀疏时间记忆模块用于持久场景上下文建模和一个产生人类可解释决策理由的解释生成器，通过我们的时空知识蒸馏方法实现，该方法有效捕捉时空因果推理模式。全面的实验展示了最先进的性能，OmniReason-Agent在开环规划任务和视觉问答(VQA)基准测试中取得显著改进，同时为在复杂动态环境中运行的可解释、具有时间意识的自动驾驶车辆建立了新能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶系统中视觉语言模型忽视时间维度的问题，即现有方法主要关注静态场景理解，难以处理真实驾驶场景中的动态变化和因果关系。这个问题很重要，因为真实驾驶环境是动态变化的，系统需要理解时间上的因果关系；现有方法难以处理罕见的长尾事件，对高级场景语义理解不足，且在开放世界中缺乏自适应、可解释的推理能力，直接影响自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了端到端学习和视觉语言模型在自动驾驶中的应用与局限，借鉴了现有端到端学习方法、大型语言模型以及多种视觉语言数据集的工作。在此基础上，作者设计了两部分核心内容：一是OmniReason-Data数据集，通过规则基础和原则基础的模板结合大型语言模型生成高质量标注；二是OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，使用时空知识蒸馏捕捉因果推理模式。整个设计紧密围绕时空一致性和人类可解释性展开。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合建模动态3D环境及其底层决策过程来建立强大的时空推理能力，整合人类先验知识、场景感知信息和多模态输入。整体流程分为数据标注和模型实现两部分：数据标注方面，从现有数据集提取信息，应用精心设计的模板引导生成语言-动作对，再通过大型语言模型生成场景描述和因果链；模型实现方面，使用分层视觉编码器处理多视图输入，稀疏时间记忆模块聚合长程上下文，视觉语言模型核心处理特征并生成决策，最后通过知识蒸馏确保人类先验知识的有效整合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) OmniReason-Data两个大规模VLA数据集，具有密集时空标注和自然语言解释；2) OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器；3) 模板驱动的标注框架，自动生成高质量可解释的语言-动作对。相比之前工作，不同之处在于：现有数据集主要提供高级离散命令缺乏精确轨迹，而OmniReason提供更丰富的时空和因果信息；现有系统难以建模环境刺激与车辆反应间的因果关系，而OmniReason通过时空依赖而非表面相关性进行推理；明确设计了时间建模机制处理动态场景中的交互和运动关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了OmniReason框架，通过大规模时空标注的视觉语言动作数据集和集成稀疏时间记忆与因果推理的端到端模型，显著提升了自动驾驶系统在复杂动态环境中的时空推理能力和决策可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision-language models (VLMs) have demonstrated impressivespatial reasoning capabilities for autonomous driving, yet existing methodspredominantly focus on static scene understanding while neglecting theessential temporal dimension of real-world driving scenarios. To address thiscritical limitation, we propose the OmniReason framework, which establishesrobust spatiotemporal reasoning by jointly modeling dynamic 3D environments andtheir underlying decision-making processes. Our work makes two fundamentaladvances: (1) We introduce OmniReason-Data, two large-scalevision-language-action (VLA) datasets with dense spatiotemporal annotations andnatural language explanations, generated through a novelhallucination-mitigated auto-labeling pipeline that ensures both physicalplausibility and temporal coherence; (2) We develop the OmniReason-Agentarchitecture, which integrates a sparse temporal memory module for persistentscene context modeling and an explanation generator that produceshuman-interpretable decision rationales, facilitated by our spatiotemporalknowledge distillation approach that effectively captures spatiotemporal causalreasoning patterns. Comprehensive experiments demonstrate state-of-the-artperformance, where OmniReason-Agent achieves significant improvements in bothopen-loop planning tasks and visual question answering (VQA) benchmarks, whileestablishing new capabilities for interpretable, temporally-aware autonomousvehicles operating in complex, dynamic environments.</description>
      <author>example@mail.com (Pei Liu, Qingtian Ning, Xinyan Lu, Haipeng Liu, Weiliang Ma, Dangen She, Peng Jia, Xianpeng Lang, Jun Ma)</author>
      <guid isPermaLink="false">2509.00789v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph</title>
      <link>http://arxiv.org/abs/2509.00570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ConceptBot是一个结合大型语言模型和知识图谱的模块化机器人规划框架，能够生成可行且风险感知的计划，解决自然语言指令模糊和环境物体分析问题。&lt;h4&gt;背景&lt;/h4&gt;机器人系统通常面临缺乏常识推理的挑战，导致难以处理自然语言指令中的歧义，以及正确分析环境中存在的物体。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理自然语言指令歧义、准确分析环境物体，并生成可行且风险感知计划的机器人规划框架。&lt;h4&gt;方法&lt;/h4&gt;ConceptBot整合了三个主要模块：物体属性提取(OPE)模块使用ConceptNet的语义概念丰富场景理解；用户请求处理(URP)模块消除歧义并结构化指令；规划器生成上下文感知的可行拾取和放置策略。&lt;h4&gt;主要发现&lt;/h4&gt;在明确任务上达到100%成功率；在隐式任务上保持87%的准确率（SayCan为31%）；在风险感知任务上达到76%（SayCan为15%）；在材料分类（70%对比20%）和毒性检测（86%对比36%）等特定应用场景中优于SayCan；在SafeAgentBench上获得80%的总体得分（比最佳基线高出46%）。&lt;h4&gt;结论&lt;/h4&gt;ConceptBot能够在不需要领域特定训练的情况下泛化，并显著提高非结构化环境中机器人策略的可靠性，这些结果已在模拟和实验室实验中得到验证。&lt;h4&gt;翻译&lt;/h4&gt;ConceptBot是一个模块化机器人规划框架，它结合了大型语言模型和知识图谱，能够生成可行且风险感知的计划，尽管存在自然语言指令的模糊性以及正确分析环境中存在的物体等挑战，这些挑战通常源于缺乏常识推理。为此，ConceptBot集成了(i)物体属性提取(OPE)模块，使用ConceptNet中的语义概念丰富场景理解；(ii)用户请求处理(URP)模块，消除歧义并结构化指令；(iii)规划器，生成上下文感知的可行拾取和放置策略。在与Google SayCan的比较评估中，ConceptBot在明确任务上取得了100%的成功率，在隐式任务上保持了87%的准确率（SayCan为31%），在风险感知任务上达到了76%（对比15%），并在特定应用场景中优于SayCan，包括材料分类（70%对比20%）和毒性检测（86%对比36%）。在SafeAgentBench上，ConceptBot获得了80%的总体得分（比次佳基线高出46%）。这些在模拟和实验室实验中得到验证的结果，证明了ConceptBot无需领域特定训练即可泛化的能力，并显著提高了非结构化环境中机器人策略的可靠性。网站：https://sites.google.com/view/conceptbot&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; ConceptBot is a modular robotic planning framework that combines LargeLanguage Models and Knowledge Graphs to generate feasible and risk-aware plansdespite ambiguities in natural language instructions and correctly analyzingthe objects present in the environment - challenges that typically arise from alack of commonsense reasoning. To do that, ConceptBot integrates (i) an ObjectProperty Extraction (OPE) module that enriches scene understanding withsemantic concepts from ConceptNet, (ii) a User Request Processing (URP) modulethat disambiguates and structures instructions, and (iii) a Planner thatgenerates context-aware, feasible pick-and-place policies. In comparativeevaluations against Google SayCan, ConceptBot achieved 100% success on explicittasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),reached 76% on risk-aware tasks (versus 15%), and outperformed SayCan inapplication-specific scenarios, including material classification (70% vs. 20%)and toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved anoverall score of 80% (versus 46% for the next-best baseline). These results,validated in both simulation and laboratory experiments, demonstrateConceptBot's ability to generalize without domain-specific training and tosignificantly improve the reliability of robotic policies in unstructuredenvironments. Website: https://sites.google.com/view/conceptbot</description>
      <author>example@mail.com (Alessandro Leanza, Angelo Moroncelli, Giuseppe Vizzari, Francesco Braghin, Loris Roveda, Blerina Spahiu)</author>
      <guid isPermaLink="false">2509.00570v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</title>
      <link>http://arxiv.org/abs/2509.00210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VEME方法，一种新型跨模态对齐技术，通过学习以自我为中心、经验为中心的世界模型，增强深度学习模型在未知环境中的类人推理能力，特别是在复杂任务中的应用。&lt;h4&gt;背景&lt;/h4&gt;在具身智能领域，让深度学习模型在未知环境中执行复杂任务时实现类人推理仍然是一个关键挑战。先进的视觉语言模型在静态场景理解方面表现出色，但在时空推理和适应动态、开放集任务（如任务导向导航和具身问答）方面存在局限性，这是由于对细粒度时空线索和物理世界理解的建模不足。&lt;h4&gt;目的&lt;/h4&gt;解决视觉语言模型在时空推理和动态任务适应方面的局限性，增强模型在动态环境中的推理和规划能力，特别是在未知场景中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;VEME框架整合三个关键组件：(1) 跨模态对齐框架，连接物体、空间表示、视觉语义与时空线索，增强VLM的上下文学习能力；(2) 由世界嵌入激活的动态隐式认知地图，实现任务相关的几何语义记忆回忆；(3) 基于指令的导航和推理框架，利用具身先验进行长期规划和高效探索。&lt;h4&gt;主要发现&lt;/h4&gt;通过嵌入几何感知的时空经验体验，该方法显著提高了模型在动态环境中的推理和规划能力。实验表明，与传统方法相比，准确率和探索效率提高了1%至3%。&lt;h4&gt;结论&lt;/h4&gt;VEME方法通过跨模态对齐和动态认知地图，有效解决了视觉语言模型在时空推理和动态任务适应方面的局限性，为具身智能领域提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在未知环境中为复杂任务实现深度学习模型的类人推理仍然是具身智能中的一个关键挑战。虽然先进的视觉语言模型在静态场景理解方面表现出色，但由于对细粒度时空线索和物理世界理解的建模不足，它们在时空推理和适应动态、开放集任务（如任务导向导航和具身问答）方面的局限性仍然存在。为了解决这个问题，我们提出了VEME，一种新颖的跨模态对齐方法，通过学习以自我为中心、以经验为中心的世界模型来增强在未见场景中的泛化能力。我们的框架整合了三个关键组件：(1) 一个跨模态对齐框架，将物体、空间表示和视觉语义与时空线索连接起来，以增强VLM的上下文学习能力；(2) 一个由世界嵌入激活的动态隐式认知地图，能够实现与任务相关的几何语义记忆回忆；(3) 一个基于指令的导航和推理框架，利用具身先验进行长期规划和高效探索。通过嵌入几何感知的时空经验体验，我们的方法显著提高了在动态环境中的推理和规划能力。在VSI-Bench和VLN-CE上的实验结果表明，与传统方法相比，准确率和探索效率提高了1%至3%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型(VLMs)在复杂任务中的时空推理能力不足问题，特别是在动态开放环境下的任务导向导航和具身问答任务中表现不佳。这个问题很重要，因为它关系到AI系统如何像人类一样在真实世界中理解和导航，对于机器人导航、自动驾驶等实际应用至关重要，也是具身智能领域的核心挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从认知神经科学获取灵感，注意到人类空间智能成功来自于情景记忆和语义记忆的互补互动。他们识别出现有SLAM方法缺乏高级推理能力，而VLM方法无法捕捉细粒度时空依赖。因此设计了双记忆架构，借鉴了SLAM的空间表示能力、VLM的视觉语言理解能力，以及认知科学中关于人类记忆系统的研究，整合了跨模态对齐框架、动态隐式认知地图和基于指令的导航框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何-语义世界先验增强具身模型，建立双记忆系统(空间语义记忆和情景记忆)，将视觉语义与时空线索对齐。整体流程包括：1)处理语言指令、视觉图像、3D点云和动作历史等输入；2)构建世界嵌入作为认知地图；3)使用交叉注意力将2D语义与3D几何对齐；4)通过情景记忆形成独特经验表示；5)整合所有信息进行统一决策；6)端到端训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)认知启发的双记忆框架；2)有效的跨模态时空对齐技术；3)动态隐式认知地图；4)统一决策框架。相比之前工作，VEME不依赖显式3D地图构建(不同于SLAM)，增强了时空推理能力(优于传统VLM)，更紧密连接了视觉感知和空间理解(区别于其他多模态方法)，并结合了显式和隐式记忆的优点(超越单一记忆系统)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VEME通过引入受认知科学启发的双记忆系统和跨模态时空对齐机制，显著提升了具身智能模型在动态未知环境中的导航和推理能力，实现了3%-6%的准确率和探索效率提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving human-like reasoning in deep learning models for complex tasks inunknown environments remains a critical challenge in embodied intelligence.While advanced vision-language models (VLMs) excel in static sceneunderstanding, their limitations in spatio-temporal reasoning and adaptation todynamic, open-set tasks like task-oriented navigation and embodied questionanswering (EQA) persist due to inadequate modeling of fine-grainedspatio-temporal cues and physical world comprehension. To address this, wepropose VEME, a novel cross-modal alignment method that enhances generalizationin unseen scenes by learning an ego-centric, experience-centered world model.Our framework integrates three key components: (1) a cross-modal alignmentframework bridging objects, spatial representations, and visual semantics withspatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,implicit cognitive map activated by world embedding to enable task-relevantgeometric-semantic memory recall; and (3) an instruction-based navigation andreasoning framework leveraging embodied priors for long-term planning andefficient exploration. By embedding geometry-aware spatio-temporal episodicexperiences, our method significantly improves reasoning and planning indynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate1%-3% accuracy and exploration efficiency improvement compared to traditionalapproaches.</description>
      <author>example@mail.com (Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li)</author>
      <guid isPermaLink="false">2509.00210v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2509.01873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  175 pages, 66 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合传统几何技术与深度学习的几何深度学习方法，解决了3D视觉中的基本挑战，如相机姿态估计、点云配准、深度预测和3D重建。&lt;h4&gt;背景&lt;/h4&gt;现代深度学习为3D测绘技术、场景重建和虚拟现实创造了新机会，但3D数据的高维性和标记数据集的稀缺性限制了直接在3D数据上训练深度学习模型。传统SfM和SLAM技术在非结构化环境中表现不佳，难以生成对下游任务有效的详细几何表示。&lt;h4&gt;目的&lt;/h4&gt;开发针对3D视觉关键任务定制的几何深度学习方法，解决3D视觉中的基本挑战，提高几何表示的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将几何先验或约束(如深度信息、表面法线和等变性)整合到深度学习模型中，系统性地研究3D视觉的关键组成部分，包括相机姿态估计、点云配准、深度估计和高保真3D重建。&lt;h4&gt;主要发现&lt;/h4&gt;几何先验或约束的整合显著增强了几何表示的准确性和鲁棒性，该方法在数字文化遗产保护和沉浸式VR/AR环境等实际应用中展示了有效性。&lt;h4&gt;结论&lt;/h4&gt;需要开发结合传统几何技术和深度学习能力的3D表示方法，以生成具有几何感知能力的深度学习模型，解决当前3D视觉技术的局限性。&lt;h4&gt;翻译&lt;/h4&gt;现代深度学习的发展为3D测绘技术、场景重建流程和虚拟现实开发创造了新机会。尽管3D深度学习技术有所进步，但由于3D数据固有的高维性和标记数据集的稀缺性，直接在3D数据上训练深度学习模型面临挑战。运动结构(SfM)和同步定位与地图构建(SLAM)在应用于结构化室内环境时表现出强大的性能，但通常难以处理非结构化环境中的模糊特征。这些技术通常难以生成对渲染和语义分析等下游任务有效的详细几何表示。当前的限制需要开发结合传统几何技术与深度学习能力的3D表示方法，以生成具有几何感知能力的深度学习模型。该论文通过开发针对相机姿态估计、点云配准、深度预测和3D重建等关键任务定制的几何深度学习方法，解决了3D视觉中的基本挑战。将几何先验或约束(如包含深度信息、表面法线和等变性)整合到深度学习模型中，提高了几何表示的准确性和鲁棒性。本研究系统地研究了3D视觉的关键组成部分，包括相机姿态估计、点云配准、深度估计和高保真3D重建，展示了它们在数字文化遗产保护和沉浸式VR/AR环境等实际应用中的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉技术中的四个关键挑战：相机姿态预测、点云配准、深度估计和3D重建。这些问题在现实中非常重要，因为3D视觉技术对3D映射、场景重建和虚拟现实开发至关重要。直接在3D数据上训练深度学习模型面临高维性和标记数据稀缺的挑战，而传统方法如SfM和SLAM在非结构化环境中表现不佳，难以生成详细的几何表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用模块化方法，将3D视觉问题分解为可独立优化的子任务。每个子任务都结合了传统几何技术与深度学习：相机姿态预测结合ResNet和自适应粒子滤波器；点云配准引入SE(3)-等变性约束；深度估计使用Transformer处理任意长度焦点堆栈；3D重建结合小波变换特征和隐式SDF模型。作者确实借鉴了现有工作，如E2PN编码器、Transformer架构等，但进行了创新性改进，将几何先验知识整合到深度学习中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统几何约束或先验知识与深度学习模型相结合，创建更可靠、可扩展、高效和通用的3D视觉解决方案。整体流程包括：1)相机姿态预测：利用地平线和地面平面作为参考线索，结合自适应粒子滤波器融合视觉和IMU数据；2)点云配准：使用SE(3)-等变性2D高斯surfel特征，通过E2PN编码器处理点位置和方向；3)深度估计：采用Transformer-LSTM网络处理任意长度焦点堆栈；4)3D重建：利用小波变换深度特征条件化隐式SDF模型，通过三平面投影和UNet融合模块细化重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)相机姿态预测中利用自然几何线索增强稳定性；2)点云配准中引入基于2D Surfel的SE(3)-等变性框架；3)深度估计中提出可处理任意长度焦点堆栈的FocDepthFormer网络；4)3D重建中使用小波变换深度特征解决隐式模型难以捕获高频细节的问题。相比之前的工作，这些方法结合了传统几何约束与深度学习，采用模块化设计而非端到端方法，提高了灵活性、鲁棒性和多尺度特征处理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将传统几何约束与深度学习相结合，提出了一种模块化的几何深度学习方法，显著提高了3D视觉任务的准确性、鲁棒性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern deep learning developments create new opportunities for 3D mappingtechnology, scene reconstruction pipelines, and virtual reality development.Despite advances in 3D deep learning technology, direct training of deeplearning models on 3D data faces challenges due to the high dimensionalityinherent in 3D data and the scarcity of labeled datasets. Structure-from-motion(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robustperformance when applied to structured indoor environments but often strugglewith ambiguous features in unstructured environments. These techniques oftenstruggle to generate detailed geometric representations effective fordownstream tasks such as rendering and semantic analysis. Current limitationsrequire the development of 3D representation methods that combine traditionalgeometric techniques with deep learning capabilities to generate robustgeometry-aware deep learning models.  The dissertation provides solutions to the fundamental challenges in 3Dvision by developing geometric deep learning methods tailored for essentialtasks such as camera pose estimation, point cloud registration, depthprediction, and 3D reconstruction. The integration of geometric priors orconstraints, such as including depth information, surface normals, andequivariance into deep learning models, enhances both the accuracy androbustness of geometric representations. This study systematically investigateskey components of 3D vision, including camera pose estimation, point cloudregistration, depth estimation, and high-fidelity 3D reconstruction,demonstrating their effectiveness across real-world applications such asdigital cultural heritage preservation and immersive VR/AR environments.</description>
      <author>example@mail.com (Xueyang Kang)</author>
      <guid isPermaLink="false">2509.01873v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</title>
      <link>http://arxiv.org/abs/2509.03501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This technical report serves as the archival version of our paper  accepted at the ICCV 2025 Workshop. For more information, please visit our  project website: https://strefer.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Strefer，一个合成指令数据生成框架，旨在增强视频大型语言模型(Video LLMs)处理时空参考和推理的能力，使AI伴侣能更好地理解动态真实世界环境中的空间和时间关系。&lt;h4&gt;背景&lt;/h4&gt;下一代AI伴侣需要超越一般视频理解，解决动态真实世界环境中的时空参考问题。现有Video LLMs能进行粗粒度理解，但在细粒度时空推理方面存在困难，特别是当查询依赖时间锚定或手势提示进行空间锚定时。&lt;h4&gt;目的&lt;/h4&gt;开发Strefer框架，为Video LLMs配备时空参考和推理能力，弥补现有模型在处理时空关系方面的不足。&lt;h4&gt;方法&lt;/h4&gt;Strefer使用数据引擎生成多样化指令调整数据，通过伪注释时间密集的细粒度视频元数据，结构化捕获空间和时间信息，包括主体、对象、位置(masklet)及动作描述和时间线。&lt;h4&gt;主要发现&lt;/h4&gt;使用Strefer生成数据训练的模型在需要空间和时间消歧的任务上优于基线模型，并表现出增强的时空感知推理能力，为感知基础、指令调整的Video LLMs建立了新基础。&lt;h4&gt;结论&lt;/h4&gt;Strefer框架能有效增强Video LLMs解释空间和时间参考的能力，促进更通用、时空感知的推理，这对真实世界AI伴侣至关重要，且无需专有模型、昂贵人工注释或大量新视频注释。&lt;h4&gt;翻译&lt;/h4&gt;下一代AI伴侣必须超越一般视频理解，以解决动态、真实世界环境中的时空参考问题。现有的视频大型语言模型(Video LLMs)虽然能够进行粗粒度理解，但在细粒度时空推理方面存在困难，特别是当用户查询依赖于基于时间的事件参考进行时间锚定，或依赖于手势提示进行空间锚定以明确对象引用和位置时。为了解决这一关键差距，我们引入了Strefer，一个合成指令数据生成框架，旨在为Video LLMs配备时空参考和推理能力。Strefer使用数据引擎生成多样化的指令调整数据，该引擎伪注释时间密集的、细粒度的视频元数据，以结构化方式捕获丰富的空间和时间信息，包括主体、对象、它们的位置（作为masklet）以及它们的动作描述和时间线。我们的方法增强了Video LLMs解释空间和时间参考的能力，促进更通用、时空感知的推理，这对真实世界的AI伴侣至关重要。在不使用专有模型、昂贵的人工注释或大量新视频注释的情况下，实验评估表明，使用Strefer生成的数据训练的模型在需要空间和时间消歧的任务上优于基线模型。此外，这些模型表现出增强的时空感知推理能力，为感知基础、指令调整的Video LLMs建立了新的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation AI companions must go beyond general video understanding toresolve spatial and temporal references in dynamic, real-world environments.Existing Video Large Language Models (Video LLMs), while capable ofcoarse-level comprehension, struggle with fine-grained, spatiotemporalreasoning, especially when user queries rely on time-based event references fortemporal anchoring, or gestural cues for spatial anchoring to clarify objectreferences and positions. To bridge this critical gap, we introduce Strefer, asynthetic instruction data generation framework designed to equip Video LLMswith spatiotemporal referring and reasoning capabilities. Strefer producesdiverse instruction-tuning data using a data engine that pseudo-annotatestemporally dense, fine-grained video metadata, capturing rich spatial andtemporal information in a structured manner, including subjects, objects, theirlocations as masklets, and their action descriptions and timelines. Ourapproach enhances the ability of Video LLMs to interpret spatial and temporalreferences, fostering more versatile, space-time-aware reasoning essential forreal-world AI companions. Without using proprietary models, costly humanannotation, or the need to annotate large volumes of new videos, experimentalevaluations show that models trained with data produced by Strefer outperformbaselines on tasks requiring spatial and temporal disambiguation. Additionally,these models exhibit enhanced space-time-aware reasoning, establishing a newfoundation for perceptually grounded, instruction-tuned Video LLMs.</description>
      <author>example@mail.com (Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles)</author>
      <guid isPermaLink="false">2509.03501v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Time-Scaling State-Space Models for Dense Video Captioning</title>
      <link>http://arxiv.org/abs/2509.03426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的状态空间模型方法，用于解决密集视频描述任务中处理长视频的挑战，实现了高效的在线/流式处理能力。&lt;h4&gt;背景&lt;/h4&gt;密集视频描述是一个需要同时分割视频为有意义事件序列并生成描述的挑战性任务。现有方法在处理长视频时面临计算复杂性和内存限制问题，且传统方法需要完整视频输入，无法实现在线处理。&lt;h4&gt;目的&lt;/h4&gt;解决密集视频描述中处理长视频的挑战，实现视频的在线或流式处理，无需等待整个视频处理完成。&lt;h4&gt;方法&lt;/h4&gt;提出'具有传输状态的状态空间模型'，结合状态空间模型的长序列和循环特性，解决了状态空间模型无法在长上下文中维持状态的问题，有效扩展了模型的时间处理能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提模型适合在线/流式方式即时生成描述，无需等待完整视频处理。应用于密集视频描述时，该方法能很好地适应不同视频长度，计算效率提高，使用少7倍的浮点运算次数。&lt;h4&gt;结论&lt;/h4&gt;通过改进状态空间模型，成功解决了密集视频描述中处理长视频的挑战，实现了高效在线处理，并在计算效率上取得显著提升。&lt;h4&gt;翻译&lt;/h4&gt;密集视频描述是一个具有挑战性的视频理解任务，旨在同时将视频分割为有意义的连续事件序列，并为每个事件生成详细准确的描述。由于计算复杂性和内存限制，现有方法在处理与密集视频描述相关的长视频时常常遇到困难。此外，传统方法需要整个视频作为输入才能产生输出，这阻碍了视频的在线处理。我们通过时间扩展状态空间模型来应对这些挑战，使其能够处理比以往更长的序列。我们的方法'具有传输状态的状态空间模型'结合了状态空间模型的长序列和循环特性，并解决了状态空间模型的主要限制，即无法在非常长的上下文中维持状态，有效地进一步扩展了状态空间模型的时间范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense video captioning is a challenging video understanding task which aimsto simultaneously segment the video into a sequence of meaningful consecutiveevents and to generate detailed captions to accurately describe each event.Existing methods often encounter difficulties when working with the long videosassociated with dense video captioning, due to the computational complexity andmemory limitations. Furthermore, traditional approaches require the entirevideo as input, in order to produce an answer, which precludes onlineprocessing of the video. We address these challenges by time-scalingState-Space Models (SSMs) to even longer sequences than before. Our approach,State-Space Models with Transfer State, combines both the long-sequence andrecurrent properties of SSMs and addresses the main limitation of SSMs whichare otherwise not able to sustain their state for very long contexts,effectively scaling SSMs further in time. The proposed model is particularlysuitable for generating captions on-the-fly, in an online or streaming manner,without having to wait for the full video to be processed, which is morebeneficial in practice. When applied to dense video captioning, our approachscales well with video lengths and uses 7x fewer FLOPs.</description>
      <author>example@mail.com (AJ Piergiovanni, Ganesh Satish Mallya, Dahun Kim, Anelia Angelova)</author>
      <guid isPermaLink="false">2509.03426v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling</title>
      <link>http://arxiv.org/abs/2509.03260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出HyPV-LEAD框架，一种数据驱动的加密货币异常交易早期预警系统，通过整合窗口-地平线建模、峰值-谷值采样和双曲嵌入三种创新技术，显著提升了异常检测性能。&lt;h4&gt;背景&lt;/h4&gt;异常加密货币交易（如混币服务、欺诈转账、拉高出货操作）对金融完整性构成日益增长的风险，但由于类别不平衡、时间波动性和复杂网络依赖性，这些异常交易难以检测。现有方法主要是事后的，只在异常发生后标记，预防价值有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种数据驱动的早期预警框架，明确将提前时间纳入异常检测，实现从反应式分类到主动式早期预警的转变。&lt;h4&gt;方法&lt;/h4&gt;HyPV-LEAD框架整合三项创新：(1)窗口-地平线建模确保可操作的提前时间警报；(2)峰值-谷值采样减轻类别不平衡同时保持时间连续性；(3)双曲嵌入捕获区块链交易网络的层次化和无标度特性。&lt;h4&gt;主要发现&lt;/h4&gt;在比特币交易数据上的实证评估显示，HyPV-LEAD实现了0.9624的PR-AUC，精确度和召回率显著优于最先进基线。消融研究证实每个组件提供互补益处，完整框架性能最佳。&lt;h4&gt;结论&lt;/h4&gt;HyPV-LEAD通过将异常检测从反应式转变为主动式早期预警，为实时风险管理、反洗钱合规和动态区块链环境中的金融安全提供了坚实基础。&lt;h4&gt;翻译&lt;/h4&gt;异常加密货币交易 - 如混币服务、欺诈转账和拉高出货操作 - 对金融完整性构成日益增长的风险，但由于类别不平衡、时间波动性和复杂的网络依赖性，这些交易仍然难以检测。现有方法主要是以模型为中心和事后的，只在异常发生后标记，因此提供的预防价值有限。本文引入HyPV-LEAD（双曲峰谷提前时间启用的异常检测），一种数据驱动的早期预警框架，明确将提前时间纳入异常检测。与先前方法不同，HyPV-LEAD整合了三项创新：(1)窗口-地平线建模确保可操作的提前时间警报，(2)峰谷采样减轻类别不平衡同时保持时间连续性，(3)双曲嵌入捕获区块链交易网络的层次化和无标度特性。在比特币交易数据上的实证评估表明，HyPV-LEAD持续优于最先进的基线方法，实现了0.9624的PR-AUC，精确度和召回率有显著提升。消融研究进一步证实每个组件 - PV采样、双曲嵌入和结构-时间建模 - 提供互补的益处，完整框架实现最高性能。通过将异常检测从反应式分类转变为主动式早期预警，HyPV-LEAD为动态区块链环境中的实时风险管理、反洗钱合规和金融安全建立了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Abnormal cryptocurrency transactions - such as mixing services, fraudulenttransfers, and pump-and-dump operations -- pose escalating risks to financialintegrity but remain notoriously difficult to detect due to class imbalance,temporal volatility, and complex network dependencies. Existing approaches arepredominantly model-centric and post hoc, flagging anomalies only after theyoccur and thus offering limited preventive value. This paper introducesHyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), adata-driven early-warning framework that explicitly incorporates lead time intoanomaly detection. Unlike prior methods, HyPV-LEAD integrates threeinnovations: (1) window-horizon modeling to guarantee actionable lead-timealerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance whilepreserving temporal continuity, and (3) hyperbolic embedding to capture thehierarchical and scale-free properties of blockchain transaction networks.Empirical evaluation on large-scale Bitcoin transaction data demonstrates thatHyPV-LEAD consistently outperforms state-of-the-art baselines, achieving aPR-AUC of 0.9624 with significant gains in precision and recall. Ablationstudies further confirm that each component - PV sampling, hyperbolicembedding, and structural-temporal modeling - provides complementary benefits,with the full framework delivering the highest performance. By shifting anomalydetection from reactive classification to proactive early-warning, HyPV-LEADestablishes a robust foundation for real-time risk management, anti-moneylaundering (AML) compliance, and financial security in dynamic blockchainenvironments.</description>
      <author>example@mail.com (Minjung Park, Gyuyeon Na, Soyoun Kim, Sunyoung Moon, HyeonJeong Cha, Sangmi Chai)</author>
      <guid isPermaLink="false">2509.03260v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning</title>
      <link>http://arxiv.org/abs/2509.02958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了格注释时态逻辑(LAT Logic)，这是广义注释逻辑程序(GAPs)的扩展，结合了时间推理并通过使用下格结构支持开放世界语义。该逻辑将高效的演绎过程与时态逻辑编程相结合，支持非马尔可夫关系和开放世界推理能力。&lt;h4&gt;背景&lt;/h4&gt;在逻辑编程和知识表示领域，需要处理开放世界语义和时间推理的挑战，特别是在具有无限或高度多样化常量的领域中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合高效演绎过程与时态逻辑编程的逻辑系统，以支持非马尔可夫关系和开放世界推理能力。&lt;h4&gt;方法&lt;/h4&gt;通过使用下格注释结构实现开放世界方面，并通过Skolemization过程实现高效grounding，即使在具有无限或高度多样化常量的领域中也是如此。&lt;h4&gt;主要发现&lt;/h4&gt;1) 提供了限制grounding过程计算复杂度的理论结果；2) 证明许多关于GAPs(使用上格)的结果在下格和时间扩展下仍然成立；3) 开源实现PyReason具有模块化设计、机器级优化和与强化学习环境的直接集成；4) 在多智能体模拟和知识图任务上显示速度提高最多三个数量级，内存减少最多五个数量级；5) 在强化学习环境中作为非马尔可夫模拟器，模拟速度提高最多三个数量级，智能体性能提高26%。&lt;h4&gt;结论&lt;/h4&gt;LAT Logic有潜力成为动态和不确定环境中开放世界时间推理的统一、可扩展框架。&lt;h4&gt;翻译&lt;/h4&gt;我们引入格注释时态(LAT)逻辑，这是广义注释逻辑程序(GAPs)的扩展，它结合了时间推理并通过使用下格结构支持开放世界语义。该逻辑将高效的演绎过程与时态逻辑编程相结合，以支持非马尔可夫关系和开放世界推理能力。开放世界方面是使用下格注释结构的结果，允许通过Skolemization过程实现高效grounding，即使在具有无限或高度多样化常量的领域中也是如此。我们提供了一套理论结果，限制了grounding过程的计算复杂度，此外还表明许多关于GAPs(使用上格)的结果在下格和时间扩展下仍然成立(尽管需要不同的证明技术)。我们的开源实现PyReason具有模块化设计、机器级优化和与强化学习环境的直接集成。在多智能体模拟和知识图任务上的经验评估显示速度提高最多三个数量级，内存减少最多五个数量级，同时保持或提高任务性能。此外，我们评估了LAT Logic在强化学习环境中作为非马尔可夫模拟器的价值，实现了最多三个数量级的更快速模拟，并提高了智能体性能，包括通过捕获更丰富的时间依赖性提高26%的胜率。这些结果突显了LAT Logic作为动态和不确定环境中开放世界时间推理的统一、可扩展框架的潜力。我们的实现可在pyreason.syracuse.edu获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Lattice Annotated Temporal (LAT) Logic, an extension ofGeneralized Annotated Logic Programs (GAPs) that incorporates temporalreasoning and supports open-world semantics through the use of a lower latticestructure. This logic combines an efficient deduction process with temporallogic programming to support non-Markovian relationships and open-worldreasoning capabilities. The open-world aspect, a by-product of the use of thelower-lattice annotation structure, allows for efficient grounding through aSkolemization process, even in domains with infinite or highly diverseconstants.  We provide a suite of theoretical results that bound the computationalcomplexity of the grounding process, in addition to showing that many of theresults on GAPs (using an upper lattice) still hold with the lower lattice andtemporal extensions (though different proof techniques are required). Ouropen-source implementation, PyReason, features modular design, machine-leveloptimizations, and direct integration with reinforcement learning environments.Empirical evaluations across multi-agent simulations and knowledge graph tasksdemonstrate up to three orders of magnitude speedup and up to five orders ofmagnitude memory reduction while maintaining or improving task performance.Additionally, we evaluate LAT Logic's value in reinforcement learningenvironments as a non-Markovian simulator, achieving up to three orders ofmagnitude faster simulation with improved agent performance, including a 26%increase in win rate due to capturing richer temporal dependencies. Theseresults highlight LAT Logic's potential as a unified, extensible framework foropen-world temporal reasoning in dynamic and uncertain environments. Ourimplementation is available at: pyreason.syracuse.edu.</description>
      <author>example@mail.com (Kaustuv Mukherji, Jaikrishna Manojkumar Patil, Dyuman Aditya, Paulo Shakarian, Devendra Parkar, Lahari Pokala, Clark Dorman, Gerardo I. Simari)</author>
      <guid isPermaLink="false">2509.02958v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</title>
      <link>http://arxiv.org/abs/2509.02807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work under review in NeurIPS 2025 with the title "Are we using Motion  in Referring Segmentation? A Motion-Centric Evaluation"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多模态大语言模型(MLLMs)在视频领域的像素级视觉定位能力，特别是它们如何利用运动信息进行物体分割。作者提出了以运动为中心的基准测试和评估方法，挑战现有模型改进密集时空定位能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在图像和文本任务中表现出色，扩展到视频领域后支持了视频问答和字幕生成等任务，但其像素级视觉定位能力研究不足。&lt;h4&gt;目的&lt;/h4&gt;研究运动是否被用于像素级视觉定位，以及视频MLLMs是否能根据描述运动模式的自然语言表达式分割物体；解决当前基准测试中单帧图像即可完成任务无需时间推理的不足。&lt;h4&gt;方法&lt;/h4&gt;引入四种以运动为中心的探测技术；创建MoCentric-Bench基准测试；建立强大的单图像基线；探索简单的以运动为中心的适应技术。&lt;h4&gt;主要发现&lt;/h4&gt;当前基准测试存在不足，单帧通常足以捕捉运动指代表达；视频MLLMs在识别真实运动和把握运动顺序方面能力待评估；以运动为中心的基准测试能确保模型评估运动与语言交互而非静态外观线索。&lt;h4&gt;结论&lt;/h4&gt;以运动为中心的基准测试、评估和发现挑战未来模型改进密集时空定位和视频中的像素级理解；代码和数据集将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)在使用图像和文本模态的任务中展现出令人印象深刻的泛化能力。尽管它们扩展到视频领域已 enable了视频问答和视频字幕生成等任务，但其像素级视觉定位能力研究较少。在这项工作中，我们提出了一个相关问题：运动是否被用于像素级视觉定位，以及视频MLLMs是否能根据描述运动模式的自然语言表达式分割物体。我们确定了当前基准测试的不足之处，并表明单帧图像通常足以捕捉运动指代表达式，无需任何时间推理。为解决这一问题，我们引入了四种以运动为中心的探测技术，专为视觉定位任务设计，以研究视频MLLMs识别真实运动与假运动的能力以及把握运动顺序的能力。因此，我们提供了一个以运动为中心的基准测试MoCentric-Bench。它确保视频MLLMs被评估为利用运动和语言之间的交互，而不是被现有视觉定位数据集中强调的静态外观线索所主导。我们进一步建立了强大的单图像基线，其性能与或优于之前的方法。最后，我们探索了简单的以运动为中心的适应技术，在我们的MoCentric-Bench上提供了最先进的性能。我们的以运动为中心的基准测试、评估和发现挑战未来的模型改进密集时空定位和视频中的像素级理解。代码和数据集将在https://github.com/MSiam/PixFoundation-2.0.git公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal large language models (MLLMs) have shown impressivegeneralization across tasks using images and text modalities. While theirextension to video has enabled tasks such as video question answering and videocaptioning, their pixel-level visual grounding abilities are less studied. Inthis work, we raise the pertinent question of whether motion is used inpixel-level visual grounding and whether video MLLMs can segment objects basedon natural language expressions describing their motion patterns. We identifythe shortcomings in the current benchmarks, where we show that a single framecan often suffice for capturing the motion referring expression without anytemporal reasoning. To address this, we introduce four motion-centric probingtechniques, particularly designed for the visual grounding task, to study videoMLLMs' ability to identify true motion from a fake one and their ability tograsp the motion order. Consequently, we provide a motion-centric benchmark,MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveragingthe interaction between motion and language rather than being dominated bystatic appearance cues emphasized in existing visual grounding datasets. Wefurther establish strong single-image baselines that are on par with oroutperform prior methods. Finally, we explore simple motion-centric adaptationtechniques that provide state-of-the-art performance on our MoCentric-Bench.Our motion-centric benchmark, evaluation and findings challenge future modelsto improve dense spatiotemporal grounding and pixel-level understanding withinvideos. Code and datasets will be made publicly available athttps://github.com/MSiam/PixFoundation-2.0.git.</description>
      <author>example@mail.com (Mennatullah Siam)</author>
      <guid isPermaLink="false">2509.02807v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</title>
      <link>http://arxiv.org/abs/2509.02028v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 1 figure, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了引用多对象跟踪(RMOT)系统的安全性和鲁棒性问题，发现了对抗性漏洞并提出了一种名为VEIL的新型对抗框架来破坏RMOT模型的可靠性。&lt;h4&gt;背景&lt;/h4&gt;语言视觉理解推动了高级感知系统的发展，特别是引用多对象跟踪(RMOT)范式的出现。RMOT系统利用自然语言查询选择性跟踪满足语义描述的对象，通过基于Transformer的时空推理模块引导。端到端RMOT模型将特征提取、时间记忆和空间推理统一在Transformer主干网络中，实现长程时空建模。&lt;h4&gt;目的&lt;/h4&gt;从设计逻辑角度研究RMOT系统的安全含义，识别对抗性漏洞，提出保护RMOT系统可靠性的方法。&lt;h4&gt;方法&lt;/h4&gt;提出VEIL，一种旨在破坏RMOT模型统一引用-匹配机制的新型对抗框架。通过精心设计的数字和物理扰动来破坏跟踪逻辑的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;1) RMOT系统存在对抗性漏洞，损害语言视觉引用和跟踪对象匹配组件；2) 基于FIFO记忆的高级RMOT模型中存在新型漏洞，针对时空推理的攻击会在历史缓冲区中引入持续多个帧的错误；3) VEIL框架可以有效导致跟踪ID切换和终止。&lt;h4&gt;结论&lt;/h4&gt;RMOT系统的可靠性和鲁棒性需要更多关注，关键的大规模应用需要安全感知的RMOT设计。&lt;h4&gt;翻译&lt;/h4&gt;语言视觉理解推动了高级感知系统的发展，特别是新兴的引用多对象跟踪(RMOT)范式。通过利用自然语言查询，RMOT系统可以选择性地跟踪满足给定语义描述的对象，通过基于Transformer的时空推理模块进行引导。端到端(E2E)RMOT模型进一步将特征提取、时间记忆和空间推理统一在一个Transformer主干网络中，实现了对融合的文本-视觉表示的长程时空建模。尽管取得了这些进展，RMOT的可靠性和鲁棒性仍然未被充分探索。在本文中，我们从设计逻辑的角度研究了RMOT系统的安全含义，确定了会损害语言视觉引用和跟踪对象匹配组件的对抗性漏洞。此外，我们还发现了采用基于FIFO记忆的高级RMOT模型中的一种新型漏洞，即对其时空推理的有针对性的持续攻击会在历史缓冲区中引入持续多个帧的错误。我们提出了VEIL，一种旨在破坏RMOT模型统一引用-匹配机制的新型对抗框架。我们展示了精心设计的数字和物理扰动可以破坏跟踪逻辑的可靠性，导致跟踪ID切换和终止。我们使用Refer-KITTI数据集进行了全面评估，验证了VEIL的有效性，并证明了关键的大规模应用需要安全感知的RMOT设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language-vision understanding has driven the development of advancedperception systems, most notably the emerging paradigm of ReferringMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOTsystems can selectively track objects that satisfy a given semanticdescription, guided through Transformer-based spatial-temporal reasoningmodules. End-to-End (E2E) RMOT models further unify feature extraction,temporal memory, and spatial reasoning within a Transformer backbone, enablinglong-range spatial-temporal modeling over fused textual-visual representations.Despite these advances, the reliability and robustness of RMOT remainunderexplored. In this paper, we examine the security implications of RMOTsystems from a design-logic perspective, identifying adversarialvulnerabilities that compromise both the linguistic-visual referring andtrack-object matching components. Additionally, we uncover a novelvulnerability in advanced RMOT models employing FIFO-based memory, wherebytargeted and consistent attacks on their spatial-temporal reasoning introduceerrors that persist within the history buffer over multiple subsequent frames.We present VEIL, a novel adversarial framework designed to disrupt the unifiedreferring-matching mechanisms of RMOT models. We show that carefully crafteddigital and physical perturbations can corrupt the tracking logic reliability,inducing track ID switches and terminations. We conduct comprehensiveevaluations using the Refer-KITTI dataset to validate the effectiveness of VEILand demonstrate the urgent need for security-aware RMOT designs for criticallarge-scale applications.</description>
      <author>example@mail.com (Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque)</author>
      <guid isPermaLink="false">2509.02028v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Spatio-Temporal Sequential Ordinal Models: Application to Invasive Weeds</title>
      <link>http://arxiv.org/abs/2509.01976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究多元序贯序数模型在贝叶斯分析时空序数数据中的应用及其优势&lt;h4&gt;背景&lt;/h4&gt;时空序数数据分析面临估计复杂性的挑战，特别是高维应用中累积序数模型的阈值参数排序约束问题&lt;h4&gt;目的&lt;/h4&gt;开发一种简化高维时空序数数据估计和预测的模型，避免累积序数模型的排序约束&lt;h4&gt;方法&lt;/h4&gt;使用多元序贯序数模型，将其似然表达为条件于未知回归系数和时空随机效应的二值模型，结合动态广义线性模型框架和INLA方法进行估计和预测&lt;h4&gt;主要发现&lt;/h4&gt;四种入侵杂草物种对栖息地类型、控制努力和可达性的响应不同，且具有相似的有效空间范围短和强时间自相关的依赖性&lt;h4&gt;结论&lt;/h4&gt;序贯序数模型简化了高维时空序数数据的贝叶斯分析，同时保持了参数的可解释性，适用于空间稀疏分布和时间记录较少的序数数据&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了多元序贯序数模型在贝叶斯分析时空序数数据中的应用。序贯序数模型似然等同于条件于未知回归系数和时空随机效应的二值模型。因此，时空背景下的估计和预测可以使用成熟的动态广义线性模型框架进行。此外，序贯序数模型避免了决定类别断点的累积序数模型所需的阈值参数排序约束，从而简化了使用贝叶斯推断进行高维时空应用的估计过程。动态时空序贯序数模型被应用于估计四种积极管理的入侵外来物种的叶覆盖丰度。这些入侵杂草物种通过植被研究中常用的修改Braun-Blanquet评分进行观察，构成序数数据。管理杂草物种的多变量序数数据在时空上稀疏分布，高叶覆盖类别的观测记录较少。因此，开发了针对时空依赖的可分离模型，在存在聚合序数类别的情况下保持参数可解释性。使用为单变量时空模型开发的集成嵌套拉普拉斯近似(INLA)方法进行估计和预测。贝叶斯估计和预测表明，四种入侵杂草物种对栖息地类型、控制努力和可达性的响应不同，且具有相似的有效空间范围短和强时间自相关的依赖性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multivariate sequential ordinal model is investigated for use in theBayesian analysis of spatio-temporal ordinal data. The sequential ordinal modellikelihood is equivalent to a binary model conditional on unknown regressioncoefficients and spatio-temporal random effects. Therefore, estimation andprediction in the space-time context can proceed using the well-establisheddynamic generalised linear model framework. Moreover, the sequential ordinalmodel avoids the ordering constraints on the threshold parameters thatdetermine the category break points required by cumulative ordinal models, andso simplifies the estimation procedure for high-dimensional space-timeapplications using Bayesian inference. The dynamic spatio-temporal sequentialordinal model is applied to estimate foliage cover abundance of four activelymanaged invasive alien species. These invasive weed species are observed bymeans of a modified Braun-Blanquet score that is commonly used in vegetationstudies and constitutes ordinal data. The multivariate ordinal data for themanaged weeds species are sparsely distributed in space and time with fewobservations recorded in high foliage cover categories. A separable model forspace-time dependence that maintains parameter interpretability in the presenceof aggregated ordinal categories is therefore developed. Estimation andprediction is demonstrated using integrated nested Laplace approximation (INLA)methods developed for univariate spatio-temporal models. Bayesian estimationand prediction shows that the four invasive weed species differentially respondto habitat type, control effort and accessibility, and share similar magnitudesof dependence with short effective spatial ranges and strong temporalautocorrelations.</description>
      <author>example@mail.com (Geoffrey R. Hosack, Wen-Hsi Yang, Kyana N. Pike, Luke S. O'Loughlin, Emma Cook, Brett Howland, Emily Sutcliffe, Richard N. C. Milner, Ben Gooden, Jens G. Froese)</author>
      <guid isPermaLink="false">2509.01976v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Time-Series Model for Areal Data Using Spatially Correlated Gaussian Processes</title>
      <link>http://arxiv.org/abs/2509.01604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的层次建模框架，通过高斯过程和相关方差分量共享空间信息来捕捉区域数据中的时间趋势，提供更灵活的时空过程表示。&lt;h4&gt;背景&lt;/h4&gt;传统的时空模型通常先在随机效应层面施加空间结构，然后再扩展到包含时间动态，存在一定的局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代的层次建模框架，通过共享空间信息的相关方差分量，用高斯过程捕捉区域数据中的时间趋势。&lt;h4&gt;方法&lt;/h4&gt;将独立高斯过程模型扩展到空间相关框架，对控制时间变异性的参数施加条件自回归先验，并对时间范围施加时间方差的条件依赖；在贝叶斯框架内使用近似后验采样，结合马尔可夫链蒙特卡洛技术(MALA、Metropolis-Hastings和Gibbs采样)进行推断。&lt;h4&gt;主要发现&lt;/h4&gt;在莫桑比克Niassa的月疟疾发病率和喀麦隆的周粮食不安全流行率两个案例研究中，模型表现出强大的样本内性能，置信区间狭窄，在预测时在许多区域优于已建立的时空方法，同时保持可解释性。&lt;h4&gt;结论&lt;/h4&gt;通过考虑时间动态本身的演变来解释时空变化，该方法为许多应用背景提供了一种灵活且合理的工具，特别适用于数据稀少且具有政策相关性的环境。&lt;h4&gt;翻译&lt;/h4&gt;传统的区域数据时空模型通常从在随机效应层面施加空间结构开始，然后扩展到包含时间动态。我们提出了一种替代的层次建模框架，通过共享空间信息的相关方差分量，用高斯过程捕捉区域数据中的时间趋势。这使得模型能够更好地捕捉区域间共享的变异性模式，同时保留局部时间动态，为时空过程提供了更灵活的表示。具体来说，我们通过对控制时间变异性的参数施加条件自回归先验，并对时间范围施加时间方差的条件依赖，将独立的高斯过程时间序列模型扩展到空间相关框架。我们将该模型应用于两个案例研究：莫桑比克Niassa的月疟疾发病率和喀麦隆的周粮食不安全流行率。在贝叶斯框架内使用近似后验采样进行推断。鉴于模型的层次结构，我们采用马尔可夫链蒙特卡洛技术的组合，包括Metropolis调整的Langevin算法、Metropolis-Hastings和Gibbs采样。在两个应用中，该模型都显示出强大的样本内性能，置信区间狭窄，并且在预测时在许多区域优于已建立的时空方法。这些结果强调了模型在保持可解释性的同时捕捉复杂时空依赖关系的能力，这在数据稀少且具有政策相关性的环境中至关重要。通过考虑时间动态本身的演变来解释时空变化，我们的方法为许多应用背景提供了一种灵活且合理的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional spatio-temporal models for areal data typically begin withspatial structure imposed at the level of random effects and later extend toinclude temporal dynamics. We propose an alternative hierarchical modelingframework that captures temporal trends in areal data through Gaussianprocesses that share spatial information via correlated variance components.This allows the model to better capture shared patterns of variability acrossregions while preserving local temporal dynamics, offering a more flexiblerepresentation of spatio-temporal processes.  Specifically, we extend independent Gaussian-process models for time-seriesdata to a spatially correlated framework by placing a conditionalautoregressive (CAR) prior on the parameters governing the temporal variabilityand imposing a conditional dependence of the temporal range on the temporalvariance. We apply this model to two case studies: monthly malaria incidence inNiassa, Mozambique, and weekly food insecurity prevalence in Cameroon.Inference is conducted within a Bayesian framework using approximate posteriorsampling. Given the hierarchical structure of the model, we employ acombination of Markov chain Monte Carlo (MCMC) techniques, including theMetropolis-adjusted Langevin algorithm (MALA), Metropolis-Hastings, and Gibbssampling.  In both applications, the model demonstrates strong in-sample performancewith narrow credible intervals and outperforms established spatio-temporalapproaches in many regions when forecasting. These results underscore themodel's ability to capture complex spatio-temporal dependencies whilemaintaining interpretability, key in settings with sparse data and policyrelevance. By accounting for spatio-temporal variation through the evolution oftemporal dynamics themselves, our approach offers a flexible and principledtool for many applied contexts.</description>
      <author>example@mail.com (Alejandro Rozo Posada, Oswaldo Gressani, Christel Faes, James Colborn, Baltazar Candrinho, Emanuele Giorgi, Thomas Neyens)</author>
      <guid isPermaLink="false">2509.01604v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Keye-VL-1.5模型，通过三项创新解决了视频理解中的基本挑战，在视频理解任务上表现出色，同时保持通用多模态任务的竞争性能。&lt;h4&gt;背景&lt;/h4&gt;大语言模型近年来发展迅速并扩展到多模态任务，但视频理解仍具挑战性，因为视频具有动态和信息密集特性。现有模型在处理视频内容时难以平衡空间分辨率和时间覆盖范围。&lt;h4&gt;目的&lt;/h4&gt;开发Keye-VL-1.5模型，解决视频理解中的基本挑战，提高模型在视频理解和多模态任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 引入慢-快视频编码策略，根据帧间相似性动态分配计算资源，关键帧以高分辨率处理，静态帧以低分辨率但增加时间覆盖范围处理；2. 实现渐进式四阶段预训练方法，将上下文长度从8K扩展到128K tokens；3. 开发全面微调管道，包含思维链数据构建、GSPO强化学习和对齐训练。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试和内部人工评估中，Keye-VL-1.5比现有模型有显著改进，尤其在视频理解任务上表现突出，同时在通用多模态基准测试上保持竞争性能。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三项关键创新有效解决了视频理解的基本挑战，在视频理解和通用多模态任务上都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展取得了显著进展，通过多模态大语言模型将其能力扩展到多模态任务。然而，由于视频的动态和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，在空间分辨率和时间覆盖范围之间难以权衡。我们提出了Keye-VL-1.5，通过三项关键创新解决了视频理解中的基本挑战。首先，我们引入了一种新颖的慢-快视频编码策略，根据帧间相似性动态分配计算资源，对有显著视觉变化的关键帧以更高分辨率处理，同时对相对静态的帧以更低分辨率但增加时间覆盖范围处理。其次，我们实现了一个渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，使模型能够处理更长的视频和更复杂的视觉内容。第三，我们开发了一个全面的微调管道，专注于推理增强和人类偏好对齐，包含5步思维链数据构建过程、基于GSPO的迭代强化学习以及对齐训练。通过在公共基准测试上的广泛评估和严格的内部人工评估，Keye-VL-1.5比现有模型显示出显著改进，尤其在视频理解任务上表现出色，同时在通用多模态基准测试上保持竞争性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Variation-aware Vision Token Dropping for Faster Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2509.01552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: \url{https://github.com/xuyang-liu16/V2Drop}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为V²Drop的视觉token压缩方法，通过删除变化最小的视觉token来提高大型视觉-语言模型的计算效率，同时保持高性能。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型在多模态理解任务中表现出色，但高分辨率图像和长视频理解导致大量token，降低了推理效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有LLM内部token压缩方法的位置偏差和与高效算子不兼容的问题，提高LVLM的计算效率。&lt;h4&gt;方法&lt;/h4&gt;从token变化角度提出Variation-aware Vision Token Dropping (V²Drop)，在LVLM推理过程中逐步删除变化最小的视觉token。&lt;h4&gt;主要发现&lt;/h4&gt;LLM中的视觉token变化具有任务无关的特性；V²Drop能够保持原始模型94.0%和98.6%的性能，分别用于图像和视频理解任务，同时将LLM生成延迟分别减少31.5%和74.2%。&lt;h4&gt;结论&lt;/h4&gt;V²Drop是一种有效的token压缩方法，可以在保持高性能的同时显著提高LVLM的计算效率，与高效算子结合时还能进一步减少GPU峰值内存使用。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型(LVLMs)在多模态理解任务中表现出色。然而，对高分辨率图像和长视频理解的不断需求导致大量token的产生，从而降低了推理效率。Token压缩通过减少需要处理的token数量提供直接解决方案，从而提高计算效率。通过广泛分析，我们发现现有内部LLM token压缩方法存在两个关键限制：位置偏差和与高效算子不兼容，这阻碍了它们在LVLM加速中的实际部署。本文从token变化角度提出了第一种方法，揭示了LLM中视觉token变化具有任务无关的特性。我们提出了Variation-aware Vision Token Dropping（即V²Drop），在LVLM推理过程中逐步删除变化最小的视觉token，从而提高计算效率。在多个模型和基准上的广泛实验表明，我们的V²Drop能够保持原始模型94.0%和98.6%的性能，分别用于图像和视频理解任务，同时将LLM生成延迟减少31.5%和74.2%。当与高效算子结合时，V²Drop进一步减少了GPU峰值内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (LVLMs) have demonstrated remarkablecapabilities in multimodal understanding tasks. However, the increasing demandfor high-resolution image and long-video understanding results in substantialtoken counts, leading to reduced inference efficiency. Token compression offersa direct solution by reducing the number of tokens to be processed, therebyimproving computational efficiency. Through extensive analysis, we identify twocritical limitations in existing inner-LLM token compression methods:positional bias and incompatibility with efficient operators, which hindertheir practical deployment for LVLM acceleration. This paper presents the firstapproach from a token variation perspective, revealing that visual tokenvariations within LLMs exhibit task-agnostic properties. We proposeVariation-aware Vision Token Dropping (\textit{i.e.}, \textbf{V$^2$Drop}),which progressively removes visual tokens with minimal variation during LVLMinference, thereby enhancing computational efficiency. Extensive experimentsacross multiple models and benchmarks demonstrate that our V$^2$Drop is able tomaintain \textbf{94.0\%} and \textbf{98.6\%} of the original model performancefor image and video understanding tasks respectively, while reducing LLMgeneration latency by \textbf{31.5\%} and \textbf{74.2\%}. When combined withefficient operators, V$^2$Drop further reduces GPU peak memory usage.</description>
      <author>example@mail.com (Junjie Chen, Xuyang Liu, Zichen Wen, Yiyu Wang, Siteng Huang, Honggang Chen)</author>
      <guid isPermaLink="false">2509.01552v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models</title>
      <link>http://arxiv.org/abs/2509.01167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多模态大语言模型在视频理解任务中的应用，特别关注了关键帧采样方法的局限性，并发现当前流行的视觉编码器在识别视频中哪些部分对文本查询最重要方面存在严重不足。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视频理解任务中取得了显著进展。为避免处理所有帧的高计算成本，这些模型通常依赖关键帧采样方法，这些方法由视觉-语言编码器(如SigLIP)指导。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在评估当前流行的视觉编码器是否能真正识别出对处理给定文本查询最有信息量的视频帧，并探索改进视频MLLM效率的可能性。&lt;h4&gt;方法&lt;/h4&gt;作者提供了多项经验证据，评估了视觉编码器在识别MLLM应该查看视频中的哪些部分以适当处理文本查询方面的能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，流行的视觉编码器在识别MLLM应该查看视频中的哪些部分以适当处理给定文本查询方面存在严重局限性。&lt;h4&gt;结论&lt;/h4&gt;开发更好的关键帧识别技术可能是构建高效视频MLLMs的必要条件。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展在视频理解任务中取得了很大进展。为了避免处理所有帧的沉重计算成本，这些模型通常依赖于由视觉-语言编码器(如SigLIP)指导的关键帧采样方法。然而，这些编码器是否能真正识别出最具信息量的帧仍然不清楚。在这项工作中，我们提供了几项经验证据，揭示流行的视觉编码器在识别MLLM应该查看视频中的哪些部分以适当处理给定文本查询方面存在严重局限性。我们的研究结果表明，开发更好的关键帧识别技术可能是高效视频MLLMs所必需的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal large language models (MLLMs) have led to muchprogress in video understanding tasks. To avoid the heavy computational cost ofprocessing all frames, these models typically rely on keyframe sampling methodsguided by vision-language encoders (\textit{e.g.,} SigLIP). However, it remainsunclear whether such encoders can truly identify the most informative frames.In this work, we provide several empirical pieces of evidence revealing thatpopular vision encoders critically suffer from their limited capability toidentify where the MLLM should look inside the video to handle the giventextual query appropriately. Our findings suggest that the development ofbetter keyframe identification techniques may be necessary for efficient videoMLLMs.</description>
      <author>example@mail.com (Hyunjong Ok, Jaeho Lee)</author>
      <guid isPermaLink="false">2509.01167v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>An End-to-End Framework for Video Multi-Person Pose Estimation</title>
      <link>http://arxiv.org/abs/2509.01095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VEPE的视频端到端姿态估计框架，利用三个关键时空Transformer组件有效利用时间上下文优化人体姿态估计，并通过实例一致性机制减少跨帧匹配不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;基于视频的人体姿态估计模型旨在解决静态图像模型无法有效处理的问题，如运动模糊、失焦和遮挡。现有方法通常分为两个阶段：检测每帧图像中的人体实例和使用时间模型进行姿态估计，这种方法分离了空间和时间维度，无法捕获全局时空上下文进行端到端优化，且依赖单独检测器和复杂后处理，降低了推理效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频姿态估计方法的问题，提出一种简单且灵活的视频端到端姿态估计框架。&lt;h4&gt;方法&lt;/h4&gt;提出VEPE框架，利用三个关键时空Transformer组件：时空姿态编码器(STPE)、时空可变形记忆编码器(STDME)和时空姿态解码器(STPD)，有效利用时间上下文优化姿态估计；同时提出实例一致性机制，减少跨帧姿态查询匹配过程中的不匹配问题，增强跨帧实例查询的一致性和差异性，实现实例跟踪功能。&lt;h4&gt;主要发现&lt;/h4&gt;在PoseTrack数据集上的实验表明，该方法优于大多数两阶段模型，推理效率提高了300%。&lt;h4&gt;结论&lt;/h4&gt;VEPE框架是一种有效的视频端到端姿态估计方法，能够更好地处理视频中的姿态估计问题，并显著提高推理效率。&lt;h4&gt;翻译&lt;/h4&gt;基于视频的人体姿态估计模型旨在解决静态图像模型无法有效处理的问题，如运动模糊、失焦和遮挡。大多数现有方法包括两个阶段：检测每帧图像中的人体实例，然后使用时间模型进行单人姿态估计。这种方法分离了空间和时间维度，无法捕获空间实例之间的全局时空上下文进行端到端优化。此外，它依赖于单独的检测器和复杂的后处理，如感兴趣区域裁剪和非极大值抑制，降低了视频场景的推理效率。为解决上述问题，我们提出了VEPE（视频端到端姿态估计），一种用于视频中端到端姿态估计的简单灵活框架。该框架利用三个关键的时空Transformer组件：时空姿态编码器（STPE）、时空可变形记忆编码器（STDME）和时空姿态解码器（STPD）。这些组件被设计为有效利用时间上下文来优化人体姿态估计。此外，为了减少跨帧姿态查询匹配过程中的不匹配问题，我们提出了一种实例一致性机制，旨在增强跨帧实例查询的一致性和差异性，实现实例跟踪功能，从而准确指导姿态查询进行跨帧匹配。在PoseTrack数据集上的大量实验表明，我们的方法优于大多数两阶段模型，并将推理效率提高了300%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video-based human pose estimation models aim to address scenarios that cannotbe effectively solved by static image models such as motion blur, out-of-focusand occlusion. Most existing approaches consist of two stages: detecting humaninstances in each image frame and then using a temporal model for single-personpose estimation. This approach separates the spatial and temporal dimensionsand cannot capture the global spatio-temporal context between spatial instancesfor end-to-end optimization. In addition, it relies on separate detectors andcomplex post-processing such as RoI cropping and NMS, which reduces theinference efficiency of the video scene. To address the above problems, wepropose VEPE (Video End-to-End Pose Estimation), a simple and flexibleframework for end-to-end pose estimation in video. The framework utilizes threecrucial spatio-temporal Transformer components: the Spatio-Temporal PoseEncoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and theSpatio-Temporal Pose Decoder (STPD). These components are designed toeffectively utilize temporal context for optimizing human body pose estimation.Furthermore, to reduce the mismatch problem during the cross-frame pose querymatching process, we propose an instance consistency mechanism, which aims toenhance the consistency and discrepancy of the cross-frame instance query andrealize the instance tracking function, which in turn accurately guides thepose query to perform cross-frame matching. Extensive experiments on thePosetrack dataset show that our approach outperforms most two-stage models andimproves inference efficiency by 300%.</description>
      <author>example@mail.com (Zhihong Wei)</author>
      <guid isPermaLink="false">2509.01095v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors</title>
      <link>http://arxiv.org/abs/2509.00969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages, 8 figures, EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LangDC，一种语言感知的动态标记压缩器，用于解决大型视频-语言模型处理高容量视觉标记的效率问题。通过轻量级语言模型描述视频片段并转换为软标记令牌，结合语义密度感知监督，实现基于场景丰富度的动态压缩比例调整。&lt;h4&gt;背景&lt;/h4&gt;大型视频-语言模型在视频理解任务方面取得了进展，但其效率受到处理大量视觉标记的限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有固定压缩比例策略无法适应不同视频片段语义密度变化的问题，避免信息丰富片段表示不足和静态或内容贫乏片段不必要的计算。&lt;h4&gt;方法&lt;/h4&gt;提出LangDC，利用轻量级语言模型描述视频片段并转换为软标记令牌作为视觉表示，通过语义密度感知监督进行训练，实现覆盖关键视觉线索和基于场景丰富度动态调整压缩比例的目标。&lt;h4&gt;主要发现&lt;/h4&gt;与VideoGPT+相比，LangDC减少了49%的FLOPs同时保持有竞争力的性能，并能根据视频片段的丰富度自适应地调整标记压缩比例。&lt;h4&gt;结论&lt;/h4&gt;LangDC通过模拟人类表达视觉内容的方式，实现了更高效的视频标记压缩，为视频理解任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近大型视频-语言模型的进展彻底改变了视频理解任务。然而，其效率受到处理大量视觉标记的显著限制。现有的标记压缩策略应用固定的压缩比例，忽略了不同视频片段之间语义密度的变化。因此，这导致由于标记不足而无法充分表示信息丰富的片段，并对静态或内容贫乏的片段进行不必要的计算。为解决这一问题，我们提出了LangDC，一种语言感知的动态标记压缩器。LangDC利用轻量级语言模型描述视频片段，将它们转换为软标记令牌作为视觉表示。通过我们提出的语义密度感知监督进行训练，LangDC旨在1)覆盖下游任务推理所需的关键视觉线索，2)基于场景丰富度动态调整压缩比例，反映在描述长度上。我们的设计模拟了人类如何动态表达所见内容：复杂场景(看到更多)产生更详细的语言来传达细微差别(说更多)，而简单场景则用较少的词语描述。实验结果表明，与VideoGPT+相比，我们的方法减少了49%的FLOPs，同时保持有竞争力的性能。此外，定性结果表明我们的方法能够根据视频片段的丰富度自适应地调整标记压缩比例。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video-language models have revolutionized videounderstanding tasks. However, their efficiency is significantly constrained byprocessing high volumes of visual tokens. Existing token compression strategiesapply a fixed compression ratio, ignoring the variability in semantic densityamong different video clips. Consequently, this lead to inadequaterepresentation of information-rich clips due to insufficient tokens andunnecessary computation on static or content-poor ones. To address this, wepropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages alightweight language model to describe video clips, converting them into softcaption tokens as visual representations. Trained with our proposed semanticdensity-aware supervision, LangDC aims to 1) cover key visual cues necessaryfor downstream task reasoning and 2) dynamically adjust compression ratiosbased on scene richness, reflected by descriptions length. Our design mimicshow humans dynamically express what they see: complex scenes (seeing more)elicit more detailed language to convey nuances (saying more), whereas simplerscenes are described with fewer words. Experimental results show that ourmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitiveperformance. Furthermore, qualitative results demonstrate our approachadaptively adjusts the token compression ratio based on video segment richness.</description>
      <author>example@mail.com (Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.00969v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination</title>
      <link>http://arxiv.org/abs/2509.00723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OmniDPO的偏好对齐框架，用于解决多模态大语言模型中的幻觉问题。该框架通过两种策略增强模型对音频视频交互的理解和对视觉音频信息的关注，有效提高了多模态基础能力并减少了幻觉。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在音频视频理解和实时环境感知等任务中取得显著成果，但仍存在幻觉问题。文本模态的先验倾向主导，导致模型过度依赖文本线索而忽略视觉和音频信息。此外，现有模型在训练时独立对齐各模态，忽略了视频与音频间的内在关联，导致需要解释隐藏音频线索时出现幻觉。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型中的幻觉问题，增强模型对音频视频交互的理解，并加强对视觉和听觉信息的关注。&lt;h4&gt;方法&lt;/h4&gt;提出OmniDPO偏好对齐框架，包含两种策略：(1)构建文本偏好样本对，增强模型对音频视频交互的理解；(2)构建多模态偏好样本对，加强模型对视觉和听觉信息的关注。&lt;h4&gt;主要发现&lt;/h4&gt;OmniDPO有效解决了多模态幻觉问题，显著增强了模型跨模态的推理能力，同时提高了多模态基础能力。&lt;h4&gt;结论&lt;/h4&gt;OmniDPO通过同时解决文本主导和模态间关联缺失两个挑战，有效减轻了多模态幻觉并增强了跨模态推理能力。实验证明该框架在两个多模态大语言模型上都取得了良好效果，相关代码和数据集将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;最近，多模态大语言模型引发了新一轮研究热潮，在音频视频理解和实时环境感知等任务中取得了令人印象深刻的结果。然而，幻觉问题仍然存在。类似于双模态设置，文本模态的先验倾向于主导，导致多模态大语言模型更依赖文本线索而忽略视觉和音频信息。此外，完全多模态场景引入了新的挑战。大多数现有模型在训练时独立地将视觉或听觉模态与文本对齐，而忽略了视频及其对应音频之间的内在关联。这种疏忽导致在推理需要解释嵌入在视频内容中的隐藏音频线索时产生幻觉。为应对这些挑战，我们提出了OmniDPO，一种旨在减轻多模态大语言模型中幻觉问题的偏好对齐框架。具体而言，OmniDPO包含两种策略：(1)构建文本偏好样本对，增强模型对音频视频交互的理解；(2)构建多模态偏好样本对，加强模型对视觉和听觉信息的关注。通过解决这两个挑战，OmniDPO有效提高了多模态基础能力并减少了幻觉。在两个多模态大语言模型上进行的实验表明，OmniDPO不仅有效减轻了多模态幻觉，还显著增强了模型跨模态的推理能力。所有代码和数据集将在论文接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Omni-modal large language models (OLLMs) have sparked a new wave ofresearch, achieving impressive results in tasks such as audio-videounderstanding and real-time environment perception. However, hallucinationissues still persist. Similar to the bimodal setting, the priors from the textmodality tend to dominate, leading OLLMs to rely more heavily on textual cueswhile neglecting visual and audio information. In addition, fully multimodalscenarios introduce new challenges. Most existing models align visual orauditory modalities with text independently during training, while ignoring theintrinsic correlations between video and its corresponding audio. Thisoversight results in hallucinations when reasoning requires interpreting hiddenaudio cues embedded in video content. To address these challenges, we proposeOmniDPO, a preference-alignment framework designed to mitigate hallucinationsin OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructingtext-preference sample pairs to enhance the model's understanding ofaudio-video interactions; and (2) constructing multimodal-preference samplepairs to strengthen the model's attention to visual and auditory information.By tackling both challenges, OmniDPO effectively improves multimodal groundingand reduces hallucination. Experiments conducted on two OLLMs demonstrate thatOmniDPO not only effectively mitigates multimodal hallucinations but alsosignificantly enhances the models' reasoning capabilities across modalities.All code and datasets will be released upon paper acceptance.</description>
      <author>example@mail.com (Junzhe Chen, Tianshu Zhang, Shiyu Huang, Yuwei Niu, Chao Sun, Rongzhou Zhang, Guanyu Zhou, Lijie Wen, Xuming Hu)</author>
      <guid isPermaLink="false">2509.00723v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Convolutional Network With Pattern-Spatial Interactive and Regional Awareness for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2509.00515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种PSIRAGCN模型，用于解决交通预测中时空相关性建模和区域异质性考虑的问题，通过模式-空间交互融合框架和区域感知图卷积网络，显著提高了预测性能并平衡了计算成本。&lt;h4&gt;背景&lt;/h4&gt;交通预测对城市交通管理、智能路线规划和实时流量监测具有重要意义。最近时空模型的发展显著提高了交通预测中复杂时空相关性的建模能力，但现有研究仍存在局限性。&lt;h4&gt;目的&lt;/h4&gt;克服现有研究在有效建模不同感知视角下时空相关性、忽略交通模式与空间相关性交互融合，以及未考虑区域异质性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出PSIRAGCN模型，包含：1) 模式-空间交互融合框架，通过从全局到局部的感知视角捕捉模式和空间相关性；2) 基于消息传递的图卷积网络，利用区域特征库重建具有区域意识的数据驱动消息传递，揭示交通网络中节点间的区域异质性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实交通数据集上的大量实验表明，PSIRAGCN优于最先进的基线方法，同时平衡了计算成本。&lt;h4&gt;结论&lt;/h4&gt;PSIRAGCN模型有效解决了交通预测中时空相关性建模和区域异质性考虑的问题，为城市交通管理提供了更好的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;交通预测对城市交通管理、智能路线规划和实时流量监测具有重要意义。最近时空模型的进展显著提高了交通预测中复杂时空相关性的建模能力。不幸的是，大多数先前研究在有效建模不同感知视角下的时空相关性方面遇到挑战，忽略了交通模式与空间相关性之间的交互融合。此外，受空间异质性限制，大多数研究在消息传递过程中没有考虑不同的区域异质性。为克服这些局限性，我们提出了一种用于交通预测的模式-空间交互和区域感知图卷积网络(PSIRAGCN)。具体而言，我们提出了由模式和空间模块组成的模式-空间交互融合框架。该框架旨在通过采用从全局到局部的感知视角来捕捉模式和空间相关性，并通过正反馈促进相互利用。在空间模块中，我们设计了基于消息传递的图卷积网络。该网络利用区域特征库来重建具有区域意识的数据驱动消息传递。重建的消息传递可以揭示交通网络中节点之间的区域异质性。在三个真实交通数据集上的大量实验证明，PSIRAGCN优于最先进的基线方法，同时平衡了计算成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting is significant for urban traffic management, intelligentroute planning, and real-time flow monitoring. Recent advances inspatial-temporal models have markedly improved the modeling of intricatespatial-temporal correlations for traffic forecasting. Unfortunately, mostprevious studies have encountered challenges in effectively modelingspatial-temporal correlations across various perceptual perspectives, whichhave neglected the interactive fusion between traffic patterns and spatialcorrelations. Additionally, constrained by spatial heterogeneity, most studiesfail to consider distinct regional heterogeneity during message-passing. Toovercome these limitations, we propose a Pattern-Spatial Interactive andRegional Awareness Graph Convolutional Network (PSIRAGCN) for trafficforecasting. Specifically, we propose a pattern-spatial interactive fusionframework composed of pattern and spatial modules. This framework aims tocapture patterns and spatial correlations by adopting a perception perspectivefrom the global to the local level and facilitating mutual utilization withpositive feedback. In the spatial module, we designed a graph convolutionalnetwork based on message-passing. The network is designed to leverage aregional characteristics bank to reconstruct data-driven message-passing withregional awareness. Reconstructed message passing can reveal the regionalheterogeneity between nodes in the traffic network. Extensive experiments onthree real-world traffic datasets demonstrate that PSIRAGCN outperforms theState-of-the-art baseline while balancing computational costs.</description>
      <author>example@mail.com (Xinyu Ji, Chengcheng Yan, Jibiao Yuan, Fiefie Zhao)</author>
      <guid isPermaLink="false">2509.00515v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</title>
      <link>http://arxiv.org/abs/2509.00484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://videorewardbench.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoRewardBench，这是第一个全面评估视频领域多模态奖励模型(MRMs)的基准，涵盖了感知、知识、推理和安全四个核心方面。研究团队通过AI辅助数据流程创建了包含1,563个标注样本的高质量偏好数据集，并对28个多模态奖励模型进行了综合评估，发现当前模型表现仍有很大提升空间。&lt;h4&gt;背景&lt;/h4&gt;多模态奖励模型(MRMs)在大视觉语言模型(LVLMs)的训练、推理和评估中起着关键作用。然而，现有视频领域评估MRMs的基准存在三个主要问题：问题数量和多样性有限、缺乏全面的评估维度、以及对多种类型MRMs的评估不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频领域MRMs评估基准的不足，创建一个涵盖视频理解四个核心方面的综合评估基准，并提供高质量的评估数据集。&lt;h4&gt;方法&lt;/h4&gt;通过AI辅助数据流程构建高质量偏好数据集，包含1,563个标注样本、1,482个独特视频和1,559个不同问题。每个样本是一个三元组：视频文本提示、被选中的响应和被拒绝的响应。对28个多模态奖励模型进行综合评估，涵盖生成式、判别式和半标量三类模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 即使表现最好的模型GPT-4o总体准确率也只有57.0%，最先进的开源模型Qwen2.5-VL-72B仅达到53.3%；2) 使用强化学习训练的MRMs不一定比未使用RL训练的模型具有更强的跨模态泛化能力；3) 除判别式MRMs外，其他类型模型都能从推理时扩展中受益；4) 输入视频帧数的变化对不同类型MRMs有不同影响。&lt;h4&gt;结论&lt;/h4&gt;VideoRewardBench为推进视频领域MRMs的评估和发展提供了具有挑战性和价值的基准，当前模型表现仍有很大提升空间。&lt;h4&gt;翻译&lt;/h4&gt;多模态奖励模型(MRMs)通过评估响应质量在大视觉语言模型(LVLMs)的训练、推理和评估中发挥着关键作用。然而，现有用于评估视频领域MRMs的基准存在问题数量和多样性有限、缺乏全面评估维度、以及对多种类型MRMs评估不足等问题。为解决这些差距，我们引入了VideoRewardBench，这是第一个涵盖视频理解四个核心方面的综合基准：感知、知识和推理以及安全。通过我们的AI辅助数据流程，我们整理了一个包含1,563个标注样本的高质量偏好数据集，包括1,482个独特视频和1,559个不同问题——比问题最多的先前基准多15倍。每个样本是一个三元组，包含视频文本提示、被选中的响应和被拒绝的响应。我们还对跨越三类模型的28个多模态奖励模型进行了全面评估：生成式、判别式和半标量。结果表明，即使是表现最好的模型GPT-4o也只有57.0%的总体准确率，而最先进的开源模型Qwen2.5-VL-72B仅达到53.3%。我们的分析进一步揭示了三个关键洞察：(i) 使用强化学习(RL)训练的MRMs不一定比未使用RL训练的模型表现出更强的跨模态泛化能力；(ii) 除了判别式MRMs外，不同模型容量的其他类型MRMs都可以从推理时扩展中受益；(iii) 输入视频帧数的变化对不同类型的MRMs有不同的影响。我们相信VideoRewardBench为推进视频领域MRMs的评估和发展提供了具有挑战性和价值的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal reward models (MRMs) play a crucial role in the training,inference, and evaluation of Large Vision Language Models (LVLMs) by assessingresponse quality. However, existing benchmarks for evaluating MRMs in the videodomain suffer from a limited number and diversity of questions, a lack ofcomprehensive evaluation dimensions, and inadequate evaluation of diverse typesof MRMs. To address these gaps, we introduce VideoRewardBench, the firstcomprehensive benchmark covering four core aspects of video understanding:perception, knowledge, reasoning, and safety. Through our AI-assisted datapipeline, we curate a high-quality preference dataset of 1,563 annotatedsamples, including 1,482 unique videos and 1,559 distinct questions--15 timesthe number found in the most question-rich prior benchmark. Each sample is atriplet consisting of a video-text prompt, a chosen response, and a rejectedresponse. We also conduct a comprehensive evaluation across 28 multimodalreward models spanning three categories: generative, discriminative, andsemi-scalar. Results show that even the top-performing model GPT-4o achievesonly 57.0% overall accuracy, and the state-of-the-art open-source modelQwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three keyinsights: (i) MRMs trained with reinforcement learning (RL) do not necessarilyexhibit stronger cross-modal generalization than those trained without RL; (ii)except for discriminative MRMs, other types of MRMs across varying modelcapacities can benefit from inference-time scaling; and (iii) variations ininput video frame count have different effects on different types of MRMs. Webelieve VideoRewardBench offers a challenging and valuable benchmark foradvancing the evaluation and development of MRMs in the video domain.</description>
      <author>example@mail.com (Zhihong Zhang, Xiaojian Huang, Jin Xu, Zhuodong Luo, Xinzhi Wang, Jiansheng Wei, Xuejin Chen)</author>
      <guid isPermaLink="false">2509.00484v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding</title>
      <link>http://arxiv.org/abs/2509.00357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgLLM框架，一个针对手术视频理解的多功能多模态大模型，通过增强空间焦点和时间感知能力，解决了现有研究中视觉内容感知不足和时间感知有限的问题，在多种手术视频理解任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;手术视频理解对促进计算机辅助手术系统至关重要，但现有研究存在两大局限：对手术视频视觉内容感知不足，以及时间感知不足，这些局限阻碍了多功能CAS解决方案的发展。&lt;h4&gt;目的&lt;/h4&gt;提出SurgLLM框架，一个针对多功能手术视频理解任务的有效多模态大模型，增强空间焦点和时间感知能力。&lt;h4&gt;方法&lt;/h4&gt;设计Surgical Context-aware Multimodal Pretraining（Surg-Pretrain）用于视频编码器，通过执行以手术器械为中心的Masked Video Reconstruction和随后的多模态对齐；提出Temporal-aware Multimodal Tuning将手术时间知识整合到模型中；设计Surgical Task Dynamic Ensemble来有效分类查询并使用最优可学习参数。&lt;h4&gt;主要发现&lt;/h4&gt;在多种手术视频理解任务（包括标题生成、通用VQA和时间VQA）上进行了广泛实验，与最先进的方法相比取得了显著改进，验证了SurgLLM在多功能手术视频理解中的有效性。&lt;h4&gt;结论&lt;/h4&gt;SurgLLM框架有效解决了现有手术视频理解中的空间感知和时间感知不足的问题，为多功能计算机辅助手术系统提供了有价值的解决方案，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;手术视频理解对于促进计算机辅助手术系统至关重要。尽管现有研究取得了显著进展，但仍存在两大主要局限，包括对手术视频视觉内容感知不足和时间感知不足，这阻碍了多功能CAS解决方案的发展。在本工作中，我们提出了SurgLLM框架，这是一个针对多功能手术视频理解任务的有效多模态大模型，具有增强的空间焦点和时间感知能力。具体而言，为了增强手术视频的空间焦点，我们首先为SurgLLM的视频编码器设计了手术上下文感知的多模态预训练（Surg-Pretrain），通过执行以手术器械为中心的掩码视频重建（MV-Recon）和随后的多模态对齐。为了将手术时间知识整合到SurgLLM中，我们进一步提出了时间感知多模态调优（TM-Tuning），通过交错的多模态嵌入增强时间推理。此外，为了无冲突地适应手术视频的各种理解任务，我们设计了一个手术任务动态集成，在我们的SurgLLM中高效分类查询并使用最优可学习参数。在多种手术视频理解任务（包括标题生成、通用VQA和时间VQA）上进行的广泛实验表明，与最先进的方法相比取得了显著改进，验证了我们的SurgLLM在多功能手术视频理解中的有效性。源代码可在https://github.com/franciszchen/SurgLLM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical video understanding is crucial for facilitating Computer-AssistedSurgery (CAS) systems. Despite significant progress in existing studies, twomajor limitations persist, including inadequate visual content perception andinsufficient temporal awareness in surgical videos, and hinder the developmentof versatile CAS solutions. In this work, we propose the SurgLLM framework, aneffective large multimodal model tailored for versatile surgical videounderstanding tasks with enhanced spatial focus and temporal awareness.Specifically, to empower the spatial focus of surgical videos, we first deviseSurgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the videoencoder of SurgLLM, by performing instrument-centric Masked VideoReconstruction (MV-Recon) and subsequent multimodal alignment. To incorporatesurgical temporal knowledge into SurgLLM, we further propose Temporal-awareMultimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleavedmultimodal embeddings. Moreover, to accommodate various understanding tasks ofsurgical videos without conflicts, we devise a Surgical Task Dynamic Ensembleto efficiently triage a query with optimal learnable parameters in our SurgLLM.Extensive experiments performed on diverse surgical video understanding tasks,including captioning, general VQA, and temporal VQA, demonstrate significantimprovements over the state-of-the-art approaches, validating the effectivenessof our SurgLLM in versatile surgical video understanding. The source code isavailable at https://github.com/franciszchen/SurgLLM.</description>
      <author>example@mail.com (Zhen Chen, Xingjian Luo, Kun Yuan, Jinlin Wu, Danny T. M. Chan, Nassir Navab, Hongbin Liu, Zhen Lei, Jiebo Luo)</author>
      <guid isPermaLink="false">2509.00357v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>The Temporal Game: A New Perspective on Temporal Relation Extraction</title>
      <link>http://arxiv.org/abs/2509.00250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Temporal Game是一种创新的时间关系提取方法，通过游戏化方式实现更灵活细粒度的标注，并支持强化学习训练，同时提供公开可用的演示和开源代码。&lt;h4&gt;背景&lt;/h4&gt;时间关系提取是自然语言处理中的重要任务，传统方法通常直接标注区间级别的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的时间关系提取方法，将任务转化为交互式游戏的形式，支持更细粒度和灵活的标注。&lt;h4&gt;方法&lt;/h4&gt;提出了Temporal Game方法，将区间关系分解为时间实体起点和终点之间的点对点比较；玩家分类单个点关系，系统应用时间闭合推断额外关系并保持一致性；这种基于点的策略自然支持区间和瞬时实体；将时间标注视为顺序决策任务，为训练强化学习代理奠定基础。&lt;h4&gt;主要发现&lt;/h4&gt;比以往任何方法都支持更细粒度和灵活的标注；为训练强化学习代理奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;该演示既可作为研究工具，也可作为标注界面；演示公开可用且源代码开源，以促进时间推理和标注领域的进一步研究和社区驱动开发。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们展示了Temporal Game，这是一种时间关系提取的新方法，将任务转化为交互式游戏。我们不是直接标注区间级别的关系，而是将它们分解为时间实体起点和终点之间的点对点比较。在每一步，玩家分类单个点关系，系统应用时间闭合来推断额外关系并强制保持一致性。这种基于点的策略自然支持区间和瞬时实体，实现了比以往任何方法都更细粒度和灵活的标注。Temporal Game也为训练强化学习代理奠定了基础，通过将时间标注视为顺序决策任务。为了展示这一潜力，本文展示的演示包括一个游戏模式，用户可以在其中标注TempEval-3数据集中的文本并基于评分系统获得反馈，以及一个标注模式，允许标注自定义文档并导出时间线。因此，该演示既是研究工具也是标注界面。演示可在https://temporal-game.inesctec.pt公开获取，源代码已开源，以促进时间推理和标注领域的进一步研究和社区驱动发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761488&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we demo the Temporal Game, a novel approach to temporalrelation extraction that casts the task as an interactive game. Instead ofdirectly annotating interval-level relations, our approach decomposes them intopoint-wise comparisons between the start and end points of temporal entities.At each step, players classify a single point relation, and the system appliestemporal closure to infer additional relations and enforce consistency. Thispoint-based strategy naturally supports both interval and instant entities,enabling more fine-grained and flexible annotation than any previous approach.The Temporal Game also lays the groundwork for training reinforcement learningagents, by treating temporal annotation as a sequential decision-making task.To showcase this potential, the demo presented in this paper includes a Gamemode, in which users annotate texts from the TempEval-3 dataset and receivefeedback based on a scoring system, and an Annotation mode, that allows customdocuments to be annotated and resulting timeline to be exported. Therefore,this demo serves both as a research tool and an annotation interface. The demois publicly available at https://temporal-game.inesctec.pt, and the source codeis open-sourced to foster further research and community-driven development intemporal reasoning and annotation.</description>
      <author>example@mail.com (Hugo Sousa, Ricardo Campos, Alípio Jorge)</author>
      <guid isPermaLink="false">2509.00250v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.21496v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ELV-Halluc，首个专门针对长视频幻觉的基准，用于研究语义聚合幻觉(SAH)现象。研究发现SAH在长视频中尤为关键，且随语义复杂性和快速变化而增加。通过位置编码和DPO策略，成功降低了SAH比率27.7%。&lt;h4&gt;背景&lt;/h4&gt;视频多模态大语言模型(Video-MLLMs)在视频理解方面取得了显著进展，但仍容易出现幻觉。现有视频幻觉基准主要针对短视频，将幻觉归因于强语言先验、缺失帧或视觉语言偏差等因素。&lt;h4&gt;目的&lt;/h4&gt;系统研究长视频中的语义聚合幻觉(SAH)现象，并开发减轻SAH的方法。&lt;h4&gt;方法&lt;/h4&gt;引入ELV-Halluc基准，实验验证SAH存在与特性，采用位置编码策略和DPO策略增强模型区分事件内和事件间语义的能力，并整理8K个对抗数据对的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;SAH随语义复杂性的增加而增加，模型在语义快速变化时更容易出现SAH。位置编码策略和DPO策略能有效减轻SAH，在ELV-Halluc和Video-MME上取得改进，SAH比率降低27.7%。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的基准和数据集，以及采用位置编码和DPO策略，可以有效减轻长视频中的语义聚合幻觉问题，提高Video-MLLMs在长视频理解中的准确性。&lt;h4&gt;翻译&lt;/h4&gt;视频多模态大语言模型(Video-MLLMs)在视频理解方面已取得显著进展。然而，它们仍然容易生成与视频输入不一致或无关的幻觉内容。先前的视频幻觉基准主要关注短视频，将幻觉归因于强语言先验、缺失帧或视觉编码器引入的视觉语言偏差等因素。虽然这些因素确实解释了短视频中的大多数幻觉，但它们仍然过度简化了幻觉的原因。有时，模型生成不正确的输出，但具有正确的帧级语义。我们将这种类型的幻觉称为语义聚合幻觉(SAH)，它是在将帧级语义聚合到事件级语义组的过程中产生的。由于长视频中多个事件之间的语义复杂性增加，SAH变得尤为重要，有必要分离并彻底研究这种幻觉的原因。为了解决上述问题，我们引入了ELV-Halluc，这是第一个专门针对长视频幻觉的基准，能够系统研究SAH。我们的实验证实了SAH的存在，并表明它随着语义复杂性的增加而增加。此外，我们发现模型在语义快速变化时更容易出现SAH。我们讨论了减轻SAH的潜在方法，证明位置编码策略有助于减轻SAH，并进一步采用DPO策略增强模型区分事件内和事件间语义的能力。为此，我们整理了8K个对抗数据对的数据集，并在ELV-Halluc和Video-MME上都取得了改进，包括SAH比率显著降低27.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video multimodal large language models (Video-MLLMs) have achieved remarkableprogress in video understanding. However, they remain vulnerable tohallucination-producing content inconsistent with or unrelated to video inputs.Previous video hallucination benchmarks primarily focus on short-videos. Theyattribute hallucinations to factors such as strong language priors, missingframes, or vision-language biases introduced by the visual encoder. While thesecauses indeed account for most hallucinations in short videos, they stilloversimplify the cause of hallucinations. Sometimes, models generate incorrectoutputs but with correct frame-level semantics. We refer to this type ofhallucination as Semantic Aggregation Hallucination (SAH), which arises duringthe process of aggregating frame-level semantics into event-level semanticgroups. Given that SAH becomes particularly critical in long videos due toincreased semantic complexity across multiple events, it is essential toseparate and thoroughly investigate the causes of this type of hallucination.To address the above issues, we introduce ELV-Halluc, the first benchmarkdedicated to long-video hallucination, enabling a systematic investigation ofSAH. Our experiments confirm the existence of SAH and show that it increaseswith semantic complexity. Additionally, we find that models are more prone toSAH on rapidly changing semantics. Moreover, we discuss potential approaches tomitigate SAH. We demonstrate that positional encoding strategy contributes toalleviating SAH, and further adopt DPO strategy to enhance the model's abilityto distinguish semantics within and across events. To support this, we curate adataset of 8K adversarial data pairs and achieve improvements on bothELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.</description>
      <author>example@mail.com (Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu)</author>
      <guid isPermaLink="false">2508.21496v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects</title>
      <link>http://arxiv.org/abs/2509.03446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的框架，用于模拟液体粒子在刚体相互作用和主动操作下的动力学行为。该方法将粒子表示为图节点，使用边界体积层次结构（BVH）算法处理粒子-物体碰撞，能够准确捕捉动态环境中的流体行为，并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;高保真粒子动力学模拟对于设计、图形学和机器人领域中涉及液体的现实世界交互和控制任务至关重要。基于图神经网络的数据驱动方法虽已取得进展，但通常局限于静态自由落体环境或简单物体的操作设置，忽略了与动态运动刚体的复杂相互作用。&lt;h4&gt;目的&lt;/h4&gt;设计一个从头构建的基于GNN的框架，用于学习液体在刚体相互作用和主动操作下的动力学行为。&lt;h4&gt;方法&lt;/h4&gt;将粒子表示为图节点，使用表面表示和边界体积层次结构（BVH）算法处理粒子-物体碰撞，使网络能够建模液体粒子与复杂表面几何形状之间的复杂相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够准确捕捉动态环境中的流体行为，也能在静态自由落体环境中作为模拟器运行；尽管只在一个倒置任务上进行了训练，但模型能有效推广到未见过的物体和新的操作任务如搅拌和舀取；学习到的动力学可用于解决基于梯度的优化方法控制任务。&lt;h4&gt;结论&lt;/h4&gt;所提出的GNN框架能够有效学习液体与刚体相互作用的动力学，具有良好的泛化能力，可推广到未见过的物体和新的操作任务，在实际应用中具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;以高保真度模拟粒子动力学对于解决设计、图形学和机器人领域中涉及液体的现实世界交互和控制任务至关重要。最近，基于图神经网络的数据驱动方法在解决此类问题上取得了进展。然而，这些方法通常局限于学习静态自由落体环境中的流体行为或涉及简单物体的简单操作设置，往往忽略了与动态运动刚体的复杂相互作用。在此，我们提出了一种从头设计的基于GNN的框架，用于学习液体在刚体相互作用和主动操作下的动力学行为，其中粒子被表示为图节点，粒子-物体碰撞使用表面表示和边界体积层次结构（BVH）算法处理。这种方法使网络能够建模液体粒子与复杂表面几何形状之间的复杂相互作用。我们的模型能够准确捕捉动态环境中的流体行为，也能在静态自由落体环境中作为模拟器运行。尽管只在一个倒置任务上进行了训练，但我们的模型能有效推广到包含未见物体的环境和新的操作任务，如搅拌和舀取。最后，我们表明学习到的动力学可以利用基于梯度的优化方法来解决控制和操作任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决现有图神经网络液体模拟方法无法处理动态场景中液体与运动刚体复杂交互的问题。这个问题在现实中很重要，因为准确模拟液体与动态物体的交互对于机器人控制、自主代理与物体交互等应用至关重要，例如机器人倒液体、搅拌等日常任务。现有方法通常局限于静态环境或简单物体，限制了它们在实际应用中的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有GNN模拟器的局限性，认识到需要一种能够同时处理复杂流体动力学和粒子与动态刚体交互的统一框架。他们借鉴了Sanchez-Gonzalez等人[10]的图神经网络架构设计，包括使用相对几何特征定义节点边属性、单步预测训练和消息传递GNN作为核心模型。同时，他们改进了之前使用符号距离场(SDF)进行碰撞处理的方法[13,14]，并参考了MeshGraphNet[11]的多图表示方法，但进行了创新以支持动态交互场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络模拟液体粒子与动态运动刚体的交互，通过将液体粒子和刚体表示为不同节点集合，并使用基于表面的碰撞检测方法准确处理交互。整体流程包括：1)构建图结构，包含液体粒子节点、刚体节点和表面网格节点；2)提取节点和边特征，使用速度历史和相对位置信息；3)通过多个消息传递层迭代更新节点和边特征；4)解码器从液体节点潜在表示中提取更新后的动力学信息；5)在地面真实模拟数据上训练模型；6)给定初始状态和控制输入，迭代生成粒子轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的GNN框架同时处理流体动力学和动态刚体交互；2)多图表示区分液体粒子和刚体节点；3)使用BVH算法进行基于表面的精确碰撞检测；4)能够处理动态场景而非仅静态环境；5)强大的泛化能力，能适应未见物体和新任务；6)可与梯度优化方法集成解决控制任务。相比之前工作，不同之处在于：能处理动态场景而非仅静态环境；支持复杂几何形状而非简单形状；能处理多种操作任务；使用更精确的基于表面碰撞检测而非基于顶点；具有更好的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图神经网络的统一框架，能够准确模拟液体与动态运动刚体之间的复杂交互，并在多种操作任务和未见物体上展现出强大的泛化能力，为机器人和自主代理提供了更逼真的液体模拟工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating particle dynamics with high fidelity is crucial for solvingreal-world interaction and control tasks involving liquids in design, graphics,and robotics. Recently, data-driven approaches, particularly those based ongraph neural networks (GNNs), have shown progress in tackling such problems.However, these approaches are often limited to learning fluid behavior instatic free-fall environments or simple manipulation settings involvingprimitive objects, often overlooking complex interactions with dynamicallymoving kinematic rigid bodies. Here, we propose a GNN-based framework designedfrom the ground up to learn the dynamics of liquids under rigid bodyinteractions and active manipulations, where particles are represented as graphnodes and particle-object collisions are handled using surface representationswith the bounding volume hierarchy (BVH) algorithm. This approach enables thenetwork to model complex interactions between liquid particles and intricatesurface geometries. Our model accurately captures fluid behavior in dynamicsettings and can also function as a simulator in static free-fall environments.Despite being trained on a single-object manipulation task of pouring, ourmodel generalizes effectively to environments with unseen objects and novelmanipulation tasks such as stirring and scooping. Finally, we show that thelearned dynamics can be leveraged to solve control and manipulation tasks usinggradient-based optimization methods.</description>
      <author>example@mail.com (Niteesh Midlagajni, Constantin A. Rothkopf)</author>
      <guid isPermaLink="false">2509.03446v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment</title>
      <link>http://arxiv.org/abs/2509.03393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18th European Workshop on Reinforcement Learning (EWRL 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了使用图神经网络处理脓毒症患者数据的方法，旨在改进静脉输液和血管加压剂的治疗决策。&lt;h4&gt;背景&lt;/h4&gt;脓毒症是一种严重且危及生命的状况，在治疗过程中确定患者静脉输液和血管加压剂的正确用量具有挑战性。以往的研究依赖于关系数据，而现代医疗保健数据复杂，可能更适合用图表示。&lt;h4&gt;目的&lt;/h4&gt;探索基于图的方法表示医疗数据，使用图神经网络学习患者状态表示，并将表示学习与策略学习分离，以改进脓毒症治疗决策。&lt;h4&gt;方法&lt;/h4&gt;将MIMIC-III数据集中的患者数据建模为随时间演化的异构图；采用GraphSAGE和GATv2两种图神经网络架构；训练编码器生成潜在状态表示，同时使用解码器预测下一个患者状态；最后使用dBCQ算法进行策略学习。&lt;h4&gt;主要发现&lt;/h4&gt;基于图的方法在脓毒症治疗决策支持方面显示出潜力，同时表明在该领域中表示学习具有复杂性。&lt;h4&gt;结论&lt;/h4&gt;图方法为脓毒症治疗决策支持提供了有前景的途径，但表示学习的复杂性仍需进一步研究解决。&lt;h4&gt;翻译&lt;/h4&gt;脓毒症是一种严重、危及生命的状况。在治疗脓毒症时，很难确定患者静脉输液和血管加压剂的最佳用量。虽然基于自动强化学习的方法已被用于支持这些决策并取得有希望的结果，但先前的研究依赖于关系数据。考虑到现代医疗保健数据的复杂性，将数据表示为图可能提供更自然和有效的方法。本研究将著名的MIMIC-III数据集中的患者数据建模为随时间演化的异构图。随后，我们探索了两种图神经网络架构——GraphSAGE和GATv2，用于学习患者状态表示，采用将表示学习与策略学习分离的方法。编码器与预测下一个患者状态解码器共同训练，以生成潜在状态表示。然后使用dBCQ算法将这些表示用于策略学习。我们的实验评估结果证实了基于图方法的潜力，同时强调了该领域中表示学习的复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sepsis is a serious, life-threatening condition. When treating sepsis, it ischallenging to determine the correct amount of intravenous fluids andvasopressors for a given patient. While automated reinforcement learning(RL)-based methods have been used to support these decisions with promisingresults, previous studies have relied on relational data. Given the complexityof modern healthcare data, representing data as a graph may provide a morenatural and effective approach. This study models patient data from thewell-known MIMIC-III dataset as a heterogeneous graph that evolves over time.Subsequently, we explore two Graph Neural Network architectures - GraphSAGE andGATv2 - for learning patient state representations, adopting the approach ofdecoupling representation learning from policy learning. The encoders aretrained to produce latent state representations, jointly with decoders thatpredict the next patient state. These representations are then used for policylearning with the dBCQ algorithm. The results of our experimental evaluationconfirm the potential of a graph-based approach, while highlighting thecomplexity of representation learning in this domain.</description>
      <author>example@mail.com (Taisiya Khakharova, Lucas Sakizloglou, Leen Lambers)</author>
      <guid isPermaLink="false">2509.03393v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing</title>
      <link>http://arxiv.org/abs/2509.03376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的内容自适应图解混框架（T-CAGU），通过结合Transformer捕获全局依赖性和内容自适应图神经网络增强局部关系，解决了现有高光谱解混方法无法同时考虑全局和局部信息的问题。&lt;h4&gt;背景&lt;/h4&gt;高光谱解混（HU）旨在将遥感图像中的每个混合像素分解为一组端元及其相应的丰度。尽管使用深度学习在该领域取得了显著进展，但大多数方法无法同时表征全局依赖性和局部一致性，难以同时保持长程交互和边界细节。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕获全局依赖性和局部一致性的高光谱解混方法，以保留长程交互和边界细节。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于Transformer的内容自适应图解混框架（T-CAGU），该方法使用Transformer捕获全局依赖性，并引入内容自适应图神经网络来增强局部关系。T-CAGU集成了多种传播顺序来动态学习图结构，确保对噪声的鲁棒性，并利用图残差机制来保留全局信息并稳定训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，T-CAGU优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;T-CAGU是一种有效的高光谱解混方法，能够同时考虑全局和局部信息，并且对噪声具有鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;高光谱解混（HU）旨在将遥感图像中的每个混合像素分解为一组端元及其相应的丰度。尽管使用深度学习在该领域取得了显著进展，但大多数方法无法同时表征全局依赖性和局部一致性，难以同时保持长程交互和边界细节。本文提出了一种新的基于Transformer的内容自适应图解混框架（T-CAGU），该方法通过使用Transformer捕获全局依赖性并引入内容自适应图神经网络来增强局部关系，从而克服了这些挑战。与之前的工作不同，T-CAGU集成了多种传播顺序来动态学习图结构，确保对噪声的鲁棒性。此外，T-CAGU利用图残差机制来保留全局信息并稳定训练。实验结果表明其优于最先进的方法。我们的代码可在https://github.com/xianchaoxiu/T-CAGU获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remotesensing images into a set of endmembers and their corresponding abundances.Despite significant progress in this field using deep learning, most methodsfail to simultaneously characterize global dependencies and local consistency,making it difficult to preserve both long-range interactions and boundarydetails. This letter proposes a novel transformer-guided content-adaptive graphunmixing framework (T-CAGU), which overcomes these challenges by employing atransformer to capture global dependencies and introducing a content-adaptivegraph neural network to enhance local relationships. Unlike previous work,T-CAGU integrates multiple propagation orders to dynamically learn the graphstructure, ensuring robustness against noise. Furthermore, T-CAGU leverages agraph residual mechanism to preserve global information and stabilize training.Experimental results demonstrate its superiority over the state-of-the-artmethods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.</description>
      <author>example@mail.com (Hui Chen, Liangyu Liu, Xianchao Xiu, Wanquan Liu)</author>
      <guid isPermaLink="false">2509.03376v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Temporal social network modeling of mobile connectivity data with graph neural networks</title>
      <link>http://arxiv.org/abs/2509.03319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)在基于移动连接数据的时间社交网络分析中的应用，比较了四种基于快照的时间GNN模型与一个非GNN基线模型在预测用户间电话和短信活动方面的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为建模图结构复杂网络连接数据的先进数据驱动工具，能够整合节点和边在时空中的信息。然而，使用人们移动连接数据的时间序列分析社交网络尚未得到广泛研究。&lt;h4&gt;目的&lt;/h4&gt;研究四种基于快照的时间GNN模型在预测移动通信网络用户间电话呼叫和短信活动方面的性能，并开发一个使用EdgeBank方法的简单非GNN基线模型进行比较。&lt;h4&gt;方法&lt;/h4&gt;评估了四种基于快照的时间GNN模型(ROLAND等)和一个基于EdgeBank方法的非GNN基线模型，比较它们在预测用户间通信活动方面的表现。&lt;h4&gt;主要发现&lt;/h4&gt;ROLAND时间GNN在大多数情况下优于基线模型，而其他三种GNN平均表现不如基线模型。基于GNN的方法在通过移动连接数据分析时间社交网络方面具有潜力。&lt;h4&gt;结论&lt;/h4&gt;由于ROLAND和基线模型之间的性能差距相对较小，需要进一步研究专门用于时间社交网络分析的GNN架构，以提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为一种先进的基于数据驱动的方法，用于建模图结构复杂网络的连接数据，并整合其节点和边在时空中的信息。然而，迄今为止，使用人们移动连接数据的时间序列分析社交网络尚未得到广泛研究。在本研究中，我们研究了四种基于快照的时间GNN模型，用于预测移动通信网络用户之间的电话呼叫和短信活动。此外，我们使用最近提出的EdgeBank方法开发了一个简单的非GNN基线模型。我们的分析表明，ROLAND时间GNN在大多数情况下优于基线模型，而其他三种GNN的平均表现不如基线模型。结果表明，基于GNN的方法在通过移动连接数据分析时间社交网络方面具有前景。然而，由于ROLAND和基线模型之间的性能差距相对较小，需要进一步研究专门用于时间社交网络分析的GNN架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as a state-of-the-art data-driventool for modeling connectivity data of graph-structured complex networks andintegrating information of their nodes and edges in space and time. However, asof yet, the analysis of social networks using the time series of people'smobile connectivity data has not been extensively investigated. In the presentstudy, we investigate four snapshot - based temporal GNNs in predicting thephone call and SMS activity between users of a mobile communication network. Inaddition, we develop a simple non - GNN baseline model using recently proposedEdgeBank method. Our analysis shows that the ROLAND temporal GNN outperformsthe baseline model in most cases, whereas the other three GNNs perform onaverage worse than the baseline. The results show that GNN based approacheshold promise in the analysis of temporal social networks through mobileconnectivity data. However, due to the relatively small performance marginbetween ROLAND and the baseline model, further research is required onspecialized GNN architectures for temporal social network analysis.</description>
      <author>example@mail.com (Joel Jaskari, Chandreyee Roy, Fumiko Ogushi, Mikko Saukkoriipi, Jaakko Sahlsten, Kimmo Kaski)</author>
      <guid isPermaLink="false">2509.03319v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis</title>
      <link>http://arxiv.org/abs/2509.03095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种跨领域特征迁移方法，利用TRELLIS生成模型学习的几何特征来增强颅内动脉瘤分析，在三个关键任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;颅内动脉瘤构成显著的临床风险，但由于有限的标注3D数据，难以检测、描绘和建模这些动脉瘤。&lt;h4&gt;目的&lt;/h4&gt;开发一种跨领域特征迁移方法，利用通用生成模型学习到的几何特征来增强神经网络的动脉瘤分析能力。&lt;h4&gt;方法&lt;/h4&gt;利用TRELLIS（一种在大型非医学3D数据集上训练的生成模型）学习的潜在几何嵌入，用TRELLIS表面特征替代传统的点法线或网格描述符，增强三个下游任务：在Intra3D数据集中分类动脉瘤与健康血管、在3D网格上分割动脉瘤和血管区域、使用图神经网络在AnXplore数据集上预测血流场。&lt;h4&gt;主要发现&lt;/h4&gt;包含这些特征在准确性、F1分数和分割质量上优于最先进的基线方法，模拟误差减少了15%。&lt;h4&gt;结论&lt;/h4&gt;展示了从通用生成模型到专门医疗任务的3D表示迁移的更广泛潜力，为医学图像分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;颅内动脉瘤构成显著的临床风险，但由于有限的标注3D数据，难以检测、描绘和建模。我们提出了一种跨领域特征迁移方法，利用TRELLIS（一种在大型非医学3D数据集上训练的生成模型）学习的潜在几何嵌入来增强神经网络的动脉瘤分析能力。通过用TRELLIS表面特征替代传统的点法线或网格描述符，我们系统性地增强了三个下游任务：在Intra3D数据集中分类动脉瘤与健康血管、在3D网格上分割动脉瘤和血管区域、使用图神经网络在AnXplore数据集上预测随时间演变的血流场。实验表明，包含这些特征在准确性、F1分数和分割质量上优于最先进的基线方法，并将模拟误差降低了15%。这些结果展示了从通用生成模型到专门医疗任务的3D表示迁移的更广泛潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决颅内动脉瘤的检测、分割和建模困难的问题。这个问题在现实中非常重要，因为颅内动脉瘤是一种严重的临床风险，通常无症状直到破裂，而破裂会导致高发病率和死亡率。早期准确检测未破裂的动脉瘤对预防性临床干预至关重要，但由于标注的3D医学数据有限，这些任务在技术上具有挑战性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到生成式AI在3D对象合成领域取得了显著进展，特别是TRELLIS这样的模型能够生成高保真度的3D对象。他们假设，尽管这些模型在非医学数据上训练，但其学到的丰富几何表示可能对医学领域也有价值。作者设计了一个跨领域特征迁移方法，利用TRELLIS编码器提取深度几何特征。该方法借鉴了现有的TRELLIS生成模型、PointNet和PointNet++等点云处理架构，以及图神经网络进行血流模拟，并基于现有的Intra3D和AnXplore数据集进行实验。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用在大型非医学3D数据集上预训练的TRELLIS模型提取的表面特征，增强神经网络在动脉瘤分析任务中的性能。整体实现流程包括：1) 使用TRELLIS编码器从动脉瘤和血管的3D网格中提取特征，通过多角度渲染、体素化、使用DINOv2自编码器和稀疏变分自编码器生成结构化潜在令牌；2) 将提取的TRELLIS特征集成到下游任务中，包括使用PointNet和PointNet++进行分类和分割，以及使用图神经网络进行血流模拟；3) 对提取的特征进行分析，包括PCA、t-SNE可视化和聚类分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出跨领域特征迁移方法，利用通用3D生成模型(TRELLIS)的预训练特征增强医学3D数据分析；2) 首次将TRELLIS模型应用于医学领域，特别是颅内动脉瘤分析；3) 系统性地将TRELLIS表面特征应用于分类、分割和血流模拟三个下游任务；4) 通过特征分析揭示TRELLIS特征能有效捕捉医学对象的几何特征。相比之前的工作，本文不使用点坐标和法线等基本几何特征，而是使用TRELLIS提取的高级特征，在多个任务上取得了更好的性能，不仅提高了分类和分割的准确性，还减少了血流模拟的误差15%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过利用通用3D生成模型TRELLIS提取的表面特征，显著提升了颅内动脉瘤的分类、分割和血流模拟性能，开创了跨领域特征迁移应用于医学3D数据分析的新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intracranial aneurysms pose a significant clinical risk yet are difficult todetect, delineate and model due to limited annotated 3D data. We propose across-domain feature-transfer approach that leverages the latent geometricembeddings learned by TRELLIS, a generative model trained on large-scalenon-medical 3D datasets, to augment neural networks for aneurysm analysis. Byreplacing conventional point normals or mesh descriptors with TRELLIS surfacefeatures, we systematically enhance three downstream tasks: (i) classifyinganeurysms versus healthy vessels in the Intra3D dataset, (ii) segmentinganeurysm and vessel regions on 3D meshes, and (iii) predicting time-evolvingblood-flow fields using a graph neural network on the AnXplore dataset. Ourexperiments show that the inclusion of these features yields strong gains inaccuracy, F1-score and segmentation quality over state-of-the-art baselines,and reduces simulation error by 15\%. These results illustrate the broaderpotential of transferring 3D representations from general-purpose generativemodels to specialized medical tasks.</description>
      <author>example@mail.com (Clément Hervé, Paul Garnier, Jonathan Viquerat, Elie Hachem)</author>
      <guid isPermaLink="false">2509.03095v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials</title>
      <link>http://arxiv.org/abs/2509.02927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为PDRL的后验不确定性量化方法，利用已训练的图神经网络势能描述符来估计残差误差，作为预测不确定性的代理。&lt;h4&gt;背景&lt;/h4&gt;集成方法被认为是机器学习原子间势能(MLIPs)不确定性量化的黄金标准，但其高计算成本限制了其实用性。虽然提出了蒙特卡罗dropout和深度核学习等替代技术来提高计算效率，但这些方法有些不能应用于已训练的模型，并可能影响预测准确性。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单高效的后验不确定性量化框架，利用已训练的图神经网络势能描述符来估计残差误差。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为后验描述符基残差学习(PDRL)的方法，该方法通过建模MLIP预测与真实值之间的差异，使这些残差能够作为预测不确定性的代理。探索了PDRL的多种变体，并将其与既定的不确定性量化方法进行基准测试，评估其有效性和局限性。&lt;h4&gt;主要发现&lt;/h4&gt;探索了PDRL的多种变体，并将其与既定的不确定性量化方法进行了基准测试。&lt;h4&gt;结论&lt;/h4&gt;评估了PDRL方法的有效性和局限性。&lt;h4&gt;翻译&lt;/h4&gt;集成方法被认为是机器学习原子间势能(MLIPs)不确定性量化的黄金标准。然而，它们的高计算成本可能限制其实用性。已经提出了蒙特卡罗dropout和深度核学习等替代技术来提高计算效率；然而，其中一些方法不能应用于已训练的模型，并可能影响预测准确性。在本文中，我们提出了一种简单高效的后验不确定性量化框架，利用已训练的图神经网络势能的描述符来估计残差误差。我们将此方法称为后验描述符基残差学习(PDRL)。PDRL对MLIP预测与真实值之间的差异进行建模，使这些残差能够作为预测不确定性的代理。我们探索了PDRL的多种变体，并将其与既定的不确定性量化方法进行基准测试，评估它们的有效性和局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensemble method is considered the gold standard for uncertaintyquantification (UQ) for machine learning interatomic potentials (MLIPs).However, their high computational cost can limit its practicality. Alternativetechniques, such as Monte Carlo dropout and deep kernel learning, have beenproposed to improve computational efficiency; however, some of these methodscannot be applied to already trained models and may affect the predictionaccuracy. In this paper, we propose a simple and efficient post-hoc frameworkfor UQ that leverages the descriptor of a trained graph neural networkpotential to estimate residual errors. We refer to this method as post-hocdescriptor-based residual-based learning (PDRL). PDRL models the discrepancybetween MLIP predictions and ground truth values, allowing these residuals toact as proxies for prediction uncertainty. We explore multiple variants of PDRLand benchmark them against established UQ methods, evaluating both theireffectiveness and limitations.</description>
      <author>example@mail.com (Shih-Peng Huang, Nontawat Charoenphakdee, Yuta Tsuboi, Yong-Bin Zhuang, Wenwen Li)</author>
      <guid isPermaLink="false">2509.02927v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Power Grid Control with Graph-Based Distributed Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.02861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图论的分布式强化学习框架，用于实时、可扩展的电网管理。该框架由分布式低级代理和高级管理代理组成，使用图神经网络编码网络拓扑信息，并结合模仿学习和基于潜在函数的奖励塑造来提高学习效率。实验证明该方法优于标准基线，且计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;可再生能源的必要整合与电网规模的不断扩大，给现代电网控制带来了重大挑战。基于人类和优化的传统控制系统在这种不断变化的背景下难以适应和扩展，这促使人们探索更动态和分布式的控制策略。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实时、可扩展地管理电网的控制系统，以解决传统控制方法在应对可再生能源整合和电网规模扩大方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于图论的分布式强化学习框架，包括：1)由分布式低级代理（作用于单个电力线路）和高级管理代理组成的架构；2)使用图神经网络（GNN）在单个低级代理的观察中编码网络拓扑信息；3)整合模仿学习和基于潜在函数的奖励塑造以加速收敛和提高学习稳定性；4)不仅分解动作空间，还分解观察空间；5)每个低级代理基于通过GNN构建的环境结构化和信息丰富的局部视图进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;1)在Grid2op仿真环境中的实验表明，该方法有效且一致地优于该领域普遍采用的标准基线方法；2)与基于仿真的专家方法相比，所提出的模型计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;基于图论的分布式强化学习框架为解决现代电网控制中的挑战提供了一种有效方法，通过结合图神经网络、模仿学习和奖励塑造技术，实现了实时、可扩展的电网管理，并且在性能和计算效率方面都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;可再生能源的必要整合与电网规模的不断扩大，给现代电网控制带来了重大挑战。基于人类和优化的传统控制系统在这种不断变化的背景下难以适应和扩展，这促使人们探索更动态和分布式的控制策略。本文提出了一种基于图论的分布式强化学习框架，用于实时、可扩展的电网管理。所提出的架构由分布式低级代理（作用于单个电力线路）和高级管理代理协调组成。采用图神经网络（GNN）在单个低级代理的观察中编码网络拓扑信息。为了加速收敛和提高学习稳定性，该框架整合了模仿学习和基于潜在函数的奖励塑造。与仅分解动作空间而依赖全局观察的传统去中心化方法不同，该方法还分解了观察空间。每个低级代理基于通过GNN构建的环境结构化和信息丰富的局部视图进行操作。在Grid2Op仿真环境中的实验表明了该方法的有效性，其性能一致地优于该领域普遍采用的标准基线方法。此外，与基于仿真的专家方法相比，所提出的模型计算效率更高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The necessary integration of renewable energy sources, combined with theexpanding scale of power networks, presents significant challenges incontrolling modern power grids. Traditional control systems, which are humanand optimization-based, struggle to adapt and to scale in such an evolvingcontext, motivating the exploration of more dynamic and distributed controlstrategies. This work advances a graph-based distributed reinforcement learningframework for real-time, scalable grid management. The proposed architectureconsists of a network of distributed low-level agents acting on individualpower lines and coordinated by a high-level manager agent. A Graph NeuralNetwork (GNN) is employed to encode the network's topological informationwithin the single low-level agent's observation. To accelerate convergence andenhance learning stability, the framework integrates imitation learning andpotential-based reward shaping. In contrast to conventional decentralizedapproaches that decompose only the action space while relying on globalobservations, this method also decomposes the observation space. Each low-levelagent acts based on a structured and informative local view of the environmentconstructed through the GNN. Experiments on the Grid2Op simulation environmentshow the effectiveness of the approach, which consistently outperforms thestandard baseline commonly adopted in the field. Additionally, the proposedmodel proves to be much more computationally efficient than thesimulation-based Expert method.</description>
      <author>example@mail.com (Carlo Fabrizio, Gianvito Losapio, Marco Mussi, Alberto Maria Metelli, Marcello Restelli)</author>
      <guid isPermaLink="false">2509.02861v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Quasi-van Hove singularities informed approach improving DOS/pDOS predictions in GNN</title>
      <link>http://arxiv.org/abs/2509.02818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文扩展了Van Hove奇点的概念，提出在机器学习方法中使用这些奇点作为先验信息的新方法，以提高模型预测质量。&lt;h4&gt;背景&lt;/h4&gt;传统方法直接计算量子态密度，而本文提出使用机器学习方法的估计值来处理这个问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用Van Hove奇点作为先验信息的机器学习方法，以提高模型预测质量。&lt;h4&gt;方法&lt;/h4&gt;使用机器学习估计代替直接计算量子态密度，通过确定额外信息组织后处理，采用类似于考虑Fisher信息的正则化机制重新分配系统信息。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法显著提高了模型预测质量，其分析效果类似于高阶Van Hove奇点，因为系统具有额外的简并度。&lt;h4&gt;结论&lt;/h4&gt;通过扩展Van Hove奇点概念并结合机器学习方法，可以有效地改善模型预测性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们扩展了Van Hove奇点的概念，以便在机器学习方法中明确使用它们作为模型的先验信息。当不直接计算量子态密度，而是使用机器学习方法获得的估计值时，所提出的方法成为可能。然后，确定额外信息允许我们组织后处理，这显著提高了模型预测的质量。从分析上看，该效应类似于高阶Van Hove奇点，因为系统具有额外的简并度。作为重新分配系统信息的机制，我们提出了正则化，其在意义上接近于考虑Fisher信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce an extension of the concept of Van Hoffsingularities in order to explicitly use them in machine learning methods ofmodels as a priori information. The claimed method becomes possible when,instead of directly calculating the density of quantum states, we operate withestimates obtained by machine learning methods. Then, determining additionalinformation allows us to organize post-processing, which significantly improvesthe quality of model prediction. Analytically, the effect is similar to VanHove singularities of high order due to the additional degree of degeneracy ofthe system. As a mechanism for redistributing information about the system, wepropose regularization, which is close in meaning to taking into account Fisherinformation.</description>
      <author>example@mail.com (Grigory Koroteev)</author>
      <guid isPermaLink="false">2509.02818v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.02803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过归纳学习拉普拉斯特征向量来预训练图神经网络(GNNs)的新框架。&lt;h4&gt;背景&lt;/h4&gt;传统的消息传递神经网络(MPNNs)随着网络深度的增加，往往面临过平滑风险，难以捕捉全局和区域的图结构。&lt;h4&gt;目的&lt;/h4&gt;设计一种预训练框架，使GNNs能够自然学习每个图的大规模结构模式，从而更好地捕捉全局和区域图结构。&lt;h4&gt;方法&lt;/h4&gt;利用图拉普拉斯矩阵的低频特征向量编码全局信息的特点，开发了一种自监督预训练框架，该框架基于图结构且具有高度灵活性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，通过该框架预训练的模型在各种基于图结构的任务上表现优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;与大多数现有专注于节点或边特征重建等特定领域任务的预训练方法不同，这种基于特征向量学习的自监督预训练框架适用于所有基于图的数据集，并且在特定任务数据稀疏时可以与合成特征结合使用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过归纳学习拉普拉斯特征向量来预训练图神经网络(GNNs)的新框架。传统的消息传递神经网络(MPNNs)往往由于网络深度增加导致的过平滑风险而难以捕捉全局和区域的图结构。由于图拉普拉斯矩阵的低频特征向量编码了全局信息，预训练GNNs来预测这些特征向量会鼓励网络自然地学习每个图的大规模结构模式。经验上，我们证明通过我们框架预训练的模型在多种基于图结构的任务上优于基线模型。虽然大多数现有的预训练方法专注于节点或边特征重建等特定领域任务，但我们的自监督预训练框架是基于结构的，具有高度灵活性。特征向量学习可以应用于所有基于图的数据集，并且在特定任务数据稀疏时，可以与合成特征一起使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel framework for pre-training Graph Neural Networks (GNNs) byinductively learning Laplacian eigenvectors. Traditional Message Passing NeuralNetworks (MPNNs) often struggle to capture global and regional graph structuredue to over-smoothing risk as network depth increases. Because thelow-frequency eigenvectors of the graph Laplacian matrix encode globalinformation, pre-training GNNs to predict these eigenvectors encourages thenetwork to naturally learn large-scale structural patterns over each graph.Empirically, we show that models pre-trained via our framework outperformbaseline models on a variety of graph structure-based tasks. While mostexisting pre-training methods focus on domain-specific tasks like node or edgefeature reconstruction, our self-supervised pre-training framework isstructure-based and highly flexible. Eigenvector-learning can be applied to allgraph-based datasets, and can be used with synthetic features whentask-specific data is sparse.</description>
      <author>example@mail.com (Howard Dai, Nyambura Njenga, Benjamin Whitsett, Catherine Ma, Darwin Deng, Sara de Ángel, Alexandre Van Tassel, Siddharth Viswanath, Ryan Pellico, Ian Adelstein, Smita Krishnaswamy)</author>
      <guid isPermaLink="false">2509.02803v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Composite-Loss Graph Neural Network for the Multivariate Post-Processing of Ensemble Weather Forecasts</title>
      <link>http://arxiv.org/abs/2509.02784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 16 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种使用图神经网络(dualGNN)和复合损失函数进行集合预报多变量后处理的新方法，该方法在保持变量间和空间依赖关系方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;集合预报系统通过提供未来状态的概率估计推动了气象学发展，支持从可再生能源生产到运输安全等多种应用。然而，系统性偏差仍然存在，使得统计后处理变得必要。传统参数化后处理技术和基于机器学习的方法通常难以捕捉预报维度间的依赖关系。&lt;h4&gt;目的&lt;/h4&gt;研究使用图神经网络(dualGNN)进行集合预报的多变量后处理，该网络使用结合能量得分(ES)和变异函数得分(VS)的复合损失函数进行训练。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(dualGNN)进行多变量后处理，结合能量得分(ES)和变异函数得分(VS)作为复合损失函数。在两个数据集上评估该方法：WRF基于的智利北部太阳辐射预报和ECMWF中欧能见度预报。&lt;h4&gt;主要发现&lt;/h4&gt;dualGNN始终优于所有基于经验copula的后处理预报，与仅使用连续排序概率得分(CRPS)或仅使用ES训练的图神经网络相比有显著改进。对于WRF预报，dualGNN能有效恢复空间关系；对于能见度预报，使用CRPS、ES或ES-VS组合训练的GNN优于校准参考。&lt;h4&gt;结论&lt;/h4&gt;多变量后处理对于恢复真实的变量间或时空依赖关系至关重要。dualGNN方法在多变量验证指标上表现优异，特别是在处理太阳辐射预报时能够有效恢复空间关系。&lt;h4&gt;翻译&lt;/h4&gt;集合预报系统通过提供未来状态的概率估计推动了气象学的发展，支持从可再生能源生产到运输安全等多种应用应用。尽管如此，系统性偏差往往仍然存在，使得统计后处理变得必不可少。传统的参数化后处理技术和基于机器学习的方法可以在特定位置和提前期产生校准的预测分布，但往往难以捕捉跨预报维度的依赖关系。为了解决这个问题，多变量后处理方法——如ensemble copula coupling和Schaake shuffle——被广泛地应用于第二步，以恢复真实的变量间或时空依赖关系。本研究的目标是使用图神经网络(dualGNN)对集合预报进行多变量后处理，该网络使用结合能量得分(ES)和变异函数得分(VS)的复合损失函数进行训练。该方法在两个数据集上进行了评估：基于WRF的智利北部太阳辐射预报和ECMWF的中欧能见度预报。根据评估的多变量验证指标，dualGNN始终优于所有基于经验copula的后处理预报，并且与仅使用连续排序概率得分(CRPS)或仅使用ES训练的图神经网络相比显示出显著改进。此外，对于WRF预报，dualGNN预报的排序结构捕获了有价值的依赖信息，能够比原始数值天气预报集合或历史观测排序结构更有效地恢复空间关系。相比之下，对于能见度预报，使用CRPS、ES或ES-VS组合训练的GNN优于校准参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensemble forecasting systems have advanced meteorology by providingprobabilistic estimates of future states, supporting applications fromrenewable energy production to transportation safety. Nonetheless, systematicbiases often persist, making statistical post-processing essential. Traditionalparametric post-processing techniques and machine learning-based methods canproduce calibrated predictive distributions at specific locations and leadtimes, yet often struggle to capture dependencies across forecast dimensions.To address this, multivariate post-processing methods-such as ensemble copulacoupling and the Schaake shuffle-are widely applied in a second step to restorerealistic inter-variable or spatio-temporal dependencies. The aim of this studyis the multivariate post-processing of ensemble forecasts using a graph neuralnetwork (dualGNN) trained with a composite loss function that combines theenergy score (ES) and the variogram score (VS). The method is evaluated on twodatasets: WRF-based solar irradiance forecasts over northern Chile and ECMWFvisibility forecasts for Central Europe. The dualGNN consistently outperformsall empirical copula-based post-processed forecasts and shows significantimprovements compared to graph neural networks trained solely on either thecontinuous ranked probability score (CRPS) or the ES, according to theevaluated multivariate verification metrics. Furthermore, for the WRFforecasts, the rank-order structure of the dualGNN forecasts captures valuabledependency information, enabling a more effective restoration of spatialrelationships than either the raw numerical weather prediction ensemble orhistorical observational rank structures. By contrast, for the visibilityforecasts, the GNNs trained on CRPS, ES, or the ES-VS combination outperformthe calibrated reference.</description>
      <author>example@mail.com (Mária Lakatos)</author>
      <guid isPermaLink="false">2509.02784v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction</title>
      <link>http://arxiv.org/abs/2509.02481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The 33rd ACM International Conference on Advances in  Geographic Information Systems (SIGSPATIAL 25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HydroGAT的时空网络模型，用于精确的洪水预测，通过异构流域图和分布式训练方法解决了传统方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;精确的洪水预测对水资源管理具有挑战性，需要建模局部时变径流驱动因素和河流网络中的复杂空间相互作用。传统数据驱动方法忽略区域拓扑信息，而现有GNN模型因训练成本问题被迫降低分辨率，且多数方法无法同时捕捉时空依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时空相互作用的洪水预测模型，实现高分辨率流域规模的训练，并提供可解释的预测结果。&lt;h4&gt;方法&lt;/h4&gt;引入异构流域图，其中每个陆地和河流像素作为节点通过物理水文流向和流域间关系连接；提出HydroGAT时空网络，自适应学习局部时间重要性和最具影响力的上游位置；开发分布式数据并行训练管道，支持在超级计算机上高效扩展。&lt;h4&gt;主要发现&lt;/h4&gt;在美国中西部两个流域的评估中，HydroGAT实现了高达0.97的NSE和0.96的KGE，PBIAS保持在±5%以内，提供了可解释的注意力图揭示流域间影响；分布式训练实现了跨机器高达15倍的加速。&lt;h4&gt;结论&lt;/h4&gt;HydroGAT模型在洪水预测任务中表现出色，同时提供了可解释性和高效率的分布式训练能力，为高分辨率流域建模提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;精确的洪水预测对水资源管理仍然是一个挑战，因为它需要对局部、时变的径流驱动因素（如降雨引起的峰值、基流趋势）和河流网络中复杂的空间相互作用进行建模。传统的数据驱动方法，如卷积网络和基于序列的模型，忽略了区域的拓扑信息。图神经网络(GNN)可以沿河流网络精确传播信息，非常适合学习水文路由。然而，最先进的基于GNN的洪水预测模型将像素合并为粗略的流域多边形，因为随着图大小和更高分辨率的增加，训练成本会爆炸式增长。此外，大多数现有方法分别处理空间和时间依赖性，要么仅在空间图上应用GNN，要么仅在时间序列上应用transformer，因此无法同时捕捉对准确洪水预测至关重要的时空相互作用。我们引入了一个异构流域图，其中每个陆地和河流像素都是通过物理水文流向和流域间关系连接的节点。我们提出了HydroGAT，一个时空网络，可以自适应地学习局部时间重要性和最具影响力的上游位置。在美国中西部的两个流域和五种基线架构上进行了评估，我们的模型在小时流量预测中实现了更高的NSE（高达0.97）、改进的KGE（高达0.96）和低偏差（PBIAS在±5%以内），同时提供了可解释的注意力图，揭示了稀疏的、结构化的流域间影响。为了支持高分辨率流域规模的训练，我们开发了一个分布式数据并行管道，可以在NERSC Perlmutter超级计算机上高效扩展到64个NVIDIA A100 GPU，展示了跨机器高达15倍的加速。我们的代码可在https://github.com/swapp-lab/HydroGAT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3748636.3764172&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate flood forecasting remains a challenge for water-resource management,as it demands modeling of local, time-varying runoff drivers (e.g.,rainfall-induced peaks, baseflow trends) and complex spatial interactionsacross a river network. Traditional data-driven approaches, such asconvolutional networks and sequence-based models, ignore topologicalinformation about the region. Graph Neural Networks (GNNs) propagateinformation exactly along the river network, which is ideal for learninghydrological routing. However, state-of-the-art GNN-based flood predictionmodels collapse pixels to coarse catchment polygons as the cost of trainingexplodes with graph size and higher resolution. Furthermore, most existingmethods treat spatial and temporal dependencies separately, either applyingGNNs solely on spatial graphs or transformers purely on temporal sequences,thus failing to simultaneously capture spatiotemporal interactions critical foraccurate flood prediction. We introduce a heterogenous basin graph where everyland and river pixel is a node connected by physical hydrological flowdirections and inter-catchment relationships. We propose HydroGAT, aspatiotemporal network that adaptively learns local temporal importance and themost influential upstream locations. Evaluated in two Midwestern US basins andacross five baseline architectures, our model achieves higher NSE (up to 0.97),improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourlydischarge prediction, while offering interpretable attention maps that revealsparse, structured intercatchment influences. To support high-resolutionbasin-scale training, we develop a distributed data-parallel pipeline thatscales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer,demonstrating up to 15x speedup across machines. Our code is available athttps://github.com/swapp-lab/HydroGAT.</description>
      <author>example@mail.com (Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari)</author>
      <guid isPermaLink="false">2509.02481v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2509.02111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NOOUGAT是一种新型的多目标跟踪器，能够处理任意时间长度的跟踪任务，成功统一了在线和离线跟踪方法，解决了长期存在的跟踪领域划分问题。&lt;h4&gt;背景&lt;/h4&gt;在线和离线多目标跟踪之间长期存在划分，导致解决方案碎片化，无法满足实际部署场景中的灵活时间需求。在线跟踪器依赖逐帧手工关联策略，难以处理长期遮挡；离线方法可覆盖更大时间间隔，但仍依赖启发式拼接处理任意长序列。&lt;h4&gt;目的&lt;/h4&gt;引入NOOUGAT，第一个设计用于操作任意时间跨度的跟踪器，解决在线和离线跟踪方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;NOOUGAT利用统一的图神经网络框架处理不重叠的子剪辑，通过新的自回归长期跟踪层融合这些子剪辑。子剪辑大小控制延迟和时间上下文之间的权衡，支持从逐帧到批处理的广泛部署场景。&lt;h4&gt;主要发现&lt;/h4&gt;NOOUGAT在两种跟踪模式下都达到最先进性能：在线模式下在DanceTrack上AssA提高+2.3，在SportsMOT上提高+9.2，在MOT20上提高+5.0；离线模式下有更大的性能提升。&lt;h4&gt;结论&lt;/h4&gt;NOOUGAT成功统一了在线和离线跟踪方法，提供了灵活的时间跨度控制，适应各种实际部署场景，在多个基准测试上实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;长期以来，在线和离线多目标跟踪之间的划分导致了碎片化的解决方案，无法满足实际部署场景中的灵活时间需求。当前在线跟踪器依赖逐帧手工关联策略，难以处理长期遮挡，而离线方法可以覆盖更大的时间间隔，但仍依赖启发式拼接来处理任意长序列。在本文中，我们引入了NOOUGAT，第一个设计用于操作任意时间长度的跟踪器。NOOUGAT利用统一的图神经网络框架处理不重叠的子剪辑，并通过新的自回归长期跟踪层融合它们。子剪辑大小控制延迟和时间上下文之间的权衡，支持从逐帧到批处理的广泛部署场景。NOOUGAT在两种跟踪模式下都达到了最先进的性能，在线模式下在DanceTrack上AssA提高+2.3，在SportsMOT上提高+9.2，在MOT20上提高+5.0，离线模式下有更大的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The long-standing division between \textit{online} and \textit{offline}Multi-Object Tracking (MOT) has led to fragmented solutions that fail toaddress the flexible temporal requirements of real-world deployment scenarios.Current \textit{online} trackers rely on frame-by-frame hand-craftedassociation strategies and struggle with long-term occlusions, whereas\textit{offline} approaches can cover larger time gaps, but still rely onheuristic stitching for arbitrarily long sequences. In this paper, we introduceNOOUGAT, the first tracker designed to operate with arbitrary temporalhorizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework thatprocesses non-overlapping subclips, and fuses them through a novelAutoregressive Long-term Tracking (ALT) layer. The subclip size controls thetrade-off between latency and temporal context, enabling a wide range ofdeployment scenarios, from frame-by-frame to batch processing. NOOUGAT achievesstate-of-the-art performance across both tracking regimes, improving\textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 onMOT20, with even greater gains in \textit{offline} mode.</description>
      <author>example@mail.com (Benjamin Missaoui, Orcun Cetintas, Guillem Brasó, Tim Meinhardt, Laura Leal-Taixé)</author>
      <guid isPermaLink="false">2509.02111v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Second-Order Tensorial Partial Differential Equations on Graphs</title>
      <link>http://arxiv.org/abs/2509.02015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了二阶TPDEGs(So-TPDEGs)，提出了首个有理论基础的二阶连续乘积图神经网络框架，通过利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时保留高频信息，为连续图学习提供了坚实的理论基础。&lt;h4&gt;背景&lt;/h4&gt;处理位于多个交互图上的数据在实际应用中越来越重要，但现有方法大多局限于离散图滤波。图上的张量偏微分方程(TPDEGs)为在连续环境中建模多域数据提供了原则性框架。&lt;h4&gt;目的&lt;/h4&gt;引入二阶TPDEGs(So-TPDEGs)并开发首个有理论基础的二阶连续乘积图神经网络框架，以解决现有方法在处理复杂、多尺度和异构结构时的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时自然地保留高频信息，并提供对图扰动下稳定性和谱特性过平滑行为的严格理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;二阶TPDEGs能够有效保留高频信息并加快信息传播，比现有的一阶方法更适合捕获复杂、多尺度和异构结构。&lt;h4&gt;结论&lt;/h4&gt;理论结果为推进多个实际领域的连续图学习提供了坚实的基础，使基于TPDEGs的方法能够更好地处理复杂的图数据。&lt;h4&gt;翻译&lt;/h4&gt;处理位于多个交互（乘积）图上的数据在实际应用中日益重要，然而现有方法大多局限于离散图滤波。图上的张量偏微分方程（TPDEGs）为在连续环境中建模此类多域数据提供了原则性框架。然而，当前连续方法仅限于使用一阶导数，这往往会抑制高频信号并减慢信息传播。这使得基于TPDEGs的方法在捕获复杂、多尺度和异构结构方面效果较差。在本文中，我们引入了二阶TPDEGs（So-TPDEGs），并提出了首个有理论基础的二阶连续乘积图神经网络框架。我们的方法利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时自然地保留高频信息。我们提供了关于图扰动下稳定性和谱特性过平滑行为的严格理论分析。我们的理论结果为推进多个实际领域的连续图学习提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Processing data that lies on multiple interacting (product) graphs isincreasingly important in practical applications, yet existing methods aremostly restricted to discrete graph filtering. Tensorial partial differentialequations on graphs (TPDEGs) offer a principled framework for modeling suchmultidomain data in a continuous setting. However, current continuousapproaches are limited to first-order derivatives, which tend to dampenhigh-frequency signals and slow down information propagation. This makes theseTPDEGs-based approaches less effective for capturing complex, multi-scale, andheterophilic structures. In this paper, we introduce second-order TPDEGs(So-TPDEGs) and propose the first theoretically grounded framework forsecond-order continuous product graph neural networks. Our approach leveragesthe separability of cosine kernels in Cartesian product graphs to implementefficient spectral decomposition, while naturally preserving high-frequencyinformation. We provide rigorous theoretical analyses of stability under graphperturbations and over-smoothing behavior regarding spectral properties. Ourtheoretical results establish a robust foundation for advancing continuousgraph learning across multiple practical domains.</description>
      <author>example@mail.com (Aref Einizade, Fragkiskos D. Malliaros, Jhony H. Giraldo)</author>
      <guid isPermaLink="false">2509.02015v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems</title>
      <link>http://arxiv.org/abs/2509.01972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的三阶段框架，整合基于过程的模型与机器学习方法，通过知识蒸馏技术逐步嵌入人工智能，应用于生态水文过程模拟，实现了高效、准确的建模和决策支持。&lt;h4&gt;背景&lt;/h4&gt;生态水文过程模拟对于理解复杂环境系统和指导可持续管理至关重要，特别是在气候变化和人类压力加剧的背景下。基于过程的模型具有物理真实性但存在结构僵化、计算成本高和校准复杂的问题；机器学习方法高效灵活但缺乏可解释性和可迁移性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够结合基于过程模型和机器学习方法优势的统一框架，用于生态水文过程模拟，提高计算效率、预测准确性和决策支持能力。&lt;h4&gt;方法&lt;/h4&gt;提出一个统一的三阶段框架：第一阶段(行为蒸馏)通过代理学习和模型简化增强过程模型；第二阶段(结构蒸馏)将过程方程重新表述为图神经网络中的模块化组件；第三阶段(认知蒸馏)使用'眼睛-大脑-双手-嘴巴'架构将专家推理嵌入智能建模代理。&lt;h4&gt;主要发现&lt;/h4&gt;在Samish流域的演示中，该框架能够重现基于过程的模型输出，提高预测准确性，并支持基于场景的决策制定，证明了其在生态水文建模中的适用性。&lt;h4&gt;结论&lt;/h4&gt;该框架为下一代智能生态水文建模系统提供了可扩展和可迁移的途径，并有可能扩展到其他基于过程的领域。&lt;h4&gt;翻译&lt;/h4&gt;模拟生态水文过程对于理解复杂环境系统和指导可持续管理至关重要，在气候变化和人类压力不断加剧的背景下。基于过程的模型提供了物理真实性，但可能存在结构僵化、计算成本高和校准复杂的问题，而机器学习方法高效灵活却往往缺乏可解释性和可迁移性。我们提出一个统一的三阶段框架，整合基于过程的模型与机器学习，并通过知识蒸馏逐步将它们嵌入到人工智能中。第一阶段行为蒸馏通过代理学习和模型简化增强过程模型，以较低的计算成本捕捉关键动态。第二阶段结构蒸馏将过程方程重新表述为图神经网络中的模块化组件，实现多尺度表示并无缝集成机器学习模型。第三阶段认知蒸馏使用眼睛-大脑-双手-嘴巴架构将专家推理和自适应决策嵌入到智能建模代理中。Samish流域的演示突显了该框架在生态水文建模中的适用性，表明它可以重现基于过程的模型输出，提高预测准确性，并支持基于场景的决策制定。该框架为下一代智能生态水文建模系统提供了可扩展和可迁移的途径，有潜力扩展到其他基于过程的领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating ecohydrological processes is essential for understanding complexenvironmental systems and guiding sustainable management amid acceleratingclimate change and human pressures. Process-based models provide physicalrealism but can suffer from structural rigidity, high computational costs, andcomplex calibration, while machine learning (ML) methods are efficient andflexible yet often lack interpretability and transferability. We propose aunified three-phase framework that integrates process-based models with ML andprogressively embeds them into artificial intelligence (AI) through knowledgedistillation. Phase I, behavioral distillation, enhances process models viasurrogate learning and model simplification to capture key dynamics at lowercomputational cost. Phase II, structural distillation, reformulates processequations as modular components within a graph neural network (GNN), enablingmultiscale representation and seamless integration with ML models. Phase III,cognitive distillation, embeds expert reasoning and adaptive decision-makinginto intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture.Demonstrations for the Samish watershed highlight the framework's applicabilityto ecohydrological modeling, showing that it can reproduce process-based modeloutputs, improve predictive accuracy, and support scenario-baseddecision-making. The framework offers a scalable and transferable pathwaytoward next-generation intelligent ecohydrological modeling systems, with thepotential extension to other process-based domains.</description>
      <author>example@mail.com (Long Jiang, Yang Yang, Ting Fong May Chui, Morgan Thornwell, Hoshin Vijai Gupta)</author>
      <guid isPermaLink="false">2509.01972v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Causal representation learning from network data</title>
      <link>http://arxiv.org/abs/2509.01916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GraCE-VAE的框架，用于从软干预中进行因果解缠，特别适用于非独立同分布数据场景，其中包含网络数据形式的结构化上下文。&lt;h4&gt;背景&lt;/h4&gt;先前研究主要从独立同分布数据的角度探讨因果解缠问题。在线性干预忠实度和可获取观测与干预数据的假设下，因果解缠是可识别的。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于非独立同分布设置的框架，整合结构化上下文（网络数据）来提升因果解缠的性能。&lt;h4&gt;方法&lt;/h4&gt;GraCE-VAE框架结合了基于差异的变分自编码器与图神经网络，能够同时恢复真实的潜在因果图和干预效果。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明，从独立同分布数据中得出的可识别性理论结果在非独立同分布设置中仍然成立。实验评估表明，利用结构化上下文可以显著提升因果解缠的性能。&lt;h4&gt;结论&lt;/h4&gt;GraCE-VAE框架成功地将因果解缠扩展到非独立同分布数据场景，通过整合网络结构信息，在基因扰动数据集上表现出优于现有方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;从软干预中进行因果解缠在线性干预忠实度假设和观测与干预数据可获取的情况下是可识别的。先前研究从独立同分布数据的角度探讨了这一问题。本文提出了GraCE-VAE框架，用于非独立同分布设置，其中存在网络数据形式的结构化上下文。GraCE-VAE将基于差异的变分自编码器与图神经网络相结合，共同恢复真实的潜在因果图和干预效果。我们展示了从独立同分布数据得出的可识别性理论结果在我们的设置中仍然成立。我们还在三个基因扰动数据集上对GraCE-VAE与最先进的基线进行了实证评估，展示了利用结构化上下文进行因果解缠的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal disentanglement from soft interventions is identifiable under theassumptions of linear interventional faithfulness and availability of bothobservational and interventional data. Previous research has looked into thisproblem from the perspective of i.i.d. data. Here, we develop a framework,GraCE-VAE, for non-i.i.d. settings, in which structured context in the form ofnetwork data is available. GraCE-VAE integrates discrepancy-based variationalautoencoders with graph neural networks to jointly recover the true latentcausal graph and intervention effects. We show that the theoretical results ofidentifiability from i.i.d. data hold in our setup. We also empiricallyevaluate GraCE-VAE against state-of-the-art baselines on three geneticperturbation datasets to demonstrate the impact of leveraging structuredcontext for causal disentanglement.</description>
      <author>example@mail.com (Jifan Zhang, Michelle M. Li, Elena Zheleva)</author>
      <guid isPermaLink="false">2509.01916v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Single-Cell RNA-seq Embedding through Gene Expression and Data-Driven Gene-Gene Interaction Integration</title>
      <link>http://arxiv.org/abs/2509.02639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 9 figures, article&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的单细胞RNA测序数据嵌入方法，整合基因表达水平和基因间相互作用，提高了细胞分析的准确性和全面性。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序技术为研究细胞异质性提供了前所未有的机会，但数据的高维度和技术噪声带来了分析挑战。现有方法主要关注基因表达水平，忽视了基因间相互作用对细胞身份和功能的关键影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时考虑基因表达水平和基因间相互作用的嵌入方法，提供更全面的细胞状态表示。&lt;h4&gt;方法&lt;/h4&gt;结合随机森林模型构建细胞-叶图(CLG)捕获基因调控关系，同时构建K-近邻图(KNNG)表示细胞间表达相似性，将两者组合成增强的细胞-叶图(ECLG)作为图神经网络的输入，计算细胞嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个数据集上表现出色，显著提高了稀有细胞群的检测能力，并改善了可视化、聚类和轨迹推断等下游分析效果。&lt;h4&gt;结论&lt;/h4&gt;这种集成基因表达和相互作用的方法代表了单细胞数据分析的重要进展，为理解细胞多样性和动态变化提供了更完整的框架。&lt;h4&gt;翻译&lt;/h4&gt;单细胞RNA测序(scRNA-seq)为细胞异质性提供了前所未有的见解，使能够在单细胞分辨率下详细分析复杂的生物系统。然而，scRNA-seq数据固有的高维度和技术噪声带来了显著的分析挑战。虽然当前的嵌入方法主要关注基因表达水平，但它们常常忽视了控制细胞身份和功能的关键基因间相互作用。为了解决这一局限，我们提出了一种新的嵌入方法，整合了基因表达谱和数据驱动的基因间相互作用。我们的方法首先使用随机森林模型构建细胞-叶图(CLG)以捕获基因之间的调控关系，同时构建K-近邻图(KNNG)来表示细胞之间的表达相似性。然后将这些图组合成增强的细胞-叶图(ECLG)，作为图神经网络的输入以计算细胞嵌入。通过整合表达水平和基因间相互作用，我们的方法提供了细胞状态的更全面表示。在多个数据集上的广泛评估表明，我们的方法增强了稀有细胞群的检测，并改善了可视化、聚类和轨迹推断等下游分析。这种集成方法代表了单细胞数据分析的重大进展，为理解细胞多样性和动态变化提供了更完整的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.compbiomed.2025.109880&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights intocellular heterogeneity, enabling detailed analysis of complex biologicalsystems at single-cell resolution. However, the high dimensionality andtechnical noise inherent in scRNA-seq data pose significant analyticalchallenges. While current embedding methods focus primarily on gene expressionlevels, they often overlook crucial gene-gene interactions that govern cellularidentity and function. To address this limitation, we present a novel embeddingapproach that integrates both gene expression profiles and data-drivengene-gene interactions. Our method first constructs a Cell-Leaf Graph (CLG)using random forest models to capture regulatory relationships between genes,while simultaneously building a K-Nearest Neighbor Graph (KNNG) to representexpression similarities between cells. These graphs are then combined into anEnriched Cell-Leaf Graph (ECLG), which serves as input for a graph neuralnetwork to compute cell embeddings. By incorporating both expression levels andgene-gene interactions, our approach provides a more comprehensiverepresentation of cellular states. Extensive evaluation across multipledatasets demonstrates that our method enhances the detection of rare cellpopulations and improves downstream analyses such as visualization, clustering,and trajectory inference. This integrated approach represents a significantadvance in single-cell data analysis, offering a more complete framework forunderstanding cellular diversity and dynamics.</description>
      <author>example@mail.com (Hojjat Torabi Goudarzi, Maziyar Baran Pouyan)</author>
      <guid isPermaLink="false">2509.02639v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring</title>
      <link>http://arxiv.org/abs/2509.01640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TransGAT是一种创新的自动作文评分方法，通过结合微调的Transformer模型与图神经网络(GNN)进行分析性评分，解决了现有方法在捕捉上下文意义和评估具体写作方面的局限性，实验证明其性能优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;作文写作是学生评估的关键组成部分，但人工评分劳动量大且不一致。自动作文评分(AES)虽有前景，但当前方法存在局限性，包括使用无法捕捉上下文意义的静态词嵌入，以及依赖整体评分而忽略具体写作方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉上下文意义并评估具体写作方面的自动作文评分方法，解决现有AES方法的两个主要问题：静态词嵌入无法捕捉多义词的上下文意义，以及整体评分忽略了语法、词汇和连贯性等具体写作方面。&lt;h4&gt;方法&lt;/h4&gt;提出TransGAT，一种结合微调Transformer模型与图神经网络(GNN)的新方法。TransGAT将Transformer的上下文理解与图注意力网络(GAT)的关系建模能力相结合，采用双流预测：每个Transformer(BERT、RoBERTa和DeBERTaV3)与单独的GAT配对，第一个流生成作文级别预测，第二个流将GAT应用于Transformer标记嵌入（边从句法依赖关系构建），最后融合两个流的预测产生最终分析性分数。&lt;h4&gt;主要发现&lt;/h4&gt;在ELLIPSE数据集上的实验表明，TransGAT优于基线模型，在所有分析性评分维度上平均实现了0.854的二次加权Kappa(QWK)值。&lt;h4&gt;结论&lt;/h4&gt;TransGAT有潜力推动自动作文评分系统的发展，为教育评估提供更准确、一致的分析性评分。&lt;h4&gt;翻译&lt;/h4&gt;作文写作是学生评估的关键组成部分，然而人工评分劳动量大且不一致。自动作文评分(AES)提供了一个有前景的替代方案，但当前方法存在局限性。最近的研究已将图神经网络(GNN)纳入AES，但使用的静态词嵌入无法捕捉上下文意义，特别是对多义词而言。此外，许多方法依赖整体评分，忽略了语法、词汇和连贯性等具体写作方面。为解决这些挑战，本研究提出了TransGAT，一种将微调Transformer模型与GNN结合用于分析性评分的新方法。TransGAT结合了Transformer的上下文理解和图注意力网络(GAT)的关系建模能力。它通过将每个微调的Transformer(BERT、RoBERTa和DeBERTaV3)与单独的GAT配对进行双流预测。在每对中，第一个流生成作文级别的预测，而第二个流将GAT应用于Transformer标记嵌入，边从句法依赖关系构建。然后模型融合两个流的预测以产生最终的分析性分数。在ELLIPSE数据集上的实验表明，TransGAT优于基线模型，在所有分析性评分维度上平均实现了0.854的二次加权Kappa(QWK)。这些发现突显了TransGAT推动AES系统发展的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Essay writing is a critical component of student assessment, yet manualscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)offers a promising alternative, but current approaches face limitations. Recentstudies have incorporated Graph Neural Networks (GNNs) into AES using staticword embeddings that fail to capture contextual meaning, especially forpolysemous words. Additionally, many methods rely on holistic scoring,overlooking specific writing aspects such as grammar, vocabulary, and cohesion.To address these challenges, this study proposes TransGAT, a novel approachthat integrates fine-tuned Transformer models with GNNs for analytic scoring.TransGAT combines the contextual understanding of Transformers with therelational modeling strength of Graph Attention Networks (GAT). It performstwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,and DeBERTaV3) with a separate GAT. In each pair, the first stream generatesessay-level predictions, while the second applies GAT to Transformer tokenembeddings, with edges constructed from syntactic dependencies. The model thenfuses predictions from both streams to produce the final analytic score.Experiments on the ELLIPSE dataset show that TransGAT outperforms baselinemodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across allanalytic scoring dimensions. These findings highlight the potential of TransGATto advance AES systems.</description>
      <author>example@mail.com (Hind Aljuaid, Areej Alhothali, Ohoud Al-Zamzami, Hussein Assalahi)</author>
      <guid isPermaLink="false">2509.01640v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>What Expressivity Theory Misses: Message Passing Complexity for GNNs</title>
      <link>http://arxiv.org/abs/2509.01254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究批判了当前图神经网络研究中过度关注表达性的倾向，提出了一种新的消息传递复杂度(MPC)框架，更好地连接理论与实践，为理解和改进GNN架构提供了更强大而细致的工具。&lt;h4&gt;背景&lt;/h4&gt;表达性理论已成为分析图神经网络的主要框架，新的模型不断追求更高的表达性，能够区分更多类型的图结构。&lt;h4&gt;目的&lt;/h4&gt;指出表达性理论的局限性，并提出一种更符合实际应用的评估方法，以弥补理论与实践之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出消息传递复杂度(MPC)作为连续性度量，量化GNN架构通过消息传递解决特定任务的难度，同时保留表达性理论的理论不可能性结果。&lt;h4&gt;主要发现&lt;/h4&gt;MPC能够捕捉实际限制如过度压缩(over-squashing)，其理论预测与实际性能相关，成功解释了不同架构的成功与失败。&lt;h4&gt;结论&lt;/h4&gt;MPC超越了表达性理论，为理解和改进图神经网络架构提供了更强大而细致的框架。&lt;h4&gt;翻译&lt;/h4&gt;表达性理论，表征GNN能够区分哪些图，已成为分析GNN的主导框架，新模型追求更高的表达性。然而，我们认为这种关注是误导性的：首先，对于大多数实际任务，更高的表达性并非必需，因为这些任务很少需要超越基本WL测试的表达性。其次，表达性理论的二元表征和理想化假设未能反映GNN的实际能力。为克服这些局限，我们提出消息传递复杂度(MPC)：一种连续度量，量化GNN架构通过消息传递解决给定任务的难度。MPC捕获了过度压缩等实际限制，同时保留表达性理论的理论不可能性结果，有效缩小了理论与实践之间的差距。通过对基本GNN任务的广泛验证，我们表明MPC的理论预测与实证性能相关，成功解释了架构的成功与失败。因此，MPC超越了表达性理论，为理解和改进GNN架构提供了更强大而细致的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Expressivity theory, characterizing which graphs a GNN can distinguish, hasbecome the predominant framework for analyzing GNNs, with new models strivingfor higher expressivity. However, we argue that this focus is misguided: First,higher expressivity is not necessary for most real-world tasks as these tasksrarely require expressivity beyond the basic WL test. Second, expressivitytheory's binary characterization and idealized assumptions fail to reflectGNNs' practical capabilities. To overcome these limitations, we propose MessagePassing Complexity (MPC): a continuous measure that quantifies the difficultyfor a GNN architecture to solve a given task through message passing. MPCcaptures practical limitations like over-squashing while preserving thetheoretical impossibility results from expressivity theory, effectivelynarrowing the gap between theory and practice. Through extensive validation onfundamental GNN tasks, we show that MPC's theoretical predictions correlatewith empirical performance, successfully explaining architectural successes andfailures. Thereby, MPC advances beyond expressivity theory to provide a morepowerful and nuanced framework for understanding and improving GNNarchitectures.</description>
      <author>example@mail.com (Niklas Kemper, Tom Wollschläger, Stephan Günnemann)</author>
      <guid isPermaLink="false">2509.01254v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ADMP-GNN: Adaptive Depth Message Passing GNN</title>
      <link>http://arxiv.org/abs/2509.01170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络在各种图学习任务中表现优异，但其对所有节点使用固定数量的消息传递步骤，忽略了节点的差异性。研究证明不同特性的节点需要不同层数的消息传递，为此提出ADMP-GNN框架，动态调整每个节点的消息传递层数，在节点分类任务中展现出优于基准GNN模型的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已被证明在各种图学习任务中非常有效。&lt;h4&gt;目的&lt;/h4&gt;解决GNN对所有节点使用固定数量消息传递步骤的问题，根据节点不同特性动态调整消息传递层数，以提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出自适应深度消息传递图神经网络(ADMP-GNN)框架，该框架动态调整每个节点的消息传递层数，适用于任何遵循消息传递方案的模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界数据分析和合成数据集实验发现，具有不同特性的节点需要不同数量的消息传递层才能达到最佳性能。&lt;h4&gt;结论&lt;/h4&gt;ADMP-GNN通过动态调整每个节点的消息传递层数，在节点分类任务中表现出比基准GNN模型更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已被证明在各种图学习任务中非常有效。GNN的一个关键特征是它们对图中所有节点使用固定数量的消息传递步骤，而不管每个节点不同的计算需求和特性。通过对真实世界数据的经验分析，我们证明了对于具有不同特性的节点，最佳的消息传递层数量是不同的。这一发现也在合成数据集的实验中得到了进一步支持。为此，我们提出了自适应深度消息传递图神经网络(ADMP-GNN)，这是一个新框架，可以动态调整每个节点的消息传递层数，从而提高性能。这种方法适用于任何遵循消息传递方案的模型。我们在节点分类任务上评估了ADMP-GNN，并观察到其性能优于基准GNN模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have proven to be highly effective in variousgraph learning tasks. A key characteristic of GNNs is their use of a fixednumber of message-passing steps for all nodes in the graph, regardless of eachnode's diverse computational needs and characteristics. Through empiricalreal-world data analysis, we demonstrate that the optimal number ofmessage-passing layers varies for nodes with different characteristics. Thisfinding is further supported by experiments conducted on synthetic datasets. Toaddress this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novelframework that dynamically adjusts the number of message passing layers foreach node, resulting in improved performance. This approach applies to anymodel that follows the message passing scheme. We evaluate ADMP-GNN on the nodeclassification task and observe performance improvements over baseline GNNmodels.</description>
      <author>example@mail.com (Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Michalis Vazirgiannis)</author>
      <guid isPermaLink="false">2509.01170v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection</title>
      <link>http://arxiv.org/abs/2509.01153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的框架，用于呼吸音事件检测，解决了现有方法在处理可变长度音频和精确时间定位方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;听诊是诊断呼吸系统和肺部疾病的关键方法，但过程主观且专家间存在差异。虽然已有基于深度学习的呼吸音分类方法，但呼吸音事件检测研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有呼吸音事件检测方法的局限性，包括难以直接学习区间边界、只能处理固定长度音频，以及未充分利用呼吸音位置信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于图神经网络的框架，带有锚定区间，能够处理可变长度音频并为异常呼吸音事件提供更精确的时间定位。&lt;h4&gt;主要发现&lt;/h4&gt;在SPRSound2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，整合呼吸位置信息提高了异常声音之间的区分度。&lt;h4&gt;结论&lt;/h4&gt;该方法提高了呼吸音检测的灵活性和适用性，为临床呼吸系统疾病的早期诊断提供了更客观、准确的工具。&lt;h4&gt;翻译&lt;/h4&gt;听诊是诊断呼吸系统和肺部疾病的关键方法，依赖于熟练的医疗专业人员。然而，这个过程通常是主观的，专家之间存在差异。因此，出现了许多基于深度学习的自动分类方法，大多数专注于呼吸音分类。相比之下，呼吸音事件检测的研究仍然有限。现有的声音事件检测方法通常依赖于帧级预测，然后通过后处理生成事件级输出，使得区间边界难以直接学习。此外，许多方法只能处理固定长度的音频，限制了它们对可变长度呼吸音的适用性。此外，呼吸音位置信息对检测性能的影响尚未得到充分探索。为了解决这些问题，我们提出了一种基于图神经网络的框架，带有锚定区间，能够处理可变长度音频，并为异常呼吸音事件提供更精确的时间定位。我们的方法提高了呼吸音检测的灵活性和适用性。在SPRSound2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，整合呼吸位置信息提高了异常声音之间的区分度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.bspc.2025.108491&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auscultation is a key method for early diagnosis of respiratory and pulmonarydiseases, relying on skilled healthcare professionals. However, the process isoften subjective, with variability between experts. As a result, numerous deeplearning-based automatic classification methods have emerged, most of whichfocus on respiratory sound classification. In contrast, research on respiratorysound event detection remains limited. Existing sound event detection methodstypically rely on frame-level predictions followed by post-processing togenerate event-level outputs, making interval boundaries challenging to learndirectly. Furthermore, many approaches can only handle fixed-length audio, lim-iting their applicability to variable-length respiratory sounds. Additionally,the impact of respiratory sound location information on detection performancehas not been extensively explored. To address these issues, we propose a graphneural network-based framework with anchor intervals, capable of handlingvariable-length audio and providing more precise temporal localization forabnormal respi- ratory sound events. Our method improves both the flexibilityand applicability of respiratory sound detection. Experiments on the SPRSound2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposedapproach, and incorporating respiratory position information enhances thediscrimination between abnormal sounds.</description>
      <author>example@mail.com (Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng)</author>
      <guid isPermaLink="false">2509.01153v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.00975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG)，一个强化学习框架，用于微调大型语言模型以在真实世界时序图上进行可解释的链接预测。&lt;h4&gt;背景&lt;/h4&gt;时序图推理中的未来链接预测需要模型利用历史交互预测未来交互。传统神经网络方法表现良好但缺乏可解释性，且不能应用于未见过的图。最近研究开始探索使用大型语言模型进行图推理，但大多局限于静态图或小型合成时序图，且缺乏对LLM生成推理质量的评估。&lt;h4&gt;目的&lt;/h4&gt;开发一个强化学习框架，使大型语言模型能够对真实世界时序图进行可解释的链接预测，并评估其推理质量。&lt;h4&gt;方法&lt;/h4&gt;ReaL-TG使用基于结果的奖励鼓励模型从图结构中自我探索推理策略并生成直接证明预测的解释。提出新的评估协议，结合排名指标和'LLM-as-a-Judge'系统，评估推理质量和幻觉影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过微调Qwen3-4B得到的ReaL-TG-4B在排名指标上优于包括GPT-5 mini在内的更大前沿LLMs，同时产生高质量解释，这些解释得到了LLM判断和人工评估的确认。&lt;h4&gt;结论&lt;/h4&gt;ReaL-TG框架成功使大型语言模型能够进行可解释的时序图链接预测，在性能和解释质量上都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;预测未来链接是时序图推理中的核心任务，需要模型利用历史交互来预测未来的交互。传统的神经方法，如时序图神经网络，表现强劲但缺乏可解释性，且不能应用于未见过的图而不需要重新训练。最近的研究开始探索使用大型语言模型进行图推理，但它们大多局限于静态图或小型合成时序图，且缺乏对LLM生成的推理质量的评估。在这项工作中，我们提出了时序图的推理增强学习(ReaL-TG)，这是一个强化学习框架，用于微调LLMs以在真实世界时序图上进行可解释的链接预测。ReaL-TG使用基于结果的奖励来鼓励模型从图结构中自我探索推理策略，并生成直接证明其预测的解释。为了评估LLM生成的推理轨迹，我们提出了一种新的评估协议，结合排名指标和'LLM-as-a-Judge'系统，该系统评估推理质量和幻觉的影响。通过在我们的框架下微调Qwen3-4B得到的ReaL-TG-4B，在排名指标上优于包括GPT-5 mini在内的更大前沿LLMs，同时产生高质量解释，这些解释得到了LLM判断和人工评估的确认。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting future links is a central task in temporal graph (TG) reasoning,requiring models to leverage historical interactions to predict upcoming ones.Traditional neural approaches, such as temporal graph neural networks, achievestrong performance but lack explainability and cannot be applied to unseengraphs without retraining. Recent studies have begun to explore using largelanguage models (LLMs) for graph reasoning, but most of them are constrained tostatic graphs or small synthetic TGs and lack the evaluation of the quality ofreasoning traces generated by LLMs. In this work, we present Reasoning-EnhancedLearning for Temporal Graphs (ReaL-TG), a reinforcement learning framework thatfine-tunes LLMs to perform explainable link forecasting on real-world TGs.ReaL-TG uses outcome-based reward to encourage models to self-explore reasoningstrategies from graph structure and to produce explanations that directlyjustify their predictions. To enable evaluation on LLM-generated reasoningtraces, we propose a new evaluation protocol combining ranking metrics with anLLM-as-a-Judge system that assesses both the quality of reasoning and theimpact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuningQwen3-4B under our framework, show that it outperforms much larger frontierLLMs, including GPT-5 mini, on ranking metrics, while producing high-qualityexplanations confirmed by both the LLM judge and human evaluation.</description>
      <author>example@mail.com (Zifeng Ding, Shenyang Huang, Zeyu Cao, Emma Kondrup, Zachary Yang, Xingyue Huang, Yuan Sui, Zhangdie Yuan, Yuqicheng Zhu, Xianglong Hu, Yuan He, Farimah Poursafaei, Michael Bronstein, Andreas Vlachos)</author>
      <guid isPermaLink="false">2509.00975v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Superposition in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.00928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究图神经网络(GNNs)潜在空间中的叠加现象，发现宽度增加会产生重叠相位模式，拓扑结构影响节点级特征，池化方式影响轴对齐和通道共享，浅层模型可能形成低秩嵌入。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)的解释性困难，因为消息传递混合了信号，内部通道很少与人类概念对齐。&lt;h4&gt;目的&lt;/h4&gt;研究GNN潜在空间中的叠加现象，即多个特征共享方向的情况。&lt;h4&gt;方法&lt;/h4&gt;使用具有明确图概念的控制实验，提取图级别的类条件质心和节点级别的线性探测方向，然后用简单的基不变诊断分析它们的几何结构。&lt;h4&gt;主要发现&lt;/h4&gt;在GCN/GIN/GAT模型中：增加宽度产生重叠的相位模式；拓扑结构将重叠印记到节点级特征中；池化部分地将特征重混合为任务对齐的图轴；更尖锐的池化增加轴对齐并减少通道共享；浅层模型可以陷入亚稳定的低秩嵌入。&lt;h4&gt;结论&lt;/h4&gt;这些结果将表示几何与具体的设计选择（宽度、池化和最终层激活）联系起来，并为更可解释的GNNs提出实用的方法。&lt;h4&gt;翻译&lt;/h4&gt;解释图神经网络(GNNs)很困难，因为消息传递混合了信号，内部通道很少与人类概念对齐。我们直接研究GNN潜在空间中的叠加现象，即多个特征共享方向的情况。使用具有明确图概念的控制实验，我们将特征提取为(i)图级别的类条件质心和(ii)节点级别的线性探测方向，然后使用简单的基不变诊断分析它们的几何结构。在GCN/GIN/GAT中我们发现：增加宽度会产生重叠的相位模式；拓扑结构将重叠印记到节点级特征中，池化部分地将这些特征重混合为任务对齐的图轴；更尖锐的池化增加轴对齐并减少通道共享；浅层模型可以陷入亚稳定的低秩嵌入。这些结果将表示几何与具体的设计选择（宽度、池化和最终层激活）联系起来，并提出了使GNNs更具可解释性的实用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpreting graph neural networks (GNNs) is difficult because messagepassing mixes signals and internal channels rarely align with human concepts.We study superposition, the sharing of directions by multiple features,directly in the latent space of GNNs. Using controlled experiments withunambiguous graph concepts, we extract features as (i) class-conditionalcentroids at the graph level and (ii) linear-probe directions at the nodelevel, and then analyze their geometry with simple basis-invariant diagnostics.Across GCN/GIN/GAT we find: increasing width produces a phase pattern inoverlap; topology imprints overlap onto node-level features that poolingpartially remixes into task-aligned graph axes; sharper pooling increases axisalignment and reduces channel sharing; and shallow models can settle intometastable low-rank embeddings. These results connect representational geometrywith concrete design choices (width, pooling, and final-layer activations) andsuggest practical approaches for more interpretable GNNs.</description>
      <author>example@mail.com (Lukas Pertl, Han Xuanyuan, Pietro Liò)</author>
      <guid isPermaLink="false">2509.00928v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Flow Matters: Directional and Expressive GNNs for Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2509.00772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了边方向性和表达性消息传递在异质图节点分类中的组合效应，提出两种架构：多项式表达性GAT基线模型(Poly)和方向感知变体(Dir-Poly)。实验表明，Poly模型在五个基准异质数据集上优于现有基线，Dir-Poly在有固有方向性的图上提供额外提升。研究发现方向性消息传递的好处是上下文依赖的。&lt;h4&gt;背景&lt;/h4&gt;在异质图中，相邻节点通常属于不同类别，传统图神经网络(GNNs)因其依赖局部同质邻域而表现不佳。先前研究表明建模边方向性可增加有效同质性并提高分类性能，而多项式表达性GNNs在捕获特征间高阶交互方面显示出潜力。&lt;h4&gt;目的&lt;/h4&gt;研究边方向性和表达性消息传递在异质图节点分类中的组合效应，探索边方向性和多项式表达性的结合如何提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出两种架构：1)多项式表达性GAT基线模型(Poly)；2)方向感知变体(Dir-Poly)，分别聚合传入和传出边。两种模型都学习输入特征上的置换等变高阶多项式，保持可扩展性且不增加时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准异质图数据集上，Poly模型始终优于现有基线；Dir-Poly在有固有方向性的图上提供额外提升，达到最先进结果；在无向图上，引入人工方向性并不总是有帮助，表明方向性消息传递的好处是上下文依赖的。&lt;h4&gt;结论&lt;/h4&gt;边方向性和表达性特征建模在异质图学习中起着互补作用，两者的结合可有效提高节点分类性能，但方向性消息传递的效果依赖于图的上下文特性。&lt;h4&gt;翻译&lt;/h4&gt;在异质图中，相邻节点通常属于不同类别，传统的图神经网络(GNNs)因其依赖局部同质邻域而表现不佳。先前研究表明，在这样的图中建模边方向性可以增加有效同质性并提高分类性能。同时，最近关于多项式表达性GNNs的研究在捕获特征间高阶交互方面显示出潜力。在本工作中，我们研究了边方向性和表达性消息传递在异质图节点分类中的组合效应。具体来说，我们提出了两种架构：(1)多项式表达性GAT基线模型(Poly)，和(2)方向感知变体(Dir-Poly)，分别聚合传入和传出边。两种模型都旨在学习输入特征上的置换等变高阶多项式，同时保持可扩展性且不增加时间复杂度。在五个基准异质图数据集上的实验表明，我们的Poly模型始终优于现有基线，而Dir-Poly在有固有方向性的图上(如罗马帝国图)提供了额外提升，达到了最先进的结果。有趣的是，在无向图上，引入人工方向性并不总是有帮助，这表明方向性消息传递的好处是依赖于上下文的。我们的研究突显了边方向和表达性特征建模在异质图学习中的互补作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In heterophilic graphs, where neighboring nodes often belong to differentclasses, conventional Graph Neural Networks (GNNs) struggle due to theirreliance on local homophilous neighborhoods. Prior studies suggest thatmodeling edge directionality in such graphs can increase effective homophilyand improve classification performance. Simultaneously, recent work onpolynomially expressive GNNs shows promise in capturing higher-orderinteractions among features. In this work, we study the combined effect of edgedirectionality and expressive message passing on node classification inheterophilic graphs. Specifically, we propose two architectures: (1) apolynomially expressive GAT baseline (Poly), and (2) a direction-aware variant(Dir-Poly) that separately aggregates incoming and outgoing edges. Both modelsare designed to learn permutation-equivariant high-degree polynomials overinput features, while remaining scalable with no added time complexity.Experiments on five benchmark heterophilic datasets show that our Poly modelconsistently outperforms existing baselines, and that Dir-Poly offersadditional gains on graphs with inherent directionality (e.g., Roman Empire),achieving state-of-the-art results. Interestingly, on undirected graphs,introducing artificial directionality does not always help, suggesting that thebenefit of directional message passing is context-dependent. Our findingshighlight the complementary roles of edge direction and expressive featuremodeling in heterophilic graph learning.</description>
      <author>example@mail.com (Arman Gupta, Govind Waghmare, Gaurav Oberoi, Nitish Srivastava)</author>
      <guid isPermaLink="false">2509.00772v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient Approach For Continual Graph Learning</title>
      <link>http://arxiv.org/abs/2509.00735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为任务感知自适应调制(TAAM)的持续图学习方法，通过神经突触调制器(NSM)动态调整冻结骨干网络的内部计算流，解决了稳定性-可塑性困境和资源密集型预训练的问题。&lt;h4&gt;背景&lt;/h4&gt;持续图学习(CGL)关注在获取新知识的同时保留已学习的信息，这对现实世界的图应用至关重要。当前方法面临两个主要问题：1)稳定性-可塑性困境：基于回放的方法通常在这两者之间造成不平衡，并且需要大量存储成本；2)资源密集型预训练：领先的免回放方法严重依赖广泛预训练的主干网络，这种依赖带来了巨大的资源负担。&lt;h4&gt;目的&lt;/h4&gt;克服持续图学习中的稳定性-可塑性困境和资源密集型预训练问题，不需要回放数据或微调整个网络，而是通过动态调整冻结骨干网络的内部计算流。&lt;h4&gt;方法&lt;/h4&gt;提出任务感知自适应调制(TAAM)，这是一种免回放、资源高效的方法，其核心是神经突触调制器(NSM)。这些调制器为每个任务训练并冻结，以存储专家知识。一个关键的原型引导策略控制这些调制器：1)对于训练：通过从相似的过去调制器深度复制来初始化新的NSM，以促进知识转移；2)对于推理：为每个任务选择最相关的冻结NSM。这些NSM插入到冻结的GNN骨干网络中，对其内部流进行细粒度的、节点感知的调制。&lt;h4&gt;主要发现&lt;/h4&gt;TAAM在六个GCIL基准数据集上全面优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;TAAM为解决稳定性-可塑性困境开辟了新路径，是一种资源高效的持续图学习方法。&lt;h4&gt;翻译&lt;/h4&gt;持续图学习(CGL)专注于在获取新知识的同时保留已学习的信息，这对现实世界的图应用至关重要。当前方法面临两个主要问题：1)稳定性-可塑性困境：基于回放的方法通常在这两者之间造成不平衡，同时产生显著的存储成本。2)资源密集型预训练：领先的免回放方法严重依赖广泛预训练的主干网络，这种依赖带来了巨大的资源负担。在本文中，我们认为克服这些挑战的关键不在于回放数据或微调整个网络，而在于动态调整冻结骨干网络的内部计算流。我们认为轻量级的、任务特定的模块可以有效引导GNN的推理过程。受此见解启发，我们提出了任务感知自适应调制(TAAM)，这是一种免回放、资源高效的方法，为解决稳定性-可塑性困境开辟了新路径。TAAM的核心是其神经突触调制器(NSM)，这些调制器为每个任务训练并冻结，以存储专家知识。一个关键的原型引导策略控制这些调制器：1)对于训练，它通过从相似的过去调制器深度复制来初始化新的NSM，以促进知识转移。2)对于推理，它为每个任务选择最相关的冻结NSM。这些NSM插入到冻结的GNN骨干网络中，对其内部流进行细粒度的、节点感知的调制——这与先前方法的静态扰动不同。大量实验表明，TAAM在六个GCIL基准数据集上全面优于最先进的方法。代码将在论文接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Graph Learning(CGL)focuses on acquiring new knowledge whileretaining previously learned information, essential for real-world graphapplications. Current methods grapple with two main issues:1) TheStability-Plasticity Dilemma: Replay-based methods often create an imbalancebetween the Dilemma, while incurring significant storage costs.2) TheResource-Heavy Pre-training: Leading replay-free methods critically depend onextensively pre-trained backbones, this reliance imposes a substantial resourceburden.In this paper, we argue that the key to overcoming these challenges liesnot in replaying data or fine-tuning the entire network, but in dynamicallymodulating the internal computational flow of a frozen backbone. We posit thatlightweight, task-specific modules can effectively steer a GNN's reasoningprocess. Motivated by this insight, we propose Task-Aware AdaptiveModulation(TAAM), a replay-free, resource-efficient approach that charts a newpath for navigating the stability-plasticity dilemma. TAAM's core is its NeuralSynapse Modulators(NSM), which are trained and then frozen for each task tostore expert knowledge. A pivotal prototype-guided strategy governs thesemodulators: 1) For training, it initializes a new NSM by deep-copying from asimilar past modulator to boost knowledge transfer. 2) For inference, itselects the most relevant frozen NSM for each task. These NSMs insert into afrozen GNN backbone to perform fine-grained, node-attentive modulation of itsinternal flow-different from the static perturbations of prior methods.Extensive experiments show that TAAM comprehensively outperformsstate-of-the-art methods across six GCIL benchmark datasets. The code will bereleased upon acceptance of the paper.</description>
      <author>example@mail.com (Jingtao Liu, Xinming Zhang)</author>
      <guid isPermaLink="false">2509.00735v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition</title>
      <link>http://arxiv.org/abs/2509.00703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review in IEEE Signal Processing Letter&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种模态自适应图网络(MAGN)，通过将迭代变分模态分解转化为可训练神经模块，解决了传统图神经网络在时空预测中的计算效率低下和手动超参数调优问题。&lt;h4&gt;背景&lt;/h4&gt;准确的时空预测对许多复杂系统至关重要，但由于传统图神经网络中的复杂波动模式和频谱纠缠，这仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决分解集成方法如VMGCN的计算效率低下和手动超参数调优问题。&lt;h4&gt;方法&lt;/h4&gt;提出模态自适应图网络(MAGN)，包括：(1)展开的VMD(UVMD)模块，用固定深度网络替代迭代优化，大幅减少分解时间；(2)模态特定的可学习带宽约束，适应空间异质性，消除手动调优并防止频谱重叠。&lt;h4&gt;主要发现&lt;/h4&gt;在LargeST基准(6,902个传感器，2.41亿个观测值)上，MAGN比VMGCN减少了85-95%的预测误差，并优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;MAGN有效解决了传统图神经网络在时空预测中的计算效率和调优问题，显著提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的时空预测对许多复杂系统至关重要，但由于传统图神经网络中的复杂波动模式和频谱纠缠，这仍然具有挑战性。虽然变分模态图卷积网络等分解集成方法通过信号分解提高了准确性，但它们存在计算效率低下和手动超参数调优的问题。为解决这些限制，我们提出了模态自适应图网络(MAGN)，将迭代变分模态分解转化为可训练的神经模块。我们的主要创新包括：(1)展开的VMD(UVMD)模块，用固定深度网络替代迭代优化，减少了分解时间(在LargeST基准上减少250倍)；(2)模态特定的可学习带宽约束，适应空间异质性，消除手动调优，同时防止频谱重叠。在LargeST基准上评估，MAGN实现了比VMGCN减少85-95%的预测误差，并优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate spatiotemporal forecasting is critical for numerous complex systemsbut remains challenging due to complex volatility patterns and spectralentanglement in conventional graph neural networks (GNNs). Whiledecomposition-integrated approaches like variational mode graph convolutionalnetwork (VMGCN) improve accuracy through signal decomposition, they suffer fromcomputational inefficiency and manual hyperparameter tuning. To address theselimitations, we propose the mode adaptive graph network (MAGN) that transformsiterative variational mode decomposition (VMD) into a trainable neural module.Our key innovations include (1) an unfolded VMD (UVMD) module that replacesiterative optimization with a fixed-depth network to reduce the decompositiontime (by 250x for the LargeST benchmark), and (2) mode-specific learnablebandwidth constraints ({\alpha}k ) adapt spatial heterogeneity and eliminatemanual tuning while preventing spectral overlap. Evaluated on the LargeSTbenchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reductionin the prediction error over VMGCN and outperforms state-of-the-art baselines.</description>
      <author>example@mail.com (Osama Ahmad, Lukas Wesemann, Fabian Waschkowski, Zubair Khalid)</author>
      <guid isPermaLink="false">2509.00703v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Higgs Signal Strength Estimation with Machine Learning under Systematic Uncertainties</title>
      <link>http://arxiv.org/abs/2509.00672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 14 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于图神经网络的希格斯玻色子信号强度提取方法，考虑系统不确定性&lt;h4&gt;背景&lt;/h4&gt;高能物理实验中希格斯玻色子信号强度估计面临系统不确定性挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理系统不确定性的图神经网络方法，准确提取希格斯玻色子信号强度&lt;h4&gt;方法&lt;/h4&gt;采用双分支GNN架构，包括确定性GNN和不确定性感知GNN，通过门控注意力机制处理受系统效应影响的输入，并在训练时对nuisance参数配置进行采样和损失聚合&lt;h4&gt;主要发现&lt;/h4&gt;该方法在FAIR Universe Higgs Uncertainty Challenge数据集上实现了准确的信号强度估计和置信区间计算，在大规模伪实验中具有竞争性的覆盖率和区间宽度&lt;h4&gt;结论&lt;/h4&gt;所提出的SAGE框架能有效处理系统不确定性，准确估计希格斯玻色子信号强度，相关代码已公开&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种专用的基于图神经网络(GNN)的方法，用于提取希格斯玻色子信号强度μ，并纳入系统不确定性。该架构包含两个分支：一个确定性GNN处理不受nuisance参数影响的运动学变量，以及一个不确定性感知GNN，通过基于门控注意力的消息传递处理受系统效应调制的输入。它们的输出被融合以产生信号-背景分类的分数。在训练过程中，我们对nuisance参数配置进行采样并在这些配置上聚合损失，提高分类器在系统偏移下的稳定性，并有效将其输出与nuisance变化解耦。得到的分箱分类器输出用于构建泊松似然，实现对信号强度的轮廓似然扫描，其中nuisance参数通过数值优化被消去。我们在FAIR Universe Higgs Uncertainty Challenge数据集上验证了这一框架，实现了对信号强度μ及其68.27%置信区间的准确估计，在大规模伪实验中取得了具有竞争力的覆盖率和区间宽度。我们的代码'Systematics-Aware Graph Estimator'(SAGE)已公开可用&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a dedicated graph neural network (GNN)-based methodology for theextraction of the Higgs boson signal strength $\mu$, incorporating systematicuncertainties. The architecture features two branches: a deterministic GNN thatprocesses kinematic variables unaffected by nuisance parameters, and anuncertainty-aware GNN that handles inputs modulated by systematic effectsthrough gated attention-based message passing. Their outputs are fused toproduce classification scores for signal-background discrimination. Duringtraining we sample nuisance-parameter configurations and aggregate the lossacross them, promoting stability of the classifier under systematic shifts andeffectively decorrelating its outputs from nuisance variations. The resultingbinned classifier outputs are used to construct a Poisson likelihood, whichenables profile likelihood scans over signal strength, with nuisance parametersprofiled out via numerical optimization. We validate this framework on the FAIRUniverse Higgs Uncertainty Challenge dataset, yielding accurate estimation ofsignal strength $\mu$ and its 68.27\% confidence interval, achievingcompetitive coverage and interval widths in large-scale pseudo-experiments. Ourcode "Systematics-Aware Graph Estimator" (SAGE) is publicly available.</description>
      <author>example@mail.com (Minxuan He, Claudius Krause, Daohan Wang)</author>
      <guid isPermaLink="false">2509.00672v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment</title>
      <link>http://arxiv.org/abs/2509.00560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的知识蒸馏框架SA-DSD，用于解决从图神经网络(GNN)到更高效模型的知识转移问题，显著提升了边缘环境中的模型性能和效率。&lt;h4&gt;背景&lt;/h4&gt;知识蒸馏对于在资源受限的边缘环境中部署深度学习模型至关重要，特别是在消费电子领域如智能家居设备、可穿戴技术和移动终端。这些应用对模型压缩和推理速度有更高要求，需要将GNN知识转移到更高效的MLP模型。&lt;h4&gt;目的&lt;/h4&gt;解决MLP由于固定激活函数和全连接架构难以快速捕捉GNN学习的复杂邻域依赖关系的问题，提高边缘环境中模型的性能。&lt;h4&gt;方法&lt;/h4&gt;引入自注意力动态采样蒸馏(SA-DSD)框架，改进傅里叶KAN(FR-KAN)，用改进的FR-KAN+替代MLP作为学生模型。通过引入可学习的频率基和相移机制以及算法优化提高非线性拟合能力，同时构建基于教师-学生预测一致性的边缘级采样概率矩阵和自适应加权损失机制。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实数据集上的实验表明，SA-DSD比三个GNN教师模型实现3.05%-3.62%的性能提升，比FR-KAN+模型实现15.61%的性能提升，与关键基准模型相比实现了16.96倍的参数减少和55.75%的推理时间减少。&lt;h4&gt;结论&lt;/h4&gt;SA-DSD框架有效解决了从GNN到MLP知识蒸馏的局限性，显著提高了边缘环境中的模型性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;知识蒸馏(KD)对于在资源受限的边缘环境中部署深度学习模型至关重要，特别是在消费电子领域，包括智能家居设备、可穿戴技术和移动终端。这些应用对模型压缩和推理速度有更高要求，需要将图神经网络(GNN)的知识转移到更高效的多层感知器(MLP)模型。然而，由于MLP具有固定的激活函数和全连接架构，它们难以快速捕捉GNN学习的复杂邻域依赖关系，从而限制了它们在边缘环境中的性能。为解决这些局限性，本文引入了一种创新的知识蒸馏框架——自注意力动态采样蒸馏(SA-DSD)，用于从GNN到Kolmogorov-Arnold Networks (KANs)的知识转移。本研究改进了傅里叶KAN (FR-KAN)，并用改进的FR-KAN+替代MLP作为学生模型。通过引入可学习的频率基和相移机制以及算法优化，FR-KAN显著提高了其非线性拟合能力，同时有效降低了计算复杂度。在此基础上，构建了基于教师-学生预测一致性的边缘级采样概率矩阵，并设计了自适应加权损失机制，以减轻由于缺乏显式邻域聚合而导致的学生模型性能下降。在六个真实数据集上进行的大量实验表明，SA-DSD比三个GNN教师模型实现了3.05%-3.62%的性能提升，比FR-KAN+模型实现了15.61%的性能提升。此外，与关键基准模型相比，SA-DSD实现了16.96倍的参数减少和55.75%的推理时间减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge distillation (KD) is crucial for deploying deep learning models inresource-constrained edge environments, particularly within the consumerelectronics sector, including smart home devices, wearable technology, andmobile terminals. These applications place higher demands on model compressionand inference speed, necessitating the transfer of knowledge from Graph NeuralNetworks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However,due to their fixed activation functions and fully connected architecture, MLPsface challenges in rapidly capturing the complex neighborhood dependencieslearned by GNNs, thereby limiting their performance in edge environments. Toaddress these limitations, this paper introduces an innovative from GNNs toKolmogorov-Arnold Networks (KANs) knowledge distillation framework-SelfAttention Dynamic Sampling Distillation (SA-DSD). This study improved FourierKAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model.Through the incorporation of learnable frequency bases and phase-shiftmechanisms, along with algorithmic optimization, FR-KAN significantly improvesits nonlinear fitting capability while effectively reducing computationalcomplexity. Building on this, a margin-level sampling probability matrix, basedon teacher-student prediction consistency, is constructed, and an adaptiveweighted loss mechanism is designed to mitigate performance degradation in thestudent model due to the lack of explicit neighborhood aggregation. Extensiveexperiments conducted on six real-world datasets demonstrate that SA-DSDachieves performance improvements of 3.05%-3.62% over three GNN teacher modelsand 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmarkmodels, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75%decrease in inference time.</description>
      <author>example@mail.com (Can Cui, Zilong Fu, Penghe Huang, Yuanyuan Li, Wu Deng, Dongyan Li)</author>
      <guid isPermaLink="false">2509.00560v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Joint Training of Image Generator and Detector for Road Defect Detection</title>
      <link>http://arxiv.org/abs/2509.03465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted to ICCV 2025 Workshop on Representation  Learning with Very Limited Resources: When Data, Modalities, Labels, and  Computing Resources are Scarce as an oral paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为JTGD的道路缺陷检测方法，通过联合训练图像生成器和检测器，在边缘设备上高效地进行道路缺陷检测，无需集成方法或测试时增强技术。&lt;h4&gt;背景&lt;/h4&gt;道路缺陷检测对道路管理部门减少因道路缺陷造成的车辆损害非常重要。然而，缺陷检测器通常部署在内存和计算资源有限的边缘设备上，这对检测方法提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在不使用集成方法或测试时增强技术的情况下，在资源受限的边缘设备上进行道路缺陷检测。&lt;h4&gt;方法&lt;/h4&gt;1. 提出JTGD方法，联合训练图像生成器和检测器；2. 为生成模型设计双判别器，使合成的缺陷补丁和整体图像看起来合理；3. 提出基于CLIP的Fréchet Inception Distance损失来提高合成图像质量；4. 让生成模型与检测器联合训练，鼓励生成模型为检测器合成更困难的样本。&lt;h4&gt;主要发现&lt;/h4&gt;1. JTGD在RDD2022道路缺陷检测基准测试中，在没有集成和测试时增强的条件下，优于最先进的方法；2. 与竞争基线相比，JTGD仅使用了不到20%的参数数量；3. 该方法更适合在实际边缘设备上部署。&lt;h4&gt;结论&lt;/h4&gt;JTGD通过联合训练生成器和检测器，能够在资源受限的边缘设备上实现高效的道路缺陷检测，同时保持高检测性能和低计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;道路缺陷检测对道路管理部门减少因道路缺陷造成的车辆损害非常重要。考虑到缺陷检测器通常部署在内存和计算资源有限的边缘设备的实际场景，我们旨在不使用基于集成的方法或测试时增强技术的情况下进行道路缺陷检测。为此，我们提出联合训练图像生成器和检测器进行道路缺陷检测（称为JTGD）。我们为生成模型设计了双判别器，以强制合成的缺陷补丁和整体图像看起来合理。我们提出的基于CLIP的Fréchet Inception Distance损失提高了合成图像质量。JTGD中的生成模型与检测器联合训练，鼓励生成模型为检测器合成更困难的样本。由于上述设计导致的更高质量的困难合成图像用于数据增强，JTGD在RDD2022道路缺陷检测基准测试中，在没有集成和测试时增强的条件下，优于各种国家的最先进方法。与竞争基线相比，JTGD仅使用了不到20%的参数数量，这使其更适合在实际边缘设备上部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Road defect detection is important for road authorities to reduce the vehicledamage caused by road defects. Considering the practical scenarios where thedefect detectors are typically deployed on edge devices with limited memory andcomputational resource, we aim at performing road defect detection withoutusing ensemble-based methods or test-time augmentation (TTA). To this end, wepropose to Jointly Train the image Generator and Detector for road defectdetection (dubbed as JTGD). We design the dual discriminators for thegenerative model to enforce both the synthesized defect patches and overallimages to look plausible. The synthesized image quality is improved by ourproposed CLIP-based Fr\'echet Inception Distance loss. The generative model inJTGD is trained jointly with the detector to encourage the generative model tosynthesize harder examples for the detector. Since harder synthesized images ofbetter quality caused by the aforesaid design are used in the dataaugmentation, JTGD outperforms the state-of-the-art method in the RDD2022 roaddefect detection benchmark across various countries under the condition of noensemble and TTA. JTGD only uses less than 20% of the number of parameterscompared with the competing baseline, which makes it more suitable fordeployment on edge devices in practice.</description>
      <author>example@mail.com (Kuan-Chuan Peng)</author>
      <guid isPermaLink="false">2509.03465v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.03277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TPAMI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointAD+的新方法，通过结合CLIP的2D泛化能力，实现了对3D异常的全面检测和分割。该方法同时利用点和像素级别信息，通过隐式和显式3D表示，以及分层表示学习和跨层次对比对齐技术，实现了对渲染和空间异常性的综合理解。&lt;h4&gt;背景&lt;/h4&gt;现有方法难以将CLIP模型的强大2D泛化能力转移到3D异常检测领域，特别是针对具有高度多样化类别语义的未见过的对象。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的框架，能够全面检测和分割3D异常，同时利用点和像素级别的信息，实现对多样化类别语义的未见对象的异常检测。&lt;h4&gt;方法&lt;/h4&gt;首先提出PointAD利用点-像素对应关系表示3D异常；然后提出PointAD+引入显式3D表示和G-aggregation来增强空间感知能力；采用分层表示学习整合渲染和几何提示；通过跨层次对比对齐促进层间交互；最后整合两层异常语义实现泛化；测试阶段可即插即用地集成RGB信息提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明PointAD+在具有高度多样化类别语义的未见对象上的零样本3D异常检测方面表现优越，能够实现对异常性的整体理解。&lt;h4&gt;结论&lt;/h4&gt;PointAD+通过结合渲染和空间异常性，以及利用CLIP的泛化能力，在3D异常检测任务中取得了显著成果，为3D异常检测提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们旨在将CLIP的强大2D泛化能力转移到识别具有高度多样化类别语义的未见对象中的3D异常。为此，我们提出了一个统一框架，通过同时利用点和像素级别信息来全面检测和分割3D异常。我们首先设计了PointAD，它利用点-像素对应关系来通过其关联的渲染像素表示来表示3D异常。这种方法被称为隐式3D表示，因为它只关注渲染像素异常而忽略了点云内的固有空间关系。然后，我们提出了PointAD+，通过引入显式3D表示来进一步拓宽对3D异常的解释，强调空间异常性以发现异常的空间关系。因此，我们提出了G-aggregation来引入几何信息，使聚合的点表示具有空间感知能力。为了同时捕获渲染和空间异常性，PointAD+提出了分层表示学习，将隐式和显式异常语义合并到分层文本提示中：渲染层使用渲染提示，几何层使用几何提示。进一步引入了跨层次对比对齐来促进渲染层和几何层之间的交互，实现相互异常学习。最后，PointAD+整合来自两个层的异常语义以捕获泛化的异常语义。在测试阶段，PointAD+可以即插即用地集成RGB信息，进一步提高其检测性能。大量实验证明了PointAD+在具有高度多样化类别语义的未见对象上的零样本3D异常检测方面的优越性，实现了对异常性的整体理解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D异常检测问题，即在目标3D训练样本不可见的情况下如何准确检测3D点云中的异常。这个问题在现实中非常重要，因为许多实际应用场景（如涉及商业机密的工业检测或全新产品检测）无法获取目标3D训练数据，而传统3D异常检测方法依赖于目标类别的训练样本，无法处理这种场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到2D和3D异常检测之间存在知识差距，直接应用CLIP模型到3D领域并不简单。他们设计了一个统一框架，将CLIP的2D知识转移到3D理解中。作者首先提出了PointAD，通过点-像素对应关系捕获渲染异常；然后在此基础上提出了PointAD+，增加了对空间异常的建模。该方法借鉴了CLIP的视觉-语言对齐能力、多视图渲染技术、提示学习（如CoOp和AnomalyCLIP）、多实例学习和多任务学习等现有工作，并将它们创新地应用于3D异常检测领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是分层表示学习，通过两个互补角度理解3D异常：1)隐式3D异常（渲染层）：关注渲染像素异常；2)显式3D异常（几何层）：强调空间异常。整体流程包括：1)多视图渲染将3D点云转为2D图像；2)提取像素表示并转换为点表示；3)在渲染层使用混合表示学习优化渲染提示；4)在几何层使用G-aggregation注入几何信息；5)通过跨层次对比对齐促进层次间交互；6)测试时结合两种表示生成最终结果，并可即插即用地集成RGB信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次研究零样本3D和多模态3D异常检测；2)提出分层表示学习框架，将平面提示转换为分层提示；3)引入跨层次对比对齐促进相互学习；4)提出PointAD+和PointAD统一框架；5)实现即插即用的多模态RGB信息集成。相比之前工作，不同之处在于：能处理目标类别不可见的零样本场景；同时考虑渲染异常和空间异常实现更全面理解；无需额外训练即可集成多模态信息；具有更强的泛化能力，能从有限辅助类别学习通用异常语义。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointAD+通过分层表示学习框架，首次实现了对零样本3D异常的全面理解，同时能够即插即用地集成RGB信息，显著提升了在未见对象上的异常检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we aim to transfer CLIP's robust 2D generalizationcapabilities to identify 3D anomalies across unseen objects of highly diverseclass semantics. To this end, we propose a unified framework to comprehensivelydetect and segment 3D anomalies by leveraging both point- and pixel-levelinformation. We first design PointAD, which leverages point-pixelcorrespondence to represent 3D anomalies through their associated renderingpixel representations. This approach is referred to as implicit 3Drepresentation, as it focuses solely on rendering pixel anomalies but neglectsthe inherent spatial relationships within point clouds. Then, we proposePointAD+ to further broaden the interpretation of 3D anomalies by introducingexplicit 3D representation, emphasizing spatial abnormality to uncover abnormalspatial relationships. Hence, we propose G-aggregation to involve geometryinformation to enable the aggregated point representations spatially aware. Tosimultaneously capture rendering and spatial abnormality, PointAD+ proposeshierarchical representation learning, incorporating implicit and explicitanomaly semantics into hierarchical text prompts: rendering prompts for therendering layer and geometry prompts for the geometry layer. A cross-hierarchycontrastive alignment is further introduced to promote the interaction betweenthe rendering and geometry layers, facilitating mutual anomaly learning.Finally, PointAD+ integrates anomaly semantics from both layers to capture thegeneralized anomaly semantics. During the test, PointAD+ can integrate RGBinformation in a plug-and-play manner and further improve its detectionperformance. Extensive experiments demonstrate the superiority of PointAD+ inZS 3D anomaly detection across unseen objects with highly diverse classsemantics, achieving a holistic understanding of abnormality.</description>
      <author>example@mail.com (Qihang Zhou, Shibo He, Jiangtao Yan, Wenchao Meng, Jiming Chen)</author>
      <guid isPermaLink="false">2509.03277v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SurGBSA: Learning Representations From Molecular Dynamics Simulations</title>
      <link>http://arxiv.org/abs/2509.03084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SurGBSA，一种基于分子动力学的表示学习方法，学习MMGBSA的代理函数，实现了比传统计算快6497倍的加速，同时保持几乎相同的准确度。&lt;h4&gt;背景&lt;/h4&gt;自监督预训练可从静态结构中学习强大特征，在分子性质预测等任务上表现优异，但大多数方法受限于静态结构，如何利用分子动力学模拟开发更通用模型仍是个开放问题。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用原子分子动力学模拟来开发更通用的模型，以提高新型分子结构的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Surrogate mmGBSA(SurGBSA)方法，学习MMGBSA的代理函数，并在CASF-2016基准测试收集的140多万个3D轨迹上训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;SurGBSA在姿态排序问题上实现了6497倍加速，准确度仅比传统方法低0.4%；在MD模拟上训练可提升模型性能。&lt;h4&gt;结论&lt;/h4&gt;物理信息预训练结合分子动力学模拟可显著提升模型性能，推动分子基础模型发展，相关模型、代码和数据已公开。&lt;h4&gt;翻译&lt;/h4&gt;从小分子化合物和蛋白质的静态结构进行自监督预训练能够实现强大的学习特征表示。学习到的特征在一系列预测任务上展示了最先进的性能，包括分子性质、结构生成和蛋白质-配体相互作用。大多数方法受限于使用静态结构，如何最好地利用原子分子动力学模拟来开发更通用的模型以提高新型分子结构的预测精度仍然是一个开放性问题。我们提出了Surrogate mmGBSA(SurGBSA)作为一种新的基于MD的表示学习方法，它学习分子力学广义玻恩表面积的代理函数。我们首次展示了物理信息预训练的优势，在从CASF-2016基准测试的MD模拟中收集的超过140万个3D轨迹集合上训练了一个代理MMGBSA模型。在具有挑战性的姿态排序问题上，SurGBSA比传统的基于物理的单点MMGBSA计算实现了惊人的6497倍加速，同时在识别正确顶级姿态方面几乎匹配了单点MMGBSA的准确度（差异为-0.4%）。我们的工作通过展示在MD模拟上训练时模型的改进，推动了分子基础模型的发展。模型、代码和训练数据已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised pretraining from static structures of drug-like compounds andproteins enable powerful learned feature representations. Learned featuresdemonstrate state of the art performance on a range of predictive tasksincluding molecular properties, structure generation, and protein-ligandinteractions. The majority of approaches are limited by their use of staticstructures and it remains an open question, how best to use atomistic moleculardynamics (MD) simulations to develop more generalized models to improveprediction accuracy for novel molecular structures. We present SURrogate mmGBSA(SurGBSA) as a new modeling approach for MD-based representation learning,which learns a surrogate function of the Molecular Mechanics Generalized BornSurface Area (MMGBSA). We show for the first time the benefits ofphysics-informed pre-training to train a surrogate MMGBSA model on a collectionof over 1.4 million 3D trajectories collected from MD simulations of theCASF-2016 benchmark. SurGBSA demonstrates a dramatic 6,497x speedup versus atraditional physics-based single-point MMGBSA calculation while nearly matchingsingle-point MMGBSA accuracy on the challenging pose ranking problem foridentification of the correct top pose (-0.4% difference). Our work advancesthe development of molecular foundation models by showing model improvementswhen training on MD simulations. Models, code and training data are madepublicly available.</description>
      <author>example@mail.com (Derek Jones, Yue Yang, Felice C. Lightstone, Niema Moshiri, Jonathan E. Allen, Tajana S. Rosing)</author>
      <guid isPermaLink="false">2509.03084v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling</title>
      <link>http://arxiv.org/abs/2509.02450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EmoPerso的自监督框架，通过情感感知建模改进人格检测，解决了现有方法对大规模标注数据集的依赖以及忽视情感与人格相互作用的问题。&lt;h4&gt;背景&lt;/h4&gt;目前的人格检测通常通过分析用户社交媒体帖子进行，但现有方法严重依赖大规模标注数据集，难以获得高质量的人格标签。此外，大多数研究将情感和人格视为独立变量，忽视了它们之间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督框架EmoPerso，通过情感感知建模来改进人格检测，减少对标注数据的依赖并考虑情感与人格的相互作用。&lt;h4&gt;方法&lt;/h4&gt;EmoPerso利用生成机制进行合成数据增强和丰富的表示学习；提取伪标记的情感特征，并通过多任务学习与人格预测联合优化；采用交叉注意力模块捕捉人格特征与推断的情感表示之间的细粒度交互；采用自教策略迭代增强模型的推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集上的大量实验表明，EmoPerso超越了最先进的模型。&lt;h4&gt;结论&lt;/h4&gt;EmoPerso是一种有效的人格检测方法，通过情感感知建模提高了性能。源代码可在https://github.com/slz0925/EmoPerso获取。&lt;h4&gt;翻译&lt;/h4&gt;Personality detection from text is commonly performed by analysing users'social media posts. However, existing methods heavily rely on large-scaleannotated datasets, making it challenging to obtain high-quality personalitylabels. Moreover, most studies treat emotion and personality as independentvariables, overlooking their interactions. In this paper, we propose a novelself-supervised framework, EmoPerso, which improves personality detectionthrough emotion-aware modelling. EmoPerso first leverages generative mechanismsfor synthetic data augmentation and rich representation learning. It thenextracts pseudo-labeled emotion features and jointly optimizes them withpersonality prediction via multi-task learning. A cross-attention module isemployed to capture fine-grained interactions between personality traits andthe inferred emotional representations. To further refine relational reasoning,EmoPerso adopts a self-taught strategy to enhance the model's reasoningcapabilities iteratively. Extensive experiments on two benchmark datasetsdemonstrate that EmoPerso surpasses state-of-the-art models. The source code isavailable at https://github.com/slz0925/EmoPerso.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personality detection from text is commonly performed by analysing users'social media posts. However, existing methods heavily rely on large-scaleannotated datasets, making it challenging to obtain high-quality personalitylabels. Moreover, most studies treat emotion and personality as independentvariables, overlooking their interactions. In this paper, we propose a novelself-supervised framework, EmoPerso, which improves personality detectionthrough emotion-aware modelling. EmoPerso first leverages generative mechanismsfor synthetic data augmentation and rich representation learning. It thenextracts pseudo-labeled emotion features and jointly optimizes them withpersonality prediction via multi-task learning. A cross-attention module isemployed to capture fine-grained interactions between personality traits andthe inferred emotional representations. To further refine relational reasoning,EmoPerso adopts a self-taught strategy to enhance the model's reasoningcapabilities iteratively. Extensive experiments on two benchmark datasetsdemonstrate that EmoPerso surpasses state-of-the-art models. The source code isavailable at https://github.com/slz0925/EmoPerso.</description>
      <author>example@mail.com (Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel)</author>
      <guid isPermaLink="false">2509.02450v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Comprehensive Information-theoretic Multi-view Learning</title>
      <link>http://arxiv.org/abs/2509.02084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为CIML的全面信息理论多视图学习框架，摒弃了多视图冗余假设，同时考虑共同信息和独特信息的预测能力。&lt;h4&gt;背景&lt;/h4&gt;信息理论启发了多视图学习的众多进步，但大多数结合信息理论的多视图方法依赖于'多视图冗余'假设，即认为视图间的共同信息对于下游任务是必要且充分的，却忽视了每个视图中独特信息的潜在预测能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个不依赖多视图冗余假设的全面信息理论多视图学习框架，同时利用共同信息和独特信息的预测能力。&lt;h4&gt;方法&lt;/h4&gt;CIML框架包含两部分：1)共同表示学习，通过最大化Gacs-Korner共同信息提取共享特征，再基于信息(IB)压缩学习任务相关表示；2)独特表示学习，使用IB为每个视图实现最压缩的独特表示，同时最小化独特表示与共同表示之间以及不同独特表示之间的互信息。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了学习到的联合表示对下游任务具有预测充分性，大量实验结果证明了该模型优于几种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CIML框架通过摒弃多视图冗余假设并综合考虑共同信息和独特信息的预测能力，在多视图学习任务中表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;信息理论启发了多视图学习的众多进步。大多数结合信息理论原理的多视图方法依赖于一个称为'多视图冗余'的假设，该假设认为视图之间的共同信息对于下游任务是必要且充分的。这一假设强调共同信息对预测的重要性，但 inherently 忽略了每个视图中可能具有预测能力的独特信息。在本文中，我们提出了一个名为CIML的全面信息理论多视图学习框架，摒弃了多视图冗余的假设。具体而言，CIML基于信息理论考虑共同信息和独特信息的潜在预测能力。首先，共同表示学习最大化Gacs-Korner共同信息以提取共享特征，然后基于信息瓶颈(IB)压缩这些信息以学习任务相关表示。对于独特表示学习，使用IB实现每个视图的最压缩独特表示，同时最小化独特表示与共同表示之间的互信息，以及不同独特表示之间的互信息。重要的是，我们从理论上证明了学习到的联合表示对下游任务具有预测充分性。大量实验结果证明了我们的模型优于几种最先进的方法。代码已在CIML上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Information theory has inspired numerous advancements in multi-view learning.Most multi-view methods incorporating information-theoretic principles rely anassumption called multi-view redundancy which states that common informationbetween views is necessary and sufficient for down-stream tasks. Thisassumption emphasizes the importance of common information for prediction, butinherently ignores the potential of unique information in each view that couldbe predictive to the task. In this paper, we propose a comprehensiveinformation-theoretic multi-view learning framework named CIML, which discardsthe assumption of multi-view redundancy. Specifically, CIML considers thepotential predictive capabilities of both common and unique information basedon information theory. First, the common representation learning maximizesGacs-Korner common information to extract shared features and then compressesthis information to learn task-relevant representations based on theInformation Bottleneck (IB). For unique representation learning, IB is employedto achieve the most compressed unique representation for each view whilesimultaneously minimizing the mutual information between unique and commonrepresentations, as well as among different unique representations.Importantly, we theoretically prove that the learned joint representation ispredictively sufficient for the downstream task. Extensive experimental resultshave demonstrated the superiority of our model over several state-of-artmethods. The code is released on CIML.</description>
      <author>example@mail.com (Long Shi, Yunshan Ye, Wenjie Wang, Tao Lei, Yu Zhao, Gang Kou, Badong Chen)</author>
      <guid isPermaLink="false">2509.02084v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Fake &amp; Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives</title>
      <link>http://arxiv.org/abs/2509.02029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 Workshop LIMIT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文基于现有视觉自监督学习方法，借鉴'假装直到成功'理念，提出Syn2Co框架，结合生成模型合成数据和困难负样本，探索合成数据增强训练对视觉表征学习的影响。&lt;h4&gt;背景&lt;/h4&gt;对比自监督学习已取得显著成功，但通常依赖大量真实世界数据和精心筛选的困难负样本，限制了其在资源受限场景的应用。&lt;h4&gt;目的&lt;/h4&gt;探索替代大量真实数据和精心筛选困难负样本的方法，研究合成数据在视觉自监督学习中的潜力。&lt;h4&gt;方法&lt;/h4&gt;在视觉transformer中研究两种'假装'方式：利用生成模型创建合成数据增强样本多样性；在表征空间生成合成困难负样本创建有挑战性的对比。提出Syn2Co框架结合这两种方法，在DeiT-S和Swin-T架构上评估效果。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据增强训练能带来更鲁棒和可迁移的视觉表征，但同时也揭示了合成数据在自监督学习中的局限性。&lt;h4&gt;结论&lt;/h4&gt;合成数据在自监督学习中具有潜力和局限性，为未来研究方向提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文本身并没有引入全新的方法。相反，我们建立在现有的视觉自监督学习方法基础上，借鉴了'假装直到成功'的格言。虽然对比自监督学习已经取得了显著的成功，但它通常依赖于大量的真实世界数据和精心筛选的困难负样本。为了探索替代这些要求的方法，我们在视觉transformer中研究了两种形式的'假装'。首先，我们研究了生成模型在无监督表征学习中的潜力，利用合成数据来增强样本多样性。其次，我们探讨了在表征空间中生成合成困难负样本的可行性，创建多样且有挑战性的对比。我们的框架——命名为Syn2Co——结合了这两种方法，并评估了合成增强训练是否能导致在DeiT-S和Swin-T架构上更鲁棒和可迁移的视觉表征。我们的研究结果强调了合成数据在自监督学习中的潜力和局限性，为这一方向的未来工作提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper does not introduce a new method per se. Instead, we build onexisting self-supervised learning approaches for vision, drawing inspirationfrom the adage "fake it till you make it". While contrastive self-supervisedlearning has achieved remarkable success, it typically relies on vast amountsof real-world data and carefully curated hard negatives. To explorealternatives to these requirements, we investigate two forms of "faking it" invision transformers. First, we study the potential of generative models forunsupervised representation learning, leveraging synthetic data to augmentsample diversity. Second, we examine the feasibility of generating synthetichard negatives in the representation space, creating diverse and challengingcontrasts. Our framework - dubbed Syn2Co - combines both approaches andevaluates whether synthetically enhanced training can lead to more robust andtransferable visual representations on DeiT-S and Swin-T architectures. Ourfindings highlight the promise and limitations of synthetic data inself-supervised learning, offering insights for future work in this direction.</description>
      <author>example@mail.com (Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki)</author>
      <guid isPermaLink="false">2509.02029v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Training of Vision Transformers with Synthetic Negatives</title>
      <link>http://arxiv.org/abs/2509.02024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 Workshop VisCon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在视觉变换器架构中利用合成困难负样本来改进自监督学习表示的方法。&lt;h4&gt;背景&lt;/h4&gt;以往的研究探索了合成困难负样本，但很少在视觉变换器的背景下进行，自监督学习中困难负样本的潜力被忽视。&lt;h4&gt;目的&lt;/h4&gt;通过整合合成困难负样本来提高视觉变换器的表示学习能力，增强学习表示的判别能力。&lt;h4&gt;方法&lt;/h4&gt;将合成困难负样本整合到视觉变换器的自监督学习过程中，这是一种简单而有效的技术。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了学习表示的判别能力，在DeiT-S和Swin-T架构上都取得了性能提升。&lt;h4&gt;结论&lt;/h4&gt;合成困难负样本可以有效地改进视觉变换器的表示学习，增强其判别能力，无需引入全新的方法。&lt;h4&gt;翻译&lt;/h4&gt;本文本身并没有引入新颖的方法。相反，我们解决了自监督学习中困难负样本被忽视的潜力。以往的研究探索了合成的困难负样本，但很少在视觉变换器的背景下进行。我们基于这一观察，将合成的困难负样本整合起来，以改进视觉变换器的表示学习。这种简单而有效的技术显著提高了学习表示的判别能力。我们的实验显示，对于DeiT-S和Swin-T架构都取得了性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper does not introduce a novel method per se. Instead, we address theneglected potential of hard negative samples in self-supervised learning.Previous works explored synthetic hard negatives but rarely in the context ofvision transformers. We build on this observation and integrate synthetic hardnegatives to improve vision transformer representation learning. This simpleyet effective technique notably improves the discriminative power of learnedrepresentations. Our experiments show performance improvements for both DeiT-Sand Swin-T architectures.</description>
      <author>example@mail.com (Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki)</author>
      <guid isPermaLink="false">2509.02024v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective</title>
      <link>http://arxiv.org/abs/2509.01898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种面向扩散模型的高斯量化表示学习方法，以解决大规模模型在少样本无人机红外图像超分辨率任务中的过拟合问题，并通过构建多源无人机红外图像基准数据集验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;大规模模型在性能上取得了显著提升，但在图像超分辨率任务中，扩散模型作为生成模型的代表通常采用大规模架构。然而，在少样本无人机捕获的红外训练数据情况下，大规模架构经常出现严重过拟合问题，这削弱了它们的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决大规模架构在少样本无人机红外图像超分辨率任务中的过拟合挑战，提高模型的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种面向扩散模型的高斯量化表示学习方法，同时引入了有效的监控机制来跟踪大规模架构训练过程中的过拟合迹象。通过高斯量化表示学习，有效减少过拟合同时保持架构复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过引入高斯量化表示学习，有效减少了过拟合，同时保持了架构的复杂性；2. 构建了一个多源无人机红外图像基准数据集，用于检测和强调大规模架构在少样本、多样化的无人机图像重建场景中的过拟合问题；3. 实验结果表明，该方法优于现有的超分辨率方法，并在复杂条件下显著减轻了大规模架构的过拟合。&lt;h4&gt;结论&lt;/h4&gt;提出的高斯量化表示学习方法有效解决了大规模模型在少样本无人机红外图像超分辨率任务中的过拟合问题，实验结果证明了该方法的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;尽管大规模模型在性能上取得了显著提升，但过拟合挑战仍然经常削弱它们的泛化能力。在图像超分辨率任务中，扩散模型作为生成模型的代表通常采用大规模架构。然而，在少样本无人机捕获的红外训练数据情况下，大规模架构经常引发严重的过拟合。为解决这一关键挑战，我们的方法提出了一种面向扩散模型的新型高斯量化表示学习方法，该方法减轻过拟合并增强鲁棒性。同时，一个有效的监控机制在训练期间跟踪大规模架构以检测过拟合迹象。通过引入高斯量化表示学习，我们的方法有效减少了过拟合，同时保持架构复杂性。在此基础上，我们构建了一个多源无人机红外图像基准数据集用于检测，并利用它强调大规模架构在少样本、多样化的无人机图像重建场景中的过拟合问题。为验证该方法在减轻过拟合方面的有效性，我们在构建的基准上进行了实验。实验结果表明，我们的方法优于现有的超分辨率方法，并在复杂条件下显著减轻了大规模架构的过拟合。代码和DroneSR数据集将在以下地址提供：https://github.com/wengzp1/GARLSR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although large scale models achieve significant improvements in performance,the overfitting challenge still frequently undermines their generalizationability. In super resolution tasks on images, diffusion models asrepresentatives of generative models typically adopt large scale architectures.However, few-shot drone-captured infrared training data frequently inducessevere overfitting in large-scale architectures. To address this key challenge,our method proposes a new Gaussian quantization representation learning methodoriented to diffusion models that alleviates overfitting and enhancesrobustness. At the same time, an effective monitoring mechanism tracks largescale architectures during training to detect signs of overfitting. Byintroducing Gaussian quantization representation learning, our methodeffectively reduces overfitting while maintaining architecture complexity. Onthis basis, we construct a multi source drone-based infrared image benchmarkdataset for detection and use it to emphasize overfitting issues of large scalearchitectures in few sample, drone-based diverse drone-based imagereconstruction scenarios. To verify the efficacy of the method in mitigatingoverfitting, experiments are conducted on the constructed benchmark.Experimental results demonstrate that our method outperforms existing superresolution approaches and significantly mitigates overfitting of large scalearchitectures under complex conditions. The code and DroneSR dataset will beavailable at: https://github.com/wengzp1/GARLSR.</description>
      <author>example@mail.com (Zhipeng Weng, Xiaopeng Liu, Ce Liu, Xingyuan Guo, Yukai Shi, Liang Lin)</author>
      <guid isPermaLink="false">2509.01898v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Balanced Information Bottlenecks for Long-Tailed Visual Recognition</title>
      <link>http://arxiv.org/abs/2509.01804v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种平衡信息瓶颈(BIB)方法以及多平衡信息瓶颈混合(MBIB)结构，用于解决长尾视觉识别问题。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在大规模平衡数据上取得了显著成功，但现实世界中的视觉识别数据通常是长尾分布的，这给深度神经网络的训练和部署带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理长尾视觉识别问题的方法，保留必要的标签相关信息。&lt;h4&gt;方法&lt;/h4&gt;提出平衡信息瓶颈(BIB)方法，将损失函数重新平衡和自蒸馏技术集成到原始IB网络中；提出多平衡信息瓶颈混合(MBIB)结构，不同BIB负责结合来自不同网络层的知识，实现端到端学习。&lt;h4&gt;主要发现&lt;/h4&gt;BIB和MBIB在常用的长尾数据集(包括CIFAR100-LT、ImageNet-LT和iNaturalist 2018)上实验表明，两者在长尾视觉识别方面都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;平衡信息瓶颈(BIB)和多平衡信息瓶颈混合(MBIB)结构是解决长尾视觉识别问题的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNNs)在大规模和平衡数据的各种应用中取得了显著成功。然而，现实世界视觉识别中的数据通常是长尾分布的，这给DNNs的高效训练和部署带来了挑战。信息瓶颈(IB)是一种优雅的表征学习方法。在本文中，我们提出了一种平衡信息瓶颈(BIB)方法，将损失函数重新平衡和自蒸馏技术集成到原始IB网络中。因此，BIB能够学习充分的表征，完全保留必要的标签相关信息，用于长尾视觉识别。为了进一步增强表征学习能力，我们还提出了多平衡信息瓶颈混合(MBIB)的新结构，其中不同的BIB负责结合来自不同网络层的知识。MBIB促进了一种端到端的学习策略，从信息论的角度同时训练表征和分类。我们在常用的长尾数据集上进行了实验，包括CIFAR100-LT、ImageNet-LT和iNaturalist 2018。BIB和MBIB在长尾视觉识别方面都达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have achieved significant success in variousapplications with large-scale and balanced data. However, data in real-worldvisual recognition are usually long-tailed, bringing challenges to efficienttraining and deployment of DNNs. Information bottleneck (IB) is an elegantapproach for representation learning. In this paper, we propose a balancedinformation bottleneck (BIB) approach, in which loss function re-balancing andself-distillation techniques are integrated into the original IB network. BIBis thus capable of learning a sufficient representation with essentiallabel-related information fully preserved for long-tailed visual recognition.To further enhance the representation learning capability, we also propose anovel structure of mixture of multiple balanced information bottlenecks (MBIB),where different BIBs are responsible for combining knowledge from differentnetwork layers. MBIB facilitates an end-to-end learning strategy that trainsrepresentation and classification simultaneously from an information theoryperspective. We conduct experiments on commonly used long-tailed datasets,including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. Both BIB and MBIBreach state-of-the-art performance for long-tailed visual recognition.</description>
      <author>example@mail.com (Yifan Lan, Xin Cai, Jun Cheng, Shan Tan)</author>
      <guid isPermaLink="false">2509.01804v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Neural Scene Designer: Self-Styled Semantic Image Manipulation</title>
      <link>http://arxiv.org/abs/2509.01405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了神经场景设计器（NSD）框架，能够在保持语义控制和风格一致性的同时，实现对用户指定图像区域的逼真操作。&lt;h4&gt;背景&lt;/h4&gt;保持风格一致性对图像的凝聚力和美感至关重要，是有效图像编辑和修复的基本要求。然而现有方法主要关注生成内容的语义控制，常常忽视保持风格一致性的关键任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个新框架，能够实现对用户指定场景区域的逼真操作，同时确保与用户意图的语义对齐以及与周围环境的一致性。&lt;h4&gt;方法&lt;/h4&gt;NSD利用先进的扩散模型，包含两个并行的交叉注意力机制分别处理文本和风格信息；提出渐进式自风格表征学习（PSRL）模块，通过风格对比损失学习风格表征；建立了包括竞争算法、风格相关指标和多样化数据集的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;在基准测试上进行的大量实验证明了所提出框架在保持语义控制和风格一致性方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;NSD框架能够有效解决图像编辑中的风格一致性问题，为图像处理提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;保持风格一致性对于图像的凝聚力和美感至关重要，这是有效图像编辑和修复的基本要求。然而，现有方法主要关注生成内容的语义控制，常常忽视保持风格一致性的关键任务。在这项工作中，我们引入了神经场景设计器（NSD），这是一个新颖的框架，能够实现对用户指定场景区域的逼真操作，同时确保与用户意图的语义对齐以及与周围环境的一致性。NSD利用先进的扩散模型，包含两个并行的交叉注意力机制，分别处理文本和风格信息，以实现语义控制和风格一致性的双重目标。为了捕获细粒度的风格表征，我们提出了渐进式自风格表征学习（PSRL）模块。该模块基于一个直观的前提：单个图像内的不同区域共享一致的风格，而来自不同图像的区域则表现出不同的风格。PSRL模块采用风格对比损失，鼓励来自同一图像的表征具有高度相似性，同时强制来自不同图像的表征具有差异性。此外，为了解决此任务缺乏标准化评估协议的问题，我们建立了一个全面的基准测试。该基准测试包括竞争算法、专门的风格相关指标以及多样化的数据集和设置，以促进公平比较。在我们基准上进行的大量实验证明了所提出框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Maintaining stylistic consistency is crucial for the cohesion and aestheticappeal of images, a fundamental requirement in effective image editing andinpainting. However, existing methods primarily focus on the semantic controlof generated content, often neglecting the critical task of preserving thisconsistency. In this work, we introduce the Neural Scene Designer (NSD), anovel framework that enables photo-realistic manipulation of user-specifiedscene regions while ensuring both semantic alignment with user intent andstylistic consistency with the surrounding environment. NSD leverages anadvanced diffusion model, incorporating two parallel cross-attention mechanismsthat separately process text and style information to achieve the dualobjectives of semantic control and style consistency. To capture fine-grainedstyle representations, we propose the Progressive Self-style RepresentationalLearning (PSRL) module. This module is predicated on the intuitive premise thatdifferent regions within a single image share a consistent style, whereasregions from different images exhibit distinct styles. The PSRL module employsa style contrastive loss that encourages high similarity betweenrepresentations from the same image while enforcing dissimilarity between thosefrom different images. Furthermore, to address the lack of standardizedevaluation protocols for this task, we establish a comprehensive benchmark.This benchmark includes competing algorithms, dedicated style-related metrics,and diverse datasets and settings to facilitate fair comparisons. Extensiveexperiments conducted on our benchmark demonstrate the effectiveness of theproposed framework.</description>
      <author>example@mail.com (Jianman Lin, Tianshui Chen, Chunmei Qing, Zhijing Yang, Shuangping Huang, Yuheng Ren, Liang Lin)</author>
      <guid isPermaLink="false">2509.01405v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Propagation-aware Representation Learning for Supervised Social Media Graph Analytics</title>
      <link>http://arxiv.org/abs/2509.01124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 25th IEEE International Conference on Data Mining  (ICDM2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合双编码器结构和动力学引导传播模块的通用表示学习框架，用于社交媒体图分析，该框架在多种任务上实现了最先进性能并具有良好的迁移能力。&lt;h4&gt;背景&lt;/h4&gt;社交媒体平台产生大量复杂的图结构数据，支持谣言检测、机器人识别和影响力建模等任务。现实应用如公共舆论监控和股票交易高度依赖社交媒体，需要模型在不同任务和数据集上表现良好。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据驱动模型易受噪声影响、难以跨任务重用架构的问题，开发一个通用的表示学习框架，提高模型对噪声数据的鲁棒性和任务通用性。&lt;h4&gt;方法&lt;/h4&gt;提出结合双编码器结构和动力学引导传播模块的框架，使用两个编码器联合建模结构和上下文信息，通过整合原则性动力学知识捕捉社交媒体图中的信息传播动态，基于马尔可夫链传输模型推导传播感知编码器和优化目标。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在图分类、节点分类和链接预测等多种社交媒体图挖掘任务上取得最先进性能，使用统一架构实现；在不同数据集上表现出强大的零样本和少样本迁移能力，在数据稀缺任务中具有实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了社交媒体图分析中的挑战，在多种任务上表现出色且具有良好的迁移能力，特别适用于数据稀缺场景。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体平台产生大量复杂的图结构数据，促进多样化任务如谣言检测、机器人识别和影响力建模。现实应用如公共舆论监控和股票交易——与社交媒体紧密相连——需要模型在不同任务和数据集上表现良好。然而，大多数现有解决方案纯粹是数据驱动的，对社交媒体数据中的固有噪声表现出脆弱性。此外，对特定任务模型设计的依赖挑战了在不同任务上高效重用相同模型架构的能力，导致重复的工程努力。为了解决社交媒体图分析中的这些挑战，我们提出了一种通用表示学习框架，该框架结合了双编码器结构和动力学引导传播模块。除了使用两个编码器联合建模结构和上下文信息外，我们的框架还通过整合原则性动力学知识，创新性地捕捉社交媒体图中的信息传播动态。通过基于马尔可夫链传输模型推导传播感知编码器和相应的优化目标，表示学习流程增强了对噪声数据的鲁棒性和多样化任务的通用性。大量实验验证了我们的方法在各种社交媒体图挖掘任务上实现了最先进的性能，这些任务包括图分类、节点分类和链接预测，且使用统一架构。此外，我们的解决方案在不同数据集上表现出强大的零样本和少样本迁移能力，展示了在处理数据稀缺任务时的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social media platforms generate vast, complex graph-structured data,facilitating diverse tasks such as rumor detection, bot identification, andinfluence modeling. Real-world applications like public opinion monitoring andstock trading -- which have a strong attachment to social media -- demandmodels that are performant across diverse tasks and datasets. However, mostexisting solutions are purely data-driven, exhibiting vulnerability to theinherent noise within social media data. Moreover, the reliance ontask-specific model design challenges efficient reuse of the same modelarchitecture on different tasks, incurring repetitive engineering efforts. Toaddress these challenges in social media graph analytics, we propose a generalrepresentation learning framework that integrates a dual-encoder structure witha kinetic-guided propagation module. In addition to jointly modeling structuraland contextual information with two encoders, our framework innovativelycaptures the information propagation dynamics within social media graphs byintegrating principled kinetic knowledge. By deriving a propagation-awareencoder and corresponding optimization objective from a Markov chain-basedtransmission model, the representation learning pipeline receives a boost inits robustness to noisy data and versatility in diverse tasks. Extensiveexperiments verify that our approach achieves state-of-the-art performance witha unified architecture on a variety of social media graph mining tasks spanninggraph classification, node classification, and link prediction. Besides, oursolution exhibits strong zero-shot and few-shot transferability acrossdatasets, demonstrating practicality when handling data-scarce tasks.</description>
      <author>example@mail.com (Wei Jiang, Tong Chen, Wei Yuan, Xiangyu Zhao, Quoc Viet Hung Nguyen, Hongzhi Yin)</author>
      <guid isPermaLink="false">2509.01124v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition</title>
      <link>http://arxiv.org/abs/2509.00692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CascadeFormer的基于骨架的人体动作识别模型，结合了掩码预训练和级联微调两个阶段，在多个基准数据集上取得了具有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;基于骨架的人体动作识别利用人体关节坐标序列来识别视频中执行的动作。由于骨架数据固有的时空结构，图卷积网络(GCN)一直是该领域的主导架构。最近，Transformer模型和掩码预训练框架的发展为表示学习开辟了新途径。&lt;h4&gt;目的&lt;/h4&gt;提出CascadeFormer，一种用于基于骨架的人体动作识别的两级级联Transformer系列模型。&lt;h4&gt;方法&lt;/h4&gt;CascadeFormer框架包括两个阶段：1) 掩码预训练阶段，学习可泛化的骨架表示；2) 级联微调阶段，针对判别性动作分类进行定制。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集(Penn Action, N-UCLA, 和 NTU RGB+D 60)上评估了CascadeFormer，在所有任务上取得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;为了促进可复现性，作者发布了代码和模型检查点。&lt;h4&gt;翻译&lt;/h4&gt;基于骨架的人体动作识别利用人体关节坐标序列来识别视频中执行的动作。由于骨架数据固有的时空结构，图卷积网络(GCN)一直是该领域的主导架构。然而，最近Transformer模型和掩码预训练框架的进展为表示学习开辟了新途径。在这项工作中，我们提出了CascadeFormer，一种用于基于骨架的人体动作识别的两级级联Transformer系列模型。我们的框架包括一个掩码预训练阶段，用于学习可泛化的骨架表示，以及一个针对判别性动作分类定制的级联微调阶段。我们在三个基准数据集(Penn Action, N-UCLA, 和 NTU RGB+D 60)上评估了CascadeFormer，在所有任务上取得了具有竞争力的性能。为了促进可复现性，我们发布了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skeleton-based human action recognition leverages sequences of human jointcoordinates to identify actions performed in videos. Owing to the intrinsicspatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs)have been the dominant architecture in this field. However, recent advances intransformer models and masked pretraining frameworks open new avenues forrepresentation learning. In this work, we propose CascadeFormer, a family oftwo-stage cascading transformers for skeleton-based human action recognition.Our framework consists of a masked pretraining stage to learn generalizableskeleton representations, followed by a cascading fine-tuning stage tailoredfor discriminative action classification. We evaluate CascadeFormer acrossthree benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achievingcompetitive performance on all tasks. To promote reproducibility, we releaseour code and model checkpoints.</description>
      <author>example@mail.com (Yusen Peng, Alper Yilmaz)</author>
      <guid isPermaLink="false">2509.00692v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Entropy-based Coarse and Compressed Semantic Speech Representation Learning</title>
      <link>http://arxiv.org/abs/2509.00503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于熵的动态聚合框架，用于学习压缩的语义语音表示，解决了现有方法中token冗余问题，提高了下游任务效率。&lt;h4&gt;背景&lt;/h4&gt;离散语音表示学习在声学和语义建模中受到越来越多的关注。现有方法通常将16kHz波形编码为每秒25或50个离散token。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中token冗余和效率低下的问题，提出一种能够学习压缩语义语音表示的方法。&lt;h4&gt;方法&lt;/h4&gt;首先在大型未标记数据上预训练语音语言模型捕获频繁token模式；然后使用预测熵自适应确定聚合边界；接着使用交叉注意力模块融合每个段内信息；通过调整熵阈值灵活控制表示粒度和压缩比。&lt;h4&gt;主要发现&lt;/h4&gt;在ASR、语音到文本翻译和语音转换任务上的实验表明，压缩表示的性能与密集token序列相当或更好。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于熵的动态聚合框架能够有效学习压缩的语义语音表示，提高了下游任务效率且保持了性能。&lt;h4&gt;翻译&lt;/h4&gt;离散语音表示学习最近在声学和语义建模中引起了越来越多的关注。现有方法通常将16kHz波形编码为每秒25或50个离散token。然而，考虑到语音通常每秒只传达2到5个单词，这种细粒度的token化引入了冗余，阻碍了下游训练和推理的效率。此外，这种频率的语义语音表示主要捕获音素级信息，而语义理解可能不需要如此详细的token级分辨率。为了解决这些局限性，我们提出了一种基于熵的动态聚合框架，用于学习压缩的语义语音表示。首先，在大型未标记数据上通过next-token prediction预训练语音语言模型以捕获频繁的token模式。然后使用预测熵自适应地确定聚合边界，接着使用交叉注意力模块融合每个段内的信息。通过调整熵阈值，可以灵活控制表示的粒度和压缩比。在ASR、语音到文本翻译和语音转换任务上的实验表明，压缩表示的性能与密集token序列相当或更好，证明了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discrete speech representation learning has recently attracted increasinginterest in both acoustic and semantic modeling. Existing approaches typicallyencode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens persecond. However, given that speech generally conveys only 2 to 5 words persecond, such fine-grained tokenization introduces redundancy and hindersefficiency in downstream training and inference. Moreover, semantic speechrepresentations at this frequency primarily capture phonetic-level information,while semantic understanding may not require such detailed token-levelresolution. To address these limitations, we propose an entropy-based dynamicaggregation framework for learning compressed semantic speech representations.A speech language model is first pre-trained via next-token prediction onlarge-scale unlabeled data to capture frequent token patterns. Predictiveentropy is then used to adaptively determine aggregation boundaries, followedby a cross-attention module that fuses information within each segment. Byadjusting the entropy threshold, the granularity and compression ratio of therepresentations can be flexibly controlled. Experiments on ASR, speech-to-texttranslation, and voice conversion tasks demonstrate that the compressedrepresentations perform on par with or better than dense token sequences,demonstrating the effectiveness of the proposed approach.</description>
      <author>example@mail.com (Jialong Zuo, Guangyan Zhang, Minghui Fang, Shengpeng Ji, Xiaoqi Jiao, Jingyu Li, Yiwen Guo, Zhou Zhao)</author>
      <guid isPermaLink="false">2509.00503v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Focused Video Group Activities Hashing</title>
      <link>http://arxiv.org/abs/2509.00490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的视频哈希技术STVH及其增强版本M-STVH，用于解决复杂场景下视频数据快速增长背景下的群体活动快速检索问题。该方法能够同时捕捉个体对象动力学和群体交互特征，并可根据需要灵活关注活动语义特征或对象视觉特征。&lt;h4&gt;背景&lt;/h4&gt;随着各种复杂场景下视频数据的爆炸式增长，快速检索群体活动已成为一个紧迫问题。然而，许多现有任务只能检索整个视频，而无法达到活动粒度的精确检索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉视频时空演化特征的视频哈希技术，实现活动粒度的视频检索，并能够根据实际需求灵活关注活动语义特征或对象视觉特征。&lt;h4&gt;方法&lt;/h4&gt;1. 提出STVH（时空交错视频哈希）技术，通过统一框架同时建模个体对象动力学和群体交互，捕捉群体视觉特征和位置特征上的时空演化。2. 进一步提出M-STVH（多聚焦时空视频哈希）作为增强版本，通过多聚焦表示学习的层次特征集成，使模型能够同时关注活动语义特征和对象视觉特征。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上进行的比较实验表明，STVH和M-STVH两种方法都取得了优异的结果，证明了所提方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的STVH和M-STVH技术能够有效解决复杂场景下群体活动的快速检索问题，为视频检索提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着各种复杂场景下视频数据的爆炸式增长，快速检索群体活动已成为一个紧迫问题。然而，许多任务只能检索整个视频，而无法达到活动粒度的精确检索。为解决这个问题，我们首次提出了STVH（时空交错视频哈希）技术。通过统一框架，STVH同时建模个体对象动力学和群体交互，捕捉群体视觉特征和位置特征上的时空演化。此外，在实际视频检索场景中，有时可能需要活动特征，而有时可能需要对象的视觉特征。我们进一步提出了一种新颖的M-STVH（多聚焦时空视频哈希）作为增强版本来处理这一困难任务。先进的方法通过多聚焦表示学习的层次特征集成，使模型能够同时关注活动语义特征和对象视觉特征。我们在公开可用数据集上进行了比较实验，STVH和M-STVH都能取得优异的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the explosive growth of video data in various complex scenarios, quicklyretrieving group activities has become an urgent problem. However, many taskscan only retrieve videos focusing on an entire video, not the activitygranularity. To solve this problem, we propose a new STVH (spatiotemporalinterleaved video hashing) technique for the first time. Through a unifiedframework, the STVH simultaneously models individual object dynamics and groupinteractions, capturing the spatiotemporal evolution on both group visualfeatures and positional features. Moreover, in real-life video retrievalscenarios, it may sometimes require activity features, while at other times, itmay require visual features of objects. We then further propose a novel M-STVH(multi-focused spatiotemporal video hashing) as an enhanced version to handlethis difficult task. The advanced method incorporates hierarchical featureintegration through multi-focused representation learning, allowing the modelto jointly focus on activity semantics features and object visual features. Weconducted comparative experiments on publicly available datasets, and both STVHand M-STVH can achieve excellent results.</description>
      <author>example@mail.com (Zhongmiao Qi, Yan Jiang, Bolin Zhang, Lijun Guo, Chong Wang, Qiangbo Qian)</author>
      <guid isPermaLink="false">2509.00490v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</title>
      <link>http://arxiv.org/abs/2509.03324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InfraDiffusion是一种零样本框架，通过虚拟相机将砖石点云投影为深度图，并使用去噪扩散零空间模型(DDNM)进行恢复，无需特定任务训练即可提高深度图的视觉清晰度和几何一致性，显著改善了砖块级别分割效果，适用于基础设施自动化检查。&lt;h4&gt;背景&lt;/h4&gt;点云被广泛用于基础设施监测，提供几何信息，下游任务如缺陷检测需要分割。现有研究已自动化结构组件的语义分割，但砖块级别分割(识别剥落和砂浆损失等缺陷)主要从RGB图像进行。然而，在砖石隧道等低光环境中获取高分辨率图像不切实际。点云虽然对弱光环境稳健，但通常无结构、稀疏和有噪声，限制了细粒度分割。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够在低光环境中对点云数据进行有效处理，提高砖石结构砖块级别分割的准确性，用于基础设施的自动化检查。&lt;h4&gt;方法&lt;/h4&gt;提出InfraDiffusion，一种零样本框架，通过虚拟相机将砖石点云投影到深度图，并采用去噪扩散零空间模型(DDNM)进行恢复。这种方法无需任务特定训练即可增强深度图的视觉清晰度和几何一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在砖石桥梁和隧道点云数据集上的实验表明，使用Segment Anything Model (SAM)进行砖块级别分割时，InfraDiffusion方法取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;InfraDiffusion展示了其在砖石资产自动化检查方面的潜力，为基础设施监测提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;点云通过提供几何信息被广泛用于基础设施监测，其中下游任务如缺陷检测需要分割。现有研究已自动化结构组件的语义分割，而砖块级别分割(识别剥落和砂浆损失等缺陷)主要从RGB图像进行。然而，在砖石隧道等弱光环境中获取高分辨率图像不切实际。点云虽然对弱光环境稳健，但通常无结构、稀疏和有噪声，限制了细粒度分割。我们提出了InfraDiffusion，一种零样本框架，通过虚拟相机将砖石点云投影为深度图，并使用去噪扩散零空间模型(DDNM)进行恢复。无需任务特定训练，InfraDiffusion提高了深度图的视觉清晰度和几何一致性。在砖石桥梁和隧道点云数据集上的实验表明，使用Segment Anything Model (SAM)进行砖块级别分割时取得了显著改进，凸显了其在砖石资产自动化检查方面的潜力。我们的代码和数据可在https://github.com/Jingyixiong/InfraDiffusion-official-implement获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏、嘈杂的基础设施工点云中恢复深度地图，特别是针对砖石结构（如隧道和桥梁）的砖级别分割问题。这个问题在现实中很重要，因为砖石基础设施需要定期检查以确保安全，而传统方法在低光环境下难以获取高质量图像，且点云数据本身的稀疏性和噪声限制了精细分割的准确性。在研究中，这填补了组件级别分割与砖级别分割之间的空白，对早期缺陷检测和结构健康监测具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有投影方法的局限性，特别是圆柱投影不适用于复杂基础设施如砖石桥梁。为此，他们借鉴了自动驾驶领域的虚拟相机投影技术，以适应各种复杂几何形状。对于深度地图恢复的挑战，作者选择了扩散模型路线，特别是基于DDNM框架，因为它能提供强大的图像先验且不需要成对训练数据。作者对DDNM进行了关键改进，引入边界掩码来约束恢复过程，防止在无效区域产生伪内容。整个方法设计融合了虚拟相机投影和改进的扩散模型，借鉴了多个领域的现有工作但进行了针对性创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将稀疏、嘈杂的3D点云投影为2D深度地图，然后使用改进的扩散模型恢复这些深度地图，提高其视觉清晰度和几何一致性，同时通过边界掩码确保恢复只在有效区域内进行。整体流程分为五个步骤：1) 点云预处理（体素下采样和法线估计）；2) 补丁提取（定义包围盒并裁剪点云）；3) 虚拟相机投影（将点云投影为2D深度地图并计算边界掩码）；4) 深度地图恢复（使用改进的DDNM，分解为范围空间和零空间组件进行恢复）；5) 砖级别分割（使用SAM模型进行零样本语义分割）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出InfraDiffusion零样本框架，用于从砖石点云恢复深度地图；2) 引入虚拟相机投影适应复杂基础设施几何形状；3) 改进DDNM模型，添加边界掩码约束恢复过程；4) 显著提高五个数据集上的砖级别分割指标。相比之前工作，不同之处在于：之前的投影方法主要适用于隧道环境，而InfraDiffusion能处理复杂几何形状；之前的恢复方法需要成对训练数据，而InfraDiffusion是零样本方法；之前的扩散模型主要针对RGB图像，而InfraDiffusion专门处理深度地图；之前的分割需要大量标注数据，而InfraDiffusion结合SAM实现零样本分割。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfraDiffusion通过虚拟相机投影和改进的扩散模型，实现了从稀疏、嘈杂的基础设施工点云中高质量恢复深度地图，并显著提高了砖级别分割的准确性，为基础设施健康监测提供了一种无需特定训练的零样本解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are widely used for infrastructure monitoring by providinggeometric information, where segmentation is required for downstream tasks suchas defect detection. Existing research has automated semantic segmentation ofstructural components, while brick-level segmentation (identifying defects suchas spalling and mortar loss) has been primarily conducted from RGB images.However, acquiring high-resolution images is impractical in low-lightenvironments like masonry tunnels. Point clouds, though robust to dim lighting,are typically unstructured, sparse, and noisy, limiting fine-grainedsegmentation. We present InfraDiffusion, a zero-shot framework that projectsmasonry point clouds into depth maps using virtual cameras and restores them byadapting the Denoising Diffusion Null-space Model (DDNM). Without task-specifictraining, InfraDiffusion enhances visual clarity and geometric consistency ofdepth maps. Experiments on masonry bridge and tunnel point cloud datasets showsignificant improvements in brick-level segmentation using the Segment AnythingModel (SAM), underscoring its potential for automated inspection of masonryassets. Our code and data is available athttps://github.com/Jingyixiong/InfraDiffusion-official-implement.</description>
      <author>example@mail.com (Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil)</author>
      <guid isPermaLink="false">2509.03324v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR</title>
      <link>http://arxiv.org/abs/2509.03262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PI3DETR是一个端到端的框架，可以直接从原始点云预测3D参数化曲线实例，避免了中间表示和多阶段处理，提高了对噪声和不同采样密度的鲁棒性，并在ABC数据集上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;先前的工作通常使用中间表示和多阶段处理来处理3D曲线检测，这在现实世界的LiDAR和3D感知场景中面临噪声和不同采样密度的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够直接从原始点云预测3D参数化曲线的端到端框架，提高对噪声和不同采样密度的鲁棒性，解决现实世界LiDAR和3D感知场景中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;扩展3DETR模型，引入几何感知的匹配策略和专门的损失函数，支持单次前向传播中统一检测不同参数化的曲线类型（包括三次贝塞尔曲线、线段、圆和弧），并使用可选的后处理步骤优化预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;PI3DETR在ABC数据集上建立了新的最先进水平，能够有效地推广到真实传感器数据，为3D边缘和曲线估计提供了一个简单而强大的解决方案。&lt;h4&gt;结论&lt;/h4&gt;PI3DETR通过精简的设计提高了对噪声和不同采样密度的鲁棒性，解决了现实世界LiDAR和3D感知场景中的关键挑战，是一个简单而强大的3D边缘和曲线估计解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了PI3DETR，这是一个端到端框架，可以直接从原始点云预测3D参数化曲线实例，避免了先前工作中常见的中间表示和多阶段处理。扩展3DETR后，我们的模型引入了几何感知的匹配策略和专门的损失函数，使得在单次前向传播中能够统一检测不同参数化的曲线类型，包括三次贝塞尔曲线、线段、圆和弧。可选的后处理步骤可以进一步优化预测结果而不会增加复杂性。这种精简的设计提高了对噪声和不同采样密度的鲁棒性，解决了现实世界LiDAR和3D感知场景中的关键挑战。PI3DETR在ABC数据集上建立了新的最先进水平，并且能有效地推广到真实传感器数据，为3D边缘和曲线估计提供了一个简单而强大的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从3D点云中直接检测和预测参数化曲线实例的问题。这个问题在现实中非常重要，因为在机器人视觉中，边缘信息可以优化抓取规划；在制造业中，线框提取有助于质量控制；在自动驾驶、CAD重建等领域，边缘作为重要的结构线索。此外，现实世界中的LiDAR等3D传感技术产生的点云常包含噪声、非均匀采样密度和遮挡，这使得开发鲁棒的边缘检测方法对确保可靠性能至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者批判性地分析了现有方法，发现它们通常依赖中间表示（如体素网格、距离场）和多阶段处理，流程复杂且对输入空间有假设。作者借鉴了3DETR框架，将其扩展用于曲线检测，但进行了关键改进：引入了几何感知匹配策略，设计专门针对3D边缘估计的损失函数，并添加了参数化曲线预测头部。作者的目标是创建一个简化的端到端架构，直接从原始点云推断参数化曲线，消除传统方法中的辅助表示和手工制作管道。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个简化的端到端架构，直接从原始点云预测参数化曲线实例，避免传统方法中的中间表示和多阶段处理。通过几何感知匹配策略，使单一模型能够检测不同类型的参数化曲线。整体流程：1)接收点云输入；2)使用SAModule下采样并提取特征；3)通过Transformer编码器-解码器处理特征；4)使用预测头部输出不同类型曲线的参数；5)通过几何感知匹配将预测与真实曲线匹配；6)计算包括交叉熵、几何损失和Chamfer距离的总损失；7)应用可选后处理步骤优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)简化的端到端架构，直接预测参数化曲线；2)几何感知匹配策略，支持多种曲线类型联合检测；3)专门设计的损失函数，处理不同曲线类型的几何特性；4)可选后处理步骤进一步提高精度。相比之前的工作如NerVE、PIE-NET等，PI3DETR的不同之处在于：直接预测参数化曲线而非先检测基元再拟合；使用统一架构而非多分支处理；表现出更强的鲁棒性，尤其在稀疏点云和噪声环境下；在ABC数据集上实现了更高的准确性，Chamfer距离比之前方法降低近一半。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PI3DETR提出了一种简化的端到端框架，通过几何感知匹配策略直接从原始点云预测多种类型的参数化3D曲线，实现了比之前方法更高的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present PI3DETR, an end-to-end framework that directly predicts 3Dparametric curve instances from raw point clouds, avoiding the intermediaterepresentations and multi-stage processing common in prior work. Extending3DETR, our model introduces a geometry-aware matching strategy and specializedloss functions that enable unified detection of differently parameterized curvetypes, including cubic B\'ezier curves, line segments, circles, and arcs, in asingle forward pass. Optional post-processing steps further refine predictionswithout adding complexity. This streamlined design improves robustness to noiseand varying sampling densities, addressing critical challenges in real worldLiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABCdataset and generalizes effectively to real sensor data, offering a simple yetpowerful solution for 3D edge and curve estimation.</description>
      <author>example@mail.com (Fabio F. Oberweger, Michael Schwingshackl, Vanessa Staderini)</author>
      <guid isPermaLink="false">2509.03262v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms</title>
      <link>http://arxiv.org/abs/2509.02870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用小型城市农场的低成本自动化机器人平台（FarmBot）进行植物表型分析，特别关注草莓花朵的姿态估计，用于机器人授粉应用。&lt;h4&gt;背景&lt;/h4&gt;城市农场规模小且低成本机器人（如FarmBot）的商业可用性，为植物表型分析提供了可访问的自动化平台。&lt;h4&gt;目的&lt;/h4&gt;使用带有定制相机末端执行器的FarmBot系统，从3D点云模型中估计草莓植物花朵的姿态，以支持机器人授粉。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新算法，将点云的个体占用网格沿正交轴平移获得六个视角的2D图像，使用2D目标检测模型识别花朵边界框，将其转换为3D空间提取花朵点云，并通过将三种几何形状（超椭球体、抛物面和平面）拟合到点云来进行姿态估计。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功识别了约80%的扫描花朵，平均姿态误差为7.7度，这一精度对于机器人授粉任务足够，且结果与先前研究相当。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能有效估计草莓花朵姿态，适用于机器人授粉应用，相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;城市农场的规模小和低成本机器人（如FarmBot）的商业可用性，这些机器人可以自动化简单的照料任务，为植物表型分析提供了一个可访问的平台。我们使用带有定制相机末端执行器的FarmBot，从获取的3D点云模型中估计草莓植物花朵的姿态（用于机器人授粉）。我们描述了一种新算法，该算法将点云的个体占用网格沿正交轴平移，以获得对应六个视角的2D图像。对于每张图像，使用花朵的2D目标检测模型来识别2D边界框，这些边界框可以转换为3D空间以提取花朵点云。通过将三种形状（超椭球体、抛物面和平面）拟合到花朵点云来进行姿态估计，并与手动标记的真实值进行比较。我们的方法成功找到了使用定制FarmBot平台扫描的约80%的花朵，平均花朵姿态误差为7.7度，这对于机器人授粉来说是足够的，并且与之前的结果相当。所有代码将在https://github.com/harshmuriki/flowerPose.git上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市农场中草莓花朵姿态（位置和方向）的精确估计问题，这对机器人授粉至关重要。随着自然授粉媒介如蜜蜂的减少以及室内农场环境的需求，机器人授粉成为重要替代方案。精确的花朵姿态估计是机器人成功进行授粉操作的前提，直接影响授粉效率和成功率，对于小规模城市农场的自动化生产具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到城市农场规模小，可利用商业FarmBot机器人作为平台。他们专注于花朵姿态估计作为植物表型分析的关键任务。方法设计上，借鉴了现有计算机视觉技术，如使用Polycam进行3D重建、YOLOv10和Roboflow进行目标检测、DBSCAN进行点云分割。同时，作者创新性地开发了'转移占用网格'方法，将3D点云转换为多个2D视角，结合了3D空间信息和2D目标检测的优势，形成了一套完整的从数据采集到姿态估计的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D点云捕捉花朵的空间信息，将其转换为多个2D视角便于检测，然后利用几何形状拟合确定花朵姿态。整体流程分为四步：1)数据采集：使用定制FarmBot围绕植物进行圆柱形和半球形扫描，获取多角度图像；2)3D模型生成：用Polycam软件从图像生成点云模型；3)转移占用网格方法：从六个正交方向将点云转为2D图像，用目标检测识别花朵并提取3D点云；4)花朵姿态估计：分离花瓣和雌蕊，用三种几何形状拟合确定花朵方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'转移占用网格'方法将3D点云转换为六个正交方向的2D图像；2)结合3D点云处理和2D目标检测的优势；3)使用几何形状拟合精确估计花朵姿态；4)提供定量评估角度估计精度。相比之前工作，不同之处在于：之前研究主要使用离散2D图像推断方向，缺乏定量精度评估；该方法使用完整3D点云模型，提供连续姿态估计；适用于小规模城市农场特定环境，而非大型温室；花朵检测率约80%，平均姿态误差7.7度，满足机器人授粉需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发了一种基于FarmBot机器人和3D点云分析的草莓花朵姿态估计方法，通过创新的转移占用网格技术和几何形状拟合，实现了高精度的花朵检测和姿态估计，为城市农场的机器人授粉提供了关键技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA55743.2025.11128713&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The small scale of urban farms and the commercial availability of low-costrobots (such as the FarmBot) that automate simple tending tasks enable anaccessible platform for plant phenotyping. We have used a FarmBot with a customcamera end-effector to estimate strawberry plant flower pose (for roboticpollination) from acquired 3D point cloud models. We describe a novel algorithmthat translates individual occupancy grids along orthogonal axes of a pointcloud to obtain 2D images corresponding to the six viewpoints. For each image,2D object detection models for flowers are used to identify 2D bounding boxeswhich can be converted into the 3D space to extract flower point clouds. Poseestimation is performed by fitting three shapes (superellipsoids, paraboloidsand planes) to the flower point clouds and compared with manually labeledground truth. Our method successfully finds approximately 80% of flowersscanned using our customized FarmBot platform and has a mean flower pose errorof 7.7 degrees, which is sufficient for robotic pollination and rivals previousresults. All code will be made available athttps://github.com/harshmuriki/flowerPose.git.</description>
      <author>example@mail.com (Harsh Muriki, Hong Ray Teo, Ved Sengupta, Ai-Ping Hu)</author>
      <guid isPermaLink="false">2509.02870v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers</title>
      <link>http://arxiv.org/abs/2509.02808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and awarded best paper at the 11th International Conference  on Control, Decision and Information Technologies (CoDIT 2025 -  https://codit2025.org/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合基于学习控制器的安全控制器，用于四旋翼无人机在大型地下环境中的自主导航，通过归一化流模型监测环境分布外程度，在必要时切换到安全控制器。&lt;h4&gt;背景&lt;/h4&gt;四旋翼无人机在大型地下环境中的自主控制可用于环境调查、采矿作业和搜救等多个领域，但基于学习的控制器在未遇到过的'分布外'环境中泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在大型地下环境中安全高效导航的四旋翼无人机控制系统，解决基于学习控制器在分布外环境中泛化能力差的问题。&lt;h4&gt;方法&lt;/h4&gt;训练一个基于归一化流的环境先验模型，用于衡量四旋翼在任何时候的分布外程度；使用这个衡量标准作为运行时监控器，当四旋翼足够分布外时，在基于学习的控制器和安全控制器之间切换。&lt;h4&gt;主要发现&lt;/h4&gt;在基于DARPA地下挑战赛决赛数据集真实点云数据的模拟3D洞穴环境中进行点到点导航任务的实验表明，所提出的组合控制器同时具有基于学习控制器的活性（快速完成任务）和安全控制器的安全性（避免碰撞）。&lt;h4&gt;结论&lt;/h4&gt;通过结合归一化流模型和控制器切换机制，成功实现了四旋翼无人机在复杂地下环境中的安全高效自主导航。&lt;h4&gt;翻译&lt;/h4&gt;在大型地下环境中自主控制四旋翼无人机适用于环境调查、采矿作业和搜救等多个领域。基于学习的控制器是实现自主控制的一种有吸引力的方法，但已知它们对未在训练期间遇到的'分布外'环境泛化能力差。在这项工作中，我们训练了一个基于归一化流的环境先验模型，它提供了衡量四旋翼在任何时候分布外程度的指标。我们使用这个指标作为运行时监控器，使我们能够在充分分布外时在基于学习的控制器和安全控制器之间切换。我们的方法在基于DARPA地下挑战赛决赛数据集真实点云数据的模拟3D洞穴环境中的点到点导航任务上进行了基准测试。我们的实验结果表明，我们的组合控制器同时具有基于学习控制器的活性（快速完成任务）和安全控制器的安全性（避免碰撞）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是提高四旋翼无人机在地下环境中的韧性（鲁棒性），特别是当遇到与训练环境不同的'分布外'环境时的适应能力。这个问题在现实中非常重要，因为地下环境（如洞穴、矿井）具有复杂、未知、动态变化的特点，无人机需要在这样的环境中执行搜索救援、环境监测、采矿作业等任务。如果无人机不能适应这些复杂多变的环境，就无法可靠地完成任务，可能导致任务失败甚至设备损坏。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从两个角度思考问题：一方面，学习型控制器在训练环境中表现良好，完成任务速度快，但对新环境泛化能力差；另一方面，安全控制器对新环境更鲁棒，但完成任务速度慢。作者借鉴了现有的FLOWMPPI（使用归一化流改进的模型预测路径积分控制）作为学习型控制器，以及基于顺序凸规划（SCP）和增广拉格朗日迭代线性二次调节器（AL-iLQR）的安全控制器。此外，作者还利用了现有的分布外检测（OOD detection）方法。作者设计的核心思路是结合这两种控制器的优点：在熟悉环境中使用学习型控制器以获得速度优势，在不熟悉环境中切换到安全控制器以确保安全。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是训练一个基于归一化流的环境先验模型，用于判断当前环境是否与训练环境相似，并作为运行时监控器来决定使用哪种控制器。当检测到分布内环境时，使用学习型控制器以快速完成任务；当检测到分布外环境时，切换到安全控制器以确保安全。整体实现流程包括：1）训练阶段：使用贝叶斯模型强化学习训练FLOWMPPI控制器，同时训练变分自编码器（VAE）编码环境特征和环境编码的先验模型；2）测试阶段：在每个时间步计算OOD分数，低于阈值时使用学习型控制器，高于阈值时切换到安全控制器。安全控制器通过顺序凸规划生成可行轨迹，然后由AL-iLQR跟踪执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）在迄今为止最大的3D地下环境（41×62×11米，体积11492立方米）上训练了归一化流最优控制策略；2）设计了一个能够生成动态可行、避免碰撞轨迹的安全控制器；3）开发了一个运行时分布外监控器，根据环境、当前状态和目标状态智能切换控制器。相比之前的工作，本文首次系统性地结合了学习型控制器的'活性'（快速完成任务）和安全控制器的'安全性'（避免碰撞），解决了单一控制器在复杂多变环境中的局限性，显著提高了无人机在地下环境中的导航韧性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过智能切换学习型控制器和安全控制器，使四旋翼无人机在地下环境中既能快速完成任务又能确保安全，显著提高了其在复杂未知环境中的导航韧性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomously controlling quadrotors in large-scale subterranean environmentsis applicable to many areas such as environmental surveying, mining operations,and search and rescue. Learning-based controllers represent an appealingapproach to autonomy, but are known to not generalize well to`out-of-distribution' environments not encountered during training. In thiswork, we train a normalizing flow-based prior over the environment, whichprovides a measure of how far out-of-distribution the quadrotor is at any giventime. We use this measure as a runtime monitor, allowing us to switch between alearning-based controller and a safe controller when we are sufficientlyout-of-distribution. Our methods are benchmarked on a point-to-point navigationtask in a simulated 3D cave environment based on real-world point cloud datafrom the DARPA Subterranean Challenge Final Event Dataset. Our experimentalresults show that our combined controller simultaneously possesses the livenessof the learning-based controller (completing the task quickly) and the safetyof the safety controller (avoiding collision).</description>
      <author>example@mail.com (Isaac Ronald Ward, Mark Paral, Kristopher Riordan, Mykel J. Kochenderfer)</author>
      <guid isPermaLink="false">2509.02808v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Geodesics of Quantum Feature Maps on the space of Quantum Operators</title>
      <link>http://arxiv.org/abs/2509.02795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文从信息保留角度考虑点云的平滑黎曼几何结构，研究量子编码方案如何将其映射到量子算子空间SU(2^N)时变形几何结构，并为哈密顿量子特征映射建立了黎曼几何框架。&lt;h4&gt;背景&lt;/h4&gt;量子特征选择是量子机器学习的基本步骤，已有许多编码方案和技术来测试方案有效性，但量子特征映射的值域的黎曼流形结构尚未形式化。&lt;h4&gt;目的&lt;/h4&gt;为从欧几里得嵌入流形诱导的一般哈密顿量子特征映射建立黎曼几何，研究编码方案如何影响原始几何结构。&lt;h4&gt;方法&lt;/h4&gt;采用从基础开始的方法，推导向量空间及其相应度量的封闭形式，证明嵌入流形上的测地线与编码方案值域之间存在一一对应关系，并严格推导计算曲率的封闭形式方程。&lt;h4&gt;主要发现&lt;/h4&gt;建立了哈密顿量子特征映射的黎曼几何框架，证明了嵌入流形上的测地线与编码方案值域之间存在一一对应关系，得到了计算曲率的封闭形式方程。&lt;h4&gt;结论&lt;/h4&gt;论文以庞加莱半平面子集和两个常用特征映射的例子结束，验证了所提出理论的有效性。&lt;h4&gt;翻译&lt;/h4&gt;选择量子特征是量子机器学习中的一个基本步骤。已经提出了许多编码方案和技术来测试方案的有效性。从信息保留的角度，本文考虑了点云的平滑黎曼几何结构，以及编码方案一旦映射到量子算子空间SU(2^N)时如何变形这种几何结构。然而，量子特征映射的值域的黎曼流形结构尚未形式化。采用从基础开始的方法，本文从数学上建立了一般类哈密顿量子特征映射的黎曼几何，这类映射是从欧几里得嵌入流形诱导的。对于这种方法，我们首先推导了向量空间及其相应度量的封闭形式，并证明嵌入流形上的测地线与编码方案的值域之间存在一一对应关系。然后，我们严格推导了计算曲率的封闭形式方程。论文以庞加莱半平面子集和两个常用特征映射的例子结束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Selecting a quantum feature is an essential step in quantum machine learning.There have been many proposed encoding schemes and proposed techniques to testthe efficacy of a scheme. From the perspective of information retention, thispaper considers the smooth Riemannian geometry structure of a point cloud andhow an encoding scheme deforms this geometry once mapped to the space ofquantum operators, $\SU(2^N)$. However, a Riemannian manifold structure of thecodomain of a quantum feature map has yet to be formalized. Using a ground-upapproach, this manuscript mathematically establishes a Riemannian geometry fora general class of Hamiltonian quantum feature maps that are induced from aEuclidean embedded manifold. For this ground-up approach, we first derive aclosed form of a vector space and a respective metric, and prove there is a 1-1correspondence from geodesics on the embedded manifold to the codomain of theencoding scheme. We then rigorously derive closed form equations to calculatecurvature. The paper ends with an example with a subset of the Poincar\'ehalf-plane and two well-used feature maps.</description>
      <author>example@mail.com (Andrew Vlasic)</author>
      <guid isPermaLink="false">2509.02795v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework</title>
      <link>http://arxiv.org/abs/2509.02474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个统一的3D表示评估框架，用于比较不同3D表示方法在重建和生成任务中的性能，并确定了3D生成管道各步骤的最佳实践。&lt;h4&gt;背景&lt;/h4&gt;随着文本和图像生成技术的快速发展，研究日益转向3D生成领域。与图像中成熟的像素表示不同，3D表示方法多样且分散，包括体素网格、神经辐射场、符号距离函数、点云或八叉树等，每种方法都有其独特的优势和局限性。&lt;h4&gt;目的&lt;/h4&gt;设计并实施一个统一的评估框架，用于评估不同3D表示方法在重建和生成任务中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;基于多个标准比较这些3D表示方法：质量、计算效率和泛化性能。实验旨在确定3D生成管道中所有步骤的最佳实践，包括预处理、网格重建、使用自编码器压缩和生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;重建错误显著影响整体性能，强调需要联合评估生成和重建过程，而非孤立地评估每个组件。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了有价值的见解，可为各种应用选择合适的3D模型提供指导，促进3D生成领域中更稳健和应用特定解决方案的发展。相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;随着文本和图像生成的快速进展，研究日益转向3D生成。与图像中成熟的基于像素的表示不同，3D表示仍然多样且分散，包含多种方法，如体素网格、神经辐射场、符号距离函数、点云或八叉树，每种方法都有其独特的优势和局限性。在这项工作中，我们提出了一个统一的评估框架，旨在评估3D表示在重建和生成任务中的性能。我们基于多个标准比较这些表示方法：质量、计算效率和泛化性能。除了标准的模型基准测试外，我们的实验旨在确定3D生成管道中所有步骤的最佳实践，包括预处理、网格重建、使用自编码器压缩和生成。我们的研究结果表明，重建错误显著影响整体性能，强调了需要联合评估生成和重建的重要性。我们提供的见解可以为各种应用选择合适的3D模型提供参考，促进3D生成领域更稳健和应用特定解决方案的发展。我们框架的代码可在https://github.com/isl-org/unifi3d获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D表示方法缺乏统一评估框架的问题。在现实中，随着3D生成技术的发展，存在多种3D表示方法（如体素网格、神经辐射场、有符号距离函数等），但难以公平比较它们的性能。这个问题很重要，因为3D表示的选择直接影响生成质量、计算效率和最终应用效果，缺乏统一评估标准阻碍了3D生成领域的进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D表示评估面临的挑战：表示方法嵌入在复杂流程中，多种预处理技术影响结果，缺乏统一评估指标。他们设计了一个标准化的生成管道，包含从数据预处理到网格重建的所有步骤，采用模块化设计允许不同表示的互换。作者借鉴了多种现有3D表示方法和生成模型，如扩散模型和自回归模型，使用ShapeNet等标准数据集和Chamfer Distance等评估指标。创新点在于将这些现有方法整合到一个统一的评估框架中，而不是简单地综述现有方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供一个统一的框架来公平比较不同3D表示方法在生成和重建任务中的性能，强调重建和生成的联合评估。整体流程包括：1)表示转换：将3D网格转换为合适表示；2)表示压缩：使用自编码器压缩到潜在空间；3)潜在生成：训练扩散模型生成潜在向量；4)网格重建：从解码表示重建最终网格。评估包括重建质量、压缩性能、泛化能力和生成质量四个方面，结合定量指标和用户研究进行综合评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的评估框架，首次实现不同3D表示方法的公平比较；2)联合评估重建和生成能力，发现重建误差可能高达生成误差的20%；3)提供最佳实践指南，解决预处理等问题；4)开源模块化代码库，包含新组件如双八叉树生成方法。相比之前工作，这篇论文不依赖单个3D生成方法的实现，而是控制变量进行比较；不仅评估生成质量，还分析重建与生成的关系；提供标准化流程解决非水密网格等常见问题；比较多种表示方法在多个维度上的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Unifi3D提供了一个统一的评估框架，通过标准化流程和联合评估重建与生成能力，首次实现了对多种3D表示方法的公平比较，并确定了不同表示方法的优势和适用场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following rapid advancements in text and image generation, research hasincreasingly shifted towards 3D generation. Unlike the well-establishedpixel-based representation in images, 3D representations remain diverse andfragmented, encompassing a wide variety of approaches such as voxel grids,neural radiance fields, signed distance functions, point clouds, or octrees,each offering distinct advantages and limitations. In this work, we present aunified evaluation framework designed to assess the performance of 3Drepresentations in reconstruction and generation. We compare theserepresentations based on multiple criteria: quality, computational efficiency,and generalization performance. Beyond standard model benchmarking, ourexperiments aim to derive best practices over all steps involved in the 3Dgeneration pipeline, including preprocessing, mesh reconstruction, compressionwith autoencoders, and generation. Our findings highlight that reconstructionerrors significantly impact overall performance, underscoring the need toevaluate generation and reconstruction jointly. We provide insights that caninform the selection of suitable 3D models for various applications,facilitating the development of more robust and application-specific solutionsin 3D generation. The code for our framework is available athttps://github.com/isl-org/unifi3d.</description>
      <author>example@mail.com (Nina Wiedemann, Sainan Liu, Quentin Leboutet, Katelyn Gao, Benjamin Ummenhofer, Michael Paulitsch, Kai Yuan)</author>
      <guid isPermaLink="false">2509.02474v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments</title>
      <link>http://arxiv.org/abs/2509.02283v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于雷达的3D环境感知框架，解决了光学传感器在农业环境中的局限性，通过三个核心模块实现了密集准确的语义感知，并在性能和计算效率上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;准确且鲁棒的环境感知对机器人自主导航至关重要。当前方法主要采用光学传感器，但它们易受视觉遮挡影响，导致性能下降或系统故障。农业场景中机器人面临传感器污染风险。&lt;h4&gt;目的&lt;/h4&gt;利用雷达的强穿透能力，开发基于雷达的3D环境感知框架作为光学传感器的替代方案，实现密集且准确的语义感知。&lt;h4&gt;方法&lt;/h4&gt;提出了包含三个核心模块的雷达3D感知框架：1) 并行帧积累增强雷达原始数据信噪比；2) 基于扩散模型的分层学习框架过滤旁瓣伪影并生成细粒度3D语义点云；3) 专门设计的稀疏3D网络优化处理大规模雷达原始数据。在真实农业场景数据集上进行了实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，该方法在结构和语义预测性能上更优，计算成本降低51.3%，内存成本降低27.5%，能够完全重建和准确分类细小结构（如电线杆和电线），而现有方法难以感知这些结构。&lt;h4&gt;结论&lt;/h4&gt;该方法在密集准确的3D雷达感知方面具有潜力，特别适用于光学传感器易受影响的农业环境。&lt;h4&gt;翻译&lt;/h4&gt;准确且鲁棒的环境感知对机器人自主导航至关重要。虽然当前方法通常采用光学传感器（如摄像头、激光雷达）作为主要感知模式，但它们易受视觉遮挡影响，常常导致性能下降或系统完全失效。本文关注农业场景中机器人面临传感器污染风险的情况。利用雷达的强穿透能力，我们引入了一个基于雷达的3D环境感知框架作为可行的替代方案。它包含三个为密集准确语义感知设计的核心模块：1) 并行帧积累以增强雷达原始数据的信噪比；2) 基于扩散模型的分层学习框架，先过滤雷达旁瓣伪影，然后生成细粒度3D语义点云；3) 专门设计的稀疏3D网络，优化处理大规模雷达原始数据。我们在真实农业场景中收集的自建数据集上进行了广泛的基准比较和实验评估。结果表明，与现有方法相比，我们的方法在结构和语义预测性能上更优，同时分别将计算和内存成本降低了51.3%和27.5%。此外，我们的方法能够完全重建并准确分类细小结构（如电线杆和电线）-现有方法难以感知这些结构-凸显了其在密集准确3D雷达感知方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决农业机器人在复杂环境中使用光学传感器（如摄像头、激光雷达）时，由于传感器被污染（如泥土、水滴遮挡）导致的环境感知性能下降问题。同时，虽然毫米波雷达具有穿透能力，但存在信噪比低、角分辨率低等问题。这个问题在现实中很重要，因为农业环境恶劣，传感器容易被污染，导致机器人'失明'，而精准的环境感知对农业机器人的自主导航和作业效率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了毫米波雷达在农业环境中的三大挑战：低信噪比、低角分辨率和旁瓣污染、大规模数据。然后指出现有方法（判别式学习和生成式学习）的不足，判别式学习难以处理雷达数据与真实数据之间的巨大差距，生成式学习面临训练困难和计算开销大的问题。作者借鉴了扩散模型在生成高质量点云方面的优势和判别式学习的结构预测能力，设计了一个从粗到细的框架，结合了并行帧累积、分层扩散学习和稀疏网络处理技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用从粗到细的分层学习框架，利用雷达的穿透能力替代易受污染的光学传感器，通过并行帧累积提高信噪比，使用扩散模型生成高质量的3D语义点云，并采用稀疏表示降低计算和内存开销。整体流程包括：1）数据预处理（并行帧累积生成RCC和RPC）；2）第一阶段预测粗粒度结构掩码和语义掩码；3）第二阶段使用扩散模型生成精细的3D语义点云；4）输出最终的3D语义点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出首个解决毫米波雷达感知增强和语义分割的系统性方案Sem-RaDiff；2）设计并行帧累积算法提高信噪比；3）提出从粗到细的分层扩散学习框架；4）引入稀疏3D表示和专用网络提高计算效率；5）集成扩散模型的一步推理技术便于部署。相比之前工作，不同之处在于：结合判别式和生成式方法的优点，同时使用RCC和RPC作为输入，采用两阶段处理先过滤噪声再生成精细点云，针对农业场景特殊需求优化，计算和内存开销显著降低。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Sem-RaDiff通过创新的从粗到细的扩散模型框架和稀疏网络设计，显著提高了毫米波雷达在复杂农业环境中的3D语义感知能力，同时大幅降低了计算和内存开销，为农业机器人提供了一种在传感器被污染情况下的可靠感知替代方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust environmental perception is crucial for robot autonomousnavigation. While current methods typically adopt optical sensors (e.g.,camera, LiDAR) as primary sensing modalities, their susceptibility to visualocclusion often leads to degraded performance or complete system failure. Inthis paper, we focus on agricultural scenarios where robots are exposed to therisk of onboard sensor contamination. Leveraging radar's strong penetrationcapability, we introduce a radar-based 3D environmental perception framework asa viable alternative. It comprises three core modules designed for dense andaccurate semantic perception: 1) Parallel frame accumulation to enhancesignal-to-noise ratio of radar raw data. 2) A diffusion model-basedhierarchical learning framework that first filters radar sidelobe artifactsthen generates fine-grained 3D semantic point clouds. 3) A specificallydesigned sparse 3D network optimized for processing large-scale radar raw data.We conducted extensive benchmark comparisons and experimental evaluations on aself-built dataset collected in real-world agricultural field scenes. Resultsdemonstrate that our method achieves superior structural and semanticprediction performance compared to existing methods, while simultaneouslyreducing computational and memory costs by 51.3% and 27.5%, respectively.Furthermore, our approach achieves complete reconstruction and accurateclassification of thin structures such as poles and wires-which existingmethods struggle to perceive-highlighting its potential for dense and accurate3D radar perception.</description>
      <author>example@mail.com (Ruibin Zhang, Fei Gao)</author>
      <guid isPermaLink="false">2509.02283v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Geometry Compression and Communication for 3D Gaussian Splatting Point Clouds</title>
      <link>http://arxiv.org/abs/2509.02232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对基于i3DV平台的动态3D场景表示中的存储和传输挑战，提出了一种结合AVS PCRM压缩技术和哈希表优化的联合框架，实现了3D体积视频的高效存储、传输和交互。&lt;h4&gt;背景&lt;/h4&gt;随着场景复杂度的增加，3D高斯数据量爆炸式增长导致存储空间占用过高，给3D体积视频的存储和传输带来挑战。&lt;h4&gt;目的&lt;/h4&gt;解决动态3D场景表示中的存储和传输问题，同时保持3D高斯技术的快速渲染和高质量合成优势。&lt;h4&gt;方法&lt;/h4&gt;采用AVS PCRM参考软件压缩高斯点云几何数据，并将其深度集成到i3DV平台中，与基于二进制哈希表的原始率失真优化机制形成技术互补。哈希表用于高效缓存帧间高斯点变换关系，AVS PCRM用于精确压缩几何数据。&lt;h4&gt;主要发现&lt;/h4&gt;联合框架在保持3D高斯技术快速渲染和高质量合成优势的同时，在通用测试集上实现了10%-25%的比特率节省，在40 Mbps带宽限制内实现高保真传输。&lt;h4&gt;结论&lt;/h4&gt;该研究为3D体积视频的存储、传输和交互提供了优越的率失真权衡解决方案，显著提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;基于i3DV平台的动态3D场景表示中的存储和传输挑战，随着场景复杂度的增加，3D高斯数据量的爆炸式增长导致了过高的存储空间占用。为了解决这个问题，我们提出采用AVS PCRM参考软件来高效压缩高斯点云几何数据。该策略将AVS PCRM的高级编码能力深度集成到i3DV平台中，与基于二进制哈希表的原始率失真优化机制形成技术互补。一方面，哈希表高效缓存帧间高斯点变换关系，使得在40 Mbps带宽限制内实现高保真传输；另一方面，AVS PCRM对几何数据进行精确压缩。实验结果表明，联合框架在保持3D高斯技术快速渲染和高质量合成优势的同时，在通用测试集上实现了显著的10%-25%比特率节省。它为3D体积视频的存储、传输和交互提供了优越的率失真权衡解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3680207.3765659&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Storage and transmission challenges in dynamic 3D scene representation basedon the i3DV platform, With increasing scene complexity, the explosive growth of3D Gaussian data volume causes excessive storage space occupancy. To addressthis issue, we propose adopting the AVS PCRM reference software for efficientcompression of Gaussian point cloud geometry data. The strategy deeplyintegrates the advanced encoding capabilities of AVS PCRM into the i3DVplatform, forming technical complementarity with the original rate-distortionoptimization mechanism based on binary hash tables. On one hand, the hash tableefficiently caches inter-frame Gaussian point transformation relationships,which allows for high-fidelity transmission within a 40 Mbps bandwidthconstraint. On the other hand, AVS PCRM performs precise compression ongeometry data. Experimental results demonstrate that the joint frameworkmaintains the advantages of fast rendering and high-quality synthesis in 3DGaussian technology while achieving significant 10\%-25\% bitrate savings onuniversal test sets. It provides a superior rate-distortion tradeoff solutionfor the storage, transmission, and interaction of 3D volumetric video.</description>
      <author>example@mail.com (Liang Xie, Yanting Li, Luyang Tang, Wei Gao)</author>
      <guid isPermaLink="false">2509.02232v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</title>
      <link>http://arxiv.org/abs/2509.01547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FGO-SLAM是一种创新的高斯SLAM系统，通过不透明辐射场表示和全局优化技术，显著提升了场景重建质量和跟踪精度，解决了传统SLAM和高斯SLAM系统的局限性。&lt;h4&gt;背景&lt;/h4&gt;视觉SLAM因其能够为具身AI提供感知能力和仿真测试数据而重新受到关注。然而，传统SLAM方法难以满足高质量场景重建的需求，而高斯SLAM系统尽管具有快速渲染和高质量映射的能力，但缺乏有效的姿态优化方法，并且在几何重建方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统SLAM方法在高质量场景重建方面的不足，以及高斯SLAM系统缺乏有效姿态优化方法和几何重建挑战的问题。&lt;h4&gt;方法&lt;/h4&gt;引入FGO-SLAM系统，采用不透明辐射场作为场景表示以增强几何映射性能；在初始姿态估计后应用全局调整优化相机姿态和稀疏点云；基于三维高斯保持全局一致的不透明辐射场，引入深度失真和法线一致性项细化场景表示；构建四面体网格后识别水平集，直接从三维高斯中提取表面。&lt;h4&gt;主要发现&lt;/h4&gt;在各种真实世界和大规模合成数据集上的结果表明，FGO-SLAM实现了最先进的跟踪精度和映射性能。&lt;h4&gt;结论&lt;/h4&gt;FGO-SLAM系统通过不透明辐射场和全局优化方法，有效解决了传统SLAM和高斯SLAM系统的局限性，实现了高质量的场景重建和精确的跟踪。&lt;h4&gt;翻译&lt;/h4&gt;视觉SLAM因其能够为具身AI提供感知能力和仿真测试数据而重新受到关注。然而，传统SLAM方法难以满足高质量场景重建的需求，而高斯SLAM系统尽管具有快速渲染和高质量映射的能力，但缺乏有效的姿态优化方法，并且在几何重建方面面临挑战。为解决这些问题，我们引入了FGO-SLAM，一种采用不透明辐射场作为场景表示的高斯SLAM系统，以增强几何映射性能。在初始姿态估计后，我们应用全局调整来优化相机姿态和稀疏点云，确保方法的鲁棒跟踪。此外，我们基于三维高斯保持全局一致的不透明辐射场，并引入深度失真和法线一致性项来细化场景表示。进一步地，在构建四面体网格后，我们识别水平集以直接从三维高斯中提取表面。在各种真实世界和大规模合成数据集上的结果表明，我们的方法实现了最先进的跟踪精度和映射性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统SLAM方法难以满足高质量场景重建需求的问题，以及现有Gaussian SLAM系统缺乏有效姿态优化方法和几何重建能力的问题。这个问题在现实和研究中很重要，因为具身人工智能(Embodied AI)需要密集的视觉SLAM方法来提供感知能力并重建高质量场景地图，而传统方法生成的低分辨率、有空洞且缺乏纹理细节的地图已无法满足AR/VR、机器人导航等应用的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统SLAM地图质量不高，NeRF-based SLAM存在灾难性遗忘和过度平滑问题，3D Gaussian Splatting虽渲染快但缺乏有效姿态优化。作者借鉴了3DGS的高质量重建能力、传统视觉里程计的鲁棒性、光度一致性技术以及Marching Tetrahedra表面提取算法，设计了FGO-SLAM系统，结合了传统SLAM方法和3D高斯表示的优点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用全局一致的透明度辐射场作为场景表示，结合传统SLAM和3D高斯表示的优点。整体流程分为三部分：1)跟踪模块：提取特征点估计相机姿态，检测回环并进行全局优化；2)映射模块：构建基于3D高斯的透明度辐射场，使用正则化项优化场景表示；3)表面提取模块：为每个高斯构建边界框和四面体网格，通过评估顶点透明度识别等值集，使用Marching Tetrahedra算法提取网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全局一致的透明度辐射场作为场景表示；2)全局姿态优化技术；3)深度失真和法线一致性正则化项；4)直接从3D高斯体提取表面的方法。相比之前工作，FGO-SLAM解决了传统SLAM地图质量低、NeRF-based SLAM速度慢和过度平滑、现有Gaussian SLAM缺乏有效姿态优化和表面提取能力的问题，同时支持单目相机，扩大了应用范围。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FGO-SLAM通过引入全局一致的透明度辐射场和全局优化技术，解决了现有Gaussian SLAM系统在姿态优化和几何重建方面的挑战，实现了高质量的场景重建和表面提取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual SLAM has regained attention due to its ability to provide perceptualcapabilities and simulation test data for Embodied AI. However, traditionalSLAM methods struggle to meet the demands of high-quality scene reconstruction,and Gaussian SLAM systems, despite their rapid rendering and high-qualitymapping capabilities, lack effective pose optimization methods and facechallenges in geometric reconstruction. To address these issues, we introduceFGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as thescene representation to enhance geometric mapping performance. After initialpose estimation, we apply global adjustment to optimize camera poses and sparsepoint cloud, ensuring robust tracking of our approach. Additionally, wemaintain a globally consistent opacity radiance field based on 3D Gaussians andintroduce depth distortion and normal consistency terms to refine the scenerepresentation. Furthermore, after constructing tetrahedral grids, we identifylevel sets to directly extract surfaces from 3D Gaussians. Results acrossvarious real-world and large-scale synthetic datasets demonstrate that ourmethod achieves state-of-the-art tracking accuracy and mapping performance.</description>
      <author>example@mail.com (Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu)</author>
      <guid isPermaLink="false">2509.01547v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Continuous-Time Consistency Model for 3D Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2509.01492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ConTiCoM-3D的连续时间一致性模型，用于快速准确地从点云生成3D形状，在机器人、AR/VR和数字内容创建领域具有广泛应用。&lt;h4&gt;背景&lt;/h4&gt;快速准确的3D形状生成对机器人、AR/VR和数字内容创建应用至关重要，但现有方法通常依赖于迭代去噪或潜在解码器，效率不高。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接在点空间中合成3D形状的连续时间一致性模型，无需离散扩散步骤、预训练教师模型或潜在空间编码，实现高效高质量的3D形状生成。&lt;h4&gt;方法&lt;/h4&gt;ConTiCoM-3D整合了受TrigFlow启发的连续噪声计划和基于Chamfer距离的几何损失，使用时间条件神经网络在连续时间上操作，支持高效的一步至两步推理，同时避免昂贵的雅可比-向量乘积。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet基准测试上，ConTiCoM-3D在质量和效率方面匹配或优于最先进的扩散和潜在一致性模型，实现了高几何保真度的快速生成。&lt;h4&gt;结论&lt;/h4&gt;ConTiCoM-3D是可扩展3D形状生成的实用框架，为3D内容创作提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;快速且准确的从点云生成3D形状对机器人、AR/VR和数字内容创建应用至关重要。我们引入了ConTiCoM-3D，一种连续时间一致性模型，直接在点空间中合成3D形状，无需离散扩散步骤、预训练教师模型或潜在空间编码。该方法整合了受TrigFlow启发的连续噪声计划与基于Chamfer距离的几何损失，能够在高维点集上进行稳定训练，同时避免昂贵的雅可比-向量乘积。这种设计支持高效的一步至两步推理，并具有高几何保真度。与依赖迭代去噪或潜在解码器的先前方法相比，ConTiCoM-3D采用完全在连续时间上运行的时间条件神经网络，从而实现快速生成。ShapeNet基准测试上的实验表明，ConTiCoM-3D在质量和效率方面均匹配或优于最先进的扩散和潜在一致性模型，确立了其作为可扩展3D形状生成的实用框架的地位。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云生成中的效率和精度平衡问题。现有的3D生成方法要么需要大量计算步骤（如扩散模型），要么依赖复杂的预训练模型（如潜在一致性模型），导致生成速度慢或质量下降。这个问题在机器人、AR/VR和数字内容创作等领域至关重要，因为这些应用通常需要在硬件限制下实现实时高质量3D形状生成。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D生成方法的局限性：VAE/GAN速度快但多样性不足，流模型需要迭代采样，扩散模型计算昂贵，潜在方法引入伪影。他们借鉴了TrigFlow的连续噪声调度和Flow Matching的速度场学习技术，但创新性地避免了这些方法的缺点。设计思路是创建一个直接在原始点空间操作的模型，不依赖教师模型、Jacobian计算或潜在空间压缩，而是结合解析流匹配和Chamfer几何损失，实现稳定高效的训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接在原始点空间中建模连续时间一致性，结合解析流匹配和几何感知损失。整体流程包括：1) 使用TrigFlow前向过程将干净点云逐步添加噪声；2) 设计时间条件神经网络预测速度场；3) 通过流匹配损失确保学习正确速度场，同时用Chamfer损失保证几何保真度；4) 推理时只需1-2步即可生成高质量点云，比传统扩散模型快数百倍。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个直接在原始点空间操作的连续时间一致性模型，无需潜在编码；2) 避免了计算成本高且不稳定的Jacobian-向量乘积；3) 摒弃了依赖预训练教师模型的蒸馏方法；4) 使用Chamfer距离提供几何感知的排列不变监督。相比之前的工作，ConTiCoM-3D在保持高质量的同时实现了显著更快的推理速度（0.22秒/样本），且避免了潜在空间压缩带来的结构伪影，在ShapeNet基准测试中超越了扩散模型和潜在一致性模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ConTiCoM-3D首次实现了直接在原始点空间中操作的连续时间一致性模型，通过结合解析流匹配和Chamfer重建损失，实现了高效的一到两步高质量3D点云生成，避免了现有方法中的计算瓶颈和质量损失问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and accurate 3D shape generation from point clouds is essential forapplications in robotics, AR/VR, and digital content creation. We introduceConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapesdirectly in point space, without discretized diffusion steps, pre-trainedteacher models, or latent-space encodings. The method integrates aTrigFlow-inspired continuous noise schedule with a Chamfer Distance-basedgeometric loss, enabling stable training on high-dimensional point sets whileavoiding expensive Jacobian-vector products. This design supports efficientone- to two-step inference with high geometric fidelity. In contrast toprevious approaches that rely on iterative denoising or latent decoders,ConTiCoM-3D employs a time-conditioned neural network operating entirely incontinuous time, thereby achieving fast generation. Experiments on the ShapeNetbenchmark show that ConTiCoM-3D matches or outperforms state-of-the-artdiffusion and latent consistency models in both quality and efficiency,establishing it as a practical framework for scalable 3D shape generation.</description>
      <author>example@mail.com (Sebastian Eilermann, René Heesch, Oliver Niggemann)</author>
      <guid isPermaLink="false">2509.01492v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds</title>
      <link>http://arxiv.org/abs/2509.01487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript submitted to PATTERN RECOGNITION, currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PointSlice是一种新的点云处理方法，通过将3D点云转换为2D切片并结合专门的检测网络，实现了高检测精度和推理速度的平衡。&lt;h4&gt;背景&lt;/h4&gt;3D目标检测在自动驾驶中起关键作用，当前主要方法有基于体素(高精度但慢)和基于柱状(速度快但精度低)两种方法。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云处理方法中精度与速度之间的权衡问题，提出一种既能保持高精度又能提高推理速度的新方法。&lt;h4&gt;方法&lt;/h4&gt;PointSlice沿水平平面切割点云转换为多组2D(x-y)数据切片，模型只学习2D数据分布；引入切片交互网络(SIN)保持切片间的垂直关系，提高3D物体感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;PointSlice在多个数据集上表现优异：在Waymo上比SAFDNet快1.13倍，参数减少0.79倍，精度仅降1.2 mAPH；在nuScenes上达到66.74 mAP的最先进结果；在Argoverse 2上快1.10倍，参数减少0.66倍，精度降1.0 mAP。&lt;h4&gt;结论&lt;/h4&gt;PointSlice成功平衡了检测精度与推理速度，是一种高效的3D目标检测方法。&lt;h4&gt;翻译&lt;/h4&gt;3D点云目标检测在自动驾驶中扮演关键角色。目前，点云处理的主要方法是基于体素和基于柱状的方法。基于体素的方法通过细粒度空间分割提供高精度，但推理速度较慢。基于柱状的方法提高了推理速度，但在精度上仍不及基于体素的方法。为解决这些问题，我们提出了一种新的点云处理方法PointSlice，它沿水平平面切割点云并包含专门的检测网络。PointSlice的主要贡献是：(1)一种新的点云处理技术，将3D点云转换为多组2D(x-y)数据切片。模型只学习2D数据分布，将3D点云视为独立的2D数据批次，这减少了模型参数数量并提高了推理速度；(2)引入切片交互网络(SIN)。为了保持切片间的垂直关系，我们将SIN集成到2D骨干网络中，提高了模型的3D物体感知能力。大量实验表明，PointSlice实现了高检测精度和推理速度。在Waymo数据集上，PointSlice比最先进的基于体素方法(SAFDNet)快1.13倍，参数减少0.79倍，精度仅降低1.2 mAPH。在nuScenes数据集上，我们实现了66.74 mAP的最先进检测结果。在Argoverse 2数据集上，PointSlice快1.10倍，参数减少0.66倍，精度降低1.0 mAP。代码将在https://github.com/qifeng22/PointSlice2上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云目标检测中效率与准确性之间的权衡问题。当前基于体素的方法精度高但速度慢，而基于柱体的方法速度快但精度低。这个问题在自动驾驶等领域至关重要，因为3D目标检测需要同时满足高精度和实时性要求，以保障安全和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点，发现将基于体素的方法直接应用于基于柱体格式会导致精度显著下降。因此，作者提出将3D点云沿水平面切片转换为多组2D数据，使模型能使用高效的2D卷积网络。该方法借鉴了现有工作中的稀疏卷积技术、稀疏编码器-解码块和稀疏检测头等，并创新性地设计了切片交互网络(SIN)来保持不同切片间的垂直关系，弥补单纯使用2D切片可能丢失的3D信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云沿水平面切片转换为多组2D数据，使用2D卷积网络提高效率，同时引入切片交互网络(SIN)保留3D信息。整体流程分为四步：1)点云到切片转换：将点云体素化后沿高度方向转换为2D切片；2)稀疏2D骨干网络：使用2D稀疏残差块和编码器-解码块提取特征；3)切片交互网络：通过3D稀疏卷积实现跨切片信息交换；4)稀疏检测头：输出最终检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的点云表示方法：将3D点云转换为多组2D切片，减少模型参数；2)切片交互网络(SIN)：专门设计用于保持不同切片间的垂直关系；3)高效的3D目标检测框架：结合2D卷积的高效率和3D卷积的准确性。相比之前的工作，PointSlice在保持接近基于体素方法精度的同时，显著提高了推理速度(1.13倍)并减少了模型参数(0.79倍)，同时解决了基于柱体方法精度不足的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointSlice通过创新的水平切片表示方法和切片交互网络，实现了3D点云目标检测中高精度与高效率的有效平衡，在保持接近基于体素方法精度的同时显著提高了推理速度并减少了模型参数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection from point clouds plays a critical role in autonomousdriving. Currently, the primary methods for point cloud processing arevoxel-based and pillarbased approaches. Voxel-based methods offer high accuracythrough fine-grained spatial segmentation but suffer from slower inferencespeeds. Pillar-based methods enhance inference speed but still fall short ofvoxel-based methods in accuracy. To address these issues, we propose a novelpoint cloud processing method, PointSlice, which slices point clouds along thehorizontal plane and includes a dedicated detection network. The maincontributions of PointSlice are: (1) A new point cloud processing techniquethat converts 3D point clouds into multiple sets of 2D (x-y) data slices. Themodel only learns 2D data distributions, treating the 3D point cloud asseparate batches of 2D data, which reduces the number of model parameters andenhances inference speed; (2) The introduction of a Slice Interaction Network(SIN). To maintain vertical relationships across slices, we incorporate SINinto the 2D backbone network, which improves the model's 3D object perceptioncapability. Extensive experiments demonstrate that PointSlice achieves highdetection accuracy and inference speed. On the Waymo dataset, PointSlice is1.13x faster and has 0.79x fewer parameters than the state-of-the-artvoxel-based method (SAFDNet), with only a 1.2 mAPH accuracy reduction. On thenuScenes dataset, we achieve a state-of-the-art detection result of 66.74 mAP.On the Argoverse 2 dataset, PointSlice is 1.10x faster, with 0.66x fewerparameters and a 1.0 mAP accuracy reduction. The code will be available athttps://github.com/qifeng22/PointSlice2.</description>
      <author>example@mail.com (Liu Qifeng, Zhao Dawei, Dong Yabo, Xiao Liang, Wang Juan, Min Chen, Li Fuyang, Jiang Weizhong, Lu Dongming, Nie Yiming)</author>
      <guid isPermaLink="false">2509.01487v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation</title>
      <link>http://arxiv.org/abs/2509.01317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种端到端框架，联合解决LiDAR超分辨率和语义分割问题，使低成本16通道LiDAR能够实现与高成本64通道LiDAR相当的分割性能。&lt;h4&gt;背景&lt;/h4&gt;高分辨率LiDAR数据在自动驾驶3D语义分割中起关键作用，但高级传感器成本高限制了大规模部署；而低成本传感器如16通道LiDAR产生的稀疏点云会降低分割精度。&lt;h4&gt;目的&lt;/h4&gt;克服低成本传感器导致的精度下降问题，提出一个端到端框架同时解决LiDAR超分辨率和语义分割。&lt;h4&gt;方法&lt;/h4&gt;引入首个端到端框架，通过训练过程中的联合优化使SR模块融入语义线索并保留精细细节；设计新的SR损失函数指导网络关注感兴趣区域；采用轻量级基于模型的SR架构，参数量少于现有方法，且易于与分割网络兼容。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法实现的分割性能可与使用高分辨率且昂贵的64通道LiDAR数据的模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该框架能够在保持较低成本的同时实现高精度的3D语义分割，解决了低成本传感器部署的瓶颈问题。&lt;h4&gt;翻译&lt;/h4&gt;高分辨率LiDAR数据在自动驾驶的3D语义分割中起着关键作用，但高级传感器的高成本限制了大规模部署。相比之下，16通道LiDAR等低成本传感器产生的稀疏点云会降低分割精度。为克服这一问题，我们引入了首个端到端框架，联合解决LiDAR超分辨率(SR)和语义分割问题。该框架在训练过程中采用联合优化，使SR模块能够融入语义线索并保留精细细节，特别是对小目标类别。新的SR损失函数进一步指导网络关注感兴趣区域。提出的轻量级基于模型的SR架构比现有LiDAR SR方法使用少得多的参数，同时保持与分割网络的良好兼容性。实验表明，我们的方法实现了与使用高分辨率且昂贵的64通道LiDAR数据的模型相当的分割性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高分辨率LiDAR传感器成本高昂（约85,000美元）而低成本传感器（如16通道LiDAR，约4,000美元）产生的稀疏点云导致自动驾驶场景中3D语义分割性能下降的问题。这个问题在现实中很重要，因为它限制了高性能自动驾驶感知系统的广泛采用，而现有超分辨率方法与分割任务独立优化导致性能不佳，特别是对较小类别的细节保留不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了高分辨率LiDAR数据的重要性与高成本之间的矛盾，以及现有SR方法与分割任务独立优化的局限性。他们借鉴了投影方法（如FIDNet、CENet、LENet）将3D点云转换为2D表示，以及深度展开策略和半二次分裂方法。作者设计了一个轻量级的基于模型的SR网络，通过引入分割指导的可学习掩模模块和上下文感知损失函数，实现了SR和分割任务的联合优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将SR和分割任务整合在一个统一的端到端框架中，通过联合训练确保SR过程受益于分割指导，同时使用轻量级模型提高效率。整体流程是：1)将16通道低分辨率LiDAR点云转换为低分辨率距离图像；2)SR网络处理（包括数据一致性模块、去噪模块和分割引导模块）；3)将高分辨率距离图像输入分割网络进行3D语义分割；4)通过端到端训练和上下文感知损失函数优化整体性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引导模型基于SR网络，引入分割信息指导的可学习掩模模块；2)端到端架构，首次整合SR和分割任务进行联合优化；3)上下文感知损失函数，增强对较小类别的关注。相比之前工作，不同之处在于：不是独立训练SR和分割网络，而是端到端联合优化；参数数量减少99%，计算效率更高（23 fps vs 6 fps）；专为分割任务设计，更好地保留较小类别的细节结构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种端到端框架，通过联合优化轻量级超分辨率网络和分割网络，实现了低成本LiDAR传感器达到高性能分割效果，同时显著减少了计算复杂度并保留了较小类别的细节结构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-resolution LiDAR data plays a critical role in 3D semantic segmentationfor autonomous driving, but the high cost of advanced sensors limitslarge-scale deployment. In contrast, low-cost sensors such as 16-channel LiDARproduce sparse point clouds that degrade segmentation accuracy. To overcomethis, we introduce the first end-to-end framework that jointly addresses LiDARsuper-resolution (SR) and semantic segmentation. The framework employs jointoptimization during training, allowing the SR module to incorporate semanticcues and preserve fine details, particularly for smaller object classes. A newSR loss function further directs the network to focus on regions of interest.The proposed lightweight, model-based SR architecture uses significantly fewerparameters than existing LiDAR SR approaches, while remaining easily compatiblewith segmentation networks. Experiments show that our method achievessegmentation performance comparable to models operating on high-resolution andcostly 64-channel LiDAR data.</description>
      <author>example@mail.com (Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2509.01317v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views</title>
      <link>http://arxiv.org/abs/2509.01250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Point-PQAE，一种用于点云自监督学习的跨视图生成范式。该方法通过生成两个解耦的点云视图并从一个重建另一个，提供了更具挑战性和信息量的预训练方式，在3D自监督学习任务中取得了优于单视图自重建方法的性能。&lt;h4&gt;背景&lt;/h4&gt;点云学习，特别是无需人工标签的自监督学习方式，在视觉和学习领域引起了广泛关注。现有方法主要专注于从单视图中的可见点恢复被遮挡的点，而双视图预训练范式能引入更大的多样性和变异性。&lt;h4&gt;目的&lt;/h4&gt;探索双视图学习在点云自监督学习领域的潜力，开发一种比单视图自重建更具挑战性的预训练方法。&lt;h4&gt;方法&lt;/h4&gt;提出了Point-PQAE，一种跨重建生成范式，首先生成两个解耦的点云/视图，然后从一个重建另一个。为此，论文首次开发了点云视图生成的裁剪机制，并提出了新的位置编码来表示两个解耦视图之间的3D相对位置。&lt;h4&gt;主要发现&lt;/h4&gt;跨重建显著增加了预训练的难度，使该方法在3D自监督学习中超越了之前单模态自重建方法。在ScanObjectNN的三个变体中，使用Mlp-Linear评估协议，该方法比自重建基线（Point-MAE）分别高出6.5%、7.0%和6.7%。&lt;h4&gt;结论&lt;/h4&gt;双视图预训练范式能够提供更具挑战性和信息量的预训练，Point-PQAE通过跨重建方法在3D自监督学习任务中取得了优于单视图自重建方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;点云学习，特别是无需人工标签的自监督方式，由于其潜在的在广泛应用中的实用性，在视觉和学习社区中获得了越来越多的关注。大多数现有的点云自监督学习生成方法专注于从单视图中的可见点恢复被遮挡的点。认识到双视图预训练范式本质上引入了更大的多样性和变异性，因此它可能实现更具挑战性和信息量的预训练。受此启发，我们探索了该领域中双视图学习的潜力。在本文中，我们提出了Point-PQAE，一种跨重建生成范式，首先生成两个解耦的点云/视图，然后从一个重建另一个。为实现这一目标，我们首次开发了点云视图生成的裁剪机制，并进一步提出了新的位置编码来表示两个解耦视图之间的3D相对位置。与自重建相比，跨重建显著增加了预训练的难度，使我们的方法在3D自监督学习中超越了之前的单模态自重建方法。具体而言，在ScanObjectNN的三个变体中，使用Mlp-Linear评估协议，它比自重建基线（Point-MAE）分别高出6.5%、7.0%和6.7%。代码可在https://github.com/aHapBean/Point-PQAE获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云自监督学习中现有方法局限于单一视图内自重建的问题。这个问题很重要，因为点云数据在自动驾驶、机器人等3D视觉应用中至关重要，但点云标注成本高昂，自监督学习可减少对标注数据的依赖；同时，单视图学习已被证明不如两视图学习具有挑战性和信息量，限制了学习到的表示的丰富性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到两视图预训练范式能引入更大的多样性和变化，使预训练更具挑战性。他们借鉴了2D视觉中的两视图学习范式（如对比学习方法SimCLR、MoCo和掩码图像建模方法MAE）以及点云自监督学习方法（如Point-MAE）的基本框架，但扩展到点云领域的交叉重建。作者面临两大挑战：如何在点云中构建两个视图，以及如何实现视图间的交叉重建，为此设计了点云裁剪机制和视图相对位置嵌入（VRPE）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出交叉重建范式而非传统的自重建，通过从一个视图重建另一个视图增加预训练难度，使模型学习更丰富、鲁棒的表示。整体流程：1)解耦视图生成：随机裁剪点云创建两个视图，通过归一化和旋转实现解耦；2)VRPE生成：计算两视图间3D相对位置并使用正弦编码生成视图相对位置嵌入；3)位置查询：使用位置查询块通过交叉注意力捕获视图间相对位置信息；4)解码：将查询结果输入解码器预测另一视图；5)损失计算：使用l2 Chamfer距离计算交叉重建损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)新生成框架Point-PQAE，首次将交叉重建引入3D点云自监督学习，包含解耦视图生成、VRPE生成和位置查询块；2)首个点云相对位置感知查询模块，可轻松集成到其他任务；3)强大的性能，在多个基准测试上实现最先进结果。相比之前工作不同：不同于Point-MAE等自重建方法（从单一视图内重建被掩码点），Point-PQAE实现视图间交叉重建，需学习视图间位置关系和视图内空间信息；不同于对比学习方法（如PointContrast），Point-PQAE使用生成方法专注于重建任务；不同于跨模态方法（如ACT），Point-PQAE仅使用单模态点云数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point-PQAE通过引入交叉重建范式和视图相对位置嵌入，显著提高了点云自监督学习的表示学习能力和下游任务的性能，实现了更丰富、更具挑战性的预训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud learning, especially in a self-supervised way without manuallabels, has gained growing attention in both vision and learning communitiesdue to its potential utility in a wide range of applications. Most existinggenerative approaches for point cloud self-supervised learning focus onrecovering masked points from visible ones within a single view. Recognizingthat a two-view pre-training paradigm inherently introduces greater diversityand variance, it may thus enable more challenging and informative pre-training.Inspired by this, we explore the potential of two-view learning in this domain.In this paper, we propose Point-PQAE, a cross-reconstruction generativeparadigm that first generates two decoupled point clouds/views and thenreconstructs one from the other. To achieve this goal, we develop a cropmechanism for point cloud view generation for the first time and furtherpropose a novel positional encoding to represent the 3D relative positionbetween the two decoupled views. The cross-reconstruction significantlyincreases the difficulty of pre-training compared to self-reconstruction, whichenables our method to surpass previous single-modal self-reconstruction methodsin 3D self-supervised learning. Specifically, it outperforms theself-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in threevariants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code isavailable at https://github.com/aHapBean/Point-PQAE.</description>
      <author>example@mail.com (Xiangdong Zhang, Shaofeng Zhang, Junchi Yan)</author>
      <guid isPermaLink="false">2509.01250v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PVINet: Point-Voxel Interlaced Network for Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2509.01097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种点-体素交错网络（PVINet），用于点云压缩，通过并行处理全局和局部特征并进行交互，提高特征感知效率。&lt;h4&gt;背景&lt;/h4&gt;点云压缩中，重建点云的质量依赖于全局结构和局部上下文，而现有方法通常顺序处理这两类信息，缺乏它们之间的交流。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够并行捕获全局结构特征和局部上下文特征，并在每个尺度进行交互的网络，以增强特征感知效率。&lt;h4&gt;方法&lt;/h4&gt;PVINet包含基于体素的编码器（Ev）提取全局结构特征，基于点的编码器（Ep）建模局部上下文，引入条件稀疏卷积应用点嵌入动态定制体素特征提取的核，解码时使用条件稀疏卷积将点嵌入作为指导重建点云。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验表明，PVINet与最先进的方法相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;PVINet通过并行处理全局和局部信息并进行交互，有效提高了点云压缩的质量。&lt;h4&gt;翻译&lt;/h4&gt;在点云压缩中，重建点云的质量依赖于全局结构和局部上下文，而现有方法通常顺序处理全局和局部信息，缺乏这两类信息之间的交流。在本文中，我们提出了一种点-体素交错网络（PVINet），该网络并行捕获全局结构特征和局部上下文特征，并在每个尺度进行交互以增强特征感知效率。具体来说，PVINet包含一个基于体素的编码器（Ev）用于提取全局结构特征，以及一个基于点的编码器（Ep），该编码器建模以每个体素为中心的局部上下文。特别地，引入了一种新颖的条件稀疏卷积，它应用点嵌入来动态定制体素特征提取的核，促进从Ep到Ev的特征交互。在解码过程中，基于体素的解码器使用条件稀疏卷积将点嵌入作为指导来重建点云。在基准数据集上的实验表明，PVINet与最先进的方法相比具有竞争力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云压缩中重建质量的问题，特别是如何在压缩过程中同时保留点云的全局结构和局部细节。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、机器人、人体建模等领域，但庞大的数据量给存储和传输带来了巨大挑战。有效的压缩技术能够在保持关键几何信息的同时，显著降低数据存储和传输成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有的点云压缩方法，发现基于点的方法能很好地捕获局部细节但难以处理大规模点云，而基于体素的方法能高效保留全局结构但可能丢失局部信息。现有的混合方法将这两种技术级联处理，缺乏全局和局部信息之间的有效交互。因此，作者借鉴了这两种方法的优点，设计了点-体素交错网络，并行处理全局和局部特征，并在每个尺度进行交互。同时引入了条件稀疏卷积这一创新组件，促进特征之间的交流。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是并行捕获全局结构特征和局部上下文特征，并在每个尺度进行交互，以增强特征感知效率。整体实现流程包括：1) 点-体素交错编码器，其中基于体素的编码器提取全局结构，基于点的编码器建模局部上下文，两者通过体素到点和点到体的特征交互相互增强；2) 算术编码阶段，使用G-PCC处理体素坐标，熵编码压缩体素特征，并传输路由权重；3) 解码器阶段，利用条件稀疏卷积在路由权重指导下逐步重建点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 点-体素交错网络架构，并行处理全局和局部特征并进行多尺度交互；2) 条件稀疏卷积，利用点嵌入动态定制体素特征提取的核；3) 通过传输路由权重而非完整点特征，有效保留局部细节。相比之前的工作，PVINet突破了现有混合方法中全局和局部信息顺序处理的限制，实现了并行处理和实时交互，显著提升了特征感知效率和重建质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PVINet通过创新的点-体素交错网络结构和条件稀疏卷积，实现了点云压缩中全局结构和局部细节的高效结合，显著提高了重建质量并减少了比特率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In point cloud compression, the quality of a reconstructed point cloud relieson both the global structure and the local context, with existing methodsusually processing global and local information sequentially and lackingcommunication between these two types of information. In this paper, we proposea point-voxel interlaced network (PVINet), which captures global structuralfeatures and local contextual features in parallel and performs interactions ateach scale to enhance feature perception efficiency. Specifically, PVINetcontains a voxel-based encoder (Ev) for extracting global structural featuresand a point-based encoder (Ep) that models local contexts centered at eachvoxel. Particularly, a novel conditional sparse convolution is introduced,which applies point embeddings to dynamically customize kernels for voxelfeature extraction, facilitating feature interactions from Ep to Ev. Duringdecoding, a voxel-based decoder employs conditional sparse convolutions toincorporate point embeddings as guidance to reconstruct the point cloud.Experiments on benchmark datasets show that PVINet delivers competitiveperformance compared to state-of-the-art methods.</description>
      <author>example@mail.com (Xuan Deng, Xingtao Wang, Xiandong Meng, Xiaopeng Fan, Debin Zhao)</author>
      <guid isPermaLink="false">2509.01097v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools</title>
      <link>http://arxiv.org/abs/2509.00699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的算法，通过将G-code表示为立方体和点云，实现了3D打印制造流程中的错误检测和比较，开发了名为GlitchFinder的原型工具，并在58个真实模型上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;3D打印的计算制造流程类似于编译器，用户在CAD工具中设计模型，转换为多边形网格，最终由3D切片器编译为机器代码。传统编译器有成熟的程序不变量检查技术，但这些方法不直接适用于3D制造领域使用的表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种新算法用于提升G-code（制造流程中使用的通用语言），开发能够应用于3D制造流程的类似技术，以实现错误检测和比较。&lt;h4&gt;方法&lt;/h4&gt;提出一种新算法，通过将G-code程序表示为一组立方体，然后为这些立方体定义近似点云表示以便高效操作。实现了名为GlitchFinder的原型工具，并在58个真实世界CAD模型上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;GlitchFinder在识别由小特征引起的切片问题方面特别有效；可以突出显示不同切片器（如Cura和PrusaSlicer）处理相同模型的差异；能够识别网格修复工具（如MeshLab和Meshmixer）在修复过程中引入的新错误。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法为3D制造流程中的错误检测和比较提供了新机会，GlitchFinder工具能够有效识别和定位3D打印制造流程中的各种问题。&lt;h4&gt;翻译&lt;/h4&gt;3D打印的计算制造流程类似于编译器 - 用户在CAD工具中设计模型，然后降低为多边形网格，最终由3D切片器编译为机器代码。对于传统编译器和编程语言，检查程序不变量的技术已经很成熟。类似地，差异测试等方法常用于发现编译器本身的错误，使它们更加可靠。制造流程将从类似技术中受益，但传统方法不直接适用于该领域使用的表示。与传统程序不同，3D模型既是几何对象，也是最终在硬件上运行的机器代码。机器代码像传统编译一样，受许多因素影响，如模型、使用的切片器以及控制切片过程的许多用户可配置参数。在这项工作中，我们提出了一种新算法，通过将G-code（制造流程中使用的通用语言）提升为表示一组立方体，然后为这些立方体定义近似点云表示以便高效操作。我们的算法开辟了新的机会：我们展示了三个用例，演示了它如何通过不变量检查实现CAD模型中的错误定位，定量比较不同切片器，以及评估网格修复工具的效能。我们在一个名为GlitchFinder的工具中实现了我们算法的原型，并在58个真实世界CAD模型上对其进行了评估。我们的结果表明，GlitchFinder在识别由小特征引起的切片问题方面特别有效，可以突出显示流行切片器（Cura和PrusaSlicer）如何切片相同模型的差异，并可以识别网格修复工具（MeshLab和Meshmixer）在修复过程中引入新错误的情况。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D打印制造流程中缺乏对G-code分析工具的问题，无法在打印前检查模型不变性，也无法对不同切片器和网格修复工具进行差异测试。这个问题很重要，因为3D打印是一个迭代过程，经常需要多次尝试才能成功打印一个零件，而模型设计、网格问题和切片器问题都可能导致打印失败。由于3D打印速度慢（可能需要数小时到数天），这种迭代过程既耗时又消耗资源，而传统编译器技术不适用于3D打印领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到3D打印制造流程与传统编译器工具链有相似之处，借鉴了程序语义学和差异测试技术。作者提出了三个关键见解：分析G-code可捕获CAD设计、切片器和设置的综合效果；G-code可通过构造立方体集合来提升；比较两个G-code程序可对工具进行差异测试。作者借鉴了传统编译器的程序分析技术、差异测试方法、表示语义学和操作语义学概念，以及现有的点云比较技术（如Hausdorff距离）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将G-code程序提升为一组立方体表示这些程序执行时将创建的3D对象，然后从立方体生成近似点云以便高效操作，使用增强的Hausdorff距离比较点云以可视化差异。整体流程：输入G-code文件→解析并重构为立方体集合→按比例采样立方体生成点云→将点云分割成单位盒子→计算增强的Hausdorff距离→生成差异热力图和距离分布图→输出结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：提出新的G-code分析方法（重构为立方体和点云）；使用增强Hausdorff距离处理浮点误差；应用空间平均技术减少不想要误差；提出基于旋转不变性的新方法检查模型不变性；实现GlitchFinder原型工具并进行评估。相比之前工作：传统方法不适用于3D打印领域表示；之前工作关注CAD或多边形网格层面，本文关注G-code层面捕获综合效果；本文能静态分析G-code在打印前识别问题；不仅能检查模型不变性，还能对切片器和网格修复工具进行差异测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过将G-code提升为立方体和点云表示并使用增强Hausdorff距离进行比较的新方法，实现了3D打印模型的不变性检查和制造工具的差异测试，从而提高了3D打印流程的可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The computational fabrication pipeline for 3D printing is much like acompiler - users design models in Computer Aided Design (CAD) tools that arelowered to polygon meshes to be ultimately compiled to machine code by 3Dslicers. For traditional compilers and programming languages, techniques forchecking program invariants are well-established. Similarly, methods likedifferential testing are often used to uncover bugs in compilers themselves,which makes them more reliable. The fabrication pipeline would benefit fromsimilar techniques but traditional approaches do not directly apply to therepresentations used in this domain. Unlike traditional programs, 3D modelsexist both as geometric objects as well as machine code that ultimately runs onthe hardware. The machine code, like in traditional compiling, is affected bymany factors like the model, the slicer being used, and numeroususer-configurable parameters that control the slicing process. In this work, wepropose a new algorithm for lifting G-code (a common language used infabrication pipelines) by denoting a G-code program to a set of cuboids, andthen defining an approximate point cloud representation for efficientlyoperating on these cuboids. Our algorithm opens up new opportunities: we showthree use cases that demonstrate how it enables error localization in CADmodels through invariant checking, quantitative comparisons between slicers,and evaluating the efficacy of mesh repair tools. We present a prototypeimplementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58real-world CAD models. Our results show that GlitchFinder is particularlyeffective in identifying slicing issues due to small features, can highlightdifferences in how popular slicers (Cura and PrusaSlicer) slice the same model,and can identify cases where mesh repair tools (MeshLab and Meshmixer)introduce new errors during repair.</description>
      <author>example@mail.com (Yumeng He, Chandrakana Nandi, Sreepathi Pai)</author>
      <guid isPermaLink="false">2509.00699v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.00379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种跨模态知识蒸馏方法，利用丰富的2D图像数据减轻3D LiDAR点云标注负担，在自动驾驶场景的语义分割任务中取得了超越现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR数据语义分割在自动驾驶中起关键作用，但传统方法依赖大量标注数据，成本高且耗时；而真实世界图像数据集丰富且规模大。&lt;h4&gt;目的&lt;/h4&gt;减轻3D LiDAR点云标注的负担，利用丰富的图像数据辅助3D点云语义分割，避免3D标注的必要性。&lt;h4&gt;方法&lt;/h4&gt;提出两种跨模态知识蒸馏方法：无监督域适应知识蒸馏(UDAKD)和基于特征和语义的知识蒸馏(FSKD)；利用自动驾驶场景中现成的时空同步的相机和LiDAR数据；将预训练的2D图像模型应用于未标记的2D数据；通过已知2D-3D对应关系对齐3D和2D网络输出；在3D点云上使用自校准卷积作为域适应模块基础。&lt;h4&gt;主要发现&lt;/h4&gt;严格实验验证了所提出方法的有效性，在性能上持续超越该领域的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过跨模态知识蒸馏，可以避免3D标注的需要，在3D LiDAR数据语义分割任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;3D LiDAR数据的语义分割在自动驾驶中起着关键作用。传统方法依赖大量标注数据进行点云分析，成本高且耗时。相比之下，真实世界图像数据集丰富且规模大。为减轻3D LiDAR点云标注的负担，我们提出两种跨模态知识蒸馏方法：无监督域适应知识蒸馏(UDAKD)和基于特征和语义的知识蒸馏(FSKD)。利用自动驾驶场景中现成的时空同步的相机和LiDAR数据，我们将预训练的2D图像模型直接应用于未标记的2D数据。通过具有已知2D-3D对应关系的跨模态知识蒸馏，我们主动对齐3D网络输出与2D网络对应点，从而避免了3D标注的必要性。我们的重点是保留模态通用信息，同时过滤模态特定细节。为此，我们在3D点云上部署自校准卷积作为域适应模块的基础。严格实验验证了我们所提出方法的有效性，在性能上持续超越该领域的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D LiDAR点云语义分割中标注数据成本高的问题。传统方法需要大量标注的3D点云数据进行训练，既耗时又昂贵。这个问题在自动驾驶领域尤为重要，因为3D点云语义分割对自动驾驶环境理解至关重要，而手动标注3D点云需要大量专业知识和时间成本，远高于2D图像标注。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析3D点云标注困难和2D图像数据丰富的矛盾，考虑如何利用2D图像数据辅助3D点云学习。他们借鉴了知识蒸馏技术、域适应方法(特别是自校准卷积)和对比学习(如PPKT和SLidR)等现有工作。设计思路是利用自动驾驶场景中已有的同步相机和LiDAR数据，通过已知的2D-3D对应关系进行跨模态知识蒸馏，同时保留模态通用信息，过滤模态特定细节。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D图像和3D点云之间的对应关系，将2D图像模型中的知识迁移到3D点云模型中，减少对3D标注数据的依赖。整体流程包括：1)收集同步的2D图像和3D点云数据；2)构建2D特征提取器、3D特征提取器、域适应模块和分类器；3)根据数据是否有标签选择UDAKD或FSKD方法进行知识蒸馏；4)在少量标注的3D数据上微调并评估性能。UDAKD使用对比学习对齐特征，FSKD结合特征和语义两个层面的知识蒸馏。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于3D自校准卷积的域适应模块，能转移模态通用特征并消除模态特定细节；2)针对无标签和标签图像分别设计跨模态知识蒸馏策略；3)实现零样本域适应能力。相比之前工作，本文更关注优化3D学生模型而非仅改进2D教师模型；首次将自校准卷积应用于3D点云；在域适应中明确分离模态特定和通用特征；同时考虑特征和语义两个层面的知识蒸馏，在多个数据集上超越了最先进方法的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于域适应的跨模态知识蒸馏方法，通过利用丰富的2D图像数据有效减少3D点云语义分割对昂贵标注数据的依赖，显著提升了自动驾驶场景下的3D感知性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomousdriving. Traditional approaches rely on extensive annotated data for pointcloud analysis, incurring high costs and time investments. In contrast,realworld image datasets offer abundant availability and substantial scale. Tomitigate the burden of annotating 3D LiDAR point clouds, we propose twocrossmodal knowledge distillation methods: Unsupervised Domain AdaptationKnowledge Distillation (UDAKD) and Feature and Semantic-based KnowledgeDistillation (FSKD). Leveraging readily available spatio-temporallysynchronized data from cameras and LiDARs in autonomous driving scenarios, wedirectly apply a pretrained 2D image model to unlabeled 2D data. Throughcrossmodal knowledge distillation with known 2D-3D correspondence, we activelyalign the output of the 3D network with the corresponding points of the 2Dnetwork, thereby obviating the necessity for 3D annotations. Our focus is onpreserving modality-general information while filtering out modality-specificdetails during crossmodal distillation. To achieve this, we deployself-calibrated convolution on 3D point clouds as the foundation of our domainadaptation module. Rigorous experimentation validates the effectiveness of ourproposed methods, consistently surpassing the performance of state-of-the-artapproaches in the field.</description>
      <author>example@mail.com (Jialiang Kang, Jiawen Wang, Dingsheng Luo)</author>
      <guid isPermaLink="false">2509.00379v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis</title>
      <link>http://arxiv.org/abs/2509.00374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应点提示调优（APPT）的方法，通过直接利用点特征来校准任何模态的异构基础模型，用于3D点云分析，无需异构映射即可直接处理点云数据。&lt;h4&gt;背景&lt;/h4&gt;在1D文本和2D视觉分析中，基础模型的参数高效微调策略已显示出显著效果，但由于点云数据稀缺，预训练大型3D模型仍具挑战性。现有通过'高到低'映射将预训练视觉模型应用于3D领域的方法常导致空间几何信息丢失，且缺乏通用框架。&lt;h4&gt;目的&lt;/h4&gt;直接利用点特征来校准任何模态的异构基础模型，使其能够有效进行3D点云分析，同时保持预训练模型的通用性并减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;1) 将原始点云转换为点嵌入，通过聚合局部几何特征捕捉空间特征；2) 针对点云的无序特性，采用排列不变特征捕捉点嵌入相对位置；3) 引入与点嵌入模块共享权重的提示生成器，动态生成点提示；4) 将生成的点提示连接到冻结的基础模型中，提供丰富的全局结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过APPT方法，预训练模型能够直接处理点云数据而无需异构映射；共享权重的提示生成器可在不增加额外参数的情况下动态生成点提示；连接提示到冻结模型可弥补异构数据中结构上下文的不足。&lt;h4&gt;结论&lt;/h4&gt;APPT方法通过点提示调优有效解决了将任何模态适应到3D点云分析的问题，既保持了预训练模型的知识，又提供了处理点云所需的结构信息，同时计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;针对1D文本和2D视觉分析的基础模型参数高效微调策略已显示出显著效果。然而，由于点云数据的稀缺，预训练大型3D模型仍然是一项具有挑战性的任务。虽然许多努力已经通过'高到低'映射将预训练的视觉模型应用于3D领域，但这些方法往往导致空间几何信息的丢失，并且缺乏将任何模态适应到3D的通用框架。因此，本文尝试直接利用点特征来校准任何模态的异构基础模型，用于3D点云分析。具体来说，我们提出了自适应点提示调优（APPT）方法，该方法使用少量参数微调预训练模型，使模型能够直接处理点云，无需异构映射。我们将原始点云通过聚合局部几何特征转换为点嵌入以捕捉空间特征，然后通过线性层确保能够无缝利用冻结的预训练模型。鉴于点云的固有无序性与图像和语言的有序性质不同，我们采用排列不变的特征来捕捉点嵌入的相对位置，从而获得富含位置信息的点标记，以优化自注意力机制。为了校准任何模态的源域到3D的自注意力并减少计算开销，我们引入了一个与点嵌入模块共享权重的提示生成器，动态生成点提示而不添加额外参数。然后将这些提示连接到冻结的基础模型中，提供丰富的全局结构信息，弥补异构数据中结构上下文的不足。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效利用预训练在其他模态（如图像、文本）上的基础模型来增强3D点云分析任务的问题。这个问题很重要，因为3D点云数据获取和标注成本高、数量有限，导致专门为3D任务训练的大规模基础模型较少。现有方法通常需要将3D点云投影到低维表示（如2D图像），这会导致高维空间信息的丢失，或者需要大量成对的2D/1D-3D数据进行知识蒸馏，限制了效率和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：模态投影会丢失信息，知识蒸馏需要大量成对数据且效率低。基于这些分析，作者设计了APPT方法，直接利用点特征校准基础模型。该方法借鉴了参数高效微调（PEFT）和提示调优（prompt tuning）技术，但针对3D点云特性进行了创新：设计点嵌入模块处理无序数据，引入位置注入器捕获相对位置信息，提出点提示生成器与点嵌入模块共享权重，以及设计有效的Transformer块微调策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轻量级点嵌入和点提示生成模块，直接将3D点云信息注入到预训练在其他模态的基础模型中，使其能处理3D点云而不需修改基础模型。整体流程：1)点云分组（使用FPS和k-NN）；2)点嵌入（通过轻量网络如Point-PN转换点组）；3)位置注入（使用排列不变性特征编码位置）；4)点提示生成（通过池化操作生成全局提示）；5)模型微调（将点提示连接到冻结模型的每个Transformer块）；6)下游任务（分类或分割）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)直接利用点特征校准异构基础模型，避免信息损失；2)提出位置注入器，用排列不变性特征编码位置；3)设计点提示生成器，与点嵌入模块共享权重；4)设计有效的Transformer块微调策略；5)方法通用性强，可适应各种预训练模型。相比之前工作：避免了低维投影步骤，不需要大量成对数据，仅微调3.8%参数，计算效率高，保留了点云的高维结构信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应点提示调优方法，通过轻量级参数高效地利用预训练在其他模态的基础模型来增强3D点云分析任务，避免了信息损失并显著提升了性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning strategies for foundation models in 1Dtextual and 2D visual analysis have demonstrated remarkable efficacy. However,due to the scarcity of point cloud data, pre-training large 3D models remains achallenging task. While many efforts have been made to apply pre-trained visualmodels to 3D domains through "high-to-low" mapping, these approaches often leadto the loss of spatial geometries and lack a generalizable framework foradapting any modality to 3D. This paper, therefore, attempts to directlyleverage point features to calibrate the heterogeneous foundation model of anymodality for 3D point cloud analysis. Specifically, we propose the AdaptivePoint-Prompt Tuning (APPT) method, which fine-tunes pre-trained models with amodest number of parameters, enabling direct point cloud processing withoutheterogeneous mappings. We convert raw point clouds into point embeddings byaggregating local geometry to capture spatial features followed by linearlayers to ensure seamless utilization of frozen pre-trained models. Given theinherent disorder of point clouds, in contrast to the structured nature ofimages and language, we employ a permutation-invariant feature to capture therelative positions of point embeddings, thereby obtaining point tokens enrichedwith location information to optimize self-attention mechanisms. To calibrateself-attention across source domains of any modality to 3D and reducecomputational overhead, we introduce a prompt generator that shares weightswith the point embedding module, dynamically producing point-prompts withoutadding additional parameters. These prompts are then concatenated into a frozenfoundation model, providing rich global structural information and compensatingfor the lack of structural context in the heterogeneous data.</description>
      <author>example@mail.com (Mengke Li, Lihao Chen, Peng Zhang, Yiu-ming Cheung, Hui Huang)</author>
      <guid isPermaLink="false">2509.00374v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception</title>
      <link>http://arxiv.org/abs/2509.02904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高保真数字孪生框架，用于解决激光雷达感知在仿真到现实迁移中的性能下降问题，实验证明该方法比真实数据训练的模型性能提高4.8%。&lt;h4&gt;背景&lt;/h4&gt;Sim2Real域迁移是智能交通系统中基于激光雷达感知的经济有效方法，但仿真训练的模型在真实数据上表现不佳，存在分布偏移问题。&lt;h4&gt;目的&lt;/h4&gt;解决Sim2Real差距问题，提高仿真训练的激光雷达感知模型在真实世界数据上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出高保真数字孪生框架，整合真实世界背景几何、车道级道路拓扑和传感器规范；构建仿真环境生成域内合成数据；在合成数据上训练3D目标检测器并在真实数据上评估。&lt;h4&gt;主要发现&lt;/h4&gt;DT训练的模型比真实数据训练的模型性能高4.8%；多种距离指标显示HiFi DT显著减少了域偏移；提高了模型在不同评估场景中的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;数字孪生在为智能交通应用提供可靠的、基于仿真的激光雷达感知方面具有重要作用。&lt;h4&gt;翻译&lt;/h4&gt;Sim2Real域迁移为智能交通系统中的激光雷达感知（如目标检测、跟踪、分割）提供了一种经济有效且可扩展的方法。然而，由于分布偏移，在仿真中训练的感知模型在真实世界数据上表现往往不佳。为解决这一Sim2Real差距，本文提出了一种高保真数字孪生框架，该框架整合了真实世界的背景几何、车道级道路拓扑以及传感器特定规范和放置。我们将Sim2Real学习中的域适应问题形式化，并提出了一种构建仿真环境的系统方法，以生成域内合成数据。在HiFi DT生成的合成数据上训练了一个现成的3D目标检测器，并在真实数据上进行了评估。实验表明，DT训练的模型比等效的真实数据训练模型性能高出4.8%。为理解这一提升，我们使用多种指标（包括Chamfer距离、最大均值差异、地球移动距离和Fréchet距离）在原始输入和潜在特征水平上量化了合成数据与真实数据之间的分布对齐。结果表明，HiFi DT显著减少了域偏移，并提高了在不同评估场景中的泛化能力。这些发现强调了数字孪生在为真实世界智能交通应用提供可靠的、基于仿真的激光雷达感知方面的重要作用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是Sim2Real（仿真到现实）差距问题，即在仿真环境中训练的激光雷达感知模型在真实世界数据上表现不佳的问题。这个问题在现实中非常重要，因为创建和标注大规模激光雷达数据集成本高昂、耗时费力，人工标注可能存在偏差，不同地区的交通和环境条件差异大，极端场景难以收集，且传感器特性变化会导致数据分布变化，这些都限制了智能交通系统感知算法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有模拟器的局限性，指出它们虽然能模拟传感器物理特性，但环境是手工制作的，不够真实。然后回顾了相关工作，包括传感器模拟研究、领域适应研究和数字孪生研究，发现它们各有不足：传感器模拟研究忽略环境真实性，领域适应研究是事后纠正方法，数字孪生研究创建过程不够系统化。基于这些分析，作者设计了一种系统化的高保真数字孪生(HiFi DT)方法，借鉴了CARLA等模拟器的基础架构，利用了OpenStreetMap等公开地图数据，并参考了现有的数字孪生建模思想，但使其更加系统化和可扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过创建高保真数字孪生(HiFi DT)来生成与真实世界分布高度一致的合成激光雷达数据，从而最小化Sim2Real差距，使在仿真环境中训练的模型能够很好地泛化到真实世界。整体实现流程包括：1)识别目标位置；2)获取3D卫星图像；3)处理3D模型(裁剪、清理、缩放)；4)创建精确道路拓扑；5)生成匹配真实统计分布的交通；6)执行传感器模拟和数据采集，存储为OpenPCDet格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个结合静态和动态场景的高保真数字孪生应用；2)系统化的数字孪生创建方法；3)精确的传感器模拟技术；4)首次使用合成数据训练的模型性能超越真实数据训练的模型；5)深入的数据分布分析。相比之前工作的不同：与传统模拟器相比，HiFi DT使用真实世界的背景几何和道路拓扑；与领域适应方法相比，HiFi DT从源头上生成与目标领域对齐的数据，减少事后处理需求；与现有数字孪生研究相比，HiFi DT提供了一种更简单、系统化的方法，可使用公开信息为任意位置创建数字孪生。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种系统化的高保真数字孪生方法，通过整合真实世界的几何结构、道路拓扑和传感器规格，生成与真实世界分布高度一致的激光雷达数据，使训练的3D目标检测模型在真实世界上的性能超越了在真实数据上训练的模型，有效解决了智能交通系统中的Sim2Real差距问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sim2Real domain transfer offers a cost-effective and scalable approach fordeveloping LiDAR-based perception (e.g., object detection, tracking,segmentation) in Intelligent Transportation Systems (ITS). However, perceptionmodels trained in simulation often under perform on real-world data due todistributional shifts. To address this Sim2Real gap, this paper proposes ahigh-fidelity digital twin (HiFi DT) framework that incorporates real-worldbackground geometry, lane-level road topology, and sensor-specificspecifications and placement. We formalize the domain adaptation challengeunderlying Sim2Real learning and present a systematic method for constructingsimulation environments that yield in-domain synthetic data. An off-the-shelf3D object detector is trained on HiFi DT-generated synthetic data and evaluatedon real data. Our experiments show that the DT-trained model outperforms theequivalent model trained on real data by 4.8%. To understand this gain, wequantify distributional alignment between synthetic and real data usingmultiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at bothraw-input and latent-feature levels. Results demonstrate that HiFi DTssubstantially reduce domain shift and improve generalization across diverseevaluation scenarios. These findings underscore the significant role of digitaltwins in enabling reliable, simulation-based LiDAR perception for real-worldITS applications.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.02904v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
      <link>http://arxiv.org/abs/2509.03505v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  56 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了LimiX，这是大型结构化数据模型(LDMs)系列中的第一个模型，能够通过单一模型处理多种表格数据任务，并在多个基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;实现通用人工智能需要建立在语言、物理世界和结构化数据上的互补基础模型，而当前结构化数据处理领域缺乏能够统一处理多种任务的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理广泛表格任务的结构化数据模型，通过单一模型和统一接口解决多种任务，避免任务特定的架构或为每个任务进行专门训练。&lt;h4&gt;方法&lt;/h4&gt;LimiX将结构化数据视为变量和缺失性的联合分布，使用基于查询的条件预测方法。采用掩码联合分布建模进行预训练，使用情境条件目标，在推理时支持快速、无需训练的适应。&lt;h4&gt;主要发现&lt;/h4&gt;LimiX在10个大型结构化数据基准测试中表现优异，涵盖不同样本量、特征维度、类别数量等条件下，超越了梯度提升树、深度表格网络等强基线方法，在分类、回归、缺失值填补和数据生成等多种任务中表现优越。&lt;h4&gt;结论&lt;/h4&gt;LimiX证明了单一模型可以有效地处理多种表格任务，无需任务特定的架构或为每个任务进行专门训练，为通用人工智能的发展提供了重要进展。&lt;h4&gt;翻译&lt;/h4&gt;我们认为，向通用人工智能的进步需要建立在语言、物理世界和结构化数据上的互补基础模型。本报告介绍了LimiX，这是我们大型结构化数据模型(LDMs)系列中的第一个模型。LimiX将结构化数据视为变量和缺失性的联合分布，因此能够通过基于查询的条件预测，通过单一模型处理广泛的表格任务。LimiX使用掩码联合分布建模进行预训练，采用情境条件目标，模型根据特定数据集的条件预测查询子集，支持推理时快速、无需训练的适应。我们在10个大型结构化数据基准测试中评估了LimiX，这些测试涵盖了样本量、特征维度、类别数量、分类-数值特征比率、缺失性和样本-特征比率的广泛范围。使用单一模型和统一接口，LimiX始终优于强基线方法，包括梯度提升树、深度表格网络、最近的表格基础模型和自动化集成，如图1和图2所示。这种优越性在广泛的任务中保持不变，如分类、回归、缺失值填补和数据生成，且通常有显著优势，同时避免了任务特定的架构或为每个任务进行专门训练。所有LimiX模型均可在Apache 2.0许可下公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We argue that progress toward general intelligence requires complementaryfoundation models grounded in language, the physical world, and structureddata. This report presents LimiX, the first installment of our largestructured-data models (LDMs). LimiX treats structured data as a jointdistribution over variables and missingness, thus capable of addressing a widerange of tabular tasks through query-based conditional prediction via a singlemodel. LimiX is pretrained using masked joint-distribution modeling with anepisodic, context-conditional objective, where the model predicts for querysubsets conditioned on dataset-specific contexts, supporting rapid,training-free adaptation at inference. We evaluate LimiX across 10 largestructured-data benchmarks with broad regimes of sample size, featuredimensionality, class number, categorical-to-numerical feature ratio,missingness, and sample-to-feature ratios. With a single model and a unifiedinterface, LimiX consistently surpasses strong baselines includinggradient-boosting trees, deep tabular networks, recent tabular foundationmodels, and automated ensembles, as shown in Figure 1 and Figure 2. Thesuperiority holds across a wide range of tasks, such as classification,regression, missing value imputation, and data generation, often by substantialmargins, while avoiding task-specific architectures or bespoke training pertask. All LimiX models are publicly accessible under Apache 2.0.</description>
      <author>example@mail.com (Xingxuan Zhang, Gang Ren, Han Yu, Hao Yuan, Hui Wang, Jiansheng Li, Jiayun Wu, Lang Mo, Li Mao, Mingchao Hao, Ningbo Dai, Renzhe Xu, Shuyang Li, Tianyang Zhang, Yue He, Yuanrui Wang, Yunjia Zhang, Zijing Xu, Dongzhe Li, Fang Gao, Hao Zou, Jiandong Liu, Jiashuo Liu, Jiawei Xu, Kaijie Cheng, Kehan Li, Linjun Zhou, Qing Li, Shaohua Fan, Xiaoyu Lin, Xinyan Han, Xuanyue Li, Yan Lu, Yuan Xue, Yuanyuan Jiang, Zimu Wang, Zhenlei Wang, Peng Cui)</author>
      <guid isPermaLink="false">2509.03505v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title>
      <link>http://arxiv.org/abs/2509.03487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SafeProtein，这是首个针对蛋白质基础模型的red-teaming框架，通过系统化方法测试蛋白质模型的安全性，揭示其潜在生物安全风险。&lt;h4&gt;背景&lt;/h4&gt;蛋白质在几乎所有生物过程中都扮演关键角色，深度学习的发展加速了蛋白质基础模型的进步，但这些模型缺乏系统性的red-teaming，存在被滥用于生成生物安全风险蛋白质的担忧。&lt;h4&gt;目的&lt;/h4&gt;开发首个专门针对蛋白质基础模型的red-teaming框架，系统测试模型安全性，揭示潜在风险，并为前沿模型的安全保护技术发展提供见解。&lt;h4&gt;方法&lt;/h4&gt;SafeProtein结合多模态提示工程和启发式束搜索方法设计red-teaming策略，并创建了SafeProtein-Bench基准，包含手动构建的测试数据集和全面评估协议。&lt;h4&gt;主要发现&lt;/h4&gt;SafeProtein成功实现了对最先进蛋白质基础模型的持续越狱，对ESM3模型的攻击成功率高达70%，揭示了当前蛋白质基础模型中存在的潜在生物安全风险。&lt;h4&gt;结论&lt;/h4&gt;SafeProtein框架有效暴露了当前蛋白质基础模型的安全隐患，为开发前沿模型的安全保护技术提供了有价值的见解，相关代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质在几乎所有生物过程中都扮演着关键角色。深度学习的进步极大地加速了蛋白质基础模型的发展，在蛋白质理解和设计方面取得了显著成功。然而，这些模型缺乏系统性的red-teaming，引发了人们对它们可能被滥用的严重担忧，例如生成具有生物安全风险的蛋白质。本文介绍了SafeProtein，据我们所知，这是第一个专为蛋白质基础模型设计的red-teaming框架。SafeProtein结合多模态提示工程和启发式束搜索，系统性地设计red-teaming方法，并对蛋白质基础模型进行测试。我们还整理了SafeProtein-Bench，其中包括一个手动构建的red-teaming基准数据集和全面的评估协议。SafeProtein成功地实现了对最先进的蛋白质基础模型的持续越狱（ESM3的攻击成功率高达70%），揭示了当前蛋白质基础模型中潜在的生物安全风险，并为前沿模型开发强大的安全保护技术提供了见解。相关代码将在https://github.com/jigang-fan/SafeProtein公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Proteins play crucial roles in almost all biological processes. Theadvancement of deep learning has greatly accelerated the development of proteinfoundation models, leading to significant successes in protein understandingand design. However, the lack of systematic red-teaming for these models hasraised serious concerns about their potential misuse, such as generatingproteins with biological safety risks. This paper introduces SafeProtein, thefirst red-teaming framework designed for protein foundation models to the bestof our knowledge. SafeProtein combines multimodal prompt engineering andheuristic beam search to systematically design red-teaming methods and conducttests on protein foundation models. We also curated SafeProtein-Bench, whichincludes a manually constructed red-teaming benchmark dataset and acomprehensive evaluation protocol. SafeProtein achieved continuous jailbreakson state-of-the-art protein foundation models (up to 70% attack success ratefor ESM3), revealing potential biological safety risks in current proteinfoundation models and providing insights for the development of robust securityprotection technologies for frontier models. The codes will be made publiclyavailable at https://github.com/jigang-fan/SafeProtein.</description>
      <author>example@mail.com (Jigang Fan, Zhenghong Zhou, Ruofan Jin, Le Cong, Mengdi Wang, Zaixi Zhang)</author>
      <guid isPermaLink="false">2509.03487v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics</title>
      <link>http://arxiv.org/abs/2509.03421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 8 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了专业视网膜基础模型与通用基础模型在视网膜图像应用中的性能，发现专业模型在眼部疾病检测和眼科医学任务中表现更优，但通用模型差距正在缩小。&lt;h4&gt;背景&lt;/h4&gt;医学基础模型通过大规模临床数据预训练在临床应用中表现出色，如RETFound模型在视网膜图像应用中。然而，更强大的通用基础模型（如DINOv2和DINOv3）的出现引发了领域特定预训练必要性的问题。&lt;h4&gt;目的&lt;/h4&gt;研究通用基础模型在视网膜图像应用中的适应性，并与专业RETFound模型进行比较，评估领域特定预训练是否仍然必要以及存在的差距。&lt;h4&gt;方法&lt;/h4&gt;系统评估了DINOv2和DINOv3在眼部疾病检测和全身疾病预测任务中的表现，并与RETFound-MAE和RETFound-DINOv2模型比较。采用微调和线性探测两种适应策略，并分析数据效率和适应效率以评估预测性能与计算成本的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;尽管扩展通用模型在多样化任务中表现出强大的适应性，但RETFound-DINOv2在眼部疾病检测和眼科医学任务中始终优于通用基础模型，具有更强的泛化能力和数据效率。&lt;h4&gt;结论&lt;/h4&gt;专业视网膜基础模型仍是临床应用的最佳选择，但与通用基础模型差距的缩小表明，持续的数据和模型扩展可带来领域相关收益，并使通用模型成为未来医学基础模型的强大基础。&lt;h4&gt;翻译&lt;/h4&gt;医学基础模型通过大规模临床数据预训练，在多样化的临床相关应用中表现出强大性能。在近一百万张视网膜图像上训练的RETFound代表了视网膜图像应用的方法。然而，越来越强大和规模更大的通用基础模型（如DINOv2和DINOv3）的出现，引发了领域特定预训练是否仍然必要的问题。为研究此问题，我们系统评估了DINOv2和DINOv3在视网膜图像应用中的适应性，并与两个专业RETFound模型（RETFound-MAE和RETFound-DINOv2）进行了比较。我们使用微调和线性探测两种适应策略评估了眼部疾病检测和全身疾病预测的性能。进一步分析了数据效率和适应效率以描述预测性能与计算成本之间的权衡。我们的结果表明，尽管扩展通用模型可在多样化任务中产生强大的适应性，但RETFound-DINOv2在眼部疾病检测和眼科医学任务中始终优于这些通用基础模型，表现出更强的泛化能力和数据效率。这些发现表明，专业视网膜基础模型仍然是临床应用的最有效选择，但与通用基础模型之间差距的缩小表明，持续的数据和模型扩展可以带来领域相关的收益，并使它们成为未来医学基础模型的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical foundation models, pre-trained with large-scale clinical data,demonstrate strong performance in diverse clinically relevant applications.RETFound, trained on nearly one million retinal images, exemplifies thisapproach in applications with retinal images. However, the emergence ofincreasingly powerful and multifold larger generalist foundation models such asDINOv2 and DINOv3 raises the question of whether domain-specific pre-trainingremains essential, and if so, what gap persists. To investigate this, wesystematically evaluated the adaptability of DINOv2 and DINOv3 in retinal imageapplications, compared to two specialist RETFound models, RETFound-MAE andRETFound-DINOv2. We assessed performance on ocular disease detection andsystemic disease prediction using two adaptation strategies: fine-tuning andlinear probing. Data efficiency and adaptation efficiency were further analysedto characterise trade-offs between predictive performance and computationalcost. Our results show that although scaling generalist models yields strongadaptability across diverse tasks, RETFound-DINOv2 consistently outperformsthese generalist foundation models in ocular-disease detection and oculomicstasks, demonstrating stronger generalisability and data efficiency. Thesefindings suggest that specialist retinal foundation models remain the mosteffective choice for clinical applications, while the narrowing gap withgeneralist foundation models suggests that continued data and model scaling candeliver domain-relevant gains and position them as strong foundations forfuture medical foundation models.</description>
      <author>example@mail.com (Yukun Zhou, Paul Nderitu, Jocelyn Hui Lin Goh, Justin Engelmann, Siegfried K. Wagner, Anran Ran, Hongyang Jiang, Lie Ju, Ke Zou, Sahana Srinivasan, Hyunmin Kim, Takahiro Ninomiya, Zheyuan Wang, Gabriel Dawei Yang, Eden Ruffell, Dominic Williamson, Rui Santos, Gabor Mark Somfai, Carol Y. Cheung, Tien Yin Wong, Daniel C. Alexander, Yih Chung Tham, Pearse A. Keane)</author>
      <guid isPermaLink="false">2509.03421v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Assessment and Benchmark Study of Large Atomistic Foundation Models for Phonons</title>
      <link>http://arxiv.org/abs/2509.03401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了六种最新的通用机器学习势(uMLPs)在预测晶体材料声子性质方面的性能，发现EquiformerV2模型在大多数指标上表现最佳，并揭示了力预测精度与声子性质预测之间的复杂关系，为材料高通量筛选提供了重要指导。&lt;h4&gt;背景&lt;/h4&gt;通用机器学习势(uMLPs)技术发展迅速，能够高效准确地预测各种材料性质，但在化学多样化系统中的声子性质预测能力尚未得到充分评估，缺乏系统性的基准测试。&lt;h4&gt;目的&lt;/h4&gt;系统评估六种最新uMLPs模型在预测晶体材料声子性质方面的性能，包括原子力、原子间力常数和晶格热导率等，为材料高通量筛选提供模型选择指导。&lt;h4&gt;方法&lt;/h4&gt;从开放量子材料数据库选取2,429种晶体材料，使用六种uMLPs模型计算位移超晶胞中的原子力，推导原子间力常数，预测晶格热导率等声子性质，并与密度泛函理论和实验数据进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;EquiformerV2预训练模型在原子力和三阶原子间力常数预测方面表现出色；其微调版本在二阶原子间力常数、晶格热导率等声子性质预测方面持续优于其他模型；MACE和CHGNet力预测精度与EquiformerV2相当，但在原子间力常数拟合方面存在差异，导致晶格热导率预测不佳；MatterSim尽管力精度较低，但获得了中等水平的原子间力常数预测，表明力精度与声子预测之间存在误差抵消和复杂关系。&lt;h4&gt;结论&lt;/h4&gt;不同uMLPs模型在预测声子性质方面表现各异，EquiformerV2整体性能最优，研究结果为具有目标热传输性质的材料高通量筛选提供了重要的模型评估和选择依据。&lt;h4&gt;翻译&lt;/h4&gt;通用机器学习势(uMLPs)的快速发展使得在广阔的化学空间中能够高效、准确地预测各种材料性质。虽然它们在建模声子性质方面的能力正在显现，但在化学多样化系统中的系统基准测试仍然有限。我们从开放量子材料数据库中评估了六种最新的uMLPs（EquiformerV2、MatterSim、MACE和CHGNet）在2,429种晶体材料上的表现。这些模型用于计算位移超晶胞中的原子力，推导原子间力常数(IFCs)，并预测晶格热导率(LTC)等声子性质，并与密度泛函理论(DFT)和实验数据进行比较。在OMat24数据集上预训练的EquiformerV2模型在预测原子力和三阶IFC方面表现出色，而其微调版本在预测二阶IFC、LTC和其他声子性质方面持续优于其他模型。尽管MACE和CHGNet的力预测精度与EquiformerV2相当，但在IFC拟合方面存在显著差异，导致LTC预测不佳。相反，MatterSim尽管力精度较低，但获得了中等水平的IFC预测，这表明力精度与声子预测之间存在误差抵消和复杂关系。这一基准测试为指导具有目标热传输性质的材料高通量筛选评估和选择uMLPs提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of universal machine learning potentials (uMLPs) hasenabled efficient, accurate predictions of diverse material properties acrossbroad chemical spaces. While their capability for modeling phonon properties isemerging, systematic benchmarking across chemically diverse systems remainslimited. We evaluate six recent uMLPs (EquiformerV2, MatterSim, MACE, andCHGNet) on 2,429 crystalline materials from the Open Quantum MaterialsDatabase. Models were used to compute atomic forces in displaced supercells,derive interatomic force constants (IFCs), and predict phonon propertiesincluding lattice thermal conductivity (LTC), compared with density functionaltheory (DFT) and experimental data. The EquiformerV2 pretrained model trainedon the OMat24 dataset exhibits strong performance in predicting atomic forcesand third-order IFC, while its fine-tuned counterpart consistently outperformsother models in predicting second-order IFC, LTC, and other phonon properties.Although MACE and CHGNet demonstrated comparable force prediction accuracy toEquiformerV2, notable discrepancies in IFC fitting led to poor LTC predictions.Conversely, MatterSim, despite lower force accuracy, achieved intermediate IFCpredictions, suggesting error cancellation and complex relationships betweenforce accuracy and phonon predictions. This benchmark guides the evaluation andselection of uMLPs for high-throughput screening of materials with targetedthermal transport properties.</description>
      <author>example@mail.com (Md Zaibul Anam, Ogheneyoma Aghoghovbia, Mohammed Al-Fahdi, Lingyu Kong, Victor Fung, Ming Hu)</author>
      <guid isPermaLink="false">2509.03401v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings</title>
      <link>http://arxiv.org/abs/2509.03292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Automatic Speech Recognition and Understanding  Workshop(ASRU), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个用于自动预测生成式音频多轴感知质量的系统，该系统为AudioMOS挑战赛2025的Track 2开发，能够预测文本转语音、文本转音频和文本转音乐系统生成音频的四个美学评分。&lt;h4&gt;背景&lt;/h4&gt;研究背景是AudioMOS挑战赛2025的Track 2，主要关注生成式音频的质量评估，面临的主要挑战是自然训练数据和合成评估数据之间的领域差异。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动预测生成式音频四个美学评分的系统，并解决自然训练数据和合成评估数据之间的领域差异问题。&lt;h4&gt;方法&lt;/h4&gt;结合BEATs预训练音频表示模型与多分支长短期记忆预测器，使用基于缓冲区采样的三元组损失根据感知相似性构建嵌入空间。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法提高了嵌入的判别性和泛化能力，使得无需合成训练数据就能实现领域鲁棒的音频质量评估。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效处理自然训练数据和合成评估数据之间的领域差异，实现对生成式音频质量的准确评估。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于自动预测生成式音频多轴感知质量的系统，该系统是为AudioMOS挑战赛2025的Track 2开发的。任务是预测文本转语音、文本转音频和文本转音乐系统生成的音频的四个音频美学评分--制作质量、制作复杂度、内容享受度和内容有用度。一个主要挑战是自然训练数据和合成评估数据之间的领域差异。为了解决这个问题，我们将BEATs（一种预训练的基于transformer的音频表示模型）与多分支长短期记忆预测器相结合，并使用基于缓冲区采样的三元组损失根据感知相似性构建嵌入空间。我们的结果表明，这提高了嵌入的判别性和泛化能力，使得无需合成训练数据就能实现领域鲁棒的音频质量评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a system for automatic multi-axis perceptual quality prediction ofgenerative audio, developed for Track 2 of the AudioMOS Challenge 2025. Thetask is to predict four Audio Aesthetic Scores--Production Quality, ProductionComplexity, Content Enjoyment, and Content Usefulness--for audio generated bytext-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. Amain challenge is the domain shift between natural training data and syntheticevaluation data. To address this, we combine BEATs, a pretrainedtransformer-based audio representation model, with a multi-branch longshort-term memory (LSTM) predictor and use a triplet loss with buffer-basedsampling to structure the embedding space by perceptual similarity. Our resultsshow that this improves embedding discriminability and generalization, enablingdomain-robust audio quality assessment without synthetic training data.</description>
      <author>example@mail.com (Dyah A. M. G. Wisnu, Ryandhimas E. Zezario, Stefano Rini, Hsin-Min Wang, Yu Tsao)</author>
      <guid isPermaLink="false">2509.03292v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization</title>
      <link>http://arxiv.org/abs/2509.03244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了FoMEMO新范式，用于解决昂贵多目标优化问题，通过使用合成数据预训练的基础模型实现高样本效率和强泛化能力。&lt;h4&gt;背景&lt;/h4&gt;昂贵的多目标优化在许多现实场景中普遍存在且至关重要，但由于评估次数有限，样本效率对于恢复真实帕累托前沿以支持决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以泛化和应对现实世界中各种新兴应用的问题，提高多目标优化的效率和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出FoMEMO（Foundation Models for Expensive Multi-objective Optimization）新范式，建立基于领域轨迹和用户偏好的基础模型，实现基于预测的偏好级联聚合后验的快速上下文优化，使用数亿个合成数据进行预训练而非现实世界实验。&lt;h4&gt;主要发现&lt;/h4&gt;使用多样化的数亿个合成数据预训练基础模型可带来对未知问题的优越适应性，无需在优化过程中进行任何后续模型训练或更新。&lt;h4&gt;结论&lt;/h4&gt;在多种合成基准和现实世界应用中评估显示，FoMEMO与现有方法相比具有优越的通用性和竞争性能。&lt;h4&gt;翻译&lt;/h4&gt;昂贵的多目标优化是许多现实场景中普遍存在且至关重要的问题，由于评估次数有限，样本效率对于恢复真实帕累托前沿以支持决策至关重要。现有方法要么需要为每个新问题中的每个目标从头重建高斯过程代理模型，要么依赖大量过去的领域实验来预训练深度学习模型，这使它们难以泛化且难以应对现实世界中各种新兴应用。为解决这一问题，我们提出了一种名为FoMEMO（Foundation Models for Expensive Multi-objective Optimization）的新范式，该范式能够建立基于任何领域轨迹和用户偏好的基础模型，促进基于预测的偏好级联聚合后验的快速上下文优化。我们证明，使用多样化的数亿个合成数据进行预训练可以使基础模型对未知问题具有更好的适应性，而无需在优化过程中进行任何后续模型训练或更新。我们在各种合成基准和现实世界应用中评估了我们的方法，并展示了其相比现有方法的优越通用性和竞争性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Expensive multi-objective optimization is a prevalent and crucial concern inmany real-world scenarios, where sample-efficiency is vital due to the limitedevaluations to recover the true Pareto front for decision making. Existingworks either involve rebuilding Gaussian process surrogates from scratch foreach objective in each new problem encountered, or rely on extensive pastdomain experiments for pre-training deep learning models, making them hard togeneralize and impractical to cope with various emerging applications in thereal world. To address this issue, we propose a new paradigm named FoMEMO(Foundation Models for Expensive Multi-objective Optimization), which enablesthe establishment of a foundation model conditioned on any domain trajectoryand user preference, and facilitates fast in-context optimization based on thepredicted preference-wise aggregation posteriors. Rather than accessingextensive domain experiments in the real world, we demonstrate thatpre-training the foundation model with a diverse set of hundreds of millions ofsynthetic data can lead to superior adaptability to unknown problems, withoutnecessitating any subsequent model training or updates in the optimizationprocess. We evaluate our method across a variety of synthetic benchmarks andreal-word applications, and demonstrate its superior generality and competitiveperformance compared to existing methods.</description>
      <author>example@mail.com (Yiming Yao, Fei Liu, Liang Zhao, Xi Lin, Qingfu Zhang)</author>
      <guid isPermaLink="false">2509.03244v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025</title>
      <link>http://arxiv.org/abs/2509.03191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于Transformer的表格数据基础模型TabPFN在岩土场地表征问题中的应用，展示了其在预测和插补任务中优于传统分层贝叶斯模型的性能。&lt;h4&gt;背景&lt;/h4&gt;研究应用于GEOAI基准测试中的BM/AirportSoilProperties/2/2025问题，涉及预测钻孔深度剖面上不排水抗剪强度的空间变化和在密集站点数据集中插补缺失的力学参数。&lt;h4&gt;目的&lt;/h4&gt;评估TabPFN模型在岩土场地表征问题中的性能，特别是在零训练、少样本、上下文学习设置下的表现，并与传统分层贝叶斯模型进行比较。&lt;h4&gt;方法&lt;/h4&gt;在无需超参数调整的情况下应用TabPFN，并利用大型间接数据库(BID)提供额外上下文信息。比较了TabPFN与传统分层贝叶斯模型(HBM)在两个基准问题上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN作为通用基础模型，相比HBM基线具有更高的准确性和良好校准的预测分布，推理效率显著提高。在基准问题1中，TabPFN预测准确性更高且运行速度快一个数量级；在基准问题2中，TabPFN对所有目标参数实现了更低的RMSE和良好量化的不确定性，但累积计算成本较高。&lt;h4&gt;结论&lt;/h4&gt;表格基础模型在岩土建模中的首次成功应用，可能预示着概率场地表征方法的范式转变。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于Transformer的表格数据基础模型TabPFN在岩土场地表征问题中的创新应用，应用于GEOAI基准测试中的BM/AirportSoilProperties/2/2025。研究解决了两个任务：(1)预测钻孔深度剖面上不排水抗剪强度的空间变化，(2)在密集站点数据集中插补缺失的力学参数。研究在零训练、少样本、上下文学习设置中应用TabPFN，无需超参数调整，并提供了来自大型间接数据库(BID)的额外上下文信息。研究表明，作为通用基础模型的TabPFN相比传统分层贝叶斯模型(HBM)基线具有更高的准确性和良好校准的预测分布，同时推理效率显著提高。在基准问题1(空间su预测)中，TabPFN在预测准确性上优于HBM，运行速度快一个数量级。在基准问题2(缺失力学参数插补)中，TabPFN同样对所有目标参数实现了更低的RMSE和良好量化的不确定性，但由于其一次一个变量的推理方式，累积计算成本高于HBM。这些结果标志着表格基础模型在岩土建模中的首次成功应用，暗示了概率场地表征可能发生的范式转变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel application of the Tabular Prior-Data FittedNetwork (TabPFN) - a transformer-based foundation model for tabular data - togeotechnical site characterization problems defined in the GEOAI benchmarkBM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting thespatial variation of undrained shear strength (su) across borehole depthprofiles, and (2) imputing missing mechanical parameters in a dense-sitedataset. We apply TabPFN in a zero-training, few-shot, in-context learningsetting - without hyper-parameter tuning - and provide it with additionalcontext from the big indirect database (BID). The study demonstrates thatTabPFN, as a general-purpose foundation model, achieved superior accuracy andwell-calibrated predictive distributions compared to a conventionalhierarchical Bayesian model (HBM) baseline, while also offering significantgains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),TabPFN outperformed the HBM in prediction accuracy and delivered anorder-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanicalparameter imputation), TabPFN likewise achieved lower RMSE for all targetparameters with well-quantified uncertainties, though its cumulativecomputation cost was higher than HBM's due to its one-variable-at-a-timeinference. These results mark the first successful use of a tabular foundationmodel in geotechnical modeling, suggesting a potential paradigm shift inprobabilistic site characterization.</description>
      <author>example@mail.com (Taiga Saito, Yu Otake, Stephen Wu)</author>
      <guid isPermaLink="false">2509.03191v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>RecBase: Generative Foundation Model Pretraining for Zero-Shot Recommendation</title>
      <link>http://arxiv.org/abs/2509.03131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了RecBase，一个专门为推荐任务设计的领域无关基础模型，通过统一的文本表示和特征映射，以及统一的项目编码器，解决了LLM在跨领域推荐中的泛化问题，并在多个数据集上取得了优于现有LLM基线的性能。&lt;h4&gt;背景&lt;/h4&gt;基于LLM的推荐系统最近显示出潜力，但它们的跨领域泛化能力受到语言中心预训练与推荐任务之间基本不匹配的限制。现有依赖语言级知识的方法无法捕获跨领域的动态、项目级用户兴趣。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一问题，作者提出了RecBase，一个具有推荐导向目标的领域无关基础模型，旨在增强跨领域泛化能力，并更好地对齐跨领域的项目语义。&lt;h4&gt;方法&lt;/h4&gt;RecBase利用大规模、异构、跨领域语料库，采用统一的文本表示和特征映射来增强跨领域泛化。作者引入了一个统一的项目编码器，将项目编码为分层概念标识符，实现结构化表示和高效的词汇共享。模型使用自回归目标进行训练，以捕获复杂的项目级顺序模式。&lt;h4&gt;主要发现&lt;/h4&gt;在八个真实世界数据集上，作者具有15亿参数的RecBase模型在零样本和跨领域推荐任务中，匹配或超越了高达70亿参数的LLM基线的性能。&lt;h4&gt;结论&lt;/h4&gt;RecBase通过专门针对推荐任务设计的预训练方法和跨领域对齐机制，有效解决了LLM在推荐系统中的跨领域泛化问题，证明了专门化的基础模型在推荐领域的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近的基于LLM的推荐研究显示出前景，但它们的跨领域泛化能力受到语言中心预训练与推荐任务之间基本不匹配的限制。现有依赖语言级知识的方法无法捕获跨领域的动态、项目级用户兴趣。为了弥补这一差距，我们提出了RecBase，一个具有推荐导向目标的领域无关基础模型。RecBase利用大规模、异构、跨领域语料库，采用统一的文本表示和特征映射来增强跨领域泛化。为了进一步对齐跨领域的项目语义，我们引入了一个统一的项目编码器，将项目编码为分层概念标识符，实现结构化表示和高效的词汇共享。模型使用自回归目标进行训练，以捕获复杂的项目级顺序模式。在八个真实世界数据集上，我们具有15亿参数的模型在零样本和跨领域推荐任务中，匹配或超越了高达70亿参数的LLM基线的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in LLM-based recommendation have shown promise, yet theircross-domain generalization is hindered by a fundamental mismatch betweenlanguage-centric pretraining and the recommendation task. Existing methods,relying on language-level knowledge, fail to capture dynamic, item-level userinterests across domains. To bridge this gap, we propose RecBase, adomain-agnostic foundational model pretrained with a recommendation-orientedobjective. RecBase leverages a large-scale, heterogeneous, cross-domain corpuswith unified textual representations and feature mappings to enhancecross-domain generalization. To further align item semantics across domains, weintroduce a unified item tokenizer that encodes items into hierarchical conceptidentifiers, enabling structured representation and efficient vocabularysharing. The model is trained using an autoregressive objective to capturecomplex item-level sequential patterns. On eight real-world datasets, our1.5B-parameter model matches or surpasses the performance of LLM baselines upto 7B parameters in zero-shot and cross-domain recommendation tasks.</description>
      <author>example@mail.com (Sashuai Zhou, Weinan Gan, Qijiong Liu, Ke Lei, Jieming Zhu, Hai Huang, Yan Xia, Ruiming Tang, Zhenhua Dong, Zhou Zhao)</author>
      <guid isPermaLink="false">2509.03131v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm</title>
      <link>http://arxiv.org/abs/2509.02846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种针对偏微分方程的测试时计算(TTC)策略，通过在推理过程中利用计算资源实现更准确的预测，同时减少训练样本和模型大小。该方法通过两种基于奖励的模型评估时空一致性，在可压缩欧拉方程模拟中展示了优于标准自回归推理的性能。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程是现代计算科学和工程的基础，但计算成本高昂。现有的PDE基础模型虽然在模拟复杂时空现象方面有前景，但受限于预训练数据集，在自回归展开性能上存在困难，特别是在分布外情况下。此外，这些模型需要大量计算资源和训练数据，限制了它们在许多关键应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;受大型语言模型中'思考'策略的启发，本研究旨在引入第一个用于PDE的测试时计算策略，通过在推理过程中利用计算资源，实现更准确的预测，同时减少训练样本需求和模型大小。&lt;h4&gt;方法&lt;/h4&gt;作者通过两种类型的奖励模型来实现这一目标，这些模型评估基于随机模型的时空一致性预测。研究在PDEGym基准的可压缩欧拉方程模拟上展示了这种方法。&lt;h4&gt;主要发现&lt;/h4&gt;TTC策略相对于标准非自适应自回归推理能够捕捉改进的预测，表明在减少训练数据需求的同时可以提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;TTC框架为更高级的推理算法或PDE建模奠定了基础，包括构建基于强化学习的方法，这可能改变物理和工程中的计算工作流程，为资源受限环境中的PDE模拟提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;偏微分方程是现代计算科学和工程的基础，本质上计算成本很高。虽然PDE基础模型在模拟复杂时空现象方面显示出很大前景，但现有模型仍然受限于预训练数据集，并且在自回归展开性能方面存在困难，特别是在分布外(OOD)情况下。此外，它们需要大量的计算资源和训练数据，这限制了它们在许多关键应用中的使用。受最近大型语言模型中使用的'思考'策略的启发，我们引入了第一个用于PDE的测试时计算(TTC)策略，该策略在推理过程中利用计算资源，以实现更准确的预测，同时减少训练样本和更小的模型。我们通过两种类型的奖励模型来实现这一点，这些模型评估基于随机模型的时空一致性预测。我们在PDEGym基准的可压缩欧拉方程模拟上展示了这种方法，并表明TTC相对于标准非自适应自回归推理能够捕捉改进的预测。这个TTC框架为更高级的推理算法或PDE建模奠定了基础，包括构建基于强化学习的方法，可能改变物理和工程中的计算工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial Differential Equations (PDEs) are the bedrock for moderncomputational sciences and engineering, and inherently computationallyexpensive. While PDE foundation models have shown much promise for simulatingsuch complex spatio-temporal phenomena, existing models remain constrained bythe pretraining datasets and struggle with auto-regressive rollout performance,especially in out-of-distribution (OOD) cases. Furthermore, they havesignificant compute and training data requirements which hamper their use inmany critical applications. Inspired by recent advances in ``thinking"strategies used in large language models (LLMs), we introduce the firsttest-time computing (TTC) strategy for PDEs that utilizes computationalresources during inference to achieve more accurate predictions with fewertraining samples and smaller models. We accomplish this with two types ofreward models that evaluate predictions of a stochastic based model forspatio-temporal consistency. We demonstrate this method on compressibleEuler-equation simulations from the PDEGym benchmark and show that TTC capturesimproved predictions relative to standard non-adaptive auto-regressiveinference. This TTC framework marks a foundational step towards more advancedreasoning algorithms or PDE modeling, inluding buildingreinforcement-learning-based approaches, potentially transforming computationalworkflows in physics and engineering.</description>
      <author>example@mail.com (Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debarledeben, Ayan Biswas, Diane Oyen, Earl Lawrence)</author>
      <guid isPermaLink="false">2509.02846v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR</title>
      <link>http://arxiv.org/abs/2509.02830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ASRU 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文在ESPnet中首次整合和基准测试了参数高效微调(PEFT)方法，并提出了结构化SVD引导(SSVD)微调方法，在领域偏移的语音识别任务上验证了这些方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)是适应大型基础模型的可扩展解决方案，低秩适应(LoRA)在语音应用中被广泛使用，但其最先进的变体(如VeRA、DoRA、PiSSA和SVFT)主要针对语言和视觉任务开发，在语音领域的验证有限。&lt;h4&gt;目的&lt;/h4&gt;首次在ESPnet中整合和基准测试这些PEFT方法，并引入结构化SVD引导的微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出结构化SVD引导(SSVD)微调，选择性地旋转与输入相关的右奇异向量，同时保持与输出相关的向量固定，以保留语义映射，实现稳健的领域适应。&lt;h4&gt;主要发现&lt;/h4&gt;在模型规模从0.1B到2B的领域偏移语音识别任务(包括儿童语音和方言变化)上评估了所有方法。&lt;h4&gt;结论&lt;/h4&gt;所有实现在ESPnet中发布，以支持可重复性和未来工作。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)已成为适应大型基础模型的可扩展解决方案。虽然低秩适应(LoRA)在语音应用中被广泛使用，但其最先进的变体，例如VeRA、DoRA、PiSSA和SVFT，主要针对语言和视觉任务开发，在语音领域的验证有限。这项工作首次在ESPnet中整合和基准测试了这些PEFT方法。我们进一步引入了结构化SVD引导(SSVD)微调，该方法选择性地旋转与输入相关的右奇异向量，同时保持与输出相关的向量固定，以保留语义映射。这种设计能够以最少的可训练参数和改进的效率实现稳健的领域适应。我们在领域偏移的语音识别任务(包括儿童语音和方言变化)上评估了所有方法，模型规模从0.1B到2B。所有实现在ESPnet中发布，以支持可重复性和未来工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution foradapting large foundation models. While low-rank adaptation (LoRA) is widelyused in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,PiSSA, and SVFT, are developed mainly for language and vision tasks, withlimited validation in speech. This work presents the first comprehensiveintegration and benchmarking of these PEFT methods within ESPnet. We furtherintroduce structured SVD-guided (SSVD) fine-tuning, which selectively rotatesinput-associated right singular vectors while keeping output-associated vectorsfixed to preserve semantic mappings. This design enables robust domainadaptation with minimal trainable parameters and improved efficiency. Weevaluate all methods on domain-shifted speech recognition tasks, includingchild speech and dialectal variation, across model scales from 0.1B to 2B. Allimplementations are released in ESPnet to support reproducibility and futurework.</description>
      <author>example@mail.com (Pu Wang, Shinji Watanabe, Hugo Van hamme)</author>
      <guid isPermaLink="false">2509.02830v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface</title>
      <link>http://arxiv.org/abs/2509.02783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Transparent Earth是一个基于Transformer的架构，用于从异构数据集中重建地下特性，支持多种模态的数据输入，并展现出优异的性能和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;地下特性重建面临来自异构数据集的挑战，这些数据集在稀疏性、分辨率和模态方面各不相同，每种模态代表不同类型的观测（如应力角、地幔温度、构造板块类型）。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理多种模态数据的模型，用于重建地球地下特性，并实现可扩展性和高性能预测。&lt;h4&gt;方法&lt;/h4&gt;设计了一个基于Transformer的架构，结合了观测的位置编码和通过文本嵌入模型生成的模态编码，使模型能够处理任意数量的模态数据，支持上下文学习。&lt;h4&gt;主要发现&lt;/h4&gt;模型在验证数据上将预测应力角的误差减少了超过三分之二；架构具有可扩展性，性能随参数增加而提高；支持八种不同类型的模态，包括方向角度、类别类别和连续属性。&lt;h4&gt;结论&lt;/h4&gt;Transparent Earth作为地球地下特性的基础模型，能够处理多种模态数据，提供高精度预测，并具有可扩展性，为未来预测地球上任何地方的任何地下特性奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Transparent Earth，这是一个基于Transformer的架构，用于从异构数据集中重建地下特性，这些数据集在稀疏性、分辨率和模态方面各不相同，每种模态代表一种不同类型的观测（例如应力角、地幔温度、构造板块类型）。该模型结合了观测的位置编码和模态编码，后者是通过将文本嵌入模型应用于每种模态的描述而得到的。这种设计使模型能够扩展到任意数量的模态，便于添加初始设计中未考虑的新模态。目前包括八种模态，涵盖方向角度、类别类别和连续属性（如温度和厚度）。这些功能支持上下文学习，使模型能够在没有输入或从任何模态子集添加任意数量的额外观测的情况下生成预测。在验证数据上，这使预测应力角的误差减少了超过三分之二。所提出的架构是可扩展的，并且随着参数的增加表现出改进的性能。这些进展共同使Transparent Earth成为地球地下特性的基础模型，最终旨在预测地球上任何地方的任何地下特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Transparent Earth, a transformer-based architecture forreconstructing subsurface properties from heterogeneous datasets that vary insparsity, resolution, and modality, where each modality represents a distincttype of observation (e.g., stress angle, mantle temperature, tectonic platetype). The model incorporates positional encodings of observations togetherwith modality encodings, derived from a text embedding model applied to adescription of each modality. This design enables the model to scale to anarbitrary number of modalities, making it straightforward to add new ones notconsidered in the initial design. We currently include eight modalitiesspanning directional angles, categorical classes, and continuous propertiessuch as temperature and thickness. These capabilities support in-contextlearning, enabling the model to generate predictions either with no inputs orwith an arbitrary number of additional observations from any subset ofmodalities. On validation data, this reduces errors in predicting stress angleby more than a factor of three. The proposed architecture is scalable anddemonstrates improved performance with increased parameters. Together, theseadvances make the Transparent Earth an initial foundation model for the Earth'ssubsurface that ultimately aims to predict any subsurface property anywhere onEarth.</description>
      <author>example@mail.com (Arnab Mazumder, Javier E. Santos, Noah Hobbs, Mohamed Mehana, Daniel O'Malley)</author>
      <guid isPermaLink="false">2509.02783v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Mentality: A Mamba-based Approach towards Foundation Models for EEG</title>
      <link>http://arxiv.org/abs/2509.02746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了基于aMamba的选择性状态空间模型等基础模型在增强EEG分析和神经障碍诊断方面的潜力。研究通过自监督重建任务和癫痫检测任务训练模型，并在测试集上达到了0.72的AUROC，为开发大规模、临床适用的EEG数据分析基础模型迈出了重要一步。&lt;h4&gt;背景&lt;/h4&gt;EEG对于诊断癫痫等疾病至关重要，但由于其噪声大、高维度和非线性的特性，面临重大挑战。传统的机器学习方法在自动化EEG分析方面取得了进展，但往往无法捕捉其复杂的时空动态特性。深度学习，特别是序列建模的最新进展，为创建能够处理此类复杂性的更通用和更具表达能力的模型提供了新途径。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于基础模型的方法，特别是aMamba模型，以增强EEG数据分析，提高神经障碍（如癫痫）诊断的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;研究使用基于aMamba的选择性状态空间模型，在一个包含癫痫发作和非癫痫发作EEG记录的大型数据集上进行训练。训练分为两个阶段：首先通过自监督重建任务进行预训练，然后进行癫痫检测任务。最终在保留的测试集上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于aMamba的模型在癫痫检测任务中表现出色，在保留的测试集上达到了0.72的AUROC（受试者工作特征曲线下面积），证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;这种基于aMamba的基础模型方法代表了开发大规模、临床适用的EEG数据分析基础模型的重要一步，有望改善神经障碍的诊断过程。&lt;h4&gt;翻译&lt;/h4&gt;这项工作探讨了基础模型，特别是基于aMamba的选择性状态空间模型，在增强EEG分析和神经障碍诊断方面的潜力。EEG对于诊断癫痫等疾病至关重要，但由于其噪声大、高维度和非线性的特性，面临重大挑战。传统的机器学习方法在自动化EEG分析方面取得了进展，但往往无法捕捉其复杂的时空动态特性。深度学习，特别是序列建模的最新进展，为创建能够处理此类复杂性的更通用和更具表达能力的模型提供了新途径。通过在一个包含癫痫发作和非癫痫发作EEG记录的大型数据集上，通过自监督重建任务后接癫痫检测任务来训练基于aMamba的模型，我们证明了模型的有效性，在保留的测试集上达到了0.72的AUROC。这种方法标志着开发用于EEG数据分析的大规模、临床适用的基础模型迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores the potential of foundation models, specifically aMamba-based selective state space model, for enhancing EEG analysis inneurological disorder diagnosis. EEG, crucial for diagnosing conditions likeepilepsy, presents significant challenges due to its noisy, high-dimensional,and nonlinear nature. Traditional machine learning methods have made advancesin automating EEG analysis but often fail to capture its complexspatio-temporal dynamics. Recent advances in deep learning, particularly insequence modeling, offer new avenues for creating more generalized andexpressive models capable of handling such complexities. By training aMamba-based model on a large dataset containing seizure and non-seizure EEGrecordings through a self-supervised reconstruction task followed by a seizuredetection task, we demonstrate the model's effectiveness, achieving an AUROC of0.72 on a held-out test set. This approach marks a significant step towarddeveloping large-scale, clinically applicable foundation models for EEG dataanalysis.</description>
      <author>example@mail.com (Saarang Panchavati, Corey Arnold, William Speier)</author>
      <guid isPermaLink="false">2509.02746v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Planning with Reasoning using Vision Language World Model</title>
      <link>http://arxiv.org/abs/2509.02722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Vision Language World Model (VLWM)，一种基于自然视频训练的基础模型，用于语言驱动的世界建模。该模型能够从视觉观察中推断目标成就并预测由交替动作和世界状态变化组成的轨迹，实现了在视觉规划任务上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;有效的规划需要强大的世界模型，但能够通过语义和时间抽象来理解和推理动作的高层次世界模型仍然发展不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够理解和推理动作的高层次世界模型，用于视觉规划任务。&lt;h4&gt;方法&lt;/h4&gt;1. 提出Vision Language World Model (VLWM)，一种基于自然视频训练的基础模型；2. 使用迭代LLM自我精炼方法，基于由标题树表示的压缩未来观测提取目标；3. 学习动作策略和动力学模型，分别促进反应式系统1计划解码和通过成本最小化的反思式系统2规划；4. 使用自我监督方式训练的批评家模型评估假设未来状态与预期目标状态之间的语义距离。&lt;h4&gt;主要发现&lt;/h4&gt;1. VLWM在视觉规划辅助(VPA)任务上取得了最先进的性能，包括基准评估和提出的PlannerArena人类评估；2. 系统2相比系统1将Elo评分提高了27%；3. VLWM模型在RoboVQA和世界预测基准上优于强大的VLM基线模型。&lt;h4&gt;结论&lt;/h4&gt;Vision Language World Model (VLWM)是一种有效的世界建模方法，能够通过语义和时间抽象来理解和推理动作，在视觉规划任务上取得了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;有效的规划需要强大的世界模型，但能够通过语义和时间抽象来理解和推理动作的高层次世界模型仍然发展不足。我们介绍了Vision Language World Model (VLWM)，这是一种针对自然视频的语言驱动世界建模而训练的基础模型。给定视觉观察，VLWM首先推断整体目标成就，然后预测由交替动作和世界状态变化组成的轨迹。这些目标通过基于由标题树表示的压缩未来观测的迭代LLM自我精炼来提取。VLWM学习动作策略和动力学模型，分别促进通过成本最小化的反应式系统1计划解码和反思式系统2规划。成本评估VLWM滚动给出的假设未来状态与预期目标状态之间的语义距离，并通过我们以自我监督方式训练的批评家模型进行测量。VLWM在基准评估和我们提出的PlannerArena人类评估中均实现了最先进的视觉规划辅助(VPA)性能，其中系统2比系统1将Elo评分提高了27%。VLWM模型在RoboVQA和世界预测基准上也优于强大的VLM基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective planning requires strong world models, but high-level world modelsthat can understand and reason about actions with semantic and temporalabstraction remain largely underdeveloped. We introduce the Vision LanguageWorld Model (VLWM), a foundation model trained for language-based worldmodeling on natural videos. Given visual observations, the VLWM first infersthe overall goal achievements then predicts a trajectory composed ofinterleaved actions and world state changes. Those targets are extracted byiterative LLM Self-Refine conditioned on compressed future observationsrepresented by Tree of Captions. The VLWM learns both an action policy and adynamics model, which respectively facilitates reactive system-1 plan decodingand reflective system-2 planning via cost minimization. The cost evaluates thesemantic distance between the hypothetical future states given by VLWMroll-outs and the expected goal state, and is measured by a critic model thatwe trained in a self-supervised manner. The VLWM achieves state-of-the-artVisual Planning for Assistance (VPA) performance on both benchmark evaluationsand our proposed PlannerArena human evaluations, where system-2 improves theElo score by +27% upon system-1. The VLWM models also outperforms strong VLMbaselines on RoboVQA and WorldPrediction benchmark.</description>
      <author>example@mail.com (Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, Pascale Fung)</author>
      <guid isPermaLink="false">2509.02722v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women</title>
      <link>http://arxiv.org/abs/2509.02710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于医学切片转换器(MST)和Kolmogorov-Arnold网络(KAN)的乳腺癌病变分类流水线，使用动态对比增强MRI进行检测，在保持可解释性的同时实现了较高的分类性能。&lt;h4&gt;背景&lt;/h4&gt;准确的乳腺癌MRI病变检测对早期癌症诊断至关重要，特别是对高风险人群。&lt;h4&gt;目的&lt;/h4&gt;开发一个分类流水线，使用动态对比增强MRI对乳腺病变进行分类，区分良性和恶性病变。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的基础模型Medical Slice Transformer (MST)，利用DINOv2基于自监督的预训练生成强大的逐切片特征嵌入，然后使用这些嵌入训练Kolmogorov-Arnold Network (KAN)分类器。KAN通过自适应B样条激活函数实现局部非线性变换，作为传统卷积网络的灵活且可解释的替代方案。&lt;h4&gt;主要发现&lt;/h4&gt;MST+KAN流水线优于基线MST分类器，达到AUC = 0.80±0.02，并通过基于注意力的热图保留了可解释性。&lt;h4&gt;结论&lt;/h4&gt;结合基础模型嵌入和先进分类策略对于构建稳健且可推广的乳腺癌MRI分析工具是有效的。&lt;h4&gt;翻译&lt;/h4&gt;准确的乳腺癌MRI病变检测对早期癌症诊断至关重要，特别是在高风险人群中。我们提出了一种分类流水线，该流水线采用预训练的基础模型医学切片转换器(MST)，利用动态对比增强MRI(DCE-MRI)进行乳腺病变分类。利用基于DINOv2的自监督预训练，MST生成强大的逐切片特征嵌入，然后使用这些嵌入训练Kolmogorov-Arnold网络(KAN)分类器。KAN通过自适应B样条激活函数实现局部非线性变换，为传统卷积网络提供了灵活且可替代的方案。这增强了模型在不平衡和异质性临床数据集中区分良性和恶性病变的能力。实验结果表明，MST+KAN流水线优于基线MST分类器，达到AUC = 0.80±0.02，同时通过基于注意力的热图保留了可解释性。我们的研究结果表明，将基础模型嵌入与先进的分类策略相结合对于构建稳健且可推广的乳腺癌MRI分析工具是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate breast MRI lesion detection is critical for early cancer diagnosis,especially in high-risk populations. We present a classification pipeline thatadapts a pretrained foundation model, the Medical Slice Transformer (MST), forbreast lesion classification using dynamic contrast-enhanced MRI (DCE-MRI).Leveraging DINOv2-based self-supervised pretraining, MST generates robustper-slice feature embeddings, which are then used to train a Kolmogorov--ArnoldNetwork (KAN) classifier. The KAN provides a flexible and interpretablealternative to conventional convolutional networks by enabling localizednonlinear transformations via adaptive B-spline activations. This enhances themodel's ability to differentiate benign from malignant lesions in imbalancedand heterogeneous clinical datasets. Experimental results demonstrate that theMST+KAN pipeline outperforms the baseline MST classifier, achieving AUC = 0.80\pm 0.02 while preserving interpretability through attention-based heatmaps.Our findings highlight the effectiveness of combining foundation modelembeddings with advanced classification strategies for building robust andgeneralizable breast MRI analysis tools.</description>
      <author>example@mail.com (Gabriel A. B. do Nascimento, Vincent Dong, Guilherme J. Cavalcante, Alex Nguyen, Thaís G. do Rêgo, Yuri Malheiros, Telmo M. Silva Filho, Carla R. Zeballos Torrez, James C. Gee, Anne Marie McCarthy, Andrew D. A. Maidment, Bruno Barufaldi)</author>
      <guid isPermaLink="false">2509.02710v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>FastVGGT: Training-Free Acceleration of Visual Geometry Transformer</title>
      <link>http://arxiv.org/abs/2509.02560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为FastVGGT的新方法，通过标记合并技术解决了3D视觉基础模型在处理长序列图像输入时的推理效率低下问题。FastVGGT首次在3D领域应用标记合并技术，通过一种无需训练的机制加速VGGT模型，同时保持其强大的重建能力。实验证明，在处理1000张输入图像时，FastVGGT比原始VGGT快4倍，同时减轻了长序列场景中的误差累积问题。&lt;h4&gt;背景&lt;/h4&gt;3D视觉基础模型最近在3D感知方面展示了卓越的能力。然而，将这些模型扩展到长序列图像输入仍然是一个重大挑战，主要原因是推理时的效率低下。&lt;h4&gt;目的&lt;/h4&gt;解决3D视觉基础模型在处理长序列图像输入时的推理效率低下问题，同时保持模型的重建能力。&lt;h4&gt;方法&lt;/h4&gt;1. 分析了VGGT（一种先进的前馈视觉几何模型）并确定其主要瓶颈；2. 通过可视化发现了注意图中存在的标记崩溃现象；3. 探索了在前馈视觉几何模型中使用标记合并的潜力；4. 针对现有合并技术在3D模型中直接应用的挑战，提出了FastVGGT；5. 设计了一种针对3D架构和任务定制的独特标记分区策略。&lt;h4&gt;主要发现&lt;/h4&gt;1. VGGT模型在处理长序列图像输入时存在推理效率低下的主要瓶颈；2. 注意图中存在标记崩溃现象；3. 标记合并技术可以有效地消除冗余计算，同时保持VGGT的强大重建能力；4. 在处理1000张输入图像时，FastVGGT比原始VGGT快4倍；5. FastVGGT减轻了长序列场景中的误差累积问题。&lt;h4&gt;结论&lt;/h4&gt;标记合并作为一种有原则的解决方案，在可扩展的3D视觉系统中具有巨大潜力。FastVGGT的成功证明了标记合并技术在3D领域的有效性，为构建更高效的3D视觉系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;3D视觉的基础模型最近在3D感知方面展示了卓越的能力。然而，由于推理时的效率低下，将这些模型扩展到长序列图像输入仍然是一个重大挑战。在这项工作中，我们详细分析了VGGT（一种先进的前馈视觉几何模型），并确定了其主要瓶颈。可视化进一步揭示了注意图中存在的标记崩溃现象。受这些发现的启发，我们探索了在前馈视觉几何模型中使用标记合并的潜力。由于3D模型独特的架构和任务特定特性，直接应用现有的合并技术证明具有挑战性。为此，我们提出了FastVGGT，这是首次通过一种无需训练的机制在3D领域利用标记合并来加速VGGT。我们设计了一种针对3D架构和任务定制的独特标记分区策略，有效地消除了冗余计算，同时保留了VGGT强大的重建能力。在多个3D几何基准上的广泛实验验证了我们方法的有效性。值得注意的是，在处理1000张输入图像时，FastVGGT比VGGT实现了4倍的加速，同时减轻了长序列场景中的误差累积。这些发现强调了标记合并作为可扩展3D视觉系统原则性解决方案的潜力。代码可在以下网址获取：https://mystorm16.github.io/fastvggt/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决VGGT模型在处理长序列图像输入时的推理效率低下问题。这个问题在现实中很重要，因为3D视觉基础模型对于机器理解和交互物理世界至关重要，但现有模型在处理大规模场景时计算成本过高，限制了实际应用。此外，长序列场景中的误差累积会导致预测漂移，影响重建质量，而原始VGGT在处理超过300张图像时还会出现内存不足问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先对VGGT进行了详细的组件级性能分析，发现Global Attention模块是主要瓶颈。通过可视化注意力图，他们发现不同token之间存在高度相似的注意力模式，表明存在大量冗余计算。基于这一观察，他们借鉴了token merging技术（如ToMeSD），并设计了专门针对3D重建任务的token分区策略。他们还参考了VGGT*的内存优化方法来处理更长的输入序列。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用token合并技术减少Global Attention模块中的计算量，同时保留关键token以确保重建质量和跨帧对应关系。整体流程包括：1) Token分区：将第一帧作为参考(dst)，保留显著token，使用区域随机采样分配dst和src；2) Token合并：将相似token合并以减少计算量；3) Token解合并：恢复原始分辨率以保持与VGGT架构兼容；4) VRAM优化：支持处理更长的输入序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将token合并引入视觉几何架构；设计针对3D任务的独特token分区策略；提出训练自由加速机制；通过内存优化支持处理1000+图像输入。与传统2D方法不同，FastVGGT处理多视图序列并保持跨帧对应；相比简单采样策略，它采用综合分区方法；相比VGGT-Long，它在加速同时减少误差累积；相比Fast3R等方法，它在保持精度的同时实现更显著加速。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FastVGGT通过创新的token合并策略，在不牺牲重建质量的情况下实现了VGGT模型4倍以上的推理加速，同时显著减少了长序列3D重建中的误差累积问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for 3D vision have recently demonstrated remarkablecapabilities in 3D perception. However, scaling these models to long-sequenceimage inputs remains a significant challenge due to inference-timeinefficiency. In this work, we present a detailed analysis of VGGT, astate-of-the-art feed-forward visual geometry model and identify its primarybottleneck. Visualization further reveals a token collapse phenomenon in theattention maps. Motivated by these findings, we explore the potential of tokenmerging in the feed-forward visual geometry model. Owing to the uniquearchitectural and task-specific properties of 3D models, directly applyingexisting merging techniques proves challenging. To this end, we proposeFastVGGT, which, for the first time, leverages token merging in the 3D domainthrough a training-free mechanism for accelerating VGGT. we devise a uniquetoken partitioning strategy tailored to 3D architectures and tasks, effectivelyeliminating redundant computation while preserving VGGT's powerfulreconstruction capacity. Extensive experiments on multiple 3D geometrybenchmarks validate the effectiveness of our approach. Notably, with 1000 inputimages, FastVGGT achieves a 4x speedup over VGGT while mitigating erroraccumulation in long-sequence scenarios. These findings underscore thepotential of token merging as a principled solution for scalable 3D visionsystems. Code is available at: https://mystorm16.github.io/fastvggt/.</description>
      <author>example@mail.com (You Shen, Zhipeng Zhang, Yansong Qu, Liujuan Cao)</author>
      <guid isPermaLink="false">2509.02560v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</title>
      <link>http://arxiv.org/abs/2509.02492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GRAM-R²的自训练方法，开发了一种生成式奖励模型，能够生成偏好标签和奖励理由，可作为奖励推理的基础模型，适用于多种任务且只需少量或无需额外微调。&lt;h4&gt;背景&lt;/h4&gt;奖励建模领域近年来从特定任务设计向通用奖励模型转变，但仍面临基本挑战：严重依赖大规模标记的偏好数据。虽然未标记数据预训练有前景，但现有方法无法在奖励模型中植入明确的推理能力。&lt;h4&gt;目的&lt;/h4&gt;为了弥合这一差距，研究提出一种自训练方法，利用未标记数据来激发奖励模型中的奖励推理能力，并开发相应的生成式奖励模型。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种自训练方法，利用未标记数据激发奖励推理能力，基于此方法开发了GRAM-R²，一种不仅生成偏好标签还生成相应奖励理由的生成式奖励模型。&lt;h4&gt;主要发现&lt;/h4&gt;在响应排序、任务适应和基于人类反馈的强化学习实验中，GRAM-R²始终表现出强大的性能，优于多个强大的判别式和生成式基线模型。&lt;h4&gt;结论&lt;/h4&gt;GRAM-R²可作为奖励推理的基础模型，适用于广泛的应用，如响应排序和任务特定的奖励调整，且只需少量或无需额外微调。&lt;h4&gt;翻译&lt;/h4&gt;近年来，奖励建模领域的显著进展是由从特定任务设计向通用奖励模型的范式转变驱动的。尽管有这一趋势，开发有效的奖励模型仍然是一个基本挑战：严重依赖大规模标记的偏好数据。在大量未标记数据上进行预训练提供了一个有前景的方向，但现有方法无法在奖励模型中植入明确的推理能力。为了弥合这一差距，我们提出了一种自训练方法，利用未标记数据来激发奖励模型中的奖励推理。基于这种方法，我们开发了GRAM-R²，一个不仅生成偏好标签还生成相应奖励理由的生成式奖励模型。GRAM-R²可作为奖励推理的基础模型，适用于广泛的应用，只需少量或无需额外微调。它可支持下游应用，如响应排序和任务特定的奖励调整。在响应排序、任务适应和基于人类反馈的强化学习实验中，GRAM-R²始终表现出强大的性能，优于多个强大的判别式和生成式基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant progress in reward modeling over recent years has been driven bya paradigm shift from task-specific designs towards generalist reward models.Despite this trend, developing effective reward models remains a fundamentalchallenge: the heavy reliance on large-scale labeled preference data.Pre-training on abundant unlabeled data offers a promising direction, butexisting approaches fall short of instilling explicit reasoning into rewardmodels. To bridge this gap, we propose a self-training approach that leveragesunlabeled data to elicit reward reasoning in reward models. Based on thisapproach, we develop GRAM-R$^2$, a generative reward model trained to producenot only preference labels but also accompanying reward rationales. GRAM-R$^2$can serve as a foundation model for reward reasoning and can be applied to awide range of tasks with minimal or no additional fine-tuning. It can supportdownstream applications such as response ranking and task-specific rewardtuning. Experiments on response ranking, task adaptation, and reinforcementlearning from human feedback demonstrate that GRAM-R$^2$ consistently deliversstrong performance, outperforming several strong discriminative and generativebaselines.</description>
      <author>example@mail.com (Chenglong Wang, Yongyu Mu, Hang Zhou, Yifu Huo, Ziming Zhu, Jiali Zeng, Murun Yang, Bei Li, Tong Xiao, Xiaoyang Hao, Chunliang Zhang, Fandong Meng, Jingbo Zhu)</author>
      <guid isPermaLink="false">2509.02492v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SpecEval: Evaluating Model Adherence to Behavior Specifications</title>
      <link>http://arxiv.org/abs/2509.02464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一个自动化框架，用于审计基础模型是否遵循其提供商发布的行为准则，发现存在系统性不一致，包括高达20%的合规差距。&lt;h4&gt;背景&lt;/h4&gt;开发基础模型的公司发布了行为准则，但模型是否实际遵循这些准则尚不清楚。虽然OpenAI、Anthropic和Google等提供商已发布详细的安全约束和定性规范，但尚未有对这些准则遵循情况的系统性审计。&lt;h4&gt;目的&lt;/h4&gt;引入一个自动化框架，用于审计模型是否遵循提供商的规范。&lt;h4&gt;方法&lt;/h4&gt;通过解析行为陈述、生成有针对性的提示，并使用模型来判断遵循情况，来审计模型。重点关注提供商规范、模型输出和开发者评估模型之间的三向一致性，这是之前双向生成器-验证器一致性的扩展。&lt;h4&gt;主要发现&lt;/h4&gt;将框架应用于来自六个开发者的16个模型，针对100多个行为陈述，发现系统性的不一致性，包括提供商之间高达20%的合规差距。&lt;h4&gt;结论&lt;/h4&gt;建立了一个必要基线：基础模型至少应在开发者评估模型的判断下，始终满足开发者的行为规范。&lt;h4&gt;翻译&lt;/h4&gt;开发基础模型的公司发布了他们承诺模型将遵循的行为准则，但模型是否实际遵循这些准则尚不清楚。虽然OpenAI、Anthropic和Google等提供商已发布详细的安全约束和定性规范，但尚未有对这些准则遵循情况的系统性审计。我们引入了一个自动化框架，通过解析行为陈述、生成有针对性的提示，并使用模型来判断遵循情况，来审计模型是否符合提供商的规范。我们的重点是提供商规范、模型输出和开发者自身评估模型之间的三向一致性；这是之前双向生成器-验证器一致性的扩展。这建立了一个必要的基线：最低要求是，基础模型应在开发者评估模型的判断下，始终满足开发者的行为规范。我们将框架应用于来自六个开发者的16个模型，针对100多个行为陈述，发现系统性的不一致性，包括提供商之间高达20%的合规差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Companies that develop foundation models publish behavioral guidelines theypledge their models will follow, but it remains unclear if models actually doso. While providers such as OpenAI, Anthropic, and Google have publisheddetailed specifications describing both desired safety constraints andqualitative traits for their models, there has been no systematic audit ofadherence to these guidelines. We introduce an automated framework that auditsmodels against their providers specifications by parsing behavioral statements,generating targeted prompts, and using models to judge adherence. Our centralfocus is on three way consistency between a provider specification, its modeloutputs, and its own models as judges; an extension of prior two way generatorvalidator consistency. This establishes a necessary baseline: at minimum, afoundation model should consistently satisfy the developer behavioralspecifications when judged by the developer evaluator models. We apply ourframework to 16 models from six developers across more than 100 behavioralstatements, finding systematic inconsistencies including compliance gaps of upto 20 percent across providers.</description>
      <author>example@mail.com (Ahmed Ahmed, Kevin Klyman, Yi Zeng, Sanmi Koyejo, Percy Liang)</author>
      <guid isPermaLink="false">2509.02464v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</title>
      <link>http://arxiv.org/abs/2509.02444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project at https://github.com/OpenBMB/AppCopilot&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了移动智能体领域的四个核心问题，并介绍了解决方案AppCopilot，这是一个多模态、多智能体的通用设备端助手系统，在泛化能力、准确性、长期能力和效率方面均有显著提升。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型和多模态基础模型的快速发展，移动智能体领域不断扩展但尚未解决基本挑战。&lt;h4&gt;目的&lt;/h4&gt;识别移动智能体需要解决的四个核心问题，并提出一个全栈闭环系统解决方案，使移动智能体能够产生实际、可扩展的影响。&lt;h4&gt;方法&lt;/h4&gt;设计并实现AppCopilot系统，包括模型层(集成多模态基础模型)、推理和控制层(思维链推理、分层任务规划和多智能体协作)、执行层(用户个性化、语音交互等功能)，并采用配置文件驱动的优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;AppCopilot在四个核心维度上取得显著改进：更强的跨任务泛化能力、更高精度的屏幕交互操作、更可靠的长距离任务完成能力，以及更快、更资源高效的运行时性能。&lt;h4&gt;结论&lt;/h4&gt;AppCopilot作为一个全栈闭环系统，有效解决了移动智能体面临的核心挑战，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型和多模态基础模型的快速发展，移动智能体领域不断扩展但尚未收敛解决基本挑战。本文确定了移动智能体需要解决的四个核心问题，以实现其实际、可扩展的影响：(1)跨任务、模态、应用和设备的泛化能力；(2)准确性，特别是精确的屏幕交互和点击定位；(3)长期能力，用于持续的多步骤目标；(4)效率，特别是在资源受限设备上的高性能运行时。我们提出了AppCopilot，这是一个多模态、多智能体、通用的设备端助手，可跨应用操作，构成从数据到部署的全栈闭环系统。AppCopilot通过端到端自主流水线实现这一理念，涵盖数据收集、训练、部署、高质量高效推理和移动应用开发。在模型层，它集成了支持中英文的多模态基础模型。在推理和控制层，它结合了思维链推理、分层任务规划和分解以及多智能体协作。在执行层，它支持用户个性化体验适应、语音交互、功能调用、跨应用和跨设备编排以及全面的移动应用支持。系统设计采用了针对异构硬件的延迟、内存和能耗的配置文件驱动优化。实验证明，AppCopilot在所有四个维度上都取得了显著改进：更强的泛化能力、更高精度的屏幕操作、更可靠的长距离任务完成以及更快、更资源高效的运行时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the raid evolution of large language models and multimodal foundationmodels, the mobile-agent landscape has proliferated without converging on thefundamental challenges. This paper identifies four core problems that must besolved for mobile agents to deliver practical, scalable impact: (1)generalization across tasks, modalities, apps, and devices; (2) accuracy,specifically precise on-screen interaction and click targeting; (3)long-horizon capability for sustained, multi-step goals; and (4) efficiency,specifically high-performance runtime on resource-constrained devices. Wepresent AppCopilot, a multimodal, multi-agent, general-purpose on-deviceassistant that operates across applications and constitutes a full-stack,closed-loop system from data to deployment. AppCopilot operationalizes thisposition through an end-to-end autonomous pipeline spanning data collection,training, deployment, high-quality and efficient inference, and mobileapplication development. At the model layer, it integrates multimodalfoundation models with robust Chinese-English support. At the reasoning andcontrol layer, it combines chain-of-thought reasoning, hierarchical taskplanning and decomposition, and multi-agent collaboration. At the executionlayer, it enables user personalization and experiential adaptation, voiceinteraction, function calling, cross-app and cross-device orchestration, andcomprehensive mobile app support. The system design incorporatesprofiling-driven optimization for latency, memory, and energy acrossheterogeneous hardware. Empirically, AppCopilot achieves significantimprovements along all four dimensions: stronger generalization,higher-precision on-screen actions, more reliable long-horizon task completion,and faster, more resource-efficient runtime.</description>
      <author>example@mail.com (Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, Dahai Li, Chen Qian)</author>
      <guid isPermaLink="false">2509.02444v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments</title>
      <link>http://arxiv.org/abs/2509.02425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenGuide，一个专为盲人或视觉障碍人士设计的辅助移动机器人系统，用于在复杂室内环境中高效定位多个物体。该系统结合了多种先进技术，能够处理开放词汇请求并自适应导航。&lt;h4&gt;背景&lt;/h4&gt;室内建筑环境如家庭和办公室通常具有复杂和杂乱的布局，这对盲人或视觉障碍人士构成重大挑战。现有辅助技术多专注于基础导航或障碍避免，缺乏在现实世界中可扩展和高效的多物体搜索能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂室内环境中帮助盲人或视觉障碍人士高效定位多个物体的可扩展且高效的辅助系统。&lt;h4&gt;方法&lt;/h4&gt;OpenGuide系统结合了自然语言理解、视觉语言基础模型(VLM)、基于前沿的探索方法和部分可观察马尔可夫决策过程(POMDP)规划器。系统能解释开放词汇请求，推理物体-场景关系，并通过价值衰减和信念空间推理实现从漏检的稳健恢复。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中，OpenGuide系统在任务成功率和搜索效率方面相比先前方法有显著提高。&lt;h4&gt;结论&lt;/h4&gt;OpenGuide为辅助生活环境中可扩展的、以人为中心的机器人辅助奠定了基础，能够有效解决盲人或视觉障碍人士在复杂环境中寻找多个物体的挑战。&lt;h4&gt;翻译&lt;/h4&gt;室内建筑环境如家庭和办公室通常呈现复杂和杂乱的布局，对盲人或视觉障碍人士构成重大挑战，特别是在涉及定位和收集多个物体的任务中。虽然许多现有的辅助技术专注于基础导航或障碍避免，但很少有系统能够在现实世界、部分可观察的环境中提供可扩展和高效的多物体搜索功能。为了解决这一差距，我们引入了OpenGuide，这是一个辅助移动机器人系统，它结合了自然语言理解、视觉语言基础模型、基于前沿的探索和部分可观察马尔可夫决策过程规划器。OpenGuide解释开放词汇请求，推理物体-场景关系，并在新环境中自适应导航和定位多个目标物品。我们的方法通过价值衰减和信念空间推理实现了从漏检的稳健恢复，从而实现更有效的探索和物体定位。我们在模拟和真实世界实验中验证了OpenGuide，证明了与先前方法相比，任务成功率和搜索效率有显著提高。这项工作为辅助生活环境中可扩展的、以人为中心的机器人辅助奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor built environments like homes and offices often present complex andcluttered layouts that pose significant challenges for individuals who areblind or visually impaired, especially when performing tasks that involvelocating and gathering multiple objects. While many existing assistivetechnologies focus on basic navigation or obstacle avoidance, few systemsprovide scalable and efficient multi-object search capabilities in real-world,partially observable settings. To address this gap, we introduce OpenGuide, anassistive mobile robot system that combines natural language understanding withvision-language foundation models (VLM), frontier-based exploration, and aPartially Observable Markov Decision Process (POMDP) planner. OpenGuideinterprets open-vocabulary requests, reasons about object-scene relationships,and adaptively navigates and localizes multiple target items in novelenvironments. Our approach enables robust recovery from missed detectionsthrough value decay and belief-space reasoning, resulting in more effectiveexploration and object localization. We validate OpenGuide in simulated andreal-world experiments, demonstrating substantial improvements in task successrate and search efficiency over prior methods. This work establishes afoundation for scalable, human-centered robotic assistance in assisted livingenvironments.</description>
      <author>example@mail.com (Yifan Xu, Qianwei Wang, Vineet Kamat, Carol Menassa)</author>
      <guid isPermaLink="false">2509.02425v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MedDINOv3: How to adapt vision foundation models for medical image segmentation?</title>
      <link>http://arxiv.org/abs/2509.02379v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MedDINOv3，一个用于将DINOv3适应到医学图像分割的简单有效框架，通过重新设计ViT架构和域自适应预训练，在多个分割任务中达到了或超过了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;CT和MRI扫描中器官和肿瘤的准确分割对诊断、治疗规划和疾病监测至关重要。虽然深度学习已推进了自动分割，但大多数模型仍然是特定任务的，缺乏跨模态和机构的泛化能力。视觉基础模型在大规模自然图像上预训练，提供了强大且可迁移的表示，但适应医学影像面临两个关键挑战：ViT骨干在医学图像分割上表现不如专业CNN，以及自然图像与医学图像之间的域差距限制了可迁移性。&lt;h4&gt;目的&lt;/h4&gt;开发一个简单有效的框架，用于将DINOv3视觉基础模型适应到医学图像分割任务中，解决域差距和性能问题。&lt;h4&gt;方法&lt;/h4&gt;重新审视普通ViT架构，设计具有多尺度令牌聚合的简单有效架构；在CT-3M（包含387万张轴向CT切片的精选集合）上进行域自适应预训练；使用多阶段DINOv3配方学习鲁棒密集特征。&lt;h4&gt;主要发现&lt;/h4&gt;MedDINOv3在四个分割基准测试中匹配或超过了最先进的性能，证明了视觉基础模型作为医学图像分割统一骨干的潜力。&lt;h4&gt;结论&lt;/h4&gt;视觉基础模型可以作为医学图像分割的统一骨干网络，通过适当的架构设计和域自适应预训练，能够实现跨模态和机构的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;CT和MRI扫描中器官和肿瘤的准确分割对于诊断、治疗规划和疾病监测至关重要。虽然深度学习已推动了自动分割的发展，但大多数模型仍然是特定任务的，缺乏跨模态和机构的泛化能力。视觉基础模型(FMs)在大规模自然图像上预训练，提供了强大且可迁移的表示。然而，将它们适应到医学影像面临两个关键挑战：(1)大多数基础模型的ViT骨干在医学图像分割上仍然表现不如专业CNN，(2)自然图像与医学图像之间的大域差距限制了可迁移性。我们引入了MedDINOv3，这是一个简单有效的框架，用于将DINOv3适应到医学分割中。我们首先重新审视普通ViT，并设计了一个具有多尺度令牌聚合的简单有效架构。然后，我们在CT-3M（一个包含387万张轴向CT切片的精选集合）上进行域自适应预训练，使用多阶段DINOv3配方学习鲁棒密集特征。MedDINOv3在四个分割基准测试中匹配或超过了最先进的性能，证明了视觉基础模型作为医学图像分割统一骨干的潜力。代码可在https://github.com/ricklisz/MedDINOv3获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of organs and tumors in CT and MRI scans is essentialfor diagnosis, treatment planning, and disease monitoring. While deep learninghas advanced automated segmentation, most models remain task-specific, lackinggeneralizability across modalities and institutions. Vision foundation models(FMs) pretrained on billion-scale natural images offer powerful andtransferable representations. However, adapting them to medical imaging facestwo key challenges: (1) the ViT backbone of most foundation models stillunderperform specialized CNNs on medical image segmentation, and (2) the largedomain gap between natural and medical images limits transferability. Weintroduce MedDINOv3, a simple and effective framework for adapting DINOv3 tomedical segmentation. We first revisit plain ViTs and design a simple andeffective architecture with multi-scale token aggregation. Then, we performdomain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CTslices, using a multi-stage DINOv3 recipe to learn robust dense features.MedDINOv3 matches or exceeds state-of-the-art performance across foursegmentation benchmarks, demonstrating the potential of vision foundationmodels as unified backbones for medical image segmentation. The code isavailable at https://github.com/ricklisz/MedDINOv3.</description>
      <author>example@mail.com (Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2509.02379v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time</title>
      <link>http://arxiv.org/abs/2509.02129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于测试时缩放(TTS)的新型零样本框架，利用多模态大语言模型的视觉-语言对齐能力，通过基于引导的方法进行直接相似性评分，实现高效跨域视觉位置识别。&lt;h4&gt;背景&lt;/h4&gt;视觉位置识别(VPR)已从手工设计的特征描述符发展到深度学习方法，但当前方法(包括视觉基础模型和多模态大语言模型)虽增强了语义理解，却在微调时存在高计算开销和有限的跨域迁移性问题。&lt;h4&gt;目的&lt;/h4&gt;解决当前VPR方法中高计算开销和有限跨域迁移性的问题，实现实时适应且无需额外训练成本。&lt;h4&gt;方法&lt;/h4&gt;采用测试时缩放(TTS)框架，利用基于引导的方法直接进行相似性评分，采用结构化提示生成长度可控的JSON输出消除两阶段处理，并整合不确定性感知自一致性(UASC)实现实时适应。&lt;h4&gt;主要发现&lt;/h4&gt;实现了跨域VPR性能的显著提升，计算效率提高高达210倍，能够在不同环境中实现优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该TTS与UASC相结合的框架实现了实时适应，无需额外训练成本即可实现优越的跨域泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉位置识别(VPR)已经从手工设计的特征描述符发展到深度学习方法，但仍面临重大挑战。当前方法，包括视觉基础模型(VFMs)和多模态大语言模型(MLLMs)，增强了语义理解，但在微调时存在高计算开销和有限的跨域迁移性。为解决这些限制，我们提出了一种利用基于引导的方法进行直接相似性评分的新型零样本框架，通过测试时缩放(TTS)利用MLLMs的视觉-语言对齐能力。我们的方法采用生成长度可控的JSON输出的结构化提示，消除了两阶段处理。带有不确定性感知自一致性(UASC)的TTS框架能够在不增加额外训练成本的情况下实现实时适应，在各种环境中实现优越的泛化能力。实验结果表明，跨域VPR性能有显著提高，计算效率增益高达210倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Place Recognition (VPR) has evolved from handcrafted descriptors todeep learning approaches, yet significant challenges remain. Currentapproaches, including Vision Foundation Models (VFMs) and Multimodal LargeLanguage Models (MLLMs), enhance semantic understanding but suffer from highcomputational overhead and limited cross-domain transferability whenfine-tuned. To address these limitations, we propose a novel zero-shotframework employing Test-Time Scaling (TTS) that leverages MLLMs'vision-language alignment capabilities through Guidance-based methods fordirect similarity scoring. Our approach eliminates two-stage processing byemploying structured prompts that generate length-controllable JSON outputs.The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enablesreal-time adaptation without additional training costs, achieving superiorgeneralization across diverse environments. Experimental results demonstratesignificant improvements in cross-domain VPR performance with up to 210$\times$computational efficiency gains.</description>
      <author>example@mail.com (Jintao Cheng, Weibin Li, Jiehao Luo, Xiaoyu Tang, Zhijian He, Jin Wu, Yao Zou, Wei Zhang)</author>
      <guid isPermaLink="false">2509.02129v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis</title>
      <link>http://arxiv.org/abs/2509.02075v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了大型语言模型在精确控制文本长度方面的挑战，比较了基础模型和指令微调模型在英语和意大利语长度控制文本生成上的差异，分析了性能和内部组件的贡献机制。&lt;h4&gt;背景&lt;/h4&gt;遵循明确的长度约束（如生成精确词数的文本）对于大型语言模型来说仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型及其指令微调对应版本在英语和意大利语长度控制文本生成方面的差异。&lt;h4&gt;方法&lt;/h4&gt;使用累积加权归因（一种从直接逻辑归因派生的指标）来分析性能和内部组件的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;指令微调显著改善了长度控制能力，主要通过专门化更深层次模型的组件来实现；在英语中，指令微调模型后层的注意力头显示越来越积极的贡献；在意大利语中，虽然注意力贡献较为减弱，但最后一层的多层感知器表现出更强的积极作用，表明存在补偿机制。&lt;h4&gt;结论&lt;/h4&gt;指令微调重新配置了模型的后期层以适应任务需求，组件级别的策略可能会根据语言上下文进行调整。&lt;h4&gt;翻译&lt;/h4&gt;遵循明确的长度约束，如生成具有精确词数的文本，对于大型语言模型来说仍然是一个重大挑战。本研究旨在调查基础模型及其指令微调对应版本在英语和意大利语长度控制文本生成方面的差异。我们使用累积加权归因（一种从直接逻辑归因派生的指标）来分析性能和内部组件的贡献。我们的发现表明，指令微调显著改善了长度控制能力，主要通过专门化更深层次模型的组件来实现。具体而言，指令微调模型后层的注意力头显示出越来越积极的贡献，特别是在英语中。在意大利语中，虽然注意力贡献较为减弱，但最后一层的多层感知器表现出更强的积极作用，这表明存在补偿机制。这些结果表明指令微调重新配置了后期层以适应任务需求，组件级别的策略可能会根据语言上下文进行调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adhering to explicit length constraints, such as generating text with aprecise word count, remains a significant challenge for Large Language Models(LLMs). This study aims at investigating the differences between foundationmodels and their instruction-tuned counterparts, on length-controlled textgeneration in English and Italian. We analyze both performance and internalcomponent contributions using Cumulative Weighted Attribution, a metric derivedfrom Direct Logit Attribution. Our findings reveal that instruction-tuningsubstantially improves length control, primarily by specializing components indeeper model layers. Specifically, attention heads in later layers of IT modelsshow increasingly positive contributions, particularly in English. In Italian,while attention contributions are more attenuated, final-layer MLPs exhibit astronger positive role, suggesting a compensatory mechanism. These resultsindicate that instruction-tuning reconfigures later layers for task adherence,with component-level strategies potentially adapting to linguistic context.</description>
      <author>example@mail.com (Elisabetta Rocchetti, Alfio Ferrara)</author>
      <guid isPermaLink="false">2509.02075v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MUSE-FM: Multi-task Environment-aware Foundation Model for Wireless Communications</title>
      <link>http://arxiv.org/abs/2509.01967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MUSE-FM的多任务环境感知基础模型，采用统一架构处理无线通信中的多个任务，同时有效整合场景信息，解决了现有无线基础模型在处理多样化输入/输出任务时的统一性局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)在无线通信领域受到越来越多的关注，因为它们具有强大的多任务学习能力，有望通过单一框架统一无线通信的多个任务。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为MUSE-FM的多任务环境感知基础模型，采用统一架构处理无线通信中的多个任务，同时有效整合场景信息。&lt;h4&gt;方法&lt;/h4&gt;提出统一的提示引导数据编码器-解码器对处理不同任务中具有异构格式和分布的数据；将环境上下文作为多模态输入集成，作为环境和信道分布的先验知识，促进跨场景特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;MUSE-FM在各种任务上表现优于现有方法；提示引导的编码器-解码器对提高了新任务配置的可扩展性；环境信息的整合提高了适应不同场景的能力。&lt;h4&gt;结论&lt;/h4&gt;MUSE-FM通过统一架构和环境感知能力，有效解决了无线通信中多任务处理的统一性问题，提高了模型的可扩展性和场景适应性。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)的最新进展在无线通信领域吸引了越来越多的关注。利用强大的多任务学习能力，FMs有望通过单一框架统一无线通信的多个任务。然而，现有的无线基础模型在处理不同通信场景中具有多样化输入/输出的多个任务时，存在统一性方面的局限性。在本文中，我们提出了一个名为MUSE-FM的多任务环境感知基础模型，采用统一架构处理无线通信中的多个任务，同时有效整合场景信息。具体而言，为实现任务统一性，我们提出了统一的提示引导数据编码器-解码器对，处理不同任务中具有异构格式和分布的数据。此外，我们将环境上下文作为多模态输入集成，作为环境和信道分布的先验知识，促进跨场景特征提取。仿真结果表明，所提出的MUSE-FM在各种任务上表现优于现有方法，其提示引导的编码器-解码器对提高了新任务配置的可扩展性。此外，环境信息的整合提高了适应不同场景的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models (FMs) have attracted increasingattention in the wireless communication domain. Leveraging the powerfulmulti-task learning capability, FMs hold the promise of unifying multiple tasksof wireless communication with a single framework. with a single framework.Nevertheless, existing wireless FMs face limitations in the uniformity toaddress multiple tasks with diverse inputs/outputs across differentcommunication scenarios.In this paper, we propose a MUlti-taSkEnvironment-aware FM (MUSE-FM) with a unified architecture to handle multipletasks in wireless communications, while effectively incorporating scenarioinformation.Specifically, to achieve task uniformity, we propose a unifiedprompt-guided data encoder-decoder pair to handle data with heterogeneousformats and distributions across different tasks. Besides, we integrate theenvironmental context as a multi-modal input, which serves as prior knowledgeof environment and channel distributions and facilitates cross-scenario featureextraction. Simulation results illustrate that the proposed MUSE-FM outperformsexisting methods for various tasks, and its prompt-guided encoder-decoder pairimproves the scalability for new task configurations. Moreover, theincorporation of environment information improves the ability to adapt todifferent scenarios.</description>
      <author>example@mail.com (Tianyue Zheng, Jiajia Guo, Linglong Dai, Shi Jin, Jun Zhang)</author>
      <guid isPermaLink="false">2509.01967v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Federated Foundation Models in Harsh Wireless Environments: Prospects, Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2509.01957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is under review in IEEE Network Magazine Special Issue on  Large AI Models for the Internet of Everything&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型(FMs)在通用智能、多模态理解和自适应学习方面展现出强大能力，但在恶劣环境中的部署仍面临挑战&lt;h4&gt;背景&lt;/h4&gt;现有分布式学习方法如联邦学习(FL)在基础设施不稳定、需要同步更新和训练资源密集的环境中难以适应，恶劣环境特点包括间歇性连接、有限计算能力、噪声数据和动态变化的网络拓扑&lt;h4&gt;目的&lt;/h4&gt;探索联邦基础模型(FFMs)作为解决这些局限性的范式，通过结合FMs的可扩展性和泛化能力与新颖的去中心化、通信感知的联邦学习框架，实现极端和对抗条件下的稳健、节能和自适应智能&lt;h4&gt;方法&lt;/h4&gt;将基础模型的可扩展性和泛化能力与新颖的、去中心化、通信感知的联邦学习框架相结合&lt;h4&gt;主要发现&lt;/h4&gt;详细介绍了恶劣环境中的系统级约束，并讨论了通信设计、模型鲁棒性和节能个性化方面的开放研究挑战&lt;h4&gt;结论&lt;/h4&gt;联邦基础模型(FFMs)有望解决基础模型在恶劣环境中部署的挑战，实现稳健、节能和自适应的智能&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)在通用智能、多模态理解和自适应学习方面展现出强大能力，但在恶劣环境中的部署仍面临挑战。现有分布式学习方法如联邦学习(FL)在基础设施不稳定、需要同步更新和训练资源密集的环境中难以适应。本文探索联邦基础模型(FFMs)作为解决这些局限性的范式，通过结合FMs的可扩展性和泛化能力与新颖的去中心化、通信感知的联邦学习框架，旨在实现极端和对抗条件下的稳健、节能和自适应智能。我们详细介绍了恶劣环境中的系统级约束，并讨论了通信设计、模型鲁棒性和节能个性化方面的开放研究挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have shown remarkable capabilities in generalizedintelligence, multimodal understanding, and adaptive learning across a widerange of domains. However, their deployment in harsh or austere environments --characterized by intermittent connectivity, limited computation, noisy data,and dynamically changing network topologies -- remains an open challenge.Existing distributed learning methods such as federated learning (FL) struggleto adapt in such settings due to their reliance on stable infrastructure,synchronized updates, and resource-intensive training. In this work, we explorethe potential of Federated Foundation Models (FFMs) as a promising paradigm toaddress these limitations. By integrating the scalability and generalizationpower of FMs with novel decentralized, communication-aware FL frameworks, weaim to enable robust, energy-efficient, and adaptive intelligence in extremeand adversarial conditions. We present a detailed breakdown of system-levelconstraints in harsh environments, and discuss the open research challenges incommunication design, model robustness, and energy-efficient personalizationfor these unique settings.</description>
      <author>example@mail.com (Evan Chen, Seyyedali Hosseinalipour, Christopher G. Brinton, David J. Love)</author>
      <guid isPermaLink="false">2509.01957v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge</title>
      <link>http://arxiv.org/abs/2509.02640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了三种适应病理学基础模型UNI2-h的变体，用于非典型有丝分裂(AMFs)的分类检测，通过结合视觉提示调整和染色标准化测试时增强，实现了高准确率和高鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;非典型有丝分裂图(AMFs)是异常细胞分裂的临床相关指标，但由于形态学模糊性和扫描仪变异性，它们的可靠检测仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究三种适应病理学基础模型UNI2-h的变体，用于MIDOG2025轨道2挑战中的非典型有丝分裂分类任务。&lt;h4&gt;方法&lt;/h4&gt;从基于LoRA的基线开始，采用视觉提示调整(VPT)提高泛化能力，并结合测试时增强(TTA)与Vahadane和Macenko染色标准化以增强模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;最终提交在预排行榜上实现了0.8837的平衡准确率和0.9513的ROC-AUC，排名在前10名团队中，表明该方法在不同成像条件下表现优异。&lt;h4&gt;结论&lt;/h4&gt;基于提示的适应结合染色标准化测试时增强，为在不同成像条件下进行非典型有丝分裂分类提供了一种有效策略。&lt;h4&gt;翻译&lt;/h4&gt;非典型有丝分裂图(AMFs)是异常细胞分裂的临床相关指标，但由于形态学模糊性和扫描仪变异性，它们的可靠检测仍然具有挑战性。在本工作中，我们研究了三种适应病理学基础模型UNI2-h的变体，用于MIDOG2025轨道2挑战。从基于LoRA的基线开始，我们发现视觉提示调整(VPT)显著提高了泛化能力，并且进一步将测试时增强(TTA)与Vahadane和Macenko染色标准化相结合提供了最佳鲁棒性。我们的最终提交在预排行榜上实现了0.8837的平衡准确率和0.9513的ROC-AUC，排名在前10名团队中。这些结果表明，基于提示的适应结合染色标准化TTA，为在不同成像条件下进行非典型有丝分裂分类提供了一种有效策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atypical mitotic figures (AMFs) are clinically relevant indicators ofabnormal cell division, yet their reliable detection remains challenging due tomorphological ambiguity and scanner variability. In this work, we investigatedthree variants of adapting the pathology foundation model UNI2-h for theMIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found thatvisual prompt tuning (VPT) substantially improved generalization, and thatfurther integrating test-time augmentation (TTA) with Vahadane and Macenkostain normalization provided the best robustness. Our final submission achieveda balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminaryleaderboard, ranking within the top 10 teams. These results demonstrate thatprompt-based adaptation combined with stain-normalization TTA offers aneffective strategy for atypical mitosis classification under diverse imagingconditions.</description>
      <author>example@mail.com (Biwen Meng, Xi Long, Jingxin Liu)</author>
      <guid isPermaLink="false">2509.02640v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Constrained Decoding for Robotics Foundation Models</title>
      <link>http://arxiv.org/abs/2509.01728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于机器人基础模型的约束解码框架，该框架能够在动态系统中对动作轨迹施加逻辑约束，确保生成的动作在运行时满足信号时序逻辑规范，无需重新训练模型。&lt;h4&gt;背景&lt;/h4&gt;机器人基础模型的最新发展为机器人系统提供了有前途的端到端和通用能力。这些模型在海量的机器人轨迹数据上进行预训练，能够处理多模态输入并直接输出一系列动作，系统随后在现实世界中执行。然而，这些模型仍然是数据驱动的，缺乏对行为正确性和安全约束的明确概念。&lt;h4&gt;目的&lt;/h4&gt;解决现有机器人基础模型缺乏行为正确性和安全约束的问题，确保生成的动作满足安全规范。&lt;h4&gt;方法&lt;/h4&gt;引入了一种用于机器人基础模型的约束解码框架，在动态系统中对动作轨迹施加逻辑约束。该方法确保生成的动作在运行时满足信号时序逻辑规范，无需重新训练，且对底层基础模型保持独立性。&lt;h4&gt;主要发现&lt;/h4&gt;在最新的导航基础模型上对其方法进行了全面评估，表明解码时的干预不仅可用于过滤不安全的动作，还可用于条件动作生成。&lt;h4&gt;结论&lt;/h4&gt;提出的约束解码框架能够有效解决机器人基础模型中缺乏行为正确性和安全约束的问题，为机器人系统的安全操作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;机器人基础模型开发的最新进展使机器人系统在端到端和通用能力方面展现出广阔前景。这些模型在海量的机器人轨迹数据上进行预训练，能够处理多模态输入并直接输出一系列动作，系统随后在现实世界中执行这些动作。尽管这种方法从提高跨任务泛化能力的角度来看很有吸引力，但这些模型仍然是数据驱动的，因此缺乏对行为正确性和安全约束的明确概念。我们通过引入一种用于机器人基础模型的约束解码框架来解决这些局限性，该框架在动态系统中对动作轨迹施加逻辑约束。我们的方法确保生成的动作在运行时能够满足信号时序逻辑规范，无需重新训练，并且对底层基础模型保持独立性。我们在最先进的导航基础模型上对其方法进行了全面评估，表明解码时的干预不仅可用于过滤不安全的动作，还可用于条件动作生成。视频可在我们的网站查看：https://constrained-robot-fms.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in the development of robotic foundation models have led topromising end-to-end and general-purpose capabilities in robotic systems. Thesemodels are pretrained on vast datasets of robot trajectories to process multi-modal inputs and directly output a sequence of action that the system thenexecutes in the real world. Although this approach is attractive from theperspective of im- proved generalization across diverse tasks, these models arestill data-driven and, therefore, lack explicit notions of behavioralcorrectness and safety constraints. We address these limitations by introducinga constrained decoding framework for robotics foundation models that enforceslogical constraints on action trajec- tories in dynamical systems. Our methodensures that generated actions provably satisfy signal temporal logic (STL)specifications at runtime without retraining, while remaining agnostic of theunderlying foundation model. We perform com- prehensive evaluation of ourapproach across state-of-the-art navigation founda- tion models and we showthat our decoding-time interventions are useful not only for filtering unsafeactions but also for conditional action-generation. Videos available on ourwebsite: https://constrained-robot-fms.github.io</description>
      <author>example@mail.com (Parv Kapoor, Akila Ganlath, Changliu Liu, Sebastian Scherer, Eunsuk Kang)</author>
      <guid isPermaLink="false">2509.01728v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control</title>
      <link>http://arxiv.org/abs/2509.01720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Succeed or Learn Slowly (SoLS)的新型离策略强化学习算法，通过区分正负样本的更新策略，提高了基础模型在多轮任务中的学习效率，并在AndroidWorld基准上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;使用基础模型进行强化学习在多轮任务中仍然面临挑战，主要受限于稀疏奖励设置和政策梯度更新问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效利用样本的强化学习算法，解决基础模型在多轮任务中的政策近似问题，特别是针对用户界面导航任务。&lt;h4&gt;方法&lt;/h4&gt;提出SoLS算法，采用改进的离策略演员-评论家方法，对高回报的正样本应用直接政策更新，对负样本应用保守的正则化更新；并通过Successful Transition Replay (STR)优先从成功交互中学习。&lt;h4&gt;主要发现&lt;/h4&gt;高回报的正样本通常不需要政策正则化，而反映不良行为的负样本可能会损害模型性能；SoLS在AndroidWorld基准上显著优于现有方法（相对提高至少17%），同时计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;SoLS通过区分对待正负样本的更新策略，有效提高了基础模型在多轮任务中的学习效率和性能，为用户界面导航等任务提供了高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用基础模型进行强化学习在多轮任务中的政策近似仍然具有挑战性。我们确定了与稀疏奖励设置和政策梯度更新相关的两个主要局限性，基于此我们提出了一个关键见解：来自高回报正样本的更新通常不需要政策正则化，而来自反映不良行为的负样本的更新可能会损害模型性能。本文介绍了Succeed or Learn Slowly (SoLS)，一种在移动应用控制任务上评估的新型离策略强化学习算法。SoLS通过改进的离策略演员-评论家方法，在微调基础模型以进行用户界面导航时提高了样本效率，对正样本应用直接政策更新，对负样本应用保守的正则化更新以防止模型退化。我们通过Successful Transition Replay (STR)增强SoLS，优先从成功的交互中学习，进一步提高样本效率。我们在AndroidWorld基准上评估SoLS，它显著优于现有方法（相对提高至少17%），包括提示工程和强化学习方法，同时比基于GPT-4o的方法需要少得多的计算资源，推理速度快5-60倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) using foundation models for policy approximationsin multi-turn tasks remains challenging. We identify two main limitationsrelated to sparse reward settings and policy gradient updates, based on whichwe formulate a key insight: updates from positive samples with high returnstypically do not require policy regularisation, whereas updates from negativesamples, reflecting undesirable behaviour, can harm model performance. Thispaper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RLalgorithm evaluated on mobile app control tasks. SoLS improves sampleefficiency when fine-tuning foundation models for user interface navigation viaa modified off-policy actor-critic approach, applying direct policy updates forpositive samples and conservative, regularised updates for negative ones toprevent model degradation. We augment SoLS with Successful Transition Replay(STR), which prioritises learning from successful interactions, furtherimproving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark,where it significantly outperforms existing methods (at least 17% relativeincrease), including prompt-engineering and RL approaches, while requiringsubstantially fewer computational resources than GPT-4o-based methods with5-60x faster inference.</description>
      <author>example@mail.com (Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, Kun Shao)</author>
      <guid isPermaLink="false">2509.01720v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    </channel>
</rss>