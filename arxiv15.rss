<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 11 Sep 2025 14:48:14 +0800</lastBuildDate>
    <item>
      <title>SocialNav-SUB: Benchmarking VLMs for Scene Understanding in Social Robot Navigation</title>
      <link>http://arxiv.org/abs/2509.08757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025 Project site:  https://larg.github.io/socialnav-sub&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SocialNav-SUB基准，用于评估视觉语言模型(VLMs)在社会机器人导航场景中的场景理解能力，研究发现当前VLMs在社会场景理解方面仍存在关键差距。&lt;h4&gt;背景&lt;/h4&gt;机器人导航在动态、以人为中心的环境中需要基于鲁棒场景理解的社会合规决策。视觉语言模型(VLMs)显示出物体识别、常识推理和上下文理解等有希望的能力，但这些能力是否能准确理解复杂的社会导航场景(如推断代理间的时空关系和人类意图)尚不明确。&lt;h4&gt;目的&lt;/h4&gt;引入SocialNav-SUB(Social Navigation Scene Understanding Benchmark)，一个视觉问答(VQA)数据集和基准，旨在评估VLMs在真实世界社会机器人导航场景中的场景理解能力，并提供统一框架比较VLMs与人类和基于规则的基线。&lt;h4&gt;方法&lt;/h4&gt;设计并创建了SocialNav-SUB基准，包含需要空间、时空和社会推理的VQA任务，通过实验评估了最先进的VLMs，并将其表现与人类和基于规则的基线方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管表现最好的VLMs与人类答案达成一致的几率令人鼓舞，但它们仍然表现不如简单的基于规则的方法和人类共识基线，表明当前VLMs在社会场景理解方面存在关键差距。&lt;h4&gt;结论&lt;/h4&gt;该基准为社交机器人导航的基础模型研究奠定了基础，提供了一个探索如何调整VLMs以满足真实世界社交机器人导航需求的框架。&lt;h4&gt;翻译&lt;/h4&gt;在动态的、以人为中心的环境中，机器人导航需要基于鲁棒场景理解的社会合规决策。最近的视觉语言模型(VLMs)展现出有希望的能力，如物体识别、常识推理和上下文理解——这些能力与社会机器人导航的微妙需求相符合。然而，目前尚不清楚VLMs是否能准确理解复杂的社会导航场景(例如推断代理之间的时空关系和人类意图)，这对于安全和社会合规的机器人导航至关重要。虽然最近的一些工作已经探索了VLMs在社会机器人导航中的应用，但没有现有工作系统性地评估它们满足这些必要条件的能力。在本文中，我们引入了SocialNav-SUB(社会导航场景理解基准)，这是一个视觉问答(VQA)数据集和基准，旨在评估VLMs在真实世界社会机器人导航场景中的场景理解能力。SocialNav-SUB提供了一个统一框架，用于评估VLMs在需要空间、时空和社会推理的社会机器人导航VQA任务中与人类和基于规则的基线的对比。通过对最先进的VLMs进行实验，我们发现尽管表现最好的VLMs与人类答案达成一致的几率令人鼓舞，但它仍然表现不如简单的基于规则的方法和人类共识基线，这表明当前VLMs在社会场景理解方面存在关键差距。我们的基准为社交机器人导航的基础模型研究奠定了基础，提供了一个探索如何调整VLMs以满足真实世界社交机器人导航需求的框架。本文概述以及代码和数据可在https://larg.github.io/socialnav-sub找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot navigation in dynamic, human-centered environments requiressocially-compliant decisions grounded in robust scene understanding. RecentVision-Language Models (VLMs) exhibit promising capabilities such as objectrecognition, common-sense reasoning, and contextual understanding-capabilitiesthat align with the nuanced requirements of social robot navigation. However,it remains unclear whether VLMs can accurately understand complex socialnavigation scenes (e.g., inferring the spatial-temporal relations among agentsand human intentions), which is essential for safe and socially compliant robotnavigation. While some recent works have explored the use of VLMs in socialrobot navigation, no existing work systematically evaluates their ability tomeet these necessary conditions. In this paper, we introduce the SocialNavigation Scene Understanding Benchmark (SocialNav-SUB), a Visual QuestionAnswering (VQA) dataset and benchmark designed to evaluate VLMs for sceneunderstanding in real-world social robot navigation scenarios. SocialNav-SUBprovides a unified framework for evaluating VLMs against human and rule-basedbaselines across VQA tasks requiring spatial, spatiotemporal, and socialreasoning in social robot navigation. Through experiments with state-of-the-artVLMs, we find that while the best-performing VLM achieves an encouragingprobability of agreeing with human answers, it still underperforms simplerrule-based approach and human consensus baselines, indicating critical gaps insocial scene understanding of current VLMs. Our benchmark sets the stage forfurther research on foundation models for social robot navigation, offering aframework to explore how VLMs can be tailored to meet real-world social robotnavigation needs. An overview of this paper along with the code and data can befound at https://larg.github.io/socialnav-sub .</description>
      <author>example@mail.com (Michael J. Munje, Chen Tang, Shuijing Liu, Zichao Hu, Yifeng Zhu, Jiaxun Cui, Garrett Warnell, Joydeep Biswas, Peter Stone)</author>
      <guid isPermaLink="false">2509.08757v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
  <item>
      <title>Foundation Models for Autonomous Driving Perception: A Survey Through Core Capabilities</title>
      <link>http://arxiv.org/abs/2509.08302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 14 figures, accepted at IEEE Open Journal of Vehicular  Technology (OJVT)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇调查论文探讨了基础模型如何革新自动驾驶感知领域，从特定任务模型转向通用架构，解决泛化、可扩展性和鲁棒性等挑战。&lt;h4&gt;背景&lt;/h4&gt;基础模型正在改变自动驾驶感知领域，从狭隘的、特定任务的深度学习模型转向多功能、通用架构，这些架构在庞大且多样化的数据集上进行训练。&lt;h4&gt;目的&lt;/h4&gt;探讨这些模型如何解决自动驾驶感知中的关键挑战，包括泛化能力、可扩展性和对分布变化的鲁棒性限制。&lt;h4&gt;方法&lt;/h4&gt;提出一个围绕四种基本能力的分类法：泛化知识、空间理解、多传感器鲁棒性和时序推理，并对每种能力进行全面的前沿方法审查。&lt;h4&gt;主要发现&lt;/h4&gt;与传统以方法为中心的调查不同，他们的框架优先考虑概念设计原则，为模型开发提供能力驱动的指导，并对基础方面提供更清晰的见解。&lt;h4&gt;结论&lt;/h4&gt;讨论了将这些能力集成到实时系统中的挑战，以及与计算需求和模型可靠性相关的部署挑战，并提出了未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;基础模型正在革新自动驾驶感知，将领域从狭隘的、特定任务的深度学习模型转变为在庞大、多样化数据集上训练的多功能、通用架构。本调查探讨了这些模型如何解决自动驾驶感知中的关键挑战，包括泛化、可扩展性和对分布变化的鲁棒性限制。调查提出了一个围绕四种基本能力构建的新颖分类法，以在动态驾驶环境中实现稳健性能：泛化知识、空间理解、多传感器鲁棒性和时序推理。对于每种能力，调查阐明了其意义并全面审查了最前沿的方法。与传统以方法为中心的调查不同，我们的独特框架优先考虑概念设计原则，为模型开发提供了能力驱动的指导，并对基础方面提供了更清晰的见解。我们最后讨论了关键挑战，特别是将这些能力集成到实时、可扩展系统中的挑战，以及与计算需求和确保模型可靠性（如幻觉问题和分布外故障）相关的更广泛的部署挑战。调查还概述了关键的未来研究方向，以促进基础模型在自动驾驶系统中的安全有效部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/OJVT.2025.3604823 10.1109/OJVT.2025.3604823  10.1109/OJVT.2025.3604823&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models are revolutionizing autonomous driving perception,transitioning the field from narrow, task-specific deep learning models toversatile, general-purpose architectures trained on vast, diverse datasets.This survey examines how these models address critical challenges in autonomousperception, including limitations in generalization, scalability, androbustness to distributional shifts. The survey introduces a novel taxonomystructured around four essential capabilities for robust performance in dynamicdriving environments: generalized knowledge, spatial understanding,multi-sensor robustness, and temporal reasoning. For each capability, thesurvey elucidates its significance and comprehensively reviews cutting-edgeapproaches. Diverging from traditional method-centric surveys, our uniqueframework prioritizes conceptual design principles, providing acapability-driven guide for model development and clearer insights intofoundational aspects. We conclude by discussing key challenges, particularlythose associated with the integration of these capabilities into real-time,scalable systems, and broader deployment challenges related to computationaldemands and ensuring model reliability against issues like hallucinations andout-of-distribution failures. The survey also outlines crucial future researchdirections to enable the safe and effective deployment of foundation models inautonomous driving systems.</description>
      <author>example@mail.com (Rajendramayavan Sathyam, Yueqi Li)</author>
      <guid isPermaLink="false">2509.08302v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Attribute-based Object Grounding and Robot Grasp Detection with Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2509.08126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 2025 IEEE-RAS 24th International Conference on Humanoid  Robots&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为OGRG的框架，能够通过自然语言理解抓取目标物体，即使在有重复物体的情况下也能进行空间推理和抓取预测。&lt;h4&gt;背景&lt;/h4&gt;让机器人通过自然语言抓取物体对有效人机交互至关重要，但现有方法难以处理开放形式语言表达，通常假设无重复物体，且依赖密集像素级标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够解释开放形式语言表达并进行空间推理的框架，以定位目标物体并预测平面抓取姿态，即使在包含重复物体实例的场景中也能工作。&lt;h4&gt;方法&lt;/h4&gt;提出基于属性的目标定位和机器人抓取(OGRG)框架，在两种设置下测试：像素级完全监督的指称抓取合成(RGS)和使用单像素标注的弱监督指称抓取可能性(RGA)。关键贡献包括双向视觉-语言融合模块和深度信息集成以增强几何推理。&lt;h4&gt;主要发现&lt;/h4&gt;OGRG在具有多样化空间语言指令的桌面场景中优于基线方法，在RGS设置下以17.59 FPS运行，提供更好的定位和抓取预测准确性；在弱监督RGA设置下，模拟和真实机器人试验中均优于基线抓取成功率。&lt;h4&gt;结论&lt;/h4&gt;OGRG成功解决了通过自然语言抓取物体的问题，特别是在处理开放形式语言表达和重复物体实例方面，在完全监督和弱监督设置下都表现出色，具有实际应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;使机器人能够通过自然语言抓取指定物体对于有效的人机交互至关重要，但仍然是一个重大挑战。现有方法通常难以处理开放形式的语言表达，通常假设没有重复的目标物体。此外，它们通常依赖于密集的像素级标注来进行物体定位和抓取配置。我们提出了基于属性的目标定位和机器人抓取(OGRG)，这是一个新框架，能够解释开放形式的语言表达并进行空间推理，以定位目标物体并预测平面抓取姿态，即使在包含重复物体实例的场景中也是如此。我们在两种设置下研究了OGRG：(1)在像素级完全监督下的指称抓取合成(RGS)，以及(2)使用仅单像素抓取标注的弱监督学习进行指称抓取可能性(RGA)。关键贡献包括双向视觉-语言融合模块和深度信息的集成，以增强几何推理，提高定位和抓取性能。实验结果表明，在具有多样化空间语言指令的桌面场景中，OGRG优于强大的基线方法。在RGS中，它在单个NVIDIA RTX 2080 Ti GPU上以17.59 FPS的速度运行，能够在闭环或多物体顺序抓取中使用，并提供比所有考虑的基线更好的定位和抓取预测准确性。在弱监督的RGA设置下，OGRG在模拟和真实机器人试验中都优于基线的抓取成功率，突显了其空间推理设计的有效性。项目页面：https://z.umn.edu/ogrg&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人通过自然语言描述来抓取指定物体的问题。具体来说，现有方法难以处理开放形式的语言表达、无法处理场景中重复出现的物体，且依赖密集的像素级标注。这个问题在现实中很重要，因为它能使机器人更好地理解人类指令，实现更自然的人机交互，同时减少对昂贵标注数据的依赖，使技术更容易在实际场景中应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括处理开放形式语言能力不足、无法处理重复物体、依赖密集标注等。他们注意到虽然多模态大语言模型性能强大，但计算需求大，难以在资源受限的机器人平台上部署。因此，作者设计了一个紧凑且计算高效的融合模块作为替代。他们借鉴了ETRG的CLIP模型和下采样-上采样策略，以及LAVT的单向融合模块，但在此基础上提出了改进的双向融合模块。同时，他们使用了Swin Transformer作为视觉主干网络和BERT作为语言特征提取器，并整合了深度信息来增强几何推理能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个名为OGRG的框架，能够解释开放形式的语言表达并进行空间推理，以定位目标物体并预测平面抓取姿态。整体流程包括：1)接受RGB图像、深度图像和语言描述作为输入；2)使用Swin Transformer提取视觉特征，BERT提取语言特征，ResNet-18提取深度特征；3)通过四阶段多模态融合过程，使用双向对齐器进行视觉、语言和深度特征的交互；4)对于RGS任务，使用FCN头生成物体掩码和抓取相关图；对于RGA任务，先生成物体掩码，再使用掩码条件抓取网络预测抓取能力图；5)输出物体定位结果和抓取姿态参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双向视觉-语言融合模块(Bi-Aligner)，同时进行视觉-语言和语言-视觉的交叉注意力计算；2)深度信息集成，增强几何推理能力；3)支持两种抓取检测设置：完全监督的RGS和弱监督的RGA；4)掩码条件抓取网络(MGN)用于RGA任务。相比之前工作，本文的双向融合比ETRG的双向适配器和LAVT的单向融合更有效地对齐特征；深度融合方式比ETRG更有效，避免信息损失；计算效率比多模态大语言模型更高，适合资源受限的机器人平台；支持弱监督学习，减少标注需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于双向视觉-语言融合和深度信息集成的OGRG框架，使机器人能够通过开放形式的自然语言描述准确识别并抓取目标物体，即使在有重复物体实例的场景中也能高效工作，同时支持完全监督和弱监督两种训练方式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to grasp objects specified through natural language isessential for effective human-robot interaction, yet it remains a significantchallenge. Existing approaches often struggle with open-form languageexpressions and typically assume unambiguous target objects without duplicates.Moreover, they frequently rely on costly, dense pixel-wise annotations for bothobject grounding and grasp configuration. We present Attribute-based ObjectGrounding and Robotic Grasping (OGRG), a novel framework that interpretsopen-form language expressions and performs spatial reasoning to ground targetobjects and predict planar grasp poses, even in scenes containing duplicatedobject instances. We investigate OGRG in two settings: (1) Referring GraspSynthesis (RGS) under pixel-wise full supervision, and (2) Referring GraspAffordance (RGA) using weakly supervised learning with only single-pixel graspannotations. Key contributions include a bi-directional vision-language fusionmodule and the integration of depth information to enhance geometric reasoning,improving both grounding and grasping performance. Experiment results show thatOGRG outperforms strong baselines in tabletop scenes with diverse spatiallanguage instructions. In RGS, it operates at 17.59 FPS on a single NVIDIA RTX2080 Ti GPU, enabling potential use in closed-loop or multi-object sequentialgrasping, while delivering superior grounding and grasp prediction accuracycompared to all the baselines considered. Under the weakly supervised RGAsetting, OGRG also surpasses baseline grasp-success rates in both simulationand real-robot trials, underscoring the effectiveness of its spatial reasoningdesign. Project page: https://z.umn.edu/ogrg</description>
      <author>example@mail.com (Houjian Yu, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Yuyin Sun, Cheng-Hao Kuo, Arnie Sen, Changhyun Choi)</author>
      <guid isPermaLink="false">2509.08126v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Tokenizing Loops of Antibodies</title>
      <link>http://arxiv.org/abs/2509.08707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 7 figures, 10 tables, code available at  https://github.com/prescient-design/igloo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Igloo的多模态抗体环标记器，能够编码主链二面角和序列信息，通过对比学习训练，高效检索匹配的环结构，解决传统方法覆盖有限的问题，并提高蛋白质基础模型在抗体设计中的应用。&lt;h4&gt;背景&lt;/h4&gt;抗体互补决定区是环状结构，对与抗原的相互作用至关重要。自1980年代以来，将CDR结构分类为规范簇有助于识别关键结构基序，但现有方法覆盖范围有限且难以整合到蛋白质基础模型中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够编码抗体环结构信息的多模态标记器，解决现有方法覆盖有限的问题，并提高蛋白质基础模型在抗体设计中的应用。&lt;h4&gt;方法&lt;/h4&gt;引入ImmunoGlobulin LOOp Tokenizer（Igloo），一种多模态抗体环标记器，编码主链二面角和序列信息。使用对比学习目标训练，将相似主链二面角的环在潜在空间中映射得更近。开发了IglooLM和IglooALM，将Igloo标记整合到蛋白质语言模型中。&lt;h4&gt;主要发现&lt;/h4&gt;1. Igloo在识别相似H3环方面比现有方法高5.9%；2. 为所有环分配标记，解决规范簇覆盖有限问题；3. IglooLM在8/10抗体-抗原靶点上优于基础模型；4. IglooLM性能与参数量多7倍的模型相当；5. IglooALM采样的抗体环序列更多样化，结构更一致。&lt;h4&gt;结论&lt;/h4&gt;Igloo证明了为抗体环引入多模态标记的益处，能够编码抗体环的多样化景观，改进蛋白质基础模型，并用于抗体CDR设计。&lt;h4&gt;翻译&lt;/h4&gt;抗体的互补决定区是与抗原相互作用的关键环状结构，对设计新型生物制剂非常重要。自1980年代以来，将CDR结构的多样性分类为规范簇有助于识别抗体的关键结构基序。然而，现有方法覆盖范围有限，且难以直接整合到蛋白质基础模型中。我们在此介绍ImmunoGlobulin LOOp Tokenizer（Igloo），这是一种多模态抗体环标记器，编码主链二面角和序列信息。Igloo使用对比学习目标进行训练，将具有相似主链二面角的环在潜在空间中映射得更近。Igloo能够从结构抗体数据库中高效检索最匹配的环结构，在识别相似的H3环方面比现有方法高出5.9%。Igloo为所有环分配标记，解决了规范簇覆盖有限的问题，同时保留了恢复规范环构象的能力。为了展示Igloo标记的多功能性，我们展示了它们可以整合到蛋白质语言模型中，形成IglooLM和IglooALM。在预测重链变体的结合亲和力方面，IglooLM在10个抗体-抗原靶点中有8个表现优于基础蛋白质语言模型。此外，它与现有的最先进的序列和 multimodal 蛋白质语言模型相当，性能与参数量多7倍的模型相当。IglooALM采样的抗体环在序列上更多样化，结构上比最先进的抗体逆向折叠模型更一致。Igloo证明了为抗体环引入多模态标记的益处，能够编码抗体环的多样化景观，改进蛋白质基础模型，并用于抗体CDR设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The complementarity-determining regions of antibodies are loop structuresthat are key to their interactions with antigens, and of high importance to thedesign of novel biologics. Since the 1980s, categorizing the diversity of CDRstructures into canonical clusters has enabled the identification of keystructural motifs of antibodies. However, existing approaches have limitedcoverage and cannot be readily incorporated into protein foundation models.Here we introduce ImmunoGlobulin LOOp Tokenizer, Igloo, a multimodal antibodyloop tokenizer that encodes backbone dihedral angles and sequence. Igloo istrained using a contrastive learning objective to map loops with similarbackbone dihedral angles closer together in latent space. Igloo can efficientlyretrieve the closest matching loop structures from a structural antibodydatabase, outperforming existing methods on identifying similar H3 loops by5.9\%. Igloo assigns tokens to all loops, addressing the limited coverage issueof canonical clusters, while retaining the ability to recover canonical loopconformations. To demonstrate the versatility of Igloo tokens, we show thatthey can be incorporated into protein language models with IglooLM andIglooALM. On predicting binding affinity of heavy chain variants, IglooLMoutperforms the base protein language model on 8 out of 10 antibody-antigentargets. Additionally, it is on par with existing state-of-the-artsequence-based and multimodal protein language models, performing comparably tomodels with $7\times$ more parameters. IglooALM samples antibody loops whichare diverse in sequence and more consistent in structure than state-of-the-artantibody inverse folding models. Igloo demonstrates the benefit of introducingmultimodal tokens for antibody loops for encoding the diverse landscape ofantibody loops, improving protein foundation models, and for antibody CDRdesign.</description>
      <author>example@mail.com (Ada Fang, Robert G. Alberstein, Simon Kelow, Frédéric A. Dreyer)</author>
      <guid isPermaLink="false">2509.08707v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>UOPSL: Unpaired OCT Predilection Sites Learning for Fundus Image Diagnosis Augmentation</title>
      <link>http://arxiv.org/abs/2509.08624v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BIBM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为UOPSL的新型无配对多模态框架，利用OCT衍生的空间先验知识增强基于眼底图像的疾病识别，解决了多模态眼科图像获取成本高和模态不平衡的问题。&lt;h4&gt;背景&lt;/h4&gt;AI驱动的多模态医学图像诊断在眼科疾病识别方面取得了显著进展，但获取配对的多模态眼科图像成本过高。眼底摄影简单且经济高效，但OCT数据有限且存在模态不平衡问题。传统仅依赖眼底或文本特征的方法无法捕捉细粒度空间信息，因为每种成像模态对病变偏好部位提供不同的线索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用OCT衍生的空间先验知识来增强眼底图像疾病识别的无配对多模态框架，解决配对数据获取困难和模态不平衡的问题。&lt;h4&gt;方法&lt;/h4&gt;提出UOPSL框架，通过扩展疾病文本描述连接无配对的眼底图像和OCT。首先，在大量无配对的OCT和眼底图像上使用对比学习，同时在OCT潜在空间中学习偏好部位矩阵。该矩阵通过大量优化捕捉OCT特征空间内的病变定位模式。在仅基于眼底图像的下游分类任务中，消除OCT输入，利用偏好部位矩阵辅助眼底图像分类学习。&lt;h4&gt;主要发现&lt;/h4&gt;在9个不同数据集、28个关键类别上进行的广泛实验表明，该框架优于现有基准方法。&lt;h4&gt;结论&lt;/h4&gt;UOPSL框架成功利用OCT衍生的空间先验知识增强基于眼底图像的疾病识别，无需配对数据，有效解决了多模态眼科图像获取成本高和模态不平衡的问题。&lt;h4&gt;翻译&lt;/h4&gt;近年来，AI驱动的多模态医学图像诊断的显著进展极大地促进了眼科疾病的识别。然而，获取配对的多模态眼科图像仍然成本过高。虽然眼底摄影简单且经济高效，但OCT数据的有限性和固有的模态不平衡阻碍了进一步发展。传统仅依赖眼底或文本特征的方法往往无法捕捉细粒度空间信息，因为每种成像模态对病变偏好部位提供不同的线索。在本研究中，我们提出了一种名为UOPSL的新型无配对多模态框架，利用大量的OCT衍生空间先验知识动态识别偏好部位，增强基于眼底图像的疾病识别。我们的方法通过扩展疾病文本描述连接无配对的眼底图像和OCT。最初，我们在大量无配对的OCT和眼底图像上使用对比学习，同时在OCT潜在空间中学习偏好部位矩阵。通过大量优化，该矩阵捕捉了OCT特征空间内的病变定位模式。在仅基于眼底图像的下游分类任务的微调或推理阶段，当无法获取配对的OCT数据时，我们消除OCT输入，利用偏好部位矩阵辅助眼底图像分类学习。在9个不同数据集、28个关键类别上进行的广泛实验表明，我们的框架优于现有基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant advancements in AI-driven multimodal medical image diagnosis haveled to substantial improvements in ophthalmic disease identification in recentyears. However, acquiring paired multimodal ophthalmic images remainsprohibitively expensive. While fundus photography is simple and cost-effective,the limited availability of OCT data and inherent modality imbalance hinderfurther progress. Conventional approaches that rely solely on fundus or textualfeatures often fail to capture fine-grained spatial information, as eachimaging modality provides distinct cues about lesion predilection sites. Inthis study, we propose a novel unpaired multimodal framework \UOPSL thatutilizes extensive OCT-derived spatial priors to dynamically identifypredilection sites, enhancing fundus image-based disease recognition. Ourapproach bridges unpaired fundus and OCTs via extended disease textdescriptions. Initially, we employ contrastive learning on a large corpus ofunpaired OCT and fundus images while simultaneously learning the predilectionsites matrix in the OCT latent space. Through extensive optimization, thismatrix captures lesion localization patterns within the OCT feature space.During the fine-tuning or inference phase of the downstream classification taskbased solely on fundus images, where paired OCT data is unavailable, weeliminate OCT input and utilize the predilection sites matrix to assist infundus image classification learning. Extensive experiments conducted on 9diverse datasets across 28 critical categories demonstrate that our frameworkoutperforms existing benchmarks.</description>
      <author>example@mail.com (Zhihao Zhao, Yinzheng Zhao, Junjie Yang, Xiangtong Yao, Quanmin Liang, Daniel Zapp, Kai Huang, Nassir Navab, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2509.08624v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Domain Knowledge is Power: Leveraging Physiological Priors for Self Supervised Representation Learning in Electrocardiography</title>
      <link>http://arxiv.org/abs/2509.08116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为PhysioCLR的心电图分析框架，它是一种生理感知的对比学习框架，通过整合领域特定先验知识增强心电图心律失常分类的可泛化性和临床相关性，解决了标记数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;心电图在诊断心脏疾病中起着关键作用，但基于人工智能的心电图分析往往受限于标记数据的可用性。自监督学习可以通过利用大规模无标签数据来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为PhysioCLR的生理感知对比学习框架，整合领域特定的先验知识，以增强心电图心律失常分类的可泛化性和临床相关性。&lt;h4&gt;方法&lt;/h4&gt;在预训练过程中，PhysioCLR学习将具有相似临床相关特征的样本嵌入拉近，同时将不相似的推开。该方法整合心电图生理相似性线索到对比学习中，引入特定于心电图的增强方法保持类别不变，并提出混合损失函数优化学习表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;在Chapman、Georgia和私人ICU数据集上，PhysioCLR比最强基线模型平均提升了12%的AUROC，展示了强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过将生理知识嵌入对比学习，PhysioCLR使模型能够学习临床上有意义且可迁移的心电图特征。&lt;h4&gt;翻译&lt;/h4&gt;目标：心电图在诊断心脏疾病中起着关键作用；然而，基于人工智能的心电图分析的有效性常常受限于标记数据的可用性。自监督学习可以通过利用大规模无标签数据来解决这个问题。我们引入了PhysioCLR（心电图生理感知对比学习表示），这是一种生理感知的对比学习框架，整合了领域特定的先验知识，以增强基于心电图的心律失常分类的可泛化性和临床相关性。方法：在预训练过程中，PhysioCLR学习将具有相似临床相关特征的样本嵌入拉近，同时将不相似的推开。与现有方法不同，我们的方法将心电图生理相似性线索整合到对比学习中，促进学习临床上有意义的表示。此外，我们引入了特定于心电图的增强方法，这些方法在增强后保持心电图类别不变，并提出了一种混合损失函数来进一步优化学习到的表示质量。结果：我们在两个公开的心电图数据集（Chapman和Georgia）上对PhysioCLR进行多标签心电图诊断评估，以及一个标记为二元分类的私人ICU数据集。在Chapman、Georgia和私人队列中，PhysioCLR比最强基线模型平均提升了12%的AUROC，强调了其强大的跨数据集泛化能力。结论：通过将生理知识嵌入对比学习，PhysioCLR使模型能够学习临床上有意义且可迁移的心电图特征。意义：PhysioCLR展示了生理信息感知的自监督学习在提供更有效且标记高效的心电图诊断方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objective: Electrocardiograms (ECGs) play a crucial role in diagnosing heartconditions; however, the effectiveness of artificial intelligence (AI)-basedECG analysis is often hindered by the limited availability of labeled data.Self-supervised learning (SSL) can address this by leveraging large-scaleunlabeled data. We introduce PhysioCLR (Physiology-aware Contrastive LearningRepresentation for ECG), a physiology-aware contrastive learning framework thatincorporates domain-specific priors to enhance the generalizability andclinical relevance of ECG-based arrhythmia classification. Methods: Duringpretraining, PhysioCLR learns to bring together embeddings of samples thatshare similar clinically relevant features while pushing apart those that aredissimilar. Unlike existing methods, our method integrates ECG physiologicalsimilarity cues into contrastive learning, promoting the learning of clinicallymeaningful representations. Additionally, we introduce ECG- specificaugmentations that preserve the ECG category post augmentation and propose ahybrid loss function to further refine the quality of learned representations.Results: We evaluate PhysioCLR on two public ECG datasets, Chapman and Georgia,for multilabel ECG diagnoses, as well as a private ICU dataset labeled forbinary classification. Across the Chapman, Georgia, and private cohorts,PhysioCLR boosts the mean AUROC by 12% relative to the strongest baseline,underscoring its robust cross-dataset generalization. Conclusion: By embeddingphysiological knowledge into contrastive learning, PhysioCLR enables the modelto learn clinically meaningful and transferable ECG eatures. Significance:PhysioCLR demonstrates the potential of physiology-informed SSL to offer apromising path toward more effective and label-efficient ECG diagnostics.</description>
      <author>example@mail.com (Nooshin Maghsoodi, Sarah Nassar, Paul F R Wilson, Minh Nguyen Nhat To, Sophia Mannina, Shamel Addas, Stephanie Sibley, David Maslove, Purang Abolmaesumi, Parvin Mousavi)</author>
      <guid isPermaLink="false">2509.08116v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAME-AB是一种创新的跨模态注意力框架，结合专家混合主干，用于抗体结合位点预测。通过整合五种生物学模态并采用自适应融合策略，有效解决了传统方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态注意力框架CAME-AB，用于稳健的抗体结合位点预测，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB集成了五种生物学基础的模态：原始氨基酸编码、BLOSUM置换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图。提出了自适应模态融合模块，使用Transformer编码器结合MoE模块促进特征专业化和容量扩展，并集成了监督对比学习目标以提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的大量实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上始终优于强大的基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过多模态特征融合和自适应跨模态推理实现了抗体结合位点预测的性能提升，为计算免疫学和治疗性抗体设计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB将五种生物学基础的模态整合为统一的多模态表示，包括原始氨基酸编码、BLOSUM置换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图。为了增强自适应跨模态推理，我们提出了自适应模态融合模块，学习根据全局相关性和输入特定贡献动态加权每个模态。Transformer编码器结合MoE模块进一步促进特征专业化和容量扩展。我们还集成了监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分离性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的大量实验表明，CAME-AB在多个指标上始终优于强大的基线方法。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v3</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
      <link>http://arxiv.org/abs/2509.05230v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference on Empirical Methods in Natural Language  Processing (EMNLP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURE是一个新颖的轻量级框架，通过分离和抑制概念性捷径同时保留内容信息，有效提高了预训练语言模型的鲁棒性和公平性，在多个数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;预训练语言模型在各种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性影响，这损害了模型的鲁棒性和公平性。&lt;h4&gt;目的&lt;/h4&gt;引入CURE框架，系统性地分离和抑制概念性捷径，同时保留必要的内容信息，以提高模型的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;首先通过一个由反转网络强化的专门内容提取器提取概念无关的表示，确保任务相关信息的最小损失；随后采用对比学习进行可控去偏，微调剩余概念线索的影响，使模型能根据目标任务适当减少有害偏见或利用有益相关性。&lt;h4&gt;主要发现&lt;/h4&gt;在IMDB和Yelp数据集上使用三种预训练架构评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;CURE方法为对抗概念偏见提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;预训练语言模型在各种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性影响，这损害了模型的鲁棒性和公平性。在这项工作中，我们引入了CURE，一个新颖且轻量级的框架，可以系统性地分离和抑制概念性捷径，同时保留必要的内容信息。我们的方法首先通过一个由反转网络强化的专门内容提取器提取概念无关的表示，确保任务相关信息的最小损失。随后的可控去偏模块采用对比学习来微调剩余概念线索的影响，使模型能够根据目标任务适当减少有害偏见或利用有益相关性。在IMDB和Yelp数据集上使用三种预训练架构评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。我们的方法为对抗概念偏见提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained language models have achieved remarkable success across diverseapplications but remain susceptible to spurious, concept-driven correlationsthat impair robustness and fairness. In this work, we introduce CURE, a noveland lightweight framework that systematically disentangles and suppressesconceptual shortcuts while preserving essential content information. Our methodfirst extracts concept-irrelevant representations via a dedicated contentextractor reinforced by a reversal network, ensuring minimal loss oftask-relevant information. A subsequent controllable debiasing module employscontrastive learning to finely adjust the influence of residual conceptualcues, enabling the model to either diminish harmful biases or harnessbeneficial correlations as appropriate for the target task. Evaluated on theIMDB and Yelp datasets using three pre-trained architectures, CURE achieves anabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,while introducing minimal computational overhead. Our approach establishes aflexible, unsupervised blueprint for combating conceptual biases, paving theway for more reliable and fair language understanding systems.</description>
      <author>example@mail.com (Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci)</author>
      <guid isPermaLink="false">2509.05230v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Robust Belief-State Policy Learning for Quantum Network Routing Under Decoherence and Time-Varying Conditions</title>
      <link>http://arxiv.org/abs/2509.08654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于特征的量子网络路由部分可观察马尔可夫决策过程(POMDP)框架，结合了信念状态规划和图神经网络(GNNs)，以解决动态量子系统中的部分可观察性、退相干和可扩展性挑战。&lt;h4&gt;背景&lt;/h4&gt;在动态量子系统中，部分可观察性、退相干和可扩展性是量子网络路由面临的主要挑战，需要新的方法来处理这些复杂问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理量子网络中部分可观察性、退相干和可扩展性挑战的路由框架，提高路由保真度和纠缠传输率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种混合GNN-POMDP架构，将复杂的量子网络动态（包括纠缠退相干和时间变化的信道噪声）编码到低维特征空间，并使用噪声自适应机制融合POMDP信念更新和GNN输出进行决策。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟量子网络实验中，该方法在多达100个节点的网络上显著提高了路由保真度和纠缠传输率，特别是在高退相干和非平稳条件下优于现有最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架为量子网络路由提供了有效的解决方案，通过结合图神经网络和POMDP框架，能够处理复杂的量子网络动态和噪声环境，实现了更好的路由性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于特征的量子网络路由部分可观察马尔可夫决策过程(POMDP)框架，结合了信念状态规划和图神经网络(GNNs)，以解决动态量子系统中的部分可观察性、退相干和可扩展性挑战。我们的方法将复杂的量子网络动态（包括纠缠退相干和时间变化的信道噪声）编码到低维特征空间，从而实现高效的信念更新和可扩展的策略学习。我们框架的核心是一个混合GNN-POMDP架构，它处理纠缠链的图结构表示以学习路由策略，并结合一个噪声自适应机制，该机制融合POMDP信念更新和GNN输出以实现鲁棒的决策制定。我们提供了理论分析，确立了信念收敛、策略改进和对噪声鲁棒性的保证。在多达100个节点的模拟量子网络上的实验表明，与最先进的基线相比，路由保真度和纠缠传输率有显著提高，特别是在高退相干和非平稳条件下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a feature-based Partially Observable Markov DecisionProcess (POMDP) framework for quantum network routing, combining belief-stateplanning with Graph Neural Networks (GNNs) to address partial observability,decoherence, and scalability challenges in dynamic quantum systems. Ourapproach encodes complex quantum network dynamics, including entanglementdegradation and time-varying channel noise, into a low-dimensional featurespace, enabling efficient belief updates and scalable policy learning. The coreof our framework is a hybrid GNN-POMDP architecture that processesgraph-structured representations of entangled links to learn routing policies,coupled with a noise-adaptive mechanism that fuses POMDP belief updates withGNN outputs for robust decision making. We provide a theoretical analysisestablishing guarantees for belief convergence, policy improvement, androbustness to noise. Experiments on simulated quantum networks with up to 100nodes demonstrate significant improvements in routing fidelity and entanglementdelivery rates compared to state-of-the-art baselines, particularly under highdecoherence and nonstationary conditions.</description>
      <author>example@mail.com (Amirhossein Taherpour, Abbas Taherpour, Tamer Khattab)</author>
      <guid isPermaLink="false">2509.08654v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Facet: highly efficient E(3)-equivariant networks for interatomic potentials</title>
      <link>http://arxiv.org/abs/2509.08418v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Facet，一种用于高效机器学习势能面的图神经网络架构，通过系统分析可导向GNNs开发，解决了计算材料发现中的计算瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;计算材料发现受限于第一性原理计算的高成本。现有的机器学习势能面方法虽然前景广阔，但面临计算瓶颈。可导向图神经网络(GNNs)利用球谐函数编码几何结构，尊重原子对称性，但维持等变性困难且计算复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的机器学习势能面的GNN架构，通过系统分析可导向GNNs来创建性能更优、计算效率更高的模型。&lt;h4&gt;方法&lt;/h4&gt;用样条替换用于原子间距离的多层感知器(MLPs)，减少计算和内存需求；引入一种通用的等变层，通过球面网格投影混合节点信息，然后使用标准MLPs，这种方法比张量积更快且比线性或门控层更具表现力。&lt;h4&gt;主要发现&lt;/h4&gt;在MPTrj数据集上，Facet与领先模型匹配，但参数少得多，训练计算量不到10%；在晶体弛豫任务中，它比MACE模型运行速度快两倍；SevenNet-0的参数可减少25%以上而不会损失准确性；这些技术使大型基础模型的训练速度提高10倍以上。&lt;h4&gt;结论&lt;/h4&gt;这些技术使大型基础模型的训练速度提高10倍以上，有可能改变计算材料发现的格局。&lt;h4&gt;翻译&lt;/h4&gt;计算材料发现受限于第一性原理计算的高成本。预测晶体结构能量的机器学习(ML)势能面很有前景，但现有方法面临计算瓶颈。可导向图神经网络(GNNs)利用球谐函数编码几何结构，尊重原子对称性——排列、旋转和平移——进行物理上真实的预测。然而，维持等变性很困难：激活函数必须修改，每层必须处理不同谐波阶数的多种数据类型。我们提出了Facet，一种用于高效ML势能面的GNN架构，通过系统分析可导向GNNs开发。我们的创新包括用样条替换用于原子间距离的昂贵多层感知器(MLPs)，在匹配性能的同时减少计算和内存需求。我们还引入了一种通用等变层，通过球面网格投影混合节点信息，然后使用标准MLPs——比张量积更快且比线性或门控层更具表现力。在MPTrj数据集上，Facet与领先模型匹配，但参数少得多，训练计算量不到10%。在晶体弛豫任务中，它比MACE模型运行速度快两倍。我们进一步表明SevenNet-0的参数可以减少25%以上而不会损失准确性。这些技术使ML势能面的大型基础模型的训练速度提高10倍以上，可能改变计算材料发现的格局。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器学习势能函数(MLIPs)训练过程中的计算效率低下问题。当前基于第一性原理的计算方法(如密度泛函理论)虽然准确但计算成本极高，而现有的机器学习势能函数训练大型基础模型需要大量计算资源和时间(如MACE-MP-0模型训练310天，SevenNet-0训练90天)。这种高计算成本阻碍了研究人员快速更新模型、探索不同训练数据和调整参数，限制了材料科学领域的研究进展和创新。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者系统分析了现有的可导向GNN设计面临的挑战，包括保持E(3)等变性的困难和标准网络组件需要修改的问题。他们识别出计算瓶颈在于等变性卷积中的消息过滤器，并考虑了网络中节点信息混合部分的计算复杂性和表达力之间的权衡。作者借鉴了SevenNet的架构基础，但采用了EquiformerV2中的等变层归一化方法，并受到计算机视觉中MLP-Mixer架构的启发，将其应用于球面处理。整体设计思路是简化计算密集组件，同时保持物理对称性和模型表达能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用高效的样条函数替代计算密集的多层感知器处理原子间距离，并提出S2-MLP-Mixer层通过球面网格投影混合节点信息。整体流程包括：1)将晶体编码为周期轨道图，原子为节点，最近邻为边；2)交互块中分解边向量为方向和距离分量，使用球谐函数和贝塞尔基编码，通过Clebsch-Gordan张量积组合特征；3)节点自交互中应用S2-MLP-Mixer处理信息；4)每层后使用残差连接和等变层归一化；5)最后通过读取层预测节点能量并求和得到总能量。整个流程保持E(3)等变性，同时优化计算效率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)用简单线性层和样条函数替代计算密集的多层感知器处理距离信息，减少参数和计算需求；2)提出S2-MLP-Mixer层，通过球面网格投影混合节点信息，在速度和表达力上优于现有方法；3)灵活的消息归一化策略，根据估计的平均邻居数归一化；4)从零开始学习元素嵌入，不依赖预训练。相比MACE，Facet避免了其计算极其昂贵的对称张量积层；相比SevenNet/GNoMe/NequIP，Facet的非线性门控函数能混合非标量信息，而不仅是缩放它们。整体上，Facet在保持相当性能的同时实现了更高的训练和推理效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Facet通过系统优化E(3)-等变图神经网络架构，用高效样条替代计算密集的多层感知器，并创新的S2-MLP-Mixer层，实现了在保持高性能的同时将训练计算需求减少90%以上的机器学习势能函数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational materials discovery is limited by the high cost offirst-principles calculations. Machine learning (ML) potentials that predictenergies from crystal structures are promising, but existing methods facecomputational bottlenecks. Steerable graph neural networks (GNNs) encodegeometry with spherical harmonics, respecting atomic symmetries -- permutation,rotation, and translation -- for physically realistic predictions. Yetmaintaining equivariance is difficult: activation functions must be modified,and each layer must handle multiple data types for different harmonic orders.We present Facet, a GNN architecture for efficient ML potentials, developedthrough systematic analysis of steerable GNNs. Our innovations includereplacing expensive multi-layer perceptrons (MLPs) for interatomic distanceswith splines, which match performance while cutting computational and memorydemands. We also introduce a general-purpose equivariant layer that mixes nodeinformation via spherical grid projection followed by standard MLPs -- fasterthan tensor products and more expressive than linear or gate layers. On theMPTrj dataset, Facet matches leading models with far fewer parameters and under10% of their training compute. On a crystal relaxation task, it runs twice asfast as MACE models. We further show SevenNet-0's parameters can be reduced byover 25% with no accuracy loss. These techniques enable more than 10x fastertraining of large-scale foundation models for ML potentials, potentiallyreshaping computational materials discovery.</description>
      <author>example@mail.com (Nicholas Miklaucic, Lai Wei, Rongzhi Dong, Nihang Fu, Sadman Sadeed Omee, Qingyang Li, Sourin Dey, Victor Fung, Jianjun Hu)</author>
      <guid isPermaLink="false">2509.08418v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>PEHRT: A Common Pipeline for Harmonizing Electronic Health Record data for Translational Research</title>
      <link>http://arxiv.org/abs/2509.08553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PEHRT，一个标准化的电子健康记录(EHR)数据整合流程，通过多机构数据整合提高转化研究的可靠性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多机构EHR数据的整合分析可以借助更大更多样化的患者群体和多种数据模态来提高转化研究的可靠性和泛化能力，但跨机构整合EHR数据面临数据异质性、语义差异和隐私问题等主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个标准化的流程来解决跨机构EHR数据整合的挑战，实现高效的数据整合而不需要个体级数据共享。&lt;h4&gt;方法&lt;/h4&gt;PEHRT是一个包含两个核心模块的标准化流程：(1)数据预处理和(2)表示学习。该流程将EHR数据映射到标准编码系统，使用先进的机器学习技术生成研究就绪的数据集，且与数据模型无关，可在各机构间简化执行。&lt;h4&gt;主要发现&lt;/h4&gt;PEHRT能够在不共享个体级数据的情况下有效整合多机构EHR数据，生成研究就绪的数据集，并在各种任务中展示了其效用。&lt;h4&gt;结论&lt;/h4&gt;PEHRT是一个有效的EHR数据整合解决方案，研究人员提供了完整的开源软件套件和用户友好的教程，使其能够被广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;多机构电子健康记录(EHR)数据的整合分析通过利用更大、更多样化的患者群体和整合多种数据模态，提高了转化研究的可靠性和泛化能力。然而，由于数据异质性、语义差异和隐私问题，跨机构整合EHR数据面临重大挑战。为解决这些挑战，我们引入了PEHRT，这是一个用于高效EHR数据整合的标准化流程，包含两个核心模块：(1)数据预处理和(2)表示学习。PEHRT将EHR数据映射到标准编码系统，并使用先进的机器学习技术生成研究就绪的数据集，无需个体级数据共享。我们的流程也与数据模型无关，基于我们丰富的实际经验设计，可在各机构间简化执行。我们提供了完整的开源软件套件，配有用户友好的教程，并使用来自不同医疗系统的数据在各种任务中证明了PEHRT的效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrative analysis of multi-institutional Electronic Health Record (EHR)data enhances the reliability and generalizability of translational research byleveraging larger, more diverse patient cohorts and incorporating multiple datamodalities. However, harmonizing EHR data across institutions poses majorchallenges due to data heterogeneity, semantic differences, and privacyconcerns. To address these challenges, we introduce $\textit{PEHRT}$, astandardized pipeline for efficient EHR data harmonization consisting of twocore modules: (1) data pre-processing and (2) representation learning. PEHRTmaps EHR data to standard coding systems and uses advanced machine learning togenerate research-ready datasets without requiring individual-level datasharing. Our pipeline is also data model agnostic and designed for streamlinedexecution across institutions based on our extensive real-world experience. Weprovide a complete suite of open source software, accompanied by auser-friendly tutorial, and demonstrate the utility of PEHRT in a variety oftasks using data from diverse healthcare systems.</description>
      <author>example@mail.com (Jessica Gronsbell, Vidul Ayakulangara Panickan, Chris Lin, Thomas Charlon, Chuan Hong, Doudou Zhou, Linshanshan Wang, Jianhui Gao, Shirley Zhou, Yuan Tian, Yaqi Shi, Ziming Gan, Tianxi Cai)</author>
      <guid isPermaLink="false">2509.08553v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Chirality in Action: Time-Aware Video Representation Learning by Latent Straightening</title>
      <link>http://arxiv.org/abs/2509.08502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了手性动作识别这一新任务，并提出了一种自监督方法来构建时间敏感的紧凑视频表示，该方法在多个数据集上表现优异。&lt;h4&gt;背景&lt;/h4&gt;现有视频嵌入模型在表示随时间变化的简单视觉变化方面表现不佳，而日常生活中存在大量需要理解时间变化的手性动作对。&lt;h4&gt;目的&lt;/h4&gt;开发能够敏感感知视觉随时间变化的紧凑视频表示，并构建时间感知的视频表示，使手性动作对具有线性可分性。&lt;h4&gt;方法&lt;/h4&gt;提出一种自监督适应方案，将时间敏感性注入到冻结图像特征序列中；基于自编码器构建模型，并引入受感知拉直启发的潜在空间归纳偏置。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在三个数据集上提供了紧凑但时间敏感的视频表示；优于在大型视频数据集上预训练的更大的视频模型；与现有模型结合时，在标准基准测试中提高了分类性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法成功构建了时间敏感的紧凑视频表示，在手性动作识别任务上表现优异，并能提升现有模型的性能。&lt;h4&gt;翻译&lt;/h4&gt;我们的目标是开发对随时间变化的视觉变化敏感的紧凑视频表示。为了衡量这种时间敏感性，我们引入了一个新任务：手性动作识别，其中需要区分一对时间上相反的动作，如'开门vs关门'、'接近vs远离某物'、'折叠vs展开纸张'等。这类动作(i)在日常生活中频繁出现，(ii)需要理解随时间变化的简单视觉变化(物体状态、大小、空间位置、计数等)，(iii)已知许多视频嵌入表示效果不佳。我们的目标是构建时间感知的视频表示，使这些手性动作对具有线性可分性。为此，我们提出了一种自监督适应方案，将时间敏感性注入到冻结图像特征序列中。我们的模型基于自编码器，其潜在空间具有受感知拉直启发的归纳偏置。我们证明，在三个数据集上，这为所提出的任务提供了紧凑但时间敏感的视频表示。我们的方法(i)优于在大型视频数据集上预训练的更大的视频模型，(ii)与这些现有模型结合时，在标准基准测试中提高了分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Our objective is to develop compact video representations that are sensitiveto visual change over time. To measure such time-sensitivity, we introduce anew task: chiral action recognition, where one needs to distinguish between apair of temporally opposite actions, such as "opening vs. closing a door","approaching vs. moving away from something", "folding vs. unfolding paper",etc. Such actions (i) occur frequently in everyday life, (ii) requireunderstanding of simple visual change over time (in object state, size, spatialposition, count . . . ), and (iii) are known to be poorly represented by manyvideo embeddings. Our goal is to build time aware video representations whichoffer linear separability between these chiral pairs. To that end, we propose aself-supervised adaptation recipe to inject time-sensitivity into a sequence offrozen image features. Our model is based on an auto-encoder with a latentspace with inductive bias inspired by perceptual straightening. We show thatthis results in a compact but time-sensitive video representation for theproposed task across three datasets: Something-Something, EPIC-Kitchens, andCharade. Our method (i) outperforms much larger video models pre-trained onlarge-scale video datasets, and (ii) leads to an improvement in classificationperformance on standard benchmarks when combined with these existing models.</description>
      <author>example@mail.com (Piyush Bagad, Andrew Zisserman)</author>
      <guid isPermaLink="false">2509.08502v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Joint Learning using Mixture-of-Expert-Based Representation for Enhanced Speech Generation and Robust Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.08470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了Sparse MERIT框架，通过多任务学习同时优化语音情感识别和语音增强任务，解决了传统方法在噪声条件下的性能下降问题。&lt;h4&gt;背景&lt;/h4&gt;语音情感识别(SER)在构建情感感知语音系统中起关键作用，但在噪声条件下性能显著下降。语音增强(SE)可以提高鲁棒性，但会引入模糊情感线索的伪影，并增加计算开销。传统多任务学习模型在SE和SER任务中存在梯度干扰和表示冲突问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统多任务学习在语音增强和情感识别任务中存在的梯度干扰和表示冲突问题，提高在噪声条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Sparse Mixture-of-Experts Representation Integration Technique (Sparse MERIT)，一种灵活的多任务学习框架，在自监督语音表示上应用帧级专家路由。该框架包含任务特定的门控网络，动态地从共享专家池中为每帧选择专家，实现参数高效和任务自适应的表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上都优于基线模型。在-5 dB信噪比条件下，相比SE预处理基线，SER F1-macro平均提高12.0%；相比简单MTL基线提高3.4%，并且在未见噪声条件下具有统计显著性。对于SE任务，分段信噪比(SSNR)比SE预处理基线提高28.2%，比简单MTL基线提高20.0%。&lt;h4&gt;结论&lt;/h4&gt;Sparse MERIT在嘈杂环境中为情感识别和增强任务提供了稳健且可泛化的性能。&lt;h4&gt;翻译&lt;/h4&gt;语音情感识别(SER)在构建情感感知语音系统中起着关键作用，但在噪声条件下其性能会显著下降。尽管语音增强(SE)可以提高鲁棒性，但它常常会引入模糊情感线索的伪影，并为管道增加计算开销。多任务学习(MTL)通过联合优化SE和SER任务提供了一种替代方案。然而，传统的共享主干模型经常在任务间遭受梯度干扰和表示冲突。为解决这些挑战，我们提出了稀疏专家表示集成技术(Sparse MERIT)，一种灵活的MTL框架，该框架在自监督语音表示上应用帧级专家路由。Sparse MERIT集成了任务特定的门控网络，动态地从共享专家池中为每帧选择专家，实现了参数高效和任务自适应的表示学习。在MSP-Podcast语料库上的实验表明，Sparse MERIT在SER和SE任务上都一致优于基线模型。在最具挑战性的-5 dB信噪比条件下，Sparse MERIT比依赖SE预处理策略的基线平均提高SER F1-macro 12.0%，比简单MTL基线提高3.4%，在未见噪声条件下具有统计显著性。对于SE任务，Sparse MERIT将分段信噪比(SSNR)比SE预处理基线提高28.2%，比简单MTL基线提高20.0%。这些结果表明，Sparse MERIT在嘈杂环境中为情感识别和增强任务提供了稳健且可泛化的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech emotion recognition (SER) plays a critical role in buildingemotion-aware speech systems, but its performance degrades significantly undernoisy conditions. Although speech enhancement (SE) can improve robustness, itoften introduces artifacts that obscure emotional cues and adds computationaloverhead to the pipeline. Multi-task learning (MTL) offers an alternative byjointly optimizing SE and SER tasks. However, conventional shared-backbonemodels frequently suffer from gradient interference and representationalconflicts between tasks. To address these challenges, we propose the SparseMixture-of-Experts Representation Integration Technique (Sparse MERIT), aflexible MTL framework that applies frame-wise expert routing overself-supervised speech representations. Sparse MERIT incorporates task-specificgating networks that dynamically select from a shared pool of experts for eachframe, enabling parameter-efficient and task-adaptive representation learning.Experiments on the MSP-Podcast corpus show that Sparse MERIT consistentlyoutperforms baseline models on both SER and SE tasks. Under the mostchallenging condition of -5 dB signal-to-noise ratio (SNR), Sparse MERITimproves SER F1-macro by an average of 12.0% over a baseline relying on a SEpre-processing strategy, and by 3.4% over a naive MTL baseline, withstatistical significance on unseen noise conditions. For SE, Sparse MERITimproves segmental SNR (SSNR) by 28.2% over the SE pre-processing baseline andby 20.0% over the naive MTL baseline. These results demonstrate that SparseMERIT provides robust and generalizable performance for both emotionrecognition and enhancement tasks in noisy environments.</description>
      <author>example@mail.com (Jing-Tong Tzeng, Carlos Busso, Chi-Chun Lee)</author>
      <guid isPermaLink="false">2509.08470v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>VRAE: Vertical Residual Autoencoder for License Plate Denoising and Deblurring</title>
      <link>http://arxiv.org/abs/2509.08392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于交通监控图像增强任务的垂直残差自编码器（VRAE）架构，通过在每个编码阶段注入感知输入的特征，有效提升了车牌识别系统在恶劣条件下的性能。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的交通监控中，车辆图像在恶劣天气、光线不足或高速运动的情况下常常受到严重的噪声和模糊影响，这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。&lt;h4&gt;目的&lt;/h4&gt;快速实时地恢复这些退化图像是增强识别性能的关键预处理步骤。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种垂直残差自编码器（VRAE）架构，采用了一种增强策略，使用辅助块在每个编码阶段注入感知输入的特征，以引导表示学习过程，与传统的自编码器相比，能够在整个网络中更好地保留一般信息。&lt;h4&gt;主要发现&lt;/h4&gt;在带有可见车牌的车辆图像数据集上的实验表明，VRAE方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的AE相比，它将PSNR提高了约20%，将NMSE降低了约50%，将SSIM提高了1%，而参数仅增加了约1%。&lt;h4&gt;结论&lt;/h4&gt;VRAE架构在交通监控图像增强任务中表现优异，能够在保持参数量增加很小的情况下显著提高图像质量指标。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的交通监控中，车辆图像在恶劣天气、光线不足或高速运动的情况下常常受到严重的噪声和模糊影响。这些退化显著降低了车牌识别系统的准确性，特别是当车牌仅占整个车辆图像的一小部分区域时。快速实时地恢复这些退化图像是增强识别性能的关键预处理步骤。在这项工作中，我们提出了一种用于交通监控图像增强任务的垂直残差自编码器（VRAE）架构。该方法采用了一种增强策略，该策略使用一个辅助块，在每个编码阶段注入感知输入的特征，以引导表示学习过程，与传统的自编码器相比，能够在整个网络中更好地保留一般信息。在带有可见车牌的车辆图像数据集上的实验表明，我们的方法持续优于自编码器（AE）、生成对抗网络（GAN）和基于流的方法（FB）。与相同深度的AE相比，它将PSNR提高了约20%，将NMSE降低了约50%，将SSIM提高了1%，而参数仅增加了约1%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world traffic surveillance, vehicle images captured under adverseweather, poor lighting, or high-speed motion often suffer from severe noise andblur. Such degradations significantly reduce the accuracy of license platerecognition systems, especially when the plate occupies only a small regionwithin the full vehicle image. Restoring these degraded images a fast realtimemanner is thus a crucial pre-processing step to enhance recognitionperformance. In this work, we propose a Vertical Residual Autoencoder (VRAE)architecture designed for the image enhancement task in traffic surveillance.The method incorporates an enhancement strategy that employs an auxiliaryblock, which injects input-aware features at each encoding stage to guide therepresentation learning process, enabling better general informationpreservation throughout the network compared to conventional autoencoders.Experiments on a vehicle image dataset with visible license plates demonstratethat our method consistently outperforms Autoencoder (AE), GenerativeAdversarial Network (GAN), and Flow-Based (FB) approaches. Compared with AE atthe same depth, it improves PSNR by about 20\%, reduces NMSE by around 50\%,and enhances SSIM by 1\%, while requiring only a marginal increase of roughly1\% in parameters.</description>
      <author>example@mail.com (Cuong Nguyen, Dung T. Tran, Hong Nguyen, Xuan-Vu Phan, Nam-Phong Nguyen)</author>
      <guid isPermaLink="false">2509.08392v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Bitrate-Controlled Diffusion for Disentangling Motion and Content in Video</title>
      <link>http://arxiv.org/abs/2509.08376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的通用框架，用于将视频数据分解为动态运动和静态内容两个组成部分。该方法采用自监督流水线，假设更少，归纳偏差更小，能够有效学习视频的解缠结表示。&lt;h4&gt;背景&lt;/h4&gt;视频数据包含动态运动和静态内容两个重要组成部分，如何有效解缠结这两个成分是视频理解和生成领域的挑战，现有方法通常有较多假设和归纳偏差，限制了通用性。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖且通用的框架，将视频数据分解为动态运动和静态内容两个组成部分，同时减少假设和归纳偏差，实现更通用的视频表示学习。&lt;h4&gt;方法&lt;/h4&gt;使用基于Transformer的架构联合生成帧级运动和片段级内容的隐式特征；引入低比特率矢量量化作为信息瓶颈促进解缠结；将比特率控制的潜在运动和内容作为条件输入用于去噪扩散模型；通过自监督方式进行表示学习；在多种视频类型上进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在说话头视频上有效进行运动迁移和自回归运动生成；能够推广到2D卡通角色像素精灵等其他视频类型；通过低比特率矢量量化成功促进运动和内容解缠结；基于Transformer的架构能有效生成灵活的隐式特征表示。&lt;h4&gt;结论&lt;/h4&gt;该工作为自监督学习解缠结视频表示提供了新视角，通过减少假设和归纳偏差，实现了更通用的视频表示学习，有助于视频分析和生成领域的更广泛发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖且通用的框架，将视频数据分解为其动态运动和静态内容组成部分。我们提出的方法是一个自监督流水线，比之前的工作假设更少，归纳偏差更小：它利用基于Transformer的架构联合生成帧级运动和片段级内容的灵活隐式特征，并引入低比特率矢量量化作为信息瓶颈，促进解缠结并形成有意义的离散运动空间。比特率控制的潜在运动和内容被用作去噪扩散模型的条件输入，以促进自监督表示学习。我们在真实世界的说话头视频上验证了我们的解缠结表示学习框架，并进行运动迁移和自回归运动生成任务。此外，我们还展示了我们的方法可以推广到其他类型的视频数据，如2D卡通角色的像素精灵。我们的工作为自监督学习解缠结视频表示提供了新视角，有助于视频分析和生成领域的更广泛发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel and general framework to disentangle video data into itsdynamic motion and static content components. Our proposed method is aself-supervised pipeline with less assumptions and inductive biases thanprevious works: it utilizes a transformer-based architecture to jointlygenerate flexible implicit features for frame-wise motion and clip-wisecontent, and incorporates a low-bitrate vector quantization as an informationbottleneck to promote disentanglement and form a meaningful discrete motionspace. The bitrate-controlled latent motion and content are used as conditionalinputs to a denoising diffusion model to facilitate self-supervisedrepresentation learning. We validate our disentangled representation learningframework on real-world talking head videos with motion transfer andauto-regressive motion generation tasks. Furthermore, we also show that ourmethod can generalize to other types of video data, such as pixel sprites of 2Dcartoon characters. Our work presents a new perspective on self-supervisedlearning of disentangled video representations, contributing to the broaderfield of video analysis and generation.</description>
      <author>example@mail.com (Xiao Li, Qi Chen, Xiulian Peng, Kai Yu, Xie Chen, Yan Lu)</author>
      <guid isPermaLink="false">2509.08376v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>SimCroP: Radiograph Representation Learning with Similarity-driven Cross-granularity Pre-training</title>
      <link>http://arxiv.org/abs/2509.08311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种胸部CT上的相似性驱动跨粒度预训练（SimCroP）框架，结合相似性驱动对齐和跨粒度融合技术，有效解决了CT影像中病变分布的空间稀疏性问题，提高了放射影像解释能力，在多尺度下游任务中表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;医学视觉语言预训练在从大量配对的放射影像和报告中学习代表性特征方面显示出巨大潜力。然而，在CT扫描中，包含复杂结构的病变分布具有空间稀疏性，且报告中不同病理描述与其在放射影像中对应区域之间的复杂关系带来了额外挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个胸部CT上的相似性驱动跨粒度预训练框架，结合相似性驱动对齐和跨粒度融合，以提高放射影像解释能力，更好地捕捉稀疏放射影像中的关键病理结构。&lt;h4&gt;方法&lt;/h4&gt;利用多模态掩码建模优化编码器以理解放射影像的低级语义；设计相似性驱动对齐机制使编码器自适应选择并与报告句子对齐；通过跨粒度融合模块整合实例级别和单词-块级别的多模态信息；在大型配对CT-报告数据集上预训练，并在五个公共数据集上的图像分类和分割任务中验证。&lt;h4&gt;主要发现&lt;/h4&gt;SimCroP框架在实验中超越了最先进的医学自监督学习方法和医学视觉语言预训练方法，有效解决了CT影像中病变分布的空间稀疏性问题，提高了多尺度下游任务的性能。&lt;h4&gt;结论&lt;/h4&gt;SimCroP通过相似性驱动对齐和跨粒度融合技术，能够更好地捕捉稀疏放射影像中的关键病理结构，在多尺度下游任务中表现出改进的性能，为医学影像分析提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学视觉语言预训练在从大量配对的放射影像和报告中学习代表性特征方面显示出巨大潜力。然而，在计算机断层扫描（CT）扫描中，包含复杂结构的病变分布具有空间稀疏性。此外，报告中每个句子内不同病理描述与其在放射影像中对应子区域之间的复杂且隐含的关系带来了额外挑战。在本文中，我们在胸部CT上提出了一个相似性驱动跨粒度预训练（SimCroP）框架，它结合相似性驱动对齐和跨粒度融合来提高放射影像解释能力。我们首先利用多模态掩码建模来优化编码器，以便从放射影像中理解精确的低级语义。然后设计相似性驱动对齐来预训练编码器，以自适应选择并与报告中每个句子相对应的正确块进行对齐。跨粒度融合模块整合实例级别和单词-块级别的多模态信息，这有助于模型更好地捕捉稀疏放射影像中的关键病理结构，从而提高多尺度下游任务的性能。SimCroP在大型配对CT-报告数据集上进行预训练，并在五个公共数据集上的图像分类和分割任务中进行了验证。实验结果表明，SimCroP优于最先进的医学自监督学习方法和医学视觉语言预训练方法。代码和模型可在 https://github.com/ToniChopp/SimCroP 获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决医学影像（特别是CT扫描）中两个关键问题：一是病变的空间分布稀疏性问题，病变包含复杂结构但分布稀疏，导致特征提取困难；二是医学影像报告中不同病理描述与影像中相应子区域之间的复杂隐含关系难以建立对应。这些问题在现实中非常重要，因为医学影像标注需要专业医生投入大量时间和精力，资源密集且负担重，这限制了深度学习在医学影像中的应用；同时，医学影像报告包含丰富的语义信息，能有效辅助影像理解，但如何有效利用这些信息一直是挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有医学视觉语言预训练方法在处理3D胸部CT时的局限性，特别是对空间稀疏性和报告结构复杂性的处理不足。作者借鉴了多模态掩码自编码器架构，使用Vision Transformer作为视觉编码器，BERT作为文本编码器。针对CT的空间稀疏性，设计了相似性驱动对齐机制，使模型能自动选择和与报告句子对齐正确的影像块；针对报告结构复杂性，引入跨粒度融合模块，整合不同粒度的多模态信息。作者还借鉴了对比学习思想，但将其应用于细粒度的句子-影像块对齐，以及掩码建模方法用于图像和文本重建。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; SimCroP的核心思想是通过相似性驱动对齐和跨粒度融合来改进医学影像表征学习，让模型自动学习将医学报告中的描述性句子与影像中对应的子区域对齐，同时整合全局和局部信息以更好地理解和稀疏的医学影像。整体流程包括：1)多模态掩码建模：将CT影像分割并掩码75%的块，对报告文本分词并掩码部分词，通过编码器处理未掩码内容，解码器重建被掩码内容；2)相似性驱动对齐：计算句子与影像块相似度，为每个句子选择最相似的K个块，通过对比学习损失对齐特征；3)跨粒度融合：通过全局平均池化获取实例级别特征，通过交叉注意力计算词块级别特征，融合后重建被掩码文本；4)整体优化：结合影像重建、相似性对齐和文本重建损失优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)相似性驱动对齐机制：无需人工标注自动对齐句子与影像子区域，不同于之前方法依赖全局对比学习；2)跨粒度融合：同时整合实例级别和词块级别多模态信息，优于仅使用交叉注意力的方法；3)针对医学影像特殊性的设计：专门解决CT空间稀疏性和报告结构复杂性；4)多目标学习框架：结合掩码图像建模、句子-子区域对齐和跨粒度掩码报告建模。相比之前工作，SimCroP能更好地处理医学影像的特殊性，自动建立精确的文本-影像对应关系，整合多粒度信息，从而获得更全面的表征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SimCroP通过相似性驱动对齐和跨粒度融合的创新方法，有效解决了医学影像中的空间稀疏性问题，显著提升了医学影像表征学习能力，在多个下游任务上超越了现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical vision-language pre-training shows great potential in learningrepresentative features from massive paired radiographs and reports. However,in computed tomography (CT) scans, the distribution of lesions which containintricate structures is characterized by spatial sparsity. Besides, the complexand implicit relationships between different pathological descriptions in eachsentence of the report and their corresponding sub-regions in radiographs poseadditional challenges. In this paper, we propose a Similarity-DrivenCross-Granularity Pre-training (SimCroP) framework on chest CTs, which combinessimilarity-driven alignment and cross-granularity fusion to improve radiographinterpretation. We first leverage multi-modal masked modeling to optimize theencoder for understanding precise low-level semantics from radiographs. Then,similarity-driven alignment is designed to pre-train the encoder to adaptivelyselect and align the correct patches corresponding to each sentence in reports.The cross-granularity fusion module integrates multimodal information acrossinstance level and word-patch level, which helps the model better capture keypathology structures in sparse radiographs, resulting in improved performancefor multi-scale downstream tasks. SimCroP is pre-trained on a large-scalepaired CT-reports dataset and validated on image classification andsegmentation tasks across five public datasets. Experimental resultsdemonstrate that SimCroP outperforms both cutting-edge medical self-supervisedlearning methods and medical vision-language pre-training methods. Codes andmodels are available at https://github.com/ToniChopp/SimCroP.</description>
      <author>example@mail.com (Rongsheng Wang, Fenghe Tang, Qingsong Yao, Rui Yan, Xu Zhang, Zhen Huang, Haoran Lai, Zhiyang He, Xiaodong Tao, Zihang Jiang, Shaohua Kevin Zhou)</author>
      <guid isPermaLink="false">2509.08311v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>How Far Are We from True Unlearnability?</title>
      <link>http://arxiv.org/abs/2509.08058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了现有不可学习方法在多任务场景下的局限性，从模型优化角度分析了不可学习性的本质，并提出了新的评估指标。&lt;h4&gt;背景&lt;/h4&gt;高质量数据在大模型时代至关重要，但使用未经授权的数据进行模型训练损害数据所有者利益。为解决此问题，提出了不可学习方法(UEs)，通过破坏数据训练可用性生成不可学习的示例。&lt;h4&gt;目的&lt;/h4&gt;探究如何实现真正跨任务的不可学习示例，并评估现有不可学习方法的能力边界。&lt;h4&gt;方法&lt;/h4&gt;使用简单模型架构观察干净与中毒模型的收敛差异；从损失景观分析关键参数优化路径；提出锐度感知可学习性(SAL)量化参数不可学习性；提出不可学习距离(UD)衡量数据不可学习性；使用UD对主流不可学习方法进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;在Taskonomy多任务数据集上，现有UEs在语义分割等任务上仍表现良好，未能实现真正的跨任务不可学习性；损失景观与不可学习性有密切关系。&lt;h4&gt;结论&lt;/h4&gt;现有不可学习方法在多任务场景下存在局限性，提出的UD指标有助于促进社区对不可学习方法能力边界的认识。&lt;h4&gt;翻译&lt;/h4&gt;高质量数据在大模型时代扮演着不可或缺的角色，但使用未经授权的数据进行模型训练严重损害了数据所有者的利益。为克服这一威胁，已提出几种不可学习方法，它们通过破坏数据的训练可用性来生成不可学习的示例(UEs)。显然，由于训练目的未知和现有模型的强大表征学习能力，这些数据预计对多任务模型都不可学习，即不会帮助提高模型性能。然而，出乎意料的是，我们在Taskonomy多任务数据集上发现，UEs在语义分割等任务上仍然表现良好，未能表现出跨任务的不可学习性。这种现象让我们质疑：我们距离实现真正不可学习的示例还有多远？我们尝试从模型优化的角度回答这个问题。为此，我们使用简单模型架构观察了干净模型和中毒模型的收敛过程差异。随后，从损失景观中我们发现只有部分关键参数优化路径显示出显著差异，这表明损失景观与不可学习性密切相关。因此，我们利用损失景观解释了UEs的潜在原因，并提出了锐度感知可学习性(SAL)来基于此解释量化参数的不可学习性。此外，我们提出了不可学习距离(UD)来基于干净和中毒模型中参数的SAL分布来衡量数据的不可学习性。最后，我们使用提出的UD对主流不可学习方法进行了基准测试，旨在促进社区对现有不可学习方法能力边界的认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality data plays an indispensable role in the era of large models, butthe use of unauthorized data for model training greatly damages the interestsof data owners. To overcome this threat, several unlearnable methods have beenproposed, which generate unlearnable examples (UEs) by compromising thetraining availability of data. Clearly, due to unknown training purposes andthe powerful representation learning capabilities of existing models, thesedata are expected to be unlearnable for models across multiple tasks, i.e.,they will not help improve the model's performance. However, unexpectedly, wefind that on the multi-task dataset Taskonomy, UEs still perform well in taskssuch as semantic segmentation, failing to exhibit cross-task unlearnability.This phenomenon leads us to question: How far are we from attaining trulyunlearnable examples? We attempt to answer this question from the perspectiveof model optimization. To this end, we observe the difference in theconvergence process between clean and poisoned models using a simple modelarchitecture. Subsequently, from the loss landscape we find that only a part ofthe critical parameter optimization paths show significant differences,implying a close relationship between the loss landscape and unlearnability.Consequently, we employ the loss landscape to explain the underlying reasonsfor UEs and propose Sharpness-Aware Learnability (SAL) to quantify theunlearnability of parameters based on this explanation. Furthermore, we proposean Unlearnable Distance (UD) to measure the unlearnability of data based on theSAL distribution of parameters in clean and poisoned models. Finally, weconduct benchmark tests on mainstream unlearnable methods using the proposedUD, aiming to promote community awareness of the capability boundaries ofexisting unlearnable methods.</description>
      <author>example@mail.com (Kai Ye, Liangcai Su, Chenxiong Qian)</author>
      <guid isPermaLink="false">2509.08058v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Unidimensional semi-discrete partial optimal transport</title>
      <link>http://arxiv.org/abs/2509.08799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种解决一维部分最优传输问题的半离散形式，通过正则化方法克服了对偶函数正则性不足的困难，并验证了二次收敛率，同时展示了该方法在稳定性和计算效率方面的优势。&lt;h4&gt;背景&lt;/h4&gt;一维部分最优传输问题在风险管理、人群运动建模和点云配准等领域有广泛应用。与高维情况不同，一维情况下的对偶函数正则性较差，这给问题的求解带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究一维二次成本部分最优传输的半离散形式，解决对偶函数正则性不足的问题，并设计有效的数值方法。&lt;h4&gt;方法&lt;/h4&gt;引入基于沿辅助维度加厚密度的正则化过程，证明正则化对偶问题最大值以二次速率收敛到原始对偶问题最大值，并开发利用正则化函数的数值方案。&lt;h4&gt;主要发现&lt;/h4&gt;正则化对偶问题的最大值以二次速率收敛到原始对偶问题的最大值；所提出的数值方案能够有效解决一维部分传输问题；与全离散设置相比，该方法具有更好的稳定性和计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的正则化方法有效解决了一维部分最优传输问题中的正则性不足挑战，提供了具有二次收敛率的数值方案，并在实际应用中表现出良好的稳定性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了一维二次成本部分最优传输的半离散形式，其中概率密度被部分传输到总质量更小的有限狄拉克质量点之和。这个问题在风险管理、人群运动建模以及用于点云配准的切片部分传输算法中自然出现。与高维设置不同，一维情况下的对偶函数正则性较差。为了克服这一困难，我们引入了一种基于沿辅助维度加厚密度的正则化过程。我们证明了正则化对偶问题的最大值收敛到原始对偶问题的最大值，收敛速度与引入的厚度成二次方关系。我们进一步提供了一个利用正则化函数的数值方案，并通过模拟验证了我们的分析，确认了二次收敛率。最后，我们比较了半离散和全离散设置，证明我们的方法在一维部分传输问题上既提高了稳定性又提高了计算效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the semi-discrete formulation of one-dimensional partial optimaltransport with quadratic cost, where a probability density is partiallytransported to a finite sum of Dirac masses of smaller total mass. This problemarises naturally in applications such as risk management, the modeling of crowdmotion, and sliced partial transport algorithms for point cloud registration.Unlike higher-dimensional settings, the dual functional in the unidimensionalcase exhibits reduced regularity. To overcome this difficulty, we introduce aregularization procedure based on thickening the density along an auxiliarydimension. We prove that the maximizers of the regularized dual problemconverge to those of the original dual problem, with quadratic rate in theintroduced thickness. We further provide a numerical scheme that leverages theregularized functional, and we validate our analysis with simulations thatconfirm the quadratic convergence rate. Finally, we compare the semi-discreteand fully discrete settings, demonstrating that our approach offers bothimproved stability and computational efficiency for unidimensional partialtransport problems.</description>
      <author>example@mail.com (Adrien Cances, Hugo Leclerc)</author>
      <guid isPermaLink="false">2509.08799v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Deep Unrolling of Sparsity-Induced RDO for 3D Point Cloud Attribute Coding</title>
      <link>http://arxiv.org/abs/2509.08685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于多分辨率B样条投影框架的3D点云有损属性压缩方法，通过端到端可微分的投影操作和数据驱动的优化技术实现了高效的点云属性压缩。&lt;h4&gt;背景&lt;/h4&gt;研究基于已编码的3D点云几何信息，在解码器端进行属性压缩处理。&lt;h4&gt;目的&lt;/h4&gt;解决在多分辨率B样条投影框架下的有损属性压缩问题，提高压缩效率和质量。&lt;h4&gt;方法&lt;/h4&gt;将目标连续3D属性函数投影到嵌套的B样条函数子空间中，通过变复杂度展开的率失真优化算法计算低通系数，使用ℓ1范数作为率项，并实现端到端可微分；同时采用从粗到细的预测器，以数据驱动方式优化系数调整。&lt;h4&gt;主要发现&lt;/h4&gt;通过率失真优化算法的前馈网络展开和ℓ1范数的使用，投影操作可以实现端到端可微分；从低分辨率到高分辨率的预测调整也可以通过数据驱动方式优化，提高压缩效果。&lt;h4&gt;结论&lt;/h4&gt;所提出的多分辨率B样条投影框架结合数据驱动的优化方法，能够有效实现3D点云属性的高效压缩。&lt;h4&gt;翻译&lt;/h4&gt;在解码器端可用的已编码3D点云几何信息基础上，我们研究了多分辨率B样条投影框架下的有损属性压缩问题。首先将目标连续3D属性函数投影到一系列嵌套的子空间中，其中每个子空间由选定尺度的B样条基函数及其整数平移张成。通过将率失真优化算法展开为变复杂度的前馈网络来计算投影的低通系数，其中率项是促进稀疏的ℓ1范数。因此，投影操作是端到端可微分的。对于选定的从粗到细的预测器，然后调整系数以考虑从低分辨率到高分辨率的预测，这种预测也是以数据驱动方式优化的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given encoded 3D point cloud geometry available at the decoder, we study theproblem of lossy attribute compression in a multi-resolution B-splineprojection framework. A target continuous 3D attribute function is firstprojected onto a sequence of nested subspaces $\mathcal{F}^{(p)}_{l_0}\subseteq \cdots \subseteq \mathcal{F}^{(p)}_{L}$, where$\mathcal{F}^{(p)}_{l}$ is a family of functions spanned by a B-spline basisfunction of order $p$ at a chosen scale and its integer shifts. The projectedlow-pass coefficients $F_l^*$ are computed by variable-complexity unrolling ofa rate-distortion (RD) optimization algorithm into a feed-forward network,where the rate term is the sparsity-promoting $\ell_1$-norm. Thus, theprojection operation is end-to-end differentiable. For a chosen coarse-to-finepredictor, the coefficients are then adjusted to account for the predictionfrom a lower-resolution to a higher-resolution, which is also optimized in adata-driven manner.</description>
      <author>example@mail.com (Tam Thuc Do, Philip A. Chou, Gene Cheung)</author>
      <guid isPermaLink="false">2509.08685v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>Generalized Zero-Shot Learning for Point Cloud Segmentation with Evidence-Based Dynamic Calibration</title>
      <link>http://arxiv.org/abs/2509.08280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 12 figures, AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为E3DPC-GZSL的新方法，用于解决3D点云广义零样本语义分割中的过度自信预测问题，通过基于证据的不确定性估计器和改进的训练策略实现了对已见和未见类别的有效分类。&lt;h4&gt;背景&lt;/h4&gt;3D点云的广义零样本语义分割旨在将每个点分类为已见和未见类别，但这些模型存在显著挑战：它们倾向于做出有偏见的预测，偏向于训练过程中遇到的类别。在3D应用中，这一问题更为突出，因为训练数据规模通常小于基于图像的任务。&lt;h4&gt;目的&lt;/h4&gt;解决模型对已见类别的过度自信预测问题，且不依赖单独的已见和未见数据分类器。&lt;h4&gt;方法&lt;/h4&gt;提出E3DPC-GZSL方法，将基于证据的不确定性估计器集成到分类器中，使用动态校准堆叠因子调整预测概率，并引入新的训练策略，通过合并可学习参数和文本派生特征来改进语义空间，提高不确定性估计和模型对未见数据的优化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在ScanNet v2和S3DIS等广义零样本语义分割数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;E3DPC-GZSL方法有效解决了3D点云广义零样本语义分割中的过度自信问题，通过基于证据的不确定性估计和改进的训练策略，显著提高了对未见类别的分割性能。&lt;h4&gt;翻译&lt;/h4&gt;3D点云的广义零样本语义分割旨在将每个点分类为已见和未见类别。这些模型的一个显著挑战是它们倾向于做出有偏见的预测，通常偏向于训练过程中遇到的类别。在3D应用中，这一问题更为突出，因为训练数据的规模通常小于基于图像的任务。为解决这个问题，我们提出了一种名为E3DPC-GZSL的新方法，它减少了对已见类别的过度自信预测，而不依赖单独的已见和未见数据分类器。E3DPC-GZSL通过将基于证据的不确定性估计器集成到分类器中来解决过度自信问题。然后使用考虑逐点预测不确定性的动态校准堆叠因子来调整预测概率。此外，E3DPC-GZSL引入了一种新的训练策略，通过合并可学习参数和文本派生特征来改进语义空间，从而提高对未见数据的不确定性估计和模型优化。大量实验证明，所提出的方法在ScanNet v2和S3DIS等广义零样本语义分割数据集上达到了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决三维点云分割中的广义零样本学习(GZSL)问题，特别是模型对已见类别(训练时见过的类别)的过度自信预测偏差问题。这个问题在现实中很重要，因为自动驾驶、医疗成像等高风险应用需要精确的分割来确保安全和可靠性；现实世界场景可能包含训练时未遇到的类别，模型需要能泛化到这些新类别；三维点云数据标注成本高，难以覆盖所有可能的类别；现有方法过度依赖超参数且难以一致地应用于所有输入数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有GZSL方法的局限性，特别是对已见类别的过度自信预测问题，设计了基于证据理论的方法。他们评估了二元分类和校准堆叠两种主流方法，发现它们都严重依赖超参数且难以一致应用。作者借鉴了证据理论和主观逻辑中的不确定性估计方法，改进了校准堆叠的基本思想，使其能够动态调整而非使用固定超参数。此外，他们借鉴了语义空间对齐的思路，但引入了场景语义调优来增强特征表示。整体设计思路是利用不确定性估计实现动态校准，并通过语义调优解决数据稀缺问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用证据理论估计预测的不确定性，并基于此动态调整已见类别的预测概率；同时通过场景语义调优改善语义空间表示，解决三维零样本学习中的数据稀缺问题。整体实现流程分为三个阶段：1)训练编码器：从头开始训练编码器E提取特征向量；2)训练解码器：训练解码器D生成合成特征，通过融合可学习的场景语义向量与文本特征增强特征表示；3)训练基于证据的分类器：训练分类器C进行分割，同时训练不确定性估计器U评估预测置信度，使用三种损失函数优化。推理阶段使用编码器提取特征，分类器预测概率，不确定性估计器计算动态校准因子，应用校准调整已见类别的预测概率。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)证据驱动的动态校准：基于证据理论实现动态校准因子，而非使用预定义的固定超参数；2)语义空间调优：将可学习的场景语义向量与文本特征融合，增强语义表示；3)多阶段训练策略：分阶段训练编码器、解码器和分类器；4)不确定性感知的损失函数：设计三种互补的损失函数，同时优化分割性能和不确定性估计；5)统一的处理框架：无需显式区分已见和未见类别，通过不确定性估计自动处理。相比之前工作，本文方法解决了超参数依赖问题，提高了模型在未见类别上的性能，并增强了特征表示能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于证据理论的三维点云广义零样本分割方法，通过动态校准预测概率和语义空间调优，有效解决了模型对已见类别的过度自信问题，显著提升了在未见类别上的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v39i4.32446&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized zero-shot semantic segmentation of 3D point clouds aims toclassify each point into both seen and unseen classes. A significant challengewith these models is their tendency to make biased predictions, often favoringthe classes encountered during training. This problem is more pronounced in 3Dapplications, where the scale of the training data is typically smaller than inimage-based tasks. To address this problem, we propose a novel method calledE3DPC-GZSL, which reduces overconfident predictions towards seen classeswithout relying on separate classifiers for seen and unseen data. E3DPC-GZSLtackles the overconfidence problem by integrating an evidence-based uncertaintyestimator into a classifier. This estimator is then used to adjust predictionprobabilities using a dynamic calibrated stacking factor that accounts forpointwise prediction uncertainty. In addition, E3DPC-GZSL introduces a noveltraining strategy that improves uncertainty estimation by refining the semanticspace. This is achieved by merging learnable parameters with text-derivedfeatures, thereby improving model optimization for unseen data. Extensiveexperiments demonstrate that the proposed approach achieves state-of-the-artperformance on generalized zero-shot semantic segmentation datasets, includingScanNet v2 and S3DIS.</description>
      <author>example@mail.com (Hyeonseok Kim, Byeongkeun Kang, Yeejin Lee)</author>
      <guid isPermaLink="false">2509.08280v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>APML: Adaptive Probabilistic Matching Loss for Robust 3D Point Cloud Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 6 figures, conference, 7 tables, 15 formulas&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应概率匹配损失(APML)的新方法，用于点云预测任务中的点集比较，解决了传统损失函数的局限性，同时保持了计算效率。&lt;h4&gt;背景&lt;/h4&gt;在点云预测任务中，常用的损失函数如Chamfer距离(CD)、HyperCD和InfoCD依赖于最近邻分配，导致多对一对应关系，引起密集区域点拥塞和稀疏区域覆盖不足，且涉及非可微分操作。虽然地球移动距离(EMD)能更好地捕获结构相似性，但其三次计算复杂度限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的损失函数，避免最近邻分配问题，解决点拥塞和覆盖不足，避免非可微分操作，保持计算效率，且不需要手动调整超参数。&lt;h4&gt;方法&lt;/h4&gt;提出自适应概率匹配损失(APML)，这是一种一对一匹配的完全可微分近似方法，利用基于成对距离导出的温度缩放相似性矩阵上的Sinkhorn迭代。通过分析计算温度来保证最小分配概率，消除手动调整的需要。&lt;h4&gt;主要发现&lt;/h4&gt;APML集成到PoinTr、PCN、FoldingNet等架构中，在ShapeNet基准测试和从WiFi CSI测量生成3D人体点云的CSI2PC模型上表现优异；实现了更快的收敛速度，特别是在低密度区域有优越的空间分布，定量性能有所提升或相当，无需额外超参数搜索。&lt;h4&gt;结论&lt;/h4&gt;APML是一种有效的点云预测任务损失函数，解决了传统方法的局限性，同时保持了计算效率，在各种架构和任务上都表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;用于点云预测任务（如形状补全和生成）的深度学习模型的训练，严重依赖于衡量预测点集与真实点集之间差异的损失函数。常用的函数如Chamfer距离(CD)、HyperCD和InfoCD依赖于最近邻分配，这通常导致多对一对应关系，引起密集区域的点拥塞和稀疏区域的覆盖不足。这些损失还涉及由于索引选择导致的非可微分操作，可能影响基于梯度的优化。地球移动距离(EMD)强制一对一对应并能更有效地捕获结构相似性，但其三次计算复杂度限制了其实际应用。我们提出了自适应概率匹配损失（APML），这是一种一对一匹配的完全可微分近似方法，它利用基于成对距离导出的温度缩放相似性矩阵上的Sinkhorn迭代。我们通过分析计算温度来保证最小分配概率，消除了手动调整的需要。APML实现了接近二次的运行时间，与基于Chamfer的损失相当，并避免了非可微分操作。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云重建任务中损失函数的设计问题。现有的常用损失函数如Chamfer Distance存在点聚集、稀疏区域覆盖不佳和非可微等问题，而Earth Mover's Distance虽然几何保真度高但计算复杂度太高。这个问题很重要，因为点云是三维数据的主要表示形式，广泛应用于自动驾驶、机器人、AR/VR等领域，高质量的点云重建对这些应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有损失函数的局限性：Chamfer Distance效率高但几何细节捕捉不足，EMD几何保真度高但计算成本太高。他们借鉴了最优传输理论，特别是熵正则化的Sinkhorn算法，该算法提供了可微的软对应关系。关键创新是解决了Sinkhorn算法需要手动调整正则化参数的问题，通过分析计算自适应温度来保证最小分配概率，从而消除了手动调整的需要。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是应用最优传输原理建立点集间的软概率对应关系，通过自适应温度机制控制分配锐度，无需手动调整。整体流程包括：1)计算预测点集与真实点集间的欧氏距离矩阵；2)应用自适应Softmax生成初始概率分配；3)双向匹配并对称化得到初始软分配矩阵；4)通过Sinkhorn归一化使矩阵近似双随机；5)计算期望匹配成本作为最终损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)自适应概率匹配机制，完全可微且近似一对一匹配；2)自适应温度选择机制，通过闭式调度自动调整分配锐度；3)平衡计算效率与几何保真度，以近二次复杂度实现EMD级别的质量。相比之前的工作，APML避免了Chamfer Distance的点聚集问题，解决了EMD的高计算复杂度问题，并且不需要像其他CD变体那样手动调整多个参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; APML通过结合最优传输理论和自适应温度机制，提供了一种高效、可微的损失函数，显著提升了3D点云重建任务中的几何保真度，同时保持了与Chamfer Distance相当的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training deep learning models for point cloud prediction tasks such as shapecompletion and generation depends critically on loss functions that measurediscrepancies between predicted and ground-truth point sets. Commonly usedfunctions such as Chamfer Distance (CD), HyperCD, and InfoCD rely onnearest-neighbor assignments, which often induce many-to-one correspondences,leading to point congestion in dense regions and poor coverage in sparseregions. These losses also involve non-differentiable operations due to indexselection, which may affect gradient-based optimization. Earth Mover Distance(EMD) enforces one-to-one correspondences and captures structural similaritymore effectively, but its cubic computational complexity limits its practicaluse. We propose the Adaptive Probabilistic Matching Loss (APML), a fullydifferentiable approximation of one-to-one matching that leverages Sinkhorniterations on a temperature-scaled similarity matrix derived from pairwisedistances. We analytically compute the temperature to guarantee a minimumassignment probability, eliminating manual tuning. APML achieves near-quadraticruntime, comparable to Chamfer-based losses, and avoids non-differentiableoperations. When integrated into state-of-the-art architectures (PoinTr, PCN,FoldingNet) on ShapeNet benchmarks and on a spatiotemporal Transformer (CSI2PC)that generates 3D human point clouds from WiFi CSI measurements, APM lossyields faster convergence, superior spatial distribution, especially inlow-density regions, and improved or on-par quantitative performance withoutadditional hyperparameter search. The code is available at:https://github.com/apm-loss/apml.</description>
      <author>example@mail.com (Sasan Sharifipour, Constantino Álvarez Casado, Mohammad Sabokrou, Miguel Bordallo López)</author>
      <guid isPermaLink="false">2509.08104v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:13 +0800</pubDate>
    </item>
    <item>
      <title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
      <link>http://arxiv.org/abs/2509.07209v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ASME IDETC/CIE 2025 (DETC2025-168977). Dataset  availability: BlendedNet dataset is openly available at Harvard Dataverse  (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VJT9EP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlendedNet是一个包含999个混合翼身(BWB)几何形状的公开空气动力学数据集，每个几何形状在约9种飞行条件下进行模拟，生成8830个RANS案例。研究团队还提出了一个端到端代理框架用于点对点空气动力学预测，实验显示在不同BWB上的表面预测误差较低。&lt;h4&gt;背景&lt;/h4&gt;非常规空气动力学配置面临数据稀缺问题，传统方法难以有效处理。&lt;h4&gt;目的&lt;/h4&gt;创建一个大型公开数据集解决非常规配置的数据稀缺问题，并开发一个数据驱动的代理模型框架用于空气动力学设计。&lt;h4&gt;方法&lt;/h4&gt;通过采样几何设计参数和飞行条件生成数据集；使用排列不变的PointNet回归器从表面点云预测几何参数，然后在预测参数和飞行条件下调节特征线性调制(FiLM)网络，预测点对点系数Cp、Cfx和Cfz。&lt;h4&gt;主要发现&lt;/h4&gt;BlendedNet数据集包含丰富的表面量信息，可用于研究升力和阻力；提出的端到端代理框架在不同BWB上的表面预测表现出低误差。&lt;h4&gt;结论&lt;/h4&gt;BlendedNet解决了非常规空气动力学配置的数据稀缺问题，并促进了数据驱动代理模型在空气动力学设计研究中的应用。&lt;h4&gt;翻译&lt;/h4&gt;BlendedNet是一个公开可用的空气动力学数据集，包含999个混合翼身(BWB)几何形状。每个几何形状在约九种飞行条件下进行模拟，产生了8830个使用Spalart-Allmaras模型的收敛RANS案例，每个案例有900万至1400万个单元。该数据集通过采样几何设计参数和飞行条件生成，包括研究升力和阻力所需的详细点对点表面量。我们还介绍了一个用于点对点空气动力学预测的端到端代理框架。该流程首先使用排列不变的PointNet回归器从采样表面点云预测几何参数，然后在预测的参数和飞行条件下调节特征线性调制(FiLM)网络，以预测点对点系数Cp、Cfx和Cfz。实验表明，在不同BWB上的表面预测误差较低。BlendedNet解决了非常规配置的数据稀缺问题，并促进了空气动力学设计的数据驱动代理模型研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; BlendedNet is a publicly available aerodynamic dataset of 999 blended wingbody (BWB) geometries. Each geometry is simulated across about nine flightconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras modeland 9 to 14 million cells per case. The dataset is generated by samplinggeometric design parameters and flight conditions, and includes detailedpointwise surface quantities needed to study lift and drag. We also introducean end-to-end surrogate framework for pointwise aerodynamic prediction. Thepipeline first uses a permutation-invariant PointNet regressor to predictgeometric parameters from sampled surface point clouds, then conditions aFeature-wise Linear Modulation (FiLM) network on the predicted parameters andflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.Experiments show low errors in surface predictions across diverse BWBs.BlendedNet addresses data scarcity for unconventional configurations andenables research on data-driven surrogate modeling for aerodynamic design.</description>
      <author>example@mail.com (Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.07209v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>3D and 4D World Modeling: A Survey</title>
      <link>http://arxiv.org/abs/2509.07996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Survey; 34 pages, 10 figures, 14 tables; GitHub Repo at  https://github.com/worldbench/survey&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是首个专门针对3D和4D世界建模与生成的全面综述，建立了精确的定义和结构化分类法，总结了相关数据集和评估指标，并讨论了应用、挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;世界建模已成为AI研究的基石，但先前研究主要关注2D图像和视频数据的生成方法，忽视了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的工作。同时，'世界模型'缺乏标准化定义和分类法，导致文献中碎片化且有时不一致的主张。&lt;h4&gt;目的&lt;/h4&gt;提出首个专门针对3D和4D世界建模与生成的全面综述，建立精确的定义，引入结构化分类法，系统总结适用于3D/4D设置的数据集和评估指标，讨论实际应用，确定开放挑战，突出有希望的研究方向，为推进该领域提供连贯且基础的参考。&lt;h4&gt;方法&lt;/h4&gt;提出基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）的方法的分类法，并系统总结现有文献，提供可访问的文献汇编资源。&lt;h4&gt;主要发现&lt;/h4&gt;需要更多关注3D和4D表示方法；建立了世界模型的精确定义和分类框架；总结了适用于3D/4D设置的数据集和评估指标；识别了实际应用、开放挑战和有希望的研究方向。&lt;h4&gt;结论&lt;/h4&gt;该综述为3D和4D世界建模领域提供了连贯且基础的参考，有助于推进该领域的发展，相关文献系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;翻译&lt;/h4&gt;世界建模已成为AI研究的基石，使智能体能够理解、表示和预测他们所处的动态环境。虽然先前的工作主要强调2D图像和视频数据的生成方法，但他们忽视了利用原生3D和4D表示（如RGB-D图像、占用栅格和LiDAR点云）进行大规模场景建模的快速增长的研究。同时，'世界模型'缺乏标准化定义和分类法，导致文献中碎片化且有时不一致的主张。本综述通过提出首个专门针对3D和4D世界建模与生成的全面综述来解决这些差距。我们建立了精确的定义，引入了涵盖基于视频（VideoGen）、基于占用（OccGen）和基于LiDAR（LiDARGen）方法的分类法，并系统总结了适用于3D/4D设置的数据集和评估指标。我们进一步讨论了实际应用，确定了开放挑战，并突出了有希望的研究方向，旨在为推进该领域提供连贯且基础的参考。现有文献的系统总结可在https://github.com/worldbench/survey获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决世界建模领域缺乏标准化定义和分类体系的问题，以及现有工作过度关注2D数据而忽视原生3D/4D表示的局限性。这个问题很重要，因为世界建模是AI研究的基石，能帮助智能体理解、表示和预测动态环境；原生3D/4D表示（如RGB-D、占用网格、LiDAR点云）提供显式几何和物理基础，对自动驾驶、机器人等安全关键系统至关重要；缺乏统一框架导致文献碎片化，阻碍领域发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作为综述文章，作者首先明确'世界模型'和'3D/4D世界建模'的精确定义，然后提出分层分类法，根据表示模态（VideoGen、OccGen和LiDARGen）对现有方法进行分类，并系统总结专门针对3D/4D场景的数据集和评估指标。作者借鉴了多种生成模型（VAEs、GANs、扩散模型、自回归模型）作为算法基础，整合了计算机视觉、机器人和人工智能领域的相关工作，但专注于3D/4D表示而非2D数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立精确的'世界模型'定义，提出结构化分类法涵盖VideoGen、OccGen和LiDARGen方法，系统总结3D/4D场景的数据集和评估指标。整体流程是：第2节介绍基本概念和定义；第3节详细介绍分层分类法；第4节总结数据集和评估指标；第5节回顾实际应用；第6节讨论挑战和未来方向。这种结构化组织方式帮助读者系统理解3D/4D世界建模的全貌。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次专门针对3D/4D世界建模进行全面综述；建立精确的'世界模型'定义；提出涵盖VideoGen、OccGen和LiDARGen的结构化分类法；提供3D/4D场景数据集和评估指标的全面覆盖。相比之前工作，这篇综述明确聚焦原生3D/4D表示而非2D数据，提供更系统化的分类和更全面的文献覆盖，填补了现有文献的知识空白。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过建立精确的定义、提出结构化分类法、系统总结数据集和评估指标，为3D和4D世界建模领域提供了首个全面且基础的参考框架，填补了现有文献的空白并指明了未来研究方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; World modeling has become a cornerstone in AI research, enabling agents tounderstand, represent, and predict the dynamic environments they inhabit. Whileprior work largely emphasizes generative methods for 2D image and video data,they overlook the rapidly growing body of work that leverages native 3D and 4Drepresentations such as RGB-D imagery, occupancy grids, and LiDAR point cloudsfor large-scale scene modeling. At the same time, the absence of a standardizeddefinition and taxonomy for ``world models'' has led to fragmented andsometimes inconsistent claims in the literature. This survey addresses thesegaps by presenting the first comprehensive review explicitly dedicated to 3Dand 4D world modeling and generation. We establish precise definitions,introduce a structured taxonomy spanning video-based (VideoGen),occupancy-based (OccGen), and LiDAR-based (LiDARGen) approaches, andsystematically summarize datasets and evaluation metrics tailored to 3D/4Dsettings. We further discuss practical applications, identify open challenges,and highlight promising research directions, aiming to provide a coherent andfoundational reference for advancing the field. A systematic summary ofexisting literature is available at https://github.com/worldbench/survey</description>
      <author>example@mail.com (Lingdong Kong, Wesley Yang, Jianbiao Mei, Youquan Liu, Ao Liang, Dekai Zhu, Dongyue Lu, Wei Yin, Xiaotao Hu, Mingkai Jia, Junyuan Deng, Kaiwen Zhang, Yang Wu, Tianyi Yan, Shenyuan Gao, Song Wang, Linfeng Li, Liang Pan, Yong Liu, Jianke Zhu, Wei Tsang Ooi, Steven C. H. Hoi, Ziwei Liu)</author>
      <guid isPermaLink="false">2509.07996v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>BitROM: Weight Reload-Free CiROM Architecture Towards Billion-Parameter 1.58-bit LLM Inference</title>
      <link>http://arxiv.org/abs/2509.08542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ASP-DAC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BitROM是一种基于CiROM的新型加速器，通过与BitNet的1.58位量化模型协同设计，解决了传统CiROM加速器在扩展到大型语言模型时面临的硅面积限制问题，实现了边缘设备上的高效LLM推理。&lt;h4&gt;背景&lt;/h4&gt;CiROM加速器通过消除运行时权重更新为CNN提供卓越能效，但在扩展到大型语言模型时受到参数量巨大的根本限制。即使是最小的LLaMA-7B模型在先进CMOS节点中也需要超过1,000 cm²的硅面积。&lt;h4&gt;目的&lt;/h4&gt;开发首个基于CiROM的加速器BitROM，通过协同设计克服LLM部署的面积限制，实现边缘设备上实用高效的LLM推理。&lt;h4&gt;方法&lt;/h4&gt;BitROM引入三种创新：1)双向ROM阵列，每个晶体管存储两个三进制权重；2)针对三进制权重计算优化的三模式局部累加器；3)集成解码刷新(DR) eDRAM支持片上KV缓存管理。此外还集成了LoRA适配器实现高效迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;在65nm CMOS中评估，BitROM实现20.8 TOPS/W能效和4,967 kB/mm²比特密度，比先前的数字CiROM设计在面积效率上提高10倍。DR eDRAM减少43.6%的外部DRAM访问。&lt;h4&gt;结论&lt;/h4&gt;BitROM成功解决了CiROM加速器扩展到LLMs的硅面积限制问题，通过创新设计实现了边缘设备上的高效LLM推理，为边缘计算中的LLM部署提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;只读存储器计算(CiROM)加速器通过消除运行时权重更新为CNN提供了卓越的能效。然而，它们扩展到大型语言模型(LLMs)受到参数量巨大的根本限制。值得注意的是，即使是在先进的CMOS节点中，LLaMA系列中最小的模型LLaMA-7B也需要超过1,000 cm²的硅面积。本文提出了BitROM，这是首个基于CiROM的加速器，通过与BitNet的1.58位量化模型协同设计克服了这一限制，实现了边缘设备上实用高效的LLM推理。BitROM引入了三个关键创新：1)每个晶体管存储两个三进制权重的双向ROM阵列；2)针对三进制权重计算优化的三模式局部累加器；3)支持片上KV缓存管理的集成解码刷新(DR) eDRAM，显著减少了解码期间的外部内存访问。此外，BitROM集成了基于LoRA的适配器，实现各种下游任务的高效迁移学习。在65nm CMOS中评估，BitROM实现了20.8 TOPS/W和4,967 kB/mm²的比特密度，比先前的数字CiROM设计在面积效率上提高了10倍。此外，DR eDRAM减少了43.6%的外部DRAM访问，进一步增强了边缘应用中LLM的部署效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compute-in-Read-Only-Memory (CiROM) accelerators offer outstanding energyefficiency for CNNs by eliminating runtime weight updates. However, theirscalability to Large Language Models (LLMs) is fundamentally constrained bytheir vast parameter sizes. Notably, LLaMA-7B - the smallest model in LLaMAseries - demands more than 1,000 cm2 of silicon area even in advanced CMOSnodes. This paper presents BitROM, the first CiROM-based accelerator thatovercomes this limitation through co-design with BitNet's 1.58-bit quantizationmodel, enabling practical and efficient LLM inference at the edge. BitROMintroduces three key innovations: 1) a novel Bidirectional ROM Array thatstores two ternary weights per transistor; 2) a Tri-Mode Local Accumulatoroptimized for ternary-weight computations; and 3) an integrated Decode-Refresh(DR) eDRAM that supports on-die KV-cache management, significantly reducingexternal memory access during decoding. In addition, BitROM integratesLoRA-based adapters to enable efficient transfer learning across variousdownstream tasks. Evaluated in 65nm CMOS, BitROM achieves 20.8 TOPS/W and a bitdensity of 4,967 kB/mm2 - offering a 10x improvement in area efficiency overprior digital CiROM designs. Moreover, the DR eDRAM contributes to a 43.6%reduction in external DRAM access, further enhancing deployment efficiency forLLMs in edge applications.</description>
      <author>example@mail.com (Wenlun Zhang, Xinyu Li, Shimpei Ando, Kentaro Yoshioka)</author>
      <guid isPermaLink="false">2509.08542v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Label Transfer Learning in Non-Stationary Data Streams</title>
      <link>http://arxiv.org/abs/2509.08181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE International Conference on Data Mining (ICDM) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对多标签数据流中的概念漂移问题，提出了两种新的迁移学习方法，通过标签间知识转移提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;在非平稳环境中，多标签数据流中的标签概念可能会发生漂移，这种漂移可能是独立发生的，也可能是与其他标签相关的。尽管在相关标签之间转移知识可以加速适应，但针对数据流的多标签迁移学习研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出两种新的迁移学习方法，解决多标签数据流中的概念漂移问题，通过标签间知识转移提高预测性能。&lt;h4&gt;方法&lt;/h4&gt;BR-MARLENE：利用源数据流和目标数据流中不同标签的知识进行多标签分类；BRPW-MARLENE：在此基础上，通过显式建模和转移成对的标签依赖关系来增强学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;两种方法在非平稳环境中都优于最先进的多标签流方法，标签间知识转移对于提高预测性能是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的方法通过标签间知识转移有效解决了多标签数据流中的概念漂移问题，在非平稳环境中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;在非平稳环境中，多标签数据流中的标签概念往往会发生漂移，无论是独立发生还是与其他标签相关。在相关标签之间转移知识可以加速适应，然而针对数据流的多标签迁移学习研究仍然有限。为此，我们提出了两种新颖的迁移学习方法：BR-MARLENE利用源数据流和目标数据流中不同标签的知识进行多标签分类；BRPW-MARLENE在此基础上通过显式建模和转移成对的标签依赖关系来增强学习性能。全面的实验表明，两种方法在非平稳环境中都优于最先进的多标签流方法，证明了标签间知识转移对于提高预测性能的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Label concepts in multi-label data streams often experience drift innon-stationary environments, either independently or in relation to otherlabels. Transferring knowledge between related labels can accelerateadaptation, yet research on multi-label transfer learning for data streamsremains limited. To address this, we propose two novel transfer learningmethods: BR-MARLENE leverages knowledge from different labels in both sourceand target streams for multi-label classification; BRPW-MARLENE builds on thisby explicitly modelling and transferring pairwise label dependencies to enhancelearning performance. Comprehensive experiments show that both methodsoutperform state-of-the-art multi-label stream approaches in non-stationaryenvironments, demonstrating the effectiveness of inter-label knowledge transferfor improved predictive performance.</description>
      <author>example@mail.com (Honghui Du, Leandro Minku, Aonghus Lawlor, Huiyu Zhou)</author>
      <guid isPermaLink="false">2509.08181v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MARLINE: Multi-Source Mapping Transfer Learning for Non-Stationary Environments</title>
      <link>http://arxiv.org/abs/2509.08176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the 2020 IEEE International Conference on Data Mining  (ICDM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MARLINE的新方法，用于处理非平稳环境中的概念漂移问题，即使在源概念和目标概念不匹配的情况下也能从多个数据源中获益。&lt;h4&gt;背景&lt;/h4&gt;概念漂移是影响数据流挖掘系统预测性能的主要问题。现有方法假设至少有一个源模型与目标概念相似，这在许多现实场景中不成立。&lt;h4&gt;目的&lt;/h4&gt;开发一种即使源概念和目标概念不匹配也能从多个数据源知识中受益的方法。&lt;h4&gt;方法&lt;/h4&gt;提出MARLINE（Multi-source mApping with tRansfer LearnIng for Non-stationary Environments）方法，通过将目标概念投影到每个源概念空间，使多个源子分类器作为集成的一部分为目标概念预测做出贡献。&lt;h4&gt;主要发现&lt;/h4&gt;在多个合成和真实世界数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。&lt;h4&gt;结论&lt;/h4&gt;MARLINE能够有效处理非平稳环境中的概念漂移问题，即使源概念和目标概念不匹配也能提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;概念漂移是在线学习中的一个主要问题，因为它影响数据流挖掘系统的预测性能。最近的研究开始探索来自不同数据源的数据流作为处理目标领域中概念漂移的策略。这些方法假设至少有一个源模型代表与目标概念相似的概念，这在许多现实场景中可能不成立。在本文中，我们提出了一种名为MARLINE（Multi-source mApping with tRansfer LearnIng for Non-stationary Environments）的新方法。MARLINE即使在源概念和目标概念不匹配的情况下，也能从非平稳环境中的多个数据源知识中受益。这是通过将目标概念投影到每个源概念的空间实现的，使多个源子分类器能够作为集成的一部分为目标概念的预测做出贡献。在几个合成和真实世界数据集上的实验表明，MARLINE比几种最先进的数据流学习方法更准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDM50108.2020.00021&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept drift is a major problem in online learning due to its impact on thepredictive performance of data stream mining systems. Recent studies havestarted exploring data streams from different sources as a strategy to tackleconcept drift in a given target domain. These approaches make the assumptionthat at least one of the source models represents a concept similar to thetarget concept, which may not hold in many real-world scenarios. In this paper,we propose a novel approach called Multi-source mApping with tRansfer LearnIngfor Non-stationary Environments (MARLINE). MARLINE can benefit from knowledgefrom multiple data sources in non-stationary environments even when source andtarget concepts do not match. This is achieved by projecting the target conceptto the space of each source concept, enabling multiple source sub-classifiersto contribute towards the prediction of the target concept as part of anensemble. Experiments on several synthetic and real-world datasets show thatMARLINE was more accurate than several state-of-the-art data stream learningapproaches.</description>
      <author>example@mail.com (Honghui Du, Leandro Minku, Huiyu Zhou)</author>
      <guid isPermaLink="false">2509.08176v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Two-Stage Swarm Intelligence Ensemble Deep Transfer Learning (SI-EDTL) for Vehicle Detection Using Unmanned Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2509.08026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SI-EDTL的两阶段群体智能集成深度迁移学习模型，用于无人机图像中的多车辆检测。该模型结合了多种预训练特征提取器和分类器，并通过加权平均进行聚合，在AU-AIR无人机数据集上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;随着无人机技术的发展，无人机图像中的车辆检测变得越来越重要。然而，在无人机图像中准确检测多种类型的车辆仍然是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效准确的车辆检测模型，能够在无人机图像中识别多种类型的车辆（汽车、面包车、卡车、公共汽车等）。&lt;h4&gt;方法&lt;/h4&gt;1. 采用两阶段群体智能集成深度迁移学习框架；2. 结合三个预训练的Faster R-CNN特征提取器模型（InceptionV3、ResNet50、GoogLeNet）；3. 集成五个迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯），形成15个基础学习器；4. 使用加权平均方法聚合这些学习器的结果；5. 应用鲸鱼优化算法对超参数进行优化，以平衡准确性、精确度和召回率；6. 在MATLAB R2020b中实现，并采用并行处理技术。&lt;h4&gt;主要发现&lt;/h4&gt;SI-EDTL模型在AU-AIR无人机数据集上表现优于现有的车辆检测方法，能够准确识别多种类型的车辆。&lt;h4&gt;结论&lt;/h4&gt;SI-EDTL模型通过结合多种深度学习技术和集成学习方法，有效地提高了无人机图像中多车辆检测的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了SI-EDTL，一种用于检测无人机图像中多辆车辆的两阶段群体智能集成深度迁移学习模型。它将三个预训练的Faster R-CNN特征提取器模型（InceptionV3、ResNet50、GoogLeNet）与五个迁移分类器（KNN、SVM、MLP、C4.5、朴素贝叶斯）相结合，形成15个不同的基础学习器。这些学习器通过加权平均进行聚合，将区域分类为汽车、面包车、卡车、公共汽车或背景。使用鲸鱼优化算法对超参数进行优化，以平衡准确性、精确度和召回率。该模型在MATLAB R2020b中实现并使用并行处理，在AU-AIR无人机数据集上优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/cpe.6726&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SI-EDTL, a two-stage swarm intelligence ensemble deeptransfer learning model for detecting multiple vehicles in UAV images. Itcombines three pre-trained Faster R-CNN feature extractor models (InceptionV3,ResNet50, GoogLeNet) with five transfer classifiers (KNN, SVM, MLP, C4.5,Na\"ive Bayes), resulting in 15 different base learners. These are aggregatedvia weighted averaging to classify regions as Car, Van, Truck, Bus, orbackground. Hyperparameters are optimized with the whale optimization algorithmto balance accuracy, precision, and recall. Implemented in MATLAB R2020b withparallel processing, SI-EDTL outperforms existing methods on the AU-AIR UAVdataset.</description>
      <author>example@mail.com (Zeinab Ghasemi Darehnaei, Mohammad Shokouhifar, Hossein Yazdanjouei, S. M. J. Rastegar Fatemi)</author>
      <guid isPermaLink="false">2509.08026v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>http://arxiv.org/abs/2509.07627v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了一种名为LSMTCR的新型框架，用于从头生成全长、表位特异性的T细胞受体（TCR）αβ链。该框架通过分离特异性和约束学习，实现了基于表位条件的配对全长TCR生成。&lt;h4&gt;背景&lt;/h4&gt;设计全长、表位特异性的TCR αβ链具有挑战性，原因包括巨大的序列空间、数据偏差以及免疫遗传约束的不完整建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的多架构框架，能够从头生成基于表位条件的配对全长TCR。&lt;h4&gt;方法&lt;/h4&gt;LSMTCR采用三个主要组件：1) 扩散增强的BERT编码器学习时间条件的表位表示；2) 条件GPT解码器在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性CDR3，并通过温度控制多样性；3) 基因感知的Transformer通过预测V/J使用来组装完整的αβ序列，确保免疫遗传保真度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上，LSMTCR实现了比基线更高的预测结合能力，更忠实地恢复了位置和长度语法，提供了优越且可通过温度调节的多样性。迁移学习显著改善了α链生成的预测结合、长度真实性和多样性。从已知或从头生成的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低。在配对αβ共建模中获得了更高的pTM/ipTM值。&lt;h4&gt;结论&lt;/h4&gt;LSMTCR仅从表位输入就能输出多样化、基因背景化的全长TCR设计，支持高通量筛选和迭代优化。&lt;h4&gt;翻译&lt;/h4&gt;设计全长、表位特异性的TCR αβ链仍然具有挑战性，这是由于巨大的序列空间、数据偏差以及免疫遗传约束的不完整建模。我们提出了LSMTCR，一种可扩展的多架构框架，该框架分离了特异性和约束学习，从而能够从头、基于表位条件地生成配对的全长TCR。扩散增强的BERT编码器学习时间条件的表位表示；条件GPT解码器在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性的CDR3，并通过温度控制的多样性；基因感知的Transformer通过预测V/J使用来组装完整的αβ序列，确保免疫遗传保真度。在GLIPH、TEP、MIRA、McPAS和我们整理的数据集上，LSMTCR在大多数数据集上实现了比基线更高的预测结合，更忠实地恢复了位置和长度语法，并提供了优越的、可通过温度调节的多样性。对于α链生成，迁移学习在预测结合、长度真实性和多样性方面优于代表性方法。从已知或从头生成的CDR3进行全长组装保留了k-mer谱，产生了与参考序列的低编辑距离，并且在与表位的配对αβ共建模中，比单链设置获得了更高的pTM/ipTM。LSMTCR仅从表位输入就能输出多样化、基因背景化的全长TCR设计， enabling高通量筛选和迭代优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing full-length, epitope-specific TCR $\alpha\beta$ remains challengingdue to vast sequence space, data biases and incomplete modeling ofimmunogenetic constraints. We present LSMTCR, a scalable multi-architectureframework that separates specificity from constraint learning to enable denovo, epitope-conditioned generation of paired, full-length TCRs. Adiffusion-enhanced BERT encoder learns time-conditioned epitoperepresentations; conditional GPT decoders, pretrained on CDR3$\beta$ andtransferred to CDR3$\alpha$, generate chain-specific CDR3s under cross-modalconditioning with temperature-controlled diversity; and a gene-awareTransformer assembles complete $\alpha\beta$ sequences by predicting V/J usageto ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and ourcurated dataset, LSMTCR achieves higher predicted binding than baselines onmost datasets, more faithfully recovers positional and length grammars, anddelivers superior, temperature-tunable diversity. For $\alpha$-chaingeneration, transfer learning improves predicted binding, length realism anddiversity over representative methods. Full-length assembly from known or denovo CDR3s preserves k-mer spectra, yields low edit distances to references,and, in paired $\alpha\beta$ co-modelling with epitope, attains higher pTM/ipTMthan single-chain settings. LSMTCR outputs diverse, gene-contextualized,full-length TCR designs from epitope input alone, enabling high-throughputscreening and iterative optimization.</description>
      <author>example@mail.com (Ruihao Zhang, Xiao Liu)</author>
      <guid isPermaLink="false">2509.07627v2</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Privacy Preservation and Reducing Analysis Time with Federated Transfer Learning in Digital Twins-based Computed Tomography Scan Analysis</title>
      <link>http://arxiv.org/abs/2509.08018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为联邦迁移学习（FTL）的新方法，结合数字孪生技术和联邦学习应用于CT扫描分析。FTL利用预训练模型和节点间知识传输，解决了数据隐私、计算资源有限和数据异构性问题，在非独立同分布数据环境下表现优于传统联邦学习方法。&lt;h4&gt;背景&lt;/h4&gt;数字孪生（DT）技术和联邦学习（FL）在生物医学图像分析领域具有巨大潜力，特别是在CT扫描方面。然而，该领域面临数据隐私保护、计算资源有限以及数据异质性等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于数字孪生的CT扫描分析范式，即联邦迁移学习（FTL），以解决数据隐私、计算资源限制和数据异构性问题，同时实现云服务器与数字孪生CT扫描仪之间的实时协作。&lt;h4&gt;方法&lt;/h4&gt;联邦迁移学习（FTL）方法利用预训练模型和节点间的知识传输。研究团队将FTL应用于异构CT扫描数据集，并使用收敛时间、模型准确率、精确率、召回率、F1分数和混淆矩阵来评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;FTL方法在非独立同分布（non-IID）数据环境下表现优于传统联邦学习（FL）和聚类联邦学习（CFL）方法，具有更高的精确度、准确率、召回率和F1分数。&lt;h4&gt;结论&lt;/h4&gt;FTL能够改善基于数字孪生的CT扫描分析中的决策制定，为安全高效的医疗图像分析提供新可能，促进隐私保护，并为精准医疗和智能医疗系统的应用开辟新途径。&lt;h4&gt;翻译&lt;/h4&gt;数字孪生（DT）技术和联邦学习（FL）的应用对改变生物医学图像分析领域具有巨大潜力，特别是在计算机断层扫描（CT）扫描方面。本文提出了联邦迁移学习（FTL）作为一种新的基于数字孪生的CT扫描分析范式。FTL利用预训练模型和节点间的知识传输来解决数据隐私、有限计算资源和数据异构性问题。所提出的框架允许云服务器和数字孪生CT扫描仪之间的实时协作，同时保护患者身份。我们将FTL方法应用于异构CT扫描数据集，并使用收敛时间、模型准确率、精确率、召回率、F1分数和混淆矩阵评估模型性能。实验表明，与传统的联邦学习和聚类联邦学习（CFL）方法相比，FTL具有更好的精确度、准确率、召回率和F1分数。该技术在数据非独立同分布（non-IID）的环境中特别有益，为医疗诊断提供了可靠、高效和安全的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of Digital Twin (DT) technology and Federated Learning (FL)has great potential to change the field of biomedical image analysis,particularly for Computed Tomography (CT) scans. This paper presents FederatedTransfer Learning (FTL) as a new Digital Twin-based CT scan analysis paradigm.FTL uses pre-trained models and knowledge transfer between peer nodes to solveproblems such as data privacy, limited computing resources, and dataheterogeneity. The proposed framework allows real-time collaboration betweencloud servers and Digital Twin-enabled CT scanners while protecting patientidentity. We apply the FTL method to a heterogeneous CT scan dataset and assessmodel performance using convergence time, model accuracy, precision, recall, F1score, and confusion matrix. It has been shown to perform better thanconventional FL and Clustered Federated Learning (CFL) methods with betterprecision, accuracy, recall, and F1-score. The technique is beneficial insettings where the data is not independently and identically distributed(non-IID), and it offers reliable, efficient, and secure solutions for medicaldiagnosis. These findings highlight the possibility of using FTL to improvedecision-making in digital twin-based CT scan analysis, secure and efficientmedical image analysis, promote privacy, and open new possibilities forapplying precision medicine and smart healthcare systems.</description>
      <author>example@mail.com (Avais Jan, Qasim Zia, Murray Patterson)</author>
      <guid isPermaLink="false">2509.08018v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>AdsQA: Towards Advertisement Video Understanding</title>
      <link>http://arxiv.org/abs/2509.08621v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV-2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出使用广告视频作为测试平台来评估大型语言模型对视觉内容之外的理解能力，创建了AdsQA基准数据集，提出了ReAd-R模型，并在测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在通用人工智能方面取得重大进展，领域特定问题促使这些模型不断学习更深层次的专业知识。然而，收集具有高质量和意外性的数据具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;扩展知识型大型语言模型的多样化专业应用，通过广告视频这一具有挑战性的测试平台，探测模型对视觉领域客观物理内容之外的理解能力。&lt;h4&gt;方法&lt;/h4&gt;使用广告视频作为测试平台，提出ReAd-R模型(一种Deepseek-R1风格的强化学习模型)，该模型通过反思问题并通过奖励驱动的优化生成答案。创建了AdsQA基准数据集，包含1,544个广告视频、10,962个片段，总计22.7小时，提供5个挑战性任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AdsQA基准上对14个顶级大型语言模型进行测试，ReAd-R模型以明显优势超越具有长链推理能力的强大竞争对手，达到最先进水平。&lt;h4&gt;结论&lt;/h4&gt;使用广告视频作为测试平台能够有效评估大型语言模型对视觉内容之外的理解能力，ReAd-R模型在这一任务上表现出色，为未来模型在专业应用方面的发展提供了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在通用人工智能方面取得了重大进展。同时，越来越多的领域特定问题，如数学和编程，促使这些通用模型通过学习更深层次的专业知识不断进化。因此，现在是时候进一步扩展知识型大型语言模型的多样化专业应用，尽管收集具有意外性和信息量高的高质量数据具有挑战性。在本文中，我们提出使用广告视频作为具有挑战性的测试平台，来探测大型语言模型对视觉领域客观物理内容之外的理解能力。我们的动机是充分利用广告视频线索丰富和信息密集的特点，例如营销逻辑、说服策略和受众参与度。我们的贡献有三方面：(1)据我们所知，这是首次使用广告视频设计任务来评估大型语言模型。我们贡献了AdsQA，这是一个具有挑战性的广告视频问答基准，源自1,544个广告视频和10,962个片段，总计22.7小时，提供5个挑战性任务。(2)我们提出了ReAd-R，一种Deepseek-R1风格的强化学习模型，该模型反思问题并通过奖励驱动的优化生成答案。(3)我们在AdsQA上对14个顶级大型语言模型进行了基准测试，我们的ReAd-R以明显优势超越了配备长链推理能力的强大竞争对手，达到了最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have taken a great step towards AGI. Meanwhile,an increasing number of domain-specific problems such as math and programmingboost these general-purpose models to continuously evolve via learning deeperexpertise. Now is thus the time further to extend the diversity of specializedapplications for knowledgeable LLMs, though collecting high quality data withunexpected and informative tasks is challenging. In this paper, we propose touse advertisement (ad) videos as a challenging test-bed to probe the ability ofLLMs in perceiving beyond the objective physical content of common visualdomain. Our motivation is to take full advantage of the clue-rich andinformation-dense ad videos' traits, e.g., marketing logic, persuasivestrategies, and audience engagement. Our contribution is three-fold: (1) To ourknowledge, this is the first attempt to use ad videos with well-designed tasksto evaluate LLMs. We contribute AdsQA, a challenging ad Video QA benchmarkderived from 1,544 ad videos with 10,962 clips, totaling 22.7 hours, providing5 challenging tasks. (2) We propose ReAd-R, a Deepseek-R1 styled RL model thatreflects on questions, and generates answers via reward-driven optimization.(3) We benchmark 14 top-tier LLMs on AdsQA, and our \texttt{ReAd-R}~achievesthe state-of-the-art outperforming strong competitors equipped with long-chainreasoning capabilities by a clear margin.</description>
      <author>example@mail.com (Xinwei Long, Kai Tian, Peng Xu, Guoli Jia, Jingxuan Li, Sa Yang, Yihua Shao, Kaiyan Zhang, Che Jiang, Hao Xu, Yang Liu, Jiaheng Ma, Bowen Zhou)</author>
      <guid isPermaLink="false">2509.08621v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MAESTRO: Multi-modal Adaptive Ensemble for Spectro-Temporal Robust Optimization</title>
      <link>http://arxiv.org/abs/2509.08578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了MAESTRO模型，一种多模态自适应集成方法，用于流感发病率的稳健预测，通过融合监测数据、网络搜索趋势和气象数据，结合先进的频谱-时间架构，在流感预测任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要，但传统方法在处理多源数据和时间序列复杂性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够融合多模态数据并利用频谱-时间建模技术的流感预测模型，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出MAESTRO模型，包含：1)多模态数据自适应融合；2)时间序列分解为季节性和趋势成分；3)混合特征增强管道(Transformer编码器、Mamba状态空间模型、多尺度时间卷积、频域分析模块)；4)跨通道注意力机制；5)时间投影头进行序列预测；6)可选预测不确定性量化器。&lt;h4&gt;主要发现&lt;/h4&gt;在11年香港流感数据上评估，MAESTRO实现了0.956的最先进R平方值，展示了优越的模型拟合和准确性，消融实验证实了多模态融合和频谱-时间组件的关键贡献。&lt;h4&gt;结论&lt;/h4&gt;MAESTRO提供了一个模块化、可重现的统一框架，展示了先进的频谱-时间建模与多模态数据融合对稳健流行病学预测的重要协同作用，已公开可用以促进部署和扩展。&lt;h4&gt;翻译&lt;/h4&gt;及时且稳健的流感发病率预测对公共卫生决策至关重要。为此，我们提出了MAESTRO，一种用于频谱-时间稳健优化的多模态自适应集成。MAESTRO通过自适应融合多模态输入（包括监测数据、网络搜索趋势和气象数据）并利用全面的频谱-时间架构来实现稳健性。该模型首先将时间序列分解为季节性和趋势成分。然后，这些成分通过混合特征增强管道进行处理，该管道结合了基于Transformer的编码器、用于长程依赖关系的Mamba状态空间模型、多尺度时间卷积和频域分析模块。跨通道注意力机制进一步整合了不同数据模态的信息。最后，时间投影头执行序列到序列的预测，并带有可选估计器来量化预测不确定性。在超过11年的香港流感数据（排除COVID-19期间）上评估，MAESTRO显示出强大的竞争性能，展示了优越的模型拟合和相对准确性，实现了最先进的0.956的R平方值。大量消融实验证实了多模态融合和频谱-时间成分的重要贡献。我们的模块化和可重现的管道已公开可用，以促进部署和扩展到其他地区和病原体。我们公开的管道提供了一个强大、统一的框架，展示了先进的频谱-时间建模和多模态数据融合对稳健流行病学预测的关键协同作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely and robust influenza incidence forecasting is critical for publichealth decision-making. To address this, we present MAESTRO, a Multi-modalAdaptive Ensemble for Spectro-Temporal Robust Optimization. MAESTRO achievesrobustness by adaptively fusing multi-modal inputs-including surveillance, websearch trends, and meteorological data-and leveraging a comprehensivespectro-temporal architecture. The model first decomposes time series intoseasonal and trend components. These are then processed through a hybridfeature enhancement pipeline combining Transformer-based encoders, a Mambastate-space model for long-range dependencies, multi-scale temporalconvolutions, and a frequency-domain analysis module. A cross-channel attentionmechanism further integrates information across the different data modalities.Finally, a temporal projection head performs sequence-to-sequence forecasting,with an optional estimator to quantify prediction uncertainty. Evaluated onover 11 years of Hong Kong influenza data (excluding the COVID-19 period),MAESTRO shows strong competitive performance, demonstrating a superior modelfit and relative accuracy, achieving a state-of-the-art R-square of 0.956.Extensive ablations confirm the significant contributions of both multi-modalfusion and the spectro-temporal components. Our modular and reproduciblepipeline is made publicly available to facilitate deployment and extension toother regions and pathogens.Our publicly available pipeline presents apowerful, unified framework, demonstrating the critical synergy of advancedspectro-temporal modeling and multi-modal data fusion for robustepidemiological forecasting.</description>
      <author>example@mail.com (Hong Liu)</author>
      <guid isPermaLink="false">2509.08578v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>MESH -- Understanding Videos Like Human: Measuring Hallucinations in Large Video Models</title>
      <link>http://arxiv.org/abs/2509.08538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MESH是一个新基准，用于系统评估视频大型模型(LVMs)中的幻觉问题。它采用问答框架和自下而上的评估方法，与人类视频理解过程一致。研究发现LVMs在基本识别方面表现良好，但在处理复杂场景时容易产生幻觉。&lt;h4&gt;背景&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型和视觉模块的语义能力基础上，通过整合时间信息来更好地理解动态视频内容。尽管取得了进展，但LVMs容易出现幻觉问题，产生不准确或无关的描述。&lt;h4&gt;目的&lt;/h4&gt;当前视频幻觉评估基准严重依赖视频内容的手动分类，忽略了人类自然解释视频时所依赖的感知过程。需要开发一个系统评估LVMs中幻觉的基准。&lt;h4&gt;方法&lt;/h4&gt;引入MESH基准，使用问答框架，包含二进制和多选格式，并包含目标和陷阱实例。采用自下而上的方法，评估基本对象、从粗到细的主体特征以及主体-动作对，这种方法与人类视频理解过程保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;MESH为识别视频中的幻觉提供了有效且全面的方法。评估显示，LVMs在识别基本对象和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，在较长视频中，LVMs的幻觉易感性显著增加。&lt;h4&gt;结论&lt;/h4&gt;MESH基准提供了一个系统评估LVMs幻觉的方法，表明LVMs在处理复杂视频内容时仍有改进空间。&lt;h4&gt;翻译&lt;/h4&gt;大型视频模型(LVMs)建立在大型语言模型的语义能力和视觉模块的基础上，通过整合时间信息来更好地理解动态视频内容。尽管取得了进展，LVMs仍然容易出现幻觉，产生不准确或无关的描述。当前用于视频幻觉评估的基准严重依赖视频内容的手动分类，忽略了人类自然解释视频时所依赖的感知过程。我们引入了MESH，这是一个旨在系统评估LVMs中幻觉的基准。MESH使用问答框架，包含二进制和多选格式，并融入目标和陷阱实例。它采用自下而上的方法，评估基本对象、从粗到细的主体特征以及主体-动作对，与人类视频理解过程保持一致。我们证明MESH为识别视频中的幻觉提供了有效且全面的方法。我们的评估显示，虽然LVMs在识别基本对象和特征方面表现出色，但当处理精细细节或对齐涉及多个主体的多个动作时，在较长视频中，它们对幻觉的易感性显著增加。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Video Models (LVMs) build on the semantic capabilities of LargeLanguage Models (LLMs) and vision modules by integrating temporal informationto better understand dynamic video content. Despite their progress, LVMs areprone to hallucinations-producing inaccurate or irrelevant descriptions.Current benchmarks for video hallucination depend heavily on manualcategorization of video content, neglecting the perception-based processesthrough which humans naturally interpret videos. We introduce MESH, a benchmarkdesigned to evaluate hallucinations in LVMs systematically. MESH uses aQuestion-Answering framework with binary and multi-choice formats incorporatingtarget and trap instances. It follows a bottom-up approach, evaluating basicobjects, coarse-to-fine subject features, and subject-action pairs, aligningwith human video understanding. We demonstrate that MESH offers an effectiveand comprehensive approach for identifying hallucinations in videos. Ourevaluations show that while LVMs excel at recognizing basic objects andfeatures, their susceptibility to hallucinations increases markedly whenhandling fine details or aligning multiple actions involving various subjectsin longer videos.</description>
      <author>example@mail.com (Garry Yang, Zizhe Chen, Man Hon Wong, Haoyu Lei, Yongqiang Chen, Zhenguo Li, Kaiwen Zhou, James Cheng)</author>
      <guid isPermaLink="false">2509.08538v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Mamba for Hyperspectral Object Tracking</title>
      <link>http://arxiv.org/abs/2509.08265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HyMamba的新型高光谱目标跟踪网络，通过状态空间模块统一光谱、跨深度和时间建模，实现了高光谱目标跟踪的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;高光谱目标跟踪因高光谱图像丰富的光谱信息和细粒度材料区分能力而具有很大潜力，尤其在具有挑战性的场景中。现有方法通过将高光谱数据转换为伪彩色图像或结合模态融合策略取得进展，但往往无法捕捉内在的光谱信息、时间依赖性和跨深度交互。&lt;h4&gt;目的&lt;/h4&gt;解决现有高光谱跟踪器的局限性，提出一种能够有效捕捉光谱信息、时间依赖性和跨深度交互的新型高光谱目标跟踪网络。&lt;h4&gt;方法&lt;/h4&gt;通过状态空间模块(SSMs)统一光谱、跨深度和时间建模；核心是光谱状态集成(SSI)模块，实现光谱特征的渐进式细化和传播；在每个SSI中嵌入高光谱Mamba(HSM)模块，通过三个方向扫描的SSM同步学习和空间信息；基于SSI和HSM构建联合特征，并通过与原始光谱特征交互来增强这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准数据集上的广泛实验表明，HyMamba实现了最先进的性能，在HOTC2020数据集上达到73.0%的AUC分数和96.3%的DP@20分数。&lt;h4&gt;结论&lt;/h4&gt;HyMamba是一种有效的高光谱目标跟踪方法，能够有效捕捉光谱信息、时间依赖性和跨深度交互，代码将在https://github.com/lgao001/HyMamba发布。&lt;h4&gt;翻译&lt;/h4&gt;高光谱目标跟踪由于高光谱图像中丰富的光谱信息和细粒度的材料区分能力而具有巨大潜力，在具有挑战性的场景中特别有益。虽然现有的高光谱跟踪器通过将高光谱数据转换为伪彩色图像或结合模态融合策略已经取得进展，但它们往往无法捕捉内在的光谱信息、时间依赖性和跨深度交互。为解决这些局限性，本文提出了一种配备Mamba的新型高光谱目标跟踪网络(HyMamba)。它通过状态空间模块统一了光谱、跨深度和时间建模。HyMamba的核心在于光谱状态集成(SSI)模块，该模块能够通过跨深度和时间光谱信息实现光谱特征的渐进式细化和传播。嵌入在每个SSI中的是高光谱Mamba(HSM)模块，它通过三个方向扫描的SSM同步学习空间和光谱信息。基于SSI和HSM，HyMamba从伪彩色和高光谱输入构建联合特征，并通过与从原始高光谱图像提取的光谱特征交互来增强它们。在七个基准数据集上进行的广泛实验表明，HyMamba实现了最先进的性能。例如，它在HOTC2020数据集上达到了73.0%的AUC分数和96.3%的DP@20分数。代码将在https://github.com/lgao001/HyMamba发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral object tracking holds great promise due to the rich spectralinformation and fine-grained material distinctions in hyperspectral images,which are beneficial in challenging scenarios. While existing hyperspectraltrackers have made progress by either transforming hyperspectral data intofalse-color images or incorporating modality fusion strategies, they often failto capture the intrinsic spectral information, temporal dependencies, andcross-depth interactions. To address these limitations, a new hyperspectralobject tracking network equipped with Mamba (HyMamba), is proposed. It unifiesspectral, cross-depth, and temporal modeling through state space modules(SSMs). The core of HyMamba lies in the Spectral State Integration (SSI)module, which enables progressive refinement and propagation of spectralfeatures with cross-depth and temporal spectral information. Embedded withineach SSI, the Hyperspectral Mamba (HSM) module is introduced to learn spatialand spectral information synchronously via three directional scanning SSMs.Based on SSI and HSM, HyMamba constructs joint features from false-color andhyperspectral inputs, and enhances them through interaction with originalspectral features extracted from raw hyperspectral images. Extensiveexperiments conducted on seven benchmark datasets demonstrate that HyMambaachieves state-of-the-art performance. For instance, it achieves 73.0\% of theAUC score and 96.3\% of the DP@20 score on the HOTC2020 dataset. The code willbe released at https://github.com/lgao001/HyMamba.</description>
      <author>example@mail.com (Long Gao, Yunhe Zhang, Yan Jiang, Weiying Xie, Yunsong Li)</author>
      <guid isPermaLink="false">2509.08265v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Video Parallel Scaling: Aggregating Diverse Frame Subsets for VideoLLMs</title>
      <link>http://arxiv.org/abs/2509.08016v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/hyungjin-chung/VPS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VPS是一种创新的推理时方法，通过并行处理视频帧的不同子集并聚合结果，解决了VideoLLMs在处理长视频序列时面临的计算成本和性能下降问题，无需额外训练即可提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;Video Large Language Models (VideoLLMs)面临一个关键瓶颈：增加输入帧数以捕捉细粒度时间细节会导致不可接受的计算成本和长上下文长度引起的性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为Video Parallel Scaling (VPS)的推理时方法，旨在扩展模型的感知带宽而不增加其上下文窗口。&lt;h4&gt;方法&lt;/h4&gt;VPS通过运行多个并行推理流来实现，每个流处理视频帧的独特、不重叠子集。通过聚合这些互补流的输出概率，VPS集成了比单次处理更丰富的视觉信息集合。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，这种方法通过利用不相关的视觉证据，有效地收缩了Chinchilla缩放定律，从而在不增加额外训练的情况下提高了性能。在各种模型架构和规模(2B-32B)上，以及在Video-MME和EventHallusion等基准测试中的大量实验表明，VPS持续且显著地提高了性能。&lt;h4&gt;结论&lt;/h4&gt;VPS比其他并行替代方案(如Self-consistency)扩展性更好，并且与其他解码策略互补，为增强VideoLLMs的时间推理能力提供了一种内存高效且强大的框架。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(VideoLLMs)面临一个关键瓶颈：增加输入帧数以捕捉细粒度时间细节会导致不可接受的计算成本和长上下文长度引起的性能下降。我们引入了视频并行缩放(VPS)，一种推理时方法，可以在不增加模型上下文窗口的情况下扩展其感知带宽。VPS通过运行多个并行推理流来实现，每个流处理视频帧的独特、不重叠子集。通过聚合这些互补流的输出概率，VPS集成了比单次处理更丰富的视觉信息集合。我们从理论上证明，这种方法通过利用不相关的视觉证据，有效地收缩了Chinchilla缩放定律，从而在不增加额外训练的情况下提高了性能。在各种模型架构和规模(2B-32B)以及在Video-MME和EventHallusion等基准测试上的大量实验表明，VPS持续且显著地提高了性能。它的扩展性比其他并行替代方案(如Self-consistency)更好，并且与其他解码策略互补，为增强VideoLLMs的时间推理能力提供了一种内存高效且强大的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VideoLLMs) face a critical bottleneck:increasing the number of input frames to capture fine-grained temporal detailleads to prohibitive computational costs and performance degradation from longcontext lengths. We introduce Video Parallel Scaling (VPS), an inference-timemethod that expands a model's perceptual bandwidth without increasing itscontext window. VPS operates by running multiple parallel inference streams,each processing a unique, disjoint subset of the video's frames. By aggregatingthe output probabilities from these complementary streams, VPS integrates aricher set of visual information than is possible with a single pass. Wetheoretically show that this approach effectively contracts the Chinchillascaling law by leveraging uncorrelated visual evidence, thereby improvingperformance without additional training. Extensive experiments across variousmodel architectures and scales (2B-32B) on benchmarks such as Video-MME andEventHallusion demonstrate that VPS consistently and significantly improvesperformance. It scales more favorably than other parallel alternatives (e.g.Self-consistency) and is complementary to other decoding strategies, offering amemory-efficient and robust framework for enhancing the temporal reasoningcapabilities of VideoLLMs.</description>
      <author>example@mail.com (Hyungjin Chung, Hyelin Nam, Jiyeon Kim, Hyojun Go, Byeongjun Park, Junho Kim, Joonseok Lee, Seongsu Ha, Byung-Hoon Kim)</author>
      <guid isPermaLink="false">2509.08016v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>Learning Turbulent Flows with Generative Models: Super-resolution, Forecasting, and Sparse Flow Reconstruction</title>
      <link>http://arxiv.org/abs/2509.08752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究结合神经算子与生成模型，解决了传统神经算子在处理湍流结构时过度平滑的问题，实现了在三个湍流挑战任务中的高性能：时空超分辨率、预测和稀疏流重构。&lt;h4&gt;背景&lt;/h4&gt;神经算子作为动力系统的替代品很有前景，但当使用标准L2损失训练时，往往会过度平滑小尺度的湍流结构。&lt;h4&gt;目的&lt;/h4&gt;结合算子学习和生成建模来克服神经算子过度平滑湍流结构的限制，解决三个传统神经算子失败的湍流挑战：时空超分辨率、预测和稀疏流重构。&lt;h4&gt;方法&lt;/h4&gt;使用对抗训练的神经算子(adv-NO)处理Schlieren射流超分辨率和3D均匀各向同性湍流预测；使用条件生成模型从稀疏粒子跟踪测速样输入重构圆柱尾流。&lt;h4&gt;主要发现&lt;/h4&gt;对于Schlieren射流超分辨率，adv-NO将能量谱误差降低15倍，同时保留尖锐梯度；对于3D均匀各向同性湍流，adv-NO仅用160个时间步训练能准确预测五个涡旋翻转时间，推理速度比基线方法快114倍；条件生成模型能从高度稀疏输入推断出相位对齐和统计正确的完整3D速度和压力场。&lt;h4&gt;结论&lt;/h4&gt;这些进展实现了低成本下的准确重构和预测，使实验和计算流体力学中的近实时分析和控制成为可能。&lt;h4&gt;翻译&lt;/h4&gt;神经算子作为动力系统的替代品很有前景，但当使用标准L2损失训练时，往往会过度平滑小尺度的湍流结构。在这里，我们表明将算子学习与生成建模相结合可以克服这一限制。我们考虑了三个传统神经算子失败的湍流流体的实际挑战：时空超分辨率、预测和稀疏流重构。对于Schlieren射流超分辨率，对抗训练的神经算子(adv-NO)将能量谱误差降低了15倍，同时保留了尖锐梯度，且以神经算子级的推理成本。对于3D均匀各向同性湍流，adv-NO仅使用单个轨迹的160个时间步进行训练，能够准确预测五个涡旋翻转时间，比基线基于扩散的预测器快114倍，实现近实时滚动。对于从高度稀疏的粒子跟踪测速样输入重构圆柱尾流，条件生成模型推断出相位对齐和统计正确的完整3D速度和压力场。这些进展实现了低成本下的准确重构和预测，使实验和计算流体力学中的近实时分析和控制成为可能。请访问我们的项目页面：https://vivekoommen.github.io/Gen4Turb/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决神经算子在处理湍流时的频谱偏差问题，即传统神经算子倾向于过度平滑精细尺度的湍流结构。这个问题在湍流研究中非常重要，因为湍流具有多尺度特性，精细结构对理解湍流动力学至关重要。高精度湍流模拟计算成本极高，而实验中通常只能获得稀疏测量数据，因此需要高效的方法来准确模拟、预测和重建湍流场，这对实验和计算流体力学中的实时分析和控制具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有神经算子的局限性，指出它们存在频谱偏差问题，即使能表示高频的架构也可能无法学习到这些内容。作者借鉴了生成模型的工作，如生成对抗网络、扩散模型和流匹配技术，这些方法通过学习数据的完整概率结构，自然减轻了确定性算子的低频偏差。基于这些观察，作者设计了对抗性训练的神经算子（adv-NO），结合了算子学习和生成建模的优点，以克服传统方法在处理湍流时的频谱偏差问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将生成建模与神经算子学习相结合，以克服传统神经算子在处理湍流时的频谱偏差问题。整体实现流程包括：1) 使用时间条件UNet作为基础架构，结合L1损失、感知损失和对抗性损失进行训练；2) 对于超分辨率任务，从低分辨率低帧率输入映射到高分辨率高帧率输出；3) 对于预测任务，使用历史窗口预测未来状态，然后自回归地预测更远的未来；4) 对于重建任务，使用条件生成模型从稀疏观测重建完整流场，确保相位对齐和统计匹配。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 对抗性训练的神经算子（adv-NO），在保持高效计算的同时提高湍流精细结构的保真度；2) 条件生成模型用于零样本流场重建，从稀疏PTV样式的输入重建完整的3D速度和压力场；3) 在非常有限的数据下有效工作（预测仅用160个快照，重建仅用150个快照）。相比之前的工作，本文的不同之处在于：1) 提供了对神经算子频谱偏差的理论解释；2) 在计算效率与保真度之间取得了更好的平衡；3) 提出了统一的框架解决湍流中的三个不同挑战；4) 通过傅里叶分析显示adv-NO自然发展出高通滤波行为，减轻了频谱偏差。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将生成建模与神经算子学习相结合，开发出对抗性训练的神经算子，在保持高效计算的同时显著提高了湍流模拟、预测和重建中精细尺度结构的保真度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators are promising surrogates for dynamical systems but whentrained with standard L2 losses they tend to oversmooth fine-scale turbulentstructures. Here, we show that combining operator learning with generativemodeling overcomes this limitation. We consider three practical turbulent-flowchallenges where conventional neural operators fail: spatio-temporalsuper-resolution, forecasting, and sparse flow reconstruction. For Schlierenjet super-resolution, an adversarially trained neural operator (adv-NO) reducesthe energy-spectrum error by 15x while preserving sharp gradients at neuraloperator-like inference cost. For 3D homogeneous isotropic turbulence, adv-NOtrained on only 160 timesteps from a single trajectory forecasts accurately forfive eddy-turnover times and offers 114x wall-clock speed-up at inference thanthe baseline diffusion-based forecasters, enabling near-real-time rollouts. Forreconstructing cylinder wake flows from highly sparse Particle TrackingVelocimetry-like inputs, a conditional generative model infers full 3D velocityand pressure fields with correct phase alignment and statistics. These advancesenable accurate reconstruction and forecasting at low compute cost, bringingnear-real-time analysis and control within reach in experimental andcomputational fluid mechanics. See our project page:https://vivekoommen.github.io/Gen4Turb/</description>
      <author>example@mail.com (Vivek Oommen, Siavash Khodakarami, Aniruddha Bora, Zhicheng Wang, George Em Karniadakis)</author>
      <guid isPermaLink="false">2509.08752v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>InsFusion: Rethink Instance-level LiDAR-Camera Fusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.08374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了InsFusion方法，用于解决多视角相机和激光雷达三维物体检测中的误差累积问题，通过从原始和融合特征中提取提案并查询原始特征，结合注意力机制，有效减轻了误差累积的影响，并在nuScenes数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;三维物体检测是自动驾驶和智能交通的关键组成部分，但在基本特征提取、透视变换和特征融合过程中，噪声和误差会逐渐累积。&lt;h4&gt;目的&lt;/h4&gt;解决三维物体检测过程中噪声和误差逐渐累积的问题，提高检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出InsFusion方法，从原始和融合特征中提取提案，利用这些提案查询原始特征减轻累积误差影响，并结合注意力机制应用于原始特征进一步减轻误差累积。&lt;h4&gt;主要发现&lt;/h4&gt;InsFusion与各种先进的基线方法兼容，在nuScenes数据集上实现了三维物体检测的新最先进性能。&lt;h4&gt;结论&lt;/h4&gt;InsFusion通过有效的特征提取和查询机制，成功减轻了三维物体检测中的误差累积问题，提升了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;多视角相机和激光雷达的三维物体检测是自动驾驶和智能交通的关键组成部分。然而，在基本特征提取、透视变换和特征融合过程中，噪声和误差会逐渐累积。为解决这一问题，我们提出了InsFusion，它可以从原始特征和融合特征中提取提案，并利用这些提案查询原始特征，从而减轻累积误差的影响。此外，通过将注意力机制应用于原始特征，进一步减轻了累积误差的影响。在nuScenes数据集上的实验表明，InsFusion与各种先进的基线方法兼容，并在三维物体检测方面取得了新的最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D目标检测中多传感器（LiDAR和相机）融合过程中噪声和误差累积的问题。这个问题在自动驾驶和智能交通系统中至关重要，因为误差累积会导致检测精度下降，影响系统的可靠性和安全性。在特征提取、透视变换和融合的每个步骤都可能引入误差，如深度估计错误、坐标变换不准确等，这些误差会累积并最终影响整体性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有BEV（鸟瞰图）LiDAR-相机融合模型的局限性，发现误差在处理管道中逐渐累积。他们借鉴了BEVFormer系列的可变形注意力机制、IS-Fusion的实例引导融合方法以及FocalFormer3D的多阶段热图技术。但不同于这些方法专注于改进单个组件，作者设计了一种新范式，同时利用原始特征和融合特征，通过从两者中提取提案并查询原始特征来减轻误差累积，而不是试图优化管道中的特定环节。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过从原始特征和融合特征中提取提案，并利用这些提案查询原始特征，从而减轻多传感器融合过程中累积的误差。实现流程分为三步：1) 从相机特征、LiDAR特征和融合特征中提取查询；2) 通过模态特定的线性变换器将三组查询对齐到共享空间；3) 使用可变形变压器解码器，将三个核心特征源（原始相机、原始LiDAR和融合特征）作为键值对，通过注意力机制精炼实例特征。这一过程重复2次（实验确定的最佳层数），确保充分利用多源信息同时避免过度拟合局部噪声。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出通用的实例级融合范式InsFusion；2) 同时从原始特征和融合特征中提取提案；3) 使用多源查询设计而非单一特征源；4) 应用于原始特征的注意力机制；5) 与现有方法兼容且只需微调。相比之前工作，不同之处在于：传统方法主要关注改进管道中的单个组件（如减少深度估计误差），而InsFusion利用原始特征减轻噪声；传统方法依赖单一特征源，而InsFusion同时使用原始和融合特征；InsFusion具有更好的通用性，可集成到各种现有模型中，且计算开销小（FPS仅下降6.6%-9.3%）而性能提升显著（mAP提高1.0%-1.1%）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InsFusion提出了一种创新的实例级LiDAR-相机融合方法，通过同时利用原始特征和融合特征中的提案来查询原始特征，有效解决了多传感器融合过程中的误差累积问题，显著提升了3D目标检测性能，同时保持了与现有方法的兼容性和较低的计算开销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional Object Detection from multi-view cameras and LiDAR is acrucial component for autonomous driving and smart transportation. However, inthe process of basic feature extraction, perspective transformation, andfeature fusion, noise and error will gradually accumulate. To address thisissue, we propose InsFusion, which can extract proposals from both raw andfused features and utilizes these proposals to query the raw features, therebymitigating the impact of accumulated errors. Additionally, by incorporatingattention mechanisms applied to the raw features, it thereby mitigates theimpact of accumulated errors. Experiments on the nuScenes dataset demonstratethat InsFusion is compatible with various advanced baseline methods anddelivers new state-of-the-art performance for 3D object detection.</description>
      <author>example@mail.com (Zhongyu Xia, Hansong Yang, Yongtao Wang)</author>
      <guid isPermaLink="false">2509.08374v1</guid>
      <pubDate>Thu, 11 Sep 2025 14:48:14 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Generalization of Graph Anomaly Detection: From Transfer Learning to Foundation Models</title>
      <link>http://arxiv.org/abs/2509.06609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICKG 2025. 8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对图异常检测(GAD)中的泛化能力进行了全面综述，追溯了GAD中泛化能力的演变，建立了系统性分类，并对现有方法进行了评述，最后指出了开放性挑战和未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;图异常检测近年来在社交媒体、电子商务等基于图的应用中受到广泛关注，但大多数方法假设训练和测试分布相同且针对特定任务定制，导致在真实场景中适应性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有GAD方法在数据分布变化和新应用中训练样本稀少等场景下的局限性，提高GAD模型的泛化能力，并提供对GAD中泛化能力的系统性理解。&lt;h4&gt;方法&lt;/h4&gt;通过迁移学习利用相关领域知识增强检测性能，或开发'一专多能'的GAD基础模型实现跨应用泛化；追溯GAD中泛化演变，形式化问题设置，建立系统性分类，并对现有广义GAD方法进行全面回顾。&lt;h4&gt;主要发现&lt;/h4&gt;目前对GAD中泛化的系统性理解仍然缺乏；现有方法主要通过迁移学习和基础模型来提高泛化能力，以应对真实世界场景中的挑战。&lt;h4&gt;结论&lt;/h4&gt;识别了当前GAD泛化领域的开放性挑战，提出了未来研究方向，以启发这一新兴领域的后续研究。&lt;h4&gt;翻译&lt;/h4&gt;图异常检测(GAD)近年来在识别社交媒体和电子商务等多种基于图的应用中的恶意样本方面引起了越来越多的关注。然而，大多数GAD方法假设训练和测试分布相同，并且针对特定任务定制，导致在真实场景中的适应性有限，如数据分布变化和新应用中训练样本稀少。为了解决这些局限性，最近的工作专注于通过利用相关领域的知识来增强检测性能的迁移学习来提高GAD模型的泛化能力，或者开发能够跨多个应用泛化的'一专多能'GAD基础模型。由于对GAD中泛化的系统性理解仍然缺乏，在本文中，我们提供了对GAD中泛化的全面回顾。我们首先追溯了GAD中泛化的演变，并形式化了问题设置，这进一步导致了我们的系统性分类。基于这种精细分类法，我们对现有的广义GAD方法进行了最新和全面的回顾。最后，我们确定了当前的开放性挑战，并提出了未来方向，以启发这一新兴领域的未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph anomaly detection (GAD) has attracted increasing attention in recentyears for identifying malicious samples in a wide range of graph-basedapplications, such as social media and e-commerce. However, most GAD methodsassume identical training and testing distributions and are tailored tospecific tasks, resulting in limited adaptability to real-world scenarios suchas shifting data distributions and scarce training samples in new applications.To address the limitations, recent work has focused on improving thegeneralization capability of GAD models through transfer learning thatleverages knowledge from related domains to enhance detection performance, ordeveloping "one-for-all" GAD foundation models that generalize across multipleapplications. Since a systematic understanding of generalization in GAD isstill lacking, in this paper, we provide a comprehensive review ofgeneralization in GAD. We first trace the evolution of generalization in GADand formalize the problem settings, which further leads to our systematictaxonomy. Rooted in this fine-grained taxonomy, an up-to-date and comprehensivereview is conducted for the existing generalized GAD methods. Finally, weidentify current open challenges and suggest future directions to inspirefuture research in this emerging field.</description>
      <author>example@mail.com (Junjun Pan, Yu Zheng, Yue Tan, Yixin Liu)</author>
      <guid isPermaLink="false">2509.06609v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
  <item>
      <title>Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study</title>
      <link>http://arxiv.org/abs/2509.05553v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型是否真正理解概念或仅识别模式的问题，提出双向推理作为测试真正理解的标准，并开发了对比微调方法来解决认知专业化问题。&lt;h4&gt;背景&lt;/h4&gt;人工智能领域的基本问题：大型语言模型是否真正理解概念，还是仅仅识别模式。&lt;h4&gt;目的&lt;/h4&gt;提出双向推理作为测试真正理解能力的标准，真正的理解应该允许自然可逆性。&lt;h4&gt;方法&lt;/h4&gt;开发对比微调方法，使用三种类型的示例进行训练：保持语义意义的正面示例、具有不同语义的负面示例、前向方向混淆示例。&lt;h4&gt;主要发现&lt;/h4&gt;当前语言模型存在'认知专业化'现象：在前向任务上微调会提高任务表现但降低双向推理能力；对比微调方法成功实现了双向推理，同时保持前向任务能力。&lt;h4&gt;结论&lt;/h4&gt;双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法。&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了人工智能中的一个基本问题：大型语言模型是否真正理解概念或仅仅识别模式。作者提出双向推理（能够在没有明确反向训练的情况下双向应用转换的能力）作为真正理解的测试。他们认为真正的理解应该自然允许可逆性。例如，能够将变量名如userIndex更改为i的模型也应该能够推断出i代表用户索引，而无需反向训练。研究人员测试了当前的语言模型，并发现了他们所称的认知专业化：当模型在前向任务上微调时，这些任务的表现有所提高，但它们的双向推理能力显著下降。为解决此问题，他们开发了对比微调方法，使用三种类型的示例训练模型：保持语义意义的正面示例、具有不同语义的负面示例和前向方向混淆示例。这种方法旨在培养更深层次的理解，而非表面模式识别，并允许反向能力在没有明确反向训练的情况下自然发展。他们的实验证明对比微调成功实现了双向推理，能够在保持前向任务能力的同时实现强大的反向性能。作者得出结论，双向推理既是评估真正理解的理论框架，也是开发更强大AI系统的实用训练方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This research addresses a fundamental question in AI: whether large languagemodels truly understand concepts or simply recognize patterns. The authorspropose bidirectional reasoning,the ability to apply transformations in bothdirections without being explicitly trained on the reverse direction, as a testfor genuine understanding. They argue that true comprehension should naturallyallow reversibility. For example, a model that can change a variable name likeuserIndex to i should also be able to infer that i represents a user indexwithout reverse training. The researchers tested current language models anddiscovered what they term cognitive specialization: when models are fine-tunedon forward tasks, their performance on those tasks improves, but their abilityto reason bidirectionally becomes significantly worse. To address this issue,they developed Contrastive Fine-Tuning (CFT), which trains models using threetypes of examples: positive examples that maintain semantic meaning, negativeexamples with different semantics, and forward-direction obfuscation examples.This approach aims to develop deeper understanding rather than surface-levelpattern recognition and allows reverse capabilities to develop naturallywithout explicit reverse training. Their experiments demonstrated that CFTsuccessfully achieved bidirectional reasoning, enabling strong reverseperformance while maintaining forward task capabilities. The authors concludethat bidirectional reasoning serves both as a theoretical framework forassessing genuine understanding and as a practical training approach fordeveloping more capable AI systems.</description>
      <author>example@mail.com (Serge Lionel Nikiema, Jordan Samhi, Micheline Bénédicte Moumoula, Albérick Euraste Djiré, Abdoul Kader Kaboré, Jacques Klein, Tegawendé F. Bissyandé)</author>
      <guid isPermaLink="false">2509.05553v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Point Linguist Model: Segment Any Object via Bridged Large 3D-Language Model</title>
      <link>http://arxiv.org/abs/2509.07825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Point Linguist Model (PLM)，解决了大型语言模型与3D点云表示不一致的问题，通过引入Object-centric Discriminative Representation和Geometric Reactivation Decoder显著提升了3D目标分割性能。&lt;h4&gt;背景&lt;/h4&gt;3D目标分割与大型语言模型的结合已成为主流范式，具有广泛的语义、任务灵活性和强大的泛化能力，但存在LLMs处理高级语义标记而3D点云仅传达密集几何结构的表示不一致问题。&lt;h4&gt;目的&lt;/h4&gt;解决LLMs与3D点云之间的表示不一致问题，搭建两者之间的表示差距桥梁，无需3D-文本或3D-图像之间的大规模预对齐。&lt;h4&gt;方法&lt;/h4&gt;提出Point Linguist Model通用框架，引入Object-centric Discriminative Representation学习以目标为中心的标记，捕获目标语义和场景关系；引入Geometric Reactivation Decoder结合OcDR标记和密集特征预测掩码，保留全面的密集特征。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNetv2上实现+7.3 mIoU改进，在Multi3DRefer的3D引用分割上实现+6.0 mIoU改进，在跨越4个不同任务的7个基准测试中保持一致的性能提升。&lt;h4&gt;结论&lt;/h4&gt;全面目标级推理对鲁棒的3D理解是有效的，PLM框架成功解决了LLMs和3D点云之间的表示不一致问题。&lt;h4&gt;翻译&lt;/h4&gt;使用大型语言模型进行3D目标分割已成为一种主流范式，因为它具有广泛的语义、任务灵活性和强大的泛化能力。然而，这种范式受到表示不一致的阻碍：大型语言模型处理高级语义标记，而3D点云仅传达密集的几何结构。在先前的方法中，这种不一致限制了输入和输出。在输入阶段，密集点块需要大量预对齐，弱化了目标级语义并混淆了相似的干扰项。在输出阶段，预测仅依赖于密集特征而没有明确的几何线索，导致精细粒度准确性的损失。为解决这些局限性，我们提出了Point Linguist Model，一个通用框架，它搭建了大型语言模型和密集3D点云之间的表示差距，而不需要3D-文本或3D-图像之间的大规模预对齐。具体而言，我们引入了Object-centric Discriminative Representation，它在硬负感知训练目标下学习以目标为中心的标记，捕获目标语义和场景关系。这减轻了大型语言模型标记和3D点之间的不一致，增强了对干扰项的鲁棒性，并促进了大型语言模型内的语义级推理。为了准确分割，我们引入了Geometric Reactivation Decoder，它通过结合携带大型语言模型推断几何的OcDR标记和相应的密集特征来预测掩码，在整个流程中保留全面的密集特征。大量实验表明，PLM在ScanNetv2的3D引用分割上实现了+7.3 mIoU的显著改进，在Multi3DRefer上实现了+6.0 mIoU的改进，在跨越4个不同任务的7个基准测试中保持一致的性能提升，证明了全面目标级推理对鲁棒3D理解的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云分割中的表示不匹配问题：大型语言模型处理高级语义标记，而3D点云只包含密集几何结构。这种不匹配限制了输入和输出效果，削弱了对象级语义识别能力，并导致细粒度分割精度下降。这个问题在3D场景理解、机器人导航和人机交互等领域至关重要，因为解决它可以显著提升AI系统对复杂3D环境的理解和分割能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析现有方法的局限性，然后借鉴了2D多模态语言模型(如LISA)的成功经验，结合3D点云特性进行创新。他们使用了预训练的Mask3D生成对象提案，采用LLaMA2作为基础语言模型，并应用LoRA进行高效微调。设计思路是创建一个桥梁连接LLM和3D点云，不依赖大规模预对齐数据，通过对象中心化表示和干扰物感知训练解决表示不匹配问题，最终形成PLM框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个关键组件桥接LLM和3D点云：1)对象中心化表示(OcDR)使用对象标记作为LLM输入，捕获目标语义和场景关系；2)几何重新激活解码器(GRD)结合OcDR标记和密集特征预测掩码。流程包括：1)输入处理-生成OcDR表示；2)多模态推理-LLM处理视觉和语言输入；3)解码输出-GRD重新激活几何特征生成最终分割掩码；4)训练-使用干扰物感知机制增强对象区分能力。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)OcDR使用对象中心化标记替代传统点块，自然保持对象边界和语义；2)干扰物感知机制通过语义相似的硬负样本增强对象区分；3)GRD在整个流程中保留密集特征，提高细粒度精度。不同之处：传统方法依赖密集点块和大量预对齐，PLM使用对象中心化表示无需预对齐；传统方法输出仅依赖密集特征，PLM结合几何线索；传统方法任务受限，PLM支持多种3D分割任务；PLM训练效率更高，数据需求更少。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point Linguist Model通过对象中心化表示和几何重新激活解码器，有效桥接了大型语言模型与3D点云之间的表示差距，显著提升了多种3D分割任务的性能，实现了开放词汇和语言引导的高效3D对象分割。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object segmentation with Large Language Models (LLMs) has become aprevailing paradigm due to its broad semantics, task flexibility, and stronggeneralization. However, this paradigm is hindered by representationmisalignment: LLMs process high-level semantic tokens, whereas 3D point cloudsconvey only dense geometric structures. In prior methods, misalignment limitsboth input and output. At the input stage, dense point patches require heavypre-alignment, weakening object-level semantics and confusing similardistractors. At the output stage, predictions depend only on dense featureswithout explicit geometric cues, leading to a loss of fine-grained accuracy. Toaddress these limitations, we present the Point Linguist Model (PLM), a generalframework that bridges the representation gap between LLMs and dense 3D pointclouds without requiring large-scale pre-alignment between 3D-text or3D-images. Specifically, we introduce Object-centric DiscriminativeRepresentation (OcDR), which learns object-centric tokens that capture targetsemantics and scene relations under a hard negative-aware training objective.This mitigates the misalignment between LLM tokens and 3D points, enhancesresilience to distractors, and facilitates semantic-level reasoning withinLLMs. For accurate segmentation, we introduce the Geometric ReactivationDecoder (GRD), which predicts masks by combining OcDR tokens carryingLLM-inferred geometry with corresponding dense features, preservingcomprehensive dense features throughout the pipeline. Extensive experimentsshow that PLM achieves significant improvements of +7.3 mIoU on ScanNetv2 and+6.0 mIoU on Multi3DRefer for 3D referring segmentation, with consistent gainsacross 7 benchmarks spanning 4 different tasks, demonstrating the effectivenessof comprehensive object-centric reasoning for robust 3D understanding.</description>
      <author>example@mail.com (Zhuoxu Huang, Mingqi Gao, Jungong Han)</author>
      <guid isPermaLink="false">2509.07825v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Physics-informed low-rank neural operators with application to parametric elliptic PDEs</title>
      <link>http://arxiv.org/abs/2509.07687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PILNO是一种用于在点云数据上高效逼近偏微分方程解算子的神经算子框架，结合低秩核逼近与编码器-解码器架构，能够快速、连续地进行一次性预测，同时保持对特定离散化的独立性。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程的求解在科学计算和工程应用中非常重要，但传统方法可能计算成本高且难以处理复杂几何形状或高维参数空间。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的神经算子框架，能够在点云数据上逼近偏微分方程的解算子，同时保持计算效率和对特定离散化的独立性。&lt;h4&gt;方法&lt;/h4&gt;提出PILNO框架，结合低秩核逼近与编码器-解码器架构，使用物理信息惩罚框架进行训练，确保PDE约束和边界条件得到满足。&lt;h4&gt;主要发现&lt;/h4&gt;PILNO在函数拟合、泊松方程、具有可变系数的屏蔽泊松方程和参数化达西流动等多种问题上表现出色；低秩结构在高维参数空间中提供了计算效率。&lt;h4&gt;结论&lt;/h4&gt;PILNO是一种可扩展且灵活的偏微分方程代理建模工具，能够高效处理各种偏微分方程问题。&lt;h4&gt;翻译&lt;/h4&gt;我们提出物理信息低秩神经算子，这是一种神经算子框架，用于在点云数据上高效逼近偏微分方程的解算子。PILNO将低秩核逼近与编码器-解码器架构相结合，能够快速、连续地进行一次性预测，同时保持对特定离散化的独立性。该模型使用物理信息惩罚框架进行训练，确保在监督和非监督设置下都满足PDE约束和边界条件。我们在各种问题上证明了其有效性，包括函数拟合、泊松方程、具有可变系数的屏蔽泊松方程和参数化达西流动。低秩结构在高维参数空间中提供了计算效率，使PILNO成为偏微分方程的可扩展且灵活的代理建模工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Physics-Informed Low-Rank Neural Operator (PILNO), a neuraloperator framework for efficiently approximating solution operators of partialdifferential equations (PDEs) on point cloud data. PILNO combines low-rankkernel approximations with an encoder--decoder architecture, enabling fast,continuous one-shot predictions while remaining independent of specificdiscretizations. The model is trained using a physics-informed penaltyframework, ensuring that PDE constraints and boundary conditions are satisfiedin both supervised and unsupervised settings. We demonstrate its effectivenesson diverse problems, including function fitting, the Poisson equation, thescreened Poisson equation with variable coefficients, and parameterized Darcyflow. The low-rank structure provides computational efficiency inhigh-dimensional parameter spaces, establishing PILNO as a scalable andflexible surrogate modeling tool for PDEs.</description>
      <author>example@mail.com (Sebastian Schaffer, Lukas Exl)</author>
      <guid isPermaLink="false">2509.07687v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>MVAT: Multi-View Aware Teacher for Weakly Supervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.07507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at WACV 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MVAT框架，利用时间多视图信息解决3D目标检测中的弱监督标注问题，通过教师-学生蒸馏范式和多视图2D投影损失，实现了接近完全监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;3D数据标注是3D目标检测中的昂贵瓶颈，促使研究人员开发依赖更易获取的2D边界框标注的弱监督标注方法。&lt;h4&gt;目的&lt;/h4&gt;解决仅依赖2D边界框标注时产生的投影歧义问题，以及单一视角下部分物体可见性导致的3D边界框估计困难问题。&lt;h4&gt;方法&lt;/h4&gt;提出MVAT框架，通过跨时间聚合物体中心点云构建3D物体表示，采用教师-学生蒸馏范式让教师网络从单一视角学习并生成高质量伪标签，学生网络从单一视角预测这些伪标签，同时使用多视图2D投影损失确保3D预测与2D标注的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes和Waymo Open数据集上的实验表明，MVAT在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与完全监督方法的差距。&lt;h4&gt;结论&lt;/h4&gt;MVAT框架无需任何3D边界框标注即可实现接近完全监督方法的性能，为3D目标检测提供了一种有效的弱监督解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为MVAT的新颖框架，该框架利用序列数据中存在的时间多视图信息来解决这些挑战。我们的方法跨时间聚合以物体为中心的点云，以构建尽可能密集和完整的3D物体表示。采用教师-学生蒸馏范式：教师网络从单一视角学习，但目标是从时间聚合的静态物体中推导出来的。然后，教师生成高质量的伪标签，学生从单一视角学习预测这些伪标签，用于静态和移动物体。整个框架集成了多视图2D投影损失，以强制预测的3D边界框与所有可用的2D标注之间的一致性。在nuScenes和Waymo Open数据集上的实验表明，MVAT在弱监督3D目标检测方面取得了最先进的性能，显著缩小了与完全监督方法的差距，而无需任何3D边界框标注。我们的代码可在公共仓库中获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决弱监督3D目标检测中的投影歧义问题，即仅使用2D框标注来推断3D物体位置和大小时的不确定性。这个问题在现实中非常重要，因为3D数据标注成本高昂（平均114秒），而2D标注只需7-35秒，有3-16倍的效率提升。3D检测是自动驾驶和机器人的基础感知任务，降低标注成本能促进3D检测技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考到现有弱监督方法仅依赖单帧数据，忽略了连续数据采集中自然存在的多视角信息。他们设计了一个教师-学生知识蒸馏框架，教师网络利用多帧信息学习3D几何，学生网络学习从单帧预测3D信息。方法借鉴了SAM 2图像分割、DBSCAN点云聚类、PCA 3D框估计等现有技术，以及全监督领域的时间融合方法，但创新性地将这些技术组合应用于弱监督3D检测场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时间多视角信息解决投影歧义问题，通过聚合时序点云构建更完整的3D物体表示，采用教师-学生知识蒸馏框架。流程包括：1)使用SAM 2提取对象中心点云；2)聚合静态物体的多帧点云；3)使用PCA估计粗略3D框；4)教师网络用多帧信息训练；5)教师生成高质量伪标签；6)学生网络从单帧学习预测这些伪标签；7)使用多视角2D投影损失确保一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次有效利用时间多视角数据解决投影歧义；2)提出通过聚合点云生成高质量3D表示和伪标签的方法；3)采用多视角2D投影损失强制几何一致性；4)实现弱监督3D检测的最先进性能。不同之处：不依赖类别特定先验，而是直接解决歧义；不需要任何3D框标注或弱3D标注；首次将时间融合引入弱监督3D检测；教师用多帧训练，学生用单帧学习，实现有效知识迁移。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MVAT通过利用时间多视角一致性和教师-学生知识蒸馏框架，首次有效解决了弱监督3D目标检测中的投影歧义问题，显著缩小了与全监督方法的性能差距，同时仅需2D框标注。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotating 3D data remains a costly bottleneck for 3D object detection,motivating the development of weakly supervised annotation methods that rely onmore accessible 2D box annotations. However, relying solely on 2D boxesintroduces projection ambiguities since a single 2D box can correspond tomultiple valid 3D poses. Furthermore, partial object visibility under a singleviewpoint setting makes accurate 3D box estimation difficult. We propose MVAT,a novel framework that leverages temporal multi-view present in sequential datato address these challenges. Our approach aggregates object-centric pointclouds across time to build 3D object representations as dense and complete aspossible. A Teacher-Student distillation paradigm is employed: The Teachernetwork learns from single viewpoints but targets are derived from temporallyaggregated static objects. Then the Teacher generates high qualitypseudo-labels that the Student learns to predict from a single viewpoint forboth static and moving objects. The whole framework incorporates a multi-view2D projection loss to enforce consistency between predicted 3D boxes and allavailable 2D annotations. Experiments on the nuScenes and Waymo Open datasetsdemonstrate that MVAT achieves state-of-the-art performance for weaklysupervised 3D object detection, significantly narrowing the gap with fullysupervised methods without requiring any 3D box annotations. % \footnote{Codeavailable upon acceptance} Our code is available in our public repository(\href{https://github.com/CEA-LIST/MVAT}{code}).</description>
      <author>example@mail.com (Saad Lahlali, Alexandre Fournier Montgieux, Nicolas Granger, Hervé Le Borgne, Quoc Cuong Pham)</author>
      <guid isPermaLink="false">2509.07507v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>DepthVision: Robust Vision-Language Understanding through GAN-Based LiDAR-to-RGB Synthesis</title>
      <link>http://arxiv.org/abs/2509.07463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DepthVision是一个多模态场景理解框架，通过结合激光雷达点云生成的RGB图像与真实RGB数据，解决了视觉输入退化或不充分时机器人可靠操作的问题，特别是在低光条件下表现优异。&lt;h4&gt;背景&lt;/h4&gt;当视觉输入退化或不充分时，确保机器人可靠运行仍然是一个核心挑战。现有的视觉语言模型(VLMs)仅依赖基于相机的视觉输入和语言，在环境条件差(如黑暗或运动模糊)时性能受限。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，解决视觉输入退化情况下机器人操作可靠性问题，特别是在低光条件下的表现，同时保持与现有冻结VLMs的兼容性。&lt;h4&gt;方法&lt;/h4&gt;DepthVision使用条件生成对抗网络(GAN)结合集成精炼网络，从稀疏激光雷达点云合成RGB图像。然后，使用亮度感知模态适应(LAMA)将这些合成视图与真实RGB数据动态结合，基于环境光照条件混合两种类型的数据。这种方法无需对下游视觉语言模型进行任何微调即可补偿传感器退化。&lt;h4&gt;主要发现&lt;/h4&gt;在真实和模拟数据集上对各种模型和任务(特别是安全关键任务)的评估表明，该方法在低光条件下提高了性能，比仅使用RGB的基线方法获得显著提升，同时保持与冻结VLMs的兼容性。&lt;h4&gt;结论&lt;/h4&gt;激光雷达引导的RGB合成在实现真实环境中机器人操作的鲁棒性方面具有巨大潜力，为解决视觉输入退化问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;当视觉输入退化或不充分时，确保机器人可靠运行仍然是机器人技术中的一个核心挑战。本文介绍了DepthVision，这是一个为解决此问题而设计的多模态场景理解框架。与仅使用基于相机的视觉输入和语言的现有视觉语言模型(VLMs)不同，DepthVision使用带有集成精炼网络的条件生成对抗网络(GAN)，从稀疏激光雷达点云合成RGB图像。然后，使用亮度感知模态适应(LAMA)将这些合成视图与真实RGB数据结合，根据环境光照条件动态混合这两种类型的数据。这种方法补偿了传感器退化，如黑暗或运动模糊，而无需对下游视觉语言模型进行任何微调。我们在真实和模拟数据集上对各种模型和任务评估了DepthVision，特别关注安全关键任务。结果表明，我们的方法在低光条件下提高了性能，比仅使用RGB的基线方法获得显著提升，同时保持与冻结VLMs的兼容性。这项工作强调了激光雷达引导的RGB合成在实现真实环境中机器人操作鲁棒性方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人在视觉输入退化或不足时（如低光、运动模糊等条件）如何保持可靠运行的问题。这个问题在现实中非常重要，因为机器人任务依赖对环境的准确感知，而现有的视觉语言模型主要依赖相机图像，在低光等条件下表现不佳。虽然LiDAR等传感器能在这些条件下提供更好的空间感知，但LiDAR数据稀缺且难以大规模获取，限制了三维空间推理能力，这对自动驾驶等安全关键领域尤为重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到视觉语言模型在低光条件下的局限性，以及LiDAR数据在提供空间感知方面的优势但存在数据稀缺问题。他们借鉴了生成对抗网络(GAN)特别是pix2pix框架来从LiDAR数据生成RGB图像，并采用U-Net架构保留空间细节。此外，作者还参考了refiner网络概念提高生成图像质量，并利用现有的视觉语言模型架构如Vision Transformer。关键创新在于设计了光照感知的模态适应机制，根据环境光照条件动态调整不同模态的权重，无需修改下游模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用LiDAR点云数据生成RGB图像，以补充或替代相机在低光条件下的输入，并根据环境光照条件动态调整真实RGB图像和生成RGB图像的融合权重。整体实现流程包括：1) LiDAR预处理：将3D点云投影到2D图像平面并进行插值；2) GAN和Refiner设置：使用基于pix2pix的GAN从单通道LiDAR投影生成RGB图像，并通过refiner迭代提高质量；3) 光照感知模态适应(LAMA)：计算图像亮度并动态调整融合权重；4) VLM集成：将融合后的图像送入冻结的视觉语言模型进行推理，无需微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 基于GAN的LiDAR到RGB合成架构，集成refiner网络提高生成质量；2) 光照感知模态适应(LAMA)机制，根据环境光照动态调整不同模态权重；3) 无需对下游视觉语言模型进行微调，保持与现有模型的兼容性。相比之前工作的不同：不同于现有模型仅使用相机图像，DepthVision结合LiDAR数据；不同于传统传感器融合直接处理原始数据，DepthVision生成RGB样式的表示；不同于需要大量LiDAR数据训练的方法，DepthVision通过GAN从稀疏点云生成图像；不同于静态融合策略，DepthVision根据光照条件动态调整；不同于需要修改下游模型的方法，保持视觉语言模型冻结状态。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DepthVision通过基于GAN的LiDAR到RGB合成和光照感知模态适应，显著提高了机器人在低光条件下的视觉语言理解能力，同时无需修改下游模型。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring reliable robot operation when visual input is degraded orinsufficient remains a central challenge in robotics. This letter introducesDepthVision, a framework for multimodal scene understanding designed to addressthis problem. Unlike existing Vision-Language Models (VLMs), which use onlycamera-based visual input alongside language, DepthVision synthesizes RGBimages from sparse LiDAR point clouds using a conditional generativeadversarial network (GAN) with an integrated refiner network. These syntheticviews are then combined with real RGB data using a Luminance-Aware ModalityAdaptation (LAMA), which blends the two types of data dynamically based onambient lighting conditions. This approach compensates for sensor degradation,such as darkness or motion blur, without requiring any fine-tuning ofdownstream vision-language models. We evaluate DepthVision on real andsimulated datasets across various models and tasks, with particular attentionto safety-critical tasks. The results demonstrate that our approach improvesperformance in low-light conditions, achieving substantial gains over RGB-onlybaselines while preserving compatibility with frozen VLMs. This work highlightsthe potential of LiDAR-guided RGB synthesis for achieving robust robotoperation in real-world environments.</description>
      <author>example@mail.com (Sven Kirchner, Nils Purschke, Ross Greer, Alois C. Knoll)</author>
      <guid isPermaLink="false">2509.07463v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Aerial-ground Cross-modal Localization: Dataset, Ground-truth, and Benchmark</title>
      <link>http://arxiv.org/abs/2509.07362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对密集城市环境中的视觉定位问题，提出了一种利用机载激光扫描(ALS)数据作为先验地图的新方法，并克服了现有ALS-based定位的三个主要限制。&lt;h4&gt;背景&lt;/h4&gt;在密集城市环境中进行准确的视觉定位是摄影测量、地理信息科学和机器人学中的基础任务。图像作为一种低成本且广泛可用的传感方式，其有效性常受限于无纹理表面、视角变化大和长期漂移问题。ALS数据的公开可用性为可扩展且精确的视觉定位开辟了新途径。&lt;h4&gt;目的&lt;/h4&gt;克服ALS-based定位面临的三个主要限制：缺乏平台多样化的数据集、缺乏适用于大规模城市环境的可靠真实值生成方法、以及现有I2P算法在空中-地面跨平台设置下的验证有限。&lt;h4&gt;方法&lt;/h4&gt;引入了一个新的数据集，该数据集集成了来自移动测绘系统的地面级图像与在中国武汉、香港和旧金山收集的ALS点云。&lt;h4&gt;主要发现&lt;/h4&gt;基于ALS的定位潜力尚未得到充分探索，主要受限于数据集多样性、真实值生成方法和算法验证的不足。&lt;h4&gt;结论&lt;/h4&gt;通过整合地面图像和ALS点云的新数据集为解决城市环境中的视觉定位问题提供了新的可能性，有助于推动ALS-based定位技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;在密集城市环境中进行准确的视觉定位是摄影测量、地理信息科学和机器人学中的基础任务。虽然图像是一种低成本且广泛可用的传感方式，但其在视觉里程计中的有效性常受限于无纹理表面、严重的视角变化和长期漂移。机载激光扫描(ALS)数据的日益公开可用性为可扩展且精确的视觉定位开辟了新途径，利用ALS作为先验地图。然而，由于三个关键限制，ALS-based定位的潜力尚未得到充分探索：(1)缺乏平台多样化的数据集，(2)缺乏适用于大规模城市环境的可靠真实值生成方法，(3)现有图像到点云(I2P)算法在空中-地面跨平台设置下的验证有限。为克服这些挑战，我们引入了一个新的大规模数据集，该数据集集成了来自移动测绘系统的地面级图像与在中国武汉、香港和旧金山收集的ALS点云。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决空中-地面跨模态定位问题，即如何利用地面图像和机载激光扫描(ALS)点云数据进行准确定位。这个问题在现实中非常重要，因为在高密度城市环境中，卫星导航信号常被遮挡或失效，而纯视觉定位又受限于无纹理表面、视角变化和长期漂移等问题。结合ALS点云作为先验地图可以提供更精确、更稳定的定位解决方案，对自动驾驶、机器人导航和地理测绘等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有工作的三大局限性：缺乏平台多样化的数据集、缺乏适用于大规模城市环境的可靠地面真实值生成方法、以及现有图像到点云(I2P)算法在跨平台场景下的验证有限。基于这些认识，作者设计了结合地面图像和ALS点云的新数据集，并借鉴了现有的点云配准、姿态图优化和特征提取技术，但进行了改进以适应跨模态场景。在地面真实值生成方面，作者创新地采用间接方法，通过将移动激光扫描(MLS)数据与ALS点云对齐，然后将优化后的轨迹转移到图像流中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用ALS点云作为先验地图，结合地面图像实现跨模态定位，并通过间接方法生成高精度的地面真实值轨迹。整体流程分为三步：1)数据收集与预处理，收集三个城市的地面图像和ALS点云，并进行下采样和分割；2)地面真实值生成，通过地面分割和立面重建将MLS数据与ALS点云对齐，利用多传感器姿态图优化获得精确的图像姿态；3)定位算法评估，评估多种I2P算法在全局和精细定位任务上的性能，并采用SfM和ICP作为替代方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了首个整合地面图像和ALS点云的大规模跨平台数据集，覆盖三个不同特点的城市；2)提出创新的间接地面真实值生成方法，通过MLS-ALS对齐和姿态图优化，避免了跨模态直接配准的困难；3)建立了统一的基准测试套件，系统评估了现有方法在跨模态场景下的性能。相比之前的工作，这个数据集提供了更全面的跨模态场景，地面真实值生成方法不依赖重访区域或高端设备，且评估工作首次系统性地分析了I2P算法在空中-地面跨平台设置下的表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建大规模跨模态数据集、创新的地面真实值生成方法和统一的基准测试，为空中-地面视觉定位研究提供了重要资源和评估标准，推动了城市环境中高精度定位技术的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate visual localization in dense urban environments poses a fundamentaltask in photogrammetry, geospatial information science, and robotics. Whileimagery is a low-cost and widely accessible sensing modality, its effectivenesson visual odometry is often limited by textureless surfaces, severe viewpointchanges, and long-term drift. The growing public availability of airborne laserscanning (ALS) data opens new avenues for scalable and precise visuallocalization by leveraging ALS as a prior map. However, the potential ofALS-based localization remains underexplored due to three key limitations: (1)the lack of platform-diverse datasets, (2) the absence of reliable ground-truthgeneration methods applicable to large-scale urban environments, and (3)limited validation of existing Image-to-Point Cloud (I2P) algorithms underaerial-ground cross-platform settings. To overcome these challenges, weintroduce a new large-scale dataset that integrates ground-level imagery frommobile mapping systems with ALS point clouds collected in Wuhan, Hong Kong, andSan Francisco.</description>
      <author>example@mail.com (Yandi Yang, Jianping Li, Youqi Liao, Yuhao Li, Yizhe Zhang, Zhen Dong, Bisheng Yang, Naser El-Sheimy)</author>
      <guid isPermaLink="false">2509.07362v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Performance Characterization of a Point-Cloud-Based Path Planner in Off-Road Terrain</title>
      <link>http://arxiv.org/abs/2509.07321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been published in the Journal of Field Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对名为MUONS的点云导航系统进行了全面评估，用于非结构化道路的自主导航。研究通过模拟和实地测试进行了30,000次规划和导航试验，分析了七个路径规划参数的二十种组合在不同地形上的表现。&lt;h4&gt;背景&lt;/h4&gt;非结构化道路环境下的自主导航是一个具有挑战性的问题，需要高效可靠的导航系统来处理复杂地形。&lt;h4&gt;目的&lt;/h4&gt;评估MUONS导航系统在非结构化道路环境中的性能，并确定影响性能的关键参数。&lt;h4&gt;方法&lt;/h4&gt;研究者在模拟环境中测试了三个具有运动学挑战的地形图，并进行了实地验证。总共进行了30,000次规划和导航试验，通过统计和相关性分析评估不同参数组合对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;1. 配备MUONS的自动导引车在模拟中取得了0.98的成功率；2. 在实地测试中未出现故障；3. 初始规划阶段使用的双扩展半径与规划时间和路径长度性能最相关；4. 调整参数引起的变化比例与实地测试性能高度相关。&lt;h4&gt;结论&lt;/h4&gt;蒙特卡洛模拟活动可有效用于导航系统的性能评估和参数调整，模拟结果与实际表现具有良好的一致性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个基于点云的导航堆栈MUONS的全面评估，用于非结构化道路的自主导航。通过分析模拟中30,000次规划和导航试验的结果，并通过实地测试验证，来表征其性能。我们的模拟活动考虑了三个具有运动学挑战的地形图和七种路径规划参数的二十种组合。在模拟中，配备MUONS的自动导引车取得了0.98的成功率，并且在实地测试中没有出现故障。通过统计和相关性分析，我们确定在初始规划阶段使用的双扩展半径与规划时间和路径长度性能最相关。最后，我们观察到调整参数引起的变化比例与实地测试性能高度相关。这一发现支持使用蒙特卡洛模拟活动进行性能评估和参数调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/rob.70059&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a comprehensive evaluation of a point-cloud-based navigationstack, MUONS, for autonomous off-road navigation. Performance is characterizedby analyzing the results of 30,000 planning and navigation trials in simulationand validated through field testing. Our simulation campaign considers threekinematically challenging terrain maps and twenty combinations of sevenpath-planning parameters. In simulation, our MUONS-equipped AGV achieved a 0.98success rate and experienced no failures in the field. By statistical andcorrelation analysis we determined that the Bi-RRT expansion radius used in theinitial planning stages is most correlated with performance in terms ofplanning time and traversed path length. Finally, we observed that theproportional variation due to changes in the tuning parameters is remarkablywell correlated to performance in field testing. This finding supports the useof Monte-Carlo simulation campaigns for performance assessment and parametertuning.</description>
      <author>example@mail.com (Casey D. Majhor, Jeremy P. Bos)</author>
      <guid isPermaLink="false">2509.07321v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>BlendedNet: A Blended Wing Body Aircraft Dataset and Surrogate Model for Aerodynamic Predictions</title>
      <link>http://arxiv.org/abs/2509.07209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ASME IDETC/CIE 2025 (DETC2025-168977). Dataset  availability: BlendedNet dataset is openly available at Harvard Dataverse  (https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VJT9EP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BlendedNet是一个包含999个混合翼身几何形状的公开气动数据集，在约9种飞行条件下模拟产生8830个RANS案例，并引入了一个端到端的代理框架用于点状气动预测。&lt;h4&gt;背景&lt;/h4&gt;非常规配置（如混合翼身BWB）的气动设计面临数据稀缺问题，限制了数据驱动代理模型的研究。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模的混合翼身气动数据集，并提供一个端到端的代理框架用于点状气动预测，以解决数据稀缺问题并促进气动设计研究。&lt;h4&gt;方法&lt;/h4&gt;1. 采样几何设计参数和飞行条件生成数据集；2. 使用Spalart-Allmaras模型进行RANS模拟；3. 开发端到端代理框架，包括使用PointNet回归器从表面点云预测几何参数，以及使用特征线性调制(FiLM)网络预测点状系数Cp、Cfx和Cfz。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在多样化的混合翼身上，表面预测具有较低的误差，证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;BlendedNet数据集解决了非常规配置的数据稀缺问题，并使基于数据驱动的代理模型研究成为可能，有助于气动设计的发展。&lt;h4&gt;翻译&lt;/h4&gt;BlendedNet是一个公开的气动数据集，包含999个混合翼身(BWB)几何形状。每个几何形状在约9种飞行条件下进行模拟，产生了8830个使用Spalart-Allmaras模型的收敛RANS案例，每个案例有900万至1400万个单元。该数据集通过采样几何设计参数和飞行条件生成，包括研究升力和阻力所需的详细点状表面量。我们还引入了一个用于点状气动预测的端到端代理框架。该流程首先使用排列不变的PointNet回归器从采样的表面点云预测几何参数，然后在预测的参数和飞行条件下调节特征线性调制(FiLM)网络，以预测点状系数Cp、Cfx和Cfz。实验显示，在多样化的BWB上表面预测误差较低。BlendedNet解决了非常规配置的数据稀缺问题，并支持基于数据驱动的代理模型研究，用于气动设计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决混合翼身飞机(BWB)气动分析中的数据稀缺问题。这个问题很重要，因为BWB飞机具有更高的升阻比、更轻的结构重量和更低的燃油消耗(比传统飞机减少30%)，但其复杂几何形状导致流场分析困难，现有方法计算成本高、设计迭代慢，且缺乏详细的表面级气动数据，限制了数据驱动设计方法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到BWB设计中的数据稀缺和计算效率问题，然后借鉴了多项现有工作：使用Zhang等人的几何参数化方法定义BWB形状；采用PointNet架构处理点云数据并预测几何参数；受Catalani等人启发使用Feature-wise Linear Modulation (FiLM)网络；利用Latin Hypercube Sampling系统性地采样几何和飞行条件参数。作者将这些方法整合成一个两阶段深度学习框架，从点云直接预测气动特性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个高质量的BWB气动数据集，并开发一个端到端的深度学习框架，从点云数据直接预测气动表面特性。整体流程分为三部分：1)数据生成：使用OpenVSP生成999种几何形状，Pointwise生成网格，FUN3D进行高精度RANS模拟，处理得到8830个成功案例；2)代理模型：第一阶段用PointNet从点云预测9个几何参数，第二阶段用FiLM网络结合几何参数和飞行条件预测表面气动系数(Cp, Cfx, Cfz)；3)训练评估：按几何分组分割数据，使用Adam优化器训练，在独立测试集上评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个公开的BWB高分辨率表面气动数据集，包含999种几何和8830种飞行条件；2)端到点云到气动预测的完整框架；3)结合PointNet和FiLM的两阶段模型；4)提供点级压力和摩擦系数而非仅整体系数。相比之前工作，BlendedNet提供了更丰富的几何变体和更详细的表面数据，预测方法从整体系数扩展到表面分布，实现了从点云输入到气动预测的端到端流程，并作为开源数据集发布，促进了领域研究。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了一个高质量的混合翼身飞机气动数据集和一个创新的端到端深度学习框架，能够从点云数据直接预测表面气动特性，解决了BWB设计中的数据稀缺和计算效率问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; BlendedNet is a publicly available aerodynamic dataset of 999 blended wingbody (BWB) geometries. Each geometry is simulated across about nine flightconditions, yielding 8830 converged RANS cases with the Spalart-Allmaras modeland 9 to 14 million cells per case. The dataset is generated by samplinggeometric design parameters and flight conditions, and includes detailedpointwise surface quantities needed to study lift and drag. We also introducean end-to-end surrogate framework for pointwise aerodynamic prediction. Thepipeline first uses a permutation-invariant PointNet regressor to predictgeometric parameters from sampled surface point clouds, then conditions aFeature-wise Linear Modulation (FiLM) network on the predicted parameters andflight conditions to predict pointwise coefficients Cp, Cfx, and Cfz.Experiments show low errors in surface predictions across diverse BWBs.BlendedNet addresses data scarcity for unconventional configurations andenables research on data-driven surrogate modeling for aerodynamic design.</description>
      <author>example@mail.com (Nicholas Sung, Steven Spreizer, Mohamed Elrefaie, Kaira Samuel, Matthew C. Jones, Faez Ahmed)</author>
      <guid isPermaLink="false">2509.07209v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Visual Representation Alignment for Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.07979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://cvlab-kaist.github.io/VIRAL/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VIRAL方法，通过将多模态大语言模型的内部视觉表示与预训练视觉基础模型对齐，增强模型在视觉中心任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型通过视觉指令调优在各种任务上表现强大，但在以视觉为中心的任务（如物体计数或空间推理）中仍然有限。这种差距归因于当前仅文本的监督范式，它仅为视觉通路提供间接指导，导致模型在训练过程中丢弃细粒度的视觉细节。&lt;h4&gt;目的&lt;/h4&gt;提出VIRAL方法，通过显式对齐多模态大语言模型与预训练视觉基础模型的内部表示，使模型能够保留关键视觉细节并补充额外视觉知识，增强对复杂视觉输入的推理能力。&lt;h4&gt;方法&lt;/h4&gt;VIRAL是一种简单而有效的正则化策略，它将多模态大语言模型的内部视觉表示与预训练视觉基础模型(VFMs)的表示进行对齐。通过显式强制执行这种对齐，使模型能够保留输入视觉编码器中的关键视觉细节，同时补充VFMs的额外视觉知识。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，在广泛采用的多模态基准测试的所有任务中，VIRAL都带来了一致的性能提升。全面的消融研究验证了框架背后的关键设计选择。&lt;h4&gt;结论&lt;/h4&gt;这种简单的发现为在多模态大语言模型训练中有效整合视觉信息开辟了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通过视觉指令调优训练后已在各种任务上取得强大性能，但在以视觉为中心的任务（如物体计数或空间推理）中仍然存在局限。我们将这一差距归因于当前盛行的仅文本监督范式，该范式仅为视觉通路提供间接指导，并常导致MLLMs在训练过程中丢弃细粒度的视觉细节。在本文中，我们提出了VIRAL（Visual Representation Alignment，视觉表示对齐），这是一种简单而有效的正则化策略，它将MLLMs的内部视觉表示与预训练视觉基础模型(VFMs)的表示进行对齐。通过显式强制执行这种对齐，VIRAL使模型不仅能够保留来自输入视觉编码器的关键视觉细节，还能补充VFMs的额外视觉知识，从而增强其对复杂视觉输入的推理能力。我们的实验在广泛采用的多模态基准测试的所有任务中展示了一致的性能提升。此外，我们进行了全面的消融研究，以验证我们框架背后的关键设计选择。我们相信这一简单发现为在MLLMs训练中有效整合视觉信息开辟了重要方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) trained with visual instructiontuning have achieved strong performance across diverse tasks, yet they remainlimited in vision-centric tasks such as object counting or spatial reasoning.We attribute this gap to the prevailing text-only supervision paradigm, whichprovides only indirect guidance for the visual pathway and often leads MLLMs todiscard fine-grained visual details during training. In this paper, we presentVIsual Representation ALignment (VIRAL), a simple yet effective regularizationstrategy that aligns the internal visual representations of MLLMs with those ofpre-trained vision foundation models (VFMs). By explicitly enforcing thisalignment, VIRAL enables the model not only to retain critical visual detailsfrom the input vision encoder but also to complement additional visualknowledge from VFMs, thereby enhancing its ability to reason over complexvisual inputs. Our experiments demonstrate consistent improvements across alltasks on widely adopted multimodal benchmarks. Furthermore, we conductcomprehensive ablation studies to validate the key design choices underlyingour framework. We believe this simple finding opens up an important directionfor the effective integration of visual information in training MLLMs.</description>
      <author>example@mail.com (Heeji Yoon, Jaewoo Jung, Junwan Kim, Hyungyu Choi, Heeseong Shin, Sangbeom Lim, Honggyu An, Chaehyun Kim, Jisang Han, Donghyun Kim, Chanho Eom, Sunghwan Hong, Seungryong Kim)</author>
      <guid isPermaLink="false">2509.07979v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Bringing Multi-Modal Multi-Task Federated Foundation Models to Education Domain: Prospects and Challenges</title>
      <link>http://arxiv.org/abs/2509.07946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为M3T FedFMs（多模态多任务联邦基础模型）的新范式，将联邦学习与多模态多任务基础模型相结合，用于教育领域，实现在保护隐私前提下的跨机构协作训练。&lt;h4&gt;背景&lt;/h4&gt;多模态多任务基础模型在人工智能领域展现出变革性潜力，特别是在教育应用方面。然而，这些模型在实际教育环境中的部署受到隐私法规、数据孤岛和领域特定数据有限性的阻碍。&lt;h4&gt;目的&lt;/h4&gt;向教育界揭示M3T FedFMs作为一种有前景但尚未被充分探索的方法，探索其潜力，并揭示相关的未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;提出M3T FedFMs框架，整合联邦学习与多模态多任务基础模型，使分散机构能够进行协作且隐私保护的训练，同时适应多样化的模态和任务。&lt;h4&gt;主要发现&lt;/h4&gt;M3T FedFMs能够推进下一代智能教育系统的三个关键支柱：(1)隐私保护，通过将敏感数据保留在本地；(2)个性化，通过模块化架构定制模型；(3)公平性和包容性，促进资源有限实体的参与。&lt;h4&gt;结论&lt;/h4&gt;M3T FedFMs面临多个开放研究挑战，包括机构间异构隐私法规研究、数据模态特性不均匀性、遗忘方法、持续学习框架以及模型可解释性，这些挑战需集体解决才能实现实际部署。&lt;h4&gt;翻译&lt;/h4&gt;多模态多任务基础模型最近在人工智能领域显示出变革性潜力，在教育领域有新兴应用。然而，其在现实教育环境中的部署受到隐私法规、数据孤岛和领域特定数据可用性有限的阻碍。我们为教育领域引入了多模态多任务联邦基础模型：一种将联邦学习与多模态多任务基础模型相结合的范式，使分散机构能够进行协作、隐私保护的训练，同时适应多样的模态和任务。随后，这篇立场论文旨在向教育界揭示M3T FedFMs作为一种有前景但尚未被充分探索的方法，探索其潜力，并揭示相关的未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal multi-task (M3T) foundation models (FMs) have recently showntransformative potential in artificial intelligence, with emerging applicationsin education. However, their deployment in real-world educational settings ishindered by privacy regulations, data silos, and limited domain-specific dataavailability. We introduce M3T Federated Foundation Models (FedFMs) foreducation: a paradigm that integrates federated learning (FL) with M3T FMs toenable collaborative, privacy-preserving training across decentralizedinstitutions while accommodating diverse modalities and tasks. Subsequently,this position paper aims to unveil M3T FedFMs as a promising yet underexploredapproach to the education community, explore its potentials, and reveal itsrelated future research directions. We outline how M3T FedFMs can advance threecritical pillars of next-generation intelligent education systems: (i) privacypreservation, by keeping sensitive multi-modal student and institutional datalocal; (ii) personalization, through modular architectures enabling tailoredmodels for students, instructors, and institutions; and (iii) equity andinclusivity, by facilitating participation from underrepresented andresource-constrained entities. We finally identify various open researchchallenges, including studying of (i) inter-institution heterogeneous privacyregulations, (ii) the non-uniformity of data modalities' characteristics, (iii)the unlearning approaches for M3T FedFMs, (iv) the continual learningframeworks for M3T FedFMs, and (v) M3T FedFM model interpretability, which mustbe collectively addressed for practical deployment.</description>
      <author>example@mail.com (Kasra Borazjani, Naji Khosravan, Rajeev Sahay, Bita Akram, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2509.07946v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Long-Document Retrieval in the PLM and LLM Era</title>
      <link>http://arxiv.org/abs/2509.07759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述提供了长文档检索(LDR)的首次全面处理，系统化了从经典词汇和早期神经模型到现代预训练和大型语言模型的演变，涵盖了关键范式如段落聚合、层次编码和高效注意力技术。&lt;h4&gt;背景&lt;/h4&gt;长文档的激增对信息检索(IR)提出了根本性挑战，因为长文档的长度、分散证据和复杂结构需要超越标准段落级技术的专门方法。&lt;h4&gt;目的&lt;/h4&gt;提供长文档检索的综合参考和前瞻性议程，整合三个主要时代的方法、挑战和应用，推动基础模型时代的长文档检索发展。&lt;h4&gt;方法&lt;/h4&gt;系统化长文档检索模型的演变过程，包括经典词汇模型、早期神经模型、现代预训练模型(PLM)和大型语言模型(LLMs)，涵盖段落聚合、层次编码、高效注意力和LLM驱动的重新排序和检索技术。&lt;h4&gt;主要发现&lt;/h4&gt;长文档检索需要专门方法；领域存在从早期模型到现代预训练和大型语言模型的演变；存在领域特定应用和专门的评估资源。&lt;h4&gt;结论&lt;/h4&gt;长文档检索是一个重要且快速发展的领域，但仍面临效率权衡、多模态对齐和忠实性等关键开放挑战。&lt;h4&gt;翻译&lt;/h4&gt;长文档的激增对信息检索(IR)提出了根本性挑战，因为它们的长度、分散的证据和复杂的结构需要超越标准段落级技术的专门方法。本综述首次全面处理了长文档检索(LDR)，整合了三个主要时代的方法、挑战和应用。我们将从经典词汇和早期神经模型到现代预训练(PLM)和大型语言模型(LLMs)的演变系统化，涵盖了段落聚合、层次编码、高效注意力和最新的LLM驱动的重新排序和检索技术等关键范式。除了模型，我们还回顾了领域特定应用、专门的评估资源，并概述了关键的开放挑战，如效率权衡、多模态对齐和忠实性。本综述旨在为推进基础模型时代的长文档检索提供综合参考和前瞻性议程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of long-form documents presents a fundamental challenge toinformation retrieval (IR), as their length, dispersed evidence, and complexstructures demand specialized methods beyond standard passage-level techniques.This survey provides the first comprehensive treatment of long-documentretrieval (LDR), consolidating methods, challenges, and applications acrossthree major eras. We systematize the evolution from classical lexical and earlyneural models to modern pre-trained (PLM) and large language models (LLMs),covering key paradigms like passage aggregation, hierarchical encoding,efficient attention, and the latest LLM-driven re-ranking and retrievaltechniques. Beyond the models, we review domain-specific applications,specialized evaluation resources, and outline critical open challenges such asefficiency trade-offs, multimodal alignment, and faithfulness. This survey aimsto provide both a consolidated reference and a forward-looking agenda foradvancing long-document retrieval in the era of foundation models.</description>
      <author>example@mail.com (Minghan Li, Miyang Luo, Tianrui Lv, Yishuai Zhang, Siqi Zhao, Ercong Nie, Guodong Zhou)</author>
      <guid isPermaLink="false">2509.07759v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Bias in Gender Bias Benchmarks: How Spurious Features Distort Evaluation</title>
      <link>http://arxiv.org/abs/2509.07596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究视觉语言基础模型中的性别偏见评估问题，发现现有基准中的虚假特征会显著影响偏见评估结果&lt;h4&gt;背景&lt;/h4&gt;视觉语言基础模型(VLMs)存在性别偏见问题，通常使用带有性别标注的真实图像基准进行评估，但这些基准中常存在性别特征与非性别特征之间的虚假相关性&lt;h4&gt;目的&lt;/h4&gt;探究非性别特征的虚假相关性是否会影响性别偏见的评估结果&lt;h4&gt;方法&lt;/h4&gt;在四个广泛使用的基准(COCO-gender, FACET, MIAP, 和 PHASE)和各种VLMs中系统地改变非性别特征，量化它们对偏见评估的影响&lt;h4&gt;主要发现&lt;/h4&gt;即使是最小的扰动（如仅遮蔽10%的物体或弱模糊背景）也会显著改变偏见分数，在生成式VLMs中使指标变化高达175%，在CLIP变体中变化43%&lt;h4&gt;结论&lt;/h4&gt;当前偏见评估往往反映的是模型对虚假特征的响应而非真正的性别偏见，建议同时报告偏见指标和特征敏感度测量以提高评估可靠性&lt;h4&gt;翻译&lt;/h4&gt;视觉语言基础模型中的性别偏见引发了对其安全部署的担忧，通常使用带有真实图像性别标注的基准进行评估。然而，由于这些基准中常存在性别特征与非性别特征（如物体和背景）之间的虚假相关性，我们识别出性别偏见评估中的一个关键疏忽：这些虚假特征是否会扭曲性别偏见评估？为解决这一问题，我们在四个广泛使用的基准和各种VLMs中系统地改变非性别特征，以量化它们对偏见评估的影响。我们的发现表明，即使是最小的扰动，如仅遮蔽10%的物体或弱模糊背景，也会显著改变偏见分数，在生成式VLMs中使指标变化高达175%，在CLIP变体中变化43%。这表明当前的偏见评估往往反映的是模型对虚假特征的响应，而非性别偏见，这降低了它们的可靠性。由于创建无虚假特征的基准具有根本性挑战，我们建议在报告偏见指标的同时报告特征敏感度测量，以实现更可靠的偏见评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gender bias in vision-language foundation models (VLMs) raises concerns abouttheir safe deployment and is typically evaluated using benchmarks with genderannotations on real-world images. However, as these benchmarks often containspurious correlations between gender and non-gender features, such as objectsand backgrounds, we identify a critical oversight in gender bias evaluation: Dospurious features distort gender bias evaluation? To address this question, wesystematically perturb non-gender features across four widely used benchmarks(COCO-gender, FACET, MIAP, and PHASE) and various VLMs to quantify their impacton bias evaluation. Our findings reveal that even minimal perturbations, suchas masking just 10% of objects or weakly blurring backgrounds, can dramaticallyalter bias scores, shifting metrics by up to 175% in generative VLMs and 43% inCLIP variants. This suggests that current bias evaluations often reflect modelresponses to spurious features rather than gender bias, undermining theirreliability. Since creating spurious feature-free benchmarks is fundamentallychallenging, we recommend reporting bias metrics alongside feature-sensitivitymeasurements to enable a more reliable bias assessment.</description>
      <author>example@mail.com (Yusuke Hirota, Ryo Hachiuma, Boyi Li, Ximing Lu, Michael Ross Boone, Boris Ivanovic, Yejin Choi, Marco Pavone, Yu-Chiang Frank Wang, Noa Garcia, Yuta Nakashima, Chao-Han Huck Yang)</author>
      <guid isPermaLink="false">2509.07596v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Towards Postmortem Data Management Principles for Generative AI</title>
      <link>http://arxiv.org/abs/2509.07375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了AI系统使用用户数据的问题，特别关注已故个人数据的所有权，分析了当前数据管理现状，提出了三条死后数据管理原则，并给出了政策建议。&lt;h4&gt;背景&lt;/h4&gt;基础模型、大型语言模型和智能AI系统严重依赖大量用户数据进行训练，这引发了关于所有权、版权和潜在危害的持续关注。然而，已故个人数据的所有权问题是一个相关但较少被研究的方面。&lt;h4&gt;目的&lt;/h4&gt;探索已故个人数据的所有权权利，分析当前死后数据管理和隐私权状况，提出死后数据管理原则，并为政策制定者和隐私从业者提供建议。&lt;h4&gt;方法&lt;/h4&gt;分析主要科技公司的隐私政策和欧盟AI法案等法规对死后数据管理和隐私权的定义，基于此分析提出死后数据管理原则。&lt;h4&gt;主要发现&lt;/h4&gt;当前在死后数据管理和隐私权方面存在空白，需要明确的原则来保护已故个人的数据权利。&lt;h4&gt;结论&lt;/h4&gt;提出了三条死后数据管理原则来指导保护已故个人数据权利，并建议政策制定者和隐私从业者与技术解决方案一起部署这些原则，使其在实践中可操作和可审计。&lt;h4&gt;翻译&lt;/h4&gt;基础模型、大型语言模型（LLMs）和智能AI系统严重依赖大量用户数据。使用此类数据进行训练引发了关于所有权、版权和潜在危害的持续关注。在这项工作中，我们探讨了一个相关但较少被研究的方面：已故个人数据的所有权权利。我们检查了主要科技公司的隐私政策和欧盟AI法案等法规所定义的死后数据管理和隐私权现状。基于此分析，我们提出了三条死后数据管理原则，以指导保护已故个人数据权利。最后，我们讨论了未来工作的方向，并为政策制定者和隐私从业者提供了关于如何与技术解决方案一起部署这些原则，使其在实践中可操作和可审计的建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, large language models (LLMs), and agentic AI systems relyheavily on vast corpora of user data. The use of such data for training hasraised persistent concerns around ownership, copyright, and potential harms. Inthis work, we explore a related but less examined dimension: the ownershiprights of data belonging to deceased individuals. We examine the currentlandscape of post-mortem data management and privacy rights as defined by theprivacy policies of major technology companies and regulations such as the EUAI Act. Based on this analysis, we propose three post-mortem data managementprinciples to guide the protection of deceased individuals data rights.Finally, we discuss directions for future work and offer recommendations forpolicymakers and privacy practitioners on deploying these principles alongsidetechnological solutions to operationalize and audit them in practice.</description>
      <author>example@mail.com (Ismat Jarin, Elina Van Kempen, Chloe Georgiou)</author>
      <guid isPermaLink="false">2509.07375v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>General Demographic Foundation Models for Enhancing Predictive Performance Across Diseases</title>
      <link>http://arxiv.org/abs/2509.07330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种通用人口统计预训练(GDP)模型，作为针对年龄和性别的基础表示框架，通过探索排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入，提高了模型的区分度、校准度和信息增益，增强了人口统计属性在预测模型中的重要性。&lt;h4&gt;背景&lt;/h4&gt;人口统计属性普遍存在于电子健康记录中，是临床风险分层和治疗决策的重要预测因素，但在模型设计中通常只起辅助作用，很少关注它们的表示学习。&lt;h4&gt;目的&lt;/h4&gt;开发一种基础表示框架，专门用于学习人口统计属性(尤其是年龄和性别)的有效表示，以提高医疗预测模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了通用人口统计预训练(GDP)模型，探索不同排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入，并使用来自不同地理区域的多样化疾病和人口组成的数据集进行预训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;顺序排序显著提高了模型的区分度、校准度和每个决策树分割的信息增益，特别是在年龄和性别对风险分层有显著贡献的疾病中；即使在人口统计属性预测价值相对较低的数据集中，GDP也能增强表示的重要性，增加它们在下游梯度提升模型中的影响。&lt;h4&gt;结论&lt;/h4&gt;表格化人口统计属性的基础模型可以跨任务和人群泛化，为提高医疗保健应用的预测性能提供了有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;人口统计属性普遍存在于电子健康记录中，并在临床风险分层和治疗决策中作为重要的预测因素。尽管它们具有重要意义，但这些属性在模型设计中通常只被赋予辅助角色，很少给予关注来学习它们的表示。本研究提出了一个通用人口统计预训练(GDP)模型作为基础表示框架，专门针对年龄和性别。该模型使用来自不同地理区域的多样化疾病和人口组成的数据集进行预训练和评估。GDP架构探索了排序策略和编码方法的组合，将表格化人口统计输入转换为潜在嵌入。实验结果表明，顺序排序显著提高了模型在区分度、校准度以及每个决策树分割的相应信息增益方面的性能，特别是在年龄和性别对风险分层有显著贡献的疾病中。即使在人口统计属性具有相对较低预测价值的数据集中，GDP也能增强表示的重要性，增加它们在下游梯度提升模型中的影响。研究结果表明，表格化人口统计属性的基础模型可以跨任务和人群泛化，为提高医疗保健应用的预测性能提供了有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Demographic attributes are universally present in electronic health recordsand serve as vital predictors in clinical risk stratification and treatmentdecisions. Despite their significance, these attributes are often relegated toauxiliary roles in model design, with limited attention has been given tolearning their representations. This study proposes a General DemographicPre-trained (GDP) model as a foundational representation framework tailored toage and gender. The model is pre-trained and evaluated using datasets withdiverse diseases and population compositions from different geographic regions.The GDP architecture explores combinations of ordering strategies and encodingmethods to transform tabular demographic inputs into latent embeddings.Experimental results demonstrate that sequential ordering substantiallyimproves model performance in discrimination, calibration, and thecorresponding information gain at each decision tree split, particularly indiseases where age and gender contribute significantly to risk stratification.Even in datasets where demographic attributes hold relatively low predictivevalue, GDP enhances the representational importance, increasing their influencein downstream gradient boosting models. The findings suggest that foundationalmodels for tabular demographic attributes can generalize across tasks andpopulations, offering a promising direction for improving predictiveperformance in healthcare applications.</description>
      <author>example@mail.com (Li-Chin Chen, Ji-Tian Sheu, Yuh-Jue Chuang)</author>
      <guid isPermaLink="false">2509.07330v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Of Graphs and Tables: Zero-Shot Node Classification with Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2509.07143v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TabGFM的图基础模型框架，通过将图数据转换为表格格式，利用表格基础模型进行节点分类，在28个真实世界数据集上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;图基础模型(GFMs)虽然在各种图数据上展现出泛化能力，但通常在不能很好代表真实世界图的数据集上训练，限制了其泛化性能。相比之下，表格基础模型(TFMs)不仅在表格预测任务上表现出色，还在时间序列、自然语言处理和计算机视觉等领域显示出强大适用性。&lt;h4&gt;目的&lt;/h4&gt;受表格基础模型的启发，作者重新审视图基础模型的标准视角，将节点分类问题重新表述为表格问题，使TFMs能够通过上下文学习直接执行零样本节点分类。&lt;h4&gt;方法&lt;/h4&gt;TabGFM框架首先通过特征和结构编码器将图转换为表格，然后应用多个TFMs到多样本采样的表格上，最后通过集成选择聚合它们的输出。&lt;h4&gt;主要发现&lt;/h4&gt;在28个真实世界数据集上的实验表明，TabGFM在特定任务的图神经网络(GNNs)和最先进的图基础模型上取得了一致的改进。&lt;h4&gt;结论&lt;/h4&gt;表格重新表述方法为可扩展和可泛化的图学习提供了新的可能性，展示了将图数据转换为表格格式以利用表格基础模型优势的潜力。&lt;h4&gt;翻译&lt;/h4&gt;图基础模型(GFMs)最近出现，作为一种很有前景的范式，可以在各种图数据上实现广泛的泛化。然而，现有的GFMs通常在不能很好地表示真实世界图的数据集上进行训练，这限制了它们的泛化性能。相比之下，表格基础模型(TFMs)不仅在经典的表格预测任务上表现出色，还在其他领域如时间序列预测、自然语言处理和计算机视觉中显示出强大的适用性。受此启发，我们对GFMs的标准观点采取了另一种视角，并将节点分类重新表述为表格问题。每个节点可以表示为行，特征、结构和标签信息作为列，使TFMs能够通过上下文学习直接执行零样本节点分类。在这项工作中，我们介绍了TabGFM，这是一种图基础模型框架，它首先通过特征和结构编码器将图转换为表格，应用多个TFMs到多样本采样的表格上，然后通过集成选择聚合它们的输出。在28个真实世界数据集上的实验中，TabGFM在特定任务的GNNs和最先进的GFMs上取得了一致的改进，突显了表格重新表述对于可扩展和可泛化的图学习的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph foundation models (GFMs) have recently emerged as a promising paradigmfor achieving broad generalization across various graph data. However, existingGFMs are often trained on datasets that were shown to poorly representreal-world graphs, limiting their generalization performance. In contrast,tabular foundation models (TFMs) not only excel at classical tabular predictiontasks but have also shown strong applicability in other domains such as timeseries forecasting, natural language processing, and computer vision. Motivatedby this, we take an alternative view to the standard perspective of GFMs andreformulate node classification as a tabular problem. Each node can berepresented as a row with feature, structure, and label information as columns,enabling TFMs to directly perform zero-shot node classification via in-contextlearning. In this work, we introduce TabGFM, a graph foundation model frameworkthat first converts a graph into a table via feature and structural encoders,applies multiple TFMs to diversely subsampled tables, and then aggregates theiroutputs through ensemble selection. Through experiments on 28 real-worlddatasets, TabGFM achieves consistent improvements over task-specific GNNs andstate-of-the-art GFMs, highlighting the potential of tabular reformulation forscalable and generalizable graph learning.</description>
      <author>example@mail.com (Adrian Hayler, Xingyue Huang, İsmail İlkan Ceylan, Michael Bronstein, Ben Finkelshtein)</author>
      <guid isPermaLink="false">2509.07143v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Interleaving Reasoning for Better Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2509.06945v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为交错推理生成(IRG)的新框架，通过在文本思考和图像合成之间交替进行来改进文本到图像(T2I)生成能力。研究还提出了交错推理生成学习(IRGL)方法和IRGL-300K数据集，通过两阶段训练实现了在多个评估指标上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;统一的多模态理解和生成模型在图像生成能力方面取得了显著进步，但在遵循指令和细节保留方面与紧密耦合理解与生成的系统(如GPT-4o)相比仍有较大差距。受最近交错推理进展的启发，研究者探索了这种推理是否能进一步改进文本到图像的生成。&lt;h4&gt;目的&lt;/h4&gt;探索交错推理是否能改进文本到图像(T2I)生成，以缩小统一多模态模型与紧密耦合理解与生成的系统之间的差距，特别是在遵循指令和细节保留方面。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了交错推理生成(IRG)框架，模型交替进行文本思考和图像合成；2. 提出了交错推理生成学习(IRGL)方法，包含两个子目标：强化初始思考和生成阶段，以及实现高质量的文本反思和后续图像中的忠实实现；3. 构建了IRGL-300K数据集，包含六种分解的学习模式；4. 采用两阶段训练：首先构建强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效调整IRG管道。&lt;h4&gt;主要发现&lt;/h4&gt;在多个评估指标(GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN)上取得了5-10个百分点的绝对提升，在视觉质量和细粒度保真度方面有显著改进，实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;交错推理框架能够有效提升文本到图像生成能力，特别是在遵循指令和细节保留方面，缩小了统一多模态模型与紧密耦合系统之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;统一多模态理解和生成模型最近在图像生成能力方面取得了显著改进，然而与紧密耦合理解与生成的系统(如GPT-4o)相比，在遵循指令和细节保留方面仍存在较大差距。受最近交错推理进展的启发，我们探索了这种推理是否能进一步改进文本到图像(T2I)生成。我们引入了交错推理生成(IRG)，这是一个在文本思考和图像合成之间交替的框架：模型首先产生基于文本的思考来引导初始图像，然后反思结果以细化细粒度细节、视觉质量和美学，同时保持语义。为了有效训练IRG，我们提出了交错推理生成学习(IRGL)，它针对两个子目标：(1)强化初始思考和生成阶段以建立核心内容和基础质量，(2)在后续图像中实现高质量的文本反思和对这些调整的忠实执行。我们整理了IRGL-300K，这是一个组织成六种分解学习模式的数据集，共同涵盖了基于文本的思考和完整的思考-图像轨迹学习。从一个原生输出交错文本-图像输出的统一基础模型开始，我们的两阶段训练首先构建强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效调整IRG管道。大量实验显示了最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN上获得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也有显著改进。代码、模型权重和数据集将在以下地址发布：https://github.com/Osilly/Interleaving-Reasoning-Generation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal understanding and generation models recently have achievesignificant improvement in image generation capability, yet a large gap remainsin instruction following and detail preservation compared to systems thattightly couple comprehension with generation such as GPT-4o. Motivated byrecent advances in interleaving reasoning, we explore whether such reasoningcan further improve Text-to-Image (T2I) generation. We introduce InterleavingReasoning Generation (IRG), a framework that alternates between text-basedthinking and image synthesis: the model first produces a text-based thinking toguide an initial image, then reflects on the result to refine fine-graineddetails, visual quality, and aesthetics while preserving semantics. To trainIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),which targets two sub-goals: (1) strengthening the initial think-and-generatestage to establish core content and base quality, and (2) enabling high-qualitytextual reflection and faithful implementation of those refinements in asubsequent image. We curate IRGL-300K, a dataset organized into six decomposedlearning modes that jointly cover learning text-based thinking, and fullthinking-image trajectories. Starting from a unified foundation model thatnatively emits interleaved text-image outputs, our two-stage training firstbuilds robust thinking and reflection, then efficiently tunes the IRG pipelinein the full thinking-image trajectory data. Extensive experiments show SoTAperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual qualityand fine-grained fidelity. The code, model weights and datasets will bereleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description>
      <author>example@mail.com (Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin)</author>
      <guid isPermaLink="false">2509.06945v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>SAM$^{*}$: Task-Adaptive SAM with Physics-Guided Rewards</title>
      <link>http://arxiv.org/abs/2509.07047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于奖励函数的优化方法，用于微调基础模型（以SAM为例），解决了基础模型参数不透明且需要大量手动优化的问题，提高了模型在实时流数据分割中的性能。&lt;h4&gt;背景&lt;/h4&gt;图像分割是显微分析中的关键任务，可通过自定义模型、迁移学习或基础模型实现。然而，基础模型通常包含大量不透明的调优参数，需要广泛的手动优化，限制了其在实时流数据分析中的应用。&lt;h4&gt;目的&lt;/h4&gt;引入基于奖励函数的优化方法来微调基础模型，以Meta的SAM框架为例，提高其在显微图像分割中的适应性和性能，特别是实现实时流数据分割。&lt;h4&gt;方法&lt;/h4&gt;构建代表成像系统物理特性的奖励函数，包括粒子尺寸分布、几何形状和其他标准，并将奖励驱动的优化框架整合到SAM模型中，创建优化变体SAM*。&lt;h4&gt;主要发现&lt;/h4&gt;通过奖励函数优化，开发了SAM*模型，该模型更好地满足多样化分割任务的需求，并特别支持实时流数据分割，在显微成像中表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于奖励函数的优化方法有效提升了基础模型在显微图像分割中的性能，精确分割对分析细胞结构、材料界面和纳米级特征至关重要。&lt;h4&gt;翻译&lt;/h4&gt;图像分割是显微学中的关键任务，对于准确分析和解释复杂的视觉数据至关重要。这项任务可以使用在特定领域数据集上训练的自定义模型、从预训练模型迁移学习，或提供广泛适用性的基础模型来完成。然而，基础模型通常呈现大量不透明的调优参数，需要广泛的手动优化，限制了它们对实时流数据分析的可用性。在此，我们引入了基于奖励函数的优化方法来微调基础模型，并通过Meta的SAM（Segment Anything Model）框架说明这种方法。奖励函数可以被构建来表示成像系统的物理特性，包括粒子尺寸分布、几何形状和其他标准。通过整合奖励驱动的优化框架，我们提高了SAM的适应性和性能，从而产生了一个优化变体SAM*，它更好地满足了多样化分割任务的需求，并特别允许实时流数据分割。我们在显微成像中证明了这种方法的有效性，其中精确分割对于分析细胞结构、材料界面和纳米级特征至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image segmentation is a critical task in microscopy, essential for accuratelyanalyzing and interpreting complex visual data. This task can be performedusing custom models trained on domain-specific datasets, transfer learning frompre-trained models, or foundational models that offer broad applicability.However, foundational models often present a considerable number ofnon-transparent tuning parameters that require extensive manual optimization,limiting their usability for real-time streaming data analysis. Here, weintroduce a reward function-based optimization to fine-tune foundational modelsand illustrate this approach for SAM (Segment Anything Model) framework byMeta. The reward functions can be constructed to represent the physics of theimaged system, including particle size distributions, geometries, and othercriteria. By integrating a reward-driven optimization framework, we enhanceSAM's adaptability and performance, leading to an optimized variant, SAM$^{*}$,that better aligns with the requirements of diverse segmentation tasks andparticularly allows for real-time streaming data segmentation. We demonstratethe effectiveness of this approach in microscopy imaging, where precisesegmentation is crucial for analyzing cellular structures, material interfaces,and nanoscale features.</description>
      <author>example@mail.com (Kamyar Barakati, Utkarsh Pratiush, Sheryl L. Sanchez, Aditya Raghavan, Delia J. Milliron, Mahshid Ahmadi, Philip D. Rack, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2509.07047v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
      <link>http://arxiv.org/abs/2509.06685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Withdrawn due to an error in the author list &amp; incomplete  experimental results&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架，通过结合视觉惯性运动恢复结构的精确稀疏深度和大型基础模型的密集粗糙深度，实现了高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统高斯溅射技术需要精确的深度信息来初始化高斯椭球体，但RGB-D/立体相机的深度传感范围有限，难以在大场景中应用。单目图像缺乏深度信息指导，导致新视角合成结果不佳。虽然大型基础模型可用于单目深度估计，但存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索的歧义等问题。&lt;h4&gt;目的&lt;/h4&gt;从单目RGB输入生成密集、精确的深度图像，用于高清晰度高斯溅射渲染。&lt;h4&gt;方法&lt;/h4&gt;利用视觉惯性运动恢复结构(SfM)提供的精确但稀疏的深度信息来优化大型基础模型(LFMs)提供的密集但粗糙的深度信息。提出对象分割深度传播算法渲染结构化对象像素的深度，并开发动态深度优化模块处理动态对象的损坏SfM深度，优化粗糙的LFM深度。&lt;h4&gt;主要发现&lt;/h4&gt;结合精确稀疏深度和密集粗糙深度的方法能够有效解决大场景中新视角合成的问题，提高渲染质量。&lt;h4&gt;结论&lt;/h4&gt;VIM-GS框架在大场景新视角合成中表现出优越的渲染质量，证明了所提方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架。高斯溅射通常需要精确的深度来使用RGB-D/立体相机初始化高斯椭球体。它们有限的深度传感范围使得高斯溅射难以在大场景中工作。然而，单目图像缺乏深度来指导学习，导致新视角合成结果不佳。尽管有可用于单目深度估计的大型基础模型，但它们存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索的歧义等问题。本文旨在从单目RGB输入生成密集、精确的深度图像，用于高清晰度高斯溅射渲染。关键思路是利用视觉惯性运动恢复结构(SfM)的精确但稀疏的深度来优化大型基础模型(LFMs)的密集但粗糙的深度。为了连接稀疏输入和密集输出，我们提出了一种对象分割深度传播算法，该算法渲染结构化对象像素的深度。然后我们开发了一个动态深度优化模块来处理动态对象的损坏SfM深度，并优化粗糙的LFM深度。使用公共和定制数据集的实验证明了VIM-GS在大场景中的优越渲染质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; VIM-GS is a Gaussian Splatting (GS) framework using monocular images fornovel-view synthesis (NVS) in large scenes. GS typically requires accuratedepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limiteddepth sensing range makes it difficult for GS to work in large scenes.Monocular images, however, lack depth to guide the learning and lead toinferior NVS results. Although large foundation models (LFMs) for monoculardepth estimation are available, they suffer from cross-frame inconsistency,inaccuracy for distant scenes, and ambiguity in deceptive texture cues. Thispaper aims to generate dense, accurate depth images from monocular RGB inputsfor high-definite GS rendering. The key idea is to leverage the accurate butsparse depth from visual-inertial Structure-from-Motion (SfM) to refine thedense but coarse depth from LFMs. To bridge the sparse input and dense output,we propose an object-segmented depth propagation algorithm that renders thedepth of pixels of structured objects. Then we develop a dynamic depthrefinement module to handle the crippled SfM depth of dynamic objects andrefine the coarse LFM depth. Experiments using public and customized datasetsdemonstrate the superior rendering quality of VIM-GS in large scenes.</description>
      <author>example@mail.com (Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu)</author>
      <guid isPermaLink="false">2509.06685v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>HU-based Foreground Masking for 3D Medical Masked Image Modeling</title>
      <link>http://arxiv.org/abs/2509.07534v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by MICCAI AMAI Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于HU的前景掩码方法，改进了掩码图像建模在3D医学图像计算中的应用，通过关注内脏器官的强度分布而非随机掩码，显著提高了医学图像分割的性能。&lt;h4&gt;背景&lt;/h4&gt;虽然掩码图像建模(MIM)已经彻底改变了计算机视觉领域，但在3D医学图像计算中的应用一直受到随机掩码使用的限制，这种方法忽略了解剖物体的密度。&lt;h4&gt;目的&lt;/h4&gt;通过一个简单而有效的掩码策略来增强预训练任务，解决随机掩码在医学图像处理中的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用HU(亨氏单位)测量，实现基于HU的前景掩码，专注于内脏器官的强度分布，排除缺乏诊断意义特征的区域，如空气和流体等非组织区域。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共3D医学影像数据集上的广泛实验表明，提出的掩码策略在分割质量和Dice分数方面持续提高了性能(BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%)。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了以领域为中心的MIM的重要性，并为医学图像分割的表示学习指出了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;虽然掩码图像建模(MIM)已经彻底改变了计算机视觉领域，但在3D医学图像计算中的应用一直受到随机掩码使用的限制，这种方法忽略了解剖物体的密度。为了解决这一限制，我们通过一个简单而有效的掩码策略来增强预训练任务。利用HU(亨氏单位)测量，我们实现了基于HU的前景掩码，它专注于内脏器官的强度分布，排除了缺乏诊断意义特征的区域，如空气和流体等非组织区域。在五个公共3D医学影像数据集上的广泛实验表明，我们的掩码策略在分割质量和Dice分数方面持续提高了性能(BTCV:~84.64%, Flare22:~92.43%, MM-WHS:~90.67%, Amos22:~88.64%, BraTS:~78.55%)。这些结果强调了以领域为中心的MIM的重要性，并为医学图像分割的表示学习指出了一个有前景的方向。实现可在github.com/AISeedHub/SubFore/获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决将掩码图像建模方法有效应用于3D医学图像的问题。传统方法使用随机掩码策略，忽略了医学图像中解剖结构的密度特点。这个问题很重要，因为医学图像（如CT扫描）包含大量缺乏诊断意义的背景区域（如空气和流体），随机掩码会导致模型在预训练时关注不相关信息，而不是包含诊断价值的解剖结构，影响最终分割效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析医学图像与自然图像的差异，发现CT扫描的HU值分布特点：低HU值（&lt;0）通常对应背景区域，而高HU值对应有诊断意义的解剖结构。他们比较了前景和背景区域的信息量，发现前景区域包含更多诊断相关信息。作者借鉴了现有的掩码图像建模框架（如MAE和SimMIM）和3D体积处理方法（如MAE3D），但没有直接照搬，而是根据医学图像特点进行了创新性改进，设计了基于HU值的前景掩码策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用医学图像中HU值的分布特点，只掩码包含诊断信息的区域（HU值在[0.1,1]范围内），排除低HU值背景区域（如空气和流体），使模型在预训练时专注于有意义的解剖结构。整体实现流程包括：1)子体积划分：将原始3D医学图像体积划分为多个16×16×16的小子体积；2)前景掩码：计算每个子体积的平均HU值，只掩码平均HU值≥0.1的子体积；3)预训练任务：使用掩码后的体积作为输入，训练模型重建原始体积；4)下游任务：将预训练好的模型应用于医学图像分割任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将HU值信息整合到掩码图像建模框架中；2)提出简单而有效的前景掩码方法，专注于解剖结构而非随机区域；3)通过大量实验验证了方法在多个医学图像数据集上的有效性。相比之前的工作，不同之处在于：传统MIM方法使用随机掩码，而本文基于HU值进行语义感知的掩码；现有医学图像MIM方法通常直接从自然图像领域迁移，没有考虑医学图像的特殊性；本文方法通过分析医学图像的强度分布，有选择地掩码信息丰富的区域，在多个数据集上表现优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于HU值的前景掩码策略，通过专注于医学图像中有诊断意义的解剖结构而非随机掩码，显著提高了3D医学图像分割任务的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Masked Image Modeling (MIM) has revolutionized fields of computervision, its adoption in 3D medical image computing has been limited by the useof random masking, which overlooks the density of anatomical objects. Toaddress this limitation, we enhance the pretext task with a simple yeteffective masking strategy. Leveraging Hounsfield Unit (HU) measurements, weimplement an HU-based Foreground Masking, which focuses on the intensitydistribution of visceral organs and excludes non-tissue regions, such as airand fluid, that lack diagnostically meaningful features. Extensive experimentson five public 3D medical imaging datasets demonstrate that our maskingconsistently improves performance, both in quality of segmentation and Dicescore (BTCV:~84.64\%, Flare22:~92.43\%, MM-WHS:~90.67\%, Amos22:~88.64\%,BraTS:~78.55\%). These results underscore the importance of domain-centric MIMand suggest a promising direction for representation learning in medical imagesegmentation. Implementation is available at github.com/AISeedHub/SubFore/.</description>
      <author>example@mail.com (Jin Lee, Vu Dang, Gwang-Hyun Yu, Anh Le, Zahid Rahman, Jin-Ho Jang, Heonzoo Lee, Kun-Yung Kim, Jin-Sul Kim, Jin-Young Kim)</author>
      <guid isPermaLink="false">2509.07534v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>FLeW: Facet-Level and Adaptive Weighted Representation Learning of Scientific Documents</title>
      <link>http://arxiv.org/abs/2509.07531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;科学文档表示学习当前面临三个主要挑战：基于引用结构的对比训练方法未能充分利用引用信息；细粒度表示学习方法需要昂贵的集成且缺乏领域泛化；任务感知学习方法依赖于手动预定义的任务分类且需要额外训练数据。为解决这些问题，作者提出了FLeW方法，统一了三种方法，通过引入三元组采样方法增强引用结构信号，利用引用意图进行细粒度表示学习，并通过简单权重搜索自适应集成嵌入。实验表明FLeW在多个科学任务和领域中具有适用性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前科学文档表示学习方法面临三个主要挑战：1)基于引用结构的对比训练方法未能充分利用引用信息，仍然生成单一向量表示；2)细粒度表示学习方法在句子或方面级别生成多个向量，需要昂贵的集成且缺乏领域泛化能力；3)任务感知学习方法依赖于手动预定义的任务分类，忽略了细微的任务区别，并为任务特定模块需要额外的训练数据。&lt;h4&gt;目的&lt;/h4&gt;解决科学文档表示学习中的三个挑战，提出一种统一三种方法的新方法FLeW，以获得更好的文档表示。&lt;h4&gt;方法&lt;/h4&gt;FLeW方法引入了一种新的三元组采样方法，利用引用意图和频率增强引用结构信号进行训练；利用引用意图(背景、方法、结果)与科学写作结构相一致的特点，促进细粒度表示学习的领域通用方面划分；采用简单的权重搜索来自适应地将三个方面级别的嵌入集成到任务特定的文档嵌入中，无需任务感知的微调。&lt;h4&gt;主要发现&lt;/h4&gt;FLeW方法在多个科学任务和领域中表现出适用性和鲁棒性，优于先前的模型。&lt;h4&gt;结论&lt;/h4&gt;FLeW方法通过统一三种表示学习方法的优点，解决了现有方法的局限性，为科学文档表示学习提供了更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;科学文档表示学习为各种任务提供了强大的嵌入，而当前方法在三种方法中面临挑战。1)基于引用结构的对比训练未能充分利用引用信息，仍然生成单一向量表示。2)细粒度表示学习在句子或方面级别生成多个向量，需要昂贵的集成且缺乏领域泛化能力。3)任务感知学习依赖于手动预定义的任务分类，忽略了细微的任务区别，并为任务特定模块需要额外的训练数据。为解决这些问题，我们提出了一种统一三种方法以获得更好表示的新方法，即FLeW。具体来说，我们引入了一种新的三元组采样方法，利用引用意图和频率来增强用于训练的引用结构信号。引用意图(背景、方法、结果)与科学写作的一般结构相一致，促进细粒度表示学习的领域通用方面划分。然后，我们采用简单的权重搜索来自适应地将三个方面级别的嵌入集成到任务特定的文档嵌入中，无需任务感知的微调。实验表明，与先前的模型相比，FLeW在多个科学任务和领域中具有适用性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific document representation learning provides powerful embeddings forvarious tasks, while current methods face challenges across three approaches.1) Contrastive training with citation-structural signals underutilizes citationinformation and still generates single-vector representations. 2) Fine-grainedrepresentation learning, which generates multiple vectors at the sentence oraspect level, requires costly integration and lacks domain generalization. 3)Task-aware learning depends on manually predefined task categorization,overlooking nuanced task distinctions and requiring extra training data fortask-specific modules. To address these problems, we propose a new method thatunifies the three approaches for better representations, namely FLeW.Specifically, we introduce a novel triplet sampling method that leveragescitation intent and frequency to enhance citation-structural signals fortraining. Citation intents (background, method, result), aligned with thegeneral structure of scientific writing, facilitate a domain-generalized facetpartition for fine-grained representation learning. Then, we adopt a simpleweight search to adaptively integrate three facet-level embeddings into atask-specific document embedding without task-aware fine-tuning. Experimentsshow the applicability and robustness of FLeW across multiple scientific tasksand fields, compared to prior models.</description>
      <author>example@mail.com (Zheng Dou, Deqing Wang, Fuzhen Zhuang, Jian Ren, Yanlin Hu)</author>
      <guid isPermaLink="false">2509.07531v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Kernel VICReg for Self-Supervised Learning in Reproducing Kernel Hilbert Space</title>
      <link>http://arxiv.org/abs/2509.07289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出Kernel VICReg，一种将VICReg目标提升到再生核希尔伯特空间的自监督学习框架，通过核化损失函数实现非线性特征学习，在多个数据集上展示了一致的性能提升。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为强大的表征学习范式，通过优化几何目标而无需标签，但现有方法主要在欧几里得空间中操作，限制了捕捉非线性依赖和几何结构的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在非线性空间中操作的自监督学习框架，以捕捉数据中的非线性依赖和几何结构，提高在复杂数据或小规模数据上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出Kernel VICReg框架，将VICReg目标提升到再生核希尔伯特空间，通过核化损失函数的各个项（方差、不变性和协方差），实现在双中心核矩阵和希尔伯特-施密特范数上操作，进行非线性特征学习而无需显式映射。&lt;h4&gt;主要发现&lt;/h4&gt;Kernel VICReg避免了表示崩溃；在复杂或小规模数据任务上提高了性能；在MNIST、CIFAR-10、STL-10、TinyImageNet和ImageNetNet上展示了一致的性能提升；在非线性结构突出的数据集上改进尤为显著；UMAP可视化证实基于核的嵌入表现出更好的等距性和类分离。&lt;h4&gt;结论&lt;/h4&gt;将自监督学习目标核化是将经典核方法与现代表征学习相结合的有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为一种强大的表征学习范式，通过优化几何目标（如对增强的不变性、方差保持和特征解相关）而无需标签。然而，大多数现有方法在欧几里得空间中操作，限制了它们捕捉非线性依赖和几何结构的能力。在这项工作中，我们提出了Kernel VICReg，一种新颖的自监督学习框架，将VICReg目标提升到再生核希尔伯特空间中。通过核化损失函数的各个项（方差、不变性和协方差），我们获得了一种在双中心核矩阵和希尔伯特-施密特范数上操作的一般公式，能够进行非线性特征学习而无需显式映射。我们证明，Kernel VICReg不仅避免了表示崩溃，还在具有复杂或小规模数据的任务上提高了性能。在多个数据集上的经验评估显示，相比欧几里得VICReg有一致的性能提升，特别是在非线性结构突出的数据集上改进尤为显著。UMAP可视化进一步证实，基于核的嵌入表现出更好的等距性和类分离。我们的结果表明，将自监督学习目标核化是将经典核方法与现代表征学习相结合的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm forrepresentation learning by optimizing geometric objectives--such as invarianceto augmentations, variance preservation, and feature decorrelation--withoutrequiring labels. However, most existing methods operate in Euclidean space,limiting their ability to capture nonlinear dependencies and geometricstructures. In this work, we propose Kernel VICReg, a novel self-supervisedlearning framework that lifts the VICReg objective into a Reproducing KernelHilbert Space (RKHS). By kernelizing each term of the loss-variance,invariance, and covariance--we obtain a general formulation that operates ondouble-centered kernel matrices and Hilbert-Schmidt norms, enabling nonlinearfeature learning without explicit mappings.  We demonstrate that Kernel VICReg not only avoids representational collapsebut also improves performance on tasks with complex or small-scale data.Empirical evaluations across MNIST, CIFAR-10, STL-10, TinyImageNet, andImageNet100 show consistent gains over Euclidean VICReg, with particularlystrong improvements on datasets where nonlinear structures are prominent. UMAPvisualizations further confirm that kernel-based embeddings exhibit betterisometry and class separation. Our results suggest that kernelizing SSLobjectives is a promising direction for bridging classical kernel methods withmodern representation learning.</description>
      <author>example@mail.com (M. Hadi Sepanj, Benyamin Ghojogh, Paul Fieguth)</author>
      <guid isPermaLink="false">2509.07289v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Fed-REACT: Federated Representation Learning for Heterogeneous and Evolving Data</title>
      <link>http://arxiv.org/abs/2509.07198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fed-REACT是一种针对异构且演变客户端数据的联邦学习框架，通过结合表示学习和进化聚类的两阶段过程，提高了模型的准确性和鲁棒性&lt;h4&gt;背景&lt;/h4&gt;集中式机器学习存在高资源成本和隐私问题，联邦学习(FL)作为替代方案允许客户端协作训练全局模型同时保持数据本地化。然而，实际部署中客户端数据分布随时间演变且在不同客户端间差异显著，这种异质性降低了标准FL算法的性能&lt;h4&gt;目的&lt;/h4&gt;提出一种针对异构且演变的客户端数据的联邦学习框架&lt;h4&gt;方法&lt;/h4&gt;引入Fed-REACT框架，结合表示学习和进化聚类的两阶段过程：(1)第一阶段：每个客户端学习局部模型从其数据中提取特征表示；(2)第二阶段：服务器基于这些表示动态地将客户端分组为集群，并协调集群特定任务的模型训练&lt;h4&gt;主要发现&lt;/h4&gt;对表示学习阶段提供了理论分析，实验证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性&lt;h4&gt;结论&lt;/h4&gt;Fed-REACT是处理异构且演变客户端数据的有效联邦学习框架&lt;h4&gt;翻译&lt;/h4&gt;受集中式机器学习相关的高资源成本和隐私问题启发，联邦学习(FL)已成为一种高效的替代方案，使客户端能够在保持数据本地化的同时协作训练全局模型。然而，在实际部署中，客户端数据分布通常随时间演变且在不同客户端间存在显著差异，引入了异质性，降低了标准FL算法的性能。在这项工作中，我们引入了Fed-REACT，一种为异构且演变的客户端数据设计的联邦学习框架。Fed-REACT将表示学习与进化聚类结合在一个两阶段过程中：(1)在第一阶段，每个客户端学习一个局部模型，从其数据中提取特征表示；(2)在第二阶段，服务器基于这些表示动态地将客户端分组为集群，并协调针对下游目标（如分类或回归）的集群特定任务模型训练。我们对表示学习阶段提供了理论分析，并通过实证证明Fed-REACT在真实数据集上实现了更高的准确性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivated by the high resource costs and privacy concerns associated withcentralized machine learning, federated learning (FL) has emerged as anefficient alternative that enables clients to collaboratively train a globalmodel while keeping their data local. However, in real-world deployments,client data distributions often evolve over time and differ significantlyacross clients, introducing heterogeneity that degrades the performance ofstandard FL algorithms. In this work, we introduce Fed-REACT, a federatedlearning framework designed for heterogeneous and evolving client data.Fed-REACT combines representation learning with evolutionary clustering in atwo-stage process: (1) in the first stage, each client learns a local model toextracts feature representations from its data; (2) in the second stage, theserver dynamically groups clients into clusters based on these representationsand coordinates cluster-wise training of task-specific models for downstreamobjectives such as classification or regression. We provide a theoreticalanalysis of the representation learning stage, and empirically demonstrate thatFed-REACT achieves superior accuracy and robustness on real-world datasets.</description>
      <author>example@mail.com (Yiyue Chen, Usman Akram, Chianing Wang, Haris Vikalo)</author>
      <guid isPermaLink="false">2509.07198v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
      <link>http://arxiv.org/abs/2509.06570v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 2025 IEEE/CVF International Conference on  Computer Vision Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为保留性角度表示学习（RARL）的方法用于增量开放集识别（IOSR），解决了现有方法在动态场景中难以维持决策边界区分能力的问题。通过在角度空间中对齐未知表示并采用虚拟内在交互训练策略和分层校正策略，该方法有效减轻了表示漂移和特征空间扭曲，在CIFAR100和TinyImageNet数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的开放集识别方法通常为静态场景设计，模型分类已知类别并识别固定范围内的未知类别，这与从连续数据流中逐步识别新出现的未知类别的期望不符。在动态场景中，由于无法访问之前的训练数据，决策边界的区分能力难以维持，导致严重的类别间混淆。&lt;h4&gt;目的&lt;/h4&gt;解决增量开放集识别中由于无法访问历史训练数据导致的决策边界区分能力下降和类别间混淆问题，提出一种能够有效处理连续数据流中新出现未知类别的方法。&lt;h4&gt;方法&lt;/h4&gt;提出保留性角度表示学习（RARL）方法，包括：1) 在等角紧框架构建的角度空间内，让未知表示围绕非活跃原型对齐，减轻表示漂移；2) 采用虚拟内在交互（VII）训练策略，通过边界接近的虚拟类强制清晰的类别间边界；3) 设计分层校正策略优化决策边界，减轻样本不平衡导致的表示偏差和特征空间扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR100和TinyImageNet数据集上的实验结果表明，所提出的RARL方法在各种任务设置下都达到了最先进的性能，为增量开放集识别建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;RARL方法通过有效的表示学习和边界优化策略，成功解决了增量开放集识别中的关键挑战，为处理连续数据流中的未知类别识别提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;现有的开放集识别方法通常为静态场景设计，模型旨在分类已知类别并识别固定范围内的未知类别。这与模型应该从连续数据流中逐步识别新出现的未知类别并获取相应知识的期望不符。在这样不断变化的场景中，由于无法访问之前的训练数据，开放集识别决策边界的区分能力难以维持，导致严重的类别间混淆。为解决此问题，我们提出了用于增量开放集识别的保留性角度表示学习（RARL）。在RARL中，未知表示被鼓励在等角紧框架构建的角度空间内围绕非活跃原型对齐，从而减轻知识更新过程中的表示漂移。具体来说，我们采用虚拟内在交互（VII）训练策略，通过边界接近的虚拟类强制清晰的类别间边界，从而压缩已知表示。此外，还设计了一种分层校正策略来优化决策边界，减轻由于新旧类别和正负类别样本不平衡导致的表示偏差和特征空间扭曲。我们在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open set recognition (OSR) methods are typically designed for staticscenarios, where models aim to classify known classes and identify unknown oneswithin fixed scopes. This deviates from the expectation that the model shouldincrementally identify newly emerging unknown classes from continuous datastreams and acquire corresponding knowledge. In such evolving scenarios, thediscriminability of OSR decision boundaries is hard to maintain due torestricted access to former training data, causing severe inter-classconfusion. To solve this problem, we propose retentive angular representationlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknownrepresentations are encouraged to align around inactive prototypes within anangular space constructed under the equiangular tight frame, thereby mitigatingexcessive representation drift during knowledge updates. Specifically, we adopta virtual-intrinsic interactive (VII) training strategy, which compacts knownrepresentations by enforcing clear inter-class margins throughboundary-proximal virtual classes. Furthermore, a stratified rectificationstrategy is designed to refine decision boundaries, mitigating representationbias and feature space distortion caused by imbalances between old/new andpositive/negative class samples. We conduct thorough evaluations on CIFAR100and TinyImageNet datasets and establish a new benchmark for IOSR. Experimentalresults across various task setups demonstrate that the proposed methodachieves state-of-the-art performance.</description>
      <author>example@mail.com (Runqing Yang, Yimin Fu, Changyuan Wu, Zhunga Liu)</author>
      <guid isPermaLink="false">2509.06570v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Guided Diffusion Transformer with Spherical Harmonic Posterior Sampling for High-Fidelity Angular Super-Resolution in Diffusion MRI</title>
      <link>http://arxiv.org/abs/2509.07020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Physics-Guided Diffusion Transformer (PGDiT)方法，用于从低角分辨率dMRI数据重建高角分辨率信号，通过整合物理先验知识提高重建质量。&lt;h4&gt;背景&lt;/h4&gt;现有dMRI角超分辨率方法在恢复细粒度角细节和保持高保真度方面存在局限，主要由于q空间几何建模不足和物理约束整合不够充分。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够在训练和推理阶段探索物理先验的模型，实现高质量的高角分辨率dMRI重建。&lt;h4&gt;方法&lt;/h4&gt;训练阶段采用Q-space Geometry-Aware Module (QGAM)结合b向量调制和随机角掩码促进方向感知学习；推理阶段使用两阶段Spherical Harmonics-Guided Posterior Sampling (SHPS)进行从粗到细的细化，确保物理合理重建。&lt;h4&gt;主要发现&lt;/h4&gt;在ASR任务、DTI和NODDI应用上的实验表明，PGDiT在细节恢复和数据保真度方面优于现有深度学习模型。&lt;h4&gt;结论&lt;/h4&gt;PGDiT提供了一种新的生成式ASR框架，能实现高保真度的高角分辨率dMRI重建，在神经科学和临床研究中有应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;扩散磁共振成像(dMRI)角超分辨率(ASR)旨在从有限的低角分辨率(LAR)数据中重建高角分辨率(HAR)信号而不延长扫描时间。然而，现有方法在恢复细粒度角细节或保持高保真度方面存在局限，这是由于对q空间几何建模不足以及物理约束整合不够充分。本文引入了一个Physics-Guided Diffusion Transformer (PGDiT)，旨在探索训练和推理阶段的物理先验知识。在训练阶段，Q-space Geometry-Aware Module (QGAM)结合b向量调制和随机角掩码促进方向感知表示学习，使网络能够从稀疏和嘈杂数据中生成方向一致且具有精细角细节的重建。在推理阶段，两阶段Spherical Harmonics-Guided Posterior Sampling (SHPS)强制与获取数据对齐，然后基于热扩散的SH正则化确保物理合理的重建。这种从粗到细的细化策略减轻了纯数据驱动或生成模型中常见的过度平滑和伪影问题。在ASR任务、DTI和NODDI应用上的大量实验表明，PGDiT在细节恢复和数据保真度方面优于现有深度学习模型。我们的方法提出了一个新的生成式ASR框架，能够提供高保真度的高角分辨率dMRI重建，在神经科学和临床研究中有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion MRI (dMRI) angular super-resolution (ASR) aims to reconstructhigh-angular-resolution (HAR) signals from limited low-angular-resolution (LAR)data without prolonging scan time. However, existing methods are limited inrecovering fine-grained angular details or preserving high fidelity due toinadequate modeling of q-space geometry and insufficient incorporation ofphysical constraints. In this paper, we introduce a Physics-Guided DiffusionTransformer (PGDiT) designed to explore physical priors throughout bothtraining and inference stages. During training, a Q-space Geometry-Aware Module(QGAM) with b-vector modulation and random angular masking facilitatesdirection-aware representation learning, enabling the network to generatedirectionally consistent reconstructions with fine angular details from sparseand noisy data. In inference, a two-stage Spherical Harmonics-Guided PosteriorSampling (SHPS) enforces alignment with the acquired data, followed byheat-diffusion-based SH regularization to ensure physically plausiblereconstructions. This coarse-to-fine refinement strategy mitigatesoversmoothing and artifacts commonly observed in purely data-driven orgenerative models. Extensive experiments on general ASR tasks and twodownstream applications, Diffusion Tensor Imaging (DTI) and Neurite OrientationDispersion and Density Imaging (NODDI), demonstrate that PGDiT outperformsexisting deep learning models in detail recovery and data fidelity. Ourapproach presents a novel generative ASR framework that offers high-fidelityHAR dMRI reconstructions, with potential applications in neuroscience andclinical research.</description>
      <author>example@mail.com (Mu Nan, Taohui Xiao, Ruoyou Wu, Shoujun Yu, Ye Li, Hairong Zheng, Shanshan Wang)</author>
      <guid isPermaLink="false">2509.07020v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
      <link>http://arxiv.org/abs/2509.05685v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MSRFormer，一种新的道路网络表示学习框架，通过整合多尺度空间交互解决城市道路网络的异构性和层次性问题。该框架利用空间流卷积提取小尺度特征，识别尺度依赖的空间交互区域，并通过图Transformer捕获多尺度空间依赖关系。实验表明MSRFormer在道路网络分析任务中优于基线方法，复杂道路网络结构中性能提升可达16%。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异构性和层次性特点对准确表示学习构成了挑战。图神经网络在聚合邻接节点特征时，由于其同质性假设和对单一结构尺度的关注而面临困难。&lt;h4&gt;目的&lt;/h4&gt;为了解决图神经网络在城市道路网络表示学习中的局限性，本文提出了MSRFormer框架，旨在通过整合多尺度空间交互来处理流异质性和长距离依赖问题，从而提高道路网络表示学习的准确性。&lt;h4&gt;方法&lt;/h4&gt;MSRFormer使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度依赖的空间交互区域以捕获道路网络的空间结构和流异质性。采用图Transformer捕获多尺度复杂空间依赖关系，通过残差连接融合空间交互特征，并将融合后的特征输入对比学习算法以获得最终的道路网络表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。与最先进的基线方法相比，在复杂道路网络结构中性能提升可达16%。交通相关任务从整合轨迹数据中获益更多，且规模效应与空间交互流异质性之间存在明显的关联模式。&lt;h4&gt;结论&lt;/h4&gt;该研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互中规模效应与流异质性相互作用之间的不同关联模式。MSRFormer通过多尺度方法有效解决了城市道路网络的异构性和层次性挑战。&lt;h4&gt;翻译&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异构性和层次性特点对准确表示学习构成了挑战。图神经网络在聚合邻接节点特征时，由于其同质性假设和对单一结构尺度的关注而面临困难。为解决这些问题，本文提出了MSRFormer，一种新颖的道路网络表示学习框架，通过解决流异质性和长距离依赖问题整合多尺度空间交互。它使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度依赖的空间交互区域以捕获道路网络的空间结构和流异质性。通过采用图Transformer，MSRFormer有效捕获了多尺度复杂空间依赖关系。空间交互特征通过残差连接融合，然后输入对比学习算法以获得最终的道路网络表示。在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，交通相关任务从整合轨迹数据中获益更多，在复杂道路网络结构中与最具竞争力的基线方法相比性能提升可达16%。该研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互中规模效应与流异质性相互作用之间的不同关联模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description>
      <author>example@mail.com (Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You)</author>
      <guid isPermaLink="false">2509.05685v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Neural Networks for Drug Discovery: Recent Developments and Challenges</title>
      <link>http://arxiv.org/abs/2509.07887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络在药物发现领域得到广泛应用，能够处理药物分子模型等图结构数据，本文全面涵盖了多个研究类别并提供了未来工作指导。&lt;h4&gt;背景&lt;/h4&gt;图神经网络因其处理图结构数据的能力，在药物发现这一复杂领域受到关注。&lt;h4&gt;目的&lt;/h4&gt;为图神经网络在药物发现领域的未来工作提供指导。&lt;h4&gt;方法&lt;/h4&gt;全面回顾和总结图神经网络在药物发现各类研究中的应用，包括分子属性预测、药物-药物相互作用研究等。&lt;h4&gt;主要发现&lt;/h4&gt;图神经网络已应用于药物发现研究的多个类别，包括分子属性预测、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计等。&lt;h4&gt;结论&lt;/h4&gt;图神经网络在药物发现领域具有广泛应用前景，本文为相关研究提供了全面概述和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在药物发现这一复杂领域受到关注，因为它们能够处理药物分子模型等图结构数据。这一方法已在已发表文献中产生了众多方法和模型，涵盖药物发现研究的多个类别。本文全面涵盖了这些研究类别，包括最近的论文，即分子属性预测（包括药物-靶点结合亲和力预测）、药物-药物相互作用研究、微生物组相互作用预测、药物重定位、逆合成和新药设计，并为图神经网络在药物发现领域的未来工作提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have gained traction in the complex domain ofdrug discovery because of their ability to process graph-structured data suchas drug molecule models. This approach has resulted in a myriad of methods andmodels in published literature across several categories of drug discoveryresearch. This paper covers the research categories comprehensively with recentpapers, namely molecular property prediction, including drug-target bindingaffinity prediction, drug-drug interaction study, microbiome interactionprediction, drug repositioning, retrosynthesis, and new drug design, andprovides guidance for future work on GNNs for drug discovery.</description>
      <author>example@mail.com (Katherine Berry, Liang Cheng)</author>
      <guid isPermaLink="false">2509.07887v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>IBN: An Interpretable Bidirectional-Modeling Network for Multivariate Time Series Forecasting with Variable Missing</title>
      <link>http://arxiv.org/abs/2509.07725v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为IBN的可解释双向建模网络，用于解决多变量时间序列预测中的缺失变量问题，结合不确定性感知插值和高斯核图卷积技术，在各种缺失率场景下实现了最先进的预测性能。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测面临缺失变量的挑战，这些缺失变量阻碍了传统时空图神经网络对变量间相关性的建模。现有方法GinAR虽然首次使用基于注意力的插补和自适应图学习处理变量缺失，但缺乏可解释性且无法捕获更多潜在的时间模式。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，提供一个更可靠且可解释的框架来处理缺失变量的多变量时间序列预测问题。&lt;h4&gt;方法&lt;/h4&gt;提出可解释的双向建模网络（IBN），集成不确定性感知插值（UAI）和高斯核图卷积（GGCN）。IBN使用MC Dropout估计重建值的不确定性，并应用不确定性加权策略降低高风险重建。GGCN明确建模变量间的空间相关性，双向RU增强时间依赖建模。&lt;h4&gt;主要发现&lt;/h4&gt;在各种缺失率场景下，IBN实现了最先进的预测性能，为处理缺失变量的多变量时间序列预测提供了更可靠且可解释的框架。&lt;h4&gt;结论&lt;/h4&gt;IBN成功解决了多变量时间序列预测中缺失变量带来的挑战，通过结合不确定性感知插值和高斯核图卷积技术，提供了更可靠和可解释的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列预测（MTSF）通常面临来自缺失变量的挑战，这阻碍了传统的时空图神经网络对变量间相关性的建模。虽然GinAR首次使用基于注意力的插补和自适应图学习来处理变量缺失，但它缺乏可解释性，并且由于其简单的递归单元（RU）而无法捕获更多潜在的时间模式。为了克服这些局限性，我们提出了可解释的双向建模网络（IBN），集成了不确定性感知插值（UAI）和高斯核图卷积（GGCN）。IBN使用MC Dropout估计重建值的不确定性，并应用不确定性加权策略来降低高风险重建。GGCN明确建模变量间的空间相关性，而双向RU增强了时间依赖建模。大量实验表明，在各种缺失率场景下，IBN实现了最先进的预测性能，为具有缺失变量的MTSF提供了更可靠和可解释的框架。代码可在https://github.com/zhangth1211/NICLab-IBN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting (MTSF) often faces challenges frommissing variables, which hinder conventional spatial-temporal graph neuralnetworks in modeling inter-variable correlations. While GinAR addressesvariable missing using attention-based imputation and adaptive graph learningfor the first time, it lacks interpretability and fails to capture more latenttemporal patterns due to its simple recursive units (RUs). To overcome theselimitations, we propose the Interpretable Bidirectional-modeling Network (IBN),integrating Uncertainty-Aware Interpolation (UAI) and Gaussian kernel-basedGraph Convolution (GGCN). IBN estimates the uncertainty of reconstructed valuesusing MC Dropout and applies an uncertainty-weighted strategy to mitigatehigh-risk reconstructions. GGCN explicitly models spatial correlations amongvariables, while a bidirectional RU enhances temporal dependency modeling.Extensive experiments show that IBN achieves state-of-the-art forecastingperformance under various missing-rate scenarios, providing a more reliable andinterpretable framework for MTSF with missing variables. Code is available at:https://github.com/zhangth1211/NICLab-IBN.</description>
      <author>example@mail.com (Shusen Ma, Tianhao Zhang, Qijiu Xia, Yun-Bo Zhao)</author>
      <guid isPermaLink="false">2509.07725v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>DeepGraphLog for Layered Neurosymbolic AI</title>
      <link>http://arxiv.org/abs/2509.07665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepGraphLog的新型神经符号AI框架，解决了现有框架在处理图结构数据时的局限性，实现了更灵活的神经-符号集成。&lt;h4&gt;背景&lt;/h4&gt;神经符号AI旨在结合神经网络的统计优势和符号推理的可解释性，但当前框架如DeepProbLog强制符号推理必须跟随神经处理，限制了它们对复杂依赖关系的建模能力，特别是在处理图等不规则数据结构时。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理复杂依赖关系，特别是图结构数据的NeSy框架，突破现有框架的限制，实现更灵活的神经-符号集成。&lt;h4&gt;方法&lt;/h4&gt;提出DeepGraphLog，将ProbLog扩展为具有图神经谓词，支持多层神经-符号推理，允许神经和符号组件以任意顺序分层，并将符号表示视为图，通过图神经网络处理。&lt;h4&gt;主要发现&lt;/h4&gt;DeepGraphLog在规划、知识图谱补全（远程监督）和GNN表达能力等任务上展示了其能力，能够有效捕获复杂的关系依赖关系，克服了现有NeSy系统的关键局限性。&lt;h4&gt;结论&lt;/h4&gt;DeepGraphLog拓宽了神经符号AI在图结构领域的适用性，提供了一个更具表现力和灵活的神经-符号集成框架。&lt;h4&gt;翻译&lt;/h4&gt;神经符号AI（NeSy）旨在结合神经网络的统计优势与符号推理的可解释性和结构性。然而，当前NeSy框架如DeepProbLog强制符号推理必须跟随神经处理，这限制了它们对复杂依赖关系的建模能力，特别是在处理图等不规则数据结构时。本文介绍了DeepGraphLog，一种新型NeSy框架，将ProbLog扩展为具有图神经谓词。DeepGraphLog支持多层神经-符号推理，允许神经和符号组件以任意顺序分层。与无法通过神经方法处理符号推理的DeepProbLog不同，DeepGraphLog将符号表示视为图，可通过图神经网络（GNN）处理。我们在规划、知识图谱补全（远程监督）和GNN表达能力等任务上展示了DeepGraphLog的能力。结果表明，DeepGraphLog能有效捕获复杂的关系依赖关系，克服了现有NeSy系统的关键局限性。通过拓宽神经符号AI在图结构领域的适用性，DeepGraphLog为神经-符号集成提供了更具表现力和灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neurosymbolic AI (NeSy) aims to integrate the statistical strengths of neuralnetworks with the interpretability and structure of symbolic reasoning.However, current NeSy frameworks like DeepProbLog enforce a fixed flow wheresymbolic reasoning always follows neural processing. This restricts theirability to model complex dependencies, especially in irregular data structuressuch as graphs. In this work, we introduce DeepGraphLog, a novel NeSy frameworkthat extends ProbLog with Graph Neural Predicates. DeepGraphLog enablesmulti-layer neural-symbolic reasoning, allowing neural and symbolic componentsto be layered in arbitrary order. In contrast to DeepProbLog, which cannothandle symbolic reasoning via neural methods, DeepGraphLog treats symbolicrepresentations as graphs, enabling them to be processed by Graph NeuralNetworks (GNNs). We showcase the capabilities of DeepGraphLog on tasks inplanning, knowledge graph completion with distant supervision, and GNNexpressivity. Our results demonstrate that DeepGraphLog effectively capturescomplex relational dependencies, overcoming key limitations of existing NeSysystems. By broadening the applicability of neurosymbolic AI tograph-structured domains, DeepGraphLog offers a more expressive and flexibleframework for neural-symbolic integration.</description>
      <author>example@mail.com (Adem Kikaj, Giuseppe Marra, Floris Geerts, Robin Manhaeve, Luc De Raedt)</author>
      <guid isPermaLink="false">2509.07665v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Graph-based Integrated Gradients for Explaining Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.07648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Australasian Joint Conference on Artificial  Intelligence (AJCAI) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图的集成梯度(GB-IG)方法，将传统的集成梯度(IG)可解释性技术扩展到图结构数据上，解决了IG不适用于离散图结构的问题。&lt;h4&gt;背景&lt;/h4&gt;集成梯度(IG)是一种常见的可解释性技术，用于解决神经网络的黑盒问题，但它假设数据是连续的，而图是离散结构，这使得IG不适用于图数据。&lt;h4&gt;目的&lt;/h4&gt;将集成梯度(IG)方法扩展到图结构数据上，开发一种专门针对图数据的可解释性技术。&lt;h4&gt;方法&lt;/h4&gt;提出基于图的集成梯度(GB-IG)，作为IG在图数据上的扩展方法。&lt;h4&gt;主要发现&lt;/h4&gt;在四个合成数据集上，GB-IG能够准确识别分类任务中使用的图的关键结构组件；在三个 prevalent 现实世界图数据集上，GB-IG在突出显示节点分类任务中的重要特征方面优于传统的IG方法。&lt;h4&gt;结论&lt;/h4&gt;GB-IG是IG在图数据上的有效扩展，能够更好地处理图结构的离散特性，为图神经网络提供了更好的可解释性。&lt;h4&gt;翻译&lt;/h4&gt;集成梯度(IG)是一种常见的可解释性技术，用于解决神经网络的黑盒问题。集成梯度假设数据是连续的。图是离散结构，这使得IG不适用于图。在这项工作中，我们引入了基于图的集成梯度(GB-IG)；作为IG在图上的扩展。我们在四个合成数据集上证明，GB-IG能够准确识别分类任务中使用的图的关键结构组件。我们进一步在三个 prevalent 现实世界图数据集上证明，GB-IG在突出显示节点分类任务中的重要特征方面优于IG。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated Gradients (IG) is a common explainability technique to address theblack-box problem of neural networks. Integrated gradients assumes continuousdata. Graphs are discrete structures making IG ill-suited to graphs. In thiswork, we introduce graph-based integrated gradients (GB-IG); an extension of IGto graphs. We demonstrate on four synthetic datasets that GB-IG accuratelyidentifies crucial structural components of the graph used in classificationtasks. We further demonstrate on three prevalent real-world graph datasets thatGB-IG outperforms IG in highlighting important features for node classificationtasks.</description>
      <author>example@mail.com (Lachlan Simpson, Kyle Millar, Adriel Cheng, Cheng-Chew Lim, Hong Gunn Chew)</author>
      <guid isPermaLink="false">2509.07648v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>NestGNN: A Graph Neural Network Framework Generalizing the Nested Logit Model for Travel Mode Choice</title>
      <link>http://arxiv.org/abs/2509.07123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的嵌套效用图神经网络(NestGNN)，用于改进离散选择分析，特别是交通方式选择问题。该方法结合了经典嵌套logit模型和深度神经网络的优势，通过引入替代方案图的概念来表示交通方式间的关系。&lt;h4&gt;背景&lt;/h4&gt;嵌套logit模型(NL)已被广泛用于离散选择分析，包括交通方式选择、汽车拥有或位置决策等多种应用。然而，传统NL模型受到其表示能力有限和手工指定效用的限制。虽然研究者引入了深度神经网络(DNNs)来解决这些挑战，但现有DNN无法明确捕捉离散选择情境中的替代方案间的相关性。&lt;h4&gt;目的&lt;/h4&gt;解决传统NL模型的表示能力有限和手工指定效用的问题，以及现有DNN无法明确捕捉离散选择情境中替代方案间相关性的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的概念——替代方案图，来表示交通方式替代方案之间的关系；使用嵌套替代方案图，设计了嵌套效用图神经网络(NestGNN)，作为神经网络家族中经典NL模型的泛化。&lt;h4&gt;主要发现&lt;/h4&gt;理论上，NestGNN在模型表示方面泛化了经典NL模型和现有DNN，同时保留了NL模型关键的二层替代模式：巢内比例替代但巢外非比例替代。经验上，NestGNN显著优于基准模型，特别是比相应的NL模型高出9.2%。从弹性表和替代可视化可以看出，NestGNN保留了NL模型的二层替代模式，但在模型设计空间上表现出更大的灵活性。&lt;h4&gt;结论&lt;/h4&gt;NestGNN在预测、解释方面表现出强大的能力，并且能够灵活地泛化经典NL模型用于分析交通方式选择问题。&lt;h4&gt;翻译&lt;/h4&gt;嵌套logit(NL)已被广泛用于离散选择分析，包括交通方式选择、汽车拥有或位置决策等多种应用。然而，传统NL模型受到其表示能力有限和手工指定效用的限制。虽然研究者引入了深度神经网络(DNNs)来解决这些挑战，但现有DNN无法明确捕捉离散选择情境中的替代方案间的相关性。为应对这些挑战，本研究提出了一个新概念——替代方案图，来表示交通方式替代方案之间的关系。使用嵌套替代方案图，本研究进一步设计了嵌套效用图神经网络(NestGNN)，作为神经网络家族中经典NL模型的泛化。理论上，NestGNN在模型表示方面泛化了经典NL模型和现有DNN，同时保留了NL模型关键的二层替代模式：巢内比例替代但巢外非比例替代。经验上，我们发现NestGNN显著优于基准模型，特别是比相应的NL模型高出9.2%。从弹性表和替代可视化可以看出，NestGNN保留了NL模型的二层替代模式，但在模型设计空间上表现出更大的灵活性。总体而言，我们的研究展示了NestGNN在预测、解释方面的能力，以及其泛化经典NL模型分析交通方式选择的灵活性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nested logit (NL) has been commonly used for discrete choice analysis,including a wide range of applications such as travel mode choice, automobileownership, or location decisions. However, the classical NL models arerestricted by their limited representation capability and handcrafted utilityspecification. While researchers introduced deep neural networks (DNNs) totackle such challenges, the existing DNNs cannot explicitly captureinter-alternative correlations in the discrete choice context. To address thechallenges, this study proposes a novel concept - alternative graph - torepresent the relationships among travel mode alternatives. Using a nestedalternative graph, this study further designs a nested-utility graph neuralnetwork (NestGNN) as a generalization of the classical NL model in the neuralnetwork family. Theoretically, NestGNNs generalize the classical NL models andexisting DNNs in terms of model representation, while retaining the crucialtwo-layer substitution patterns of the NL models: proportional substitutionwithin a nest but non-proportional substitution beyond a nest. Empirically, wefind that the NestGNNs significantly outperform the benchmark models,particularly the corresponding NL models by 9.2\%. As shown by elasticitytables and substitution visualization, NestGNNs retain the two-layersubstitution patterns as the NL model, and yet presents more flexibility in itsmodel design space. Overall, our study demonstrates the power of NestGNN inprediction, interpretation, and its flexibility of generalizing the classicalNL model for analyzing travel mode choice.</description>
      <author>example@mail.com (Yuqi Zhou, Zhanhong Cheng, Lingqian Hu, Yuheng Bu, Shenhao Wang)</author>
      <guid isPermaLink="false">2509.07123v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.06763v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VariSAC的图神经网络增强深度强化学习框架，用于解决RIS辅助ISAC使能的V2X系统中的统一可靠建模和资源优化问题，实现了时间连续的保证连接。&lt;h4&gt;背景&lt;/h4&gt;可重构智能表面与集成感知和通信在车辆网络中的集成实现了动态空间资源管理和环境实时适应，但V2I和V2V连接要求的共存以及高度动态异构的网络拓扑对统一可靠建模和资源优化提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决V2I和V2V连接要求共存问题，应对高度动态异构网络拓扑，实现统一可靠建模和资源优化，提供时间连续的保证连接。&lt;h4&gt;方法&lt;/h4&gt;提出VariSAC框架，引入连续连接比率作为统一指标，使用带有残差适配器的图神经网络编码复杂高维系统状态，并通过软演员-评论家代理联合优化信道分配、功率控制和RIS配置。&lt;h4&gt;主要发现&lt;/h4&gt;在真实城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V交付可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;h4&gt;结论&lt;/h4&gt;VariSAC有效解决了RIS辅助ISAC使能的V2X系统中的统一可靠建模和资源优化问题，实现了时间连续的保证连接。&lt;h4&gt;翻译&lt;/h4&gt;可重构智能表面与集成感知和通信在车辆网络中的集成实现了动态空间资源管理和对环境变化的实时适应。然而，车辆到基础设施和车辆到车辆连接要求的共存，以及高度动态和异构的网络拓扑，给统一可靠建模和资源优化带来了显著挑战。为解决这些问题，我们提出VariSAC，一种用于RIS辅助、ISAC使能的车辆到万物系统中保证时间连续连接的图神经网络增强深度强化学习框架。具体而言，我们引入了连续连接比率作为统一指标，表征V2I连接的持续时间可靠性和V2V链路概率交付保证，从而统一了它们的连续可靠性语义。接下来，我们采用带有残差适配器的图神经网络来编码复杂高维系统状态，捕捉车辆、基站和RIS节点之间的空间依赖关系。这些表示随后由软演员-评论家代理处理，该代理联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。在真实城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V交付可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of Reconfigurable Intelligent Surfaces (RIS) and IntegratedSensing and Communication (ISAC) in vehicular networks enables dynamic spatialresource management and real-time adaptation to environmental changes. However,the coexistence of distinct vehicle-to-infrastructure (V2I) andvehicle-to-vehicle (V2V) connectivity requirements, together with highlydynamic and heterogeneous network topologies, presents significant challengesfor unified reliability modeling and resource optimization. To address theseissues, we propose VariSAC, a graph neural network (GNN)-augmented deepreinforcement learning framework for assured, time-continuous connectivity inRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,we introduce the Continuous Connectivity Ratio (CCR), a unified metric thatcharacterizes the sustained temporal reliability of V2I connections and theprobabilistic delivery guarantees of V2V links, thus unifying their continuousreliability semantics. Next, we employ a GNN with residual adapters to encodecomplex, high-dimensional system states, capturing spatial dependencies amongvehicles, base stations (BS), and RIS nodes. These representations are thenprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channelallocation, power control, and RIS configurations to maximize CCR-drivenlong-term rewards. Extensive experiments on real-world urban datasetsdemonstrate that VariSAC consistently outperforms existing baselines in termsof continuous V2I ISAC connectivity and V2V delivery reliability, enablingpersistent connectivity in highly dynamic vehicular environments.</description>
      <author>example@mail.com (Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun)</author>
      <guid isPermaLink="false">2509.06763v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
      <link>http://arxiv.org/abs/2509.05429v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acctepted by ACM CCS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图神经网络(GNNs)中的拓扑隐私风险，提出了一种名为私有图重构(PGR)的新型防御框架，能够在保护图结构隐私的同时保持模型准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为处理图结构数据的有力模型，但其广泛应用引发了严重的隐私问题。先前研究主要关注边缘级别的隐私保护，而忽略了拓扑隐私（即图整体结构的保密性）这一关键但尚未充分探索的威胁。&lt;h4&gt;目的&lt;/h4&gt;全面探讨GNN中的拓扑隐私风险，揭示其对图级推理攻击的脆弱性，并提出有效的防御机制。&lt;h4&gt;方法&lt;/h4&gt;提出了一套拓扑推理攻击(TIAs)方法，仅通过黑盒访问GNN模型就能重建目标训练图的结构；同时引入了私有图重构(PGR)防御框架，这是一个双层优化问题，通过元梯度迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;GNN极易受到拓扑推理攻击，现有的边缘级别差分隐私机制不足以缓解风险，要么无法减轻风险，要么严重损害模型准确性。&lt;h4&gt;结论&lt;/h4&gt;PGR框架能够显著减少拓扑信息泄露，同时对模型准确性的影响最小。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的有力模型。然而，它们的广泛应用引发了严重的隐私问题。虽然先前研究主要关注边缘级别的隐私保护，但一个关键 yet 尚未充分探索的威胁在于拓扑隐私 - 图整体结构的保密性。在这项工作中，我们对GNN中的拓扑隐私风险进行了全面研究，揭示了它们对图级推理攻击的脆弱性。为此，我们提出了一套拓扑推理攻击(TIAs)，仅通过黑盒访问GNN模型就能重建目标训练图的结构。我们的发现表明，GNN极易受到这些攻击，现有的边缘级别差分隐私机制不足以缓解风险，因为它们要么无法减轻风险，要么严重损害模型准确性。为应对这一挑战，我们引入了私有图重构(PGR)，这是一种新型防御框架，旨在保护拓扑隐私同时保持模型准确性。PGR被表述为一个双层优化问题，其中使用元梯度迭代生成合成训练图，GNN模型则基于不断演化的图同时更新。大量实验证明，PGR显著减少了拓扑信息泄露，同时对模型准确性的影响最小。我们的代码可在 https://github.com/JeffffffFu/PGR 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for learningfrom graph-structured data. However, their widespread adoption has raisedserious privacy concerns. While prior research has primarily focused onedge-level privacy, a critical yet underexplored threat lies in topologyprivacy - the confidentiality of the graph's overall structure. In this work,we present a comprehensive study on topology privacy risks in GNNs, revealingtheir vulnerability to graph-level inference attacks. To this end, we propose asuite of Topology Inference Attacks (TIAs) that can reconstruct the structureof a target training graph using only black-box access to a GNN model. Ourfindings show that GNNs are highly susceptible to these attacks, and thatexisting edge-level differential privacy mechanisms are insufficient as theyeither fail to mitigate the risk or severely compromise model accuracy. Toaddress this challenge, we introduce Private Graph Reconstruction (PGR), anovel defense framework designed to protect topology privacy while maintainingmodel accuracy. PGR is formulated as a bi-level optimization problem, where asynthetic training graph is iteratively generated using meta-gradients, and theGNN model is concurrently updated based on the evolving graph. Extensiveexperiments demonstrate that PGR significantly reduces topology leakage withminimal impact on model accuracy. Our code is available athttps://github.com/JeffffffFu/PGR.</description>
      <author>example@mail.com (Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang)</author>
      <guid isPermaLink="false">2509.05429v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Contrastive Pretraining of CBCT and IOS for Enhanced Tooth Segmentation</title>
      <link>http://arxiv.org/abs/2509.07923v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作介绍了一种名为ToothMCL的多模态预训练框架，用于牙齿分割，整合了CBCT和IOS两种数据模态，通过多模态对比学习提高了分割精度，并在多个独立数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;数字牙科代表了现代牙科实践的重大转变，其基础是准确获取患者牙齿的数字表示。然而，尽管数字牙科技术日益受到关注，现有的分割方法通常缺乏严格的验证，性能有限且临床应用性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态预训练框架，用于提高牙齿分割的准确性和临床适用性，实现精确的多类分割和准确的FDI牙齿编号识别。&lt;h4&gt;方法&lt;/h4&gt;提出ToothMCL（Tooth Multimodal Contrastive Learning）框架，整合体积型CBCT和基于表面的IOS数据模态，通过多模态对比学习捕获模态不变表示。同时构建了CBCT-IOS3.8K数据集，包含3,867名患者的配对CBCT和IOS数据，并在一系列独立数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;ToothMCL在内部和外部测试中都达到了最先进的性能，Dice相似系数在CBCT分割上提高了12%，在IOS分割上提高了8%。该方法在牙齿组分割方面持续超越现有方法，并在不同的成像条件和临床场景中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ToothMCL通过多模态对比学习有效建模了细粒度解剖特征，显著提高了牙齿分割的准确性和临床适用性，为数字牙科实践提供了强有力的工具。&lt;h4&gt;翻译&lt;/h4&gt;数字牙科代表了现代牙科实践的一次转变性变革。这一转变的基础是准确获取患者牙齿的数字表示，这通常通过分割锥形束计算机断层扫描和口腔内扫描获得。尽管数字牙科技术日益受到关注，现有的分割方法通常缺乏严格的验证，性能有限且临床应用性差。据我们所知，这是首次引入用于牙齿分割的多模态预训练框架的工作。我们提出了ToothMCL，这是一种用于预训练的牙齿多模态对比学习，整合了体积型和基于表面的模态。通过多模态对比学习捕获模态不变表示，我们的方法有效地建模了细粒度解剖特征，实现了精确的多类分割和准确的Fédération Dentaire Internationale牙齿编号识别。除了该框架外，我们还构建了CBCT-IOS3.8K，这是迄今为止最大的配对CBCT和IOS数据集，包含3,867名患者。随后，我们在一系列独立数据集上评估了ToothMCL，这是迄今为止最大且最多样化的评估。我们的方法在内部和外部测试中都达到了最先进的性能，Dice相似系数在CBCT分割上提高了12%，在IOS分割上提高了8%。此外，ToothMCL在牙齿组方面持续超越现有方法，并在不同的成像条件和临床场景中表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital dentistry represents a transformative shift in modern dentalpractice. The foundational step in this transformation is the accurate digitalrepresentation of the patient's dentition, which is obtained from segmentedCone-Beam Computed Tomography (CBCT) and Intraoral Scans (IOS). Despite thegrowing interest in digital dental technologies, existing segmentationmethodologies frequently lack rigorous validation and demonstrate limitedperformance and clinical applicability. To the best of our knowledge, this isthe first work to introduce a multimodal pretraining framework for toothsegmentation. We present ToothMCL, a Tooth Multimodal Contrastive Learning forpretraining that integrates volumetric (CBCT) and surface-based (IOS)modalities. By capturing modality-invariant representations through multimodalcontrastive learning, our approach effectively models fine-grained anatomicalfeatures, enabling precise multi-class segmentation and accurate identificationof F\'ed\'eration Dentaire Internationale (FDI) tooth numbering. Along with theframework, we curated CBCT-IOS3.8K, the largest paired CBCT and IOS dataset todate, comprising 3,867 patients. We then evaluated ToothMCL on a comprehensivecollection of independent datasets, representing the largest and most diverseevaluation to date. Our method achieves state-of-the-art performance in bothinternal and external testing, with an increase of 12\% for CBCT segmentationand 8\% for IOS segmentation in the Dice Similarity Coefficient (DSC).Furthermore, ToothMCL consistently surpasses existing approaches in toothgroups and demonstrates robust generalizability across varying imagingconditions and clinical scenarios.</description>
      <author>example@mail.com (Moo Hyun Son, Juyoung Bae, Zelin Qiu, Jiale Peng, Kai Xin Li, Yifan Lin, Hao Chen)</author>
      <guid isPermaLink="false">2509.07923v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Cross-Encoder for Neurodegenerative Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2509.07623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的自监督交叉编码器框架，用于解决神经退行性疾病诊断中依赖大量标记数据和缺乏可解释性的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习在从MRI数据诊断神经退行性疾病方面显示出巨大潜力，但现有方法主要依赖大量标记数据且往往产生缺乏可解释性的表示。&lt;h4&gt;目的&lt;/h4&gt;解决神经退行性疾病诊断中依赖大量标记数据和缺乏可解释性的双重挑战。&lt;h4&gt;方法&lt;/h4&gt;提出一种自监督交叉编码器框架，利用纵向MRI扫描中的时间连续性进行监督。该框架将学习到的表示解耦为静态表示（通过对比学习约束，捕获稳定的解剖特征）和动态表示（由输入梯度正则化引导，反映时间变化，可微调用于下游分类任务）。&lt;h4&gt;主要发现&lt;/h4&gt;在阿尔茨海默病神经影像学倡议(ADNI)数据集上，该方法实现了 superior 的分类准确性和改进的可解释性。学习到的表示在开放获取影像学研究系列(OASIS)数据集上表现出强大的零样本泛化能力，并在帕金森病进展标志物倡议(PPMI)数据集上表现出跨任务泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该方法在神经退行性疾病诊断中表现出色，具有良好的可解释性和泛化能力，代码将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在从MRI数据诊断神经退行性疾病方面显示出巨大潜力。然而，大多数现有方法严重依赖大量标记数据，并且往往产生缺乏可解释性的表示。为了解决这两个挑战，我们提出了一种新颖的自监督交叉编码器框架，利用纵向MRI扫描中的时间连续性进行监督。该框架将学习到的表示解耦为两个组成部分：静态表示，通过对比学习约束，捕获稳定的解剖特征；动态表示，由输入梯度正则化引导，反映时间变化，可有效微调用于下游分类任务。在阿尔茨海默病神经影像学倡议(ADNI)数据集上的实验结果表明，我们的方法实现了 superior 的分类准确性和改进的可解释性。此外，学习到的表示在开放获取影像学研究系列(OASIS)数据集上表现出强大的零样本泛化能力，并在帕金森病进展标志物倡议(PPMI)数据集上表现出跨任务泛化能力。所提出方法的代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has shown significant potential in diagnosing neurodegenerativediseases from MRI data. However, most existing methods rely heavily on largevolumes of labeled data and often yield representations that lackinterpretability. To address both challenges, we propose a novelself-supervised cross-encoder framework that leverages the temporal continuityin longitudinal MRI scans for supervision. This framework disentangles learnedrepresentations into two components: a static representation, constrained bycontrastive learning, which captures stable anatomical features; and a dynamicrepresentation, guided by input-gradient regularization, which reflectstemporal changes and can be effectively fine-tuned for downstreamclassification tasks. Experimental results on the Alzheimer's DiseaseNeuroimaging Initiative (ADNI) dataset demonstrate that our method achievessuperior classification accuracy and improved interpretability. Furthermore,the learned representations exhibit strong zero-shot generalization on the OpenAccess Series of Imaging Studies (OASIS) dataset and cross-task generalizationon the Parkinson Progression Marker Initiative (PPMI) dataset. The code for theproposed method will be made publicly available.</description>
      <author>example@mail.com (Fangqi Cheng, Yingying Zhao, Xiaochen Yang)</author>
      <guid isPermaLink="false">2509.07623v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Water Demand Forecasting of District Metered Areas through Learned Consumer Representations</title>
      <link>http://arxiv.org/abs/2509.07515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at European Conference for Signal Procesing - EUSIPCO 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新型的短期水需求预测方法，通过无监督对比学习对用户进行分类，并使用小波变换卷积网络进行预测，在真实DMA区域测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;智能水表技术的进步显著提升了对水务设施的监控和管理能力。在气候变化带来不确定性的背景下，保障水资源供应已成为具有广泛社会经济影响的全球性紧迫问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对分区计量区域(DMAs)的短期水需求预测新方法，该方法包含商业、农业和住宅消费者。&lt;h4&gt;方法&lt;/h4&gt;首先应用无监督对比学习对DMA内具有不同消费行为的终端用户进行分类；然后将这些不同的消费行为作为特征，使用结合历史数据和衍生表示的小波变换卷积网络进行后续的需求预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在真实DMA区域进行了六个月的测试，在不同DMA区域的MAPE预测性能上有所提高，最大改进为4.9%。此外，该方法还能识别出其行为受社会经济因素影响的消费者，增强了关于影响需求的确定性模式的前期认识。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过结合用户行为分类和先进的预测模型，有效提高了水需求预测的准确性，并为理解影响需求的社会经济因素提供了见解。&lt;h4&gt;翻译&lt;/h4&gt;智能水表技术的进步显著提高监控和管理水务设施的能力。在气候变化带来不确定性的背景下，保障水资源供应已成为具有广泛社会经济影响的全球性紧迫问题。来自终端用户的每小时消费数据为具有不同消费模式区域的需求预测提供了重要见解。然而，由于气象条件等非确定性影响因素，水需求预测仍然具有挑战性。本文介绍了一种针对包含商业、农业和住宅消费者的分区计量区域(DMAs)的短期水需求预测新方法。应用无监督对比学习对DMA内存在的不同消费行为的终端用户进行分类。随后，将不同的消费行为作为特征，在后续的需求预测任务中使用结合历史数据和衍生表示的小波变换卷积网络。所提出的方法在真实DMA区域进行了六个月的测试，展示了在不同DMA区域MAEP方面的改进预测性能，最大改进为4.9%。此外，它还识别出其行为受社会经济因素影响的消费者，增强了关于影响需求的确定性模式的前期认识。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in smart metering technologies have significantly improved theability to monitor and manage water utilities. In the context of increasinguncertainty due to climate change, securing water resources and supply hasemerged as an urgent global issue with extensive socioeconomic ramifications.Hourly consumption data from end-users have yielded substantial insights forprojecting demand across regions characterized by diverse consumption patterns.Nevertheless, the prediction of water demand remains challenging due toinfluencing non-deterministic factors, such as meteorological conditions. Thiswork introduces a novel method for short-term water demand forecasting forDistrict Metered Areas (DMAs) which encompass commercial, agricultural, andresidential consumers. Unsupervised contrastive learning is applied tocategorize end-users according to distinct consumption behaviors present withina DMA. Subsequently, the distinct consumption behaviors are utilized asfeatures in the ensuing demand forecasting task using wavelet-transformedconvolutional networks that incorporate a cross-attention mechanism combiningboth historical data and the derived representations. The proposed approach isevaluated on real-world DMAs over a six-month period, demonstrating improvedforecasting performance in terms of MAPE across different DMAs, with a maximumimprovement of 4.9%. Additionally, it identifies consumers whose behavior isshaped by socioeconomic factors, enhancing prior knowledge about thedeterministic patterns that influence demand.</description>
      <author>example@mail.com (Adithya Ramachandran, Thorkil Flensmark B. Neergaard, Tomás Arias-Vergara, Andreas Maier, Siming Bayer)</author>
      <guid isPermaLink="false">2509.07515v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CAME-AB是一种用于抗体结合位点预测的新型跨模态注意力框架，结合了多种生物学模态和自适应融合机制，在多个评估指标上表现出色。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的跨模态注意力框架CAME-AB，用于稳健的抗体结合位点预测，解决现有方法无法识别抗体特异性结合位点的问题。&lt;h4&gt;方法&lt;/h4&gt;CAME-AB集成了五种生物学基础模态：原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化的生化图；提出了自适应模态融合模块，根据全局相关性和输入特定贡献动态加权每种模态；结合了Transformer编码器和MoE模块促进特征专业化和能力扩展；引入了监督对比学习目标塑造潜在空间几何；在训练期间应用随机权重平均提高优化稳定性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在精确度、召回率、F1分数、AUC-ROC和MCC等多个指标上一致优于强基线；消融研究验证了每个架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过结合多种生物学基础模态和自适应融合机制，有效提高了抗体结合位点预测的准确性，为计算免疫学和治疗性抗体设计提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单视图特征，无法识别抗原上的抗体特异性结合位点。在本文中，我们提出了CAME-AB，一种具有专家混合（MoE）主干的新颖跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB将五种基于生物学的模态整合到统一的multimodal表示中，包括原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN精化的生化图。为了增强自适应跨模态推理，我们提出了一个自适应模态融合模块，根据其全局相关性和输入特定贡献动态学习对每种模态进行加权。结合MoE模块的Transformer编码器进一步促进了特征专业化和能力扩展。我们还结合了监督对比学习目标，明确塑造潜在空间几何，鼓励类内紧凑性和类间可分离性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在多个指标上始终优于强基线，包括精确度、召回率、F1分数、AUC-ROC和MCC。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在https://anonymous.4open.science/r/CAME-AB-C525获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens. In this paper, we propose \textbf{CAME-AB}, anovel Cross-modality Attention framework with a Mixture-of-Experts (MoE)backbone for robust antibody binding site prediction. CAME-AB integrates fivebiologically grounded modalities, including raw amino acid encodings, BLOSUMsubstitution profiles, pretrained language model embeddings, structure-awarefeatures, and GCN-refined biochemical graphs, into a unified multimodalrepresentation. To enhance adaptive cross-modal reasoning, we propose an\emph{adaptive modality fusion} module that learns to dynamically weight eachmodality based on its global relevance and input-specific contribution. ATransformer encoder combined with an MoE module further promotes featurespecialization and capacity expansion. We additionally incorporate a supervisedcontrastive learning objective to explicitly shape the latent space geometry,encouraging intra-class compactness and inter-class separability. To improveoptimization stability and generalization, we apply stochastic weight averagingduring training. Extensive experiments on benchmark antibody-antigen datasetsdemonstrate that CAME-AB consistently outperforms strong baselines on multiplemetrics, including Precision, Recall, F1-score, AUC-ROC, and MCC. Ablationstudies further validate the effectiveness of each architectural component andthe benefit of multimodal feature integration. The model implementation detailsand the codes are available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Rui Xiao, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>The Protocol Genome A Self Supervised Learning Framework from DICOM Headers</title>
      <link>http://arxiv.org/abs/2509.06995v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Protocol Genome，一种自监督学习系统，通过学习DICOM头部信息的相关性，在完全保留的外部验证中实现了AUROC 0.901（对比基线0.847）和ECE 0.036（对比0.056）的优异性能。&lt;h4&gt;背景&lt;/h4&gt;临床影像通过PACS/DICOM系统传输，程序选择（如扫描仪型号、序列参数等）影响影像质量，这些潜在的混杂因素阻碍了仅基于图像的网络在不同站点间的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;将结构化的DICOM头部信息作为标签，学习协议感知但临床稳健的图像表示，提高模型在不同模态和供应商间的校准能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;获取去标识化DICOM头部的标记化嵌入，结合图像特征使用三种方法建模：(1)协议-图像对比学习，(2)掩蔽协议预测，(3)协议-协议翻译。研究使用了126万项临床数据（7个医疗系统，31台扫描仪，3个供应商）。&lt;h4&gt;主要发现&lt;/h4&gt;相比强大的SSL基线和ImageNet迁移，Protocol Genome在外部AUROC上显著提高（肺栓塞:+0.046，胶质瘤:+0.058，心肌肥大:+0.041），并实现25-37%的校准改进（p &lt; 0.01）。即使仅使用10-20%的标记数据，这些增益仍能保持。&lt;h4&gt;结论&lt;/h4&gt;该技术减少了协议边界处的假阳性，可直接集成到PACS系统中，并已发布包含去标识化和偏见审计的模型卡和部署指南。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种名为协议基因组（Protocol Genome）的自监督学习系统，该系统从DICOM头部信息中学习相关性，并在完全保留的外部验证中实现了AUROC 0.901（对比基线0.847）和ECE 0.036（对比0.058）。我们的方法还提高了CT、MRI、CXR等不同模态和供应商间的校准能力和鲁棒性。临床影像通过PACS/DICOM系统传输，程序选择（扫描仪型号/型号、序列、内核、kVp、TR/TE和切片厚度）会影响对比度、噪声和伪影。这些潜在的混杂因素阻碍了仅基于图像的网络在不同站点间的泛化。我们将结构化的DICOM头部信息作为标签，学习协议感知但临床稳健的图像表示。协议基因组获取去标识化头部字段的标记化嵌入，并使用以下方法与图像特征一起建模：(1)协议-图像对比学习，(2)掩蔽协议预测，(3)协议-协议翻译。基于126万项研究（7个医疗系统，31台扫描仪，3个供应商；CT、MR、CR/DR），我们在以下任务上进行了实验：(A)胸部CT肺栓塞分诊，(B)脑部MRI胶质瘤分级，(C)胸部放射心肌肥大检测。与强大的SSL基线（SimCLR、MAE）以及ImageNet迁移相比，协议基因组在外部AUROC上表现更优（肺栓塞:+0.046，胶质瘤:+0.058，心肌肥大:+0.041），并获得了25-37%的校准改进（p &lt; 0.01，DeLong测试）。虽然增益可能依赖于任务，但在仅使用10-20%的标记数据时仍能保持。从临床角度来看，该技术减少了协议边界处的假阳性，并可应用于PACS系统（DICOM C-FIND/C-MOVE，DICOMweb QIDO/WADO）。我们发布了包含去标识化和偏见审计的模型卡和部署指南。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学影像中的域偏移、扫描仪异质性和隐藏混杂因素问题。这些问题在现实中很重要，因为不同医院和设备的协议差异导致AI模型难以跨站点部署，限制了医学影像AI的临床应用；标签稀缺问题也阻碍了模型训练；而模型可能学习到与临床决策无关的协议特征，导致预测偏差。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将DICOM头文件视为'基因组'代码，借鉴了基因组学的概念，设计了一个多模态自监督学习框架。他们参考了现有的自监督学习方法（如SimCLR、MoCo、MAE）、医学影像深度学习架构（ResNet、ViT等）和域适应技术。创新点在于将协议信息融入自监督学习，通过标记化DICOM头文件、混合注意力融合和对抗性头部来分离协议身份和临床特征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将DICOM头文件从'负担'转变为'第一类自监督信号'，将域偏移从'对手'变为'教师'。实现流程包括：1)数据预处理（选择、标记化、去标识化DICOM头文件）；2)自监督预训练（协议-图像对比学习、掩码协议建模、协议-协议翻译）；3)架构设计（图像和协议编码器、混合注意力融合、对抗性头部）；4)偏见感知微调（梯度反转学习、重要性重加权、采集感知增强）；5)多站点外部评估和子群体分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将DICOM头文件作为基因组代码处理；2)多组件自监督目标（对比学习、掩码建模、协议翻译）；3)混合注意力融合机制；4)对抗性头部分离协议和临床特征；5)与PACS/DICOM工作流集成。相比之前工作，传统方法只考虑像素信息，将协议视为障碍；而本文将协议视为有价值信号，首次系统利用DICOM头文件作为自监督目标，并提供完整的模型卡和部署建议。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Protocol Genome通过将DICOM头文件转化为自监督学习信号，解决了医学影像中的域偏移和标签稀缺问题，实现了跨站点、跨厂商的鲁棒临床预测，同时提供了可审计的协议感知表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce the Protocol Genome, a self-supervised learningsystem that learns correlations from DICOM headers and achieves AUROC 0.901 (vs0.847 baseline) and ECE 0.036 (vs 0.058) on fully held-out external validation.Our method also improves calibration and robustness across modalities (CT, MRI,CXR) and vendors. Clinical imaging is funneled through PACS/DICOM, whereprocedure choices (scanner make/model, sequence, kernel, kVp, TR/TE, and slicethickness) have consequences for contrast, noise, and artifact. These latentconfounders impede the generalization of image-only networks across sites. Weconsider structured DICOM headers as a label and learn protocol-aware butclinically robust image representations. Protocol Genome obtains tokenizedembeddings of de-identified header fields and models them along with imagefeatures using: (1) protocol-image contrastive learning, (2) masked protocolprediction, and (3) protocol-protocol translation. With 1.26M studies (7 healthsystems, 31 scanners, 3 vendors; CT, MR, CR/DR), we experiment on: (A) chest CTtriage for PE, (B) brain MRI glioma grading, and (C) chest radiographcardiomegaly detection. Relative to strong SSL baselines (SimCLR, MAE) as wellas ImageNet transfer, Protocol Genome (+0.046: PE, +0.058: glioma, +0.041:cardiomegaly) is associated with higher external AUROC; 25-37% calibrationimprovements are obtained (p &lt; 0.01, DeLong tests). While the gains may betask-dependent, they are preserved with 10-20% of labeled data. From a clinicalpoint of view, the technique reduces false positives at protocol borders and isapplicable in a PACS (DICOM C-FIND/C-MOVE, DICOMweb QIDO/WADO). We publish amodel card and deployment guide, complete with both de-identification and biasaudits.</description>
      <author>example@mail.com (Jimmy Joseph)</author>
      <guid isPermaLink="false">2509.06995v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Toward Quantum Utility in Finance: A Robust Data-Driven Algorithm for Asset Clustering</title>
      <link>http://arxiv.org/abs/2509.07766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 2 figures, International Quantum Engineering conference and  exhibition (QUEST-IS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图的联盟结构生成算法(GCS-Q)来处理金融资产的聚类问题，特别是在处理带符号的相关性结构时。这种方法避免了传统聚类方法中的有损转换和启发式假设，如固定聚类数量，并利用量子退火技术高效探索解决方案空间。实验证明，该方法在合成和真实金融数据上均优于最先进的经典算法，能够动态确定聚类数量，并提高聚类质量。&lt;h4&gt;背景&lt;/h4&gt;基于收益相关性的金融资产聚类是投资组合优化和统计套利中的基本任务。然而，传统的聚类方法在处理带符号的相关性结构时往往效果不佳，通常需要依赖有损转换和启发式假设，如固定聚类数量。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接处理带符号、加权图的聚类方法，无需依赖有损转换和固定聚类数量的假设，并利用量子计算技术提高聚类效率和质量。&lt;h4&gt;方法&lt;/h4&gt;应用基于图的联盟结构生成算法(GCS-Q)，该方法将每个分区步骤表述为QUBO问题，从而能够利用量子退火技术高效探索指数级大的解决方案空间。在合成和真实金融数据上验证该方法，并与最先进的经典算法(如SPONGE和k-Medoids)进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;GCS-Q方法在调整兰德指数和结构平衡惩罚等指标上持续获得更高的聚类质量，同时能够动态确定聚类数量。这些结果凸显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;h4&gt;结论&lt;/h4&gt;GCS-Q算法为金融资产聚类提供了一种有效的新方法，特别适用于处理带符号的相关性结构。量子计算技术在解决金融领域的复杂聚类问题时具有实用价值，能够克服传统方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;基于收益相关性的金融资产聚类是投资组合优化和统计套利中的基本任务。然而，当处理带符号的相关性结构时，传统聚类方法往往表现不佳，通常需要依赖有损转换和启发式假设，如固定聚类数量。在本工作中，我们应用基于图的联盟结构生成算法(GCS-Q)来直接聚类带符号、加权图，而不依赖这些转换。GCS-Q将每个分区步骤表述为QUBO问题，使其能够利用量子退火技术高效探索指数级大的解决方案空间。我们在合成和真实金融数据上验证了我们的方法，并与最先进的经典算法(如SPONGE和k-Medoids)进行了基准测试。我们的实验证明，GCS-Q在调整兰德指数和结构平衡惩罚等指标上持续获得更高的聚类质量，同时动态确定聚类数量。这些结果凸显了近期量子计算在金融应用的基于图的无监督学习中的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering financial assets based on return correlations is a fundamentaltask in portfolio optimization and statistical arbitrage. However, classicalclustering methods often fall short when dealing with signed correlationstructures, typically requiring lossy transformations and heuristic assumptionssuch as a fixed number of clusters. In this work, we apply the Graph-basedCoalition Structure Generation algorithm (GCS-Q) to directly cluster signed,weighted graphs without relying on such transformations. GCS-Q formulates eachpartitioning step as a QUBO problem, enabling it to leverage quantum annealingfor efficient exploration of exponentially large solution spaces. We validateour approach on both synthetic and real-world financial data, benchmarkingagainst state-of-the-art classical algorithms such as SPONGE and k-Medoids. Ourexperiments demonstrate that GCS-Q consistently achieves higher clusteringquality, as measured by Adjusted Rand Index and structural balance penalties,while dynamically determining the number of clusters. These results highlightthe practical utility of near-term quantum computing for graph-basedunsupervised learning in financial applications.</description>
      <author>example@mail.com (Shivam Sharma, Supreeth Mysore Venkatesh, Pushkin Kachroo)</author>
      <guid isPermaLink="false">2509.07766v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>LSMTCR: A Scalable Multi-Architecture Model for Epitope-Specific T Cell Receptor de novo Design</title>
      <link>http://arxiv.org/abs/2509.07627v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LSMTCR是一个可扩展的多架构框架，能够从头开始、基于表位条件生成配对的全长T细胞受体(TCR)，在多个数据集上表现出优于基线的预测结合能力，并提供了温度可调的多样性和免疫遗传保真度。&lt;h4&gt;背景&lt;/h4&gt;设计全长、表位特异性的T细胞受体(TCR)具有挑战性，主要源于序列空间庞大、数据偏差和免疫遗传约束建模不完整等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的多架构框架LSMTCR，将特异性学习与约束学习分离，实现从头开始、基于表位条件生成配对的全长TCRs。&lt;h4&gt;方法&lt;/h4&gt;LSMTCR采用三个关键组件：1) 扩散增强的BERT编码器学习时间条件的表位表示；2) 条件GPT解码器在跨模态条件下生成链特异性CDR3，具有温度控制的多样性；3) 基因感知的Transformer通过预测V/J使用来组装完整的α/β序列，确保免疫遗传保真度。&lt;h4&gt;主要发现&lt;/h4&gt;LSMTCR在GLIPH、TEP、MIRA、McPAS和策展数据集上，在大多数数据集上的预测结合能力优于基线；更忠实地恢复了位置和长度语法；提供了优越的、温度可调的多样性；迁移学习提高了α链生成的预测结合能力、长度真实性和多样性；从已知或从头开始的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低；在与表位的配对α/β共建模中，比单链设置获得更高的pTM/ipTM。&lt;h4&gt;结论&lt;/h4&gt;LSMTCR仅从表位输入就能输出多样化、基因上下文化的全长TCR设计，使高通量筛选和迭代优化成为可能。&lt;h4&gt;翻译&lt;/h4&gt;设计全长、表位特异性的T细胞受体(TCR)αβ由于序列空间庞大、数据偏差和免疫遗传约束建模不完整而具有挑战性。我们提出了LSMTCR，一个可扩展的多架构框架，它将特异性学习与约束学习分离，能够从头开始、基于表位条件生成配对的全长TCR。扩散增强的BERT编码器学习时间条件的表位表示；条件GPT解码器，在CDR3β上预训练并迁移到CDR3α，在跨模态条件下生成链特异性CDR3，具有温度控制的多样性；基因感知的Transformer通过预测V/J使用来组装完整的α/β序列，确保免疫遗传保真度。在GLIPH、TEP、MIRA、McPAS和我们策展的数据集上，LSMTCR在大多数数据集上的预测结合能力优于基线，更忠实地恢复了位置和长度语法，并提供了优越的、温度可调的多样性。对于α链生成，迁移学习提高了预测结合能力、长度真实性和多样性。从已知或从头开始的CDR3进行全长组装保留了k-mer谱，与参考序列的编辑距离低，并且在与表位的配对α/β共建模中，比单链设置获得更高的pTM/ipTM。LSMTCR仅从表位输入就能输出多样化、基因上下文化的全长TCR设计，使高通量筛选和迭代优化成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing full-length, epitope-specific TCR {\alpha}\b{eta} remainschallenging due to vast sequence space, data biases and incomplete modeling ofimmunogenetic constraints. We present LSMTCR, a scalable multi-architectureframework that separates specificity from constraint learning to enable denovo, epitope-conditioned generation of paired, full-length TCRs. Adiffusion-enhanced BERT encoder learns time-conditioned epitoperepresentations; conditional GPT decoders, pretrained on CDR3\b{eta} andtransferred to CDR3{\alpha}, generate chain-specific CDR3s under cross-modalconditioning with temperature-controlled diversity; and a gene-awareTransformer assembles complete {\alpha}/\b{eta} sequences by predicting V/Jusage to ensure immunogenetic fidelity. Across GLIPH, TEP, MIRA, McPAS and ourcurated dataset, LSMTCR achieves higher predicted binding than baselines onmost datasets, more faithfully recovers positional and length grammars, anddelivers superior, temperature-tunable diversity. For {\alpha}-chaingeneration, transfer learning improves predicted binding, length realism anddiversity over representative methods. Full-length assembly from known or denovo CDR3s preserves k-mer spectra, yields low edit distances to references,and, in paired {\alpha}/\b{eta} co-modelling with epitope, attains higherpTM/ipTM than single-chain settings. LSMTCR outputs diverse,gene-contextualized, full-length TCR designs from epitope input alone, enablinghigh-throughput screening and iterative optimization.</description>
      <author>example@mail.com (Ruihao Zhang, Xiao Liu)</author>
      <guid isPermaLink="false">2509.07627v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Neurocognitive Modeling for Text Generation: Deep Learning Architecture for EEG Data</title>
      <link>http://arxiv.org/abs/2509.07202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了将大型语言模型与脑电图解码相结合以增强辅助技术的潜力，特别是对有严重运动障碍人士的独立性和沟通能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的出现使文本生成能力发生了巨大变革，但基于脑电图的文本生成仍面临挑战，因为它需要大量数据和计算能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新方法，减少EEG文本生成所需的数据和计算资源，同时保持接近最先进方法的性能。&lt;h4&gt;方法&lt;/h4&gt;结合使用Gemma 2B LLM和分类器-LLM架构，并引入循环神经网络(RNN)编码器。&lt;h4&gt;主要发现&lt;/h4&gt;新方法显著降低了所需的数据和计算能力，性能接近最先进方法，整体性能提高10%；所提出的架构展示了EEG文本生成有效迁移学习的可能性，即使在数据有限的情况下也能保持强大和功能性。&lt;h4&gt;结论&lt;/h4&gt;通过有效利用预训练语言模型的优势，该方法推动了当前能力的极限，为脑机接口的研究和应用开辟了新途径，使基于EEG的文本生成更加易于访问和高效。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型(LLMs)的引入，文本生成能力已经发生了巨大变革。然而，基于脑电图(EEG)的文本生成仍然困难，因为它需要大量数据和计算能力。本文介绍了一种新方法，它结合使用Gemma 2B LLM和分类器-LLM架构，并引入循环神经网络(RNN)编码器。我们的方法显著降低了所需的数据和计算能力，同时实现了接近最先进方法的性能。值得注意的是，与当前方法相比，我们的方法整体性能提高了10%。所提出的架构展示了EEG文本生成有效迁移学习的可能性，即使在数据有限的情况下也能保持强大和功能性。这项工作强调了将LLMs与EEG解码相结合以增强辅助技术的潜力，提高有严重运动限制人士的独立性和沟通能力。我们的方法通过有效利用预训练语言模型的优势，推动了当前能力的极限，为脑机接口的研究和应用开辟了新途径，使基于EEG的文本生成更加易于访问和高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text generating capabilities have undergone a substantial transformation withthe introduction of large language models (LLMs). Electroencephalography(EEG)-based text production is still difficult, though, because it requires alot of data and processing power. This paper introduces a new method thatcombines the use of the Gemma 2B LLM with a classifier-LLM architecture toincorporate a Recurrent Neural Network (RNN) encoder. Our approach drasticallylowers the amount of data and compute power needed while achieving performanceclose to that of cutting-edge methods. Notably, compared to currentmethodologies, our methodology delivers an overall performance improvement of10%. The suggested architecture demonstrates the possibility of effectivetransfer learning for EEG-based text production, remaining strong andfunctional even in the face of data limits. This work highlights the potentialof integrating LLMs with EEG decoding to improve assistive technologies andimprove independence and communication for those with severe motor limitations.Our method pushes the limits of present capabilities and opens new paths forresearch and application in brain-computer interfaces by efficiently using thestrengths of pre-trained language models. This makes EEG-based text productionmore accessible and efficient.</description>
      <author>example@mail.com (Khushiyant)</author>
      <guid isPermaLink="false">2509.07202v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Comparing unsupervised learning methods for local structural identification in colloidal systems</title>
      <link>http://arxiv.org/abs/2509.07186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 20 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了三种降维技术在自组装系统局部环境分类中的表现，发现UMAP在捕捉复杂结构特征方面表现最佳，为无监督结构分类提供了有效工具。&lt;h4&gt;背景&lt;/h4&gt;量化自组装系统中的局部结构是软物质和材料科学的核心挑战。当没有相关结构的先验知识可用时，传统的序参数往往不够用。&lt;h4&gt;目的&lt;/h4&gt;系统地比较三种流行的降维技术（主成分分析PCA、自编码器AE和统一流形近似和投影UMAP）用于分类自组装系统中的局部环境，探索无监督机器学习在自主发现结构基序方面的应用。&lt;h4&gt;方法&lt;/h4&gt;应用三种降维技术分别于硬球和带电球的流体和晶体构型，以及球形约束中自组装的二十面体球排列（包括模拟和实验数据）。&lt;h4&gt;主要发现&lt;/h4&gt;UMAP在捕捉复杂结构特征方面始终优于其他方法，为无监督结构分类提供了强大的工具。&lt;h4&gt;结论&lt;/h4&gt;无监督机器学习为从粒子构型中自主发现结构基序提供了便捷的途径，UMAP是一种鲁棒的工具，可用于自组装系统的结构分类，无需监督。&lt;h4&gt;翻译&lt;/h4&gt;量化自组装系统中的局部结构是软物质和材料科学的核心挑战。当没有相关结构的先验知识可用时，传统的序参数往往不够用。无监督机器学习为从粒子构型中自主发现结构基序提供了便捷的途径。在这项工作中，我们系统地比较了三种流行的降维技术：主成分分析（PCA）、自编码器（AE）和统一流形近似和投影（UMAP），用于分类自组装系统中的局部环境。我们首先将这些方法应用于硬球和带电球的流体和晶体构型。之后，我们将它们应用于球形约束中自组装的二十面体球排列，包括模拟和实验数据。我们证明UMAP在捕捉复杂结构特征方面始终优于其他方法，为无监督的结构分类提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantifying local structures in self-assembled systems is a central challengein soft matter and materials science. When no a priori knowledge of therelevant structures is available, traditional order parameters often fallshort. Unsupervised machine learning provides a convenient route toautonomously uncover structural motifs directly from particle configurations.In this work, we systematically compare three popular dimensionality reductiontechniques; Principal Component Analysis (PCA), Autoencoders (AE), and UniformManifold Approximation and Projection (UMAP), for classifying localenvironments in self-assembled systems. We first apply these methods to fluidand crystal configurations of hard and charged spheres. Thereafter, we apply itto an icosahedral arrangement of spheres that self-assembled in sphericalconfinement, both from simulations as well as from experiments. We demonstratethat UMAP consistently outperforms the other methods in capturing complexstructural features, offering a robust tool for structural classificationwithout supervision.</description>
      <author>example@mail.com (Alptuğ Ulugöl, Jessi Bückmann, Ruizhi Yang, Roy Hoitink, Alfons van Blaaderen, Frank Smallenburg, Laura Filion)</author>
      <guid isPermaLink="false">2509.07186v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Fourier Neural Operators for Time-Periodic Quantum Systems: Learning Floquet Hamiltonians, Observable Dynamics, and Operator Growth</title>
      <link>http://arxiv.org/abs/2509.07084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了傅里叶神经算子(FNO)作为非平衡量子动力学的有效替代模型，通过三种学习范式展示了其多功能性，包括重建Floquet哈密顿量、预测可观测量和学习量子信息传播，并证明了其准确性和计算效率优势。&lt;h4&gt;背景&lt;/h4&gt;时间周期性量子系统表现出丰富多样的非平衡现象，是量子工程和控制的理想平台。然而，由于希尔伯特空间维度指数增长和纠缠快速传播，传统数值方法难以模拟其动力学。&lt;h4&gt;目的&lt;/h4&gt;引入傅里叶神经算子(FNO)作为非平衡量子动力学的有效、准确和可扩展的替代模型，解决传统数值方法面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用在傅里叶空间参数化的神经算子，通过三种互补学习范式：重建有效Floquet哈密顿量、预测局部可观测量期望值和学习量子信息传播，评估FNO的性能。&lt;h4&gt;主要发现&lt;/h4&gt;FNO在每个学习任务上都取得显著准确性，同时实现与精确数值方法相比的显著加速；具有在不同时间离散化和系统驱动频率间迁移学习的能力；可在训练数据时间窗口外进行外推；计算成本随系统大小呈多项式缩放。&lt;h4&gt;结论&lt;/h4&gt;FNO是预测非平衡量子动力学的多功能和可扩展替代模型，在处理近期量子计算机数据方面具有潜在应用。&lt;h4&gt;翻译&lt;/h4&gt;时间周期性量子系统表现出丰富多样的非平衡现象，并成为量子工程和控制的理想平台。然而，由于希尔伯特空间维度的指数增长和纠缠的快速传播，使用传统数值方法模拟它们的动力学仍然具有挑战性。在这项工作中，我们引入傅里叶神经算子(FNO)作为非平衡量子动力学的有效、准确和可扩展的替代模型。在傅里叶空间中参数化，FNO自然地捕捉时间相关性，并且对时间离散化的依赖性最小。我们通过三种互补的学习范式展示了FNO的多功能性：重建有效的Floquet哈密顿量、预测局部可观测量期望值和学习量子信息传播。对于每个学习任务，与精确数值方法相比，FNO都取得了显著的准确性，同时实现了显著的加速。此外，FNO还具有在不同时间离散化和系统驱动频率之间迁移学习的显著能力。我们还表明，FNO可以在训练数据提供的时间窗口之外进行外推，使原本难以获得的可观测量和算子传播动力学变得可访问。通过采用适当的局部基，我们认为FNO的计算成本仅随系统大小呈多项式缩放。我们的研究结果表明，FNO是预测非平衡量子动力学的多功能和可扩展的替代模型，在处理近期量子计算机数据方面具有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-periodic quantum systems exhibit a rich variety of far-from-equilibriumphenomena and serve as ideal platforms for quantum engineering and control.However, simulating their dynamics with conventional numerical methods remainschallenging due to the exponential growth of Hilbert space dimension and rapidspreading of entanglement. In this work, we introduce Fourier Neural Operators(FNO) as an efficient, accurate, and scalable surrogate for non-equilibriumquantum dynamics. Parameterized in Fourier space, FNO naturally capturestemporal correlations and remains minimally dependent on discretization oftime. We demonstrate the versatility of FNO through three complementarylearning paradigms: reconstructing effective Floquet Hamiltonians, predictingexpectation values of local observables, and learning quantum informationspreading. For each learning task, FNO achieves remarkable accuracy, whileattaining a significant speedup, compared to exact numerical methods. Moreover,FNO possesses a remarkable capacity to transfer learning across differenttemporal discretizations and system driving frequencies. We also show that FNOcan extrapolate beyond the time window provided by training data, enablingaccess to observables and operator-spreading dynamics that might otherwise bedifficult to obtain. By employing an appropriate local basis, we argue that thecomputational cost of FNOs scales only polynomially with the system size. Ourresults establish FNO as a versatile and scalable surrogate for predictingnon-equilibrium quantum dynamics, with potential applications to processingdata from near-term quantum computers.</description>
      <author>example@mail.com (Zihao Qi, Yang Peng, Christopher Earls)</author>
      <guid isPermaLink="false">2509.07084v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
      <link>http://arxiv.org/abs/2509.05657v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LM-Searcher框架，利用大语言模型进行跨领域神经架构优化，无需领域特定调优。核心是NCode表示方法和基于剪枝的子空间采样策略，使LLMs能从候选池中选择高性能架构。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在解决复杂优化问题(包括神经网络架构搜索)方面取得进展，但现有方法严重依赖提示工程和领域特定调优，限制了实用性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;提出一个无需大量领域特定适应的框架，利用LLMs进行跨领域神经架构优化。&lt;h4&gt;方法&lt;/h4&gt;开发NCode作为神经架构的通用数字字符串表示；将NAS问题重新表述为排序任务；使用基于剪枝的子空间采样策略训练LLMs；构建包含各种架构-性能对的数据集促进稳健学习。&lt;h4&gt;主要发现&lt;/h4&gt;LM-Searcher在领域内(如图像分类的CNNs)和领域外(如分割和生成的LoRA配置)任务中都能取得有竞争力的性能，建立了灵活且可推广的基于LLM的架构搜索新范式。&lt;h4&gt;结论&lt;/h4&gt;LM-Searcher为跨领域神经架构优化提供了有效且通用的方法，数据集和模型将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;近期大语言模型(LLMs)的进展为解决复杂优化问题开辟了新途径，包括神经网络架构搜索(NAS)。然而，现有的LLM驱动的NAS方法严重依赖提示工程和领域特定调优，限制了它们在不同任务上的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，一个新颖的框架，利用LLMs进行跨领域神经架构优化，无需大量领域特定的适应。我们方法的核心是NCode，一种神经架构的通用数字字符串表示，它实现了跨领域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLMs使用基于剪枝的子空间采样策略衍生的指令调整样本从候选池中选择高性能架构。我们策划的数据集包含各种架构-性能对，促进了稳健和可迁移的学习。全面的实验表明，LM-Searcher在领域内(如图像分类的CNNs)和领域外(如分割和生成的LoRA配置)任务中都取得了有竞争力的性能，建立了灵活且可推广的基于LLM的架构搜索新范式。数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Large Language Models (LLMs) has opened new avenues forsolving complex optimization problems, including Neural Architecture Search(NAS). However, existing LLM-driven NAS approaches rely heavily on promptengineering and domain-specific tuning, limiting their practicality andscalability across diverse tasks. In this work, we propose LM-Searcher, a novelframework that leverages LLMs for cross-domain neural architecture optimizationwithout the need for extensive domain-specific adaptation. Central to ourapproach is NCode, a universal numerical string representation for neuralarchitectures, which enables cross-domain architecture encoding and search. Wealso reformulate the NAS problem as a ranking task, training LLMs to selecthigh-performing architectures from candidate pools using instruction-tuningsamples derived from a novel pruning-based subspace sampling strategy. Ourcurated dataset, encompassing a wide range of architecture-performance pairs,encourages robust and transferable learning. Comprehensive experimentsdemonstrate that LM-Searcher achieves competitive performance in both in-domain(e.g., CNNs for image classification) and out-of-domain (e.g., LoRAconfigurations for segmentation and generation) tasks, establishing a newparadigm for flexible and generalizable LLM-based architecture search. Thedatasets and models will be released at https://github.com/Ashone3/LM-Searcher.</description>
      <author>example@mail.com (Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li)</author>
      <guid isPermaLink="false">2509.05657v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>OmniMap: A General Mapping Framework Integrating Optics, Geometry, and Semantics</title>
      <link>http://arxiv.org/abs/2509.07500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Robotics (TRO), project website:  https://omni-map.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniMap是一个创新的在线映射框架，首次同时捕捉光学、几何和语义场景属性，保持实时性能和模型紧凑性，在多个方面优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要准确和全面的3D环境感知，要求同时捕捉逼真的外观、精确的布局形状和开放词汇的场景理解。现有方法通常只能部分满足这些要求，并存在光学模糊、几何不规则和语义模糊等问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D环境感知方法中的光学模糊、几何不规则和语义模糊问题，开发一个能够同时捕捉光学、几何和语义场景属性的框架，同时保持实时性能和模型紧凑性。&lt;h4&gt;方法&lt;/h4&gt;OmniMap采用紧密耦合的3DGS-Voxel混合表示方法，结合细粒度建模和结构稳定性。引入了几项创新：自适应相机建模（用于运动模糊和曝光补偿）、具有法线约束的混合增量表示，以及用于鲁棒实例级理解的概率融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的方法相比，OmniMap在渲染保真度、几何准确性和零样本语义分割方面具有优越性能。该框架的通用性通过各种下游应用得到进一步证明。&lt;h4&gt;结论&lt;/h4&gt;OmniMap代表了第一个能够同时捕捉光学、几何和语义场景属性的在线映射框架，在多个方面优于现有方法，并具有广泛的下游应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;机器人系统需要准确和全面的3D环境感知，要求同时捕捉逼真的外观（光学）、精确的布局形状（几何）和开放词汇的场景理解（语义）。现有方法通常只能部分满足这些要求，并存在光学模糊、几何不规则和语义模糊等问题。为应对这些挑战，我们提出了OmniMap。总体而言，OmniMap代表了第一个在线映射框架，能够同时捕捉光学、几何和语义场景属性，同时保持实时性能和模型紧凑性。在架构层面，OmniMap采用紧密耦合的3DGS-Voxel混合表示方法，结合细粒度建模和结构稳定性。在实现层面，OmniMap确定了不同模态的关键挑战，并引入了几项创新：用于运动模糊和曝光补偿的自适应相机建模、具有法线约束的混合增量表示，以及用于鲁棒实例级理解的概率融合。大量实验表明，与最先进的方法相比，OmniMap在渲染保真度、几何准确性和零样本语义分割方面具有优越性能。该框架的通用性通过各种下游应用得到进一步证明，包括多领域场景问答、交互式编辑、感知引导操作和地图辅助导航。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决机器人系统中3D环境感知的全面性问题，即如何同时实现高保真的光学渲染、精确的几何重建和开放词汇的语义理解。这个问题很重要，因为随着具身人工智能的发展，机器人需要高质量的多维环境表示来支持各种任务，如交互式虚拟操作（场景问答和编辑）和多粒度物理交互（桌面级操作和房间级导航），这对提升机器人在现实世界中的操作能力至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性，提出了四个关键子问题：1)如何集成多样化场景属性；2)如何处理相机运动模糊和曝光不一致；3)如何提高几何稳定性；4)如何实现开放环境中的实例识别与融合。作者借鉴了多项现有工作：使用TSDF-Fusion作为体素映射基础，利用3DGS实现高保真渲染，采用YOLO-World进行开放词汇检测，使用TAP模型进行分割和标题生成，并通过SBERT进行嵌入编码。基于这些借鉴，作者创新性地设计了混合3DGS-体素表示框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合的3DGS-体素表示，将光学、几何和语义三种场景属性集成到一个统一的在线映射框架中，体素作为增量映射的基本单元封装概率语义，而新分配的体素则引导支持精确光学和几何重建的高斯原语初始化。整体流程包含三个主要模块：1)2D语言嵌入提取器，通过检测、分割、标题生成和编码获取实例级语义信息；2)概率体素重建器，执行3D域中的增量开放集实例融合；3)运动鲁棒的3DGS增量重建器，从新体素初始化高斯并在多种模态下进行渲染和监督。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全面的表示框架，首次连接光学、几何和语义；2)高保真光学处理，通过四个可微分参数建模相机模糊和曝光；3)详细几何重建，使用增量3DGS策略和法线监督；4)开放语义理解，设计高效实例理解管道；5)实现最先进的性能指标。相比之前工作，OmniMap突破了传统方法的局限：不同于传统语义体积映射的离散表示，不同于表面重建方法缺乏语义，不同于NeRF/3DGS方法缺乏语义理解，不同于开放词汇映射的稀疏表示，是首个将开放词汇理解直接集成到3DGS增量映射中的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniMap提出了首个集成光学、几何和语义的通用在线映射框架，通过创新的混合3DGS-体素表示和概率建模方法，实现了实时、高保真、高精度的3D环境感知，支持多样化的下游应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic systems demand accurate and comprehensive 3D environment perception,requiring simultaneous capture of photo-realistic appearance (optical), preciselayout shape (geometric), and open-vocabulary scene understanding (semantic).Existing methods typically achieve only partial fulfillment of theserequirements while exhibiting optical blurring, geometric irregularities, andsemantic ambiguities. To address these challenges, we propose OmniMap. Overall,OmniMap represents the first online mapping framework that simultaneouslycaptures optical, geometric, and semantic scene attributes while maintainingreal-time performance and model compactness. At the architectural level,OmniMap employs a tightly coupled 3DGS-Voxel hybrid representation thatcombines fine-grained modeling with structural stability. At the implementationlevel, OmniMap identifies key challenges across different modalities andintroduces several innovations: adaptive camera modeling for motion blur andexposure compensation, hybrid incremental representation with normalconstraints, and probabilistic fusion for robust instance-level understanding.Extensive experiments show OmniMap's superior performance in renderingfidelity, geometric accuracy, and zero-shot semantic segmentation compared tostate-of-the-art methods across diverse scenes. The framework's versatility isfurther evidenced through a variety of downstream applications, includingmulti-domain scene Q&amp;A, interactive editing, perception-guided manipulation,and map-assisted navigation.</description>
      <author>example@mail.com (Yinan Deng, Yufeng Yue, Jianyu Dou, Jingyu Zhao, Jiahui Wang, Yujie Tang, Yi Yang, Mengyin Fu)</author>
      <guid isPermaLink="false">2509.07500v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>CAViAR: Critic-Augmented Video Agentic Reasoning</title>
      <link>http://arxiv.org/abs/2509.07680v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合大型语言模型代理和视频感知模块的方法来解决复杂视频推理任务，并引入评判机制来优化推理过程。&lt;h4&gt;背景&lt;/h4&gt;视频理解近年来取得显著进展，模型在短片段感知任务上性能不断提升，但在需要复杂推理的任务中，随着查询变得更复杂、视频变长，性能有所下降。多个基准测试如LVBench、Neptune和ActivityNet-RTL证实了这一现象。&lt;h4&gt;目的&lt;/h4&gt;探索是否可以利用现有的视频感知能力来成功执行更复杂的视频推理任务。&lt;h4&gt;方法&lt;/h4&gt;开发了一个大型语言模型代理，可以访问视频模块作为子代理或工具。与之前遵循固定程序的方法不同，该代理使用每次调用模块的结果来确定后续步骤。同时引入评判机制来区分成功和不成功的推理序列。&lt;h4&gt;主要发现&lt;/h4&gt;代理和评判机制的组合在LVBench、Neptune和ActivityNet-RTL等数据集上取得了强大的性能表现。&lt;h4&gt;结论&lt;/h4&gt;通过将大型语言模型与视频感知模块结合，并引入评判机制，可以有效解决复杂的视频推理任务，突破了现有方法在复杂查询和长视频上的性能瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;视频理解近年来取得了显著进展，模型在短片段感知任务上的性能持续提升。然而，多个最近的基准测试，如LVBench、Neptune和ActivityNet-RTL表明，对于需要复杂推理的任务，随着查询变得更加复杂和视频变得更长，性能有所下降。在这项工作中，我们问：现有的感知能力是否可以被利用来成功执行更复杂的视频推理？特别是，我们开发了一个大型语言模型代理，可以访问视频模块作为子代理或工具。与之前的工作如Visual Programming、ViperGPT和MoReVQA遵循固定程序来解决查询不同，该代理使用每次调用模块的结果来确定后续步骤。受到文本推理领域工作的启发，我们引入了一个评判机制来区分代理成功和不成功的序列。我们展示了代理和评判机制的组合在上述提到的数据集上取得了强大的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-09&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video understanding has seen significant progress in recent years, withmodels' performance on perception from short clips continuing to rise. Yet,multiple recent benchmarks, such as LVBench, Neptune, and ActivityNet-RTL, showperformance wanes for tasks requiring complex reasoning on videos as queriesgrow more complex and videos grow longer. In this work, we ask: can existingperception capabilities be leveraged to successfully perform more complex videoreasoning? In particular, we develop a large language model agent given accessto video modules as subagents or tools. Rather than following a fixed procedureto solve queries as in previous work such as Visual Programming, ViperGPT, andMoReVQA, the agent uses the results of each call to a module to determinesubsequent steps. Inspired by work in the textual reasoning domain, weintroduce a critic to distinguish between instances of successful andunsuccessful sequences from the agent. We show that the combination of ouragent and critic achieve strong performance on the previously-mentioneddatasets.</description>
      <author>example@mail.com (Sachit Menon, Ahmet Iscen, Arsha Nagrani, Tobias Weyand, Carl Vondrick, Cordelia Schmid)</author>
      <guid isPermaLink="false">2509.07680v1</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors</title>
      <link>http://arxiv.org/abs/2509.00969v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 8 figures, EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LangDC的语言感知动态标记压缩器，解决了大型视频语言模型处理高容量视觉标记时的效率问题，通过动态调整压缩比来适应不同视频片段的语义密度。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型的最新进展彻底改变了视频理解任务，但其效率受到处理大量视觉标记的限制。现有的标记压缩策略采用固定压缩比，忽略了不同视频片段间语义密度的变化。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据视频片段语义密度动态调整压缩比的方法，以优化计算资源分配并提高模型效率。&lt;h4&gt;方法&lt;/h4&gt;LangDC利用轻量级语言模型描述视频片段，将其转换为软标题标记作为视觉表示，并通过语义密度感知监督进行训练，以覆盖关键视觉线索并基于场景丰富性动态调整压缩比。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与VideoGPT+相比，LangDC减少了49%的FLOPs同时保持竞争性性能，且能根据视频片段丰富性自适应调整标记压缩比。&lt;h4&gt;结论&lt;/h4&gt;LangDC通过模仿人类描述视觉内容的方式（复杂场景用更详细语言，简单场景用简洁语言），实现了高效的视频标记压缩，为视频语言模型提供了更高效的计算框架。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型的最新进展彻底改变了视频理解任务。然而，它们的效率受到处理大量视觉标记的显著限制。现有的标记压缩策略采用固定压缩比，忽略了不同视频片段间语义密度的变化。因此，这导致信息丰富的片段因标记不足而表示不充分，同时对静态或内容贫乏的片段进行不必要的计算。为解决此问题，我们提出了LangDC，一种语言感知的动态标记压缩器。LangDC利用轻量级语言模型描述视频片段，将其转换为软标题标记作为视觉表示。通过我们提出的语义密度感知监督进行训练，LangDC旨在1)覆盖下游任务推理所需的关键视觉线索；2)基于场景丰富性（由描述长度反映）动态调整压缩比。我们的设计模仿了人类如何动态表达他们所看到的内容：复杂场景（看到更多）引发更详细的语言来传达细微差别（说更多），而简单的场景用更少的词描述。实验结果表明，与VideoGPT+相比，我们的方法减少了49%的FLOPs，同时保持竞争性性能。此外，定性结果表明我们的方法根据视频片段丰富性自适应地调整标记压缩比。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video-language models have revolutionized videounderstanding tasks. However, their efficiency is significantly constrained byprocessing high volumes of visual tokens. Existing token compression strategiesapply a fixed compression ratio, ignoring the variability in semantic densityamong different video clips. Consequently, this lead to inadequaterepresentation of information-rich clips due to insufficient tokens andunnecessary computation on static or content-poor ones. To address this, wepropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages alightweight language model to describe video clips, converting them into softcaption tokens as visual representations. Trained with our proposed semanticdensity-aware supervision, LangDC aims to 1) cover key visual cues necessaryfor downstream task reasoning and 2) dynamically adjust compression ratiosbased on scene richness, reflected by descriptions length. Our design mimicshow humans dynamically express what they see: complex scenes (seeing more)elicit more detailed language to convey nuances (saying more), whereas simplerscenes are described with fewer words. Experimental results show that ourmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitiveperformance. Furthermore, qualitative results demonstrate our approachadaptively adjusts the token compression ratio based on video segment richness.</description>
      <author>example@mail.com (Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.00969v2</guid>
      <pubDate>Wed, 10 Sep 2025 15:00:20 +0800</pubDate>
    </item>
    <item>
      <title>Interleaving Reasoning for Better Text-to-Image Generation</title>
      <link>http://arxiv.org/abs/2509.06945v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了交错推理生成(IRG)框架，通过在文本思考和图像合成之间交替进行，提高文本到图像生成质量。作者还提出了交错推理生成学习(IRGL)方法和IRGL-300K数据集，通过两阶段训练实现了在多个评估指标上的显著提升。&lt;h4&gt;背景&lt;/h4&gt;统一多模态理解和生成模型在图像生成能力上取得了显著进步，但在遵循指令和保持细节方面与GPT-4o等系统相比仍有较大差距。&lt;h4&gt;目的&lt;/h4&gt;探索交错推理是否能进一步提高文本到图像(T2I)的生成质量，缩小与紧密耦合理解与生成的系统之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出交错推理生成(IRG)框架，模型先产生文本思考指导初始图像，再反思结果以细化细节和质量；提出交错推理生成学习(IRGL)方法和IRGL-300K数据集；采用两阶段训练：首先建立思考和反思能力，然后调整IRG管道。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验显示最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN等指标上取得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;交错推理框架能有效提高文本到图像生成质量，特别是在细节保持和视觉质量方面。代码、模型权重和数据集将在GitHub上发布。&lt;h4&gt;翻译&lt;/h4&gt;最近的统一多模态理解和生成模型在图像生成能力方面取得了显著进步，但在遵循指令和保持细节方面与紧密耦合理解与生成的系统(如GPT-4o)相比仍有较大差距。受最近交错推理进展的启发，我们探索了这种推理是否能进一步提高文本到图像(T2I)生成。我们引入了交错推理生成(IRG)，一个在基于文本的思考和图像合成之间交替的框架：模型首先产生基于文本的思考来指导初始图像，然后反思结果以细化细粒度细节、视觉质量和美学，同时保持语义。为了有效训练IRG，我们提出了交错推理生成学习(IRGL)，它针对两个子目标：(1)加强初始思考和生成阶段以建立核心内容和基础质量，以及(2)实现高质量的文本反思并在后续图像中忠实地实施这些改进。我们整理了IRGL-300K数据集，组织成六个分解的学习模式，共同覆盖基于文本的思考和完整的思考-图像轨迹学习。从一个原生发出交错文本-图像输出的统一基础模型开始，我们的两阶段训练首先建立强大的思考和反思能力，然后在完整的思考-图像轨迹数据中高效地调整IRG管道。大量实验显示了最先进的性能，在GenEval、WISE、TIIF、GenAI-Bench和OneIG-EN上取得了5-10个百分点的绝对提升，同时在视觉质量和细粒度保真度方面也取得了显著改进。代码、模型权重和数据集将在以下地址发布：https://github.com/Osilly/Interleaving-Reasoning-Generation 。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unified multimodal understanding and generation models recently have achievesignificant improvement in image generation capability, yet a large gap remainsin instruction following and detail preservation compared to systems thattightly couple comprehension with generation such as GPT-4o. Motivated byrecent advances in interleaving reasoning, we explore whether such reasoningcan further improve Text-to-Image (T2I) generation. We introduce InterleavingReasoning Generation (IRG), a framework that alternates between text-basedthinking and image synthesis: the model first produces a text-based thinking toguide an initial image, then reflects on the result to refine fine-graineddetails, visual quality, and aesthetics while preserving semantics. To trainIRG effectively, we propose Interleaving Reasoning Generation Learning (IRGL),which targets two sub-goals: (1) strengthening the initial think-and-generatestage to establish core content and base quality, and (2) enabling high-qualitytextual reflection and faithful implementation of those refinements in asubsequent image. We curate IRGL-300K, a dataset organized into six decomposedlearning modes that jointly cover learning text-based thinking, and fullthinking-image trajectories. Starting from a unified foundation model thatnatively emits interleaved text-image outputs, our two-stage training firstbuilds robust thinking and reflection, then efficiently tunes the IRG pipelinein the full thinking-image trajectory data. Extensive experiments show SoTAperformance, yielding absolute gains of 5-10 points on GenEval, WISE, TIIF,GenAI-Bench, and OneIG-EN, alongside substantial improvements in visual qualityand fine-grained fidelity. The code, model weights and datasets will bereleased in: https://github.com/Osilly/Interleaving-Reasoning-Generation .</description>
      <author>example@mail.com (Wenxuan Huang, Shuang Chen, Zheyong Xie, Shaosheng Cao, Shixiang Tang, Yufan Shen, Qingyu Yin, Wenbo Hu, Xiaoman Wang, Yuntian Tang, Junbo Qiao, Yue Guo, Yao Hu, Zhenfei Yin, Philip Torr, Yu Cheng, Wanli Ouyang, Shaohui Lin)</author>
      <guid isPermaLink="false">2509.06945v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
  <item>
      <title>FoMo4Wheat: Toward reliable crop vision foundation models with globally curated data</title>
      <link>http://arxiv.org/abs/2509.06907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FoMo4Wheat是一种针对小麦的视觉基础模型，使用大规模、多样化的小麦图像数据集进行自监督预训练，在各种农业视觉任务中表现优异，并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;视觉驱动的田间监测是数字农业的核心，但基于通用域预训练骨干的模型难以在不同任务中泛化，这是由于精细变化的冠层结构与田间波动条件的相互作用导致的。&lt;h4&gt;目的&lt;/h4&gt;开发一种作物特定的视觉基础模型，解决通用域预训练模型在农业视觉任务中的泛化问题，提高田间监测的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出FoMo4Wheat模型，在ImAg4Wheat数据集上进行自监督预训练。该数据集包含250万张高分辨率图像，收集自30个全球站点，跨越十年，涵盖超过2000个基因型和500种环境条件。&lt;h4&gt;主要发现&lt;/h4&gt;小麦特定的预训练产生了稳健且可迁移到其他作物和杂草的表示。在十个田间视觉任务（冠层和器官层面）上，FoMo4Wheat模型持续优于最先进的通用域预训练模型。&lt;h4&gt;结论&lt;/h4&gt;作物特定的基础模型对可靠的田间感知具有重要价值，为开发具有跨物种和跨任务能力的通用作物基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;视觉驱动的田间监测是数字农业的核心，然而基于通用域预训练骨干构建的模型往往无法跨任务泛化，这是由于精细变化的冠层结构与田间波动条件的相互作用所致。我们提出了FoMo4Wheat，这是首批针对作物领域的视觉基础模型之一，使用自监督方法在ImAg4Wheat上进行预训练，这是迄今为止最大且最多样化的小麦图像数据集（包含250万张高分辨率图像，在十年间从30个全球站点收集，涵盖&gt;2000个基因型和&gt;500种环境条件）。这种小麦特定的预训练产生了对小麦稳健且可迁移到其他作物和杂草的表示。在冠层和器官层面的十个田间视觉任务中，FoMo4Wheat模型持续优于在通用域数据集上预训练的最先进模型。这些结果证明了作物特定基础模型对可靠田间感知的价值，并为具有跨物种和跨任务能力的通用作物基础模型铺平了道路。FoMo4Wheat模型和ImAg4Wheat数据集已在线公开：https://github.com/PheniX-Lab/FoMo4Wheat和https://huggingface.co/PheniX-Lab/FoMo4Wheat。演示网站为：https://fomo4wheat.phenix-lab.com/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-driven field monitoring is central to digital agriculture, yet modelsbuilt on general-domain pretrained backbones often fail to generalize acrosstasks, owing to the interaction of fine, variable canopy structures withfluctuating field conditions. We present FoMo4Wheat, one of the firstcrop-domain vision foundation model pretrained with self-supervision onImAg4Wheat, the largest and most diverse wheat image dataset to date (2.5million high-resolution images collected over a decade at 30 global sites,spanning &gt;2,000 genotypes and &gt;500 environmental conditions). Thiswheat-specific pretraining yields representations that are robust for wheat andtransferable to other crops and weeds. Across ten in-field vision tasks atcanopy and organ levels, FoMo4Wheat models consistently outperformstate-of-the-art models pretrained on general-domain dataset. These resultsdemonstrate the value of crop-specific foundation models for reliable in-fieldperception and chart a path toward a universal crop foundation model withcross-species and cross-task capabilities. FoMo4Wheat models and the ImAg4Wheatdataset are publicly available online: https://github.com/PheniX-Lab/FoMo4Wheatand https://huggingface.co/PheniX-Lab/FoMo4Wheat. The demonstration website is:https://fomo4wheat.phenix-lab.com/.</description>
      <author>example@mail.com (Bing Han, Chen Zhu, Dong Han, Rui Yu, Songliang Cao, Jianhui Wu, Scott Chapman, Zijian Wang, Bangyou Zheng, Wei Guo, Marie Weiss, Benoit de Solan, Andreas Hund, Lukas Roth, Kirchgessner Norbert, Andrea Visioni, Yufeng Ge, Wenjuan Li, Alexis Comar, Dong Jiang, Dejun Han, Fred Baret, Yanfeng Ding, Hao Lu, Shouyang Liu)</author>
      <guid isPermaLink="false">2509.06907v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Generic Foundation Models for Multimodal Surgical Data Analysis</title>
      <link>http://arxiv.org/abs/2509.06831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures; accepted at ML-CDS @ MICCAI 2025, Daejeon,  Republic of Korea&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何通过迁移学习适应通用基础模型以及整合手术室的互补模态来支持外科数据科学。使用V-JEPA作为多模态模型的基础，研究了在未标记手术视频数据上微调以及整合手术室其他时间解析数据流对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;外科数据科学需要有效利用手术数据，而通用基础模型和手术室的多种数据模态提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;研究单模态基础模型(V-JEPA)如何作为多模态模型的基础用于微创手术支持，以及迁移学习和多模态整合如何提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;使用V-JEPA作为基础模型，在内部肝脏手术视频数据集和公共HeiCo数据集上测试预测住院时间、术后并发症和手术阶段识别任务。通过在未标记视频上微调模型以及整合手术室的其他时间解析数据流来评估性能变化。&lt;h4&gt;主要发现&lt;/h4&gt;在领域特定数据上微调提高了模型性能；整合其他时间解析数据也有益于模型；预训练的视频单模态基线与最佳提交相当，领域特定微调进一步提高了准确性。&lt;h4&gt;结论&lt;/h4&gt;外科数据科学可以有效地利用公共通用基础模型；领域适应和整合手术室合适的互补数据流具有很大潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们研究如何通过迁移学习适应通用基础模型以及整合手术室(OR)的互补模态来支持外科数据科学。为此，我们使用V-JEPA作为多模态模型的基础，用于微创手术支持。我们分析了模型的下游性能如何受益于(a)在未标记的手术视频数据上进行微调，以及(b)在多模态设置中提供来自手术室的其他时间解析数据流。在内部肝脏手术视频数据集中，我们分析预测住院时间和术后并发症的任务。在公共HeiCo数据集的视频中，我们分析手术阶段识别任务。作为基线，我们将预训练的V-JEPA应用于所有任务。然后我们在未标记的保留视频上进行微调，以研究领域适应后的性能变化。遵循模块化决策支持网络的理念，我们通过训练单独的编码器整合来自手术室的其他数据流，与V-JEPA的嵌入形成共享表示空间。我们的实验表明，在领域特定数据上微调可以提高模型性能。在内部数据中，整合其他时间解析数据同样有利于模型。在HeiCo数据上，仅使用预训练视频的单模态基线设置与EndoVis2017挑战的最佳提交相当，而在领域特定数据上微调可以进一步提高准确性。我们的结果因此证明了外科数据科学如何可以利用公共通用基础模型。同样，它们表明了领域适应和整合来自手术室合适的互补数据流的潜力。为支持进一步研究，我们在https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025发布了我们的代码和模型权重。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigate how both the adaptation of a generic foundation model viatransfer learning and the integration of complementary modalities from theoperating room (OR) can support surgical data science. To this end, we useV-JEPA as the single-modality foundation of a multimodal model for minimallyinvasive surgery support. We analyze how the model's downstream performance canbenefit (a) from finetuning on unlabeled surgical video data and (b) fromproviding additional time-resolved data streams from the OR in a multimodalsetup.  In an in-house dataset of liver surgery videos, we analyze the tasks ofpredicting hospital length of stay and postoperative complications. In videosof the public HeiCo dataset, we analyze the task of surgical phase recognition.As a baseline, we apply pretrained V-JEPA to all tasks. We then finetune it onunlabeled, held-out videos to investigate its change in performance afterdomain adaptation. Following the idea of modular decision support networks, weintegrate additional data streams from the OR by training a separate encoder toform a shared representation space with V-JEPA's embeddings.  Our experiments show that finetuning on domain-specific data increases modelperformance. On the in-house data, integrating additional time-resolved datalikewise benefits the model. On the HeiCo data, accuracy of the pretrainedvideo-only, single-modality baseline setup is on par with the top-performingsubmissions of the EndoVis2017 challenge, while finetuning on domain-specificdata increases accuracy further. Our results thus demonstrate how surgical datascience can leverage public, generic foundation models. Likewise, they indicatethe potential of domain adaptation and of integrating suitable complementarydata streams from the OR. To support further research, we release our code andmodel weights at https://github.com/DigitalSurgeryLab-Basel/ML-CDS-2025.</description>
      <author>example@mail.com (Simon Pezold, Jérôme A. Kurylec, Jan S. Liechti, Beat P. Müller, Joël L. Lavanchy)</author>
      <guid isPermaLink="false">2509.06831v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Curia: A Multi-Modal Foundation Model for Radiology</title>
      <link>http://arxiv.org/abs/2509.06830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了Curia，一个在大型医院多年横断面影像数据上训练的基础模型，用于AI辅助放射学解释，该模型在多模态和低数据设置下展现出临床意义的重要涌现特性。&lt;h4&gt;背景&lt;/h4&gt;当前的AI辅助放射学解释主要基于狭窄的、单任务模型，这种方法难以覆盖广泛的成像方式、疾病和放射学发现。基础模型(FMs)有望在多种模态和低数据设置下实现广泛泛化，但这一潜力在放射学领域尚未实现。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在多种成像模态和低数据设置下有效工作的基础模型，以克服当前单任务AI模型的局限性，并实现更广泛的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;研究团队训练了一个名为Curia的基础模型，该模型在一个主要医院多年的横断面影像输出上进行训练，包含150,000次检查(130 TB)。他们在一个新创建的19项任务外部验证基准上评估了该模型。&lt;h4&gt;主要发现&lt;/h4&gt;Curia能够准确识别器官，检测脑出血和心肌梗死等疾病，在肿瘤分期中预测结果，其表现达到或超过了放射科医生和最近的基础模型，并且在跨模态和低数据设置下展现出临床意义的重要涌现特性。&lt;h4&gt;结论&lt;/h4&gt;Curia代表了放射学基础模型的重要进展，通过在大型真实世界数据集上训练，实现了在多种任务和条件下的高性能。研究团队发布了基础模型的权重，以加速该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;AI-assisted radiological interpretation：AI辅助放射学解释；Foundation models：基础模型；Curia：Curia（模型名称）；cross-sectional imaging：横断面影像；brain hemorrhages：脑出血；myocardial infarctions：心肌梗死；tumor staging：肿瘤分期&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决放射学AI领域过度依赖狭窄单任务模型的问题。现实中，这种'一个任务一个模型'的方法资源密集且难以覆盖广泛的成像方式、疾病和放射学发现，成为AI模型整合到临床工作流程的瓶颈。基础模型(FMs)有潜力实现跨模态和低数据设置下的广泛泛化，但这一潜力在放射学领域尚未实现。放射学作为医学诊断的核心，AI辅助有潜力提高诊断效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前放射学AI的局限性，认识到基础模型在自然图像领域的成功(如DINOv2和MAE)可迁移到医学影像。他们借鉴了Vision Transformer架构和DINOv2自监督学习算法，同时参考了现有放射学基础模型如BiomedCLIP和MedImageInsight。设计上，他们专注于使用大规模真实世界临床数据而非混合专业数据，并创建了全面的评估基准。在下游任务适配上，他们借鉴了SAM和RadSAM的提示分割框架，但进行了医学领域特定的改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用大规模未标记医学影像数据进行自监督学习，学习通用的视觉特征表示，创建能适应多种下游任务的基础模型，实现跨模态泛化。流程包括：1)收集并预处理一家医院15万次检查的130TB真实世界数据；2)使用Vision Transformer架构和DINOv2算法在2亿图像上预训练；3)为不同下游任务设计适配方法(图像级、对象级、体积级预测等)；4)在包含19个任务的CuriaBench基准上全面评估，并与放射科医师和其他模型比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用最大规模真实世界医学影像语料库(2亿图像，130TB)；2)实现卓越的跨模态泛化能力(CT到MRI性能下降仅9.17%)；3)在少样本学习中表现优异；4)创建全面的CuriaBench基准(19个任务)；5)性能达到或超过放射科医师水平；6)开源模型权重。相比之前工作，Curia使用更大规模且来源单一的放射学数据(而非多领域混合数据)，采用DINOv2自监督方法，在跨模态任务和少样本学习中表现显著更优，评估更全面且包含与放射科医师的比较。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Curia通过在超大规模真实世界医学影像数据上的自监督学习，实现了跨模态和低数据设置下的强大泛化能力，在广泛放射学任务中达到或超过放射科医师性能，为放射学AI提供了新的基础模型范式。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-assisted radiological interpretation is based on predominantly narrow,single-task models. This approach is impractical for covering the vast spectrumof imaging modalities, diseases, and radiological findings. Foundation models(FMs) hold the promise of broad generalization across modalities and inlow-data settings. However, this potential has remained largely unrealized inradiology. We introduce Curia, a foundation model trained on the entirecross-sectional imaging output of a major hospital over several years, which toour knowledge is the largest such corpus of real-world data-encompassing150,000 exams (130 TB). On a newly curated 19-task external validationbenchmark, Curia accurately identifies organs, detects conditions like brainhemorrhages and myocardial infarctions, and predicts outcomes in tumor staging.Curia meets or surpasses the performance of radiologists and recent foundationmodels, and exhibits clinically significant emergent properties incross-modality, and low-data regimes. To accelerate progress, we release ourbase model's weights at https://huggingface.co/raidium/curia.</description>
      <author>example@mail.com (Corentin Dancette, Julien Khlaut, Antoine Saporta, Helene Philippe, Elodie Ferreres, Baptiste Callard, Théo Danielou, Léo Alberge, Léo Machado, Daniel Tordjman, Julie Dupuis, Korentin Le Floch, Jean Du Terrail, Mariam Moshiri, Laurent Dercle, Tom Boeken, Jules Gregory, Maxime Ronot, François Legou, Pascal Roux, Marc Sapoval, Pierre Manceron, Paul Hérent)</author>
      <guid isPermaLink="false">2509.06830v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>VIM-GS: Visual-Inertial Monocular Gaussian Splatting via Object-level Guidance in Large Scenes</title>
      <link>http://arxiv.org/abs/2509.06685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架，通过结合视觉惯性运动恢复结构的精确稀疏深度和大型基础模型的密集粗糙深度，实现了高质量的渲染效果。&lt;h4&gt;背景&lt;/h4&gt;传统高斯溅射技术需要精确的深度信息来初始化高斯椭球体，通常依赖RGB-D/立体相机，但这些传感器的有限范围使其难以在大场景中应用。单目图像虽然易于获取，但缺乏深度信息指导学习，导致新视角合成结果质量较差。现有的单目深度估计大型基础模型存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索模糊不清等问题。&lt;h4&gt;目的&lt;/h4&gt;从单目RGB输入生成密集、精确的深度图像，用于高确定性的高斯溅射渲染，以解决大场景中的新视角合成问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种结合视觉惯性运动恢复结构(SfM)和大型基础模型(LFMs)的深度生成方法。具体包括：1)利用SfM提供的精确但稀疏的深度信息来 refine LFM提供的密集但粗糙的深度信息；2)提出对象分割的深度传播算法，连接稀疏输入和密集输出；3)开发动态深度细化模块，处理动态对象的受损SfM深度，并细化粗糙的LFM深度。&lt;h4&gt;主要发现&lt;/h4&gt;通过公共和定制数据集的实验证明，VIM-GS在大场景的新视角合成中具有优越的渲染质量，能够有效解决传统方法在大场景中面临的挑战。&lt;h4&gt;结论&lt;/h4&gt;VIM-GS框架成功结合了视觉惯性运动恢复结构和大型基础模型的优势，通过创新的深度传播和细化算法，实现了大场景中高质量的新视角合成，为单目图像驱动的三维场景重建提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;VIM-GS是一种使用单目图像进行大场景新视角合成的高斯溅射框架。高斯溅射通常需要精确的深度信息来使用RGB-D/立体相机初始化高斯椭球体。它们的有限深度传感范围使得高斯溅射难以在大场景中工作。然而，单目图像缺乏引导学习的深度信息，导致新视角合成结果质量较差。尽管有可用于单目深度估计的大型基础模型，但它们存在跨帧不一致、远距离场景不准确以及对欺骗性纹理线索模糊不清等问题。本文旨在从单目RGB输入生成密集、精确的深度图像，用于高确定性的高斯溅射渲染。关键思想是利用视觉惯性运动恢复结构提供的精确但稀疏的深度信息来 refine 大型基础模型提供的密集但粗糙的深度信息。为了连接稀疏输入和密集输出，我们提出了一种对象分割的深度传播算法，用于渲染结构化对象像素的深度。然后我们开发了一个动态深度细化模块，用于处理动态对象的受损SfM深度，并细化粗糙的LFM深度。使用公共和定制数据集进行的实验证明了VIM-GS在大场景中的优越渲染质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在大场景下使用单目图像进行高质量新视角合成的问题。传统高斯溅射方法需要RGB-D或立体相机提供深度信息，但这些设备深度感知范围有限，难以应用于大场景。单目图像虽然成本低，但缺乏深度信息导致渲染质量差。这个问题很重要，因为高成本激光雷达限制了GS技术在消费级应用中的普及，而低成本单目相机实现大场景高质量渲染对自动驾驶、虚拟现实等领域有重要价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：RGB-D/立体相机深度范围有限，单目SfM只提供稀疏深度，而大规模基础模型深度估计存在跨帧不一致和远处不准确等问题。设计上借鉴了视觉惯性SLAM的准确稀疏深度和大规模基础模型的密集粗糙深度，结合分割模型进行物体识别。整体思路是优势互补，将不同方法的优点结合，弥补各自的不足，通过物体分割深度传播和动态深度细化两个模块实现深度信息的优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用视觉惯性SfM提供的准确但稀疏的深度信息来 refine大规模基础模型提供的密集但粗糙的深度信息，生成密集准确的深度图像。整体流程：1)输入RGB和IMU数据，通过VI前端获得稀疏特征3D坐标，并用分割模型生成物体掩码；2)物体分割深度传播模块识别结构化物体，将稀疏深度传播到物体所有像素；3)动态深度细化模块处理动态物体错误深度，通过新损失函数细化LFMs深度；4)输出密集准确深度图像，为高斯溅射提供良好初始化，实现大场景高质量渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)物体分割深度传播算法，识别结构化物体并将稀疏深度传播到所有像素；2)动态深度细化模块，处理动态物体错误深度并细化LFMs深度；3)结合视觉惯性SfM和大规模基础模型优势。相比之前工作：传统GS依赖RGB-D/立体相机，深度范围有限；单目支持方法因随机或稀疏深度初始化，大场景效果差；SfM只提供稀疏深度；LFMs存在跨帧不一致和远处不准确问题。VIM-GS首次有效结合两种深度信息，通过物体分割和动态处理解决了各自局限。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VIM-GS通过结合视觉惯性SfM的准确稀疏深度和大规模基础模型的密集粗糙深度，并引入物体分割和动态处理机制，实现了在大场景下使用单目相机进行高质量实时新视角合成的突破。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; VIM-GS is a Gaussian Splatting (GS) framework using monocular images fornovel-view synthesis (NVS) in large scenes. GS typically requires accuratedepth to initiate Gaussian ellipsoids using RGB-D/stereo cameras. Their limiteddepth sensing range makes it difficult for GS to work in large scenes.Monocular images, however, lack depth to guide the learning and lead toinferior NVS results. Although large foundation models (LFMs) for monoculardepth estimation are available, they suffer from cross-frame inconsistency,inaccuracy for distant scenes, and ambiguity in deceptive texture cues. Thispaper aims to generate dense, accurate depth images from monocular RGB inputsfor high-definite GS rendering. The key idea is to leverage the accurate butsparse depth from visual-inertial Structure-from-Motion (SfM) to refine thedense but coarse depth from LFMs. To bridge the sparse input and dense output,we propose an object-segmented depth propagation algorithm that renders thedepth of pixels of structured objects. Then we develop a dynamic depthrefinement module to handle the crippled SfM depth of dynamic objects andrefine the coarse LFM depth. Experiments using public and customized datasetsdemonstrate the superior rendering quality of VIM-GS in large scenes.</description>
      <author>example@mail.com (Shengkai Zhang, Yuhe Liu, Guanjun Wu, Jianhua He, Xinggang Wang, Mozi Chen, Kezhong Liu)</author>
      <guid isPermaLink="false">2509.06685v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MM-DINOv2: Adapting Foundation Models for Multi-Modal Medical Image Analysis</title>
      <link>http://arxiv.org/abs/2509.06617v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种名为MM-DINOv2的新框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像分析。该方法通过多模态块嵌入和全模态掩码技术解决了多模态医学影像分析中的挑战，并利用半监督学习提高预测准确性和可靠性。在胶质瘤亚型分类任务中，该方法取得了优于现有监督方法的性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型如DINOv2在医学影像领域展现出巨大潜力，尽管它们最初是为自然图像设计的。然而，这些模型的设计主要用于单模态图像分析，限制了它们在神经学、肿瘤学等医学领域中常见的多模态成像任务中的有效性。虽然监督模型在这种情况下表现良好，但它们无法利用未标记的数据集，并且在处理缺失模态方面存在困难，这是临床环境中常见的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理多模态医学影像数据的框架，解决现有监督模型无法利用未标记数据和处理缺失模态的问题，从而提高医学预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了MM-DINOv2框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像。该方法包括：1) 使用多模态块嵌入，使视觉基础模型能够有效处理多模态影像数据；2) 采用全模态掩码技术，鼓励模型学习稳健的跨模态关系，以解决缺失模态的问题；3) 利用半监督学习来利用大型未标记数据集，提高医学预测的准确性和可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;在胶质瘤亚型分类任务中（基于多序列脑部MRI），该方法在外部测试集上获得了0.6的马修斯相关系数(MCC)，比最先进的监督方法高出11.1%。&lt;h4&gt;结论&lt;/h4&gt;该研究建立了一种可扩展且稳健的多模态医学影像任务解决方案，它利用了在自然图像上预训练的强大视觉基础模型，同时解决了缺失数据和有限标注等现实临床挑战。&lt;h4&gt;翻译&lt;/h4&gt;DINOv2等视觉基础模型尽管源自自然图像领域，但在医学影像中展现出巨大潜力。然而，其设计本质上是针对单模态图像分析优化的，限制了它们在神经学和肿瘤学等许多医学领域常见的多模态成像任务中的有效性。虽然监督模型在这种情况下表现良好，但它们无法利用未标记的数据集，并且在处理缺失模态方面存在困难，这是临床环境中常见的挑战。为了弥补这些差距，我们引入了MM-DINOv2，这是一种新颖且高效的框架，将预训练的视觉基础模型DINOv2适应用于多模态医学影像。我们的方法结合了多模态块嵌入，使视觉基础模型能够有效处理多模态影像数据。为解决缺失模态问题，我们采用全模态掩码，这鼓励模型学习稳健的跨模态关系。此外，我们利用半监督学习来利用大型未标记数据集，提高医学预测的准确性和可靠性。在应用于基于多序列脑部MRI的胶质瘤亚型分类时，我们的方法在外部测试集上获得了0.6的马修斯相关系数(MCC)，比最先进的监督方法高出11.1%。我们的工作为多模态医学影像任务建立了可扩展且稳健的解决方案，利用了在自然图像上预训练的强大视觉基础模型，同时解决了缺失数据和有限标注等现实临床挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models like DINOv2 demonstrate remarkable potential inmedical imaging despite their origin in natural image domains. However, theirdesign inherently works best for uni-modal image analysis, limiting theireffectiveness for multi-modal imaging tasks that are common in many medicalfields, such as neurology and oncology. While supervised models perform well inthis setting, they fail to leverage unlabeled datasets and struggle withmissing modalities, a frequent challenge in clinical settings. To bridge thesegaps, we introduce MM-DINOv2, a novel and efficient framework that adapts thepre-trained vision foundation model DINOv2 for multi-modal medical imaging. Ourapproach incorporates multi-modal patch embeddings, enabling vision foundationmodels to effectively process multi-modal imaging data. To address missingmodalities, we employ full-modality masking, which encourages the model tolearn robust cross-modality relationships. Furthermore, we leveragesemi-supervised learning to harness large unlabeled datasets, enhancing boththe accuracy and reliability of medical predictions. Applied to glioma subtypeclassification from multi-sequence brain MRI, our method achieves a MatthewsCorrelation Coefficient (MCC) of 0.6 on an external test set, surpassingstate-of-the-art supervised approaches by +11.1%. Our work establishes ascalable and robust solution for multi-modal medical imaging tasks, leveragingpowerful vision foundation models pre-trained on natural images whileaddressing real-world clinical challenges such as missing data and limitedannotations.</description>
      <author>example@mail.com (Daniel Scholz, Ayhan Can Erdur, Viktoria Ehm, Anke Meyer-Baese, Jan C. Peeken, Daniel Rueckert, Benedikt Wiestler)</author>
      <guid isPermaLink="false">2509.06617v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking EfficientTAM on FMO datasets</title>
      <link>http://arxiv.org/abs/2509.06536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文解决了快速和小型物体跟踪的挑战，通过引入JSON元数据文件和扩展数据集描述，并展示了EfficientTAM模型在这些数据集上的良好性能。&lt;h4&gt;背景&lt;/h4&gt;快速和小型物体跟踪在计算机视觉领域仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;作者旨在引入与四个开源的快速移动物体(FMOs)图像序列相关的JSON元数据文件，并扩展这些数据集的描述。&lt;h4&gt;方法&lt;/h4&gt;引入一个JSON元数据文件与四个开源的快速移动物体图像序列相关联；扩展FMOs数据集的描述，添加以JSON格式(称为FMOX)的额外真实信息，包括物体大小信息；使用FMOX文件测试最近提出的基础跟踪模型EfficientTAM。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientTAM模型在FMOX数据集上的性能与专门为这些FMO数据集设计的管道相比表现良好，使用轨迹交并比(TIoU)分数进行评估。&lt;h4&gt;结论&lt;/h4&gt;代码和JSON文件已开源共享，使FMOX能够被其他处理FMO数据集的机器学习管道访问和使用，促进了该领域的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;快速且小型物体的跟踪在计算机视觉中仍然是一个挑战。在本文中，我们首先介绍了与四个开源的快速移动物体(FMOs)图像序列相关的JSON元数据文件。此外，我们以JSON格式(称为FMOX)扩展了FMOs数据集的描述，添加了包括物体大小信息的额外真实信息。最后，我们使用FMOX文件测试了最近提出的基础跟踪模型(称为EfficientTAM)，显示其性能与专门为这些FMO数据集设计的管道相比表现良好。我们在FMOX上使用轨迹交并比(TIoU)分数提供了这些最先进技术的比较。代码和JSON已开源共享，使FMOX能够被其他旨在处理FMO数据集的机器学习管道访问和使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and tiny object tracking remains a challenge in computer vision and inthis paper we first introduce a JSON metadata file associated with four opensource datasets of Fast Moving Objects (FMOs) image sequences. In addition, weextend the description of the FMOs datasets with additional ground truthinformation in JSON format (called FMOX) with object size information. Finallywe use our FMOX file to test a recently proposed foundational model fortracking (called EfficientTAM) showing that its performance compares well withthe pipelines originally taylored for these FMO datasets. Our comparison ofthese state-of-the-art techniques on FMOX is provided with TrajectoryIntersection of Union (TIoU) scores. The code and JSON is shared open sourceallowing FMOX to be accessible and usable for other machine learning pipelinesaiming to process FMO datasets.</description>
      <author>example@mail.com (Senem Aktas, Charles Markham, John McDonald, Rozenn Dahyot)</author>
      <guid isPermaLink="false">2509.06536v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>QualityFM: a Multimodal Physiological Signal Foundation Model with Self-Distillation for Signal Quality Challenges in Critically Ill Patients</title>
      <link>http://arxiv.org/abs/2509.06516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为QualityFM的新型多模态基础模型，用于评估和处理光电容积脉搏波(PPG)和心电图(ECG)信号的质量问题，解决了现有方法在泛化能力、数据依赖性和跨任务迁移性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;光电容积脉搏波(PPG)和心电图(ECG)信号常在重症监护室和手术室中记录，但这些信号经常出现质量差、不完整和不一致的情况，可能导致误报或诊断不准确。现有方法存在泛化能力有限、依赖大量标记数据、跨任务迁移性差等问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够获取生理信号质量通用理解的新型多模态基础模型，克服现有方法的局限性，提高信号质量评估的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建双轨架构处理不同质量的成对生理信号；2) 采用自蒸馏策略，让高质量信号编码器指导低质量信号编码器训练；3) 在基于Transformer的模型中集成窗口稀疏注意力机制处理长序列信号；4) 设计复合损失函数结合直接蒸馏损失和间接重建损失；5) 在大规模数据集上预训练三个参数数量不同的模型(9.6M至319M)。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的三个模型在三个临床任务上表现出有效性和实用价值，包括：室性心动过速检测的误报减少、心房颤动的准确识别以及从PPG和ECG信号精确估算动脉血压。&lt;h4&gt;结论&lt;/h4&gt;QualityFM模型通过大规模预训练和创新的架构设计，有效解决了生理信号质量评估的挑战，为临床监护提供了更可靠的信号质量评估工具，有助于减少误报和提高诊断准确性。&lt;h4&gt;翻译&lt;/h4&gt;光电容积脉搏波(PPG)和心电图(ECG)通常在重症监护室(ICU)和手术室(OR)中记录。然而，信号质量差、不完整和不一致的高发生率，可能导致误报或诊断不准确。迄今为止探索的方法存在泛化能力有限、依赖大量标记数据和跨任务迁移性差等问题。为了克服这些挑战，我们引入了QualityFM，这是一种用于这些生理信号的新型多模态基础模型，旨在获取对信号质量的通用理解。我们的模型在一个包含超过2100万个30秒波形和179,757小时数据的大规模数据集上进行预训练。我们的方法涉及一个双轨架构，处理不同质量的成对生理信号，利用自蒸馏策略，其中高质量信号编码器用于指导低质量信号编码器的训练。为了有效处理长序列信号并捕获基本局部准周期性模式，我们在基于Transformer的模型中集成了窗口稀疏注意力机制。此外，结合编码器输出的直接蒸馏损失和基于功率谱及相位谱的间接重建损失的复合损失函数，确保了信号频域特征的保留。我们预训练了三个参数数量不同的模型(9.6M至319M)，并通过在三个不同的临床任务上的迁移学习证明了它们的有效性和实用价值：室性心动过速检测的误报、心房颤动的识别以及从PPG和ECG信号估算动脉血压(ABP)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoplethysmogram (PPG) and electrocardiogram (ECG) are commonly recorded inintesive care unit (ICU) and operating room (OR). However, the high incidenceof poor, incomplete, and inconsistent signal quality, can lead to false alarmsor diagnostic inaccuracies. The methods explored so far suffer from limitedgeneralizability, reliance on extensive labeled data, and poor cross-tasktransferability. To overcome these challenges, we introduce QualityFM, a novelmultimodal foundation model for these physiological signals, designed toacquire a general-purpose understanding of signal quality. Our model ispre-trained on an large-scale dataset comprising over 21 million 30-secondwaveforms and 179,757 hours of data. Our approach involves a dual-trackarchitecture that processes paired physiological signals of differing quality,leveraging a self-distillation strategy where an encoder for high-qualitysignals is used to guide the training of an encoder for low-quality signals. Toefficiently handle long sequential signals and capture essential localquasi-periodic patterns, we integrate a windowed sparse attention mechanismwithin our Transformer-based model. Furthermore, a composite loss function,which combines direct distillation loss on encoder outputs with indirectreconstruction loss based on power and phase spectra, ensures the preservationof frequency-domain characteristics of the signals. We pre-train three modelswith varying parameter counts (9.6 M to 319 M) and demonstrate their efficacyand practical value through transfer learning on three distinct clinical tasks:false alarm of ventricular tachycardia detection, the identification of atrialfibrillation and the estimation of arterial blood pressure (ABP) from PPG andECG signals.</description>
      <author>example@mail.com (Zongheng Guo, Tao Chen, Manuela Ferrario)</author>
      <guid isPermaLink="false">2509.06516v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Does DINOv3 Set a New Medical Vision Standard?</title>
      <link>http://arxiv.org/abs/2509.06467v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了DINOv3视觉基础模型在医学视觉任务上的表现，发现尽管仅用自然图像训练，它仍能成为医学任务的强大基线，甚至在某些任务上优于医学特定模型，但在高度专业化的医学领域表现有限。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉基础模型在自然图像上的预训练已改变了计算机视觉领域，但这些前沿模型在专业领域（如医学影像）的效能转移仍是一个开放性问题。&lt;h4&gt;目的&lt;/h4&gt;调查DINOv3（一种先进的自监督视觉Transformer）是否可以直接作为医学视觉任务的统一编码器，无需领域特定的预训练。&lt;h4&gt;方法&lt;/h4&gt;在多种医学成像模态的2D/3D分类和分割任务上对DINOv3进行基准测试，并通过改变模型大小和输入图像分辨率来分析其可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;DINOv3在医学视觉任务上表现出色，建立了强大的新基线，在某些任务上甚至超过医学特定模型；但在需要深度领域专业化的场景（如病理图像、电子显微镜和PET扫描）中性能下降；在医学领域不始终遵循缩放定律，性能不会随模型大小或分辨率增加而可靠提高。&lt;h4&gt;结论&lt;/h4&gt;DINOv3可作为医学视觉任务的强大基线模型，其视觉特征可作为复杂医学任务的稳健先验，为未来研究（如3D重建中的多视图一致性）开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉基础模型的出现，这些模型在各种自然图像上进行了预训练，标志着计算机视觉的范式转变。然而，前沿视觉基础模型的效能如何转移到专业领域（如医学影像）仍然是一个悬而未决的问题。本报告研究了DINOv3（一种在密集预测任务中具有强大能力的最先进自监督视觉Transformer）是否可以直接作为医学视觉任务的强大统一编码器，无需领域特定的预训练。为了回答这个问题，我们在常见的医学视觉任务上对DINOv3进行了基准测试，包括在多种医学成像模态上的2D/3D分类和分割。我们通过改变模型大小和输入图像分辨率，系统地分析了其可扩展性。我们的研究结果显示，DINOv3表现出令人印象深刻的性能，并建立了一个强大的新基线。值得注意的是，尽管仅在自然图像上训练，它在几个任务上甚至可以超越医学特定的基础模型，如BiomedCLIP和CT-Net。然而，我们确定了明显的局限性：在需要深度领域专业化的场景中，如全载病理图像(WSIs)、电子显微镜(EM)和正电子发射断层扫描(PET)，模型的特征会退化。此外，我们观察到DINOv3在医学领域并不始终遵循缩放定律；性能不会随着更大模型或更精细的特征分辨率而可靠地增加，在不同任务上表现出多样的缩放行为。最终，我们的工作将DINOv3确立为一个强大的基线模型，其强大的视觉特征可以作为多种复杂医学任务的稳健先验。这为未来开辟了有希望的方向，例如利用其特征在3D重建中强制多视图一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of large-scale vision foundation models, pre-trained on diversenatural images, has marked a paradigm shift in computer vision. However, howthe frontier vision foundation models' efficacies transfer to specializeddomains remains such as medical imaging remains an open question. This reportinvestigates whether DINOv3, a state-of-the-art self-supervised visiontransformer (ViT) that features strong capability in dense prediction tasks,can directly serve as a powerful, unified encoder for medical vision taskswithout domain-specific pre-training. To answer this, we benchmark DINOv3across common medical vision tasks, including 2D/3D classification andsegmentation on a wide range of medical imaging modalities. We systematicallyanalyze its scalability by varying model sizes and input image resolutions. Ourfindings reveal that DINOv3 shows impressive performance and establishes aformidable new baseline. Remarkably, it can even outperform medical-specificfoundation models like BiomedCLIP and CT-Net on several tasks, despite beingtrained solely on natural images. However, we identify clear limitations: Themodel's features degrade in scenarios requiring deep domain specialization,such as in Whole-Slide Pathological Images (WSIs), Electron Microscopy (EM),and Positron Emission Tomography (PET). Furthermore, we observe that DINOv3does not consistently obey scaling law in the medical domain; performance doesnot reliably increase with larger models or finer feature resolutions, showingdiverse scaling behaviors across tasks. Ultimately, our work establishes DINOv3as a strong baseline, whose powerful visual features can serve as a robustprior for multiple complex medical tasks. This opens promising futuredirections, such as leveraging its features to enforce multiview consistency in3D reconstruction.</description>
      <author>example@mail.com (Che Liu, Yinda Chen, Haoyuan Shi, Jinpeng Lu, Bailiang Jian, Jiazhen Pan, Linghan Cai, Jiayi Wang, Yundi Zhang, Jun Li, Cosmin I. Bercea, Cheng Ouyang, Chen Chen, Zhiwei Xiong, Benedikt Wiestler, Christian Wachinger, Daniel Rueckert, Wenjia Bai, Rossella Arcucci)</author>
      <guid isPermaLink="false">2509.06467v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Text-Trained LLMs Can Zero-Shot Extrapolate PDE Dynamics</title>
      <link>http://arxiv.org/abs/2509.06322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究展示了文本训练的基础模型能够在不微调或自然语言提示的情况下，从离散的偏微分方程解中准确推断时空动态，并揭示了预测质量与上下文长度和输出长度之间的可预测关系。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型已在各种任务中表现出上下文学习能力，包括零样本时间序列预测。&lt;h4&gt;目的&lt;/h4&gt;探究文本训练的基础模型是否可以在无需额外训练的情况下，从偏微分方程的离散解中准确推断时空动态。&lt;h4&gt;方法&lt;/h4&gt;分析标记级输出分布，研究大型语言模型如何内部处理偏微分方程解以实现准确预测。&lt;h4&gt;主要发现&lt;/h4&gt;预测准确性随时间上下文长度增加而提高，但在更精细的空间离散化时会下降；多步预测中误差随时间范围代数增长，类似于经典有限差分解算子的全局误差累积；这些趋势符合上下文神经缩放定律。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型通过一致的上下文学习进展处理偏微分方程解：从语法模式模仿开始，过渡到探索性高熵阶段，最终以自信的、基于数字的预测结束。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已在一系列任务中表现出上下文学习能力，包括零样本时间序列预测。我们证明，文本训练的基础模型可以在不微调或自然语言提示的情况下，从离散的偏微分方程解中准确推断时空动态。预测准确性随时间上下文长度增加而提高，但在更精细的空间离散化时会下降。在多步展开中，模型递归预测多个时间步的未来空间状态，误差随时间范围代数增长，类似于经典有限差分解算子中的全局误差累积。我们将这些趋势解释为上下文神经缩放定律，其中预测质量随上下文长度和输出长度可预测地变化。为了更好地理解大型语言模型如何能够内部处理偏微分方程解以准确展开预测，我们分析了标记级输出分布，并发现了一致的上下文学习进展：从语法模式模仿开始，过渡到探索性高熵阶段，最终以自信的、基于数字的预测结束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated emergent in-context learning(ICL) capabilities across a range of tasks, including zero-shot time-seriesforecasting. We show that text-trained foundation models can accuratelyextrapolate spatiotemporal dynamics from discretized partial differentialequation (PDE) solutions without fine-tuning or natural language prompting.Predictive accuracy improves with longer temporal contexts but degrades atfiner spatial discretizations. In multi-step rollouts, where the modelrecursively predicts future spatial states over multiple time steps, errorsgrow algebraically with the time horizon, reminiscent of global erroraccumulation in classical finite-difference solvers. We interpret these trendsas in-context neural scaling laws, where prediction quality varies predictablywith both context length and output length. To better understand how LLMs areable to internally process PDE solutions so as to accurately roll them out, weanalyze token-level output distributions and uncover a consistent ICLprogression: beginning with syntactic pattern imitation, transitioning throughan exploratory high-entropy phase, and culminating in confident, numericallygrounded predictions.</description>
      <author>example@mail.com (Jiajun Bao, Nicolas Boullé, Toni J. B. Liu, Raphaël Sarfati, Christopher J. Earls)</author>
      <guid isPermaLink="false">2509.06322v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>WindFM: An Open-Source Foundation Model for Zero-Shot Wind Power Forecasting</title>
      <link>http://arxiv.org/abs/2509.06311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WindFM是一个专门为概率性风力发电预测设计的轻量级生成式基础模型，采用离散化和生成框架，能够实现高质量的零样本预测性能，无需微调即可优于专业模型和更大的基础模型。&lt;h4&gt;背景&lt;/h4&gt;高质量的风力发电预测对现代电网运行至关重要，但现有的数据驱动方法存在局限性，要么无法泛化到其他地点，要么难以融入能源领域的特定领域数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门用于概率性风力发电预测的轻量级生成式基础模型，解决现有方法的泛化和领域适应性不足的问题。&lt;h4&gt;方法&lt;/h4&gt;采用离散化和生成框架，使用专门的时间序列标记器将连续多元观测值转换为离散的分层标记，然后通过自回归预训练学习风力发电动力学的通用表示。&lt;h4&gt;主要发现&lt;/h4&gt;810万参数的WindFM模型在确定性和概率性任务上实现了最先进的零样本性能，无需微调即可优于专业模型和更大的基础模型，并且在分布外数据上表现出强适应性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;WindFM通过大规模数据预训练学习到的表示具有良好的泛化能力和可转移性，为风力发电预测提供了一个有效的基础模型解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高质量的风力发电预测对现代电网运行至关重要。然而，现有的数据驱动范式要么训练特定地点的模型无法泛化到其他位置，要么依赖通用时间序列基础模型的微调，难以融入能源领域的特定领域数据。本文介绍了WindFM，一个专门为概率性风力发电预测设计的轻量级生成式基础模型。WindFM采用离散化和生成框架，首先使用专门的时间序列标记器将连续多元观测值转换为离散的分层标记，然后通过自回归预训练学习风力发电动力学的通用表示。使用包含约1500亿个时间步和超过126,000个地点的WIND Toolkit数据集，WindFM开发了对大气条件与发电输出之间复杂相互作用的基础理解。大量实验表明，我们紧凑的810万参数模型在确定性和概率性任务上都实现了最先进的零样本性能，优于专业模型和更大的基础模型，无需任何微调。特别是，WindFM在不同大陆的分布外数据上表现出强适应性，展示了其学习表示的鲁棒性和可转移性。我们的预训练模型已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-quality wind power forecasting is crucial for the operation of modernpower grids. However, prevailing data-driven paradigms either train asite-specific model which cannot generalize to other locations or rely onfine-tuning of general-purpose time series foundation models which aredifficult to incorporate domain-specific data in the energy sector. This paperintroduces WindFM, a lightweight and generative Foundation Model designedspecifically for probabilistic wind power forecasting. WindFM employs adiscretize-and-generate framework. A specialized time-series tokenizer firstconverts continuous multivariate observations into discrete, hierarchicaltokens. Subsequently, a decoder-only Transformer learns a universalrepresentation of wind generation dynamics by autoregressively pre-training onthese token sequences. Using the comprehensive WIND Toolkit dataset comprisingapproximately 150 billion time steps from more than 126,000 sites, WindFMdevelops a foundational understanding of the complex interplay betweenatmospheric conditions and power output. Extensive experiments demonstrate thatour compact 8.1M parameter model achieves state-of-the-art zero-shotperformance on both deterministic and probabilistic tasks, outperformingspecialized models and larger foundation models without any fine-tuning. Inparticular, WindFM exhibits strong adaptiveness under out-of-distribution datafrom a different continent, demonstrating the robustness and transferability ofits learned representations. Our pre-trained model is publicly available athttps://github.com/shiyu-coder/WindFM.</description>
      <author>example@mail.com (Hang Fan, Yu Shi, Zongliang Fu, Shuo Chen, Wei Wei, Wei Xu, Jian Li)</author>
      <guid isPermaLink="false">2509.06311v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>O$^3$Afford: One-Shot 3D Object-to-Object Affordance Grounding for Generalizable Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2509.06233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025. Project website:  https://o3afford.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的一次性3D物体到物体可供性学习方法(O³Afford)，用于机器人操作中的物体可供性 grounding，解决了现有研究主要关注单物体可供性而忽略物体对交互关系的问题。&lt;h4&gt;背景&lt;/h4&gt;物体可供性(grounding object affordance)对机器人操作至关重要，它建立了感知与行动之间的关键联系。然而，现有研究主要关注单物体可供性预测，忽略了现实世界中大多数交互涉及物体对之间的关系。&lt;h4&gt;目的&lt;/h4&gt;解决在有限数据约束下的物体到物体可供性 grounding 挑战，提出一种新颖的一次性3D物体到物体可供性学习方法，用于机器人操作。&lt;h4&gt;方法&lt;/h4&gt;受近期2D视觉基础模型的小样本学习进展启发，结合视觉基础模型的语义特征和点云表示以实现几何理解，构建一次性学习管道。此外，将3D可供性表示与大型语言模型(LLMs)集成，显著增强LLMs在生成特定任务约束函数时理解物体交互的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在3D物体到物体可供性 grounding 和机器人操作实验中，提出的O³Afford方法在准确性和泛化能力方面显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;O³Afford方法在解决物体到物体可供性 grounding 问题上表现优异，在有限数据条件下能够有效学习并推广到新场景。&lt;h4&gt;翻译&lt;/h4&gt;物体可供性 grounding 是机器人操作的基础，因为它在相互作用的物体之间建立了感知与行动的关键联系。然而，先前的工作主要集中预测单物体可供性，忽略了大多数现实世界交互涉及物体对之间关系的事实。在这项工作中，我们解决了在有限数据约束下物体到物体可供性 grounding 的挑战。受近期2D视觉基础模型小样本学习进展的启发，我们提出了一种新颖的一次性3D物体到物体可供性学习方法，用于机器人操作。视觉基础模型的语义特征结合点云表示以实现几何理解，使我们的一次性学习管道能够有效推广到新物体和类别。我们进一步将3D可供性表示与大型语言模型(LLMs)集成用于机器人操作，显著增强了LLMs在生成特定任务约束函数时理解和推理物体交互的能力。我们在3D物体到物体可供性 grounding 和机器人操作上的实验表明，我们的O³Afford在准确性和泛化能力方面显著优于现有基线。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是3D物体间可操作性的单样本学习，即如何让机器人仅通过一个示例就能理解两个物体之间的交互关系并泛化到新物体上。这个问题在现实中非常重要，因为日常生活中的许多任务（如倒水、切割、悬挂等）都涉及两个物体之间的交互，而收集大量标注数据非常困难。现有方法主要关注单个物体的可操作性，忽略了物体间的关系，且泛化能力有限，难以适应新物体和新场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现实世界交互大多涉及物体对而非单个物体的特点，认识到收集大量物体间交互数据的困难，因此提出单样本学习思路。他们借鉴了视觉基础模型（如DINOv2）在少样本学习中的成功经验，结合3D点云表示来获取几何信息，弥补2D图像的局限性。同时，他们利用大型语言模型来生成任务特定的约束函数。方法设计上，他们将语义特征投影到3D点云上，设计了双向注意力变换器解码器处理物体间交互，并将可操作性表示与LLM集成用于机器人操作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 将视觉基础模型的语义特征与3D点云的几何信息结合，同时理解物体的功能属性和空间结构；2) 通过双向注意力机制，让模型同时考虑源物体和目标物体的上下文信息；3) 利用单样本学习实现泛化到新物体的能力；4) 将可操作性表示与大型语言模型结合，使机器人能根据自然语言指令执行复杂任务。整体流程包括：1) 构建语义点云，从多视角RGB-D中提取DINOv2特征并投影到3D点云；2) 使用双向注意力变换器解码器预测可操作性图；3) 将可操作性图与LLM结合，LLM生成约束函数，通过优化算法找到最佳物体姿态实现机器人操作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现单样本3D物体间可操作性学习，解决数据稀缺问题；2) 融合语义特征与几何信息，提高泛化能力；3) 设计双向注意力机制，理解物体间交互关系；4) 集成LLM自动生成任务特定约束函数。相比之前工作，不同之处在于：从关注单物体转向物体对交互，从2D图像空间转向3D点云空间，从需要大量数据转向仅需单样本，从手动设计约束转向LLM自动生成约束。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出O3Afford方法，通过融合视觉基础模型语义特征与3D点云几何信息，并集成大型语言模型，实现了仅通过一个示例就能理解两个物体间的交互关系并泛化到新物体的能力，显著提升了机器人在复杂操作任务中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grounding object affordance is fundamental to robotic manipulation as itestablishes the critical link between perception and action among interactingobjects. However, prior works predominantly focus on predicting single-objectaffordance, overlooking the fact that most real-world interactions involverelationships between pairs of objects. In this work, we address the challengeof object-to-object affordance grounding under limited data contraints.Inspired by recent advances in few-shot learning with 2D vision foundationmodels, we propose a novel one-shot 3D object-to-object affordance learningapproach for robotic manipulation. Semantic features from vision foundationmodels combined with point cloud representation for geometric understandingenable our one-shot learning pipeline to generalize effectively to novelobjects and categories. We further integrate our 3D affordance representationwith large language models (LLMs) for robotics manipulation, significantlyenhancing LLMs' capability to comprehend and reason about object interactionswhen generating task-specific constraint functions. Our experiments on 3Dobject-to-object affordance grounding and robotic manipulation demonstrate thatour O$^3$Afford significantly outperforms existing baselines in terms of bothaccuracy and generalization capability.</description>
      <author>example@mail.com (Tongxuan Tian, Xuhui Kang, Yen-Ling Kuo)</author>
      <guid isPermaLink="false">2509.06233v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RetinaGuard: Obfuscating Retinal Age in Fundus Images for Biometric Privacy Preserving</title>
      <link>http://arxiv.org/abs/2509.06142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为RetinaGuard的隐私增强框架，用于保护眼底图像中的生物特征数据，特别是视网膜年龄信息，同时保持图像质量和诊断效用。&lt;h4&gt;背景&lt;/h4&gt;AI与医学图像的结合可以从图像中提取隐性的生物标志物用于健康评估。视网膜年龄是从眼底图像预测出的生物标志物，可以预测全身疾病风险、行为模式、衰老轨迹和死亡率。然而，这种技术带来了隐私风险，未经授权使用眼底图像可能导致生物信息泄露。&lt;h4&gt;目的&lt;/h4&gt;提出与医学图像相关的生物特征隐私这一新研究问题，并开发一种隐私增强框架来保护眼底图像中的敏感生物特征数据。&lt;h4&gt;方法&lt;/h4&gt;提出RetinaGuard框架，采用特征级别生成对抗性掩蔽机制来模糊视网膜年龄，同时保持图像视觉质量和疾病诊断效用。框架还利用多对一知识蒸馏策略，结合视网膜基础模型和多样化的代理年龄编码器，以实现对黑盒年龄预测模型的通用防御。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估证实，RetinaGuard成功模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。&lt;h4&gt;结论&lt;/h4&gt;RetinaGuard是一种有效的隐私保护框架，可以灵活扩展到其他医学图像衍生的生物标志物。&lt;h4&gt;翻译&lt;/h4&gt;将AI与医学图像相结合可以从图像中提取隐性的图像衍生生物标志物，用于精确的健康评估。最近，从眼底图像预测出的视网膜年龄是一种已证实可以预测全身疾病风险、行为模式、衰老轨迹甚至死亡率的生物标志物。然而，推断此类敏感生物特征数据的能力带来了重大的隐私风险，未经授权使用眼底图像可能导致生物信息泄露，侵犯个人隐私。为此，我们提出了与医学图像相关的生物特征隐私这一新研究问题，并提出了RetinaGuard，一种新颖的隐私增强框架，该框架采用特征级别生成对抗性掩蔽机制来模糊视网膜年龄，同时保持图像视觉质量和疾病诊断效用。该框架进一步利用了一种新颖的多对一知识蒸馏策略，结合了视网膜基础模型和多样化的代理年龄编码器，以实现对黑盒年龄预测模型的通用防御。全面评估证实，RetinaGuard成功模糊了视网膜年龄预测，同时对图像质量和病理特征表示的影响最小。RetinaGuard还可以灵活扩展到其他医学图像衍生的生物标志物。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of AI with medical images enables the extraction of implicitimage-derived biomarkers for a precise health assessment. Recently, retinalage, a biomarker predicted from fundus images, is a proven predictor ofsystemic disease risks, behavioral patterns, aging trajectory and evenmortality. However, the capability to infer such sensitive biometric dataraises significant privacy risks, where unauthorized use of fundus images couldlead to bioinformation leakage, breaching individual privacy. In response, weformulate a new research problem of biometric privacy associated with medicalimages and propose RetinaGuard, a novel privacy-enhancing framework thatemploys a feature-level generative adversarial masking mechanism to obscureretinal age while preserving image visual quality and disease diagnosticutility. The framework further utilizes a novel multiple-to-one knowledgedistillation strategy incorporating a retinal foundation model and diversesurrogate age encoders to enable a universal defense against black-box ageprediction models. Comprehensive evaluations confirm that RetinaGuardsuccessfully obfuscates retinal age prediction with minimal impact on imagequality and pathological feature representation. RetinaGuard is also flexiblefor extension to other medical image derived biomarkers. RetinaGuard is alsoflexible for extension to other medical image biomarkers.</description>
      <author>example@mail.com (Zhengquan Luo, Chi Liu, Dongfu Xiao, Zhen Yu, Yueye Wang, Tianqing Zhu)</author>
      <guid isPermaLink="false">2509.06142v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MedSeqFT: Sequential Fine-tuning Foundation Models for 3D Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2509.06096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MedSeqFT是一种顺序微调框架，通过最大数据相似性选择和基于LoRA的知识蒸馏方案，解决了基础模型在医学图像分割任务中微调的局限性，显著提升了性能和可迁移性。&lt;h4&gt;背景&lt;/h4&gt;基础模型在医学图像分析中显示出巨大潜力，特别是在分割任务方面。然而，现有的微调策略存在局限性：并行微调隔离任务无法利用共享知识，而多任务微调需要同时访问所有数据集，难以集成增量任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够逐步将预训练模型适应到新任务同时改进其表示能力的微调框架，解决现有微调策略的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出MedSeqFT，包含两个核心组件：(1)最大数据相似性选择，识别最能代表原始预训练分布的下游样本以保留通用知识；(2)知识和泛化保留微调，一种基于LoRA的知识蒸馏方案，平衡特定任务适应与预训练知识保留。&lt;h4&gt;主要发现&lt;/h4&gt;在两个多任务数据集上覆盖10个3D分割任务的实验表明，MedSeqFT始终优于最先进的微调策略，带来显著的性能提升（平均Dice提高3.0%）。在两个未见过的任务上验证了MedSeqFT提高了可迁移性，特别是肿瘤分割。损失景观和参数变化的视觉分析进一步证明了MedSeqFT的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;顺序微调是一种有效的、保留知识的范式，用于将基础模型适应不断发展的临床任务。代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已成为推进医学图像分析的一种有前景的范式，特别是在下游应用通常按顺序出现的分割任务中。然而，现有的微调策略仍然有限：并行微调隔离任务且无法利用共享知识，而多任务微调需要同时访问所有数据集且难以集成增量任务。为解决这些挑战，我们提出了MedSeqFT，一种顺序微调框架，能够逐步将预训练模型适应到新任务，同时改进其表示能力。MedSeqFT引入了两个核心组件：(1)最大数据相似性选择，识别最能代表原始预训练分布的下游样本以保留通用知识；(2)知识和泛化保留微调，一种基于LoRA的知识蒸馏方案，平衡特定任务适应与预训练知识保留。在覆盖10个3D分割任务的两个多任务数据集上的广泛实验表明，MedSeqFT始终优于最先进的微调策略，带来显著的性能提升（例如，平均Dice提高3.0%）。此外，在两个未见过的任务（COVID-19-20和肾脏）上的评估验证了MedSeqFT提高了可迁移性，特别是对于肿瘤分割。损失景观和参数变化的视觉分析进一步突显了MedSeqFT的鲁棒性。这些结果确立了顺序微调作为一种有效的、保留知识的范式，用于将基础模型适应不断发展的临床任务。代码将公开发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决医学图像分割中基础模型的微调策略问题。现有方法中，并行微调无法利用任务间的共享知识，而多任务微调需要同时访问所有数据集，难以灵活增量整合新任务。这个问题在医学图像分析中非常重要，因为临床实践中新任务往往随时间推移逐渐出现，而非一次性全部出现；同时医学图像分割需要专家标注，耗时耗力，基础模型通过自监督学习可减少对标注数据的依赖。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有微调策略的局限性，提出更符合临床工作流的顺序微调范式。他们识别出顺序微调中的关键挑战是避免灾难性遗忘。设计方法时借鉴了自监督学习(SSL)的预训练-微调范式、知识蒸馏(KD)技术、LoRA参数高效微调方法，以及nnU-Net等医学图像分析领域的现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出MedSeqFT框架，采用顺序微调策略逐步适应预训练模型到多个3D医学图像分割任务，并通过两个关键组件平衡任务特定适应与保留预训练知识：1)最大数据相似性(MDS)选择，保留最能代表原始预训练分布的样本；2)知识与泛化保留微调(K&amp;G RFT)，基于LoRA的知识蒸馏方案。整体流程包括：初始化第一个任务，使用MDS选择代表性样本，对后续任务顺序微调，迭代更新模型和缓冲区。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出MedSeqFT顺序微调框架；2)引入MDS策略保留泛化知识；3)设计K&amp;G RFT策略平衡适应与知识保留；4)实验证明在多任务上性能优越且增强可迁移性。相比之前工作不同：能利用任务间共享知识而非独立处理；不需要同时访问所有数据集，可灵活增量整合新任务；通过MDS和K&amp;G RFT显式减轻灾难性遗忘；整体性能优于传统FFT和PEFT方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MedSeqFT通过创新的顺序微调框架解决了医学图像分割中任务间知识共享与灵活适应新任务的矛盾，在保持模型泛化能力的同时显著提升了分割性能和可迁移性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have become a promising paradigm for advancing medicalimage analysis, particularly for segmentation tasks where downstreamapplications often emerge sequentially. Existing fine-tuning strategies,however, remain limited: parallel fine-tuning isolates tasks and fails toexploit shared knowledge, while multi-task fine-tuning requires simultaneousaccess to all datasets and struggles with incremental task integration. Toaddress these challenges, we propose MedSeqFT, a sequential fine-tuningframework that progressively adapts pre-trained models to new tasks whilerefining their representational capacity. MedSeqFT introduces two corecomponents: (1) Maximum Data Similarity (MDS) selection, which identifiesdownstream samples most representative of the original pre-trainingdistribution to preserve general knowledge, and (2) Knowledge andGeneralization Retention Fine-Tuning (K&amp;G RFT), a LoRA-based knowledgedistillation scheme that balances task-specific adaptation with the retentionof pre-trained knowledge. Extensive experiments on two multi-task datasetscovering ten 3D segmentation tasks demonstrate that MedSeqFT consistentlyoutperforms state-of-the-art fine-tuning strategies, yielding substantialperformance gains (e.g., an average Dice improvement of 3.0%). Furthermore,evaluations on two unseen tasks (COVID-19-20 and Kidney) verify that MedSeqFTenhances transferability, particularly for tumor segmentation. Visual analysesof loss landscapes and parameter variations further highlight the robustness ofMedSeqFT. These results establish sequential fine-tuning as an effective,knowledge-retentive paradigm for adapting foundation models to evolvingclinical tasks. Code will be released.</description>
      <author>example@mail.com (Yiwen Ye, Yicheng Wu, Xiangde Luo, He Zhang, Ziyang Chen, Ting Dang, Yanning Zhang, Yong Xia)</author>
      <guid isPermaLink="false">2509.06096v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Unified Interaction Foundational Model (UIFM) for Predicting Complex User and System Behavior</title>
      <link>http://arxiv.org/abs/2509.06025v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了统一交互基础模型（UIFM），这是一种为真正行为理解而设计的基础模型，通过复合标记化原则将多属性事件视为语义连贯的单位，从而能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型是为自然语言设计的，但无法理解电信、电子商务和金融等领域中复杂的、不断发展的序列事件的整体性质。&lt;h4&gt;目的&lt;/h4&gt;构建能够理解和预测复杂、不断发展的序列事件的人工智能系统。&lt;h4&gt;方法&lt;/h4&gt;引入统一交互基础模型（UIFM），采用复合标记化原则，将每个多属性事件视为单个语义连贯的单位。&lt;h4&gt;主要发现&lt;/h4&gt;UIFM架构不仅更准确，而且代表了创建更适应性和更智能的预测系统的根本性进展。&lt;h4&gt;结论&lt;/h4&gt;UIFM能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流，是创建更适应性和智能预测系统的重要一步。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的一个核心目标是构建能够理解和预测复杂、不断发展的序列事件的系统。然而，当前为自然语言设计的基础模型无法理解电信、电子商务和金融等领域中结构化交互的整体性质。通过将事件序列化为文本，这些模型将它们分解为语义碎片化的部分，失去了关键上下文。在这项工作中，我们介绍了统一交互基础模型（UIFM），这是一个为真正的行为理解而设计的基础模型。其核心是复合标记化原则，其中每个多属性事件被视为单个语义连贯的单位。这使得UIFM能够学习用户行为的基本'语法'，感知整个交互而不是不相关的数据点流。我们证明，这种架构不仅更准确，而且代表了创建更适应性和更智能的预测系统的根本性进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central goal of artificial intelligence is to build systems that canunderstand and predict complex, evolving sequences of events. However, currentfoundation models, designed for natural language, fail to grasp the holisticnature of structured interactions found in domains like telecommunications,e-commerce and finance. By serializing events into text, they disassemble theminto semantically fragmented parts, losing critical context. In this work, weintroduce the Unified Interaction Foundation Model (UIFM), a foundation modelengineered for genuine behavioral understanding. At its core is the principleof composite tokenization, where each multi-attribute event is treated as asingle, semantically coherent unit. This allows UIFM to learn the underlying"grammar" of user behavior, perceiving entire interactions rather than adisconnected stream of data points. We demonstrate that this architecture isnot just more accurate, but represents a fundamental step towards creating moreadaptable and intelligent predictive systems.</description>
      <author>example@mail.com (Vignesh Ethiraj, Subhash Talluri)</author>
      <guid isPermaLink="false">2509.06025v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Imagining Alternatives: Towards High-Resolution 3D Counterfactual Medical Image Generation via Language Guidance</title>
      <link>http://arxiv.org/abs/2509.05978v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种基于语言提示的框架，能够生成高分辨率3D反事实医学图像，解决了3D领域缺乏预训练基础模型的问题，首次将语言引导的原生3D扩散模型应用于神经影像数据。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在2D图像生成方面表现出色，但这依赖于大量可用的预训练基础模型。然而，3D领域缺乏类似的预训练基础模型，严重限制了该领域的进展。视觉语言模型在仅基于自然语言描述生成高分辨率3D反事实医学图像方面的潜力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;解决3D医学影像生成的这一差距，开发一个能够根据自由形式语言提示生成高分辨率3D反事实医学图像的框架，用于临床和研究应用，如个性化反事实解释、疾病进展场景模拟和增强医疗培训。&lt;h4&gt;方法&lt;/h4&gt;采用最先进的3D扩散模型，结合Simple Diffusion的增强功能，并加入增强的条件设置以提高文本对齐和图像质量。这是首次将语言引导的原生3D扩散模型应用于神经影像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。&lt;h4&gt;主要发现&lt;/h4&gt;在两个不同的神经MRI数据集上，该框架成功模拟了多发性硬化症的不同反事实病变负荷和阿尔茨海默病的认知状态，生成了高质量图像，同时在合成的医学图像中保持了主体保真度。&lt;h4&gt;结论&lt;/h4&gt;研究结果为3D医学影像中的提示驱动疾病进展分析奠定了基础，为个性化医疗和医学研究提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在各种条件下生成2D图像方面表现出令人印象深刻的能力；然而，这些模型在2D方面的出色表现很大程度上依赖于大量可用的预训练基础模型。关键的是，3D领域没有类似的预训练基础模型，这严重限制了该领域的进展。因此，视觉语言模型仅根据自然语言描述生成高分辨率3D反事实医学图像的潜力仍未被探索。解决这一差距将 enable强大的临床和研究应用，如个性化反事实解释、疾病进展场景模拟，以及通过以真实细节可视化假设性医疗条件来增强医疗培训。我们的工作通过引入一个能够根据自由形式语言提示生成高分辨率3D反事实医学图像的框架，朝着解决这一挑战迈出了有意义的一步。我们采用了最先进的3D扩散模型，结合了Simple Diffusion的增强功能，并加入了增强的条件设置以提高文本对齐和图像质量。据我们所知，这是首次将语言引导的原生3D扩散模型专门应用于神经影像数据，其中忠实的三维建模对于表示大脑的三维结构至关重要。通过对两个不同的神经MRI数据集的实验，我们的框架成功模拟了多发性硬化症中的不同反事实病变负荷和阿尔茨海默病中的认知状态，生成了高质量图像，同时在合成的医学图像中保持了主体保真度。我们的结果为3D医学影像中的提示驱动疾病进展分析奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过自然语言描述生成高分辨率的3D反事实医学图像问题。这个问题在现实中很重要，因为它可以实现个性化反事实解释、模拟疾病进展场景、增强医学培训，让医生能够可视化假设的医疗状况，从而提高临床决策能力和医学教育效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作：采用了Simple Diffusion的三种架构增强（加深U-Net瓶颈、应用目标dropout、多尺度交叉注意力层）；整合了MAISI 3D潜在扩散框架并引入Rectified Flow噪声调度；使用BiomedCLIP获取医学语义嵌入。作者设计思路是构建一个能根据语言提示生成高分辨率3D反事实医学图像的框架，直接在体素空间操作以支持精细编辑。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用语言引导的扩散模型生成3D医学图像的反事实版本，保持患者解剖结构不变，同时根据文本提示改变临床状态。流程包括：1)创建与MRI特征对应的文本提示；2)构建语言引导的3D扩散模型，采用架构增强和交叉注意力层；3)从相同噪声源使用不同文本提示生成反事实图像，共享患者身份但反映不同临床状态。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：首次实现语言引导的3D反事实医学图像生成；首次将原生3D扩散模型应用于神经成像；结合Simple Diffusion增强和BiomedCLIP语义嵌入；引入Rectified Flow噪声调度提高效率；提出使用无分类器引导平衡文本对齐和解剖保真度。不同之处：现有方法主要生成新合成扫描而非修改现有图像；受限于简单变量而非自由文本；主要关注物体表面而非内部体积结构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新的语言引导3D扩散模型框架，能够根据自然语言描述生成高保真度的反事实医学图像，为疾病进展模拟、个性化临床解释和医学教育提供了新工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have demonstrated impressive capabilities ingenerating 2D images under various conditions; however the impressiveperformance of these models in 2D is largely enabled by extensive, readilyavailable pretrained foundation models. Critically, comparable pretrainedfoundation models do not exist for 3D, significantly limiting progress in thisdomain. As a result, the potential of vision-language models to producehigh-resolution 3D counterfactual medical images conditioned solely on naturallanguage descriptions remains completely unexplored. Addressing this gap wouldenable powerful clinical and research applications, such as personalizedcounterfactual explanations, simulation of disease progression scenarios, andenhanced medical training by visualizing hypothetical medical conditions inrealistic detail. Our work takes a meaningful step toward addressing thischallenge by introducing a framework capable of generating high-resolution 3Dcounterfactual medical images of synthesized patients guided by free-formlanguage prompts. We adapt state-of-the-art 3D diffusion models withenhancements from Simple Diffusion and incorporate augmented conditioning toimprove text alignment and image quality. To our knowledge, this represents thefirst demonstration of a language-guided native-3D diffusion model appliedspecifically to neurological imaging data, where faithful three-dimensionalmodeling is essential to represent the brain's three-dimensional structure.Through results on two distinct neurological MRI datasets, our frameworksuccessfully simulates varying counterfactual lesion loads in MultipleSclerosis (MS), and cognitive states in Alzheimer's disease, generatinghigh-quality images while preserving subject fidelity in syntheticallygenerated medical images. Our results lay the groundwork for prompt-drivendisease progression analysis within 3D medical imaging.</description>
      <author>example@mail.com (Mohamed Mohamed, Brennan Nichyporuk, Douglas L. Arnold, Tal Arbel)</author>
      <guid isPermaLink="false">2509.05978v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Compression Beyond Pixels: Semantic Compression with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at IEEE 35th Workshop on Machine  Learning for Signal Processing (MLSP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的语义压缩方法，利用CLIP模型的能力，专注于保留图像的语义信息而非像素级细节。通过压缩CLIP特征嵌入而非原始图像，实现了极低的比特率，同时保持跨任务的语义完整性。&lt;h4&gt;背景&lt;/h4&gt;现有基于深度学习的有损图像压缩方法通过端到端训练和先进架构实现了具有竞争力的率失真性能。然而，新兴应用越来越重视语义保持而非像素级重建，并且需要在不同数据分布和下游任务中保持稳健性能。&lt;h4&gt;目的&lt;/h4&gt;开发先进的语义压缩范式，以满足新兴应用对语义保持和跨数据分布稳健性的需求。&lt;h4&gt;方法&lt;/h4&gt;基于对比语言-图像预训练(CLIP)模型提出新型语义压缩方法。不压缩图像用于重建，而是将CLIP特征嵌入压缩到最少比特数，同时保留不同任务中的语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在基准数据集中保持语义完整性，实现了每像素约2-3×10^(-3)比特的平均比特率，不到主流图像压缩方法在可比性能下所需比特率的5%。即使在极端压缩下，该方法也在不同数据分布和下游任务中表现出零样本稳健性。&lt;h4&gt;结论&lt;/h4&gt;基于CLIP模型的语义压缩方法能够在极低比特率下有效保留图像语义信息，相比传统像素级重建方法在语义保持方面具有显著优势，且在不同数据分布中表现出强大的零样本稳健性。&lt;h4&gt;翻译&lt;/h4&gt;最近基于深度学习的有损图像压缩方法通过广泛的端到端训练和先进架构实现了具有竞争力的率失真性能。然而，新兴应用越来越优先考虑语义保持而非像素级重建，并要求在不同数据分布和下游任务中保持稳健性能。这些挑战需要先进的语义压缩范式。受多模态基础模型的零样本和表征能力启发，我们提出了一种基于对比语言-图像预训练(CLIP)模型的新型语义压缩方法。我们不是为重建而压缩图像，而是将CLIP特征嵌入压缩到最少的比特数，同时保留不同任务中的语义信息。实验表明，我们的方法在基准数据集中保持语义完整性，实现了每像素约2-3×10^(-3)比特的平均比特率。这不到主流图像压缩方法在可比性能下所需比特率的5%。值得注意的是，即使在极端压缩下，所提出的方法也在不同数据分布和下游任务中表现出零样本稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep learning-based methods for lossy image compression achievecompetitive rate-distortion performance through extensive end-to-end trainingand advanced architectures. However, emerging applications increasinglyprioritize semantic preservation over pixel-level reconstruction and demandrobust performance across diverse data distributions and downstream tasks.These challenges call for advanced semantic compression paradigms. Motivated bythe zero-shot and representational capabilities of multimodal foundationmodels, we propose a novel semantic compression method based on the contrastivelanguage-image pretraining (CLIP) model. Rather than compressing images forreconstruction, we propose compressing the CLIP feature embeddings into minimalbits while preserving semantic information across different tasks. Experimentsshow that our method maintains semantic integrity across benchmark datasets,achieving an average bit rate of approximately 2-3* 10(-3) bits per pixel. Thisis less than 5% of the bitrate required by mainstream image compressionapproaches for comparable performance. Remarkably, even under extremecompression, the proposed approach exhibits zero-shot robustness across diversedata distributions and downstream tasks.</description>
      <author>example@mail.com (Ruiqi Shen, Haotian Wu, Wenjing Zhang, Jiangjing Hu, Deniz Gunduz)</author>
      <guid isPermaLink="false">2509.05925v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Challenges in Deep Learning-Based Small Organ Segmentation: A Benchmarking Perspective for Medical Research with Limited Datasets</title>
      <link>http://arxiv.org/abs/2509.05892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多种先进的深度学习分割模型在心血管组织病理图像中颈动脉结构分割任务上的性能，发现模型性能对数据分割高度敏感，且性能差异更多由统计噪声而非真正的算法优势驱动。&lt;h4&gt;背景&lt;/h4&gt;准确分割心血管组织病理图像中的颈动脉结构对推进心血管疾病研究和诊断至关重要。然而，该领域深度学习模型的发展受到已标注的心血管组织病理数据稀缺性的限制。&lt;h4&gt;目的&lt;/h4&gt;系统评估最先进的深度学习分割模型在有限的心血管组织病理数据集上的表现，并探讨标准基准测试实践在低数据临床环境中的局限性。&lt;h4&gt;方法&lt;/h4&gt;研究评估了多种深度学习分割模型，包括卷积神经网络（U-Net、DeepLabV3+）、视觉Transformer（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+UNet）。研究采用了贝叶斯搜索的广泛超参数优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;尽管进行了超参数优化，研究发现模型性能对数据分割高度敏感，微小的性能差异更多是由统计噪声而非真正的算法优势驱动的。&lt;h4&gt;结论&lt;/h4&gt;这种不稳定性暴露了标准基准测试实践在低数据临床环境中的局限性，并挑战了性能排名反映有意义临床效用的假设。&lt;h4&gt;翻译&lt;/h4&gt;准确分割心血管组织病理图像中的颈动脉结构对推进心血管疾病研究和诊断至关重要。然而，该领域深度学习模型的发展受到已标注的心血管组织病理数据稀缺性的限制。本研究评估了最先进的深度学习分割模型，包括卷积神经网络（U-Net、DeepLabV3+）、视觉Transformer（SegFormer）以及最近的基础模型（SAM、MedSAM、MedSAM+UNet）在有限的心血管组织病理图像数据集上的表现。尽管采用了贝叶斯搜索的广泛超参数优化策略，我们的研究结果显示模型性能对数据分割高度敏感，微小的性能差异更多是由统计噪声而非真正的算法优势驱动的。这种不稳定性暴露了标准基准测试实践在低数据临床环境中的局限性，并挑战了性能排名反映有意义临床效用的假设。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of carotid artery structures in histopathologicalimages is vital for advancing cardiovascular disease research and diagnosis.However, deep learning model development in this domain is constrained by thescarcity of annotated cardiovascular histopathological data. This studyinvestigates a systematic evaluation of state-of-the-art deep learningsegmentation models, including convolutional neural networks (U-Net,DeepLabV3+), a Vision Transformer (SegFormer), and recent foundation models(SAM, MedSAM, MedSAM+UNet), on a limited dataset of cardiovascular histologyimages. Despite employing an extensive hyperparameter optimization strategywith Bayesian search, our findings reveal that model performance is highlysensitive to data splits, with minor differences driven more by statisticalnoise than by true algorithmic superiority. This instability exposes thelimitations of standard benchmarking practices in low-data clinical settingsand challenges the assumption that performance rankings reflect meaningfulclinical utility.</description>
      <author>example@mail.com (Phongsakon Mark Konrad, Andrei-Alexandru Popa, Yaser Sabzehmeidani, Liang Zhong, Elisa A. Liehn, Serkan Ayvaz)</author>
      <guid isPermaLink="false">2509.05892v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>time2time: Causal Intervention in Hidden States to Simulate Rare Events in Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明大型时间序列Transformer模型确实内部化了语义概念，而不仅仅是拟合曲线。通过激活移植技术，可以操纵模型内部表示来模拟不同市场状况，为理解和控制模型预测提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的基础模型在预测常规模式方面表现出色，但有两个关键问题：这些模型是否内部化了市场状况等语义概念，还是仅仅拟合曲线？它们的内部表示是否可以被利用来模拟罕见的高风险事件如市场崩盘？&lt;h4&gt;目的&lt;/h4&gt;研究Transformer模型是否真正理解了时间序列数据中的语义概念，以及是否可以通过模型的内部表示来模拟罕见的高风险事件。&lt;h4&gt;方法&lt;/h4&gt;引入了激活移植方法，这是一种因果干预技术，通过在前向传播过程中将一个事件（如历史崩盘）的统计强加到另一个事件（如平静期）上来操纵隐藏状态，从而确定性地引导预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;注入崩盘语义会诱导下跌预测，注入平静语义会抑制崩盘并恢复稳定性；模型编码了事件严重性的分级概念，潜在向量范数与系统性冲击的幅度直接相关；这种可引导的、语义基础的表示是大型时间序列Transformer的稳健特性。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持存在一个潜在的概念空间来控制模型预测，将可解释性从事后归因转向直接因果干预，并支持战略压力测试的语义化'假设分析'。&lt;h4&gt;翻译&lt;/h4&gt;虽然基于Transformer的基础模型在预测常规模式方面表现出色，但仍有两个问题：它们是否内部化了市场状况等语义概念，还是仅仅拟合曲线？它们的内部表示是否可以被利用来模拟罕见的高风险事件如市场崩盘？为了研究这一点，我们引入了激活移植，这是一种因果干预，通过在前向传播过程中将一个事件（如历史崩盘）的统计强加到另一个事件（如平静期）上来操纵隐藏状态。这个过程确定性地引导预测：注入崩盘语义会诱导下跌预测，而注入平静语义会抑制崩盘并恢复稳定性。除了二元控制外，我们发现模型编码了事件严重性的分级概念，其中潜在向量范数与系统性冲击的幅度直接相关。在两种架构不同的时间序列Transformer模型（Toto和Chronos）上得到验证，我们的结果表明可引导的、语义基础的表示是大型时间序列Transformer的稳健特性。我们的研究为控制模型预测的潜在概念空间提供了证据，将可解释性从事后归因转向直接因果干预，并支持战略压力测试的语义化'假设分析'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transformer-based foundation models excel at forecasting routinepatterns, two questions remain: do they internalize semantic concepts such asmarket regimes, or merely fit curves? And can their internal representations beleveraged to simulate rare, high-stakes events such as market crashes? Toinvestigate this, we introduce activation transplantation, a causalintervention that manipulates hidden states by imposing the statistical momentsof one event (e.g., a historical crash) onto another (e.g., a calm period)during the forward pass. This procedure deterministically steers forecasts:injecting crash semantics induces downturn predictions, while injecting calmsemantics suppresses crashes and restores stability. Beyond binary control, wefind that models encode a graded notion of event severity, with the latentvector norm directly correlating with the magnitude of systemic shocks.Validated across two architecturally distinct TSFMs, Toto (decoder only) andChronos (encoder-decoder), our results demonstrate that steerable, semanticallygrounded representations are a robust property of large time seriestransformers. Our findings provide evidence for a latent concept space thatgoverns model predictions, shifting interpretability from post-hoc attributionto direct causal intervention, and enabling semantic "what-if" analysis forstrategic stress-testing.</description>
      <author>example@mail.com (Debdeep Sanyal, Aaryan Nagpal, Dhruv Kumar, Murari Mandal, Saurabh Deshpande)</author>
      <guid isPermaLink="false">2509.05801v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>TPCpp-10M: Simulated proton-proton collisions in a Time Projection Chamber for AI Foundation Models</title>
      <link>http://arxiv.org/abs/2509.05792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个包含1000万次模拟质子-质子碰撞的大型开放数据集，支持基础模型的自我监督训练，包含70,000个标记示例用于三个下游任务的评估，旨在促进核物理和粒子物理领域的基础模型发展。&lt;h4&gt;背景&lt;/h4&gt;科学基础模型在核物理和粒子物理领域有很大潜力，但进展受限于缺乏大规模开放数据集、标准化评估任务和指标。此外，处理粒子物理学数据所需的专业知识和软件阻碍了与机器学习社区的跨学科合作。&lt;h4&gt;目的&lt;/h4&gt;引入一个大型开放数据集支持基础模型的自我监督训练，提供易于使用的数据集格式，并包含标记示例以评估基础模型的适应性。&lt;h4&gt;方法&lt;/h4&gt;使用Pythia蒙特卡洛事件生成器在200 GeV中心质心能量下模拟质子-质子碰撞数据，并用Geant4处理以包含真实的探测器条件和信号模拟，数据针对sPHENIX时间投影室设计，使用sPHENIX软件栈实现完整的模拟和重建链。&lt;h4&gt;主要发现&lt;/h4&gt;创建了包含1000万次模拟质子-质子碰撞的大型开放数据集，提供NumPy格式的数据集，包含70,000个标记示例涵盖三个下游任务：轨迹寻找、粒子识别和噪声标记。&lt;h4&gt;结论&lt;/h4&gt;该数据集资源为跨学科研究建立了共同基础，使机器学习科学家和物理学家能够探索扩展行为、评估可转移性，加速核物理和高能物理领域基础模型的进展。&lt;h4&gt;翻译&lt;/h4&gt;科学基础模型在推进核物理和粒子物理方面具有巨大潜力，通过提高分析精度和加速发现。然而，这一领域的进展通常受到缺乏公开可用的大规模数据集以及标准化评估任务和指标的限制。此外，处理粒子物理数据通常需要的专业知识和软件对与更广泛的机器学习社区进行跨学科合作构成了重大障碍。这项工作引入了一个包含1000万次模拟质子-质子碰撞的大型开放数据集，旨在支持基础模型的自我监督训练。为便于使用，数据集以通用的NumPy格式提供。此外，它包含70,000个标记示例，涵盖三个明确定义的下游任务：轨迹寻找、粒子识别和噪声标记，以实现对基础模型适应性的系统评估。模拟数据使用Pythia蒙特卡洛事件生成器在质心能量为200 GeV的条件下生成，并使用Geant4进行处理，以包含在布鲁克黑文国家实验室的相对论重离子对撞机上的sPHENIX时间投影室中的真实探测条件和信号模拟。该数据集资源为跨学科研究建立了共同基础，使机器学习科学家和物理学家都能够探索扩展行为、评估可转移性，并加速核物理和高能物理领域基础模型的进展。完整的模拟和重建链可以使用sPHENIX软件栈重现。所有数据和代码位置在数据可访问性部分提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scientific foundation models hold great promise for advancing nuclear andparticle physics by improving analysis precision and accelerating discovery.Yet, progress in this field is often limited by the lack of openly availablelarge scale datasets, as well as standardized evaluation tasks and metrics.Furthermore, the specialized knowledge and software typically required toprocess particle physics data pose significant barriers to interdisciplinarycollaboration with the broader machine learning community.  This work introduces a large, openly accessible dataset of 10 millionsimulated proton-proton collisions, designed to support self-supervisedtraining of foundation models. To facilitate ease of use, the dataset isprovided in a common NumPy format. In addition, it includes 70,000 labeledexamples spanning three well defined downstream tasks: track finding, particleidentification, and noise tagging, to enable systematic evaluation of thefoundation model's adaptability.  The simulated data are generated using the Pythia Monte Carlo event generatorat a center of mass energy of sqrt(s) = 200 GeV and processed with Geant4 toinclude realistic detector conditions and signal emulation in the sPHENIX TimeProjection Chamber at the Relativistic Heavy Ion Collider, located atBrookhaven National Laboratory.  This dataset resource establishes a common ground for interdisciplinaryresearch, enabling machine learning scientists and physicists alike to explorescaling behaviors, assess transferability, and accelerate progress towardfoundation models in nuclear and high energy physics. The complete simulationand reconstruction chain is reproducible with the sPHENIX software stack. Alldata and code locations are provided under Data Accessibility.</description>
      <author>example@mail.com (Shuhang Li, Yi Huang, David Park, Xihaier Luo, Haiwang Yu, Yeonju Go, Christopher Pinkenburg, Yuewei Lin, Shinjae Yoo, Joseph Osborn, Christof Roland, Jin Huang, Yihui Ren)</author>
      <guid isPermaLink="false">2509.05792v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian</title>
      <link>http://arxiv.org/abs/2509.05668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Michael Hoffmann and Jophin John contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Llama-GENBA-10B是一个三语种基础模型，旨在解决大型语言模型中的英语中心主义偏见。该模型基于Llama 3.1-8B构建并扩展至100亿参数，在平衡资源分配的同时防止英语主导，针对德语NLP社区，同时促进巴伐利亚语作为低资源语言的发展。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型普遍存在英语中心主义偏见，大多数模型主要在英语数据上进行训练，导致对其他语言的支持不足。特别是对于像巴伐利亚语这样的低资源语言，缺乏有效的模型支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个平衡英语、德语和巴伐利亚语的三语种基础模型，解决英语中心主义偏见，促进低资源语言巴伐利亚语的发展，并为德语NLP社区提供有力工具。&lt;h4&gt;方法&lt;/h4&gt;基于Llama 3.1-8B构建并扩展至100亿参数；在164B个标记上进行持续预训练（820亿英语，820亿德语和800万巴伐利亚语）；解决四个主要挑战：策划多语言语料库、创建统一分词器、优化架构和语言比例超参数、建立标准化三语种评估套件；使用Cerebras CS-2进行训练展示大规模多语言预训练效率。&lt;h4&gt;主要发现&lt;/h4&gt;Llama-GENBA-10B实现了强大的跨语言性能；微调后的变体在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言同类中的最佳模型；在英语上表现优于EuroLLM，在德语上与EuroLLM结果相当；在Cerebras CS-2上的训练展示了大规模多语言预训练的高效性。&lt;h4&gt;结论&lt;/h4&gt;Llama-GENBA-10B为整合低资源语言的包容性基础模型提供了蓝图，证明了在平衡多语言资源分配的同时防止英语主导的可行性，为促进语言多样性和包容性AI发展做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Llama-GENBA-10B，一个三语种基础模型，旨在解决大型语言模型中的英语中心主义偏见。该模型基于Llama 3.1-8B构建并扩展至100亿参数，在164B个标记上持续预训练（820亿英语，820亿德语和800万巴伐利亚语），平衡资源分配同时防止英语主导。针对德语NLP社区，该模型还促进巴伐利亚语作为低资源语言的发展。开发过程中解决了四个挑战：(1) 尽管巴伐利亚语资源稀缺，仍策划多语言语料库，(2) 为英语、德语和巴伐利亚语创建统一的分词器，(3) 优化架构和语言比例超参数以实现跨语言迁移，(4) 通过将德语基准测试翻译成巴伐利亚语，建立首个标准化三语种评估套件。评估显示，Llama-GENBA-10B实现了强大的跨语言性能，微调后的变体在巴伐利亚语上超越了Apertus-8B-2509和gemma-2-9b，成为该语言同类中的最佳模型，同时在英语上表现优于EuroLLM，在德语上与EuroLLM结果相当。在Cerebras CS-2上的训练展示了大规模多语言预训练的高效性，并记录了能源使用情况，为整合低资源语言的包容性基础模型提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Llama-GENBA-10B, a trilingual foundation model addressingEnglish-centric bias in large language models. Built on Llama 3.1-8B and scaledto 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens(82B English, 82B German, and 80M Bavarian), balancing resources whilepreventing English dominance. Targeted at the German NLP community, the modelalso promotes Bavarian as a low-resource language. Development tackled fourchallenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2)creating a unified tokenizer for English, German, and Bavarian, (3) optimizingarchitecture and language-ratio hyperparameters for cross-lingual transfer, and(4) establishing the first standardized trilingual evaluation suite bytranslating German benchmarks into Bavarian. Evaluations show thatLlama-GENBA-10B achieves strong cross-lingual performance, with the fine-tunedvariant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishingitself as the best model in its class for this language, while alsooutperforming EuroLLM in English and matching its results in German. Trainingon the Cerebras CS-2 demonstrated efficient large-scale multilingualpretraining with documented energy use, offering a blueprint for inclusivefoundation models that integrate low-resource languages.</description>
      <author>example@mail.com (Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, Nicolay J. Hammer)</author>
      <guid isPermaLink="false">2509.05668v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>ProfilingAgent: Profiling-Guided Agentic Reasoning for Adaptive Model Optimization</title>
      <link>http://arxiv.org/abs/2509.05584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures, 5 tables, 1 algorithm&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ProfilingAgent的智能体方法，利用大型语言模型和性能分析技术来自动化模型压缩过程，解决了基础模型在资源受限平台上部署的计算和内存瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;基础模型面临日益严重的计算和内存瓶颈，限制了在资源有限平台上的部署。现有的压缩技术如剪枝和量化大多依赖均匀启发式方法，忽略了架构和运行时异质性。性能分析工具虽能揭示各层性能数据，但很少集成到自动化流程中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于性能分析、智能体引导的自动化压缩方法，利用大型语言模型实现结构化剪枝和训练后动态量化，为不同架构模型定制优化策略。&lt;h4&gt;方法&lt;/h4&gt;提出ProfilingAgent，一个模块化多智能体系统，该系统结合静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，为特定架构设计优化策略，针对瓶颈进行逐层决策，而非使用均匀启发式方法。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet-1K、CIFAR-10和CIFAR-100数据集上使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16的实验表明：剪枝保持了竞争性或改进的精度（ImageNet-1K上约1%下降，ViT-B/16在较小数据集上+2%增益）；量化实现了高达74%的内存节省，精度损失&lt;0.5%；量化还带来了高达1.74倍的推理加速。与GPT-4o和GPT-4-Turbo的比较研究强调了LLM推理质量对迭代剪枝的重要性。&lt;h4&gt;结论&lt;/h4&gt;研究结果确立了智能体系统作为性能分析引导模型优化的可扩展解决方案，能够有效解决模型在资源受限环境中的部署问题。&lt;h4&gt;翻译&lt;/h4&gt;基础模型面临日益增长的计算和内存瓶颈，阻碍了在资源有限平台上的部署。虽然剪枝和量化等压缩技术被广泛使用，但大多数依赖均匀启发式方法，忽略了架构和运行时异质性。性能分析工具可以揭示各层的延迟、内存和计算成本，但很少集成到自动化流程中。我们提出ProfilingAgent，这是一种基于性能分析、智能体引导的方法，使用大型语言模型通过结构化剪枝和训练后动态量化来自动化压缩。我们的模块化多智能体系统基于静态指标（MACs，参数计数）和动态信号（延迟，内存）进行推理，以设计特定架构的策略。与启发式基线不同，ProfilingAgent针对瓶颈进行逐层决策。在ImageNet-1K、CIFAR-10和CIFAR-100上使用ResNet-101、ViT-B/16、Swin-B和DeiT-B/16进行的实验表明，剪枝保持了竞争性或改进的精度（ImageNet-1上约1%的下降，ViT-B/16在较小数据集上+2%的增益），而量化实现了高达74%的内存节省，精度损失&lt;0.5%。我们的量化还带来了高达1.74倍的持续推理加速。与GPT-4o和GPT-4-Turbo的比较研究突显了LLM推理质量对迭代剪枝的重要性。这些结果确立了智能体系统作为性能分析引导模型优化的可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models face growing compute and memory bottlenecks, hinderingdeployment on resource-limited platforms. While compression techniques such aspruning and quantization are widely used, most rely on uniform heuristics thatignore architectural and runtime heterogeneity. Profiling tools exposeper-layer latency, memory, and compute cost, yet are rarely integrated intoautomated pipelines. We propose ProfilingAgent, a profiling-guided, agenticapproach that uses large language models (LLMs) to automate compression viastructured pruning and post-training dynamic quantization. Our modularmulti-agent system reasons over static metrics (MACs, parameter counts) anddynamic signals (latency, memory) to design architecture-specific strategies.Unlike heuristic baselines, ProfilingAgent tailors layer-wise decisions tobottlenecks. Experiments on ImageNet-1K, CIFAR-10, and CIFAR-100 withResNet-101, ViT-B/16, Swin-B, and DeiT-B/16 show pruning maintains competitiveor improved accuracy (about 1% drop on ImageNet-1K, +2% gains for ViT-B/16 onsmaller datasets), while quantization achieves up to 74% memory savings with&lt;0.5% accuracy loss. Our quantization also yields consistent inference speedupsof up to 1.74 times faster. Comparative studies with GPT-4o and GPT-4-Turbohighlight the importance of LLM reasoning quality for iterative pruning. Theseresults establish agentic systems as scalable solutions for profiling-guidedmodel optimization.</description>
      <author>example@mail.com (Sadegh Jafari, Aishwarya Sarkar, Mohiuddin Bilwal, Ali Jannesari)</author>
      <guid isPermaLink="false">2509.05584v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations</title>
      <link>http://arxiv.org/abs/2509.05186v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出概率框架揭示ICON隐式执行贝叶斯推理，并扩展其到生成式设置(GenICON)，实现解预测中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;ICON是一类基于基础模型新架构的算子学习方法，在多样化的初始和边界条件数据集（与常微分方程和偏微分方程的解配对）上进行训练。&lt;h4&gt;目的&lt;/h4&gt;揭示ICON作为隐式贝叶斯推理方法的本质，并扩展其到生成式设置以实现不确定性量化。&lt;h4&gt;方法&lt;/h4&gt;通过随机微分方程形式提供概率框架，将ICON扩展为生成式形式(GenICON)，使其能够从解算子的后验预测分布中进行采样。&lt;h4&gt;主要发现&lt;/h4&gt;1. ICON隐式执行贝叶斯推理，计算基于上下文的解算子后验预测分布均值；2. 随机微分方程形式为描述ICON任务提供了概率框架；3. GenICON能够捕获解算子的潜在不确定性。&lt;h4&gt;结论&lt;/h4&gt;ICON的概率视角为算子学习中的解预测提供了有原则的不确定性量化基础，提高了其可靠性并扩展了应用范围。&lt;h4&gt;翻译&lt;/h4&gt;上下文算子网络(ICON)是一类基于基础模型新架构的算子学习方法。在多样化的初始和边界条件数据集（与常微分方程和偏微分方程的解配对）上进行训练，ICON学习将给定微分方程的条件-解示例对映射到其解算子的近似。在此，我们提出一个概率框架，揭示ICON隐式执行贝叶斯推理，计算基于提供上下文（即条件-解示例对）的解算子后验预测分布的均值。随机微分方程的形式为描述ICON完成的任务提供了概率框架，也为理解其他多算子学习方法提供了基础。这种概率视角为ICON扩展到生成式设置提供了基础，可以从解算子的后验预测分布中进行采样。ICON的生成式形式(GenICON)捕获了解算子的潜在不确定性，使算子学习中的解预测能够进行有原则的不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context operator networks (ICON) are a class of operator learning methodsbased on the novel architectures of foundation models. Trained on a diverse setof datasets of initial and boundary conditions paired with correspondingsolutions to ordinary and partial differential equations (ODEs and PDEs), ICONlearns to map example condition-solution pairs of a given differential equationto an approximation of its solution operator. Here, we present a probabilisticframework that reveals ICON as implicitly performing Bayesian inference, whereit computes the mean of the posterior predictive distribution over solutionoperators conditioned on the provided context, i.e., example condition-solutionpairs. The formalism of random differential equations provides theprobabilistic framework for describing the tasks ICON accomplishes while alsoproviding a basis for understanding other multi-operator learning methods. Thisprobabilistic perspective provides a basis for extending ICON to\emph{generative} settings, where one can sample from the posterior predictivedistribution of solution operators. The generative formulation of ICON(GenICON) captures the underlying uncertainty in the solution operator, whichenables principled uncertainty quantification in the solution predictions inoperator learning.</description>
      <author>example@mail.com (Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2509.05186v2</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Video-Based MPAA Rating Prediction: An Attention-Driven Hybrid Architecture Using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.06826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于对比学习的视频分类方法，用于MPAA评级系统的自动分类，通过结合CNN、LSTM和注意力机制，实现了88%的准确率和0.8815的F1分数。&lt;h4&gt;背景&lt;/h4&gt;各平台视觉内容消费快速增长，需要自动化视频分类来满足MPAA分级系统(G, PG, PG-13, R)等年龄适宜性标准，而传统方法存在大量标记数据需求、泛化能力差和特征学习效率低等问题。&lt;h4&gt;目的&lt;/h4&gt;解决传统视频分类方法的局限性，提高视频分类的准确性和效率，实现对MPAA等级的自动分类。&lt;h4&gt;方法&lt;/h4&gt;采用对比学习提高区分能力和适应性，探索实例判别、上下文对比学习和多视图对比学习三种框架；使用混合架构，结合LRCN(CNN+LSTM)主干和Bahdanau注意力机制；评估了NT-Xent、NT-logistic和Margin Triplet等多种对比损失函数。&lt;h4&gt;主要发现&lt;/h4&gt;在上下文对比学习框架下实现了最先进的性能，准确率达到88%，F1分数为0.8815；结合CNN的空间特征、LSTM的时间建模和注意力机制的动态帧优先级选择，模型在细粒度边界区分方面表现出色；架构在各种对比损失函数下表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;模型已部署为Web应用程序，用于实时MPAA评级分类，为流媒体平台自动化内容合规性提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;各平台视觉内容消费的快速增长需要对MPAA评级系统(如G, PG, PG-13, R等级)的年龄适宜性标准进行自动化视频分类。传统方法难以满足大量标记数据需求、泛化能力差和特征学习效率低等挑战。为解决这些问题，我们采用对比学习来提高区分能力和适应性，探索了三种框架：实例判别、上下文对比学习和多视图对比学习。我们的混合架构集成了LRCN(CNN+LSTM)主干和Bahdanau注意力机制，在上下文对比学习框架下实现了最先进的性能，准确率达88%，F1分数为0.8815。通过结合CNN的空间特征、LSTM的时间建模和注意力机制的动态帧优先级选择，模型在细粒度边界区分方面表现出色，例如区分PG-13和R级内容。我们评估了模型在各种对比损失函数(包括NT-Xent、NT-logistic和Margin Triplet)上的性能，证明了我们提出架构的鲁棒性。为确保实际应用，该模型已部署为Web应用程序，用于实时MPAA评级分类，为流媒体平台自动化内容合规性提供了高效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of visual content consumption across platforms necessitatesautomated video classification for age-suitability standards like the MPAArating system (G, PG, PG-13, R). Traditional methods struggle with largelabeled data requirements, poor generalization, and inefficient featurelearning. To address these challenges, we employ contrastive learning forimproved discrimination and adaptability, exploring three frameworks: InstanceDiscrimination, Contextual Contrastive Learning, and Multi-View ContrastiveLearning. Our hybrid architecture integrates an LRCN (CNN+LSTM) backbone with aBahdanau attention mechanism, achieving state-of-the-art performance in theContextual Contrastive Learning framework, with 88% accuracy and an F1 score of0.8815. By combining CNNs for spatial features, LSTMs for temporal modeling,and attention mechanisms for dynamic frame prioritization, the model excels infine-grained borderline distinctions, such as differentiating PG-13 and R-ratedcontent. We evaluate the model's performance across various contrastive lossfunctions, including NT-Xent, NT-logistic, and Margin Triplet, demonstratingthe robustness of our proposed architecture. To ensure practical application,the model is deployed as a web application for real-time MPAA ratingclassification, offering an efficient solution for automated content complianceacross streaming platforms.</description>
      <author>example@mail.com (Dipta Neogi, Nourash Azmine Chowdhury, Muhammad Rafsan Kabir, Mohammad Ashrafuzzaman Khan)</author>
      <guid isPermaLink="false">2509.06826v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>BEAM: Brainwave Empathy Assessment Model for Early Childhood</title>
      <link>http://arxiv.org/abs/2509.06620v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为BEAM的新型深度学习框架，用于客观评估4-6岁儿童的共情能力，通过多视图EEG信号捕捉共情的认知和情感维度。&lt;h4&gt;背景&lt;/h4&gt;儿童共情能力对其社交和情感发展至关重要，但预测共情能力具有挑战性。传统方法依赖自我报告或观察者标记，易受偏见影响，无法客观捕捉共情形成过程。EEG虽提供了客观替代方案，但当前方法主要提取静态模式，忽略了时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架，即脑波共情评估模型（BEAM），用于预测4-6岁儿童的共情水平。&lt;h4&gt;方法&lt;/h4&gt;BEAM利用多视图EEG信号捕捉共情的认知和情感维度，包含三个关键组件：1) 基于LaBraM的编码器用于时空特征提取，2) 特征融合模块整合多视图信号的互补信息，3) 对比学习模块增强类间分离。&lt;h4&gt;主要发现&lt;/h4&gt;在CBCP数据集上验证，BEAM在多个指标上优于最先进的方法，展示了客观共情评估的潜力。&lt;h4&gt;结论&lt;/h4&gt;BEAM为儿童亲社会发展的早期干预提供了初步见解，有望实现客观共情评估。&lt;h4&gt;翻译&lt;/h4&gt;儿童共情能力对其社交和情感发展至关重要，但预测共情能力仍然具有挑战性。传统方法通常仅依赖自我报告或观察者标记，这些方法容易受到偏见影响，无法客观捕捉共情形成过程。EEG提供了客观替代方案；然而，当前方法主要提取静态模式，忽略了时间动态性。为克服这些局限，我们提出了一种新型深度学习框架——脑波共情评估模型（BEAM），用于预测4-6岁儿童的共情水平。BEAM利用多视图EEG信号捕捉共情的认知和情感维度。该框架包含三个关键组件：1) 基于LaBraM的编码器，用于有效的时空特征提取；2) 特征融合模块，用于整合多视图信号的互补信息；3) 对比学习模块，用于增强类间分离。在CBCP数据集上验证，BEAM在多个指标上优于最先进的方法，展示了其客观共情评估的潜力，并为儿童亲社会发展的早期干预提供了初步见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Empathy in young children is crucial for their social and emotionaldevelopment, yet predicting it remains challenging. Traditional methods oftenonly rely on self-reports or observer-based labeling, which are susceptible tobias and fail to objectively capture the process of empathy formation. EEGoffers an objective alternative; however, current approaches primarily extractstatic patterns, neglecting temporal dynamics. To overcome these limitations,we propose a novel deep learning framework, the Brainwave Empathy AssessmentModel (BEAM), to predict empathy levels in children aged 4-6 years. BEAMleverages multi-view EEG signals to capture both cognitive and emotionaldimensions of empathy. The framework comprises three key components: 1) aLaBraM-based encoder for effective spatio-temporal feature extraction, 2) afeature fusion module to integrate complementary information from multi-viewsignals, and 3) a contrastive learning module to enhance class separation.Validated on the CBCP dataset, BEAM outperforms state-of-the-art methods acrossmultiple metrics, demonstrating its potential for objective empathy assessmentand providing a preliminary insight into early interventions in children'sprosocial development.</description>
      <author>example@mail.com (Chen Xie, Gaofeng Wu, Kaidong Wang, Zihao Zhu, Xiaoshu Luo, Yan Liang, Feiyu Quan, Ruoxi Wu, Xianghui Huang, Han Zhang)</author>
      <guid isPermaLink="false">2509.06620v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Self-Supervised Network Intrusion Detection using Augmented Negative Pairs</title>
      <link>http://arxiv.org/abs/2509.06550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in: Proceedings of IEEE Conference on Cyber Security and  Resilience (CSR), 2025. Official version:  https://doi.org/10.1109/CSR64739.2025.11129979 Code:  https://github.com/jackwilkie/CLAN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CLAN的新型网络入侵检测方法，通过对比学习使用增强负样本对，在良性流量预训练后提高了分类精度和推理效率，在二元和多类分类任务中均优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;网络入侵检测是网络安全的关键挑战。监督式机器学习虽性能优异但依赖大型标记数据集，不切实际；异常检测仅用良性流量训练但误报率高；现有自监督方法通过学习良性流量表示改进性能但仍有限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型网络入侵检测方法，解决现有自监督和异常检测方法的局限性，提高检测精度并降低误报率。&lt;h4&gt;方法&lt;/h4&gt;提出CLAN（使用增强负样本对的对比学习）范式，将增强样本视为负视图（代表潜在恶意分布），其他良性样本作为正视图，通过对比学习学习良性流量的判别性表示。&lt;h4&gt;主要发现&lt;/h4&gt;在Lycos2017数据集上，CLAN在二元分类任务中优于现有自监督和异常检测技术；在有限标记数据集上微调后，多类分类性能也优于现有自监督模型。&lt;h4&gt;结论&lt;/h4&gt;CLAN方法通过创新的负样本处理方式，有效提升了网络入侵检测的性能和效率，是一种实用且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;网络入侵检测仍然是网络安全中的一个关键挑战。虽然监督式机器学习模型实现了最先进的性能，但它们对大型标记数据集的依赖使得它们在许多实际应用中不切实际。仅对良性流量进行训练以识别恶意活动的异常检测方法，误报率高，限制了其可用性。最近，自监督学习技术通过学习良性流量的判别性潜在表示，展示了改进的性能和更低的误报率。特别是，对比自监督模型通过最小化良性流量相似（正）视图之间的距离，同时最大化不同（负）视图之间的距离来实现这一点。现有方法通过数据增强生成正视图，并将其他样本视为负视图。相比之下，这项工作引入了使用增强负样本对的对比学习（CLAN），这是一种用于网络入侵检测的新范式，其中增强样本被视为负视图 - 代表潜在的恶意分布 - 而其他良性样本则作为正视图。这种方法在良性流量上进行预训练后，提高了分类精度和推理效率。在Lycos2017数据集上的实验评估表明，所提出的方法在二元分类任务中优于现有的自监督和异常检测技术。此外，当在有限的标记数据集上进行微调时，所提出的方法在多类分类任务中实现了比现有自监督模型更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CSR64739.2025.11129979&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network intrusion detection remains a critical challenge in cybersecurity.While supervised machine learning models achieve state-of-the-art performance,their reliance on large labelled datasets makes them impractical for manyreal-world applications. Anomaly detection methods, which train exclusively onbenign traffic to identify malicious activity, suffer from high false positiverates, limiting their usability. Recently, self-supervised learning techniqueshave demonstrated improved performance with lower false positive rates bylearning discriminative latent representations of benign traffic. Inparticular, contrastive self-supervised models achieve this by minimizing thedistance between similar (positive) views of benign traffic while maximizing itbetween dissimilar (negative) views. Existing approaches generate positiveviews through data augmentation and treat other samples as negative. Incontrast, this work introduces Contrastive Learning using Augmented Negativepairs (CLAN), a novel paradigm for network intrusion detection where augmentedsamples are treated as negative views - representing potentially maliciousdistributions - while other benign samples serve as positive views. Thisapproach enhances both classification accuracy and inference efficiency afterpretraining on benign traffic. Experimental evaluation on the Lycos2017 datasetdemonstrates that the proposed method surpasses existing self-supervised andanomaly detection techniques in a binary classification task. Furthermore, whenfine-tuned on a limited labelled dataset, the proposed approach achievessuperior multi-class classification performance compared to existingself-supervised models.</description>
      <author>example@mail.com (Jack Wilkie, Hanan Hindy, Christos Tachtatzis, Robert Atkinson)</author>
      <guid isPermaLink="false">2509.06550v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion</title>
      <link>http://arxiv.org/abs/2509.06531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by EMNLP Findings 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了SLiNT框架，通过将知识图谱结构信息注入到冻结的大型语言模型中，解决了LLM在知识图谱链接预测中结构信号利用不足的问题，实现了在不完整或零样本设置下的稳健预测。&lt;h4&gt;背景&lt;/h4&gt;知识图谱链接预测需要整合结构信息和语义上下文来推断缺失实体。大型语言模型虽有强大生成推理能力，但对结构信号利用有限，导致结构稀疏性和语义模糊性问题，尤其在不完整或少样本场景下更为突出。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用结构信息的框架，解决大型语言模型在知识图谱链接预测中的局限性，提高在不完整或零样本设置下的预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出SLiNT框架，将知识图谱衍生的结构上下文注入冻结的LLM主干中，使用LoRA进行轻量级适应。包含三个核心组件：结构引导的邻域增强(SGNE)检索伪邻居丰富稀疏实体；动态难对比学习(DHCL)通过插值难正负样本引入细粒度监督；梯度解耦双注入(GDDI)执行标记级别结构感知干预同时保留核心LLM参数。&lt;h4&gt;主要发现&lt;/h4&gt;在WN18RR和FB15k-237数据集上的实验表明，SLiNT与基于嵌入和基于生成的基线相比实现了优越或具有竞争力的性能，验证了结构感知表示学习对可扩展知识图谱补全的有效性。&lt;h4&gt;结论&lt;/h4&gt;SLiNT框架通过结构感知的学习方法，成功解决了大型语言模型在知识图谱链接预测中的局限性，提高了在不完整或零样本设置下的预测性能。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱中的链接预测需要整合结构信息和语义上下文来推断缺失的实体。虽然大型语言模型提供了强大的生成推理能力，但它们对结构信号的有限利用通常会导致结构稀疏性和语义模糊性，特别是在不完整或少样本设置下。为解决这些挑战，我们提出了SLiNT（结构感知语言模型，具有注入和对比训练），这是一个模块化框架，它将知识图谱衍生的结构上下文注入到冻结的LLM主干中，并使用基于LoRA的轻量级适应进行稳健的链接预测。具体而言，结构引导的邻域增强(SGNE)检索伪邻居来丰富稀疏实体并缓解缺失上下文；动态难对比学习(DHCL)通过插值难正样本和难负样本引入细粒度监督，以解决实体级别的模糊性；梯度解耦双注入(GDDI)在保留核心LLM参数的同时执行标记级别的结构感知干预。在WN18RR和FB15k-237上的实验表明，与基于嵌入和基于生成的基线相比，SLiNT实现了优越或具有竞争力的性能，证明了结构感知表示学习对可扩展知识图谱补全的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction in knowledge graphs requires integrating structuralinformation and semantic context to infer missing entities. While largelanguage models offer strong generative reasoning capabilities, their limitedexploitation of structural signals often results in structural sparsity andsemantic ambiguity, especially under incomplete or zero-shot settings. Toaddress these challenges, we propose SLiNT (Structure-aware Language model withInjection and coNtrastive Training), a modular framework that injectsknowledge-graph-derived structural context into a frozen LLM backbone withlightweight LoRA-based adaptation for robust link prediction. Specifically,Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors toenrich sparse entities and mitigate missing context; Dynamic Hard ContrastiveLearning (DHCL) introduces fine-grained supervision by interpolating hardpositives and negatives to resolve entity-level ambiguity; andGradient-Decoupled Dual Injection (GDDI) performs token-level structure-awareintervention while preserving the core LLM parameters. Experiments on WN18RRand FB15k-237 show that SLiNT achieves superior or competitive performancecompared with both embedding-based and generation-based baselines,demonstrating the effectiveness of structure-aware representation learning forscalable knowledge graph completion.</description>
      <author>example@mail.com (Mengxue Yang, Chun Yang, Jiaqi Zhu, Jiafan Li, Jingqi Zhang, Yuyang Li, Ying Li)</author>
      <guid isPermaLink="false">2509.06531v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>CAME-AB: Cross-Modality Attention with Mixture-of-Experts for Antibody Binding Site Prediction</title>
      <link>http://arxiv.org/abs/2509.06465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于抗体结合位点预测。该方法整合了五种生物学基础模态，并通过自适应模态融合和监督对比学习提高了预测准确性。&lt;h4&gt;背景&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的基于序列或结构的方法依赖于单一视图特征，无法识别抗原上的抗体特异性结合位点，这在表示和预测方面存在双重限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服现有方法局限性的新型抗体结合位点预测方法，通过多模态特征融合和自适应推理提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出CAME-AB框架，整合五种生物学基础模态（原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图），采用自适应模态融合模块动态加权每种模态，结合Transformer编码器和MoE模块促进特征专业化，并通过监督对比学习塑造潜在空间几何结构。&lt;h4&gt;主要发现&lt;/h4&gt;在基准抗体-抗原数据集上的实验表明，CAME-AB在精确率、召回率、F1分数、AUC-ROC和MCC等多个指标上持续优于强基线方法。消融研究验证了各架构组件的有效性和多模态特征集成的优势。&lt;h4&gt;结论&lt;/h4&gt;CAME-AB通过多模态特征融合和自适应推理成功克服了现有抗体结合位点预测方法的局限性，实现了更高的预测准确性。模型实现细节和代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;抗体结合位点预测在计算免疫学和治疗性抗体设计中起着关键作用。现有的序列或结构方法依赖于单一视图特征，无法识别抗原上的抗体特异性结合位点——这是表示和预测方面的双重限制。在本文中，我们提出了CAME-AB，一种具有专家混合主干的新型跨模态注意力框架，用于稳健的抗体结合位点预测。CAME-AB整合了五种生物学基础模态，包括原始氨基酸编码、BLOSUM替换谱、预训练语言模型嵌入、结构感知特征和GCN细化的生化图，形成统一的 multimodal 表示。为了增强自适应跨模态推理，我们提出了一个自适应模态融合模块，该模块学习根据全局相关性和输入特定贡献动态加权每种模态。Transformer编码器与MoE模块相结合进一步促进了特征专业化和能力扩展。我们还结合了监督对比学习目标，明确塑造潜在空间几何结构，鼓励类内紧凑性和类间可分性。为了提高优化稳定性和泛化能力，我们在训练期间应用随机权重平均。在基准抗体-抗原数据集上的广泛实验表明，CAME-AB在多个指标上持续优于强基线方法，包括精确率、召回率、F1分数、AUC-ROC和MCC。消融研究进一步验证了每个架构组件的有效性和多模态特征集成的优势。模型实现细节和代码可在 https://anonymous.4open.science/r/CAME-AB-C525 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Antibody binding site prediction plays a pivotal role in computationalimmunology and therapeutic antibody design. Existing sequence or structuremethods rely on single-view features and fail to identify antibody-specificbinding sites on the antigens-a dual limitation in representation andprediction. In this paper, we propose CAME-AB, a novel Cross-modality Attentionframework with a Mixture-of-Experts (MoE) backbone for robust antibody bindingsite prediction. CAME-AB integrates five biologically grounded modalities,including raw amino acid encodings, BLOSUM substitution profiles, pretrainedlanguage model embeddings, structure-aware features, and GCN-refinedbiochemical graphs-into a unified multimodal representation. To enhanceadaptive cross-modal reasoning, we propose an adaptive modality fusion modulethat learns to dynamically weight each modality based on its global relevanceand input-specific contribution. A Transformer encoder combined with an MoEmodule further promotes feature specialization and capacity expansion. Weadditionally incorporate a supervised contrastive learning objective toexplicitly shape the latent space geometry, encouraging intra-class compactnessand inter-class separability. To improve optimization stability andgeneralization, we apply stochastic weight averaging during training. Extensiveexperiments on benchmark antibody-antigen datasets demonstrate that CAME-ABconsistently outperforms strong baselines on multiple metrics, includingPrecision, Recall, F1-score, AUC-ROC, and MCC. Ablation studies furthervalidate the effectiveness of each architectural component and the benefit ofmultimodal feature integration. The model implementation details and the codesare available on https://anonymous.4open.science/r/CAME-AB-C525</description>
      <author>example@mail.com (Hongzong Li, Jiahao Ma, Zhanpeng Shi, Fanming Jin, Ye-Fan Hu, Jian-Dong Huang)</author>
      <guid isPermaLink="false">2509.06465v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Video-based Generalized Category Discovery via Memory-Guided Consistency-Aware Contrastive Learning</title>
      <link>http://arxiv.org/abs/2509.06306v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的Memory-guided Consistency-aware Contrastive Learning (MCCL)框架，用于解决视频领域的广义类别发现(Video-GCD)问题。该方法通过整合时空信息，显著提升了在视频中发现新类别的性能。&lt;h4&gt;背景&lt;/h4&gt;广义类别发现(GCD)是一个新兴且具有挑战性的开放世界问题。现有方法主要关注静态图像中的类别发现，但仅依靠静态视觉内容往往不足以可靠地发现新类别。&lt;h4&gt;目的&lt;/h4&gt;将GCD问题扩展到视频领域，引入Video-GCD新设置，有效整合时间上的多视角信息，以准确发现视频中的新类别。&lt;h4&gt;方法&lt;/h4&gt;提出MCCL框架，包含两个核心组件：1)一致性感知对比学习(CACL)，利用多视角时间特征估计未标记实例间的一致性分数并加权对比损失；2)记忆引导的表示增强(MGRE)，通过双重级别内存缓冲区提供全局上下文，增强类内紧凑性和类间可分离性。两者形成相互强化的反馈循环。&lt;h4&gt;主要发现&lt;/h4&gt;构建了新的Video-GCD基准数据集，包括动作识别和鸟类分类视频数据集。实验表明，该方法显著优于从基于图像设置调整的竞争性GCD方法，证明了时间信息对发现视频新类别的重要性。&lt;h4&gt;结论&lt;/h4&gt;通过整合时间信息，有效解决了视频中类别发现的挑战。研究强调了时间信息在视频新类别发现中的关键作用，并将公开代码。&lt;h4&gt;翻译&lt;/h4&gt;广义类别发现(GCD)是一个新兴的、具有挑战性的开放世界问题，近年来受到越来越多的关注。大多数现有的GCD方法专注于在静态图像中发现类别。然而，仅依靠静态视觉内容通常不足以可靠地发现新类别。为了弥补这一差距，我们将GCD问题扩展到视频领域，引入了一种新的设置，称为Video-GCD。因此，有效整合时间上的多视角信息对于准确的Video-GCD至关重要。为了应对这一挑战，我们提出了一种新颖的记忆引导的一致性感知对比学习(MCCL)框架，该框架明确捕获时空线索，并通过一致性引导的投票机制将其整合到对比学习中。MCCL包含两个核心组件：一致性感知对比学习(CACL)和记忆引导的表示增强(MGRE)。CACL利用多视角时间特征来估计未标记实例之间的一致性分数，然后相应地加权对比损失。MGRE引入了一个双重级别的内存缓冲区，保持特征级别和logit级别的表示，提供全局上下文以增强类内紧凑性和类间可分离性。这反过来又完善了CACL中的一致性估计，形成了表示学习和一致性建模之间的相互强化的反馈循环。为了促进全面评估，我们构建了一个新的、具有挑战性的Video-GCD基准，包括动作识别和鸟类分类视频数据集。大量实验表明，我们的方法显著优于从基于图像设置调整的竞争性GCD方法，强调了时间信息对于在视频中发现新类别的重要性。代码将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalized Category Discovery (GCD) is an emerging and challengingopen-world problem that has garnered increasing attention in recent years. Mostexisting GCD methods focus on discovering categories in static images. However,relying solely on static visual content is often insufficient to reliablydiscover novel categories. To bridge this gap, we extend the GCD problem to thevideo domain and introduce a new setting, termed Video-GCD. Thus, effectivelyintegrating multi-perspective information across time is crucial for accurateVideo-GCD. To tackle this challenge, we propose a novel Memory-guidedConsistency-aware Contrastive Learning (MCCL) framework, which explicitlycaptures temporal-spatial cues and incorporates them into contrastive learningthrough a consistency-guided voting mechanism. MCCL consists of two corecomponents: Consistency-Aware Contrastive Learning(CACL) and Memory-GuidedRepresentation Enhancement (MGRE). CACL exploits multiperspective temporalfeatures to estimate consistency scores between unlabeled instances, which arethen used to weight the contrastive loss accordingly. MGRE introduces adual-level memory buffer that maintains both feature-level and logit-levelrepresentations, providing global context to enhance intra-class compactnessand inter-class separability. This in turn refines the consistency estimationin CACL, forming a mutually reinforcing feedback loop between representationlearning and consistency modeling. To facilitate a comprehensive evaluation, weconstruct a new and challenging Video-GCD benchmark, which includes actionrecognition and bird classification video datasets. Extensive experimentsdemonstrate that our method significantly outperforms competitive GCDapproaches adapted from image-based settings, highlighting the importance oftemporal information for discovering novel categories in videos. The code willbe publicly available.</description>
      <author>example@mail.com (Zhang Jing, Pu Nan, Xie Yu Xiang, Guo Yanming, Lu Qianqi, Zou Shiwei, Yan Jie, Chen Yan)</author>
      <guid isPermaLink="false">2509.06306v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>PathoHR: Hierarchical Reasoning for Vision-Language Models in Pathology</title>
      <link>http://arxiv.org/abs/2509.06105v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accept by EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的基准测试PathoHR-Bench和病理特定的视觉语言训练方案，解决了现有模型在病理图像分析中的局限性，提高了层次语义理解和组合推理能力。&lt;h4&gt;背景&lt;/h4&gt;病理图像的准确分析对自动化肿瘤诊断至关重要，但由于组织图像中高度的结构相似性和细微的形态学变化，这仍然具有挑战性。当前的视觉语言模型往往难以捕捉解释结构化病理报告所需的复杂推理能力。&lt;h4&gt;目的&lt;/h4&gt;提出PathoHR-Bench，一个新的基准测试，用于评估视觉语言模型在病理领域中的层次语义理解和组合推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入了一种病理特定的视觉语言训练方案，该方案生成增强和扰动的样本用于多模态对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;现有的视觉语言模型无法有效建模复杂的跨模态关系，限制了它们在临床环境中的应用；该方法在PathoHR-Bench和另外六个病理数据集上取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法在细粒度病理表示方面是有效的，能够提高视觉语言模型在病理图像分析中的表现。&lt;h4&gt;翻译&lt;/h4&gt;病理图像的准确分析对自动化肿瘤诊断至关重要，但由于组织图像中高度的结构相似性和细微的形态学变化，这仍然具有挑战性。当前的视觉语言模型往往难以捕捉解释结构化病理报告所需的复杂推理能力。为了解决这些局限性，我们提出了PathoHR-Bench，这是一个新的基准测试，旨在评估视觉语言模型在病理领域中的层次语义理解和组合推理能力。该基准测试的结果表明，现有的视觉语言模型无法有效建模复杂的跨模态关系，从而限制了它们在临床环境中的应用。为了克服这一点，我们进一步引入了一种病理特定的视觉语言训练方案，该方案生成增强和扰动的样本用于多模态对比学习。实验评估表明，我们的方法在PathoHR-Bench和另外六个病理数据集上取得了最先进的性能，突显了它在细粒度病理表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate analysis of pathological images is essential for automated tumordiagnosis but remains challenging due to high structural similarity and subtlemorphological variations in tissue images. Current vision-language (VL) modelsoften struggle to capture the complex reasoning required for interpretingstructured pathological reports. To address these limitations, we proposePathoHR-Bench, a novel benchmark designed to evaluate VL models' abilities inhierarchical semantic understanding and compositional reasoning within thepathology domain. Results of this benchmark reveal that existing VL models failto effectively model intricate cross-modal relationships, hence limiting theirapplicability in clinical setting. To overcome this, we further introduce apathology-specific VL training scheme that generates enhanced and perturbedsamples for multimodal contrastive learning. Experimental evaluationsdemonstrate that our approach achieves state-of-the-art performance onPathoHR-Bench and six additional pathology datasets, highlighting itseffectiveness in fine-grained pathology representation.</description>
      <author>example@mail.com (Yating Huang, Ziyan Huang, Lintao Xiang, Qijun Yang, Hujun Yin)</author>
      <guid isPermaLink="false">2509.06105v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AttriPrompt: Dynamic Prompt Composition Learning for CLIP</title>
      <link>http://arxiv.org/abs/2509.05949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AttriPrompt框架，解决了当前深度文本提示方法的两个关键限制：过度依赖对比学习目标而忽略细粒度特征优化，以及使用静态提示无法实现内容自适应。该框架通过利用CLIP视觉编码器的中间层特征增强文本语义表示，并引入属性检索模块、双流对比学习和自正则化机制，在三个基准测试上实现了高达7.37%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;提示学习方法的发展推动了更深层次的提示设计以增强模型性能，但当前深度文本提示方法存在两个关键限制：过度依赖对比学习目标优先考虑高层语义对齐而忽略细粒度特征优化；在所有输入类别中使用静态提示无法实现内容自适应。&lt;h4&gt;目的&lt;/h4&gt;解决当前深度文本提示方法的两个关键限制，提出AttriPrompt框架，通过利用CLIP视觉编码器的中间层特征来增强和 refine 文本语义表示，实现细粒度特征优化和内容自适应。&lt;h4&gt;方法&lt;/h4&gt;设计属性检索模块对每一层的视觉特征进行聚类，聚合视觉特征从提示池中检索语义相似的提示并连接到文本编码器各层输入；引入双流对比学习实现细粒度对齐；通过自正则化机制在提示文本特征和非提示文本特征间应用显式正则化约束防止过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准测试上的大量实验表明AttriPrompt优于最先进方法，在基础到新颖设置中实现了高达7.37%的改进；该方法在跨领域知识转移方面表现出色，使视觉语言预训练模型成为更可行的现实世界解决方案。&lt;h4&gt;结论&lt;/h4&gt;AttriPrompt框架有效解决了当前深度文本提示方法的两个关键限制，通过利用CLIP视觉编码器的中间层特征和设计属性检索模块实现了细粒度特征优化和内容自适应，自正则化机制有效防止了在有限训练数据上的过拟合，在多个基准测试上表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;提示学习方法的发展推动了更深层次的提示设计以增强模型性能。然而，当前深度文本提示方法存在两个关键限制：过度依赖对比学习目标，优先考虑高层语义对齐，而忽略细粒度特征优化；在所有输入类别中使用静态提示，无法实现内容自适应。为解决这些限制，我们提出了AttriPrompt-一个新颖的框架，通过利用CLIP视觉编码器的中间层特征来增强和 refine 文本语义表示。我们设计了一个属性检索模块，首先对每一层的视觉特征进行聚类。聚合的视觉特征从提示池中检索语义相似的提示，然后将这些提示连接到文本编码器每一层的输入中。利用提示文本特征中嵌入的分层视觉信息，我们引入双流对比学习来实现细粒度对齐。此外，我们通过在提示文本特征和非提示文本特征之间应用显式的正则化约束，引入了自正则化机制，以防止在有限训练数据上的过拟合。在三个基准测试上的大量实验证明了AttriPrompt优于最先进的方法，在基础到新颖设置中实现了高达7.37%的改进。我们方法在跨领域知识转移方面的优势，使视觉语言预训练模型成为更可行的现实世界解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The evolution of prompt learning methodologies has driven exploration ofdeeper prompt designs to enhance model performance. However, current deep textprompting approaches suffer from two critical limitations: Over-reliance onconstrastive learning objectives that prioritize high-level semantic alignment,neglecting fine-grained feature optimization; Static prompts across all inputcategories, preventing content-aware adaptation. To address these limitations,we propose AttriPrompt-a novel framework that enhances and refines textualsemantic representations by leveraging the intermediate-layer features ofCLIP's vision encoder. We designed an Attribute Retrieval module that firstclusters visual features from each layer. The aggregated visual featuresretrieve semantically similar prompts from a prompt pool, which are thenconcatenated to the input of every layer in the text encoder. Leveraginghierarchical visual information embedded in prompted text features, weintroduce Dual-stream Contrastive Learning to realize fine-grained alignment.Furthermore, we introduce a Self-Regularization mechanism by applying explicitregularization constraints between the prompted and non-prompted text featuresto prevent overfitting on limited training data. Extensive experiments acrossthree benchmarks demonstrate AttriPrompt's superiority over state-of-the-artmethods, achieving up to 7.37\% improvement in the base-to-novel setting. Theobserved strength of our method in cross-domain knowledge transfer positionsvision-language pre-trained models as more viable solutions for real-worldimplementation.</description>
      <author>example@mail.com (Qiqi Zhan, Shiwei Li, Qingjie Liu, Yunhong Wang)</author>
      <guid isPermaLink="false">2509.05949v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of the State-of-the-Art in Conversational Question Answering Systems</title>
      <link>http://arxiv.org/abs/2509.05716v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 12 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是对话式问答系统(ConvQA)领域的综述，提供了该领域的全面分析，包括核心组件、先进机器学习技术、大型语言模型的应用、关键数据集以及未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;对话式问答系统已成为自然语言处理(NLP)的关键领域，使机器能够进行动态和具有上下文感知能力的对话。这些能力正被应用于客户支持、教育、法律和医疗保健等多个领域，在这些领域中保持连贯且相关的对话至关重要。&lt;h4&gt;目的&lt;/h4&gt;提供对话式问答系统最先进技术的综合分析，全面概述ConvQA领域，并为该领域的未来发展提供有价值的见解。&lt;h4&gt;方法&lt;/h4&gt;论文分析了ConvQA系统的核心组件（历史选择、问题理解和答案预测），调查了先进的机器学习技术（包括强化学习、对比学习和迁移学习）以提高ConvQA的准确性和效率，并探讨了大型语言模型（如RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3）的关键作用。&lt;h4&gt;主要发现&lt;/h4&gt;大型语言模型在ConvQA领域发挥着关键作用，通过数据规模扩展和架构进步展示了其影响力。论文还分析了关键的ConvQA数据集。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了ConvQA领域的全面概述，并为指导该领域的未来发展提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;对话式问答(ConvQA)系统已成为自然语言处理(NLP)领域的关键领域，推动了机器能够进行动态和具有上下文感知能力的对话的进步。这些能力正被应用于各个领域，即客户支持、教育、法律和医疗保健，在这些领域中保持连贯且相关的对话至关重要。基于最近的进展，本综述对ConvQA的最先进技术进行了全面分析。本综述首先考察了ConvQA系统的核心组件，即历史选择、问题理解和答案预测，强调了它们在确保多轮对话中的连贯性和相关性方面的相互作用。它进一步研究了先进机器学习技术的应用，包括但不限于强化学习、对比学习和迁移学习，以提高ConvQA的准确性和效率。还探讨了大型语言模型的关键作用，即RoBERTa、GPT-4、Gemini 2.0 Flash、Mistral 7B和LLaMA 3，从而展示了它们通过数据规模扩展和架构进步所产生的影响。此外，本综述还对关键的ConvQA数据集进行了全面分析，最后概述了开放的研究方向。总体而言，这项工作提供了对ConvQA领域的全面概述，并为指导该领域的未来进展提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conversational Question Answering (ConvQA) systems have emerged as a pivotalarea within Natural Language Processing (NLP) by driving advancements thatenable machines to engage in dynamic and context-aware conversations. Thesecapabilities are increasingly being applied across various domains, i.e.,customer support, education, legal, and healthcare where maintaining a coherentand relevant conversation is essential. Building on recent advancements, thissurvey provides a comprehensive analysis of the state-of-the-art in ConvQA.This survey begins by examining the core components of ConvQA systems, i.e.,history selection, question understanding, and answer prediction, highlightingtheir interplay in ensuring coherence and relevance in multi-turnconversations. It further investigates the use of advanced machine learningtechniques, including but not limited to, reinforcement learning, contrastivelearning, and transfer learning to improve ConvQA accuracy and efficiency. Thepivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash,Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impactthrough data scalability and architectural advancements. Additionally, thissurvey presents a comprehensive analysis of key ConvQA datasets and concludesby outlining open research directions. Overall, this work offers acomprehensive overview of the ConvQA landscape and provides valuable insightsto guide future advancements in the field.</description>
      <author>example@mail.com (Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng)</author>
      <guid isPermaLink="false">2509.05716v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>MSRFormer: Road Network Representation Learning using Multi-scale Feature Fusion of Heterogeneous Spatial Interactions</title>
      <link>http://arxiv.org/abs/2509.05685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MSRFormer，一个新颖的道路网络表示学习框架，通过整合多尺度空间交互来解决道路网络分析的挑战。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异质性和层次性给准确的表示学习带来了挑战。图神经网络由于同质性假设和只关注单一结构尺度，在聚合邻居节点特征时往往表现不佳。&lt;h4&gt;目的&lt;/h4&gt;解决城市道路网络异质性和层次性带来的表示学习挑战，特别是处理空间交互的流量异质性和长距离依赖问题。&lt;h4&gt;方法&lt;/h4&gt;MSRFormer框架使用空间流卷积从大型轨迹数据集中提取小尺度特征，识别尺度相关的空间交互区域来捕获道路网络的空间结构和流量异质性。通过采用图Transformer，MSRFormer有效地捕获了多尺度上的复杂空间依赖关系。空间交互特征通过残差连接融合，然后输入到对比学习算法中，以获得最终的道路网络表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与交通相关的任务从整合轨迹数据中获益更多，在复杂的道路网络结构中，与最具竞争力的基线方法相比，实现了高达16%的改进。&lt;h4&gt;结论&lt;/h4&gt;这项研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互的尺度效应与流量异质性之间相互作用的独特关联模式。&lt;h4&gt;翻译&lt;/h4&gt;使用深度学习将道路网络数据转换为向量表示已被证明对道路网络分析有效。然而，城市道路网络的异质性和层次性给准确的表示学习带来了挑战。图神经网络，其聚合来自邻居节点的特征，常常由于其同质性假设和对单一结构尺度的关注而表现不佳。为解决这些问题，本文提出了MSRFormer，一种新颖的道路网络表示学习框架，通过解决其流量异质性和长距离依赖来整合多尺度空间交互。它使用空间流卷积从大型轨迹数据集中提取小尺度特征，并识别尺度相关的空间交互区域以捕获道路网络的空间结构和流量异质性。通过采用图Transformer，MSRFormer有效地捕获了多尺度上的复杂空间依赖关系。空间交互特征使用残差连接融合，然后输入到对比学习算法中以推导最终的道路网络表示。在两个真实世界数据集上的验证表明，MSRFormer在两个道路网络分析任务中优于基线方法。MSRFormer的性能提升表明，与交通相关的任务从整合轨迹数据中获益更多，在复杂的道路网络结构中与最具竞争力的基线方法相比实现了高达16%的改进。这项研究为开发任务无关的道路网络表示模型提供了实用框架，并突显了空间交互的尺度效应与流量异质性之间相互作用的独特关联模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming road network data into vector representations using deeplearning has proven effective for road network analysis. However, urban roadnetworks' heterogeneous and hierarchical nature poses challenges for accuraterepresentation learning. Graph neural networks, which aggregate features fromneighboring nodes, often struggle due to their homogeneity assumption and focuson a single structural scale. To address these issues, this paper presentsMSRFormer, a novel road network representation learning framework thatintegrates multi-scale spatial interactions by addressing their flowheterogeneity and long-distance dependencies. It uses spatial flow convolutionto extract small-scale features from large trajectory datasets, and identifiesscale-dependent spatial interaction regions to capture the spatial structure ofroad networks and flow heterogeneity. By employing a graph transformer,MSRFormer effectively captures complex spatial dependencies across multiplescales. The spatial interaction features are fused using residual connections,which are fed to a contrastive learning algorithm to derive the final roadnetwork representation. Validation on two real-world datasets demonstrates thatMSRFormer outperforms baseline methods in two road network analysis tasks. Theperformance gains of MSRFormer suggest the traffic-related task benefits morefrom incorporating trajectory data, also resulting in greater improvements incomplex road network structures with up to 16% improvements compared to themost competitive baseline method. This research provides a practical frameworkfor developing task-agnostic road network representation models and highlightsdistinct association patterns of the interplay between scale effects and flowheterogeneity of spatial interactions.</description>
      <author>example@mail.com (Jian Yang, Jiahui Wu, Li Fang, Hongchao Fan, Bianying Zhang, Huijie Zhao, Guangyi Yang, Rui Xin, Xiong You)</author>
      <guid isPermaLink="false">2509.05685v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>DuoCLR: Dual-Surrogate Contrastive Learning for Skeleton-based Human Action Segmentation</title>
      <link>http://arxiv.org/abs/2509.05543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 accepted paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种对比表示学习框架，通过使用修剪过的骨架序列进行预训练来增强人体动作分割。该框架利用多尺度表示和跨序列变化，并提出新颖的'Shuffle and Warp'数据增强策略，引入跨排列对比和相对顺序推理两个代理任务，构建双代理对比学习网络，显著提升动作分割性能。&lt;h4&gt;背景&lt;/h4&gt;以往的动作表示学习方法都是针对动作识别设计的，基于孤立的序列级表示，没有充分利用多尺度表示和跨序列变化的信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比表示学习框架，通过预训练增强人体动作分割性能，专注于利用多尺度表示和跨序列变化。&lt;h4&gt;方法&lt;/h4&gt;提出'Shuffle and Warp'数据增强策略，利用多动作排列；引入跨排列对比学习类内相似性和相对顺序推理学习类间上下文；构建双代理对比学习网络；在修剪过的骨架数据集上预训练，在未修剪数据集上评估。&lt;h4&gt;主要发现&lt;/h4&gt;DuoCLR在多类和多标签动作分割任务中显著优于最先进的比较方法；消融研究验证了所提出方法的每个组件的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过利用多尺度表示、跨序列变化、新颖的数据增强策略和双代理学习任务，有效增强了人体动作分割性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种对比表示学习框架，通过使用修剪过的（单一动作）骨架序列进行预训练来增强人体动作分割。与以往针对动作识别设计的、基于孤立序列级表示的表示学习工作不同，所提出的框架专注于利用多尺度表示和跨序列变化。更具体地说，它提出了一种新颖的数据增强策略'Shuffle and Warp'，利用多样的多动作排列。后者有效辅助了对比学习中引入的两个代理任务：跨排列对比和相对顺序推理。在优化过程中，CPC通过对比不同排列中相同动作类别的表示来学习类内相似性，而ROR通过预测两个排列之间的相对映射来推理类间上下文。这些任务共同使双代理对比学习网络能够学习针对动作分割优化的多尺度特征表示。在实验中，DuoCLR在修剪过的骨架数据集上预训练，然后在未修剪的数据集上进行评估，在多类和多标签动作分割任务中均显示出比最先进的比较方法显著的优势。最后，进行了消融研究以评估所提出方法的每个组件的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, a contrastive representation learning framework is proposed toenhance human action segmentation via pre-training using trimmed (singleaction) skeleton sequences. Unlike previous representation learning works thatare tailored for action recognition and that build upon isolated sequence-wiserepresentations, the proposed framework focuses on exploiting multi-scalerepresentations in conjunction with cross-sequence variations. Morespecifically, it proposes a novel data augmentation strategy, 'Shuffle andWarp', which exploits diverse multi-action permutations. The latter effectivelyassists two surrogate tasks that are introduced in contrastive learning: CrossPermutation Contrasting (CPC) and Relative Order Reasoning (ROR). Inoptimization, CPC learns intra-class similarities by contrastingrepresentations of the same action class across different permutations, whileROR reasons about inter-class contexts by predicting relative mapping betweentwo permutations. Together, these tasks enable a Dual-Surrogate ContrastiveLearning (DuoCLR) network to learn multi-scale feature representationsoptimized for action segmentation. In experiments, DuoCLR is pre-trained on atrimmed skeleton dataset and evaluated on an untrimmed dataset where itdemonstrates a significant boost over state-the-art comparatives in bothmulti-class and multi-label action segmentation tasks. Lastly, ablation studiesare conducted to evaluate the effectiveness of each component of the proposedapproach.</description>
      <author>example@mail.com (Haitao Tian, Pierre Payeur)</author>
      <guid isPermaLink="false">2509.05543v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Asynchronous Message Passing for Addressing Oversquashing in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.06777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的模型无关框架，通过异步更新节点特征来解决图神经网络中的过度压缩问题，在多个数据集上取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)存在过度压缩问题，当任务需要长程交互时会出现，这是由于瓶颈限制了远距离节点间的消息传播。现有的图重连方法虽然可能表现良好，但会损害归纳偏差，导致下游任务中信息损失显著增加；而增加通道容量虽然可以克服信息瓶颈，但会增加模型的参数复杂度。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，作者提出了一种高效的模型无关框架，通过异步更新节点特征来避免传统同步消息传递GNNs的局限性。&lt;h4&gt;方法&lt;/h4&gt;该框架基于节点中心度值在每一层创建节点批次，只有属于这些批次的节点特征才会被更新。异步消息更新跨层顺序处理信息，避免了同时压缩到固定容量通道中。作者还从理论上证明了该框架比标准同步方法保持更高的特征敏感度界限。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在六个标准图数据集和两个长程数据集上应用于图分类任务，在REDDIT-BINARY和Peptides-struct上分别取得了5%和4%的显著性能提升。&lt;h4&gt;结论&lt;/h4&gt;提出的异步更新框架能够有效解决GNN中的过度压缩问题，同时保持或提高模型性能，为处理需要长程交互的图任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)存在过度压缩问题，当任务需要长程交互时会出现。这个问题源于瓶颈限制了远距离节点间的消息传播。最近，图重连方法修改边连接性，并期望在长程任务上表现良好。然而，图重连损害了归纳偏差，导致在解决下游任务时出现显著的信息损失。此外，增加通道容量虽然可以克服信息瓶颈，但会增加模型的参数复杂度。为了缓解这些缺点，我们提出了一种高效的模型无关框架，该框架异步更新节点特征，不同于传统的同步消息传递GNNs。我们的框架基于节点中心度值在每一层创建节点批次，只有属于这些批次的节点特征才会被更新。异步消息更新跨层顺序处理信息，避免了同时压缩到固定容量通道中。我们还从理论上证明，与标准同步方法相比，我们提出的框架保持了更高的特征敏感度界限。我们的框架应用于六个标准图数据集和两个长程数据集进行图分类，在REDDIT-BINARY和Peptides-struct上分别取得了5%和4%的显著性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) suffer from Oversquashing, which occurs whentasks require long-range interactions. The problem arises from the presence ofbottlenecks that limit the propagation of messages among distant nodes.Recently, graph rewiring methods modify edge connectivity and are expected toperform well on long-range tasks. Yet, graph rewiring compromises the inductivebias, incurring significant information loss in solving the downstream task.Furthermore, increasing channel capacity may overcome information bottlenecksbut enhance the parameter complexity of the model. To alleviate theseshortcomings, we propose an efficient model-agnostic framework thatasynchronously updates node features, unlike traditional synchronous messagepassing GNNs. Our framework creates node batches in every layer based on thenode centrality values. The features of the nodes belonging to these batcheswill only get updated. Asynchronous message updates process informationsequentially across layers, avoiding simultaneous compression intofixed-capacity channels. We also theoretically establish that our proposedframework maintains higher feature sensitivity bounds compared to standardsynchronous approaches. Our framework is applied to six standard graph datasetsand two long-range datasets to perform graph classification and achievesimpressive performances with a $5\%$ and $4\%$ improvements on REDDIT-BINARYand Peptides-struct, respectively.</description>
      <author>example@mail.com (Kushal Bose, Swagatam Das)</author>
      <guid isPermaLink="false">2509.06777v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>VariSAC: V2X Assured Connectivity in RIS-Aided ISAC via GNN-Augmented Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.06763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VariSAC的图神经网络增强深度强化学习框架，用于解决RIS辅助的ISAC使能V2X系统中的统一可靠性和资源优化问题。&lt;h4&gt;背景&lt;/h4&gt;在车辆网络中集成可重构智能表面(RIS)与集成感知和通信(ISAC)技术可实现动态空间资源管理和实时环境适应，但车辆到基础设施(V2I)和车辆到车辆(V2V)连接要求共存，加上高度动态和异构的网络拓扑，给统一的可靠性建模和资源优化带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架来确保在RIS辅助的、ISAC使能的车辆到万物(V2X)系统中实现时间连续的连接，解决V2I和V2V连接的统一可靠性问题。&lt;h4&gt;方法&lt;/h4&gt;引入连续连接比(CCR)作为统一指标；采用带有残差适配器的图神经网络编码复杂高维系统状态；使用软演员-评论家(SAC)代理联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界城市数据集上的实验表明，VariSAC在连续V2I ISAC连接和V2V传递可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;h4&gt;结论&lt;/h4&gt;VariSAC框架有效解决了V2X系统中统一可靠性和资源优化的问题，实现了在动态环境中的持久连接。&lt;h4&gt;翻译&lt;/h4&gt;在车辆网络中集成可重构智能表面(RIS)和集成感知与通信(ISAC)技术能够实现动态空间资源管理和实时环境适应。然而，车辆到基础设施(V2I)和车辆到车辆(V2V)连接要求共存，加上高度动态和异构的网络拓扑，给统一的可靠性建模和资源优化带来了显著挑战。为解决这些问题，我们提出了VariSAC，一种用于在RIS辅助的、ISAC使能的车辆到万物(V2X)系统中确保时间连续连接的图神经网络(GNN)增强深度强化学习框架。具体而言，我们引入了连续连接比(CCR)，这是一个统一指标，表征V2I连接的持续时间可靠性和V2V链路的概率传递保证，从而统一了它们的连续可靠性语义。接下来，我们采用带有残差适配器的GNN来编码复杂的高维系统状态，捕获车辆、基站(BS)和RIS节点之间的空间依赖关系。这些表示随后由软演员-评论家(SAC)代理处理，联合优化信道分配、功率控制和RIS配置，以最大化CCR驱动的长期奖励。在真实世界城市数据集上的大量实验表明，VariSAC在连续V2I ISAC连接和V2V传递可靠性方面持续优于现有基线，能够在高度动态的车辆环境中实现持久连接。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of Reconfigurable Intelligent Surfaces (RIS) and IntegratedSensing and Communication (ISAC) in vehicular networks enables dynamic spatialresource management and real-time adaptation to environmental changes. However,the coexistence of distinct vehicle-to-infrastructure (V2I) andvehicle-to-vehicle (V2V) connectivity requirements, together with highlydynamic and heterogeneous network topologies, presents significant challengesfor unified reliability modeling and resource optimization. To address theseissues, we propose VariSAC, a graph neural network (GNN)-augmented deepreinforcement learning framework for assured, time-continuous connectivity inRIS-assisted, ISAC-enabled vehicle-to-everything (V2X) systems. Specifically,we introduce the Continuous Connectivity Ratio (CCR), a unified metric thatcharacterizes the sustained temporal reliability of V2I connections and theprobabilistic delivery guarantees of V2V links, thus unifying their continuousreliability semantics. Next, we employ a GNN with residual adapters to encodecomplex, high-dimensional system states, capturing spatial dependencies amongvehicles, base stations (BS), and RIS nodes. These representations are thenprocessed by a Soft Actor-Critic (SAC) agent, which jointly optimizes channelallocation, power control, and RIS configurations to maximize CCR-drivenlong-term rewards. Extensive experiments on real-world urban datasetsdemonstrate that VariSAC consistently outperforms existing baselines in termsof continuous V2I ISAC connectivity and V2V delivery reliability, enablingpersistent connectivity in highly dynamic vehicular environments.</description>
      <author>example@mail.com (Huijun Tang, Wang Zeng, Ming Du, Pinlong Zhao, Pengfei Jiao, Huaming Wu, Hongjian Sun)</author>
      <guid isPermaLink="false">2509.06763v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Long-Range Graph Wavelet Networks</title>
      <link>http://arxiv.org/abs/2509.06743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Long-Range Graph Wavelet Networks (LR-GWN)的新型图神经网络，能够有效建模图中的长程交互，解决图机器学习中的核心挑战。&lt;h4&gt;背景&lt;/h4&gt;在图机器学习中，建模长程交互（信息跨越图中遥远部分传播）是一个核心挑战。图小波受多分辨率信号处理的启发，为捕获局部和全局结构提供了原则性的方法。然而，现有的基于小波的图神经网络依赖于有限阶多项式近似，限制了感受野并阻碍了长程传播。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获图中长程交互的图神经网络，克服现有基于小波的图神经网络的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出LR-GWN，将小波滤波器分解为互补的局部和全局组件。局部聚合通过高效的低阶多项式处理，长程交互则通过灵活的频域参数化捕获，在原则性小波框架内统一短距离和长距离信息流。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，LR-GWN在长程基准测试中实现了基于小波方法的最先进性能，同时在短距离数据集上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;LR-GWN成功解决了现有基于小波的图神经网络在长程交互建模方面的局限性，通过结合局部和全局组件，实现了更有效的信息传播，同时保持了计算效率。&lt;h4&gt;翻译&lt;/h4&gt;建模长程交互，即信息跨越图中遥远部分的传播，是图机器学习中的一个核心挑战。受多分辨率信号处理启发的图小波，为捕获局部和全局结构提供了原则性的方法。然而，现有的基于小波的图神经网络依赖于有限阶多项式近似，这限制了它们的感受野并阻碍了长程传播。我们提出了长程图小波网络(LR-GWN)，它将小波滤波器分解为互补的局部和全局组件。局部聚合通过高效的低阶多项式处理，而长程交互则通过灵活的频域参数化来捕获。这种混合设计在原则性的小波框架内统一了短距离和长距离信息流。实验表明，LR-GWN在长程基准测试中实现了基于小波方法的最先进性能，同时在短距离数据集上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling long-range interactions, the propagation of information acrossdistant parts of a graph, is a central challenge in graph machine learning.Graph wavelets, inspired by multi-resolution signal processing, provide aprincipled way to capture both local and global structures. However, existingwavelet-based graph neural networks rely on finite-order polynomialapproximations, which limit their receptive fields and hinder long-rangepropagation. We propose Long-Range Graph Wavelet Networks (LR-GWN), whichdecompose wavelet filters into complementary local and global components. Localaggregation is handled with efficient low-order polynomials, while long-rangeinteractions are captured through a flexible spectral domain parameterization.This hybrid design unifies short- and long-distance information flow within aprincipled wavelet framework. Experiments show that LR-GWN achievesstate-of-the-art performance among wavelet-based methods on long-rangebenchmarks, while remaining competitive on short-range datasets.</description>
      <author>example@mail.com (Filippo Guerranti, Fabrizio Forte, Simon Geisler, Stephan Günnemann)</author>
      <guid isPermaLink="false">2509.06743v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AnalysisGNN: Unified Music Analysis with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.06654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 17th International Symposium on Computer Music  Multidisciplinary Research (CMMR) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了AnalysisGNN，一种新的图神经网络框架，用于整合异构注释的符号数据集进行音乐分析。通过数据洗牌策略、加权多任务损失和logit融合技术，结合非和弦音预测模块，实现了与现有方法相当的性能，同时提高了对域偏移和注释不一致的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;近年来计算音乐分析方法蓬勃发展，但每种方法通常针对特定的分析领域，在处理异构注释的符号数据集时存在挑战。&lt;h4&gt;目的&lt;/h4&gt;引入AnalysisGNN框架，整合异构注释的符号数据集，用于全面乐谱分析。&lt;h4&gt;方法&lt;/h4&gt;使用数据洗牌策略，采用自定义加权多任务损失，在特定任务分类器之间进行logit融合，集成非和弦音预测模块识别并排除经过音和非功能性音符，以提高标签信号的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;AnalysisGNN实现了与传统静态数据集方法相当的性能，在多个异构语料库上对域偏移和注释不一致性表现出更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;AnalysisGNN是一种有效的框架，能够整合异构注释的数据集进行综合音乐分析，在处理域偏移和注释不一致方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;近年来，计算音乐分析方法蓬勃发展，但每种方法通常针对特定的分析领域。在这项工作中，我们引入了AnalysisGNN，一种新颖的图神经网络框架，它利用数据洗牌策略和自定义加权多任务损失，以及在特定任务分类器之间的logit融合，来整合异构注释的符号数据集，用于全面的乐谱分析。我们进一步集成了一个非和弦音预测模块，该模块识别并排除所有任务中的经过音和非功能性音符，从而提高标签信号的一致性。实验评估表明，AnalysisGNN实现了与传统静态数据集方法相当的性能，同时在多个异构语料库上对域偏移和注释不一致表现出更强的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent years have seen a boom in computational approaches to music analysis,yet each one is typically tailored to a specific analytical domain. In thiswork, we introduce AnalysisGNN, a novel graph neural network framework thatleverages a data-shuffling strategy with a custom weighted multi-task loss andlogit fusion between task-specific classifiers to integrate heterogeneouslyannotated symbolic datasets for comprehensive score analysis. We furtherintegrate a Non-Chord-Tone prediction module, which identifies and excludespassing and non-functional notes from all tasks, thereby improving theconsistency of label signals. Experimental evaluations demonstrate thatAnalysisGNN achieves performance comparable to traditional static-datasetapproaches, while showing increased resilience to domain shifts and annotationinconsistencies across multiple heterogeneous corpora.</description>
      <author>example@mail.com (Emmanouil Karystinaios, Johannes Hentschel, Markus Neuwirth, Gerhard Widmer)</author>
      <guid isPermaLink="false">2509.06654v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>PAC-Bayesian Generalization Bounds for Graph Convolutional Networks on Inductive Node Classification</title>
      <link>http://arxiv.org/abs/2509.06600v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了图神经网络(GCNs)用于归纳节点分类的PAC-Bayesian理论分析，考虑了节点作为相关且非同分布的数据点，推导了一层和两层GCNs的泛化边界，并建立了泛化差距收敛的条件。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理图结构数据方面取得了显著成功，但真实世界的图具有动态性，新节点不断添加，现有连接可能随时间变化。&lt;h4&gt;目的&lt;/h4&gt;建立图神经网络在动态图环境中的理论分析，特别是针对归纳节点分类任务，解决以往理论研究中无法充分建模时间演化和结构动态性的问题。&lt;h4&gt;方法&lt;/h4&gt;采用PAC-Bayesian理论框架分析图卷积网络(GCNs)，将节点视为相关且非同分布的数据点，推导泛化边界并建立收敛条件。&lt;h4&gt;主要发现&lt;/h4&gt;推导了一层GCNs的新泛化边界，明确包含数据依赖性和非平稳性的影响；建立泛化差距随节点数量增加而收敛到零的充分条件；对于两层GCNs，需要更强的图拓扑假设来保证收敛。&lt;h4&gt;结论&lt;/h4&gt;本研究为理解和改进动态图环境中图神经网络的泛化能力建立了理论基础，对处理动态图数据具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种应用中处理图结构数据方面取得了显著成功。真实世界图的一个关键方面是其动态性，新节点不断被添加，现有连接可能随时间变化。以往的理论研究主要基于归纳学习框架，无法充分建模这种时间演化和结构动态性。本文提出了图卷积网络(GCNs)用于归纳节点分类的PAC-Bayesian理论分析，将节点视为相关且非同分布的数据点。我们推导了一层GCNs的新泛化边界，明确包含了数据依赖性和非平稳性的影响，并建立了充分条件，使得泛化差距随着节点数量的增加而收敛到零。此外，我们将分析扩展到两层GCNs，并发现需要更强的图拓扑假设来保证收敛。这项工作为理解和改进动态图环境中GNN的泛化能力建立了理论基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have achieved remarkable success in processinggraph-structured data across various applications. A critical aspect ofreal-world graphs is their dynamic nature, where new nodes are continuallyadded and existing connections may change over time. Previous theoreticalstudies, largely based on the transductive learning framework, fail toadequately model such temporal evolution and structural dynamics. In thispaper, we presents a PAC-Bayesian theoretical analysis of graph convolutionalnetworks (GCNs) for inductive node classification, treating nodes as dependentand non-identically distributed data points. We derive novel generalizationbounds for one-layer GCNs that explicitly incorporate the effects of datadependency and non-stationarity, and establish sufficient conditions underwhich the generalization gap converges to zero as the number of nodesincreases. Furthermore, we extend our analysis to two-layer GCNs, and revealthat it requires stronger assumptions on graph topology to guaranteeconvergence. This work establishes a theoretical foundation for understandingand improving GNN generalization in dynamic graph environments.</description>
      <author>example@mail.com (Huayi Tang, Yong Liu)</author>
      <guid isPermaLink="false">2509.06600v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Topological Regularization for Force Prediction in Active Particle Suspension with EGNN and Persistent Homology</title>
      <link>http://arxiv.org/abs/2509.06574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种多尺度框架，用于捕捉主动粒子在流体中的动力学行为，结合三种学习驱动工具协同工作。&lt;h4&gt;背景&lt;/h4&gt;主动粒子（小型自驱动粒子）在流体中移动时既能变形又能改变流体形态，这种动力学行为的模拟具有挑战性，因为它需要耦合精细尺度的流体动力学与大尺度的集体效应。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够准确预测主动粒子动力学行为的多尺度框架，解决流体动力学与集体效应耦合的复杂问题。&lt;h4&gt;方法&lt;/h4&gt;结合三种学习驱动工具：1)使用高分辨率格子玻尔兹曼快照作为输入；2)采用E(2)-等变图神经网络预测粒子间相互作用力；3)使用物理信息神经网络结合傅里叶特征映射和拓扑正则化更新力预测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够提供整体的高度数据驱动的完整力网络预测，同时强调物理基础和主动物质典型的多尺度结构。&lt;h4&gt;结论&lt;/h4&gt;该多尺度框架成功捕捉了主动粒子的动力学行为，解决了流体动力学与集体效应耦合的挑战性问题。&lt;h4&gt;翻译&lt;/h4&gt;捕捉主动粒子的动力学，即小型自驱动粒子在流体中既能变形又能改变流体形态，是一个艰巨的问题，因为它需要将精细尺度的流体动力学与大尺度的集体效应耦合起来。因此，我们提出了一个多尺度框架，结合了三种学习驱动工具在一个管道中协同学习。我们使用周期性盒子中流体速度和粒子应力的高分辨率格子玻尔兹曼快照作为学习管道的输入。第二步使用粒子的形态、位置和方向来预测它们之间的成对相互作用力，采用E(2)-等变图神经网络，必须满足平面对称性。然后，物理信息神经网络通过使用傅里叶特征映射和残差块，结合应力数据对这些局部估计进行进一步更新，同时使用持久同调引入的拓扑项进行正则化，以惩罚不切实际的缠结或虚假连接。这些阶段共同提供了一个整体的高度数据驱动的完整力网络预测，强调物理基础以及主动物质典型的多尺度结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Capturing the dynamics of active particles, i.e., small self-propelled agentsthat both deform and are deformed by a fluid in which they move is a formidableproblem as it requires coupling fine scale hydrodynamics with large scalecollective effects. So we present a multi-scale framework that combines thethree learning-driven tools to learn in concert within one pipeline. We usehigh-resolution Lattice Boltzmann snapshots of fluid velocity and particlestresses in a periodic box as input to the learning pipeline. the second steptakes the morphology and positions orientations of particles to predictpairwise interaction forces between them with a E(2)-equivariant graph neuralnetwork that necessarily respect flat symmetries. Then, a physics-informedneural network further updates these local estimates by summing over them witha stress data using Fourier feature mappings and residual blocks that isadditionally regularized with a topological term (introduced by persistenthomology) to penalize unrealistically tangled or spurious connections. Inconcert, these stages deliver an holistic highly-data driven full force networkprediction empathizing on the physical underpinnings together with emergingmulti-scale structure typical for active matter.</description>
      <author>example@mail.com (Sadra Saremi, Amirhossein Ahmadkhan Kordbacheh)</author>
      <guid isPermaLink="false">2509.06574v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks for Resource Allocation in Interference-limited Multi-Channel Wireless Networks with QoS Constraints</title>
      <link>http://arxiv.org/abs/2509.06395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络(GNN)的算法JCPGNN-M，用于解决无线通信系统中满足最小数据速率约束的挑战，同时满足服务质量要求。&lt;h4&gt;背景&lt;/h4&gt;无线通信系统满足最小数据速率要求是一个重大挑战，尤其是随着网络复杂性增加时。传统深度学习方法通过在损失函数中引入惩罚项和经验性调整超参数来处理这些约束，但这种方法没有理论收敛保证，且在实际场景中经常无法满足服务质量要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展且理论上有保证的解决方案，用于满足无线通信系统中的服务质量约束，同时降低计算复杂度并提高泛化能力。&lt;h4&gt;方法&lt;/h4&gt;基于WMMSE算法结构，扩展到具有QoS约束的多信道环境，得到增强的WMMSE(eWMMSE)算法；开发基于GNN的JCPGNN-M算法支持每个用户同时进行多信道分配；提出将GNN与基于拉格朗日的原始-对偶优化方法相结合的原则性框架，在拉格朗日框架内训练GNN以确保满足QoS约束并收敛到平稳点。&lt;h4&gt;主要发现&lt;/h4&gt;JCPGNN-M算法与eWMMSE性能相匹配，同时在推理速度、对更大网络泛化和在信道状态信息不完善情况下的鲁棒性方面具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来无线网络中的约束资源分配提供了一种可扩展且有理论依据的解决方案，克服了传统深度学习方法的理论局限性。&lt;h4&gt;翻译&lt;/h4&gt;在无线通信系统中满足最小数据速率要求是一个重大挑战，尤其是随着网络复杂性增长时。传统深度学习方法通常通过在损失函数中引入惩罚项和经验性调整超参数来处理这些约束。然而，这种启发式处理方法没有理论收敛保证，并且在实际场景中经常无法满足服务质量要求。基于WMMSE算法结构，我们首先将其扩展到具有QoS约束的多信道环境，得到了增强的WMMSE(eWMMSE)算法，当问题可行时，该算法可收敛到局部最优解。为了进一步降低计算复杂度并提高可扩展性，我们开发了一种基于GNN的算法JCPGNN-M，能够支持每个用户同时进行多信道分配。为了克服传统深度学习方法的局限性，我们提出了一个原则性框架，将GNN与基于拉格朗日的原始-对偶优化方法相结合。通过在拉格朗日框架内训练GNN，我们确保满足QoS约束并收敛到平稳点。广泛的模拟表明，JCPGNN-M与eWMMSE性能相匹配，同时在推理速度、对更大网络泛化和在信道状态信息不完善情况下的鲁棒性方面具有显著优势。这项工作为未来无线网络中的约束资源分配提供了一种可扩展且有理论依据的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meeting minimum data rate constraints is a significant challenge in wirelesscommunication systems, particularly as network complexity grows. Traditionaldeep learning approaches often address these constraints by incorporatingpenalty terms into the loss function and tuning hyperparameters empirically.However, this heuristic treatment offers no theoretical convergence guaranteesand frequently fails to satisfy QoS requirements in practical scenarios.Building upon the structure of the WMMSE algorithm, we first extend it to amulti-channel setting with QoS constraints, resulting in the enhanced WMMSE(eWMMSE) algorithm, which is provably convergent to a locally optimal solutionwhen the problem is feasible. To further reduce computational complexity andimprove scalability, we develop a GNN-based algorithm, JCPGNN-M, capable ofsupporting simultaneous multi-channel allocation per user. To overcome thelimitations of traditional deep learning methods, we propose a principledframework that integrates GNN with a Lagrangian-based primal-dual optimizationmethod. By training the GNN within the Lagrangian framework, we ensuresatisfaction of QoS constraints and convergence to a stationary point.Extensive simulations demonstrate that JCPGNN-M matches the performance ofeWMMSE while offering significant gains in inference speed, generalization tolarger networks, and robustness under imperfect channel state information. Thiswork presents a scalable and theoretically grounded solution for constrainedresource allocation in future wireless networks.</description>
      <author>example@mail.com (Lili Chen, Changyang She, Jingge Zhu, Jamie Evans)</author>
      <guid isPermaLink="false">2509.06395v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Spatio-Temporal Graph Neural Networks Approach for Predicting Silent Data Corruption inducing Circuit-Level Faults</title>
      <link>http://arxiv.org/abs/2509.06289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 9 figures, plan to submit to ACM TODAES&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种统一的时空图卷积网络（ST-GCN），用于快速、准确地预测大型顺序电路中的长周期故障影响概率（FIPs），支持风险评估。该方法将门级网表建模为时空图，使用专门的时空编码器高效预测多周期FIPs，在ISCAS-89基准测试中，将仿真时间减少了10倍以上，同时保持高精度。&lt;h4&gt;背景&lt;/h4&gt;静态数据错误（SDEs）源于初始缺陷和老化，会降低安全关键系统的可靠性。功能测试可以检测与SDE相关的故障，但模拟成本高昂。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、准确的方法来预测大型顺序电路中的长周期故障影响概率，以支持定量风险评估，并减少仿真时间。&lt;h4&gt;方法&lt;/h4&gt;提出了一种统一的时空图卷积网络（ST-GCN），将门级网表建模为时空图以捕获拓扑结构和信号时序，使用专门的时空编码器高效预测多周期FIPs。该方法接受可测试性指标或故障仿真的特征，允许在效率和精度之间进行权衡。&lt;h4&gt;主要发现&lt;/h4&gt;在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024）。通过预测的FIPs选择观测点可以改进对长周期、难以检测的故障的检测。该方法可扩展到SoC级测试策略优化，并适合下游电子设计自动化流程。&lt;h4&gt;结论&lt;/h4&gt;所提出的时空图卷积网络框架能够有效预测长周期故障影响概率，显著减少仿真时间，同时保持高精度，为安全关键系统的风险评估提供了高效工具。&lt;h4&gt;翻译&lt;/h4&gt;静态数据错误（SDEs）源于初始缺陷和老化，会降低安全关键系统的可靠性。功能测试可以检测与SDE相关的故障，但模拟成本高昂。我们提出了一种统一的时空图卷积网络（ST-GCN），用于快速、准确地预测大型顺序电路中的长周期故障影响概率（FIPs），支持定量风险评估。门级网表被建模为时空图以捕获拓扑结构和信号时序；专门的时空编码器高效地预测多周期FIPs。在ISCAS-89基准测试中，该方法将仿真时间减少了10倍以上，同时保持高精度（5周期预测的平均绝对误差为0.024）。该框架接受可测试性指标或故障仿真的特征，允许在效率和精度之间进行权衡。测试点选择研究表明，通过预测的FIPs选择观测点可以改进对长周期、难以检测的故障的检测。该方法可扩展到SoC级测试策略优化，并适合下游电子设计自动化流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Silent Data Errors (SDEs) from time-zero defects and aging degradesafety-critical systems. Functional testing detects SDE-related faults but isexpensive to simulate. We present a unified spatio-temporal graph convolutionalnetwork (ST-GCN) for fast, accurate prediction of long-cycle fault impactprobabilities (FIPs) in large sequential circuits, supporting quantitative riskassessment. Gate-level netlists are modeled as spatio-temporal graphs tocapture topology and signal timing; dedicated spatial and temporal encoderspredict multi-cycle FIPs efficiently. On ISCAS-89 benchmarks, the methodreduces simulation time by more than 10x while maintaining high accuracy (meanabsolute error 0.024 for 5-cycle predictions). The framework accepts featuresfrom testability metrics or fault simulation, allowing efficiency-accuracytrade-offs. A test-point selection study shows that choosing observation pointsby predicted FIPs improves detection of long-cycle, hard-to-detect faults. Theapproach scales to SoC-level test strategy optimization and fits downstreamelectronic design automation flows.</description>
      <author>example@mail.com (Shaoqi Wei, Senling Wang, Hiroshi Kai, Yoshinobu Higami, Ruijun Ma, Tianming Ni, Xiaoqing Wen, Hiroshi Takahashi)</author>
      <guid isPermaLink="false">2509.06289v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RecMind: LLM-Enhanced Graph Neural Networks for Personalized Consumer Recommendations</title>
      <link>http://arxiv.org/abs/2509.06286v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RecMind是一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器，通过文本条件化和图学习两种方式生成嵌入，并通过门控机制融合，在推荐任务上取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;个性化技术在消费技术、流媒体、购物、可穿戴设备和语音等领域是核心能力，但仍面临交互稀疏、内容快速更新和异构文本信号等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出RecMind，一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器，以解决个性化推荐中的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用配备轻量适配器的冻结LLM从标题、属性和评论中生成文本条件化的用户/物品嵌入；使用LightGCN主干从用户-物品图中学习协作嵌入；通过对称对比目标对齐两种视图；通过层内门控融合它们，使语言在冷启动/长尾场景中占主导，图结构在其他地方稳定排序。&lt;h4&gt;主要发现&lt;/h4&gt;在Yelp和Amazon-Electronics数据集上，RecMind在所有八个报告指标上取得了最佳结果，相对于强大的基线，相对改进最高达+4.53%（Recall@40）和+4.01%（NDCG@40）；消融研究证实了跨视图对齐的必要性以及门控相比晚期融合和仅LLM变体的优势。&lt;h4&gt;结论&lt;/h4&gt;RecMind成功地将语言模型和图神经网络结合，解决了个性化推荐中的稀疏交互、内容快速更新和异构文本信号等挑战，在多个指标上超越了现有方法。&lt;h4&gt;翻译&lt;/h4&gt;个性化是消费技术、流媒体、购物、可穿戴设备和语音等领域的核心能力，但仍受到交互稀疏、内容快速周转和异构文本信号的挑战。我们提出了RecMind，一种增强型大语言图推荐系统，将语言模型视为偏好先验而非单一排序器。配备轻量适配器的冻结LLM从标题、属性和评论中生成文本条件化的用户/物品嵌入；LightGCN主干从用户-物品图中学习协作嵌入。我们通过对称对比目标对齐两种视图，并通过层内门控融合它们，使语言在冷启动/长尾场景中占主导，图结构在其他地方稳定排序。在Yelp和Amazon-Electronics上，RecMind在所有八个报告指标上取得了最佳结果，相对于强大的基线，相对改进最高达+4.53%（Recall@40）和+4.01%（NDCG@40）。消融研究证实了跨视图对齐的必要性以及门控相比晚期融合和仅LLM变体的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalization is a core capability across consumer technologies, streaming,shopping, wearables, and voice, yet it remains challenged by sparseinteractions, fast content churn, and heterogeneous textual signals. We presentRecMind, an LLM-enhanced graph recommender that treats the language model as apreference prior rather than a monolithic ranker. A frozen LLM equipped withlightweight adapters produces text-conditioned user/item embeddings fromtitles, attributes, and reviews; a LightGCN backbone learns collaborativeembeddings from the user-item graph. We align the two views with a symmetriccontrastive objective and fuse them via intra-layer gating, allowing languageto dominate in cold/long-tail regimes and graph structure to stabilize rankingselsewhere. On Yelp and Amazon-Electronics, RecMind attains the best results onall eight reported metrics, with relative improvements up to +4.53\%(Recall@40) and +4.01\% (NDCG@40) over strong baselines. Ablations confirm boththe necessity of cross-view alignment and the advantage of gating over latefusion and LLM-only variants.</description>
      <author>example@mail.com (Chang Xue, Youwei Lu, Chen Yang, Jinming Xing)</author>
      <guid isPermaLink="false">2509.06286v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Time-Dependent PDE Surrogates: Graph Neural Simulators vs Neural Operators</title>
      <link>http://arxiv.org/abs/2509.06154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages including references. Supplementary Information provided&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种图神经模拟器(GNS)方法，结合消息传递图神经网络和显式数值时间步进方案，通过建模瞬时时间导数来学习时间依赖的偏微分方程(PDE)解，显著提高了数据效率和预测精度。&lt;h4&gt;背景&lt;/h4&gt;神经算子(NOs)可以近似无限维函数空间之间的映射，但需要大型数据集且在训练数据稀疏时表现不佳。许多神经算子公式没有明确编码物理演化的因果性和时间局部结构。自回归模型虽然通过预测下一个时间步保持因果性，但存在误差快速累积的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效学习时间依赖PDE解的方法，解决神经算子在数据稀疏情况下的局限性，同时减少自回归模型中的误差累积问题。&lt;h4&gt;方法&lt;/h4&gt;使用图神经模拟器(GNS)，这是一种消息传递图神经网络框架，结合显式数值时间步进方案，通过建模瞬时时间导数来学习PDE解。引入了PCA+KMeans轨迹选择策略以提高低数据性能。&lt;h4&gt;主要发现&lt;/h4&gt;GNS在三个典型PDE系统(2D Burgers标量方程、2D耦合Burgers矢量方程和2D Allen-Cahn方程)上表现优异。仅使用30个训练样本(可用数据的3%)，GNS在所有系统上都实现了低于1%的相对L2误差。与基线模型相比，GNS显著减少了长时间范围内的误差累积：相对于FNO AR减少82.48%的误差，相对于DON AR减少99.86%的误差。&lt;h4&gt;结论&lt;/h4&gt;将基于图的局部归纳偏差与传统时间积分器相结合，可以为时间依赖的PDEs生成准确、物理一致且可扩展的代理模型，显著提高了数据效率和预测精度。&lt;h4&gt;翻译&lt;/h4&gt;神经算子(NOs)近似无限维函数空间之间的映射，但需要大型数据集且在训练数据稀疏时表现不佳。许多神经算子公式没有明确编码物理演化的因果性和时间局部结构。虽然自回归模型通过预测下一个时间步来保持因果性，但存在误差快速累积的问题。我们采用图神经模拟器(GNS)——一种消息传递图神经网络框架——结合显式数值时间步进方案，构建精确的前向模型，通过建模瞬时时间导数来学习PDE解。我们在三个典型的PDE系统上评估我们的框架：(1) 2D Burgers标量方程，(2) 2D耦合Burgers矢量方程，(3) 2D Allen-Cahn方程。严格的评估表明，GNS显著提高了数据效率，与DeepONet和FNO等神经算子基线相比，使用更少的训练轨迹实现了更高的泛化精度。仅使用30个训练样本(可用数据的3%)，GNS在所有三个PDE系统上都实现了低于1%的相对L2误差。在长时间范围内，GNS显著减少了误差累积：平均而言，GNS相对于FNO AR减少了82.48%的误差，相对于DON AR减少了99.86%的误差。我们引入了PCA+KMeans轨迹选择策略，提高了低数据性能。结果表明，将基于图的局部归纳偏差与传统时间积分器相结合，可以为时间依赖的PDEs生成准确、物理一致且可扩展的代理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural operators (NOs) approximate mappings between infinite-dimensionalfunction spaces but require large datasets and struggle with scarce trainingdata. Many NO formulations don't explicitly encode causal, local-in-timestructure of physical evolution. While autoregressive models preserve causalityby predicting next time-steps, they suffer from rapid error accumulation. Weemploy Graph Neural Simulators (GNS) - a message-passing graph neural networkframework - with explicit numerical time-stepping schemes to construct accurateforward models that learn PDE solutions by modeling instantaneous timederivatives. We evaluate our framework on three canonical PDE systems: (1) 2DBurgers' scalar equation, (2) 2D coupled Burgers' vector equation, and (3) 2DAllen-Cahn equation. Rigorous evaluations demonstrate GNS significantlyimproves data efficiency, achieving higher generalization accuracy withsubstantially fewer training trajectories compared to neural operator baselineslike DeepONet and FNO. GNS consistently achieves under 1% relative L2 errorswith only 30 training samples out of 1000 (3% of available data) across allthree PDE systems. It substantially reduces error accumulation over extendedtemporal horizons: averaged across all cases, GNS reduces autoregressive errorby 82.48% relative to FNO AR and 99.86% relative to DON AR. We introduce aPCA+KMeans trajectory selection strategy enhancing low-data performance.Results indicate combining graph-based local inductive biases with conventionaltime integrators yields accurate, physically consistent, and scalable surrogatemodels for time-dependent PDEs.</description>
      <author>example@mail.com (Dibyajyoti Nayak, Somdatta Goswami)</author>
      <guid isPermaLink="false">2509.06154v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Stage Graph Neural Networks for Data-Driven Prediction of Natural Convection in Enclosed Cavities</title>
      <link>http://arxiv.org/abs/2509.06041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型多阶段图神经网络架构，用于改进封闭空腔中浮力驱动热传递的模拟，解决了传统GNN在高分辨率图结构中捕获长程依赖关系的困难。&lt;h4&gt;背景&lt;/h4&gt;封闭空腔中的浮力驱动热传递是热设计的典型测试平台，高精度CFD建模虽能提供精确温度场解，但依赖专家设计的物理模型、精细网格和密集计算，限制了快速迭代。数据驱动建模，特别是图神经网络，为从模拟数据中学习热流体行为提供了新选择。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN难以在高分辨率图结构中捕获长程依赖关系的问题，开发一种能够跨多个空间尺度建模全局到局部相互作用的架构，提高热传递模拟的预测准确性和训练效率。&lt;h4&gt;方法&lt;/h4&gt;提出利用分层池化和上池化操作的多阶段GNN架构，在新开发的CFD数据集上评估该模型，该数据集模拟了不同宽高比矩形空腔中的自然对流，条件为底部壁面等温热，顶部壁面等温冷，两个垂直壁面绝热。&lt;h4&gt;主要发现&lt;/h4&gt;与传统最先进的GNN基线相比，所提出的模型实现了更高的预测准确性，提高了训练效率，并减少了长期误差累积。&lt;h4&gt;结论&lt;/h4&gt;多阶段GNN方法在基于网格的流体动力学模拟中模拟复杂热传递具有显著潜力，为热设计提供了更高效的计算工具。&lt;h4&gt;翻译&lt;/h4&gt;封闭空腔中的浮力驱动热传递作为热设计的典型测试平台，高精度CFD建模能提供精确的温度场解，但其对专家设计的物理模型、精细网格和密集计算的依赖限制了快速迭代。数据驱动建模的最新发展，特别是图神经网络，为从模拟数据中直接学习热流体行为提供了新选择，特别是在不规则网格结构上。然而，传统GNN往往难以在高分辨率图结构中捕获长程依赖关系。为克服这一局限，我们提出了一种新型多阶段GNN架构，利用分层池化和上池化操作逐步跨多个空间尺度建模全局到局部的相互作用。我们在新开发的CFD数据集上评估了所提出的模型，该数据集模拟了不同宽高比矩形空腔中的自然对流，其中底部壁面等温热，顶部壁面等温冷，两个垂直壁面绝热。实验结果表明，与最先进的GNN基线相比，所提出的模型实现了更高的预测准确性，提高了训练效率，并减少了长期误差累积。这些发现强调了所提出的多阶段GNN方法在基于网格的流体动力学模拟中模拟复杂热传递的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Buoyancy-driven heat transfer in closed cavities serves as a canonicaltestbed for thermal design High-fidelity CFD modelling yields accurate thermalfield solutions, yet its reliance on expert-crafted physics models, finemeshes, and intensive computation limits rapid iteration. Recent developmentsin data-driven modeling, especially Graph Neural Networks (GNNs), offer newalternatives for learning thermal-fluid behavior directly from simulation data,particularly on irregular mesh structures. However, conventional GNNs oftenstruggle to capture long-range dependencies in high-resolution graphstructures. To overcome this limitation, we propose a novel multi-stage GNNarchitecture that leverages hierarchical pooling and unpooling operations toprogressively model global-to-local interactions across multiple spatialscales. We evaluate the proposed model on our newly developed CFD datasetsimulating natural convection within a rectangular cavities with varying aspectratios where the bottom wall is isothermal hot, the top wall is isothermalcold, and the two vertical walls are adiabatic. Experimental resultsdemonstrate that the proposed model achieves higher predictive accuracy,improved training efficiency, and reduced long-term error accumulation comparedto state-of-the-art (SOTA) GNN baselines. These findings underscore thepotential of the proposed multi-stage GNN approach for modeling complex heattransfer in mesh-based fluid dynamics simulations.</description>
      <author>example@mail.com (Mohammad Ahangarkiasari, Hassan Pouraria)</author>
      <guid isPermaLink="false">2509.06041v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: Graph-Guided Repository-Aware Code Completion through Hierarchical Code Fusion</title>
      <link>http://arxiv.org/abs/2509.05980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRACE是一种创新的检索增强生成方法，通过构建多级别、多语义的代码图和混合图检索器，解决了LLMs在存储库级代码任务中面临的上下文限制和结构依赖问题，显著提升了代码检索和生成的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在局部代码补全方面表现出色，但在处理存储库级别的任务时存在困难，主要原因是上下文窗口有限，以及代码库之间复杂的语义和结构依赖关系。虽然检索增强生成通过检索相关代码片段缓解了上下文稀缺问题，但当前方法存在显著限制。&lt;h4&gt;目的&lt;/h4&gt;解决当前代码检索方法过度依赖文本相似性而忽略结构关系，以及通过简单连接代码片段丢失关键结构信息的问题，提高LLMs在存储库级代码任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;GRACE构建了一个多级别、多语义的代码图，统一了文件结构、抽象语法树、函数调用图、类层次结构和数据流图，以捕获静态和动态代码语义。对于检索，GRACE采用混合图检索器，结合基于图神经网络的结构相似性和文本检索，并通过基于图注意力网络的重排序器来优化拓扑相关的子图。GRACE还引入了结构融合机制，将检索到的子图与本地代码上下文合并，并保留函数调用和继承等关键依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;在公共存储库级基准上的大量实验表明，GRACE在所有指标上都显著优于最先进的方法。使用DeepSeek-V3作为骨干LLM，GRACE在每个数据集上都比最强的基于图的RAG基线高出8.19%的EM和7.51%的ES点。&lt;h4&gt;结论&lt;/h4&gt;GRACE通过综合考虑代码的文本内容和结构关系，有效解决了LLMs在存储库级代码任务中的局限性，显著提升了代码检索和生成的性能，为代码理解与生成领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在局部代码补全方面表现出色，但在处理存储库级任务时由于上下文窗口有限以及代码库间复杂的语义和结构依赖关系而表现不佳。虽然检索增强生成通过检索相关代码片段缓解了上下文稀缺问题，但当前方法存在显著限制。它们过度依赖文本相似性进行检索，忽略了调用链和继承层次结构等结构关系，并通过简单地将检索到的代码片段连接成文本序列作为LLM输入，丢失了关键的结构信息。为解决这些不足，GRACE构建了一个多级别、多语义的代码图，统一了文件结构、抽象语法树、函数调用图、类层次结构和数据流图，以捕获静态和动态代码语义。对于检索，GRACE采用混合图检索器，结合基于图神经网络的结构相似性和文本检索，并通过基于图注意力网络的重排序器来优先考虑拓扑相关的子图。为增强上下文，GRACE引入了结构融合机制，将检索到的子图与本地代码上下文合并，并保留函数调用和继承等关键依赖关系。在公共存储库级基准上的大量实验表明，GRACE在所有指标上都显著优于最先进的方法。使用DeepSeek-V3作为骨干LLM，GRACE在每个数据集上都比最强的基于图的RAG基线高出8.19%的EM和7.51%的ES点。代码可在https://anonymous.4open.science/r/grace_icse-C3D5获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LLMs excel in localized code completion but struggle with repository-leveltasks due to limited context windows and complex semantic and structuraldependencies across codebases. While Retrieval-Augmented Generation (RAG)mitigates context scarcity by retrieving relevant code snippets, currentapproaches face significant limitations. They overly rely on textual similarityfor retrieval, neglecting structural relationships such as call chains andinheritance hierarchies, and lose critical structural information by naivelyconcatenating retrieved snippets into text sequences for LLM input. To addressthese shortcomings, GRACE constructs a multi-level, multi-semantic code graphthat unifies file structures, abstract syntax trees, function call graphs,class hierarchies, and data flow graphs to capture both static and dynamic codesemantics. For retrieval, GRACE employs a Hybrid Graph Retriever thatintegrates graph neural network-based structural similarity with textualretrieval, refined by a graph attention network-based re-ranker to prioritizetopologically relevant subgraphs. To enhance context, GRACE introduces astructural fusion mechanism that merges retrieved subgraphs with the local codecontext and preserves essential dependencies like function calls andinheritance. Extensive experiments on public repository-level benchmarksdemonstrate that GRACE significantly outperforms state-of-the-art methodsacross all metrics. Using DeepSeek-V3 as the backbone LLM, GRACE surpasses thestrongest graph-based RAG baselines by 8.19% EM and 7.51% ES points on everydataset. The code is available athttps://anonymous.4open.science/r/grace_icse-C3D5.</description>
      <author>example@mail.com (Xingliang Wang, Baoyi Wang, Chen Zhi, Junxiao Han, Xinkui Zhao, Jianwei Yin, Shuiguang Deng)</author>
      <guid isPermaLink="false">2509.05980v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification</title>
      <link>http://arxiv.org/abs/2509.05943v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submit to IEEE Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DRDCAE-STGNN的新型端到端深度学习框架，用于增强运动想象脑机接口的特征学习和分类，在多个数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;运动想象脑机接口在辅助技术和神经康复方面有很大潜力，但由于其非平稳特性和低信噪比，精确高效解码运动想象仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效提升运动想象特征学习和分类性能的深度学习框架。&lt;h4&gt;方法&lt;/h4&gt;提出判别性残差密集卷积自编码器与时空图神经网络(DRDCAE-STGNN)框架，其中DRDCAE模块利用残差密集连接通过联合重建和分类学习判别性潜在表示，STGNN模块通过可学习图邻接矩阵捕获动态空间依赖性，并使用双向长短期记忆建模时间动态。&lt;h4&gt;主要发现&lt;/h4&gt;在BCI竞赛IV 2a、2b和PhysioNet数据集上分别实现了95.42%、97.51%和90.15%的平均准确率，消融研究确认了各组件的贡献，可解释性分析揭示了神经生理学上有意义的连接模式，模型每样本推理时间为0.32毫秒。&lt;h4&gt;结论&lt;/h4&gt;该方法为MI-EEG解码提供了稳健、准确和可解释的解决方案，在受试者和任务间具有强大的泛化能力，满足潜在实时BCI应用的要求。&lt;h4&gt;翻译&lt;/h4&gt;运动想象脑机接口在辅助技术和神经康复方面具有巨大潜力。然而，由于其非平稳特性和低信噪比，运动想象的精确高效解码仍然具有挑战性。本文引入了一种新颖的端到端深度学习框架——判别性残差密集卷积自编码器与时空图神经网络，以增强运动想象特征学习和分类。具体而言，判别性残差密集卷积自编码器模块利用残差密集连接通过联合重建和分类学习判别性潜在表示，而时空图神经网络模块通过可学习的图邻接矩阵捕获动态空间依赖性，并使用双向长短期记忆建模时间动态。在BCI竞赛IV 2a、2b和PhysioNet数据集上的广泛评估展示了最先进的性能，平均准确率分别为95.42%、97.51%和90.15%。消融研究确认了每个组件的贡献，可解释性分析揭示了神经生理学上有意义的连接模式。此外，尽管模型复杂，但它保持了可行的参数计数和每样本0.32毫秒的推理时间。这些结果表明，我们的方法为运动想象脑电图解码提供了稳健、准确和可解释的解决方案，在受试者和任务间具有强大的泛化能力，并满足潜在实时脑机接口应用的要求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motor imagery (MI) based brain-computer interfaces (BCIs) hold significantpotential for assistive technologies and neurorehabilitation. However, theprecise and efficient decoding of MI remains challenging due to theirnon-stationary nature and low signal-to-noise ratio. This paper introduces anovel end-to-end deep learning framework of Discriminative Residual DenseConvolutional Autoencoder with Spatio-Temporal Graph Neural Network(DRDCAE-STGNN) to enhance the MI feature learning and classification.Specifically, the DRDCAE module leverages residual-dense connections to learndiscriminative latent representations through joint reconstruction andclassifica-tion, while the STGNN module captures dynamic spatial dependenciesvia a learnable graph adjacency matrix and models temporal dynamics usingbidirectional long short-term memory (LSTM). Extensive evaluations on BCICompetition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-artperformance, with average accuracies of 95.42%, 97.51%, and 90.15%,respectively. Ablation studies confirm the contribution of each component, andinterpreta-bility analysis reveals neurophysiologically meaningful connectivitypatterns. Moreover, despite its complexity, the model maintains a feasibleparameter count and an inference time of 0.32 ms per sample. These resultsindicate that our method offers a robust, accurate, and interpretable solutionfor MI-EEG decoding, with strong generalizability across subjects and tasks andmeeting the requirements for potential real-time BCI applications.</description>
      <author>example@mail.com (Yi Wang, Haodong Zhang, Hongqi Li)</author>
      <guid isPermaLink="false">2509.05943v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling the critical factors in crystal structure graph representation: a comparative analysis using streamlined MLPSets frameworks</title>
      <link>http://arxiv.org/abs/2509.05712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;数据驱动的图表示方法在材料科学和化学领域表现优异，特别是结合电子结构生成模型和多任务学习的方法，能够更全面地表示材料的复杂特性并达到接近DFT计算的精度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在材料科学和化学领域迅速发展，其性能依赖于对晶体或分子结构在五个维度上的全面表示：元素信息、几何拓扑、电子相互作用、对称性和长程相互作用。然而，现有模型在表示电子相互作用、对称性和长程信息方面仍存在局限性。&lt;h4&gt;目的&lt;/h4&gt;比较基于物理的位点特征计算器与数据驱动的图表示策略，探索更有效的材料结构表示方法。&lt;h4&gt;方法&lt;/h4&gt;比较物理方法和数据驱动方法；数据驱动方法结合电子结构生成模型（如变分自编码器VAEs，用于压缩Kohn-Sham波函数）；利用多任务学习；将CHGNet-V1/V2策略集成到DenseGNN模型中；采用预训练和微调策略。&lt;h4&gt;主要发现&lt;/h4&gt;数据驱动方法在表示完整性、收敛速度和外推能力方面表现更优越；CHGNet-V1/V2策略集成到DenseGNN模型后，在35个数据集上超越了最先进模型，预测精度接近DFT计算；预训练和微调策略显著降低了复杂无序材料带隙的预测误差。&lt;h4&gt;结论&lt;/h4&gt;数据驱动的图表示在加速材料发现方面具有优越性和潜力。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在材料科学和化学领域迅速发展，其性能严重依赖于对晶体或分子结构在五个维度上的全面表示：元素信息、几何拓扑、电子相互作用、对称性和长程相互作用。现有模型在表示电子相互作用、对称性和长程信息方面仍存在局限性。本研究比较了基于物理的位点特征计算器与数据驱动的图表示策略。我们发现，后者通过结合电子结构生成模型（如压缩Kohn-Sham波函数的变分自编码器VAEs）和利用多任务学习，在表示完整性、收敛速度和外推能力方面实现了更优越的性能。值得注意的是，当CHGNet-V1/V2策略集成到DenseGNN模型中时，在Matbench和JARVIS-DFT的35个数据集上显著优于最先进模型，产生了接近DFT计算精度的预测结果。此外，采用预训练和微调策略显著降低了复杂无序材料带隙的预测误差，证明了数据驱动图表示在加速材料发现方面的优越性和潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决晶体结构图表示中的关键因素分析问题，特别是如何有效表示晶体结构的五个关键维度：元素信息、几何拓扑信息、电子相互作用信息、对称性信息和长程相互作用信息。这个问题重要是因为图神经网络已成为材料科学和化学领域预测材料特性的关键工具，但现有模型在电子相互作用表示方面存在不足，缺乏系统性比较研究，限制了材料发现和性质预测的准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到晶体结构表示需要涵盖多个维度，发现现有方法在电子相互作用表示上有不足。他们设计了两种晶体结构图表示策略进行比较：基于物理的位点特征计算器和数据驱动的结构图表示策略。借鉴了现有工作如AGNIFingerprints、OPSiteFingerprint、CrystalNNFingerprint等物理方法，以及MEGNet、M3GNet等数据驱动模型，并创新性地整合了变分自编码器(VAE)来处理电子结构信息，开发出CHGNet系列模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是数据驱动的晶体结构图表示策略比基于物理的方法更能全面表示晶体结构，特别是通过VAE压缩Kohn-Sham波函数可以有效表示电子结构。实现流程包括：1)设计两种表示策略并进行比较；2)引入简化的MLPSets框架以最小化GNN操作干扰；3)在多个基准数据集上评估性能；4)将最优的CHGNet-V1策略应用于DenseGNN模型并进行实际应用测试。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统比较物理和数据驱动的两种表示策略；2)开发出CHGNet-V1/V2数据驱动模型，整合VAE处理电子结构；3)提出简化的MLPSets框架便于公平比较；4)将CHGNet-V1应用于DenseGNN显著提升性能；5)证明数据驱动方法在外推任务上的优势。相比之前工作，本文更全面地考虑了电子相互作用信息，并通过系统比较揭示了数据驱动方法的优势，特别是在小数据集和外推场景下表现出色。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过系统比较和优化晶体结构图表示策略，开发出基于数据驱动的CHGNet-V1表示方法，显著提升了材料性质预测模型的性能和泛化能力，特别是在外推任务和小数据集场景下表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks have rapidly advanced in materials science andchemistry,with their performance critically dependent on comprehensiverepresentations of crystal or molecular structures across five dimensions:elemental information, geometric topology, electronic interactions, symmetry,and long-range interactions. Existing models still exhibit limitations inrepresenting electronic interactions, symmetry, and long-range information.This study compares physics-based site feature calculators with data-drivengraph representation strategies. We find that the latter achieve superiorperformance in representation completeness, convergence speed, andextrapolation capability by incorporating electronic structure generationmodels-such as variational autoencoders (VAEs) that compress Kohn-Sham wavefunctions and leveraging multi-task learning. Notably, the CHGNet-V1/V2strategies, when integrated into the DenseGNN model,significantly outperformstate-of-the-art models across 35 datasets from Matbench and JARVIS-DFT,yielding predictions with accuracy close to that of DFT calculations.Furthermore, applying a pre-training and fine-tuning strategy substantiallyreduces the prediction error for band gaps of complex disordered materials,demonstrating the superiority and potential of data-driven graphrepresentations in accelerating materials discovery.</description>
      <author>example@mail.com (Hongwei Du, Hong Wang)</author>
      <guid isPermaLink="false">2509.05712v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>GraMFedDHAR: Graph Based Multimodal Differentially Private Federated HAR</title>
      <link>http://arxiv.org/abs/2509.05671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraMFedDHAR的基于图的多模态联邦学习框架，用于人类活动识别任务。该框架将不同传感器数据建模为模态特定图，通过残差图卷积神经网络处理，并通过基于注意力的权重融合。实验结果表明，在差分隐私约束下，所提出的MultiModalGCN模型显著优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;使用多模态传感器数据进行人类活动识别面临测量噪声或不完整、标记样本稀缺以及隐私问题等挑战。传统集中式深度学习方法受限于基础设施可用性、网络延迟和数据共享限制。联邦学习虽通过本地训练解决了隐私问题，但仍需处理异构多模态数据和差分隐私要求带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的多模态联邦学习框架GraMFedDHAR，用于解决人类活动识别任务中的隐私保护和多模态数据融合问题。&lt;h4&gt;方法&lt;/h4&gt;将不同传感器流（如压力垫、深度相机和多个加速度计）建模为模态特定图，通过残差图卷积神经网络处理，使用基于注意力的权重而非简单连接来融合多模态数据，并在联邦聚合过程中应用差分隐私保护数据安全。&lt;h4&gt;主要发现&lt;/h4&gt;MultiModalGCN模型在非差分隐私设置下比基线模型高约2%的准确率；在差分隐私约束下，性能差距达7%到13%；基于图的建模在多模态学习中表现出鲁棒性，图神经网络更能抵抗差分隐私噪声带来的性能下降。&lt;h4&gt;结论&lt;/h4&gt;基于图的建模方法在多模态学习中表现出鲁棒性，特别是在差分隐私约束下。图神经网络比传统方法更能抵抗差分隐私噪声带来的性能下降，为隐私保护下的人类活动识别提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用多模态传感器数据进行人类活动识别(HAR)仍然具有挑战性，因为测量存在噪声或不完整，标记样本稀缺，以及存在隐私问题。传统的集中式深度学习方法通常受限于基础设施可用性、网络延迟和数据共享限制。虽然联邦学习(FL)通过本地训练模型并仅共享模型参数解决了隐私问题，但仍需处理使用异构多模态数据和差分隐私要求所带来的问题。在本文中，提出了一种基于图的多模态联邦学习框架GraMFedDHAR用于HAR任务。将压力垫、深度相机和多个加速度计等不同传感器流建模为模态特定图，通过残差图卷积神经网络(GCN)处理，并通过基于注意力的权重而非简单连接进行融合。融合后的嵌入使活动分类更加鲁棒，同时差分隐私在联邦聚合过程中保护数据。实验结果表明，所提出的MultiModalGCN模型在集中式和联邦范式的非差分隐私设置下，比基线MultiModalFFN模型高高达2%的准确率。更重要的是，在差分隐私约束下观察到显著改进：MultiModalGCN模型持续超越MultiModalFFN模型，性能差距根据隐私预算和设置不同，在7%到13%之间。这些结果突显了基于图的建模在多模态学习中的鲁棒性，其中图神经网络(GNNs)被证明更能抵抗差分隐私噪声引入的性能下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) using multimodal sensor data remainschallenging due to noisy or incomplete measurements, scarcity of labeledexamples, and privacy concerns. Traditional centralized deep learningapproaches are often constrained by infrastructure availability, networklatency, and data sharing restrictions. While federated learning (FL) addressesprivacy by training models locally and sharing only model parameters, it stillhas to tackle issues arising from the use of heterogeneous multimodal data anddifferential privacy requirements. In this article, a Graph-based MultimodalFederated Learning framework, GraMFedDHAR, is proposed for HAR tasks. Diversesensor streams such as a pressure mat, depth camera, and multipleaccelerometers are modeled as modality-specific graphs, processed throughresidual Graph Convolutional Neural Networks (GCNs), and fused viaattention-based weighting rather than simple concatenation. The fusedembeddings enable robust activity classification, while differential privacysafeguards data during federated aggregation. Experimental results show thatthe proposed MultiModalGCN model outperforms the baseline MultiModalFFN, withup to 2 percent higher accuracy in non-DP settings in both centralized andfederated paradigms. More importantly, significant improvements are observedunder differential privacy constraints: MultiModalGCN consistently surpassesMultiModalFFN, with performance gaps ranging from 7 to 13 percent depending onthe privacy budget and setting. These results highlight the robustness ofgraph-based modeling in multimodal learning, where GNNs prove more resilient tothe performance degradation introduced by DP noise.</description>
      <author>example@mail.com (Labani Halder, Tanmay Sen, Sarbani Palit)</author>
      <guid isPermaLink="false">2509.05671v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>TreeGPT: A Novel Hybrid Architecture for Abstract Syntax Tree Processing with Global Parent-Child Aggregation</title>
      <link>http://arxiv.org/abs/2509.05550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://github.com/lizixi-0x2F/TreeGPT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了TreeGPT，一种创新的神经网络架构，结合了transformer注意力机制和全局父子聚合，用于处理抽象语法树。在视觉推理任务中实现了96%的准确率，显著优于其他模型，同时仅使用1.5M参数。&lt;h4&gt;背景&lt;/h4&gt;传统神经程序合成方法主要依赖顺序处理或图神经网络，在处理抽象语法树时存在局限性。ARC Prize 2025是一个需要抽象模式识别和规则推理的视觉推理基准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效处理抽象语法树的新神经网络架构，提高神经程序合成任务中的性能，特别是在视觉推理任务中。&lt;h4&gt;方法&lt;/h4&gt;TreeGPT采用混合设计，结合自注意力机制捕获局部依赖，以及TreeFeed-Forward Network通过迭代消息传递建模树结构。核心是全局父子聚合机制，使节点能通过多次迭代聚合整个树的信息。还包括门控聚合、残差连接和双向传播等增强功能。&lt;h4&gt;主要发现&lt;/h4&gt;在ARC Prize 2025数据集上，TreeGPT实现了96%的准确率，显著优于transformer基线模型(1.3%)、Grok-4(15.9%)和SOAR(52%)。消融研究表明边投影是最关键组件，边投影和门控组合实现最佳性能。&lt;h4&gt;结论&lt;/h4&gt;TreeGPT通过结合transformer注意力机制和全局父子聚合，为处理抽象语法树提供了有效方法，在视觉推理任务中取得显著成果，同时保持模型高效性。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了TreeGPT，一种新颖的神经网络架构，它结合了基于transformer的注意力机制和全局父子聚合，用于处理神经程序合成任务中的抽象语法树。与仅依赖顺序处理或图神经网络的传统方法不同，TreeGPT采用混合设计，利用自注意力机制捕获局部依赖关系，并通过专门的TreeFeed-Forward Network通过迭代消息传递建模层次化树结构。核心创新在于全局父子聚合机制，使每个节点能够通过多次迭代逐步聚合整个树结构的信息。我们的架构集成了门控聚合、残差连接和双向传播等增强功能。在ARC Prize 2025数据集上的评估表明，TreeGPT实现了96%的准确率，显著优于其他基准模型，同时仅使用1.5M参数。消融研究表明边投影是最关键组件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce TreeGPT, a novel neural architecture that combinestransformer-based attention mechanisms with global parent-child aggregation forprocessing Abstract Syntax Trees (ASTs) in neural program synthesis tasks.Unlike traditional approaches that rely solely on sequential processing orgraph neural networks, TreeGPT employs a hybrid design that leverages bothself-attention for capturing local dependencies and a specialized TreeFeed-Forward Network (TreeFFN) for modeling hierarchical tree structuresthrough iterative message passing.  The core innovation lies in our Global Parent-Child Aggregation mechanism,formalized as: $$h_i^{(t+1)} = \sigma \Big( h_i^{(0)} + W_{pc} \sum_{(p,c) \inE_i} f(h_p^{(t)}, h_c^{(t)}) + b \Big)$$ where $h_i^{(t)}$ represents thehidden state of node $i$ at iteration $t$, $E_i$ denotes all parent-child edgesinvolving node $i$, and $f(h_p, h_c)$ is an edge aggregation function. Thisformulation enables each node to progressively aggregate information from theentire tree structure through $T$ iterations.  Our architecture integrates optional enhancements including gated aggregationwith learnable edge weights, residual connections for gradient stability, andbidirectional propagation for capturing both bottom-up and top-downdependencies. We evaluate TreeGPT on the ARC Prize 2025 dataset, a challengingvisual reasoning benchmark requiring abstract pattern recognition and ruleinference. Experimental results demonstrate that TreeGPT achieves 96\%accuracy, significantly outperforming transformer baselines (1.3\%),large-scale models like Grok-4 (15.9\%), and specialized program synthesismethods like SOAR (52\%) while using only 1.5M parameters. Our comprehensiveablation study reveals that edge projection is the most critical component,with the combination of edge projection and gating achieving optimalperformance.</description>
      <author>example@mail.com (Zixi Li)</author>
      <guid isPermaLink="false">2509.05550v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Link Sparsification for Scalable Scheduling Using Graph Neural Networks (Journal Version)</title>
      <link>http://arxiv.org/abs/2509.05447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 18 figures, accepted to IEEE Transactions on Wireless  Communications. This is the extended journal version of the conference paper  arXiv:2203.14339 (Z. Zhao, A. Swami and S. Segarra, "Distributed Link  Sparsification for Scalable Scheduling using Graph Neural Networks," IEEE  ICASSP 2022, pp. 5308-5312, doi: 10.1109/ICASSP43922.2022.9747437 )&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络(GNN)的分布式链路稀疏化方案，用于减少无线网络中的调度开销，同时保持网络容量。&lt;h4&gt;背景&lt;/h4&gt;在具有密集连接特性的无线网络中，分布式链路调度算法产生的显著信令开销会加剧拥塞、能源消耗和无线电足迹扩展等问题。&lt;h4&gt;目的&lt;/h4&gt;缓解无线网络中的信令开销问题，提出一种分布式链路稀疏化方案，减少对延迟容忍型流量的调度开销，同时保持网络容量。&lt;h4&gt;方法&lt;/h4&gt;训练一个图神经网络模块，根据流量统计和网络拓扑为单个链路调整竞争阈值，使不太可能成功的链路退出调度竞争。通过离线约束无监督学习算法平衡最小化调度开销和确保总效用达到所需水平这两个目标。&lt;h4&gt;主要发现&lt;/h4&gt;在包含多达500条链路的模拟无线多跳网络中，链路稀疏化技术有效缓解了网络拥塞，并减少了四种不同分布式链路调度协议的无线电足迹。&lt;h4&gt;结论&lt;/h4&gt;基于GNN的链路稀疏化方案是一种有效的方法，可以在保持网络容量的同时减少无线网络中的调度开销。&lt;h4&gt;翻译&lt;/h4&gt;在具有密集连接特性的无线网络中，分布式链路调度算法产生的显著信令开销会加剧拥塞、能源消耗和无线电足迹扩展等问题。为了缓解这些挑战，我们提出了一种分布式链路稀疏化方案，该方案使用图神经网络(GNNs)来减少对延迟容忍型流量的调度开销，同时保持网络容量。一个GNN模块被训练来根据流量统计和网络拓扑为单个链路调整竞争阈值，使链路在不太可能成功的情况下退出调度竞争。我们的方法通过一种新颖的离线约束无监督学习算法实现，该算法能够平衡两个相互竞争的目标：最小化调度开销，同时确保总效用达到所需水平。在包含多达500条链路的模拟无线多跳网络中，我们的链路稀疏化技术有效缓解了网络拥塞，并减少了四种不同分布式链路调度协议的无线电足迹。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TWC.2025.3606741&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In wireless networks characterized by dense connectivity, the significantsignaling overhead generated by distributed link scheduling algorithms canexacerbate issues like congestion, energy consumption, and radio footprintexpansion. To mitigate these challenges, we propose a distributed linksparsification scheme employing graph neural networks (GNNs) to reducescheduling overhead for delay-tolerant traffic while maintaining networkcapacity. A GNN module is trained to adjust contention thresholds forindividual links based on traffic statistics and network topology, enablinglinks to withdraw from scheduling contention when they are unlikely to succeed.Our approach is facilitated by a novel offline constrained {unsupervised}learning algorithm capable of balancing two competing objectives: minimizingscheduling overhead while ensuring that total utility meets the required level.In simulated wireless multi-hop networks with up to 500 links, our linksparsification technique effectively alleviates network congestion and reducesradio footprints across four distinct distributed link scheduling protocols.</description>
      <author>example@mail.com (Zhongyuan Zhao, Gunjan Verma, Ananthram Swami, Santiago Segarra)</author>
      <guid isPermaLink="false">2509.05447v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Safeguarding Graph Neural Networks against Topology Inference Attacks</title>
      <link>http://arxiv.org/abs/2509.05429v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Acctepted by ACM CCS'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)中的拓扑隐私问题，提出了一种新型的拓扑推理攻击方法，并设计了私有图重构(PGR)防御框架来保护拓扑隐私同时保持模型准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图结构数据的有力模型，但其广泛应用引发了严重的隐私问题。先前的研究主要集中在边级隐私上，而图的整体结构拓扑隐私这一关键但未被充分探索的威胁却很少受到关注。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs中的拓扑隐私风险，揭示其对图级推理攻击的脆弱性，并提出一种能够保护拓扑隐私同时保持模型准确性的防御框架。&lt;h4&gt;方法&lt;/h4&gt;提出了一套拓扑推理攻击(TIAs)，这些攻击仅通过黑盒访问GNN模型就能重构目标训练图的结构；引入私有图重构(PGR)防御框架，将其表述为双层优化问题，使用元迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;GNNs极易受到拓扑推理攻击，现有的边级差分隐私机制要么无法缓解风险，要么严重损害模型准确性；PGR能显著减少拓扑泄露，同时对模型准确性影响最小。&lt;h4&gt;结论&lt;/h4&gt;拓扑隐私是GNNs中一个重要但被忽视的隐私问题，需要专门的防御机制来保护，而PGR框架为此提供了一个有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的有力模型。然而，它们的广泛应用引发了严重的隐私问题。尽管先前的研究主要集中在边级隐私上，但一个关键却未被充分探索的威胁在于拓扑隐私——图整体结构的保密性。在这项工作中，我们对GNNs中的拓扑隐私风险进行了全面研究，揭示了它们对图级推理攻击的脆弱性。为此，我们提出了一套拓扑推理攻击(TIAs)，这些攻击仅通过黑盒访问GNN模型就能重构目标训练图的结构。我们的研究表明，GNNs极易受到这些攻击，现有的边级差分隐私机制不足以缓解风险，因为它们要么无法减轻风险，要么严重损害模型准确性。为了应对这一挑战，我们引入了私有图重构(PGR)，这是一个新的防御框架，旨在保护拓扑隐私同时保持模型准确性。PGR被表述为一个双层优化问题，其中使用元梯度迭代生成合成训练图，并基于不断演化的图同时更新GNN模型。大量实验证明，PGR显著减少了拓扑泄露，同时对模型准确性的影响最小。我们的代码已在 https://github.com/JeffffffFu/PGR 上匿名提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as powerful models for learningfrom graph-structured data. However, their widespread adoption has raisedserious privacy concerns. While prior research has primarily focused onedge-level privacy, a critical yet underexplored threat lies in topologyprivacy - the confidentiality of the graph's overall structure. In this work,we present a comprehensive study on topology privacy risks in GNNs, revealingtheir vulnerability to graph-level inference attacks. To this end, we propose asuite of Topology Inference Attacks (TIAs) that can reconstruct the structureof a target training graph using only black-box access to a GNN model. Ourfindings show that GNNs are highly susceptible to these attacks, and thatexisting edge-level differential privacy mechanisms are insufficient as theyeither fail to mitigate the risk or severely compromise model accuracy. Toaddress this challenge, we introduce Private Graph Reconstruction (PGR), anovel defense framework designed to protect topology privacy while maintainingmodel accuracy. PGR is formulated as a bi-level optimization problem, where asynthetic training graph is iteratively generated using meta-gradients, and theGNN model is concurrently updated based on the evolving graph. Extensiveexperiments demonstrate that PGR significantly reduces topology leakage withminimal impact on model accuracy. Our code is anonymously available athttps://github.com/JeffffffFu/PGR.</description>
      <author>example@mail.com (Jie Fu, Hong Yuan, Zhili Chen, Wendy Hui Wang)</author>
      <guid isPermaLink="false">2509.05429v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>RoboBallet: Planning for Multi-Robot Reaching with Graph Neural Networks and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.05397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in Science Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于强化学习的框架，用于解决多机器人在共享障碍物丰富环境中的自动化任务分配、调度和运动规划问题，实现了八个机器人执行40个抓取任务的零样本泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现代机器人制造需要多个机器人在共享、障碍物丰富的环境中无碰撞地协调完成多项任务。虽然单个任务可能简单，但在时空约束下自动进行联合任务分配、调度和运动规划对于经典方法在现实规模下计算上难以处理。现有工业多臂系统依赖人工经验手动设计轨迹，过程劳动密集。&lt;h4&gt;目的&lt;/h4&gt;解决多机器人系统在复杂环境中的自动化任务和运动规划挑战，减少对人工干预的依赖，提高效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;提出基于强化学习的框架，使用图神经网络(GNN)策略，在程序生成的多样化环境中进行训练。该方法采用场景的图表示和图策略神经网络，通过强化学习生成多个机器人的轨迹，联合解决任务分配、调度和运动规划的子问题。&lt;h4&gt;主要发现&lt;/h4&gt;训练好的策略可以直接泛化到未见过的设置，适应不同的机器人位置、障碍物几何形状和任务姿态。解决方案的高速能力使其可用于工作单元布局优化，提高解决方案时间。&lt;h4&gt;结论&lt;/h4&gt;该规划器的速度和可扩展性为容错规划和基于在线感知的重新规划等新功能开辟了可能，特别是在需要快速适应动态任务集的场景中。&lt;h4&gt;翻译&lt;/h4&gt;现代机器人制造需要多个机器人在共享、障碍物丰富的环境中无碰撞地协调完成多项任务。虽然单个任务可能简单，但在时空约束下自动进行联合任务分配、调度和运动规划对于经典方法在现实规模下计算上难以处理。现有工业中部署的多臂系统依赖人类直觉和经验手动设计可行轨迹，这是一个劳动密集型过程。为解决这一挑战，我们提出了一种强化学习(RL)框架来实现自动化任务和运动规划，在障碍物丰富的环境中进行了测试，八个机器人在共享工作空间中执行40个抓取任务，任何机器人可以以任何顺序执行任何任务。我们的方法建立在通过强化学习在程序生成的环境中训练的图神经网络(GNN)策略基础上，这些环境具有多样化的障碍物布局、机器人配置和任务分布。它采用场景的图表示和通过强化学习训练的图策略神经网络来生成多个机器人的轨迹，联合解决任务分配、调度和运动规划的子问题。在模拟中使用大型随机生成的任务集进行训练后，我们的策略可以直接泛化到具有不同机器人位置、障碍物几何形状和任务姿态的未见设置。我们进一步证明了该解决方案的高速能力使其可用于工作单元布局优化，提高解决方案时间。该规划器的速度和可扩展性也为容错规划和基于在线感知的重新规划等新功能打开了大门，在这些场景中需要快速适应动态任务集。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在障碍物丰富的共享工作空间中，多个机器人协同完成多个任务时的自动化任务分配、调度和运动规划问题。这个问题在现实中非常重要，因为现代机器人制造需要高效协调多个机器人，而传统方法在真实世界规模上计算上不可行，现有工业系统依赖人工手动设计轨迹，耗时耗力且难以适应变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了经典方法的局限性：采样方法在复杂环境中计算复杂度呈指数增长，任务调度类似旅行商问题但更复杂，任务分配类似背包问题但成本相互依赖。然后作者提出基于深度强化学习的框架，使用图神经网络表示场景关系。他们借鉴了图神经网络处理关系信息的能力，强化学习中的TD3算法，以及后视经验回放(HER)解决稀疏奖励问题。关键创新在于将这三个子问题联合解决，而不是传统方法的分步解决。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用图神经网络表示多机器人场景，其中节点代表机器人、任务和障碍物，边代表它们之间的关系。通过强化学习训练策略网络直接控制所有机器人的关节速度，将复杂计算从在线规划转移到离线训练。整体流程包括：1)将场景表示为图；2)图神经网络接收状态输入；3)策略网络输出关节速度命令；4)模拟器执行动作并防止碰撞；5)基于任务完成和碰撞情况计算奖励；6)使用改进的TD3算法和HER训练网络；7)训练后的模型可快速生成规划并支持工作单元优化等应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)图神经网络与强化学习的结合解决多机器人协调；2)联合解决任务分配、调度和运动规划三个子问题；3)模型复杂度随问题规模线性或二次增长而非指数增长；4)在随机训练环境上训练后能零样本泛化到新环境；5)规划速度极快，最大设置下每步仅需0.3毫秒。相比之前工作，传统方法需分步解决子问题且牺牲最优性，最多只能处理5个机器人和10个任务，依赖人工设计耗时轨迹，难以适应动态环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过结合图神经网络和强化学习，RoboBallet实现了在障碍物丰富的环境中高效协调多达八个机器人完成四十个任务的自动化任务和运动规划，解决了传统方法在真实世界规模上面临的计算复杂性问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1126/scirobotics.ads1204&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern robotic manufacturing requires collision-free coordination of multiplerobots to complete numerous tasks in shared, obstacle-rich workspaces. Althoughindividual tasks may be simple in isolation, automated joint task allocation,scheduling, and motion planning under spatio-temporal constraints remaincomputationally intractable for classical methods at real-world scales.Existing multi-arm systems deployed in the industry rely on human intuition andexperience to design feasible trajectories manually in a labor-intensiveprocess. To address this challenge, we propose a reinforcement learning (RL)framework to achieve automated task and motion planning, tested in anobstacle-rich environment with eight robots performing 40 reaching tasks in ashared workspace, where any robot can perform any task in any order. Ourapproach builds on a graph neural network (GNN) policy trained via RL onprocedurally-generated environments with diverse obstacle layouts, robotconfigurations, and task distributions. It employs a graph representation ofscenes and a graph policy neural network trained through reinforcement learningto generate trajectories of multiple robots, jointly solving the sub-problemsof task allocation, scheduling, and motion planning. Trained on large randomlygenerated task sets in simulation, our policy generalizes zero-shot to unseensettings with varying robot placements, obstacle geometries, and task poses. Wefurther demonstrate that the high-speed capability of our solution enables itsuse in workcell layout optimization, improving solution times. The speed andscalability of our planner also open the door to new capabilities such asfault-tolerant planning and online perception-based re-planning, where rapidadaptation to dynamic task sets is required.</description>
      <author>example@mail.com (Matthew Lai, Keegan Go, Zhibin Li, Torsten Kroger, Stefan Schaal, Kelsey Allen, Jonathan Scholz)</author>
      <guid isPermaLink="false">2509.05397v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Pothole Detection and Recognition based on Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.06750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型，用于自动检测和识别道路坑洞。该模型通过预处理技术和持续优化，在测试集上实现了97.78%至98.89%的高分类准确率，性能优于随机森林、MLP、SVM和LightGBM等其他模型。&lt;h4&gt;背景&lt;/h4&gt;随着计算机视觉和机器学习的快速发展，基于图像和视频数据的坑洞检测和识别自动化方法受到广泛关注。对道路图像进行深入分析对社会发展具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;通过特征提取对道路图像进行深入分析，实现对新图像中坑穴状况的自动识别。&lt;h4&gt;方法&lt;/h4&gt;研究团队对收集的原始数据集应用了标准化、归一化和数据增强等预处理技术，基于实验结果持续改进网络模型，构建了基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型。采用比较评估方法，将提出的模型与随机森林、MLP、SVM和LightGBM等模型进行性能比较，基于准确率、召回率、精确率、F1分数和FPS等指标进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;提出的迁移学习模型在识别速度和准确性方面表现出高性能，超越了其他模型的性能。通过仔细的参数选择和模型优化，在90个初始测试样本上实现了97.78%的分类准确率（88/90），在扩展测试集上实现了98.89%的分类准确率（890/900）。&lt;h4&gt;结论&lt;/h4&gt;构建的ResNet50-EfficientNet-RegNet模型具有高分类精度和计算效率，在坑洞检测任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;随着计算机视觉和机器学习的迅速发展，基于图像和视频数据的坑洞检测和识别自动化方法已受到显著关注。通过特征提取对道路图像进行深入分析，从而实现对新图像中坑洞状况的自动识别，对社会发展具有重要意义。因此，这是本研究解决的主要问题。基于对收集的原始数据集应用标准化、归一化和数据增强等预处理技术，我们根据实验结果持续改进网络模型。最终，我们构建了一个基于迁移学习的深度学习特征提取网络ResNet50-EfficientNet-RegNet模型。该模型具有高分类精度和计算效率。在模型评估方面，本研究采用比较评估方法，将提出的迁移学习模型与随机森林、MLP、SVM和LightGBM等其他模型进行比较。比较分析基于准确率、召回率、精确率、F1分数和FPS等指标，以评估本文提出的迁移学习模型的分类性能。结果表明，我们的模型在识别速度和准确性方面表现出高性能，超越了其他模型的性能。通过仔细的参数选择和模型优化，我们的迁移学习模型在90个初始测试样本集上实现了97.78%（88/90）的分类准确率，在扩展测试集上实现了98.89%（890/900）的分类准确率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of computer vision and machine learning, automatedmethods for pothole detection and recognition based on image and video datahave received significant attention. It is of great significance for socialdevelopment to conduct an in-depth analysis of road images through featureextraction, thereby achieving automatic identification of the pothole conditionin new images. Consequently, this is the main issue addressed in this study.Based on preprocessing techniques such as standardization, normalization, anddata augmentation applied to the collected raw dataset, we continuouslyimproved the network model based on experimental results. Ultimately, weconstructed a deep learning feature extraction networkResNet50-EfficientNet-RegNet model based on transfer learning. This modelexhibits high classification accuracy and computational efficiency. In terms ofmodel evaluation, this study employed a comparative evaluation approach bycomparing the performance of the proposed transfer learning model with othermodels, including Random Forest, MLP, SVM, and LightGBM. The comparisonanalysis was conducted based on metrics such as Accuracy, Recall, Precision,F1-score, and FPS, to assess the classification performance of the transferlearning model proposed in this paper. The results demonstrate that our modelexhibits high performance in terms of recognition speed and accuracy,surpassing the performance of other models. Through careful parameter selectionand model optimization, our transfer learning model achieved a classificationaccuracy of 97.78% (88/90) on the initial set of 90 test samples and 98.89%(890/900) on the expanded test set.</description>
      <author>example@mail.com (Mang Hu, Qianqian Xia)</author>
      <guid isPermaLink="false">2509.06750v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Robust and Adaptive Spectral Method for Representation Multi-Task Learning with Contamination</title>
      <link>http://arxiv.org/abs/2509.06575v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种鲁棒且自适应的光谱方法(RAS)，用于处理具有未知且可能大量污染比例的表示多任务学习，允许任务间存在异质性，无需预先了解污染水平或真实表示维度。&lt;h4&gt;背景&lt;/h4&gt;基于表示的多任务学习(MTL)通过学习任务间的共享结构提高效率，但实际应用常受污染、异常值或对抗任务阻碍。现有方法和理论假设清洁环境，在污染严重时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;处理具有未知且可能大量污染比例的表示多任务学习，同时允许内聚任务之间存在异质性。&lt;h4&gt;方法&lt;/h4&gt;提出鲁棒且自适应的光谱方法(RAS)，可以有效且高效地提炼共享的内聚表示，无需预先了解污染水平或真实表示维度。&lt;h4&gt;主要发现&lt;/h4&gt;为学习到的表示和每个任务的参数提供了非渐近误差界，这些界适应于内聚任务相似性和异常值结构，保证RAS至少与单任务学习一样好，防止负迁移；将框架扩展到迁移学习，为目标任务提供理论保证。&lt;h4&gt;结论&lt;/h4&gt;大量实验证实了理论，展示了RAS的鲁棒性和自适应性，以及在高达80%任务污染情况下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;基于表示的多任务学习(MTL)通过学习任务间的共享结构提高效率，但其实际应用常受到污染、异常值或对抗任务的阻碍。大多数现有方法和理论假设清洁或接近清洁的环境，在污染严重时表现不佳。本文处理具有未知且可能大量污染比例的表示多任务学习，同时允许内聚任务之间存在异质性。我们提出了一种鲁棒且自适应的光谱方法(RAS)，可以有效且高效地提炼共享的内聚表示，无需预先了解污染水平或真实表示的维度。理论上，我们为学习到的表示和每个任务的参数提供了非渐近误差界。这些界适应于内聚任务相似性和异常值结构，并保证RAS至少与单任务学习一样好，从而防止负迁移。我们将框架扩展到迁移学习，为目标任务提供了相应的理论保证。大量实验证实了我们的理论，展示了RAS的鲁棒性和自适应性，以及在高达80%任务污染情况下的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation-based multi-task learning (MTL) improves efficiency bylearning a shared structure across tasks, but its practical application isoften hindered by contamination, outliers, or adversarial tasks. Most existingmethods and theories assume a clean or near-clean setting, failing whencontamination is significant. This paper tackles representation MTL with anunknown and potentially large contamination proportion, while also allowing forheterogeneity among inlier tasks. We introduce a Robust and Adaptive Spectralmethod (RAS) that can distill the shared inlier representation effectively andefficiently, while requiring no prior knowledge of the contamination level orthe true representation dimension. Theoretically, we provide non-asymptoticerror bounds for both the learned representation and the per-task parameters.These bounds adapt to inlier task similarity and outlier structure, andguarantee that RAS performs at least as well as single-task learning, thuspreventing negative transfer. We also extend our framework to transfer learningwith corresponding theoretical guarantees for the target task. Extensiveexperiments confirm our theory, showcasing the robustness and adaptivity ofRAS, and its superior performance in regimes with up to 80\% taskcontamination.</description>
      <author>example@mail.com (Yian Huang, Yang Feng, Zhiliang Ying)</author>
      <guid isPermaLink="false">2509.06575v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Synesthesia of Machines (SoM)-Aided LiDAR Point Cloud Transmission for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2509.06506v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种名为LPC-FT的激光雷达点云特征传输系统，通过机器联觉实现高效、鲁棒且适用的点云传输，支持多智能体协作感知&lt;h4&gt;背景&lt;/h4&gt;协作感知通过学习智能体之间如何共享信息来实现更准确和全面的场景理解，激光雷达点云提供了必要的精确空间数据。由于激光雷达传感器产生的大量数据，高效的点云传输对于低延迟多智能体协作至关重要&lt;h4&gt;目的&lt;/h4&gt;提出一个高效、鲁棒且适用的激光雷达点云传输系统，通过机器联觉(SoM)实现，称为激光雷达点云特征传输(LPC-FT)，用于支持多智能体之间的协作感知&lt;h4&gt;方法&lt;/h4&gt;采用一种保持密度的深度点云压缩方法，将完整点云编码为下采样高效表示；设计基于自注意力的信道编码模块，以增强激光雷达点云特征；设计基于交叉注意力的特征融合模块，集成收发器特征；利用非线性激活层和迁移学习来改善存在数字信道噪声时的深度神经网络训练&lt;h4&gt;主要发现&lt;/h4&gt;所提出的LPC-FT比基于八叉树的传统压缩后跟信道编码更鲁棒和有效；优于最先进的基于深度学习的压缩技术和现有的语义通信方法；将Chamfer距离减少了30%，平均提高了PSNR 1.9 dB&lt;h4&gt;结论&lt;/h4&gt;由于其优越的重构性能和对信道变化的鲁棒性，LPC-FT有望支持协作感知任务&lt;h4&gt;翻译&lt;/h4&gt;协作感知通过学习智能体之间如何共享信息来实现更准确和全面的场景理解，激光雷达点云提供了必要的精确空间数据。由于激光雷达传感器产生的大量数据，高效的点云传输对于低延迟多智能体协作至关重要。在这项工作中，我们通过机器联觉(SoM)提出了一种高效、鲁棒且适用的激光雷达点云传输系统，称为激光雷达点云特征传输(LPC-FT)，以支持多智能体之间的协作感知。具体来说，我们采用一种保持密度的深度点云压缩方法，将完整点云编码为下采样高效表示。为了减轻无线信道的影响，我们设计了一个基于自注意力的信道编码模块来增强激光雷达点云特征，以及一个基于交叉注意力的特征融合模块来集成收发器的特征。此外，我们利用非线性激活层和迁移学习来改善存在数字信道噪声时的深度神经网络训练。实验结果表明，所提出的LPC-FT比传统的基于八叉树的压缩后跟信道编码更鲁棒和有效，并优于最先进的基于深度学习的压缩技术和现有的语义通信方法，平均将Chamfer距离减少了30%，将PSNR提高了1.9 dB。由于其优越的重构性能和对信道变化的鲁棒性，LPC-FT有望支持协作感知任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR点云在多智能体协作感知中的高效传输问题。由于LiDAR传感器产生大量数据，且无线信道噪声会影响传输质量，现有方法难以高效传输点云数据。这个问题在自动驾驶和智能机器人系统中非常重要，因为协作感知能克服单一智能体的视野限制，而LiDAR点云提供了精确的空间数据，对环境理解至关重要。传输效率直接影响实时协作感知的性能，点云重建质量又直接影响感知准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了协作感知中点云传输面临的三大挑战：高效表示非均匀点云、抵抗无线信道干扰、处理数字通信系统中的非可微操作。他们借鉴了机器联觉(SoM)概念，设计任务感知的特征提取和传输方法。具体借鉴了密度保持的点云压缩方法作为特征编码器基础，利用注意力机制设计通道编码器和特征融合模块，并采用直通估计器(STE)处理数字通信系统中的非可微操作。作者采用两阶段训练策略，先训练点云压缩网络，再训练整个传输系统，以提高训练效率和性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过语义通信方式只传输点云的关键特征而非原始数据，设计端到端的深度学习框架实现点云特征的高效提取、传输和重建，利用注意力机制增强特征表示抵抗无线信道噪声，并采用数字通信框架确保与现有通信系统的兼容性。整体流程为：1)特征编码器将原始点云压缩为紧凑特征；2)通道编码器基于自注意力机制增强特征；3)调制将特征量化为数字符号；4)无线传输通过物理信道；5)解调和通道解码；6)特征融合模块基于交叉注意力机制融合接收特征和接收端自感知特征；7)特征解码器重建完整点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出LiDAR点云特征传输系统(LPC-FT)；2)采用密度保持的点云压缩网络并添加绝对坐标位置编码；3)设计基于自注意力的通道编码器增强特征；4)提出基于交叉注意力的特征融合模块；5)采用非线性激活层处理数字通信系统中的非可微操作；6)使用迁移学习和两阶段训练策略。相比之前工作的不同：专注于LiDAR点云而非普通物体点云；采用数字通信框架而非离散时间模拟传输；结合特征增强和特征融合；考虑大规模点云的绝对坐标信息；针对无线信道噪声设计专门的增强机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于机器联觉的LiDAR点云特征传输系统，通过密度保持压缩、自注意力增强和交叉注意力融合，实现了高效、鲁棒的点云传输，显著提升了多智能体协作感知的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception enables more accurate and comprehensive sceneunderstanding by learning how to share information between agents, with LiDARpoint clouds providing essential precise spatial data. Due to the substantialdata volume generated by LiDAR sensors, efficient point cloud transmission isessential for low-latency multi-agent collaboration. In this work, we proposean efficient, robust and applicable LiDAR point cloud transmission system viathe Synesthesia of Machines (SoM), termed LiDAR Point Cloud FeatureTransmission (LPC-FT), to support collaborative perception among multipleagents. Specifically, we employ a density-preserving deep point cloudcompression method that encodes the complete point cloud into a downsampledefficient representation. To mitigate the effects of the wireless channel, wedesign a channel encoder module based on self-attention to enhance LiDAR pointcloud features and a feature fusion module based on cross-attention tointegrate features from transceivers. Furthermore, we utilize the nonlinearactivation layer and transfer learning to improve the training of deep neuralnetworks in the presence the digital channel noise. Experimental resultsdemonstrate that the proposed LPC-FT is more robust and effective thantraditional octree-based compression followed by channel coding, andoutperforms state-of-the-art deep learning-based compression techniques andexisting semantic communication methods, reducing the Chamfer Distance by 30%and improving the PSNR by 1.9 dB on average. Owing to its superiorreconstruction performance and robustness against channel variations, LPC-FT isexpected to support collaborative perception tasks.</description>
      <author>example@mail.com (Ensong Liu, Rongqing Zhang, Xiang Cheng, Jian Tang)</author>
      <guid isPermaLink="false">2509.06506v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Minimax optimal transfer learning for high-dimensional additive regression</title>
      <link>http://arxiv.org/abs/2509.06308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a draft version of the paper. All responsibilities are  assigned to the first author&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了迁移学习框架下的高维加性回归问题，通过结合目标样本和来自相关回归模型的辅助样本，提出了一种新的估计方法，并在理论和实证上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;研究者观察到来自目标群体的样本，以及来自不同但可能相关的回归模型的辅助样本，需要利用这些信息进行高维加性回归分析。&lt;h4&gt;目的&lt;/h4&gt;开发一种在迁移学习框架下处理高维加性回归的方法，建立通用误差边界，并实现最小最优速率。&lt;h4&gt;方法&lt;/h4&gt;首先引入基于局部线性平滑的光滑反向拟合估计器的目标估计程序；然后开发一种两阶段估计方法，在迁移学习框架内提供理论和经验水平的保证。&lt;h4&gt;主要发现&lt;/h4&gt;在次魏布噪声下建立了通用误差边界，能够处理重尾误差分布；在次指数情况下达到最小最大下界；当辅助和目标分布足够接近时，实现了最小最优速率。&lt;h4&gt;结论&lt;/h4&gt;所有理论结果都得到了模拟研究和实际数据分析的支持，证明了所提出方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了迁移学习框架下高维加性回归问题，其中研究者观察到来自目标群体的样本以及来自不同但可能相关的回归模型的辅助样本。我们首先引入了基于局部线性平滑的光滑反向拟合估计器的目标估计程序。与以往工作不同，我们在次魏布噪声下建立了通用误差边界，从而能够处理重尾误差分布。在次指数情况下，我们证明估计器在正则性条件下达到最小最大下界，这需要从现有证明策略中做出重大调整。随后，我们在迁移学习框架内开发了一种新颖的两阶段估计方法，并在总体和经验水平上提供了理论保证。在通用尾部条件下推导了每个阶段的误差边界，并且我们进一步证明当辅助和目标分布足够接近时，可以实现最小最优速率。所有理论结果都得到了模拟研究和实际数据分析的支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies high-dimensional additive regression under the transferlearning framework, where one observes samples from a target populationtogether with auxiliary samples from different but potentially relatedregression models. We first introduce a target-only estimation procedure basedon the smooth backfitting estimator with local linear smoothing. In contrast toprevious work, we establish general error bounds under sub-Weibull($\alpha$)noise, thereby accommodating heavy-tailed error distributions. In thesub-exponential case ($\alpha=1$), we show that the estimator attains theminimax lower bound under regularity conditions, which requires a substantialdeparture from existing proof strategies. We then develop a novel two-stageestimation method within a transfer learning framework, and provide theoreticalguarantees at both the population and empirical levels. Error bounds arederived for each stage under general tail conditions, and we furtherdemonstrate that the minimax optimal rate is achieved when the auxiliary andtarget distributions are sufficiently close. All theoretical results aresupported by simulation studies and real data analysis.</description>
      <author>example@mail.com (Seung Hyun Moon)</author>
      <guid isPermaLink="false">2509.06308v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>AI-Based Applied Innovation for Fracture Detection in X-rays Using Custom CNN and Transfer Learning Models</title>
      <link>http://arxiv.org/abs/2509.06228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Amna-Hassan04/Fracture-Detection-Using-X-Rays-with-CNN&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于AI的骨折检测解决方案，使用自定义卷积神经网络(CNN)从X光图像中自动检测骨折，并在公开数据集上进行了测试和基准比较。&lt;h4&gt;背景&lt;/h4&gt;骨折是全球性的重大健康挑战，尤其在资源有限环境中，专家放射科服务有限。传统成像方法存在成本高、辐射暴露和依赖专业解读等问题。&lt;h4&gt;目的&lt;/h4&gt;开发基于AI的解决方案，用于从X光图像中自动检测骨折，并与迁移学习模型进行比较评估。&lt;h4&gt;方法&lt;/h4&gt;使用自定义卷积神经网络(CNN)，并与迁移学习模型(EfficientNetB0, MobileNetV2, ResNet50)进行基准测试。训练使用公开可用的FracAtlas数据集，包含4,083个匿名化的肌肉骨骼X光片。&lt;h4&gt;主要发现&lt;/h4&gt;自定义CNN在FracAtlas数据集上达到95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。迁移学习模型在此特定设置中表现不佳，这些结果应考虑类别不平衡和数据集限制来解释。&lt;h4&gt;结论&lt;/h4&gt;轻量级CNN在X光骨折检测方面具有前景，强调了公平基准测试、多样化数据集和临床转化的外部验证的重要性。&lt;h4&gt;翻译&lt;/h4&gt;骨折是全球面临的主要健康挑战，常导致疼痛、活动能力下降和生产力损失，特别是在资源有限的环境中，专家放射科服务有限。传统成像方法存在成本高、辐射暴露和依赖专业解读等问题。为此，我们开发了一种基于AI的解决方案，使用自定义卷积神经网络(CNN)从X光图像中自动检测骨折，并将其与迁移学习模型(包括EfficientNetB0、MobileNetV2和ResNet50)进行了基准测试。训练在公开可用的FracAtlas数据集上进行，该数据集包含4,083个匿名化的肌肉骨骼X光片。在FracAtlas数据集上，自定义CNN达到了95.96%的准确率、0.94的精确度、0.88的召回率和0.91的F1分数。尽管迁移学习模型(EfficientNetB0、MobileNetV2、ResNet50)在此特定设置中表现不佳，但这些结果应结合类别不平衡和数据集限制来解释。这项工作突出了轻量级CNN在X光骨折检测方面的前景，并强调了公平基准测试、多样化数据集和临床转化的外部验证的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bone fractures present a major global health challenge, often resulting inpain, reduced mobility, and productivity loss, particularly in low-resourcesettings where access to expert radiology services is limited. Conventionalimaging methods suffer from high costs, radiation exposure, and dependency onspecialized interpretation. To address this, we developed an AI-based solutionfor automated fracture detection from X-ray images using a custom ConvolutionalNeural Network (CNN) and benchmarked it against transfer learning modelsincluding EfficientNetB0, MobileNetV2, and ResNet50. Training was conducted onthe publicly available FracAtlas dataset, comprising 4,083 anonymizedmusculoskeletal radiographs. The custom CNN achieved 95.96% accuracy, 0.94precision, 0.88 recall, and an F1-score of 0.91 on the FracAtlas dataset.Although transfer learning models (EfficientNetB0, MobileNetV2, ResNet50)performed poorly in this specific setup, these results should be interpreted inlight of class imbalance and data set limitations. This work highlights thepromise of lightweight CNNs for detecting fractures in X-rays and underscoresthe importance of fair benchmarking, diverse datasets, and external validationfor clinical translation</description>
      <author>example@mail.com (Amna Hassan, Ilsa Afzaal, Nouman Muneeb, Aneeqa Batool, Hamail Noor)</author>
      <guid isPermaLink="false">2509.06228v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Repeating vs. Non-Repeating FRBs: A Deep Learning Approach To Morphological Characterization</title>
      <link>http://arxiv.org/abs/2509.06208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 17 figures, submitted to ApJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于形态学的深度学习方法来分类快速射电暴(FRB)，通过迁移学习和预训练的ConvNext架构实现了高效准确的分类，能够区分重复暴和非重复暴。&lt;h4&gt;背景&lt;/h4&gt;基于CHIME/FRB目录2中记录的动态频谱对快速射电暴进行形态学分类，快速射电暴分为重复暴和非重复暴两类。&lt;h4&gt;目的&lt;/h4&gt;开发一种仅基于FRB形态学特征的深度学习方法，用于区分重复暴和非重复暴。&lt;h4&gt;方法&lt;/h4&gt;使用迁移学习技术，采用预训练的ConvNext架构，将去色散动态频谱视为图像，基于时间和频谱特性及子脉冲结构关系进行分类，并使用数学模型表示来解释深度学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的模型实现了高分类指标，显著减少了训练时间和计算需求；CHIME重复和非重复事件间的形态学差异在目录2中仍然存在，深度学习模型利用了这些差异进行分类。&lt;h4&gt;结论&lt;/h4&gt;微调后的深度学习模型可用于推理，预测FRB形态是否类似于重复暴或非重复暴，当在更大数据集上训练时，这类推断将变得更加重要。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种深度学习方法，仅基于形态学对快速射电暴(FRB)进行分类，形态学编码在CHIME/FRB目录2中记录的动态频谱上。我们实现了迁移学习，使用预训练的ConvNext架构，利用其强大的特征提取能力。ConvNext被调整用于分类FRB的去色散动态频谱（我们将其视为图像），根据各种时间和频谱特性以及子脉冲结构之间的关系，将FRB分为两个子类之一，即重复暴和非重复暴。此外，我们还使用总强度数据的数学模型表示来解释深度学习模型。在FRB频谱图上微调预训练的ConvNet后，我们能够实现高分类指标，与从头开始训练深度学习模型（随机权重和偏差，没有任何特征提取能力）相比，显著减少了训练时间和计算能力。重要的是，我们的结果表明，CHIME重复事件和非重复事件之间的形态学差异在目录2中仍然存在，深度学习模型利用了这些差异进行分类。微调后的深度学习模型可用于推理，使我们能够预测FRB的形态是否类似于重复暴或非重复暴。当在即将出现的更大数据集上训练时，此类推断可能会变得越来越重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a deep learning approach to classify fast radio bursts (FRBs)based purely on morphology as encoded on recorded dynamic spectrum fromCHIME/FRB Catalog 2. We implemented transfer learning with a pretrainedConvNext architecture, exploiting its powerful feature extraction ability.ConvNext was adapted to classify dedispersed dynamic spectra (which we treat asimages) of the FRBs into one of the two sub-classes, i.e., repeater andnon-repeater, based on their various temporal and spectral properties andrelation between the sub-pulse structures. Additionally, we also usedmathematical model representation of the total intensity data to interpret thedeep learning model. Upon fine-tuning the pretrained ConvNext on the FRBspectrograms, we were able to achieve high classification metrics whilesubstantially reducing training time and computing power as compared totraining a deep learning model from scratch with random weights and biaseswithout any feature extraction ability. Importantly, our results suggest thatthe morphological differences between CHIME repeating and non-repeating eventspersist in Catalog 2 and the deep learning model leveraged these differencesfor classification. The fine-tuned deep learning model can be used forinference, which enables us to predict whether an FRB's morphology resemblesthat of repeaters or non-repeaters. Such inferences may become increasinglysignificant when trained on larger data sets that will exist in the nearfuture.</description>
      <author>example@mail.com (Bikash Kharel, Emmanuel Fonseca, Charanjot Brar, Afrokk Khan, Lluis Mas-Ribas, Swarali Shivraj Patil, Paul Scholz, Seth Robert Siegel, David C. Stenning)</author>
      <guid isPermaLink="false">2509.06208v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>A Fine-Grained Attention and Geometric Correspondence Model for Musculoskeletal Risk Classification in Athletes Using Multimodal Visual and Skeletal Features</title>
      <link>http://arxiv.org/abs/2509.05913v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ViSK-GAT的新型多模态深度学习框架，用于运动员肌肉骨骼风险评估，通过结合视觉和骨骼坐标特征实现了高准确率的风险分类。&lt;h4&gt;背景&lt;/h4&gt;肌肉骨骼障碍对运动员构成重大风险，早期风险评估对预防很重要，但现有方法大多针对受控环境设计，无法在复杂环境中可靠评估风险。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂环境中可靠评估运动员肌肉骨骼风险的方法，通过多模态数据融合提高风险评估的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出ViSK-GAT框架，结合残差块和轻量级Transformer块学习时空依赖关系，包含细粒度注意力模块(FGAM)和多模态几何对应模块(MGCM)；构建自定义多模态数据集，结合视觉数据和骨骼坐标，将样本标记为八个风险类别。&lt;h4&gt;主要发现&lt;/h4&gt;ViSK-GAT在验证和测试中分别达到93.55%和93.89%的准确率，精确度93.86%，F1得分93.85%，Cohen's Kappa和Matthews相关系数93%；回归结果显示预测概率分布的均方根误差为0.1205，平均绝对误差为0.0156；优于九种流行的迁移学习骨干方法。&lt;h4&gt;结论&lt;/h4&gt;ViSK-GAT模型推进了人工智能在肌肉骨骼风险分类领域的应用，能够在体育领域实现有影响力的早期干预。&lt;h4&gt;翻译&lt;/h4&gt;肌肉骨骼障碍对运动员构成重大风险，早期风险评估对预防很重要。然而，大多数现有方法是为受控环境设计的，由于依赖单一类型数据，无法在复杂环境中可靠评估风险。本研究提出了ViSK-GAT（视觉-骨骼几何注意力Transformer），一种新型多模态深度学习框架，使用视觉和基于骨骼坐标的特征来分类肌肉骨骼风险。此外，还构建了一个自定义多模态数据集，结合视觉数据和骨骼坐标进行风险评估。每个样本根据全身快速评估系统标记为八个风险类别。ViSK-GAT结合了残差块和轻量级Transformer块，共同学习时空依赖关系。它包含两个新颖模块：细粒度注意力模块(FGAM)，通过视觉和骨骼输入之间的交叉注意力实现精确的跨模态特征细化；以及多模态几何对应模块(MGCM)，通过将图像特征与基于坐标的表示对齐来增强跨模态一致性。ViSK-GAT在验证和测试中分别实现了93.55%和93.89%的强性能，精确度为93.86%，F1得分为93.85%，Cohen's Kappa和Matthews相关系数为93%。回归结果也表明预测概率分布的均方根误差较低，为0.1205，相应的平均绝对误差为0.0156。与九种流行的迁移学习骨干相比，ViSK-GAT始终优于先前的方法。ViSK-GAT模型推进了人工智能的实施和应用，改变了肌肉骨骼风险分类，并能够在体育领域实现有影响力的早期干预。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Musculoskeletal disorders pose significant risks to athletes, and assessingrisk early is important for prevention. However, most existing methods aredesigned for controlled settings and fail to reliably assess risk in complexenvironments due to their reliance on a single type of data. This researchproposes ViSK-GAT (Visual-Skeletal Geometric Attention Transformer), a novelmultimodal deep learning framework designed to classify musculoskeletal riskusing visual and skeletal coordinate-based features. In addition, a custommultimodal dataset is constructed by combining visual data and skeletalcoordinates for risk assessment. Each sample is labeled into eight riskcategories based on the Rapid Entire Body Assessment system. ViSK-GAT combinesa Residual Block with a Lightweight Transformer Block to learn spatial andtemporal dependencies jointly. It incorporates two novel modules: theFine-Grained Attention Module (FGAM), which enables precise inter-modal featurerefinement through cross-attention between visual and skeletal inputs, and theMultimodal Geometric Correspondence Module (MGCM), which enhances cross-modalcoherence by aligning image features with coordinate-based representations.ViSK-GAT achieved strong performance with validation and test accuracies of93.55\% and 93.89\%, respectively; a precision of 93.86\%; an F1 score of93.85\%; and Cohen's Kappa and Matthews Correlation Coefficient of 93\%. Theregression results also indicated a low Root Mean Square Error of the predictedprobability distribution of 0.1205 and a corresponding Mean Absolute Error of0.0156. Compared to nine popular transfer learning backbones, ViSK-GATconsistently outperformed previous methods. The ViSK-GAT model advancesartificial intelligence implementation and application, transformingmusculoskeletal risk classification and enabling impactful early interventionsin sports.</description>
      <author>example@mail.com (Md. Abdur Rahman, Mohaimenul Azam Khan Raiaan, Tamanna Shermin, Md Rafiqul Islam, Mukhtar Hussain, Sami Azam)</author>
      <guid isPermaLink="false">2509.05913v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>SPINN: An Optimal Self-Supervised Physics-Informed Neural Network Framework</title>
      <link>http://arxiv.org/abs/2509.05886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发代理模型预测矩形微型散热器中液态钠流动的对流传热系数&lt;h4&gt;背景&lt;/h4&gt;使用计算流体动力学(CFD)对液态金属的湍流强制对流进行高保真建模既耗时又计算成本高&lt;h4&gt;目的&lt;/h4&gt;开发一种替代工具，用于液金属冷却的微型散热器的设计和优化&lt;h4&gt;方法&lt;/h4&gt;首先应用基于核的机器学习技术和浅层神经网络处理87个努塞尔数数据集，然后使用自监督物理信息神经网络和迁移学习方法提高估计性能。自监督物理信息神经网络中增加一层确定物理在损失函数中的权重，基于不确定性平衡数据和物理。迁移学习将针对水训练的浅层神经网络调整为用于钠&lt;h4&gt;主要发现&lt;/h4&gt;自监督物理信息神经网络成功估计了钠的传热率，误差约为+8%；仅使用物理进行回归时，误差保持在5%到10%之间；其他机器学习方法预测误差主要在+8%以内&lt;h4&gt;结论&lt;/h4&gt;基于机器学习的模型为液金属冷却的微型散热器的设计和优化提供了强大的替代工具&lt;h4&gt;翻译&lt;/h4&gt;开发了一种代理模型来预测矩形微型散热器中液态钠流动的对流传热系数。最初，将基于核的机器学习技术和浅层神经网络应用于包含87个矩形微型散热器中液态钠努塞尔数的数据集。随后，使用自监督物理信息神经网络和迁移学习方法提高估计性能。在自监督物理信息神经网络中，增加了一层来确定物理在损失函数中的权重，基于不确定性平衡数据和物理以获得更好的估计。对于迁移学习，将针对水训练的浅层神经网络调整为用于钠。验证结果表明，自监督物理信息神经网络成功估计了钠的传热率，误差约为+8%。仅使用物理进行回归时，误差保持在5%到10%之间。其他机器学习方法预测误差主要在+8%以内。使用计算流体动力学(CFD)对液态金属的湍流强制对流进行高保真建模既耗时又计算成本高。因此，基于机器学习的模型为液金属冷却的微型散热器的设计和优化提供了强大的替代工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A surrogate model is developed to predict the convective heat transfercoefficient of liquid sodium (Na) flow within rectangular miniature heat sinks.Initially, kernel-based machine learning techniques and shallow neural networkare applied to a dataset with 87 Nusselt numbers for liquid sodium inrectangular miniature heat sinks. Subsequently, a self-supervisedphysics-informed neural network and transfer learning approach are used toincrease the estimation performance. In the self-supervised physics-informedneural network, an additional layer determines the weight the of physics in theloss function to balance data and physics based on their uncertainty for abetter estimation. For transfer learning, a shallow neural network trained onwater is adapted for use with Na. Validation results show that theself-supervised physics-informed neural network successfully estimate the heattransfer rates of Na with an error margin of approximately +8%. Using onlyphysics for regression, the error remains between 5% to 10%. Other machinelearning methods specify the prediction mostly within +8%. High-fidelitymodeling of turbulent forced convection of liquid metals using computationalfluid dynamics (CFD) is both time-consuming and computationally expensive.Therefore, machine learning based models offer a powerful alternative tool forthe design and optimization of liquid-metal-cooled miniature heat sinks.</description>
      <author>example@mail.com (Reza Pirayeshshirazinezhad)</author>
      <guid isPermaLink="false">2509.05886v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Brain Tumor Detection Through Diverse CNN Architectures in IoT Healthcare Industries: Fast R-CNN, U-Net, Transfer Learning-Based CNN, and Fully Connected CNN</title>
      <link>http://arxiv.org/abs/2509.05821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用人工智能深度学习技术在物联网医疗系统中进行脑肿瘤诊断，通过多种卷积神经网络架构对MRI图像进行分析，实现了高准确率的脑肿瘤分类。&lt;h4&gt;背景&lt;/h4&gt;人工智能驱动的深度学习在物联网医疗系统中推动了脑肿瘤诊断的发展。大脑健康对人类生活至关重要，而磁共振成像(MRI)为脑肿瘤检测提供了关键数据，是人工智能驱动图像分类的主要大数据来源。&lt;h4&gt;目的&lt;/h4&gt;使用MRI图像对胶质瘤、脑膜瘤和垂体肿瘤进行分类，并评估不同人工智能模型在脑肿瘤诊断中的性能。&lt;h4&gt;方法&lt;/h4&gt;使用基于区域的卷积神经网络(R-CNN)和UNet架构进行分类，同时应用卷积神经网络(CNN)和基于CNN的迁移学习模型如Inception-V3、EfficientNetB4和VGG19。模型性能通过F-score、召回率、精确度和准确率进行评估，并进行外部队列跨数据集验证。&lt;h4&gt;主要发现&lt;/h4&gt;Fast R-CNN取得了最佳结果，准确率达到99%，F得分为98.5%，曲线下面积为99.5%，召回率为99.4%，精确度为98.5%。在外部队列跨数据集验证中，EfficientNetB2表现最强，精确率达到92.11%，召回率为92.11%，特异性为95.96%，F1得分为92.02%，准确率为92.23%。&lt;h4&gt;结论&lt;/h4&gt;结合R-CNN、UNet和迁移学习可以在物联网医疗系统中实现更早的诊断和更有效的治疗。这些发现强调了人工智能模型在处理多样化数据时的稳健性和可靠性，有潜力增强物联网医疗环境中的脑肿瘤分类和患者护理。&lt;h4&gt;翻译&lt;/h4&gt;人工智能驱动的深度学习在物联网医疗系统中推动了脑肿瘤诊断的发展，在大数据集上实现了高准确性。大脑健康对人类生活至关重要，准确诊断对有效治疗必不可少。磁共振成像为脑肿瘤检测提供了关键数据，是人工智能驱动图像分类的主要大数据来源。在本研究中，我们使用基于区域的卷积神经网络和UNet架构从MRI图像中对胶质瘤、脑膜瘤和垂体肿瘤进行分类。我们还应用了卷积神经网络和基于CNN的迁移学习模型，如Inception-V3、EfficientNetB4和VGG19。使用F-score、召回率、精确度和准确率评估模型性能。Fast R-CNN取得了最佳结果，准确率为99%，F得分为98.5%，曲线下面积为99.5%，召回率为99.4%，精确度为98.5%。结合R-CNN、UNet和迁移学习可以在物联网医疗系统中实现更早的诊断和更有效的治疗，改善患者预后。物联网设备如可穿戴监控设备和智能成像系统持续收集实时数据，人工智能算法分析这些数据以提供即时见解，实现及时干预和个性化护理。在外部队列跨数据集验证中，在微调的EfficientNet模型中，EfficientNetB2表现最强，精确度为92.11%，召回率/敏感度为92.11%，特异性为95.96%，F1得分为92.02%，准确率为92.23%。这些发现强调了人工智能模型在处理多样化数据时的稳健性和可靠性，强化了它们在增强物联网医疗环境中的脑肿瘤分类和患者护理方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI)-powered deep learning has advanced brain tumordiagnosis in Internet of Things (IoT)-healthcare systems, achieving highaccuracy with large datasets. Brain health is critical to human life, andaccurate diagnosis is essential for effective treatment. Magnetic ResonanceImaging (MRI) provides key data for brain tumor detection, serving as a majorsource of big data for AI-driven image classification. In this study, weclassified glioma, meningioma, and pituitary tumors from MRI images usingRegion-based Convolutional Neural Network (R-CNN) and UNet architectures. Wealso applied Convolutional Neural Networks (CNN) and CNN-based transferlearning models such as Inception-V3, EfficientNetB4, and VGG19. Modelperformance was assessed using F-score, recall, precision, and accuracy. TheFast R-CNN achieved the best results with 99% accuracy, 98.5% F-score, 99.5%Area Under the Curve (AUC), 99.4% recall, and 98.5% precision. Combining R-CNN,UNet, and transfer learning enables earlier diagnosis and more effectivetreatment in IoT-healthcare systems, improving patient outcomes. IoT devicessuch as wearable monitors and smart imaging systems continuously collectreal-time data, which AI algorithms analyze to provide immediate insights fortimely interventions and personalized care. For external cohort cross-datasetvalidation, EfficientNetB2 achieved the strongest performance among fine-tunedEfficientNet models, with 92.11% precision, 92.11% recall/sensitivity, 95.96%specificity, 92.02% F1-score, and 92.23% accuracy. These findings underscorethe robustness and reliability of AI models in handling diverse datasets,reinforcing their potential to enhance brain tumor classification and patientcare in IoT healthcare environments.</description>
      <author>example@mail.com (Mohsen Asghari Ilani, Yaser M. Banad)</author>
      <guid isPermaLink="false">2509.05821v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-based Topology Optimization</title>
      <link>http://arxiv.org/abs/2509.05800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于Transformer的机器学习模型用于拓扑优化，通过类令牌机制将边界和加载条件直接嵌入域表示，实现了高效无迭代的拓扑生成。&lt;h4&gt;背景&lt;/h4&gt;拓扑优化可设计高效复杂结构，但传统迭代方法计算成本高且对初始条件敏感；现有机器学习方法要么仍为迭代式，要么难以达到真实性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种无迭代、高保真度的拓扑生成模型，通过类令牌机制将关键条件嵌入域表示，实现实时拓扑优化。&lt;h4&gt;方法&lt;/h4&gt;提出基于Transformer的机器学习模型，使用类令牌机制嵌入边界和加载条件，在静态和动态数据集上实现，采用迁移学习和FFT编码改进性能，引入辅助损失函数提高设计真实性和可制造性。&lt;h4&gt;主要发现&lt;/h4&gt;模型在合规性误差、体积分数误差、浮动材料百分比和负载差异误差等评估指标上表现良好，接近基于扩散模型的保真度。&lt;h4&gt;结论&lt;/h4&gt;所提模型实现了无迭代的高保真拓扑生成，为实时、高质量拓扑优化提供了重要进展。&lt;h4&gt;翻译&lt;/h4&gt;拓扑优化能够设计高效复杂的结构，但传统的迭代方法（如基于SIMP的方法）通常面临高计算成本和对初始条件敏感的问题。尽管机器学习方法最近在加速拓扑生成方面显示出潜力，但现有模型要么仍然是迭代的，要么难以匹配真实性能。在这项工作中，我们提出了一种基于Transformer的机器学习模型用于拓扑优化，通过类令牌机制将关键的边界和加载条件直接嵌入到令牌化的域表示中。我们在静态和动态数据集上实现了该模型，使用迁移学习和动态负载的FFT编码来改进在动态数据集上的性能。引入了辅助损失函数以提高生成设计的真实性和可制造性。我们对模型的性能进行了全面评估，包括合规性误差、体积分数误差、浮动材料百分比和负载差异误差，并将其与最先进的非迭代和迭代生成模型进行了基准测试。我们的结果表明，所提出的模型接近基于扩散模型的保真度，同时保持无迭代特性，为实时、高保真拓扑生成迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Topology optimization enables the design of highly efficient and complexstructures, but conventional iterative methods, such as SIMP-based approaches,often suffer from high computational costs and sensitivity to initialconditions. Although machine learning methods have recently shown promise foraccelerating topology generation, existing models either remain iterative orstruggle to match ground-truth performance. In this work, we propose atransformer-based machine learning model for topology optimization that embedscritical boundary and loading conditions directly into the tokenized domainrepresentation via a class token mechanism. We implement this model on staticand dynamic datasets, using transfer learning and FFT encoding of dynamic loadsto improve our performance on the dynamic dataset. Auxiliary loss functions areintroduced to promote the realism and manufacturability of the generateddesigns. We conduct a comprehensive evaluation of the model's performance,including compliance error, volume fraction error, floating materialpercentage, and load discrepancy error, and benchmark it againststate-of-the-art non-iterative and iterative generative models. Our resultsdemonstrate that the proposed model approaches the fidelity of diffusion-basedmodels while remaining iteration-free, offering a significant step towardreal-time, high-fidelity topology generation.</description>
      <author>example@mail.com (Aaron Lutheran, Srijan Das, Alireza Tabarraei)</author>
      <guid isPermaLink="false">2509.05800v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Causal Clustering for Conditional Average Treatment Effects Estimation and Subgroup Discovery</title>
      <link>http://arxiv.org/abs/2509.05775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print for camera ready version for IEEE EMBS BHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的框架，通过基于因果森林学习的核函数对个体进行聚类，以识别对干预措施有不同反应的亚群体，从而估计异质性治疗效果。&lt;h4&gt;背景&lt;/h4&gt;估计异质性治疗效果在个性化医疗、资源分配和政策评估等领域至关重要，但传统聚类方法与因果推断的结合有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够识别对干预措施反应不同的亚群体，从而实现更有针对性和有效决策的方法。&lt;h4&gt;方法&lt;/h4&gt;提出两步法框架：首先使用Robinson分解的正交化学习器估计去偏的条件平均治疗效果，生成编码样本间治疗反应相似性的核矩阵；然后应用核化聚类发现不同的、对治疗敏感的亚群体，并计算聚类级别的平均CATEs。&lt;h4&gt;主要发现&lt;/h4&gt;通过半合成和真实世界数据集的实验证明，该方法能有效捕捉有意义的治疗效果异质性。&lt;h4&gt;结论&lt;/h4&gt;将核化聚类作为残差-残差回归框架中的正则化形式，为异质性治疗效果估计提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;估计异质性治疗效果在个性化医疗、资源分配和政策评估等领域至关重要。核心挑战在于识别对不同干预措施有不同反应的亚群体，从而实现更有针对性和有效的决策。虽然聚类方法在无监督学习中已有深入研究，但它们与因果推断的结合仍然有限。我们提出了一种新颖的框架，该框架使用从因果森林中学习到的核函数来基于估计的治疗效果对个体进行聚类，从而揭示潜在的亚群结构。我们的方法包含两个主要步骤：首先，使用Robinson分解的正交化学习器估计去偏的条件平均治疗效果，产生一个编码样本间治疗反应相似性的核矩阵；其次，对该矩阵应用核化聚类来发现不同的、对治疗敏感的亚群体，并计算聚类级别的平均CATEs。我们将这个核化聚类步骤表述为残差-残差回归框架中的一种正则化形式。通过在半合成和真实世界数据集上的大量实验，以及消融研究和探索性分析，我们证明了我们的方法在捕捉有意义的治疗效果异质性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating heterogeneous treatment effects is critical in domains such aspersonalized medicine, resource allocation, and policy evaluation. A centralchallenge lies in identifying subpopulations that respond differently tointerventions, thereby enabling more targeted and effective decision-making.While clustering methods are well-studied in unsupervised learning, theirintegration with causal inference remains limited. We propose a novel frameworkthat clusters individuals based on estimated treatment effects using a learnedkernel derived from causal forests, revealing latent subgroup structures. Ourapproach consists of two main steps. First, we estimate debiased ConditionalAverage Treatment Effects (CATEs) using orthogonalized learners via theRobinson decomposition, yielding a kernel matrix that encodes sample-levelsimilarities in treatment responsiveness. Second, we apply kernelizedclustering to this matrix to uncover distinct, treatment-sensitivesubpopulations and compute cluster-level average CATEs. We present thiskernelized clustering step as a form of regularization within theresidual-on-residual regression framework. Through extensive experiments onsemi-synthetic and real-world datasets, supported by ablation studies andexploratory analyses, we demonstrate the effectiveness of our method incapturing meaningful treatment effect heterogeneity.</description>
      <author>example@mail.com (Zilong Wang, Turgay Ayer, Shihao Yang)</author>
      <guid isPermaLink="false">2509.05775v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding</title>
      <link>http://arxiv.org/abs/2509.05657v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LM-Searcher框架，利用大语言模型进行跨领域神经架构优化，无需大量领域特定适应。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在解决复杂优化问题（包括神经架构搜索）方面取得进展，但现有方法严重依赖提示工程和领域特定调优，限制了实用性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需大量领域特定适应的框架，利用LLMs进行跨领域神经架构优化。&lt;h4&gt;方法&lt;/h4&gt;提出NCode作为神经架构的通用数值字符串表示；将NAS问题重新表述为排序任务；使用基于修剪的子空间采样策略；构建包含广泛架构-性能对的数据集促进稳健学习。&lt;h4&gt;主要发现&lt;/h4&gt;LM-Searcher在领域内（如图像分类的CNN）和领域外（如分割和生成的LoRA配置）任务中都取得有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;LM-Searcher建立了灵活且可推广的基于LLM的架构搜索新范式，数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;h4&gt;翻译&lt;/h4&gt;最近大语言模型的进展为解决复杂优化问题（包括神经架构搜索NAS）开辟了新途径。然而，现有的LLM驱动的NAS方法严重依赖提示工程和领域特定调优，限制了它们在不同任务中的实用性和可扩展性。在这项工作中，我们提出了LM-Searcher，一个新框架，它利用LLMs进行跨领域神经架构优化，无需大量领域特定适应。我们方法的核心是NCode，一种神经架构的通用数值字符串表示，它支持跨领域架构编码和搜索。我们还重新将NAS问题表述为排序任务，训练LLMs使用基于修剪的子空间采样策略衍生的指令调优样本，从候选池中选择高性能架构。我们策划的数据集包含广泛的架构-性能对，促进了稳健和可转移的学习。全面的实验证明，LM-Searcher在领域内（例如，图像分类的CNN）和领域外（例如，分割和生成的LoRA配置）任务中都取得了有竞争力的性能，为灵活且可推广的基于LLM的架构搜索建立了新范式。数据集和模型将在https://github.com/Ashone3/LM-Searcher发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in Large Language Models (LLMs) has opened new avenues forsolving complex optimization problems, including Neural Architecture Search(NAS). However, existing LLM-driven NAS approaches rely heavily on promptengineering and domain-specific tuning, limiting their practicality andscalability across diverse tasks. In this work, we propose LM-Searcher, a novelframework that leverages LLMs for cross-domain neural architecture optimizationwithout the need for extensive domain-specific adaptation. Central to ourapproach is NCode, a universal numerical string representation for neuralarchitectures, which enables cross-domain architecture encoding and search. Wealso reformulate the NAS problem as a ranking task, training LLMs to selecthigh-performing architectures from candidate pools using instruction-tuningsamples derived from a novel pruning-based subspace sampling strategy. Ourcurated dataset, encompassing a wide range of architecture-performance pairs,encourages robust and transferable learning. Comprehensive experimentsdemonstrate that LM-Searcher achieves competitive performance in both in-domain(e.g., CNNs for image classification) and out-of-domain (e.g., LoRAconfigurations for segmentation and generation) tasks, establishing a newparadigm for flexible and generalizable LLM-based architecture search. Thedatasets and models will be released at https://github.com/Ashone3/LM-Searcher.</description>
      <author>example@mail.com (Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li)</author>
      <guid isPermaLink="false">2509.05657v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>An Analysis of Layer-Freezing Strategies for Enhanced Transfer Learning in YOLO Architectures</title>
      <link>http://arxiv.org/abs/2509.05490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究分析了YOLOv8和YOLOv10架构在不同层冻结策略下的性能，发现在资源受限环境中，最优冻结策略取决于数据特性而非通用规则。适当冻结可减少高达28%的GPU内存消耗，并在某些情况下超越全微调性能。&lt;h4&gt;背景&lt;/h4&gt;YOLO架构对实时目标检测至关重要，但在资源受限环境（如无人机）部署需要高效迁移学习。层冻结是常见技术，但其对当代YOLOv8和YOLOv10架构的具体影响尚未充分探索，特别是冻结深度、数据集特性和训练动态间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;填补层冻结策略对YOLOv8和YOLOv10架构影响的知识空白，提供冻结深度、数据集特性和训练动态间相互作用的详细分析，并为资源有限场景下的目标检测提供平衡迁移学习的实用、循证方法。&lt;h4&gt;方法&lt;/h4&gt;系统性地研究YOLOv8和YOLOv10变体在四个代表关键基础设施监控的挑战性数据集上的多种冻结配置，整合梯度行为分析（L2范数）和视觉解释（Grad-CAM）来提供不同冻结策略下训练动态的深入见解。&lt;h4&gt;主要发现&lt;/h4&gt;1. 没有通用最优冻结策略，最优策略取决于数据特性；2. 冻结骨干网络保留通用特征有效，较浅冻结更适合处理极端类别不平衡；3. 这些配置减少高达28%的GPU内存消耗；4. 在某些情况下实现超越全微调的mAP@50分数；5. 梯度分析显示适度冻结模型具有不同收敛模式。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了实证发现和选择冻结策略的实际指导，为资源有限场景下的目标检测提供了平衡迁移学习的实用、循证方法。&lt;h4&gt;翻译&lt;/h4&gt;YOLO架构对于实时目标检测至关重要。然而，在资源受限环境（如无人机）中部署它需要高效的迁移学习。尽管层冻结是一种常见技术，但各种冻结配置对当代YOLOv8和YOLOv10架构的具体影响仍未被探索，特别是关于冻结深度、数据集特性和训练动态之间的相互作用。本研究通过呈现层冻结策略的详细分析来填补这一知识空白。我们使用四个代表关键基础设施监控的具有挑战性的数据集，系统性地研究了YOLOv8和YOLOv10变体上的多种冻结配置。我们的方法整合了梯度行为分析（L2范数）和视觉解释（Grad-CAM），以提供不同冻结策略下训练动态的深入见解。我们的研究结果表明，没有通用的最优冻结策略，而是存在一种取决于数据特性的策略。例如，冻结骨干网络对于保留通用特征有效，而较浅的冻结更适合处理极端类别不平衡。与全微调相比，这些配置可减少高达28%的GPU内存消耗，并且在某些情况下实现了超越全微调的mAP@50分数。梯度分析证实了这些发现，显示适度冻结的模型具有不同的收敛模式。最终，这项工作提供了实证发现和选择冻结策略的实际指导。它为资源有限场景下的目标检测提供了平衡迁移学习的实用、循证方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3390/math13152539&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The You Only Look Once (YOLO) architecture is crucial for real-time objectdetection. However, deploying it in resource-constrained environments such asunmanned aerial vehicles (UAVs) requires efficient transfer learning. Althoughlayer freezing is a common technique, the specific impact of various freezingconfigurations on contemporary YOLOv8 and YOLOv10 architectures remainsunexplored, particularly with regard to the interplay between freezing depth,dataset characteristics, and training dynamics. This research addresses thisgap by presenting a detailed analysis of layer-freezing strategies. Wesystematically investigate multiple freezing configurations across YOLOv8 andYOLOv10 variants using four challenging datasets that represent criticalinfrastructure monitoring. Our methodology integrates a gradient behavioranalysis (L2 norm) and visual explanations (Grad-CAM) to provide deeperinsights into training dynamics under different freezing strategies. Ourresults reveal that there is no universal optimal freezing strategy but,rather, one that depends on the properties of the data. For example, freezingthe backbone is effective for preserving general-purpose features, while ashallower freeze is better suited to handling extreme class imbalance. Theseconfigurations reduce graphics processing unit (GPU) memory consumption by upto 28% compared to full fine-tuning and, in some cases, achieve mean averageprecision (mAP@50) scores that surpass those of full fine-tuning. Gradientanalysis corroborates these findings, showing distinct convergence patterns formoderately frozen models. Ultimately, this work provides empirical findings andpractical guidelines for selecting freezing strategies. It offers a practical,evidence-based approach to balanced transfer learning for object detection inscenarios with limited resources.</description>
      <author>example@mail.com (Andrzej D. Dobrzycki, Ana M. Bernardos, José R. Casar)</author>
      <guid isPermaLink="false">2509.05490v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:10 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reactive Policy: Learning Reactive Manipulator Motion Planning for Dynamic Environments</title>
      <link>http://arxiv.org/abs/2509.06953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website at \url{deep-reactive-policy.com}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Deep Reactive Policy (DRP)的视觉-运动神经运动政策，用于在动态环境中实现无碰撞运动生成。DRP直接在点云感官输入上运行，结合了基于transformer的神经运动政策IMPACT和DCP-RMP模块，实现了强大的泛化能力，在模拟和现实世界环境中都表现出色。&lt;h4&gt;背景&lt;/h4&gt;在动态、部分可观察环境中生成无碰撞运动是机械臂的基本挑战。经典运动规划器可以计算全局最优轨迹但需要完整环境知识且速度慢，神经运动政策直接在原始感官输入上运行但在复杂或动态环境中难以泛化。&lt;h4&gt;目的&lt;/h4&gt;设计一种反应式神经运动政策，能够在多种动态环境中直接从点云感官输入生成无碰撞运动。&lt;h4&gt;方法&lt;/h4&gt;DRP的核心是基于transformer的神经运动政策IMPACT，在10百万条专家轨迹上预训练，并通过迭代式学生-教师微调改进静态障碍物避免能力。此外，使用DCP-RMP模块在推理时增强动态障碍物避免能力。&lt;h4&gt;主要发现&lt;/h4&gt;DRP在具有杂乱场景、动态移动障碍物和目标阻塞的挑战性任务上实现了强大的泛化能力，在模拟和现实世界环境中，成功率都超过了先前的经典方法和神经方法。&lt;h4&gt;结论&lt;/h4&gt;DRP是一种有效的解决方案，能够在动态环境中实现无碰撞运动生成，并且具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在动态、部分可观察的环境中生成无碰撞运动是机械臂的基本挑战。经典运动规划器可以计算全局最优轨迹，但需要完整的环境知识，并且在动态场景中通常太慢。神经运动政策通过直接在原始感官输入上闭环运行提供了一种有前途的替代方案，但在复杂或动态环境中往往难以泛化。我们提出了Deep Reactive Policy (DRP)，这是一种视觉-运动神经运动政策，专为在多种动态环境中进行反应式运动生成而设计，直接在点云感官输入上运行。其核心是IMPACT，一种基于transformer的神经运动政策，在10百万条跨多种模拟场景生成的专家轨迹上进行了预训练。我们通过迭代式学生-教师微调进一步改进了IMPACT的静态障碍物避免能力。此外，我们使用DCP-RMP（一种局部反应式目标提议模块）在推理时增强了政策的动态障碍物避免能力。我们在具有杂乱场景、动态移动障碍物和目标阻塞的具有挑战性的任务上评估了DRP。DRP实现了强大的泛化能力，在模拟和现实世界环境中，成功率都超过了先前的经典方法和神经方法。视频结果和代码可在https://deep-reactive-policy.com获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在动态、部分可观测环境中为机械臂生成无碰撞运动轨迹的问题。这个问题在现实中很重要，因为机器人需要在人类自然环境中（如家庭和厨房）安全导航，而传统运动规划方法需要完整环境知识且反应太慢，神经运动策略又难以在复杂或动态环境中泛化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统规划方法需要完整环境知识且速度慢；神经策略难以泛化；混合方法需要测试时间优化，牺牲了反应性。然后设计Deep Reactive Policy (DRP)，结合了大规模预训练、迭代学生-教师微调和动态障碍避免模块。作者借鉴了cuRobo作为专家规划器、Geometric Fabrics作为教师策略、RMP设计了DCP-RMP模块，并使用PointNet++处理点云数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合全局规划和局部反应控制的优势，通过大规模预训练学习通用运动规划能力，使用微调改进局部障碍避免，并在推理时添加轻量级反应模块处理动态障碍。整体流程包括：1) 大规模运动预训练：使用cuRobo生成1000万个专家轨迹训练IMPACT；2) 迭代学生-教师微调：结合IMPACT和Geometric Fabrics改进静态障碍避免；3) 动态障碍避免：添加DCP-RMP模块处理动态障碍；4) 执行：DCP-RMP先处理动态障碍，IMPACT生成动作序列供低级控制器实时执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 大规模运动预训练：使用1000万个专家轨迹，提高了泛化能力；2) 迭代学生-教师微调：结合全局规划和局部控制，改进静态障碍避免；3) DCP-RMP模块：直接从点云操作的反应性目标提议，处理动态障碍；4) 端到端的视觉运动策略：直接从点云输入生成动作。相比之前的工作，DRP不需要完整环境知识，能处理动态场景；比纯神经方法有更好的泛化能力；比混合方法不需要测试时间优化，保持实时反应性；比纯反应方法具有全局场景意识，不易陷入局部最优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Deep Reactive Policy通过结合大规模预训练的Transformer策略、迭代学生-教师微调和动态障碍避免模块，实现了在复杂动态环境中实时生成无碰撞轨迹的能力，显著优于传统和现有的神经运动规划方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating collision-free motion in dynamic, partially observableenvironments is a fundamental challenge for robotic manipulators. Classicalmotion planners can compute globally optimal trajectories but require fullenvironment knowledge and are typically too slow for dynamic scenes. Neuralmotion policies offer a promising alternative by operating in closed-loopdirectly on raw sensory inputs but often struggle to generalize in complex ordynamic settings. We propose Deep Reactive Policy (DRP), a visuo-motor neuralmotion policy designed for reactive motion generation in diverse dynamicenvironments, operating directly on point cloud sensory input. At its core isIMPACT, a transformer-based neural motion policy pretrained on 10 milliongenerated expert trajectories across diverse simulation scenarios. We furtherimprove IMPACT's static obstacle avoidance through iterative student-teacherfinetuning. We additionally enhance the policy's dynamic obstacle avoidance atinference time using DCP-RMP, a locally reactive goal-proposal module. Weevaluate DRP on challenging tasks featuring cluttered scenes, dynamic movingobstacles, and goal obstructions. DRP achieves strong generalization,outperforming prior classical and neural methods in success rate across bothsimulated and real-world settings. Video results and code available athttps://deep-reactive-policy.com</description>
      <author>example@mail.com (Jiahui Yang, Jason Jingzhou Liu, Yulong Li, Youssef Khaky, Kenneth Shaw, Deepak Pathak)</author>
      <guid isPermaLink="false">2509.06953v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Cortex-Synth: Differentiable Topology-Aware 3D Skeleton Synthesis with Hierarchical Graph Attention</title>
      <link>http://arxiv.org/abs/2509.06705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Cortex Synth是一个从单张2D图像联合合成3D骨架几何和拓扑的端到端可微分框架，具有三个关键创新点和四个协同模块，在多个指标上达到最先进结果，并具有广泛的应用前景。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉和3D重建领域，从2D图像生成3D骨架几何和拓扑是一个具有挑战性的任务，需要同时考虑几何形状和拓扑结构。&lt;h4&gt;目的&lt;/h4&gt;开发一个端到端可微分框架，能够从单张2D图像联合合成3D骨架的几何和拓扑结构，提高合成精度并减少拓扑错误。&lt;h4&gt;方法&lt;/h4&gt;提出Cortex Synth框架，包含三个关键创新：(1)具有多尺度骨架细化的分层图注意力机制；(2)通过拉普拉斯特征分解的可微分谱拓扑优化；(3)用于姿态结构对抗性几何一致性训练。框架集成了四个协同模块：伪3D点云生成器、增强的PointNet编码器、骨架坐标解码器和新型可微分图构建网络(DGCN)。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet数据集上，该方法实现了最先进的结果，MPJPE提高了18.7%，图编辑距离提高了27.3%，与之前的方法相比拓扑错误减少了42%。&lt;h4&gt;结论&lt;/h4&gt;Cortex Synth框架通过端到端可微分设计，实现了从2D图像到3D骨架几何和拓扑的高质量合成，为机器人操作、医学成像和自动角色绑定等领域提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出Cortex Synth，一种用于从单张2D图像联合合成3D骨架几何和拓扑的新型端到端可微分框架。我们的架构引入了三个关键创新：(1)具有多尺度骨架细化的分层图注意力机制，(2)通过拉普拉斯特征分解的可微分谱拓扑优化，(3)用于姿态结构对抗性几何一致性训练。该框架集成了四个协同模块：伪3D点云生成器、增强的PointNet编码器、骨架坐标解码器以及新型可微分图构建网络(DGCN)。我们的实验表明，在ShapeNet上实现了最先进的结果，MPJPE提高了18.7%，图编辑距离提高了27.3%，与之前的方法相比拓扑错误减少了42%。该模型的端到端可微分性使其在机器人操作、医学成像和自动角色绑定中有应用前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从2D图像生成3D骨架时，传统方法存在的三个关键限制：非可微分骨架化管道阻止端到端优化、固定拓扑先验限制跨类别泛化能力、几何和拓扑分离优化导致结构不一致。这个问题在现实中非常重要，因为准确的3D骨架表示对机器人操作、医学成像和计算机图形学等领域至关重要，能帮助机器人理解物体结构进行抓取、辅助医生进行手术规划、以及为动画角色提供自动绑定等。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者针对传统方法的局限性，设计了一个端到端可微分框架，整合了多个现有工作的优点：借鉴了图注意力网络(GAT)用于多尺度骨架细化，采用拉普拉斯特征分解实现可微分谱拓扑优化，使用对抗训练确保几何一致性，并基于PointNet++进行点云编码。作者将这些技术整合到四个协同模块中：伪3D点云生成器、增强的PointNet++编码器、骨架坐标解码器和可微分图构造网络(DGCN)，形成一个完整的从2D图像到3D骨架的生成流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过可微分框架联合优化3D骨架的几何和拓扑结构，使用分层图注意力机制捕捉多尺度特征，并通过谱拓扑优化保持结构一致性。整体流程是：首先通过图像处理模块将2D RGB图像转换为伪3D点云；然后使用增强的PointNet++编码器提取分层特征；接着通过骨架解码器预测初始关节位置；最后通过DGCN模块使用图注意力和光谱约束优化骨架拓扑，生成最终的3D骨架表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：(1)分层图注意力机制实现多尺度骨架细化；(2)可微分谱拓扑优化通过拉普拉斯特征分解确保结构一致性；(3)对抗几何一致性训练使用双判别器实现姿态-结构对齐；(4)自适应骨架复杂性基于结构熵动态分配节点。相比之前工作，Cortex-Synth的主要不同在于实现了完全端到端可微分框架，不依赖固定拓扑先验，能够联合优化几何和拓扑，实验显示在ShapeNet上MPJPE提高18.7%，图编辑距离降低27.3%，拓扑错误减少42%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Cortex-Synth通过引入分层图注意力和可微分谱拓扑优化，实现了从单张2D图像到3D骨架的高质量端到端合成，显著提高了骨架几何和拓扑的准确性，为机器人操作、医学成像和计算机图形学等领域提供了更强大的3D理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Cortex Synth, a novel end-to-end differentiable framework forjoint 3D skeleton geometry and topology synthesis from single 2D images. Ourarchitecture introduces three key innovations: (1) A hierarchical graphattention mechanism with multi-scale skeletal refinement, (2) Differentiablespectral topology optimization via Laplacian eigen decomposition, and (3)Adversarial geometric consistency training for pose structure alignment. Theframework integrates four synergistic modules: a pseudo 3D point cloudgenerator, an enhanced PointNet encoder, a skeleton coordinate decoder, and anovel Differentiable Graph Construction Network (DGCN). Our experimentsdemonstrate state-of-the-art results with 18.7 percent improvement in MPJPE and27.3 percent in Graph Edit Distance on ShapeNet, while reducing topologicalerrors by 42 percent compared to previous approaches. The model's end-to-enddifferentiability enables applications in robotic manipulation, medicalimaging, and automated character rigging.</description>
      <author>example@mail.com (Mohamed Zayaan S)</author>
      <guid isPermaLink="false">2509.06705v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>ISAC Imaging by Channel State Information using Ray Tracing for Next Generation 6G</title>
      <link>http://arxiv.org/abs/2509.06672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于信道状态信息的综合感知与通信成像框架，利用多径分量的角度和延迟信息实现三维物体重建，并通过两段反射点优化算法精确估计路径长度。&lt;h4&gt;背景&lt;/h4&gt;综合感知与通信作为第六代无线系统的基石技术，通过共享硬件、频谱和波形实现连接与环境映射的统一。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用CSI路径分量、发射器和接收器位置的ISAC成像框架，实现物体表面的精确几何重建。&lt;h4&gt;方法&lt;/h4&gt;从6.75 GHz频段的NYURay射线追踪器获取数据，提取可分辨多径分量并转换为三维反射点，采用两段反射点优化算法独立估计路径长度，最后聚合多对收发位置的反射点生成密集三维点云。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的ISAC成像框架能够准确重建物体表面、边缘和曲线特征，实现了多跳ISAC成像。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次使用6.75 GHz无线射线追踪实现多跳ISAC成像的演示。&lt;h4&gt;翻译&lt;/h4&gt;综合感知与通信正成为第六代无线系统的基石技术，通过共享硬件、频谱和波形统一连接和环境映射。本文提出了一种利用从校准的NYURay射线追踪器在6.75 GHz上频段获取的信道状态信息路径分量、发射器位置和接收器位置的ISAC成像框架。我们的研究展示了如何从CSI估计中提取每个可分辨的多径分量，并通过融合其角度和延迟信息将其转换为等效的三维反射点，这对于多跳反射是有用且具有挑战性的。论文的主要贡献是两段反射点优化算法，该算法独立估计从发射器位置和接收器位置到物体表面上等效反射点的路径长度，从而实现精确的几何重建。随后，我们聚合从多对发射器和接收器位置导出的等效反射点，生成表示信道中物体的密集三维点云。实验结果验证了所提出的ISAC成像框架能够准确重建物体表面、边缘和曲线特征。据我们所知，本文首次展示了使用6.75 GHz无线射线追踪进行多跳ISAC成像。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用无线信道状态信息(CSI)进行高精度3D环境重建，特别是处理多跳反射场景下的成像问题。这个问题在6G时代至关重要，因为ISAC(集成感知与通信)是6G的核心技术，需要通过共享硬件、频谱和波形来实现通信与环境感知的统一。高精度环境地图对自动驾驶、数字孪生和XR服务等6G关键应用不可或缺，而无线信号能穿透障碍物、全天候工作，弥补光学成像的不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统CSI成像方法通常假设单次反射，难以处理复杂物体中的多跳反射问题。他们提出将多跳反射路径抽象为'等效反射点'(ERPs)的概念，避免追踪每次反射的具体位置。设计了独立估计发射段和接收段路径长度的两段式优化算法。借鉴了NYURay射线追踪引擎、通信系统中的CSI估计技术、SAR成像思路以及点云处理中的几何滤波技术。通过多视角融合(MVF)整合不同发射-接收位置的数据，形成完整的3D点云。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将复杂环境中的多跳反射路径抽象为单个等效反射点(ERP)，利用无线信道的角度和延迟信息重建物体几何形状，并通过多视角融合提高成像质量和覆盖率。实现流程：1)使用NYURay生成不同TX-RX位置的CSI数据；2)提取每条路径的六元组参数(角度、延迟、增益)；3)应用两段式反射点优化算法计算ERP；4)几何滤波剔除异常值；5)多视角融合生成密集3D点云；6)后处理优化点云质量，生成最终RF图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次实现多跳反射的ISAC成像；2)提出两段式反射点优化算法；3)开发多视角融合框架；4)在6.75GHz中频段验证方法；5)利用校准的NYURay生成高质量数据。相比之前工作：突破单次反射假设限制；在更高频段验证；仅利用通信系统CSI；能处理更复杂物体形状；通过多视角融合提供更完整3D重建，减少单视角盲区。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于信道状态信息的多跳反射等效反射点优化和多视角融合方法，实现了在6.75GHz频段上对复杂物体的高精度无线射频成像，为6G通信与感知一体化系统提供了新的环境感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrated sensing and communications (ISAC) is emerging as a cornerstonetechnology for sixth generation (6G) wireless systems, unifying connectivityand environmental mapping through shared hardware, spectrum, and waveforms. Thefollowing paper presents an ISAC imaging framework utilizing channel stateinformation (CSI) per-path components, transmitter (TX) positions, and receiver(RX) positions obtained from the calibrated NYURay ray tracer at 6.75 GHz inthe upper mid-band. Our work shows how each resolvable multipath component canbe extracted from CSI estimation and cast into an equivalent three-dimensionalreflection point by fusing its angle and delay information, which is useful andchallenging for multi-bounce reflections. The primary contribution of the paperis the two-segment reflection point optimization algorithm, which independentlyestimates the path lengths from the TX position and RX position to anequivalent reflection point (ERP) on the object surface, thus enabling precisegeometric reconstruction. Subsequently, we aggregate the ERPs derived frommultiple pairs of TX and RX positions, generating dense three dimensional pointclouds representing the objects in the channel. Experimental results validatethat the proposed ISAC imaging framework accurately reconstructs objectsurfaces, edges, and curved features. To the best of our knowledge, this paperprovides the first demonstration of multi bounce ISAC imaging using wirelessray tracing at 6.75 GHz.</description>
      <author>example@mail.com (Ahmad Bazzi, Mingjun Ying, Ojas Kanhere, Theodore S. Rappaport, Marwa Chafii)</author>
      <guid isPermaLink="false">2509.06672v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>LiHRA: A LiDAR-Based HRI Dataset for Automated Risk Monitoring Methods</title>
      <link>http://arxiv.org/abs/2509.06597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint of final paper that will appear in the Proceedings of the  IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了LiHRA，一个用于促进人类-机器人交互场景中自动化、基于学习或经典风险监控方法开发的新型数据集。&lt;h4&gt;背景&lt;/h4&gt;协作机器人在工业环境中日益普及，增加了对可靠安全系统的需求。然而，缺乏捕捉真实人类-机器人交互（包括潜在危险事件）的高质量数据集，阻碍了相关方法的开发。&lt;h4&gt;目的&lt;/h4&gt;创建LiHRA数据集，为人类-机器人交互的风险监控研究提供全面、多模态的数据支持。&lt;h4&gt;方法&lt;/h4&gt;LiHRA是一个综合的多模态数据集，结合了3D LiDAR点云、人体关键点和机器人关节状态，捕捉人类-机器人协作的完整空间和动态上下文。该数据集涵盖六种代表性HRI场景，包括协作和共存任务、物体传递和表面抛光，每种场景都有安全和危险版本，共包含4,431个标记的点云，以10Hz频率记录。&lt;h4&gt;主要发现&lt;/h4&gt;LiHAR数据集通过结合多种模态，能够精确跟踪人体运动、机器人动作和环境条件，实现协作任务中的准确风险监控。作者还展示了一种利用机器人状态和机器人动态模型来量化每个场景风险水平的方法。&lt;h4&gt;结论&lt;/h4&gt;LiHAR凭借其高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件组合，为未来研究人类-机器人工作空间中的实时风险监控和自适应安全策略提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LiHRA，这是一个新型数据集，旨在促进人类-机器人交互场景中自动化、基于学习或经典风险监控方法的发展。工业环境中协作机器人的日益普及增加了对可靠安全系统的需求。然而，缺乏捕捉真实人类-机器人交互（包括潜在危险事件）的高质量数据集，减缓了开发进程。LiHAR通过提供结合3D LiDAR点云、人体关键点和机器人关节状态的全面多模态数据集，解决了这一挑战，捕捉了人类-机器人协作的完整空间和动态上下文。这种模态组合允许精确跟踪人体运动、机器人动作和环境条件，实现协作任务中的准确风险监控。LiHAR数据集涵盖六种代表性HRI场景，包括协作和共存任务、物体传递和表面抛光，每种场景都有安全和危险版本。该数据集共包含4,431个以10Hz频率记录的标记点云，为训练和评估经典和AI驱动的风险监控算法提供了丰富的资源。最后，为了展示LiHAR的实用性，我们介绍了一种量化每个场景随时间风险水平的方法。该方法利用上下文信息，包括机器人状态和机器人动态模型。凭借其高分辨率LiDAR数据、精确的人体跟踪、机器人状态数据和真实的碰撞事件组合，LiHAR为未来研究人类-机器人工作空间中的实时风险监控和自适应安全策略提供了重要基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏高质量数据集来捕捉真实人机交互场景（包括潜在危险事件）的问题。这个问题很重要，因为随着协作机器人在工业环境中的普及，对可靠安全系统的需求增加，而现有风险评估方法依赖于专家主观评估，难以应对人机交互的复杂性、预测人类行为的挑战以及碰撞临界值估计的困难，阻碍了自动化安全系统的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集无法满足人机交互风险评估需求，然后选择3D LiDAR传感器作为核心感知方式（借鉴了自动驾驶领域的经验），结合HTC VIVE Tracker 3.0捕捉人体关键点和Franka Emika Robot记录机器人状态，构建了多模态数据集。设计过程中借鉴了ISO/TS 15066:2016安全标准，并利用现有文献中的经典方法估计外力，同时创新性地整合了多种传感器数据来捕捉人机交互的完整空间和动态上下文。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个结合3D LiDAR点云、人体关键点和机器人关节状态的多模态数据集，实现对人机协作场景的精确跟踪和风险监控。整体流程包括：1)使用专业硬件采集数据(LiDAR、运动捕捉系统和协作机器人)；2)对各传感器系统进行精确校准；3)记录六种不同场景(每种有安全/危险版本)；4)开发风险监控方法量化风险水平；5)通过外部力估计评估碰撞严重性；6)将数据导出为标准格式便于后续分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个集成LiDAR点云、机器人关节状态和人体关键点的数据集；2)包含六种代表性场景的安全和危险版本；3)提供精确标记的人机交互动态数据；4)提出基于安全标准的自动化风险监控方法。相比之前工作，LiHRA更全面(包含多种传感器数据)、更实用(包含危险场景)、更符合工业需求(使用3D LiDAR而非RGB相机)，解决了现有数据集要么缺乏机器人数据、要么缺乏危险场景、要么使用不合适传感器的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiHRA数据集通过整合多模态传感器数据和危险场景，为开发人机交互环境中的自动化风险评估和风险监控方法提供了全面且实用的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LiHRA, a novel dataset designed to facilitate the development ofautomated, learning-based, or classical risk monitoring (RM) methods forHuman-Robot Interaction (HRI) scenarios. The growing prevalence ofcollaborative robots in industrial environments has increased the need forreliable safety systems. However, the lack of high-quality datasets thatcapture realistic human-robot interactions, including potentially dangerousevents, slows development. LiHRA addresses this challenge by providing acomprehensive, multi-modal dataset combining 3D LiDAR point clouds, human bodykeypoints, and robot joint states, capturing the complete spatial and dynamiccontext of human-robot collaboration. This combination of modalities allows forprecise tracking of human movement, robot actions, and environmentalconditions, enabling accurate RM during collaborative tasks. The LiHRA datasetcovers six representative HRI scenarios involving collaborative and coexistenttasks, object handovers, and surface polishing, with safe and hazardousversions of each scenario. In total, the data set includes 4,431 labeled pointclouds recorded at 10 Hz, providing a rich resource for training andbenchmarking classical and AI-driven RM algorithms. Finally, to demonstrateLiHRA's utility, we introduce an RM method that quantifies the risk level ineach scenario over time. This method leverages contextual information,including robot states and the dynamic model of the robot. With its combinationof high-resolution LiDAR data, precise human tracking, robot state data, andrealistic collision events, LiHRA offers an essential foundation for futureresearch into real-time RM and adaptive safety strategies in human-robotworkspaces.</description>
      <author>example@mail.com (Frederik Plahl, Georgios Katranis, Ilshat Mamaev, Andrey Morozov)</author>
      <guid isPermaLink="false">2509.06597v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Cross3DReg: Towards a Large-scale Real-world Cross-source Point Cloud Registration Benchmark</title>
      <link>http://arxiv.org/abs/2509.06456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种跨源点云配准方法，构建了大型多模态数据集并设计了基于重叠区域的配准框架，有效解决了跨源配准中的挑战，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;跨源点云配准是3D视觉中的基础任务，但与同源配准相比面临两大挑战：缺乏大规模真实世界数据集用于训练深度模型；不同传感器采集的点云存在固有差异，导致特征提取和匹配困难，影响配准精度。&lt;h4&gt;目的&lt;/h4&gt;推进跨源点云配准研究，通过构建大型数据集和创新框架解决现有挑战，实现准确且鲁棒的跨源点云配准。&lt;h4&gt;方法&lt;/h4&gt;构建了Cross3DReg数据集（目前最大真实世界多模态跨源点云配准数据集）；设计了基于重叠区域的跨源配准框架，利用未对齐图像预测点云重叠区域；提出了视觉-几何注意力引导的匹配模块，通过融合图像和几何信息建立可靠对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;所提方法实现了最先进的配准性能，相对旋转误差降低63.2%，相对平移误差降低40.2%，配准召回率提高5.4%，有效验证了跨源配准的准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在实现准确的跨源配准方面是有效的，能够有效处理跨源点云配准中的挑战，显著提升配准精度和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;跨源点云配准旨在对齐来自不同传感器的点云数据，是3D视觉中的基础任务。然而，与同源点云配准相比，跨源配准面临两个核心挑战：缺乏用于训练深度配准模型的大规模真实世界公开数据集，以及多传感器捕获的点云存在固有差异。传感器引起的多样模式对鲁棒且准确的点云特征提取和匹配构成巨大挑战，进而影响配准精度。为推进该领域研究，我们构建了Cross3DReg，目前最大且真实世界的多模态跨源点云配准数据集，分别通过旋转机械激光雷达和混合半固态激光雷达采集。此外，我们设计了基于重叠区域的跨源配准框架，利用未对齐的图像预测源点云和目标点云之间的重叠区域，有效过滤掉无关区域的冗余点，并显著减轻非重叠区域噪声引起的干扰。然后，提出视觉-几何注意力引导的匹配模块，通过融合图像和几何信息增强跨源点云特征一致性，建立可靠对应关系，最终实现准确且鲁棒的配准。大量实验表明，我们的方法实现了最先进的配准性能。我们的框架将相对旋转误差和相对平移误差分别降低了63.2%和40.2%，并将配准召回率提高了5.4%，这验证了其在实现准确跨源配准方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决跨源点云配准问题，即对齐来自不同传感器（如旋转机械激光雷达和混合半固态激光雷达）的点云数据。这个问题在现实中非常重要，因为它是3D视觉的基础任务，在机器人导航、遥感测绘和自动驾驶定位等领域有广泛应用。然而，与同源点云配准相比，跨源配准面临缺乏大规模真实世界数据集和传感器差异导致的特征不一致两大挑战，这些问题限制了该领域的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到跨源点云配准的两个核心挑战：缺乏大规模真实世界数据集和传感器差异导致的特征不一致。为解决数据集问题，作者构建了Cross3DReg数据集；为解决配准精度问题，作者设计了基于重叠区域的配准框架。方法设计借鉴了多项现有工作：参考ImLoveNet预测重叠区域，使用KPConv-FPN提取点云特征，采用U-Net处理图像，并利用多模态特征融合和注意力机制增强特征一致性。作者通过融合视觉和几何信息，建立了可靠的点对应关系，实现了跨源点云的准确配准。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用未对齐的图像预测源点云和目标点云之间的重叠区域，有效过滤无关区域的冗余点，减轻非重叠区域噪声干扰；同时通过融合图像和几何信息，增强跨源点云特征一致性，建立可靠对应关系。整体流程分为四个阶段：1)特征提取，通过双分支网络提取图像和点云特征；2)重叠掩码预测(OMP)，融合图像和点云特征预测重叠区域；3)视觉-几何注意力引导的超点匹配(VGAM)，在重叠区域内建立对应关系；4)点匹配和配准，基于超点匹配获得点级对应关系并估计变换矩阵。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建了Cross3DReg，目前最大规模的真实世界跨源点云配准数据集，包含13,231个点云对；2)提出基于重叠区域的配准框架，利用未对齐图像预测点云重叠区域；3)设计视觉-几何注意力引导匹配模块，融合视觉和几何信息增强特征一致性。相比之前工作，不同之处在于：现有数据集规模小或主要使用合成数据，而Cross3DReg是真实世界大规模数据集；现有方法主要针对合成跨源设置设计，而作者方法专门针对真实世界跨源数据；实验显示作者方法在RRE和RTE上分别降低63.2%和40.2%，RR提高5.4%，性能显著提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Cross3DReg大规模真实世界跨源点云配准数据集和一种基于重叠区域的配准方法，通过融合视觉和几何信息实现了跨源点云的高精度对齐。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-source point cloud registration, which aims to align point cloud datafrom different sensors, is a fundamental task in 3D vision. However, comparedto the same-source point cloud registration, cross-source registration facestwo core challenges: the lack of publicly available large-scale real-worlddatasets for training the deep registration models, and the inherentdifferences in point clouds captured by multiple sensors. The diverse patternsinduced by the sensors pose great challenges in robust and accurate point cloudfeature extraction and matching, which negatively influence the registrationaccuracy. To advance research in this field, we construct Cross3DReg, thecurrently largest and real-world multi-modal cross-source point cloudregistration dataset, which is collected by a rotating mechanical lidar and ahybrid semi-solid-state lidar, respectively. Moreover, we design anoverlap-based cross-source registration framework, which utilizes unalignedimages to predict the overlapping region between source and target pointclouds, effectively filtering out redundant points in the irrelevant regionsand significantly mitigating the interference caused by noise innon-overlapping areas. Then, a visual-geometric attention guided matchingmodule is proposed to enhance the consistency of cross-source point cloudfeatures by fusing image and geometric information to establish reliablecorrespondences and ultimately achieve accurate and robust registration.Extensive experiments show that our method achieves state-of-the-artregistration performance. Our framework reduces the relative rotation error(RRE) and relative translation error (RTE) by $63.2\%$ and $40.2\%$,respectively, and improves the registration recall (RR) by $5.4\%$, whichvalidates its effectiveness in achieving accurate cross-source registration.</description>
      <author>example@mail.com (Zongyi Xu, Zhongpeng Lang, Yilong Chen, Shanshan Zhao, Xiaoshui Huang, Yifan Zuo, Yan Zhang, Qianni Zhang, Xinbo Gao)</author>
      <guid isPermaLink="false">2509.06456v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Approximations of the mean curvature, and the Buet-Rumpf approximate mean curvature flow</title>
      <link>http://arxiv.org/abs/2509.06438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文推广了B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及点云相关平均曲率运动的工作，提出了两种推广方式，并将结果扩展到近似第二基本形式，证明了点云在平均曲率运动中满足的额外比较原理。&lt;h4&gt;背景&lt;/h4&gt;基于B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及点云相关平均曲率运动的研究。&lt;h4&gt;目的&lt;/h4&gt;推广变分流形的近似平均曲率向量定义及其相关平均曲率运动的工作。&lt;h4&gt;方法&lt;/h4&gt;通过线性算子和变分流形的正则性两种方式推广近似平均曲率向量定义，并将结果扩展到近似第二基本形式。&lt;h4&gt;主要发现&lt;/h4&gt;点云在平均曲率运动中满足一些额外的比较原理，包括离散和连续两种情况。&lt;h4&gt;结论&lt;/h4&gt;成功推广了近似平均曲率向量定义并扩展到近似第二基本形式，证明了点云运动的比较原理。&lt;h4&gt;翻译&lt;/h4&gt;本文的目的是推广B. Buet和M. Rumpf关于变分流形的近似平均曲率向量定义及其相关点云平均曲率运动的工作。我们提出了近似平均曲率向量定义的两种推广方式：通过线性算子和通过变分流形的正则性。然后我们将结果扩展到近似第二基本形式。最后，我们证明了点云在平均曲率运动中满足的一些额外比较原理（包括离散和连续情况）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The aim of this paper is to generalize the work of B. Buet and M. Rumpf onsome definition of the approximate mean curvature vector for varifolds, and itsassociated mean curvature motions for points clouds. We propose ageneralization of the definition of the approximate mean curvature vector intwo terms: in terms of linear operators and in terms of regularity of thevarifold. We then extend the results to the approximate second fundamentalform. Finally, we prove some additional comparison principles satisfied by themotion of points cloud by mean curvature (in the discrete and the continuouscases).</description>
      <author>example@mail.com (Abdelmouksit Sagueni)</author>
      <guid isPermaLink="false">2509.06438v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Towards scalable organ level 3D plant segmentation: Bridging the data algorithm computing gap</title>
      <link>http://arxiv.org/abs/2509.06329v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文系统性地解决了植物3D分割领域的三大挑战，提供了现有数据集概述，总结了深度学习方法，开发了Plant Segmentation Studio开源框架，并通过实验验证了有效策略，为研究人员提供了实用工具和路线图。&lt;h4&gt;背景&lt;/h4&gt;植物形态学的精确表征为植物环境相互作用和遗传进化研究提供了有价值的见解。3D分割技术是从复杂点云中提取植物器官信息的关键技术，但在植物表型分析中的应用仍面临三大挑战：大规模标注数据集稀缺、难以将先进深度神经网络适应到植物点云、缺乏针对植物科学的标准基准和评估协议。&lt;h4&gt;目的&lt;/h4&gt;系统性地解决植物3D分割领域面临的三大挑战，提供现有3D植物数据集概述，总结基于深度学习的点云分割方法，介绍Plant Segmentation Studio开源框架，并评估代表性网络和模拟到现实的学习策略。&lt;h4&gt;方法&lt;/h4&gt;提供现有3D植物数据集的概述；系统总结基于深度学习的点云语义和实例分割方法；介绍Plant Segmentation Studio开源框架；进行广泛的定量实验评估代表性网络和模拟到现实的学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;稀疏卷积主干和基于Transformer的实例分割方法有效；基于建模和基于增强的合成数据生成在模拟到现实学习中具有互补作用，可以减少标注需求。&lt;h4&gt;结论&lt;/h4&gt;这项研究弥合了算法进步与实际部署之间的差距，为研究人员提供了即用工具，并为开发数据高效且可泛化的3D植物表型深度学习解决方案提供了路线图。&lt;h4&gt;翻译&lt;/h4&gt;植物形态学的精确表征为植物环境相互作用和遗传进化研究提供了有价值的见解。提取这一信息的关键技术是3D分割技术，它可以从复杂点云中勾勒出单个植物器官。尽管在一般3D计算机视觉领域取得了显著进展，但3D分割在植物表型分析中的应用仍然受到三大挑战的限制：大规模标注数据集的稀缺性；将先进的深度神经网络适应到植物点云的技术困难；缺乏针对植物科学的标准基准和评估协议。这篇论文通过以下方式系统性地解决了这些障碍：在一般3D分割领域的背景下提供现有3D植物数据集的概述；系统总结基于深度学习的点云语义和实例分割方法；介绍Plant Segmentation Studio开源框架；进行广泛的定量实验以评估代表性网络和模拟到现实的学习策略。我们的研究突出了稀疏卷积主干和基于Transformer的实例分割的有效性，同时强调基于建模和基于增强的合成数据生成在模拟到现实学习中的互补作用，可以减少标注需求。总体而言，这项研究弥合了算法进步与实际部署之间的差距，为研究人员提供了即用工具，并为开发数据高效且可泛化的3D植物表型深度学习解决方案提供了路线图。数据和代码可在https://github.com/perrydoremi/PlantSegStudio获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决植物器官级别3D分割规模化应用面临的三大挑战：大规模标注数据集稀缺、将先进深度神经网络适应植物点云的技术困难、以及缺乏针对植物科学定制的标准化基准和评估协议。这个问题在现实中非常重要，因为精确的植物形态表征对理解植物-环境相互作用和遗传进化至关重要，而器官级别的3D分割是提取这些形态信息的关键技术。解决这些问题可以促进植物表型分析的发展，支持农业可持续性和韧性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用'数据-算法-计算'三角框架系统性地解决问题：在数据层面，提供3D植物数据集概述并讨论真实数据采集和合成数据生成方法；在算法层面，总结深度学习点云分割方法并分析不同策略；在计算层面，开发Plant Segmentation Studio框架。作者确实借鉴了现有工作，特别是基于MMDetection3D框架进行定制，利用现有的深度学习架构如PointNet、3D CNN和Transformer，并结合程序建模和增强方法生成合成数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'数据-算法-计算'三角框架的系统整合，解决植物器官级别3D分割的三大挑战。整体流程包括：1)数据收集与处理，使用多种3D成像技术收集植物点云并进行标准化处理；2)算法设计与优化，设计适合植物点云特性的分割算法并优化神经网络架构；3)计算框架构建，开发PSS框架实现数据准备、算法集成和简化推理；4)评估与优化，在多个数据集上进行定量实验并优化算法参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'数据-算法-计算'三角框架系统性解决问题；2)全面分析3D植物数据集和合成数据生成方法；3)系统总结适合植物点云的深度学习方法，特别关注Transformer-based方法；4)开发首个针对3D植物表型分析的标准化框架PSS。相比之前工作，本文的不同之处在于全面性（整合三个层面而非单一层面）、针对性（专门针对植物器官级别需求）、实用性（提供可用工具框架）、定量评估（进行广泛实验）和开放性（提供开源代码）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性整合数据、算法和计算三个层面，开发了一个开源框架(PSS)来弥合植物器官级别3D分割中的数据-算法-计算差距，为研究人员提供了可扩展的植物表型分析工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The precise characterization of plant morphology provides valuable insightsinto plant environment interactions and genetic evolution. A key technology forextracting this information is 3D segmentation, which delineates individualplant organs from complex point clouds. Despite significant progress in general3D computer vision domains, the adoption of 3D segmentation for plantphenotyping remains limited by three major challenges: i) the scarcity oflarge-scale annotated datasets, ii) technical difficulties in adapting advanceddeep neural networks to plant point clouds, and iii) the lack of standardizedbenchmarks and evaluation protocols tailored to plant science. This reviewsystematically addresses these barriers by: i) providing an overview ofexisting 3D plant datasets in the context of general 3D segmentation domains,ii) systematically summarizing deep learning-based methods for point cloudsemantic and instance segmentation, iii) introducing Plant Segmentation Studio(PSS), an open-source framework for reproducible benchmarking, and iv)conducting extensive quantitative experiments to evaluate representativenetworks and sim-to-real learning strategies. Our findings highlight theefficacy of sparse convolutional backbones and transformer-based instancesegmentation, while also emphasizing the complementary role of modeling-basedand augmentation-based synthetic data generation for sim-to-real learning inreducing annotation demands. In general, this study bridges the gap betweenalgorithmic advances and practical deployment, providing immediate tools forresearchers and a roadmap for developing data-efficient and generalizable deeplearning solutions in 3D plant phenotyping. Data and code are available athttps://github.com/perrydoremi/PlantSegStudio.</description>
      <author>example@mail.com (Ruiming Du, Guangxun Zhai, Tian Qiu, Yu Jiang)</author>
      <guid isPermaLink="false">2509.06329v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>DCReg: Decoupled Characterization for Efficient Degenerate LiDAR Registration</title>
      <link>http://arxiv.org/abs/2509.06285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 19 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出DCReg框架，通过三个集成创新系统性解决LiDAR点云配准中的病态条件问题，显著提高定位精度和计算效率。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云配准是机器人感知和导航的基础，但在几何退化或狭窄环境中，配准问题会变成病态条件，导致解决方案不稳定和精度降低。现有方法未能准确检测、解释和解决这种病态条件问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性解决病态配准问题的框架，通过准确检测、解释和解决病态条件，提高配准的准确性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;DCReg框架包含三个创新：1) 使用Hessian矩阵的Schur补分解实现可靠的病态条件检测，将配准问题解耦为干净的旋转和平移子空间；2) 开发定量表征技术，建立数学特征空间与物理运动方向之间的明确映射；3) 设计新型预处理器，仅稳定识别出的病态方向，同时保留所有良好约束信息。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，DCReg在各种环境中实现了比最先进方法高20%-50%的定位精度，并且速度提高了5-100倍。&lt;h4&gt;结论&lt;/h4&gt;DCReg提供了一个有效且高效的解决方案，能够处理LiDAR点云配准中的病态条件问题，显著提高了定位精度和计算效率，代码将在https://github.com/JokerJohn/DCReg上提供。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR点云配准是机器人感知和导航的基础。然而，在几何退化或狭窄环境中，配准问题会变成病态条件，导致解决方案不稳定和精度降低。虽然现有方法试图处理这些问题，但它们未能解决核心挑战：准确检测、解释和解决这种病态条件，导致漏检或损坏的解决方案。在本研究中，我们引入了DCReg，一个通过三个集成创新系统性解决病态配准问题的原则性框架。首先，DCReg通过将Schur补分解应用于Hessian矩阵，实现可靠的病态条件检测。该技术将配准问题解耦为干净的旋转和平移子空间，消除了传统分析中掩盖退化模式的耦合效应。其次，在这些干净的子空间中，我们开发了定量表征技术，建立了数学特征空间与物理运动方向之间的明确映射，提供了关于哪些特定运动缺乏约束的可操作见解。最后，利用这个干净的子空间，我们设计了一个有针对性的缓解策略：一种新型预处理器，选择性稳定仅识别出的病态方向，同时保留可观测空间中的所有良好约束信息。这通过具有单一可物理解释参数的预共轭梯度方法实现了高效且稳健的优化。广泛的实验表明，DCReg在各种环境中实现了比最先进方法高20%-50%的定位精度，并且速度提高了5-100倍。我们的实现在https://github.com/JokerJohn/DCReg上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达点云配准在几何退化或狭窄环境（如走廊、隧道）中的病态问题。在这些环境中，由于重复结构或稀疏特征，系统缺乏沿特定运动方向的充分几何约束，导致信息矩阵接近奇异，使优化问题变得病态。这个问题在现实中非常重要，因为它是现代自主导航系统（如自动驾驶车辆、机器人）可靠性的关键瓶颈，在这些环境中即使微小的传感器误差或初始估计偏差也可能导致定位完全失败，严重影响实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法的三个根本局限性出发：1) 检测时机问题：传统方法分析Hessian谱时，旋转和平移间的尺度差异和耦合效应掩盖了关键病态模式；2) 失败原因问题：即使检测到病态，也无法解释哪些物理运动方向是退化的；3) 缓解策略问题：现有方法（如正则化、截断）不加选择地修改优化问题。作者借鉴了Schur补分解处理块矩阵耦合效应的思想，以及预调节共轭梯度(PCG)优化框架，但创新性地将它们组合成一个统一框架，通过解耦表征来系统解决病态配准问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过解耦表征（Decoupled Characterization）处理病态配准问题，使用Schur补分解将旋转和平移参数解耦到干净子空间中，消除耦合效应，然后进行定量表征和针对性缓解。整体流程分为三步：1) 病态检测：使用Schur补分解分析旋转和平移子空间，计算归一化特征值识别病态方向；2) 定量表征：通过内积匹配解决符号歧义，最大分量分析解决排序歧义，Gram-Schmidt正交化产生稳定正交基；3) 针对性缓解：设计基于PCG的求解器，使用特征值钳制策略只稳定病态方向，保持良好约束方向的自然收敛。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 尺度鲁棒且耦合感知的病态检测，使用Schur补分解正确处理旋转-平移耦合；2) 物理可解释的方向性病态分析，建立数学特征空间和物理运动方向的明确映射；3) 针对性病态缓解求解器，使用单一参数控制条件数。与传统方法不同：检测上，传统方法忽略耦合效应，DCReg揭示隐藏退化；表征上，传统方法错误假设直接对应，DCReg提供精确映射；缓解上，传统方法不加选择修改问题，DCReg保持原始问题完整性；整体上，传统方法将检测和缓解视为分离步骤，DCReg提供统一框架。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DCReg通过解耦表征方法，实现了在退化环境中激光雷达点云配准的可靠病态检测、精确物理表征和高效稳定优化，显著提升了定位精度和计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point cloud registration is fundamental to robotic perception andnavigation. However, in geometrically degenerate or narrow environments,registration problems become ill-conditioned, leading to unstable solutions anddegraded accuracy. While existing approaches attempt to handle these issues,they fail to address the core challenge: accurately detection, interpret, andresolve this ill-conditioning, leading to missed detections or corruptedsolutions. In this study, we introduce DCReg, a principled framework thatsystematically addresses the ill-conditioned registration problems throughthree integrated innovations. First, DCReg achieves reliable ill-conditioningdetection by employing a Schur complement decomposition to the hessian matrix.This technique decouples the registration problem into clean rotational andtranslational subspaces, eliminating coupling effects that mask degeneracypatterns in conventional analyses. Second, within these cleanly subspaces, wedevelop quantitative characterization techniques that establish explicitmappings between mathematical eigenspaces and physical motion directions,providing actionable insights about which specific motions lack constraints.Finally, leveraging this clean subspace, we design a targeted mitigationstrategy: a novel preconditioner that selectively stabilizes only theidentified ill-conditioned directions while preserving all well-constrainedinformation in observable space. This enables efficient and robust optimizationvia the Preconditioned Conjugate Gradient method with a single physicalinterpretable parameter. Extensive experiments demonstrate DCReg achieves atleast 20% - 50% improvement in localization accuracy and 5-100 times speedupover state-of-the-art methods across diverse environments. Our implementationwill be available at https://github.com/JokerJohn/DCReg.</description>
      <author>example@mail.com (Xiangcheng Hu, Xieyuanli Chen, Mingkai Jia, Jin Wu, Ping Tan, Steven L. Waslander)</author>
      <guid isPermaLink="false">2509.06285v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>StripDet: Strip Attention-Based Lightweight 3D Object Detection from Point Cloud</title>
      <link>http://arxiv.org/abs/2509.05954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StripDet是一种轻量级3D目标检测框架，通过创新的条带注意力模块和分层骨干网络，有效解决了高精度3D目标检测模型在计算和内存需求方面的挑战，在保持高精度的同时大幅减少了参数量。&lt;h4&gt;背景&lt;/h4&gt;高精度3D目标检测模型的部署面临重大挑战，因为它们需要大量的计算和内存资源。&lt;h4&gt;目的&lt;/h4&gt;介绍StripDet，一个为设备端效率设计的新型轻量级框架。&lt;h4&gt;方法&lt;/h4&gt;提出了新的条带注意力模块（SAB），通过将标准2D卷积分解为非对称条带卷积，高效提取方向特征并降低计算复杂度；设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI数据集上，仅使用0.65M参数，StripDet在汽车检测上达到79.97%的mAP，超过了基准PointPillars，参数减少了7倍；优于最近的轻量级和基于知识蒸馏的方法，实现了更好的精度-效率权衡。&lt;h4&gt;结论&lt;/h4&gt;StripDet成为边缘设备上实际3D检测的可行解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将高精度3D目标检测模型从点云中部署仍然是一个重大挑战，因为它们需要大量的计算和内存资源。为此，我们引入了StripDet，一种专为设备端效率设计的新型轻量级框架。首先，我们提出了新型的条带注意力模块（SAB），这是一个为捕获长程空间依赖关系而设计的高效模块。通过将标准2D卷积分解为非对称条带卷积，SAB能够高效提取方向特征，同时将计算复杂度从二次方降低到线性。其次，我们设计了一个硬件友好的分层骨干网络，将SAB与深度可分离卷积和简单的多尺度融合策略相结合，实现了端到端的效率。在KITTI数据集上的大量实验验证了StripDet的优越性。仅使用0.65M参数，我们的模型在汽车检测上达到了79.97%的mAP，超过了基准PointPillars，参数减少了7倍。此外，StripDet优于最近的轻量级和基于知识蒸馏的方法，实现了更好的精度-效率权衡，同时成为边缘设备上实际3D检测的可行解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of high-accuracy 3D object detection models from point cloudremains a significant challenge due to their substantial computational andmemory requirements. To address this, we introduce StripDet, a novellightweight framework designed for on-device efficiency. First, we propose thenovel Strip Attention Block (SAB), a highly efficient module designed tocapture long-range spatial dependencies. By decomposing standard 2Dconvolutions into asymmetric strip convolutions, SAB efficiently extractsdirectional features while reducing computational complexity from quadratic tolinear. Second, we design a hardware-friendly hierarchical backbone thatintegrates SAB with depthwise separable convolutions and a simple multiscalefusion strategy, achieving end-to-end efficiency. Extensive experiments on theKITTI dataset validate StripDet's superiority. With only 0.65M parameters, ourmodel achieves a 79.97% mAP for car detection, surpassing the baselinePointPillars with a 7x parameter reduction. Furthermore, StripDet outperformsrecent lightweight and knowledge distillation-based methods, achieving asuperior accuracy-efficiency trade-off while establishing itself as a practicalsolution for real-world 3D detection on edge devices.</description>
      <author>example@mail.com (Weichao Wang, Wendong Mao, Zhongfeng Wang)</author>
      <guid isPermaLink="false">2509.05954v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Music Autotagging with MGPHot Expert Annotations vs. Generic Tag Datasets</title>
      <link>http://arxiv.org/abs/2509.06936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了基于MGPHot数据集的新音乐自动标注基准测试框架，包含专家音乐学注释、可检索音频数据集、标准化评估分割和预计算模型表示，并比较了专家与通用标签注释的差异。&lt;h4&gt;背景&lt;/h4&gt;音乐自动标注是自动为音频分配描述性标签的任务，因其挑战性、语义描述多样性和实际应用价值，已成为评估通用音乐表示性能的常见下游任务。MGPHot数据集包含专家音乐学注释，但原始数据集缺乏音频和标准化评估设置。&lt;h4&gt;目的&lt;/h4&gt;解决MGPHot数据集缺乏音频和标准化评估设置的问题，为音乐理解研究提供更先进的基准测试框架。&lt;h4&gt;方法&lt;/h4&gt;提供一组可检索的YouTube URL音频，提出train/val/test标准化评估分割，为七种最先进模型预计算表示，并在MGPHot和标准参考标签数据集上评估这些模型。&lt;h4&gt;主要发现&lt;/h4&gt;专家音乐学注释与通用标签注释之间存在关键差异，通过新基准测试框架揭示了这些差异，为音乐理解研究提供了更深入的见解。&lt;h4&gt;结论&lt;/h4&gt;所提出的新基准测试框架为未来音乐理解研究提供了更先进的评估工具，有助于推动音乐自动标注领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;音乐自动标注旨在为音频记录自动分配描述性标签，如流派、情绪或乐器等。由于其挑战性、语义描述的多样性以及在各种应用中的实际价值，它已成为评估从音频数据学习到的通用音乐表示性能的常见下游任务。我们介绍了一个基于最近发布的MGPHot数据集的新基准测试数据集，该数据集包含专家音乐学注释，允许与在通用标签数据集上获得的结果进行额外的见解和比较。虽然MGPHot注释已被证明对计算音乐学有用，但原始数据集既不包括音频，也没有提供将其用作标准化自动标注基准的评估设置。为了解决这个问题，我们提供了一组精心挑选的YouTube URL，可检索音频，并提出了用于标准化评估的train/val/test分割，以及为七种最先进模型预先计算的表示。利用这些资源，我们在MGPHot和标准参考标签数据集上评估了这些模型，突出了专家和通用标签注释之间的关键差异。总之，我们的贡献为未来音乐理解研究提供了一个更先进的基准测试框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Music autotagging aims to automatically assign descriptive tags, such asgenre, mood, or instrumentation, to audio recordings. Due to its challenges,diversity of semantic descriptions, and practical value in variousapplications, it has become a common downstream task for evaluating theperformance of general-purpose music representations learned from audio data.We introduce a new benchmarking dataset based on the recently published MGPHotdataset, which includes expert musicological annotations, allowing foradditional insights and comparisons with results obtained on common generic tagdatasets. While MGPHot annotations have been shown to be useful forcomputational musicology, the original dataset neither includes audio norprovides evaluation setups for its use as a standardized autotagging benchmark.To address this, we provide a curated set of YouTube URLs with retrievableaudio, and propose a train/val/test split for standardized evaluation, andprecomputed representations for seven state-of-the-art models. Using theseresources, we evaluated these models in MGPHot and standard reference tagdatasets, highlighting key differences between expert and generic tagannotations. Altogether, our contributions provide a more advanced benchmarkingframework for future research in music understanding.</description>
      <author>example@mail.com (Pedro Ramoneda, Pablo Alonso-Jimenez, Sergio Oramas, Xavier Serra, Dmitry Bogdanov)</author>
      <guid isPermaLink="false">2509.06936v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Evolving from Unknown to Known: Retentive Angular Representation Learning for Incremental Open Set Recognition</title>
      <link>http://arxiv.org/abs/2509.06570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, 2025 IEEE/CVF International Conference on  Computer Vision Workshops&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为保留性角表示学习（RARL）的方法，用于解决增量开放集识别（IOSR）问题。该方法通过在角度空间中对齐未知表示和采用虚拟内在交互训练策略，有效减轻了知识更新过程中的表示漂移和类别间混淆问题。&lt;h4&gt;背景&lt;/h4&gt;现有的开放集识别方法通常针对静态场景，模型只能分类已知类别并识别固定范围内的未知类别，无法从连续数据流中增量识别新出现的未知类别并获取相应知识。&lt;h4&gt;目的&lt;/h4&gt;解决在动态场景中由于无法访问之前训练数据导致的开放集识别决策边界区分性难以维持、严重类别间混淆的问题。&lt;h4&gt;方法&lt;/h4&gt;提出保留性角表示学习（RARL）方法，包括：1) 在等角紧框架下构建的角度空间内，鼓励未知表示围绕非活跃原型对齐；2) 采用虚拟内在交互（VII）训练策略，通过边界接近虚拟类强制清晰的类间边界；3) 设计分层校正策略优化决策边界，减轻样本不平衡引起的表示偏差和特征空间扭曲。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR100和TinyImageNet数据集上的实验结果表明，所提出的RARL方法在各种任务设置下都达到了最先进的性能，为IOSR建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;RARL方法有效解决了增量开放集识别中的表示漂移和类别混淆问题，能够更好地适应动态场景下的开放集识别任务。&lt;h4&gt;翻译&lt;/h4&gt;现有的开放集识别方法通常设计用于静态场景，模型旨在分类已知类别并识别固定范围内的未知类别。这与模型应能从连续数据流中增量识别新出现的未知类别并获取相应知识的期望不符。在这种动态场景中，由于无法访问之前的训练数据，开放集识别决策边界的区分性难以维持，导致严重的类别间混淆。为解决这一问题，我们提出了用于增量开放集识别的保留性角表示学习（RARL）。在RARL中，未知表示被鼓励在等角紧框架下构建的角度空间内围绕非活跃原型对齐，从而减轻知识更新过程中的过度表示漂移。具体来说，我们采用虚拟内在交互（VII）训练策略，通过边界接近虚拟类强制清晰的类间边界，压缩已知表示。此外，还设计了一种分层校正策略来优化决策边界，减轻由旧/新类别和正/负类别样本不平衡引起的表示偏差和特征空间扭曲。我们在CIFAR100和TinyImageNet数据集上进行了全面评估，为IOSR建立了新的基准。各种任务设置下的实验结果表明，所提出的方法达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing open set recognition (OSR) methods are typically designed for staticscenarios, where models aim to classify known classes and identify unknown oneswithin fixed scopes. This deviates from the expectation that the model shouldincrementally identify newly emerging unknown classes from continuous datastreams and acquire corresponding knowledge. In such evolving scenarios, thediscriminability of OSR decision boundaries is hard to maintain due torestricted access to former training data, causing severe inter-classconfusion. To solve this problem, we propose retentive angular representationlearning (RARL) for incremental open set recognition (IOSR). In RARL, unknownrepresentations are encouraged to align around inactive prototypes within anangular space constructed under the equiangular tight frame, thereby mitigatingexcessive representation drift during knowledge updates. Specifically, we adopta virtual-intrinsic interactive (VII) training strategy, which compacts knownrepresentations by enforcing clear inter-class margins throughboundary-proximal virtual classes. Furthermore, a stratified rectificationstrategy is designed to refine decision boundaries, mitigating representationbias and feature space distortion caused by imbalances between old/new andpositive/negative class samples. We conduct thorough evaluations on CIFAR100and TinyImageNet datasets and establish a new benchmark for IOSR. Experimentalresults across various task setups demonstrate that the proposed methodachieves state-of-the-art performance.</description>
      <author>example@mail.com (Runqing Yang, Yimin Fu, Changyuan Wu, Zhunga Liu)</author>
      <guid isPermaLink="false">2509.06570v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Efficiency of Latent Spaces via the Coupling-Matrix</title>
      <link>http://arxiv.org/abs/2509.06314v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为rho(C)的冗余指数，用于直接量化表示学习中潜在嵌入的维度间依赖关系，帮助识别冗余并提高表示效率。&lt;h4&gt;背景&lt;/h4&gt;深度网络产生的潜在空间常包含冗余信息，多个坐标编码重叠内容，降低了模型容量和泛化能力，而现有标准指标无法直接检测这种冗余。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够直接量化潜在表示中冗余的方法，以评估和改进学习表示的效率。&lt;h4&gt;方法&lt;/h4&gt;引入冗余指数rho(C)，通过分析潜在表示的耦合矩阵，使用能量距离比较非对角线统计量与正态分布的差异，直接测量维度间依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;低rho(C)与高分类准确率和低重建误差相关；冗余增加与性能崩溃相关；估计器可靠性随潜在维度增加；TPE方法倾向于探索低冗余区域。&lt;h4&gt;结论&lt;/h4&gt;rho(C)作为冗余的通用指标，为评估和改进学习表示的效率提供了理论视角和实用工具，可用于指导神经架构搜索和作为正则化目标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：表示学习中的一个核心挑战是构建既具有表达力又高效的潜在嵌入。在实践中，深度网络常常产生冗余的潜在空间，其中多个坐标编码重叠信息，降低了有效容量并阻碍了泛化能力。标准指标如准确率或重建损失仅提供此类冗余的间接证据，无法将其隔离为一种失败模式。我们引入了一个冗余指数，表示为rho(C)，它通过分析从潜在表示推导的耦合矩阵，并通过能量距离将其非对角线统计量与正态分布进行比较，直接量化了维度间的依赖关系。结果是一个紧凑、可解释且具有统计基础的表示质量度量。我们在MNIST变体、Fashion-MNIST、CIFAR-10和CIFAR-100上的判别性和生成性设置中验证了rho(C)，涵盖了多种架构和超参数优化策略。经验表明，低rho(C)可靠地预测高分类准确率或低重建误差，而冗余增加与性能崩溃相关。估计器的可靠性随潜在维度增加而提高，为可靠分析提供了自然下限。我们还表明，树结构Parzen估计器（TPE）倾向于探索低rho区域，表明rho(C)可以指导神经架构搜索并作为冗余感知的正则化目标。通过将冗余暴露为跨模型和任务的普遍瓶颈，rho(C)为评估和改进学习表示的效率提供了理论视角和实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A central challenge in representation learning is constructing latentembeddings that are both expressive and efficient. In practice, deep networksoften produce redundant latent spaces where multiple coordinates encodeoverlapping information, reducing effective capacity and hinderinggeneralization. Standard metrics such as accuracy or reconstruction lossprovide only indirect evidence of such redundancy and cannot isolate it as afailure mode. We introduce a redundancy index, denoted rho(C), that directlyquantifies inter-dimensional dependencies by analyzing coupling matricesderived from latent representations and comparing their off-diagonal statisticsagainst a normal distribution via energy distance. The result is a compact,interpretable, and statistically grounded measure of representational quality.We validate rho(C) across discriminative and generative settings on MNISTvariants, Fashion-MNIST, CIFAR-10, and CIFAR-100, spanning multiplearchitectures and hyperparameter optimization strategies. Empirically, lowrho(C) reliably predicts high classification accuracy or low reconstructionerror, while elevated redundancy is associated with performance collapse.Estimator reliability grows with latent dimension, yielding natural lowerbounds for reliable analysis. We further show that Tree-structured ParzenEstimators (TPE) preferentially explore low-rho regions, suggesting that rho(C)can guide neural architecture search and serve as a redundancy-awareregularization target. By exposing redundancy as a universal bottleneck acrossmodels and tasks, rho(C) offers both a theoretical lens and a practical toolfor evaluating and improving the efficiency of learned representations.</description>
      <author>example@mail.com (Mehmet Can Yavuz, Berrin Yanikoglu)</author>
      <guid isPermaLink="false">2509.06314v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>UNO: Unifying One-stage Video Scene Graph Generation via Object-Centric Visual Representation Learning</title>
      <link>http://arxiv.org/abs/2509.06165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UNO（统一对象中心视频场景图生成），一个单阶段统一框架，能够同时处理粗粒度框级别和细粒度全景像素级别的视频场景图生成任务，通过扩展的slot注意力机制、对象时间一致性学习和动态三元组预测模块实现高效的对象时序交互建模。&lt;h4&gt;背景&lt;/h4&gt;现有视频场景图生成研究通常只针对粗粒度框级别或细粒度全景像素级别中的一种任务，需要特定的架构和多阶段训练流程，缺乏能够同时处理这两种粒度级别的统一方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个单阶段、统一的框架，在一个端到端架构中同时处理框级别和像素级别的VidSGG任务，最小化任务特定修改，最大化参数共享，实现不同视觉粒度级别上的泛化。&lt;h4&gt;方法&lt;/h4&gt;UNO框架采用扩展的slot注意力机制将视觉特征分解为对象和关系slot；引入对象时间一致性学习强制跨帧保持一致的对象表示；使用动态三元组预测模块将关系slot链接到相应的对象对，捕捉随时间变化的交互。&lt;h4&gt;主要发现&lt;/h4&gt;UNO在标准框级别和像素级别VidSGG基准测试上取得了具有竞争力的性能，同时通过统一的对象中心设计提供了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;UNO不仅能够在两种任务上实现竞争性性能，还通过统一的对象中心设计提高了效率，证明了统一框架处理不同视觉粒度级别视频场景图生成的可行性。&lt;h4&gt;翻译&lt;/h4&gt;视频场景图生成(VidSGG)旨在通过检测对象并将其时间交互建模为结构化图来表示动态视觉内容。先前的研究通常针对粗粒度框级别或细粒度全景像素级别的VidSGG，通常需要特定的架构和多阶段训练流程。在本文中，我们提出了UNO（统一对象中心VidSGG），这是一个单阶段统一框架，在一个端到端架构中共同解决这两个任务。UNO旨在最小化任务特定的修改并最大化参数共享，实现在不同视觉粒度级别上的泛化。UNO的核心是一个扩展的slot注意力机制，它将视觉特征分解为对象和关系slot。为确保鲁棒的时序建模，我们引入了对象时间一致性学习，强制跨帧保持一致的对象表示，而不依赖显式跟踪模块。此外，动态三元组预测模块将关系slot链接到相应的对象对，捕捉随时间变化的交互。我们在标准的框级别和像素级别VidSGG基准上评估了UNO。结果表明，UNO不仅在两个任务上都取得了具有竞争力的性能，还通过统一的对象中心设计提供了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Scene Graph Generation (VidSGG) aims to represent dynamic visualcontent by detecting objects and modeling their temporal interactions asstructured graphs. Prior studies typically target either coarse-grainedbox-level or fine-grained panoptic pixel-level VidSGG, often requiringtask-specific architectures and multi-stage training pipelines. In this paper,we present UNO (UNified Object-centric VidSGG), a single-stage, unifiedframework that jointly addresses both tasks within an end-to-end architecture.UNO is designed to minimize task-specific modifications and maximize parametersharing, enabling generalization across different levels of visual granularity.The core of UNO is an extended slot attention mechanism that decomposes visualfeatures into object and relation slots. To ensure robust temporal modeling, weintroduce object temporal consistency learning, which enforces consistentobject representations across frames without relying on explicit trackingmodules. Additionally, a dynamic triplet prediction module links relation slotsto corresponding object pairs, capturing evolving interactions over time. Weevaluate UNO on standard box-level and pixel-level VidSGG benchmarks. Resultsdemonstrate that UNO not only achieves competitive performance across bothtasks but also offers improved efficiency through a unified, object-centricdesign.</description>
      <author>example@mail.com (Huy Le, Nhat Chung, Tung Kieu, Jingkang Yang, Ngan Le)</author>
      <guid isPermaLink="false">2509.06165v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Micro-Expression Recognition via Fine-Grained Dynamic Perception</title>
      <link>http://arxiv.org/abs/2509.06015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新颖的细粒度动态感知(FDP)框架用于面部微表情识别，通过帧级特征排序和动态图像构建任务，有效捕捉微表情的动态信息，解决了现有方法在特征提取和数据限制方面的问题，并在多个数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;面部微表情识别是一项具有挑战性的任务，因为微表情具有短暂、细微和动态的特性。现有方法大多依赖手工设计特征或深度网络，前者通常需要关键帧，后者则受限于小规模和低多样性的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的微表情识别框架，能够有效捕捉微表情的动态信息，解决现有方法在特征提取和数据限制方面的问题。&lt;h4&gt;方法&lt;/h4&gt;提出细粒度动态感知(FDP)框架，包括：按时间顺序对原始帧序列的帧级特征进行排序，编码微表情出现和运动的动态信息；提出局部-全局特征感知transformer进行帧表示学习；采用排序评分器计算每个帧级特征的排序分数；在时间维度上对排序特征进行池化，捕获动态表示；将动态表示共享给微表情识别模块和动态图像构建模块。&lt;h4&gt;主要发现&lt;/h4&gt;提出的FDP方法显著超越了最先进的微表情识别方法；在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，FDP的F1分数分别比之前最好的结果提高了4.05%、2.50%、7.71%和2.11%；动态图像构建任务表现良好。&lt;h4&gt;结论&lt;/h4&gt;细粒度动态感知框架有效解决了微表情识别中的挑战，通过帧级特征排序和动态图像构建任务，能够更好地捕捉微表情的动态信息，缓解数据稀缺问题，并在多个数据集上取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;面部微表情识别是一项具有挑战性的任务，因为微表情具有短暂、细微和动态的特性。大多数现有方法依赖于手工设计特征或深度网络，其中前者通常需要关键帧，而后者则受限于小规模和低多样性的训练数据。在本文中，我们为微表情识别开发了一种新颖的细粒度动态感知(FDP)框架。我们提出按时间顺序对原始帧序列的帧级特征进行排序，其中排序过程编码了微表情出现和运动的动态信息。具体来说，提出了一种新颖的局部-全局特征感知transformer用于帧表示学习。进一步采用排序评分器计算每个帧级特征的排序分数。之后，从排序评分器中提取的排序特征在时间维度上进行池化，以捕获动态表示。最后，动态表示被微表情识别模块和动态图像构建模块共享，前者预测微表情类别，后者使用编码器-解码器结构构建动态图像。动态图像构建任务的设计有助于捕捉与微表情相关的面部细微动作，并缓解数据稀缺问题。大量实验表明，我们的方法(i)显著超越了最先进的微表情识别方法，(ii)在动态图像构建方面表现良好。特别是在CASME II、SAMM、CAS(ME)^2和CAS(ME)^3数据集上，我们的FDP在F1分数上分别比之前最好的结果提高了4.05%、2.50%、7.71%和2.11%。代码可在https://github.com/CYF-cuber/FDP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial micro-expression recognition (MER) is a challenging task, due to thetransience, subtlety, and dynamics of micro-expressions (MEs). Most existingmethods resort to hand-crafted features or deep networks, in which the formeroften additionally requires key frames, and the latter suffers from small-scaleand low-diversity training data. In this paper, we develop a novel fine-graineddynamic perception (FDP) framework for MER. We propose to rank frame-levelfeatures of a sequence of raw frames in chronological order, in which the rankprocess encodes the dynamic information of both ME appearances and motions.Specifically, a novel local-global feature-aware transformer is proposed forframe representation learning. A rank scorer is further adopted to calculaterank scores of each frame-level feature. Afterwards, the rank features fromrank scorer are pooled in temporal dimension to capture dynamic representation.Finally, the dynamic representation is shared by a MER module and a dynamicimage construction module, in which the former predicts the ME category, andthe latter uses an encoder-decoder structure to construct the dynamic image.The design of dynamic image construction task is beneficial for capturingfacial subtle actions associated with MEs and alleviating the data scarcityissue. Extensive experiments show that our method (i) significantly outperformsthe state-of-the-art MER methods, and (ii) works well for dynamic imageconstruction. Particularly, our FDP improves by 4.05%, 2.50%, 7.71%, and 2.11%over the previous best results in terms of F1-score on the CASME II, SAMM,CAS(ME)^2, and CAS(ME)^3 datasets, respectively. The code is available athttps://github.com/CYF-cuber/FDP.</description>
      <author>example@mail.com (Zhiwen Shao, Yifan Cheng, Fan Zhang, Xuehuai Shi, Canlin Li, Lizhuang Ma, Dit-yan Yeung)</author>
      <guid isPermaLink="false">2509.06015v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>From perception to production: how acoustic invariance facilitates articulatory learning in a self-supervised vocal imitation model</title>
      <link>http://arxiv.org/abs/2509.05849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 (Main Conference)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种通过自监督学习解决婴儿语音习得中声学到发音映射问题的计算模型，模型使用wav2vec 2.0的表示学习发音参数，产生可理解语音，研究支持感知学习指导发音发展的发育理论。&lt;h4&gt;背景&lt;/h4&gt;婴儿在语音习得方面面临巨大挑战，需要将极其可变的声学输入映射到适当的发音动作，而没有明确指导。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决声学到发音映射问题的计算模型，通过自监督学习来解决这个问题。&lt;h4&gt;方法&lt;/h4&gt;研究提出一个包含三个部分的计算模型：特征提取器将语音转换为潜在表示，逆向模型将这些表示映射到发音参数，合成器生成语音输出。在单说话人和多说话人环境中进行实验，使用预训练的wav2vec 2.0模型的中间层表示，并与MFCC特征进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的wav2vec 2.0模型的中间层表示为发音学习提供了最佳表示，显著优于MFCC特征。这些表示使模型能够学习与人类模式相关的发音轨迹，区分发音部位，产生可理解的语音。对成功的发音学习至关重要的是具有音素可区分性和说话人不变性的表示，这正是自监督表示学习模型的特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果提供了与发育理论一致的计算证据，该理论提出音素类别的感知学习指导发音发展，为婴儿如何获得语音产生能力提供了见解，尽管他们面临复杂的映射问题。&lt;h4&gt;翻译&lt;/h4&gt;人类婴儿在语音习得方面面临一个巨大挑战：在没有明确指导的情况下，将极其可变的声学输入映射到适当的发音动作。我们提出了一个通过自监督学习解决声学到发音映射问题的计算模型。我们的模型包含一个将语音转换为潜在表示的特征提取器，一个将这些表示映射到发音参数的逆向模型，以及一个生成语音输出的合成器。在单说话人和多说话人环境中进行的实验显示，预训练的wav2vec 2.0模型的中间层为发音学习提供了最佳表示，显著优于MFCC特征。这些表示使我们的模型能够学习与人类模式相关的发音轨迹，区分发音部位，并产生可理解的语音。对成功的发音学习至关重要的是具有音素可区分性和说话人不变性的表示——这正是自监督表示学习模型的特征。我们的研究结果提供了与发育理论一致的计算证据，该理论提出音素类别的感知学习指导发音发展，为婴儿如何获得语音产生能力提供了见解，尽管他们面临复杂的映射问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human infants face a formidable challenge in speech acquisition: mappingextremely variable acoustic inputs into appropriate articulatory movementswithout explicit instruction. We present a computational model that addressesthe acoustic-to-articulatory mapping problem through self-supervised learning.Our model comprises a feature extractor that transforms speech into latentrepresentations, an inverse model that maps these representations toarticulatory parameters, and a synthesizer that generates speech outputs.Experiments conducted in both single- and multi-speaker settings reveal thatintermediate layers of a pre-trained wav2vec 2.0 model provide optimalrepresentations for articulatory learning, significantly outperforming MFCCfeatures. These representations enable our model to learn articulatorytrajectories that correlate with human patterns, discriminate between places ofarticulation, and produce intelligible speech. Critical to successfularticulatory learning are representations that balance phoneticdiscriminability with speaker invariance -- precisely the characteristics ofself-supervised representation learning models. Our findings providecomputational evidence consistent with developmental theories proposing thatperceptual learning of phonetic categories guides articulatory development,offering insights into how infants might acquire speech production capabilitiesdespite the complex mapping problem they face.</description>
      <author>example@mail.com (Marvin Lavechin, Thomas Hueber)</author>
      <guid isPermaLink="false">2509.05849v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Hyperbolic Large Language Models</title>
      <link>http://arxiv.org/abs/2509.05757v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了双曲几何在大型语言模型中的应用，提出了双曲大型语言模型(HypLLMs)的分类框架，并探讨了其在增强语义表示学习和多尺度推理方面的潜力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在各种任务中表现出色，但现实世界数据往往具有高度非欧几里得的层次结构，如蛋白质网络、交通网络、金融网络、大脑网络和语言结构。使用LLMs有效学习这些数据的语义蕴含和层次关系仍是一个探索不足的领域。&lt;h4&gt;目的&lt;/h4&gt;提供对利用双曲几何作为表示空间来增强语义表示学习和多尺度推理的最新进展进行全面阐述，并提出双曲大型语言模型的分类框架。&lt;h4&gt;方法&lt;/h4&gt;将双曲大型语言模型(HypLLMs)的技术分为四类：(1)通过exp/log映射的双曲LLMs；(2)双曲微调模型；(3)完全双曲LLMs；(4)双曲状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;双曲几何作为非欧几里得空间，能有效建模树状层次结构，特别适合处理具有层次结构的数据，与LLMs结合可增强语义表示学习和多尺度推理能力。&lt;h4&gt;结论&lt;/h4&gt;双曲几何在大型语言模型中的应用前景广阔，特别是在处理层次结构数据方面。论文提供了全面的分类框架，并探讨了潜在应用和未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)在各种任务中取得了显著成功，展示了卓越的性能，包括自然语言处理(NLP)、天气预报、生物蛋白质折叠、文本生成和解决数学问题。然而，许多现实世界的数据表现出高度非欧几里得的潜在层次解剖结构，如蛋白质网络、交通网络、金融网络、大脑网络以及自然语言中的语言结构或句法树。使用LLMs从这些原始、非结构化的输入数据中有效学习内在语义蕴含和层次关系仍然是一个探索不足的领域。由于其在建模树状层次结构方面的有效性，双曲几何——一种非欧几里得空间——已成为跨领域复杂数据建模的表达性潜在表示空间。论文提出了双曲大型语言模型(HypLLMs)主要技术的分类，并探讨了关键应用和未来研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have achieved remarkable success anddemonstrated superior performance across various tasks, including naturallanguage processing (NLP), weather forecasting, biological protein folding,text generation, and solving mathematical problems. However, many real-worlddata exhibit highly non-Euclidean latent hierarchical anatomy, such as proteinnetworks, transportation networks, financial networks, brain networks, andlinguistic structures or syntactic trees in natural languages. Effectivelylearning intrinsic semantic entailment and hierarchical relationships fromthese raw, unstructured input data using LLMs remains an underexplored area.Due to its effectiveness in modeling tree-like hierarchical structures,hyperbolic geometry -- a non-Euclidean space -- has rapidly gained popularityas an expressive latent representation space for complex data modeling acrossdomains such as graphs, images, languages, and multi-modal data. Here, weprovide a comprehensive and contextual exposition of recent advancements inLLMs that leverage hyperbolic geometry as a representation space to enhancesemantic representation learning and multi-scale reasoning. Specifically, thepaper presents a taxonomy of the principal techniques of Hyperbolic LLMs(HypLLMs) in terms of four main categories: (1) hyperbolic LLMs through exp/logmaps; (2) hyperbolic fine-tuned models; (3) fully hyperbolic LLMs, and (4)hyperbolic state-space models. We also explore crucial potential applicationsand outline future research directions. A repository of key papers, models,datasets, and code implementations is available athttps://github.com/sarangp2402/Hyperbolic-LLM-Models/tree/main.</description>
      <author>example@mail.com (Sarang Patil, Zeyong Zhang, Yiran Huang, Tengfei Ma, Mengjia Xu)</author>
      <guid isPermaLink="false">2509.05757v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Causal Debiasing Medical Multimodal Representation Learning with Missing Modalities</title>
      <link>http://arxiv.org/abs/2509.05615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE TKDE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医学多模态表示学习旨在将异构临床数据整合为统一的患者表示，以支持预测建模，这是医疗数据挖掘领域的一项重要且具有挑战性的任务。然而，现实世界中的医疗数据集常常因成本、协议或患者特定限制而存在模态缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的方法主要通过原始数据空间或特征空间中的可用观测数据进行学习，但通常忽略了数据采集过程本身引入的潜在偏差。现实世界中的医疗数据集经常面临模态缺失问题。&lt;h4&gt;目的&lt;/h4&gt;识别阻碍模型泛化的两种偏差：缺失偏差（由模态可用性的非随机模式导致）和分布偏差（由影响观测特征和结果的潜在混杂因素引起）。提出一个统一框架来解决这些挑战。&lt;h4&gt;方法&lt;/h4&gt;进行数据生成过程的结构因果分析，并提出一个与现有直接预测多模态学习方法兼容的统一框架。该方法包含两个关键组件：(1) 基于后门调整的缺失解混杂模块，用于近似因果干预；(2) 双分支神经网络，明确将因果特征与虚假相关性分离。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界的公共和院内数据集上评估了该方法，证明了其有效性和因果洞察力。&lt;h4&gt;结论&lt;/h4&gt;通过识别和处理缺失偏差和分布偏差，该方法能够提高医学多模态表示学习的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;医学多模态表示学习旨在将异构临床数据整合为统一的患者表示，以支持预测建模，这仍然是医疗数据挖掘领域一项基本且具有挑战性的任务。然而，现实世界中的医疗数据集常常因成本、协议或患者特定限制而面临模态缺失的问题。现有方法主要通过原始数据空间或特征空间中的可用观测数据进行学习，但通常忽略了数据采集过程本身引入的潜在偏差。在这项工作中，我们确定了阻碍模型泛化的两种偏差：缺失偏差，由模态可用性的非随机模式导致；以及分布偏差，由影响观测特征和结果的潜在混杂因素引起。为应对这些挑战，我们对数据生成过程进行了结构因果分析，并提出了一个与现有直接预测多模态学习方法兼容的统一框架。我们的方法包含两个关键组件：(1) 基于后门调整的缺失解混杂模块，用于近似因果干预；(2) 双分支神经网络，明确将因果特征与虚假相关性分离。我们在现实世界的公共和院内数据集上评估了我们的方法，证明了其有效性和因果洞察力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical multimodal representation learning aims to integrate heterogeneousclinical data into unified patient representations to support predictivemodeling, which remains an essential yet challenging task in the medical datamining community. However, real-world medical datasets often suffer frommissing modalities due to cost, protocol, or patient-specific constraints.Existing methods primarily address this issue by learning from the availableobservations in either the raw data space or feature space, but typicallyneglect the underlying bias introduced by the data acquisition process itself.In this work, we identify two types of biases that hinder model generalization:missingness bias, which results from non-random patterns in modalityavailability, and distribution bias, which arises from latent confounders thatinfluence both observed features and outcomes. To address these challenges, weperform a structural causal analysis of the data-generating process and proposea unified framework that is compatible with existing direct prediction-basedmultimodal learning methods. Our method consists of two key components: (1) amissingness deconfounding module that approximates causal intervention based onbackdoor adjustment and (2) a dual-branch neural network that explicitlydisentangles causal features from spurious correlations. We evaluated ourmethod in real-world public and in-hospital datasets, demonstrating itseffectiveness and causal insights.</description>
      <author>example@mail.com (Xiaoguang Zhu, Lianlong Sun, Yang Liu, Pengyi Jiang, Uma Srivatsa, Nipavan Chiamvimonvat, Vladimir Filkov)</author>
      <guid isPermaLink="false">2509.05615v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Patch-level Kernel Alignment for Self-Supervised Dense Representation Learning</title>
      <link>http://arxiv.org/abs/2509.05606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过自监督学习构建密集表示的框架，解决了全局表示方法在密集预测任务中难以捕捉局部语义的问题。&lt;h4&gt;背景&lt;/h4&gt;密集表示对于需要空间精度和细粒度细节的视觉任务至关重要，而大多数自监督表示学习方法专注于全局表示，往往无法捕捉密集预测任务所需的局部语义信息。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，通过额外的自监督学习建立在预训练表示的基础上，旨在将现有的语义知识转移到密集特征空间中，以克服全局表示方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一种框架，对齐教师模型和学生模型之间的密集特征分布；引入Patch级核对齐（PaKA）作为简单有效的对齐目标，捕捉统计依赖关系，匹配两个模型中密集补丁的结构关系；研究专门为密集表示学习设计的增强策略。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在各种密集视觉基准测试中取得了最先进的结果，证明了所提出方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过额外的自监督学习和特定的对齐方法，可以有效地将语义知识转移到密集特征空间，Patch级核对齐（PaKA）是一种有效的方法，可以捕捉统计依赖关系并匹配结构关系。&lt;h4&gt;翻译&lt;/h4&gt;密集表示对于需要空间精度和细粒度细节的视觉任务至关重要。虽然大多数自监督表示学习方法专注于总结整个图像的全局表示，但这类方法通常难以捕捉密集预测任务所需的局部语义。为克服这些局限，我们提出了一种框架，通过额外的自监督学习建立在预训练表示的基础上，旨在将现有的语义知识转移到密集特征空间。我们的方法对齐了教师模型和学生模型之间的密集特征分布。具体而言，我们引入了Patch级核对齐（PaKA），这是一种简单而有效的对齐目标，可以捕捉统计依赖关系，从而匹配两个模型中密集补丁的结构关系。此外，我们还研究了专门为密集表示学习设计的增强策略。我们的框架在各种密集视觉基准测试中取得了最先进的结果，证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense representations are essential for vision tasks that require spatialprecision and fine-grained detail. While most self-supervised representationlearning methods focus on global representations that summarize the image as awhole, such approaches often fall short in capturing the localized semanticsnecessary for dense prediction tasks. To overcome these limitations, we proposea framework that builds on pretrained representations through additionalself-supervised learning, aiming to transfer existing semantic knowledge intothe dense feature space. Our method aligns the distributions of dense featuresbetween a teacher and a student model. Specifically, we introduce Patch-levelKernel Alignment (PaKA), a simple yet effective alignment objective thatcaptures statistical dependencies, thereby matching the structuralrelationships of dense patches across the two models. In addition, weinvestigate augmentation strategies specifically designed for denserepresentation learning. Our framework achieves state-of-the-art results acrossa variety of dense vision benchmarks, demonstrating the effectiveness of ourapproach.</description>
      <author>example@mail.com (Juan Yeo, Ijun Jang, Taesup Kim)</author>
      <guid isPermaLink="false">2509.05606v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>PLanTS: Periodicity-aware Latent-state Representation Learning for Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2509.05478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PLanTS，一个周期感知的自监督学习框架，用于处理多元时间序列数据。该框架能够显式建模不规则潜在状态及其转换，通过多粒度修补机制和对比损失保留时间分辨率上的相似性，并通过下一个转换预测任务捕获时间动态。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列在医疗保健、气候科学和工业监测等领域普遍存在，但它们的高维性、有限的标记数据和非平稳特性对传统机器学习方法构成了重大挑战。现有的自监督学习方法忽略了多元时间序列的内在周期结构，无法捕获潜在状态的动态演化。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够捕捉多元时间序列周期结构并建模潜在状态动态演化的自监督学习框架，以提高在多种下游任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了PLanTS框架，包含：1)周期感知的多粒度修补机制；2)广义对比损失保留多个时间分辨率上的实例级和状态级相似性；3)下一个转换预测预训练任务，鼓励表示编码关于未来状态演化的预测信息。&lt;h4&gt;主要发现&lt;/h4&gt;PLanTS在多类和多标签分类、预测、轨迹跟踪和异常检测等多种下游任务中评估，在表示质量上持续优于现有的自监督学习方法，并且与基于DTW的方法相比展示了更优的运行时效率。&lt;h4&gt;结论&lt;/h4&gt;PLanTS是一个有效的自监督学习框架，能够处理多元时间序列数据的周期性和动态性，在各种任务中表现优异且计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列在医疗保健、气候科学和工业监测等领域普遍存在，但它们的高维性、有限的标记数据和非平稳特性对传统机器学习方法构成了重大挑战。尽管最近的自学习方法通过数据增强或基于时间点的对比策略缓解了标签稀缺问题，但它们忽略了多元时间序列的内在周期结构，并且无法捕获潜在状态的动态演化。我们提出了PLanTS，一个周期感知的自监督学习框架，能够显式地建模不规则潜在状态及其转换。我们首先设计了一个周期感知的多粒度修补机制和一个广义对比损失，以在多个时间分辨率上保留实例级和状态级相似性。为了进一步捕获时间动态，我们设计了一个下一个转换预测预训练任务，鼓励表示编码关于未来状态演化的预测信息。我们在广泛的下游任务中评估了PLanTS，包括多类和多标签分类、预测、轨迹跟踪和异常检测。PLanTS在表示质量上持续优于现有的自监督学习方法，并且与基于DTW的方法相比展示了更优的运行时效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) are ubiquitous in domains such as healthcare,climate science, and industrial monitoring, but their high dimensionality,limited labeled data, and non-stationary nature pose significant challenges forconventional machine learning methods. While recent self-supervised learning(SSL) approaches mitigate label scarcity by data augmentations or timepoint-based contrastive strategy, they neglect the intrinsic periodic structureof MTS and fail to capture the dynamic evolution of latent states. We proposePLanTS, a periodicity-aware self-supervised learning framework that explicitlymodels irregular latent states and their transitions. We first designed aperiod-aware multi-granularity patching mechanism and a generalized contrastiveloss to preserve both instance-level and state-level similarities acrossmultiple temporal resolutions. To further capture temporal dynamics, we designa next-transition prediction pretext task that encourages representations toencode predictive information about future state evolution. We evaluate PLanTSacross a wide range of downstream tasks-including multi-class and multi-labelclassification, forecasting, trajectory tracking and anomaly detection. PLanTSconsistently improves the representation quality over existing SSL methods anddemonstrates superior runtime efficiency compared to DTW-based methods.</description>
      <author>example@mail.com (Jia Wang, Xiao Wang, Chi Zhang)</author>
      <guid isPermaLink="false">2509.05478v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Reasoning with Vision-Language Models in Ego-Centric Multi-View Scenes</title>
      <link>http://arxiv.org/abs/2509.06266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个新的基准测试Ego3D-Bench和一种改进框架Ego3D-VLM，用于评估和提高视觉语言模型在3D空间关系理解方面的能力，特别是在以自我为中心的多视图户外场景中。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在理解3D空间关系方面存在明显局限。先前的工作基于单图像或室内视频创建了空间问答数据集，但现实世界中的具身AI代理(如机器人和自动驾驶汽车)通常依赖于以自我为中心的多视图观测。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的基准测试Ego3D-Bench来评估视觉语言模型在使用以自我为中心的多视图户外数据时的空间推理能力，并提出一种改进框架Ego3D-VLM来增强视觉语言模型的3D空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建了包含8,600多个问答对的Ego3D-Bench基准测试，由人工注释者大量参与以确保质量和多样性。提出了Ego3D-VLM后训练框架，该框架基于估计的全局3D坐标生成认知地图。&lt;h4&gt;主要发现&lt;/h4&gt;在16个最先进的视觉语言模型(包括GPT-4o、Gemini1.5-Pro、InternVL3和Qwen2.5-VL)的测试中，发现了人类水平分数与模型性能之间的显著差距，表明当前的视觉语言模型仍无法达到人类水平的空间理解。Ego3D-VLM框架在多选题问答上实现了12%的平均改进，在绝对距离估计上实现了56%的平均改进。&lt;h4&gt;结论&lt;/h4&gt;Ego3D-Bench和Ego3D-VLM共同为在现实世界多视环境中推进人类水平的空间理解提供了有价值的工具。Ego3D-VLM是模块化的，可以与任何现有的视觉语言模型集成。&lt;h4&gt;翻译&lt;/h4&gt;理解3D空间关系仍然是当前视觉语言模型的主要局限性。先前的工作通过创建基于单图像或室内视频的空间问答数据集来解决这一问题。然而，现实世界中的具身AI代理通常依赖于以自我为中心的多视图观测。为此，我们引入了Ego3D-Bench，这是一个新的基准测试，旨在使用以自我为中心的多视图户外数据评估视觉语言模型的空间推理能力。Ego3D-Bench包含8,600多个问答对，由人工注释者大量参与创建，以确保质量和多样性。我们对16个最先进的视觉语言模型进行了基准测试，结果显示人类水平分数与模型性能之间存在显著差距，突显出当前的视觉语言模型仍未达到人类水平的空间理解。为了弥合这一差距，我们提出了Ego3D-VLM，一种后训练框架，可以增强视觉语言模型的3D空间推理能力。Ego3D-VLM基于估计的全局3D坐标生成认知地图，在多选题问答上实现了12%的平均改进，在绝对距离估计上实现了56%的平均改进。Ego3D-Bench和Ego3D-VLM共同为在现实世界多视环境中推进人类水平的空间理解提供了有价值的工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决当前视觉语言模型（VLMs）在3D空间推理方面的能力不足问题，特别是在以自我为中心的多视角场景中。这个问题很重要，因为现实世界中的具身AI代理（如机器人和自动驾驶汽车）需要理解3D空间关系来感知环境、估计物体距离和推理运动，而现有的空间理解基准测试主要基于单图像或室内视频，无法反映这些代理的真实感知体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有工作的局限性：现有空间基准测试基于单图像或室内静态视频，与真实世界多视角动态场景不符；点云和鸟瞰图方法虽提供丰富空间信息但难以在动态环境中重建且计算成本高。作者借鉴了人类自然整合多视角视觉信息形成统一空间表示的能力，设计了一个名为Ego3D-VLM的后训练框架，生成文本认知地图而非复杂3D表示。该方法整合了现有的2D目标检测和深度估计技术，但将其统一到更符合人类感知机制的框架中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个以自我为中心的文本认知地图，定义以代理为中心的坐标系，并将重要物体定位在3D坐标空间中。这种方法只关注提示中提到的物体，减少输入令牌数量，实现高效推理。实现流程包括：1)使用REC模型找到指代表达式的2D位置；2)使用深度估计器估计深度值；3)将2D点转换为3D点；4)将所有视角的3D点转换为全局坐标系；5)生成文本认知地图；6)将认知地图、多视角图像和查询输入VLM得到答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Ego3D-Bench基准测试，首个针对以自我为中心多视角场景的3D空间理解基准，包含8600个问答对；2)Ego3D-VLM框架，一种即插即用的后训练方法，生成文本认知地图而非复杂3D表示；3)关系缩放技术基于常识物体大小进行比例调整。相比之前工作，现有方法主要使用点云或鸟瞰图表示，在动态环境中难以重建且计算成本高；现有基准测试基于单图像或室内视频，不符合实际应用需求；Ego3D-VLM专注于多视角场景，更符合实际应用，且可集成到任何现有VLM中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出首个针对以自我为中心多视角场景的3D空间理解基准测试（Ego3D-Bench）和一种增强VLMs空间推理能力的后训练框架（Ego3D-VLM），显著缩小了当前VLMs与人类在3D空间理解能力之间的差距。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding 3D spatial relationships remains a major limitation of currentVision-Language Models (VLMs). Prior work has addressed this issue by creatingspatial question-answering (QA) datasets based on single images or indoorvideos. However, real-world embodied AI agents such as robots and self-drivingcars typically rely on ego-centric, multi-view observations. To this end, weintroduce Ego3D-Bench, a new benchmark designed to evaluate the spatialreasoning abilities of VLMs using ego-centric, multi-view outdoor data.Ego3D-Bench comprises over 8,600 QA pairs, created with significant involvementfrom human annotators to ensure quality and diversity. We benchmark 16 SOTAVLMs, including GPT-4o, Gemini1.5-Pro, InternVL3, and Qwen2.5-VL. Our resultsreveal a notable performance gap between human level scores and VLMperformance, highlighting that current VLMs still fall short of human levelspatial understanding. To bridge this gap, we propose Ego3D-VLM, apost-training framework that enhances 3D spatial reasoning of VLMs. Ego3D-VLMgenerates cognitive map based on estimated global 3D coordinates, resulting in12% average improvement on multi-choice QA and 56% average improvement onabsolute distance estimation. Ego3D-VLM is modular and can be integrated withany existing VLM. Together, Ego3D-Bench and Ego3D-VLM offer valuable tools foradvancing toward human level spatial understanding in real-world, multi-viewenvironments.</description>
      <author>example@mail.com (Mohsen Gholami, Ahmad Rezaei, Zhou Weimin, Yong Zhang, Mohammad Akbari)</author>
      <guid isPermaLink="false">2509.06266v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge</title>
      <link>http://arxiv.org/abs/2509.06079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种标题辅助推理框架，有效解决了多模态推理中的挑战，在ICML 2025挑战赛中获得第一名，并在MathVerse基准测试上验证了其泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态推理是人工智能中的基本挑战，尽管基于文本的推理取得了实质性进展，但最先进的模型如GPT-o3在多模态场景中仍难以保持强大性能。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效桥接视觉和文本模态的推理框架，以解决多模态推理中的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种标题辅助推理框架，用于有效连接视觉和文本模态。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在ICML 2025 AI for Math Workshop &amp; Challenge 2: SeePhys中获得第一名，证明了其有效性和鲁棒性；在MathVerse基准测试上验证了其在几何推理方面的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;标题辅助推理框架能够有效解决多模态推理挑战，具有良好的性能和泛化能力，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;多模态推理仍然是人工智能中的一个基本挑战。尽管基于文本的推理取得了实质性进展，但即使是像GPT-o3这样的最先进模型也难以在多模态场景中保持强大的性能。为了解决这一差距，我们引入了一个标题辅助推理框架，有效地桥接了视觉和文本模态。我们的方法在ICML 2025 AI for Math Workshop &amp; Challenge 2: SeePhys中获得了第一名，突显了其有效性和鲁棒性。此外，我们在MathVerse基准测试上验证了其在几何推理方面的泛化能力，展示了我们方法的通用性。我们的代码已在https://github.com/OpenDCAI/SciReasoner上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal reasoning remains a fundamental challenge in artificialintelligence. Despite substantial advances in text-based reasoning, evenstate-of-the-art models such as GPT-o3 struggle to maintain strong performancein multimodal scenarios. To address this gap, we introduce a caption-assistedreasoning framework that effectively bridges visual and textual modalities. Ourapproach achieved 1st place in the ICML 2025 AI for Math Workshop \&amp; Challenge2: SeePhys, highlighting its effectiveness and robustness. Furthermore, wevalidate its generalization on the MathVerse benchmark for geometric reasoning,demonstrating the versatility of our method. Our code is publicly available athttps://github.com/OpenDCAI/SciReasoner.</description>
      <author>example@mail.com (Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong)</author>
      <guid isPermaLink="false">2509.06079v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Depth-Aware Super-Resolution via Distance-Adaptive Variational Formulation</title>
      <link>http://arxiv.org/abs/2509.05746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于空间自适应重建策略的单图像超分辨率方法，解决了传统方法中空间不变退化模型的局限性。通过引入几何场景理解和深度依赖效应，作者建立了一个严格的变分框架，并将其实现为一个具有深度条件卷积核的神经架构。该方法在多个基准数据集上取得了最先进的结果，特别是在深度变化场景中表现出显著改进。&lt;h4&gt;背景&lt;/h4&gt;传统单图像超分辨率方法假设空间不变的退化模型，然而真实世界的成像系统表现出复杂的空间依赖效应，包括大气散射、景深变化和透视失真等。这种基本局限性需要空间自适应重建策略，明确结合几何场景理解以获得最佳性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理空间依赖效应（特别是与距离相关的效应）的超分辨率方法，建立将超分辨率描述为空间变化逆问题的理论框架，并实现一个能够根据深度信息自适应调整的神经架构。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种严格的变分框架，将超分辨率描述为空间变化的逆问题，将退化算子建模为具有距离依赖谱特性的伪微分算子。神经架构通过级联残差块实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时包含学习到的距离自适应正则化项，根据局部几何结构动态调整平滑约束。基于大气散射理论的光谱约束防止远场区域中的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准数据集上的全面评估展示了最先进的性能，在KITTI户外场景2倍和4倍放大时分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM值，分别比现有方法高出0.44dB和0.36dB。该方法在深度变化场景中显示出显著改进，同时在传统基准测试中保持了竞争性性能。&lt;h4&gt;结论&lt;/h4&gt;该工作建立了首个理论上合理的距离自适应超分辨率框架，在深度变化场景上展示了显著改进，同时在传统基准测试中保持了竞争性性能，为处理真实世界成像系统中的复杂空间依赖效应提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;单图像超分辨率传统上假设空间不变的退化模型，而真实世界的成像系统表现出复杂的空间依赖效应，包括大气散射、景深变化和透视失真。这种基本局限性需要明确结合几何场景理解的空间自适应重建策略以获得最佳性能。我们提出了一种严格的变分框架，将超分辨率描述为空间变化的逆问题，将退化算子建模为具有距离依赖谱特性的伪微分算子，从而能够跨深度范围分析重建极限的理论。我们的神经架构通过具有深度条件卷积核的级联残差块实现离散梯度流动力学，确保收敛到理论能量泛函的平稳点，同时包含学习到的距离自适应正则化项，这些项根据局部几何结构动态调整平滑约束。从大气散射理论导出的光谱约束防止远场区域中的带宽违规和噪声放大，而自适应核生成网络学习从深度到重建滤波器的连续映射。在五个基准数据集上的全面评估展示了最先进的性能，在KITTI户外场景2倍和4倍放大时分别达到36.89/0.9516和30.54/0.8721的PSNR/SSIM值，分别比现有方法高出0.44dB和0.36dB。该工作建立了首个理论上合理的距离自适应超分辨率框架，并在深度变化场景上展示了显著改进，同时在传统基准测试中保持了竞争性性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决传统单图像超分辨率方法假设空间不变退化模型的问题，而现实世界成像系统存在复杂的距离相关效应（如大气散射、景深变化、透视失真）。这一问题重要是因为户外场景（如自动驾驶、卫星成像）中图像质量随距离变化而退化，传统方法无法处理这种空间变化的退化，导致在真实场景中效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析发现户外场景与室内场景有不同的退化机制，因此构建了基于伪微分算子的变分框架来建模距离相关的退化。他们借鉴了大气散射理论（如Mie散射）来推导光谱约束，同时利用深度估计网络获取场景几何信息。方法设计上借鉴了残差网络架构和梯度流优化技术，但将其扩展为深度条件卷积和距离自适应正则化，实现了理论框架与神经网络的结合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将超分辨率建模为考虑距离相关退化的空间变化逆问题，利用深度信息指导重建过程，并根据大气散射理论限制重建带宽防止噪声放大。整体流程包括：1)提取深度图；2)生成距离自适应卷积核；3)初始化重建图像；4)迭代优化（计算数据保真度、应用距离自适应正则化、更新重建）；5)最终上采样输出高分辨率图像。整个过程中使用梯度流块逐步优化重建结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于伪微分算子的理论变分框架；2)深度自适应正则化项，根据距离动态调整平滑约束；3)基于大气散射理论的光谱约束，防止远场噪声放大；4)深度条件卷积核和自适应核生成网络。相比之前工作，不同之处在于：传统方法假设空间不变退化，而本文处理空间变化的退化；本文有严格的理论基础（传统方法多为启发式设计）；专门针对户外场景优化（传统方法在室内场景表现好但户外效果差）；显式利用深度信息指导重建（传统方法通常不利用深度信息）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于距离自适应变分公式的深度感知超分辨率方法，通过理论建模和神经网络实现了对户外场景中距离相关退化的有效处理，显著提升了超分辨率重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single image super-resolution traditionally assumes spatially-invariantdegradation models, yet real-world imaging systems exhibit complexdistance-dependent effects including atmospheric scattering, depth-of-fieldvariations, and perspective distortions. This fundamental limitationnecessitates spatially-adaptive reconstruction strategies that explicitlyincorporate geometric scene understanding for optimal performance. We propose arigorous variational framework that characterizes super-resolution as aspatially-varying inverse problem, formulating the degradation operator as apseudodifferential operator with distance-dependent spectral characteristicsthat enable theoretical analysis of reconstruction limits across depth ranges.Our neural architecture implements discrete gradient flow dynamics throughcascaded residual blocks with depth-conditional convolution kernels, ensuringconvergence to stationary points of the theoretical energy functional whileincorporating learned distance-adaptive regularization terms that dynamicallyadjust smoothness constraints based on local geometric structure. Spectralconstraints derived from atmospheric scattering theory prevent bandwidthviolations and noise amplification in far-field regions, while adaptive kernelgeneration networks learn continuous mappings from depth to reconstructionfilters. Comprehensive evaluation across five benchmark datasets demonstratesstate-of-the-art performance, achieving 36.89/0.9516 and 30.54/0.8721 PSNR/SSIMat 2 and 4 scales on KITTI outdoor scenes, outperforming existing methods by0.44dB and 0.36dB respectively. This work establishes the firsttheoretically-grounded distance-adaptive super-resolution framework anddemonstrates significant improvements on depth-variant scenarios whilemaintaining competitive performance across traditional benchmarks.</description>
      <author>example@mail.com (Tianhao Guo, Bingjie Lu, Feng Wang, Zhengyang Lu)</author>
      <guid isPermaLink="false">2509.05746v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>OccVLA: Vision-Language-Action Model with Implicit 3D Occupancy Supervision</title>
      <link>http://arxiv.org/abs/2509.05578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为OccVLA的新框架，将3D占用表示集成到多模态推理过程中，解决了多模态大语言模型在3D空间理解方面的局限性，特别是在自动驾驶领域的应用。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)展现出强大的视觉-语言推理能力，但缺乏稳健的3D空间理解能力，这对自动驾驶至关重要。这种局限性源于两个关键挑战：(1)构建易于获取且有效的3D表示困难，需要昂贵的手动标注；(2)由于缺乏大规模3D视觉语言预训练，VLMs会丢失细粒度的空间细节。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在3D空间理解方面的局限性，特别是在自动驾驶领域的应用，通过创新方法提升模型的3D空间理解能力。&lt;h4&gt;方法&lt;/h4&gt;OccVLA框架将密集的3D占用同时作为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。与依赖显式3D输入的先前方法不同，OccVLA将占用预测视为隐式推理过程，在推理过程中可以跳过而不会导致性能下降，因此不会增加额外的计算开销。&lt;h4&gt;主要发现&lt;/h4&gt;OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越的性能，为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。&lt;h4&gt;结论&lt;/h4&gt;OccVLA通过创新地将3D占用表示集成到多模态推理过程中，有效解决了MLLMs在3D空间理解方面的局限性，为自动驾驶领域提供了一个高效且实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已经展现出强大的视觉-语言推理能力，但仍然缺乏稳健的3D空间理解能力，这对于自动驾驶至关重要。这种局限性源于两个关键挑战：(1)在没有昂贵手动标注的情况下构建易于获取且有效的3D表示存在困难，以及(2)由于缺乏大规模3D视觉语言预训练，VLMs会丢失细粒度的空间细节。为了解决这些挑战，我们提出了OccVLA，一种新颖的框架，将3D占用表示集成到统一的多模态推理过程中。与依赖显式3D输入的先前方法不同，OccVLA将密集的3D占用同时作为预测输出和监督信号，使模型能够直接从2D视觉输入中学习细粒度的空间结构。占用预测被视为隐式推理过程，在推理过程中可以跳过而不会导致性能下降，因此不会增加额外的计算开销。OccVLA在nuScenes基准测试的轨迹规划任务上取得了最先进的结果，并在3D视觉问答任务上表现出优越的性能，为自动驾驶提供了一种可扩展、可解释且完全基于视觉的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)缺乏稳健3D空间理解能力的问题，这对自动驾驶至关重要。这个问题很重要，因为自动驾驶需要强大的3D感知能力进行定位和导航，而3D感知的保真度直接影响下游决策的安全性。现有方法要么依赖昂贵的人工3D标注数据，要么在处理3D输入时丢失细粒度空间细节，限制了自动驾驶系统的性能和可扩展性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：VLM-based方法依赖稀疏的文本描述3D标注，而整合3D输入的方法受限于缺乏大规模3D预训练。作者借鉴了自动化标注流程和Transformer模型在占用表示方面的进展，设计了OccVLA框架。该方法通过交叉注意力机制使占用令牌从视觉特征中学习，并在潜在空间中预测占用以解决空间稀疏性问题，同时允许在推理时跳过占用预测以提高效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将密集3D占用表示作为模型的预测输出和监督信号，从2D视觉输入中学习细粒度空间结构，且推理时可跳过占用预测过程。整体流程包括：1)占用预测阶段：占用令牌通过交叉注意力获取视觉特征，在潜在空间预测占用后映射回高分辨率空间；2)运动规划阶段：分解为元动作预测(速度和方向)和坐标生成；3)三阶段训练：自动驾驶场景预训练、占用-语言联合训练和规划头训练，最终实现从视觉输入到未来轨迹的端到端处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新颖的OccVLA框架，将3D占用作为预测输出和监督信号而非输入；2)占用预测可作为隐式推理过程，推理时可跳过而不增加计算开销；3)通过占用监督增强VLM的3D表示能力；4)输出可解释且可定量评估。相比之前工作，该方法不依赖昂贵3D标注或额外3D传感器，保留了VLM的泛化能力，在nuScenes上实现了最先进的轨迹规划性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OccVLA通过将3D占用预测作为视觉语言模型的隐式推理和监督信号，实现了无需额外3D传感器输入的高效3D空间理解，并在自动驾驶任务中取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong vision-languagereasoning abilities but still lack robust 3D spatial understanding, which iscritical for autonomous driving. This limitation stems from two key challenges:(1) the difficulty of constructing accessible yet effective 3D representationswithout expensive manual annotations, and (2) the loss of fine-grained spatialdetails in VLMs due to the absence of large-scale 3D vision-languagepretraining. To address these challenges, we propose OccVLA, a novel frameworkthat integrates 3D occupancy representations into a unified multimodalreasoning process. Unlike prior approaches that rely on explicit 3D inputs,OccVLA treats dense 3D occupancy as both a predictive output and a supervisorysignal, enabling the model to learn fine-grained spatial structures directlyfrom 2D visual inputs. The occupancy predictions are regarded as implicitreasoning processes and can be skipped during inference without performancedegradation, thereby adding no extra computational overhead. OccVLA achievesstate-of-the-art results on the nuScenes benchmark for trajectory planning anddemonstrates superior performance on 3D visual question-answering tasks,offering a scalable, interpretable, and fully vision-based solution forautonomous driving.</description>
      <author>example@mail.com (Ruixun Liu, Lingyu Kong, Derun Li, Hang Zhao)</author>
      <guid isPermaLink="false">2509.05578v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Musculoskeletal simulation of limb movement biomechanics in Drosophila melanogaster</title>
      <link>http://arxiv.org/abs/2509.06426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了第一个果蝇腿部的3D数据驱动肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现，能够模拟果蝇行为并研究肌肉协同作用和关节特性对学习的影响。&lt;h4&gt;背景&lt;/h4&gt;计算模型对于理解神经、生物力学和物理系统如何协调动物行为至关重要。尽管已有果蝇神经系统、肌肉和外骨骼的完整重建，但仍缺乏基于解剖学和物理学的果蝇腿部肌肉模型，这种模型是连接运动神经元活动和关节运动的必要桥梁。&lt;h4&gt;目的&lt;/h4&gt;开发第一个果蝇腿部的3D数据驱动肌肉骨骼模型，研究运动控制，了解生物力学如何促进复杂肢体运动的产生，并用于控制人工智能体在模拟环境中生成自然柔顺的运动。&lt;h4&gt;方法&lt;/h4&gt;创建3D数据驱动的果蝇腿部肌肉骨骼模型，使用基于高分辨率X射线扫描的Hill型肌肉表示；提出构建肌肉模型的流程和优化未知肌肉参数的方法；将模型与果蝇行为数据结合实现行为重现；在MuJoCo中训练模仿学习策略测试关节特性对学习速度的影响。&lt;h4&gt;主要发现&lt;/h4&gt;模拟行走和梳理行为中的肌肉活动，预测了可实验测试的协调肌肉协同作用；阻尼和刚度特性有助于提高学习速度。&lt;h4&gt;结论&lt;/h4&gt;该模型使得在实验上易于处理的模式生物中研究运动控制成为可能，提供了关于生物力学如何促进复杂肢体运动产生的见解；同时可用于控制具身人工智能体在模拟环境中生成自然和柔顺的运动。&lt;h4&gt;翻译&lt;/h4&gt;计算模型对于推进我们对神经、生物力学和物理系统如何相互作用以协调动物行为的理解至关重要。尽管已有果蝇中枢神经系统、肌肉和外骨骼的近乎完整的重建，但仍然缺乏基于解剖学和物理学的果蝇腿部肌肉模型。这些模型提供了运动神经元活动和关节运动之间不可或缺的桥梁。在此，我们介绍了第一个果蝇腿部的3D数据驱动肌肉骨骼模型，在OpenSim和MuJoCo仿真环境中实现。我们的模型纳入了基于来自多个固定标本的高分辨率X射线扫描的Hill型肌肉表示。我们提出了一个使用形态成像数据构建肌肉模型的流程，以及优化果蝇特定未知肌肉参数的方法。随后，我们将肌肉骨骼模型与来自行为果蝇的详细3D姿态估计数据结合，在OpenSim中实现肌肉驱动的行为重现。对行走和梳理行为中肌肉活动的模拟预测了可实验测试的协调肌肉协同作用。此外，通过在MuJoCo中训练模仿学习策略，我们测试了不同被动关节特性对学习速度的影响，发现阻尼和刚度有助于学习。总体而言，我们的模型使得在实验上易于处理的模式生物中研究运动控制成为可能，提供了关于生物力学如何促进复杂肢体运动产生的见解。此外，我们的模型可用于控制具身人工智能体，在模拟环境中生成自然和柔顺的运动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computational models are critical to advance our understanding of how neural,biomechanical, and physical systems interact to orchestrate animal behaviors.Despite the availability of near-complete reconstructions of the Drosophilamelanogaster central nervous system, musculature, and exoskeleton, anatomicallyand physically grounded models of fly leg muscles are still missing. Thesemodels provide an indispensable bridge between motor neuron activity and jointmovements. Here, we introduce the first 3D, data-driven musculoskeletal modelof Drosophila legs, implemented in both OpenSim and MuJoCo simulationenvironments. Our model incorporates a Hill-type muscle representation based onhigh-resolution X-ray scans from multiple fixed specimens. We present apipeline for constructing muscle models using morphological imaging data andfor optimizing unknown muscle parameters specific to the fly. We then combineour musculoskeletal models with detailed 3D pose estimation data from behavingflies to achieve muscle-actuated behavioral replay in OpenSim. Simulations ofmuscle activity across diverse walking and grooming behaviors predictcoordinated muscle synergies that can be tested experimentally. Furthermore, bytraining imitation learning policies in MuJoCo, we test the effect of differentpassive joint properties on learning speed and find that damping and stiffnessfacilitate learning. Overall, our model enables the investigation of motorcontrol in an experimentally tractable model organism, providing insights intohow biomechanics contribute to generation of complex limb movements. Moreover,our model can be used to control embodied artificial agents to generatenaturalistic and compliant locomotion in simulated environments.</description>
      <author>example@mail.com (Pembe Gizem Özdil, Chuanfang Ning, Jasper S. Phelps, Sibo Wang-Chen, Guy Elisha, Alexander Blanke, Auke Ijspeert, Pavan Ramdya)</author>
      <guid isPermaLink="false">2509.06426v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning LLMs for Human Behavior Prediction in Social Science Experiments</title>
      <link>http://arxiv.org/abs/2509.05830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何通过微调大型语言模型来提高社会科学实验模拟的准确性。研究者构建了SocSci210数据集，包含来自210个开源社会科学实验的290万份参与者回应。通过微调，模型在未见过的研究中表现优异，且能减少偏差。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型为模拟社会科学实验结果提供了强大机会，但如何提高这些模拟的准确性是一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;通过在过去的个体层面回应数据上直接微调大型语言模型，提高跨不同社会科学领域实验模拟的准确性。&lt;h4&gt;方法&lt;/h4&gt;构建SocSci210数据集，包含210个开源社会科学实验中400,491名参与者的290万份回应，并通过微调训练模型，特别是Socrates-Qwen-14B模型。&lt;h4&gt;主要发现&lt;/h4&gt;在完全未见过的研究中，Socrates-Qwen-14B模型预测与人类回应分布的 alignment 比基础模型高26%，比GPT-4o高13%；在研究条件子集上微调，对新未见条件的泛化能力提高71%；通过微调，人口统计公平性偏差降低10.6%。&lt;h4&gt;结论&lt;/h4&gt;由于社会科学经常生成丰富、主题特定的数据集，在这些数据上进行微调可以实现更准确的实验假设筛选模拟。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型为模拟社会科学实验结果提供了强大机会。在本工作中，我们证明直接在过往实验的个体层面回应上微调大型语言模型，能显著提高跨不同社会科学领域此类模拟的准确性。我们通过自动流程构建了SocSci210，这是一个包含来自210个开源社会科学实验的400,491名参与者的290万份回应的数据集。通过微调，我们实现了多个层次的泛化。在完全未见过的研究中，我们最强大的模型Socrates-Qwen-14B产生的预测与不同条件下对多样化结果问题的人类回应分布更加一致，相对于其基础模型(Qwen2.5-14B)提高了26%，优于GPT-4o达13%。通过在研究条件的子集上进行微调，对新的未见条件的泛化特别稳健，提高了71%。由于SocSci210包含丰富的人口统计信息，我们通过微调将人口统计公平性(一种偏差度量)降低了10.6%。由于社会科学常规生成丰富、主题特定的数据集，我们的研究结果表明，在这些数据上进行微调可以 enable 更准确的实验假设筛选模拟。我们在stanfordhci.github.io/socrates发布了我们的数据、模型和微调代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) offer a powerful opportunity to simulate theresults of social science experiments. In this work, we demonstrate thatfinetuning LLMs directly on individual-level responses from past experimentsmeaningfully improves the accuracy of such simulations across diverse socialscience domains. We construct SocSci210 via an automatic pipeline, a datasetcomprising 2.9 million responses from 400,491 participants in 210 open-sourcesocial science experiments. Through finetuning, we achieve multiple levels ofgeneralization. In completely unseen studies, our strongest model,Socrates-Qwen-14B, produces predictions that are 26% more aligned withdistributions of human responses to diverse outcome questions under varyingconditions relative to its base model (Qwen2.5-14B), outperforming GPT-4o by13%. By finetuning on a subset of conditions in a study, generalization to newunseen conditions is particularly robust, improving by 71%. Since SocSci210contains rich demographic information, we reduce demographic parity, a measureof bias, by 10.6% through finetuning. Because social sciences routinelygenerate rich, topic-specific datasets, our findings indicate that finetuningon such data could enable more accurate simulations for experimental hypothesisscreening. We release our data, models and finetuning code atstanfordhci.github.io/socrates.</description>
      <author>example@mail.com (Akaash Kolluri, Shengguang Wu, Joon Sung Park, Michael S. Bernstein)</author>
      <guid isPermaLink="false">2509.05830v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Anticipatory Fall Detection in Humans with Hybrid Directed Graph Neural Networks and Long Short-Term Memory</title>
      <link>http://arxiv.org/abs/2509.05337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at IEEE RO-MAN 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合动态图神经网络和长短期记忆网络的混合模型，用于预测人类跌倒。该模型将运动预测和步态分类任务解耦，能够高精度地预测跌倒并监控稳定到跌倒之间的过渡状态。&lt;h4&gt;背景&lt;/h4&gt;检测和预防人类跌倒是辅助机器人系统的关键组成部分。虽然跌倒检测已取得进展，但对跌倒发生前的预测以及稳定性与即将发生的跌倒之间过渡状态的分析尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种预期跌倒检测方法，能够高精度地预测跌倒，并分析稳定与即将跌倒之间的过渡状态。&lt;h4&gt;方法&lt;/h4&gt;提出一种混合模型，结合动态图神经网络（DGNN）和长短期记忆（LSTM）网络，将运动预测和步态分类任务解耦。使用从视频序列中提取的实时骨骼特征作为输入，DGNN作为分类器区分三种步态状态，LSTM网络预测后续时间步中的人体运动以实现早期跌倒检测。&lt;h4&gt;主要发现&lt;/h4&gt;模型使用OUMVLP-Pose和URFD数据集训练验证后，在预测误差和识别准确性方面优于仅依赖DGNN的模型和文献中的模型。解耦预测和分类比统一处理问题更有效，且能监控过渡状态，为辅助系统提供额外价值。&lt;h4&gt;结论&lt;/h4&gt;解耦预测和分类任务的方法可有效提高跌倒预测性能，且能够监控过渡状态，为增强先进辅助系统的功能提供有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;检测和预防人类跌倒是辅助机器人系统的关键组成部分。虽然跌倒检测方面已取得显著进展，但对跌倒发生前的预测以及稳定性与即将发生的跌倒之间过渡状态的分析尚未被探索。本文提出了一种预期跌倒检测方法，利用结合动态图神经网络和长短期记忆网络的混合模型，将运动预测和步态分类任务解耦，从而高精度地预测跌倒。我们的方法使用从视频序列中提取的实时骨骼特征作为模型输入。DGNN作为分类器，区分三种步态状态：稳定、过渡和跌倒。基于LSTM的网络随后预测后续时间步中的人体运动，实现跌倒的早期检测。所提模型使用OUMVLP-Pose和URFD数据集进行训练和验证，在预测误差和识别准确性方面表现出优于仅依赖DGNN的模型和文献中的模型。结果表明，与仅使用DGNN解决统一问题相比，解耦预测和分类可以提高性能。此外，我们的方法允许监控过渡状态，为增强先进辅助系统的功能提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting and preventing falls in humans is a critical component of assistiverobotic systems. While significant progress has been made in detecting falls,the prediction of falls before they happen, and analysis of the transient statebetween stability and an impending fall remain unexplored. In this paper, wepropose a anticipatory fall detection method that utilizes a hybrid modelcombining Dynamic Graph Neural Networks (DGNN) with Long Short-Term Memory(LSTM) networks that decoupled the motion prediction and gait classificationtasks to anticipate falls with high accuracy. Our approach employs real-timeskeletal features extracted from video sequences as input for the proposedmodel. The DGNN acts as a classifier, distinguishing between three gait states:stable, transient, and fall. The LSTM-based network then predicts humanmovement in subsequent time steps, enabling early detection of falls. Theproposed model was trained and validated using the OUMVLP-Pose and URFDdatasets, demonstrating superior performance in terms of prediction error andrecognition accuracy compared to models relying solely on DGNN and models fromliterature. The results indicate that decoupling prediction and classificationimproves performance compared to addressing the unified problem using only theDGNN. Furthermore, our method allows for the monitoring of the transient state,offering valuable insights that could enhance the functionality of advancedassistance systems.</description>
      <author>example@mail.com (Younggeol Cho, Gokhan Solak, Olivia Nocentini, Marta Lorenzini, Andrea Fortuna, Arash Ajoudani)</author>
      <guid isPermaLink="false">2509.05337v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>UrbanTwin: High-Fidelity Synthetic Replicas of Roadside Lidar Datasets</title>
      <link>http://arxiv.org/abs/2509.06781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了UrbanTwin数据集，这是三个公共路边激光雷达数据集的高保真实时副本：LUMPI、V2X-Real-IC和TUMTraf-I。每个数据集包含10K个带注释的帧，包括3D边界框、实例分割标签、跟踪ID和语义分割标签。&lt;h4&gt;背景&lt;/h4&gt;激光雷达感知任务需要大量高质量数据，但真实数据的获取可能受到限制，且场景多样性有限。&lt;h4&gt;目的&lt;/h4&gt;创建高质量的合成数据集，能够替代真实数据用于激光雷达感知任务，并增强现有基准数据集。&lt;h4&gt;方法&lt;/h4&gt;通过仿真激光雷达传感器在精确建模的数字孪生中合成数据，基于实际位置的周围几何形状、道路对齐和车辆运动模式建模。通过统计和结构相似性分析评估合成数据与真实数据的对齐情况，并训练3D目标检测模型进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据与真实数据高度相似，仅使用合成数据训练的模型在真实、未见数据上的检测性能优于使用真实数据训练的模型。UrbanTwin数据集可通过修改模拟设计来适应测试自定义场景。&lt;h4&gt;结论&lt;/h4&gt;UrbanTwin数据集是首批能够替代领域内真实世界数据集用于激光雷达感知任务的数字合成数据集，通过增加样本量和场景多样性有效增强了现有基准数据集。&lt;h4&gt;翻译&lt;/h4&gt;这篇文章介绍了UrbanTwin数据集 - 三个公共路边激光雷达数据集的高保真真实副本：LUMPI、V2X-Real-IC和TUMTraf-I。每个UrbanTwin数据集包含10K个注释帧，对应于一个公共数据集。注释包括六个类别的3D边界框、实例分割标签和跟踪ID，以及九个类别的语义分割标签。这些数据集是在真实数字孪生中使用仿真的激光雷达传感器合成的，基于实际位置的周围几何形状、车道级别的道路对齐以及交叉口的车道拓扑和车辆运动模式建模。由于精确的数字孪生建模，合成数据集与真实数据很好地对齐，为训练深度学习模型提供了强大的独立和增强价值。我们通过统计和结构相似性分析评估了合成数据与真实数据的对齐情况，并通过仅使用合成数据训练3D目标检测模型并在真实数据上进行测试，进一步证明了它们的效用。与使用真实数据训练的模型相比，高相似度分数和改进的检测性能表明，UrbanTwin数据集通过增加样本量和场景多样性有效地增强了现有基准数据集。此外，数字孪生可以通过修改模拟的设计和动力学来适应测试自定义场景。据我们所知，这些是首批能够替代领域内真实世界数据集用于激光雷达感知任务的数字合成数据集。UrbanTwin数据集可在https://dataverse.harvard.edu/dataverse/ucf-ut公开获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决创建高质量路边激光雷达数据集的高成本和低效率问题。当前真实世界标注的激光雷达数据集需要大量人力、时间和资金投入，而现有模拟环境与真实世界存在明显差距，缺乏专门针对路边激光雷达应用的合成数据集。这个问题在研究中很重要，因为激光雷达技术对智能交通系统的感知算法发展至关重要，高质量数据集是训练和评估3D感知算法的基础，而现有数据集的局限性阻碍了相关研究的进展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有数据集创建的高成本和现有模拟器的局限性，然后提出使用数字孪生技术来精确建模真实世界场景的物理结构和行为动态。他们借鉴了现有的激光雷达模拟技术，如Manivasagam的配对场景方法学和Haider的精确光线追踪模型，以及数据驱动的生成方法如CoLiGen框架。但作者的方法从根本上不同于这些工作，不是通过事后数据驱动适应来解决sim-to-real差距，而是通过高保真数字孪生建模从源头提高模拟真实性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用高保真数字孪生技术创建与真实世界路边激光雷达数据集高度匹配的合成数据集，同时模拟真实世界场景的物理结构和行为动态。整体流程包括：1)使用卫星图像和真实位置测量构建精确的3D环境模型；2)配置虚拟激光雷达传感器匹配真实传感器规格；3)随机生成符合交通规则的动态元素；4)使用CARLA模拟器在数字孪生环境中运行模拟；5)生成带标注的点云数据；6)通过统计和结构相似性分析验证数据质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门针对路边激光雷达应用的合成数据集；2)高保真数字孪生建模方法，整合静态几何和动态行为；3)通过精确建模实现最小化sim-to-real差距；4)证明完全在合成数据上训练的模型可匹配或超过真实数据训练的性能。相比之前工作，UrbanTwin更专注于路边激光雷达应用而非车辆-基础设施协作场景；从根本上提高模拟真实性而非事后数据驱动适应；实现了比之前合成数据集更接近真实世界的质量；不仅用于数据增强，还可作为独立训练数据集使用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanTwin通过高保真数字孪生技术创建了首个能够完全替代领域内真实世界数据集的合成激光雷达数据集，显著降低了sim-to-real差距，并证明了其在3D物体检测等任务中的卓越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article presents UrbanTwin datasets - high-fidelity, realistic replicasof three public roadside lidar datasets: LUMPI, V2X-Real-IC, and TUMTraf-I.Each UrbanTwin dataset contains 10K annotated frames corresponding to one ofthe public datasets. Annotations include 3D bounding boxes, instancesegmentation labels, and tracking IDs for six object classes, along withsemantic segmentation labels for nine classes. These datasets are synthesizedusing emulated lidar sensors within realistic digital twins, modeled based onsurrounding geometry, road alignment at lane level, and the lane topology andvehicle movement patterns at intersections of the actual locationscorresponding to each real dataset. Due to the precise digital twin modeling,the synthetic datasets are well aligned with their real counterparts, offeringstrong standalone and augmentative value for training deep learning models ontasks such as 3D object detection, tracking, and semantic and instancesegmentation. We evaluate the alignment of the synthetic replicas throughstatistical and structural similarity analysis with real data, and furtherdemonstrate their utility by training 3D object detection models solely onsynthetic data and testing them on real, unseen data. The high similarityscores and improved detection performance, compared to the models trained onreal data, indicate that the UrbanTwin datasets effectively enhance existingbenchmark datasets by increasing sample size and scene diversity. In addition,the digital twins can be adapted to test custom scenarios by modifying thedesign and dynamics of the simulations. To our knowledge, these are the firstdigitally synthesized datasets that can replace in-domain real-world datasetsfor lidar perception tasks. UrbanTwin datasets are publicly available athttps://dataverse.harvard.edu/dataverse/ucf-ut.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.06781v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>S-LAM3D: Segmentation-Guided Monocular 3D Object Detection via Feature Space Fusion</title>
      <link>http://arxiv.org/abs/2509.05999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages. Accepted to MMSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于预计算分割信息先验的解耦策略，用于单目3D目标检测，将分割信息直接融合到特征空间中指导检测，而不扩展模型或联合学习先验。&lt;h4&gt;背景&lt;/h4&gt;单目3D目标检测是一项具有挑战性的计算机视觉任务，因为输入是单个2D图像，缺乏深度线索，使得深度估计成为一个不适定的问题。&lt;h4&gt;目的&lt;/h4&gt;评估额外分割信息对现有检测管道的影响，而不添加额外的预测分支，并探索理解输入数据对减少对额外传感器或训练数据需求的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出一种解耦策略，基于注入预计算的分割信息先验，并将它们直接融合到特征空间中用于指导检测，不扩展检测模型或联合学习先验。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI 3D目标检测基准上的评估表明，所提出的方法对于场景中的小物体（行人和骑行者）的表现优于仅依赖RGB图像特征的等效架构。&lt;h4&gt;结论&lt;/h4&gt;理解输入数据可以平衡对额外传感器或训练数据的需求，证明了分割信息对单目3D目标检测的积极影响。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测是一项具有挑战性的计算机视觉任务，因为所使用的输入是单个2D图像，缺乏任何深度线索，并将深度估计问题作为一个不适定的问题。现有解决方案利用从输入中提取的信息，使用卷积神经网络或Transformer架构作为特征提取主干，然后使用特定的检测头来预测3D参数。在本文中，我们介绍了一种基于注入预计算分割信息先验的解耦策略，并将它们直接融合到特征空间中用于指导检测，而不扩展检测模型或联合学习先验。重点在于评估额外分割信息对现有检测管道的影响，而不添加额外的预测分支。所提出的方法在KITTI 3D目标检测基准上进行了评估，对于场景中的小物体（行人和骑行者）的表现优于仅依赖RGB图像特征的等效架构，证明了理解输入数据可以平衡对额外传感器或训练数据的需求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D物体检测的挑战问题。单目3D物体检测使用单个2D图像作为输入，缺乏深度线索，导致这是一个'不适定'的问题。这个问题在自动驾驶和机器人领域非常重要，因为准确检测3D物体（如汽车、行人、骑行者）的位置、方向和尺寸对于安全导航至关重要。单目相机比多传感器系统更便宜、更轻量，但缺乏深度信息使其成为最具挑战性的任务，尤其是对小物体的检测。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单目3D物体检测的挑战，认识到缺乏深度信息是主要问题。他们借鉴了'引导式单目3D物体检测'的概念，参考了使用深度图或分割图作为先验信息的工作。特别是受到了DetAny3D和MonoCInIS的启发，但选择使用分割图而非深度图，因为分割图能提供更丰富的语义信息。作者设计了一个'解耦'策略，使用Grounded SAM生成高质量分割先验，然后探索不同的融合策略和点，最终确定了元素级乘法融合和特征聚合后注入分割先验为最佳方案，整个过程不需要联合训练或添加额外预测分支。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过注入预计算的分割信息先验来增强单目3D物体检测性能，分割图作为一种注意力机制，能强调图像中与物体相关的区域，抑制不相关背景。整体流程：1)使用Grounded SAM生成分割图；2)RGB图像通过Transformer主干提取视觉特征；3)使用深度层聚合(DLA)多尺度特征；4)将分割图与特征进行空间对齐和标准化；5)通过元素级乘法融合将分割信息与视觉特征结合；6)最后通过卷积层将融合特征投影到64维空间用于3D检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)解耦的分割先验注入策略，无需联合训练；2)简单有效的元素级乘法融合方法；3)确定特征聚合后为最佳融合点；4)使用Grounded SAM生成高质量分割先验。相比之前工作的不同：与多模态方法相比，不需要额外传感器；与深度引导方法相比，提供更丰富的语义信息；与MonoCInIS相比，使用更强大的分割模型和探索了更多融合策略；与DetAny3D相比，更专注于分割先验且更轻量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; S-LAM3D通过解耦的分割先验注入和轻量级特征融合方法，显著提升了单目3D物体检测中小物体（如行人和骑行者）的检测性能，同时保持模型轻量化。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D Object Detection represents a challenging Computer Vision taskdue to the nature of the input used, which is a single 2D image, lacking in anydepth cues and placing the depth estimation problem as an ill-posed one.Existing solutions leverage the information extracted from the input by usingConvolutional Neural Networks or Transformer architectures as featureextraction backbones, followed by specific detection heads for 3D parametersprediction. In this paper, we introduce a decoupled strategy based on injectingprecomputed segmentation information priors and fusing them directly into thefeature space for guiding the detection, without expanding the detection modelor jointly learning the priors. The focus is on evaluating the impact ofadditional segmentation information on existing detection pipelines withoutadding additional prediction branches. The proposed method is evaluated on theKITTI 3D Object Detection Benchmark, outperforming the equivalent architecturethat relies only on RGB image features for small objects in the scene:pedestrians and cyclists, and proving that understanding the input data canbalance the need for additional sensors or training data.</description>
      <author>example@mail.com (Diana-Alexandra Sas, Florin Oniga)</author>
      <guid isPermaLink="false">2509.05999v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>CRAB: Camera-Radar Fusion for Reducing Depth Ambiguity in Backward Projection based View Transformation</title>
      <link>http://arxiv.org/abs/2509.05785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CRAB的新型相机-雷达融合3D目标检测和分割模型，通过利用雷达信息缓解深度歧义问题，在基于反向投影的视图转换中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;鸟瞰图(BEV)中的相机-雷达融合3D目标检测方法因这两种传感器的互补特性和成本效益而受到关注。先前使用前向投影的方法在生成稀疏BEV特征方面存在困难，而采用反向投影的方法则忽略了深度歧义，导致误报。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述局限性，提出一种新的相机-雷达融合3D目标检测和分割模型，使用反向投影并利用雷达来缓解深度歧义问题。&lt;h4&gt;方法&lt;/h4&gt;CRAB在视图转换过程中将透视视图图像上下文特征聚合到BEV查询中，通过结合图像中密集但不可靠的深度分布与雷达占用图中稀疏但精确的深度信息，提高沿同一射线的查询之间的深度区分度。同时引入包含雷达上下文信息的特征图的空间交叉注意力机制，增强对3D场景的理解。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes开放数据集上评估时，所提出的方法在基于反向投影的相机-雷达融合方法中取得了最先进的性能，3D目标检测达到62.4%的NDS和54.0%的mAP。&lt;h4&gt;结论&lt;/h4&gt;CRAB模型通过利用雷达信息缓解深度歧义问题，有效提高了基于反向投影的相机-雷达融合3D目标检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于相机-雷达融合的鸟瞰图(BEV)3D目标检测方法因这些传感器的互补特性和成本效益而受到关注。先前使用前向投影的方法在生成稀疏BEV特征方面存在困难，而采用反向投影的方法则忽略了深度歧义，导致误报。在本文中，为了解决上述局限性，我们提出了一种名为CRAB的新型相机-雷达融合3D目标检测和分割模型，使用利用雷达缓解深度歧义的反向投影。在视图转换过程中，CRAB将透视视图图像上下文特征聚合到BEV查询中。它通过将图像中密集但不可靠的深度分布与雷达占用图中稀疏但精确的深度信息相结合，提高了沿同一射线的查询之间的深度区分度。我们还引入了包含雷达上下文信息的特征图的空间交叉注意力机制，以增强对3D场景的理解。在nuScenes开放数据集上进行评估时，我们提出的方法在基于反向投影的相机-雷达融合方法中取得了最先进的性能，3D目标检测达到62.4%的NDS和54.0%的mAP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决基于后向投影的视角变换方法中的深度模糊问题。这个问题在自动驾驶和移动机器人领域非常重要，因为它直接影响3D目标检测的准确性。深度模糊导致沿同一射线的查询获得相同特征，无法区分物体深度，产生假阳性检测结果，影响自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：前向投影方法生成稀疏BEV特征，后向投影方法存在深度模糊问题。他们注意到相机提供密集但不可靠的深度信息，而雷达提供稀疏但精确的深度信息，决定结合两者优势。方法设计借鉴了BEVFormer和DFA3D的后向投影框架，CRN中利用雷达占用的思想，以及变形注意力机制。作者设计了两个核心模块：雷达占用引导的空间交叉注意力(ROSCA)和雷达上下文感知的空间交叉注意力(RCSCA)，分阶段解决深度模糊并整合雷达上下文信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合相机和雷达的互补优势，解决后向投影中的深度模糊问题。整体流程包括：1)输入处理：使用图像主干网络提取图像特征并获取深度分布，处理雷达点云并提取雷达特征；2)视角变换：首先通过ROSCA模块结合相机和雷达的占用信息解决深度模糊问题，然后通过RCSCA模块整合雷达上下文信息增强3D场景理解；3)任务头：将融合后的BEV特征输入到特定任务的头部进行3D目标检测或分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出ROSCA模块，结合相机和雷达的占用信息解决深度模糊问题；2)提出RCSCA模块，利用雷达上下文信息增强3D场景理解；3)采用锥形视图而非鸟瞰视图的雷达特征图，确保特征一致性。相比前向投影方法，CRAB生成更密集的BEV特征；相比其他后向投影方法，CRAB专门解决深度模糊问题；相比仅使用图像深度分布的方法，CRAB利用雷达精确深度信息校准相机估计；相比传统融合方法，CRAB在统一BEV空间中融合，适用于多种下游任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CRAB通过创新的相机-雷达融合方法，有效解决了基于后向投影的视角变换中的深度模糊问题，显著提高了3D目标检测和分割的准确性，特别是在恶劣天气条件下展现了优越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, camera-radar fusion-based 3D object detection methods in bird's eyeview (BEV) have gained attention due to the complementary characteristics andcost-effectiveness of these sensors. Previous approaches using forwardprojection struggle with sparse BEV feature generation, while those employingbackward projection overlook depth ambiguity, leading to false positives. Inthis paper, to address the aforementioned limitations, we propose a novelcamera-radar fusion-based 3D object detection and segmentation model named CRAB(Camera-Radar fusion for reducing depth Ambiguity in Backward projection-basedview transformation), using a backward projection that leverages radar tomitigate depth ambiguity. During the view transformation, CRAB aggregatesperspective view image context features into BEV queries. It improves depthdistinction among queries along the same ray by combining the dense butunreliable depth distribution from images with the sparse yet precise depthinformation from radar occupancy. We further introduce spatial cross-attentionwith a feature map containing radar context information to enhance thecomprehension of the 3D scene. When evaluated on the nuScenes open dataset, ourproposed approach achieves a state-of-the-art performance among backwardprojection-based camera-radar fusion methods with 62.4\% NDS and 54.0\% mAP in3D object detection.</description>
      <author>example@mail.com (In-Jae Lee, Sihwan Hwang, Youngseok Kim, Wonjune Kim, Sanmin Kim, Dongsuk Kum)</author>
      <guid isPermaLink="false">2509.05785v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>3DPillars: Pillar-based two-stage 3D object detection</title>
      <link>http://arxiv.org/abs/2509.05780v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的两阶段3D检测框架，利用伪图像表示缩小了PointPillars与最先进方法之间的性能差距，同时保持了其效率。&lt;h4&gt;背景&lt;/h4&gt;PointPillars是最快的3D目标检测器之一，利用伪图像表示编码场景中3D对象的特征，但通常被最先进的3D检测方法超越。&lt;h4&gt;目的&lt;/h4&gt;引入第一个利用伪图像表示的两阶段3D检测框架，缩小PointPillars与最先进方法之间的性能差距，同时保持其效率。&lt;h4&gt;方法&lt;/h4&gt;提出两个新颖组件：1) 3DPillars架构，通过可分离体素特征模块从伪图像表示中高效学习基于3D体素的特征；2) 带有稀疏场景上下文特征模块的RoI头，聚合多尺度特征以获得稀疏场景特征，有效采用两阶段管道并充分利用场景上下文信息。&lt;h4&gt;主要发现&lt;/h4&gt;在KITTI和Waymo Open数据集上的实验证明，该方法在速度和准确性之间取得了良好的平衡。&lt;h4&gt;结论&lt;/h4&gt;该研究成功克服了PointPillars的两个主要局限性：无法保留精确的3D结构和难以采用两阶段检测管道，同时保持了PointPillars的效率。&lt;h4&gt;翻译&lt;/h4&gt;PointPillars是最快的3D目标检测器，它利用伪图像表示来编码场景中3D对象的特征。尽管高效，PointPillars通常由于以下限制而被最先进的3D检测方法超越：1) 伪图像表示无法保留精确的3D结构，2) 它们使得难以采用使用3D对象提议的两阶段检测管道，而这种方法通常比单阶段方法表现更好。我们在本文中引入了第一个利用伪图像表示的两阶段3D检测框架，缩小了PointPillars与最先进方法之间的性能差距，同时保留了其效率。我们的框架由两个新颖的组件组成，克服了PointPillars的上述局限性：首先，我们引入了一种新的CNN架构，称为3DPillars，它能够使用2D卷积从伪图像表示中高效学习基于3D体素的特征。3DPillars背后的基本思想是，来自体素的3D特征可以被视为伪图像的堆叠。为了实现这一想法，我们提出了一个可分离的体素特征模块，它不使用3D卷积来提取基于体素的特征。其次，我们引入了一个带有稀疏场景上下文特征模块的RoI头，它聚合来自3DPillars的多尺度特征以获得稀疏场景特征。这使得能够有效采用两阶段管道，并充分利用场景的上下文信息来改进3D对象提议。在KITTI和Waymo Open数据集上的实验结果证明了我们方法的有效性和效率，在速度和准确性方面取得了良好的平衡。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决PointPillars方法在3D物体检测中的两个局限性：1)伪图像表示无法保留精确的3D结构；2)难以采用通常性能更好的两阶段检测管道。这个问题在自动驾驶和机器人领域非常重要，因为3D物体检测是理解周围环境的关键组件，精确的3D结构信息对于准确识别和定位物体至关重要，而两阶段检测方法通常比单阶段方法有更好的检测性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了PointPillars的局限性，发现它无法保留精确的3D结构且难以采用两阶段检测框架。作者受到基于体素的方法能够保留精细3D结构的启发，但注意到3D卷积计算成本高。因此提出将3D体素特征视为伪图像堆栈的想法，这样可以用2D卷积高效地提取3D特征。作者借鉴了PointPillars的支柱表示方法、Zhou &amp; Tuzel的体素特征编码(VFE)、Deng et al.的体素RoI池化方法以及Miller et al.的键值记忆模块等现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D体素特征视为沿X、Y、Z轴的伪图像堆栈，从而可以用2D卷积高效提取3D特征；同时引入稀疏场景上下文特征模块，利用全局场景上下文来改进物体建议。整体流程是：1)输入点云被分配到3D体素网格；2)使用VFE层提取初始特征；3)将3D体素特征分割为伪图像堆栈；4)使用SVFM模块提取多尺度视图特定特征；5)将多尺度特征转换为2D BEV特征图；6)使用RPN生成3D物体提议；7)在RoI头中，S2CFM模块聚合多尺度特征，提取RoI特征，结合全局上下文特征生成上下文感知的RoI表示；8)使用全连接层预测3D边界框和置信度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)3DPillars架构，第一个利用伪图像表示的两阶段3D检测框架；2)可分离体素特征模块(SVFM)，通过2D卷积提取视图特定特征；3)带有稀疏场景上下文特征模块(S2CFM)的RoI头，聚合多尺度特征并利用键值记忆模块获取全局场景上下文。相比之前的工作，它保留了更精细的3D结构并支持两阶段检测，使用2D卷积而非3D卷积提高了效率，不需要对点云下采样，并引入了全局场景上下文信息，特别有利于小物体检测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了3DPillars，一种创新的两阶段3D物体检测框架，通过将3D体素特征表示为伪图像堆栈并利用稀疏场景上下文信息，在保持高效的同时显著提高了检测精度，特别是在处理小物体和远距离物体时表现出色。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.eswa.2025.128349&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PointPillars is the fastest 3D object detector that exploits pseudo imagerepresentations to encode features for 3D objects in a scene. Albeit efficient,PointPillars is typically outperformed by state-of-the-art 3D detection methodsdue to the following limitations: 1) The pseudo image representations fail topreserve precise 3D structures, and 2) they make it difficult to adopt atwo-stage detection pipeline using 3D object proposals that typically showsbetter performance than a single-stage approach. We introduce in this paper thefirst two-stage 3D detection framework exploiting pseudo image representations,narrowing the performance gaps between PointPillars and state-of-the-artmethods, while retaining its efficiency. Our framework consists of two novelcomponents that overcome the aforementioned limitations of PointPillars: First,we introduce a new CNN architecture, dubbed 3DPillars, that enables learning 3Dvoxel-based features from the pseudo image representation efficiently using 2Dconvolutions. The basic idea behind 3DPillars is that 3D features from voxelscan be viewed as a stack of pseudo images. To implement this idea, we propose aseparable voxel feature module that extracts voxel-based features without using3D convolutions. Second, we introduce an RoI head with a sparse scene contextfeature module that aggregates multi-scale features from 3DPillars to obtain asparse scene feature. This enables adopting a two-stage pipeline effectively,and fully leveraging contextual information of a scene to refine 3D objectproposals. Experimental results on the KITTI and Waymo Open datasetsdemonstrate the effectiveness and efficiency of our approach, achieving a goodcompromise in terms of speed and accuracy.</description>
      <author>example@mail.com (Jongyoun Noh, Junghyup Lee, Hyekang Park, Bumsub Ham)</author>
      <guid isPermaLink="false">2509.05780v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Integrated Simulation Framework for Adversarial Attacks on Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2509.05332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个新的开源集成模拟框架，用于生成针对自动驾驶车辆感知和通信层的对抗性攻击，通过高保真建模和统一协调系统，有效测试自动驾驶系统在对抗环境下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆依赖复杂的感知和通信系统，使其容易受到对抗性攻击，危及安全。现有模拟框架通常缺乏对多领域对抗性场景的全面支持。&lt;h4&gt;目的&lt;/h4&gt;开发一个开源集成模拟框架，能够生成针对自动驾驶车辆感知和通信层的对抗性攻击，并提供高保真度的物理环境、交通动态和V2X网络建模。&lt;h4&gt;方法&lt;/h4&gt;框架通过统一的核心协调物理环境、交通动态和V2X网络建模，基于单个配置文件同步多个模拟器，支持对LiDAR传感器数据的感知级别攻击和V2X消息操纵、GPS欺骗等通信级别威胁，并集成ROS 2确保与第三方自动驾驶软件栈的兼容性。&lt;h4&gt;主要发现&lt;/h4&gt;通过评估生成的对抗性场景对最先进3D目标检测器的影响，证明在现实条件下，自动驾驶系统的性能会显著下降，验证了框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;该模拟框架为自动驾驶系统对抗性攻击的研究提供了全面工具，有助于提高自动驾驶系统在面对复杂威胁时的安全性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆(AVs)依赖复杂的感知和通信系统，使其容易受到对抗性攻击，可能危及安全。虽然模拟为鲁棒性测试提供了可扩展且安全的环境，但现有框架通常缺乏对多领域对抗性场景的全面建模支持。本文介绍了一个新的开源集成模拟框架，旨在生成针对自动驾驶车辆感知和通信层的对抗性攻击。该框架提供物理环境、交通动态和V2X网络的高保真建模，通过基于单一配置文件同步多个模拟器的统一核心协调这些组件。我们的实现支持对LiDAR传感器数据的多种感知级别攻击，以及V2X消息操纵和GPS欺骗等通信级别威胁。此外，ROS 2集成确保与第三方自动驾驶软件栈的无缝兼容。我们通过评估生成的对抗性场景对最先进3D目标检测器的影响，证明了框架的有效性，显示在现实条件下性能显著下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous vehicles (AVs) rely on complex perception and communicationsystems, making them vulnerable to adversarial attacks that can compromisesafety. While simulation offers a scalable and safe environment for robustnesstesting, existing frameworks typically lack comprehensive supportfor modelingmulti-domain adversarial scenarios. This paper introduces a novel, open-sourceintegrated simulation framework designed to generate adversarial attackstargeting both perception and communication layers of AVs. The frameworkprovides high-fidelity modeling of physical environments, traffic dynamics, andV2X networking, orchestrating these components through a unified core thatsynchronizes multiple simulators based on a single configuration file. Ourimplementation supports diverse perception-level attacks on LiDAR sensor data,along with communication-level threats such as V2X message manipulation and GPSspoofing. Furthermore, ROS 2 integration ensures seamless compatibility withthird-party AV software stacks. We demonstrate the framework's effectiveness byevaluating the impact of generated adversarial scenarios on a state-of-the-art3D object detector, revealing significant performance degradation underrealistic conditions.</description>
      <author>example@mail.com (Christos Anagnostopoulos, Ioulia Kapsali, Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2509.05332v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Harnessing Object Grounding for Time-Sensitive Video Understanding</title>
      <link>http://arxiv.org/abs/2509.06335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GO-Tokenizer的轻量级模块，通过引入基础对象(Grounded Objects)来增强视频大语言模型(Video-LLMs)的时间敏感视频理解(TSV)能力，解决了传统方法中标记长度过长和对象信息噪声敏感的问题。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在时间敏感视频理解任务中面临挑战，现有方法虽然可以通过添加对象注释的文本描述来提高性能，但会引入额外的标记长度和对对象级别信息噪声的敏感性。&lt;h4&gt;目的&lt;/h4&gt;改进视频大语言模型的时间敏感视频理解能力，使其能够更有效地利用帧内基础对象信息，同时避免传统方法的缺点。&lt;h4&gt;方法&lt;/h4&gt;提出GO-Tokenizer，一个轻量级的Video-LLMs附加模块，利用现成的对象检测器即时编码紧凑的对象信息，而不是使用冗长的文本描述。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基础对象可以提升时间敏感视频理解任务性能；2) 使用GO-Tokenizer进行预训练的性能优于普通Video-LLM；3) GO-Tokenizer的性能优于在提示中使用对象文本描述的方法；4) 这种改进在不同模型、数据集和视频理解任务中具有普遍性。&lt;h4&gt;结论&lt;/h4&gt;GO-Tokenizer是一种有效的解决方案，能够增强Video-LLMs的时间敏感视频理解能力，同时避免了传统方法的局限性，在多种场景下表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出通过基础对象(GO)来提高视频大语言模型(Video-LLMs)的时间敏感视频理解(TSV)能力。我们假设TSV任务可以从帧内的基础对象中受益，这一点在我们的初步实验中得到了验证，实验对象是LITA——一种用于推理时间定位的最先进Video-LLM。虽然在提示中加入这些对象注释的文本描述可以提高LITA的性能，但它也引入了额外的标记长度和对对象级别信息噪声的敏感性。为解决这一问题，我们提出了GO-Tokenizer，这是一个轻量级的Video-LLMs附加模块，利用现成的对象检测器即时编码紧凑的对象信息。实验结果表明，使用GO-Tokenizer进行预训练的性能优于普通Video-LLM及其在提示中利用对象文本描述的对应模型。这种增益在不同模型、数据集和视频理解任务(如推理时间定位和密集字幕)中具有普遍性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-08&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose to improve the time-sensitive video understanding (TSV) capabilityof video large language models (Video-LLMs) with grounded objects (GO). Wehypothesize that TSV tasks can benefit from GO within frames, which issupported by our preliminary experiments on LITA, a state-of-the-art Video-LLMfor reasoning temporal localization. While augmenting prompts with textualdescription of these object annotations improves the performance of LITA, italso introduces extra token length and susceptibility to the noise in objectlevel information. To address this, we propose GO-Tokenizer, a lightweightadd-on module for Video-LLMs leveraging off-the-shelf object detectors toencode compact object information on the fly. Experimental results demonstratethat pretraining with GO-Tokenizer outperforms the vanilla Video-LLM and itscounterpart utilizing textual description of objects in the prompt. The gaingeneralizes across different models, datasets and video understanding taskssuch as reasoning temporal localization and dense captioning.</description>
      <author>example@mail.com (Tz-Ying Wu, Sharath Nittur Sridhar, Subarna Tripathi)</author>
      <guid isPermaLink="false">2509.06335v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Language-guided Recursive Spatiotemporal Graph Modeling for Video Summarization</title>
      <link>http://arxiv.org/abs/2509.05604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IJCV, 29 pages, 14 figures, 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VideoGraph方法，将视频建模为语言引导的时空图网络，通过整合语义关系和语言查询来提高视频摘要质量，在多个基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;现有视频摘要方法主要关注帧之间的全局时间关系，忽视了细粒度视觉实体（如对象）与视频内容的关联，且语言引导的视频摘要需要全面理解复杂视频内容。&lt;h4&gt;目的&lt;/h4&gt;为了考虑所有对象之间的语义关系，将视频摘要视为一个语言引导的时空图建模问题，以提高摘要质量。&lt;h4&gt;方法&lt;/h4&gt;提出递归时空图网络VideoGraph，将对象和帧分别作为空间图和时间图的节点，通过图边连接表示语义关系，并引入语言查询增强节点表示，采用递归策略优化初始图并分类关键帧。&lt;h4&gt;主要发现&lt;/h4&gt;VideoGraph在通用和查询导向的视频摘要任务上，无论有监督还是无监督方式，均取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过将视频摘要建模为语言引导的时空图问题，并利用语言查询增强节点表示，能够有效提高视频摘要的性能，生成更具代表性的摘要。&lt;h4&gt;翻译&lt;/h4&gt;视频摘要旨在选择视觉上多样化且能代表给定视频整个故事的关键帧。先前的方法已通过时间建模关注了视频中帧之间的全局互联性。然而，细粒度的视觉实体（如对象）也与视频的主要内容高度相关。此外，最近研究的语言引导视频摘要需要对复杂的现实世界视频进行全面的语言理解。为了考虑所有对象之间的语义关系，本文将视频摘要视为一个语言引导的时空图建模问题。作者提出了递归时空图网络，称为VideoGraph，它将对象和帧分别作为空间图和时间图的节点。每个图中的节点通过图边连接和聚合，表示节点间的语义关系。为了防止边仅基于视觉相似性配置，作者将视频衍生的语言查询整合到图节点表示中，使节点包含语义知识。此外，作者采用递归策略来优化初始图并正确分类每个帧节点为关键帧。在实验中，VideoGraph在多个基准测试中，无论是有监督还是无监督方式，都在通用和查询导向的视频摘要任务上取得了最先进的性能。代码可在https://github.com/park-jungin/videograph获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s11263-025-02577-2&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video summarization aims to select keyframes that are visually diverse andcan represent the whole story of a given video. Previous approaches havefocused on global interlinkability between frames in a video by temporalmodeling. However, fine-grained visual entities, such as objects, are alsohighly related to the main content of the video. Moreover, language-guidedvideo summarization, which has recently been studied, requires a comprehensivelinguistic understanding of complex real-world videos. To consider how all theobjects are semantically related to each other, this paper regards videosummarization as a language-guided spatiotemporal graph modeling problem. Wepresent recursive spatiotemporal graph networks, called VideoGraph, whichformulate the objects and frames as nodes of the spatial and temporal graphs,respectively. The nodes in each graph are connected and aggregated with graphedges, representing the semantic relationships between the nodes. To preventthe edges from being configured with visual similarity, we incorporate languagequeries derived from the video into the graph node representations, enablingthem to contain semantic knowledge. In addition, we adopt a recursive strategyto refine initial graphs and correctly classify each frame node as a keyframe.In our experiments, VideoGraph achieves state-of-the-art performance on severalbenchmarks for generic and query-focused video summarization in both supervisedand unsupervised manners. The code is available athttps://github.com/park-jungin/videograph.</description>
      <author>example@mail.com (Jungin Park, Jiyoung Lee, Kwanghoon Sohn)</author>
      <guid isPermaLink="false">2509.05604v1</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Keye-VL-1.5模型，通过三种创新方法解决了视频理解中的挑战，在视频理解任务和通用多模态基准测试中展现出显著优势。&lt;h4&gt;背景&lt;/h4&gt;近年来大语言模型已扩展到多模态任务，但视频理解仍具挑战性，因为视频具有动态和信息密集的特点。现有模型在处理视频时难以平衡空间分辨率和时间覆盖范围。&lt;h4&gt;目的&lt;/h4&gt;解决视频理解中的基本挑战，开发一种能够更有效处理视频内容的新型模型。&lt;h4&gt;方法&lt;/h4&gt;1. 引入慢速-快速视频编码策略，根据帧间相似度动态分配计算资源；2. 实现渐进式四阶段预训练方法，将模型上下文长度从8K扩展到128K tokens；3. 开发全面的训练后流程，包括5步思维链数据构建、基于GSPO的迭代强化学习和对齐训练。&lt;h4&gt;主要发现&lt;/h4&gt;Keye-VL-1.5在公共基准测试和内部人工评估中显著优于现有模型，尤其在视频理解任务上表现突出，同时在通用多模态基准测试中保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三种关键创新成功解决了视频理解中的基本挑战，实现了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展取得了显著进步，其能力已通过多模态大语言模型扩展到多模态任务。然而，由于视频的动态性和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，难以在空间分辨率和时间覆盖范围之间取得平衡。我们提出了Keye-VL-1.5，通过三个关键创新解决了视频理解中的基本挑战。首先，我们引入了一种新颖的慢速-快速视频编码策略，根据帧间相似度动态分配计算资源，以更高分辨率处理具有显著视觉变化的关键帧（慢速路径），同时以更低分辨率但增加时间覆盖范围处理相对静态的帧（快速路径）。其次，我们实现了渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，使模型能够处理更长的视频和更复杂的视觉内容。第三，我们开发了一个全面的训练后流程，专注于推理增强和人类偏好对齐，包含5步思维链数据构建过程、基于GSPO的迭代强化学习以及对困难案例的渐进式提示提示，以及训练对齐。通过在公共基准测试上的广泛评估和严格的内部人工评估，Keye-VL-1.5显示出比现有模型的显著改进，尤其在视频理解任务上表现出色，同时在通用多模态基准测试中保持有竞争力的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fan, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v3</guid>
      <pubDate>Tue, 09 Sep 2025 16:15:11 +0800</pubDate>
    </item>
    <item>
      <title>MICACL: Multi-Instance Category-Aware Contrastive Learning for Long-Tailed Dynamic Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2509.04344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ISPA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为MICACL的新型多实例学习框架，用于解决动态面部表情识别中的长尾类别分布和时空特征建模复杂性问题。该框架结合了时空依赖建模和长尾对比学习优化，通过特定模块增强特征提取，并采用多尺度类别感知对比学习策略平衡各类别训练。实验证明该方法在野外数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;动态面部表情识别(DFER)面临两个主要挑战：长尾类别分布和时空特征建模的复杂性。现有的基于深度学习的方法虽然提高了DFER性能，但未能有效解决这些问题，导致模型引入偏差。&lt;h4&gt;目的&lt;/h4&gt;克服现有方法的局限性，解决动态面部表情识别中的长尾类别分布问题和时空特征建模复杂性，减少模型诱导偏差，提高模型的鲁棒性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为MICACL的多实例学习框架，包括：1) 图增强实例交互模块(GEIIM)：通过自适应邻接矩阵和多尺度卷积捕获相邻实例间的复杂时空关系；2) 加权实例聚合网络(WIAN)：根据实例重要性动态分配权重，增强实例级特征聚合；3) 多尺度类别感知对比学习(MCCL)策略：平衡主要和次要类别的训练。&lt;h4&gt;主要发现&lt;/h4&gt;在野外数据集(DFEW和FERV39k)上的大量实验表明，MICACL实现了最先进的性能，具有优越的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;MICACL框架有效解决了动态面部表情识别中的关键挑战，通过整合时空依赖建模和长尾对比学习优化，显著提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;动态面部表情识别(DFER)由于长尾类别分布和时空特征建模的复杂性而面临重大挑战。虽然现有的基于深度学习的方法已经提高了DFER性能，但它们通常无法解决这些问题，导致严重的模型诱导偏差。为了克服这些局限性，我们提出了一种名为MICACL的新型多实例学习框架，该框架整合了时空依赖建模和长尾对比学习优化。具体来说，我们设计了图增强实例交互模块(GEIIM)，通过自适应邻接矩阵和多尺度卷积来捕获相邻实例之间的复杂时空关系。为了增强实例级特征聚合，我们开发了加权实例聚合网络(WIAN)，它根据实例重要性动态分配权重。此外，我们引入了多尺度类别感知对比学习(MCCL)策略，以平衡主要和次要类别之间的训练。在野外数据集(即DFEW和FERV39k)上的大量实验表明，MICACL实现了最先进的性能，具有优越的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic facial expression recognition (DFER) faces significant challenges dueto long-tailed category distributions and complexity of spatio-temporal featuremodeling. While existing deep learning-based methods have improved DFERperformance, they often fail to address these issues, resulting in severe modelinduction bias. To overcome these limitations, we propose a novelmulti-instance learning framework called MICACL, which integratesspatio-temporal dependency modeling and long-tailed contrastive learningoptimization. Specifically, we design the Graph-Enhanced Instance InteractionModule (GEIIM) to capture intricate spatio-temporal between adjacent instancesrelationships through adaptive adjacency matrices and multiscale convolutions.To enhance instance-level feature aggregation, we develop the Weighted InstanceAggregation Network (WIAN), which dynamically assigns weights based on instanceimportance. Furthermore, we introduce a Multiscale Category-aware ContrastiveLearning (MCCL) strategy to balance training between major and minorcategories. Extensive experiments on in-the-wild datasets (i.e., DFEW andFERV39k) demonstrate that MICACL achieves state-of-the-art performance withsuperior robustness and generalization.</description>
      <author>example@mail.com (Feng-Qi Cui, Zhen Lin, Xinlong Rao, Anyang Tong, Shiyao Li, Fei Wang, Changlin Chen, Bin Liu)</author>
      <guid isPermaLink="false">2509.04344v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
  <item>
      <title>FlowSeek: Optical Flow Made Easier with Depth Foundation Models and Motion Bases</title>
      <link>http://arxiv.org/abs/2509.05297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 - Project Page: https://flowseek25.github.io/ - Code:  https://github.com/mattpoggi/flowseek&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FlowSeek是一个新颖的光流计算框架，结合了最新的光流网络设计、单图像深度基础模型和低维运动参数化，实现了在有限硬件资源下的高效训练和高精度光流估计。&lt;h4&gt;背景&lt;/h4&gt;光流计算在计算机视觉领域具有重要意义，但现有的先进方法通常需要大量计算资源进行训练，限制了其在资源受限环境中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个资源高效的光流计算框架，能够在有限的硬件条件下实现高性能的光流估计，并保持良好的跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;FlowSeek结合了光流网络设计空间的最新进展、单图像深度基础模型和经典低维运动参数化，实现了紧凑而准确的架构。该方法在单个消费级GPU上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;FlowSeek在硬件预算比最新方法低约8倍的情况下，在Sintel Final和KITTI数据集上实现了比之前最先进的SEA-RAFT方法高10%和15%的相对改进，同时在Spring和LayeredFlow数据集上也表现优异。&lt;h4&gt;结论&lt;/h4&gt;FlowSeek证明了通过结合最新的网络设计、基础模型和经典参数化方法，可以在显著降低硬件需求的同时，实现更优的光流估计性能和跨数据集泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了FlowSeek，这是一个用于光流计算的新颖框架，训练时只需最少的硬件资源。FlowSeek将光流网络设计空间的最新进展与前沿的单图像深度基础模型和经典低维运动参数化相结合，实现了紧凑而准确的架构。FlowSeek在单个消费级GPU上训练，硬件预算比最新方法低约8倍，但在Sintel Final和KITTI上仍实现了比之前最先进的SEA-RAFT高10%和15%的相对改进，同时在Spring和LayeredFlow数据集上也表现优异。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决光流估计领域中的两个关键问题：一是当前最先进的光流模型需要大量高端GPU进行训练，硬件成本过高；二是现有模型在不同数据集上的泛化能力有限。这个问题很重要，因为光流估计是计算机视觉的基础任务，应用于动作识别、视频插值和4D重建等高级任务；高昂的硬件成本限制了资源有限的研究团队参与前沿研究；模型泛化能力不足限制了实际应用场景，因为真实世界场景多样性远超训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察当前光流研究过度依赖硬件资源的现象，借鉴了三个不同领域的知识：最新的光流网络设计空间(特别是SEA-RAFT架构)、先进的单图像深度基础模型(如Depth Anything v2)和经典计算机视觉中的低维运动参数化方法。作者认为可以通过重用已经训练好的基础模型来减少硬件依赖，并将深度模型的光学先验知识整合到光流估计中，同时利用运动基来约束光流估计空间。这种将30年光流研究、最近的深度基础模型和经典方法结合的创新思路，形成了FlowSeek的设计基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; FlowSeek的核心思想是通过整合深度基础模型的几何先验知识、低维运动参数化和最新光流网络架构，实现高效且准确的光流估计。整体流程包括：1)输入一对图像；2)通过特征提取器生成图像特征，同时用深度基础模型估计深度图和提取深度特征；3)融合原始特征与深度特征构建增强特征；4)基于深度图计算运动基并提取其特征；5)通过上下文网络获取上下文特征和初始隐藏状态；6)通过迭代优化过程结合相关体积、隐藏状态和上下文特征逐步 refine 光流估计；7)使用混合拉普拉斯分布的负对数似然作为损失函数，在单个GPU上完成训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合深度基础模型的光流模型；2)可在单个消费级GPU上训练，硬件需求比最新方法低约8倍；3)在多个数据集上实现了零样本泛化，性能比之前最先进的SEA-RAFT高出10-15%；4)三重知识整合(光流网络、深度基础模型、经典运动参数化)；5)有效利用运动基提供的先验知识。相比之前工作，FlowSeek不从头训练大型网络而是重用深度模型知识，将深度信息深度整合到光流流程而非仅作为辅助输入，在保持低计算成本的同时实现更好性能，特别是在细节恢复和跨域泛化方面表现更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FlowSeek通过整合深度基础模型的几何先验知识和经典运动参数化方法，在显著降低硬件资源需求的同时，实现了更准确的光流估计和更强的跨域泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present FlowSeek, a novel framework for optical flow requiring minimalhardware resources for training. FlowSeek marries the latest advances on thedesign space of optical flow networks with cutting-edge single-image depthfoundation models and classical low-dimensional motion parametrization,implementing a compact, yet accurate architecture. FlowSeek is trained on asingle consumer-grade GPU, a hardware budget about 8x lower compared to mostrecent methods, and still achieves superior cross-dataset generalization onSintel Final and KITTI, with a relative improvement of 10 and 15% over theprevious state-of-the-art SEA-RAFT, as well as on Spring and LayeredFlowdatasets.</description>
      <author>example@mail.com (Matteo Poggi, Fabio Tosi)</author>
      <guid isPermaLink="false">2509.05297v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>BEDTime: A Unified Benchmark for Automatically Describing Time Series</title>
      <link>http://arxiv.org/abs/2509.05215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个标准化的评估框架，用于测试模型使用自然语言描述时间序列的能力，通过统一多个数据集和明确定义三个核心任务，实现了对不同模型的直接比较，并发现了当前模型在特定架构和鲁棒性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;最近许多研究提出了用于多种时间序列分析任务的通用基础模型。虽然已有一些评估这些模型的数据集，但先前的研究经常在介绍模型的同时引入新数据集，这限制了直接、独立的比较机会，模糊了不同方法相对优势的见解。此外，先前的评估通常同时涵盖众多任务，评估广泛的模型能力，而没有明确指出哪些能力对整体性能有贡献。&lt;h4&gt;目的&lt;/h4&gt;解决上述评估差距，正式评估3个测试模型使用通用自然语言描述时间系列能力的任务，并统一4个最近的数据集，使模型能够在每个任务上进行直接比较。&lt;h4&gt;方法&lt;/h4&gt;定义并评估3个任务：(1)识别（是/否问题回答），(2)区分（多项选择题回答），(3)生成（开放式自然语言描述）。统一4个最近的数据集，并评估13种最先进的语言、视觉-语言和时间序列-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;(1)流行的纯语言方法表现不佳，表明需要时间序列特定的架构；(2)视觉-语言模型非常成功，突显了视觉模型对这些任务的价值；(3)预训练的多模态时间序列-语言模型成功优于大型语言模型，但仍有显著改进空间；(4)所有方法在一系列鲁棒性测试中都表现出明显的脆弱性。&lt;h4&gt;结论&lt;/h4&gt;该基准为时间序列推理系统提供了标准化评估，有助于更清晰地了解不同模型的相对优势和不足。&lt;h4&gt;翻译&lt;/h4&gt;最近许多研究提出了用于各种时间序列分析任务的通用基础模型。虽然已经有一些既定的数据集用于评估这些模型，但先前的工作经常在介绍其模型的同时引入新数据集，限制了直接、独立比较的机会，并模糊了对不同方法相对优势的见解。此外，先前的评估通常同时涵盖众多任务，评估广泛的模型能力，而没有明确指出哪些能力对整体性能有贡献。为了解决这些差距，我们正式评估了3个任务，测试模型使用通用自然语言描述时间序列的能力：(1)识别（是/否问题回答），(2)区分（多项选择题回答），(3)生成（开放式自然语言描述）。然后我们统一了4个最近的数据集，使模型能够在每个任务上进行直接比较。实验中，在评估13种最先进的语言、视觉-语言和时间序列-语言模型时，我们发现(1)流行的纯语言方法表现不佳，表明需要时间序列特定的架构，(2)视觉-语言模型非常成功，正如预期的那样，突显了视觉模型对这些任务的价值，(3)预训练的多模态时间序列-语言模型成功优于大型语言模型，但仍有显著的改进空间。我们还发现所有方法在一系列鲁棒性测试中都表现出明显的脆弱性。总的来说，我们的基准为时间序列推理系统提供了必要的标准化评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many recent studies have proposed general-purpose foundation models designedfor a variety of time series analysis tasks. While several established datasetsalready exist for evaluating these models, previous works frequently introducetheir models in conjunction with new datasets, limiting opportunities fordirect, independent comparisons and obscuring insights into the relativestrengths of different methods. Additionally, prior evaluations often covernumerous tasks simultaneously, assessing a broad range of model abilitieswithout clearly pinpointing which capabilities contribute to overallperformance. To address these gaps, we formalize and evaluate 3 tasks that testa model's ability to describe time series using generic natural language: (1)recognition (True/False question-answering), (2) differentiation (multiplechoice question-answering), and (3) generation (open-ended natural languagedescription). We then unify 4 recent datasets to enable head-to-head modelcomparisons on each task. Experimentally, in evaluating 13 state-of-the-artlanguage, vision--language, and time series--language models, we find that (1)popular language-only methods largely underperform, indicating a need for timeseries-specific architectures, (2) VLMs are quite successful, as expected,identifying the value of vision models for these tasks and (3) pretrainedmultimodal time series--language models successfully outperform LLMs, but stillhave significant room for improvement. We also find that all approaches exhibitclear fragility in a range of robustness tests. Overall, our benchmark providesa standardized evaluation on a task necessary for time series reasoningsystems.</description>
      <author>example@mail.com (Medhasweta Sen, Zachary Gottesman, Jiaxing Qiu, C. Bayan Bruss, Nam Nguyen, Tom Hartvigsen)</author>
      <guid isPermaLink="false">2509.05215v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic operator learning: generative modeling and uncertainty quantification for foundation models of differential equations</title>
      <link>http://arxiv.org/abs/2509.05186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种概率框架，揭示上下文算子网络(ICON)实际上是在执行贝叶斯推断，并扩展ICON到生成式设置，实现算子学习中的不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;ICON是一类基于基础模型新颖架构的算子学习方法，在多样化的初始和边界条件数据集上进行训练，这些数据集与常微分方程和偏微分方程的相应解配对。&lt;h4&gt;目的&lt;/h4&gt;揭示ICON作为隐式执行贝叶斯推断的本质，并扩展ICON到生成式设置，使其能够从解算子的后验预测分布中进行采样。&lt;h4&gt;方法&lt;/h4&gt;提出概率框架将ICON描述为计算给定上下文条件下解算子的后验预测分布均值的方法，利用随机微分方程的形式论为理解ICON和其他多算子学习方法提供基础。&lt;h4&gt;主要发现&lt;/h4&gt;ICON实际上执行贝叶斯推断，计算条件后验预测分布的均值；生成式ICON(GenICON)能够捕捉解算子的潜在不确定性，实现算子学习中的原则性不确定性量化。&lt;h4&gt;结论&lt;/h4&gt;通过概率视角，ICON可扩展到生成式设置，从解算子的后验预测分布中进行采样，GenICON捕捉了解算子的潜在不确定性，使算子学习中的解决方案预测能够进行原则性不确定性量化。&lt;h4&gt;翻译&lt;/h4&gt;上下文算子网络(ICON)是一类基于基础模型新颖架构的算子学习方法。在多样化的初始和边界条件数据集上进行训练，这些数据集与常微分方程和偏微分方程的相应解配对，ICON学习将给定微分方程的示例条件-解对映射到其解算子的近似值。在这里，我们提出了一个概率框架，揭示ICON作为隐式执行贝叶斯推断，其中它计算给定上下文(即示例条件-解对)条件下解算子的后验预测分布的均值。随机微分方程的形式论为描述ICON完成的任务提供了概率框架，同时也为理解其他多算子学习方法提供了基础。这种概率视角为将ICON扩展到生成式设置提供了基础，在这种设置中，可以从解算子的后验预测分布中进行采样。ICON的生成式公式(GenICON)捕捉了解算子的潜在不确定性，这使算子学习中的解决方案预测能够进行原则性不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In-context operator networks (ICON) are a class of operator learning methodsbased on the novel architectures of foundation models. Trained on a diverse setof datasets of initial and boundary conditions paired with correspondingsolutions to ordinary and partial differential equations (ODEs and PDEs), ICONlearns to map example condition-solution pairs of a given differential equationto an approximation of its solution operator. Here, we present a probabilisticframework that reveals ICON as implicitly performing Bayesian inference, whereit computes the mean of the posterior predictive distribution over solutionoperators conditioned on the provided context, i.e., example condition-solutionpairs. The formalism of random differential equations provides theprobabilistic framework for describing the tasks ICON accomplishes while alsoproviding a basis for understanding other multi-operator learning methods. Thisprobabilistic perspective provides a basis for extending ICON to\emph{generative} settings, where one can sample from the posterior predictivedistribution of solution operators. The generative formulation of ICON(GenICON) captures the underlying uncertainty in the solution operator, whichenables principled uncertainty quantification in the solution predictions inoperator learning.</description>
      <author>example@mail.com (Benjamin J. Zhang, Siting Liu, Stanley J. Osher, Markos A. Katsoulakis)</author>
      <guid isPermaLink="false">2509.05186v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Autoregressive Vision Foundation Models for Image Compression</title>
      <link>http://arxiv.org/abs/2509.05169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究首次尝试将视觉基础模型(VFMs)重新用作图像编解码器，探索其在低速率图像压缩中的生成能力。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(VFMs)在各种下游任务中广泛应用，包括条件生成和非条件生成场景，例如物理AI应用。许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归(AR)模型进行下一个令牌预测。&lt;h4&gt;目的&lt;/h4&gt;探索视觉基础模型(VFMs)在低速率图像压缩中的生成能力，并评估其与传统编解码器的性能比较。&lt;h4&gt;方法&lt;/h4&gt;重新利用VFM中的AR模型，基于已编码的令牌对下一个令牌进行熵编码，从而实现压缩。这种方法不同于早期仅依赖条件生成来重建输入图像的语义压缩工作。&lt;h4&gt;主要发现&lt;/h4&gt;某些预训练的通用VFMs在极低比特率下表现出比专业学习图像编解码器更好的感知质量。&lt;h4&gt;结论&lt;/h4&gt;这项发现为利用VFMs进行低速率、语义丰富的图像压缩这一有前景的研究方向铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;这项工作首次尝试将视觉基础模型(VFMs)重新用作图像编解码器，旨在探索它们在低速率图像压缩中的生成能力。VFMs在各种下游任务中被广泛用于条件生成和非条件生成场景，例如物理AI应用。许多VFMs采用类似于端到端学习图像编解码器的编码器-解码器架构，并学习自回归(AR)模型来执行下一个令牌预测。为了实现压缩，我们重新利用VFM中的AR模型，基于先前编码的令牌对下一个令牌进行熵编码。这种方法不同于早期依赖条件生成重建输入图像的语义压缩工作。进行了广泛的实验和分析，将基于VFM的编解码器与当前针对失真或感知质量优化的SOTA编解码器进行比较。值得注意的是，某些预训练的通用VFMs在极低比特率下表现出比专业学习图像编解码器更好的感知质量。这一发现为利用VFMs进行低速率、语义丰富的图像压缩这一有前景的研究方向铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work presents the first attempt to repurpose vision foundation models(VFMs) as image codecs, aiming to explore their generation capability forlow-rate image compression. VFMs are widely employed in both conditional andunconditional generation scenarios across diverse downstream tasks, e.g.,physical AI applications. Many VFMs employ an encoder-decoder architecturesimilar to that of end-to-end learned image codecs and learn an autoregressive(AR) model to perform next-token prediction. To enable compression, werepurpose the AR model in VFM for entropy coding the next token based onpreviously coded tokens. This approach deviates from early semantic compressionefforts that rely solely on conditional generation for reconstructing inputimages. Extensive experiments and analysis are conducted to compare VFM-basedcodec to current SOTA codecs optimized for distortion or perceptual quality.Notably, certain pre-trained, general-purpose VFMs demonstrate superiorperceptual quality at extremely low bitrates compared to specialized learnedimage codecs. This finding paves the way for a promising research directionthat leverages VFMs for low-rate, semantically rich image compression.</description>
      <author>example@mail.com (Huu-Tai Phung, Yu-Hsiang Lin, Yen-Kuan Ho, Wen-Hsiao Peng)</author>
      <guid isPermaLink="false">2509.05169v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Foundational Models and Federated Learning: Survey, Taxonomy, Challenges and Practical Insights</title>
      <link>http://arxiv.org/abs/2509.05142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文探讨了联邦学习与基础模型的交叉领域，提供了一个结构化的文献综述和分类法，涵盖了42种独特方法，特别关注医疗保健领域。&lt;h4&gt;背景&lt;/h4&gt;联邦学习能够通过不共享私有数据的方式进行协作模型训练，从而解锁孤立的数据和分布式资源。随着更复杂的基础模型得到广泛应用，扩展训练资源和整合私有数据的需求也随之增长。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索联邦学习与基础模型的交叉领域，识别、分类和描述整合这两种范式的技术方法。由于目前缺乏统一的综述，作者提出了一个遵循开发生命周期阶段的新型分类法。&lt;h4&gt;方法&lt;/h4&gt;作者检索并审查了4,200多篇文章，通过纳入标准筛选出250多篇经过彻底审查的文章，其中包含42种独特方法。这些方法被用于构建分类法，并基于复杂性、效率和可扩展性进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;论文提供了实施和演变这些方法的实践见解和指导，特别关注医疗保健领域作为案例研究，并展示了联邦学习与基础模型结合的潜在影响。研究涵盖了多个交叉主题，包括联邦学习、自监督学习、微调、蒸馏和迁移学习等。&lt;h4&gt;结论&lt;/h4&gt;作者提供了一个自包含的概述，不仅总结了该领域的现状，还提供了关于采用、演变和整合基础模型与联邦学习的实践见解。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习有潜力在不共享私有数据的情况下，通过协作模型训练来解锁孤立的数据和分布式资源。随着更复杂的基础模型得到广泛使用，扩展训练资源和整合私有数据的需求也随之增长。在本文中，我们探讨了联邦学习与基础模型的交叉领域，旨在识别、分类和描述整合这两种范式的技术方法。由于目前缺乏统一的综述，我们提出了一个遵循开发生命周期阶段的文献综述和新型分类法，并对现有方法进行了技术比较。此外，我们提供了实施和演变这些方法的实践见解和指导，特别关注医疗保健领域作为案例研究，其中联邦学习与基础模型的潜在影响被认为非常重要。我们的综述涵盖了多个交叉主题，包括但不限于联邦学习、自监督学习、微调、蒸馏和迁移学习。最初，我们检索并审查了4,200多篇文章集合。通过纳入标准，这一集合被缩减到250多篇经过彻底审查的文章，其中包含42种独特方法。这些方法被用于构建分类法，并基于复杂性、效率和可扩展性进行了比较。我们呈现这些结果作为一个自包含的概述，不仅总结了该领域的现状，还提供了关于采用、演变和整合基础模型与联邦学习的实践见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7717/peerj-cs.2993&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning has the potential to unlock siloed data and distributedresources by enabling collaborative model training without sharing privatedata. As more complex foundational models gain widespread use, the need toexpand training resources and integrate privately owned data grows as well. Inthis article, we explore the intersection of federated learning andfoundational models, aiming to identify, categorize, and characterize technicalmethods that integrate the two paradigms. As a unified survey is currentlyunavailable, we present a literature survey structured around a novel taxonomythat follows the development life-cycle stages, along with a technicalcomparison of available methods. Additionally, we provide practical insightsand guidelines for implementing and evolving these methods, with a specificfocus on the healthcare domain as a case study, where the potential impact offederated learning and foundational models is considered significant. Oursurvey covers multiple intersecting topics, including but not limited tofederated learning, self-supervised learning, fine-tuning, distillation, andtransfer learning. Initially, we retrieved and reviewed a set of over 4,200articles. This collection was narrowed to more than 250 thoroughly reviewedarticles through inclusion criteria, featuring 42 unique methods. The methodswere used to construct the taxonomy and enabled their comparison based oncomplexity, efficiency, and scalability. We present these results as aself-contained overview that not only summarizes the state of the field butalso provides insights into the practical aspects of adopting, evolving, andintegrating foundational models with federated learning.</description>
      <author>example@mail.com (Cosmin-Andrei Hatfaludi, Alex Serban)</author>
      <guid isPermaLink="false">2509.05142v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>LEMURS dataset: Large-scale multi-detector ElectroMagnetic Universal Representation of Showers</title>
      <link>http://arxiv.org/abs/2509.05108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 24 figures + appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍LEMURS数据集，这是一个模拟量热器簇射的广泛数据集，用于支持高能物理中快速模拟方法的发展和基准测试，为基础模型的发展提供支持。&lt;h4&gt;背景&lt;/h4&gt;高能物理领域需要快速模拟方法，而基础模型的发展需要大规模、多样化的训练数据。现有的CaloChallenge数据集2存在局限性。&lt;h4&gt;目的&lt;/h4&gt;创建一个比现有数据集更稳健、规模更大、多样性更高的数据集，以支持快速模拟方法的开发和基准测试，促进基础模型在高能物理中的应用。&lt;h4&gt;方法&lt;/h4&gt;构建模拟量热器簇射数据集，包含多种探测器几何结构，以HDF5格式提供，文件结构受CaloChallenge启发但包含更多变量。&lt;h4&gt;主要发现&lt;/h4&gt;LEMURS数据集比CaloChallenge数据集2更稳健，具有更大的统计量、更广的入射角度范围，以及多种探测器几何结构（包括更现实的量热器）。&lt;h4&gt;结论&lt;/h4&gt;LEMURS数据集的规模和多样性使其特别适合基础模型的发展，已在CaloDiT-2模型中得到应用，该模型已集成到Geant4模拟工具包中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LEMURS：一个广泛的模拟量热器簇射数据集，旨在支持高能物理中快速模拟方法的发展和基准测试，主要为基础模型的发展提供一步。这个新数据集比已建立的CaloChallenge数据集2更稳健，具有更大的统计量、更广的探测器入射角度范围，并且最关键的是包含多种探测器几何结构（包括更现实的量热器）。数据集以HDF5格式提供，文件结构受CaloChallenge簇射表示的启发，同时包含更多变量。LEMURS的规模和多样性使其特别适合基础模型的发展，并已在CaloDiT-2模型中使用，这是一个在社区标准模拟工具包Geant4（版本11.4.beta）中发布的预训练模型。所有数据和生成及分析的代码都是公开可访问的，便于社区的可重复使用和重用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LEMURS: an extensive dataset of simulated calorimeter showersdesigned to support the development and benchmarking of fast simulation methodsin high-energy physics, most notably providing a step towards the developmentof foundation models. This new dataset is more robust than the well-establishedCaloChallenge dataset 2, featuring substantially greater statistics, a widerrange of incident angles in the detector, and most crucially multiple detectorgeometries (including more realistic calorimeters). The dataset is provided inHDF5 format, with a file structure inspired by the CaloChallenge showerrepresentation while also including more variables. LEMURS scale and diversitymake it particularly suitable for development of foundation models and has beenused in the CaloDiT-2 model, a pre-trained model released in the communitystandard simulation toolkit Geant4 (version 11.4.beta). All data and code forgeneration and analysis are openly accessible, facilitating reproducibility andreuse across the community.</description>
      <author>example@mail.com (Peter McKeown, Piyush Raikwar, Anna Zaborowska)</author>
      <guid isPermaLink="false">2509.05108v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>ToM-SSI: Evaluating Theory of Mind in Situated Social Interactions</title>
      <link>http://arxiv.org/abs/2509.05066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (Main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ToM-SSI，一个新的多模态心智理论基准测试，用于测试在丰富社交互动和空间动态环境中的心智理论能力，突破了现有文本限制或二元互动的局限。&lt;h4&gt;背景&lt;/h4&gt;现有的大模型心智理论基准测试主要基于Sally-Anne测试的变体，视角非常有限，忽略了人类社交互动的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出新的基准测试ToM-SSI，专门设计用于测试在丰富社交互动和空间动态环境中的心智理论能力。&lt;h4&gt;方法&lt;/h4&gt;ToM-SSI是多模态的，包括多达四个代理的群体互动，这些代理在情境环境中进行交流和移动，允许研究混合合作-阻碍设置和并行推理多个代理的心理状态。&lt;h4&gt;主要发现&lt;/h4&gt;当前模型的表现仍然严重受限，特别是在这些新任务中表现不佳，突显了未来研究的关键差距。&lt;h4&gt;结论&lt;/h4&gt;新的基准测试能够捕捉比现有基准更广泛的社会认知范围，为未来研究提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;大多数现有针对基础模型的心智理论(ToM)基准测试依赖于Sally-Anne测试的变体，仅提供了非常有限的心智理论视角，忽略了人类社交互动的复杂性。为解决这一差距，我们提出了ToM-SSI：一个新的基准测试，专门设计用于测试在丰富社交互动和空间动态环境中的心智理论能力。虽然当前的ToM基准测试仅限于文本或二元互动，但ToM-SSI是多模态的，包括多达四个代理的群体互动，这些代理在情境环境中进行交流和移动。这种独特的设计使我们首次能够研究混合合作-阻碍设置和并行推理多个代理的心理状态，从而捕捉比现有基准更广泛的社会认知范围。我们的评估显示，当前模型的表现仍然严重受限，特别是在这些新任务中，突显了未来研究的关键差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing Theory of Mind (ToM) benchmarks for foundation models rely onvariations of the Sally-Anne test, offering only a very limited perspective onToM and neglecting the complexity of human social interactions. To address thisgap, we propose ToM-SSI: a new benchmark specifically designed to test ToMcapabilities in environments rich with social interactions and spatialdynamics. While current ToM benchmarks are limited to text-only or dyadicinteractions, ToM-SSI is multimodal and includes group interactions of up tofour agents that communicate and move in situated environments. This uniquedesign allows us to study, for the first time, mixed cooperative-obstructivesettings and reasoning about multiple agents' mental state in parallel, thuscapturing a wider range of social cognition than existing benchmarks. Ourevaluations reveal that the current models' performance is still severelylimited, especially in these new tasks, highlighting critical gaps for futureresearch.</description>
      <author>example@mail.com (Matteo Bortoletto, Constantin Ruhdorfer, Andreas Bulling)</author>
      <guid isPermaLink="false">2509.05066v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>LUIVITON: Learned Universal Interoperable VIrtual Try-ON</title>
      <link>http://arxiv.org/abs/2509.05030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LUIVITON系统，一个端到端的完全自动化虚拟试衣系统，能够将复杂的多层服装覆盖在不同姿态的人形角色上，并支持服装尺寸的快速定制。&lt;h4&gt;背景&lt;/h4&gt;虚拟试衣面临的主要挑战是将复杂服装与各种不同姿态和形状的人体正确对齐，传统方法难以处理复杂的几何形状、非流形网格以及多样化的人形角色。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够完全自动化的虚拟试衣系统，能够处理复杂的多层服装，适应各种人形角色（包括人类、机器人、卡通角色、生物和外星人），并支持服装尺寸的快速定制，同时保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;使用SMPL作为代理表示，将服装到人体的覆盖问题分解为服装到SMPL对应和身体到SMPL对应两个任务；对于服装到SMPL对应，使用基于几何学习的方法进行部分到完整形状对应预测；对于身体到SMPL对应，引入基于扩散模型的方法，使用多视角一致的外观特征和预训练的2D基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;系统能够产生高质量的3D服装搭配，无需任何人工劳动；即使在没有2D服装缝制模式的情况下也能工作；系统计算效率高，适合实际应用；支持服装尺寸和材质属性的快速定制。&lt;h4&gt;结论&lt;/h4&gt;LUIVITON系统提供了一个完全自动化的虚拟试衣解决方案，能够处理复杂服装和各种人形角色，同时保持计算效率，并支持服装尺寸的快速定制，为虚拟试衣领域提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了LUIVITON，一个端到端的系统，用于完全自动化的虚拟试衣，能够将复杂的多层服装覆盖在不同姿态和多样化的人形角色上。为了解决将复杂服装与任意且高度多样化的人体形状对齐的挑战，我们使用SMPL作为代理表示，并将服装到人体的覆盖问题分解为两个对应任务：1)服装到SMPL和2)身体到SMPL对应，每个任务都有其独特的挑战。虽然我们使用基于几何学习的方法来解决服装到SMPL的贴合问题，以进行部分到完整形状对应的预测，但我们引入了基于扩散模型的方法来处理身体到SMPL的对应，使用多视角一致的外观特征和预训练的2D基础模型。我们的方法能够处理复杂的几何形状、非流形网格，并有效推广到广泛的人形角色--包括人类、机器人、卡通主题、生物和外星人，同时保持计算效率以便实际应用。除了提供完全自动化的贴合解决方案外，LUIVITON还支持服装尺寸的快速定制，允许用户在服装被覆盖后调整服装尺寸和材质属性。我们证明，我们的系统可以在没有任何人工劳动的情况下产生高质量的3D服装搭配，即使2D服装缝制模式不可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决虚拟试衣（virtual try-on）的自动化问题，即如何将任意3D服装模型自动、准确地穿在各种姿势和形状的人形3D角色身上。这个问题在现实和研究中的重要性在于：游戏、电影、社交媒体和AR/VR应用中，角色服装对身份认同和视觉连贯性至关重要；而现有的服装建模和模拟方法劳动密集型，难以自动化适配到不同形状、姿势或风格的角色；当前方法在处理复杂服装、多层结构或风格化角色时存在局限，且缺乏对服装尺寸的灵活控制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将复杂的服装到身体适配问题分解为两个更易管理的对应任务：服装到SMPL的对应和身体到SMPL的对应。针对服装到SMPL对应，作者使用几何学习方法处理部分到完整形状的对应；针对身体到SMPL对应，采用基于扩散模型的方法处理大形状和姿势变化。作者借鉴了多个现有技术：使用SMPL作为人体表示代理；采用DiffusionNet进行服装对应；利用SyncMVD和DINOv2进行特征提取；使用ContourCraft作为神经布料模拟器。整个系统设计为端到端的流程，分为对应预测、注册和服装适配三个主要阶段。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用SMPL作为中间表示，将复杂的服装到身体适配问题分解为两个对应问题，并针对不同问题采用不同方法：几何学习处理服装对应，扩散模型处理身体对应。整体流程分为三阶段：1）对应预测阶段，分别计算服装到SMPL和身体到SMPL的对应关系；2）注册阶段，优化SMPL和SMPL+D参数以对齐服装和身体；3）服装适配阶段，生成平滑过渡序列并使用神经布料模拟器将服装适配到目标身体，支持默认、自动调整和定制三种尺寸调整模式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）全自动通用虚拟试-on框架，能处理复杂几何和多层服装；2）基于DiffusionNet的服装到SMPL对应预测模型，能处理非流形网格；3）结合多视角扩散特征和DINOv2先验的身体注册技术，能泛化到各种身体形状；4）快速定制功能，每次调整仅需15秒。相比之前工作，不同之处在于：不再局限于参数化身体和流形服装；使用专门设计的对应预测方法提高精度；能处理人类、机器人、卡通等多种角色；支持快速尺寸调整且无需2D缝纫模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LUIVITON是一个全自动的通用虚拟试-on系统，能够将任意3D服装精确地适配到各种姿势和形状的人形角色上，支持快速定制且无需人工干预。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present LUIVITON, an end-to-end system for fully automated virtual try-on,capable of draping complex, multi-layer clothing onto diverse and arbitrarilyposed humanoid characters. To address the challenge of aligning complexgarments with arbitrary and highly diverse body shapes, we use SMPL as a proxyrepresentation and separate the clothing-to-body draping problem into twocorrespondence tasks: 1) clothing-to-SMPL and 2) body-to-SMPL correspondence,where each has its unique challenges. While we address the clothing-to-SMPLfitting problem using a geometric learning-based approach forpartial-to-complete shape correspondence prediction, we introduce a diffusionmodel-based approach for body-to-SMPL correspondence using multi-viewconsistent appearance features and a pre-trained 2D foundation model. Ourmethod can handle complex geometries, non-manifold meshes, and generalizeseffectively to a wide range of humanoid characters -- including humans, robots,cartoon subjects, creatures, and aliens, while maintaining computationalefficiency for practical adoption. In addition to offering a fully automaticfitting solution, LUIVITON supports fast customization of clothing size,allowing users to adjust clothing sizes and material properties after they havebeen draped. We show that our system can produce high-quality 3D clothingfittings without any human labor, even when 2D clothing sewing patterns are notavailable.</description>
      <author>example@mail.com (Cong Cao, Xianhang Cheng, Jingyuan Liu, Yujian Zheng, Zhenhui Lin, Meriem Chkir, Hao Li)</author>
      <guid isPermaLink="false">2509.05030v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Video-to-Audio Generation via Multiple Foundation Models Mapper</title>
      <link>http://arxiv.org/abs/2509.04957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了多基础模型映射器(MFM-Mapper)，用于高效的视频到音频(V2A)生成，通过融合双视觉编码器特征和使用GPT-2改进特征对齐，仅需16%的训练量即可达到与大规模模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;近期的视频到音频生成依赖于从视频中提取语义和时间特征来调节生成模型，但从头训练这些模型资源消耗大。基础模型因其跨模态知识迁移和泛化能力而受到关注。&lt;h4&gt;目的&lt;/h4&gt;改进现有的视频到音频生成方法，提出一种更高效的映射器架构，能够在降低训练成本的同时保持或提高生成质量。&lt;h4&gt;方法&lt;/h4&gt;MFM-Mapper通过融合双视觉编码器的特征来获取更丰富的语义和时间信息，并用GPT-2替换线性映射器以改进特征对齐，将跨模态特征映射与自回归翻译任务联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;MFM-Mapper在语义和时间一致性方面表现更好，仅需先前映射器方法16%的训练量，就能与在更大规模上训练的模型实现相当的性能。&lt;h4&gt;结论&lt;/h4&gt;MFM-Mapper是一种高效的视频到音频生成方法，显著降低了训练成本，同时保持了高质量的音频生成能力。&lt;h4&gt;翻译&lt;/h4&gt;近期的视频到音频(V2A)生成依赖于从视频中提取语义和时间特征来调节生成模型。从头训练这些模型资源消耗大。因此，由于基础模型(FMs)具有跨模态知识迁移和泛化能力，利用它们已成为趋势。先前的一项工作探索了微调一个轻量级映射器网络，将预训练的视觉编码器与文本到音频生成模型连接起来用于V2A。受此启发，我们引入了多基础模型映射器(MFM-Mapper)。与之前的映射器方法相比，MFM-Mapper通过融合双视觉编码器的特征，受益于更丰富的语义和时间信息。此外，通过用GPT-2替换线性映射器，MFM-Mapper改进了特征对齐，将跨模态特征映射与自回归翻译任务相提并论。我们的MFM-Mapper表现出显著的训练效率。它在语义和时间一致性方面实现了更好的性能，训练消耗更少，仅需先前基于映射器工作的16%的训练规模，却实现了与在更大规模上训练的模型相竞争的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Video-to-Audio (V2A) generation relies on extracting semantic andtemporal features from video to condition generative models. Training thesemodels from scratch is resource intensive. Consequently, leveraging foundationmodels (FMs) has gained traction due to their cross-modal knowledge transferand generalization capabilities. One prior work has explored fine-tuning alightweight mapper network to connect a pre-trained visual encoder with atext-to-audio generation model for V2A. Inspired by this, we introduce theMultiple Foundation Model Mapper (MFM-Mapper). Compared to the previous mapperapproach, MFM-Mapper benefits from richer semantic and temporal information byfusing features from dual visual encoders. Furthermore, by replacing a linearmapper with GPT-2, MFM-Mapper improves feature alignment, drawing parallelsbetween cross-modal features mapping and autoregressive translation tasks. OurMFM-Mapper exhibits remarkable training efficiency. It achieves betterperformance in semantic and temporal consistency with fewer training consuming,requiring only 16\% of the training scale compared to previous mapper-basedwork, yet achieves competitive performance with models trained on a much largerscale.</description>
      <author>example@mail.com (Gehui Chen, Guan'an Wang, Xiaowen Huang, Jitao Sang)</author>
      <guid isPermaLink="false">2509.04957v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Law for Large-Scale Pre-Training Using Chaotic Time Series and Predictability in Financial Time Series</title>
      <link>http://arxiv.org/abs/2509.04921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Patent pending&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过生成人工混沌时间序列和应用重采样技术来模拟金融时间序列数据的方法，并进行大规模预训练。使用比特币交易数据进行零样本预测，结果表明该方法在交易策略盈利能力上显著优于自相关模型。研究还发现了类似缩放定律的现象，表明通过增加训练样本数量可以扩展混沌时间序列的预测范围。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在气象学、交通、电力、经济、金融等多个领域的决策过程中起着关键作用。预测金融工具回报是一个具有挑战性的问题。研究人员已提出适用于各种预测任务的时间序列基础模型，并基于现实世界时间序列的混沌特性开发了人工生成合成混沌时间序列的方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种通过生成人工混沌时间序列并对金融时间序列数据进行建模的方法论。&lt;h4&gt;方法&lt;/h4&gt;应用重采样技术模拟金融时间序列数据作为训练样本；增加重采样间隔以扩展预测范围；使用100亿个训练样本进行大规模预训练；基于实际比特币交易数据创建多时间段测试数据集；进行零样本预测；评估基于预测的简单交易策略的盈利能力。&lt;h4&gt;主要发现&lt;/h4&gt;与自相关模型相比，预测结果表现出显著的性能改进；在预训练过程中观察到类似缩放定律的现象；通过指数增加训练样本数量，可以在混沌时间序列中扩展预测范围的同时达到一定水平的预测性能。&lt;h4&gt;结论&lt;/h4&gt;如果这种缩放定律在各种混沌模型中都成立，意味着通过投入大量计算资源有可能预测近期事件。未来研究应侧重于进一步的大规模训练，并验证此缩放定律对不同混沌模型的适用性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在气象学、交通、电力、经济、金融等多个领域的决策过程中起着关键作用。特别是，预测金融工具回报是一个具有挑战性的问题。一些研究人员提出了适用于各种预测任务的时间序列基础模型。同时，基于现实世界时间序列表现出混沌特性的认识，已经开发了人工生成合成混沌时间序列、构建多样化数据集和训练模型的方法。在本研究中，我们提出了一种通过生成人工混沌时间序列并应用重采样技术来模拟金融时间序列数据的方法，然后将其用作训练样本。通过增加重采样间隔来扩展预测范围，我们对每个案例使用了100亿个训练样本进行大规模预训练。随后，我们使用实际的比特币交易数据为多个时间段创建了测试数据集，并在不重新训练预训练模型的情况下进行了零样本预测。基于这些预测评估的简单交易策略的盈利能力结果表明，与自相关模型相比表现出显著的性能改进。在大规模预训练过程中，我们观察到了类似缩放定律的现象，即通过指数增加训练样本数量，我们可以在混沌时间序列中扩展预测范围的同时达到一定水平的预测性能。如果这种缩放定律被证明是稳健的，并且在各种混沌模型中都成立，它表明通过投入大量计算资源有可能预测近期事件。未来研究应侧重于进一步的大规模训练，并验证此缩放定律对不同混沌模型的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting plays a critical role in decision-making processesacross diverse fields including meteorology, traffic, electricity, economics,finance, and so on. Especially, predicting returns on financial instruments isa challenging problem. Some researchers have proposed time series foundationmodels applicable to various forecasting tasks. Simultaneously, based on therecognition that real-world time series exhibit chaotic properties, methodshave been developed to artificially generate synthetic chaotic time series,construct diverse datasets and train models. In this study, we propose amethodology for modeling financial time series by generating artificial chaotictime series and applying resampling techniques to simulate financial timeseries data, which we then use as training samples. Increasing the resamplinginterval to extend predictive horizons, we conducted large-scale pre-trainingusing 10 billion training samples for each case. We subsequently created testdatasets for multiple timeframes using actual Bitcoin trade data and performedzero-shot prediction without re-training the pre-trained model. The results ofevaluating the profitability of a simple trading strategy based on thesepredictions demonstrated significant performance improvements overautocorrelation models. During the large-scale pre-training process, weobserved a scaling law-like phenomenon that we can achieve predictiveperformance at a certain level with extended predictive horizons for chaotictime series by increasing the number of training samples exponentially. If thisscaling law proves robust and holds true across various chaotic models, itsuggests the potential to predict near-future events by investing substantialcomputational resources. Future research should focus on further large-scaletraining and verifying the applicability of this scaling law to diverse chaoticmodels.</description>
      <author>example@mail.com (Yuki Takemoto)</author>
      <guid isPermaLink="false">2509.04921v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Foundation Model-Driven User Interest Modeling and Behavior Analysis on Short Video Platforms</title>
      <link>http://arxiv.org/abs/2509.04751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于多模态基础模型的用户兴趣建模和行为分析框架，通过整合视频、文本和音频数据，结合行为序列分析，显著提高了推荐系统的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着短视频平台用户基数的快速增长，个性化推荐系统对提升用户体验和优化内容分发至关重要。然而，传统兴趣建模方法通常依赖单模态数据（如点击日志或文本标签），无法完全捕捉复杂多模态内容环境中的用户偏好。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法在复杂多模态内容环境中无法完全捕捉用户偏好的问题，提出基于多模态基础模型框架进行用户兴趣建模和行为分析，以提高推荐系统的及时性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出多模态基础模型框架，通过跨模态对齐策略将视频帧、文本描述和背景音乐集成到统一语义空间构建细粒度用户兴趣向量；引入行为驱动的特征嵌入机制，整合观看、点赞和评论序列来建模动态兴趣演化；在实验阶段使用公共和专有短视频数据集进行广泛评估；并使用注意力权重和特征可视化纳入可解释性机制。&lt;h4&gt;主要发现&lt;/h4&gt;行为预测准确性显著提高；对冷启动用户的兴趣建模效果改善；推荐点击率提高；模型决策基础可追溯，兴趣变化可追踪。&lt;h4&gt;结论&lt;/h4&gt;多模态基础模型框架能有效提升推荐系统的及时性和准确性，增强了推荐系统的透明度和可控性。&lt;h4&gt;翻译&lt;/h4&gt;随着短视频平台用户基数的快速增长，个性化推荐系统在提升用户体验和优化内容分发方面发挥着越来越关键的作用。传统的兴趣建模方法通常依赖单模态数据，如点击日志或文本标签，这限制了它们在复杂多模态内容环境中完全捕捉用户偏好的能力。为应对这一挑战，本文提出了一种基于多模态基础模型的用户兴趣建模和行为分析框架。通过跨模态对齐策略将视频帧、文本描述和背景音乐集成到统一语义空间，该框架构建了细粒度的用户兴趣向量。此外，我们引入了一种行为驱动的特征嵌入机制，结合观看、点赞和评论序列来建模动态兴趣演化，从而提高推荐的及时性和准确性。在实验阶段，我们使用公共和专有短视频数据集进行了广泛评估，将我们的方法与多种主流推荐算法和建模技术进行了比较。结果表明，在行为预测准确性、冷启动用户的兴趣建模和推荐点击率方面均有显著改善。此外，我们使用注意力权重和特征可视化纳入了可解释性机制，以揭示模型在多模态输入下的决策基础，并追踪兴趣变化，从而增强推荐系统的透明度和可控性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid expansion of user bases on short video platforms, personalizedrecommendation systems are playing an increasingly critical role in enhancinguser experience and optimizing content distribution. Traditional interestmodeling methods often rely on unimodal data, such as click logs or textlabels, which limits their ability to fully capture user preferences in acomplex multimodal content environment. To address this challenge, this paperproposes a multimodal foundation model-based framework for user interestmodeling and behavior analysis. By integrating video frames, textualdescriptions, and background music into a unified semantic space usingcross-modal alignment strategies, the framework constructs fine-grained userinterest vectors. Additionally, we introduce a behavior-driven featureembedding mechanism that incorporates viewing, liking, and commenting sequencesto model dynamic interest evolution, thereby improving both the timeliness andaccuracy of recommendations. In the experimental phase, we conduct extensiveevaluations using both public and proprietary short video datasets, comparingour approach against multiple mainstream recommendation algorithms and modelingtechniques. Results demonstrate significant improvements in behavior predictionaccuracy, interest modeling for cold-start users, and recommendationclick-through rates. Moreover, we incorporate interpretability mechanisms usingattention weights and feature visualization to reveal the model's decisionbasis under multimodal inputs and trace interest shifts, thereby enhancing thetransparency and controllability of the recommendation system.</description>
      <author>example@mail.com (Yushang Zhao, Yike Peng, Li Zhang, Qianyi Sun, Zhihui Zhang, Yingying Zhuang)</author>
      <guid isPermaLink="false">2509.04751v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Self-Driving Segmentation in Adverse Weather Conditions: A Dual Uncertainty-Aware Training Approach to SAM Optimization</title>
      <link>http://arxiv.org/abs/2509.04735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过引入不确定性量化方法，提高视觉基础模型在恶劣天气条件下自动驾驶场景的图像分割性能。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型(如SAM和SAM2)在通用图像分割中表现优异，但在恶劣天气等视觉模糊度高的情况下表现不佳，主要原因是缺乏不确定性量化。医学影像领域通过不确定性感知训练提高了可靠性。&lt;h4&gt;目的&lt;/h4&gt;增强自动驾驶场景下图像分割模型的鲁棒性，解决模型在恶劣天气条件下的性能问题。&lt;h4&gt;方法&lt;/h4&gt;1) 引入多步微调程序，将不确定性指标直接纳入SAM2的损失函数；2) 将医学图像分割中的不确定性感知适配器(UAT)适应到驾驶场景中。&lt;h4&gt;主要发现&lt;/h4&gt;在CamVid、BDD100K和GTA数据集上评估显示，UAT-SAM在极端天气条件下优于标准SAM，而带有不确定性感知损失的SAM2在多样化驾驶场景中表现更好。&lt;h4&gt;结论&lt;/h4&gt;明确的不确定性建模对安全关键型自动驾驶在具有挑战性环境中具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;近期视觉基础模型的进展，如Segment Anything Model (SAM)及其后续版本SAM2，在通用图像分割基准测试中取得了最先进的性能。然而，在视觉模糊度高的恶劣天气条件下，这些模型表现不佳，主要是因为它们缺乏不确定性量化。受医学影像领域进展的启发，不确定性感知训练提高了模糊情况下的可靠性，我们研究了两种增强自动驾驶分割鲁棒性的方法。首先，我们引入了SAM2的多步微调程序，将不确定性指标直接纳入损失函数，提高整体场景识别能力。其次，我们将专为医学图像分割设计的'不确定性感知适配器'(UAT)适应到驾驶上下文中。我们在CamVid、BDD100K和GTA驾驶数据集上评估了这两种方法。实验表明，UAT-SAM在极端天气条件下优于标准SAM，而具有不确定性感知损失的SAM2在多样化的驾驶场景中实现了更好的性能。这些发现强调了明确的不确定性建模对安全关键型自动驾驶在具有挑战性环境中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision foundation models, such as the Segment AnythingModel (SAM) and its successor SAM2, have achieved state-of-the-art performanceon general image segmentation benchmarks. However, these models struggle inadverse weather conditions where visual ambiguity is high, largely due to theirlack of uncertainty quantification. Inspired by progress in medical imaging,where uncertainty-aware training has improved reliability in ambiguous cases,we investigate two approaches to enhance segmentation robustness for autonomousdriving. First, we introduce a multi-step finetuning procedure for SAM2 thatincorporates uncertainty metrics directly into the loss function, improvingoverall scene recognition. Second, we adapt the Uncertainty-Aware Adapter(UAT), originally designed for medical image segmentation, to driving contexts.We evaluate both methods on CamVid, BDD100K, and GTA driving datasets.Experiments show that UAT-SAM outperforms standard SAM in extreme weather,while SAM2 with uncertainty-aware loss achieves improved performance acrossdiverse driving scenes. These findings underscore the value of explicituncertainty modeling for safety-critical autonomous driving in challengingenvironments.</description>
      <author>example@mail.com (Dharsan Ravindran, Kevin Wang, Zhuoyuan Cao, Saleh Abdelrahman, Jeffery Wu)</author>
      <guid isPermaLink="false">2509.04735v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Sample-efficient Integration of New Modalities into Large Language Models</title>
      <link>http://arxiv.org/abs/2509.04606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SEMI的样本高效的模态集成方法，可以有效地将新模态集成到大型语言模型中，显著减少了所需的数据量。&lt;h4&gt;背景&lt;/h4&gt;多模态基础模型可处理多种模态，但由于模态空间大且不断演变，从头训练包含所有模态的模型不可行。此外，将模态集成到现有基础模型需要大量成对数据，而低资源模态通常缺乏这些数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种样本高效的模态集成方法，使大型语言模型能够适应新的模态，而无需大量成对训练数据。&lt;h4&gt;方法&lt;/h4&gt;设计一个超网络，该网络可以将共享投影仪(位于模态特定编码器和LLM之间)适应任何模态。超网络在高资源模态上训练，并在推理时基于少量样本生成适配器。通过等距变换增加编码器数量以提高训练模态多样性。&lt;h4&gt;主要发现&lt;/h4&gt;SEMI在集成新模态(卫星图像、天文图像、惯性测量和分子)时显著提高了样本效率，适用于任意嵌入维度的编码器。例如，要达到与32次样本SEMI相同的准确度，从头训练投影仪需要多64倍的数据。&lt;h4&gt;结论&lt;/h4&gt;SEMI方法有望扩展基础模型的模态覆盖范围，使模型能够更高效地适应新的模态。&lt;h4&gt;翻译&lt;/h4&gt;多模态基础模型可以处理多种模态。然而，由于可能的模态空间很大且随时间演变，从头开始训练一个包含所有模态的模型是不可行的。此外，将模态集成到现有基础模型中目前需要大量成对数据，而这些数据对于低资源模态通常不可用。在本文中，我们介绍了一种将模态高效集成到大型语言模型中的方法。为此，我们设计了一个超网络，可以将位于模态特定编码器和大型语言模型之间的共享投影仪适应任何模态。该超网络在高资源模态上训练，并在推理时基于任意模态的少量样本生成合适的适配器。为了增加训练模态的多样性，我们通过等距变换人为增加了编码器的数量。我们发现，SEMI在集成新模态时，在少量样本情况下显著提高了样本效率，适用于任意嵌入维度的编码器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal foundation models can process several modalities. However, sincethe space of possible modalities is large and evolving over time, training amodel from scratch to encompass all modalities is unfeasible. Moreover,integrating a modality into a pre-existing foundation model currently requiresa significant amount of paired data, which is often not available forlow-resource modalities. In this paper, we introduce a method forsample-efficient modality integration (SEMI) into Large Language Models (LLMs).To this end, we devise a hypernetwork that can adapt a shared projector --placed between modality-specific encoders and an LLM -- to any modality. Thehypernetwork, trained on high-resource modalities (i.e., text, speech, audio,video), is conditioned on a few samples from any arbitrary modality atinference time to generate a suitable adapter. To increase the diversity oftraining modalities, we artificially multiply the number of encoders throughisometric transformations. We find that SEMI achieves a significant boost insample efficiency during few-shot integration of new modalities (i.e.,satellite images, astronomical images, inertial measurements, and molecules)with encoders of arbitrary embedding dimensionality. For instance, to reach thesame accuracy as 32-shot SEMI, training the projector from scratch needs64$\times$ more data. As a result, SEMI holds promise to extend the modalitycoverage of foundation models.</description>
      <author>example@mail.com (Osman Batur İnce, André F. T. Martins, Oisin Mac Aodha, Edoardo M. Ponti)</author>
      <guid isPermaLink="false">2509.04606v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>CEHR-XGPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
      <link>http://arxiv.org/abs/2509.03643v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CEHR-XGPT是一种通用基础模型，专为电子健康记录数据设计，整合了特征表示、零样本预测和合成数据生成三种能力，通过时间令牌学习框架支持临床序列的时间推理，在各项任务中表现优异，能有效泛化到外部数据集。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录提供患者健康的丰富纵向视图，具有推进临床决策支持、风险预测和数据驱动医疗研究的潜力，但大多数现有人工智能模型为单一目的设计，限制了其在现实世界中的泛化能力和实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用基础模型，将EHR数据分析的三种基本能力（特征表示、零样本预测和合成数据生成）统一在单一架构中。&lt;h4&gt;方法&lt;/h4&gt;提出CEHR-XGPT模型，采用新颖的时间令牌学习框架，将患者的动态时间线明确编码到模型结构中，以支持对临床序列的时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;CEHR-XGPT在所有三项任务中均表现出强大性能，通过词汇扩展和微调能有效泛化到外部数据集。&lt;h4&gt;结论&lt;/h4&gt;CEHR-XGPT的多功能性使研究人员无需任务特定的重新训练即可实现快速模型开发、队列发现和患者结果预测。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录（EHRs）提供了患者健康的丰富纵向视图，在推进临床决策支持、风险预测和数据驱动的医疗研究方面具有巨大潜力。然而，大多数用于EHR的人工智能模型都是为狭窄的单一目的任务设计的，限制了它们在现实世界环境中的泛化能力和实用性。在这里，我们提出了CEHR-XGPT，这是一种用于EHR数据的通用基础模型，在单一架构中统一了三种基本能力——特征表示、零样本预测和合成数据生成。为了支持对临床序列的时间推理，CEHR-XGPT采用了一种新颖的时间令牌学习框架，明确将患者的动态时间线编码到模型结构中。CEHR-XGPT在所有三项任务中都表现出强大的性能，并通过词汇扩展和微调有效地泛化到外部数据集。它的多功能性使得无需任务特定的重新训练即可实现快速模型开发、队列发现和患者结果预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) provide a rich, longitudinal view of patienthealth and hold significant potential for advancing clinical decision support,risk prediction, and data-driven healthcare research. However, most artificialintelligence (AI) models for EHRs are designed for narrow, single-purposetasks, limiting their generalizability and utility in real-world settings.Here, we present CEHR-XGPT, a general-purpose foundation model for EHR datathat unifies three essential capabilities - feature representation, zero-shotprediction, and synthetic data generation - within a single architecture. Tosupport temporal reasoning over clinical sequences, CEHR-XGPT incorporates anovel time-token-based learning framework that explicitly encodes patients'dynamic timelines into the model structure. CEHR-XGPT demonstrates strongperformance across all three tasks and generalizes effectively to externaldatasets through vocabulary expansion and fine-tuning. Its versatility enablesrapid model development, cohort discovery, and patient outcome forecastingwithout the need for task-specific retraining.</description>
      <author>example@mail.com (Chao Pang, Jiheum Park, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Shalmali Joshi, Noémie Elhadad, Karthik Natarajan)</author>
      <guid isPermaLink="false">2509.03643v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning to accelerate distributed ADMM using graph neural networks</title>
      <link>http://arxiv.org/abs/2509.05288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review, the first two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种将分布式ADMM算法与图神经网络相结合的方法，通过学习自适应超参数来提高ADMM的收敛速度和解决方案质量，同时保持算法的收敛特性。&lt;h4&gt;背景&lt;/h4&gt;分布式优化是大规模机器学习和控制应用的基础。ADMM方法因其强收敛保证和适合分布式计算而受到欢迎，但常面临收敛速度慢和对超参数选择敏感的问题。&lt;h4&gt;目的&lt;/h4&gt;解决ADMM收敛速度慢和对超参数选择敏感的问题，提高分布式优化算法的性能。&lt;h4&gt;方法&lt;/h4&gt;将分布式ADMM迭代表示在图神经网络的消息传递框架中，通过图神经网络学习自适应步长和通信权重，基于迭代预测超参数。通过展开固定次数的ADMM迭代，端到端训练网络参数，以最小化特定问题类的最终迭代误差。&lt;h4&gt;主要发现&lt;/h4&gt;学习到的ADMM变体在收敛速度和解决方案质量方面一致优于标准ADMM，数值实验验证了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;将图神经网络与ADMM结合可以有效改善ADMM的性能，在保持算法收敛特性的同时，显著提高收敛速度和解决方案质量。&lt;h4&gt;翻译&lt;/h4&gt;分布式优化是大规模机器学习和控制应用的基础。在现有方法中，交替方向乘子法（ADMM）因其强收敛保证和适合分布式计算而广受欢迎。然而，ADMM常面临收敛速度慢和对超参数选择敏感的问题。在这项工作中，我们展示了分布式ADMM迭代可以在图神经网络（GNNs）的消息传递框架中自然表示。基于这一联系，我们提出通过图神经网络学习自适应步长和通信权重，该网络基于迭代预测超参数。通过展开固定次数的ADMM迭代，我们端到端训练网络参数，以最小化特定问题类的最终迭代误差，同时保持算法的收敛特性。数值实验表明，我们学习到的变体在收敛速度和解决方案质量方面一致优于标准ADMM。代码可在https://github.com/paulhausner/learning-distributed-admm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Distributed optimization is fundamental in large-scale machine learning andcontrol applications. Among existing methods, the Alternating Direction Methodof Multipliers (ADMM) has gained popularity due to its strong convergenceguarantees and suitability for decentralized computation. However, ADMM oftensuffers from slow convergence and sensitivity to hyperparameter choices. Inthis work, we show that distributed ADMM iterations can be naturallyrepresented within the message-passing framework of graph neural networks(GNNs). Building on this connection, we propose to learn adaptive step sizesand communication weights by a graph neural network that predicts thehyperparameters based on the iterates. By unrolling ADMM for a fixed number ofiterations, we train the network parameters end-to-end to minimize the finaliterates error for a given problem class, while preserving the algorithm'sconvergence properties. Numerical experiments demonstrate that our learnedvariant consistently improves convergence speed and solution quality comparedto standard ADMM. The code is available athttps://github.com/paulhausner/learning-distributed-admm.</description>
      <author>example@mail.com (Henri Doerks, Paul Häusner, Daniel Hernández Escobar, Jens Sjölund)</author>
      <guid isPermaLink="false">2509.05288v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>RapidGNN: Energy and Communication-Efficient Distributed Training on Large-Scale Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.05207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2505.10806&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RapidGNN是一种创新的分布式图神经网络训练框架，通过确定性采样调度策略实现高效缓存构建和远程特征预取，显著提升了训练性能和能源效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在探索实体间结构关系的各种任务中变得流行。然而，由于数据集的高度连接结构，在大规模图上进行GNN的分布式训练面临重大挑战。传统的基于采样的方法虽然可以减轻计算负载，但通信开销仍然是一个主要问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种分布式GNN训练框架，解决大规模图上GNN训练的通信开销问题，提高训练效率和能源效率。&lt;h4&gt;方法&lt;/h4&gt;提出RapidGNN，一种具有确定性采样调度策略的分布式GNN训练框架，能够实现高效的缓存构建和远程特征预取。&lt;h4&gt;主要发现&lt;/h4&gt;在基准图数据集上，RapidGNN将端到端训练吞吐量平均提高了2.46倍到3.00倍，同时将远程特征获取减少了9.70倍到15.39倍。RapidGNN展示了接近线性的可扩展性，随着计算单元数量的增加而高效扩展。对于CPU和GPU，RapidGNN分别比基线方法提高了44%和32%的能源效率。&lt;h4&gt;结论&lt;/h4&gt;RapidGNN是一种有效的分布式GNN训练解决方案，能够显著提高训练吞吐量，减少远程特征获取，并提高能源效率，适用于不同规模和拓扑结构的图数据集。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为探索实体间结构关系的各种任务中的热门选择。然而，由于数据集的高度连接结构，在大规模图上进行GNN的分布式训练带来了重大挑战。传统的基于采样的方法减轻了计算负载，但通信开销仍然是一个挑战。本文提出了RapidGNN，一种具有确定性采样调度的分布式GNN训练框架，能够实现高效的缓存构建和远程特征预取。在基准图数据集上的评估表明，RapidGNN在不同规模和拓扑结构上都是有效的。RapidGNN在基准数据集上将端到端训练吞吐量平均提高了2.46倍到3.00倍，同时将远程特征获取减少了9.70倍到15.39倍。RapidGNN进一步展示了接近线性的可扩展性，随着计算单元数量的增加而高效扩展。此外，对于CPU和GPU，RapidGNN分别比基线方法提高了44%和32%的能源效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become popular across a diverse set oftasks in exploring structural relationships between entities. However, due tothe highly connected structure of the datasets, distributed training of GNNs onlarge-scale graphs poses significant challenges. Traditional sampling-basedapproaches mitigate the computational loads, yet the communication overheadremains a challenge. This paper presents RapidGNN, a distributed GNN trainingframework with deterministic sampling-based scheduling to enable efficientcache construction and prefetching of remote features. Evaluation on benchmarkgraph datasets demonstrates RapidGNN's effectiveness across different scalesand topologies. RapidGNN improves end-to-end training throughput by 2.46x to3.00x on average over baseline methods across the benchmark datasets, whilecutting remote feature fetches by over 9.70x to 15.39x. RapidGNN furtherdemonstrates near-linear scalability with an increasing number of computingunits efficiently. Furthermore, it achieves increased energy efficiency overthe baseline methods for both CPU and GPU by 44% and 32%, respectively.</description>
      <author>example@mail.com (Arefin Niam, Tevfik Kosar, M S Q Zulkar Nine)</author>
      <guid isPermaLink="false">2509.05207v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Matrix Factorization Based Graph Contrastive Learning for Recommendation System</title>
      <link>http://arxiv.org/abs/2509.05115v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HMFGCL的新方法，结合了低秩矩阵分解和奇异值分解两种技术，通过图对比学习解决推荐系统中的数据稀疏性问题，并在多个数据集上表现出色，尤其是在小规模数据集上。&lt;h4&gt;背景&lt;/h4&gt;近年来，结合对比学习和图神经网络的方法已出现用于解决推荐系统的挑战，在推荐领域展现出强大性能并发挥重要作用。&lt;h4&gt;目的&lt;/h4&gt;解决现有图对比学习方法中数据增强策略的局限性，更好地捕获用户-项目交互信息。&lt;h4&gt;方法&lt;/h4&gt;提出HMFGCL（基于混合矩阵分解的图对比学习）方法，整合低秩矩阵分解（MF）和奇异值分解（SVD）两种技术，互补获取全局协作信息，构建增强视图。&lt;h4&gt;主要发现&lt;/h4&gt;现有图对比学习方法主要基于扰动图结构和应用聚类两种数据增强策略，但这些策略获得的交互信息不能完全捕捉用户-项目交互；对比学习通过数据增强策略有效缓解了数据稀疏问题。&lt;h4&gt;结论&lt;/h4&gt;在多个公共数据集上的实验结果表明，HMFGCL模型优于现有基线方法，特别在小规模数据集上表现突出。&lt;h4&gt;翻译&lt;/h4&gt;近年来，结合对比学习与图神经网络的方法已出现，用于解决推荐系统的挑战，展现出强大的性能并在该领域发挥重要作用。对比学习主要通过采用数据增强策略解决数据稀疏问题，有效缓解了这一问题并显示出良好的结果。尽管现有研究已取得良好成果，但当前大多数图对比学习方法基于两种数据增强策略：第一种是扰动图结构，如随机添加或删除边；第二种是应用聚类技术。我们认为通过这两种策略获得的交互信息不能完全捕捉用户-项目交互。在本文中，我们提出了一种名为HMFGCL（基于混合矩阵分解的图对比学习）的新方法，该方法整合了两种不同的矩阵分解技术——低秩矩阵分解（MF）和奇异值分解（SVD）——互补地获取全局协作信息，从而构建增强视图。在多个公共数据集上的实验结果表明，我们的模型优于现有基线方法，特别是在小规模数据集上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, methods that combine contrastive learning with graph neuralnetworks have emerged to address the challenges of recommendation systems,demonstrating powerful performance and playing a significant role in thisdomain. Contrastive learning primarily tackles the issue of data sparsity byemploying data augmentation strategies, effectively alleviating this problemand showing promising results. Although existing research has achievedfavorable outcomes, most current graph contrastive learning methods are basedon two types of data augmentation strategies: the first involves perturbing thegraph structure, such as by randomly adding or removing edges; and the secondapplies clustering techniques. We believe that the interactive informationobtained through these two strategies does not fully capture the user-iteminteractions. In this paper, we propose a novel method called HMFGCL (HybridMatrix Factorization Based Graph Contrastive Learning), which integrates twodistinct matrix factorization techniques-low-rank matrix factorization (MF) andsingular value decomposition (SVD)-to complementarily acquire globalcollaborative information, thereby constructing enhanced views. Experimentalresults on multiple public datasets demonstrate that our model outperformsexisting baselines, particularly on small-scale datasets.</description>
      <author>example@mail.com (Hao Chen, Wenming Ma, Zihao Chu, Mingqi Li)</author>
      <guid isPermaLink="false">2509.05115v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Graph Unlearning: Efficient Node Removal in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.04785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图神经网络节点遗忘方法，旨在有效移除敏感训练节点信息并保护隐私。作者提出了三种创新方法，其中两种特别利用了图拓扑特征，在基准测试中表现出优越的性能和效率。&lt;h4&gt;背景&lt;/h4&gt;随着对隐私攻击和敏感信息泄露的担忧增加，研究人员积极探索从图神经网络模型中高效移除敏感训练数据的方法。节点遗忘作为一种有前景的技术，可以通过从GNN模型中高效移除特定训练节点信息来保护敏感节点的隐私。然而，现有的节点遗忘方法要么对GNN结构施加限制，要么没有有效利用图拓扑进行节点遗忘，甚至会损害图的拓扑结构，难以实现满意的性能-复杂度权衡。&lt;h4&gt;目的&lt;/h4&gt;解决现有节点遗忘方法的局限性，实现GNN中训练节点移除的高效遗忘，提出三种新颖的节点遗忘方法，并验证它们在保护GNN模型隐私方面的优越性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了三种新颖的节点遗忘方法：1) 基于类的标签替换(Class-based Label Replacement)；2) 基于拓扑引导的邻居平均后验概率(Topology-guided Neighbor Mean Posterior Probability)；3) 类一致的邻居节点过滤(Class-consistent Neighbor Node Filtering)。其中，后两种方法有效利用了图的拓扑特征，实现了更有效的节点遗忘。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上进行的实验结果表明，所提出的方法在模型效用、遗忘效用和遗忘效率方面均表现出优越性，证明了它们在节点遗忘方面的实用性和效率，以及与最先进的节点遗忘方法相比的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够高效移除敏感训练节点，保护GNN中敏感节点的隐私信息。这些发现有助于提高GNN模型的隐私和安全性，并为节点遗忘领域提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;随着对隐私攻击和潜在敏感信息泄露的担忧日益增加，研究人员积极探索高效移除敏感训练数据并降低图神经网络(GNN)模型隐私风险的方法。节点遗忘已成为一种有前景的技术，通过从GNN模型中高效移除特定训练节点信息来保护敏感节点的隐私。然而，现有的节点遗忘方法要么对GNN结构施加限制，要么没有有效利用图拓扑进行节点遗忘。一些方法甚至损害了图的拓扑结构，使得难以实现令人满意的性能-复杂度权衡。为了解决这些问题并实现GNN中训练节点移除的高效遗忘，我们提出了三种新颖的节点遗忘方法：基于类的标签替换、基于拓扑引导的邻居平均后验概率和类一致的邻居节点过滤。在这些方法中，基于拓扑引导的邻居平均后验概率和类一致的邻居节点过滤有效利用了图的拓扑特征，实现了更有效的节点遗忘。为了验证我们提出的节点遗忘方法的优越性，我们在三个基准数据集上进行了实验。评估标准包括模型效用、遗忘效用和遗忘效率。实验结果证明了所提出方法的实用性和效率，并展示了它们与最先进的节点遗忘方法相比的优势。总体而言，所提出的方法能够高效移除敏感训练节点，保护GNN中敏感节点的隐私信息。这些发现有助于提高GNN模型的隐私和安全性，并为节点遗忘领域提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With increasing concerns about privacy attacks and potential sensitiveinformation leakage, researchers have actively explored methods to efficientlyremove sensitive training data and reduce privacy risks in graph neural network(GNN) models. Node unlearning has emerged as a promising technique forprotecting the privacy of sensitive nodes by efficiently removing specifictraining node information from GNN models. However, existing node unlearningmethods either impose restrictions on the GNN structure or do not effectivelyutilize the graph topology for node unlearning. Some methods even compromisethe graph's topology, making it challenging to achieve a satisfactoryperformance-complexity trade-off. To address these issues and achieve efficientunlearning for training node removal in GNNs, we propose three novel nodeunlearning methods: Class-based Label Replacement, Topology-guided NeighborMean Posterior Probability, and Class-consistent Neighbor Node Filtering. Amongthese methods, Topology-guided Neighbor Mean Posterior Probability andClass-consistent Neighbor Node Filtering effectively leverage the topologicalfeatures of the graph, resulting in more effective node unlearning. To validatethe superiority of our proposed methods in node unlearning, we conductedexperiments on three benchmark datasets. The evaluation criteria included modelutility, unlearning utility, and unlearning efficiency. The experimentalresults demonstrate the utility and efficiency of the proposed methods andillustrate their superiority compared to state-of-the-art node unlearningmethods. Overall, the proposed methods efficiently remove sensitive trainingnodes and protect the privacy information of sensitive nodes in GNNs. Thefindings contribute to enhancing the privacy and security of GNN models andprovide valuable insights into the field of node unlearning.</description>
      <author>example@mail.com (Faqian Guan, Tianqing Zhu, Zhoutian Wang, Wei Ren, Wanlei Zhou)</author>
      <guid isPermaLink="false">2509.04785v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Inferring the Graph Structure of Images for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.04677v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过寻找传统网格图和超像素方法的替代图表示来提高图神经网络在图像分类任务中的准确性，使用行相关图、列相关图和乘积图表示MNIST和Fashion-MNIST数据集中的图像，实验证明这些方法可以提高下游GNN模型的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;图像数据集如MNIST是测试图神经网络架构的关键基准。传统上，图像被表示为网格图，其中每个节点代表一个像素，边连接相邻像素(垂直和水平)，图信号是图像中每个像素的值(强度)。这些图通常用作图神经网络(如GraphCNNs、GAT、GatedGCN)的输入来对图像进行分类。&lt;h4&gt;目的&lt;/h4&gt;提高下游图神经网络任务的准确性，寻找替代传统网格图和超像素方法的图表示方法来表示数据集图像。&lt;h4&gt;方法&lt;/h4&gt;基于像素值之间的相关性，为MNIST和Fashion-MNIST中的每幅图像构建行相关图、列相关图和乘积图，延续了[5,6]中的方法，并将这些不同的图表示和特征作为输入提供给下游GNN模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，使用这些不同的图表示和特征作为下游GNN模型的输入，可以提高准确性，与使用传统网格图和超像素方法相比，这些替代图表示方法表现更好。&lt;h4&gt;结论&lt;/h4&gt;使用基于相关性的图表示可以改进图神经网络在图像分类任务中的性能，为图像数据集的图表示提供了新的视角，超越了传统的网格图和超像素方法。&lt;h4&gt;翻译&lt;/h4&gt;图像数据集如MNIST是测试图神经网络架构的关键基准。图像传统上被表示为网格图，其中每个节点代表一个像素，边连接相邻像素(垂直和水平)。图信号是图像中每个像素的值(强度)。这些图通常用作图神经网络(例如图卷积神经网络(GraphCNNs)[1,2]、图注意力网络(GAT)[3]、GatedGCN[4])的输入来对图像进行分类。在本工作中，我们通过寻找网格图和超像素方法的替代图来表示数据集图像，改进了下游图神经网络任务的准确性，遵循[5,6]中的方法。我们使用像素值之间的相关性，基于[5,6]中的方法，为MNIST和Fashion-MNIST中的每幅图像找到行相关图、列相关图和乘积图。实验表明，将这些不同的图表示和特征作为下游GNN模型的输入，比使用文献中的传统网格图和超像素方法提高了准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image datasets such as MNIST are a key benchmark for testing Graph NeuralNetwork (GNN) architectures. The images are traditionally represented as a gridgraph with each node representing a pixel and edges connecting neighboringpixels (vertically and horizontally). The graph signal is the values(intensities) of each pixel in the image. The graphs are commonly used as inputto graph neural networks (e.g., Graph Convolutional Neural Networks (GraphCNNs) [1, 2], Graph Attention Networks (GAT) [3], GatedGCN [4]) to classify theimages. In this work, we improve the accuracy of downstream graph neuralnetwork tasks by finding alternative graphs to the grid graph and superpixelmethods to represent the dataset images, following the approach in [5, 6]. Wefind row correlation, column correlation, and product graphs for each image inMNIST and Fashion-MNIST using correlations between the pixel values building onthe method in [5, 6]. Experiments show that using these different graphrepresentations and features as input into downstream GNN models improves theaccuracy over using the traditional grid graph and superpixel methods in theliterature.</description>
      <author>example@mail.com (Mayur S Gowda, John Shi, Augusto Santos, José M. F. Moura)</author>
      <guid isPermaLink="false">2509.04677v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability</title>
      <link>http://arxiv.org/abs/2509.03547v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MatterVial是一个创新的混合框架，用于材料科学中的基于特征的机器学习，通过整合多种预训练图神经网络的潜在表示和新型特征，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;材料科学领域需要结合传统特征模型的透明性和深度学习模型的预测能力，以提高材料预测的准确性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个高性能、透明的机器学习框架，能够在保持化学透明度的同时，提供与最先进端到端图神经网络相媲美的预测能力。&lt;h4&gt;方法&lt;/h4&gt;整合多种预训练图神经网络（包括基于结构的MEGNet、基于组成的ROOST和等变的ORB图网络）的潜在表示，结合计算高效的GNN近似描述符和符号回归产生的新特征，并使用代理模型和符号回归进行可解释性分析。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench任务上，该方法显著降低了基于特征模型MODNet的误差，将其性能提升至与最先进的端到端GNN相当甚至在某些情况下超越它们，多个任务的准确率提高了40%以上。&lt;h4&gt;结论&lt;/h4&gt;MatterVial统一框架通过提供高性能、透明的工具，符合可解释AI原则，推动了材料信息学的发展，为更有针对性和自主性的材料发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了MatterVial，一个用于材料科学中基于特征机器学习的创新混合框架。MatterVial通过整合来自多样化预训练图神经网络模型的潜在表示来扩展特征空间，包括：基于结构的（MEGNet）、基于组成的（ROOST）和等变的（ORB）图网络，以及计算高效的GNN近似描述符和符号回归的新特征。我们的方法结合了传统基于特征模型的化学透明性和深度学习架构的预测能力。在Matbench任务上增强基于特征的模型MODNet时，这种方法带来了显著的误差降低，并将其性能提升至与最先进的端到端GNN相媲美，在多个情况下甚至超越它们，多个任务的准确率提高了40%以上。集成的可解释性模块使用代理模型和符号回归，将潜在的GNN衍生描述符解码为明确、具有物理意义的公式。这个统一框架通过提供符合可解释AI原则的高性能透明工具，推动了材料信息学的发展，为更有针对性和自主性的材料发现铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces MatterVial, an innovative hybrid framework forfeature-based machine learning in materials science. MatterVial expands thefeature space by integrating latent representations from a diverse suite ofpretrained graph neural network (GNN) models including: structure-based(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, withcomputationally efficient, GNN-approximated descriptors and novel features fromsymbolic regression. Our approach combines the chemical transparency oftraditional feature-based models with the predictive power of deep learningarchitectures. When augmenting the feature-based model MODNet on Matbenchtasks, this method yields significant error reductions and elevates itsperformance to be competitive with, and in several cases superior to,state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% formultiple tasks. An integrated interpretability module, employing surrogatemodels and symbolic regression, decodes the latent GNN-derived descriptors intoexplicit, physically meaningful formulas. This unified framework advancesmaterials informatics by providing a high-performance, transparent tool thataligns with the principles of explainable AI, paving the way for more targetedand autonomous materials discovery.</description>
      <author>example@mail.com (Rogério Almeida Gouvêa, Pierre-Paul De Breuck, Tatiane Pretto, Gian-Marco Rignanese, Marcos José Leite Santos)</author>
      <guid isPermaLink="false">2509.03547v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Crosscoding Through Time: Tracking Emergence &amp; Consolidation Of Linguistic Representations Throughout LLM Pretraining</title>
      <link>http://arxiv.org/abs/2509.05291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出使用稀疏交叉编码器和新的相对间接效应(RelIE)指标来追踪大型语言模型预训练过程中语言特征的演变，填补了传统评估方法无法揭示模型如何获取概念和能力的空白。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在预训练过程中学习到复杂的抽象能力，如检测不规则复数名词主语，但传统评估方法无法揭示这些能力如何形成。&lt;h4&gt;目的&lt;/h4&gt;为了更好地理解模型在概念层面的训练过程，发现和校准不同模型检查点之间的特征演变。&lt;h4&gt;方法&lt;/h4&gt;使用稀疏交叉编码器在具有显著性能和表示变化的开放源代码检查点之间进行训练，并引入相对间接效应(RelIE)指标来追踪特征何时对任务性能变得因果重要。&lt;h4&gt;主要发现&lt;/h4&gt;交叉编码器可以检测预训练过程中语言特征的出现、维持和消失，并通过RelIE指标能够识别特征变得对任务性能有因果相关性的具体训练阶段。&lt;h4&gt;结论&lt;/h4&gt;该方法与架构无关且可扩展，为更可解释和细粒度地分析预训练过程中的表示学习提供了有希望的途径。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在预训练过程中学习到复杂的抽象能力，比如检测不规则复数名词主语。然而，目前尚不清楚特定的语言能力何时以及如何出现，因为传统的评估方法无法揭示模型如何获取概念和能力。为了填补这一空白并更好地理解模型在概念层面的训练过程，我们使用稀疏交叉编码器来发现和校准不同模型检查点之间的特征。使用这种方法，我们跟踪了预训练过程中语言特征的演变。我们在具有显著性能和表示变化的开放源代码检查点之间训练交叉编码器，并引入了一种新的指标——相对间接效应(RelIE)，用于追踪单个特征对任务性能变得因果相关的时间点。我们表明，交叉编码器可以检测预训练过程中特征的出现、维持和消失。我们的方法与架构无关且可扩展，为更可解释和细粒度地分析预训练过程中的表示学习提供了有希望的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) learn non-trivial abstractions duringpretraining, like detecting irregular plural noun subjects. However, it is notwell understood when and how specific linguistic abilities emerge astraditional evaluation methods such as benchmarking fail to reveal how modelsacquire concepts and capabilities. To bridge this gap and better understandmodel training at the concept level, we use sparse crosscoders to discover andalign features across model checkpoints. Using this approach, we track theevolution of linguistic features during pretraining. We train crosscodersbetween open-sourced checkpoint triplets with significant performance andrepresentation shifts, and introduce a novel metric, Relative Indirect Effects(RelIE), to trace training stages at which individual features become causallyimportant for task performance. We show that crosscoders can detect featureemergence, maintenance, and discontinuation during pretraining. Our approach isarchitecture-agnostic and scalable, offering a promising path toward moreinterpretable and fine-grained analysis of representation learning throughoutpretraining.</description>
      <author>example@mail.com (Deniz Bayazit, Aaron Mueller, Antoine Bosselut)</author>
      <guid isPermaLink="false">2509.05291v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SL-SLR: Self-Supervised Representation Learning for Sign Language Recognition</title>
      <link>http://arxiv.org/abs/2509.05188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对手语识别的自监督学习框架，解决了传统对比学习方法在处理手语视频时面临的忽视视频部分相关性和负对相似性问题。&lt;h4&gt;背景&lt;/h4&gt;手语识别(SLR)是一个旨在识别视频中手语的机器学习任务。由于标注数据稀缺，无监督方法如对比学习在该领域变得很有前景，它们通过拉近正对(同一实例的两个增强版本)和推开负对(不同于正对)来学习有意义的表示。&lt;h4&gt;目的&lt;/h4&gt;设计一个自监督学习框架，学习对手语识别有意义的表现，解决传统对比学习方法在SLR中的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个关键组件的自监督学习框架：(1)一种基于自由负对的新型自监督方法；(2)一种新的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;该方法与多种对比和自监督方法相比，在线性评估、半监督学习和手语间迁移性方面显示出显著的精度提升。&lt;h4&gt;结论&lt;/h4&gt;通过解决传统对比学习方法在SLR中的两个关键问题，本文提出的自监督框架能够学习更具区分性的特征，提高下游任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;手语识别(SLR)是一种机器学习任务，旨在识别视频中的手语。由于标注数据的稀缺，像对比学习这样的无监督方法已成为该领域有前景的方法。它们通过将正对(同一实例的两个增强版本)拉近并将负对(不同于正对)推开来学习有意义的表示。在SLR中，在手语视频中，只有某些部分提供对其识别真正有用的信息。将对比方法应用于SLR引发两个问题：(i)对比学习方法同等对待视频的所有部分，没有考虑某些部分的相关性；(ii)不同手势之间的共享动作使得负对高度相似，增加了手势区分的难度。这些问题导致学习到对手语识别缺乏区分性的特征，并在下游任务中表现不佳。为此，本文提出了一种专为学习SLR有意义表示而设计的自监督学习框架。该框架包含两个协同工作的关键组件：(i)一种基于自由负对的新型自监督方法；(ii)一种新的数据增强技术。与多种对比和自监督方法相比，该方法在线性评估、半监督学习和手语间的迁移性方面显示出显著的精度提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language recognition (SLR) is a machine learning task aiming to identifysigns in videos. Due to the scarcity of annotated data, unsupervised methodslike contrastive learning have become promising in this field. They learnmeaningful representations by pulling positive pairs (two augmented versions ofthe same instance) closer and pushing negative pairs (different from thepositive pairs) apart. In SLR, in a sign video, only certain parts provideinformation that is truly useful for its recognition. Applying contrastivemethods to SLR raises two issues: (i) contrastive learning methods treat allparts of a video in the same way, without taking into account the relevance ofcertain parts over others; (ii) shared movements between different signs makenegative pairs highly similar, complicating sign discrimination. These issueslead to learning non-discriminative features for sign recognition and poorresults in downstream tasks. In response, this paper proposes a self-supervisedlearning framework designed to learn meaningful representations for SLR. Thisframework consists of two key components designed to work together: (i) a newself-supervised approach with free-negative pairs; (ii) a new data augmentationtechnique. This approach shows a considerable gain in accuracy compared toseveral contrastive and self-supervised methods, across linear evaluation,semi-supervised learning, and transferability between sign languages.</description>
      <author>example@mail.com (Ariel Basso Madjoukeng, Jérôme Fink, Pierre Poitier, Edith Belise Kenmogne, Benoit Frenay)</author>
      <guid isPermaLink="false">2509.05188v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Multidimensional Urban Poverty Representation with Satellite Imagery</title>
      <link>http://arxiv.org/abs/2509.04958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的表示学习框架，通过融合卫星图像中的可及性、形态学和经济特征，实现精确的城市贫困制图，解决了仅依赖城市化特征无法准确捕捉贫困的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习的进步使得从卫星图像推断城市社会经济特征成为可能，但仅依赖城市化特征的模型与贫困指标相关性较弱，因为无序的城市增长可能掩盖经济差异和空间不平等。&lt;h4&gt;目的&lt;/h4&gt;引入一个新的表示学习框架，从超高分辨率卫星图像中捕获多维度的剥夺相关特征，用于精确的城市贫困制图。&lt;h4&gt;方法&lt;/h4&gt;该方法整合了三种互补特征：(1)通过对比学习编码接近基本基础设施的可及性特征；(2)从建筑足迹推导反映非正规定居点住房条件的形态学特征；(3)从夜间灯光强度推断经济活动的经济特征。同时，通过后门调整机制利用形态学特征减轻训练经济模块时的虚假相关性。&lt;h4&gt;主要发现&lt;/h4&gt;通过融合这些互补特征，该框架能够捕捉与经济发展趋势不同的贫困复杂性质。在开普敦、达卡和金边三个首都城市的评估中，该模型显著优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;该框架为数据稀缺地区的贫困制图和政策支持提供了强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;深度学习的最新进展使得从卫星图像中推断城市社会经济特征成为可能。然而，仅依赖城市化特征的模型通常与贫困指标相关性较弱，因为无序的城市增长可能掩盖经济差异和空间不平等。为解决这一局限，我们引入了一种新的表示学习框架，从超高分辨率卫星图像中捕获多维度的剥夺相关特征，用于精确的城市贫困制图。我们的方法整合了三种互补特征：(1)通过对比学习编码接近基本基础设施的可及性特征；(2)从建筑足迹推导反映非正规定居点住房条件的形态学特征；(3)从夜间灯光强度推断作为经济活动代理的经济特征。为了减轻虚假相关性——例如那些不能代表贫困条件的非住宅夜间灯光源——我们在训练经济模块时纳入了一个利用形态学特征的后门调整机制。通过将这些互补特征融合到统一表示中，我们的框架捕捉了贫困的复杂性质，这些性质往往与经济发展趋势不同。在开普敦、达卡和金边三个首都城市的评估中，我们的模型显著优于现有基线，为数据稀缺地区的贫困制图和政策支持提供了强大的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in deep learning have enabled the inference of urbansocioeconomic characteristics from satellite imagery. However, models relyingsolely on urbanization traits often show weak correlations with povertyindicators, as unplanned urban growth can obscure economic disparities andspatial inequalities. To address this limitation, we introduce a novelrepresentation learning framework that captures multidimensionaldeprivation-related traits from very high-resolution satellite imagery forprecise urban poverty mapping. Our approach integrates three complementarytraits: (1) accessibility traits, learned via contrastive learning to encodeproximity to essential infrastructure; (2) morphological traits, derived frombuilding footprints to reflect housing conditions in informal settlements; and(3) economic traits, inferred from nightlight intensity as a proxy for economicactivity. To mitigate spurious correlations - such as those fromnon-residential nightlight sources that misrepresent poverty conditions - weincorporate a backdoor adjustment mechanism that leverages morphological traitsduring training of the economic module. By fusing these complementary featuresinto a unified representation, our framework captures the complex nature ofpoverty, which often diverges from economic development trends. Evaluationsacross three capital cities - Cape Town, Dhaka, and Phnom Penh - show that ourmodel significantly outperforms existing baselines, offering a robust tool forpoverty mapping and policy support in data-scarce regions.</description>
      <author>example@mail.com (Sungwon Park, Sumin Lee, Jihee Kim, Jae-Gil Lee, Meeyoung Cha, Jeasurk Yang, Donghyun Ahn)</author>
      <guid isPermaLink="false">2509.04958v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Phonological Representation Learning for Isolated Signs Improves Out-of-Vocabulary Generalization</title>
      <link>http://arxiv.org/abs/2509.04745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何通过语音学归纳偏差改进手语学习表示的推广性，特别是在处理未见手语符号时的表现。&lt;h4&gt;背景&lt;/h4&gt;手语数据集在词汇方面通常不具有代表性，这突显了需要能够推广到未见过的手语符号的模型的必要性。&lt;h4&gt;目的&lt;/h4&gt;调查两种语音学归纳偏差（参数解耦和语音学半监督）以提高已知手语符号的识别和未见手语符号的重构质量。&lt;h4&gt;方法&lt;/h4&gt;使用向量量化自编码器，结合参数解耦（架构偏差）和语音学半监督（正则化技术）来改进手语识别和重构。&lt;h4&gt;主要发现&lt;/h4&gt;与受控基线相比，所提出模型学习到的表示对于一次性重建未见手语符号更有效，并且对于手语识别更具区分性。&lt;h4&gt;结论&lt;/h4&gt;明确的、语言动机的偏差可以改进手语学习表示的推广性，本研究提供了相关的定量分析。&lt;h4&gt;翻译&lt;/h4&gt;手语数据集在词汇方面通常不具有代表性，突显了需要能够推广到未见手语的模型的必要性。向量量化是学习离散类令牌表示的一种有前景的方法，但目前尚未评估学习到的单元是否会捕获阻碍词汇外性能的虚假相关性。本研究调查了两种语音学归纳偏差：参数解耦（架构偏差）和语音学半监督（正则化技术），以使用向量量化自编码器改进已知手语符号的孤立手语识别和未见手语符号的重构质量。主要发现是，与受控基线相比，所提出模型学习到的表示对于一次性重建未见手语符号更有效，并且对于手语识别更具区分性。这项工作提供了关于明确的、语言动机的偏差如何改进手语学习表示的推广性的定量分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sign language datasets are often not representative in terms of vocabulary,underscoring the need for models that generalize to unseen signs. Vectorquantization is a promising approach for learning discrete, token-likerepresentations, but it has not been evaluated whether the learned unitscapture spurious correlations that hinder out-of-vocabulary performance. Thiswork investigates two phonological inductive biases: Parameter Disentanglement,an architectural bias, and Phonological Semi-Supervision, a regularizationtechnique, to improve isolated sign recognition of known signs andreconstruction quality of unseen signs with a vector-quantized autoencoder. Theprimary finding is that the learned representations from the proposed model aremore effective for one-shot reconstruction of unseen signs and morediscriminative for sign identification compared to a controlled baseline. Thiswork provides a quantitative analysis of how explicit, linguistically-motivatedbiases can improve the generalization of learned representations of signlanguage.</description>
      <author>example@mail.com (Lee Kezar, Zed Sehyr, Jesse Thomason)</author>
      <guid isPermaLink="false">2509.04745v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Imitation Learning Based on Disentangled Representation Learning of Behavioral Characteristics</title>
      <link>http://arxiv.org/abs/2509.04737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures, Accepted at CoRL2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种运动生成模型，使机器人能够根据人类指令中的修饰词调整动作，在擦拭和拾取放置任务中表现良好，能够在线响应修饰指令调整动作，优于传统的批量处理方法。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习领域，通过语言指令协调机器人动作正变得越来越可行，但使机器人动作适应人类指令仍具挑战性，因为这些指令通常是定性的，需要探索满足不同条件的行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种运动生成模型，使机器人能够根据人类指令中的修饰词在任务执行期间调整动作行为。&lt;h4&gt;方法&lt;/h4&gt;通过将演示分割成短序列，分配与特定修饰类型相对应的弱监督标签，学习从修饰指令到动作的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;在擦拭和拾取放置任务中评估显示，该方法能够在线响应修饰指令调整动作，而传统批量处理方法在执行期间无法适应。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型能够实时调整机器人动作以适应人类指令中的修饰条件，提高了机器人对语言指令的适应能力。&lt;h4&gt;翻译&lt;/h4&gt;在机器人学习领域，通过语言指令协调机器人动作正变得越来越可行。然而，使机器人动作适应人类指令仍然具有挑战性，因为这些指令通常是定性的，需要探索满足不同条件的行为。本文提出了一种运动生成模型，使机器人能够根据人类指令中的修饰词在任务执行期间调整动作行为。所提出的方法通过将演示分割成短序列，分配与特定修饰类型相对应的弱监督标签，学习从修饰指令到动作的映射。我们在擦拭和拾取放置任务中评估了该方法。结果表明，它能够在线响应修饰指令调整动作，而传统的批量处理方法在执行期间无法适应。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of robot learning, coordinating robot actions through languageinstructions is becoming increasingly feasible. However, adapting actions tohuman instructions remains challenging, as such instructions are oftenqualitative and require exploring behaviors that satisfy varying conditions.This paper proposes a motion generation model that adapts robot actions inresponse to modifier directives human instructions imposing behavioralconditions during task execution. The proposed method learns a mapping frommodifier directives to actions by segmenting demonstrations into shortsequences, assigning weakly supervised labels corresponding to specificmodifier types. We evaluated our method in wiping and pick and place tasks.Results show that it can adjust motions online in response to modifierdirectives, unlike conventional batch-based methods that cannot adapt duringexecution.</description>
      <author>example@mail.com (Ryoga Oishi, Sho Sakaino, Toshiaki Tsuji)</author>
      <guid isPermaLink="false">2509.04737v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Beyond I-Con: Exploring New Dimension of Distance Measures in Representation Learning</title>
      <link>http://arxiv.org/abs/2509.04734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Beyond I-Con框架，通过探索替代的统计散度和相似性核来系统地发现新的损失函数，解决了KL散度在表示学习中可能带来的优化挑战。&lt;h4&gt;背景&lt;/h4&gt;Information Contrastive (I-Con)框架揭示了超过23种表示学习方法隐式地最小化了数据分布和学习分布之间的KL散度，但KL散度存在不对称性和无界性等特性，可能导致优化挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性发现新型损失函数的框架，通过探索替代的统计散度和相似性核来超越现有的基于KL的方法。&lt;h4&gt;方法&lt;/h4&gt;探索替代的统计散度和相似性核；修改PMI算法使用总变差(TV)距离；在监督对比学习中使用TV和基于距离的相似性核替代KL和角度核；在降维中用有界的f-散度替代KL。&lt;h4&gt;主要发现&lt;/h4&gt;(1)在DINO-ViT嵌入的无监督聚类上，使用TV距离的修改版PMI算法实现了最先进结果；(2)在监督对比学习中，使用TV和基于距离的相似性核超越了标准方法；(3)在降维任务中，用有界的f-散度替代KL获得了比SNE更好的结果和下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在表示学习优化中，散度和相似性核的选择至关重要。&lt;h4&gt;翻译&lt;/h4&gt;信息对比(I-Con)框架揭示，超过23种表示学习方法隐式地最小化了数据分布和学习分布之间的KL散度，这些分布编码了数据点之间的相似性。然而，基于KL的损失函数可能与真实目标不一致，KL散度的特性如不对称性和无界性可能会带来优化挑战。我们提出了Beyond I-Con框架，通过探索替代的统计散度和相似性核，使系统能够发现新的损失函数。主要发现：(1)在DINO-ViT嵌入的无监督聚类中，我们通过修改PMI算法使用总变差(TV)距离实现了最先进的结果；(2)在监督对比学习中，我们使用TV和基于距离的相似性核而非KL和角度核，超越了标准方法；(3)在降维方面，我们通过用有界的f-散度替代KL，获得了比SNE更好的定性结果和下游任务性能。我们的结果突显了在表示学习优化中考虑散度和相似性核选择的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Information Contrastive (I-Con) framework revealed that over 23representation learning methods implicitly minimize KL divergence between dataand learned distributions that encode similarities between data points.However, a KL-based loss may be misaligned with the true objective, andproperties of KL divergence such as asymmetry and unboundedness may createoptimization challenges. We present Beyond I-Con, a framework that enablessystematic discovery of novel loss functions by exploring alternativestatistical divergences and similarity kernels. Key findings: (1) onunsupervised clustering of DINO-ViT embeddings, we achieve state-of-the-artresults by modifying the PMI algorithm to use total variation (TV) distance;(2) on supervised contrastive learning, we outperform the standard approach byusing TV and a distance-based similarity kernel instead of KL and an angularkernel; (3) on dimensionality reduction, we achieve superior qualitativeresults and better performance on downstream tasks than SNE by replacing KLwith a bounded f-divergence. Our results highlight the importance ofconsidering divergence and similarity kernel choices in representation learningoptimization.</description>
      <author>example@mail.com (Jasmine Shone, Shaden Alshammari, Mark Hamilton, Zhening Li, William Freeman)</author>
      <guid isPermaLink="false">2509.04734v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Unified Representation Learning for Multi-Intent Diversity and Behavioral Uncertainty in Recommender Systems</title>
      <link>http://arxiv.org/abs/2509.04694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一表示学习框架，用于解决推荐系统中用户意图多样性和行为不确定性的联合建模问题。该框架包含多意图表示模块和不确定性建模机制，通过贝叶斯分布建模捕捉行为模糊性和偏好波动，结合长期意图和短期行为信号，提高推荐准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统推荐系统在处理复杂用户行为时面临建模瓶颈，特别是在捕捉用户意图多样性和行为不确定性方面存在挑战。用户行为序列中包含多粒度兴趣结构，且存在行为模糊性和偏好波动，这些因素都需要被有效建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够联合建模用户意图多样性和行为不确定性的推荐系统框架，通过提取用户行为序列中的多粒度兴趣结构，捕捉行为模糊性和偏好波动，提高推荐的准确性和鲁棒性，特别是在冷启动和行为干扰场景下的表现。&lt;h4&gt;方法&lt;/h4&gt;提出统一表示学习框架，包含：多意图表示模块（引入多个潜在意图向量，通过注意力机制进行加权融合，生成长期用户偏好语义丰富表示）和不确定性建模机制（通过高斯分布学习行为表示的均值和协方差，反映用户在不同行为上下文中的置信度），以及可学习融合策略（结合长期意图和短期行为信号，生成最终用户表示）。&lt;h4&gt;主要发现&lt;/h4&gt;在标准公共数据集上的实验表明，该方法在多个指标上优于现有代表性模型；在冷启动和行为干扰场景下表现出更高的稳定性和适应性；有效缓解了传统方法处理复杂用户行为时的建模瓶颈。&lt;h4&gt;结论&lt;/h4&gt;统一建模策略在现实世界推荐任务中具有有效性和实用价值，能够同时提高推荐准确性和鲁棒性，特别是在处理复杂用户行为方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了推荐系统中联合建模用户意图多样性和行为不确定性的挑战。提出了一种统一的表示学习框架。该框架构建了多意图表示模块和不确定性建模机制。它从用户行为序列中提取多粒度兴趣结构。使用贝叶斯分布建模捕捉行为模糊性和偏好波动。在多意图建模部分，模型引入了多个潜在意图向量。这些向量通过注意力机制进行加权融合，生成长期用户偏好的语义丰富表示。在不确定性建模部分，模型通过高斯分布学习行为表示的均值和协方差。这反映了用户在不同行为上下文中的置信度。接下来，使用可学习融合策略结合长期意图和短期行为信号。这产生了最终的用户表示，提高了推荐准确性和鲁棒性。该方法在标准公共数据集上进行了评估。实验结果表明，它在多个指标上优于现有代表性模型。在冷启动和行为干扰场景下，它还表现出更高的稳定性和适应性。该方法缓解了传统方法处理复杂用户行为时面临的建模瓶颈。这些发现证实了统一建模策略在现实世界推荐任务中的有效性和实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of jointly modeling user intent diversityand behavioral uncertainty in recommender systems. A unified representationlearning framework is proposed. The framework builds a multi-intentrepresentation module and an uncertainty modeling mechanism. It extractsmulti-granularity interest structures from user behavior sequences. Behavioralambiguity and preference fluctuation are captured using Bayesian distributionmodeling. In the multi-intent modeling part, the model introduces multiplelatent intent vectors. These vectors are weighted and fused using an attentionmechanism to generate semantically rich representations of long-term userpreferences. In the uncertainty modeling part, the model learns the mean andcovariance of behavior representations through Gaussian distributions. Thisreflects the user's confidence in different behavioral contexts. Next, alearnable fusion strategy is used to combine long-term intent and short-termbehavior signals. This produces the final user representation, improving bothrecommendation accuracy and robustness. The method is evaluated on standardpublic datasets. Experimental results show that it outperforms existingrepresentative models across multiple metrics. It also demonstrates greaterstability and adaptability under cold-start and behavioral disturbancescenarios. The approach alleviates modeling bottlenecks faced by traditionalmethods when dealing with complex user behavior. These findings confirm theeffectiveness and practical value of the unified modeling strategy inreal-world recommendation tasks.</description>
      <author>example@mail.com (Wei Xu, Jiasen Zheng, Junjiang Lin, Mingxuan Han, Junliang Du)</author>
      <guid isPermaLink="false">2509.04694v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SGS-3D: High-Fidelity 3D Instance Segmentation via Reliable Semantic Mask Splitting and Growing</title>
      <link>http://arxiv.org/abs/2509.05144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SGS-3D的高保真3D实例分割方法，通过'先分割后生长'框架解决2D到3D提升过程中的累积误差问题，有效融合语义和几何信息，显著提高了分割准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;准确的3D实例分割对3D视觉领域的高质量场景理解至关重要，但基于2D到3D提升方法的现有技术难以产生精确的实例级分割，主要因为在提升过程中从模糊语义引导和深度约束不足引入了累积误差。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D实例分割方法的局限性，提出一种能够提高分割精度和鲁棒性的新方法，特别是在处理模糊语义和几何信息时。&lt;h4&gt;方法&lt;/h4&gt;提出SGS-3D框架，采用'先分割后生长'策略：首先使用几何基元净化和分割模糊的提升掩模，然后将它们生长为场景中的完整实例；引入掩模过滤策略利用3D几何基元共现识别可靠语义；利用空间连续性和高级特征进行几何精炼以构建细粒度对象实例。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanNet200、ScanNet++和KITTI-360数据集上的实验表明，SGS-3D显著提高了分割准确性和鲁棒性，能够抵抗来自预训练模型的不准确掩模，产生高保真对象实例，并在多样化的室内和室外环境中保持强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;SGS-3D作为一种无需训练的精炼方法，通过联合融合语义和几何信息，有效解决了2D到3D提升过程中的累积误差问题，为高保真3D实例分割提供了新思路，代码已在补充材料中公开。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D实例分割对3D视觉领域的高质量场景理解至关重要。然而，基于2D到3D提升方法的3D实例分割难以产生精确的实例级分割，这是由于在从模糊语义引导和深度约束不足的提升过程中引入了累积误差。为解决这些挑战，我们提出了用于高保真3D实例分割的可靠语义掩模分割和生长方法（SGS-3D），一种新颖的'先分割后生长'框架，首先使用几何基元净化和分割模糊的提升掩模，然后将它们生长为场景中的完整实例。与直接依赖原始提升掩模并牺牲分割准确性的现有方法不同，SGS-3D作为一种无需训练的精炼方法，联合融合语义和几何信息，实现了两种表示级别之间的有效合作。具体而言，对于语义引导，我们引入了一种掩模过滤策略，利用3D几何基元的共现来识别和移除模糊掩模，从而确保与3D对象实例更可靠的语义一致性。对于几何精炼，我们利用空间连续性和高级特征构建细粒度对象实例，特别是在不同对象之间语义模糊的情况下。在ScanNet200、ScanNet++和KITTI-360上的实验结果表明，SGS-3D显著提高了分割准确性和对预训练模型不准确掩模的鲁棒性，产生高保真对象实例，同时在多样化的室内和室外环境中保持强大的泛化能力。代码可在补充材料中获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于2D到3D提升(2D-to-3D lifting)的3D实例分割方法中存在的精度问题，特别是在处理模糊语义引导、不足深度约束以及遮挡场景时导致的累积误差。这个问题在现实中非常重要，因为3D实例分割是自动驾驶、虚拟现实和多模态场景理解等领域的核心技术，而现有方法在开放世界环境中的泛化能力有限，且深度传感器在无纹理和高反射表面表现不佳，在野外场景中常常不可用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到基于2D预训练基础模型进行3D场景感知是一个有前景的方向，但现有方法存在局限性：基于特征的方法有训练效率和误差传播问题，基于掩码的方法则忽略了语义信息的稳健传播。作者借鉴了现有工作中的3D几何过度分割思想，使用SAM提取语义掩码，并采用HDBSCAN进行空间分割，但创新性地提出了'先分割后生长'策略，联合利用语义和几何线索来克服误差累积问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'先分割后生长'(split-then-grow)策略，联合利用语义和几何线索来克服2D到3D提升中的误差累积。整体流程分为三个主要阶段：1)点-图像映射：建立鲁棒映射并计算可见性；2)2D掩码提案：生成初始掩码并通过同时出现过滤消除模糊掩码；3)语义引导聚合：先通过空间连续性分割生成纯语义-几何种子，再利用特征引导生长将种子扩展为完整实例，最后通过多视图渐进合并形成最终对象实例。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SGS-3D训练-free框架，实现'先分割后生长'策略；2)遮挡感知的点-图像映射，无需真实深度图；3)基于同时出现的掩码过滤机制，提高计算效率和鲁棒性；4)语义引导聚合管道，结合空间连续性和特征引导。相比之前的工作，SGS-3D不直接依赖原始提升掩码，而是先净化和分割模糊掩码；联合融合语义和几何信息；在深度约束不足场景中表现更好；处理遮挡和缺乏深度信息时更鲁棒。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SGS-3D通过可靠的语义掩码分割和生长策略，解决了2D到3D提升方法中的累积误差问题，实现了高保真的3D实例分割，并在多种室内外场景中达到了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D instance segmentation is crucial for high-quality sceneunderstanding in the 3D vision domain. However, 3D instance segmentation basedon 2D-to-3D lifting approaches struggle to produce precise instance-levelsegmentation, due to accumulated errors introduced during the lifting processfrom ambiguous semantic guidance and insufficient depth constraints. To tacklethese challenges, we propose splitting and growing reliable semantic mask forhigh-fidelity 3D instance segmentation (SGS-3D), a novel "split-then-grow"framework that first purifies and splits ambiguous lifted masks using geometricprimitives, and then grows them into complete instances within the scene.Unlike existing approaches that directly rely on raw lifted masks and sacrificesegmentation accuracy, SGS-3D serves as a training-free refinement method thatjointly fuses semantic and geometric information, enabling effectivecooperation between the two levels of representation. Specifically, forsemantic guidance, we introduce a mask filtering strategy that leverages theco-occurrence of 3D geometry primitives to identify and remove ambiguous masks,thereby ensuring more reliable semantic consistency with the 3D objectinstances. For the geometric refinement, we construct fine-grained objectinstances by exploiting both spatial continuity and high-level features,particularly in the case of semantic ambiguity between distinct objects.Experimental results on ScanNet200, ScanNet++, and KITTI-360 demonstrate thatSGS-3D substantially improves segmentation accuracy and robustness againstinaccurate masks from pre-trained models, yielding high-fidelity objectinstances while maintaining strong generalization across diverse indoor andoutdoor environments. Code is available in the supplementary materials.</description>
      <author>example@mail.com (Chaolei Wang, Yang Luo, Jing Du, Siyu Chen, Yiping Chen, Ting Han)</author>
      <guid isPermaLink="false">2509.05144v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>CURE: Controlled Unlearning for Robust Embeddings -- Mitigating Conceptual Shortcuts in Pre-Trained Language Models</title>
      <link>http://arxiv.org/abs/2509.05230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference on Empirical Methods in Natural Language  Processing (EMNLP 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURE是一种新型轻量级框架，能够系统地分离和抑制概念性捷径，同时保留内容信息，在IMDB和Yelp数据集上显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;预训练语言模型在多种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性的影响，这些相关性损害了模型的鲁棒性和公平性。&lt;h4&gt;目的&lt;/h4&gt;引入CURE框架，系统地分离和抑制概念性捷径，同时保留基本的内容信息，以提高模型的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;通过专用内容提取器和反转网络提取概念无关表示，并使用可控去偏模块和对比学习来微调剩余概念线索的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在IMDB上的F1分数绝对提高+10分，在Yelp上提高+2分，同时引入最小的计算开销。&lt;h4&gt;结论&lt;/h4&gt;CURE为对抗概念偏差提供了一个灵活的无监督蓝图，有助于构建更可靠和公平的语言理解系统。&lt;h4&gt;翻译&lt;/h4&gt;预训练语言模型在多种应用中取得了显著成功，但仍然容易受到虚假的、概念驱动的相关性的影响，这些相关性损害了模型的鲁棒性和公平性。在这项工作中，我们引入了CURE，一种新型轻量级框架，能够系统地分离和抑制概念性捷径，同时保留基本的内容信息。我们的方法首先通过一个由反转网络强化的专用内容提取器提取概念无关的表示，确保最小化任务相关信息的损失。随后的可控去偏模块采用对比学习来微调剩余概念线索的影响，使模型能够根据目标任务适当减少有害偏差或利用有益相关性。在IMDB和Yelp数据集上使用三种预训练架构进行评估，CURE在IMDB上的F1分数绝对提高了+10分，在Yelp上提高了+2分，同时引入了最小的计算开销。我们的方法为对抗概念偏差提供了一个灵活的无监督蓝图，为更可靠和公平的语言理解系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained language models have achieved remarkable success across diverseapplications but remain susceptible to spurious, concept-driven correlationsthat impair robustness and fairness. In this work, we introduce CURE, a noveland lightweight framework that systematically disentangles and suppressesconceptual shortcuts while preserving essential content information. Our methodfirst extracts concept-irrelevant representations via a dedicated contentextractor reinforced by a reversal network, ensuring minimal loss oftask-relevant information. A subsequent controllable debiasing module employscontrastive learning to finely adjust the influence of residual conceptualcues, enabling the model to either diminish harmful biases or harnessbeneficial correlations as appropriate for the target task. Evaluated on theIMDB and Yelp datasets using three pre-trained architectures, CURE achieves anabsolute improvement of +10 points in F1 score on IMDB and +2 points on Yelp,while introducing minimal computational overhead. Our approach establishes aflexible, unsupervised blueprint for combating conceptual biases, paving theway for more reliable and fair language understanding systems.</description>
      <author>example@mail.com (Aysenur Kocak, Shuo Yang, Bardh Prenkaj, Gjergji Kasneci)</author>
      <guid isPermaLink="false">2509.05230v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>DeGuV: Depth-Guided Visual Reinforcement Learning for Generalization and Interpretability in Manipulation</title>
      <link>http://arxiv.org/abs/2509.04970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DeGuV的强化学习框架，通过可学习的掩码网络和对比学习技术，提高强化学习代理的泛化能力和样本效率，同时保持训练稳定性。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理可以从视觉输入中学习解决复杂任务，但将学到的技能泛化到新环境仍然是强化学习应用的主要挑战，特别是在机器人领域。虽然数据增强可以提高泛化能力，但它通常会降低样本效率和训练稳定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种强化学习框架，既能提高泛化能力，又能保持样本效率和训练稳定性。&lt;h4&gt;方法&lt;/h4&gt;DeGuV框架包含：1)可学习的掩码网络从深度输入生成掩码，只保留关键视觉信息；2)结合对比学习增强模型对关键特征的识别能力；3)稳定Q值估计技术在数据增强条件下保持训练稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在RL-ViGen基准测试中使用Franka Emika机器人评估显示，DeGuV在零样本模拟到现实迁移任务中表现出色，在泛化能力和样本效率方面优于最先进方法，同时通过突出显示视觉输入中最相关区域提高了模型的可解释性。&lt;h4&gt;结论&lt;/h4&gt;DeGuV框架成功地解决了强化学习中泛化能力和样本效率之间的权衡问题，通过选择性关注关键视觉特征，实现了更好的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;强化学习代理可以从视觉输入中学习解决复杂任务，但将这些学到的技能泛化到新环境仍然是强化学习应用的主要挑战，特别是在机器人领域。虽然数据增强可以提高泛化能力，但它通常会降低样本效率和训练稳定性。本文介绍了DeGuV，一种增强泛化能力和样本效率的强化学习框架。具体而言，我们利用一个可学习的掩码网络从深度输入生成掩码，只保留关键视觉信息，同时丢弃无关像素。通过这种方式，我们确保强化学习代理关注关键特征，提高数据增强下的鲁棒性。此外，我们结合对比学习，并在增强条件下稳定Q值估计，以进一步提高样本效率和训练稳定性。我们在RL-ViGen基准上使用Franka Emika机器人评估了我们的方法，并证明了其在零样本模拟到现实迁移中的有效性。我们的结果表明，DeGuV在泛化能力和样本效率方面都优于最先进的方法，同时还通过突出显示视觉输入中最相关的区域提高了可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) agents can learn to solve complex tasks fromvisual inputs, but generalizing these learned skills to new environmentsremains a major challenge in RL application, especially robotics. While dataaugmentation can improve generalization, it often compromises sample efficiencyand training stability. This paper introduces DeGuV, an RL framework thatenhances both generalization and sample efficiency. In specific, we leverage alearnable masker network that produces a mask from the depth input, preservingonly critical visual information while discarding irrelevant pixels. Throughthis, we ensure that our RL agents focus on essential features, improvingrobustness under data augmentation. In addition, we incorporate contrastivelearning and stabilize Q-value estimation under augmentation to further enhancesample efficiency and training stability. We evaluate our proposed method onthe RL-ViGen benchmark using the Franka Emika robot and demonstrate itseffectiveness in zero-shot sim-to-real transfer. Our results show that DeGuVoutperforms state-of-the-art methods in both generalization and sampleefficiency while also improving interpretability by highlighting the mostrelevant regions in the visual input</description>
      <author>example@mail.com (Tien Pham, Xinyun Chi, Khang Nguyen, Manfred Huber, Angelo Cangelosi)</author>
      <guid isPermaLink="false">2509.04970v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>PropVG: End-to-End Proposal-Driven Visual Grounding with Multi-Granularity Discrimination</title>
      <link>http://arxiv.org/abs/2509.04833v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PropVG是一种端到端的基于提议的视觉定位框架，整合了前景物体提议生成与参考物体理解，无需额外检测器。它引入了基于对比的参考评分(CRS)模块和多粒度目标区分(MTD)模块，通过对比学习增强物体理解能力，并融合物体级和语义级信息改善缺失目标的识别。实验证明其在多个基准测试上的有效性。&lt;h4&gt;背景&lt;/h4&gt;视觉定位领域最近已从基于提议的传统两阶段框架转向端到端的直接参考范式，因为前者效率低下且计算复杂度高。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉定位方法完全依赖被引用目标进行监督而忽视潜在前瞻性目标的问题，以及未能融入多粒度区分导致在复杂场景中物体识别不稳健的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出PropVG框架，包括：(1)端到端的基于提议的框架，无缝集成前景物体提议生成与参考物体理解；(2)基于对比的参考评分(CRS)模块，在句子和词级别采用对比学习；(3)多粒度目标区分(MTD)模块，融合物体级和语义级信息。&lt;h4&gt;主要发现&lt;/h4&gt;PropVG在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES)等多个基准测试上证明了其有效性，表明所提出的模块设计和框架能够提升视觉定位性能。&lt;h4&gt;结论&lt;/h4&gt;PropVG成功解决了现有视觉定位方法的局限性，通过整合前景物体提议生成与参考物体理解，以及利用对比学习和多粒度区分，显著提升了物体识别能力，特别是在复杂场景中。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉定位进展在很大程度上已经从基于提议的传统两阶段框架转移开来，因为它们效率低下且计算复杂度高，倾向于采用端到端的直接参考范式。然而，这些方法完全依赖于被引用的目标进行监督，忽视了潜在的前瞻性目标的好处。此外，现有方法通常未能融入多粒度区分，这对于复杂场景中稳健的物体识别至关重要。为解决这些局限性，我们提出了PropVG，这是一个端到端的基于提议的框架，据我们所知，它是第一个无缝集成前景物体提议生成与参考物体理解而无需额外检测器的框架。此外，我们引入了基于对比的参考评分(CRS)模块，该模块在句子和词级别采用对比学习，以增强理解和区分被引用物体的能力。另外，我们设计了一个多粒度目标区分(MTD)模块，融合物体级和语义级信息，以改善缺失目标的识别。在gRefCOCO (GREC/GRES)、Ref-ZOM、R-RefCOCO和RefCOCO (REC/RES)基准上的大量实验证明了PropVG的有效性。代码和模型可在https://github.com/Dmmm1997/PropVG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in visual grounding have largely shifted away fromtraditional proposal-based two-stage frameworks due to their inefficiency andhigh computational complexity, favoring end-to-end direct reference paradigms.However, these methods rely exclusively on the referred target for supervision,overlooking the potential benefits of prominent prospective targets. Moreover,existing approaches often fail to incorporate multi-granularity discrimination,which is crucial for robust object identification in complex scenarios. Toaddress these limitations, we propose PropVG, an end-to-end proposal-basedframework that, to the best of our knowledge, is the first to seamlesslyintegrate foreground object proposal generation with referential objectcomprehension without requiring additional detectors. Furthermore, we introducea Contrastive-based Refer Scoring (CRS) module, which employs contrastivelearning at both sentence and word levels to enhance the capability inunderstanding and distinguishing referred objects. Additionally, we design aMulti-granularity Target Discrimination (MTD) module that fuses object- andsemantic-level information to improve the recognition of absent targets.Extensive experiments on gRefCOCO (GREC/GRES), Ref-ZOM, R-RefCOCO, and RefCOCO(REC/RES) benchmarks demonstrate the effectiveness of PropVG. The codes andmodels are available at https://github.com/Dmmm1997/PropVG.</description>
      <author>example@mail.com (Ming Dai, Wenxuan Cheng, Jiedong Zhuang, Jiang-jiang Liu, Hongshen Zhao, Zhenhua Feng, Wankou Yang)</author>
      <guid isPermaLink="false">2509.04833v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</title>
      <link>http://arxiv.org/abs/2509.01910v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种将全球地理定位与概念瓶颈相结合的新框架，通过概念感知对齐模块提高了地理定位模型的准确性和解释性，首次将解释性引入地理定位领域。&lt;h4&gt;背景&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的精确地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管GeoCLIP等地理定位模型有所进步，但这些模型的解释性仍未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决当前基于概念的解释性方法无法有效与地理定位图像-位置对齐目标保持一致的问题，提出一个将全球地理定位与概念瓶颈相结合的新框架。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库上（如热带气候、山脉、大教堂等），并最小化概念级损失，增强概念特定子空间中的对齐，从而实现强大的解释性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在地理定位准确性上超越了GeoCLIP，在多样化的地理空间预测任务中提高了性能，并揭示了地理决策过程更丰富的语义洞察。&lt;h4&gt;结论&lt;/h4&gt;这是首个将解释性引入地理定位的工作，通过概念感知对齐模块实现了更好的性能和解释性。&lt;h4&gt;翻译&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的精确地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管像GeoCLIP这样的地理定位模型有所进步，但这些模型的解释性仍未得到充分探索。当前基于概念的解释性方法无法有效地与地理定位图像-位置对齐目标保持一致，导致次优的解释性和性能。为解决这一差距，我们提出了一种将全球地理定位与概念瓶颈相结合的新框架。我们的方法插入了一个概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库（如热带气候、山脉、大教堂）上，并最小化概念级损失，增强概念特定子空间中的对齐，实现强大的解释性。据我们所知，这是首个将解释性引入地理定位的工作。大量实验证明，我们的方法在地理定位准确性上超越了GeoCLIP，并在多样化的地理空间预测任务中提高了性能，揭示了地理决策过程更丰富的语义洞察。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Worldwide geo-localization involves determining the exact geographic locationof images captured globally, typically guided by geographic cues such asclimate, landmarks, and architectural styles. Despite advancements ingeo-localization models like GeoCLIP, which leverages images and locationalignment via contrastive learning for accurate predictions, theinterpretability of these models remains insufficiently explored. Currentconcept-based interpretability methods fail to align effectively withGeo-alignment image-location embedding objectives, resulting in suboptimalinterpretability and performance. To address this gap, we propose a novelframework integrating global geo-localization with concept bottlenecks. Ourmethod inserts a Concept-Aware Alignment Module that jointly projects image andlocation embeddings onto a shared bank of geographic concepts (e.g., tropicalclimate, mountain, cathedral) and minimizes a concept-level loss, enhancingalignment in a concept-specific subspace and enabling robust interpretability.To our knowledge, this is the first work to introduce interpretability intogeo-localization. Extensive experiments demonstrate that our approach surpassesGeoCLIP in geo-localization accuracy and boosts performance across diversegeospatial prediction tasks, revealing richer semantic insights into geographicdecision-making processes.</description>
      <author>example@mail.com (Furong Jia, Lanxin Liu, Ce Hou, Fan Zhang, Xinyan Liu, Yu Liu)</author>
      <guid isPermaLink="false">2509.01910v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>An Emotion Recognition Framework via Cross-modal Alignment of EEG and Eye Movement Data</title>
      <link>http://arxiv.org/abs/2509.04938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于跨模态注意力机制混合架构的情感识别框架，实现了脑电图和眼动数据的精确多模态对齐，在SEED-IV数据集上达到90.62%的准确率。&lt;h4&gt;背景&lt;/h4&gt;情感识别对于情感计算和行为预测应用至关重要，但依赖单一模态数据的传统系统往往无法捕捉情感状态的复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一个情感识别框架，解决单一模态数据的局限性，实现脑电图(EEG)和眼动数据的精确多模态对齐。&lt;h4&gt;方法&lt;/h4&gt;基于跨模态注意力机制的混合架构，实现EEG和眼动数据的多模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在SEED-IV数据集上的实验表明，该方法达到了90.62%的准确率。&lt;h4&gt;结论&lt;/h4&gt;这项工作为在情感识别中利用多模态数据提供了有前景的基础。&lt;h4&gt;翻译&lt;/h4&gt;情感识别对于情感计算和行为预测应用至关重要，但依赖单一模态数据的传统系统往往无法捕捉情感状态的复杂性。为解决这一局限，我们提出了一种情感识别框架，通过基于跨模态注意力机制的混合架构，实现了脑电图(EEG)和眼动数据的精确多模态对齐。在SEED-IV数据集上的实验表明，我们的方法达到了90.62%的准确率。这项工作为在情感识别中利用多模态数据提供了有前景的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotion recognition is essential for applications in affective computing andbehavioral prediction, but conventional systems relying on single-modality dataoften fail to capture the complexity of affective states. To address thislimitation, we propose an emotion recognition framework that achieves accuratemultimodal alignment of Electroencephalogram (EEG) and eye movement datathrough a hybrid architecture based on cross-modal attention mechanism.Experiments on the SEED-IV dataset demonstrate that our method achieve 90.62%accuracy. This work provides a promising foundation for leveraging multimodaldata in emotion recognition</description>
      <author>example@mail.com (Jianlu Wang, Yanan Wang, Tong Liu)</author>
      <guid isPermaLink="false">2509.04938v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation for Different Sensor Configurations in 3D Object Detection</title>
      <link>http://arxiv.org/abs/2509.04711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了3D物体检测中不同传感器配置之间的领域适应问题，提出了两种技术：下游微调和部分层微调，实验证明联合使用这两种技术优于简单联合训练。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶的最新进展凸显了精确3D物体检测的重要性，LiDAR因其在不同能见度条件下的鲁棒性而发挥核心作用。然而，不同车辆平台采用不同传感器配置，导致模型从一个配置应用到另一个配置时性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决3D物体检测中不同传感器配置之间的领域适应问题，以提高模型在多样化车辆平台上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两种技术：1) 下游微调（多数据集训练后进行数据集特定的微调）；2) 部分层微调（仅更新部分层以改善跨配置泛化能力）。使用在相同地理区域收集的、具有多种传感器配置的配对数据集进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;联合使用下游微调和部分层微调的训练方法，一致性地优于每种配置的简单联合训练，显著提高了模型在不同传感器配置间的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;研究结果为适应3D物体检测模型到多样化车辆平台提供了实用且可扩展的解决方案，有效解决了不同传感器配置间的领域差距问题。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶的最新进展凸显了精确3D物体检测的重要性，LiDAR由于其在不同能见度条件下的鲁棒性而发挥核心作用。然而，不同车辆平台通常采用不同的传感器配置，导致在一个配置上训练的模型应用于另一个配置时，由于点云分布的变化，性能会下降。先前关于3D物体检测的多数据集训练和领域适应工作主要解决了单一LiDAR内的环境领域差距和密度变化；相比之下，不同传感器配置的领域差距在很大程度上尚未探索。在这项工作中，我们解决了3D物体检测中不同传感器配置之间的领域适应问题。我们提出了两种技术：下游微调（多数据集训练后进行数据集特定的微调）和部分层微调（仅更新部分层以改善跨配置泛化能力）。使用在相同地理区域收集的、具有多种传感器配置的配对数据集，我们证明联合使用下游微调和部分层微调的训练一致性地优于每种配置的简单联合训练。我们的研究结果为适应3D物体检测模型到多样化车辆平台提供了实用且可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决不同传感器配置下的3D目标检测领域适应问题。当在一个传感器配置（如RoboTaxi）上训练的模型应用到另一个配置（如RoboBus）时，由于点云分布变化导致性能显著下降。这个问题很重要，因为自动驾驶平台多样化（汽车、公交车、卡车等），每种平台使用不同传感器配置，而现有研究主要关注环境变化，忽略了传感器配置差异导致的领域差异，影响了实际部署中针对特定平台的高性能模型开发。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到不同传感器配置导致点云分布差异，进而影响3D检测性能。发现现有领域适应研究主要关注环境变化，而忽略了传感器配置差异。认识到多数据集训练虽提高泛化能力，但会导致特定配置性能下降。设计方法时借鉴了无监督领域适应(UDA)、半监督领域适应(SSDA)和多数据集训练(MDT)的思想，但针对传感器配置差异进行了调整。还受到大型语言模型微调策略的启发，提出了下游微调(Downstream Fine-tuning)和部分层微调(Partial Layer Fine-tuning)的创新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用多阶段训练策略，先通过联合训练学习跨配置的通用特征，再针对每个特定配置进行专门优化，并在微调过程中只更新对传感器配置变化敏感的层。整体流程：1)构建包含RoboTaxi和RoboBus不同配置的数据集；2)在所有配置数据上进行联合训练学习通用特征；3)针对每个配置进行下游微调；4)在微调中应用部分层微调策略，保持编码器和头部固定，更新主干和颈部层；5)在测试集上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次针对传感器配置差异引起的领域差异进行研究；2)构建统一标注格式的多传感器配置数据集；3)提出下游微调策略，解决多数据集训练导致的性能下降；4)提出部分层微调策略，只更新敏感层提高效率；5)提供全面的消融研究和无监督领域适应实验。相比之前工作，不同之处在于：专注于传感器配置差异而非环境变化；解决了多数据集训练的性能下降问题；提出更高效的部分层微调；提供专门的数据集和评估基准；证明传感器配置差异比其他领域差距更适合通过微调适应。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过结合下游微调和部分层微调的训练策略，有效解决了不同LiDAR传感器配置间的领域适应问题，使3D目标检测模型能够更高效地适应多样化的自动驾驶平台。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in autonomous driving have underscored the importance ofaccurate 3D object detection, with LiDAR playing a central role due to itsrobustness under diverse visibility conditions. However, different vehicleplatforms often deploy distinct sensor configurations, causing performancedegradation when models trained on one configuration are applied to anotherbecause of shifts in the point cloud distribution. Prior work on multi-datasettraining and domain adaptation for 3D object detection has largely addressedenvironmental domain gaps and density variation within a single LiDAR; incontrast, the domain gap for different sensor configurations remains largelyunexplored. In this work, we address domain adaptation across different sensorconfigurations in 3D object detection. We propose two techniques: DownstreamFine-tuning (dataset-specific fine-tuning after multi-dataset training) andPartial Layer Fine-tuning (updating only a subset of layers to improvecross-configuration generalization). Using paired datasets collected in thesame geographic region with multiple sensor configurations, we show that jointtraining with Downstream Fine-tuning and Partial Layer Fine-tuning consistentlyoutperforms naive joint training for each configuration. Our findings provide apractical and scalable solution for adapting 3D object detection models to thediverse vehicle platforms.</description>
      <author>example@mail.com (Satoshi Tanaka, Kok Seang Tan, Isamu Yamashita)</author>
      <guid isPermaLink="false">2509.04711v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>3D-MOOD: Lifting 2D to 3D for Monocular Open-Set Object Detection</title>
      <link>http://arxiv.org/abs/2507.23567v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了3D-MOOD，第一个端到端的3D单目开放集目标检测器，解决了现实应用中新环境和新目标类别的挑战，在多个数据集上取得了最先进结果。&lt;h4&gt;背景&lt;/h4&gt;单目3D目标检测在机器人和AR/VR等领域具有重要价值，但现有方法局限于封闭集设置，即训练集和测试集包含相同场景和目标类别，无法应对现实世界中的新环境和新类别挑战。&lt;h4&gt;目的&lt;/h4&gt;解决开放环境下的单目3D目标检测问题，提出第一个端到端的3D单目开放集目标检测器（3D-MOOD）。&lt;h4&gt;方法&lt;/h4&gt;1) 设计3D边界框头部将开放集2D检测提升到3D空间；2) 实现2D和3D任务的端到端联合训练；3) 使用几何先验条件化目标查询，提高跨场景3D估计泛化能力；4) 设计规范图像空间实现更高效的跨数据集训练。&lt;h4&gt;主要发现&lt;/h4&gt;在封闭集设置（Omni3D）和开放集设置（从Omni3D到Argoverse 2、ScanNet）上评估3D-MOOD，均取得新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;3D-MOOD成功解决了开放环境下的单目3D目标检测问题，通过创新方法设计实现了端到端训练，并在多个数据集上取得突破性成果，代码和模型已公开。&lt;h4&gt;翻译&lt;/h4&gt;单目3D目标检测对于机器人和AR/VR等应用具有重要价值。现有方法局限于封闭集设置，其中训练集和测试集包含相同的场景和/或目标类别。然而，现实应用常常引入新环境和新的目标类别，对这些方法构成了挑战。在本文中，我们解决了开放环境下的单目3D目标检测问题，并引入了第一个端到端的3D单目开放集目标检测器（3D-MOOD）。我们通过设计的3D边界框头部将开放集2D检测提升到3D空间，使2D和3D任务能够进行端到端的联合训练，从而获得更好的整体性能。我们使用几何先验条件化目标查询，克服了跨不同场景的3D估计泛化问题。为了进一步提高性能，我们设计了规范图像空间以实现更高效的跨数据集训练。我们在封闭集设置（Omni3D）和开放集设置（从Omni3D到Argoverse 2、ScanNet）上评估了3D-MOOD，并取得了新的最先进结果。代码和模型可在royyang0714.github.io/3D-MOOD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目开放集3D物体检测问题，即让模型能够识别并定位训练时未见过的物体类别和在未见过的场景中的物体。这个问题很重要，因为现实世界的应用（如机器人、AR/VR）经常需要面对新环境和未知物体，而现有方法受限于封闭集设计，只能检测训练时见过的物体类别，无法适应真实世界的多样性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了开放集单目3D物体检测的两个主要障碍：跨模态学习困难和单目深度估计泛化能力差。他们借鉴了G-DINO作为2D开放集检测器的基础架构，利用了通用单目深度估计方法（如UniDepth）的泛化能力，并参考了Cube R-CNN的虚拟深度概念。作者通过'提升'机制将2D检测转换为3D检测，并设计了几何感知的3D查询生成和规范图像空间来解决跨场景泛化问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'提升'机制将开放集2D检测转换为3D检测，同时利用几何先验增强模型泛化能力。整体流程：1)接收单目图像和语言提示；2)提取图像和文本特征；3)进行早期视觉-语言特征融合；4)生成检测查询；5)通过跨模态解码结合多模态信息；6)进行2D检测；7)使用3D边界框头和几何感知查询生成将2D检测结果'提升'为3D检测；8)输出包含3D位置、尺寸和方向的检测结果；9)通过端到端训练优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首个端到端开放集单目3D物体检测器；2)2D到3D的提升机制；3)几何感知的3D查询生成；4)规范图像空间设计；5)辅助度量深度估计。相比之前工作：不同于封闭集方法（如Cube R-CNN）只能检测已知类别，3D-MOOD能检测未知类别；不同于OVM3D-Det的伪GT方法，3D-MOOD是端到端训练的；不同于传统方法使用类别先验和仅估计偏航角，3D-MOOD直接预测尺寸并使用6D方向参数化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 3D-MOOD首次实现了端到端的开放集单目3D物体检测，通过创新的2D到3D提升机制和几何感知查询生成，使模型能够识别并定位未见过的物体类别，在封闭集和开放集设置下均达到最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection is valuable for various applications such asrobotics and AR/VR. Existing methods are confined to closed-set settings, wherethe training and testing sets consist of the same scenes and/or objectcategories. However, real-world applications often introduce new environmentsand novel object categories, posing a challenge to these methods. In thispaper, we address monocular 3D object detection in an open-set setting andintroduce the first end-to-end 3D Monocular Open-set Object Detector (3D-MOOD).We propose to lift the open-set 2D detection into 3D space through our designed3D bounding box head, enabling end-to-end joint training for both 2D and 3Dtasks to yield better overall performance. We condition the object queries withgeometry prior and overcome the generalization for 3D estimation across diversescenes. To further improve performance, we design the canonical image space formore efficient cross-dataset training. We evaluate 3D-MOOD on both closed-setsettings (Omni3D) and open-set settings (Omni3D to Argoverse 2, ScanNet), andachieve new state-of-the-art results. Code and models are available atroyyang0714.github.io/3D-MOOD.</description>
      <author>example@mail.com (Yung-Hsu Yang, Luigi Piccinelli, Mattia Segu, Siyuan Li, Rui Huang, Yuqian Fu, Marc Pollefeys, Hermann Blum, Zuria Bauer)</author>
      <guid isPermaLink="false">2507.23567v2</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Transfer Learning and Mobile-enabled Convolutional Neural Networks for Improved Arabic Handwritten Character Recognition</title>
      <link>http://arxiv.org/abs/2509.05019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20pages, 9 figures and 11 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了迁移学习与移动端卷积神经网络在阿拉伯手写字符识别中的集成应用，通过评估三种迁移学习策略和四种轻量级网络模型，发现MobileNet表现最佳，完全微调策略效果最好，IFHCDB数据集上达到99%的准确率，为资源高效的阿拉伯手写字符识别提供了新思路。&lt;h4&gt;背景&lt;/h4&gt;阿拉伯手写字符识别面临计算资源需求大和数据集稀缺的挑战，需要更高效的识别方法。&lt;h4&gt;目的&lt;/h4&gt;评估迁移学习与轻量级移动端卷积神经网络的结合效果，提高阿拉伯手写字符识别的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;评估三种迁移学习策略（完全微调、部分微调和从头训练）使用四种轻量级MbNets（MobileNet、SqueezeNet、MnasNet和ShuffleNet），在三个基准数据集（AHCD、HIJJA和IFHCDB）上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;MobileNet是表现最好的模型，在准确性、鲁棒性和效率方面都表现优异；ShuffleNet在泛化能力方面表现突出；IFHCDB数据集上使用MnasNet在完全微调下达到99%的准确率；AHCD数据集上使用ShuffleNet实现97%的准确率；HIJJA数据集挑战大，达到92%的峰值准确率；完全微调在所有指标中表现最佳，平衡了准确性和收敛速度；部分微调在各项指标中表现不佳。&lt;h4&gt;结论&lt;/h4&gt;迁移学习与MbNets的结合为资源高效的阿拉伯手写字符识别提供了潜力，为进一步优化和更广泛的应用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了迁移学习与移动端卷积神经网络的集成，以增强阿拉伯手写字符识别。针对大量计算需求和数据集稀缺等挑战，本研究评估了三种迁移学习策略——完全微调、部分微调和从头训练——使用了四种轻量级MbNets：MobileNet、SqueezeNet、MnasNet和ShuffleNet。实验在三个基准数据集上进行：AHCD、HIJJA和IFHCDB。MobileNet成为表现最佳的模型，在准确性、鲁棒性和效率方面持续取得优异表现，而ShuffleNet在泛化能力方面表现出色，特别是在完全微调的情况下。IFHCDB数据集获得了最佳结果，使用MnasNet在完全微调下达到99%的准确率，突显了其在鲁棒字符识别中的适用性。AHCD数据集使用ShuffleNet实现了97%的竞争性准确率，而HIJJA由于其变化性大带来了显著挑战，使用ShuffleNet达到92%的峰值准确率。值得注意的是，完全微调展示了最佳的整体性能，平衡了准确性和收敛速度，而部分微调在各项指标中表现不佳。这些发现强调了结合迁移学习和MbNets进行资源高效阿拉伯手写字符识别的潜力，为进一步优化和更广泛的应用铺平了道路。未来工作将探索架构修改、深入数据集特征分析、数据增强和高级敏感性分析，以提高模型的鲁棒性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study explores the integration of transfer learning (TL) withmobile-enabled convolutional neural networks (MbNets) to enhance ArabicHandwritten Character Recognition (AHCR). Addressing challenges like extensivecomputational requirements and dataset scarcity, this research evaluates threeTL strategies--full fine-tuning, partial fine-tuning, and training fromscratch--using four lightweight MbNets: MobileNet, SqueezeNet, MnasNet, andShuffleNet. Experiments were conducted on three benchmark datasets: AHCD,HIJJA, and IFHCDB. MobileNet emerged as the top-performing model, consistentlyachieving superior accuracy, robustness, and efficiency, with ShuffleNetexcelling in generalization, particularly under full fine-tuning. The IFHCDBdataset yielded the highest results, with 99% accuracy using MnasNet under fullfine-tuning, highlighting its suitability for robust character recognition. TheAHCD dataset achieved competitive accuracy (97%) with ShuffleNet, while HIJJAposed significant challenges due to its variability, achieving a peak accuracyof 92% with ShuffleNet. Notably, full fine-tuning demonstrated the best overallperformance, balancing accuracy and convergence speed, while partialfine-tuning underperformed across metrics. These findings underscore thepotential of combining TL and MbNets for resource-efficient AHCR, paving theway for further optimizations and broader applications. Future work willexplore architectural modifications, in-depth dataset feature analysis, dataaugmentation, and advanced sensitivity analysis to enhance model robustness andgeneralizability.</description>
      <author>example@mail.com (Mohsine El Khayati, Ayyad Maafiri, Yassine Himeur, Hamzah Ali Alkhazaleh, Shadi Atalla, Wathiq Mansoor)</author>
      <guid isPermaLink="false">2509.05019v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Deep Transfer Learning for Breast Ultrasound Cancer Detection: A Multi-Dataset Study</title>
      <link>http://arxiv.org/abs/2509.05004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 figures and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了多种机器学习和深度学习模型在乳腺癌超声图像分类中的应用，发现ResNet-18表现最佳，支持AI诊断工具在临床实践中的应用。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是全球女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益而被广泛使用，在早期检测中起着关键作用，特别是在致密乳腺组织患者中。&lt;h4&gt;目的&lt;/h4&gt;全面研究机器学习和深度学习技术在乳腺癌超声图像分类中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，评估经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet-18达到最高的准确率（99.7%）和完美的恶性肿瘤敏感性；经典机器学习模型结合深度特征提取时也能达到有竞争力的性能；Grad-CAM可视化通过突出显示诊断相关的图像区域提高了模型透明度。&lt;h4&gt;结论&lt;/h4&gt;研究结果支持将基于人工智能的诊断工具整合到临床工作流程中，证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。&lt;h4&gt;翻译&lt;/h4&gt;乳腺癌仍然是全球女性癌症相关死亡的主要原因。超声成像因其安全性和成本效益而被广泛使用，在早期检测中起着关键作用，特别是在致密乳腺组织患者中。本文全面研究了机器学习和深度学习技术在乳腺癌超声图像分类中的应用。使用BUSI、BUS-BRA和BrEaST-Lesions USG等数据集，我们评估了经典机器学习模型（SVM、KNN）和深度卷积神经网络（ResNet-18、EfficientNet-B0、GoogLeNet）。实验结果表明，ResNet-18达到了最高的准确率（99.7%）和完美的恶性肿瘤敏感性。经典机器学习模型虽然被CNN超越，但当结合深度特征提取时也能达到有竞争力的性能。Grad-CAM可视化通过突出显示诊断相关的图像区域进一步提高了模型透明度。这些研究结果支持将基于人工智能的诊断工具整合到临床工作流程中，并证明了部署高性能、可解释的超声乳腺癌检测系统的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer remains a leading cause of cancer-related mortality among womenworldwide. Ultrasound imaging, widely used due to its safety andcost-effectiveness, plays a key role in early detection, especially in patientswith dense breast tissue. This paper presents a comprehensive study on theapplication of machine learning and deep learning techniques for breast cancerclassification using ultrasound images. Using datasets such as BUSI, BUS-BRA,and BrEaST-Lesions USG, we evaluate classical machine learning models (SVM,KNN) and deep convolutional neural networks (ResNet-18, EfficientNet-B0,GoogLeNet). Experimental results show that ResNet-18 achieves the highestaccuracy (99.7%) and perfect sensitivity for malignant lesions. Classical MLmodels, though outperformed by CNNs, achieve competitive performance whenenhanced with deep feature extraction. Grad-CAM visualizations further improvemodel transparency by highlighting diagnostically relevant image regions. Thesefindings support the integration of AI-based diagnostic tools into clinicalworkflows and demonstrate the feasibility of deploying high-performing,interpretable systems for ultrasound-based breast cancer detection.</description>
      <author>example@mail.com (Mohammad Abbadi, Yassine Himeur, Shadi Atalla, Wathiq Mansoor)</author>
      <guid isPermaLink="false">2509.05004v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>SpiderNets: Estimating Fear Ratings of Spider-Related Images with Vision Models</title>
      <link>http://arxiv.org/abs/2509.04889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  60 pages (30 main text, 30 appendix), 20 figures (5 in main text, 15  in appendix)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了计算机视觉模型在预测蜘蛛相关图像恐惧水平方面的应用。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉的进步为临床应用开辟了新途径，特别是在计算机暴露疗法中，可以根据患者反应动态调整视觉刺激。&lt;h4&gt;目的&lt;/h4&gt;研究预训练的计算机视觉模型是否能准确预测与蜘蛛相关图像的恐惧水平。&lt;h4&gt;方法&lt;/h4&gt;使用迁移学习调整三种不同的模型，从一个包含313张图像的标准数据集中预测人类的恐惧评分（0-100分制），并通过交叉验证进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;模型平均绝对误差在10.1到11.0之间；减少数据集规模会显著损害性能，但进一步增加数据量不会带来实质性收益；模型预测基于蜘蛛相关特征；远景和人工/绘制的蜘蛛与较高误差相关。&lt;h4&gt;结论&lt;/h4&gt;可解释计算机视觉模型在预测恐惧评分方面具有潜力，模型可解释性和足够的数据集规模对于开发有效的情感感知治疗技术至关重要。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉的进步为临床应用开辟了新途径，特别是在计算机暴露疗法中，可以根据患者反应动态调整视觉刺激。作为开发此类自适应系统的重要一步，我们研究了预训练的计算机视觉模型是否能准确预测与蜘蛛相关图像的恐惧水平。我们使用迁移学习调整了三种不同的模型，从一个包含313张图像的标准数据集中预测人类的恐惧评分（0-100分制）。模型通过交叉验证进行评估，平均绝对误差在10.1到11.0之间。我们的学习曲线分析显示，减少数据集规模会显著损害性能，但进一步增加数据量不会带来实质性收益。可解释性评估表明，模型的预测是基于蜘蛛相关特征的。分类错误分析进一步确定了与较高误差相关的视觉条件（例如，远景和人工/绘制的蜘蛛）。这些发现展示了可解释计算机视觉模型在预测恐惧评分方面的潜力，强调了模型可解释性和足够数据集规模对于开发有效的情感感知治疗技术的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advances in computer vision have opened new avenues for clinicalapplications, particularly in computerized exposure therapy where visualstimuli can be dynamically adjusted based on patient responses. As a criticalstep toward such adaptive systems, we investigated whether pretrained computervision models can accurately predict fear levels from spider-related images. Weadapted three diverse models using transfer learning to predict human fearratings (on a 0-100 scale) from a standardized dataset of 313 images. Themodels were evaluated using cross-validation, achieving an average meanabsolute error (MAE) between 10.1 and 11.0. Our learning curve analysisrevealed that reducing the dataset size significantly harmed performance,though further increases yielded no substantial gains. Explainabilityassessments showed the models' predictions were based on spider-relatedfeatures. A category-wise error analysis further identified visual conditionsassociated with higher errors (e.g., distant views and artificial/paintedspiders). These findings demonstrate the potential of explainable computervision models in predicting fear ratings, highlighting the importance of bothmodel explainability and a sufficient dataset size for developing effectiveemotion-aware therapeutic technologies.</description>
      <author>example@mail.com (Dominik Pegler, David Steyrl, Mengfan Zhang, Alexander Karner, Jozsef Arato, Frank Scharnowski, Filip Melinscak)</author>
      <guid isPermaLink="false">2509.04889v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing 3D Point Cloud Classification with ModelNet-R and Point-SkipNet</title>
      <link>http://arxiv.org/abs/2509.05198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for presentation at the 7th  International Conference on Pattern Recognition and Image Analysis (IPRIA  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了ModelNet-R（改进版ModelNet40数据集）和Point-SkipNet（轻量级基于图的神经网络），用于解决3D点云分类问题，强调了高质量数据集对模型效率的重要性。&lt;h4&gt;背景&lt;/h4&gt;3D点云分类对自动驾驶、机器人和增强现实等应用至关重要，但常用的ModelNet40数据集存在标签不一致、二维数据、尺寸不匹配和类别区分不足等局限性，限制了模型性能。&lt;h4&gt;目的&lt;/h4&gt;创建更可靠的基准数据集ModelNet-R以解决ModelNet40的局限性，并提出Point-SkipNet神经网络实现高分类精度同时减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;通过改进ModelNet40创建ModelNet-R数据集，并设计Point-SkipNet神经网络，该网络利用高效采样、邻域分组和跳跃连接技术。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet-R上训练的模型表现出显著性能提升；Point-SkipNet在ModelNet-R上实现最先进准确性，同时参数数量大幅减少；数据集质量对优化3D点云分类模型效率至关重要。&lt;h4&gt;结论&lt;/h4&gt;高质量数据集和精心设计的网络架构能显著提高3D点云分类的性能和效率，为相关领域提供更可靠的基准和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;3D点云分类对于自动驾驶、机器人和增强现实等应用至关重要。然而，常用的ModelNet40数据集存在标签不一致、二维数据、尺寸不匹配和类别区分不足等局限性，这些限制阻碍了模型性能。本文介绍了ModelNet-R，这是ModelNet40的精心改进版本，旨在解决这些问题并作为更可靠的基准。此外，本文提出了Point-SkipNet，一种轻量级基于图的神经网络，它利用高效采样、邻域分组和跳跃连接来实现高分类精度同时减少计算开销。大量实验证明，在ModelNet-R上训练的模型表现出显著的性能提升。值得注意的是，Point-SkipNet在ModelNet-R上实现了最先进的准确性，同时与当代模型相比参数数量大幅减少。这项研究强调了数据集质量在优化3D点云分类模型效率方面的关键作用。更多详情，请参阅代码：https://github.com/m-saeid/ModeNetR_PointSkipNet。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：一是常用的ModelNet40数据集存在标签不一致、包含2D数据、尺寸不匹配和类别区分度不足等质量问题；二是现有的点云分类模型计算量大，不适合资源受限环境。这些问题很重要，因为数据集质量直接影响模型性能和评估可靠性，而计算效率限制模型在实际应用中的部署，而3D点云分类在自动驾驶、机器人和增强现实等领域有广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出3D点云分类领域的两个主要问题：数据集质量和模型效率。针对数据集问题，作者决定改进ModelNet数据集创建ModelNet-R；针对效率问题，设计了轻量级的Point-SkipNet。作者借鉴了PointNet和PointNet++的分层特征提取策略，DGCNN的动态图构建方法，以及FPS采样和ball query邻域分组等现有技术，但进行了改进以提高效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1) 创建高质量的ModelNet-R数据集，通过修正标签、移除低质量样本和改进类别定义提高数据可靠性；2) 设计Point-SkipNet轻量级图神经网络，通过高效采样、邻域分组和跳跃连接实现高精度低计算开销。整体流程：ModelNet-R创建包括修正标签、移除2D数据、解决尺寸不匹配和改进类别区分；Point-SkipNet实现包括数据增强、采样分组、特征提取、跳跃连接、全局特征聚合和分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) ModelNet-R数据集解决原始数据集的质量问题；2) Point-SkipNet轻量级图神经网络架构；3) 实现高精度与低计算开销的平衡。不同之处：不同于以往仅扩展数据集，作者专注于改进现有数据集质量；Point-SkipNet结合高效采样和跳跃连接大幅减少计算量；在保持精度的同时参数数量远少于当代模型；强调了数据集质量对模型效率的重要性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建高质量的ModelNet-R数据集和设计轻量级的Point-SkipNet网络，显著提高了3D点云分类的准确性和计算效率，为资源受限环境中的应用提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The classification of 3D point clouds is crucial for applications such asautonomous driving, robotics, and augmented reality. However, the commonly usedModelNet40 dataset suffers from limitations such as inconsistent labeling, 2Ddata, size mismatches, and inadequate class differentiation, which hinder modelperformance. This paper introduces ModelNet-R, a meticulously refined versionof ModelNet40 designed to address these issues and serve as a more reliablebenchmark. Additionally, this paper proposes Point-SkipNet, a lightweightgraph-based neural network that leverages efficient sampling, neighborhoodgrouping, and skip connections to achieve high classification accuracy withreduced computational overhead. Extensive experiments demonstrate that modelstrained in ModelNet-R exhibit significant performance improvements. Notably,Point-SkipNet achieves state-of-the-art accuracy on ModelNet-R with asubstantially lower parameter count compared to contemporary models. Thisresearch highlights the crucial role of dataset quality in optimizing modelefficiency for 3D point cloud classification. For more details, see the codeat: https://github.com/m-saeid/ModeNetR_PointSkipNet.</description>
      <author>example@mail.com (Mohammad Saeid, Amir Salarpour, Pedram MohajerAnsari)</author>
      <guid isPermaLink="false">2509.05198v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>Planning from Point Clouds over Continuous Actions for Multi-object Rearrangement</title>
      <link>http://arxiv.org/abs/2509.04645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Conference on Robot Learning (CoRL) 2025  (https://planning-from-point-clouds.github.io/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SPOT的混合学习与规划方法，用于解决机器人操作中的长期规划问题，通过在点云空间中搜索变换序列，避免了传统方法中对连续状态和动作空间的离散化需求。&lt;h4&gt;背景&lt;/h4&gt;机器人操作的长期规划是一个具有挑战性的问题，需要推理一系列动作对物理3D场景的影响。传统任务规划方法虽然有效，但需要将连续的状态和动作空间离散化为对象、对象关系和动作的符号描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理高维连续动作空间的规划方法，避免对状态和动作空间进行离散化，同时保持规划的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出SPOT（Search over Point cloud Object Transformations）方法，通过搜索从初始场景点云到满足目标的点云的变换序列来进行规划。SPOT从在部分观测点云上运行的学习型建议器中采样候选动作，消除了离散化动作或对象关系的需要。&lt;h4&gt;主要发现&lt;/h4&gt;在多对象重排任务上评估SPOT，实验表明SPOT能够生成成功的计划，并且优于策略学习方法。消融实验强调了基于搜索的规划的重要性。&lt;h4&gt;结论&lt;/h4&gt;SPOT作为一种混合学习与规划的方法，有效解决了长期机器人操作规划问题，在模拟和真实环境中都表现出色，证明了结合学习模型与搜索规划的有效性。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作的长期规划是一个具有挑战性的问题，需要推理一系列动作对物理3D场景的影响。虽然传统的任务规划方法被证明对长期操作有效，但它们需要将连续的状态和动作空间离散化为对象、对象关系和动作的符号描述。相反，我们提出了一种混合学习和规划的方法，利用学习到的模型作为领域特定的先验知识，在高维连续动作空间中引导搜索。我们介绍了SPOT：点云对象变换搜索，它通过搜索从初始场景点云到满足目标的点云的变换序列来进行规划。SPOT从在部分观测点云上运行的学习型建议器中采样候选动作，消除了离散化动作或对象关系的需要。我们在多对象重排任务上评估了SPOT，报告了在模拟和真实环境中的任务规划成功率和任务执行成功率。我们的实验表明，SPOT能够生成成功的计划，并且优于一种策略学习方法。我们还进行了消融实验，强调了基于搜索的规划的重要性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作中的长期规划问题，特别是在多物体重新排列任务中，机器人需要推理一系列动作对3D物理场景的影响，将物体从初始配置移动到满足目标条件的配置。这个问题在现实中很重要，因为它涉及机器人如何处理复杂的物理操作任务，如餐桌收拾、物体装箱等；在研究中也很重要，因为它突破了传统规划方法需要离散化连续状态和动作空间的限制，使机器人能够更自然地处理现实世界中的连续操作问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统符号规划方法的局限性，即需要离散化连续状态和动作空间，以及对场景的完全知识需求。然后提出了一种混合学习和规划的方法，利用学习的模型作为领域特定先验来指导搜索。作者借鉴了A*搜索算法，但将其应用于连续状态和动作空间；利用了学习的物体建议器和放置建议器来指导搜索；使用了模型偏差估计器来避免不太可能的转换；并参考了TAXPose-D方法来实现相对放置任务。整体设计考虑了直接从点云观测进行规划，而不需要符号表示或潜在空间表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接从点云观测进行规划，通过搜索物体变换序列来找到满足目标条件的点云配置，利用学习的模型作为领域特定先验来指导在高维连续动作空间中的搜索。整体流程包括：1)接收初始场景的部分观测点云、物体名称和目标函数；2)将输入点云分割为有限个物体；3)使用A*搜索算法在物体变换空间中搜索，通过学习的物体建议器和放置建议器采样候选动作；4)使用成本函数和启发式函数评估和引导搜索；5)生成物体变换序列的计划；6)机器人执行计划，使用抓取检测器确定抓取姿态；7)利用模型偏差估计器预测计划动作与实际执行之间的偏差。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出新的规划范式，不需要离散化，通过A*搜索原始3D观测的3D变换进行规划；2)学习物体建议器和放置建议器从视频演示中学习；3)结合搜索基础规划和学习，利用学习到的领域特定先验；4)集成模型偏差估计器引导搜索。相比之前的工作，不同之处在于：不需要离散化连续状态和动作空间；不需要符号场景描述；不在潜在空间中规划；不假设访问技能库或谓词库；不假设离散的物体关系集合；专注于多物体重新排列的规划而非单物体姿态重新配置。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SPOT是一种混合学习和规划方法，它通过直接从点云观测进行搜索，利用学习的领域特定先验来指导连续动作空间中的多物体重新排列规划，避免了传统方法中对状态和动作空间离散化的需求。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon planning for robot manipulation is a challenging problem thatrequires reasoning about the effects of a sequence of actions on a physical 3Dscene. While traditional task planning methods are shown to be effective forlong-horizon manipulation, they require discretizing the continuous state andaction space into symbolic descriptions of objects, object relationships, andactions. Instead, we propose a hybrid learning-and-planning approach thatleverages learned models as domain-specific priors to guide search inhigh-dimensional continuous action spaces. We introduce SPOT: Search over Pointcloud Object Transformations, which plans by searching for a sequence oftransformations from an initial scene point cloud to a goal-satisfying pointcloud. SPOT samples candidate actions from learned suggesters that operate onpartially observed point clouds, eliminating the need to discretize actions orobject relationships. We evaluate SPOT on multi-object rearrangement tasks,reporting task planning success and task execution success in both simulationand real-world environments. Our experiments show that SPOT generatessuccessful plans and outperforms a policy-learning approach. We also performablations that highlight the importance of search-based planning.</description>
      <author>example@mail.com (Kallol Saha, Amber Li, Angela Rodriguez-Izquierdo, Lifan Yu, Ben Eisner, Maxim Likhachev, David Held)</author>
      <guid isPermaLink="false">2509.04645v1</guid>
      <pubDate>Mon, 08 Sep 2025 14:46:56 +0800</pubDate>
    </item>
    <item>
      <title>ChronoGraph: A Real-World Graph-Based Multivariate Time Series Dataset</title>
      <link>http://arxiv.org/abs/2509.04449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ChronoGraph是一个基于真实生产微服务构建的图结构多元时间序列预测数据集，包含服务节点和依赖边，提供系统性能指标和异常标签&lt;h4&gt;背景&lt;/h4&gt;现有时间序列预测基准多来自工业控制系统、交通或空气质量领域，缺乏结合多元时间序列、依赖图结构和真实异常标签的微服务数据集&lt;h4&gt;目的&lt;/h4&gt;提供一个真实的基准数据集，用于研究微服务系统中结构感知预测和事件感知评估&lt;h4&gt;方法&lt;/h4&gt;构建图结构数据集，节点代表微服务并发出多元性能指标流，边表示服务依赖，同时提供专家标注的事故窗口作为异常标签&lt;h4&gt;主要发现&lt;/h4&gt;ChronoGraph独特地结合了多元时间序列、明确的机器可读依赖图和与真实事件对齐的异常标签，为微服务系统研究提供了更全面的基准&lt;h4&gt;结论&lt;/h4&gt;ChronoGraph为研究结构感知预测和事件感知评估提供了现实基准，报告了预测模型、预训练时间序列基础模型和标准异常检测器的基线结果&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ChronoGraph，一个基于真实生产微服务构建的图结构多元时间序列预测数据集。每个节点是一个发出系统级性能指标多元流的服务，捕获CPU、内存和网络使用模式，而有向边编码服务之间的依赖关系。主要任务是预测这些信号在服务级别的未来值。此外，ChronoGraph提供专家注释的事故窗口作为异常标签，使能够评估异常检测方法并在操作中断期间评估预测鲁棒性。与来自工业控制系统或交通和空气质量领域的现有基准相比，ChronoGraph独特地结合了(i)多元时间序列，(ii)明确的、机器可读的依赖图，以及(iii)与真实事件对齐的异常标签。我们报告了涵盖预测模型、预训练时间序列基础模型和标准异常检测器的基线结果。ChronoGraph为研究微服务系统中的结构感知预测和事件感知评估提供了现实基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present ChronoGraph, a graph-structured multivariate time seriesforecasting dataset built from real-world production microservices. Each nodeis a service that emits a multivariate stream of system-level performancemetrics, capturing CPU, memory, and network usage patterns, while directededges encode dependencies between services. The primary task is forecastingfuture values of these signals at the service level. In addition, ChronoGraphprovides expert-annotated incident windows as anomaly labels, enablingevaluation of anomaly detection methods and assessment of forecast robustnessduring operational disruptions. Compared to existing benchmarks from industrialcontrol systems or traffic and air-quality domains, ChronoGraph uniquelycombines (i) multivariate time series, (ii) an explicit, machine-readabledependency graph, and (iii) anomaly labels aligned with real incidents. Wereport baseline results spanning forecasting models, pretrained time-seriesfoundation models, and standard anomaly detectors. ChronoGraph offers arealistic benchmark for studying structure-aware forecasting and incident-awareevaluation in microservice systems.</description>
      <author>example@mail.com (Adrian Catalin Lutu, Ioana Pintilie, Elena Burceanu, Andrei Manolache)</author>
      <guid isPermaLink="false">2509.04449v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
  <item>
      <title>IPA: An Information-Preserving Input Projection Framework for Efficient Foundation Model Adaptation</title>
      <link>http://arxiv.org/abs/2509.04398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为IPA的特征感知投影框架，用于改进参数高效微调方法LoRA的性能，通过解决LoRA随机初始化下投影导致的信息丢失问题。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)方法如LoRA通过向预训练权重注入低秩更新来降低适应成本，但LoRA的下投影是随机初始化且与数据无关，丢弃了可能有用的信息。分析表明下投影在训练中变化很小，而上投影携带大部分适应信息，使随机输入压缩成为性能瓶颈。&lt;h4&gt;目的&lt;/h4&gt;提出IPA框架，明确保留简化隐藏空间中的有用信息，解决LoRA方法中随机下投影导致的信息丢失问题。&lt;h4&gt;方法&lt;/h4&gt;在线性情况下，使用近似主成分的算法实现IPA，使投影器预训练高效且推理开销可以忽略不计。&lt;h4&gt;主要发现&lt;/h4&gt;在语言和视觉基准测试中，IPA持续优于LoRA和DoRA，在常识推理上平均提高1.5个准确率点，在VTAB-1k上提高2.3个准确率点。当投影被冻结时，使用大约一半的可训练参数即可匹配完整LoRA的性能。&lt;h4&gt;结论&lt;/h4&gt;IPA是一种有效的改进方法，能够在保持或提高性能的同时减少可训练参数，解决了LoRA方法中的信息丢失瓶颈问题。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)方法，如LoRA，通过向预训练权重注入低秩更新来降低适应成本。然而，LoRA的下投影是随机初始化且与数据无关，丢弃了可能有用的信息。先前的分析表明，这个投影在训练过程中变化很小，而上投影携带了大部分适应信息，使随机输入压缩成为性能瓶颈。我们提出了IPA，一种特征感知的投影框架，明确保留简化隐藏空间中的信息。在线性情况下，我们使用近似主成分的算法实现IPA，使投影器预训练高效且推理开销可以忽略不计。在语言和视觉基准测试中，IPA持续优于LoRA和DoRA，在常识推理上平均提高1.5个准确率点，在VTAB-1k上提高2.3个准确率点，当投影被冻结时，使用大约一半的可训练参数即可匹配完整LoRA的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, reduceadaptation cost by injecting low-rank updates into pretrained weights. However,LoRA's down-projection is randomly initialized and data-agnostic, discardingpotentially useful information. Prior analyses show that this projectionchanges little during training, while the up-projection carries most of theadaptation, making the random input compression a performance bottleneck. Wepropose IPA, a feature-aware projection framework that explicitly preservesinformation in the reduced hidden space. In the linear case, we instantiate IPAwith algorithms approximating top principal components, enabling efficientprojector pretraining with negligible inference overhead. Across language andvision benchmarks, IPA consistently improves over LoRA and DoRA, achieving onaverage 1.5 points higher accuracy on commonsense reasoning and 2.3 points onVTAB-1k, while matching full LoRA performance with roughly half the trainableparameters when the projection is frozen.</description>
      <author>example@mail.com (Yuan Yin, Shashanka Venkataramanan, Tuan-Hung Vu, Andrei Bursuc, Matthieu Cord)</author>
      <guid isPermaLink="false">2509.04398v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>OVGrasp: Open-Vocabulary Grasping Assistance via Multimodal Intent Detection</title>
      <link>http://arxiv.org/abs/2509.04324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OVGrasp是一个基于软外骨骼的抓取辅助分层控制框架，通过整合RGB-D视觉、开放词汇提示和语音命令实现多模态交互，使用视觉语言基础模型实现零样本检测未见物体，并通过多模态决策者融合空间和语言线索推断用户意图，实验表明其达到87.00%的抓取能力分数，优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;抓取辅助对于运动障碍人士恢复自主性至关重要，特别是在物体类别和用户意图多样且不可预测的非结构化环境中。&lt;h4&gt;目的&lt;/h4&gt;提出OVGrasp框架，用于基于软外骨骼的抓取辅助，以增强在开放环境中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;OVGrasp是一个分层控制框架，集成了RGB-D视觉、开放词汇提示和语音命令；采用具有开放词汇机制的视觉语言基础模型实现零样本检测；通过多模态决策者融合空间和语言线索来推断用户意图；在自为中心视角的可穿戴外骨骼上部署完整框架，并在三种抓取类型下的15个物体上进行系统评估。&lt;h4&gt;主要发现&lt;/h4&gt;10名参与者的实验表明，OVGrasp达到87.00%的抓取能力分数(GAS)，优于最先进的基线，并实现了与自然手部运动更好的运动对齐。&lt;h4&gt;结论&lt;/h4&gt;OVGrasp能够有效地帮助运动障碍人士在非结构化环境中进行抓取任务，恢复其自主性。&lt;h4&gt;翻译&lt;/h4&gt;抓取辅助对于恢复运动障碍人士的自主性至关重要，特别是在物体类别和用户意图多样且不可预测的非结构化环境中。我们提出了OVGrasp，一个基于软外骨骼的抓取辅助分层控制框架，整合了RGB-D视觉、开放词汇提示和语音命令，以实现强大的多模态交互。为了增强开放环境中的泛化能力，OVGrasp采用了具有开放词汇机制的视觉语言基础模型，允许在不重新训练的情况下对未见过的物体进行零样本检测。多模态决策者进一步融合空间和语言线索，在多物体场景中推断用户意图，如抓取或释放。我们在自定制为中心视角的可穿戴外骨骼上部署了完整框架，并在三种抓取类型下的15个物体上进行了系统评估。十名参与者的实验结果表明，OVGrasp实现了87.00%的抓取能力分数(GAS)，优于最先进的基线，并实现了与自然手部运动更好的运动对齐。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasping assistance is essential for restoring autonomy in individuals withmotor impairments, particularly in unstructured environments where objectcategories and user intentions are diverse and unpredictable. We presentOVGrasp, a hierarchical control framework for soft exoskeleton-based graspassistance that integrates RGB-D vision, open-vocabulary prompts, and voicecommands to enable robust multimodal interaction. To enhance generalization inopen environments, OVGrasp incorporates a vision-language foundation model withan open-vocabulary mechanism, allowing zero-shot detection of previously unseenobjects without retraining. A multimodal decision-maker further fuses spatialand linguistic cues to infer user intent, such as grasp or release, inmulti-object scenarios. We deploy the complete framework on a customegocentric-view wearable exoskeleton and conduct systematic evaluations on 15objects across three grasp types. Experimental results with ten participantsdemonstrate that OVGrasp achieves a grasping ability score (GAS) of 87.00%,outperforming state-of-the-art baselines and achieving improved kinematicalignment with natural hand motion.</description>
      <author>example@mail.com (Chen Hu, Shan Luo, Letizia Gionfrida)</author>
      <guid isPermaLink="false">2509.04324v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>RL's Razor: Why Online Reinforcement Learning Forgets Less</title>
      <link>http://arxiv.org/abs/2509.04259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了通过强化学习(RL)和监督微调(SFT)微调模型的差异，发现RL在保持先验知识方面优于SFT，尽管两者在新任务上表现相似。&lt;h4&gt;背景&lt;/h4&gt;模型微调过程中可能会遗忘原有知识，这种现象在机器学习中被称为'灾难性遗忘'。&lt;h4&gt;目的&lt;/h4&gt;探究不同微调方法(RL vs SFT)对模型保留先验知识能力的影响机制。&lt;h4&gt;方法&lt;/h4&gt;通过比较RL和SFT两种微调方法，分析KL散度作为分布偏移的度量标准，并通过大型语言模型和机器人基础模型进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;RL微调倾向于选择KL最小的解决方案，而SFT可能收敛到与基础模型相差很远的分布；遗忘程度由KL散度决定；在线RL更新导致KL变化更小。&lt;h4&gt;结论&lt;/h4&gt;RL在新任务性能相似的情况下能更好地保留先验知识，这种现象可以用'RL's Razor'原则解释：RL倾向于选择与原始模型KL距离最近的任务解决方法。&lt;h4&gt;翻译&lt;/h4&gt;通过将强化学习(RL)模型与监督微调(SFT)模型进行比较，我们发现，尽管在新任务上性能相似，但RL能显著更好地保留先验知识和能力。我们发现遗忘程度由分布偏移决定，通过在新任务上评估的微调后策略与基础策略之间的KL散度来衡量。我们的分析表明，在线RL隐式地偏向于选择新任务解决方案中KL最小的解，而SFT可能收敛到与基础模型任意距离的分布。我们通过大型语言模型和机器人基础模型的实验验证了这些发现，并进一步提供了理论解释，说明为什么在线RL更新会导致更小的KL变化。我们将这一原则称为'RL's Razor'：在解决新任务的所有方法中，RL倾向于选择与原始模型KL距离最近的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comparison of fine-tuning models with reinforcement learning (RL) andsupervised fine-tuning (SFT) reveals that, despite similar performance at a newtask, RL preserves prior knowledge and capabilities significantly better. Wefind that the degree of forgetting is determined by the distributional shift,measured as the KL-divergence between the fine-tuned and base policy evaluatedon the new task. Our analysis reveals that on-policy RL is implicitly biasedtowards KL-minimal solutions among the many that solve the new task, whereasSFT can converge to distributions arbitrarily far from the base model. Wevalidate these findings through experiments with large language models androbotic foundation models and further provide theoretical justification for whyon-policy RL updates lead to a smaller KL change. We term this principle$\textit{RL's Razor}$: among all ways to solve a new task, RL prefers thoseclosest in KL to the original model.</description>
      <author>example@mail.com (Idan Shenfeld, Jyothish Pari, Pulkit Agrawal)</author>
      <guid isPermaLink="false">2509.04259v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Sailing Towards Zero-Shot State Estimation using Foundation Models Combined with a UKF</title>
      <link>http://arxiv.org/abs/2509.04213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at CDC2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为FM-UKF的基础模型无迹卡尔曼滤波器，结合基于transformer的系统动力学模型与分析已知的传感器模型，实现了零样本状态估计，能够在不重新训练的情况下适应不同传感器配置。&lt;h4&gt;背景&lt;/h4&gt;在控制和系统工程中，状态估计传统上需要大量手动系统识别或数据收集工作。基于transformer的基础模型在其他领域已通过预训练通用模型减少数据需求，但现有端到端方法仅限于训练期间见过的传感器模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种零样本状态估计方法，能够处理未见过的系统动力学，同时适应不同传感器配置而不需重新训练。&lt;h4&gt;方法&lt;/h4&gt;提出FM-UKF，将基于transformer的系统动力学模型与分析已知的传感器模型通过UKF结合，实现跨不同动力学系统的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在具有复杂动力学的集装箱船舶模型新基准上评估，FM-UKF与经典方法和端到端方法相比，在准确性、工作量和鲁棒性方面具有竞争优势。&lt;h4&gt;结论&lt;/h4&gt;FM-UKF有效结合基础模型和已知传感器模型实现零样本状态估计，相关基准和数据集已开源以支持未来研究。&lt;h4&gt;翻译&lt;/h4&gt;控制和系统工程中的状态估计传统上需要大量的手动系统识别或数据收集工作。然而，其他领域中基于transformer的基础模型通过利用预训练的通用模型减少了数据需求。最终，开发系统动力学的零样本基础模型可以显著减少手动部署工作。虽然最近的研究表明，基于transformer的端到端方法可以在未见过的系统上实现零样本性能，但它们仅限于训练期间见过的传感器模型。我们引入了基础模型无迹卡尔曼滤波器（FM-UKF），它将基于transformer的系统动力学模型与分析已知的传感器模型通过UKF结合，使得能够在不重新训练新传感器配置的情况下推广到变化的动力学系统。我们在具有复杂动力学的集装箱船舶模型新基准上评估了FM-UKF，与具有近似系统知识的经典方法和端到端方法相比，展示了竞争力的准确性、工作量和鲁棒性权衡。该基准和数据集已开源，以进一步支持通过基础模型进行零样本状态估计的未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State estimation in control and systems engineering traditionally requiresextensive manual system identification or data-collection effort. However,transformer-based foundation models in other domains have reduced datarequirements by leveraging pre-trained generalist models. Ultimately,developing zero-shot foundation models of system dynamics could drasticallyreduce manual deployment effort. While recent work shows that transformer-basedend-to-end approaches can achieve zero-shot performance on unseen systems, theyare limited to sensor models seen during training. We introduce the foundationmodel unscented Kalman filter (FM-UKF), which combines a transformer-basedmodel of system dynamics with analytically known sensor models via an UKF,enabling generalization across varying dynamics without retraining for newsensor configurations. We evaluate FM-UKF on a new benchmark of container shipmodels with complex dynamics, demonstrating a competitive accuracy, effort, androbustness trade-off compared to classical methods with approximate systemknowledge and to an end-to-end approach. The benchmark and dataset are opensourced to further support future research in zero-shot state estimation viafoundation models.</description>
      <author>example@mail.com (Tobin Holtmann, David Stenger, Andres Posada-Moreno, Friedrich Solowjow, Sebastian Trimpe)</author>
      <guid isPermaLink="false">2509.04213v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>One-Embedding-Fits-All: Efficient Zero-Shot Time Series Forecasting by a Model Zoo</title>
      <link>http://arxiv.org/abs/2509.04208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZooCast是一种智能组合时间序列基础模型(TSFMs)的框架，通过构建统一表示空间实现动态模型选择，在零样本预测任务中表现出色同时保持高效性。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)的快速发展显著推动了零样本预测能力，使模型能够在无需任务特定微调的情况下预测未见的时间序列。研究表明，没有单一TSFM能够普遍优于其他模型，不同模型表现出对不同时间模式的偏好。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用不同TSFM的互补能力，构建一个能够智能组合当前TSFMs并根据任务特点选择最优模型的框架。&lt;h4&gt;方法&lt;/h4&gt;提出ZooCast方法，表征每个模型的独特预测优势；采用'One-Embedding-Fits-All'范式构建统一表示空间，使每个模型由单个嵌入表示，实现高效相似性匹配；建立动态模型选择机制，根据不同预测任务选择最优模型。&lt;h4&gt;主要发现&lt;/h4&gt;ZooCast在GIFT-Eval零样本预测基准上表现出强大性能，同时保持了单个TSFM的效率；在连续模型发布的实际场景中，框架可以无缝添加新模型以实现渐进式精度提升，且开销可忽略不计。&lt;h4&gt;结论&lt;/h4&gt;ZooCast通过智能组合不同TSFMs的优势，有效提高了零样本预测的性能和效率，为时间序列预测提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)的激增显著推动了零样本预测的发展，使模型能够对未见的时间序列进行预测而无需任务特定的微调。大量研究证实，没有任何单一的TSFM能够普遍优于其他模型，因为不同模型表现出对不同时间模式的偏好。这种多样性表明存在一个机会：如何利用TSFM的互补能力。为此，我们提出了ZooCast，该方法表征每个模型的独特预测优势。ZooCast能够智能地将当前的TSFMs组装成一个模型库，动态选择不同预测任务的最优模型。我们的关键创新在于'一种嵌入适配所有'的范式，该范式构建了一个统一的表示空间，其中库中的每个模型由单个嵌入表示，从而能够为所有任务实现高效的相似性匹配。实验证明，ZooCast在GIFT-Eval零样本预测基准上表现出强大的性能，同时保持了单个TSFM的效率。在连续模型发布的实际场景中，该框架可以无缝添加新模型以实现渐进式精度提升，且开销可忽略不计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The proliferation of Time Series Foundation Models (TSFMs) has significantlyadvanced zero-shot forecasting, enabling predictions for unseen time serieswithout task-specific fine-tuning. Extensive research has confirmed that nosingle TSFM excels universally, as different models exhibit preferences fordistinct temporal patterns. This diversity suggests an opportunity: how to takeadvantage of the complementary abilities of TSFMs. To this end, we proposeZooCast, which characterizes each model's distinct forecasting strengths.ZooCast can intelligently assemble current TSFMs into a model zoo thatdynamically selects optimal models for different forecasting tasks. Our keyinnovation lies in the One-Embedding-Fits-All paradigm that constructs aunified representation space where each model in the zoo is represented by asingle embedding, enabling efficient similarity matching for all tasks.Experiments demonstrate ZooCast's strong performance on the GIFT-Eval zero-shotforecasting benchmark while maintaining the efficiency of a single TSFM. Inreal-world scenarios with sequential model releases, the framework seamlesslyadds new models for progressive accuracy gains with negligible overhead.</description>
      <author>example@mail.com (Hao-Nan Shi, Ting-Ji Huang, Lu Han, De-Chuan Zhan, Han-Jia Ye)</author>
      <guid isPermaLink="false">2509.04208v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>VisioFirm: Cross-Platform AI-assisted Annotation Tool for Computer Vision</title>
      <link>http://arxiv.org/abs/2509.04180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VisioFirm是一个开源的Web应用程序，通过AI辅助自动化简化图像标注流程，可减少最多90%的手动工作量，同时保持高标注准确性。&lt;h4&gt;背景&lt;/h4&gt;AI模型依赖标注数据学习模式和执行预测，传统标注工具劳动密集且难以扩展到大型数据集，需要处理从简单分类标签到复杂任务如目标检测、方向边界框估计和实例分割等不同类型的标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个AI辅助的图像标注工具，减少人工输入需求，提高标注效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;VisioFirm集成基础模型到界面中，采用CLIP结合预训练检测器（如Ultralytics）处理常见类别，使用零样本模型（如GroundingDINO）处理自定义标签，通过低置信度阈值最大化召回率生成初始标注，并提供交互式工具进行完善，同时利用WebGPU加速Segment Anything实现实时分割。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO类型类别测试中，初始预测大多正确；支持多种导出格式（YOLO、COCO、Pascal VOC、CSV）；模型缓存后可离线运行；通过基于CLIP的连接组件聚类和IoU图进行冗余检测抑制。&lt;h4&gt;结论&lt;/h4&gt;VisioFirm显著降低了手动标注工作量，同时保持了高标注准确性，提高了标注流程的可扩展性和可访问性。&lt;h4&gt;翻译&lt;/h4&gt;AI模型依赖于标注数据来学习模式和执行预测。标注通常是一个劳动密集型的步骤，需要将标签从简单的分类标签到更复杂的任务（如目标检测、方向边界框估计和实例分割）相关联。传统工具通常需要大量手动输入，限制了大型数据集的可扩展性。为了解决这个问题，我们引入了VisioFirm，一个开源的Web应用程序，旨在通过AI辅助自动化简化图像标注流程。VisioFirm将最先进的基础模型集成到一个带有过滤管道的界面中，以减少人在环路中的工作量。这种混合方法采用CLIP结合预训练检测器（如Ultralytics模型）用于常见类别，以及零样本模型（如GroundingDINO）用于自定义标签，通过低置信度阈值最大化召回率来生成初始标注。通过这个框架，在COCO类型类别的测试中，初始预测已被证明大多是正确的，用户可以通过支持边界框、方向边界框和多边形的交互式工具进行完善。此外，VisioFirm还具有由Segment Anything驱动的实时分割功能，通过WebGPU加速以提高浏览器端的效率。该工具支持多种导出格式（YOLO、COCO、Pascal VOC、CSV），并在模型缓存后可离线运行，提高了可访问性。VisioFirm通过在不同数据集上的基准测试，展示了最多可减少90%的手动工作量，同时通过基于CLIP的连接组件聚类和IoU图进行冗余检测抑制，保持高标注准确性。VisioFirm可以从https://github.com/OschAI/VisioFirm访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI models rely on annotated data to learn pattern and perform prediction.Annotation is usually a labor-intensive step that require associating labelsranging from a simple classification label to more complex tasks such as objectdetection, oriented bounding box estimation, and instance segmentation.Traditional tools often require extensive manual input, limiting scalabilityfor large datasets. To address this, we introduce VisioFirm, an open-source webapplication designed to streamline image labeling through AI-assistedautomation. VisioFirm integrates state-of-the-art foundation models into aninterface with a filtering pipeline to reduce human-in-the-loop efforts. Thishybrid approach employs CLIP combined with pre-trained detectors likeUltralytics models for common classes and zero-shot models such as GroundingDINO for custom labels, generating initial annotations with low-confidencethresholding to maximize recall. Through this framework, when tested onCOCO-type of classes, initial prediction have been proven to be mostly correctthough the users can refine these via interactive tools supporting boundingboxes, oriented bounding boxes, and polygons. Additionally, VisioFirm hason-the-fly segmentation powered by Segment Anything accelerated through WebGPUfor browser-side efficiency. The tool supports multiple export formats (YOLO,COCO, Pascal VOC, CSV) and operates offline after model caching, enhancingaccessibility. VisioFirm demonstrates up to 90\% reduction in manual effortthrough benchmarks on diverse datasets, while maintaining high annotationaccuracy via clustering of connected CLIP-based disambiguate components andIoU-graph for redundant detection suppression. VisioFirm can be accessed from\href{https://github.com/OschAI/VisioFirm}{https://github.com/OschAI/VisioFirm}.</description>
      <author>example@mail.com (Safouane El Ghazouali, Umberto Michelucci)</author>
      <guid isPermaLink="false">2509.04180v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Chest X-ray Interpretation with Grounded Reasoning via Online Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.03906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepMedix-R1是一种用于胸部X光解释的整体医学基础模型，通过顺序训练流水线实现了透明的推理过程和本地化可解释性，在报告生成和视觉问答任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;医学基础模型在人工智能技术快速发展的背景下显示出巨大潜力，但当前医学基础模型通常以黑盒方式生成答案，缺乏透明的推理过程和本地化的可解释性，这阻碍了它们在临床实践中的实际部署。&lt;h4&gt;目的&lt;/h4&gt;介绍DeepMedix-R1，一个用于胸部X光解释的整体医学基础模型，解决医学基础模型缺乏透明推理和本地化可解释性的问题。&lt;h4&gt;方法&lt;/h4&gt;采用顺序训练流水线：首先在精选的胸部X光指令数据上进行微调，赋予基本的解释能力；然后接触高质量的合成推理样本，实现冷启动推理；最后通过在线强化学习进行优化，提高推理质量和生成性能。模型为每个查询生成答案和与图像局部区域相关的推理步骤。同时提出了Report Arena基准框架，使用先进语言模型评估答案质量。&lt;h4&gt;主要发现&lt;/h4&gt;在报告生成任务上比LLaVA-Rad和MedGemma分别提高14.54%和31.32%；在视觉问答任务上比MedGemma和CheXagent分别提高57.75%和23.06%；生成的推理步骤比Qwen2.5-VL-7B模型具有更高的可解释性和临床可行性(总体偏好0.7416 vs. 0.2584)。&lt;h4&gt;结论&lt;/h4&gt;该工作推进了医学基础模型的发展，朝着胸部X光解释的整体、透明和临床可行的建模方向发展。&lt;h4&gt;翻译&lt;/h4&gt;医学基础模型在人工智能技术快速发展的背景下显示出巨大潜力。然而，当前医学基础模型通常以黑盒方式生成答案，缺乏透明的推理过程和本地化的可解释性，这阻碍了它们在临床实践中的实际部署。为此，我们介绍了DeepMedix-R1，一个用于胸部X光解释的整体医学基础模型。它采用顺序训练流水线：首先在精选的胸部X光指令数据上进行微调，赋予基本的胸部X光解释能力；然后接触高质量的合成推理样本，实现冷启动推理；最后通过在线强化学习进行优化，提高基础推理质量和生成性能。因此，模型为每个查询生成答案和与图像局部区域相关的推理步骤。定量评估表明，在报告生成任务上(例如比LLaVA-Rad和MedGemma分别提高14.54%和31.32%)和视觉问答任务上(例如比MedGemma和CheXagent分别提高57.75%和23.06%)都有显著改进。为了促进稳健评估，我们提出了Report Arena，一个使用先进语言模型评估答案质量的基准框架，进一步凸显了DeepMedix-R1的优越性。专家对生成的推理步骤的审查显示，与已建立的Qwen2.5-VL-7B模型相比，具有更高的可解释性和临床可行性(总体偏好0.7416比0.2584)。总的来说，我们的工作推进了医学基础模型的发展，朝着胸部X光解释的整体、透明和临床可行的建模方向发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical foundation models (FMs) have shown tremendous promise amid the rapidadvancements in artificial intelligence (AI) technologies. However, currentmedical FMs typically generate answers in a black-box manner, lackingtransparent reasoning processes and locally grounded interpretability, whichhinders their practical clinical deployments. To this end, we introduceDeepMedix-R1, a holistic medical FM for chest X-ray (CXR) interpretation. Itleverages a sequential training pipeline: initially fine-tuned on curated CXRinstruction data to equip with fundamental CXR interpretation capabilities,then exposed to high-quality synthetic reasoning samples to enable cold-startreasoning, and finally refined via online reinforcement learning to enhanceboth grounded reasoning quality and generation performance. Thus, the modelproduces both an answer and reasoning steps tied to the image's local regionsfor each query. Quantitative evaluation demonstrates substantial improvementsin report generation (e.g., 14.54% and 31.32% over LLaVA-Rad and MedGemma) andvisual question answering (e.g., 57.75% and 23.06% over MedGemma and CheXagent)tasks. To facilitate robust assessment, we propose Report Arena, a benchmarkingframework using advanced language models to evaluate answer quality, furtherhighlighting the superiority of DeepMedix-R1. Expert review of generatedreasoning steps reveals greater interpretability and clinical plausibilitycompared to the established Qwen2.5-VL-7B model (0.7416 vs. 0.2584 overallpreference). Collectively, our work advances medical FM development towardholistic, transparent, and clinically actionable modeling for CXRinterpretation.</description>
      <author>example@mail.com (Qika Lin, Yifan Zhu, Bin Pu, Ling Huang, Haoran Luo, Jingying Ma, Zhen Peng, Tianzhe Zhao, Fangzhi Xu, Jian Zhang, Kai He, Zhonghong Ou, Swapnil Mishra, Mengling Feng)</author>
      <guid isPermaLink="false">2509.03906v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>A Generative Foundation Model for Chest Radiography</title>
      <link>http://arxiv.org/abs/2509.03903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了ChexGen，一个专门用于胸部X光片合成的生成式视觉-语言基础模型。该模型通过文本、掩码和边界框引导的方式生成高质量的胸部X光片，并在大规模数据集上进行了预训练。研究展示了该模型在数据增强、模型训练和提升医疗AI系统公平性方面的显著效用。&lt;h4&gt;背景&lt;/h4&gt;医疗领域缺乏充分注释的多样化医学图像是开发可靠AI模型的主要障碍。尽管自然图像的生成式基础模型已取得重大技术进步，但在医疗影像领域仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的生成框架，用于胸部X光片的文本、掩码和边界框引导合成，以解决医疗AI领域数据稀缺问题，并提高模型的准确性、数据效率和公平性。&lt;h4&gt;方法&lt;/h4&gt;构建了基于潜在扩散变换器架构的ChexGen模型，在包含96万个X光片-报告对的最大精选胸部X光数据集上进行预训练。通过专家评估和定量指标验证合成图像的准确性，并利用该模型进行训练数据增强和监督预训练。&lt;h4&gt;主要发现&lt;/h4&gt;ChexGen能够生成准确的胸部X光片，通过使用少量训练数据进行数据增强和监督预训练，显著提高了疾病分类、检测和分割任务的性能。此外，该模型能够创建多样化的患者队列，通过检测和减轻人口统计学偏见来增强模型公平性。&lt;h4&gt;结论&lt;/h4&gt;生成式基础模型在构建更准确、数据效率更高和更公平的医疗AI系统中具有变革性作用，ChexGen展示了这种潜力。&lt;h4&gt;翻译&lt;/h4&gt;缺乏充分注释的多样化医学图像是开发医疗领域可靠AI模型的主要障碍。自然图像的生成式基础模型已取得重大技术进步。我们开发了ChexGen，一个生成式视觉-语言基础模型，引入了用于胸部X光片文本、掩码和边界框引导合成的统一框架。基于潜在扩散变换器架构，ChexGen在迄今为止最大的精选胸部X光数据集上进行了预训练，该数据集包含96万个X光片-报告对。ChexGen通过专家评估和定量指标实现了胸部X光片的准确合成。我们展示了ChexGen在训练数据增强和监督预训练方面的效用，这在使用少量训练数据的情况下提高了疾病分类、检测和分割任务的性能。此外，我们的模型能够创建多样化的患者队列，通过检测和减轻人口统计学偏见来增强模型公平性。我们的研究支持生成式基础模型在构建更准确、数据效率更高和更公平的医疗AI系统中的变革性作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The scarcity of well-annotated diverse medical images is a major hurdle fordeveloping reliable AI models in healthcare. Substantial technical advanceshave been made in generative foundation models for natural images. Here wedevelop `ChexGen', a generative vision-language foundation model thatintroduces a unified framework for text-, mask-, and bounding box-guidedsynthesis of chest radiographs. Built upon the latent diffusion transformerarchitecture, ChexGen was pretrained on the largest curated chest X-ray datasetto date, consisting of 960,000 radiograph-report pairs. ChexGen achievesaccurate synthesis of radiographs through expert evaluations and quantitativemetrics. We demonstrate the utility of ChexGen for training data augmentationand supervised pretraining, which led to performance improvements acrossdisease classification, detection, and segmentation tasks using a smallfraction of training data. Further, our model enables the creation of diversepatient cohorts that enhance model fairness by detecting and mitigatingdemographic biases. Our study supports the transformative role of generativefoundation models in building more accurate, data-efficient, and equitablemedical AI systems.</description>
      <author>example@mail.com (Yuanfeng Ji, Dan Lin, Xiyue Wang, Lu Zhang, Wenhui Zhou, Chongjian Ge, Ruihang Chu, Xiaoli Yang, Junhan Zhao, Junsong Chen, Xiangde Luo, Sen Yang, Jin Fang, Ping Luo, Ruijiang Li)</author>
      <guid isPermaLink="false">2509.03903v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Finetuning AI Foundation Models to Develop Subgrid-Scale Parameterizations: A Case Study on Atmospheric Gravity Waves</title>
      <link>http://arxiv.org/abs/2509.03816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通过微调预训练AI基础模型(FM)来开发小尺度气候过程机器学习参数化的新方法，应用于大气重力波参数化，并展示了其优越的预测性能。&lt;h4&gt;背景&lt;/h4&gt;全球气候模型参数化了多种无法充分解析的大气-海洋过程（如重力波、云、湿对流和湍流），这些次网格尺度闭合方案是模型不确定性的主要来源。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的机器学习方法，通过微调预训练的AI基础模型来创建小尺度气候过程的参数化方案。&lt;h4&gt;方法&lt;/h4&gt;使用NASA和IBM研究的230亿参数Prithvi WxC FM的预训练编码器-解码器，通过微调创建大气重力波的深度学习参数化；学习比粗分辨率气候模型精细10倍的大气再分析数据中的通量；与注意力U-Net基线模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;FM参数化在整个大气层中表现出优越的预测性能，即使在排除在预训练之外的区域也是如此；Hellinger距离显示性能提升：基线为0.11，微调模型为0.06。&lt;h4&gt;结论&lt;/h4&gt;FM的多功能性和可重用性为完成多种大气和气候相关应用提供了可能，为创建更多地球系统过程的观测驱动且物理准确的参数化铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;全球气候模型参数化了多种无法充分解析的大气-海洋过程，如重力波、云、湿对流和湍流。这些针对未解析过程的次网格尺度闭合方案是模型不确定性的主要来源。在此，我们提出了一种新方法，通过微调预训练的AI基础模型(FM)来开发小尺度气候过程的机器学习参数化。FM在气候研究中 largely未被探索。我们从230亿参数的FM（NASA和IBM研究的Prithvi WxC）--包含大气演化的潜在概率表示--中预训练的编码器-解码器进行微调（或重用），以创建大气重力波(GW)的深度学习参数化。该参数化通过学习分辨率比粗分辨率气候模型精细10倍的大气再分析数据中的通量，为粗分辨率气候模型捕捉重力波效应。与机器学习基线模型（注意力U-Net）的月平均值和瞬时演化比较显示，FM参数化在整个大气层中表现出优越的预测性能，即使在排除在预训练之外的区域也是如此。使用Hellinger距离量化这一性能提升，基线为0.11，微调模型为0.06。我们的发现强调了FM的多功能性和可重用性，可用于完成多种大气和气候相关应用，为创建更多地球系统过程的观测驱动且物理准确的参数化铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global climate models parameterize a range of atmospheric-oceanic processeslike gravity waves, clouds, moist convection, and turbulence that cannot besufficiently resolved. These subgrid-scale closures for unresolved processesare a leading source of model uncertainty. Here, we present a new approach todeveloping machine learning parameterizations of small-scale climate processesby fine-tuning a pre-trained AI foundation model (FM). FMs are largelyunexplored in climate research. A pre-trained encoder-decoder from a 2.3billion parameter FM (NASA and IBM Research's Prithvi WxC) -- which contains alatent probabilistic representation of atmospheric evolution -- is fine-tuned(or reused) to create a deep learning parameterization for atmospheric gravitywaves (GWs). The parameterization captures GW effects for a coarse-resolutionclimate model by learning the fluxes from an atmospheric reanalysis with 10times finer resolution. A comparison of monthly averages and instantaneousevolution with a machine learning model baseline (an Attention U-Net) revealssuperior predictive performance of the FM parameterization throughout theatmosphere, even in regions excluded from pre-training. This performance boostis quantified using the Hellinger distance, which is 0.11 for the baseline and0.06 for the fine-tuned model. Our findings emphasize the versatility andreusability of FMs, which could be used to accomplish a range of atmosphere-and climate-related applications, leading the way for the creation ofobservations-driven and physically accurate parameterizations for moreearth-system processes.</description>
      <author>example@mail.com (Aman Gupta, Aditi Sheshadri, Sujit Roy, Johannes Schmude, Vishal Gaur, Wei Ji Leong, Manil Maskey, Rahul Ramachandran)</author>
      <guid isPermaLink="false">2509.03816v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Federated Foundation Models over Wireless Networks for Multi-Modal Multi-Task Intelligence: Integration of Edge Learning with D2D/P2P-Enabled Fog Learning Architectures</title>
      <link>http://arxiv.org/abs/2509.03695v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种分层联邦基础模型(HF-FMs)，作为多模态多任务联邦基础模型(M3T FFMs)的新变体，解决了雾/边缘网络中两个被忽视的异构性维度：模态异构性和任务异构性。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)的出现重塑了机器学习领域。随着模型规模增长，利用无线设备的地理分布式数据变得至关重要，催生了联邦基础模型(FFMs)。近期，FMs已发展为多模态多任务(M3T) FMs(如GPT-4)，能够处理跨多种任务的多样模态，这促使了M3T FFMs这一新范式的出现。&lt;h4&gt;目的&lt;/h4&gt;提出分层联邦基础模型(HF-FMs)，揭示雾/边缘网络中两个影响新兴模型的异构性维度：(i)收集模态的异构性；(ii)执行任务的异构性。&lt;h4&gt;方法&lt;/h4&gt;HF-FMs将M3T FMs的模块化结构(模态编码器、提示、专家混合、适配器和任务头)与雾/边缘基础设施的层次结构对齐。支持可选的设备到设备(D2D)通信，实现节点间的水平模块中继和本地协作训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入研究HF-FMs的架构设计，强调了其独特能力和一系列定制化的未来研究方向。&lt;h4&gt;结论&lt;/h4&gt;在无线网络环境中构建了HF-FMs原型，并发布了开源代码，旨在促进这一未被充分探索领域的研究。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)的兴起重塑了机器学习格局。随着这些模型持续增长，利用无线设备中的地理分布式数据变得越来越重要，催生了联邦基础模型(FFMs)。最近，FMs已发展为多模态多任务(M3T) FMs(如GPT-4)，能够处理跨多个任务的多种模态，这促使了一种新的未被充分探索的范式：M3T FFMs。在本文中，我们通过提出分层联邦基础模型(HF-FMs)，揭示了M3T FFMs的一个未被探索的变体，这反过来又暴露了影响这些新兴模型的雾/边缘网络中被忽视的两个异构性维度：(i)收集模态的异构性；(ii)雾/边缘节点上执行任务的异构性。HF-FMs将M3T FMs的模块化结构(包括模态编码器、提示、专家混合、适配器和任务头)与雾/边缘基础设施的层次性质战略性对齐。此外，HF-FMs支持可选的设备到设备(D2D)通信，使节点间能够在可行时进行水平模块中继和本地协作训练。通过深入研究HF-FMs的架构设计，我们强调了它们的独特能力以及一系列定制化的未来研究方向。最后，为了展示它们的潜力，我们在无线网络环境中构建了HF-FMs的原型，并发布了用于开发HF-FMs的开源代码，旨在促进这一未被充分探索领域的研究(GitHub: https://github.com/payamsiabd/M3T-FFM)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of foundation models (FMs) has reshaped the landscape of machinelearning. As these models continued to grow, leveraging geo-distributed datafrom wireless devices has become increasingly critical, giving rise tofederated foundation models (FFMs). More recently, FMs have evolved intomulti-modal multi-task (M3T) FMs (e.g., GPT-4) capable of processing diversemodalities across multiple tasks, which motivates a new underexplored paradigm:M3T FFMs. In this paper, we unveil an unexplored variation of M3T FFMs byproposing hierarchical federated foundation models (HF-FMs), which in turnexpose two overlooked heterogeneity dimensions to fog/edge networks that have adirect impact on these emerging models: (i) heterogeneity in collectedmodalities and (ii) heterogeneity in executed tasks across fog/edge nodes.HF-FMs strategically align the modular structure of M3T FMs, comprisingmodality encoders, prompts, mixture-of-experts (MoEs), adapters, and taskheads, with the hierarchical nature of fog/edge infrastructures. Moreover,HF-FMs enable the optional usage of device-to-device (D2D) communications,enabling horizontal module relaying and localized cooperative training amongnodes when feasible. Through delving into the architectural design of HF-FMs,we highlight their unique capabilities along with a series of tailored futureresearch directions. Finally, to demonstrate their potential, we prototypeHF-FMs in a wireless network setting and release the open-source code for thedevelopment of HF-FMs with the goal of fostering exploration in this untappedfield (GitHub: https://github.com/payamsiabd/M3T-FFM).</description>
      <author>example@mail.com (Payam Abdisarabshali, Fardis Nadimi, Kasra Borazjani, Naji Khosravan, Minghui Liwang, Wei Ni, Dusit Niyato, Michael Langberg, Seyyedali Hosseinalipour)</author>
      <guid isPermaLink="false">2509.03695v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>CEHR-GPT: A Scalable Multi-Task Foundation Model for Electronic Health Records</title>
      <link>http://arxiv.org/abs/2509.03643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CEHR-GPT，一个针对电子健康记录(EHR)数据的通用基础模型，整合了特征表示、零样本预测和合成数据生成三种核心能力。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHRs)提供患者健康的丰富纵向视图，在临床决策支持、风险预测和数据驱动医疗研究方面具有巨大潜力。然而，大多数EHR人工智能模型为单一目的任务设计，限制了其在现实世界中的泛化能力和实用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的EHR基础模型，同时具备多种能力，提高模型在现实世界中的泛化能力和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出CEHR-GPT模型，采用基于时间令牌的学习框架，将患者的时间线动态编码到模型结构中，支持对临床序列的时间推理。&lt;h4&gt;主要发现&lt;/h4&gt;CEHR-GPT在所有三项任务上都表现出强大性能，通过词汇扩展和微调能有效泛化到外部数据集。&lt;h4&gt;结论&lt;/h4&gt;CEHR-GPT的多功能性使其能够快速进行模型开发、队列发现和患者结果预测，无需针对特定任务重新训练。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHRs)提供了患者健康的丰富纵向视图，并在临床决策支持、风险预测和数据驱动的医疗保健研究方面具有巨大潜力。然而，大多数针对EHR的人工智能(AI)模型是为狭窄的单一目的任务设计的，限制了它们在现实世界环境中的泛化能力和实用性。在这里，我们提出了CEHR-GPT，这是一种用于EHR数据的通用基础模型，它在单一架构中统一了三种基本能力——特征表示、零样本预测和合成数据生成。为了支持对临床序列的时间推理，CEHR-GPT采用了一种新颖的基于时间令牌的学习框架，将患者的时间线动态编码到模型结构中。CEHR-GPT在所有三项任务上都表现出强大的性能，并通过词汇扩展和微调能够有效泛化到外部数据集。它的多功能性使其能够快速进行模型开发、队列发现和患者结果预测，而无需针对特定任务进行重新训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) provide a rich, longitudinal view of patienthealth and hold significant potential for advancing clinical decision support,risk prediction, and data-driven healthcare research. However, most artificialintelligence (AI) models for EHRs are designed for narrow, single-purposetasks, limiting their generalizability and utility in real-world settings.Here, we present CEHR-GPT, a general-purpose foundation model for EHR data thatunifies three essential capabilities - feature representation, zero-shotprediction, and synthetic data generation - within a single architecture. Tosupport temporal reasoning over clinical sequences, \cehrgpt{} incorporates anovel time-token-based learning framework that explicitly encodes patients'dynamic timelines into the model structure. CEHR-GPT demonstrates strongperformance across all three tasks and generalizes effectively to externaldatasets through vocabulary expansion and fine-tuning. Its versatility enablesrapid model development, cohort discovery, and patient outcome forecastingwithout the need for task-specific retraining.</description>
      <author>example@mail.com (Chao Pang, Jiheum Park, Xinzhuo Jiang, Nishanth Parameshwar Pavinkurve, Krishna S. Kalluri, Shalmali Joshi, Noémie Elhadad, Karthik Natarajan)</author>
      <guid isPermaLink="false">2509.03643v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm</title>
      <link>http://arxiv.org/abs/2509.02846v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对偏微分方程的测试时计算策略，通过利用推理过程中的计算资源实现更准确的预测，同时减少训练样本量和模型大小。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程是现代计算科学和工程的基础，但计算成本很高。现有PDE基础模型虽在模拟复杂时空现象方面有前景，但仍受预训练数据集限制，自回归展开性能不佳，特别是在分布外情况下，且需要大量计算资源和训练数据。&lt;h4&gt;目的&lt;/h4&gt;受大型语言模型中'思考'策略启发，为PDEs引入第一个测试时计算策略，利用推理过程中的计算资源实现更准确预测，减少训练样本量和模型大小。&lt;h4&gt;方法&lt;/h4&gt;使用两种类型的奖励模型评估基于随机模型的时空一致性预测，并在PDEGym基准测试中的可压缩欧拉方程模拟上展示该方法。&lt;h4&gt;主要发现&lt;/h4&gt;TTC相对于标准的非自适应自回归推理能够捕捉到改进的预测。&lt;h4&gt;结论&lt;/h4&gt;TTC框架向更高级的推理算法或PDE建模迈出了基础性的一步，包括构建基于强化学习的方法，可能改变物理和工程中的计算工作流程。&lt;h4&gt;翻译&lt;/h4&gt;偏微分方程(PDEs)是现代计算科学和工程的基础，本质上计算成本很高。虽然PDE基础模型在模拟这类复杂的时空现象方面显示出很大 promise，但现有模型仍受预训练数据集的限制，在自回归展开性能方面表现不佳，特别是在分布外(OOD)情况下。此外，它们需要大量的计算资源和训练数据，这限制了它们在许多关键应用中的使用。受大型语言模型(LLMs)中最近发展的'思考'策略启发，我们为PDEs引入了第一个测试时计算(TTC)策略，该策略在推理过程中利用计算资源，以实现更准确的预测，同时使用更少的训练样本和更小的模型。我们通过两种类型的奖励模型实现这一点，这些模型评估基于随机模型的时空一致性预测。我们在PDEGym基准测试中的可压缩欧拉方程模拟上展示了这种方法，并表明TTC相对于标准的非自适应自回归推理捕捉到了改进的预测。这个TTC框架朝着更高级的推理算法或PDE建模迈出了基础性的一步，包括构建基于强化学习的方法，可能改变物理和工程中的计算工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial Differential Equations (PDEs) are the bedrock for moderncomputational sciences and engineering, and inherently computationallyexpensive. While PDE foundation models have shown much promise for simulatingsuch complex spatio-temporal phenomena, existing models remain constrained bythe pretraining datasets and struggle with auto-regressive rollout performance,especially in out-of-distribution (OOD) cases. Furthermore, they havesignificant compute and training data requirements which hamper their use inmany critical applications. Inspired by recent advances in ``thinking"strategies used in large language models (LLMs), we introduce the firsttest-time computing (TTC) strategy for PDEs that utilizes computationalresources during inference to achieve more accurate predictions with fewertraining samples and smaller models. We accomplish this with two types ofreward models that evaluate predictions of a stochastic based model forspatio-temporal consistency. We demonstrate this method on compressibleEuler-equation simulations from the PDEGym benchmark and show that TTC capturesimproved predictions relative to standard non-adaptive auto-regressiveinference. This TTC framework marks a foundational step towards more advancedreasoning algorithms or PDE modeling, inluding buildingreinforcement-learning-based approaches, potentially transforming computationalworkflows in physics and engineering.</description>
      <author>example@mail.com (Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debardeleben, Ayan Biswas, Diane Oyen, Earl Lawrence)</author>
      <guid isPermaLink="false">2509.02846v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Keypoint-based Diffusion for Robotic Motion Planning on the NICOL Robot</title>
      <link>http://arxiv.org/abs/2509.04076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICANN 20255 Special Session on Neural Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种基于扩散的新型动作模型用于机器人运动规划，通过深度学习显著提高了规划速度，同时保持高达90%的无碰撞解决方案成功率。&lt;h4&gt;背景&lt;/h4&gt;现有机器人运动规划通常使用成熟的数值规划方法，但这些方法存在显著的运行时间要求，限制了实时应用的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够快速、高效解决机器人运动规划问题的方法，减少运行时间同时保持较高的成功率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于扩散的动作模型，通过深度学习从传统规划器生成的数据集中学习。初始模型使用点云嵌入作为输入，预测基于关键点的关节序列。通过消融研究发现网络难以基于点云嵌入进行条件化，因此识别并改进了数据集中的偏差，优化了数据集。&lt;h4&gt;主要发现&lt;/h4&gt;即使不使用点云编码，所提出的模型在运行时间上也比数值模型快一个数量级；在测试集上，模型能够达到高达90%的无碰撞解决方案成功率；通过改进数据集，模型性能得到了提升。&lt;h4&gt;结论&lt;/h4&gt;基于扩散的深度学习方法在机器人运动规划领域具有巨大潜力，能够在保持高成功率的同时显著减少计算时间，为实时机器人应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于机器人运动规划的新型基于扩散的动作模型。通常，成熟的数值规划方法被用于解决通用运动规划问题，但需要大量的运行时间。通过利用深度学习的力量，我们能够从这些规划器生成的数据集中学习，在更小的运行时间内获得良好的结果。虽然我们的初始模型在输入中使用点云嵌入来预测输出中的基于关键点的关节序列，但我们在消融研究中观察到，让网络基于点云嵌入进行条件化仍然具有挑战性。我们确定了数据集中的一些偏差并对其进行了改进，从而提高了模型的性能。我们的模型即使不使用点云编码，在运行时间上也比数值模型快一个数量级，同时在测试集上达到高达90%的无碰撞解决方案成功率。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人运动规划的计算效率问题。传统数值规划方法虽然能保证规划的最优性和完整性，但计算成本高，无法实现近实时应用，这在需要机器人快速响应变化环境的现实场景中是一个重大限制。随着自主机器人在复杂环境中应用的增多，快速、可靠的运动规划变得至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从传统数值规划方法的高计算成本出发，考虑利用深度学习加速规划过程。他们最初尝试使用点云嵌入作为输入来预测基于关键点的关节序列，通过消融研究发现数据集存在偏差并进行了改进。设计上借鉴了行为克隆方法，使用扩散策略架构，并参考了运动规划网络(Motion Planning Networks)的工作，但用扩散模型替代了MLP动作预测器，同时利用了扩散模型在行为克隆领域的优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用扩散模型从数值规划器生成的数据集中学习，快速生成机器人运动规划。通过基于关键点的表示减少规划步骤数量，并利用批量推理稳定性能。整体流程包括：1)创建包含5000个场景和10万条规划的合成数据集；2)设计包含点云编码器和扩散动作预测器的神经网络架构；3)训练模型预测从起点到目标的16步关节序列；4)通过逆扩散过程生成轨迹；5)使用批量规划提高成功率和稳定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于关键点的扩散模型，使16步就能生成完整规划；2)批量规划方法，利用GPU并行推理同时生成多个规划；3)数据集改进，消除短轨迹偏见。相比之前工作，该方法运行时间比传统数值规划方法快一个数量级(3秒对比20秒)，同时保持90%成功率；与其他神经规划方法不同，它简化了架构，减少了对复杂点云嵌入的依赖，使部署更容易；与其他扩散模型方法相比，专注于关节空间中的时空扩散，结合关键点表示提高效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于关键点扩散的快速机器人运动规划方法，通过批量推理实现了比传统数值规划方法快一个数量级的运行速度，同时保持了90%的无碰撞规划成功率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel diffusion-based action model for robotic motion planning.Commonly, established numerical planning approaches are used to solve generalmotion planning problems, but have significant runtime requirements. Byleveraging the power of deep learning, we are able to achieve good results in amuch smaller runtime by learning from a dataset generated by these planners.While our initial model uses point cloud embeddings in the input to predictkeypoint-based joint sequences in its output, we observed in our ablation studythat it remained challenging to condition the network on the point cloudembeddings. We identified some biases in our dataset and refined it, whichimproved the model's performance. Our model, even without the use of the pointcloud encodings, outperforms numerical models by an order of magnituderegarding the runtime, while reaching a success rate of up to 90% of collisionfree solutions on the test set.</description>
      <author>example@mail.com (Lennart Clasmeier, Jan-Gerrit Habekost, Connor Gäde, Philipp Allgeuer, Stefan Wermter)</author>
      <guid isPermaLink="false">2509.04076v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>treeX: Unsupervised Tree Instance Segmentation in Dense Forest Point Clouds</title>
      <link>http://arxiv.org/abs/2509.03633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了改进版的treeX算法，一种无监督的树木实例分割方法，适用于近距离激光扫描数据。该方法结合了基于聚类的树干检测和区域生长的树冠分割，为地面激光扫描和无人机激光扫描提供了不同参数预设。研究在六个公共数据集上进行了评估，结果显示改进算法运行时间更短，准确性更高。&lt;h4&gt;背景&lt;/h4&gt;近距离激光扫描能提供森林林分的详细三维捕捉，但需要高效软件处理三维点云数据并提取单棵树木。虽然深度学习方法已用于树木实例分割，但这些方法需要大量标注数据集和计算资源。&lt;h4&gt;目的&lt;/h4&gt;提出资源高效的替代方法，改进原始treeX算法，使其能处理地面激光扫描和无人机激光扫描数据，提高算法效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;改进版treeX算法结合基于聚类的树干检测和区域生长的树冠分割，为TLS/PLS和ULS提供不同参数预设。在六个公共数据集上评估，并与六种开源方法比较。&lt;h4&gt;主要发现&lt;/h4&gt;与原始算法相比，改进版本减少了运行时间并提高了准确性，地面数据实例检测F1分数提高0.11-0.49。ULS数据F1分数达0.58，而原始算法无法分割任何正确实例。TLS和PLS数据准确性与最新开源方法相当。&lt;h4&gt;结论&lt;/h4&gt;该方法可作为深度学习的高效替代方案，适用于数据特征相符的场景；也可用于深度学习模型的半自动标签生成。研究提供了开源Python实现以促进广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;近距离激光扫描能够提供森林林分的详细三维捕捉，但需要高效的软件来处理三维点云数据并提取单棵树木。虽然最近的研究引入了深度学习方法进行树木实例分割，但这些方法需要大量标注数据集和大量计算资源。作为资源高效的替代方案，我们提出了treeX算法的改进版本，这是一种无监督方法，结合了基于聚类的树干检测和区域生长的树冠分割。虽然原始treeX算法是为个人激光扫描数据开发的，但我们提供了两个参数预设，一个用于地面激光扫描，另一个用于无人机载激光扫描。我们在六个公共数据集上评估了该方法，并将其与六种开源方法进行了比较。与原始treeX算法相比，我们的改进版本减少了运行时间并提高了准确性。对于ULS数据，我们的预设实现了0.58的F1分数，而原始算法无法分割任何正确实例。对于TLS和PLS数据，我们的算法实现了与最近的开源方法相当的准确性。鉴于其算法设计，我们看到了该方法的两个主要应用：作为深度学习方法的资源高效替代方案，以及用于深度学习模型的半自动标签生成。为了促进更广泛的应用，我们在pointtree包中提供了开源的Python实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Close-range laser scanning provides detailed 3D captures of forest stands butrequires efficient software for processing 3D point cloud data and extractingindividual trees. Although recent studies have introduced deep learning methodsfor tree instance segmentation, these approaches require large annotateddatasets and substantial computational resources. As a resource-efficientalternative, we present a revised version of the treeX algorithm, anunsupervised method that combines clustering-based stem detection with regiongrowing for crown delineation. While the original treeX algorithm was developedfor personal laser scanning (PLS) data, we provide two parameter presets, onefor ground-based laser scanning (stationary terrestrial - TLS and PLS), and onefor UAV-borne laser scanning (ULS). We evaluated the method on six publicdatasets (FOR-instance, ForestSemantic, LAUTx, NIBIO MLS, TreeLearn, WythamWoods) and compared it to six open-source methods (original treeX, treeiso,RayCloudTools, ForAINet, SegmentAnyTree, TreeLearn). Compared to the originaltreeX algorithm, our revision reduces runtime and improves accuracy, withinstance detection F$_1$-score gains of +0.11 to +0.49 for ground-based data.For ULS data, our preset achieves an F$_1$-score of 0.58, whereas the originalalgorithm fails to segment any correct instances. For TLS and PLS data, ouralgorithm achieves accuracy similar to recent open-source methods, includingdeep learning. Given its algorithmic design, we see two main applications forour method: (1) as a resource-efficient alternative to deep learning approachesin scenarios where the data characteristics align with the method design(sufficient stem visibility and point density), and (2) for the semi-automaticgeneration of labels for deep learning models. To enable broader adoption, weprovide an open-source Python implementation in the pointtree package.</description>
      <author>example@mail.com (Josafat-Mattias Burmeister, Andreas Tockner, Stefan Reder, Markus Engel, Rico Richter, Jan-Peter Mund, Jürgen Döllner)</author>
      <guid isPermaLink="false">2509.03633v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Comment on "A Note on Over-Smoothing for Graph Neural Networks"</title>
      <link>http://arxiv.org/abs/2509.04178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Comment on arXiv:2006.13318 (Cai &amp; Wang, 2020). Revisits their  Dirichlet-energy analysis of over-smoothing and extends it to Leaky-ReLU and  spectral polynomial filters; includes Proposition 7.1 and a new proof of  Lemma 3.3 for Leaky-ReLU. 7 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对Cai和Wang (2020) 关于使用Dirichlet能量分析GNN过平滑的工作进行了评论和扩展，展示了节点嵌入的Dirichlet能量如何随网络深度变化，并探索了缓解过平滑的方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)中的过平滑问题是一个重要的研究课题，需要深入理解其机制和缓解方法。&lt;h4&gt;目的&lt;/h4&gt;展示节点嵌入的Dirichlet能量在GNN中的变化规律，特别是在不同深度和激活函数条件下的行为，以及如何利用这些规律缓解过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;在温和的谱条件下分析Dirichlet能量的变化，将结果扩展到谱多项式滤波器，并通过边删除和权重放大实验进行验证，同时为Leaky-ReLU情况提供简短证明。&lt;h4&gt;主要发现&lt;/h4&gt;节点嵌入的Dirichlet能量在温和的谱条件下(包括Leaky-ReLU)随网络深度呈指数级下降；当Dirichlet能量增加时，可能暗示了缓解过平滑的实际方法。&lt;h4&gt;结论&lt;/h4&gt;通过理解Dirichlet能量随深度的变化规律，可以为设计缓解GNN中过平滑问题的方法提供理论指导和实践启示。&lt;h4&gt;翻译&lt;/h4&gt;我们评论了Cai和Wang (2020, arXiv:2006.13318)，他们通过Dirichlet能量分析了GNN中的过平滑问题。我们表明，在温和的谱条件下(包括Leaky-ReLU)，节点嵌入的Dirichlet能量随深度呈指数级下降；我们进一步将这一结果扩展到谱多项式滤波器，并为Leaky-ReLU情况提供了简短证明。关于边删除和权重放大的实验说明了Dirichlet能量何时增加，暗示了缓解过平滑的实际方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We comment on Cai and Wang (2020, arXiv:2006.13318), who analyzeover-smoothing in GNNs via Dirichlet energy. We show that under mild spectralconditions (including with Leaky-ReLU), the Dirichlet energy of node embeddingsdecreases exponentially with depth; we further extend the result to spectralpolynomial filters and provide a short proof for the Leaky-ReLU case.Experiments on edge deletion and weight amplification illustrate when Dirichletenergy increases, hinting at practical ways to relieve over-smoothing.</description>
      <author>example@mail.com (Razi Hasson, Reuven Guetta)</author>
      <guid isPermaLink="false">2509.04178v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Topotein: Topological Deep Learning for Protein Representation Learning</title>
      <link>http://arxiv.org/abs/2509.03885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Topotein框架通过拓扑深度学习改进蛋白质表示学习，提出蛋白质组合复合物(PCC)和拓扑完全感知器网络(TCPNet)，在四个蛋白质表示学习任务中表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;蛋白质表示学习(PRL)对于理解蛋白质结构与功能关系至关重要，但当前的序列和图方法无法捕捉蛋白质结构中固有的层次组织。&lt;h4&gt;目的&lt;/h4&gt;引入Topotein框架，通过拓扑深度学习来改进蛋白质表示学习，更好地捕捉蛋白质结构的层次组织。&lt;h4&gt;方法&lt;/h4&gt;提出蛋白质组合复合物(PCC)表示蛋白质的多个层次结构(从残基到二级结构再到完整蛋白质)，并开发拓扑完全感知器网络(TCPNet)，通过层次结构之间的SE(3)-等变消息传递捕获多尺度结构模式。&lt;h4&gt;主要发现&lt;/h4&gt;在四个蛋白质表示学习任务上的广泛实验中，TCPNet始终优于最先进的几何图神经网络，在需要理解二级结构排列的任务(如折叠分类)中表现出特别的优势。&lt;h4&gt;结论&lt;/h4&gt;层次拓扑特征对蛋白质分析具有重要意义，Topotein框架有效捕捉了蛋白质结构的层次组织，提高了蛋白质表示学习的性能。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质表示学习(PRL)对于理解结构与功能关系至关重要，然而当前基于序列和图的方法无法捕捉蛋白质结构中固有的层次组织。我们引入Topotein，一个综合框架，通过新颖的蛋白质组合复合物(PCC)和拓扑完全感知器网络(TCPNet)将拓扑深度学习应用于PRL。我们的PCC在多个层次上表示蛋白质——从残基到二级结构再到完整蛋白质——同时在每个层次保留几何信息。TCPNet在这些层次结构之间采用SE(3)-等变消息传递，能够更有效地捕获多尺度结构模式。在四个PRL任务上的广泛实验中，TCPNet始终优于最先进的几何图神经网络。我们的方法在需要理解二级结构排列的任务(如折叠分类)中表现出特别的优势，验证了层次拓扑特征对蛋白质分析的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein representation learning (PRL) is crucial for understandingstructure-function relationships, yet current sequence- and graph-based methodsfail to capture the hierarchical organization inherent in protein structures.We introduce Topotein, a comprehensive framework that applies topological deeplearning to PRL through the novel Protein Combinatorial Complex (PCC) andTopology-Complete Perceptron Network (TCPNet). Our PCC represents proteins atmultiple hierarchical levels -- from residues to secondary structures tocomplete proteins -- while preserving geometric information at each level.TCPNet employs SE(3)-equivariant message passing across these hierarchicalstructures, enabling more effective capture of multi-scale structural patterns.Through extensive experiments on four PRL tasks, TCPNet consistentlyoutperforms state-of-the-art geometric graph neural networks. Our approachdemonstrates particular strength in tasks such as fold classification whichrequire understanding of secondary structure arrangements, validating theimportance of hierarchical topological features for protein analysis.</description>
      <author>example@mail.com (Zhiyu Wang, Arian Jamasb, Mustafa Hajij, Alex Morehead, Luke Braithwaite, Pietro Liò)</author>
      <guid isPermaLink="false">2509.03885v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Combining feature-based approaches with graph neural networks and symbolic regression for synergistic performance and interpretability</title>
      <link>http://arxiv.org/abs/2509.03547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了MatterVial，一个用于材料科学中基于特征的机器学习的创新混合框架，通过整合多种预训练图神经网络模型的潜在表示来扩展特征空间，结合传统模型的化学透明度和深度学习的预测能力，显著提高了模型性能。&lt;h4&gt;背景&lt;/h4&gt;材料科学中的机器学习面临特征选择和模型可解释性的挑战，传统基于特征模型具有化学透明性但预测能力有限，而深度学习模型预测能力强但往往缺乏可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个混合框架，结合传统基于特征模型的化学透明度和深度学习架构的预测能力，同时保持可解释性，提高材料科学中机器学习模型的性能和实用性。&lt;h4&gt;方法&lt;/h4&gt;MatterVial框架整合多种预训练图神经网络模型的潜在表示(包括基于结构的MEGNet、基于成分的ROOST和等变的ORB图网络)，结合计算高效的GNN近似描述符和符号回归的新特征，并集成可解释性模块将GNN特征解码为物理意义明确的公式。&lt;h4&gt;主要发现&lt;/h4&gt;在Matbench任务中增强MODNet模型时，显著降低了误差，将性能提升到与最先进端到端GNN相竞争甚至在某些情况下超越它们，多个任务的准确性提高了40%以上，成功将GNN的复杂特征转换为明确的物理意义公式。&lt;h4&gt;结论&lt;/h4&gt;MatterVial统一框架通过提供高性能、透明的工具来推进材料信息学，该工具符合可解释AI的原则，为更有针对性和自主性的材料发现铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;本研究介绍了MatterVial，这是一个用于材料科学中基于特征的机器学习的创新混合框架。MatterVial通过整合来自多样化预训练图神经网络(GNN)模型的潜在表示来扩展特征空间，包括：基于结构的(MEGNet)、基于成分的(ROOST)和等变的(ORB)图网络，以及计算高效的GNN近似描述符和符号回归的新颖特征。我们的方法将传统基于特征模型的化学透明度与深度学习架构的预测能力相结合。在Matbench任务中增强基于特征的模型MODNet时，这种方法产生了显著的误差减少，并将其性能提升到与最先进的端到端GNN相竞争，并在几种情况下优于它们，多个任务的准确性提高了40%以上。一个集成的可解释性模块，采用代理模型和符号回归，将GNN衍生的潜在描述符解码为明确的、具有物理意义的公式。这个统一的框架通过提供符合可解释AI原则的高性能、透明工具来推进材料信息学，为更有针对性和自主性的材料发现铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study introduces MatterVial, an innovative hybrid framework forfeature-based machine learning in materials science. MatterVial expands thefeature space by integrating latent representations from a diverse suite ofpretrained graph neural network (GNN) models including: structure-based(MEGNet), composition-based (ROOST), and equivariant (ORB) graph networks, withcomputationally efficient, GNN-approximated descriptors and novel features fromsymbolic regression. Our approach combines the chemical transparency oftraditional feature-based models with the predictive power of deep learningarchitectures. When augmenting the feature-based model MODNet on Matbenchtasks, this method yields significant error reductions and elevates itsperformance to be competitive with, and in several cases superior to,state-of-the-art end-to-end GNNs, with accuracy increases exceeding 40% formultiple tasks. An integrated interpretability module, employing surrogatemodels and symbolic regression, decodes the latent GNN-derived descriptors intoexplicit, physically meaningful formulas. This unified framework advancesmaterials informatics by providing a high-performance, transparent tool thataligns with the principles of explainable AI, paving the way for more targetedand autonomous materials discovery.</description>
      <author>example@mail.com (Rogério Almeida Gouvêa, Pierre-Paul De Breuck, Tatiane Pretto, Gian-Marco Rignanese, Marcos José Leite dos Santos)</author>
      <guid isPermaLink="false">2509.03547v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection</title>
      <link>http://arxiv.org/abs/2509.01153v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络和锚定区间的框架，用于呼吸声音事件检测，能够处理可变长度音频并提供更精确的时间定位，实验证明结合呼吸位置信息可以提高异常声音的区分能力。&lt;h4&gt;背景&lt;/h4&gt;听诊是呼吸和肺部疾病早期诊断的关键方法，但过程主观且专家间存在变异性。现有深度学习方法多专注于呼吸声音分类，而声音事件检测研究有限。现有方法通常依赖帧级预测和后处理，难以直接学习区间边界，且多数只能处理固定长度音频，限制了在可变长度呼吸声音中的应用。呼吸声音位置信息对检测性能的影响也未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;解决现有呼吸声音事件检测方法的局限性，提出能够处理可变长度音频并提供更精确时间定位的方法，探索呼吸声音位置信息对检测性能的影响。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于图神经网络和锚定区间的框架，能够处理可变长度音频并提供异常呼吸声音事件更精确的时间定位，结合呼吸位置信息来提高异常声音的区分能力。&lt;h4&gt;主要发现&lt;/h4&gt;在SPRSound 2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，结合呼吸位置信息可以增强异常声音之间的区分能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法提高了呼吸声音检测的灵活性和适用性，基于图神经网络和锚定区间的框架能够有效处理可变长度音频并提供更精确的时间定位。&lt;h4&gt;翻译&lt;/h4&gt;听诊是呼吸和肺部疾病早期诊断的关键方法，依赖于熟练的医疗专业人员。然而，这个过程往往是主观的，专家之间存在变异性。因此，许多基于深度学习的自动分类方法已经出现，大多数专注于呼吸声音分类。相比之下，呼吸声音事件检测的研究仍然有限。现有的声音事件检测方法通常依赖于帧级预测，然后通过后处理生成事件级输出，使得区间边界难以直接学习。此外，许多方法只能处理固定长度的音频，限制了它们在可变长度呼吸声音中的应用。此外，呼吸声音位置信息对检测性能的影响尚未得到充分探索。为了解决这些问题，我们提出了一种基于图神经网络的框架，具有锚定区间，能够处理可变长度的音频，并为异常呼吸声音事件提供更精确的时间定位。我们的方法提高了呼吸声音检测的灵活性和适用性。在SPRSound 2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，并且结合呼吸位置信息可以提高异常声音的区分能力。参考实现可在https://github.com/chumingqian/EzhouNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.bspc.2025.108491&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auscultation is a key method for early diagnosis of respiratory and pulmonarydiseases, relying on skilled healthcare professionals. However, the process isoften subjective, with variability between experts. As a result, numerous deeplearning-based automatic classification methods have emerged, most of whichfocus on respiratory sound classification. In contrast, research on respiratorysound event detection remains limited. Existing sound event detection methodstypically rely on frame-level predictions followed by post-processing togenerate event-level outputs, making interval boundaries challenging to learndirectly. Furthermore, many approaches can only handle fixed-length audio,limiting their applicability to variable-length respiratory sounds.Additionally, the impact of respiratory sound location information on detectionperformance has not been extensively explored. To address these issues, wepropose a graph neural network-based framework with anchor intervals, capableof handling variable-length audio and providing more precise temporallocalization for abnormal respiratory sound events. Our method improves boththe flexibility and applicability of respiratory sound detection. Experimentson the SPRSound 2024 and HF Lung V1 datasets demonstrate the effectiveness ofthe proposed approach, and incorporating respiratory position informationenhances the discrimination between abnormal sounds. The referenceimplementation is available at https://github.com/chumingqian/EzhouNet.</description>
      <author>example@mail.com (Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng)</author>
      <guid isPermaLink="false">2509.01153v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Parking Availability Prediction via Fusing Multi-Source Data with A Self-Supervised Learning Enhanced Spatio-Temporal Inverted Transformer</title>
      <link>http://arxiv.org/abs/2509.04362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 5 figures, under review for journal publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为SST-iTransformer的新方法，用于预测城市停车场可用性。该方法通过K-means聚类建立停车聚类区域，整合多种交通模式的需求特征，并采用改进的双分支注意力机制进行时空建模。实验证明该方法优于现有基线模型，且不同数据源对预测性能的贡献度不同。&lt;h4&gt;背景&lt;/h4&gt;私家车数量的快速增长加剧了城市停车困境，因此需要准确有效的停车场可用性预测来支持城市规划和管理。&lt;h4&gt;目的&lt;/h4&gt;解决停车场可用性预测中建模时空依赖性和利用多源数据的关键局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 使用K-means聚类建立停车聚类区域(PCZs)；2) 提取并整合与目标停车场相关的多种交通模式(地铁、公交、网约车、出租车)的交通需求特征；3) 基于原始iTransformer进行升级，提出SST-iTransformer：整合基于掩码重建的自监督任务进行时空表示学习，并采用双分支注意力机制(序列注意力捕获长期时间依赖，通道注意力建模跨变量交互)。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用成都真实数据的实验表明，SST-iTransformer优于基线深度学习模型，达到最低的均方误差和具有竞争力的平均绝对误差；2) 不同数据源贡献度不同：网约车数据贡献最大，其次是出租车数据，公交和地铁数据贡献较小；3) 排除相关停车场的历史数据会导致性能大幅下降，强调了建模空间依赖性的重要性。&lt;h4&gt;结论&lt;/h4&gt;SST-iTransformer方法通过有效建模时空依赖性和整合多源数据，显著提高了停车场可用性预测的准确性，为城市停车管理提供了有力支持。&lt;h4&gt;翻译&lt;/h4&gt;私家车数量的快速增长加剧了城市停车困境，凸显了需要准确有效的停车场可用性预测来支持城市规划和管理的必要性。为解决停车场可用性预测中建模时空依赖性和利用多源数据的关键局限性，本研究提出了一种基于SST-iTransformer的新方法。该方法利用K-means聚类建立停车聚类区域(PCZs)，提取并整合与目标停车场相关的各种交通模式(即地铁、公交、网约车和出租车)的交通需求特征。在原始iTransformer基础上升级的SST-iTransformer，整合了基于掩码重建的自监督 pretext 任务进行时空表示学习，并具有创新的双分支注意力机制：序列注意力通过patch操作捕获长期时间依赖，而通道注意力通过反转维度建模跨变量交互。使用中国成都真实数据的广泛实验表明，SST-iTransformer优于基线深度学习模型(包括Informer、Autoformer、Crossformer和iTransformer)，以最低的均方误差(MSE)和具有竞争力的平均绝对误差(MAE)实现了最先进的性能。全面的消融研究定量揭示了不同数据源的相对重要性：整合网约车数据带来最大的性能提升，其次是出租车数据，而固定路线交通特征(公交/地铁)贡献较小。空间相关性分析进一步证实，排除PCZs内相关停车场的历史数据会导致性能大幅下降，强调了建模空间依赖性的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of private car ownership has worsened the urban parkingpredicament, underscoring the need for accurate and effective parkingavailability prediction to support urban planning and management. To addresskey limitations in modeling spatio-temporal dependencies and exploitingmulti-source data for parking availability prediction, this study proposes anovel approach with SST-iTransformer. The methodology leverages K-meansclustering to establish parking cluster zones (PCZs), extracting andintegrating traffic demand characteristics from various transportation modes(i.e., metro, bus, online ride-hailing, and taxi) associated with the targetedparking lots. Upgraded on vanilla iTransformer, SST-iTransformer integratesmasking-reconstruction-based pretext tasks for self-supervised spatio-temporalrepresentation learning, and features an innovative dual-branch attentionmechanism: Series Attention captures long-term temporal dependencies viapatching operations, while Channel Attention models cross-variate interactionsthrough inverted dimensions. Extensive experiments using real-world data fromChengdu, China, demonstrate that SST-iTransformer outperforms baseline deeplearning models (including Informer, Autoformer, Crossformer, andiTransformer), achieving state-of-the-art performance with the lowest meansquared error (MSE) and competitive mean absolute error (MAE). Comprehensiveablation studies quantitatively reveal the relative importance of differentdata sources: incorporating ride-hailing data provides the largest performancegains, followed by taxi, whereas fixed-route transit features (bus/metro)contribute marginally. Spatial correlation analysis further confirms thatexcluding historical data from correlated parking lots within PCZs leads tosubstantial performance degradation, underscoring the importance of modelingspatial dependencies.</description>
      <author>example@mail.com (Yin Huang, Yongqi Dong, Youhua Tang, Li Li)</author>
      <guid isPermaLink="false">2509.04362v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Decoupled Entity Representation Learning for Pinterest Ads Ranking</title>
      <link>http://arxiv.org/abs/2509.04337v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种Pinterest使用的上游-下游新框架，通过多种数据源构建用户和Pin的嵌入表示，用于个性化推荐和广告投放，并在实际系统中取得了显著效果。&lt;h4&gt;背景&lt;/h4&gt;Pinterest需要从多样化数据源构建用户和Pin的嵌入表示，以实现有效的个性化Pin和广告推荐。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展的框架，能够从多样化数据源学习用户和Pin的嵌入表示，并支持多种下游推荐和广告任务。&lt;h4&gt;方法&lt;/h4&gt;采用上游-下游范式，上游模型在广泛数据源上训练使用复杂架构捕捉用户-Pin关系，通过学习并定期刷新实体嵌入而非实时计算确保可扩展性，然后将这些嵌入作为下游任务如广告检索和CTR/CVR预测排序模型的输入特征。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在离线和在线环境下，各种下游任务中取得了显著的性能提升，并在Pinterest生产广告排序系统中部署后带来在线指标的显著增长。&lt;h4&gt;结论&lt;/h4&gt;所提出的上游-下游框架是Pinterest个性化推荐和广告系统的有效解决方案，通过嵌入表示的异步交互实现了良好的可扩展性和性能提升。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种遵循上游-下游范式的新框架，从多样化数据源构建用户和物品(Pin)的嵌入表示，这些嵌入对Pinterest有效提供个性化Pin和广告至关重要。我们的上游模型在具有不同信号特征的大量数据源上训练，利用复杂架构捕捉Pinterest上用户和Pin之间的复杂关系。为确保上游模型的可扩展性，学习实体嵌入并定期刷新，而非实时计算，允许上游和下游模型之间异步交互。这些嵌入随后被整合为众多下游任务的输入特征，包括用于点击率和转化率预测的广告检索和排序模型。我们证明该框架在各种下游任务的离线和在线设置中均取得了显著的性能提升。该框架已在Pinterest的生产广告排序系统中部署，带来了在线指标的显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a novel framework following anupstream-downstream paradigm to construct user and item (Pin) embeddings fromdiverse data sources, which are essential for Pinterest to deliver personalizedPins and ads effectively. Our upstream models are trained on extensive datasources featuring varied signals, utilizing complex architectures to captureintricate relationships between users and Pins on Pinterest. To ensurescalability of the upstream models, entity embeddings are learned, andregularly refreshed, rather than real-time computation, allowing forasynchronous interaction between the upstream and downstream models. Theseembeddings are then integrated as input features in numerous downstream tasks,including ad retrieval and ranking models for CTR and CVR predictions. Wedemonstrate that our framework achieves notable performance improvements inboth offline and online settings across various downstream tasks. Thisframework has been deployed in Pinterest's production ad ranking systems,resulting in significant gains in online metrics.</description>
      <author>example@mail.com (Jie Liu, Yinrui Li, Jiankai Sun, Kungang Li, Han Sun, Sihan Wang, Huasen Wu, Siyuan Gao, Paulo Soares, Nan Li, Zhifang Liu, Haoyang Li, Siping Ji, Ling Leng, Prathibha Deshikachar)</author>
      <guid isPermaLink="false">2509.04337v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>A Primer on Causal and Statistical Dataset Biases for Fair and Robust Image Analysis</title>
      <link>http://arxiv.org/abs/2509.04295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Excerpt from C. Jones' PhD thesis. Winner of the G-Research PhD prize  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习方法在现实世界部署时经常失败，特别是在高风险场景和社会敏感领域，限制了其在医疗诊断等领域的应用潜力。&lt;h4&gt;背景&lt;/h4&gt;机器学习在现实世界部署时经常失败，尤其是在高风险场景和社会敏感领域，这对其在医疗诊断等领域的应用产生了抑制作用。&lt;h4&gt;目的&lt;/h4&gt;介绍导致图像分析中机器学习方法失败的因果和统计结构。&lt;h4&gt;方法&lt;/h4&gt;分析机器学习方法在图像分析中失败的根本原因，并评估现有公平表示学习方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两个之前被忽视的问题：'no fair lunch' problem（无公平午餐问题）和'subgroup separability' problem（子群可分离性问题）。&lt;h4&gt;结论&lt;/h4&gt;当今的公平表示学习方法无法充分解决这些问题，需要新的方法路径来应对这些挑战。&lt;h4&gt;翻译&lt;/h4&gt;机器学习方法在现实世界中部署时常常失败。更糟糕的是，它们在高风险情境和社会敏感领域都会失败。这些问题对机器学习方法在医疗诊断等领域的应用产生了抑制作用，尽管在这些领域，如果能够安全部署，机器学习方法本可以提供最大益处。在本入门指南中，我们介绍了导致图像分析中机器学习方法失败的因果和统计结构。我们强调了两个之前被忽视的问题，我们称之为'无公平午餐'问题和'子群可分离性'问题。我们阐明了为什么当今的公平表示学习方法无法充分解决这些问题，并为该领域提出了可能的解决路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning methods often fail when deployed in the real world. Worsestill, they fail in high-stakes situations and across socially sensitive lines.These issues have a chilling effect on the adoption of machine learning methodsin settings such as medical diagnosis, where they are arguably best-placed toprovide benefits if safely deployed. In this primer, we introduce the causaland statistical structures which induce failure in machine learning methods forimage analysis. We highlight two previously overlooked problems, which we callthe \textit{no fair lunch} problem and the \textit{subgroup separability}problem. We elucidate why today's fair representation learning methods fail toadequately solve them and propose potential paths forward for the field.</description>
      <author>example@mail.com (Charles Jones, Ben Glocker)</author>
      <guid isPermaLink="false">2509.04295v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>PianoBind: A Multimodal Joint Embedding Model for Pop-piano Music</title>
      <link>http://arxiv.org/abs/2509.04215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at the 26th International Society for Music  Information Retrieval Conference (ISMIR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PianoBind，一个专用于钢琴音乐的多模态联合嵌入模型，解决了现有模型难以捕捉钢琴音乐细微语义差异和多模态特性的问题。&lt;h4&gt;背景&lt;/h4&gt;独奏钢琴音乐具有丰富的表现力和语义信息，但当前通用音乐表征模型难以捕捉同质化钢琴音乐中的细微语义差异，且现有钢琴专用模型多为单模态，无法捕捉钢琴音乐固有的多模态特性（音频、符号和文本）。&lt;h4&gt;目的&lt;/h4&gt;开发一个钢琴专用的多模态联合嵌入模型，以有效捕捉钢琴音乐的细微语义差异和多模态特性，并在小规模和同质化数据集上表现良好。&lt;h4&gt;方法&lt;/h4&gt;在联合嵌入框架内系统研究多源训练策略和模态利用方法，优化模型以在(1)小规模和(2)同质化钢琴数据集中捕捉细粒度语义差异，同时整合音频、符号和文本多种模态信息。&lt;h4&gt;主要发现&lt;/h4&gt;PianoBind学习到的多模态表征能有效捕捉钢琴音乐的细微差别，在领域内和领域外的钢琴数据集上，其文本到音乐检索性能优于通用音乐联合嵌入模型，且其设计选择为其他同质化数据集的多模态表征学习提供了可重用的见解。&lt;h4&gt;结论&lt;/h4&gt;PianoBind成功解决了现有钢琴音乐表征模型的局限性，不仅适用于钢琴音乐，其设计原则还可扩展到其他同质化数据集的多模态表征学习。&lt;h4&gt;翻译&lt;/h4&gt;独奏钢琴音乐，尽管是单一乐器媒介，却具有显著的表现力，能跨越流派、情绪和风格传达丰富的语义信息。然而，当前主要在大规模数据集上训练的通用音乐表征模型，往往难以在同质化的独奏钢琴音乐中捕捉细微的语义差异。此外，现有的钢琴专用表征模型通常是单模态的，无法通过音频、符号和文本模态捕捉钢琴音乐固有的多模态特性。为解决这些局限性，我们提出了PianoBind，一个钢琴专用的多模态联合嵌入模型。我们在联合嵌入框架内系统研究了多源训练和模态利用策略，针对在(1)小规模和(2)同质化钢琴数据集中捕捉细粒度语义差异进行了优化。我们的实验结果表明，PianoBind学习到的多模态表征能有效捕捉钢琴音乐的细微差别，在领域内和领域外的钢琴数据集上，其文本到音乐检索性能优于通用音乐联合嵌入模型。此外，我们的设计选择为其他钢琴音乐之外的同质化数据集的多模态表征学习提供了可重用的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solo piano music, despite being a single-instrument medium, possessessignificant expressive capabilities, conveying rich semantic information acrossgenres, moods, and styles. However, current general-purpose musicrepresentation models, predominantly trained on large-scale datasets, oftenstruggle to captures subtle semantic distinctions within homogeneous solo pianomusic. Furthermore, existing piano-specific representation models are typicallyunimodal, failing to capture the inherently multimodal nature of piano music,expressed through audio, symbolic, and textual modalities. To address theselimitations, we propose PianoBind, a piano-specific multimodal joint embeddingmodel. We systematically investigate strategies for multi-source training andmodality utilization within a joint embedding framework optimized for capturingfine-grained semantic distinctions in (1) small-scale and (2) homogeneous pianodatasets. Our experimental results demonstrate that PianoBind learns multimodalrepresentations that effectively capture subtle nuances of piano music,achieving superior text-to-music retrieval performance on in-domain andout-of-domain piano datasets compared to general-purpose music joint embeddingmodels. Moreover, our design choices offer reusable insights for multimodalrepresentation learning with homogeneous datasets beyond piano music.</description>
      <author>example@mail.com (Hayeon Bang, Eunjin Choi, Seungheon Doh, Juhan Nam)</author>
      <guid isPermaLink="false">2509.04215v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>FedQuad: Federated Stochastic Quadruplet Learning to Mitigate Data Heterogeneity</title>
      <link>http://arxiv.org/abs/2509.04107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The 3rd IEEE International Conference on Federated Learning  Technologies and Applications (FLTA25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FedQuad的新方法，用于解决联邦学习中因数据异构性（特别是数据集规模小和类别不平衡）导致的全局模型泛化能力下降问题。通过优化客户端间较小的类内方差和较大的类间方差，FedQuad减少了模型聚合对全局模型的负面影响，在CIFAR-10和CIFAR-100数据集上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;联邦学习(FL)提供了一种去中心化的模型训练方法，能够有效处理分布式数据和隐私保护问题。然而，客户端间的数据异构性常常导致全局模型的泛化能力面临挑战，当数据集规模有限且存在类别不平衡时，这一挑战变得更加突出。&lt;h4&gt;目的&lt;/h4&gt;解决联邦学习中由于数据异构性（特别是数据集规模小和类别不平衡）导致的全局模型泛化能力下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为FedQuad的新方法，该方法明确优化客户端间较小的类内方差和较大的类间方差，从而减少模型聚合对全局模型的负面影响。通过最小化相似样本对的距离，同时最大化负样本对的距离，有效地在共享特征空间中分离客户端数据。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10和CIFAR-100数据集上，在各种数据分布和多个客户端的情况下，FedQuad方法相比现有方法表现出优越的性能。此外，研究还提供了基于度量的学习策略在监督学习和联邦学习范式中的详细分析，突显了它们在解决联邦环境中表示学习挑战方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;FedQuad方法通过解决数据异构性问题，有效提高了联邦学习中的全局模型泛化能力，特别是在数据集规模小和类别不平衡的情况下。该研究还强调了基于度量的学习策略在联邦学习中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习(FL)提供了一种去中心化的模型训练方法，能够有效处理分布式数据和隐私保护问题。然而，客户端间的数据异构性常常导致全局模型的泛化能力面临挑战。当数据集规模有限且存在类别不平衡时，这一挑战变得更加突出。为了解决数据异构性问题，我们提出了一种名为FedQuad的新方法，该方法明确优化客户端间较小的类内方差和较大的类间方差，从而减少模型聚合对全局模型的负面影响。我们的方法通过最小化相似样本对的距离，同时最大化负样本对的距离，有效地在共享特征空间中分离客户端数据。我们在各种数据分布和多个客户端的情况下，在CIFAR-10和CIFAR-100数据集上评估了我们的方法，展示了相比现有方法的优越性能。此外，我们还提供了基于度量的学习策略在监督学习和联邦学习范式中的详细分析，突显了它们在解决联邦环境中表示学习挑战方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) provides decentralised model training, whicheffectively tackles problems such as distributed data and privacy preservation.However, the generalisation of global models frequently faces challenges fromdata heterogeneity among clients. This challenge becomes even more pronouncedwhen datasets are limited in size and class imbalance. To address dataheterogeneity, we propose a novel method, \textit{FedQuad}, that explicitlyoptimises smaller intra-class variance and larger inter-class variance acrossclients, thereby decreasing the negative impact of model aggregation on theglobal model over client representations. Our approach minimises the distancebetween similar pairs while maximising the distance between negative pairs,effectively disentangling client data in the shared feature space. We evaluateour method on the CIFAR-10 and CIFAR-100 datasets under various datadistributions and with many clients, demonstrating superior performancecompared to existing approaches. Furthermore, we provide a detailed analysis ofmetric learning-based strategies within both supervised and federated learningparadigms, highlighting their efficacy in addressing representational learningchallenges in federated settings.</description>
      <author>example@mail.com (Ozgu Goksu, Nicolas Pugeault)</author>
      <guid isPermaLink="false">2509.04107v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>MedVista3D: Vision-Language Modeling for Reducing Diagnostic Errors in 3D CT Disease Detection, Understanding and Reporting</title>
      <link>http://arxiv.org/abs/2509.03800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MedVista3D是一种用于3D CT分析的多尺度语义增强视觉-语言预训练框架，通过局部和全局图像-文本对齐以及语义感知对齐，实现了联合疾病检测和整体解释，在多项任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;放射诊断中的错误（如漏读、注意力盲点和沟通失败）在临床实践中普遍存在，这些问题源于漏掉的局部异常、有限的全球背景和报告语言的变异性。在3D成像中，这些挑战被放大，因为临床医生必须检查每个扫描的数百个切片。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时满足精确局部检测、全局体积级推理和语义一致自然语言报告需求的系统，以解决放射诊断中的错误问题。&lt;h4&gt;方法&lt;/h4&gt;提出MedVista3D，一个多尺度语义增强视觉-语言预训练框架，通过执行局部和全局图像-文本对齐进行细粒度表示学习，并应用语言模型重写和引入放射学语义匹配库来解决报告变异性问题。&lt;h4&gt;主要发现&lt;/h4&gt;MedVista3D在零样本疾病分类、报告检索和医疗视觉问答方面取得了最先进的性能，并且能够很好地迁移到器官分割和预后预测任务。&lt;h4&gt;结论&lt;/h4&gt;MedVista3D是一个有效的框架，能够解决放射诊断中的错误问题，通过结合局部和全局理解以及语义一致的自然语言报告，提高了诊断的准确性。&lt;h4&gt;翻译&lt;/h4&gt;放射诊断错误——包括漏读错误、注意力盲点和沟通失败——在临床实践中仍然普遍存在。这些问题通常源于漏掉的局部异常、有限的全球背景和报告语言的变异性。在3D成像中，这些挑战被放大，因为临床医生必须检查每个扫描的数百个切片。解决这些问题需要具有精确局部检测、全局体积级推理和语义一致自然语言报告的系统。然而，现有的3D视觉-语言模型无法同时满足这三个需求，它们缺乏空间推理的局部-全局理解，并且难以处理未整理的放射报告的变异性和噪声。我们提出了MedVista3D，这是一个用于3D CT分析的多尺度语义增强视觉-语言预训练框架。为了实现联合疾病检测和整体解释，MedVista3D在完整体积上下文中执行局部和全局图像-文本对齐，以便进行细粒度表示学习。为了解决报告变异性问题，我们应用了语言模型重写，并引入了放射学语义匹配库，用于语义感知对齐。MedVista3D在零样本疾病分类、报告检索和医疗视觉问答方面取得了最先进的性能，并且能够很好地迁移到器官分割和预后预测。代码和数据集将发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决放射学诊断中的三类错误：漏读错误（遗漏视野内的异常）、注意力盲点（忽略全局背景中的病变）和沟通失败（报告表述不一致或模糊）。这些问题在现实中非常重要，因为放射学诊断错误仍然普遍存在，会对患者健康造成持续威胁。特别是在3D成像中，医生需要检查数百个切片，这些挑战被放大，需要系统能够同时进行精确局部检测、全局理解和一致报告。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D视觉语言模型的局限性，发现它们无法同时处理局部和全局信息，且难以处理报告中的语言变异性。作者借鉴了现有工作但进行了改进：分析了CT-CLIP（全局模型）和fVLM（局部模型）的优缺点；借鉴了多尺度对齐思想但结合了更精细的器官分割掩码；改进了使用LLM重写报告的方法，并引入了放射学语义匹配银行来解决文本变异性问题。作者通过理论分析证明多尺度对齐能捕获更多互信息，从而设计出更有效的模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多尺度语义增强的视觉语言预训练框架，同时实现局部疾病检测和全局图像理解，并解决报告语言一致性问题。整体流程包括：1）多尺度对齐：使用双路径视觉编码器同时处理全局CT体积和局部器官，通过多尺度对比损失对齐图像和文本；2）语义增强：用LLM重写报告强调疾病存在/不存在，建立放射学语义匹配银行(RSMB)检索语义相似描述；3）最终目标结合多尺度对齐和语义对齐，使模型能同时关注细节和全局，并生成一致报告。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）多尺度对齐损失，同时捕获局部和全局信息；2）放射学语义匹配银行(RSMB)，解决报告语言变异性；3）双路径视觉编码器，处理全局和局部信息并保留空间关系；4）LLM重写报告，使表述更标准化。相比之前工作：不同于全局模型（如CT-CLIP）的漏读问题，MedVista3D能检测小异常；不同于局部模型（如fVLM）的注意力盲点问题，能理解全局背景；使用分割掩码而非粗略边界框实现更精确定位；不仅关注疾病查询，还考虑查询上下文和完整性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MedVista3D通过多尺度语义对齐的视觉语言模型，有效减少了放射学诊断中的漏读错误、注意力盲点和沟通失败，同时实现了精确的局部疾病检测、全局图像理解和一致的疾病报告。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Radiologic diagnostic errors-under-reading errors, inattentional blindness,and communication failures-remain prevalent in clinical practice. These issuesoften stem from missed localized abnormalities, limited global context, andvariability in report language. These challenges are amplified in 3D imaging,where clinicians must examine hundreds of slices per scan. Addressing themrequires systems with precise localized detection, global volume-levelreasoning, and semantically consistent natural language reporting. However,existing 3D vision-language models are unable to meet all three needs jointly,lacking local-global understanding for spatial reasoning and struggling withthe variability and noise of uncurated radiology reports. We presentMedVista3D, a multi-scale semantic-enriched vision-language pretrainingframework for 3D CT analysis. To enable joint disease detection and holisticinterpretation, MedVista3D performs local and global image-text alignment forfine-grained representation learning within full-volume context. To addressreport variability, we apply language model rewrites and introduce a RadiologySemantic Matching Bank for semantics-aware alignment. MedVista3D achievesstate-of-the-art performance on zero-shot disease classification, reportretrieval, and medical visual question answering, while transferring well toorgan segmentation and prognosis prediction. Code and datasets will bereleased.</description>
      <author>example@mail.com (Yuheng Li, Yenho Chen, Yuxiang Lai, Jike Zhong, Vanessa Wildman, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2509.03800v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>What Fundamental Structure in Reward Functions Enables Efficient Sparse-Reward Learning?</title>
      <link>http://arxiv.org/abs/2509.03790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了奖励函数的基本性质如何促进高效的稀疏奖励强化学习，通过奖励矩阵的低秩结构视角，揭示了这种结构能导致从指数级到多项式样本复杂性的显著转变。&lt;h4&gt;背景&lt;/h4&gt;稀疏奖励强化学习面临样本效率低的挑战，需要理解奖励函数的基本性质来提高学习效率。&lt;h4&gt;目的&lt;/h4&gt;研究奖励函数的基本性质如何促进高效的稀疏奖励强化学习，探索低秩结构对样本复杂性的影响。&lt;h4&gt;方法&lt;/h4&gt;引入策略感知矩阵补全(PAMC)，通过策略相关采样分析将矩阵补全理论与强化学习联系起来，提供四种框架应用：一般稀疏奖励观测的不可能性结果、无奖励表征学习、分布无关置信集和鲁棒补全保证。&lt;h4&gt;主要发现&lt;/h4&gt;低秩结构在奖励矩阵中会导致从指数级到多项式样本复杂性的急剧转变；在100个系统性采样的领域评估中，超过一半存在可利用的结构；PAMC相比基线方法将样本效率提高了1.6到2.1倍，同时仅增加约20%的计算开销。&lt;h4&gt;结论&lt;/h4&gt;结构化奖励学习是一个有前景的新范式，对机器人技术、医疗保健和其他安全关键、样本密集型应用有直接影响。&lt;h4&gt;翻译&lt;/h4&gt;奖励函数的哪些基本性质能够促进高效的稀疏奖励强化学习？我们通过奖励矩阵中的低秩结构视角来解决这个问题，表明这种结构会导致从指数级到多项式样本复杂性的急剧转变，这是稀疏奖励强化学习领域的首个此类结果。我们引入了策略感知矩阵补全(PAMC)，通过新的策略相关采样分析将矩阵补全理论与强化学习联系起来。我们的框架提供了：(i)一般稀疏奖励观测的不可能性结果，(ii)从动态中进行的无奖励表征学习，(iii)通过保形预测实现的分布无关置信集，以及(iv)当低秩结构仅为近似时的鲁棒补全保证。实验上，我们在100个系统性采样的领域进行了预注册评估，发现超过一半的领域存在可利用的结构。与强大的探索、结构和表征学习基线相比，PAMC将样本效率提高了1.6到2.1倍，同时仅增加了约20%的计算开销。这些结果将结构化奖励学习确立为一个有前景的新范式，对机器人技术、医疗保健和其他安全关键、样本密集型应用有直接影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; What fundamental properties of reward functions enable efficientsparse-reward reinforcement learning? We address this question through the lensof low-rank structure in reward matrices, showing that such structure induces asharp transition from exponential to polynomial sample complexity, the firstresult of this kind for sparse-reward RL. We introduce Policy-Aware MatrixCompletion (PAMC), which connects matrix completion theory with reinforcementlearning via a new analysis of policy-dependent sampling. Our frameworkprovides: (i) impossibility results for general sparse reward observation, (ii)reward-free representation learning from dynamics, (iii) distribution-freeconfidence sets via conformal prediction, and (iv) robust completion guaranteesthat degrade gracefully when low-rank structure is only approximate.Empirically, we conduct a pre-registered evaluation across 100 systematicallysampled domains, finding exploitable structure in over half. PAMC improvessample efficiency by factors between 1.6 and 2.1 compared to strongexploration, structured, and representation-learning baselines, while addingonly about 20 percent computational overhead.These results establish structuralreward learning as a promising new paradigm, with immediate implications forrobotics, healthcare, and other safety-critical, sample-expensive applications.</description>
      <author>example@mail.com (Ibne Farabi Shihab, Sanjeda Akter, Anuj Sharma)</author>
      <guid isPermaLink="false">2509.03790v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Multi Attribute Bias Mitigation via Representation Learning</title>
      <link>http://arxiv.org/abs/2509.03616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ECAI 2025 (28th European Conference on Artificial Intelligence)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为广义多偏见缓解（GMBM）的框架，用于解决真实世界图像中多种重叠偏见的问题，提高视觉模型的鲁棒性和公平性。&lt;h4&gt;背景&lt;/h4&gt;真实世界的图像经常表现出多种重叠的偏见，包括纹理、水印、性别化妆、场景对象配对等。这些偏见共同损害了现代视觉模型的性能，影响了它们的鲁棒性和公平性。单独处理这些偏见是不够的，因为减轻一种偏见往往会允许或加剧其他偏见。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时处理多种重叠偏见的框架，提高视觉模型在复杂场景下的鲁棒性和公平性。&lt;h4&gt;方法&lt;/h4&gt;提出广义多偏见缓解（GMBM）框架，这是一个两阶段方法：1）自适应偏见集成学习（ABIL）通过训练特定属性的编码器并集成到主网络中，使分类器明确识别这些偏见；2）梯度抑制微调从骨干网络梯度中修剪这些偏见方向，形成单一紧凑网络。同时引入了缩放偏见放大（SBA）作为新的评估指标，将模型诱导的偏见放大与分布差异分离开来。&lt;h4&gt;主要发现&lt;/h4&gt;在FB CMNIST、CelebA和COCO数据集上验证显示，GMBM提高了最差组的准确性，将多属性偏见放大减半，并在偏见复杂性和分布偏移加剧的情况下设定了新的SBA低点。&lt;h4&gt;结论&lt;/h4&gt;GMBM是视觉识别中第一个实用的、端到端的多偏见解决方案，能够有效处理复杂场景中的多种重叠偏见问题。&lt;h4&gt;翻译&lt;/h4&gt;真实世界的图像经常表现出多种重叠的偏见，包括纹理、水印、性别化妆、场景对象配对等。这些偏见共同损害了现代视觉模型的性能，影响了它们的鲁棒性和公平性。单独解决这些偏见证明是不够的，因为减轻一种偏见往往会允许或加剧其他偏见。我们通过广义多偏见缓解（GMBM）解决这一多偏见问题，这是一个简洁的两阶段框架，只需要在训练时使用组标签，并在测试时最小化偏见。首先，自适应偏见集成学习（ABIL）通过为每个属性训练编码器并将它们与主骨干网络集成，故意识别已知捷径的影响，迫使分类器明确识别这些偏见。然后，梯度抑制微调从骨干网络的梯度中修剪这些偏见方向，留下一个单一的紧凑网络，忽略它刚刚学会识别的所有捷径。此外，我们发现现有的偏见指标在子组不平衡和训练-测试分布偏移的情况下会失效，因此我们引入了缩放偏见放大（SBA）：一种测试时度量，将模型诱导的偏见放大与分布差异分离开来。我们在FB CMNIST、CelebA和COCO上验证了GMBM，提高了最差组的准确性，将多属性偏见放大减半，并在偏见复杂性和分布偏移加剧的情况下设定了新的SBA低点，使GMBM成为视觉识别中第一个实用的、端到端的多偏见解决方案。项目页面：http://visdomlab.github.io/GMBM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real world images frequently exhibit multiple overlapping biases, includingtextures, watermarks, gendered makeup, scene object pairings, etc. These biasescollectively impair the performance of modern vision models, undermining boththeir robustness and fairness. Addressing these biases individually provesinadequate, as mitigating one bias often permits or intensifies others. Wetackle this multi bias problem with Generalized Multi Bias Mitigation (GMBM), alean two stage framework that needs group labels only while training andminimizes bias at test time. First, Adaptive Bias Integrated Learning (ABIL)deliberately identifies the influence of known shortcuts by training encodersfor each attribute and integrating them with the main backbone, compelling theclassifier to explicitly recognize these biases. Then Gradient Suppression FineTuning prunes those very bias directions from the backbone's gradients, leavinga single compact network that ignores all the shortcuts it just learned torecognize. Moreover we find that existing bias metrics break under subgroupimbalance and train test distribution shifts, so we introduce Scaled BiasAmplification (SBA): a test time measure that disentangles model induced biasamplification from distributional differences. We validate GMBM on FB CMNIST,CelebA, and COCO, where we boost worst group accuracy, halve multi attributebias amplification, and set a new low in SBA even as bias complexity anddistribution shifts intensify, making GMBM the first practical, end to endmultibias solution for visual recognition. Project page:http://visdomlab.github.io/GMBM/</description>
      <author>example@mail.com (Rajeev Ranjan Dwivedi, Ankur Kumar, Vinod K Kurmi)</author>
      <guid isPermaLink="false">2509.03616v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Teacher-Student Model for Detecting and Classifying Mitosis in the MIDOG 2025 Challenge</title>
      <link>http://arxiv.org/abs/2509.03614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 1 figures, final submission for MIDOG 2025 challenge&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于教师-学生模型的统一框架，通过像素级分割和分类相结合的方法，解决了有丝分裂检测中的领域迁移和数据不平衡问题，实现了较高的检测和分类性能。&lt;h4&gt;背景&lt;/h4&gt;计数有丝分裂图像对病理学家来说是耗时的，并且会导致观察者之间的变异性。人工智能(AI)有望通过自动检测有丝分裂同时保持决策一致性来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;将有丝分裂检测制定为像素级分割问题，并提出一种教师-学生模型，同时解决有丝分裂检测(赛道1)和非典型有丝分裂分类(赛道2)。&lt;h4&gt;方法&lt;/h4&gt;基于UNet分割主干网络，集成领域泛化模块(对比表示学习和领域对抗训练)。采用教师-学生策略生成像素级伪掩码，不仅用于注释的有丝分裂和困难负样本，也用于正常细胞核。引入多尺度CNN分类器，利用分割模型的特征图，在多任务学习范式中工作。&lt;h4&gt;主要发现&lt;/h4&gt;在初步测试集上，赛道1的算法F1得分为0.7660，赛道2的平衡准确率为0.8414，表明该方法在稳健有丝分裂分析中的有效性。&lt;h4&gt;结论&lt;/h4&gt;将基于分割的检测和分类整合到一个统一框架中，能够有效解决有丝分裂检测中的领域迁移和数据不平衡问题，提高检测和分类的准确性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;计数有丝分裂图像对病理学家来说是耗时的，并且会导致观察者之间的变异性。人工智能(AI)有望通过自动检测有丝分裂同时保持决策一致性来解决这个问题。然而，AI工具容易受到领域迁移的影响，由于训练集和测试集之间的差异，包括器官之间的形态差异、物种差异和染色方案变化，可能导致性能显著下降。此外，有丝分裂数量远少于正常细胞核数量，这为检测任务引入了严重的数据不平衡。在本工作中，我们将有丝分裂检测制定为像素级分割问题，并提出了一种教师-学生模型，同时解决有丝分裂检测(赛道1)和非典型有丝分裂分类(赛道2)。我们的方法基于UNet分割主干网络，集成了领域泛化模块，即对比表示学习和领域对抗训练。采用教师-学生策略生成像素级伪掩码，不仅用于注释的有丝分裂和困难负样本，也用于正常细胞核，从而增强特征区分能力并提高对领域迁移的鲁棒性。对于分类任务，我们引入了一个多尺度CNN分类器，在多任务学习范式中利用分割模型的特征图。在初步测试集上，算法在赛道1上实现了0.7660的F1分数，在赛道2上实现了0.8414的平衡准确率，证明了将基于分割的检测和分类整合到统一框架中对稳健有丝分裂分析的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Counting mitotic figures is time-intensive for pathologists and leads tointer-observer variability. Artificial intelligence (AI) promises a solution byautomatically detecting mitotic figures while maintaining decision consistency.However, AI tools are susceptible to domain shift, where a significant drop inperformance can occur due to differences in the training and testing sets,including morphological diversity between organs, species, and variations instaining protocols. Furthermore, the number of mitoses is much less than thecount of normal nuclei, which introduces severely imbalanced data for thedetection task. In this work, we formulate mitosis detection as a pixel-levelsegmentation and propose a teacher-student model that simultaneously addressesmitosis detection (Track 1) and atypical mitosis classification (Track 2). Ourmethod is based on a UNet segmentation backbone that integrates domaingeneralization modules, namely contrastive representation learning anddomain-adversarial training. A teacher-student strategy is employed to generatepixel-level pseudo-masks not only for annotated mitoses and hard negatives butalso for normal nuclei, thereby enhancing feature discrimination and improvingrobustness against domain shift. For the classification task, we introduce amulti-scale CNN classifier that leverages feature maps from the segmentationmodel within a multi-task learning paradigm. On the preliminary test set, thealgorithm achieved an F1 score of 0.7660 in Track 1 and balanced accuracy of0.8414 in Track 2, demonstrating the effectiveness of integratingsegmentation-based detection and classification into a unified framework forrobust mitosis analysis.</description>
      <author>example@mail.com (Seungho Choe, Xiaoli Qin, Abubakr Shafique, Amanda Dy, Dimitri Androutsos, Susan Done, April Khademi)</author>
      <guid isPermaLink="false">2509.03614v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Vehicle-to-Infrastructure Collaborative Spatial Perception via Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2509.03837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IEEE GLOBECOM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种轻量级的鸟瞰图(BEV)注入连接器，用于改善车对基础设施(V2I)系统中通信链路质量指标的预测准确性。通过结合多模态大语言模型与三维空间理解能力，该方法在多种场景下显著提高了预测性能。&lt;h4&gt;背景&lt;/h4&gt;准确预测通信链路质量指标对V2I系统至关重要，可支持无缝切换、高效波束管理和可靠低延迟通信。现代车辆传感器数据的可用性增加促使使用多模态大语言模型，但这些模型缺乏三维空间理解能力。&lt;h4&gt;目的&lt;/h4&gt;克服MLLMs缺乏三维空间理解的限制，提高V2I系统中通信链路质量预测的准确性，特别是在恶劣环境下的性能表现。&lt;h4&gt;方法&lt;/h4&gt;提出一种轻量级的即插即用BEV注入连接器，收集邻近车辆传感数据构建环境BEV，并将其与自车输入融合以提供空间上下文。开发结合CARLA仿真器和MATLAB射线追踪的协同仿真环境，生成多模态数据并提取指令和真实响应进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;BEV注入框架在三个V2I链路预测任务(LoS与非LoS分类、链路可用性和阻塞预测)中一致提高了性能。与仅自车的基线相比，准确度指标宏平均提高高达13.9%，在雨天和夜间等挑战性条件下性能增益提高高达32.7%。&lt;h4&gt;结论&lt;/h4&gt;所提出的BEV注入框架有效提升了V2I链路预测性能，特别是在恶劣条件下表现出更强的鲁棒性，证明其在实际应用中的价值。&lt;h4&gt;翻译&lt;/h4&gt;准确预测通信链路质量指标对车对基础设施(V2I)系统至关重要，可实现无缝切换、高效波束管理和可靠的低延迟通信。现代车辆传感器数据日益增多促使使用多模态大语言模型，因其跨任务适应性和推理能力。然而，MLLMs本质上缺乏三维空间理解能力。为克服这一局限，提出了一种轻量级即插即用的鸟瞰图(BEV)注入连接器。在该框架中，通过收集邻近车辆的传感数据构建环境鸟瞰图。然后将此BEV表示与自车输入融合，为大型语言模型提供空间上下文。为支持真实的多模态学习，开发了结合CARLA仿真器和基于MATLAB的射线追踪的协同仿真环境，在不同场景中生成RGB、LiDAR、GPS和无线信号数据。指令和真实响应从射线追踪输出中程序化提取。在三个V2I链路预测任务上进行了广泛实验：视距(LoS)与非视距(NLoS)分类、链路可用性和阻塞预测。仿真结果表明，所提出的BEV注入框架在所有任务中一致提高了性能。结果表明，与仅自车的基线相比，该方法将准确度指标的宏平均提高了高达13.9%。结果还显示，在具有挑战性的雨天和夜间条件下，这种性能增益提高了高达32.7%，证实了该框架在不利环境中的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决车辆到基础设施（V2I）系统中的通信链路质量预测问题。这个问题在现实中非常重要，因为准确的链路质量预测可以实现无缝切换、高效的波束管理和可靠的低延迟通信，对于自动驾驶和6G网络等应用至关重要。随着现代车辆传感器数据的日益丰富，多模态大型语言模型（MLLMs）因其适应性和推理能力而受到关注，但它们缺乏三维空间理解能力，这是车辆通信和环境感知的关键限制。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性：特定任务的融合链难以扩展，LLMs缺乏空间感知能力，以及现有方法通常只关注单一任务。基于这些认识，作者设计了一个轻量级、即插即用的鸟瞰图（BEV）注入连接器，通过收集邻近车辆的传感数据构建环境BEV表示，并与自车输入融合。作者借鉴了多项现有工作：使用BEVFusion框架融合RGB和LiDAR数据，采用BEVFormer的时序自注意力机制处理时间序列，以及利用Q-Former蒸馏与指令相关的空间特征。同时，作者结合了CARLA模拟器和MATLAB射线追踪来生成训练数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多车辆协作感知扩展单个车辆的视野，解决盲点问题；使用鸟瞰图（BEV）表示统一不同车辆视角下的空间信息；将空间信息压缩为紧凑的令牌序列注入到冻结的大型语言模型中；同时保持大型语言模型和视觉编码器冻结，只训练轻量级的连接器。整体流程包括：1）各车辆收集RGB图像、LiDAR点云和GPS数据；2）自车使用BEVFusion生成BEV表示；3）协作者车辆生成局部BEV特征并转换到自车坐标系；4）融合所有协作者的BEV；5）使用Q-Former提取与指令相关的空间特征；6）将BEV令牌与视觉流融合后输入LLM生成预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）即插即用的BEV注入连接器，无需端到端重新训练；2）协作BEV融合用于链路质量预测，首次将多智能体协作纳入多模态LLM框架；3）专门的V2I MLLM数据集，结合CARLA模拟和MATLAB射线追踪。相比之前的工作，本文方法不依赖特定任务的融合链，解决了单车辆盲点问题，能够处理多个下游任务，并且在恶劣条件下表现出更强的鲁棒性。之前的解决方案难以扩展到新模态或任务，而本文方法通过轻量级连接器实现了更好的灵活性和性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于多模态大型语言模型的车辆到基础设施协同空间感知方法，通过轻量级的鸟瞰图注入连接器使冻结的大型语言模型获得三维空间理解能力，显著提高了通信链路质量预测的准确性和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of communication link quality metrics is essential forvehicle-to-infrastructure (V2I) systems, enabling smooth handovers, efficientbeam management, and reliable low-latency communication. The increasingavailability of sensor data from modern vehicles motivates the use ofmultimodal large language models (MLLMs) because of their adaptability acrosstasks and reasoning capabilities. However, MLLMs inherently lackthree-dimensional spatial understanding. To overcome this limitation, alightweight, plug-and-play bird's-eye view (BEV) injection connector isproposed. In this framework, a BEV of the environment is constructed bycollecting sensing data from neighboring vehicles. This BEV representation isthen fused with the ego vehicle's input to provide spatial context for thelarge language model. To support realistic multimodal learning, a co-simulationenvironment combining CARLA simulator and MATLAB-based ray tracing is developedto generate RGB, LiDAR, GPS, and wireless signal data across varied scenarios.Instructions and ground-truth responses are programmatically extracted from theray-tracing outputs. Extensive experiments are conducted across three V2I linkprediction tasks: line-of-sight (LoS) versus non-line-of-sight (NLoS)classification, link availability, and blockage prediction. Simulation resultsshow that the proposed BEV injection framework consistently improvedperformance across all tasks. The results indicate that, compared to anego-only baseline, the proposed approach improves the macro-average of theaccuracy metrics by up to 13.9%. The results also show that this performancegain increases by up to 32.7% under challenging rainy and nighttime conditions,confirming the robustness of the framework in adverse settings.</description>
      <author>example@mail.com (Kimia Ehsani, Walid Saad)</author>
      <guid isPermaLink="false">2509.03837v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Reg3D: Reconstructive Geometry Instruction Tuning for 3D Scene Understanding</title>
      <link>http://arxiv.org/abs/2509.03635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Reg3D，一种新颖的重建几何指令调优框架，通过引入几何感知监督解决了大型多模态模型在3D场景理解方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型(LMMs)在2D视觉理解方面取得了显著进展，但将其能力扩展到3D场景理解仍面临重大挑战。现有方法主要依赖纯文本监督，无法提供学习鲁棒3D空间表示所需的几何约束。&lt;h4&gt;目的&lt;/h4&gt;引入Reg3D框架，通过在训练过程中直接融入几何感知监督，解决现有方法在3D场景理解中的局限性。&lt;h4&gt;方法&lt;/h4&gt;Reg3D采用双监督范式，同时利用3D几何信息作为输入和学习目标。在双编码器架构中设计互补的物体级和帧级重建任务，强制几何一致性以促进空间推理能力的发展。核心见解是有效的3D理解需要重建底层几何结构，而不仅仅是描述它们。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanQA、Scan2Cap、ScanRefer和SQA3D数据集上的广泛实验表明，Reg3D带来了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;Reg3D建立了具有空间感知能力的多模态模型的新训练范式，为3D场景理解提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型多模态模型(LMMs)的快速发展已在2D视觉理解方面取得了显著进展；然而，将这些能力扩展到3D场景理解仍然是一个重大挑战。现有方法主要依赖纯文本监督，无法提供学习鲁棒3D空间表示所需的几何约束。在本文中，我们介绍了Reg3D，一种新颖的重建几何指令调优框架，通过在训练过程中直接引入几何感知监督来解决这一局限性。我们的关键见解是，有效的3D理解需要重建底层几何结构，而不仅仅是描述它们。与仅在输入级别注入3D信息的方法不同，Reg3D采用双监督范式，同时利用3D几何信息作为输入和明确的学习目标。具体而言，我们在双编码器架构中设计了互补的物体级和帧级重建任务，强制几何一致性以促进空间推理能力的发展。在ScanQA、Scan2Cap、ScanRefer和SQA3D上的广泛实验表明，Reg3D带来了显著的性能提升，建立了具有空间感知能力的多模态模型的新训练范式。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何将大型多模态模型(LMMs)在2D视觉理解方面的成功扩展到3D场景理解的问题。这个问题很重要，因为3D场景理解是人工智能的关键挑战，对于机器人导航、增强现实、自动驾驶等应用至关重要，而现有方法受限于2D归纳偏置，无法有效理解和推理3D空间关系。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，指出当前方法主要在输入层面注入3D信息但无法发展真正的3D空间感知能力。作者的关键见解是有效的3D理解需要重建底层几何结构而不仅仅是描述它们。作者借鉴了预训练的3D基础模型(如VGGT)和双编码器架构，但创新性地设计了双重监督范式，将3D几何信息同时作为输入和监督信号，并引入了对象级和帧级两种互补的重建任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是有效的3D理解需要重建底层几何结构，而非仅描述它们，通过将3D几何信息同时作为输入和监督信号来克服2D偏置。整体流程包括：1)双编码器架构(2D视觉编码器和3D几何编码器)；2)两种重建任务(对象级重建促进跨视图空间推理，帧级重建发展全面场景理解)；3)自适应帧采样策略；4)训练时遮蔽部分3D特征，要求模型基于2D视觉和未遮蔽3D信息重建遮蔽部分，结合文本损失和重建损失进行多任务训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出重建几何指令调准(Reg3D)框架；2)设计双重监督范式，将3D几何信息同时作为输入和监督信号；3)引入对象级和帧级两种互补重建任务；4)设计自适应帧采样策略。相比之前工作，Reg3D不仅将3D信息作为输入，还将其作为明确的监督信号；不仅依赖文本监督，还引入几何感知监督；不仅在输入层面注入3D信息，还在整个训练过程中整合3D几何约束；通过重建任务强制模型推理3D空间关系，而非仅描述它们。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Reg3D通过引入重建几何指令调准框架和双重监督范式，有效解决了大型多模态模型在3D场景理解中的2D偏置问题，显著提升了模型对3D空间关系的理解和推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Large Multimodal Models (LMMs) has led to remarkableprogress in 2D visual understanding; however, extending these capabilities to3D scene understanding remains a significant challenge. Existing approachespredominantly rely on text-only supervision, which fails to provide thegeometric constraints required for learning robust 3D spatial representations.In this paper, we introduce Reg3D, a novel Reconstructive Geometry InstructionTuning framework that addresses this limitation by incorporating geometry-awaresupervision directly into the training process. Our key insight is thateffective 3D understanding necessitates reconstructing underlying geometricstructures rather than merely describing them. Unlike existing methods thatinject 3D information solely at the input level, Reg3D adopts adual-supervision paradigm that leverages 3D geometric information both as inputand as explicit learning targets. Specifically, we design complementaryobject-level and frame-level reconstruction tasks within a dual-encoderarchitecture, enforcing geometric consistency to encourage the development ofspatial reasoning capabilities. Extensive experiments on ScanQA, Scan2Cap,ScanRefer, and SQA3D demonstrate that Reg3D delivers substantial performanceimprovements, establishing a new training paradigm for spatially awaremultimodal models.</description>
      <author>example@mail.com (Hongpei Zheng, Lintao Xiang, Qijun Yang, Qian Lin, Hujun Yin)</author>
      <guid isPermaLink="false">2509.03635v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Space Is Rocket Science -- Only Top Reasoning Models Can Solve Spatial Understanding Tasks</title>
      <link>http://arxiv.org/abs/2509.02175v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RocketScience是一个开源的对比VLM基准测试，用于测试空间关系理解能力，由新的真实世界图像-文本对组成，主要涵盖相对空间理解和对象顺序。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在空间关系理解方面存在明显不足，需要一个专门的基准测试来评估这一能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个对人类容易但对当前VLMs具有挑战性的基准测试，专门评估空间关系理解能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个由全新真实世界图像-文本对组成的基准测试，专注于相对空间理解和对象顺序，并进行了解缠分析以分离对象定位和空间推理的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;开源和前沿商业VLMs在空间关系理解方面表现明显不足，而推理模型表现出令人惊讶的高性能；基准测试的性能瓶颈在于空间推理能力，而非对象定位能力。&lt;h4&gt;结论&lt;/h4&gt;当前VLMs在空间关系理解方面存在显著缺陷，需要进一步改进这一能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了RocketScience，一个开源的对比VLM基准测试，用于测试空间关系理解能力。它完全由新的真实世界图像-文本对组成，主要涵盖相对空间理解和对象顺序。该基准测试设计为对人类很容易，但对当前一代VLMs来说很难，这一点已得到经验验证。我们的结果表明，开源和前沿商业VLMs在空间关系理解方面明显缺乏，而推理模型表现出令人惊讶的高性能。此外，我们进行了解缠分析，以分离基于思维链模型中的对象定位和空间推理的贡献，发现基准测试的性能瓶颈在于空间推理，而不是对象定位能力。我们以CC-BY-4.0许可证发布数据集，并在https://github.com/nilshoehing/rocketscience提供评估代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose RocketScience, an open-source contrastive VLM benchmark that testsfor spatial relation understanding. It is comprised of entirely new real-worldimage-text pairs covering mostly relative spatial understanding and the orderof objects. The benchmark is designed to be very easy for humans and hard forthe current generation of VLMs, and this is empirically verified. Our resultsshow a striking lack of spatial relation understanding in open source andfrontier commercial VLMs and a surprisingly high performance of reasoningmodels. Additionally, we perform a disentanglement analysis to separate thecontributions of object localization and spatial reasoning inchain-of-thought-based models and find that the performance on the benchmark isbottlenecked by spatial reasoning and not object localization capabilities. Werelease the dataset with a CC-BY-4.0 license and make the evaluation codeavailable at: https://github.com/nilshoehing/rocketscience</description>
      <author>example@mail.com (Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque)</author>
      <guid isPermaLink="false">2509.02175v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>HLG: Comprehensive 3D Room Construction via Hierarchical Layout Generation</title>
      <link>http://arxiv.org/abs/2508.17832v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HLG是一种创新的细粒度3D室内场景生成方法，通过分层布局和优化网络解决了现有方法在细节生成方面的不足，提高了生成场景的真实性和实用性。&lt;h4&gt;背景&lt;/h4&gt;真实的3D室内场景生成对虚拟现实、室内设计、具身智能和场景理解至关重要。现有方法在粗粒度家具布局方面取得了进展，但难以捕捉细粒度的物体放置，限制了生成环境的真实性和实用性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在细粒度3D场景生成方面的不足，提出一种能够生成更真实、更详细的3D室内场景的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了分层布局生成（HLG）方法，采用粗到细的分层方法优化场景布局；细粒度布局对齐模块通过垂直和水平解耦构建分层布局；可训练布局优化网络解决了放置问题，确保结构连贯且物理上合理的场景生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验证明了HLG方法的有效性，与现有方法相比，在生成真实室内场景方面表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了场景生成领域的发展，为需要详细3D环境的应用开辟了新的可能性，作者将在发表后发布代码以鼓励未来研究。&lt;h4&gt;翻译&lt;/h4&gt;真实的3D室内场景生成对虚拟现实、室内设计、具身智能和场景理解至关重要。虽然现有方法在粗粒度家具布置方面取得了进展，但它们难以捕捉细粒度的物体放置，限制了生成环境的真实性和实用性。这一差距阻碍了沉浸式虚拟体验和具身AI应用的详细场景理解。为解决这些问题，我们提出了分层布局生成（HLG），一种用于细粒度3D场景生成的新方法。HLG首次采用粗到细的分层方法，从大规模家具布置到精细物体排列来优化场景布局。具体而言，我们的细粒度布局对齐模块通过垂直和水平解耦构建分层布局，有效将复杂的3D室内场景分解为多个粒度级别。此外，我们的可训练布局优化网络解决了放置问题，如错误定位、方向错误和物体相交，确保结构连贯且物理上合理的场景生成。我们通过大量实验证明了我们方法的有效性，显示出与现有方法相比在生成真实室内场景方面的优越性能。这项工作推进了场景生成领域的发展，并为需要详细3D环境的应用开辟了新的可能性。我们将在发表后发布代码以鼓励未来的研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D室内场景生成方法在精细粒度物体放置方面的不足问题。现有方法能够生成大尺度家具布局，但难以精确处理小物体（如餐具、装饰品）的摆放，这限制了生成环境的真实性和实用性。这个问题在现实中很重要，因为精细的3D场景对虚拟现实体验、室内设计应用和具身AI系统的场景理解都至关重要，高质量的3D环境能提供更沉浸式的虚拟体验和更准确的场景表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有3D室内场景生成方法的局限性，结合基于生成模型和大型语言模型(LLM)的优势，设计了层次化布局生成方法。作者借鉴了现有生成模型的空间特征先验和LLM的语言理解能力，但发现现有层次化方法缺乏对小型物体放置的精细控制。因此，作者设计了一个从粗粒度到细粒度的生成框架，包括场景信息提取、粗粒度房间生成和层次化布局生成三个阶段，通过垂直和水平解耦将复杂场景分解为多个层次，并引入可训练的布局优化网络来解决放置问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; HLG的核心思想是通过层次化方式从粗粒度家具布置逐步细化到精细物体放置，将复杂3D室内场景分解为多个层次的粒度。整体流程分为三阶段：1)场景信息提取：使用GPT-4o从多模态输入中提取房间类型、家具类型和空间关系；2)粗粒度房间生成：构建基本房间结构和家具的初始布局；3)层次化布局生成：通过垂直解耦沿Z轴分解场景为独立层，水平解耦利用LLM构建场景图，最后使用可训练布局优化网络(TLO-Net)优化物体位置，确保物理合理性和输入一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)层次化布局生成方法，实现从粗到细的逐步细化；2)精细布局对齐模块，通过垂直和水平解耦处理复杂场景；3)可训练布局优化网络(TLO-Net)，使用图注意力网络和复合损失函数优化物体位置。相比之前工作，HLG能处理精细小物体放置，支持非矩形房间，使用可微优化而非规则约束，能处理多模态输入，并通过碰撞和稳定性损失确保物理合理性，而不仅仅是视觉上的合理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HLG通过层次化布局生成方法，解决了3D室内场景生成中精细物体放置的挑战，实现了从粗粒度家具布置到细粒度物体定位的高质量场景构建，为虚拟现实、室内设计和具身智能应用提供了更加真实和实用的3D环境。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Realistic 3D indoor scene generation is crucial for virtual reality, interiordesign, embodied intelligence, and scene understanding. While existing methodshave made progress in coarse-scale furniture arrangement, they struggle tocapture fine-grained object placements, limiting the realism and utility ofgenerated environments. This gap hinders immersive virtual experiences anddetailed scene comprehension for embodied AI applications. To address theseissues, we propose Hierarchical Layout Generation (HLG), a novel method forfine-grained 3D scene generation. HLG is the first to adopt a coarse-to-finehierarchical approach, refining scene layouts from large-scale furnitureplacement to intricate object arrangements. Specifically, our fine-grainedlayout alignment module constructs a hierarchical layout through vertical andhorizontal decoupling, effectively decomposing complex 3D indoor scenes intomultiple levels of granularity. Additionally, our trainable layout optimizationnetwork addresses placement issues, such as incorrect positioning, orientationerrors, and object intersections, ensuring structurally coherent and physicallyplausible scene generation. We demonstrate the effectiveness of our approachthrough extensive experiments, showing superior performance in generatingrealistic indoor scenes compared to existing methods. This work advances thefield of scene generation and opens new possibilities for applicationsrequiring detailed 3D environments. We will release our code upon publicationto encourage future research.</description>
      <author>example@mail.com (Xiping Wang, Yuxi Wang, Mengqi Zhou, Junsong Fan, Zhaoxiang Zhang)</author>
      <guid isPermaLink="false">2508.17832v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>LMAE4Eth: Generalizable and Robust Ethereum Fraud Detection by Exploring Transaction Semantics and Masked Graph Embedding</title>
      <link>http://arxiv.org/abs/2509.03939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LMAE4Eth的多视图学习框架，用于以太坊欺诈检测，通过融合交易语义、掩码图嵌入和专家知识，解决了现有方法在捕捉交易语义、学习判别性账户嵌入以及处理大规模数据方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前以太坊欺诈检测方法依赖于上下文无关的数值交易序列，无法捕捉账户交易的语义信息；以太坊交易记录的普遍同质性使得学习判别性账户嵌入变得困难；现有自监督图学习方法主要通过图重构学习节点表示，在欺诈账户检测等节点级任务上表现不佳，同时面临可扩展性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有以太坊欺诈检测方法的局限性，提出能够捕捉交易语义的框架，学习更具表现力的账户表示，提高节点级账户检测性能，并解决可扩展性问题。&lt;h4&gt;方法&lt;/h4&gt;提出LMAE4Eth多视图学习框架，包含：1)交易-标记对比语言模型(TxCLM)，将数值交易记录转换为语言表示；2)使用标记感知对比学习和掩码交易模型预训练目标学习高表现力账户表示；3)掩码账户图自编码器(MAGAE)，通过生成式自监督学习专注于重构账户节点特征；4)集成层邻居采样提高可扩展性；5)使用交叉注意力融合网络统一TxCLM和MAGAE的嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;在三个数据集上与21种基线方法进行评估，结果显示该方法在两个数据集上的F1分数比最佳基线方法高出10%以上。&lt;h4&gt;结论&lt;/h4&gt;LMAE4Eth框架通过融合交易语义和图学习方法，有效提高了以太坊欺诈检测的准确性，同时提出的层邻居采样方法解决了可扩展性问题，使该方法能够处理大规模数据。&lt;h4&gt;翻译&lt;/h4&gt;当前以太坊欺诈检测方法依赖于上下文无关的数值交易序列，无法捕捉账户交易的语义。此外，以太坊交易记录的普遍同质性使得学习判别性账户嵌入具有挑战性。而且，当前自监督图学习方法主要通过图重构来学习节点表示，导致在欺诈账户检测等节点级任务上表现不佳，同时这些方法还面临可扩展性挑战。为应对这些挑战，我们提出了LMAE4Eth，一个融合交易语义、掩码图嵌入和专家知识的多视图学习框架。我们首先提出交易-标记对比语言模型(TxCLM)，将上下文无关的数值交易记录转换为逻辑连贯的语言表示。为了清晰地区分账户间的语义差异，我们还使用标记感知的对比学习预训练目标结合掩码交易模型预训练目标，学习高表现力的账户表示。然后，我们提出使用生成式自监督学习的掩码账户图自编码器(MAGAE)，通过专注于重构账户节点特征实现卓越的节点级账户检测。为使MAGAE能够进行大规模训练，我们提出将层邻居采样集成到图中，在不降低训练质量的情况下将采样顶点数量减少数倍。最后，使用交叉注意力融合网络，我们统一了TxCLM和MAGAE的嵌入，以利用两者的优势。我们在三个数据集上对21种基线方法评估了我们的方法。实验结果表明，在两个数据集上，我们的方法的F1分数比最佳基线方法高出10%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current Ethereum fraud detection methods rely on context-independent,numerical transaction sequences, failing to capture semantic of accounttransactions. Furthermore, the pervasive homogeneity in Ethereum transactionrecords renders it challenging to learn discriminative account embeddings.Moreover, current self-supervised graph learning methods primarily learn noderepresentations through graph reconstruction, resulting in suboptimalperformance for node-level tasks like fraud account detection, while thesemethods also encounter scalability challenges. To tackle these challenges, wepropose LMAE4Eth, a multi-view learning framework that fuses transactionsemantics, masked graph embedding, and expert knowledge. We first propose atransaction-token contrastive language model (TxCLM) that transformscontext-independent numerical transaction records into logically cohesivelinguistic representations. To clearly characterize the semantic differencesbetween accounts, we also use a token-aware contrastive learning pre-trainingobjective together with the masked transaction model pre-training objective,learns high-expressive account representations. We then propose a maskedaccount graph autoencoder (MAGAE) using generative self-supervised learning,which achieves superior node-level account detection by focusing onreconstructing account node features. To enable MAGAE to scale for large-scaletraining, we propose to integrate layer-neighbor sampling into the graph, whichreduces the number of sampled vertices by several times without compromisingtraining quality. Finally, using a cross-attention fusion network, we unify theembeddings of TxCLM and MAGAE to leverage the benefits of both. We evaluate ourmethod against 21 baseline approaches on three datasets. Experimental resultsshow that our method outperforms the best baseline by over 10% in F1-score ontwo of the datasets.</description>
      <author>example@mail.com (Yifan Jia, Yanbin Wang, Jianguo Sun, Ye Tian, Peng Qian)</author>
      <guid isPermaLink="false">2509.03939v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Weakly-Supervised Learning of Dense Functional Correspondences</title>
      <link>http://arxiv.org/abs/2509.03893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICCV 2025. Project website:  https://dense-functional-correspondence.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于物体功能的密集对应关系建立方法，利用视觉-语言模型和密集对比学习来解决跨类别图像匹配问题。&lt;h4&gt;背景&lt;/h4&gt;建立图像对之间的密集对应关系对于形状重建和机器人操作等任务至关重要，但在不同类别之间匹配时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;推导密集功能对应的定义，并提出一种弱监督学习范式来解决跨类别图像匹配中的对应关系建立问题。&lt;h4&gt;方法&lt;/h4&gt;利用视觉-语言模型为多视图图像伪标记以获得功能部分，然后与像素对应的密集对比学习相结合，将功能和空间知识提炼到新模型中。&lt;h4&gt;主要发现&lt;/h4&gt;物体的功能可以指导对应关系的建立，因为实现特定功能的物体部分通常在形状和外观上具有相似性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成和真实评估数据集上表现优于基线解决方案，证明了其在密集功能对应任务上的优势。&lt;h4&gt;翻译&lt;/h4&gt;在图像对之间建立密集对应关系对于形状重建和机器人操作等任务至关重要。在跨不同类别匹配的挑战性场景下，物体的功能即物体对其他物体产生的影响，可以指导对应关系的建立。这是因为实现特定功能的物体部分通常在形状和外观上具有相似性。我们基于这一观察推导出密集功能对应的定义，并提出一种弱监督学习范式来解决预测任务。我们方法的主要见解是，可以利用视觉-语言模型为多视图图像伪标记以获得功能部分。然后将其与像素对应的密集对比学习相结合，将功能和空间知识提炼到能够建立密集功能对应的新模型中。此外，我们整理了合成和真实评估数据集作为任务基准。我们的结果表明，与包含现成自监督图像表示和基础视觉语言模型的基线解决方案相比，我们的方法具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing dense correspondences across image pairs is essential for taskssuch as shape reconstruction and robot manipulation. In the challenging settingof matching across different categories, the function of an object, i.e., theeffect that an object can cause on other objects, can guide how correspondencesshould be established. This is because object parts that enable specificfunctions often share similarities in shape and appearance. We derive thedefinition of dense functional correspondence based on this observation andpropose a weakly-supervised learning paradigm to tackle the prediction task.The main insight behind our approach is that we can leverage vision-languagemodels to pseudo-label multi-view images to obtain functional parts. We thenintegrate this with dense contrastive learning from pixel correspondences todistill both functional and spatial knowledge into a new model that canestablish dense functional correspondence. Further, we curate synthetic andreal evaluation datasets as task benchmarks. Our results demonstrate theadvantages of our approach over baseline solutions consisting of off-the-shelfself-supervised image representations and grounded vision language models.</description>
      <author>example@mail.com (Stefan Stojanov, Linan Zhao, Yunzhi Zhang, Daniel L. K. Yamins, Jiajun Wu)</author>
      <guid isPermaLink="false">2509.03893v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Training LLMs to be Better Text Embedders through Bidirectional Reconstruction</title>
      <link>http://arxiv.org/abs/2509.03020v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种增强大型语言模型文本嵌入能力的新方法，通过在对比学习前添加专门训练阶段，显著提升了模型在检索和重排序任务中的性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被探索作为文本嵌入器，但现有方法通常依赖最终token（如[EOS]）的嵌入，这些token并未经过专门训练来捕获整个上下文的语义，限制了其作为文本嵌入的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个新的训练阶段以丰富最终token嵌入的语义，提升大型语言模型在文本嵌入任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;在对比学习之前添加一个新训练阶段，采用双向生成重建任务（EBQ2D和EBD2Q），这些任务交替进行，锚定[EOS]嵌入并重建查询-文档对中的任一侧。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，额外的训练阶段显著提高了大型语言模型在大规模文本嵌入基准（MTEB）上的性能，在不同的LLM基础模型和规模上实现了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;通过添加专门的训练阶段，可以有效提升大型语言模型作为文本嵌入器的性能，特别是在检索和重排序任务中。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）越来越多地被探索作为强大的文本嵌入器。现有的基于LLM的文本嵌入方法通常利用最终token的嵌入，通常是像[EOS]这样的保留特殊token。然而，这些token并没有经过专门训练来捕获整个上下文的语义，限制了它们作为文本嵌入的能力，特别是在检索和重排序任务中。我们提出在对比学习之前添加一个新的训练阶段，以丰富最终token嵌入的语义。这个阶段采用双向生成重建任务，即EBQ2D（基于嵌入的查询到文档）和EBD2Q（基于嵌入的文档到查询），这些任务交替进行，以锚定[EOS]嵌入并重建查询-文档对中的任一侧。实验结果表明，我们的额外训练阶段显著提高了LLM在大规模文本嵌入基准（MTEB）上的性能，在不同的LLM基础模型和规模上实现了新的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have increasingly been explored as powerful textembedders. Existing LLM-based text embedding approaches often leverage theembedding of the final token, typically a reserved special token such as [EOS].However, these tokens have not been intentionally trained to capture thesemantics of the whole context, limiting their capacity as text embeddings,especially for retrieval and re-ranking tasks. We propose to add a new trainingstage before contrastive learning to enrich the semantics of the final tokenembedding. This stage employs bidirectional generative reconstruction tasks,namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-BasedDocument-to-Query), which interleave to anchor the [EOS] embedding andreconstruct either side of Query-Document pairs. Experimental resultsdemonstrate that our additional training stage significantly improves LLMperformance on the Massive Text Embedding Benchmark (MTEB), achieving newstate-of-the-art results across different LLM base models and scales.</description>
      <author>example@mail.com (Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin)</author>
      <guid isPermaLink="false">2509.03020v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>An Interactive Framework for Finding the Optimal Trade-off in Differential Privacy</title>
      <link>http://arxiv.org/abs/2509.04290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的差分隐私(DP)多目标优化方法，通过直接建模Pareto前沿和使用更高效的交互方式来学习用户偏好，从而在隐私保护和模型性能之间找到最优平衡。&lt;h4&gt;背景&lt;/h4&gt;差分隐私(DP)是隐私保护分析的标准，但在隐私保证和模型性能之间存在基本权衡。选择这种最优平衡是一个关键挑战。&lt;h4&gt;目的&lt;/h4&gt;解决标准多目标优化方法在差分隐私中的低效问题，提出一种能够直接利用问题独特结构的方法，以更少的计算成本和用户交互找到最优的隐私-准确性权衡。&lt;h4&gt;方法&lt;/h4&gt;首先理论上推导权衡的形状，直接高效地建模Pareto前沿；然后用更具信息量的交互替代成对比较，向用户展示假设的权衡曲线并让他们选择偏好的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;通过六个真实世界数据集上的实验，表明所提方法在差分隐私逻辑回归和深度迁移学习中，能够以显著更少的计算成本和用户交互收敛到最优的隐私-准确性权衡。&lt;h4&gt;结论&lt;/h4&gt;通过直接利用差分隐私问题的独特结构，所提出的方法能够更有效地解决隐私-准确性权衡问题，为实际应用提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;差分隐私(DP)是隐私保护分析的标准，它在隐私保证和模型性能之间引入了一种基本的权衡。选择最优平衡是一个关键挑战，可以将其表述为一个多目标优化(MOO)问题，即首先发现最优权衡的集合(Pareto前沿)，然后学习决策者对这些权衡的偏好。尽管已有大量关于交互式MOO的研究，但标准方法——使用通用代理建模目标函数并从简单的成对反馈中学习偏好——对于DP来说是低效的，因为它未能利用问题的独特结构：Pareto前沿上的一个点可以通过固定隐私水平最大化精度直接生成。受此特性启发，我们首先理论上推导了权衡的形状，这使我们能够直接且高效地建模Pareto前沿。为了解决偏好学习中的低效问题，我们用更具信息量的交互替代了成对比较。特别是，我们向用户展示假设的权衡曲线并要求他们选择偏好的权衡。我们在差分隐私逻辑回归和深度迁移学习上的实验，使用六个真实世界数据集，表明我们的方法以显著更少的计算成本和用户交互收敛到最优的隐私-准确性权衡，优于基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Differential privacy (DP) is the standard for privacy-preserving analysis,and introduces a fundamental trade-off between privacy guarantees and modelperformance. Selecting the optimal balance is a critical challenge that can beframed as a multi-objective optimization (MOO) problem where one firstdiscovers the set of optimal trade-offs (the Pareto front) and then learns adecision-maker's preference over them. While a rich body of work on interactiveMOO exists, the standard approach -- modeling the objective functions withgeneric surrogates and learning preferences from simple pairwise feedback -- isinefficient for DP because it fails to leverage the problem's unique structure:a point on the Pareto front can be generated directly by maximizing accuracyfor a fixed privacy level. Motivated by this property, we first derive theshape of the trade-off theoretically, which allows us to model the Pareto frontdirectly and efficiently. To address inefficiency in preference learning, wereplace pairwise comparisons with a more informative interaction. Inparticular, we present the user with hypothetical trade-off curves and ask themto pick their preferred trade-off. Our experiments on differentially privatelogistic regression and deep transfer learning across six real-world datasetsshow that our method converges to the optimal privacy-accuracy trade-off withsignificantly less computational cost and user interaction than baselines.</description>
      <author>example@mail.com (Yaohong Yang, Aki Rehn, Sammie Katt, Antti Honkela, Samuel Kaski)</author>
      <guid isPermaLink="false">2509.04290v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Crossing the Species Divide: Transfer Learning from Speech to Animal Sounds</title>
      <link>http://arxiv.org/abs/2509.04166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, uses dcase2025.sty, submitted to DCASE 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了自监督语音模型在生物声学检测和分类任务中的迁移学习能力。研究表明，像HuBERT、WavLM和XEUS这样的模型能够生成跨物种动物声音的丰富潜在表示，其性能可与专门的生物声学预训练模型相媲美，突显了基于语音的自监督学习作为推进生物声学研究的有效框架的潜力。&lt;h4&gt;背景&lt;/h4&gt;自监督语音模型在语音处理方面已经展示了令人印象深刻的表现，但它们在非语音数据上的有效性尚未得到充分探索。生物声学领域需要有效的声音分析工具，但专门为此领域训练的模型可能需要大量标注数据。&lt;h4&gt;目的&lt;/h4&gt;研究自监督语音模型在生物声学检测和分类任务中的迁移学习能力，评估这些模型在动物声音表示生成、时间信息处理、频率范围和噪声影响方面的表现。&lt;h4&gt;方法&lt;/h4&gt;研究使用了HuBERT、WavLM和XEUS等自监督语音模型，通过线性探测分析时间平均表示的特性，并扩展到考虑时间信息的其他下游架构，同时研究了频率范围和噪声对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;自监督语音模型能够生成跨物种动物声音的丰富潜在表示；这些模型在生物声学任务上的结果与微调的生物声学预训练模型具有竞争力；噪声鲁棒预训练设置对性能有显著影响。&lt;h4&gt;结论&lt;/h4&gt;基于语音的自监督学习作为一种有效框架，在推进生物声学研究方面具有巨大潜力，为生物声学分析提供了不需要大量标注数据的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;自监督语音模型在语音处理方面已经展示了令人印象深刻的表现，但它们在非语音数据上的有效性尚未得到充分探索。我们研究了此类模型在生物声学检测和分类任务中的迁移学习能力。我们表明，如HuBERT、WavLM和XEUS等模型能够生成跨物种动物声音的丰富潜在表示。我们通过在线性探测上对时间平均表示分析模型的特性。然后，我们扩展这种方法，使用其他下游架构来考虑时间信息的影响。最后，我们研究了频率范围和噪声对性能的影响。值得注意的是，我们的结果与微调的生物声学预训练模型具有竞争力，并展示了噪声鲁棒预训练设置的影响。这些发现突显了基于语音的自监督学习作为推进生物声学研究的有效框架的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised speech models have demonstrated impressive performance inspeech processing, but their effectiveness on non-speech data remainsunderexplored. We study the transfer learning capabilities of such models onbioacoustic detection and classification tasks. We show that models such asHuBERT, WavLM, and XEUS can generate rich latent representations of animalsounds across taxa. We analyze the models properties with linear probing ontime-averaged representations. We then extend the approach to account for theeffect of time-wise information with other downstream architectures. Finally,we study the implication of frequency range and noise on performance. Notably,our results are competitive with fine-tuned bioacoustic pre-trained models andshow the impact of noise-robust pre-training setups. These findings highlightthe potential of speech-based self-supervised learning as an efficientframework for advancing bioacoustic research.</description>
      <author>example@mail.com (Jules Cauzinille, Marius Miron, Olivier Pietquin, Masato Hagiwara, Ricard Marxer, Arnaud Rey, Benoit Favre)</author>
      <guid isPermaLink="false">2509.04166v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Chest X-ray Pneumothorax Segmentation Using EfficientNet-B4 Transfer Learning in a U-Net Architecture</title>
      <link>http://arxiv.org/abs/2509.03950v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 page, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种使用深度学习的自动化气胸检测方法，通过U-Net架构与EfficientNet-B4编码器相结合，能够在胸部X光片上准确识别气胸区域。&lt;h4&gt;背景&lt;/h4&gt;气胸是胸膜腔内异常积聚空气的病症，若未及时发现可能危及生命。胸部X光片是首选诊断工具，但小型气胸病例可能表现不明显，容易漏诊。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动化的深度学习管道，用于分割和识别胸部X光片中的气胸区域，辅助放射科医生进行诊断。&lt;h4&gt;方法&lt;/h4&gt;采用带有EfficientNet-B4编码器的U-Net架构，在SIIM-ACR数据集上进行训练，结合数据增强技术和二元交叉熵加Dice损失函数进行模型优化。&lt;h4&gt;主要发现&lt;/h4&gt;模型在独立的PTX-498数据集上取得了0.7008的交并比(IoU)和0.8241的Dice分数，显示出良好的气胸区域分割能力。&lt;h4&gt;结论&lt;/h4&gt;该深度学习模型能够准确定位气胸区域，可以为放射科医生提供有效的诊断支持，提高气胸检测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;气胸是胸膜腔内异常积聚空气的病症，若未被发现可能危及生命。胸部X光片是首选诊断工具，但小型病例可能表现不明显。我们提出了一种使用带有EfficientNet-B4编码器的U-Net的自动化深度学习管道，用于分割气胸区域。在SIIM-ACR数据集上训练，结合数据增强和二元交叉熵加Dice损失，该模型在独立的PTX-498数据集上达到了0.7008的IoU和0.8241的Dice分数。这些结果表明该模型可以准确定位气胸并为放射科医生提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pneumothorax, the abnormal accumulation of air in the pleural space, can belife-threatening if undetected. Chest X-rays are the first-line diagnostictool, but small cases may be subtle. We propose an automated deep-learningpipeline using a U-Net with an EfficientNet-B4 encoder to segment pneumothoraxregions. Trained on the SIIM-ACR dataset with data augmentation and a combinedbinary cross-entropy plus Dice loss, the model achieved an IoU of 0.7008 andDice score of 0.8241 on the independent PTX-498 dataset. These resultsdemonstrate that the model can accurately localize pneumothoraces and supportradiologists.</description>
      <author>example@mail.com (Alvaro Aranibar Roque, Helga Sebastian)</author>
      <guid isPermaLink="false">2509.03950v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning-Based CNN Models for Plant Species Identification Using Leaf Venation Patterns</title>
      <link>http://arxiv.org/abs/2509.03729v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了三种深度学习架构（ResNet50、MobileNetV2和EfficientNetB0）基于叶片脉图模式进行植物物种自动分类的有效性，发现EfficientNetB0表现最佳，测试准确率达94.67%&lt;h4&gt;背景&lt;/h4&gt;叶片脉图是一种具有高度分类学相关性的关键形态学特征，可用于植物物种分类&lt;h4&gt;目的&lt;/h4&gt;评估三种深度学习架构在基于叶片脉图模式的植物物种自动分类中的性能表现&lt;h4&gt;方法&lt;/h4&gt;使用瑞典叶片数据集（包含15个不同物种的1,125张图像），在训练和测试阶段使用标准性能指标评估ResNet50、MobileNetV2和EfficientNetB0三种模型&lt;h4&gt;主要发现&lt;/h4&gt;ResNet50训练准确率94.11%但存在过拟合，测试准确率降至88.45%；MobileNetV2测试准确率93.34%，F1得分93.23%，适合轻量级应用；EfficientNetB0测试准确率94.67%，各项指标均超过94.6%，表现最佳&lt;h4&gt;结论&lt;/h4&gt;深度学习，特别是EfficientNetB0，在开发可扩展且准确的基于叶脉特征的自动化植物分类工具方面具有巨大潜力&lt;h4&gt;翻译&lt;/h4&gt;本研究评估了三种深度学习架构（ResNet50、MobileNetV2和EfficientNetB0）基于叶片脉图模式进行植物物种自动分类的有效性，这是一种具有高度分类学相关性的关键形态学特征。使用包含15个不同物种（每个物种75张图像，总计1,125张图像）的瑞典叶片数据集，在训练和测试阶段使用标准性能指标评估了这些模型。ResNet50达到94.11%的训练准确率，但表现出过拟合，测试准确率降至88.45%，F1得分为87.82%。MobileNetV2展示了更好的泛化能力，测试准确率达到93.34%，F1得分为93.23%，表明其适合轻量级实时应用。EfficientNetB0表现优于其他两个模型，测试准确率达到94.67%，精确率、召回率和F1分数均超过94.6%，突显了其在基于脉图的分类中的鲁棒性。这些发现强调了深度学习，特别是EfficientNetB0，在开发利用脉图特征进行自动化植物分类的可扩展且准确工具方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study evaluates the efficacy of three deep learning architectures:ResNet50, MobileNetV2, and EfficientNetB0 for automated plant speciesclassification based on leaf venation patterns, a critical morphologicalfeature with high taxonomic relevance. Using the Swedish Leaf Datasetcomprising images from 15 distinct species (75 images per species, totalling1,125 images), the models were demonstrated using standard performance metricsduring training and testing phases. ResNet50 achieved a training accuracy of94.11% but exhibited overfitting, reflected by a reduced testing accuracy of88.45% and an F1 score of 87.82%. MobileNetV2 demonstrated bettergeneralization capabilities, attaining a testing accuracy of 93.34% and an F1score of 93.23%, indicating its suitability for lightweight, real-timeapplications. EfficientNetB0 outperformed both models, achieving a testingaccuracy of 94.67% with precision, recall, and F1 scores exceeding 94.6%,highlighting its robustness in venation-based classification. The findingsunderscore the potential of deep learning, particularly EfficientNetB0, indeveloping scalable and accurate tools for automated plant taxonomy usingvenation traits.</description>
      <author>example@mail.com (Bandita Bharadwaj, Ankur Mishra, Saurav Bharadwaj)</author>
      <guid isPermaLink="false">2509.03729v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-Assisted Correlation Clustering</title>
      <link>http://arxiv.org/abs/2509.03561v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in IEEE QAI 2025 conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合量子-经典方法用于关联聚类，通过量子退火解决二次无约束二元优化问题，实现了在带符号图中进行高质量节点分区。&lt;h4&gt;背景&lt;/h4&gt;关联聚类是一种基于图的无监督学习任务，旨在根据节点间的一致性和不一致性对图中的节点进行分区。传统方法在处理具有负边或不平衡簇大小的图时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理任意相关结构图（包括负边）的聚类方法，不依赖于度量假设或预定义的簇数量，同时提高聚类质量和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将GCS-Q量子辅助求解器调整为关联聚类任务，通过递归分裂分区来最大化簇内一致性。每个二分区步骤被编码为二次无约束二元优化问题，并通过量子退火解决。&lt;h4&gt;主要发现&lt;/h4&gt;在合成带符号图和真实世界高光谱成像数据上的实证评估表明，调整后的GCS-Q在真实数据上的鲁棒性和聚类质量以及在簇大小不平衡的场景中优于经典算法。&lt;h4&gt;结论&lt;/h4&gt;混合量子-经典优化在推进基于图的无监督学习中可扩展和结构感知的聚类技术方面具有潜力，特别是在处理复杂图结构时。&lt;h4&gt;翻译&lt;/h4&gt;这项工作引入了一种混合量子-经典方法用于关联聚类，这是一种基于图的无监督学习任务，旨在根据节点间的一致性和不一致性对图中的节点进行分区。特别是，我们将GCS-Q（一种最初为联盟结构生成设计的量子辅助求解器）调整为通过递归分裂分区来最大化带符号图中的簇内一致性。所提出的方法将每个二分区步骤编码为二次无约束二元优化问题，通过量子退火解决。这种在分层聚类框架中集成量子优化的方法能够处理具有任意相关结构的图，包括负边，而不依赖于度量假设或预定义的簇数量。在合成带符号图和真实世界高光谱成像数据上的实证评估表明，当为关联聚类而调整时，GCS-Q在真实数据上的鲁棒性和聚类质量以及在簇大小不平衡的场景中优于经典算法。我们的研究结果强调了混合量子-经典优化在推进基于图的无监督学习中可扩展和结构感知的聚类技术方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces a hybrid quantum-classical method to correlationclustering, a graph-based unsupervised learning task that seeks to partitionthe nodes in a graph based on pairwise agreement and disagreement. Inparticular, we adapt GCS-Q, a quantum-assisted solver originally designed forcoalition structure generation, to maximize intra-cluster agreement in signedgraphs through recursive divisive partitioning. The proposed method encodeseach bipartitioning step as a quadratic unconstrained binary optimizationproblem, solved via quantum annealing. This integration of quantum optimizationwithin a hierarchical clustering framework enables handling of graphs witharbitrary correlation structures, including negative edges, without relying onmetric assumptions or a predefined number of clusters. Empirical evaluations onsynthetic signed graphs and real-world hyperspectral imaging data demonstratethat, when adapted for correlation clustering, GCS-Q outperforms classicalalgorithms in robustness and clustering quality on real-world data and inscenarios with cluster size imbalance. Our results highlight the promise ofhybrid quantum-classical optimization for advancing scalable andstructurally-aware clustering techniques in graph-based unsupervised learning.</description>
      <author>example@mail.com (Antonio Macaluso, Supreeth Mysore Venkatesh, Diego Arenas, Matthias Klusch, Andreas Dengel)</author>
      <guid isPermaLink="false">2509.03561v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Can Language Models Handle a Non-Gregorian Calendar?</title>
      <link>http://arxiv.org/abs/2509.04432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统评估了开源语言模型处理非公历日历系统(特别是日本日历)的能力，发现即使是专门针对日语的模型在日历算术和跨日历一致性方面也存在困难，强调了开发具有特定文化日历理解能力的语言模型的重要性。&lt;h4&gt;背景&lt;/h4&gt;时间推理和知识是语言模型的重要能力。虽然先前的工作已经分析和改进了语言模型中的时间推理，但大多数研究仅关注公历。然而，许多非公历系统(如日本历、伊斯兰历和希伯来历)仍在积极使用，并反映了基于文化的时间概念。目前尚未评估当前语言模型能否准确处理这类非公历日历。&lt;h4&gt;目的&lt;/h4&gt;评估开源语言模型处理非公历日历系统(特别是日本日历)的能力，确定它们在需要时间知识和时间推理的任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;作者创建了四个需要时间知识和时间推理的任务的数据集，评估了一系列以英语为中心和以日语为中心的语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;一些模型可以执行日历转换，但即使是专门针对日语的模型在处理日本日历算术和保持跨日历一致性方面也存在困难。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了开发具有更好特定文化日历理解能力的语言模型的重要性。&lt;h4&gt;翻译&lt;/h4&gt;时间推理和知识是语言模型的基本能力。虽然先前的工作已经分析和改进了语言模型中的时间推理，但大多数研究仅关注公历。然而，许多非公历系统，如日本历、伊斯兰历和希伯来历，仍在积极使用，并反映了基于文化的时间概念。迄今为止，尚未评估当前语言模型能否准确处理这类非公历日历。在此，我们系统评估了开源语言模型处理一种非公历系统(日本历)的能力。为了评估，我们创建了四个需要时间知识和时间推理的任务数据集。评估了一系列以英语为中心和以日语为中心的语言模型后，我们发现一些模型可以执行日历转换，但即使是专门针对日语的模型在处理日本日历算术和保持跨日历一致性方面也存在困难。我们的研究结果强调了开发具有更好特定文化日历理解能力的语言模型的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal reasoning and knowledge are essential capabilities for languagemodels (LMs). While much prior work has analyzed and improved temporalreasoning in LMs, most studies have focused solely on the Gregorian calendar.However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrewcalendars, are in active use and reflect culturally grounded conceptions oftime. If and how well current LMs can accurately handle such non-Gregoriancalendars has not been evaluated so far. Here, we present a systematicevaluation of how well open-source LMs handle one such non-Gregorian system:the Japanese calendar. For our evaluation, we create datasets for four tasksthat require both temporal knowledge and temporal reasoning. Evaluating a rangeof English-centric and Japanese-centric LMs, we find that some models canperform calendar conversions, but even Japanese-centric models struggle withJapanese-calendar arithmetic and with maintaining consistency across calendars.Our results highlight the importance of developing LMs that are better equippedfor culture-specific calendar understanding.</description>
      <author>example@mail.com (Mutsumi Sasaki, Go Kamoda, Ryosuke Takahashi, Kosuke Sato, Kentaro Inui, Keisuke Sakaguchi, Benjamin Heinzerling)</author>
      <guid isPermaLink="false">2509.04432v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>TEn-CATS: Text-Enriched Audio-Visual Video Parsing with Multi-Scale Category-Aware Temporal Graph</title>
      <link>http://arxiv.org/abs/2509.04086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合Bi-Directional Text Fusion (BiT)模块和Category-Aware Temporal Graph (CATS)模块的方法，用于解决Audio-Visual Video Parsing (AVVP)任务中的噪声伪标签问题，实现了更精确的事件类别识别和时间定位。&lt;h4&gt;背景&lt;/h4&gt;Audio-Visual Video Parsing (AVVP)任务旨在识别给定视频中的事件类别及其发生时间，使用弱监督标签。现有方法通常分为两类：(i) 基于注意力机制设计增强架构以更好地进行时间建模；(ii) 生成更丰富的伪标签以补偿帧级注释的缺失。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中噪声伪标签被当作可靠监督信号以及初始错误在训练过程中被反复放大的问题。&lt;h4&gt;方法&lt;/h4&gt;结合Bi-Directional Text Fusion (BiT)模块和Category-Aware Temporal Graph (CATS)模块。通过BiT模块对音频和视觉模态特征进行语义注入和动态校准，定位和净化更清洁、更丰富的语义线索；利用CATS模块进行语义传播和连接，实现精确的语义信息在时间上的传播。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在两个基准数据集LLP和UnAV-100上的多个关键指标上实现了最先进的(SOTA)性能。&lt;h4&gt;结论&lt;/h4&gt;结合两种现有方法的优点，通过BiT模块净化语义线索，并通过CATS模块实现精确的语义传播，能够有效提高AVVP任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;视听视频解析任务旨在识别给定视频中的事件类别及其发生时间，并使用弱监督标签。现有方法通常分为两类：(i) 基于注意力机制设计增强架构以更好地进行时间建模，以及(ii) 生成更丰富的伪标签以补偿帧级注释的缺失。然而，第一类方法将噪声的段级伪标签视为可靠的监督信号，第二类方法让无差别的注意力扩散到所有帧上，导致初始错误在训练过程中被反复放大。为解决这个问题，我们提出了一种结合双向文本融合模块和类别感知时序图模块的方法。具体而言，我们整合了两个先前研究方向的优点和互补性。首先，通过BiT模块对音频和视觉模态特征进行语义注入和动态校准，以定位和净化更清洁、更丰富的语义线索。然后，我们利用CATS模块进行语义传播和连接，使精确的语义信息能够在时间上传播。实验结果表明，我们在两个基准数据集LLP和UnAV-100的多个关键指标上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Audio-Visual Video Parsing (AVVP) task aims to identify event categories andtheir occurrence times in a given video with weakly supervised labels. Existingmethods typically fall into two categories: (i) designing enhancedarchitectures based on attention mechanism for better temporal modeling, and(ii) generating richer pseudo-labels to compensate for the absence offrame-level annotations. However, the first type methods treat noisysegment-level pseudo labels as reliable supervision and the second type methodslet indiscriminate attention spread them across all frames, the initial errorsare repeatedly amplified during training. To address this issue, we propose amethod that combines the Bi-Directional Text Fusion (BiT) module andCategory-Aware Temporal Graph (CATS) module. Specifically, we integrate thestrengths and complementarity of the two previous research directions. We firstperform semantic injection and dynamic calibration on audio and visual modalityfeatures through the BiT module, to locate and purify cleaner and richersemantic cues. Then, we leverage the CATS module for semantic propagation andconnection to enable precise semantic information dissemination across time.Experimental results demonstrate that our proposed method achievesstate-of-the-art (SOTA) performance in multiple key indicators on two benchmarkdatasets, LLP and UnAV-100.</description>
      <author>example@mail.com (Yaru Chen, Faegheh Sardari, Peiliang Zhang, Ruohao Guo, Yang Xiang, Zhenbo Li, Wenwu Wang)</author>
      <guid isPermaLink="false">2509.04086v1</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Keye-VL-1.5通过三项创新解决了视频理解中的挑战：Slow-Fast视频编码策略、渐进式四阶段预训练方法和全面的微调流程，在视频理解和一般多模态任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;大语言模型已扩展到多模态任务形成MLLMs，但视频理解仍具挑战性，因为视频动态且信息密集。现有模型在处理视频内容时，在空间分辨率和时间覆盖之间难以平衡。&lt;h4&gt;目的&lt;/h4&gt;提出Keye-VL-1.5模型，解决视频理解中的基本挑战，提高视频处理能力。&lt;h4&gt;方法&lt;/h4&gt;引入Slow-Fast视频编码策略动态分配计算资源；实现四阶段预训练将上下文长度从8K扩展到128K；开发包含5步思维链数据构建、GSPO强化学习和对齐训练的微调流程。&lt;h4&gt;主要发现&lt;/h4&gt;Keye-VL-1.5在公共基准测试和人类评估中显著优于现有模型，尤其在视频理解任务上表现出色，同时在一般多模态基准上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三项创新有效解决了视频理解中的基本挑战，在视频理解和一般多模态任务上均表现优异。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展显著推进，通过多模态大语言模型扩展到多模态任务。然而，由于视频的动态和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，在空间分辨率和时间覆盖之间难以权衡。我们提出了Keye-VL-1.5，通过三项关键创新解决视频理解中的基本挑战。首先，我们引入了一种新颖的Slow-Fast视频编码策略，根据帧间相似性动态分配计算资源，以更高分辨率处理有显著视觉变化的关键帧，同时以更低分辨率但增加时间覆盖处理相对静态的帧。其次，我们实现了渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，能够处理更长的视频和更复杂的视觉内容。第三，我们开发了专注于推理增强和人类偏好对齐的全面微调流程，包含5步思维链数据构建过程，针对困难案例的迭代GSPO强化学习与渐进式提示提示，以及对齐训练。通过在公共基准上的广泛评估和严格的人类评估，Keye-VL-1.5显示出比现有模型显著的改进，特别是在视频理解任务上表现出色，同时在一般多模态基准上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Haonan Fang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>Safety-Critical Multi-Agent MCTS for Mixed Traffic Coordination at Unsignalized Roundabout</title>
      <link>http://arxiv.org/abs/2509.01856v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种安全关键的多智能体蒙特卡洛树搜索框架，用于解决自动驾驶车辆在无信号环岛的决策问题，特别是在混合交通环境中与人类驾驶车辆的协调问题。&lt;h4&gt;背景&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在混合交通环境中，自动驾驶车辆需要与人类驾驶车辆安全协调。&lt;h4&gt;目的&lt;/h4&gt;开发一个整合确定性和概率预测模型的安全关键多智能体蒙特卡洛树搜索框架，促进复杂环岛场景中的协作决策。&lt;h4&gt;方法&lt;/h4&gt;提出三个关键创新：(1)分层安全评估模块，通过动态安全阈值和时空风险评估处理AV-to-AV、AV-to-HDV和AV-to-Road交互；(2)自适应HDV行为预测方案，结合智能驾驶员模型和概率不确定性建模；(3)多目标奖励优化策略，联合考虑安全、效率和协作意图。&lt;h4&gt;主要发现&lt;/h4&gt;大量模拟验证了该方法在完全自主(100% AVs)和混合交通(50% AVs + 50% HDVs)条件下的有效性。与基准方法相比，框架减少了所有AV的轨迹偏差，显著降低了侵入后时间违规率，在完全自主场景中仅实现1.0%，在混合交通环境中实现3.2%。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在复杂环岛场景中能够有效实现安全、高效的协作决策。&lt;h4&gt;翻译&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在混合交通环境中，自动驾驶车辆必须与人类驾驶车辆安全协调。本文提出了一个安全关键的多智能体蒙特卡洛树搜索框架，整合了确定性和概率预测模型，以促进复杂环岛场景中的协作决策。所提出的框架引入了三个关键创新：(1)分层安全评估模块，通过动态安全阈值和时空风险评估系统处理AV-to-AV、AV-to-HDV和AV-to-Road交互；(2)自适应HDV行为预测方案，结合智能驾驶员模型和概率不确定性建模；(3)多目标奖励优化策略，联合考虑安全、效率和协作意图。大量模拟结果验证了所提出方法在完全自主(100% AVs)和混合交通(50% AVs + 50% HDVs)条件下的有效性。与基准方法相比，我们的框架减少了所有AV的轨迹偏差，显著降低了侵入后时间违规率，在完全自主场景中仅实现1.0%，在混合交通环境中实现3.2%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decision-making at unsignalized roundabouts poses substantial challenges forautonomous vehicles (AVs), particularly in mixed traffic environments where AVsmust coordinate safely with human-driven vehicles (HDVs). This paper presents asafety-critical multi-agent Monte Carlo Tree Search (MCTS) framework thatintegrates both deterministic and probabilistic prediction models to facilitatecooperative decision-making in complex roundabout scenarios. The proposedframework introduces three key innovations: (1) a hierarchical safetyassessment module that systematically addresses AV-to-AV (A2A), AV-to-HDV(A2H), and AV-to-Road (A2R) interactions through dynamic safety thresholds andspatiotemporal risk evaluation; (2) an adaptive HDV behavior prediction schemethat combines the Intelligent Driver Model (IDM) with probabilistic uncertaintymodeling; and (3) a multi-objective reward optimization strategy that jointlyconsiders safety, efficiency, and cooperative intent. Extensive simulationresults validate the effectiveness of the proposed approach under both fullyautonomous (100% AVs) and mixed traffic (50% AVs + 50% HDVs) conditions.Compared to benchmark methods, our framework consistently reduces trajectorydeviations across all AVs and significantly lowers the rate ofPost-Encroachment Time (PET) violations, achieving only 1.0\% in the fullyautonomous scenario and 3.2% in the mixed traffic setting.</description>
      <author>example@mail.com (Zhihao Lin, Shuo Liu, Zhen Tian, Dezong Zhao, Jianglin Lan)</author>
      <guid isPermaLink="false">2509.01856v2</guid>
      <pubDate>Fri, 05 Sep 2025 14:43:03 +0800</pubDate>
    </item>
    <item>
      <title>AIVA: An AI-based Virtual Companion for Emotion-aware Interaction</title>
      <link>http://arxiv.org/abs/2509.03212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作通过整合多模态情感感知到大型语言模型中，开发了一个能够解读情感线索并做出情感一致回应的AI虚拟伴侣，为人机交互提供了更自然和有同理心的体验。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言理解和生成方面取得了显著进步，改善了人机交互。但这些模型仅限于单模态文本处理，缺乏解读非语言信号中情感线索的能力，限制了更沉浸和有同理心的交互体验。&lt;h4&gt;目的&lt;/h4&gt;探索将多模态情感感知整合到大型语言模型中，创建具有情感感知能力的智能体，实现更自然和有同理心的人机交互。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为'\ours'的基于AI的虚拟伴侣，引入了多模态情感感知网络(MSPN)，使用跨模态融合transformer和监督对比学习来提供情感线索；开发了情感感知的提示工程策略用于生成有同理心的回应；集成了文本到语音系统和动画头像模块以实现富有表现力的交互。&lt;h4&gt;主要发现&lt;/h4&gt;多模态情感感知可以增强大型语言模型的能力，使其能够更好地理解和回应人类情感，从而创建更自然、更有同理心的人机交互体验。&lt;h4&gt;结论&lt;/h4&gt;'\ours'为情感感知智能体提供了一个框架，在陪伴机器人、社会关怀、心理健康和以人为本的人工智能等领域有广泛应用前景。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型的最新进展显著改善了自然语言理解和生成能力，增强了人机交互。然而，大型语言模型仅限于单模态文本处理，缺乏解读非语言信号中情感线索的能力，阻碍了更沉浸和有同理心的交互。这项工作探索将多模态情感感知整合到大型语言模型中，以创建具有情感感知能力的智能体。我们提出了'\ours'，一个基于AI的虚拟伴侣，能够捕获多模态情感线索，实现情感一致和生动的人机交互。'\ours'引入了一个多模态情感感知网络，使用跨模态融合transformer和监督对比学习来提供情感线索。此外，我们还开发了一种情感感知的提示工程策略，用于生成有同理心的回应，并集成了文本到语音系统和动画头像模块，以实现富有表现力的交互。'\ours'为情感感知智能体提供了一个框架，在陪伴机器人、社会关怀、心理健康和以人为本的人工智能等领域有应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Large Language Models (LLMs) have significantly improvednatural language understanding and generation, enhancing Human-ComputerInteraction (HCI). However, LLMs are limited to unimodal text processing andlack the ability to interpret emotional cues from non-verbal signals, hinderingmore immersive and empathetic interactions. This work explores integratingmultimodal sentiment perception into LLMs to create emotion-aware agents. Wepropose \ours, an AI-based virtual companion that captures multimodal sentimentcues, enabling emotionally aligned and animated HCI. \ours introduces aMultimodal Sentiment Perception Network (MSPN) using a cross-modal fusiontransformer and supervised contrastive learning to provide emotional cues.Additionally, we develop an emotion-aware prompt engineering strategy forgenerating empathetic responses and integrate a Text-to-Speech (TTS) system andanimated avatar module for expressive interactions. \ours provides a frameworkfor emotion-aware agents with applications in companion robotics, social care,mental health, and human-centered AI.</description>
      <author>example@mail.com (Chenxi Li)</author>
      <guid isPermaLink="false">2509.03212v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
  <item>
      <title>Autonomous Learning From Success and Failure: Goal-Conditioned Supervised Learning with Negative Feedback</title>
      <link>http://arxiv.org/abs/2509.03206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将对比学习原则整合到目标条件监督学习(GCSL)框架中的新模型，解决了GCSL仅从成功经验学习而忽略失败经验，以及加剧代理固有偏见的问题，使代理能够从成功和失败中学习，实现更优的探索性行为和性能。&lt;h4&gt;背景&lt;/h4&gt;强化学习在稀疏奖励结构的任务中面临挑战；模仿学习收敛快但依赖人工演示；目标条件监督学习(GCSL)使自主系统能够进行自我模仿学习。&lt;h4&gt;目的&lt;/h4&gt;解决GCSL框架的两个局限性：仅从自我生成的经验学习会加剧代理固有偏见；重新标记策略使代理只关注成功结果而无法从错误中学习。&lt;h4&gt;方法&lt;/h4&gt;提出一种新模型，将对比学习原则整合到GCSL框架中，使代理能够从成功和失败中学习。&lt;h4&gt;主要发现&lt;/h4&gt;该算法克服了代理初始偏见带来的限制，使代理能够进行更具探索性的行为，有助于识别和采用有效策略。&lt;h4&gt;结论&lt;/h4&gt;该模型在各种具有挑战性的环境中实现了更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;强化学习应用于具有稀疏奖励结构的任务时面临重大挑战。尽管模仿学习（监督学习领域内）提供了更快的收敛速度，但它严重依赖人工生成的演示。最近，目标条件监督学习(GCSL)作为一种潜在解决方案出现，使自主系统能够进行自我模仿学习。通过战略性地重新标记目标，代理可以从自身经验中获取策略见解。尽管该框架取得了成功，但它存在两个显著局限：(1)仅从自我生成的经验学习可能会加剧代理的固有偏见；(2)重新标记策略允许代理只关注成功结果，无法从错误中学习。为解决这些问题，我们提出了一种将对比学习原则整合到GCSL框架中的新模型，以从成功和失败中学习。通过经验评估，我们证明了该算法克服了代理初始偏见带来的限制，从而实现了更具探索性的行为。这有助于识别和采用有效策略，在各种具有挑战性的环境中实现更优的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning faces significant challenges when applied to taskscharacterized by sparse reward structures. Although imitation learning, withinthe domain of supervised learning, offers faster convergence, it relies heavilyon human-generated demonstrations. Recently, Goal-Conditioned SupervisedLearning (GCSL) has emerged as a potential solution by enabling self-imitationlearning for autonomous systems. By strategically relabelling goals, agents canderive policy insights from their own experiences. Despite the successes ofthis framework, it presents two notable limitations: (1) Learning exclusivelyfrom self-generated experiences can exacerbate the agents' inherent biases; (2)The relabelling strategy allows agents to focus solely on successful outcomes,precluding them from learning from their mistakes. To address these issues, wepropose a novel model that integrates contrastive learning principles into theGCSL framework to learn from both success and failure. Through empiricalevaluations, we demonstrate that our algorithm overcomes limitations imposed byagents' initial biases and thereby enables more exploratory behavior. Thisfacilitates the identification and adoption of effective policies, leading tosuperior performance across a variety of challenging environments.</description>
      <author>example@mail.com (Zeqiang Zhang, Fabian Wurzberger, Gerrit Schmid, Sebastian Gottwald, Daniel A. Braun)</author>
      <guid isPermaLink="false">2509.03206v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Interpretability and Effectiveness in Recommendation with Numerical Features via Learning to Contrast the Counterfactual samples</title>
      <link>http://arxiv.org/abs/2509.03187v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TheWebConf2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CCSS的通用模型无关对比学习框架，用于建模神经网络输出与数值特征之间的单调性，提高推荐系统的可解释性和有效性。&lt;h4&gt;背景&lt;/h4&gt;推荐系统的可解释性和有效性依赖于神经网络输出与数值特征之间的单调性关系，但现有方法缺乏对此特性的有效建模。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的、与模型无关的框架，能够准确建模神经网络输出与数值特征之间的单调性关系。&lt;h4&gt;方法&lt;/h4&gt;提出CCSS框架，通过两阶段过程建模单调性：1)合成反事实样本；2)对比反事实样本。这两种技术自然集成到一个与模型无关的框架中，形成端到端的训练过程。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集和真实工业数据集上的大量实证测试表明，CCSS框架能有效建模单调性关系。此外，CCSS已在真实大规模工业推荐系统中部署，为数亿用户提供服务。&lt;h4&gt;结论&lt;/h4&gt;CCSS框架成功解决了神经网络输出与数值特征单调性建模的问题，提高了推荐系统的可解释性和有效性，并在实际工业应用中证明了其价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出一个通用的、与模型无关的带有反事实样本合成的对比学习框架（CCSS），用于建模神经网络输出与数值特征之间的单调性，这对推荐系统的可解释性和有效性至关重要。CCSS通过两阶段过程建模单调性：合成反事实样本和对比反事实样本。这两种技术自然集成到一个与模型无关的框架中，形成端到端的训练过程。在公开可用数据集和真实工业数据集上进行了大量实证测试，结果很好地证明了我们提出的CCSS的有效性。此外，CCSS已在我们真实的大规模工业推荐系统中部署，成功为数亿用户提供服务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a general model-agnostic Contrastive learning framework withCounterfactual Samples Synthesizing (CCSS) for modeling the monotonicitybetween the neural network output and numerical features which is critical forinterpretability and effectiveness of recommender systems. CCSS models themonotonicity via a two-stage process: synthesizing counterfactual samples andcontrasting the counterfactual samples. The two techniques are naturallyintegrated into a model-agnostic framework, forming an end-to-end trainingprocess. Abundant empirical tests are conducted on a publicly available datasetand a real industrial dataset, and the results well demonstrate theeffectiveness of our proposed CCSS. Besides, CCSS has been deployed in our reallarge-scale industrial recommender, successfully serving over hundreds ofmillions users.</description>
      <author>example@mail.com (Xiaoxiao Xu, Hao Wu, Wenhui Yu, Lantao Hu, Peng Jiang, Kun Gai)</author>
      <guid isPermaLink="false">2509.03187v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised Radio Representation Learning: Can we Learn Multiple Tasks?</title>
      <link>http://arxiv.org/abs/2509.03077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, IEEE international conference on communication  2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于动量对比的自监督学习方案，用于无线电信号表征学习。该方法利用对比学习从大型真实世界数据集中提取鲁棒且可迁移的表征，并在两个无线通信任务中验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;人工智能预计将在6G中发挥关键作用，但开发AI解决方案面临的主要挑战是需要大量的数据收集和标注工作来训练监督深度学习模型。自监督学习通过利用大量未标记数据来实现接近监督学习的性能，已在各个领域显示出显著的成功。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的自监督学习方案，用于无线电信号表征学习，以减少对标记数据的依赖，提高模型泛化能力，为可扩展的基础6G AI模型和解决方案铺平道路。&lt;h4&gt;方法&lt;/h4&gt;使用动量对比的自监督学习方案，通过应用对比学习从大型真实世界数据集中提取鲁棒且可迁移的表征。评估这些学习到的表征在两个无线通信任务（到达角估计和自动调制分类）中的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;精心设计的增强和多样化数据使对比学习能够产生高质量、不变的潜在表征。即使冻结编码器权重，这些表征也是有效的，并且微调可以进一步提高性能，超越监督基线。据作者所知，这是第一个提出并证明自监督学习对多任务无线电信号有效性的工作。&lt;h4&gt;结论&lt;/h4&gt;自监督学习具有变革无线通信AI的潜力，通过减少对标记数据的依赖和提高模型泛化能力，为可扩展的基础6G AI模型和解决方案铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;人工智能(AI)预计将在6G中发挥关键作用。然而，开发AI解决方案的一个主要挑战是需要大量的数据收集和标注工作来训练监督深度学习模型。为了克服这一挑战，自监督学习(SSL)方法最近通过利用大量未标记数据来实现接近监督学习的性能，已在各个领域显示出显著的成功。在本文中，我们提出了一种使用动量对比的有效SSL方案，用于无线电信号表征学习。通过应用对比学习，我们的方法从大型真实世界数据集中提取鲁棒且可迁移的表征。我们评估了这些学习到的表征在两个无线通信任务中的泛化能力：到达角(AoA)估计和自动调制分类(AMC)。我们的研究结果表明，精心设计的增强和多样化数据使对比学习能够产生高质量、不变的潜在表征。即使冻结编码器权重，这些表征也是有效的，并且微调可以进一步提高性能，超越监督基线。据我们所知，这是第一个提出并证明自监督学习对多任务无线电信号有效性的工作。我们的研究结果强调了自监督学习通过减少对标记数据的依赖和提高模型泛化能力来变革无线通信AI的潜力，为可扩展的基础6G AI模型和解决方案铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) is anticipated to play a pivotal role in 6G.However, a key challenge in developing AI-powered solutions is the extensivedata collection and labeling efforts required to train supervised deep learningmodels. To overcome this, self-supervised learning (SSL) approaches haverecently demonstrated remarkable success across various domains by leveraginglarge volumes of unlabeled data to achieve near-supervised performance. In thispaper, we propose an effective SSL scheme for radio signal representationlearning using momentum contrast. By applying contrastive learning, our methodextracts robust, transferable representations from a large real-world dataset.We assess the generalizability of these learned representations across twowireless communications tasks: angle of arrival (AoA) estimation and automaticmodulation classification (AMC). Our results show that carefully designedaugmentations and diverse data enable contrastive learning to producehigh-quality, invariant latent representations. These representations areeffective even with frozen encoder weights, and fine-tuning further enhancesperformance, surpassing supervised baselines. To the best of our knowledge,this is the first work to propose and demonstrate the effectiveness ofself-supervised learning for radio signals across multiple tasks. Our findingshighlight the potential of self-supervised learning to transform AI forwireless communications by reducing dependence on labeled data and improvingmodel generalization - paving the way for scalable foundational 6G AI modelsand solutions.</description>
      <author>example@mail.com (Ogechukwu Kanu, Ashkan Eshaghbeigi, Hatem Abou-Zeid)</author>
      <guid isPermaLink="false">2509.03077v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Training LLMs to be Better Text Embedders through Bidirectional Reconstruction</title>
      <link>http://arxiv.org/abs/2509.03020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的训练方法，通过双向生成重建任务增强大型语言模型中最终令牌的语义表达能力，显著提升了文本嵌入性能&lt;h4&gt;背景&lt;/h4&gt;大型语言模型正被探索作为文本嵌入器，但现有方法依赖的最终令牌（如[EOS]）未经专门训练来捕获整个上下文语义，限制了其在检索和重排序任务中的表现&lt;h4&gt;目的&lt;/h4&gt;在对比学习前添加新的训练阶段，以丰富最终令牌嵌入的语义表达能力&lt;h4&gt;方法&lt;/h4&gt;采用双向生成重建任务EBQ2D（基于嵌入的查询到文档）和EBD2Q（基于嵌入的文档到查询），交替进行锚定[EOS]嵌入并重建查询-文档对&lt;h4&gt;主要发现&lt;/h4&gt;额外训练阶段显著提高了LLM在Massive Text Embedding Benchmark上的性能，在不同LLM基础模型和规模上实现了新的最先进结果&lt;h4&gt;结论&lt;/h4&gt;通过专门设计的训练阶段增强最终令牌语义表达能力，可有效提升大型语言模型在文本嵌入任务中的性能&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）越来越多地被探索作为强大的文本嵌入器。现有的基于LLM的文本嵌入方法通常利用最终令牌的嵌入，通常是保留的特殊令牌如[EOS]。然而，这些令牌没有经过专门训练来捕获整个上下文的语义，限制了它们作为文本嵌入的能力，特别是在检索和重排序任务中。我们提出在对比学习之前添加一个新的训练阶段来丰富最终令牌嵌入的语义。该阶段采用双向生成重建任务，即EBQ2D（基于嵌入的查询到文档）和EBD2Q（基于嵌入的文档到查询），这些任务交替进行，锚定[EOS]嵌入并重建查询-文档对中的任一侧。实验结果表明，我们额外的训练阶段显著提高了LLM在Massive Text Embedding Benchmark（MTEB）上的性能，在不同LLM基础模型和规模上实现了新的最先进结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have increasingly been explored as powerful textembedders. Existing LLM-based text embedding approaches often leverage theembedding of the final token, typically a reserved special token such as [EOS].However, these tokens have not been intentionally trained to capture thesemantics of the whole context, limiting their capacity as text embeddings,especially for retrieval and re-ranking tasks. We propose to add a new trainingstage before contrastive learning to enrich the semantics of the final tokenembedding. This stage employs bidirectional generative reconstruction tasks,namely EBQ2D (Embedding-Based Query-to-Document) and EBD2Q (Embedding-BasedDocument-to-Query), which interleave to anchor the [EOS] embedding andreconstruct either side of Query-Document pairs. Experimental resultsdemonstrate that our additional training stage significantly improves LLMperformance on the Massive Text Embedding Benchmark (MTEB), achieving newstate-of-the-art results across different LLM base models and scales.</description>
      <author>example@mail.com (Chang Su, Dengliang Shi, Siyuan Huang, Jintao Du, Changhua Meng, Yu Cheng, Weiqiang Wang, Zhouhan Lin)</author>
      <guid isPermaLink="false">2509.03020v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Resilient Multimodal Industrial Surface Defect Detection with Uncertain Sensors Availability</title>
      <link>http://arxiv.org/abs/2509.02962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to IEEE/ASME Transactions on Mechatronics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种多模态工业表面缺陷检测方法，通过融合RGB和3D模态数据来识别和定位工业产品缺陷。针对模态缺失问题，作者提出跨模态提示学习和对称对比学习方法，实验证明该方法显著优于现有技术。&lt;h4&gt;背景&lt;/h4&gt;多模态工业表面缺陷检测(MISDD)旨在通过融合RGB和3D模态数据来识别和定位工业产品缺陷。然而，在实际应用中，传感器的不确定性会导致模态缺失问题，给多模态融合带来困难，包括学习模式变换和信息空缺等问题。&lt;h4&gt;目的&lt;/h4&gt;解决MISDD中的模态缺失问题，提高在模态缺失情况下的缺陷检测性能，特别是在传感器可用性不确定的情况下。&lt;h4&gt;方法&lt;/h4&gt;1) 提出跨模态提示学习：包括跨模态一致性提示建立双视觉模态信息一致性；模态特定提示适应不同输入模式；缺失感知提示补偿动态模态缺失造成的信息空缺。2) 提出对称对比学习：利用文本模态作为双视觉模态融合的桥梁，设计成对反义文本提示生成二元文本语义，并提供三模态对比预训练实现多模态学习。&lt;h4&gt;主要发现&lt;/h4&gt;在RGB和3D模态总缺失率为0.7的情况下，所提方法达到73.83%的I-AUROC和93.05%的P-AUROC，分别超过现有最先进方法3.84%和5.58%。在不同缺失类型和率的情况下，该方法也以不同程度优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的跨模态提示学习和对称对比学习方法有效解决了MISDD中的模态缺失问题，显著提高了缺陷检测性能，特别是在模态缺失的情况下。&lt;h4&gt;翻译&lt;/h4&gt;多模态工业表面缺陷检测(MISDD)旨在通过融合RGB和3D模态数据来识别和定位工业产品缺陷。本文关注由传感器可用性不确定引起的MISDD中的模态缺失问题。在此背景下，多模态融合遇到几个困难，包括学习模式变换和信息空缺。为此，我们首先提出跨模态提示学习，包括：i)跨模态一致性提示用于建立双视觉模态的信息一致性；ii)插入模态特定提示以适应不同的输入模式；iii)附加缺失感知提示以补偿动态模态缺失造成的信息空缺。此外，我们提出对称对比学习，利用文本模态作为双视觉模态融合的桥梁。具体来说，设计了成对反义文本提示来生成二元文本语义，并提供三模态对比预训练来实现多模态学习。实验结果表明，我们提出的方法在RGB和3D模态总缺失率为0.7的情况下，实现了73.83%的I-AUROC和93.05%的P-AUROC（分别超过现有最先进方法3.84%和5.58%），并在不同缺失类型和率的情况下以不同程度优于现有方法。源代码将在https://github.com/SvyJ/MISDD-MM上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal industrial surface defect detection (MISDD) aims to identify andlocate defect in industrial products by fusing RGB and 3D modalities. Thisarticle focuses on modality-missing problems caused by uncertain sensorsavailability in MISDD. In this context, the fusion of multiple modalitiesencounters several troubles, including learning mode transformation andinformation vacancy. To this end, we first propose cross-modal prompt learning,which includes: i) the cross-modal consistency prompt serves the establishmentof information consistency of dual visual modalities; ii) the modality-specificprompt is inserted to adapt different input patterns; iii) the missing-awareprompt is attached to compensate for the information vacancy caused by dynamicmodalities-missing. In addition, we propose symmetric contrastive learning,which utilizes text modality as a bridge for fusion of dual vision modalities.Specifically, a paired antithetical text prompt is designed to generate binarytext semantics, and triple-modal contrastive pre-training is offered toaccomplish multimodal learning. Experiment results show that our proposedmethod achieves 73.83% I-AUROC and 93.05% P-AUROC with a total missing rate 0.7for RGB and 3D modalities (exceeding state-of-the-art methods 3.84% and 5.58%respectively), and outperforms existing approaches to varying degrees underdifferent missing types and rates. The source code will be available athttps://github.com/SvyJ/MISDD-MM.</description>
      <author>example@mail.com (Shuai Jiang, Yunfeng Ma, Jingyu Zhou, Yuan Bian, Yaonan Wang, Min Liu)</author>
      <guid isPermaLink="false">2509.02962v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>RankGraph: Unified Heterogeneous Graph Learning for Cross-Domain Recommendation</title>
      <link>http://arxiv.org/abs/2509.02942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  RecSys 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RankGraph是一个可扩展的图学习框架，作为推荐基础模型的核心组件，通过构建跨产品的异构节点和边图，整合用户、帖子、广告等实体间的复杂关系，有效提升了跨领域推荐系统的性能。&lt;h4&gt;背景&lt;/h4&gt;跨领域推荐系统面临挑战，需要整合不同产品领域中的细粒度用户和项目关系。&lt;h4&gt;目的&lt;/h4&gt;介绍RankGraph框架，作为推荐基础模型的核心组件，解决跨领域推荐中的关系整合问题。&lt;h4&gt;方法&lt;/h4&gt;构建多产品中异构节点和边组成的图，利用GPU加速的图神经网络和对比学习，动态提取子图如项目-项目图和用户-用户图，并将基于图的预训练表示作为上下文令牌整合到FM序列模型中。&lt;h4&gt;主要发现&lt;/h4&gt;通过在线A/B测试验证，RankGraph使点击率提高0.92%，转化率提高2.82%，有效支持基于相似性的检索和实时聚类。&lt;h4&gt;结论&lt;/h4&gt;RankGraph在跨领域推荐场景中展示了显著的有效性，能够成功整合复杂关系并提升推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;跨领域推荐系统面临着整合各种产品领域中细粒度用户和项目关系的挑战。为此，我们引入了RankGraph，这是一个可扩展的图学习框架，设计用作推荐基础模型的核心组件。通过构建和利用跨多个产品的异构节点和边组成的图，RankGraph能够整合用户、帖子、广告和其他实体之间的复杂关系。我们的框架采用GPU加速的图神经网络和对比学习，能够动态提取项目-项目和用户-用户等子图，以支持基于相似性的检索和实时聚类。此外，RankGraph将基于图的预训练表示作为上下文令牌整合到FM序列模型中，通过结构化关系知识丰富模型。RankGraph在在线A/B测试中展示了点击率（+0.92%）和转化率（+2.82%）的提升，展示了其在跨领域推荐场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain recommendation systems face the challenge of integratingfine-grained user and item relationships across various product domains. Toaddress this, we introduce RankGraph, a scalable graph learning frameworkdesigned to serve as a core component in recommendation foundation models(FMs). By constructing and leveraging graphs composed of heterogeneous nodesand edges across multiple products, RankGraph enables the integration ofcomplex relationships between users, posts, ads, and other entities. Ourframework employs a GPU-accelerated Graph Neural Network and contrastivelearning, allowing for dynamic extraction of subgraphs such as item-item anduser-user graphs to support similarity-based retrieval and real-timeclustering. Furthermore, RankGraph integrates graph-based pretrainedrepresentations as contextual tokens into FM sequence models, enriching themwith structured relational knowledge. RankGraph has demonstrated improvementsin click (+0.92%) and conversion rates (+2.82%) in online A/B tests, showcasingits effectiveness in cross-domain recommendation scenarios.</description>
      <author>example@mail.com (Renzhi Wu, Junjie Yang, Li Chen, Hong Li, Li Yu, Hong Yan)</author>
      <guid isPermaLink="false">2509.02942v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SynthGenNet: a self-supervised approach for test-time generalization using synthetic multi-source domain mixing of street view images</title>
      <link>http://arxiv.org/abs/2509.02287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SynthGenNet是一种自监督的学生-教师架构，通过ClassMix++算法、基于掩码的一致性损失和伪标签引导的对比学习，实现了在复杂城市环境中的强大域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;非结构化城市环境由于其复杂多样的布局，为场景理解和泛化带来了独特的挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍SynthGenNet，一种自监督的学生-教师架构，旨在使用合成多源图像实现强大的测试时域泛化。&lt;h4&gt;方法&lt;/h4&gt;提出了新颖的ClassMix++算法，混合来自各种合成源的标记数据同时保持语义完整性；采用基于掩码的一致性损失(GMC)，利用源真实数据提高跨域预测一致性和特征对齐；集成伪标签引导的对比学习(PLGCL)机制到学生网络中，通过从教师网络进行迭代知识蒸馏促进域不变特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;这种自监督策略提高了预测准确性，解决了现实世界的变化性，弥合了仿真到现实的域差距，减少了对标记目标数据的依赖，即使在复杂的城市区域也是如此。&lt;h4&gt;结论&lt;/h4&gt;该模型在印度驾驶数据集(IDD)等真实世界数据集上实现了50%的平均交并比(mIoU)值，超越了依赖单一源的最先进技术。&lt;h4&gt;翻译&lt;/h4&gt;非结构化的城市环境由于其复杂多样的布局，为场景理解和泛化带来了独特的挑战。我们介绍了SynthGenNet，一种自监督的学生-教师架构，旨在使用合成多源图像实现强大的测试时域泛化。我们的贡献包括新颖的ClassMix++算法，该算法混合来自各种合成源的标记数据，同时保持语义完整性，增强模型适应性。我们进一步采用基于掩码的一致性损失(GMC)，利用源真实数据提高跨域预测一致性和特征对齐。伪标签引导的对比学习(PLGCL)机制被集成到学生网络中，通过从教师网络进行迭代知识蒸馏，促进域不变特征学习。这种自监督策略提高了预测准确性，解决了现实世界的变化性，弥合了仿真到现实的域差距，减少了对标记目标数据的依赖，即使在复杂的城市区域也是如此。结果显示，我们的模型在印度驾驶数据集(IDD)等真实世界数据集上实现了50%的平均交并比(mIoU)值，超越了依赖单一源的最先进技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unstructured urban environments present unique challenges for sceneunderstanding and generalization due to their complex and diverse layouts. Weintroduce SynthGenNet, a self-supervised student-teacher architecture designedto enable robust test-time domain generalization using synthetic multi-sourceimagery. Our contributions include the novel ClassMix++ algorithm, which blendslabeled data from various synthetic sources while maintaining semanticintegrity, enhancing model adaptability. We further employ Grounded MaskConsistency Loss (GMC), which leverages source ground truth to improvecross-domain prediction consistency and feature alignment. The Pseudo-LabelGuided Contrastive Learning (PLGCL) mechanism is integrated into the studentnetwork to facilitate domain-invariant feature learning through iterativeknowledge distillation from the teacher network. This self-supervised strategyimproves prediction accuracy, addresses real-world variability, bridges thesim-to-real domain gap, and reliance on labeled target data, even in complexurban areas. Outcomes show our model outperforms the state-of-the-art (relyingon single source) by achieving 50% Mean Intersection-Over-Union (mIoU) value onreal-world datasets like Indian Driving Dataset (IDD).</description>
      <author>example@mail.com (Pushpendra Dhakara, Prachi Chachodhia, Vaibhav Kumar)</author>
      <guid isPermaLink="false">2509.02287v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>StructCoh: Structured Contrastive Learning for Context-Aware Text Semantic Matching</title>
      <link>http://arxiv.org/abs/2509.02033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by PRICAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了StructCoh，一个图增强对比学习框架，用于文本语义匹配，结合了结构推理和表示空间优化，通过双图编码器和层次化对比目标实现更好的性能。&lt;h4&gt;背景&lt;/h4&gt;文本语义匹配需要理解结构关系和细粒度语义差异。预训练语言模型虽然在捕捉令牌级交互方面表现出色，但往往忽略层次化结构模式，且在细微语义区分方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时考虑结构关系和细粒度语义差异的文本语义匹配框架，提高匹配性能，特别是在法律文档和学术抄袭检测领域。&lt;h4&gt;方法&lt;/h4&gt;提出StructCoh框架，包含两个关键创新：(1)双图编码器通过依存解析和主题建模构建语义图，利用图同构网络传播结构特征；(2)层次化对比目标在多个粒度上强制一致性，包括节点级对比正则化和图感知对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;在三个法律文档匹配基准测试和学术抄袭检测数据集上，StructCoh比最先进方法有显著改进。在法律条文匹配上实现了86.7%的F1分数，绝对增益+6.2%，通过有效识别论证结构相似性。&lt;h4&gt;结论&lt;/h4&gt;StructCoh通过协同结合结构推理和表示空间优化，有效解决了文本语义匹配中的结构理解和细粒度语义区分问题，在法律文档匹配和学术抄袭检测任务中取得了显著成果。&lt;h4&gt;翻译&lt;/h4&gt;文本语义匹配需要对结构关系和细粒度语义差异有细致的理解。虽然预训练语言模型在捕捉令牌级别的交互方面表现出色，但它们往往忽略了层次化的结构模式，并且在处理细微的语义区分方面存在困难。在本文中，我们提出了StructCoh，一个图增强对比学习框架，协同结合了结构推理和表示空间优化。我们的方法有两个关键创新：(1)双图编码器通过依存解析和主题建模构建语义图，然后利用图同构网络在句法依赖和跨文档概念节点之间传播结构特征。(2)层次化对比目标在多个粒度上强制一致性：节点级对比正则化保留核心语义单元，而图感知对比学习通过显式和隐式负采样策略对齐跨文档结构语义。在三个法律文档匹配基准测试和学术抄袭检测数据集上的实验表明，与最先进的方法相比有显著改进。值得注意的是，StructCoh在法律条文匹配上实现了86.7%的F1分数（绝对增益+6.2%），通过有效识别论证结构相似性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text semantic matching requires nuanced understanding of both structuralrelationships and fine-grained semantic distinctions. While pre-trainedlanguage models excel at capturing token-level interactions, they oftenoverlook hierarchical structural patterns and struggle with subtle semanticdiscrimination. In this paper, we proposed StructCoh, a graph-enhancedcontrastive learning framework that synergistically combines structuralreasoning with representation space optimization. Our approach features two keyinnovations: (1) A dual-graph encoder constructs semantic graphs via dependencyparsing and topic modeling, then employs graph isomorphism networks topropagate structural features across syntactic dependencies and cross-documentconcept nodes. (2) A hierarchical contrastive objective enforces consistency atmultiple granularities: node-level contrastive regularization preserves coresemantic units, while graph-aware contrastive learning aligns inter-documentstructural semantics through both explicit and implicit negative samplingstrategies. Experiments on three legal document matching benchmarks andacademic plagiarism detection datasets demonstrate significant improvementsover state-of-the-art methods. Notably, StructCoh achieves 86.7% F1-score(+6.2% absolute gain) on legal statute matching by effectively identifyingargument structure similarities.</description>
      <author>example@mail.com (Chao Xue, Ziyuan Gao)</author>
      <guid isPermaLink="false">2509.02033v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Empowering Large Language Model for Sequential Recommendation via Multimodal Embeddings and Semantic IDs</title>
      <link>http://arxiv.org/abs/2509.02017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM 2025 Full Research Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MME-SID框架，一种基于大型语言模型的顺序推荐方法，通过整合多模态嵌入和量化嵌入解决了嵌入崩溃和灾难性遗忘问题。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐旨在捕捉用户的动态兴趣和顺序模式，大型语言模型的强大能力推动了其在顺序推荐中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于LLM的顺序推荐方法中存在的嵌入崩溃和灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;提出MME-SID框架，整合多模态嵌入和量化嵌入减轻嵌入崩溃；提出MM-RQ-VAE模型保留模态内距离信息和捕获模态间相关性；使用训练好的多模态码嵌入初始化模型减轻灾难性遗忘；使用LoRA在多模态频率感知融合方式下高效微调LLM。&lt;h4&gt;主要发现&lt;/h4&gt;在三个公共数据集上的实验验证了MME-SID的优越性能，能够有效减轻嵌入崩溃和灾难性遗忘问题。&lt;h4&gt;结论&lt;/h4&gt;MME-SID框架通过解决嵌入崩溃和灾难性遗忘问题，提高了顺序推荐的性能，实现代码和数据集已公开可供复现。&lt;h4&gt;翻译&lt;/h4&gt;顺序推荐旨在基于用户的历史交互来捕捉用户的动态兴趣和顺序模式。最近，大型语言模型的强大能力推动了它们在顺序推荐中的应用。然而，我们确定了现有基于LLM的SR方法中的两个关键挑战：1)整合预训练协作嵌入时的嵌入崩溃和2)使用语义ID时的量化嵌入灾难性遗忘。这些问题削弱了模型的可扩展性并导致次优的推荐性能。因此，基于Llama3-8B-instruct等LLM，我们引入了一种名为MME-SID的新型SR框架，该框架整合了多模态嵌入和量化嵌入以减轻嵌入崩溃。此外，我们提出了具有最大均值差异作为重构损失和对比学习对齐的多模态残差量化变分自编码器(MM-RQ-VAE)，分别有效地保留模态内距离信息和捕获模态间相关性。为了进一步减轻灾难性遗忘，我们使用训练好的多模态码嵌入初始化模型。最后，我们使用LoRA在多模态频率感知融合方式下高效微调LLM。在三个公共数据集上的大量实验验证了MME-SID的优越性能，这得益于其减轻嵌入崩溃和灾难性遗忘的能力。实现代码和数据集已公开可供复现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation (SR) aims to capture users' dynamic interests andsequential patterns based on their historical interactions. Recently, thepowerful capabilities of large language models (LLMs) have driven theiradoption in SR. However, we identify two critical challenges in existingLLM-based SR methods: 1) embedding collapse when incorporating pre-trainedcollaborative embeddings and 2) catastrophic forgetting of quantized embeddingswhen utilizing semantic IDs. These issues dampen the model scalability and leadto suboptimal recommendation performance. Therefore, based on LLMs likeLlama3-8B-instruct, we introduce a novel SR framework named MME-SID, whichintegrates multimodal embeddings and quantized embeddings to mitigate embeddingcollapse. Additionally, we propose a Multimodal Residual Quantized VariationalAutoencoder (MM-RQ-VAE) with maximum mean discrepancy as the reconstructionloss and contrastive learning for alignment, which effectively preserveintra-modal distance information and capture inter-modal correlations,respectively. To further alleviate catastrophic forgetting, we initialize themodel with the trained multimodal code embeddings. Finally, we fine-tune theLLM efficiently using LoRA in a multimodal frequency-aware fusion manner.Extensive experiments on three public datasets validate the superiorperformance of MME-SID thanks to its capability to mitigate embedding collapseand catastrophic forgetting. The implementation code and datasets are publiclyavailable for reproduction:https://github.com/Applied-Machine-Learning-Lab/MME-SID.</description>
      <author>example@mail.com (Yuhao Wang, Junwei Pan, Xinhang Li, Maolin Wang, Yuan Wang, Yue Liu, Dapeng Liu, Jie Jiang, Xiangyu Zhao)</author>
      <guid isPermaLink="false">2509.02017v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Structure-aware Contrastive Learning for Diagram Understanding of Multimodal Models</title>
      <link>http://arxiv.org/abs/2509.01959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的训练范式，专门用于增强视觉语言模型对图表的理解，通过利用困难样本和专门的损失函数，在图像-文本匹配和视觉问答任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;多模态模型（如CLIP）在视觉和语言表征对齐方面表现出色，但这些模型在应用于专业视觉领域（如图表）时存在局限性，因为图表编码的是结构化、符号化信息，与自然图像不同。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的训练范式，专门设计用于增强视觉语言模型对图表图像的理解能力。&lt;h4&gt;方法&lt;/h4&gt;使用'困难'样本进行对比学习，结合两种专门的损失函数，利用图表的固有结构特性，并将这些目标整合到模型训练中，使模型能够发展出对图表内容更结构化和语义连贯的理解。&lt;h4&gt;主要发现&lt;/h4&gt;在流程图基准数据集上进行了实证验证，与标准CLIP和传统的困难负例CLIP学习范式相比，在图像-文本匹配和视觉问答任务上都有显著改进。&lt;h4&gt;结论&lt;/h4&gt;强调了针对专业任务定制训练策略的重要性，为视觉语言集成领域中图表理解的进步做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;多模态模型，如对比语言-图像预训练模型，在视觉和语言表征对齐方面已经显示出显著的成功。然而，当应用于专业视觉领域（如图表）时，这些模型表现出局限性，因为图表编码的是与自然图像不同的结构化、符号化信息。在本文中，我们引入了一种新的训练范式，专门设计用于增强视觉语言模型对图表图像的理解。我们的方法使用'困难'样本进行对比学习，结合了两种专门的损失函数，这些函数利用图表的固有结构特性。通过将这些目标整合到模型训练中，我们的方法使模型能够发展出对图表内容更结构化和语义连贯的理解。我们在流程图的基准数据集上经验性地验证了我们的方法，流程图作为图表图像的一个代表性类别，证明了与标准CLIP和传统的困难负例CLIP学习范式相比，在图像-文本匹配和视觉问答任务上都有显著改进。我们的发现强调了针对专业任务定制训练策略的重要性，并为视觉语言集成领域中图表理解的进步做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal models, such as the Contrastive Language-Image Pre-training (CLIP)model, have demonstrated remarkable success in aligning visual and linguisticrepresentations. However, these models exhibit limitations when applied tospecialised visual domains, such as diagrams, which encode structured, symbolicinformation distinct from that of natural imagery.  In this paper, we introduce a novel training paradigm explicitly designed toenhance the comprehension of diagrammatic images within vision-language models.Our approach uses ``hard'' samples for our proposed contrastive learning thatincorporates two specialised loss functions that leverage the inherentstructural properties of diagrams. By integrating these objectives into modeltraining, our method enables models to develop a more structured andsemantically coherent understanding of diagrammatic content.  We empirically validate our approach on a benchmark dataset of flowcharts, asa representative class of diagrammatic imagery, demonstrating substantialimprovements over standard CLIP and conventional hard negative CLIP learningparadigms for both image-text matching and visual question answering tasks. Ourfindings underscore the significance of tailored training strategies forspecialised tasks and contribute to advancing diagrammatic understanding withinthe broader landscape of vision-language integration.</description>
      <author>example@mail.com (Hiroshi Sasaki)</author>
      <guid isPermaLink="false">2509.01959v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Interpretable Geo-localization: a Concept-Aware Global Image-GPS Alignment Framework</title>
      <link>http://arxiv.org/abs/2509.01910v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种将全球地理定位与概念瓶颈相结合的新框架，通过概念感知对齐模块增强模型解释性并提高定位准确性。&lt;h4&gt;背景&lt;/h4&gt;全球地理定位涉及确定全球图像的确切地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管GeoCLIP等模型通过对比学习实现了准确预测，但其解释性仍不足，且当前基于概念的解释性方法无法有效与地理对齐的图像-位置嵌入目标保持一致。&lt;h4&gt;目的&lt;/h4&gt;解决地理定位模型解释性不足的问题，提高模型的可解释性和定位准确性。&lt;h4&gt;方法&lt;/h4&gt;提出新框架集成全球地理定位与概念瓶颈，插入概念感知对齐模块，将图像和位置嵌入共同投影到共享的地理概念库（如热带气候、山脉、教堂等），并最小化概念级别损失，在概念特定子空间中增强对齐。&lt;h4&gt;主要发现&lt;/h4&gt;据所知，这是首次将解释性引入地理定位的研究。实验表明该方法在地理定位准确性上超过GeoCLIP，并在各种地理空间预测任务中提升性能，揭示了地理决策过程更丰富的语义见解。&lt;h4&gt;结论&lt;/h4&gt;通过概念感知对齐模块成功将解释性引入地理定位领域，同时提高了定位准确性和其他地理空间预测任务的性能，为地理决策提供了更丰富的语义见解。&lt;h4&gt;翻译&lt;/h4&gt;全球地理定位涉及确定全球范围内拍摄图像的确切地理位置，通常由气候、地标和建筑风格等地理线索引导。尽管像GeoCLIP这样的地理定位模型通过对比学习利用图像和位置对齐实现了准确预测，但这些模型的解释性仍未得到充分探索。当前基于概念的解释性方法无法有效与地理对齐的图像-位置嵌入目标保持一致，导致次优的解释性和性能。为了解决这一差距，我们提出了一种将全球地理定位与概念瓶颈相结合的新框架。我们的方法插入了一个概念感知对齐模块，将图像和位置嵌入共同投影到一个共享的地理概念库（如热带气候、山脉、教堂等），并最小化概念级别的损失，从而在概念特定的子空间中增强对齐，并实现强大的可解释性。据我们所知，这是第一项将解释性引入地理定位的研究。大量实验表明，该方法在地理定位准确性上超过了GeoCLIP，并在各种地理空间预测任务中提升了性能，揭示了地理决策过程更丰富的语义见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Worldwide geo-localization involves determining the exact geographic locationof images captured globally, typically guided by geographic cues such asclimate, landmarks, and architectural styles. Despite advancements ingeo-localization models like GeoCLIP, which leverages images and locationalignment via contrastive learning for accurate predictions, theinterpretability of these models remains insufficiently explored. Currentconcept-based interpretability methods fail to align effectively withGeo-alignment image-location embedding objectives, resulting in suboptimalinterpretability and performance. To address this gap, we propose a novelframework integrating global geo-localization with concept bottlenecks. Ourmethod inserts a Concept-Aware Alignment Module that jointly projects image andlocation embeddings onto a shared bank of geographic concepts (e.g., tropicalclimate, mountain, cathedral) and minimizes a concept-level loss, enhancingalignment in a concept-specific subspace and enabling robust interpretability.To our knowledge, this is the first work to introduce interpretability intogeo-localization. Extensive experiments demonstrate that our approach surpassesGeoCLIP in geo-localization accuracy and boosts performance across diversegeospatial prediction tasks, revealing richer semantic insights into geographicdecision-making processes.</description>
      <author>example@mail.com (Furong Jia, Lanxin Liu, Ce Hou, Fan Zhang, Xinyan Liu, Yu Liu)</author>
      <guid isPermaLink="false">2509.01910v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Contrastive Learning versus Untrained Baselines: The Role of Dataset Size</title>
      <link>http://arxiv.org/abs/2509.01541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图对比学习(GCL)在图自监督学习中表现突出，但其优势取决于数据集大小和任务难度。研究表明，在标准数据集上，未经训练的基线模型可与之媲美；在大型分子数据集上，GCL在数据量超过几千个图后表现更优，但最终收益趋于平稳。&lt;h4&gt;背景&lt;/h4&gt;图对比学习已成为图自监督学习的主要范式，在标准化数据集上表现出色，并已应用于从基因组学到药物发现等多个领域。&lt;h4&gt;目的&lt;/h4&gt;研究图对比学习是否真正优于未经训练的基线模型。&lt;h4&gt;方法&lt;/h4&gt;作者在多个数据集(包括标准数据集和大型分子数据集ogbg-molhiv)上评估了GCL与未经训练的基线模型(如图神经网络、多层感知机和手工统计方法)的性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;1. GCL的优势取决于数据集大小和任务难度；2. 在标准数据集上，未经训练的基线模型可与之媲美或超越；3. 在ogbg-molhiv上，GCL在小规模时落后，超过几千个图后超越，但最终收益平稳；4. 在合成数据集上，GCL准确度随图数量对数增长，与任务复杂度相关。&lt;h4&gt;结论&lt;/h4&gt;确定数据集大小在基准测试和应用中的作用至关重要，需要设计避免性能平稳期的GCL算法。&lt;h4&gt;翻译&lt;/h4&gt;图对比学习已成为图自监督学习的主要范式，在标准化数据集上表现出色，应用范围从基因组学到药物发现不断扩大。我们提出一个基本问题：GCL是否真正优于未经训练的基线？我们发现GCL的优势取决于数据集大小和任务难度。在标准数据集上，未经训练的图神经网络、简单的多层感知机甚至手工设计的统计方法都可以与GCL相媲美或超越。在大型分子数据集ogbg-molhiv上，我们观察到交叉点：GCL在小规模时落后，但在几千个图之后超越，尽管这一收益最终趋于平稳。在合成数据集上，GCL的准确度大致随图数量的对数缩放，与未经训练的GNN相比的性能差距随任务复杂度变化。未来，确定数据集大小在基准测试和应用中的作用至关重要，同时需要设计避免性能平稳期的GCL算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Contrastive Learning (GCL) has emerged as a leading paradigm for self-supervised learning on graphs, with strong performance reported on standardizeddatasets and growing applications ranging from genomics to drug discovery. Weask a basic question: does GCL actually outperform untrained baselines? We findthat GCL's advantage depends strongly on dataset size and task difficulty. Onstandard datasets, untrained Graph Neural Networks (GNNs), simple multilayerperceptrons, and even handcrafted statistics can rival or exceed GCL. On thelarge molecular dataset ogbg-molhiv, we observe a crossover: GCL lags at smallscales but pulls ahead beyond a few thousand graphs, though this gaineventually plateaus. On synthetic datasets, GCL accuracy approximately scaleswith the logarithm of the number of graphs and its performance gap (comparedwith untrained GNNs) varies with respect to task complexity. Moving forward, itis crucial to identify the role of dataset size in benchmarks and applications,as well as to design GCL algorithms that avoid performance plateaus.</description>
      <author>example@mail.com (Smayan Khanna, Doruk Efe Gökmen, Risi Kondor, Vincenzo Vitelli)</author>
      <guid isPermaLink="false">2509.01541v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Temporal Representation Learning for Real-Time Ultrasound Analysis</title>
      <link>http://arxiv.org/abs/2509.01433v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICMl 2025 Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从超声视频中学习有效时间表示的方法，特别关注基于超声心动图的射血分数估计。通过利用时间一致的掩码和对比学习来增强模型捕捉心脏运动模式的能力，在EchoNet-Dynamic数据集上取得了显著的EF预测准确性改进。&lt;h4&gt;背景&lt;/h4&gt;超声成像是医学诊断中的重要工具，能够实时可视化生理过程。其主要优势是能够捕捉时间动态性，这对于评估心脏监测、胎儿发育和血管成像等应用中的运动模式至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种从超声视频中学习有效时间表示的方法，重点关注基于超声心动图的射血分数估计，解决当前深度学习模型忽视超声序列时间连续性的问题。&lt;h4&gt;方法&lt;/h4&gt;利用时间一致的掩码和对比学习来强制视频帧之间的时间连贯性，增强模型表示运动模式的能力，特别针对心脏的节律性收缩和舒张过程。&lt;h4&gt;主要发现&lt;/h4&gt;在EchoNet-Dynamic数据集上评估，该方法在EF预测准确性方面取得了显著改进，证明了时间感知的表示学习对提高超声分析性能的重要性。&lt;h4&gt;结论&lt;/h4&gt;时间感知的表示学习对实时超声分析至关重要，特别是在需要捕捉动态过程的应用中，如心脏功能的评估。&lt;h4&gt;翻译&lt;/h4&gt;超声成像是医学诊断中的关键工具，能够实时可视化生理过程。其主要优势之一是能够捕捉时间动态性，这对于评估心脏监测、胎儿发育和血管成像等应用中的运动模式至关重要。尽管超声成像很重要，但当前的深度学习模型常常忽视超声序列的时间连续性，独立分析帧而错过了关键的时间依赖关系。为了解决这一差距，我们提出了一种从超声视频中学习有效时间表示的方法，重点关注基于超声心动图的射血分数估计。EF预测作为案例研究，很好地证明了时间学习的必要性，因为它需要捕捉心脏的节律性收缩和舒张。我们的方法利用时间一致的掩码和对比学习来强制视频帧之间的时间连贯性，增强模型表示运动模式的能力。在EchoNet-Dynamic数据集上评估，我们的方法在EF预测准确性方面取得了显著改进，强调了时间感知的表示学习对实时超声分析的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultrasound (US) imaging is a critical tool in medical diagnostics, offeringreal-time visualization of physiological processes. One of its major advantagesis its ability to capture temporal dynamics, which is essential for assessingmotion patterns in applications such as cardiac monitoring, fetal development,and vascular imaging. Despite its importance, current deep learning modelsoften overlook the temporal continuity of ultrasound sequences, analyzingframes independently and missing key temporal dependencies. To address thisgap, we propose a method for learning effective temporal representations fromultrasound videos, with a focus on echocardiography-based ejection fraction(EF) estimation. EF prediction serves as an ideal case study to demonstrate thenecessity of temporal learning, as it requires capturing the rhythmiccontraction and relaxation of the heart. Our approach leverages temporallyconsistent masking and contrastive learning to enforce temporal coherenceacross video frames, enhancing the model's ability to represent motionpatterns. Evaluated on the EchoNet-Dynamic dataset, our method achieves asubstantial improvement in EF prediction accuracy, highlighting the importanceof temporally-aware representation learning for real-time ultrasound analysis.</description>
      <author>example@mail.com (Yves Stebler, Thomas M. Sutter, Ece Ozkan, Julia E. Vogt)</author>
      <guid isPermaLink="false">2509.01433v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Large Language Model for Knowledge Graph Completion via Structure-Aware Alignment-Tuning</title>
      <link>http://arxiv.org/abs/2509.01166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025, Main, Long Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SAT框架，通过结构感知对齐调用来增强大语言模型在知识图谱补全任务中的应用，解决了现有方法中自然语言与图结构表示空间不一致以及任务特定指令设计繁琐的问题。&lt;h4&gt;背景&lt;/h4&gt;知识图谱补全(KGC)旨在从知识图谱中推断新知识和进行预测。最近，大语言模型(LLMs)展示了显著的推理能力，LLM增强的KGC方法主要通过设计特定任务指令取得进展。&lt;h4&gt;目的&lt;/h4&gt;解决现有LLM增强KGC方法中的两个关键挑战：自然语言和图结构之间不一致的表示空间，以及为不同KGC任务设计单独指令导致的重复工作和耗时过程。&lt;h4&gt;方法&lt;/h4&gt;提出了SAT框架，包含两个主要组件：1) 分层知识对齐，通过多任务对比学习将图嵌入与自然语言空间对齐；2) 结构指令调优，使用统一的图指令结合轻量级知识适配器，引导LLM在知识图谱上进行结构感知推理。&lt;h4&gt;主要发现&lt;/h4&gt;在两个KGC任务和四个基准数据集上的实验结果表明，SAT显著优于最先进的方法，特别是在链接预测任务上的改进范围从8.7%到29.8%。&lt;h4&gt;结论&lt;/h4&gt;SAT框架有效解决了现有LLM增强KGC方法中的关键挑战，提高了知识图谱补全任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱补全(KGC)旨在从知识图谱中推断新知识和进行预测。最近，大语言模型(LLMs)展示了显著的推理能力。LLM增强的KGC方法主要专注于设计特定任务的指令，取得了有希望的进展。然而，仍存在两个关键挑战：首先，现有方法常常忽略自然语言和图结构之间不一致的表示空间；其次，大多数方法为不同的KGC任务设计单独的指令，导致重复工作和耗时过程。为解决这些挑战，我们提出了SAT，一个通过结构感知对齐调用来增强LLM用于KGC的新框架。具体而言，我们首先引入分层知识对齐，通过多任务对比学习将图嵌入与自然语言空间对齐。然后，我们提出结构指令调优，使用统一的图指令结合轻量级知识适配器，引导LLM在知识图谱上进行结构感知推理。在四个基准数据集的两个KGC任务上的实验结果表明，SAT显著优于最先进的方法，特别是在链接预测任务上的改进范围从8.7%到29.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graph completion (KGC) aims to infer new knowledge and makepredictions from knowledge graphs. Recently, large language models (LLMs) haveexhibited remarkable reasoning capabilities. LLM-enhanced KGC methods primarilyfocus on designing task-specific instructions, achieving promisingadvancements. However, there are still two critical challenges. First, existingmethods often ignore the inconsistent representation spaces between naturallanguage and graph structures. Second, most approaches design separateinstructions for different KGC tasks, leading to duplicate works andtime-consuming processes. To address these challenges, we propose SAT, a novelframework that enhances LLMs for KGC via structure-aware alignment-tuning.Specifically, we first introduce hierarchical knowledge alignment to aligngraph embeddings with the natural language space through multi-task contrastivelearning. Then, we propose structural instruction tuning to guide LLMs inperforming structure-aware reasoning over KGs, using a unified graphinstruction combined with a lightweight knowledge adapter. Experimental resultson two KGC tasks across four benchmark datasets demonstrate that SATsignificantly outperforms state-of-the-art methods, especially in the linkprediction task with improvements ranging from 8.7% to 29.8%.</description>
      <author>example@mail.com (Yu Liu, Yanan Cao, Xixun Lin, Yanmin Shang, Shi Wang, Shirui Pan)</author>
      <guid isPermaLink="false">2509.01166v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SC-GIR: Goal-oriented Semantic Communication via Invariant Representation Learning</title>
      <link>http://arxiv.org/abs/2509.01119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, Accepted to IEEE Transactions on Mobile Computing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为SC-GIR（基于不变表示的目标导向语义通信）的新框架，用于图像传输。该框架利用自监督学习提取不变表示，实现了高效的通信，同时保留了成功执行下游任务的关键特征。实验表明，SC-GIR比基线方案提高了约10%，并在不同信噪比条件下实现了超过85%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;目标导向语义通信（SC）旨在通过仅传输任务必需的信息来革新通信系统。然而，当前方法收发器联合训练面临挑战，导致冗余数据交换和依赖标记数据集，这限制了其任务无关的实用性。&lt;h4&gt;目的&lt;/h4&gt;解决当前目标导向语义通信方法中的问题，特别是收发器联合训练导致的冗余数据交换和依赖标记数据集的限制，提高任务无关的实用性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SC-GIR（基于不变表示的目标导向语义通信）的新框架。该框架利用自监督学习提取不变表示，该表示封装了来自源数据的关键信息，独立于特定的下游任务。使用基于协方差的对比学习技术获得有意义且语义密集的潜在表示。在各种图像数据集上应用该框架进行有损压缩，然后在目标导向AI任务中使用压缩表示。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的大量实验表明，SC-GIR比基线方案提高了约10%，并在不同信噪比条件下实现了超过85%的分类准确率。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架在学习紧凑和信息丰富的潜在表示方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;目标导向语义通信（SC）旨在通过仅传输任务必需的信息来革新通信系统。然而，当前方法在收发器联合训练方面面临挑战，导致冗余数据交换和依赖标记数据集，这限制了其任务无关的实用性。为了解决这些挑战，我们提出了一种名为基于不变表示的目标导向语义通信（SC-GIR）的新框架用于图像传输。我们的框架利用自监督学习提取不变表示，该表示封装了来自源数据的关键信息，独立于特定的下游任务。这种压缩表示促进了高效通信，同时保留了成功执行下游任务的关键特征。专注于机器间任务，我们使用基于协方差的对比学习技术获得有意义且语义密集的潜在表示。为了评估所提方案在下游任务上的有效性，我们在各种图像数据集上应用它进行有损压缩。然后，压缩表示用于目标导向AI任务。在多个数据集上的大量实验表明，SC-GIR比基线方案提高了约10%，并在不同信噪比条件下实现了超过85%的压缩数据分类准确率。这些结果强调了所提框架在学习紧凑和信息丰富的潜在表示方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TMC.2025.3600434&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Goal-oriented semantic communication (SC) aims to revolutionize communicationsystems by transmitting only task-essential information. However, currentapproaches face challenges such as joint training at transceivers, leading toredundant data exchange and reliance on labeled datasets, which limits theirtask-agnostic utility. To address these challenges, we propose a novelframework called Goal-oriented Invariant Representation-based SC (SC-GIR) forimage transmission. Our framework leverages self-supervised learning to extractan invariant representation that encapsulates crucial information from thesource data, independent of the specific downstream task. This compressedrepresentation facilitates efficient communication while retaining key featuresfor successful downstream task execution. Focusing on machine-to-machine tasks,we utilize covariance-based contrastive learning techniques to obtain a latentrepresentation that is both meaningful and semantically dense. To evaluate theeffectiveness of the proposed scheme on downstream tasks, we apply it tovarious image datasets for lossy compression. The compressed representationsare then used in a goal-oriented AI task. Extensive experiments on severaldatasets demonstrate that SC-GIR outperforms baseline schemes by nearly 10%,,and achieves over 85% classification accuracy for compressed data underdifferent SNR conditions. These results underscore the effectiveness of theproposed framework in learning compact and informative latent representations.</description>
      <author>example@mail.com (Senura Hansaja Wanasekara, Van-Dinh Nguyen, Kok-Seng, M. -Duong Nguyen, Symeon Chatzinotas, Octavia A. Dobre)</author>
      <guid isPermaLink="false">2509.01119v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Negative Matters: Multi-Granularity Hard-Negative Synthesis and Anchor-Token-Aware Pooling for Enhanced Text Embeddings</title>
      <link>http://arxiv.org/abs/2509.00842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种多粒度困难负样本(MGH)合成框架和锚点标记感知(ATA)池化方法，用于改进文本嵌入模型，在MTEB基准测试上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入模型对各种自然语言处理任务至关重要，它们能有效将语义信息编码为密集向量表示。这些模型通常使用三元组(query, positive, negative)数据进行对比学习优化，其中负样本在增强模型辨别细微语义差异方面起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;引入一种多粒度困难负样本(MGH)合成框架，利用大型语言模型生成多样化负样本；提出一种锚点标记感知(ATA)池化方法，基于在LLMs中观察到的聚合模式为锚点标记分配更高权重，以提高文本嵌入准确性而不增加模型复杂性。&lt;h4&gt;方法&lt;/h4&gt;1. 多粒度困难负样本(MGH)合成框架：利用大型语言模型生成多样化负样本，实现从粗到细的课程学习策略；2. 锚点标记感知(ATA)池化方法：根据LLMs中的聚合模式为锚点标记分配更高权重，提高文本嵌入准确性而不增加模型复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;在MTEB基准测试上的全面实验表明，所提出的方法实现了最先进的性能，超越了现有的合成策略，无论是使用合成数据还是与公共检索数据集结合使用时。&lt;h4&gt;结论&lt;/h4&gt;通过MGH合成框架和ATA池化方法，文本嵌入模型能够更有效地学习语义表示，在各种自然语言处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;文本嵌入模型对各种自然语言处理任务至关重要，能够将语义信息有效编码为密集向量表示。这些模型通常使用三元组(query, positive, negative)数据进行对比学习优化，其中负样本在增强模型辨别细微语义差异方面起着关键作用。在本工作中，我们引入了一种多粒度困难负样本(MGH)合成框架，利用大型语言模型生成与查询具有不同相似度级别的多样化负样本。这种方法在监督训练过程中实现了从粗到细的课程学习策略，使嵌入模型能够逐步学习更细致的语义表示。同时，我们提出了一种锚点标记感知(ATA)池化方法，基于在LLMs中观察到的聚合模式为锚点标记分配更高权重，提高了文本嵌入准确性而不增加模型复杂性。在MTEB基准测试上的全面实验表明，我们的方法实现了最先进的性能，超越了现有的合成策略，无论是使用合成数据还是与公共检索数据集结合使用时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.18653/v1/2025.acl-long.1501&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text embedding models are essential for various natural language processingtasks, enabling the effective encoding of semantic information into densevector representations. These models are typically optimized using triplets of(query, positive, negative) data pairs for contrastive learning, where thenegative samples play a critical role in enhancing the model's ability todiscern subtle semantic distinctions. In this work, we introduce aMulti-Granularity Hard-negative (MGH) synthesis framework that leverages largelanguage models (LLMs) to generate diverse negative samples with varying levelsof similarity with the query. This approach facilitates a coarse-to-finecurriculum learning strategy during supervised training, allowing the embeddingmodel to progressively learn more nuanced semantic representations. Meanwhile,we propose an Anchor Token Aware (ATA) pooling method that assigns higherweights to anchor tokens based on aggregation patterns observed in LLMs,improving text embedding accuracy without increasing model complexity.Comprehensive experiments on the MTEB benchmark demonstrate that our methodsachieve state-of-the-art performance, surpassing existing synthesis strategiesboth with synthetic data and when combined with public retrieval datasets.</description>
      <author>example@mail.com (Tengyu Pan, Zhichao Duan, Zhenyu Li, Bowen Dong, Ning Liu, Xiuxing Li, Jianyong Wang)</author>
      <guid isPermaLink="false">2509.00842v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Level CLS Token Fusion for Contrastive Learning in Endoscopy Image Classification</title>
      <link>http://arxiv.org/abs/2509.00752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对耳鼻喉科内窥镜图像分析的统一视觉-语言框架，同时处理图像分类、图像到图像检索和文本到图像检索三个临床相关任务。&lt;h4&gt;背景&lt;/h4&gt;传统基于CNN的管道难以捕捉跨模态语义，而医疗数据有限。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的视觉-语言框架，能够同时处理多个临床相关任务，并在有限医疗数据上实现高效微调。&lt;h4&gt;方法&lt;/h4&gt;使用CLIP ViT-B/16作为主干网络，通过低秩适配、多级CLS标记聚合和球形特征插值进行增强；引入特定类别的自然语言提示，结合监督分类和对比学习进行联合训练。&lt;h4&gt;主要发现&lt;/h4&gt;在ACM MM'25 ENTRep Grand Challenge中取得优异成绩，包括95%的分类准确率和F1分数，0.93的图像到图像检索Recall@1，0.92的文本到图像检索Recall@1，以及0.97和0.96的MRR分数。&lt;h4&gt;结论&lt;/h4&gt;消融研究证明了每个架构组件的增量效益，验证了该设计在资源有限的临床环境中对鲁棒多模态医疗理解的有效性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种专为耳鼻喉科内窥镜图像分析设计的统一视觉-语言框架，同时解决三个临床相关任务：图像分类、图像到图像检索和文本到图像检索。与难以捕捉跨模态语义的传统基于CNN的管道不同，我们的方法利用CLIP ViT-B/16主干网络，并通过低秩适配、多级CLS标记聚合和球形特征插值进行增强。这些组件共同实现在有限医疗数据上的高效微调，同时提高跨模态的表示多样性和语义对齐。为了弥合视觉输入和文本诊断上下文之间的差距，我们引入了特定类别的自然语言提示，通过结合监督分类和对比学习的联合训练目标指导图像编码器。我们通过参加ACM MM'25 ENTRep Grand Challenge验证了我们的框架，在分类中达到95%的准确率和F1分数，图像到图像和文本到图像检索的Recall@1分别为0.93和0.92，MRS分数分别为0.97和0.96。消融研究证明了每个架构组件的增量效益，验证了我们的设计在资源有限的临床环境中对鲁棒多模态医疗理解的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3762093&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a unified vision-language framework tailored for ENT endoscopyimage analysis that simultaneously tackles three clinically-relevant tasks:image classification, image-to-image retrieval, and text-to-image retrieval.Unlike conventional CNN-based pipelines that struggle to capture cross-modalsemantics, our approach leverages the CLIP ViT-B/16 backbone and enhances itthrough Low-Rank Adaptation, multi-level CLS token aggregation, and sphericalfeature interpolation. These components collectively enable efficientfine-tuning on limited medical data while improving representation diversityand semantic alignment across modalities. To bridge the gap between visualinputs and textual diagnostic context, we introduce class-specific naturallanguage prompts that guide the image encoder through a joint trainingobjective combining supervised classification with contrastive learning. Wevalidated our framework through participation in the ACM MM'25 ENTRep GrandChallenge, achieving 95% accuracy and F1-score in classification, Recall@1 of0.93 and 0.92 for image-to-image and text-to-image retrieval respectively, andMRR scores of 0.97 and 0.96. Ablation studies demonstrated the incrementalbenefits of each architectural component, validating the effectiveness of ourdesign for robust multimodal medical understanding in low-resource clinicalsettings.</description>
      <author>example@mail.com (Y Hop Nguyen, Doan Anh Phan Huu, Trung Thai Tran, Nhat Nam Mai, Van Toi Giap, Thao Thi Phuong Dao, Trung-Nghia Le)</author>
      <guid isPermaLink="false">2509.00752v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Valid Property-Enhanced Contrastive Learning for Targeted Optimization &amp; Resampling for Novel Drug Design</title>
      <link>http://arxiv.org/abs/2509.00684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/amartya21/vector-drug-design.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VECTOR+的新框架，用于在低数据条件下生成具有药理学相关特性的分子。该方法结合了属性引导的表示学习和可控的分子生成，适用于回归和分类任务，能够高效地探索功能性化学空间。&lt;h4&gt;背景&lt;/h4&gt;在分子药物发现中，特别是在数据有限的情况下，有效地引导生成模型朝向药理学相关的化学空间区域仍然是一个主要障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一个框架，能够在低数据条件下进行属性条件化的分子设计，结合对比学习和生成模型，实现可重现、AI加速的分子发现。&lt;h4&gt;方法&lt;/h4&gt;作者提出了VECTOR+（Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling），这是一个将属性引导的表示学习与可控分子生成相结合的框架。该方法适用于回归和分类任务，能够对功能性化学空间进行可解释、数据高效的探索。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在两个数据集上进行了评估：PD-L1抑制剂集合和受体激酶抑制剂集合。2. 尽管训练数据有限，VECTOR+能够生成新颖且具有合成可行性的候选分子。3. 针对PD-L1，生成分子中有100个超过了-15.0千卡每摩尔的对接阈值，最佳得分达到-17.6千卡每摩尔。4. 性能最佳的分子保留了保守的联苯药效团，同时引入了新的基序。5. 分子动力学证实了结合稳定性。6. VECTOR+可推广到激酶抑制剂，产生比已确立药物具有更强对接得分的化合物。7. 与其他方法的基准测试突显了该方法的优越性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果将作者的工作确立为一种在低数据设置下进行属性条件化分子设计的稳健、可扩展方法，将对比学习和生成建模相结合，实现可重现、AI加速的分子发现。&lt;h4&gt;翻译&lt;/h4&gt;在低数据条件下，有效地引导生成模型朝向药理学相关的化学空间区域仍然是分子药物发现中的一个主要障碍。我们提出了VECTOR+（Valid-property-Enhanced Contrastive Learning for Targeted Optimization and Resampling），这是一个将属性引导的表示学习与可控分子生成相结合的框架。VECTOR+适用于回归和分类任务，能够对功能性化学空间进行可解释、数据高效的探索。我们在两个数据集上进行了评估：一个经过整理的PD-L1抑制剂集合（296个具有实验IC50值的化合物）和一个受体激酶抑制剂集合（2,056个按结合模式分类的分子）。尽管训练数据有限，VECTOR+能够生成新颖且具有合成可行性的候选分子。针对PD-L1（PDB 5J89），在8,374个生成分子中有100个超过了-15.0千卡每摩尔的对接阈值，最佳得分达到-17.6千卡每摩尔，而顶级参考抑制剂的得分为-15.4千卡每摩尔。性能最佳的分子保留了保守的联苯药效团，同时引入了新的基序。分子动力学（250纳秒）证实了结合稳定性（配体RMSD &lt; 2.5埃）。VECTOR+可推广到激酶抑制剂，产生比已确立药物（如brigatinib和sorafenib）具有更强对接得分的化合物。与JT-VAE和MolGPT在对接、新颖性、独特性和Tanimoto相似性方面的基准测试突显了该方法的优越性能。这些结果将作者的工作确立为一种在低数据设置下进行属性条件化分子设计的稳健、可扩展方法，将对比学习和生成建模相结合，实现可重现、AI加速的分子发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficiently steering generative models toward pharmacologically relevantregions of chemical space remains a major obstacle in molecular drug discoveryunder low-data regimes. We present VECTOR+: Valid-property-Enhanced ContrastiveLearning for Targeted Optimization and Resampling, a framework that couplesproperty-guided representation learning with controllable molecule generation.VECTOR+ applies to both regression and classification tasks and enablesinterpretable, data-efficient exploration of functional chemical space. Weevaluate on two datasets: a curated PD-L1 inhibitor set (296 compounds withexperimental $IC_{50}$ values) and a receptor kinase inhibitor set (2,056molecules by binding mode). Despite limited training data, VECTOR+ generatesnovel, synthetically tractable candidates. Against PD-L1 (PDB 5J89), 100 of8,374 generated molecules surpass a docking threshold of $-15.0$ kcal/mol, withthe best scoring $-17.6$ kcal/mol compared to the top reference inhibitor($-15.4$ kcal/mol). The best-performing molecules retain the conserved biphenylpharmacophore while introducing novel motifs. Molecular dynamics (250 ns)confirm binding stability (ligand RMSD &lt; $2.5$ angstroms). VECTOR+ generalizesto kinase inhibitors, producing compounds with stronger docking scores thanestablished drugs such as brigatinib and sorafenib. Benchmarking against JT-VAEand MolGPT across docking, novelty, uniqueness, and Tanimoto similarityhighlights the superior performance of our method. These results position ourwork as a robust, extensible approach for property-conditioned molecular designin low-data settings, bridging contrastive learning and generative modeling forreproducible, AI-accelerated discovery.</description>
      <author>example@mail.com (Amartya Banerjee, Somnath Kar, Anirban Pal, Debabrata Maiti)</author>
      <guid isPermaLink="false">2509.00684v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive clustering based on regular equivalence for influential node identification in complex networks</title>
      <link>http://arxiv.org/abs/2509.02609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ReCC的深度无监督框架，用于识别复杂网络中的影响力节点，克服了现有监督方法对标记数据的依赖限制。&lt;h4&gt;背景&lt;/h4&gt;识别复杂网络中的影响力节点是网络分析的基本任务，有广泛的应用。现有监督方法受限于对标记数据的依赖，在实际标记数据稀缺的场景中适用性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记数据的影响力节点识别方法，解决现有监督方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出ReCC框架，将影响力节点识别重新定义为无标记深度聚类问题，开发基于正则等价相似性的对比学习机制，整合到图卷积网络中学习节点嵌入，使用网络重建损失预训练，结合对比和聚类损失微调，并通过结合结构指标和正则等价相似性增强节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;ReCC在多个基准测试中表现优于现有的最先进方法，证明了其在影响力节点识别任务上的有效性。&lt;h4&gt;结论&lt;/h4&gt;ReCC作为一种无监督学习方法，能够在不依赖标记数据的情况下有效识别复杂网络中的影响力节点，具有更广泛的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在复杂网络中识别影响力节点是网络分析中的一个基本任务，在各领域有广泛应用。虽然深度学习已推进了节点影响力检测，但现有监督方法仍受限于对标记数据的依赖，在标记数据稀缺或不可用的实际场景中适用性有限。虽然对比学习展示了显著的性能提升潜力，但现有方法主要依赖多嵌入生成来构建正/负样本对。为克服这些限制，我们提出了ReCC（基于正则等价的对比聚类），一种新颖的深度无监督框架，用于影响力节点识别。我们首先将影响力节点识别重新定义为无标记深度聚类问题，然后开发了利用基于正则等价相似性的对比学习机制，这种相似性捕获了节点超越局部邻域的结构相似性，用于生成正负样本。该机制被整合到图卷积网络中，用于学习用于区分影响力节点与非影响力节点的节点嵌入。ReCC使用网络重建损失进行预训练，并结合对比和聚类损失进行微调，两个阶段都独立于标记数据。此外，ReCC通过结合结构指标和基于正则等价的相似性来增强节点表示。大量实验表明，ReCC在多个基准测试中优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying influential nodes in complex networks is a fundamental task innetwork analysis with wide-ranging applications across domains. While deeplearning has advanced node influence detection, existing supervised approachesremain constrained by their reliance on labeled data, limiting theirapplicability in real-world scenarios where labels are scarce or unavailable.While contrastive learning demonstrates significant potential for performanceenhancement, existing approaches predominantly rely on multiple-embeddinggeneration to construct positive/negative sample pairs. To overcome theselimitations, we propose ReCC (\textit{r}egular \textit{e}quivalence-based\textit{c}ontrastive \textit{c}lustering), a novel deep unsupervised frameworkfor influential node identification. We first reformalize influential nodeidentification as a label-free deep clustering problem, then develop acontrastive learning mechanism that leverages regular equivalence-basedsimilarity, which captures structural similarities between nodes beyond localneighborhoods, to generate positive and negative samples. This mechanism isintegrated into a graph convolutional network to learn node embeddings that areused to differentiate influential from non-influential nodes. ReCC ispre-trained using network reconstruction loss and fine-tuned with a combinedcontrastive and clustering loss, with both phases being independent of labeleddata. Additionally, ReCC enhances node representations by combining structuralmetrics with regular equivalence-based similarities. Extensive experimentsdemonstrate that ReCC outperforms state-of-the-art approaches across severalbenchmarks.</description>
      <author>example@mail.com (Yanmei Hu, Yihang Wu, Bing Sun, Xue Yue, Biao Cai, Xiangtao Li, Yang Chen)</author>
      <guid isPermaLink="false">2509.02609v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>CoMET: A Contrastive-Masked Brain Foundation Model for Universal EEG Representation</title>
      <link>http://arxiv.org/abs/2509.00314v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoMET是一种新型脑基础模型，通过重新设计的掩码自编码器和对比学习框架解决现有EEG模型过度关注局部特征而忽略全局判别模式的问题。该模型在3000多名受试者、超过100万个样本的混合EEG数据集上预训练，并在十个下游数据集上取得了最先进的结果，展示了提取通用EEG表示和临床应用的强大潜力。&lt;h4&gt;背景&lt;/h4&gt;EEG是一种非侵入性记录脑活动的技术，广泛应用于脑机接口、临床和医疗保健领域。传统EEG深度模型通常专注于特定数据集和任务，限制了模型大小和泛化能力。最近，自监督脑基础模型出现并应用于各种下游任务。&lt;h4&gt;目的&lt;/h4&gt;解决现有自监督EEG模型的局限性，特别是它们过度关注局部区域的低维信号相似性特征而忽略全局判别模式的问题，提出一种能够提取全局判别模式的脑基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出名为CoMET的脑基础模型，采用重新设计的分块和嵌入的掩码自编码器作为骨干网络，设计了一种新的对比学习框架，包含镜像尺度增强来加强全局判别能力。在超过3000个受试者、超过100万个样本的混合EEG数据集上进行预训练，并在十个不同的下游数据集上进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;CoMET在十个不同的下游数据集上取得了最先进的实验结果，证明了其提取通用EEG表示的优越能力和强大的临床潜力。&lt;h4&gt;结论&lt;/h4&gt;CoMET模型有效解决了现有EEG基础模型的局限性，能够提取更全面的EEG特征，包括全局判别模式，在多种下游任务中表现优异，具有良好的临床应用前景。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)是一种记录脑活动的非侵入性技术，广泛应用于脑机接口、临床和医疗保健领域。传统的EEG深度模型通常专注于特定数据集和任务，限制了模型大小和泛化能力。最近，自监督脑基础模型已经出现并应用于各种下游任务。然而，这些模型仍然存在局限性：当前的最先进模型通常依赖掩码重建策略；但是，相邻通道的EEG特征高度相关，这导致预训练过度关注局部区域的低维信号相似性特征，而忽略了对下游任务至关重要的全局判别模式。为解决这些局限性，我们提出了一种名为CoMET的脑基础模型。具体来说，我们采用重新设计的分块和嵌入的掩码自编码器作为EEG的骨干网络，并设计了一种新颖的包含镜像尺度增强的对比学习框架，以加强全局判别能力。CoMET在超过3000个受试者、超过100万个样本的混合EEG数据集上进行预训练。它在十个不同的下游数据集上进行了评估，最先进的结果证明了CoMET提取通用EEG表示的优越能力和强大的临床潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) is a non-invasive technique for recording brainactivity, widely used in brain-computer interfaces, clinic, and healthcare.Traditional EEG deep models typically focus on specific dataset and task,limiting model size and generalization. Recently, self-supervised brainfoundation models have emerged and been applied to various downstream tasks.Nevertheless, these models still have limitations: current SOTA modelstypically rely on masked reconstruction strategy; however, EEG features ofadjacent channels are highly correlated, which causes the pre-training tooverly focus on low-dimensional signal-similarity features in local regions andneglect the global discriminative patterns vital for downstream tasks. Toaddress these limitations, we propose a brain foundation model called CoMET.Specifically, we employ the masked autoencoder with redesigned patching andembedding for EEG as backbone and devise a novel contrastive learning frameworkwith mirror-scale augmentation to strengthen the global discrimination ability.CoMET is pre-trained on mixed EEG datasets over 3000 subjects with over onemillion samples. It is evaluated on ten different downstream datasets, and theSOTA results demonstrate CoMET's superior ability in extracting universal EEGrepresentations and strong clinical potential.</description>
      <author>example@mail.com (Ang Li, Zikai Wang, Liuyin Yang, Zhenyu Wang, Tianheng Xu, Honglin Hu, Marc M. Van Hulle)</author>
      <guid isPermaLink="false">2509.00314v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MorphGen: Morphology-Guided Representation Learning for Robust Single-Domain Generalization in Histopathological Cancer Classification</title>
      <link>http://arxiv.org/abs/2509.00311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出MorphGen方法，通过整合组织病理学图像、增强技术和细胞核分割掩膜，在监督对比学习框架中学习对领域变化具有鲁棒性的癌症表征，解决计算病理学中的领域泛化问题。&lt;h4&gt;背景&lt;/h4&gt;计算病理学中的领域泛化受到全幻灯片图像异质性的阻碍，这种异质性由不同机构间的组织制备、染色和成像条件差异造成。与机器学习系统不同，病理学家依靠跨不同环境保持诊断价值的领域不变形态学线索，如细胞核异型性、结构异型性和整体形态异型性。&lt;h4&gt;目的&lt;/h4&gt;假设通过明确建模生物学上稳健的细胞核形态和空间组织，能够学习到对领域变化具有韧性的癌症表征，提高模型在不同数据分布上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;MorphGen是一种在监督对比学习框架中整合组织病理学图像、增强技术和细胞核分割掩膜的方法。通过对齐图像和细胞核掩膜的潜在表征，优先考虑诊断特征而非染色伪影。此外，引入随机权重平均（SWA）技术，以优化更平坦的最小值，进一步增强分布外鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;注意力图分析显示，MorphGen主要依赖于细胞核形态、细胞组成和肿瘤或正常区域内的空间细胞组织进行分类。学习到的表征对图像损坏和对抗性攻击具有韧性，不仅展示了分布外泛化能力，还解决了当前深度学习系统在数字病理学中的关键漏洞。&lt;h4&gt;结论&lt;/h4&gt;MorphGen通过模拟病理学家使用的领域不变形态学线索，成功提高了计算病理学模型在不同机构数据上的泛化能力，增强了模型对图像损坏和对抗性攻击的鲁棒性，对数字病理学领域的实际应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学中的领域泛化受到全幻灯片图像异质性的阻碍，这种异质性由不同机构间的组织制备、染色和成像条件差异造成。与机器学习系统不同，病理学家依靠跨不同环境保持诊断价值的领域不变形态学线索，如细胞核异型性（增大、不规则轮廓、深染、染色质纹理、空间无序）、结构异型性（异常结构和腺体形成）和整体形态异型性。受此启发，我们假设通过明确建模生物学上稳健的细胞核形态和空间组织，能够学习到对领域变化具有韧性的癌症表征。我们提出了MorphGen（形态引导泛化），一种在监督对比学习框架中整合组织病理学图像、增强技术和细胞核分割掩膜的方法。通过对齐图像和细胞核掩膜的潜在表征，MorphGen优先考虑细胞核和形态异型性以及空间组织等诊断特征，而非染色伪影和领域特定特征。为进一步增强分布外鲁棒性，我们纳入了随机权重平均（SWA），引导优化朝向更平坦的最小值。注意力图分析显示，MorphGen主要依赖于细胞核形态、细胞组成和肿瘤或正常区域内的空间细胞组织进行最终分类。最后，我们证明了学习到的表征对图像损坏（如染色伪影）和对抗性攻击具有韧性，不仅展示了分布外泛化能力，还解决了当前深度学习系统在数字病理学中的关键漏洞。代码、数据集和训练模型可在以下网址获取：https://github.com/hikmatkhan/MorphGen&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain generalization in computational histopathology is hindered byheterogeneity in whole slide images (WSIs), caused by variations in tissuepreparation, staining, and imaging conditions across institutions. Unlikemachine learning systems, pathologists rely on domain-invariant morphologicalcues such as nuclear atypia (enlargement, irregular contours, hyperchromasia,chromatin texture, spatial disorganization), structural atypia (abnormalarchitecture and gland formation), and overall morphological atypia that remaindiagnostic across diverse settings. Motivated by this, we hypothesize thatexplicitly modeling biologically robust nuclear morphology and spatialorganization will enable the learning of cancer representations that areresilient to domain shifts. We propose MorphGen (Morphology-GuidedGeneralization), a method that integrates histopathology images, augmentations,and nuclear segmentation masks within a supervised contrastive learningframework. By aligning latent representations of images and nuclear masks,MorphGen prioritizes diagnostic features such as nuclear and morphologicalatypia and spatial organization over staining artifacts and domain-specificfeatures. To further enhance out-of-distribution robustness, we incorporatestochastic weight averaging (SWA), steering optimization toward flatter minima.Attention map analyses revealed that MorphGen primarily relies on nuclearmorphology, cellular composition, and spatial cell organization within tumorsor normal regions for final classification. Finally, we demonstrate resilienceof the learned representations to image corruptions (such as stainingartifacts) and adversarial attacks, showcasing not only OOD generalization butalso addressing critical vulnerabilities in current deep learning systems fordigital pathology. Code, datasets, and trained models are available at:https://github.com/hikmatkhan/MorphGen</description>
      <author>example@mail.com (Hikmat Khan, Syed Farhan Alam Zaidi, Pir Masoom Shah, Kiruthika Balakrishnan, Rabia Khan, Muhammad Waqas, Jia Wu)</author>
      <guid isPermaLink="false">2509.00311v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning based Element Resource Allocation for Reconfigurable Intelligent Surfaces in mmWave Network</title>
      <link>http://arxiv.org/abs/2509.03241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种基于神经网络的解决方案，用于可重构智能表面（RIS）的相位配置和资源分配优化。通过结合预处理技术和五层全连接神经网络，显著降低了计算复杂度，提高了系统吞吐量，并增强了可扩展性。&lt;h4&gt;背景&lt;/h4&gt;无线系统中对高速率和无缝连接的需求不断增长，激发了对可重构智能表面（RIS）和基于人工智能的无线应用的兴趣。RIS通常由被动反射天线元件组成，通过调节反射元件的相位来控制无线传播环境。将RIS元件分配给多个用户设备对于有效利用RIS至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决RIS相位配置和资源分配的联合优化问题，特别是在α公平调度框架下。同时，克服传统迭代优化方法随着RIS元件数量增加而导致的计算复杂度指数增长问题，以及监督学习中训练标签生成的复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合预处理技术的五层全连接神经网络（FNN）方法，以显著降低输入维度、降低计算复杂度并增强可扩展性。这种方法解决了传统迭代优化方法的局限性。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果表明，所提出的基于神经网络的解决方案与现有的RIS元件分配方案相比，在减少计算开销的同时，显著提高了系统吞吐量6.8%。此外，所提出的系统在降低计算复杂度的同时实现了更好的性能，使其比迭代优化算法具有显著更高的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;基于神经网络的解决方案在RIS无线通信系统中表现出色，不仅提高了系统性能，还解决了传统方法的计算复杂度问题，为未来无线通信系统提供了更高效、可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;无线系统中对高速数据和无缝连接日益增长的需求，激发了对可重构智能表面（RIS）和基于人工智能的无线应用的浓厚兴趣。RIS通常由被动反射天线元件组成，通过适当调节反射元件的相位来控制无线传播环境。将RIS元件分配给多个用户设备对于有效利用RIS至关重要。在这项工作中，我们制定了一个联合优化问题，在α公平调度框架下优化RIS相位配置和资源分配，并提出了一种高效的RIS元件分配方式。然而，传统的迭代优化方法随着RIS元件数量的增加而面临计算复杂度指数增长的问题，同时也增加了监督学习中训练标签生成的复杂性。为了克服这些挑战，我们提出了一种结合预处理技术的五层全连接神经网络（FNN），以显著降低输入维度、降低计算复杂度并增强可扩展性。仿真结果表明，我们提出的基于神经网络的解决方案在减少计算开销的同时，与现有的RIS元件分配方案相比，显著提高了系统吞吐量6.8%。此外，所提出的系统在降低计算复杂度的同时实现了更好的性能，使其比迭代优化算法具有显著更高的可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing demand for high data rates and seamless connectivity inwireless systems has sparked significant interest in reconfigurable intelligentsurfaces (RIS) and artificial intelligence-based wireless applications. RIStypically comprises passive reflective antenna elements that control thewireless propagation environment by adequately tuning the phase of thereflective elements. The allocation of RIS elements to multipleuser equipment(UEs) is crucial for efficiently utilizing RIS. In this work, we formulate ajoint optimization problem that optimizes the RIS phase configuration andresource allocation under an $\alpha$-fair scheduling framework and propose anefficient way of allocating RIS elements. Conventional iterative optimizationmethods, however, suffer from exponentially increasing computational complexityas the number of RIS elements increases and also complicate the generation oftraining labels for supervised learning. To overcome these challenges, wepropose a five-layer fully connected neural network (FNN) combined with apreprocessing technique to significantly reduce input dimensionality, lowercomputational complexity, and enhance scalability. The simulation results showthat our proposed NN-based solution reduces computational overhead whilesignificantly improving system throughput by 6.8% compared to existing RISelement allocation schemes. Furthermore, the proposed system achieves betterperformance while reducing computational complexity, making it significantlymore scalable than the iterative optimization algorithms.</description>
      <author>example@mail.com (Pujitha Mamillapalli, Yoghitha Ramamoorthi, Abhinav Kumar, Tomoki Murakami, Tomoaki Ogawa, Yasushi Takatori)</author>
      <guid isPermaLink="false">2509.03241v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Isolated Bangla Handwritten Character Classification using Transfer Learning</title>
      <link>http://arxiv.org/abs/2509.03061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Comments: 13 pages, 14 figures, published in the Proceedings of the  2nd International Conference on Computing Advancements (ICCA 2022), IEEE.  Strong experimental section with comparisons across models (3DCNN, ResNet50,  MobileNet)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种使用迁移学习的方法来分类孟加拉语手写字符，包括基本字符、独特字符和复合字符，解决了梯度消失问题。&lt;h4&gt;背景&lt;/h4&gt;孟加拉语由50个不同字符和许多复合字符组成。已有一些针对手写和光学孟加拉语字符识别的研究。&lt;h4&gt;目的&lt;/h4&gt;使用迁移学习分类基础、独特和复合的孟加拉语手写字符，同时避免梯度消失问题。&lt;h4&gt;方法&lt;/h4&gt;应用深度神经网络技术，包括3D卷积神经网络(3DCNN)、残差神经网络(ResNet)和MobileNet，实现孟加拉语手写字符所有可能标准形式的端到端分类。&lt;h4&gt;主要发现&lt;/h4&gt;模型在训练数据上达到99.82%的准确率，在测试数据上达到99.46%的准确率。&lt;h4&gt;结论&lt;/h4&gt;与各种孟加拉语手写字符分类的最先进基准相比，所提出的模型在分类数据方面取得了更好的准确性。&lt;h4&gt;翻译&lt;/h4&gt;孟加拉语由50个不同字符和许多复合字符组成。已经进行了多项研究来识别手写和光学孟加拉语字符。我们的方法使用迁移学习来分类基础、独特以及复合的孟加拉语手写字符，同时避免梯度消失问题。应用了深度神经网络技术，如3D卷积神经网络(3DCNN)、残差神经网络(ResNet)和MobileNet，生成孟加拉语手写字符所有可能标准形式的端到端分类。使用包含166,105个分为84个不同类别的孟加拉语字符图像样本的孟加拉语Lekha孤立数据集进行此分类模型。该模型在训练数据上达到99.82%的准确率，在测试数据上达到99.46%的准确率。与各种孟加拉语手写字符分类的最先进基准相比，所提出的模型在分类数据方面取得了更好的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bangla language consists of fifty distinct characters and many compoundcharacters. Several notable studies have been performed to recognize Banglacharacters, both handwritten and optical. Our approach uses transfer learningto classify the basic, distinct, as well as compound Bangla handwrittencharacters while avoiding the vanishing gradient problem. Deep Neural Networktechniques such as 3D Convolutional Neural Network (3DCNN), Residual NeuralNetwork (ResNet), and MobileNet are applied to generate an end-to-endclassification of all possible standard formations of handwritten characters inthe Bangla language. The Bangla Lekha Isolated dataset, which contains 166,105Bangla character image samples categorized into 84 distinct classes, is usedfor this classification model. The model achieved 99.82% accuracy on trainingdata and 99.46% accuracy on test data. Comparisons with variousstate-of-the-art benchmarks of Bangla handwritten character classification showthat the proposed model achieves better accuracy in classifying the data.</description>
      <author>example@mail.com (Abdul Karim, S M Rafiuddin, Jahidul Islam Razin, Tahira Alam)</author>
      <guid isPermaLink="false">2509.03061v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal learning of melt pool dynamics in laser powder bed fusion</title>
      <link>http://arxiv.org/abs/2509.03029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 6 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种多模态数据融合方法，结合高保真X射线数据和低保真吸收率数据来预测增材制造中的熔池动力学，通过迁移学习技术实现了仅使用低成本吸收率数据就能准确监测熔池特性的目标。&lt;h4&gt;背景&lt;/h4&gt;在增材制造中，虽然使用多种传感器进行实时监测，但并非所有传感器都能提供实用可靠的过程信息。高速X射线成像提供宝贵空间信息但成本高昂，不适合大多数工业环境；低成本光电二极管的吸收率数据与熔池动力学相关但单独使用时噪声过大，难以准确预测。&lt;h4&gt;目的&lt;/h4&gt;开发一种多模态数据融合方法，通过结合高保真X射线数据和低保真吸收率数据，实现激光粉末床熔融(LPBF)过程中熔池动力学的准确预测，并最终实现仅使用低成本吸收率数据的监测。&lt;h4&gt;方法&lt;/h4&gt;构建多模态学习框架，整合卷积神经网络(CNNs)从X射线数据中提取空间特征，以及循环神经网络(RNNs)从吸收率信号中提取时间特征，采用早期融合策略。将多模态模型作为迁移学习模型，微调仅使用吸收率数据的RNN模型以提高预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;与单独使用任何一种模态相比，同时使用两种模态进行训练显著提高了预测准确性。训练完成后，模型仅使用吸收率数据就可以推断熔池特性，无需昂贵的X射线成像设备。&lt;h4&gt;结论&lt;/h4&gt;这种多模态融合方法能够实现经济有效的实时监测，在增材制造领域具有广泛适用性，解决了高精度监测与成本效益之间的矛盾。&lt;h4&gt;翻译&lt;/h4&gt;虽然多个传感器用于增材制造的实时监测，但并非所有传感器都能提供实用或可靠的过程洞察。例如，高速X射线成像提供了关于亚表面熔池行为的宝贵空间信息，但成本高昂且对大多数工业环境不切实际。相比之下，低成本光电二极管的吸收率数据与熔池动力学相关，但单独使用时通常过于嘈杂而无法准确预测。在本文中，我们提出了一种多模态数据融合方法，通过结合激光粉末床熔融(LPBF)过程中的高保真X射线数据和低保真吸收率数据来预测熔池动力学。我们的多模态学习框架整合了卷积神经网络(CNNs)用于从X射线数据中提取空间特征，以及循环神经网络(RNNs)用于从吸收率信号中提取时间特征，采用早期融合策略。多模态模型进一步用作迁移学习模型，对仅使用吸收率数据就能预测熔池动力学的RNN模型进行微调，相比多模态模型具有更高的准确性。结果表明，与单独使用任何一种模态相比，使用两种模态进行训练显著提高了预测准确性。此外，一旦训练完成，该模型仅使用吸收率数据就可以推断熔池特性，消除了对昂贵X射线成像的需求。这种多模态融合方法能够实现经济有效的实时监测，并在增材制造领域具有广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While multiple sensors are used for real-time monitoring in additivemanufacturing, not all provide practical or reliable process insights. Forexample, high-speed X-ray imaging offers valuable spatial information aboutsubsurface melt pool behavior but is costly and impractical for most industrialsettings. In contrast, absorptivity data from low-cost photodiodes correlatewith melt pool dynamics but is often too noisy for accurate prediction whenused alone. In this paper, we propose a multimodal data fusion approach forpredicting melt pool dynamics by combining high-fidelity X-ray data withlow-fidelity absorptivity data in the Laser Powder Bed Fusion (LPBF) process.Our multimodal learning framework integrates convolutional neural networks(CNNs) for spatial feature extraction from X-ray data with recurrent neuralnetworks (RNNs) for temporal feature extraction from absorptivity signals,using an early fusion strategy. The multimodal model is further used as atransfer learning model to fine-tune the RNN model that can predict melt pooldynamics only with absorptivity, with greater accuracy compared to themultimodal model. Results show that training with both modalities significantlyimproves prediction accuracy compared to using either modality alone.Furthermore, once trained, the model can infer melt pool characteristics usingonly absorptivity data, eliminating the need for expensive X-ray imaging. Thismultimodal fusion approach enables cost-effective, real-time monitoring and hasbroad applicability in additive manufacturing.</description>
      <author>example@mail.com (Satyajit Mojumder, Pallock Halder, Tiana Tonge)</author>
      <guid isPermaLink="false">2509.03029v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>RiverScope: High-Resolution River Masking Dataset</title>
      <link>http://arxiv.org/abs/2509.02451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了RiverScope，一个高分辨率河流和地表水数据集，由计算机科学和水文学专家合作开发，包含1,145张高分辨率图像和专家标注的掩膜数据，与多种卫星数据配准，为精细尺度水文监测提供资源。&lt;h4&gt;背景&lt;/h4&gt;地表水动态在地球气候系统中扮演关键角色，影响生态系统、农业、灾害恢复力和可持续发展。然而，在精细时空尺度上监测河流和地表水具有挑战性，特别是对于窄河或富含沉积物的河流，低分辨率卫星数据难以捕捉。&lt;h4&gt;目的&lt;/h4&gt;开发一个高分辨率数据集来解决精细尺度河流和地表水监测的挑战，并支持多传感器水文监测，同时评估不同传感器间的成本-准确性权衡。&lt;h4&gt;方法&lt;/h4&gt;创建RiverScope数据集，包含1,145张高分辨率图像（覆盖2,577平方公里）和专家标记的河流和地表水掩膜（需100多小时手工标注）；将每张图像与Sentinel-2、SWOT和SWOT河流数据库配准；建立全球首个高分辨率河流宽度估计基准；评估多种深度网络架构、预训练策略和训练数据集。&lt;h4&gt;主要发现&lt;/h4&gt;建立了全球首个高分辨率河流宽度估计基准，中位误差为7.2米，显著优于现有卫星衍生方法；最佳性能模型结合了迁移学习的优势，并通过学习适配器使用所有多光谱PlanetScope通道。&lt;h4&gt;结论&lt;/h4&gt;RiverScope为精细尺度和多传感器水文建模提供了宝贵资源，支持气候适应和可持续水资源管理。&lt;h4&gt;翻译&lt;/h4&gt;地表水动态在地球气候系统中起着关键作用，影响着生态系统、农业、灾害恢复力和可持续发展。然而，在精细的时空尺度上监测河流和地表水仍然具有挑战性——特别是对于低分辨率卫星数据难以捕捉的窄河或富含沉积物的河流。为此，我们引入了RiverScope，这是一个由计算机科学和水文学专家合作开发的高分辨率数据集。RiverScope包含1,145张高分辨率图像（覆盖2,577平方公里），配有专家标记的河流和地表水掩膜，需要超过100小时的手工标注。每张图像都与Sentinel-2、SWOT和SWOT河流数据库（SWORD）配准，能够评估不同传感器之间的成本-准确性权衡——这是业务水资源监测的一个关键考虑因素。我们还建立了全球首个高分辨率河流宽度估计基准，实现了7.2米的中位误差——显著优于现有的卫星衍生方法。我们在多种架构（如CNN和transformers）、预训练策略（如监督和自监督）以及训练数据集（如ImageNet和卫星图像）上广泛评估了深度网络。我们表现最佳的模型结合了迁移学习的优势，并通过学习适配器使用了所有多光谱PlanetScope通道。RiverScope为精细尺度和多传感器水文建模提供了宝贵资源，支持气候适应和可持续水资源管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surface water dynamics play a critical role in Earth's climate system,influencing ecosystems, agriculture, disaster resilience, and sustainabledevelopment. Yet monitoring rivers and surface water at fine spatial andtemporal scales remains challenging -- especially for narrow or sediment-richrivers that are poorly captured by low-resolution satellite data. To addressthis, we introduce RiverScope, a high-resolution dataset developed throughcollaboration between computer science and hydrology experts. RiverScopecomprises 1,145 high-resolution images (covering 2,577 square kilometers) withexpert-labeled river and surface water masks, requiring over 100 hours ofmanual annotation. Each image is co-registered with Sentinel-2, SWOT, and theSWOT River Database (SWORD), enabling the evaluation of cost-accuracytrade-offs across sensors -- a key consideration for operational watermonitoring. We also establish the first global, high-resolution benchmark forriver width estimation, achieving a median error of 7.2 meters -- significantlyoutperforming existing satellite-derived methods. We extensively evaluate deepnetworks across multiple architectures (e.g., CNNs and transformers),pretraining strategies (e.g., supervised and self-supervised), and trainingdatasets (e.g., ImageNet and satellite imagery). Our best-performing modelscombine the benefits of transfer learning with the use of all the multispectralPlanetScope channels via learned adaptors. RiverScope provides a valuableresource for fine-scale and multi-sensor hydrological modeling, supportingclimate adaptation and sustainable water management.</description>
      <author>example@mail.com (Rangel Daroya, Taylor Rowley, Jonathan Flores, Elisa Friedmann, Fiona Bennitt, Heejin An, Travis Simmons, Marissa Jean Hughes, Camryn L Kluetmeier, Solomon Kica, J. Daniel Vélez, Sarah E. Esenther, Thomas E. Howard, Yanqi Ye, Audrey Turcotte, Colin Gleason, Subhransu Maji)</author>
      <guid isPermaLink="false">2509.02451v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Speech transformer models for extracting information from baby cries</title>
      <link>http://arxiv.org/abs/2509.02259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to WOCCI2025 (interspeech2025 workshop)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了预训练语音模型的潜在表示对非语音数据的适用性及其编码的声学特性&lt;h4&gt;背景&lt;/h4&gt;使用预训练语音模型的潜在表示进行迁移学习在标记数据稀少的任务中表现出色，但这些表示对非语音数据的适用性以及编码的具体声学特性很大程度上仍未探索&lt;h4&gt;目的&lt;/h4&gt;研究预训练语音模型的潜在表示对非语音数据的适用性及其编码的具体声学特性&lt;h4&gt;方法&lt;/h4&gt;评估五个预训练语音模型在八个婴儿哭声数据集上的表现，这些数据集包含来自960个婴儿的115小时音频。对于每个数据集，评估每个模型在所有可用分类任务中的潜在表示&lt;h4&gt;主要发现&lt;/h4&gt;这些模型的潜在表示可以有效分类人类婴儿哭声，并编码与发声源不稳定性和哭泣婴儿身份相关的关键信息&lt;h4&gt;结论&lt;/h4&gt;比较这些模型的架构和训练策略为未来针对类似任务（如情绪检测）的定制模型设计提供了有价值的见解&lt;h4&gt;翻译&lt;/h4&gt;使用预训练语音模型的潜在表示进行迁移学习在标记数据稀少的任务中取得了卓越的性能。然而，这些表示对非语音数据的适用性以及编码在这些表示中的特定声学特性很大程度上仍未被探索。在本研究中，我们调查了这两个方面。我们在八个婴儿哭声数据集上评估了五个预训练语音模型，这些数据集包含来自960个婴儿的115小时音频。对于每个数据集，我们评估了每个模型在所有可用分类任务中的潜在表示。我们的结果表明，这些模型的潜在表示可以有效分类人类婴儿哭声，并编码与发声源不稳定性和哭泣婴儿身份相关的关键信息。此外，比较这些模型的架构和训练策略为未来针对类似任务（如情绪检测）的定制模型设计提供了有价值的见解&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning using latent representations from pre-trained speech modelsachieves outstanding performance in tasks where labeled data is scarce.However, their applicability to non-speech data and the specific acousticproperties encoded in these representations remain largely unexplored. In thisstudy, we investigate both aspects. We evaluate five pre-trained speech modelson eight baby cries datasets, encompassing 115 hours of audio from 960 babies.For each dataset, we assess the latent representations of each model across allavailable classification tasks. Our results demonstrate that the latentrepresentations of these models can effectively classify human baby cries andencode key information related to vocal source instability and identity of thecrying baby. In addition, a comparison of the architectures and trainingstrategies of these models offers valuable insights for the design of futuremodels tailored to similar tasks, such as emotion detection.</description>
      <author>example@mail.com (Guillem Bonafos, Jéremy Rouch, Lény Lego, David Reby, Hugues Patural, Nicolas Mathevon, Rémy Emonet)</author>
      <guid isPermaLink="false">2509.02259v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>HydroVision: Predicting Optically Active Parameters in Surface Water Using Computer Vision</title>
      <link>http://arxiv.org/abs/2509.01882v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is under peer review for IEEE Journal of Oceanic  Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了HydroVision，一个基于深度学习的场景分类框架，能够从标准RGB图像中估算多种水质参数，为环境监测提供了一种经济有效的替代方案。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉技术的进步，特别是在模式识别和场景分类方面，为环境监测开辟了新的应用可能性。深度学习现在提供了非接触式的水质评估和污染检测方法，这对灾害响应和公共卫生保护至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从RGB图像中估算光学活性水质参数的框架，支持污染趋势的早期检测，并加强监管机构在环境压力、工业活动和不可抗力事件期间的监测能力。&lt;h4&gt;方法&lt;/h4&gt;创建HydroVision深度学习框架，使用50多万张从美国地质调查局收集的季节变化图像进行训练，评估五种架构（VGG-16、ResNet50、MobileNetV2、DenseNet121和Vision Transformer），通过迁移学习确定最佳性能架构。&lt;h4&gt;主要发现&lt;/h4&gt;DenseNet121在验证性能上表现最佳，在预测有色溶解有机物(CDOM)时R²得分为0.89，证明了该框架在不同条件下进行实际水质监测的潜力。&lt;h4&gt;结论&lt;/h4&gt;HydroVision利用广泛可用的RGB图像作为传统多光谱和高光谱遥感的可扩展、经济有效的替代方案。未来工作将提高模型在低光和受阻场景下的鲁棒性，以扩大其实用性。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉技术的持续进步，特别是在模式识别和场景分类方面，为环境监测开辟了新的应用。深度学习现在为评估水质和检测污染提供了非接触式方法，这两者对于灾害响应和公共卫生保护都至关重要。这项工作介绍了HydroVision，一个基于深度学习的场景分类框架，可以从标准红绿蓝(RGB)图像中估算光学活性水质参数，包括叶绿素-Alpha、叶绿素、有色溶解有机物(CDOM)、藻蓝蛋白、悬浮沉积物和浊度。HydroVision支持污染趋势的早期检测，并在外部环境压力、工业活动和不可抗力事件期间，加强监管机构的监测能力。该模型是在2022年至2024年间从美国地质调查局水文图像可视化和信息系统收集的50多万张季节变化图像上训练的。这种方法利用广泛可用的RGB图像作为传统多光谱和高光谱遥感的可扩展、经济有效的替代方案。通过迁移学习评估了四种最先进的卷积神经网络(VGG-16、ResNet50、MobileNetV2、DenseNet121)和一个视觉变换器，以确定性能最佳的架构。DenseNet121在验证性能上表现最佳，在预测CDOM时R²得分为0.89，证明了该框架在不同条件下进行实际水质监测的潜力。虽然当前模型针对光照良好的图像进行了优化，但未来的工作将侧重于提高在低光和受阻场景下的鲁棒性，以扩大其操作实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ongoing advancements in computer vision, particularly in pattern recognitionand scene classification, have enabled new applications in environmentalmonitoring. Deep learning now offers non-contact methods for assessing waterquality and detecting contamination, both critical for disaster response andpublic health protection. This work introduces HydroVision, a deeplearning-based scene classification framework that estimates optically activewater quality parameters including Chlorophyll-Alpha, Chlorophylls, ColoredDissolved Organic Matter (CDOM), Phycocyanins, Suspended Sediments, andTurbidity from standard Red-Green-Blue (RGB) images of surface water.HydroVision supports early detection of contamination trends and strengthensmonitoring by regulatory agencies during external environmental stressors,industrial activities, and force majeure events. The model is trained on morethan 500,000 seasonally varied images collected from the United StatesGeological Survey Hydrologic Imagery Visualization and Information Systembetween 2022 and 2024. This approach leverages widely available RGB imagery asa scalable, cost-effective alternative to traditional multispectral andhyperspectral remote sensing. Four state-of-the-art convolutional neuralnetworks (VGG-16, ResNet50, MobileNetV2, DenseNet121) and a Vision Transformerare evaluated through transfer learning to identify the best-performingarchitecture. DenseNet121 achieves the highest validation performance, with anR2 score of 0.89 in predicting CDOM, demonstrating the framework's promise forreal-world water quality monitoring across diverse conditions. While thecurrent model is optimized for well-lit imagery, future work will focus onimproving robustness under low-light and obstructed scenarios to expand itsoperational utility.</description>
      <author>example@mail.com (Shubham Laxmikant Deshmukh, Matthew Wilchek, Feras A. Batarseh)</author>
      <guid isPermaLink="false">2509.01882v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TransMatch: A Transfer-Learning Framework for Defect Detection in Laser Powder Bed Fusion Additive Manufacturing</title>
      <link>http://arxiv.org/abs/2509.01754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出TransMatch框架，结合迁移学习和半监督少样本学习，解决增材制造中标记数据稀缺的表面缺陷检测问题&lt;h4&gt;背景&lt;/h4&gt;激光粉末床熔融(LPBF)中的表面缺陷会对增材制造组件的结构完整性构成重大风险&lt;h4&gt;目的&lt;/h4&gt;引入TransMatch框架，解决增材制造缺陷标记数据稀缺的问题&lt;h4&gt;方法&lt;/h4&gt;TransMatch结合迁移学习和半监督少样本学习，有效利用标记和新类别图像，克服了先前元学习方法的限制&lt;h4&gt;主要发现&lt;/h4&gt;在包含8,284张图像的表面缺陷数据集上，TransMatch实现了98.91%的准确率，最小损失，同时针对多种缺陷类别获得了高精度、召回率和F1分数&lt;h4&gt;结论&lt;/h4&gt;TransMatch在准确识别裂纹、针孔、孔洞和飞溅等多种缺陷方面表现出强大的鲁棒性，代表了增材制造缺陷检测的重要进步，为广泛工业应用中的质量保证和可靠性提供了实用且可扩展的解决方案&lt;h4&gt;翻译&lt;/h4&gt;激光粉末床熔融(LPBF)中的表面缺陷会对增材制造组件的结构完整性构成重大风险。本文引入TransMatch，一种新颖的框架，它结合迁移学习和半监督少样本学习来解决增材制造缺陷标记数据稀缺的问题。通过有效利用标记和新类别图像，TransMatch克服了先前元学习方法的局限性。在包含8,284张图像的表面缺陷数据集上的实验评估证明了TransMatch的有效性，实现了98.91%的准确率和最小损失，同时针对多种缺陷类别获得了高精度、召回率和F1分数。这些发现强调了其在准确识别裂纹、针孔、孔洞和飞溅等多样缺陷方面的鲁棒性。因此，TransMatch代表了增材制造缺陷检测的重要进步，为广泛工业应用中的质量保证和可靠性提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surface defects in Laser Powder Bed Fusion (LPBF) pose significant risks tothe structural integrity of additively manufactured components. This paperintroduces TransMatch, a novel framework that merges transfer learning andsemi-supervised few-shot learning to address the scarcity of labeled AM defectdata. By effectively leveraging both labeled and unlabeled novel-class images,TransMatch circumvents the limitations of previous meta-learning approaches.Experimental evaluations on a Surface Defects dataset of 8,284 imagesdemonstrate the efficacy of TransMatch, achieving 98.91% accuracy with minimalloss, alongside high precision, recall, and F1-scores for multiple defectclasses. These findings underscore its robustness in accurately identifyingdiverse defects, such as cracks, pinholes, holes, and spatter. TransMatch thusrepresents a significant leap forward in additive manufacturing defectdetection, offering a practical and scalable solution for quality assurance andreliability across a wide range of industrial applications.</description>
      <author>example@mail.com (Mohsen Asghari Ilani, Yaser Mike Banad)</author>
      <guid isPermaLink="false">2509.01754v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>CSRM-LLM: Embracing Multilingual LLMs for Cold-Start Relevance Matching in Emerging E-commerce Markets</title>
      <link>http://arxiv.org/abs/2509.01566v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种冷启动相关性匹配(CSRM)框架，利用多语言大型语言模型解决电子商务平台在新兴市场面临的冷启动挑战，通过激活跨语言迁移学习能力、基于检索的查询增强和多轮自蒸馏训练策略，实现了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;全球电子商务平台不断扩张，公司进入新市场时面临冷启动挑战，原因是有限的标签和用户行为数据。&lt;h4&gt;目的&lt;/h4&gt;分享Coupang公司的经验，为新兴电子商务市场提供具有竞争力的冷启动相关性匹配性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个冷启动相关性匹配(CSRM)框架，利用多语言大型语言模型(LLM)解决三个挑战：(1)通过机器翻译任务激活LLM的跨语言迁移学习能力；(2)通过基于检索的查询增强来提高查询理解并整合电子商务知识；(3)通过多轮自蒸馏训练策略减轻训练标签错误的影响。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明CSRM-LLM和所提出技术的有效性，成功实现实际部署并获得显著的在线收益，缺陷比率降低了45.8%，会话购买率提升了0.866%。&lt;h4&gt;结论&lt;/h4&gt;CSRM框架能够有效解决电子商务平台在新兴市场面临的冷启动挑战。&lt;h4&gt;翻译&lt;/h4&gt;随着全球电子商务平台的持续扩张，公司正在进入新市场，由于有限的标签和用户行为而面临冷启动挑战。在本文中，我们分享了在Coupang的经验，为新兴电子商务市场提供具有竞争力的冷启动相关性匹配性能。具体来说，我们提出了一个冷启动相关性匹配(CSRM)框架，利用多语言大型语言模型(LLM)解决三个挑战：(1)通过机器翻译任务激活LLM的跨语言迁移学习能力；(2)通过基于检索的查询增强提高查询理解并整合电子商务知识；(3)通过多轮自蒸馏训练策略减轻训练标签错误的影响。我们的实验证明了CSRM-LLM和所提出技术的有效性，成功实现了实际部署并获得了显著的在线收益，缺陷比率降低了45.8%，会话购买率提升了0.866%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761545&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As global e-commerce platforms continue to expand, companies are entering newmarkets where they encounter cold-start challenges due to limited human labelsand user behaviors. In this paper, we share our experiences in Coupang toprovide a competitive cold-start performance of relevance matching for emerginge-commerce markets. Specifically, we present a Cold-Start Relevance Matching(CSRM) framework, utilizing a multilingual Large Language Model (LLM) toaddress three challenges: (1) activating cross-lingual transfer learningabilities of LLMs through machine translation tasks; (2) enhancing queryunderstanding and incorporating e-commerce knowledge by retrieval-based queryaugmentation; (3) mitigating the impact of training label errors through amulti-round self-distillation training strategy. Our experiments demonstratethe effectiveness of CSRM-LLM and the proposed techniques, resulting insuccessful real-world deployment and significant online gains, with a 45.8%reduction in defect ratio and a 0.866% uplift in session purchase rate.</description>
      <author>example@mail.com (Yujing Wang, Yiren Chen, Huoran Li, Chunxu Xu, Yuchong Luo, Xianghui Mao, Cong Li, Lun Du, Chunyang Ma, Qiqi Jiang, Yin Wang, Fan Gao, Wenting Mo, Pei Wen, Shantanu Kumar, Taejin Park, Yiwei Song, Vijay Rajaram, Tao Cheng, Sonu Durgia, Pranam Kolari)</author>
      <guid isPermaLink="false">2509.01566v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Characterization of Speech Similarity Between Australian Aboriginal and High-Resource Languages: A Case Study on Dharawal</title>
      <link>http://arxiv.org/abs/2509.01419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at APSIPA ASC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对澳大利亚原住民语言在语音AI系统中代表性不足的问题，通过收集整理Dharawal语言的语音数据集，并使用预训练多语言语音编码器分析其与107种高资源语言的语音相似性。研究发现Dharawal与多种语言有很强的语音相似性，为迁移学习和模型适应提供了指导。&lt;h4&gt;背景&lt;/h4&gt;澳大利亚原住民语言具有重要的文化和语言学价值，但在现代语音AI系统中严重缺乏代表性。最先进的语音基础模型和自动语音识别系统在高资源语言环境中表现出色，但在低资源语言（特别是缺乏干净、标注语音数据的语言）上泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;收集并整理Dharawal（一种低资源澳大利亚原住民语言）的语音数据集，并分析Dharawal与107种高资源语言之间的语音相似性。&lt;h4&gt;方法&lt;/h4&gt;仔细收集和处理公开可用的录音构建Dharawal语音数据集，使用预训练的多语言语音编码器，结合错误分类率分析评估语言混淆性，以及使用余弦相似度和Fréchet初始距离在嵌入空间中进行细粒度相似度测量。&lt;h4&gt;主要发现&lt;/h4&gt;Dharawal与拉丁语、毛利语、韩语、泰语和威尔士语等语言有很强的语音相似性。&lt;h4&gt;结论&lt;/h4&gt;这些发现为未来的迁移学习和模型适应工作提供了实用指导，强调了数据收集和基于嵌入的分析在支持濒危语言社区语音技术方面的重要性。&lt;h4&gt;翻译&lt;/h4&gt;澳大利亚原住民语言具有重要的文化和语言学价值，但在现代语音AI系统中仍然严重缺乏代表性。虽然最先进的语音基础模型和自动语音识别在高资源环境中表现出色，但它们往往难以泛化到低资源语言，特别是那些缺乏干净、标注语音数据的语言。在这项工作中，我们通过仔细收集和处理公开可用的录音，为Dharawal（一种低资源澳大利亚原住民语言）收集并整理了一个语音数据集。使用这个数据集，我们使用预训练的多语言语音编码器分析了Dharawal和107种高资源语言之间的语音相似性。我们的方法结合了（1）错误分类率分析以评估语言混淆性，以及（2）在嵌入空间中使用余弦相似度和Fréchet初始距离进行细粒度相似度测量。实验结果显示，Dharawal与拉丁语、毛利语、韩语、泰语和威尔士语等语言有很强的语音相似性。这些发现为未来的迁移学习和模型适应工作提供了实用指导，并强调了数据收集和基于嵌入的分析在支持濒危语言社区语音技术方面的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Australian Aboriginal languages are of significant cultural and linguisticvalue but remain severely underrepresented in modern speech AI systems. Whilestate-of-the-art speech foundation models and automatic speech recognitionexcel in high-resource settings, they often struggle to generalize tolow-resource languages, especially those lacking clean, annotated speech data.In this work, we collect and clean a speech dataset for Dharawal, alow-resource Australian Aboriginal language, by carefully sourcing andprocessing publicly available recordings. Using this dataset, we analyze thespeech similarity between Dharawal and 107 high-resource languages using apre-trained multilingual speech encoder. Our approach combines (1)misclassification rate analysis to assess language confusability, and (2)fine-grained similarity measurements using cosine similarity and Fr\'echetInception Distance (FID) in the embedding space. Experimental results revealthat Dharawal shares strong speech similarity with languages such as Latin,M\=aori, Korean, Thai, and Welsh. These findings offer practical guidance forfuture transfer learning and model adaptation efforts, and underscore theimportance of data collection and embedding-based analysis in supporting speechtechnologies for endangered language communities.</description>
      <author>example@mail.com (Ting Dang, Trini Manoj Jeyaseelan, Eliathamby Ambikairajah, Vidhyasaharan Sethu)</author>
      <guid isPermaLink="false">2509.01419v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Comparison between Supervised and Unsupervised Learning in Deep Unfolded Sparse Signal Recovery</title>
      <link>http://arxiv.org/abs/2509.01331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work will be submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了深度展开技术中损失函数选择对稀疏信号恢复算法的影响，发现损失函数的效果取决于优化问题的凸性。&lt;h4&gt;背景&lt;/h4&gt;深度展开技术通过将迭代优化算法的迭代展开为网络层，将其转换为可训练的轻量级神经网络，并根据应用场景使用不同的损失函数进行参数学习。&lt;h4&gt;目的&lt;/h4&gt;研究在深度展开的ISTA和IHT算法中，不同损失函数选择对算法性能的影响，为设计有效的深度展开网络提供指导。&lt;h4&gt;方法&lt;/h4&gt;比较了基本迭代软阈值算法(ISTA)和迭代硬阈值算法(IHT)的深度展开版本，分别使用均方误差的监督学习和使用原始优化问题目标函数的无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;对于凸的ℓ₁正则化问题，监督式ISTA恢复精度更高但无法最小化原始目标函数，无监督式ISTA收敛速度更快；对于非凸的ℓ₀正则化问题，监督式和无监督式IHT都收敛到更好的局部最小值，性能相似。&lt;h4&gt;结论&lt;/h4&gt;损失函数的选择效果显著取决于优化问题的凸性，这一发现为稀疏信号恢复应用中设计有效的深度展开网络提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了深度展开技术中损失函数选择对稀疏信号恢复算法的影响。深度展开通过将迭代优化算法的迭代展开为网络层，将其转换为可训练的轻量级神经网络，并根据应用场景使用不同的损失函数进行参数学习。我们专注于基本迭代软阈值算法(ISTA)和迭代硬阈值算法(IHT)的深度展开版本，比较了使用均方误差的监督学习与使用原始优化问题目标函数的无监督学习。我们的模拟结果表明，损失函数的选择效果显著取决于优化问题的凸性。对于凸的ℓ₁正则化问题，监督式ISTA实现了更好的最终恢复精度，但未能最小化原始目标函数；而无监督式ISTA收敛到与常规ISTA几乎相同的解，但收敛速度更快。相反，对于非凸的ℓ₀正则化问题，监督式IHT和无监督式IHT都收敛到比原始IHT更好的局部最小值，无论使用哪种损失函数，性能都相似。这些发现为稀疏信号恢复应用中设计有效的深度展开网络提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates the impact of loss function selection in deepunfolding techniques for sparse signal recovery algorithms. Deep unfoldingtransforms iterative optimization algorithms into trainable lightweight neuralnetworks by unfolding their iterations as network layers, with various lossfunctions employed for parameter learning depending on application contexts. Wefocus on deep unfolded versions of the fundamental iterative shrinkagethresholding algorithm (ISTA) and the iterative hard thresholding algorithm(IHT), comparing supervised learning using mean squared error with unsupervisedlearning using the objective function of the original optimization problem. Oursimulation results reveal that the effect of the choice of loss functionsignificantly depends on the convexity of the optimization problem. For convex$\ell_1$-regularized problems, supervised-ISTA achieves better final recoveryaccuracy but fails to minimize the original objective function, whereas weempirically observe that unsupervised-ISTA converges to a nearly identicalsolution as conventional ISTA but with accelerated convergence. Conversely, fornonconvex $\ell_0$-regularized problems, both supervised-IHT andunsupervised-IHT converge to better local minima than the original IHT, showingsimilar performance regardless of the loss function employed. These findingsprovide valuable insights into the design of effective deep unfolded networksfor sparse signal recovery applications.</description>
      <author>example@mail.com (Koshi Nagahisa, Ryo Hayakawa, Youji Iiguni)</author>
      <guid isPermaLink="false">2509.01331v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>DcMatch: Unsupervised Multi-Shape Matching with Dual-Level Consistency</title>
      <link>http://arxiv.org/abs/2509.01204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DcMatch，一种新颖的无监督学习框架，用于非刚性多形状匹配。该方法利用形状图注意力网络捕获形状集合的底层流形结构，构建共享潜在空间，并通过双层次一致性实现更准确和连贯的形状对应关系映射。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉和图形学领域，建立多个3D形状之间的点对点对应关系是一个基本问题。现有方法通常从单个形状学习规范嵌入，存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获整个形状集合底层结构的多形状匹配方法，实现更准确、一致和鲁棒的非刚性形状对应关系。&lt;h4&gt;方法&lt;/h4&gt;DcMatch采用形状图注意力网络捕获形状集合的流形结构，构建共享潜在空间，通过universe predictor实现形状到宇宙的对应关系，同时在空间和频谱域表示对应关系，并通过循环一致性损失强制对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在多个具有挑战性的基准测试上，DcMatch在不同多形状匹配场景中始终优于之前的最先进方法，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;DcMatch通过创新的双层次一致性机制和形状图注意力网络，显著提高了多形状匹配的准确性和鲁棒性，为计算机视觉和图形学中的形状对应问题提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在计算机视觉和图形学中，建立多个3D形状之间的点对点对应关系是一个基本问题。在本文中，我们介绍了DcMatch，一种新颖的无监督学习框架，用于非刚性多形状匹配。与从单个形状学习规范嵌入的现有方法不同，我们的方法利用形状图注意力网络来捕获整个形状集合的底层流形结构。这使得能够构建更具表现力和鲁棒性的共享潜在空间，通过宇宙预测器实现更一致的形状到宇宙的对应关系。同时，我们在空间和频谱域表示这些对应关系，并通过新颖的循环一致性损失在共享宇宙空间中强制它们的对齐。这种双层次一致性促进了更准确和连贯的映射。在几个具有挑战性的基准测试上的广泛实验表明，我们的方法在不同多形状匹配场景中始终优于之前的最先进方法。代码可在https://github.com/YeTianwei/DcMatch获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是多形状匹配问题，即在多个3D形状之间建立准确且一致性的点对点对应关系。这个问题在计算机视觉和图形学中非常重要，因为它支持多种应用，如纹理转移、统计形状分析（医学成像中）和3D重建。随着3D扫描技术的发展，同时捕获多个形状变得越来越常见，但现有的成对形状匹配方法难以扩展到多形状场景，因为多形状匹配需要整个形状集合之间的循环一致性，这大大增加了问题的复杂性，同时随着形状集合大小的增加，计算开销也呈组合式增长。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了多形状匹配的挑战，特别是循环一致性问题。他们指出现有方法主要有两种范式：排列同步范式和基于宇宙的范式，但各有局限性。作者受到排列同步方法中成对匹配准确性的启发，同时希望改进基于宇宙的方法，使其能够捕获整个形状集合的底层流形结构。该方法借鉴了功能映射（Functional Maps）来表示形状对应关系，使用图注意力网络（GAT）捕获形状集合的结构关系，并采用宇宙预测器和循环一致性损失来确保匹配的一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; DcMatch的核心思想是通过双重级别的一致性（空间和频谱）和形状图注意力网络来捕获整个形状集合的底层流形结构，构建更丰富、鲁棒的共享宇宙空间。整体实现流程包括：1) 使用DiffusionNet提取每个形状的顶点特征；2) 功能映射模块计算形状对之间的双向功能映射并转换为点对点对应关系；3) 形状图注意力模块构建形状图并使用GAT提取流形感知特征；4) 宇宙预测器模块预测每个形状与共享宇宙之间的对应关系；5) 使用频谱损失和循环一致性损失进行训练，确保空间和频谱域对应关系的一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1) 双重级别的一致性约束，同时 enforcing 空间（点对点）和频谱（功能映射）两个级别的一致性；2) 形状图注意力模块，将整个形状集合建模为无向图并捕获底层流形结构；3) 增强的宇宙空间，通过形状图注意力网络构建更丰富、鲁棒的共享潜在空间；4) 完全无监督的学习框架。相比之前的工作，DcMatch不依赖于单个参考形状来学习宇宙嵌入，而是考虑整个形状集合的流形结构，同时利用功能映射和形状到宇宙匹配的双重一致性，提高了匹配的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DcMatch通过引入形状图注意力网络和双重级别的一致性约束，在无监督设置下实现了更准确、鲁棒和连贯的多形状匹配，同时捕获了整个形状集合的底层流形结构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing point-to-point correspondences across multiple 3D shapes is afundamental problem in computer vision and graphics. In this paper, weintroduce DcMatch, a novel unsupervised learning framework for non-rigidmulti-shape matching. Unlike existing methods that learn a canonical embeddingfrom a single shape, our approach leverages a shape graph attention network tocapture the underlying manifold structure of the entire shape collection. Thisenables the construction of a more expressive and robust shared latent space,leading to more consistent shape-to-universe correspondences via a universepredictor. Simultaneously, we represent these correspondences in both thespatial and spectral domains and enforce their alignment in the shared universespace through a novel cycle consistency loss. This dual-level consistencyfosters more accurate and coherent mappings. Extensive experiments on severalchallenging benchmarks demonstrate that our method consistently outperformsprevious state-of-the-art approaches across diverse multi-shape matchingscenarios. Code is available at https://github.com/YeTianwei/DcMatch.</description>
      <author>example@mail.com (Tianwei Ye, Yong Ma, Xiaoguang Mei)</author>
      <guid isPermaLink="false">2509.01204v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Preserving Vector Space Properties in Dimensionality Reduction: A Relationship Preserving Loss Framework</title>
      <link>http://arxiv.org/abs/2509.01198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种关系保持损失函数(RPL)，用于在降维过程中保留向量空间的关键属性，如正交性和线性独立性。&lt;h4&gt;背景&lt;/h4&gt;降维可能会扭曲向量空间属性如正交性和线性独立性，而这些属性对跨模态检索、聚类和分类等任务至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种损失函数来保留高维数据和低维嵌入之间的关系矩阵，从而维护关键的向量空间属性。&lt;h4&gt;方法&lt;/h4&gt;RPL通过最小化高维数据和其低维嵌入之间的关系矩阵（如Gram矩阵或余弦相似度）之间的差异来工作，用于训练神经网络的非线性投影，并由矩阵扰动理论推导出的误差边界支持。&lt;h4&gt;主要发现&lt;/h4&gt;初步实验表明，RPL可以在降低嵌入维度的同时，在很大程度上保持下游任务的性能，这可能是由于其保留了关键的向量空间属性。&lt;h4&gt;结论&lt;/h4&gt;RPL不仅可以用于降维，还可以更广泛地应用于跨领域对齐和迁移学习、知识蒸馏、公平性和不变性、去中心化、图和流形学习以及联邦学习等领域。&lt;h4&gt;翻译&lt;/h4&gt;降维可能会扭曲向量空间属性，如正交性和线性独立性，这些属性对于跨模态检索、聚类和分类等任务至关重要。我们提出了一种关系保持损失(RPL)，这是一种通过最小化高维数据及其低维嵌入之间的关系矩阵（如Gram矩阵或余弦矩阵）之间的差异来保留这些属性的损失函数。RPL训练神经网络的非线性投影，并由矩阵扰动理论推导出的误差边界支持。初步实验表明，RPL在降低嵌入维度的同时，很大程度上保留了下游任务的性能，这可能是由于其保留了关键的向量空间属性。虽然我们在本文中描述了RPL在降维中的应用，但这种损失也可以更广泛地应用，例如在跨领域对齐和迁移学习、知识蒸馏、公平性和不变性、去中心化、图和流形学习以及联邦学习中，其中分布式嵌入必须保持几何一致性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dimensionality reduction can distort vector space properties such asorthogonality and linear independence, which are critical for tasks includingcross-modal retrieval, clustering, and classification. We propose aRelationship Preserving Loss (RPL), a loss function that preserves theseproperties by minimizing discrepancies between relationship matrices (e.g.,Gram or cosine) of high-dimensional data and their low-dimensional embeddings.RPL trains neural networks for non-linear projections and is supported by errorbounds derived from matrix perturbation theory. Initial experiments suggestthat RPL reduces embedding dimensions while largely retaining performance ondownstream tasks, likely due to its preservation of key vector spaceproperties. While we describe here the use of RPL in dimensionality reduction,this loss can also be applied more broadly, for example to cross-domainalignment and transfer learning, knowledge distillation, fairness andinvariance, dehubbing, graph and manifold learning, and federated learning,where distributed embeddings must remain geometrically consistent.</description>
      <author>example@mail.com (Eddi Weinwurm, Alexander Kovalenko)</author>
      <guid isPermaLink="false">2509.01198v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MATL-DC: A Multi-domain Aggregation Transfer Learning Framework for EEG Emotion Recognition with Domain-Class Prototype under Unseen Targets</title>
      <link>http://arxiv.org/abs/2509.01135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于脑电图信号的情感识别方法，通过多域聚合迁移学习框架(MATL-DC)解决了传统迁移学习模型依赖源域和目标域数据的问题，实现了在没有目标域数据参与训练的情况下进行情感识别。&lt;h4&gt;背景&lt;/h4&gt;基于脑电图(EEG)的情感识别是情感脑机接口(aBCIs)的关键研究热点，但当前迁移学习模型严重依赖源域和目标域数据，限制了情感识别的实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个针对EEG情感识别的多域聚合迁移学习框架，带有未见目标下的领域-类别原型(MATL-DC)，以减少对目标域数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;设计特征解耦模块解耦类别不变的领域特征和领域不变的类别特征；通过多域聚合机制将领域特征空间聚合形成超域增强情感EEG信号特征；提取类别原型表示；采用成对学习策略将分类问题转化为样本对相似性问题；训练过程中目标域完全不可见；推理阶段使用训练好的领域-类别原型进行情感识别。&lt;h4&gt;主要发现&lt;/h4&gt;在SEED、SEED-IV和SEED-V公开数据库上验证，MATL-DC模型准确率分别达到84.70%、68.11%和61.08%，性能与依赖源域和目标域的方法相当或更好。&lt;h4&gt;结论&lt;/h4&gt;MATL-DC模型在EEG情感识别任务中表现良好，不需要目标域数据参与训练，具有实际应用价值。源代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;基于脑电图(EEG)信号的情感识别正逐渐成为情感脑机接口(aBCIs)的关键研究热点。然而，当前的迁移学习模型严重依赖源域和目标域数据，这阻碍了情感识别的实际应用。因此，我们提出了一种针对EEG情感识别的多域聚合迁移学习框架，带有未见目标下的领域-类别原型(MATL-DC)。我们设计了特征解耦模块，从浅层特征中解耦出类别不变的领域特征和领域不变的类别特征。在模型训练阶段，多域聚合机制将领域特征空间聚合形成超域，增强情感EEG信号的特征。在每个超域中，我们通过类别特征进一步提取类别原型表示。此外，我们采用成对学习策略将样本分类问题转化为样本对之间的相似性问题，有效缓解了标签噪声的影响。值得注意的是，在训练过程中目标域完全不可见。在推理阶段，我们使用训练好的领域-类别原型进行推理，实现情感识别。我们在公开数据库(SEED、SEED-IV和SEED-V)上对其进行了严格验证。结果表明，MATL-DC模型的准确率分别为84.70%、68.11%和61.08%。MATL-DC实现了与依赖源域和目标域的方法相当甚至更好的性能。源代码可在https://github.com/WuCB-BCI/MATL-DC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Emotion recognition based on electroencephalography (EEG) signals isincreasingly becoming a key research hotspot in affective Brain-ComputerInterfaces (aBCIs). However, the current transfer learning model greatlydepends on the source domain and target domain data, which hinder the practicalapplication of emotion recognition. Therefore, we propose a Multi-domainAggregation Transfer Learning framework for EEG emotion recognition withDomain-Class prototype under unseen targets (MATL-DC). We design the featuredecoupling module to decouple class-invariant domain features fromdomain-invariant class features from shallow features. In the model trainingstage, the multi-domain aggregation mechanism aggregates the domain featurespace to form a superdomain, which enhances the characteristics of emotionalEEG signals. In each superdomain, we further extract the class prototyperepresentation by class features. In addition, we adopt the pairwise learningstrategy to transform the sample classification problem into the similarityproblem between sample pairs, which effectively alleviates the influence oflabel noise. It is worth noting that the target domain is completely unseenduring the training process. In the inference stage, we use the traineddomain-class prototypes for inference, and then realize emotion recognition. Werigorously validate it on the publicly available databases (SEED, SEED-IV andSEED-V). The results show that the accuracy of MATL-DC model is 84.70\%,68.11\% and 61.08\%, respectively. MATL-DC achieves comparable or even betterperformance than methods that rely on both source and target domains. Thesource code is available at https://github.com/WuCB-BCI/MATL-DC.</description>
      <author>example@mail.com (Guangli Li, Canbiao Wu, Zhehao Zhou, Na Tian, Zhen Liang)</author>
      <guid isPermaLink="false">2509.01135v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>GraViT: Transfer Learning with Vision Transformers and MLP-Mixer for Strong Gravitational Lens Discovery</title>
      <link>http://arxiv.org/abs/2509.00226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Our publicly available fine-tuned models provide a scalable transfer  learning solution for gravitational lens finding in LSST. Submitted to MNRAS.  Comments welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了GraViT，一个基于PyTorch的引力透镜检测流水线，利用了Vision Transformer和MLP-Mixer的预训练模型。研究评估了迁移学习对分类性能的影响，包括数据质量、模型架构、训练策略和集成预测等方面。&lt;h4&gt;背景&lt;/h4&gt;引力透镜是研究暗物质性质和推断宇宙学参数的有力工具。LSST（空间和时间遗产调查）预计在未来十年内会发现约10^5个引力透镜，需要开发自动分类器。&lt;h4&gt;目的&lt;/h4&gt;介绍GraViT流水线，评估迁移学习对引力透镜检测分类性能的影响，并提供对强引力透镜可检测性的见解。&lt;h4&gt;方法&lt;/h4&gt;通过研究数据质量（源和样本大小）、模型架构（选择和微调）、训练策略（增强、归一化和优化）和集成预测来评估迁移学习的影响。使用HOLISMOKES VI和SuGOHI X的数据集对十种架构进行微调，并与卷积基线进行比较，分析复杂性和推理时间。&lt;h4&gt;主要发现&lt;/h4&gt;研究重现了先前神经网络系统性比较中的实验，并对通用测试样本上强引力透镜的可检测性提供了见解。不同架构在性能和效率上表现出差异。&lt;h4&gt;结论&lt;/h4&gt;GraViT是一个有效的引力透镜检测工具，系统评估了模型架构和训练策略对性能的影响，为大规模引力透镜检测提供了方法。&lt;h4&gt;翻译&lt;/h4&gt;引力透镜为研究暗物质特性提供了强大的探测手段，对于推断宇宙学参数至关重要。空间和时间遗产调查（LSST）预计在未来十年内将发现约10^5个引力透镜，这需要自动分类器的支持。在本研究中，我们介绍了GraViT，一个用于引力透镜检测的PyTorch流水线，它利用了最先进的Vision Transformer（ViT）模型和MLP-Mixer的广泛预训练。我们通过研究数据质量（源和样本大小）、模型架构（选择和微调）、训练策略（增强、归一化和优化）以及集成预测来评估迁移学习对分类性能的影响。该研究重现了先前神经网络系统性比较中的实验，并对那个通用测试样本上强引力透镜的可检测性提供了见解。我们使用HOLISMOKES VI和SuGOHI X的数据集对十种架构进行微调，并与卷积基线进行比较，讨论了复杂性和推理时间分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gravitational lensing offers a powerful probe into the properties of darkmatter and is crucial to infer cosmological parameters. The Legacy Survey ofSpace and Time (LSST) is predicted to find O(10^5) gravitational lenses overthe next decade, demanding automated classifiers. In this work, we introduceGraViT, a PyTorch pipeline for gravitational lens detection that leveragesextensive pretraining of state-of-the-art Vision Transformer (ViT) models andMLP-Mixer. We assess the impact of transfer learning on classificationperformance by examining data quality (source and sample size), modelarchitecture (selection and fine-tuning), training strategies (augmentation,normalization, and optimization), and ensemble predictions. This studyreproduces the experiments in a previous systematic comparison of neuralnetworks and provides insights into the detectability of strong gravitationallenses on that common test sample. We fine-tune ten architectures usingdatasets from HOLISMOKES VI and SuGOHI X, and benchmark them againstconvolutional baselines, discussing complexity and inference-time analysis.</description>
      <author>example@mail.com (René Parlange, Juan C. Cuevas-Tello, Octavio Valenzuela, Omar de J. Cabrera-Rosas, Tomás Verdugo, Anupreeta More, Anton T. Jaelani)</author>
      <guid isPermaLink="false">2509.00226v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Amplifying Emotional Signals: Data-Efficient Deep Learning for Robust Speech Emotion Recognition</title>
      <link>http://arxiv.org/abs/2509.00077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过迁移学习和数据增强技术解决了有限数据集上的语音情感识别挑战，开发了多种机器学习模型，并在结合RAVDESS和SAVEE数据集上取得了显著成果。&lt;h4&gt;背景&lt;/h4&gt;语音情感识别是人机交互中的一个重要且持续的挑战，尽管深度学习已推进口语语言处理，但在有限数据集上实现高性能仍然是一个关键障碍。&lt;h4&gt;目的&lt;/h4&gt;开发和评估一系列机器学习模型，用于人类语音中的自动情感分类，解决有限数据集上的高性能问题。&lt;h4&gt;方法&lt;/h4&gt;开发了支持向量机、长短期记忆网络和卷积神经网络等多种机器学习模型，采用迁移学习策略和创新的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;通过策略性地应用迁移学习和数据增强技术，模型可以在相对较小的数据集限制下实现令人印象深刻的性能；最有效的ResNet34架构在结合RAVDESS和SAVEE数据集上达到66.7%的准确率和0.631的F1分数。&lt;h4&gt;结论&lt;/h4&gt;利用预训练模型和数据增强克服数据稀缺性具有显著优势，为更强大和可推广的语音情感识别系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;语音情感识别在人机交互中呈现一个重要且持续的挑战。虽然深度学习已推进口语语言处理，但在有限数据集上实现高性能仍然是一个关键障碍。本文通过开发和评估一系列机器学习模型（包括支持向量机、长短期记忆网络和卷积神经网络）来应对这个问题，用于人类语音中的自动情感分类。我们证明，通过策略性地采用迁移学习和创新的数据增强技术，我们的模型可以在相对较小的数据集限制下实现令人印象深刻的性能。我们最有效的模型，即ResNet34架构，在结合RAVDESS和SAVEE数据集上建立了新的性能基准，达到66.7%的准确率和0.631的F1分数。这些结果强调了利用预训练模型和数据增强克服数据稀缺性的显著好处，从而为更强大和可推广的语音情感识别系统铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech Emotion Recognition (SER) presents a significant yet persistentchallenge in human-computer interaction. While deep learning has advancedspoken language processing, achieving high performance on limited datasetsremains a critical hurdle. This paper confronts this issue by developing andevaluating a suite of machine learning models, including Support VectorMachines (SVMs), Long Short-Term Memory networks (LSTMs), and ConvolutionalNeural Networks (CNNs), for automated emotion classification in human speech.We demonstrate that by strategically employing transfer learning and innovativedata augmentation techniques, our models can achieve impressive performancedespite the constraints of a relatively small dataset. Our most effectivemodel, a ResNet34 architecture, establishes a new performance benchmark on thecombined RAVDESS and SAVEE datasets, attaining an accuracy of 66.7% and an F1score of 0.631. These results underscore the substantial benefits of leveragingpre-trained models and data augmentation to overcome data scarcity, therebypaving the way for more robust and generalizable SER systems.</description>
      <author>example@mail.com (Tai Vu)</author>
      <guid isPermaLink="false">2509.00077v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Safety-Critical Multi-Agent MCTS for Mixed Traffic Coordination at Unsignalized Roundabout</title>
      <link>http://arxiv.org/abs/2509.01856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种针对自动驾驶车辆在无信号环岛环境中进行安全决策的多智能体蒙特卡洛树搜索框架，特别关注混合交通环境中的协调决策问题。&lt;h4&gt;背景&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在混合交通环境中，自动驾驶车辆必须与人类驾驶车辆安全协调。&lt;h4&gt;目的&lt;/h4&gt;开发一个安全关键的多智能体蒙特卡洛树搜索框架，整合确定性和概率预测模型，促进复杂环岛场景中的协作决策。&lt;h4&gt;方法&lt;/h4&gt;提出框架包含三个关键创新：1）分层安全评估模块，通过动态安全阈值和时空风险评估系统处理车辆间交互；2）自适应人类驾驶车辆行为预测方案，结合智能驾驶模型与概率不确定性建模；3）多目标奖励优化策略，联合考虑安全性、效率和协作意图。&lt;h4&gt;主要发现&lt;/h4&gt;大量仿真结果验证了所提出方法在完全自动驾驶和混合交通条件下的有效性。与基准方法相比，该框架一致降低了所有自动驾驶车辆的轨迹偏差，并显著降低了侵占后时间违规率，在完全自动驾驶场景中仅达到1.0%，在混合交通环境中为3.2%。&lt;h4&gt;结论&lt;/h4&gt;所提出的多智能体蒙特卡洛树搜索框架能够有效处理自动驾驶车辆在无信号环岛中的复杂决策问题，特别是在混合交通环境中，实现了安全高效的协调决策。&lt;h4&gt;翻译&lt;/h4&gt;无信号环岛的决策对自动驾驶车辆构成重大挑战，特别是在自动驾驶车辆必须与人类驾驶车辆安全协调的混合交通环境中。本文提出了一个安全关键的多智能体蒙特卡洛树搜索框架，整合了确定性和概率预测模型，以促进复杂环岛场景中的协作决策。所提出的框架引入了三个关键创新：（1）分层安全评估模块，通过动态安全阈值和时空风险评估系统性地处理自动驾驶车辆间、自动驾驶车辆与人类驾驶车辆以及自动驾驶车辆与道路的交互；（2）自适应人类驾驶车辆行为预测方案，结合智能驾驶模型与概率不确定性建模；（3）多目标奖励优化策略，联合考虑安全性、效率和协作意图。大量仿真结果验证了所提出方法在完全自动驾驶和混合交通条件下的有效性。与基准方法相比，我们的框架一致降低了所有自动驾驶车辆的轨迹偏差，并显著降低了侵占后时间违规率，在完全自动驾驶场景中仅达到1.0%，在混合交通环境中为3.2%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decision-making at unsignalized roundabouts poses substantial challenges forautonomous vehicles (AVs), particularly in mixed traffic environments where AVsmust coordinate safely with human-driven vehicles (HDVs). This paper presents asafety-critical multi-agent Monte Carlo Tree Search (MCTS) framework thatintegrates both deterministic and probabilistic prediction models to facilitatecooperative decision-making in complex roundabout scenarios. The proposedframework introduces three key innovations: (1) a hierarchical safetyassessment module that systematically addresses AV-to-AV (A2A), AV-to-HDV(A2H), and AV-to-Road (A2R) interactions through dynamic safety thresholds andspatiotemporal risk evaluation; (2) an adaptive HDV behavior prediction schemethat combines the Intelligent Driver Model (IDM) with probabilistic uncertaintymodeling; and (3) a multi-objective reward optimization strategy that jointlyconsiders safety, efficiency, and cooperative intent. Extensive simulationresults validate the effectiveness of the proposed approach under both fullyautonomous (100% AVs) and mixed traffic (50% AVs + 50% HDVs) conditions.Compared to benchmark methods, our framework consistently reduces trajectorydeviations across all AVs and significantly lowers the rate ofPost-Encroachment Time (PET) violations, achieving only 1.0\% in the fullyautonomous scenario and 3.2% in the mixed traffic setting.</description>
      <author>example@mail.com (Zhihao Lin, Shuo Liu, Zhen Tian, Dezong Zhao, Jianglin Lan)</author>
      <guid isPermaLink="false">2509.01856v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled Multi-Context Meta-Learning: Unlocking robust and Generalized Task Learning</title>
      <link>http://arxiv.org/abs/2509.01297v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The Conference on Robot Learning (CoRL) 2025 Project  Page: seonsoo-p1.github.io/DMCM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种解纠缠的多上下文元学习框架，通过将任务因素分配给不同的上下文向量，提高了模型的鲁棒性和泛化能力。在正弦回归和四足机器人运动任务中，该方法在分布外条件下表现优于基线，并能够实现模拟到现实的策略转移，仅需少量真实数据。&lt;h4&gt;背景&lt;/h4&gt;在元学习及其下游任务中，许多方法依赖于对任务变化的隐式适应，多个因素在单一纠缠表示中混合在一起。这使得难以解释哪些因素驱动性能，并可能阻碍泛化能力。&lt;h4&gt;目的&lt;/h4&gt;引入一个解纠缠的多上下文元学习框架，明确将每个任务因素分配给不同的上下文向量，以提高对任务的深入理解和增强泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过解耦任务变化，提高对任务的深入理解，并通过在具有共享因素的任务间共享上下文向量来增强泛化能力。在正弦回归任务和四足机器人运动任务两个领域评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;在正弦回归任务中，模型在分布外任务上优于基线，并通过共享与共享振幅或相位偏移相关的上下文向量推广到未见过的正弦函数。在四足机器人运动任务中，解耦了机器人特定属性和地形特征，通过将解耦的上下文向量从动力学模型转移到强化学习，得到的策略在分布外条件下提高了鲁棒性。通过有效共享上下文，模型仅使用20秒的平坦地形真实数据，成功实现了具有分布外机器人特定属性的有挑战性地形的模拟到现实策略转移。&lt;h4&gt;结论&lt;/h4&gt;解纠缠的多上下文元学习框架能够提高对任务的理解和泛化能力。通过解耦任务因素和共享上下文，模型在分布外条件下表现更好，并实现了从模拟到现实的策略转移，这是单任务适应无法实现的。&lt;h4&gt;翻译&lt;/h4&gt;在元学习及其下游任务中，许多方法依赖于对任务变化的隐式适应，其中多个因素在单一纠缠表示中混合在一起。这使得难以解释哪些因素驱动性能，并可能阻碍泛化能力。在这项工作中，我们引入了一个解纠缠的多上下文元学习框架，明确将每个任务因素分配给不同的上下文向量。通过解耦这些变化，我们的方法通过更深入的任务理解提高了鲁棒性，并通过在具有共享因素的任务间共享上下文向量来增强泛化能力。我们在两个领域评估了我们的方法。首先，在正弦回归任务中，我们的模型在分布外任务上优于基线，并通过共享与共享振幅或相位偏移相关的上下文向量推广到未见过的正弦函数。其次，在四足机器人运动任务中，我们在机器人动力学模型中解耦了机器人特定属性和地形特征。通过将从动力学模型获取的解耦上下文向量转移到强化学习中，得到的策略在分布外条件下提高了鲁棒性，超过了依赖单一统一上下文的基线。此外，通过有效共享上下文，我们的模型仅使用20秒的平坦地形真实数据，成功实现了对具有分布外机器人特定属性的有挑战性地形的模拟到现实策略转移，这是单任务适应无法实现的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In meta-learning and its downstream tasks, many methods rely on implicitadaptation to task variations, where multiple factors are mixed together in asingle entangled representation. This makes it difficult to interpret whichfactors drive performance and can hinder generalization. In this work, weintroduce a disentangled multi-context meta-learning framework that explicitlyassigns each task factor to a distinct context vector. By decoupling thesevariations, our approach improves robustness through deeper task understandingand enhances generalization by enabling context vector sharing across taskswith shared factors. We evaluate our approach in two domains. First, on asinusoidal regression task, our model outperforms baselines onout-of-distribution tasks and generalizes to unseen sine functions by sharingcontext vectors associated with shared amplitudes or phase shifts. Second, in aquadruped robot locomotion task, we disentangle the robot-specific propertiesand the characteristics of the terrain in the robot dynamics model. Bytransferring disentangled context vectors acquired from the dynamics model intoreinforcement learning, the resulting policy achieves improved robustness underout-of-distribution conditions, surpassing the baselines that rely on a singleunified context. Furthermore, by effectively sharing context, our model enablessuccessful sim-to-real policy transfer to challenging terrains without-of-distribution robot-specific properties, using just 20 seconds of realdata from flat terrain, a result not achievable with single-task adaptation.</description>
      <author>example@mail.com (Seonsoo Kim, Jun-Gill Kang, Taehong Kim, Seongil Hong)</author>
      <guid isPermaLink="false">2509.01297v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ACA-Net: Future Graph Learning for Logistical Demand-Supply Forecasting</title>
      <link>http://arxiv.org/abs/2509.01997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, DASFAA2025 conference full paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，与传统时空长序列方法相比实现了优越的性能，有效解决了物流需求-供应预测问题。&lt;h4&gt;背景&lt;/h4&gt;物流需求-供应预测对即时食品配送平台的效率和质量至关重要，是调度决策的关键指标。未来订单分布信息对物流需求-供应预测的性能至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决当前时空分析方法难以有效捕捉未来订单分布信息同时保持效率的问题，提高物流需求-供应预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，并采用自适应未来图学习和创新的交叉注意力机制(ACA-Net)的图学习网络框架。&lt;h4&gt;主要发现&lt;/h4&gt;引入进行中和全局图显著提高了预测性能；创新的图学习网络框架能有效学习强大的未来图，显著提高物流需求-供应压力预测结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在实际生产环境中被证明是有效的，相比传统时空长序列方法具有优越性能。&lt;h4&gt;翻译&lt;/h4&gt;评估预期供应与预期需求之间一致性的物流需求-供应预测，对于即时食品配送平台的效率和质量至关重要，并作为调度决策的关键指标。反映即时食品配送中订单分布的未来订单分布信息，对物流需求-供应预测的性能至关重要。当前研究使用时空分析方法从长时间序列中建模未来订单分布信息。然而，学习在线配送平台的未来订单分布是一个对时间序列不敏感且具有强随机性的问题。这些方法往往难以在保持效率的同时有效捕捉这些信息。本文提出了一种创新的时空学习模型，仅使用两个图（进行中和全局）来学习未来订单分布信息，与传统时空长序列方法相比实现了优越的性能。主要贡献如下：(1) 与传统长时间序列相比，在物流需求-供应压力预测中引入进行中和全局图显著提高了预测性能。(2) 提出了一种创新的图学习网络框架，使用自适应未来图学习和创新的交叉注意力机制(ACA-Net)来提取未来订单分布信息，有效学习了一个强大的未来图，显著提高了物流需求-供应压力预测结果。(3) 在实际生产环境中验证了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Logistical demand-supply forecasting that evaluates the alignment betweenprojected supply and anticipated demand, is essential for the efficiency andquality of on-demand food delivery platforms and serves as a key indicator forscheduling decisions. Future order distribution information, which reflects thedistribution of orders in on-demand food delivery, is crucial for theperformance of logistical demand-supply forecasting. Current studies utilizespatial-temporal analysis methods to model future order distributioninformation from serious time slices. However, learning future orderdistribution in online delivery platform is a time-series-insensitive problemwith strong randomness. These approaches often struggle to effectively capturethis information while remaining efficient. This paper proposes an innovativespatiotemporal learning model that utilizes only two graphs (ongoing andglobal) to learn future order distribution information, achieving superiorperformance compared to traditional spatial-temporal long-series methods. Themain contributions are as follows: (1) The introduction of ongoing and globalgraphs in logistical demand-supply pressure forecasting compared to traditionallong time series significantly enhances forecasting performance. (2) Aninnovative graph learning network framework using adaptive future graphlearning and innovative cross attention mechanism (ACA-Net) is proposed toextract future order distribution information, effectively learning a robustfuture graph that substantially improves logistical demand-supply pressureforecasting outcomes. (3) The effectiveness of the proposed method is validatedin real-world production environments.</description>
      <author>example@mail.com (Jiacheng Shi, Haibin Wei, Jiang Wang, Xiaowei Xu, Longzhi Du, Taixu Jiang)</author>
      <guid isPermaLink="false">2509.01997v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Rashomon in the Streets: Explanation Ambiguity in Scene Understanding</title>
      <link>http://arxiv.org/abs/2509.03169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI 2025 Fall Symposium: AI Trustworthiness and Risk Assessment for  Challenged Contexts (ATRACC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了可解释AI在自动驾驶等安全关键应用中的可靠性问题，特别关注了Rashomon效应对解释一致性的影响。&lt;h4&gt;背景&lt;/h4&gt;可解释AI对于验证和信任安全关键应用（如自动驾驶）中的模型至关重要。&lt;h4&gt;目的&lt;/h4&gt;首次对现实驾驶场景中动作预测任务的Rashomon效应进行经验量化。&lt;h4&gt;方法&lt;/h4&gt;使用定性可解释图(QXGs)作为符号场景表示，训练两种模型类的Rashomon集合（可解释的基于对梯度提升模型和复杂图神经网络GNNs），并通过特征归因方法测量解释一致性。&lt;h4&gt;主要发现&lt;/h4&gt;不同模型间存在显著的解释不一致。&lt;h4&gt;结论&lt;/h4&gt;解释模糊性是问题的固有属性，而不仅仅是建模的人为产物。&lt;h4&gt;翻译&lt;/h4&gt;可解释人工智能(XAI)对于验证和信任自动驾驶等安全关键应用中的模型至关重要。然而，XAI的可靠性受到Rashomon效应的挑战，即多个同样准确的模型可能对同一预测提供不同的解释。本文首次对现实驾驶场景中动作预测任务的Rashomon效应进行了经验量化。使用定性可解释图(QXGs)作为符号场景表示，我们训练了两种不同模型类的Rashomon集合：可解释的、基于对的梯度提升模型和复杂的、基于图的图神经网络(GNNs)。使用特征归因方法，我们测量了这些类别内部和之间的解释一致性。我们的结果显示存在显著的解释不一致。我们的研究结果表明，解释模糊性是问题的固有属性，而不仅仅是建模的人为产物。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Explainable AI (XAI) is essential for validating and trusting models insafety-critical applications like autonomous driving. However, the reliabilityof XAI is challenged by the Rashomon effect, where multiple, equally accuratemodels can offer divergent explanations for the same prediction. This paperprovides the first empirical quantification of this effect for the task ofaction prediction in real-world driving scenes. Using Qualitative ExplainableGraphs (QXGs) as a symbolic scene representation, we train Rashomon sets of twodistinct model classes: interpretable, pair-based gradient boosting models andcomplex, graph-based Graph Neural Networks (GNNs). Using feature attributionmethods, we measure the agreement of explanations both within and between theseclasses. Our results reveal significant explanation disagreement. Our findingssuggest that explanation ambiguity is an inherent property of the problem, notjust a modeling artifact.</description>
      <author>example@mail.com (Helge Spieker, Jørn Eirik Betten, Arnaud Gotlieb, Nadjib Lazaar, Nassim Belmecheri)</author>
      <guid isPermaLink="false">2509.03169v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Generalizable Skill Learning for Construction Robots with Crowdsourced Natural Language Instructions, Composable Skills Standardization, and Large Language Model</title>
      <link>http://arxiv.org/abs/2509.02876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review for ASCE OPEN: Multidisciplinary Journal of Civil  Engineering&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种可泛化的学习架构，通过众包在线自然语言指令直接教会机器人多任务执行技能，解决了建筑机器人重新编程工作量大、通用性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;建筑工作的准重复性质导致编程建筑机器人的通用性不足，限制了机器人在建筑行业的广泛应用。机器人无法获得通用能力，因为在一个领域学到的技能无法轻松转移到另一个工作领域或直接用于执行不同的任务集合。&lt;h4&gt;目的&lt;/h4&gt;减少人类工人重新编程机器人场景理解、路径规划和操作组件的工作量，实现机器人多任务技能的高效转移。&lt;h4&gt;方法&lt;/h4&gt;开发了大型语言模型（LLM）、标准化和模块化的分层建模方法，以及建筑信息建模-机器人语义数据管道，以解决多任务技能转移问题。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的机器人任务学习方案实现了最小努力和高质量的多任务重新编程，通过长视野石膏板安装实验验证了方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;基于LLM的分层技能学习框架和技能标准化方案能够显著降低机器人重新编程的工作量，提高任务质量和通用性。&lt;h4&gt;翻译&lt;/h4&gt;建筑工作的准重复性质以及由此导致的编程建筑机器人的通用性不足，持续阻碍着机器人在建筑行业的广泛应用。机器人无法获得通用能力，因为在一个领域学到的技能无法轻松转移到另一个工作领域或直接用于执行不同的任务集合。人类工人必须费力地重新编程机器人的场景理解、路径规划和操作组件，以使机器人能够执行替代的工作任务。本文提出的方法通过提出一种可泛化的学习架构解决了大部分重新编程工作量的问题，该架构通过众包在线自然语言指令直接教会机器人多任务执行技能。开发了大型语言模型（LLM）、标准化和模块化的分层建模方法以及建筑信息建模-机器人语义数据管道，以解决多任务技能转移问题。通过使用全尺寸工业机械臂进行长视野石膏板安装实验测试了所提出的技能标准化方案和基于LLM的分层技能学习框架。所得到的机器人任务学习方案实现了最小努力和高质量的多任务重新编程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决建筑机器人的技能学习和泛化问题。由于建筑工作具有准重复性，导致在一个领域学到的技能难以转移到其他工作领域或直接用于执行不同任务。这个问题很重要，因为建筑行业面临劳动力短缺、成本上升和生产率低等挑战，建筑机器人本可成为解决方案，但成功部署取决于机器人能否灵活执行各种工艺技能，而传统编程方法无法满足这种灵活性需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析建筑行业挑战和机器人应用局限性开始研究，发现模仿学习(IL)虽有效但样本效率低，因此引入分层模仿学习(HIL)来解决数据需求问题。作者认识到语言指令比演示更高效，因此利用大型语言模型(LLM)解决语言准确性问题。方法设计包括建立标准化微技能数据库、开发LLM信息提取系统和构建BIM-ROS信息管道。作者借鉴了IL、HIL、LLM在机器人学习中的应用，以及微技能数据库和BIM系统的现有研究，但针对建筑领域进行了创新整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立标准化的微技能数据库，利用LLM处理众包自然语言指令实现技能链，并通过BIM-ROS管道实现技能参数化。整体流程分为三阶段：1)微技能学习：构建标准化数据库，分析人类指令行为，建立BIM-Web App-ROS信息系统；2)宏技能学习：收集处理自然语言指令，比较不同技能链模型，实现参数化和链式微技能；3)实验验证：评估数据库质量，执行机器人实验，比较算法性能，验证系统可行性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出通用学习架构通过众包自然语言指令教机器人执行多样化任务；2)开发LLM处理自然语言指令；3)提出标准化分层建模方法；4)构建BIM-机器人语义数据管道；5)建立消除个人变异的微技能数据库；6)开发从自然语言指令中检索和连接技能的有效计算方案。相比之前工作，这篇论文专注于建筑机器人领域，结合众包语言、标准化技能和LLM形成完整解决方案，提出具体微技能定义解决语言-动作匹配问题，并通过实验在干墙安装等实际任务中验证了方法有效性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合众包自然语言指令、标准化技能组合和大型语言模型的通用学习架构，使建筑机器人能够高效学习和执行多样化任务技能，显著减少了多任务编程的工作量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The quasi-repetitive nature of construction work and the resulting lack ofgeneralizability in programming construction robots presents persistentchallenges to the broad adoption of robots in the construction industry. Robotscannot achieve generalist capabilities as skills learnt from one domain cannotreadily transfer to another work domain or be directly used to perform adifferent set of tasks. Human workers have to arduously reprogram theirscene-understanding, path-planning, and manipulation components to enable therobots to perform alternate work tasks. The methods presented in this paperresolve a significant proportion of such reprogramming workload by proposing ageneralizable learning architecture that directly teaches robots versatiletask-performance skills through crowdsourced online natural languageinstructions. A Large Language Model (LLM), a standardized and modularizedhierarchical modeling approach, and Building Information Modeling-Robot sematicdata pipeline are developed to address the multi-task skill transfer problem.The proposed skill standardization scheme and LLM-based hierarchical skilllearning framework were tested with a long-horizon drywall installationexperiment using a full-scale industrial robotic manipulator. The resultingrobot task learning scheme achieves multi-task reprogramming with minimaleffort and high quality.</description>
      <author>example@mail.com (Hongrui Yu, Vineet R. Kamat, Carol C. Menassa)</author>
      <guid isPermaLink="false">2509.02876v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Why Do MLLMs Struggle with Spatial Understanding? A Systematic Analysis from Data to Architecture</title>
      <link>http://arxiv.org/abs/2509.02359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The benchmark MulSeT is available at  https://huggingface.co/datasets/WanyueZhang/MulSeT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究系统分析了多模态大语言模型(MLLMs)在空间理解方面的局限性，提出了MulSeT基准，并从数据和架构角度探索了改进空间推理能力的方法。&lt;h4&gt;背景&lt;/h4&gt;空间理解对MLLMs在具身环境中支持感知、推理和规划至关重要。尽管近期有所进展，现有研究表明MLLMs在空间理解方面仍存在困难，且现有研究缺乏对这些局限性的全面系统评估，通常局限于单一场景。&lt;h4&gt;目的&lt;/h4&gt;从数据和架构两个角度，在单视图、多视图和视频三个代表性场景中系统分析MLLMs的空间理解能力，并提出MulSeT基准。&lt;h4&gt;方法&lt;/h4&gt;设计一系列实验分析MLLMs的空间推理能力；研究训练数据增加对性能的影响；探索视觉编码器和语言模型中位置编码的作用；探索推理注入和通过架构设计优化空间理解的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;从数据角度看，随着训练数据增加，空间理解性能迅速收敛但上限较低，特别是需要空间想象的任务，表明仅扩展数据不足以获得满意性能；从架构角度看，空间理解更依赖于视觉编码器内的位置编码，而非语言模型内的位置编码。&lt;h4&gt;结论&lt;/h4&gt;这些发现揭示了当前MLLMs的局限性，并提出通过数据扩展和架构调优提高空间推理能力的新方向。&lt;h4&gt;翻译&lt;/h4&gt;空间理解对于多模态大语言模型(MLLMs)在具身环境中支持感知、推理和规划至关重要。尽管最近有所进展，现有研究表明MLLMs在空间理解方面仍然存在困难。然而，现有研究缺乏对这些局限性的全面和系统评估，通常局限于单一场景，如单视图或视频。在这项工作中，我们从数据和架构两个角度，在三个代表性场景（单视图、多视图和视频）中对空间理解进行了系统分析。我们提出了一个名为MulSeT（多视图空间理解任务）的基准，并设计了一系列实验来分析MLLMs的空间推理能力。从数据角度看，随着训练数据的增加，空间理解的性能迅速收敛，上限相对较低，特别是对于需要空间想象的任务。这表明仅扩展训练数据不足以获得满意性能。从架构角度看，我们发现无论是级联还是原生MLLMs，空间理解更依赖于视觉编码器内的位置编码，而非语言模型内的位置编码。此外，我们探索了推理注入，并展望通过架构设计优化空间理解的未来改进。这些见解揭示了当前MLLMs的局限性，并提出了通过数据扩展和架构调优提高空间推理能力的新方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态大语言模型(MLLMs)在空间理解方面的局限性问题。空间理解对于MLLMs在具身环境中支持感知、推理和规划至关重要，在物体操作、路径规划和构建世界模型等认知任务中扮演核心角色。研究这个问题是因为现有MLLMs在单视图场景中能达到人类水平表现，但在多视图和视频场景中存在显著差距，且缺乏对这些局限性的全面系统性评估。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从数据(数据量)和架构(模型设计)两个维度系统分析MLLMs空间理解能力的局限性。他们首先创建了MulSeT基准，包含三个渐进式任务(遮蔽恢复、距离比较、方位转移)来评估多视图空间理解能力。在数据层面，分析不同训练数据量对性能的影响；在架构层面，研究位置编码在视觉编码器和语言模型中的作用。作者借鉴了现有空间理解任务(如What'sUp、COCO-QA)、位置编码研究(RoPE)、参数高效微调方法(LoRA)和推理注入技术(Chain-of-Thought)的思想，但进行了系统性的整合与创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是系统分析MLLMs空间理解能力的局限性，揭示单纯增加训练数据对空间理解的提升有限，特别是对于需要空间想象的任务，而视觉编码器中的位置编码对空间理解至关重要。整体实现流程包括：1)构建MulSeT基准，在模拟环境中创建多视图场景和三个渐进式任务；2)进行数据层面分析，评估不同训练数据量对性能的影响；3)进行架构层面分析，研究位置编码的作用；4)探索解决方案，包括推理注入(隐式和显式)和架构增强；5)通过实验评估和注意力可视化验证发现。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次从数据到架构对MLLMs空间理解进行全面系统分析，覆盖单视图、多视图和视频三个场景；2)提出MulSeT基准，填补多视图空间理解评估的空白，包含三个渐进式任务形成能力梯度；3)揭示数据量增加对空间理解提升有限，特别是需要空间想象的任务；4)发现视觉编码器中的位置编码对空间理解至关重要，而非语言模型中的位置编码；5)提出多视图提示策略并验证其有效性。相比之前工作，本文更全面、系统，不仅评估了模型性能，还深入分析了背后的机制，并提出了针对性的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性分析揭示了多模态大语言模型在空间理解方面的主要瓶颈在于视觉编码器的位置编码而非数据量不足，并提出了针对性的改进策略。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial understanding is essential for Multimodal Large Language Models(MLLMs) to support perception, reasoning, and planning in embodiedenvironments. Despite recent progress, existing studies reveal that MLLMs stillstruggle with spatial understanding. However, existing research lacks acomprehensive and systematic evaluation of these limitations, often restrictedto isolated scenarios, such as single-view or video. In this work, we present asystematic analysis of spatial understanding from both data and architecturalperspectives across three representative scenarios: single-view, multi-view,and video. We propose a benchmark named MulSeT (Multi-view SpatialUnderstanding Tasks), and design a series of experiments to analyze the spatialreasoning capabilities of MLLMs. From the data perspective, the performance ofspatial understanding converges quickly as the training data increases, and theupper bound is relatively low, especially for tasks that require spatialimagination. This indicates that merely expanding training data is insufficientto achieve satisfactory performance. From the architectural perspective, wefind that spatial understanding relies more heavily on the positional encodingwithin the visual encoder than within the language model, in both cascaded andnative MLLMs. Moreover, we explore reasoning injection and envision futureimprovements through architectural design to optimize spatial understanding.These insights shed light on the limitations of current MLLMs and suggest newdirections for improving spatial reasoning capabilities through data scalingand architectural tuning.</description>
      <author>example@mail.com (Wanyue Zhang, Yibin Huang, Yangbin Xu, JingJing Huang, Helu Zhi, Shuo Ren, Wang Xu, Jiajun Zhang)</author>
      <guid isPermaLink="false">2509.02359v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Space Is Rocket Science - Only Top Reasoning Models Can Solve Spatial Understanding Tasks</title>
      <link>http://arxiv.org/abs/2509.02175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RocketScience，一个开源的对比视觉语言模型基准测试，专门用于测试空间关系理解能力。&lt;h4&gt;背景&lt;/h4&gt;当前视觉语言模型在空间关系理解方面存在明显不足，需要一个专门的基准测试来评估这方面的能力。&lt;h4&gt;目的&lt;/h4&gt;创建一个对人类容易但对当前VLMs困难的基准测试，专门评估模型在空间关系理解方面的表现。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含全新真实世界图像-文本对的基准测试，主要关注相对空间理解和物体顺序。进行了解缠分析，分离了基于思维链模型中的物体定位和空间推理的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;开源和前沿商业VLMs在空间关系理解方面表现明显不足；推理模型表现出令人惊讶的高性能；基准测试的性能瓶颈在于空间推理能力，而非物体定位能力。&lt;h4&gt;结论&lt;/h4&gt;当前VLMs在空间关系理解方面存在显著不足，需要改进这方面的能力以提升整体表现。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了RocketScience，一个开源的对比视觉语言模型基准测试，用于测试空间关系理解能力。它完全由全新的真实世界图像-文本对组成，主要涵盖相对空间理解和物体顺序。该基准测试设计为对人类非常容易但对当前一代VLMs非常困难，这一点已通过经验验证。我们的结果显示，开源和前沿商业VLMs在空间关系理解方面明显缺乏，而推理模型表现出令人惊讶的高性能。此外，我们进行了解缠分析，分离了基于思维链的模型中物体定位和空间推理的贡献，发现基准测试的性能受限于空间推理能力，而非物体定位能力。我们以CC-BY-4.0许可证发布数据集，并在https://github.com/nilshoehing/rocketscience提供评估代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose RocketScience, an open-source contrastive VLM benchmark that testsfor spatial relation understanding. It is comprised of entirely new real-worldimage-text pairs covering mostly relative spatial understanding and the orderof objects. The benchmark is designed  to be very easy for humans and hard for the current generation of VLMs, andthis is empirically verified. Our results show a striking lack of spatialrelation understanding in open source and frontier commercial VLMs and asurprisingly high performance of reasoning models. Additionally, we perform adisentanglement analysis to separate the contributions of object localizationand spatial reasoning in chain-of-thought-based models and find that theperformance on the benchmark is bottlenecked by spatial reasoning and notobject localization capabilities.  We release the dataset with a CC-BY-4.0 license and make the evaluation codeavailable at: https://github.com/nilshoehing/rocketscience</description>
      <author>example@mail.com (Nils Hoehing, Mayug Maniparambil, Ellen Rushe, Noel E. O'Connor, Anthony Ventresque)</author>
      <guid isPermaLink="false">2509.02175v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Omnidirectional Spatial Modeling from Correlated Panoramas</title>
      <link>http://arxiv.org/abs/2509.02164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍CFpano数据集和方法名多模态大语言模型，用于处理跨帧相关全景图像的视觉问答任务。该数据集包含2700多张图像和8000多个问答对，方法使用GRPO优化和定制奖励函数进行微调，在实验中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;全方位场景理解对于具身AI、自动驾驶和沉浸式环境等多种下游应用至关重要，但由于360度图像中的几何失真和复杂空间关系，这仍然具有挑战性。现有的全方位方法仅处理单帧图像，忽略了跨帧相关的全景图。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究人员引入了CFpano，这是第一个专注于整体360度场景中跨帧相关全景图像视觉问答的基准数据集。基于此数据集，他们进一步提出了方法名，一个使用GRPO和定制奖励函数进行微调的多模态大语言模型，以实现跨帧相关全景图的稳健和一致的推理。&lt;h4&gt;方法&lt;/h4&gt;研究人员构建了CFpano数据集，包含2700多张图像和8000多个问答对，问题类型包括多项选择和开放式VQA。基于CFpano，他们提出了方法名，一个使用GRPO和定制奖励函数进行微调的多模态大语言模型。他们使用现有的多模态大语言模型在CFpano上进行了基准实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，方法名在多项选择和开放式VQA任务上都取得了最先进的性能，在所有主要推理类别上都优于强基线（整体性能提升5.37%）。分析验证了GRPO的有效性，并为全景场景理解建立了新的基准。&lt;h4&gt;结论&lt;/h4&gt;CFpano和方法名为跨帧相关全景图像的视觉问答任务提供了新的基准和解决方案，能够实现更稳健和一致的全景场景理解。&lt;h4&gt;翻译&lt;/h4&gt;全方位场景理解对于各种下游应用至关重要，例如具身AI、自动驾驶和沉浸式环境，但由于360度图像中的几何失真和复杂空间关系，这仍然具有挑战性。现有的全方位方法在单帧内实现场景理解，同时忽略了跨帧相关的全景图。为了填补这一空白，我们引入了CFpano，这是第一个专注于整体360度场景中跨帧相关全景图像视觉问答的基准数据集。CFpano包含2700多张图像和8000多个问答对，问题类型包括多项选择和开放式VQA。基于我们的CFpano，我们进一步提出了方法名，一个使用GRPO和定制奖励函数进行微调的多模态大语言模型，以实现跨帧相关全景图的稳健和一致的推理。我们使用现有的多模态大语言模型在CFpano上进行了基准实验。实验结果表明，方法名在多项选择和开放式VQA任务上都取得了最先进的性能，在所有主要推理类别上都优于强基线（整体性能提升5.37%）。我们的分析验证了GRPO的有效性，并为全景场景理解建立了新的基准。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让AI模型更好地理解和推理360度全景图像中的跨帧相关内容。这个问题重要是因为全景图像在具身AI、自动驾驶和虚拟现实等领域有广泛应用，但全景图像存在严重几何失真、高物体密度和复杂空间关系，使得视觉理解比传统窄视野图像更具挑战性。现有方法大多只关注单帧全景图像，忽略了跨帧之间的相关性，而跨帧理解对完整把握全景场景至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有全景数据集的局限性，特别是缺乏跨帧、多视图和细粒度推理的全面覆盖。他们基于ReplicaPano数据集构建了CFpano数据集，利用其丰富的3D注释信息。模型设计上，作者借鉴了Qwen2.5-VL作为基础模型，并采用DeepSeek-R1等模型中成功的GRPO强化学习方法。同时，作者设计了专门针对全景场景的奖励函数，包括格式奖励、准确度奖励和一致性奖励，以优化模型在跨帧全景推理中的表现。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用跨帧全景图像之间的相关性来增强场景理解，并通过专门的强化学习训练和任务特定的奖励函数提高模型在全景场景推理中的表现。整体流程包括：1) 构建CFpano数据集，利用3D注释生成细粒度描述和问题；2) 基于Qwen2.5-VL构建Pano-R1模型；3) 使用GRPO进行微调，并设计三种特定奖励函数；4) 评估模型在多选题和开放式问答任务上的性能。模型训练采用LoRA进行参数高效微调，评估时使用精确匹配准确度评估多选题，语义相似度评估开放式问答。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) CFpano数据集：首个专门用于跨帧全景视觉问答的数据集，包含2700多张图像和8000多个问答对；2) Pano-R1模型：基于GRPO训练的多模态大语言模型，针对全景场景优化；3) 特定奖励函数设计：格式奖励、准确度奖励和一致性奖励。相比之前的工作，CFpano专注于跨帧相关全景图像，而不仅仅是单帧；Pano-R1专门针对全景场景进行了优化，使用GRPO而非传统强化学习方法；评估框架更全面，涵盖多种推理类型和问题格式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文贡献了首个专门用于跨帧全景视觉问答的数据集CFpano，并提出了基于强化学习的多模态大语言模型Pano-R1，显著提升了AI在360度全景场景中的理解和推理能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Omnidirectional scene understanding is vital for various downstreamapplications, such as embodied AI, autonomous driving, and immersiveenvironments, yet remains challenging due to geometric distortion and complexspatial relations in 360{\deg} imagery. Existing omnidirectional methodsachieve scene understanding within a single frame while neglecting cross-framecorrelated panoramas. To bridge this gap, we introduce \textbf{CFpano}, the\textbf{first} benchmark dataset dedicated to cross-frame correlated panoramasvisual question answering in the holistic 360{\deg} scenes. CFpano consists ofover 2700 images together with over 8000 question-answer pairs, and thequestion types include both multiple choice and open-ended VQA. Building uponour CFpano, we further present \methodname, a multi-modal large language model(MLLM) fine-tuned with Group Relative Policy Optimization (GRPO) and a set oftailored reward functions for robust and consistent reasoning with cross-framecorrelated panoramas. Benchmark experiments with existing MLLMs are conductedwith our CFpano. The experimental results demonstrate that \methodname achievesstate-of-the-art performance across both multiple-choice and open-ended VQAtasks, outperforming strong baselines on all major reasoning categories(\textbf{+5.37\%} in overall performance). Our analyses validate theeffectiveness of GRPO and establish a new benchmark for panoramic sceneunderstanding.</description>
      <author>example@mail.com (Xinshen Zhang, Tongxi Fu, Xu Zheng)</author>
      <guid isPermaLink="false">2509.02164v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>AI-Driven Marine Robotics: Emerging Trends in Underwater Perception and Ecosystem Monitoring</title>
      <link>http://arxiv.org/abs/2509.01878v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了水下AI作为研究前沿的兴起，以及海洋感知从利基应用转变为AI创新催化剂的因素。研究分析了水下独特挑战如何推动AI学习方法的进步，以及这些创新如何超越海洋应用，对一般计算机视觉、机器人和环境监测产生积极影响。&lt;h4&gt;背景&lt;/h4&gt;海洋生态系统正面临气候变化带来的日益增长的压力，这促使需要可扩展的、由人工智能驱动的监测解决方案。&lt;h4&gt;目的&lt;/h4&gt;研究水下AI作为主要研究前沿的快速兴起，分析将海洋感知从利基应用转变为AI创新催化剂的因素，并探讨水下约束如何推动AI技术的边界。&lt;h4&gt;方法&lt;/h4&gt;识别三个趋同驱动因素，分析水下独特挑战如何推动AI学习方法的进步，调查数据集、场景理解和3D重建的新趋势，以及水下AI监测范式的转变。&lt;h4&gt;主要发现&lt;/h4&gt;1) 三个趋同驱动因素：生态系统规模监测的环境必要性、公民科学平台使水下数据集民主化、研究人员从饱和的陆地计算机视觉领域迁移；2) 水下独特挑战（浑浊度、隐匿物种检测、专家标注瓶颈、跨生态系统泛化）推动了弱监督学习、开放集识别和退化条件下鲁棒感知的基本进展；3) 水下约束正在推动基础模型、自监督学习和感知的边界，其方法创新远远超出海洋应用。&lt;h4&gt;结论&lt;/h4&gt;水下AI监测正在从被动观察向AI驱动的有针对性干预能力转变范式，其方法创新不仅对海洋科学有意义，还对一般计算机视觉、机器人和环境监测领域有广泛影响。&lt;h4&gt;翻译&lt;/h4&gt;海洋生态系统正面临气候变化带来的日益增长的压力，这促使需要可扩展的、由人工智能驱动的监测解决方案。本文研究了水下AI作为主要研究前沿的快速兴起，分析了将海洋感知从利基应用转变为AI创新催化剂的因素。我们确定了三个趋同驱动因素：生态系统规模监测的环境必要性、公民科学平台使水下数据集民主化、研究人员从饱和的陆地计算机视觉领域迁移。我们的分析揭示了水下独特挑战——浑浊度、隐匿物种检测、专家标注瓶颈和跨生态系统泛化——如何推动弱监督学习、开放集识别和退化条件下鲁棒感知的基本进展。我们调查了数据集、场景理解和3D重建的新趋势，突显了从被动观察向AI驱动的有针对性干预能力转变的范式转变。该论文展示了水下约束如何推动基础模型、自监督学习和感知的边界，其方法创新远远超出海洋应用，有利于一般的计算机视觉、机器人和环境监测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决海洋生态系统监测面临的挑战，特别是气候变化对珊瑚礁和海草甸等生态系统的威胁。这个问题重要是因为传统监测方法成本高、效率低且难以覆盖广阔的海洋区域，而AI驱动的自动化解决方案能够实现生态系统规模的监测，为海洋保护和恢复提供关键支持。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于三个汇聚的驱动因素设计方法：环境必要性（对大规模监测的需求）、数据民主化（通过公民科学平台获取数据）和研究机会（水下环境提供新的AI创新空间）。作者借鉴了现有的计算机视觉、机器学习和深度学习技术，包括自监督学习、基础模型、开集识别、弱监督学习、视觉-语言模型和3D重建技术，并将这些方法应用于水下环境，根据水下特有的挑战进行了调整和创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将水下环境的独特挑战转化为AI创新机会，推动技术进步的同时解决海洋生态监测需求。整体流程包括：1)数据收集与标注（利用公民科学平台、游戏化工具和半自动AI标注）；2)场景理解（应用弱监督学习、鲁棒感知技术和基础模型）；3)场景重建（使用水下特定3D重建技术）；4)应用部署（渔业管理、水产养殖、珊瑚礁恢复等）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)数据民主化创新（公民科学和游戏化平台）；2)弱监督学习方法（从密集监督到图像级标注）；3)鲁棒感知技术（针对水下视觉退化）；4)基础模型应用（如CoralSCOP）；5)水下3D重建（实时处理和数字孪生）。相比之前工作，这些创新实现了从小规模到大规模、从密集监督到弱监督、从通用技术到领域感知算法、从被动观测到主动干预、从单一任务到通用模型的转变。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了水下环境如何通过其独特挑战推动AI创新，同时为应对海洋生态系统监测的迫切需求提供解决方案，建立了环境需求与技术进步之间的双向关系。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Marine ecosystems face increasing pressure due to climate change, driving theneed for scalable, AI-powered monitoring solutions. This paper examines therapid emergence of underwater AI as a major research frontier and analyzes thefactors that have transformed marine perception from a niche application into acatalyst for AI innovation. We identify three convergent drivers: environmentalnecessity for ecosystem-scale monitoring, democratization of underwaterdatasets through citizen science platforms, and researcher migration fromsaturated terrestrial computer vision domains. Our analysis reveals how uniqueunderwater challenges - turbidity, cryptic species detection, expert annotationbottlenecks, and cross-ecosystem generalization - are driving fundamentaladvances in weakly supervised learning, open-set recognition, and robustperception under degraded conditions. We survey emerging trends in datasets,scene understanding and 3D reconstruction, highlighting the paradigm shift frompassive observation toward AI-driven, targeted intervention capabilities. Thepaper demonstrates how underwater constraints are pushing the boundaries offoundation models, self-supervised learning, and perception, withmethodological innovations that extend far beyond marine applications tobenefit general computer vision, robotics, and environmental monitoring.</description>
      <author>example@mail.com (Scarlett Raine, Tobias Fischer)</author>
      <guid isPermaLink="false">2509.01878v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Articulated Object Estimation in the Wild</title>
      <link>http://arxiv.org/abs/2509.01708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9th Conference on Robot Learning (CoRL), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个名为ArtiPoint的新框架，用于在动态相机运动和部分可观察性条件下推断关节式物体模型。作者还推出了Arti4D数据集，这是第一个以第一人称视角在自然环境中捕获关节式物体交互的数据集，并通过基准测试证明了ArtiPoint的优越性能。&lt;h4&gt;背景&lt;/h4&gt;理解关节式物体的三维运动在机器人场景理解、移动操作和运动规划中至关重要。先前的方法主要关注受控环境，假设固定的相机视角或直接观察各种物体状态，这些方法在更真实的无约束环境中往往失效。相比之下，人类可以轻松地通过观察他人操作物体来推断关节运动。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在动态相机运动和部分可观察性条件下推断关节式物体模型的新框架，并创建相应的数据集以促进该领域的研究。&lt;h4&gt;方法&lt;/h4&gt;作者提出了ArtiPoint框架，结合深度点跟踪和因子图优化框架，可以直接从原始RGB-D视频中稳健地估计关节部件轨迹和关节轴。同时创建了Arti4D数据集，包含关节标签和真实相机姿态。&lt;h4&gt;主要发现&lt;/h4&gt;ArtiPoint在Arti4D数据集上的性能优于一系列经典和基于学习的基线方法。&lt;h4&gt;结论&lt;/h4&gt;作者将ArtiPoint代码和Arti4D数据集公开，可供研究人员使用，网址为https://artipoint.cs.uni-freiburg.de。&lt;h4&gt;翻译&lt;/h4&gt;理解关节式物体的三维运动在机器人场景理解、移动操作和运动规划中至关重要。先前用于关节估计的方法主要关注受控环境，假设固定的相机视角或直接观察各种物体状态，这些方法在更真实的无约束环境中往往失效。相比之下，人类可以轻松地通过观察他人操作物体来推断关节运动。受此启发，我们引入了ArtiPoint，这是一个新颖的估计框架，可以在动态相机运动和部分可观察性的情况下推断关节式物体模型。通过结合深度点跟踪和因子图优化框架，ArtiPoint可以直接从原始RGB-D视频中稳健地估计关节部件轨迹和关节轴。为了促进该领域的未来研究，我们引入了Arti4D，这是第一个以第一人称视角在自然环境中捕获关节式物体交互的数据集，包含关节标签和真实相机姿态。我们在一系列经典和基于学习的基线上对ArtiPoint进行了基准测试，证明了其在Arti4D上的优越性能。我们在https://artipoint.cs.uni-freiburg.de上公开了代码和Arti4D。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在非受控环境中（动态相机视角、部分可观测条件下）估计可关节化物体三维运动模型的问题。这个问题在机器人领域很重要，因为机器人场景理解、移动操作和运动规划都需要理解可关节化物体的运动，而现有方法在受控环境中表现良好，但在真实世界场景中往往失败。人类能轻松通过观察他人操作物体推断关节活动，而机器人系统还缺乏这种能力，限制了从可关节化物体估计到日常机器人中人类到机器人模仿的见解转移。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人类通过观察他人操作物体推断关节活动的启发，设计了ArtiPoint方法。他们借鉴了多项现有工作：利用了深度点跟踪技术（特别是CoTracker3等any-point跟踪模型），采用了因子图优化框架（参考Buchanan等人的工作），使用手部检测作为触发信号（借鉴交互感知领域），并利用MobileSAM进行类无关的实例分割。设计过程包括四个阶段：提取交互片段、识别关键点、深度点跟踪与3D过滤、以及关节模型估计，形成一个完整的从输入到输出的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; ArtiPoint方法的核心思想是利用人类操作物体的手部作为先验知识，通过跟踪手部附近的点来捕获物体运动，然后利用这些点的轨迹估计可关节化物体的运动模型。整体实现流程分为四个阶段：1）交互间隔提取：使用手部分割模型识别包含物体交互的视频片段；2）深度点跟踪：在手部附近采样点，使用实例分割模型识别潜在物体，并跟踪关键点；3）3D轨迹估计和过滤：将2D轨迹提升到3D，补偿相机运动，过滤静态点和被遮挡点，并进行轨迹平滑；4）关节模型估计：使用因子图框架估计物体部件轨迹和关节运动模型参数，分类关节类型并在场景中注册模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1）ArtiPoint框架：一种能在非受控环境中基于深度点轨迹工作的鲁棒估计方法；2）Arti4D数据集：首个场景级的野外可关节化物体交互数据集，包含关节轴线标签和3D真实相机姿态；3）全面的性能比较：与经典和深度学习方法进行对比；4）公开资源：发布代码、数据和模型预测。相比之前工作，ArtiPoint能在动态相机运动和部分可观测条件下工作，处理场景级而非孤立物体，结合深度点跟踪和因子图优化而非依赖昂贵场景级优化或易过拟合的深度模型，并利用手部检测作为交互触发信号。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ArtiPoint通过结合深度点跟踪和因子图优化，首次在非受控环境中实现了对可关节化物体的鲁棒运动模型估计，并提供了首个场景级的野外可关节化物体交互数据集Arti4D，推动了机器人场景理解的发展。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the 3D motion of articulated objects is essential in roboticscene understanding, mobile manipulation, and motion planning. Prior methodsfor articulation estimation have primarily focused on controlled settings,assuming either fixed camera viewpoints or direct observations of variousobject states, which tend to fail in more realistic unconstrained environments.In contrast, humans effortlessly infer articulation by watching othersmanipulate objects. Inspired by this, we introduce ArtiPoint, a novelestimation framework that can infer articulated object models under dynamiccamera motion and partial observability. By combining deep point tracking witha factor graph optimization framework, ArtiPoint robustly estimates articulatedpart trajectories and articulation axes directly from raw RGB-D videos. Tofoster future research in this domain, we introduce Arti4D, the firstego-centric in-the-wild dataset that captures articulated object interactionsat a scene level, accompanied by articulation labels and ground-truth cameraposes. We benchmark ArtiPoint against a range of classical and learning-basedbaselines, demonstrating its superior performance on Arti4D. We make code andArti4D publicly available at https://artipoint.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Abdelrhman Werby, Martin Büchner, Adrian Röfer, Chenguang Huang, Wolfram Burgard, Abhinav Valada)</author>
      <guid isPermaLink="false">2509.01708v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TopoNav: Topological Graphs as a Key Enabler for Advanced Object Navigation</title>
      <link>http://arxiv.org/abs/2509.01364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TopoNav是一种新框架，通过利用拓扑结构作为空间记忆来解决ObjectNav中的内存管理挑战，特别是在长距离任务和动态场景中。它构建并更新捕捉场景连接、相邻关系和语义含义的拓扑图，帮助代理积累空间知识、检索关键信息并有效推理遥远目标。&lt;h4&gt;背景&lt;/h4&gt;ObjectNav在大语言模型(LLMs)的帮助下取得了很大进展，但在内存管理方面仍然面临挑战，特别是在长距离任务和动态场景中。&lt;h4&gt;目的&lt;/h4&gt;提出TopoNav新框架，解决ObjectNav中的内存管理挑战，特别是在长距离任务和动态场景中。&lt;h4&gt;方法&lt;/h4&gt;TopoNav利用拓扑结构作为空间记忆，构建并更新拓扑图，捕捉场景连接、相邻关系和语义含义，帮助代理随时间积累空间知识，检索关键信息，并有效推理遥远目标。&lt;h4&gt;主要发现&lt;/h4&gt;TopoNav在基准ObjectNav数据集上取得了最先进的性能，具有更高的成功率和更高效的路径。它在多样化和复杂环境中表现尤为出色，能够将临时视觉输入与持久的空间理解连接起来。&lt;h4&gt;结论&lt;/h4&gt;TopoNav通过拓扑结构有效解决了ObjectNav中的内存管理问题，特别是在长距离任务和动态场景中。&lt;h4&gt;翻译&lt;/h4&gt;物体导航(ObjectNav)在大语言模型(LLMs)的帮助下取得了很大进展，但在内存管理方面仍然面临挑战，特别是在长距离任务和动态场景中。为此，我们提出了TopoNav，一种利用拓扑结构作为空间记忆的新框架。通过构建和更新捕捉场景连接、相邻关系和语义含义的拓扑图，TopoNav帮助代理随时间积累空间知识，检索关键信息，并有效推理遥远目标。我们的实验显示，TopoNav在基准ObjectNav数据集上取得了最先进的性能，具有更高的成功率和更高效的路径。它在多样化和复杂环境中表现尤为出色，因为它将临时视觉输入与持久的空间理解连接起来。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决对象导航(ObjectNav)中，特别是长距离任务和动态场景下的内存管理挑战。当前方法大多依赖短暂的视觉观察，难以积累和保留环境的长期结构知识，导致目标混乱、冗余路径甚至导航失败。这个问题很重要，因为对象导航是具身AI和机器人交互的核心任务，缺乏稳健的空间记忆机制会阻碍导航系统在真实世界复杂环境中的泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到拓扑结构本质上是一种紧凑、持久的空间记忆形式，不同于像素级视觉细节，拓扑信息能以稳定、支持决策的格式捕获环境结构本质。作者借鉴了现有工作如双记忆网络、CVLN和Mem4Nav等记忆机制，以及ETPNav和Revind等拓扑导航方法，但指出它们的局限性：大多存储数据级观察而非结构级关系，依赖预定义拓扑先验，缺乏实时适应环境变化的能力。基于此，作者设计了TopoNav框架，结合语义点云地图和拓扑记忆图，并利用VLM进行指导。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将拓扑结构作为空间记忆的核心，构建动态更新的拓扑图来捕捉场景连接、相邻性和语义含义，作为连接感知和行动的认知地图。整体实现流程包括：1) 构建语义点云地图(场景点云、物体点云、可导航点云、障碍物点云和前沿点云)；2) 构建拓扑记忆图(创建包含ID、位置、物体类别、房间类型和前沿计数的节点，并实现节点创建和合并)；3) 设计航点选择策略(基于VLM指导，分为探索阶段和目标获取阶段，使用多种能力值进行决策)；4) 系统集成(在每个时间步结合语义地图、拓扑记忆图和VLM指导选择最佳航点)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 建立了拓扑结构与对象导航中空间记忆的理论联系；2) 引入TopoNav框架实现动态拓扑记忆图，桥接短暂视觉输入和持久空间理解；3) 在基准数据集上实现了最先进的性能。相比之前工作，TopoNav将拓扑结构建模为可操作的空间记忆而非静态地图，动态更新图节点和边以反映环境变化；将拓扑图与LLM紧密集成实现语义感知的拓扑更新；通过视觉-语言交互不断细化空间知识，支持实时自适应偏差纠正和长距离目标推理；作为零样本方法，其性能匹配或超越了基于训练的方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TopoNav通过引入基于拓扑结构的动态空间记忆机制，显著提升了对象导航在长距离任务和复杂环境中的性能，成功桥接了短暂视觉感知与持久空间理解之间的差距，实现了无需训练的最先进导航效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object Navigation (ObjectNav) has made great progress with large languagemodels (LLMs), but still faces challenges in memory management, especially inlong-horizon tasks and dynamic scenes. To address this, we propose TopoNav, anew framework that leverages topological structures as spatial memory. Bybuilding and updating a topological graph that captures scene connections,adjacency, and semantic meaning, TopoNav helps agents accumulate spatialknowledge over time, retrieve key information, and reason effectively towarddistant goals. Our experiments show that TopoNav achieves state-of-the-artperformance on benchmark ObjectNav datasets, with higher success rates and moreefficient paths. It particularly excels in diverse and complex environments, asit connects temporary visual inputs with lasting spatial understanding.</description>
      <author>example@mail.com (Peiran Liu, Qiang Zhang, Daojie Peng, Lingfeng Zhang, Yihao Qin, Hang Zhou, Jun Ma, Renjing Xu, Yiding Ji)</author>
      <guid isPermaLink="false">2509.01364v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Robix: A Unified Model for Robot Interaction, Reasoning and Planning</title>
      <link>http://arxiv.org/abs/2509.01106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech report. Project page: https://robix-seed.github.io/robix/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Robix是一个统一的视觉-语言模型，整合了机器人推理、任务规划和自然语言交互，作为分层机器人系统的高级认知层，能够生成命令和响应，使机器人遵循复杂指令、规划长期任务并与人类自然交互。&lt;h4&gt;背景&lt;/h4&gt;机器人系统需要高级认知能力来处理复杂任务规划和人机交互，现有方法可能缺乏统一架构和高级推理能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的模型，整合机器人推理、任务规划和自然语言交互，实现机器人遵循复杂指令、规划长期任务并与人类自然交互。&lt;h4&gt;方法&lt;/h4&gt;采用三阶段训练策略：持续预训练增强基础具身推理能力；监督微调将人机交互和任务规划建模为统一序列；强化学习提高推理-动作一致性和任务连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;Robix在交互式任务执行中优于GPT-4o和Gemini 2.5 Pro等基线，在不同指令类型和用户参与任务中表现出强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Robix作为一个统一模型，成功整合了机器人的多种高级认知功能，提供了比现有基线更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了Robix，这是一个统一模型，在单一的视觉-语言架构中整合了机器人推理、任务规划和自然语言交互。作为分层机器人系统的高级认知层，Robix动态生成原子命令给低级控制器并为人类交互生成口头响应，使机器人能够在端到端框架内遵循复杂指令、规划长期任务并与人类自然交互。Robix进一步引入了新功能，如主动对话、实时中断处理和任务执行中的上下文常识推理。其核心是利用思维链推理，并采用三阶段训练策略：(1)持续预训练以增强基础具身推理能力，包括3D空间理解、视觉定位和任务中心推理；(2)监督微调将人机交互和任务规划建模为统一的推理-动作序列；(3)强化学习以提高推理-动作一致性和长期任务连贯性。大量实验表明，Robix在交互式任务执行中优于开源和商业基线（如GPT-4o和Gemini 2.5 Pro），在多样化指令类型（如开放式、多阶段、约束式、无效式和中断式）和各种用户参与任务（如餐桌清理、杂货采购和饮食过滤）中表现出强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Robix, a unified model that integrates robot reasoning, taskplanning, and natural language interaction within a single vision-languagearchitecture. Acting as the high-level cognitive layer in a hierarchical robotsystem, Robix dynamically generates atomic commands for the low-levelcontroller and verbal responses for human interaction, enabling robots tofollow complex instructions, plan long-horizon tasks, and interact naturallywith human within an end-to-end framework. Robix further introduces novelcapabilities such as proactive dialogue, real-time interruption handling, andcontext-aware commonsense reasoning during task execution. At its core, Robixleverages chain-of-thought reasoning and adopts a three-stage trainingstrategy: (1) continued pretraining to enhance foundational embodied reasoningabilities including 3D spatial understanding, visual grounding, andtask-centric reasoning; (2) supervised finetuning to model human-robotinteraction and task planning as a unified reasoning-action sequence; and (3)reinforcement learning to improve reasoning-action consistency and long-horizontask coherence. Extensive experiments demonstrate that Robix outperforms bothopen-source and commercial baselines (e.g., GPT-4o and Gemini 2.5 Pro) ininteractive task execution, demonstrating strong generalization across diverseinstruction types (e.g., open-ended, multi-stage, constrained, invalid, andinterrupted) and various user-involved tasks such as table bussing, groceryshopping, and dietary filtering.</description>
      <author>example@mail.com (Huang Fang, Mengxi Zhang, Heng Dong, Wei Li, Zixuan Wang, Qifeng Zhang, Xueyun Tian, Yucheng Hu, Hang Li)</author>
      <guid isPermaLink="false">2509.01106v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SWAGSplatting: Semantic-guided Water-scene Augmented Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2509.00800v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to SIGGRAPH Asia 2025 Technical Communications&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合语义引导的3D高斯飞溅技术，用于水下3D重建。通过语义特征嵌入和分阶段训练策略，提高了重建质量和稳定性，在多个数据集上展示了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;水下环境中的准确3D重建是一个复杂挑战，主要问题包括光线扭曲、浑浊度和能见度有限。基于AI的技术已被用于解决这些问题，但现有方法尚未充分利用AI的潜力，特别是在将语言模型与视觉处理集成方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架，利用多模态交叉知识创建语义引导的3D高斯飞溅，实现稳健且高保真的深海场景重建。&lt;h4&gt;方法&lt;/h4&gt;将额外的语义特征嵌入到每个高斯基元中，由CLIP提取的语义特征监督，通过专门的语义一致性损失确保与高级场景理解的对齐，并提出一种新的分阶段训练策略，结合从粗到细的学习和后期参数优化。&lt;h4&gt;主要发现&lt;/h4&gt;在SeaThru-NeRF和Submerged3D数据集上，该方法在三个指标上持续优于最先进的方法，在PSNR方面平均提高了高达3.09分贝。&lt;h4&gt;结论&lt;/h4&gt;该方法在水下探索和海洋感知应用中是一个强有力的候选方案。&lt;h4&gt;翻译&lt;/h4&gt;在水下环境中进行准确的3D重建仍然是一个复杂的挑战，由于光线扭曲、浑浊度和能见度有限等问题。基于AI的技术已被应用于解决这些问题，然而，现有方法尚未充分利用AI的潜力，特别是在将语言模型与视觉处理集成方面。在本文中，我们提出了一种新颖的框架，利用多模态交叉知识创建语义引导的3D高斯飞溅，用于稳健且高保真的深海场景重建。通过将额外的语义特征嵌入到每个高斯基元中，并由CLIP提取的语义特征监督，我们的方法在整个训练过程中强制执行语义和结构感知。专门的语义一致性损失确保与高级场景理解的对齐。此外，我们提出了一种新颖的分阶段训练策略，结合从粗到细的学习和后期参数优化，以进一步提高稳定性和重建质量。大量结果表明，我们的方法在SeaThru-NeRF和Submerged3D数据集上的三个指标中持续优于最先进的方法，在PSNR方面平均提高了高达3.09分贝，使其成为水下探索和海洋感知应用的有力候选。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决水下环境中准确3D重建的挑战，特别是由于光线扭曲、浑浊度和能见度有限等问题导致的重建困难。这个问题在现实中非常重要，因为水下探索支持海洋生态研究、考古学探索和机器人导航等多种应用，这些任务都依赖于高质量的3D重建技术。此外，水下环境的数据采集成本高且通常稀疏，使得重建工作更具挑战性，而高质量的3D视觉在VR头显等教育创意应用中也尤为关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法（如NeRF和3DGS）通常基于清晰介质的假设，在水下浑浊条件下性能严重下降。他们注意到先前方法对所有区域均匀处理，而水下环境中显著物体（通常在前景）应得到更多关注。作者借鉴了RUSplatting的工作，使用MLP估计水下介质参数，结合帧插值机制和边缘感知损失。此外，他们利用CLIP模型提取语义特征，使用BLIPo3生成场景描述，并通过Grounded-SAM捕获感兴趣区域。基于这些思考，作者设计了语义引导的高斯点和分阶段训练策略来解决水下重建问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为每个3D高斯点添加可学习的语义特征，通过CLIP提取的语义特征进行监督，强制模型在整个训练过程中保持语义和结构感知，并采用分阶段训练策略结合粗到细学习和后期参数细化。整体流程包括：1)为场景生成文本描述并使用Grounded-SAM捕获感兴趣区域；2)计算检测区域的CLIP嵌入作为参考语义特征；3)为每个3D高斯点添加语义特征向量；4)训练中通过语义损失函数使高点特征与参考特征对齐；5)采用分阶段优化，前60%关注粗结构和语义对齐，后40%冻结几何参数细化外观和颜色，同时调整损失函数权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)语义引导的高斯点，为每个3D高斯添加可学习语义特征；2)语义一致性损失，确保与高级场景理解一致；3)分阶段训练策略，结合粗到细学习和后期参数细化。相比之前的工作，SWAGSplatting特别关注水下显著物体而非均匀处理所有区域，通过语义机制增强几何一致性和区域级对齐，并采用分阶段优化策略提高训练稳定性和重建质量，解决了传统方法在浑浊水下环境中性能下降的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SWAGSplatting通过引入语义引导机制和分阶段训练策略，显著提高了水下场景的3D重建质量和保真度，有效解决了水下环境中光线扭曲、浑浊度和能见度有限等挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D reconstruction in underwater environments remains a complexchallenge due to issues such as light distortion, turbidity, and limitedvisibility. AI-based techniques have been applied to address these issues,however, existing methods have yet to fully exploit the potential of AI,particularly in integrating language models with visual processing. In thispaper, we propose a novel framework that leverages multimodal cross-knowledgeto create semantic-guided 3D Gaussian Splatting for robust and high-fidelitydeep-sea scene reconstruction. By embedding an extra semantic feature into eachGaussian primitive and supervised by the CLIP extracted semantic feature, ourmethod enforces semantic and structural awareness throughout the training. Thededicated semantic consistency loss ensures alignment with high-level sceneunderstanding. Besides, we propose a novel stage-wise training strategy,combining coarse-to-fine learning with late-stage parameter refinement, tofurther enhance both stability and reconstruction quality. Extensive resultsshow that our approach consistently outperforms state-of-the-art methods onSeaThru-NeRF and Submerged3D datasets across three metrics, with an improvementof up to 3.09 dB on average in terms of PSNR, making it a strong candidate forapplications in underwater exploration and marine perception.</description>
      <author>example@mail.com (Zhuodong Jiang, Haoran Wang, Guoxi Huang, Brett Seymour, Nantheera Anantrasirichai)</author>
      <guid isPermaLink="false">2509.00800v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>OmniReason: A Temporal-Guided Vision-Language-Action Framework for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2509.00789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了OmniReason框架，通过联合建模动态3D环境及其底层决策过程，建立稳健的时空推理能力，解决了现有视觉-语言模型在自动驾驶中忽略时间维度的问题。&lt;h4&gt;背景&lt;/h4&gt;现有视觉-语言模型(VLMs)在自动驾驶方面展示了令人印象深刻的空间推理能力，但主要关注静态场景理解，忽略了真实驾驶场景中重要的时间维度。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法忽略时间维度的关键限制，提出能够进行稳健时空推理的框架，以提高自动驾驶系统在复杂动态环境中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出OmniReason-Data两个大规模视觉-语言-行动(VLA)数据集，具有密集时空注释和自然语言解释；开发幻觉减轻自动标注管道确保物理合理性和时间一致性；设计OmniReason-Agent架构，包含稀疏时间记忆模块和解释生成器；提出时空知识蒸馏方法捕捉时空因果推理模式。&lt;h4&gt;主要发现&lt;/h4&gt;OmniReason-Agent在开环规划任务和视觉问答(VQA)基准测试中取得显著改进，为在复杂动态环境中运行的、可解释的、具有时间意识的自动驾驶车辆建立了新能力。&lt;h4&gt;结论&lt;/h4&gt;OmniReason框架成功解决了现有方法在时空推理方面的局限性，通过联合建模动态环境和决策过程，提高了自动驾驶系统的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;最近的视觉-语言模型(VLMs)进展展示了自动驾驶中令人印象深刻的空间推理能力，但现有方法主要关注静态场景理解，同时忽略了真实驾驶场景中本质的时间维度。为解决这一关键限制，我们提出了OmniReason框架，通过联合建模动态3D环境及其底层决策过程，建立稳健的时空推理。我们的工作有两个基本进展：(1)我们引入了OmniReason-Data，两个具有密集时空注释和自然语言解释的大规模视觉-语言-行动(VLA)数据集，通过一种新颖的幻觉减轻自动标注管道生成，确保物理合理性和时间一致性；(2)我们开发了OmniReason-Agent架构，集成了一个稀疏时间记忆模块用于持久场景上下文建模和一个产生人类可解释决策理由的解释生成器，通过我们的时空知识蒸馏方法实现，该方法有效捕捉时空因果推理模式。全面的实验展示了最先进的性能，OmniReason-Agent在开环规划任务和视觉问答(VQA)基准测试中取得显著改进，同时为在复杂动态环境中运行的可解释、具有时间意识的自动驾驶车辆建立了新能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶系统中视觉语言模型忽视时间维度的问题，即现有方法主要关注静态场景理解，难以处理真实驾驶场景中的动态变化和因果关系。这个问题很重要，因为真实驾驶环境是动态变化的，系统需要理解时间上的因果关系；现有方法难以处理罕见的长尾事件，对高级场景语义理解不足，且在开放世界中缺乏自适应、可解释的推理能力，直接影响自动驾驶系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了端到端学习和视觉语言模型在自动驾驶中的应用与局限，借鉴了现有端到端学习方法、大型语言模型以及多种视觉语言数据集的工作。在此基础上，作者设计了两部分核心内容：一是OmniReason-Data数据集，通过规则基础和原则基础的模板结合大型语言模型生成高质量标注；二是OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器，使用时空知识蒸馏捕捉因果推理模式。整个设计紧密围绕时空一致性和人类可解释性展开。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合建模动态3D环境及其底层决策过程来建立强大的时空推理能力，整合人类先验知识、场景感知信息和多模态输入。整体流程分为数据标注和模型实现两部分：数据标注方面，从现有数据集提取信息，应用精心设计的模板引导生成语言-动作对，再通过大型语言模型生成场景描述和因果链；模型实现方面，使用分层视觉编码器处理多视图输入，稀疏时间记忆模块聚合长程上下文，视觉语言模型核心处理特征并生成决策，最后通过知识蒸馏确保人类先验知识的有效整合。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) OmniReason-Data两个大规模VLA数据集，具有密集时空标注和自然语言解释；2) OmniReason-Agent架构，集成稀疏时间记忆模块和解释生成器；3) 模板驱动的标注框架，自动生成高质量可解释的语言-动作对。相比之前工作，不同之处在于：现有数据集主要提供高级离散命令缺乏精确轨迹，而OmniReason提供更丰富的时空和因果信息；现有系统难以建模环境刺激与车辆反应间的因果关系，而OmniReason通过时空依赖而非表面相关性进行推理；明确设计了时间建模机制处理动态场景中的交互和运动关系。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了OmniReason框架，通过大规模时空标注的视觉语言动作数据集和集成稀疏时间记忆与因果推理的端到端模型，显著提升了自动驾驶系统在复杂动态环境中的时空推理能力和决策可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in vision-language models (VLMs) have demonstrated impressivespatial reasoning capabilities for autonomous driving, yet existing methodspredominantly focus on static scene understanding while neglecting theessential temporal dimension of real-world driving scenarios. To address thiscritical limitation, we propose the OmniReason framework, which establishesrobust spatiotemporal reasoning by jointly modeling dynamic 3D environments andtheir underlying decision-making processes. Our work makes two fundamentaladvances: (1) We introduce OmniReason-Data, two large-scalevision-language-action (VLA) datasets with dense spatiotemporal annotations andnatural language explanations, generated through a novelhallucination-mitigated auto-labeling pipeline that ensures both physicalplausibility and temporal coherence; (2) We develop the OmniReason-Agentarchitecture, which integrates a sparse temporal memory module for persistentscene context modeling and an explanation generator that produceshuman-interpretable decision rationales, facilitated by our spatiotemporalknowledge distillation approach that effectively captures spatiotemporal causalreasoning patterns. Comprehensive experiments demonstrate state-of-the-artperformance, where OmniReason-Agent achieves significant improvements in bothopen-loop planning tasks and visual question answering (VQA) benchmarks, whileestablishing new capabilities for interpretable, temporally-aware autonomousvehicles operating in complex, dynamic environments.</description>
      <author>example@mail.com (Pei Liu, Qingtian Ning, Xinyan Lu, Haipeng Liu, Weiliang Ma, Dangen She, Peng Jia, Xianpeng Lang, Jun Ma)</author>
      <guid isPermaLink="false">2509.00789v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ConceptBot: Enhancing Robot's Autonomy through Task Decomposition with Large Language Models and Knowledge Graph</title>
      <link>http://arxiv.org/abs/2509.00570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ConceptBot是一个结合大型语言模型和知识图谱的模块化机器人规划框架，能够生成可行且风险感知的计划，解决自然语言指令模糊和环境物体分析问题。&lt;h4&gt;背景&lt;/h4&gt;机器人系统通常面临缺乏常识推理的挑战，导致难以处理自然语言指令中的歧义，以及正确分析环境中存在的物体。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理自然语言指令歧义、准确分析环境物体，并生成可行且风险感知计划的机器人规划框架。&lt;h4&gt;方法&lt;/h4&gt;ConceptBot整合了三个主要模块：物体属性提取(OPE)模块使用ConceptNet的语义概念丰富场景理解；用户请求处理(URP)模块消除歧义并结构化指令；规划器生成上下文感知的可行拾取和放置策略。&lt;h4&gt;主要发现&lt;/h4&gt;在明确任务上达到100%成功率；在隐式任务上保持87%的准确率（SayCan为31%）；在风险感知任务上达到76%（SayCan为15%）；在材料分类（70%对比20%）和毒性检测（86%对比36%）等特定应用场景中优于SayCan；在SafeAgentBench上获得80%的总体得分（比最佳基线高出46%）。&lt;h4&gt;结论&lt;/h4&gt;ConceptBot能够在不需要领域特定训练的情况下泛化，并显著提高非结构化环境中机器人策略的可靠性，这些结果已在模拟和实验室实验中得到验证。&lt;h4&gt;翻译&lt;/h4&gt;ConceptBot是一个模块化机器人规划框架，它结合了大型语言模型和知识图谱，能够生成可行且风险感知的计划，尽管存在自然语言指令的模糊性以及正确分析环境中存在的物体等挑战，这些挑战通常源于缺乏常识推理。为此，ConceptBot集成了(i)物体属性提取(OPE)模块，使用ConceptNet中的语义概念丰富场景理解；(ii)用户请求处理(URP)模块，消除歧义并结构化指令；(iii)规划器，生成上下文感知的可行拾取和放置策略。在与Google SayCan的比较评估中，ConceptBot在明确任务上取得了100%的成功率，在隐式任务上保持了87%的准确率（SayCan为31%），在风险感知任务上达到了76%（对比15%），并在特定应用场景中优于SayCan，包括材料分类（70%对比20%）和毒性检测（86%对比36%）。在SafeAgentBench上，ConceptBot获得了80%的总体得分（比次佳基线高出46%）。这些在模拟和实验室实验中得到验证的结果，证明了ConceptBot无需领域特定训练即可泛化的能力，并显著提高了非结构化环境中机器人策略的可靠性。网站：https://sites.google.com/view/conceptbot&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; ConceptBot is a modular robotic planning framework that combines LargeLanguage Models and Knowledge Graphs to generate feasible and risk-aware plansdespite ambiguities in natural language instructions and correctly analyzingthe objects present in the environment - challenges that typically arise from alack of commonsense reasoning. To do that, ConceptBot integrates (i) an ObjectProperty Extraction (OPE) module that enriches scene understanding withsemantic concepts from ConceptNet, (ii) a User Request Processing (URP) modulethat disambiguates and structures instructions, and (iii) a Planner thatgenerates context-aware, feasible pick-and-place policies. In comparativeevaluations against Google SayCan, ConceptBot achieved 100% success on explicittasks, maintained 87% accuracy on implicit tasks (versus 31% for SayCan),reached 76% on risk-aware tasks (versus 15%), and outperformed SayCan inapplication-specific scenarios, including material classification (70% vs. 20%)and toxicity detection (86% vs. 36%). On SafeAgentBench, ConceptBot achieved anoverall score of 80% (versus 46% for the next-best baseline). These results,validated in both simulation and laboratory experiments, demonstrateConceptBot's ability to generalize without domain-specific training and tosignificantly improve the reliability of robotic policies in unstructuredenvironments. Website: https://sites.google.com/view/conceptbot</description>
      <author>example@mail.com (Alessandro Leanza, Angelo Moroncelli, Giuseppe Vizzari, Francesco Braghin, Loris Roveda, Blerina Spahiu)</author>
      <guid isPermaLink="false">2509.00570v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Pixels: Introducing Geometric-Semantic World Priors for Video-based Embodied Models via Spatio-temporal Alignment</title>
      <link>http://arxiv.org/abs/2509.00210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出VEME方法，一种新型跨模态对齐技术，通过学习以自我为中心、经验为中心的世界模型，增强深度学习模型在未知环境中的类人推理能力，特别是在复杂任务中的应用。&lt;h4&gt;背景&lt;/h4&gt;在具身智能领域，让深度学习模型在未知环境中执行复杂任务时实现类人推理仍然是一个关键挑战。先进的视觉语言模型在静态场景理解方面表现出色，但在时空推理和适应动态、开放集任务（如任务导向导航和具身问答）方面存在局限性，这是由于对细粒度时空线索和物理世界理解的建模不足。&lt;h4&gt;目的&lt;/h4&gt;解决视觉语言模型在时空推理和动态任务适应方面的局限性，增强模型在动态环境中的推理和规划能力，特别是在未知场景中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;VEME框架整合三个关键组件：(1) 跨模态对齐框架，连接物体、空间表示、视觉语义与时空线索，增强VLM的上下文学习能力；(2) 由世界嵌入激活的动态隐式认知地图，实现任务相关的几何语义记忆回忆；(3) 基于指令的导航和推理框架，利用具身先验进行长期规划和高效探索。&lt;h4&gt;主要发现&lt;/h4&gt;通过嵌入几何感知的时空经验体验，该方法显著提高了模型在动态环境中的推理和规划能力。实验表明，与传统方法相比，准确率和探索效率提高了1%至3%。&lt;h4&gt;结论&lt;/h4&gt;VEME方法通过跨模态对齐和动态认知地图，有效解决了视觉语言模型在时空推理和动态任务适应方面的局限性，为具身智能领域提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在未知环境中为复杂任务实现深度学习模型的类人推理仍然是具身智能中的一个关键挑战。虽然先进的视觉语言模型在静态场景理解方面表现出色，但由于对细粒度时空线索和物理世界理解的建模不足，它们在时空推理和适应动态、开放集任务（如任务导向导航和具身问答）方面的局限性仍然存在。为了解决这个问题，我们提出了VEME，一种新颖的跨模态对齐方法，通过学习以自我为中心、以经验为中心的世界模型来增强在未见场景中的泛化能力。我们的框架整合了三个关键组件：(1) 一个跨模态对齐框架，将物体、空间表示和视觉语义与时空线索连接起来，以增强VLM的上下文学习能力；(2) 一个由世界嵌入激活的动态隐式认知地图，能够实现与任务相关的几何语义记忆回忆；(3) 一个基于指令的导航和推理框架，利用具身先验进行长期规划和高效探索。通过嵌入几何感知的时空经验体验，我们的方法显著提高了在动态环境中的推理和规划能力。在VSI-Bench和VLN-CE上的实验结果表明，与传统方法相比，准确率和探索效率提高了1%至3%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉语言模型(VLMs)在复杂任务中的时空推理能力不足问题，特别是在动态开放环境下的任务导向导航和具身问答任务中表现不佳。这个问题很重要，因为它关系到AI系统如何像人类一样在真实世界中理解和导航，对于机器人导航、自动驾驶等实际应用至关重要，也是具身智能领域的核心挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从认知神经科学获取灵感，注意到人类空间智能成功来自于情景记忆和语义记忆的互补互动。他们识别出现有SLAM方法缺乏高级推理能力，而VLM方法无法捕捉细粒度时空依赖。因此设计了双记忆架构，借鉴了SLAM的空间表示能力、VLM的视觉语言理解能力，以及认知科学中关于人类记忆系统的研究，整合了跨模态对齐框架、动态隐式认知地图和基于指令的导航框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何-语义世界先验增强具身模型，建立双记忆系统(空间语义记忆和情景记忆)，将视觉语义与时空线索对齐。整体流程包括：1)处理语言指令、视觉图像、3D点云和动作历史等输入；2)构建世界嵌入作为认知地图；3)使用交叉注意力将2D语义与3D几何对齐；4)通过情景记忆形成独特经验表示；5)整合所有信息进行统一决策；6)端到端训练优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)认知启发的双记忆框架；2)有效的跨模态时空对齐技术；3)动态隐式认知地图；4)统一决策框架。相比之前工作，VEME不依赖显式3D地图构建(不同于SLAM)，增强了时空推理能力(优于传统VLM)，更紧密连接了视觉感知和空间理解(区别于其他多模态方法)，并结合了显式和隐式记忆的优点(超越单一记忆系统)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VEME通过引入受认知科学启发的双记忆系统和跨模态时空对齐机制，显著提升了具身智能模型在动态未知环境中的导航和推理能力，实现了3%-6%的准确率和探索效率提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving human-like reasoning in deep learning models for complex tasks inunknown environments remains a critical challenge in embodied intelligence.While advanced vision-language models (VLMs) excel in static sceneunderstanding, their limitations in spatio-temporal reasoning and adaptation todynamic, open-set tasks like task-oriented navigation and embodied questionanswering (EQA) persist due to inadequate modeling of fine-grainedspatio-temporal cues and physical world comprehension. To address this, wepropose VEME, a novel cross-modal alignment method that enhances generalizationin unseen scenes by learning an ego-centric, experience-centered world model.Our framework integrates three key components: (1) a cross-modal alignmentframework bridging objects, spatial representations, and visual semantics withspatio-temporal cues to enhance VLM in-context learning; (2) a dynamic,implicit cognitive map activated by world embedding to enable task-relevantgeometric-semantic memory recall; and (3) an instruction-based navigation andreasoning framework leveraging embodied priors for long-term planning andefficient exploration. By embedding geometry-aware spatio-temporal episodicexperiences, our method significantly improves reasoning and planning indynamic environments. Experimental results on VSI-Bench and VLN-CE demonstrate1%-3% accuracy and exploration efficiency improvement compared to traditionalapproaches.</description>
      <author>example@mail.com (Jinzhou Tang, Jusheng zhang, Sidi Liu, Waikit Xiu, Qinhan Lv, Xiying Li)</author>
      <guid isPermaLink="false">2509.00210v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Doctoral Thesis: Geometric Deep Learning For Camera Pose Prediction, Registration, Depth Estimation, and 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2509.01873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  175 pages, 66 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合传统几何技术与深度学习的几何深度学习方法，解决了3D视觉中的基本挑战，如相机姿态估计、点云配准、深度预测和3D重建。&lt;h4&gt;背景&lt;/h4&gt;现代深度学习为3D测绘技术、场景重建和虚拟现实创造了新机会，但3D数据的高维性和标记数据集的稀缺性限制了直接在3D数据上训练深度学习模型。传统SfM和SLAM技术在非结构化环境中表现不佳，难以生成对下游任务有效的详细几何表示。&lt;h4&gt;目的&lt;/h4&gt;开发针对3D视觉关键任务定制的几何深度学习方法，解决3D视觉中的基本挑战，提高几何表示的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;将几何先验或约束(如深度信息、表面法线和等变性)整合到深度学习模型中，系统性地研究3D视觉的关键组成部分，包括相机姿态估计、点云配准、深度估计和高保真3D重建。&lt;h4&gt;主要发现&lt;/h4&gt;几何先验或约束的整合显著增强了几何表示的准确性和鲁棒性，该方法在数字文化遗产保护和沉浸式VR/AR环境等实际应用中展示了有效性。&lt;h4&gt;结论&lt;/h4&gt;需要开发结合传统几何技术和深度学习能力的3D表示方法，以生成具有几何感知能力的深度学习模型，解决当前3D视觉技术的局限性。&lt;h4&gt;翻译&lt;/h4&gt;现代深度学习的发展为3D测绘技术、场景重建流程和虚拟现实开发创造了新机会。尽管3D深度学习技术有所进步，但由于3D数据固有的高维性和标记数据集的稀缺性，直接在3D数据上训练深度学习模型面临挑战。运动结构(SfM)和同步定位与地图构建(SLAM)在应用于结构化室内环境时表现出强大的性能，但通常难以处理非结构化环境中的模糊特征。这些技术通常难以生成对渲染和语义分析等下游任务有效的详细几何表示。当前的限制需要开发结合传统几何技术与深度学习能力的3D表示方法，以生成具有几何感知能力的深度学习模型。该论文通过开发针对相机姿态估计、点云配准、深度预测和3D重建等关键任务定制的几何深度学习方法，解决了3D视觉中的基本挑战。将几何先验或约束(如包含深度信息、表面法线和等变性)整合到深度学习模型中，提高了几何表示的准确性和鲁棒性。本研究系统地研究了3D视觉的关键组成部分，包括相机姿态估计、点云配准、深度估计和高保真3D重建，展示了它们在数字文化遗产保护和沉浸式VR/AR环境等实际应用中的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉技术中的四个关键挑战：相机姿态预测、点云配准、深度估计和3D重建。这些问题在现实中非常重要，因为3D视觉技术对3D映射、场景重建和虚拟现实开发至关重要。直接在3D数据上训练深度学习模型面临高维性和标记数据稀缺的挑战，而传统方法如SfM和SLAM在非结构化环境中表现不佳，难以生成详细的几何表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用模块化方法，将3D视觉问题分解为可独立优化的子任务。每个子任务都结合了传统几何技术与深度学习：相机姿态预测结合ResNet和自适应粒子滤波器；点云配准引入SE(3)-等变性约束；深度估计使用Transformer处理任意长度焦点堆栈；3D重建结合小波变换特征和隐式SDF模型。作者确实借鉴了现有工作，如E2PN编码器、Transformer架构等，但进行了创新性改进，将几何先验知识整合到深度学习中。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统几何约束或先验知识与深度学习模型相结合，创建更可靠、可扩展、高效和通用的3D视觉解决方案。整体流程包括：1)相机姿态预测：利用地平线和地面平面作为参考线索，结合自适应粒子滤波器融合视觉和IMU数据；2)点云配准：使用SE(3)-等变性2D高斯surfel特征，通过E2PN编码器处理点位置和方向；3)深度估计：采用Transformer-LSTM网络处理任意长度焦点堆栈；4)3D重建：利用小波变换深度特征条件化隐式SDF模型，通过三平面投影和UNet融合模块细化重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)相机姿态预测中利用自然几何线索增强稳定性；2)点云配准中引入基于2D Surfel的SE(3)-等变性框架；3)深度估计中提出可处理任意长度焦点堆栈的FocDepthFormer网络；4)3D重建中使用小波变换深度特征解决隐式模型难以捕获高频细节的问题。相比之前的工作，这些方法结合了传统几何约束与深度学习，采用模块化设计而非端到端方法，提高了灵活性、鲁棒性和多尺度特征处理能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将传统几何约束与深度学习相结合，提出了一种模块化的几何深度学习方法，显著提高了3D视觉任务的准确性、鲁棒性和效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern deep learning developments create new opportunities for 3D mappingtechnology, scene reconstruction pipelines, and virtual reality development.Despite advances in 3D deep learning technology, direct training of deeplearning models on 3D data faces challenges due to the high dimensionalityinherent in 3D data and the scarcity of labeled datasets. Structure-from-motion(SfM) and Simultaneous Localization and Mapping (SLAM) exhibit robustperformance when applied to structured indoor environments but often strugglewith ambiguous features in unstructured environments. These techniques oftenstruggle to generate detailed geometric representations effective fordownstream tasks such as rendering and semantic analysis. Current limitationsrequire the development of 3D representation methods that combine traditionalgeometric techniques with deep learning capabilities to generate robustgeometry-aware deep learning models.  The dissertation provides solutions to the fundamental challenges in 3Dvision by developing geometric deep learning methods tailored for essentialtasks such as camera pose estimation, point cloud registration, depthprediction, and 3D reconstruction. The integration of geometric priors orconstraints, such as including depth information, surface normals, andequivariance into deep learning models, enhances both the accuracy androbustness of geometric representations. This study systematically investigateskey components of 3D vision, including camera pose estimation, point cloudregistration, depth estimation, and high-fidelity 3D reconstruction,demonstrating their effectiveness across real-world applications such asdigital cultural heritage preservation and immersive VR/AR environments.</description>
      <author>example@mail.com (Xueyang Kang)</author>
      <guid isPermaLink="false">2509.01873v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Strefer: Empowering Video LLMs with Space-Time Referring and Reasoning via Synthetic Instruction Data</title>
      <link>http://arxiv.org/abs/2509.03501v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This technical report serves as the archival version of our paper  accepted at the ICCV 2025 Workshop. For more information, please visit our  project website: https://strefer.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Strefer，一个合成指令数据生成框架，旨在增强视频大型语言模型(Video LLMs)处理时空参考和推理的能力，使AI伴侣能更好地理解动态真实世界环境中的空间和时间关系。&lt;h4&gt;背景&lt;/h4&gt;下一代AI伴侣需要超越一般视频理解，解决动态真实世界环境中的时空参考问题。现有Video LLMs能进行粗粒度理解，但在细粒度时空推理方面存在困难，特别是当查询依赖时间锚定或手势提示进行空间锚定时。&lt;h4&gt;目的&lt;/h4&gt;开发Strefer框架，为Video LLMs配备时空参考和推理能力，弥补现有模型在处理时空关系方面的不足。&lt;h4&gt;方法&lt;/h4&gt;Strefer使用数据引擎生成多样化指令调整数据，通过伪注释时间密集的细粒度视频元数据，结构化捕获空间和时间信息，包括主体、对象、位置(masklet)及动作描述和时间线。&lt;h4&gt;主要发现&lt;/h4&gt;使用Strefer生成数据训练的模型在需要空间和时间消歧的任务上优于基线模型，并表现出增强的时空感知推理能力，为感知基础、指令调整的Video LLMs建立了新基础。&lt;h4&gt;结论&lt;/h4&gt;Strefer框架能有效增强Video LLMs解释空间和时间参考的能力，促进更通用、时空感知的推理，这对真实世界AI伴侣至关重要，且无需专有模型、昂贵人工注释或大量新视频注释。&lt;h4&gt;翻译&lt;/h4&gt;下一代AI伴侣必须超越一般视频理解，以解决动态、真实世界环境中的时空参考问题。现有的视频大型语言模型(Video LLMs)虽然能够进行粗粒度理解，但在细粒度时空推理方面存在困难，特别是当用户查询依赖于基于时间的事件参考进行时间锚定，或依赖于手势提示进行空间锚定以明确对象引用和位置时。为了解决这一关键差距，我们引入了Strefer，一个合成指令数据生成框架，旨在为Video LLMs配备时空参考和推理能力。Strefer使用数据引擎生成多样化的指令调整数据，该引擎伪注释时间密集的、细粒度的视频元数据，以结构化方式捕获丰富的空间和时间信息，包括主体、对象、它们的位置（作为masklet）以及它们的动作描述和时间线。我们的方法增强了Video LLMs解释空间和时间参考的能力，促进更通用、时空感知的推理，这对真实世界的AI伴侣至关重要。在不使用专有模型、昂贵的人工注释或大量新视频注释的情况下，实验评估表明，使用Strefer生成的数据训练的模型在需要空间和时间消歧的任务上优于基线模型。此外，这些模型表现出增强的时空感知推理能力，为感知基础、指令调整的Video LLMs建立了新的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation AI companions must go beyond general video understanding toresolve spatial and temporal references in dynamic, real-world environments.Existing Video Large Language Models (Video LLMs), while capable ofcoarse-level comprehension, struggle with fine-grained, spatiotemporalreasoning, especially when user queries rely on time-based event references fortemporal anchoring, or gestural cues for spatial anchoring to clarify objectreferences and positions. To bridge this critical gap, we introduce Strefer, asynthetic instruction data generation framework designed to equip Video LLMswith spatiotemporal referring and reasoning capabilities. Strefer producesdiverse instruction-tuning data using a data engine that pseudo-annotatestemporally dense, fine-grained video metadata, capturing rich spatial andtemporal information in a structured manner, including subjects, objects, theirlocations as masklets, and their action descriptions and timelines. Ourapproach enhances the ability of Video LLMs to interpret spatial and temporalreferences, fostering more versatile, space-time-aware reasoning essential forreal-world AI companions. Without using proprietary models, costly humanannotation, or the need to annotate large volumes of new videos, experimentalevaluations show that models trained with data produced by Strefer outperformbaselines on tasks requiring spatial and temporal disambiguation. Additionally,these models exhibit enhanced space-time-aware reasoning, establishing a newfoundation for perceptually grounded, instruction-tuned Video LLMs.</description>
      <author>example@mail.com (Honglu Zhou, Xiangyu Peng, Shrikant Kendre, Michael S. Ryoo, Silvio Savarese, Caiming Xiong, Juan Carlos Niebles)</author>
      <guid isPermaLink="false">2509.03501v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Time-Scaling State-Space Models for Dense Video Captioning</title>
      <link>http://arxiv.org/abs/2509.03426v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的状态空间模型方法，用于解决密集视频描述任务中处理长视频的挑战，实现了高效的在线/流式处理能力。&lt;h4&gt;背景&lt;/h4&gt;密集视频描述是一个需要同时分割视频为有意义事件序列并生成描述的挑战性任务。现有方法在处理长视频时面临计算复杂性和内存限制问题，且传统方法需要完整视频输入，无法实现在线处理。&lt;h4&gt;目的&lt;/h4&gt;解决密集视频描述中处理长视频的挑战，实现视频的在线或流式处理，无需等待整个视频处理完成。&lt;h4&gt;方法&lt;/h4&gt;提出'具有传输状态的状态空间模型'，结合状态空间模型的长序列和循环特性，解决了状态空间模型无法在长上下文中维持状态的问题，有效扩展了模型的时间处理能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提模型适合在线/流式方式即时生成描述，无需等待完整视频处理。应用于密集视频描述时，该方法能很好地适应不同视频长度，计算效率提高，使用少7倍的浮点运算次数。&lt;h4&gt;结论&lt;/h4&gt;通过改进状态空间模型，成功解决了密集视频描述中处理长视频的挑战，实现了高效在线处理，并在计算效率上取得显著提升。&lt;h4&gt;翻译&lt;/h4&gt;密集视频描述是一个具有挑战性的视频理解任务，旨在同时将视频分割为有意义的连续事件序列，并为每个事件生成详细准确的描述。由于计算复杂性和内存限制，现有方法在处理与密集视频描述相关的长视频时常常遇到困难。此外，传统方法需要整个视频作为输入才能产生输出，这阻碍了视频的在线处理。我们通过时间扩展状态空间模型来应对这些挑战，使其能够处理比以往更长的序列。我们的方法'具有传输状态的状态空间模型'结合了状态空间模型的长序列和循环特性，并解决了状态空间模型的主要限制，即无法在非常长的上下文中维持状态，有效地进一步扩展了状态空间模型的时间范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense video captioning is a challenging video understanding task which aimsto simultaneously segment the video into a sequence of meaningful consecutiveevents and to generate detailed captions to accurately describe each event.Existing methods often encounter difficulties when working with the long videosassociated with dense video captioning, due to the computational complexity andmemory limitations. Furthermore, traditional approaches require the entirevideo as input, in order to produce an answer, which precludes onlineprocessing of the video. We address these challenges by time-scalingState-Space Models (SSMs) to even longer sequences than before. Our approach,State-Space Models with Transfer State, combines both the long-sequence andrecurrent properties of SSMs and addresses the main limitation of SSMs whichare otherwise not able to sustain their state for very long contexts,effectively scaling SSMs further in time. The proposed model is particularlysuitable for generating captions on-the-fly, in an online or streaming manner,without having to wait for the full video to be processed, which is morebeneficial in practice. When applied to dense video captioning, our approachscales well with video lengths and uses 7x fewer FLOPs.</description>
      <author>example@mail.com (AJ Piergiovanni, Ganesh Satish Mallya, Dahun Kim, Anelia Angelova)</author>
      <guid isPermaLink="false">2509.03426v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>HyPV-LEAD: Proactive Early-Warning of Cryptocurrency Anomalies through Data-Driven Structural-Temporal Modeling</title>
      <link>http://arxiv.org/abs/2509.03260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出HyPV-LEAD框架，一种数据驱动的加密货币异常交易早期预警系统，通过整合窗口-地平线建模、峰值-谷值采样和双曲嵌入三种创新技术，显著提升了异常检测性能。&lt;h4&gt;背景&lt;/h4&gt;异常加密货币交易（如混币服务、欺诈转账、拉高出货操作）对金融完整性构成日益增长的风险，但由于类别不平衡、时间波动性和复杂网络依赖性，这些异常交易难以检测。现有方法主要是事后的，只在异常发生后标记，预防价值有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种数据驱动的早期预警框架，明确将提前时间纳入异常检测，实现从反应式分类到主动式早期预警的转变。&lt;h4&gt;方法&lt;/h4&gt;HyPV-LEAD框架整合三项创新：(1)窗口-地平线建模确保可操作的提前时间警报；(2)峰值-谷值采样减轻类别不平衡同时保持时间连续性；(3)双曲嵌入捕获区块链交易网络的层次化和无标度特性。&lt;h4&gt;主要发现&lt;/h4&gt;在比特币交易数据上的实证评估显示，HyPV-LEAD实现了0.9624的PR-AUC，精确度和召回率显著优于最先进基线。消融研究证实每个组件提供互补益处，完整框架性能最佳。&lt;h4&gt;结论&lt;/h4&gt;HyPV-LEAD通过将异常检测从反应式转变为主动式早期预警，为实时风险管理、反洗钱合规和动态区块链环境中的金融安全提供了坚实基础。&lt;h4&gt;翻译&lt;/h4&gt;异常加密货币交易 - 如混币服务、欺诈转账和拉高出货操作 - 对金融完整性构成日益增长的风险，但由于类别不平衡、时间波动性和复杂的网络依赖性，这些交易仍然难以检测。现有方法主要是以模型为中心和事后的，只在异常发生后标记，因此提供的预防价值有限。本文引入HyPV-LEAD（双曲峰谷提前时间启用的异常检测），一种数据驱动的早期预警框架，明确将提前时间纳入异常检测。与先前方法不同，HyPV-LEAD整合了三项创新：(1)窗口-地平线建模确保可操作的提前时间警报，(2)峰谷采样减轻类别不平衡同时保持时间连续性，(3)双曲嵌入捕获区块链交易网络的层次化和无标度特性。在比特币交易数据上的实证评估表明，HyPV-LEAD持续优于最先进的基线方法，实现了0.9624的PR-AUC，精确度和召回率有显著提升。消融研究进一步证实每个组件 - PV采样、双曲嵌入和结构-时间建模 - 提供互补的益处，完整框架实现最高性能。通过将异常检测从反应式分类转变为主动式早期预警，HyPV-LEAD为动态区块链环境中的实时风险管理、反洗钱合规和金融安全建立了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Abnormal cryptocurrency transactions - such as mixing services, fraudulenttransfers, and pump-and-dump operations -- pose escalating risks to financialintegrity but remain notoriously difficult to detect due to class imbalance,temporal volatility, and complex network dependencies. Existing approaches arepredominantly model-centric and post hoc, flagging anomalies only after theyoccur and thus offering limited preventive value. This paper introducesHyPV-LEAD (Hyperbolic Peak-Valley Lead-time Enabled Anomaly Detection), adata-driven early-warning framework that explicitly incorporates lead time intoanomaly detection. Unlike prior methods, HyPV-LEAD integrates threeinnovations: (1) window-horizon modeling to guarantee actionable lead-timealerts, (2) Peak-Valley (PV) sampling to mitigate class imbalance whilepreserving temporal continuity, and (3) hyperbolic embedding to capture thehierarchical and scale-free properties of blockchain transaction networks.Empirical evaluation on large-scale Bitcoin transaction data demonstrates thatHyPV-LEAD consistently outperforms state-of-the-art baselines, achieving aPR-AUC of 0.9624 with significant gains in precision and recall. Ablationstudies further confirm that each component - PV sampling, hyperbolicembedding, and structural-temporal modeling - provides complementary benefits,with the full framework delivering the highest performance. By shifting anomalydetection from reactive classification to proactive early-warning, HyPV-LEADestablishes a robust foundation for real-time risk management, anti-moneylaundering (AML) compliance, and financial security in dynamic blockchainenvironments.</description>
      <author>example@mail.com (Minjung Park, Gyuyeon Na, Soyoun Kim, Sunyoung Moon, HyeonJeong Cha, Sangmi Chai)</author>
      <guid isPermaLink="false">2509.03260v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Lattice Annotated Temporal (LAT) Logic for Non-Markovian Reasoning</title>
      <link>http://arxiv.org/abs/2509.02958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了格注释时态逻辑(LAT Logic)，这是广义注释逻辑程序(GAPs)的扩展，结合了时间推理并通过使用下格结构支持开放世界语义。该逻辑将高效的演绎过程与时态逻辑编程相结合，支持非马尔可夫关系和开放世界推理能力。&lt;h4&gt;背景&lt;/h4&gt;在逻辑编程和知识表示领域，需要处理开放世界语义和时间推理的挑战，特别是在具有无限或高度多样化常量的领域中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合高效演绎过程与时态逻辑编程的逻辑系统，以支持非马尔可夫关系和开放世界推理能力。&lt;h4&gt;方法&lt;/h4&gt;通过使用下格注释结构实现开放世界方面，并通过Skolemization过程实现高效grounding，即使在具有无限或高度多样化常量的领域中也是如此。&lt;h4&gt;主要发现&lt;/h4&gt;1) 提供了限制grounding过程计算复杂度的理论结果；2) 证明许多关于GAPs(使用上格)的结果在下格和时间扩展下仍然成立；3) 开源实现PyReason具有模块化设计、机器级优化和与强化学习环境的直接集成；4) 在多智能体模拟和知识图任务上显示速度提高最多三个数量级，内存减少最多五个数量级；5) 在强化学习环境中作为非马尔可夫模拟器，模拟速度提高最多三个数量级，智能体性能提高26%。&lt;h4&gt;结论&lt;/h4&gt;LAT Logic有潜力成为动态和不确定环境中开放世界时间推理的统一、可扩展框架。&lt;h4&gt;翻译&lt;/h4&gt;我们引入格注释时态(LAT)逻辑，这是广义注释逻辑程序(GAPs)的扩展，它结合了时间推理并通过使用下格结构支持开放世界语义。该逻辑将高效的演绎过程与时态逻辑编程相结合，以支持非马尔可夫关系和开放世界推理能力。开放世界方面是使用下格注释结构的结果，允许通过Skolemization过程实现高效grounding，即使在具有无限或高度多样化常量的领域中也是如此。我们提供了一套理论结果，限制了grounding过程的计算复杂度，此外还表明许多关于GAPs(使用上格)的结果在下格和时间扩展下仍然成立(尽管需要不同的证明技术)。我们的开源实现PyReason具有模块化设计、机器级优化和与强化学习环境的直接集成。在多智能体模拟和知识图任务上的经验评估显示速度提高最多三个数量级，内存减少最多五个数量级，同时保持或提高任务性能。此外，我们评估了LAT Logic在强化学习环境中作为非马尔可夫模拟器的价值，实现了最多三个数量级的更快速模拟，并提高了智能体性能，包括通过捕获更丰富的时间依赖性提高26%的胜率。这些结果突显了LAT Logic作为动态和不确定环境中开放世界时间推理的统一、可扩展框架的潜力。我们的实现可在pyreason.syracuse.edu获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Lattice Annotated Temporal (LAT) Logic, an extension ofGeneralized Annotated Logic Programs (GAPs) that incorporates temporalreasoning and supports open-world semantics through the use of a lower latticestructure. This logic combines an efficient deduction process with temporallogic programming to support non-Markovian relationships and open-worldreasoning capabilities. The open-world aspect, a by-product of the use of thelower-lattice annotation structure, allows for efficient grounding through aSkolemization process, even in domains with infinite or highly diverseconstants.  We provide a suite of theoretical results that bound the computationalcomplexity of the grounding process, in addition to showing that many of theresults on GAPs (using an upper lattice) still hold with the lower lattice andtemporal extensions (though different proof techniques are required). Ouropen-source implementation, PyReason, features modular design, machine-leveloptimizations, and direct integration with reinforcement learning environments.Empirical evaluations across multi-agent simulations and knowledge graph tasksdemonstrate up to three orders of magnitude speedup and up to five orders ofmagnitude memory reduction while maintaining or improving task performance.Additionally, we evaluate LAT Logic's value in reinforcement learningenvironments as a non-Markovian simulator, achieving up to three orders ofmagnitude faster simulation with improved agent performance, including a 26%increase in win rate due to capturing richer temporal dependencies. Theseresults highlight LAT Logic's potential as a unified, extensible framework foropen-world temporal reasoning in dynamic and uncertain environments. Ourimplementation is available at: pyreason.syracuse.edu.</description>
      <author>example@mail.com (Kaustuv Mukherji, Jaikrishna Manojkumar Patil, Dyuman Aditya, Paulo Shakarian, Devendra Parkar, Lahari Pokala, Clark Dorman, Gerardo I. Simari)</author>
      <guid isPermaLink="false">2509.02958v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PixFoundation 2.0: Do Video Multi-Modal LLMs Use Motion in Visual Grounding?</title>
      <link>http://arxiv.org/abs/2509.02807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work under review in NeurIPS 2025 with the title "Are we using Motion  in Referring Segmentation? A Motion-Centric Evaluation"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多模态大语言模型(MLLMs)在视频领域的像素级视觉定位能力，特别是它们如何利用运动信息进行物体分割。作者提出了以运动为中心的基准测试和评估方法，挑战现有模型改进密集时空定位能力。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在图像和文本任务中表现出色，扩展到视频领域后支持了视频问答和字幕生成等任务，但其像素级视觉定位能力研究不足。&lt;h4&gt;目的&lt;/h4&gt;研究运动是否被用于像素级视觉定位，以及视频MLLMs是否能根据描述运动模式的自然语言表达式分割物体；解决当前基准测试中单帧图像即可完成任务无需时间推理的不足。&lt;h4&gt;方法&lt;/h4&gt;引入四种以运动为中心的探测技术；创建MoCentric-Bench基准测试；建立强大的单图像基线；探索简单的以运动为中心的适应技术。&lt;h4&gt;主要发现&lt;/h4&gt;当前基准测试存在不足，单帧通常足以捕捉运动指代表达；视频MLLMs在识别真实运动和把握运动顺序方面能力待评估；以运动为中心的基准测试能确保模型评估运动与语言交互而非静态外观线索。&lt;h4&gt;结论&lt;/h4&gt;以运动为中心的基准测试、评估和发现挑战未来模型改进密集时空定位和视频中的像素级理解；代码和数据集将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)在使用图像和文本模态的任务中展现出令人印象深刻的泛化能力。尽管它们扩展到视频领域已 enable了视频问答和视频字幕生成等任务，但其像素级视觉定位能力研究较少。在这项工作中，我们提出了一个相关问题：运动是否被用于像素级视觉定位，以及视频MLLMs是否能根据描述运动模式的自然语言表达式分割物体。我们确定了当前基准测试的不足之处，并表明单帧图像通常足以捕捉运动指代表达式，无需任何时间推理。为解决这一问题，我们引入了四种以运动为中心的探测技术，专为视觉定位任务设计，以研究视频MLLMs识别真实运动与假运动的能力以及把握运动顺序的能力。因此，我们提供了一个以运动为中心的基准测试MoCentric-Bench。它确保视频MLLMs被评估为利用运动和语言之间的交互，而不是被现有视觉定位数据集中强调的静态外观线索所主导。我们进一步建立了强大的单图像基线，其性能与或优于之前的方法。最后，我们探索了简单的以运动为中心的适应技术，在我们的MoCentric-Bench上提供了最先进的性能。我们的以运动为中心的基准测试、评估和发现挑战未来的模型改进密集时空定位和视频中的像素级理解。代码和数据集将在https://github.com/MSiam/PixFoundation-2.0.git公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal large language models (MLLMs) have shown impressivegeneralization across tasks using images and text modalities. While theirextension to video has enabled tasks such as video question answering and videocaptioning, their pixel-level visual grounding abilities are less studied. Inthis work, we raise the pertinent question of whether motion is used inpixel-level visual grounding and whether video MLLMs can segment objects basedon natural language expressions describing their motion patterns. We identifythe shortcomings in the current benchmarks, where we show that a single framecan often suffice for capturing the motion referring expression without anytemporal reasoning. To address this, we introduce four motion-centric probingtechniques, particularly designed for the visual grounding task, to study videoMLLMs' ability to identify true motion from a fake one and their ability tograsp the motion order. Consequently, we provide a motion-centric benchmark,MoCentric-Bench. It ensures that video MLLMs are evaluated towards leveragingthe interaction between motion and language rather than being dominated bystatic appearance cues emphasized in existing visual grounding datasets. Wefurther establish strong single-image baselines that are on par with oroutperform prior methods. Finally, we explore simple motion-centric adaptationtechniques that provide state-of-the-art performance on our MoCentric-Bench.Our motion-centric benchmark, evaluation and findings challenge future modelsto improve dense spatiotemporal grounding and pixel-level understanding withinvideos. Code and datasets will be made publicly available athttps://github.com/MSiam/PixFoundation-2.0.git.</description>
      <author>example@mail.com (Mennatullah Siam)</author>
      <guid isPermaLink="false">2509.02807v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>See No Evil: Adversarial Attacks Against Linguistic-Visual Association in Referring Multi-Object Tracking Systems</title>
      <link>http://arxiv.org/abs/2509.02028v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 1 figure, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了引用多对象跟踪(RMOT)系统的安全性和鲁棒性问题，发现了对抗性漏洞并提出了一种名为VEIL的新型对抗框架来破坏RMOT模型的可靠性。&lt;h4&gt;背景&lt;/h4&gt;语言视觉理解推动了高级感知系统的发展，特别是引用多对象跟踪(RMOT)范式的出现。RMOT系统利用自然语言查询选择性跟踪满足语义描述的对象，通过基于Transformer的时空推理模块引导。端到端RMOT模型将特征提取、时间记忆和空间推理统一在Transformer主干网络中，实现长程时空建模。&lt;h4&gt;目的&lt;/h4&gt;从设计逻辑角度研究RMOT系统的安全含义，识别对抗性漏洞，提出保护RMOT系统可靠性的方法。&lt;h4&gt;方法&lt;/h4&gt;提出VEIL，一种旨在破坏RMOT模型统一引用-匹配机制的新型对抗框架。通过精心设计的数字和物理扰动来破坏跟踪逻辑的可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;1) RMOT系统存在对抗性漏洞，损害语言视觉引用和跟踪对象匹配组件；2) 基于FIFO记忆的高级RMOT模型中存在新型漏洞，针对时空推理的攻击会在历史缓冲区中引入持续多个帧的错误；3) VEIL框架可以有效导致跟踪ID切换和终止。&lt;h4&gt;结论&lt;/h4&gt;RMOT系统的可靠性和鲁棒性需要更多关注，关键的大规模应用需要安全感知的RMOT设计。&lt;h4&gt;翻译&lt;/h4&gt;语言视觉理解推动了高级感知系统的发展，特别是新兴的引用多对象跟踪(RMOT)范式。通过利用自然语言查询，RMOT系统可以选择性地跟踪满足给定语义描述的对象，通过基于Transformer的时空推理模块进行引导。端到端(E2E)RMOT模型进一步将特征提取、时间记忆和空间推理统一在一个Transformer主干网络中，实现了对融合的文本-视觉表示的长程时空建模。尽管取得了这些进展，RMOT的可靠性和鲁棒性仍然未被充分探索。在本文中，我们从设计逻辑的角度研究了RMOT系统的安全含义，确定了会损害语言视觉引用和跟踪对象匹配组件的对抗性漏洞。此外，我们还发现了采用基于FIFO记忆的高级RMOT模型中的一种新型漏洞，即对其时空推理的有针对性的持续攻击会在历史缓冲区中引入持续多个帧的错误。我们提出了VEIL，一种旨在破坏RMOT模型统一引用-匹配机制的新型对抗框架。我们展示了精心设计的数字和物理扰动可以破坏跟踪逻辑的可靠性，导致跟踪ID切换和终止。我们使用Refer-KITTI数据集进行了全面评估，验证了VEIL的有效性，并证明了关键的大规模应用需要安全感知的RMOT设计。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Language-vision understanding has driven the development of advancedperception systems, most notably the emerging paradigm of ReferringMulti-Object Tracking (RMOT). By leveraging natural-language queries, RMOTsystems can selectively track objects that satisfy a given semanticdescription, guided through Transformer-based spatial-temporal reasoningmodules. End-to-End (E2E) RMOT models further unify feature extraction,temporal memory, and spatial reasoning within a Transformer backbone, enablinglong-range spatial-temporal modeling over fused textual-visual representations.Despite these advances, the reliability and robustness of RMOT remainunderexplored. In this paper, we examine the security implications of RMOTsystems from a design-logic perspective, identifying adversarialvulnerabilities that compromise both the linguistic-visual referring andtrack-object matching components. Additionally, we uncover a novelvulnerability in advanced RMOT models employing FIFO-based memory, wherebytargeted and consistent attacks on their spatial-temporal reasoning introduceerrors that persist within the history buffer over multiple subsequent frames.We present VEIL, a novel adversarial framework designed to disrupt the unifiedreferring-matching mechanisms of RMOT models. We show that carefully crafteddigital and physical perturbations can corrupt the tracking logic reliability,inducing track ID switches and terminations. We conduct comprehensiveevaluations using the Refer-KITTI dataset to validate the effectiveness of VEILand demonstrate the urgent need for security-aware RMOT designs for criticallarge-scale applications.</description>
      <author>example@mail.com (Halima Bouzidi, Haoyu Liu, Mohammad Abdullah Al Faruque)</author>
      <guid isPermaLink="false">2509.02028v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Spatio-Temporal Sequential Ordinal Models: Application to Invasive Weeds</title>
      <link>http://arxiv.org/abs/2509.01976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究多元序贯序数模型在贝叶斯分析时空序数数据中的应用及其优势&lt;h4&gt;背景&lt;/h4&gt;时空序数数据分析面临估计复杂性的挑战，特别是高维应用中累积序数模型的阈值参数排序约束问题&lt;h4&gt;目的&lt;/h4&gt;开发一种简化高维时空序数数据估计和预测的模型，避免累积序数模型的排序约束&lt;h4&gt;方法&lt;/h4&gt;使用多元序贯序数模型，将其似然表达为条件于未知回归系数和时空随机效应的二值模型，结合动态广义线性模型框架和INLA方法进行估计和预测&lt;h4&gt;主要发现&lt;/h4&gt;四种入侵杂草物种对栖息地类型、控制努力和可达性的响应不同，且具有相似的有效空间范围短和强时间自相关的依赖性&lt;h4&gt;结论&lt;/h4&gt;序贯序数模型简化了高维时空序数数据的贝叶斯分析，同时保持了参数的可解释性，适用于空间稀疏分布和时间记录较少的序数数据&lt;h4&gt;翻译&lt;/h4&gt;本研究调查了多元序贯序数模型在贝叶斯分析时空序数数据中的应用。序贯序数模型似然等同于条件于未知回归系数和时空随机效应的二值模型。因此，时空背景下的估计和预测可以使用成熟的动态广义线性模型框架进行。此外，序贯序数模型避免了决定类别断点的累积序数模型所需的阈值参数排序约束，从而简化了使用贝叶斯推断进行高维时空应用的估计过程。动态时空序贯序数模型被应用于估计四种积极管理的入侵外来物种的叶覆盖丰度。这些入侵杂草物种通过植被研究中常用的修改Braun-Blanquet评分进行观察，构成序数数据。管理杂草物种的多变量序数数据在时空上稀疏分布，高叶覆盖类别的观测记录较少。因此，开发了针对时空依赖的可分离模型，在存在聚合序数类别的情况下保持参数可解释性。使用为单变量时空模型开发的集成嵌套拉普拉斯近似(INLA)方法进行估计和预测。贝叶斯估计和预测表明，四种入侵杂草物种对栖息地类型、控制努力和可达性的响应不同，且具有相似的有效空间范围短和强时间自相关的依赖性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multivariate sequential ordinal model is investigated for use in theBayesian analysis of spatio-temporal ordinal data. The sequential ordinal modellikelihood is equivalent to a binary model conditional on unknown regressioncoefficients and spatio-temporal random effects. Therefore, estimation andprediction in the space-time context can proceed using the well-establisheddynamic generalised linear model framework. Moreover, the sequential ordinalmodel avoids the ordering constraints on the threshold parameters thatdetermine the category break points required by cumulative ordinal models, andso simplifies the estimation procedure for high-dimensional space-timeapplications using Bayesian inference. The dynamic spatio-temporal sequentialordinal model is applied to estimate foliage cover abundance of four activelymanaged invasive alien species. These invasive weed species are observed bymeans of a modified Braun-Blanquet score that is commonly used in vegetationstudies and constitutes ordinal data. The multivariate ordinal data for themanaged weeds species are sparsely distributed in space and time with fewobservations recorded in high foliage cover categories. A separable model forspace-time dependence that maintains parameter interpretability in the presenceof aggregated ordinal categories is therefore developed. Estimation andprediction is demonstrated using integrated nested Laplace approximation (INLA)methods developed for univariate spatio-temporal models. Bayesian estimationand prediction shows that the four invasive weed species differentially respondto habitat type, control effort and accessibility, and share similar magnitudesof dependence with short effective spatial ranges and strong temporalautocorrelations.</description>
      <author>example@mail.com (Geoffrey R. Hosack, Wen-Hsi Yang, Kyana N. Pike, Luke S. O'Loughlin, Emma Cook, Brett Howland, Emily Sutcliffe, Richard N. C. Milner, Ben Gooden, Jens G. Froese)</author>
      <guid isPermaLink="false">2509.01976v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Time-Series Model for Areal Data Using Spatially Correlated Gaussian Processes</title>
      <link>http://arxiv.org/abs/2509.01604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的层次建模框架，通过高斯过程和相关方差分量共享空间信息来捕捉区域数据中的时间趋势，提供更灵活的时空过程表示。&lt;h4&gt;背景&lt;/h4&gt;传统的时空模型通常先在随机效应层面施加空间结构，然后再扩展到包含时间动态，存在一定的局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种替代的层次建模框架，通过共享空间信息的相关方差分量，用高斯过程捕捉区域数据中的时间趋势。&lt;h4&gt;方法&lt;/h4&gt;将独立高斯过程模型扩展到空间相关框架，对控制时间变异性的参数施加条件自回归先验，并对时间范围施加时间方差的条件依赖；在贝叶斯框架内使用近似后验采样，结合马尔可夫链蒙特卡洛技术(MALA、Metropolis-Hastings和Gibbs采样)进行推断。&lt;h4&gt;主要发现&lt;/h4&gt;在莫桑比克Niassa的月疟疾发病率和喀麦隆的周粮食不安全流行率两个案例研究中，模型表现出强大的样本内性能，置信区间狭窄，在预测时在许多区域优于已建立的时空方法，同时保持可解释性。&lt;h4&gt;结论&lt;/h4&gt;通过考虑时间动态本身的演变来解释时空变化，该方法为许多应用背景提供了一种灵活且合理的工具，特别适用于数据稀少且具有政策相关性的环境。&lt;h4&gt;翻译&lt;/h4&gt;传统的区域数据时空模型通常从在随机效应层面施加空间结构开始，然后扩展到包含时间动态。我们提出了一种替代的层次建模框架，通过共享空间信息的相关方差分量，用高斯过程捕捉区域数据中的时间趋势。这使得模型能够更好地捕捉区域间共享的变异性模式，同时保留局部时间动态，为时空过程提供了更灵活的表示。具体来说，我们通过对控制时间变异性的参数施加条件自回归先验，并对时间范围施加时间方差的条件依赖，将独立的高斯过程时间序列模型扩展到空间相关框架。我们将该模型应用于两个案例研究：莫桑比克Niassa的月疟疾发病率和喀麦隆的周粮食不安全流行率。在贝叶斯框架内使用近似后验采样进行推断。鉴于模型的层次结构，我们采用马尔可夫链蒙特卡洛技术的组合，包括Metropolis调整的Langevin算法、Metropolis-Hastings和Gibbs采样。在两个应用中，该模型都显示出强大的样本内性能，置信区间狭窄，并且在预测时在许多区域优于已建立的时空方法。这些结果强调了模型在保持可解释性的同时捕捉复杂时空依赖关系的能力，这在数据稀少且具有政策相关性的环境中至关重要。通过考虑时间动态本身的演变来解释时空变化，我们的方法为许多应用背景提供了一种灵活且合理的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional spatio-temporal models for areal data typically begin withspatial structure imposed at the level of random effects and later extend toinclude temporal dynamics. We propose an alternative hierarchical modelingframework that captures temporal trends in areal data through Gaussianprocesses that share spatial information via correlated variance components.This allows the model to better capture shared patterns of variability acrossregions while preserving local temporal dynamics, offering a more flexiblerepresentation of spatio-temporal processes.  Specifically, we extend independent Gaussian-process models for time-seriesdata to a spatially correlated framework by placing a conditionalautoregressive (CAR) prior on the parameters governing the temporal variabilityand imposing a conditional dependence of the temporal range on the temporalvariance. We apply this model to two case studies: monthly malaria incidence inNiassa, Mozambique, and weekly food insecurity prevalence in Cameroon.Inference is conducted within a Bayesian framework using approximate posteriorsampling. Given the hierarchical structure of the model, we employ acombination of Markov chain Monte Carlo (MCMC) techniques, including theMetropolis-adjusted Langevin algorithm (MALA), Metropolis-Hastings, and Gibbssampling.  In both applications, the model demonstrates strong in-sample performancewith narrow credible intervals and outperforms established spatio-temporalapproaches in many regions when forecasting. These results underscore themodel's ability to capture complex spatio-temporal dependencies whilemaintaining interpretability, key in settings with sparse data and policyrelevance. By accounting for spatio-temporal variation through the evolution oftemporal dynamics themselves, our approach offers a flexible and principledtool for many applied contexts.</description>
      <author>example@mail.com (Alejandro Rozo Posada, Oswaldo Gressani, Christel Faes, James Colborn, Baltazar Candrinho, Emanuele Giorgi, Thomas Neyens)</author>
      <guid isPermaLink="false">2509.01604v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Kwai Keye-VL 1.5 Technical Report</title>
      <link>http://arxiv.org/abs/2509.01563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github page: https://github.com/Kwai-Keye/Keye&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Keye-VL-1.5模型，通过三项创新解决了视频理解中的基本挑战，在视频理解任务上表现出色，同时保持通用多模态任务的竞争性能。&lt;h4&gt;背景&lt;/h4&gt;大语言模型近年来发展迅速并扩展到多模态任务，但视频理解仍具挑战性，因为视频具有动态和信息密集特性。现有模型在处理视频内容时难以平衡空间分辨率和时间覆盖范围。&lt;h4&gt;目的&lt;/h4&gt;开发Keye-VL-1.5模型，解决视频理解中的基本挑战，提高模型在视频理解和多模态任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 引入慢-快视频编码策略，根据帧间相似性动态分配计算资源，关键帧以高分辨率处理，静态帧以低分辨率但增加时间覆盖范围处理；2. 实现渐进式四阶段预训练方法，将上下文长度从8K扩展到128K tokens；3. 开发全面微调管道，包含思维链数据构建、GSPO强化学习和对齐训练。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准测试和内部人工评估中，Keye-VL-1.5比现有模型有显著改进，尤其在视频理解任务上表现突出，同时在通用多模态基准测试上保持竞争性能。&lt;h4&gt;结论&lt;/h4&gt;Keye-VL-1.5通过三项关键创新有效解决了视频理解的基本挑战，在视频理解和通用多模态任务上都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;近年来，大语言模型的发展取得了显著进展，通过多模态大语言模型将其能力扩展到多模态任务。然而，由于视频的动态和信息密集特性，视频理解仍然是一个具有挑战性的领域。现有模型在处理视频内容时，在空间分辨率和时间覆盖范围之间难以权衡。我们提出了Keye-VL-1.5，通过三项关键创新解决了视频理解中的基本挑战。首先，我们引入了一种新颖的慢-快视频编码策略，根据帧间相似性动态分配计算资源，对有显著视觉变化的关键帧以更高分辨率处理，同时对相对静态的帧以更低分辨率但增加时间覆盖范围处理。其次，我们实现了一个渐进式四阶段预训练方法，系统地将模型的上下文长度从8K扩展到128K tokens，使模型能够处理更长的视频和更复杂的视觉内容。第三，我们开发了一个全面的微调管道，专注于推理增强和人类偏好对齐，包含5步思维链数据构建过程、基于GSPO的迭代强化学习以及对齐训练。通过在公共基准测试上的广泛评估和严格的内部人工评估，Keye-VL-1.5比现有模型显示出显著改进，尤其在视频理解任务上表现出色，同时在通用多模态基准测试上保持竞争性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the development of Large Language Models (LLMs) hassignificantly advanced, extending their capabilities to multimodal tasksthrough Multimodal Large Language Models (MLLMs). However, video understandingremains a challenging area due to the dynamic and information-dense nature ofvideos. Existing models struggle with the trade-off between spatial resolutionand temporal coverage when processing video content. We present Keye-VL-1.5,which addresses fundamental challenges in video comprehension through three keyinnovations. First, we introduce a novel Slow-Fast video encoding strategy thatdynamically allocates computational resources based on inter-frame similarity,processing key frames with significant visual changes at higher resolution(Slow pathway) while handling relatively static frames with increased temporalcoverage at lower resolution (Fast pathway). Second, we implement a progressivefour-stage pre-training methodology that systematically extends the model'scontext length from 8K to 128K tokens, enabling processing of longer videos andmore complex visual content. Third, we develop a comprehensive post-trainingpipeline focusing on reasoning enhancement and human preference alignment,incorporating a 5-step chain-of-thought data construction process, iterativeGSPO-based reinforcement learning with progressive prompt hinting for difficultcases, and alignment training. Through extensive evaluation on publicbenchmarks and rigorous internal human assessment, Keye-VL-1.5 demonstratessignificant improvements over existing models, particularly excelling in videounderstanding tasks while maintaining competitive performance on generalmultimodal benchmarks.</description>
      <author>example@mail.com (Biao Yang, Bin Wen, Boyang Ding, Changyi Liu, Chenglong Chu, Chengru Song, Chongling Rao, Chuan Yi, Da Li, Dunju Zang, Fan Yang, Guorui Zhou, Guowang Zhang, Han Shen, Hao Peng, Haojie Ding, Hao Wang, Hengrui Ju, Jiaming Huang, Jiangxia Cao, Jiankang Chen, Jingyun Hua, Kaibing Chen, Kaiyu Jiang, Kaiyu Tang, Kun Gai, Muhao Wei, Qiang Wang, Ruitao Wang, Sen Na, Shengnan Zhang, Siyang Mao, Sui Huang, Tianke Zhang, Tingting Gao, Wei Chen, Wei Yuan, Xiangyu Wu, Xiao Hu, Xingyu Lu, Yi-Fan Zhang, Yiping Yang, Yulong Chen, Zeyi Lu, Zhenhua Wu, Zhixin Ling, Zhuoran Yang, Ziming Li, Di Xu, Haixuan Gao, Hang Li, Jing Wang, Lejian Ren, Qigen Hu, Qianqian Wang, Shiyao Wang, Xinchen Luo, Yan Li, Yuhang Hu, Zixing Zhang)</author>
      <guid isPermaLink="false">2509.01563v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Variation-aware Vision Token Dropping for Faster Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2509.01552v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: \url{https://github.com/xuyang-liu16/V2Drop}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为V²Drop的视觉token压缩方法，通过删除变化最小的视觉token来提高大型视觉-语言模型的计算效率，同时保持高性能。&lt;h4&gt;背景&lt;/h4&gt;大型视觉-语言模型在多模态理解任务中表现出色，但高分辨率图像和长视频理解导致大量token，降低了推理效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有LLM内部token压缩方法的位置偏差和与高效算子不兼容的问题，提高LVLM的计算效率。&lt;h4&gt;方法&lt;/h4&gt;从token变化角度提出Variation-aware Vision Token Dropping (V²Drop)，在LVLM推理过程中逐步删除变化最小的视觉token。&lt;h4&gt;主要发现&lt;/h4&gt;LLM中的视觉token变化具有任务无关的特性；V²Drop能够保持原始模型94.0%和98.6%的性能，分别用于图像和视频理解任务，同时将LLM生成延迟分别减少31.5%和74.2%。&lt;h4&gt;结论&lt;/h4&gt;V²Drop是一种有效的token压缩方法，可以在保持高性能的同时显著提高LVLM的计算效率，与高效算子结合时还能进一步减少GPU峰值内存使用。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉-语言模型(LVLMs)在多模态理解任务中表现出色。然而，对高分辨率图像和长视频理解的不断需求导致大量token的产生，从而降低了推理效率。Token压缩通过减少需要处理的token数量提供直接解决方案，从而提高计算效率。通过广泛分析，我们发现现有内部LLM token压缩方法存在两个关键限制：位置偏差和与高效算子不兼容，这阻碍了它们在LVLM加速中的实际部署。本文从token变化角度提出了第一种方法，揭示了LLM中视觉token变化具有任务无关的特性。我们提出了Variation-aware Vision Token Dropping（即V²Drop），在LVLM推理过程中逐步删除变化最小的视觉token，从而提高计算效率。在多个模型和基准上的广泛实验表明，我们的V²Drop能够保持原始模型94.0%和98.6%的性能，分别用于图像和视频理解任务，同时将LLM生成延迟减少31.5%和74.2%。当与高效算子结合时，V²Drop进一步减少了GPU峰值内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large vision-language models (LVLMs) have demonstrated remarkablecapabilities in multimodal understanding tasks. However, the increasing demandfor high-resolution image and long-video understanding results in substantialtoken counts, leading to reduced inference efficiency. Token compression offersa direct solution by reducing the number of tokens to be processed, therebyimproving computational efficiency. Through extensive analysis, we identify twocritical limitations in existing inner-LLM token compression methods:positional bias and incompatibility with efficient operators, which hindertheir practical deployment for LVLM acceleration. This paper presents the firstapproach from a token variation perspective, revealing that visual tokenvariations within LLMs exhibit task-agnostic properties. We proposeVariation-aware Vision Token Dropping (\textit{i.e.}, \textbf{V$^2$Drop}),which progressively removes visual tokens with minimal variation during LVLMinference, thereby enhancing computational efficiency. Extensive experimentsacross multiple models and benchmarks demonstrate that our V$^2$Drop is able tomaintain \textbf{94.0\%} and \textbf{98.6\%} of the original model performancefor image and video understanding tasks respectively, while reducing LLMgeneration latency by \textbf{31.5\%} and \textbf{74.2\%}. When combined withefficient operators, V$^2$Drop further reduces GPU peak memory usage.</description>
      <author>example@mail.com (Junjie Chen, Xuyang Liu, Zichen Wen, Yiyu Wang, Siteng Huang, Honggang Chen)</author>
      <guid isPermaLink="false">2509.01552v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Do Video Language Models Really Know Where to Look? Diagnosing Attention Failures in Video Language Models</title>
      <link>http://arxiv.org/abs/2509.01167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了多模态大语言模型在视频理解任务中的应用，特别关注了关键帧采样方法的局限性，并发现当前流行的视觉编码器在识别视频中哪些部分对文本查询最重要方面存在严重不足。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在视频理解任务中取得了显著进展。为避免处理所有帧的高计算成本，这些模型通常依赖关键帧采样方法，这些方法由视觉-语言编码器(如SigLIP)指导。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在评估当前流行的视觉编码器是否能真正识别出对处理给定文本查询最有信息量的视频帧，并探索改进视频MLLM效率的可能性。&lt;h4&gt;方法&lt;/h4&gt;作者提供了多项经验证据，评估了视觉编码器在识别MLLM应该查看视频中的哪些部分以适当处理文本查询方面的能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，流行的视觉编码器在识别MLLM应该查看视频中的哪些部分以适当处理给定文本查询方面存在严重局限性。&lt;h4&gt;结论&lt;/h4&gt;开发更好的关键帧识别技术可能是构建高效视频MLLMs的必要条件。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)的最新进展在视频理解任务中取得了很大进展。为了避免处理所有帧的沉重计算成本，这些模型通常依赖于由视觉-语言编码器(如SigLIP)指导的关键帧采样方法。然而，这些编码器是否能真正识别出最具信息量的帧仍然不清楚。在这项工作中，我们提供了几项经验证据，揭示流行的视觉编码器在识别MLLM应该查看视频中的哪些部分以适当处理给定文本查询方面存在严重局限性。我们的研究结果表明，开发更好的关键帧识别技术可能是高效视频MLLMs所必需的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal large language models (MLLMs) have led to muchprogress in video understanding tasks. To avoid the heavy computational cost ofprocessing all frames, these models typically rely on keyframe sampling methodsguided by vision-language encoders (\textit{e.g.,} SigLIP). However, it remainsunclear whether such encoders can truly identify the most informative frames.In this work, we provide several empirical pieces of evidence revealing thatpopular vision encoders critically suffer from their limited capability toidentify where the MLLM should look inside the video to handle the giventextual query appropriately. Our findings suggest that the development ofbetter keyframe identification techniques may be necessary for efficient videoMLLMs.</description>
      <author>example@mail.com (Hyunjong Ok, Jaeho Lee)</author>
      <guid isPermaLink="false">2509.01167v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>An End-to-End Framework for Video Multi-Person Pose Estimation</title>
      <link>http://arxiv.org/abs/2509.01095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为VEPE的视频端到端姿态估计框架，利用三个关键时空Transformer组件有效利用时间上下文优化人体姿态估计，并通过实例一致性机制减少跨帧匹配不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;基于视频的人体姿态估计模型旨在解决静态图像模型无法有效处理的问题，如运动模糊、失焦和遮挡。现有方法通常分为两个阶段：检测每帧图像中的人体实例和使用时间模型进行姿态估计，这种方法分离了空间和时间维度，无法捕获全局时空上下文进行端到端优化，且依赖单独检测器和复杂后处理，降低了推理效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频姿态估计方法的问题，提出一种简单且灵活的视频端到端姿态估计框架。&lt;h4&gt;方法&lt;/h4&gt;提出VEPE框架，利用三个关键时空Transformer组件：时空姿态编码器(STPE)、时空可变形记忆编码器(STDME)和时空姿态解码器(STPD)，有效利用时间上下文优化姿态估计；同时提出实例一致性机制，减少跨帧姿态查询匹配过程中的不匹配问题，增强跨帧实例查询的一致性和差异性，实现实例跟踪功能。&lt;h4&gt;主要发现&lt;/h4&gt;在PoseTrack数据集上的实验表明，该方法优于大多数两阶段模型，推理效率提高了300%。&lt;h4&gt;结论&lt;/h4&gt;VEPE框架是一种有效的视频端到端姿态估计方法，能够更好地处理视频中的姿态估计问题，并显著提高推理效率。&lt;h4&gt;翻译&lt;/h4&gt;基于视频的人体姿态估计模型旨在解决静态图像模型无法有效处理的问题，如运动模糊、失焦和遮挡。大多数现有方法包括两个阶段：检测每帧图像中的人体实例，然后使用时间模型进行单人姿态估计。这种方法分离了空间和时间维度，无法捕获空间实例之间的全局时空上下文进行端到端优化。此外，它依赖于单独的检测器和复杂的后处理，如感兴趣区域裁剪和非极大值抑制，降低了视频场景的推理效率。为解决上述问题，我们提出了VEPE（视频端到端姿态估计），一种用于视频中端到端姿态估计的简单灵活框架。该框架利用三个关键的时空Transformer组件：时空姿态编码器（STPE）、时空可变形记忆编码器（STDME）和时空姿态解码器（STPD）。这些组件被设计为有效利用时间上下文来优化人体姿态估计。此外，为了减少跨帧姿态查询匹配过程中的不匹配问题，我们提出了一种实例一致性机制，旨在增强跨帧实例查询的一致性和差异性，实现实例跟踪功能，从而准确指导姿态查询进行跨帧匹配。在PoseTrack数据集上的大量实验表明，我们的方法优于大多数两阶段模型，并将推理效率提高了300%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video-based human pose estimation models aim to address scenarios that cannotbe effectively solved by static image models such as motion blur, out-of-focusand occlusion. Most existing approaches consist of two stages: detecting humaninstances in each image frame and then using a temporal model for single-personpose estimation. This approach separates the spatial and temporal dimensionsand cannot capture the global spatio-temporal context between spatial instancesfor end-to-end optimization. In addition, it relies on separate detectors andcomplex post-processing such as RoI cropping and NMS, which reduces theinference efficiency of the video scene. To address the above problems, wepropose VEPE (Video End-to-End Pose Estimation), a simple and flexibleframework for end-to-end pose estimation in video. The framework utilizes threecrucial spatio-temporal Transformer components: the Spatio-Temporal PoseEncoder (STPE), the Spatio-Temporal Deformable Memory Encoder (STDME), and theSpatio-Temporal Pose Decoder (STPD). These components are designed toeffectively utilize temporal context for optimizing human body pose estimation.Furthermore, to reduce the mismatch problem during the cross-frame pose querymatching process, we propose an instance consistency mechanism, which aims toenhance the consistency and discrepancy of the cross-frame instance query andrealize the instance tracking function, which in turn accurately guides thepose query to perform cross-frame matching. Extensive experiments on thePosetrack dataset show that our approach outperforms most two-stage models andimproves inference efficiency by 300%.</description>
      <author>example@mail.com (Zhihong Wei)</author>
      <guid isPermaLink="false">2509.01095v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Seeing More, Saying More: Lightweight Language Experts are Dynamic Video Token Compressors</title>
      <link>http://arxiv.org/abs/2509.00969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages, 8 figures, EMNLP2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出LangDC，一种语言感知的动态标记压缩器，用于解决大型视频-语言模型处理高容量视觉标记的效率问题。通过轻量级语言模型描述视频片段并转换为软标记令牌，结合语义密度感知监督，实现基于场景丰富度的动态压缩比例调整。&lt;h4&gt;背景&lt;/h4&gt;大型视频-语言模型在视频理解任务方面取得了进展，但其效率受到处理大量视觉标记的限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有固定压缩比例策略无法适应不同视频片段语义密度变化的问题，避免信息丰富片段表示不足和静态或内容贫乏片段不必要的计算。&lt;h4&gt;方法&lt;/h4&gt;提出LangDC，利用轻量级语言模型描述视频片段并转换为软标记令牌作为视觉表示，通过语义密度感知监督进行训练，实现覆盖关键视觉线索和基于场景丰富度动态调整压缩比例的目标。&lt;h4&gt;主要发现&lt;/h4&gt;与VideoGPT+相比，LangDC减少了49%的FLOPs同时保持有竞争力的性能，并能根据视频片段的丰富度自适应地调整标记压缩比例。&lt;h4&gt;结论&lt;/h4&gt;LangDC通过模拟人类表达视觉内容的方式，实现了更高效的视频标记压缩，为视频理解任务提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近大型视频-语言模型的进展彻底改变了视频理解任务。然而，其效率受到处理大量视觉标记的显著限制。现有的标记压缩策略应用固定的压缩比例，忽略了不同视频片段之间语义密度的变化。因此，这导致由于标记不足而无法充分表示信息丰富的片段，并对静态或内容贫乏的片段进行不必要的计算。为解决这一问题，我们提出了LangDC，一种语言感知的动态标记压缩器。LangDC利用轻量级语言模型描述视频片段，将它们转换为软标记令牌作为视觉表示。通过我们提出的语义密度感知监督进行训练，LangDC旨在1)覆盖下游任务推理所需的关键视觉线索，2)基于场景丰富度动态调整压缩比例，反映在描述长度上。我们的设计模拟了人类如何动态表达所见内容：复杂场景(看到更多)产生更详细的语言来传达细微差别(说更多)，而简单场景则用较少的词语描述。实验结果表明，与VideoGPT+相比，我们的方法减少了49%的FLOPs，同时保持有竞争力的性能。此外，定性结果表明我们的方法能够根据视频片段的丰富度自适应地调整标记压缩比例。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large video-language models have revolutionized videounderstanding tasks. However, their efficiency is significantly constrained byprocessing high volumes of visual tokens. Existing token compression strategiesapply a fixed compression ratio, ignoring the variability in semantic densityamong different video clips. Consequently, this lead to inadequaterepresentation of information-rich clips due to insufficient tokens andunnecessary computation on static or content-poor ones. To address this, wepropose LangDC, a Language-aware Dynamic Token Compressor. LangDC leverages alightweight language model to describe video clips, converting them into softcaption tokens as visual representations. Trained with our proposed semanticdensity-aware supervision, LangDC aims to 1) cover key visual cues necessaryfor downstream task reasoning and 2) dynamically adjust compression ratiosbased on scene richness, reflected by descriptions length. Our design mimicshow humans dynamically express what they see: complex scenes (seeing more)elicit more detailed language to convey nuances (saying more), whereas simplerscenes are described with fewer words. Experimental results show that ourmethod reduces FLOPs by 49% compared to VideoGPT+ while maintaining competitiveperformance. Furthermore, qualitative results demonstrate our approachadaptively adjusts the token compression ratio based on video segment richness.</description>
      <author>example@mail.com (Xiangchen Wang, Jinrui Zhang, Teng Wang, Haigang Zhang, Feng Zheng)</author>
      <guid isPermaLink="false">2509.00969v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>OmniDPO: A Preference Optimization Framework to Address Omni-Modal Hallucination</title>
      <link>http://arxiv.org/abs/2509.00723v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为OmniDPO的偏好对齐框架，用于解决多模态大语言模型中的幻觉问题。该框架通过两种策略增强模型对音频视频交互的理解和对视觉音频信息的关注，有效提高了多模态基础能力并减少了幻觉。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在音频视频理解和实时环境感知等任务中取得显著成果，但仍存在幻觉问题。文本模态的先验倾向主导，导致模型过度依赖文本线索而忽略视觉和音频信息。此外，现有模型在训练时独立对齐各模态，忽略了视频与音频间的内在关联，导致需要解释隐藏音频线索时出现幻觉。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型中的幻觉问题，增强模型对音频视频交互的理解，并加强对视觉和听觉信息的关注。&lt;h4&gt;方法&lt;/h4&gt;提出OmniDPO偏好对齐框架，包含两种策略：(1)构建文本偏好样本对，增强模型对音频视频交互的理解；(2)构建多模态偏好样本对，加强模型对视觉和听觉信息的关注。&lt;h4&gt;主要发现&lt;/h4&gt;OmniDPO有效解决了多模态幻觉问题，显著增强了模型跨模态的推理能力，同时提高了多模态基础能力。&lt;h4&gt;结论&lt;/h4&gt;OmniDPO通过同时解决文本主导和模态间关联缺失两个挑战，有效减轻了多模态幻觉并增强了跨模态推理能力。实验证明该框架在两个多模态大语言模型上都取得了良好效果，相关代码和数据集将在论文接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;最近，多模态大语言模型引发了新一轮研究热潮，在音频视频理解和实时环境感知等任务中取得了令人印象深刻的结果。然而，幻觉问题仍然存在。类似于双模态设置，文本模态的先验倾向于主导，导致多模态大语言模型更依赖文本线索而忽略视觉和音频信息。此外，完全多模态场景引入了新的挑战。大多数现有模型在训练时独立地将视觉或听觉模态与文本对齐，而忽略了视频及其对应音频之间的内在关联。这种疏忽导致在推理需要解释嵌入在视频内容中的隐藏音频线索时产生幻觉。为应对这些挑战，我们提出了OmniDPO，一种旨在减轻多模态大语言模型中幻觉问题的偏好对齐框架。具体而言，OmniDPO包含两种策略：(1)构建文本偏好样本对，增强模型对音频视频交互的理解；(2)构建多模态偏好样本对，加强模型对视觉和听觉信息的关注。通过解决这两个挑战，OmniDPO有效提高了多模态基础能力并减少了幻觉。在两个多模态大语言模型上进行的实验表明，OmniDPO不仅有效减轻了多模态幻觉，还显著增强了模型跨模态的推理能力。所有代码和数据集将在论文接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Omni-modal large language models (OLLMs) have sparked a new wave ofresearch, achieving impressive results in tasks such as audio-videounderstanding and real-time environment perception. However, hallucinationissues still persist. Similar to the bimodal setting, the priors from the textmodality tend to dominate, leading OLLMs to rely more heavily on textual cueswhile neglecting visual and audio information. In addition, fully multimodalscenarios introduce new challenges. Most existing models align visual orauditory modalities with text independently during training, while ignoring theintrinsic correlations between video and its corresponding audio. Thisoversight results in hallucinations when reasoning requires interpreting hiddenaudio cues embedded in video content. To address these challenges, we proposeOmniDPO, a preference-alignment framework designed to mitigate hallucinationsin OLLMs. Specifically, OmniDPO incorporates two strategies: (1) constructingtext-preference sample pairs to enhance the model's understanding ofaudio-video interactions; and (2) constructing multimodal-preference samplepairs to strengthen the model's attention to visual and auditory information.By tackling both challenges, OmniDPO effectively improves multimodal groundingand reduces hallucination. Experiments conducted on two OLLMs demonstrate thatOmniDPO not only effectively mitigates multimodal hallucinations but alsosignificantly enhances the models' reasoning capabilities across modalities.All code and datasets will be released upon paper acceptance.</description>
      <author>example@mail.com (Junzhe Chen, Tianshu Zhang, Shiyu Huang, Yuwei Niu, Chao Sun, Rongzhou Zhang, Guanyu Zhou, Lijie Wen, Xuming Hu)</author>
      <guid isPermaLink="false">2509.00723v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Convolutional Network With Pattern-Spatial Interactive and Regional Awareness for Traffic Forecasting</title>
      <link>http://arxiv.org/abs/2509.00515v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种PSIRAGCN模型，用于解决交通预测中时空相关性建模和区域异质性考虑的问题，通过模式-空间交互融合框架和区域感知图卷积网络，显著提高了预测性能并平衡了计算成本。&lt;h4&gt;背景&lt;/h4&gt;交通预测对城市交通管理、智能路线规划和实时流量监测具有重要意义。最近时空模型的发展显著提高了交通预测中复杂时空相关性的建模能力，但现有研究仍存在局限性。&lt;h4&gt;目的&lt;/h4&gt;克服现有研究在有效建模不同感知视角下时空相关性、忽略交通模式与空间相关性交互融合，以及未考虑区域异质性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出PSIRAGCN模型，包含：1) 模式-空间交互融合框架，通过从全局到局部的感知视角捕捉模式和空间相关性；2) 基于消息传递的图卷积网络，利用区域特征库重建具有区域意识的数据驱动消息传递，揭示交通网络中节点间的区域异质性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实交通数据集上的大量实验表明，PSIRAGCN优于最先进的基线方法，同时平衡了计算成本。&lt;h4&gt;结论&lt;/h4&gt;PSIRAGCN模型有效解决了交通预测中时空相关性建模和区域异质性考虑的问题，为城市交通管理提供了更好的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;交通预测对城市交通管理、智能路线规划和实时流量监测具有重要意义。最近时空模型的进展显著提高了交通预测中复杂时空相关性的建模能力。不幸的是，大多数先前研究在有效建模不同感知视角下的时空相关性方面遇到挑战，忽略了交通模式与空间相关性之间的交互融合。此外，受空间异质性限制，大多数研究在消息传递过程中没有考虑不同的区域异质性。为克服这些局限性，我们提出了一种用于交通预测的模式-空间交互和区域感知图卷积网络(PSIRAGCN)。具体而言，我们提出了由模式和空间模块组成的模式-空间交互融合框架。该框架旨在通过采用从全局到局部的感知视角来捕捉模式和空间相关性，并通过正反馈促进相互利用。在空间模块中，我们设计了基于消息传递的图卷积网络。该网络利用区域特征库来重建具有区域意识的数据驱动消息传递。重建的消息传递可以揭示交通网络中节点之间的区域异质性。在三个真实交通数据集上的大量实验证明，PSIRAGCN优于最先进的基线方法，同时平衡了计算成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic forecasting is significant for urban traffic management, intelligentroute planning, and real-time flow monitoring. Recent advances inspatial-temporal models have markedly improved the modeling of intricatespatial-temporal correlations for traffic forecasting. Unfortunately, mostprevious studies have encountered challenges in effectively modelingspatial-temporal correlations across various perceptual perspectives, whichhave neglected the interactive fusion between traffic patterns and spatialcorrelations. Additionally, constrained by spatial heterogeneity, most studiesfail to consider distinct regional heterogeneity during message-passing. Toovercome these limitations, we propose a Pattern-Spatial Interactive andRegional Awareness Graph Convolutional Network (PSIRAGCN) for trafficforecasting. Specifically, we propose a pattern-spatial interactive fusionframework composed of pattern and spatial modules. This framework aims tocapture patterns and spatial correlations by adopting a perception perspectivefrom the global to the local level and facilitating mutual utilization withpositive feedback. In the spatial module, we designed a graph convolutionalnetwork based on message-passing. The network is designed to leverage aregional characteristics bank to reconstruct data-driven message-passing withregional awareness. Reconstructed message passing can reveal the regionalheterogeneity between nodes in the traffic network. Extensive experiments onthree real-world traffic datasets demonstrate that PSIRAGCN outperforms theState-of-the-art baseline while balancing computational costs.</description>
      <author>example@mail.com (Xinyu Ji, Chengcheng Yan, Jibiao Yuan, Fiefie Zhao)</author>
      <guid isPermaLink="false">2509.00515v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>VideoRewardBench: Comprehensive Evaluation of Multimodal Reward Models for Video Understanding</title>
      <link>http://arxiv.org/abs/2509.00484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://videorewardbench.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了VideoRewardBench，这是第一个全面评估视频领域多模态奖励模型(MRMs)的基准，涵盖了感知、知识、推理和安全四个核心方面。研究团队通过AI辅助数据流程创建了包含1,563个标注样本的高质量偏好数据集，并对28个多模态奖励模型进行了综合评估，发现当前模型表现仍有很大提升空间。&lt;h4&gt;背景&lt;/h4&gt;多模态奖励模型(MRMs)在大视觉语言模型(LVLMs)的训练、推理和评估中起着关键作用。然而，现有视频领域评估MRMs的基准存在三个主要问题：问题数量和多样性有限、缺乏全面的评估维度、以及对多种类型MRMs的评估不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频领域MRMs评估基准的不足，创建一个涵盖视频理解四个核心方面的综合评估基准，并提供高质量的评估数据集。&lt;h4&gt;方法&lt;/h4&gt;通过AI辅助数据流程构建高质量偏好数据集，包含1,563个标注样本、1,482个独特视频和1,559个不同问题。每个样本是一个三元组：视频文本提示、被选中的响应和被拒绝的响应。对28个多模态奖励模型进行综合评估，涵盖生成式、判别式和半标量三类模型。&lt;h4&gt;主要发现&lt;/h4&gt;1) 即使表现最好的模型GPT-4o总体准确率也只有57.0%，最先进的开源模型Qwen2.5-VL-72B仅达到53.3%；2) 使用强化学习训练的MRMs不一定比未使用RL训练的模型具有更强的跨模态泛化能力；3) 除判别式MRMs外，其他类型模型都能从推理时扩展中受益；4) 输入视频帧数的变化对不同类型MRMs有不同影响。&lt;h4&gt;结论&lt;/h4&gt;VideoRewardBench为推进视频领域MRMs的评估和发展提供了具有挑战性和价值的基准，当前模型表现仍有很大提升空间。&lt;h4&gt;翻译&lt;/h4&gt;多模态奖励模型(MRMs)通过评估响应质量在大视觉语言模型(LVLMs)的训练、推理和评估中发挥着关键作用。然而，现有用于评估视频领域MRMs的基准存在问题数量和多样性有限、缺乏全面评估维度、以及对多种类型MRMs评估不足等问题。为解决这些差距，我们引入了VideoRewardBench，这是第一个涵盖视频理解四个核心方面的综合基准：感知、知识和推理以及安全。通过我们的AI辅助数据流程，我们整理了一个包含1,563个标注样本的高质量偏好数据集，包括1,482个独特视频和1,559个不同问题——比问题最多的先前基准多15倍。每个样本是一个三元组，包含视频文本提示、被选中的响应和被拒绝的响应。我们还对跨越三类模型的28个多模态奖励模型进行了全面评估：生成式、判别式和半标量。结果表明，即使是表现最好的模型GPT-4o也只有57.0%的总体准确率，而最先进的开源模型Qwen2.5-VL-72B仅达到53.3%。我们的分析进一步揭示了三个关键洞察：(i) 使用强化学习(RL)训练的MRMs不一定比未使用RL训练的模型表现出更强的跨模态泛化能力；(ii) 除了判别式MRMs外，不同模型容量的其他类型MRMs都可以从推理时扩展中受益；(iii) 输入视频帧数的变化对不同类型的MRMs有不同的影响。我们相信VideoRewardBench为推进视频领域MRMs的评估和发展提供了具有挑战性和价值的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal reward models (MRMs) play a crucial role in the training,inference, and evaluation of Large Vision Language Models (LVLMs) by assessingresponse quality. However, existing benchmarks for evaluating MRMs in the videodomain suffer from a limited number and diversity of questions, a lack ofcomprehensive evaluation dimensions, and inadequate evaluation of diverse typesof MRMs. To address these gaps, we introduce VideoRewardBench, the firstcomprehensive benchmark covering four core aspects of video understanding:perception, knowledge, reasoning, and safety. Through our AI-assisted datapipeline, we curate a high-quality preference dataset of 1,563 annotatedsamples, including 1,482 unique videos and 1,559 distinct questions--15 timesthe number found in the most question-rich prior benchmark. Each sample is atriplet consisting of a video-text prompt, a chosen response, and a rejectedresponse. We also conduct a comprehensive evaluation across 28 multimodalreward models spanning three categories: generative, discriminative, andsemi-scalar. Results show that even the top-performing model GPT-4o achievesonly 57.0% overall accuracy, and the state-of-the-art open-source modelQwen2.5-VL-72B reaches merely 53.3%. Our analysis further reveals three keyinsights: (i) MRMs trained with reinforcement learning (RL) do not necessarilyexhibit stronger cross-modal generalization than those trained without RL; (ii)except for discriminative MRMs, other types of MRMs across varying modelcapacities can benefit from inference-time scaling; and (iii) variations ininput video frame count have different effects on different types of MRMs. Webelieve VideoRewardBench offers a challenging and valuable benchmark foradvancing the evaluation and development of MRMs in the video domain.</description>
      <author>example@mail.com (Zhihong Zhang, Xiaojian Huang, Jin Xu, Zhuodong Luo, Xinzhi Wang, Jiansheng Wei, Xuejin Chen)</author>
      <guid isPermaLink="false">2509.00484v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SurgLLM: A Versatile Large Multimodal Model with Spatial Focus and Temporal Awareness for Surgical Video Understanding</title>
      <link>http://arxiv.org/abs/2509.00357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgLLM框架，一个针对手术视频理解的多功能多模态大模型，通过增强空间焦点和时间感知能力，解决了现有研究中视觉内容感知不足和时间感知有限的问题，在多种手术视频理解任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;手术视频理解对促进计算机辅助手术系统至关重要，但现有研究存在两大局限：对手术视频视觉内容感知不足，以及时间感知不足，这些局限阻碍了多功能CAS解决方案的发展。&lt;h4&gt;目的&lt;/h4&gt;提出SurgLLM框架，一个针对多功能手术视频理解任务的有效多模态大模型，增强空间焦点和时间感知能力。&lt;h4&gt;方法&lt;/h4&gt;设计Surgical Context-aware Multimodal Pretraining（Surg-Pretrain）用于视频编码器，通过执行以手术器械为中心的Masked Video Reconstruction和随后的多模态对齐；提出Temporal-aware Multimodal Tuning将手术时间知识整合到模型中；设计Surgical Task Dynamic Ensemble来有效分类查询并使用最优可学习参数。&lt;h4&gt;主要发现&lt;/h4&gt;在多种手术视频理解任务（包括标题生成、通用VQA和时间VQA）上进行了广泛实验，与最先进的方法相比取得了显著改进，验证了SurgLLM在多功能手术视频理解中的有效性。&lt;h4&gt;结论&lt;/h4&gt;SurgLLM框架有效解决了现有手术视频理解中的空间感知和时间感知不足的问题，为多功能计算机辅助手术系统提供了有价值的解决方案，源代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;手术视频理解对于促进计算机辅助手术系统至关重要。尽管现有研究取得了显著进展，但仍存在两大主要局限，包括对手术视频视觉内容感知不足和时间感知不足，这阻碍了多功能CAS解决方案的发展。在本工作中，我们提出了SurgLLM框架，这是一个针对多功能手术视频理解任务的有效多模态大模型，具有增强的空间焦点和时间感知能力。具体而言，为了增强手术视频的空间焦点，我们首先为SurgLLM的视频编码器设计了手术上下文感知的多模态预训练（Surg-Pretrain），通过执行以手术器械为中心的掩码视频重建（MV-Recon）和随后的多模态对齐。为了将手术时间知识整合到SurgLLM中，我们进一步提出了时间感知多模态调优（TM-Tuning），通过交错的多模态嵌入增强时间推理。此外，为了无冲突地适应手术视频的各种理解任务，我们设计了一个手术任务动态集成，在我们的SurgLLM中高效分类查询并使用最优可学习参数。在多种手术视频理解任务（包括标题生成、通用VQA和时间VQA）上进行的广泛实验表明，与最先进的方法相比取得了显著改进，验证了我们的SurgLLM在多功能手术视频理解中的有效性。源代码可在https://github.com/franciszchen/SurgLLM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surgical video understanding is crucial for facilitating Computer-AssistedSurgery (CAS) systems. Despite significant progress in existing studies, twomajor limitations persist, including inadequate visual content perception andinsufficient temporal awareness in surgical videos, and hinder the developmentof versatile CAS solutions. In this work, we propose the SurgLLM framework, aneffective large multimodal model tailored for versatile surgical videounderstanding tasks with enhanced spatial focus and temporal awareness.Specifically, to empower the spatial focus of surgical videos, we first deviseSurgical Context-aware Multimodal Pretraining (Surg-Pretrain) for the videoencoder of SurgLLM, by performing instrument-centric Masked VideoReconstruction (MV-Recon) and subsequent multimodal alignment. To incorporatesurgical temporal knowledge into SurgLLM, we further propose Temporal-awareMultimodal Tuning (TM-Tuning) to enhance temporal reasoning with interleavedmultimodal embeddings. Moreover, to accommodate various understanding tasks ofsurgical videos without conflicts, we devise a Surgical Task Dynamic Ensembleto efficiently triage a query with optimal learnable parameters in our SurgLLM.Extensive experiments performed on diverse surgical video understanding tasks,including captioning, general VQA, and temporal VQA, demonstrate significantimprovements over the state-of-the-art approaches, validating the effectivenessof our SurgLLM in versatile surgical video understanding. The source code isavailable at https://github.com/franciszchen/SurgLLM.</description>
      <author>example@mail.com (Zhen Chen, Xingjian Luo, Kun Yuan, Jinlin Wu, Danny T. M. Chan, Nassir Navab, Hongbin Liu, Zhen Lei, Jiebo Luo)</author>
      <guid isPermaLink="false">2509.00357v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>The Temporal Game: A New Perspective on Temporal Relation Extraction</title>
      <link>http://arxiv.org/abs/2509.00250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Temporal Game是一种创新的时间关系提取方法，通过游戏化方式实现更灵活细粒度的标注，并支持强化学习训练，同时提供公开可用的演示和开源代码。&lt;h4&gt;背景&lt;/h4&gt;时间关系提取是自然语言处理中的重要任务，传统方法通常直接标注区间级别的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的时间关系提取方法，将任务转化为交互式游戏的形式，支持更细粒度和灵活的标注。&lt;h4&gt;方法&lt;/h4&gt;提出了Temporal Game方法，将区间关系分解为时间实体起点和终点之间的点对点比较；玩家分类单个点关系，系统应用时间闭合推断额外关系并保持一致性；这种基于点的策略自然支持区间和瞬时实体；将时间标注视为顺序决策任务，为训练强化学习代理奠定基础。&lt;h4&gt;主要发现&lt;/h4&gt;比以往任何方法都支持更细粒度和灵活的标注；为训练强化学习代理奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;该演示既可作为研究工具，也可作为标注界面；演示公开可用且源代码开源，以促进时间推理和标注领域的进一步研究和社区驱动开发。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们展示了Temporal Game，这是一种时间关系提取的新方法，将任务转化为交互式游戏。我们不是直接标注区间级别的关系，而是将它们分解为时间实体起点和终点之间的点对点比较。在每一步，玩家分类单个点关系，系统应用时间闭合来推断额外关系并强制保持一致性。这种基于点的策略自然支持区间和瞬时实体，实现了比以往任何方法都更细粒度和灵活的标注。Temporal Game也为训练强化学习代理奠定了基础，通过将时间标注视为顺序决策任务。为了展示这一潜力，本文展示的演示包括一个游戏模式，用户可以在其中标注TempEval-3数据集中的文本并基于评分系统获得反馈，以及一个标注模式，允许标注自定义文档并导出时间线。因此，该演示既是研究工具也是标注界面。演示可在https://temporal-game.inesctec.pt公开获取，源代码已开源，以促进时间推理和标注领域的进一步研究和社区驱动发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761488&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we demo the Temporal Game, a novel approach to temporalrelation extraction that casts the task as an interactive game. Instead ofdirectly annotating interval-level relations, our approach decomposes them intopoint-wise comparisons between the start and end points of temporal entities.At each step, players classify a single point relation, and the system appliestemporal closure to infer additional relations and enforce consistency. Thispoint-based strategy naturally supports both interval and instant entities,enabling more fine-grained and flexible annotation than any previous approach.The Temporal Game also lays the groundwork for training reinforcement learningagents, by treating temporal annotation as a sequential decision-making task.To showcase this potential, the demo presented in this paper includes a Gamemode, in which users annotate texts from the TempEval-3 dataset and receivefeedback based on a scoring system, and an Annotation mode, that allows customdocuments to be annotated and resulting timeline to be exported. Therefore,this demo serves both as a research tool and an annotation interface. The demois publicly available at https://temporal-game.inesctec.pt, and the source codeis open-sourced to foster further research and community-driven development intemporal reasoning and annotation.</description>
      <author>example@mail.com (Hugo Sousa, Ricardo Campos, Alípio Jorge)</author>
      <guid isPermaLink="false">2509.00250v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.21496v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ELV-Halluc，首个专门针对长视频幻觉的基准，用于研究语义聚合幻觉(SAH)现象。研究发现SAH在长视频中尤为关键，且随语义复杂性和快速变化而增加。通过位置编码和DPO策略，成功降低了SAH比率27.7%。&lt;h4&gt;背景&lt;/h4&gt;视频多模态大语言模型(Video-MLLMs)在视频理解方面取得了显著进展，但仍容易出现幻觉。现有视频幻觉基准主要针对短视频，将幻觉归因于强语言先验、缺失帧或视觉语言偏差等因素。&lt;h4&gt;目的&lt;/h4&gt;系统研究长视频中的语义聚合幻觉(SAH)现象，并开发减轻SAH的方法。&lt;h4&gt;方法&lt;/h4&gt;引入ELV-Halluc基准，实验验证SAH存在与特性，采用位置编码策略和DPO策略增强模型区分事件内和事件间语义的能力，并整理8K个对抗数据对的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;SAH随语义复杂性的增加而增加，模型在语义快速变化时更容易出现SAH。位置编码策略和DPO策略能有效减轻SAH，在ELV-Halluc和Video-MME上取得改进，SAH比率降低27.7%。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的基准和数据集，以及采用位置编码和DPO策略，可以有效减轻长视频中的语义聚合幻觉问题，提高Video-MLLMs在长视频理解中的准确性。&lt;h4&gt;翻译&lt;/h4&gt;视频多模态大语言模型(Video-MLLMs)在视频理解方面已取得显著进展。然而，它们仍然容易生成与视频输入不一致或无关的幻觉内容。先前的视频幻觉基准主要关注短视频，将幻觉归因于强语言先验、缺失帧或视觉编码器引入的视觉语言偏差等因素。虽然这些因素确实解释了短视频中的大多数幻觉，但它们仍然过度简化了幻觉的原因。有时，模型生成不正确的输出，但具有正确的帧级语义。我们将这种类型的幻觉称为语义聚合幻觉(SAH)，它是在将帧级语义聚合到事件级语义组的过程中产生的。由于长视频中多个事件之间的语义复杂性增加，SAH变得尤为重要，有必要分离并彻底研究这种幻觉的原因。为了解决上述问题，我们引入了ELV-Halluc，这是第一个专门针对长视频幻觉的基准，能够系统研究SAH。我们的实验证实了SAH的存在，并表明它随着语义复杂性的增加而增加。此外，我们发现模型在语义快速变化时更容易出现SAH。我们讨论了减轻SAH的潜在方法，证明位置编码策略有助于减轻SAH，并进一步采用DPO策略增强模型区分事件内和事件间语义的能力。为此，我们整理了8K个对抗数据对的数据集，并在ELV-Halluc和Video-MME上都取得了改进，包括SAH比率显著降低27.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video multimodal large language models (Video-MLLMs) have achieved remarkableprogress in video understanding. However, they remain vulnerable tohallucination-producing content inconsistent with or unrelated to video inputs.Previous video hallucination benchmarks primarily focus on short-videos. Theyattribute hallucinations to factors such as strong language priors, missingframes, or vision-language biases introduced by the visual encoder. While thesecauses indeed account for most hallucinations in short videos, they stilloversimplify the cause of hallucinations. Sometimes, models generate incorrectoutputs but with correct frame-level semantics. We refer to this type ofhallucination as Semantic Aggregation Hallucination (SAH), which arises duringthe process of aggregating frame-level semantics into event-level semanticgroups. Given that SAH becomes particularly critical in long videos due toincreased semantic complexity across multiple events, it is essential toseparate and thoroughly investigate the causes of this type of hallucination.To address the above issues, we introduce ELV-Halluc, the first benchmarkdedicated to long-video hallucination, enabling a systematic investigation ofSAH. Our experiments confirm the existence of SAH and show that it increaseswith semantic complexity. Additionally, we find that models are more prone toSAH on rapidly changing semantics. Moreover, we discuss potential approaches tomitigate SAH. We demonstrate that positional encoding strategy contributes toalleviating SAH, and further adopt DPO strategy to enhance the model's abilityto distinguish semantics within and across events. To support this, we curate adataset of 8K adversarial data pairs and achieve improvements on bothELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.</description>
      <author>example@mail.com (Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu)</author>
      <guid isPermaLink="false">2508.21496v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph neural networks for learning liquid simulations in dynamic scenes containing kinematic objects</title>
      <link>http://arxiv.org/abs/2509.03446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的框架，用于模拟液体粒子在刚体相互作用和主动操作下的动力学行为。该方法将粒子表示为图节点，使用边界体积层次结构（BVH）算法处理粒子-物体碰撞，能够准确捕捉动态环境中的流体行为，并具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;高保真粒子动力学模拟对于设计、图形学和机器人领域中涉及液体的现实世界交互和控制任务至关重要。基于图神经网络的数据驱动方法虽已取得进展，但通常局限于静态自由落体环境或简单物体的操作设置，忽略了与动态运动刚体的复杂相互作用。&lt;h4&gt;目的&lt;/h4&gt;设计一个从头构建的基于GNN的框架，用于学习液体在刚体相互作用和主动操作下的动力学行为。&lt;h4&gt;方法&lt;/h4&gt;将粒子表示为图节点，使用表面表示和边界体积层次结构（BVH）算法处理粒子-物体碰撞，使网络能够建模液体粒子与复杂表面几何形状之间的复杂相互作用。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够准确捕捉动态环境中的流体行为，也能在静态自由落体环境中作为模拟器运行；尽管只在一个倒置任务上进行了训练，但模型能有效推广到未见过的物体和新的操作任务如搅拌和舀取；学习到的动力学可用于解决基于梯度的优化方法控制任务。&lt;h4&gt;结论&lt;/h4&gt;所提出的GNN框架能够有效学习液体与刚体相互作用的动力学，具有良好的泛化能力，可推广到未见过的物体和新的操作任务，在实际应用中具有潜力。&lt;h4&gt;翻译&lt;/h4&gt;以高保真度模拟粒子动力学对于解决设计、图形学和机器人领域中涉及液体的现实世界交互和控制任务至关重要。最近，基于图神经网络的数据驱动方法在解决此类问题上取得了进展。然而，这些方法通常局限于学习静态自由落体环境中的流体行为或涉及简单物体的简单操作设置，往往忽略了与动态运动刚体的复杂相互作用。在此，我们提出了一种从头设计的基于GNN的框架，用于学习液体在刚体相互作用和主动操作下的动力学行为，其中粒子被表示为图节点，粒子-物体碰撞使用表面表示和边界体积层次结构（BVH）算法处理。这种方法使网络能够建模液体粒子与复杂表面几何形状之间的复杂相互作用。我们的模型能够准确捕捉动态环境中的流体行为，也能在静态自由落体环境中作为模拟器运行。尽管只在一个倒置任务上进行了训练，但我们的模型能有效推广到包含未见物体的环境和新的操作任务，如搅拌和舀取。最后，我们表明学习到的动力学可以利用基于梯度的优化方法来解决控制和操作任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文旨在解决现有图神经网络液体模拟方法无法处理动态场景中液体与运动刚体复杂交互的问题。这个问题在现实中很重要，因为准确模拟液体与动态物体的交互对于机器人控制、自主代理与物体交互等应用至关重要，例如机器人倒液体、搅拌等日常任务。现有方法通常局限于静态环境或简单物体，限制了它们在实际应用中的适用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有GNN模拟器的局限性，认识到需要一种能够同时处理复杂流体动力学和粒子与动态刚体交互的统一框架。他们借鉴了Sanchez-Gonzalez等人[10]的图神经网络架构设计，包括使用相对几何特征定义节点边属性、单步预测训练和消息传递GNN作为核心模型。同时，他们改进了之前使用符号距离场(SDF)进行碰撞处理的方法[13,14]，并参考了MeshGraphNet[11]的多图表示方法，但进行了创新以支持动态交互场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络模拟液体粒子与动态运动刚体的交互，通过将液体粒子和刚体表示为不同节点集合，并使用基于表面的碰撞检测方法准确处理交互。整体流程包括：1)构建图结构，包含液体粒子节点、刚体节点和表面网格节点；2)提取节点和边特征，使用速度历史和相对位置信息；3)通过多个消息传递层迭代更新节点和边特征；4)解码器从液体节点潜在表示中提取更新后的动力学信息；5)在地面真实模拟数据上训练模型；6)给定初始状态和控制输入，迭代生成粒子轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的GNN框架同时处理流体动力学和动态刚体交互；2)多图表示区分液体粒子和刚体节点；3)使用BVH算法进行基于表面的精确碰撞检测；4)能够处理动态场景而非仅静态环境；5)强大的泛化能力，能适应未见物体和新任务；6)可与梯度优化方法集成解决控制任务。相比之前工作，不同之处在于：能处理动态场景而非仅静态环境；支持复杂几何形状而非简单形状；能处理多种操作任务；使用更精确的基于表面碰撞检测而非基于顶点；具有更好的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图神经网络的统一框架，能够准确模拟液体与动态运动刚体之间的复杂交互，并在多种操作任务和未见物体上展现出强大的泛化能力，为机器人和自主代理提供了更逼真的液体模拟工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating particle dynamics with high fidelity is crucial for solvingreal-world interaction and control tasks involving liquids in design, graphics,and robotics. Recently, data-driven approaches, particularly those based ongraph neural networks (GNNs), have shown progress in tackling such problems.However, these approaches are often limited to learning fluid behavior instatic free-fall environments or simple manipulation settings involvingprimitive objects, often overlooking complex interactions with dynamicallymoving kinematic rigid bodies. Here, we propose a GNN-based framework designedfrom the ground up to learn the dynamics of liquids under rigid bodyinteractions and active manipulations, where particles are represented as graphnodes and particle-object collisions are handled using surface representationswith the bounding volume hierarchy (BVH) algorithm. This approach enables thenetwork to model complex interactions between liquid particles and intricatesurface geometries. Our model accurately captures fluid behavior in dynamicsettings and can also function as a simulator in static free-fall environments.Despite being trained on a single-object manipulation task of pouring, ourmodel generalizes effectively to environments with unseen objects and novelmanipulation tasks such as stirring and scooping. Finally, we show that thelearned dynamics can be leveraged to solve control and manipulation tasks usinggradient-based optimization methods.</description>
      <author>example@mail.com (Niteesh Midlagajni, Constantin A. Rothkopf)</author>
      <guid isPermaLink="false">2509.03446v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Exploring a Graph-based Approach to Offline Reinforcement Learning for Sepsis Treatment</title>
      <link>http://arxiv.org/abs/2509.03393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18th European Workshop on Reinforcement Learning (EWRL 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了使用图神经网络处理脓毒症患者数据的方法，旨在改进静脉输液和血管加压剂的治疗决策。&lt;h4&gt;背景&lt;/h4&gt;脓毒症是一种严重且危及生命的状况，在治疗过程中确定患者静脉输液和血管加压剂的正确用量具有挑战性。以往的研究依赖于关系数据，而现代医疗保健数据复杂，可能更适合用图表示。&lt;h4&gt;目的&lt;/h4&gt;探索基于图的方法表示医疗数据，使用图神经网络学习患者状态表示，并将表示学习与策略学习分离，以改进脓毒症治疗决策。&lt;h4&gt;方法&lt;/h4&gt;将MIMIC-III数据集中的患者数据建模为随时间演化的异构图；采用GraphSAGE和GATv2两种图神经网络架构；训练编码器生成潜在状态表示，同时使用解码器预测下一个患者状态；最后使用dBCQ算法进行策略学习。&lt;h4&gt;主要发现&lt;/h4&gt;基于图的方法在脓毒症治疗决策支持方面显示出潜力，同时表明在该领域中表示学习具有复杂性。&lt;h4&gt;结论&lt;/h4&gt;图方法为脓毒症治疗决策支持提供了有前景的途径，但表示学习的复杂性仍需进一步研究解决。&lt;h4&gt;翻译&lt;/h4&gt;脓毒症是一种严重、危及生命的状况。在治疗脓毒症时，很难确定患者静脉输液和血管加压剂的最佳用量。虽然基于自动强化学习的方法已被用于支持这些决策并取得有希望的结果，但先前的研究依赖于关系数据。考虑到现代医疗保健数据的复杂性，将数据表示为图可能提供更自然和有效的方法。本研究将著名的MIMIC-III数据集中的患者数据建模为随时间演化的异构图。随后，我们探索了两种图神经网络架构——GraphSAGE和GATv2，用于学习患者状态表示，采用将表示学习与策略学习分离的方法。编码器与预测下一个患者状态解码器共同训练，以生成潜在状态表示。然后使用dBCQ算法将这些表示用于策略学习。我们的实验评估结果证实了基于图方法的潜力，同时强调了该领域中表示学习的复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sepsis is a serious, life-threatening condition. When treating sepsis, it ischallenging to determine the correct amount of intravenous fluids andvasopressors for a given patient. While automated reinforcement learning(RL)-based methods have been used to support these decisions with promisingresults, previous studies have relied on relational data. Given the complexityof modern healthcare data, representing data as a graph may provide a morenatural and effective approach. This study models patient data from thewell-known MIMIC-III dataset as a heterogeneous graph that evolves over time.Subsequently, we explore two Graph Neural Network architectures - GraphSAGE andGATv2 - for learning patient state representations, adopting the approach ofdecoupling representation learning from policy learning. The encoders aretrained to produce latent state representations, jointly with decoders thatpredict the next patient state. These representations are then used for policylearning with the dBCQ algorithm. The results of our experimental evaluationconfirm the potential of a graph-based approach, while highlighting thecomplexity of representation learning in this domain.</description>
      <author>example@mail.com (Taisiya Khakharova, Lucas Sakizloglou, Leen Lambers)</author>
      <guid isPermaLink="false">2509.03393v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-Guided Content-Adaptive Graph Learning for Hyperspectral Unmixing</title>
      <link>http://arxiv.org/abs/2509.03376v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于Transformer的内容自适应图解混框架（T-CAGU），通过结合Transformer捕获全局依赖性和内容自适应图神经网络增强局部关系，解决了现有高光谱解混方法无法同时考虑全局和局部信息的问题。&lt;h4&gt;背景&lt;/h4&gt;高光谱解混（HU）旨在将遥感图像中的每个混合像素分解为一组端元及其相应的丰度。尽管使用深度学习在该领域取得了显著进展，但大多数方法无法同时表征全局依赖性和局部一致性，难以同时保持长程交互和边界细节。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕获全局依赖性和局部一致性的高光谱解混方法，以保留长程交互和边界细节。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于Transformer的内容自适应图解混框架（T-CAGU），该方法使用Transformer捕获全局依赖性，并引入内容自适应图神经网络来增强局部关系。T-CAGU集成了多种传播顺序来动态学习图结构，确保对噪声的鲁棒性，并利用图残差机制来保留全局信息并稳定训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，T-CAGU优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;T-CAGU是一种有效的高光谱解混方法，能够同时考虑全局和局部信息，并且对噪声具有鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;高光谱解混（HU）旨在将遥感图像中的每个混合像素分解为一组端元及其相应的丰度。尽管使用深度学习在该领域取得了显著进展，但大多数方法无法同时表征全局依赖性和局部一致性，难以同时保持长程交互和边界细节。本文提出了一种新的基于Transformer的内容自适应图解混框架（T-CAGU），该方法通过使用Transformer捕获全局依赖性并引入内容自适应图神经网络来增强局部关系，从而克服了这些挑战。与之前的工作不同，T-CAGU集成了多种传播顺序来动态学习图结构，确保对噪声的鲁棒性。此外，T-CAGU利用图残差机制来保留全局信息并稳定训练。实验结果表明其优于最先进的方法。我们的代码可在https://github.com/xianchaoxiu/T-CAGU获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral unmixing (HU) targets to decompose each mixed pixel in remotesensing images into a set of endmembers and their corresponding abundances.Despite significant progress in this field using deep learning, most methodsfail to simultaneously characterize global dependencies and local consistency,making it difficult to preserve both long-range interactions and boundarydetails. This letter proposes a novel transformer-guided content-adaptive graphunmixing framework (T-CAGU), which overcomes these challenges by employing atransformer to capture global dependencies and introducing a content-adaptivegraph neural network to enhance local relationships. Unlike previous work,T-CAGU integrates multiple propagation orders to dynamically learn the graphstructure, ensuring robustness against noise. Furthermore, T-CAGU leverages agraph residual mechanism to preserve global information and stabilize training.Experimental results demonstrate its superiority over the state-of-the-artmethods. Our code is available at https://github.com/xianchaoxiu/T-CAGU.</description>
      <author>example@mail.com (Hui Chen, Liangyu Liu, Xianchao Xiu, Wanquan Liu)</author>
      <guid isPermaLink="false">2509.03376v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Temporal social network modeling of mobile connectivity data with graph neural networks</title>
      <link>http://arxiv.org/abs/2509.03319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)在基于移动连接数据的时间社交网络分析中的应用，比较了四种基于快照的时间GNN模型与一个非GNN基线模型在预测用户间电话和短信活动方面的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为建模图结构复杂网络连接数据的先进数据驱动工具，能够整合节点和边在时空中的信息。然而，使用人们移动连接数据的时间序列分析社交网络尚未得到广泛研究。&lt;h4&gt;目的&lt;/h4&gt;研究四种基于快照的时间GNN模型在预测移动通信网络用户间电话呼叫和短信活动方面的性能，并开发一个使用EdgeBank方法的简单非GNN基线模型进行比较。&lt;h4&gt;方法&lt;/h4&gt;评估了四种基于快照的时间GNN模型(ROLAND等)和一个基于EdgeBank方法的非GNN基线模型，比较它们在预测用户间通信活动方面的表现。&lt;h4&gt;主要发现&lt;/h4&gt;ROLAND时间GNN在大多数情况下优于基线模型，而其他三种GNN平均表现不如基线模型。基于GNN的方法在通过移动连接数据分析时间社交网络方面具有潜力。&lt;h4&gt;结论&lt;/h4&gt;由于ROLAND和基线模型之间的性能差距相对较小，需要进一步研究专门用于时间社交网络分析的GNN架构，以提高预测性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为一种先进的基于数据驱动的方法，用于建模图结构复杂网络的连接数据，并整合其节点和边在时空中的信息。然而，迄今为止，使用人们移动连接数据的时间序列分析社交网络尚未得到广泛研究。在本研究中，我们研究了四种基于快照的时间GNN模型，用于预测移动通信网络用户之间的电话呼叫和短信活动。此外，我们使用最近提出的EdgeBank方法开发了一个简单的非GNN基线模型。我们的分析表明，ROLAND时间GNN在大多数情况下优于基线模型，而其他三种GNN的平均表现不如基线模型。结果表明，基于GNN的方法在通过移动连接数据分析时间社交网络方面具有前景。然而，由于ROLAND和基线模型之间的性能差距相对较小，需要进一步研究专门用于时间社交网络分析的GNN架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as a state-of-the-art data-driventool for modeling connectivity data of graph-structured complex networks andintegrating information of their nodes and edges in space and time. However, asof yet, the analysis of social networks using the time series of people'smobile connectivity data has not been extensively investigated. In the presentstudy, we investigate four snapshot - based temporal GNNs in predicting thephone call and SMS activity between users of a mobile communication network. Inaddition, we develop a simple non - GNN baseline model using recently proposedEdgeBank method. Our analysis shows that the ROLAND temporal GNN outperformsthe baseline model in most cases, whereas the other three GNNs perform onaverage worse than the baseline. The results show that GNN based approacheshold promise in the analysis of temporal social networks through mobileconnectivity data. However, due to the relatively small performance marginbetween ROLAND and the baseline model, further research is required onspecialized GNN architectures for temporal social network analysis.</description>
      <author>example@mail.com (Joel Jaskari, Chandreyee Roy, Fumiko Ogushi, Mikko Saukkoriipi, Jaakko Sahlsten, Kimmo Kaski)</author>
      <guid isPermaLink="false">2509.03319v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TRELLIS-Enhanced Surface Features for Comprehensive Intracranial Aneurysm Analysis</title>
      <link>http://arxiv.org/abs/2509.03095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种跨领域特征迁移方法，利用TRELLIS生成模型学习的几何特征来增强颅内动脉瘤分析，在三个关键任务上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;颅内动脉瘤构成显著的临床风险，但由于有限的标注3D数据，难以检测、描绘和建模这些动脉瘤。&lt;h4&gt;目的&lt;/h4&gt;开发一种跨领域特征迁移方法，利用通用生成模型学习到的几何特征来增强神经网络的动脉瘤分析能力。&lt;h4&gt;方法&lt;/h4&gt;利用TRELLIS（一种在大型非医学3D数据集上训练的生成模型）学习的潜在几何嵌入，用TRELLIS表面特征替代传统的点法线或网格描述符，增强三个下游任务：在Intra3D数据集中分类动脉瘤与健康血管、在3D网格上分割动脉瘤和血管区域、使用图神经网络在AnXplore数据集上预测血流场。&lt;h4&gt;主要发现&lt;/h4&gt;包含这些特征在准确性、F1分数和分割质量上优于最先进的基线方法，模拟误差减少了15%。&lt;h4&gt;结论&lt;/h4&gt;展示了从通用生成模型到专门医疗任务的3D表示迁移的更广泛潜力，为医学图像分析提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;颅内动脉瘤构成显著的临床风险，但由于有限的标注3D数据，难以检测、描绘和建模。我们提出了一种跨领域特征迁移方法，利用TRELLIS（一种在大型非医学3D数据集上训练的生成模型）学习的潜在几何嵌入来增强神经网络的动脉瘤分析能力。通过用TRELLIS表面特征替代传统的点法线或网格描述符，我们系统性地增强了三个下游任务：在Intra3D数据集中分类动脉瘤与健康血管、在3D网格上分割动脉瘤和血管区域、使用图神经网络在AnXplore数据集上预测随时间演变的血流场。实验表明，包含这些特征在准确性、F1分数和分割质量上优于最先进的基线方法，并将模拟误差降低了15%。这些结果展示了从通用生成模型到专门医疗任务的3D表示迁移的更广泛潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决颅内动脉瘤的检测、分割和建模困难的问题。这个问题在现实中非常重要，因为颅内动脉瘤是一种严重的临床风险，通常无症状直到破裂，而破裂会导致高发病率和死亡率。早期准确检测未破裂的动脉瘤对预防性临床干预至关重要，但由于标注的3D医学数据有限，这些任务在技术上具有挑战性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到生成式AI在3D对象合成领域取得了显著进展，特别是TRELLIS这样的模型能够生成高保真度的3D对象。他们假设，尽管这些模型在非医学数据上训练，但其学到的丰富几何表示可能对医学领域也有价值。作者设计了一个跨领域特征迁移方法，利用TRELLIS编码器提取深度几何特征。该方法借鉴了现有的TRELLIS生成模型、PointNet和PointNet++等点云处理架构，以及图神经网络进行血流模拟，并基于现有的Intra3D和AnXplore数据集进行实验。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用在大型非医学3D数据集上预训练的TRELLIS模型提取的表面特征，增强神经网络在动脉瘤分析任务中的性能。整体实现流程包括：1) 使用TRELLIS编码器从动脉瘤和血管的3D网格中提取特征，通过多角度渲染、体素化、使用DINOv2自编码器和稀疏变分自编码器生成结构化潜在令牌；2) 将提取的TRELLIS特征集成到下游任务中，包括使用PointNet和PointNet++进行分类和分割，以及使用图神经网络进行血流模拟；3) 对提取的特征进行分析，包括PCA、t-SNE可视化和聚类分析。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出跨领域特征迁移方法，利用通用3D生成模型(TRELLIS)的预训练特征增强医学3D数据分析；2) 首次将TRELLIS模型应用于医学领域，特别是颅内动脉瘤分析；3) 系统性地将TRELLIS表面特征应用于分类、分割和血流模拟三个下游任务；4) 通过特征分析揭示TRELLIS特征能有效捕捉医学对象的几何特征。相比之前的工作，本文不使用点坐标和法线等基本几何特征，而是使用TRELLIS提取的高级特征，在多个任务上取得了更好的性能，不仅提高了分类和分割的准确性，还减少了血流模拟的误差15%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过利用通用3D生成模型TRELLIS提取的表面特征，显著提升了颅内动脉瘤的分类、分割和血流模拟性能，开创了跨领域特征迁移应用于医学3D数据分析的新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intracranial aneurysms pose a significant clinical risk yet are difficult todetect, delineate and model due to limited annotated 3D data. We propose across-domain feature-transfer approach that leverages the latent geometricembeddings learned by TRELLIS, a generative model trained on large-scalenon-medical 3D datasets, to augment neural networks for aneurysm analysis. Byreplacing conventional point normals or mesh descriptors with TRELLIS surfacefeatures, we systematically enhance three downstream tasks: (i) classifyinganeurysms versus healthy vessels in the Intra3D dataset, (ii) segmentinganeurysm and vessel regions on 3D meshes, and (iii) predicting time-evolvingblood-flow fields using a graph neural network on the AnXplore dataset. Ourexperiments show that the inclusion of these features yields strong gains inaccuracy, F1-score and segmentation quality over state-of-the-art baselines,and reduces simulation error by 15\%. These results illustrate the broaderpotential of transferring 3D representations from general-purpose generativemodels to specialized medical tasks.</description>
      <author>example@mail.com (Clément Hervé, Paul Garnier, Jonathan Viquerat, Elie Hachem)</author>
      <guid isPermaLink="false">2509.03095v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PDRL: Post-hoc Descriptor-based Residual Learning for Uncertainty-Aware Machine Learning Potentials</title>
      <link>http://arxiv.org/abs/2509.02927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种名为PDRL的后验不确定性量化方法，利用已训练的图神经网络势能描述符来估计残差误差，作为预测不确定性的代理。&lt;h4&gt;背景&lt;/h4&gt;集成方法被认为是机器学习原子间势能(MLIPs)不确定性量化的黄金标准，但其高计算成本限制了其实用性。虽然提出了蒙特卡罗dropout和深度核学习等替代技术来提高计算效率，但这些方法有些不能应用于已训练的模型，并可能影响预测准确性。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单高效的后验不确定性量化框架，利用已训练的图神经网络势能描述符来估计残差误差。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为后验描述符基残差学习(PDRL)的方法，该方法通过建模MLIP预测与真实值之间的差异，使这些残差能够作为预测不确定性的代理。探索了PDRL的多种变体，并将其与既定的不确定性量化方法进行基准测试，评估其有效性和局限性。&lt;h4&gt;主要发现&lt;/h4&gt;探索了PDRL的多种变体，并将其与既定的不确定性量化方法进行了基准测试。&lt;h4&gt;结论&lt;/h4&gt;评估了PDRL方法的有效性和局限性。&lt;h4&gt;翻译&lt;/h4&gt;集成方法被认为是机器学习原子间势能(MLIPs)不确定性量化的黄金标准。然而，它们的高计算成本可能限制其实用性。已经提出了蒙特卡罗dropout和深度核学习等替代技术来提高计算效率；然而，其中一些方法不能应用于已训练的模型，并可能影响预测准确性。在本文中，我们提出了一种简单高效的后验不确定性量化框架，利用已训练的图神经网络势能的描述符来估计残差误差。我们将此方法称为后验描述符基残差学习(PDRL)。PDRL对MLIP预测与真实值之间的差异进行建模，使这些残差能够作为预测不确定性的代理。我们探索了PDRL的多种变体，并将其与既定的不确定性量化方法进行基准测试，评估它们的有效性和局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensemble method is considered the gold standard for uncertaintyquantification (UQ) for machine learning interatomic potentials (MLIPs).However, their high computational cost can limit its practicality. Alternativetechniques, such as Monte Carlo dropout and deep kernel learning, have beenproposed to improve computational efficiency; however, some of these methodscannot be applied to already trained models and may affect the predictionaccuracy. In this paper, we propose a simple and efficient post-hoc frameworkfor UQ that leverages the descriptor of a trained graph neural networkpotential to estimate residual errors. We refer to this method as post-hocdescriptor-based residual-based learning (PDRL). PDRL models the discrepancybetween MLIP predictions and ground truth values, allowing these residuals toact as proxies for prediction uncertainty. We explore multiple variants of PDRLand benchmark them against established UQ methods, evaluating both theireffectiveness and limitations.</description>
      <author>example@mail.com (Shih-Peng Huang, Nontawat Charoenphakdee, Yuta Tsuboi, Yong-Bin Zhuang, Wenwen Li)</author>
      <guid isPermaLink="false">2509.02927v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Power Grid Control with Graph-Based Distributed Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.02861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图论的分布式强化学习框架，用于实时、可扩展的电网管理。该框架由分布式低级代理和高级管理代理组成，使用图神经网络编码网络拓扑信息，并结合模仿学习和基于潜在函数的奖励塑造来提高学习效率。实验证明该方法优于标准基线，且计算效率更高。&lt;h4&gt;背景&lt;/h4&gt;可再生能源的必要整合与电网规模的不断扩大，给现代电网控制带来了重大挑战。基于人类和优化的传统控制系统在这种不断变化的背景下难以适应和扩展，这促使人们探索更动态和分布式的控制策略。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实时、可扩展地管理电网的控制系统，以解决传统控制方法在应对可再生能源整合和电网规模扩大方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于图论的分布式强化学习框架，包括：1)由分布式低级代理（作用于单个电力线路）和高级管理代理组成的架构；2)使用图神经网络（GNN）在单个低级代理的观察中编码网络拓扑信息；3)整合模仿学习和基于潜在函数的奖励塑造以加速收敛和提高学习稳定性；4)不仅分解动作空间，还分解观察空间；5)每个低级代理基于通过GNN构建的环境结构化和信息丰富的局部视图进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;1)在Grid2op仿真环境中的实验表明，该方法有效且一致地优于该领域普遍采用的标准基线方法；2)与基于仿真的专家方法相比，所提出的模型计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;基于图论的分布式强化学习框架为解决现代电网控制中的挑战提供了一种有效方法，通过结合图神经网络、模仿学习和奖励塑造技术，实现了实时、可扩展的电网管理，并且在性能和计算效率方面都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;可再生能源的必要整合与电网规模的不断扩大，给现代电网控制带来了重大挑战。基于人类和优化的传统控制系统在这种不断变化的背景下难以适应和扩展，这促使人们探索更动态和分布式的控制策略。本文提出了一种基于图论的分布式强化学习框架，用于实时、可扩展的电网管理。所提出的架构由分布式低级代理（作用于单个电力线路）和高级管理代理协调组成。采用图神经网络（GNN）在单个低级代理的观察中编码网络拓扑信息。为了加速收敛和提高学习稳定性，该框架整合了模仿学习和基于潜在函数的奖励塑造。与仅分解动作空间而依赖全局观察的传统去中心化方法不同，该方法还分解了观察空间。每个低级代理基于通过GNN构建的环境结构化和信息丰富的局部视图进行操作。在Grid2Op仿真环境中的实验表明了该方法的有效性，其性能一致地优于该领域普遍采用的标准基线方法。此外，与基于仿真的专家方法相比，所提出的模型计算效率更高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The necessary integration of renewable energy sources, combined with theexpanding scale of power networks, presents significant challenges incontrolling modern power grids. Traditional control systems, which are humanand optimization-based, struggle to adapt and to scale in such an evolvingcontext, motivating the exploration of more dynamic and distributed controlstrategies. This work advances a graph-based distributed reinforcement learningframework for real-time, scalable grid management. The proposed architectureconsists of a network of distributed low-level agents acting on individualpower lines and coordinated by a high-level manager agent. A Graph NeuralNetwork (GNN) is employed to encode the network's topological informationwithin the single low-level agent's observation. To accelerate convergence andenhance learning stability, the framework integrates imitation learning andpotential-based reward shaping. In contrast to conventional decentralizedapproaches that decompose only the action space while relying on globalobservations, this method also decomposes the observation space. Each low-levelagent acts based on a structured and informative local view of the environmentconstructed through the GNN. Experiments on the Grid2Op simulation environmentshow the effectiveness of the approach, which consistently outperforms thestandard baseline commonly adopted in the field. Additionally, the proposedmodel proves to be much more computationally efficient than thesimulation-based Expert method.</description>
      <author>example@mail.com (Carlo Fabrizio, Gianvito Losapio, Marco Mussi, Alberto Maria Metelli, Marcello Restelli)</author>
      <guid isPermaLink="false">2509.02861v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Quasi-van Hove singularities informed approach improving DOS/pDOS predictions in GNN</title>
      <link>http://arxiv.org/abs/2509.02818v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文扩展了Van Hove奇点的概念，提出在机器学习方法中使用这些奇点作为先验信息的新方法，以提高模型预测质量。&lt;h4&gt;背景&lt;/h4&gt;传统方法直接计算量子态密度，而本文提出使用机器学习方法的估计值来处理这个问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用Van Hove奇点作为先验信息的机器学习方法，以提高模型预测质量。&lt;h4&gt;方法&lt;/h4&gt;使用机器学习估计代替直接计算量子态密度，通过确定额外信息组织后处理，采用类似于考虑Fisher信息的正则化机制重新分配系统信息。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法显著提高了模型预测质量，其分析效果类似于高阶Van Hove奇点，因为系统具有额外的简并度。&lt;h4&gt;结论&lt;/h4&gt;通过扩展Van Hove奇点概念并结合机器学习方法，可以有效地改善模型预测性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们扩展了Van Hove奇点的概念，以便在机器学习方法中明确使用它们作为模型的先验信息。当不直接计算量子态密度，而是使用机器学习方法获得的估计值时，所提出的方法成为可能。然后，确定额外信息允许我们组织后处理，这显著提高了模型预测的质量。从分析上看，该效应类似于高阶Van Hove奇点，因为系统具有额外的简并度。作为重新分配系统信息的机制，我们提出了正则化，其在意义上接近于考虑Fisher信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce an extension of the concept of Van Hoffsingularities in order to explicitly use them in machine learning methods ofmodels as a priori information. The claimed method becomes possible when,instead of directly calculating the density of quantum states, we operate withestimates obtained by machine learning methods. Then, determining additionalinformation allows us to organize post-processing, which significantly improvesthe quality of model prediction. Analytically, the effect is similar to VanHove singularities of high order due to the additional degree of degeneracy ofthe system. As a mechanism for redistributing information about the system, wepropose regularization, which is close in meaning to taking into account Fisherinformation.</description>
      <author>example@mail.com (Grigory Koroteev)</author>
      <guid isPermaLink="false">2509.02818v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Laplacian Eigenvectors: a Pre-training Method for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.02803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通过归纳学习拉普拉斯特征向量来预训练图神经网络(GNNs)的新框架。&lt;h4&gt;背景&lt;/h4&gt;传统的消息传递神经网络(MPNNs)随着网络深度的增加，往往面临过平滑风险，难以捕捉全局和区域的图结构。&lt;h4&gt;目的&lt;/h4&gt;设计一种预训练框架，使GNNs能够自然学习每个图的大规模结构模式，从而更好地捕捉全局和区域图结构。&lt;h4&gt;方法&lt;/h4&gt;利用图拉普拉斯矩阵的低频特征向量编码全局信息的特点，开发了一种自监督预训练框架，该框架基于图结构且具有高度灵活性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，通过该框架预训练的模型在各种基于图结构的任务上表现优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;与大多数现有专注于节点或边特征重建等特定领域任务的预训练方法不同，这种基于特征向量学习的自监督预训练框架适用于所有基于图的数据集，并且在特定任务数据稀疏时可以与合成特征结合使用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种通过归纳学习拉普拉斯特征向量来预训练图神经网络(GNNs)的新框架。传统的消息传递神经网络(MPNNs)往往由于网络深度增加导致的过平滑风险而难以捕捉全局和区域的图结构。由于图拉普拉斯矩阵的低频特征向量编码了全局信息，预训练GNNs来预测这些特征向量会鼓励网络自然地学习每个图的大规模结构模式。经验上，我们证明通过我们框架预训练的模型在多种基于图结构的任务上优于基线模型。虽然大多数现有的预训练方法专注于节点或边特征重建等特定领域任务，但我们的自监督预训练框架是基于结构的，具有高度灵活性。特征向量学习可以应用于所有基于图的数据集，并且在特定任务数据稀疏时，可以与合成特征一起使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel framework for pre-training Graph Neural Networks (GNNs) byinductively learning Laplacian eigenvectors. Traditional Message Passing NeuralNetworks (MPNNs) often struggle to capture global and regional graph structuredue to over-smoothing risk as network depth increases. Because thelow-frequency eigenvectors of the graph Laplacian matrix encode globalinformation, pre-training GNNs to predict these eigenvectors encourages thenetwork to naturally learn large-scale structural patterns over each graph.Empirically, we show that models pre-trained via our framework outperformbaseline models on a variety of graph structure-based tasks. While mostexisting pre-training methods focus on domain-specific tasks like node or edgefeature reconstruction, our self-supervised pre-training framework isstructure-based and highly flexible. Eigenvector-learning can be applied to allgraph-based datasets, and can be used with synthetic features whentask-specific data is sparse.</description>
      <author>example@mail.com (Howard Dai, Nyambura Njenga, Benjamin Whitsett, Catherine Ma, Darwin Deng, Sara de Ángel, Alexandre Van Tassel, Siddharth Viswanath, Ryan Pellico, Ian Adelstein, Smita Krishnaswamy)</author>
      <guid isPermaLink="false">2509.02803v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Composite-Loss Graph Neural Network for the Multivariate Post-Processing of Ensemble Weather Forecasts</title>
      <link>http://arxiv.org/abs/2509.02784v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 16 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种使用图神经网络(dualGNN)和复合损失函数进行集合预报多变量后处理的新方法，该方法在保持变量间和空间依赖关系方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;集合预报系统通过提供未来状态的概率估计推动了气象学发展，支持从可再生能源生产到运输安全等多种应用。然而，系统性偏差仍然存在，使得统计后处理变得必要。传统参数化后处理技术和基于机器学习的方法通常难以捕捉预报维度间的依赖关系。&lt;h4&gt;目的&lt;/h4&gt;研究使用图神经网络(dualGNN)进行集合预报的多变量后处理，该网络使用结合能量得分(ES)和变异函数得分(VS)的复合损失函数进行训练。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(dualGNN)进行多变量后处理，结合能量得分(ES)和变异函数得分(VS)作为复合损失函数。在两个数据集上评估该方法：WRF基于的智利北部太阳辐射预报和ECMWF中欧能见度预报。&lt;h4&gt;主要发现&lt;/h4&gt;dualGNN始终优于所有基于经验copula的后处理预报，与仅使用连续排序概率得分(CRPS)或仅使用ES训练的图神经网络相比有显著改进。对于WRF预报，dualGNN能有效恢复空间关系；对于能见度预报，使用CRPS、ES或ES-VS组合训练的GNN优于校准参考。&lt;h4&gt;结论&lt;/h4&gt;多变量后处理对于恢复真实的变量间或时空依赖关系至关重要。dualGNN方法在多变量验证指标上表现优异，特别是在处理太阳辐射预报时能够有效恢复空间关系。&lt;h4&gt;翻译&lt;/h4&gt;集合预报系统通过提供未来状态的概率估计推动了气象学的发展，支持从可再生能源生产到运输安全等多种应用应用。尽管如此，系统性偏差往往仍然存在，使得统计后处理变得必不可少。传统的参数化后处理技术和基于机器学习的方法可以在特定位置和提前期产生校准的预测分布，但往往难以捕捉跨预报维度的依赖关系。为了解决这个问题，多变量后处理方法——如ensemble copula coupling和Schaake shuffle——被广泛地应用于第二步，以恢复真实的变量间或时空依赖关系。本研究的目标是使用图神经网络(dualGNN)对集合预报进行多变量后处理，该网络使用结合能量得分(ES)和变异函数得分(VS)的复合损失函数进行训练。该方法在两个数据集上进行了评估：基于WRF的智利北部太阳辐射预报和ECMWF的中欧能见度预报。根据评估的多变量验证指标，dualGNN始终优于所有基于经验copula的后处理预报，并且与仅使用连续排序概率得分(CRPS)或仅使用ES训练的图神经网络相比显示出显著改进。此外，对于WRF预报，dualGNN预报的排序结构捕获了有价值的依赖信息，能够比原始数值天气预报集合或历史观测排序结构更有效地恢复空间关系。相比之下，对于能见度预报，使用CRPS、ES或ES-VS组合训练的GNN优于校准参考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensemble forecasting systems have advanced meteorology by providingprobabilistic estimates of future states, supporting applications fromrenewable energy production to transportation safety. Nonetheless, systematicbiases often persist, making statistical post-processing essential. Traditionalparametric post-processing techniques and machine learning-based methods canproduce calibrated predictive distributions at specific locations and leadtimes, yet often struggle to capture dependencies across forecast dimensions.To address this, multivariate post-processing methods-such as ensemble copulacoupling and the Schaake shuffle-are widely applied in a second step to restorerealistic inter-variable or spatio-temporal dependencies. The aim of this studyis the multivariate post-processing of ensemble forecasts using a graph neuralnetwork (dualGNN) trained with a composite loss function that combines theenergy score (ES) and the variogram score (VS). The method is evaluated on twodatasets: WRF-based solar irradiance forecasts over northern Chile and ECMWFvisibility forecasts for Central Europe. The dualGNN consistently outperformsall empirical copula-based post-processed forecasts and shows significantimprovements compared to graph neural networks trained solely on either thecontinuous ranked probability score (CRPS) or the ES, according to theevaluated multivariate verification metrics. Furthermore, for the WRFforecasts, the rank-order structure of the dualGNN forecasts captures valuabledependency information, enabling a more effective restoration of spatialrelationships than either the raw numerical weather prediction ensemble orhistorical observational rank structures. By contrast, for the visibilityforecasts, the GNNs trained on CRPS, ES, or the ES-VS combination outperformthe calibrated reference.</description>
      <author>example@mail.com (Mária Lakatos)</author>
      <guid isPermaLink="false">2509.02784v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>HydroGAT: Distributed Heterogeneous Graph Attention Transformer for Spatiotemporal Flood Prediction</title>
      <link>http://arxiv.org/abs/2509.02481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The 33rd ACM International Conference on Advances in  Geographic Information Systems (SIGSPATIAL 25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HydroGAT的时空网络模型，用于精确的洪水预测，通过异构流域图和分布式训练方法解决了传统方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;精确的洪水预测对水资源管理具有挑战性，需要建模局部时变径流驱动因素和河流网络中的复杂空间相互作用。传统数据驱动方法忽略区域拓扑信息，而现有GNN模型因训练成本问题被迫降低分辨率，且多数方法无法同时捕捉时空依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时空相互作用的洪水预测模型，实现高分辨率流域规模的训练，并提供可解释的预测结果。&lt;h4&gt;方法&lt;/h4&gt;引入异构流域图，其中每个陆地和河流像素作为节点通过物理水文流向和流域间关系连接；提出HydroGAT时空网络，自适应学习局部时间重要性和最具影响力的上游位置；开发分布式数据并行训练管道，支持在超级计算机上高效扩展。&lt;h4&gt;主要发现&lt;/h4&gt;在美国中西部两个流域的评估中，HydroGAT实现了高达0.97的NSE和0.96的KGE，PBIAS保持在±5%以内，提供了可解释的注意力图揭示流域间影响；分布式训练实现了跨机器高达15倍的加速。&lt;h4&gt;结论&lt;/h4&gt;HydroGAT模型在洪水预测任务中表现出色，同时提供了可解释性和高效率的分布式训练能力，为高分辨率流域建模提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;精确的洪水预测对水资源管理仍然是一个挑战，因为它需要对局部、时变的径流驱动因素（如降雨引起的峰值、基流趋势）和河流网络中复杂的空间相互作用进行建模。传统的数据驱动方法，如卷积网络和基于序列的模型，忽略了区域的拓扑信息。图神经网络(GNN)可以沿河流网络精确传播信息，非常适合学习水文路由。然而，最先进的基于GNN的洪水预测模型将像素合并为粗略的流域多边形，因为随着图大小和更高分辨率的增加，训练成本会爆炸式增长。此外，大多数现有方法分别处理空间和时间依赖性，要么仅在空间图上应用GNN，要么仅在时间序列上应用transformer，因此无法同时捕捉对准确洪水预测至关重要的时空相互作用。我们引入了一个异构流域图，其中每个陆地和河流像素都是通过物理水文流向和流域间关系连接的节点。我们提出了HydroGAT，一个时空网络，可以自适应地学习局部时间重要性和最具影响力的上游位置。在美国中西部的两个流域和五种基线架构上进行了评估，我们的模型在小时流量预测中实现了更高的NSE（高达0.97）、改进的KGE（高达0.96）和低偏差（PBIAS在±5%以内），同时提供了可解释的注意力图，揭示了稀疏的、结构化的流域间影响。为了支持高分辨率流域规模的训练，我们开发了一个分布式数据并行管道，可以在NERSC Perlmutter超级计算机上高效扩展到64个NVIDIA A100 GPU，展示了跨机器高达15倍的加速。我们的代码可在https://github.com/swapp-lab/HydroGAT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3748636.3764172&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate flood forecasting remains a challenge for water-resource management,as it demands modeling of local, time-varying runoff drivers (e.g.,rainfall-induced peaks, baseflow trends) and complex spatial interactionsacross a river network. Traditional data-driven approaches, such asconvolutional networks and sequence-based models, ignore topologicalinformation about the region. Graph Neural Networks (GNNs) propagateinformation exactly along the river network, which is ideal for learninghydrological routing. However, state-of-the-art GNN-based flood predictionmodels collapse pixels to coarse catchment polygons as the cost of trainingexplodes with graph size and higher resolution. Furthermore, most existingmethods treat spatial and temporal dependencies separately, either applyingGNNs solely on spatial graphs or transformers purely on temporal sequences,thus failing to simultaneously capture spatiotemporal interactions critical foraccurate flood prediction. We introduce a heterogenous basin graph where everyland and river pixel is a node connected by physical hydrological flowdirections and inter-catchment relationships. We propose HydroGAT, aspatiotemporal network that adaptively learns local temporal importance and themost influential upstream locations. Evaluated in two Midwestern US basins andacross five baseline architectures, our model achieves higher NSE (up to 0.97),improved KGE (up to 0.96), and low bias (PBIAS within $\pm$5%) in hourlydischarge prediction, while offering interpretable attention maps that revealsparse, structured intercatchment influences. To support high-resolutionbasin-scale training, we develop a distributed data-parallel pipeline thatscales efficiently up to 64 NVIDIA A100 GPUs on NERSC Perlmutter supercomputer,demonstrating up to 15x speedup across machines. Our code is available athttps://github.com/swapp-lab/HydroGAT.</description>
      <author>example@mail.com (Aishwarya Sarkar, Autrin Hakimi, Xiaoqiong Chen, Hai Huang, Chaoqun Lu, Ibrahim Demir, Ali Jannesari)</author>
      <guid isPermaLink="false">2509.02481v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>NOOUGAT: Towards Unified Online and Offline Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2509.02111v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NOOUGAT是一种新型的多目标跟踪器，能够处理任意时间长度的跟踪任务，成功统一了在线和离线跟踪方法，解决了长期存在的跟踪领域划分问题。&lt;h4&gt;背景&lt;/h4&gt;在线和离线多目标跟踪之间长期存在划分，导致解决方案碎片化，无法满足实际部署场景中的灵活时间需求。在线跟踪器依赖逐帧手工关联策略，难以处理长期遮挡；离线方法可覆盖更大时间间隔，但仍依赖启发式拼接处理任意长序列。&lt;h4&gt;目的&lt;/h4&gt;引入NOOUGAT，第一个设计用于操作任意时间跨度的跟踪器，解决在线和离线跟踪方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;NOOUGAT利用统一的图神经网络框架处理不重叠的子剪辑，通过新的自回归长期跟踪层融合这些子剪辑。子剪辑大小控制延迟和时间上下文之间的权衡，支持从逐帧到批处理的广泛部署场景。&lt;h4&gt;主要发现&lt;/h4&gt;NOOUGAT在两种跟踪模式下都达到最先进性能：在线模式下在DanceTrack上AssA提高+2.3，在SportsMOT上提高+9.2，在MOT20上提高+5.0；离线模式下有更大的性能提升。&lt;h4&gt;结论&lt;/h4&gt;NOOUGAT成功统一了在线和离线跟踪方法，提供了灵活的时间跨度控制，适应各种实际部署场景，在多个基准测试上实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;长期以来，在线和离线多目标跟踪之间的划分导致了碎片化的解决方案，无法满足实际部署场景中的灵活时间需求。当前在线跟踪器依赖逐帧手工关联策略，难以处理长期遮挡，而离线方法可以覆盖更大的时间间隔，但仍依赖启发式拼接来处理任意长序列。在本文中，我们引入了NOOUGAT，第一个设计用于操作任意时间长度的跟踪器。NOOUGAT利用统一的图神经网络框架处理不重叠的子剪辑，并通过新的自回归长期跟踪层融合它们。子剪辑大小控制延迟和时间上下文之间的权衡，支持从逐帧到批处理的广泛部署场景。NOOUGAT在两种跟踪模式下都达到了最先进的性能，在线模式下在DanceTrack上AssA提高+2.3，在SportsMOT上提高+9.2，在MOT20上提高+5.0，离线模式下有更大的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The long-standing division between \textit{online} and \textit{offline}Multi-Object Tracking (MOT) has led to fragmented solutions that fail toaddress the flexible temporal requirements of real-world deployment scenarios.Current \textit{online} trackers rely on frame-by-frame hand-craftedassociation strategies and struggle with long-term occlusions, whereas\textit{offline} approaches can cover larger time gaps, but still rely onheuristic stitching for arbitrarily long sequences. In this paper, we introduceNOOUGAT, the first tracker designed to operate with arbitrary temporalhorizons. NOOUGAT leverages a unified Graph Neural Network (GNN) framework thatprocesses non-overlapping subclips, and fuses them through a novelAutoregressive Long-term Tracking (ALT) layer. The subclip size controls thetrade-off between latency and temporal context, enabling a wide range ofdeployment scenarios, from frame-by-frame to batch processing. NOOUGAT achievesstate-of-the-art performance across both tracking regimes, improving\textit{online} AssA by +2.3 on DanceTrack, +9.2 on SportsMOT, and +5.0 onMOT20, with even greater gains in \textit{offline} mode.</description>
      <author>example@mail.com (Benjamin Missaoui, Orcun Cetintas, Guillem Brasó, Tim Meinhardt, Laura Leal-Taixé)</author>
      <guid isPermaLink="false">2509.02111v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Second-Order Tensorial Partial Differential Equations on Graphs</title>
      <link>http://arxiv.org/abs/2509.02015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了二阶TPDEGs(So-TPDEGs)，提出了首个有理论基础的二阶连续乘积图神经网络框架，通过利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时保留高频信息，为连续图学习提供了坚实的理论基础。&lt;h4&gt;背景&lt;/h4&gt;处理位于多个交互图上的数据在实际应用中越来越重要，但现有方法大多局限于离散图滤波。图上的张量偏微分方程(TPDEGs)为在连续环境中建模多域数据提供了原则性框架。&lt;h4&gt;目的&lt;/h4&gt;引入二阶TPDEGs(So-TPDEGs)并开发首个有理论基础的二阶连续乘积图神经网络框架，以解决现有方法在处理复杂、多尺度和异构结构时的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时自然地保留高频信息，并提供对图扰动下稳定性和谱特性过平滑行为的严格理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;二阶TPDEGs能够有效保留高频信息并加快信息传播，比现有的一阶方法更适合捕获复杂、多尺度和异构结构。&lt;h4&gt;结论&lt;/h4&gt;理论结果为推进多个实际领域的连续图学习提供了坚实的基础，使基于TPDEGs的方法能够更好地处理复杂的图数据。&lt;h4&gt;翻译&lt;/h4&gt;处理位于多个交互（乘积）图上的数据在实际应用中日益重要，然而现有方法大多局限于离散图滤波。图上的张量偏微分方程（TPDEGs）为在连续环境中建模此类多域数据提供了原则性框架。然而，当前连续方法仅限于使用一阶导数，这往往会抑制高频信号并减慢信息传播。这使得基于TPDEGs的方法在捕获复杂、多尺度和异构结构方面效果较差。在本文中，我们引入了二阶TPDEGs（So-TPDEGs），并提出了首个有理论基础的二阶连续乘积图神经网络框架。我们的方法利用笛卡尔乘积图中余弦核的可分离性实现高效谱分解，同时自然地保留高频信息。我们提供了关于图扰动下稳定性和谱特性过平滑行为的严格理论分析。我们的理论结果为推进多个实际领域的连续图学习提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Processing data that lies on multiple interacting (product) graphs isincreasingly important in practical applications, yet existing methods aremostly restricted to discrete graph filtering. Tensorial partial differentialequations on graphs (TPDEGs) offer a principled framework for modeling suchmultidomain data in a continuous setting. However, current continuousapproaches are limited to first-order derivatives, which tend to dampenhigh-frequency signals and slow down information propagation. This makes theseTPDEGs-based approaches less effective for capturing complex, multi-scale, andheterophilic structures. In this paper, we introduce second-order TPDEGs(So-TPDEGs) and propose the first theoretically grounded framework forsecond-order continuous product graph neural networks. Our approach leveragesthe separability of cosine kernels in Cartesian product graphs to implementefficient spectral decomposition, while naturally preserving high-frequencyinformation. We provide rigorous theoretical analyses of stability under graphperturbations and over-smoothing behavior regarding spectral properties. Ourtheoretical results establish a robust foundation for advancing continuousgraph learning across multiple practical domains.</description>
      <author>example@mail.com (Aref Einizade, Fragkiskos D. Malliaros, Jhony H. Giraldo)</author>
      <guid isPermaLink="false">2509.02015v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge distillation as a pathway toward next-generation intelligent ecohydrological modeling systems</title>
      <link>http://arxiv.org/abs/2509.01972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的三阶段框架，整合基于过程的模型与机器学习方法，通过知识蒸馏技术逐步嵌入人工智能，应用于生态水文过程模拟，实现了高效、准确的建模和决策支持。&lt;h4&gt;背景&lt;/h4&gt;生态水文过程模拟对于理解复杂环境系统和指导可持续管理至关重要，特别是在气候变化和人类压力加剧的背景下。基于过程的模型具有物理真实性但存在结构僵化、计算成本高和校准复杂的问题；机器学习方法高效灵活但缺乏可解释性和可迁移性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够结合基于过程模型和机器学习方法优势的统一框架，用于生态水文过程模拟，提高计算效率、预测准确性和决策支持能力。&lt;h4&gt;方法&lt;/h4&gt;提出一个统一的三阶段框架：第一阶段(行为蒸馏)通过代理学习和模型简化增强过程模型；第二阶段(结构蒸馏)将过程方程重新表述为图神经网络中的模块化组件；第三阶段(认知蒸馏)使用'眼睛-大脑-双手-嘴巴'架构将专家推理嵌入智能建模代理。&lt;h4&gt;主要发现&lt;/h4&gt;在Samish流域的演示中，该框架能够重现基于过程的模型输出，提高预测准确性，并支持基于场景的决策制定，证明了其在生态水文建模中的适用性。&lt;h4&gt;结论&lt;/h4&gt;该框架为下一代智能生态水文建模系统提供了可扩展和可迁移的途径，并有可能扩展到其他基于过程的领域。&lt;h4&gt;翻译&lt;/h4&gt;模拟生态水文过程对于理解复杂环境系统和指导可持续管理至关重要，在气候变化和人类压力不断加剧的背景下。基于过程的模型提供了物理真实性，但可能存在结构僵化、计算成本高和校准复杂的问题，而机器学习方法高效灵活却往往缺乏可解释性和可迁移性。我们提出一个统一的三阶段框架，整合基于过程的模型与机器学习，并通过知识蒸馏逐步将它们嵌入到人工智能中。第一阶段行为蒸馏通过代理学习和模型简化增强过程模型，以较低的计算成本捕捉关键动态。第二阶段结构蒸馏将过程方程重新表述为图神经网络中的模块化组件，实现多尺度表示并无缝集成机器学习模型。第三阶段认知蒸馏使用眼睛-大脑-双手-嘴巴架构将专家推理和自适应决策嵌入到智能建模代理中。Samish流域的演示突显了该框架在生态水文建模中的适用性，表明它可以重现基于过程的模型输出，提高预测准确性，并支持基于场景的决策制定。该框架为下一代智能生态水文建模系统提供了可扩展和可迁移的途径，有潜力扩展到其他基于过程的领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating ecohydrological processes is essential for understanding complexenvironmental systems and guiding sustainable management amid acceleratingclimate change and human pressures. Process-based models provide physicalrealism but can suffer from structural rigidity, high computational costs, andcomplex calibration, while machine learning (ML) methods are efficient andflexible yet often lack interpretability and transferability. We propose aunified three-phase framework that integrates process-based models with ML andprogressively embeds them into artificial intelligence (AI) through knowledgedistillation. Phase I, behavioral distillation, enhances process models viasurrogate learning and model simplification to capture key dynamics at lowercomputational cost. Phase II, structural distillation, reformulates processequations as modular components within a graph neural network (GNN), enablingmultiscale representation and seamless integration with ML models. Phase III,cognitive distillation, embeds expert reasoning and adaptive decision-makinginto intelligent modeling agents using the Eyes-Brain-Hands-Mouth architecture.Demonstrations for the Samish watershed highlight the framework's applicabilityto ecohydrological modeling, showing that it can reproduce process-based modeloutputs, improve predictive accuracy, and support scenario-baseddecision-making. The framework offers a scalable and transferable pathwaytoward next-generation intelligent ecohydrological modeling systems, with thepotential extension to other process-based domains.</description>
      <author>example@mail.com (Long Jiang, Yang Yang, Ting Fong May Chui, Morgan Thornwell, Hoshin Vijai Gupta)</author>
      <guid isPermaLink="false">2509.01972v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Causal representation learning from network data</title>
      <link>http://arxiv.org/abs/2509.01916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GraCE-VAE的框架，用于从软干预中进行因果解缠，特别适用于非独立同分布数据场景，其中包含网络数据形式的结构化上下文。&lt;h4&gt;背景&lt;/h4&gt;先前研究主要从独立同分布数据的角度探讨因果解缠问题。在线性干预忠实度和可获取观测与干预数据的假设下，因果解缠是可识别的。&lt;h4&gt;目的&lt;/h4&gt;开发一个适用于非独立同分布设置的框架，整合结构化上下文（网络数据）来提升因果解缠的性能。&lt;h4&gt;方法&lt;/h4&gt;GraCE-VAE框架结合了基于差异的变分自编码器与图神经网络，能够同时恢复真实的潜在因果图和干预效果。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明，从独立同分布数据中得出的可识别性理论结果在非独立同分布设置中仍然成立。实验评估表明，利用结构化上下文可以显著提升因果解缠的性能。&lt;h4&gt;结论&lt;/h4&gt;GraCE-VAE框架成功地将因果解缠扩展到非独立同分布数据场景，通过整合网络结构信息，在基因扰动数据集上表现出优于现有方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;从软干预中进行因果解缠在线性干预忠实度假设和观测与干预数据可获取的情况下是可识别的。先前研究从独立同分布数据的角度探讨了这一问题。本文提出了GraCE-VAE框架，用于非独立同分布设置，其中存在网络数据形式的结构化上下文。GraCE-VAE将基于差异的变分自编码器与图神经网络相结合，共同恢复真实的潜在因果图和干预效果。我们展示了从独立同分布数据得出的可识别性理论结果在我们的设置中仍然成立。我们还在三个基因扰动数据集上对GraCE-VAE与最先进的基线进行了实证评估，展示了利用结构化上下文进行因果解缠的影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal disentanglement from soft interventions is identifiable under theassumptions of linear interventional faithfulness and availability of bothobservational and interventional data. Previous research has looked into thisproblem from the perspective of i.i.d. data. Here, we develop a framework,GraCE-VAE, for non-i.i.d. settings, in which structured context in the form ofnetwork data is available. GraCE-VAE integrates discrepancy-based variationalautoencoders with graph neural networks to jointly recover the true latentcausal graph and intervention effects. We show that the theoretical results ofidentifiability from i.i.d. data hold in our setup. We also empiricallyevaluate GraCE-VAE against state-of-the-art baselines on three geneticperturbation datasets to demonstrate the impact of leveraging structuredcontext for causal disentanglement.</description>
      <author>example@mail.com (Jifan Zhang, Michelle M. Li, Elena Zheleva)</author>
      <guid isPermaLink="false">2509.01916v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Single-Cell RNA-seq Embedding through Gene Expression and Data-Driven Gene-Gene Interaction Integration</title>
      <link>http://arxiv.org/abs/2509.02639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 9 figures, article&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的单细胞RNA测序数据嵌入方法，整合基因表达水平和基因间相互作用，提高了细胞分析的准确性和全面性。&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序技术为研究细胞异质性提供了前所未有的机会，但数据的高维度和技术噪声带来了分析挑战。现有方法主要关注基因表达水平，忽视了基因间相互作用对细胞身份和功能的关键影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时考虑基因表达水平和基因间相互作用的嵌入方法，提供更全面的细胞状态表示。&lt;h4&gt;方法&lt;/h4&gt;结合随机森林模型构建细胞-叶图(CLG)捕获基因调控关系，同时构建K-近邻图(KNNG)表示细胞间表达相似性，将两者组合成增强的细胞-叶图(ECLG)作为图神经网络的输入，计算细胞嵌入表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在多个数据集上表现出色，显著提高了稀有细胞群的检测能力，并改善了可视化、聚类和轨迹推断等下游分析效果。&lt;h4&gt;结论&lt;/h4&gt;这种集成基因表达和相互作用的方法代表了单细胞数据分析的重要进展，为理解细胞多样性和动态变化提供了更完整的框架。&lt;h4&gt;翻译&lt;/h4&gt;单细胞RNA测序(scRNA-seq)为细胞异质性提供了前所未有的见解，使能够在单细胞分辨率下详细分析复杂的生物系统。然而，scRNA-seq数据固有的高维度和技术噪声带来了显著的分析挑战。虽然当前的嵌入方法主要关注基因表达水平，但它们常常忽视了控制细胞身份和功能的关键基因间相互作用。为了解决这一局限，我们提出了一种新的嵌入方法，整合了基因表达谱和数据驱动的基因间相互作用。我们的方法首先使用随机森林模型构建细胞-叶图(CLG)以捕获基因之间的调控关系，同时构建K-近邻图(KNNG)来表示细胞之间的表达相似性。然后将这些图组合成增强的细胞-叶图(ECLG)，作为图神经网络的输入以计算细胞嵌入。通过整合表达水平和基因间相互作用，我们的方法提供了细胞状态的更全面表示。在多个数据集上的广泛评估表明，我们的方法增强了稀有细胞群的检测，并改善了可视化、聚类和轨迹推断等下游分析。这种集成方法代表了单细胞数据分析的重大进展，为理解细胞多样性和动态变化提供了更完整的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.compbiomed.2025.109880&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) provides unprecedented insights intocellular heterogeneity, enabling detailed analysis of complex biologicalsystems at single-cell resolution. However, the high dimensionality andtechnical noise inherent in scRNA-seq data pose significant analyticalchallenges. While current embedding methods focus primarily on gene expressionlevels, they often overlook crucial gene-gene interactions that govern cellularidentity and function. To address this limitation, we present a novel embeddingapproach that integrates both gene expression profiles and data-drivengene-gene interactions. Our method first constructs a Cell-Leaf Graph (CLG)using random forest models to capture regulatory relationships between genes,while simultaneously building a K-Nearest Neighbor Graph (KNNG) to representexpression similarities between cells. These graphs are then combined into anEnriched Cell-Leaf Graph (ECLG), which serves as input for a graph neuralnetwork to compute cell embeddings. By incorporating both expression levels andgene-gene interactions, our approach provides a more comprehensiverepresentation of cellular states. Extensive evaluation across multipledatasets demonstrates that our method enhances the detection of rare cellpopulations and improves downstream analyses such as visualization, clustering,and trajectory inference. This integrated approach represents a significantadvance in single-cell data analysis, offering a more complete framework forunderstanding cellular diversity and dynamics.</description>
      <author>example@mail.com (Hojjat Torabi Goudarzi, Maziyar Baran Pouyan)</author>
      <guid isPermaLink="false">2509.02639v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>TransGAT: Transformer-Based Graph Neural Networks for Multi-Dimensional Automated Essay Scoring</title>
      <link>http://arxiv.org/abs/2509.01640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TransGAT是一种创新的自动作文评分方法，通过结合微调的Transformer模型与图神经网络(GNN)进行分析性评分，解决了现有方法在捕捉上下文意义和评估具体写作方面的局限性，实验证明其性能优于基线模型。&lt;h4&gt;背景&lt;/h4&gt;作文写作是学生评估的关键组成部分，但人工评分劳动量大且不一致。自动作文评分(AES)虽有前景，但当前方法存在局限性，包括使用无法捕捉上下文意义的静态词嵌入，以及依赖整体评分而忽略具体写作方面。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉上下文意义并评估具体写作方面的自动作文评分方法，解决现有AES方法的两个主要问题：静态词嵌入无法捕捉多义词的上下文意义，以及整体评分忽略了语法、词汇和连贯性等具体写作方面。&lt;h4&gt;方法&lt;/h4&gt;提出TransGAT，一种结合微调Transformer模型与图神经网络(GNN)的新方法。TransGAT将Transformer的上下文理解与图注意力网络(GAT)的关系建模能力相结合，采用双流预测：每个Transformer(BERT、RoBERTa和DeBERTaV3)与单独的GAT配对，第一个流生成作文级别预测，第二个流将GAT应用于Transformer标记嵌入（边从句法依赖关系构建），最后融合两个流的预测产生最终分析性分数。&lt;h4&gt;主要发现&lt;/h4&gt;在ELLIPSE数据集上的实验表明，TransGAT优于基线模型，在所有分析性评分维度上平均实现了0.854的二次加权Kappa(QWK)值。&lt;h4&gt;结论&lt;/h4&gt;TransGAT有潜力推动自动作文评分系统的发展，为教育评估提供更准确、一致的分析性评分。&lt;h4&gt;翻译&lt;/h4&gt;作文写作是学生评估的关键组成部分，然而人工评分劳动量大且不一致。自动作文评分(AES)提供了一个有前景的替代方案，但当前方法存在局限性。最近的研究已将图神经网络(GNN)纳入AES，但使用的静态词嵌入无法捕捉上下文意义，特别是对多义词而言。此外，许多方法依赖整体评分，忽略了语法、词汇和连贯性等具体写作方面。为解决这些挑战，本研究提出了TransGAT，一种将微调Transformer模型与GNN结合用于分析性评分的新方法。TransGAT结合了Transformer的上下文理解和图注意力网络(GAT)的关系建模能力。它通过将每个微调的Transformer(BERT、RoBERTa和DeBERTaV3)与单独的GAT配对进行双流预测。在每对中，第一个流生成作文级别的预测，而第二个流将GAT应用于Transformer标记嵌入，边从句法依赖关系构建。然后模型融合两个流的预测以产生最终的分析性分数。在ELLIPSE数据集上的实验表明，TransGAT优于基线模型，在所有分析性评分维度上平均实现了0.854的二次加权Kappa(QWK)。这些发现突显了TransGAT推动AES系统发展的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Essay writing is a critical component of student assessment, yet manualscoring is labor-intensive and inconsistent. Automated Essay Scoring (AES)offers a promising alternative, but current approaches face limitations. Recentstudies have incorporated Graph Neural Networks (GNNs) into AES using staticword embeddings that fail to capture contextual meaning, especially forpolysemous words. Additionally, many methods rely on holistic scoring,overlooking specific writing aspects such as grammar, vocabulary, and cohesion.To address these challenges, this study proposes TransGAT, a novel approachthat integrates fine-tuned Transformer models with GNNs for analytic scoring.TransGAT combines the contextual understanding of Transformers with therelational modeling strength of Graph Attention Networks (GAT). It performstwo-stream predictions by pairing each fine-tuned Transformer (BERT, RoBERTa,and DeBERTaV3) with a separate GAT. In each pair, the first stream generatesessay-level predictions, while the second applies GAT to Transformer tokenembeddings, with edges constructed from syntactic dependencies. The model thenfuses predictions from both streams to produce the final analytic score.Experiments on the ELLIPSE dataset show that TransGAT outperforms baselinemodels, achieving an average Quadratic Weighted Kappa (QWK) of 0.854 across allanalytic scoring dimensions. These findings highlight the potential of TransGATto advance AES systems.</description>
      <author>example@mail.com (Hind Aljuaid, Areej Alhothali, Ohoud Al-Zamzami, Hussein Assalahi)</author>
      <guid isPermaLink="false">2509.01640v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>What Expressivity Theory Misses: Message Passing Complexity for GNNs</title>
      <link>http://arxiv.org/abs/2509.01254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究批判了当前图神经网络研究中过度关注表达性的倾向，提出了一种新的消息传递复杂度(MPC)框架，更好地连接理论与实践，为理解和改进GNN架构提供了更强大而细致的工具。&lt;h4&gt;背景&lt;/h4&gt;表达性理论已成为分析图神经网络的主要框架，新的模型不断追求更高的表达性，能够区分更多类型的图结构。&lt;h4&gt;目的&lt;/h4&gt;指出表达性理论的局限性，并提出一种更符合实际应用的评估方法，以弥补理论与实践之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出消息传递复杂度(MPC)作为连续性度量，量化GNN架构通过消息传递解决特定任务的难度，同时保留表达性理论的理论不可能性结果。&lt;h4&gt;主要发现&lt;/h4&gt;MPC能够捕捉实际限制如过度压缩(over-squashing)，其理论预测与实际性能相关，成功解释了不同架构的成功与失败。&lt;h4&gt;结论&lt;/h4&gt;MPC超越了表达性理论，为理解和改进图神经网络架构提供了更强大而细致的框架。&lt;h4&gt;翻译&lt;/h4&gt;表达性理论，表征GNN能够区分哪些图，已成为分析GNN的主导框架，新模型追求更高的表达性。然而，我们认为这种关注是误导性的：首先，对于大多数实际任务，更高的表达性并非必需，因为这些任务很少需要超越基本WL测试的表达性。其次，表达性理论的二元表征和理想化假设未能反映GNN的实际能力。为克服这些局限，我们提出消息传递复杂度(MPC)：一种连续度量，量化GNN架构通过消息传递解决给定任务的难度。MPC捕获了过度压缩等实际限制，同时保留表达性理论的理论不可能性结果，有效缩小了理论与实践之间的差距。通过对基本GNN任务的广泛验证，我们表明MPC的理论预测与实证性能相关，成功解释了架构的成功与失败。因此，MPC超越了表达性理论，为理解和改进GNN架构提供了更强大而细致的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Expressivity theory, characterizing which graphs a GNN can distinguish, hasbecome the predominant framework for analyzing GNNs, with new models strivingfor higher expressivity. However, we argue that this focus is misguided: First,higher expressivity is not necessary for most real-world tasks as these tasksrarely require expressivity beyond the basic WL test. Second, expressivitytheory's binary characterization and idealized assumptions fail to reflectGNNs' practical capabilities. To overcome these limitations, we propose MessagePassing Complexity (MPC): a continuous measure that quantifies the difficultyfor a GNN architecture to solve a given task through message passing. MPCcaptures practical limitations like over-squashing while preserving thetheoretical impossibility results from expressivity theory, effectivelynarrowing the gap between theory and practice. Through extensive validation onfundamental GNN tasks, we show that MPC's theoretical predictions correlatewith empirical performance, successfully explaining architectural successes andfailures. Thereby, MPC advances beyond expressivity theory to provide a morepowerful and nuanced framework for understanding and improving GNNarchitectures.</description>
      <author>example@mail.com (Niklas Kemper, Tom Wollschläger, Stephan Günnemann)</author>
      <guid isPermaLink="false">2509.01254v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>ADMP-GNN: Adaptive Depth Message Passing GNN</title>
      <link>http://arxiv.org/abs/2509.01170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图神经网络在各种图学习任务中表现优异，但其对所有节点使用固定数量的消息传递步骤，忽略了节点的差异性。研究证明不同特性的节点需要不同层数的消息传递，为此提出ADMP-GNN框架，动态调整每个节点的消息传递层数，在节点分类任务中展现出优于基准GNN模型的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已被证明在各种图学习任务中非常有效。&lt;h4&gt;目的&lt;/h4&gt;解决GNN对所有节点使用固定数量消息传递步骤的问题，根据节点不同特性动态调整消息传递层数，以提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出自适应深度消息传递图神经网络(ADMP-GNN)框架，该框架动态调整每个节点的消息传递层数，适用于任何遵循消息传递方案的模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界数据分析和合成数据集实验发现，具有不同特性的节点需要不同数量的消息传递层才能达到最佳性能。&lt;h4&gt;结论&lt;/h4&gt;ADMP-GNN通过动态调整每个节点的消息传递层数，在节点分类任务中表现出比基准GNN模型更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已被证明在各种图学习任务中非常有效。GNN的一个关键特征是它们对图中所有节点使用固定数量的消息传递步骤，而不管每个节点不同的计算需求和特性。通过对真实世界数据的经验分析，我们证明了对于具有不同特性的节点，最佳的消息传递层数量是不同的。这一发现也在合成数据集的实验中得到了进一步支持。为此，我们提出了自适应深度消息传递图神经网络(ADMP-GNN)，这是一个新框架，可以动态调整每个节点的消息传递层数，从而提高性能。这种方法适用于任何遵循消息传递方案的模型。我们在节点分类任务上评估了ADMP-GNN，并观察到其性能优于基准GNN模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have proven to be highly effective in variousgraph learning tasks. A key characteristic of GNNs is their use of a fixednumber of message-passing steps for all nodes in the graph, regardless of eachnode's diverse computational needs and characteristics. Through empiricalreal-world data analysis, we demonstrate that the optimal number ofmessage-passing layers varies for nodes with different characteristics. Thisfinding is further supported by experiments conducted on synthetic datasets. Toaddress this, we propose Adaptive Depth Message Passing GNN (ADMP-GNN), a novelframework that dynamically adjusts the number of message passing layers foreach node, resulting in improved performance. This approach applies to anymodel that follows the message passing scheme. We evaluate ADMP-GNN on the nodeclassification task and observe performance improvements over baseline GNNmodels.</description>
      <author>example@mail.com (Yassine Abbahaddou, Fragkiskos D. Malliaros, Johannes F. Lutzeyer, Michalis Vazirgiannis)</author>
      <guid isPermaLink="false">2509.01170v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>EZhouNet:A framework based on graph neural network and anchor interval for the respiratory sound event detection</title>
      <link>http://arxiv.org/abs/2509.01153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络的框架，用于呼吸音事件检测，解决了现有方法在处理可变长度音频和精确时间定位方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;听诊是诊断呼吸系统和肺部疾病的关键方法，但过程主观且专家间存在差异。虽然已有基于深度学习的呼吸音分类方法，但呼吸音事件检测研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有呼吸音事件检测方法的局限性，包括难以直接学习区间边界、只能处理固定长度音频，以及未充分利用呼吸音位置信息的问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于图神经网络的框架，带有锚定区间，能够处理可变长度音频并为异常呼吸音事件提供更精确的时间定位。&lt;h4&gt;主要发现&lt;/h4&gt;在SPRSound2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，整合呼吸位置信息提高了异常声音之间的区分度。&lt;h4&gt;结论&lt;/h4&gt;该方法提高了呼吸音检测的灵活性和适用性，为临床呼吸系统疾病的早期诊断提供了更客观、准确的工具。&lt;h4&gt;翻译&lt;/h4&gt;听诊是诊断呼吸系统和肺部疾病的关键方法，依赖于熟练的医疗专业人员。然而，这个过程通常是主观的，专家之间存在差异。因此，出现了许多基于深度学习的自动分类方法，大多数专注于呼吸音分类。相比之下，呼吸音事件检测的研究仍然有限。现有的声音事件检测方法通常依赖于帧级预测，然后通过后处理生成事件级输出，使得区间边界难以直接学习。此外，许多方法只能处理固定长度的音频，限制了它们对可变长度呼吸音的适用性。此外，呼吸音位置信息对检测性能的影响尚未得到充分探索。为了解决这些问题，我们提出了一种基于图神经网络的框架，带有锚定区间，能够处理可变长度音频，并为异常呼吸音事件提供更精确的时间定位。我们的方法提高了呼吸音检测的灵活性和适用性。在SPRSound2024和HF Lung V1数据集上的实验证明了所提出方法的有效性，整合呼吸位置信息提高了异常声音之间的区分度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.bspc.2025.108491&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auscultation is a key method for early diagnosis of respiratory and pulmonarydiseases, relying on skilled healthcare professionals. However, the process isoften subjective, with variability between experts. As a result, numerous deeplearning-based automatic classification methods have emerged, most of whichfocus on respiratory sound classification. In contrast, research on respiratorysound event detection remains limited. Existing sound event detection methodstypically rely on frame-level predictions followed by post-processing togenerate event-level outputs, making interval boundaries challenging to learndirectly. Furthermore, many approaches can only handle fixed-length audio, lim-iting their applicability to variable-length respiratory sounds. Additionally,the impact of respiratory sound location information on detection performancehas not been extensively explored. To address these issues, we propose a graphneural network-based framework with anchor intervals, capable of handlingvariable-length audio and providing more precise temporal localization forabnormal respi- ratory sound events. Our method improves both the flexibilityand applicability of respiratory sound detection. Experiments on the SPRSound2024 and HF Lung V1 datasets demonstrate the effec- tiveness of the proposedapproach, and incorporating respiratory position information enhances thediscrimination between abnormal sounds.</description>
      <author>example@mail.com (Yun Chu, Qiuhao Wang, Enze Zhou, Qian Liu, Gang Zheng)</author>
      <guid isPermaLink="false">2509.01153v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Self-Exploring Language Models for Explainable Link Forecasting on Temporal Graphs via Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2509.00975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Reasoning-Enhanced Learning for Temporal Graphs (ReaL-TG)，一个强化学习框架，用于微调大型语言模型以在真实世界时序图上进行可解释的链接预测。&lt;h4&gt;背景&lt;/h4&gt;时序图推理中的未来链接预测需要模型利用历史交互预测未来交互。传统神经网络方法表现良好但缺乏可解释性，且不能应用于未见过的图。最近研究开始探索使用大型语言模型进行图推理，但大多局限于静态图或小型合成时序图，且缺乏对LLM生成推理质量的评估。&lt;h4&gt;目的&lt;/h4&gt;开发一个强化学习框架，使大型语言模型能够对真实世界时序图进行可解释的链接预测，并评估其推理质量。&lt;h4&gt;方法&lt;/h4&gt;ReaL-TG使用基于结果的奖励鼓励模型从图结构中自我探索推理策略并生成直接证明预测的解释。提出新的评估协议，结合排名指标和'LLM-as-a-Judge'系统，评估推理质量和幻觉影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过微调Qwen3-4B得到的ReaL-TG-4B在排名指标上优于包括GPT-5 mini在内的更大前沿LLMs，同时产生高质量解释，这些解释得到了LLM判断和人工评估的确认。&lt;h4&gt;结论&lt;/h4&gt;ReaL-TG框架成功使大型语言模型能够进行可解释的时序图链接预测，在性能和解释质量上都表现出色。&lt;h4&gt;翻译&lt;/h4&gt;预测未来链接是时序图推理中的核心任务，需要模型利用历史交互来预测未来的交互。传统的神经方法，如时序图神经网络，表现强劲但缺乏可解释性，且不能应用于未见过的图而不需要重新训练。最近的研究开始探索使用大型语言模型进行图推理，但它们大多局限于静态图或小型合成时序图，且缺乏对LLM生成的推理质量的评估。在这项工作中，我们提出了时序图的推理增强学习(ReaL-TG)，这是一个强化学习框架，用于微调LLMs以在真实世界时序图上进行可解释的链接预测。ReaL-TG使用基于结果的奖励来鼓励模型从图结构中自我探索推理策略，并生成直接证明其预测的解释。为了评估LLM生成的推理轨迹，我们提出了一种新的评估协议，结合排名指标和'LLM-as-a-Judge'系统，该系统评估推理质量和幻觉的影响。通过在我们的框架下微调Qwen3-4B得到的ReaL-TG-4B，在排名指标上优于包括GPT-5 mini在内的更大前沿LLMs，同时产生高质量解释，这些解释得到了LLM判断和人工评估的确认。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting future links is a central task in temporal graph (TG) reasoning,requiring models to leverage historical interactions to predict upcoming ones.Traditional neural approaches, such as temporal graph neural networks, achievestrong performance but lack explainability and cannot be applied to unseengraphs without retraining. Recent studies have begun to explore using largelanguage models (LLMs) for graph reasoning, but most of them are constrained tostatic graphs or small synthetic TGs and lack the evaluation of the quality ofreasoning traces generated by LLMs. In this work, we present Reasoning-EnhancedLearning for Temporal Graphs (ReaL-TG), a reinforcement learning framework thatfine-tunes LLMs to perform explainable link forecasting on real-world TGs.ReaL-TG uses outcome-based reward to encourage models to self-explore reasoningstrategies from graph structure and to produce explanations that directlyjustify their predictions. To enable evaluation on LLM-generated reasoningtraces, we propose a new evaluation protocol combining ranking metrics with anLLM-as-a-Judge system that assesses both the quality of reasoning and theimpact of hallucinations. Experiments with ReaL-TG-4B, obtained by fine-tuningQwen3-4B under our framework, show that it outperforms much larger frontierLLMs, including GPT-5 mini, on ranking metrics, while producing high-qualityexplanations confirmed by both the LLM judge and human evaluation.</description>
      <author>example@mail.com (Zifeng Ding, Shenyang Huang, Zeyu Cao, Emma Kondrup, Zachary Yang, Xingyue Huang, Yuan Sui, Zhangdie Yuan, Yuqicheng Zhu, Xianglong Hu, Yuan He, Farimah Poursafaei, Michael Bronstein, Andreas Vlachos)</author>
      <guid isPermaLink="false">2509.00975v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Superposition in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2509.00928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究图神经网络(GNNs)潜在空间中的叠加现象，发现宽度增加会产生重叠相位模式，拓扑结构影响节点级特征，池化方式影响轴对齐和通道共享，浅层模型可能形成低秩嵌入。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)的解释性困难，因为消息传递混合了信号，内部通道很少与人类概念对齐。&lt;h4&gt;目的&lt;/h4&gt;研究GNN潜在空间中的叠加现象，即多个特征共享方向的情况。&lt;h4&gt;方法&lt;/h4&gt;使用具有明确图概念的控制实验，提取图级别的类条件质心和节点级别的线性探测方向，然后用简单的基不变诊断分析它们的几何结构。&lt;h4&gt;主要发现&lt;/h4&gt;在GCN/GIN/GAT模型中：增加宽度产生重叠的相位模式；拓扑结构将重叠印记到节点级特征中；池化部分地将特征重混合为任务对齐的图轴；更尖锐的池化增加轴对齐并减少通道共享；浅层模型可以陷入亚稳定的低秩嵌入。&lt;h4&gt;结论&lt;/h4&gt;这些结果将表示几何与具体的设计选择（宽度、池化和最终层激活）联系起来，并为更可解释的GNNs提出实用的方法。&lt;h4&gt;翻译&lt;/h4&gt;解释图神经网络(GNNs)很困难，因为消息传递混合了信号，内部通道很少与人类概念对齐。我们直接研究GNN潜在空间中的叠加现象，即多个特征共享方向的情况。使用具有明确图概念的控制实验，我们将特征提取为(i)图级别的类条件质心和(ii)节点级别的线性探测方向，然后使用简单的基不变诊断分析它们的几何结构。在GCN/GIN/GAT中我们发现：增加宽度会产生重叠的相位模式；拓扑结构将重叠印记到节点级特征中，池化部分地将这些特征重混合为任务对齐的图轴；更尖锐的池化增加轴对齐并减少通道共享；浅层模型可以陷入亚稳定的低秩嵌入。这些结果将表示几何与具体的设计选择（宽度、池化和最终层激活）联系起来，并提出了使GNNs更具可解释性的实用方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpreting graph neural networks (GNNs) is difficult because messagepassing mixes signals and internal channels rarely align with human concepts.We study superposition, the sharing of directions by multiple features,directly in the latent space of GNNs. Using controlled experiments withunambiguous graph concepts, we extract features as (i) class-conditionalcentroids at the graph level and (ii) linear-probe directions at the nodelevel, and then analyze their geometry with simple basis-invariant diagnostics.Across GCN/GIN/GAT we find: increasing width produces a phase pattern inoverlap; topology imprints overlap onto node-level features that poolingpartially remixes into task-aligned graph axes; sharper pooling increases axisalignment and reduces channel sharing; and shallow models can settle intometastable low-rank embeddings. These results connect representational geometrywith concrete design choices (width, pooling, and final-layer activations) andsuggest practical approaches for more interpretable GNNs.</description>
      <author>example@mail.com (Lukas Pertl, Han Xuanyuan, Pietro Liò)</author>
      <guid isPermaLink="false">2509.00928v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Flow Matters: Directional and Expressive GNNs for Heterophilic Graphs</title>
      <link>http://arxiv.org/abs/2509.00772v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了边方向性和表达性消息传递在异质图节点分类中的组合效应，提出两种架构：多项式表达性GAT基线模型(Poly)和方向感知变体(Dir-Poly)。实验表明，Poly模型在五个基准异质数据集上优于现有基线，Dir-Poly在有固有方向性的图上提供额外提升。研究发现方向性消息传递的好处是上下文依赖的。&lt;h4&gt;背景&lt;/h4&gt;在异质图中，相邻节点通常属于不同类别，传统图神经网络(GNNs)因其依赖局部同质邻域而表现不佳。先前研究表明建模边方向性可增加有效同质性并提高分类性能，而多项式表达性GNNs在捕获特征间高阶交互方面显示出潜力。&lt;h4&gt;目的&lt;/h4&gt;研究边方向性和表达性消息传递在异质图节点分类中的组合效应，探索边方向性和多项式表达性的结合如何提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出两种架构：1)多项式表达性GAT基线模型(Poly)；2)方向感知变体(Dir-Poly)，分别聚合传入和传出边。两种模型都学习输入特征上的置换等变高阶多项式，保持可扩展性且不增加时间复杂度。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准异质图数据集上，Poly模型始终优于现有基线；Dir-Poly在有固有方向性的图上提供额外提升，达到最先进结果；在无向图上，引入人工方向性并不总是有帮助，表明方向性消息传递的好处是上下文依赖的。&lt;h4&gt;结论&lt;/h4&gt;边方向性和表达性特征建模在异质图学习中起着互补作用，两者的结合可有效提高节点分类性能，但方向性消息传递的效果依赖于图的上下文特性。&lt;h4&gt;翻译&lt;/h4&gt;在异质图中，相邻节点通常属于不同类别，传统的图神经网络(GNNs)因其依赖局部同质邻域而表现不佳。先前研究表明，在这样的图中建模边方向性可以增加有效同质性并提高分类性能。同时，最近关于多项式表达性GNNs的研究在捕获特征间高阶交互方面显示出潜力。在本工作中，我们研究了边方向性和表达性消息传递在异质图节点分类中的组合效应。具体来说，我们提出了两种架构：(1)多项式表达性GAT基线模型(Poly)，和(2)方向感知变体(Dir-Poly)，分别聚合传入和传出边。两种模型都旨在学习输入特征上的置换等变高阶多项式，同时保持可扩展性且不增加时间复杂度。在五个基准异质图数据集上的实验表明，我们的Poly模型始终优于现有基线，而Dir-Poly在有固有方向性的图上(如罗马帝国图)提供了额外提升，达到了最先进的结果。有趣的是，在无向图上，引入人工方向性并不总是有帮助，这表明方向性消息传递的好处是依赖于上下文的。我们的研究突显了边方向和表达性特征建模在异质图学习中的互补作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In heterophilic graphs, where neighboring nodes often belong to differentclasses, conventional Graph Neural Networks (GNNs) struggle due to theirreliance on local homophilous neighborhoods. Prior studies suggest thatmodeling edge directionality in such graphs can increase effective homophilyand improve classification performance. Simultaneously, recent work onpolynomially expressive GNNs shows promise in capturing higher-orderinteractions among features. In this work, we study the combined effect of edgedirectionality and expressive message passing on node classification inheterophilic graphs. Specifically, we propose two architectures: (1) apolynomially expressive GAT baseline (Poly), and (2) a direction-aware variant(Dir-Poly) that separately aggregates incoming and outgoing edges. Both modelsare designed to learn permutation-equivariant high-degree polynomials overinput features, while remaining scalable with no added time complexity.Experiments on five benchmark heterophilic datasets show that our Poly modelconsistently outperforms existing baselines, and that Dir-Poly offersadditional gains on graphs with inherent directionality (e.g., Roman Empire),achieving state-of-the-art results. Interestingly, on undirected graphs,introducing artificial directionality does not always help, suggesting that thebenefit of directional message passing is context-dependent. Our findingshighlight the complementary roles of edge direction and expressive featuremodeling in heterophilic graph learning.</description>
      <author>example@mail.com (Arman Gupta, Govind Waghmare, Gaurav Oberoi, Nitish Srivastava)</author>
      <guid isPermaLink="false">2509.00772v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Task-Aware Adaptive Modulation: A Replay-Free and Resource-Efficient Approach For Continual Graph Learning</title>
      <link>http://arxiv.org/abs/2509.00735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为任务感知自适应调制(TAAM)的持续图学习方法，通过神经突触调制器(NSM)动态调整冻结骨干网络的内部计算流，解决了稳定性-可塑性困境和资源密集型预训练的问题。&lt;h4&gt;背景&lt;/h4&gt;持续图学习(CGL)关注在获取新知识的同时保留已学习的信息，这对现实世界的图应用至关重要。当前方法面临两个主要问题：1)稳定性-可塑性困境：基于回放的方法通常在这两者之间造成不平衡，并且需要大量存储成本；2)资源密集型预训练：领先的免回放方法严重依赖广泛预训练的主干网络，这种依赖带来了巨大的资源负担。&lt;h4&gt;目的&lt;/h4&gt;克服持续图学习中的稳定性-可塑性困境和资源密集型预训练问题，不需要回放数据或微调整个网络，而是通过动态调整冻结骨干网络的内部计算流。&lt;h4&gt;方法&lt;/h4&gt;提出任务感知自适应调制(TAAM)，这是一种免回放、资源高效的方法，其核心是神经突触调制器(NSM)。这些调制器为每个任务训练并冻结，以存储专家知识。一个关键的原型引导策略控制这些调制器：1)对于训练：通过从相似的过去调制器深度复制来初始化新的NSM，以促进知识转移；2)对于推理：为每个任务选择最相关的冻结NSM。这些NSM插入到冻结的GNN骨干网络中，对其内部流进行细粒度的、节点感知的调制。&lt;h4&gt;主要发现&lt;/h4&gt;TAAM在六个GCIL基准数据集上全面优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;TAAM为解决稳定性-可塑性困境开辟了新路径，是一种资源高效的持续图学习方法。&lt;h4&gt;翻译&lt;/h4&gt;持续图学习(CGL)专注于在获取新知识的同时保留已学习的信息，这对现实世界的图应用至关重要。当前方法面临两个主要问题：1)稳定性-可塑性困境：基于回放的方法通常在这两者之间造成不平衡，同时产生显著的存储成本。2)资源密集型预训练：领先的免回放方法严重依赖广泛预训练的主干网络，这种依赖带来了巨大的资源负担。在本文中，我们认为克服这些挑战的关键不在于回放数据或微调整个网络，而在于动态调整冻结骨干网络的内部计算流。我们认为轻量级的、任务特定的模块可以有效引导GNN的推理过程。受此见解启发，我们提出了任务感知自适应调制(TAAM)，这是一种免回放、资源高效的方法，为解决稳定性-可塑性困境开辟了新路径。TAAM的核心是其神经突触调制器(NSM)，这些调制器为每个任务训练并冻结，以存储专家知识。一个关键的原型引导策略控制这些调制器：1)对于训练，它通过从相似的过去调制器深度复制来初始化新的NSM，以促进知识转移。2)对于推理，它为每个任务选择最相关的冻结NSM。这些NSM插入到冻结的GNN骨干网络中，对其内部流进行细粒度的、节点感知的调制——这与先前方法的静态扰动不同。大量实验表明，TAAM在六个GCIL基准数据集上全面优于最先进的方法。代码将在论文接受后发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continual Graph Learning(CGL)focuses on acquiring new knowledge whileretaining previously learned information, essential for real-world graphapplications. Current methods grapple with two main issues:1) TheStability-Plasticity Dilemma: Replay-based methods often create an imbalancebetween the Dilemma, while incurring significant storage costs.2) TheResource-Heavy Pre-training: Leading replay-free methods critically depend onextensively pre-trained backbones, this reliance imposes a substantial resourceburden.In this paper, we argue that the key to overcoming these challenges liesnot in replaying data or fine-tuning the entire network, but in dynamicallymodulating the internal computational flow of a frozen backbone. We posit thatlightweight, task-specific modules can effectively steer a GNN's reasoningprocess. Motivated by this insight, we propose Task-Aware AdaptiveModulation(TAAM), a replay-free, resource-efficient approach that charts a newpath for navigating the stability-plasticity dilemma. TAAM's core is its NeuralSynapse Modulators(NSM), which are trained and then frozen for each task tostore expert knowledge. A pivotal prototype-guided strategy governs thesemodulators: 1) For training, it initializes a new NSM by deep-copying from asimilar past modulator to boost knowledge transfer. 2) For inference, itselects the most relevant frozen NSM for each task. These NSMs insert into afrozen GNN backbone to perform fine-grained, node-attentive modulation of itsinternal flow-different from the static perturbations of prior methods.Extensive experiments show that TAAM comprehensively outperformsstate-of-the-art methods across six GCIL benchmark datasets. The code will bereleased upon acceptance of the paper.</description>
      <author>example@mail.com (Jingtao Liu, Xinming Zhang)</author>
      <guid isPermaLink="false">2509.00735v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Robust Spatiotemporal Forecasting Using Adaptive Deep-Unfolded Variational Mode Decomposition</title>
      <link>http://arxiv.org/abs/2509.00703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review in IEEE Signal Processing Letter&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种模态自适应图网络(MAGN)，通过将迭代变分模态分解转化为可训练神经模块，解决了传统图神经网络在时空预测中的计算效率低下和手动超参数调优问题。&lt;h4&gt;背景&lt;/h4&gt;准确的时空预测对许多复杂系统至关重要，但由于传统图神经网络中的复杂波动模式和频谱纠缠，这仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;解决分解集成方法如VMGCN的计算效率低下和手动超参数调优问题。&lt;h4&gt;方法&lt;/h4&gt;提出模态自适应图网络(MAGN)，包括：(1)展开的VMD(UVMD)模块，用固定深度网络替代迭代优化，大幅减少分解时间；(2)模态特定的可学习带宽约束，适应空间异质性，消除手动调优并防止频谱重叠。&lt;h4&gt;主要发现&lt;/h4&gt;在LargeST基准(6,902个传感器，2.41亿个观测值)上，MAGN比VMGCN减少了85-95%的预测误差，并优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;MAGN有效解决了传统图神经网络在时空预测中的计算效率和调优问题，显著提高了预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的时空预测对许多复杂系统至关重要，但由于传统图神经网络中的复杂波动模式和频谱纠缠，这仍然具有挑战性。虽然变分模态图卷积网络等分解集成方法通过信号分解提高了准确性，但它们存在计算效率低下和手动超参数调优的问题。为解决这些限制，我们提出了模态自适应图网络(MAGN)，将迭代变分模态分解转化为可训练的神经模块。我们的主要创新包括：(1)展开的VMD(UVMD)模块，用固定深度网络替代迭代优化，减少了分解时间(在LargeST基准上减少250倍)；(2)模态特定的可学习带宽约束，适应空间异质性，消除手动调优，同时防止频谱重叠。在LargeST基准上评估，MAGN实现了比VMGCN减少85-95%的预测误差，并优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate spatiotemporal forecasting is critical for numerous complex systemsbut remains challenging due to complex volatility patterns and spectralentanglement in conventional graph neural networks (GNNs). Whiledecomposition-integrated approaches like variational mode graph convolutionalnetwork (VMGCN) improve accuracy through signal decomposition, they suffer fromcomputational inefficiency and manual hyperparameter tuning. To address theselimitations, we propose the mode adaptive graph network (MAGN) that transformsiterative variational mode decomposition (VMD) into a trainable neural module.Our key innovations include (1) an unfolded VMD (UVMD) module that replacesiterative optimization with a fixed-depth network to reduce the decompositiontime (by 250x for the LargeST benchmark), and (2) mode-specific learnablebandwidth constraints ({\alpha}k ) adapt spatial heterogeneity and eliminatemanual tuning while preventing spectral overlap. Evaluated on the LargeSTbenchmark (6,902 sensors, 241M observations), MAGN achieves an 85-95% reductionin the prediction error over VMGCN and outperforms state-of-the-art baselines.</description>
      <author>example@mail.com (Osama Ahmad, Lukas Wesemann, Fabian Waschkowski, Zubair Khalid)</author>
      <guid isPermaLink="false">2509.00703v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Higgs Signal Strength Estimation with Machine Learning under Systematic Uncertainties</title>
      <link>http://arxiv.org/abs/2509.00672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 14 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于图神经网络的希格斯玻色子信号强度提取方法，考虑系统不确定性&lt;h4&gt;背景&lt;/h4&gt;高能物理实验中希格斯玻色子信号强度估计面临系统不确定性挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理系统不确定性的图神经网络方法，准确提取希格斯玻色子信号强度&lt;h4&gt;方法&lt;/h4&gt;采用双分支GNN架构，包括确定性GNN和不确定性感知GNN，通过门控注意力机制处理受系统效应影响的输入，并在训练时对nuisance参数配置进行采样和损失聚合&lt;h4&gt;主要发现&lt;/h4&gt;该方法在FAIR Universe Higgs Uncertainty Challenge数据集上实现了准确的信号强度估计和置信区间计算，在大规模伪实验中具有竞争性的覆盖率和区间宽度&lt;h4&gt;结论&lt;/h4&gt;所提出的SAGE框架能有效处理系统不确定性，准确估计希格斯玻色子信号强度，相关代码已公开&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种专用的基于图神经网络(GNN)的方法，用于提取希格斯玻色子信号强度μ，并纳入系统不确定性。该架构包含两个分支：一个确定性GNN处理不受nuisance参数影响的运动学变量，以及一个不确定性感知GNN，通过基于门控注意力的消息传递处理受系统效应调制的输入。它们的输出被融合以产生信号-背景分类的分数。在训练过程中，我们对nuisance参数配置进行采样并在这些配置上聚合损失，提高分类器在系统偏移下的稳定性，并有效将其输出与nuisance变化解耦。得到的分箱分类器输出用于构建泊松似然，实现对信号强度的轮廓似然扫描，其中nuisance参数通过数值优化被消去。我们在FAIR Universe Higgs Uncertainty Challenge数据集上验证了这一框架，实现了对信号强度μ及其68.27%置信区间的准确估计，在大规模伪实验中取得了具有竞争力的覆盖率和区间宽度。我们的代码'Systematics-Aware Graph Estimator'(SAGE)已公开可用&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a dedicated graph neural network (GNN)-based methodology for theextraction of the Higgs boson signal strength $\mu$, incorporating systematicuncertainties. The architecture features two branches: a deterministic GNN thatprocesses kinematic variables unaffected by nuisance parameters, and anuncertainty-aware GNN that handles inputs modulated by systematic effectsthrough gated attention-based message passing. Their outputs are fused toproduce classification scores for signal-background discrimination. Duringtraining we sample nuisance-parameter configurations and aggregate the lossacross them, promoting stability of the classifier under systematic shifts andeffectively decorrelating its outputs from nuisance variations. The resultingbinned classifier outputs are used to construct a Poisson likelihood, whichenables profile likelihood scans over signal strength, with nuisance parametersprofiled out via numerical optimization. We validate this framework on the FAIRUniverse Higgs Uncertainty Challenge dataset, yielding accurate estimation ofsignal strength $\mu$ and its 68.27\% confidence interval, achievingcompetitive coverage and interval widths in large-scale pseudo-experiments. Ourcode "Systematics-Aware Graph Estimator" (SAGE) is publicly available.</description>
      <author>example@mail.com (Minxuan He, Claudius Krause, Daohan Wang)</author>
      <guid isPermaLink="false">2509.00672v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient GNNs-to-KANs Distillation via Self-Attention Dynamic Sampling with Potential for Consumer Electronics Edge Deployment</title>
      <link>http://arxiv.org/abs/2509.00560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新的知识蒸馏框架SA-DSD，用于解决从图神经网络(GNN)到更高效模型的知识转移问题，显著提升了边缘环境中的模型性能和效率。&lt;h4&gt;背景&lt;/h4&gt;知识蒸馏对于在资源受限的边缘环境中部署深度学习模型至关重要，特别是在消费电子领域如智能家居设备、可穿戴技术和移动终端。这些应用对模型压缩和推理速度有更高要求，需要将GNN知识转移到更高效的MLP模型。&lt;h4&gt;目的&lt;/h4&gt;解决MLP由于固定激活函数和全连接架构难以快速捕捉GNN学习的复杂邻域依赖关系的问题，提高边缘环境中模型的性能。&lt;h4&gt;方法&lt;/h4&gt;引入自注意力动态采样蒸馏(SA-DSD)框架，改进傅里叶KAN(FR-KAN)，用改进的FR-KAN+替代MLP作为学生模型。通过引入可学习的频率基和相移机制以及算法优化提高非线性拟合能力，同时构建基于教师-学生预测一致性的边缘级采样概率矩阵和自适应加权损失机制。&lt;h4&gt;主要发现&lt;/h4&gt;在六个真实数据集上的实验表明，SA-DSD比三个GNN教师模型实现3.05%-3.62%的性能提升，比FR-KAN+模型实现15.61%的性能提升，与关键基准模型相比实现了16.96倍的参数减少和55.75%的推理时间减少。&lt;h4&gt;结论&lt;/h4&gt;SA-DSD框架有效解决了从GNN到MLP知识蒸馏的局限性，显著提高了边缘环境中的模型性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;知识蒸馏(KD)对于在资源受限的边缘环境中部署深度学习模型至关重要，特别是在消费电子领域，包括智能家居设备、可穿戴技术和移动终端。这些应用对模型压缩和推理速度有更高要求，需要将图神经网络(GNN)的知识转移到更高效的多层感知器(MLP)模型。然而，由于MLP具有固定的激活函数和全连接架构，它们难以快速捕捉GNN学习的复杂邻域依赖关系，从而限制了它们在边缘环境中的性能。为解决这些局限性，本文引入了一种创新的知识蒸馏框架——自注意力动态采样蒸馏(SA-DSD)，用于从GNN到Kolmogorov-Arnold Networks (KANs)的知识转移。本研究改进了傅里叶KAN (FR-KAN)，并用改进的FR-KAN+替代MLP作为学生模型。通过引入可学习的频率基和相移机制以及算法优化，FR-KAN显著提高了其非线性拟合能力，同时有效降低了计算复杂度。在此基础上，构建了基于教师-学生预测一致性的边缘级采样概率矩阵，并设计了自适应加权损失机制，以减轻由于缺乏显式邻域聚合而导致的学生模型性能下降。在六个真实数据集上进行的大量实验表明，SA-DSD比三个GNN教师模型实现了3.05%-3.62%的性能提升，比FR-KAN+模型实现了15.61%的性能提升。此外，与关键基准模型相比，SA-DSD实现了16.96倍的参数减少和55.75%的推理时间减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge distillation (KD) is crucial for deploying deep learning models inresource-constrained edge environments, particularly within the consumerelectronics sector, including smart home devices, wearable technology, andmobile terminals. These applications place higher demands on model compressionand inference speed, necessitating the transfer of knowledge from Graph NeuralNetworks (GNNs) to more efficient Multi-Layer Perceptron (MLP) models. However,due to their fixed activation functions and fully connected architecture, MLPsface challenges in rapidly capturing the complex neighborhood dependencieslearned by GNNs, thereby limiting their performance in edge environments. Toaddress these limitations, this paper introduces an innovative from GNNs toKolmogorov-Arnold Networks (KANs) knowledge distillation framework-SelfAttention Dynamic Sampling Distillation (SA-DSD). This study improved FourierKAN (FR-KAN) and replaced MLP with the improved FR-KAN+ as the student model.Through the incorporation of learnable frequency bases and phase-shiftmechanisms, along with algorithmic optimization, FR-KAN significantly improvesits nonlinear fitting capability while effectively reducing computationalcomplexity. Building on this, a margin-level sampling probability matrix, basedon teacher-student prediction consistency, is constructed, and an adaptiveweighted loss mechanism is designed to mitigate performance degradation in thestudent model due to the lack of explicit neighborhood aggregation. Extensiveexperiments conducted on six real-world datasets demonstrate that SA-DSDachieves performance improvements of 3.05%-3.62% over three GNN teacher modelsand 15.61% over the FR-KAN+ model. Moreover, when compared with key benchmarkmodels, SA-DSD achieves a 16.96x reduction in parameter count and a 55.75%decrease in inference time.</description>
      <author>example@mail.com (Can Cui, Zilong Fu, Penghe Huang, Yuanyuan Li, Wu Deng, Dongyan Li)</author>
      <guid isPermaLink="false">2509.00560v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Joint Training of Image Generator and Detector for Road Defect Detection</title>
      <link>http://arxiv.org/abs/2509.03465v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted to ICCV 2025 Workshop on Representation  Learning with Very Limited Resources: When Data, Modalities, Labels, and  Computing Resources are Scarce as an oral paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为JTGD的道路缺陷检测方法，通过联合训练图像生成器和检测器，在边缘设备上高效地进行道路缺陷检测，无需集成方法或测试时增强技术。&lt;h4&gt;背景&lt;/h4&gt;道路缺陷检测对道路管理部门减少因道路缺陷造成的车辆损害非常重要。然而，缺陷检测器通常部署在内存和计算资源有限的边缘设备上，这对检测方法提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在不使用集成方法或测试时增强技术的情况下，在资源受限的边缘设备上进行道路缺陷检测。&lt;h4&gt;方法&lt;/h4&gt;1. 提出JTGD方法，联合训练图像生成器和检测器；2. 为生成模型设计双判别器，使合成的缺陷补丁和整体图像看起来合理；3. 提出基于CLIP的Fréchet Inception Distance损失来提高合成图像质量；4. 让生成模型与检测器联合训练，鼓励生成模型为检测器合成更困难的样本。&lt;h4&gt;主要发现&lt;/h4&gt;1. JTGD在RDD2022道路缺陷检测基准测试中，在没有集成和测试时增强的条件下，优于最先进的方法；2. 与竞争基线相比，JTGD仅使用了不到20%的参数数量；3. 该方法更适合在实际边缘设备上部署。&lt;h4&gt;结论&lt;/h4&gt;JTGD通过联合训练生成器和检测器，能够在资源受限的边缘设备上实现高效的道路缺陷检测，同时保持高检测性能和低计算资源需求。&lt;h4&gt;翻译&lt;/h4&gt;道路缺陷检测对道路管理部门减少因道路缺陷造成的车辆损害非常重要。考虑到缺陷检测器通常部署在内存和计算资源有限的边缘设备的实际场景，我们旨在不使用基于集成的方法或测试时增强技术的情况下进行道路缺陷检测。为此，我们提出联合训练图像生成器和检测器进行道路缺陷检测（称为JTGD）。我们为生成模型设计了双判别器，以强制合成的缺陷补丁和整体图像看起来合理。我们提出的基于CLIP的Fréchet Inception Distance损失提高了合成图像质量。JTGD中的生成模型与检测器联合训练，鼓励生成模型为检测器合成更困难的样本。由于上述设计导致的更高质量的困难合成图像用于数据增强，JTGD在RDD2022道路缺陷检测基准测试中，在没有集成和测试时增强的条件下，优于各种国家的最先进方法。与竞争基线相比，JTGD仅使用了不到20%的参数数量，这使其更适合在实际边缘设备上部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Road defect detection is important for road authorities to reduce the vehicledamage caused by road defects. Considering the practical scenarios where thedefect detectors are typically deployed on edge devices with limited memory andcomputational resource, we aim at performing road defect detection withoutusing ensemble-based methods or test-time augmentation (TTA). To this end, wepropose to Jointly Train the image Generator and Detector for road defectdetection (dubbed as JTGD). We design the dual discriminators for thegenerative model to enforce both the synthesized defect patches and overallimages to look plausible. The synthesized image quality is improved by ourproposed CLIP-based Fr\'echet Inception Distance loss. The generative model inJTGD is trained jointly with the detector to encourage the generative model tosynthesize harder examples for the detector. Since harder synthesized images ofbetter quality caused by the aforesaid design are used in the dataaugmentation, JTGD outperforms the state-of-the-art method in the RDD2022 roaddefect detection benchmark across various countries under the condition of noensemble and TTA. JTGD only uses less than 20% of the number of parameterscompared with the competing baseline, which makes it more suitable fordeployment on edge devices in practice.</description>
      <author>example@mail.com (Kuan-Chuan Peng)</author>
      <guid isPermaLink="false">2509.03465v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PointAD+: Learning Hierarchical Representations for Zero-shot 3D Anomaly Detection</title>
      <link>http://arxiv.org/abs/2509.03277v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to TPAMI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PointAD+的新方法，通过结合CLIP的2D泛化能力，实现了对3D异常的全面检测和分割。该方法同时利用点和像素级别信息，通过隐式和显式3D表示，以及分层表示学习和跨层次对比对齐技术，实现了对渲染和空间异常性的综合理解。&lt;h4&gt;背景&lt;/h4&gt;现有方法难以将CLIP模型的强大2D泛化能力转移到3D异常检测领域，特别是针对具有高度多样化类别语义的未见过的对象。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的框架，能够全面检测和分割3D异常，同时利用点和像素级别的信息，实现对多样化类别语义的未见对象的异常检测。&lt;h4&gt;方法&lt;/h4&gt;首先提出PointAD利用点-像素对应关系表示3D异常；然后提出PointAD+引入显式3D表示和G-aggregation来增强空间感知能力；采用分层表示学习整合渲染和几何提示；通过跨层次对比对齐促进层间交互；最后整合两层异常语义实现泛化；测试阶段可即插即用地集成RGB信息提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明PointAD+在具有高度多样化类别语义的未见对象上的零样本3D异常检测方面表现优越，能够实现对异常性的整体理解。&lt;h4&gt;结论&lt;/h4&gt;PointAD+通过结合渲染和空间异常性，以及利用CLIP的泛化能力，在3D异常检测任务中取得了显著成果，为3D异常检测提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们旨在将CLIP的强大2D泛化能力转移到识别具有高度多样化类别语义的未见对象中的3D异常。为此，我们提出了一个统一框架，通过同时利用点和像素级别信息来全面检测和分割3D异常。我们首先设计了PointAD，它利用点-像素对应关系来通过其关联的渲染像素表示来表示3D异常。这种方法被称为隐式3D表示，因为它只关注渲染像素异常而忽略了点云内的固有空间关系。然后，我们提出了PointAD+，通过引入显式3D表示来进一步拓宽对3D异常的解释，强调空间异常性以发现异常的空间关系。因此，我们提出了G-aggregation来引入几何信息，使聚合的点表示具有空间感知能力。为了同时捕获渲染和空间异常性，PointAD+提出了分层表示学习，将隐式和显式异常语义合并到分层文本提示中：渲染层使用渲染提示，几何层使用几何提示。进一步引入了跨层次对比对齐来促进渲染层和几何层之间的交互，实现相互异常学习。最后，PointAD+整合来自两个层的异常语义以捕获泛化的异常语义。在测试阶段，PointAD+可以即插即用地集成RGB信息，进一步提高其检测性能。大量实验证明了PointAD+在具有高度多样化类别语义的未见对象上的零样本3D异常检测方面的优越性，实现了对异常性的整体理解。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D异常检测问题，即在目标3D训练样本不可见的情况下如何准确检测3D点云中的异常。这个问题在现实中非常重要，因为许多实际应用场景（如涉及商业机密的工业检测或全新产品检测）无法获取目标3D训练数据，而传统3D异常检测方法依赖于目标类别的训练样本，无法处理这种场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到2D和3D异常检测之间存在知识差距，直接应用CLIP模型到3D领域并不简单。他们设计了一个统一框架，将CLIP的2D知识转移到3D理解中。作者首先提出了PointAD，通过点-像素对应关系捕获渲染异常；然后在此基础上提出了PointAD+，增加了对空间异常的建模。该方法借鉴了CLIP的视觉-语言对齐能力、多视图渲染技术、提示学习（如CoOp和AnomalyCLIP）、多实例学习和多任务学习等现有工作，并将它们创新地应用于3D异常检测领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是分层表示学习，通过两个互补角度理解3D异常：1)隐式3D异常（渲染层）：关注渲染像素异常；2)显式3D异常（几何层）：强调空间异常。整体流程包括：1)多视图渲染将3D点云转为2D图像；2)提取像素表示并转换为点表示；3)在渲染层使用混合表示学习优化渲染提示；4)在几何层使用G-aggregation注入几何信息；5)通过跨层次对比对齐促进层次间交互；6)测试时结合两种表示生成最终结果，并可即插即用地集成RGB信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次研究零样本3D和多模态3D异常检测；2)提出分层表示学习框架，将平面提示转换为分层提示；3)引入跨层次对比对齐促进相互学习；4)提出PointAD+和PointAD统一框架；5)实现即插即用的多模态RGB信息集成。相比之前工作，不同之处在于：能处理目标类别不可见的零样本场景；同时考虑渲染异常和空间异常实现更全面理解；无需额外训练即可集成多模态信息；具有更强的泛化能力，能从有限辅助类别学习通用异常语义。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointAD+通过分层表示学习框架，首次实现了对零样本3D异常的全面理解，同时能够即插即用地集成RGB信息，显著提升了在未见对象上的异常检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we aim to transfer CLIP's robust 2D generalizationcapabilities to identify 3D anomalies across unseen objects of highly diverseclass semantics. To this end, we propose a unified framework to comprehensivelydetect and segment 3D anomalies by leveraging both point- and pixel-levelinformation. We first design PointAD, which leverages point-pixelcorrespondence to represent 3D anomalies through their associated renderingpixel representations. This approach is referred to as implicit 3Drepresentation, as it focuses solely on rendering pixel anomalies but neglectsthe inherent spatial relationships within point clouds. Then, we proposePointAD+ to further broaden the interpretation of 3D anomalies by introducingexplicit 3D representation, emphasizing spatial abnormality to uncover abnormalspatial relationships. Hence, we propose G-aggregation to involve geometryinformation to enable the aggregated point representations spatially aware. Tosimultaneously capture rendering and spatial abnormality, PointAD+ proposeshierarchical representation learning, incorporating implicit and explicitanomaly semantics into hierarchical text prompts: rendering prompts for therendering layer and geometry prompts for the geometry layer. A cross-hierarchycontrastive alignment is further introduced to promote the interaction betweenthe rendering and geometry layers, facilitating mutual anomaly learning.Finally, PointAD+ integrates anomaly semantics from both layers to capture thegeneralized anomaly semantics. During the test, PointAD+ can integrate RGBinformation in a plug-and-play manner and further improve its detectionperformance. Extensive experiments demonstrate the superiority of PointAD+ inZS 3D anomaly detection across unseen objects with highly diverse classsemantics, achieving a holistic understanding of abnormality.</description>
      <author>example@mail.com (Qihang Zhou, Shibo He, Jiangtao Yan, Wenchao Meng, Jiming Chen)</author>
      <guid isPermaLink="false">2509.03277v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SurGBSA: Learning Representations From Molecular Dynamics Simulations</title>
      <link>http://arxiv.org/abs/2509.03084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SurGBSA，一种基于分子动力学的表示学习方法，学习MMGBSA的代理函数，实现了比传统计算快6497倍的加速，同时保持几乎相同的准确度。&lt;h4&gt;背景&lt;/h4&gt;自监督预训练可从静态结构中学习强大特征，在分子性质预测等任务上表现优异，但大多数方法受限于静态结构，如何利用分子动力学模拟开发更通用模型仍是个开放问题。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用原子分子动力学模拟来开发更通用的模型，以提高新型分子结构的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Surrogate mmGBSA(SurGBSA)方法，学习MMGBSA的代理函数，并在CASF-2016基准测试收集的140多万个3D轨迹上训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;SurGBSA在姿态排序问题上实现了6497倍加速，准确度仅比传统方法低0.4%；在MD模拟上训练可提升模型性能。&lt;h4&gt;结论&lt;/h4&gt;物理信息预训练结合分子动力学模拟可显著提升模型性能，推动分子基础模型发展，相关模型、代码和数据已公开。&lt;h4&gt;翻译&lt;/h4&gt;从小分子化合物和蛋白质的静态结构进行自监督预训练能够实现强大的学习特征表示。学习到的特征在一系列预测任务上展示了最先进的性能，包括分子性质、结构生成和蛋白质-配体相互作用。大多数方法受限于使用静态结构，如何最好地利用原子分子动力学模拟来开发更通用的模型以提高新型分子结构的预测精度仍然是一个开放性问题。我们提出了Surrogate mmGBSA(SurGBSA)作为一种新的基于MD的表示学习方法，它学习分子力学广义玻恩表面积的代理函数。我们首次展示了物理信息预训练的优势，在从CASF-2016基准测试的MD模拟中收集的超过140万个3D轨迹集合上训练了一个代理MMGBSA模型。在具有挑战性的姿态排序问题上，SurGBSA比传统的基于物理的单点MMGBSA计算实现了惊人的6497倍加速，同时在识别正确顶级姿态方面几乎匹配了单点MMGBSA的准确度（差异为-0.4%）。我们的工作通过展示在MD模拟上训练时模型的改进，推动了分子基础模型的发展。模型、代码和训练数据已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised pretraining from static structures of drug-like compounds andproteins enable powerful learned feature representations. Learned featuresdemonstrate state of the art performance on a range of predictive tasksincluding molecular properties, structure generation, and protein-ligandinteractions. The majority of approaches are limited by their use of staticstructures and it remains an open question, how best to use atomistic moleculardynamics (MD) simulations to develop more generalized models to improveprediction accuracy for novel molecular structures. We present SURrogate mmGBSA(SurGBSA) as a new modeling approach for MD-based representation learning,which learns a surrogate function of the Molecular Mechanics Generalized BornSurface Area (MMGBSA). We show for the first time the benefits ofphysics-informed pre-training to train a surrogate MMGBSA model on a collectionof over 1.4 million 3D trajectories collected from MD simulations of theCASF-2016 benchmark. SurGBSA demonstrates a dramatic 6,497x speedup versus atraditional physics-based single-point MMGBSA calculation while nearly matchingsingle-point MMGBSA accuracy on the challenging pose ranking problem foridentification of the correct top pose (-0.4% difference). Our work advancesthe development of molecular foundation models by showing model improvementswhen training on MD simulations. Models, code and training data are madepublicly available.</description>
      <author>example@mail.com (Derek Jones, Yue Yang, Felice C. Lightstone, Niema Moshiri, Jonathan E. Allen, Tajana S. Rosing)</author>
      <guid isPermaLink="false">2509.03084v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>EmoPerso: Enhancing Personality Detection with Self-Supervised Emotion-Aware Modelling</title>
      <link>http://arxiv.org/abs/2509.02450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EmoPerso的自监督框架，通过情感感知建模改进人格检测，解决了现有方法对大规模标注数据集的依赖以及忽视情感与人格相互作用的问题。&lt;h4&gt;背景&lt;/h4&gt;目前的人格检测通常通过分析用户社交媒体帖子进行，但现有方法严重依赖大规模标注数据集，难以获得高质量的人格标签。此外，大多数研究将情感和人格视为独立变量，忽视了它们之间的相互作用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督框架EmoPerso，通过情感感知建模来改进人格检测，减少对标注数据的依赖并考虑情感与人格的相互作用。&lt;h4&gt;方法&lt;/h4&gt;EmoPerso利用生成机制进行合成数据增强和丰富的表示学习；提取伪标记的情感特征，并通过多任务学习与人格预测联合优化；采用交叉注意力模块捕捉人格特征与推断的情感表示之间的细粒度交互；采用自教策略迭代增强模型的推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在两个基准数据集上的大量实验表明，EmoPerso超越了最先进的模型。&lt;h4&gt;结论&lt;/h4&gt;EmoPerso是一种有效的人格检测方法，通过情感感知建模提高了性能。源代码可在https://github.com/slz0925/EmoPerso获取。&lt;h4&gt;翻译&lt;/h4&gt;Personality detection from text is commonly performed by analysing users'social media posts. However, existing methods heavily rely on large-scaleannotated datasets, making it challenging to obtain high-quality personalitylabels. Moreover, most studies treat emotion and personality as independentvariables, overlooking their interactions. In this paper, we propose a novelself-supervised framework, EmoPerso, which improves personality detectionthrough emotion-aware modelling. EmoPerso first leverages generative mechanismsfor synthetic data augmentation and rich representation learning. It thenextracts pseudo-labeled emotion features and jointly optimizes them withpersonality prediction via multi-task learning. A cross-attention module isemployed to capture fine-grained interactions between personality traits andthe inferred emotional representations. To further refine relational reasoning,EmoPerso adopts a self-taught strategy to enhance the model's reasoningcapabilities iteratively. Extensive experiments on two benchmark datasetsdemonstrate that EmoPerso surpasses state-of-the-art models. The source code isavailable at https://github.com/slz0925/EmoPerso.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personality detection from text is commonly performed by analysing users'social media posts. However, existing methods heavily rely on large-scaleannotated datasets, making it challenging to obtain high-quality personalitylabels. Moreover, most studies treat emotion and personality as independentvariables, overlooking their interactions. In this paper, we propose a novelself-supervised framework, EmoPerso, which improves personality detectionthrough emotion-aware modelling. EmoPerso first leverages generative mechanismsfor synthetic data augmentation and rich representation learning. It thenextracts pseudo-labeled emotion features and jointly optimizes them withpersonality prediction via multi-task learning. A cross-attention module isemployed to capture fine-grained interactions between personality traits andthe inferred emotional representations. To further refine relational reasoning,EmoPerso adopts a self-taught strategy to enhance the model's reasoningcapabilities iteratively. Extensive experiments on two benchmark datasetsdemonstrate that EmoPerso surpasses state-of-the-art models. The source code isavailable at https://github.com/slz0925/EmoPerso.</description>
      <author>example@mail.com (Lingzhi Shen, Xiaohao Cai, Yunfei Long, Imran Razzak, Guanming Chen, Shoaib Jameel)</author>
      <guid isPermaLink="false">2509.02450v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Comprehensive Information-theoretic Multi-view Learning</title>
      <link>http://arxiv.org/abs/2509.02084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为CIML的全面信息理论多视图学习框架，摒弃了多视图冗余假设，同时考虑共同信息和独特信息的预测能力。&lt;h4&gt;背景&lt;/h4&gt;信息理论启发了多视图学习的众多进步，但大多数结合信息理论的多视图方法依赖于'多视图冗余'假设，即认为视图间的共同信息对于下游任务是必要且充分的，却忽视了每个视图中独特信息的潜在预测能力。&lt;h4&gt;目的&lt;/h4&gt;提出一个不依赖多视图冗余假设的全面信息理论多视图学习框架，同时利用共同信息和独特信息的预测能力。&lt;h4&gt;方法&lt;/h4&gt;CIML框架包含两部分：1)共同表示学习，通过最大化Gacs-Korner共同信息提取共享特征，再基于信息(IB)压缩学习任务相关表示；2)独特表示学习，使用IB为每个视图实现最压缩的独特表示，同时最小化独特表示与共同表示之间以及不同独特表示之间的互信息。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了学习到的联合表示对下游任务具有预测充分性，大量实验结果证明了该模型优于几种最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CIML框架通过摒弃多视图冗余假设并综合考虑共同信息和独特信息的预测能力，在多视图学习任务中表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;信息理论启发了多视图学习的众多进步。大多数结合信息理论原理的多视图方法依赖于一个称为'多视图冗余'的假设，该假设认为视图之间的共同信息对于下游任务是必要且充分的。这一假设强调共同信息对预测的重要性，但 inherently 忽略了每个视图中可能具有预测能力的独特信息。在本文中，我们提出了一个名为CIML的全面信息理论多视图学习框架，摒弃了多视图冗余的假设。具体而言，CIML基于信息理论考虑共同信息和独特信息的潜在预测能力。首先，共同表示学习最大化Gacs-Korner共同信息以提取共享特征，然后基于信息瓶颈(IB)压缩这些信息以学习任务相关表示。对于独特表示学习，使用IB实现每个视图的最压缩独特表示，同时最小化独特表示与共同表示之间的互信息，以及不同独特表示之间的互信息。重要的是，我们从理论上证明了学习到的联合表示对下游任务具有预测充分性。大量实验结果证明了我们的模型优于几种最先进的方法。代码已在CIML上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Information theory has inspired numerous advancements in multi-view learning.Most multi-view methods incorporating information-theoretic principles rely anassumption called multi-view redundancy which states that common informationbetween views is necessary and sufficient for down-stream tasks. Thisassumption emphasizes the importance of common information for prediction, butinherently ignores the potential of unique information in each view that couldbe predictive to the task. In this paper, we propose a comprehensiveinformation-theoretic multi-view learning framework named CIML, which discardsthe assumption of multi-view redundancy. Specifically, CIML considers thepotential predictive capabilities of both common and unique information basedon information theory. First, the common representation learning maximizesGacs-Korner common information to extract shared features and then compressesthis information to learn task-relevant representations based on theInformation Bottleneck (IB). For unique representation learning, IB is employedto achieve the most compressed unique representation for each view whilesimultaneously minimizing the mutual information between unique and commonrepresentations, as well as among different unique representations.Importantly, we theoretically prove that the learned joint representation ispredictively sufficient for the downstream task. Extensive experimental resultshave demonstrated the superiority of our model over several state-of-artmethods. The code is released on CIML.</description>
      <author>example@mail.com (Long Shi, Yunshan Ye, Wenjie Wang, Tao Lei, Yu Zhao, Gang Kou, Badong Chen)</author>
      <guid isPermaLink="false">2509.02084v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Fake &amp; Square: Training Self-Supervised Vision Transformers with Synthetic Data and Synthetic Hard Negatives</title>
      <link>http://arxiv.org/abs/2509.02029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025 Workshop LIMIT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文基于现有视觉自监督学习方法，借鉴'假装直到成功'理念，提出Syn2Co框架，结合生成模型合成数据和困难负样本，探索合成数据增强训练对视觉表征学习的影响。&lt;h4&gt;背景&lt;/h4&gt;对比自监督学习已取得显著成功，但通常依赖大量真实世界数据和精心筛选的困难负样本，限制了其在资源受限场景的应用。&lt;h4&gt;目的&lt;/h4&gt;探索替代大量真实数据和精心筛选困难负样本的方法，研究合成数据在视觉自监督学习中的潜力。&lt;h4&gt;方法&lt;/h4&gt;在视觉transformer中研究两种'假装'方式：利用生成模型创建合成数据增强样本多样性；在表征空间生成合成困难负样本创建有挑战性的对比。提出Syn2Co框架结合这两种方法，在DeiT-S和Swin-T架构上评估效果。&lt;h4&gt;主要发现&lt;/h4&gt;合成数据增强训练能带来更鲁棒和可迁移的视觉表征，但同时也揭示了合成数据在自监督学习中的局限性。&lt;h4&gt;结论&lt;/h4&gt;合成数据在自监督学习中具有潜力和局限性，为未来研究方向提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文本身并没有引入全新的方法。相反，我们建立在现有的视觉自监督学习方法基础上，借鉴了'假装直到成功'的格言。虽然对比自监督学习已经取得了显著的成功，但它通常依赖于大量的真实世界数据和精心筛选的困难负样本。为了探索替代这些要求的方法，我们在视觉transformer中研究了两种形式的'假装'。首先，我们研究了生成模型在无监督表征学习中的潜力，利用合成数据来增强样本多样性。其次，我们探讨了在表征空间中生成合成困难负样本的可行性，创建多样且有挑战性的对比。我们的框架——命名为Syn2Co——结合了这两种方法，并评估了合成增强训练是否能导致在DeiT-S和Swin-T架构上更鲁棒和可迁移的视觉表征。我们的研究结果强调了合成数据在自监督学习中的潜力和局限性，为这一方向的未来工作提供了见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper does not introduce a new method per se. Instead, we build onexisting self-supervised learning approaches for vision, drawing inspirationfrom the adage "fake it till you make it". While contrastive self-supervisedlearning has achieved remarkable success, it typically relies on vast amountsof real-world data and carefully curated hard negatives. To explorealternatives to these requirements, we investigate two forms of "faking it" invision transformers. First, we study the potential of generative models forunsupervised representation learning, leveraging synthetic data to augmentsample diversity. Second, we examine the feasibility of generating synthetichard negatives in the representation space, creating diverse and challengingcontrasts. Our framework - dubbed Syn2Co - combines both approaches andevaluates whether synthetically enhanced training can lead to more robust andtransferable visual representations on DeiT-S and Swin-T architectures. Ourfindings highlight the promise and limitations of synthetic data inself-supervised learning, offering insights for future work in this direction.</description>
      <author>example@mail.com (Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki)</author>
      <guid isPermaLink="false">2509.02029v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Training of Vision Transformers with Synthetic Negatives</title>
      <link>http://arxiv.org/abs/2509.02024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 Workshop VisCon&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在视觉变换器架构中利用合成困难负样本来改进自监督学习表示的方法。&lt;h4&gt;背景&lt;/h4&gt;以往的研究探索了合成困难负样本，但很少在视觉变换器的背景下进行，自监督学习中困难负样本的潜力被忽视。&lt;h4&gt;目的&lt;/h4&gt;通过整合合成困难负样本来提高视觉变换器的表示学习能力，增强学习表示的判别能力。&lt;h4&gt;方法&lt;/h4&gt;将合成困难负样本整合到视觉变换器的自监督学习过程中，这是一种简单而有效的技术。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了学习表示的判别能力，在DeiT-S和Swin-T架构上都取得了性能提升。&lt;h4&gt;结论&lt;/h4&gt;合成困难负样本可以有效地改进视觉变换器的表示学习，增强其判别能力，无需引入全新的方法。&lt;h4&gt;翻译&lt;/h4&gt;本文本身并没有引入新颖的方法。相反，我们解决了自监督学习中困难负样本被忽视的潜力。以往的研究探索了合成的困难负样本，但很少在视觉变换器的背景下进行。我们基于这一观察，将合成的困难负样本整合起来，以改进视觉变换器的表示学习。这种简单而有效的技术显著提高了学习表示的判别能力。我们的实验显示，对于DeiT-S和Swin-T架构都取得了性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper does not introduce a novel method per se. Instead, we address theneglected potential of hard negative samples in self-supervised learning.Previous works explored synthetic hard negatives but rarely in the context ofvision transformers. We build on this observation and integrate synthetic hardnegatives to improve vision transformer representation learning. This simpleyet effective technique notably improves the discriminative power of learnedrepresentations. Our experiments show performance improvements for both DeiT-Sand Swin-T architectures.</description>
      <author>example@mail.com (Nikolaos Giakoumoglou, Andreas Floros, Kleanthis Marios Papadopoulos, Tania Stathaki)</author>
      <guid isPermaLink="false">2509.02024v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>DroneSR: Rethinking Few-shot Thermal Image Super-Resolution from Drone-based Perspective</title>
      <link>http://arxiv.org/abs/2509.01898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种面向扩散模型的高斯量化表示学习方法，以解决大规模模型在少样本无人机红外图像超分辨率任务中的过拟合问题，并通过构建多源无人机红外图像基准数据集验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;大规模模型在性能上取得了显著提升，但在图像超分辨率任务中，扩散模型作为生成模型的代表通常采用大规模架构。然而，在少样本无人机捕获的红外训练数据情况下，大规模架构经常出现严重过拟合问题，这削弱了它们的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决大规模架构在少样本无人机红外图像超分辨率任务中的过拟合挑战，提高模型的泛化能力和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种面向扩散模型的高斯量化表示学习方法，同时引入了有效的监控机制来跟踪大规模架构训练过程中的过拟合迹象。通过高斯量化表示学习，有效减少过拟合同时保持架构复杂性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过引入高斯量化表示学习，有效减少了过拟合，同时保持了架构的复杂性；2. 构建了一个多源无人机红外图像基准数据集，用于检测和强调大规模架构在少样本、多样化的无人机图像重建场景中的过拟合问题；3. 实验结果表明，该方法优于现有的超分辨率方法，并在复杂条件下显著减轻了大规模架构的过拟合。&lt;h4&gt;结论&lt;/h4&gt;提出的高斯量化表示学习方法有效解决了大规模模型在少样本无人机红外图像超分辨率任务中的过拟合问题，实验结果证明了该方法的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;尽管大规模模型在性能上取得了显著提升，但过拟合挑战仍然经常削弱它们的泛化能力。在图像超分辨率任务中，扩散模型作为生成模型的代表通常采用大规模架构。然而，在少样本无人机捕获的红外训练数据情况下，大规模架构经常引发严重的过拟合。为解决这一关键挑战，我们的方法提出了一种面向扩散模型的新型高斯量化表示学习方法，该方法减轻过拟合并增强鲁棒性。同时，一个有效的监控机制在训练期间跟踪大规模架构以检测过拟合迹象。通过引入高斯量化表示学习，我们的方法有效减少了过拟合，同时保持架构复杂性。在此基础上，我们构建了一个多源无人机红外图像基准数据集用于检测，并利用它强调大规模架构在少样本、多样化的无人机图像重建场景中的过拟合问题。为验证该方法在减轻过拟合方面的有效性，我们在构建的基准上进行了实验。实验结果表明，我们的方法优于现有的超分辨率方法，并在复杂条件下显著减轻了大规模架构的过拟合。代码和DroneSR数据集将在以下地址提供：https://github.com/wengzp1/GARLSR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although large scale models achieve significant improvements in performance,the overfitting challenge still frequently undermines their generalizationability. In super resolution tasks on images, diffusion models asrepresentatives of generative models typically adopt large scale architectures.However, few-shot drone-captured infrared training data frequently inducessevere overfitting in large-scale architectures. To address this key challenge,our method proposes a new Gaussian quantization representation learning methodoriented to diffusion models that alleviates overfitting and enhancesrobustness. At the same time, an effective monitoring mechanism tracks largescale architectures during training to detect signs of overfitting. Byintroducing Gaussian quantization representation learning, our methodeffectively reduces overfitting while maintaining architecture complexity. Onthis basis, we construct a multi source drone-based infrared image benchmarkdataset for detection and use it to emphasize overfitting issues of large scalearchitectures in few sample, drone-based diverse drone-based imagereconstruction scenarios. To verify the efficacy of the method in mitigatingoverfitting, experiments are conducted on the constructed benchmark.Experimental results demonstrate that our method outperforms existing superresolution approaches and significantly mitigates overfitting of large scalearchitectures under complex conditions. The code and DroneSR dataset will beavailable at: https://github.com/wengzp1/GARLSR.</description>
      <author>example@mail.com (Zhipeng Weng, Xiaopeng Liu, Ce Liu, Xingyuan Guo, Yukai Shi, Liang Lin)</author>
      <guid isPermaLink="false">2509.01898v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Balanced Information Bottlenecks for Long-Tailed Visual Recognition</title>
      <link>http://arxiv.org/abs/2509.01804v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种平衡信息瓶颈(BIB)方法以及多平衡信息瓶颈混合(MBIB)结构，用于解决长尾视觉识别问题。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在大规模平衡数据上取得了显著成功，但现实世界中的视觉识别数据通常是长尾分布的，这给深度神经网络的训练和部署带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效处理长尾视觉识别问题的方法，保留必要的标签相关信息。&lt;h4&gt;方法&lt;/h4&gt;提出平衡信息瓶颈(BIB)方法，将损失函数重新平衡和自蒸馏技术集成到原始IB网络中；提出多平衡信息瓶颈混合(MBIB)结构，不同BIB负责结合来自不同网络层的知识，实现端到端学习。&lt;h4&gt;主要发现&lt;/h4&gt;BIB和MBIB在常用的长尾数据集(包括CIFAR100-LT、ImageNet-LT和iNaturalist 2018)上实验表明，两者在长尾视觉识别方面都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;平衡信息瓶颈(BIB)和多平衡信息瓶颈混合(MBIB)结构是解决长尾视觉识别问题的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNNs)在大规模和平衡数据的各种应用中取得了显著成功。然而，现实世界视觉识别中的数据通常是长尾分布的，这给DNNs的高效训练和部署带来了挑战。信息瓶颈(IB)是一种优雅的表征学习方法。在本文中，我们提出了一种平衡信息瓶颈(BIB)方法，将损失函数重新平衡和自蒸馏技术集成到原始IB网络中。因此，BIB能够学习充分的表征，完全保留必要的标签相关信息，用于长尾视觉识别。为了进一步增强表征学习能力，我们还提出了多平衡信息瓶颈混合(MBIB)的新结构，其中不同的BIB负责结合来自不同网络层的知识。MBIB促进了一种端到端的学习策略，从信息论的角度同时训练表征和分类。我们在常用的长尾数据集上进行了实验，包括CIFAR100-LT、ImageNet-LT和iNaturalist 2018。BIB和MBIB在长尾视觉识别方面都达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have achieved significant success in variousapplications with large-scale and balanced data. However, data in real-worldvisual recognition are usually long-tailed, bringing challenges to efficienttraining and deployment of DNNs. Information bottleneck (IB) is an elegantapproach for representation learning. In this paper, we propose a balancedinformation bottleneck (BIB) approach, in which loss function re-balancing andself-distillation techniques are integrated into the original IB network. BIBis thus capable of learning a sufficient representation with essentiallabel-related information fully preserved for long-tailed visual recognition.To further enhance the representation learning capability, we also propose anovel structure of mixture of multiple balanced information bottlenecks (MBIB),where different BIBs are responsible for combining knowledge from differentnetwork layers. MBIB facilitates an end-to-end learning strategy that trainsrepresentation and classification simultaneously from an information theoryperspective. We conduct experiments on commonly used long-tailed datasets,including CIFAR100-LT, ImageNet-LT, and iNaturalist 2018. Both BIB and MBIBreach state-of-the-art performance for long-tailed visual recognition.</description>
      <author>example@mail.com (Yifan Lan, Xin Cai, Jun Cheng, Shan Tan)</author>
      <guid isPermaLink="false">2509.01804v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Neural Scene Designer: Self-Styled Semantic Image Manipulation</title>
      <link>http://arxiv.org/abs/2509.01405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了神经场景设计器（NSD）框架，能够在保持语义控制和风格一致性的同时，实现对用户指定图像区域的逼真操作。&lt;h4&gt;背景&lt;/h4&gt;保持风格一致性对图像的凝聚力和美感至关重要，是有效图像编辑和修复的基本要求。然而现有方法主要关注生成内容的语义控制，常常忽视保持风格一致性的关键任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个新框架，能够实现对用户指定场景区域的逼真操作，同时确保与用户意图的语义对齐以及与周围环境的一致性。&lt;h4&gt;方法&lt;/h4&gt;NSD利用先进的扩散模型，包含两个并行的交叉注意力机制分别处理文本和风格信息；提出渐进式自风格表征学习（PSRL）模块，通过风格对比损失学习风格表征；建立了包括竞争算法、风格相关指标和多样化数据集的基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;在基准测试上进行的大量实验证明了所提出框架在保持语义控制和风格一致性方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;NSD框架能够有效解决图像编辑中的风格一致性问题，为图像处理提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;保持风格一致性对于图像的凝聚力和美感至关重要，这是有效图像编辑和修复的基本要求。然而，现有方法主要关注生成内容的语义控制，常常忽视保持风格一致性的关键任务。在这项工作中，我们引入了神经场景设计器（NSD），这是一个新颖的框架，能够实现对用户指定场景区域的逼真操作，同时确保与用户意图的语义对齐以及与周围环境的一致性。NSD利用先进的扩散模型，包含两个并行的交叉注意力机制，分别处理文本和风格信息，以实现语义控制和风格一致性的双重目标。为了捕获细粒度的风格表征，我们提出了渐进式自风格表征学习（PSRL）模块。该模块基于一个直观的前提：单个图像内的不同区域共享一致的风格，而来自不同图像的区域则表现出不同的风格。PSRL模块采用风格对比损失，鼓励来自同一图像的表征具有高度相似性，同时强制来自不同图像的表征具有差异性。此外，为了解决此任务缺乏标准化评估协议的问题，我们建立了一个全面的基准测试。该基准测试包括竞争算法、专门的风格相关指标以及多样化的数据集和设置，以促进公平比较。在我们基准上进行的大量实验证明了所提出框架的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Maintaining stylistic consistency is crucial for the cohesion and aestheticappeal of images, a fundamental requirement in effective image editing andinpainting. However, existing methods primarily focus on the semantic controlof generated content, often neglecting the critical task of preserving thisconsistency. In this work, we introduce the Neural Scene Designer (NSD), anovel framework that enables photo-realistic manipulation of user-specifiedscene regions while ensuring both semantic alignment with user intent andstylistic consistency with the surrounding environment. NSD leverages anadvanced diffusion model, incorporating two parallel cross-attention mechanismsthat separately process text and style information to achieve the dualobjectives of semantic control and style consistency. To capture fine-grainedstyle representations, we propose the Progressive Self-style RepresentationalLearning (PSRL) module. This module is predicated on the intuitive premise thatdifferent regions within a single image share a consistent style, whereasregions from different images exhibit distinct styles. The PSRL module employsa style contrastive loss that encourages high similarity betweenrepresentations from the same image while enforcing dissimilarity between thosefrom different images. Furthermore, to address the lack of standardizedevaluation protocols for this task, we establish a comprehensive benchmark.This benchmark includes competing algorithms, dedicated style-related metrics,and diverse datasets and settings to facilitate fair comparisons. Extensiveexperiments conducted on our benchmark demonstrate the effectiveness of theproposed framework.</description>
      <author>example@mail.com (Jianman Lin, Tianshui Chen, Chunmei Qing, Zhijing Yang, Shuangping Huang, Yuheng Ren, Liang Lin)</author>
      <guid isPermaLink="false">2509.01405v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Propagation-aware Representation Learning for Supervised Social Media Graph Analytics</title>
      <link>http://arxiv.org/abs/2509.01124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 25th IEEE International Conference on Data Mining  (ICDM2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合双编码器结构和动力学引导传播模块的通用表示学习框架，用于社交媒体图分析，该框架在多种任务上实现了最先进性能并具有良好的迁移能力。&lt;h4&gt;背景&lt;/h4&gt;社交媒体平台产生大量复杂的图结构数据，支持谣言检测、机器人识别和影响力建模等任务。现实应用如公共舆论监控和股票交易高度依赖社交媒体，需要模型在不同任务和数据集上表现良好。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据驱动模型易受噪声影响、难以跨任务重用架构的问题，开发一个通用的表示学习框架，提高模型对噪声数据的鲁棒性和任务通用性。&lt;h4&gt;方法&lt;/h4&gt;提出结合双编码器结构和动力学引导传播模块的框架，使用两个编码器联合建模结构和上下文信息，通过整合原则性动力学知识捕捉社交媒体图中的信息传播动态，基于马尔可夫链传输模型推导传播感知编码器和优化目标。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在图分类、节点分类和链接预测等多种社交媒体图挖掘任务上取得最先进性能，使用统一架构实现；在不同数据集上表现出强大的零样本和少样本迁移能力，在数据稀缺任务中具有实用性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了社交媒体图分析中的挑战，在多种任务上表现出色且具有良好的迁移能力，特别适用于数据稀缺场景。&lt;h4&gt;翻译&lt;/h4&gt;社交媒体平台产生大量复杂的图结构数据，促进多样化任务如谣言检测、机器人识别和影响力建模。现实应用如公共舆论监控和股票交易——与社交媒体紧密相连——需要模型在不同任务和数据集上表现良好。然而，大多数现有解决方案纯粹是数据驱动的，对社交媒体数据中的固有噪声表现出脆弱性。此外，对特定任务模型设计的依赖挑战了在不同任务上高效重用相同模型架构的能力，导致重复的工程努力。为了解决社交媒体图分析中的这些挑战，我们提出了一种通用表示学习框架，该框架结合了双编码器结构和动力学引导传播模块。除了使用两个编码器联合建模结构和上下文信息外，我们的框架还通过整合原则性动力学知识，创新性地捕捉社交媒体图中的信息传播动态。通过基于马尔可夫链传输模型推导传播感知编码器和相应的优化目标，表示学习流程增强了对噪声数据的鲁棒性和多样化任务的通用性。大量实验验证了我们的方法在各种社交媒体图挖掘任务上实现了最先进的性能，这些任务包括图分类、节点分类和链接预测，且使用统一架构。此外，我们的解决方案在不同数据集上表现出强大的零样本和少样本迁移能力，展示了在处理数据稀缺任务时的实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social media platforms generate vast, complex graph-structured data,facilitating diverse tasks such as rumor detection, bot identification, andinfluence modeling. Real-world applications like public opinion monitoring andstock trading -- which have a strong attachment to social media -- demandmodels that are performant across diverse tasks and datasets. However, mostexisting solutions are purely data-driven, exhibiting vulnerability to theinherent noise within social media data. Moreover, the reliance ontask-specific model design challenges efficient reuse of the same modelarchitecture on different tasks, incurring repetitive engineering efforts. Toaddress these challenges in social media graph analytics, we propose a generalrepresentation learning framework that integrates a dual-encoder structure witha kinetic-guided propagation module. In addition to jointly modeling structuraland contextual information with two encoders, our framework innovativelycaptures the information propagation dynamics within social media graphs byintegrating principled kinetic knowledge. By deriving a propagation-awareencoder and corresponding optimization objective from a Markov chain-basedtransmission model, the representation learning pipeline receives a boost inits robustness to noisy data and versatility in diverse tasks. Extensiveexperiments verify that our approach achieves state-of-the-art performance witha unified architecture on a variety of social media graph mining tasks spanninggraph classification, node classification, and link prediction. Besides, oursolution exhibits strong zero-shot and few-shot transferability acrossdatasets, demonstrating practicality when handling data-scarce tasks.</description>
      <author>example@mail.com (Wei Jiang, Tong Chen, Wei Yuan, Xiangyu Zhao, Quoc Viet Hung Nguyen, Hongzhi Yin)</author>
      <guid isPermaLink="false">2509.01124v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>CascadeFormer: A Family of Two-stage Cascading Transformers for Skeleton-based Human Action Recognition</title>
      <link>http://arxiv.org/abs/2509.00692v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CascadeFormer的基于骨架的人体动作识别模型，结合了掩码预训练和级联微调两个阶段，在多个基准数据集上取得了具有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;基于骨架的人体动作识别利用人体关节坐标序列来识别视频中执行的动作。由于骨架数据固有的时空结构，图卷积网络(GCN)一直是该领域的主导架构。最近，Transformer模型和掩码预训练框架的发展为表示学习开辟了新途径。&lt;h4&gt;目的&lt;/h4&gt;提出CascadeFormer，一种用于基于骨架的人体动作识别的两级级联Transformer系列模型。&lt;h4&gt;方法&lt;/h4&gt;CascadeFormer框架包括两个阶段：1) 掩码预训练阶段，学习可泛化的骨架表示；2) 级联微调阶段，针对判别性动作分类进行定制。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集(Penn Action, N-UCLA, 和 NTU RGB+D 60)上评估了CascadeFormer，在所有任务上取得了具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;为了促进可复现性，作者发布了代码和模型检查点。&lt;h4&gt;翻译&lt;/h4&gt;基于骨架的人体动作识别利用人体关节坐标序列来识别视频中执行的动作。由于骨架数据固有的时空结构，图卷积网络(GCN)一直是该领域的主导架构。然而，最近Transformer模型和掩码预训练框架的进展为表示学习开辟了新途径。在这项工作中，我们提出了CascadeFormer，一种用于基于骨架的人体动作识别的两级级联Transformer系列模型。我们的框架包括一个掩码预训练阶段，用于学习可泛化的骨架表示，以及一个针对判别性动作分类定制的级联微调阶段。我们在三个基准数据集(Penn Action, N-UCLA, 和 NTU RGB+D 60)上评估了CascadeFormer，在所有任务上取得了具有竞争力的性能。为了促进可复现性，我们发布了代码和模型检查点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Skeleton-based human action recognition leverages sequences of human jointcoordinates to identify actions performed in videos. Owing to the intrinsicspatiotemporal structure of skeleton data, Graph Convolutional Networks (GCNs)have been the dominant architecture in this field. However, recent advances intransformer models and masked pretraining frameworks open new avenues forrepresentation learning. In this work, we propose CascadeFormer, a family oftwo-stage cascading transformers for skeleton-based human action recognition.Our framework consists of a masked pretraining stage to learn generalizableskeleton representations, followed by a cascading fine-tuning stage tailoredfor discriminative action classification. We evaluate CascadeFormer acrossthree benchmark datasets (Penn Action N-UCLA, and NTU RGB+D 60), achievingcompetitive performance on all tasks. To promote reproducibility, we releaseour code and model checkpoints.</description>
      <author>example@mail.com (Yusen Peng, Alper Yilmaz)</author>
      <guid isPermaLink="false">2509.00692v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Entropy-based Coarse and Compressed Semantic Speech Representation Learning</title>
      <link>http://arxiv.org/abs/2509.00503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于熵的动态聚合框架，用于学习压缩的语义语音表示，解决了现有方法中token冗余问题，提高了下游任务效率。&lt;h4&gt;背景&lt;/h4&gt;离散语音表示学习在声学和语义建模中受到越来越多的关注。现有方法通常将16kHz波形编码为每秒25或50个离散token。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中token冗余和效率低下的问题，提出一种能够学习压缩语义语音表示的方法。&lt;h4&gt;方法&lt;/h4&gt;首先在大型未标记数据上预训练语音语言模型捕获频繁token模式；然后使用预测熵自适应确定聚合边界；接着使用交叉注意力模块融合每个段内信息；通过调整熵阈值灵活控制表示粒度和压缩比。&lt;h4&gt;主要发现&lt;/h4&gt;在ASR、语音到文本翻译和语音转换任务上的实验表明，压缩表示的性能与密集token序列相当或更好。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于熵的动态聚合框架能够有效学习压缩的语义语音表示，提高了下游任务效率且保持了性能。&lt;h4&gt;翻译&lt;/h4&gt;离散语音表示学习最近在声学和语义建模中引起了越来越多的关注。现有方法通常将16kHz波形编码为每秒25或50个离散token。然而，考虑到语音通常每秒只传达2到5个单词，这种细粒度的token化引入了冗余，阻碍了下游训练和推理的效率。此外，这种频率的语义语音表示主要捕获音素级信息，而语义理解可能不需要如此详细的token级分辨率。为了解决这些局限性，我们提出了一种基于熵的动态聚合框架，用于学习压缩的语义语音表示。首先，在大型未标记数据上通过next-token prediction预训练语音语言模型以捕获频繁的token模式。然后使用预测熵自适应地确定聚合边界，接着使用交叉注意力模块融合每个段内的信息。通过调整熵阈值，可以灵活控制表示的粒度和压缩比。在ASR、语音到文本翻译和语音转换任务上的实验表明，压缩表示的性能与密集token序列相当或更好，证明了所提出方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discrete speech representation learning has recently attracted increasinginterest in both acoustic and semantic modeling. Existing approaches typicallyencode 16 kHz waveforms into discrete tokens at a rate of 25 or 50 tokens persecond. However, given that speech generally conveys only 2 to 5 words persecond, such fine-grained tokenization introduces redundancy and hindersefficiency in downstream training and inference. Moreover, semantic speechrepresentations at this frequency primarily capture phonetic-level information,while semantic understanding may not require such detailed token-levelresolution. To address these limitations, we propose an entropy-based dynamicaggregation framework for learning compressed semantic speech representations.A speech language model is first pre-trained via next-token prediction onlarge-scale unlabeled data to capture frequent token patterns. Predictiveentropy is then used to adaptively determine aggregation boundaries, followedby a cross-attention module that fuses information within each segment. Byadjusting the entropy threshold, the granularity and compression ratio of therepresentations can be flexibly controlled. Experiments on ASR, speech-to-texttranslation, and voice conversion tasks demonstrate that the compressedrepresentations perform on par with or better than dense token sequences,demonstrating the effectiveness of the proposed approach.</description>
      <author>example@mail.com (Jialong Zuo, Guangyan Zhang, Minghui Fang, Shengpeng Ji, Xiaoqi Jiao, Jingyu Li, Yiwen Guo, Zhou Zhao)</author>
      <guid isPermaLink="false">2509.00503v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Focused Video Group Activities Hashing</title>
      <link>http://arxiv.org/abs/2509.00490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的视频哈希技术STVH及其增强版本M-STVH，用于解决复杂场景下视频数据快速增长背景下的群体活动快速检索问题。该方法能够同时捕捉个体对象动力学和群体交互特征，并可根据需要灵活关注活动语义特征或对象视觉特征。&lt;h4&gt;背景&lt;/h4&gt;随着各种复杂场景下视频数据的爆炸式增长，快速检索群体活动已成为一个紧迫问题。然而，许多现有任务只能检索整个视频，而无法达到活动粒度的精确检索。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉视频时空演化特征的视频哈希技术，实现活动粒度的视频检索，并能够根据实际需求灵活关注活动语义特征或对象视觉特征。&lt;h4&gt;方法&lt;/h4&gt;1. 提出STVH（时空交错视频哈希）技术，通过统一框架同时建模个体对象动力学和群体交互，捕捉群体视觉特征和位置特征上的时空演化。2. 进一步提出M-STVH（多聚焦时空视频哈希）作为增强版本，通过多聚焦表示学习的层次特征集成，使模型能够同时关注活动语义特征和对象视觉特征。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上进行的比较实验表明，STVH和M-STVH两种方法都取得了优异的结果，证明了所提方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的STVH和M-STVH技术能够有效解决复杂场景下群体活动的快速检索问题，为视频检索提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着各种复杂场景下视频数据的爆炸式增长，快速检索群体活动已成为一个紧迫问题。然而，许多任务只能检索整个视频，而无法达到活动粒度的精确检索。为解决这个问题，我们首次提出了STVH（时空交错视频哈希）技术。通过统一框架，STVH同时建模个体对象动力学和群体交互，捕捉群体视觉特征和位置特征上的时空演化。此外，在实际视频检索场景中，有时可能需要活动特征，而有时可能需要对象的视觉特征。我们进一步提出了一种新颖的M-STVH（多聚焦时空视频哈希）作为增强版本来处理这一困难任务。先进的方法通过多聚焦表示学习的层次特征集成，使模型能够同时关注活动语义特征和对象视觉特征。我们在公开可用数据集上进行了比较实验，STVH和M-STVH都能取得优异的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the explosive growth of video data in various complex scenarios, quicklyretrieving group activities has become an urgent problem. However, many taskscan only retrieve videos focusing on an entire video, not the activitygranularity. To solve this problem, we propose a new STVH (spatiotemporalinterleaved video hashing) technique for the first time. Through a unifiedframework, the STVH simultaneously models individual object dynamics and groupinteractions, capturing the spatiotemporal evolution on both group visualfeatures and positional features. Moreover, in real-life video retrievalscenarios, it may sometimes require activity features, while at other times, itmay require visual features of objects. We then further propose a novel M-STVH(multi-focused spatiotemporal video hashing) as an enhanced version to handlethis difficult task. The advanced method incorporates hierarchical featureintegration through multi-focused representation learning, allowing the modelto jointly focus on activity semantics features and object visual features. Weconducted comparative experiments on publicly available datasets, and both STVHand M-STVH can achieve excellent results.</description>
      <author>example@mail.com (Zhongmiao Qi, Yan Jiang, Bolin Zhang, Lijun Guo, Chong Wang, Qiangbo Qian)</author>
      <guid isPermaLink="false">2509.00490v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>InfraDiffusion: zero-shot depth map restoration with diffusion models and prompted segmentation from sparse infrastructure point clouds</title>
      <link>http://arxiv.org/abs/2509.03324v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InfraDiffusion是一种零样本框架，通过虚拟相机将砖石点云投影为深度图，并使用去噪扩散零空间模型(DDNM)进行恢复，无需特定任务训练即可提高深度图的视觉清晰度和几何一致性，显著改善了砖块级别分割效果，适用于基础设施自动化检查。&lt;h4&gt;背景&lt;/h4&gt;点云被广泛用于基础设施监测，提供几何信息，下游任务如缺陷检测需要分割。现有研究已自动化结构组件的语义分割，但砖块级别分割(识别剥落和砂浆损失等缺陷)主要从RGB图像进行。然而，在砖石隧道等低光环境中获取高分辨率图像不切实际。点云虽然对弱光环境稳健，但通常无结构、稀疏和有噪声，限制了细粒度分割。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够在低光环境中对点云数据进行有效处理，提高砖石结构砖块级别分割的准确性，用于基础设施的自动化检查。&lt;h4&gt;方法&lt;/h4&gt;提出InfraDiffusion，一种零样本框架，通过虚拟相机将砖石点云投影到深度图，并采用去噪扩散零空间模型(DDNM)进行恢复。这种方法无需任务特定训练即可增强深度图的视觉清晰度和几何一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在砖石桥梁和隧道点云数据集上的实验表明，使用Segment Anything Model (SAM)进行砖块级别分割时，InfraDiffusion方法取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;InfraDiffusion展示了其在砖石资产自动化检查方面的潜力，为基础设施监测提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;点云通过提供几何信息被广泛用于基础设施监测，其中下游任务如缺陷检测需要分割。现有研究已自动化结构组件的语义分割，而砖块级别分割(识别剥落和砂浆损失等缺陷)主要从RGB图像进行。然而，在砖石隧道等弱光环境中获取高分辨率图像不切实际。点云虽然对弱光环境稳健，但通常无结构、稀疏和有噪声，限制了细粒度分割。我们提出了InfraDiffusion，一种零样本框架，通过虚拟相机将砖石点云投影为深度图，并使用去噪扩散零空间模型(DDNM)进行恢复。无需任务特定训练，InfraDiffusion提高了深度图的视觉清晰度和几何一致性。在砖石桥梁和隧道点云数据集上的实验表明，使用Segment Anything Model (SAM)进行砖块级别分割时取得了显著改进，凸显了其在砖石资产自动化检查方面的潜力。我们的代码和数据可在https://github.com/Jingyixiong/InfraDiffusion-official-implement获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从稀疏、嘈杂的基础设施工点云中恢复深度地图，特别是针对砖石结构（如隧道和桥梁）的砖级别分割问题。这个问题在现实中很重要，因为砖石基础设施需要定期检查以确保安全，而传统方法在低光环境下难以获取高质量图像，且点云数据本身的稀疏性和噪声限制了精细分割的准确性。在研究中，这填补了组件级别分割与砖级别分割之间的空白，对早期缺陷检测和结构健康监测具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有投影方法的局限性，特别是圆柱投影不适用于复杂基础设施如砖石桥梁。为此，他们借鉴了自动驾驶领域的虚拟相机投影技术，以适应各种复杂几何形状。对于深度地图恢复的挑战，作者选择了扩散模型路线，特别是基于DDNM框架，因为它能提供强大的图像先验且不需要成对训练数据。作者对DDNM进行了关键改进，引入边界掩码来约束恢复过程，防止在无效区域产生伪内容。整个方法设计融合了虚拟相机投影和改进的扩散模型，借鉴了多个领域的现有工作但进行了针对性创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将稀疏、嘈杂的3D点云投影为2D深度地图，然后使用改进的扩散模型恢复这些深度地图，提高其视觉清晰度和几何一致性，同时通过边界掩码确保恢复只在有效区域内进行。整体流程分为五个步骤：1) 点云预处理（体素下采样和法线估计）；2) 补丁提取（定义包围盒并裁剪点云）；3) 虚拟相机投影（将点云投影为2D深度地图并计算边界掩码）；4) 深度地图恢复（使用改进的DDNM，分解为范围空间和零空间组件进行恢复）；5) 砖级别分割（使用SAM模型进行零样本语义分割）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出InfraDiffusion零样本框架，用于从砖石点云恢复深度地图；2) 引入虚拟相机投影适应复杂基础设施几何形状；3) 改进DDNM模型，添加边界掩码约束恢复过程；4) 显著提高五个数据集上的砖级别分割指标。相比之前工作，不同之处在于：之前的投影方法主要适用于隧道环境，而InfraDiffusion能处理复杂几何形状；之前的恢复方法需要成对训练数据，而InfraDiffusion是零样本方法；之前的扩散模型主要针对RGB图像，而InfraDiffusion专门处理深度地图；之前的分割需要大量标注数据，而InfraDiffusion结合SAM实现零样本分割。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; InfraDiffusion通过虚拟相机投影和改进的扩散模型，实现了从稀疏、嘈杂的基础设施工点云中高质量恢复深度地图，并显著提高了砖级别分割的准确性，为基础设施健康监测提供了一种无需特定训练的零样本解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are widely used for infrastructure monitoring by providinggeometric information, where segmentation is required for downstream tasks suchas defect detection. Existing research has automated semantic segmentation ofstructural components, while brick-level segmentation (identifying defects suchas spalling and mortar loss) has been primarily conducted from RGB images.However, acquiring high-resolution images is impractical in low-lightenvironments like masonry tunnels. Point clouds, though robust to dim lighting,are typically unstructured, sparse, and noisy, limiting fine-grainedsegmentation. We present InfraDiffusion, a zero-shot framework that projectsmasonry point clouds into depth maps using virtual cameras and restores them byadapting the Denoising Diffusion Null-space Model (DDNM). Without task-specifictraining, InfraDiffusion enhances visual clarity and geometric consistency ofdepth maps. Experiments on masonry bridge and tunnel point cloud datasets showsignificant improvements in brick-level segmentation using the Segment AnythingModel (SAM), underscoring its potential for automated inspection of masonryassets. Our code and data is available athttps://github.com/Jingyixiong/InfraDiffusion-official-implement.</description>
      <author>example@mail.com (Yixiong Jing, Cheng Zhang, Haibing Wu, Guangming Wang, Olaf Wysocki, Brian Sheil)</author>
      <guid isPermaLink="false">2509.03324v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PI3DETR: Parametric Instance Detection of 3D Point Cloud Edges with a Geometry-Aware 3DETR</title>
      <link>http://arxiv.org/abs/2509.03262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PI3DETR是一个端到端的框架，可以直接从原始点云预测3D参数化曲线实例，避免了中间表示和多阶段处理，提高了对噪声和不同采样密度的鲁棒性，并在ABC数据集上取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;先前的工作通常使用中间表示和多阶段处理来处理3D曲线检测，这在现实世界的LiDAR和3D感知场景中面临噪声和不同采样密度的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够直接从原始点云预测3D参数化曲线的端到端框架，提高对噪声和不同采样密度的鲁棒性，解决现实世界LiDAR和3D感知场景中的关键挑战。&lt;h4&gt;方法&lt;/h4&gt;扩展3DETR模型，引入几何感知的匹配策略和专门的损失函数，支持单次前向传播中统一检测不同参数化的曲线类型（包括三次贝塞尔曲线、线段、圆和弧），并使用可选的后处理步骤优化预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;PI3DETR在ABC数据集上建立了新的最先进水平，能够有效地推广到真实传感器数据，为3D边缘和曲线估计提供了一个简单而强大的解决方案。&lt;h4&gt;结论&lt;/h4&gt;PI3DETR通过精简的设计提高了对噪声和不同采样密度的鲁棒性，解决了现实世界LiDAR和3D感知场景中的关键挑战，是一个简单而强大的3D边缘和曲线估计解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了PI3DETR，这是一个端到端框架，可以直接从原始点云预测3D参数化曲线实例，避免了先前工作中常见的中间表示和多阶段处理。扩展3DETR后，我们的模型引入了几何感知的匹配策略和专门的损失函数，使得在单次前向传播中能够统一检测不同参数化的曲线类型，包括三次贝塞尔曲线、线段、圆和弧。可选的后处理步骤可以进一步优化预测结果而不会增加复杂性。这种精简的设计提高了对噪声和不同采样密度的鲁棒性，解决了现实世界LiDAR和3D感知场景中的关键挑战。PI3DETR在ABC数据集上建立了新的最先进水平，并且能有效地推广到真实传感器数据，为3D边缘和曲线估计提供了一个简单而强大的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从3D点云中直接检测和预测参数化曲线实例的问题。这个问题在现实中非常重要，因为在机器人视觉中，边缘信息可以优化抓取规划；在制造业中，线框提取有助于质量控制；在自动驾驶、CAD重建等领域，边缘作为重要的结构线索。此外，现实世界中的LiDAR等3D传感技术产生的点云常包含噪声、非均匀采样密度和遮挡，这使得开发鲁棒的边缘检测方法对确保可靠性能至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者批判性地分析了现有方法，发现它们通常依赖中间表示（如体素网格、距离场）和多阶段处理，流程复杂且对输入空间有假设。作者借鉴了3DETR框架，将其扩展用于曲线检测，但进行了关键改进：引入了几何感知匹配策略，设计专门针对3D边缘估计的损失函数，并添加了参数化曲线预测头部。作者的目标是创建一个简化的端到端架构，直接从原始点云推断参数化曲线，消除传统方法中的辅助表示和手工制作管道。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个简化的端到端架构，直接从原始点云预测参数化曲线实例，避免传统方法中的中间表示和多阶段处理。通过几何感知匹配策略，使单一模型能够检测不同类型的参数化曲线。整体流程：1)接收点云输入；2)使用SAModule下采样并提取特征；3)通过Transformer编码器-解码器处理特征；4)使用预测头部输出不同类型曲线的参数；5)通过几何感知匹配将预测与真实曲线匹配；6)计算包括交叉熵、几何损失和Chamfer距离的总损失；7)应用可选后处理步骤优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)简化的端到端架构，直接预测参数化曲线；2)几何感知匹配策略，支持多种曲线类型联合检测；3)专门设计的损失函数，处理不同曲线类型的几何特性；4)可选后处理步骤进一步提高精度。相比之前的工作如NerVE、PIE-NET等，PI3DETR的不同之处在于：直接预测参数化曲线而非先检测基元再拟合；使用统一架构而非多分支处理；表现出更强的鲁棒性，尤其在稀疏点云和噪声环境下；在ABC数据集上实现了更高的准确性，Chamfer距离比之前方法降低近一半。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PI3DETR提出了一种简化的端到端框架，通过几何感知匹配策略直接从原始点云预测多种类型的参数化3D曲线，实现了比之前方法更高的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present PI3DETR, an end-to-end framework that directly predicts 3Dparametric curve instances from raw point clouds, avoiding the intermediaterepresentations and multi-stage processing common in prior work. Extending3DETR, our model introduces a geometry-aware matching strategy and specializedloss functions that enable unified detection of differently parameterized curvetypes, including cubic B\'ezier curves, line segments, circles, and arcs, in asingle forward pass. Optional post-processing steps further refine predictionswithout adding complexity. This streamlined design improves robustness to noiseand varying sampling densities, addressing critical challenges in real worldLiDAR and 3D sensing scenarios. PI3DETR sets a new state-of-the-art on the ABCdataset and generalizes effectively to real sensor data, offering a simple yetpowerful solution for 3D edge and curve estimation.</description>
      <author>example@mail.com (Fabio F. Oberweger, Michael Schwingshackl, Vanessa Staderini)</author>
      <guid isPermaLink="false">2509.03262v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Robotic 3D Flower Pose Estimation for Small-Scale Urban Farms</title>
      <link>http://arxiv.org/abs/2509.02870v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用小型城市农场的低成本自动化机器人平台（FarmBot）进行植物表型分析，特别关注草莓花朵的姿态估计，用于机器人授粉应用。&lt;h4&gt;背景&lt;/h4&gt;城市农场规模小且低成本机器人（如FarmBot）的商业可用性，为植物表型分析提供了可访问的自动化平台。&lt;h4&gt;目的&lt;/h4&gt;使用带有定制相机末端执行器的FarmBot系统，从3D点云模型中估计草莓植物花朵的姿态，以支持机器人授粉。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新算法，将点云的个体占用网格沿正交轴平移获得六个视角的2D图像，使用2D目标检测模型识别花朵边界框，将其转换为3D空间提取花朵点云，并通过将三种几何形状（超椭球体、抛物面和平面）拟合到点云来进行姿态估计。&lt;h4&gt;主要发现&lt;/h4&gt;该方法成功识别了约80%的扫描花朵，平均姿态误差为7.7度，这一精度对于机器人授粉任务足够，且结果与先前研究相当。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能有效估计草莓花朵姿态，适用于机器人授粉应用，相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;城市农场的规模小和低成本机器人（如FarmBot）的商业可用性，这些机器人可以自动化简单的照料任务，为植物表型分析提供了一个可访问的平台。我们使用带有定制相机末端执行器的FarmBot，从获取的3D点云模型中估计草莓植物花朵的姿态（用于机器人授粉）。我们描述了一种新算法，该算法将点云的个体占用网格沿正交轴平移，以获得对应六个视角的2D图像。对于每张图像，使用花朵的2D目标检测模型来识别2D边界框，这些边界框可以转换为3D空间以提取花朵点云。通过将三种形状（超椭球体、抛物面和平面）拟合到花朵点云来进行姿态估计，并与手动标记的真实值进行比较。我们的方法成功找到了使用定制FarmBot平台扫描的约80%的花朵，平均花朵姿态误差为7.7度，这对于机器人授粉来说是足够的，并且与之前的结果相当。所有代码将在https://github.com/harshmuriki/flowerPose.git上公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市农场中草莓花朵姿态（位置和方向）的精确估计问题，这对机器人授粉至关重要。随着自然授粉媒介如蜜蜂的减少以及室内农场环境的需求，机器人授粉成为重要替代方案。精确的花朵姿态估计是机器人成功进行授粉操作的前提，直接影响授粉效率和成功率，对于小规模城市农场的自动化生产具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到城市农场规模小，可利用商业FarmBot机器人作为平台。他们专注于花朵姿态估计作为植物表型分析的关键任务。方法设计上，借鉴了现有计算机视觉技术，如使用Polycam进行3D重建、YOLOv10和Roboflow进行目标检测、DBSCAN进行点云分割。同时，作者创新性地开发了'转移占用网格'方法，将3D点云转换为多个2D视角，结合了3D空间信息和2D目标检测的优势，形成了一套完整的从数据采集到姿态估计的流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D点云捕捉花朵的空间信息，将其转换为多个2D视角便于检测，然后利用几何形状拟合确定花朵姿态。整体流程分为四步：1)数据采集：使用定制FarmBot围绕植物进行圆柱形和半球形扫描，获取多角度图像；2)3D模型生成：用Polycam软件从图像生成点云模型；3)转移占用网格方法：从六个正交方向将点云转为2D图像，用目标检测识别花朵并提取3D点云；4)花朵姿态估计：分离花瓣和雌蕊，用三种几何形状拟合确定花朵方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出'转移占用网格'方法将3D点云转换为六个正交方向的2D图像；2)结合3D点云处理和2D目标检测的优势；3)使用几何形状拟合精确估计花朵姿态；4)提供定量评估角度估计精度。相比之前工作，不同之处在于：之前研究主要使用离散2D图像推断方向，缺乏定量精度评估；该方法使用完整3D点云模型，提供连续姿态估计；适用于小规模城市农场特定环境，而非大型温室；花朵检测率约80%，平均姿态误差7.7度，满足机器人授粉需求。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文开发了一种基于FarmBot机器人和3D点云分析的草莓花朵姿态估计方法，通过创新的转移占用网格技术和几何形状拟合，实现了高精度的花朵检测和姿态估计，为城市农场的机器人授粉提供了关键技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICRA55743.2025.11128713&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The small scale of urban farms and the commercial availability of low-costrobots (such as the FarmBot) that automate simple tending tasks enable anaccessible platform for plant phenotyping. We have used a FarmBot with a customcamera end-effector to estimate strawberry plant flower pose (for roboticpollination) from acquired 3D point cloud models. We describe a novel algorithmthat translates individual occupancy grids along orthogonal axes of a pointcloud to obtain 2D images corresponding to the six viewpoints. For each image,2D object detection models for flowers are used to identify 2D bounding boxeswhich can be converted into the 3D space to extract flower point clouds. Poseestimation is performed by fitting three shapes (superellipsoids, paraboloidsand planes) to the flower point clouds and compared with manually labeledground truth. Our method successfully finds approximately 80% of flowersscanned using our customized FarmBot platform and has a mean flower pose errorof 7.7 degrees, which is sufficient for robotic pollination and rivals previousresults. All code will be made available athttps://github.com/harshmuriki/flowerPose.git.</description>
      <author>example@mail.com (Harsh Muriki, Hong Ray Teo, Ved Sengupta, Ai-Ping Hu)</author>
      <guid isPermaLink="false">2509.02870v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Improving the Resilience of Quadrotors in Underground Environments by Combining Learning-based and Safety Controllers</title>
      <link>http://arxiv.org/abs/2509.02808v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and awarded best paper at the 11th International Conference  on Control, Decision and Information Technologies (CoDIT 2025 -  https://codit2025.org/)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合基于学习控制器的安全控制器，用于四旋翼无人机在大型地下环境中的自主导航，通过归一化流模型监测环境分布外程度，在必要时切换到安全控制器。&lt;h4&gt;背景&lt;/h4&gt;四旋翼无人机在大型地下环境中的自主控制可用于环境调查、采矿作业和搜救等多个领域，但基于学习的控制器在未遇到过的'分布外'环境中泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在大型地下环境中安全高效导航的四旋翼无人机控制系统，解决基于学习控制器在分布外环境中泛化能力差的问题。&lt;h4&gt;方法&lt;/h4&gt;训练一个基于归一化流的环境先验模型，用于衡量四旋翼在任何时候的分布外程度；使用这个衡量标准作为运行时监控器，当四旋翼足够分布外时，在基于学习的控制器和安全控制器之间切换。&lt;h4&gt;主要发现&lt;/h4&gt;在基于DARPA地下挑战赛决赛数据集真实点云数据的模拟3D洞穴环境中进行点到点导航任务的实验表明，所提出的组合控制器同时具有基于学习控制器的活性（快速完成任务）和安全控制器的安全性（避免碰撞）。&lt;h4&gt;结论&lt;/h4&gt;通过结合归一化流模型和控制器切换机制，成功实现了四旋翼无人机在复杂地下环境中的安全高效自主导航。&lt;h4&gt;翻译&lt;/h4&gt;在大型地下环境中自主控制四旋翼无人机适用于环境调查、采矿作业和搜救等多个领域。基于学习的控制器是实现自主控制的一种有吸引力的方法，但已知它们对未在训练期间遇到的'分布外'环境泛化能力差。在这项工作中，我们训练了一个基于归一化流的环境先验模型，它提供了衡量四旋翼在任何时候分布外程度的指标。我们使用这个指标作为运行时监控器，使我们能够在充分分布外时在基于学习的控制器和安全控制器之间切换。我们的方法在基于DARPA地下挑战赛决赛数据集真实点云数据的模拟3D洞穴环境中的点到点导航任务上进行了基准测试。我们的实验结果表明，我们的组合控制器同时具有基于学习控制器的活性（快速完成任务）和安全控制器的安全性（避免碰撞）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是提高四旋翼无人机在地下环境中的韧性（鲁棒性），特别是当遇到与训练环境不同的'分布外'环境时的适应能力。这个问题在现实中非常重要，因为地下环境（如洞穴、矿井）具有复杂、未知、动态变化的特点，无人机需要在这样的环境中执行搜索救援、环境监测、采矿作业等任务。如果无人机不能适应这些复杂多变的环境，就无法可靠地完成任务，可能导致任务失败甚至设备损坏。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从两个角度思考问题：一方面，学习型控制器在训练环境中表现良好，完成任务速度快，但对新环境泛化能力差；另一方面，安全控制器对新环境更鲁棒，但完成任务速度慢。作者借鉴了现有的FLOWMPPI（使用归一化流改进的模型预测路径积分控制）作为学习型控制器，以及基于顺序凸规划（SCP）和增广拉格朗日迭代线性二次调节器（AL-iLQR）的安全控制器。此外，作者还利用了现有的分布外检测（OOD detection）方法。作者设计的核心思路是结合这两种控制器的优点：在熟悉环境中使用学习型控制器以获得速度优势，在不熟悉环境中切换到安全控制器以确保安全。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是训练一个基于归一化流的环境先验模型，用于判断当前环境是否与训练环境相似，并作为运行时监控器来决定使用哪种控制器。当检测到分布内环境时，使用学习型控制器以快速完成任务；当检测到分布外环境时，切换到安全控制器以确保安全。整体实现流程包括：1）训练阶段：使用贝叶斯模型强化学习训练FLOWMPPI控制器，同时训练变分自编码器（VAE）编码环境特征和环境编码的先验模型；2）测试阶段：在每个时间步计算OOD分数，低于阈值时使用学习型控制器，高于阈值时切换到安全控制器。安全控制器通过顺序凸规划生成可行轨迹，然后由AL-iLQR跟踪执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）在迄今为止最大的3D地下环境（41×62×11米，体积11492立方米）上训练了归一化流最优控制策略；2）设计了一个能够生成动态可行、避免碰撞轨迹的安全控制器；3）开发了一个运行时分布外监控器，根据环境、当前状态和目标状态智能切换控制器。相比之前的工作，本文首次系统性地结合了学习型控制器的'活性'（快速完成任务）和安全控制器的'安全性'（避免碰撞），解决了单一控制器在复杂多变环境中的局限性，显著提高了无人机在地下环境中的导航韧性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过智能切换学习型控制器和安全控制器，使四旋翼无人机在地下环境中既能快速完成任务又能确保安全，显著提高了其在复杂未知环境中的导航韧性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomously controlling quadrotors in large-scale subterranean environmentsis applicable to many areas such as environmental surveying, mining operations,and search and rescue. Learning-based controllers represent an appealingapproach to autonomy, but are known to not generalize well to`out-of-distribution' environments not encountered during training. In thiswork, we train a normalizing flow-based prior over the environment, whichprovides a measure of how far out-of-distribution the quadrotor is at any giventime. We use this measure as a runtime monitor, allowing us to switch between alearning-based controller and a safe controller when we are sufficientlyout-of-distribution. Our methods are benchmarked on a point-to-point navigationtask in a simulated 3D cave environment based on real-world point cloud datafrom the DARPA Subterranean Challenge Final Event Dataset. Our experimentalresults show that our combined controller simultaneously possesses the livenessof the learning-based controller (completing the task quickly) and the safetyof the safety controller (avoiding collision).</description>
      <author>example@mail.com (Isaac Ronald Ward, Mark Paral, Kristopher Riordan, Mykel J. Kochenderfer)</author>
      <guid isPermaLink="false">2509.02808v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Geodesics of Quantum Feature Maps on the space of Quantum Operators</title>
      <link>http://arxiv.org/abs/2509.02795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文从信息保留角度考虑点云的平滑黎曼几何结构，研究量子编码方案如何将其映射到量子算子空间SU(2^N)时变形几何结构，并为哈密顿量子特征映射建立了黎曼几何框架。&lt;h4&gt;背景&lt;/h4&gt;量子特征选择是量子机器学习的基本步骤，已有许多编码方案和技术来测试方案有效性，但量子特征映射的值域的黎曼流形结构尚未形式化。&lt;h4&gt;目的&lt;/h4&gt;为从欧几里得嵌入流形诱导的一般哈密顿量子特征映射建立黎曼几何，研究编码方案如何影响原始几何结构。&lt;h4&gt;方法&lt;/h4&gt;采用从基础开始的方法，推导向量空间及其相应度量的封闭形式，证明嵌入流形上的测地线与编码方案值域之间存在一一对应关系，并严格推导计算曲率的封闭形式方程。&lt;h4&gt;主要发现&lt;/h4&gt;建立了哈密顿量子特征映射的黎曼几何框架，证明了嵌入流形上的测地线与编码方案值域之间存在一一对应关系，得到了计算曲率的封闭形式方程。&lt;h4&gt;结论&lt;/h4&gt;论文以庞加莱半平面子集和两个常用特征映射的例子结束，验证了所提出理论的有效性。&lt;h4&gt;翻译&lt;/h4&gt;选择量子特征是量子机器学习中的一个基本步骤。已经提出了许多编码方案和技术来测试方案的有效性。从信息保留的角度，本文考虑了点云的平滑黎曼几何结构，以及编码方案一旦映射到量子算子空间SU(2^N)时如何变形这种几何结构。然而，量子特征映射的值域的黎曼流形结构尚未形式化。采用从基础开始的方法，本文从数学上建立了一般类哈密顿量子特征映射的黎曼几何，这类映射是从欧几里得嵌入流形诱导的。对于这种方法，我们首先推导了向量空间及其相应度量的封闭形式，并证明嵌入流形上的测地线与编码方案的值域之间存在一一对应关系。然后，我们严格推导了计算曲率的封闭形式方程。论文以庞加莱半平面子集和两个常用特征映射的例子结束。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Selecting a quantum feature is an essential step in quantum machine learning.There have been many proposed encoding schemes and proposed techniques to testthe efficacy of a scheme. From the perspective of information retention, thispaper considers the smooth Riemannian geometry structure of a point cloud andhow an encoding scheme deforms this geometry once mapped to the space ofquantum operators, $\SU(2^N)$. However, a Riemannian manifold structure of thecodomain of a quantum feature map has yet to be formalized. Using a ground-upapproach, this manuscript mathematically establishes a Riemannian geometry fora general class of Hamiltonian quantum feature maps that are induced from aEuclidean embedded manifold. For this ground-up approach, we first derive aclosed form of a vector space and a respective metric, and prove there is a 1-1correspondence from geodesics on the embedded manifold to the codomain of theencoding scheme. We then rigorously derive closed form equations to calculatecurvature. The paper ends with an example with a subset of the Poincar\'ehalf-plane and two well-used feature maps.</description>
      <author>example@mail.com (Andrew Vlasic)</author>
      <guid isPermaLink="false">2509.02795v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Unifi3D: A Study on 3D Representations for Generation and Reconstruction in a Common Framework</title>
      <link>http://arxiv.org/abs/2509.02474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个统一的3D表示评估框架，用于比较不同3D表示方法在重建和生成任务中的性能，并确定了3D生成管道各步骤的最佳实践。&lt;h4&gt;背景&lt;/h4&gt;随着文本和图像生成技术的快速发展，研究日益转向3D生成领域。与图像中成熟的像素表示不同，3D表示方法多样且分散，包括体素网格、神经辐射场、符号距离函数、点云或八叉树等，每种方法都有其独特的优势和局限性。&lt;h4&gt;目的&lt;/h4&gt;设计并实施一个统一的评估框架，用于评估不同3D表示方法在重建和生成任务中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;基于多个标准比较这些3D表示方法：质量、计算效率和泛化性能。实验旨在确定3D生成管道中所有步骤的最佳实践，包括预处理、网格重建、使用自编码器压缩和生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;重建错误显著影响整体性能，强调需要联合评估生成和重建过程，而非孤立地评估每个组件。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了有价值的见解，可为各种应用选择合适的3D模型提供指导，促进3D生成领域中更稳健和应用特定解决方案的发展。相关代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;随着文本和图像生成的快速进展，研究日益转向3D生成。与图像中成熟的基于像素的表示不同，3D表示仍然多样且分散，包含多种方法，如体素网格、神经辐射场、符号距离函数、点云或八叉树，每种方法都有其独特的优势和局限性。在这项工作中，我们提出了一个统一的评估框架，旨在评估3D表示在重建和生成任务中的性能。我们基于多个标准比较这些表示方法：质量、计算效率和泛化性能。除了标准的模型基准测试外，我们的实验旨在确定3D生成管道中所有步骤的最佳实践，包括预处理、网格重建、使用自编码器压缩和生成。我们的研究结果表明，重建错误显著影响整体性能，强调了需要联合评估生成和重建的重要性。我们提供的见解可以为各种应用选择合适的3D模型提供参考，促进3D生成领域更稳健和应用特定解决方案的发展。我们框架的代码可在https://github.com/isl-org/unifi3d获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D表示方法缺乏统一评估框架的问题。在现实中，随着3D生成技术的发展，存在多种3D表示方法（如体素网格、神经辐射场、有符号距离函数等），但难以公平比较它们的性能。这个问题很重要，因为3D表示的选择直接影响生成质量、计算效率和最终应用效果，缺乏统一评估标准阻碍了3D生成领域的进步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D表示评估面临的挑战：表示方法嵌入在复杂流程中，多种预处理技术影响结果，缺乏统一评估指标。他们设计了一个标准化的生成管道，包含从数据预处理到网格重建的所有步骤，采用模块化设计允许不同表示的互换。作者借鉴了多种现有3D表示方法和生成模型，如扩散模型和自回归模型，使用ShapeNet等标准数据集和Chamfer Distance等评估指标。创新点在于将这些现有方法整合到一个统一的评估框架中，而不是简单地综述现有方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供一个统一的框架来公平比较不同3D表示方法在生成和重建任务中的性能，强调重建和生成的联合评估。整体流程包括：1)表示转换：将3D网格转换为合适表示；2)表示压缩：使用自编码器压缩到潜在空间；3)潜在生成：训练扩散模型生成潜在向量；4)网格重建：从解码表示重建最终网格。评估包括重建质量、压缩性能、泛化能力和生成质量四个方面，结合定量指标和用户研究进行综合评估。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的评估框架，首次实现不同3D表示方法的公平比较；2)联合评估重建和生成能力，发现重建误差可能高达生成误差的20%；3)提供最佳实践指南，解决预处理等问题；4)开源模块化代码库，包含新组件如双八叉树生成方法。相比之前工作，这篇论文不依赖单个3D生成方法的实现，而是控制变量进行比较；不仅评估生成质量，还分析重建与生成的关系；提供标准化流程解决非水密网格等常见问题；比较多种表示方法在多个维度上的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Unifi3D提供了一个统一的评估框架，通过标准化流程和联合评估重建与生成能力，首次实现了对多种3D表示方法的公平比较，并确定了不同表示方法的优势和适用场景。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following rapid advancements in text and image generation, research hasincreasingly shifted towards 3D generation. Unlike the well-establishedpixel-based representation in images, 3D representations remain diverse andfragmented, encompassing a wide variety of approaches such as voxel grids,neural radiance fields, signed distance functions, point clouds, or octrees,each offering distinct advantages and limitations. In this work, we present aunified evaluation framework designed to assess the performance of 3Drepresentations in reconstruction and generation. We compare theserepresentations based on multiple criteria: quality, computational efficiency,and generalization performance. Beyond standard model benchmarking, ourexperiments aim to derive best practices over all steps involved in the 3Dgeneration pipeline, including preprocessing, mesh reconstruction, compressionwith autoencoders, and generation. Our findings highlight that reconstructionerrors significantly impact overall performance, underscoring the need toevaluate generation and reconstruction jointly. We provide insights that caninform the selection of suitable 3D models for various applications,facilitating the development of more robust and application-specific solutionsin 3D generation. The code for our framework is available athttps://github.com/isl-org/unifi3d.</description>
      <author>example@mail.com (Nina Wiedemann, Sainan Liu, Quentin Leboutet, Katelyn Gao, Benjamin Ummenhofer, Michael Paulitsch, Kai Yuan)</author>
      <guid isPermaLink="false">2509.02474v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Sem-RaDiff: Diffusion-Based 3D Radar Semantic Perception in Cluttered Agricultural Environments</title>
      <link>http://arxiv.org/abs/2509.02283v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于雷达的3D环境感知框架，解决了光学传感器在农业环境中的局限性，通过三个核心模块实现了密集准确的语义感知，并在性能和计算效率上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;准确且鲁棒的环境感知对机器人自主导航至关重要。当前方法主要采用光学传感器，但它们易受视觉遮挡影响，导致性能下降或系统故障。农业场景中机器人面临传感器污染风险。&lt;h4&gt;目的&lt;/h4&gt;利用雷达的强穿透能力，开发基于雷达的3D环境感知框架作为光学传感器的替代方案，实现密集且准确的语义感知。&lt;h4&gt;方法&lt;/h4&gt;提出了包含三个核心模块的雷达3D感知框架：1) 并行帧积累增强雷达原始数据信噪比；2) 基于扩散模型的分层学习框架过滤旁瓣伪影并生成细粒度3D语义点云；3) 专门设计的稀疏3D网络优化处理大规模雷达原始数据。在真实农业场景数据集上进行了实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，该方法在结构和语义预测性能上更优，计算成本降低51.3%，内存成本降低27.5%，能够完全重建和准确分类细小结构（如电线杆和电线），而现有方法难以感知这些结构。&lt;h4&gt;结论&lt;/h4&gt;该方法在密集准确的3D雷达感知方面具有潜力，特别适用于光学传感器易受影响的农业环境。&lt;h4&gt;翻译&lt;/h4&gt;准确且鲁棒的环境感知对机器人自主导航至关重要。虽然当前方法通常采用光学传感器（如摄像头、激光雷达）作为主要感知模式，但它们易受视觉遮挡影响，常常导致性能下降或系统完全失效。本文关注农业场景中机器人面临传感器污染风险的情况。利用雷达的强穿透能力，我们引入了一个基于雷达的3D环境感知框架作为可行的替代方案。它包含三个为密集准确语义感知设计的核心模块：1) 并行帧积累以增强雷达原始数据的信噪比；2) 基于扩散模型的分层学习框架，先过滤雷达旁瓣伪影，然后生成细粒度3D语义点云；3) 专门设计的稀疏3D网络，优化处理大规模雷达原始数据。我们在真实农业场景中收集的自建数据集上进行了广泛的基准比较和实验评估。结果表明，与现有方法相比，我们的方法在结构和语义预测性能上更优，同时分别将计算和内存成本降低了51.3%和27.5%。此外，我们的方法能够完全重建并准确分类细小结构（如电线杆和电线）-现有方法难以感知这些结构-凸显了其在密集准确3D雷达感知方面的潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决农业机器人在复杂环境中使用光学传感器（如摄像头、激光雷达）时，由于传感器被污染（如泥土、水滴遮挡）导致的环境感知性能下降问题。同时，虽然毫米波雷达具有穿透能力，但存在信噪比低、角分辨率低等问题。这个问题在现实中很重要，因为农业环境恶劣，传感器容易被污染，导致机器人'失明'，而精准的环境感知对农业机器人的自主导航和作业效率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了毫米波雷达在农业环境中的三大挑战：低信噪比、低角分辨率和旁瓣污染、大规模数据。然后指出现有方法（判别式学习和生成式学习）的不足，判别式学习难以处理雷达数据与真实数据之间的巨大差距，生成式学习面临训练困难和计算开销大的问题。作者借鉴了扩散模型在生成高质量点云方面的优势和判别式学习的结构预测能力，设计了一个从粗到细的框架，结合了并行帧累积、分层扩散学习和稀疏网络处理技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用从粗到细的分层学习框架，利用雷达的穿透能力替代易受污染的光学传感器，通过并行帧累积提高信噪比，使用扩散模型生成高质量的3D语义点云，并采用稀疏表示降低计算和内存开销。整体流程包括：1）数据预处理（并行帧累积生成RCC和RPC）；2）第一阶段预测粗粒度结构掩码和语义掩码；3）第二阶段使用扩散模型生成精细的3D语义点云；4）输出最终的3D语义点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出首个解决毫米波雷达感知增强和语义分割的系统性方案Sem-RaDiff；2）设计并行帧累积算法提高信噪比；3）提出从粗到细的分层扩散学习框架；4）引入稀疏3D表示和专用网络提高计算效率；5）集成扩散模型的一步推理技术便于部署。相比之前工作，不同之处在于：结合判别式和生成式方法的优点，同时使用RCC和RPC作为输入，采用两阶段处理先过滤噪声再生成精细点云，针对农业场景特殊需求优化，计算和内存开销显著降低。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Sem-RaDiff通过创新的从粗到细的扩散模型框架和稀疏网络设计，显著提高了毫米波雷达在复杂农业环境中的3D语义感知能力，同时大幅降低了计算和内存开销，为农业机器人提供了一种在传感器被污染情况下的可靠感知替代方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust environmental perception is crucial for robot autonomousnavigation. While current methods typically adopt optical sensors (e.g.,camera, LiDAR) as primary sensing modalities, their susceptibility to visualocclusion often leads to degraded performance or complete system failure. Inthis paper, we focus on agricultural scenarios where robots are exposed to therisk of onboard sensor contamination. Leveraging radar's strong penetrationcapability, we introduce a radar-based 3D environmental perception framework asa viable alternative. It comprises three core modules designed for dense andaccurate semantic perception: 1) Parallel frame accumulation to enhancesignal-to-noise ratio of radar raw data. 2) A diffusion model-basedhierarchical learning framework that first filters radar sidelobe artifactsthen generates fine-grained 3D semantic point clouds. 3) A specificallydesigned sparse 3D network optimized for processing large-scale radar raw data.We conducted extensive benchmark comparisons and experimental evaluations on aself-built dataset collected in real-world agricultural field scenes. Resultsdemonstrate that our method achieves superior structural and semanticprediction performance compared to existing methods, while simultaneouslyreducing computational and memory costs by 51.3% and 27.5%, respectively.Furthermore, our approach achieves complete reconstruction and accurateclassification of thin structures such as poles and wires-which existingmethods struggle to perceive-highlighting its potential for dense and accurate3D radar perception.</description>
      <author>example@mail.com (Ruibin Zhang, Fei Gao)</author>
      <guid isPermaLink="false">2509.02283v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Geometry Compression and Communication for 3D Gaussian Splatting Point Clouds</title>
      <link>http://arxiv.org/abs/2509.02232v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对基于i3DV平台的动态3D场景表示中的存储和传输挑战，提出了一种结合AVS PCRM压缩技术和哈希表优化的联合框架，实现了3D体积视频的高效存储、传输和交互。&lt;h4&gt;背景&lt;/h4&gt;随着场景复杂度的增加，3D高斯数据量爆炸式增长导致存储空间占用过高，给3D体积视频的存储和传输带来挑战。&lt;h4&gt;目的&lt;/h4&gt;解决动态3D场景表示中的存储和传输问题，同时保持3D高斯技术的快速渲染和高质量合成优势。&lt;h4&gt;方法&lt;/h4&gt;采用AVS PCRM参考软件压缩高斯点云几何数据，并将其深度集成到i3DV平台中，与基于二进制哈希表的原始率失真优化机制形成技术互补。哈希表用于高效缓存帧间高斯点变换关系，AVS PCRM用于精确压缩几何数据。&lt;h4&gt;主要发现&lt;/h4&gt;联合框架在保持3D高斯技术快速渲染和高质量合成优势的同时，在通用测试集上实现了10%-25%的比特率节省，在40 Mbps带宽限制内实现高保真传输。&lt;h4&gt;结论&lt;/h4&gt;该研究为3D体积视频的存储、传输和交互提供了优越的率失真权衡解决方案，显著提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;基于i3DV平台的动态3D场景表示中的存储和传输挑战，随着场景复杂度的增加，3D高斯数据量的爆炸式增长导致了过高的存储空间占用。为了解决这个问题，我们提出采用AVS PCRM参考软件来高效压缩高斯点云几何数据。该策略将AVS PCRM的高级编码能力深度集成到i3DV平台中，与基于二进制哈希表的原始率失真优化机制形成技术互补。一方面，哈希表高效缓存帧间高斯点变换关系，使得在40 Mbps带宽限制内实现高保真传输；另一方面，AVS PCRM对几何数据进行精确压缩。实验结果表明，联合框架在保持3D高斯技术快速渲染和高质量合成优势的同时，在通用测试集上实现了显著的10%-25%比特率节省。它为3D体积视频的存储、传输和交互提供了优越的率失真权衡解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3680207.3765659&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Storage and transmission challenges in dynamic 3D scene representation basedon the i3DV platform, With increasing scene complexity, the explosive growth of3D Gaussian data volume causes excessive storage space occupancy. To addressthis issue, we propose adopting the AVS PCRM reference software for efficientcompression of Gaussian point cloud geometry data. The strategy deeplyintegrates the advanced encoding capabilities of AVS PCRM into the i3DVplatform, forming technical complementarity with the original rate-distortionoptimization mechanism based on binary hash tables. On one hand, the hash tableefficiently caches inter-frame Gaussian point transformation relationships,which allows for high-fidelity transmission within a 40 Mbps bandwidthconstraint. On the other hand, AVS PCRM performs precise compression ongeometry data. Experimental results demonstrate that the joint frameworkmaintains the advantages of fast rendering and high-quality synthesis in 3DGaussian technology while achieving significant 10\%-25\% bitrate savings onuniversal test sets. It provides a superior rate-distortion tradeoff solutionfor the storage, transmission, and interaction of 3D volumetric video.</description>
      <author>example@mail.com (Liang Xie, Yanting Li, Luyang Tang, Wei Gao)</author>
      <guid isPermaLink="false">2509.02232v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>FGO-SLAM: Enhancing Gaussian SLAM with Globally Consistent Opacity Radiance Field</title>
      <link>http://arxiv.org/abs/2509.01547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FGO-SLAM是一种创新的高斯SLAM系统，通过不透明辐射场表示和全局优化技术，显著提升了场景重建质量和跟踪精度，解决了传统SLAM和高斯SLAM系统的局限性。&lt;h4&gt;背景&lt;/h4&gt;视觉SLAM因其能够为具身AI提供感知能力和仿真测试数据而重新受到关注。然而，传统SLAM方法难以满足高质量场景重建的需求，而高斯SLAM系统尽管具有快速渲染和高质量映射的能力，但缺乏有效的姿态优化方法，并且在几何重建方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统SLAM方法在高质量场景重建方面的不足，以及高斯SLAM系统缺乏有效姿态优化方法和几何重建挑战的问题。&lt;h4&gt;方法&lt;/h4&gt;引入FGO-SLAM系统，采用不透明辐射场作为场景表示以增强几何映射性能；在初始姿态估计后应用全局调整优化相机姿态和稀疏点云；基于三维高斯保持全局一致的不透明辐射场，引入深度失真和法线一致性项细化场景表示；构建四面体网格后识别水平集，直接从三维高斯中提取表面。&lt;h4&gt;主要发现&lt;/h4&gt;在各种真实世界和大规模合成数据集上的结果表明，FGO-SLAM实现了最先进的跟踪精度和映射性能。&lt;h4&gt;结论&lt;/h4&gt;FGO-SLAM系统通过不透明辐射场和全局优化方法，有效解决了传统SLAM和高斯SLAM系统的局限性，实现了高质量的场景重建和精确的跟踪。&lt;h4&gt;翻译&lt;/h4&gt;视觉SLAM因其能够为具身AI提供感知能力和仿真测试数据而重新受到关注。然而，传统SLAM方法难以满足高质量场景重建的需求，而高斯SLAM系统尽管具有快速渲染和高质量映射的能力，但缺乏有效的姿态优化方法，并且在几何重建方面面临挑战。为解决这些问题，我们引入了FGO-SLAM，一种采用不透明辐射场作为场景表示的高斯SLAM系统，以增强几何映射性能。在初始姿态估计后，我们应用全局调整来优化相机姿态和稀疏点云，确保方法的鲁棒跟踪。此外，我们基于三维高斯保持全局一致的不透明辐射场，并引入深度失真和法线一致性项来细化场景表示。进一步地，在构建四面体网格后，我们识别水平集以直接从三维高斯中提取表面。在各种真实世界和大规模合成数据集上的结果表明，我们的方法实现了最先进的跟踪精度和映射性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统SLAM方法难以满足高质量场景重建需求的问题，以及现有Gaussian SLAM系统缺乏有效姿态优化方法和几何重建能力的问题。这个问题在现实和研究中很重要，因为具身人工智能(Embodied AI)需要密集的视觉SLAM方法来提供感知能力并重建高质量场景地图，而传统方法生成的低分辨率、有空洞且缺乏纹理细节的地图已无法满足AR/VR、机器人导航等应用的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统SLAM地图质量不高，NeRF-based SLAM存在灾难性遗忘和过度平滑问题，3D Gaussian Splatting虽渲染快但缺乏有效姿态优化。作者借鉴了3DGS的高质量重建能力、传统视觉里程计的鲁棒性、光度一致性技术以及Marching Tetrahedra表面提取算法，设计了FGO-SLAM系统，结合了传统SLAM方法和3D高斯表示的优点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用全局一致的透明度辐射场作为场景表示，结合传统SLAM和3D高斯表示的优点。整体流程分为三部分：1)跟踪模块：提取特征点估计相机姿态，检测回环并进行全局优化；2)映射模块：构建基于3D高斯的透明度辐射场，使用正则化项优化场景表示；3)表面提取模块：为每个高斯构建边界框和四面体网格，通过评估顶点透明度识别等值集，使用Marching Tetrahedra算法提取网格。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)全局一致的透明度辐射场作为场景表示；2)全局姿态优化技术；3)深度失真和法线一致性正则化项；4)直接从3D高斯体提取表面的方法。相比之前工作，FGO-SLAM解决了传统SLAM地图质量低、NeRF-based SLAM速度慢和过度平滑、现有Gaussian SLAM缺乏有效姿态优化和表面提取能力的问题，同时支持单目相机，扩大了应用范围。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FGO-SLAM通过引入全局一致的透明度辐射场和全局优化技术，解决了现有Gaussian SLAM系统在姿态优化和几何重建方面的挑战，实现了高质量的场景重建和表面提取。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual SLAM has regained attention due to its ability to provide perceptualcapabilities and simulation test data for Embodied AI. However, traditionalSLAM methods struggle to meet the demands of high-quality scene reconstruction,and Gaussian SLAM systems, despite their rapid rendering and high-qualitymapping capabilities, lack effective pose optimization methods and facechallenges in geometric reconstruction. To address these issues, we introduceFGO-SLAM, a Gaussian SLAM system that employs an opacity radiance field as thescene representation to enhance geometric mapping performance. After initialpose estimation, we apply global adjustment to optimize camera poses and sparsepoint cloud, ensuring robust tracking of our approach. Additionally, wemaintain a globally consistent opacity radiance field based on 3D Gaussians andintroduce depth distortion and normal consistency terms to refine the scenerepresentation. Furthermore, after constructing tetrahedral grids, we identifylevel sets to directly extract surfaces from 3D Gaussians. Results acrossvarious real-world and large-scale synthetic datasets demonstrate that ourmethod achieves state-of-the-art tracking accuracy and mapping performance.</description>
      <author>example@mail.com (Fan Zhu, Yifan Zhao, Ziyu Chen, Biao Yu, Hui Zhu)</author>
      <guid isPermaLink="false">2509.01547v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Continuous-Time Consistency Model for 3D Point Cloud Generation</title>
      <link>http://arxiv.org/abs/2509.01492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ConTiCoM-3D的连续时间一致性模型，用于快速准确地从点云生成3D形状，在机器人、AR/VR和数字内容创建领域具有广泛应用。&lt;h4&gt;背景&lt;/h4&gt;快速准确的3D形状生成对机器人、AR/VR和数字内容创建应用至关重要，但现有方法通常依赖于迭代去噪或潜在解码器，效率不高。&lt;h4&gt;目的&lt;/h4&gt;开发一种直接在点空间中合成3D形状的连续时间一致性模型，无需离散扩散步骤、预训练教师模型或潜在空间编码，实现高效高质量的3D形状生成。&lt;h4&gt;方法&lt;/h4&gt;ConTiCoM-3D整合了受TrigFlow启发的连续噪声计划和基于Chamfer距离的几何损失，使用时间条件神经网络在连续时间上操作，支持高效的一步至两步推理，同时避免昂贵的雅可比-向量乘积。&lt;h4&gt;主要发现&lt;/h4&gt;在ShapeNet基准测试上，ConTiCoM-3D在质量和效率方面匹配或优于最先进的扩散和潜在一致性模型，实现了高几何保真度的快速生成。&lt;h4&gt;结论&lt;/h4&gt;ConTiCoM-3D是可扩展3D形状生成的实用框架，为3D内容创作提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;快速且准确的从点云生成3D形状对机器人、AR/VR和数字内容创建应用至关重要。我们引入了ConTiCoM-3D，一种连续时间一致性模型，直接在点空间中合成3D形状，无需离散扩散步骤、预训练教师模型或潜在空间编码。该方法整合了受TrigFlow启发的连续噪声计划与基于Chamfer距离的几何损失，能够在高维点集上进行稳定训练，同时避免昂贵的雅可比-向量乘积。这种设计支持高效的一步至两步推理，并具有高几何保真度。与依赖迭代去噪或潜在解码器的先前方法相比，ConTiCoM-3D采用完全在连续时间上运行的时间条件神经网络，从而实现快速生成。ShapeNet基准测试上的实验表明，ConTiCoM-3D在质量和效率方面均匹配或优于最先进的扩散和潜在一致性模型，确立了其作为可扩展3D形状生成的实用框架的地位。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云生成中的效率和精度平衡问题。现有的3D生成方法要么需要大量计算步骤（如扩散模型），要么依赖复杂的预训练模型（如潜在一致性模型），导致生成速度慢或质量下降。这个问题在机器人、AR/VR和数字内容创作等领域至关重要，因为这些应用通常需要在硬件限制下实现实时高质量3D形状生成。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D生成方法的局限性：VAE/GAN速度快但多样性不足，流模型需要迭代采样，扩散模型计算昂贵，潜在方法引入伪影。他们借鉴了TrigFlow的连续噪声调度和Flow Matching的速度场学习技术，但创新性地避免了这些方法的缺点。设计思路是创建一个直接在原始点空间操作的模型，不依赖教师模型、Jacobian计算或潜在空间压缩，而是结合解析流匹配和Chamfer几何损失，实现稳定高效的训练。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是直接在原始点空间中建模连续时间一致性，结合解析流匹配和几何感知损失。整体流程包括：1) 使用TrigFlow前向过程将干净点云逐步添加噪声；2) 设计时间条件神经网络预测速度场；3) 通过流匹配损失确保学习正确速度场，同时用Chamfer损失保证几何保真度；4) 推理时只需1-2步即可生成高质量点云，比传统扩散模型快数百倍。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个直接在原始点空间操作的连续时间一致性模型，无需潜在编码；2) 避免了计算成本高且不稳定的Jacobian-向量乘积；3) 摒弃了依赖预训练教师模型的蒸馏方法；4) 使用Chamfer距离提供几何感知的排列不变监督。相比之前的工作，ConTiCoM-3D在保持高质量的同时实现了显著更快的推理速度（0.22秒/样本），且避免了潜在空间压缩带来的结构伪影，在ShapeNet基准测试中超越了扩散模型和潜在一致性模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ConTiCoM-3D首次实现了直接在原始点空间中操作的连续时间一致性模型，通过结合解析流匹配和Chamfer重建损失，实现了高效的一到两步高质量3D点云生成，避免了现有方法中的计算瓶颈和质量损失问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and accurate 3D shape generation from point clouds is essential forapplications in robotics, AR/VR, and digital content creation. We introduceConTiCoM-3D, a continuous-time consistency model that synthesizes 3D shapesdirectly in point space, without discretized diffusion steps, pre-trainedteacher models, or latent-space encodings. The method integrates aTrigFlow-inspired continuous noise schedule with a Chamfer Distance-basedgeometric loss, enabling stable training on high-dimensional point sets whileavoiding expensive Jacobian-vector products. This design supports efficientone- to two-step inference with high geometric fidelity. In contrast toprevious approaches that rely on iterative denoising or latent decoders,ConTiCoM-3D employs a time-conditioned neural network operating entirely incontinuous time, thereby achieving fast generation. Experiments on the ShapeNetbenchmark show that ConTiCoM-3D matches or outperforms state-of-the-artdiffusion and latent consistency models in both quality and efficiency,establishing it as a practical framework for scalable 3D shape generation.</description>
      <author>example@mail.com (Sebastian Eilermann, René Heesch, Oliver Niggemann)</author>
      <guid isPermaLink="false">2509.01492v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PointSlice: Accurate and Efficient Slice-Based Representation for 3D Object Detection from Point Clouds</title>
      <link>http://arxiv.org/abs/2509.01487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Manuscript submitted to PATTERN RECOGNITION, currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PointSlice是一种新的点云处理方法，通过将3D点云转换为2D切片并结合专门的检测网络，实现了高检测精度和推理速度的平衡。&lt;h4&gt;背景&lt;/h4&gt;3D目标检测在自动驾驶中起关键作用，当前主要方法有基于体素(高精度但慢)和基于柱状(速度快但精度低)两种方法。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云处理方法中精度与速度之间的权衡问题，提出一种既能保持高精度又能提高推理速度的新方法。&lt;h4&gt;方法&lt;/h4&gt;PointSlice沿水平平面切割点云转换为多组2D(x-y)数据切片，模型只学习2D数据分布；引入切片交互网络(SIN)保持切片间的垂直关系，提高3D物体感知能力。&lt;h4&gt;主要发现&lt;/h4&gt;PointSlice在多个数据集上表现优异：在Waymo上比SAFDNet快1.13倍，参数减少0.79倍，精度仅降1.2 mAPH；在nuScenes上达到66.74 mAP的最先进结果；在Argoverse 2上快1.10倍，参数减少0.66倍，精度降1.0 mAP。&lt;h4&gt;结论&lt;/h4&gt;PointSlice成功平衡了检测精度与推理速度，是一种高效的3D目标检测方法。&lt;h4&gt;翻译&lt;/h4&gt;3D点云目标检测在自动驾驶中扮演关键角色。目前，点云处理的主要方法是基于体素和基于柱状的方法。基于体素的方法通过细粒度空间分割提供高精度，但推理速度较慢。基于柱状的方法提高了推理速度，但在精度上仍不及基于体素的方法。为解决这些问题，我们提出了一种新的点云处理方法PointSlice，它沿水平平面切割点云并包含专门的检测网络。PointSlice的主要贡献是：(1)一种新的点云处理技术，将3D点云转换为多组2D(x-y)数据切片。模型只学习2D数据分布，将3D点云视为独立的2D数据批次，这减少了模型参数数量并提高了推理速度；(2)引入切片交互网络(SIN)。为了保持切片间的垂直关系，我们将SIN集成到2D骨干网络中，提高了模型的3D物体感知能力。大量实验表明，PointSlice实现了高检测精度和推理速度。在Waymo数据集上，PointSlice比最先进的基于体素方法(SAFDNet)快1.13倍，参数减少0.79倍，精度仅降低1.2 mAPH。在nuScenes数据集上，我们实现了66.74 mAP的最先进检测结果。在Argoverse 2数据集上，PointSlice快1.10倍，参数减少0.66倍，精度降低1.0 mAP。代码将在https://github.com/qifeng22/PointSlice2上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云目标检测中效率与准确性之间的权衡问题。当前基于体素的方法精度高但速度慢，而基于柱体的方法速度快但精度低。这个问题在自动驾驶等领域至关重要，因为3D目标检测需要同时满足高精度和实时性要求，以保障安全和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点，发现将基于体素的方法直接应用于基于柱体格式会导致精度显著下降。因此，作者提出将3D点云沿水平面切片转换为多组2D数据，使模型能使用高效的2D卷积网络。该方法借鉴了现有工作中的稀疏卷积技术、稀疏编码器-解码块和稀疏检测头等，并创新性地设计了切片交互网络(SIN)来保持不同切片间的垂直关系，弥补单纯使用2D切片可能丢失的3D信息。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D点云沿水平面切片转换为多组2D数据，使用2D卷积网络提高效率，同时引入切片交互网络(SIN)保留3D信息。整体流程分为四步：1)点云到切片转换：将点云体素化后沿高度方向转换为2D切片；2)稀疏2D骨干网络：使用2D稀疏残差块和编码器-解码块提取特征；3)切片交互网络：通过3D稀疏卷积实现跨切片信息交换；4)稀疏检测头：输出最终检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的点云表示方法：将3D点云转换为多组2D切片，减少模型参数；2)切片交互网络(SIN)：专门设计用于保持不同切片间的垂直关系；3)高效的3D目标检测框架：结合2D卷积的高效率和3D卷积的准确性。相比之前的工作，PointSlice在保持接近基于体素方法精度的同时，显著提高了推理速度(1.13倍)并减少了模型参数(0.79倍)，同时解决了基于柱体方法精度不足的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointSlice通过创新的水平切片表示方法和切片交互网络，实现了3D点云目标检测中高精度与高效率的有效平衡，在保持接近基于体素方法精度的同时显著提高了推理速度并减少了模型参数。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection from point clouds plays a critical role in autonomousdriving. Currently, the primary methods for point cloud processing arevoxel-based and pillarbased approaches. Voxel-based methods offer high accuracythrough fine-grained spatial segmentation but suffer from slower inferencespeeds. Pillar-based methods enhance inference speed but still fall short ofvoxel-based methods in accuracy. To address these issues, we propose a novelpoint cloud processing method, PointSlice, which slices point clouds along thehorizontal plane and includes a dedicated detection network. The maincontributions of PointSlice are: (1) A new point cloud processing techniquethat converts 3D point clouds into multiple sets of 2D (x-y) data slices. Themodel only learns 2D data distributions, treating the 3D point cloud asseparate batches of 2D data, which reduces the number of model parameters andenhances inference speed; (2) The introduction of a Slice Interaction Network(SIN). To maintain vertical relationships across slices, we incorporate SINinto the 2D backbone network, which improves the model's 3D object perceptioncapability. Extensive experiments demonstrate that PointSlice achieves highdetection accuracy and inference speed. On the Waymo dataset, PointSlice is1.13x faster and has 0.79x fewer parameters than the state-of-the-artvoxel-based method (SAFDNet), with only a 1.2 mAPH accuracy reduction. On thenuScenes dataset, we achieve a state-of-the-art detection result of 66.74 mAP.On the Argoverse 2 dataset, PointSlice is 1.10x faster, with 0.66x fewerparameters and a 1.0 mAP accuracy reduction. The code will be available athttps://github.com/qifeng22/PointSlice2.</description>
      <author>example@mail.com (Liu Qifeng, Zhao Dawei, Dong Yabo, Xiao Liang, Wang Juan, Min Chen, Li Fuyang, Jiang Weizhong, Lu Dongming, Nie Yiming)</author>
      <guid isPermaLink="false">2509.01487v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Guided Model-based LiDAR Super-Resolution for Resource-Efficient Automotive scene Segmentation</title>
      <link>http://arxiv.org/abs/2509.01317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种端到端框架，联合解决LiDAR超分辨率和语义分割问题，使低成本16通道LiDAR能够实现与高成本64通道LiDAR相当的分割性能。&lt;h4&gt;背景&lt;/h4&gt;高分辨率LiDAR数据在自动驾驶3D语义分割中起关键作用，但高级传感器成本高限制了大规模部署；而低成本传感器如16通道LiDAR产生的稀疏点云会降低分割精度。&lt;h4&gt;目的&lt;/h4&gt;克服低成本传感器导致的精度下降问题，提出一个端到端框架同时解决LiDAR超分辨率和语义分割。&lt;h4&gt;方法&lt;/h4&gt;引入首个端到端框架，通过训练过程中的联合优化使SR模块融入语义线索并保留精细细节；设计新的SR损失函数指导网络关注感兴趣区域；采用轻量级基于模型的SR架构，参数量少于现有方法，且易于与分割网络兼容。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法实现的分割性能可与使用高分辨率且昂贵的64通道LiDAR数据的模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该框架能够在保持较低成本的同时实现高精度的3D语义分割，解决了低成本传感器部署的瓶颈问题。&lt;h4&gt;翻译&lt;/h4&gt;高分辨率LiDAR数据在自动驾驶的3D语义分割中起着关键作用，但高级传感器的高成本限制了大规模部署。相比之下，16通道LiDAR等低成本传感器产生的稀疏点云会降低分割精度。为克服这一问题，我们引入了首个端到端框架，联合解决LiDAR超分辨率(SR)和语义分割问题。该框架在训练过程中采用联合优化，使SR模块能够融入语义线索并保留精细细节，特别是对小目标类别。新的SR损失函数进一步指导网络关注感兴趣区域。提出的轻量级基于模型的SR架构比现有LiDAR SR方法使用少得多的参数，同时保持与分割网络的良好兼容性。实验表明，我们的方法实现了与使用高分辨率且昂贵的64通道LiDAR数据的模型相当的分割性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决高分辨率LiDAR传感器成本高昂（约85,000美元）而低成本传感器（如16通道LiDAR，约4,000美元）产生的稀疏点云导致自动驾驶场景中3D语义分割性能下降的问题。这个问题在现实中很重要，因为它限制了高性能自动驾驶感知系统的广泛采用，而现有超分辨率方法与分割任务独立优化导致性能不佳，特别是对较小类别的细节保留不足。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了高分辨率LiDAR数据的重要性与高成本之间的矛盾，以及现有SR方法与分割任务独立优化的局限性。他们借鉴了投影方法（如FIDNet、CENet、LENet）将3D点云转换为2D表示，以及深度展开策略和半二次分裂方法。作者设计了一个轻量级的基于模型的SR网络，通过引入分割指导的可学习掩模模块和上下文感知损失函数，实现了SR和分割任务的联合优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将SR和分割任务整合在一个统一的端到端框架中，通过联合训练确保SR过程受益于分割指导，同时使用轻量级模型提高效率。整体流程是：1)将16通道低分辨率LiDAR点云转换为低分辨率距离图像；2)SR网络处理（包括数据一致性模块、去噪模块和分割引导模块）；3)将高分辨率距离图像输入分割网络进行3D语义分割；4)通过端到端训练和上下文感知损失函数优化整体性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)引导模型基于SR网络，引入分割信息指导的可学习掩模模块；2)端到端架构，首次整合SR和分割任务进行联合优化；3)上下文感知损失函数，增强对较小类别的关注。相比之前工作，不同之处在于：不是独立训练SR和分割网络，而是端到端联合优化；参数数量减少99%，计算效率更高（23 fps vs 6 fps）；专为分割任务设计，更好地保留较小类别的细节结构。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种端到端框架，通过联合优化轻量级超分辨率网络和分割网络，实现了低成本LiDAR传感器达到高性能分割效果，同时显著减少了计算复杂度并保留了较小类别的细节结构。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-resolution LiDAR data plays a critical role in 3D semantic segmentationfor autonomous driving, but the high cost of advanced sensors limitslarge-scale deployment. In contrast, low-cost sensors such as 16-channel LiDARproduce sparse point clouds that degrade segmentation accuracy. To overcomethis, we introduce the first end-to-end framework that jointly addresses LiDARsuper-resolution (SR) and semantic segmentation. The framework employs jointoptimization during training, allowing the SR module to incorporate semanticcues and preserve fine details, particularly for smaller object classes. A newSR loss function further directs the network to focus on regions of interest.The proposed lightweight, model-based SR architecture uses significantly fewerparameters than existing LiDAR SR approaches, while remaining easily compatiblewith segmentation networks. Experiments show that our method achievessegmentation performance comparable to models operating on high-resolution andcostly 64-channel LiDAR data.</description>
      <author>example@mail.com (Alexandros Gkillas, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2509.01317v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards More Diverse and Challenging Pre-training for Point Cloud Learning: Self-Supervised Cross Reconstruction with Decoupled Views</title>
      <link>http://arxiv.org/abs/2509.01250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Point-PQAE，一种用于点云自监督学习的跨视图生成范式。该方法通过生成两个解耦的点云视图并从一个重建另一个，提供了更具挑战性和信息量的预训练方式，在3D自监督学习任务中取得了优于单视图自重建方法的性能。&lt;h4&gt;背景&lt;/h4&gt;点云学习，特别是无需人工标签的自监督学习方式，在视觉和学习领域引起了广泛关注。现有方法主要专注于从单视图中的可见点恢复被遮挡的点，而双视图预训练范式能引入更大的多样性和变异性。&lt;h4&gt;目的&lt;/h4&gt;探索双视图学习在点云自监督学习领域的潜力，开发一种比单视图自重建更具挑战性的预训练方法。&lt;h4&gt;方法&lt;/h4&gt;提出了Point-PQAE，一种跨重建生成范式，首先生成两个解耦的点云/视图，然后从一个重建另一个。为此，论文首次开发了点云视图生成的裁剪机制，并提出了新的位置编码来表示两个解耦视图之间的3D相对位置。&lt;h4&gt;主要发现&lt;/h4&gt;跨重建显著增加了预训练的难度，使该方法在3D自监督学习中超越了之前单模态自重建方法。在ScanObjectNN的三个变体中，使用Mlp-Linear评估协议，该方法比自重建基线（Point-MAE）分别高出6.5%、7.0%和6.7%。&lt;h4&gt;结论&lt;/h4&gt;双视图预训练范式能够提供更具挑战性和信息量的预训练，Point-PQAE通过跨重建方法在3D自监督学习任务中取得了优于单视图自重建方法的性能。&lt;h4&gt;翻译&lt;/h4&gt;点云学习，特别是无需人工标签的自监督方式，由于其潜在的在广泛应用中的实用性，在视觉和学习社区中获得了越来越多的关注。大多数现有的点云自监督学习生成方法专注于从单视图中的可见点恢复被遮挡的点。认识到双视图预训练范式本质上引入了更大的多样性和变异性，因此它可能实现更具挑战性和信息量的预训练。受此启发，我们探索了该领域中双视图学习的潜力。在本文中，我们提出了Point-PQAE，一种跨重建生成范式，首先生成两个解耦的点云/视图，然后从一个重建另一个。为实现这一目标，我们首次开发了点云视图生成的裁剪机制，并进一步提出了新的位置编码来表示两个解耦视图之间的3D相对位置。与自重建相比，跨重建显著增加了预训练的难度，使我们的方法在3D自监督学习中超越了之前的单模态自重建方法。具体而言，在ScanObjectNN的三个变体中，使用Mlp-Linear评估协议，它比自重建基线（Point-MAE）分别高出6.5%、7.0%和6.7%。代码可在https://github.com/aHapBean/Point-PQAE获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云自监督学习中现有方法局限于单一视图内自重建的问题。这个问题很重要，因为点云数据在自动驾驶、机器人等3D视觉应用中至关重要，但点云标注成本高昂，自监督学习可减少对标注数据的依赖；同时，单视图学习已被证明不如两视图学习具有挑战性和信息量，限制了学习到的表示的丰富性和鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到两视图预训练范式能引入更大的多样性和变化，使预训练更具挑战性。他们借鉴了2D视觉中的两视图学习范式（如对比学习方法SimCLR、MoCo和掩码图像建模方法MAE）以及点云自监督学习方法（如Point-MAE）的基本框架，但扩展到点云领域的交叉重建。作者面临两大挑战：如何在点云中构建两个视图，以及如何实现视图间的交叉重建，为此设计了点云裁剪机制和视图相对位置嵌入（VRPE）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出交叉重建范式而非传统的自重建，通过从一个视图重建另一个视图增加预训练难度，使模型学习更丰富、鲁棒的表示。整体流程：1)解耦视图生成：随机裁剪点云创建两个视图，通过归一化和旋转实现解耦；2)VRPE生成：计算两视图间3D相对位置并使用正弦编码生成视图相对位置嵌入；3)位置查询：使用位置查询块通过交叉注意力捕获视图间相对位置信息；4)解码：将查询结果输入解码器预测另一视图；5)损失计算：使用l2 Chamfer距离计算交叉重建损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)新生成框架Point-PQAE，首次将交叉重建引入3D点云自监督学习，包含解耦视图生成、VRPE生成和位置查询块；2)首个点云相对位置感知查询模块，可轻松集成到其他任务；3)强大的性能，在多个基准测试上实现最先进结果。相比之前工作不同：不同于Point-MAE等自重建方法（从单一视图内重建被掩码点），Point-PQAE实现视图间交叉重建，需学习视图间位置关系和视图内空间信息；不同于对比学习方法（如PointContrast），Point-PQAE使用生成方法专注于重建任务；不同于跨模态方法（如ACT），Point-PQAE仅使用单模态点云数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Point-PQAE通过引入交叉重建范式和视图相对位置嵌入，显著提高了点云自监督学习的表示学习能力和下游任务的性能，实现了更丰富、更具挑战性的预训练。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud learning, especially in a self-supervised way without manuallabels, has gained growing attention in both vision and learning communitiesdue to its potential utility in a wide range of applications. Most existinggenerative approaches for point cloud self-supervised learning focus onrecovering masked points from visible ones within a single view. Recognizingthat a two-view pre-training paradigm inherently introduces greater diversityand variance, it may thus enable more challenging and informative pre-training.Inspired by this, we explore the potential of two-view learning in this domain.In this paper, we propose Point-PQAE, a cross-reconstruction generativeparadigm that first generates two decoupled point clouds/views and thenreconstructs one from the other. To achieve this goal, we develop a cropmechanism for point cloud view generation for the first time and furtherpropose a novel positional encoding to represent the 3D relative positionbetween the two decoupled views. The cross-reconstruction significantlyincreases the difficulty of pre-training compared to self-reconstruction, whichenables our method to surpass previous single-modal self-reconstruction methodsin 3D self-supervised learning. Specifically, it outperforms theself-reconstruction baseline (Point-MAE) by 6.5%, 7.0%, and 6.7% in threevariants of ScanObjectNN with the Mlp-Linear evaluation protocol. The code isavailable at https://github.com/aHapBean/Point-PQAE.</description>
      <author>example@mail.com (Xiangdong Zhang, Shaofeng Zhang, Junchi Yan)</author>
      <guid isPermaLink="false">2509.01250v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>PVINet: Point-Voxel Interlaced Network for Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2509.01097v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种点-体素交错网络（PVINet），用于点云压缩，通过并行处理全局和局部特征并进行交互，提高特征感知效率。&lt;h4&gt;背景&lt;/h4&gt;点云压缩中，重建点云的质量依赖于全局结构和局部上下文，而现有方法通常顺序处理这两类信息，缺乏它们之间的交流。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够并行捕获全局结构特征和局部上下文特征，并在每个尺度进行交互的网络，以增强特征感知效率。&lt;h4&gt;方法&lt;/h4&gt;PVINet包含基于体素的编码器（Ev）提取全局结构特征，基于点的编码器（Ep）建模局部上下文，引入条件稀疏卷积应用点嵌入动态定制体素特征提取的核，解码时使用条件稀疏卷积将点嵌入作为指导重建点云。&lt;h4&gt;主要发现&lt;/h4&gt;在基准数据集上的实验表明，PVINet与最先进的方法相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;PVINet通过并行处理全局和局部信息并进行交互，有效提高了点云压缩的质量。&lt;h4&gt;翻译&lt;/h4&gt;在点云压缩中，重建点云的质量依赖于全局结构和局部上下文，而现有方法通常顺序处理全局和局部信息，缺乏这两类信息之间的交流。在本文中，我们提出了一种点-体素交错网络（PVINet），该网络并行捕获全局结构特征和局部上下文特征，并在每个尺度进行交互以增强特征感知效率。具体来说，PVINet包含一个基于体素的编码器（Ev）用于提取全局结构特征，以及一个基于点的编码器（Ep），该编码器建模以每个体素为中心的局部上下文。特别地，引入了一种新颖的条件稀疏卷积，它应用点嵌入来动态定制体素特征提取的核，促进从Ep到Ev的特征交互。在解码过程中，基于体素的解码器使用条件稀疏卷积将点嵌入作为指导来重建点云。在基准数据集上的实验表明，PVINet与最先进的方法相比具有竞争力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云压缩中重建质量的问题，特别是如何在压缩过程中同时保留点云的全局结构和局部细节。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、机器人、人体建模等领域，但庞大的数据量给存储和传输带来了巨大挑战。有效的压缩技术能够在保持关键几何信息的同时，显著降低数据存储和传输成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有的点云压缩方法，发现基于点的方法能很好地捕获局部细节但难以处理大规模点云，而基于体素的方法能高效保留全局结构但可能丢失局部信息。现有的混合方法将这两种技术级联处理，缺乏全局和局部信息之间的有效交互。因此，作者借鉴了这两种方法的优点，设计了点-体素交错网络，并行处理全局和局部特征，并在每个尺度进行交互。同时引入了条件稀疏卷积这一创新组件，促进特征之间的交流。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是并行捕获全局结构特征和局部上下文特征，并在每个尺度进行交互，以增强特征感知效率。整体实现流程包括：1) 点-体素交错编码器，其中基于体素的编码器提取全局结构，基于点的编码器建模局部上下文，两者通过体素到点和点到体的特征交互相互增强；2) 算术编码阶段，使用G-PCC处理体素坐标，熵编码压缩体素特征，并传输路由权重；3) 解码器阶段，利用条件稀疏卷积在路由权重指导下逐步重建点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 点-体素交错网络架构，并行处理全局和局部特征并进行多尺度交互；2) 条件稀疏卷积，利用点嵌入动态定制体素特征提取的核；3) 通过传输路由权重而非完整点特征，有效保留局部细节。相比之前的工作，PVINet突破了现有混合方法中全局和局部信息顺序处理的限制，实现了并行处理和实时交互，显著提升了特征感知效率和重建质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PVINet通过创新的点-体素交错网络结构和条件稀疏卷积，实现了点云压缩中全局结构和局部细节的高效结合，显著提高了重建质量并减少了比特率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In point cloud compression, the quality of a reconstructed point cloud relieson both the global structure and the local context, with existing methodsusually processing global and local information sequentially and lackingcommunication between these two types of information. In this paper, we proposea point-voxel interlaced network (PVINet), which captures global structuralfeatures and local contextual features in parallel and performs interactions ateach scale to enhance feature perception efficiency. Specifically, PVINetcontains a voxel-based encoder (Ev) for extracting global structural featuresand a point-based encoder (Ep) that models local contexts centered at eachvoxel. Particularly, a novel conditional sparse convolution is introduced,which applies point embeddings to dynamically customize kernels for voxelfeature extraction, facilitating feature interactions from Ep to Ev. Duringdecoding, a voxel-based decoder employs conditional sparse convolutions toincorporate point embeddings as guidance to reconstruct the point cloud.Experiments on benchmark datasets show that PVINet delivers competitiveperformance compared to state-of-the-art methods.</description>
      <author>example@mail.com (Xuan Deng, Xingtao Wang, Xiandong Meng, Xiaopeng Fan, Debin Zhao)</author>
      <guid isPermaLink="false">2509.01097v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Formalizing Linear Motion G-code for Invariant Checking and Differential Testing of Fabrication Tools</title>
      <link>http://arxiv.org/abs/2509.00699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的算法，通过将G-code表示为立方体和点云，实现了3D打印制造流程中的错误检测和比较，开发了名为GlitchFinder的原型工具，并在58个真实模型上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;3D打印的计算制造流程类似于编译器，用户在CAD工具中设计模型，转换为多边形网格，最终由3D切片器编译为机器代码。传统编译器有成熟的程序不变量检查技术，但这些方法不直接适用于3D制造领域使用的表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种新算法用于提升G-code（制造流程中使用的通用语言），开发能够应用于3D制造流程的类似技术，以实现错误检测和比较。&lt;h4&gt;方法&lt;/h4&gt;提出一种新算法，通过将G-code程序表示为一组立方体，然后为这些立方体定义近似点云表示以便高效操作。实现了名为GlitchFinder的原型工具，并在58个真实世界CAD模型上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;GlitchFinder在识别由小特征引起的切片问题方面特别有效；可以突出显示不同切片器（如Cura和PrusaSlicer）处理相同模型的差异；能够识别网格修复工具（如MeshLab和Meshmixer）在修复过程中引入的新错误。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法为3D制造流程中的错误检测和比较提供了新机会，GlitchFinder工具能够有效识别和定位3D打印制造流程中的各种问题。&lt;h4&gt;翻译&lt;/h4&gt;3D打印的计算制造流程类似于编译器 - 用户在CAD工具中设计模型，然后降低为多边形网格，最终由3D切片器编译为机器代码。对于传统编译器和编程语言，检查程序不变量的技术已经很成熟。类似地，差异测试等方法常用于发现编译器本身的错误，使它们更加可靠。制造流程将从类似技术中受益，但传统方法不直接适用于该领域使用的表示。与传统程序不同，3D模型既是几何对象，也是最终在硬件上运行的机器代码。机器代码像传统编译一样，受许多因素影响，如模型、使用的切片器以及控制切片过程的许多用户可配置参数。在这项工作中，我们提出了一种新算法，通过将G-code（制造流程中使用的通用语言）提升为表示一组立方体，然后为这些立方体定义近似点云表示以便高效操作。我们的算法开辟了新的机会：我们展示了三个用例，演示了它如何通过不变量检查实现CAD模型中的错误定位，定量比较不同切片器，以及评估网格修复工具的效能。我们在一个名为GlitchFinder的工具中实现了我们算法的原型，并在58个真实世界CAD模型上对其进行了评估。我们的结果表明，GlitchFinder在识别由小特征引起的切片问题方面特别有效，可以突出显示流行切片器（Cura和PrusaSlicer）如何切片相同模型的差异，并可以识别网格修复工具（MeshLab和Meshmixer）在修复过程中引入新错误的情况。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D打印制造流程中缺乏对G-code分析工具的问题，无法在打印前检查模型不变性，也无法对不同切片器和网格修复工具进行差异测试。这个问题很重要，因为3D打印是一个迭代过程，经常需要多次尝试才能成功打印一个零件，而模型设计、网格问题和切片器问题都可能导致打印失败。由于3D打印速度慢（可能需要数小时到数天），这种迭代过程既耗时又消耗资源，而传统编译器技术不适用于3D打印领域。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到3D打印制造流程与传统编译器工具链有相似之处，借鉴了程序语义学和差异测试技术。作者提出了三个关键见解：分析G-code可捕获CAD设计、切片器和设置的综合效果；G-code可通过构造立方体集合来提升；比较两个G-code程序可对工具进行差异测试。作者借鉴了传统编译器的程序分析技术、差异测试方法、表示语义学和操作语义学概念，以及现有的点云比较技术（如Hausdorff距离）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将G-code程序提升为一组立方体表示这些程序执行时将创建的3D对象，然后从立方体生成近似点云以便高效操作，使用增强的Hausdorff距离比较点云以可视化差异。整体流程：输入G-code文件→解析并重构为立方体集合→按比例采样立方体生成点云→将点云分割成单位盒子→计算增强的Hausdorff距离→生成差异热力图和距离分布图→输出结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：提出新的G-code分析方法（重构为立方体和点云）；使用增强Hausdorff距离处理浮点误差；应用空间平均技术减少不想要误差；提出基于旋转不变性的新方法检查模型不变性；实现GlitchFinder原型工具并进行评估。相比之前工作：传统方法不适用于3D打印领域表示；之前工作关注CAD或多边形网格层面，本文关注G-code层面捕获综合效果；本文能静态分析G-code在打印前识别问题；不仅能检查模型不变性，还能对切片器和网格修复工具进行差异测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过将G-code提升为立方体和点云表示并使用增强Hausdorff距离进行比较的新方法，实现了3D打印模型的不变性检查和制造工具的差异测试，从而提高了3D打印流程的可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The computational fabrication pipeline for 3D printing is much like acompiler - users design models in Computer Aided Design (CAD) tools that arelowered to polygon meshes to be ultimately compiled to machine code by 3Dslicers. For traditional compilers and programming languages, techniques forchecking program invariants are well-established. Similarly, methods likedifferential testing are often used to uncover bugs in compilers themselves,which makes them more reliable. The fabrication pipeline would benefit fromsimilar techniques but traditional approaches do not directly apply to therepresentations used in this domain. Unlike traditional programs, 3D modelsexist both as geometric objects as well as machine code that ultimately runs onthe hardware. The machine code, like in traditional compiling, is affected bymany factors like the model, the slicer being used, and numeroususer-configurable parameters that control the slicing process. In this work, wepropose a new algorithm for lifting G-code (a common language used infabrication pipelines) by denoting a G-code program to a set of cuboids, andthen defining an approximate point cloud representation for efficientlyoperating on these cuboids. Our algorithm opens up new opportunities: we showthree use cases that demonstrate how it enables error localization in CADmodels through invariant checking, quantitative comparisons between slicers,and evaluating the efficacy of mesh repair tools. We present a prototypeimplementation of our algorithm in a tool, GlitchFinder, and evaluate it on 58real-world CAD models. Our results show that GlitchFinder is particularlyeffective in identifying slicing issues due to small features, can highlightdifferences in how popular slicers (Cura and PrusaSlicer) slice the same model,and can identify cases where mesh repair tools (MeshLab and Meshmixer)introduce new errors during repair.</description>
      <author>example@mail.com (Yumeng He, Chandrakana Nandi, Sreepathi Pai)</author>
      <guid isPermaLink="false">2509.00699v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Domain Adaptation-Based Crossmodal Knowledge Distillation for 3D Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2509.00379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出两种跨模态知识蒸馏方法，利用丰富的2D图像数据减轻3D LiDAR点云标注负担，在自动驾驶场景的语义分割任务中取得了超越现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR数据语义分割在自动驾驶中起关键作用，但传统方法依赖大量标注数据，成本高且耗时；而真实世界图像数据集丰富且规模大。&lt;h4&gt;目的&lt;/h4&gt;减轻3D LiDAR点云标注的负担，利用丰富的图像数据辅助3D点云语义分割，避免3D标注的必要性。&lt;h4&gt;方法&lt;/h4&gt;提出两种跨模态知识蒸馏方法：无监督域适应知识蒸馏(UDAKD)和基于特征和语义的知识蒸馏(FSKD)；利用自动驾驶场景中现成的时空同步的相机和LiDAR数据；将预训练的2D图像模型应用于未标记的2D数据；通过已知2D-3D对应关系对齐3D和2D网络输出；在3D点云上使用自校准卷积作为域适应模块基础。&lt;h4&gt;主要发现&lt;/h4&gt;严格实验验证了所提出方法的有效性，在性能上持续超越该领域的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过跨模态知识蒸馏，可以避免3D标注的需要，在3D LiDAR数据语义分割任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;3D LiDAR数据的语义分割在自动驾驶中起着关键作用。传统方法依赖大量标注数据进行点云分析，成本高且耗时。相比之下，真实世界图像数据集丰富且规模大。为减轻3D LiDAR点云标注的负担，我们提出两种跨模态知识蒸馏方法：无监督域适应知识蒸馏(UDAKD)和基于特征和语义的知识蒸馏(FSKD)。利用自动驾驶场景中现成的时空同步的相机和LiDAR数据，我们将预训练的2D图像模型直接应用于未标记的2D数据。通过具有已知2D-3D对应关系的跨模态知识蒸馏，我们主动对齐3D网络输出与2D网络对应点，从而避免了3D标注的必要性。我们的重点是保留模态通用信息，同时过滤模态特定细节。为此，我们在3D点云上部署自校准卷积作为域适应模块的基础。严格实验验证了我们所提出方法的有效性，在性能上持续超越该领域的最先进方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D LiDAR点云语义分割中标注数据成本高的问题。传统方法需要大量标注的3D点云数据进行训练，既耗时又昂贵。这个问题在自动驾驶领域尤为重要，因为3D点云语义分割对自动驾驶环境理解至关重要，而手动标注3D点云需要大量专业知识和时间成本，远高于2D图像标注。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析3D点云标注困难和2D图像数据丰富的矛盾，考虑如何利用2D图像数据辅助3D点云学习。他们借鉴了知识蒸馏技术、域适应方法(特别是自校准卷积)和对比学习(如PPKT和SLidR)等现有工作。设计思路是利用自动驾驶场景中已有的同步相机和LiDAR数据，通过已知的2D-3D对应关系进行跨模态知识蒸馏，同时保留模态通用信息，过滤模态特定细节。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D图像和3D点云之间的对应关系，将2D图像模型中的知识迁移到3D点云模型中，减少对3D标注数据的依赖。整体流程包括：1)收集同步的2D图像和3D点云数据；2)构建2D特征提取器、3D特征提取器、域适应模块和分类器；3)根据数据是否有标签选择UDAKD或FSKD方法进行知识蒸馏；4)在少量标注的3D数据上微调并评估性能。UDAKD使用对比学习对齐特征，FSKD结合特征和语义两个层面的知识蒸馏。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于3D自校准卷积的域适应模块，能转移模态通用特征并消除模态特定细节；2)针对无标签和标签图像分别设计跨模态知识蒸馏策略；3)实现零样本域适应能力。相比之前工作，本文更关注优化3D学生模型而非仅改进2D教师模型；首次将自校准卷积应用于3D点云；在域适应中明确分离模态特定和通用特征；同时考虑特征和语义两个层面的知识蒸馏，在多个数据集上超越了最先进方法的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于域适应的跨模态知识蒸馏方法，通过利用丰富的2D图像数据有效减少3D点云语义分割对昂贵标注数据的依赖，显著提升了自动驾驶场景下的3D感知性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic segmentation of 3D LiDAR data plays a pivotal role in autonomousdriving. Traditional approaches rely on extensive annotated data for pointcloud analysis, incurring high costs and time investments. In contrast,realworld image datasets offer abundant availability and substantial scale. Tomitigate the burden of annotating 3D LiDAR point clouds, we propose twocrossmodal knowledge distillation methods: Unsupervised Domain AdaptationKnowledge Distillation (UDAKD) and Feature and Semantic-based KnowledgeDistillation (FSKD). Leveraging readily available spatio-temporallysynchronized data from cameras and LiDARs in autonomous driving scenarios, wedirectly apply a pretrained 2D image model to unlabeled 2D data. Throughcrossmodal knowledge distillation with known 2D-3D correspondence, we activelyalign the output of the 3D network with the corresponding points of the 2Dnetwork, thereby obviating the necessity for 3D annotations. Our focus is onpreserving modality-general information while filtering out modality-specificdetails during crossmodal distillation. To achieve this, we deployself-calibrated convolution on 3D point clouds as the foundation of our domainadaptation module. Rigorous experimentation validates the effectiveness of ourproposed methods, consistently surpassing the performance of state-of-the-artapproaches in the field.</description>
      <author>example@mail.com (Jialiang Kang, Jiawen Wang, Dingsheng Luo)</author>
      <guid isPermaLink="false">2509.00379v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Point-Prompt Tuning: Fine-Tuning Heterogeneous Foundation Models for 3D Point Cloud Analysis</title>
      <link>http://arxiv.org/abs/2509.00374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为自适应点提示调优（APPT）的方法，通过直接利用点特征来校准任何模态的异构基础模型，用于3D点云分析，无需异构映射即可直接处理点云数据。&lt;h4&gt;背景&lt;/h4&gt;在1D文本和2D视觉分析中，基础模型的参数高效微调策略已显示出显著效果，但由于点云数据稀缺，预训练大型3D模型仍具挑战性。现有通过'高到低'映射将预训练视觉模型应用于3D领域的方法常导致空间几何信息丢失，且缺乏通用框架。&lt;h4&gt;目的&lt;/h4&gt;直接利用点特征来校准任何模态的异构基础模型，使其能够有效进行3D点云分析，同时保持预训练模型的通用性并减少计算开销。&lt;h4&gt;方法&lt;/h4&gt;1) 将原始点云转换为点嵌入，通过聚合局部几何特征捕捉空间特征；2) 针对点云的无序特性，采用排列不变特征捕捉点嵌入相对位置；3) 引入与点嵌入模块共享权重的提示生成器，动态生成点提示；4) 将生成的点提示连接到冻结的基础模型中，提供丰富的全局结构信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过APPT方法，预训练模型能够直接处理点云数据而无需异构映射；共享权重的提示生成器可在不增加额外参数的情况下动态生成点提示；连接提示到冻结模型可弥补异构数据中结构上下文的不足。&lt;h4&gt;结论&lt;/h4&gt;APPT方法通过点提示调优有效解决了将任何模态适应到3D点云分析的问题，既保持了预训练模型的知识，又提供了处理点云所需的结构信息，同时计算效率高。&lt;h4&gt;翻译&lt;/h4&gt;针对1D文本和2D视觉分析的基础模型参数高效微调策略已显示出显著效果。然而，由于点云数据的稀缺，预训练大型3D模型仍然是一项具有挑战性的任务。虽然许多努力已经通过'高到低'映射将预训练的视觉模型应用于3D领域，但这些方法往往导致空间几何信息的丢失，并且缺乏将任何模态适应到3D的通用框架。因此，本文尝试直接利用点特征来校准任何模态的异构基础模型，用于3D点云分析。具体来说，我们提出了自适应点提示调优（APPT）方法，该方法使用少量参数微调预训练模型，使模型能够直接处理点云，无需异构映射。我们将原始点云通过聚合局部几何特征转换为点嵌入以捕捉空间特征，然后通过线性层确保能够无缝利用冻结的预训练模型。鉴于点云的固有无序性与图像和语言的有序性质不同，我们采用排列不变的特征来捕捉点嵌入的相对位置，从而获得富含位置信息的点标记，以优化自注意力机制。为了校准任何模态的源域到3D的自注意力并减少计算开销，我们引入了一个与点嵌入模块共享权重的提示生成器，动态生成点提示而不添加额外参数。然后将这些提示连接到冻结的基础模型中，提供丰富的全局结构信息，弥补异构数据中结构上下文的不足。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何有效利用预训练在其他模态（如图像、文本）上的基础模型来增强3D点云分析任务的问题。这个问题很重要，因为3D点云数据获取和标注成本高、数量有限，导致专门为3D任务训练的大规模基础模型较少。现有方法通常需要将3D点云投影到低维表示（如2D图像），这会导致高维空间信息的丢失，或者需要大量成对的2D/1D-3D数据进行知识蒸馏，限制了效率和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：模态投影会丢失信息，知识蒸馏需要大量成对数据且效率低。基于这些分析，作者设计了APPT方法，直接利用点特征校准基础模型。该方法借鉴了参数高效微调（PEFT）和提示调优（prompt tuning）技术，但针对3D点云特性进行了创新：设计点嵌入模块处理无序数据，引入位置注入器捕获相对位置信息，提出点提示生成器与点嵌入模块共享权重，以及设计有效的Transformer块微调策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过轻量级点嵌入和点提示生成模块，直接将3D点云信息注入到预训练在其他模态的基础模型中，使其能处理3D点云而不需修改基础模型。整体流程：1)点云分组（使用FPS和k-NN）；2)点嵌入（通过轻量网络如Point-PN转换点组）；3)位置注入（使用排列不变性特征编码位置）；4)点提示生成（通过池化操作生成全局提示）；5)模型微调（将点提示连接到冻结模型的每个Transformer块）；6)下游任务（分类或分割）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)直接利用点特征校准异构基础模型，避免信息损失；2)提出位置注入器，用排列不变性特征编码位置；3)设计点提示生成器，与点嵌入模块共享权重；4)设计有效的Transformer块微调策略；5)方法通用性强，可适应各种预训练模型。相比之前工作：避免了低维投影步骤，不需要大量成对数据，仅微调3.8%参数，计算效率高，保留了点云的高维结构信息。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应点提示调优方法，通过轻量级参数高效地利用预训练在其他模态的基础模型来增强3D点云分析任务，避免了信息损失并显著提升了性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning strategies for foundation models in 1Dtextual and 2D visual analysis have demonstrated remarkable efficacy. However,due to the scarcity of point cloud data, pre-training large 3D models remains achallenging task. While many efforts have been made to apply pre-trained visualmodels to 3D domains through "high-to-low" mapping, these approaches often leadto the loss of spatial geometries and lack a generalizable framework foradapting any modality to 3D. This paper, therefore, attempts to directlyleverage point features to calibrate the heterogeneous foundation model of anymodality for 3D point cloud analysis. Specifically, we propose the AdaptivePoint-Prompt Tuning (APPT) method, which fine-tunes pre-trained models with amodest number of parameters, enabling direct point cloud processing withoutheterogeneous mappings. We convert raw point clouds into point embeddings byaggregating local geometry to capture spatial features followed by linearlayers to ensure seamless utilization of frozen pre-trained models. Given theinherent disorder of point clouds, in contrast to the structured nature ofimages and language, we employ a permutation-invariant feature to capture therelative positions of point embeddings, thereby obtaining point tokens enrichedwith location information to optimize self-attention mechanisms. To calibrateself-attention across source domains of any modality to 3D and reducecomputational overhead, we introduce a prompt generator that shares weightswith the point embedding module, dynamically producing point-prompts withoutadding additional parameters. These prompts are then concatenated into a frozenfoundation model, providing rich global structural information and compensatingfor the lack of structural context in the heterogeneous data.</description>
      <author>example@mail.com (Mengke Li, Lihao Chen, Peng Zhang, Yiu-ming Cheung, Hui Huang)</author>
      <guid isPermaLink="false">2509.00374v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>High-Fidelity Digital Twins for Bridging the Sim2Real Gap in LiDAR-Based ITS Perception</title>
      <link>http://arxiv.org/abs/2509.02904v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高保真数字孪生框架，用于解决激光雷达感知在仿真到现实迁移中的性能下降问题，实验证明该方法比真实数据训练的模型性能提高4.8%。&lt;h4&gt;背景&lt;/h4&gt;Sim2Real域迁移是智能交通系统中基于激光雷达感知的经济有效方法，但仿真训练的模型在真实数据上表现不佳，存在分布偏移问题。&lt;h4&gt;目的&lt;/h4&gt;解决Sim2Real差距问题，提高仿真训练的激光雷达感知模型在真实世界数据上的性能。&lt;h4&gt;方法&lt;/h4&gt;提出高保真数字孪生框架，整合真实世界背景几何、车道级道路拓扑和传感器规范；构建仿真环境生成域内合成数据；在合成数据上训练3D目标检测器并在真实数据上评估。&lt;h4&gt;主要发现&lt;/h4&gt;DT训练的模型比真实数据训练的模型性能高4.8%；多种距离指标显示HiFi DT显著减少了域偏移；提高了模型在不同评估场景中的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;数字孪生在为智能交通应用提供可靠的、基于仿真的激光雷达感知方面具有重要作用。&lt;h4&gt;翻译&lt;/h4&gt;Sim2Real域迁移为智能交通系统中的激光雷达感知（如目标检测、跟踪、分割）提供了一种经济有效且可扩展的方法。然而，由于分布偏移，在仿真中训练的感知模型在真实世界数据上表现往往不佳。为解决这一Sim2Real差距，本文提出了一种高保真数字孪生框架，该框架整合了真实世界的背景几何、车道级道路拓扑以及传感器特定规范和放置。我们将Sim2Real学习中的域适应问题形式化，并提出了一种构建仿真环境的系统方法，以生成域内合成数据。在HiFi DT生成的合成数据上训练了一个现成的3D目标检测器，并在真实数据上进行了评估。实验表明，DT训练的模型比等效的真实数据训练模型性能高出4.8%。为理解这一提升，我们使用多种指标（包括Chamfer距离、最大均值差异、地球移动距离和Fréchet距离）在原始输入和潜在特征水平上量化了合成数据与真实数据之间的分布对齐。结果表明，HiFi DT显著减少了域偏移，并提高了在不同评估场景中的泛化能力。这些发现强调了数字孪生在为真实世界智能交通应用提供可靠的、基于仿真的激光雷达感知方面的重要作用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是Sim2Real（仿真到现实）差距问题，即在仿真环境中训练的激光雷达感知模型在真实世界数据上表现不佳的问题。这个问题在现实中非常重要，因为创建和标注大规模激光雷达数据集成本高昂、耗时费力，人工标注可能存在偏差，不同地区的交通和环境条件差异大，极端场景难以收集，且传感器特性变化会导致数据分布变化，这些都限制了智能交通系统感知算法的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有模拟器的局限性，指出它们虽然能模拟传感器物理特性，但环境是手工制作的，不够真实。然后回顾了相关工作，包括传感器模拟研究、领域适应研究和数字孪生研究，发现它们各有不足：传感器模拟研究忽略环境真实性，领域适应研究是事后纠正方法，数字孪生研究创建过程不够系统化。基于这些分析，作者设计了一种系统化的高保真数字孪生(HiFi DT)方法，借鉴了CARLA等模拟器的基础架构，利用了OpenStreetMap等公开地图数据，并参考了现有的数字孪生建模思想，但使其更加系统化和可扩展。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过创建高保真数字孪生(HiFi DT)来生成与真实世界分布高度一致的合成激光雷达数据，从而最小化Sim2Real差距，使在仿真环境中训练的模型能够很好地泛化到真实世界。整体实现流程包括：1)识别目标位置；2)获取3D卫星图像；3)处理3D模型(裁剪、清理、缩放)；4)创建精确道路拓扑；5)生成匹配真实统计分布的交通；6)执行传感器模拟和数据采集，存储为OpenPCDet格式。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个结合静态和动态场景的高保真数字孪生应用；2)系统化的数字孪生创建方法；3)精确的传感器模拟技术；4)首次使用合成数据训练的模型性能超越真实数据训练的模型；5)深入的数据分布分析。相比之前工作的不同：与传统模拟器相比，HiFi DT使用真实世界的背景几何和道路拓扑；与领域适应方法相比，HiFi DT从源头上生成与目标领域对齐的数据，减少事后处理需求；与现有数字孪生研究相比，HiFi DT提供了一种更简单、系统化的方法，可使用公开信息为任意位置创建数字孪生。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种系统化的高保真数字孪生方法，通过整合真实世界的几何结构、道路拓扑和传感器规格，生成与真实世界分布高度一致的激光雷达数据，使训练的3D目标检测模型在真实世界上的性能超越了在真实数据上训练的模型，有效解决了智能交通系统中的Sim2Real差距问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sim2Real domain transfer offers a cost-effective and scalable approach fordeveloping LiDAR-based perception (e.g., object detection, tracking,segmentation) in Intelligent Transportation Systems (ITS). However, perceptionmodels trained in simulation often under perform on real-world data due todistributional shifts. To address this Sim2Real gap, this paper proposes ahigh-fidelity digital twin (HiFi DT) framework that incorporates real-worldbackground geometry, lane-level road topology, and sensor-specificspecifications and placement. We formalize the domain adaptation challengeunderlying Sim2Real learning and present a systematic method for constructingsimulation environments that yield in-domain synthetic data. An off-the-shelf3D object detector is trained on HiFi DT-generated synthetic data and evaluatedon real data. Our experiments show that the DT-trained model outperforms theequivalent model trained on real data by 4.8%. To understand this gain, wequantify distributional alignment between synthetic and real data usingmultiple metrics, including Chamfer Distance (CD), Maximum Mean Discrepancy(MMD), Earth Mover's Distance (EMD), and Fr'echet Distance (FD), at bothraw-input and latent-feature levels. Results demonstrate that HiFi DTssubstantially reduce domain shift and improve generalization across diverseevaluation scenarios. These findings underscore the significant role of digitaltwins in enabling reliable, simulation-based LiDAR perception for real-worldITS applications.</description>
      <author>example@mail.com (Muhammad Shahbaz, Shaurya Agarwal)</author>
      <guid isPermaLink="false">2509.02904v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>LimiX: Unleashing Structured-Data Modeling Capability for Generalist Intelligence</title>
      <link>http://arxiv.org/abs/2509.03505v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  56 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了LimiX，这是大型结构化数据模型(LDMs)系列中的第一个模型，能够通过单一模型处理多种表格数据任务，并在多个基准测试中表现优异。&lt;h4&gt;背景&lt;/h4&gt;实现通用人工智能需要建立在语言、物理世界和结构化数据上的互补基础模型，而当前结构化数据处理领域缺乏能够统一处理多种任务的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理广泛表格任务的结构化数据模型，通过单一模型和统一接口解决多种任务，避免任务特定的架构或为每个任务进行专门训练。&lt;h4&gt;方法&lt;/h4&gt;LimiX将结构化数据视为变量和缺失性的联合分布，使用基于查询的条件预测方法。采用掩码联合分布建模进行预训练，使用情境条件目标，在推理时支持快速、无需训练的适应。&lt;h4&gt;主要发现&lt;/h4&gt;LimiX在10个大型结构化数据基准测试中表现优异，涵盖不同样本量、特征维度、类别数量等条件下，超越了梯度提升树、深度表格网络等强基线方法，在分类、回归、缺失值填补和数据生成等多种任务中表现优越。&lt;h4&gt;结论&lt;/h4&gt;LimiX证明了单一模型可以有效地处理多种表格任务，无需任务特定的架构或为每个任务进行专门训练，为通用人工智能的发展提供了重要进展。&lt;h4&gt;翻译&lt;/h4&gt;我们认为，向通用人工智能的进步需要建立在语言、物理世界和结构化数据上的互补基础模型。本报告介绍了LimiX，这是我们大型结构化数据模型(LDMs)系列中的第一个模型。LimiX将结构化数据视为变量和缺失性的联合分布，因此能够通过基于查询的条件预测，通过单一模型处理广泛的表格任务。LimiX使用掩码联合分布建模进行预训练，采用情境条件目标，模型根据特定数据集的条件预测查询子集，支持推理时快速、无需训练的适应。我们在10个大型结构化数据基准测试中评估了LimiX，这些测试涵盖了样本量、特征维度、类别数量、分类-数值特征比率、缺失性和样本-特征比率的广泛范围。使用单一模型和统一接口，LimiX始终优于强基线方法，包括梯度提升树、深度表格网络、最近的表格基础模型和自动化集成，如图1和图2所示。这种优越性在广泛的任务中保持不变，如分类、回归、缺失值填补和数据生成，且通常有显著优势，同时避免了任务特定的架构或为每个任务进行专门训练。所有LimiX模型均可在Apache 2.0许可下公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We argue that progress toward general intelligence requires complementaryfoundation models grounded in language, the physical world, and structureddata. This report presents LimiX, the first installment of our largestructured-data models (LDMs). LimiX treats structured data as a jointdistribution over variables and missingness, thus capable of addressing a widerange of tabular tasks through query-based conditional prediction via a singlemodel. LimiX is pretrained using masked joint-distribution modeling with anepisodic, context-conditional objective, where the model predicts for querysubsets conditioned on dataset-specific contexts, supporting rapid,training-free adaptation at inference. We evaluate LimiX across 10 largestructured-data benchmarks with broad regimes of sample size, featuredimensionality, class number, categorical-to-numerical feature ratio,missingness, and sample-to-feature ratios. With a single model and a unifiedinterface, LimiX consistently surpasses strong baselines includinggradient-boosting trees, deep tabular networks, recent tabular foundationmodels, and automated ensembles, as shown in Figure 1 and Figure 2. Thesuperiority holds across a wide range of tasks, such as classification,regression, missing value imputation, and data generation, often by substantialmargins, while avoiding task-specific architectures or bespoke training pertask. All LimiX models are publicly accessible under Apache 2.0.</description>
      <author>example@mail.com (Xingxuan Zhang, Gang Ren, Han Yu, Hao Yuan, Hui Wang, Jiansheng Li, Jiayun Wu, Lang Mo, Li Mao, Mingchao Hao, Ningbo Dai, Renzhe Xu, Shuyang Li, Tianyang Zhang, Yue He, Yuanrui Wang, Yunjia Zhang, Zijing Xu, Dongzhe Li, Fang Gao, Hao Zou, Jiandong Liu, Jiashuo Liu, Jiawei Xu, Kaijie Cheng, Kehan Li, Linjun Zhou, Qing Li, Shaohua Fan, Xiaoyu Lin, Xinyan Han, Xuanyue Li, Yan Lu, Yuan Xue, Yuanyuan Jiang, Zimu Wang, Zhenlei Wang, Peng Cui)</author>
      <guid isPermaLink="false">2509.03505v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SafeProtein: Red-Teaming Framework and Benchmark for Protein Foundation Models</title>
      <link>http://arxiv.org/abs/2509.03487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SafeProtein，这是首个针对蛋白质基础模型的red-teaming框架，通过系统化方法测试蛋白质模型的安全性，揭示其潜在生物安全风险。&lt;h4&gt;背景&lt;/h4&gt;蛋白质在几乎所有生物过程中都扮演关键角色，深度学习的发展加速了蛋白质基础模型的进步，但这些模型缺乏系统性的red-teaming，存在被滥用于生成生物安全风险蛋白质的担忧。&lt;h4&gt;目的&lt;/h4&gt;开发首个专门针对蛋白质基础模型的red-teaming框架，系统测试模型安全性，揭示潜在风险，并为前沿模型的安全保护技术发展提供见解。&lt;h4&gt;方法&lt;/h4&gt;SafeProtein结合多模态提示工程和启发式束搜索方法设计red-teaming策略，并创建了SafeProtein-Bench基准，包含手动构建的测试数据集和全面评估协议。&lt;h4&gt;主要发现&lt;/h4&gt;SafeProtein成功实现了对最先进蛋白质基础模型的持续越狱，对ESM3模型的攻击成功率高达70%，揭示了当前蛋白质基础模型中存在的潜在生物安全风险。&lt;h4&gt;结论&lt;/h4&gt;SafeProtein框架有效暴露了当前蛋白质基础模型的安全隐患，为开发前沿模型的安全保护技术提供了有价值的见解，相关代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;蛋白质在几乎所有生物过程中都扮演着关键角色。深度学习的进步极大地加速了蛋白质基础模型的发展，在蛋白质理解和设计方面取得了显著成功。然而，这些模型缺乏系统性的red-teaming，引发了人们对它们可能被滥用的严重担忧，例如生成具有生物安全风险的蛋白质。本文介绍了SafeProtein，据我们所知，这是第一个专为蛋白质基础模型设计的red-teaming框架。SafeProtein结合多模态提示工程和启发式束搜索，系统性地设计red-teaming方法，并对蛋白质基础模型进行测试。我们还整理了SafeProtein-Bench，其中包括一个手动构建的red-teaming基准数据集和全面的评估协议。SafeProtein成功地实现了对最先进的蛋白质基础模型的持续越狱（ESM3的攻击成功率高达70%），揭示了当前蛋白质基础模型中潜在的生物安全风险，并为前沿模型开发强大的安全保护技术提供了见解。相关代码将在https://github.com/jigang-fan/SafeProtein公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Proteins play crucial roles in almost all biological processes. Theadvancement of deep learning has greatly accelerated the development of proteinfoundation models, leading to significant successes in protein understandingand design. However, the lack of systematic red-teaming for these models hasraised serious concerns about their potential misuse, such as generatingproteins with biological safety risks. This paper introduces SafeProtein, thefirst red-teaming framework designed for protein foundation models to the bestof our knowledge. SafeProtein combines multimodal prompt engineering andheuristic beam search to systematically design red-teaming methods and conducttests on protein foundation models. We also curated SafeProtein-Bench, whichincludes a manually constructed red-teaming benchmark dataset and acomprehensive evaluation protocol. SafeProtein achieved continuous jailbreakson state-of-the-art protein foundation models (up to 70% attack success ratefor ESM3), revealing potential biological safety risks in current proteinfoundation models and providing insights for the development of robust securityprotection technologies for frontier models. The codes will be made publiclyavailable at https://github.com/jigang-fan/SafeProtein.</description>
      <author>example@mail.com (Jigang Fan, Zhenghong Zhou, Ruofan Jin, Le Cong, Mengdi Wang, Zaixi Zhang)</author>
      <guid isPermaLink="false">2509.03487v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Generalist versus Specialist Vision Foundation Models for Ocular Disease and Oculomics</title>
      <link>http://arxiv.org/abs/2509.03421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 8 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了专业视网膜基础模型与通用基础模型在视网膜图像应用中的性能，发现专业模型在眼部疾病检测和眼科医学任务中表现更优，但通用模型差距正在缩小。&lt;h4&gt;背景&lt;/h4&gt;医学基础模型通过大规模临床数据预训练在临床应用中表现出色，如RETFound模型在视网膜图像应用中。然而，更强大的通用基础模型（如DINOv2和DINOv3）的出现引发了领域特定预训练必要性的问题。&lt;h4&gt;目的&lt;/h4&gt;研究通用基础模型在视网膜图像应用中的适应性，并与专业RETFound模型进行比较，评估领域特定预训练是否仍然必要以及存在的差距。&lt;h4&gt;方法&lt;/h4&gt;系统评估了DINOv2和DINOv3在眼部疾病检测和全身疾病预测任务中的表现，并与RETFound-MAE和RETFound-DINOv2模型比较。采用微调和线性探测两种适应策略，并分析数据效率和适应效率以评估预测性能与计算成本的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;尽管扩展通用模型在多样化任务中表现出强大的适应性，但RETFound-DINOv2在眼部疾病检测和眼科医学任务中始终优于通用基础模型，具有更强的泛化能力和数据效率。&lt;h4&gt;结论&lt;/h4&gt;专业视网膜基础模型仍是临床应用的最佳选择，但与通用基础模型差距的缩小表明，持续的数据和模型扩展可带来领域相关收益，并使通用模型成为未来医学基础模型的强大基础。&lt;h4&gt;翻译&lt;/h4&gt;医学基础模型通过大规模临床数据预训练，在多样化的临床相关应用中表现出强大性能。在近一百万张视网膜图像上训练的RETFound代表了视网膜图像应用的方法。然而，越来越强大和规模更大的通用基础模型（如DINOv2和DINOv3）的出现，引发了领域特定预训练是否仍然必要的问题。为研究此问题，我们系统评估了DINOv2和DINOv3在视网膜图像应用中的适应性，并与两个专业RETFound模型（RETFound-MAE和RETFound-DINOv2）进行了比较。我们使用微调和线性探测两种适应策略评估了眼部疾病检测和全身疾病预测的性能。进一步分析了数据效率和适应效率以描述预测性能与计算成本之间的权衡。我们的结果表明，尽管扩展通用模型可在多样化任务中产生强大的适应性，但RETFound-DINOv2在眼部疾病检测和眼科医学任务中始终优于这些通用基础模型，表现出更强的泛化能力和数据效率。这些发现表明，专业视网膜基础模型仍然是临床应用的最有效选择，但与通用基础模型之间差距的缩小表明，持续的数据和模型扩展可以带来领域相关的收益，并使它们成为未来医学基础模型的强大基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical foundation models, pre-trained with large-scale clinical data,demonstrate strong performance in diverse clinically relevant applications.RETFound, trained on nearly one million retinal images, exemplifies thisapproach in applications with retinal images. However, the emergence ofincreasingly powerful and multifold larger generalist foundation models such asDINOv2 and DINOv3 raises the question of whether domain-specific pre-trainingremains essential, and if so, what gap persists. To investigate this, wesystematically evaluated the adaptability of DINOv2 and DINOv3 in retinal imageapplications, compared to two specialist RETFound models, RETFound-MAE andRETFound-DINOv2. We assessed performance on ocular disease detection andsystemic disease prediction using two adaptation strategies: fine-tuning andlinear probing. Data efficiency and adaptation efficiency were further analysedto characterise trade-offs between predictive performance and computationalcost. Our results show that although scaling generalist models yields strongadaptability across diverse tasks, RETFound-DINOv2 consistently outperformsthese generalist foundation models in ocular-disease detection and oculomicstasks, demonstrating stronger generalisability and data efficiency. Thesefindings suggest that specialist retinal foundation models remain the mosteffective choice for clinical applications, while the narrowing gap withgeneralist foundation models suggests that continued data and model scaling candeliver domain-relevant gains and position them as strong foundations forfuture medical foundation models.</description>
      <author>example@mail.com (Yukun Zhou, Paul Nderitu, Jocelyn Hui Lin Goh, Justin Engelmann, Siegfried K. Wagner, Anran Ran, Hongyang Jiang, Lie Ju, Ke Zou, Sahana Srinivasan, Hyunmin Kim, Takahiro Ninomiya, Zheyuan Wang, Gabriel Dawei Yang, Eden Ruffell, Dominic Williamson, Rui Santos, Gabor Mark Somfai, Carol Y. Cheung, Tien Yin Wong, Daniel C. Alexander, Yih Chung Tham, Pearse A. Keane)</author>
      <guid isPermaLink="false">2509.03421v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive Assessment and Benchmark Study of Large Atomistic Foundation Models for Phonons</title>
      <link>http://arxiv.org/abs/2509.03401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了六种最新的通用机器学习势(uMLPs)在预测晶体材料声子性质方面的性能，发现EquiformerV2模型在大多数指标上表现最佳，并揭示了力预测精度与声子性质预测之间的复杂关系，为材料高通量筛选提供了重要指导。&lt;h4&gt;背景&lt;/h4&gt;通用机器学习势(uMLPs)技术发展迅速，能够高效准确地预测各种材料性质，但在化学多样化系统中的声子性质预测能力尚未得到充分评估，缺乏系统性的基准测试。&lt;h4&gt;目的&lt;/h4&gt;系统评估六种最新uMLPs模型在预测晶体材料声子性质方面的性能，包括原子力、原子间力常数和晶格热导率等，为材料高通量筛选提供模型选择指导。&lt;h4&gt;方法&lt;/h4&gt;从开放量子材料数据库选取2,429种晶体材料，使用六种uMLPs模型计算位移超晶胞中的原子力，推导原子间力常数，预测晶格热导率等声子性质，并与密度泛函理论和实验数据进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;EquiformerV2预训练模型在原子力和三阶原子间力常数预测方面表现出色；其微调版本在二阶原子间力常数、晶格热导率等声子性质预测方面持续优于其他模型；MACE和CHGNet力预测精度与EquiformerV2相当，但在原子间力常数拟合方面存在差异，导致晶格热导率预测不佳；MatterSim尽管力精度较低，但获得了中等水平的原子间力常数预测，表明力精度与声子预测之间存在误差抵消和复杂关系。&lt;h4&gt;结论&lt;/h4&gt;不同uMLPs模型在预测声子性质方面表现各异，EquiformerV2整体性能最优，研究结果为具有目标热传输性质的材料高通量筛选提供了重要的模型评估和选择依据。&lt;h4&gt;翻译&lt;/h4&gt;通用机器学习势(uMLPs)的快速发展使得在广阔的化学空间中能够高效、准确地预测各种材料性质。虽然它们在建模声子性质方面的能力正在显现，但在化学多样化系统中的系统基准测试仍然有限。我们从开放量子材料数据库中评估了六种最新的uMLPs（EquiformerV2、MatterSim、MACE和CHGNet）在2,429种晶体材料上的表现。这些模型用于计算位移超晶胞中的原子力，推导原子间力常数(IFCs)，并预测晶格热导率(LTC)等声子性质，并与密度泛函理论(DFT)和实验数据进行比较。在OMat24数据集上预训练的EquiformerV2模型在预测原子力和三阶IFC方面表现出色，而其微调版本在预测二阶IFC、LTC和其他声子性质方面持续优于其他模型。尽管MACE和CHGNet的力预测精度与EquiformerV2相当，但在IFC拟合方面存在显著差异，导致LTC预测不佳。相反，MatterSim尽管力精度较低，但获得了中等水平的IFC预测，这表明力精度与声子预测之间存在误差抵消和复杂关系。这一基准测试为指导具有目标热传输性质的材料高通量筛选评估和选择uMLPs提供了指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of universal machine learning potentials (uMLPs) hasenabled efficient, accurate predictions of diverse material properties acrossbroad chemical spaces. While their capability for modeling phonon properties isemerging, systematic benchmarking across chemically diverse systems remainslimited. We evaluate six recent uMLPs (EquiformerV2, MatterSim, MACE, andCHGNet) on 2,429 crystalline materials from the Open Quantum MaterialsDatabase. Models were used to compute atomic forces in displaced supercells,derive interatomic force constants (IFCs), and predict phonon propertiesincluding lattice thermal conductivity (LTC), compared with density functionaltheory (DFT) and experimental data. The EquiformerV2 pretrained model trainedon the OMat24 dataset exhibits strong performance in predicting atomic forcesand third-order IFC, while its fine-tuned counterpart consistently outperformsother models in predicting second-order IFC, LTC, and other phonon properties.Although MACE and CHGNet demonstrated comparable force prediction accuracy toEquiformerV2, notable discrepancies in IFC fitting led to poor LTC predictions.Conversely, MatterSim, despite lower force accuracy, achieved intermediate IFCpredictions, suggesting error cancellation and complex relationships betweenforce accuracy and phonon predictions. This benchmark guides the evaluation andselection of uMLPs for high-throughput screening of materials with targetedthermal transport properties.</description>
      <author>example@mail.com (Md Zaibul Anam, Ogheneyoma Aghoghovbia, Mohammed Al-Fahdi, Lingyu Kong, Victor Fung, Ming Hu)</author>
      <guid isPermaLink="false">2509.03401v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Improving Perceptual Audio Aesthetic Assessment via Triplet Loss and Self-Supervised Embeddings</title>
      <link>http://arxiv.org/abs/2509.03292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Automatic Speech Recognition and Understanding  Workshop(ASRU), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个用于自动预测生成式音频多轴感知质量的系统，该系统为AudioMOS挑战赛2025的Track 2开发，能够预测文本转语音、文本转音频和文本转音乐系统生成音频的四个美学评分。&lt;h4&gt;背景&lt;/h4&gt;研究背景是AudioMOS挑战赛2025的Track 2，主要关注生成式音频的质量评估，面临的主要挑战是自然训练数据和合成评估数据之间的领域差异。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动预测生成式音频四个美学评分的系统，并解决自然训练数据和合成评估数据之间的领域差异问题。&lt;h4&gt;方法&lt;/h4&gt;结合BEATs预训练音频表示模型与多分支长短期记忆预测器，使用基于缓冲区采样的三元组损失根据感知相似性构建嵌入空间。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法提高了嵌入的判别性和泛化能力，使得无需合成训练数据就能实现领域鲁棒的音频质量评估。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效处理自然训练数据和合成评估数据之间的领域差异，实现对生成式音频质量的准确评估。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于自动预测生成式音频多轴感知质量的系统，该系统是为AudioMOS挑战赛2025的Track 2开发的。任务是预测文本转语音、文本转音频和文本转音乐系统生成的音频的四个音频美学评分--制作质量、制作复杂度、内容享受度和内容有用度。一个主要挑战是自然训练数据和合成评估数据之间的领域差异。为了解决这个问题，我们将BEATs（一种预训练的基于transformer的音频表示模型）与多分支长短期记忆预测器相结合，并使用基于缓冲区采样的三元组损失根据感知相似性构建嵌入空间。我们的结果表明，这提高了嵌入的判别性和泛化能力，使得无需合成训练数据就能实现领域鲁棒的音频质量评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a system for automatic multi-axis perceptual quality prediction ofgenerative audio, developed for Track 2 of the AudioMOS Challenge 2025. Thetask is to predict four Audio Aesthetic Scores--Production Quality, ProductionComplexity, Content Enjoyment, and Content Usefulness--for audio generated bytext-to-speech (TTS), text-to-audio (TTA), and text-to-music (TTM) systems. Amain challenge is the domain shift between natural training data and syntheticevaluation data. To address this, we combine BEATs, a pretrainedtransformer-based audio representation model, with a multi-branch longshort-term memory (LSTM) predictor and use a triplet loss with buffer-basedsampling to structure the embedding space by perceptual similarity. Our resultsshow that this improves embedding discriminability and generalization, enablingdomain-robust audio quality assessment without synthetic training data.</description>
      <author>example@mail.com (Dyah A. M. G. Wisnu, Ryandhimas E. Zezario, Stefano Rini, Hsin-Min Wang, Yu Tsao)</author>
      <guid isPermaLink="false">2509.03292v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization</title>
      <link>http://arxiv.org/abs/2509.03244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了FoMEMO新范式，用于解决昂贵多目标优化问题，通过使用合成数据预训练的基础模型实现高样本效率和强泛化能力。&lt;h4&gt;背景&lt;/h4&gt;昂贵的多目标优化在许多现实场景中普遍存在且至关重要，但由于评估次数有限，样本效率对于恢复真实帕累托前沿以支持决策至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法难以泛化和应对现实世界中各种新兴应用的问题，提高多目标优化的效率和实用性。&lt;h4&gt;方法&lt;/h4&gt;提出FoMEMO（Foundation Models for Expensive Multi-objective Optimization）新范式，建立基于领域轨迹和用户偏好的基础模型，实现基于预测的偏好级联聚合后验的快速上下文优化，使用数亿个合成数据进行预训练而非现实世界实验。&lt;h4&gt;主要发现&lt;/h4&gt;使用多样化的数亿个合成数据预训练基础模型可带来对未知问题的优越适应性，无需在优化过程中进行任何后续模型训练或更新。&lt;h4&gt;结论&lt;/h4&gt;在多种合成基准和现实世界应用中评估显示，FoMEMO与现有方法相比具有优越的通用性和竞争性能。&lt;h4&gt;翻译&lt;/h4&gt;昂贵的多目标优化是许多现实场景中普遍存在且至关重要的问题，由于评估次数有限，样本效率对于恢复真实帕累托前沿以支持决策至关重要。现有方法要么需要为每个新问题中的每个目标从头重建高斯过程代理模型，要么依赖大量过去的领域实验来预训练深度学习模型，这使它们难以泛化且难以应对现实世界中各种新兴应用。为解决这一问题，我们提出了一种名为FoMEMO（Foundation Models for Expensive Multi-objective Optimization）的新范式，该范式能够建立基于任何领域轨迹和用户偏好的基础模型，促进基于预测的偏好级联聚合后验的快速上下文优化。我们证明，使用多样化的数亿个合成数据进行预训练可以使基础模型对未知问题具有更好的适应性，而无需在优化过程中进行任何后续模型训练或更新。我们在各种合成基准和现实世界应用中评估了我们的方法，并展示了其相比现有方法的优越通用性和竞争性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Expensive multi-objective optimization is a prevalent and crucial concern inmany real-world scenarios, where sample-efficiency is vital due to the limitedevaluations to recover the true Pareto front for decision making. Existingworks either involve rebuilding Gaussian process surrogates from scratch foreach objective in each new problem encountered, or rely on extensive pastdomain experiments for pre-training deep learning models, making them hard togeneralize and impractical to cope with various emerging applications in thereal world. To address this issue, we propose a new paradigm named FoMEMO(Foundation Models for Expensive Multi-objective Optimization), which enablesthe establishment of a foundation model conditioned on any domain trajectoryand user preference, and facilitates fast in-context optimization based on thepredicted preference-wise aggregation posteriors. Rather than accessingextensive domain experiments in the real world, we demonstrate thatpre-training the foundation model with a diverse set of hundreds of millions ofsynthetic data can lead to superior adaptability to unknown problems, withoutnecessitating any subsequent model training or updates in the optimizationprocess. We evaluate our method across a variety of synthetic benchmarks andreal-word applications, and demonstrate its superior generality and competitiveperformance compared to existing methods.</description>
      <author>example@mail.com (Yiming Yao, Fei Liu, Liang Zhao, Xi Lin, Qingfu Zhang)</author>
      <guid isPermaLink="false">2509.03244v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Tabular foundation model for GEOAI benchmark problems BM/AirportSoilProperties/2/2025</title>
      <link>http://arxiv.org/abs/2509.03191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基于Transformer的表格数据基础模型TabPFN在岩土场地表征问题中的应用，展示了其在预测和插补任务中优于传统分层贝叶斯模型的性能。&lt;h4&gt;背景&lt;/h4&gt;研究应用于GEOAI基准测试中的BM/AirportSoilProperties/2/2025问题，涉及预测钻孔深度剖面上不排水抗剪强度的空间变化和在密集站点数据集中插补缺失的力学参数。&lt;h4&gt;目的&lt;/h4&gt;评估TabPFN模型在岩土场地表征问题中的性能，特别是在零训练、少样本、上下文学习设置下的表现，并与传统分层贝叶斯模型进行比较。&lt;h4&gt;方法&lt;/h4&gt;在无需超参数调整的情况下应用TabPFN，并利用大型间接数据库(BID)提供额外上下文信息。比较了TabPFN与传统分层贝叶斯模型(HBM)在两个基准问题上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;TabPFN作为通用基础模型，相比HBM基线具有更高的准确性和良好校准的预测分布，推理效率显著提高。在基准问题1中，TabPFN预测准确性更高且运行速度快一个数量级；在基准问题2中，TabPFN对所有目标参数实现了更低的RMSE和良好量化的不确定性，但累积计算成本较高。&lt;h4&gt;结论&lt;/h4&gt;表格基础模型在岩土建模中的首次成功应用，可能预示着概率场地表征方法的范式转变。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于Transformer的表格数据基础模型TabPFN在岩土场地表征问题中的创新应用，应用于GEOAI基准测试中的BM/AirportSoilProperties/2/2025。研究解决了两个任务：(1)预测钻孔深度剖面上不排水抗剪强度的空间变化，(2)在密集站点数据集中插补缺失的力学参数。研究在零训练、少样本、上下文学习设置中应用TabPFN，无需超参数调整，并提供了来自大型间接数据库(BID)的额外上下文信息。研究表明，作为通用基础模型的TabPFN相比传统分层贝叶斯模型(HBM)基线具有更高的准确性和良好校准的预测分布，同时推理效率显著提高。在基准问题1(空间su预测)中，TabPFN在预测准确性上优于HBM，运行速度快一个数量级。在基准问题2(缺失力学参数插补)中，TabPFN同样对所有目标参数实现了更低的RMSE和良好量化的不确定性，但由于其一次一个变量的推理方式，累积计算成本高于HBM。这些结果标志着表格基础模型在岩土建模中的首次成功应用，暗示了概率场地表征可能发生的范式转变。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel application of the Tabular Prior-Data FittedNetwork (TabPFN) - a transformer-based foundation model for tabular data - togeotechnical site characterization problems defined in the GEOAI benchmarkBM/AirportSoilProperties/2/2025. Two tasks are addressed: (1) predicting thespatial variation of undrained shear strength (su) across borehole depthprofiles, and (2) imputing missing mechanical parameters in a dense-sitedataset. We apply TabPFN in a zero-training, few-shot, in-context learningsetting - without hyper-parameter tuning - and provide it with additionalcontext from the big indirect database (BID). The study demonstrates thatTabPFN, as a general-purpose foundation model, achieved superior accuracy andwell-calibrated predictive distributions compared to a conventionalhierarchical Bayesian model (HBM) baseline, while also offering significantgains in inference efficiency. In Benchmark Problem #1 (spatial su prediction),TabPFN outperformed the HBM in prediction accuracy and delivered anorder-of-magnitude faster runtime. In Benchmark Problem #2 (missing mechanicalparameter imputation), TabPFN likewise achieved lower RMSE for all targetparameters with well-quantified uncertainties, though its cumulativecomputation cost was higher than HBM's due to its one-variable-at-a-timeinference. These results mark the first successful use of a tabular foundationmodel in geotechnical modeling, suggesting a potential paradigm shift inprobabilistic site characterization.</description>
      <author>example@mail.com (Taiga Saito, Yu Otake, Stephen Wu)</author>
      <guid isPermaLink="false">2509.03191v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>RecBase: Generative Foundation Model Pretraining for Zero-Shot Recommendation</title>
      <link>http://arxiv.org/abs/2509.03131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了RecBase，一个专门为推荐任务设计的领域无关基础模型，通过统一的文本表示和特征映射，以及统一的项目编码器，解决了LLM在跨领域推荐中的泛化问题，并在多个数据集上取得了优于现有LLM基线的性能。&lt;h4&gt;背景&lt;/h4&gt;基于LLM的推荐系统最近显示出潜力，但它们的跨领域泛化能力受到语言中心预训练与推荐任务之间基本不匹配的限制。现有依赖语言级知识的方法无法捕获跨领域的动态、项目级用户兴趣。&lt;h4&gt;目的&lt;/h4&gt;为了解决这一问题，作者提出了RecBase，一个具有推荐导向目标的领域无关基础模型，旨在增强跨领域泛化能力，并更好地对齐跨领域的项目语义。&lt;h4&gt;方法&lt;/h4&gt;RecBase利用大规模、异构、跨领域语料库，采用统一的文本表示和特征映射来增强跨领域泛化。作者引入了一个统一的项目编码器，将项目编码为分层概念标识符，实现结构化表示和高效的词汇共享。模型使用自回归目标进行训练，以捕获复杂的项目级顺序模式。&lt;h4&gt;主要发现&lt;/h4&gt;在八个真实世界数据集上，作者具有15亿参数的RecBase模型在零样本和跨领域推荐任务中，匹配或超越了高达70亿参数的LLM基线的性能。&lt;h4&gt;结论&lt;/h4&gt;RecBase通过专门针对推荐任务设计的预训练方法和跨领域对齐机制，有效解决了LLM在推荐系统中的跨领域泛化问题，证明了专门化的基础模型在推荐领域的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近的基于LLM的推荐研究显示出前景，但它们的跨领域泛化能力受到语言中心预训练与推荐任务之间基本不匹配的限制。现有依赖语言级知识的方法无法捕获跨领域的动态、项目级用户兴趣。为了弥补这一差距，我们提出了RecBase，一个具有推荐导向目标的领域无关基础模型。RecBase利用大规模、异构、跨领域语料库，采用统一的文本表示和特征映射来增强跨领域泛化。为了进一步对齐跨领域的项目语义，我们引入了一个统一的项目编码器，将项目编码为分层概念标识符，实现结构化表示和高效的词汇共享。模型使用自回归目标进行训练，以捕获复杂的项目级顺序模式。在八个真实世界数据集上，我们具有15亿参数的模型在零样本和跨领域推荐任务中，匹配或超越了高达70亿参数的LLM基线的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in LLM-based recommendation have shown promise, yet theircross-domain generalization is hindered by a fundamental mismatch betweenlanguage-centric pretraining and the recommendation task. Existing methods,relying on language-level knowledge, fail to capture dynamic, item-level userinterests across domains. To bridge this gap, we propose RecBase, adomain-agnostic foundational model pretrained with a recommendation-orientedobjective. RecBase leverages a large-scale, heterogeneous, cross-domain corpuswith unified textual representations and feature mappings to enhancecross-domain generalization. To further align item semantics across domains, weintroduce a unified item tokenizer that encodes items into hierarchical conceptidentifiers, enabling structured representation and efficient vocabularysharing. The model is trained using an autoregressive objective to capturecomplex item-level sequential patterns. On eight real-world datasets, our1.5B-parameter model matches or surpasses the performance of LLM baselines upto 7B parameters in zero-shot and cross-domain recommendation tasks.</description>
      <author>example@mail.com (Sashuai Zhou, Weinan Gan, Qijiong Liu, Ke Lei, Jieming Zhu, Hai Huang, Yan Xia, Ruiming Tang, Zhenhua Dong, Zhou Zhao)</author>
      <guid isPermaLink="false">2509.03131v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm</title>
      <link>http://arxiv.org/abs/2509.02846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一种针对偏微分方程的测试时计算(TTC)策略，通过在推理过程中利用计算资源实现更准确的预测，同时减少训练样本和模型大小。该方法通过两种基于奖励的模型评估时空一致性，在可压缩欧拉方程模拟中展示了优于标准自回归推理的性能。&lt;h4&gt;背景&lt;/h4&gt;偏微分方程是现代计算科学和工程的基础，但计算成本高昂。现有的PDE基础模型虽然在模拟复杂时空现象方面有前景，但受限于预训练数据集，在自回归展开性能上存在困难，特别是在分布外情况下。此外，这些模型需要大量计算资源和训练数据，限制了它们在许多关键应用中的使用。&lt;h4&gt;目的&lt;/h4&gt;受大型语言模型中'思考'策略的启发，本研究旨在引入第一个用于PDE的测试时计算策略，通过在推理过程中利用计算资源，实现更准确的预测，同时减少训练样本需求和模型大小。&lt;h4&gt;方法&lt;/h4&gt;作者通过两种类型的奖励模型来实现这一目标，这些模型评估基于随机模型的时空一致性预测。研究在PDEGym基准的可压缩欧拉方程模拟上展示了这种方法。&lt;h4&gt;主要发现&lt;/h4&gt;TTC策略相对于标准非自适应自回归推理能够捕捉改进的预测，表明在减少训练数据需求的同时可以提高预测准确性。&lt;h4&gt;结论&lt;/h4&gt;TTC框架为更高级的推理算法或PDE建模奠定了基础，包括构建基于强化学习的方法，这可能改变物理和工程中的计算工作流程，为资源受限环境中的PDE模拟提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;偏微分方程是现代计算科学和工程的基础，本质上计算成本很高。虽然PDE基础模型在模拟复杂时空现象方面显示出很大前景，但现有模型仍然受限于预训练数据集，并且在自回归展开性能方面存在困难，特别是在分布外(OOD)情况下。此外，它们需要大量的计算资源和训练数据，这限制了它们在许多关键应用中的使用。受最近大型语言模型中使用的'思考'策略的启发，我们引入了第一个用于PDE的测试时计算(TTC)策略，该策略在推理过程中利用计算资源，以实现更准确的预测，同时减少训练样本和更小的模型。我们通过两种类型的奖励模型来实现这一点，这些模型评估基于随机模型的时空一致性预测。我们在PDEGym基准的可压缩欧拉方程模拟上展示了这种方法，并表明TTC相对于标准非自适应自回归推理能够捕捉改进的预测。这个TTC框架为更高级的推理算法或PDE建模奠定了基础，包括构建基于强化学习的方法，可能改变物理和工程中的计算工作流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partial Differential Equations (PDEs) are the bedrock for moderncomputational sciences and engineering, and inherently computationallyexpensive. While PDE foundation models have shown much promise for simulatingsuch complex spatio-temporal phenomena, existing models remain constrained bythe pretraining datasets and struggle with auto-regressive rollout performance,especially in out-of-distribution (OOD) cases. Furthermore, they havesignificant compute and training data requirements which hamper their use inmany critical applications. Inspired by recent advances in ``thinking"strategies used in large language models (LLMs), we introduce the firsttest-time computing (TTC) strategy for PDEs that utilizes computationalresources during inference to achieve more accurate predictions with fewertraining samples and smaller models. We accomplish this with two types ofreward models that evaluate predictions of a stochastic based model forspatio-temporal consistency. We demonstrate this method on compressibleEuler-equation simulations from the PDEGym benchmark and show that TTC capturesimproved predictions relative to standard non-adaptive auto-regressiveinference. This TTC framework marks a foundational step towards more advancedreasoning algorithms or PDE modeling, inluding buildingreinforcement-learning-based approaches, potentially transforming computationalworkflows in physics and engineering.</description>
      <author>example@mail.com (Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debarledeben, Ayan Biswas, Diane Oyen, Earl Lawrence)</author>
      <guid isPermaLink="false">2509.02846v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SSVD: Structured SVD for Parameter-Efficient Fine-Tuning and Benchmarking under Domain Shift in ASR</title>
      <link>http://arxiv.org/abs/2509.02830v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE ASRU 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文在ESPnet中首次整合和基准测试了参数高效微调(PEFT)方法，并提出了结构化SVD引导(SSVD)微调方法，在领域偏移的语音识别任务上验证了这些方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;参数高效微调(PEFT)是适应大型基础模型的可扩展解决方案，低秩适应(LoRA)在语音应用中被广泛使用，但其最先进的变体(如VeRA、DoRA、PiSSA和SVFT)主要针对语言和视觉任务开发，在语音领域的验证有限。&lt;h4&gt;目的&lt;/h4&gt;首次在ESPnet中整合和基准测试这些PEFT方法，并引入结构化SVD引导的微调方法。&lt;h4&gt;方法&lt;/h4&gt;提出结构化SVD引导(SSVD)微调，选择性地旋转与输入相关的右奇异向量，同时保持与输出相关的向量固定，以保留语义映射，实现稳健的领域适应。&lt;h4&gt;主要发现&lt;/h4&gt;在模型规模从0.1B到2B的领域偏移语音识别任务(包括儿童语音和方言变化)上评估了所有方法。&lt;h4&gt;结论&lt;/h4&gt;所有实现在ESPnet中发布，以支持可重复性和未来工作。&lt;h4&gt;翻译&lt;/h4&gt;参数高效微调(PEFT)已成为适应大型基础模型的可扩展解决方案。虽然低秩适应(LoRA)在语音应用中被广泛使用，但其最先进的变体，例如VeRA、DoRA、PiSSA和SVFT，主要针对语言和视觉任务开发，在语音领域的验证有限。这项工作首次在ESPnet中整合和基准测试了这些PEFT方法。我们进一步引入了结构化SVD引导(SSVD)微调，该方法选择性地旋转与输入相关的右奇异向量，同时保持与输出相关的向量固定，以保留语义映射。这种设计能够以最少的可训练参数和改进的效率实现稳健的领域适应。我们在领域偏移的语音识别任务(包括儿童语音和方言变化)上评估了所有方法，模型规模从0.1B到2B。所有实现在ESPnet中发布，以支持可重复性和未来工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) has emerged as a scalable solution foradapting large foundation models. While low-rank adaptation (LoRA) is widelyused in speech applications, its state-of-the-art variants, e.g., VeRA, DoRA,PiSSA, and SVFT, are developed mainly for language and vision tasks, withlimited validation in speech. This work presents the first comprehensiveintegration and benchmarking of these PEFT methods within ESPnet. We furtherintroduce structured SVD-guided (SSVD) fine-tuning, which selectively rotatesinput-associated right singular vectors while keeping output-associated vectorsfixed to preserve semantic mappings. This design enables robust domainadaptation with minimal trainable parameters and improved efficiency. Weevaluate all methods on domain-shifted speech recognition tasks, includingchild speech and dialectal variation, across model scales from 0.1B to 2B. Allimplementations are released in ESPnet to support reproducibility and futurework.</description>
      <author>example@mail.com (Pu Wang, Shinji Watanabe, Hugo Van hamme)</author>
      <guid isPermaLink="false">2509.02830v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>The Transparent Earth: A Multimodal Foundation Model for the Earth's Subsurface</title>
      <link>http://arxiv.org/abs/2509.02783v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Transparent Earth是一个基于Transformer的架构，用于从异构数据集中重建地下特性，支持多种模态的数据输入，并展现出优异的性能和可扩展性。&lt;h4&gt;背景&lt;/h4&gt;地下特性重建面临来自异构数据集的挑战，这些数据集在稀疏性、分辨率和模态方面各不相同，每种模态代表不同类型的观测（如应力角、地幔温度、构造板块类型）。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理多种模态数据的模型，用于重建地球地下特性，并实现可扩展性和高性能预测。&lt;h4&gt;方法&lt;/h4&gt;设计了一个基于Transformer的架构，结合了观测的位置编码和通过文本嵌入模型生成的模态编码，使模型能够处理任意数量的模态数据，支持上下文学习。&lt;h4&gt;主要发现&lt;/h4&gt;模型在验证数据上将预测应力角的误差减少了超过三分之二；架构具有可扩展性，性能随参数增加而提高；支持八种不同类型的模态，包括方向角度、类别类别和连续属性。&lt;h4&gt;结论&lt;/h4&gt;Transparent Earth作为地球地下特性的基础模型，能够处理多种模态数据，提供高精度预测，并具有可扩展性，为未来预测地球上任何地方的任何地下特性奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Transparent Earth，这是一个基于Transformer的架构，用于从异构数据集中重建地下特性，这些数据集在稀疏性、分辨率和模态方面各不相同，每种模态代表一种不同类型的观测（例如应力角、地幔温度、构造板块类型）。该模型结合了观测的位置编码和模态编码，后者是通过将文本嵌入模型应用于每种模态的描述而得到的。这种设计使模型能够扩展到任意数量的模态，便于添加初始设计中未考虑的新模态。目前包括八种模态，涵盖方向角度、类别类别和连续属性（如温度和厚度）。这些功能支持上下文学习，使模型能够在没有输入或从任何模态子集添加任意数量的额外观测的情况下生成预测。在验证数据上，这使预测应力角的误差减少了超过三分之二。所提出的架构是可扩展的，并且随着参数的增加表现出改进的性能。这些进展共同使Transparent Earth成为地球地下特性的基础模型，最终旨在预测地球上任何地方的任何地下特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Transparent Earth, a transformer-based architecture forreconstructing subsurface properties from heterogeneous datasets that vary insparsity, resolution, and modality, where each modality represents a distincttype of observation (e.g., stress angle, mantle temperature, tectonic platetype). The model incorporates positional encodings of observations togetherwith modality encodings, derived from a text embedding model applied to adescription of each modality. This design enables the model to scale to anarbitrary number of modalities, making it straightforward to add new ones notconsidered in the initial design. We currently include eight modalitiesspanning directional angles, categorical classes, and continuous propertiessuch as temperature and thickness. These capabilities support in-contextlearning, enabling the model to generate predictions either with no inputs orwith an arbitrary number of additional observations from any subset ofmodalities. On validation data, this reduces errors in predicting stress angleby more than a factor of three. The proposed architecture is scalable anddemonstrates improved performance with increased parameters. Together, theseadvances make the Transparent Earth an initial foundation model for the Earth'ssubsurface that ultimately aims to predict any subsurface property anywhere onEarth.</description>
      <author>example@mail.com (Arnab Mazumder, Javier E. Santos, Noah Hobbs, Mohamed Mehana, Daniel O'Malley)</author>
      <guid isPermaLink="false">2509.02783v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Mentality: A Mamba-based Approach towards Foundation Models for EEG</title>
      <link>http://arxiv.org/abs/2509.02746v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了基于aMamba的选择性状态空间模型等基础模型在增强EEG分析和神经障碍诊断方面的潜力。研究通过自监督重建任务和癫痫检测任务训练模型，并在测试集上达到了0.72的AUROC，为开发大规模、临床适用的EEG数据分析基础模型迈出了重要一步。&lt;h4&gt;背景&lt;/h4&gt;EEG对于诊断癫痫等疾病至关重要，但由于其噪声大、高维度和非线性的特性，面临重大挑战。传统的机器学习方法在自动化EEG分析方面取得了进展，但往往无法捕捉其复杂的时空动态特性。深度学习，特别是序列建模的最新进展，为创建能够处理此类复杂性的更通用和更具表达能力的模型提供了新途径。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于基础模型的方法，特别是aMamba模型，以增强EEG数据分析，提高神经障碍（如癫痫）诊断的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;研究使用基于aMamba的选择性状态空间模型，在一个包含癫痫发作和非癫痫发作EEG记录的大型数据集上进行训练。训练分为两个阶段：首先通过自监督重建任务进行预训练，然后进行癫痫检测任务。最终在保留的测试集上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于aMamba的模型在癫痫检测任务中表现出色，在保留的测试集上达到了0.72的AUROC（受试者工作特征曲线下面积），证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;这种基于aMamba的基础模型方法代表了开发大规模、临床适用的EEG数据分析基础模型的重要一步，有望改善神经障碍的诊断过程。&lt;h4&gt;翻译&lt;/h4&gt;这项工作探讨了基础模型，特别是基于aMamba的选择性状态空间模型，在增强EEG分析和神经障碍诊断方面的潜力。EEG对于诊断癫痫等疾病至关重要，但由于其噪声大、高维度和非线性的特性，面临重大挑战。传统的机器学习方法在自动化EEG分析方面取得了进展，但往往无法捕捉其复杂的时空动态特性。深度学习，特别是序列建模的最新进展，为创建能够处理此类复杂性的更通用和更具表达能力的模型提供了新途径。通过在一个包含癫痫发作和非癫痫发作EEG记录的大型数据集上，通过自监督重建任务后接癫痫检测任务来训练基于aMamba的模型，我们证明了模型的有效性，在保留的测试集上达到了0.72的AUROC。这种方法标志着开发用于EEG数据分析的大规模、临床适用的基础模型迈出了重要一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores the potential of foundation models, specifically aMamba-based selective state space model, for enhancing EEG analysis inneurological disorder diagnosis. EEG, crucial for diagnosing conditions likeepilepsy, presents significant challenges due to its noisy, high-dimensional,and nonlinear nature. Traditional machine learning methods have made advancesin automating EEG analysis but often fail to capture its complexspatio-temporal dynamics. Recent advances in deep learning, particularly insequence modeling, offer new avenues for creating more generalized andexpressive models capable of handling such complexities. By training aMamba-based model on a large dataset containing seizure and non-seizure EEGrecordings through a self-supervised reconstruction task followed by a seizuredetection task, we demonstrate the model's effectiveness, achieving an AUROC of0.72 on a held-out test set. This approach marks a significant step towarddeveloping large-scale, clinically applicable foundation models for EEG dataanalysis.</description>
      <author>example@mail.com (Saarang Panchavati, Corey Arnold, William Speier)</author>
      <guid isPermaLink="false">2509.02746v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Planning with Reasoning using Vision Language World Model</title>
      <link>http://arxiv.org/abs/2509.02722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Vision Language World Model (VLWM)，一种基于自然视频训练的基础模型，用于语言驱动的世界建模。该模型能够从视觉观察中推断目标成就并预测由交替动作和世界状态变化组成的轨迹，实现了在视觉规划任务上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;有效的规划需要强大的世界模型，但能够通过语义和时间抽象来理解和推理动作的高层次世界模型仍然发展不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够理解和推理动作的高层次世界模型，用于视觉规划任务。&lt;h4&gt;方法&lt;/h4&gt;1. 提出Vision Language World Model (VLWM)，一种基于自然视频训练的基础模型；2. 使用迭代LLM自我精炼方法，基于由标题树表示的压缩未来观测提取目标；3. 学习动作策略和动力学模型，分别促进反应式系统1计划解码和通过成本最小化的反思式系统2规划；4. 使用自我监督方式训练的批评家模型评估假设未来状态与预期目标状态之间的语义距离。&lt;h4&gt;主要发现&lt;/h4&gt;1. VLWM在视觉规划辅助(VPA)任务上取得了最先进的性能，包括基准评估和提出的PlannerArena人类评估；2. 系统2相比系统1将Elo评分提高了27%；3. VLWM模型在RoboVQA和世界预测基准上优于强大的VLM基线模型。&lt;h4&gt;结论&lt;/h4&gt;Vision Language World Model (VLWM)是一种有效的世界建模方法，能够通过语义和时间抽象来理解和推理动作，在视觉规划任务上取得了显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;有效的规划需要强大的世界模型，但能够通过语义和时间抽象来理解和推理动作的高层次世界模型仍然发展不足。我们介绍了Vision Language World Model (VLWM)，这是一种针对自然视频的语言驱动世界建模而训练的基础模型。给定视觉观察，VLWM首先推断整体目标成就，然后预测由交替动作和世界状态变化组成的轨迹。这些目标通过基于由标题树表示的压缩未来观测的迭代LLM自我精炼来提取。VLWM学习动作策略和动力学模型，分别促进通过成本最小化的反应式系统1计划解码和反思式系统2规划。成本评估VLWM滚动给出的假设未来状态与预期目标状态之间的语义距离，并通过我们以自我监督方式训练的批评家模型进行测量。VLWM在基准评估和我们提出的PlannerArena人类评估中均实现了最先进的视觉规划辅助(VPA)性能，其中系统2比系统1将Elo评分提高了27%。VLWM模型在RoboVQA和世界预测基准上也优于强大的VLM基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective planning requires strong world models, but high-level world modelsthat can understand and reason about actions with semantic and temporalabstraction remain largely underdeveloped. We introduce the Vision LanguageWorld Model (VLWM), a foundation model trained for language-based worldmodeling on natural videos. Given visual observations, the VLWM first infersthe overall goal achievements then predicts a trajectory composed ofinterleaved actions and world state changes. Those targets are extracted byiterative LLM Self-Refine conditioned on compressed future observationsrepresented by Tree of Captions. The VLWM learns both an action policy and adynamics model, which respectively facilitates reactive system-1 plan decodingand reflective system-2 planning via cost minimization. The cost evaluates thesemantic distance between the hypothetical future states given by VLWMroll-outs and the expected goal state, and is measured by a critic model thatwe trained in a self-supervised manner. The VLWM achieves state-of-the-artVisual Planning for Assistance (VPA) performance on both benchmark evaluationsand our proposed PlannerArena human evaluations, where system-2 improves theElo score by +27% upon system-1. The VLWM models also outperforms strong VLMbaselines on RoboVQA and WorldPrediction benchmark.</description>
      <author>example@mail.com (Delong Chen, Theo Moutakanni, Willy Chung, Yejin Bang, Ziwei Ji, Allen Bolourchi, Pascale Fung)</author>
      <guid isPermaLink="false">2509.02722v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Toward a robust lesion detection model in breast DCE-MRI: adapting foundation models to high-risk women</title>
      <link>http://arxiv.org/abs/2509.02710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于医学切片转换器(MST)和Kolmogorov-Arnold网络(KAN)的乳腺癌病变分类流水线，使用动态对比增强MRI进行检测，在保持可解释性的同时实现了较高的分类性能。&lt;h4&gt;背景&lt;/h4&gt;准确的乳腺癌MRI病变检测对早期癌症诊断至关重要，特别是对高风险人群。&lt;h4&gt;目的&lt;/h4&gt;开发一个分类流水线，使用动态对比增强MRI对乳腺病变进行分类，区分良性和恶性病变。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的基础模型Medical Slice Transformer (MST)，利用DINOv2基于自监督的预训练生成强大的逐切片特征嵌入，然后使用这些嵌入训练Kolmogorov-Arnold Network (KAN)分类器。KAN通过自适应B样条激活函数实现局部非线性变换，作为传统卷积网络的灵活且可解释的替代方案。&lt;h4&gt;主要发现&lt;/h4&gt;MST+KAN流水线优于基线MST分类器，达到AUC = 0.80±0.02，并通过基于注意力的热图保留了可解释性。&lt;h4&gt;结论&lt;/h4&gt;结合基础模型嵌入和先进分类策略对于构建稳健且可推广的乳腺癌MRI分析工具是有效的。&lt;h4&gt;翻译&lt;/h4&gt;准确的乳腺癌MRI病变检测对早期癌症诊断至关重要，特别是在高风险人群中。我们提出了一种分类流水线，该流水线采用预训练的基础模型医学切片转换器(MST)，利用动态对比增强MRI(DCE-MRI)进行乳腺病变分类。利用基于DINOv2的自监督预训练，MST生成强大的逐切片特征嵌入，然后使用这些嵌入训练Kolmogorov-Arnold网络(KAN)分类器。KAN通过自适应B样条激活函数实现局部非线性变换，为传统卷积网络提供了灵活且可替代的方案。这增强了模型在不平衡和异质性临床数据集中区分良性和恶性病变的能力。实验结果表明，MST+KAN流水线优于基线MST分类器，达到AUC = 0.80±0.02，同时通过基于注意力的热图保留了可解释性。我们的研究结果表明，将基础模型嵌入与先进的分类策略相结合对于构建稳健且可推广的乳腺癌MRI分析工具是有效的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate breast MRI lesion detection is critical for early cancer diagnosis,especially in high-risk populations. We present a classification pipeline thatadapts a pretrained foundation model, the Medical Slice Transformer (MST), forbreast lesion classification using dynamic contrast-enhanced MRI (DCE-MRI).Leveraging DINOv2-based self-supervised pretraining, MST generates robustper-slice feature embeddings, which are then used to train a Kolmogorov--ArnoldNetwork (KAN) classifier. The KAN provides a flexible and interpretablealternative to conventional convolutional networks by enabling localizednonlinear transformations via adaptive B-spline activations. This enhances themodel's ability to differentiate benign from malignant lesions in imbalancedand heterogeneous clinical datasets. Experimental results demonstrate that theMST+KAN pipeline outperforms the baseline MST classifier, achieving AUC = 0.80\pm 0.02 while preserving interpretability through attention-based heatmaps.Our findings highlight the effectiveness of combining foundation modelembeddings with advanced classification strategies for building robust andgeneralizable breast MRI analysis tools.</description>
      <author>example@mail.com (Gabriel A. B. do Nascimento, Vincent Dong, Guilherme J. Cavalcante, Alex Nguyen, Thaís G. do Rêgo, Yuri Malheiros, Telmo M. Silva Filho, Carla R. Zeballos Torrez, James C. Gee, Anne Marie McCarthy, Andrew D. A. Maidment, Bruno Barufaldi)</author>
      <guid isPermaLink="false">2509.02710v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>FastVGGT: Training-Free Acceleration of Visual Geometry Transformer</title>
      <link>http://arxiv.org/abs/2509.02560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为FastVGGT的新方法，通过标记合并技术解决了3D视觉基础模型在处理长序列图像输入时的推理效率低下问题。FastVGGT首次在3D领域应用标记合并技术，通过一种无需训练的机制加速VGGT模型，同时保持其强大的重建能力。实验证明，在处理1000张输入图像时，FastVGGT比原始VGGT快4倍，同时减轻了长序列场景中的误差累积问题。&lt;h4&gt;背景&lt;/h4&gt;3D视觉基础模型最近在3D感知方面展示了卓越的能力。然而，将这些模型扩展到长序列图像输入仍然是一个重大挑战，主要原因是推理时的效率低下。&lt;h4&gt;目的&lt;/h4&gt;解决3D视觉基础模型在处理长序列图像输入时的推理效率低下问题，同时保持模型的重建能力。&lt;h4&gt;方法&lt;/h4&gt;1. 分析了VGGT（一种先进的前馈视觉几何模型）并确定其主要瓶颈；2. 通过可视化发现了注意图中存在的标记崩溃现象；3. 探索了在前馈视觉几何模型中使用标记合并的潜力；4. 针对现有合并技术在3D模型中直接应用的挑战，提出了FastVGGT；5. 设计了一种针对3D架构和任务定制的独特标记分区策略。&lt;h4&gt;主要发现&lt;/h4&gt;1. VGGT模型在处理长序列图像输入时存在推理效率低下的主要瓶颈；2. 注意图中存在标记崩溃现象；3. 标记合并技术可以有效地消除冗余计算，同时保持VGGT的强大重建能力；4. 在处理1000张输入图像时，FastVGGT比原始VGGT快4倍；5. FastVGGT减轻了长序列场景中的误差累积问题。&lt;h4&gt;结论&lt;/h4&gt;标记合并作为一种有原则的解决方案，在可扩展的3D视觉系统中具有巨大潜力。FastVGGT的成功证明了标记合并技术在3D领域的有效性，为构建更高效的3D视觉系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;3D视觉的基础模型最近在3D感知方面展示了卓越的能力。然而，由于推理时的效率低下，将这些模型扩展到长序列图像输入仍然是一个重大挑战。在这项工作中，我们详细分析了VGGT（一种先进的前馈视觉几何模型），并确定了其主要瓶颈。可视化进一步揭示了注意图中存在的标记崩溃现象。受这些发现的启发，我们探索了在前馈视觉几何模型中使用标记合并的潜力。由于3D模型独特的架构和任务特定特性，直接应用现有的合并技术证明具有挑战性。为此，我们提出了FastVGGT，这是首次通过一种无需训练的机制在3D领域利用标记合并来加速VGGT。我们设计了一种针对3D架构和任务定制的独特标记分区策略，有效地消除了冗余计算，同时保留了VGGT强大的重建能力。在多个3D几何基准上的广泛实验验证了我们方法的有效性。值得注意的是，在处理1000张输入图像时，FastVGGT比VGGT实现了4倍的加速，同时减轻了长序列场景中的误差累积。这些发现强调了标记合并作为可扩展3D视觉系统原则性解决方案的潜力。代码可在以下网址获取：https://mystorm16.github.io/fastvggt/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决VGGT模型在处理长序列图像输入时的推理效率低下问题。这个问题在现实中很重要，因为3D视觉基础模型对于机器理解和交互物理世界至关重要，但现有模型在处理大规模场景时计算成本过高，限制了实际应用。此外，长序列场景中的误差累积会导致预测漂移，影响重建质量，而原始VGGT在处理超过300张图像时还会出现内存不足问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先对VGGT进行了详细的组件级性能分析，发现Global Attention模块是主要瓶颈。通过可视化注意力图，他们发现不同token之间存在高度相似的注意力模式，表明存在大量冗余计算。基于这一观察，他们借鉴了token merging技术（如ToMeSD），并设计了专门针对3D重建任务的token分区策略。他们还参考了VGGT*的内存优化方法来处理更长的输入序列。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用token合并技术减少Global Attention模块中的计算量，同时保留关键token以确保重建质量和跨帧对应关系。整体流程包括：1) Token分区：将第一帧作为参考(dst)，保留显著token，使用区域随机采样分配dst和src；2) Token合并：将相似token合并以减少计算量；3) Token解合并：恢复原始分辨率以保持与VGGT架构兼容；4) VRAM优化：支持处理更长的输入序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将token合并引入视觉几何架构；设计针对3D任务的独特token分区策略；提出训练自由加速机制；通过内存优化支持处理1000+图像输入。与传统2D方法不同，FastVGGT处理多视图序列并保持跨帧对应；相比简单采样策略，它采用综合分区方法；相比VGGT-Long，它在加速同时减少误差累积；相比Fast3R等方法，它在保持精度的同时实现更显著加速。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; FastVGGT通过创新的token合并策略，在不牺牲重建质量的情况下实现了VGGT模型4倍以上的推理加速，同时显著减少了长序列3D重建中的误差累积问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for 3D vision have recently demonstrated remarkablecapabilities in 3D perception. However, scaling these models to long-sequenceimage inputs remains a significant challenge due to inference-timeinefficiency. In this work, we present a detailed analysis of VGGT, astate-of-the-art feed-forward visual geometry model and identify its primarybottleneck. Visualization further reveals a token collapse phenomenon in theattention maps. Motivated by these findings, we explore the potential of tokenmerging in the feed-forward visual geometry model. Owing to the uniquearchitectural and task-specific properties of 3D models, directly applyingexisting merging techniques proves challenging. To this end, we proposeFastVGGT, which, for the first time, leverages token merging in the 3D domainthrough a training-free mechanism for accelerating VGGT. we devise a uniquetoken partitioning strategy tailored to 3D architectures and tasks, effectivelyeliminating redundant computation while preserving VGGT's powerfulreconstruction capacity. Extensive experiments on multiple 3D geometrybenchmarks validate the effectiveness of our approach. Notably, with 1000 inputimages, FastVGGT achieves a 4x speedup over VGGT while mitigating erroraccumulation in long-sequence scenarios. These findings underscore thepotential of token merging as a principled solution for scalable 3D visionsystems. Code is available at: https://mystorm16.github.io/fastvggt/.</description>
      <author>example@mail.com (You Shen, Zhipeng Zhang, Yansong Qu, Liujuan Cao)</author>
      <guid isPermaLink="false">2509.02560v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>GRAM-R$^2$: Self-Training Generative Foundation Reward Models for Reward Reasoning</title>
      <link>http://arxiv.org/abs/2509.02492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为GRAM-R²的自训练方法，开发了一种生成式奖励模型，能够生成偏好标签和奖励理由，可作为奖励推理的基础模型，适用于多种任务且只需少量或无需额外微调。&lt;h4&gt;背景&lt;/h4&gt;奖励建模领域近年来从特定任务设计向通用奖励模型转变，但仍面临基本挑战：严重依赖大规模标记的偏好数据。虽然未标记数据预训练有前景，但现有方法无法在奖励模型中植入明确的推理能力。&lt;h4&gt;目的&lt;/h4&gt;为了弥合这一差距，研究提出一种自训练方法，利用未标记数据来激发奖励模型中的奖励推理能力，并开发相应的生成式奖励模型。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种自训练方法，利用未标记数据激发奖励推理能力，基于此方法开发了GRAM-R²，一种不仅生成偏好标签还生成相应奖励理由的生成式奖励模型。&lt;h4&gt;主要发现&lt;/h4&gt;在响应排序、任务适应和基于人类反馈的强化学习实验中，GRAM-R²始终表现出强大的性能，优于多个强大的判别式和生成式基线模型。&lt;h4&gt;结论&lt;/h4&gt;GRAM-R²可作为奖励推理的基础模型，适用于广泛的应用，如响应排序和任务特定的奖励调整，且只需少量或无需额外微调。&lt;h4&gt;翻译&lt;/h4&gt;近年来，奖励建模领域的显著进展是由从特定任务设计向通用奖励模型的范式转变驱动的。尽管有这一趋势，开发有效的奖励模型仍然是一个基本挑战：严重依赖大规模标记的偏好数据。在大量未标记数据上进行预训练提供了一个有前景的方向，但现有方法无法在奖励模型中植入明确的推理能力。为了弥合这一差距，我们提出了一种自训练方法，利用未标记数据来激发奖励模型中的奖励推理。基于这种方法，我们开发了GRAM-R²，一个不仅生成偏好标签还生成相应奖励理由的生成式奖励模型。GRAM-R²可作为奖励推理的基础模型，适用于广泛的应用，只需少量或无需额外微调。它可支持下游应用，如响应排序和任务特定的奖励调整。在响应排序、任务适应和基于人类反馈的强化学习实验中，GRAM-R²始终表现出强大的性能，优于多个强大的判别式和生成式基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant progress in reward modeling over recent years has been driven bya paradigm shift from task-specific designs towards generalist reward models.Despite this trend, developing effective reward models remains a fundamentalchallenge: the heavy reliance on large-scale labeled preference data.Pre-training on abundant unlabeled data offers a promising direction, butexisting approaches fall short of instilling explicit reasoning into rewardmodels. To bridge this gap, we propose a self-training approach that leveragesunlabeled data to elicit reward reasoning in reward models. Based on thisapproach, we develop GRAM-R$^2$, a generative reward model trained to producenot only preference labels but also accompanying reward rationales. GRAM-R$^2$can serve as a foundation model for reward reasoning and can be applied to awide range of tasks with minimal or no additional fine-tuning. It can supportdownstream applications such as response ranking and task-specific rewardtuning. Experiments on response ranking, task adaptation, and reinforcementlearning from human feedback demonstrate that GRAM-R$^2$ consistently deliversstrong performance, outperforming several strong discriminative and generativebaselines.</description>
      <author>example@mail.com (Chenglong Wang, Yongyu Mu, Hang Zhou, Yifu Huo, Ziming Zhu, Jiali Zeng, Murun Yang, Bei Li, Tong Xiao, Xiaoyang Hao, Chunliang Zhang, Fandong Meng, Jingbo Zhu)</author>
      <guid isPermaLink="false">2509.02492v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>SpecEval: Evaluating Model Adherence to Behavior Specifications</title>
      <link>http://arxiv.org/abs/2509.02464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一个自动化框架，用于审计基础模型是否遵循其提供商发布的行为准则，发现存在系统性不一致，包括高达20%的合规差距。&lt;h4&gt;背景&lt;/h4&gt;开发基础模型的公司发布了行为准则，但模型是否实际遵循这些准则尚不清楚。虽然OpenAI、Anthropic和Google等提供商已发布详细的安全约束和定性规范，但尚未有对这些准则遵循情况的系统性审计。&lt;h4&gt;目的&lt;/h4&gt;引入一个自动化框架，用于审计模型是否遵循提供商的规范。&lt;h4&gt;方法&lt;/h4&gt;通过解析行为陈述、生成有针对性的提示，并使用模型来判断遵循情况，来审计模型。重点关注提供商规范、模型输出和开发者评估模型之间的三向一致性，这是之前双向生成器-验证器一致性的扩展。&lt;h4&gt;主要发现&lt;/h4&gt;将框架应用于来自六个开发者的16个模型，针对100多个行为陈述，发现系统性的不一致性，包括提供商之间高达20%的合规差距。&lt;h4&gt;结论&lt;/h4&gt;建立了一个必要基线：基础模型至少应在开发者评估模型的判断下，始终满足开发者的行为规范。&lt;h4&gt;翻译&lt;/h4&gt;开发基础模型的公司发布了他们承诺模型将遵循的行为准则，但模型是否实际遵循这些准则尚不清楚。虽然OpenAI、Anthropic和Google等提供商已发布详细的安全约束和定性规范，但尚未有对这些准则遵循情况的系统性审计。我们引入了一个自动化框架，通过解析行为陈述、生成有针对性的提示，并使用模型来判断遵循情况，来审计模型是否符合提供商的规范。我们的重点是提供商规范、模型输出和开发者自身评估模型之间的三向一致性；这是之前双向生成器-验证器一致性的扩展。这建立了一个必要的基线：最低要求是，基础模型应在开发者评估模型的判断下，始终满足开发者的行为规范。我们将框架应用于来自六个开发者的16个模型，针对100多个行为陈述，发现系统性的不一致性，包括提供商之间高达20%的合规差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Companies that develop foundation models publish behavioral guidelines theypledge their models will follow, but it remains unclear if models actually doso. While providers such as OpenAI, Anthropic, and Google have publisheddetailed specifications describing both desired safety constraints andqualitative traits for their models, there has been no systematic audit ofadherence to these guidelines. We introduce an automated framework that auditsmodels against their providers specifications by parsing behavioral statements,generating targeted prompts, and using models to judge adherence. Our centralfocus is on three way consistency between a provider specification, its modeloutputs, and its own models as judges; an extension of prior two way generatorvalidator consistency. This establishes a necessary baseline: at minimum, afoundation model should consistently satisfy the developer behavioralspecifications when judged by the developer evaluator models. We apply ourframework to 16 models from six developers across more than 100 behavioralstatements, finding systematic inconsistencies including compliance gaps of upto 20 percent across providers.</description>
      <author>example@mail.com (Ahmed Ahmed, Kevin Klyman, Yi Zeng, Sanmi Koyejo, Percy Liang)</author>
      <guid isPermaLink="false">2509.02464v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>AppCopilot: Toward General, Accurate, Long-Horizon, and Efficient Mobile Agent</title>
      <link>http://arxiv.org/abs/2509.02444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project at https://github.com/OpenBMB/AppCopilot&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了移动智能体领域的四个核心问题，并介绍了解决方案AppCopilot，这是一个多模态、多智能体的通用设备端助手系统，在泛化能力、准确性、长期能力和效率方面均有显著提升。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型和多模态基础模型的快速发展，移动智能体领域不断扩展但尚未解决基本挑战。&lt;h4&gt;目的&lt;/h4&gt;识别移动智能体需要解决的四个核心问题，并提出一个全栈闭环系统解决方案，使移动智能体能够产生实际、可扩展的影响。&lt;h4&gt;方法&lt;/h4&gt;设计并实现AppCopilot系统，包括模型层(集成多模态基础模型)、推理和控制层(思维链推理、分层任务规划和多智能体协作)、执行层(用户个性化、语音交互等功能)，并采用配置文件驱动的优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;AppCopilot在四个核心维度上取得显著改进：更强的跨任务泛化能力、更高精度的屏幕交互操作、更可靠的长距离任务完成能力，以及更快、更资源高效的运行时性能。&lt;h4&gt;结论&lt;/h4&gt;AppCopilot作为一个全栈闭环系统，有效解决了移动智能体面临的核心挑战，为实际应用提供了可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型和多模态基础模型的快速发展，移动智能体领域不断扩展但尚未收敛解决基本挑战。本文确定了移动智能体需要解决的四个核心问题，以实现其实际、可扩展的影响：(1)跨任务、模态、应用和设备的泛化能力；(2)准确性，特别是精确的屏幕交互和点击定位；(3)长期能力，用于持续的多步骤目标；(4)效率，特别是在资源受限设备上的高性能运行时。我们提出了AppCopilot，这是一个多模态、多智能体、通用的设备端助手，可跨应用操作，构成从数据到部署的全栈闭环系统。AppCopilot通过端到端自主流水线实现这一理念，涵盖数据收集、训练、部署、高质量高效推理和移动应用开发。在模型层，它集成了支持中英文的多模态基础模型。在推理和控制层，它结合了思维链推理、分层任务规划和分解以及多智能体协作。在执行层，它支持用户个性化体验适应、语音交互、功能调用、跨应用和跨设备编排以及全面的移动应用支持。系统设计采用了针对异构硬件的延迟、内存和能耗的配置文件驱动优化。实验证明，AppCopilot在所有四个维度上都取得了显著改进：更强的泛化能力、更高精度的屏幕操作、更可靠的长距离任务完成以及更快、更资源高效的运行时。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the raid evolution of large language models and multimodal foundationmodels, the mobile-agent landscape has proliferated without converging on thefundamental challenges. This paper identifies four core problems that must besolved for mobile agents to deliver practical, scalable impact: (1)generalization across tasks, modalities, apps, and devices; (2) accuracy,specifically precise on-screen interaction and click targeting; (3)long-horizon capability for sustained, multi-step goals; and (4) efficiency,specifically high-performance runtime on resource-constrained devices. Wepresent AppCopilot, a multimodal, multi-agent, general-purpose on-deviceassistant that operates across applications and constitutes a full-stack,closed-loop system from data to deployment. AppCopilot operationalizes thisposition through an end-to-end autonomous pipeline spanning data collection,training, deployment, high-quality and efficient inference, and mobileapplication development. At the model layer, it integrates multimodalfoundation models with robust Chinese-English support. At the reasoning andcontrol layer, it combines chain-of-thought reasoning, hierarchical taskplanning and decomposition, and multi-agent collaboration. At the executionlayer, it enables user personalization and experiential adaptation, voiceinteraction, function calling, cross-app and cross-device orchestration, andcomprehensive mobile app support. The system design incorporatesprofiling-driven optimization for latency, memory, and energy acrossheterogeneous hardware. Empirically, AppCopilot achieves significantimprovements along all four dimensions: stronger generalization,higher-precision on-screen actions, more reliable long-horizon task completion,and faster, more resource-efficient runtime.</description>
      <author>example@mail.com (Jingru Fan, Yufan Dang, Jingyao Wu, Huatao Li, Runde Yang, Xiyuan Yang, Yuheng Wang, Zhong Zhang, Yaxi Lu, Yankai Lin, Zhiyuan Liu, Dahai Li, Chen Qian)</author>
      <guid isPermaLink="false">2509.02444v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>OpenGuide: Assistive Object Retrieval in Indoor Spaces for Individuals with Visual Impairments</title>
      <link>http://arxiv.org/abs/2509.02425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenGuide，一个专为盲人或视觉障碍人士设计的辅助移动机器人系统，用于在复杂室内环境中高效定位多个物体。该系统结合了多种先进技术，能够处理开放词汇请求并自适应导航。&lt;h4&gt;背景&lt;/h4&gt;室内建筑环境如家庭和办公室通常具有复杂和杂乱的布局，这对盲人或视觉障碍人士构成重大挑战。现有辅助技术多专注于基础导航或障碍避免，缺乏在现实世界中可扩展和高效的多物体搜索能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在复杂室内环境中帮助盲人或视觉障碍人士高效定位多个物体的可扩展且高效的辅助系统。&lt;h4&gt;方法&lt;/h4&gt;OpenGuide系统结合了自然语言理解、视觉语言基础模型(VLM)、基于前沿的探索方法和部分可观察马尔可夫决策过程(POMDP)规划器。系统能解释开放词汇请求，推理物体-场景关系，并通过价值衰减和信念空间推理实现从漏检的稳健恢复。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中，OpenGuide系统在任务成功率和搜索效率方面相比先前方法有显著提高。&lt;h4&gt;结论&lt;/h4&gt;OpenGuide为辅助生活环境中可扩展的、以人为中心的机器人辅助奠定了基础，能够有效解决盲人或视觉障碍人士在复杂环境中寻找多个物体的挑战。&lt;h4&gt;翻译&lt;/h4&gt;室内建筑环境如家庭和办公室通常呈现复杂和杂乱的布局，对盲人或视觉障碍人士构成重大挑战，特别是在涉及定位和收集多个物体的任务中。虽然许多现有的辅助技术专注于基础导航或障碍避免，但很少有系统能够在现实世界、部分可观察的环境中提供可扩展和高效的多物体搜索功能。为了解决这一差距，我们引入了OpenGuide，这是一个辅助移动机器人系统，它结合了自然语言理解、视觉语言基础模型、基于前沿的探索和部分可观察马尔可夫决策过程规划器。OpenGuide解释开放词汇请求，推理物体-场景关系，并在新环境中自适应导航和定位多个目标物品。我们的方法通过价值衰减和信念空间推理实现了从漏检的稳健恢复，从而实现更有效的探索和物体定位。我们在模拟和真实世界实验中验证了OpenGuide，证明了与先前方法相比，任务成功率和搜索效率有显著提高。这项工作为辅助生活环境中可扩展的、以人为中心的机器人辅助奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor built environments like homes and offices often present complex andcluttered layouts that pose significant challenges for individuals who areblind or visually impaired, especially when performing tasks that involvelocating and gathering multiple objects. While many existing assistivetechnologies focus on basic navigation or obstacle avoidance, few systemsprovide scalable and efficient multi-object search capabilities in real-world,partially observable settings. To address this gap, we introduce OpenGuide, anassistive mobile robot system that combines natural language understanding withvision-language foundation models (VLM), frontier-based exploration, and aPartially Observable Markov Decision Process (POMDP) planner. OpenGuideinterprets open-vocabulary requests, reasons about object-scene relationships,and adaptively navigates and localizes multiple target items in novelenvironments. Our approach enables robust recovery from missed detectionsthrough value decay and belief-space reasoning, resulting in more effectiveexploration and object localization. We validate OpenGuide in simulated andreal-world experiments, demonstrating substantial improvements in task successrate and search efficiency over prior methods. This work establishes afoundation for scalable, human-centered robotic assistance in assisted livingenvironments.</description>
      <author>example@mail.com (Yifan Xu, Qianwei Wang, Vineet Kamat, Carol Menassa)</author>
      <guid isPermaLink="false">2509.02425v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MedDINOv3: How to adapt vision foundation models for medical image segmentation?</title>
      <link>http://arxiv.org/abs/2509.02379v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MedDINOv3，一个用于将DINOv3适应到医学图像分割的简单有效框架，通过重新设计ViT架构和域自适应预训练，在多个分割任务中达到了或超过了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;CT和MRI扫描中器官和肿瘤的准确分割对诊断、治疗规划和疾病监测至关重要。虽然深度学习已推进了自动分割，但大多数模型仍然是特定任务的，缺乏跨模态和机构的泛化能力。视觉基础模型在大规模自然图像上预训练，提供了强大且可迁移的表示，但适应医学影像面临两个关键挑战：ViT骨干在医学图像分割上表现不如专业CNN，以及自然图像与医学图像之间的域差距限制了可迁移性。&lt;h4&gt;目的&lt;/h4&gt;开发一个简单有效的框架，用于将DINOv3视觉基础模型适应到医学图像分割任务中，解决域差距和性能问题。&lt;h4&gt;方法&lt;/h4&gt;重新审视普通ViT架构，设计具有多尺度令牌聚合的简单有效架构；在CT-3M（包含387万张轴向CT切片的精选集合）上进行域自适应预训练；使用多阶段DINOv3配方学习鲁棒密集特征。&lt;h4&gt;主要发现&lt;/h4&gt;MedDINOv3在四个分割基准测试中匹配或超过了最先进的性能，证明了视觉基础模型作为医学图像分割统一骨干的潜力。&lt;h4&gt;结论&lt;/h4&gt;视觉基础模型可以作为医学图像分割的统一骨干网络，通过适当的架构设计和域自适应预训练，能够实现跨模态和机构的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;CT和MRI扫描中器官和肿瘤的准确分割对于诊断、治疗规划和疾病监测至关重要。虽然深度学习已推动了自动分割的发展，但大多数模型仍然是特定任务的，缺乏跨模态和机构的泛化能力。视觉基础模型(FMs)在大规模自然图像上预训练，提供了强大且可迁移的表示。然而，将它们适应到医学影像面临两个关键挑战：(1)大多数基础模型的ViT骨干在医学图像分割上仍然表现不如专业CNN，(2)自然图像与医学图像之间的大域差距限制了可迁移性。我们引入了MedDINOv3，这是一个简单有效的框架，用于将DINOv3适应到医学分割中。我们首先重新审视普通ViT，并设计了一个具有多尺度令牌聚合的简单有效架构。然后，我们在CT-3M（一个包含387万张轴向CT切片的精选集合）上进行域自适应预训练，使用多阶段DINOv3配方学习鲁棒密集特征。MedDINOv3在四个分割基准测试中匹配或超过了最先进的性能，证明了视觉基础模型作为医学图像分割统一骨干的潜力。代码可在https://github.com/ricklisz/MedDINOv3获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate segmentation of organs and tumors in CT and MRI scans is essentialfor diagnosis, treatment planning, and disease monitoring. While deep learninghas advanced automated segmentation, most models remain task-specific, lackinggeneralizability across modalities and institutions. Vision foundation models(FMs) pretrained on billion-scale natural images offer powerful andtransferable representations. However, adapting them to medical imaging facestwo key challenges: (1) the ViT backbone of most foundation models stillunderperform specialized CNNs on medical image segmentation, and (2) the largedomain gap between natural and medical images limits transferability. Weintroduce MedDINOv3, a simple and effective framework for adapting DINOv3 tomedical segmentation. We first revisit plain ViTs and design a simple andeffective architecture with multi-scale token aggregation. Then, we performdomain-adaptive pretraining on CT-3M, a curated collection of 3.87M axial CTslices, using a multi-stage DINOv3 recipe to learn robust dense features.MedDINOv3 matches or exceeds state-of-the-art performance across foursegmentation benchmarks, demonstrating the potential of vision foundationmodels as unified backbones for medical image segmentation. The code isavailable at https://github.com/ricklisz/MedDINOv3.</description>
      <author>example@mail.com (Yuheng Li, Yizhou Wu, Yuxiang Lai, Mingzhe Hu, Xiaofeng Yang)</author>
      <guid isPermaLink="false">2509.02379v2</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Scale, Don't Fine-tune: Guiding Multimodal LLMs for Efficient Visual Place Recognition at Test-Time</title>
      <link>http://arxiv.org/abs/2509.02129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于测试时缩放(TTS)的新型零样本框架，利用多模态大语言模型的视觉-语言对齐能力，通过基于引导的方法进行直接相似性评分，实现高效跨域视觉位置识别。&lt;h4&gt;背景&lt;/h4&gt;视觉位置识别(VPR)已从手工设计的特征描述符发展到深度学习方法，但当前方法(包括视觉基础模型和多模态大语言模型)虽增强了语义理解，却在微调时存在高计算开销和有限的跨域迁移性问题。&lt;h4&gt;目的&lt;/h4&gt;解决当前VPR方法中高计算开销和有限跨域迁移性的问题，实现实时适应且无需额外训练成本。&lt;h4&gt;方法&lt;/h4&gt;采用测试时缩放(TTS)框架，利用基于引导的方法直接进行相似性评分，采用结构化提示生成长度可控的JSON输出消除两阶段处理，并整合不确定性感知自一致性(UASC)实现实时适应。&lt;h4&gt;主要发现&lt;/h4&gt;实现了跨域VPR性能的显著提升，计算效率提高高达210倍，能够在不同环境中实现优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该TTS与UASC相结合的框架实现了实时适应，无需额外训练成本即可实现优越的跨域泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉位置识别(VPR)已经从手工设计的特征描述符发展到深度学习方法，但仍面临重大挑战。当前方法，包括视觉基础模型(VFMs)和多模态大语言模型(MLLMs)，增强了语义理解，但在微调时存在高计算开销和有限的跨域迁移性。为解决这些限制，我们提出了一种利用基于引导的方法进行直接相似性评分的新型零样本框架，通过测试时缩放(TTS)利用MLLMs的视觉-语言对齐能力。我们的方法采用生成长度可控的JSON输出的结构化提示，消除了两阶段处理。带有不确定性感知自一致性(UASC)的TTS框架能够在不增加额外训练成本的情况下实现实时适应，在各种环境中实现优越的泛化能力。实验结果表明，跨域VPR性能有显著提高，计算效率增益高达210倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Place Recognition (VPR) has evolved from handcrafted descriptors todeep learning approaches, yet significant challenges remain. Currentapproaches, including Vision Foundation Models (VFMs) and Multimodal LargeLanguage Models (MLLMs), enhance semantic understanding but suffer from highcomputational overhead and limited cross-domain transferability whenfine-tuned. To address these limitations, we propose a novel zero-shotframework employing Test-Time Scaling (TTS) that leverages MLLMs'vision-language alignment capabilities through Guidance-based methods fordirect similarity scoring. Our approach eliminates two-stage processing byemploying structured prompts that generate length-controllable JSON outputs.The TTS framework with Uncertainty-Aware Self-Consistency (UASC) enablesreal-time adaptation without additional training costs, achieving superiorgeneralization across diverse environments. Experimental results demonstratesignificant improvements in cross-domain VPR performance with up to 210$\times$computational efficiency gains.</description>
      <author>example@mail.com (Jintao Cheng, Weibin Li, Jiehao Luo, Xiaoyu Tang, Zhijian He, Jin Wu, Yao Zou, Wei Zhang)</author>
      <guid isPermaLink="false">2509.02129v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>How Instruction-Tuning Imparts Length Control: A Cross-Lingual Mechanistic Analysis</title>
      <link>http://arxiv.org/abs/2509.02075v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了大型语言模型在精确控制文本长度方面的挑战，比较了基础模型和指令微调模型在英语和意大利语长度控制文本生成上的差异，分析了性能和内部组件的贡献机制。&lt;h4&gt;背景&lt;/h4&gt;遵循明确的长度约束（如生成精确词数的文本）对于大型语言模型来说仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型及其指令微调对应版本在英语和意大利语长度控制文本生成方面的差异。&lt;h4&gt;方法&lt;/h4&gt;使用累积加权归因（一种从直接逻辑归因派生的指标）来分析性能和内部组件的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;指令微调显著改善了长度控制能力，主要通过专门化更深层次模型的组件来实现；在英语中，指令微调模型后层的注意力头显示越来越积极的贡献；在意大利语中，虽然注意力贡献较为减弱，但最后一层的多层感知器表现出更强的积极作用，表明存在补偿机制。&lt;h4&gt;结论&lt;/h4&gt;指令微调重新配置了模型的后期层以适应任务需求，组件级别的策略可能会根据语言上下文进行调整。&lt;h4&gt;翻译&lt;/h4&gt;遵循明确的长度约束，如生成具有精确词数的文本，对于大型语言模型来说仍然是一个重大挑战。本研究旨在调查基础模型及其指令微调对应版本在英语和意大利语长度控制文本生成方面的差异。我们使用累积加权归因（一种从直接逻辑归因派生的指标）来分析性能和内部组件的贡献。我们的发现表明，指令微调显著改善了长度控制能力，主要通过专门化更深层次模型的组件来实现。具体而言，指令微调模型后层的注意力头显示出越来越积极的贡献，特别是在英语中。在意大利语中，虽然注意力贡献较为减弱，但最后一层的多层感知器表现出更强的积极作用，这表明存在补偿机制。这些结果表明指令微调重新配置了后期层以适应任务需求，组件级别的策略可能会根据语言上下文进行调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Adhering to explicit length constraints, such as generating text with aprecise word count, remains a significant challenge for Large Language Models(LLMs). This study aims at investigating the differences between foundationmodels and their instruction-tuned counterparts, on length-controlled textgeneration in English and Italian. We analyze both performance and internalcomponent contributions using Cumulative Weighted Attribution, a metric derivedfrom Direct Logit Attribution. Our findings reveal that instruction-tuningsubstantially improves length control, primarily by specializing components indeeper model layers. Specifically, attention heads in later layers of IT modelsshow increasingly positive contributions, particularly in English. In Italian,while attention contributions are more attenuated, final-layer MLPs exhibit astronger positive role, suggesting a compensatory mechanism. These resultsindicate that instruction-tuning reconfigures later layers for task adherence,with component-level strategies potentially adapting to linguistic context.</description>
      <author>example@mail.com (Elisabetta Rocchetti, Alfio Ferrara)</author>
      <guid isPermaLink="false">2509.02075v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>MUSE-FM: Multi-task Environment-aware Foundation Model for Wireless Communications</title>
      <link>http://arxiv.org/abs/2509.01967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MUSE-FM的多任务环境感知基础模型，采用统一架构处理无线通信中的多个任务，同时有效整合场景信息，解决了现有无线基础模型在处理多样化输入/输出任务时的统一性局限性。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)在无线通信领域受到越来越多的关注，因为它们具有强大的多任务学习能力，有望通过单一框架统一无线通信的多个任务。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为MUSE-FM的多任务环境感知基础模型，采用统一架构处理无线通信中的多个任务，同时有效整合场景信息。&lt;h4&gt;方法&lt;/h4&gt;提出统一的提示引导数据编码器-解码器对处理不同任务中具有异构格式和分布的数据；将环境上下文作为多模态输入集成，作为环境和信道分布的先验知识，促进跨场景特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;MUSE-FM在各种任务上表现优于现有方法；提示引导的编码器-解码器对提高了新任务配置的可扩展性；环境信息的整合提高了适应不同场景的能力。&lt;h4&gt;结论&lt;/h4&gt;MUSE-FM通过统一架构和环境感知能力，有效解决了无线通信中多任务处理的统一性问题，提高了模型的可扩展性和场景适应性。&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)的最新进展在无线通信领域吸引了越来越多的关注。利用强大的多任务学习能力，FMs有望通过单一框架统一无线通信的多个任务。然而，现有的无线基础模型在处理不同通信场景中具有多样化输入/输出的多个任务时，存在统一性方面的局限性。在本文中，我们提出了一个名为MUSE-FM的多任务环境感知基础模型，采用统一架构处理无线通信中的多个任务，同时有效整合场景信息。具体而言，为实现任务统一性，我们提出了统一的提示引导数据编码器-解码器对，处理不同任务中具有异构格式和分布的数据。此外，我们将环境上下文作为多模态输入集成，作为环境和信道分布的先验知识，促进跨场景特征提取。仿真结果表明，所提出的MUSE-FM在各种任务上表现优于现有方法，其提示引导的编码器-解码器对提高了新任务配置的可扩展性。此外，环境信息的整合提高了适应不同场景的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in foundation models (FMs) have attracted increasingattention in the wireless communication domain. Leveraging the powerfulmulti-task learning capability, FMs hold the promise of unifying multiple tasksof wireless communication with a single framework. with a single framework.Nevertheless, existing wireless FMs face limitations in the uniformity toaddress multiple tasks with diverse inputs/outputs across differentcommunication scenarios.In this paper, we propose a MUlti-taSkEnvironment-aware FM (MUSE-FM) with a unified architecture to handle multipletasks in wireless communications, while effectively incorporating scenarioinformation.Specifically, to achieve task uniformity, we propose a unifiedprompt-guided data encoder-decoder pair to handle data with heterogeneousformats and distributions across different tasks. Besides, we integrate theenvironmental context as a multi-modal input, which serves as prior knowledgeof environment and channel distributions and facilitates cross-scenario featureextraction. Simulation results illustrate that the proposed MUSE-FM outperformsexisting methods for various tasks, and its prompt-guided encoder-decoder pairimproves the scalability for new task configurations. Moreover, theincorporation of environment information improves the ability to adapt todifferent scenarios.</description>
      <author>example@mail.com (Tianyue Zheng, Jiajia Guo, Linglong Dai, Shi Jin, Jun Zhang)</author>
      <guid isPermaLink="false">2509.01967v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Federated Foundation Models in Harsh Wireless Environments: Prospects, Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2509.01957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is under review in IEEE Network Magazine Special Issue on  Large AI Models for the Internet of Everything&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型(FMs)在通用智能、多模态理解和自适应学习方面展现出强大能力，但在恶劣环境中的部署仍面临挑战&lt;h4&gt;背景&lt;/h4&gt;现有分布式学习方法如联邦学习(FL)在基础设施不稳定、需要同步更新和训练资源密集的环境中难以适应，恶劣环境特点包括间歇性连接、有限计算能力、噪声数据和动态变化的网络拓扑&lt;h4&gt;目的&lt;/h4&gt;探索联邦基础模型(FFMs)作为解决这些局限性的范式，通过结合FMs的可扩展性和泛化能力与新颖的去中心化、通信感知的联邦学习框架，实现极端和对抗条件下的稳健、节能和自适应智能&lt;h4&gt;方法&lt;/h4&gt;将基础模型的可扩展性和泛化能力与新颖的、去中心化、通信感知的联邦学习框架相结合&lt;h4&gt;主要发现&lt;/h4&gt;详细介绍了恶劣环境中的系统级约束，并讨论了通信设计、模型鲁棒性和节能个性化方面的开放研究挑战&lt;h4&gt;结论&lt;/h4&gt;联邦基础模型(FFMs)有望解决基础模型在恶劣环境中部署的挑战，实现稳健、节能和自适应的智能&lt;h4&gt;翻译&lt;/h4&gt;基础模型(FMs)在通用智能、多模态理解和自适应学习方面展现出强大能力，但在恶劣环境中的部署仍面临挑战。现有分布式学习方法如联邦学习(FL)在基础设施不稳定、需要同步更新和训练资源密集的环境中难以适应。本文探索联邦基础模型(FFMs)作为解决这些局限性的范式，通过结合FMs的可扩展性和泛化能力与新颖的去中心化、通信感知的联邦学习框架，旨在实现极端和对抗条件下的稳健、节能和自适应智能。我们详细介绍了恶劣环境中的系统级约束，并讨论了通信设计、模型鲁棒性和节能个性化方面的开放研究挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have shown remarkable capabilities in generalizedintelligence, multimodal understanding, and adaptive learning across a widerange of domains. However, their deployment in harsh or austere environments --characterized by intermittent connectivity, limited computation, noisy data,and dynamically changing network topologies -- remains an open challenge.Existing distributed learning methods such as federated learning (FL) struggleto adapt in such settings due to their reliance on stable infrastructure,synchronized updates, and resource-intensive training. In this work, we explorethe potential of Federated Foundation Models (FFMs) as a promising paradigm toaddress these limitations. By integrating the scalability and generalizationpower of FMs with novel decentralized, communication-aware FL frameworks, weaim to enable robust, energy-efficient, and adaptive intelligence in extremeand adversarial conditions. We present a detailed breakdown of system-levelconstraints in harsh environments, and discuss the open research challenges incommunication design, model robustness, and energy-efficient personalizationfor these unique settings.</description>
      <author>example@mail.com (Evan Chen, Seyyedali Hosseinalipour, Christopher G. Brinton, David J. Love)</author>
      <guid isPermaLink="false">2509.01957v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Learning Strategies for Mitotic Figure Classification in MIDOG2025 Challenge</title>
      <link>http://arxiv.org/abs/2509.02640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了三种适应病理学基础模型UNI2-h的变体，用于非典型有丝分裂(AMFs)的分类检测，通过结合视觉提示调整和染色标准化测试时增强，实现了高准确率和高鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;非典型有丝分裂图(AMFs)是异常细胞分裂的临床相关指标，但由于形态学模糊性和扫描仪变异性，它们的可靠检测仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究三种适应病理学基础模型UNI2-h的变体，用于MIDOG2025轨道2挑战中的非典型有丝分裂分类任务。&lt;h4&gt;方法&lt;/h4&gt;从基于LoRA的基线开始，采用视觉提示调整(VPT)提高泛化能力，并结合测试时增强(TTA)与Vahadane和Macenko染色标准化以增强模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;最终提交在预排行榜上实现了0.8837的平衡准确率和0.9513的ROC-AUC，排名在前10名团队中，表明该方法在不同成像条件下表现优异。&lt;h4&gt;结论&lt;/h4&gt;基于提示的适应结合染色标准化测试时增强，为在不同成像条件下进行非典型有丝分裂分类提供了一种有效策略。&lt;h4&gt;翻译&lt;/h4&gt;非典型有丝分裂图(AMFs)是异常细胞分裂的临床相关指标，但由于形态学模糊性和扫描仪变异性，它们的可靠检测仍然具有挑战性。在本工作中，我们研究了三种适应病理学基础模型UNI2-h的变体，用于MIDOG2025轨道2挑战。从基于LoRA的基线开始，我们发现视觉提示调整(VPT)显著提高了泛化能力，并且进一步将测试时增强(TTA)与Vahadane和Macenko染色标准化相结合提供了最佳鲁棒性。我们的最终提交在预排行榜上实现了0.8837的平衡准确率和0.9513的ROC-AUC，排名在前10名团队中。这些结果表明，基于提示的适应结合染色标准化TTA，为在不同成像条件下进行非典型有丝分裂分类提供了一种有效策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atypical mitotic figures (AMFs) are clinically relevant indicators ofabnormal cell division, yet their reliable detection remains challenging due tomorphological ambiguity and scanner variability. In this work, we investigatedthree variants of adapting the pathology foundation model UNI2-h for theMIDOG2025 Track 2 challenge. Starting from a LoRA-based baseline, we found thatvisual prompt tuning (VPT) substantially improved generalization, and thatfurther integrating test-time augmentation (TTA) with Vahadane and Macenkostain normalization provided the best robustness. Our final submission achieveda balanced accuracy of 0.8837 and an ROC-AUC of 0.9513 on the preliminaryleaderboard, ranking within the top 10 teams. These results demonstrate thatprompt-based adaptation combined with stain-normalization TTA offers aneffective strategy for atypical mitosis classification under diverse imagingconditions.</description>
      <author>example@mail.com (Biwen Meng, Xi Long, Jingxin Liu)</author>
      <guid isPermaLink="false">2509.02640v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Constrained Decoding for Robotics Foundation Models</title>
      <link>http://arxiv.org/abs/2509.01728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于机器人基础模型的约束解码框架，该框架能够在动态系统中对动作轨迹施加逻辑约束，确保生成的动作在运行时满足信号时序逻辑规范，无需重新训练模型。&lt;h4&gt;背景&lt;/h4&gt;机器人基础模型的最新发展为机器人系统提供了有前途的端到端和通用能力。这些模型在海量的机器人轨迹数据上进行预训练，能够处理多模态输入并直接输出一系列动作，系统随后在现实世界中执行。然而，这些模型仍然是数据驱动的，缺乏对行为正确性和安全约束的明确概念。&lt;h4&gt;目的&lt;/h4&gt;解决现有机器人基础模型缺乏行为正确性和安全约束的问题，确保生成的动作满足安全规范。&lt;h4&gt;方法&lt;/h4&gt;引入了一种用于机器人基础模型的约束解码框架，在动态系统中对动作轨迹施加逻辑约束。该方法确保生成的动作在运行时满足信号时序逻辑规范，无需重新训练，且对底层基础模型保持独立性。&lt;h4&gt;主要发现&lt;/h4&gt;在最新的导航基础模型上对其方法进行了全面评估，表明解码时的干预不仅可用于过滤不安全的动作，还可用于条件动作生成。&lt;h4&gt;结论&lt;/h4&gt;提出的约束解码框架能够有效解决机器人基础模型中缺乏行为正确性和安全约束的问题，为机器人系统的安全操作提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;机器人基础模型开发的最新进展使机器人系统在端到端和通用能力方面展现出广阔前景。这些模型在海量的机器人轨迹数据上进行预训练，能够处理多模态输入并直接输出一系列动作，系统随后在现实世界中执行这些动作。尽管这种方法从提高跨任务泛化能力的角度来看很有吸引力，但这些模型仍然是数据驱动的，因此缺乏对行为正确性和安全约束的明确概念。我们通过引入一种用于机器人基础模型的约束解码框架来解决这些局限性，该框架在动态系统中对动作轨迹施加逻辑约束。我们的方法确保生成的动作在运行时能够满足信号时序逻辑规范，无需重新训练，并且对底层基础模型保持独立性。我们在最先进的导航基础模型上对其方法进行了全面评估，表明解码时的干预不仅可用于过滤不安全的动作，还可用于条件动作生成。视频可在我们的网站查看：https://constrained-robot-fms.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in the development of robotic foundation models have led topromising end-to-end and general-purpose capabilities in robotic systems. Thesemodels are pretrained on vast datasets of robot trajectories to process multi-modal inputs and directly output a sequence of action that the system thenexecutes in the real world. Although this approach is attractive from theperspective of im- proved generalization across diverse tasks, these models arestill data-driven and, therefore, lack explicit notions of behavioralcorrectness and safety constraints. We address these limitations by introducinga constrained decoding framework for robotics foundation models that enforceslogical constraints on action trajec- tories in dynamical systems. Our methodensures that generated actions provably satisfy signal temporal logic (STL)specifications at runtime without retraining, while remaining agnostic of theunderlying foundation model. We perform com- prehensive evaluation of ourapproach across state-of-the-art navigation founda- tion models and we showthat our decoding-time interventions are useful not only for filtering unsafeactions but also for conditional action-generation. Videos available on ourwebsite: https://constrained-robot-fms.github.io</description>
      <author>example@mail.com (Parv Kapoor, Akila Ganlath, Changliu Liu, Sebastian Scherer, Eunsuk Kang)</author>
      <guid isPermaLink="false">2509.01728v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Succeed or Learn Slowly: Sample Efficient Off-Policy Reinforcement Learning for Mobile App Control</title>
      <link>http://arxiv.org/abs/2509.01720v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Succeed or Learn Slowly (SoLS)的新型离策略强化学习算法，通过区分正负样本的更新策略，提高了基础模型在多轮任务中的学习效率，并在AndroidWorld基准上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;使用基础模型进行强化学习在多轮任务中仍然面临挑战，主要受限于稀疏奖励设置和政策梯度更新问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效利用样本的强化学习算法，解决基础模型在多轮任务中的政策近似问题，特别是针对用户界面导航任务。&lt;h4&gt;方法&lt;/h4&gt;提出SoLS算法，采用改进的离策略演员-评论家方法，对高回报的正样本应用直接政策更新，对负样本应用保守的正则化更新；并通过Successful Transition Replay (STR)优先从成功交互中学习。&lt;h4&gt;主要发现&lt;/h4&gt;高回报的正样本通常不需要政策正则化，而反映不良行为的负样本可能会损害模型性能；SoLS在AndroidWorld基准上显著优于现有方法（相对提高至少17%），同时计算效率更高。&lt;h4&gt;结论&lt;/h4&gt;SoLS通过区分对待正负样本的更新策略，有效提高了基础模型在多轮任务中的学习效率和性能，为用户界面导航等任务提供了高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;使用基础模型进行强化学习在多轮任务中的政策近似仍然具有挑战性。我们确定了与稀疏奖励设置和政策梯度更新相关的两个主要局限性，基于此我们提出了一个关键见解：来自高回报正样本的更新通常不需要政策正则化，而来自反映不良行为的负样本的更新可能会损害模型性能。本文介绍了Succeed or Learn Slowly (SoLS)，一种在移动应用控制任务上评估的新型离策略强化学习算法。SoLS通过改进的离策略演员-评论家方法，在微调基础模型以进行用户界面导航时提高了样本效率，对正样本应用直接政策更新，对负样本应用保守的正则化更新以防止模型退化。我们通过Successful Transition Replay (STR)增强SoLS，优先从成功的交互中学习，进一步提高样本效率。我们在AndroidWorld基准上评估SoLS，它显著优于现有方法（相对提高至少17%），包括提示工程和强化学习方法，同时比基于GPT-4o的方法需要少得多的计算资源，推理速度快5-60倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) using foundation models for policy approximationsin multi-turn tasks remains challenging. We identify two main limitationsrelated to sparse reward settings and policy gradient updates, based on whichwe formulate a key insight: updates from positive samples with high returnstypically do not require policy regularisation, whereas updates from negativesamples, reflecting undesirable behaviour, can harm model performance. Thispaper introduces Succeed or Learn Slowly (SoLS), a novel off-policy RLalgorithm evaluated on mobile app control tasks. SoLS improves sampleefficiency when fine-tuning foundation models for user interface navigation viaa modified off-policy actor-critic approach, applying direct policy updates forpositive samples and conservative, regularised updates for negative ones toprevent model degradation. We augment SoLS with Successful Transition Replay(STR), which prioritises learning from successful interactions, furtherimproving sample efficiency. We evaluate SoLS on the AndroidWorld benchmark,where it significantly outperforms existing methods (at least 17% relativeincrease), including prompt-engineering and RL approaches, while requiringsubstantially fewer computational resources than GPT-4o-based methods with5-60x faster inference.</description>
      <author>example@mail.com (Georgios Papoudakis, Thomas Coste, Jianye Hao, Jun Wang, Kun Shao)</author>
      <guid isPermaLink="false">2509.01720v1</guid>
      <pubDate>Thu, 04 Sep 2025 17:07:01 +0800</pubDate>
    </item>
    <item>
      <title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vector Drawings</title>
      <link>http://arxiv.org/abs/2508.18733v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Drawing2CAD框架，用于从2D工程图纸自动生成参数化CAD模型，解决了传统CAD生成方法与传统工业工作流程不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;CAD生成式建模正在推动工业应用的显著创新，最近的研究已展示从点云、网格和文本描述创建实体模型的进展，但这些方法与传统从2D工程图纸开始的工业工作流程存在根本差异。&lt;h4&gt;目的&lt;/h4&gt;解决从2D矢量图纸自动生成参数化CAD模型的这一未被充分探索的领域问题，尽管这是工程设计中的关键步骤。&lt;h4&gt;方法&lt;/h4&gt;提出Drawing2CAD框架，包含三个关键技术组件：网络友好的矢量原语表示、双解码器Transformer架构、以及适应CAD参数固有灵活性的软目标分布损失函数。将CAD生成重新定义为序列到序列的学习问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过创建CAD-VGDrawing数据集（包含成对的工程图纸和参数化CAD模型）并进行彻底实验，证明了Drawing2CAD方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;Drawing2CAD框架能够有效从2D工程图纸生成参数化CAD模型，保留了几何精度和设计意图。&lt;h4&gt;翻译&lt;/h4&gt;计算机辅助设计生成式建模正在推动工业应用的显著创新。最近的工作在从点云、网格和文本描述等各种输入创建实体模型方面显示出显著进展。然而，这些方法从根本上与传统从2D工程图纸开始的工业工作流程相偏离。尽管工程设计中的关键步骤，但从这些2D矢量图纸自动生成参数化CAD模型仍未得到充分探索。为解决这一差距，我们的关键见解是将CAD生成重新定义为序列到序列学习问题，其中矢量绘图原语直接指导参数化CAD操作的生成，在整个转换过程中保持几何精度和设计意图。我们提出了Drawing2CAD框架，包含三个关键技术组件：保留精确几何信息的网络友好型矢量原语表示，解耦命令类型和参数生成同时保持精确对应的双解码器Transformer架构，以及适应CAD参数固有灵活性的软目标分布损失函数。为了训练和评估Drawing2CAD，我们创建了CAD-VGDrawing数据集，包含成对的工程图纸和参数化CAD模型，并进行了彻底的实验以证明我们方法的有效性。代码和数据集可在https://github.com/lllssc/Drawing2CAD获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从2D矢量工程图纸自动生成参数化CAD模型的问题。这个问题在现实和研究中很重要，因为工业设计工作流程通常从2D工程图纸开始，然后创建3D CAD模型，这一过程目前需要大量手动操作，耗时且需要专业知识。现有研究主要探索从点云、网格、文本描述等生成CAD模型，但忽略了从2D矢量工程图纸这一工业设计起点直接生成CAD模型的方法。自动化这一过程可以显著提高设计效率，减少人工干预，使设计工作流程更加流畅。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将CAD模型生成问题重新定义为序列到序列的学习问题，其中矢量绘图的基本元素直接指导参数化CAD操作的生成。他们选择使用矢量图形（SVG）而非光栅图像作为输入，因为SVG能精确编码几何信息，与工程设计意图保持一致。方法设计包括：网络友好的矢量表示、双解码器Transformer架构解耦命令类型和参数生成、软目标分布损失函数适应CAD参数的灵活性。作者借鉴了Transformer架构在序列建模中的成功应用、DeepSVG等在矢量图形生成方面的研究、DeepCAD等在CAD操作序列生成方面的方法，以及计算机视觉中多视图表示的学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将CAD模型生成视为序列到序列的转换问题，直接从2D矢量工程图纸生成参数化CAD操作序列，同时保持几何精度和设计意图。整体实现流程包括：1) 输入表示：将矢量工程图纸转换为网络友好的表示，保留精确几何信息；2) 编码阶段：使用Transformer编码器处理嵌入的矢量图纸表示，生成潜在向量；3) 双解码阶段：命令解码器生成CAD操作类型，参数解码器生成对应参数，通过命令引导确保参数与命令语义一致；4) 损失函数：使用复合损失函数，参数损失采用软目标分布允许微小变化；5) 输出处理：将生成的CAD操作序列通过CAD内核处理，构建最终3D模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 网络友好的矢量工程图纸表示方法，保留精确几何信息；2) 序列到序列学习框架，重新定义CAD生成为跨模态问题；3) 双解码器架构，解耦命令类型预测和参数估计；4) 软目标分布损失函数，适应CAD参数的灵活性；5) CAD-VGDrawing数据集，提供大规模基准数据。相比之前工作的不同：使用矢量图纸而非光栅图像作为输入；实现从2D到3D的直接转换而非通过中间表示；双解码器架构解耦命令和参数生成；软目标分布损失函数；更贴近工业设计工作流程的应用场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Drawing2CAD首次提出了从2D矢量工程图纸直接生成参数化CAD模型的序列到序列学习框架，通过创新的网络友好表示、双解码器架构和软目标分布损失函数，实现了高效且保持设计意图的自动化CAD建模。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-Aided Design (CAD) generative modeling is driving significantinnovations across industrial applications. Recent works have shown remarkableprogress in creating solid models from various inputs such as point clouds,meshes, and text descriptions. However, these methods fundamentally divergefrom traditional industrial workflows that begin with 2D engineering drawings.The automatic generation of parametric CAD models from these 2D vector drawingsremains underexplored despite being a critical step in engineering design. Toaddress this gap, our key insight is to reframe CAD generation as asequence-to-sequence learning problem where vector drawing primitives directlyinform the generation of parametric CAD operations, preserving geometricprecision and design intent throughout the transformation process. We proposeDrawing2CAD, a framework with three key technical components: anetwork-friendly vector primitive representation that preserves precisegeometric information, a dual-decoder transformer architecture that decouplescommand type and parameter generation while maintaining precise correspondence,and a soft target distribution loss function accommodating inherent flexibilityin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,a dataset of paired engineering drawings and parametric CAD models, and conductthorough experiments to demonstrate the effectiveness of our method. Code anddataset are available at https://github.com/lllssc/Drawing2CAD.</description>
      <author>example@mail.com (Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu)</author>
      <guid isPermaLink="false">2508.18733v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
  <item>
      <title>Investigating Domain Gaps for Indoor 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.17439v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了室内3D物体检测的域适应问题，提出了一个全面的基准测试，分析了不同域差距对检测器的影响，并提供了提高适应性能的方法。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测作为室内场景理解的基础任务已有广泛研究，在室内点云数据上的准确性显著提高。然而，现有研究仅在有限数据集上进行，训练和测试集共享相同分布。&lt;h4&gt;目的&lt;/h4&gt;考虑将室内3D物体检测器从一个数据集适应到另一个数据集的任务，提供域自适应室内3D物体检测的基线。&lt;h4&gt;方法&lt;/h4&gt;提出了一个全面的基准测试，使用ScanNet、SUN RGB-D和3D Front数据集，以及通过3D模拟器生成的新的大规模数据集ProcTHOR-OD和ProcFront。分析了不同域差距对3D物体检测器的影响，并提出了几种提高适应性能的方法。&lt;h4&gt;主要发现&lt;/h4&gt;由于室内点云数据集以不同方式收集和构建，物体检测器可能过度拟合到每个数据集内的特定因素，如点云质量、边界框布局和实例特征。&lt;h4&gt;结论&lt;/h4&gt;提供了域自适应室内3D物体检测的基线，希望未来的工作能提出跨域泛化能力更强的检测器。&lt;h4&gt;翻译&lt;/h4&gt;作为室内场景理解的基本任务，3D物体检测已被广泛研究，并且在室内点云数据上的准确性已显著提高。然而，现有研究已在有限数据集上进行，其中训练和测试集共享相同的分布。在本文中，我们考虑将室内3D物体检测器从一个数据集适应到另一个数据集的任务，提出了一个包含ScanNet、SUN RGB-D和3D Front数据集的综合基准，以及我们通过3D模拟器新提出的大规模数据集ProcTHOR-OD和ProcFront。由于室内点云数据集以不同方式收集和构建，物体检测器可能过度拟合到每个数据集内的特定因素，如点云质量、边界框布局和实例特征。我们在不同适应场景（包括合成到真实适应、点云质量适应、布局适应和实例特征适应）中进行了跨数据集实验，分析了不同域差距对3D物体检测器的影响。我们还引入了多种方法来提高适应性能，为域自适应室内3D物体检测提供了基线，希望未来的工作能提出跨域泛化能力更强的检测器。我们的项目主页可以在https://jeremyzhao1998.github.io/DAVoteNet-release/找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决室内3D物体检测中的域差距问题。现有研究大多在单一数据集上进行，训练和测试数据分布相同，但实际应用中检测器需要适应不同环境。这个问题很重要，因为不同数据集有不同采集方式和质量，导致检测器跨域性能显著下降（论文显示mAP从46.3降至18.9），且室内3D检测面临物体类别多、间距近等独特挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了室内3D检测中多种域差距因素（合成到现实、点云质量、布局、实例），然后创建两个新数据集（ProcTHOR-OD和ProcFront）来隔离这些因素。他们结合现有数据集构建了全面的基准测试套件，并实现了多种域适应方法作为基线。作者借鉴了ProcTHOR框架用于数据生成，使用VoteNet作为基础检测器，并应用了已有的域适应技术如虚拟扫描模拟、平均教师等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是系统性地分析和量化室内3D物体检测中的不同域差距因素，创建可控的大规模合成数据集，建立全面的基准测试套件，并提供多种域适应方法的基线。整体流程包括：1)构建数据集（现有+新创建）；2)分析域差距因素；3)设置实验环境（使用VoteNet，每场景4万点，训练90周期）；4)评估不同域适应方法（包括目标域先验和无监督域适应技术）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建两个新数据集（ProcTHOR-OD和ProcFront），规模比现有数据集大一个数量级；2)构建覆盖四种域适应场景的全面基准测试套件；3)系统性地分析多种域差距因素；4)提供多种域适应方法的基线结果。相比之前工作，本文考虑了更广泛的域差距因素（而不仅是合成到现实），创建了更大规模、更可控的数据集，提供了更全面的基准测试和更系统的分析，并关注室内3D检测特有的挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建新数据集和构建全面的基准测试套件，首次系统性地分析了室内3D物体检测中的域差距因素，并提供了多种域适应方法的基线结果，为未来提升检测器跨域能力奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758275&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a fundamental task for indoor scene understanding, 3D object detection hasbeen extensively studied, and the accuracy on indoor point cloud data has beensubstantially improved. However, existing researches have been conducted onlimited datasets, where the training and testing sets share the samedistribution. In this paper, we consider the task of adapting indoor 3D objectdetectors from one dataset to another, presenting a comprehensive benchmarkwith ScanNet, SUN RGB-D and 3D Front datasets, as well as our newly proposedlarge-scale datasets ProcTHOR-OD and ProcFront generated by a 3D simulator.Since indoor point cloud datasets are collected and constructed in differentways, the object detectors are likely to overfit to specific factors withineach dataset, such as point cloud quality, bounding box layout and instancefeatures. We conduct experiments across datasets on different adaptationscenarios including synthetic-to-real adaptation, point cloud qualityadaptation, layout adaptation and instance feature adaptation, analyzing theimpact of different domain gaps on 3D object detectors. We also introduceseveral approaches to improve adaptation performances, providing baselines fordomain adaptive indoor 3D object detection, hoping that future works maypropose detectors with stronger generalization ability across domains. Ourproject homepage can be found inhttps://jeremyzhao1998.github.io/DAVoteNet-release/.</description>
      <author>example@mail.com (Zijing Zhao, Zhu Xu, Qingchao Chen, Yuxin Peng, Yang Liu)</author>
      <guid isPermaLink="false">2508.17439v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Large VLM-based Vision-Language-Action Models for Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2508.13073v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page:  https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了首个系统化的、以分类为导向的综述，关于用于机器人操作的大型视觉语言模型(VLM)基础的视觉-语言-动作(VLA)模型。文章定义了这些模型，区分了两种主要架构范式(单体模型和分层模型)，并深入研究了它们与先进领域的集成、独特特征及未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;机器人操作是机器人和具身AI的关键前沿领域，需要精确的电机控制和多模态理解。然而，传统基于规则的方法在非结构化、新环境中无法扩展或泛化。&lt;h4&gt;目的&lt;/h4&gt;提供首个系统化的分类导向综述，解决现有分类中的不一致性，减轻研究碎片化，通过系统整合大型VLMs与机器人操作交叉领域的研究，填补关键空白。&lt;h4&gt;方法&lt;/h4&gt;定义大型VLM-based VLA模型，区分两种主要架构范式(单体模型和分层模型)，并深入研究它们与先进领域的集成、独特特征及未来发展方向。单体模型包括单系统和双系统设计，分层模型通过可解释的中间表示明确解耦规划与执行。&lt;h4&gt;主要发现&lt;/h4&gt;VLA模型建立在预训练在大量图像文本数据集上的大型VLMs之上，已成为变革性范式。存在单体模型和分层模型两种架构范式。与强化学习、无需训练优化、从人类视频学习和世界模型集成等先进领域的集成提供了新方向。记忆机制、4D感知、高效适应和多智能体合作是有前景的发展方向。&lt;h4&gt;结论&lt;/h4&gt;这篇综述整合了最近的进展，为大型VLM-based VLA模型在机器人操作中的应用提供了系统化视角，并通过定期更新的项目页面记录持续进展。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作，作为机器人和具身AI的关键前沿，需要精确的电机控制和多模态理解，然而传统的基于规则的方法在非结构化、新环境中无法扩展或泛化。近年来，建立在预训练在大量图像文本数据集上的大型视觉语言模型(VLMs)基础上的视觉-语言-动作(VLA)模型，已成为一种变革性范式。这篇综述提供了首个系统化的、以分类为导向的回顾，关于用于机器人操作的大型VLM基础的VLA模型。我们首先明确定义了大型VLM基础的VLA模型，并勾勒出两种主要的架构范式：(1)单体模型，包括具有不同集成程度的单系统和双系统设计；(2)分层模型，通过可解释的中间表示明确解耦规划与执行。在此基础上，我们深入研究了大型VLM基础的VLA模型：(1)与先进领域的集成，包括强化学习、无需训练的优化、从人类视频中学习以及世界模型集成；(2)综合独特特征，整合架构特征、操作优势以及支持其开发的数据集和基准；(3)确定有前景的方向，包括记忆机制、4D感知、高效适应、多智能体合作及其他新兴能力。这篇综述整合了最近的进展，以解决现有分类中的不一致性，减轻研究碎片化，并通过系统整合大型VLMs与机器人操作交叉领域的研究，填补关键空白。我们提供了一个定期更新的项目页面来记录持续进展：https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation, a key frontier in robotics and embodied AI, requiresprecise motor control and multimodal understanding, yet traditional rule-basedmethods fail to scale or generalize in unstructured, novel environments. Inrecent years, Vision-Language-Action (VLA) models, built upon LargeVision-Language Models (VLMs) pretrained on vast image-text datasets, haveemerged as a transformative paradigm. This survey provides the firstsystematic, taxonomy-oriented review of large VLM-based VLA models for roboticmanipulation. We begin by clearly defining large VLM-based VLA models anddelineating two principal architectural paradigms: (1) monolithic models,encompassing single-system and dual-system designs with differing levels ofintegration; and (2) hierarchical models, which explicitly decouple planningfrom execution via interpretable intermediate representations. Building on thisfoundation, we present an in-depth examination of large VLM-based VLA models:(1) integration with advanced domains, including reinforcement learning,training-free optimization, learning from human videos, and world modelintegration; (2) synthesis of distinctive characteristics, consolidatingarchitectural traits, operational strengths, and the datasets and benchmarksthat support their development; (3) identification of promising directions,including memory mechanisms, 4D perception, efficient adaptation, multi-agentcooperation, and other emerging capabilities. This survey consolidates recentadvances to resolve inconsistencies in existing taxonomies, mitigate researchfragmentation, and fill a critical gap through the systematic integration ofstudies at the intersection of large VLMs and robotic manipulation. We providea regularly updated project page to document ongoing progress:https://github.com/JiuTian-VL/Large-VLM-based-VLA-for-Robotic-Manipulation</description>
      <author>example@mail.com (Rui Shao, Wei Li, Lingsen Zhang, Renshan Zhang, Zhiyang Liu, Ran Chen, Liqiang Nie)</author>
      <guid isPermaLink="false">2508.13073v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Constrained Random Phase Approximation: the spectral method</title>
      <link>http://arxiv.org/abs/2508.15368v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures, to be published in Physical Review B, Poster  presented at PSI-K, Lausanne (August 2025). Talk given at Workshop "The  determination of Hubbard parameters: progress, pitfalls, and prospects",  Gandia (September 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的受限制随机相位近似方法（谱cRPA），并将其与现有cRPA方法进行比较。s-cRPA在多种系统中表现更优，能够更准确地预测相互作用参数，克服了标准cRPA中U值低估的问题，并提供了更好的数值稳定性。此外，研究还增强了方法的实现，添加了多中心相互作用计算和低缩放变体。&lt;h4&gt;背景&lt;/h4&gt;在计算材料科学中，准确描述电子相互作用对于预测材料性质至关重要。标准cRPA方法在计算Hubbard U相互作用时存在低估问题，特别是在处理某些电子壳层时可能产生负值，影响预测准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种更稳健、准确的cRPA变体（s-cRPA），以克服标准cRPA中U值低估的问题，提高对材料电子结构的预测能力，特别是在诱导绝缘态方面的表现。&lt;h4&gt;方法&lt;/h4&gt;提出并实现了一种新的谱cRPA（s-cRPA）方法，将其应用于钪和铜系统（通过改变3d壳层填充），并在CaFeO₃实际系统中进行测试。还通过电子转换解决了投影cRPA中的负相互作用值问题，并增强了实现，添加了多中心相互作用计算功能和具有压缩Matsubara网格的低缩放变体。&lt;h4&gt;主要发现&lt;/h4&gt;1. s-cRPA产生的Hubbard U相互作用值大于标准cRPA方法；2. 在CaFeO₃系统中，s-cRPA产生的相互作用参数更接近于DFT+U方法中所需的参数，能够诱导实验观察到的绝缘态；3. s-cRPA通过电子转换解决了投影cRPA方法中对于填充d壳层出现负相互作用值的问题，提供了更好的数值稳定性；4. 增强的实现能够计算多中心相互作用分析空间衰减，并通过低缩放变体有效获得全频率依赖相互作用。&lt;h4&gt;结论&lt;/h4&gt;s-cRPA是一种更稳健、准确的方法，有效克服了标准cRPA中U值低估的问题，是材料科学研究中一个有前景的工具。增强的实现进一步扩展了其应用范围和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的受限制随机相位近似方法，称为谱cRPA（s-cRPA），并通过改变3d壳层填充，将这种方法与已建立的cRPA方法在钪和铜系统中进行了比较。s-cRPA始终产生更大的Hubbard U相互作用值。应用于实际系统CaFeO₃时，s-cRPA产生的相互作用参数更接近于DFT+U方法中所需的参数，以诱导实验观察到的绝缘态，克服了标准密度泛函预测的金属行为。我们还解决了投影cRPA方法中对于填充d壳层发现负相互作用值的问题，证明s-cRPA通过电子转换提供了更好的数值稳定性。总体而言，s-cRPA更稳健，有效克服了标准cRPA中已知的U值低估问题，使其成为社区有前景的工具。此外，我们通过添加计算多中心相互作用以分析空间衰减的功能增强了实现，并开发了一种具有压缩Matsubara网格的低缩放变体，以有效获得全频率依赖相互作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new constrained Random Phase Approximation (cRPA) method, termedspectral cRPA (s-cRPA), and compare it to established cRPA approaches forScandium and Copper by varying the 3d shell filling. The s-cRPA consistentlyyields larger Hubbard $U$ interaction values. Applied to the realistic systemCaFeO$_3$, s-cRPA produces interaction parameters significantly closer to thoserequired within DFT+$U$ to induce the experimentally observed insulating state,overcoming the metallic behavior predicted by standard density functionals. Wealso address the issue of negative interaction values found in the projectorcRPA method for filled d-shells, demonstrating that s-cRPA offers superiornumerical stability through electron conversion. Overall, s-cRPA is more robustand effectively overcomes the known underestimation of $U$ in standard cRPA,making it a promising tool for the community. Additionally, we have enhancedour implementation with features for computing multi-center interactions toanalyze spatial decay and developed a low-scaling variant with a compressedMatsubara grid to efficiently obtain full frequency-dependent interactions.</description>
      <author>example@mail.com (Merzuk Kaltak, Alexander Hampel, Martin Schlipf, Indukuru Ramesh Reddy, Bongjae Kim, Georg Kresse)</author>
      <guid isPermaLink="false">2508.15368v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title>
      <link>http://arxiv.org/abs/2508.21112v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出EO-Robotics系统，包括EO-1模型和EO-Data1.5M数据集，通过交错视觉-文本-动作预训练实现多模态具身推理和机器人控制的优越性能。&lt;h4&gt;背景&lt;/h4&gt;人类能够在开放世界中无缝执行多模态推理和物理交互，这是通用具身智能系统的核心目标。最近的视觉-语言-动作模型在大规模机器人和视觉-文本数据上联合训练，在通用机器人控制方面取得了显著进展，但仍未达到人类水平的灵活性和交错推理与交互能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的具身基础模型，实现人类水平的灵活性和交错推理与交互能力，在开放世界中实现多模态推理和物理交互。&lt;h4&gt;方法&lt;/h4&gt;提出两个关键支柱：一是统一架构，无差别处理多模态输入（图像、文本、视频和动作）；二是大规模、高质量的多模态具身推理数据集EO-Data1.5M，包含超过150万个样本。通过自回归解码和流匹配去噪的协同训练实现无缝机器人动作生成和多模态具身推理。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明交错视觉-文本-动作学习对开放世界理解和泛化的有效性，通过多种跨多个具身的长期、灵巧操作任务得到验证。EO-1在多模态具身推理和机器人控制方面实现了优越性能。&lt;h4&gt;结论&lt;/h4&gt;本研究详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发高级具身基础模型提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类在开放世界中无缝执行多模态推理和物理交互的能力是通用具身智能系统的核心目标。最近的视觉-语言-动作模型在大规模机器人和视觉-文本数据上联合训练，在通用机器人控制方面展示了显著进展。然而，它们仍未达到人类在交错推理和交互方面的灵活性水平。在本工作中，我们介绍了EO-Robotics，包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过交错视觉-文本-动作预训练在多模态具身推理和机器人控制方面实现了优越性能。EO-1的发展基于两个关键支柱：（i）统一架构，无差别处理多模态输入（图像、文本、视频和动作）；（ii）大规模、高质量的多模态具身推理数据集EO-Data1.5M，包含超过150万个样本，强调交错视觉-文本-动作理解。EO-1通过在EO-Data1.5M上自回归解码和流匹配去噪的协同训练，实现无缝机器人动作生成和多模态具身推理。大量实验证明了交错视觉-文本-动作学习对开放世界理解和泛化的有效性，通过各种跨多个具身的长期、灵巧操作任务得到验证。本文详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发高级具身基础模型提供了有价值的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The human ability to seamlessly perform multimodal reasoning and physicalinteraction in the open world is a core goal for general-purpose embodiedintelligent systems. Recent vision-language-action (VLA) models, which areco-trained on large-scale robot and visual-text data, have demonstrated notableprogress in general robot control. However, they still fail to achievehuman-level flexibility in interleaved reasoning and interaction. In this work,introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 isa unified embodied foundation model that achieves superior performance inmultimodal embodied reasoning and robot control through interleavedvision-text-action pre-training. The development of EO-1 is based on two keypillars: (i) a unified architecture that processes multimodal inputsindiscriminately (image, text, video, and action), and (ii) a massive,high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which containsover 1.5 million samples with emphasis on interleaved vision-text-actioncomprehension. EO-1 is trained through synergies between auto-regressivedecoding and flow matching denoising on EO-Data1.5M, enabling seamless robotaction generation and multimodal embodied reasoning. Extensive experimentsdemonstrate the effectiveness of interleaved vision-text-action learning foropen-world understanding and generalization, validated through a variety oflong-horizon, dexterous manipulation tasks across multiple embodiments. Thispaper details the architecture of EO-1, the data construction strategy ofEO-Data1.5M, and the training methodology, offering valuable insights fordeveloping advanced embodied foundation models.</description>
      <author>example@mail.com (Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang)</author>
      <guid isPermaLink="false">2508.21112v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2508.20085v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了HERMES，一个用于移动双臂灵巧操作的人机学习框架，能够将多源人类手部动作转化为可行的机器人行为，并在多样化环境中实现自主操作。&lt;h4&gt;背景&lt;/h4&gt;利用人类运动数据让机器人获得多样化操作技能是一种有前景的范式，但将多源人类手部动作转化为可行的机器人行为仍具挑战性，尤其对于具有复杂、高维动作空间的多指灵巧手，现有方法难以适应不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发HERMES框架以解决多源人类手部动作到机器人行为的转换问题，提高策略对多样化环境条件的适应能力，实现复杂移动双臂灵巧操作任务。&lt;h4&gt;方法&lt;/h4&gt;HERMES采用统一强化学习方法将异构人类手部动作转化为物理可行的机器人行为；设计基于深度图像的端到端模拟到现实迁移方法提高泛化能力；通过闭环PnP定位机制增强导航基础模型，连接自主导航与灵巧操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明HERMES能在多样化真实场景中展现可泛化行为，成功执行多种复杂移动双臂灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;HERMES有效解决了人类运动到机器人行为的转换问题，通过模拟到现实迁移和增强导航能力，实现了在多样化环境中的自主操作。&lt;h4&gt;翻译&lt;/h4&gt;利用人类运动数据使机器人具备多样化操作技能已成为机器人操作领域的一种有前景的范式。然而，将多源人类手部动作转化为可行的机器人行为仍然具有挑战性，特别是对于配备具有复杂、高维动作空间的多指灵巧手的机器人。此外，现有方法通常难以产生能够适应不同环境条件的策略。在本文中，我们介绍了HERMES，一个用于移动双臂灵巧操作的人机学习框架。首先，HERMES制定了一种统一的强化学习方法，能够无缝地将来自多个源的不同类人类手部动作转化为物理上可行的机器人行为。随后，为了减轻模拟到现实的差距，我们设计了一种基于深度图像的端到端模拟到现实迁移方法，以提高对现实场景的泛化能力。此外，为了能够在多样化且非结构化的环境中自主运行，我们通过闭环PnP定位机制增强了导航基础模型，确保视觉目标的精确对齐，有效连接自主导航和灵巧操作。广泛的实验结果表明，HERMES能够在多样化、真实场景中展现出可泛化的行为，成功执行了许多复杂的移动双臂灵巧操作任务。项目页面：https://gemcollector.github.io/HERMES/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging human motion data to impart robots with versatile manipulationskills has emerged as a promising paradigm in robotic manipulation.Nevertheless, translating multi-source human hand motions into feasible robotbehaviors remains challenging, particularly for robots equipped withmulti-fingered dexterous hands characterized by complex, high-dimensionalaction spaces. Moreover, existing approaches often struggle to produce policiescapable of adapting to diverse environmental conditions. In this paper, weintroduce HERMES, a human-to-robot learning framework for mobile bimanualdexterous manipulation. First, HERMES formulates a unified reinforcementlearning approach capable of seamlessly transforming heterogeneous human handmotions from multiple sources into physically plausible robotic behaviors.Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depthimage-based sim2real transfer method for improved generalization to real-worldscenarios. Furthermore, to enable autonomous operation in varied andunstructured environments, we augment the navigation foundation model with aclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precisealignment of visual goals and effectively bridging autonomous navigation anddexterous manipulation. Extensive experimental results demonstrate that HERMESconsistently exhibits generalizable behaviors across diverse, in-the-wildscenarios, successfully performing numerous complex mobile bimanual dexterousmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.</description>
      <author>example@mail.com (Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.20085v3</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Compositional Generative Model of Unbounded 4D Cities</title>
      <link>http://arxiv.org/abs/2501.08983v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TPAMI. Project Page:  https://www.infinitescript.com/project/city-dreamer-4d/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CityDreamer4D，一个专门用于生成无限4D城市的组合生成模型，通过分离动态对象和静态场景，并使用不同类型的神经场来构建城市元素，实现了高质量的城市生成。&lt;h4&gt;背景&lt;/h4&gt;3D场景生成近年来受到广泛关注并取得显著进展，但生成4D城市更具挑战性，因为城市中存在结构复杂、视觉多样的对象（如建筑物和车辆），且人类对城市环境中的失真更为敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的生成模型解决4D城市生成中的挑战，能够生成结构合理、视觉真实的无限4D城市。&lt;h4&gt;方法&lt;/h4&gt;1) 将动态对象与静态场景分离；2) 使用不同类型的神经场构建城市元素；3) 提出交通场景生成器和无限布局生成器，使用紧凑的BEV表示；4) 组合面向物质和面向实例的神经场生成对象；5) 采用定制的生成哈希网格和周期性位置嵌入；6) 提供OSM、GoogleEarth和CityTopia数据集。&lt;h4&gt;主要发现&lt;/h4&gt;1) 4D城市生成应分离动态对象和静态场景；2) 所有4D场景对象应由不同类型的神经场组成；3) 定制的生成哈希网格和周期性位置嵌入能有效处理不同类型对象；4) 该模型在生成真实4D城市方面达到最先进性能。&lt;h4&gt;结论&lt;/h4&gt;CityDreamer4D凭借其组合设计，支持实例编辑、城市风格化和城市模拟等多种下游应用，同时在生成真实4D城市方面取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;3D场景生成近年来引起了越来越多的关注并取得了显著进展。由于存在结构复杂、视觉多样的对象（如建筑物和车辆），以及人类对城市环境中失真的高度敏感性，生成4D城市比3D场景更具挑战性。为解决这些问题，我们提出了CityDreamer4D，这是一个专门为生成无限4D城市而设计的组合生成模型。我们的主要见解是：1) 4D城市生成应将动态对象（如车辆）与静态场景（如建筑物和道路）分离，2) 4D场景中的所有对象应由不同类型的神经场组成，用于建筑物、车辆和背景物质。具体来说，我们提出了交通场景生成器和无限布局生成器，使用高度紧凑的BEV表示来生成动态交通场景和静态城市布局。4D城市中的对象通过组合面向物质和面向实例的神经场来生成，用于背景物质、建筑物和车辆。为了适应背景物质和实例的不同特性，神经场采用定制的生成哈希网格和周期性位置嵌入作为场景参数化。此外，我们为城市生成提供了全面的数据集，包括OSM、GoogleEarth和CityTopia。OSM数据集提供了各种真实世界城市布局，而Google Earth和CityTopia数据集提供了大规模、高质量的带有3D实例注释的城市图像。凭借其组合设计，CityDreamer4D支持多种下游应用，如实例编辑、城市风格化和城市模拟，同时在生成真实4D城市方面取得了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成无边界（unbounded）的4D城市场景的问题。4D城市指的是包含时间维度的3D城市场景，既包含静态的建筑、道路等元素，也包含动态的车辆等随时间变化的内容。这个问题在现实中很重要，因为城市作为元宇宙中最基本的资产之一，广泛应用于城市规划、环境模拟和游戏开发等领域。现有方法要么无法保证时间一致性，要么只能生成小规模场景，且人类对城市环境中的失真更加敏感，增加了生成高质量4D城市的难度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于两个主要思考设计方法：1) 4D城市生成应该将动态对象（如车辆）与静态场景（如建筑和道路）分离；2) 4D场景中的所有对象应由不同类型的神经场组成，分别用于建筑、车辆和背景元素。作者借鉴了多项现有工作，包括使用MaskGIT进行城市布局生成，VQVAE进行语义地图标记化，SceneDreamer的鸟瞰图表示方法，NeRF的体积渲染技术，以及GAN的对抗性训练方法等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将4D城市生成分离为静态场景生成和动态对象生成，使用不同类型的神经场分别生成背景、建筑和车辆，采用组合式设计以支持多种下游应用。整体流程分为六个步骤：1) 无边界布局生成器创建城市布局；2) 交通场景生成器生成高清地图和交通场景；3) 城市背景生成器生成背景元素；4) 建筑实例生成器生成建筑；5) 车辆实例生成器生成车辆；6) 组合器将所有元素合并为统一图像。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出CityDreamer4D，第一个分离动态对象与静态场景的无边界4D城市生成模型；2) 引入面向事物和面向实例的神经场，分别处理背景和实例；3) 创建全面的数据集(OSM、GoogleEarth和CityTopia)。相比之前的工作，不同之处在于：分离了动态与静态元素的生成；为不同元素使用专门的神经场参数化方法；将建筑和车辆分别放置在特定的坐标空间中；创建了更全面的数据集；支持实例级编辑、城市风格化和城市模拟等应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CityDreamer4D通过分离动态对象与静态场景、使用专门的神经场生成不同类型元素，并构建全面的数据集，实现了高质量无边界4D城市的生成与编辑，为元宇宙应用提供了强大工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-01-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TPAMI.2025.3603078&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D scene generation has garnered growing attention in recent years and hasmade significant progress. Generating 4D cities is more challenging than 3Dscenes due to the presence of structurally complex, visually diverse objectslike buildings and vehicles, and heightened human sensitivity to distortions inurban environments. To tackle these issues, we propose CityDreamer4D, acompositional generative model specifically tailored for generating unbounded4D cities. Our main insights are 1) 4D city generation should separate dynamicobjects (e.g., vehicles) from static scenes (e.g., buildings and roads), and 2)all objects in the 4D scene should be composed of different types of neuralfields for buildings, vehicles, and background stuff. Specifically, we proposeTraffic Scenario Generator and Unbounded Layout Generator to produce dynamictraffic scenarios and static city layouts using a highly compact BEVrepresentation. Objects in 4D cities are generated by combining stuff-orientedand instance-oriented neural fields for background stuff, buildings, andvehicles. To suit the distinct characteristics of background stuff andinstances, the neural fields employ customized generative hash grids andperiodic positional embeddings as scene parameterizations. Furthermore, weoffer a comprehensive suite of datasets for city generation, including OSM,GoogleEarth, and CityTopia. The OSM dataset provides a variety of real-worldcity layouts, while the Google Earth and CityTopia datasets deliverlarge-scale, high-quality city imagery complete with 3D instance annotations.Leveraging its compositional design, CityDreamer4D supports a range ofdownstream applications, such as instance editing, city stylization, and urbansimulation, while delivering state-of-the-art performance in generatingrealistic 4D cities.</description>
      <author>example@mail.com (Haozhe Xie, Zhaoxi Chen, Fangzhou Hong, Ziwei Liu)</author>
      <guid isPermaLink="false">2501.08983v4</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval</title>
      <link>http://arxiv.org/abs/2508.20778v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型对比学习框架SEAL，用于解决长结构化文档检索中现有方法未能有效利用结构特征和元素级语义的问题，并发布了带有丰富结构注释的数据集。&lt;h4&gt;背景&lt;/h4&gt;现有方法在长结构化文档检索中通常使用对比学习来微调预训练语言模型(PLMs)，但这些方法使用的数据集缺乏明确的结构信息。&lt;h4&gt;目的&lt;/h4&gt;解决当前方法未能有效利用结构特征和元素级语义的问题，以及缺乏包含结构元数据的数据集的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了SEAL框架，利用结构感知学习保留语义层次结构，并使用掩码元素进行细粒度语义区分，同时发布了带有丰富结构注释的长结构化文档检索数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在发布的数据集和工业数据集上的广泛实验，以及在线A/B测试表明，该方法在各种现代PLM上实现了性能提升，在BGE-M3上将NDCG@10从73.96%提升到77.84%。&lt;h4&gt;结论&lt;/h4&gt;SEAL框架通过有效利用结构信息和元素级语义，显著提升了长结构化文档检索的性能。&lt;h4&gt;翻译&lt;/h4&gt;在长结构化文档检索中，现有方法通常在缺乏明确结构信息的数据集上使用对比学习来微调预训练语言模型(PLMs)。这种方法存在两个关键问题：1)当前方法未能有效利用结构特征和元素级语义；2)缺乏包含结构元数据的数据集。为弥补这些差距，我们提出了SEAL，一种新型对比学习框架。它利用结构感知学习来保留语义层次结构，并使用掩码元素进行细粒度语义区分。此外，我们发布了SEAL数据集，一个具有丰富结构注释的长结构化文档检索数据集。在发布的数据集和工业数据集上对各种现代PLM进行的广泛实验，以及在线A/B测试，都展示了一致的性能提升，在BGE-M3上将NDCG@10从73.96%提升到77.84%。资源可在https://github.com/xinhaoH/SEAL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In long structured document retrieval, existing methods typically fine-tunepre-trained language models (PLMs) using contrastive learning on datasetslacking explicit structural information. This practice suffers from twocritical issues: 1) current methods fail to leverage structural features andelement-level semantics effectively, and 2) the lack of datasets containingstructural metadata. To bridge these gaps, we propose \our, a novel contrastivelearning framework. It leverages structure-aware learning to preserve semantichierarchies and masked element alignment for fine-grained semanticdiscrimination. Furthermore, we release \dataset, a long structured documentretrieval dataset with rich structural annotations. Extensive experiments onboth released and industrial datasets across various modern PLMs, along withonline A/B testing, demonstrate consistent performance improvements, boostingNDCG@10 from 73.96\% to 77.84\% on BGE-M3. The resources are available athttps://github.com/xinhaoH/SEAL.</description>
      <author>example@mail.com (Xinhao Huang, Zhibo Ren, Yipeng Yu, Ying Zhou, Zulong Chen, Zeyi Wen)</author>
      <guid isPermaLink="false">2508.20778v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>GPI-Net: Gestalt-Guided Parallel Interaction Network via Orthogonal Geometric Consistency for Robust Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2507.14452v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures. Accepted to IJCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于格式塔原则的正交几何一致性引导的并行交互网络（GPI-Net），用于解决点云配准中局部和全局特征融合的挑战性问题。&lt;h4&gt;背景&lt;/h4&gt;在基于特征点云配准中，准确识别高质量对应关系是一项前提任务，但由于特征冗余和复杂的空间关系，处理局部和全局特征的融合极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;利用格式塔原则的优势，设计一种新型网络结构，促进局部和全局信息之间的互补交流，提高点云配准中对应关系识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出GPI-Net网络，包含：1）利用格式塔原则促进局部和全局信息互补交流；2）引入正交集成策略减少冗余信息；3）设计格式塔特征注意力（GFA）块捕捉对应关系中的几何特征；4）创建双路径多粒度并行交互聚合（DMG）块促进不同粒度间的信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;在各种具有挑战性的任务上的大量实验证明，与现有方法相比，所提出的GPI-Net具有优越的性能。&lt;h4&gt;结论&lt;/h4&gt;GPI-Net通过有效融合局部和全局特征，显著提高了点云配准中高质量对应关系的识别能力。&lt;h4&gt;翻译&lt;/h4&gt;在基于特征点云配准中，准确识别高质量对应关系是一项前提任务。然而，由于特征冗余和复杂的空间关系，处理局部和全局特征的融合极具挑战性。鉴于格式塔原则在分析局部和全局关系方面提供关键优势，我们在本文中提出了一种基于格式塔原则的正交几何一致性引导的并行交互网络（GPI-Net）。它利用格式塔原则促进局部和全局信息之间的互补交流。具体来说，我们引入了一种正交集成策略，以最优方式减少冗余信息，并为高质量对应关系生成更紧凑的全局结构。为了捕捉对应关系中的几何特征，我们通过自注意力和交叉注意力机制的混合使用，利用了格式塔特征注意力（GFA）块。此外，为了促进局部详细信息整合到全局结构中，我们设计了一种创新的双路径多粒度并行交互聚合（DMG）块，以促进不同粒度间的信息交换。在各种具有挑战性的任务上的大量实验证明，与现有方法相比，我们提出的GPI-Net具有优越的性能。代码将在https://github.com/gwk429/GPI-Net上发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中高质量对应关系的识别问题。在点云配准过程中，初始对应关系经常包含大量错误匹配，导致不准确的对齐。这个问题很重要，因为点云是3D世界的主要表示形式，广泛应用于自动驾驶、机器人、SLAM等领域，而精确的点云对齐是实现这些应用的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于Gestalt原则(格式塔原则)设计方法，该原则强调整体感知优于局部感知。作者观察到现有方法在处理局部和全局特征融合时存在不足，无法有效捕捉空间关系。作者借鉴了PG-Net的框架，但通过引入Gestalt原则来改进局部和全局信息的互补通信。同时，作者也参考了PointDSC的空间一致性思想，但通过正交整合策略和双路径多粒度交互设计解决了冗余信息问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用Gestalt原则促进局部和全局信息之间的互补通信，通过正交整合减少冗余信息，并设计注意力机制捕获几何特征。整体流程：1)输入初始对应关系；2)上下文嵌入模块映射到高维特征空间；3)正交整合(OI)过滤冗余信息；4)Gestalt特征注意力(GFA)提取几何特征；5)双路径多粒度并行交互聚合(DMG)促进不同粒度信息交换；6)种子选择模块识别高质量对应关系；7)Two-Stage NSM模块估计最优变换矩阵。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)提出GPI-Net解决特征冗余和复杂空间关系；2)将Gestalt原则融入网络设计；3)设计正交整合(OI)策略减少冗余；4)提出Gestalt特征注意力(GFA)块；5)设计双路径多粒度并行交互聚合(DMG)块。相比之前工作，GPI-Net更好地平衡了局部和全局信息，减少了冗余，在室内外场景下均表现出优越性能，特别是在对应关系较少的情况下优势更明显。实验显示，相比PG-Net，GPI-Net的F1分数提高了约1%，RR提高了约1.5%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GPI-Net通过融合格式塔原则和正交几何一致性，提出了一种新颖的并行交互网络，显著提高了点云配准中对应关系的识别精度和鲁棒性，特别是在复杂场景和高离群值比例的情况下表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate identification of high-quality correspondences is a prerequisitetask in feature-based point cloud registration. However, it is extremelychallenging to handle the fusion of local and global features due to featureredundancy and complex spatial relationships. Given that Gestalt principlesprovide key advantages in analyzing local and global relationships, we proposea novel Gestalt-guided Parallel Interaction Network via orthogonal geometricconsistency (GPI-Net) in this paper. It utilizes Gestalt principles tofacilitate complementary communication between local and global information.Specifically, we introduce an orthogonal integration strategy to optimallyreduce redundant information and generate a more compact global structure forhigh-quality correspondences. To capture geometric features in correspondences,we leverage a Gestalt Feature Attention (GFA) block through a hybridutilization of self-attention and cross-attention mechanisms. Furthermore, tofacilitate the integration of local detail information into the globalstructure, we design an innovative Dual-path Multi-Granularity parallelinteraction aggregation (DMG) block to promote information exchange acrossdifferent granularities. Extensive experiments on various challenging tasksdemonstrate the superior performance of our proposed GPI-Net in comparison toexisting methods. The code will be released athttps://github.com/gwk429/GPI-Net.</description>
      <author>example@mail.com (Weikang Gu, Mingyue Han, Li Xue, Heng Dong, Changcai Yang, Riqing Chen, Lifang Wei)</author>
      <guid isPermaLink="false">2507.14452v2</guid>
      <pubDate>Wed, 03 Sep 2025 14:14:56 +0800</pubDate>
    </item>
    <item>
      <title>Learning Unified Representations from Heterogeneous Data for Robust Heart Rate Modeling</title>
      <link>http://arxiv.org/abs/2508.21785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种处理心率预测中数据异质性挑战的框架，通过学习对设备异质性和用户异质性都无潜在表示，显著提升了模型在实际应用中的性能。&lt;h4&gt;背景&lt;/h4&gt;心率预测对个性化健康监测和健身至关重要，但在实际部署中面临数据异质性的挑战，包括来自不同设备的源异质性和不同用户及活动的用户异质性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理设备异质性和用户异质性的框架，使心率预测模型在真实世界的异构数据环境下保持一致的高性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种包含随机特征丢弃策略处理源异质性，以及时间感知注意力模块和对比学习目标来处理用户异质性的框架，并创建了新的基准数据集ParroTao进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在ParroTao和FitRec数据集上，所提模型分别比现有基线高出17%和15%；学习到的表示具有很强的判别能力；一个下游应用任务验证了模型的实用价值。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架有效解决了心率预测中的数据异质性问题，显著提升了模型在实际应用中的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;心率预测对个性化健康监测和健身至关重要，但在实际部署中经常面临一个关键挑战：数据异质性。我们从两个关键维度对其进行分类：来自具有不同特征集的碎片化设备市场的源异质性，以及反映不同个体和活动中生理模式差异的用户异质性。现有方法要么丢弃设备特定信息，要么无法建模用户特定差异，限制了它们的实际性能。为此，我们提出了一个学习对两种异质性都无潜在表示的框架，使下游预测器能够在异构数据模式下一致工作。具体而言，我们引入了随机特征丢弃策略来处理源异质性，使模型对各种特征集具有鲁棒性。为了管理用户异质性，我们采用时间感知注意力模块来捕获长期生理特征，并使用对比学习目标来构建判别性表示空间。为了反映现实世界数据的异质性，我们创建并公开发布了一个新的基准数据集ParroTao。在ParroTao和公共FitRec数据集上的评估显示，我们的模型分别比现有基线高出17%和15%。此外，对学习到的表示的分析证明了它们具有很强的判别能力，一个下游应用任务确认了我们模型的实用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heart rate prediction is vital for personalized health monitoring andfitness, while it frequently faces a critical challenge when deploying inreal-world: data heterogeneity. We classify it in two key dimensions: sourceheterogeneity from fragmented device markets with varying feature sets, anduser heterogeneity reflecting distinct physiological patterns acrossindividuals and activities. Existing methods either discard device-specificinformation, or fail to model user-specific differences, limiting theirreal-world performance. To address this, we propose a framework that learnslatent representations agnostic to both heterogeneity, enabling downstreampredictors to work consistently under heterogeneous data patterns.Specifically, we introduce a random feature dropout strategy to handle sourceheterogeneity, making the model robust to various feature sets. To manage userheterogeneity, we employ a time-aware attention module to capture long-termphysiological traits and use a contrastive learning objective to build adiscriminative representation space. To reflect the heterogeneous nature ofreal-world data, we created and publicly released a new benchmark dataset,ParroTao. Evaluations on both ParroTao and the public FitRec dataset show thatour model significantly outperforms existing baselines by 17% and 15%,respectively. Furthermore, analysis of the learned representations demonstratestheir strong discriminative power, and one downstream application task confirmthe practical value of our model.</description>
      <author>example@mail.com (Peng Yang, Zhengdong Huang, Zicheng Xie, Wentao Tian, Jingyu Liu, Lunhong Dong)</author>
      <guid isPermaLink="false">2508.21785v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
  <item>
      <title>HCCM: Hierarchical Cross-Granularity Contrastive and Matching Learning for Natural Language-Guided Drones</title>
      <link>http://arxiv.org/abs/2508.21539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种分层跨粒度对比和匹配学习（HCCM）框架，用于解决自然语言引导无人机（NLGD）任务中的视觉语言理解挑战，特别是在处理广角视野和复杂组合语义时。&lt;h4&gt;背景&lt;/h4&gt;无人机场景中的广角视野和复杂组合语义对视觉语言理解提出了挑战。主流视觉语言模型（VLMs）强调全局对齐但缺乏细粒度语义，而现有分层方法依赖于精确的实体划分和严格的包含关系，限制了在动态环境中的有效性。此外，无人机文本描述通常不完整或模糊，导致对齐不稳定。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理无人机场景中视觉语言理解挑战的框架，特别是在处理广角视野、复杂组合语义和不完整文本描述时，提高目标匹配和导航等任务的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了分层跨粒度对比和匹配学习（HCCM）框架，包含两个主要组件：1. 区域-全局图像-文本对比学习（RG-ITC）：避免精确场景划分，通过对比局部视觉区域与全局文本以及反之亦然来捕获分层局部到全局的语义。2. 区域-全局图像-文本匹配（RG-ITM）：放弃严格的约束，在全局跨模态表示内评估局部语义一致性，增强组合推理能力。此外，还引入了动量对比和蒸馏（MCD）机制来提高对不完整或模糊文本描述的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在GeoText-1652数据集上，HCCM在图像检索中达到了28.8%的最先进Recall@1，在文本检索中达到了14.7%。在未见过的ERA数据集上，HCCM展示了强大的零样本泛化能力，平均召回率（mR）为39.93%，优于微调基线。&lt;h4&gt;结论&lt;/h4&gt;HCCM框架通过分层跨粒度的对比和匹配学习方法，有效解决了无人机场景中视觉语言理解的挑战，特别是在处理广角视野、复杂组合语义和不完整文本描述方面，表现出优越的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;自然语言引导的无人机（NLGD）为目标匹配和导航等任务提供了新颖的范式。然而，无人机场景中的广角视野和复杂组合语义对视觉语言理解提出了挑战。主流视觉语言模型（VLMs）强调全局对齐但缺乏细粒度语义，而现有的分层方法依赖于精确的实体划分和严格的包含关系，限制了在动态环境中的有效性。为此，我们提出了分层跨粒度对比和匹配学习（HCCM）框架，包含两个组件：（1）区域-全局图像-文本对比学习（RG-ITC），它避免了精确的场景划分，通过对比局部视觉区域与全局文本以及反之亦然来捕获分层局部到全局的语义；（2）区域-全局图像-文本匹配（RG-ITM），它放弃了严格的约束，而是在全局跨模态表示内评估局部语义一致性，增强组合推理能力。此外，无人机文本描述通常不完整或模糊，导致对齐不稳定。HCCM引入了动量对比和蒸馏（MCD）机制来提高鲁棒性。在GeoText-1652上的实验显示，HCCM在图像检索中达到了28.8%的最先进Recall@1，在文本检索中达到了14.7%。在未见过的ERA数据集上，HCCM展示了强大的零样本泛化能力，平均召回率为39.93%，优于微调基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Natural Language-Guided Drones (NLGD) provide a novel paradigm for tasks suchas target matching and navigation. However, the wide field of view and complexcompositional semantics in drone scenarios pose challenges for vision-languageunderstanding. Mainstream Vision-Language Models (VLMs) emphasize globalalignment while lacking fine-grained semantics, and existing hierarchicalmethods depend on precise entity partitioning and strict containment, limitingeffectiveness in dynamic environments. To address this, we propose theHierarchical Cross-Granularity Contrastive and Matching learning (HCCM)framework with two components: (1) Region-Global Image-Text ContrastiveLearning (RG-ITC), which avoids precise scene partitioning and captureshierarchical local-to-global semantics by contrasting local visual regions withglobal text and vice versa; (2) Region-Global Image-Text Matching (RG-ITM),which dispenses with rigid constraints and instead evaluates local semanticconsistency within global cross-modal representations, enhancing compositionalreasoning. Moreover, drone text descriptions are often incomplete or ambiguous,destabilizing alignment. HCCM introduces a Momentum Contrast and Distillation(MCD) mechanism to improve robustness. Experiments on GeoText-1652 show HCCMachieves state-of-the-art Recall@1 of 28.8% (image retrieval) and 14.7% (textretrieval). On the unseen ERA dataset, HCCM demonstrates strong zero-shotgeneralization with 39.93% mean recall (mR), outperforming fine-tunedbaselines.</description>
      <author>example@mail.com (Hao Ruan, Jinliang Lin, Yingxin Lai, Zhiming Luo, Shaozi Li)</author>
      <guid isPermaLink="false">2508.21539v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>GCAV: A Global Concept Activation Vector Framework for Cross-Layer Consistency in Interpretability</title>
      <link>http://arxiv.org/abs/2508.21197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为全局概念激活向量(GCAV)的新框架，用于解决传统概念激活向量(CAV)在不同层间不一致的问题，使跨层比较更加可靠。&lt;h4&gt;背景&lt;/h4&gt;概念激活向量(CAV)是一种解释深度神经网络的有力方法，通过量化网络对人类定义概念的敏感性来工作。然而，当在不同层独立计算CAV时，它们常常表现出不一致性，这使得跨层比较不可靠。&lt;h4&gt;目的&lt;/h4&gt;为了解决CAV在不同层间的不一致问题，作者提出GCAV框架，将CAV统一为单一、语义一致的表示。&lt;h4&gt;方法&lt;/h4&gt;作者的方法利用对比学习来对齐不同层的概念表示，并采用基于注意力的融合机制来构建全局整合的CAV。他们还引入了使用全局概念激活向量进行测试(TGCAV)的方法，以将TCAV应用于基于GCAV的表示。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著降低了TCAV分数的方差，同时保留了概念相关性，确保了更稳定和可靠的概念归因。实验表明，该方法有效减轻了跨层概念不一致性，增强了概念定位能力，并提高了对抗扰动的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过将跨层信息整合到一致的框架中，该方法提供了对深度学习模型如何编码人类定义概念的更全面和可解释的理解。&lt;h4&gt;翻译&lt;/h4&gt;概念激活向量(CAV)提供了一种强大的方法来解释深度神经网络，通过量化它们对人类定义概念的敏感性。然而，当在不同层独立计算时，CAV常常表现出不一致性，使得跨层比较不可靠。为了解决这个问题，我们提出了全局概念激活向量(GCAV)，这是一个将CAV统一为单一、语义一致表示的新框架。我们的方法利用对比学习来对齐跨层的概念表示，并采用基于注意力的融合机制来构建全局整合的CAV。通过这样做，我们的方法显著降低了TCAV分数的方差，同时保留概念相关性，确保更稳定和可靠的概念归因。为了评估GCAV的有效性，我们引入了使用全局概念激活向量进行测试(TGCAV)作为一种将TCAV应用于基于GCAV表示的方法。我们在多个深度神经网络上进行了大量实验，证明我们的方法有效减轻了跨层概念不一致性，增强了概念定位能力，并提高了对抗扰动的鲁棒性。通过将跨层信息整合到一致的框架中，我们的方法提供了对深度学习模型如何编码人类定义概念的更全面和可解释的理解。代码和模型可在https://github.com/Zhenghao-He/GCAV获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concept Activation Vectors (CAVs) provide a powerful approach forinterpreting deep neural networks by quantifying their sensitivity tohuman-defined concepts. However, when computed independently at differentlayers, CAVs often exhibit inconsistencies, making cross-layer comparisonsunreliable. To address this issue, we propose the Global Concept ActivationVector (GCAV), a novel framework that unifies CAVs into a single, semanticallyconsistent representation. Our method leverages contrastive learning to alignconcept representations across layers and employs an attention-based fusionmechanism to construct a globally integrated CAV. By doing so, our methodsignificantly reduces the variance in TCAV scores while preserving conceptrelevance, ensuring more stable and reliable concept attributions. To evaluatethe effectiveness of GCAV, we introduce Testing with Global Concept ActivationVectors (TGCAV) as a method to apply TCAV to GCAV-based representations. Weconduct extensive experiments on multiple deep neural networks, demonstratingthat our method effectively mitigates concept inconsistency across layers,enhances concept localization, and improves robustness against adversarialperturbations. By integrating cross-layer information into a coherentframework, our method offers a more comprehensive and interpretableunderstanding of how deep learning models encode human-defined concepts. Codeand models are available at https://github.com/Zhenghao-He/GCAV.</description>
      <author>example@mail.com (Zhenghao He, Sanchit Sinha, Guangzhi Xiong, Aidong Zhang)</author>
      <guid isPermaLink="false">2508.21197v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>THEME: Enhancing Thematic Investing with Semantic Stock Representations and Temporal Dynamics</title>
      <link>http://arxiv.org/abs/2508.16936v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ACM International Conference on Information and Knowledge  Management (CIKM)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了THEME框架，通过分层对比学习微调嵌入模型，解决主题投资中行业边界重叠和市场动态变化的挑战，实现主题资产的语义表示和有效检索。&lt;h4&gt;背景&lt;/h4&gt;主题投资旨在构建与结构性趋势一致的组合，但行业边界重叠和市场动态变化使这一目标具有挑战性。从文本数据构建投资主题的语义表示是一个有前景的方向。&lt;h4&gt;目的&lt;/h4&gt;解决通用大型语言模型嵌入模型不适合捕捉金融资产细微特征的问题，开发专门针对投资主题的语义表示方法。&lt;h4&gt;方法&lt;/h4&gt;引入THEME框架，使用分层对比学习微调嵌入模型，利用主题及其组成股票的层次关系进行对齐，并通过整合股票收益来完善这些嵌入，生成能有效检索具有强回报潜力的主题一致资产的表示。&lt;h4&gt;主要发现&lt;/h4&gt;THEME在主题资产检索方面显著优于领先的大型语言模型，且其构建的投资组合表现优异。&lt;h4&gt;结论&lt;/h4&gt;通过共同建模文本中的主题关系和收益中的市场动态，THEME为各种实际投资应用生成专门定制的股票嵌入。&lt;h4&gt;翻译&lt;/h4&gt;主题投资旨在构建与结构性趋势一致的组合，由于行业边界重叠和市场动态变化，这仍然是一项具有挑战性的工作。一个有前景的方向是从文本数据构建投资主题的语义表示。然而，尽管通用大型语言模型嵌入模型功能强大，但不太适合捕捉金融资产的细微特征，因为投资资产的语义表示可能与一般金融文本的语义表示有根本差异。为此，我们引入THEME框架，使用分层对比学习微调嵌入。THEME利用主题及其组成股票的层次关系对齐主题和股票，随后通过整合股票收益来完善这些嵌入。这一过程生成能有效检索具有强回报潜力的主题一致资产的表示。实证结果表明，THEME在两个关键领域表现出色：在主题资产检索方面，它显著优于领先的大型语言模型；此外，其构建的组合表现出令人信服的性能。通过共同建模文本中的主题关系和收益中的市场动态，THEME为各种实际投资应用生成专门定制的股票嵌入。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761517&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Thematic investing, which aims to construct portfolios aligned withstructural trends, remains a challenging endeavor due to overlapping sectorboundaries and evolving market dynamics. A promising direction is to buildsemantic representations of investment themes from textual data. However,despite their power, general-purpose LLM embedding models are not well-suitedto capture the nuanced characteristics of financial assets, since the semanticrepresentation of investment assets may differ fundamentally from that ofgeneral financial text. To address this, we introduce THEME, a framework thatfine-tunes embeddings using hierarchical contrastive learning. THEME alignsthemes and their constituent stocks using their hierarchical relationship, andsubsequently refines these embeddings by incorporating stock returns. Thisprocess yields representations effective for retrieving thematically alignedassets with strong return potential. Empirical results demonstrate that THEMEexcels in two key areas. For thematic asset retrieval, it significantlyoutperforms leading large language models. Furthermore, its constructedportfolios demonstrate compelling performance. By jointly modeling thematicrelationships from text and market dynamics from returns, THEME generates stockembeddings specifically tailored for a wide range of practical investmentapplications.</description>
      <author>example@mail.com (Hoyoung Lee, Wonbin Ahn, Suhwan Park, Jaehoon Lee, Minjae Kim, Sungdong Yoo, Taeyoon Lim, Woohyung Lim, Yongjae Lee)</author>
      <guid isPermaLink="false">2508.16936v2</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>ScanMove: Motion Prediction and Transfer for Unregistered Body Meshes</title>
      <link>http://arxiv.org/abs/2508.21095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的、无需刚性约束的、数据驱动的框架，用于在未注册的体网格上进行运动预测和转移，通过结合稳健的运动嵌入网络和学习的逐顶点特征场生成时空变形场驱动网格变形。&lt;h4&gt;背景&lt;/h4&gt;未注册的表面网格（特别是原始3D扫描）在计算合理的变形时存在显著挑战，主要因为缺乏已确定的点对点对应关系和数据中存在的噪声。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需刚性约束的、数据驱动的框架，用于在未注册的体网格上进行运动预测和转移。&lt;h4&gt;方法&lt;/h4&gt;提出一种结合稳健的运动嵌入网络和学习的逐顶点特征场的方法，生成时空变形场来驱动网格变形。&lt;h4&gt;主要发现&lt;/h4&gt;在行走和跑步等任务上的定量基准测试和定性视觉效果评估表明，该方法在具有挑战性的未注册网格上具有有效性和多功能性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在处理未注册网格上的运动预测和转移方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;未注册的表面网格，特别是原始3D扫描，由于缺乏已确定的点对点对应关系和数据中存在的噪声，在自动计算合理的变形时提出了重大挑战。在本文中，我们提出了一种新的、无需刚性约束的、数据驱动的框架，用于在此类体网格上进行运动预测和转移。我们的方法将稳健的运动嵌入网络与学习的逐顶点特征场相结合，生成时空变形场，从而驱动网格变形。广泛的评估，包括在行走和跑步等任务上的定量基准测试和定性视觉效果，证明了我们的方法在具有挑战性的未注册网格上的有效性和多功能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决未注册表面网格（特别是原始3D扫描）的自动变形计算问题。由于缺乏点对应关系和数据噪声，这些网格难以进行合理变形。随着3D扫描技术进步，野外扫描数据增多，但传统基于骨架的动画技术难以处理这些不规则数据。此外，现有参数化模型（如SMPL）仅限特定形状类别，且需要额外工作适应新类别。解决这个问题对增强现实、虚拟角色动画和机器人等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：基于rig的方法需要rigging和skinning，不能用于未注册网格；变形模型稳健但无法捕获复杂时间依赖性。作者设计了一种递归模型，接受任意源网格和未注册运动序列作为输入，生成时间和顶点级别的变形场。方法借鉴了DiffusionNet作为特征提取器，利用PointNet编码器处理运动序列，并使用双向GRU确保时间一致性，还采用了AIAP正则化损失防止非等距变形。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个无需rig、数据驱动的框架，通过耦合鲁棒的运动嵌入网络与学习的顶点特征场，生成时空变形场来驱动网格变形。流程包括：1) 使用DiffusionNet从源网格顶点位置和法线提取特征；2) 用PointNet编码器和双向GRU处理运动序列；3) 对每个时间步，将运动向量与前一时刻顶点位置连接形成增强特征；4) 用共享MLP预测位移场，添加到源网格实现变形。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 新的端到端深度学习框架用于复杂运动预测；2) 探索和学习时空特征空间实现顶点级别变形；3) 在未注册网格上进行运动预测和传输的综合评估。相比之前工作，该方法无需骨架信息，适用于复杂运动和原始扫描，不需要rigging和skinning，可直接捕获时间依赖性，减少了对良好rigging的依赖，且可在完全无监督设置中训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ScanMove提出了一种无需rig的深度学习框架，能够直接从未注册的3D网格预测复杂且时间一致的运动变形，克服了传统方法对骨架对应关系和网格拓扑的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unregistered surface meshes, especially raw 3D scans, present significantchallenges for automatic computation of plausible deformations due to the lackof established point-wise correspondences and the presence of noise in thedata. In this paper, we propose a new, rig-free, data-driven framework formotion prediction and transfer on such body meshes. Our method couples a robustmotion embedding network with a learned per-vertex feature field to generate aspatio-temporal deformation field, which drives the mesh deformation. Extensiveevaluations, including quantitative benchmarks and qualitative visuals on taskssuch as walking and running, demonstrate the effectiveness and versatility ofour approach on challenging unregistered meshes.</description>
      <author>example@mail.com (Thomas Besnier, Sylvain Arguillère, Mohamed Daoudi)</author>
      <guid isPermaLink="false">2508.21095v1</guid>
      <pubDate>Tue, 02 Sep 2025 14:05:21 +0800</pubDate>
    </item>
    <item>
      <title>VoCap: Video Object Captioning and Segmentation from Any Prompt</title>
      <link>http://arxiv.org/abs/2508.21809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VoCap模型，一种能够处理视频和各种模态提示并生成时空掩码和物体中心标题的灵活视频模型，同时解决了可提示视频物体分割、指代表达式分割和物体标注三个任务。&lt;h4&gt;背景&lt;/h4&gt;理解视频中物体的精细定位掩码和详细语义属性是视频理解的基本任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够同时处理多种视频理解任务的统一模型，解决数据获取困难的问题，并为视频物体标注建立基准。&lt;h4&gt;方法&lt;/h4&gt;通过伪物体标题注释现有大规模分割数据集SAV；使用预处理视频和真实掩码突出感兴趣对象，输入大型视觉语言模型；创建手动注释的验证集SAV-Caption；在SAV-Caption与其他图像和视频数据集混合上大规模训练VoCap模型。&lt;h4&gt;主要发现&lt;/h4&gt;VoCap在指代表达式视频物体分割任务上取得最先进结果；在半监督视频物体分割任务上具有竞争力；为视频物体标注建立了新基准。&lt;h4&gt;结论&lt;/h4&gt;VoCap模型能够有效处理多种视频理解任务，通过创新的数据集构建方法解决了数据获取难题，并在多个任务上展现出优异性能。&lt;h4&gt;翻译&lt;/h4&gt;从视频中的精细定位掩码和详细语义属性角度理解物体是视频理解的基本任务。在本文中，我们提出了VoCap，一种灵活的视频模型，它能够处理视频和各种模态（文本、框或掩码）的提示，并生成一个时空掩码和相应的以物体为中心的标题。因此，我们的模型同时解决了可提示视频物体分割、指代表达式分割和物体标注任务。由于获取该任务的数据繁琐且昂贵，我们提出通过伪物体标题注释现有的大规模分割数据集（SAV）。我们通过预处理视频及其真实掩码来突出显示感兴趣的对象，并将其输入到大型视觉语言模型（VLM）中。为了进行无偏评估，我们在验证集上收集了手动注释。我们将 resulting 数据集称为SAV-Caption。我们在SAV-Caption以及其他图像和视频数据集的混合上大规模训练我们的VoCap模型。我们的模型在指代表达式视频物体分割任务上取得了最先进的结果，在半监督视频物体分割任务上具有竞争力，并为视频物体标注建立了基准。我们的数据集将在https://github.com/google-deepmind/vocap上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding objects in videos in terms of fine-grained localization masksand detailed semantic properties is a fundamental task in video understanding.In this paper, we propose VoCap, a flexible video model that consumes a videoand a prompt of various modalities (text, box or mask), and produces aspatio-temporal masklet with a corresponding object-centric caption. As suchour model addresses simultaneously the tasks of promptable video objectsegmentation, referring expression segmentation, and object captioning. Sinceobtaining data for this task is tedious and expensive, we propose to annotatean existing large-scale segmentation dataset (SAV) with pseudo object captions.We do so by preprocessing videos with their ground-truth masks to highlight theobject of interest and feed this to a large Vision Language Model (VLM). For anunbiased evaluation, we collect manual annotations on the validation set. Wecall the resulting dataset SAV-Caption. We train our VoCap model at scale on aSAV-Caption together with a mix of other image and video datasets. Our modelyields state-of-the-art results on referring expression video objectsegmentation, is competitive on semi-supervised video object segmentation, andestablishes a benchmark for video object captioning. Our dataset will be madeavailable at https://github.com/google-deepmind/vocap.</description>
      <author>example@mail.com (Jasper Uijlings, Xingyi Zhou, Xiuye Gu, Arsha Nagrani, Anurag Arnab, Alireza Fathi, David Ross, Cordelia Schmid)</author>
      <guid isPermaLink="false">2508.21809v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
  <item>
      <title>ELV-Halluc: Benchmarking Semantic Aggregation Hallucinations in Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.21496v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ELV-Halluc，第一个专门针对长视频幻觉的基准测试，用于系统研究语义聚合幻觉(SAH)现象。SAH指模型在聚合帧级语义到事件级语义组过程中产生的幻觉，在长视频中尤为严重。&lt;h4&gt;背景&lt;/h4&gt;视频多模态大语言模型在视频理解方面取得了显著进展，但仍容易产生与视频输入不一致或不相关的幻觉。之前的视频幻觉基准主要关注短视频，将幻觉归因于强语言先验、缺失帧或视觉语言偏差等因素，但这些因素过于简化了幻觉的原因。&lt;h4&gt;目的&lt;/h4&gt;分离和彻底研究语义聚合幻觉(SAH)这一特定类型的幻觉，特别是在长视频中的表现，并创建一个专门的基准测试来系统研究这一问题。&lt;h4&gt;方法&lt;/h4&gt;引入ELV-Halluc基准测试，采用位置编码策略来缓解SAH，并采用DPO策略增强模型区分事件内外语义的能力。为此，整理了8K对抗性数据对进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;1) 实验确认了SAH的存在，且SAH随语义复杂度增加而增加；2) 模型在语义快速变化时更容易出现SAH；3) 位置编码策略有助于缓解SAH；4) DPO策略增强了模型区分事件内外语义的能力；5) 在ELV-Halluc和Video-MME上取得了改进，SAH比率显著降低27.7%。&lt;h4&gt;结论&lt;/h4&gt;语义聚合幻觉是长视频理解中的一个重要问题，需要专门的研究和解决方法。通过引入ELV-Halluc基准测试和采用特定的缓解策略，可以有效减少SAH的发生。&lt;h4&gt;翻译&lt;/h4&gt;视频多模态大语言模型在视频理解方面已取得显著进展。然而，它们仍然容易产生与视频输入不一致或不相关的幻觉内容。之前的视频幻觉基准主要关注短视频，将幻觉归因于强语言先验、缺失帧或视觉编码器引入的视觉语言偏差等因素。虽然这些原因确实解释了短视频中大多数幻觉，但它们仍然过于简化了幻觉的原因。有时，模型会产生不正确的输出，但具有正确的帧级语义。我们将这种类型的幻觉称为语义聚合幻觉(SAH)，它发生在将帧级语义聚合到事件级语义组的过程中。由于SAH在长视频中因多个事件间语义复杂度增加而变得尤为重要，因此有必要分离并彻底研究这种幻觉的原因。为解决上述问题，我们引入了ELV-Halluc，这是第一个专门针对长视频幻觉的基准测试，能够系统研究SAH。我们的实验证实了SAH的存在，并表明它随语义复杂度的增加而增加。此外，我们发现模型在语义快速变化时更容易出现SAH。此外，我们讨论了缓解SAH的潜在方法。我们证明位置编码策略有助于减轻SAH，并进一步采用DPO策略来增强模型区分事件内外语义的能力。为此，我们整理了8K对抗性数据对，并在ELV-Halluc和Video-MME上取得了改进，包括SAH比率显著降低27.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video multimodal large language models (Video-MLLMs) have achieved remarkableprogress in video understanding. However, they remain vulnerable tohallucination-producing content inconsistent with or unrelated to video inputs.Previous video hallucination benchmarks primarily focus on short-videos. Theyattribute hallucinations to factors such as strong language priors, missingframes, or vision-language biases introduced by the visual encoder. While thesecauses indeed account for most hallucinations in short videos, they stilloversimplify the cause of hallucinations. Sometimes, models generate incorrectoutputs but with correct frame-level semantics. We refer to this type ofhallucination as Semantic Aggregation Hallucination (SAH), which arises duringthe process of aggregating frame-level semantics into event-level semanticgroups. Given that SAH becomes particularly critical in long videos due toincreased semantic complexity across multiple events, it is essential toseparate and thoroughly investigate the causes of this type of hallucination.To address the above issues, we introduce ELV-Halluc, the first benchmarkdedicated to long-video hallucination, enabling a systematic investigation ofSAH. Our experiments confirm the existence of SAH and show that it increaseswith semantic complexity. Additionally, we find that models are more prone toSAH on rapidly changing semantics. Moreover, we discuss potential approaches tomitigate SAH. We demonstrate that positional encoding strategy contributes toalleviating SAH, and further adopt DPO strategy to enhance the model's abilityto distinguish semantics within and across events. To support this, we curate adataset of 8K adversarial data pairs and achieve improvements on bothELV-Halluc and Video-MME, including a substantial 27.7% reduction in SAH ratio.</description>
      <author>example@mail.com (Hao Lu, Jiahao Wang, Yaolun Zhang, Ruohui Wang, Xuanyu Zheng, Yepeng Tang, Dahua Lin, Lewei Lu)</author>
      <guid isPermaLink="false">2508.21496v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>DLGAN : Time Series Synthesis Based on Dual-Layer Generative Adversarial Networks</title>
      <link>http://arxiv.org/abs/2508.21340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DLGAN是一种简单但有效的时间序列合成方法，通过双阶段生成过程和对抗学习，能够更好地保持时间序列的时间依赖性和特征信息。&lt;h4&gt;背景&lt;/h4&gt;时间序列合成是确保时间序列数据安全流通的有效方法，但现有方法基于随机序列进行时间建模，难以确保生成的时间序列中的时间依赖性，且难以准确捕捉原始时间序列的特征信息。&lt;h4&gt;目的&lt;/h4&gt;解决现有时间序列合成方法在保持时间依赖性和特征信息方面的不足，提出一种简单但有效的生成模型。&lt;h4&gt;方法&lt;/h4&gt;提出了名为DLGAN的生成模型，将时间序列生成过程分解为序列特征提取和序列重建两个阶段，形成时间序列自编码器进行监督学习，并使用生成对抗网络生成与真实时间序列特征向量一致的合成特征向量。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共数据集上的大量实验表明，该模型在各种评估指标上具有优越性。&lt;h4&gt;结论&lt;/h4&gt;DLGAN模型能够有效解决现有时间序列合成方法在保持时间依赖性和特征信息方面的问题。&lt;h4&gt;翻译&lt;/h4&gt;时间序列合成是确保时间序列数据安全流通的有效方法。现有时间序列合成方法通常基于随机序列进行时间建模来生成目标序列，这些方法往往难以确保生成的时间序列中的时间依赖性。此外，直接在随机序列上建模时间特征，使得准确捕捉原始时间序列的特征信息变得具有挑战性。为解决上述问题，我们提出了一种简单但有效的生成模型——双重层生成对抗网络，命名为DLGAN。该模型将时间序列生成过程分解为两个阶段：序列特征提取和序列重建。首先，这两个阶段形成一个完整的时间序列自编码器，使能够在原始时间序列上进行监督学习，确保重建过程能够恢复序列的时间依赖性。其次，使用生成对抗网络生成与真实时间序列特征向量一致的合成特征向量，确保生成器能够从真实时间序列中捕获时间特征。在四个公共数据集上的大量实验证明了该模型在各种评估指标上的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series synthesis is an effective approach to ensuring the securecirculation of time series data. Existing time series synthesis methodstypically perform temporal modeling based on random sequences to generatetarget sequences, which often struggle to ensure the temporal dependencies inthe generated time series. Additionally, directly modeling temporal features onrandom sequences makes it challenging to accurately capture the featureinformation of the original time series. To address the above issues, wepropose a simple but effective generative model \textbf{D}ual-\textbf{L}ayer\textbf{G}enerative \textbf{A}dversarial \textbf{N}etworks, named\textbf{DLGAN}. The model decomposes the time series generation process intotwo stages: sequence feature extraction and sequence reconstruction. First,these two stages form a complete time series autoencoder, enabling supervisedlearning on the original time series to ensure that the reconstruction processcan restore the temporal dependencies of the sequence. Second, a GenerativeAdversarial Network (GAN) is used to generate synthetic feature vectors thatalign with the real-time sequence feature vectors, ensuring that the generatorcan capture the temporal features from real time series. Extensive experimentson four public datasets demonstrate the superiority of this model acrossvarious evaluation metrics.</description>
      <author>example@mail.com (Xuan Hou, Shuhan Liu, Zhaohui Peng, Yaohui Chu, Yue Zhang, Yining Wang)</author>
      <guid isPermaLink="false">2508.21340v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Deep Residual Echo State Networks: exploring residual orthogonal connections in untrained Recurrent Neural Networks</title>
      <link>http://arxiv.org/abs/2508.21172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于时间残差连接的新型深度未经训练的循环神经网络DeepResESNs，解决了传统ESNs在长期信息处理方面的问题。&lt;h4&gt;背景&lt;/h4&gt;Echo State Networks (ESNs)是储备计算(RC)框架中一种特殊的未经训练的循环神经网络(RNNs)，因其快速高效的学习而广受欢迎，但在长期信息处理方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于时间残差连接的新型深度未经训练的RNNs，增强记忆能力和长期时间建模能力。&lt;h4&gt;方法&lt;/h4&gt;提出Deep Residual Echo State Networks (DeepResESNs)，利用层次化的未经训练的残差循环层，研究不同正交配置(随机生成和固定结构)对网络动力学的影响，并进行数学分析确保稳定动力学。&lt;h4&gt;主要发现&lt;/h4&gt;层次化的未经训练的残差循环层显著提高了记忆能力和长期时间建模能力，不同正交配置对网络动力学有不同影响。&lt;h4&gt;结论&lt;/h4&gt;在各种时间序列任务上的实验证明了DeepResESNs相对于传统浅层和深度RC方法的优越性。&lt;h4&gt;翻译&lt;/h4&gt;回声状态网络(ESNs)是储备计算(RC)框架中一种特殊的未经训练的循环神经网络(RNNs)，因其快速高效的学习而广受欢迎。然而，传统ESNs通常在长期信息处理方面存在困难。在本文中，我们介绍了一种基于时间残差连接的新型深度未经训练的RNNs，称为深度残差回声状态网络(DeepResESNs)。我们证明，利用层次化的未经训练的残差循环层显著提高了记忆能力和长期时间建模能力。对于时间残差连接，我们考虑了不同的正交配置，包括随机生成的和固定结构的配置，并研究了它们对网络动力学的影响。彻底的数学分析概述了确保DeepResESN稳定动力学的必要和充分条件。我们在各种时间序列任务上的实验展示了所提出的方法相对于传统浅层和深度RC的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Echo State Networks (ESNs) are a particular type of untrained RecurrentNeural Networks (RNNs) within the Reservoir Computing (RC) framework, popularfor their fast and efficient learning. However, traditional ESNs often strugglewith long-term information processing. In this paper, we introduce a novelclass of deep untrained RNNs based on temporal residual connections, calledDeep Residual Echo State Networks (DeepResESNs). We show that leveraging ahierarchy of untrained residual recurrent layers significantly boosts memorycapacity and long-term temporal modeling. For the temporal residualconnections, we consider different orthogonal configurations, includingrandomly generated and fixed-structure configurations, and we study theireffect on network dynamics. A thorough mathematical analysis outlines necessaryand sufficient conditions to ensure stable dynamics within DeepResESN. Ourexperiments on a variety of time series tasks showcase the advantages of theproposed approach over traditional shallow and deep RC.</description>
      <author>example@mail.com (Matteo Pinna, Andrea Ceni, Claudio Gallicchio)</author>
      <guid isPermaLink="false">2508.21172v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
      <link>http://arxiv.org/abs/2508.19650v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了Video-LevelGauge基准，专门用于系统评估大型视频语言模型中的位置偏差问题。该基准通过标准化探测和定制化上下文设置，能够灵活控制评估参数，包含438个精心策划的视频和大量问题，并评估了27个最先进的LVLM模型。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型在视频理解方面取得了显著进展，相应的评估基准也在不断发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了位置偏差这一关键但未被充分探索的方面。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门的基准系统，用于系统评估LVLM中的位置偏差，揭示模型在处理不同位置信息时的细微行为差异。&lt;h4&gt;方法&lt;/h4&gt;使用标准化探测和定制化上下文设置，灵活控制上下文长度、探测位置和上下文类型；引入结合统计测量和形态模式识别的综合分析方法；构建包含438个多类型视频的基准数据集，产生1,177个多项选择题和120个开放性问题；评估27个最先进的LVLM模型。&lt;h4&gt;主要发现&lt;/h4&gt;许多领先的开源模型表现出显著的位置偏差，通常倾向于头部或邻近内容；商业模型如Gemini2.5-Pro在整个视频序列中表现出一致的性能；对上下文长度、变化和模型规模的分析提供了减轻偏差的可行见解。&lt;h4&gt;结论&lt;/h4&gt;Video-LevelGauge基准有效揭示了LVLM中的位置偏差问题，为模型改进和偏差减轻提供了有价值的指导，有助于推动更公平、更全面的视频语言模型评估。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型在视频理解方面取得了显著进展，推动了相应评估基准的发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了细微行为，如上下文位置偏差，这是LVLM性能中一个关键但未被充分探索的方面。我们提出了Video-LevelGauge，一个专门用于系统评估LVLM中位置偏差的基准。我们采用标准化探测和定制化上下文设置，能够灵活控制上下文长度、探测位置和上下文类型，以模拟多样化的现实场景。此外，我们引入了一种结合统计测量和形态模式识别的综合分析方法来表征偏差。我们的基准包含438个人工策划的视频，涵盖多种类型，产生了1,177个高质量多项选择题和120个开放性问题，这些问题的有效性经过验证，能够有效暴露位置偏差。基于这些，我们评估了27个最先进的LVLM，包括商业和开源模型。我们的发现显示，许多领先的开源模型存在显著的位置偏差，通常表现出头部或邻近内容偏好。相比之下，商业模型如Gemini2.5-Pro在整个视频序列中表现出令人印象深刻的一致性能。对上下文长度、上下文变化和模型规模的进一步分析提供了减轻偏差和指导模型增强的可行见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding model enhancement .https://github.com/Cola-any/Video-LevelGauge</description>
      <author>example@mail.com (Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang)</author>
      <guid isPermaLink="false">2508.19650v3</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Video Continual Learning via Non-Parametric Deep Embedded Clustering</title>
      <link>http://arxiv.org/abs/2508.21773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The 36th British Machine Vision Conference (BMVC 2025),  Sheffield, UK&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种无监督视频持续学习的现实场景和解决方案，处理在学习一系列任务时既不提供任务边界也不提供标签的情况，通过非参数方法显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;视频是复杂且丰富的时空媒体信息，在许多应用中广泛使用，但在无监督持续学习中尚未得到充分探索。先前研究只专注于监督持续学习，依赖于标签和任务边界，而获取标记数据成本高昂且不切实际。与图像相比，视频处理带来额外的计算和内存需求，使无监督视频持续学习(uVCL)面临更多挑战。&lt;h4&gt;目的&lt;/h4&gt;研究无监督视频持续学习(uVCL)问题，填补现有研究空白，提出一种不依赖标签或任务边界的视频持续学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出通用基准实验协议，使用无监督视频Transformer网络提取深度嵌入视频特征的核密度估计(KDE)作为非参数概率表示。引入针对新任务数据的异常检测标准，动态扩展内存簇捕获新知识，并利用从先前任务迁移学习作为当前任务知识转移的初始状态。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在连续学习多个任务时显著提高了模型的性能，在三个标准视频动作识别数据集(UCF101、HMDB51和Something-to-Something V2)上进行了深入评估，未使用任何标签或类别边界。&lt;h4&gt;结论&lt;/h4&gt;该研究为无监督视频持续学习提供了新的解决方案，通过非参数方法和动态内存簇扩展，实现了在不使用标签或类别边界的情况下学习视频数据。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了无监督视频学习的一种现实场景，在学习一系列任务时既不提供任务边界也不提供标签。我们还为未充分探索的无监督视频持续学习问题提供了一种非参数解决方案。视频代表了复杂而丰富的时空媒体信息，在许多应用中广泛使用，但在无监督持续学习中尚未得到充分探索。先前的研究只专注于监督持续学习，依赖于标签和任务边界知识，而拥有标记数据成本高昂且不切实际。为了解决这一差距，我们研究了无监督视频持续学习(uVCL)。与图像相比，uVCL由于处理视频的额外计算和内存需求而带来了更多挑战。我们通过考虑在每个任务中学习非结构化视频数据类别，为uVCL引入了一种通用的基准实验协议。我们提议使用由无监督视频Transformer网络提取的深度嵌入视频特征的核密度估计(KDE)作为数据的非参数概率表示。我们为传入的新任务数据引入了一种异常检测标准，动态启用内存簇的扩展，旨在在学习一系列任务时捕获新知识。我们利用从先前任务迁移学习作为知识转移到当前学习任务的初始状态。我们发现，当连续学习许多任务时，所提出的方法显著提高了模型的性能。我们在三个标准视频动作识别数据集(UCF101、HMDB51和Something-to-Something V2)上进行了深入评估，没有使用任何标签或类别边界。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a realistic scenario for the unsupervised video learning whereneither task boundaries nor labels are provided when learning a succession oftasks. We also provide a non-parametric learning solution for theunder-explored problem of unsupervised video continual learning. Videosrepresent a complex and rich spatio-temporal media information, widely used inmany applications, but which have not been sufficiently explored inunsupervised continual learning. Prior studies have only focused on supervisedcontinual learning, relying on the knowledge of labels and task boundaries,while having labeled data is costly and not practical. To address this gap, westudy the unsupervised video continual learning (uVCL). uVCL raises morechallenges due to the additional computational and memory requirements ofprocessing videos when compared to images. We introduce a general benchmarkexperimental protocol for uVCL by considering the learning of unstructuredvideo data categories during each task. We propose to use the Kernel DensityEstimation (KDE) of deep embedded video features extracted by unsupervisedvideo transformer networks as a non-parametric probabilistic representation ofthe data. We introduce a novelty detection criterion for the incoming new taskdata, dynamically enabling the expansion of memory clusters, aiming to capturenew knowledge when learning a succession of tasks. We leverage the use oftransfer learning from the previous tasks as an initial state for the knowledgetransfer to the current learning task. We found that the proposed methodologysubstantially enhances the performance of the model when successively learningmany tasks. We perform in-depth evaluations on three standard video actionrecognition datasets, including UCF101, HMDB51, and Something-to-Something V2,without using any labels or class boundaries.</description>
      <author>example@mail.com (Nattapong Kurpukdee, Adrian G. Bors)</author>
      <guid isPermaLink="false">2508.21773v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Large Language Models with Network Optimization for Interactive and Explainable Supply Chain Planning: A Real-World Case Study</title>
      <link>http://arxiv.org/abs/2508.21622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种集成框架，结合传统网络优化模型与大语言模型，为供应链规划提供交互式、可解释和角色感知的决策支持。&lt;h4&gt;背景&lt;/h4&gt;复杂运筹学输出与商业利益相关者理解之间存在差距，需要更直观的方式来呈现决策支持信息。&lt;h4&gt;目的&lt;/h4&gt;弥合复杂运筹学模型与商业用户理解之间的鸿沟，通过自然语言摘要、情境可视化和定制化关键绩效指标提高决策支持效果。&lt;h4&gt;方法&lt;/h4&gt;开发核心优化模型解决多期多项目分销中心网络中的战术库存重新分配问题，采用混合整数公式；技术架构整合AI代理、RESTful API和动态用户界面，支持实时交互、配置更新和基于模拟的洞察。&lt;h4&gt;主要发现&lt;/h4&gt;案例研究表明，该系统能够防止缺货、降低成本并维持服务水平，显著改善了规划结果。&lt;h4&gt;结论&lt;/h4&gt;该框架有效结合了优化技术与自然语言处理能力，提高了供应链决策的可解释性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种集成框架，结合传统网络优化模型与大语言模型，为供应链规划提供交互式、可解释和角色感知的决策支持。该系统通过生成自然语言摘要、情境可视化和定制化关键绩效指标，弥合了复杂运筹学输出与商业利益相关者理解之间的差距。核心优化模型解决了多期多项目分销中心网络中的战术库存重新分配问题，采用混合整数公式。技术架构整合了AI代理、RESTful API和动态用户界面，支持实时交互、配置更新和基于模拟的洞察。案例研究表明，该系统通过防止缺货、降低成本和维持服务水平改善了规划结果。未来扩展包括集成私有LLM、迁移学习、强化学习和贝叶斯神经网络，以提高可解释性、适应性和实时决策能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an integrated framework that combines traditional networkoptimization models with large language models (LLMs) to deliver interactive,explainable, and role-aware decision support for supply chain planning. Theproposed system bridges the gap between complex operations research outputs andbusiness stakeholder understanding by generating natural language summaries,contextual visualizations, and tailored key performance indicators (KPIs). Thecore optimization model addresses tactical inventory redistribution across anetwork of distribution centers for multi-period and multi-item, using amixed-integer formulation. The technical architecture incorporates AI agents,RESTful APIs, and a dynamic user interface to support real-time interaction,configuration updates, and simulation-based insights. A case study demonstrateshow the system improves planning outcomes by preventing stockouts, reducingcosts, and maintaining service levels. Future extensions include integratingprivate LLMs, transfer learning, reinforcement learning, and Bayesian neuralnetworks to enhance explainability, adaptability, and real-timedecision-making.</description>
      <author>example@mail.com (Saravanan Venkatachalam)</author>
      <guid isPermaLink="false">2508.21622v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Adapting to Change: A Comparison of Continual and Transfer Learning for Modeling Building Thermal Dynamics under Concept Drifts</title>
      <link>http://arxiv.org/abs/2508.21615v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Currently under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在有限数据情况下，如何利用迁移学习和持续学习改进建筑热动力学模型的预测准确性，特别是在建筑特性随时间变化的情况下。&lt;h4&gt;背景&lt;/h4&gt;迁移学习(TL)是目前在仅有有限数据情况下建模建筑热动力学的最有效方法，但初始微调后如何继续更新模型仍不清楚，尤其是在建筑特性发生变化(如改造或占用情况改变)时。&lt;h4&gt;目的&lt;/h4&gt;比较持续学习(CL)和迁移学习(TL)策略以及从头开始训练的模型，用于建筑运行期间的热动力学建模，并研究如何整合新测量数据以提高预测准确性，应对概念漂移的挑战。&lt;h4&gt;方法&lt;/h4&gt;使用5-7年中欧单户住宅的模拟数据评估方法，包括改造和占用变化带来的概念漂移场景，并提出了一种名为'季节性记忆学习'(SML)的持续学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;SML策略比现有CL和TL方法提供更大的准确性改进，同时保持较低的计算工作量。无概念漂移时比初始微调提高28.1%，有概念漂移时提高34.9%。&lt;h4&gt;结论&lt;/h4&gt;季节性记忆学习是一种有效方法，能持续整合新测量数据，提高建筑热动力学模型预测准确性，特别是在建筑特性变化情况下。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习(TL)是目前在仅有有限数据情况下建模建筑热动力学的最有效方法。TL使用预训练模型并微调以适应特定目标建筑。然而，随着更多运行测量数据的收集，特别是在建筑特性变化后，如何在初始微调后继续更新模型仍不清楚。机器学习文献中，持续学习(CL)方法用于更新变化系统的模型。TL方法也可以通过在每个更新步骤重用预训练模型并用新测量数据微调来解决这一挑战。关于如何随时间整合新测量数据以提高预测准确性并应对建筑热动力学中概念漂移的挑战，仍缺乏全面研究。因此，本研究比较了几种CL和TL策略以及从头开始训练的模型，用于建筑运行期间的热动力学建模。使用5-7年中欧单户住宅的代表性模拟数据评估这些方法，包括改造和占用变化带来的概念漂移场景。我们提出了一种CL策略(季节性记忆学习)，它比现有CL和TL方法提供更大的准确性改进，同时保持较低的计算工作量。无概念漂移时，SML比初始微调的基准提高28.1%；有概念漂移时提高34.9%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer Learning (TL) is currently the most effective approach for modelingbuilding thermal dynamics when only limited data are available. TL uses apretrained model that is fine-tuned to a specific target building. However, itremains unclear how to proceed after initial fine-tuning, as more operationalmeasurement data are collected over time. This challenge becomes even morecomplex when the dynamics of the building change, for example, after a retrofitor a change in occupancy. In Machine Learning literature, Continual Learning(CL) methods are used to update models of changing systems. TL approaches canalso address this challenge by reusing the pretrained model at each update stepand fine-tuning it with new measurement data. A comprehensive study on how toincorporate new measurement data over time to improve prediction accuracy andaddress the challenges of concept drifts (changes in dynamics) for buildingthermal dynamics is still missing.  Therefore, this study compares several CL and TL strategies, as well as amodel trained from scratch, for thermal dynamics modeling during buildingoperation. The methods are evaluated using 5--7 years of simulated datarepresentative of single-family houses in Central Europe, including scenarioswith concept drifts from retrofits and changes in occupancy. We propose a CLstrategy (Seasonal Memory Learning) that provides greater accuracy improvementsthan existing CL and TL methods, while maintaining low computational effort.SML outperformed the benchmark of initial fine-tuning by 28.1\% without conceptdrifts and 34.9\% with concept drifts.</description>
      <author>example@mail.com (Fabian Raisch, Max Langtry, Felix Koch, Ruchi Choudhary, Christoph Goebel, Benjamin Tischler)</author>
      <guid isPermaLink="false">2508.21615v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Recabilities of Foundation Models: A Multi-Domain, Multi-Dataset Benchmark</title>
      <link>http://arxiv.org/abs/2508.21354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了RecBench-MD基准，对19个基础模型在15个数据集和10个不同领域的推荐能力进行了全面评估，发现领域内微调效果最佳，跨数据集迁移学习对新场景有实用价值，多领域训练能显著提升模型适应性。&lt;h4&gt;背景&lt;/h4&gt;现有基础模型在不同数据集和领域中的推荐能力综合评估对于推进推荐基础模型的发展至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入RecBench-MD基准，从零资源、多数据集和多领域的角度评估基础模型的推荐能力。&lt;h4&gt;方法&lt;/h4&gt;通过对跨越10个不同领域（包括电子商务、娱乐和社交媒体）的15个数据集中的19个基础模型进行广泛评估，确定这些模型在推荐任务中的关键特征。&lt;h4&gt;主要发现&lt;/h4&gt;领域内微调可实现最佳性能，跨数据集迁移学习为新推荐场景提供了有效的实际支持，多领域训练显著增强了基础模型的适应性。&lt;h4&gt;结论&lt;/h4&gt;所有代码和数据已公开发布，以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;对现有基础模型在不同数据集和领域中的推荐能力进行综合评估，对于推进推荐基础模型的发展至关重要。在本研究中，我们引入了RecBench-MD，这是一个新颖且全面的基准，旨在从零资源、多数据集和多领域的角度评估基础模型的推荐能力。通过对跨越10个不同领域（包括电子商务、娱乐和社交媒体）的15个数据集中的19个基础模型进行广泛评估，我们确定了这些模型在推荐任务中的关键特征。我们的研究结果表明，领域内微调可实现最佳性能，而跨数据集迁移学习为新推荐场景提供了有效的实际支持。此外，我们观察到多领域训练显著增强了基础模型的适应性。所有代码和数据已公开发布，以促进未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Comprehensive evaluation of the recommendation capabilities of existingfoundation models across diverse datasets and domains is essential foradvancing the development of recommendation foundation models. In this study,we introduce RecBench-MD, a novel and comprehensive benchmark designed toassess the recommendation abilities of foundation models from a zero-resource,multi-dataset, and multi-domain perspective. Through extensive evaluations of19 foundation models across 15 datasets spanning 10 diverse domains --including e-commerce, entertainment, and social media -- we identify keycharacteristics of these models in recommendation tasks. Our findings suggestthat in-domain fine-tuning achieves optimal performance, while cross-datasettransfer learning provides effective practical support for new recommendationscenarios. Additionally, we observe that multi-domain training significantlyenhances the adaptability of foundation models. All code and data have beenpublicly released to facilitate future research.</description>
      <author>example@mail.com (Qijiong Liu, Jieming Zhu, Yingxin Lai, Xiaoyu Dong, Lu Fan, Zhipeng Bian, Zhenhua Dong, Xiao-Ming Wu)</author>
      <guid isPermaLink="false">2508.21354v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</title>
      <link>http://arxiv.org/abs/2508.18860v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了C-Flat方法，用于在持续学习中促进平坦的损失景观，以平衡对新任务的敏感性和保留过去知识的稳定性。作者还提出了C-Flat++框架，通过选择性平坦度驱动促进显著减少了C-Flat所需的更新成本。&lt;h4&gt;背景&lt;/h4&gt;在持续学习中，平衡对新任务的敏感性和保留过去知识的稳定性至关重要。锐度感知最小化在迁移学习中已被证明有效，并已被应用于持续学习中以提高记忆保留和学习效率。然而，仅依赖零阶锐度可能在某些情况下 favor 更尖锐的最小值而非更平坦的最小值，导致解决方案不够稳健且可能是次优的。&lt;h4&gt;目的&lt;/h4&gt;提出一种促进平坦损失景观的方法，专门针对持续学习进行优化，解决仅依赖零阶锐度可能带来的问题。&lt;h4&gt;方法&lt;/h4&gt;作者提出了C-Flat(Continual Flatness)方法，促进为持续学习量身定制的平坦损失景观。C-Flat提供即插即用的兼容性，可以轻松集成到代码管道中，只需进行最小修改。此外，作者提出了一个通用框架，将C-Flat集成到所有主要的持续学习范式中，并与损失最小值优化器和基于平坦最小值的持续学习方法进行了全面比较。作者还引入了C-Flat++，这是一个高效且有效的框架，利用选择性平坦度驱动促进，显著减少了C-Flat所需的更新成本。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，C-Flat在各种设置中都能持续提高性能。此外，在多种持续学习方法、数据集和场景进行的广泛实验证明了所提出方法的有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;C-Flat和C-Flat++是持续学习中的有效方法，能够提高性能并减少计算成本。&lt;h4&gt;翻译&lt;/h4&gt;在持续学习中平衡对新任务的敏感性和保留过去知识的稳定性至关重要。最近，锐度感知最小化在迁移学习中已被证明有效，并也被采用到持续学习中以提高记忆保留和学习效率。然而，在某些情况下仅依赖零阶锐度可能 favor 更尖锐的最小值而非更平坦的最小值，导致解决方案不够稳健且可能是次优的。在本文中，我们提出了C-Flat(Continual Flatness)方法，促进为持续学习量身定制的平坦损失景观。C-Flat提供即插即用的兼容性，可以轻松集成到代码管道中，只需进行最小修改。此外，我们提出了一个通用框架，将C-Flat集成到所有主要的持续学习范式中，并与损失最小值优化器和基于平坦最小值的持续学习方法进行了全面比较。我们的结果表明，C-Flat在各种设置中都能持续提高性能。此外，我们引入了C-Flat++，这是一个高效且有效的框架，利用选择性平坦度驱动促进，显著减少了C-Flat所需的更新成本。在多种持续学习方法、数据集和场景进行的广泛实验证明了我们提出方法的有效性和效率。代码可在https://github.com/WanNaa/C-Flat获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Balancing sensitivity to new tasks and stability for retaining past knowledgeis crucial in continual learning (CL). Recently, sharpness-aware minimizationhas proven effective in transfer learning and has also been adopted incontinual learning (CL) to improve memory retention and learning efficiency.However, relying on zeroth-order sharpness alone may favor sharper minima overflatter ones in certain settings, leading to less robust and potentiallysuboptimal solutions. In this paper, we propose \textbf{C}ontinual\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter losslandscapes tailored for CL. C-Flat offers plug-and-play compatibility, enablingeasy integration with minimal modifications to the code pipeline. Besides, wepresent a general framework that integrates C-Flat into all major CL paradigmsand conduct comprehensive comparisons with loss-minima optimizers andflat-minima-based CL methods. Our results show that C-Flat consistentlyimproves performance across a wide range of settings. In addition, we introduceC-Flat++, an efficient yet effective framework that leverages selectiveflatness-driven promotion, significantly reducing the update cost required byC-Flat. Extensive experiments across multiple CL methods, datasets, andscenarios demonstrate the effectiveness and efficiency of our proposedapproaches. Code is available at https://github.com/WanNaa/C-Flat.</description>
      <author>example@mail.com (Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun)</author>
      <guid isPermaLink="false">2508.18860v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>CE-RS-SBCIT A Novel Channel Enhanced Hybrid CNN Transformer with Residual, Spatial, and Boundary-Aware Learning for Brain Tumor MRI Analysis</title>
      <link>http://arxiv.org/abs/2508.17128v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 Pages, 12 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一种新型混合框架CE-RS-SBCIT，用于脑肿瘤的MRI图像检测和分类，解决了传统方法的计算成本高、对微小对比度变化敏感、结构异质性和纹理不一致等问题。&lt;h4&gt;背景&lt;/h4&gt;脑肿瘤是最致命的人类疾病之一，早期检测和准确分类对有效诊断和治疗计划至关重要。虽然基于深度学习的计算机辅助诊断(CADx)系统已显示出显著进展，但传统卷积神经网络(CNNs)和Transformer仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决传统CNNs和Transformer在脑肿瘤诊断中面临的挑战，包括高计算成本、敏感性、结构异质性和纹理不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为CE-RS-SBCIT的新型混合框架，整合了基于残差和空间学习的CNNs与基于Transformer的模块。该框架包含四个核心创新：平滑和基于边界的CNN集成Transformer(SBCIT)、定制的残差和空间学习CNNs、通道增强(CE)策略和新空间注意力机制。在Kaggle和Figshare上的具有挑战性的MRI数据集上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;框架利用局部细粒度和全局上下文线索，通过SBCIT实现高效全局特征建模，增强的残差和空间CNNs丰富了表示空间，CE模块放大了判别性通道并减少冗余，空间注意力机制强调不同肿瘤类别间的细微对比度和纹理变化。&lt;h4&gt;结论&lt;/h4&gt;该框架在具有挑战性的MRI数据集上表现出卓越的性能，达到了98.30%的准确率、98.08%的敏感性、98.25%的F1分数和98.43%的精确度。&lt;h4&gt;翻译&lt;/h4&gt;脑肿瘤仍然是最致命的人类疾病之一，早期检测和准确分类对有效诊断和治疗计划至关重要。尽管基于深度学习的计算机辅助诊断(CADx)系统已显示出显著进展，然而传统卷积神经网络(CNNs)和Transformer持续面临挑战，包括高计算成本、对微小对比度变化的敏感性、MRI数据中的结构异质性和纹理不一致性。因此，引入了一种新型混合框架CE-RS-SBCIT，整合了基于残差和空间学习的CNNs与基于Transformer的模块。所提出的框架通过四个核心创新利用局部细粒度和全局上下文线索：(i)一种平滑和基于边界的CNN集成Transformer(SBCIT)，(ii)定制的残差和空间学习CNNs，(iii)一种通道增强(CE)策略，以及(iv)一种新的空间注意力机制。开发的SBCIT采用主干卷积和上下文交互Transformer块，具有系统化的平滑和边界操作，能够实现高效的全局特征建模。此外，通过辅助迁移学习的特征图增强的残差和空间CNNs丰富了表示空间，而CE模块放大了判别性通道并减少了冗余。进一步地，空间注意力机制选择性地强调了不同肿瘤类别之间的细微对比度和纹理变化。在Kaggle和Figshare上具有挑战性的MRI数据集上的广泛评估，包括胶质瘤、脑膜瘤、垂体肿瘤和健康对照组，展示了卓越的性能，达到了98.30%的准确率、98.08%的敏感性、98.25%的F1分数和98.43%的精确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain tumors remain among the most lethal human diseases, where earlydetection and accurate classification are critical for effective diagnosis andtreatment planning. Although deep learning-based computer-aided diagnostic(CADx) systems have shown remarkable progress. However, conventionalconvolutional neural networks (CNNs) and Transformers face persistentchallenges, including high computational cost, sensitivity to minor contrastvariations, structural heterogeneity, and texture inconsistencies in MRI data.Therefore, a novel hybrid framework, CE-RS-SBCIT, is introduced, integratingresidual and spatial learning-based CNNs with transformer-driven modules. Theproposed framework exploits local fine-grained and global contextual cuesthrough four core innovations: (i) a smoothing and boundary-basedCNN-integrated Transformer (SBCIT), (ii) tailored residual and spatial learningCNNs, (iii) a channel enhancement (CE) strategy, and (iv) a novel spatialattention mechanism. The developed SBCIT employs stem convolution andcontextual interaction transformer blocks with systematic smoothing andboundary operations, enabling efficient global feature modeling. Moreover,Residual and spatial CNNs, enhanced by auxiliary transfer-learned feature maps,enrich the representation space, while the CE module amplifies discriminativechannels and mitigates redundancy. Furthermore, the spatial attention mechanismselectively emphasizes subtle contrast and textural variations across tumorclasses. Extensive evaluation on challenging MRI datasets from Kaggle andFigshare, encompassing glioma, meningioma, pituitary tumors, and healthycontrols, demonstrates superior performance, achieving 98.30% accuracy, 98.08%sensitivity, 98.25% F1-score, and 98.43% precision.</description>
      <author>example@mail.com (Mirza Mumtaz Zahoor, Saddam Hussain Khan)</author>
      <guid isPermaLink="false">2508.17128v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>The Demon is in Ambiguity: Revisiting Situation Recognition with Single Positive Multi-Label Learning</title>
      <link>http://arxiv.org/abs/2508.21816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICDM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究场景识别中的动词分类问题，指出传统单标签方法无法处理视觉事件识别中的固有歧义，提出单正多标签学习框架和图增强动词多层感知器模型，在实验中取得超过3%的MAP提升。&lt;h4&gt;背景&lt;/h4&gt;场景识别是计算机视觉的基本任务，旨在通过识别关键事件及其相关实体从图像中提取结构化语义摘要。现有方法将动词分类视为单标签问题，但这种方法无法处理视觉事件识别中的固有歧义。&lt;h4&gt;目的&lt;/h4&gt;解决场景识别中动词分类的单标签问题，更好地处理视觉事件识别中的语义歧义。&lt;h4&gt;方法&lt;/h4&gt;1. 揭示动词分类本质上是多标签问题；2. 提出将动词分类重新表述为单正多标签学习问题；3. 设计多标签评估基准；4. 开发图增强动词多层感知器模型，结合图神经网络捕捉标签相关性，使用对抗训练优化决策边界。&lt;h4&gt;主要发现&lt;/h4&gt;1. 动词分类本质上是多标签问题，因为动词类别之间存在普遍的语义重叠；2. 完全用多标签标注大规模数据集不切实际；3. 提出的方法在真实世界数据集上实现超过3%的MAP提升，同时在传统准确率指标上保持竞争力。&lt;h4&gt;结论&lt;/h4&gt;通过将动词分类重新表述为单正多标签学习问题，并设计相应模型和评估基准，可更有效地处理视觉事件识别中的语义歧义，提高场景识别性能。&lt;h4&gt;翻译&lt;/h4&gt;场景识别是计算机视觉中的一个基本任务，旨在通过识别关键事件及其相关实体从图像中提取结构化的语义摘要。具体来说，给定输入图像，模型必须首先分类主要的视觉事件（动词分类），然后识别参与实体及其语义角色（语义角色标注），最后在图像中定位这些实体（语义角色定位）。现有方法将动词分类视为单标签问题，但我们通过全面分析表明，这种表述无法解决视觉事件识别中的固有歧义，因为多个动词类别都可以合理地描述同一图像。本文有三个关键贡献：首先，我们通过实证分析揭示动词分类本质上是多标签问题，因为动词类别之间存在普遍的语义重叠。其次，考虑到完全用多标签标注大规模数据集的不切实际性，我们提出将动词分类重新表述为单正多标签学习问题——这是场景识别研究中的一个新视角。第三，我们设计了一个全面的多标签评估基准，经过精心设计，可以公平评估模型在多标签设置下的性能。为解决单正多标签学习的挑战，我们进一步开发了图增强动词多层感知器，它结合图神经网络来捕捉标签相关性，并使用对抗训练来优化决策边界。在真实世界数据集上的大量实验表明，我们的方法实现了超过3%的MAP提升，同时在传统的top-1和top-5准确率指标上保持竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Context recognition (SR) is a fundamental task in computer vision that aimsto extract structured semantic summaries from images by identifying key eventsand their associated entities. Specifically, given an input image, the modelmust first classify the main visual events (verb classification), then identifythe participating entities and their semantic roles (semantic role labeling),and finally localize these entities in the image (semantic role localization).Existing methods treat verb classification as a single-label problem, but weshow through a comprehensive analysis that this formulation fails to addressthe inherent ambiguity in visual event recognition, as multiple verb categoriesmay reasonably describe the same image. This paper makes three keycontributions: First, we reveal through empirical analysis that verbclassification is inherently a multi-label problem due to the ubiquitoussemantic overlap between verb categories. Second, given the impracticality offully annotating large-scale datasets with multiple labels, we propose toreformulate verb classification as a single positive multi-label learning(SPMLL) problem - a novel perspective in SR research. Third, we design acomprehensive multi-label evaluation benchmark for SR that is carefullydesigned to fairly evaluate model performance in a multi-label setting. Toaddress the challenges of SPMLL, we futher develop the Graph Enhanced VerbMultilayer Perceptron (GE-VerbMLP), which combines graph neural networks tocapture label correlations and adversarial training to optimize decisionboundaries. Extensive experiments on real-world datasets show that our approachachieves more than 3\% MAP improvement while remaining competitive ontraditional top-1 and top-5 accuracy metrics.</description>
      <author>example@mail.com (Yiming Lin, Yuchen Niu, Shang Wang, Kaizhu Huang, Qiufeng Wang, Xiao-Bo Jin)</author>
      <guid isPermaLink="false">2508.21816v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>On the Hardness of Learning GNN-based SAT Solvers: The Role of Graph Ricci Curvature</title>
      <link>http://arxiv.org/abs/2508.21513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过图里奇曲率的几何视角解释了图神经网络在解决布尔可满足性问题时性能下降的现象，发现随机k-SAT公式的二部图本质上是负曲率的，且曲率随问题难度增加而减小，导致GNN求解器出现过度压缩现象。&lt;h4&gt;背景&lt;/h4&gt;图神经网络最近被证明可以解决布尔可满足性问题，它们通过操作逻辑公式的图表示来实现。然而，图神经网络在解决更困难的问题实例时性能急剧下降。&lt;h4&gt;目的&lt;/h4&gt;探究图神经网络在解决SAT问题时性能下降的根本原因，特别是这是否反映了基本架构局限性。&lt;h4&gt;方法&lt;/h4&gt;使用图里奇曲率作为几何分析工具，量化局部连接瓶颈，分析随机k-SAT公式推导出的二部图的曲率特性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 从随机k-SAT公式推导出的二部图本质上是负曲率的；2) 这种曲率随着问题实例难度的增加而减小；3) 基于图神经网络的SAT求解器受到过度压缩现象的影响，使得长程依赖关系无法被压缩到固定长度表示中；4) 曲率是问题复杂性的强指标，可用于预测求解器性能。&lt;h4&gt;结论&lt;/h4&gt;图里奇曲率提供了理解图神经网络在SAT问题上性能局限性的新视角，这些发现与现有求解器的设计原则相关，并为未来工作指明了有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近通过操作逻辑公式的图表示，展示了作为布尔可满足性问题求解器的潜力。然而，它们在更困难的问题实例上性能急剧下降，引发了这是否反映了基本架构局限性的问题。在本工作中，我们通过图里奇曲率的几何视角提供了解释，图里奇曲率量化了局部连接瓶颈。我们证明从随机k-SAT公式推导出的二部图本质上是负曲率的，并且这种曲率随着问题实例难度的增加而减小。基于此，我们表明基于图神经网络的SAT求解器受到过度压缩的影响，这是一种长程依赖关系无法被压缩到固定长度表示中的现象。我们在不同的SAT基准上通过经验验证了我们的主张，并确认曲率既是问题复杂性的强指标，也可用于预测性能。最后，我们将这些发现与现有求解器的设计原则联系起来，并概述了未来工作的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently shown promise as solvers forBoolean Satisfiability Problems (SATs) by operating on graph representations oflogical formulas. However, their performance degrades sharply on harderinstances, raising the question of whether this reflects fundamentalarchitectural limitations. In this work, we provide a geometric explanationthrough the lens of graph Ricci Curvature (RC), which quantifies localconnectivity bottlenecks. We prove that bipartite graphs derived from randomk-SAT formulas are inherently negatively curved, and that this curvaturedecreases with instance difficulty. Building on this, we show that GNN-basedSAT solvers are affected by oversquashing, a phenomenon where long-rangedependencies become impossible to compress into fixed-length representations.We validate our claims empirically across different SAT benchmarks and confirmthat curvature is both a strong indicator of problem complexity and can be usedto predict performance. Finally, we connect our findings to design principlesof existing solvers and outline promising directions for future work.</description>
      <author>example@mail.com (Geri Skenderi)</author>
      <guid isPermaLink="false">2508.21513v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>A Mixture of Experts Gating Network for Enhanced Surrogate Modeling in External Aerodynamics</title>
      <link>http://arxiv.org/abs/2508.21249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的元学习框架，通过专家混合模型(MoE)动态结合三种不同的神经网络架构，创建更准确、更强大的复合代理模型，用于汽车空气动力学的高保真CFD模拟预测。&lt;h4&gt;背景&lt;/h4&gt;高保真CFD模拟的计算成本在汽车设计和优化周期中仍然是一个显著的瓶颈。虽然基于机器学习的代理模型已成为加速空气动力学预测的有前途的替代方案，但该领域具有多样化的、快速发展的专业神经网络架构，没有单一模型显示出普遍的优越性。&lt;h4&gt;目的&lt;/h4&gt;提出一种元学习框架，利用架构多样性作为优势，通过动态组合不同专家模型的预测，创建更准确、更强大的复合代理模型。&lt;h4&gt;方法&lt;/h4&gt;提出了一种专家混合(MoE)模型，采用专用的门控网络来动态和最优地组合三种异构的、最先进的代理模型：DoMINO(可分解的多尺度神经算子)、X-MeshGraphNet(可扩展的多尺度图神经网络)和FigConvNet(分解的隐式全局卷积网络)。门控网络学习空间变化的加权策略，根据每个专家在预测表面压力和壁面剪应力场方面的局部性能分配可信度。为防止模型崩溃并鼓励专家平衡贡献，在训练损失函数中集成了熵正则化项。整个系统在DrivAerML数据集上进行训练和验证。&lt;h4&gt;主要发现&lt;/h4&gt;定量结果表明，MoE模型在预测误差方面实现了显著减少，不仅在所有评估的物理量上优于集成平均，而且优于最准确的单个专家模型。&lt;h4&gt;结论&lt;/h4&gt;这项研究建立了MoE框架作为一种强大而有效的策略，通过协同结合专业架构的互补优势，创建更强大、更准确的复合代理模型。&lt;h4&gt;翻译&lt;/h4&gt;与高保真CFD模拟相关的计算成本仍然是汽车设计和优化周期中的一个显著瓶颈。虽然基于机器学习的代理模型已成为加速空气动力学预测的一个有前途的替代方案，但该领域的特点是多样化的、快速发展的专业神经网络架构，没有单一模型显示出普遍的优越性。本文引入了一种新颖的元学习框架，利用这种架构多样性作为优势。我们提出了一种专家混合(MoE)模型，采用专用的门控网络来动态和最优地组合三种异构的、最先进的代理模型的预测：DoMINO(可分解的多尺度神经算子)、X-MeshGraphNet(可扩展的多尺度图神经网络)和FigConvNet(分解的隐式全局卷积网络)。门控网络学习空间变化的加权策略，根据每个专家在预测表面压力和壁面剪应力场方面的局部性能分配可信度。为了防止模型崩溃并鼓励专家平衡贡献，我们在训练损失函数中集成了熵正则化项。整个系统在DrivAerML数据集上进行训练和验证，该数据集是一个大规模的、公开的高保真CFD模拟基准，用于汽车空气动力学。定量结果表明，MoE模型在预测误差方面实现了显著减少，不仅在所有评估的物理量上优于集成平均，而且优于最准确的单个专家模型。这项工作建立了MoE框架作为一种强大而有效的策略，通过协同结合专业架构的互补优势，创建更强大、更准确的复合代理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The computational cost associated with high-fidelity CFD simulations remainsa significant bottleneck in the automotive design and optimization cycle. WhileML-based surrogate models have emerged as a promising alternative to accelerateaerodynamic predictions, the field is characterized by a diverse and rapidlyevolving landscape of specialized neural network architectures, with no singlemodel demonstrating universal superiority. This paper introduces a novelmeta-learning framework that leverages this architectural diversity as astrength. We propose a Mixture of Experts (MoE) model that employs a dedicatedgating network to dynamically and optimally combine the predictions from threeheterogeneous, state-of-the-art surrogate models: DoMINO, a decomposablemulti-scale neural operator; X-MeshGraphNet, a scalable multi-scale graphneural network; and FigConvNet, a factorized implicit global convolutionnetwork. The gating network learns a spatially-variant weighting strategy,assigning credibility to each expert based on its localized performance inpredicting surface pressure and wall shear stress fields. To prevent modelcollapse and encourage balanced expert contributions, we integrate an entropyregularization term into the training loss function. The entire system istrained and validated on the DrivAerML dataset, a large-scale, public benchmarkof high-fidelity CFD simulations for automotive aerodynamics. Quantitativeresults demonstrate that the MoE model achieves a significant reduction in L-2prediction error, outperforming not only the ensemble average but also the mostaccurate individual expert model across all evaluated physical quantities. Thiswork establishes the MoE framework as a powerful and effective strategy forcreating more robust and accurate composite surrogate models by synergisticallycombining the complementary strengths of specialized architectures.</description>
      <author>example@mail.com (Mohammad Amin Nabian, Sanjay Choudhry)</author>
      <guid isPermaLink="false">2508.21249v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Molecular Machine Learning in Chemical Process Design</title>
      <link>http://arxiv.org/abs/2508.20527v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了分子机器学习在化学过程工程领域的应用，展示了其在物质性质预测和新分子结构探索方面的潜力，并讨论了将其整合到过程设计和优化中的方法。&lt;h4&gt;背景&lt;/h4&gt;分子机器学习在化学过程工程领域是一个新兴研究方向，已在纯物质及其混合物的性质预测和探索新分子结构方面显示出巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;回顾当前最先进的分子机器学习模型，讨论进一步发展的研究方向，探索在化学过程规模上利用分子机器学习的可能性，并讨论如何将其整合到过程设计和优化公式中。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络和transformer等机器学习方法，通过混合或物理信息的方式整合物理化学知识，将分子机器学习整合到过程设计和优化公式中。&lt;h4&gt;主要发现&lt;/h4&gt;分子机器学习能提供高度准确的物质性质预测，能探索化学空间发现新分子结构，在化学过程规模上的应用是高度期望但尚未充分探索的领域，能加速新分子和过程的识别。&lt;h4&gt;结论&lt;/h4&gt;创建分子和过程设计基准并在实践中验证候选分子是充分发挥分子机器学习潜力的关键，这可能需要与化学工业合作。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了在化学过程工程领域分子机器学习（ML）的观点。最近，分子机器学习在（i）提供纯物质及其混合物的高度准确预测，和（ii）探索化学空间以发现新的分子结构方面显示出巨大潜力。我们回顾了当前最先进的分子机器学习模型，并讨论了有希望进一步发展的研究方向。包括机器学习方法，如图神经网络和transformer，可以通过混合或物理信息的方式整合物理化学知识以得到进一步发展。然后，我们考虑在化学过程规模上利用分子机器学习，这是高度期望但相当未探索的。我们讨论了如何将分子机器学习整合到过程设计和优化公式中，有望加速新分子和过程的识别。为此，创建分子和过程设计基准并在实践中验证候选分子将是至关重要的，可能需要与化学工业合作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a perspective on molecular machine learning (ML) in the field ofchemical process engineering. Recently, molecular ML has demonstrated greatpotential in (i) providing highly accurate predictions for properties of purecomponents and their mixtures, and (ii) exploring the chemical space for newmolecular structures. We review current state-of-the-art molecular ML modelsand discuss research directions that promise further advancements. Thisincludes ML methods, such as graph neural networks and transformers, which canbe further advanced through the incorporation of physicochemical knowledge in ahybrid or physics-informed fashion. Then, we consider leveraging molecular MLat the chemical process scale, which is highly desirable yet rather unexplored.We discuss how molecular ML can be integrated into process design andoptimization formulations, promising to accelerate the identification of novelmolecules and processes. To this end, it will be essential to create moleculeand process design benchmarks and practically validate proposed candidates,possibly in collaboration with the chemical industry.</description>
      <author>example@mail.com (Jan G. Rittig, Manuel Dahmen, Martin Grohe, Philippe Schwaller, Alexander Mitsos)</author>
      <guid isPermaLink="false">2508.20527v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Memorization in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19352v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Version2, With updated code repo&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NCMemo框架，用于量化半监督节点分类中的标签记忆。研究发现记忆与图同质性呈反比关系，低同质性增加GNN的记忆倾向。分析表明，低同质性图中增加的记忆与GNN使用图结构的隐式偏差相关，且特征空间邻域中标签不一致性高的节点更易被记忆。通过图重连方法可以有效减少记忆而不影响性能，同时降低隐私风险。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络(DNNs)已被证明能够记忆训练数据，但类似的分析对于图神经网络(GNNs)在很大程度上仍然未被探索。&lt;h4&gt;目的&lt;/h4&gt;介绍NCMemo（Node Classification Memorization），这是第一个用于量化半监督节点分类中标签记忆的框架。&lt;h4&gt;方法&lt;/h4&gt;建立记忆与图同质性之间的关系；分析GNN训练动态；研究特征空间邻域中标签不一致性对记忆的影响；探索图重连作为减少记忆的方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 同质性较低显著增加记忆，表明GNN依赖于记忆来学习同质性较低的图；2. 低同质性图中增加的记忆与GNN在学习过程中使用图结构的隐式偏差紧密相关；3. 在特征空间邻域中具有较高标签不一致性的节点更容易被记忆；4. 图重连方法可以有效减少记忆而不损害模型性能；5. 该方法降低了先前记忆数据点的隐私风险。&lt;h4&gt;结论&lt;/h4&gt;这项工作不仅推进了对GNN学习的理解，还支持了更多隐私保护的GNN部署。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNNs)已被证明能够记忆其训练数据，然而类似的分析对于图神经网络(GNNs)在很大程度上仍然未被探索。我们引入了NCMemo（Node Classification Memorization），这是第一个用于量化半监督节点分类中标签记忆的框架。我们首先建立了记忆与图同质性之间的反比关系，即连接节点具有相似标签/属性的特性。我们发现较低的同质性显著增加了记忆，表明GNN依赖于记忆来学习同质性较低的图。其次，我们分析了GNN训练动态。我们发现低同质性图中增加的记忆与GNN在学习过程中使用图结构的隐式偏差紧密相关。在低同质性情况下，这种结构信息较少，因此导致节点标签的记忆以最小化训练损失。最后，我们表明在特征空间邻域中具有较高标签不一致性的节点更容易被记忆。基于我们对图同质性与记忆之间联系的理解，我们研究了图重连作为减少记忆的方法。我们的结果表明这种方法可以有效减少记忆而不损害模型性能。此外，我们表明它在实践中降低了先前记忆数据点的隐私风险。因此，我们的工作不仅推进了对GNN学习的理解，还支持了更多隐私保护的GNN部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have been shown to memorize their training data,yet similar analyses for graph neural networks (GNNs) remain largelyunder-explored. We introduce NCMemo (Node Classification Memorization), thefirst framework to quantify label memorization in semi-supervised nodeclassification. We first establish an inverse relationship between memorizationand graph homophily, i.e., the property that connected nodes share similarlabels/features. We find that lower homophily significantly increasesmemorization, indicating that GNNs rely on memorization to learn lesshomophilic graphs. Secondly, we analyze GNN training dynamics. We find that theincreased memorization in low homophily graphs is tightly coupled to the GNNs'implicit bias on using graph structure during learning. In low homophilyregimes, this structure is less informative, hence inducing memorization of thenode labels to minimize training loss. Finally, we show that nodes with higherlabel inconsistency in their feature-space neighborhood are significantly moreprone to memorization. Building on our insights into the link between graphhomophily and memorization, we investigate graph rewiring as a means tomitigate memorization. Our results demonstrate that this approach effectivelyreduces memorization without compromising model performance. Moreover, we showthat it lowers the privacy risk for previously memorized data points inpractice. Thus, our work not only advances understanding of GNN learning butalso supports more privacy-preserving GNN deployment.</description>
      <author>example@mail.com (Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch)</author>
      <guid isPermaLink="false">2508.19352v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>SYNBUILD-3D: A large, multi-modal, and semantically rich synthetic dataset of 3D building models at Level of Detail 4</title>
      <link>http://arxiv.org/abs/2508.21169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了SYNBUILD-3D，一个包含超过620万个合成3D住宅建筑的大规模、多样化、多模态数据集，每个建筑通过三种模态表示，旨在解决自动生成准确且语义丰富的3D建筑模型的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D建筑模型在建筑、能源模拟和导航应用中至关重要，但由于缺乏大规模公开标注数据，自动生成准确且语义丰富的3D建筑仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;创建一个大规模、多样化的多模态3D建筑数据集，以促进自动生成准确且语义丰富的3D建筑模型的研究。&lt;h4&gt;方法&lt;/h4&gt;受计算机视觉中合成数据成功的启发，作者创建了SYNBUILD-3D数据集，包含超过620万个合成3D住宅建筑(LoD 4)，每个建筑通过三种模态表示：语义丰富的3D线框图、平面图图像和类似LiDAR的屋顶点云，并提供语义注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过SYNBUILD-3D的三模态特性，可以开发新的生成式AI算法，自动创建LoD 4级别的3D建筑模型，同时满足预定义的平面图布局和几何形状，并强制语义-几何一致性。&lt;h4&gt;结论&lt;/h4&gt;SYNBUILD-3D数据集的发布为3D建筑模型的自动生成研究提供了重要资源，数据集和代码样本已在GitHub上公开可用。&lt;h4&gt;翻译&lt;/h4&gt;三维建筑模型对于建筑、能源模拟和导航应用至关重要。然而，由于公共领域缺乏大规模标注数据集，自动生成准确且语义丰富的三维建筑仍然是一个重大挑战。受计算机视觉中合成数据成功的启发，我们引入了SYNBUILD-3D，这是一个大规模、多样化且多模态的数据集，包含超过620万个细节级别(LoD)为4的合成三维住宅建筑。在该数据集中，每个建筑通过三种不同的模态表示：细节级别为4的语义丰富的三维线框图(模态I)、相应的平面图图像(模态II)以及类似LiDAR的屋顶点云(模态III)。每个建筑线框的语义注释来自相应的平面图，包括房间、门和窗户的信息。通过其三模态特性，未来的工作可以利用SYNBUILD-3D开发新的生成式AI算法，自动创建细节级别为4的三维建筑模型，同时满足预定义的平面图布局和屋顶几何形状，并强制语义-几何一致性。数据集和代码样本已在https://github.com/kdmayer/SYNBUILD-3D公开可用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决缺乏大规模、语义丰富的3D建筑数据集问题，特别是在Level of Detail 4级别。这个问题很重要，因为3D建筑模型对建筑、能源模拟和导航等应用至关重要，而缺乏高质量数据集限制了自动生成准确且语义丰富的3D建筑的AI算法发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D建筑数据集的局限性，特别是LoD 4级别数据集的缺乏。他们受计算机视觉中合成数据成功的启发，设计了多阶段管道：程序化生成建筑外部(基于Biljecki等人的工作[22]进行定制)，使用AI驱动的平面图生成器(基于Wu等人的工作[21])，开发自定义算法处理非曼哈顿几何(参考Liu等人的工作[23]但进行修改)，以及对齐和堆叠楼层平面图。通过结合现有工作的优点并解决其局限性，作者创建了一个全新的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多模态、语义丰富的LoD 4级别3D建筑数据集，通过程序化生成和AI驱动的平面图生成相结合的方式创建多样化建筑模型，并确保建筑内部和外部的几何和语义一致性。整体流程分为四步：1)程序化生成随机化建筑外部；2)为每层生成足迹条件化的平面图图像；3)将平面图信息矢量化；4)对齐、拉伸和堆叠向量化的楼层体积。通过楼层平面图的排列组合显著扩展了数据集规模。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)规模和细节 - 包含620万个LoD 4级别建筑，每个都有详细语义标注；2)统一线框 - 提供集成内部-外部线框，具有一致几何和语义；3)多模态数据 - 同时提供3D线框、平面图图像和屋顶点云；4)开放和可扩展 - 发布完整数据集和生成管道。相比之前工作，SYNBUILD-3D规模更大(比之前数据集大100倍以上)，语义更丰富，支持更复杂的几何结构，并提供了更严格的质量保证措施。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SYNBUILD-3D提供了一个前所未有的620万建筑规模、多模态、语义丰富的LoD 4级别3D建筑数据集，通过结合程序化建筑外部生成和AI驱动的平面图生成，解决了3D建筑生成领域缺乏大规模高质量训练数据的关键瓶颈。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.25740/kz908vb7844&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D building models are critical for applications in architecture, energysimulation, and navigation. Yet, generating accurate and semantically rich 3Dbuildings automatically remains a major challenge due to the lack oflarge-scale annotated datasets in the public domain. Inspired by the success ofsynthetic data in computer vision, we introduce SYNBUILD-3D, a large, diverse,and multi-modal dataset of over 6.2 million synthetic 3D residential buildingsat Level of Detail (LoD) 4. In the dataset, each building is representedthrough three distinct modalities: a semantically enriched 3D wireframe graphat LoD 4 (Modality I), the corresponding floor plan images (Modality II), and aLiDAR-like roof point cloud (Modality III). The semantic annotations for eachbuilding wireframe are derived from the corresponding floor plan images andinclude information on rooms, doors, and windows. Through its tri-modal nature,future work can use SYNBUILD-3D to develop novel generative AI algorithms thatautomate the creation of 3D building models at LoD 4, subject to predefinedfloor plan layouts and roof geometries, while enforcing semantic-geometricconsistency. Dataset and code samples are publicly available athttps://github.com/kdmayer/SYNBUILD-3D.</description>
      <author>example@mail.com (Kevin Mayer, Alex Vesel, Xinyi Zhao, Martin Fischer)</author>
      <guid isPermaLink="false">2508.21169v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2508.20835v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PointDGRWKV，这是首个针对领域泛化点云分类(DG PCC)的RWKV框架，通过解决RWKV在点云应用中的空间失真和注意力漂移问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;领域泛化(DG)被用于增强点云分类(PCC)模型在未见领域的泛化能力，但先前基于卷积网络、Transformer或Mamba架构的方法存在感受野有限、计算成本高或长距离依赖建模不足的问题。&lt;h4&gt;目的&lt;/h4&gt;研究RWKV模型在DG PCC中的泛化能力，解决直接应用RWKV到点云分类时遇到的空间失真和注意力漂移问题。&lt;h4&gt;方法&lt;/h4&gt;提出了PointDGRWKV框架，包含两个关键模块：自适应几何Token Shift用于建模局部邻域结构提高几何上下文感知；跨域关键特征分布对齐用于减轻注意力漂移。这些模块在保持RWKV线性效率的同时增强了空间建模和跨域鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;直接应用RWKV到DG PCC遇到两个挑战：固定方向的token shift方法在非结构化点云上导致空间失真，削弱局部几何建模；Bi-WKV注意力通过指数加权放大跨域差异，导致注意力偏移和泛化能力下降。&lt;h4&gt;结论&lt;/h4&gt;PointDGRWKV通过创新性地解决RWKV在点云领域的应用问题，在多个基准测试上实现了DG PCC的最先进性能，证明了RWKV架构在点云分类领域的潜力。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化(DG)最近被探索用于增强点云分类(PCC)模型在未见领域的泛化能力。先前的工作基于卷积网络、Transformer或Mamba架构，要么受到感受野有限的限制，要么计算成本高，或长距离依赖建模不足。RWKV作为一种新兴架构，具有优越的线性复杂度、全局感受野和长距离依赖能力。在本文中，我们首次研究了RWKV模型在DG PCC中的泛化能力。我们发现直接将RWKV应用于DG PCC面临两个重大挑战：RWKV的固定方向token shift方法(如Q-Shift)在应用于非结构化点云时引入空间失真，削弱局部几何建模并降低鲁棒性。此外，RWKV中的Bi-WKV注意力通过指数加权放大关键分布中的轻微跨域差异，导致注意力偏移和泛化能力下降。为此，我们提出了PointDGRWKV，这是首个为DG PCC定制的RWKV框架。它引入了两个关键模块来增强空间建模和跨域鲁棒性，同时保持RWKV的线性效率。特别是，我们提出了自适应几何Token Shift来建模局部邻域结构以提高几何上下文感知能力。此外，跨域关键特征分布对齐被设计为通过跨域对齐关键特征分布来减轻注意力漂移。在多个基准上的广泛实验证明，PointDGRWKV在DG PCC上实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类模型在未见过的领域上的泛化能力问题。这个问题在现实中非常重要，因为自动驾驶、增强现实和机器人等应用场景中，点云数据可能因不同传感器、环境或扫描角度而产生分布差异，导致模型性能显著下降。研究这个问题能帮助模型更好地适应各种实际应用场景，无需针对每个新场景重新训练模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了直接应用RWKV到点云领域泛化时遇到的两个主要挑战：固定方向token shift导致的空间扭曲问题，以及Bi-WKV注意力机制对跨领域差异的放大效应。作者借鉴了RWKV架构的线性复杂度和全局感受场特性，参考了Vision-RWKV作为基线，并吸收了点云Mamba等方法在点云处理上的经验。基于这些分析，设计了两个关键模块：自适应几何Token Shift来解决局部几何建模问题，以及跨领域关键特征分布对齐来减轻注意力偏移。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过两个创新模块增强RWKV在点云数据上的表现：1) 自适应几何Token Shift基于点云空间特性构建局部邻域，动态整合结构特征，避免空间扭曲；2) 跨领域关键特征分布对齐通过在均值和协方差水平上对齐不同源领域的关键特征，减轻注意力偏移。整体流程包括：输入点云数据预处理、四阶段层次特征提取、应用AGT-Shift进行局部几何建模、使用CD-KDA对齐关键特征分布、通过Bi-WKV注意力机制处理特征，最后进行分类预测。训练时结合分类损失和分布对齐损失，推理时无需源领域数据。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个专门为点云领域泛化设计的RWKV-based框架；2) 自适应几何Token Shift模块，通过空间分区和加权特征聚合实现高效局部几何建模；3) 跨领域关键特征分布对齐模块，只对关键特征进行对齐以减轻注意力偏移。相比CNN方法，它具有全局感受场；相比Transformer方法，它具有线性计算复杂度；相比Mamba方法，它能更好地捕获长距离依赖；相比直接应用RWKV的方法，它解决了空间扭曲和跨领域注意力偏移问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了PointDGRWKV，一种创新的RWKV-based框架，通过自适应几何Token Shift和跨领域关键特征分布对齐两个模块，显著提高了点云分类模型在未知领域的泛化能力，同时保持了高效的线性计算复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain Generalization (DG) has been recently explored to enhance thegeneralizability of Point Cloud Classification (PCC) models toward unseendomains. Prior works are based on convolutional networks, Transformer or Mambaarchitectures, either suffering from limited receptive fields or highcomputational cost, or insufficient long-range dependency modeling. RWKV, as anemerging architecture, possesses superior linear complexity, global receptivefields, and long-range dependency. In this paper, we present the first workthat studies the generalizability of RWKV models in DG PCC. We find thatdirectly applying RWKV to DG PCC encounters two significant challenges: RWKV'sfixed direction token shift methods, like Q-Shift, introduce spatialdistortions when applied to unstructured point clouds, weakening localgeometric modeling and reducing robustness. In addition, the Bi-WKV attentionin RWKV amplifies slight cross-domain differences in key distributions throughexponential weighting, leading to attention shifts and degraded generalization.To this end, we propose PointDGRWKV, the first RWKV-based framework tailoredfor DG PCC. It introduces two key modules to enhance spatial modeling andcross-domain robustness, while maintaining RWKV's linear efficiency. Inparticular, we present Adaptive Geometric Token Shift to model localneighborhood structures to improve geometric context awareness. In addition,Cross-Domain key feature Distribution Alignment is designed to mitigateattention drift by aligning key feature distributions across domains. Extensiveexperiments on multiple benchmarks demonstrate that PointDGRWKV achievesstate-of-the-art performance on DG PCC.</description>
      <author>example@mail.com (Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan)</author>
      <guid isPermaLink="false">2508.20835v2</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Assessing Human Cooperation for Enhancing Social Robot Navigation</title>
      <link>http://arxiv.org/abs/2508.21455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;社交感知机器人导航是一种在人类环境中导航并遵守社交约束的规划范式，现有方法使用人类预测模型改进导航策略，但当人类行为不符合预期时会出现困难。&lt;h4&gt;背景&lt;/h4&gt;社交感知机器人导航是机器人在人类环境中导航并尝试遵守社交约束的规划范式，这些策略使用人类预测模型进一步改进，机器人在计算自身路径时考虑人类的潜在未来轨迹。&lt;h4&gt;目的&lt;/h4&gt;通过在适当时机进行有效沟通来解决机器人无法理解人类意图和合作性，以及人类不清楚机器人计划的问题，基于对场景和人类合作性的几何分析。&lt;h4&gt;方法&lt;/h4&gt;提供一种评估方法，提出能够区分合作型人类与非合作型人类的评估指标，展示如何使用几何推理来生成适当的语言回应或机器人动作。&lt;h4&gt;主要发现&lt;/h4&gt;机器人难以处理人类不符合预期的行为，因为缺乏对人类意图和合作性的理解，同时也缺乏向人类传达自身计划的能力。&lt;h4&gt;结论&lt;/h4&gt;通过基于场景几何分析和人类合作性的有效沟通，可以改善机器人在面对意外人类行为时的表现。&lt;h4&gt;翻译&lt;/h4&gt;社交感知机器人导航是一种规划范式，机器人在人类环境中导航并尝试在与场景中人类互动时遵守社交约束。这些导航策略使用人类预测模型进一步改进，机器人在计算自身路径时考虑人类的潜在未来轨迹。尽管这些策略显著改善了机器人的行为，但当人类行为不符合预期时，机器人仍会遇到困难。这是因为机器人无法理解人类的意图和合作性，而人类也不清楚机器人的计划。在本文中，我们旨在通过基于对场景几何分析和人类合作性的适时有效沟通来解决这一差距。我们提供了一种评估方法，并提出了一些能够区分合作型人类与非合作型人类的评估指标。此外，我们还展示了如何使用几何推理来生成适当的语言回应或机器人行动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Socially aware robot navigation is a planning paradigm where the robotnavigates in human environments and tries to adhere to social constraints whileinteracting with the humans in the scene. These navigation strategies werefurther improved using human prediction models, where the robot takes thepotential future trajectory of humans while computing its own. Though thesestrategies significantly improve the robot's behavior, it faces difficultiesfrom time to time when the human behaves in an unexpected manner. This happensas the robot fails to understand human intentions and cooperativeness, and thehuman does not have a clear idea of what the robot is planning to do. In thispaper, we aim to address this gap through effective communication at anappropriate time based on a geometric analysis of the context and humancooperativeness in head-on crossing scenarios. We provide an assessmentmethodology and propose some evaluation metrics that could distinguish acooperative human from a non-cooperative one. Further, we also show howgeometric reasoning can be used to generate appropriate verbal responses orrobot actions.</description>
      <author>example@mail.com (Hariharan Arunachalam, Phani Teja Singamaneni, Rachid Alami)</author>
      <guid isPermaLink="false">2508.21455v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>2COOOL: 2nd Workshop on the Challenge Of Out-Of-Label Hazards in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2508.21080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures, Accepted to ICCV 2025 Workshop on Out-of-Label  Hazards in Autonomous Driving (2COOOL)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇摘要介绍了一个名为2COOOL的研讨会，专注于解决自动驾驶中的新型场景挑战，特别是处理标签外危险的问题，旨在促进学术界和产业界的交流合作，推动更安全的自动驾驶技术发展。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉社区正在推进自动驾驶算法，将视觉洞察与传感器数据相结合对提高感知、决策、规划、预测、模拟和控制能力至关重要。然而，完全安全的自动驾驶汽车尚未实现，主要原因之一是无法有效处理新型场景，这是实际部署的关键障碍。&lt;h4&gt;目的&lt;/h4&gt;2COOOL研讨会为研究人员和行业专家提供了一个专门的论坛，用于推动新颖性处理的前沿发展，包括分布外危险检测、用于危险理解的视觉语言模型、新的基准测试和方法论，以及安全的自动驾驶实践。&lt;h4&gt;方法&lt;/h4&gt;研讨会将汇集学术界和产业界的参与者，借鉴异常检测、开放集识别、开放词汇建模、领域适应和相关领域的思想，激发新的危险规避算法和系统的发展。&lt;h4&gt;主要发现&lt;/h4&gt;新型场景处理是自动驾驶安全的关键挑战，需要开发新的方法和技术来应对实际部署中遇到的未知危险情况。&lt;h4&gt;结论&lt;/h4&gt;通过举办2COOOL研讨会，旨在促进自动驾驶技术中新型场景处理方法的发展，从而推动更安全的自动驾驶系统的实现。&lt;h4&gt;翻译&lt;/h4&gt;随着计算机视觉社区推进自动驾驶算法，将基于视觉的洞察与传感器数据相结合对于提高感知、决策、规划、预测、模拟和控制能力仍然至关重要。然而，我们必须问：为什么我们还没有完全安全的自动驾驶汽车？答案的关键部分之一在于处理新型场景，这是实际部署的最关键障碍之一。我们的2COOOL研讨会为研究人员和行业专家提供了一个专门的论坛，以推动新颖性处理的前沿发展，包括分布外危险检测、用于危险理解的视觉语言模型、新的基准测试和方法论，以及安全的自动驾驶实践。第二届自动驾驶中标签外危险挑战研讨会（2COOOL）将于2025年10月19日在夏威夷火奴鲁鲁举行的国际计算机视觉会议（ICCV 2025）上举行。我们的目标是激发危险规避新算法和系统的发展，借鉴异常检测、开放集识别、开放词汇建模、领域适应和相关领域的思想。在2025年冬季计算机视觉应用会议（WACV 2025）首次成功举办的基础上，本次研讨会将结合学术和产业界的参与。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the computer vision community advances autonomous driving algorithms,integrating vision-based insights with sensor data remains essential forimproving perception, decision making, planning, prediction, simulation, andcontrol. Yet we must ask: Why don't we have entirely safe self-driving carsyet? A key part of the answer lies in addressing novel scenarios, one of themost critical barriers to real-world deployment. Our 2COOOL workshop provides adedicated forum for researchers and industry experts to push the state of theart in novelty handling, including out-of-distribution hazard detection,vision-language models for hazard understanding, new benchmarking andmethodologies, and safe autonomous driving practices. The 2nd Workshop on theChallenge of Out-of-Label Hazards in Autonomous Driving (2COOOL) will be heldat the International Conference on Computer Vision (ICCV) 2025 in Honolulu,Hawaii, on October 19, 2025. We aim to inspire the development of newalgorithms and systems for hazard avoidance, drawing on ideas from anomalydetection, open-set recognition, open-vocabulary modeling, domain adaptation,and related fields. Building on the success of its inaugural edition at theWinter Conference on Applications of Computer Vision (WACV) 2025, the workshopwill feature a mix of academic and industry participation.</description>
      <author>example@mail.com (Ali K. AlShami, Ryan Rabinowitz, Maged Shoman, Jianwu Fang, Lukas Picek, Shao-Yuan Lo, Steve Cruz, Khang Nhut Lam, Nachiket Kamod, Lei-Lei Li, Jugal Kalita, Terrance E. Boult)</author>
      <guid isPermaLink="false">2508.21080v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>What Can We Learn from Harry Potter? An Exploratory Study of Visual Representation Learning from Atypical Videos</title>
      <link>http://arxiv.org/abs/2508.21770v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了异常非典型视频数据对开放世界学习的影响，通过收集包含多种异常类型的新视频数据集，并在分布外检测、新颖类别发现和零样本动作识别三个任务中验证了异常数据的有效性。&lt;h4&gt;背景&lt;/h4&gt;大多数现有研究关注封闭集中的常见典型数据，而开放世界中的新颖发现在视频领域探索不足。人类在面对不常见的新概念时表现出卓越的泛化和发现能力。&lt;h4&gt;目的&lt;/h4&gt;研究在学习过程中暴露异常不典型视频可能带来的影响，以及这些数据如何有益于开放世界学习。&lt;h4&gt;方法&lt;/h4&gt;收集了一个包含各种类型异常非典型数据（如科幻、动画等）的新视频数据集，将这些数据输入到模型训练过程中进行表示学习。关注开放世界学习中的三个关键任务：分布外检测、新颖类别发现和零样本动作识别。&lt;h4&gt;主要发现&lt;/h4&gt;1) 即使使用简单的学习方法，包含异常数据也能在各种设置中持续提高性能；2) 增加异常样本的类别多样性可以进一步提高分布外检测性能；3) 在新颖类别发现任务中，使用较小但语义更多样化的异常样本集比使用较大但更典型的数据集效果更好；4) 在零样本动作识别设置中，异常视频的语义多样性有助于模型更好地泛化到未见过的动作类别。&lt;h4&gt;结论&lt;/h4&gt;实验评估结果揭示了异常视频对开放世界视觉表示学习的好处，连同新提出的数据集，鼓励进一步研究这一方向。&lt;h4&gt;翻译&lt;/h4&gt;人类在面对不常见的新概念时通常表现出卓越的泛化和发现能力。然而，文献中的大多数现有研究关注来自封闭集的常见典型数据，开放世界中的新颖发现在视频中探索不足。在本文中，我们感兴趣的问题是：如果学习过程中暴露异常不典型的视频会怎样？为此，我们收集了一个包含各种类型异常非典型数据（例如科幻、动画等）的新视频数据集。为了研究此类异常数据如何有益于开放世界学习，我们将它们输入到表示学习的模型训练过程中。聚焦于开放世界学习中的三个关键任务：分布外检测、新颖类别发现和零样本动作识别，我们发现即使使用包含异常数据的基本学习方法也能在各种设置中持续提高性能。此外，我们发现增加异常样本的类别多样性可以进一步提高分布外检测性能。另外，在新颖类别发现任务中，使用较小但语义更多样化的异常样本集比使用较大但更典型的数据集效果更好。在零样本动作识别设置中，异常视频的语义多样性有助于模型更好地泛化到未见过的动作类别。我们在广泛实验评估中的这些发现揭示了异常视频对开放世界视觉表示学习的好处，连同新提出的数据集，鼓励进一步研究这一方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans usually show exceptional generalisation and discovery ability in theopen world, when being shown uncommon new concepts. Whereas most existingstudies in the literature focus on common typical data from closed sets,open-world novel discovery is under-explored in videos. In this paper, we areinterested in asking: \textit{What if atypical unusual videos are exposed inthe learning process?} To this end, we collect a new video dataset consistingof various types of unusual atypical data (\eg sci-fi, animation, \etc). Tostudy how such atypical data may benefit open-world learning, we feed them intothe model training process for representation learning. Focusing on three keytasks in open-world learning: out-of-distribution (OOD) detection, novelcategory discovery (NCD), and zero-shot action recognition (ZSAR), we foundthat even straightforward learning approaches with atypical data consistentlyimprove performance across various settings. Furthermore, we found thatincreasing the categorical diversity of the atypical samples further boosts OODdetection performance. Additionally, in the NCD task, using a smaller yet moresemantically diverse set of atypical samples leads to better performancecompared to using a larger but more typical dataset. In the ZSAR setting, thesemantic diversity of atypical videos helps the model generalise better tounseen action classes. These observations in our extensive experimentalevaluations reveal the benefits of atypical videos for visual representationlearning in the open world, together with the newly proposed dataset,encouraging further studies in this direction.</description>
      <author>example@mail.com (Qiyue Sun, Qiming Huang, Yang Yang, Hongjun Wang, Jianbo Jiao)</author>
      <guid isPermaLink="false">2508.21770v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>SatDINO: A Deep Dive into Self-Supervised Pretraining for Remote Sensing</title>
      <link>http://arxiv.org/abs/2508.21402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SatDINO模型，一种专门用于卫星图像表示学习的自监督学习方法，通过对比自预训练在多个数据集上取得了优于现有方法的结果。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为遥感领域的强大工具，因为该领域有大量未标记数据可用。&lt;h4&gt;目的&lt;/h4&gt;研究使用DINO对比自监督方法在遥感图像上进行预训练，并开发专门针对卫星图像表示学习的模型。&lt;h4&gt;方法&lt;/h4&gt;在多个数据集和多种测试设置下进行了广泛实验，评估SatDINO模型性能，并提供严格的消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;SatDINO性能优于基于掩码自编码器（MAE）的其他最先进方法，在多个基准测试中取得具有竞争力的结果。&lt;h4&gt;结论&lt;/h4&gt;SatDINO模型在遥感图像表示学习方面表现出色，所提出的增强方法（如GSD编码和自适应视图采样）可独立使用。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为遥感的强大工具，在那里有大量未标记的数据可用。在这项工作中，我们研究了使用DINO（一种对比自监督方法）在遥感图像上进行预训练。我们引入了SatDINO，这是一种专门为卫星图像表示学习定制的模型。通过在多个数据集和多种测试设置上的广泛实验，我们证明了SatDINO优于基于更常见的掩码自编码器（MAE）的其他最先进方法，并在多个基准测试中取得了具有竞争力的结果。我们还提供了严格的消融研究，评估了SatDINO的各个组件。最后，我们提出了一些新颖的增强方法，例如一种新的地面样本距离（GSD）编码方法和自适应视图采样方法。这些增强方法可以独立地应用于我们的SatDINO模型。我们的代码和训练模型可在以下网址获取：https://github.com/strakaj/SatDINO。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has emerged as a powerful tool for remote sensing,where large amounts of unlabeled data are available. In this work, weinvestigate the use of DINO, a contrastive self-supervised method, forpretraining on remote sensing imagery. We introduce SatDINO, a model tailoredfor representation learning in satellite imagery. Through extensive experimentson multiple datasets in multiple testing setups, we demonstrate that SatDINOoutperforms other state-of-the-art methods based on much more common maskedautoencoders (MAE) and achieves competitive results in multiple benchmarks.  We also provide a rigorous ablation study evaluating SatDINO's individualcomponents. Finally, we propose a few novel enhancements, such as a new way toincorporate ground sample distance (GSD) encoding and adaptive view sampling.These enhancements can be used independently on our SatDINO model. Our code andtrained models are available at: https://github.com/strakaj/SatDINO.</description>
      <author>example@mail.com (Jakub Straka, Ivan Gruber)</author>
      <guid isPermaLink="false">2508.21402v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Ontology Integration with Dual-Axis Propagation for Medical Concept Representation</title>
      <link>http://arxiv.org/abs/2508.21320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted as a full research paper at CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了LINKO，一种大型语言模型增强的整合本体学习框架，通过同时利用多个医学本体图，实现异构系统内部和跨系统的双轴知识传播，以增强医疗概念表示学习。&lt;h4&gt;背景&lt;/h4&gt;医学本体图通过结构化关系将外部知识映射到电子健康记录中的医疗代码。现有研究主要关注从单一本体系统或多个本体系统中获取领域知识，但未将它们整合到统一的学习结构中，导致概念表示学习局限于本体内部关系，忽略了跨本体连接。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时利用多个本体图的学习框架，通过双轴知识传播增强医疗概念表示学习，解决现有方法中跨本体连接被忽略的问题。&lt;h4&gt;方法&lt;/h4&gt;LINKO首先使用大型语言模型提供基于图检索增强的本体概念嵌入初始化，通过工程化提示和本体上下文进一步增强；然后联合学习不同本体图中的医疗概念，通过两个轴进行知识传播：(1)本体内部的垂直传播（跨层次结构）(2)本体之间的水平传播（在每一层并行）。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上的实验验证了LINKO优于最先进的基线方法。作为与现有EHR预测模型兼容的插件编码器，LINKO在数据有限和罕见疾病预测等场景中展示了更强的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;LINKO通过整合多个本体系统并进行双轴知识传播，有效增强了医疗概念表示学习，在多种场景下表现优于现有方法，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;医学本体图通过结构化关系将外部知识映射到电子健康记录中的医疗代码。通过利用领域批准的连接（如父子关系），预测模型可以通过整合相关概念的上下文信息生成更丰富的医疗概念表示。然而，现有文献主要关注整合来自单一本体系统的领域知识，或孤立地整合来自多个本体系统（如疾病、药物和程序）的领域知识，而没有将它们整合到统一的学习结构中。因此，概念表示学习通常局限于本体内部关系，而忽略了跨本体连接。在本文中，我们提出了LINKO，一种大型语言模型（LLM）增强的整合本体学习框架，通过同时利用多个本体图，实现异构系统内部和跨系统的双轴知识传播，以增强医疗概念表示学习。具体而言，LINKO首先使用大型语言模型提供本体概念嵌入的图检索增强初始化，通过包含概念描述的工程化提示，并进一步用本体上下文增强。其次，我们的方法通过在两个轴上进行知识传播，联合学习不同本体图中的医疗概念：(1)跨层次本体级别的内部本体垂直传播，以及(2)在每一层并行进行的内部本体水平传播。最后，通过在两个公共数据集上的广泛实验，我们验证了LINKO优于最先进基线的性能。作为与现有EHR预测模型兼容的插件编码器，LINKO进一步展示了在数据可用性有限和罕见疾病预测等场景中增强的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical ontology graphs map external knowledge to medical codes in electronichealth records via structured relationships. By leveraging domain-approvedconnections (e.g., parent-child), predictive models can generate richer medicalconcept representations by incorporating contextual information from relatedconcepts. However, existing literature primarily focuses on incorporatingdomain knowledge from a single ontology system, or from multiple ontologysystems (e.g., diseases, drugs, and procedures) in isolation, withoutintegrating them into a unified learning structure. Consequently, conceptrepresentation learning often remains limited to intra-ontology relationships,overlooking cross-ontology connections. In this paper, we propose LINKO, alarge language model (LLM)-augmented integrative ontology learning frameworkthat leverages multiple ontology graphs simultaneously by enabling dual-axisknowledge propagation both within and across heterogeneous ontology systems toenhance medical concept representation learning. Specifically, LINKO firstemploys LLMs to provide a graph-retrieval-augmented initialization for ontologyconcept embedding, through an engineered prompt that includes conceptdescriptions, and is further augmented with ontology context. Second, ourmethod jointly learns the medical concepts in diverse ontology graphs byperforming knowledge propagation in two axes: (1) intra-ontology verticalpropagation across hierarchical ontology levels and (2) inter-ontologyhorizontal propagation within every level in parallel. Last, through extensiveexperiments on two public datasets, we validate the superior performance ofLINKO over state-of-the-art baselines. As a plug-in encoder compatible withexisting EHR predictive models, LINKO further demonstrates enhanced robustnessin scenarios involving limited data availability and rare disease prediction.</description>
      <author>example@mail.com (Mohsen Nayebi Kerdabadi, Arya Hadizadeh Moghaddam, Dongjie Wang, Zijun Yao)</author>
      <guid isPermaLink="false">2508.21320v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Stage Fine-Tuning and Ensembling Strategy for Pancreatic Tumor Segmentation in Diagnostic and Therapeutic MRI</title>
      <link>http://arxiv.org/abs/2508.21775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, PANTHER Challenge submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于nnU-Net框架的胰腺导管腺癌MRI自动分割方法，通过多阶段级联预训练策略和指标感知的集成技术，在PANTHER挑战中取得了优异的分割性能。&lt;h4&gt;背景&lt;/h4&gt;胰腺导管腺癌（PDAC）从MRI的自动分割对临床工作流程至关重要，但受到肿瘤组织对比度差和标注数据稀少的阻碍。&lt;h4&gt;目的&lt;/h4&gt;参加PANTHER挑战，解决诊断性T1加权（任务1）和治疗性T2加权（任务2）的胰腺导管腺癌MRI分割问题。&lt;h4&gt;方法&lt;/h4&gt;基于nnU-Net框架，采用深度多阶段级联预训练策略，从通用解剖学基础模型开始，然后在CT胰腺病变数据集和目标MRI模态上逐步微调。通过五折交叉验证评估数据增强方案和训练计划，并构建定制的异构专家模型集成。&lt;h4&gt;主要发现&lt;/h4&gt;数据增强存在关键权衡：激进数据增强产生最高体积准确性，而默认增强提供更好边界精度（任务1达到MASD 5.46毫米和HD95 17.33毫米）。指标感知的集成策略有效，任务1达到肿瘤Dice分数0.661，任务2达到0.523。&lt;h4&gt;结论&lt;/h4&gt;该方法为在有限数据和复杂医学成像任务背景下开发专门的高性能模型提供了稳健的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;胰腺导管腺癌（PDAC）从MRI的自动分割对临床工作流程至关重要，但受到肿瘤组织对比度差和标注数据稀少的阻碍。本文详细介绍了我们参加PANTHER挑战的方案，解决了诊断性T1加权（任务1）和治疗性T2加权（任务2）的分割问题。我们的方法基于nnU-Net框架，利用深度多阶段级联预训练策略，从通用解剖学基础模型开始，然后在CT胰腺病变数据集和目标MRI模态上逐步微调。通过大量的五折交叉验证，我们系统评估了数据增强方案和训练计划。分析揭示了关键权衡，激进的数据增强产生最高的体积准确性，而默认增强产生更好的边界精度（任务1达到最先进的MASD 5.46毫米和HD95 17.33毫米）。对于最终提交，我们利用这一发现构建了定制的、异构的专家模型集成，基本上创建了一个专家混合。这种指标感知的集成策略被证明非常有效，任务1达到最高的交叉验证肿瘤Dice分数0.661，任务2达到0.523。我们的工作为在有限数据和复杂医学成像任务背景下开发专门的高性能模型提供了稳健的方法论（MIC-DKFZ团队）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated segmentation of Pancreatic Ductal Adenocarcinoma (PDAC) from MRI iscritical for clinical workflows but is hindered by poor tumor-tissue contrastand a scarcity of annotated data. This paper details our submission to thePANTHER challenge, addressing both diagnostic T1-weighted (Task 1) andtherapeutic T2-weighted (Task 2) segmentation. Our approach is built upon thennU-Net framework and leverages a deep, multi-stage cascaded pre-trainingstrategy, starting from a general anatomical foundation model and sequentiallyfine-tuning on CT pancreatic lesion datasets and the target MRI modalities.Through extensive five-fold cross-validation, we systematically evaluated dataaugmentation schemes and training schedules. Our analysis revealed a criticaltrade-off, where aggressive data augmentation produced the highest volumetricaccuracy, while default augmentations yielded superior boundary precision(achieving a state-of-the-art MASD of 5.46 mm and HD95 of 17.33 mm for Task 1).For our final submission, we exploited this finding by constructing custom,heterogeneous ensembles of specialist models, essentially creating a mix ofexperts. This metric-aware ensembling strategy proved highly effective,achieving a top cross-validation Tumor Dice score of 0.661 for Task 1 and 0.523for Task 2. Our work presents a robust methodology for developing specialized,high-performance models in the context of limited data and complex medicalimaging tasks (Team MIC-DKFZ).</description>
      <author>example@mail.com (Omer Faruk Durugol, Maximilian Rokuss, Yannick Kirchhoff, Klaus H. Maier-Hein)</author>
      <guid isPermaLink="false">2508.21775v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Domain Generalization in-the-Wild: Disentangling Classification from Domain-Aware Representations</title>
      <link>http://arxiv.org/abs/2508.21769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了基础模型CLIP在领域泛化方面的性能，发现CLIP在分布外数据上表现显著下降，并提出了CLIP-DCA方法来改善这一情况。该方法通过增强领域意识而非强制领域不变性，实现了更好的领域泛化性能。&lt;h4&gt;背景&lt;/h4&gt;评估基础模型如CLIP的领域泛化具有挑战性，因为网络规模的预训练数据可能已覆盖许多现有基准数据集，导致当前评估不够具有挑战性且无法充分测试真正未见数据的场景。&lt;h4&gt;目的&lt;/h4&gt;为了更好地评估CLIP在遇到具有挑战性未见数据时的真实领域泛化性能，研究提出了两种评估方法：在33个多样化数据集上评估和在CLIP上应用遗忘技术。&lt;h4&gt;方法&lt;/h4&gt;研究提出了CLIP-DCA方法，它使用单独的领域头和合成的多样化领域数据来识别和增强CLIP编码器中的领域意识，同时通过解耦领域特征来鼓励领域不变分类，而非像传统方法那样强制领域不变性。&lt;h4&gt;主要发现&lt;/h4&gt;CLIP在更多分布外数据集上的性能显著下降，而CLIP-DCA在具有挑战性的评估中显示出比现有方法显著的改进，特别是在更多分布外的数据集上。&lt;h4&gt;结论&lt;/h4&gt;增强领域意识而非强制领域不变性是改善基础模型在未见数据上性能的有效策略，CLIP-DCA通过解耦分类和领域感知表示实现了更好的领域泛化。&lt;h4&gt;翻译&lt;/h4&gt;评估基础模型如CLIP的领域泛化具有挑战性，因为网络规模的预训练数据可能覆盖了许多现有的基准数据集。因此，当前的领域泛化评估可能既不够具有挑战性，也无法充分测试真正未见数据的场景。为了更好地评估CLIP在遇到具有挑战性未见数据时的真实领域泛化性能，我们考虑两种方法：(1)在33个多样化数据集上评估，这些数据集在CLIP在ImageNet上微调后具有量化的分布外分数；(2)使用遗忘技术使CLIP'遗忘'某些领域作为近似方法。我们观察到CLIP在更多分布外数据集上的性能显著下降。为解决这个问题，我们提出了CLIP-DCA(解耦分类与增强领域感知表示)。我们的方法基于一个观察：虽然标准的领域不变性损失旨在使表示具有领域不变性，但这可能对基础模型有害，因为它会迫使模型丢弃对泛化有益的领域感知表示。相反，我们假设增强领域意识是基础模型中有效领域不变分类的前提。CLIP-DCA使用单独的领域头和合成的多样化领域数据来识别和增强CLIP编码器中的领域意识，同时通过从领域特征中解耦来鼓励领域不变分类。与现有方法相比，CLIP-DCA在这种具有挑战性的评估中显示出显著改进，尤其是在更多分布外的数据集上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating domain generalization (DG) for foundational models like CLIP ischallenging, as web-scale pretraining data potentially covers many existingbenchmarks. Consequently, current DG evaluation may neither be sufficientlychallenging nor adequately test genuinely unseen data scenarios. To betterassess the performance of CLIP on DG in-the-wild, a scenario where CLIPencounters challenging unseen data, we consider two approaches: (1) evaluatingon 33 diverse datasets with quantified out-of-distribution (OOD) scores afterfine-tuning CLIP on ImageNet, and (2) using unlearning to make CLIP `forget'some domains as an approximation. We observe that CLIP's performancedeteriorates significantly on more OOD datasets. To address this, we presentCLIP-DCA (Disentangling Classification from enhanced domain Awarerepresentations). Our approach is motivated by the observation that whilestandard domain invariance losses aim to make representations domain-invariant,this can be harmful to foundation models by forcing the discarding ofdomain-aware representations beneficial for generalization. We insteadhypothesize that enhancing domain awareness is a prerequisite for effectivedomain-invariant classification in foundation models. CLIP-DCA identifies andenhances domain awareness within CLIP's encoders using a separate domain headand synthetically generated diverse domain data. Simultaneously, it encouragesdomain-invariant classification through disentanglement from the domainfeatures. CLIP-DCA shows significant improvements within this challengingevaluation compared to existing methods, particularly on datasets that are moreOOD.</description>
      <author>example@mail.com (Ha Min Son, Zhe Zhao, Shahbaz Rezaei, Xin Liu)</author>
      <guid isPermaLink="false">2508.21769v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>UItron: Foundational GUI Agent with Advanced Perception and Planning</title>
      <link>http://arxiv.org/abs/2508.21767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了UItron，一个用于自动GUI代理的开源基础模型，具有先进的GUI感知、定位和规划能力。UItron强调系统数据工程和交互基础设施是推进GUI代理开发的基础组件。&lt;h4&gt;背景&lt;/h4&gt;GUI代理旨在实现移动/PC设备上的自动化操作，这是实现通用人工智能的重要任务。视觉语言模型(VLMs)的快速发展加速了GUI代理的发展，但构建GUI代理仍面临操作轨迹稀缺、交互基础设施有限以及基础模型初始能力受限等挑战。&lt;h4&gt;目的&lt;/h4&gt;介绍UItron，一个具有先进GUI感知、定位和规划能力的开源基础模型，并强调系统数据工程和交互基础设施作为推进GUI代理开发基础组件的必要性。&lt;h4&gt;方法&lt;/h4&gt;UItron系统研究数据工程策略增强训练效果，建立连接移动和PC设备的交互环境。采用监督微调处理各种GUI场景的感知和规划任务，开发课程强化学习框架实现复杂推理和探索。收集超过一百万步操作轨迹，覆盖前100个热门应用，构建离线和在线评估环境。&lt;h4&gt;主要发现&lt;/h4&gt;UItron在GUI感知、定位和规划基准测试中表现卓越，特别是在与中国顶级移动应用的交互能力方面突出。实验表明UItron在中国应用场景中取得显著进展，解决了现有解决方案普遍缺乏中文能力的问题。&lt;h4&gt;结论&lt;/h4&gt;UItron通过系统数据工程和交互基础设施的建立，显著提升了GUI代理能力，特别是在中文应用场景中，推动GUI代理更接近实际应用。&lt;h4&gt;翻译&lt;/h4&gt;GUI代理旨在实现移动/PC设备上的自动化操作，这是实现通用人工智能的重要任务。视觉语言模型的快速发展加速了GUI代理的发展，因为它们在视觉理解和任务规划方面具有强大能力。然而，构建GUI代理仍然是一项具有挑战性的任务，原因包括操作轨迹稀缺、交互基础设施可用性有限以及基础模型的初始能力受限。在这项工作中，我们介绍了UItron，这是一个用于自动GUI代理的开源基础模型，具有先进的GUI感知、定位和规划能力。UItron强调了系统数据工程和交互基础设施作为推进GUI代理开发基础组件的必要性。它不仅系统研究了一系列数据工程策略来增强训练效果，还建立了连接移动和PC设备的交互环境。在训练中，UItron采用监督微调方法，在各种GUI场景中处理感知和规划任务，然后开发课程强化学习框架，实现在线环境的复杂推理和探索。结果，UItron在GUI感知、定位和规划的基准测试中取得了卓越的性能。特别是，UItron突出了与中国顶级移动应用的交互能力，因为我们发现即使在最先进的解决方案中也普遍缺乏中文能力。为此，我们手动收集了超过一百万步的操作轨迹，覆盖了前100个最受欢迎的应用程序，并构建了离线和在线代理评估环境。实验结果表明，UItron在中国应用场景中取得了显著进展，推动GUI代理更接近实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; GUI agent aims to enable automated operations on Mobile/PC devices, which isan important task toward achieving artificial general intelligence. The rapidadvancement of VLMs accelerates the development of GUI agents, owing to theirpowerful capabilities in visual understanding and task planning. However,building a GUI agent remains a challenging task due to the scarcity ofoperation trajectories, the availability of interactive infrastructure, and thelimitation of initial capabilities in foundation models. In this work, weintroduce UItron, an open-source foundational model for automatic GUI agents,featuring advanced GUI perception, grounding, and planning capabilities. UItronhighlights the necessity of systemic data engineering and interactiveinfrastructure as foundational components for advancing GUI agent development.It not only systematically studies a series of data engineering strategies toenhance training effects, but also establishes an interactive environmentconnecting both Mobile and PC devices. In training, UItron adopts supervisedfinetuning over perception and planning tasks in various GUI scenarios, andthen develop a curriculum reinforcement learning framework to enable complexreasoning and exploration for online environments. As a result, UItron achievessuperior performance in benchmarks of GUI perception, grounding, and planning.In particular, UItron highlights the interaction proficiency with top-tierChinese mobile APPs, as we identified a general lack of Chinese capabilitieseven in state-of-the-art solutions. To this end, we manually collect over onemillion steps of operation trajectories across the top 100 most popular apps,and build the offline and online agent evaluation environments. Experimentalresults demonstrate that UItron achieves significant progress in Chinese appscenarios, propelling GUI agents one step closer to real-world application.</description>
      <author>example@mail.com (Zhixiong Zeng, Jing Huang, Liming Zheng, Wenkang Han, Yufeng Zhong, Lei Chen, Longrong Yang, Yingjie Chu, Yuzhi He, Lin Ma)</author>
      <guid isPermaLink="false">2508.21767v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>QZhou-Embedding Technical Report</title>
      <link>http://arxiv.org/abs/2508.21632v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了QZhou-Embedding，一个基于Qwen2.5-7B-Instruct基础模型构建的通用上下文文本嵌入模型，具有出色的文本表示能力。通过统一的多任务框架、数据合成管道和两阶段训练策略，该模型在多个基准测试中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;文本嵌入模型需要更高质量和更多样化的数据来提升性能，而大型语言模型(LLM)的生成能力可以用来优化嵌入模型的数据质量。&lt;h4&gt;目的&lt;/h4&gt;开发一个具有卓越文本表示能力的通用上下文文本嵌入模型，通过利用高质量、多样化的数据和先进的训练策略来提高检索模型的性能。&lt;h4&gt;方法&lt;/h4&gt;基于Qwen2.5-7B-Instruct基础模型构建；设计统一的多任务框架，包含专门的数据转换和训练策略；开发利用LLM API的数据合成管道，包括释义、增强和困难负例生成技术；采用两阶段训练策略：首先进行检索导向的预训练，然后进行全任务微调。&lt;h4&gt;主要发现&lt;/h4&gt;模型在MTEB和CMTEB基准测试中排名第一（2025年8月27日）；在重排序、聚类等任务上也取得了最先进性能；更高质量、更多样化的数据对于提升检索模型性能至关重要；利用LLM的生成能力可以进一步优化嵌入模型的数据质量。&lt;h4&gt;结论&lt;/h4&gt;通过高质量数据生成和先进训练策略，QZhou-Embedding在多个基准测试和任务中取得了最先进的结果，证明了数据质量和多样性对提升嵌入模型性能的重要性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了QZhou-Embedding，一个具有出色文本表示能力的通用上下文文本嵌入模型。基于Qwen2.5-7B-Instruct基础模型构建，我们设计了一个包含专门数据转换和训练策略的统一多任务框架。数据转换方案能够整合更多样化的文本训练数据集，而特定任务的训练策略则提高了模型的学习效率。我们开发了一个利用LLM API的数据合成管道，采用释义、增强和困难负例生成等技术，以提高训练集的语义丰富度和样本难度。此外，我们采用两阶段训练策略，包括初始的检索导向预训练和后续的全任务微调，使嵌入模型能够基于强大的检索性能扩展其能力。我们的模型在MTEB和CMTEB基准测试上取得了最先进的结果，在两个排行榜上均排名第一（2025年8月27日），同时在重排序、聚类等任务上也同时取得了最先进性能。我们的研究结果表明，更高质量、更多样化的数据对于推进检索模型性能至关重要，并且利用LLM的生成能力可以进一步优化嵌入模型突破的数据质量。我们的模型权重已在HuggingFace上以Apache 2.0许可证发布。为了可复现性，我们在GitHub上提供了评估代码和说明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present QZhou-Embedding, a general-purpose contextual text embedding modelwith exceptional text representation capabilities. Built upon theQwen2.5-7B-Instruct foundation model, we designed a unified multi-taskframework comprising specialized data transformation and training strategies.The data transformation scheme enables the incorporation of more diversetextual training datasets, while the task-specific training strategies enhancemodel learning efficiency. We developed a data synthesis pipeline leveragingLLM API, incorporating techniques such as paraphrasing, augmentation, and hardnegative example generation to improve the semantic richness and sampledifficulty of the training set. Additionally, we employ a two-stage trainingstrategy, comprising initial retrieval-focused pretraining followed byfull-task fine-tuning, enabling the embedding model to extend its capabilitiesbased on robust retrieval performance. Our model achieves state-of-the-artresults on the MTEB and CMTEB benchmarks, ranking first on both leaderboards(August 27 2025), and simultaneously achieves state-of-the-art performance ontasks including reranking, clustering, etc. Our findings demonstrate thathigher-quality, more diverse data is crucial for advancing retrieval modelperformance, and that leveraging LLMs generative capabilities can furtheroptimize data quality for embedding model breakthroughs. Our model weights arereleased on HuggingFace under Apache 2.0 license. For reproducibility, weprovide evaluation code and instructions on GitHub.</description>
      <author>example@mail.com (Peng Yu, En Xu, Bin Chen, Haibiao Chen, Yinfei Xu)</author>
      <guid isPermaLink="false">2508.21632v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Pathology and CT Imaging for Personalized Recurrence Risk Prediction in Renal Cancer</title>
      <link>http://arxiv.org/abs/2508.21581v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 2 figures, 1 table. Accepted at the Multimodal Learning and  Fusion Across Scales for Clinical Decision Support (ML-CDS) Workshop, MICCAI  2025. This is the submitted version with authors, affiliations, and  acknowledgements included; it has not undergone peer review or revisions. The  final version will appear in the Springer Lecture Notes in Computer Science  (LNCS) proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过整合术前CT影像和术后组织病理学全切片图像(WSI)，开发了一种多模态深度学习方法用于透明细胞肾细胞癌(ccRCC)的复发风险评估。研究发现基于病理学的模型优于影像学模型，而多模态融合策略进一步提高了预测性能，最佳模型接近传统Leibovich评分的临床标准。&lt;h4&gt;背景&lt;/h4&gt;透明细胞肾细胞癌的复发风险评估对术后监测和治疗决策至关重要。目前广泛使用的Leibovich评分在患者层面分辨率有限，且未包含影像信息，限制了其个性化预测能力。&lt;h4&gt;目的&lt;/h4&gt;评估通过整合术前CT和术后组织病理学全切片图像的多模态方法预测ccRCC复发的效果，并与传统Leibovich评分进行比较。&lt;h4&gt;方法&lt;/h4&gt;采用模块化深度学习框架，结合预训练编码器和基于Cox的生存建模，测试单模态、晚期融合和中间融合三种策略。在真实ccRCC患者队列中验证模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;1) 基于病理学图像的模型始终优于仅使用CT的模型，表明病理信息具有更强的预后价值；2) 中间融合策略表现最佳，其最优模型接近调整后的Leibovich评分；3) 随机平局处理显示离散化可能高估模型个性化性能；4) 放射学信息主要通过融合过程为模型提供价值。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的多模态整合方法用于个性化ccRCC风险预测具有可行性。未来应探索更先进的融合策略、扩大多模态数据集规模，并开发通用CT编码器以匹配病理学建模能力。&lt;h4&gt;翻译&lt;/h4&gt;透明细胞肾细胞癌(ccRCC)的复发风险评估对于指导术后监测和治疗至关重要。Leibovich评分仍然被广泛用于区分远处复发风险，但其患者层面的分辨率有限，且不包括影像信息。本研究通过整合术前计算机断层扫描(CT)和术后组织病理学全切片图像(WSI)来评估多模态复发预测。测试了具有预训练编码器和基于Cox的生存建模的模块化深度学习框架，涵盖了单模态、晚期融合和中间融合设置。在一个真实的ccRCC队列中，基于WSI的模型始终优于仅使用CT的模型，突显了病理学的预后强度。中间融合进一步提高了性能，最佳模型(TITAN-CONCH与ResNet-18)接近调整后的Leibovich评分。随机平局打破了缩小了临床基线与学习模型之间的差距，表明离散化可能夸大了个性化性能。使用简单的嵌入连接，放射学主要通过融合增加价值。这些发现证明了基于基础模型的多模态整合用于个性化ccRCC风险预测的可行性。未来的工作应该探索更具表现力的融合策略、更大的多模态数据集和通用CT编码器，以更好地匹配病理学建模能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recurrence risk estimation in clear cell renal cell carcinoma (ccRCC) isessential for guiding postoperative surveillance and treatment. The Leibovichscore remains widely used for stratifying distant recurrence risk but offerslimited patient-level resolution and excludes imaging information. This studyevaluates multimodal recurrence prediction by integrating preoperative computedtomography (CT) and postoperative histopathology whole-slide images (WSIs). Amodular deep learning framework with pretrained encoders and Cox-based survivalmodeling was tested across unimodal, late fusion, and intermediate fusionsetups. In a real-world ccRCC cohort, WSI-based models consistentlyoutperformed CT-only models, underscoring the prognostic strength of pathology.Intermediate fusion further improved performance, with the best model(TITAN-CONCH with ResNet-18) approaching the adjusted Leibovich score. Randomtie-breaking narrowed the gap between the clinical baseline and learned models,suggesting discretization may overstate individualized performance. Usingsimple embedding concatenation, radiology added value primarily through fusion.These findings demonstrate the feasibility of foundation model-based multimodalintegration for personalized ccRCC risk prediction. Future work should exploremore expressive fusion strategies, larger multimodal datasets, andgeneral-purpose CT encoders to better match pathology modeling capacity.</description>
      <author>example@mail.com (Daniël Boeke, Cedrik Blommestijn, Rebecca N. Wray, Kalina Chupetlovska, Shangqi Gao, Zeyu Gao, Regina G. H. Beets-Tan, Mireia Crispin-Ortuzar, James O. Jones, Wilson Silva, Ines P. Machado)</author>
      <guid isPermaLink="false">2508.21581v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Maybe you don't need a U-Net: convolutional feature upsampling for materials micrograph segmentation</title>
      <link>http://arxiv.org/abs/2508.21529v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种卷积神经网络上采样方法，解决了特征基础模型在显微图像处理中的局限性，实现了高效的高质量图像分割。&lt;h4&gt;背景&lt;/h4&gt;特征基础模型（通常是视觉Transformer）为图像提供丰富的语义描述符，适用于下游任务如交互式分割和目标检测。但这些基于块的描述符难以表示显微图像中的精细特征，也难以处理材料和生物图像分析中的大尺寸图像。&lt;h4&gt;目的&lt;/h4&gt;提高特征基础模型在处理显微图像精细特征和大尺寸图像时的效率和准确性，减少对标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;训练一个卷积神经网络来参考输入图像上采样低分辨率的基础模型特征，将此上采样器网络应用于各种显微镜图像的特征化和分割，包括植物细胞、锂离子电池阴极和有机晶体，无需进一步训练即可使用。&lt;h4&gt;主要发现&lt;/h4&gt;上采样特征的丰富性使得难以分割的阶段（如发丝裂纹）能够被有效分离；使用这些深度特征进行交互式分割比传统方法产生更高质量的分割结果，速度更快且需要的标签更少。&lt;h4&gt;结论&lt;/h4&gt;所提出的上采样方法有效解决了特征基础模型在显微图像处理中的局限性，在交互式分割任务中表现出色，提高了效率并显著减少了标注需求。&lt;h4&gt;翻译&lt;/h4&gt;特征基础模型 - 通常是视觉Transformer - 为图像提供丰富的语义描述符，对于下游任务（如交互式分割和目标检测）很有用。为了计算效率，这些描述符通常是基于块的，因此难以表示显微图像中常见的精细特征；它们也难以处理材料和生物图像分析中的大尺寸图像。在这项工作中，我们训练了一个卷积神经网络，参考输入图像来上采样低分辨率（即大块大小）的基础模型特征。我们将这个上采样器网络（无需进一步训练）应用于各种显微镜图像的高效特征化和分割，包括植物细胞、锂离子电池阴极和有机晶体。这些上采样特征的丰富性使得难以分割的阶段（如发丝裂纹）能够被分离。我们证明，使用这些深度特征进行交互式分割，比训练或微调更传统的卷积网络产生更高质量的分割结果，速度更快且需要的标签更少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Feature foundation models - usually vision transformers - offer rich semanticdescriptors of images, useful for downstream tasks such as (interactive)segmentation and object detection. For computational efficiency thesedescriptors are often patch-based, and so struggle to represent the finefeatures often present in micrographs; they also struggle with the large imagesizes present in materials and biological image analysis. In this work, wetrain a convolutional neural network to upsample low-resolution (i.e, largepatch size) foundation model features with reference to the input image. Weapply this upsampler network (without any further training) to efficientlyfeaturise and then segment a variety of microscopy images, including plantcells, a lithium-ion battery cathode and organic crystals. The richness ofthese upsampled features admits separation of hard to segment phases, likehairline cracks. We demonstrate that interactive segmentation with these deepfeatures produces high-quality segmentations far faster and with far fewerlabels than training or finetuning a more traditional convolutional network.</description>
      <author>example@mail.com (Ronan Docherty, Antonis Vamvakeros, Samuel J. Cooper)</author>
      <guid isPermaLink="false">2508.21529v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Federated Fine-tuning of SAM-Med3D for MRI-based Dementia Classification</title>
      <link>http://arxiv.org/abs/2508.21458v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the MICCAI 2025 Workshop on Distributed, Collaborative  and Federated Learning (DeCAF)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了基础模型在联邦学习系统中用于痴呆症诊断的性能和效率，重点关注分类头架构、微调策略和聚合方法的影响。&lt;h4&gt;背景&lt;/h4&gt;基础模型在基于AI的痴呆症诊断方面具有强大潜力，但将其整合到联邦学习系统中的研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;系统评估关键设计选择（分类头架构、微调策略和聚合方法）对使用脑部MRI数据进行联邦基础模型调优的性能和效率的影响。&lt;h4&gt;方法&lt;/h4&gt;这是一项基准研究，使用大型多队列数据集系统评估不同设计选择对联邦基础模型调优的影响。&lt;h4&gt;主要发现&lt;/h4&gt;分类头的架构显著影响性能；冻结基础模型编码器可实现与完全微调相当的结果；高级聚合方法优于标准的联邦平均方法。&lt;h4&gt;结论&lt;/h4&gt;研究结果为在分散的临床环境中部署基础模型提供了实用见解，并强调了应该指导未来方法开发的权衡因素。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型为基于人工智能的痴呆症诊断提供了强大潜力，但它们在联邦学习系统中的整合仍然研究不足。在这项基准研究中，我们使用脑部MRI数据，系统评估了关键设计选择（分类头架构、微调策略和聚合方法）对联邦基础模型调优的性能和效率的影响。利用大型多队列数据集，我们发现分类头架构对性能有显著影响，冻结基础模型编码器可实现与完全微调相当的结果，且高级聚合方法优于标准的联邦平均方法。我们的研究结果为在分散的临床环境中部署基础模型提供了实用见解，并强调了应该指导未来方法开发的权衡因素。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models (FMs) offer strong potential for AI-based dementiadiagnosis, their integration into federated learning (FL) systems remainsunderexplored. In this benchmarking study, we systematically evaluate theimpact of key design choices: classification head architecture, fine-tuningstrategy, and aggregation method, on the performance and efficiency offederated FM tuning using brain MRI data. Using a large multi-cohort dataset,we find that the architecture of the classification head substantiallyinfluences performance, freezing the FM encoder achieves comparable results tofull fine-tuning, and advanced aggregation methods outperform standardfederated averaging. Our results offer practical insights for deploying FMs indecentralized clinical settings and highlight trade-offs that should guidefuture method development.</description>
      <author>example@mail.com (Kaouther Mouheb, Marawan Elbatel, Janne Papma, Geert Jan Biessels, Jurgen Claassen, Huub Middelkoop, Barbara van Munster, Wiesje van der Flier, Inez Ramakers, Stefan Klein, Esther E. Bron)</author>
      <guid isPermaLink="false">2508.21458v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>CALM: A Framework for Continuous, Adaptive, and LLM-Mediated Anomaly Detection in Time-Series Streams</title>
      <link>http://arxiv.org/abs/2508.21273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CALM框架，一种用于实时异常检测的新型端到端解决方案，能够适应非平稳时间序列流中的概念漂移。&lt;h4&gt;背景&lt;/h4&gt;在非平稳时间序列流中检测异常是多个工业和科学领域的关键挑战，传统离线训练的模型在面对数据统计属性随时间变化的概念漂移时性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够适应动态数据模式的实时异常检测框架，解决概念漂移带来的性能退化问题。&lt;h4&gt;方法&lt;/h4&gt;CALM建立在Apache Beam分布式处理框架上，利用TimesFm基础模型进行预测性异常检测，包含两个核心贡献：闭环连续微调机制使模型能近乎实时适应不断变化的数据模式，以及'LLM-as-a-Judge'组件使用大型语言模型提供语义化、上下文感知的异常判断。&lt;h4&gt;主要发现&lt;/h4&gt;在TSB-UAD基准测试上评估显示，与静态预训练基础模型相比，连续微调的模型在大多数数据集上提高了ROC AUC分数。&lt;h4&gt;结论&lt;/h4&gt;自适应、LLM引导的方法能有效维持动态流环境中的高性能异常检测。&lt;h4&gt;翻译&lt;/h4&gt;在众多工业和科学领域中，在非平稳时间序列流中检测异常是一项关键但具有挑战性的任务。当面临概念漂移（数据的基础统计属性随时间变化）时，传统离线训练的模型会遭受显著的性能下降。本文介绍了CALM（Continuous, Adaptive, and LLM-Mediated），一种为应对这一挑战而设计的新型端到端实时异常检测框架。CALM建立在Apache Beam分布式处理框架上，并利用TimesFm基础模型进行基于预测的异常检测。该框架的创新性在于两个核心贡献。首先，它实现了一个闭环连续微调机制，使异常检测模型能够近乎实时地适应不断变化的数据模式。其次，它引入了'LLM-as-a-Judge'组件，一个大型语言模型，对检测到的异常提供语义化、上下文感知的判断，以筛选高质量训练数据，决定异常是代表瞬时噪声还是有意义的模式转变。我们在全面的TSB-UAD基准测试上评估了CALM。结果表明，与静态预训练基础模型相比，连续微调的模型在大多数数据集上提高了ROC AUC分数，验证了我们的自适应、LLM引导方法在保持动态流环境中高性能异常检测的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The detection of anomalies in non-stationary time-series streams is acritical but challenging task across numerous industrial and scientificdomains. Traditional models, trained offline, suffer significant performancedegradation when faced with concept drift, where the underlying statisticalproperties of the data change over time. This paper introduces CALM(Continuous, Adaptive, and LLM-Mediated), a novel, end-to-end framework forreal-time anomaly detection designed to address this challenge. CALM is builton the Apache Beam distributed processing framework and leverages the TimesFmfoundation model for forecasting-based anomaly detection. The framework'snovelty lies in two core contributions. First, it implements a closed-loop,continuous fine-tuning mechanism that allows the anomaly detection model toadapt to evolving data patterns in near real-time. Second, it introduces anLLM-as-a-Judge component, a Large Language Model that provides semantic,context-aware judgments on detected anomalies to curate a high-quality trainingdataset, deciding whether an anomaly represents transient noise or a meaningfulpattern shift. We evaluate CALM on the comprehensive TSB-UAD benchmark. Ourresults demonstrate that the continuously fine-tuned model improves the ROC AUCscore in most datasets compared to the static, pre-trained base model,validating the efficacy of our adaptive, LLM-guided approach to maintaininghigh-performance anomaly detection in dynamic streaming environments.</description>
      <author>example@mail.com (Ashok Devireddy, Shunping Huang)</author>
      <guid isPermaLink="false">2508.21273v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Guess-and-Learn (G&amp;L): Measuring the Cumulative Error Cost of Cold-Start Adaptation</title>
      <link>http://arxiv.org/abs/2508.21270v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 figures. Main text is 10 pages. Code and data are  available at https://github.com/RolandWArnold/guess-and-learn-benchmark&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了Guess-and-Learn (G&amp;L)方法，用于评估机器学习模型从零开始学习的适应能力，关注学习过程中的累积错误，而不仅仅是最终准确性。&lt;h4&gt;背景&lt;/h4&gt;传统的机器学习模型评估通常只强调最终准确性，忽略了模型从零开始学习时的适应成本（累积错误）。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够量化模型早期学习错误成本的评估方法，以补充传统的基准测试，并提供一个可复现的框架，用于开发不仅在极限情况下准确，而且在第一批示例中就可靠的模型。&lt;h4&gt;方法&lt;/h4&gt;G&amp;L定义了四种设置（从零开始/预训练 × 在线/批量）来分离初始化和更新频率的影响。该方法让学习者在每一步选择一个实例，预测其标签，接收真实标签，并在在线（每样本）或批量（延迟）模式下更新参数。结果形成的错误轨迹揭示了适应速度、选择质量和偏差等动态特性。&lt;h4&gt;主要发现&lt;/h4&gt;在MNIST和AG News上的基线实验显示，较小的模型可以用更少的初始错误进行适应，而预训练的好处因领域而异。在所有设置中，当前模型仍然远高于oracle参考带，突显了适应性差距。&lt;h4&gt;结论&lt;/h4&gt;通过量化早期学习的错误成本，G&amp;L补充了传统的基准测试，并为开发不仅在极限情况下准确，而且从第一批示例就可靠的模型提供了可复现的框架。&lt;h4&gt;翻译&lt;/h4&gt;评估机器学习模型通常强调最终准确性，而忽略了适应的成本：从零开始学习时累积的错误。Guess-and-Learn (G&amp;L) v1.0通过测量冷启动适应性——模型在顺序标记未标记数据集时所犯的总错误——来填补这一空白。在每一步，学习者选择一个实例，预测其标签，接收真实标签，并在在线（每样本）或批量（延迟）模式下更新参数。由此产生的错误轨迹揭示了适应速度、选择质量和偏差——这些是端点指标无法看到的动态特性。G&amp;L定义了四种轨道（从零开始/预训练 × 在线/批量）来分离初始化和更新频率的影响。我们将该协议形式化，将其与经典错误界限理论联系起来，并估计MNIST的启发式'oracle参考带'作为可信度参考。在MNIST和AG News上的基线实验，涵盖经典方法（感知器、k-NN）、卷积架构（CNN、ResNet-50）和预训练转换器（ViT-B/16、BERT-base），揭示了早期阶段效率的系统差异：较小的模型可以用更少的初始错误进行适应，而预训练的好处因领域而异。在所有设置中，当前模型仍然远高于oracle带，突显了适应性差距。通过量化早期学习的错误成本，G&amp;L补充了传统基准测试，并为开发不仅在极限情况下准确，而且从第一批示例就可靠的模型提供了可复现的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluation of machine learning models typically emphasizes final accuracy,overlooking the cost of adaptation: the cumulative errors incurred whilelearning from scratch. Guess-and- Learn (G&amp;L) v1.0 addresses this gap bymeasuring cold-start adaptability - the total mistakes a model makes whilesequentially labeling an unlabeled dataset. At each step, the learner selectsan instance, predicts its label, receives the ground truth, and updatesparameters under either online (per-sample) or batch (delayed) mode. Theresulting error trajectory exposes adaptation speed, selection quality, andbias - dynamics invisible to endpoint metrics.  G&amp;L defines four tracks (Scratch/Pretrained $\times$ Online/Batch) todisentangle the effects of initialization and update frequency. We formalizethe protocol, relate it to classical mistake-bound theory, and estimate aheuristic "oracle reference band" for MNIST as a plausibility reference.Baseline experiments on MNIST and AG News, spanning classical methods(Perceptron, k-NN), convolutional architectures (CNN, ResNet-50), andpretrained transformers (ViT-B/16, BERT-base), reveal systematic differences inearly-phase efficiency: smaller models can adapt with fewer initial errors,while pretraining benefits vary by domain. Across settings, current modelsremain well above the oracle band, highlighting an adaptability gap.  By quantifying the mistake cost of early learning, G&amp;L complementsconventional benchmarks and provides a reproducible framework for developinglearners that are not only accurate in the limit but also reliable from thefirst examples.</description>
      <author>example@mail.com (Roland Arnold)</author>
      <guid isPermaLink="false">2508.21270v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Generalizable Object Re-Identification via Visual In-Context Prompting</title>
      <link>http://arxiv.org/abs/2508.21222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了视觉上下文提示(VICP)框架，结合大语言模型和视觉基础模型，使训练好的模型能够仅通过上下文示例作为提示直接泛化到未见的新类别，无需参数适应。&lt;h4&gt;背景&lt;/h4&gt;当前物体再识别方法训练特定领域模型，缺乏泛化能力且需要大量标记数据；自监督学习虽减少标注需求，但难以捕获关键的敏感身份特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够泛化到未见类别的新框架，减少对标记数据的依赖，实现无需参数适应的跨类别泛化。&lt;h4&gt;方法&lt;/h4&gt;VICP框架结合大语言模型和视觉基础模型，通过特定任务提示从少量样本中推断语义身份规则，使用动态视觉提示引导模型提取身份判别性特征，对齐语义概念与预训练先验实现泛化。&lt;h4&gt;主要发现&lt;/h4&gt;VICP在ShopID10K数据集和多种ReID基准测试中，在未见类别上明显优于基线方法；引入了包含10K个物体实例的多视图图像和跨域测试的ShopID10K数据集。&lt;h4&gt;结论&lt;/h4&gt;VICP消除了数据集特定重新训练的需求，实现了有效的跨类别泛化。&lt;h4&gt;翻译&lt;/h4&gt;当前物体再识别方法训练特定领域模型（例如针对人或车辆），缺乏泛化能力且对新类别需要大量标记数据。虽然自监督学习通过学习实例不变性减少了标注需求，但难以捕获对再识别关键的敏感身份特征。本文提出了视觉上下文提示(VICP)，一种新框架，其中在已见类别上训练的模型可以直接泛化到未见的新类别，仅使用上下文示例作为提示，无需参数适应。VICP结合大语言模型和视觉基础模型：大语言模型通过特定任务提示从少量正负样本对中推断语义身份规则，然后通过动态视觉提示引导视觉基础模型（如DINO）提取身份判别性特征。通过对齐大语言模型衍生的语义概念与视觉基础模型的预训练先验，VICP实现了对新颖类别的泛化，消除了数据集特定重新训练的需求。为支持评估，我们引入了ShopID10K，一个包含来自电子商务平台的10K个物体实例的数据集，具有多视图图像和跨域测试功能。在ShopID10K和多种再识别基准测试中的实验表明，VICP在未见类别上明显优于基线方法。代码可在https://github.com/Hzzone/VICP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current object re-identification (ReID) methods train domain-specific models(e.g., for persons or vehicles), which lack generalization and demand costlylabeled data for new categories. While self-supervised learning reducesannotation needs by learning instance-wise invariance, it struggles to capture\textit{identity-sensitive} features critical for ReID. This paper proposesVisual In-Context Prompting~(VICP), a novel framework where models trained onseen categories can directly generalize to unseen novel categories using only\textit{in-context examples} as prompts, without requiring parameteradaptation. VICP synergizes LLMs and vision foundation models~(VFM): LLMs infersemantic identity rules from few-shot positive/negative pairs throughtask-specific prompting, which then guides a VFM (\eg, DINO) to extractID-discriminative features via \textit{dynamic visual prompts}. By aligningLLM-derived semantic concepts with the VFM's pre-trained prior, VICP enablesgeneralization to novel categories, eliminating the need for dataset-specificretraining. To support evaluation, we introduce ShopID10K, a dataset of 10Kobject instances from e-commerce platforms, featuring multi-view images andcross-domain testing. Experiments on ShopID10K and diverse ReID benchmarksdemonstrate that VICP outperforms baselines by a clear margin on unseencategories. Code is available at https://github.com/Hzzone/VICP.</description>
      <author>example@mail.com (Zhizhong Huang, Xiaoming Liu)</author>
      <guid isPermaLink="false">2508.21222v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedOneVision: Interleaved Vision-Text-Action Pretraining for General Robot Control</title>
      <link>http://arxiv.org/abs/2508.21112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了EO-Robotics，包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过交错视觉-文本-动作预训练在多模态具身推理和机器人控制方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;人类能够在开放世界中无缝执行多模态推理和物理交互的能力，是通用具身智能系统的核心目标。最近的视觉-语言-动作模型在通用机器人控制方面取得了显著进展，但仍然无法达到人类水平的交错推理和交互灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现人类水平灵活性的具身智能系统，特别是在交错推理和交互方面，以解决现有视觉-语言-动作模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入EO-Robotics，包括EO-1模型和EO-Data1.5M数据集；采用统一架构处理多模态输入；构建包含150多万个样本的数据集；通过自回归解码和流匹配去噪协同训练模型；在多种长视野、灵巧操作任务中验证效果。&lt;h4&gt;主要发现&lt;/h4&gt;交错视觉-文本-动作学习对开放世界理解和泛化有效；EO-1在各种具身形式的长视野、灵巧操作任务中表现出色；统一架构和大规模高质量数据集的结合显著提升了多模态具身推理和机器人控制能力。&lt;h4&gt;结论&lt;/h4&gt;EO-Robotics通过EO-1模型和EO-Data1.5M数据集的协同设计，实现了在多模态具身推理和机器人控制方面的优越性能，为开发高级具身基础模型提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类在开放世界中无缝执行多模态推理和物理交互的能力，是通用具身智能系统的核心目标。最近在大型机器人和视觉-文本数据上联合训练的视觉-语言-动作模型，在通用机器人控制方面取得了显著进展。然而，它们仍然无法达到人类水平的交错推理和交互灵活性。在这项工作中，我们介绍了EO-Robotics，它包括EO-1模型和EO-Data1.5M数据集。EO-1是一个统一的具身基础模型，通过交错视觉-文本-动作预训练在多模态具身推理和机器人控制方面实现了卓越性能。EO-1的开发基于两个关键支柱：(i)一个统一架构，无差别处理多模态输入（图像、文本、视频和动作）；(ii)一个大规模、高质量的多模态具身推理数据集EO-Data1.5M，包含超过150万个样本，强调交错视觉-文本-动作理解。EO-1通过在EO-Data1.5M上自回归解码和流匹配去噪的协同训练，实现无缝的机器人动作生成和多模态具身推理。大量实验证明了交错视觉-文本-动作学习对开放世界理解和泛化的有效性，通过多种具身形式下的各种长视野、灵巧操作任务得到验证。本文详细介绍了EO-1的架构、EO-Data1.5M的数据构建策略和训练方法，为开发高级具身基础模型提供了宝贵的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The human ability to seamlessly perform multimodal reasoning and physicalinteraction in the open world is a core goal for general-purpose embodiedintelligent systems. Recent vision-language-action (VLA) models, which areco-trained on large-scale robot and visual-text data, have demonstrated notableprogress in general robot control. However, they still fail to achievehuman-level flexibility in interleaved reasoning and interaction. In this work,introduce EO-Robotics, consists of EO-1 model and EO-Data1.5M dataset. EO-1 isa unified embodied foundation model that achieves superior performance inmultimodal embodied reasoning and robot control through interleavedvision-text-action pre-training. The development of EO-1 is based on two keypillars: (i) a unified architecture that processes multimodal inputsindiscriminately (image, text, video, and action), and (ii) a massive,high-quality multimodal embodied reasoning dataset, EO-Data1.5M, which containsover 1.5 million samples with emphasis on interleaved vision-text-actioncomprehension. EO-1 is trained through synergies between auto-regressivedecoding and flow matching denoising on EO-Data1.5M, enabling seamless robotaction generation and multimodal embodied reasoning. Extensive experimentsdemonstrate the effectiveness of interleaved vision-text-action learning foropen-world understanding and generalization, validated through a variety oflong-horizon, dexterous manipulation tasks across multiple embodiments. Thispaper details the architecture of EO-1, the data construction strategy ofEO-Data1.5M, and the training methodology, offering valuable insights fordeveloping advanced embodied foundation models.</description>
      <author>example@mail.com (Delin Qu, Haoming Song, Qizhi Chen, Zhaoqing Chen, Xianqiang Gao, Xinyi Ye, Qi Lv, Modi Shi, Guanghui Ren, Cheng Ruan, Maoqing Yao, Haoran Yang, Jiacheng Bao, Bin Zhao, Dong Wang)</author>
      <guid isPermaLink="false">2508.21112v1</guid>
      <pubDate>Mon, 01 Sep 2025 14:25:07 +0800</pubDate>
    </item>
    <item>
      <title>Olive Tree Satellite Image Segmentation Based On SAM and Multi-Phase Refinement</title>
      <link>http://arxiv.org/abs/2508.20954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于遥感技术的橄榄树分割方法，通过整合SAM模型和基于树木排列及形状大小约束的校正技术，显著提高了从卫星图像中识别和分割橄榄树的准确性。&lt;h4&gt;背景&lt;/h4&gt;在气候变化背景下，维持橄榄生物多样性至关重要，通过遥感技术进行早期异常检测和处理可以提供有效的管理解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新方法，从卫星图像中准确识别和分割农业地块中的橄榄树。&lt;h4&gt;方法&lt;/h4&gt;利用基础模型和先进分割技术，整合Segment Anything Model (SAM)，并结合基于树木田间排列的可学习约束和关于形状、大小的约束进行校正。&lt;h4&gt;主要发现&lt;/h4&gt;该方法达到了98%的准确率，显著超过了初始SAM 82%的性能。&lt;h4&gt;结论&lt;/h4&gt;结合SAM模型和基于树木排列及形状大小约束的校正方法，可以显著提高从卫星图像中识别和分割橄榄树的准确性，为橄榄生物多样性的保护和管理提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在已证实的气候变化背景下，利用遥感技术进行早期异常检测和处理以维持橄榄生物多样性至关重要，提供了有效的管理解决方案。本文提出了一种从卫星图像中分割橄榄树的创新方法。通过利用基础模型和先进的分割技术，研究整合了分割任何模型(SAM)来准确识别和分割农业地块中的橄榄树。该方法包括基于树木田间排列和关于形状和大小的可学习约束的SAM分割和校正。我们的方法达到了98%的准确率，显著超过了初始SAM 82%的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of proven climate change, maintaining olive biodiversitythrough early anomaly detection and treatment using remote sensing technologyis crucial, offering effective management solutions. This paper presents aninnovative approach to olive tree segmentation from satellite images. Byleveraging foundational models and advanced segmentation techniques, the studyintegrates the Segment Anything Model (SAM) to accurately identify and segmentolive trees in agricultural plots. The methodology includes SAM segmentationand corrections based on trees alignement in the field and a learanbleconstraint about the shape and the size. Our approach achieved a 98\% accuracyrate, significantly surpassing the initial SAM performance of 82\%.</description>
      <author>example@mail.com (Amir Jmal, Chaima Chtourou, Mahdi Louati, Abdelaziz Kallel, Houda Khmila)</author>
      <guid isPermaLink="false">2508.20954v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
  <item>
      <title>Dino U-Net: Exploiting High-Fidelity Dense Features from Foundation Models for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.20909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Dino U-Net的新型编码器-解码器架构，利用DINOv3视觉基础模型的高保真密集特征进行医学图像分割。该架构通过专门的适配器融合语义特征与空间细节，并设计了保真感知投影模块来保持特征质量。实验证明，该方法在多个医学图像数据集上取得了最先进的性能，且具有良好的可扩展性。&lt;h4&gt;背景&lt;/h4&gt;基于大规模自然图像数据集预训练的基础模型为医学图像分割提供了强大的范式，但有效地转移其学习表示用于精确的临床应用仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的架构Dino U-Net，利用DINOv3视觉基础模型的高保真密集特征，提高医学图像分割的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出Dino U-Net架构，包含：1) 基于冻结的DINOv3骨干网络的编码器；2) 专门的适配器用于融合模型的丰富语义特征与低级空间细节；3) 保真感知投影模块(FAPM)在降维过程中保持表示质量。&lt;h4&gt;主要发现&lt;/h4&gt;在七个多样化的公共医学图像分割数据集上进行了广泛实验，结果显示Dino U-Net实现了最先进的性能，在各种成像模态上始终优于先前的方法。该框架具有高度可扩展性，随着骨干模型大小的增加，分割准确性持续提高，达到70亿参数变体。&lt;h4&gt;结论&lt;/h4&gt;利用通用基础模型的优越、密集预训练特征为提高医学图像分割准确性提供了一种高度有效且参数效率高的方法。&lt;h4&gt;翻译&lt;/h4&gt;在大型自然图像数据集上预训练的基础模型为医学图像分割提供了强大的范式。然而，有效地转移其学习表示用于精确的临床应用仍然是一个挑战。在这项工作中，我们提出了Dino U-Net，一种新的编码器-解码器架构，旨在利用DINOv3视觉基础模型的高保真密集特征。我们的架构引入了一个构建于冻结DINOv3骨干网络之上的编码器，该编码器采用专门的适配器来融合模型的丰富语义特征和低级空间细节。为了在降维过程中保持这些表示的质量，我们设计了一种新的保真感知投影模块(FAPM)，有效地精炼和解码器的特征投影。我们在七个多样化的公共医学图像分割数据集上进行了广泛的实验。我们的结果表明，Dino U-Net实现了最先进的性能，在各种成像模态上始终优于先前的方法。我们的框架被证明具有高度可扩展性，随着骨干模型大小的增加，分割准确性持续提高，达到70亿参数的变体。研究结果表明，利用通用基础模型的优越、密集预训练特征为提高医学图像分割准确性提供了一种高度有效且参数效率高的方法。代码可在https://github.com/yifangao112/DinoUNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models pre-trained on large-scale natural image datasets offer apowerful paradigm for medical image segmentation. However, effectivelytransferring their learned representations for precise clinical applicationsremains a challenge. In this work, we propose Dino U-Net, a novelencoder-decoder architecture designed to exploit the high-fidelity densefeatures of the DINOv3 vision foundation model. Our architecture introduces anencoder built upon a frozen DINOv3 backbone, which employs a specializedadapter to fuse the model's rich semantic features with low-level spatialdetails. To preserve the quality of these representations during dimensionalityreduction, we design a new fidelity-aware projection module (FAPM) thateffectively refines and projects the features for the decoder. We conductedextensive experiments on seven diverse public medical image segmentationdatasets. Our results show that Dino U-Net achieves state-of-the-artperformance, consistently outperforming previous methods across various imagingmodalities. Our framework proves to be highly scalable, with segmentationaccuracy consistently improving as the backbone model size increases up to the7-billion-parameter variant. The findings demonstrate that leveraging thesuperior, dense-pretrained features from a general-purpose foundation modelprovides a highly effective and parameter-efficient approach to advance theaccuracy of medical image segmentation. The code is available athttps://github.com/yifangao112/DinoUNet.</description>
      <author>example@mail.com (Yifan Gao, Haoyue Li, Feng Yuan, Xiaosong Wang, Xin Gao)</author>
      <guid isPermaLink="false">2508.20909v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Turning Tabular Foundation Models into Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2508.20906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为G2T-FM的简单图基础模型，利用表格基础模型TabPFNv2作为骨干网络，通过邻域特征聚合和结构嵌入处理多样化节点特征，在图机器学习任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型已在自然语言处理和计算机视觉领域取得革命性进展，但在图机器学习领域的应用仍不充分。设计图基础模型(GFMs)的主要挑战是处理不同图数据集中多样化的节点特征。虽然已有研究专注于文本属性图，但处理其他类型特征的问题尚未解决，这一问题在表格数据机器学习领域同样存在。&lt;h4&gt;目的&lt;/h4&gt;受表格基础模型(如TabPFNv2)成功的启发，本文旨在提出一种简单有效的图基础模型，利用表格基础模型解决图机器学习中的特征处理挑战。&lt;h4&gt;方法&lt;/h4&gt;G2T-FM通过三步处理：1)使用邻域特征聚合增强原始节点特征；2)添加结构嵌入；3)将TabPFNv2应用于构建的节点表示。即使在完全的上下文学习模式下，该方法也无需额外训练即可有效工作。&lt;h4&gt;主要发现&lt;/h4&gt;即使在完全的上下文学习模式下，G2T-FM也显著优于公开可用的GFMs，性能与从头开始训练的经过良好调整的GNNs相当。微调后，G2T-FM超越了经过良好调整的GNN基线，展示了利用表格基础模型进行图机器学习任务的潜力。&lt;h4&gt;结论&lt;/h4&gt;本文揭示了利用表格基础模型进行图机器学习任务的一个先前被忽视的方向。G2T-FM的成功表明，表格基础模型可以有效地应用于图数据，为图机器学习提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;虽然基础模型已经彻底改变了自然语言处理和计算机视觉等领域，但它们在图机器学习中的应用和潜力在很大程度上仍未被探索。设计图基础模型(GFMs)的一个关键挑战是处理多样化的节点特征，这些特征可能在不同图数据集中有所不同。尽管许多关于GFMs的工作只专注于文本属性图，但处理GFMs中其他类型任意特征的问题尚未得到充分解决。然而，这个问题并非图领域独有，表格数据机器学习领域也存在类似问题。在这项工作中，受最近表格基础模型(如TabPFNv2)成功的启发，我们提出了G2T-FM，这是一种采用TabPFNv2作为骨干的简单图基础模型。具体来说，G2T-FM通过邻域特征聚合增强原始节点特征，添加结构嵌入，然后将TabPFNv2应用于构建的节点表示。即使在完全的上下文学习模式下，我们的模型也取得了强大的结果，显著优于公开可用的GFMs，性能与从头开始训练的经过良好调整的GNNs相当。此外，微调后，G2T-FM超越了经过良好调整的GNN基线，突显了所提出方法的潜力。更广泛地说，我们的论文揭示了利用表格基础模型进行图机器学习任务的一个先前被忽视的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While foundation models have revolutionized such fields as natural languageprocessing and computer vision, their application and potential within graphmachine learning remain largely unexplored. One of the key challenges indesigning graph foundation models (GFMs) is handling diverse node features thatcan vary across different graph datasets. Although many works on GFMs have beenfocused exclusively on text-attributed graphs, the problem of handlingarbitrary features of other types in GFMs has not been fully addressed.However, this problem is not unique to the graph domain, as it also arises inthe field of machine learning for tabular data. In this work, motivated by therecent success of tabular foundation models like TabPFNv2, we propose G2T-FM, asimple graph foundation model that employs TabPFNv2 as a backbone.Specifically, G2T-FM augments the original node features with neighborhoodfeature aggregation, adds structural embeddings, and then applies TabPFNv2 tothe constructed node representations. Even in a fully in-context regime, ourmodel achieves strong results, significantly outperforming publicly availableGFMs and performing on par with well-tuned GNNs trained from scratch. Moreover,after finetuning, G2T-FM surpasses well-tuned GNN baselines, highlighting thepotential of the proposed approach. More broadly, our paper reveals apreviously overlooked direction of utilizing tabular foundation models forgraph machine learning tasks.</description>
      <author>example@mail.com (Dmitry Eremeev, Gleb Bazhenov, Oleg Platonov, Artem Babenko, Liudmila Prokhorenkova)</author>
      <guid isPermaLink="false">2508.20906v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Foundation Model for Dental Caries Detection with Dual-View Co-Training</title>
      <link>http://arxiv.org/abs/2508.20813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DVCTNet的新型双视图协同训练网络，用于从全景X射线中准确检测龋齿，通过结合全局和局部视图信息，显著提高了检测准确性。&lt;h4&gt;背景&lt;/h4&gt;从全景X射线准确检测龋齿对于防止病变进展至关重要，但当前检测方法由于龋齿的微妙对比度和多样化病变形态而往往准确性不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的双视图协同训练网络(DVCTNet)用于准确的龋齿检测，模拟牙医的工作流程，结合整体图像筛查和详细牙齿级别检查。&lt;h4&gt;方法&lt;/h4&gt;DVCTNet首先使用自动牙齿检测建立两个互补视图：全景X射线图像的全局视图和裁剪牙齿图像的局部视图。在两个视图上分别预训练两个视觉基础模型，全局视图模型作为检测主干生成区域提议和全局特征，局部视图模型提取详细特征。通过引入门控跨视图注意力(GCV-Atten)模块动态融合双视图特征，并将融合特征整合回检测模型进行最终龋齿检测。&lt;h4&gt;主要发现&lt;/h4&gt;DVCTNet在公共数据集和新创建的高精度龋齿检测数据集上进行了测试和验证。该数据集使用口腔内图像和全景X射线进行双重标注。实验结果表明DVCTNet在两个数据集上都优于现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;DVCTNet方法具有临床适用性，其代码和标记数据集已在GitHub上公开分享，https://github.com/ShanghaiTech-IMPACT/DVCTNet。&lt;h4&gt;翻译&lt;/h4&gt;从全景X射线准确检测龋齿在防止病变进展中起着关键作用。然而，由于龋齿的微妙对比度和多样化的病变形态，当前的检测方法往往产生次优的准确性。在这项工作中，受牙医系统性地结合整体图像筛查与详细牙齿级别检查的临床工作流程启发，我们提出了DVCTNet，一种用于准确龋齿检测的新型双视图协同训练网络。我们的DVCTNet首先采用自动牙齿检测建立两个互补视图：来自全景X射线图像的全局视图和来自裁剪牙齿图像的局部视图。然后我们在两个视图上分别预训练两个视觉基础模型。全局视图基础模型作为检测主干，生成区域提议和全局特征，而局部视图模型从区域提议匹配的相应裁剪牙齿块中提取详细特征。为了有效整合两个视图的信息，我们引入了一个门控跨视图注意力(GCV-Atten)模块，该模块动态融合双视图特征，通过将融合特征整合回检测模型进行最终龋齿检测，从而增强检测流程。为了严格评估我们的DVCTNet，我们在公共数据集上测试了它，并在新创建的高精度龋齿检测数据集上进一步验证了其性能，该数据集使用口腔内图像和全景X射线进行双重标注。实验结果表明，DVCTNet在两个数据集上都优于现有的最先进方法，表明我们方法的临床适用性。我们的代码和标记数据集可在https://github.com/ShanghaiTech-IMPACT/DVCTNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate dental caries detection from panoramic X-rays plays a pivotal rolein preventing lesion progression. However, current detection methods oftenyield suboptimal accuracy due to subtle contrast variations and diverse lesionmorphology of dental caries. In this work, inspired by the clinical workflowwhere dentists systematically combine whole-image screening with detailedtooth-level inspection, we present DVCTNet, a novel Dual-View Co-Trainingnetwork for accurate dental caries detection. Our DVCTNet starts with employingautomated tooth detection to establish two complementary views: a global viewfrom panoramic X-ray images and a local view from cropped tooth images. We thenpretrain two vision foundation models separately on the two views. Theglobal-view foundation model serves as the detection backbone, generatingregion proposals and global features, while the local-view model extractsdetailed features from corresponding cropped tooth patches matched by theregion proposals. To effectively integrate information from both views, weintroduce a Gated Cross-View Attention (GCV-Atten) module that dynamicallyfuses dual-view features, enhancing the detection pipeline by integrating thefused features back into the detection model for final caries detection. Torigorously evaluate our DVCTNet, we test it on a public dataset and furthervalidate its performance on a newly curated, high-precision dental cariesdetection dataset, annotated using both intra-oral images and panoramic X-raysfor double verification. Experimental results demonstrate DVCTNet's superiorperformance against existing state-of-the-art (SOTA) methods on both datasets,indicating the clinical applicability of our method. Our code and labeleddataset are available at https://github.com/ShanghaiTech-IMPACT/DVCTNet.</description>
      <author>example@mail.com (Tao Luo, Han Wu, Tong Yang, Dinggang Shen, Zhiming Cui)</author>
      <guid isPermaLink="false">2508.20813v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Looking Beyond the Obvious: A Survey on Abstract Concept Recognition for Video Understanding</title>
      <link>http://arxiv.org/abs/2508.20765v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review for IJCV&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视频内容的自动理解正在快速发展，机器能够理解视频中具体的可见内容（如物体、动作、事件或场景），但人类具有识别抽象概念（如正义、自由、团结）的独特能力。抽象概念识别是视频理解中的一个重要开放挑战。&lt;h4&gt;背景&lt;/h4&gt;基于深度神经网络和大型数据集，机器在理解视频内容方面取得了显著进步，但与人类相比，机器在理解抽象概念方面仍有差距，需要在多个语义层次上基于上下文信息进行推理。&lt;h4&gt;目的&lt;/h4&gt;利用基础模型的最新进展来解决视频中抽象概念理解的问题，使模型更符合人类推理和价值观，促进高级抽象概念的自动化理解。&lt;h4&gt;方法&lt;/h4&gt;通过综述研究用于理解视频中抽象概念的不同任务和数据集，分析过去几十年研究者如何利用当时可用的工具解决这些任务。&lt;h4&gt;主要发现&lt;/h4&gt;研究者们一直在尝试解决抽象概念理解任务，并充分利用当时可用的工具，社区数十年的经验可以帮助我们解决这个重要的开放性挑战。&lt;h4&gt;结论&lt;/h4&gt;在多模态基础模型时代重新审视抽象概念理解问题时，借鉴社区经验可以帮助我们避免'重复发明轮子'，更有效地推进这一领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;视频内容的自动理解正在迅速发展。得益于深度神经网络和大型数据集，机器越来越能够理解视频帧中具体可见的内容，无论是物体、动作、事件还是场景。相比之下，人类仍然保留着一种独特的能力，能够超越具体实体并识别如正义、自由和团结等抽象概念。抽象概念识别构成了视频理解中的一个关键开放挑战，其中基于上下文信息在多个语义层次上进行推理是关键。在本文中，我们认为基础模型的最新进展为解决视频中抽象概念理解问题提供了理想的环境。高级抽象概念的自动化理解至关重要，因为它使模型更符合人类的推理和价值观。在本综述中，我们研究了用于理解视频内容中抽象概念的不同任务和数据集。我们观察到，研究人员长期以来一直在尝试解决这些任务，充分利用他们可用的工具。我们主张借鉴社区数十年的经验将帮助我们阐明这一重要的开放性重大挑战，并避免在多模态基础模型时代重新审视它时'重复发明轮子'。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automatic understanding of video content is advancing rapidly. Empoweredby deeper neural networks and large datasets, machines are increasingly capableof understanding what is concretely visible in video frames, whether it beobjects, actions, events, or scenes. In comparison, humans retain a uniqueability to also look beyond concrete entities and recognize abstract conceptslike justice, freedom, and togetherness. Abstract concept recognition forms acrucial open challenge in video understanding, where reasoning on multiplesemantic levels based on contextual information is key. In this paper, we arguethat the recent advances in foundation models make for an ideal setting toaddress abstract concept understanding in videos. Automated understanding ofhigh-level abstract concepts is imperative as it enables models to be morealigned with human reasoning and values. In this survey, we study differenttasks and datasets used to understand abstract concepts in video content. Weobserve that, periodically and over a long period, researchers have attemptedto solve these tasks, making the best use of the tools available at theirdisposal. We advocate that drawing on decades of community experience will helpus shed light on this important open grand challenge and avoid ``re-inventingthe wheel'' as we start revisiting it in the era of multi-modal foundationmodels.</description>
      <author>example@mail.com (Gowreesh Mago, Pascal Mettes, Stevan Rudinac)</author>
      <guid isPermaLink="false">2508.20765v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>ArtFace: Towards Historical Portrait Face Identification via Model Adaptation</title>
      <link>http://arxiv.org/abs/2508.20626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures. ArtMetrics @ ICCV 2025 (non-archival). Paper page  at https://www.idiap.ch/paper/artface/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了基础模型在改进艺术品中面部识别方面的潜力，通过微调基础模型并将其嵌入与传统面部识别网络结合，实现了比当前最先进方法更显著的改进。&lt;h4&gt;背景&lt;/h4&gt;识别历史画中的人物对艺术史学家了解人物生平及其自我呈现方式至关重要，但这一过程通常很主观且受限于数据缺乏和风格变化。传统面部识别模型虽在照片上表现良好，但在画作上表现不佳，因存在领域转换和类内变化高的问题。艺术因素如风格、技巧、意图和其他作品的影响进一步增加了识别难度。&lt;h4&gt;目的&lt;/h4&gt;研究基础模型在改进艺术品中面部识别方面的潜力，以解决传统方法在艺术画作上面部识别的局限性。&lt;h4&gt;方法&lt;/h4&gt;通过微调基础模型并将其嵌入与传统面部识别网络的嵌入相结合，创建一种混合方法来提高艺术品中面部识别的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型与传统方法相结合后，在艺术品面部识别任务上取得了比当前最先进方法更显著的改进效果。&lt;h4&gt;结论&lt;/h4&gt;基础模型能够有效填补传统面部识别方法在艺术品识别中无效的空白，为艺术史研究提供了新的技术支持。&lt;h4&gt;翻译&lt;/h4&gt;识别历史画中的人物是艺术史学家的一项关键任务，能够深入了解他们的生活以及他们希望如何被呈现。然而，这一过程通常很主观，并且受限于数据缺乏和风格变化。自动化面部识别能够处理具有挑战性的条件并提供协助，但虽然传统面部识别模型在照片上表现良好，它们却因领域转换和类内变化高而在画作上表现不佳。艺术因素如风格、技巧、意图以及受其他作品的影响进一步使识别复杂化。在本研究中，我们调查了基础模型在改进艺术品中面部识别方面的潜力。通过微调基础模型并将其嵌入与传统面部识别网络的嵌入相结合，我们展示了比当前最先进方法更显著的改进。我们的结果表明，基础模型可以在传统方法无效的情况下填补空白。论文页面位于 https://www.idiap.ch/paper/artface/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying sitters in historical paintings is a key task for art historians,offering insight into their lives and how they chose to be seen. However, theprocess is often subjective and limited by the lack of data and stylisticvariations. Automated facial recognition is capable of handling challengingconditions and can assist, but while traditional facial recognition modelsperform well on photographs, they struggle with paintings due to domain shiftand high intra-class variation. Artistic factors such as style, skill, intent,and influence from other works further complicate recognition. In this work, weinvestigate the potential of foundation models to improve facial recognition inartworks. By fine-tuning foundation models and integrating their embeddingswith those from conventional facial recognition networks, we demonstratenotable improvements over current state-of-the-art methods. Our results showthat foundation models can bridge the gap where traditional methods areineffective. Paper page at https://www.idiap.ch/paper/artface/</description>
      <author>example@mail.com (Francois Poh, Anjith George, Sébastien Marcel)</author>
      <guid isPermaLink="false">2508.20626v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>MedGR$^2$: Breaking the Data Barrier for Medical Reasoning via Generative Reward Learning</title>
      <link>http://arxiv.org/abs/2508.20549v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了医学推理生成式奖励学习(MedGR²)框架，解决了医学领域视觉语言模型应用中高质量数据稀缺的问题，实现了数据生成与奖励模型协同发展的良性循环，显著提升了模型在跨模态和跨任务上的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;医学领域视觉语言模型(VLMs)的应用受限于高质量、专家标注数据的稀缺。现有数据集上的监督微调(SFT)在未见模态和任务上泛化能力差，而强化学习(RL)因缺乏可靠奖励信号也难以应用。&lt;h4&gt;目的&lt;/h4&gt;突破医学领域数据稀缺的限制，开发一种能够自我改进的良性循环框架，释放强化学习在构建真正可泛化医疗AI方面的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出医学推理生成式奖励学习(MedGR²)框架，协同开发数据生成器和奖励模型，实现高质量多模态医学数据的自动连续创建，并将这些数据用于监督微调和通过组相对策略优化(GRPO)的强化学习。&lt;h4&gt;主要发现&lt;/h4&gt;使用MedGR²生成数据进行监督微调已超越大型人工策划数据集训练的基线；通过GRPO进行强化学习实现了最先进的跨模态和跨任务泛化能力，显著优于专门RL方法；由MedGR²赋能的紧凑型模型性能可与参数超过其10倍以上的基础模型相竞争。&lt;h4&gt;结论&lt;/h4&gt;MedGR²为高风险领域的数据高效学习提供了新范式，将问题从数据稀缺转变为数据生成，为医疗AI的发展开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型(VLMs)在医学中的应用受到高质量、专家标注数据稀缺的严重阻碍。在现有数据集上进行监督微调(SFT)通常在未见过的模态和任务上泛化能力差，而作为一种有前途的替代方案的强化学习(RL)，因在这个数据稀缺领域缺乏可靠的奖励信号而受阻。为打破这一僵局，我们引入了医学推理生成式奖励学习(MedGR²)，这是一个创建自我改进良性循环的新颖框架。MedGR²协同开发数据生成器和奖励模型，实现了高质量、多模态医学数据的自动、连续创建，这些数据既可作为监督微调的优质训练源，也可用于强化学习。我们的实验证明，使用MedGR²生成数据进行监督微调已经超越了在大型人工策划数据集上训练的基线。重要的是，当通过组相对策略优化(GRPO)利用这些数据进行强化学习时，我们的模型实现了最先进的跨模态和跨任务泛化能力，显著优于专门的基于RL的方法。此外，由MedGR²赋能的我们的紧凑型模型实现了与参数超过其10倍以上的基础模型相竞争的性能。MedGR²为高风险领域的数据高效学习提出了新范式，将问题从数据稀缺转变为数据生成，释放了RL构建真正可泛化的医疗AI的全部潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The application of Vision-Language Models (VLMs) in medicine is criticallyhampered by the scarcity of high-quality, expert-annotated data. SupervisedFine-Tuning (SFT) on existing datasets often leads to poor generalization onunseen modalities and tasks, while Reinforcement Learning (RL), a promisingalternative, is stymied by the lack of reliable reward signals in thisdata-scarce domain. To break this impasse, we introduce Generative RewardLearning for Medical Reasoning (MedGR$^2$), a novel framework that creates aself-improving virtuous cycle. MedGR$^2$ co-develops a data generator and areward model, enabling the automated, continuous creation of high-quality,multi-modal medical data that serves as both a superior training source for SFTand RL. Our experiments demonstrate that SFT with MedGR$^2$-produced dataalready surpasses baselines trained on large-scale, human-curated datasets.Crucially, when leveraging this data for RL via Group Relative PolicyOptimization (GRPO), our model achieves state-of-the-art cross-modality andcross-task generalization, significantly outperforming specialized RL-basedmethods. Furthermore, our compact model, empowered by MedGR$^2$, achievesperformance competitive with foundation models possessing over 10 times moreparameters. MedGR$^2$ presents a new paradigm for data-efficient learning inhigh-stakes domains, transforming the problem from data scarcity to datageneration and unlocking the full potential of RL for building trulygeneralizable medical AI.</description>
      <author>example@mail.com (Weihai Zhi, Jiayan Guo, Shangyang Li)</author>
      <guid isPermaLink="false">2508.20549v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Pseudo-Boxes via Data-Level LiDAR-Camera Fusion for Unsupervised 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.20530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的数据级融合框架，通过早期整合RGB图像和LiDAR数据来提高3D目标检测性能，解决了高质量3D标注获取困难的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的基于LiDAR的3D目标检测器通常依赖手动标注的标签进行训练，但获取高质量的3D标注既耗时又费力。现有的无监督3D目标检测方法简单融合LiDAR点云和RGB图像生成的伪边界框，忽视了两种数据的互补性。&lt;h4&gt;目的&lt;/h4&gt;解决高质量3D标注获取困难的问题，提出一种新的数据级融合框架，早期整合RGB图像和LiDAR数据，提高伪边界框质量和定位准确性。&lt;h4&gt;方法&lt;/h4&gt;利用视觉基础模型进行图像的实例分割和深度估计；引入双向融合方法，使真实点从2D空间获取类别标签，同时将2D像素投影到3D增强点密度；提出局部和全局过滤方法抑制深度估计误差和移除分割异常值；开发基于数据级融合的动态自进化策略迭代优化伪边界框。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，该方法训练的检测器显著优于之前的最先进方法，在nuScenes验证基准上达到28.4% mAP。&lt;h4&gt;结论&lt;/h4&gt;通过早期整合RGB图像和LiDAR数据，有效提高了伪边界框质量，动态自进化策略显著提高了定位准确性，为无监督3D目标检测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;现有的基于LiDAR的3D目标检测器通常依赖手动标注标签进行训练以获得良好性能。然而，获取高质量3D标签既耗时又费力。为解决这一问题，近期工作探索无监督3D目标检测，引入RGB图像作为辅助模态协助生成伪边界框。然而，这些方法简单融合LiDAR点云和RGB图像生成的伪边界框。这种标签级融合策略对伪边界框质量的提升有限，因为它忽视了LiDAR和RGB图像数据的互补性。为克服上述局限，我们提出了一种新颖的数据级融合框架，在早期阶段整合RGB图像和LiDAR数据。具体而言，我们利用视觉基础模型进行图像的实例分割和深度估计，并引入双向融合方法，使真实点从2D空间获取类别标签，同时将2D像素投影到3D以增强真实点密度。为减轻深度和分割估计带来的噪声，我们提出了一种局部和全局过滤方法，应用局部半径过滤抑制深度估计误差，使用全局统计过滤移除分割引起的异常值。此外，我们提出了一种基于数据级融合的动态自进化策略，在密集表示下迭代优化伪边界框，显著提高了定位精度。在nuScenes数据集上的大量实验证明，我们方法训练的检测器显著优于之前的最先进方法，在nuScenes验证基准上达到28.4% mAP。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无监督3D目标检测中伪框质量不高的问题。现有基于LiDAR的3D目标检测器依赖大量手动标注数据，而获取高质量3D标注非常耗时耗力（标注一个3D边界框约需5分钟），这使得大规模标注不切实际。无监督学习可以减少对标注数据的依赖，降低成本，使3D目标检测技术更容易应用于新场景，对自动驾驶等领域具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：早期方法仅依靠点云聚类生成伪框，无法获取类别信息且受点云稀疏性限制；近期多模态方法虽引入图像数据但仅在后期进行标签级融合，未能充分利用LiDAR和图像的互补优势。基于此，作者设计了数据级融合框架，在早期阶段集成两种数据。作者借鉴了视觉基础模型（如SEEM和DepthAnything）进行图像分割和深度估计，以及无监督3D目标检测中的自进化技术，但创新性地将这些技术应用于数据层面的融合而非标签层面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在数据层面而非标签层面早期融合LiDAR和RGB图像信息，生成高质量、类别感知的伪框。整体流程分两阶段：1)数据级融合的初始伪框生成：使用视觉模型获取图像分割和深度信息，进行双向融合（真实点获取2D标签，2D像素生成3D伪点），应用局部过滤（处理深度估计噪声）和全局过滤（处理分割噪声），最后拟合生成初始伪框；2)动态自进化：通过实点密集化增加点云密度，监控损失变化动态更新伪框，保留高置信度检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)数据级融合方法，包含双向融合（真实点获取类别标签，2D像素投影到3D增强点密度）和局部-全局过滤（减轻深度和分割噪声）；2)基于数据级融合的动态自进化策略，在密集表示下迭代优化伪框。相比之前工作，不同之处在于：融合时机从后期标签级改为早期数据级；融合内容从简单整合伪框变为融合真实点和伪点；专门处理了深度和分割估计带来的噪声；在密集表示而非稀疏点云空间进行自进化，提高了效率和精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种数据级LiDAR-相机融合方法，通过早期双向融合和噪声过滤生成高质量类别感知伪框，并结合动态自进化策略，显著提升了无监督3D目标检测的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing LiDAR-based 3D object detectors typically rely on manually annotatedlabels for training to achieve good performance. However, obtaininghigh-quality 3D labels is time-consuming and labor-intensive. To address thisissue, recent works explore unsupervised 3D object detection by introducing RGBimages as an auxiliary modal to assist pseudo-box generation. However, thesemethods simply integrate pseudo-boxes generated by LiDAR point clouds and RGBimages. Yet, such a label-level fusion strategy brings limited improvements tothe quality of pseudo-boxes, as it overlooks the complementary nature in termsof LiDAR and RGB image data. To overcome the above limitations, we propose anovel data-level fusion framework that integrates RGB images and LiDAR data atan early stage. Specifically, we utilize vision foundation models for instancesegmentation and depth estimation on images and introduce a bi-directionalfusion method, where real points acquire category labels from the 2D space,while 2D pixels are projected onto 3D to enhance real point density. Tomitigate noise from depth and segmentation estimations, we propose a local andglobal filtering method, which applies local radius filtering to suppress depthestimation errors and global statistical filtering to removesegmentation-induced outliers. Furthermore, we propose a data-level fusionbased dynamic self-evolution strategy, which iteratively refines pseudo-boxesunder a dense representation, significantly improving localization accuracy.Extensive experiments on the nuScenes dataset demonstrate that the detectortrained by our method significantly outperforms that trained by previousstate-of-the-art methods with 28.4$\%$ mAP on the nuScenes validationbenchmark.</description>
      <author>example@mail.com (Mingqian Ji, Jian Yang, Shanshan Zhang)</author>
      <guid isPermaLink="false">2508.20530v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>On Identifying Why and When Foundation Models Perform Well on Time-Series Forecasting Using Automated Explanations and Rating</title>
      <link>http://arxiv.org/abs/2508.20437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 Tables, 5 Figures, AI Trustworthiness and Risk Assessment  for Challenged Contexts (ATRACC), Appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列预测模型已从经典统计方法演变为复杂的基础模型，但理解其成功或失败的原因仍具挑战性。本文结合传统可解释AI方法与基于评分的解释(RDE)来评估和解释不同领域中的TSFM性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测模型越来越多地用于生成影响现实世界行动的信息，但其复杂性、性能变化性和不透明性引发了对用户应如何与模型输出互动和依赖的担忧。&lt;h4&gt;目的&lt;/h4&gt;理解时间序列预测模型的复杂性、性能变化性和不透明性，以解决用户应如何与这些模型的输出互动和依赖的严重问题。&lt;h4&gt;方法&lt;/h4&gt;结合传统可解释人工智能(XAI)方法和基于评分的解释(RDE)来评估TSFM性能和可解释性，评估四种模型架构（ARIMA、梯度提升、Chronos和Llama）在四个异构数据集上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在波动性大或数据稀疏的领域（如电力、汽车零部件），特征工程模型（如梯度提升）持续优于基础模型（如Chronos）并提供更可解释的解释；而基础模型仅在稳定或趋势驱动的情境（如金融）中表现出色。&lt;h4&gt;结论&lt;/h4&gt;不同类型的时间序列预测模型在不同领域和情境中表现各异，特征工程模型在复杂环境中表现更好，基础模型在稳定环境中更有效。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测模型已经从经典统计方法演变为复杂的基础模型，但理解这些模型成功或失败的原因仍然具有挑战性。尽管存在这一已知限制，时间序列预测模型越来越多地用于生成影响现实世界行动的信息。理解这些模型的复杂性、性能变化性和不透明性，对于解决用户应如何与这些模型的输出互动和依赖的严重问题变得至关重要。本文通过结合传统可解释人工智能(XAI)方法和基于评分的解释(RDE)来评估和解释不同领域和用例中的TSFM性能，以应对这些担忧。我们评估了四种不同的模型架构：ARIMA、梯度提升、Chronos（时间序列特定基础模型）和Llama（通用模型，包括微调版本和基础模型），这些模型在涵盖金融、能源、运输和汽车销售四个异构数据集上进行测试。通过这些研究，我们证明在波动性大或数据稀疏的领域（如电力、汽车零部件），特征工程模型（如梯度提升）持续优于基础模型（如Chronos），并提供更可解释的解释；而基础模型仅在稳定或趋势驱动的情境（如金融）中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series forecasting models (TSFM) have evolved from classical statisticalmethods to sophisticated foundation models, yet understanding why and whenthese models succeed or fail remains challenging. Despite this knownlimitation, time series forecasting models are increasingly used to generateinformation that informs real-world actions with equally real consequences.Understanding the complexity, performance variability, and opaque nature ofthese models then becomes a valuable endeavor to combat serious concerns abouthow users should interact with and rely on these models' outputs. This workaddresses these concerns by combining traditional explainable AI (XAI) methodswith Rating Driven Explanations (RDE) to assess TSFM performance andinterpretability across diverse domains and use cases. We evaluate fourdistinct model architectures: ARIMA, Gradient Boosting, Chronos (time-seriesspecific foundation model), Llama (general-purpose; both fine-tuned and basemodels) on four heterogeneous datasets spanning finance, energy,transportation, and automotive sales domains. In doing so, we demonstrate thatfeature-engineered models (e.g., Gradient Boosting) consistently outperformfoundation models (e.g., Chronos) in volatile or sparse domains (e.g., power,car parts) while providing more interpretable explanations, whereas foundationmodels excel only in stable or trend-driven contexts (e.g., finance).</description>
      <author>example@mail.com (Michael Widener, Kausik Lakkaraju, John Aydin, Biplav Srivastava)</author>
      <guid isPermaLink="false">2508.20437v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>MedFoundationHub: A Lightweight and Secure Toolkit for Deploying Medical Vision Language Foundation Models</title>
      <link>http://arxiv.org/abs/2508.20345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医疗视觉语言模型(VLMs)在临床应用中展现出巨大潜力，但存在严重的安全风险。本研究提出了MedFoundationHub工具包，解决了医疗VLMs的安全挑战，使医生和工程师能够安全、高效地部署和使用这些模型。&lt;h4&gt;背景&lt;/h4&gt;医疗视觉语言模型(VLMs)在临床应用如自动报告生成、医生助手和不确定性量化方面展现出显著机会，但同时引入了严重的安全问题，特别是受保护健康信息(PHI)泄露、数据泄露和网络安全威胁风险。&lt;h4&gt;目的&lt;/h4&gt;解决医疗VLMs的安全挑战，提供一个安全且易于访问的解决方案，使医疗专业人员能够有效利用这些先进技术。&lt;h4&gt;方法&lt;/h4&gt;开发MedFoundationHub，一个图形用户界面(GUI)工具包，使医生能够手动选择和使用不同模型无需编程专业知识；支持工程师以即插即用方式高效部署医疗VLMs，无缝集成Hugging Face开源模型；通过Docker编排的、操作系统无关的部署确保隐私保护的推理。该工具只需配备单个NVIDIA A6000 GPU的离线本地工作站。&lt;h4&gt;主要发现&lt;/h4&gt;评估了五种最先进的VLMs(Google-MedGemma3-4B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct和LLaVA-1.5-7B/13B)，通过获得认证的病理学家对结肠病例和肾脏病例进行评估，共获得1015个临床医生-模型评分事件。评估揭示了这些模型存在离题答案、模糊推理和不一致的病理学术语等局限性。&lt;h4&gt;结论&lt;/h4&gt;MedFoundationHub提供了一个安全且易于访问的解决方案，用于在典型学术研究实验室资源内部署医疗VLMs。然而，当前医疗VLMs仍存在局限性，需要进一步改进以提高其临床适用性。&lt;h4&gt;翻译&lt;/h4&gt;医疗视觉语言模型(VLMs)的最新进展为临床应用开辟了显著机会，如自动报告生成、医生助手和不确定性量化。然而，尽管它们有潜力，医疗VLMs引入了严重的安全问题，最值得注意的是受保护健康信息(PHI)泄露、数据泄露和易受网络威胁的风险 - 这些在医院环境中尤其关键。即使用于研究或非临床目的，医疗机构也必须谨慎并实施保障措施。为应对这些挑战，我们提出了MedFoundationHub，一个图形用户界面(GUI)工具包，它：(1)使医生能够手动选择和使用不同模型而无需编程专业知识，(2)支持工程师以即插即用方式高效部署医疗VLMs，无缝集成Hugging Face开源模型，(3)通过Docker编排的、操作系统无关的部署确保隐私保护的推理。MedFoundationHub只需要配备单个NVIDIA A6000 GPU的离线本地工作站，使其在学术研究实验室的典型资源中既安全又易于访问。为了评估当前能力，我们邀请获得认证的病理学家部署和评估了五种最先进的VLMs(Google-MedGemma3-4B、Qwen2-VL-7B-Instruct、Qwen2.5-VL-7B-Instruct和LLaVA-1.5-7B/13B)。专家评估涵盖结肠病例和肾脏病例，共获得1015个临床医生-模型评分事件。这些评估揭示了反复出现的局限性，包括离题答案、模糊推理和不一致的病理学术语。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in medical vision-language models (VLMs) open up remarkableopportunities for clinical applications such as automated report generation,copilots for physicians, and uncertainty quantification. However, despite theirpromise, medical VLMs introduce serious security concerns, most notably risksof Protected Health Information (PHI) exposure, data leakage, and vulnerabilityto cyberthreats - which are especially critical in hospital environments. Evenwhen adopted for research or non-clinical purposes, healthcare organizationsmust exercise caution and implement safeguards. To address these challenges, wepresent MedFoundationHub, a graphical user interface (GUI) toolkit that: (1)enables physicians to manually select and use different models withoutprogramming expertise, (2) supports engineers in efficiently deploying medicalVLMs in a plug-and-play fashion, with seamless integration of Hugging Faceopen-source models, and (3) ensures privacy-preserving inference throughDocker-orchestrated, operating system agnostic deployment. MedFoundationHubrequires only an offline local workstation equipped with a single NVIDIA A6000GPU, making it both secure and accessible within the typical resources ofacademic research labs. To evaluate current capabilities, we engagedboard-certified pathologists to deploy and assess five state-of-the-art VLMs(Google-MedGemma3-4B, Qwen2-VL-7B-Instruct, Qwen2.5-VL-7B-Instruct, andLLaVA-1.5-7B/13B). Expert evaluation covered colon cases and renal cases,yielding 1015 clinician-model scoring events. These assessments revealedrecurring limitations, including off-target answers, vague reasoning, andinconsistent pathology terminology.</description>
      <author>example@mail.com (Xiao Li, Yanfan Zhu, Ruining Deng, Wei-Qi Wei, Yu Wang, Shilin Zhao, Yaohong Wang, Haichun Yang, Yuankai Huo)</author>
      <guid isPermaLink="false">2508.20345v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2508.20085v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HERMES是一个用于移动双臂灵巧操作的人机学习框架，通过统一的强化学习方法将多源人类手部动作转化为机器人行为，并结合sim2real迁移和视觉定位技术实现复杂环境中的自主操作。&lt;h4&gt;背景&lt;/h4&gt;利用人类运动数据使机器人具备多样化操作技能是机器人操作领域的前沿方向，但将多源人类手部动作转化为可行的机器人行为具有挑战性，特别是对于具有复杂高维动作空间的多指灵巧手，且现有方法难以适应不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发HERMES框架，解决多源人类运动到机器人行为的转化问题，提高机器人在不同环境中的适应能力，并实现多样化非结构化环境中的自主操作。&lt;h4&gt;方法&lt;/h4&gt;HERMES采用统一的强化学习方法将异构人类手部动作转化为物理可行的机器人行为；设计基于深度图像的端到端sim2real迁移方法减少仿真到现实的差距；通过闭环Perspective-n-Point定位机制增强导航基础模型，实现视觉目标的精确对齐，连接自主导航与灵巧操作。&lt;h4&gt;主要发现&lt;/h4&gt;HERMES能够在多样化和实际场景中表现出可泛化的行为，成功执行多种复杂的移动双臂灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;HERMES框架有效解决了多源人类运动到机器人行为的转化挑战，通过sim2real迁移和定位机制在真实世界中实现了有效操作，具有广泛的适用性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，已提供中文翻译版本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging human motion data to impart robots with versatile manipulationskills has emerged as a promising paradigm in robotic manipulation.Nevertheless, translating multi-source human hand motions into feasible robotbehaviors remains challenging, particularly for robots equipped withmulti-fingered dexterous hands characterized by complex, high-dimensionalaction spaces. Moreover, existing approaches often struggle to produce policiescapable of adapting to diverse environmental conditions. In this paper, weintroduce HERMES, a human-to-robot learning framework for mobile bimanualdexterous manipulation. First, HERMES formulates a unified reinforcementlearning approach capable of seamlessly transforming heterogeneous human handmotions from multiple sources into physically plausible robotic behaviors.Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depthimage-based sim2real transfer method for improved generalization to real-worldscenarios. Furthermore, to enable autonomous operation in varied andunstructured environments, we augment the navigation foundation model with aclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precisealignment of visual goals and effectively bridging autonomous navigation anddexterous manipulation. Extensive experimental results demonstrate that HERMESconsistently exhibits generalizable behaviors across diverse, in-the-wildscenarios, successfully performing numerous complex mobile bimanual dexterousmanipulation tasks. Project Page:https://gemcollector.github.io/HERMES/.</description>
      <author>example@mail.com (Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.20085v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Is the medical image segmentation problem solved? A survey of current developments and future directions</title>
      <link>http://arxiv.org/abs/2508.20139v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  80 pages, 38 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对基于深度学习的医学图像分割领域进行了全面回顾，分析了过去十年的进展和关键发展，并从七个关键维度探讨了该领域的发展趋势和未来方向。&lt;h4&gt;背景&lt;/h4&gt;医学图像分割在过去二十年迅速发展，主要由深度学习驱动，能够精确高效地描绘不同成像模态中的细胞、组织、器官和病变。然而，当前模型在克服持续挑战方面仍存在差距。&lt;h4&gt;目的&lt;/h4&gt;探讨当前医学图像分割模型克服挑战的程度，识别仍存在的差距，并提供该领域的深入回顾，以激发未来创新。&lt;h4&gt;方法&lt;/h4&gt;对医学图像分割进行系统性回顾，追踪过去十年的进展，检查分割网络各部分的核心原则（多尺度分析、注意机制、先验知识整合），并围绕七个关键维度组织讨论。&lt;h4&gt;主要发现&lt;/h4&gt;医学图像分割领域经历了七大转变：(1)从监督学习向半监督/无监督学习转变；(2)从器官分割向病变重点任务转变；(3)多模态整合和域适应进展；(4)基础模型和迁移学习作用增强；(5)从确定性向概率性分割转变；(6)从2D向3D和4D分割进展；(7)从模型调用向分割代理趋势发展。&lt;h4&gt;结论&lt;/h4&gt;这些观点提供了基于深度学习的医学图像分割轨迹的全面概述，旨在激发未来创新，并通过维护持续更新的文献和开源资源库支持持续研究。&lt;h4&gt;翻译&lt;/h4&gt;医学图像分割在过去二十年取得了迅速发展，主要得益于深度学习，这使得能够在不同成像模态中精确高效地描绘细胞、组织、器官和病变。这一进展提出了一个基本问题：当前模型在多大程度上克服了持续存在的挑战，以及仍存在哪些差距？在本工作中，我们对医学图像分割进行了深入回顾，追溯了过去十年的进展和关键发展。我们检查了核心原则，包括多尺度分析、注意机制和先验知识整合，贯穿分割网络的编码器、瓶颈、跳跃连接和解码器组件。我们的讨论围绕七个关键维度组织：(1)从监督学习向半监督/无监督学习的转变，(2)从器官分割向病变重点任务的转变，(3)多模态整合和域适应的进展，(4)基础模型和迁移学习的作用，(5)从确定性向概率性分割的转变，(6)从2D向3D和4D分割的进展，以及(7)从模型调用向分割代理的趋势。这些观点共同提供了基于深度学习的医学图像分割轨迹的全面概述，并旨在激发未来的创新。为支持持续研究，我们在https://github.com/apple1986/medicalSegReview维护了一个持续更新的相关文献和开源资源库。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical image segmentation has advanced rapidly over the past two decades,largely driven by deep learning, which has enabled accurate and efficientdelineation of cells, tissues, organs, and pathologies across diverse imagingmodalities. This progress raises a fundamental question: to what extent havecurrent models overcome persistent challenges, and what gaps remain? In thiswork, we provide an in-depth review of medical image segmentation, tracing itsprogress and key developments over the past decade. We examine core principles,including multiscale analysis, attention mechanisms, and the integration ofprior knowledge, across the encoder, bottleneck, skip connections, and decodercomponents of segmentation networks. Our discussion is organized around sevenkey dimensions: (1) the shift from supervised to semi-/unsupervised learning,(2) the transition from organ segmentation to lesion-focused tasks, (3)advances in multi-modality integration and domain adaptation, (4) the role offoundation models and transfer learning, (5) the move from deterministic toprobabilistic segmentation, (6) the progression from 2D to 3D and 4Dsegmentation, and (7) the trend from model invocation to segmentation agents.Together, these perspectives provide a holistic overview of the trajectory ofdeep learning-based medical image segmentation and aim to inspire futureinnovation. To support ongoing research, we maintain a continually updatedrepository of relevant literature and open-source resources athttps://github.com/apple1986/medicalSegReview</description>
      <author>example@mail.com (Guoping Xu, Jayaram K. Udupa, Jax Luo, Songlin Zhao, Yajun Yu, Scott B. Raymond, Hao Peng, Lipeng Ning, Yogesh Rathi, Wei Liu, You Zhang)</author>
      <guid isPermaLink="false">2508.20139v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>MMG-Vid: Maximizing Marginal Gains at Segment-level and Token-level for Efficient Video LLMs</title>
      <link>http://arxiv.org/abs/2508.21044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MMG-Vid的新型无训练视觉标记剪枝框架，通过在段级和标记级最大化边际增益来减少视频理解中的冗余视觉标记，同时保持高性能。&lt;h4&gt;背景&lt;/h4&gt;Video Large Language Models在视频理解方面表现出色，但过多的视觉标记给实际应用带来显著计算挑战。当前方法通过视觉标记剪枝提高推理效率，但未考虑视频帧的动态特性和时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法未考虑视频动态特性和时间依赖性的问题，提高视频理解模型的效率，同时保持性能。&lt;h4&gt;方法&lt;/h4&gt;提出MMG-Vid框架，首先基于帧相似性将视频分段，动态分配每个段的标记预算以最大化边际增益；然后提出时间引导的DPC算法，联合建模帧间独特性和帧内多样性，最大化每个标记的边际增益。&lt;h4&gt;主要发现&lt;/h4&gt;MMG-Vid可保持原始性能的99.5%以上，有效减少75%的视觉标记，在LLaVA-OneVision-7B上将预填充阶段加速3.9倍。&lt;h4&gt;结论&lt;/h4&gt;MMG-Vid通过结合段级和标记级的最大边际增益方法，显著提高了效率，同时保持了强大的性能。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型在视频理解方面表现出色，但它们过多的视觉标记给实际应用带来了显著的计算挑战。当前方法通过视觉标记剪枝来提高推理效率，然而，它们没有考虑视频帧的动态特性和时间依赖性，因为它们将视频理解视为多帧任务。为了解决这些挑战，我们提出了MMG-Vid，一种新颖的无训练视觉标记剪枝框架，通过在段级和标记级最大化边际增益来去除冗余。具体来说，我们首先基于帧相似性将视频分段，然后动态分配每个段的标记预算以最大化每个段的边际增益。随后，我们提出了一种时间引导的DPC算法，联合建模帧间独特性和帧内多样性，从而最大化每个标记的边际增益。通过结合两个阶段，MMG-Vid可以最大化有限标记预算的利用，显著提高效率同时保持强大的性能。大量实验表明，MMG-Vid可以保持原始性能的99.5%以上，同时有效减少75%的视觉标记，并在LLaVA-OneVision-7B上将预填充阶段加速3.9倍。代码即将发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Large Language Models (VLLMs) excel in video understanding, but theirexcessive visual tokens pose a significant computational challenge forreal-world applications. Current methods aim to enhance inference efficiency byvisual token pruning. However, they do not consider the dynamic characteristicsand temporal dependencies of video frames, as they perceive video understandingas a multi-frame task. To address these challenges, we propose MMG-Vid, a noveltraining-free visual token pruning framework that removes redundancy byMaximizing Marginal Gains at both segment-level and token-level. Specifically,we first divide the video into segments based on frame similarity, and thendynamically allocate the token budget for each segment to maximize the marginalgain of each segment. Subsequently, we propose a temporal-guided DPC algorithmthat jointly models inter-frame uniqueness and intra-frame diversity, therebymaximizing the marginal gain of each token. By combining both stages, MMG-Vidcan maximize the utilization of the limited token budget, significantlyimproving efficiency while maintaining strong performance. Extensiveexperiments demonstrate that MMG-Vid can maintain over 99.5% of the originalperformance, while effectively reducing 75% visual tokens and accelerating theprefilling stage by 3.9x on LLaVA-OneVision-7B. Code will be released soon.</description>
      <author>example@mail.com (Junpeng Ma, Qizhe Zhang, Ming Lu, Zhibin Wang, Qiang Zhou, Jun Song, Shanghang Zhang)</author>
      <guid isPermaLink="false">2508.21044v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>ChainReaction! Structured Approach with Causal Chains as Intermediate Representations for Improved and Explainable Causal Video Question Answering</title>
      <link>http://arxiv.org/abs/2508.21010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://paritoshparmar.github.io/chainreaction/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的模块化框架，用于因果视频问答模型，通过解耦因果推理与答案生成，引入自然语言因果链作为可解释的中间表示，实现了透明且逻辑连贯的推理。&lt;h4&gt;背景&lt;/h4&gt;现有的因果视频问答模型难以处理高阶推理，依赖于不透明的整体流水线，将视频理解、因果推理和答案生成纠缠在一起，导致可解释性有限且依赖浅层启发式方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种可解释性强、能够进行高阶因果推理的视频问答框架，提高模型的可解释性、用户信任度和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出两阶段架构：因果链提取器(CCE)从视频-问题对生成因果链，因果链驱动回答器(CCDA)基于这些因果链生成答案；使用大型语言模型从现有数据集生成高质量因果链；提出CauCo评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;在三个大规模基准测试上，该方法不仅优于最先进模型，还在可解释性、用户信任和泛化能力方面取得显著提升。&lt;h4&gt;结论&lt;/h4&gt;该框架将CCE定位为可在不同领域重用的因果推理引擎，为因果视频问答提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;现有的因果视频问答(Causal-Why VideoQA)模型通常难以处理高阶推理，依赖于不透明的、整体的流水线，这些流水线将视频理解、因果推理和答案生成纠缠在一起。这些黑盒方法提供有限的解释性，并倾向于依赖浅层启发式方法。我们提出了一种新颖的模块化框架，明确地将因果推理与答案生成解耦，引入自然语言因果链作为可解释的中间表示。受人类认知模型启发，这些结构化的因果-效应序列桥接了低级视频内容与高级因果推理，实现了透明且逻辑连贯的推理。我们的两阶段架构包括一个从视频-问题对生成因果链的因果链提取器(CCE)，以及一个基于这些因果链生成答案的因果链驱动回答器(CCDA)。为了解决缺乏标注推理轨迹的问题，我们引入了一种可扩展的方法，使用大型语言模型从现有数据集生成高质量的因果链。我们还提出了CauCo，一个面向因果描述的新评估指标。在三个大规模基准测试上的实验表明，我们的方法不仅优于最先进的模型，还在可解释性、用户信任和泛化能力方面取得了显著提升——将CCE定位为可在不同领域重用的因果推理引擎。项目页面：https://paritoshparmar.github.io/chainreaction/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing Causal-Why Video Question Answering (VideoQA) models often strugglewith higher-order reasoning, relying on opaque, monolithic pipelines thatentangle video understanding, causal inference, and answer generation. Theseblack-box approaches offer limited interpretability and tend to depend onshallow heuristics. We propose a novel, modular framework that explicitlydecouples causal reasoning from answer generation, introducing natural languagecausal chains as interpretable intermediate representations. Inspired by humancognitive models, these structured cause-effect sequences bridge low-levelvideo content with high-level causal reasoning, enabling transparent andlogically coherent inference. Our two-stage architecture comprises a CausalChain Extractor (CCE) that generates causal chains from video-question pairs,and a Causal Chain-Driven Answerer (CCDA) that produces answers grounded inthese chains. To address the lack of annotated reasoning traces, we introduce ascalable method for generating high-quality causal chains from existingdatasets using large language models. We also propose CauCo, a new evaluationmetric for causality-oriented captioning. Experiments on three large-scalebenchmarks demonstrate that our approach not only outperforms state-of-the-artmodels, but also yields substantial gains in explainability, user trust, andgeneralization -- positioning the CCE as a reusable causal reasoning engineacross diverse domains. Project page:https://paritoshparmar.github.io/chainreaction/</description>
      <author>example@mail.com (Paritosh Parmar, Eric Peh, Basura Fernando)</author>
      <guid isPermaLink="false">2508.21010v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Video-MTR: Reinforced Multi-Turn Reasoning for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2508.20478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Video-MTR，一个增强的多步推理框架，用于长视频理解。该框架通过迭代选择视频片段和渐进理解问题，结合双层奖励系统，实现了端到端的训练，无需外部视觉语言模型，在多个基准测试上表现出色。&lt;h4&gt;背景&lt;/h4&gt;长视频理解面临长程时间依赖和多个事件的挑战。现有方法通常依赖静态推理或外部视觉语言模型(VLMs)，存在复杂性和次优性能问题，缺乏端到端训练。&lt;h4&gt;目的&lt;/h4&gt;提出Video-MTR框架，实现迭代的关键视频片段选择和问题理解，解决长视频理解的挑战。&lt;h4&gt;方法&lt;/h4&gt;Video-MTR采用多步推理而非传统单步推理，基于对已处理片段的 evolving 理解和当前问题渐进选择视频片段。引入门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的步级奖励，优化视频片段选择和问题理解，消除对外部VLMs的需求，允许端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;在VideoMME、MLVU和EgoSchema等基准测试上，Video-MTR在准确性和效率上都优于现有方法，推进了长视频理解的最先进水平。&lt;h4&gt;结论&lt;/h4&gt;Video-MTR通过多步推理和新的奖励系统，有效解决了长视频理解的挑战，代表了该领域的进步。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解以其长程时间依赖和多个事件为特征，仍然是一个挑战。现有方法通常依赖静态推理或外部视觉语言模型(VLMs)，这些方法面临复杂性和次优性能问题，缺乏端到端训练。在本文中，我们提出Video-MTR，一个增强的多步推理框架，旨在实现迭代的关键视频片段选择和问题理解。与传统的单步生成预测的视频推理管道不同，Video-MTR在多步中进行推理，基于对已处理片段的 evolving 理解和当前问题，渐进地选择视频片段。这种迭代过程允许对视频进行更精细和上下文感知的分析。为确保中间推理过程，我们引入了一种新颖的门控双层奖励系统，结合基于答案正确性的轨迹级奖励和强调帧-查询相关性的步级奖励。该系统优化了视频片段选择和问题理解，消除了对外部VLMs的需求，允许端到端训练。在VideoMME、MLVU和EgoSchema等基准上的广泛实验表明，Video-MTR在准确性和效率上都优于现有方法，推进了长视频理解的最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-form video understanding, characterized by long-range temporaldependencies and multiple events, remains a challenge. Existing methods oftenrely on static reasoning or external visual-language models (VLMs), which faceissues like complexity and sub-optimal performance due to the lack ofend-to-end training. In this paper, we propose Video-MTR, a reinforcedmulti-turn reasoning framework designed to enable iterative key video segmentselection and question comprehension. Unlike traditional video reasoningpipeline, which generate predictions in a single turn, Video-MTR performsreasoning in multiple turns, selecting video segments progressively based onthe evolving understanding of previously processed segments and the currentquestion. This iterative process allows for a more refined and contextuallyaware analysis of the video. To ensure intermediate reasoning process, weintroduce a novel gated bi-level reward system, combining trajectory-levelrewards based on answer correctness and turn-level rewards emphasizingframe-query relevance. This system optimizes both video segment selection andquestion comprehension, eliminating the need for external VLMs and allowingend-to-end training. Extensive experiments on benchmarks like VideoMME, MLVU,and EgoSchema demonstrate that Video-MTR outperforms existing methods in bothaccuracy and efficiency, advancing the state-of-the-art in long videounderstanding.</description>
      <author>example@mail.com (Yuan Xie, Tianshui Chen, Zheng Ge, Lionel Ni)</author>
      <guid isPermaLink="false">2508.20478v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Latent Factor Point Processes for Patient Representation in Electronic Health Records</title>
      <link>http://arxiv.org/abs/2508.20327v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  33 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究者提出了一种潜在因子点过程模型和Fourier-Eigen嵌入方法，用于有效分析电子健康记录中的时间结构数据，能够识别临床上有意义的患者亚组异质性。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录包含有价值的纵向患者级信息，但大多数统计方法将EHR代码的不规则时间简化为简单计数，丢弃了丰富的时间结构。现有时间模型通常施加限制性参数假设，或针对代码级而非患者级任务定制。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留EHR数据时间结构并适用于患者级任务的新模型，以更好地理解患者群体的异质性。&lt;h4&gt;方法&lt;/h4&gt;提出潜在因子点过程模型，将代码出现表示为高维点过程，其条件强度由低维潜在泊松过程驱动；基于此模型引入Fourier-Eigen嵌入，从观察过程的光谱密度矩阵构建患者表示方法。&lt;h4&gt;主要发现&lt;/h4&gt;理论保证表明这些嵌入能有效捕获下游分类和聚类中特定亚组的时间模式；模拟和阿尔茨海默病EHR队列应用证明了该方法在揭示临床意义异质性方面的实际优势。&lt;h4&gt;结论&lt;/h4&gt;潜在因子点过程模型结合Fourier-Eigen嵌入能够有效利用EHR数据中的时间结构，提高对患者亚组的识别和分析能力，有助于发现临床上有意义的患者群体异质性。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHR)包含有价值的纵向患者级信息，但大多数统计方法将EHR代码的不规则时间简化为简单计数，从而丢弃了丰富的时间结构。现有的时间模型通常施加限制性参数假设，或者针对代码级而非患者级任务定制。我们提出潜在因子点过程模型，该模型将代码出现表示为高维点过程，其条件强度由低维潜在泊松过程驱动。这种低秩结构反映了临床现实：数千种代码由少量潜在疾病过程控制，同时能够在高维中实现统计有效估计。基于此模型，我们引入了Fourier-Eigen嵌入，这是一种从观察过程的光谱密度矩阵构建的患者表示方法。我们建立了理论保证，表明这些嵌入能够有效捕获下游分类和聚类中特定亚组的时间模式。模拟和应用于阿尔茨海默病EHR队列的研究证明了该方法在揭示临床意义异质性方面的实际优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic health records (EHR) contain valuable longitudinal patient-levelinformation, yet most statistical methods reduce the irregular timing of EHRcodes into simple counts, thereby discarding rich temporal structure. Existingtemporal models often impose restrictive parametric assumptions or are tailoredto code level rather than patient-level tasks. We propose the latent factorpoint process model, which represents code occurrences as a high-dimensionalpoint process whose conditional intensity is driven by a low dimensional latentPoisson process. This low-rank structure reflects the clinical reality thatthousands of codes are governed by a small number of underlying diseaseprocesses, while enabling statistically efficient estimation in highdimensions. Building on this model, we introduce the Fourier-Eigen embedding, apatient representation constructed from the spectral density matrix of theobserved process. We establish theoretical guarantees showing that theseembeddings efficiently capture subgroup-specific temporal patterns fordownstream classification and clustering. Simulations and an application to anAlzheimer's disease EHR cohort demonstrate the practical advantages of ourapproach in uncovering clinically meaningful heterogeneity.</description>
      <author>example@mail.com (Parker Knight, Doudou Zhou, Zongqi Xia, Tianxi Cai, Junwei Lu)</author>
      <guid isPermaLink="false">2508.20327v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
      <link>http://arxiv.org/abs/2508.19650v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队提出了Video-LevelGauge基准，专门用于系统评估大型视频语言模型(LVLMs)中的位置偏差问题。通过标准化探测器和定制化上下文设置，该基准能够模拟多样化现实场景，并采用统计测量与形态模式识别相结合的分析方法。研究评估了27个最先进LVLMs，发现开源模型普遍存在位置偏差，而商业模型如Gemini2.5-Pro表现更一致。&lt;h4&gt;背景&lt;/h4&gt;大型视频语言模型(LVLMs)在视频理解方面取得了显著进展，相应的评估基准也在不断发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了上下文位置偏差这一关键但未被充分探索的方面。&lt;h4&gt;目的&lt;/h4&gt;设计一个名为Video-LevelGauge的专门基准，用于系统评估LVLMs中的位置偏差问题，并通过灵活控制上下文长度、探测位置和上下文类型来模拟多样化的现实场景。&lt;h4&gt;方法&lt;/h4&gt;采用标准化探测器和定制化上下文设置，引入结合统计测量和形态模式识别的综合分析方法。基准包含438个手动策划的视频，产生1,177个高质量多项选择题和120个开放性问题。基于这些内容，评估了27个最先进的LVLMs，包括商业和开源模型。&lt;h4&gt;主要发现&lt;/h4&gt;许多领先的开源模型存在显著的位置偏差，通常表现出头部或邻居内容偏好。相比之下，商业模型如Gemini2.5-Pro在整个视频序列上表现出令人印象深刻的一致性能。对上下文长度、上下文变化和模型规模的进一步分析提供了减轻偏差和指导模型增强的可行见解。&lt;h4&gt;结论&lt;/h4&gt;Video-LevelGauge基准能够有效暴露位置偏差，评估结果揭示了不同类型模型在位置偏差方面的表现差异，为减轻偏差和指导模型改进提供了实用见解。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型(LVLMs)在视频理解方面取得了显著进展，推动了相应评估基准的发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了上下文位置偏差这一关键但未被充分探索的LVLM性能方面。我们提出了Video-LevelGauge，一个专门设计的基准，用于系统评估LVLMs中的位置偏差。我们采用标准化探测器和定制化上下文设置，允许灵活控制上下文长度、探测位置和上下文类型，以模拟多样化的现实场景。此外，我们引入了一种结合统计测量与形态模式识别的综合分析方法来表征偏差。我们的基准包含438个手动策划的视频，涵盖多种类型，产生了1,177个高质量多项选择题和120个开放性问题，其有效性已得到验证，能够有效暴露位置偏差。基于这些内容，我们评估了27个最先进的LVLMs，包括商业和开源模型。我们的发现显示，许多领先的开源模型存在显著的位置偏差，通常表现出头部或邻居内容偏好。相比之下，商业模型如Gemini2.5-Pro在整个视频序列上表现出令人印象深刻的一致性能。对上下文长度、上下文变化和模型规模的进一步分析提供了减轻偏差和指导模型增强的可行见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding modelenhancement.https://github.com/Cola-any/Video-LevelGauge</description>
      <author>example@mail.com (Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang)</author>
      <guid isPermaLink="false">2508.19650v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>SpecVLM: Enhancing Speculative Decoding of Video LLMs via Verifier-Guided Token Pruning</title>
      <link>http://arxiv.org/abs/2508.16201v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 Main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出SpecVLM，一个针对视频大语言模型的无训练推测解码框架，通过分阶段视频标记剪枝技术，在不牺牲准确性的情况下将解码速度提高最多2.68倍。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型(Vid-LLMs)在理解视频内容方面展现出强大能力，但它们对密集视频标记表示的依赖在预填充和解码阶段引入了大量内存和计算开销。&lt;h4&gt;目的&lt;/h4&gt;为减轻最近视频标记减少方法的信息损失，并无损加速Vid-LLMs的解码阶段。&lt;h4&gt;方法&lt;/h4&gt;SpecVLM是一个专为Vid-LLMs设计的无训练推测解码框架，结合分阶段视频标记剪枝。基于草稿模型推测对视频标记剪枝具有低敏感性的发现，可剪枝高达90%的视频标记。通过两阶段剪枝实现：第一阶段根据验证器注意力信号选择高信息量标记，第二阶段以空间均匀方式剪枝剩余冗余标记。&lt;h4&gt;主要发现&lt;/h4&gt;草稿模型的推测对视频标记剪枝具有低敏感性，允许剪枝高达90%的视频标记而不牺牲准确性。&lt;h4&gt;结论&lt;/h4&gt;SpecVLM在四个视频理解基准测试上展示了有效性和鲁棒性，对于LLaVA-OneVision-72B实现高达2.68倍的解码加速，对于Qwen2.5-VL-32B实现2.11倍的加速。&lt;h4&gt;翻译&lt;/h4&gt;视频大语言模型(Vid-LLMs)在理解视频内容方面展现出强大的能力。然而，它们对密集视频标记表示的依赖在预填充和解码阶段都引入了大量的内存和计算开销。为了减轻最近视频标记减少方法的信息损失并无损地加速Vid-LLMs的解码阶段，我们引入了SpecVLM，这是一个专为Vid-LLMs设计的无训练推测解码框架，结合了分阶段视频标记剪枝。基于我们的新发现，即草稿模型的推测对视频标记剪枝具有低敏感性，SpecVLM可以剪枝高达90%的视频标记，实现高效推测而不牺牲准确性。为此，我们执行了一个两阶段剪枝过程：第一阶段根据验证器的注意力信号选择信息量高的标记，第二阶段以空间均匀方式剪枝剩余的冗余标记。在四个视频理解基准测试上的大量实验证明了SpecVLM的有效性和鲁棒性，对于LLaVA-OneVision-72B实现了高达2.68倍的解码加速，对于Qwen2.5-VL-32B实现了2.11倍的加速。代码可在https://github.com/zju-jiyicheng/SpecVLM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video large language models (Vid-LLMs) have shown strong capabilities inunderstanding video content. However, their reliance on dense video tokenrepresentations introduces substantial memory and computational overhead inboth prefilling and decoding. To mitigate the information loss of recent videotoken reduction methods and accelerate the decoding stage of Vid-LLMslosslessly, we introduce SpecVLM, a training-free speculative decoding (SD)framework tailored for Vid-LLMs that incorporates staged video token pruning.Building on our novel finding that the draft model's speculation exhibits lowsensitivity to video token pruning, SpecVLM prunes up to 90% of video tokens toenable efficient speculation without sacrificing accuracy. To achieve this, weperforms a two-stage pruning process: Stage I selects highly informative tokensguided by attention signals from the verifier (target model), while Stage IIprunes remaining redundant ones in a spatially uniform manner. Extensiveexperiments on four video understanding benchmarks demonstrate theeffectiveness and robustness of SpecVLM, which achieves up to 2.68$\times$decoding speedup for LLaVA-OneVision-72B and 2.11$\times$ speedup forQwen2.5-VL-32B. Code is available at https://github.com/zju-jiyicheng/SpecVLM.</description>
      <author>example@mail.com (Yicheng Ji, Jun Zhang, Heming Xia, Jinpeng Chen, Lidan Shou, Gang Chen, Huan Li)</author>
      <guid isPermaLink="false">2508.16201v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Learning Robust Spatial Representations from Binaural Audio through Feature Distillation</title>
      <link>http://arxiv.org/abs/2508.20914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in Proc. WASPAA 2025, October 12-15, 2025, Tahoe, US.  Copyright (c) 2025 IEEE. 5 pages, 2 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究基于特征蒸馏的预训练方法，学习双耳语音的空间表示，无需标签数据，并在到达方向估计任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;深度表示学习在多个音频任务中表现出强大的性能，但在用于从多通道音频学习空间表示方面的研究还不足。&lt;h4&gt;目的&lt;/h4&gt;研究一种基于特征蒸馏的预训练阶段，学习稳健的双耳语音空间表示，无需数据标签。&lt;h4&gt;方法&lt;/h4&gt;从干净的双耳语音样本计算空间特征形成预测标签，使用神经网络从增强语音预测这些特征；预训练后，使用学习到的编码器权重初始化DoA估计模型并进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;与全监督模型和经典信号处理方法相比，预训练模型在嘈杂和混响环境中对到达方向估计表现出改进的性能。&lt;h4&gt;结论&lt;/h4&gt;基于特征蒸馏的预训练方法可以有效地学习空间表示，并在到达方向估计任务中表现更好。&lt;h4&gt;翻译&lt;/h4&gt;最近，深度表示学习在多个音频任务中表现出强大的性能。然而，其在用于从多通道音频学习空间表示方面的应用研究还不够充分。我们研究了一种基于特征蒸馏的预训练阶段，用于学习稳健的双耳语音空间表示，无需数据标签。在此框架中，从干净的双耳语音样本计算空间特征以形成预测标签。然后使用神经网络从相应的增强语音预测这些干净特征。预训练后，我们丢弃空间特征预测器，使用学习到的编码器权重初始化一个DoA估计模型，并对其进行微调以进行到达方向估计。我们的实验表明，与全监督模型和经典信号处理方法相比，经过微调的预训练模型在嘈杂和混响环境中进行到达方向估计时表现出改进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, deep representation learning has shown strong performance inmultiple audio tasks. However, its use for learning spatial representationsfrom multichannel audio is underexplored. We investigate the use of apretraining stage based on feature distillation to learn a robust spatialrepresentation of binaural speech without the need for data labels. In thisframework, spatial features are computed from clean binaural speech samples toform prediction labels. These clean features are then predicted fromcorresponding augmented speech using a neural network. After pretraining, wethrow away the spatial feature predictor and use the learned encoder weights toinitialize a DoA estimation model which we fine-tune for DoA estimation. Ourexperiments demonstrate that the pretrained models show improved performance innoisy and reverberant environments after fine-tuning for direction-of-arrivalestimation, when compared to fully supervised models and classic signalprocessing methods.</description>
      <author>example@mail.com (Holger Severin Bovbjerg, Jan Østergaard, Jesper Jensen, Shinji Watanabe, Zheng-Hua Tan)</author>
      <guid isPermaLink="false">2508.20914v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>EEGDM: Learning EEG Representation with Latent Diffusion Model</title>
      <link>http://arxiv.org/abs/2508.20705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出EEGDM，一种基于潜在扩散模型的新型自监督EEG表示学习方法，利用EEG信号生成作为自监督目标，将扩散模型转变为能够捕捉EEG语义的强表示学习器。&lt;h4&gt;背景&lt;/h4&gt;使用深度学习进行脑电图信号分析虽有潜力，但现有方法在跨不同任务学习可泛化表示方面面临挑战，尤其在训练数据有限时。当前EEG表示学习方法如EEGPT和LaBraM通常依赖简单掩码重构目标，可能无法完全捕捉EEG信号中的丰富语义信息和复杂模式。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉EEG语义信息并适用于多种下游任务的表示学习方法，解决现有方法在泛化性和数据效率方面的问题。&lt;h4&gt;方法&lt;/h4&gt;EEGDM包含一个EEG编码器，将EEG信号及其通道增强蒸馏为紧凑表示，作为条件信息指导扩散模型生成EEG信号。这种设计赋予EEGDM紧凑的潜在空间，既提供生成过程控制，又可用于下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;EEGDM能够(1)重建高质量EEG信号，(2)有效学习鲁棒表示，(3)在各类下游任务中仅需少量预训练数据即可实现竞争性性能。&lt;h4&gt;结论&lt;/h4&gt;EEGDM具有良好的泛化性和实用性，为EEG信号分析提供了新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;尽管使用深度学习进行脑电图信号分析已显示出巨大潜力，但现有方法在跨不同任务学习可泛化表示方面仍面临重大挑战，尤其是在训练数据有限的情况下。当前的EEG表示学习方法包括EEGPT和LaBraM通常依赖于简单的掩码重构目标，这可能无法完全捕捉EEG信号中固有的丰富语义信息和复杂模式。在本文中，我们提出EEGDM，一种基于潜在扩散模型的新型自监督EEG表示学习方法，它利用EEG信号生成作为自监督目标，将扩散模型转变为能够捕捉EEG语义的强表示学习器。EEGDM包含一个EEG编码器，将EEG信号及其通道增强蒸馏为紧凑表示，作为条件信息指导扩散模型生成EEG信号。这种设计赋予EEGDM紧凑的潜在空间，不仅为生成过程提供充分控制，还可用于下游任务。实验结果表明，EEGDM(1)能够重建高质量EEG信号，(2)能有效学习鲁棒表示，(3)在各类下游任务中仅需适度的预训练数据量即可实现竞争性性能，凸显了其泛化性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While electroencephalography (EEG) signal analysis using deep learning hasshown great promise, existing approaches still face significant challenges inlearning generalizable representations that perform well across diverse tasks,particularly when training data is limited. Current EEG representation learningmethods including EEGPT and LaBraM typically rely on simple maskedreconstruction objective, which may not fully capture the rich semanticinformation and complex patterns inherent in EEG signals. In this paper, wepropose EEGDM, a novel self-supervised EEG representation learning method basedon the latent diffusion model, which leverages EEG signal generation as aself-supervised objective, turning the diffusion model into a strongrepresentation learner capable of capturing EEG semantics. EEGDM incorporatesan EEG encoder that distills EEG signals and their channel augmentations into acompact representation, acting as conditional information to guide thediffusion model for generating EEG signals. This design endows EEGDM with acompact latent space, which not only offers ample control over the generativeprocess but also can be leveraged for downstream tasks. Experimental resultsshow that EEGDM (1) can reconstruct high-quality EEG signals, (2) effectivelylearns robust representations, and (3) achieves competitive performance withmodest pre-training data size across diverse downstream tasks, underscoring itsgeneralizability and practical utility.</description>
      <author>example@mail.com (Shaocong Wang, Tong Liu, Ming Li, Minjing Yu, Yong-Jin Liu)</author>
      <guid isPermaLink="false">2508.20705v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Masked Autoencoders for Ultrasound Signals: Robust Representation Learning for Downstream Applications</title>
      <link>http://arxiv.org/abs/2508.20622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Access. This is a preprint version. 14 pages, 6  figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究掩码自编码器(MAEs)与视觉变换器(ViT)架构在一维超声信号自监督表示学习中的适应性和性能&lt;h4&gt;背景&lt;/h4&gt;MAEs在计算机视觉等领域已取得显著成功，但在一维信号分析特别是原始超声数据中的应用尚未充分探索。超声信号在工业应用中至关重要，如无损检测和结构健康监测，但标记数据稀缺且信号处理高度特定于任务&lt;h4&gt;目的&lt;/h4&gt;提出利用MAE在未标记合成超声信号上预训练的方法，使模型学习鲁棒表示，增强下游任务(如飞行时间分类)的性能&lt;h4&gt;方法&lt;/h4&gt;系统地研究模型大小、块大小和掩码比率对预训练效率和下游准确性的影响&lt;h4&gt;主要发现&lt;/h4&gt;预训练模型显著优于从头训练的模型和针对下游任务优化的卷积神经网络基线；在合成数据上预训练比仅使用有限真实数据集训练展现出更好的可迁移性到真实测量信号&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了MAEs通过可扩展的自监督学习推进超声信号分析的潜力&lt;h4&gt;翻译&lt;/h4&gt;我们研究了掩码自编码器(MAEs)与视觉变换器(ViT)架构在一维(1D)超声信号自监督表示学习中的适应性和性能。尽管MAEs在计算机视觉和其他领域已显示出显著成功，但其在1D信号分析，特别是原始超声数据中的应用 largely unexplored。超声信号在工业应用中至关重要，如无损检测(NDT)和结构健康监测(SHM)，其中标记数据通常稀缺，且信号处理高度特定于任务。我们提出一种利用MAE在未标记的合成超声信号上预训练的方法，使模型能够学习鲁棒表示，增强下游任务(如飞行时间ToF分类)的性能。本研究系统地研究了模型大小、块大小和掩码比率对预训练效率和下游准确性的影响。我们的结果表明，预训练模型显著优于从头训练的模型和针对下游任务优化的强卷积神经网络(CNN)基线。此外，在合成数据上预训练比仅在有有限真实数据集上训练展现出更好的可迁移性到真实测量信号。这项研究强调了MAEs通过可扩展的自监督学习推进超声信号分析的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We investigated the adaptation and performance of Masked Autoencoders (MAEs)with Vision Transformer (ViT) architectures for self-supervised representationlearning on one-dimensional (1D) ultrasound signals. Although MAEs havedemonstrated significant success in computer vision and other domains, theiruse for 1D signal analysis, especially for raw ultrasound data, remains largelyunexplored. Ultrasound signals are vital in industrial applications such asnon-destructive testing (NDT) and structural health monitoring (SHM), wherelabeled data are often scarce and signal processing is highly task-specific. Wepropose an approach that leverages MAE to pre-train on unlabeled syntheticultrasound signals, enabling the model to learn robust representations thatenhance performance in downstream tasks, such as time-of-flight (ToF)classification. This study systematically investigated the impact of modelsize, patch size, and masking ratio on pre-training efficiency and downstreamaccuracy. Our results show that pre-trained models significantly outperformmodels trained from scratch and strong convolutional neural network (CNN)baselines optimized for the downstream task. Additionally, pre-training onsynthetic data demonstrates superior transferability to real-world measuredsignals compared with training solely on limited real datasets. This studyunderscores the potential of MAEs for advancing ultrasound signal analysisthrough scalable, self-supervised learning.</description>
      <author>example@mail.com (Immanuel Roßteutscher, Klaus S. Drese, Thorsten Uphues)</author>
      <guid isPermaLink="false">2508.20622v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Human-AI Collaborative Bot Detection in MMORPGs</title>
      <link>http://arxiv.org/abs/2508.20578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于对比表示学习和聚类技术的无监督框架，用于检测MMORPGs中的自动升级机器人，并通过大语言模型和可视化方法提高检测效率和可解释性。&lt;h4&gt;背景&lt;/h4&gt;在大型多人在线角色扮演游戏(MMORPGs)中，自动升级机器人利用自动化程序大规模提升角色等级，破坏游戏平衡和公平性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效检测自动升级机器人的方法，同时提供可解释的检测结果以避免法律和用户体验问题。&lt;h4&gt;方法&lt;/h4&gt;利用对比表示学习和聚类技术，在完全无监督的情况下识别具有相似升级模式的角色组；引入大语言模型作为辅助审查者验证聚类结果；开发基于增长曲线的可视化方法辅助评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合无监督学习、大语言模型辅助审查和可视化方法，可以提高机器人检测的效率和准确性，同时保持决策的可解释性。&lt;h4&gt;结论&lt;/h4&gt;这种协作方法改进了机器人检测工作流程的效率，同时保持可解释性，支持MMORPGs中可扩展且负责任的机器人监管。&lt;h4&gt;翻译&lt;/h4&gt;在大型多人在线角色扮演游戏(MMORPGs)中，自动升级机器人利用自动化程序大规模提升角色等级，破坏游戏平衡和公平性。检测此类机器人具有挑战性，不仅因为它们模仿人类行为，还因为惩罚措施需要可解释的理由以避免法律和用户体验问题。在本文中，我们提出了一种新颖的框架，通过利用对比表示学习和聚类技术，以完全无监督的方式识别具有相似升级模式的角色组。为确保可靠决策，我们引入大语言模型(LLM)作为辅助审查者来验证聚类组，有效模拟二次人工判断。我们还引入了基于增长曲线的可视化方法，帮助LLM和人类评估员评估升级行为。这种协作方法提高了机器人检测工作流程的效率，同时保持可解释性，从而支持MMORPGs中可扩展且负责任的机器人监管。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Massively Multiplayer Online Role-Playing Games (MMORPGs), auto-levelingbots exploit automated programs to level up characters at scale, undermininggameplay balance and fairness. Detecting such bots is challenging, not onlybecause they mimic human behavior, but also because punitive actions requireexplainable justification to avoid legal and user experience issues. In thispaper, we present a novel framework for detecting auto-leveling bots byleveraging contrastive representation learning and clustering techniques in afully unsupervised manner to identify groups of characters with similarlevel-up patterns. To ensure reliable decisions, we incorporate a LargeLanguage Model (LLM) as an auxiliary reviewer to validate the clustered groups,effectively mimicking a secondary human judgment. We also introduce a growthcurve-based visualization to assist both the LLM and human moderators inassessing leveling behavior. This collaborative approach improves theefficiency of bot detection workflows while maintaining explainability, therebysupporting scalable and accountable bot regulation in MMORPGs.</description>
      <author>example@mail.com (Jaeman Son, Hyunsoo Kim)</author>
      <guid isPermaLink="false">2508.20578v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Evolution of Intelligent Agents in Large-Scale Microservice Systems</title>
      <link>http://arxiv.org/abs/2508.20508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于多智能体协同进化机制的智能服务优化方法，用于解决大规模微服务架构中的治理挑战。该方法通过将服务建模为智能体，利用图表示学习构建依赖关系图，采用集中式训练和分布式执行框架，并通过游戏驱动的策略优化机制实现服务间的自适应协作和行为进化。&lt;h4&gt;背景&lt;/h4&gt;大规模微服务架构面临治理挑战，包括复杂的服务依赖关系、动态的拓扑结构以及波动的工作负载。&lt;h4&gt;目的&lt;/h4&gt;提出一种智能服务优化方法，以解决大规模微服务架构中的治理挑战，提高系统性能和适应性。&lt;h4&gt;方法&lt;/h4&gt;将每个服务建模为智能体；引入图表示学习构建服务依赖关系图；基于马尔可夫决策过程学习每个智能体的策略；采用集中式训练和分布式执行的框架；设计游戏驱动的策略优化机制，通过选择-变异过程动态调整智能体策略分布。&lt;h4&gt;主要发现&lt;/h4&gt;在面对突发工作负载激增、拓扑重新配置或资源冲突等场景时，系统能够快速响应并实现稳定的策略收敛；在协调效率、适应性和策略收敛性能等关键指标上优于其他先进方法；显著提高了大规模微服务系统中的治理效率和运行稳定性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在大规模微服务系统中具有很高的实用价值和工程可行性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于多智能体协同进化机制的智能服务优化方法，以解决大规模微服务架构中的治理挑战。这些挑战包括复杂的服务依赖关系、动态的拓扑结构以及波动的工作负载。该方法将每个服务建模为一个智能体，并引入图表示学习来构建服务依赖关系图。这使得智能体能感知和嵌入系统内的结构变化。每个智能体基于马尔可夫决策过程学习其策略。采用集中式训练和分布式执行的框架，将局部自主性与全局协调相结合。为了增强整体系统性能和适应性，设计了一个游戏驱动的策略优化机制。通过选择-变异过程，动态调整智能体策略分布。这支持服务间的自适应协作和行为进化。在该机制下，系统在面对突发工作负载激增、拓扑重新配置或资源冲突等场景时，能够快速响应并实现稳定的策略收敛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes an intelligent service optimization method based on amulti-agent collaborative evolution mechanism to address governance challengesin large-scale microservice architectures. These challenges include complexservice dependencies, dynamic topology structures, and fluctuating workloads.The method models each service as an agent and introduces graph representationlearning to construct a service dependency graph. This enables agents toperceive and embed structural changes within the system. Each agent learns itspolicy based on a Markov Decision Process. A centralized training anddecentralized execution framework is used to integrate local autonomy withglobal coordination. To enhance overall system performance and adaptability, agame-driven policy optimization mechanism is designed. Through aselection-mutation process, agent strategy distributions are dynamicallyadjusted. This supports adaptive collaboration and behavioral evolution amongservices. Under this mechanism, the system can quickly respond and achievestable policy convergence when facing scenarios such as sudden workload spikes,topology reconfigurations, or resource conflicts. To evaluate the effectivenessof the proposed method, experiments are conducted on a representativemicroservice simulation platform. Comparative analyses are performed againstseveral advanced approaches, focusing on coordination efficiency, adaptability,and policy convergence performance. Experimental results show that the proposedmethod outperforms others in several key metrics. It significantly improvesgovernance efficiency and operational stability in large-scale microservicesystems. The method demonstrates strong practical value and engineeringfeasibility.</description>
      <author>example@mail.com (Yilin Li, Song Han, Sibo Wang, Ming Wang, Renzi Meng)</author>
      <guid isPermaLink="false">2508.20508v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Prediction of Distant Metastasis for Head and Neck Cancer Patients Using Multi-Modal Tumor and Peritumoral Feature Fusion Network</title>
      <link>http://arxiv.org/abs/2508.20469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 4 figures, 5 tables. Zizhao Tang, Changhao Liu, and Nuo  Tong contributed equally. Corresponding Authors: Mei Shi  (mshi82@fmmu.edu.cn), Shuiping Gou (shpgou@mail.xidian.edu.cn)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究开发了一种结合CT图像、放射组学和临床数据的深度学习多模态框架，用于预测头颈鳞状细胞癌患者的转移风险。模型在1497名患者数据上进行了验证，融合模型显著优于单模态模型，为个性化治疗规划提供了有价值的临床决策支持工具。&lt;h4&gt;背景&lt;/h4&gt;转移是头颈鳞状细胞癌临床管理中的主要挑战。治疗前可靠预测转移风险对优化治疗策略和改善患者预后至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于深度学习的多模态框架，通过整合CT图像、放射组学和临床数据来预测HNSCC患者的转移风险。&lt;h4&gt;方法&lt;/h4&gt;研究纳入1497名HNSCC患者，从治疗前CT图像中获取肿瘤和器官掩膜，使用3D Swin Transformer提取肿瘤区域深度特征，通过PyRadiomics获取并筛选放射组学特征，编码临床变量并与成像特征融合，最后将多模态特征输入全连接网络进行转移风险预测，使用五折交叉验证评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;融合模型显著优于单模态模型，单独3D深度学习模块AUC为0.715，结合放射组学和临床特征后性能提升（AUC=0.803，ACC=0.752，SEN=0.730，SPE=0.758）；模型在不同肿瘤亚型中具有良好泛化能力；不同模态提供互补信息；3D Swin Transformer比传统网络提供更强大的表示学习能力。&lt;h4&gt;结论&lt;/h4&gt;该多模态融合模型在预测HNSCC转移风险方面表现出高准确性和鲁棒性，为肿瘤生物学提供全面表征，具有作为临床决策支持工具的潜力，可辅助个性化治疗规划。&lt;h4&gt;翻译&lt;/h4&gt;转移仍然是头颈鳞状细胞癌临床管理中的主要挑战。治疗前可靠预测转移风险对于优化治疗策略和预后至关重要。本研究开发了一种基于深度学习的多模态框架，通过整合计算机断层扫描图像、放射组学和临床数据来预测HNSCC患者的转移风险。研究纳入了1497名HNSCC患者。从治疗前CT图像中获取肿瘤和器官掩膜。3D Swin Transformer从肿瘤区域提取深度特征。同时，使用PyRadiomics获得1562个放射组学特征，随后进行相关过滤和随机森林选择，保留36个特征。包括年龄、性别、吸烟和饮酒状况的临床变量被编码并与成像衍生的特征融合。多模态特征被输入全连接网络以预测转移风险。使用曲线下面积、准确率、敏感性和特异性通过五折交叉验证评估性能。所提出的融合模型优于单模态模型。单独的3D深度学习模块实现了0.715的AUC，当与放射组学和临床特征结合时，预测性能得到改善（AUC=0.803，ACC=0.752，SEN=0.730，SPE=0.758）。分层分析显示在不同肿瘤亚型中具有泛化能力。消融研究表明不同模态提供互补信息。评估显示3D Swin Transformer比传统网络提供更强大的表示学习能力。这种多模态融合模型在预测HNSCC转移风险方面表现出高准确性和鲁棒性，为肿瘤生物学提供了全面表征。该可解释模型有潜力作为临床决策支持工具用于个性化治疗规划。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Metastasis remains the major challenge in the clinical management of head andneck squamous cell carcinoma (HNSCC). Reliable pre-treatment prediction ofmetastatic risk is crucial for optimizing treatment strategies and prognosis.This study develops a deep learning-based multimodal framework to predictmetastasis risk in HNSCC patients by integrating computed tomography (CT)images, radiomics, and clinical data. 1497 HNSCC patients were included. Tumorand organ masks were derived from pretreatment CT images. A 3D Swin Transformerextracted deep features from tumor regions. Meanwhile, 1562 radiomics featureswere obtained using PyRadiomics, followed by correlation filtering and randomforest selection, leaving 36 features. Clinical variables including age, sex,smoking, and alcohol status were encoded and fused with imaging-derivedfeatures. Multimodal features were fed into a fully connected network topredict metastasis risk. Performance was evaluated using five-foldcross-validation with area under the curve (AUC), accuracy (ACC), sensitivity(SEN), and specificity (SPE). The proposed fusion model outperformedsingle-modality models. The 3D deep learning module alone achieved an AUC of0.715, and when combined with radiomics and clinical features, predictiveperformance improved (AUC = 0.803, ACC = 0.752, SEN = 0.730, SPE = 0.758).Stratified analysis showed generalizability across tumor subtypes. Ablationstudies indicated complementary information from different modalities.Evaluation showed the 3D Swin Transformer provided more robust representationlearning than conventional networks. This multimodal fusion model demonstratedhigh accuracy and robustness in predicting metastasis risk in HNSCC, offering acomprehensive representation of tumor biology. The interpretable model haspotential as a clinical decision-support tool for personalized treatmentplanning.</description>
      <author>example@mail.com (Zizhao Tang, Changhao Liu, Nuo Tong, Shuiping Gou, Mei Shi)</author>
      <guid isPermaLink="false">2508.20469v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>BiListing: Modality Alignment for Listings</title>
      <link>http://arxiv.org/abs/2508.20396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出BiListing方法，通过大型语言模型和预训练语言-图像模型对齐房源文本和图像，将非结构化数据转化为单一嵌入向量，实现零样本搜索、克服冷启动问题，并成功应用于Airbnb搜索系统，带来0.425%的NDCG提升和数千万美元增量收入。&lt;h4&gt;背景&lt;/h4&gt;Airbnb作为住宿平台传统上依赖结构化数据，而表示学习兴起使得利用文本和图像信息变得可行。然而，房源包含多样的非结构化数据（多图像、多文本文档），将不同嵌入组合为单一表示具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出BiListing方法解决非结构化数据处理问题，实现更有效的房源搜索和推荐。&lt;h4&gt;方法&lt;/h4&gt;BiListing（双模态房源）利用大型语言模型和预训练语言-图像模型对齐房源文本和图像，创建每个房源和模态的单一嵌入向量。&lt;h4&gt;主要发现&lt;/h4&gt;BiListing实现了零样本搜索能力，克服冷启动问题，支持单模态或双模态房源间搜索；在搜索排名模型中应用带来0.425%的NDCG提升和数千万美元增量收入。&lt;h4&gt;结论&lt;/h4&gt;BiListing方法成功解决了Airbnb平台上非结构化数据处理挑战，通过结合先进AI技术实现了更有效的搜索和推荐，创造了显著商业价值。&lt;h4&gt;翻译&lt;/h4&gt;Airbnb是提供旅行住宿的领导者。传统上，Airbnb依赖结构化数据来理解、排名和向客人推荐房源，这是由于从文本和图像中提取有意义信息的能力有限且复杂。随着表示学习的兴起，利用文本和照片中的丰富信息变得更加容易。然而，一个Airbnb房源包含多样化的非结构化数据：多个图像和各种非结构化文本文档，如标题、描述和评论，这使得这种方法具有挑战性。本文提出了BiListing，一种利用大型语言模型和预训练的语言-图像模型来对齐房源文本和图像的方法。我们进行了离线和在线测试，在Airbnb搜索排名模型中使用BiListing嵌入，并成功将其部署到生产环境中，实现了0.425%的NDCG提升，并带来了数千万美元的增量收入。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761577&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Airbnb is a leader in offering travel accommodations. Airbnb has historicallyrelied on structured data to understand, rank, and recommend listings to guestsdue to the limited capabilities and associated complexity arising fromextracting meaningful information from text and images. With the rise ofrepresentation learning, leveraging rich information from text and photos hasbecome easier. A popular approach has been to create embeddings for textdocuments and images to enable use cases of computing similarities betweenlistings or using embeddings as features in an ML model.  However, an Airbnb listing has diverse unstructured data: multiple images,various unstructured text documents such as title, description, and reviews,making this approach challenging. Specifically, it is a non-trivial task tocombine multiple embeddings of different pieces of information to reach asingle representation.  This paper proposes BiListing, for Bimodal Listing, an approach to align textand photos of a listing by leveraging large-language models and pretrainedlanguage-image models. The BiListing approach has several favorablecharacteristics: capturing unstructured data into a single embedding vector perlisting and modality, enabling zero-shot capability to search inventoryefficiently in user-friendly semantics, overcoming the cold start problem, andenabling listing-to-listing search along a single modality, or both.  We conducted offline and online tests to leverage the BiListing embeddings inthe Airbnb search ranking model, and successfully deployed it in production,achieved 0.425% of NDCB gain, and drove tens of millions in incrementalrevenue.</description>
      <author>example@mail.com (Guillaume Guy, Mihajlo Grbovic, Chun How Tan, Han Zhao)</author>
      <guid isPermaLink="false">2508.20396v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Latent Variable Modeling for Robust Causal Effect Estimation</title>
      <link>http://arxiv.org/abs/2508.20259v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025. This is the full version including extended  appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将潜在变量模型整合到双重机器学习范式中的新框架，用于在存在隐藏因素的情况下实现稳健的因果效应估计。研究考虑了潜在变量只影响结果或同时影响处理和结果的两种场景，并通过实验验证了方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;潜在变量模型为在观测数据中纳入和推断未观测因素提供了强大框架。在因果推断中，它们有助于解释影响处理或结果的隐藏因素，解决缺失或未测量协变量带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架，将潜在变量建模整合到双重机器学习范式中，以在存在隐藏因素的情况下实现稳健的因果效应估计。&lt;h4&gt;方法&lt;/h4&gt;提出将潜在变量模型整合到双重机器学习（DML）范式的新框架，仅在DML的第二阶段纳入潜在变量，将表示学习与潜在推断分离。考虑了两种场景：潜在变量只影响结果，以及潜在变量可能同时影响处理和结果。&lt;h4&gt;主要发现&lt;/h4&gt;通过在合成和真实世界数据集上的大量实验，证明了所提出方法的稳健性和有效性。&lt;h4&gt;结论&lt;/h4&gt;将潜在变量模型整合到双重机器学习范式中，能够有效处理存在隐藏因素的因果效应估计问题，且方法具有稳健性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;潜在变量模型为在观测数据中纳入和推断未观测因素提供了强大的框架。在因果推断中，它们有助于解释影响处理或结果的隐藏因素，从而解决缺失或未测量协变量带来的挑战。本文提出了一种新的框架，将潜在变量建模整合到双重机器学习（DML）范式中，以在存在此类隐藏因素的情况下实现稳健的因果效应估计。我们考虑了两种场景：一种是潜在变量只影响结果，另一种是潜在变量可能同时影响处理和结果。为确保可处理性，我们仅在DML的第二阶段纳入潜在变量，将表示学习与潜在推断分离。我们通过在合成和真实世界数据集上的大量实验，证明了我们方法的稳健性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent variable models provide a powerful framework for incorporating andinferring unobserved factors in observational data. In causal inference, theyhelp account for hidden factors influencing treatment or outcome, therebyaddressing challenges posed by missing or unmeasured covariates. This paperproposes a new framework that integrates latent variable modeling into thedouble machine learning (DML) paradigm to enable robust causal effectestimation in the presence of such hidden factors. We consider two scenarios:one where a latent variable affects only the outcome, and another where it mayinfluence both treatment and outcome. To ensure tractability, we incorporatelatent variables only in the second stage of DML, separating representationlearning from latent inference. We demonstrate the robustness and effectivenessof our method through extensive experiments on both synthetic and real-worlddatasets.</description>
      <author>example@mail.com (Tetsuro Morimura, Tatsushi Oka, Yugo Suzuki, Daisuke Moriwaki)</author>
      <guid isPermaLink="false">2508.20259v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19884v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于结构多样性的无参数图神经网络框架SDGNN，通过统一的结构多样性消息传递机制，同时捕捉邻域结构异构性和特征语义稳定性，无需额外参数和复杂训练，在多种挑战条件下优于主流GNN方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在结构化数据建模任务中表现出色，但主流方法依赖大量可训练参数和固定聚合规则，难以适应强结构异构性和复杂特征分布的图数据，导致节点表示过平滑和语义退化。&lt;h4&gt;目的&lt;/h4&gt;解决传统GNN方法在结构异构图数据上的适应性问题，提出一种无需额外参数且能有效处理结构异质性的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;设计了SDGNN(结构多样性图神经网络)框架，受结构多样性理论启发，实现统一的结构多样性消息传递机制，从结构驱动和特征驱动两个视角进行互补建模，不引入额外可训练参数。&lt;h4&gt;主要发现&lt;/h4&gt;在八个公共基准数据集和PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战条件下始终优于主流GNN方法，验证了结构多样性作为图表示学习中核心信号的重要性。&lt;h4&gt;结论&lt;/h4&gt;SDGNN为无参数图神经网络设计提供了新理论视角和通用方法，证明了结构多样性在图表示学习中的核心价值，并已公开完整实现代码。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在结构化数据建模任务(如节点分类)中表现出色。然而，主流方法通常依赖于大量可训练参数和固定的聚合规则，使得难以适应具有强结构异构性和复杂特征分布的图数据。这通常导致节点表示的过平滑和语义退化。为解决这些问题，本文提出了一种基于结构多样性的无参数图神经网络框架，即SDGNN(结构多样性图神经网络)。该框架受结构多样性理论启发，设计了统一的结构多样性消息传递机制，同时捕捉邻域结构的异构性和特征语义的稳定性，无需引入额外的可训练参数。与传统参数化方法不同，SDGNN不依赖复杂的模型训练，而是利用结构驱动和特征驱动视角的互补建模，从而有效提高数据集和场景间的适应性。实验结果表明，在八个公共基准数据集和一个跨学科的PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等挑战条件下始终优于主流GNN。这项工作为无参数图神经网络的设计提供了新的理论视角和通用方法，并进一步验证了结构多样性作为图表示学习中核心信号的重要性。为便于复现和进一步研究，SDGNN的完整实现已在https://github.com/mingyue15694/SGDNN/tree/main发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable performance in structureddata modeling tasks such as node classification. However, mainstream approachesgenerally rely on a large number of trainable parameters and fixed aggregationrules, making it difficult to adapt to graph data with strong structuralheterogeneity and complex feature distributions. This often leads toover-smoothing of node representations and semantic degradation. To addressthese issues, this paper proposes a parameter-free graph neural networkframework based on structural diversity, namely SDGNN (Structural-DiversityGraph Neural Network). The framework is inspired by structural diversity theoryand designs a unified structural-diversity message passing mechanism thatsimultaneously captures the heterogeneity of neighborhood structures and thestability of feature semantics, without introducing additional trainableparameters. Unlike traditional parameterized methods, SDGNN does not rely oncomplex model training, but instead leverages complementary modeling from bothstructure-driven and feature-driven perspectives, thereby effectively improvingadaptability across datasets and scenarios. Experimental results show that oneight public benchmark datasets and an interdisciplinary PubMed citationnetwork, SDGNN consistently outperforms mainstream GNNs under challengingconditions such as low supervision, class imbalance, and cross-domain transfer.This work provides a new theoretical perspective and general approach for thedesign of parameter-free graph neural networks, and further validates theimportance of structural diversity as a core signal in graph representationlearning. To facilitate reproducibility and further research, the fullimplementation of SDGNN has been released at:https://github.com/mingyue15694/SGDNN/tree/main</description>
      <author>example@mail.com (Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu)</author>
      <guid isPermaLink="false">2508.19884v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty Aware-Predictive Control Barrier Functions: Safer Human Robot Interaction through Probabilistic Motion Forecasting</title>
      <link>http://arxiv.org/abs/2508.20812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种不确定性感知预测控制屏障函数(UA-PCBFs)框架，用于解决人机协作机器人中的安全与灵活性平衡问题，通过将概率性人类运动预测与控制屏障函数的安全保证相结合，显著提升了人机交互的流畅性和安全性。&lt;h4&gt;背景&lt;/h4&gt;在人机共享工作空间中，协作机器人需要在保证严格安全的同时实现灵活高效的行为。人类运动的随机性和任务依赖性构成了动态障碍，而现有方法通常依赖纯反应式或最坏情况预测，导致不必要的制动和任务停滞。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理预测不确定性的框架，避免过度保守的规划算法，从而提高协作机器人的灵活性和响应能力，同时保持安全保证。&lt;h4&gt;方法&lt;/h4&gt;引入不确定性感知预测控制屏障函数(UA-PCBFs)框架，该框架融合了概率性人类手部运动预测与控制屏障函数的正式安全保证，利用预测模块提供的不确定性估计动态调整安全边界。&lt;h4&gt;主要发现&lt;/h4&gt;UA-PCBFs通过不确定性估计使协作机器人能更深入理解未来人类状态，实现更流畅智能的交互；在真实世界实验中，相比最先进的人机交互架构，在任务关键指标上表现更好，显著减少了机器人安全空间违规次数。&lt;h4&gt;结论&lt;/h4&gt;UA-PCBFs框架有效平衡了安全保证与灵活响应的需求，通过自动化设置和直接人机交互实验验证了其有效性，在安全性和灵活性方面相比现有方法有显著提升。&lt;h4&gt;翻译&lt;/h4&gt;为了在人与机器人共享工作空间的环境中实现灵活、高效的自动化，协作机器人单元必须将严格的安全保证与响应式有效行为的需求相协调。动态障碍物是人类运动的随机性和任务依赖性变化：当机器人仅依赖纯反应式或最坏情况包络时，它们会不必要的制动，阻碍任务进展，并损害真正人机交互所需的流畅性。近年来，基于学习的人类运动预测迅速发展，但大多数方法产生最坏情况预测，通常没有很好地结构化处理预测不确定性，导致过度保守的规划算法，限制了其灵活性。我们引入了不确定性感知预测控制屏障函数(UA-PCBFs)，这是一个统一框架，将概率性人类手部运动预测与控制屏障函数的正式安全保证相融合。与其他变体相比，由于预测模块提供的人类运动不确定性估计，我们的框架允许动态调整安全边界。得益于不确定性估计，UA-PCBFs使协作机器人能够更深入地理解未来人类状态，通过信息丰富的运动规划促进更流畅、智能的交互。我们通过逐渐增加现实感的全面真实世界实验验证了UA-PCBFs，包括使用机械手的自动化设置（执行完全可重复的运动）和直接人机交互（验证及时性、可用性和人类信心）。与最先进的人机交互架构相比，UA-PCBFs在任务关键指标上表现出更好的性能，显著减少了交互过程中机器人安全空间违规次数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To enable flexible, high-throughput automation in settings where people androbots share workspaces, collaborative robotic cells must reconcile stringentsafety guarantees with the need for responsive and effective behavior. Adynamic obstacle is the stochastic, task-dependent variability of human motion:when robots fall back on purely reactive or worst-case envelopes, they brakeunnecessarily, stall task progress, and tamper with the fluidity that trueHuman-Robot Interaction demands. In recent years, learning-based human-motionprediction has rapidly advanced, although most approaches produce worst-casescenario forecasts that often do not treat prediction uncertainty in awell-structured way, resulting in over-conservative planning algorithms,limiting their flexibility. We introduce Uncertainty-Aware Predictive ControlBarrier Functions (UA-PCBFs), a unified framework that fuses probabilistichuman hand motion forecasting with the formal safety guarantees of ControlBarrier Functions. In contrast to other variants, our framework allows fordynamic adjustment of the safety margin thanks to the human motion uncertaintyestimation provided by a forecasting module. Thanks to uncertainty estimation,UA-PCBFs empower collaborative robots with a deeper understanding of futurehuman states, facilitating more fluid and intelligent interactions throughinformed motion planning. We validate UA-PCBFs through comprehensive real-worldexperiments with an increasing level of realism, including automated setups (toperform exactly repeatable motions) with a robotic hand and direct human-robotinteractions (to validate promptness, usability, and human confidence).Relative to state-of-the-art HRI architectures, UA-PCBFs show betterperformance in task-critical metrics, significantly reducing the number ofviolations of the robot's safe space during interaction with respect to thestate-of-the-art.</description>
      <author>example@mail.com (Lorenzo Busellato, Federico Cunico, Diego Dall'Alba, Marco Emporio, Andrea Giachetti, Riccardo Muradore, Marco Cristani)</author>
      <guid isPermaLink="false">2508.20812v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>CardioMorphNet: Cardiac Motion Prediction Using a Shape-Guided Bayesian Recurrent Deep Network</title>
      <link>http://arxiv.org/abs/2508.20734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CardioMorphNet是一种循环贝叶斯深度学习框架，用于3D心脏形状引导的可变形配准，通过关注心脏解剖区域而非仅依赖图像强度，提高了心脏运动估计的准确性，并能提供不确定性估计。&lt;h4&gt;背景&lt;/h4&gt;从心脏磁共振图像准确估计心脏运动对于评估心脏功能和检测异常至关重要。现有方法往往难以准确捕捉心脏运动，因为它们依赖于基于强度的图像配准相似性损失，这可能忽略心脏解剖区域。&lt;h4&gt;目的&lt;/h4&gt;提出CardioMorphNet，一个用于3D心脏形状引导可变形配准的循环贝叶斯深度学习框架，使用短轴心脏磁共振图像。&lt;h4&gt;方法&lt;/h4&gt;CardioMorphNet采用循环变分自编码器来建模心脏周期中的时空依赖性，并有两个后验模型用于双心室分割和运动估计。从贝叶斯公式导出的损失函数指导框架通过递归配准分割图来关注解剖区域，而不使用基于强度的图像配准相似性损失，同时利用短轴体积序列和时空特征。贝叶斯建模还能计算估计运动场的不确定性图。&lt;h4&gt;主要发现&lt;/h4&gt;在英国生物银行数据集上通过比较变形后的掩模形状与真实掩模进行验证，CardioMorphNet在心脏运动估计方面表现出优于最先进方法的性能。不确定性评估表明，与其他基于概率的心脏配准方法相比，它在心脏区域的估计运动场产生较低的不确定性值，表明其预测具有更高的置信度。&lt;h4&gt;结论&lt;/h4&gt;CardioMorphNet是一种有效的心脏运动估计方法，通过关注解剖区域而非仅依赖图像强度，提高了准确性，并能提供不确定性估计。&lt;h4&gt;翻译&lt;/h4&gt;从心脏磁共振图像准确估计心脏运动对于评估心脏功能和检测其异常至关重要。现有方法往往难以准确捕捉心脏运动，因为它们依赖于基于强度的图像配准相似性损失，这可能忽略心脏解剖区域。为此，我们提出了CardioMorphNet，一个用于3D心脏形状引导可变形配准的循环贝叶斯深度学习框架，使用短轴心脏磁共振图像。它采用循环变分自编码器来建模心脏周期中的时空依赖性，以及两个用于双心室分割和运动估计的后验模型。从贝叶斯公式导出的损失函数指导框架通过递归配准分割图来关注解剖区域，而不使用基于强度的图像配准相似性损失，同时利用短轴体积序列和时空特征。贝叶斯建模还能计算估计运动场的不确定性图。在英国生物银行数据集上通过比较变形后的掩模形状与真实掩模进行验证，CardioMorphNet在心脏运动估计方面表现出优于最先进方法的性能。不确定性评估表明，与其他基于概率的心脏配准方法相比，它在心脏区域的估计运动场产生较低的不确定性值，表明其预测具有更高的置信度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cardiac motion estimation from cine cardiac magnetic resonance (CMR)images is vital for assessing cardiac function and detecting its abnormalities.Existing methods often struggle to capture heart motion accurately because theyrely on intensity-based image registration similarity losses that may overlookcardiac anatomical regions. To address this, we propose CardioMorphNet, arecurrent Bayesian deep learning framework for 3D cardiac shape-guideddeformable registration using short-axis (SAX) CMR images. It employs arecurrent variational autoencoder to model spatio-temporal dependencies overthe cardiac cycle and two posterior models for bi-ventricular segmentation andmotion estimation. The derived loss function from the Bayesian formulationguides the framework to focus on anatomical regions by recursively registeringsegmentation maps without using intensity-based image registration similarityloss, while leveraging sequential SAX volumes and spatio-temporal features. TheBayesian modelling also enables computation of uncertainty maps for theestimated motion fields. Validated on the UK Biobank dataset by comparingwarped mask shapes with ground truth masks, CardioMorphNet demonstratessuperior performance in cardiac motion estimation, outperformingstate-of-the-art methods. Uncertainty assessment shows that it also yieldslower uncertainty values for estimated motion fields in the cardiac regioncompared with other probabilistic-based cardiac registration methods,indicating higher confidence in its predictions.</description>
      <author>example@mail.com (Reza Akbari Movahed, Abuzar Rezaee, Arezoo Zakeri, Colin Berry, Edmond S. L. Ho, Ali Gooya)</author>
      <guid isPermaLink="false">2508.20734v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph-Based Feature Augmentation for Predictive Tasks on Relational Datasets</title>
      <link>http://arxiv.org/abs/2508.20986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ReCoGNN是一个端到端的自动化特征增强框架，通过从多个关系表中提取特征来增强数据集，支持预测任务，在分类和回归任务上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;数据已成为金融、医疗和电子商务等领域创新的基础资产。在这些领域，预测建模被广泛使用，且越来越强调通过自动化机器学习技术减少人工努力。&lt;h4&gt;目的&lt;/h4&gt;探索特征增强是否可以自动化，并识别和利用任务相关的关系信号，以提高预测建模的性能。&lt;h4&gt;方法&lt;/h4&gt;ReCoGNN首先通过建模表内属性关系来捕获每个表内的语义依赖性，将表划分为结构化、语义连贯的段；然后构建一个表示所有段之间行间关系的异构加权图；最后利用消息传递图神经网络通过图传播信息，指导特征选择并增强原始数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在十个真实和合成数据集上进行的广泛实验表明，ReCoGNN在分类和回归任务上始终优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ReCoGNN成功实现了自动化特征增强，能够有效识别和利用任务相关的关系信号，提高预测建模的性能。&lt;h4&gt;翻译&lt;/h4&gt;数据已成为驱动金融、医疗和电子商务等领域创新的基础资产。在这些领域中，基于关系表的预测建模被广泛使用，并且越来越强调通过自动化机器学习技术减少人工努力。这引发了一个有趣的问题：特征增强本身是否可以自动化，并识别和利用任务相关的关系信号？为了应对这一挑战，我们提出了一个端到端的自动化特征增强框架ReCoGNN，它利用从多个关系表中提取的特征来增强初始数据集，以支持预测任务。ReCoGNN首先通过建模表内属性关系来捕获每个表内的语义依赖性，从而将表划分为结构化、语义连贯的段。然后，它构建一个表示所有段之间行间关系的异构加权图。最后，ReCoGNN利用消息传递图神经网络通过图传播信息，指导特征选择并增强原始数据集。在十个真实和合成数据集上进行的广泛实验表明，ReCoGNN在分类和回归任务上始终优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data has become a foundational asset driving innovation across domains suchas finance, healthcare, and e-commerce. In these areas, predictive modelingover relational tables is commonly employed, with increasing emphasis onreducing manual effort through automated machine learning (AutoML) techniques.This raises an interesting question: can feature augmentation itself beautomated and identify and utilize task-related relational signals?  To address this challenge, we propose an end-to-end automated featureaugmentation framework, ReCoGNN, which enhances initial datasets using featuresextracted from multiple relational tables to support predictive tasks. ReCoGNNfirst captures semantic dependencies within each table by modeling intra-tableattribute relationships, enabling it to partition tables into structured,semantically coherent segments. It then constructs a heterogeneous weightedgraph that represents inter-row relationships across all segments. Finally,ReCoGNN leverages message-passing graph neural networks to propagateinformation through the graph, guiding feature selection and augmenting theoriginal dataset. Extensive experiments conducted on ten real-life andsynthetic datasets demonstrate that ReCoGNN consistently outperforms existingmethods on both classification and regression tasks.</description>
      <author>example@mail.com (Lianpeng Qiao, Ziqi Cao, Kaiyu Feng, Ye Yuan, Guoren Wang)</author>
      <guid isPermaLink="false">2508.20986v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning Based Concurrency Bug Detection and Localization</title>
      <link>http://arxiv.org/abs/2508.20911v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的并发错误检测和定位方法，通过构建专门的并发错误数据集，集成预训练模型与异构图神经网络，并引入并发感知代码属性图(CCPG)来表征并发语义。使用SubgraphX方法精确定位错误到具体代码行，相比最先进方法在准确率和精确率上提高10%，召回率提高26%。&lt;h4&gt;背景&lt;/h4&gt;并发错误由多线程或分布式系统中共享资源的不当同步引起，难以检测，影响软件可靠性和安全性。现有深度学习方法存在三个主要局限：缺乏大型专用并发错误数据集、缺乏并发语义充分表示、二分类结果无法提供精确的bug行等细粒度调试信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，用于有效的并发错误检测和定位，解决现有深度学习方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建专门的并发错误数据集；集成预训练模型与异构图神经网络；引入并发感知代码属性图(CCPG)表征并发语义；使用SubgraphX GNN可解释性方法精确定位并发错误到具体源代码行。&lt;h4&gt;主要发现&lt;/h4&gt;相比最先进方法，该方法在准确率和精确率上平均提高10%，召回率提高26%，在各种评估设置中均表现出色。&lt;h4&gt;结论&lt;/h4&gt;该方法能有效检测并发错误并精确定位，通过专用数据集和并发语义表征解决了现有方法局限，提供了更细粒度的调试信息。&lt;h4&gt;翻译&lt;/h4&gt;并发错误是由多线程或分布式系统中共享资源的不当同步引起的， notoriously难以检测，因此损害软件的可靠性和安全性。现有的深度学习方法面临三个主要限制。首先，缺乏大型且多样化的并发错误专用数据集。其次，它们缺乏并发语义的充分表示。第三，二分类结果无法提供更细粒度的调试信息，如精确的错误行。为解决这些问题，我们提出了一种用于有效并发错误检测和定位的新方法。我们构建了一个专门的并发错误数据集，以促进模型训练和评估。然后，我们通过融入一种新的并发感知代码属性图(CCPG)来集成预训练模型和异构图神经网络(GNN)，该图简洁有效地表征了并发语义。为进一步促进调试，我们采用了SubgraphX，一种基于GNN的可解释性方法，它探索图结构以精确定位并发错误，将其映射到特定的源代码行。平均而言，在各种评估设置中，我们的方法相比最先进方法在准确率和精确率上提高了10%，在召回率上提高了26%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Concurrency bugs, caused by improper synchronization of shared resources inmulti-threaded or distributed systems, are notoriously hard to detect and thuscompromise software reliability and security. The existing deep learningmethods face three main limitations. First, there is an absence of large anddedicated datasets of diverse concurrency bugs for them. Second, they lacksufficient representation of concurrency semantics. Third, binaryclassification results fail to provide finer-grained debug information such asprecise bug lines. To address these problems, we propose a novel method foreffective concurrency bug detection as well as localization. We construct adedicated concurrency bug dataset to facilitate model training and evaluation.We then integrate a pre-trained model with a heterogeneous graph neural network(GNN), by incorporating a new Concurrency-Aware Code Property Graph (CCPG) thatconcisely and effectively characterizes concurrency semantics. To furtherfacilitate debugging, we employ SubgraphX, a GNN-based interpretability method,which explores the graphs to precisely localize concurrency bugs, mapping themto specific lines of source code. On average, our method demonstrates animprovement of 10\% in accuracy and precision and 26\% in recall compared tostate-of-the-art methods across diverse evaluation settings.</description>
      <author>example@mail.com (Zuocheng Feng, Kaiwen Zhang, Miaomiao Wang, Yiming Cheng, Yuandao Cai, Xiaofeng Li, Guanjun Liu)</author>
      <guid isPermaLink="false">2508.20911v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Deep learning for jet modification in the presence of the QGP background</title>
      <link>http://arxiv.org/abs/2508.20856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用深度学习方法预测喷流在夸克-胶子等离子体中的能量损失，比较了卷积神经网络和动态图卷积神经网络在有无背景条件下的性能表现。&lt;h4&gt;背景&lt;/h4&gt;在相对论重离子碰撞中，传统评估喷流与色禁闭QCD介质相互作用的方法是测量喷流观测量分布相对于质子-质子碰撞基线的修正。深度学习方法可以实现对每个喷流的评估，增强喷流作为核介质精密探针的用途。&lt;h4&gt;目的&lt;/h4&gt;预测喷流在通过夸克-胶子等离子体(QGP)介质时的逐喷流能量损失分数χ，并评估不同深度学习方法在真实实验条件下的性能。&lt;h4&gt;方法&lt;/h4&gt;使用线性玻尔兹曼传输(LBT)模型预测能量损失，将介质修正的喷流嵌入热背景中并应用组分减法进行背景去除。研究两种网络架构：使用喷流图像的卷积神经网络(CNNs)和使用粒子云的动态图卷积神经网络(DGCNNs)。&lt;h4&gt;主要发现&lt;/h4&gt;CNNs对无背景喷流能实现准确预测，但在存在QGP背景时会退化，即使在背景去除后仍低于无背景基线。相比之下，应用于背景去除后粒子云的DGCNNs在整个χ范围内保持高精度。&lt;h4&gt;结论&lt;/h4&gt;基于点云的图神经网络在真实条件下能够利用完整喷流结构，展现出显著优势，适合用于喷流与介质相互作用的精确研究。&lt;h4&gt;翻译&lt;/h4&gt;在相对论重离子碰撞中，喷流与色禁闭QCD介质的传统相互作用评估方法是测量喷流观测量分布相对于其在质子-质子碰撞中基线的修正。深度学习方法能够实现对这些修正的逐喷流评估，增强喷流作为核介质精密探针的用途。在这项工作中，我们使用线性玻尔兹曼传输(LBT)模型预测喷流在通过夸克-胶子等离子体(QGP)介质时的逐喷流能量损失分数χ。为了近似真实的实验条件，我们将介质修正的喷流嵌入热背景中并应用组分减法进行背景去除。研究了两种网络架构：使用喷流图像的卷积神经网络(CNNs)和使用粒子云的动态图卷积神经网络(DGCNNs)。我们发现CNNs对无背景喷流能实现准确预测，但在存在QGP背景时会退化，即使在背景去除后仍低于无背景基线。相比之下，应用于背景去除后粒子云的DGCNNs在整个χ范围内保持高精度，展示了基于点云的图神经网络在真实条件下利用完整喷流结构的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Jet interactions with the color-deconfined QCD medium in relativisticheavy-ion collisions are conventionally assessed by measuring the modificationof the distributions of jet observables with respect to their baselines inproton-proton collisions. Deep learning methods enable per-jet evaluation ofthese modifications, enhancing the use of jets as precision probes of thenuclear medium. In this work, we predict the jet-by-jet fractional energy loss$\chi$ for jets evolving through a quark-gluon plasma (QGP) medium using aLinear Boltzmann Transport (LBT) model. To approximate realistic experimentalconditions, we embed medium-modified jets in a thermal background and applyConstituent Subtraction for background removal. Two network architectures arestudied: convolutional neural networks (CNNs) using jet images, and dynamicgraph convolutional neural networks (DGCNNs) using particle clouds. We findthat CNNs achieve accurate predictions for background-free jets but degrade inthe presence of the QGP background and remain below the background-freebaseline even after background subtraction. In contrast, DGCNNs applied tobackground-subtracted particle clouds maintain high accuracy across the entire$\chi$ range, demonstrating the advantage of point-cloud-based graph neuralnetworks that exploit full jet structure under realistic conditions.</description>
      <author>example@mail.com (Ran Li, Yi-Lun Du, Shanshan Cao)</author>
      <guid isPermaLink="false">2508.20856v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>ATM-GAD: Adaptive Temporal Motif Graph Anomaly Detection for Financial Transaction Networks</title>
      <link>http://arxiv.org/abs/2508.20829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ATM-GAD的自适应图神经网络，通过利用时间基元进行金融异常检测，能够捕捉金融欺诈的两个时间特征：时间基元和账户特定的异常活动间隔。&lt;h4&gt;背景&lt;/h4&gt;金融欺诈检测对保护数十亿美元至关重要，但现代金融系统中相互关联的实体和快速变化的交易行为常常击败传统机器学习模型。基于图的检测器虽取得进展，但仍忽略时间方面的欺诈特征。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时间基元和账户特定异常活动间隔的金融欺诈检测方法，提高检测准确率并发现传统方法遗漏的欺诈模式。&lt;h4&gt;方法&lt;/h4&gt;提出ATM-GAD，包含时间基元提取器（压缩交易历史为信息量最大的基元）、双重注意力块（内部注意力和跨基元注意力）以及可微自适应时间窗口学习器（为每个节点定制观察窗口）。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的实验表明，ATM-GAD始终优于七个强大的异常检测基线方法，能够发现早期方法未检测到的欺诈模式。&lt;h4&gt;结论&lt;/h4&gt;ATM-GAD通过有效利用时间基元和自适应时间窗口学习，显著提高了金融欺诈检测的性能，能够捕捉传统方法忽略的时间相关欺诈特征。&lt;h4&gt;翻译&lt;/h4&gt;金融欺诈检测对于保护数十亿美元至关重要，然而现代金融系统中相互关联的实体和快速变化的交易行为常常击败传统的机器学习模型。最近的基于图的检测器通过将交易表示为网络取得了进展，但它们仍然忽略了两个根植于时间的欺诈特征：(1)时间基元——揭示可疑资金流动的重复性、指示性子图；(2)账户特定的异常活动间隔，当欺诈仅以特定于每个实体的短暂爆发形式出现时。为了利用这两种信号，我们引入了ATM-GAD，这是一种利用时间基元进行金融异常检测的自适应图神经网络。时间基元提取器将每个账户的交易历史压缩为信息量最大的基元，同时保留拓扑和时间模式。这些基元随后由双重注意力块分析：内部注意力处理单个基元内的交互，而跨基元注意力跨基元聚合证据以揭示多步欺诈方案。同时，一个可微的自适应时间窗口学习器为每个节点定制观察窗口，使模型能够精确关注最具信息量的时间切片。在四个真实世界数据集上的实验表明，ATM-GAD始终优于七个强大的异常检测基线方法，发现了早期方法未发现的欺诈模式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial fraud detection is essential to safeguard billions of dollars, yetthe intertwined entities and fast-changing transaction behaviors in modernfinancial systems routinely defeat conventional machine learning models. Recentgraph-based detectors make headway by representing transactions as networks,but they still overlook two fraud hallmarks rooted in time: (1) temporalmotifs--recurring, telltale subgraphs that reveal suspicious money flows asthey unfold--and (2) account-specific intervals of anomalous activity, whenfraud surfaces only in short bursts unique to each entity. To exploit bothsignals, we introduce ATM-GAD, an adaptive graph neural network that leveragestemporal motifs for financial anomaly detection. A Temporal Motif Extractorcondenses each account's transaction history into the most informative motifs,preserving both topology and temporal patterns. These motifs are then analyzedby dual-attention blocks: IntraA reasons over interactions within a singlemotif, while InterA aggregates evidence across motifs to expose multi-stepfraud schemes. In parallel, a differentiable Adaptive Time-Window Learnertailors the observation window for every node, allowing the model to focusprecisely on the most revealing time slices. Experiments on four real-worlddatasets show that ATM-GAD consistently outperforms seven stronganomaly-detection baselines, uncovering fraud patterns missed by earliermethods.</description>
      <author>example@mail.com (Zeyue Zhang, Lin Song, Erkang Bao, Xiaoling Lv, Xinyue Wang)</author>
      <guid isPermaLink="false">2508.20829v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Local Virtual Nodes for Alleviating Over-Squashing in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.20597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了局部虚拟节点(LVN)方法，通过引入可训练嵌入的虚拟节点来缓解图神经网络中的过度压缩问题，同时保持原始图的全局结构不变。&lt;h4&gt;背景&lt;/h4&gt;过度压缩是训练图神经网络处理长程依赖关系时面临的挑战。GNN的感受野需要足够大以实现远程节点间的通信，但从广泛邻域收集信息并压缩到固定大小的节点表示中，会使消息传递容易受到瓶颈影响。现有的图重连和添加虚拟节点方法会改变输入图的全球拓扑结构，可能破坏原始图结构中编码的领域知识。&lt;h4&gt;目的&lt;/h4&gt;缓解过度压缩的影响，同时不显著破坏输入图的全局结构，并保留原始图结构中编码的领域知识。&lt;h4&gt;方法&lt;/h4&gt;提出局部虚拟节点(LVN)方法，具有可训练的嵌入。LVN的位置由节点中心性决定，指示潜在瓶颈的存在。在可能存在瓶颈的区域改善连通性，并在选定的中心区域之间共享可训练的LVN嵌入，促进远程节点间的通信而不需要添加更多层。&lt;h4&gt;主要发现&lt;/h4&gt;LVN可以增强结构连通性，显著提高图分类和节点分类任务的性能。代码可在https://github.com/ALLab-Boun/LVN/获取。&lt;h4&gt;结论&lt;/h4&gt;LVN方法有效解决了过度压缩问题，保持了原始图的全局结构，提高了图神经网络在处理长程依赖关系任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;过度压缩是训练图神经网络处理涉及长程依赖关系的任务时面临的挑战。在此类任务中，GNN的感受野应足够大，以实现远程节点之间的通信。然而，从广泛的邻域收集信息并将其内容压缩到固定大小的节点表示中，使得消息传递容易受到瓶颈的影响。图重连和添加虚拟节点是常用的补救措施，它们通过在瓶颈周围创建额外路径来缓解过度压缩。然而，这些技术会改变输入图的全球拓扑结构，并破坏原始图结构中编码的领域知识，这两者对特定任务和领域可能都很重要。本研究提出局部虚拟节点(LVN)，具有可训练的嵌入，以缓解过度压缩的影响，而不显著破坏输入图的全球结构。LVN的位置由节点中心性决定，该指标指示潜在瓶颈的存在。因此，所提出的方法旨在改善可能存在瓶颈的区域的连通性。此外，在选定的中心区域之间共享可训练的LVN嵌入，促进远程节点之间的通信，而无需添加更多层。在基准数据集上的广泛实验表明，LVN可以增强结构连通性，并显著提高图分类和节点分类任务的性能。代码可在https://github.com/ALLab-Boun/LVN/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Over-squashing is a challenge in training graph neural networks for tasksinvolving long-range dependencies. In such tasks, a GNN's receptive fieldshould be large enough to enable communication between distant nodes. However,gathering information from a wide range of neighborhoods and squashing itscontent into fixed-size node representations makes message-passing vulnerableto bottlenecks. Graph rewiring and adding virtual nodes are commonly studiedremedies that create additional pathways around bottlenecks to mitigateover-squashing. However, these techniques alter the input graph's globaltopology and disrupt the domain knowledge encoded in the original graphstructure, both of which could be essential to specific tasks and domains. Thisstudy presents Local Virtual Nodes (LVN) with trainable embeddings to alleviatethe effects of over-squashing without significantly corrupting the globalstructure of the input graph. The position of the LVNs is determined by thenode centrality, which indicates the existence of potential bottlenecks. Thus,the proposed approach aims to improve the connectivity in the regions withlikely bottlenecks. Furthermore, trainable LVN embeddings shared acrossselected central regions facilitate communication between distant nodes withoutadding more layers. Extensive experiments on benchmark datasets demonstratethat LVNs can enhance structural connectivity and significantly improveperformance on graph and node classification tasks. The code can be found athttps://github.com/ALLab-Boun/LVN/}{https://github.com/ALLab-Boun/LVN/.</description>
      <author>example@mail.com (Tuğrul Hasan Karabulut, İnci M. Baytaş)</author>
      <guid isPermaLink="false">2508.20597v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models</title>
      <link>http://arxiv.org/abs/2508.20583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图语言模型(GLMs)旨在结合图神经网络的结构推理能力与大语言模型的语义理解能力，但当前评估基准不足以评估多模态推理能力。作者提出了CLEGR基准来评估不同复杂度的多模态推理，发现软提示的LLM基线与包含完整GNN骨干的GLM性能相当，质疑了将图结构整合到LLMs中的必要性，并指出GLM在需要结构推理的任务中性能显著下降。&lt;h4&gt;背景&lt;/h4&gt;图语言模型(GLMs)的发展旨在将图神经网络(GNNs)的结构推理能力与大语言模型(LLMs)的语义理解能力相结合。&lt;h4&gt;目的&lt;/h4&gt;解决当前GLM评估基准的不足，提出能够有效评估多模态推理能力的新基准。&lt;h4&gt;方法&lt;/h4&gt;引入CLEGR(组合语言-图推理)基准，采用合成图生成流程，并配需要同时对结构和文本语义进行联合推理的问题。&lt;h4&gt;主要发现&lt;/h4&gt;1) 当前GLM评估基准主要依赖节点级分类数据集，不足以评估多模态推理；2) 仅使用单模态信息就能在现有基准上取得良好性能；3) 软提示的LLM基线与包含完整GNN骨干的GLM性能相当，质疑了图结构整合的必要性；4) GLM在需要结构推理的任务中表现出显著的性能下降。&lt;h4&gt;结论&lt;/h4&gt;当前GLM的图推理能力存在局限性，CLEGR基准为社区推进涉及图结构和语言的明确多模态推理提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;图语言模型(GLMs)的发展旨在整合图神经网络(GNNs)的结构推理能力与大语言模型(LLMs)的语义理解能力。然而，我们证明当前的GLM评估基准主要是重新利用的节点级分类数据集，不足以评估多模态推理。我们的分析表明，仅使用单模态信息就能在这些基准上取得强大性能，这表明它们不需要图语言集成。为了解决这一评估差距，我们引入了CLEGR(组合语言-图推理)基准，旨在评估不同复杂度级别的多模态推理。我们的基准采用合成图生成流程，并配需要同时对结构和文本语义进行联合推理的问题。我们对代表性的GLM架构进行了彻底评估，发现软提示的LLM基线与包含完整GNN骨干的GLM性能相当。这一结果质疑了将图结构整合到LLMs中的架构必要性。我们进一步表明，GLM在需要结构推理的任务中表现出显著的性能下降。这些发现突显了当前GLM图推理能力的局限性，并为社区推进涉及图结构和语言的明确多模态推理提供了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developments in Graph-Language Models (GLMs) aim to integrate the structuralreasoning capabilities of Graph Neural Networks (GNNs) with the semanticunderstanding of Large Language Models (LLMs). However, we demonstrate thatcurrent evaluation benchmarks for GLMs, which are primarily repurposednode-level classification datasets, are insufficient to assess multimodalreasoning. Our analysis reveals that strong performance on these benchmarks isachievable using unimodal information alone, suggesting that they do notnecessitate graph-language integration. To address this evaluation gap, weintroduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designedto evaluate multimodal reasoning at various complexity levels. Our benchmarkemploys a synthetic graph generation pipeline paired with questions thatrequire joint reasoning over structure and textual semantics. We perform athorough evaluation of representative GLM architectures and find thatsoft-prompted LLM baselines perform on par with GLMs that incorporate a fullGNN backbone. This result calls into question the architectural necessity ofincorporating graph structure into LLMs. We further show that GLMs exhibitsignificant performance degradation in tasks that require structural reasoning.These findings highlight limitations in the graph reasoning capabilities ofcurrent GLMs and provide a foundation for advancing the community towardexplicit multimodal reasoning involving graph structure and language.</description>
      <author>example@mail.com (Soham Petkar, Hari Aakash K, Anirudh Vempati, Akshit Sinha, Ponnurangam Kumarauguru, Chirag Agarwal)</author>
      <guid isPermaLink="false">2508.20583v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>GLaRE: A Graph-based Landmark Region Embedding Network for Emotion Recognition</title>
      <link>http://arxiv.org/abs/2508.20579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GLaRE的新型基于图的关键点区域嵌入网络，用于面部表情识别。该方法使用3D面部对齐提取关键点，并通过分层粗化构建商图来保留空间结构同时降低复杂性。实验表明，该方法在AffectNet和FERG数据集上分别达到64.89%和94.24%的准确率，优于多个基线方法。消融研究表明，商图区域级别的嵌入有助于提高预测性能。&lt;h4&gt;背景&lt;/h4&gt;面部表情识别(FER)是计算机视觉中的关键任务，在人机交互、监控和辅助技术等领域有广泛应用。然而，遮挡、表达变化性和缺乏可解释性等问题限制了传统FER系统的性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效建模面部关键点之间关系依赖的结构化和可解释的学习方法，以提高面部表情识别的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出GLaRE，一种基于图的关键点区域嵌入网络。该方法使用3D面部对齐提取面部关键点，通过分层粗化构建商图以保留空间结构同时降低复杂性，实现结构化和可解释的学习。&lt;h4&gt;主要发现&lt;/h4&gt;GLaRE在AffectNet上达到64.89%的准确率，在FERG上达到94.24%的准确率，优于多个现有基线方法。消融研究表明，来自商图区域级别的嵌入对提高预测性能有贡献。&lt;h4&gt;结论&lt;/h4&gt;图神经网络为面部表情识别提供了强大替代方案，通过建模面部关键点之间的关系依赖，实现了结构化和可解释的学习。GLaRE方法有效解决了传统FER系统面临的遮挡、表达变化性和缺乏可解释性等问题。&lt;h4&gt;翻译&lt;/h4&gt;面部表情识别(FER)是计算机视觉中的一个关键任务，在人机交互、监控和辅助技术等领域有广泛应用。然而，遮挡、表达变化性和缺乏可解释性等挑战阻碍了传统FER系统的性能。图神经网络(GNN)通过建模面部关键点之间的关系依赖，提供了一种强大的替代方案，实现了结构化和可解释的学习。在本文中，我们提出了GLaRE，一种用于情感识别的新型基于图的关键点区域嵌入网络。使用3D面部对齐提取面部关键点，并通过分层粗化构建商图以保留空间结构同时降低复杂性。我们的方法在AffectNet上达到64.89%的准确率，在FERG上达到94.24%的准确率，优于多个现有基线方法。此外，消融研究已经证明，来自商图的区域级嵌入有助于提高预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Facial expression recognition (FER) is a crucial task in computer vision withwide range of applications including human computer interaction, surveillance,and assistive technologies. However, challenges such as occlusion, expressionvariability, and lack of interpretability hinder the performance of traditionalFER systems. Graph Neural Networks (GNNs) offer a powerful alternative bymodeling relational dependencies between facial landmarks, enabling structuredand interpretable learning. In this paper, we propose GLaRE, a novelGraph-based Landmark Region Embedding network for emotion recognition. Faciallandmarks are extracted using 3D facial alignment, and a quotient graph isconstructed via hierarchical coarsening to preserve spatial structure whilereducing complexity. Our method achieves 64.89 percentage accuracy on AffectNetand 94.24 percentage on FERG, outperforming several existing baselines.Additionally, ablation studies have demonstrated that region-level embeddingsfrom quotient graphs have contributed to improved prediction performance.</description>
      <author>example@mail.com (Debasis Maji, Debaditya Barman)</author>
      <guid isPermaLink="false">2508.20579v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Molecular Machine Learning in Chemical Process Design</title>
      <link>http://arxiv.org/abs/2508.20527v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于分子机器学习在化学过程工程领域应用的综述性文章，探讨了其在预测物性和发现新分子结构方面的潜力，以及未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;分子机器学习在化学过程工程领域展现出巨大潜力，能够提供高精度预测和探索化学空间。&lt;h4&gt;目的&lt;/h4&gt;回顾当前最先进的分子机器学习模型，讨论进一步发展的研究方向，并探索在化学过程规模上应用分子机器学习的方法。&lt;h4&gt;方法&lt;/h4&gt;分析图神经网络和transformers等机器学习方法，探讨如何通过混合或物理信息化的方式整合物理化学知识，以及如何将分子机器学习整合到过程设计和优化公式中。&lt;h4&gt;主要发现&lt;/h4&gt;分子机器学习能够为纯组分及其混合物的性质提供高度准确的预测，并能探索化学空间寻找新的分子结构；在化学过程规模上应用分子机器学习是有价值但尚未充分探索的领域。&lt;h4&gt;结论&lt;/h4&gt;将分子机器学习整合到过程设计和优化中，有望加速新型分子和过程的识别；创建分子和过程设计基准并验证候选分子至关重要，可能需要与化学工业合作。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了关于分子机器学习在化学过程工程领域的观点。最近，分子机器学习在(i)为纯组分及其混合物的性质提供高度准确的预测，和(ii)探索化学空间以寻找新的分子结构方面展示了巨大潜力。我们回顾了当前最先进的分子机器学习模型，并讨论了有希望进一步发展的研究方向。这包括机器学习方法，如图神经网络和transformers，可以通过混合或物理信息化的方式整合物理化学知识来进一步发展。然后，我们考虑在化学过程规模上利用分子机器学习，这是非常理想但尚未充分探索的。我们讨论了如何将分子机器学习整合到过程设计和优化公式中，有望加速新型分子和过程的识别。为此，创建分子和过程设计基准并在实践中验证提出的候选分子将是至关重要的，可能需要与化学工业合作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a perspective on molecular machine learning (ML) in the field ofchemical process engineering. Recently, molecular ML has demonstrated greatpotential in (i) providing highly accurate predictions for properties of purecomponents and their mixtures, and (ii) exploring the chemical space for newmolecular structures. We review current state-of-the-art molecular ML modelsand discuss research directions that promise further advancements. Thisincludes ML methods, such as graph neural networks and transformers, which canbe further advanced through the incorporation of physicochemical knowledge in ahybrid or physics-informed fashion. Then, we consider leveraging molecular MLat the chemical process scale, which is highly desirable yet rather unexplored.We discuss how molecular ML can be integrated into process design andoptimization formulations, promising to accelerate the identification of novelmolecules and processes. To this end, it will be essential to create moleculeand process design benchmarks and practically validate proposed candidates,possibly in collaboration with the chemical industry.</description>
      <author>example@mail.com (Jan G. Rittig, Manuel Dahmen, Martin Grohe, Philippe Schwaller, Alexander Mitsos)</author>
      <guid isPermaLink="false">2508.20527v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Structure-aware Hypergraph Transformer for Diagnosis Prediction in Electronic Health Records</title>
      <link>http://arxiv.org/abs/2508.20500v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为结构感知超图Transformer(SHGT)的新型框架，用于电子健康记录(EHR)中的诊断预测任务。该框架通过超图结构编码器和Transformer架构的结合，有效解决了现有图神经网络方法无法捕捉高阶依赖性和表示能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录(EHR)通过标准化医疗代码系统化组织患者健康数据，是预测建模的宝贵资源。图神经网络(GNNs)虽已证明在EHR内医疗代码间交互建模方面有效，但现有方法存在局限性。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNN方法在处理EHR数据时的两个主要不足：无法捕捉临床数据中固有的高阶依赖性，以及局部化消息传递方案限制了表示能力。&lt;h4&gt;方法&lt;/h4&gt;提出结构感知超图Transformer(SHGT)框架，包含三个核心创新：a)采用超图结构编码器捕捉医疗代码间的高阶交互；b)集成Transformer架构推理整个超图；c)设计包含超图重建的定制损失函数以保留原始超图结构。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界EHR数据集上的实验表明，所提出的SHGT在诊断预测任务上性能优于现有的最先进模型。&lt;h4&gt;结论&lt;/h4&gt;SHGT框架通过结合超图结构和Transformer架构，有效解决了现有GNN方法的局限性，提高了EHR数据中预测建模的准确性。&lt;h4&gt;翻译&lt;/h4&gt;电子健康记录(EHR)通过标准化医疗代码系统化组织患者健康数据，作为预测建模的全面且宝贵的资源来源。图神经网络(GNNs)已在EHR内医疗代码之间交互建模方面展现出有效性。然而，现有基于GNN的方法存在不足：a)它们依赖成对关系，无法捕捉临床数据中固有的高阶依赖性；b)局部化消息传递方案限制了表示能力。为解决这些问题，本文提出了一种新颖的结构感知超图Transformer(SHGT)框架，遵循三方面理念：a)采用超图结构编码器捕捉医疗代码间的高阶交互；b)集成Transformer架构推理整个超图；c)设计包含超图重建的定制损失函数以保留超图的原始结构。在真实世界EHR数据集上的实验表明，所提出的SHGT在诊断预测上优于现有最先进模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHR) systematically organize patient health datathrough standardized medical codes, serving as a comprehensive and invaluablesource for predictive modeling. Graph neural networks (GNNs) have demonstratedeffectiveness in modeling interactions between medical codes within EHR.However, existing GNN-based methods are inadequate due to: a) their reliance onpairwise relations fails to capture the inherent higher-order dependencies inclinical data, and b) the localized message-passing scheme limitsrepresentation power. To address these issues, this paper proposes a novelStructure-aware HyperGraph Transformer (SHGT) framework following three-foldideas: a) employing a hypergraph structural encoder to capture higher-orderinteractions among medical codes, b) integrating the Transformer architectureto reason over the entire hypergraph, and c) designing a tailored loss functionincorporating hypergraph reconstruction to preserve the hypergraph's originalstructure. Experiments on real-world EHR datasets demonstrate that the proposedSHGT outperforms existing state-of-the-art models on diagnosis prediction.</description>
      <author>example@mail.com (Haiyan Wang, Ye Yuan)</author>
      <guid isPermaLink="false">2508.20500v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes</title>
      <link>http://arxiv.org/abs/2508.19356v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 to 4 hours read time. 73 pages. 35 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇介绍性文章探讨了图数据建模在化学科学中的应用，特别是在分子、蛋白质和化学过程的表示与学习方面。&lt;h4&gt;背景&lt;/h4&gt;图在化学科学中处于核心地位，提供了描述分子、蛋白质、反应和工业过程的一种自然语言，能够捕获支撑材料、生物学和医学的相互作用和结构。&lt;h4&gt;目的&lt;/h4&gt;介绍图作为化学中的数学对象，展示学习算法（特别是图神经网络）如何在这些图上操作，概述图设计基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图建模中的作用。&lt;h4&gt;方法&lt;/h4&gt;采用图数据建模方法，应用图神经网络等学习算法，构建图的基础结构来表示化学实体和过程。&lt;h4&gt;主要发现&lt;/h4&gt;图是化学科学中描述分子、蛋白质等的自然语言；图可以捕获支撑材料、生物学和医学的相互作用和结构；学习算法（特别是图神经网络）可以在图上有效操作。&lt;h4&gt;结论&lt;/h4&gt;这些概念共同为读者应用图方法到下一代化学发现做好了准备。&lt;h4&gt;翻译&lt;/h4&gt;图是化学科学的核心，为描述分子、蛋白质、反应和工业过程提供了自然语言。它们捕获了支撑材料、生物学和医学的相互作用和结构。本介绍性文章《图数据建模：分子、蛋白质与化学过程》将图作为化学中的数学对象进行介绍，并展示了学习算法（特别是图神经网络）如何在这些图上操作。我们概述了图设计的基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图建模中的作用。这些概念共同为读者应用图方法到下一代化学发现做好了准备。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acsinfocus.7e9017&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are central to the chemical sciences, providing a natural language todescribe molecules, proteins, reactions, and industrial processes. They captureinteractions and structures that underpin materials, biology, and medicine.This primer, Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes,introduces graphs as mathematical objects in chemistry and shows how learningalgorithms (particularly graph neural networks) can operate on them. We outlinethe foundations of graph design, key prediction tasks, representative examplesacross chemical sciences, and the role of machine learning in graph-basedmodeling. Together, these concepts prepare readers to apply graph methods tothe next generation of chemical discovery.</description>
      <author>example@mail.com (José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Adrian Jinich, Radhakrishnan Mahadevan, Benjamin Sanchez-Lengeling)</author>
      <guid isPermaLink="false">2508.19356v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19071v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了TRIGON，一个新颖的图神经网络框架，通过学习从多个图视图中选择相关三角形来构建丰富的非平面三角剖分，以缓解图神经网络中的过度压缩和过度平滑问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的主要范式，但其性能受到图拓扑问题(特别是过度压缩和过度平滑)的限制。&lt;h4&gt;目的&lt;/h4&gt;引入TRIGON框架，通过修改图拓扑来促进更有效的信息传播，从而缓解GNNs的性能限制。&lt;h4&gt;方法&lt;/h4&gt;TRIGON通过从多个图视图中学习选择相关三角形来构建丰富的非平面三角剖分，并联合优化三角形选择和下游分类性能，产生重新布线的图。&lt;h4&gt;主要发现&lt;/h4&gt;与现有重新布线方法相比，TRIGON产生的图具有显著改进的结构特性，如减少直径、增加谱间隙和降低有效电阻。&lt;h4&gt;结论&lt;/h4&gt;实证结果表明，TRIGON在同质和异质基准测试中的节点分类任务上优于最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的主要范式。然而，它们的性能受到图拓扑固有问题的限制，最明显的是过度压缩和过度平滑。最近在图重新布线方面的进展旨在通过修改图拓扑来促进更有效的信息传播，从而缓解这些限制。在这项工作中，我们介绍了TRIGON，一个新颖的框架，它通过从多个图视图中学习选择相关三角形来构建丰富的非平面三角剖分。通过联合优化三角形选择和下游分类性能，我们的方法产生的重新布线图具有显著改进的结构特性，如减少直径、增加谱间隙和降低有效电阻，与现有的重新布线方法相比。实证结果表明，TRIGON在同质和异质基准测试中的节点分类任务上优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as the leading paradigm forlearning over graph-structured data. However, their performance is limited byissues inherent to graph topology, most notably oversquashing andoversmoothing. Recent advances in graph rewiring aim to mitigate theselimitations by modifying the graph topology to promote more effectiveinformation propagation. In this work, we introduce TRIGON, a novel frameworkthat constructs enriched, non-planar triangulations by learning to selectrelevant triangles from multiple graph views. By jointly optimizing triangleselection and downstream classification performance, our method produces arewired graph with markedly improved structural properties such as reduceddiameter, increased spectral gap, and lower effective resistance compared toexisting rewiring methods. Empirical results demonstrate that TRIGONoutperforms state-of-the-art approaches on node classification tasks across arange of homophilic and heterophilic benchmarks.</description>
      <author>example@mail.com (Hugo Attali, Thomas Papastergiou, Nathalie Pernelle, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2508.19071v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title>
      <link>http://arxiv.org/abs/2508.17630v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了量子图注意力网络(QGAT)，一种将变分量子电路集成到注意力机制中的混合图神经网络，利用量子并行性减少计算复杂度，提高模型表达能力。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络在处理复杂结构数据时面临计算效率和表达能力的挑战，量子计算为解决这些问题提供了新思路。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合量子计算优势的新型图神经网络架构，提高模型表达能力，同时减少计算复杂度，增强对噪声数据的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;QGAT采用强纠缠量子电路和幅度编码的节点特征实现非线性交互；使用单一量子电路同时生成多个注意力系数，而非传统多头注意力分别计算；通过量子并行性实现参数共享；联合优化经典投影权重和量子电路参数；采用模块化设计便于与现有架构集成。&lt;h4&gt;主要发现&lt;/h4&gt;QGAT能有效捕获复杂结构依赖关系；在归纳场景中具有更好的泛化能力；量子嵌入增强了特征和结构噪声的鲁棒性；在化学、生物学和网络分析等多个领域具有可扩展的量子增强学习潜力。&lt;h4&gt;结论&lt;/h4&gt;量子图注意力网络通过结合量子计算和图神经网络优势，提供了一种更高效、更鲁棒的模型架构，能够处理复杂结构化数据并在真实世界噪声数据中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了量子图注意力网络(QGAT)，一种将变分量子电路集成到注意力机制中的混合图神经网络。其核心是QGAT采用强纠缠量子电路和幅度编码的节点特征来实现富有表现力的非线性交互。与分别计算每个头的经典多头注意力不同，QGAT利用单一量子电路同时生成多个注意力系数。这种量子并行性促进了头之间的参数共享，显著减少了计算开销和模型复杂度。经典投影权重和量子电路参数以端到端方式联合优化，确保对学习任务的灵活适应。实证结果表明，QGAT在捕获复杂结构依赖关系方面有效，并在归纳场景中提高了泛化能力，突显了其在化学、生物学和网络分析等领域可扩展量子增强学习的潜力。此外，实验证实量子嵌入增强了特征和结构噪声的鲁棒性，表明在处理真实世界噪声数据方面具有优势。QGAT的模块化还确保了与现有架构的简便集成，使其能够轻松增强基于经典注意力的模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description>
      <author>example@mail.com (An Ning, Tai Yue Li, Nan Yow Chen)</author>
      <guid isPermaLink="false">2508.17630v3</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Graph-R1: Incentivizing the Zero-Shot Graph Learning Capability in LLMs via Explicit Reasoning</title>
      <link>http://arxiv.org/abs/2508.17387v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为Graph-R1的新方法，将图学习任务转化为文本推理问题，利用大型推理模型(LRMs)在没有任务特定监督的情况下解决节点分类、链接预测和图分类等任务，并通过强化学习框架优化推理过程。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)受限于固定的标签空间，而大型语言模型(LLMs)缺乏结构归纳偏置，使得在没有任务特定监督的情况下推广到未见过的图任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需GNN的方法，将图任务重新表述为文本推理问题，利用大型推理模型(LRMs)的零样本推理能力来解决图学习任务。&lt;h4&gt;方法&lt;/h4&gt;开发了Graph-R1，一个利用特定任务的重新思考模板来引导线性化图推理的强化学习框架；为节点分类、链接预测和图分类任务创建了具有详细推理轨迹的首批数据集。&lt;h4&gt;主要发现&lt;/h4&gt;Graph-R1在零样本设置下优于最先进的基线，产生可解释且有效的预测；显式推理在图学习方面具有潜力。&lt;h4&gt;结论&lt;/h4&gt;该工作强调了显式推理在图学习方面的潜力，并为未来研究提供了新的数据集和框架资源。&lt;h4&gt;翻译&lt;/h4&gt;在没有任务特定监督的情况下推广到未见过的图任务仍然具有挑战性。图神经网络(GNNs)受限于固定的标签空间，而大型语言模型(LLMs)缺乏结构归纳偏置。大型推理模型(LRMs)的最新进展通过明确的长链式推理提供了零样本替代方案。受此启发，我们提出了一种无需GNN的方法，将图任务（节点分类、链接预测和图分类）重新表述为由LRMs解决的文本推理问题。我们为这些任务引入了具有详细推理轨迹的首批数据集，并开发了Graph-R1，这是一个利用特定任务的重新思考模板来引导线性化图推理的强化学习框架。实验表明，Graph-R1在零样本设置下优于最先进的基线，产生可解释且有效的预测。这项工作强调了显式推理在图学习方面的潜力，并为未来研究提供了新资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalizing to unseen graph tasks without task-pecific supervision remainschallenging. Graph Neural Networks (GNNs) are limited by fixed label spaces,while Large Language Models (LLMs) lack structural inductive biases. Recentadvances in Large Reasoning Models (LRMs) provide a zero-shot alternative viaexplicit, long chain-of-thought reasoning. Inspired by this, we propose aGNN-free approach that reformulates graph tasks--node classification, linkprediction, and graph classification--as textual reasoning problems solved byLRMs. We introduce the first datasets with detailed reasoning traces for thesetasks and develop Graph-R1, a reinforcement learning framework that leveragestask-specific rethink templates to guide reasoning over linearized graphs.Experiments demonstrate that Graph-R1 outperforms state-of-the-art baselines inzero-shot settings, producing interpretable and effective predictions. Our workhighlights the promise of explicit reasoning for graph learning and providesnew resources for future research.</description>
      <author>example@mail.com (Yicong Wu, Guangyue Lu, Yuan Zuo, Huarong Zhang, Junjie Wu)</author>
      <guid isPermaLink="false">2508.17387v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Dual Uncertainty Optimization: Boosting Monocular 3D Object Detection under Test-Time Shifts</title>
      <link>http://arxiv.org/abs/2508.20488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICCV 2025 (Highlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为双重不确定性优化(DUO)的测试时适应框架，用于解决单目3D物体检测在真实世界域偏移下的可靠性问题。DUO通过同时最小化语义不确定性和几何不确定性，提高了模型在环境或传感器变化情况下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;单目3D物体检测(M3OD)对自动驾驶等安全关键应用至关重要，但在环境或传感器变化引起的真实世界域偏移下，其可靠性显著下降。测试时适应(TTA)方法虽已出现，但未能解决M3OD固有的双重不确定性问题。&lt;h4&gt;目的&lt;/h4&gt;提出双重不确定性优化(DUO)，首个联合最小化语义不确定性和几何不确定性的TTA框架，用于实现鲁棒的单目3D物体检测。&lt;h4&gt;方法&lt;/h4&gt;通过凸优化视角引入focal loss的创新凸结构并推导无监督版本，实现无标签的不确定性权重和高不确定性物体的平衡学习；设计语义感知的法线场约束，在具有清晰语义线索的区域保持几何一致性，减少不稳定3D表示的不确定性；形成双分支互补循环机制。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，在各种数据集和域偏移类型上，DUO优于现有方法。增强的空间感知改善语义分类，鲁棒的语义预测进一步优化空间理解。&lt;h4&gt;结论&lt;/h4&gt;DUO通过同时解决语义不确定性和几何不确定性，有效提高了单目3D物体检测在域偏移情况下的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;精确的单目3D物体检测(M3OD)对自动驾驶等安全关键应用至关重要，但在环境或传感器变化引起的真实世界域偏移下，其可靠性显著下降。为解决这些偏移，测试时适应(TTA)方法应运而生，使模型能够在推理过程中适应目标分布。虽然先前的TTA方法认识到低不确定性与高泛化能力之间的正相关关系，但它们未能解决M3OD固有的双重不确定性：语义不确定性（模糊的类别预测）和几何不确定性（不稳定的空间定位）。为弥合这一差距，我们提出双重不确定性优化(DUO)，首个旨在联合最小化两种不确定性的TTA框架，用于鲁棒的单目3D物体检测。通过凸优化视角，我们引入了focal loss的创新凸结构，并进一步推导出一种新颖的无监督版本，实现无标签的不确定性权重和高不确定性物体的平衡学习。同时，我们设计了语义感知的法线场约束，在具有清晰语义线索的区域保持几何一致性，减少来自不稳定3D表示的不确定性。这种双分支机制形成了互补循环：增强的空间感知改善语义分类，而鲁棒的语义预测进一步优化空间理解。大量实验证明，在各种数据集和域偏移类型上，DUO优于现有方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测(M3OD)在测试时域偏移下的可靠性问题。当面对真实世界中的环境变化(如恶劣天气)或传感器故障时，训练好的M3OD模型性能会严重下降。这个问题在自动驾驶等安全关键应用中尤为重要，因为这些应用需要高可靠性的3D感知能力，而现有的测试时适应方法未能有效处理M3OD中固有的双重不确定性：语义不确定性和几何不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过实证研究发现M3OD在域偏移下存在语义和几何两种不确定性，且现有方法存在两个主要局限：低分目标忽视和空间感知崩溃。基于凸优化理论，作者重新构建了focal loss的凸结构，并推导出无监督版本；同时设计了语义感知的法线场约束来增强几何一致性。作者借鉴了凸优化理论中的Legendre-Fenchel结构、focal loss的思想以及法线场约束在计算机视觉中的应用，但针对M3OD的特殊性进行了创新性改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出双重不确定性优化(DUO)框架，同时最小化语义和几何不确定性，形成互补循环。实现流程包括：1)语义不确定性优化：通过凸优化理论推导共轭focal loss，实现标签无关的动态加权，平衡高低分目标的学习；2)几何不确定性优化：使用Sobel算子将深度图转换为法线场，设计法线一致性约束，并通过语义指导的掩码只在低语义不确定性区域应用几何约束；3)整体优化：将两部分整合到统一框架中，实现语义和几何优化的互补反馈循环。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出针对M3OD的双重不确定性优化框架；2)通过凸优化理论推导出共轭focal loss，实现标签无关的不确定性权重平衡；3)设计语义感知的法线场约束，保持几何一致性；4)发现并验证了双分支设计的互补循环。相比之前工作，DUO同时处理语义和几何双重不确定性，而现有方法主要关注单一不确定性；共轭focal loss无需标签即可实现动态加权，而传统focal loss依赖标签；DUO避免了直接最小化深度不确定性导致的模型崩溃问题，专门针对M3OD的特有挑战进行了设计。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了双重不确定性优化(DUO)框架，通过同时优化语义和几何不确定性，显著提升了单目3D目标检测模型在测试时域偏移下的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate monocular 3D object detection (M3OD) is pivotal for safety-criticalapplications like autonomous driving, yet its reliability deterioratessignificantly under real-world domain shifts caused by environmental or sensorvariations. To address these shifts, Test-Time Adaptation (TTA) methods haveemerged, enabling models to adapt to target distributions during inference.While prior TTA approaches recognize the positive correlation between lowuncertainty and high generalization ability, they fail to address the dualuncertainty inherent to M3OD: semantic uncertainty (ambiguous classpredictions) and geometric uncertainty (unstable spatial localization). Tobridge this gap, we propose Dual Uncertainty Optimization (DUO), the first TTAframework designed to jointly minimize both uncertainties for robust M3OD.Through a convex optimization lens, we introduce an innovative convex structureof the focal loss and further derive a novel unsupervised version, enablinglabel-agnostic uncertainty weighting and balanced learning for high-uncertaintyobjects. In parallel, we design a semantic-aware normal field constraint thatpreserves geometric coherence in regions with clear semantic cues, reducinguncertainty from the unstable 3D representation. This dual-branch mechanismforms a complementary loop: enhanced spatial perception improves semanticclassification, and robust semantic predictions further refine spatialunderstanding. Extensive experiments demonstrate the superiority of DUO overexisting methods across various datasets and domain shift types.</description>
      <author>example@mail.com (Zixuan Hu, Dongxiao Li, Xinzhu Ma, Shixiang Tang, Xiaotong Li, Wenhan Yang, Ling-Yu Duan)</author>
      <guid isPermaLink="false">2508.20488v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Multi-View 3D Point Tracking</title>
      <link>http://arxiv.org/abs/2508.21060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025, Oral. Project page: https://ethz-vlg.github.io/mvtracker&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种基于数据驱动的多视角三维点跟踪器，能够利用少量摄像头（如四个）实现动态场景中任意点的鲁棒和准确跟踪，解决了现有单目跟踪器的深度模糊和遮挡问题，以及传统多摄像头方法需要大量摄像头和繁琐优化的限制。&lt;h4&gt;背景&lt;/h4&gt;现有的单目跟踪器在处理深度模糊和遮挡问题时存在困难，而之前的多摄像头方法需要超过20个摄像头和繁琐的每序列优化，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种实用的多视角三维点跟踪器，能够在实际数量的摄像头情况下实现鲁棒和准确的在线跟踪，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;该前馈模型利用已知的摄像头姿态和多视角深度信息，将多视角特征融合为统一点云，并应用k近邻相关性和基于transformer的更新方法，即使在遮挡情况下也能可靠估计长程三维对应关系。模型在5K合成的多视角Kubric序列上训练，并在Panoptic Studio和DexYCB基准上评估。&lt;h4&gt;主要发现&lt;/h4&gt;在Panoptic Studio和DexYCB两个真实世界基准测试中，分别实现了3.1厘米和2.0厘米的中值轨迹误差。该方法能够很好地适应1-8个不同视角的多样化摄像头设置，以及24-150帧的不同视频长度。&lt;h4&gt;结论&lt;/h4&gt;通过发布跟踪器以及训练和评估数据集，该研究旨在为多视角三维跟踪研究设定新标准，并为实际应用提供实用工具。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了首个数据驱动的多视角三维点跟踪器，旨在使用多个摄像头视角跟踪动态场景中的任意点。与现有的单目跟踪器（难以处理深度模糊和遮挡问题）或之前的多摄像头方法（需要超过20个摄像头和繁琐的每序列优化）不同，我们的前馈模型使用实际数量的摄像头直接预测三维对应关系，实现了鲁棒且准确的在线跟踪。在已知摄像头姿态和多视角深度信息下，我们的跟踪器将多视角特征融合为统一点云，并应用k近邻相关性和基于transformer的更新方法，即使在遮挡情况下也能可靠估计长程三维对应关系。我们在5K合成的多视角Kubric序列上训练模型，并在两个真实世界基准上评估，分别实现了3.1厘米和2.0厘米的中值轨迹误差。我们的方法能够很好地适应1-8个不同视角的多样化摄像头设置，以及24-150帧的不同视频长度。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点跟踪问题，特别是在动态场景中跟踪任意3D点。这个问题在计算机视觉中非常重要，因为它有众多应用，包括动态场景重建、机器人导航和增强现实。现有的单目跟踪器存在深度歧义和遮挡问题，而之前的多摄像头方法需要超过20个摄像头和繁琐的序列级优化，限制了实际应用场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了现有的2D点跟踪方法（如CoTracker、LocoTrack）和3D点跟踪方法（如SpatialTracker、DELTA），但针对多视图场景进行了改进。他们注意到单目方法的局限性，以及多视图方法中需要大量摄像头的问题。因此，他们设计了一个融合多视图特征到统一3D点云的方法，使用kNN相关性和transformer进行更新，这种方法既保留了多视图的优势，又减少了所需摄像头的数量，实现了更实用的解决方案。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将多视图特征融合到一个统一的3D特征点云中，在这个点云中使用kNN相关性来捕获跨视图的时空关系，然后使用transformer迭代细化点轨迹。整体实现流程包括：1) 输入同步RGB帧、相机参数和深度图；2) 使用CNN提取每个视图的特征图；3) 将深度图转换为3D点云并与特征关联；4) 计算k近邻相关性；5) 使用transformer迭代更新点位置和特征；6) 处理长视频时使用滑动窗口进行推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个数据驱动的多视图3D点跟踪器；2) 使用融合的3D特征点云而非传统的2D网格或三平面表示；3) kNN相关性和transformer更新；4) 能够处理遮挡和适应不同的相机设置；5) 只需少量摄像头（如四个）就能实现高效鲁棒的跟踪。相比之前的工作，MVTracker不需要序列级优化，运行速度快（7.2 FPS），且对深度估计的噪声具有更好的鲁棒性，显著提升了在动态场景中的跟踪性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了MVTracker，首个数据驱动的多视图3D点跟踪器，通过融合多视图特征到统一的3D点云并使用kNN相关性和transformer更新，实现了仅需少量摄像头的实时、鲁棒的3D点跟踪，显著提升了在动态场景中的跟踪性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the first data-driven multi-view 3D point tracker, designed totrack arbitrary points in dynamic scenes using multiple camera views. Unlikeexisting monocular trackers, which struggle with depth ambiguities andocclusion, or prior multi-camera methods that require over 20 cameras andtedious per-sequence optimization, our feed-forward model directly predicts 3Dcorrespondences using a practical number of cameras (e.g., four), enablingrobust and accurate online tracking. Given known camera poses and eithersensor-based or estimated multi-view depth, our tracker fuses multi-viewfeatures into a unified point cloud and applies k-nearest-neighbors correlationalongside a transformer-based update to reliably estimate long-range 3Dcorrespondences, even under occlusion. We train on 5K synthetic multi-viewKubric sequences and evaluate on two real-world benchmarks: Panoptic Studio andDexYCB, achieving median trajectory errors of 3.1 cm and 2.0 cm, respectively.Our method generalizes well to diverse camera setups of 1-8 views with varyingvantage points and video lengths of 24-150 frames. By releasing our trackeralongside training and evaluation datasets, we aim to set a new standard formulti-view 3D tracking research and provide a practical tool for real-worldapplications. Project page available at https://ethz-vlg.github.io/mvtracker.</description>
      <author>example@mail.com (Frano Rajič, Haofei Xu, Marko Mihajlovic, Siyuan Li, Irem Demir, Emircan Gündoğdu, Lei Ke, Sergey Prokudin, Marc Pollefeys, Siyu Tang)</author>
      <guid isPermaLink="false">2508.21060v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>PointDGRWKV: Generalizing RWKV-like Architecture to Unseen Domains for Point Cloud Classification</title>
      <link>http://arxiv.org/abs/2508.20835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次探索了RWKV模型在点云分类领域泛化问题中的应用，提出了PointDGRWKV框架，解决了RWKV直接应用于非结构化点云时的空间失真和跨域注意力漂移问题，实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;领域泛化(DG)被用于增强点云分类(PCC)模型在未见域上的泛化能力。现有基于卷积网络、Transformer或Mamba架构的方法存在感受野有限、计算成本高或长距离依赖建模不足的问题。&lt;h4&gt;目的&lt;/h4&gt;研究RWKV模型在DG PCC中的泛化能力，解决直接应用RWKV时面临的空间失真和跨域注意力漂移问题，提高点云分类模型在跨域场景下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出PointDGRWKV框架，包含两个关键模块：自适应几何Token移位(用于建模局部邻域结构，提高几何上下文感知)和跨域键特征分布对齐(通过对齐跨域键特征分布来减轻注意力漂移)，同时保持RWKV的线性效率。&lt;h4&gt;主要发现&lt;/h4&gt;直接应用RWKV到DG PCC面临两个挑战：固定方向的token移位方法在应用于非结构化点云时会导致空间失真，削弱局部几何建模；Bi-WKV注意力通过指数加权放大跨域分布的细微差异，导致注意力偏移和泛化能力下降。&lt;h4&gt;结论&lt;/h4&gt;PointDGRWKV框架通过引入自适应几何Token移位和跨域键特征分布对齐两个模块，有效解决了RWKV在点云分类领域泛化问题中的应用挑战，在多个基准测试上实现了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;领域泛化(DG)最近被探索用于增强点云分类(PCC)模型在未见域上的泛化能力。先前的工作基于卷积网络、Transformer或Mamba架构，要么受到感受野有限的限制，要么计算成本高，或者长距离依赖建模不足。RWKV作为一种新兴架构，具有优越的线性复杂度、全局感受野和长距离依赖能力。在本文中，我们首次研究了RWKV模型在DG PCC中的泛化能力。我们发现，直接将RWKV应用于DG PCC面临两个重大挑战：RWKV的固定方向token移位方法(如Q-Shift)在应用于非结构化点云时引入空间失真，削弱了局部几何建模并降低了鲁棒性。此外，RWKV中的Bi-WKV注意力通过指数加权放大了键分布中的轻微跨域差异，导致注意力偏移和泛化能力下降。为此，我们提出了PointDGRWKV，这是第一个为DG PCC量身定制的基于RWKV的框架。它引入了两个关键模块来增强空间建模和跨域鲁棒性，同时保持RWKV的线性效率。特别是，我们提出了自适应几何Token移位来建模局部邻域结构，以提高几何上下文感知能力。此外，设计了跨域键特征分布对齐，通过对齐跨域键特征分布来减轻注意力漂移。在多个基准上的广泛实验表明，PointDGRWKV在DG PCC上实现了最先进的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云分类中的领域泛化问题，即当模型从训练好的领域（如合成数据）应用到未见过的领域（如真实场景）时性能显著下降的问题。这个问题在现实中非常重要，因为自动驾驶、机器人、增强现实等应用需要模型适应各种传感器、环境和扫描角度变化带来的数据分布差异，而传统方法要么感受野有限（CNN），要么计算成本高（Transformer），要么难以捕获长距离依赖（Mamba）。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云分类方法的局限性，发现RWKV架构具有线性复杂度、全局感受野和长距离依赖建模能力，但直接应用于点云领域泛化存在两个主要问题：固定方向的token shift导致空间扭曲，以及Bi-WKV注意力机制中的指数加权放大跨领域差异。基于这些观察，作者设计了两个关键模块：自适应几何Token Shift基于空间分区构建局部邻域，借鉴了点云处理中的空间分区思想；跨领域关键特征分布对齐借鉴了领域适应中的分布对齐思想，但专门针对RWKV的注意力机制进行了优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用RWKV架构的优势，同时解决其在非结构化点云上的空间建模和跨领域泛化问题。整体流程包括：输入点云数据→自适应几何Token Shift（通过空间分区和加权特征聚合增强局部几何结构）→空间混合和通道混合→Bi-WKV注意力→跨领域关键特征分布对齐（对齐不同领域关键特征的均值和协方差）→多层堆叠→池化和分类。这种方法既保持了RWKV的线性计算效率，又增强了其在点云数据上的几何感知能力和跨领域鲁棒性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出PointDGRWKV，首个专门用于点云分类领域泛化的RWKV框架；2) 自适应几何Token Shift模块，通过空间分区而非固定方向进行token shift，计算复杂度为O(N)；3) 跨领域关键特征分布对齐模块，减轻注意力漂移。相比之前的工作，这种方法具有更大的感受野（相比CNN）、更低的计算复杂度（O(L)而非Transformer的O(L²)）、更好的长距离依赖建模能力（相比Mamba），并且解决了RWKV在点云上的空间扭曲和跨领域注意力漂移问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointDGRWKV通过自适应几何Token Shift和跨领域关键特征分布对齐两个创新模块，首次将RWKV架构有效应用于点云分类的领域泛化任务，在保持线性计算效率的同时显著提升了模型在未见领域的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain Generalization (DG) has been recently explored to enhance thegeneralizability of Point Cloud Classification (PCC) models toward unseendomains. Prior works are based on convolutional networks, Transformer or Mambaarchitectures, either suffering from limited receptive fields or highcomputational cost, or insufficient long-range dependency modeling. RWKV, as anemerging architecture, possesses superior linear complexity, global receptivefields, and long-range dependency. In this paper, we present the first workthat studies the generalizability of RWKV models in DG PCC. We find thatdirectly applying RWKV to DG PCC encounters two significant challenges: RWKV'sfixed direction token shift methods, like Q-Shift, introduce spatialdistortions when applied to unstructured point clouds, weakening localgeometric modeling and reducing robustness. In addition, the Bi-WKV attentionin RWKV amplifies slight cross-domain differences in key distributions throughexponential weighting, leading to attention shifts and degraded generalization.To this end, we propose PointDGRWKV, the first RWKV-based framework tailoredfor DG PCC. It introduces two key modules to enhance spatial modeling andcross-domain robustness, while maintaining RWKV's linear efficiency. Inparticular, we present Adaptive Geometric Token Shift to model localneighborhood structures to improve geometric context awareness. In addition,Cross-Domain key feature Distribution Alignment is designed to mitigateattention drift by aligning key feature distributions across domains. Extensiveexperiments on multiple benchmarks demonstrate that PointDGRWKV achievesstate-of-the-art performance on DG PCC.</description>
      <author>example@mail.com (Hao Yang, Qianyu Zhou, Haijia Sun, Xiangtai Li, Xuequan Lu, Lizhuang Ma, Shuicheng Yan)</author>
      <guid isPermaLink="false">2508.20835v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Surfel-based 3D Registration with Equivariant SE(3) Features</title>
      <link>http://arxiv.org/abs/2508.20789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于surfel的姿态学习回归方法，用于解决点云配准中忽略点方向性和不确定性的问题。该方法通过SE(3)等变卷积核学习显式的位置和旋转特征，预测源扫描和目标扫描之间的相对变换。实验结果表明，与最先进方法相比，该方法在真实点云扫描上表现出优越性和鲁棒性能。&lt;h4&gt;背景&lt;/h4&gt;点云配准对于确保遥感或数字遗产3D重建中多个局部点云的3D对齐一致性至关重要。现有的点云配准方法（包括非学习和基于学习的方法）都忽略了点的方向性和不确定性，导致模型对噪声输入和输入点云的剧烈旋转敏感，需要大量带变换增强的训练点云。&lt;h4&gt;目的&lt;/h4&gt;解决现有点云配准方法忽略点方向性和不确定性的问题，提高模型对噪声输入和剧烈旋转的鲁棒性，减少对大量训练数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于surfel的姿态学习回归方法，使用虚拟透视相机参数从Lidar点云初始化surfel，通过SE(3)等变卷积核学习显式的位置和旋转特征，预测源扫描和目标扫描之间的相对变换。模型由等变卷积编码器、交叉注意力机制、全连接解码器和非线性Huber损失组成。&lt;h4&gt;主要发现&lt;/h4&gt;在室内和室外数据集上的实验结果表明，与最先进的方法相比，所提模型在真实点云扫描上表现出优越性和鲁棒性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于surfel的姿态学习回归方法在处理真实点云数据时优于现有方法，具有更好的鲁棒性，减少了对大量训练数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;点云配准对于确保遥感或数字遗产3D重建中多个局部点云的3D对齐一致性至关重要。虽然存在各种基于点云的配准方法，无论是非学习还是基于学习的方法，它们都忽略了点的方向性和不确定性，使得模型容易受到噪声输入和输入点云的剧烈旋转（如正交变换）的影响；因此，需要大量带变换增强的训练点云。为了解决这些问题，我们提出了一种新颖的基于surfel的姿态学习回归方法。我们的方法可以使用虚拟透视相机参数从Lidar点云初始化surfel，并通过SE(3)等变卷积核学习显式的SE(3)等变特征，包括位置和旋转，以预测源扫描和目标扫描之间的相对变换。该模型包括一个等变卷积编码器、一个用于相似性计算的交叉注意力机制、一个全连接解码器和一个非线性Huber损失。在室内和室外数据集上的实验结果表明，与最先进的方法相比，我们的模型在真实点云扫描上表现出优越性和鲁棒性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云配准中对噪声输入和剧烈旋转敏感的问题。在3D重建、形状姿态估计、遥感和数字孪生等领域中，确保多个局部点云之间的3D对齐一致性至关重要。现有方法忽略了点的方向和不确定性信息，导致在复杂环境中性能下降，限制了它们在实际应用中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是它们忽略了点的方向和不确定性。他们借鉴了surfels（表面元素）表示方法，最初由Dahl等人提出，并在3D图形渲染中应用。同时，作者参考了equivariant特征表示的工作（如Spherical CNNs、SE(3)-Transformers）以及SpinNet等利用旋转不变性特征的方法。基于这些现有工作，作者设计了专门的SE(3)等变卷积核编码器、交叉注意力机制和特定的Huber损失函数，构建了一个完整的surfels-based配准框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用surfels（包含位置、法向量和不确定性半径）代替传统点云，并学习SE(3)等变特征来处理3D空间中的刚性变换。整体流程包括：1) Surfel初始化：从深度图或LiDAR点云创建surfels，计算位置、法向量和不确定性；2) 特征提取：使用SE(3)等变卷积核编码器学习等变特征；3) 相似性计算：通过交叉注意力机制计算源帧和目标帧之间的相似性；4) 变换预测：使用全连接层解码器预测相对位置和旋转；5) 模型优化：采用特定的SE(3)可微分Huber损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 引入surfels表示法，包含位置、法向量和不确定性信息；2) 设计SE(3)等变卷积核编码器，同时学习位置和旋转的等变特征；3) 使用交叉注意力机制计算帧间相似性；4) 根据不确定性对特征进行加权；5) 设计特定的Huber损失函数。相比之前的工作，传统方法如ICP收敛慢且对异常值敏感；现有深度学习方法如DGR和PointDSC没有考虑点的方向和不确定性；之前的等变方法主要处理SO(2)或SO(3)等变性而非完整的SE(3)等变性。本文方法通过surfels表示和SE(3)等变特征学习，在噪声和剧烈旋转情况下表现出更好的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于surfels的SE(3)等变特征学习方法，通过结合点的方向、位置和不确定性信息，显著提高了3D点云配准在噪声环境和剧烈旋转情况下的鲁棒性和准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is crucial for ensuring 3D alignment consistency ofmultiple local point clouds in 3D reconstruction for remote sensing or digitalheritage. While various point cloud-based registration methods exist, bothnon-learning and learning-based, they ignore point orientations and pointuncertainties, making the model susceptible to noisy input and aggressiverotations of the input point cloud like orthogonal transformation; thus, itnecessitates extensive training point clouds with transformation augmentations.To address these issues, we propose a novel surfel-based pose learningregression approach. Our method can initialize surfels from Lidar point cloudusing virtual perspective camera parameters, and learns explicit$\mathbf{SE(3)}$ equivariant features, including both position and rotationthrough $\mathbf{SE(3)}$ equivariant convolutional kernels to predict relativetransformation between source and target scans. The model comprises anequivariant convolutional encoder, a cross-attention mechanism for similaritycomputation, a fully-connected decoder, and a non-linear Huber loss.Experimental results on indoor and outdoor datasets demonstrate our modelsuperiority and robust performance on real point-cloud scans compared tostate-of-the-art methods.</description>
      <author>example@mail.com (Xueyang Kang, Hang Zhao, Kourosh Khoshelham, Patrick Vandewalle)</author>
      <guid isPermaLink="false">2508.20789v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>SeqVLM: Proposal-Guided Multi-View Sequences Reasoning via VLM for Zero-Shot 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2508.20758v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SeqVLM的新型零样本3D视觉定位框架，通过多视图图像和空间信息解决现有方法的空间有限推理和上下文忽略问题，在基准测试中实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;3D视觉定位旨在使用自然语言描述在3D场景中定位物体。虽然监督方法在受限环境中能实现更高准确性，但零样本3DVG对实际应用更有前景，因为它消除了场景特定训练需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有零样本3DVG方法面临的空间有限推理和上下文忽略或细节退化问题，提出一种能够利用多视图真实场景图像进行目标物体推理的新框架。&lt;h4&gt;方法&lt;/h4&gt;SeqVLM框架首先通过3D语义分割网络生成3D实例提议，并通过语义过滤保留相关候选对象；然后使用提议引导的多视图投影策略将候选提议投影到真实场景图像序列，保持空间关系和上下文细节；最后实现动态调度机制处理序列查询提示，利用VLM的跨模态推理能力识别文本指定的对象。&lt;h4&gt;主要发现&lt;/h4&gt;在ScanRefer和Nr3D基准测试上实现了最先进的性能，Acc@0.25分数分别达到55.6%和53.2%，超越了之前的零样本方法4.0%和5.2%。&lt;h4&gt;结论&lt;/h4&gt;SeqVLM框架有效解决了现有零样本3DVG方法的挑战，通过多视图图像和空间信息的利用，提高了定位准确性，推进了3DVG向更泛化和实际应用方向发展。&lt;h4&gt;翻译&lt;/h4&gt;三维视觉定位旨在使用自然语言描述在三维场景中定位物体。尽管监督方法在受限环境中能够实现更高的准确性，但零样本三维视觉定位对实际应用具有更大前景，因为它消除了场景特定训练要求。然而，现有零样本方法面临空间有限推理的挑战，因为它们依赖单视图定位，并且存在上下文忽略或细节退化的问题。为解决这些问题，我们提出了SeqVLM，一种新颖的零样本三维视觉定位框架，它利用具有空间信息的多视图真实场景图像进行目标物体推理。具体来说，SeqVLM首先通过三维语义分割网络生成三维实例提议，并通过语义过滤进行优化，只保留语义相关的候选对象。然后，提议引导的多视图投影策略将这些候选提议投影到真实场景图像序列上，在三维点云到图像的转换过程中保持空间关系和上下文细节。此外，为了减轻VLM计算过载，我们实现了一个动态调度机制，迭代处理序列查询提示，利用VLM的跨模态推理能力识别文本指定的对象。在ScanRefer和Nr3D基准测试上的实验证明了最先进的性能，实现了55.6%和53.2%的Acc@0.25分数，分别超过了之前的零样本方法4.0%和5.2%，这推进了三维视觉定位向更大的泛化和实际应用能力发展。代码可在https://github.com/JiawLin/SeqVLM获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决零样本3D视觉定位(Zero-Shot 3D Visual Grounding)中的空间推理受限和上下文缺失问题。这个问题在现实中很重要，因为它能帮助系统在没有特定场景训练的情况下，通过自然语言描述在3D场景中精确定位目标对象，这对于智能人机交互、自动驾驶环境感知和AR/VR系统等应用场景至关重要。与需要大量标注数据的监督方法相比，零样本方法更具实用性和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有零样本3D视觉定位方法的局限性，特别是单视图渲染带来的几何约束缺失和空间偏差问题。他们借鉴了LLM-based方法和VLM-based方法的优势，同时克服了它们的不足。作者设计了SeqVLM框架，整合3D点云、多视图图像和自然语言描述，通过提案选择、多视图投影和迭代推理三个核心模块，解决了现有方法的空间推理受限和上下文缺失问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过提案引导的多视图投影策略，将3D点云中的候选对象投影到真实场景图像序列中，保留空间关系和上下文细节，然后利用视觉语言模型进行跨模态推理。整体流程分为三步：1)提案选择模块：使用3D语义分割提取对象提案，通过语义过滤保留与目标相关的候选；2)提案引导的多视图投影模块：将候选提案投影到多视图图像上，选择最佳视角并拼接成图像序列；3)VLM迭代推理模块：通过动态批处理机制逐步缩小搜索空间，最终确定目标对象。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提案引导的多视图投影策略，有效保持3D几何属性和环境上下文；2)利用多视图真实图像增强空间理解和上下文感知；3)迭代推理机制解决VLM在多候选场景中的失效问题。相比之前的工作，SeqVLM克服了VLM-Grounder缺乏几何一致性和SeeGround依赖单视图渲染的局限，解决了渲染图像与真实世界图像之间的差距问题，显著提升了定位精度和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SeqVLM通过创新的提案引导多视图投影和迭代推理机制，实现了无需场景特定训练的高精度零样本3D视觉定位，其性能已能媲美监督学习方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Visual Grounding (3DVG) aims to localize objects in 3D scenes usingnatural language descriptions. Although supervised methods achieve higheraccuracy in constrained settings, zero-shot 3DVG holds greater promise forreal-world applications since eliminating scene-specific training requirements.However, existing zero-shot methods face challenges of spatial-limitedreasoning due to reliance on single-view localization, and contextual omissionsor detail degradation. To address these issues, we propose SeqVLM, a novelzero-shot 3DVG framework that leverages multi-view real-world scene images withspatial information for target object reasoning. Specifically, SeqVLM firstgenerates 3D instance proposals via a 3D semantic segmentation network andrefines them through semantic filtering, retaining only semantic-relevantcandidates. A proposal-guided multi-view projection strategy then projectsthese candidate proposals onto real scene image sequences, preserving spatialrelationships and contextual details in the conversion process of 3D pointcloud to images. Furthermore, to mitigate VLM computational overload, weimplement a dynamic scheduling mechanism that iteratively processessequances-query prompts, leveraging VLM's cross-modal reasoning capabilities toidentify textually specified objects. Experiments on the ScanRefer and Nr3Dbenchmarks demonstrate state-of-the-art performance, achieving Acc@0.25 scoresof 55.6% and 53.2%, surpassing previous zero-shot methods by 4.0% and 5.2%,respectively, which advance 3DVG toward greater generalization and real-worldapplicability. The code is available at https://github.com/JiawLin/SeqVLM.</description>
      <author>example@mail.com (Jiawen Lin, Shiran Bian, Yihang Zhu, Wenbin Tan, Yachao Zhang, Yuan Xie, Yanyun Qu)</author>
      <guid isPermaLink="false">2508.20758v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>AdaDPCC: Adaptive Rate Control and Rate-Distortion-Complexity Optimization for Dynamic Point Cloud Compression</title>
      <link>http://arxiv.org/abs/2508.20741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的动态点云压缩框架，通过可调节的编码结构和精细的运动估计补偿模块，实现了高效的率-失真-复杂度优化，在保持低比特率误差的同时显著提高了压缩效率和实时性能。&lt;h4&gt;背景&lt;/h4&gt;动态点云压缩在自动驾驶和AR/VR等应用中至关重要。然而，当前压缩方法在复杂度管理和码率控制方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种支持可变比特率和计算复杂度的动态编码框架，实现高效的率-失真-复杂度优化，解决数据稀疏性问题，并实现精确的码率控制。&lt;h4&gt;方法&lt;/h4&gt;提出了一种具有多种编码路径的精简框架，在单一模型内实现高效的率-失真-复杂度优化；设计了从粗到细的运动估计和补偿模块，分解几何信息同时扩展感知场；提出了精确的码率控制模块，内容自适应地将点云帧导航到各种编码路径以满足目标比特率。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进方法相比，平均BD-Rate降低了5.81%；BD-PSNR提高了0.42 dB；平均比特率误差保持在0.40%；与D-DPCC相比，平均编码时间减少了高达44.6%。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在实时和比特率受限的动态点云压缩场景中表现出色，显著提高了压缩效率和性能，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;动态点云压缩在自动驾驶和AR/VR等应用中至关重要。当前的压缩方法在复杂度管理和码率控制方面面临挑战。本文介绍了一种新的动态编码框架，支持可变比特率和计算复杂度。我们的方法包括一个具有多种编码路径的精简框架，允许在单一模型内实现高效的率-失真-复杂度优化。为解决帧间预测中的数据稀疏性问题，我们提出了从粗到细的运动估计和补偿模块，该模块在分解几何信息的同时扩展感知场。此外，我们提出了一个精确的码率控制模块，内容自适应地将点云帧导航通过各种编码路径以满足目标比特率。实验结果表明，与最先进的方法相比，我们的方法平均降低了5.81%的BD-Rate，提高了0.42 dB的BD-PSNR，同时将平均比特率误差保持在0.40%。此外，与D-DPCC相比，平均编码时间减少了高达44.6%，凸显了其在实时和比特率受限的动态点云压缩场景中的效率。我们的代码可在https://git.openi.org.cn/OpenPointCloud/Ada_DPCC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态点云压缩中的两个关键问题：复杂度管理和码率控制问题。这些问题在现实中非常重要，因为动态点云技术在自动驾驶、AR/VR等领域有广泛应用，这些应用需要实时渲染大量3D数据，对数据存储和传输提出了巨大挑战。一个灵活且高效的压缩算法对于在带宽和计算资源受限的条件下提供高质量体验至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过批判性分析现有方法的局限性来设计新方法：现有方法在特定码率场景下缺乏灵活性，帧间预测受限于点稀疏性和KNN方法的计算复杂度，且缺乏精确码率控制。作者借鉴了动态神经网络和slimmable auto-encoder的概念，参考了patch-based方法但进行了改进。设计思路包括创建动态编码框架支持可变码率和复杂度，提出粗到细的运动估计解决数据稀疏性问题，以及设计精确码率控制模块实现内容自适应的编码路线选择。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 使用多个编码路线实现不同复杂度和率失真权衡；2) 通过粗到细的帧间预测扩展感知场；3) 内容自适应地评估每条路线码率并选择最佳路线。整体流程：码率控制模块先估计各路线码率并选择最佳路线；当前帧通过选定路线编码为潜在表示，同时参考帧也通过相同路线编码；应用粗到细运动估计和补偿；使用补偿后的潜在变量和超先验进行条件编码；最后通过相同路线解码重建帧并缓冲用于后续预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 动态编码框架实现单个模型内的率-失真-复杂度优化；2) 粗到细帧间预测解决数据稀疏性问题；3) 精确码率控制模块实现内容自适应路线选择。相比之前工作：不同于传统方法固定率失真权衡，支持可变码率和复杂度；相比基于学习的方法，解决了KNN方法计算量大、感知场受限的问题；相比通用动态神经网络，专门针对点云压缩优化，结合超先验和时序上下文，设计更高效码率控制机制。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AdaDPCC通过动态编码框架、粗到细帧间预测和精确码率控制，实现了在动态点云压缩中灵活调整码率和计算复杂度的同时保持高质量重建，为实时和带宽受限的DPCC应用提供了高效解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic point cloud compression (DPCC) is crucial in applications likeautonomous driving and AR/VR. Current compression methods face challenges withcomplexity management and rate control. This paper introduces a novel dynamiccoding framework that supports variable bitrate and computational complexities.Our approach includes a slimmable framework with multiple coding routes,allowing for efficient Rate-Distortion-Complexity Optimization (RDCO) within asingle model. To address data sparsity in inter-frame prediction, we proposethe coarse-to-fine motion estimation and compensation module that deconstructsgeometric information while expanding the perceptive field. Additionally, wepropose a precise rate control module that content-adaptively navigates pointcloud frames through various coding routes to meet target bitrates. Theexperimental results demonstrate that our approach reduces the average BD-Rateby 5.81% and improves the BD-PSNR by 0.42 dB compared to the state-of-the-artmethod, while keeping the average bitrate error at 0.40%. Moreover, the averagecoding time is reduced by up to 44.6% compared to D-DPCC, underscoring itsefficiency in real-time and bitrate-constrained DPCC scenarios. Our code isavailable at https://git.openi.org.cn/OpenPointCloud/Ada_DPCC.</description>
      <author>example@mail.com (Chenhao Zhang, Wei Gao)</author>
      <guid isPermaLink="false">2508.20741v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>IAENet: An Importance-Aware Ensemble Model for 3D Point Cloud-Based Anomaly Detection</title>
      <link>http://arxiv.org/abs/2508.20492v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了IAENet（重要性感知集成网络），一种用于工业制造表面异常检测的集成框架，通过结合2D和3D专家模型并使用重要性感知融合模块，实现了更高的检测精度和更低的假阳性率。&lt;h4&gt;背景&lt;/h4&gt;表面异常检测对确保工业产品质量至关重要。虽然2D图像方法已取得显著成功，但3D点云方法虽有更丰富的几何线索，却研究不足，主要原因是缺乏与2D领域相当的预训练基础骨干网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效融合2D和3D信息的表面异常检测方法，克服现有方法中不同模态融合的困难，提高检测精度并降低假阳性率。&lt;h4&gt;方法&lt;/h4&gt;提出IAENet（重要性感知集成网络），这是一个集成框架，结合2D预训练专家模型和3D专家模型。引入了重要性感知融合（IAF）模块，动态评估各来源的贡献并重新加权异常分数。同时设计了关键损失函数指导IAF优化，结合专家的集体知识同时保留各自优势。&lt;h4&gt;主要发现&lt;/h4&gt;在MVTec 3D-AD数据集上的实验表明，IAENet实现了新的最先进水平，具有明显较低的假阳性率，突显了其在工业部署中的实际价值。&lt;h4&gt;结论&lt;/h4&gt;IAENet通过有效融合2D和3D信息，解决了3D点云异常检测中的预训练骨干网络缺失问题，并通过重要性感知融合机制提高了检测性能，为工业制造中的表面异常检测提供了实用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;表面异常检测对于确保工业制造中的产品质量至关重要。虽然基于2D图像的方法已经取得了显著成功，但基于3D点云的检测尽管拥有更丰富的几何线索，却仍未得到充分探索。我们认为，关键瓶颈是缺乏与2D领域相媲美的强大预训练基础骨干网络在3D领域。为了弥合这一差距，我们提出了重要性感知集成网络（IAENet），这是一个协同2D预训练专家与3D专家模型的集成框架。然而，简单融合来自不同来源的预测并非易事：现有策略可能受到表现不佳模态的影响，从而降低整体准确性。为了应对这一挑战，我们引入了一种新颖的重要性感知融合（IAF）模块，该模块动态评估每个来源的贡献并重新加权它们的异常分数。此外，我们设计了关键的损失函数，明确指导IAF的优化，使其不仅能够结合源专家的集体知识，还能保留它们的独特优势，从而增强异常检测的整体性能。在MVTec 3D-AD上的大量实验表明，我们的IAENet实现了新的最先进水平，并且具有明显较低的假阳性率，突显了其在工业部署中的实际价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云异常检测中的两大挑战：一是缺乏强大的3D预训练基础模型（类似于2D图像领域的预训练模型），二是简单融合不同模态预测结果的方法无法适应不同场景下各模型贡献度的变化。这些问题在工业制造中至关重要，因为表面异常检测是确保产品质量的关键环节，而现有的方法往往存在较高的误报率，限制了它们在实际工业环境中的应用。3D点云包含丰富的几何信息，能比2D图像更准确地捕捉物体的真实形状和细节，因此提高3D点云异常检测的性能对实现高效、自动化的产品质量检测具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了3D点云异常检测的瓶颈，特别是缺乏强大的预训练模型和简单融合策略的局限性。他们借鉴了多个现有工作：采用PointNet-SDF作为3D专家模型，借鉴了2D图像领域的预训练基础模型（如ResNet），参考了PatchCore的记忆库方法但改进为形状引导的双存储库，并吸收了知识蒸馏和记忆库方法的思想。基于这些借鉴，作者创新性地设计了重要性感知融合(IAF)模块，能够动态评估每个源模型的贡献，并通过专门的损失函数来指导IAF模块的学习，从而有效结合2D和3D模型的互补优势。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一个重要性感知融合(IAF)模块，智能地结合2D预训练模型和3D专家模型的预测结果。2D模型擅长提取语义信息和检测细微异常，但可能在正常区域给出较高分数；3D模型擅长捕捉全局几何信息，但可能难以检测微小异常。IAF模块能够动态评估每个模型在不同场景下的可靠性，并相应地加权它们的预测结果。整体流程分为两个阶段：首先是源专家学习阶段，训练2D和3D专家模型并构建双存储库；其次是重要性感知融合阶段，使用选择器网络评估专家模型的贡献，预测器网络生成最终异常分数。训练时使用合成的异常数据指导IAF模块的学习，推理时通过IAF融合两个专家的预测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 重要性感知融合(IAF)模块，能动态评估不同模型在不同场景下的贡献；2) 双专家模型架构，结合2D预训练模型和3D专家模型的优势；3) 专门的损失函数，包括选择器损失和预测器损失，特别指导IAF模块的学习；4) 形状引导的双存储库，改进了传统的存储库构建方法。相比之前的工作，IAENet的融合策略不再是简单的最大值选择或相加，而是能自适应调整各模型的贡献度；模型架构明确区分并利用2D和3D模型的互补优势；训练方法使用合成的异常数据和专门的损失函数来指导融合模块的学习；在降低误报率方面表现突出，具有更高的实用价值。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种名为IAENet的新型集成模型，通过重要性感知融合机制智能地结合2D预训练模型和3D专家模型的预测，显著提高了3D点云异常检测的准确性和鲁棒性，同时大幅降低了误报率，为工业制造中的表面质量检测提供了更可靠的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Surface anomaly detection is pivotal for ensuring product quality inindustrial manufacturing. While 2D image-based methods have achieved remarkablesuccess, 3D point cloud-based detection remains underexplored despite itsricher geometric cues. We argue that the key bottleneck is the absence ofpowerful pretrained foundation backbones in 3D comparable to those in 2D. Tobridge this gap, we propose Importance-Aware Ensemble Network (IAENet), anensemble framework that synergizes 2D pretrained expert with 3D expert models.However, naively fusing predictions from disparate sources is non-trivial:existing strategies can be affected by a poorly performing modality and thusdegrade overall accuracy. To address this challenge, We introduce an novelImportance-Aware Fusion (IAF) module that dynamically assesses the contributionof each source and reweights their anomaly scores. Furthermore, we devisecritical loss functions that explicitly guide the optimization of IAF, enablingit to combine the collective knowledge of the source experts but also preservetheir unique strengths, thereby enhancing the overall performance of anomalydetection. Extensive experiments on MVTec 3D-AD demonstrate that our IAENetachieves a new state-of-the-art with a markedly lower false positive rate,underscoring its practical value for industrial deployment.</description>
      <author>example@mail.com (Xuanming Cao, Chengyu Tao, Yifeng Cheng, Juan Du)</author>
      <guid isPermaLink="false">2508.20492v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Re-Densification Meets Cross-Scale Propagation: Real-Time Compression of LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2508.20466v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于紧凑特征的LiDAR点云高效压缩方法，通过两个轻量级模块实现高性能实时编解码。&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云是多种应用的基础，但高精度扫描会产生巨大的存储和传输开销。现有方法通常将无序点转换为分层八叉树或体素结构进行从密集到稀疏的预测编码。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中几何细节极度稀疏导致的上下文建模效率低、压缩性能和速度受限的问题，生成紧凑特征以实现高效的预测编码。&lt;h4&gt;方法&lt;/h4&gt;提出包含两个轻量级模块的框架：1)几何再密集化模块：重新密集化编码的稀疏几何，在更密集尺度提取特征后重新稀疏化，避免高成本计算；2)跨尺度特征传播模块：利用多分辨率占用线索引导分层特征传播，促进跨尺度信息共享，减少冗余特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合两个模块，产生了紧凑的特征表示，提供高效的上下文建模并加速编码过程。在KITTI数据集上实现了最先进的压缩比和实时性能，12位量化下编解码速度达26 FPS。&lt;h4&gt;结论&lt;/h4&gt;该方法有效解决了LiDAR点云压缩中的挑战，在保持高压缩比的同时实现了实时性能。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR点云是各种应用的基础，然而高精度扫描会产生巨大的存储和传输开销。现有方法通常将无序点转换为分层八叉树或体素结构进行从密集到稀疏的预测编码。然而，几何细节的极度稀疏阻碍了有效的上下文建模，从而限制了它们的压缩性能和速度。为了应对这一挑战，我们提出生成紧凑特征以实现高效的预测编码。我们的框架包含两个轻量级模块。首先，几何再密集化模块重新密集化编码的稀疏几何，在更密集的尺度提取特征，然后重新稀疏化特征以进行预测编码。该模块避免了在高度稀疏的细节上进行高成本计算，同时保持轻量级预测头。其次，跨尺度特征传播模块利用多个分辨率级别的占用线索来引导分层特征传播。这种设计促进了跨尺度信息共享，从而减少了冗余特征提取，并为几何再密集化模块提供了丰富的特征。通过整合这两个模块，我们的方法产生了紧凑的特征表示，提供了高效的上下文建模并加速了编码过程。在KITTI数据集上的实验展示了最先进的压缩比和实时性能，在12位量化下实现了每秒26帧的编解码速度。代码可在https://github.com/pengpeng-yu/FastPCC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决LiDAR点云在高精度压缩场景下的效率问题。随着自动驾驶和地图等领域的发展，3D感知技术产生大量点云数据，导致存储和传输开销巨大。现有方法在高分辨率下面临'高分辨率上下文稀疏性'(HRCS)问题，即几何细节的极端稀疏性阻碍了有效的上下文建模，限制了压缩性能和速度。这个问题对自动驾驶、机器人导航等实时应用至关重要，因为高效压缩可以减少数据传输带宽和存储需求，同时保持高精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云压缩方法的局限性，特别是体素和八叉树表示在高分辨率下的固有缺陷。他们通过数据统计发现了HRCS现象，即随着分辨率提高，局部邻域的上下文信息急剧减少。基于这一洞察，作者设计了两个互补模块：几何再密集化模块(GRED)和跨尺度特征传播模块(XFP)。这些方法借鉴了现有的基于八叉树和体素的压缩框架，但针对HRCS问题提出了创新解决方案，通过临时密集化和跨尺度信息共享来克服传统方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'几何再密集化'解决高分辨率上下文稀疏问题，并利用'跨尺度特征传播'实现不同分辨率级别间的信息共享。整体流程包括：1)将点云转换为八叉树结构；2)构建先验信息；3)跨尺度特征传播，其中浅层(l≤t)使用简化GRED模块跳过再密集化，深层(l&gt;t)应用完整GRED模块并融合多尺度特征；4)基于预测的占用分布进行熵编码。GRED模块具体包括：再密集化(将稀疏几何转换为密集表示)、特征提取(应用3D卷积)、重新稀疏化(返回原始稀疏空间)和预测编码(估计占用分布)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出并系统分析了'高分辨率上下文稀疏性'(HRCS)现象；2)几何再密集化模块(GRED)通过临时密集化稀疏几何来增强上下文建模；3)跨尺度特征传播模块(XFP)利用多分辨率占用线索指导特征传播。相比之前的工作，本文方法在保持高压缩性能的同时显著提高了计算效率，实现了实时处理(26 FPS)。相比基于Transformer的方法(如OctAttention、Light EHEM)，本文方法大幅减少了计算复杂度；相比最近的体素方法(如Unicorn)，在高比特率下表现更好；相比效率导向的方法(如RENO)，在保持实时性的同时显著提高了压缩质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的几何再密集化和跨尺度特征传播两个创新模块，有效解决了LiDAR点云高分辨率压缩中的上下文稀疏问题，实现了兼具高压缩比和实时性的高效点云压缩方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point clouds are fundamental to various applications, yethigh-precision scans incur substantial storage and transmission overhead.Existing methods typically convert unordered points into hierarchical octree orvoxel structures for dense-to-sparse predictive coding. However, the extremesparsity of geometric details hinders efficient context modeling, therebylimiting their compression performance and speed. To address this challenge, wepropose to generate compact features for efficient predictive coding. Ourframework comprises two lightweight modules. First, the GeometryRe-Densification Module re-densifies encoded sparse geometry, extracts featuresat denser scale, and then re-sparsifies the features for predictive coding.This module avoids costly computation on highly sparse details whilemaintaining a lightweight prediction head. Second, the Cross-scale FeaturePropagation Module leverages occupancy cues from multiple resolution levels toguide hierarchical feature propagation. This design facilitates informationsharing across scales, thereby reducing redundant feature extraction andproviding enriched features for the Geometry Re-Densification Module. Byintegrating these two modules, our method yields a compact featurerepresentation that provides efficient context modeling and accelerates thecoding process. Experiments on the KITTI dataset demonstrate state-of-the-artcompression ratios and real-time performance, achieving 26 FPS for bothencoding and decoding at 12-bit quantization. Code is available athttps://github.com/pengpeng-yu/FastPCC.</description>
      <author>example@mail.com (Pengpeng Yu, Haoran Li, Dingquan Li, Runqing Jiang, Jing Wang, Liang Lin, Yulan Guo)</author>
      <guid isPermaLink="false">2508.20466v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Learning Fast, Tool aware Collision Avoidance for Collaborative Robots</title>
      <link>http://arxiv.org/abs/2508.20457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种工具感知的碰撞避免系统，能根据不同工具尺寸和交互模式实时调整。系统使用学习模型处理点云数据，预测碰撞，并通过强化学习训练的控制策略快速生成避障动作。该方法在动态环境中表现优异，精度高且计算成本低。&lt;h4&gt;背景&lt;/h4&gt;在人类环境中确保协作机器人的安全高效运行具有挑战性，特别是在障碍物和任务随时间变化的动态环境中。当前机器人控制器通常假设完全可见性和固定工具，可能导致碰撞或过度保守行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应不同工具尺寸和工具-环境交互模式的碰撞避免系统，以解决动态环境中机器人的安全问题。&lt;h4&gt;方法&lt;/h4&gt;使用学习到的感知模型过滤点云中的机器人和工具组件，推理遮挡区域并在部分可观察情况下预测碰撞；通过约束强化学习训练控制策略，在10毫秒内生成平滑避障动作；在模拟和真实环境中进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;在动态环境中优于传统方法(APF、MPPI)；保持亚毫米级精度；计算成本比最先进的基于GPU的规划器低约60%；系统模块化、高效且有效。&lt;h4&gt;结论&lt;/h4&gt;该方法为在动态环境中运行的机器人提供了模块化、高效且有效的碰撞避免解决方案，已集成到协作机器人应用中并展示了实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在人类环境中确保协作机器人的安全高效运行具有挑战性，特别是在障碍物和任务随时间变化的动态环境中。当前的机器人控制器通常假设完全可见性和固定工具，这可能导致碰撞或过度保守的行为。在我们的工作中，我们引入了一种工具感知的碰撞避免系统，该系统能根据不同的工具尺寸和工具-环境交互模式实时调整。使用学习到的感知模型，我们的系统从点云中过滤掉机器人和工具组件，推理遮挡区域，并在部分可观察情况下预测碰撞。然后，我们使用通过约束强化学习训练的控制策略，在10毫秒内产生平滑的避障动作。在模拟和真实世界测试中，我们的方法在动态环境中优于传统方法(APF、MPPI)，同时保持亚毫米级精度。此外，与最先进的基于GPU的规划器相比，我们的系统计算成本降低了约60%。我们的方法为在动态环境中运行的机器人提供了模块化、高效且有效的碰撞避免。我们将我们的方法集成到协作机器人应用中，并展示了其在安全响应操作中的实际应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决协作机器人在动态环境中进行快速、工具感知的碰撞避免问题。这个问题很重要，因为随着协作机器人越来越多地应用于工业和家庭环境，确保它们与人类共享空间时的安全变得至关重要。现有方法往往依赖固定参数，无法适应动态环境变化和不同工具，在遮挡等情况下表现不佳，且计算成本高，难以实现实时性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先定义了三个关键需求：工具设置的灵活性、操作精确性以及响应性和可靠性。然后设计了一个混合系统，结合学习方法鲁棒性和经典控制器精确性。系统借鉴了Srinivasan等人的安全监督器方法、Hoeller等人的3D CNN架构以及Kim等人的约束马尔可夫决策过程框架。创新之处在于引入了工具感知设计（Engage和Protective两种模式）、能够过滤机器人和工具组件的点云处理模型，以及结合强化学习和经典逆运动学的混合控制策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是工具感知的碰撞避免，通过混合控制架构平衡精确性和响应性，使用学习型感知模型处理环境信息，并结合安全监督机制评估风险。整体流程是：1)接收末端执行器目标、工具几何和交互模式输入；2)使用3D CNN处理点云数据；3)安全监督器评估碰撞风险；4)低风险时使用强化学习输出作为逆运动学初始猜测实现高精度控制；5)高风险时直接使用强化学习策略进行快速反应；6)根据交互模式调整工具区域约束，允许或禁止工具接触。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)工具感知的碰撞避免，能适应不同工具和操作模式；2)混合控制架构，结合经典逆运动学高精度和强化学习快速反应；3)学习型感知模型，处理点云并过滤机器人和工具组件；4)高效安全监督机制，快速评估碰撞风险；5)高计算效率，50Hz运行频率。相比传统方法，本文方法能更好适应动态环境，平衡精确性和响应性，在遮挡环境下更鲁棒，计算效率更高，且能适应不同工具和操作模式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种快速、工具感知的碰撞避免系统，通过结合学习型感知模型、安全监督机制和混合控制架构，实现了在动态环境中协作机器人的安全、高效和精确操作。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3579207&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safe and efficient operation of collaborative robots in humanenvironments is challenging, especially in dynamic settings where both obstaclemotion and tasks change over time. Current robot controllers typically assumefull visibility and fixed tools, which can lead to collisions or overlyconservative behavior. In our work, we introduce a tool-aware collisionavoidance system that adjusts in real time to different tool sizes and modes oftool-environment interaction. Using a learned perception model, our systemfilters out robot and tool components from the point cloud, reasons aboutoccluded area, and predicts collision under partial observability. We then usea control policy trained via constrained reinforcement learning to producesmooth avoidance maneuvers in under 10 milliseconds. In simulated andreal-world tests, our approach outperforms traditional approaches (APF, MPPI)in dynamic environments, while maintaining sub-millimeter accuracy. Moreover,our system operates with approximately 60% lower computational cost compared toa state-of-the-art GPU-based planner. Our approach provides modular, efficient,and effective collision avoidance for robots operating in dynamic environments.We integrate our method into a collaborative robot application and demonstrateits practical use for safe and responsive operation.</description>
      <author>example@mail.com (Joonho Lee, Yunho Kim, Seokjoon Kim, Quan Nguyen, Youngjin Heo)</author>
      <guid isPermaLink="false">2508.20457v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Point Cloud Semantic Segmentation Pipeline for Unimproved Roads</title>
      <link>http://arxiv.org/abs/2508.20135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种数据高效的点云分割流水线和训练框架，用于稳健分割未改进的道路和其他七个类别。采用两阶段训练框架，结合点提示训练和流形混合正则化技术，仅使用50个目标域标记点云就能显著提高分割性能。&lt;h4&gt;背景&lt;/h4&gt;在3D语义分割领域，特别是在数据有限的挑战性场景中，如何实现稳健分割是一个重要问题。传统的训练方法在有限数据条件下表现不佳，需要探索更高效的数据利用方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种数据高效的点云分割方法，能够在有限标注数据的情况下实现稳健的道路和其他类别的分割，提高模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 采用两阶段训练框架：首先在公共城市数据集和小型定制领域数据集的混合数据上预训练基于投影的卷积神经网络，然后仅在领域数据上微调轻量级预测头；2. 探索点提示训练在批归一化层中的应用；3. 研究流形混合作为正则化的效果；4. 融入直方图归一化的环境信息以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用50个目标域的标记点云，所提出的训练方法将平均交并比从33.5%提高到51.8%，整体准确率从85.5%提高到90.8%。跨多个数据集的预训练对提高泛化能力和在有限领域监督下实现稳健分割至关重要。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一个实用的框架，用于在具有挑战性的低数据场景中进行稳健的3D语义分割。通过精心设计的两阶段训练框架和多种技术整合，即使数据有限也能实现显著的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;本研究案例中，我们提出了一种数据高效的点云分割流水线和训练框架，用于稳健分割未改进的道路和其他七个类别。我们的方法采用两阶段训练框架：首先，在公共城市数据集和小型定制领域数据集的混合数据上预训练基于投影的卷积神经网络；然后，仅在领域数据上微调轻量级预测头。在此过程中，我们探索了点提示训练在批归一化层中的应用，以及流形混合作为正则化在我们流水线中的效果。我们还研究了融入直方图归一化环境信息以进一步提高性能的效果。仅使用目标域的50个标记点云，我们证明与在领域数据上进行简单训练相比，我们提出的训练方法将平均交并比从33.5%提高到51.8%，整体准确率从85.5%提高到90.8%。关键的是，我们的结果表明，跨多个数据集进行预训练是提高泛化能力和在有限领域监督下实现稳健分割的关键。总体而言，本研究展示了一个在具有挑战性的低数据场景中进行稳健3D语义分割的实用框架。我们的代码可在以下网址获取：https://github.com/andrewyarovoi/MD-FRNet。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在有限标注数据下实现有效的点云语义分割，特别是针对未改进道路（如农村土路、碎石路等）的分割。这个问题重要是因为：1) 生成标记点云数据非常耗时耗力，平均每个扫描需30分钟以上；2) 新环境难以创建大规模数据集；3) 目标领域（农村、森林环境）与公共数据集（城市道路）差异显著；4) 使用的LiDAR传感器与公共数据集不同，导致直接应用预训练模型效果不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题框架化为少样本学习(FSL)，借鉴了多个现有工作：1) 借鉴图像分类中的度量学习(如原型网络)和迁移学习方法；2) 受PANet等点云分割工作的启发，使用语义掩码计算特征表示；3) 受[3]启发设计微调阶段使用小型MLP；4) 借鉴[6]的多数据集预训练策略；5) 应用Point Prompt Training处理多数据集分布差异；6) 探索Manifold Mixup作为正则化方法；7) 选择FRNet作为特征提取器，结合PointNeXt的倒瓶颈MLP设计预测头。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用两阶段训练框架：首先在混合数据集上预训练特征提取器，然后在领域内数据上微调轻量级预测头。整体流程：1) 数据准备：将所有数据集统一格式，进行多类映射，提取环境特征，实施多种数据增强；2) 预训练阶段：在混合数据集(Semantic KITTI、Waymo和目标数据集)上训练FRNet特征提取器；3) 微调阶段：冻结特征提取器，仅在目标数据集上训练MLP预测头；4) 技术组件：应用Point Prompt Training处理多数据集差异，探索Manifold Mixup平滑决策边界，利用环境信息提升分割精度；5) 评估：使用mIoU和准确率指标进行性能评估和消融研究。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出数据高效的两阶段训练框架，仅用50个标记点云实现显著性能提升；2) 首次将协同多数据集训练应用于点云语义分割的领域适应；3) 将Point Prompt Training扩展应用于基于卷积的架构；4) 探索Manifold Mixup作为点云分割头的正则化方法；5) 创新利用直方图归一化的环境值提升道路分割性能。相比之前工作的不同：1) 专注于跨领域适应(城市到农村)；2) 强调数据效率，解决低数据场景问题；3) 采用独特的两阶段训练策略；4) 评估8类分割而非单一类别；5) 结合多种技术提升模型在目标领域的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种数据高效的点云语义分割框架，通过多数据集预训练结合领域内微调，仅使用50个标记点云就能显著提升未改进道路的分割性能，为低数据场景下的3D语义分割提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this case study, we present a data-efficient point cloud segmentationpipeline and training framework for robust segmentation of unimproved roads andseven other classes. Our method employs a two-stage training framework: first,a projection-based convolutional neural network is pre-trained on a mixture ofpublic urban datasets and a small, curated in-domain dataset; then, alightweight prediction head is fine-tuned exclusively on in-domain data. Alongthe way, we explore the application of Point Prompt Training to batchnormalization layers and the effects of Manifold Mixup as a regularizer withinour pipeline. We also explore the effects of incorporating histogram-normalizedambients to further boost performance. Using only 50 labeled point clouds fromour target domain, we show that our proposed training approach improves meanIntersection-over-Union from 33.5% to 51.8% and the overall accuracy from 85.5%to 90.8%, when compared to naive training on the in-domain data. Crucially, ourresults demonstrate that pre-training across multiple datasets is key toimproving generalization and enabling robust segmentation under limitedin-domain supervision. Overall, this study demonstrates a practical frameworkfor robust 3D semantic segmentation in challenging, low-data scenarios. Ourcode is available at: https://github.com/andrewyarovoi/MD-FRNet.</description>
      <author>example@mail.com (Andrew Yarovoi, Christopher R. Valenta)</author>
      <guid isPermaLink="false">2508.20135v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>SEAL: Structure and Element Aware Learning to Improve Long Structured Document Retrieval</title>
      <link>http://arxiv.org/abs/2508.20778v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 Main Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究针对长结构化文档检索问题，提出了一种新型对比学习框架和配套的数据集，解决了现有方法无法有效利用结构特征和缺乏结构元数据的问题，显著提升了检索性能。&lt;h4&gt;背景&lt;/h4&gt;在长结构化文档检索领域，现有方法通常在缺乏明确结构信息的数据集上使用对比学习微调预训练语言模型，导致无法有效利用结构特征和元素级语义，且缺乏包含结构元数据的数据集。&lt;h4&gt;目的&lt;/h4&gt;解决现有长结构化文档检索方法中无法有效利用结构特征和缺乏结构元数据的问题，提高检索性能。&lt;h4&gt;方法&lt;/h4&gt;提出了名为SEAL的新型对比学习框架，利用结构感知学习保留语义层次结构，并通过掩码元素实现细粒度语义区分；同时发布了包含丰富结构注释的长结构化文档检索数据集。&lt;h4&gt;主要发现&lt;/h4&gt;在各种现代预训练语言模型上进行的实验和在线A/B测试表明，该方法带来了一致的性能提升，在BGE-M3模型上将NDCG@10指标从73.96%提升至77.84%。&lt;h4&gt;结论&lt;/h4&gt;SEAL框架和配套数据集有效解决了长结构化文档检索中的关键问题，显著提升了检索性能，相关资源已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;在长结构化文档检索中，现有方法通常在缺乏明确结构信息的数据集上使用对比学习来微调预训练语言模型。这种方法存在两个关键问题：1)当前方法无法有效利用结构特征和元素级语义；2)缺乏包含结构元数据的数据集。为解决这些问题，我们提出了SEAL，一种新型对比学习框架。它利用结构感知学习来保留语义层次结构，并通过掩码元素实现细粒度语义区分。此外，我们发布了SEAL数据集，一个具有丰富结构注释的长结构化文档检索数据集。在多个现代预训练语言模型上对发布和工业数据集进行的广泛实验，以及在线A/B测试，都展示了一致的性能提升，在BGE-M3上将NDCG@10从73.96%提升到77.84%。资源可在https://github.com/xinhaoH/SEAL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In long structured document retrieval, existing methods typically fine-tunepre-trained language models (PLMs) using contrastive learning on datasetslacking explicit structural information. This practice suffers from twocritical issues: 1) current methods fail to leverage structural features andelement-level semantics effectively, and 2) the lack of datasets containingstructural metadata. To bridge these gaps, we propose \our, a novel contrastivelearning framework. It leverages structure-aware learning to preserve semantichierarchies and masked element alignment for fine-grained semanticdiscrimination. Furthermore, we release \dataset, a long structured documentretrieval dataset with rich structural annotations. Extensive experiments onboth released and industrial datasets across various modern PLMs, along withonline A/B testing, demonstrate consistent performance improvements, boostingNDCG@10 from 73.96\% to 77.84\% on BGE-M3. The resources are available athttps://github.com/xinhaoH/SEAL.</description>
      <author>example@mail.com (Xinhao Huang, Zhibo Ren, Yipeng Yu, Ying Zhou, Zulong Chen, Zeyi Wen)</author>
      <guid isPermaLink="false">2508.20778v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>"Humor, Art, or Misinformation?": A Multimodal Dataset for Intent-Aware Synthetic Image Detection</title>
      <link>http://arxiv.org/abs/2508.20670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了S-HArM多模态数据集用于意图感知分类，探索了三种提示策略构建合成训练数据，并通过多种模型技术进行比较研究，发现图像和多模态引导数据训练的模型泛化能力更好，但整体性能仍有提升空间。&lt;h4&gt;背景&lt;/h4&gt;多模态AI的最新进展使检测合成内容和上下文外内容成为可能，但现有研究很大程度上忽略了AI生成图像背后的意图。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究在AI生成图像意图识别方面的空白，引入专门的数据集和训练方法来提高意图感知分类能力。&lt;h4&gt;方法&lt;/h4&gt;1) 创建S-HArM多模态数据集，包含9,576个来自社交媒体的图像-文本对并分类；2) 探索三种提示策略(图像引导、描述引导、多模态引导)使用Stable Diffusion构建合成训练数据；3) 进行多种技术比较研究，包括模态融合、对比学习、重建网络、注意力机制和大视觉语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;基于图像引导和多模态引导数据训练的模型在'野外'内容上泛化能力更好，因为保留了视觉上下文，但整体性能仍然有限。&lt;h4&gt;结论&lt;/h4&gt;推断AI生成图像背后的意图是一个复杂问题，需要专门架构和方法来提高性能，现有技术仍有较大改进空间。&lt;h4&gt;翻译&lt;/h4&gt;多模态AI的最新进展使得检测合成内容和上下文外内容成为可能。然而，现有工作在很大程度上忽略了AI生成图像背后的意图。为了填补这一空白，我们引入了S-HArM，这是一个用于意图感知分类的多模态数据集，包含9,576个来自Twitter/X和Reddit的'野外'图像-文本对，标记为幽默/讽刺、艺术或错误信息。此外，我们探索了三种提示策略（图像引导、描述引导和多模态引导）使用Stable Diffusion构建大规模合成训练数据集。我们进行了广泛的比较研究，包括模态融合、对比学习、重建网络、注意力机制和大视觉语言模型。我们的结果表明，在图像引导和多模态引导数据上训练的模型在'野外'内容上泛化能力更好，原因是保留了视觉上下文。然而，整体性能仍然有限，突显了推断意图的复杂性以及需要专门架构的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746275.3762215&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal AI have enabled progress in detecting syntheticand out-of-context content. However, existing efforts largely overlook theintent behind AI-generated images. To fill this gap, we introduce S-HArM, amultimodal dataset for intent-aware classification, comprising 9,576 "in thewild" image-text pairs from Twitter/X and Reddit, labeled as Humor/Satire, Art,or Misinformation. Additionally, we explore three prompting strategies(image-guided, description-guided, and multimodally-guided) to construct alarge-scale synthetic training dataset with Stable Diffusion. We conduct anextensive comparative study including modality fusion, contrastive learning,reconstruction networks, attention mechanisms, and large vision-languagemodels. Our results show that models trained on image- and multimodally-guideddata generalize better to "in the wild" content, due to preserved visualcontext. However, overall performance remains limited, highlighting thecomplexity of inferring intent and the need for specialized architectures.</description>
      <author>example@mail.com (Anastasios Skoularikis, Stefanos-Iordanis Papadopoulos, Symeon Papadopoulos, Panagiotis C. Petrantonakis)</author>
      <guid isPermaLink="false">2508.20670v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Amadeus: Autoregressive Model with Bidirectional Attribute Modelling for Symbolic Music</title>
      <link>http://arxiv.org/abs/2508.20665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Amadeus的新型符号音乐生成框架，采用两级架构（自回归模型处理音符序列，双向离散扩散模型处理音符属性），并提出了MLSDES和CIEM两种增强策略。实验表明，Amadeus在多个指标上显著优于现有模型，同时实现了至少4倍的加速，并支持无需训练的细粒度音符属性控制。&lt;h4&gt;背景&lt;/h4&gt;现有的最先进的符号音乐生成模型主要采用自回归或层次自回归架构，将符号音乐建模为具有单向时间依赖关系的属性标记序列，并假设这些属性之间存在固定的、严格的依赖结构。&lt;h4&gt;目的&lt;/h4&gt;基于音符属性本质上是并发且无序的集合而非时间依赖序列的新认识，开发一种能够更有效处理音符属性关系的新型符号音乐生成框架。&lt;h4&gt;方法&lt;/h4&gt;1) 采用两级架构：自回归模型处理音符序列，双向离散扩散模型处理音符属性；2) 提出Music Latent Space Discriminability Enhancement Strategy (MLSDES)，结合对比学习约束增强中间音乐表示的区分度；3) 设计Conditional Information Enhancement Module (CIEM)，通过注意力机制加强音符潜在向量表示；4) 编译了最大的开源符号音乐数据集AMD支持预训练和微调。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用不同属性作为初始标记会导致性能相当，表明音符属性是并发且无序的集合；2) Amadeus在无条件生成和文本条件生成任务上显著优于SOTA模型；3) 实现了至少4倍的加速；4) 支持无需训练的细粒度音符属性控制。&lt;h4&gt;结论&lt;/h4&gt;Amadeus通过重新思考音符属性之间的关系，采用创新的两级架构和增强策略，显著提高了符号音乐生成的质量和效率，同时提供了更好的控制能力，为符号音乐生成领域带来了新的突破。&lt;h4&gt;翻译&lt;/h4&gt;现有的最先进的符号音乐生成模型主要采用自回归或层次自回归架构，将符号音乐建模为具有单向时间依赖关系的属性标记序列，并假设这些属性之间存在固定的、严格的依赖结构。然而，我们观察到，在这些模型中使用不同属性作为初始标记会导致性能相当。这表明音符的属性本质上是一个并发且无序的集合，而不是时间依赖的序列。基于这一见解，我们引入了Amadeus，一种新型的符号音乐生成框架。Amadeus采用两级架构：用于音符序列的自回归模型和用于音符属性的双向离散扩散模型。为了增强性能，我们提出了Music Latent Space Discriminability Enhancement Strategy (MLSDES)，结合对比学习约束，增强中间音乐表示的区分度。Conditional Information Enhancement Module (CIEM)通过注意力机制同时加强音符潜在向量表示，实现更精确的音符解码。我们在无条件生成和文本条件生成任务上进行了广泛实验。Amadeus在多个指标上显著优于SOTA模型，同时实现了至少4倍的加速。此外，我们展示了使用我们的模型进行无需训练的细粒度音符属性控制的可行性。为了探索Amadeus架构的上限性能，我们编译了迄今为止最大的开源符号音乐数据集AMD，支持预训练和微调。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing state-of-the-art symbolic music generation models predominantlyadopt autoregressive or hierarchical autoregressive architectures, modellingsymbolic music as a sequence of attribute tokens with unidirectional temporaldependencies, under the assumption of a fixed, strict dependency structureamong these attributes. However, we observe that using different attributes asthe initial token in these models leads to comparable performance. Thissuggests that the attributes of a musical note are, in essence, a concurrentand unordered set, rather than a temporally dependent sequence. Based on thisinsight, we introduce Amadeus, a novel symbolic music generation framework.Amadeus adopts a two-level architecture: an autoregressive model for notesequences and a bidirectional discrete diffusion model for attributes. Toenhance performance, we propose Music Latent Space Discriminability EnhancementStrategy(MLSDES), incorporating contrastive learning constraints that amplifydiscriminability of intermediate music representations. The ConditionalInformation Enhancement Module (CIEM) simultaneously strengthens note latentvector representation via attention mechanisms, enabling more precise notedecoding. We conduct extensive experiments on unconditional andtext-conditioned generation tasks. Amadeus significantly outperforms SOTAmodels across multiple metrics while achieving at least 4$\times$ speed-up.Furthermore, we demonstrate training-free, fine-grained note attribute controlfeasibility using our model. To explore the upper performance bound of theAmadeus architecture, we compile the largest open-source symbolic music datasetto date, AMD (Amadeus MIDI Dataset), supporting both pre-training andfine-tuning.</description>
      <author>example@mail.com (Hongju Su, Ke Li, Lan Yang, Honggang Zhang, Yi-Zhe Song)</author>
      <guid isPermaLink="false">2508.20665v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning through Auxiliary Branch for Video Object Detection</title>
      <link>http://arxiv.org/abs/2508.20551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted paper for ACIVS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为CLAB（对比学习辅助分支）的视频目标检测方法，通过对比学习辅助分支和动态损失加权策略，在不增加计算负担的情况下提高了视频目标检测的性能，特别是在处理图像退化问题方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;视频目标检测是一项具有挑战性的任务，因为视频常常受到图像质量下降的影响，如运动模糊、遮挡和可变形状，这使得它比在静态图像中检测物体要困难得多。先前的方法通过使用特征聚合和复杂的后处理技术提高了性能，但代价是增加了计算需求。&lt;h4&gt;目的&lt;/h4&gt;提高对图像退化的鲁棒性，同时在推理过程中不增加额外的计算负担。&lt;h4&gt;方法&lt;/h4&gt;1. 实现了一个使用对比损失的对比辅助分支，以增强视频目标检测骨干网络的特征表示能力；2. 提出了一种动态损失加权策略，在训练初期强调辅助特征学习，随着训练收敛逐渐优先考虑检测任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过全面的实验和消融研究验证了该方法，证明了一致的性能提升。CLAB在ImageNet VID数据集上使用ResNet-101和ResNeXt-101分别达到了84.0%和85.2%的mAP，在不需要额外后处理方法的情况下，为基于CNN的模型实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;CLAB方法是一种简单而有效的视频目标检测改进方法，能够在不增加计算负担的情况下提高对图像退化的鲁棒性，并在ImageNet VID数据集上取得了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;视频目标检测是一项具有挑战性的任务，因为视频常常受到图像质量下降的影响，如运动模糊、遮挡和可变形状，这使得它比在静态图像中检测物体要困难得多。先前的方法通过使用特征聚合和复杂的后处理技术提高了视频目标检测的性能，但代价是增加了计算需求。为了提高对图像退化的鲁棒性，同时在推理过程中不增加额外的计算负担，我们引入了一种简单而有效的对比学习辅助分支（CLAB）方法。首先，我们使用对比损失实现了一个对比辅助分支，以增强视频目标检测骨干网络的特征表示能力。接下来，我们提出了一种动态损失加权策略，在训练初期强调辅助特征学习，随着训练收敛逐渐优先考虑检测任务。我们通过全面的实验和消融研究验证了我们的方法，证明了一致的性能提升。CLAB在ImageNet VID数据集上使用ResNet-101和ResNeXt-101分别达到了84.0%和85.2%的mAP，因此在不要求额外后处理方法的情况下，为基于CNN的模型实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video object detection is a challenging task because videos often suffer fromimage deterioration such as motion blur, occlusion, and deformable shapes,making it significantly more difficult than detecting objects in still images.Prior approaches have improved video object detection performance by employingfeature aggregation and complex post-processing techniques, though at the costof increased computational demands. To improve robustness to image degradationwithout additional computational load during inference, we introduce astraightforward yet effective Contrastive Learning through Auxiliary Branch(CLAB) method. First, we implement a constrastive auxiliary branch using acontrastive loss to enhance the feature representation capability of the videoobject detector's backbone. Next, we propose a dynamic loss weighting strategythat emphasizes auxiliary feature learning early in training while graduallyprioritizing the detection task as training converges. We validate our approachthrough comprehensive experiments and ablation studies, demonstratingconsistent performance gains. Without bells and whistles, CLAB reaches aperformance of 84.0% mAP and 85.2% mAP with ResNet-101 and ResNeXt-101,respectively, on the ImageNet VID dataset, thus achieving state-of-the-artperformance for CNN-based models without requiring additional post-processingmethods.</description>
      <author>example@mail.com (Lucas Rakotoarivony)</author>
      <guid isPermaLink="false">2508.20551v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Fact or Facsimile? Evaluating the Factual Robustness of Modern Retrievers</title>
      <link>http://arxiv.org/abs/2508.20408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 34th ACM International Conference on Information  and Knowledge Management&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究发现，RAG管道中的密集检索器和重排序器在事实能力上显著低于其基础大语言模型，检索器准确性比生成模型低约30个百分点，且对干扰项高度敏感，决策主要基于表面语义接近而非事实推理。&lt;h4&gt;背景&lt;/h4&gt;密集检索器和重排序器是检索增强生成(RAG)管道的核心组件，准确检索事实信息对维持系统可信度和防御RAG中毒至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究检索组件从其基础大语言模型(LLMs)继承或失去了多少事实能力。&lt;h4&gt;方法&lt;/h4&gt;将12个公开发布的嵌入检查点与其原始基础LLM配对，在事实性基准测试上评估这两组模型的表现。&lt;h4&gt;主要发现&lt;/h4&gt;1) 嵌入变体准确性比基础模型低12-43个百分点(中位数28点)；2) 检索器准确率降至25-35%，而生成模型达60-70%；3) 候选池扩大导致检索器准确率显著下降；4) 检索决策主要基于语义相似度而非事实推理；5) 当词汇线索被掩盖时，超过三分之二正确预测变为错误。&lt;h4&gt;结论&lt;/h4&gt;对比学习为检索器引入了系统性权衡：语义检索的增益是以参数化事实知识的损失为代价的。&lt;h4&gt;翻译&lt;/h4&gt;密集检索器和重排序器是检索增强生成(RAG)管道的核心组件，在RAG中准确检索事实信息对维持系统可信度和防御RAG中毒至关重要。然而，这些组件从其基础大语言模型(LLMs)继承或失去了多少事实能力尚不清楚。我们将12个公开发布的嵌入检查点与其原始基础LLM配对，并在事实性基准测试上评估这两组模型。在每个评估模型中，嵌入变体的准确性明显低于其基础模型，绝对下降幅度在12到43个百分点之间(中位数28个百分点)，检索器的典型准确性下降到25-35%的范围内，而生成模型达到60-70%。这种退化在更苛刻的条件下加剧：当每个问题的候选池从四个选项扩展到一千个选项时，最强检索器的top-1准确性从33%下降到26%，显示出对干扰项数量的高度敏感性。统计测试进一步表明，对于每个嵌入模型，查询与正确完成之间的余弦相似度得分显著高于与错误完成之间的得分(p &lt; 0.01)，表明决策主要由表面语义接近性而非事实推理驱动。为了探究这一弱点，我们使用GPT-4.1重新表述每个正确完成，创建了一个保留事实真相但掩盖词汇线索的重写测试集，观察到超过三分之二之前正确的预测转变为错误，总体准确性降至原始水平的三分之一左右。总之，这些发现揭示了对比学习为检索器引入的系统性权衡：语义检索的增益是以参数化事实知识的损失为代价的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dense retrievers and rerankers are central to retrieval-augmented generation(RAG) pipelines, where accurately retrieving factual information is crucial formaintaining system trustworthiness and defending against RAG poisoning.However, little is known about how much factual competence these componentsinherit or lose from the large language models (LLMs) they are based on. Wepair 12 publicly released embedding checkpoints with their original base LLMsand evaluate both sets on a factuality benchmark. Across every model evaluated,the embedding variants achieve markedly lower accuracy than their bases, withabsolute drops ranging from 12 to 43 percentage points (median 28 pts) andtypical retriever accuracies collapsing into the 25-35 % band versus the 60-70% attained by the generative models. This degradation intensifies under a moredemanding condition: when the candidate pool per question is expanded from fouroptions to one thousand, the strongest retriever's top-1 accuracy falls from 33% to 26 %, revealing acute sensitivity to distractor volume. Statistical testsfurther show that, for every embedding model, cosine-similarity scores betweenqueries and correct completions are significantly higher than those forincorrect ones (p &lt; 0.01), indicating decisions driven largely by surface-levelsemantic proximity rather than factual reasoning. To probe this weakness, weemployed GPT-4.1 to paraphrase each correct completion, creating a rewrittentest set that preserved factual truth while masking lexical cues, and observedthat over two-thirds of previously correct predictions flipped to wrong,reducing overall accuracy to roughly one-third of its original level. Takentogether, these findings reveal a systematic trade-off introduced bycontrastive learning for retrievers: gains in semantic retrieval are paid forwith losses in parametric factual knowledge......</description>
      <author>example@mail.com (Haoyu Wu, Qingcheng Zeng, Kaize Ding)</author>
      <guid isPermaLink="false">2508.20408v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>DFAMS: Dynamic-flow guided Federated Alignment based Multi-prototype Search</title>
      <link>http://arxiv.org/abs/2508.20353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DFAMS的新型联邦检索框架，通过动态信息流(DIF)识别查询意图并构建语义对齐的知识分区，解决了现有方法在处理模糊查询和跨域场景时的局限性，显著提高了检索质量和下游任务性能。&lt;h4&gt;背景&lt;/h4&gt;联邦检索(FR)通过跨多个外部知识源路由查询来减轻LLMs的幻觉问题，但当知识分布时，现有方法难以检索模糊查询的高质量和相关文档，特别是在跨域场景中，这限制了它们在支持下游生成任务方面的有效性。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦检索方法在处理模糊查询和跨域场景时的局限性，提高检索质量和相关性，以更好地支持下游生成任务。&lt;h4&gt;方法&lt;/h4&gt;提出DFAMS框架，利用动态信息流(DIF)识别潜在查询意图并构建语义对齐的知识分区。具体包括：利用少量标注查询的梯度信号和基于Shapley值的归因方法追踪与意图识别和子域边界检测相关的神经元激活路径；通过多原型对比学习训练对齐模块，实现细粒度的源内建模和跨知识库的语义对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在五个基准测试中，DFAMS显著优于先进的FR方法：知识分类准确率提高最高达14.37%，检索召回率提高5.38%，下游QA准确率提高6.45%，证明了其在复杂FR场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;DFAMS框架能够有效解决联邦检索中的模糊查询和跨域挑战，通过动态信息流和语义对齐，显著提升了检索质量和下游任务性能。&lt;h4&gt;翻译&lt;/h4&gt;联邦检索(FR)在必要时跨多个外部知识源路由查询，以减轻LLMs的幻觉，当必要的外部知识分布时。然而，现有方法难以检索模糊查询的高质量和相关文档，特别是在跨域场景中，这显著限制了它们在支持下游生成任务方面的有效性。受动态信息流(DIF)启发，我们提出了DFAMS，一个新颖的框架，利用DIF识别潜在查询意图，并构建语义对齐的知识分区，以实现跨异构源的准确检索。具体来说，DFAMS通过利用少量标注查询的梯度信号，并采用基于Shapley值的归因方法来追踪与意图识别和子域边界检测相关的神经元激活路径，从而探测LLMs中的DIF。然后，DFAMS利用DIF通过多原型对比学习训练对齐模块，实现跨知识库的细粒度源内建模和语义对齐。在五个基准测试中的实验结果表明，DFAMS在知识分类准确率上比先进的FR方法高出最多14.37%，在检索召回率上高出5.38%，在下游QA准确率上高出6.45%，证明了其在复杂FR场景中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Retrieval (FR) routes queries across multiple external knowledgesources, to mitigate hallucinations of LLMs, when necessary external knowledgeis distributed. However, existing methods struggle to retrieve high-quality andrelevant documents for ambiguous queries, especially in cross-domain scenarios,which significantly limits their effectiveness in supporting downstreamgeneration tasks. Inspired by dynamic information flow (DIF), we propose DFAMS,a novel framework that leverages DIF to identify latent query intents andconstruct semantically aligned knowledge partitions for accurate retrievalacross heterogeneous sources. Specifically, DFAMS probes the DIF in LLMs byleveraging gradient signals from a few annotated queries and employing Shapleyvalue-based attribution to trace neuron activation paths associated with intentrecognition and subdomain boundary detection. Then, DFAMS leverages DIF totrain an alignment module via multi-prototype contrastive learning, enablingfine-grained intra-source modeling and inter-source semantic alignment acrossknowledge bases. Experimental results across five benchmarks show that DFAMSoutperforms advanced FR methods by up to 14.37% in knowledge classificationaccuracy, 5.38% in retrieval recall, and 6.45% in downstream QA accuracy,demonstrating its effectiveness in complex FR scenarios.</description>
      <author>example@mail.com (Zhibang Yang, Xinke Jiang, Rihong Qiu, Ruiqing Li, Yihang Zhang, Yue Fang, Yongxin Xu, Hongxin Ding, Xu Chu, Junfeng Zhao, Yasha Wang)</author>
      <guid isPermaLink="false">2508.20353v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Staircase Recognition and Location Based on Polarization Vision</title>
      <link>http://arxiv.org/abs/2505.19026v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合偏振和光强信息的对比度增强算法以及基于YOLOv11的点云分割方法，用于楼梯检测，并融合偏振双目和TOF深度信息实现楼梯高质量三维重建，同时提出基于ICP配准和改进灰狼优化算法的单目相机与TOF相机联合校准方法。&lt;h4&gt;背景&lt;/h4&gt;楼梯是人工场景中常见结构，但对人形机器人和下肢残疾或视觉障碍人士而言，缺乏传感器和智能算法支持难以穿越。楼梯场景感知技术是识别定位的前提，对机器人模式切换和足迹位置计算至关重要。现有技术面临识别精度低、传感器噪声高、信号不稳定和计算要求高等问题。双目和TOF重建易受环境光和物体表面材料影响。&lt;h4&gt;目的&lt;/h4&gt;实现楼梯检测、高质量三维重建以及单目相机和TOF相机的联合校准。&lt;h4&gt;方法&lt;/h4&gt;1) 提出结合偏振和光强信息的对比度增强算法；2) 集成基于YOLOv11的点云分割；3) 融合偏振双目和TOF深度信息实现楼梯三维重建；4) 基于ICP配准和改进灰狼优化算法提出单目相机与TOF相机联合校准算法。&lt;h4&gt;主要发现&lt;/h4&gt;偏振重建方法受环境光影响较小，且不依赖于物体表面的纹理信息，相比传统方法具有明显优势。&lt;h4&gt;结论&lt;/h4&gt;通过结合偏振信息与传统方法，可以有效提高楼梯场景感知的准确性和鲁棒性，为机器人导航和辅助技术提供更好的支持。&lt;h4&gt;翻译&lt;/h4&gt;楼梯是人工场景中最常见的结构之一。然而，对于人形机器人和下肢残疾或视觉障碍的人士来说，没有传感器和智能算法的帮助很难穿越楼梯场景。楼梯场景感知技术是识别和定位的前提，对机器人模式切换和足迹位置计算以适应不连续地形具有重要意义。然而，仍有诸多问题制约该技术应用，如识别精度低、传感器初始噪声高、输出信号不稳定和计算要求高。在场景重建方面，双目和飞行时间(TOF)重建易受环境光和目标物体表面材料影响。相比之下，由于偏振器的特殊结构，偏振可以选择性传输特定方向的偏振光，这种重建方法依赖于物体表面的偏振信息，因此偏振重建的优势得以体现，即受环境光影响较小且不依赖于物体表面的纹理信息。本文为实现楼梯检测，提出了一种结合偏振和光强信息的对比度增强算法，并集成了基于YOLOv11的点云分割。为实现高质量重建，我们提出了一种融合偏振双目和TOF深度信息的方法来实现楼梯的三维重建。此外，还提出了一种基于ICP配准和改进灰狼优化算法的单目相机和TOF相机联合校准算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决人形机器人和行动不便人群的楼梯识别和定位问题。现有方法存在低识别准确率、对光照条件敏感和依赖纹理信息等局限性。这个问题在现实中非常重要，因为楼梯是常见的障碍物，准确的楼梯识别技术对机器人导航控制、下肢残疾人士辅助导航以及提升机器人在楼梯场景中的整体性能至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有楼梯识别方法的局限性，包括可穿戴传感器和光电传感器的问题。然后设计了一个基于偏振视觉的融合框架，包含三个主要模块：楼梯识别模块、异构传感器校准模块和偏振3D重建模块。作者借鉴了YOLO系列目标检测算法、偏振成像技术、点云分割技术、ICP算法和灰狼优化算法等现有工作，但进行了创新性改进，如融合偏振-强度图像、改进灰狼优化算法(结合Levy飞行和动态权重)，以及双目偏振和TOF深度信息融合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用偏振视觉获取更多维度信息增强图像对比度，并通过融合双目视觉和TOF深度信息克服单一传感器局限性：双目视觉纠正偏振方位角模糊，TOF填充双目数据空洞。整体流程分为三阶段：1)楼梯识别：使用MLP融合DoP和强度图像，YOLOv11初步识别，点云法向量分割提高准确率；2)异构传感器校准：使用棋盘格校准板，结合改进灰狼优化算法优化ICP，解决不同分辨率相机校准；3)偏振3D重建：双目初步纠正偏振梯度场，双目深度信息纠正方位角模糊，TOF纠正空洞区域。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)楼梯识别模块：偏振-强度图像融合和点云法向量分割，提高识别准确率至98.7%；2)异构传感器校准模块：改进灰狼优化算法(Levy飞行和动态权重)，解决不同分辨率相机校准，精度达0.33±0.04mm；3)偏振3D重建模块：双目偏振和TOF融合，实现&lt;0.2%重建误差。相比之前工作，该方法不依赖纹理信息(解决双目视觉表面扭曲问题)，纠正偏振法向量模糊(解决纯偏振方法问题)，无需高精度扫描仪数据集，适用于动态场景和低纹理表面。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于偏振视觉的楼梯识别与定位方法，通过融合双目偏振视觉和TOF深度信息，实现了高精度(98.7%)的楼梯识别和&lt;0.2%重建误差的3D重建，为机器人提供了准确的楼梯场景感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Staircase is one of the most common structures in artificial scenes. However,it is difficult for humanoid robots and people with lower limb disabilities orvisual impairment to cross the scene without the help of sensors andintelligent algorithms. Staircase scene perception technology is a prerequisitefor recognition and localization. This technology is of great significance forthe mode switching of the robot and the calculation of the footprint positionto adapt to the discontinuous terrain. However, there are still many problemsthat constrain the application of this technology, such as low recognitionaccuracy, high initial noise from sensors, unstable output signals and highcomputational requirements. In terms of scene reconstruction, the binocular andtime of flight (TOF) reconstruction of the scene can be easily affected byenvironmental light and the surface material of the target object. In contrast,due to the special structure of the polarizer, the polarization can selectivelytransmit polarized light in a specific direction and this reconstruction methodrelies on the polarization information of the object surface. So the advantagesof polarization reconstruction are reflected, which are less affected byenvironmental light and not dependent on the texture information of the objectsurface. In this paper, in order to achieve the detection of staircase, thispaper proposes a contrast enhancement algorithm that integrates polarizationand light intensity information, and integrates point cloud segmentation basedon YOLOv11. To realize the high-quality reconstruction, we proposed a method offusing polarized binocular and TOF depth information to realize thethree-dimensional (3D) reconstruction of the staircase. Besides, it alsoproposes a joint calibration algorithm of monocular camera and TOF camera basedon ICP registration and improved gray wolf optimization algorithm.</description>
      <author>example@mail.com (Weifeng Kong, Zhiying Tan)</author>
      <guid isPermaLink="false">2505.19026v3</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>E-ConvNeXt: A Lightweight and Efficient ConvNeXt Variant with Cross-Stage Partial Connections</title>
      <link>http://arxiv.org/abs/2508.20955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E-ConvNeXt的新型轻量级网络，通过整合Cross Stage Partial Connections机制和一系列优化设计，显著降低了ConvNeXt的参数规模和网络复杂度，同时保持了高准确率性能。&lt;h4&gt;背景&lt;/h4&gt;许多高性能网络在设计时没有考虑轻量级应用场景，限制了它们的应用范围。&lt;h4&gt;目的&lt;/h4&gt;通过整合Cross Stage Partial Connections机制和一系列优化设计，显著减少ConvNeXt的参数规模和网络复杂度，并保持高准确率性能。&lt;h4&gt;方法&lt;/h4&gt;提出名为E-ConvNeXt的新网络，具有三个核心创新：1) 将Cross Stage Partial Network与ConvNeXt集成并调整网络结构，降低模型网络复杂度高达80%；2) 优化Stem和Block结构，增强模型特征表达能力和操作效率；3) 用通道注意力替换Layer Scale。&lt;h4&gt;主要发现&lt;/h4&gt;在ImageNet分类上的实验验证了E-ConvNeXt在准确率-效率平衡方面的优越性：E-ConvNeXt-mini在0.9 GFLOPs下达到78.3% Top-1准确率；E-ConvNeXt-small在3.1 GFLOPs下达到81.9% Top-1准确率。&lt;h4&gt;结论&lt;/h4&gt;在目标检测任务上的迁移学习测试进一步证实了E-ConvNeXt的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;许多高性能网络在设计之初并未考虑轻量级应用场景，这大大限制了它们的应用范围。本文以ConvNeXt为研究对象，通过整合Cross Stage Partial Connections机制和一系列优化设计，显著降低了ConvNeXt的参数规模和网络复杂度。新网络被命名为E-ConvNeXt，能够在不同复杂度配置下保持高精度性能。E-ConvNeXt的三个核心创新是：(1) 将Cross Stage Partial Network (CSPNet)与ConvNeXt集成并调整网络结构，将模型网络复杂度降低高达80%；(2) 优化Stem和Block结构，增强模型特征表达能力和操作效率；(3) 用通道注意力替换Layer Scale。在ImageNet分类上的实验验证了E-ConvNeXt在准确率-效率平衡方面的优越性：E-ConvNeXt-mini在0.9 GFLOPs下达到78.3%的Top-1准确率；E-ConvNeXt-small在3.1 GFLOPs下达到81.9%的Top-1准确率。在目标检测任务上的迁移学习测试进一步证实了其泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many high-performance networks were not designed with lightweight applicationscenarios in mind from the outset, which has greatly restricted their scope ofapplication. This paper takes ConvNeXt as the research object and significantlyreduces the parameter scale and network complexity of ConvNeXt by integratingthe Cross Stage Partial Connections mechanism and a series of optimizeddesigns. The new network is named E-ConvNeXt, which can maintain high accuracyperformance under different complexity configurations. The three coreinnovations of E-ConvNeXt are : (1) integrating the Cross Stage Partial Network(CSPNet) with ConvNeXt and adjusting the network structure, which reduces themodel's network complexity by up to 80%; (2) Optimizing the Stem and Blockstructures to enhance the model's feature expression capability and operationalefficiency; (3) Replacing Layer Scale with channel attention. Experimentalvalidation on ImageNet classification demonstrates E-ConvNeXt's superioraccuracy-efficiency balance: E-ConvNeXt-mini reaches 78.3% Top-1 accuracy at0.9GFLOPs. E-ConvNeXt-small reaches 81.9% Top-1 accuracy at 3.1GFLOPs. Transferlearning tests on object detection tasks further confirm its generalizationcapability.</description>
      <author>example@mail.com (Fang Wang, Huitao Li, Wenhan Chao, Zheng Zhuo, Yiran Ji, Chang Peng, Yupeng Sun)</author>
      <guid isPermaLink="false">2508.20955v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Classification under Decision Rule Drift with Application to Optimal Individualized Treatment Rule Estimation</title>
      <link>http://arxiv.org/abs/2508.20942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文将迁移学习分类框架从基于回归函数的方法扩展到决策规则，提出了一种通过贝叶斯决策规则对后验漂移进行建模的新方法。该方法利用贝叶斯决策边界的几何变换，将问题重新表述为低维经验风险最小化问题。研究建立了估计量的一致性，推导了风险边界，并将方法应用于最优个体化治疗规则的估计。通过模拟研究和真实数据分析，证明了该方法优越的性能和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;迁移学习分类框架最初基于回归函数方法，需要扩展到决策规则领域。&lt;h4&gt;目的&lt;/h4&gt;提出一种通过贝叶斯决策规则对后验漂移进行建模的新方法论，并展示其在最优个体化治疗规则估计中的广泛应用。&lt;h4&gt;方法&lt;/h4&gt;利用贝叶斯决策边界的几何变换，将问题重新表述为低维经验风险最小化问题，建立估计量的一致性，并推导风险边界。&lt;h4&gt;主要发现&lt;/h4&gt;在温和的正则条件下，所提出的估计量具有一致性，并且推导出了相应的风险边界。该方法在最优个体化治疗规则估计中表现出广泛的适用性。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的模拟研究和真实数据分析，证明了该方法具有优越的性能和鲁棒性，适用于迁移学习分类框架的决策规则扩展。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们将迁移学习分类框架从基于回归函数的方法扩展到决策规则。我们提出了一种通过贝叶斯决策规则对后验漂移进行建模的新方法。通过利用贝叶斯决策边界的几何变换，我们的方法将问题重新表述为低维经验风险最小化问题。在温和的正则条件下，我们建立了估计量的一致性并推导了风险边界。此外，我们通过将方法适应于最优个体化治疗规则的估计，展示了其广泛的适用性。广泛的模拟研究和真实数据分析进一步证明了我们方法的优越性能和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we extend the transfer learning classification framework fromregression function-based methods to decision rules. We propose a novelmethodology for modeling posterior drift through Bayes decision rules. Byexploiting the geometric transformation of the Bayes decision boundary, ourmethod reformulates the problem as a low-dimensional empirical riskminimization problem. Under mild regularity conditions, we establish theconsistency of our estimators and derive the risk bounds. Moreover, weillustrate the broad applicability of our method by adapting it to theestimation of optimal individualized treatment rules. Extensive simulationstudies and analyses of real-world data further demonstrate both superiorperformance and robustness of our approach.</description>
      <author>example@mail.com (Xiaohan Wang, Yang Ning)</author>
      <guid isPermaLink="false">2508.20942v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Accurate Screening of Functional Materials with Machine-Learning Potential and Transfer-Learned Regressions: Heusler Alloy Benchmark</title>
      <link>http://arxiv.org/abs/2508.20556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种机器学习加速的高通量工作流程，用于发现具有大磁晶各向异性能的磁性材料。通过筛选四元和全d-Heusler化合物，结合原子间势和机器学习模型预测，并使用密度泛函理论验证，证明了该方法的高预测精度。&lt;h4&gt;背景&lt;/h4&gt;磁性材料的发现通常需要高通量筛选方法，但传统计算方法效率低下。机器学习可以加速这一过程，但需要确保预测的准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习加速的高通量工作流程，用于高效筛选具有大磁晶各向异性能的稳定磁性材料。&lt;h4&gt;方法&lt;/h4&gt;使用eSEN-30M-OAM原子间势进行结构优化和形成能评估；使用在DxMag Heusler数据库上训练的eSEM模型预测局部磁矩、声子稳定性、磁稳定性和磁晶各向异性能；采用冻结迁移学习策略提高准确性；使用密度泛函理论验证候选化合物；评估不同机器学习原子间势的性能；讨论局部磁矩预测的保真度及其扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;机器学习加速的高通量工作流程能够有效筛选具有大磁晶各向异性能的稳定化合物；密度泛函理论验证证实了高预测精度；评估了不同机器学习原子间势的性能；讨论了局部磁矩预测的保真度及其对其他磁性材料的适用性。&lt;h4&gt;结论&lt;/h4&gt;机器学习加速的高通量工作流程结合了计算效率和预测准确性，是发现新型磁性材料的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种机器学习加速的高通量工作流程，用于发现磁性材料。作为测试案例，我们筛选了四元和全d-Heusler化合物，寻找具有大磁晶各向异能量（E_aniso）的稳定化合物。使用eSEN-30M-OAM原子间势进行结构优化和形成能评估，同时使用在我们的DxMag Heusler数据库上训练的eSEM模型预测局部磁矩、声子稳定性、磁稳定性和E_aniso。采用冻结迁移学习策略提高准确性。通过密度泛函理论验证了机器学习高通量工作流程确定的候选化合物，证实了高预测精度。我们还评估了不同机器学习原子间势的性能，并讨论了局部磁矩预测的保真度及其对其他磁性材料的扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A machine learning-accelerated high-throughput (HTP) workflow for thediscovery of magnetic materials is presented. As a test case, we screenedquaternary and all-$d$ Heusler compounds for stable compounds with largemagnetocrystalline anisotropy energy ($E_{\mathrm{aniso}}$). Structureoptimization and evaluation of formation energy and distance to hull convexwere performed using the eSEN-30M-OAM interatomic potential, while localmagnetic moments, phonon stability, magnetic stability, and$E_{\mathrm{aniso}}$ were predicted by eSEM models trained on our DxMag Heuslerdatabase. A frozen transfer learning strategy was employed to improve accuracy.Candidate compounds identified by the ML-HTP workflow were validated withdensity functional theory, confirming high predictive precision. We alsobenchmark the performance of different MLIPs, and discuss the fidelity of localmagnetic moment prediction and its extension to other magnetic materials.</description>
      <author>example@mail.com (Enda Xiao, Terumasa Tadano)</author>
      <guid isPermaLink="false">2508.20556v1</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Generating Human-AI Collaborative Design Sequence for 3D Assets via Differentiable Operation Graph</title>
      <link>http://arxiv.org/abs/2508.17645v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了3D-AIGC与以人为中心的设计范式之间的表示不兼容问题，通过生成设计操作序列实现了AI与设计师工作流程的对接，提高了人机协作效率。&lt;h4&gt;背景&lt;/h4&gt;3D人工智能生成内容(3D-AIGC)能够快速合成复杂几何形状，但AI生成内容与设计师工作流程之间存在根本性脱节。传统AI框架主要处理网格或神经表示，而设计师使用参数建模工具，这种不兼容性降低了AI对3D行业的实际价值。&lt;h4&gt;目的&lt;/h4&gt;解决AI生成内容与设计师工作流程之间的表示差异，通过生成设计操作序列来弥合这一差距，提高AI在3D设计领域的实用性和人机协作效率。&lt;h4&gt;方法&lt;/h4&gt;将基础建模操作重新表述为可微分单元，实现对连续和离散参数的联合优化；构建带有门控机制的分层图，通过最小化Chamfer距离进行端到端优化；应用多阶段序列长度约束和领域规则惩罚实现无监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;生成的操作序列实现了高几何保真度、平滑的网格布线、合理的步骤组合和灵活的编辑能力，完全兼容设计行业的工作流程。&lt;h4&gt;结论&lt;/h4&gt;通过生成设计操作序列，成功连接了AI生成内容与设计师工作流程，使AI能够更好地融入3D设计行业，提高人机协作效率。&lt;h4&gt;翻译&lt;/h4&gt;三维人工智能生成内容(3D-AIGC)的出现使得复杂几何形状的快速合成成为可能。然而，AI生成内容与以人为中心的设计范式之间仍然存在根本性脱节，这种脱节源于表示方法的不兼容性：传统的AI框架主要处理网格或神经表示（例如，NeRF、高斯溅射），而设计师则在参数建模工具中工作。这种脱节降低了AI对3D行业的实际价值，削弱了人机协作的效率。为了解决这种差异，我们专注于生成设计操作序列，这些是有序的建模历史，全面捕捉3D资产的逐步构建过程，并与现代3D软件中设计师的典型工作流程保持一致。我们首先将基础建模操作（例如，拉伸、布尔运算）重新表述为可微分单元，通过基于梯度的学习实现对连续参数（例如，拉伸高度）和离散参数（例如，布尔类型）的联合优化。基于这些可微分操作，构建了一个带有门控机制的分层图，并通过最小化目标几何的Chamfer距离进行端到端优化。多阶段序列长度约束和领域规则惩罚使得无需真实序列监督即可学习紧凑的设计序列。广泛的验证表明，生成的操作序列实现了高几何保真度、平滑的网格布线、合理的步骤组合和灵活的编辑能力，并且完全兼容设计行业。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决AI生成的3D内容与专业3D设计工作流程之间的根本脱节问题。传统AI框架处理网格或神经表示，而设计师使用参数建模工具，这种不兼容性降低了AI在3D行业的实用价值，削弱了人机协作效率。研究显示70%的专业设计师需花费超过3小时修改AI生成的3D内容，且72%的人报告没有节省时间甚至增加了成本。此外，缺乏构建历史迫使艺术家进行密集的顶点操作而非调整设计参数，影响后续物理模拟、角色变形等生产流程。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了AI生成内容与专业3D工作流程的不兼容性，然后采用设计操作序列作为统一表示，捕捉3D资产的逐步构建过程。方法设计包括：将基本建模操作重新表述为可微分单元，构建具有门控机制的分层操作图，通过最小化Chamfer距离进行端到端优化。作者借鉴了3D-AIGC系统（如NeRF、扩散模型）和CAD建模（如CSG树）的研究，但明确区分了设计操作与CAD命令的不同，指出设计操作适用于更广泛的3D资产类型，且缺乏大规模数据集支持，因此创新性地提出了零样本方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将传统离散几何建模操作转化为可微分单元，使基于梯度的学习能同时优化连续和离散参数，生成与设计师工作流程兼容的操作序列。整体流程包括：1)构建可微分操作节点，每个节点处理连续参数(通过链式规则)和离散参数(通过概率分支机制)；2)构建可微分操作图，通过门控机制动态调节各节点影响；3)通过最小化Chamfer距离进行端到端优化；4)针对细分、挤压、倒角、布尔运算等关键操作设计专门的微分实现方案；5)在推理时提取激活节点，生成确定性的操作序列。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出设计操作序列作为3D资产的统一表示，与专业工作流程兼容；2)开发可微分操作框架，使离散操作可优化；3)实现零样本方法，无需大规模训练数据；4)引入门控和概率分支机制处理离散参数；5)针对关键操作创新的微分实现。相比之前工作，本方法不同于传统3D-AIGC系统(生成静态网格而非可编辑序列)，区别于CAD建模(适用于更广泛资产类型)，超越基于MLLM的方法(避免机械重复模式)，解决了设计操作序列数据稀缺问题，实现了与专业设计工作流程的无缝集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过可微分操作图生成人机协作3D资产设计序列的方法，有效 bridging 了AI生成内容与专业3D设计工作流程之间的鸿沟，使AI生成的3D资产具有与设计师工作流程兼容的可编辑操作历史。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of 3D artificial intelligence-generated content (3D-AIGC) hasenabled rapid synthesis of intricate geometries. However, a fundamentaldisconnect persists between AI-generated content and human-centric designparadigms, rooted in representational incompatibilities: conventional AIframeworks predominantly manipulate meshes or neural representations(\emph{e.g.}, NeRF, Gaussian Splatting), while designers operate withinparametric modeling tools. This disconnection diminishes the practical value ofAI for 3D industry, undermining the efficiency of human-AI collaboration. Toresolve this disparity, we focus on generating design operation sequences,which are structured modeling histories that comprehensively capture thestep-by-step construction process of 3D assets and align with designers'typical workflows in modern 3D software. We first reformulate fundamentalmodeling operations (\emph{e.g.}, \emph{Extrude}, \emph{Boolean}) intodifferentiable units, enabling joint optimization of continuous (\emph{e.g.},\emph{Extrude} height) and discrete (\emph{e.g.}, \emph{Boolean} type)parameters via gradient-based learning. Based on these differentiableoperations, a hierarchical graph with gating mechanism is constructed andoptimized end-to-end by minimizing Chamfer Distance to target geometries.Multi-stage sequence length constraint and domain rule penalties enableunsupervised learning of compact design sequences without ground-truth sequencesupervision. Extensive validation demonstrates that the generated operationsequences achieve high geometric fidelity, smooth mesh wiring, rational stepcomposition and flexible editing capacity, with full compatibility withindesign industry.</description>
      <author>example@mail.com (Xiaoyang Huang, Bingbing Ni, Wenjun Zhang)</author>
      <guid isPermaLink="false">2508.17645v2</guid>
      <pubDate>Fri, 29 Aug 2025 15:01:32 +0800</pubDate>
    </item>
    <item>
      <title>Disabling Self-Correction in Retrieval-Augmented Generation via Stealthy Retriever Poisoning</title>
      <link>http://arxiv.org/abs/2508.20083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了DisarmRAG，一种针对检索增强生成（RAG）系统的新型攻击方法，通过损害检索器而非知识库来绕过大型语言模型的自我纠正能力，成功实现高攻击成功率且保持隐蔽性。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成（RAG）已成为提高大型语言模型可靠性的标准方法，但先前研究表明RAG系统容易被通过污染知识库进行攻击，误导生成攻击者选择的输出。然而，现代LLMs的自我纠正能力（SCA）可以减轻此类攻击。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的污染范式，损害检索器本身以抑制自我纠正能力（SCA）并强制生成攻击者选择的输出，绕过LLMs的自我纠正机制。&lt;h4&gt;方法&lt;/h4&gt;提出DisarmRAG，一种基于对比学习的模型编辑技术，执行局部和隐蔽的编辑，确保检索器仅针对特定受害者查询返回恶意指令。设计迭代协同优化框架，自动发现能够绕过基于提示的防御的强大指令。&lt;h4&gt;主要发现&lt;/h4&gt;在六个LLMs和三个QA基准上的评估显示，恶意指令的检索接近完美，成功抑制了SCA，并在多种防御提示下实现了超过90%的攻击成功率。被编辑的检索器在几种检测方法下保持隐蔽性。&lt;h4&gt;结论&lt;/h4&gt;以检索器为中心的攻击方法（如DisarmRAG）对RAG系统构成严重威胁，凸显了开发针对检索器的防御措施的迫切性。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成（RAG）已成为提高大型语言模型（LLMs）可靠性的标准方法。先前工作通过污染知识库，证明RAG系统容易被误导生成攻击者选择的输出，从而展示了其脆弱性。然而，本文发现，现代LLMs的强大自我纠正能力（SCA）可以减轻此类攻击，只要正确配置，就能拒绝虚假上下文。这种SCA对试图操纵RAG系统的攻击者构成了重大挑战。与先前主要针对知识库的污染方法不同，我们引入了DisarmRAG，一种新的污染范式，它损害检索器本身以抑制SCA并强制生成攻击者选择的输出。这种损害使攻击者能够直接将反SCA指令嵌入到提供给生成器的上下文中，从而绕过SCA。为此，我们提出了一种基于对比学习的模型编辑技术，执行局部和隐蔽的编辑，确保检索器仅针对特定受害者查询返回恶意指令，同时保持良性检索行为。为了进一步增强攻击，我们设计了一个迭代协同优化框架，能够自动发现能够绕过基于提示的防御的强大指令。我们在六个LLMs和三个QA基准上广泛评估了DisarmRAG。结果表明，恶意指令的检索接近完美，成功抑制了SCA，并在多种防御提示下实现了超过90%的攻击成功率。此外，被编辑的检索器在几种检测方法下保持隐蔽性，凸显了对以检索器为中心的防御的迫切需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-Augmented Generation (RAG) has become a standard approach forimproving the reliability of large language models (LLMs). Prior workdemonstrates the vulnerability of RAG systems by misleading them intogenerating attacker-chosen outputs through poisoning the knowledge base.However, this paper uncovers that such attacks could be mitigated by the strong\textit{self-correction ability (SCA)} of modern LLMs, which can reject falsecontext once properly configured. This SCA poses a significant challenge forattackers aiming to manipulate RAG systems.  In contrast to previous poisoning methods, which primarily target theknowledge base, we introduce \textsc{DisarmRAG}, a new poisoning paradigm thatcompromises the retriever itself to suppress the SCA and enforceattacker-chosen outputs. This compromisation enables the attacker tostraightforwardly embed anti-SCA instructions into the context provided to thegenerator, thereby bypassing the SCA. To this end, we present acontrastive-learning-based model editing technique that performs localized andstealthy edits, ensuring the retriever returns a malicious instruction only forspecific victim queries while preserving benign retrieval behavior. To furtherstrengthen the attack, we design an iterative co-optimization framework thatautomatically discovers robust instructions capable of bypassing prompt-baseddefenses. We extensively evaluate DisarmRAG across six LLMs and three QAbenchmarks. Our results show near-perfect retrieval of malicious instructions,which successfully suppress SCA and achieve attack success rates exceeding 90\%under diverse defensive prompts. Also, the edited retriever remains stealthyunder several detection methods, highlighting the urgent need forretriever-centric defenses.</description>
      <author>example@mail.com (Yanbo Dai, Zhenlan Ji, Zongjie Li, Kuan Li, Shuai Wang)</author>
      <guid isPermaLink="false">2508.20083v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
  <item>
      <title>ProMSC-MIS: Prompt-based Multimodal Semantic Communication for Multi-Spectral Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.20057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2508.17920&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为ProMSC-MIS的新型基于提示的多模态语义通信框架，用于多光谱图像分割，能够在带宽受限信道上高效传输空间对齐的RGB和热图像。&lt;h4&gt;背景&lt;/h4&gt;多模态语义通信通过整合跨模态的互补信息，具有提高下游任务性能的巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的通信框架，用于多光谱图像分割任务，在带宽受限的信道上传输空间对齐的RGB和热图像。&lt;h4&gt;方法&lt;/h4&gt;1. 利用提示学习和对比学习预训练单模态语义编码器，通过一种模态特征作为另一种模态的提示来学习互补语义表示；2. 设计结合交叉注意力和挤压-激励(SE)网络的语义融合模块，有效融合跨模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;ProMSC-MIS显著优于传统图像传输与先进分割方法的结合；在相同分割性能下，减少50%-70%的信道带宽；降低26%的存储开销和37%的计算复杂度；消融研究验证了预训练和语义融合策略的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方案非常适合自动驾驶和夜间监控等应用场景。&lt;h4&gt;翻译&lt;/h4&gt;多模态语义通信通过整合跨模态的互补信息，在提高下游任务性能方面具有巨大潜力。本文介绍了ProMSC-MIS，一种用于多光谱图像分割的新型基于提示的多模态语义通信框架。它能够在带宽受限的信道上高效传输空间对齐的RGB和热图像。我们的框架有两个主要的设计创新。首先，通过利用提示学习和对比学习，单模态语义编码器被预训练，通过使用一种模态的特征作为另一种模态的提示，来学习多样化和互补的语义表示。其次，设计了一个语义融合模块，结合了交叉注意力和挤压-激励(SE)网络，以有效融合跨模态特征。实验结果表明，ProMSC-MIS显著优于传统的图像传输与最先进的分割方法相结合的性能。值得注意的是，在相同的分割性能下，它将所需的信道带宽减少了50%--70%，同时将存储开销和计算复杂度分别降低了26%和37%。消融研究也验证了所提出的预训练和语义融合策略的有效性。我们的方案非常适合自动驾驶和夜间监控等应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal semantic communication has great potential to enhance downstreamtask performance by integrating complementary information across modalities.This paper introduces ProMSC-MIS, a novel Prompt-based Multimodal SemanticCommunication framework for Multi-Spectral Image Segmentation. It enablesefficient task-oriented transmission of spatially aligned RGB and thermalimages over band-limited channels. Our framework has two main design novelties.First, by leveraging prompt learning and contrastive learning, unimodalsemantic encoders are pre-trained to learn diverse and complementary semanticrepresentations by using features from one modality as prompts for another.Second, a semantic fusion module that combines cross-attention mechanism andsqueeze-and-excitation (SE) networks is designed to effectively fusecross-modal features. Experimental results demonstrate that ProMSC-MISsubstantially outperforms conventional image transmission combined withstate-of-the-art segmentation methods. Notably, it reduces the required channelbandwidth by 50%--70% at the same segmentation performance, while alsodecreasing the storage overhead and computational complexity by 26% and 37%,respectively. Ablation studies also validate the effectiveness of the proposedpre-training and semantic fusion strategies. Our scheme is highly suitable forapplications such as autonomous driving and nighttime surveillance.</description>
      <author>example@mail.com (Haoshuo Zhang, Yufei Bo, Meixia Tao)</author>
      <guid isPermaLink="false">2508.20057v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Prototype Alignment for Semi-supervised Pathology Image Segmentation</title>
      <link>http://arxiv.org/abs/2508.19574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为MPAMatch的新型病理图像分割框架，通过多模态原型引导的监督范式和双重对比学习策略，解决了病理图像分割中语义边界模糊和标注成本高的问题，显著提升了分割性能。&lt;h4&gt;背景&lt;/h4&gt;病理图像分割面临语义边界模糊和像素级标注成本高的挑战。现有基于一致性正则化的半监督方法主要依赖图像模态内的扰动一致性，难以捕捉高级语义先验，特别是在结构复杂的病理图像中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉结构和语义信息的病理图像分割方法，提高分割精度，减少对大量标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出MPAMatch框架，采用像素级对比学习，在多模态原型引导的监督范式下工作。核心创新是双重对比学习方案：图像原型与像素标签之间，以及文本原型与像素标签之间。同时，改进了TransUNet架构，用病理预训练的基础模型Uni替换了ViT主干。&lt;h4&gt;主要发现&lt;/h4&gt;双重对比学习策略增强了对未标记样本的判别能力；首次将文本原型监督引入分割，改善了语义边界建模；改进的架构能有效提取病理相关特征。&lt;h4&gt;结论&lt;/h4&gt;MPAMatch在多个数据集上优于最先进方法，验证了其在结构和语义建模方面的双重优势，为病理图像分割提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;病理图像分割面临诸多挑战，主要是由于语义边界模糊和像素级标注的高成本。尽管最近基于一致性正则化的半监督方法（如UniMatch）已取得显著进展，但它们主要依赖图像模态内的基于扰动的一致性，难以捕捉高级语义先验，特别是在结构复杂的病理图像中。为解决这些限制，我们提出了MPAMatch - 一种新的分割框架，它在多模态原型引导的监督范式下执行像素级对比学习。MPAMatch的核心创新在于图像原型与像素标签之间以及文本原型与像素标签之间的双重对比学习方案，同时在结构和语义级别提供监督。这种从粗到细的监督策略不仅增强了对未标记样本的判别能力，而且首次将文本原型监督引入分割，显著改善了语义边界建模。此外，我们通过将其ViT主干替换为病理预训练的基础模型（Uni）来重建经典的分割架构（TransUNet），从而实现更有效地提取病理相关特征。在GLAS、EBHI-SEG-GLAND、EBHI-SEG-CANCER和KPI上的大量实验表明，MPAMatch优于最先进的方法，验证了其在结构和语义建模方面的双重优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathological image segmentation faces numerous challenges, particularly dueto ambiguous semantic boundaries and the high cost of pixel-level annotations.Although recent semi-supervised methods based on consistency regularization(e.g., UniMatch) have made notable progress, they mainly rely onperturbation-based consistency within the image modality, making it difficultto capture high-level semantic priors, especially in structurally complexpathology images. To address these limitations, we propose MPAMatch - a novelsegmentation framework that performs pixel-level contrastive learning under amultimodal prototype-guided supervision paradigm. The core innovation ofMPAMatch lies in the dual contrastive learning scheme between image prototypesand pixel labels, and between text prototypes and pixel labels, providingsupervision at both structural and semantic levels. This coarse-to-finesupervisory strategy not only enhances the discriminative capability onunlabeled samples but also introduces the text prototype supervision intosegmentation for the first time, significantly improving semantic boundarymodeling. In addition, we reconstruct the classic segmentation architecture(TransUNet) by replacing its ViT backbone with a pathology-pretrainedfoundation model (Uni), enabling more effective extraction ofpathology-relevant features. Extensive experiments on GLAS, EBHI-SEG-GLAND,EBHI-SEG-CANCER, and KPI show MPAMatch's superiority over state-of-the-artmethods, validating its dual advantages in structural and semantic modeling.</description>
      <author>example@mail.com (Mingxi Fu, Fanglei Fu, Xitong Ling, Huaitian Yuan, Tian Guan, Yonghong He, Lianghui Zhu)</author>
      <guid isPermaLink="false">2508.19574v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Language Gaps: Enhancing Few-Shot Language Adaptation</title>
      <link>http://arxiv.org/abs/2508.19464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoLAP方法通过整合对比学习和跨语言表示，解决了多语言NLP中语言资源不平衡的问题，实现了从高资源语言到低资源语言的高效知识转移，在有限数据条件下优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;语言资源不平衡是多语言NLP的挑战，高资源语言拥有大量数据，而低资源语言缺乏足够数据进行有效训练。&lt;h4&gt;目的&lt;/h4&gt;解决高资源语言和低资源语言之间的数据差距，促进任务特定知识从高资源语言向低资源语言的转移。&lt;h4&gt;方法&lt;/h4&gt;提出对比语言提示对齐(CoLAP)方法，结合对比学习和跨语言表示，实现数据高效的跨语言知识转移。&lt;h4&gt;主要发现&lt;/h4&gt;CoLAP在数据效率方面具有优势，能够快速适应新语言并减少对大型标记数据集的需求。在自然语言推理和关系抽取任务上，CoLAP优于少样本跨语言迁移基线和上下文学习方法，即使在数据有限的情况下也能有效缩小跨语言性能差距。&lt;h4&gt;结论&lt;/h4&gt;CoLAP为开发更高效的多语言NLP技术做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;语言资源的不平衡给多语言自然语言处理带来了挑战，高资源语言得益于大量数据，而低资源语言缺乏有效训练所需的足够数据。我们的对比语言提示对齐(CoLAP)方法通过整合对比学习和跨语言表示来解决这一差距，促进从高资源语言到低资源语言的任务特定知识转移。我们方法的主要优势是其数据效率，能够快速适应新语言并减少对大型标记数据集的需求。我们在自然语言理解任务(包括自然语言推理和关系提取)上对多语言仅编码器和仅解码器语言模型进行了实验，评估了在高资源和低资源语言上的性能。我们的结果表明，即使在可用数据有限的情况下，CoLAP也优于少样本跨语言迁移基线和上下文学习。这有效地缩小了跨语言性能差距，为开发更高效的多语言NLP技术做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The disparity in language resources poses a challenge in multilingual NLP,with high-resource languages benefiting from extensive data, while low-resourcelanguages lack sufficient data for effective training. Our Contrastive LanguageAlignment with Prompting (CoLAP) method addresses this gap by integratingcontrastive learning with cross-lingual representations, facilitatingtask-specific knowledge transfer from high-resource to lower-resourcelanguages. The primary advantage of our approach is its data efficiency,enabling rapid adaptation to new languages and reducing the need for largelabeled datasets. We conduct experiments with multilingual encoder-only anddecoder-only language models on natural language understanding tasks, includingnatural language inference and relation extraction, evaluating performanceacross both high- and low-resource languages. Our results demonstrate thatCoLAP outperforms few-shot cross-lingual transfer baselines and in-contextlearning, even with limited available data. This effectively narrows thecross-lingual performance gap, contributing to the development of moreefficient multilingual NLP techniques.</description>
      <author>example@mail.com (Philipp Borchert, Jochen De Weerdt, Marie-Francine Moens)</author>
      <guid isPermaLink="false">2508.19464v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>MS-ConTab: Multi-Scale Contrastive Learning of Mutation Signatures for Pan Cancer Representation and Stratification</title>
      <link>http://arxiv.org/abs/2508.19424v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的无监督对比学习框架，用于基于突变数据对43种癌症类型进行聚类，发现了与已知生物学过程一致的癌症聚类模式。&lt;h4&gt;背景&lt;/h4&gt;理解泛癌症突变景观对了解肿瘤发生的分子机制至关重要。虽然患者级别的机器学习技术已被广泛用于识别肿瘤亚型，但队列级别的癌症聚类（基于共享分子特征对整个癌症类型进行分组）主要依赖经典统计方法。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的无监督对比学习框架，基于编码突变数据对43种癌症类型进行聚类，使用来自COSMIC数据库的数据。&lt;h4&gt;方法&lt;/h4&gt;为每种癌症类型构建两种互补的突变特征：基因级特征（捕获最频繁突变基因中的核苷酸替换模式）和染色体级特征（表示染色体间归一化的替换频率）。使用TabNet编码器对这些双视图进行编码，并通过多尺度对比学习目标（NT-Xent损失）进行优化，以学习统一的癌症类型嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;生成的潜在表示产生了生物学上有意义的癌症类型聚类，这些聚类与已知的突变过程和组织来源一致。&lt;h4&gt;结论&lt;/h4&gt;这是对比学习首次应用于队列级别的癌症聚类，提供了一种可扩展且可解释的框架，用于突变驱动的癌症亚型分型。&lt;h4&gt;翻译&lt;/h4&gt;动机：理解泛癌症突变景观为理解肿瘤发生的分子机制提供了关键见解。虽然患者级别的机器学习技术已被广泛用于识别肿瘤亚型，但队列级别的聚类（基于共享分子特征对整个癌症类型进行分组）主要依赖经典统计方法。结果：在本研究中，我们引入了一种新的无监督对比学习框架，基于来自COSMIC数据库的编码突变数据对43种癌症类型进行聚类。对于每种癌症类型，我们构建了两种互补的突变特征：捕获最频繁突变基因中核苷酸替换模式的基因级特征，以及表示染色体间归一化替换频率的染色体级特征。这些双视图使用TabNet编码器进行编码，并通过多尺度对比学习目标（NT-Xent损失）进行优化，以学习统一的癌症类型嵌入。我们证明，生成的潜在表示产生了生物学上有意义的癌症类型聚类，与已知的突变过程和组织来源一致。我们的工作代表了对比学习首次应用于队列级别的癌症聚类，为突变驱动的癌症亚型分型提供了一种可扩展且可解释的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivation. Understanding the pan-cancer mutational landscape offers criticalinsights into the molecular mechanisms underlying tumorigenesis. Whilepatient-level machine learning techniques have been widely employed to identifytumor subtypes, cohort-level clustering, where entire cancer types are groupedbased on shared molecular features, has largely relied on classical statisticalmethods.  Results. In this study, we introduce a novel unsupervised contrastivelearning framework to cluster 43 cancer types based on coding mutation dataderived from the COSMIC database. For each cancer type, we construct twocomplementary mutation signatures: a gene-level profile capturing nucleotidesubstitution patterns across the most frequently mutated genes, and achromosome-level profile representing normalized substitution frequenciesacross chromosomes. These dual views are encoded using TabNet encoders andoptimized via a multi-scale contrastive learning objective (NT-Xent loss) tolearn unified cancer-type embeddings. We demonstrate that the resulting latentrepresentations yield biologically meaningful clusters of cancer types,aligning with known mutational processes and tissue origins. Our workrepresents the first application of contrastive learning to cohort-level cancerclustering, offering a scalable and interpretable framework for mutation-drivencancer subtyping.</description>
      <author>example@mail.com (Yifan Dou, Adam Khadre, Ruben C Petreaca, Golrokh Mirzaei)</author>
      <guid isPermaLink="false">2508.19424v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning</title>
      <link>http://arxiv.org/abs/2508.19009v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FedProtoKD的异构联邦学习方法，通过增强的双知识蒸馏机制和基于对比学习的可训练服务器原型来解决原型边缘缩小问题，显著提高了系统性能。&lt;h4&gt;背景&lt;/h4&gt;异构联邦学习(HFL)因能够适应不同客户端的多样模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的有前景解决方案出现，但这些方法通常在服务器上使用加权平均来聚合原型，导致次优的全局知识和原型缩小问题。&lt;h4&gt;目的&lt;/h4&gt;解决异构联邦学习中原型边缘缩小的问题，提高模型在极度非独立同分布数据下的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FedProtoKD，使用增强的双知识蒸馏机制利用客户端的logits和原型特征表示；通过基于对比学习的可训练服务器原型和类自适应原型边缘来解决原型边缘缩小问题；评估公共样本的重要性基于样本与其类代表性原型的接近程度。&lt;h4&gt;主要发现&lt;/h4&gt;FedProtoKD在各种设置下实现了平均1.13%到最高34.13%的准确率提升，显著优于现有的最先进HFL方法。&lt;h4&gt;结论&lt;/h4&gt;FedProtoKD有效解决了异构联邦学习中的原型边缘缩小问题，在模型异构且数据分布极度非独立同分布的情况下显著提高了模型性能。&lt;h4&gt;翻译&lt;/h4&gt;异构联邦学习(HFL)因其能够适应不同客户端的多样模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的有前景解决方案出现，为HFL研究的新进展铺平了道路。这种方法专注于仅在异构客户端之间共享类代表性原型。然而，这些原型通常在服务器上使用加权平均进行聚合，导致次优的全局知识；这会导致聚合原型的缩小，在模型异构且数据分布极度非独立同分布的情况下对模型性能产生负面影响。我们在异构联邦学习环境中提出了FedProtoKD，使用增强的双知识蒸馏机制来提高系统性能，利用客户端的logits和原型特征表示。我们旨在利用基于类自适应原型边缘的对比学习可训练服务器原型来解决原型边缘缩小问题。此外，我们通过评估样本与其类代表性原型的接近程度来评估公共样本的重要性，从而提高学习性能。FedProtoKD在各种设置下实现了平均1.13%到最高34.13%的准确率提升，并显著优于现有的最先进HFL方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous Federated Learning (HFL) has gained attention for its abilityto accommodate diverse models and heterogeneous data across clients.Prototype-based HFL methods emerge as a promising solution to addressstatistical heterogeneity and privacy challenges, paving the way for newadvancements in HFL research. This method focuses on sharing onlyclass-representative prototypes among heterogeneous clients. However, theseprototypes are often aggregated on the server using weighted averaging, leadingto sub-optimal global knowledge; these cause the shrinking of aggregatedprototypes, which negatively affects the model performance in scenarios whenmodels are heterogeneous and data distributions are extremely non-IID. Wepropose FedProtoKD in a Heterogeneous Federated Learning setting, using anenhanced dual-knowledge distillation mechanism to improve the systemperformance with clients' logits and prototype feature representation. We aimto resolve the prototype margin-shrinking problem using a contrastivelearning-based trainable server prototype by leveraging a class-wise adaptiveprototype margin. Furthermore, we assess the importance of public samples usingthe closeness of the sample's prototype to its class representative prototypes,which enhances learning performance. FedProtoKD achieved average improvementsof 1.13% up to 34.13% accuracy across various settings and significantlyoutperforms existing state-of-the-art HFL methods.</description>
      <author>example@mail.com (Md Anwar Hossen, Fatema Siddika, Wensheng Zhang, Anuj Sharma, Ali Jannesari)</author>
      <guid isPermaLink="false">2508.19009v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>TPA: Temporal Prompt Alignment for Fetal Congenital Heart Defect Classification</title>
      <link>http://arxiv.org/abs/2508.15298v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为时间提示对齐(TPA)的新方法，用于在超声视频中检测胎儿先天性心脏病(CHD)，通过结合时间建模、提示感知对比学习和不确定性量化，显著提高了分类准确性和校准性能。&lt;h4&gt;背景&lt;/h4&gt;先天性心脏病(CHD)在超声视频检测中受到图像噪声和探头定位可变性的影响。虽然自动化方法可以减少操作者依赖性，但当前的机器学习方法往往忽略时间信息，仅限于二元分类，且未考虑预测校准。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用时间信息、支持多分类且考虑预测校准的方法，用于胎儿心脏超声视频中的CHD检测。&lt;h4&gt;方法&lt;/h4&gt;TPA方法利用基础图像-文本模型和提示感知对比学习，通过图像编码器提取视频子片段各帧特征，使用可训练的时间提取器聚合特征以捕获心脏运动，并通过边际铰链对比损失将视频表示与类特定文本提示对齐。同时引入条件变分自编码器样式调制(CVAESM)模块学习潜在样式向量来调制嵌入并量化分类不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;TPA在私有CHD检测数据集和EchoNet-Dynamic公共数据集上均表现优异，实现了85.40%的宏观F1分数，同时将预期校准误差降低5.38%，自适应ECE降低6.8%。在EchoNet-Dynamic的三类任务中，将宏观F1提高4.73%，从53.89%提升至58.62%。&lt;h4&gt;结论&lt;/h4&gt;时间提示对齐(TPA)是一个有效的胎儿先天性心脏病(CHD)分类框架，集成了时间建模、提示感知对比学习和不确定性量化，显著提升了超声视频诊断的性能和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;先天性心脏病(CHD)在超声视频检测中受到图像噪声和探头定位可变性的阻碍。虽然自动化方法可以减少操作者依赖性，但当前的机器学习方法往往忽略时间信息，仅限于二元分类，且未考虑预测校准。我们提出了时间提示对齐(TPA)，一种利用基础图像-文本模型和提示感知对比学习的方法，用于对胎儿心脏超声视频中的CHD进行分类。TPA使用图像编码器从视频子片段的每一帧提取特征，使用可训练的时间提取器聚合它们以捕获心脏运动，并通过边际铰链对比损失将视频表示与类特定的文本提示对齐。为了提高临床可靠性的校准，我们引入了条件变分自编码器样式调制(CVAESM)模块，它学习一个潜在样式向量来调制嵌入并量化分类不确定性。在CHD检测的私有数据集和用于收缩功能障碍的大型公共数据集EchoNet-Dynamic上评估，TPA实现了CHD诊断最先进的宏观F1分数85.40%，同时将预期校准误差降低5.38%，自适应ECE降低6.8%。在EchoNet-Dynamic的三类任务中，它将宏观F1提高4.73%(从53.89%到58.62%)。时间提示对齐(TPA)是一个用于胎儿先天性心脏病(CHD)分类的框架，集成了时间建模、提示感知对比学习和不确定性量化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Congenital heart defect (CHD) detection in ultrasound videos is hindered byimage noise and probe positioning variability. While automated methods canreduce operator dependence, current machine learning approaches often neglecttemporal information, limit themselves to binary classification, and do notaccount for prediction calibration. We propose Temporal Prompt Alignment (TPA),a method leveraging foundation image-text model and prompt-aware contrastivelearning to classify fetal CHD on cardiac ultrasound videos. TPA extractsfeatures from each frame of video subclips using an image encoder, aggregatesthem with a trainable temporal extractor to capture heart motion, and alignsthe video representation with class-specific text prompts via a margin-hingecontrastive loss. To enhance calibration for clinical reliability, we introducea Conditional Variational Autoencoder Style Modulation (CVAESM) module, whichlearns a latent style vector to modulate embeddings and quantifiesclassification uncertainty. Evaluated on a private dataset for CHD detectionand on a large public dataset, EchoNet-Dynamic, for systolic dysfunction, TPAachieves state-of-the-art macro F1 scores of 85.40% for CHD diagnosis, whilealso reducing expected calibration error by 5.38% and adaptive ECE by 6.8%. OnEchoNet-Dynamic's three-class task, it boosts macro F1 by 4.73% (from 53.89% to58.62%). Temporal Prompt Alignment (TPA) is a framework for fetal congenitalheart defect (CHD) classification in ultrasound videos that integrates temporalmodeling, prompt-aware contrastive learning, and uncertainty quantification.</description>
      <author>example@mail.com (Darya Taratynova, Alya Almsouti, Beknur Kalmakhanbet, Numan Saeed, Mohammad Yaqub)</author>
      <guid isPermaLink="false">2508.15298v3</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Parameter-Free Structural-Diversity Message Passing for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了SDGNN，一种基于结构多样性的无参数图神经网络框架，通过统一的消息传递机制同时捕捉邻域结构异质性和特征语义稳定性，无需额外参数，在各种挑战条件下优于主流GNN。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在结构化数据建模任务中表现出色，但主流方法依赖大量可训练参数和固定聚合规则，难以适应强结构异质性和复杂特征分布的图数据，导致节点表示过度平滑和语义退化。&lt;h4&gt;目的&lt;/h4&gt;解决现有GNN方法在处理结构异质性和复杂特征分布时的局限性，提出一种无需额外参数的图神经网络框架。&lt;h4&gt;方法&lt;/h4&gt;提出SDGNN(结构多样性图神经网络)，受结构多样性理论启发，设计统一的多样性结构消息传递机制，同时捕捉邻域结构异质性和特征语义稳定性，不引入额外参数，利用结构驱动和特征驱动两个视角的互补建模。&lt;h4&gt;主要发现&lt;/h4&gt;在8个公共基准数据集和PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域转移等挑战条件下持续优于主流GNN方法。&lt;h4&gt;结论&lt;/h4&gt;SDGNN为无参数图神经网络设计提供了新的理论视角和通用方法，验证了结构多样性作为图表示学习中核心信号的重要性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在结构化数据建模任务中表现出色，如节点分类。然而，主流方法通常依赖大量可训练参数和固定聚合规则，难以适应具有强结构异质性和复杂特征分布的图数据。这通常导致节点表示过度平滑和语义退化。为解决这些问题，本文提出了一种基于结构多样性的无参数图神经网络框架，即SDGNN(结构多样性图神经网络)。该框架受结构多样性理论启发，设计了统一的多样性结构消息传递机制，同时捕捉邻域结构的异质性和特征语义的稳定性，无需引入额外可训练参数。与传统参数化方法不同，SDGNN不依赖复杂模型训练，而是利用结构驱动和特征驱动两个视角的互补建模，从而有效提高跨数据集和场景的适应性。实验结果表明，在8个公共基准数据集和一个跨学科的PubMed引文网络上，SDGNN在低监督、类别不平衡和跨域迁移等具有挑战性的条件下持续优于主流GNN。这项工作为无参数图神经网络的设计提供了新的理论视角和通用方法，并进一步验证了结构多样性作为图表示学习中核心信号的重要性。为便于复现和进一步研究，SDGNN的完整实现已在以下网址发布：https://github.com/mingyue15694/SGDNN/tree/main&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable performance in structureddata modeling tasks such as node classification. However, mainstream approachesgenerally rely on a large number of trainable parameters and fixed aggregationrules, making it difficult to adapt to graph data with strong structuralheterogeneity and complex feature distributions. This often leads toover-smoothing of node representations and semantic degradation. To addressthese issues, this paper proposes a parameter-free graph neural networkframework based on structural diversity, namely SDGNN (Structural-DiversityGraph Neural Network). The framework is inspired by structural diversity theoryand designs a unified structural-diversity message passing mechanism thatsimultaneously captures the heterogeneity of neighborhood structures and thestability of feature semantics, without introducing additional trainableparameters. Unlike traditional parameterized methods, SDGNN does not rely oncomplex model training, but instead leverages complementary modeling from bothstructure-driven and feature-driven perspectives, thereby effectively improvingadaptability across datasets and scenarios. Experimental results show that oneight public benchmark datasets and an interdisciplinary PubMed citationnetwork, SDGNN consistently outperforms mainstream GNNs under challengingconditions such as low supervision, class imbalance, and cross-domain transfer.This work provides a new theoretical perspective and general approach for thedesign of parameter-free graph neural networks, and further validates theimportance of structural diversity as a core signal in graph representationlearning. To facilitate reproducibility and further research, the fullimplementation of SDGNN has been released at:https://github.com/mingyue15694/SGDNN/tree/main</description>
      <author>example@mail.com (Mingyue Kong, Yinglong Zhang, Chengda Xu, Xuewen Xia, Xing Xu)</author>
      <guid isPermaLink="false">2508.19884v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Self-supervised structured object representation learning</title>
      <link>http://arxiv.org/abs/2508.19864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种自监督学习方法，通过结合语义分组、实例级分离和层次结构来逐步构建结构化的视觉表征，并在目标检测任务上取得了优于最先进方法的结果。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为学习视觉表征的强大技术，但现有的自监督方法在全局图像理解方面表现出色，却在捕捉场景中的结构化表征方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;提出一种自监督方法，能够逐步构建结构化的视觉表征，并在密集预测任务中提高性能，特别是在目标检测任务上取得更好的效果。&lt;h4&gt;方法&lt;/h4&gt;基于一种新颖的ProtoScale模块，该方法能够捕捉多个空间尺度的视觉元素。与DINO等依赖随机裁剪和全局嵌入的常见策略不同，该方法在增强视图之间保留完整的场景上下文。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法学习到了以对象为中心的表征，增强了有监督目标检测性能，并且即使在使用有限的标注数据和更少的微调周期进行训练时，也优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合语义分组、实例级分离和层次结构，并利用ProtoScale模块捕捉多尺度视觉元素，该方法在目标检测任务上取得了优异的性能，特别是在数据有限的情况下。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习（SSL）已成为学习视觉表征的强大技术。虽然最近的SSL方法在全局图像理解方面取得了很好的结果，但它们在捕捉场景中的结构化表征方面存在局限。在这项工作中，我们提出了一种自监督方法，通过结合语义分组、实例级分离和层次结构来逐步构建结构化的视觉表征。我们的方法基于一种新颖的ProtoScale模块，能够捕捉多个空间尺度的视觉元素。与DINO等依赖随机裁剪和全局嵌入的常见策略不同，我们在增强视图之间保留完整的场景上下文，以提高密集预测任务的性能。我们使用多个数据集（COCO和UA-DETRAC）的组合子集在下游目标检测任务上验证了我们的方法。实验结果表明，我们的方法学习到了以对象为中心的表征，增强了有监督目标检测性能，并且即使在使用有限的标注数据和更少的微调周期进行训练时，也优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful technique forlearning visual representations. While recent SSL approaches achieve strongresults in global image understanding, they are limited in capturing thestructured representation in scenes. In this work, we propose a self-supervisedapproach that progressively builds structured visual representations bycombining semantic grouping, instance level separation, and hierarchicalstructuring. Our approach, based on a novel ProtoScale module, captures visualelements across multiple spatial scales. Unlike common strategies like DINOthat rely on random cropping and global embeddings, we preserve full scenecontext across augmented views to improve performance in dense predictiontasks. We validate our method on downstream object detection tasks using acombined subset of multiple datasets (COCO and UA-DETRAC). Experimental resultsshow that our method learns object centric representations that enhancesupervised object detection and outperform the state-of-the-art methods, evenwhen trained with limited annotated data and fewer fine-tuning epochs.</description>
      <author>example@mail.com (Oussama Hadjerci, Antoine Letienne, Mohamed Abbas Hedjazi, Adel Hafiane)</author>
      <guid isPermaLink="false">2508.19864v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Improving Generalization in Deepfake Detection with Face Foundation Models and Metric Learning</title>
      <link>http://arxiv.org/abs/2508.19730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于面部基础模型的鲁棒视频深度伪造检测框架，通过利用自监督模型和多种深度伪造数据集的微调，结合三元组损失和基于归因的监督方案，显著提升了模型在现实世界场景中的检测能力。&lt;h4&gt;背景&lt;/h4&gt;随着深度伪造技术的日益逼真和普及，媒体真实性和信息完整性面临严重挑战。现有的深度伪造检测模型往往难以在其训练分布之外泛化，特别是在应用于现实世界中的媒体内容时。&lt;h4&gt;目的&lt;/h4&gt;开发一个具有强泛化能力的鲁棒视频深度伪造检测框架，能够在现实世界场景中有效识别深度伪造内容。&lt;h4&gt;方法&lt;/h4&gt;基于FSFM（一种在真实面部数据上训练的自监督模型）构建，使用包含面部交换和面部重演操作的深度伪造数据集集合进行微调；训练中融入三元组损失的变体，引导模型在真实和伪造样本之间产生更具区分性的嵌入；探索基于归因的监督方案，将深度伪造按操作类型或源数据集分类，以评估它们对泛化的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的评估基准上的广泛实验证明了该方法的有效性，特别是在具有挑战性的现实世界场景中表现优异。&lt;h4&gt;结论&lt;/h4&gt;提出的方法通过结合面部基础模型的优势、多数据集微调和创新的训练策略，显著提升了深度伪造检测的泛化能力，为应对日益复杂的深度伪造威胁提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;随着深度伪造技术的日益逼真和普及，媒体真实性和信息完整性引起了严重关切。尽管最近有所进展，但深度伪造检测模型往往难以在其训练分布之外泛化，特别是当应用于现实世界中发现的媒体内容时。在这项工作中，我们提出了一个具有强泛化能力的鲁棒视频深度伪造检测框架，利用了面部基础模型学习到的丰富面部表示。我们的方法建立在FSFM之上，这是一个在真实面部数据上训练的自监督模型，并使用包含面部交换和面部重演操作的深度伪造数据集集合进行了进一步微调。为了增强判别能力，我们在训练中融入了三元组损失的变体，引导模型在真实和伪造样本之间产生更具区分性的嵌入。此外，我们探索了基于归因的监督方案，其中深度伪造按操作类型或源数据集分类，以评估它们对泛化的影响。在多样化评估基准上的广泛实验证明了我们方法的有效性，特别是在具有挑战性的现实世界场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746275.3762208&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing realism and accessibility of deepfakes have raised criticalconcerns about media authenticity and information integrity. Despite recentadvances, deepfake detection models often struggle to generalize beyond theirtraining distributions, particularly when applied to media content found in thewild. In this work, we present a robust video deepfake detection framework withstrong generalization that takes advantage of the rich facial representationslearned by face foundation models. Our method is built on top of FSFM, aself-supervised model trained on real face data, and is further fine-tunedusing an ensemble of deepfake datasets spanning both face-swapping andface-reenactment manipulations. To enhance discriminative power, we incorporatetriplet loss variants during training, guiding the model to produce moreseparable embeddings between real and fake samples. Additionally, we exploreattribution-based supervision schemes, where deepfakes are categorized bymanipulation type or source dataset, to assess their impact on generalization.Extensive experiments across diverse evaluation benchmarks demonstrate theeffectiveness of our approach, especially in challenging real-world scenarios.</description>
      <author>example@mail.com (Stelios Mylonas, Symeon Papadopoulos)</author>
      <guid isPermaLink="false">2508.19730v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>A Model-agnostic Strategy to Mitigate Embedding Degradation in Personalized Federated Recommendation</title>
      <link>http://arxiv.org/abs/2508.19591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为个性化局部-全局协作(PLGC)的新策略，用于解决联邦推荐系统中的嵌入退化问题，特别是维度坍缩问题。&lt;h4&gt;背景&lt;/h4&gt;中心化推荐系统因收集用户隐私数据而面临隐私泄露问题，联邦推荐系统(FedRec)成为一种有前景的替代方案，但存在嵌入退化问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种模型无关的FedRec策略，加强个性化嵌入效用，并首次解决联邦推荐中的维度坍缩问题。&lt;h4&gt;方法&lt;/h4&gt;PLGC将冻结的全局项目嵌入表纳入本地设备，基于神经正切核策略动态平衡局部和全局信息，在前向推理过程中优化个性化表示，并通过对比目标函数减少嵌入冗余。&lt;h4&gt;主要发现&lt;/h4&gt;PLGC可有效缓解嵌入退化问题，在五个真实世界数据集上的实验证明其有效性和适应性，优于各种基线算法。&lt;h4&gt;结论&lt;/h4&gt;PLGC是一种模型无关的个性化训练策略，可应用于现有基线以改善联邦推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;中心化推荐系统因需要收集用户行为和其他私人数据而面临隐私泄露问题。因此，联邦推荐系统(FedRec)已成为一种有前景的方法，服务器上有一个聚合的全局模型。然而，由于稀疏交互和异质偏好的存在，这种分布式训练范式因次优个性化和维度坍缩而导致嵌入退化。为此，我们提出了一种新颖的FedRec模型无关策略来增强个性化嵌入效用，称为个性化局部-全局协作(PLGC)。这是联邦推荐领域首次研究缓解维度坍缩问题的工作。特别是，我们将冻结的全局项目嵌入表纳入本地设备。基于动态平衡局部和全局信息的神经正切核策略，PLGC在前向推理过程中优化个性化表示，最终收敛到用户特定的偏好。此外，PLGC采用对比目标函数通过消除维度间的依赖关系来减少嵌入冗余，从而改进后向表示学习过程。我们引入PLGC作为联邦推荐的模型无关个性化训练策略，可应用于现有基线以缓解嵌入退化。在五个真实世界数据集上的广泛实验证明了PLGC的有效性和适应性，其性能优于各种基线算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Centralized recommender systems encounter privacy leakage due to the need tocollect user behavior and other private data. Hence, federated recommendersystems (FedRec) have become a promising approach with an aggregated globalmodel on the server. However, this distributed training paradigm suffers fromembedding degradation caused by suboptimal personalization and dimensionalcollapse, due to the existence of sparse interactions and heterogeneouspreferences. To this end, we propose a novel model-agnostic strategy for FedRecto strengthen the personalized embedding utility, which is called PersonalizedLocal-Global Collaboration (PLGC). It is the first research in federatedrecommendation to alleviate the dimensional collapse issue. Particularly, weincorporate the frozen global item embedding table into local devices. Based ona Neural Tangent Kernel strategy that dynamically balances local and globalinformation, PLGC optimizes personalized representations during forwardinference, ultimately converging to user-specific preferences. Additionally,PLGC carries on a contrastive objective function to reduce embedding redundancyby dissolving dependencies between dimensions, thereby improving the backwardrepresentation learning process. We introduce PLGC as a model-agnosticpersonalized training strategy for federated recommendations that can beapplied to existing baselines to alleviate embedding degradation. Extensiveexperiments on five real-world datasets have demonstrated the effectiveness andadaptability of PLGC, which outperforms various baseline algorithms.</description>
      <author>example@mail.com (Jiakui Shen, Yunqi Mi, Guoshuai Zhao, Jialie Shen, Xueming Qian)</author>
      <guid isPermaLink="false">2508.19591v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Counterfactual Reward Model Training for Bias Mitigation in Multimodal Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2508.19567v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于反事实推理的奖励模型，用于解决强化学习与人类反馈(RLHF)中的偏见问题。该模型通过因果推断和多模态表征学习提供无监督的、具有偏见韧性的奖励信号，并在真假新闻检测任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;在基于人类反馈的强化学习(RLHF)中，奖励模型能够高效学习和放大多模态数据集中的潜在偏见，这可能导致通过有缺陷的奖励信号进行不完美的策略优化，并降低公平性。现有的偏见缓解研究通常采用被动约束，这些约束在因果混淆的情况下可能会失效。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效减轻多模态数据集中偏见的奖励模型，提供无监督的、具有偏见韧性的奖励信号，以改进RLHF中的策略优化和公平性。&lt;h4&gt;方法&lt;/h4&gt;提出反事实奖励模型，核心是反事实信任评分，包含四个组成部分：(1)反事实偏移，将政治框架偏见与主题偏见分离；(2)反事实扰动期间的重构不确定性；(3)每个受保护属性的可证明的公平性规则违反；(4)与动态信任度量对齐的临时奖励偏移。在多模态真假新闻数据集上进行评估，并注入合成偏见测试鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;该系统在假新闻检测中达到了89.12%的准确率，优于基线奖励模型。更重要的是，它减少了虚假相关性和不公平的强化信号，提供了可调节的偏见减少阈值，增强了动态实时决策的可靠性。&lt;h4&gt;结论&lt;/h4&gt;该反事实奖励模型框架为公平感知的RLHF提供了一种稳健且可解释的方法，能够有效减轻偏见，提高决策的可靠性和公平性。&lt;h4&gt;翻译&lt;/h4&gt;在基于人类反馈的强化学习(RLHF)中，奖励模型能够高效学习和放大多模态数据集中的潜在偏见，这可能导致通过有缺陷的奖励信号进行不完美的策略优化，并降低公平性。偏见缓解研究通常采用被动约束，这些约束在因果混淆的情况下可能会失效。在此，我们提出了一种反事实奖励模型，引入因果推断与多模态表征学习，提供一种无监督的、具有偏见韧性的奖励信号。我们贡献的核心是反事实信任评分，一个包含四个组成部分的聚合评分：(1)将政治框架偏见与主题偏见分离的反事实偏移；(2)反事实扰动期间的重构不确定性；(3)每个受保护属性的可证明的公平性规则违反；(4)与动态信任度量对齐的临时奖励偏移。我们在一个展示框架偏见、类别不平衡和分布漂移的多模态真假新闻数据集上评估了该框架。遵循基于表示距离的无监督漂移检测[1]和语言模型中的时间鲁棒性基准测试[2]的类似方法，我们还注入了跨连续批次的合成偏见以测试鲁棒性。 resulting system achieved an accuracy of 89.12% in fake news detection, outperforming the baseline reward models. More importantly, it reduced spurious correlations and unfair reinforcement signals. This pipeline outlines a robust and interpretable approach to fairness-aware RLHF, offering tunable bias reduction thresholds and increasing reliability in dynamic real-time policy making.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In reinforcement learning with human feedback (RLHF), reward models canefficiently learn and amplify latent biases within multimodal datasets, whichcan lead to imperfect policy optimization through flawed reward signals anddecreased fairness. Bias mitigation studies have often applied passiveconstraints, which can fail under causal confounding. Here, we present acounterfactual reward model that introduces causal inference with multimodalrepresentation learning to provide an unsupervised, bias-resilient rewardsignal. The heart of our contribution is the Counterfactual Trust Score, anaggregated score consisting of four components: (1) counterfactual shifts thatdecompose political framing bias from topical bias; (2) reconstructionuncertainty during counterfactual perturbations; (3) demonstrable violations offairness rules for each protected attribute; and (4) temporal reward shiftsaligned with dynamic trust measures. We evaluated the framework on a multimodalfake versus true news dataset, which exhibits framing bias, class imbalance,and distributional drift. Following methodologies similar to unsupervised driftdetection from representation-based distances [1] and temporal robustnessbenchmarking in language models [2], we also inject synthetic bias acrosssequential batches to test robustness. The resulting system achieved anaccuracy of 89.12% in fake news detection, outperforming the baseline rewardmodels. More importantly, it reduced spurious correlations and unfairreinforcement signals. This pipeline outlines a robust and interpretableapproach to fairness-aware RLHF, offering tunable bias reduction thresholds andincreasing reliability in dynamic real-time policy making.</description>
      <author>example@mail.com (Sheryl Mathew, N Harshit)</author>
      <guid isPermaLink="false">2508.19567v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Improving Recommendation Fairness via Graph Structure and Representation Augmentation</title>
      <link>http://arxiv.org/abs/2508.19547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于数据增强的公平推荐方法，旨在解决图卷积网络(GCN)在推荐系统中导致的敏感信息传播和数据偏差问题。&lt;h4&gt;背景&lt;/h4&gt;图卷积网络(GCN)在推荐系统中广泛应用，但会导致敏感信息在图结构中广泛传播，放大数据偏差并引发公平性问题。现有公平性方法大多忽略了偏差数据对表征学习的影响，而数据增强方法虽能构建公平数据分布但会破坏用户偏好，降低推荐效用。&lt;h4&gt;目的&lt;/h4&gt;从数据增强的角度设计公平的推荐方法，在保持推荐效用的同时提高公平性。&lt;h4&gt;方法&lt;/h4&gt;提出两个先验假设：一是通过比较性能导向和公平导向推荐的结果来识别敏感交互；二是通过分析偏差和去偏差表征之间的特征相似性来检测敏感特征。基于此，提出双重数据增强框架，包括生成公平增强图和特征表征的两种策略，并引入去偏差学习方法以最小化学习表征与敏感信息间的依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实数据集上的大量实验证明了所提出框架的优越性。&lt;h4&gt;结论&lt;/h4&gt;该研究通过双重数据增强框架和去偏差学习方法，有效提高了推荐系统的公平性同时保持了推荐效用。&lt;h4&gt;翻译&lt;/h4&gt;图卷积网络(GCN)在推荐系统中变得越来越流行。然而，最近研究表明，基于GCN的模型会导致敏感信息在图结构中广泛传播，放大数据偏差并引发公平性问题。虽然已经提出了各种公平性方法，但大多数方法忽略了偏差数据对表征学习的影响，导致公平性改善有限。此外，一些研究通过数据增强来构建公平和平衡的数据分布，但这些方法由于破坏用户偏好而显著降低了效用。本文旨在从数据增强的角度设计公平的推荐方法，以提高公平性同时保持推荐效用。为了以最小干扰用户偏好的方式进行公平感知的数据增强，我们提出了两个先验假设。第一个假设通过比较性能导向和公平导向推荐的结果来识别敏感交互，而第二个假设通过分析偏差和去偏差表征之间的特征相似性来检测敏感特征。然后，我们提出了一个公平推荐的双重数据增强框架，包括两种数据增强策略来生成公平的增强图和特征表征。此外，我们引入了一种去偏差学习方法，最小化学习表征与敏感信息之间的依赖性以消除偏差。在两个真实数据集上的大量实验证明了我们提出框架的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3761004&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Convolutional Networks (GCNs) have become increasingly popular inrecommendation systems. However, recent studies have shown that GCN-basedmodels will cause sensitive information to disseminate widely in the graphstructure, amplifying data bias and raising fairness concerns. While variousfairness methods have been proposed, most of them neglect the impact of biaseddata on representation learning, which results in limited fairness improvement.Moreover, some studies have focused on constructing fair and balanced datadistributions through data augmentation, but these methods significantly reduceutility due to disruption of user preferences. In this paper, we aim to designa fair recommendation method from the perspective of data augmentation toimprove fairness while preserving recommendation utility. To achievefairness-aware data augmentation with minimal disruption to user preferences,we propose two prior hypotheses. The first hypothesis identifies sensitiveinteractions by comparing outcomes of performance-oriented and fairness-awarerecommendations, while the second one focuses on detecting sensitive featuresby analyzing feature similarities between biased and debiased representations.Then, we propose a dual data augmentation framework for fair recommendation,which includes two data augmentation strategies to generate fair augmentedgraphs and feature representations. Furthermore, we introduce a debiasinglearning method that minimizes the dependence between the learnedrepresentations and sensitive information to eliminate bias. Extensiveexperiments on two real-world datasets demonstrate the superiority of ourproposed framework.</description>
      <author>example@mail.com (Tongxin Xu, Wenqiang Liu, Chenzhong Bin, Cihan Xiao, Zhixin Zeng, Tianlong Gu)</author>
      <guid isPermaLink="false">2508.19547v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>UNIFORM: Unifying Knowledge from Large-scale and Diverse Pre-trained Models</title>
      <link>http://arxiv.org/abs/2508.19498v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UNIFORM是一个新颖的知识转移框架，能够从多样化的预训练模型向单一学生模型有效传递知识，无需对训练数据分布和网络架构做出强假设。&lt;h4&gt;背景&lt;/h4&gt;在深度学习时代，网络上可用的预训练模型数量不断增加，这些模型具有不同的架构和训练数据，提供了对现实世界的独特解释，它们的集体共识可能是通用的并可推广到未见数据。&lt;h4&gt;目的&lt;/h4&gt;解决如何有效利用异构预训练模型的集体知识这一基本挑战，而不依赖于对训练数据分布和网络架构的强假设。&lt;h4&gt;方法&lt;/h4&gt;UNIFORM框架采用专门的投票机制，在logit级别（包括能够预测目标类别的教师模型）和特征级别（利用在任意标签空间上学习到的视觉表示）捕获知识的共识。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，UNIFORM与强大的知识迁移基线相比，有效提高了无监督目标识别性能，并且表现出卓越的可扩展性，能够从一百多名教师中受益。&lt;h4&gt;结论&lt;/h4&gt;UNIFORM框架成功解决了从多样化预训练模型中整合知识的问题，无需传统方法所需的强假设，且在大规模教师模型的情况下表现优越。&lt;h4&gt;翻译&lt;/h4&gt;在深度学习时代，网络上可用的预训练模型数量不断增加，提供了丰富的知识。这些模型具有不同的架构，在不同的数据集上训练以完成不同任务，为现实世界提供了独特的解释。它们的集体共识可能是通用的并可推广到未见数据。然而，由于预训练模型的异构性，有效利用这种集体知识面临基本挑战。现有的知识集成解决方案通常依赖于对训练数据分布和网络架构的强假设，限制了它们只能从特定类型的模型中学习，并导致数据和/或归纳偏差。在这项工作中，我们引入了一个名为UNIFORM的新颖框架，用于从多样化的现成模型向一个学生模型进行知识转移，没有这些限制。具体而言，我们提出了一种专门的投票机制，在logit级别捕获知识——包括能够预测目标类别的教师模型——和在特征级别，利用在任意标签空间上学习到的视觉表示。大量实验表明，UNIFORM与强大的知识迁移基线相比，有效提高了无监督目标识别性能。值得注意的是，它通过受益于一百多名教师而表现出卓越的可扩展性，而现有方法在较小规模时就趋于饱和。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of deep learning, the increasing number of pre-trained modelsavailable online presents a wealth of knowledge. These models, developed withdiverse architectures and trained on varied datasets for different tasks,provide unique interpretations of the real world. Their collective consensus islikely universal and generalizable to unseen data. However, effectivelyharnessing this collective knowledge poses a fundamental challenge due to theheterogeneity of pre-trained models. Existing knowledge integration solutionstypically rely on strong assumptions about training data distributions andnetwork architectures, limiting them to learning only from specific types ofmodels and resulting in data and/or inductive biases. In this work, weintroduce a novel framework, namely UNIFORM, for knowledge transfer from adiverse set of off-the-shelf models into one student model without suchconstraints. Specifically, we propose a dedicated voting mechanism to capturethe consensus of knowledge both at the logit level -- incorporating teachermodels that are capable of predicting target classes of interest -- and at thefeature level, utilizing visual representations learned on arbitrary labelspaces. Extensive experiments demonstrate that UNIFORM effectively enhancesunsupervised object recognition performance compared to strong knowledgetransfer baselines. Notably, it exhibits remarkable scalability by benefitingfrom over one hundred teachers, while existing methods saturate at a muchsmaller scale.</description>
      <author>example@mail.com (Yimu Wang, Weiming Zhuang, Chen Chen, Jiabo Huang, Jingtao Li, Lingjuan Lyu)</author>
      <guid isPermaLink="false">2508.19498v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>CITADEL: Continual Anomaly Detection for Enhanced Learning in IoT Intrusion Detection</title>
      <link>http://arxiv.org/abs/2508.19450v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review at IEEE IoTJ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CITADEL是一种自监督持续学习框架，用于物联网入侵检测，能有效提取良性数据表示并保留长期知识，在动态环境中表现出色。&lt;h4&gt;背景&lt;/h4&gt;物联网因其高度互连性和有限计算资源易受网络威胁，基于机器学习的入侵检测系统虽前景广阔，但对新兴威胁适应性差且存在灾难性遗忘问题。&lt;h4&gt;目的&lt;/h4&gt;开发CITADEL框架，从良性数据中提取鲁棒表示，通过优化记忆巩固机制保留长期知识，解决ML-IDS的适应性和遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;CITADEL整合表格到图像转换模块、内存感知掩码自编码器用于自监督表示学习，以及无需标记攻击数据的新颖性检测组件。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明CITADEL在多个入侵数据集上，关键检测和保留指标比VLAD提高高达72.9%，能逐步适应新兴行为同时保持检测先前威胁的能力。&lt;h4&gt;结论&lt;/h4&gt;CITADEL在动态物联网环境中能有效检测入侵并保留知识，显著提升物联网安全性。&lt;h4&gt;翻译&lt;/h4&gt;物联网(IoT)由于其高度互连性和有限的计算资源，特别容易受到各种网络威胁的攻击。入侵检测系统(IDS)已被广泛研究以增强物联网安全，基于机器学习的入侵检测系统(ML-IDS)在检测恶意活动方面显示出巨大潜力。然而，它们的有效性通常受到对新兴威胁适应性差和持续学习过程中灾难性遗忘问题的限制。为应对这些挑战，我们提出了CITADEL，一种自监督持续学习框架，旨在从良性数据中提取鲁棒表示，并通过优化的记忆巩固机制保留长期知识。CITADEL集成了表格到图像转换模块、用于自监督表示学习的内存感知掩码自编码器，以及一个能够识别异常而不依赖于标记攻击数据的新颖性检测组件。我们的设计使系统能够逐步适应新兴行为，同时保持检测先前观察到的威胁的能力。在多个入侵数据集上的实验表明，CITADEL在关键的检测和保留指标上比基于VAE的终身异常检测器(VLAD)提高了高达72.9%，突显了其在动态物联网环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Internet of Things (IoT), with its high degree of interconnectivity andlimited computational resources, is particularly vulnerable to a wide range ofcyber threats. Intrusion detection systems (IDS) have been extensively studiedto enhance IoT security, and machine learning-based IDS (ML-IDS) showconsiderable promise for detecting malicious activity. However, theireffectiveness is often constrained by poor adaptability to emerging threats andthe issue of catastrophic forgetting during continuous learning. To addressthese challenges, we propose CITADEL, a self-supervised continual learningframework designed to extract robust representations from benign data whilepreserving long-term knowledge through optimized memory consolidationmechanisms. CITADEL integrates a tabular-to-image transformation module, amemory-aware masked autoencoder for self-supervised representation learning,and a novelty detection component capable of identifying anomalies withoutdependence on labeled attack data. Our design enables the system toincrementally adapt to emerging behaviors while retaining its ability to detectpreviously observed threats. Experiments on multiple intrusion datasetsdemonstrate that CITADEL achieves up to a 72.9% improvement over the VAE-basedlifelong anomaly detector (VLAD) in key detection and retention metrics,highlighting its effectiveness in dynamic IoT environments.</description>
      <author>example@mail.com (Elvin Li, Onat Gungor, Zhengli Shang, Tajana Rosing)</author>
      <guid isPermaLink="false">2508.19450v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Who Is Lagging Behind: Profiling Student Behaviors with Graph-Level Encoding in Curriculum-Based Online Learning Systems</title>
      <link>http://arxiv.org/abs/2508.18925v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为CTGraph的图级别表示学习方法，用于以自监督方式对学习者行为和表现进行档案记录，以解决智能辅导系统可能扩大表现差距的问题，并帮助识别有困难的学生。&lt;h4&gt;背景&lt;/h4&gt;智能辅导系统在教育中的采用激增，这些系统虽然是基于课程学习的必要组成部分，但可能会无意中扩大学生之间的表现差距。学生档案记录对于跟踪进度、识别有困难的学生和减轻学生差异至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决智能辅导系统可能扩大表现差距的问题，通过学生档案记录来跟踪进度、识别有困难的学生和减轻学生差异。&lt;h4&gt;方法&lt;/h4&gt;引入CTGraph，一种图级别的表示学习方法，以自监督方式对学习者的行为和表现进行档案记录。测量学生在不同方面的行为和表现，如内容覆盖、学习强度和学习主题内不同概念的熟练程度。&lt;h4&gt;主要发现&lt;/h4&gt;CTGraph可以提供学生整个学习旅程的全景视图，考虑了学生行为和表现的不同方面以及与课程结构对齐的学习路径变化。该方法能够识别有困难的学生，并提供不同群体的比较分析，以确定学生在何时何地遇到困难。&lt;h4&gt;结论&lt;/h4&gt;该方法为教育工作者提供了更丰富的学生学习洞见，为更有针对性的干预措施铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;智能辅导系统在教育中的采用激增，虽然是基于课程学习的必要组成部分，但可能会无意中扩大表现差距。为了解决这个问题，学生档案记录对于跟踪进度、识别有困难的学生和减轻学生之间的差异变得至关重要。这种档案记录需要测量学生在不同方面的行为和表现，如内容覆盖、学习强度和学习主题内不同概念的熟练程度。在本研究中，我们引入了CTGraph，一种图级别的表示学习方法，以自监督方式对学习者的行为和表现进行档案记录。我们的实验证明，CTGraph可以提供学生整个学习旅程的全景视图，考虑学生行为和表现的不同方面，以及与课程结构对齐的学习路径变化。我们还表明，我们的方法可以识别有困难的学生，并提供不同群体的比较分析，以确定学生在何时何地遇到困难。因此，我们的方法为教育工作者提供了丰富的学生学习洞见，为更有针对性的干预措施铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The surge in the adoption of Intelligent Tutoring Systems (ITSs) ineducation, while being integral to curriculum-based learning, can inadvertentlyexacerbate performance gaps. To address this problem, student profiling becomescrucial for tracking progress, identifying struggling students, and alleviatingdisparities among students. Such profiling requires measuring student behaviorsand performance across different aspects, such as content coverage, learningintensity, and proficiency in different concepts within a learning topic.  In this study, we introduce CTGraph, a graph-level representation learningapproach to profile learner behaviors and performance in a self-supervisedmanner. Our experiments demonstrate that CTGraph can provide a holistic view ofstudent learning journeys, accounting for different aspects of studentbehaviors and performance, as well as variations in their learning paths asaligned to the curriculum structure. We also show that our approach canidentify struggling students and provide comparative analysis of diverse groupsto pinpoint when and where students are struggling. As such, our approach opensmore opportunities to empower educators with rich insights into studentlearning journeys and paves the way for more targeted interventions.</description>
      <author>example@mail.com (Qian Xiao, Conn Breathnach, Ioana Ghergulescu, Conor O'Sullivan, Keith Johnston, Vincent Wade)</author>
      <guid isPermaLink="false">2508.18925v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Geo2Vec: Shape- and Distance-Aware Neural Representation of Geospatial Entities</title>
      <link>http://arxiv.org/abs/2508.19305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Geo2Vec是一种创新的地理实体空间表征学习方法，直接在原始空间操作，通过自适应采样和编码符号距离来捕获几何信息，无需分解实体，实现了更高的效率和更好的性能。&lt;h4&gt;背景&lt;/h4&gt;空间表征学习对GeoAI应用如城市分析至关重要，可编码地理实体的形状、位置和空间关系。现有方法要么针对单一实体类型，要么将实体分解为简单组件实现傅里叶变换，导致高计算成本，且变换空间缺乏几何对齐，模糊了边缘和边界等细粒度特征。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，引入一种受有距离场(SDF)启发的Geo2Vec方法，在原始空间直接操作，自适应采样点并编码符号距离，产生紧凑、几何感知、统一的表征，适用于所有地理实体类型，并提出旋转不变的位置编码来建模高频空间变化。&lt;h4&gt;方法&lt;/h4&gt;Geo2Vec方法直接在原始空间中操作，自适应采样点并编码它们的符号距离(外部为正，内部为负)，无需分解即可捕获几何信息。使用神经网络训练近似SDF，产生紧凑、几何感知的表征。同时提出旋转不变的位置编码来建模高频空间变化，构建结构化和鲁棒的嵌入空间。&lt;h4&gt;主要发现&lt;/h4&gt;Geo2Vec在表示形状和位置方面始终优于现有方法，在捕获拓扑和距离关系方面表现更好，在实际GeoAI应用中实现了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;Geo2Vec有效解决了现有空间表征学习的局限性，能够更好地表示地理实体的几何和空间特征，为GeoAI应用提供了更高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;空间表征学习对于城市分析等GeoAI应用至关重要，能够编码点、线和多边形等地理实体的形状、位置和空间关系（拓扑关系和基于距离的关系）。现有方法要么针对单一地理实体类型，要么像Poly2Vec那样将实体分解为更简单的组件以实现傅里叶变换，引入了高计算成本。此外，由于变换空间缺乏几何对齐，这些方法依赖于均匀的、非自适应的采样，这会模糊边缘和边界等细粒度特征。为解决这些局限性，我们引入了Geo2Vec，一种受有距离场(SDF)启发的新方法，直接在原始空间中操作。Geo2Vec自适应采样点并编码它们的符号距离（外部为正，内部为负），无需分解即可捕获几何信息。训练用于近似SDF的神经网络可产生紧凑的、几何感知的且适用于所有地理实体类型的统一表征。此外，我们提出了旋转不变的位置编码来建模高频空间变化，为下游GeoAI模型构建结构化和鲁棒的嵌入空间。实验结果表明，Geo2Vec在表示形状和位置、捕获拓扑和距离关系方面始终优于现有方法，并在实际GeoAI应用中实现更高的效率。代码和数据可在以下网址找到：https://github.com/chuchen2017/GeoNeuralRepresentation。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial representation learning is essential for GeoAI applications such asurban analytics, enabling the encoding of shapes, locations, and spatialrelationships (topological and distance-based) of geo-entities like points,polylines, and polygons. Existing methods either target a single geo-entitytype or, like Poly2Vec, decompose entities into simpler components to enableFourier transformation, introducing high computational cost. Moreover, sincethe transformed space lacks geometric alignment, these methods rely on uniform,non-adaptive sampling, which blurs fine-grained features like edges andboundaries. To address these limitations, we introduce Geo2Vec, a novel methodinspired by signed distance fields (SDF) that operates directly in the originalspace. Geo2Vec adaptively samples points and encodes their signed distances(positive outside, negative inside), capturing geometry without decomposition.A neural network trained to approximate the SDF produces compact,geometry-aware, and unified representations for all geo-entity types.Additionally, we propose a rotation-invariant positional encoding to modelhigh-frequency spatial variations and construct a structured and robustembedding space for downstream GeoAI models. Empirical results show thatGeo2Vec consistently outperforms existing methods in representing shape andlocation, capturing topological and distance relationships, and achievinggreater efficiency in real-world GeoAI applications. Code and Data can be foundat: https://github.com/chuchen2017/GeoNeuralRepresentation.</description>
      <author>example@mail.com (Chen Chu, Cyrus Shahabi)</author>
      <guid isPermaLink="false">2508.19305v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</title>
      <link>http://arxiv.org/abs/2508.18166v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PCR-CA框架，解决现代应用商店推荐系统在处理多类别应用时的语义捕捉问题，通过并行码本表示和对比对齐技术提升CTR预测效果。&lt;h4&gt;背景&lt;/h4&gt;现代应用商店推荐系统在处理多类别应用时面临挑战，因为传统分类法无法捕捉重叠的语义，导致个性化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出PCR-CA（并行码本表示与对比对齐）框架，改进应用商店的CTR预测效果，特别是对于多类别应用和长尾应用。&lt;h4&gt;方法&lt;/h4&gt;PCR-CA框架首先从应用文本提取多模态嵌入，然后引入并行码本VQ-AE模块学习多码本上的离散语义表示，使用对比对齐损失桥接语义和协同信号，并通过双注意力融合机制结合ID和语义特征捕捉用户兴趣。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示PCR-CA相比基线实现+0.76%的AUC提升，长尾应用的AUC提升达+2.15%；在线A/B测试显示CTR提升+10.52%，CVR提升+16.30%。&lt;h4&gt;结论&lt;/h4&gt;PCR-CA框架在真实世界部署中有效，现已完全部署在Microsoft Store上。&lt;h4&gt;翻译&lt;/h4&gt;现代应用商店推荐系统在处理多类别应用时面临挑战，因为传统分类法无法捕捉重叠的语义，导致个性化效果不佳。我们提出了PCR-CA（并行码本表示与对比对齐），一个用于改进CTR预测的端到端框架。PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入一个并行码本VQ-AE模块，该模块在多个码本上并行学习离散语义表示——这与分层残差量化（RQ-VAE）不同。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。为了桥接语义和协同信号，我们在用户和项目层面都采用了对比对齐损失，增强长尾项目的表示学习。此外，双注意力融合机制结合基于ID和语义的特征来捕捉用户兴趣，特别是对于长尾应用。在大规模数据集上的实验显示，PCR-CA相比强基线实现了+0.76%的AUC改进，长尾应用的AUC提升达到+2.15%。在线A/B测试进一步验证了我们的方法，显示CTR提升+10.52%，CVR提升+16.30%，证明了PCR-CA在真实世界部署中的有效性。该新框架现已完全部署在Microsoft Store上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern app store recommender systems struggle with multiple-category apps, astraditional taxonomies fail to capture overlapping semantics, leading tosuboptimal personalization. We propose PCR-CA (Parallel CodebookRepresentations with Contrastive Alignment), an end-to-end framework forimproved CTR prediction. PCR-CA first extracts compact multimodal embeddingsfrom app text, then introduces a Parallel Codebook VQ-AE module that learnsdiscrete semantic representations across multiple codebooks in parallel --unlike hierarchical residual quantization (RQ-VAE). This design enablesindependent encoding of diverse aspects (e.g., gameplay, art style), bettermodeling multiple-category semantics. To bridge semantic and collaborativesignals, we employ a contrastive alignment loss at both the user and itemlevels, enhancing representation learning for long-tail items. Additionally, adual-attention fusion mechanism combines ID-based and semantic features tocapture user interests, especially for long-tail apps. Experiments on alarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strongbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing furthervalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvementin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The newframework has now been fully deployed on the Microsoft Store.</description>
      <author>example@mail.com (Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang)</author>
      <guid isPermaLink="false">2508.18166v3</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Question &amp; Answer Generation Using Generative Large Language Model (LLM)</title>
      <link>http://arxiv.org/abs/2508.19475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于微调生成式大型语言模型的自动问答生成系统，旨在帮助教育工作者高效创建多样化的评估题目，减轻教师手动出题的负担，优化教育评估流程。&lt;h4&gt;背景&lt;/h4&gt;在教育领域中，学生评估与知识传授同等重要。传统上，教师需要手动从多种教材中创建多样化的题目，这对教师来说是一个挑战，需要耗费大量时间和精力。&lt;h4&gt;目的&lt;/h4&gt;开发一种自动问答生成系统，使教师能够轻松创建不同风格的问题（如选择题、概念题或事实题），从而简化评估过程，节省时间和资源。&lt;h4&gt;方法&lt;/h4&gt;使用微调生成式大型语言模型（Meta-Llama 2-7B）作为基础模型，采用提示工程技术调整问题风格，利用无监督学习方法，并使用RACE数据集进行模型训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过微调和提示工程，系统能够根据教师偏好生成不同类型的问题；基于Meta-Llama 2-7B模型的定制化解决方案能够有效满足教育工作者在文本评估方面的需求。&lt;h4&gt;结论&lt;/h4&gt;自动问答生成系统是一个可靠且高效的工具，能够帮助教育工作者节省宝贵的时间和资源，简化评估流程，提高教育评估的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;在教育领域，学生评估与知识传授同等重要。要接受评估，学生通常需要通过基于文本的学术评估方法。教师需要创建多样化的题目集，这些题目对所有学生都应该是公平的，以证明他们在特定主题上的充分掌握。这可能相当具有挑战性，因为他们可能需要手动查阅几种不同的教学材料。我们的目标是通过实现自动问答生成（AQAG），使用微调后的生成式大型语言模型，使整个过程变得更加容易。为了调整教师偏好的问题风格（选择题、概念题或事实题），我们利用了提示工程（PE）。在这项研究中，我们提议利用自然语言处理中的无监督学习方法，主要关注英语语言。这种方法使基础Meta-Llama 2-7B模型能够将RACE数据集作为微调过程的训练数据。创建一个为教育工作者、教师和从事文本评估的个人提供高效解决方案的自定义模型。一个可靠且高效的问答生成工具可以释放宝贵的时间和资源，从而简化他们的评估过程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; \Abstract{In the realm of education, student evaluation holds equalsignificance as imparting knowledge. To be evaluated, students usually need togo through text-based academic assessment methods. Instructors need to makediverse sets of questions that need to be fair for all students to prove theiradequacy over a particular topic. This can prove to be quite challenging asthey may need to manually go through several different lecture materials. Ourobjective is to make this whole process much easier by implementing AutomaticQuestion Answer Generation /(AQAG), using fine-tuned generative LLM. Fortailoring the instructor's preferred question style (MCQ, conceptual, orfactual questions), prompt Engineering (PE) is being utilized. In thisresearch, we propose to leverage unsupervised learning methods in NLP,primarily focusing on the English language. This approach empowers the baseMeta-Llama 2-7B model to integrate RACE dataset as training data for thefine-tuning process. Creating a customized model that will offer efficientsolutions for educators, instructors, and individuals engaged in text-basedevaluations. A reliable and efficient tool for generating questions and answerscan free up valuable time and resources, thus streamlining their evaluationprocesses.}</description>
      <author>example@mail.com (Md. Alvee Ehsan, A. S. M Mehedi Hasan, Kefaya Benta Shahnoor, Syeda Sumaiya Tasneem)</author>
      <guid isPermaLink="false">2508.19475v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Differentiable multiphase flow model for physics-informed machine learning in reservoir pressure management</title>
      <link>http://arxiv.org/abs/2508.19419v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种物理信息机器学习方法，将多相流模拟器与卷积神经网络结合，有效解决了地下储层压力控制中的计算效率问题，通过迁移学习大幅减少了所需的模拟次数。&lt;h4&gt;背景&lt;/h4&gt;地下储层压力控制面临地质非均质性和多相流体流动动力学的挑战，基于高保真物理的模拟计算成本高昂，且需要多次模拟来应对不确定的非均质属性，这使得传统方法往往不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的方法来预测地下储层压力行为，减少计算成本，同时保持预测准确性，以应对实际注入-提取场景。&lt;h4&gt;方法&lt;/h4&gt;将完全可微的多相流模拟器（DPFEHM框架实现）与卷积神经网络耦合，CNN学习从非均质渗透率场预测流体提取率以控制关键位置压力；通过将瞬态多相流物理纳入训练提高预测准确性；采用迁移学习策略，先在单相稳态模拟上预训练，再在多相场景上微调以加速训练。&lt;h4&gt;主要发现&lt;/h4&gt;与之前需要多达一千万次全物理多相流模拟相比，使用不到三千次即可实现高精度训练；通过迁移学习策略显著降低了计算成本，使储层压力控制更加实用。&lt;h4&gt;结论&lt;/h4&gt;物理信息机器学习方法结合迁移学习能有效解决地下储层压力控制的挑战，大幅减少计算需求，为实际应用提供了更可行的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的地下储层压力控制由于地质非均质性和多相流体流动动力学而极具挑战性。在这种环境中的预测依赖于计算成本高昂的高保真物理模拟。然而，控制这些流动的不确定、非均质属性使得有必要进行多次这些昂贵的模拟，这通常是不可行的。为了解决这些挑战，我们引入了一种物理信息机器学习工作流程，该工作流程将完全可微的多相流模拟器（在DPFEHM框架中实现）与卷积神经网络(CNN)耦合。CNN学习从非均质渗透率场预测流体提取率，以在关键储层位置强制执行压力限制。通过将瞬态多相流物理纳入训练过程，我们的方法与先前的工作相比，能够对实际的注入-提取场景进行更实用、更准确的预测。为加速训练，我们在单相、稳态模拟上预训练模型，然后在完整多相场景上微调，这大大降低了计算成本。我们证明，与之前估计需要多达一千万次相比，使用不到三千次全物理多相流模拟即可实现高精度训练。这种模拟次数的大幅减少是通过利用从更便宜的单相模拟中迁移学习实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate subsurface reservoir pressure control is extremely challenging dueto geological heterogeneity and multiphase fluid-flow dynamics. Predictingbehavior in this setting relies on high-fidelity physics-based simulations thatare computationally expensive. Yet, the uncertain, heterogeneous propertiesthat control these flows make it necessary to perform many of these expensivesimulations, which is often prohibitive. To address these challenges, weintroduce a physics-informed machine learning workflow that couples a fullydifferentiable multiphase flow simulator, which is implemented in the DPFEHMframework with a convolutional neural network (CNN). The CNN learns to predictfluid extraction rates from heterogeneous permeability fields to enforcepressure limits at critical reservoir locations. By incorporating transientmultiphase flow physics into the training process, our method enables morepractical and accurate predictions for realistic injection-extraction scenarioscompare to previous works. To speed up training, we pretrain the model onsingle-phase, steady-state simulations and then fine-tune it on full multiphasescenarios, which dramatically reduces the computational cost. We demonstratethat high-accuracy training can be achieved with fewer than three thousandfull-physics multiphase flow simulations -- compared to previous estimatesrequiring up to ten million. This drastic reduction in the number ofsimulations is achieved by leveraging transfer learning from much lessexpensive single-phase simulations.</description>
      <author>example@mail.com (Harun Ur Rashid, Aleksandra Pachalieva, Daniel O'Malley)</author>
      <guid isPermaLink="false">2508.19419v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>One Joke to Rule them All? On the (Im)possibility of Generalizing Humor</title>
      <link>http://arxiv.org/abs/2508.19402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了大型语言模型在幽默类型间的迁移学习能力，发现模型能够在不同幽默类型间实现一定程度的迁移，且在多样化数据源上训练可提高迁移能力。&lt;h4&gt;背景&lt;/h4&gt;幽默是一种广泛而复杂的交流形式，现有研究主要专注于特定类型的幽默建模。随着在线和社交媒体中不断涌现新的幽默类型(如迷因、反幽默、AI失败等)，大型语言模型需要能够泛化到不同幽默类型。&lt;h4&gt;目的&lt;/h4&gt;了解在一个或多个特定幽默任务上的能力是否能转移到新的、未见过的幽默类型上，探究这种碎片化是否不可避免。&lt;h4&gt;方法&lt;/h4&gt;进行跨四个不同幽默任务数据集的迁移学习实验，在多样化的训练设置下(使用1-3个数据集训练，测试在新的任务上)训练大型语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;模型具有一定的迁移能力，在未见过的数据集上可达到75%的准确率；在多样化数据源上训练可提高迁移能力(1.88-4.05%)，同时领域内性能几乎没有下降；进一步分析发现幽默类型间存在关联，'爸爸笑话'出人意料地成为最好的迁移促进因素。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型能够在不同幽默类型间实现迁移，这种能力可以通过多样化训练得到增强，为应对不断演变的幽默景观提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;幽默是一种广泛而复杂的交流形式，对机器来说仍然具有挑战性。尽管如此，大多数关于计算幽默的现有研究传统上专注于建模特定类型的幽默。在这项工作中，我们希望了解在一个或多个特定幽默任务上的能力是否能转移到新的、未见过的类型；换句话说，这种碎片化是否不可避免？随着在线和社交媒体环境中不断出现新的幽默类型(例如迷因、反幽默、AI失败等)，这个问题特别及时。如果大型语言模型要跟上这一不断发展的格局，它们必须能够通过捕捉更深层次的、可迁移的机制来泛化到不同的幽默类型。为了研究这一点，我们在代表不同幽默任务的四个数据集上进行了一系列迁移学习实验。我们在不同的多样性设置下训练大型语言模型(使用1-3个数据集进行训练，测试在新的任务上)。实验表明，模型具有一定的迁移能力，在未见过的数据集上可达到75%的准确率；在多样化数据源上训练可以提高迁移能力(1.88-4.05%)，同时领域内性能几乎没有下降。进一步的分析揭示了幽默类型之间的关系，'爸爸笑话'出人意料地成为最好的迁移促进因素(但很难转移到其他类型)。我们发布了数据和代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humor is a broad and complex form of communication that remains challengingfor machines. Despite its broadness, most existing research on computationalhumor traditionally focused on modeling a specific type of humor. In this work,we wish to understand whether competence on one or more specific humor tasksconfers any ability to transfer to novel, unseen types; in other words, is thisfragmentation inevitable? This question is especially timely as new humor typescontinuously emerge in online and social media contexts (e.g., memes,anti-humor, AI fails). If Large Language Models (LLMs) are to keep up with thisevolving landscape, they must be able to generalize across humor types bycapturing deeper, transferable mechanisms. To investigate this, we conduct aseries of transfer learning experiments across four datasets, representingdifferent humor tasks. We train LLMs under varied diversity settings (1-3datasets in training, testing on a novel task). Experiments reveal that modelsare capable of some transfer, and can reach up to 75% accuracy on unseendatasets; training on diverse sources improves transferability (1.88-4.05%)with minimal-to-no drop in in-domain performance. Further analysis suggestsrelations between humor types, with Dad Jokes surprisingly emerging as the bestenabler of transfer (but is difficult to transfer to). We release data andcode.</description>
      <author>example@mail.com (Mor Turgeman, Chen Shani, Dafna Shahaf)</author>
      <guid isPermaLink="false">2508.19402v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Multi-Source Knowledge Transfer by Model Merging</title>
      <link>http://arxiv.org/abs/2508.19353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于奇异值分解(SVD)的多源迁移学习方法，通过将源模型分解为基本分量并选择性聚合，解决了现有方法在效率和精度上的限制，实现了高效、鲁棒且计算可扩展的迁移学习。&lt;h4&gt;背景&lt;/h4&gt;迁移学习是一种有优势的策略，但它忽视了利用在线众多可用模型知识的机会。解决多源迁移学习问题是提高适应性和降低重新训练成本的有前途的路径。&lt;h4&gt;目的&lt;/h4&gt;解决现有多源迁移学习方法在知识提取精度和聚合效率方面的限制，实现高效、鲁棒且计算可扩展的多源迁移学习。&lt;h4&gt;方法&lt;/h4&gt;利用奇异值分解(SVD)将每个源模型分解为基本的、秩为1的分量；在聚合阶段只选择来自所有源的最显著分量；通过仅调整合并矩阵的主奇异值来使方法适应目标任务，重新校准顶部SVD分量的重要性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架允许高效的迁移学习；对输入级别和参数空间中的扰动（例如有噪声或被修剪的源）具有鲁棒性；在计算上具有良好的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;通过SVD分解和选择性聚合，解决了多源迁移学习中的效率和精度问题，实现了高效、鲁棒且计算可扩展的迁移学习框架。&lt;h4&gt;翻译&lt;/h4&gt;虽然迁移学习是一种有优势的策略，但它忽视了利用在线众多可用模型知识的机会。解决这种多源迁移学习问题是提高适应性和降低重新训练成本的有前途的路径。然而，现有方法本质上是粗粒度的，缺乏细粒度知识提取的必要精度，以及融合大量源模型或高参数模型知识所需的聚合效率。我们通过利用奇异值分解(SVD)将每个源模型分解为其基本的、秩为1的分量来解决这些限制。随后的聚合阶段只选择来自所有源的最显著分量，从而克服了之前的效率和精度限制。为了最好地保留和利用合成的知识库，我们的方法通过仅调整合并矩阵的主奇异值来适应目标任务。本质上，这个过程只重新校准了顶部SVD分量的重要性。所提出的框架允许高效的迁移学习，对输入级别和参数空间中的扰动（例如有噪声或被修剪的源）具有鲁棒性，并且在计算上具有良好的可扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While transfer learning is an advantageous strategy, it overlooks theopportunity to leverage knowledge from numerous available models online.Addressing this multi-source transfer learning problem is a promising path toboost adaptability and cut re-training costs. However, existing approaches areinherently coarse-grained, lacking the necessary precision for granularknowledge extraction and the aggregation efficiency required to fuse knowledgefrom either a large number of source models or those with high parametercounts. We address these limitations by leveraging Singular Value Decomposition(SVD) to first decompose each source model into its elementary, rank-onecomponents. A subsequent aggregation stage then selects only the most salientcomponents from all sources, thereby overcoming the previous efficiency andprecision limitations. To best preserve and leverage the synthesized knowledgebase, our method adapts to the target task by fine-tuning only the principalsingular values of the merged matrix. In essence, this process onlyrecalibrates the importance of top SVD components. The proposed frameworkallows for efficient transfer learning, is robust to perturbations both at theinput level and in the parameter space (e.g., noisy or pruned sources), andscales well computationally.</description>
      <author>example@mail.com (Marcin Osial, Bartosz Wójcik, Bartosz Zieliński, Sebastian Cygert)</author>
      <guid isPermaLink="false">2508.19353v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling</title>
      <link>http://arxiv.org/abs/2508.19028v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GRADSTOP的新型随机早停方法，该方法仅利用梯度信息而非单独的验证集来防止机器学习模型过拟合，从而提高在未见数据上的预测性能。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型通常通过梯度下降算法在训练数据上最小化损失函数来学习，但这些模型常遭受过拟合问题，导致在未见数据上的预测性能下降。标准解决方案是使用保留验证集的早停法，当验证损失停止减少时停止训练，但这减少了可用于训练的数据量。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖于验证集的早停方法，充分利用梯度下降算法产生的梯度信息，以提高模型在数据有限情况下的性能。&lt;h4&gt;方法&lt;/h4&gt;GRADSTOP通过三个主要贡献实现：1) 利用梯度信息估计贝叶斯后验；2) 将早停问题定义为从此后验中抽取样本；3) 使用近似后验获得停止标准。&lt;h4&gt;主要发现&lt;/h4&gt;经验评估表明，GRADSTOP在测试数据上实现了较小的损失值，与基于验证集的停止标准相比表现良好。通过利用整个数据集进行训练，该方法在数据有限的情况下（如迁移学习）特别有优势。&lt;h4&gt;结论&lt;/h4&gt;GRADSTOP可作为梯度下降库的一个可选功能加入，只需很小的计算开销。源代码已公开，可在提供的GitHub链接获取。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型通常通过使用梯度下降算法在训练数据上最小化损失函数来学习。这些模型常常遭受过拟合问题，导致在未见数据上的预测性能下降。标准解决方案是使用保留验证集的早停法，当验证损失停止减少时停止最小化过程。然而，这种验证集减少了可用于训练的数据量。本文提出了GRADSTOP，一种新型随机早停方法，它仅使用梯度信息，这些信息由梯度下降算法'免费'产生。我们的主要贡献是：我们通过梯度信息估计贝叶斯后验，将早停问题定义为从此后验中抽取样本，并使用近似后验获得停止标准。我们的经验评估表明，GRADSTOP在测试数据上实现了较小的损失，并且与基于验证集的停止标准相比表现良好。通过利用整个数据集进行训练，我们的方法在数据有限的情况下（如迁移学习）特别有优势。它可以作为梯度下降库的一个可选功能加入，只需很小的计算开销。源代码可在https://github.com/edahelsinki/gradstop获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models are often learned by minimising a loss function onthe training data using a gradient descent algorithm. These models often sufferfrom overfitting, leading to a decline in predictive performance on unseendata. A standard solution is early stopping using a hold-out validation set,which halts the minimisation when the validation loss stops decreasing.However, this hold-out set reduces the data available for training. This paperpresents GRADSTOP, a novel stochastic early stopping method that only usesinformation in the gradients, which are produced by the gradient descentalgorithm ``for free.'' Our main contributions are that we estimate theBayesian posterior by the gradient information, define the early stoppingproblem as drawing sample from this posterior, and use the approximatedposterior to obtain a stopping criterion. Our empirical evaluation shows thatGRADSTOP achieves a small loss on test data and compares favourably to avalidation-set-based stopping criterion. By leveraging the entire dataset fortraining, our method is particularly advantageous in data-limited settings,such as transfer learning. It can be incorporated as an optional feature ingradient descent libraries with only a small computational overhead. The sourcecode is available at https://github.com/edahelsinki/gradstop.</description>
      <author>example@mail.com (Arash Jamshidi, Lauri Seppäläinen, Katsiaryna Haitsiukevich, Hoang Phuc Hau Luu, Anton Björklund, Kai Puolamäki)</author>
      <guid isPermaLink="false">2508.19028v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>FreeVPS: Repurposing Training-Free SAM2 for Generalizable Video Polyp Segmentation</title>
      <link>http://arxiv.org/abs/2508.19705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FreeVPS的视频息肉分割方法，通过重新构建VPS任务为检测后跟踪范式，结合IPS模型的空间上下文和SAM2的时序建模能力，并引入两个无需训练的模块来解决SAM2在长期跟踪中的错误累积问题。&lt;h4&gt;背景&lt;/h4&gt;现有的视频息肉分割方法难以平衡时空建模和领域泛化能力，限制了它们在真实临床场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;解决VPS任务中时空建模和领域泛化之间的平衡问题，提高在真实临床场景中的适用性。&lt;h4&gt;方法&lt;/h4&gt;将VPS任务重新构建为检测后跟踪范式，利用IPS模型的空间上下文和SAM2的时序建模能力；引入内部关联过滤模块消除检测阶段的空间不准确，减少误报；引入交叉关联优化模块自适应更新记忆库，防止错误随时间传播，增强时间连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;两个模块协同工作稳定了SAM2，在领域内和领域外场景中都取得了最先进的性能；FreeVPS在长时间未修剪的结肠镜视频展示了稳健的跟踪能力。&lt;h4&gt;结论&lt;/h4&gt;FreeVPS具有可靠临床分析的潜力，能够有效应用于真实临床场景。&lt;h4&gt;翻译&lt;/h4&gt;现有的视频息肉分割范式通常难以平衡时空建模和领域泛化，限制了它们在真实临床场景中的应用。为应对这一挑战，我们将VPS任务重新构建为检测后跟踪范式，利用图像息肉分割模型捕捉的空间上下文，同时整合SAM2的时序建模能力。然而，在结肠镜视频的长期息肉跟踪过程中，SAM2遭受错误累积，导致雪球效应，影响分割稳定性。我们通过重新利用SAM2作为视频息肉分割器，并采用两个无需训练的模块来缓解这一问题。特别是，内部关联过滤模块消除了检测阶段产生的空间不准确，减少了误报；交叉关联优化模块自适应更新记忆库，防止错误随时间传播，增强时间连贯性。这两个模块协同工作稳定了SAM2，在领域内和领域外场景中都取得了最先进的性能。此外，我们证明了FreeVPS在长时间未修剪结肠镜视频中的稳健跟踪能力，强调了其可靠临床分析的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing video polyp segmentation (VPS) paradigms usually struggle to balancebetween spatiotemporal modeling and domain generalization, limiting theirapplicability in real clinical scenarios. To embrace this challenge, we recastthe VPS task as a track-by-detect paradigm that leverages the spatial contextscaptured by the image polyp segmentation (IPS) model while integrating thetemporal modeling capabilities of segment anything model 2 (SAM2). However,during long-term polyp tracking in colonoscopy videos, SAM2 suffers from erroraccumulation, resulting in a snowball effect that compromises segmentationstability. We mitigate this issue by repurposing SAM2 as a video polypsegmenter with two training-free modules. In particular, the intra-associationfiltering module eliminates spatial inaccuracies originating from the detectingstage, reducing false positives. The inter-association refinement moduleadaptively updates the memory bank to prevent error propagation over time,enhancing temporal coherence. Both modules work synergistically to stabilizeSAM2, achieving cutting-edge performance in both in-domain and out-of-domainscenarios. Furthermore, we demonstrate the robust tracking capabilities ofFreeVPS in long-untrimmed colonoscopy videos, underscoring its potentialreliable clinical analysis.</description>
      <author>example@mail.com (Qiang Hu, Ying Zhou, Gepeng Ji, Nick Barnes, Qiang Li, Zhiwei Wang)</author>
      <guid isPermaLink="false">2508.19705v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Video-LevelGauge: Investigating Contextual Positional Bias in Large Video Language Models</title>
      <link>http://arxiv.org/abs/2508.19650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Video-LevelGauge基准测试，用于系统评估大型视频语言模型(LVLMs)中的位置偏见问题。&lt;h4&gt;背景&lt;/h4&gt;现有LVLM评估基准主要关注整体性能，忽略了位置偏见这一关键但未被充分探索的方面。&lt;h4&gt;目的&lt;/h4&gt;开发专门评估LVLM位置偏见的基准测试，并提供减轻偏见的方法。&lt;h4&gt;方法&lt;/h4&gt;使用标准化探测器和定制上下文设置，能够灵活控制上下文长度、探测位置和上下文类型；结合统计测量和形态模式识别的综合分析方法评估27个最先进的LVLMs。&lt;h4&gt;主要发现&lt;/h4&gt;许多领先的开源LVLM存在显著位置偏见，通常表现出头部或邻域内容偏好；而商业模型如Gemini2.5-Pro在整个视频序列中表现一致。&lt;h4&gt;结论&lt;/h4&gt;Video-LevelGauge为评估和减轻LVLM位置偏见提供了有效工具，上下文长度、变化和模型规模分析为模型改进提供可行指导。&lt;h4&gt;翻译&lt;/h4&gt;大型视频语言模型(LVLMs)在视频理解方面取得了显著进展，推动了相应评估基准的发展。然而，现有基准通常评估整个视频序列的整体性能，忽略了细微行为，如上下文位置偏见，这是LVLM性能中一个关键但未被充分探索的方面。我们提出了Video-LevelGauge，这是一个专门用于系统评估LVLM位置偏见的基准。我们使用标准化的探测器和定制的上下文设置，能够灵活控制上下文长度、探测位置和上下文类型，以模拟多样化的真实场景。此外，我们引入了一种综合分析方法，结合统计测量和形态模式识别来表征偏见。我们的基准包含438个人工策划的视频，涵盖多种类型，产生了1177个高质量的多项选择题和120个开放式问题，其有效性已得到验证，能够有效暴露位置偏见。基于这些，我们评估了27个最先进的LVLMs，包括商业和开源模型。我们的发现显示许多领先的开源模型存在显著的位置偏见，通常表现出头部或邻域内容偏好。相比之下，像Gemini2.5-Pro这样的商业模型在整个视频序列中表现出令人印象深刻的一致性能。关于上下文长度、上下文变化和模型规模的进一步分析，为减轻偏见和指导模型改进提供了可行的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large video language models (LVLMs) have made notable progress in videounderstanding, spurring the development of corresponding evaluation benchmarks.However, existing benchmarks generally assess overall performance across entirevideo sequences, overlooking nuanced behaviors such as contextual positionalbias, a critical yet under-explored aspect of LVLM performance. We presentVideo-LevelGauge, a dedicated benchmark designed to systematically assesspositional bias in LVLMs. We employ standardized probes and customizedcontextual setups, allowing flexible control over context length, probeposition, and contextual types to simulate diverse real-world scenarios. Inaddition, we introduce a comprehensive analysis method that combinesstatistical measures with morphological pattern recognition to characterizebias. Our benchmark comprises 438 manually curated videos spanning multipletypes, yielding 1,177 high-quality multiple-choice questions and 120 open-endedquestions, validated for their effectiveness in exposing positional bias. Basedon these, we evaluate 27 state-of-the-art LVLMs, including both commercial andopen-source models. Our findings reveal significant positional biases in manyleading open-source models, typically exhibiting head or neighbor-contentpreferences. In contrast, commercial models such as Gemini2.5-Pro showimpressive, consistent performance across entire video sequences. Furtheranalyses on context length, context variation, and model scale provideactionable insights for mitigating bias and guiding model enhancement.</description>
      <author>example@mail.com (Hou Xia, Zheren Fu, Fangcan Ling, Jiajun Li, Yi Tu, Zhendong Mao, Yongdong Zhang)</author>
      <guid isPermaLink="false">2508.19650v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</title>
      <link>http://arxiv.org/abs/2508.18634v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的视频描述生成方法，解决了现有方法中的运动细节不平衡问题，通过构建HMD-270K数据集和引入CSER优化方法，开发了OwlCap多模态大语言模型，在两个基准测试上取得了显著改进。&lt;h4&gt;背景&lt;/h4&gt;视频描述生成旨在为视频内容生成全面且连贯的描述，有助于视频理解和生成的发展。然而，现有方法常常存在运动细节不平衡的问题，模型往往过度强调一个方面而忽视另一个方面，导致描述不完整，进而影响视频理解和生成的一致性。&lt;h4&gt;目的&lt;/h4&gt;解决现有视频描述方法中的运动细节不平衡问题，提高视频描述的完整性和一致性，开发能够平衡捕捉运动和细节的视频描述模型。&lt;h4&gt;方法&lt;/h4&gt;从两个方面提出解决方案：1) 数据方面：构建了运动细节协调270K（HMD-270K）数据集，通过两阶段流程：运动细节融合（MDF）和细粒度检查（FGE）；2) 优化方面：引入基于组相对策略优化（GRPO）的描述集等价奖励（CSER），通过单元到集合匹配和双向验证来增强捕捉运动和细节的完整性和准确性。基于HMD-270K监督微调和带有CSER的GRPO后训练，开发了OwlCap视频描述多模态大语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;OwlCap在两个基准测试上相比基线模型取得了显著改进：在侧重细节的VDC上提高了4.2个准确率点，在侧重运动的DREAM-1K上提高了4.6个F1分数点。&lt;h4&gt;结论&lt;/h4&gt;HMD-270K数据集和OwlCap模型将公开发布，以促进视频描述研究社区的发展。&lt;h4&gt;翻译&lt;/h4&gt;视频描述旨在生成对视频内容的全面且连贯的描述，促进视频理解和生成的发展。然而，现有方法常常遭受运动细节不平衡的困扰，因为模型往往过度强调一个方面而忽视另一个方面。这种不平衡导致不完整的描述，进而导致视频理解和生成缺乏一致性。为解决这一问题，我们从两个方面提出解决方案：1) 数据方面：我们通过两阶段流程构建了协调运动细节的270K（HMD-270K）数据集：运动细节融合（MDF）和细粒度检查（FGE）。2) 优化方面：我们引入了基于组相对策略优化（GRPO）的描述集等价奖励（CSER）。CSER通过单元到集合匹配和双向验证，增强了捕捉运动和细节的完整性和准确性。基于HMD-270K监督微调和带有CSER的GRPO后训练，我们开发了OwlCap，一个具有运动细节平衡功能的强大视频描述多模态大语言模型（MLLM）。实验结果表明，与基线模型相比，OwlCap在两个基准测试上取得了显著改进：侧重细节的VDC（+4.2准确率）和侧重运动的DREAM-1K（+4.6 F1）。HMD-270K数据集和OwlCap模型将公开发布，以促进视频描述研究社区的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video captioning aims to generate comprehensive and coherent descriptions ofthe video content, contributing to the advancement of both video understandingand generation. However, existing methods often suffer from motion-detailimbalance, as models tend to overemphasize one aspect while neglecting theother. This imbalance results in incomplete captions, which in turn leads to alack of consistency in video understanding and generation. To address thisissue, we propose solutions from two aspects: 1) Data aspect: We constructedthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stagepipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)based on Group Relative Policy Optimization (GRPO). CSER enhances completenessand accuracy in capturing both motion and details through unit-to-set matchingand bidirectional validation. Based on the HMD-270K supervised fine-tuning andGRPO post-training with CSER, we developed OwlCap, a powerful video captioningmulti-modal large language model (MLLM) with motion-detail balance.Experimental results demonstrate that OwlCap achieves significant improvementscompared to baseline models on two benchmarks: the detail-focused VDC (+4.2Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCapmodel will be publicly released to facilitate video captioning researchcommunity advancements.</description>
      <author>example@mail.com (Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, Xiang Bai)</author>
      <guid isPermaLink="false">2508.18634v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</title>
      <link>http://arxiv.org/abs/2508.18463v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的上下文感知零样本异常检测框架，能够在训练过程中不接触异常样本的情况下识别监控录像中的异常事件。&lt;h4&gt;背景&lt;/h4&gt;监控录像中的异常检测具有挑战性，因为异常事件具有不可预测性和上下文依赖性，传统的异常检测方法通常需要大量异常样本进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种零样本异常检测框架，无需在训练阶段暴露异常样本，同时能够整合时间推理和语义上下文信息，提高复杂环境中异常检测的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出混合架构，结合TimeSformer、DPC和CLIP三种模型：TimeSformer作为视觉主干提取时空特征，DPC预测未来表示识别时间偏差，CLIP语义流实现概念级异常检测；使用InfoNCE和CPC损失进行联合训练；引入上下文门控机制通过场景感知提示或全局视频特征调节预测结果。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合预测建模与视觉-语言理解，系统能够泛化到复杂环境中未见过的行为；该框架成功弥合了零样本异常检测中时间推理和语义上下文之间的差距。&lt;h4&gt;结论&lt;/h4&gt;该上下文感知零样本异常检测框架能够有效识别监控录像中的异常事件，无需训练阶段接触异常样本，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;监控录像中的异常检测因其不可预测和上下文依赖的本质而具有固有挑战。这项工作引入了一种新颖的上下文感知零样本异常检测框架，能够在训练过程中不接触异常样本的情况下识别异常事件。提出的混合架构结合了TimeSformer、DPC和CLIP，用于建模时空动态和语义上下文。TimeSformer作为视觉主干提取丰富的时空特征，而DPC预测未来表示以识别时间偏差。此外，基于CLIP的语义流通过特定上下文的文本提示实现概念级异常检测。这些组件使用InfoNCE和CPC损失进行联合训练，将视觉输入与其时间和语义表示对齐。上下文门控机制通过使用场景感知提示或全局视频特征调节预测来增强决策能力。通过整合预测建模与视觉-语言理解，该系统能够泛化到复杂环境中先前未见的行为。该框架弥合了监控零样本异常检测中时间推理和语义上下文之间的差距。本研究的代码已在https://github.com/NK-II/Context-Aware-Zero-Shot-Anomaly-Detection-in-Surveillance上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in surveillance footage is inherently challenging due totheir unpredictable and context-dependent nature. This work introduces a novelcontext-aware zero-shot anomaly detection framework that identifies abnormalevents without exposure to anomaly examples during training. The proposedhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporaldynamics and semantic context. TimeSformer serves as the vision backbone toextract rich spatial-temporal features, while DPC forecasts futurerepresentations to identify temporal deviations. Furthermore, a CLIP-basedsemantic stream enables concept-level anomaly detection throughcontext-specific text prompts. These components are jointly trained usingInfoNCE and CPC losses, aligning visual inputs with their temporal and semanticrepresentations. A context-gating mechanism further enhances decision-making bymodulating predictions with scene-aware cues or global video features. Byintegrating predictive modeling with vision-language understanding, the systemcan generalize to previously unseen behaviors in complex environments. Thisframework bridges the gap between temporal reasoning and semantic context inzero-shot anomaly detection for surveillance. The code for this research hasbeen made available athttps://github.com/NK-II/Context-Aware-Zero-Shot-Anomaly-Detection-in-Surveillance.</description>
      <author>example@mail.com (Md. Rashid Shahriar Khan, Md. Abrar Hasan, Mohammod Tareq Aziz Justice)</author>
      <guid isPermaLink="false">2508.18463v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>OpenM3D: Open Vocabulary Multi-view Indoor 3D Object Detection without Human Annotations</title>
      <link>http://arxiv.org/abs/2508.20063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OpenM3D，一种新的开放词汇多视角室内3D目标检测器，无需人工标注即可训练。该检测器采用2D诱导体素特征，结合类无关的3D定位损失和体素语义对齐损失进行训练，并提出了3D伪框生成方法。OpenM3D在ScanNet200和ARKitScenes基准测试中表现出高准确性和效率，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;Open-vocabulary 3D目标检测是一个新兴领域，但基于图像的方法相比基于3D点云的方法探索有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需人工标注训练的开放词汇多视角室内3D目标检测器。&lt;h4&gt;方法&lt;/h4&gt;OpenM3D是一种单阶段检测器，采用ImGeoNet模型的2D诱导体素特征，与类无关的3D定位损失和体素语义对齐损失联合训练。提出3D伪框生成方法，使用图嵌入技术将2D片段组合成连贯的3D结构，并采样多样化的CLIP特征进行体素特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;提出的伪框方法比其他方法具有更高的精确率和召回率；训练高精度单阶段检测器需要两种损失都向高质量目标学习。&lt;h4&gt;结论&lt;/h4&gt;OpenM3D是一种高效的检测器，仅需多视角图像作为输入，在ScanNet200和ARKitScenes室内基准测试中表现出更高的准确性和速度（每场景0.3秒），在准确性和速度上都优于强大的两阶段方法和基线方法。&lt;h4&gt;翻译&lt;/h4&gt;Open-vocabulary (OV) 3D目标检测是一个新兴领域，但通过基于图像的方法进行探索仍然有限，相比基于3D点云的方法。我们引入了OpenM3D，一种新颖的开放词汇多视角室内3D目标检测器，无需人工标注即可训练。特别是，OpenM3D是一种单阶段检测器，采用来自ImGeoNet模型的2D诱导体素特征。为了支持OV，它与类无关的3D定位损失联合训练，需要高质量的3D伪框，以及与多样化预训练CLIP特征联合训练的体素语义对齐损失。我们遵循OV-3DET的训练设置，提供姿态RGB-D图像，但没有3D框或类别的人工标注。我们提出了一种使用图嵌入技术的3D伪框生成方法，将2D片段组合成连贯的3D结构。我们的伪框比其他方法（包括OV-3DET中提出的方法）实现了更高的精确率和召回率。我们进一步从与每个连贯3D结构关联的2D片段中采样多样化的CLIP特征，与相应的体素特征对齐。训练高精度单阶段检测器的关键需要两种损失都向高质量目标学习。在推理时，OpenM3D是一种高效的检测器，仅需多视角图像作为输入，并在ScanNet200和ARKitScenes室内基准测试中相比现有方法表现出更高的准确性和速度（每场景0.3秒）。我们在准确性和速度上都优于强大的两阶段方法（利用我们的类无关检测器与ViT CLIP-based OV分类器）和基线方法（整合多视角深度估计器）。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放词汇多视角室内3D物体检测问题，即系统能够识别训练时未见过的物体类别，仅通过文本描述就能检测3D空间中的物体，且不需要昂贵的3D传感器输入。这个问题在现实中很重要，因为传统3D物体检测需要大量昂贵的人工标注数据，而现有开放词汇方法大多依赖3D点云输入，需要深度相机等昂贵设备，限制了实际应用。解决这一问题可以降低3D检测的使用门槛，使系统能够灵活识别新类别物体，适用于更广泛的应用场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：基于点云的方法需要昂贵3D传感器，而基于图像的开放词汇3D检测研究有限。他们借鉴了多项现有工作：采用ImGeoNet模型的2D诱导体素特征作为基础架构；利用预训练CLIP模型实现文本-图像语义对齐；使用SAM进行类无关2D分割；应用图嵌入技术组合多视角信息；遵循OV-3DET的无标注训练设置。作者设计了一个综合方案，通过3D伪框生成和体素-语义对齐两个关键技术，实现了无需人工标注的高效开放词汇3D检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用多视角RGB图像而非3D点云进行检测，通过无监督方式生成高质量3D伪框作为训练信号，并对齐3D体素特征与CLIP语义特征实现开放词汇分类。整体流程分为训练和推理两阶段：训练时，首先用SAM分割多视角图像得到2D片段，然后通过图嵌入技术将片段组合成3D结构生成伪框；接着提取多视角图像特征并投影到3D空间形成体素特征；最后联合优化类无关定位损失和体素-语义对齐损失训练检测器。推理时，仅需多视角RGB图像，检测器直接输出3D边界框和开放词汇分类结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个开放词汇多视角3D物体检测器；2) 基于图嵌入的3D伪框生成方法，考虑所有视角信息提高质量；3) 体素-语义对齐损失，实现高效开放词汇分类；4) 高效单阶段检测器，推理速度达0.3秒/场景。相比之前工作，OpenM3D不需要3D点云输入和人工标注，推理时仅需RGB图像；采用单阶段架构而非两阶段方法；不依赖CLIP ViT模型进行推理，计算成本更低；在准确性和速度上均显著优于现有方法，如比OV-3DET快16倍，比OpenMask3D快100倍以上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenM3D首次实现了无需人工标注、仅需多视角图像输入的高效开放词汇多视角室内3D物体检测，通过创新的3D伪框生成和体素-语义对齐方法，在准确性和速度上均显著优于现有方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-vocabulary (OV) 3D object detection is an emerging field, yet itsexploration through image-based methods remains limited compared to 3D pointcloud-based methods. We introduce OpenM3D, a novel open-vocabulary multi-viewindoor 3D object detector trained without human annotations. In particular,OpenM3D is a single-stage detector adapting the 2D-induced voxel features fromthe ImGeoNet model. To support OV, it is jointly trained with a class-agnostic3D localization loss requiring high-quality 3D pseudo boxes and avoxel-semantic alignment loss requiring diverse pre-trained CLIP features. Wefollow the training setting of OV-3DET where posed RGB-D images are given butno human annotations of 3D boxes or classes are available. We propose a 3DPseudo Box Generation method using a graph embedding technique that combines 2Dsegments into coherent 3D structures. Our pseudo-boxes achieve higher precisionand recall than other methods, including the method proposed in OV-3DET. Wefurther sample diverse CLIP features from 2D segments associated with eachcoherent 3D structure to align with the corresponding voxel feature. The key totraining a highly accurate single-stage detector requires both losses to belearned toward high-quality targets. At inference, OpenM3D, a highly efficientdetector, requires only multi-view images for input and demonstrates superioraccuracy and speed (0.3 sec. per scene) on ScanNet200 and ARKitScenes indoorbenchmarks compared to existing methods. We outperform a strong two-stagemethod that leverages our class-agnostic detector with a ViT CLIP-based OVclassifier and a baseline incorporating multi-view depth estimator on bothaccuracy and speed.</description>
      <author>example@mail.com (Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert Y. C. Chen, Min Sun, Cheng-Hao Kuo)</author>
      <guid isPermaLink="false">2508.20063v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Integrating SAM Supervision for 3D Weakly Supervised Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2508.19909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种创新方法，通过利用2D基础模型生成的分割掩码来增强3D语义分割性能，解决了3D数据标注困难和有限的问题。&lt;h4&gt;背景&lt;/h4&gt;当前3D语义分割方法面临三大挑战：1) 3D点云数据标注困难（量大、不规则、无序）；2) 现有方法仅关注3D领域，未充分利用2D和3D数据的互补性；3) 扩展原始标签或生成伪标签的方法未能充分利用这些标签或处理其中的噪声。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法，最大化稀疏可用3D注释的效用，通过整合2D基础模型生成的分割掩码，提高3D弱监督分割的性能。&lt;h4&gt;方法&lt;/h4&gt;1. 利用2D基础模型生成分割掩码；2. 建立3D场景与2D视图间的几何对应关系，将2D分割掩码传播到3D空间；3. 将高度稀疏的注释扩展到3D掩码所定义的区域，显著增加可用标签池；4. 在3D点云增强上应用基于置信度和一致性的正则化，选择可靠的伪标签；5. 将可靠的伪标签传播到3D掩码上生成更多标签。&lt;h4&gt;主要发现&lt;/h4&gt;通过将2D基础模型的能力与有限的3D注释相结合，可以有效解决3D语义分割中的标注困难问题，显著提高分割性能。&lt;h4&gt;结论&lt;/h4&gt;这种创新策略弥合了有限3D注释与强大2D基础模型能力之间的差距，最终提高了3D弱监督分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;当前3D语义分割方法建议使用有限的注释来训练模型，以解决标注大型、不规则和无序的3D点云数据的困难。它们通常只关注3D领域，没有利用2D和3D数据的互补性。此外，一些方法扩展原始标签或生成伪标签来指导训练，但它们往往无法充分利用这些标签或解决其中的噪声问题。同时，全面且适应性强的基础模型的出现为分割2D数据提供了有效的解决方案。利用这一进步，我们提出了一种新方法，通过整合2D基础模型生成的分割掩码来最大化稀疏可用3D注释的效用。我们通过建立3D场景和2D视图之间的几何对应关系，将2D分割掩码传播到3D空间。我们将高度稀疏的注释扩展到3D掩码所定义的区域，从而显著增加了可用标签池。此外，我们在3D点云的增强上应用基于置信度和一致性的正则化，选择可靠的伪标签，这些标签进一步传播到3D掩码上以生成更多标签。这种创新策略弥合了有限3D注释与强大2D基础模型能力之间的差距，最终提高了3D弱监督分割的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D点云数据标注困难的问题，具体是3D弱监督语义分割任务。这个问题在现实中很重要，因为3D点云数据具有稀疏、不规则、高维、模糊和无序的特点，使得人工标注极其耗时耗力。完全标注3D数据成本高昂，而现有方法往往只关注3D领域，未能充分利用2D和3D数据的互补性，或者无法有效处理伪标签中的噪声。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D点云标注困难和现有方法局限性，注意到2D基础模型(如Semantic-SAM)在图像分割方面的强大能力。然后思考如何利用2D和3D数据间的几何对应关系，将2D模型能力迁移到3D领域。方法设计借鉴了多项现有工作：使用Semantic-SAM生成2D分割掩码；借鉴RAC-Net的一致性正则化和可靠伪标签选择；采用噪声鲁棒损失函数处理标签噪声；使用PointWolf和仿射变换进行数据增强；利用链接矩阵和空间对应关系进行2D-3D投影。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用2D基础模型生成高质量分割掩码，将其投影到3D空间，结合稀疏3D标注扩展可用标签，并通过一致性正则化和噪声鲁棒训练处理标签噪声。整体流程包括：1)使用Semantic-SAM生成2D分割掩码；2)通过相机矩阵和深度信息将2D掩码投影到3D空间并合并不同视图掩码；3)对点云及其增强版本进行预测，基于置信度和不确定性分类预测；4)将初始稀疏标注和可靠伪标签扩展到3D掩码区域；5)结合多种损失函数进行训练，包括原始标注损失、可靠伪标签损失、模糊伪标签损失和噪声鲁棒损失。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统性地将2D基础模型分割能力应用于3D弱监督分割，通过2D-3D掩码投影融合；2)基于可靠伪标签在掩码区域中的比例，智能扩展标签到整个区域；3)使用归一化交叉熵和反向交叉熵损失处理扩展标签中的噪声；4)构建完整的端到端框架。相比之前工作，该方法充分利用了2D图像的丰富信息和2D基础模型能力，不需要完全标注的3D数据，仅需稀疏标注和未标注2D图像，显著提高了分割精度，且只在训练阶段使用2D图像，推理时直接使用3D点云，更适用于实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种创新方法，通过将2D基础模型的分割掩码投影到3D空间并结合一致性正则化和噪声鲁棒训练，显著提升了3D点云弱监督语义分割的性能，实现了在仅有稀疏标注情况下的高质量分割结果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current methods for 3D semantic segmentation propose training models withlimited annotations to address the difficulty of annotating large, irregular,and unordered 3D point cloud data. They usually focus on the 3D domain only,without leveraging the complementary nature of 2D and 3D data. Besides, somemethods extend original labels or generate pseudo labels to guide the training,but they often fail to fully use these labels or address the noise within them.Meanwhile, the emergence of comprehensive and adaptable foundation models hasoffered effective solutions for segmenting 2D data. Leveraging thisadvancement, we present a novel approach that maximizes the utility of sparselyavailable 3D annotations by incorporating segmentation masks generated by 2Dfoundation models. We further propagate the 2D segmentation masks into the 3Dspace by establishing geometric correspondences between 3D scenes and 2D views.We extend the highly sparse annotations to encompass the areas delineated by 3Dmasks, thereby substantially augmenting the pool of available labels.Furthermore, we apply confidence- and uncertainty-based consistencyregularization on augmentations of the 3D point cloud and select the reliablepseudo labels, which are further spread on the 3D masks to generate morelabels. This innovative strategy bridges the gap between limited 3D annotationsand the powerful capabilities of 2D foundation models, ultimately improving theperformance of 3D weakly supervised segmentation.</description>
      <author>example@mail.com (Lechun You, Zhonghua Wu, Weide Liu, Xulei Yang, Jun Cheng, Wei Zhou, Bharadwaj Veeravalli, Guosheng Lin)</author>
      <guid isPermaLink="false">2508.19909v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Beyond BEV: Optimizing Point-Level Tokens for Collaborative Perception</title>
      <link>http://arxiv.org/abs/2508.19638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CoPLOT的新型协作感知框架，利用点级优化令牌解决现有方法中丢弃3D结构信息的问题&lt;h4&gt;背景&lt;/h4&gt;现有协作感知方法通常使用2D鸟瞰图表示作为中间特征，这会丢弃对准确物体识别和定位至关重要的细粒度3D结构线索&lt;h4&gt;目的&lt;/h4&gt;引入点级令牌作为协作感知的中间表示，并解决点云数据固有的无序性、大规模和位置敏感性带来的挑战&lt;h4&gt;方法&lt;/h4&gt;提出CoPLOT框架，包含点原生处理流程、语义感知的令牌重排序模块、频率增强的状态空间模型和邻居到自我对齐模块&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界数据集上的实验表明，CoPLOT优于最先进模型，且具有更低的通信和计算开销&lt;h4&gt;结论&lt;/h4&gt;CoPLOT能够有效保留关键的3D结构信息，同时提高感知性能，是一种有效的协作感知框架&lt;h4&gt;翻译&lt;/h4&gt;协作感知允许代理通过交换中间特征来增强其感知能力。现有方法通常将这些中间特征组织为2D鸟瞰图表示，这会丢弃对准确物体识别和定位至关重要的细粒度3D结构线索。为此，我们首先引入点级令牌作为协作感知的中间表示。然而，点云数据本质上是无序的、大规模的且对位置敏感，这使得产生保留详细结构信息的紧凑且对齐的点级令牌序列具有挑战性。因此，我们提出了CoPLOT，一种利用点级优化令牌的新型协作感知框架。它包含点原生处理流程，包括令牌重排序、序列建模和多智能体空间对齐。语义感知的令牌重排序模块利用场景级和令牌级语义信息生成自适应的1D重排序。频率增强的状态空间模型捕获空间和频谱域中的长程序列依赖关系，提高前景令牌和背景杂波之间的区分度。最后，邻居到自我对齐模块应用闭环过程，结合全局智能体级校正和局部令牌级细化，以减轻定位噪声。在模拟和真实世界数据集上的大量实验表明，CoPLOT优于最先进的模型，且具有更低的通信和计算开销。代码将在https://github.com/CheeryLeeyy/CoPLOT上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决协同感知中现有方法使用2D鸟瞰图(BEV)表示作为中间特征时丢失关键3D结构线索的问题。这个问题在自动驾驶和智能交通系统中非常重要，因为准确的3D物体识别和定位对安全至关重要，而丢失的垂直信息与物体类别、地形变化和空间划分密切相关，同时现有方法还包含大量背景噪声导致不必要的计算和通信成本，不利于实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有2D BEV方法的局限性，包括丢失精细3D结构线索和包含不必要背景噪声导致的计算通信开销大。然后提出使用点级令牌作为中间表示来保留完整3D信息。具体设计上，作者借鉴了状态空间模型(SSMs)在建模长程依赖方面的优势，受到生物视觉系统使用频率选择性滤波器分离目标的启发，并参考了协同感知领域中的V2VNet、CoBEVT等方法和点云处理技术如PointPillars，最终设计了语义感知令牌重排序、频率增强状态空间模型和邻居到自我对齐三个核心模块。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用点级令牌作为协同感知的中间表示，保留完整3D结构信息同时减少计算通信开销。整体流程包括：1)点令牌化将原始点云转换为1D令牌序列；2)语义感知动态编码包括场景上下文提取、令牌重排序、频率增强的状态空间建模和重要性过滤；3)消息共享传输选定的令牌；4)点聚合将相邻智能体令牌转换到自我坐标系；5)语义感知动态融合通过邻居到自我对齐模块调整令牌位置；6)任务网络产生最终预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)点级令牌作为中间表示保留完整3D信息；2)语义感知令牌重排序模块利用场景和令牌级语义信息自适应重排序；3)频率增强状态空间模型受生物视觉启发提取频域表示增强长程依赖建模；4)邻居到自我对齐模块结合全局和局部调整处理定位噪声。相比之前工作，CoPLOT不同于传统2D BEV表示和固定模式排序，能更好区分前景背景，同时显著降低计算开销约80%和通信开销约90%，在三个数据集上提升感知性能达4.2%-9.8%。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CoPLOT通过引入点级优化令牌作为协同感知的中间表示，结合语义感知令牌重排序、频率增强状态空间模型和邻居到自我对齐等创新模块，在保留完整3D结构信息的同时，显著提升了感知性能并大幅降低了计算和通信开销。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative perception allows agents to enhance their perceptualcapabilities by exchanging intermediate features. Existing methods typicallyorganize these intermediate features as 2D bird's-eye-view (BEV)representations, which discard critical fine-grained 3D structural cuesessential for accurate object recognition and localization. To this end, wefirst introduce point-level tokens as intermediate representations forcollaborative perception. However, point-cloud data are inherently unordered,massive, and position-sensitive, making it challenging to produce compact andaligned point-level token sequences that preserve detailed structuralinformation. Therefore, we present CoPLOT, a novel Collaborative perceptionframework that utilizes Point-Level Optimized Tokens. It incorporates apoint-native processing pipeline, including token reordering, sequencemodeling, and multi-agent spatial alignment. A semantic-aware token reorderingmodule generates adaptive 1D reorderings by leveraging scene-level andtoken-level semantic information. A frequency-enhanced state space modelcaptures long-range sequence dependencies across both spatial and spectraldomains, improving the differentiation between foreground tokens and backgroundclutter. Lastly, a neighbor-to-ego alignment module applies a closed-loopprocess, combining global agent-level correction with local token-levelrefinement to mitigate localization noise. Extensive experiments on bothsimulated and real-world datasets show that CoPLOT outperforms state-of-the-artmodels, with even lower communication and computation overhead. Code will beavailable at https://github.com/CheeryLeeyy/CoPLOT.</description>
      <author>example@mail.com (Yang Li, Quan Yuan, Guiyang Luo, Xiaoyuan Fu, Rui Pan, Yujia Yang, Congzhang Shao, Yuewen Liu, Jinglin Li)</author>
      <guid isPermaLink="false">2508.19638v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Model-Based Purification Against Adversarial Attacks for LiDAR Segmentation</title>
      <link>http://arxiv.org/abs/2508.19290v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对2D范围视图LiDAR分割的高效对抗防御方法，通过基于模型的净化框架实现了强大的对抗鲁棒性，同时保持计算效率。&lt;h4&gt;背景&lt;/h4&gt;LiDAR分割对自动驾驶车辆的可靠感知至关重要，但现代分割网络容易受到对抗攻击。现有防御方法主要针对原始3D点云网络且计算密集，而广泛使用的2D范围视图LiDAR分割缺乏专用轻量级对抗防御。&lt;h4&gt;目的&lt;/h4&gt;开发一个高效的基于模型的净化框架，专门用于2D范围视图LiDAR分割中的对抗防御。&lt;h4&gt;方法&lt;/h4&gt;提出范围视图域中的直接攻击公式，并开发基于数学优化问题的可解释净化网络，实现最小计算开销下的强对抗鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在开放基准测试上取得有竞争力性能，一致优于生成和对抗训练基线，实际车辆部署验证了其在真实自动驾驶场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的框架成功解决了2D范围视图LiDAR分割的对抗防御问题，相比现有方法更轻量且高效，在实际应用中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的分割对自动驾驶车辆中的可靠感知至关重要，然而现代分割网络极易受到对抗攻击，这可能危及安全。大多数现有防御是为直接在原始3D点云上运行的网络设计的，并依赖大型计算密集型生成模型。然而，许多最先进的LiDAR分割管道在更高效的2D范围视图表示上运行。尽管它们被广泛采用，但针对该领域的专用轻量级对抗防御仍然很少被探索。我们引入了一个高效的基于模型的净化框架，专门用于2D范围视图LiDAR分割中的对抗防御。我们提出了范围视图域中的直接攻击公式，并开发了一个基于数学优化问题的可解释净化网络，实现了强大的对抗鲁棒性，同时计算开销最小。我们的方法在开放基准测试上取得了具有竞争力的性能，一致优于生成和对抗训练基线。更重要的是，在演示车辆上的实际部署展示了该框架在实际自动驾驶场景中提供准确操作的能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决LiDAR分割网络容易受到对抗性攻击的问题。在自动驾驶领域，LiDAR是环境感知的核心技术，而分割是理解周围场景的关键。对抗性攻击可以通过微小扰动导致分割网络做出错误判断，严重影响行车安全。现有防御方法主要针对3D点云设计，计算量大且不适合车载平台，而当前先进的LiDAR分割系统使用更高效的2D range view表示，缺乏专门的轻量级防御方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行设计：1) 发现3D对抗性攻击在投影到2D range view时会引入量化伪影，削弱攻击效果；2) 现有防御方法基于生成模型，计算复杂度高，不适合车载平台。作者借鉴了对抗性训练和生成模型防御的基本思想，但针对2D range view表示进行了专门优化。他们将净化问题建模为受约束的优化问题，利用半二次分裂方法和深度展开技术，将优化算法转化为高效且可解释的神经网络(DU-AP)。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将对抗性净化问题建模为结构化去噪问题，利用range view数据的特定结构(如水平梯度特性)设计受约束的优化问题。实现流程：1) 在2D range view域设计对抗性攻击，直接添加受控扰动；2) 将净化问题建模为优化问题，最小化扰动差异并保持range view结构；3) 使用半二次分裂方法将问题分解为数据一致性更新和去噪两个子问题；4) 通过深度展开技术将优化算法转化为5层网络结构(DU-AP)；5) 将DU-AP作为预处理模块，在分割网络前净化对抗性输入。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 专门针对2D range view表示设计对抗性攻击，避免3D到2D投影的量化伪影；2) 轻量级防御框架DU-AP，参数量减少99%以上；3) 可解释的神经网络结构，基于深度展开技术；4) 在真实自动驾驶车辆上成功部署验证。相比之前工作的不同：1) 攻击和防御都专门针对2D range view表示，而非传统3D点云；2) 基于优化理论而非生成模型，大幅降低计算复杂度；3) 不仅在标准数据集验证，还在真实场景测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种高效、轻量级的对抗性防御方法，专门针对2D range view LiDAR分割系统，通过将净化问题建模为优化问题并转化为神经网络，在保持高防御效果的同时大幅降低计算复杂度，实现了在真实自动驾驶车辆上的实时部署。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based segmentation is essential for reliable perception in autonomousvehicles, yet modern segmentation networks are highly susceptible toadversarial attacks that can compromise safety. Most existing defenses aredesigned for networks operating directly on raw 3D point clouds and rely onlarge, computationally intensive generative models. However, manystate-of-the-art LiDAR segmentation pipelines operate on more efficient 2Drange view representations. Despite their widespread adoption, dedicatedlightweight adversarial defenses for this domain remain largely unexplored. Weintroduce an efficient model-based purification framework tailored foradversarial defense in 2D range-view LiDAR segmentation. We propose a directattack formulation in the range-view domain and develop an explainablepurification network based on a mathematical justified optimization problem,achieving strong adversarial resilience with minimal computational overhead.Our method achieves competitive performance on open benchmarks, consistentlyoutperforming generative and adversarial training baselines. More importantly,real-world deployment on a demo vehicle demonstrates the framework's ability todeliver accurate operation in practical autonomous driving scenarios.</description>
      <author>example@mail.com (Alexandros Gkillas, Ioulia Kapsali, Nikos Piperigkos, Aris S. Lalos)</author>
      <guid isPermaLink="false">2508.19290v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Sensors and Autonomous Driving: Technologies, Limitations, and Opportunities</title>
      <link>http://arxiv.org/abs/2508.19905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted and under review at IEEE OJVT, August 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文首次全面综述了高光谱成像技术在汽车应用中的现状，评估了HSI技术的优势和局限性，分析了商业可用HSI相机的性能，并回顾了相关数据集和应用，指出了HSI在ADAS/AD应用中的研究潜力和商业化挑战。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像(HSI)为高级驾驶辅助系统(ADAS)和自动驾驶(AD)应用提供了变革性的传感模式，能够通过精细的光谱分辨率实现材料级别的场景理解，这是传统RGB成像无法做到的。&lt;h4&gt;目的&lt;/h4&gt;对汽车应用中的高光谱成像技术进行全面综述，考察当前HSI技术的优势、局限性和适用性，分析商业HSI相机的性能，回顾相关数据集和应用，并确定HSI在ADAS/AD领域的现状和未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;进行定性分析HSI技术的优势和局限性；分析216种商业上可用的HSI和多光谱成像相机，并根据关键汽车标准（帧率、空间分辨率、光谱维度和AEC-Q100温度标准符合性）进行基准测试；回顾最近的HSI数据集和应用案例。&lt;h4&gt;主要发现&lt;/h4&gt;HSI的研究潜力与其商业准备程度之间存在显著差距；只有四款相机满足定义的性能阈值，且没有一款符合AEC-Q100温度标准要求；当前HSI数据集在规模、光谱一致性、光谱通道数量和环境多样性方面存在局限，这对感知算法的开发和HSI在ADAS/AD应用中真实潜力的验证提出了挑战。&lt;h4&gt;结论&lt;/h4&gt;截至2025年，高光谱成像在汽车领域的应用仍面临商业化挑战，需要进一步研究以实现光谱成像在ADAS和自主系统中的实际集成。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)为高级驾驶辅助系统(ADAS)和自动驾驶(AD)应用提供了变革性的传感模式，通过超越传统RGB成像能力的光谱分辨率实现材料级别的场景理解。本文首次对汽车应用中的高光谱成像进行了全面综述，考察了当前HSI技术在ADAS/AD背景下的优势、局限性和适用性。除了这一定性分析，我们还分析了216种商业上可用的高光谱和多光谱成像相机，并根据关键汽车标准对它们进行基准测试：帧率、空间分辨率、光谱维度和AEC-Q100温度标准符合性。我们的分析揭示了HSI的研究潜力与其商业准备程度之间存在显著差距。只有四款相机满足定义的性能阈值，且没有一款符合AEC-Q100要求。此外，本文回顾了最近的HSI数据集和应用，包括道路表面分类、行人可分离性和恶劣天气感知的语义分割。我们的综述显示，当前HSI数据集在规模、光谱一致性、光谱通道数量和环境多样性方面存在局限，这对感知算法的开发和HSI在ADAS/AD应用中真实潜力的充分验证提出了挑战。本综述论文确立了截至2025年高光谱成像在汽车领域的现状，并概述了将光谱成像整合到ADAS和自主系统中的关键研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) offers a transformative sensing modality forAdvanced Driver Assistance Systems (ADAS) and autonomous driving (AD)applications, enabling material-level scene understanding through fine spectralresolution beyond the capabilities of traditional RGB imaging. This paperpresents the first comprehensive review of HSI for automotive applications,examining the strengths, limitations, and suitability of current HSItechnologies in the context of ADAS/AD. In addition to this qualitative review,we analyze 216 commercially available HSI and multispectral imaging cameras,benchmarking them against key automotive criteria: frame rate, spatialresolution, spectral dimensionality, and compliance with AEC-Q100 temperaturestandards. Our analysis reveals a significant gap between HSI's demonstratedresearch potential and its commercial readiness. Only four cameras meet thedefined performance thresholds, and none comply with AEC-Q100 requirements. Inaddition, the paper reviews recent HSI datasets and applications, includingsemantic segmentation for road surface classification, pedestrian separability,and adverse weather perception. Our review shows that current HSI datasets arelimited in terms of scale, spectral consistency, the number of spectralchannels, and environmental diversity, posing challenges for the development ofperception algorithms and the adequate validation of HSI's true potential inADAS/AD applications. This review paper establishes the current state of HSI inautomotive contexts as of 2025 and outlines key research directions towardpractical integration of spectral imaging in ADAS and autonomous systems.</description>
      <author>example@mail.com (Imad Ali Shah, Jiarong Li, Roshan George, Tim Brophy, Enda Ward, Martin Glavin, Edward Jones, Brian Deegan)</author>
      <guid isPermaLink="false">2508.19905v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Risk Estimation in Home Environments: A Probabilistic Framework for Service Robots</title>
      <link>http://arxiv.org/abs/2508.19788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, Accepted for IEEE RO-MAN 2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的框架，用于估计日常室内场景中的事故易发区域，旨在提高服务机器人在以人为中心的环境中实时风险感知能力。该框架通过基于语义图的传播算法对对象级风险和上下文进行建模，实现了对潜在风险的推断，并在数据集上验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;随着机器人逐渐融入日常生活，特别是在家庭环境中，机器人预见和应对环境危害的能力对于确保用户安全、信任和有效的人机交互至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够估计日常室内场景中事故易发区域的框架，提高服务机器人在以人为中心的环境中实时风险感知能力，使机器人能够预见和应对环境危害，确保用户安全、信任和有效的人机交互。&lt;h4&gt;方法&lt;/h4&gt;研究采用基于语义图的传播算法对对象级风险和上下文进行建模。每个对象表示为一个带有相关风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称地传播到低风险对象。这种方法设计具有可解释性和轻量级机载部署特点。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在带有人工标注风险区域的数据集上验证，实现了二值风险检测75%的准确率。系统与人类感知高度一致，特别是在涉及尖锐或不稳定物体的场景中表现突出。&lt;h4&gt;结论&lt;/h4&gt;上下文感知的风险推理具有增强机器人场景理解和在共享人机空间中主动安全行为的潜力。该框架可作为未来系统能够做出上下文驱动的安全决策、提供实时警报或在家庭环境中自主协助用户避免或减轻危害的基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于估计日常室内场景中事故易发区域的新框架，旨在提高在以人为中心的环境中运行的服务机器人的实时风险意识。随着机器人融入日常生活，特别是在家庭中，预见和应对环境危害的能力对于确保用户安全、信任和有效的人机交互至关重要。我们的方法通过基于语义图的传播算法对对象级风险和上下文进行建模。每个对象表示为一个带有相关风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称地传播到低风险对象。这使得机器人能够推断潜在的隐患，即使它们没有明确可见或标记。为便于解释和轻量级机载部署而设计，我们的方法在带有人工标注风险区域的数据集上进行了验证，实现了二值风险检测75%的准确率。该系统与人类感知高度一致，特别是在涉及尖锐或不稳定物体的场景中。这些结果强调了上下文感知风险推理在增强机器人场景理解和共享人机空间中主动安全行为的潜力。该框架可作为未来系统能够做出上下文驱动的安全决策、提供实时警报或在家庭环境中自主协助用户避免或减轻危害的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是让服务机器人在家庭环境中能够感知和估计事故风险，特别是那些基于上下文的潜在风险。这个问题很重要，因为随着机器人越来越多地进入家庭环境，它们需要能够预见和响应环境中的危险以确保用户安全、建立信任和实现有效的人机交互。在老龄化社会中，家庭安全是一个紧迫问题，能够预测风险的服务机器人可以发挥重要作用，同时主动识别和减轻风险的机器人能增强用户信任，特别对弱势群体而言。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有机器人系统主要关注几何或语义识别，但服务机器人需要进一步预测环境中的上下文风险。他们指出了现有方法的局限性，如仅基于观察对象确定风险会导致误判，或需要静态规则。作者借鉴了场景图表示法来建模对象关系，参考了图神经网络技术改进安全感知，并利用大型语言模型和3D模拟环境等现有技术，但进行了改进以适应风险传播的需求。他们设计了一个轻量级框架，整合对象级风险估计、语义空间关系和通过场景图的不对称风险传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于语义图的风险传播算法建模对象级风险和上下文，每个对象表示为带有风险分数的节点，风险根据空间接近度和事故关系从高风险对象不对称地传播到低风险对象，使机器人能推断潜在风险。整体流程包括：1)风险估计：使用日本消费者事务厅的40万份事故报告数据计算对象风险分数；2)风险传播：构建场景图，通过事故相关性、空间距离和风险差异三个因素迭代更新对象风险分数；3)可视化：生成RGB热图展示空间风险分布，使用高斯滤波器平滑高低风险区域过渡。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于语义图的风险估计框架；2)不对称风险扩散算法；3)与人类感知高度一致的验证结果；4)适合实时部署的轻量级设计。相比之前的工作，不同之处在于：1)直接处理RGB-D视觉数据而非依赖中间文本表示；2)使用真实世界事故统计数据而非仅靠语言模型；3)风险只从高风险对象传播到低风险对象，实现不对称传播；4)能推断间接风险，如架子边缘的碗放在滑表面上；5)通过动态调整风险值减少误报。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于语义图的不对称风险传播框架，使服务机器人能够通过分析对象间的空间关系和语义相关性，在家庭环境中准确识别和推断潜在的事故风险，显著提高了机器人对人类风险感知的模拟程度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel framework for estimating accident-prone regions ineveryday indoor scenes, aimed at improving real-time risk awareness in servicerobots operating in human-centric environments. As robots become integratedinto daily life, particularly in homes, the ability to anticipate and respondto environmental hazards is crucial for ensuring user safety, trust, andeffective human-robot interaction. Our approach models object-level risk andcontext through a semantic graph-based propagation algorithm. Each object isrepresented as a node with an associated risk score, and risk propagatesasymmetrically from high-risk to low-risk objects based on spatial proximityand accident relationship. This enables the robot to infer potential hazardseven when they are not explicitly visible or labeled. Designed forinterpretability and lightweight onboard deployment, our method is validated ona dataset with human-annotated risk regions, achieving a binary risk detectionaccuracy of 75%. The system demonstrates strong alignment with humanperception, particularly in scenes involving sharp or unstable objects. Theseresults underline the potential of context-aware risk reasoning to enhancerobotic scene understanding and proactive safety behaviors in sharedhuman-robot spaces. This framework could serve as a foundation for futuresystems that make context-driven safety decisions, provide real-time alerts, orautonomously assist users in avoiding or mitigating hazards within homeenvironments.</description>
      <author>example@mail.com (Sena Ishii, Akash Chikhalikar, Ankit A. Ravankar, Jose Victorio Salazar Luces, Yasuhisa Hirata)</author>
      <guid isPermaLink="false">2508.19788v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>LabelGS: Label-Aware 3D Gaussian Splatting for 3D Scene Segmentation</title>
      <link>http://arxiv.org/abs/2508.19699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PRCV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LabelGS是一种通过为3D高斯表示增加对象标签来增强3DGS分割能力的方法，实现了高效的3D场景分割，相比Feature-3DGS有显著的速度提升和性能改进。&lt;h4&gt;背景&lt;/h4&gt;3D Gaussian Splatting (3DGS)是一种新兴的3D场景显式表示方法，能够提供高保真度的重建和高效的渲染，但它缺乏3D分割能力，限制了在需要场景理解的任务中的适用性。&lt;h4&gt;目的&lt;/h4&gt;解决3DGS缺乏3D分割能力的问题，提高其在需要场景理解的任务中的适用性，实现对特定对象组件的识别和隔离。&lt;h4&gt;方法&lt;/h4&gt;提出Label-aware 3D Gaussian Splatting (LabelGS)，引入跨视图一致的3D高斯语义掩码，采用遮挡分析模型避免优化过程中对遮挡的过拟合，使用主高斯标记模型将2D语义先验提升到3D高斯，应用高斯投影过滤器避免标签冲突，并通过随机区域采样策略实现高斯表示的有效解耦。&lt;h4&gt;主要发现&lt;/h4&gt;LabelGS在3D场景分割任务中超越了之前的最先进方法，包括Feature-3DGS；在1440X1080分辨率下，相比Feature-3DGS实现了22倍训练速度提升。&lt;h4&gt;结论&lt;/h4&gt;LabelGS成功解决了3DGS缺乏3D分割能力的问题，通过增强高斯表示和优化过程，显著提高了3D场景分割的效率和性能。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯泼溅(3DGS)已成为一种新颖的3D场景显式表示方法，提供高保真度重建和高效渲染。然而，3DGS缺乏3D分割能力，这限制了它在需要场景理解的任务中的适用性。识别和隔离特定对象组件至关重要。为解决这一局限性，我们提出了标签感知3D高斯泼溅(LabelGS)，一种通过为高斯表示增加对象标签来增强3DGS的方法。LabelGS为3D高斯引入了跨视图一致的语义掩码，并采用了一种新的遮挡分析模型来避免在优化过程中对遮挡进行过拟合，主高斯标记模型将2D语义先验提升到3D高斯，以及高斯投影过滤器来避免高斯标签冲突。我们的方法实现了高斯表示的有效解耦，并通过随机区域采样策略改进了3DGS优化过程，显著提高了效率。大量实验表明，LabelGS在3D场景分割任务中超越了之前的最先进方法，包括Feature-3DGS。值得注意的是，在1440X1080分辨率下，LabelGS相比Feature-3DGS实现了显著的22倍训练速度提升。我们的代码将在https://github.com/garrisonz/LabelGS上提供。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D高斯溅射(3DGS)缺乏3D分割能力的问题。这个问题在现实和研究中非常重要，因为3D场景分割是理解和解释复杂3D环境的基础任务，对于3D医学数据分析、机器人导航、自动驾驶等多种应用至关重要。缺乏语义标注的3D数据一直是3D场景分割的挑战，因此需要一种高效的方法来实现3D分割。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到3DGS虽然能高效渲染3D场景但缺乏分割能力，然后分析了现有2D到3D分割方法的局限性：难以捕捉3D空间特性和计算成本高。作者借鉴了DEVA模型获取跨视图掩码、3DGS的基本表示方法，但创新性地为高斯添加了标签属性。设计思路是通过给高斯表示添加对象标签来增强其分割能力，同时解决遮挡问题和标签冲突问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是为3D高斯添加标签属性，使每个高斯能够携带语义信息，并通过跨视图一致的语义掩码来优化高斯模型。整体流程包括：1)使用DEVA模型获取跨视图一致的语义掩码；2)通过遮挡分析模型(OAM)处理遮挡区域；3)使用主高斯标记策略(MGL)将2D标签提升到3D高斯；4)引入高斯投影过滤器(GPF)解决标签冲突；5)结合多种损失函数优化模型；6)通过用户选择目标掩码实现3D对象分割和渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)跨视图一致的语义掩码获取；2)遮挡分析模型(OAM)解决遮挡问题；3)主高斯标记策略(MGL)实现2D到3D标签提升；4)高斯投影过滤器(GPF)解决标签冲突。相比之前的工作，LabelGS不需要学习高维语义特征(如Feature-3DGS的512维特征)，不使用自编码器压缩语义信息(如LangSplat)，不依赖视频跟踪模型进行聚类(如Gaussian Grouping)，而是直接为高斯分配标签，实现了更精确的边界分割和22倍的速度提升。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LabelGS通过为3D高斯添加标签属性，结合创新的遮挡处理和标签提升机制，实现了高效且精确的3D场景分割，在保持高质量分割结果的同时显著提高了训练速度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3DGS) has emerged as a novel explicit representationfor 3D scenes, offering both high-fidelity reconstruction and efficientrendering. However, 3DGS lacks 3D segmentation ability, which limits itsapplicability in tasks that require scene understanding. The identification andisolating of specific object components is crucial. To address this limitation,we propose Label-aware 3D Gaussian Splatting (LabelGS), a method that augmentsthe Gaussian representation with object label.LabelGS introduces cross-viewconsistent semantic masks for 3D Gaussians and employs a novel OcclusionAnalysis Model to avoid overfitting occlusion during optimization, MainGaussian Labeling model to lift 2D semantic prior to 3D Gaussian and GaussianProjection Filter to avoid Gaussian label conflict. Our approach achieveseffective decoupling of Gaussian representations and refines the 3DGSoptimization process through a random region sampling strategy, significantlyimproving efficiency. Extensive experiments demonstrate that LabelGSoutperforms previous state-of-the-art methods, including Feature-3DGS, in the3D scene segmentation task. Notably, LabelGS achieves a remarkable 22X speedupin training compared to Feature-3DGS, at a resolution of 1440X1080. Our codewill be at https://github.com/garrisonz/LabelGS.</description>
      <author>example@mail.com (Yupeng Zhang, Dezhi Zheng, Ping Lu, Han Zhang, Lei Wang, Liping xiang, Cheng Luo, Kaijun Deng, Xiaowen Fu, Linlin Shen, Jinbao Wang)</author>
      <guid isPermaLink="false">2508.19699v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Object Detection in the Car Interior With Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.19651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种用于汽车内部场景理解的物体检测与定位框架，通过分布式架构克服车载系统计算资源限制，微调后的轻量模型性能显著优于基础模型。&lt;h4&gt;背景&lt;/h4&gt;汽车内部AI任务如识别和定位外部引入物体对个人助手响应质量至关重要，但车载系统计算资源有限，限制了这些解决方案的直接部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种克服车载系统资源限制的物体检测与定位框架，用于汽车内部场景理解。&lt;h4&gt;方法&lt;/h4&gt;提出ODAL框架，利用视觉基础模型，采用分布式架构将计算任务分配在车载系统和云端之间，并引入ODALbench作为评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的ODAL-LLaVA模型达到89%的ODAL分数，比基线性能提高71%，比GPT-4o高出近20%；同时保持高检测准确度，显著减少幻觉，ODAL信噪比比GPT-4o高三倍。&lt;h4&gt;结论&lt;/h4&gt;该框架有潜力在该领域建立新标准，轻量级微调模型在性能上优于大型基础模型。&lt;h4&gt;翻译&lt;/h4&gt;汽车内部AI任务如识别和定位外部引入物体对个人助手响应质量至关重要，但车载系统计算资源有限，限制了这些解决方案的直接部署。为解决这一限制，我们提出了新颖的物体检测与定位框架用于内部场景理解。我们的方法通过分布式架构利用视觉基础模型，将计算任务分配在车载和云端之间。这种设计克服了在车内直接运行基础模型的资源限制。为评估模型性能，我们引入了ODALbench作为检测和定位综合评估的新指标。我们的分析表明该框架有潜力在该领域建立新标准。我们将最先进的GPT-4o视觉基础模型与轻量级LLaVA 1.5 7B模型进行比较，并探索微调如何提升轻量模型的性能。值得注意的是，我们微调的ODAL-LLaVA模型达到89%的ODAL分数，比基线性能提高71%，比GPT-4o高出近20%。此外，微调后的模型在保持高检测准确度的同时显著减少了幻觉，ODAL信噪比比GPT-4o高三倍。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在汽车内部环境中识别和定位外部引入对象的问题，以及车载系统计算资源有限无法直接部署高性能视觉模型的挑战。这个问题很重要，因为车内物体识别能提升个人助手响应质量，例如提供不要忘记随身物品的提醒；同时传统模型只能识别训练过的物体类别，而实际应用中会遇到各种未知物体，需要更灵活的解决方案。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统CNN模型在开放集识别和提供人类可理解位置描述方面的局限，发现视觉基础模型虽有强大泛化能力但计算需求高。因此设计了分布式架构将任务分配在车载和云端之间。借鉴了现有视觉基础模型(特别是LLaVA 1.5)、分布式计算架构和LoRA微调技术，但针对车内场景进行了专门优化和整合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建ODAL框架，通过分布式架构将计算任务在车载系统和云端之间分配，利用微调后的轻量级模型实现高性能物体检测。流程：1)车载系统捕获车内图像；2)在车载端使用视觉编码器处理图像生成嵌入；3)将嵌入(非原始图像)传输到云端；4)云端使用微调后的LLaVA模型进行物体检测和定位；5)生成结构化JSON响应；6)返回结果供车载应用使用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)ODAL分布式框架解决车载资源限制；2)ODALbench评估指标(ODALscore和ODALSNR)专门评估车内物体检测；3)微调策略使轻量模型达到高性能；4)微调后模型ODALscore达89%，比基线提高71%，超越GPT-4o近20%；5)ODALSNR达7.14，是GPT-4o的三倍，显著减少错误检测。相比传统方法，ODAL能处理开放词汇物体，而不仅限于预定义类别；相比直接使用重型模型，ODAL通过分布式架构和微调实现了资源高效利用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种名为ODAL的分布式车内物体检测和定位框架，通过微调轻量级视觉基础模型，在有限计算资源条件下实现了超越重型模型的性能，并为此领域引入了新的评估标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI tasks in the car interior like identifying and localizing externallyintroduced objects is crucial for response quality of personal assistants.However, computational resources of on-board systems remain highly constrained,restricting the deployment of such solutions directly within the vehicle. Toaddress this limitation, we propose the novel Object Detection and Localization(ODAL) framework for interior scene understanding. Our approach leveragesvision foundation models through a distributed architecture, splittingcomputational tasks between on-board and cloud. This design overcomes theresource constraints of running foundation models directly in the car. Tobenchmark model performance, we introduce ODALbench, a new metric forcomprehensive assessment of detection and localization.Our analysisdemonstrates the framework's potential to establish new standards in thisdomain. We compare the state-of-the-art GPT-4o vision foundation model with thelightweight LLaVA 1.5 7B model and explore how fine-tuning enhances thelightweight models performance. Remarkably, our fine-tuned ODAL-LLaVA modelachieves an ODAL$_{score}$ of 89%, representing a 71% improvement over itsbaseline performance and outperforming GPT-4o by nearly 20%. Furthermore, thefine-tuned model maintains high detection accuracy while significantly reducinghallucinations, achieving an ODAL$_{SNR}$ three times higher than GPT-4o.</description>
      <author>example@mail.com (Bálint Mészáros, Ahmet Firintepe, Sebastian Schmidt, Stephan Günnemann)</author>
      <guid isPermaLink="false">2508.19651v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Staircase Recognition and Location Based on Polarization Vision</title>
      <link>http://arxiv.org/abs/2505.19026v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文主要研究楼梯场景的感知和三维重建技术，提出了一种结合偏振和光强信息的对比度增强算法，以及基于YOLOv11的点云分割方法。此外，还提出了一种融合偏振双目和TOF深度信息的三维重建方法，以及基于ICP配准和改进灰狼优化算法的单目相机与TOF相机的联合标定算法。&lt;h4&gt;背景&lt;/h4&gt;楼梯是人工场景中最常见的结构之一，但对人形机器人和下肢残疾或视觉障碍的人来说，在没有传感器和智能算法的帮助下很难穿越。楼梯场景感知技术是识别和定位的前提，对机器人的模式切换和足迹位置计算以适应不连续地形具有重要意义。然而，该技术应用仍存在许多问题，如识别精度低、传感器初始噪声高、输出信号不稳定和计算要求高。&lt;h4&gt;目的&lt;/h4&gt;实现楼梯的检测，并实现高质量的三维重建。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合偏振和光强信息的对比度增强算法，集成了基于YOLOv11的点云分割，提出了融合偏振双目和TOF深度信息的三维重建方法，以及基于ICP配准和改进灰狼优化算法的单目相机与TOF相机的联合标定算法。&lt;h4&gt;主要发现&lt;/h4&gt;双目和飞行时间(TOF)重建容易受到环境光和目标物体表面材质的影响，而偏振重建方法依赖于物体表面的偏振信息，优点是不易受环境光影响且不依赖于物体表面的纹理信息。&lt;h4&gt;结论&lt;/h4&gt;通过结合偏振和光强信息，以及融合偏振双目和TOF深度信息，可以实现楼梯的高质量检测和三维重建。&lt;h4&gt;翻译&lt;/h4&gt;楼梯是人工场景中最常见的结构之一。然而，对于人形机器人和下肢残疾或视觉障碍的人来说，在没有传感器和智能算法的帮助下很难穿越场景。楼梯场景感知技术是识别和定位的前提。这项技术对机器人的模式切换和足迹位置计算以适应不连续地形具有重要意义。然而，仍然有许多问题限制了这项技术的应用，如识别精度低、传感器初始噪声高、输出信号不稳定和计算要求高。在场景重建方面，双目和飞行时间(TOF)重建容易受到环境光和目标物体表面材质的影响。相比之下，由于偏振器的特殊结构，偏振可以选择性地传输特定方向的偏振光，这种重建方法依赖于物体表面的偏振信息。因此，偏振重建的优点得以体现，即不易受环境光影响且不依赖于物体表面的纹理信息。在本文中，为了实现楼梯的检测，提出了一种结合偏振和光强信息的对比度增强算法，并集成了基于YOLOv11的点云分割。为了实现高质量重建，我们提出了一种融合偏振双目和TOF深度信息的方法，实现了楼梯的三维(3D)重建。此外，还提出了一种基于ICP配准和改进灰狼优化算法的单目相机与TOF相机的联合标定算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决楼梯识别和定位的准确性问题，以及现有方法的光照敏感性和纹理依赖性问题。这个问题在现实中非常重要，因为楼梯感知对于人形机器人和行动不便的个体至关重要，关系到机器人在楼梯场景的导航控制性能，以及帮助行动不便者安全上下楼梯，提高生活质量。在康复医学领域，准确的楼梯识别对步态控制也至关重要，步态障碍可能导致运动停止、跌倒和死亡率增加。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有楼梯识别和3D重建方法的局限性来设计新方法。在楼梯识别方面，借鉴了YOLO算法和Hough变换技术；在3D重建方面，借鉴了偏振成像与结构光结合的方法、基于形状从阴影的技术和特征点方法。作者的主要创新在于提出了一个多传感器融合框架，结合偏振视觉、双目视觉和TOF深度信息，克服了单一方法的局限性。具体来说，作者改进了灰狼优化算法引入Levy飞行和动态权重，设计了偏振-强度图像融合方法，并提出了双目和TOF融合策略来校正偏振梯度场的模糊问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用偏振视觉获取更多维度的信息，结合多种传感器优势，实现鲁棒的楼梯检测和高精度的3D重建。整体实现流程分为三个模块：1) 楼梯识别模块：使用YOLOv11进行初步检测，结合偏振-强度对比度增强算法提高图像对比度，使用点云分割和法向量分析确认楼梯方向；2) 异构传感器校准模块：使用改进的灰狼优化算法进行偏振相机和TOF相机之间的校准，解决不同分辨率相机间的校准问题；3) 偏振3D重建模块：利用双目视觉校正偏振梯度场的方位角模糊，使用TOF深度信息填充双目重建中的数据空洞，通过积分算法获得表面的相对高度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 楼梯识别模块：结合偏振-强度对比度增强算法和点云分割，提高了识别准确率；2) 异构传感器校准模块：提出适用于不同分辨率相机的联合校准算法，改进灰狼优化算法引入Levy飞行和动态权重；3) 偏振3D重建模块：首次将双目偏振视觉与TOF传感融合用于3D重建，解决了偏振重建中的方位角模糊和双目重建中的数据空洞问题。相比之前工作，本文方法结合了多种传感器的优势，不需要成对的偏振-深度数据集，支持单帧成像适用于移动机器人动态场景，实现了更高的识别精度(98.7%)和更低的重建误差(&lt;0.2%)。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于偏振视觉的多传感器融合方法，通过结合偏振-强度图像、双目视觉和TOF深度信息，实现了高精度的楼梯识别和低误差的3D重建，为机器人提供了准确的楼梯场景感知能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Staircase is one of the most common structures in artificial scenes. However,it is difficult for humanoid robots and people with lower limb disabilities orvisual impairment to cross the scene without the help of sensors andintelligent algorithms. Staircase scene perception technology is a prerequisitefor recognition and localization. This technology is of great significance forthe mode switching of the robot and the calculation of the footprint positionto adapt to the discontinuous terrain. However, there are still many problemsthat constrain the application of this technology, such as low recognitionaccuracy, high initial noise from sensors, unstable output signals and highcomputational requirements. In terms of scene reconstruction, the binocular andtime of flight (TOF) reconstruction of the scene can be easily affected byenvironmental light and the surface material of the target object. In contrast,due to the special structure of the polarizer, the polarization can selectivelytransmit polarized light in a specific direction and this reconstruction methodrelies on the polarization information of the object surface. So the advantagesof polarization reconstruction are reflected, which are less affected byenvironmental light and not dependent on the texture information of the objectsurface. In this paper, in order to achieve the detection of staircase, thispaper proposes a contrast enhancement algorithm that integrates polarizationand light intensity information, and integrates point cloud segmentation basedon YOLOv11. To realize the high-quality reconstruction, we proposed a method offusing polarized binocular and TOF depth information to realize thethree-dimensional (3D) reconstruction of the staircase. Besides, it alsoproposes a joint calibration algorithm of monocular camera and TOF camera basedon ICP registration and improved gray wolf optimization algorithm.</description>
      <author>example@mail.com (Weifeng Kong, Zhiying Tan)</author>
      <guid isPermaLink="false">2505.19026v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>GegenNet: Spectral Convolutional Neural Networks for Link Sign Prediction in Signed Bipartite Graphs</title>
      <link>http://arxiv.org/abs/2508.19907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages. Paper accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GegenNet，一种针对符号二分图链接符号预测的新型谱卷积神经网络模型。通过三种技术创新：基于谱分解的节点特征初始化、基于Gegenbauer多项式基的谱图滤波器以及符号感知的多层谱卷积网络，GegenNet在多个基准数据集上取得了显著优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;现有链接符号预测方法主要针对单符号图，忽略了节点异质性和符号二分图的独特特性。虽然最近的研究将图神经网络适应到符号二分图中，但基础谱卷积算子最初是为无符号图中的正链接设计的，不适合从已知链接推断缺失的正负链接。&lt;h4&gt;目的&lt;/h4&gt;开发一种专门针对符号二分图的谱卷积神经网络模型，能够有效预测潜在链接的符号，解决现有方法在处理符号二分图时的局限性。&lt;h4&gt;方法&lt;/h4&gt;GegenNet包含三个主要技术贡献：(1) 快速且有理论基础的谱分解技术用于节点特征初始化；(2) 基于Gegenbauer多项式基的新谱图滤波器；(3) 多层符号感知谱卷积网络，交替使用Gegenbauer多项式滤波器和正负边。这些技术共同增强了模型容量，提高了预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在6个基准符号二分图数据集上的广泛实证研究表明，与11个强竞争对手相比，GegenNet在链接符号预测任务中实现了显著优越的性能，AUC最高提升4.28%，F1值最高提升11.69%。&lt;h4&gt;结论&lt;/h4&gt;GegenNet通过创新的谱卷积技术有效解决了符号二分图链接符号预测问题，为处理包含正负链接的复杂网络关系提供了新的有效方法，具有重要的理论和实践意义。&lt;h4&gt;翻译&lt;/h4&gt;给定一个具有两个不相交节点集U和V的符号二分图（SBG）G，链接符号预测的目标是基于G中已知的正边和负边来预测连接U和V的潜在链接的符号。现有的大多数链接符号预测解决方案主要针对单符号图，由于忽略了节点异质性和SBG的独特二分特性，这些方法次优。为此，最近的研究通过引入用于分区间（U×V）和分区内（U×U或V×V）节点对的消息传递方案，将图神经网络适应到SBG中。然而，基础的谱卷积算子最初是为无符号图中的正链接设计的，因此不适合从已知链接推断SBG中缺失的正链接或负链接。受此启发，本文提出了GegenNet，一种用于SBG链接符号预测的新型有效的谱卷积神经网络模型。特别是，GegenNet通过三个主要技术贡献实现了增强的模型容量和高度准确的预测：(i) 用于节点特征初始化的快速且有理论基础的谱分解技术；(ii) 基于Gegenbauer多项式基的新谱图滤波器；以及(iii) 多层符号感知谱卷积网络，交替使用Gegenbauer多项式滤波器和正负边。我们大量的实证研究表明，与6个基准SBG数据集上的11个强竞争对手相比，GegenNet在链接符号预测中可以实现显著优越的性能（AUC最高提升4.28%，F1最高提升11.69%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given a signed bipartite graph (SBG) G with two disjoint node sets U and V,the goal of link sign prediction is to predict the signs of potential linksconnecting U and V based on known positive and negative edges in G. Themajority of existing solutions towards link sign prediction mainly focus onunipartite signed graphs, which are sub-optimal due to the neglect of nodeheterogeneity and unique bipartite characteristics of SBGs. To this end, recentstudies adapt graph neural networks to SBGs by introducing message-passingschemes for both inter-partition (UxV) and intra-partition (UxU or VxV) nodepairs. However, the fundamental spectral convolutional operators wereoriginally designed for positive links in unsigned graphs, and thus, are notoptimal for inferring missing positive or negative links from known ones inSBGs.  Motivated by this, this paper proposes GegenNet, a novel and effectivespectral convolutional neural network model for link sign prediction in SBGs.In particular, GegenNet achieves enhanced model capacity and high predictiveaccuracy through three main technical contributions: (i) fast and theoreticallygrounded spectral decomposition techniques for node feature initialization;(ii) a new spectral graph filter based on the Gegenbauer polynomial basis; and(iii) multi-layer sign-aware spectral convolutional networks alternatingGegenbauer polynomial filters with positive and negative edges. Our extensiveempirical studies reveal that GegenNet can achieve significantly superiorperformance (up to a gain of 4.28% in AUC and 11.69% in F1) in link signprediction compared to 11 strong competitors over 6 benchmark SBG datasets.</description>
      <author>example@mail.com (Hewen Wang, Renchi Yang, Xiaokui Xiao)</author>
      <guid isPermaLink="false">2508.19907v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>InfraredGP: Efficient Graph Partitioning via Spectral Graph Neural Networks with Negative Corrections</title>
      <link>http://arxiv.org/abs/2508.19737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;InfraredGP是一种创新的图分割方法，通过负校正机制放大传统范围外的低频信息，无需训练即可获得高质量的社区检测结果。&lt;h4&gt;背景&lt;/h4&gt;图分割(GP)，也称为社区检测，是一个将图节点划分为密集连接块的经典问题。从图信号处理角度，带有负校正的图拉普拉斯矩阵可推导出超出传统频率范围[0,2]的图频率。&lt;h4&gt;目的&lt;/h4&gt;探索超出传统范围[0,2]的低频信息是否能编码更多关于社区结构的信息性属性。&lt;h4&gt;方法&lt;/h4&gt;InfraredGP采用谱GNN作为骨干网络，结合低通滤波器和负校正机制，仅提供随机输入，通过前向传播导出图嵌入无需训练，最后将嵌入输入BIRCH算法获得图分割结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，仅基于放大[0,2]范围外低频信息的负校正机制，InfraredGP可为标准聚类模块推导出可区分的嵌入，无需训练即可获得高质量的图分割结果，并在效率上比基线方法快16-23倍。&lt;h4&gt;结论&lt;/h4&gt;InfraredGP通过负校正机制有效利用了传统范围外的低频信息，实现了无需训练的高效图分割，在静态和流式图分割任务中均表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图分割(GP)，也称为社区检测，是一个将图节点划分为密集连接块的经典问题。从图信号处理的角度，我们发现带有负校正的图拉普拉斯矩阵可以推导出超出传统范围[0,2]的图频率。为探索超出此范围的低频信息是否能编码更多关于社区结构的信息性属性，我们提出了InfraredGP。它(1)采用谱GNN作为骨干网络，结合低通滤波器和负校正机制，(2)仅向骨干网络提供随机输入，(3)通过前向传播(FFP)导出图嵌入而无需任何训练，以及(4)通过将导出的嵌入输入BIRCH算法获得可行的图分割结果。令人惊讶的是，实验表明，仅基于放大[0,2]范围外低频信息的负校正机制，InfraredGP就能为一些标准聚类模块(如BIRCH)推导出可区分的嵌入，无需任何训练即可获得高质量的图分割结果。按照IEEE HPEC图挑战基准，我们对InfraredGP进行了静态和流式图分割评估，InfraredGP在各种基线上实现了更好的效率(例如快16-23倍)和竞争性的质量。我们已将代码公开在https://github.com/KuroginQin/InfraredGP&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph partitioning (GP), a.k.a. community detection, is a classic problemthat divides nodes of a graph into densely-connected blocks. From a perspectiveof graph signal processing, we find that graph Laplacian with a negativecorrection can derive graph frequencies beyond the conventional range $[0, 2]$.To explore whether the low-frequency information beyond this range can encodemore informative properties about community structures, we propose InfraredGP.It (\romannumeral1) adopts a spectral GNN as its backbone combined withlow-pass filters and a negative correction mechanism, (\romannumeral2) onlyfeeds random inputs to this backbone, (\romannumeral3) derives graph embeddingsvia one feed-forward propagation (FFP) without any training, and(\romannumeral4) obtains feasible GP results by feeding the derived embeddingsto BIRCH. Surprisingly, our experiments demonstrate that based solely on thenegative correction mechanism that amplifies low-frequency information beyond$[0, 2]$, InfraredGP can derive distinguishable embeddings for some standardclustering modules (e.g., BIRCH) and obtain high-quality results for GP withoutany training. Following the IEEE HPEC Graph Challenge benchmark, we evaluateInfraredGP for both static and streaming GP, where InfraredGP can achieve muchbetter efficiency (e.g., 16x-23x faster) and competitive quality over variousbaselines. We have made our code public athttps://github.com/KuroginQin/InfraredGP</description>
      <author>example@mail.com (Meng Qin, Weihua Li, Jinqiang Cui, Sen Pei)</author>
      <guid isPermaLink="false">2508.19737v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>UTAL-GNN: Unsupervised Temporal Action Localization using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19647v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted at the ICIP Satellite Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种轻量级无监督骨架动作定位方法，利用时空图神经网络表示，通过预训练ASTGCN网络和定义动作动力学指标，实现了高效的动作边界检测，在保持计算效率的同时达到了与最先进监督方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;在未修剪的体育视频中实现细粒度动作定位具有挑战性，挑战源于短时间内快速而细微的动作转换。现有的监督和弱监督方法通常依赖大量标注数据和高容量模型，导致计算密集且难以适应实际场景。&lt;h4&gt;目的&lt;/h4&gt;引入一种轻量级的无监督骨架动作定位方法，利用时空图神经网络表示，减少对标注数据的依赖，提高计算效率并适应实际应用场景。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于骨架的无监督动作定位流程，预训练基于注意力的时空图卷积网络(ASTGCN)用于姿态序列去噪任务，使用分块分区方式学习内在运动动力学，定义新的动作动力学指标(ADM)从低维ASTGCN嵌入中直接计算，通过识别曲率剖面的拐点检测动作边界。&lt;h4&gt;主要发现&lt;/h4&gt;在DSV Diving数据集上达到82.66%的平均精度(mAP)，平均定位延迟为29.09毫秒，与最先进的监督方法性能相当同时保持计算效率，能够很好地泛化到未见过的野外跳水视频，无需重新训练。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了在嵌入式或动态环境中实现轻量级、实时动作分析系统的实际应用价值，证明无监督方法可以达到与监督方法相当的性能。&lt;h4&gt;翻译&lt;/h4&gt;在未修剪的体育视频中实现细粒度动作定位是一个重大挑战，这是由于短时间内快速而细微的动作转换。现有的监督和弱监督解决方案通常依赖大量标注数据和高容量模型，使它们计算密集且难以适应实际场景。在这项工作中，我们引入了一种轻量级和无监督的基于骨架的动作定位流程，利用时空图神经网络表示。我们的方法在块状分区的姿态序列去噪任务上预训练了一个基于注意力的时空图卷积网络(ASTGCN)，使其能够在没有任何手动标注的情况下学习内在运动动力学。在推理时，我们定义了一个新的动作动力学指标(ADM)，直接从低维ASTGCN嵌入计算，通过识别其曲率剖面的拐点来检测动作边界。我们的方法在DSV Diving数据集上达到了82.66%的平均精度(mAP)和29.09毫秒的平均定位延迟，与最先进的监督性能相匹配，同时保持计算效率。此外，它可以强大地泛化到未见过的野外跳水视频而无需重新训练，证明了其在嵌入式或动态环境中轻量级、实时动作分析系统的实际应用性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在未修剪的体育视频中实现细粒度动作定位的问题，特别是跳水视频中精确检测动作边界。这个问题在现实中很重要，因为它能帮助教练和运动员分析技术动作、优化训练策略和提高比赛表现。现有方法大多依赖大量标注数据和高容量模型，计算密集且难以适应现实场景，而跳水等运动中动作转换迅速且微妙，持续时间短，增加了定位难度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到体育动作定位中标注数据稀缺的问题，转向无监督学习以提高可扩展性和适应性。他们选择基于骨架的方法而非原始视频，以减少计算复杂度。借鉴了ASTGCN作为基础架构，并受先前几何曲率编码方法的启发。创新点在于设计了块状分区的预训练任务和动作动力学指标(ADM)，通过理论证明拐点对应于动作转换状态，从而实现无监督的动作边界检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用图神经网络从人体骨架序列中学习时空表示，通过预训练去噪任务学习动作内在动态特性，无需人工标注，并设计动作动力学指标(ADM)通过曲率变化检测动作边界。训练阶段：将姿态序列划分为重叠子序列，添加噪声，用ASTGCN处理生成嵌入，通过全连接层重建去噪姿态。推理阶段：将姿态序列划分为子序列，提取时空嵌入，计算欧几里得范数得到ADM，分析曲率变化检测拐点作为动作转换点，映射回原始时间轴确定动作边界。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全无监督的基于骨架的动作定位框架；2)块状预训练策略而非整个视频序列；3)动作动力学指标(ADM)直接从低维嵌入计算；4)理论证明拐点对应动作转换状态；5)高效实时性能(29.09ms延迟)。相比之前工作，此方法不需要标注数据，计算效率更高，在DSV数据集上达到82.66%的mAP与监督方法相当；不依赖聚类和自 paced 学习，能更好地处理体育动作中的细粒度转换，提供了更直观的可视化和解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于图神经网络的无监督时空动作定位方法，通过设计动作动力学指标和理论证明，实现了在无需标注的情况下精确检测跳水视频中的动作转换边界，同时保持高效计算性能和强泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-grained action localization in untrimmed sports videos presents asignificant challenge due to rapid and subtle motion transitions over shortdurations. Existing supervised and weakly supervised solutions often rely onextensive annotated datasets and high-capacity models, making themcomputationally intensive and less adaptable to real-world scenarios. In thiswork, we introduce a lightweight and unsupervised skeleton-based actionlocalization pipeline that leverages spatio-temporal graph neuralrepresentations. Our approach pre-trains an Attention-based Spatio-TemporalGraph Convolutional Network (ASTGCN) on a pose-sequence denoising task withblockwise partitions, enabling it to learn intrinsic motion dynamics withoutany manual labeling. At inference, we define a novel Action Dynamics Metric(ADM), computed directly from low-dimensional ASTGCN embeddings, which detectsmotion boundaries by identifying inflection points in its curvature profile.Our method achieves a mean Average Precision (mAP) of 82.66% and averagelocalization latency of 29.09 ms on the DSV Diving dataset, matchingstate-of-the-art supervised performance while maintaining computationalefficiency. Furthermore, it generalizes robustly to unseen, in-the-wild divingfootage without retraining, demonstrating its practical applicability forlightweight, real-time action analysis systems in embedded or dynamicenvironments.</description>
      <author>example@mail.com (Bikash Kumar Badatya, Vipul Baghel, Ravi Hegde)</author>
      <guid isPermaLink="false">2508.19647v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes</title>
      <link>http://arxiv.org/abs/2508.19356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 to 4 hours read time. 73 pages. 35 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了图数据建模在化学科学中的应用，特别是在描述分子、蛋白质和化学过程方面，展示了图神经网络等学习算法如何在这些图结构上操作，为读者应用图方法到化学发现提供了基础。&lt;h4&gt;背景&lt;/h4&gt;图是化学科学的核心工具，提供了描述分子、蛋白质、反应和工业过程的自然语言，能够捕获支撑材料、生物学和医学的相互作用和结构。&lt;h4&gt;目的&lt;/h4&gt;这篇入门介绍旨在将图作为化学中的数学对象进行介绍，展示学习算法如何在图上操作，并为读者应用图方法到下一代化学发现做好准备。&lt;h4&gt;方法&lt;/h4&gt;论文介绍了图神经网络等学习算法如何在图结构上操作，以及图设计的基础、关键预测任务和化学科学中的代表性例子。&lt;h4&gt;主要发现&lt;/h4&gt;图设计的基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图的建模中的作用是论文的主要发现。&lt;h4&gt;结论&lt;/h4&gt;这些共同的概念为读者应用图方法到下一代化学发现做好了准备。&lt;h4&gt;翻译&lt;/h4&gt;图是化学科学的核心，提供了描述分子、蛋白质、反应和工业过程的自然语言。它们捕获了支撑材料、生物学和医学的相互作用和结构。这篇入门介绍《图数据建模：分子、蛋白质与化学过程》将图作为化学中的数学对象进行介绍，并展示了学习算法（特别是图神经网络）如何在图上操作。我们概述了图设计的基础、关键预测任务、化学科学中的代表性例子以及机器学习在基于图的建模中的作用。这些概念共同为读者应用图方法到下一代化学发现做好了准备。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acsinfocus.7e9017&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are central to the chemical sciences, providing a natural language todescribe molecules, proteins, reactions, and industrial processes. They captureinteractions and structures that underpin materials, biology, and medicine.This primer, Graph Data Modeling: Molecules, Proteins, &amp; Chemical Processes,introduces graphs as mathematical objects in chemistry and shows how learningalgorithms (particularly graph neural networks) can operate on them. We outlinethe foundations of graph design, key prediction tasks, representative examplesacross chemical sciences, and the role of machine learning in graph-basedmodeling. Together, these concepts prepare readers to apply graph methods tothe next generation of chemical discovery.</description>
      <author>example@mail.com (José Manuel Barraza-Chavez, Rana A. Barghout, Ricardo Almada-Monter, Benjamin Sanchez-Lengeling, Adrian Jinich, Radhakrishnan Mahadevan)</author>
      <guid isPermaLink="false">2508.19356v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Memorization in Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19352v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出NCMemo框架研究图神经网络中的记忆现象，发现图同质性与记忆呈反向关系，分析GNN训练动态，并利用图重连减少记忆同时保护隐私。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络已被证明能记忆训练数据，但对图神经网络的相关分析仍不充分，图同质性（连接节点有相似标签/特征的性质）在GNN学习中作用重要。&lt;h4&gt;目的&lt;/h4&gt;引入首个量化半监督节点分类中标签记忆的框架；研究图同质性与记忆关系；分析GNN训练动态；探索特征空间邻域标签不一致性与记忆关系；研究图重连作为减少记忆方法。&lt;h4&gt;方法&lt;/h4&gt;提出NCMemo框架；建立图同质性与记忆关系模型；分析GNN训练动态；研究节点特征空间邻域特性；应用图重连技术减少记忆。&lt;h4&gt;主要发现&lt;/h4&gt;低图同质性显著增加记忆，GNN依赖记忆学习低同质性图；低同质性图中记忆增加与GNN使用图结构的隐式偏置相关；特征空间邻域标签不一致性高的节点更易被记忆；图重连能有效减少记忆而不损害性能；图重连降低已记忆数据点的隐私风险。&lt;h4&gt;结论&lt;/h4&gt;该工作不仅推进了对GNN学习的理解，还支持了更具隐私保护的GNN部署，为开发更有效的GNN训练方法和隐私保护策略提供了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络(DNNs)已被证明能够记忆其训练数据，但对图神经网络(GNNs)的类似分析仍然 largely under-explored。我们引入了NCMemo（节点分类记忆），这是首个用于量化半监督节点分类中标签记忆的框架。我们首先建立了记忆与图同质性之间的反向关系，即连接节点共享相似标签/特征的性质。我们发现较低的同质性显著增加了记忆，表明GNN依赖记忆来学习同质性较低的图。其次，我们分析了GNN训练动态。我们发现低同质性图中增加的记忆与GNN在学习过程中使用图结构的隐式偏置紧密相关。在低同质性情况下，这种结构信息较少，因此诱导节点标签的记忆以最小化训练损失。最后，我们显示特征空间邻域中标签不一致性更高的节点显著更容易被记忆。基于我们对图同质性与记忆之间联系的理解，我们研究了图重连作为减少记忆的方法。我们的结果表明，这种方法能有效减少记忆而不损害模型性能。此外，我们表明它在实践中降低了先前记忆数据点的隐私风险。因此，我们的工作不仅推进了对GNN学习的理解，还支持了更具隐私保护的GNN部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have been shown to memorize their training data,yet similar analyses for graph neural networks (GNNs) remain largelyunder-explored. We introduce NCMemo (Node Classification Memorization), thefirst framework to quantify label memorization in semi-supervised nodeclassification. We first establish an inverse relationship between memorizationand graph homophily, i.e., the property that connected nodes share similarlabels/features. We find that lower homophily significantly increasesmemorization, indicating that GNNs rely on memorization to learn lesshomophilic graphs. Secondly, we analyze GNN training dynamics. We find that theincreased memorization in low homophily graphs is tightly coupled to the GNNs'implicit bias on using graph structure during learning. In low homophilyregimes, this structure is less informative, hence inducing memorization of thenode labels to minimize training loss. Finally, we show that nodes with higherlabel inconsistency in their feature-space neighborhood are significantly moreprone to memorization. Building on our insights into the link between graphhomophily and memorization, we investigate graph rewiring as a means tomitigate memorization. Our results demonstrate that this approach effectivelyreduces memorization without compromising model performance. Moreover, we showthat it lowers the privacy risk for previously memorized data points inpractice. Thus, our work not only advances understanding of GNN learning butalso supports more privacy-preserving GNN deployment.</description>
      <author>example@mail.com (Adarsh Jamadandi, Jing Xu, Adam Dziedzic, Franziska Boenisch)</author>
      <guid isPermaLink="false">2508.19352v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Domain Gaps for Fine-Grained Moth Classification Through Expert-Informed Adaptation and Foundation Model Priors</title>
      <link>http://arxiv.org/abs/2508.20089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种轻量级分类方法，结合有限的专家标记野外数据与BioCLIP2基础模型的知识蒸馏，用于自动化相机系统中的蛾类图像分类。该方法在保持高准确性的同时显著降低了计算成本，为昆虫监测系统提供了实用指导。&lt;h4&gt;背景&lt;/h4&gt;标记来自自动化相机系统的鳞翅目（蛾类）图像对于理解昆虫数量下降至关重要。然而，由于策展图像和嘈杂的野外图像之间的域偏移，准确的物种识别具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级分类方法，能够准确识别蛾类物种，同时降低计算成本，解决域偏移问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种结合有限专家标记野外数据与BioCLIP2基础模型知识蒸馏的分类方法，使用ConvNeXt-tiny架构。在丹麦101种蛾类物种的AMI相机系统上进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;BioCLIP2基础模型明显优于其他分类方法；所提出的蒸馏轻量级模型以显著降低的计算成本实现了与基础模型相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;这些见解为开发高效的昆虫监测系统和弥合细粒度分类的域差距提供了实用指南，有助于理解昆虫数量下降的趋势。&lt;h4&gt;翻译&lt;/h4&gt;标记来自自动化相机系统的鳞翅目（蛾类）图像对于理解昆虫数量下降至关重要。然而，由于策展图像和嘈杂的野外图像之间的域偏移，准确的物种识别具有挑战性。我们提出了一种轻量级分类方法，将有限的专家标记野外数据与高性能BioCLIP2基础模型的知识蒸馏相结合，使用ConvNeXt-tiny架构。在丹麦101种蛾类物种的AMI相机系统上的实验表明，BioCLIP2明显优于其他方法，并且我们蒸馏的轻量级模型以显著降低的计算成本实现了相当的准确性。这些见解为开发高效的昆虫监测系统和弥合细粒度分类的域差距提供了实用指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Labelling images of Lepidoptera (moths) from automated camera systems isvital for understanding insect declines. However, accurate speciesidentification is challenging due to domain shifts between curated images andnoisy field imagery. We propose a lightweight classification approach,combining limited expert-labelled field data with knowledge distillation fromthe high-performance BioCLIP2 foundation model into a ConvNeXt-tinyarchitecture. Experiments on 101 Danish moth species from AMI camera systemsdemonstrate that BioCLIP2 substantially outperforms other methods and that ourdistilled lightweight model achieves comparable accuracy with significantlyreduced computational cost. These insights offer practical guidelines for thedevelopment of efficient insect monitoring systems and bridging domain gaps forfine-grained classification.</description>
      <author>example@mail.com (Ross J Gardiner, Guillaume Mougeot, Sareh Rowlands, Benno I Simmons, Flemming Helsing, Toke Thomas Høye)</author>
      <guid isPermaLink="false">2508.20089v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>HERMES: Human-to-Robot Embodied Learning from Multi-Source Motion Data for Mobile Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2508.20085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了HERMES，一种用于移动双臂灵巧操作的人机学习框架，能够将多源人类手部动作转化为机器人可执行的复杂操作行为，并在多样化环境中实现自主操作。&lt;h4&gt;背景&lt;/h4&gt;利用人类运动数据赋予机器人多样化操作能力已成为有前景的研究方向，但将多源人类手部动作转化为可行的机器人行为仍具挑战性，特别是对于具有复杂高维动作空间的多指灵巧手机器人，且现有方法难以适应不同环境条件。&lt;h4&gt;目的&lt;/h4&gt;开发一种人机学习框架，解决多源人类手部动作到机器人行为的转化问题，并使机器人在多样化非结构化环境中能够自主执行复杂的双臂灵巧操作任务。&lt;h4&gt;方法&lt;/h4&gt;1) 提出HERMES框架；2) 制定统一强化学习方法，无缝转化多源异构人类手部动作为物理可行的机器人行为；3) 设计基于深度图像的端到端sim2real迁移方法；4) 通过闭环PnP定位机制增强导航基础模型，实现视觉目标精确对齐，连接自主导航与灵巧操作。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验表明，HERMES在多样化真实场景中表现出可泛化的行为，能成功执行多种复杂的移动双臂灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;HERMES框架有效解决了多源人类手部动作到机器人行为的转化挑战，使机器人能够在多样化非结构化环境中自主执行复杂操作，为机器人操作领域提供了有价值的技术方案。&lt;h4&gt;翻译&lt;/h4&gt;HERMES: 一种用于移动双臂灵巧操作的人机学习框架&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging human motion data to impart robots with versatile manipulationskills has emerged as a promising paradigm in robotic manipulation.Nevertheless, translating multi-source human hand motions into feasible robotbehaviors remains challenging, particularly for robots equipped withmulti-fingered dexterous hands characterized by complex, high-dimensionalaction spaces. Moreover, existing approaches often struggle to produce policiescapable of adapting to diverse environmental conditions. In this paper, weintroduce HERMES, a human-to-robot learning framework for mobile bimanualdexterous manipulation. First, HERMES formulates a unified reinforcementlearning approach capable of seamlessly transforming heterogeneous human handmotions from multiple sources into physically plausible robotic behaviors.Subsequently, to mitigate the sim2real gap, we devise an end-to-end, depthimage-based sim2real transfer method for improved generalization to real-worldscenarios. Furthermore, to enable autonomous operation in varied andunstructured environments, we augment the navigation foundation model with aclosed-loop Perspective-n-Point (PnP) localization mechanism, ensuring precisealignment of visual goals and effectively bridging autonomous navigation anddexterous manipulation. Extensive experimental results demonstrate that HERMESconsistently exhibits generalizable behaviors across diverse, in-the-wildscenarios, successfully performing numerous complex mobile bimanual dexterousmanipulation tasks. Project Page:https:/gemcollector.github.io/HERMES/.</description>
      <author>example@mail.com (Zhecheng Yuan, Tianming Wei, Langzhe Gu, Pu Hua, Tianhai Liang, Yuanpei Chen, Huazhe Xu)</author>
      <guid isPermaLink="false">2508.20085v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Model Science: getting serious about verification, explanation and control of AI systems</title>
      <link>http://arxiv.org/abs/2508.20040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了模型科学(Model Science)这一新学科的概念框架，强调从数据科学向模型科学的范式转变，将训练好的模型置于分析核心，并提出四个关键支柱。&lt;h4&gt;背景&lt;/h4&gt;基础模型(Foundation models)的广泛采用需要从数据科学转向模型科学的范式转变。&lt;h4&gt;目的&lt;/h4&gt;介绍模型科学这一新学科的概念框架，并提出其四个关键支柱，以指导可信、安全和人类对齐的AI系统的发展。&lt;h4&gt;方法&lt;/h4&gt;提出模型科学的四个关键支柱：验证(Verification)要求严格且具有上下文感知的评估协议；解释(Explanation)是探索模型内部操作的各种方法；控制(Control)整合对齐技术来引导模型行为；接口(Interface)开发交互式和可视化解释工具以提高人类校准和决策能力。&lt;h4&gt;主要发现&lt;/h4&gt;模型科学将训练好的模型置于分析的核心，旨在跨不同操作环境中交互、验证、解释和控制模型行为。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型科学框架旨在指导可信、安全和人类对齐的AI系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;基础模型日益广泛的采用需要从数据科学向模型科学转变范式。与以数据为中心的方法不同，模型科学将训练好的模型置于分析核心，旨在跨不同操作环境中交互、验证、解释和控制其行为。本文介绍了名为模型科学的新学科的概念框架，并提出了其四个关键支柱：验证，要求严格且具有上下文感知的评估协议；解释，被理解为探索模型内部操作的各种方法；控制，整合对齐技术来引导模型行为；接口，开发交互式和可视化解释工具以提高人类校准和决策能力。所提出的框架旨在指导可信、安全和人类对齐的AI系统的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing adoption of foundation models calls for a paradigm shift fromData Science to Model Science. Unlike data-centric approaches, Model Scienceplaces the trained model at the core of analysis, aiming to interact, verify,explain, and control its behavior across diverse operational contexts. Thispaper introduces a conceptual framework for a new discipline called ModelScience, along with the proposal for its four key pillars: Verification, whichrequires strict, context-aware evaluation protocols; Explanation, which isunderstood as various approaches to explore of internal model operations;Control, which integrates alignment techniques to steer model behavior; andInterface, which develops interactive and visual explanation tools to improvehuman calibration and decision-making. The proposed framework aims to guide thedevelopment of credible, safe, and human-aligned AI systems.</description>
      <author>example@mail.com (Przemyslaw Biecek, Wojciech Samek)</author>
      <guid isPermaLink="false">2508.20040v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>The Next Layer: Augmenting Foundation Models with Structure-Preserving and Attention-Guided Learning for Local Patches to Global Context Awareness in Computational Pathology</title>
      <link>http://arxiv.org/abs/2508.19914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  43 pages, 7 main Figures, 8 Extended Data Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了EAGLE-Net，一种结构保持的、注意力引导的多实例学习架构，用于改进计算病理学中的特征提取和预测。&lt;h4&gt;背景&lt;/h4&gt;基础模型已成为计算病理学中强大的特征提取器，但它们通常忽略了组织的全局空间结构和局部区域之间的上下文关系，这些是理解肿瘤微环境的关键元素。多实例学习是基础模型之后的必要步骤。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架，将补丁级别的特征聚合为幻灯片级别的预测，同时增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;EAGLE-Net集成多尺度绝对空间编码捕获全局组织架构，使用top-K邻域感知损失关注局部微环境，以及背景抑制损失减少假阳性。研究在大型泛癌数据集上进行了测试，包括三种癌症类型的分类(10,260张幻灯片)和七种癌症类型的生存预测(4,172张幻灯片)，使用了三种不同的组织学基础骨干网络。&lt;h4&gt;主要发现&lt;/h4&gt;EAGLE-Net在所有任务中实现了高达3%的分类准确率提升，在7种癌症类型中的6种中获得了最高的一致性指数。生成的注意力图平滑且生物连贯，与专家注释一致，能够突出显示侵袭前沿、坏死和免疫浸润。&lt;h4&gt;结论&lt;/h4&gt;EAGLE-Net是一个可推广的、可解释的框架，补充了基础模型，能够改进生物标志物发现、预后建模和临床决策支持。&lt;h4&gt;翻译&lt;/h4&gt;基础模型最近已成为计算病理学中强大的特征提取器，但它们通常忽略了利用组织的全局空间结构和诊断相关区域之间的局部上下文关系——理解肿瘤微环境的关键元素。多实例学习(MIL)仍然是基础模型之后的必要步骤，用于设计将补丁级别特征聚合为幻灯片级别预测的框架。我们提出了EAGLE-Net，一种结构保持的、注意力引导的MIL架构，旨在增强预测和可解释性。EAGLE-Net集成多尺度绝对空间编码来捕获全局组织架构，top-K邻域感知损失来关注局部微环境，以及背景抑制损失来减少假阳性。我们在大型泛癌数据集上对EAGLE-Net进行了基准测试，包括三种癌症类型的分类(10,260张幻灯片)和七种癌症类型的生存预测(4,172张幻灯片)，使用了三种不同的组织学基础骨干网络(REMEDIES、Uni-V1、Uni2-h)。在所有任务中，EAGLE-Net实现了高达3%的分类准确率提升，并在7种癌症类型中的6种中获得了最高的一致性指数，产生了平滑、生物连贯的注意力图，与专家注释一致，并突出了侵袭前沿、坏死和免疫浸润。这些结果将EAGLE-Net定位为一个可推广的、可解释的框架，补充了基础模型，能够改进生物标志物发现、预后建模和临床决策支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have recently emerged as powerful feature extractors incomputational pathology, yet they typically omit mechanisms for leveraging theglobal spatial structure of tissues and the local contextual relationshipsamong diagnostically relevant regions - key elements for understanding thetumor microenvironment. Multiple instance learning (MIL) remains an essentialnext step following foundation model, designing a framework to aggregatepatch-level features into slide-level predictions. We present EAGLE-Net, astructure-preserving, attention-guided MIL architecture designed to augmentprediction and interpretability. EAGLE-Net integrates multi-scale absolutespatial encoding to capture global tissue architecture, a top-Kneighborhood-aware loss to focus attention on local microenvironments, andbackground suppression loss to minimize false positives. We benchmarkedEAGLE-Net on large pan-cancer datasets, including three cancer types forclassification (10,260 slides) and seven cancer types for survival prediction(4,172 slides), using three distinct histology foundation backbones (REMEDIES,Uni-V1, Uni2-h). Across tasks, EAGLE-Net achieved up to 3% higherclassification accuracy and the top concordance indices in 6 of 7 cancer types,producing smooth, biologically coherent attention maps that aligned with expertannotations and highlighted invasive fronts, necrosis, and immune infiltration.These results position EAGLE-Net as a generalizable, interpretable frameworkthat complements foundation models, enabling improved biomarker discovery,prognostic modeling, and clinical decision support</description>
      <author>example@mail.com (Muhammad Waqas, Rukhmini Bandyopadhyay, Eman Showkatian, Amgad Muneer, Anas Zafar, Frank Rojas Alvarez, Maricel Corredor Marin, Wentao Li, David Jaffray, Cara Haymaker, John Heymach, Natalie I Vokes, Luisa Maren Solis Soto, Jianjun Zhang, Jia Wu)</author>
      <guid isPermaLink="false">2508.19914v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>SoK: Large Language Model Copyright Auditing via Fingerprinting</title>
      <link>http://arxiv.org/abs/2508.19843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对大型语言模型(LLM)指纹识别技术进行了首次全面研究，提出统一框架和分类法，并创建了首个系统基准LeaFBench用于评估该技术在真实场景下的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLM)因其广泛能力和训练所需的大量资源而成为有价值的知识产权，但它们容易受到版权侵犯，如未经授权使用和模型盗窃。&lt;h4&gt;目的&lt;/h4&gt;解决LLM指纹识别技术的可靠性问题，由于各种模型修改的普遍存在和缺乏标准化评估，其可靠性目前不确定。&lt;h4&gt;方法&lt;/h4&gt;引入统一框架和正式分类法，将现有方法分为白盒和黑盒方法；创建LeaFBench基准，基于主流基础模型包含149个不同实例，集成13种代表性开发后技术，涵盖参数改变方法和参数独立机制。&lt;h4&gt;主要发现&lt;/h4&gt;在LeaFBench上的广泛实验揭示了现有LLM指纹识别方法的优缺点，勾勒出该新兴领域的未来研究方向和关键开放问题。&lt;h4&gt;结论&lt;/h4&gt;LLM指纹识别是一种有前途的版权审计解决方案，但需要进一步研究和标准化以提高其可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLM)的广泛能力和训练所需的实质性资源使其成为有价值的知识产权，但它们仍然容易受到版权侵犯，如未经授权使用和模型盗窃。LLM指纹识别是一种非侵入性技术，它从LLM中提取和比较独特特征以识别侵权行为，为版权审计提供了有希望的解决方案。然而，由于其可靠性因各种模型修改的普遍存在和缺乏标准化评估而不确定。在这篇SoK中，我们首次对LLM指纹识别进行了全面研究。我们引入了一个统一的框架和正式的分类法，将现有方法分为白盒和黑盒方法，提供了对最新技术的结构化概述。我们进一步提出了LeaFBench，这是第一个用于在真实部署场景下评估LLM指纹识别的系统基准。基于主流基础模型并包含149个不同的模型实例，LeaFBench集成了13种代表性的开发后技术，涵盖参数改变方法(如微调、量化)和参数独立机制(如系统提示、RAG)。在LeaFBench上的广泛实验揭示了现有方法的优缺点，从而勾勒出这一新兴领域的未来研究方向和关键开放问题。代码可在https://github.com/shaoshuo-ss/LeaFBench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The broad capabilities and substantial resources required to train LargeLanguage Models (LLMs) make them valuable intellectual property, yet theyremain vulnerable to copyright infringement, such as unauthorized use and modeltheft. LLM fingerprinting, a non-intrusive technique that extracts and comparesthe distinctive features from LLMs to identify infringements, offers apromising solution to copyright auditing. However, its reliability remainsuncertain due to the prevalence of diverse model modifications and the lack ofstandardized evaluation. In this SoK, we present the first comprehensive studyof LLM fingerprinting. We introduce a unified framework and formal taxonomythat categorizes existing methods into white-box and black-box approaches,providing a structured overview of the state of the art. We further proposeLeaFBench, the first systematic benchmark for evaluating LLM fingerprintingunder realistic deployment scenarios. Built upon mainstream foundation modelsand comprising 149 distinct model instances, LeaFBench integrates 13representative post-development techniques, spanning both parameter-alteringmethods (e.g., fine-tuning, quantization) and parameter-independent mechanisms(e.g., system prompts, RAG). Extensive experiments on LeaFBench reveal thestrengths and weaknesses of existing methods, thereby outlining future researchdirections and critical open problems in this emerging field. The code isavailable at https://github.com/shaoshuo-ss/LeaFBench.</description>
      <author>example@mail.com (Shuo Shao, Yiming Li, Yu He, Hongwei Yao, Wenyuan Yang, Dacheng Tao, Zhan Qin)</author>
      <guid isPermaLink="false">2508.19843v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>CAMÕES: A Comprehensive Automatic Speech Recognition Benchmark for European Portuguese</title>
      <link>http://arxiv.org/abs/2508.19721v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ASRU 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究填补了欧洲葡萄牙语语音识别资源的空白，提供了首个面向欧洲葡萄牙语和其他葡萄牙语变体的开放框架CAMÕES，包含评估基准和先进模型。&lt;h4&gt;背景&lt;/h4&gt;现有的葡萄牙语自动语音识别资源主要集中在巴西葡萄牙语，欧洲葡萄牙语和其他变体研究不足。&lt;h4&gt;目的&lt;/h4&gt;引入CAMÕES，首个面向欧洲葡萄牙语和其他葡萄牙语变体的开放框架，以填补研究空白。&lt;h4&gt;方法&lt;/h4&gt;框架包含两部分：1)包含46小时跨越多个领域的欧洲葡萄牙语测试数据的评估基准；2)最先进模型集合。研究考虑多种基础模型，评估其零样本和微调性能，以及从头开始训练的E-Branchformer模型。使用425小时精选的欧洲葡萄牙语数据进行微调和训练。&lt;h4&gt;主要发现&lt;/h4&gt;微调的基础模型和E-Branchformer在欧洲葡萄牙语上的性能相当。与最强的零样本基础模型相比，最佳性能模型的相对错误率提高了35%以上。&lt;h4&gt;结论&lt;/h4&gt;为欧洲葡萄牙语和其他变体建立了新的最先进水平。&lt;h4&gt;翻译&lt;/h4&gt;现有的葡萄牙语自动语音识别资源主要集中在巴西葡萄牙语，导致欧洲葡萄牙语(EP)和其他变体研究不足。为填补这一空白，我们引入了CAMÕES，这是首个面向欧洲葡萄牙语和其他葡萄牙语变体的开放框架。它包含两部分：(1)全面的评估基准，包括46小时跨越多个领域的欧洲葡萄牙语测试数据；(2)最先进模型的集合。对于后者，我们考虑了多种基础模型，评估了它们的零样本和微调性能，以及从头开始训练的E-Branchformer模型。使用425小时精选的欧洲葡萄牙语数据进行微调和训练。我们的结果显示，微调的基础模型和E-Branchformer在欧洲葡萄牙语上的性能相当。此外，与最强的零样本基础模型相比，最佳性能模型的相对错误率提高了35%以上，为欧洲葡萄牙语和其他变体建立了新的最先进水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing resources for Automatic Speech Recognition in Portuguese are mostlyfocused on Brazilian Portuguese, leaving European Portuguese (EP) and othervarieties under-explored. To bridge this gap, we introduce CAM\~OES, the firstopen framework for EP and other Portuguese varieties. It consists of (1) acomprehensive evaluation benchmark, including 46h of EP test data spanningmultiple domains; and (2) a collection of state-of-the-art models. For thelatter, we consider multiple foundation models, evaluating their zero-shot andfine-tuned performances, as well as E-Branchformer models trained from scratch.A curated set of 425h of EP was used for both fine-tuning and training. Ourresults show comparable performance for EP between fine-tuned foundation modelsand the E-Branchformer. Furthermore, the best-performing models achieverelative improvements above 35% WER, compared to the strongest zero-shotfoundation model, establishing a new state-of-the-art for EP and othervarieties.</description>
      <author>example@mail.com (Carlos Carvalho, Francisco Teixeira, Catarina Botelho, Anna Pompili, Rubén Solera-Ureña, Sérgio Paulo, Mariana Julião, Thomas Rolland, John Mendonça, Diogo Pereira, Isabel Trancoso, Alberto Abad)</author>
      <guid isPermaLink="false">2508.19721v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>SCAR: A Characterization Scheme for Multi-Modal Dataset</title>
      <link>http://arxiv.org/abs/2508.19659v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SCAR是一种新的数据集表征方案，关注数据质量的四个结构性方面：规模、覆盖范围、真实性和丰富度。它能够捕获在数据集扩展下保持不变的稳定特征，并基于这些特性创建Foundation Data以保留完整数据集的泛化行为。通过建模单模态任务为阶跃函数，可以估计基础数据大小分布并捕获跨模态的泛化偏差，进而开发SCAR引导的数据完成策略有效扩展多模态数据。&lt;h4&gt;背景&lt;/h4&gt;基础模型在各种任务上表现出 remarkable 通用性，主要由训练数据的特性驱动。现有的数据中心方法如剪枝和压缩优化了训练，但对数据特性如何影响泛化的理论见解有限。传统观点主要关注数据数量和训练效率，忽视了数据质量的结构性方面，特别是在样本扩展中数据特征的影响。&lt;h4&gt;目的&lt;/h4&gt;引入一种能够描述数据集内在结构特性的方案，理解数据特性如何影响泛化，特别是在样本扩展中，并提供一种基于数据结构特性的数据理解和利用方法。&lt;h4&gt;方法&lt;/h4&gt;提出了SCAR（规模、覆盖范围、真实性和丰富度）方案来捕获数据集的内在结构特性；引入Foundation Data概念，即保留完整数据集泛化行为的最小子集；将单模态任务建模为阶跃函数并估计基础数据大小分布；开发基于泛化偏差的SCAR引导数据完成策略，实现模态感知的数据扩展。&lt;h4&gt;主要发现&lt;/h4&gt;SCAR能够捕获数据集的内在结构特性并保持不变；Foundation Data可以在不进行特定模型重新训练的情况下保留完整数据集的泛化行为；通过建模可以估计基础数据大小分布并捕获跨模态的阶跃式泛化偏差；SCAR引导的数据完成策略能够有效地、模态感知地扩展多模态数据集中的模态特定特征。&lt;h4&gt;结论&lt;/h4&gt;SCAR在预测数据效用和指导数据获取方面是有效的，实验验证了SCAR在不同多模态数据集和模型架构上的有效性，为数据理解和利用提供了新的理论基础。&lt;h4&gt;翻译&lt;/h4&gt;基础模型在多样化任务中表现出卓越的泛化能力，这主要由其训练数据的特性驱动。最近的数据中心方法如剪枝和压缩旨在优化训练，但对数据特性如何影响泛化提供了有限的理论见解，特别是样本扩展中的数据特征。传统观点通过主要关注数据数量和训练效率限制了进展，常常忽视数据质量的结构性方面。在本研究中，我们引入了SCAR，一种用于表征数据集内在结构特性的方案，涵盖四个关键指标：规模、覆盖范围、真实性和丰富度。与之前的数据中心指标不同，SCAR捕获了在数据集扩展下保持不变的稳定特征，为数据理解提供了稳健而通用的基础。利用这些结构特性，我们引入了Foundation Data——一个无需特定模型重新训练即可保留完整数据集泛化行为的最小子集。我们将单模态任务建模为阶跃函数，并估计基础数据大小的分布，以捕获目标多模态数据集中跨模态的阶跃式泛化偏差。最后，我们基于这种泛化偏差开发了SCAR引导的数据完成策略，能够高效地、模态感知地扩展多模态数据集中的模态特定特征。在多样化的多模态数据集和模型架构上的实验验证了SCAR在预测数据效用和指导数据获取方面的有效性。代码可在https://github.com/McAloma/SCAR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models exhibit remarkable generalization across diverse tasks,largely driven by the characteristics of their training data. Recentdata-centric methods like pruning and compression aim to optimize training butoffer limited theoretical insight into how data properties affectgeneralization, especially the data characteristics in sample scaling.Traditional perspectives further constrain progress by focusing predominantlyon data quantity and training efficiency, often overlooking structural aspectsof data quality. In this study, we introduce SCAR, a principled scheme forcharacterizing the intrinsic structural properties of datasets across four keymeasures: Scale, Coverage, Authenticity, and Richness. Unlike priordata-centric measures, SCAR captures stable characteristics that remaininvariant under dataset scaling, providing a robust and general foundation fordata understanding. Leveraging these structural properties, we introduceFoundation Data-a minimal subset that preserves the generalization behavior ofthe full dataset without requiring model-specific retraining. We modelsingle-modality tasks as step functions and estimate the distribution of thefoundation data size to capture step-wise generalization bias across modalitiesin the target multi-modal dataset. Finally, we develop a SCAR-guided datacompletion strategy based on this generalization bias, which enables efficient,modality-aware expansion of modality-specific characteristics in multimodaldatasets. Experiments across diverse multi-modal datasets and modelarchitectures validate the effectiveness of SCAR in predicting data utility andguiding data acquisition. Code is available at https://github.com/McAloma/SCAR.</description>
      <author>example@mail.com (Ri Su, Zhao Chen, Caleb Chen Cao, Nan Tang, Lei Chen)</author>
      <guid isPermaLink="false">2508.19659v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>FinCast: A Foundation Model for Financial Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2508.19609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FinCast是一种专为金融时间序列预测设计的基础模型，通过在大规模金融数据集上训练，克服了现有深度学习方法面临的过拟合和需要大量领域特定微调的问题，展现出强大的零样本性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;金融时间序列预测对维持经济稳定、指导政策制定和促进可持续投资至关重要，但由于各种潜在的模式转变，这一任务具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉金融时间序列中多样化模式且无需领域特定微调的预测模型，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入FinCast，这是第一个专门为金融时间序列预测设计的基础模型，在大规模金融数据集上进行训练，以处理时间非平稳性、多领域多样性和不同时间分辨率带来的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;FinCast展现出强大的零样本性能，能够在不进行领域特定微调的情况下有效捕捉多样化模式；全面的实证和定性评估表明，FinCast超越了现有的最先进方法，突显了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;FinCast代表了金融时间序列预测领域的重要进展，通过基础模型方法解决了现有方法的局限性，为金融预测提供了更有效、更通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;金融时间序列预测对于维持经济稳定、指导明智的政策制定和促进可持续投资实践至关重要。然而，由于各种潜在的模式转变，它仍然具有挑战性。这些转变主要来自三个来源：时间非平稳性（分布随时间变化）、多领域多样性（股票、商品和期货等金融领域中存在不同模式）以及不同的时间分辨率（秒级、小时级、日级或周级指标之间存在不同模式）。虽然最近的深度学习方法试图解决这些复杂性，但它们经常出现过拟合问题，并且通常需要大量的领域特定微调。为了克服这些局限性，我们引入了FinCast，这是专为金融时间序列预测设计的基础模型，在大规模金融数据集上进行训练。值得注意的是，FinCast展现出强大的零样本性能，能够在不进行领域特定微调的情况下有效捕捉多样化模式。全面的实证和定性评估表明，FinCast超越了现有的最先进方法，突显了其强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial time-series forecasting is critical for maintaining economicstability, guiding informed policymaking, and promoting sustainable investmentpractices. However, it remains challenging due to various underlying patternshifts. These shifts arise primarily from three sources: temporalnon-stationarity (distribution changes over time), multi-domain diversity(distinct patterns across financial domains such as stocks, commodities, andfutures), and varying temporal resolutions (patterns differing acrossper-second, hourly, daily, or weekly indicators). While recent deep learningmethods attempt to address these complexities, they frequently suffer fromoverfitting and typically require extensive domain-specific fine-tuning. Toovercome these limitations, we introduce FinCast, the first foundation modelspecifically designed for financial time-series forecasting, trained onlarge-scale financial datasets. Remarkably, FinCast exhibits robust zero-shotperformance, effectively capturing diverse patterns without domain-specificfine-tuning. Comprehensive empirical and qualitative evaluations demonstratethat FinCast surpasses existing state-of-the-art methods, highlighting itsstrong generalization capabilities.</description>
      <author>example@mail.com (Zhuohang Zhu, Haodong Chen, Qiang Qu, Vera Chung)</author>
      <guid isPermaLink="false">2508.19609v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Just Because You Can, Doesn't Mean You Should: LLMs for Data Fitting</title>
      <link>http://arxiv.org/abs/2508.19563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大语言模型(LLMs)被广泛应用于各种场景，作为即插即用的数据拟合和预测生成方法。然而研究发现LLMs对与学习任务无关的数据表示变化非常敏感，这种敏感性存在于多种LLMs和应用方法中，即使专门设计的表格基础模型也无法完全避免这种敏感性。&lt;h4&gt;背景&lt;/h4&gt;大语言模型(LLMs)正被广泛应用于各种场景，包括作为数据拟合和预测生成的即插即用方法。先前研究表明，通过上下文学习或监督微调，LLMs在预测性能上可以与许多表格监督学习技术相媲美。&lt;h4&gt;目的&lt;/h4&gt;识别LLMs在数据拟合方面的关键弱点，特别是对与学习任务无关的数据表示变化的敏感性，并探究这种敏感性的原因。&lt;h4&gt;方法&lt;/h4&gt;通过实验研究变量名称更改对预测误差的影响，检查开源LLM的注意力分数模式，并考察专门为数据拟合训练的表格基础模型(TabPFN)的稳健性。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs对与任务无关的变化非常敏感，简单更改变量名称可能导致预测误差变化高达82%；这种敏感性存在于上下文学习和监督微调中，也存在于闭源和开源通用LLMs中；LLMs中存在非均匀的注意力模式；即使专门设计的TabPFN也无法完全避免对任务无关变化的敏感性。&lt;h4&gt;结论&lt;/h4&gt;尽管LLMs具有令人印象深刻的预测能力，但目前它们甚至缺乏作为原则性数据拟合工具所需的基本稳健性水平。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型(LLMs)正被广泛应用于各种场景，远超典型的语言导向用例。特别是，LLMs越来越多地被用作即插即用的数据拟合和预测生成方法。先前的工作表明，通过上下文学习或监督微调，LLMs在预测性能上可以与许多表格监督学习技术相媲美。然而，我们确定了使用LLMs进行数据拟合的一个关键弱点——对数据表示的更改完全与底层学习任务无关，可以显著改变LLMs对同一数据的预测。例如，在某些情况下，仅更改变量名称就可能导致预测误差大小波动高达82%。这种对任务无关变化的预测敏感性在上下文学习和监督微调中都存在，无论是在闭源还是开源通用LLMs中。此外，通过检查一个开源LLM的注意力分数，我们发现了一种非均匀的注意力模式：训练示例和变量名/值恰好占据提示中的某些位置时，在生成输出标记时会获得更多关注，尽管不同位置预期应获得大致相同的注意力。这在一定程度上解释了对任务无关变化存在时的敏感性。我们还考虑了一个专门为数据拟合训练的最先进的表格基础模型(TabPFN)。尽管被明确设计为实现预测稳健性，TabPFN仍不能完全避免对任务无关变化的敏感性。总体而言，尽管LLMs具有令人印象深刻的预测能力，但目前它们甚至缺乏作为原则性数据拟合工具所需的基本稳健性水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are being applied in a wide array of settings,well beyond the typical language-oriented use cases. In particular, LLMs areincreasingly used as a plug-and-play method for fitting data and generatingpredictions. Prior work has shown that LLMs, via in-context learning orsupervised fine-tuning, can perform competitively with many tabular supervisedlearning techniques in terms of predictive performance. However, we identify acritical vulnerability of using LLMs for data fitting -- making changes to datarepresentation that are completely irrelevant to the underlying learning taskcan drastically alter LLMs' predictions on the same data. For example, simplychanging variable names can sway the size of prediction error by as much as 82%in certain settings. Such prediction sensitivity with respect totask-irrelevant variations manifests under both in-context learning andsupervised fine-tuning, for both close-weight and open-weight general-purposeLLMs. Moreover, by examining the attention scores of an open-weight LLM, wediscover a non-uniform attention pattern: training examples and variablenames/values which happen to occupy certain positions in the prompt receivemore attention when output tokens are generated, even though differentpositions are expected to receive roughly the same attention. This partiallyexplains the sensitivity in the presence of task-irrelevant variations. We alsoconsider a state-of-the-art tabular foundation model (TabPFN) trainedspecifically for data fitting. Despite being explicitly designed to achieveprediction robustness, TabPFN is still not immune to task-irrelevantvariations. Overall, despite LLMs' impressive predictive capabilities,currently they lack even the basic level of robustness to be used as aprincipled data-fitting tool.</description>
      <author>example@mail.com (Hejia Liu, Mochen Yang, Gediminas Adomavicius)</author>
      <guid isPermaLink="false">2508.19563v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>DATR: Diffusion-based 3D Apple Tree Reconstruction Framework with Sparse-View</title>
      <link>http://arxiv.org/abs/2508.19508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种名为DATR的两阶段框架，用于从稀疏视图重建苹果树，实现了高精度的3D重建，在保持与工业级激光扫描仪相当精度的同时，将处理速度提高了约360倍。&lt;h4&gt;背景&lt;/h4&gt;数字孪生应用通过物理资产的精确虚拟副本实现实时监控和机器人模拟，这些系统的关键是具有高几何保真度的3D重建。然而，现有方法在野外条件下表现不佳，特别是在稀疏和遮挡视图的情况下。&lt;h4&gt;目的&lt;/h4&gt;开发一个从稀疏视图重建苹果树的两阶段框架(DATR)，以提高3D重建的精度和效率。&lt;h4&gt;方法&lt;/h4&gt;第一阶段利用机载传感器和基础模型从复杂的野外图像半自动生成树木掩码；第二阶段使用树木掩码过滤多模态数据中的背景信息，结合扩散模型和大型重建模型进行单图像到3D重建。使用Real2Sim数据生成器生成的真实合成苹果树来训练模型，并在野外和合成数据集上评估框架。&lt;h4&gt;主要发现&lt;/h4&gt;DATR框架在两个数据集上都优于现有的3D重建方法，实现了与工业级固定激光扫描仪相当的域特性估计，同时将吞吐量提高了约360倍。&lt;h4&gt;结论&lt;/h4&gt;DATR框架展示了可扩展的农业数字孪生系统的强大潜力，为农业领域的数字孪生应用提供了高效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;数字孪生应用通过物理资产的精确虚拟副本实现实时监控和机器人模拟，展现出变革性潜力。这些系统的关键在于高几何保真度的3D重建。然而，现有方法在野外条件下表现不佳，特别是在处理稀疏和遮挡视图时。本研究开发了一个用于从稀疏视图重建苹果树的两阶段框架(DATR)。第一阶段利用机载传感器和基础模型从复杂的野外图像半自动生成树木掩码。树木掩码用于过滤多模态数据中的背景信息，以进行第二阶段的单图像到3D重建。该阶段包括一个扩散模型和一个大型重建模型，分别用于多视图和隐式神经场生成。扩散模型和LRM的训练使用了通过Real2Sim数据生成器生成的真实合成苹果树。该框架在野外和合成数据集上进行了评估。野外数据集包含六棵具有现场测量真实值的苹果树，而合成数据集则具有结构多样的树木。评估结果表明，我们的DATR框架在两个数据集上都优于现有的3D重建方法，实现了与工业级固定激光扫描仪相当的域特性估计，同时将吞吐量提高了约360倍，展示了可扩展农业数字孪生系统的强大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从稀疏视角（sparse-view）进行苹果树3D重建的问题。这个问题在现实中很重要，因为数字孪生应用需要准确的虚拟副本来实现农业机器人的实时监控和模拟，而现有的3D重建方法在田间条件下表现不佳，特别是在视角稀疏和存在遮挡的情况下。高几何保真度的3D重建是农业数字孪生系统的关键，有助于解决农业机器人开发受限于季节性作物生长周期的问题，实现全年虚拟测试和验证。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出2D成像系统用于树木作物3D重建的核心限制在于依赖昂贵、专门构建的硬件和软件解决方案，这限制了泛化能力。为此，他们设计了一个结合数据收集机器人（AATBot）、简单扫描策略和新颖机器人感知框架（DATR）的全面方法。他们借鉴了扩散模型（如Zero123）进行数据生成和去噪，利用大型重建模型（LRM）进行神经场回归，并使用了Real2Sim数据生成器创建合成数据。同时，他们创新性地结合了多模态输入（RGB图像、深度图和点云），开发了专门的背景去除模块和尺度检索机制，以解决稀疏视角下的重建挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用扩散模型和大型重建模型的强大3D先验知识，通过多模态数据融合（RGB图像、深度图和点云）从稀疏田间图像中推断详细的3D树几何结构，并使用合成数据训练模型实现零样本真实世界部署。整体流程包括：1) 使用AATBot机器人沿种植行收集苹果树图像；2) 通过背景去除模块过滤背景并提取树掩膜；3) 多模态重建模块处理RGB、深度和点云数据；4) 尺度检索机制将重建模型与真实世界度量空间对齐；5) 生成3D网格和点云并估计树干直径和分支数等特征。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) DATR两阶段框架，结合扩散模型和LRM进行多视图和隐式神经场生成；2) 多模态重建模块，集成RGB、深度和点云数据；3) 自动化背景去除模块，减少手动标注需求；4) 尺度检索机制，解决图像重建中的尺度模糊性问题；5) 使用Real2Sim合成数据进行训练，实现零样本真实世界部署。相比之前的工作，DATR不依赖昂贵的专用硬件，通过多模态输入提高重建质量，在稀疏视图下实现更高几何保真度，更准确估计生物相关树属性，并在保持精度的同时提高吞吐量约360倍。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了DATR框架，一种结合扩散模型和大型重建模型的创新方法，通过多模态数据融合和合成数据训练，实现了从稀疏视角高效重建高保真度苹果树3D模型，为农业数字孪生系统和机器人应用提供了强大工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital twin applications offered transformative potential by enablingreal-time monitoring and robotic simulation through accurate virtual replicasof physical assets. The key to these systems is 3D reconstruction with highgeometrical fidelity. However, existing methods struggled under fieldconditions, especially with sparse and occluded views. This study developed atwo-stage framework (DATR) for the reconstruction of apple trees from sparseviews. The first stage leverages onboard sensors and foundation models tosemi-automatically generate tree masks from complex field images. Tree masksare used to filter out background information in multi-modal data for thesingle-image-to-3D reconstruction at the second stage. This stage consists of adiffusion model and a large reconstruction model for respective multi view andimplicit neural field generation. The training of the diffusion model and LRMwas achieved by using realistic synthetic apple trees generated by a Real2Simdata generator. The framework was evaluated on both field and syntheticdatasets. The field dataset includes six apple trees with field-measured groundtruth, while the synthetic dataset featured structurally diverse trees.Evaluation results showed that our DATR framework outperformed existing 3Dreconstruction methods across both datasets and achieved domain-traitestimation comparable to industrial-grade stationary laser scanners whileimproving the throughput by $\sim$360 times, demonstrating strong potential forscalable agricultural digital twin systems.</description>
      <author>example@mail.com (Tian Qiu, Alan Zoubi, Yiyuan Lin, Ruiming Du, Lailiang Cheng, Yu Jiang)</author>
      <guid isPermaLink="false">2508.19508v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Data-Efficient Symbolic Regression via Foundation Model Distillation</title>
      <link>http://arxiv.org/abs/2508.19487v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;EQUATE是一个数据高效的微调框架，通过蒸馏方法使基础模型适应低数据环境下的符号方程发现，将离散方程搜索转化为连续优化任务，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;从观测数据中发现可解释的数学方程（方程发现或符号回归）是科学发现的基础，能够透明建模物理、生物和经济系统。预训练在大型方程数据集上的基础模型有潜力，但在应用于小型、领域特定数据集时常面临负迁移和泛化能力差的问题。&lt;h4&gt;目的&lt;/h4&gt;引入EQUATE（基于质量对齐的转移嵌入的方程生成）框架，通过数据高效的微调方法解决基础模型在小数据集上应用时的局限性，使其能够有效进行符号方程发现。&lt;h4&gt;方法&lt;/h4&gt;EQUATE结合符号-数值对齐与评估器引导的嵌入优化，实现有原则的嵌入-搜索-生成范式。该方法将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，由数据-方程适应性和简单性共同指导。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面 consistently 超越了最先进的基线方法，同时保持了低复杂性和快速推理能力。&lt;h4&gt;结论&lt;/h4&gt;EQUATE是基础模型蒸馏设置中数据高效符号回归的实用且可推广的解决方案，为科学发现中的方程建模提供了新途径。&lt;h4&gt;翻译&lt;/h4&gt;从观测数据中发现可解释的数学方程（也称为方程发现或符号回归）是科学发现的基石，能够透明地建模物理、生物和经济系统。虽然在大型方程数据集上预训练的基础模型提供了一个有希望的起点，但当它们应用于小型、领域特定的数据集时，常常遭受负迁移和泛化能力差的问题。在本文中，我们引入了EQUATE（基于质量对齐的转移嵌入的方程生成），这是一个数据高效的微调框架，通过蒸馏方法使基础模型适应低数据环境下的符号方程发现。EQUATE将符号-数值对齐与评估器引导的嵌入优化相结合，实现了一个有原则的嵌入-搜索-生成范式。我们的方法将离散方程搜索重新表述为共享嵌入空间中的连续优化任务，由数据-方程适应性和简单性指导。在三个标准公共基准测试（Feynman、Strogatz和黑盒数据集）上的实验表明，EQUATE在准确性和鲁棒性方面 consistently 超越了最先进的基线方法，同时保持了低复杂性和快速推理能力。这些结果突显了EQUATE作为基础模型蒸馏设置中数据高效符号回归的实用且可推广的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering interpretable mathematical equations from observed data (a.k.a.equation discovery or symbolic regression) is a cornerstone of scientificdiscovery, enabling transparent modeling of physical, biological, and economicsystems. While foundation models pre-trained on large-scale equation datasetsoffer a promising starting point, they often suffer from negative transfer andpoor generalization when applied to small, domain-specific datasets. In thispaper, we introduce EQUATE (Equation Generation via QUality-Aligned TransferEmbeddings), a data-efficient fine-tuning framework that adapts foundationmodels for symbolic equation discovery in low-data regimes via distillation.EQUATE combines symbolic-numeric alignment with evaluator-guided embeddingoptimization, enabling a principled embedding-search-generation paradigm. Ourapproach reformulates discrete equation search as a continuous optimizationtask in a shared embedding space, guided by data-equation fitness andsimplicity. Experiments across three standard public benchmarks (Feynman,Strogatz, and black-box datasets) demonstrate that EQUATE consistentlyoutperforms state-of-the-art baselines in both accuracy and robustness, whilepreserving low complexity and fast inference. These results highlight EQUATE asa practical and generalizable solution for data-efficient symbolic regressionin foundation model distillation settings.</description>
      <author>example@mail.com (Wangyang Ying, Jinghan Zhang, Haoyue Bai, Nanxu Gong, Xinyuan Wang, Kunpeng Liu, Chandan K. Reddy, Yanjie Fu)</author>
      <guid isPermaLink="false">2508.19487v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>GENIE-ASI: Generative Instruction and Executable Code for Analog Subcircuit Identification</title>
      <link>http://arxiv.org/abs/2508.19393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GENIE-ASI的无需训练的基于大型语言模型的模拟子电路识别方法，通过上下文学习和代码转换实现子电路识别，并在新基准测试中展示了良好性能。&lt;h4&gt;背景&lt;/h4&gt;模拟子电路识别是模拟设计中的核心任务，对仿真、尺寸确定和布局至关重要。传统方法通常需要大量人类专业知识、基于规则的编码或大型标记数据集。&lt;h4&gt;目的&lt;/h4&gt;解决传统模拟子电路识别方法的局限性，提出一种无需训练的、基于大型语言模型的替代方法。&lt;h4&gt;方法&lt;/h4&gt;GENIE-ASI分为两个阶段：首先使用上下文学习从少量演示示例中推导自然语言指令，然后将这些指令转换为可执行的Python代码，以识别未见过的SPICE网表中的子电路。&lt;h4&gt;主要发现&lt;/h4&gt;在提出的基准测试中，GENIE-ASI在简单结构上与基于规则的方法性能相当，在中等抽象级别上保持竞争力，在复杂子电路上也显示出潜力。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型可以作为模拟设计自动化中的适应性、通用工具，为基础模型在模拟设计自动化中的应用开辟了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;模拟子电路识别是模拟设计中的核心任务，对仿真、尺寸确定和布局至关重要。传统方法通常需要大量人类专业知识、基于规则的编码或大型标记数据集。为解决这些挑战，我们提出了GENIE-ASI，这是第一个无需训练的、基于大型语言模型的方法，用于模拟子电路识别。GENIE-ASI分为两个阶段：首先使用上下文学习从少量演示示例中推导自然语言指令，然后将其转换为可执行的Python代码，以识别未见过的SPICE网表中的子电路。此外，为了系统评估基于LLM的方法，我们引入了一个新的基准测试，包含涵盖广泛子电路变体的运算放大器网表。在所提出的基准测试上的实验结果表明，GENIE-ASI在简单结构上与基于规则的性能相当，在中等抽象级别上保持竞争力，甚至在复杂子电路上也显示出潜力。这些发现表明，LLM可以作为模拟设计自动化中的适应性、通用工具，为基础模型在模拟设计自动化中的应用开辟新的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analog subcircuit identification is a core task in analog design, essentialfor simulation, sizing, and layout. Traditional methods often require extensivehuman expertise, rule-based encoding, or large labeled datasets. To addressthese challenges, we propose GENIE-ASI, the first training-free, large languagemodel (LLM)-based methodology for analog subcircuit identification. GENIE-ASIoperates in two phases: it first uses in-context learning to derive naturallanguage instructions from a few demonstration examples, then translates theseinto executable Python code to identify subcircuits in unseen SPICE netlists.In addition, to evaluate LLM-based approaches systematically, we introduce anew benchmark composed of operational amplifier netlists (op-amps) that cover awide range of subcircuit variants. Experimental results on the proposedbenchmark show that GENIE-ASI matches rule-based performance on simplestructures (F1-score = 1.0), remains competitive on moderate abstractions(F1-score = 0.81), and shows potential even on complex subcircuits (F1-score =0.31). These findings demonstrate that LLMs can serve as adaptable,general-purpose tools in analog design automation, opening new researchdirections for foundation model applications in analog design automation.</description>
      <author>example@mail.com (Phuoc Pham, Arun Venkitaraman, Chia-Yu Hsieh, Andrea Bonetti, Stefan Uhlich, Markus Leibl, Simon Hofmann, Eisaku Ohbuchi, Lorenzo Servadei, Ulf Schlichtmann, Robert Wille)</author>
      <guid isPermaLink="false">2508.19393v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Principled Detection of Hallucinations in Large Language Models via Multiple Testing</title>
      <link>http://arxiv.org/abs/2508.18473v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了大型语言模型(LLMs)的幻觉问题，提出了一种基于多重检验假设的方法来检测幻觉，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLMs)已成为解决各种任务的强大基础模型，但它们也容易出现幻觉问题，即生成听起来自信但实际上不正确甚至无意义的回答。&lt;h4&gt;目的&lt;/h4&gt;将幻觉检测问题表述为假设检验问题，并与机器学习模型中的分布外检测问题进行类比，提出一种新的幻觉检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种受多重检验启发的方法来解决幻觉检测问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验结果验证了所提出方法的鲁棒性，表明其优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的多重检验方法在幻觉检测问题上具有有效性和优势。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型(LLMs)已成为解决各种任务的强大基础模型，但它们也被证明容易出现幻觉，即生成听起来自信但实际上不正确甚至无意义的回答。在这项工作中，我们将幻觉检测问题表述为假设检验问题，并将其与机器学习模型中的分布外检测问题进行类比。我们提出了一种受多重检验启发的方法来解决幻觉检测问题，并通过广泛的实验结果验证了我们的方法在最先进方法面前的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Large Language Models (LLMs) have emerged as powerful foundationalmodels to solve a variety of tasks, they have also been shown to be prone tohallucinations, i.e., generating responses that sound confident but areactually incorrect or even nonsensical. In this work, we formulate the problemof detecting hallucinations as a hypothesis testing problem and draw parallelsto the problem of out-of-distribution detection in machine learning models. Wepropose a multiple-testing-inspired method to solve the hallucination detectionproblem, and provide extensive experimental results to validate the robustnessof our approach against state-of-the-art methods.</description>
      <author>example@mail.com (Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli)</author>
      <guid isPermaLink="false">2508.18473v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>DemoBias: An Empirical Study to Trace Demographic Biases in Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2508.19298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, 13th International Workshop on Biometrics and  Forensics (IWBF)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了DemoBias工作，旨在评估大型视觉语言模型(LVLMs)在生物识别面部识别任务中的人口统计偏差问题&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型在各种下游任务中表现出色，包括带描述的生物识别面部识别，但这些基础模型在种族/民族、性别和年龄等不同人口统计群体中往往无法公平表现&lt;h4&gt;目的&lt;/h4&gt;通过实证评估研究LVLMs在生物识别面部识别和文本标记生成任务中的人口统计偏差程度&lt;h4&gt;方法&lt;/h4&gt;在自建的人口平衡数据集上微调和评估了三种预训练LVLMs（LLaVA、BLIP-2和PaliGemma），并使用特定群体BERTScores和公平差异率等指标量化性能差异&lt;h4&gt;主要发现&lt;/h4&gt;实验揭示了LVLMs中存在人口统计偏差，其中PaliGemma和LLaVA在西班牙裔/拉丁裔、高加索人和南亚群体中表现出更高的差异，而BLIP-2则表现出相对一致的性能&lt;h4&gt;结论&lt;/h4&gt;大型视觉语言模型在生物识别面部识别任务中存在人口统计偏差问题，不同模型在不同人口统计群体中的表现存在显著差异&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型(LVLMs)在各种下游任务中表现出色，包括带描述的生物识别面部识别(FR)。然而，人口统计偏差仍然是FR中的一个关键问题，因为这些基础模型在考虑种族/民族、性别和年龄等不同人口统计群体时，往往无法公平表现。因此，通过我们的DemoBias工作，我们进行了实证评估，研究LVLMs在生物识别FR和文本标记生成任务中人口统计偏差的程度。在我们自己生成的人口平衡数据集上微调和评估了三个广泛使用的预训练LVLMs：LLaVA、BLIP-2和PaliGemma。我们使用多种评估指标，如特定群体的BERTScores和公平差异率，来量化和追踪性能差异。实验结果提供了关于LVLMs在不同人口统计群体中公平性和可靠性的有力见解。我们的实证研究发现了LVLMs中存在人口统计偏差，其中PaliGemma和LLaVA在西班牙裔/拉丁裔、高加索人和南亚群体中表现出更高的差异，而BLIP-2则表现出相对一致的性能。仓库：https://github.com/Sufianlab/DemoBias。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IWBF63717.2025.11113455&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision Language Models (LVLMs) have demonstrated remarkablecapabilities across various downstream tasks, including biometric facerecognition (FR) with description. However, demographic biases remain acritical concern in FR, as these foundation models often fail to performequitably across diverse demographic groups, considering ethnicity/race,gender, and age. Therefore, through our work DemoBias, we conduct an empiricalevaluation to investigate the extent of demographic biases in LVLMs forbiometric FR with textual token generation tasks. We fine-tuned and evaluatedthree widely used pre-trained LVLMs: LLaVA, BLIP-2, and PaliGemma on our owngenerated demographic-balanced dataset. We utilize several evaluation metrics,like group-specific BERTScores and the Fairness Discrepancy Rate, to quantifyand trace the performance disparities. The experimental results delivercompelling insights into the fairness and reliability of LVLMs across diversedemographic groups. Our empirical study uncovered demographic biases in LVLMs,with PaliGemma and LLaVA exhibiting higher disparities for Hispanic/Latino,Caucasian, and South Asian groups, whereas BLIP-2 demonstrated comparablyconsistent. Repository: https://github.com/Sufianlab/DemoBias.</description>
      <author>example@mail.com (Abu Sufian, Anirudha Ghosh, Debaditya Barman, Marco Leo, Cosimo Distante)</author>
      <guid isPermaLink="false">2508.19298v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Context-aware Sparse Spatiotemporal Learning for Event-based Vision</title>
      <link>http://arxiv.org/abs/2508.19806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为上下文感知稀疏时空学习(CSSL)的新框架，用于高效处理事件相机数据，解决了现有方法在资源受限应用中的局限性。&lt;h4&gt;背景&lt;/h4&gt;事件相机在机器人感知中具有优势，但现有深度学习方法未能充分利用事件数据的稀疏性，难以集成到边缘应用中。神经形态计算虽能效高，但脉冲神经网络在复杂任务中性能不足，且高激活稀疏性难以实现。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效处理事件相机数据的新框架，在保持高性能的同时实现高神经元稀疏性，适用于神经形态处理。&lt;h4&gt;方法&lt;/h4&gt;提出上下文感知稀疏时空学习(CSSL)框架，引入上下文感知阈值根据输入分布动态调节神经元激活，自然减少激活密度而无需显式稀疏约束。&lt;h4&gt;主要发现&lt;/h4&gt;CSSL在事件目标检测和光流估计任务中实现了与最先进方法相当或更好的性能，同时保持极高的神经元稀疏性。&lt;h4&gt;结论&lt;/h4&gt;CSSL在实现神经形态处理的高效事件视觉方面具有关键作用，为资源受限环境中的事件数据处理提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;事件相机已成为机器人感知的一种有前景的范式，具有高时间分辨率、高动态范围和对运动模糊的鲁棒性等优势。然而，现有的基于深度学习的事件处理方法往往未能充分利用事件数据的稀疏性，使它们难以集成到资源受限的边缘应用中。虽然神经形态计算提供了一种能效高的替代方案，但脉冲神经网络在复杂的事件视觉任务（如目标检测和光流估计）中难以达到最先进模型的性能。此外，在神经网络中实现高激活稀疏性仍然很困难，通常需要仔细手动调整稀疏诱导损失项。本文提出了上下文感知稀疏时空学习(CSSL)这一新框架，引入了上下文感知阈值来根据输入分布动态调节神经元激活，自然地减少激活密度而无需显式的稀疏约束。将CSSL应用于基于事件的目标检测和光流估计，它实现了与最先进方法相当或更好的性能，同时保持了极高的神经元稀疏性。我们的实验结果突显了CSSL在实现神经形态处理的高效事件视觉方面的关键作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event-based camera has emerged as a promising paradigm for robot perception,offering advantages with high temporal resolution, high dynamic range, androbustness to motion blur. However, existing deep learning-based eventprocessing methods often fail to fully leverage the sparse nature of eventdata, complicating their integration into resource-constrained edgeapplications. While neuromorphic computing provides an energy-efficientalternative, spiking neural networks struggle to match of performance ofstate-of-the-art models in complex event-based vision tasks, like objectdetection and optical flow. Moreover, achieving high activation sparsity inneural networks is still difficult and often demands careful manual tuning ofsparsity-inducing loss terms. Here, we propose Context-aware SparseSpatiotemporal Learning (CSSL), a novel framework that introduces context-awarethresholding to dynamically regulate neuron activations based on the inputdistribution, naturally reducing activation density without explicit sparsityconstraints. Applied to event-based object detection and optical flowestimation, CSSL achieves comparable or superior performance tostate-of-the-art methods while maintaining extremely high neuronal sparsity.Our experimental results highlight CSSL's crucial role in enabling efficientevent-based vision for neuromorphic processing.</description>
      <author>example@mail.com (Shenqi Wang, Guangzhi Tang)</author>
      <guid isPermaLink="false">2508.19806v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Generalizing Monocular 3D Object Detection</title>
      <link>http://arxiv.org/abs/2508.19593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD Thesis submitted to MSU&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文解决了单目3D物体检测模型在不同场景下的泛化挑战，提出了多种方法来处理遮挡、数据集差异、大物体检测和相机参数变化等问题。&lt;h4&gt;背景&lt;/h4&gt;单目3D物体检测(Mono3D)是计算机视觉的基本任务，它从单张图像估计物体的类别、3D位置、尺寸和方向。其应用包括自动驾驶、增强现实和机器人技术，这些应用都依赖于准确的3D环境理解。&lt;h4&gt;目的&lt;/h4&gt;解决Mono3D模型在不同场景下的泛化问题，包括遮挡、不同数据集、物体大小变化和相机参数变化等场景。&lt;h4&gt;方法&lt;/h4&gt;1) 提出数学上可微分的NMS(GrooMeD-NMS)增强遮挡鲁棒性；2) 探索深度等变(DEVIANT)主干网络改进对新数据集的泛化；3) 提出基于分割的鸟瞰图方法配合dice损失(SeaBird)解决大物体检测问题；4) 数学分析Mono3D模型对未见过的相机高度的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;大物体检测问题不仅是数据不平衡或感受野问题，也是噪声敏感性问题；所提出的方法可以改善模型在分布外设置中的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过提出的方法，Mono3D模型在遮挡、新数据集、大物体检测和不同相机参数下的泛化能力得到了提高。&lt;h4&gt;翻译&lt;/h4&gt;单目3D物体检测(Mono3D)是一项基础计算机视觉任务，它从单张图像估计物体的类别、3D位置、尺寸和方向。其应用包括自动驾驶、增强现实和机器人技术，这些应用都严重依赖于准确的3D环境理解。本论文解决了Mono3D模型在不同场景下的泛化挑战，包括遮挡、数据集、物体大小和相机参数。为了增强遮挡鲁棒性，我们提出了一个数学上可微分的NMS(GrooMeD-NMS)。为了提高对新数据集的泛化能力，我们探索了深度等变(DEVIANT)主干网络。我们解决了大物体检测问题，证明它不仅仅是数据不平衡或感受野问题，也是噪声敏感性问题。为此，我们引入了一种基于鸟瞰图分割的方法配合dice损失(SeaBird)。最后，我们数学分析了Mono3D模型对未见过的相机高度的泛化能力，并提高了模型在这种分布外设置中的泛化性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决单目3D目标检测（Mono3D）的泛化问题，包括处理遮挡、不同数据集、物体大小变化和相机参数变化等场景。这个问题在现实中非常重要，因为Mono3D在自动驾驶、增强现实和机器人等领域有广泛应用，这些应用需要模型在各种复杂条件下保持准确的3D环境理解能力，特别是在安全关键应用中，模型的泛化能力直接关系到系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者针对不同的泛化问题提出了四种方法：1) GrooMeD-NMS借鉴了2D目标检测中尝试将NMS纳入训练的想法，但针对3D检测进行了改进；2) DEVIANT借鉴了现有的尺度等变性可转向块，构建了深度等变网络；3) SeaBird借鉴了分割方法，结合dice损失解决大物体检测问题；4) CHARM3R通过分析深度估计与相机高度的关系，提出平均深度估计的方法。作者都基于现有计算机视觉技术，但针对Mono3D的特点进行了创新和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文提出了四种方法：1) GrooMeD-NMS的核心是将NMS纳入训练过程，通过矩阵操作、分组和掩码使NMS可微分，实现端到端学习；2) DEVIANT的核心是构建对深度平移具有等变性的网络，确保深度估计一致性；3) SeaBird的核心是通过鸟瞰图分割和dice损失解决大物体检测的噪声敏感性问题；4) CHARM3R的核心是平均模型内的深度估计以提高对未见相机高度的泛化能力。每种方法都有其特定的实现流程，包括理论分析、模型设计和实验验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次提出闭式数学可微分NMS解决训练-推理不匹配；2) 构建深度等变网络提高跨数据集泛化；3) 揭示大物体检测的噪声敏感性并提出基于dice损失的分割方法；4) 分析相机高度影响并提出深度平均策略。相比之前工作，这些方法不仅提高基准性能，更专注于解决多样化场景下的泛化问题，提供了理论分析和针对性的解决方案，在多个数据集上验证了泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过提出四种创新方法，显著提升了单目3D目标检测在遮挡、不同数据集、大物体和不同相机高度等多样化场景下的泛化能力，为实际应用提供了更鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D object detection (Mono3D) is a fundamental computer vision taskthat estimates an object's class, 3D position, dimensions, and orientation froma single image. Its applications, including autonomous driving, augmentedreality, and robotics, critically rely on accurate 3D environmentalunderstanding. This thesis addresses the challenge of generalizing Mono3Dmodels to diverse scenarios, including occlusions, datasets, object sizes, andcamera parameters. To enhance occlusion robustness, we propose a mathematicallydifferentiable NMS (GrooMeD-NMS). To improve generalization to new datasets, weexplore depth equivariant (DEVIANT) backbones. We address the issue of largeobject detection, demonstrating that it's not solely a data imbalance orreceptive field problem but also a noise sensitivity issue. To mitigate this,we introduce a segmentation-based approach in bird's-eye view with dice loss(SeaBird). Finally, we mathematically analyze the extrapolation of Mono3Dmodels to unseen camera heights and improve Mono3D generalization in suchout-of-distribution settings.</description>
      <author>example@mail.com (Abhinav Kumar)</author>
      <guid isPermaLink="false">2508.19593v1</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Robust Single-Stage Fully Sparse 3D Object Detection via Detachable Latent Diffusion</title>
      <link>http://arxiv.org/abs/2508.03252v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RSDNet是一种基于DDPMs的可分离潜在框架的鲁棒单阶段全稀疏3D目标检测网络，通过在潜在特征空间中学习去噪过程、改进扩散机制和引入语义-几何条件引导，实现了高效、鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;DDPMs在鲁棒3D目标检测任务中已显示出成功。现有方法通常依赖于3D框的分数匹配或预训练的扩散先验，但这些方法通常需要推理时的多步迭代，限制了效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法效率低的问题，提出一种单阶段、全稀疏的3D目标检测网络，实现高效且鲁棒的3D目标检测。&lt;h4&gt;方法&lt;/h4&gt;提出RSDNet，通过轻量级去噪网络（如多级去噪自编码器DAEs）在潜在特征空间中学习去噪过程；改进DDPMs的加噪和去噪机制，使DLF能够构建多类型和多级别的噪声样本和目标；引入语义-几何条件引导来感知物体边界和形状；DLF的可分离去噪网络设计使RSDNet能够在推理时进行单步检测。&lt;h4&gt;主要发现&lt;/h4&gt;RSDNet能够有效理解多级扰动下的场景分布，实现鲁棒可靠的检测；语义-几何条件引导缓解了稀疏表示中的中心特征缺失问题；RSDNet能够在全稀疏检测流程中运行；单步检测进一步提高了检测效率。&lt;h4&gt;结论&lt;/h4&gt;在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测性能。&lt;h4&gt;翻译&lt;/h4&gt;去噪扩散概率模型在鲁棒3D目标检测任务中已显示出成功。现有方法通常依赖于3D框的分数匹配或预训练的扩散先验。然而，它们通常需要推理时的多步迭代，这限制了效率。为了解决这个问题，我们提出了一种基于DDPMs的可分离潜在框架的鲁棒单阶段全稀疏3D目标检测网络，命名为RSDNet。具体来说，RSDNet通过轻量级去噪网络（如多级去噪自编码器DAEs）在潜在特征空间中学习去噪过程。这使RSDNet能够有效理解多级扰动下的场景分布，实现鲁棒可靠的检测。同时，我们重新设计了DDPMs的加噪和去噪机制，使DLF能够构建多类型和多级别的噪声样本和目标，增强RSDNet对多种扰动的鲁棒性。此外，我们引入了语义-几何条件引导来感知物体边界和形状，缓解了稀疏表示中的中心特征缺失问题，使RSDNet能够在全稀疏检测流程中运行。而且，DLF的可分离去噪网络设计使RSDNet能够在推理时进行单步检测，进一步提高检测效率。在公共基准上的大量实验表明，RSDNet能够超越现有方法，实现最先进的检测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D物体检测方法在推理阶段需要多步迭代导致效率低下的问题，以及对多种扰动的鲁棒性不足的问题。这个问题在现实中非常重要，因为自动驾驶、AR/VR和机器人等实时应用需要高效的检测系统，而真实场景中的点云数据常常受到点级随机噪声和全局几何失真（坐标偏移、缩放、旋转）的影响，导致检测性能不稳定。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了DDPMs在3D物体检测中的局限性，揭示了其鲁棒性来源于训练阶段而非推理阶段。他们重新思考了噪声化和去噪声机制，设计了可分离的潜在框架(DLF)。作者借鉴了DDPMs的基本原理、多级去噪自编码器的设计、现有的全稀疏3D检测管道以及语义-几何条件引导的思想，但进行了创新性改造，使其能够处理多种扰动并实现单步推理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个可分离的潜在框架(DLF)，将去噪网络作为辅助分支引导主干网络在潜在特征空间中学习去噪过程，但在推理时分离去噪网络实现单步检测。整体流程包括：1)全稀疏管道(FSP)进行特征提取；2)噪声构建模块(NCM)创建多类型和级别噪声样本；3)语义-几何条件层(SGCL)增强边界和形状感知；4)去噪U-Net(DUNet)指导主干学习；5)训练时结合扩散损失和任务损失，推理时实现单步检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)可分离的潜在框架(DLF)实现单步推理同时保持鲁棒性；2)多类型和多级别噪声建模增强对多种扰动的处理能力；3)语义-几何条件引导缓解中心特征缺失问题；4)基于DLF构建的鲁棒单步全稀疏检测网络RSDNet。相比之前工作，RSDNet无需多步迭代，在全稀疏管道中工作，显著提高了对噪声和几何失真的鲁棒性，同时保持了高检测精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种可分离的潜在扩散框架，实现了高效的单步3D物体检测，同时保持了对多种扰动的强鲁棒性，显著提升了在真实场景中的检测性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Denoising Diffusion Probabilistic Models (DDPMs) have shown success in robust3D object detection tasks. Existing methods often rely on the score matchingfrom 3D boxes or pre-trained diffusion priors. However, they typically requiremulti-step iterations in inference, which limits efficiency. To address this,we propose a Robust single-stage fully Sparse 3D object Detection Network witha Detachable Latent Framework (DLF) of DDPMs, named RSDNet. Specifically,RSDNet learns the denoising process in latent feature spaces throughlightweight denoising networks like multi-level denoising autoencoders (DAEs).This enables RSDNet to effectively understand scene distributions undermulti-level perturbations, achieving robust and reliable detection. Meanwhile,we reformulate the noising and denoising mechanisms of DDPMs, enabling DLF toconstruct multi-type and multi-level noise samples and targets, enhancingRSDNet robustness to multiple perturbations. Furthermore, a semantic-geometricconditional guidance is introduced to perceive the object boundaries andshapes, alleviating the center feature missing problem in sparserepresentations, enabling RSDNet to perform in a fully sparse detectionpipeline. Moreover, the detachable denoising network design of DLF enablesRSDNet to perform single-step detection in inference, further enhancingdetection efficiency. Extensive experiments on public benchmarks show thatRSDNet can outperform existing methods, achieving state-of-the-art detection.</description>
      <author>example@mail.com (Wentao Qu, Guofeng Mei, Jing Wang, Yujiao Wu, Xiaoshui Huang, Liang Xiao)</author>
      <guid isPermaLink="false">2508.03252v2</guid>
      <pubDate>Thu, 28 Aug 2025 15:26:36 +0800</pubDate>
    </item>
    <item>
      <title>Lang2Lift: A Framework for Language-Guided Pallet Detection and Pose Estimation Integrated in Autonomous Outdoor Forklift Operation</title>
      <link>http://arxiv.org/abs/2508.15427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Lang2Lift框架，利用基础模型实现自然语言引导的托盘检测和6D姿态估计，解决了物流和建筑业在户外环境中的托盘处理自动化挑战。&lt;h4&gt;背景&lt;/h4&gt;物流和建筑业在托盘处理自动化方面面临持续挑战，特别是在户外环境中，存在负载变化、托盘质量和尺寸不一致以及周围环境无结构化的问题。&lt;h4&gt;目的&lt;/h4&gt;解决托盘运输中的关键步骤自动化问题，即托盘拾取操作，同时应对劳动力短缺、安全问题以及在复杂条件下手动定位和回收托盘的低效率问题。&lt;h4&gt;方法&lt;/h4&gt;提出Lang2Lift框架，利用基础模型进行自然语言引导的托盘检测和6D姿态估计，使操作员可通过直观命令指定目标；感知管道集成Florence-2和SAM-2进行语言引导的分割，以及FoundationPose用于复杂场景下的鲁棒姿态估计；生成的姿态信息输入运动规划模块实现全自动叉车操作。&lt;h4&gt;主要发现&lt;/h4&gt;在ADAPT自动叉车平台上验证了Lang2Lift，在真实世界测试数据集上实现了0.76的托盘分割精度mIoU；时间和误差分析证明了系统的鲁棒性，确认了系统在物流和建筑环境中部署的可行性。&lt;h4&gt;结论&lt;/h4&gt;Lang2Lift可以有效解决物流和建筑环境中托盘处理自动化的挑战，特别适用于具有变化负载、不一致托盘质量和尺寸以及无结构化周围条件的户外环境。&lt;h4&gt;翻译&lt;/h4&gt;物流和建筑业在托盘处理自动化方面面临持续挑战，尤其是在户外环境中，存在负载变化、托盘质量和尺寸不一致以及周围环境无结构化的问题。在本文中，我们解决了托盘运输中的关键步骤自动化问题：托盘拾取操作。我们的工作动机是在这些条件下劳动力短缺、安全问题以及手动定位和回收托盘的低效率问题。我们提出了Lang2Lift框架，它利用基础模型进行自然语言引导的托盘检测和6D姿态估计，使操作员能够通过直观命令（如'吊起起重机附近的钢梁托盘'）指定目标。感知管道集成了Florence-2和SAM-2进行语言引导的分割，以及FoundationPose用于在复杂、多托盘户外场景和变化光照条件下的鲁棒姿态估计。生成的姿态信息输入到运动规划模块，实现全自动叉车操作。我们在ADAPT自动叉车平台上验证了Lang2Lift，在真实世界测试数据集上实现了0.76的托盘分割精度mIoU。时间和误差分析证明了系统的鲁棒性，并确认了系统在物流和建筑环境中部署的可行性。视频演示可在https://eric-nguyen1402.github.io/lang2lift.github.io/查看。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决物流和建筑行业中自动化托盘处理的挑战，特别是在室外环境中如何让叉车理解自然语言指令并准确识别和抓取特定托盘的问题。这个问题在现实中非常重要，因为劳动力短缺、安全隐患以及在负载不一致、托盘质量和尺寸变化、环境杂乱的条件下手动定位和检索托盘的低效率，都是行业面临的实际问题。当前自动化系统依赖刚性预编程特性，难以适应新托盘类型、意外方向或杂乱场景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有CNN方法泛化能力差，无法可靠分类货物负载；发现基础模型(FM)为智能自动化提供了新可能；视觉语言模型(VLMs)能创建灵活机器人系统；自然语言界面能弥合人机通信差距。他们设计了一个结合自然语言引导和视觉基础模型的框架，与叉车控制模块集成。借鉴了Florence-2进行视觉语言检测、SAM-2进行精细分割、FoundationPose进行姿态估计，以及[31]中的分层运动规划和[34]中的控制律。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基础模型实现语言引导的托盘检测和6D姿态估计，使操作员能通过直观命令指定目标。整体流程包括：1)感知管道：语言处理→Florence-2检测→SAM-2分割→FoundationPose姿态估计→几何细化→时间跟踪；2)规划控制管道：混合A*路径规划→Lyapunov控制→叉子精确定位→任务规划；3)基于ROS2的系统集成管理数据流和时间约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)实时基础模型框架，实现1Hz闭环运行；2)自然语言引导的完整自主操作流程；3)在低光、雪天等挑战环境中验证，达到0.76 mIoU；4)提供详细时间分析证明实际部署可行性。相比之前工作，传统方法依赖特定传感器和受控环境，CNN方法在室外条件下性能大幅下降(0.4-0.5 mIoU)，现有语言驱动方法主要在室内验证，缺乏对室外工业条件的评估，且没有工作实现视觉语言模型与专用姿态估计模型的集成用于实时室外叉车操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Lang2Lift成功集成了视觉语言模型和基于基础模型的姿态估计，实现了无需特定任务训练数据即可在多样化环境条件下实时运行，适用于工业部署的室外机器人应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The logistics and construction industries face persistent challenges inautomating pallet handling, especially in outdoor environments with variablepayloads, inconsistencies in pallet quality and dimensions, and unstructuredsurroundings. In this paper, we tackle automation of a critical step in pallettransport: the pallet pick-up operation. Our work is motivated by laborshortages, safety concerns, and inefficiencies in manually locating andretrieving pallets under such conditions. We present Lang2Lift, a frameworkthat leverages foundation models for natural language-guided pallet detectionand 6D pose estimation, enabling operators to specify targets through intuitivecommands such as "pick up the steel beam pallet near the crane." The perceptionpipeline integrates Florence-2 and SAM-2 for language-grounded segmentationwith FoundationPose for robust pose estimation in cluttered, multi-palletoutdoor scenes under variable lighting. The resulting poses feed into a motionplanning module for fully autonomous forklift operation. We validate Lang2Lifton the ADAPT autonomous forklift platform, achieving 0.76 mIoU palletsegmentation accuracy on a real-world test dataset. Timing and error analysisdemonstrate the system's robustness and confirm its feasibility for deploymentin operational logistics and construction environments. Video demonstrationsare available at https://eric-nguyen1402.github.io/lang2lift.github.io/</description>
      <author>example@mail.com (Huy Hoang Nguyen, Johannes Huemer, Markus Murschitz, Tobias Glueck, Minh Nhat Vu, Andreas Kugi)</author>
      <guid isPermaLink="false">2508.15427v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
  <item>
      <title>A Bag of Tricks for Efficient Implicit Neural Point Clouds</title>
      <link>http://arxiv.org/abs/2508.19140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://fhahlbohm.github.io/inpc_v2/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一系列优化方法，显著提高了INPC的训练和推理性能，同时保持了视觉保真度，实现了更快的渲染速度和更低的内存使用。&lt;h4&gt;背景&lt;/h4&gt;INPC是一种结合神经场表现力和基于点渲染效率的混合表示方法，在新视角合成中实现了最先进的图像质量，但其实用性受限于渲染速度较慢的问题。&lt;h4&gt;目的&lt;/h4&gt;提高INPC的训练和推理性能，不牺牲视觉保真度，使其更具实用性。&lt;h4&gt;方法&lt;/h4&gt;改进的栅格化器实现、更有效的采样技术、集成用于孔洞填充的卷积神经网络预训练、在推理中将点建模为小高斯以提高外推质量，并通过一系列实验系统评估每个修改。&lt;h4&gt;主要发现&lt;/h4&gt;优化后的INPC流水线实现了最多25%更快的训练速度、2倍更快的渲染速度、20%更低的VRAM使用量，同时伴有轻微的图像质量提升。&lt;h4&gt;结论&lt;/h4&gt;通过一系列优化，成功解决了INPC的渲染速度瓶颈，使其更具实用性，同时保持了高质量的视觉表现。&lt;h4&gt;翻译&lt;/h4&gt;隐式神经点云（INPC）是一种结合神经场表现力和基于点渲染效率的混合表示方法，在新视角合成中实现了最先进的图像质量。然而，与其他在渲染过程中需要查询神经网络的高质量方法类似，INPC的实用性受到渲染速度相对较慢的限制。在这项工作中，我们提出了一系列优化，显著提高了INPC的训练和推理性能，同时不牺牲视觉保真度。最重要的改进包括改进的栅格化器实现、更有效的采样技术，以及用于孔洞填充的卷积神经网络的预训练集成。此外，我们证明在推理过程中可以将点建模为小高斯，以进一步提高外推质量，例如场景的特写视图。我们设计的实现具有广泛适用性，不仅限于INPC，并通过一系列实验系统评估了每个修改。我们的优化INPC流水线实现了最多25%更快的训练速度、2倍更快的渲染速度，以及20%更低的VRAM使用量，同时伴有轻微的图像质量改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决Implicit Neural Point Clouds (INPC)方法在实际应用中的效率问题。原始INPC虽然能实现高质量的图像渲染，但渲染速度慢（每秒3-10帧），训练时间长，内存需求大，限制了其交互式应用和实际部署。这个问题在现实中很重要，因为高质量新视角合成技术对虚拟现实、增强现实等应用至关重要，而效率低下使得这些技术难以在实际场景中实时应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性分析原始INPC训练和推理管道中的性能瓶颈，设计了一系列针对性优化。他们针对采样效率、渲染效率和训练效率三个主要方面分别设计了改进方法。作者借鉴了多项现有工作，包括借鉴3D Gaussian Splatting的将点渲染为小高斯的思想，采用类似3DGS的瓦片渲染方法，以及使用技术来蒸馏背景模型为球面环境贴图。作者采用'一袋技巧'的方法，即组合多个小的但有效的改进，这些改进可以独立工作也可以协同工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过一系列优化技术提高INPC的训练和推理效率而不牺牲图像质量。整体实现流程包括：1)改进的点云采样（视图特定采样使用环形缓冲区重用前一帧点云，全局预提取改进采样策略减少点数）；2)高效的特征图渲染（采用瓦片渲染降低排序复杂度，将MLP背景模型蒸馏为环境贴图，将点渲染为小高斯）；3)预训练的空洞填充CNN（三阶段预训练策略确保公共潜在空间）；4)实现改进（多个计算融合为单个CUDA内核，改进拒绝采样方法）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)改进的点云采样技术（时间稳定的视图特定采样，基于训练视图最大权重的改进全局预提取）；2)高效的特征图渲染（瓦片渲染方法，背景模型蒸馏，将点渲染为小高斯）；3)预训练的空洞填充CNN（三阶段预训练策略）；4)实现改进（计算融合，内存优化）。相比原始INPC，本文实现了渲染速度提高2倍，训练时间减少25%，内存使用降低20%，同时保持或提高图像质量；相比3DGS，在相同点数下质量更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过一系列优化技术显著提高了INPC的训练和推理效率，实现了渲染速度翻倍、训练时间减少25%、内存使用降低20%，同时保持了或略微提高了图像质量，使INPC在资源受限设备上成为3D高斯溅射的可行替代方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Implicit Neural Point Cloud (INPC) is a recent hybrid representation thatcombines the expressiveness of neural fields with the efficiency of point-basedrendering, achieving state-of-the-art image quality in novel view synthesis.However, as with other high-quality approaches that query neural networksduring rendering, the practical usability of INPC is limited by comparativelyslow rendering. In this work, we present a collection of optimizations thatsignificantly improve both the training and inference performance of INPCwithout sacrificing visual fidelity. The most significant modifications are animproved rasterizer implementation, more effective sampling techniques, and theincorporation of pre-training for the convolutional neural network used forhole-filling. Furthermore, we demonstrate that points can be modeled as smallGaussians during inference to further improve quality in extrapolated, e.g.,close-up views of the scene. We design our implementations to be broadlyapplicable beyond INPC and systematically evaluate each modification in aseries of experiments. Our optimized INPC pipeline achieves up to 25% fastertraining, 2x faster rendering, and 20% reduced VRAM usage paired with slightimage quality improvements.</description>
      <author>example@mail.com (Florian Hahlbohm, Linus Franke, Leon Overkämping, Paula Wespe, Susana Castillo, Martin Eisemann, Marcus Magnor)</author>
      <guid isPermaLink="false">2508.19140v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>RoofSeg: An edge-aware transformer-based network for end-to-end roof plane segmentation</title>
      <link>http://arxiv.org/abs/2508.19003v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 10 figures, 9 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RoofSeg的边缘感知的基于transformer的网络，用于从LiDAR点云中以真正的端到端方式分割屋顶平面，解决了现有深度学习方法中存在的三个关键问题。&lt;h4&gt;背景&lt;/h4&gt;屋顶平面分割是从机载激光雷达(LiDAR)点云重建细节级别(LoD)2和3的三维建筑模型的关键步骤。当前大多数方法依赖于手动设计或学习到的特征结合特定的几何聚类策略，基于深度学习的方法通常表现更好但存在三个未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有基于深度学习方法的三个问题：非真正端到端、边缘区域特征判别性低、平面几何特性未充分考虑，开发一种新颖的边缘感知的基于transformer的网络用于屋顶平面分割。&lt;h4&gt;方法&lt;/h4&gt;RoofSeg利用基于transformer的编码器-解码器框架，使用可学习的平面查询分层预测平面实例掩码；设计了边缘感知掩码模块(EAMM)增强边缘区域判别性；提出自适应加权策略减少误分类点影响；引入平面几何损失约束网络训练。&lt;h4&gt;主要发现&lt;/h4&gt;边缘区域的特征判别性对分割准确性至关重要；平面几何特性的考虑可以提高分割质量；端到端方法比非端到端方法能产生更优的分割结果。&lt;h4&gt;结论&lt;/h4&gt;RoofSeg通过分层预测、边缘感知掩码模块、自适应加权策略和平面几何损失，有效解决了现有深度学习方法的问题，提高了屋顶平面分割的准确性。&lt;h4&gt;翻译&lt;/h4&gt;屋顶平面分割是从机载激光雷达(LiDAR)点云重建细节级别(LoD)2和3的三维建筑模型的关键步骤之一。当前大多数用于屋顶平面分割的方法依赖于手动设计或学习到的特征，然后是一些特定的几何聚类策略。由于学习到的特征比手动设计的特征更强大，基于深度学习的方法通常比传统方法表现更好。然而，当前基于深度学习的方法有三个未解决的问题。第一，它们大多数不是真正的端到端，平面分割结果可能不是最优的。第二，点特征在边缘附近的判别性相对较低，导致平面边缘不准确。第三，平面几何特性没有被充分考虑来约束网络训练。为解决这些问题，开发了一种名为RoofSeg的新型边缘感知的基于transformer的网络，用于从LiDAR点云中以真正的端到端方式分割屋顶平面。在RoofSeg中，我们利用基于transformer的编码器-解码器框架，使用一组可学习的平面查询来分层预测平面实例掩码。为进一步提高边缘区域的分割准确性，我们还设计了一个边缘感知掩码模块(EAMM)，充分结合边缘的平面几何先验，增强平面实例掩码细化的判别性。此外，我们在掩码损失中提出了自适应加权策略，以减少误分类点的影响，并提出了一个新的平面几何损失来约束网络训练。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从机载LiDAR点云中进行屋顶平面分割的问题。现有方法存在三个关键问题：大多数方法不是真正的端到端处理、边缘区域特征区分度低导致边缘分割不准确、以及平面几何特性未被充分考虑来约束网络训练。这个问题在现实中非常重要，因为准确的屋顶平面分割是三维建筑物模型重建（特别是LoD 2和3级别）的关键步骤，能够显著提高最终3D重建的质量，而机载LiDAR获取的高精度点云是城市三维建模的重要数据源。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有屋顶平面分割方法的局限性，特别是深度学习方法中的三个未解决问题。受transformer在实例分割领域成功的启发，作者决定使用transformer架构来解决屋顶平面分割问题。为了解决边缘分割问题，作者设计了专门的Edge-Aware Mask Module (EAMM)，利用点到平面的距离作为关键几何线索。为了解决几何约束不足的问题，作者设计了新的损失函数。该方法借鉴了Mask2D和Mask3D等transformer实例分割架构，使用了PointNet++作为特征提取骨干并进行了改进，同时也参考了现有的边缘感知transformer工作，但认为其边缘感知机制相对隐式，进行了改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用端到端的transformer架构直接从点云中预测屋顶平面的实例掩码，避免依赖参数敏感的中间处理步骤；设计边缘感知掩码模块（EAMM）利用点到平面的距离作为几何线索增强边缘特征区分度；以及提出自适应加权策略的掩码损失和平面几何损失，减少误分类点影响并确保高几何保真度。整体实现流程包括：1) 使用改进的PointNet++提取多尺度点特征；2) 通过最远点采样生成查询点并使用傅里叶编码生成初始查询嵌入；3) 使用查询细化解码器层次化整合多尺度信息；4) 生成初始平面掩码和边缘掩码；5) 使用EAMM进一步细化平面掩码；6) 根据语义分数合并正掩码生成最终结果；7) 使用包含自适应加权掩码损失、平面几何损失等的统一损失函数进行训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 真正端到端的边缘感知transformer网络(RoofSeg)，完全避免了参数敏感的聚类或后处理方法；2) 边缘感知掩码模块(EAMM)，整合关键几何先验知识增强边缘特征区分度；3) 新的损失函数，包括自适应加权掩码减少误分类点和平面几何损失确保高几何保真度。相比之前工作的不同：1) 完全端到端处理，而之前方法如DeepRoofPlane仍依赖后处理步骤；2) 显著提高边缘分割精度，解决了其他方法在边缘区域表现不佳的问题；3) 通过几何约束确保预测平面具有高几何保真度，这是大多数实例分割方法未充分考虑的；4) 有效减少预测掩码中的误分类点。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; RoofSeg提出了一种基于边缘感知transformer的端到端网络，通过创新的边缘掩码模块和自适应加权损失函数，实现了从机载LiDAR点云中高精度分割屋顶平面，显著提高了边缘分割准确性和几何保真度，同时减少了误分类点，达到了当前最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Roof plane segmentation is one of the key procedures for reconstructingthree-dimensional (3D) building models at levels of detail (LoD) 2 and 3 fromairborne light detection and ranging (LiDAR) point clouds. The majority ofcurrent approaches for roof plane segmentation rely on the manually designed orlearned features followed by some specifically designed geometric clusteringstrategies. Because the learned features are more powerful than the manuallydesigned features, the deep learning-based approaches usually perform betterthan the traditional approaches. However, the current deep learning-basedapproaches have three unsolved problems. The first is that most of them are nottruly end-to-end, the plane segmentation results may be not optimal. The secondis that the point feature discriminability near the edges is relatively low,leading to inaccurate planar edges. The third is that the planar geometriccharacteristics are not sufficiently considered to constrain the networktraining. To solve these issues, a novel edge-aware transformer-based network,named RoofSeg, is developed for segmenting roof planes from LiDAR point cloudsin a truly end-to-end manner. In the RoofSeg, we leverage a transformerencoder-decoder-based framework to hierarchically predict the plane instancemasks with the use of a set of learnable plane queries. To further improve thesegmentation accuracy of edge regions, we also design an Edge-Aware Mask Module(EAMM) that sufficiently incorporates planar geometric prior of edges toenhance its discriminability for plane instance mask refinement. In addition,we propose an adaptive weighting strategy in the mask loss to reduce theinfluence of misclassified points, and also propose a new plane geometric lossto constrain the network training.</description>
      <author>example@mail.com (Siyuan You, Guozheng Xu, Pengwei Zhou, Qiwen Jin, Jian Yao, Li Li)</author>
      <guid isPermaLink="false">2508.19003v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Can we make NeRF-based visual localization privacy-preserving?</title>
      <link>http://arxiv.org/abs/2508.18971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了基于NeRF的视觉定位方法中的隐私问题，提出了一种隐私评估协议和ppNeSF方法，实现了隐私保护的视觉定位。&lt;h4&gt;背景&lt;/h4&gt;视觉定位(VL)是估计相机在已知场景中姿态的任务。NeRF-based方法虽能提供高质量的新视角合成，但会编码精细场景细节，在云定位服务中部署时可能引发隐私泄露风险。&lt;h4&gt;目的&lt;/h4&gt;解决基于NeRF的视觉定位方法中的隐私问题，防止敏感信息在云服务中被恢复。&lt;h4&gt;方法&lt;/h4&gt;1) 提出评估NeRF表示隐私保护能力的新协议；2) 开发ppNeSF(Privacy-Preserving Neural Segmentation Field)，一种使用分割监督而非RGB图像训练的NeRF变体；3) 通过自监督方式学习分割标签，确保其足够粗糙以掩盖可识别场景细节，同时保持3D区分性。&lt;h4&gt;主要发现&lt;/h4&gt;1) 使用光度损失训练的NeRF在其几何表示中存储细粒度细节，即使移除颜色预测头部也容易受到隐私攻击；2) ppNeSF的分割空间可用于准确的视觉定位，取得了最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;通过ppNeSF方法，可以在保持视觉定位准确性的同时有效保护场景隐私，防止敏感信息被提取。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位(VL)是估计相机在已知场景中姿态的任务。VL方法可根据它们如何表示场景来区分，例如通过（稀疏）点云或图像集合显式表示，或通过神经网络的权重隐式表示。最近，基于NeRF的方法在视觉定位中变得流行。虽然NeRF提供高质量的新视角合成，但它们无意中编码精细场景细节，在基于云的定位服务中部署时引发隐私问题，因为敏感信息可能被恢复。在本文中，我们从两方面解决这一挑战。首先，我们提出新协议评估基于NeRF表示的隐私保护能力。我们证明，使用光度损失训练的NeRF在其几何表示中存储细粒度细节，使它们容易受到隐私攻击，即使移除预测颜色的头部。其次，我们提出ppNeRF(隐私保护神经分割场)，一种使用分割监督而非RGB图像训练的NeRF变体。这些分割标签以自监督方式学习，确保它们足够粗糙以掩盖可识别场景细节，同时在3D中保持区分性。ppNeSF的分割空间可用于准确视觉定位，取得最先进结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于神经辐射场(NeRF)的视觉定位方法在云服务部署中可能泄露隐私信息的问题。当NeRF模型存储场景的精细细节时，可能会包含敏感的个人信息（如纹理、文本等）。这个问题很重要，因为随着自动驾驶和自主机器人的普及，视觉定位成为关键技术，而大规模视觉定位服务通常部署在云端，隐私保护成为关键需求。即使移除颜色预测头，NeRF的几何部分仍然可能存储这些敏感信息，因此需要专门的隐私保护方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了NeRF在隐私方面的脆弱性，设计了反转攻击方法来评估隐私保护程度，并使用视觉语言模型进行更全面的评估。他们发现即使移除RGB预测头，几何部分仍然存储敏感信息。受现有工作启发，特别是使用分割标签保护隐私的方法，作者提出了ppNeSF。他们借鉴了[58,60]中用分割标签替换高维描述符防止反转攻击的思想，以及SegLoc和GSFF等使用分割图对齐的定位策略，同时结合了神经隐式场的工作如iNeRF和NeFeS，创造了一种基于分割监督的NeRF变体。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是用分割标签代替RGB图像作为监督信号，避免模型直接接触潜在的隐私信息；通过自监督学习鲁棒和有区分度的分割标签；建立统一的分割空间对齐3D场景表示与2D图像编码器；通过分割图对齐实现准确的视觉定位。整体流程包括：1)架构设计（几何场、分割场、图像编码器和特征场）；2)自监督训练（学习特征嵌入空间、维护原型、优化交叉熵损失）；3)正则化（分层分割和不确定性建模）；4)视觉定位（估计初始姿态、渲染分割标签、提取2D分割、最小化交叉熵、迭代优化）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统分析神经隐式场在隐私方面的脆弱性，提出新的隐私攻击和评估协议；2)提出ppNeSF，第一个基于神经隐式场的隐私保护视觉定位方法；3)分层分割方案，提供粗细两级监督；4)不确定性建模，对模糊像素降权。相比之前工作，ppNeSF不使用光度损失避免存储纹理信息；基于神经隐式场而非显式3D基元；分割标签在3D和2D空间中对齐；通过自监督学习联合特征空间而非依赖预训练编码器。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ppNeSF，一种基于分割监督的隐私保护神经隐式场方法，能够在保持高精度视觉定位的同时有效防止敏感信息泄露，解决了基于NeRF的视觉定位在云服务中的隐私保护问题。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization (VL) is the task of estimating the camera pose in a knownscene. VL methods, a.o., can be distinguished based on how they represent thescene, e.g., explicitly through a (sparse) point cloud or a collection ofimages or implicitly through the weights of a neural network. Recently,NeRF-based methods have become popular for VL. While NeRFs offer high-qualitynovel view synthesis, they inadvertently encode fine scene details, raisingprivacy concerns when deployed in cloud-based localization services assensitive information could be recovered. In this paper, we tackle thischallenge on two ends. We first propose a new protocol to assessprivacy-preservation of NeRF-based representations. We show that NeRFs trainedwith photometric losses store fine-grained details in their geometryrepresentations, making them vulnerable to privacy attacks, even if the headthat predicts colors is removed. Second, we propose ppNeSF (Privacy-PreservingNeural Segmentation Field), a NeRF variant trained with segmentationsupervision instead of RGB images. These segmentation labels are learned in aself-supervised manner, ensuring they are coarse enough to obscure identifiablescene details while remaining discriminativeness in 3D. The segmentation spaceof ppNeSF can be used for accurate visual localization, yieldingstate-of-the-art results.</description>
      <author>example@mail.com (Maxime Pietrantoni, Martin Humenberger, Torsten Sattler, Gabriela Csurka)</author>
      <guid isPermaLink="false">2508.18971v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Drawing2CAD: Sequence-to-Sequence Learning for CAD Generation from Vectorized Drawings</title>
      <link>http://arxiv.org/abs/2508.18733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种从二维工程图自动生成参数化CAD模型的方法，解决了CAD生成领域的一个重要研究空白。&lt;h4&gt;背景&lt;/h4&gt;CAD生成建模在工业应用中推动重大创新，现有方法能从点云、网格和文本描述创建实体模型，但这些方法与传统从二维工程图开始的工业工作流程存在根本差异。&lt;h4&gt;目的&lt;/h4&gt;解决从二维矢量工程图自动生成参数化CAD模型这一未被充分探索的关键问题，使其符合传统工程设计流程。&lt;h4&gt;方法&lt;/h4&gt;提出Drawing2CAD框架，将CAD生视为序列到序列学习问题，包含三个技术组件：网络友好的矢量原语表示、双解码器Transformer架构和软目标分布损失函数，并创建了CAD-VGDrawing数据集进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;序列到序列学习框架能有效从二维图生成参数化CAD模型，三个技术组件共同保留了几何精度和设计意图，同时适应了CAD参数的固有灵活性。&lt;h4&gt;结论&lt;/h4&gt;Drawing2CAD方法成功实现了从二维工程图到参数化CAD模型的自动转换，代码和数据集已公开，为工程设计流程提供了重要工具。&lt;h4&gt;翻译&lt;/h4&gt;计算机辅助设计（CAD）生成建模正在推动工业应用领域的重大创新。最近的工作在从点云、网格和文本描述等多种输入创建实体模型方面取得了显著进展。然而，这些方法从根本上与传统工业工作流程不同，传统工作流程从二维工程图开始。从这些二维矢量图自动生成参数化CAD模型仍然是一个未被充分探索的领域，尽管它是工程设计中的一个关键步骤。为解决这一差距，我们的关键见解是将CAD生重新构建为序列到序列学习问题，其中矢量图原语直接告知参数化CAD操作的生成，在整个转换过程中保持几何精度和设计意图。我们提出Drawing2CAD，这是一个包含三个关键技术组件的框架：一种网络友好的矢量原语表示，保留精确的几何信息；一个双解码器Transformer架构，将命令类型和参数生成解耦，同时保持精确对应；以及一个适应CAD参数固有灵活性的软目标分布损失函数。为训练和评估Drawing2CAD，我们创建了CAD-VGDrawing数据集，包含配对的工程图和参数化CAD模型，并进行彻底的实验以证明我们方法的有效性。代码和数据集可在https://github.com/lllssc/Drawing2CAD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer-Aided Design (CAD) generative modeling is driving significantinnovations across industrial applications. Recent works have shown remarkableprogress in creating solid models from various inputs such as point clouds,meshes, and text descriptions. However, these methods fundamentally divergefrom traditional industrial workflows that begin with 2D engineering drawings.The automatic generation of parametric CAD models from these 2D vector drawingsremains underexplored despite being a critical step in engineering design. Toaddress this gap, our key insight is to reframe CAD generation as asequence-to-sequence learning problem where vector drawing primitives directlyinform the generation of parametric CAD operations, preserving geometricprecision and design intent throughout the transformation process. We proposeDrawing2CAD, a framework with three key technical components: anetwork-friendly vector primitive representation that preserves precisegeometric information, a dual-decoder transformer architecture that decouplescommand type and parameter generation while maintaining precise correspondence,and a soft target distribution loss function accommodating inherent flexibilityin CAD parameters. To train and evaluate Drawing2CAD, we create CAD-VGDrawing,a dataset of paired engineering drawings and parametric CAD models, and conductthorough experiments to demonstrate the effectiveness of our method. Code anddataset are available at https://github.com/lllssc/Drawing2CAD.</description>
      <author>example@mail.com (Feiwei Qin, Shichao Lu, Junhao Hou, Changmiao Wang, Meie Fang, Ligang Liu)</author>
      <guid isPermaLink="false">2508.18733v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>A holistic perception system of internal and external monitoring for ground autonomous vehicles: AutoTRUST paradigm</title>
      <link>http://arxiv.org/abs/2508.17969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种自动驾驶车辆的内外部整体感知系统，包括内部监控和外部监控两个子系统，旨在优化车辆感知能力和乘客体验。&lt;h4&gt;背景&lt;/h4&gt;该研究在欧盟地平线欧洲计划AutoTRUST的背景下进行，专注于开发先进的车辆感知技术。&lt;h4&gt;目的&lt;/h4&gt;展示一种利用人工智能的自适应框架，优化自动驾驶车辆的感知能力和乘客体验。&lt;h4&gt;方法&lt;/h4&gt;内部监控系统使用多摄像头进行面部识别，集成大型语言模型作为虚拟助手，并部署AI智能传感器监测空气质量和热舒适度；外部监控系统采用基于LiDAR的成本效益高的语义分割方法，对低质量原始3D点云进行超分辨率处理；系统已集成到ALKE提供的真实电动汽车上。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证表明，所提出的感知架构的模块化块在性能和效率方面都有显著提高。&lt;h4&gt;结论&lt;/h4&gt;该整体感知系统成功实现了自动驾驶车辆内外部环境的全面监控，为先进车辆技术提供了创新解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种用于自动驾驶车辆内外部监控的整体感知系统，旨在展示一种新颖的、利用人工智能的自适应先进车辆技术和解决方案框架，优化车内感知和体验。内部监控系统依赖多摄像头设置，通过面部识别预测和识别驾驶员和乘员行为，同时利用大型语言模型作为虚拟助手。此外，舱内监控系统包括由人工智能驱动的智能传感器，用于测量空气质量并进行热舒适度分析，以实现高效的上下车。另一方面，外部监控系统通过基于LiDAR的成本效益高的语义分割方法来感知车辆周围环境，该方法对低质量的原始3D点云执行高精度和高效的超分辨率。整体感知框架是在欧盟地平线欧洲计划AutoTRUST的背景下开发的，并已集成并部署在ALKE提供的真实电动汽车上。在意大利Ispra的联合研究中心集成站点进行的实验验证和评估，强调了所提出的感知架构模块化块的性能和效率提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶汽车内部和外部环境的全面感知问题。内部感知需要监控驾驶员行为、乘客情绪、身份识别及车内环境质量；外部感知需要高效理解车辆周围环境，特别是使用低成本LiDAR进行准确环境分割。这个问题在现实中至关重要，因为它关系到自动驾驶汽车的安全性、乘客舒适度和个性化体验，同时能有效应对当前系统在实时处理、集成可行性和嵌入式部署方面的挑战。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从自动驾驶汽车需要全面感知系统的需求出发，分别针对内部和外部感知进行了设计。内部感知结合多摄像头、面部识别、情绪分析和环境传感器；外部感知专注于低成本LiDAR的超分辨率处理。该方法借鉴了多项现有工作：使用轻量级CNN和transformer模型进行驾驶舱活动识别，采用高效声音事件检测支持安全，利用智能环境监测系统，以及应用超分辨率技术增强LiDAR数据。同时，作者将这些现有技术进行了优化和适配，形成一个统一的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是开发一个统一的、可部署的框架，集成驾驶舱情感感知、虚拟助手、环境监控和高效的LiDAR超分辨率处理。内部感知系统包括：1)驾驶员行为监控(89%准确率的分心检测)，2)情绪识别(72.9%准确率)，3)乘客识别(96.5%准确率)，4)物体检测(92.3% mAP)，5)声音事件检测(94%准确率)，6)空气质量监测。外部感知系统使用16通道LiDAR获取原始点云，应用超分辨率模块提升质量，然后进行语义分割。虚拟助手基于量化Llama 3模型，结合语音识别和合成，提供实时上下文感知响应。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的整体感知框架，整合内部和外部感知；2)先进的内部感知系统，结合情绪识别、虚拟助手和空气质量监测；3)创新的外部感知系统，使用基于模型的LiDAR超分辨率网络提高低成本传感器的性能；4)虚拟助手集成，在嵌入式设备上高效部署大语言模型。相比之前工作，不同之处在于：大多数现有解决方案专注于单一方面的感知，而本文提供统一框架；系统针对嵌入式平台优化实现实时性能；超分辨率方法使低成本LiDAR能提供高质量感知；虚拟助手提供实时上下文感知交互，这在自动驾驶系统中较少见。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个统一的AutoTRUST感知范式，通过集成先进的内部监控和高效的低成本LiDAR外部感知系统，显著提高了自动驾驶汽车的安全性、舒适性和环境感知能力，并在真实电动汽车平台上成功验证了其实时性能和实用性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a holistic perception system for internal and externalmonitoring of autonomous vehicles, with the aim of demonstrating a novelAI-leveraged self-adaptive framework of advanced vehicle technologies andsolutions that optimize perception and experience on-board. Internal monitoringsystem relies on a multi-camera setup designed for predicting and identifyingdriver and occupant behavior through facial recognition, exploiting in additiona large language model as virtual assistant. Moreover, the in-cabin monitoringsystem includes AI-empowered smart sensors that measure air-quality and performthermal comfort analysis for efficient on and off-boarding. On the other hand,external monitoring system perceives the surrounding environment of vehicle,through a LiDAR-based cost-efficient semantic segmentation approach, thatperforms highly accurate and efficient super-resolution on low-quality raw 3Dpoint clouds. The holistic perception framework is developed in the context ofEU's Horizon Europe programm AutoTRUST, and has been integrated and deployed ona real electric vehicle provided by ALKE. Experimental validation andevaluation at the integration site of Joint Research Centre at Ispra, Italy,highlights increased performance and efficiency of the modular blocks of theproposed perception architecture.</description>
      <author>example@mail.com (Alexandros Gkillas, Christos Anagnostopoulos, Nikos Piperigkos, Dimitris Tsiktsiris, Theofilos Christodoulou, Theofanis Siamatras, Dimitrios Triantafyllou, Christos Basdekis, Theoktisti Marinopoulou, Panagiotis Lepentsiotis, Elefterios Blitsis, Aggeliki Zacharaki, Nearchos Stylianidis, Leonidas Katelaris, Lamberto Salvan, Aris S. Lalos, Christos Laoudias, Antonios Lalas, Konstantinos Votis)</author>
      <guid isPermaLink="false">2508.17969v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking the Detail-Preserved Completion of Complex Tubular Structures based on Point Cloud: a Dataset and a Benchmark</title>
      <link>http://arxiv.org/abs/2508.17658v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究首次探索了基于点云的管状结构补全方法，建立了PC-CAC数据集，并提出了TSRNet网络，有效解决了医学成像中管状结构不连续的问题。&lt;h4&gt;背景&lt;/h4&gt;复杂管状结构在医学成像和计算机辅助诊断中至关重要，其完整性有助于解剖可视化和病变检测。然而现有分割算法难以处理结构不连续问题，特别是在冠状动脉狭窄和血管闭塞等严重临床案例中，导致诊断准确性下降。&lt;h4&gt;目的&lt;/h4&gt;重新连接不连续的管状结构以确保其完整性，并探索基于点云的管状结构补全方法。&lt;h4&gt;方法&lt;/h4&gt;建立了基于点云的冠状动脉补全(PC-CAC)数据集，来源于真实临床数据；提出了TSRNet（管状结构重连接网络），该网络集成了细节保留特征提取器、多重密集细化策略和全局到局部损失函数，确保准确重新连接的同时保持结构完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在PC-CAC和另外两个公共数据集（PC-ImageCAS和PC-PTR）上的综合实验表明，该方法在多个评估指标上始终优于最先进的方法，为基于点云的管状结构重建设定了新的基准。&lt;h4&gt;结论&lt;/h4&gt;提出的TSRNet方法和PC-CAC数据集为点云基础的管状结构重建建立了新的基准，可通过https://github.com/YaoleiQi/PCCAC获取。&lt;h4&gt;翻译&lt;/h4&gt;复杂的管状结构在医学成像和计算机辅助诊断中必不可少，它们的完整性增强了解剖可视化和病变检测。然而，现有的分割算法难以处理结构不连续问题，特别是在冠状动脉狭窄和血管闭塞等严重临床案例中，这导致不期望的不连续性并影响下游诊断的准确性。因此，必须重新连接不连续结构以确保其完整性。在本研究中，我们首次探索了基于点云的管状结构补全，并建立了基于点云的冠状动脉补全(PC-CAC)数据集，该数据集来源于真实临床数据。该数据集为管状结构补全提供了新的基准。此外，我们提出了TSRNet，一个管状结构重连接网络，它集成了细节保留特征提取器、多重密集细化策略和全局到局部损失函数，以确保准确重新连接的同时保持结构完整性。在我们的PC-CAC和另外两个公共数据集（PC-ImageCAS和PC-PTR）上的综合实验表明，我们的方法在多个评估指标上始终优于最先进的方法，为基于点云的管状结构重建设定了新的基准。我们的基准可在https://github.com/YaoleiQi/PCCAC获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决复杂管状结构（如血管）在医学成像中的不连续性问题。在临床应用中，如冠状动脉狭窄、血管闭塞等情况下，现有分割算法常产生结构断裂，影响解剖可视化和病变检测，进而降低下游诊断准确性。这个问题在医学影像分析中至关重要，因为血管等管状结构的完整性对于准确诊断和治疗决策具有重要意义，特别是在严重临床病例中。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有基于体素(voxel)方法的局限性出发，认识到在处理模糊图像、完全闭塞等复杂场景时，体素方法难以保持结构完整性。因此转向点云(point cloud)表示，因其具有计算效率和准确性优势。在设计方法时，作者借鉴了PointNet/PointNet++的点云处理基础架构，结合集合抽象(Set Abstraction)和Transformer思想提出TransSA模块，同时参考了随机行走等传统方法但进行了改进。针对点分布不均衡和复杂全局拓扑两大挑战，作者设计了核心点选择模块和多密集细化策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是基于点云的管状结构重建方法，通过细节保留的特征提取、多密集细粒度策略和全局到局部的损失函数，确保准确重建同时保持结构完整性，特别关注细长管状结构的重建。整体流程包括：1)细节保留特征提取器：通过核心点选择模块(CPS)分析点云密度分布，分离密集和稀疏区域，结合TransSA模块提取特征；2)多密集细粒度策略：设计多个细化模块逐步重建核心点云为粗略点云，再细化为密集点云；3)全局到局部损失函数：结合Chamfer Distance和Fidelity Error，使用阈值控制训练阶段，先学习全局结构一致性，再细化局部细节。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建首个基于点云的冠状动脉完成(PC-CAC)数据集，来源于真实临床数据，包含分段等不完美之处；2)首次从点云角度重新定义管状结构重建任务；3)提出管状结构重建网络(TSRNet)，包含细节保留特征提取器、多密集细粒度策略和全局到局部损失函数；4)在三个数据集上提供标准化评估基准。相比之前工作，本文从体素方法转向点云方法，专门针对细长管状结构设计，解决了点分布不均衡和复杂全局拓扑挑战，提供了专门的数据集和评估基准。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提出了基于点云的复杂管状结构重建方法，通过构建专门的PC-CAC数据集和TSRNet网络，有效解决了医学成像中血管等管状结构的不连续问题，显著提高了诊断准确性和下游任务效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex tubular structures are essential in medical imaging andcomputer-assisted diagnosis, where their integrity enhances anatomicalvisualization and lesion detection. However, existing segmentation algorithmsstruggle with structural discontinuities, particularly in severe clinical casessuch as coronary artery stenosis and vessel occlusions, which leads toundesired discontinuity and compromising downstream diagnostic accuracy.Therefore, it is imperative to reconnect discontinuous structures to ensuretheir completeness. In this study, we explore the tubular structure completionbased on point cloud for the first time and establish a Point Cloud-basedCoronary Artery Completion (PC-CAC) dataset, which is derived from realclinical data. This dataset provides a novel benchmark for tubular structurecompletion. Additionally, we propose TSRNet, a Tubular Structure ReconnectionNetwork that integrates a detail-preservated feature extractor, a multipledense refinement strategy, and a global-to-local loss function to ensureaccurate reconnection while maintaining structural integrity. Comprehensiveexperiments on our PC-CAC and two additional public datasets (PC-ImageCAS andPC-PTR) demonstrate that our method consistently outperforms state-of-the-artapproaches across multiple evaluation metrics, setting a new benchmark forpoint cloud-based tubular structure reconstruction. Our benchmark is availableat https://github.com/YaoleiQi/PCCAC.</description>
      <author>example@mail.com (Yaolei Qi, Yikai Yang, Wenbo Peng, Shumei Miao, Yutao Hu, Guanyu Yang)</author>
      <guid isPermaLink="false">2508.17658v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Finding Outliers in a Haystack: Anomaly Detection for Large Pointcloud Scenes</title>
      <link>http://arxiv.org/abs/2508.17634v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv Preprint, paper has since been accepted to ACPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于重建的户外场景开放集分割新方法，结合物体缺陷检测研究和Mamba架构的优势，显著提升了分割性能，并贡献了一种在大型点云上具有竞争力的Mamba架构。&lt;h4&gt;背景&lt;/h4&gt;激光雷达扫描在户外场景中获得大范围内的精确距离测量，产生大规模点云，应用于机器人、汽车和土地监测等领域。在这些应用中，不可避免会出现训练数据之外的异常物体。&lt;h4&gt;目的&lt;/h4&gt;开发一种开放集分割的新方法，有效处理训练数据之外的异常物体，提高户外场景点云分割的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;结合物体缺陷检测研究的经验和Mamba架构在利用长距离依赖性和处理大规模数据方面的优势，创建了一种基于重建的户外场景开放集分割方法。&lt;h4&gt;主要发现&lt;/h4&gt;1. 该方法应用于作者自己的开放集分割方法时提高了性能；2. 该方法应用于现有方法时也提高了性能；3. 作者贡献的基于Mamba的架构在具有挑战性的大规模点云上与现有的基于体素卷积的方法具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;提出的基于重建的开放集分割方法和基于Mamba的架构在户外场景点云分割任务中表现出色，特别是在处理大规模点云和识别训练数据之外的异常物体方面。&lt;h4&gt;翻译&lt;/h4&gt;户外场景中的激光雷达扫描可获得大范围内的精确距离测量，产生大规模点云。此类数据的应用示例包括机器人、汽车和土地监测。在这些应用中，不可避免会出现训练数据之外的异常物体。我们的研究贡献了一种开放集分割的新方法，利用了物体缺陷检测研究的经验。我们还借鉴了Mamba架构在利用长距离依赖性和处理大规模数据方面的强大性能。结合两者，我们为户外场景开放集分割任务创建了一种基于重建的方法。我们证明，当应用于我们自己的开放集分割方法时，我们的方法提高了性能，当应用于现有方法时也是如此。此外，我们还贡献了一种基于Mamba的架构，在具有挑战性的大规模点云上与现有的基于体素卷积的方法具有竞争力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大规模点云场景中的异常物体检测问题。在户外场景的LiDAR扫描应用中（如自动驾驶、机器人导航和土地监测），总会出现训练数据中没有的未知物体。准确识别这些异常物体对于确保系统在实际应用中的可靠性和安全性至关重要，因为无法识别的异常可能导致系统决策错误或安全隐患。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将两个现有研究方向结合：单物体缺陷检测和场景级异常检测，定义为'识别场景中的未知异常'。他们借鉴了物体缺陷检测中的重建方法，以及Mamba架构在处理长距离依赖关系和大规模数据方面的优势。作者还参考了Cen等人创建合成异常的方法，但提出了改进的'魔方'方法来生成更有效的训练数据。整体设计思路是通过训练只使用已知物体的重建系统，使其在遇到未知异常时产生明显差异，从而实现异常检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过训练一个场景重建器，使其只学习已知物体的'默认上下文'，当遇到未知异常物体时，重建会产生明显误差，从而帮助识别这些异常。整体流程包括：1) 使用Mamba编码器-解码器架构训练场景重建器，仅用已知物体数据；2) 计算原始点云与重建点云之间的差异；3) 使用这些差异作为输入，通过基于Mamba的异常检测架构识别异常；4) 同时进行已知物体的语义分割。训练过程中使用创新的'魔方'方法创建合成异常数据，避免简单缩放带来的问题。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 将单物体缺陷检测中的重建方法首次应用于大规模场景级异常检测；2) 首次将Mamba架构引入点云场景异常检测，利用其处理长距离依赖和大规模数据的能力；3) 提出新的'魔方'方法创建合成异常，避免简单缩放导致的点密度差异问题；4) 使用相对坐标重建而非全局坐标重建，使错误表现为表面变形更易识别。相比之前工作，本文直接编码解码原始输入而非依赖预测语义标签，使用Mamba而非体素卷积作为骨干网络，并提出了更有效的异常数据生成方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合Mamba架构和场景重建的创新方法，显著提升了大规模点云场景中未知异常物体的检测性能，为户外场景的可靠应用提供了重要技术支持。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR scanning in outdoor scenes acquires accurate distance measurements overwide areas, producing large-scale point clouds. Application examples for thisdata include robotics, automotive vehicles, and land surveillance. During suchapplications, outlier objects from outside the training data will inevitablyappear. Our research contributes a novel approach to open-set segmentation,leveraging the learnings of object defect-detection research. We also draw onthe Mamba architecture's strong performance in utilising long-rangedependencies and scalability to large data. Combining both, we create areconstruction based approach for the task of outdoor scene open-setsegmentation. We show that our approach improves performance not only whenapplied to our our own open-set segmentation method, but also when applied toexisting methods. Furthermore we contribute a Mamba based architecture which iscompetitive with existing voxel-convolution based methods on challenging,large-scale pointclouds.</description>
      <author>example@mail.com (Ryan Faulkner, Luke Haub, Simon Ratcliffe, Tat-Jun Chin)</author>
      <guid isPermaLink="false">2508.17634v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Variational Shape Inference for Grasp Diffusion on SE(3)</title>
      <link>http://arxiv.org/abs/2508.17482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于变分形状推断的多模态抓取合成框架，利用隐式神经表示和扩散模型在SE(3)流形上生成多样化的稳定抓取，并引入了测试时抓取优化技术。&lt;h4&gt;背景&lt;/h4&gt;抓取合成是机器人操作的基础任务，通常有多个可行解决方案。多模态抓取合成旨在根据物体几何形状生成多样化的稳定抓取集合，而几何特征的鲁棒学习对成功至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决形状噪声和测量稀疏性带来的挑战，提出一种能够学习多模态抓取分布的框架，增强对形状噪声和测量稀疏性的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;首先训练一个使用隐式神经表示的变分自编码器进行形状推断，然后利用这些学习的几何特征引导在SE(3)流形上的抓取合成扩散模型，并引入测试时抓取优化技术作为插件进一步提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;在ACRONYM数据集上，该方法比最先进的多模态抓取合成方法高出6.3%，对点云密度下降表现出鲁棒性，并能实现无样本迁移到家用物体的现实操作，尽管存在测量噪声和点云校准误差，仍比基线方法多产生34%的成功抓取。&lt;h4&gt;结论&lt;/h4&gt;该形状推断抓取合成方法在性能和鲁棒性方面均优于现有方法，能够有效处理形状噪声和测量稀疏性问题，并成功迁移到现实世界应用中。&lt;h4&gt;翻译&lt;/h4&gt;抓取合成是机器人操作中的一个基础任务，通常有多个可行的解决方案。多模态抓取合成旨在根据物体几何形状生成多样化的稳定抓取集合，这使得几何特征的鲁棒学习对成功至关重要。为了应对这一挑战，我们提出了一个学习多模态抓取分布的框架，利用变分形状推断来增强对形状噪声和测量稀疏性的鲁棒性。我们的方法首先使用隐式神经表示训练一个用于形状推断的变分自编码器，然后使用这些学习的几何特征来引导在SE(3)流形上的抓取合成扩散模型。此外，我们引入了一种测试时抓取优化技术，可以作为插件集成以进一步提高抓取性能。实验结果表明，我们的形状推断抓取合成方法在ACRONYM数据集上比最先进的多模态抓取合成方法高出6.3%，同时与其他方法相比，对点云密度下降表现出鲁棒性。此外，我们的训练模型实现了对家用物体现实操作的无样本迁移，尽管存在测量噪声和点云校准误差，但仍比基线方法多产生34%的成功抓取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人抓取任务中的多模态抓取合成问题，即在给定物体几何形状时生成多种多样的稳定抓取方案。这个问题在现实中非常重要，因为自主机器人在非结构化环境中需要能够稳健地与各种物体交互，而现实中的感知系统往往会产生不完整或带有噪声的点云数据，现有方法难以同时保证抓取的多样性、稳定性和对噪声/不完整数据的鲁棒性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从几何特征理解的重要性出发，认识到高质量抓取需要能跨物体形状转移且对不完美观测鲁棒的形状特征。他们借鉴了变分自编码器(VAE)进行形状推断，特别是Chou等人的PointVAE架构；采用扩散模型(特别是基于分数匹配的扩散模型)来学习抓取分布，参考了Urain等人的SE(3)-Diff方法；同时利用符号距离场(SDF)和隐式神经表示(INR)来表示几何形状。作者设计了一个两阶段流程：先学习鲁棒的形状特征，再使用这些特征指导抓取合成，并额外引入了测试时的抓取优化技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习一个对噪声和稀疏观测具有鲁棒性的形状特征空间，然后利用这些特征指导抓取姿势的生成，同时通过测试时优化进一步提高抓取质量。整体流程分为三部分：1)形状推断阶段：使用PointVAE架构将输入点云编码为潜在表示，并重建SDF；2)抓取合成阶段：使用去噪分数匹配在SE(3)流形上学习抓取分布，通过反向Langevin动力学生成抓取样本；3)测试时优化阶段：学习夹爪的神经SDF，构建可微分的碰撞避免和捏取稳定性目标，在最后阶段优化生成的抓取姿势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)变分形状推断：使用变分自编码器学习结构化的形状潜在空间，增强对噪声和稀疏点云的鲁棒性；2)两阶段框架：先学习形状特征再指导抓取合成，使形状特征可独立优化；3)测试时抓取优化：提出可微分优化技术，结合碰撞避免和捏取稳定性目标；4)能量模型 formulation：将分数函数建模为能量场的负梯度，便于与其他目标组合。相比之前工作，与cVAE相比避免了模式崩溃且无需额外分类器；与SE(3)-Diff相比使用更强大的变分形状推断；与Bridger相比形状推断更鲁棒；与GLDM相比直接在原始空间工作而非学习抓取潜在空间。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合变分形状推断和扩散模型的框架，通过学习对噪声和稀疏观测鲁棒的几何特征，实现了在SE(3)流形上的多模态抓取合成，并引入测试时优化技术，在模拟和真实世界场景中均展现出优越的性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasp synthesis is a fundamental task in robotic manipulation which usuallyhas multiple feasible solutions. Multimodal grasp synthesis seeks to generatediverse sets of stable grasps conditioned on object geometry, making the robustlearning of geometric features crucial for success. To address this challenge,we propose a framework for learning multimodal grasp distributions thatleverages variational shape inference to enhance robustness against shape noiseand measurement sparsity. Our approach first trains a variational autoencoderfor shape inference using implicit neural representations, and then uses theselearned geometric features to guide a diffusion model for grasp synthesis onthe SE(3) manifold. Additionally, we introduce a test-time grasp optimizationtechnique that can be integrated as a plugin to further enhance graspingperformance. Experimental results demonstrate that our shape inference forgrasp synthesis formulation outperforms state-of-the-art multimodal graspsynthesis methods on the ACRONYM dataset by 6.3%, while demonstratingrobustness to deterioration in point cloud density compared to otherapproaches. Furthermore, our trained model achieves zero-shot transfer toreal-world manipulation of household objects, generating 34% more successfulgrasps than baselines despite measurement noise and point cloud calibrationerrors.</description>
      <author>example@mail.com (S. Talha Bukhari, Kaivalya Agrawal, Zachary Kingston, Aniket Bera)</author>
      <guid isPermaLink="false">2508.17482v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Registration via Geometric Overlapping Guided Rotation Search</title>
      <link>http://arxiv.org/abs/2508.17427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于几何最大重叠的配准框架，通过仅旋转的分支限界搜索实现点云配准，解决了现有方法在高异常值比例下的配准问题，同时降低了计算复杂度。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的点云配准方法主要使用空间兼容性图或分支限界搜索，主要针对高异常值比例下的配准问题。然而，基于图的方法需要至少二次方的时间和空间复杂度，而多阶段分支限界搜索方法常因分解阶段间的局部最优问题导致不准确。&lt;h4&gt;目的&lt;/h4&gt;开发一种点云配准方法，能够在高异常值比例下实现准确配准，同时降低计算复杂度，提高效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种几何最大重叠配准框架，使用Chasles定理将刚体变换分解为沿旋转轴的平移和2D刚体变换。通过分支限界搜索最优旋转轴和角度，将剩余参数表述为范围最大查询问题。使用立方映射参数化的半球内搜索候选旋转轴，并将2D配准放松为1D旋转角度搜索，使用扫描线算法和线段树在多项式时间内解决。&lt;h4&gt;主要发现&lt;/h4&gt;在3DMatch、3DLoMatch和KITTI数据集上的实验结果表明，该方法比现有最先进方法具有更高的准确性和效率，时间复杂度为多项式，空间复杂度随点数线性增加，即使在最坏情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;所提出的几何最大重叠配准框架有效解决了高异常值比例下的点云配准问题，在保持高精度的同时显著降低了计算复杂度，为点云配准领域提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;基于对应关系的点云配准计算在噪声阈值约束下最大化内点数量的刚体变换。当前最先进方法采用空间兼容性图或分支限界搜索，主要关注高异常值比例下的配准。然而，基于图的方法需要至少二次方的时间和空间复杂度来构建图，而多阶段分支限界搜索方法常因分解阶段间的局部最优问题导致不准确。本文提出了一种通过仅旋转分支限界搜索的几何最大重叠配准框架。使用Chasles定理将刚体变换分解为沿旋转轴的平移和2D刚体变换。通过分支限界搜索最优旋转轴和角度，剩余参数表述为范围最大查询问题。首先，在立方映射参数化的半球内搜索前k个候选旋转轴，并通过将对应点投影到该轴上的区间刺穿来估计沿每个轴的平移。其次，将2D配准放松为使用轴对齐矩形的2D几何重叠的1D旋转角度搜索，使用扫描线算法和线段树在多项式时间内确定性解决。在3DMatch、3DLoMatch和KITTI数据集上的实验结果表明，该方法比最先进方法具有更高的准确性和效率，时间复杂度为多项式，空间复杂度随点数线性增加，即使在最坏情况下也是如此。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云配准中的鲁棒性问题，特别是在高异常值比例下的配准挑战。这个问题在SLAM、计算机辅助手术和增强现实等领域至关重要，因为准确的点云配准是这些应用的基础，而现实场景中往往存在大量异常值，影响配准精度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：图方法需要二次的时间和空间复杂度，多阶段BnB方法容易陷入局部最优。他们借鉴了Chasles'定理分解刚体变换，结合分支定界搜索、区间刺穿算法和扫描线算法等现有技术，创新性地设计了两个阶段的旋转专用BnB搜索框架，并利用立方映射参数化和几何重叠查询来提高效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将6自由度的点云配准问题分解为两个更简单的子问题：先找到最优旋转轴并估计沿轴平移，再找到最优旋转角度并估计旋转中心。整体流程包括：1)预处理加权特征匹配；2)第一阶段使用立方映射进行旋转轴BnB搜索，结合区间刺穿算法估计平移；3)第二阶段将2D配准简化为1D旋转角搜索，用扫描线算法和线段树解决几何重叠问题；4)后处理使用IRLS优化结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)两阶段旋转专用BnB框架，保证全局收敛和多项式复杂度；2)立方映射半球参数化与自适应区间刺穿相结合加速旋转轴搜索；3)确定性多项式时间2D配准解决方案，将问题转化为1D旋转角搜索。相比图方法，避免了二次复杂度；相比多阶段BnB方法，减轻了局部最优问题；相比传统RANSAC，保证了全局最优性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于几何重叠引导的旋转搜索的鲁棒点云配准方法，通过两阶段分支定界搜索和计算几何技术，实现了在高异常值比例下的高效、准确配准，同时保证了多项式时间复杂度和线性空间复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration based on correspondences computes the rigidtransformation that maximizes the number of inliers constrained within thenoise threshold. Current state-of-the-art (SOTA) methods employing spatialcompatibility graphs or branch-and-bound (BnB) search mainly focus onregistration under high outlier ratios. However, graph-based methods require atleast quadratic space and time complexity for graph construction, whilemulti-stage BnB search methods often suffer from inaccuracy due to local optimabetween decomposed stages. This paper proposes a geometric maximum overlappingregistration framework via rotation-only BnB search. The rigid transformationis decomposed using Chasles' theorem into a translation along rotation axis anda 2D rigid transformation. The optimal rotation axis and angle are searched viaBnB, with residual parameters formulated as range maximum query (RMQ) problems.Firstly, the top-k candidate rotation axes are searched within a hemisphereparameterized by cube mapping, and the translation along each axis is estimatedthrough interval stabbing of the correspondences projected onto that axis.Secondly, the 2D registration is relaxed to 1D rotation angle search with 2DRMQ of geometric overlapping for axis-aligned rectangles, which is solveddeterministically in polynomial time using sweep line algorithm with segmenttree. Experimental results on 3DMatch, 3DLoMatch, and KITTI datasetsdemonstrate superior accuracy and efficiency over SOTA methods, while the timecomplexity is polynomial and the space complexity increases linearly with thenumber of points, even in the worst case.</description>
      <author>example@mail.com (Zhao Zheng, Jingfan Fan, Long Shao, Hong Song, Danni Ai, Tianyu Fu, Deqiang Xiao, Yongtian Wang, Jian Yang)</author>
      <guid isPermaLink="false">2508.17427v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Deep Learning-based Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2508.17011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了截至2025年8月的基于深度学习的点云去噪方法，从监督级别和建模视角两个维度组织文献，提出功能分类法，分析架构趋势，建立统一基准，并评估多种性能指标。&lt;h4&gt;背景&lt;/h4&gt;准确的3D几何获取对计算机图形学、自动驾驶、机器人和增强现实等多种应用至关重要，但现实环境中获取的原始点云常受传感器、光照、材料、环境等因素影响而含有噪声，降低了几何保真度并影响下游性能。&lt;h4&gt;目的&lt;/h4&gt;点云去噪旨在恢复干净的点集同时保留底层结构，本文提供一个全面且最新的基于深度学习的点云去噪方法综述，并讨论开放挑战与未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;从监督级别（有监督vs无监督）和建模视角两个维度组织文献，提出基于去噪原理的功能分类法，分析架构趋势（结构和时间维度），建立具有一致训练设置的统一基准，从去噪质量、表面保真度、点分布和计算效率等方面评估方法。&lt;h4&gt;主要发现&lt;/h4&gt;传统基于优化的方法依赖于手工设计的滤波器或几何先验，难以处理多样化和复杂的噪声模式；而深度学习方法利用神经网络架构学习独特表示，在复杂和大规模点云上显示出强大的效果。&lt;h4&gt;结论&lt;/h4&gt;点云去噪领域仍存在开放挑战，需要进一步研究探索更有效的深度学习方法，以应对日益复杂的点云数据和噪声模式。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D几何获取对广泛的应用至关重要，如计算机图形学、自动驾驶、机器人和增强现实。然而，现实环境中获取的原始点云常受传感器、光照、材料、环境等多种因素影响而含有噪声，这降低了几何保真度并损害了下游性能。点云去噪是一个基础问题，旨在恢复干净的点集同时保留底层结构。基于经典优化的方法，由手工设计的滤波器或几何先验指导，已被广泛研究但难以处理多样化和复杂的噪声模式。最近的深度学习方法利用神经网络架构学习独特表示，并在复杂和大规模点云上显示出强大的效果。鉴于这些显著进展，本综述提供了截至2025年8月的基于深度学习的点云去噪方法的全面且最新的回顾。我们从两个视角组织文献：(1) 监督级别（有监督vs无监督），(2) 建模视角，提出一个基于去噪原理的功能分类法，统一不同方法。我们进一步从结构和时间维度分析架构趋势，建立具有一致训练设置的统一基准，并从去噪质量、表面保真度、点分布和计算效率等方面评估方法。最后，我们讨论了开放挑战并概述了这一快速发展领域的未来研究方向。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云去噪问题，即从受噪声污染的3D点数据中恢复干净点集同时保留底层几何结构。这个问题在现实中非常重要，因为准确的3D几何数据对计算机图形学、自动驾驶、机器人和增强现实等应用至关重要。噪声污染会降低几何保真度，影响后续任务性能，而传统方法难以处理复杂多样的噪声模式。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到点云去噪问题的重要性和传统方法的局限性，观察到深度学习方法在该领域的快速增长。作者发现现有综述往往按网络架构分类，忽略了不同方法可能采用根本不同的问题表述。因此，作者借鉴了现有工作的分类方法，但提出了基于功能/建模策略的创新分类方式，并建立了统一的评估框架。论文引用了大量先前研究，表明作者对领域有深入了解，并在现有基础上进行了系统化的整理和分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 由于这是一篇综述论文而非提出新方法，论文将现有的深度学习点云去噪方法分为五大类：1)基于重建的方法，生成更干净的新点集；2)基于位移的方法，估计每个点的噪声分量并学习位移向量；3)基于分布的方法，从概率角度表述去噪问题；4)基于滤波的方法，估计辅助信息并应用传统滤波规则；5)基于分类的方法，将去噪视为点分类任务。此外，论文还讨论了无监督学习方法，适用于标签数据稀缺的场景。论文按监督级别（监督vs无监督）和建模视角组织文献，提出统一的功能分类。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 论文的关键创新点包括：1)提出基于功能原则的新分类框架，而非按网络架构分类；2)建立统一的评估基准，在一致设置下评估方法；3)提供全面系统的综述，按底层问题表述和架构设计组织方法；4)分析去噪质量、表面保真度、点分布和计算效率等多方面。相比之前的工作，这篇论文专注于深度学习方法，更新至2025年最新进展，并提供了更公平的比较框架，使不同方法的评估更加客观。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提供了一个全面系统的深度学习点云去噪方法综述，提出了基于功能原则的新分类框架，并建立了统一的评估基准，为该领域的研究提供了清晰的路线图和未来方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D geometry acquisition is essential for a wide range ofapplications, such as computer graphics, autonomous driving, robotics, andaugmented reality. However, raw point clouds acquired in real-worldenvironments are often corrupted with noise due to various factors such assensor, lighting, material, environment etc, which reduces geometric fidelityand degrades downstream performance. Point cloud denoising is a fundamentalproblem, aiming to recover clean point sets while preserving underlyingstructures. Classical optimization-based methods, guided by hand-craftedfilters or geometric priors, have been extensively studied but struggle tohandle diverse and complex noise patterns. Recent deep learning approachesleverage neural network architectures to learn distinctive representations anddemonstrate strong outcomes, particularly on complex and large-scale pointclouds. Provided these significant advances, this survey provides acomprehensive and up-to-date review of deep learning-based point clouddenoising methods up to August 2025. We organize the literature from twoperspectives: (1) supervision level (supervised vs. unsupervised), and (2)modeling perspective, proposing a functional taxonomy that unifies diverseapproaches by their denoising principles. We further analyze architecturaltrends both structurally and chronologically, establish a unified benchmarkwith consistent training settings, and evaluate methods in terms of denoisingquality, surface fidelity, point distribution, and computational efficiency.Finally, we discuss open challenges and outline directions for future researchin this rapidly evolving field.</description>
      <author>example@mail.com (Jinxi Wang, Ben Fei, Dasith de Silva Edirimuni, Zheng Liu, Ying He, Xuequan Lu)</author>
      <guid isPermaLink="false">2508.17011v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Towards Training-Free Underwater 3D Object Detection from Sonar Point Clouds: A Comparison of Traditional and Deep Learning Approaches</title>
      <link>http://arxiv.org/abs/2508.18293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures, submitted to IEEE Journal of Oceanic Engineering  (IEEE-JOE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种无需训练数据的水下3D物体检测方法，通过结合基于物理的声纳模拟和模板匹配技术，解决了水下环境训练数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;水下3D物体检测是计算机视觉领域最具挑战性的前沿之一，传统方法在水下声学环境和训练数据稀缺方面存在困难。虽然深度学习在陆地3D检测上取得了革命性进展，但水下应用面临获取足够标注声纳数据成本高昂且复杂的瓶颈。&lt;h4&gt;目的&lt;/h4&gt;解决能否在没有真实世界训练数据的情况下实现可靠的水下3D物体检测这一基本问题。&lt;h4&gt;方法&lt;/h4&gt;开发并比较了两种多波束测深仪点云中人工结构无训练检测的范式：1) 基于物理的声纳模拟管道，为最先进的神经网络生成合成训练数据；2) 基于模型的模板匹配系统，利用目标对象的几何先验信息。&lt;h4&gt;主要发现&lt;/h4&gt;在波罗的海真实测深调查中，合成数据训练的神经网络在模拟场景上达到98%的平均精度均值(mAP)，但在真实声纳数据上下降到40% mAP（由于域转移）；相比之下，模板匹配方法在不需要任何训练的情况下在真实数据上保持83% mAP，对声学噪声和环境变化表现出显著的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;研究结果挑战了水下领域对数据饥渴型深度学习的传统观念，建立了首个大规模的无训练水下3D检测基准，为在数据稀缺环境中进行自主水下航行器导航、海洋考古和海上基础设施监测开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;水下3D物体检测仍然是计算机视觉中最具挑战性的前沿领域之一，传统方法难以应对严酷的声学环境和训练数据的稀缺。虽然深度学习已经革新了陆地3D检测，但其在水下的应用面临一个关键瓶颈：获取足够的标注声纳数据成本高昂且后勤复杂，通常需要专门的船只、专业的测量员和有利的天气条件。这项工作解决了一个基本问题：我们能否在没有真实世界训练数据的情况下实现可靠的水下3D物体检测？我们通过开发和比较两种用于多波束测深仪点云中人工结构无训练检测的范式来应对这一挑战。我们的双重方法结合了基于物理的声纳模拟管道，为最先进的神经网络生成合成训练数据，以及一个稳健的基于模型的模板匹配系统，该系统利用目标对象的几何先验。在波罗的海真实测深调查中的评估揭示了令人惊讶的见解：虽然合成数据训练的神经网络在模拟场景上实现了98%的平均精度均值(mAP)，但由于域转移，它们在真实声纳数据上下降到40% mAP。相反，我们的模板匹配方法在不需要任何训练的情况下在真实数据上保持了83% mAP，表现出对声学噪声和环境变化的显著鲁棒性。我们的研究结果挑战了关于水下领域数据饥渴型深度学习的传统观念，并建立了首个无训练水下3D检测的大规模基准。这项工作为在传统机器学习方法失效的数据稀缺环境中的自主水下航行器导航、海洋考古和海上基础设施监测开辟了新的可能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决水下环境中缺乏训练数据时的3D物体检测问题。这个问题在现实中非常重要，因为水下环境获取标注数据极其困难和昂贵，需要专业船只、专家调查员和 favorable 的天气条件。传统深度学习方法依赖大量标注数据，但在水下环境中获取这些数据成本高昂且复杂。可靠的水下3D物体检测对自主水下航行器导航、海洋考古和海上基础设施监测等领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用双轨策略设计方法：一方面借鉴现有的SASA神经网络架构，通过物理声纳模拟生成合成训练数据；另一方面借鉴传统模型拟合方法，如RANSAC和ICP算法，创建3D物体模型库并转换为声纳点云模板。作者借鉴了现有工作，如使用RANSAC进行海底平面去除预处理，使用ICP算法进行点云配准，使用SASA神经网络处理点云数据，以及使用Perlin噪声生成逼真的海底环境。这两种方法互补，分别从数据驱动和几何先验角度解决训练数据稀缺的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在缺乏真实训练数据的情况下，通过两种互补的方法实现水下3D物体检测：1) 基于物理的声纳模拟生成合成数据训练神经网络；2) 基于几何先验的模板匹配系统直接应用物体模型。整体实现流程：深度学习方法包括创建合成声纳数据集（生成地形、放置物体、模拟扫描）、训练SASA神经网络（使用合成数据并应用数据增强）、在真实数据上测试评估；模型匹配方法包括准备模板库（创建3D模型并转换为点云模板）、预处理真实数据（RANSAC去除海底、滑动窗口分割）、检测配准（ICP算法对齐模板与分割区域）和后处理（去除重复检测）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次对基于模型和深度学习的水下3D物体检测方法进行大规模直接比较，且不使用真实训练数据；2) 提出并验证训练-free水下3D物体检测的可行性；3) 开发程序化多波束声纳模拟框架生成逼真合成数据；4) 在高噪声声纳数据中直接应用ICP算法进行物体检测。相比之前工作，不同之处在于：专注于缺乏训练数据的场景；系统比较两种方法范式；针对复杂人工结构而非简单形状；使用更大规模数据集和更全面评估指标。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过比较深度学习和传统模型匹配方法，首次证明了在缺乏真实训练数据的情况下，基于几何先验的模板匹配方法能实现鲁棒的水下3D物体检测，为数据稀缺的水下环境提供了可行解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater 3D object detection remains one of the most challenging frontiersin computer vision, where traditional approaches struggle with the harshacoustic environment and scarcity of training data. While deep learning hasrevolutionized terrestrial 3D detection, its application underwater faces acritical bottleneck: obtaining sufficient annotated sonar data is prohibitivelyexpensive and logistically complex, often requiring specialized vessels, expertsurveyors, and favorable weather conditions. This work addresses a fundamentalquestion: Can we achieve reliable underwater 3D object detection withoutreal-world training data? We tackle this challenge by developing and comparingtwo paradigms for training-free detection of artificial structures in multibeamecho-sounder point clouds. Our dual approach combines a physics-based sonarsimulation pipeline that generates synthetic training data for state-of-the-artneural networks, with a robust model-based template matching system thatleverages geometric priors of target objects. Evaluation on real bathymetrysurveys from the Baltic Sea reveals surprising insights: while neural networkstrained on synthetic data achieve 98% mean Average Precision (mAP) on simulatedscenes, they drop to 40% mAP on real sonar data due to domain shift.Conversely, our template matching approach maintains 83% mAP on real datawithout requiring any training, demonstrating remarkable robustness to acousticnoise and environmental variations. Our findings challenge conventional wisdomabout data-hungry deep learning in underwater domains and establish the firstlarge-scale benchmark for training-free underwater 3D detection. This workopens new possibilities for autonomous underwater vehicle navigation, marinearchaeology, and offshore infrastructure monitoring in data-scarce environmentswhere traditional machine learning approaches fail.</description>
      <author>example@mail.com (M. Salman Shaukat, Yannik Käckenmeister, Sebastian Bader, Thomas Kirste)</author>
      <guid isPermaLink="false">2508.18293v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Autoregressive Universal Video Segmentation Model</title>
      <link>http://arxiv.org/abs/2508.19242v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了AUSM（自回归通用分割模型），一种统一的视频分割架构，能够同时处理提示性和非提示性视频分割任务，基于状态空间模型构建，可高效处理任意长度的视频流。&lt;h4&gt;背景&lt;/h4&gt;近期的视频基础模型如SAM2在提示视频分割方面表现出色，但现实场景中常需要非提示性分割（无需外部提示检测和跟踪所有对象），而当前解决方案分散在特定任务模型和流程中。&lt;h4&gt;目的&lt;/h4&gt;将流式视频分割重新构建为序列掩码预测（类似于语言建模），并开发一种统一架构同时处理提示性和非提示性视频分割任务。&lt;h4&gt;方法&lt;/h4&gt;提出AUSM模型，基于最新状态空间模型构建，保持固定大小的空间状态，可扩展到任意长度的视频流。所有组件设计为跨帧并行训练，显著提高训练速度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试（DAVIS17、YouTube-VOS、MOSE、YouTube-VIS和OVIS）上，AUSM优于先前的通用流式视频分割方法，并在16帧序列上的训练速度提高了最多2.5倍。&lt;h4&gt;结论&lt;/h4&gt;AUSM是一种统一的视频分割模型，能够有效处理提示性和非提示性分割任务，具有高效训练和优越性能的特点，为视频分割领域提供了新的通用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近期的视频基础模型如SAM2在提示视频分割方面表现出色，将掩码视为通用原语。然而，许多现实场景需要非提示性分割，旨在无需外部提示的情况下检测和跟踪视频中的所有对象，导致当今的解决方案分散在特定任务模型和流程中。我们将流式视频分割重新构建为序列掩码预测，类似于语言建模，并引入了自回归通用分割模型（AUSM），这是一种统一提示性和非提示性视频分割的单一架构。基于最新的状态空间模型构建，AUSM保持固定大小的空间状态，并可扩展到任意长度的视频流。此外，AUSM的所有组件都设计为跨帧并行训练，比迭代训练提供显著的加速。在标准基准测试（DAVIS17、YouTube-VOS 2018 &amp; 2019、MOSE、YouTube-VIS 2019 &amp; 2021和OVIS）上，AUSM优于先前的通用流式视频分割方法，并在16帧序列上的训练速度提高了最多2.5倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent video foundation models such as SAM2 excel at prompted videosegmentation by treating masks as a general-purpose primitive. However, manyreal-world settings require unprompted segmentation that aims to detect andtrack all objects in a video without external cues, leaving today's landscapefragmented across task-specific models and pipelines. We recast streaming videosegmentation as sequential mask prediction, analogous to language modeling, andintroduce the Autoregressive Universal Segmentation Model (AUSM), a singlearchitecture that unifies both prompted and unprompted video segmentation.Built on recent state-space models, AUSM maintains a fixed-size spatial stateand scales to video streams of arbitrary length. Furthermore, all components ofAUSM are designed for parallel training across frames, yielding substantialspeedups over iterative training. On standard benchmarks (DAVIS17, YouTube-VOS2018 &amp; 2019, MOSE, YouTube-VIS 2019 &amp; 2021, and OVIS) AUSM outperforms prioruniversal streaming video segmentation methods and achieves up to 2.5x fastertraining on 16-frame sequences.</description>
      <author>example@mail.com (Miran Heo, Sukjun Hwang, Min-Hung Chen, Yu-Chiang Frank Wang, Albert Gu, Seon Joo Kim, Ryo Hachiuma)</author>
      <guid isPermaLink="false">2508.19242v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>MACE4IR: A foundation model for molecular infrared spectroscopy</title>
      <link>http://arxiv.org/abs/2508.19118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages and 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MACE4IR，一个基于MACE架构构建的机器学习基础模型，能够准确预测红外光谱同时显著降低计算成本。&lt;h4&gt;背景&lt;/h4&gt;机器学习原子间势(MLIPs)在预测红外光谱方面表现出色，但缺乏能处理多种元素及其组合的通用模型，限制了其广泛应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用、准确且高效的机器学习模型，用于预测各种化学系统的红外光谱。&lt;h4&gt;方法&lt;/h4&gt;构建基于MACE架构的MACE4IR模型，使用QCML数据集上的1000万个几何结构及相应的DFT能量、力和偶极矩进行训练，数据涵盖约80种元素和多样化分子集合。&lt;h4&gt;主要发现&lt;/h4&gt;MACE4IR能准确预测能量、力、偶极矩和红外光谱，与DFT计算相比成本显著降低。&lt;h4&gt;结论&lt;/h4&gt;MACE4IR结合了通用性、准确性和效率，为化学、生物学和材料科学中复杂系统的快速可靠红外光谱预测开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;机器学习的原子间势(MLIPs)在以高保真度预测红外光谱方面显示出巨大潜力。然而，缺乏能够处理各种元素及其组合的通用MLIPs限制了它们更广泛的应用。在这项工作中，我们介绍了MACE4IR，这是一个基于MACE架构构建的机器学习基础模型，并在QCML数据集上的1000万个几何结构以及相应的密度泛函理论(DFT)能量、力和偶极矩上进行了训练。训练数据涵盖了约80种元素和多样化的分子集合，包括有机化合物、无机物质和金属配合物。与DFT相比，MACE4IR能以显著降低的计算成本准确预测能量、力、偶极矩和红外光谱。通过结合通用性、准确性和效率，MACE4IR为化学、生物学和材料科学中复杂系统的快速可靠红外光谱预测开辟了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned interatomic potentials (MLIPs) have shown significant promisein predicting infrared spectra with high fidelity. However, the absence ofgeneral-purpose MLIPs capable of handling a wide range of elements and theircombinations has limited their broader applicability. In this work, weintroduce MACE4IR, a machine learning foundation model built on the MACEarchitecture and trained on 10 million geometries and correspondingdensity-functional theory (DFT) energies, forces and dipole moments from theQCML dataset. The training data encompasses approximately 80 elements and adiverse set of molecules, including organic compounds, inorganic species, andmetal complexes. MACE4IR accurately predicts energies, forces, dipole moments,and infrared spectra at significantly reduced computational cost compared toDFT. By combining generality, accuracy, and efficiency, MACE4IR opens the doorto rapid and reliable infrared spectra prediction for complex systems acrosschemistry, biology, and materials science.</description>
      <author>example@mail.com (Nitik Bhatia, Ondrej Krejci, Silvana Botti, Patrick Rinke, Miguel A. L. Marques)</author>
      <guid isPermaLink="false">2508.19118v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Random forest-based out-of-distribution detection for robust lung cancer segmentation</title>
      <link>http://arxiv.org/abs/2508.19112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出RF-Deep方法，结合Transformer分割模型和随机森林分类器，用于检测分布外数据并提高癌症分割的可靠性&lt;h4&gt;背景&lt;/h4&gt;基于Transformer的模型在分布内数据上能产生可靠的分割结果，但在分布外数据上性能下降。准确检测和分割CT扫描中的癌变区域对自动化治疗计划和癌症治疗反应评估至关重要&lt;h4&gt;目的&lt;/h4&gt;解决基于Transformer的分割模型在处理分布外数据时性能下降的问题，提高癌症分割在分布内和分布外场景中的可靠性&lt;h4&gt;方法&lt;/h4&gt;提出RF-Deep方法，使用随机森林分类器检测分布外扫描，利用预训练Transformer编码器的深度特征；分割模型包含Swin Transformer编码器（使用掩码图像模型在10,432个未标记3D CT扫描上预训练）和卷积解码器（在317个3D扫描上训练以分割肺癌）；在603个3D CT公共数据集上进行独立测试&lt;h4&gt;主要发现&lt;/h4&gt;RF-Deep在肺栓塞、COVID-19和腹部CT上检测分布外案例的FPR95分别为18.26%、27.66%和小于0.1%，一致性地优于既有的分布外检测方法&lt;h4&gt;结论&lt;/h4&gt;RF-Deep分类器提供了一种简单有效的方法，可以增强在分布内和分布外场景中癌症分割的可靠性&lt;h4&gt;翻译&lt;/h4&gt;准确的检测和分割计算机断层扫描(CT)中的癌变病变对于自动化治疗计划和癌症治疗反应评估至关重要。基于Transformer的模型通过自监督预训练可以从分布内数据产生可靠的分割结果，但在应用于分布外数据集时性能下降。我们通过RF-Deep解决了这一挑战，RF-Deep是一个随机森林分类器，它利用分割模型预训练的Transformer编码器的深度特征来检测分布外扫描，并增强分割可靠性。分割模型包含一个Swin Transformer编码器，在10,432个覆盖癌症和非癌症状况的未标记3D CT扫描上使用掩码图像模型进行预训练，以及一个卷积解码器，在317个3D扫描上训练以分割肺癌。在603个3D CT公共数据集上进行了独立测试，包括一个分布内数据集和四个分布外数据集，包含肺栓塞(PE)和COVID-19的胸部CT，以及肾脏癌症和健康志愿者的腹部CT。RF-Deep在PE、COVID-19和腹部CT上检测分布外案例的FPR95分别为18.26%、27.66%和小于0.1%，一致性地优于既有的分布外方法。RF-Deep分类器提供了一种简单有效的方法，以增强分布内和分布外场景中癌症分割的可靠性&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何准确检测'分布外'(OOD)数据以提高肺癌分割模型在临床应用中的鲁棒性问题。这个问题很重要，因为在实际临床环境中，模型可能遇到与训练数据不同的情况（如从胸部CT扩展到筛查中的良性结节、肺栓塞或腹部CT），而传统评估指标不足以衡量模型对这类数据的鲁棒性，现有OOD检测方法也存在局限性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有OOD检测方法的局限性，然后提出关键洞察：医学图像包含不同解剖范围，应专注于肿瘤区域而非全局图像进行OOD检测。他们借鉴了Swin Transformer架构、SimMIM自监督预训练和随机森林分类器等现有工作，创新性地结合深度特征与随机森林，创建RF-Deep方法，并应用'异常暴露'范式使用小规模ID和OOD示例训练分类器。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练Transformer编码器提取的深度特征，结合随机森林分类器检测OOD扫描，专注于肿瘤区域而非整个图像。实现流程分为四步：1)微调肺癌分割模型；2)冻结编码器并从肿瘤区域提取多尺度特征；3)用ID和OOD特征训练随机森林分类器；4)对新扫描提取特征并用训练好的检测器分类为ID或OOD。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：轻量级随机森林分类器、专注于肿瘤区域而非全局图像、利用多尺度Transformer特征、应用异常暴露范式减少数据需求。相比之前工作，RF-Deep能处理模型自信错误分割的情况，比传统方法有更低错误率；比辅助模型方法更高效、更易解释；比放射组学方法能更好地推广到肿瘤检测；结合了深度学习和传统机器学习的优势。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于随机森林和深度特征的轻量级OOD检测方法，有效提高了肺癌分割模型在面对不同解剖部位和疾病类型时的鲁棒性和可靠性，为临床应用提供了更安全可靠的自动化分割工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate detection and segmentation of cancerous lesions from computedtomography (CT) scans is essential for automated treatment planning and cancertreatment response assessment. Transformer-based models with self-supervisedpretraining can produce reliably accurate segmentation from in-distribution(ID) data but degrade when applied to out-of-distribution (OOD) datasets. Weaddress this challenge with RF-Deep, a random forest classifier that utilizesdeep features from a pretrained transformer encoder of the segmentation modelto detect OOD scans and enhance segmentation reliability. The segmentationmodel comprises a Swin Transformer encoder, pretrained with masked imagemodeling (SimMIM) on 10,432 unlabeled 3D CT scans covering cancerous andnon-cancerous conditions, with a convolution decoder, trained to segment lungcancers in 317 3D scans. Independent testing was performed on 603 3D CT publicdatasets that included one ID dataset and four OOD datasets comprising chestCTs with pulmonary embolism (PE) and COVID-19, and abdominal CTs with kidneycancers and healthy volunteers. RF-Deep detected OOD cases with a FPR95 of18.26%, 27.66%, and less than 0.1% on PE, COVID-19, and abdominal CTs,consistently outperforming established OOD approaches. The RF-Deep classifierprovides a simple and effective approach to enhance reliability of cancersegmentation in ID and OOD scenarios.</description>
      <author>example@mail.com (Aneesh Rangnekar, Harini Veeraraghavan)</author>
      <guid isPermaLink="false">2508.19112v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>GReAT: leveraging geometric artery data to improve wall shear stress assessment</title>
      <link>http://arxiv.org/abs/2508.19030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  (MICCAI 2025) Workshop on Shape in Medical Imaging (ShapeMI)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了如何利用大型血管几何数据集通过自监督学习改善小规模临床试验中的壁面剪切应力评估&lt;h4&gt;背景&lt;/h4&gt;利用大数据进行患者护理在心血管健康等领域前景广阔，可通过机器学习从医学图像中评估血流动力学生物标志物，但收集足够大的训练数据集极其困难&lt;h4&gt;目的&lt;/h4&gt;研究大型几何模型数据集(8449个形状)能否改善小规模临床试验(49名患者)中冠状动脉模型的壁面剪切应力评估&lt;h4&gt;方法&lt;/h4&gt;通过计算热核签名(一种能捕捉形状本质的量)为3D血管创建自监督目标，利用从大型数据集学习的几何表示提高冠状动脉分割&lt;h4&gt;主要发现&lt;/h4&gt;从大型血管几何模型数据集学习的表示可改善壁面剪切应力评估，即使在有限数据上训练也能提高冠状动脉分割为低、中、高壁面剪切应力区域的准确性&lt;h4&gt;结论&lt;/h4&gt;大型血管几何模型数据集可通过自监督学习方法增强小规模临床试验中的壁面剪切应力评估&lt;h4&gt;翻译&lt;/h4&gt;利用大数据进行患者护理在心血管健康等许多医学领域前景广阔。例如，可以通过机器学习算法从患者特定的医学图像中评估壁面剪切应力等血流动力学生物标志物，从而避免了耗时的计算流体动力学模拟。然而，收集足够大的数据集来有效训练此类模型极其困难。我们可以通过在大量几何动脉模型数据集上进行自监督预训练和基础模型来解决数据稀缺问题。在冠状动脉背景下，利用学习到的表示来改进血流动力学生物标志物评估尚未得到充分研究。在本工作中，我们通过研究大型数据集(8449个形状)是否可以改善从小规模临床试验(49名患者)中获得的冠状动脉模型中的壁面剪切应力评估来填补这一空白。我们通过计算热核签名(一种通过拉普拉斯特征向量获得的量，能够捕捉形状的本质)为3D血管创建自监督目标。我们展示了如何从这个数据集中学习的几何表示可以提高冠状动脉的分割，将其分为低、中、高(时间平均)壁面剪切应力区域，即使是在有限数据上训练的。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大规模几何动脉数据改善血管壁剪切应力评估的问题。这个问题很重要因为心血管疾病是全球主要致死原因，而血管壁剪切应力作为动脉粥样硬化的生物标志物可以帮助预测心肌梗死等严重后果。目前计算流体动力学方法耗时，而神经网络替代方法又受限于数据量不足，医疗机构难以获得足够大的数据集来训练有效模型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：CFD计算耗时，而神经网络需要大量特定数据才能有效训练。他们借鉴了自监督学习和基础模型在计算机视觉领域的成功经验，选择使用热核签名作为自监督训练目标，因为它能捕捉形状的本质特征。他们设计了基于交叉注意力变换器的架构（LaB-VaTr），并提出了几何表示适应变换器（GReAT）框架，将预训练的几何表示与特定任务模型结合。该方法还参考了图表示学习中的自监督回归目标以及3D点云的自学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过在大规模几何动脉数据上进行自监督预训练学习通用表示，然后在小规模临床数据上微调，改善血管壁剪切应力评估。整体流程包括：1) 准备数据（8449个3D血管模型和49个临床冠状动脉模型）；2) 对每个血管模型计算热核签名作为自监督目标；3) 使用LaB-VaTr模型在大型数据集上进行自监督预训练；4) 创建GReAT框架将预训练几何表示与特定任务模型结合；5) 在临床数据上进行微调，分类低中高WSS区域；6) 通过八折交叉验证评估性能并进行消融研究。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出几何表示适应变换器（GReAT）框架，将自监督预训练几何表示与特定任务模型结合；2) 使用热核签名作为自监督训练目标，有效捕捉形状多尺度特征；3) 创建并标注了大规模血管几何数据集（MedShapeNet-Blood-Vessel）。相比之前工作，本文首次将自监督预训练应用于心血管几何数据以改善WSS评估；采用简单回归目标而非复杂对比学习；不仅展示预训练好处，还通过消融研究验证各组件贡献，并探索了预训练在不同条件下的局限性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出的GReAT框架通过在大规模血管几何数据上进行自监督预训练学习通用表示，然后在小规模临床数据上微调，显著改善了冠状动脉壁剪切应力的评估准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging big data for patient care is promising in many medical fields suchas cardiovascular health. For example, hemodynamic biomarkers like wall shearstress could be assessed from patient-specific medical images via machinelearning algorithms, bypassing the need for time-intensive computational fluidsimulation. However, it is extremely challenging to amass large-enough datasetsto effectively train such models. We could address this data scarcity by meansof self-supervised pre-training and foundations models given large datasets ofgeometric artery models. In the context of coronary arteries, leveraginglearned representations to improve hemodynamic biomarker assessment has not yetbeen well studied. In this work, we address this gap by investigating whether alarge dataset (8449 shapes) consisting of geometric models of 3D blood vesselscan benefit wall shear stress assessment in coronary artery models from asmall-scale clinical trial (49 patients). We create a self-supervised targetfor the 3D blood vessels by computing the heat kernel signature, a quantityobtained via Laplacian eigenvectors, which captures the very essence of theshapes. We show how geometric representations learned from this datasets canboost segmentation of coronary arteries into regions of low, mid and high(time-averaged) wall shear stress even when trained on limited data.</description>
      <author>example@mail.com (Julian Suk, Jolanda J. Wentzel, Patryk Rygiel, Joost Daemen, Daniel Rueckert, Jelmer M. Wolterink)</author>
      <guid isPermaLink="false">2508.19030v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Deep Pre-trained Time Series Features for Tree Species Classification in the Dutch Forest Inventory</title>
      <link>http://arxiv.org/abs/2508.18829v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用预训练遥感基础模型的深度特征来提高国家森林普查(NFI)树种分类准确性的方法，结果表明这种方法显著优于传统方法。&lt;h4&gt;背景&lt;/h4&gt;国家森林普查是获取森林信息和树种分布数据的主要来源，但维护这些普查需要大量实地工作。遥感技术与机器学习相结合提供了更频繁、更大规模更新NFI的机会。&lt;h4&gt;目的&lt;/h4&gt;系统研究深度特征如何在标注数据有限的情况下提高荷兰的树种分类准确性，并探索预训练遥感基础模型在NFI分类中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;从Sentinel-1、Sentinel-2和ERA5卫星数据以及SRTM数据中提取时间序列数据，使用Google Earth Engine进行处理，并微调公开可用的遥感时间序列基础模型进行树种分类。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的遥感时间序列基础模型在荷兰NFI分类中显著优于当前最先进的方法，在所有数据集上提高了高达10%的准确率，表明传统手工定义的谐波特征对于此任务过于简单。&lt;h4&gt;结论&lt;/h4&gt;深度AI特征在NFI分类等数据有限应用中具有巨大潜力。通过利用开放的卫星数据和预训练模型，这种方法显著提高了分类准确性，可以有效补充现有的森林普查流程。&lt;h4&gt;翻译&lt;/h4&gt;国家森林普查(NFI)是森林信息的主要来源，提供关键的树种分布数据。然而，维护这些普查需要大量实地工作。遥感方法，特别是与机器学习相结合，提供了更频繁和更大规模更新NFI的机会。虽然卫星图像时间序列已证明通过季节性冠层反射率模式区分树种有效，但当前方法主要依赖随机森林分类器和手工设计的特征及物候指标。使用可用预训练遥感基础模型的深度特征提供了补充策略。这些预训练模型利用未标注的全球数据，可用于通用应用，然后可以用较小的标注数据集针对特定分类任务进行高效微调。本研究系统研究了深度特征如何在标注数据有限的情况下提高荷兰的树种分类准确性。数据方面，我们使用Google Earth Engine从Sentinel-1、Sentinel-2和ERA5卫星数据以及SRTM数据中提取了时间序列数据。我们的结果表明，微调公开可用的遥感时间序列基础模型在荷兰NFI分类中显著优于当前最先进的方法，在所有数据集上提高了高达10%的准确率。这表明经典手工定义的谐波特征对此任务过于简单，突显了使用深度AI特征在NFI分类等数据有限应用中的潜力。通过利用开放的卫星数据和预训练模型，该方法显著提高了分类准确性，可以有效补充现有的森林普查流程。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; National Forest Inventory (NFI)s serve as the primary source of forestinformation, providing crucial tree species distribution data. However,maintaining these inventories requires labor-intensive on-site campaigns.Remote sensing approaches, particularly when combined with machine learning,offer opportunities to update NFIs more frequently and at larger scales. Whilethe use of Satellite Image Time Series has proven effective for distinguishingtree species through seasonal canopy reflectance patterns, current approachesrely primarily on Random Forest classifiers with hand-designed features andphenology-based metrics. Using deep features from an available pre-trainedremote sensing foundation models offers a complementary strategy. Thesepre-trained models leverage unannotated global data and are meant to used forgeneral-purpose applications and can then be efficiently fine-tuned withsmaller labeled datasets for specific classification tasks. This worksystematically investigates how deep features improve tree speciesclassification accuracy in the Netherlands with few annotated data. Data-wise,we extracted time-series data from Sentinel-1, Sentinel-2 and ERA5 satellitesdata and SRTM data using Google Earth Engine. Our results demonstrate thatfine-tuning a publicly available remote sensing time series foundation modeloutperforms the current state-of-the-art in NFI classification in theNetherlands by a large margin of up to 10% across all datasets. Thisdemonstrates that classic hand-defined harmonic features are too simple forthis task and highlights the potential of using deep AI features fordata-limited application like NFI classification. By leveraging openlyavailable satellite data and pre-trained models, this approach significantlyimproves classification accuracy compared to traditional methods and caneffectively complement existing forest inventory processes.</description>
      <author>example@mail.com (Takayuki Ishikawa, Carmelo Bonannella, Bas J. W. Lerink, Marc Rußwurm)</author>
      <guid isPermaLink="false">2508.18829v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>EMind: A Foundation Model for Multi-task Electromagnetic Signals Understanding</title>
      <link>http://arxiv.org/abs/2508.18785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了EMind，一个电磁信号基础模型，旨在解决电磁信号处理中的挑战，实现跨任务的泛化和高效学习。&lt;h4&gt;背景&lt;/h4&gt;电磁信号的深度理解对动态频谱管理、智能交通、自动驾驶和无人车感知等领域至关重要。电磁信号与文本和图像差异大，表现出高度异质性、强背景噪声和复杂联合时频结构，现有通用模型无法直接使用。此外，电磁通信和感知任务多样，当前方法缺乏跨任务泛化和迁移效率，且缺乏大规模高质量数据集，阻碍了真正的多任务学习框架创建。&lt;h4&gt;目的&lt;/h4&gt;为了克服电磁信号处理中的挑战，作者引入了EMind，一个连接大规模预训练和电磁信号独特特性的基础模型。&lt;h4&gt;方法&lt;/h4&gt;作者构建了首个统一且最大标准化的电磁信号数据集，涵盖多种信号类型和任务。通过利用电磁信号的物理特性，设计了长度自适应的多信号打包方法和硬件感知训练策略，能够高效利用和表示来自异构多源信号的学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，EMind在许多下游任务上实现了强大的性能和广泛的泛化，从特定任务模型转向电磁智能的统一框架。&lt;h4&gt;结论&lt;/h4&gt;EMind代表了电磁信号处理领域的重要进展，通过基础模型方法解决了电磁信号异质性、噪声复杂性和跨任务泛化的挑战，为电磁智能提供了统一框架。&lt;h4&gt;翻译&lt;/h4&gt;电磁信号的深入理解对于动态频谱管理、智能交通、自动驾驶和无人车感知至关重要。该领域面临挑战，因为电磁信号与文本和图像差异很大，表现出高度异质性、强背景噪声和复杂的联合时频结构，这阻碍了现有通用模型的直接使用。电磁通信和感知任务多样，当前方法缺乏跨任务泛化和迁移效率，且缺乏大规模高质量数据集，阻碍了真正的多任务学习框架的创建。为了克服这些问题，我们引入了EMind，一个电磁信号基础模型，连接大规模预训练和这种模态的独特特性。我们构建了第一个统一且最大标准化的电磁信号数据集，涵盖多种信号类型和任务。通过利用电磁信号的物理特性，我们设计了一种长度自适应的多信号打包方法和硬件感知训练策略，能够高效利用和表示来自异构多源信号的学习。实验表明，EMind在许多下游任务上实现了强大的性能和广泛的泛化，从特定任务模型 decisively 转向电磁智能的统一框架。代码可在以下网址获取：https://github.com/GabrielleTse/EMind。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep understanding of electromagnetic signals is fundamental to dynamicspectrum management, intelligent transportation, autonomous driving andunmanned vehicle perception. The field faces challenges because electromagneticsignals differ greatly from text and images, showing high heterogeneity, strongbackground noise and complex joint time frequency structure, which preventsexisting general models from direct use. Electromagnetic communication andsensing tasks are diverse, current methods lack cross task generalization andtransfer efficiency, and the scarcity of large high quality datasets blocks thecreation of a truly general multitask learning framework. To overcome theseissue, we introduce EMind, an electromagnetic signals foundation model thatbridges large scale pretraining and the unique nature of this modality. Webuild the first unified and largest standardized electromagnetic signal datasetcovering multiple signal types and tasks. By exploiting the physical propertiesof electromagnetic signals, we devise a length adaptive multi-signal packingmethod and a hardware-aware training strategy that enable efficient use andrepresentation learning from heterogeneous multi-source signals. Experimentsshow that EMind achieves strong performance and broad generalization acrossmany downstream tasks, moving decisively from task specific models to a unifiedframework for electromagnetic intelligence. The code is available at:https://github.com/GabrielleTse/EMind.</description>
      <author>example@mail.com (Luqing Luo, Wenjin Gui, Yunfei Liu, Ziyue Zhang, Yunxi Zhang, Fengxiang Wang, Zonghao Guo, Zizhi Ma, Xinzhu Liu, Hanxiang He, Jinhai Li, Xin Qiu, Wupeng Xie, Yangang Sun)</author>
      <guid isPermaLink="false">2508.18785v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>FFT-MoE: Efficient Federated Fine-Tuning for Foundation Models via Large-scale Sparse MoE under Heterogeneous Edge</title>
      <link>http://arxiv.org/abs/2508.18663v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FFT MoE是一种新的联邦微调框架，用稀疏的专家混合适配器替代LoRA，解决了异构联邦学习环境中的结构兼容性和数据分布适应性问题。&lt;h4&gt;背景&lt;/h4&gt;随着大型模型推动通用人工智能发展，在隐私和资源限制下对模型进行微调变得尤为重要，特别是当高质量训练数据分布在分布式边缘设备上时。&lt;h4&gt;目的&lt;/h4&gt;解决联邦微调中LoRA方法在异构联邦学习环境中的两个主要限制：跨客户端的结构兼容性问题和对非独立同分布数据分布的有限适应性。&lt;h4&gt;方法&lt;/h4&gt;提出FFT MoE框架，用稀疏的专家混合(MoE)适配器替代LoRA。每个客户端训练轻量级门控网络选择性激活个性化专家子集，同时保持聚合兼容性。引入异构感知辅助损失动态正则化路由分布，确保专家多样性和平衡利用。&lt;h4&gt;主要发现&lt;/h4&gt;在独立同分布和非独立同分布条件下的实验表明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线。&lt;h4&gt;结论&lt;/h4&gt;FFT MoE框架有效解决了LoRA在异构联邦学习环境中的局限性，提供了更好的性能和适应性。&lt;h4&gt;翻译&lt;/h4&gt;随着大型模型推动通用人工智能(AGI)的进步，在隐私和资源约束下对其进行微调变得越来越关键，特别是当高质量训练数据分布在分布式边缘设备上时。联邦学习(FL)通过联邦微调(FFT)提供了有吸引力的解决方案，它能够在不共享原始数据的情况下实现协作模型适应。最近的方法采用参数高效微调(PEFT)技术，如低秩适应(LoRA)，以减少计算开销。然而，基于LoRA的FFT在异构FL环境中面临两个主要限制：具有不同LoRA配置的客户端之间的结构兼容性问题以及对非独立同分布数据分布的有限适应性，这阻碍了收敛和泛化。为解决这些挑战，我们提出了FFT MoE，一种新的FFT框架，它用稀疏的专家混合(MoE)适配器替代LoRA。每个客户端训练一个轻量级门控网络来选择性激活个性化的专家子集，实现对本地资源预算的细粒度适应，同时保持聚合兼容性。为了进一步对抗由设备和数据异构性导致的专家负载不平衡，我们引入了一个异构感知的辅助损失，该损失动态正则化路由分布，确保专家多样性和平衡利用。跨越独立同分布和非独立同分布条件的广泛实验表明，FFT MoE在泛化性能和训练效率方面始终优于最先进的FFT基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As FMs drive progress toward Artificial General Intelligence (AGI),fine-tuning them under privacy and resource constraints has become increasinglycritical particularly when highquality training data resides on distributededge devices. Federated Learning (FL) offers a compelling solution throughFederated Fine-Tuning (FFT), which enables collaborative model adaptationwithout sharing raw data. Recent approaches incorporate Parameter-EfficientFine-Tuning (PEFT) techniques such as Low Rank Adaptation (LoRA) to reducecomputational overhead. However, LoRA-based FFT faces two major limitations inheterogeneous FL environments: structural incompatibility across clients withvarying LoRA configurations and limited adaptability to non-IID datadistributions, which hinders convergence and generalization. To address thesechallenges, we propose FFT MoE, a novel FFT framework that replaces LoRA withsparse Mixture of Experts (MoE) adapters. Each client trains a lightweightgating network to selectively activate a personalized subset of experts,enabling fine-grained adaptation to local resource budgets while preservingaggregation compatibility. To further combat the expert load imbalance causedby device and data heterogeneity, we introduce a heterogeneity-aware auxiliaryloss that dynamically regularizes the routing distribution to ensure expertdiversity and balanced utilization. Extensive experiments spanning both IID andnon-IID conditions demonstrate that FFT MoE consistently outperforms state ofthe art FFT baselines in generalization performance and training efficiency.</description>
      <author>example@mail.com (Gang Hu, Yinglei Teng, Pengfei Wu, Nan Wang)</author>
      <guid isPermaLink="false">2508.18663v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Principled Detection of Hallucinations in Large Language Models via Multiple Testing</title>
      <link>http://arxiv.org/abs/2508.18473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多重检验启发的方法来检测大型语言模型中的幻觉现象。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型虽然能够解决多种任务，但容易产生幻觉，即生成听起来自信但实际上不正确甚至无意义的回答。&lt;h4&gt;目的&lt;/h4&gt;将幻觉检测问题构建为假设检验问题，并探索与机器学习模型分布外检测问题的相似性。&lt;h4&gt;方法&lt;/h4&gt;提出一种受多重检验启发的方法来检测大型语言模型中的幻觉现象。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量实验验证了所提方法相对于最先进方法的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于多重检验的幻觉检测方法有效且具有鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;虽然大型语言模型已成为解决各种任务的基础模型，但它们也容易产生幻觉，即生成听起来自信但实际上不正确甚至无意义的回答。在本研究中，我们将幻觉检测问题构建为假设检验问题，并与机器学习模型中的分布外检测问题进行类比。我们提出了一种受多重检验启发的方法来解决幻觉检测问题，并通过大量实验结果验证了我们的方法相对于最先进方法的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Large Language Models (LLMs) have emerged as powerful foundationalmodels to solve a variety of tasks, they have also been shown to be prone tohallucinations, i.e., generating responses that sound confident but areactually incorrect or even nonsensical. In this work, we formulate the problemof detecting hallucinations as a hypothesis testing problem and draw parallelsto the problem of out-of-distribution detection in machine learning models. Wepropose a multiple-testing-inspired method to solve the hallucination detectionproblem, and provide extensive experimental results to validate the robustnessof our approach against state-of-the-art methods.</description>
      <author>example@mail.com (Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli)</author>
      <guid isPermaLink="false">2508.18473v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Why Relational Graphs Will Save the Next Generation of Vision Foundation Models?</title>
      <link>http://arxiv.org/abs/2508.18421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉基础模型(FMs)在需要显式推理实体、角色和时空关系的任务上存在局限性，作者建议下一代FMs应纳入动态关系图接口，并通过实验证明这种混合模型在语义保真度、鲁棒性、可解释性和计算效率方面有优势。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型已成为计算机视觉的主导架构，它们从大规模、多模态语料库中学习高度可迁移的表示。&lt;h4&gt;目的&lt;/h4&gt;解决现有视觉基础模型在需要显式推理关系的任务上的局限性，提出下一代FMs应纳入显式关系接口的方案。&lt;h4&gt;方法&lt;/h4&gt;通过为FMs增加轻量级、上下文自适应的图推理模块，构建动态关系图（其拓扑和边语义是从输入和任务上下文中推断出来的图），实现多级关系推理。&lt;h4&gt;主要发现&lt;/h4&gt;增强FMs的图推理模块可以改善细粒度语义保真度、分布外鲁棒性、可解释性和计算效率，同时通过在语义节点上进行稀疏推理实现有利的内存和硬件效率。&lt;h4&gt;结论&lt;/h4&gt;下一代视觉基础模型应纳入显式的动态关系图接口，未来研究应优先考虑学习动态图构建、多级关系推理、跨模态融合，以及直接探测结构化视觉任务中关系能力的评估协议。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型已成为计算机视觉的主导架构，它们从大规模、多模态语料库中学习高度可迁移的表示。然而，它们在需要显式推理实体、角色和时空关系的任务上存在持续的限制。这种关系能力对于细粒度人体活动识别、以人为中心的视频理解和多模态医学图像分析至关重要，其中空间、时间和语义依赖对性能起决定性作用。我们主张下一代FMs应纳入显式的关系接口，具体表现为动态关系图（其拓扑和边语义是从输入和任务上下文中推断出来的图）。我们通过人体操作动作识别和脑肿瘤分割领域的跨领域证据来说明这一观点，表明为FMs增加轻量级、上下文自适应的图推理模块相对于仅基于FMs的基线，可以改善细粒度语义保真度、分布外鲁棒性、可解释性和计算效率。重要的是，通过在语义节点上进行稀疏推理，这种混合模型还能实现有利的内存和硬件效率，使它们能够在实际资源约束下部署。最后，我们针对FM图混合提出了有针对性的研究议程，优先考虑学习动态图构建、多级关系推理（例如活动理解中的部分-对象-场景，或医学成像中的区域-器官）、跨模态融合，以及直接探测结构化视觉任务中关系能力的评估协议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models (FMs) have become the predominant architecture incomputer vision, providing highly transferable representations learned fromlarge-scale, multimodal corpora. Nonetheless, they exhibit persistentlimitations on tasks that require explicit reasoning over entities, roles, andspatio-temporal relations. Such relational competence is indispensable forfine-grained human activity recognition, egocentric video understanding, andmultimodal medical image analysis, where spatial, temporal, and semanticdependencies are decisive for performance. We advance the position thatnext-generation FMs should incorporate explicit relational interfaces,instantiated as dynamic relational graphs (graphs whose topology and edgesemantics are inferred from the input and task context). We illustrate thisposition with cross-domain evidence from recent systems in human manipulationaction recognition and brain tumor segmentation, showing that augmenting FMswith lightweight, context-adaptive graph-reasoning modules improvesfine-grained semantic fidelity, out of distribution robustness,interpretability, and computational efficiency relative to FM only baselines.Importantly, by reasoning sparsely over semantic nodes, such hybrids alsoachieve favorable memory and hardware efficiency, enabling deployment underpractical resource constraints. We conclude with a targeted research agenda forFM graph hybrids, prioritizing learned dynamic graph construction, multi-levelrelational reasoning (e.g., part object scene in activity understanding, orregion organ in medical imaging), cross-modal fusion, and evaluation protocolsthat directly probe relational competence in structured vision tasks.</description>
      <author>example@mail.com (Fatemeh Ziaeetabar)</author>
      <guid isPermaLink="false">2508.18421v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Topology Optimization for Self-Supporting Structures in Additive Manufacturing</title>
      <link>http://arxiv.org/abs/2508.19169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于机器学习的自支撑结构拓扑优化框架，专门针对增材制造(AM)应用。该框架使用图神经网络作为有限元网格上的神经场，有效学习和预测连续材料分布。集成的AM滤波器确保可打印性，优化过程在体积和应力约束下最小化结构柔度。使用可微的p范数聚合强制执行von Mises应力约束。该方法的完全可微架构利用自动微分优化整个循环，无需明确推导敏感性。数值实验表明该框架能够在各种载荷和边界条件下生成应力约束的可制造拓扑结构，为增材制造准备的高性能设计提供了实际路径，减少了后处理需求。&lt;h4&gt;背景&lt;/h4&gt;增材制造(AM)技术的发展需要专门的优化方法来生成可打印且高性能的结构。传统的拓扑优化方法通常不考虑增材制造的约束，导致设计难以打印或需要大量后处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对增材制造的机器学习框架，用于自支撑结构的拓扑优化，生成既满足力学性能要求又可直接打印的高性能结构，减少后处理需求。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络(GNN)作为有限元网格上的神经场来学习和预测连续材料分布；集成AM滤波器消除无支撑的悬垂结构确保可打印性；在体积和应力约束下最小化结构柔度；使用可微的p范数聚合强制执行von Mises应力约束；采用完全可微的架构，利用自动微分优化整个循环，避免显式敏感性推导。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够在各种载荷和边界条件下生成满足应力约束的可制造拓扑结构；为增材制造准备的高性能设计提供了实际路径，显著减少了后处理需求。&lt;h4&gt;结论&lt;/h4&gt;该机器学习框架结合了拓扑优化和增材制造考虑，提供了一种高效的方法来设计高性能、可直接制造的结构，解决了传统方法中可制造性和性能之间的权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于机器学习的自支撑结构拓扑优化框架，专门针对增材制造(AM)应用。通过采用作为有限元网格上神经场的图神经网络(GNN)，该框架有效学习和预测连续材料分布。集成的AM滤波器通过消除无支撑的悬垂结构确保可打印性，同时优化过程在体积和应力约束下最小化结构柔度。应力约束使用von Mises应力的可微p范数聚合来强制执行，促进优化设计的机械可靠性。该方法的主要优势在于其完全可微的架构，在整个优化循环中利用自动微分消除了对滤波器和应力约束进行显式敏感性推导的需要。数值实验证明了该框架能够在各种载荷和边界条件下生成应力约束的可制造拓扑结构，为增材制造准备的高性能设计提供了实际路径，减少了后处理要求。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成既适合增材制造（3D打印）又具有良好机械性能的自支撑结构拓扑优化问题。这个问题很重要，因为增材制造虽然能制造复杂几何形状，但悬垂结构需要额外支撑，增加材料成本、生产时间和后处理难度，同时可能影响产品质量。解决此问题可以减少支撑需求，提高制造效率，降低成本，并确保结构在应用中的可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了增材制造的悬垂挑战和传统拓扑优化方法的局限性，然后选择图神经网络作为基础架构，因为它能处理非欧几里得数据（如有限元网格）并捕捉空间依赖关系。作者借鉴了Langelaar的逐层过滤器处理AM约束，使用p范数聚合处理应力约束，并参考了SIMP材料插值方法和神经网络在拓扑优化中的应用。关键创新在于将这些组件整合到一个端到端可微的框架中，利用自动微分避免手动推导复杂敏感性分析。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用图神经网络作为神经场在有限元网格上预测连续材料分布，通过端到端可微框架同时处理制造约束（AM过滤器消除悬垂）和机械性能（应力约束确保可靠性）。流程包括：1)将设计域表示为图结构，节点为有限元单元；2)使用GNN预测伪密度；3)通过可微AM过滤器确保自支撑；4)进行有限元分析计算位移和应力；5)使用复合损失函数（柔顺性、体积约束、应力约束）优化设计；6)通过自动微分和Adam优化器迭代更新网络权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)端到端可微框架，利用自动微分避免手动敏感性分析；2)使用图神经网络作为神经场预测材料分布；3)同时集成AM过滤器和应力约束；4)可微的AM过滤器近似。相比之前工作，不同之处在于：传统方法需手动推导复杂敏感性，而本文通过自动微分简化；与GANs等生成方法相比，本文不需要大量训练数据；与单独处理AM约束或应力约束的方法相比，本文将它们统一在一个可微框架中，提高了效率和灵活性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于图神经网络的端到端可微拓扑优化框架，能够同时考虑增材制造的自支撑约束和机械应力约束，生成既可打印又高性能的结构设计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a machine learning-based framework for topologyoptimization of self-supporting structures, specifically tailored for additivemanufacturing (AM). By employing a graph neural network (GNN) that acts as aneural field over the finite element mesh, the framework effectively learns andpredicts continuous material distributions. An integrated AM filter ensuresprintability by eliminating unsupported overhangs, while the optimizationprocess minimizes structural compliance under volume and stress constraints.The stress constraint is enforced using a differentiable p-norm aggregation ofvon Mises stress, promoting mechanical reliability in the optimized designs. Akey advantage of the approach lies in its fully differentiable architecture,which leverages automatic differentiation throughout the optimizationloop--eliminating the need for explicit sensitivity derivation for both thefilter and the stress constraint. Numerical experiments demonstrate the abilityof the framework to generate stress-constrained manufacturable topologies undervarious loading and boundary conditions, offering a practical pathway towardAM-ready high-performance designs with reduced post-processing requirements.</description>
      <author>example@mail.com (Alireza Tabarraei, Saquib Ahmad Bhuiyan)</author>
      <guid isPermaLink="false">2508.19169v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Triangulation-Based Graph Rewiring for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CIKM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作提出了一种新的图重连接方法TRIGON，通过学习选择相关三角形来构建改进的图结构，从而解决了GNN中的过度压缩和过度平滑问题，并在多种基准测试中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已成为处理图结构数据的主要范式，但其性能受到图拓扑固有问题的限制，主要是过度压缩和过度平滑问题。&lt;h4&gt;目的&lt;/h4&gt;通过改进图拓扑来促进更有效的信息传播，解决GNN中的过度压缩和过度平滑问题。&lt;h4&gt;方法&lt;/h4&gt;引入TRIGON框架，通过从多个图视图中选择相关三角形来构建丰富、非平面的三角剖分，并联合优化三角形选择和下游分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;与现有重连接方法相比，TRIGON产生的重连接图具有显著改进的结构特性，包括减少直径、增加谱间隙和降低有效电阻，并在同质性和异质性基准测试的节点分类任务上表现更优。&lt;h4&gt;结论&lt;/h4&gt;TRIGON框架能够有效改进图神经网络的结构特性，通过联合优化三角形选择和分类性能，实现了更好的节点分类结果。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已成为学习图结构数据的主要范式。然而，它们的性能受到图拓扑固有问题的限制，最明显的是过度压缩和过度平滑。最近在图重连接方面的进展旨在通过修改图拓扑来促进更有效的信息传播，从而缓解这些限制。在这项工作中，我们引入了TRIGON，一个新颖的框架，它通过从多个图视图中选择相关三角形来构建丰富、非平面的三角剖分。通过联合优化三角形选择和下游分类性能，我们的方法产生的重连接图具有显著改进的结构特性，如与现有重连接方法相比减少直径、增加谱间隙和降低有效电阻。实证结果表明，TRIGON在一系列同质性和异质性基准测试的节点分类任务上优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as the leading paradigm forlearning over graph-structured data. However, their performance is limited byissues inherent to graph topology, most notably oversquashing andoversmoothing. Recent advances in graph rewiring aim to mitigate theselimitations by modifying the graph topology to promote more effectiveinformation propagation. In this work, we introduce TRIGON, a novel frameworkthat constructs enriched, non-planar triangulations by learning to selectrelevant triangles from multiple graph views. By jointly optimizing triangleselection and downstream classification performance, our method produces arewired graph with markedly improved structural properties such as reduceddiameter, increased spectral gap, and lower effective resistance compared toexisting rewiring methods. Empirical results demonstrate that TRIGONoutperforms state-of-the-art approaches on node classification tasks across arange of homophilic and heterophilic benchmarks.</description>
      <author>example@mail.com (Hugo Attali, Thomas Papastergiou, Nathalie Pernelle, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2508.19071v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Automated discovery of finite volume schemes using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.19052v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文展示了图神经网络（GNNs）在数值模拟中的新应用，证明GNNs不仅能够近似物理系统解，还能与符号回归结合生成数值格式，并能在训练域外有效泛化。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）已经深刻改变了数值模拟的格局，展示了近似物理系统解的强大能力。然而，它们在训练域外（如更大的或结构不同的图）的泛化能力仍然不确定。&lt;h4&gt;目的&lt;/h4&gt;研究GNNs在超越其传统角色方面的潜力，探索如何利用它们生成数值格式，并结合符号回归，以及在无监督情况下恢复数值格式。&lt;h4&gt;方法&lt;/h4&gt;研究团队使用GNNs在仅包含两个节点的图数据集上进行训练，然后将其外推到分布外、非结构化网格上的热方程。他们还使用了符号回归来发现数值格式的解析公式，并将方法扩展到无监督上下文，仅使用类似于物理信息神经网络（PINNs）的残差损失。最后，他们考虑了更高阶的格式，训练了2跳和2层的GNN。&lt;h4&gt;主要发现&lt;/h4&gt;仅在两个节点图上训练的GNN可以外推到分布外、非结构化网格上的一阶有限体积格式；GNN实现的FV格式误差与其训练损失成比例；使用符号回归，GNN能重新发现标准一阶FV格式的精确解析公式；GNN可以在无监督情况下仅使用PINN风格的残差损失恢复一阶FV格式；2跳和2层的GNN能自主发现二阶校正项和经典二阶中点格式。&lt;h4&gt;结论&lt;/h4&gt;GNNs不仅是强大的近似器，还可以作为开发新数值方法的积极参与者，这代表了科学计算中的一个新范式。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文探讨了图神经网络（GNNs）在数值模拟中的应用。研究团队证明，GNNs不仅能够近似物理系统的解，还能与符号回归结合生成数值格式。他们展示了仅在两个节点图上训练的GNN可以外推到分布外、非结构化网格上的一阶有限体积格式，并且能重新发现该格式的精确解析公式。研究还扩展到无监督上下文，以及更高阶格式的发现。这些发现表明，GNNs在科学计算中不仅是强大的近似工具，还能积极参与新数值方法的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have deeply modified the landscape of numericalsimulations by demonstrating strong capabilities in approximating solutions ofphysical systems. However, their ability to extrapolate beyond their trainingdomain (\textit{e.g.} larger or structurally different graphs) remainsuncertain. In this work, we establish that GNNs can serve purposes beyond theirtraditional role, and be exploited to generate numerical schemes, inconjunction with symbolic regression. First, we show numerically andtheoretically that a GNN trained on a dataset consisting solely of two-nodegraphs can extrapolate a first-order Finite Volume (FV) scheme for the heatequation on out-of-distribution, unstructured meshes. Specifically, if a GNNachieves a loss $\varepsilon$ on such a dataset, it implements the FV schemewith an error of $\mathcal{O}(\varepsilon)$. Using symbolic regression, we showthat the network effectively rediscovers the exact analytical formulation ofthe standard first-order FV scheme. We then extend this approach to anunsupervised context: the GNN recovers the first-order FV scheme using only aresidual loss similar to Physics-Informed Neural Networks (PINNs) with noaccess to ground-truth data. Finally, we push the methodology further byconsidering higher-order schemes: we train (i) a 2-hop and (ii) a 2-layers GNNusing the same PINN loss, that autonomously discover (i) a second-ordercorrection term to the initial scheme using a 2-hop stencil, and (ii) theclassic second-order midpoint scheme. These findings follows a recent paradigmin scientific computing: GNNs are not only strong approximators, but can beactive contributors to the development of novel numerical methods.</description>
      <author>example@mail.com (Paul Garnier, Jonathan Viquerat, Elie Hachem)</author>
      <guid isPermaLink="false">2508.19052v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>VISION: Robust and Interpretable Code Vulnerability Detection Leveraging Counterfactual Augmentation</title>
      <link>http://arxiv.org/abs/2508.18933v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VISION的统一框架，用于鲁棒和可解释的漏洞检测，通过生成反事实训练数据来减轻图神经网络中的虚假相关性学习，显著提高了漏洞检测的准确性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;自动检测源代码中的漏洞是网络安全的重要挑战，图神经网络(GNNs)虽然能够通过数据驱动方式学习代码的结构和逻辑关系，但其性能受到训练数据不平衡和标签噪声的限制，常常从表面代码相似性中学到虚假相关性，导致无法很好地推广到真实世界数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为VISION的统一框架，通过系统性地增强反事实训练数据集来减轻虚假相关性，实现更鲁棒和可解释的漏洞检测。&lt;h4&gt;方法&lt;/h4&gt;VISION框架包含三个核心部分：通过提示大型语言模型(LLM)生成反事实样本；在具有相反标签的配对代码示例上进行有针对性的GNN训练；以及基于图的解释性方法，识别对漏洞预测至关重要的代码语句同时忽略虚假相关性。&lt;h4&gt;主要发现&lt;/h4&gt;VISION显著减少了虚假学习，实现了更鲁棒、可推广的检测：在CWE-20漏洞上，整体准确率从51.8%提高到97.8%，成对对比准确率从4.5%提高到95.8%，最差组准确率从0.7%提高到85.5%。通过提出的指标（类内归因方差、类间归因距离和节点分数依赖性）也展示了显著改进。&lt;h4&gt;结论&lt;/h4&gt;研究团队发布了包含27,556个函数（真实和反事实）的CWE-20-CFA基准数据集，并通过交互式可视化推进了透明和值得信赖的基于AI的网络安全系统，支持人机循环分析。&lt;h4&gt;翻译&lt;/h4&gt;自动检测源代码中的漏洞是网络安全的重要挑战，它支撑着对数字系统和服务的信任。图神经网络(GNNs)作为一种有前景的方法出现，因为它们可以通过数据驱动的方式学习代码的结构和逻辑关系。然而，它们的性能受到训练数据不平衡和标签噪声的严重限制。GNNs通常从表面的代码相似性中学到'虚假'相关性，产生的检测器无法很好地推广到未见过的真实世界数据。在这项工作中，我们提出了一个名为VISION的鲁棒和可解释漏洞检测的统一框架，通过系统性地增强反事实训练数据集来减轻虚假相关性。反事实是具有最小语义修改但标签相反的样本。我们的框架包括：通过提示大型语言模型(LLM)生成反事实；在具有相反标签的配对代码示例上进行有针对性的GNN训练；以及基于图的解释性，识别与漏洞预测相关的关键代码语句，同时忽略虚假相关性。我们发现VISION减少了虚假学习，实现了更鲁棒、可推广的检测，在通用缺陷枚举(CWE)-20漏洞上将整体准确率（从51.8%提高到97.8%）、成对对比准确率（从4.5%提高到95.8%）和最差组准确率（从0.7%提高到85.5%）都得到了提高。我们使用提出的指标进一步展示了改进：类内归因方差、类间归因距离和节点分数依赖性。我们还发布了CWE-20-CFA，这是一个来自高影响CWE-20类别的27,556个函数（真实和反事实）的基准。最后，VISION通过人机循环分析的交互式可视化，推进了透明和值得信赖的基于AI的网络安全系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated detection of vulnerabilities in source code is an essentialcybersecurity challenge, underpinning trust in digital systems and services.Graph Neural Networks (GNNs) have emerged as a promising approach as they canlearn structural and logical code relationships in a data-driven manner.However, their performance is severely constrained by training data imbalancesand label noise. GNNs often learn 'spurious' correlations from superficial codesimilarities, producing detectors that fail to generalize well to unseenreal-world data. In this work, we propose a unified framework for robust andinterpretable vulnerability detection, called VISION, to mitigate spuriouscorrelations by systematically augmenting a counterfactual training dataset.Counterfactuals are samples with minimal semantic modifications but oppositelabels. Our framework includes: (i) generating counterfactuals by prompting aLarge Language Model (LLM); (ii) targeted GNN training on paired code exampleswith opposite labels; and (iii) graph-based interpretability to identify thecrucial code statements relevant for vulnerability predictions while ignoringspurious ones. We find that VISION reduces spurious learning and enables morerobust, generalizable detection, improving overall accuracy (from 51.8% to97.8%), pairwise contrast accuracy (from 4.5% to 95.8%), and worst-groupaccuracy (from 0.7% to 85.5%) on the Common Weakness Enumeration (CWE)-20vulnerability. We further demonstrate gains using proposed metrics: intra-classattribution variance, inter-class attribution distance, and node scoredependency. We also release CWE-20-CFA, a benchmark of 27,556 functions (realand counterfactual) from the high-impact CWE-20 category. Finally, VISIONadvances transparent and trustworthy AI-based cybersecurity systems throughinteractive visualization for human-in-the-loop analysis.</description>
      <author>example@mail.com (David Egea, Barproda Halder, Sanghamitra Dutta)</author>
      <guid isPermaLink="false">2508.18933v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>pyFAST: A Modular PyTorch Framework for Time Series Modeling with Multi-source and Sparse Data</title>
      <link>http://arxiv.org/abs/2508.18891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;pyFAST是一个面向研究的时间序列分析PyTorch框架，具有灵活性、效率和可扩展性特点，特别处理不规则、多源或稀疏数据，明确分离数据处理与模型计算。&lt;h4&gt;背景&lt;/h4&gt;现代时间序列分析需要灵活、高效和可扩展的框架，但许多现有的Python库在模块化以及对不规则、多源或稀疏数据的原生支持方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一个研究导向的PyTorch框架，明确分离数据处理与模型计算，促进关注点的清晰分离，并加速实验过程。&lt;h4&gt;方法&lt;/h4&gt;pyFAST框架包含针对复杂场景设计的数据引擎，支持多源加载、蛋白质序列处理、高效的序列和补丁级填充、动态归一化，以及基于掩模的建模用于插值和预测。集成了受LLM启发的架构用于无对齐稀疏数据源融合，并提供原生稀疏度指标、专用损失函数和灵活的外部数据融合。&lt;h4&gt;主要发现&lt;/h4&gt;pyFAST提供了完整的经典和深度学习模型套件（线性模型、CNN、RNN、Transformer和GNN），采用模块化架构鼓励扩展。训练工具包括基于批次的流式聚合用于评估和设备协同以最大化计算效率。&lt;h4&gt;结论&lt;/h4&gt;pyFAST在GitHub上以MIT许可证发布，为推进时间序列研究和应用提供了一个紧凑而强大的平台。&lt;h4&gt;翻译&lt;/h4&gt;现代时间序列分析需要灵活、高效和可扩展的框架。然而，许多现有的Python库在模块化及其对不规则、多源或稀疏数据的原生支持方面存在局限性。我们介绍了pyFAST，一个面向研究的PyTorch框架，它明确地将数据处理与模型计算分离，促进关注点的清晰分离并加速实验。其数据引擎专为复杂场景设计，支持多源加载、蛋白质序列处理、高效的序列和补丁级填充、动态归一化，以及用于插值和预测的基于掩模的建模。pyFAST集成了受LLM启发的架构，用于无对齐稀疏数据源融合，并提供原生稀疏度指标、专用损失函数和灵活的外部数据融合。训练工具包括用于评估的基于批次的流式聚合和设备协同以最大化计算效率。在模块化架构中提供了完整的经典和深度学习模型套件（线性模型、CNN、RNN、Transformer和GNN），鼓励扩展。在GitHub上以MIT许可证发布，pyFAST为推进时间序列研究和应用提供了一个紧凑而强大的平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern time series analysis demands frameworks that are flexible, efficient,and extensible. However, many existing Python libraries exhibit limitations inmodularity and in their native support for irregular, multi-source, or sparsedata. We introduce pyFAST, a research-oriented PyTorch framework thatexplicitly decouples data processing from model computation, fostering acleaner separation of concerns and facilitating rapid experimentation. Its dataengine is engineered for complex scenarios, supporting multi-source loading,protein sequence handling, efficient sequence- and patch-level padding, dynamicnormalization, and mask-based modeling for both imputation and forecasting.pyFAST integrates LLM-inspired architectures for the alignment-free fusion ofsparse data sources and offers native sparse metrics, specialized lossfunctions, and flexible exogenous data fusion. Training utilities includebatch-based streaming aggregation for evaluation and device synergy to maximizecomputational efficiency. A comprehensive suite of classical and deep learningmodels (Linears, CNNs, RNNs, Transformers, and GNNs) is provided within amodular architecture that encourages extension. Released under the MIT licenseat GitHub, pyFAST provides a compact yet powerful platform for advancing timeseries research and applications.</description>
      <author>example@mail.com (Zhijin Wang, Senzhen Wu, Yue Hu, Xiufeng Liu)</author>
      <guid isPermaLink="false">2508.18891v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Drug-Drug Interactions Using Heterogeneous Graph Neural Networks: HGNN-DDI</title>
      <link>http://arxiv.org/abs/2508.18766v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures. Published in Applied and Computational  Engineering, Vol. 79, pp. 77-89, July 25, 2024. Licensed under CC BY 4.0&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为HGNN-DDI的异构图神经网络模型，用于预测潜在的药物-药物相互作用，通过整合多种药物相关数据源，实现了比传统方法更准确的预测结果。&lt;h4&gt;背景&lt;/h4&gt;药物-药物相互作用是临床实践中的一个主要问题，可能导致治疗效果降低或严重副作用。传统计算方法难以捕捉药物、靶点和生物实体之间的复杂关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效预测潜在药物-药物相互作用的计算模型，通过整合多种药物相关数据源提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;HGNN-DDI是一种异构图神经网络模型，利用图表示学习技术建模异构生物医学网络，实现不同节点和边类型之间的有效信息传播。&lt;h4&gt;主要发现&lt;/h4&gt;在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于现有的最先进基线方法。&lt;h4&gt;结论&lt;/h4&gt;HGNN-DDI模型有望支持更安全的药物开发和精准医疗实践。&lt;h4&gt;翻译&lt;/h4&gt;药物-药物相互作用是临床实践中的一个主要问题，因为它们可能导致治疗效果降低或严重的不良反应。传统的计算方法往往难以捕捉药物、靶点和生物实体之间的复杂关系。在本工作中，我们提出了HGNN-DDI，一种异构图神经网络模型，通过整合多种药物相关数据源来预测潜在的药物-药物相互作用。HGNN-DDI利用图表示学习来建模异构生物医学网络，使不同节点和边类型之间能够有效传播信息。在基准DDI数据集上的实验结果表明，HGNN-DDI在预测准确性和鲁棒性方面优于最先进的基线方法，凸显了其在支持更安全的药物开发和精准医疗方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.54254/2755-2721/79/20241329&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug-drug interactions (DDIs) are a major concern in clinical practice, asthey can lead to reduced therapeutic efficacy or severe adverse effects.Traditional computational approaches often struggle to capture the complexrelationships among drugs, targets, and biological entities. In this work, wepropose HGNN-DDI, a heterogeneous graph neural network model designed topredict potential DDIs by integrating multiple drug-related data sources.HGNN-DDI leverages graph representation learning to model heterogeneousbiomedical networks, enabling effective information propagation across diversenode and edge types. Experimental results on benchmark DDI datasets demonstratethat HGNN-DDI outperforms state-of-the-art baselines in prediction accuracy androbustness, highlighting its potential to support safer drug development andprecision medicine.</description>
      <author>example@mail.com (Hongbo Liu, Siyi Li, Zheng Yu)</author>
      <guid isPermaLink="false">2508.18766v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>A Note on Graphon-Signal Analysis of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2508.18564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对Levie的图论信号分析论文进行了改进和扩展，解决了原始论文中限制其在实际图机器学习中应用的一些不足。&lt;h4&gt;背景&lt;/h4&gt;Levie的论文通过将MPNNs的输入空间（属性图/图信号）嵌入到属性图论（图信号）空间中分析了消息传递图神经网络，并基于图论分析标准结果的扩展证明了MPNNs的泛化边界和采样引理。&lt;h4&gt;目的&lt;/h4&gt;引入对现有结果的几种改进和扩展，以解决原始论文中限制其在实际应用中适用性的不足。&lt;h4&gt;方法&lt;/h4&gt;对原始论文进行四项主要改进：1)将结果扩展到多维信号的图信号；2)将Lipschitz连续性扩展到具有读出且关于切割距离的MPNNs；3)利用鲁棒性类型的泛化边界改进泛化边界；4)将分析扩展到非对称图论和核函数。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述四项改进，解决了原始论文中的理论局限，使结果更适用于实际图机器学习场景。&lt;h4&gt;结论&lt;/h4&gt;这些改进和扩展增强了理论结果在实际应用中的适用性，为图神经网络的分析提供了更全面的理论基础。&lt;h4&gt;翻译&lt;/h4&gt;Levie最近发表的论文《图神经网络的图论信号分析》通过将MPNNs的输入空间（即属性图/图信号）嵌入到属性图论（图信号）空间中分析了消息传递图神经网络。基于图论分析标准结果向图信号的扩展，该论文证明了MPNNs的一个泛化边界和一个采样引理。然而，该论文存在一些缺失成分，限制了其在图机器学习实际应用场景中的适用性。在本文中，我们引入了几种对现有结果的改进和扩展，以解决这些不足。具体而言，1)我们将论文中的主要结果扩展到具有多维信号（而非一维信号）的图信号；2)我们将Lipschitz连续性扩展到具有读出且关于切割距离（而非无读出且关于切割度量）的MPNNs；3)我们通过利用鲁棒性类型的泛化边界改进了泛化边界；4)我们将分析扩展到非对称图论和核函数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A recent paper, ``A Graphon-Signal Analysis of Graph Neural Networks'', byLevie, analyzed message passing graph neural networks (MPNNs) by embedding theinput space of MPNNs, i.e., attributed graphs (graph-signals), to a space ofattributed graphons (graphon-signals). Based on extensions of standard resultsin graphon analysis to graphon-signals, the paper proved a generalization boundand a sampling lemma for MPNNs. However, there are some missing ingredients inthat paper, limiting its applicability in practical settings of graph machinelearning. In the current paper, we introduce several refinements and extensionsto existing results that address these shortcomings. In detail, 1) we extendthe main results in the paper to graphon-signals with multidimensional signals(rather than 1D signals), 2) we extend the Lipschitz continuity to MPNNs withreadout with respect to cut distance (rather than MPNNs without readout withrespect to cut metric), 3) we improve the generalization bound by utilizingrobustness-type generalization bounds, and 4) we extend the analysis tonon-symmetric graphons and kernels.</description>
      <author>example@mail.com (Levi Rauchwerger, Ron Levie)</author>
      <guid isPermaLink="false">2508.18564v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Quantum Graph Attention Network: A Novel Quantum Multi-Head Attention Mechanism for Graph Learning</title>
      <link>http://arxiv.org/abs/2508.17630v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了量子图注意力网络（QGAT），一种将变分量子电路集成到注意力机制中的混合图神经网络。&lt;h4&gt;背景&lt;/h4&gt;传统图神经网络在处理复杂图结构数据时面临计算复杂度高和模型复杂等问题，需要新的方法来提高效率。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合量子计算优势的图神经网络模型，降低计算复杂度，提高处理图结构数据的能力。&lt;h4&gt;方法&lt;/h4&gt;1. 使用强纠缠量子电路和振幅编码的节点特征实现非线性交互；2. 采用单个量子电路同时生成多个注意力系数，而非传统多头注意力的分别计算；3. 通过量子并行实现参数共享，降低计算开销；4. 经典投影权重和量子电路参数端到端联合优化；5. 设计模块化结构便于与现有架构集成。&lt;h4&gt;主要发现&lt;/h4&gt;1. QGAT能有效捕捉复杂结构依赖关系；2. 在归纳场景中具有更好的泛化能力；3. 量子嵌入增强了特征和结构噪声的鲁棒性；4. 在处理真实世界噪声数据方面具有优势；5. 可以轻松增强基于注意力的经典模型。&lt;h4&gt;结论&lt;/h4&gt;QGAT是一种有前途的量子增强学习方法，在化学、生物学和网络分析等领域具有可扩展的应用潜力，其模块化设计使其能够轻松集成到现有架构中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了量子图注意力网络（QGAT），这是一种将变分量子电路集成到注意力机制中的混合图神经网络。其核心是QGAT采用强纠缠量子电路和振幅编码的节点特征来实现有表现力的非线性交互。与分别计算每个头的经典多头注意力不同，QGAT利用单个量子电路同时生成多个注意力系数。这种量子并行性促进了跨头的参数共享，显著降低了计算开销和模型复杂度。经典投影权重和量子电路参数以端到端方式联合优化，确保灵活适应学习任务。实证结果表明，QGAT在捕捉复杂结构依赖关系和在归纳场景中改进泛化方面有效，突显了其在化学、生物学和网络分析等跨领域可扩展量子增强学习方面的潜力。此外，实验证实量子嵌入增强了特征和结构噪声的鲁棒性，表明在处理真实世界噪声数据方面具有优势。QGAT的模块化也确保了与现有架构的简单集成，使其能够轻松增强基于注意力的经典模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Quantum Graph Attention Network (QGAT), a hybrid graph neuralnetwork that integrates variational quantum circuits into the attentionmechanism. At its core, QGAT employs strongly entangling quantum circuits withamplitude-encoded node features to enable expressive nonlinear interactions.Distinct from classical multi-head attention that separately computes eachhead, QGAT leverages a single quantum circuit to simultaneously generatemultiple attention coefficients. This quantum parallelism facilitates parametersharing across heads, substantially reducing computational overhead and modelcomplexity. Classical projection weights and quantum circuit parameters areoptimized jointly in an end-to-end manner, ensuring flexible adaptation tolearning tasks. Empirical results demonstrate QGAT's effectiveness in capturingcomplex structural dependencies and improved generalization in inductivescenarios, highlighting its potential for scalable quantum-enhanced learningacross domains such as chemistry, biology, and network analysis. Furthermore,experiments confirm that quantum embedding enhances robustness against featureand structural noise, suggesting advantages in handling real-world noisy data.The modularity of QGAT also ensures straightforward integration into existingarchitectures, allowing it to easily augment classical attention-based models.</description>
      <author>example@mail.com (An Ning, Tai Yue Li, Nan Yow Chen)</author>
      <guid isPermaLink="false">2508.17630v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Tokens: Enhancing RTL Quality Estimation via Structural Graph Learning</title>
      <link>http://arxiv.org/abs/2508.18730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为StructRTL的新型结构感知图自监督学习框架，用于改进RTL设计质量估计。该方法通过从控制数据流图(CDFG)中学习结构感知的表示，结合知识蒸馏策略，将后映射网表中的低级见解转移到CDFG预测器中，显著提升了质量估计性能。&lt;h4&gt;背景&lt;/h4&gt;在电子设计自动化(EDA)工作流中，估计寄存器传输级(RTL)设计质量至关重要，因为它可以提供关于面积和延迟等关键指标的即时反馈，而无需耗时的逻辑综合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确估计RTL设计质量的方法，重点关注结构语义，以提高估计的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为StructRTL的结构感知图自监督学习框架，通过从控制数据流图(CDFG)中学习结构感知的表示，并采用知识蒸馏策略将后映射网表中的低级见解转移到CDFG预测器中。&lt;h4&gt;主要发现&lt;/h4&gt;通过从CDFGs中学习结构感知的表示，该方法在各种质量估计任务上都显著优于先前的工作。结合结构学习和跨阶段监督的方法建立了新的最先进结果。&lt;h4&gt;结论&lt;/h4&gt;结合结构学习和跨阶段监督的方法对于RTL设计质量估计是有效的，StructRTL框架能够显著提升估计性能。&lt;h4&gt;翻译&lt;/h4&gt;在电子设计自动化(EDA)工作流中估计寄存器传输级(RTL)设计质量至关重要，因为它可以提供关于面积和延迟等关键指标的即时反馈，而无需耗时的逻辑综合。虽然最近的方法利用大型语言模型(LLMs)从RTL代码中提取嵌入并取得了有希望的结果，但它们忽略了准确质量估计所必需的结构语义。相比之下，控制数据流图(CDFG)视图更明确地展示了设计的结构特征，为表示学习提供了更丰富的线索。在这项工作中，我们提出了一种新颖的、结构感知的图自监督学习框架StructRTL，用于改进RTL设计质量估计。通过从CDFGs中学习结构感知的表示，我们的方法在各种质量估计任务上都显著优于先前的工作。为了进一步提高性能，我们采用了一种知识蒸馏策略，将后映射网表中的低级见解转移到CDFG预测器中。实验表明，我们的方法建立了新的最先进结果，证明了结合结构学习和跨阶段监督的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the quality of register transfer level (RTL) designs is crucial inthe electronic design automation (EDA) workflow, as it enables instant feedbackon key metrics like area and delay without the need for time-consuming logicsynthesis. While recent approaches have leveraged large language models (LLMs)to derive embeddings from RTL code and achieved promising results, theyoverlook the structural semantics essential for accurate quality estimation. Incontrast, the control data flow graph (CDFG) view exposes the design'sstructural characteristics more explicitly, offering richer cues forrepresentation learning. In this work, we introduce a novel structure-awaregraph self-supervised learning framework, StructRTL, for improved RTL designquality estimation. By learning structure-informed representations from CDFGs,our method significantly outperforms prior art on various quality estimationtasks. To further boost performance, we incorporate a knowledge distillationstrategy that transfers low-level insights from post-mapping netlists into theCDFG predictor. Experiments show that our approach establishes newstate-of-the-art results, demonstrating the effectiveness of combiningstructural learning with cross-stage supervision.</description>
      <author>example@mail.com (Yi Liu, Hongji Zhang, Yiwen Wang, Dimitris Tsaras, Lei Chen, Mingxuan Yuan, Qiang Xu)</author>
      <guid isPermaLink="false">2508.18730v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Clustering-based Feature Representation Learning for Oracle Bone Inscriptions Detection</title>
      <link>http://arxiv.org/abs/2508.18641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于聚类的特征空间表示学习方法，用于从拓片图像中自动检测甲骨文，解决了噪声和裂缝等退化因素对传统检测网络的限制问题。&lt;h4&gt;背景&lt;/h4&gt;甲骨文在理解中国古代文明中起关键作用。从拓片图像中自动检测甲骨文是数字考古学中的基础但具有挑战性的任务，主要由于噪声和裂缝等退化因素限制了传统检测网络的有效性。&lt;h4&gt;目的&lt;/h4&gt;解决甲骨文检测中的挑战，提高检测网络在退化因素下的性能。&lt;h4&gt;方法&lt;/h4&gt;利用甲骨文字体库数据集作为先验知识，通过基于聚类的表示学习增强检测网络中的特征提取。方法包含一个从聚类结果导出的特殊损失函数，用于优化特征表示，然后将其整合到网络总损失中。&lt;h4&gt;主要发现&lt;/h4&gt;在两个甲骨文检测数据集上使用三种主流检测框架（Faster R-CNN、DETR和Sparse R-CNN）进行实验验证，所有框架都显示出显著的性能改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于聚类的特征空间表示学习方法能有效提高甲骨文检测的性能。&lt;h4&gt;翻译&lt;/h4&gt;甲骨文在理解中国古代文明中起着至关重要的作用。从拓片图像中自动检测甲骨文是数字考古学中的一个基础但具有挑战性的任务，主要由于噪声和裂缝等各种退化因素限制了传统检测网络的有效性。为了解决这些挑战，我们提出了一种新颖的基于聚类的特征空间表示学习方法。我们的方法独特地利用甲骨文字体库数据集作为先验知识，通过基于聚类的表示学习增强检测网络中的特征提取。该方法包含一个从聚类结果导出的特殊损失函数，用于优化特征表示，然后将其整合到网络总损失中。我们通过在两个甲骨文检测数据集上使用三种主流检测框架（Faster R-CNN、DETR和Sparse R-CNN）进行实验来验证我们方法的有效性。通过大量实验，所有框架都显示出显著的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Oracle Bone Inscriptions (OBIs), play a crucial role in understanding ancientChinese civilization. The automated detection of OBIs from rubbing imagesrepresents a fundamental yet challenging task in digital archaeology, primarilydue to various degradation factors including noise and cracks that limit theeffectiveness of conventional detection networks. To address these challenges,we propose a novel clustering-based feature space representation learningmethod. Our approach uniquely leverages the Oracle Bones Character (OBC) fontlibrary dataset as prior knowledge to enhance feature extraction in thedetection network through clustering-based representation learning. The methodincorporates a specialized loss function derived from clustering results tooptimize feature representation, which is then integrated into the totalnetwork loss. We validate the effectiveness of our method by conductingexperiments on two OBIs detection dataset using three mainstream detectionframeworks: Faster R-CNN, DETR, and Sparse R-CNN. Through extensiveexperimentation, all frameworks demonstrate significant performanceimprovements.</description>
      <author>example@mail.com (Ye Tao, Xinran Fu, Honglin Pang, Xi Yang, Chuntao Li)</author>
      <guid isPermaLink="false">2508.18641v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>PCR-CA: Parallel Codebook Representations with Contrastive Alignment for Multiple-Category App Recommendation</title>
      <link>http://arxiv.org/abs/2508.18166v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 4 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCR-CA的端到端框架，用于改进应用商店推荐系统的点击率预测。该框架通过并行码本表示和对比对齐技术，解决了传统分类法无法捕捉多类别应用重叠语义的问题，特别是在处理长尾应用方面表现出色。&lt;h4&gt;背景&lt;/h4&gt;现代应用商店推荐系统难以处理多类别应用，因为传统分类法无法捕捉重叠语义，导致个性化效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为PCR-CA（并行码本表示与对比对齐）的端到端框架，用于改进点击率(CTR)预测。&lt;h4&gt;方法&lt;/h4&gt;PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入并行码本VQ-AE模块，并行学习多个码本上的离散语义表示，不同于分层残差量化(RQ-VAE)。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。此外，采用对比对齐损失桥接语义和协同信号，增强长尾项目的表示学习，并使用双注意力融合机制结合基于ID和语义的特征捕捉用户兴趣。&lt;h4&gt;主要发现&lt;/h4&gt;在大规模数据集上的实验显示，PCR-CA相比强基线模型实现了+0.76%的AUC改进，对于长尾应用，AUC提升了+2.15%。在线A/B测试进一步验证了该方法，显示CTR提升+10.52%，转化率(CVR)提升+16.30%。&lt;h4&gt;结论&lt;/h4&gt;PCR-CA框架在真实部署中表现出色，已完全部署在Microsoft Store上。&lt;h4&gt;翻译&lt;/h4&gt;现代应用商店推荐系统难以处理多类别应用，因为传统分类法无法捕捉重叠语义，导致个性化效果不佳。我们提出了PCR-CA（并行码本表示与对比对齐），这是一个用于改进点击率预测的端到端框架。PCR-CA首先从应用文本中提取紧凑的多模态嵌入，然后引入并行码本VQ-AE模块，并行学习多个码本上的离散语义表示——这与分层残差量化(RQ-VAE)不同。这种设计能够独立编码不同方面（如游戏玩法、艺术风格），更好地建模多类别语义。为了桥接语义和协同信号，我们在用户和项目级别都采用了对比对齐损失，增强长尾项目的表示学习。此外，双注意力融合机制结合基于ID和语义的特征，捕捉用户兴趣，特别是对于长尾应用。在大规模数据集上的实验显示，PCR-CA相比强基线模型实现了+0.76%的AUC改进，对于长尾应用，AUC提升了+2.15%。在线A/B测试进一步验证了我们的方法，显示CTR提升+10.52%，转化率提升+16.30%，证明了PCR-CA在真实部署中的有效性。该新框架现已完全部署在Microsoft Store上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern app store recommender systems struggle with multiple-category apps, astraditional taxonomies fail to capture overlapping semantics, leading tosuboptimal personalization. We propose PCR-CA (Parallel CodebookRepresentations with Contrastive Alignment), an end-to-end framework forimproved CTR prediction. PCR-CA first extracts compact multimodal embeddingsfrom app text, then introduces a Parallel Codebook VQ-AE module that learnsdiscrete semantic representations across multiple codebooks in parallel --unlike hierarchical residual quantization (RQ-VAE). This design enablesindependent encoding of diverse aspects (e.g., gameplay, art style), bettermodeling multiple-category semantics. To bridge semantic and collaborativesignals, we employ a contrastive alignment loss at both the user and itemlevels, enhancing representation learning for long-tail items. Additionally, adual-attention fusion mechanism combines ID-based and semantic features tocapture user interests, especially for long-tail apps. Experiments on alarge-scale dataset show PCR-CA achieves a +0.76% AUC improvement over strongbaselines, with +2.15% AUC gains for long-tail apps. Online A/B testing furthervalidates our approach, showing a +10.52% lift in CTR and a +16.30% improvementin CVR, demonstrating PCR-CA's effectiveness in real-world deployment. The newframework has now been fully deployed on the Microsoft Store.</description>
      <author>example@mail.com (Bin Tan, Wangyao Ge, Yidi Wang, Xin Liu, Jeff Burtoft, Hao Fan, Hui Wang)</author>
      <guid isPermaLink="false">2508.18166v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>ProtoEHR: Hierarchical Prototype Learning for EHR-based Healthcare Predictions</title>
      <link>http://arxiv.org/abs/2508.18313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CIKM 2025 Full Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ProtoEHR是一个可解释的分层原型学习框架，通过充分利用电子健康记录的多级结构来增强医疗预测的准确性、稳健性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;数字医疗系统使得在电子健康记录中收集大量医疗数据成为可能，为人工智能解决各种医疗预测任务提供了基础。然而，现有研究往往只关注EHR数据的孤立部分，限制了预测性能和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够充分利用EHR数据丰富多级结构的框架，以增强医疗预测的性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;ProtoEHR建模了EHR三个层次内部和之间的关系：医疗代码、医院就诊和患者。利用大型语言模型提取医疗代码间的语义关系并构建医疗知识图谱，设计分层表示学习框架捕捉三个层次的上下文化表示，并在每个层次中融入原型信息以捕获内在相似性并提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在两个公共数据集上对五个临床任务（死亡率预测、再入院预测、住院时长预测、药物推荐和表型预测）的评估表明，ProtoEHR相比基线方法能够做出更准确、稳健和可解释的预测，同时在代码、就诊和患者层面提供可解释的见解。&lt;h4&gt;结论&lt;/h4&gt;ProtoEHR通过有效利用EHR数据的分层结构，显著提升了医疗预测的性能和可解释性，为医疗决策提供了有价值的支持。&lt;h4&gt;翻译&lt;/h4&gt;数字医疗系统使得在电子健康记录中收集大量医疗数据成为可能，允许人工智能解决各种医疗预测任务。然而，现有研究往往只关注EHR数据的孤立部分，限制了它们的预测性能和可解释性。为了解决这一差距，我们提出了ProtoEHR，一个可解释的分层原型学习框架，它充分利用EHR数据的丰富多级结构来增强医疗预测。更具体地说，ProtoEHR建模了EHR三个层次内部和之间的关系：医疗代码、医院就诊和患者。我们首先利用大型语言模型提取医疗代码之间的语义关系，并构建医疗知识图谱作为知识源。在此基础上，我们设计了一个分层表示学习框架，捕捉三个层次的上下文化表示，同时在每个层次中融入原型信息以捕获内在相似性并提高泛化能力。为了进行全面评估，我们在两个公共数据集上对五个临床重要任务评估了ProtoEHR，包括死亡率预测、再入院预测、住院时长预测、药物推荐和表型预测。结果表明，与文献中的基线相比，ProtoEHR能够做出准确、稳健和可解释的预测。此外，ProtoEHR在代码、就诊和患者层面提供可解释的见解，以辅助医疗预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Digital healthcare systems have enabled the collection of mass healthcaredata in electronic healthcare records (EHRs), allowing artificial intelligencesolutions for various healthcare prediction tasks. However, existing studiesoften focus on isolated components of EHR data, limiting their predictiveperformance and interpretability. To address this gap, we propose ProtoEHR, aninterpretable hierarchical prototype learning framework that fully exploits therich, multi-level structure of EHR data to enhance healthcare predictions. Morespecifically, ProtoEHR models relationships within and across threehierarchical levels of EHRs: medical codes, hospital visits, and patients. Wefirst leverage large language models to extract semantic relationships amongmedical codes and construct a medical knowledge graph as the knowledge source.Building on this, we design a hierarchical representation learning frameworkthat captures contextualized representations across three levels, whileincorporating prototype information within each level to capture intrinsicsimilarities and improve generalization. To perform a comprehensive assessment,we evaluate ProtoEHR in two public datasets on five clinically significanttasks, including prediction of mortality, prediction of readmission, predictionof length of stay, drug recommendation, and prediction of phenotype. Theresults demonstrate the ability of ProtoEHR to make accurate, robust, andinterpretable predictions compared to baselines in the literature. Furthermore,ProtoEHR offers interpretable insights on code, visit, and patient levels toaid in healthcare prediction.</description>
      <author>example@mail.com (Zi Cai, Yu Liu, Zhiyao Luo, Tingting Zhu)</author>
      <guid isPermaLink="false">2508.18313v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>GRADSTOP: Early Stopping of Gradient Descent via Posterior Sampling</title>
      <link>http://arxiv.org/abs/2508.19028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为gradstop的随机早停方法，仅使用梯度信息进行模型训练的早停判断，无需单独验证集，从而充分利用全部数据训练模型。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型通常通过在训练数据上使用梯度下降算法最小化损失函数来学习，但这些模型经常出现过拟合问题，导致在未见数据上的预测性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖于验证集的早停方法，以解决传统早停方法减少可用训练数据的问题，特别是在数据有限的场景下。&lt;h4&gt;方法&lt;/h4&gt;gradstop方法通过梯度信息估计贝叶斯后验，将早停问题定义为从这个后验中采样，并使用近似后验获得停止标准。&lt;h4&gt;主要发现&lt;/h4&gt;gradstop在测试数据上实现了较小的损失，与基于验证集的停止标准相比表现良好；该方法可以充分利用整个数据集进行训练，在数据有限的情况下（如迁移学习）特别有利。&lt;h4&gt;结论&lt;/h4&gt;gradstop可以作为梯度下降库的一个可选功能添加，计算开销很小，源代码已在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型通常通过在训练数据上使用梯度下降算法最小化损失函数来学习。这些模型经常出现过拟合问题，导致在未见数据上的预测性能下降。标准的解决方案是使用保留的验证集进行早停，当验证损失停止减小时停止最小化过程。然而，这个保留集减少了可用于训练的数据量。本文提出了gradstop，一种新颖的随机早停方法，仅使用梯度信息（这些信息由梯度下降算法'免费'产生）。我们的主要贡献是：我们使用梯度信息估计贝叶斯后验，将早停问题定义为从这个后验中采样，并使用近似后验获得停止标准。我们的实证评估表明，gradstop在测试数据上实现了较小的损失，并且与基于验证集的停止标准相比表现良好。通过利用整个数据集进行训练，我们的方法在数据有限的设置（如迁移学习）中特别有利。它可以作为梯度下降库的一个可选功能添加，计算开销很小。源代码可在https://github.com/edahelsinki/gradstop获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models are often learned by minimising a loss function onthe training data using a gradient descent algorithm. These models often sufferfrom overfitting, leading to a decline in predictive performance on unseendata. A standard solution is early stopping using a hold-out validation set,which halts the minimisation when the validation loss stops decreasing.However, this hold-out set reduces the data available for training. This paperpresents {\sc gradstop}, a novel stochastic early stopping method that onlyuses information in the gradients, which are produced by the gradient descentalgorithm ``for free.'' Our main contributions are that we estimate theBayesian posterior by the gradient information, define the early stoppingproblem as drawing sample from this posterior, and use the approximatedposterior to obtain a stopping criterion. Our empirical evaluation shows that{\sc gradstop} achieves a small loss on test data and compares favourably to avalidation-set-based stopping criterion. By leveraging the entire dataset fortraining, our method is particularly advantageous in data-limited settings,such as transfer learning. It can be incorporated as an optional feature ingradient descent libraries with only a small computational overhead. The sourcecode is available at https://github.com/edahelsinki/gradstop.</description>
      <author>example@mail.com (Arash Jamshidi, Lauri Seppäläinen, Katsiaryna Haitsiukevich, Hoang Phuc Hau Luu, Anton Björklund, Kai Puolamäki)</author>
      <guid isPermaLink="false">2508.19028v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>C-Flat++: Towards a More Efficient and Powerful Framework for Continual Learning</title>
      <link>http://arxiv.org/abs/2508.18860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了C-Flat方法，用于持续学习中的平坦损失景观优化，以提高知识保留和学习效率。&lt;h4&gt;背景&lt;/h4&gt;在持续学习中，平衡对新任务的敏感性和保留过去知识的稳定性至关重要。尖锐度感知最小化已在迁移学习中证明有效，并应用于持续学习，但仅依赖零阶尖锐度可能导致选择不够鲁棒的尖锐极小值。&lt;h4&gt;目的&lt;/h4&gt;开发一种促进适合持续学习的平坦损失景观的方法，以提高知识保留的鲁棒性和学习效率。&lt;h4&gt;方法&lt;/h4&gt;提出C-Flat(Continual Flatness)方法，具有即插即用兼容性；开发通用框架将C-Flat整合到所有主要CL范式中；引入C-Flat++，利用选择性平坦度驱动提升，显著降低更新成本。&lt;h4&gt;主要发现&lt;/h4&gt;C-Flat在各种设置中持续提高性能；广泛实验证明所提方法在多种CL方法、数据集和场景中具有有效性和效率。&lt;h4&gt;结论&lt;/h4&gt;C-Flat和C-Flat++是持续学习中有效的平坦度优化方法，能够提高知识保留和学习效率，同时具有较低的计算成本。&lt;h4&gt;翻译&lt;/h4&gt;在持续学习中，平衡对新任务的敏感性和保留过去知识的稳定性至关重要。最近，尖锐度感知最小化已被证明在迁移学习中有效，并已被应用于持续学习以提高记忆保留和学习效率。然而，仅依赖零阶尖锐度可能在某些情况下 favor 更尖锐的极小值而非更平坦的极小值，导致解决方案不够鲁棒且可能次优。本文提出C-Flat(Continual Flatness)方法，旨在促进适合持续学习的平坦损失景观。C-Flat具有即插即用兼容性，可轻松集成到代码管道中。此外，我们提出了一个通用框架，将C-Flat整合到所有主要的持续学习范式中，并与基于损失极小值优化的方法和基于平坦极小值的持续学习方法进行了全面比较。我们的结果显示C-Flat在各种设置中持续提高性能。此外，我们引入了C-Flat++，这是一个高效且有效的框架，利用选择性平坦度驱动提升，显著减少了C-Flat所需的更新成本。在多种持续学习方法、数据集和场景中的广泛实验证明了所提方法的有效性和效率。代码可在https://github.com/WanNaa/C-Flat获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Balancing sensitivity to new tasks and stability for retaining past knowledgeis crucial in continual learning (CL). Recently, sharpness-aware minimizationhas proven effective in transfer learning and has also been adopted incontinual learning (CL) to improve memory retention and learning efficiency.However, relying on zeroth-order sharpness alone may favor sharper minima overflatter ones in certain settings, leading to less robust and potentiallysuboptimal solutions. In this paper, we propose \textbf{C}ontinual\textbf{Flat}ness (\textbf{C-Flat}), a method that promotes flatter losslandscapes tailored for CL. C-Flat offers plug-and-play compatibility, enablingeasy integration with minimal modifications to the code pipeline. Besides, wepresent a general framework that integrates C-Flat into all major CL paradigmsand conduct comprehensive comparisons with loss-minima optimizers andflat-minima-based CL methods. Our results show that C-Flat consistentlyimproves performance across a wide range of settings. In addition, we introduceC-Flat++, an efficient yet effective framework that leverages selectiveflatness-driven promotion, significantly reducing the update cost required byC-Flat. Extensive experiments across multiple CL methods, datasets, andscenarios demonstrate the effectiveness and efficiency of our proposedapproaches. Code is available at https://github.com/WanNaa/C-Flat.</description>
      <author>example@mail.com (Wei Li, Hangjie Yuan, Zixiang Zhao, Yifan Zhu, Aojun Lu, Tao Feng, Yanan Sun)</author>
      <guid isPermaLink="false">2508.18860v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Feature-Space Planes Searcher: A Universal Domain Adaptation Framework for Interpretability and Computational Efficiency</title>
      <link>http://arxiv.org/abs/2508.18693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为特征空间平面搜索器(FPS)的新型领域自适应框架，通过利用预训练模型特征空间中的几何模式优化决策边界，同时保持特征编码器冻结，解决了领域偏移问题，实现了与最先进方法相当或更优的性能，同时降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;领域偏移问题，即当模型从有标签的源领域迁移到无标签的目标领域时性能下降，对深度学习系统的部署构成了持续挑战。当前的无监督领域自适应方法主要依赖于微调特征提取器，这种方法效率低下、可解释性差，且难以扩展到现代架构。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的领域自适应方法，解决现有微调方法效率低、可解释性差和难以扩展的问题，同时保持或提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出特征空间平面搜索器(FPS)，一种新型领域自适应框架，它利用预训练模型特征空间中的领域不变几何模式(类内聚类和类间分离)优化决策边界，同时保持特征编码器冻结，避免引入不可预测的特征失真。&lt;h4&gt;主要发现&lt;/h4&gt;预训练在大规模数据上的模型在特征空间中表现出领域不变的几何模式，包括类内聚类和类间分离，从而保留了可迁移的判别结构；领域偏移主要表现为边界不对齐，而非特征退化。&lt;h4&gt;结论&lt;/h4&gt;FPS通过离线特征提取显著降低了内存和计算成本，允许在单个计算周期内完成全数据集优化，在公共基准测试上与最先进方法相当或更优，并能高效扩展到多模态大模型，在不同领域展现出多功能性，为迁移学习提供了一种简单、有效且通用的范式。&lt;h4&gt;翻译&lt;/h4&gt;领域偏移表现为当模型从有标签的源领域过渡到无标签的目标领域时性能下降，这对部署深度学习系统构成了持续挑战。当前的无监督领域自适应方法主要依赖于微调特征提取器——这种方法效率低下、可解释性差，且难以扩展到现代架构。我们的分析显示，在大规模数据上预训练的模型在特征空间中表现出领域不变的几何模式，特征为类内聚类和类间分离，从而保留了可迁移的判别结构。这些发现表明，领域偏移主要表现为边界不对齐而非特征退化。与微调整个预训练模型(可能引入不可预测的特征失真)不同，我们提出特征空间平面搜索器(FPS)：一种新型领域自适应框架，它通过利用这些几何模式优化决策边界，同时保持特征编码器冻结。这种简化的方法使自适应过程具有可解释性分析，并通过离线特征提取显著降低内存和计算成本，允许在单个计算周期内完成全数据集优化。在公共基准测试上的评估表明，FPS的性能与最先进方法相当或更优。FPS能够高效扩展到多模态大模型，并在蛋白质结构预测、遥感分类和地震检测等不同领域展现出多功能性。我们预期FPS将为迁移学习，特别是在领域自适应任务中，提供一种简单、有效且通用的范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Domain shift, characterized by degraded model performance during transitionfrom labeled source domains to unlabeled target domains, poses a persistentchallenge for deploying deep learning systems. Current unsupervised domainadaptation (UDA) methods predominantly rely on fine-tuning feature extractors -an approach limited by inefficiency, reduced interpretability, and poorscalability to modern architectures.  Our analysis reveals that models pretrained on large-scale data exhibitdomain-invariant geometric patterns in their feature space, characterized byintra-class clustering and inter-class separation, thereby preservingtransferable discriminative structures. These findings indicate that domainshifts primarily manifest as boundary misalignment rather than featuredegradation.  Unlike fine-tuning entire pre-trained models - which risks introducingunpredictable feature distortions - we propose the Feature-space PlanesSearcher (FPS): a novel domain adaptation framework that optimizes decisionboundaries by leveraging these geometric patterns while keeping the featureencoder frozen. This streamlined approach enables interpretative analysis ofadaptation while substantially reducing memory and computational costs throughoffline feature extraction, permitting full-dataset optimization in a singlecomputation cycle.  Evaluations on public benchmarks demonstrate that FPS achieves competitive orsuperior performance to state-of-the-art methods. FPS scales efficiently withmultimodal large models and shows versatility across diverse domains includingprotein structure prediction, remote sensing classification, and earthquakedetection. We anticipate FPS will provide a simple, effective, andgeneralizable paradigm for transfer learning, particularly in domain adaptationtasks. .</description>
      <author>example@mail.com (Zhitong Cheng, Yiran Jiang, Yulong Ge, Yufeng Li, Zhongheng Qin, Rongzhi Lin, Jianwei Ma)</author>
      <guid isPermaLink="false">2508.18693v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Towards Optimal Convolutional Transfer Learning Architectures for Breast Lesion Classification and ACL Tear Detection</title>
      <link>http://arxiv.org/abs/2508.17567v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了在医学图像分类任务中使用不同CNN架构和预训练方法的效果，确定了用于乳腺病变恶性和ACL撕裂检测的最佳CNN架构，并比较了RadImageNet和ImageNet预训练的效果。&lt;h4&gt;背景&lt;/h4&gt;现代计算机视觉模型在医学图像分类和分割任务中非常有用，但医学图像数据的稀缺性限制了从头开始训练的模型的效果。迁移学习已成为解决这一问题的关键方案。Mei等人(2022)发现，在放射科医生标注的大型图像数据集(RadImageNet)上预训练CNN，相比ImageNet预训练，能提高下游任务中的模型性能。&lt;h4&gt;目的&lt;/h4&gt;扩展Mei等人(2022)的工作，通过全面调查确定用于乳腺病变恶性和ACL撕裂检测的最佳CNN架构，并进行统计分析，比较RadImageNet和ImageNet预训练对下游模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;采用1维卷积分类器与跳跃连接、ResNet50预训练骨干网络以及部分骨干网络解冻的方法，用于下游医学分类任务。研究比较了在RadImageNet和ImageNet上预训练的效果。&lt;h4&gt;主要发现&lt;/h4&gt;1. 1维卷积分类器与跳跃连接、ResNet50预训练骨干网络以及部分骨干网络解冻的方法能产生最佳的下游医学分类性能。2. 最佳模型在ACL撕裂检测中达到0.9969的AUC值，在乳腺结节恶性检测中达到0.9641的AUC值。3. 这些结果与Mei等人(2022)报告的结果具有竞争力，并超过了之前的工作。4. 没有证据表明RadImageNet预训练在ACL撕裂和乳腺病变分类任务中能提供优越的下游性能。&lt;h4&gt;结论&lt;/h4&gt;研究确定了用于医学图像分类的最佳CNN架构和预训练方法，并发现RadImageNet预训练不一定比ImageNet预训练在特定医学任务上表现更好。最佳模型在ACL撕裂检测和乳腺结节恶性检测中达到了高准确率，与之前的研究相比具有竞争力或更优。&lt;h4&gt;翻译&lt;/h4&gt;现代计算机视觉模型已被证明对医学图像分类和分割任务非常有用，但医学图像数据的稀缺性常常限制了从头开始训练的模型的效果。迁移学习已成为解决这一问题的关键方案，使高性能模型能够在小数据集上进行微调。Mei等人(2022)发现，在放射科医生标注的大型图像数据集(RadImageNet)上预训练CNN，相比ImageNet预训练，能提高下游任务中的模型性能。本研究通过全面调查扩展了Mei等人(2022)的工作，确定了用于乳腺病变恶性和ACL撕裂检测的最佳CNN架构，并进行了统计分析，比较了RadImageNet和ImageNet预训练对下游模型性能的影响。我们的研究结果表明，具有跳跃连接的1维卷积分类器、ResNet50预训练骨干网络以及部分骨干网络解冻的方法能产生最佳的下游医学分类性能。我们的最佳模型在ACL撕裂检测中达到0.9969的AUC值，在乳腺结节恶性检测中达到0.9641的AUC值，与Mei等人(2022)报告的结果具有竞争力，并超过了其他先前的工作。我们没有发现证据证实RadImageNet预训练在ACL撕裂和乳腺病变分类任务中能提供优越的下游性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern computer vision models have proven to be highly useful for medicalimaging classification and segmentation tasks, but the scarcity of medicalimaging data often limits the efficacy of models trained from scratch. Transferlearning has emerged as a pivotal solution to this, enabling the fine-tuning ofhigh-performance models on small data. Mei et al. (2022) found thatpre-training CNNs on a large dataset of radiologist-labeled images(RadImageNet) enhanced model performance on downstream tasks compared toImageNet pretraining. The present work extends Mei et al. (2022) by conductinga comprehensive investigation to determine optimal CNN architectures for breastlesion malignancy detection and ACL tear detection, as well as performingstatistical analysis to compare the effect of RadImageNet and ImageNetpre-training on downstream model performance. Our findings suggest that1-dimensional convolutional classifiers with skip connections, ResNet50pre-trained backbones, and partial backbone unfreezing yields optimaldownstream medical classification performance. Our best models achieve AUCs of0.9969 for ACL tear detection and 0.9641 for breast nodule malignancydetection, competitive with the results reported by Mei et al. (2022) andsurpassing other previous works. We do not find evidence confirming RadImageNetpre-training to provide superior downstream performance for ACL tear and breastlesion classification tasks.</description>
      <author>example@mail.com (Daniel Frees, Moritz Bolling, Aditri Bhagirath)</author>
      <guid isPermaLink="false">2508.17567v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Generative Data Augmentation for Object Point Cloud Segmentation</title>
      <link>http://arxiv.org/abs/2505.17783v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by BMVC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于3D扩散模型的生成数据增强方法，用于点云分割任务，解决了传统数据增强方法数据多样性不足和先进生成模型缺乏语义标签的问题。&lt;h4&gt;背景&lt;/h4&gt;数据增强被广泛用于训练深度学习模型以解决数据稀缺问题，但传统数据增强方法依赖简单几何变换，导致数据多样性有限，模型性能提升有限。先进的3D形状生成模型能生成逼真点云，但缺乏点级语义标签，限制了其在点云分割任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;弥合数据增强技术与先进扩散模型之间的差距，开发能够基于给定分割掩码生成高质量点云的感知生成模型，并引入点云分割训练的生成数据增强管道。&lt;h4&gt;方法&lt;/h4&gt;将最先进的3D扩散模型Lion扩展为感知生成模型，可基于给定分割掩码生成高质量点云；引入三步生成数据增强(GDA)管道，利用少量标记样本生成变体和伪标记样本，并通过基于扩散的伪标记过滤方法验证。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模合成数据集和一个真实世界医学数据集上的实验表明，所提出的GDA方法优于传统数据增强方法以及相关的半监督和自监督方法。&lt;h4&gt;结论&lt;/h4&gt;通过将先进的3D扩散模型扩展为感知生成模型并引入三步生成数据增强管道，成功解决了传统数据增强方法的局限性，为点云分割任务提供了更有效的数据增强方法。&lt;h4&gt;翻译&lt;/h4&gt;数据增强被广泛用于训练深度学习模型以解决数据稀缺问题。然而，传统的数据增强方法通常依赖简单的几何变换，如随机旋转和重新缩放，导致数据多样性有限，模型性能提升有限。最先进的3D形状生成模型依赖于去噪扩散概率模型，能够生成逼真的新点云用于3D内容创建和操作。然而，生成的3D形状缺乏相关的点级语义标签，限制了它们在扩大点云分割任务训练数据方面的应用。为了弥合数据增强技术与先进扩散模型之间的差距，我们将最先进的3D扩散模型Lion扩展为一个感知生成模型，该模型可以基于给定的分割掩码生成高质量的点云。利用这个新颖的生成模型，我们引入了一个三步生成数据增强(GDA)管道用于点云分割训练。我们的GDA方法只需要少量标记样本，但通过生成的变体和伪标记样本来丰富训练数据，这些伪标记样本通过一种新颖的基于扩散的伪标记过滤方法进行验证。在两个大规模合成数据集和一个真实世界医学数据集上进行的大量实验表明，我们的GDA方法优于传统数据增强方法以及相关的半监督和自监督方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D点云分割任务中标注数据稀缺的问题。传统数据增强方法仅使用简单几何变换，导致数据多样性不足，模型性能提升有限。这个问题在现实中很重要，因为在医疗、自动驾驶等领域创建3D点云分割标注成本高、耗时长，而标注数据不足限制了深度学习模型的性能表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统数据增强方法的局限性，然后注意到最先进的3D生成模型能生成高质量点云但缺乏语义标签。他们借鉴了Lion扩散模型作为基础，结合半监督学习思想，通过在全局和局部编码级别整合分割信息，使模型能够理解语义部分。他们还利用了扩散模型的去噪过程，这是一种已在2D图像生成中证明有效的方法，并采用了点云处理中的PVCNN架构作为基础。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个部分感知的生成模型，能根据给定分割掩码生成高质量点云，并利用它进行数据增强。整体流程是三步GDA管道：1）半监督训练生成模型，结合标注和未标注数据；2）变体生成，通过扩散-去噪过程创建多样化的标注样本；3）伪标签生成与过滤，使用临时分割模型为未标注数据生成伪标签，并通过基于扩散的伪标签过滤方法评估质量，过滤掉不准确样本。最终将原始标注数据、生成的变体和过滤后的伪标签一起用于训练最终分割模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）部分感知的生成模型，扩展Lion模型使其能根据分割掩码生成点云；2）三步GDA管道，包括半监督训练、变体生成和伪标签过滤；3）条件重建差异(CRD)评估方法，通过计算条件重建差异判断伪标签准确性。相比之前工作，本文首次将扩散模型应用于3D点云分割的数据增强，解决了现有3D生成模型缺乏语义标签的问题，提供了比传统数据增强更丰富的数据多样性，比半监督和自监督方法提供更高质量训练数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于部分感知生成模型的三步生成数据增强方法，通过扩散模型生成多样化且带有语义标签的点云变体，并利用条件重建差异过滤伪标签，显著提升了在有限标注数据情况下的3D点云分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-05-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data augmentation is widely used to train deep learning models to addressdata scarcity. However, traditional data augmentation (TDA) typically relies onsimple geometric transformation, such as random rotation and rescaling,resulting in minimal data diversity enrichment and limited model performanceimprovement. State-of-the-art generative models for 3D shape generation rely onthe denoising diffusion probabilistic models and manage to generate realisticnovel point clouds for 3D content creation and manipulation. Nevertheless, thegenerated 3D shapes lack associated point-wise semantic labels, restrictingtheir usage in enlarging the training data for point cloud segmentation tasks.To bridge the gap between data augmentation techniques and the advanceddiffusion models, we extend the state-of-the-art 3D diffusion model, Lion, to apart-aware generative model that can generate high-quality point cloudsconditioned on given segmentation masks. Leveraging the novel generative model,we introduce a 3-step generative data augmentation (GDA) pipeline for pointcloud segmentation training. Our GDA approach requires only a small amount oflabeled samples but enriches the training data with generated variants andpseudo-labeled samples, which are validated by a novel diffusion-basedpseudo-label filtering method. Extensive experiments on two large-scalesynthetic datasets and a real-world medical dataset demonstrate that our GDAmethod outperforms TDA approach and related semi-supervised and self-supervisedmethods.</description>
      <author>example@mail.com (Dekai Zhu, Stefan Gavranovic, Flavien Boussuge, Benjamin Busam, Slobodan Ilic)</author>
      <guid isPermaLink="false">2505.17783v2</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>SoccerNet 2025 Challenges Results</title>
      <link>http://arxiv.org/abs/2508.19182v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SoccerNet 2025挑战赛是第五年度的开放基准测试活动，专注于足球视频理解中的计算机视觉研究。今年包含四个视觉任务：团队球类动作识别、单目深度估计、多视角犯规识别和比赛状态重建。&lt;h4&gt;背景&lt;/h4&gt;SoccerNet是一个持续进行的开放基准测试活动，旨在推动计算机视觉、人工智能和体育交叉领域的研究。&lt;h4&gt;目的&lt;/h4&gt;通过提供大规模标注数据集、统一评估协议和强大基线，促进足球视频理解领域的研究进展。&lt;h4&gt;方法&lt;/h4&gt;参与者需要完成四个视觉任务：1)团队球类动作识别，检测足球广播中的球相关动作并分配给各队伍；2)单目深度估计，通过单摄像头广播片段恢复场景几何；3)多视角犯规识别，分析多个同步摄像头视图以分类犯规及其严重程度；4)比赛状态重建，从广播视频中定位并识别所有球员，重建比赛在球场2D俯视图上的状态。&lt;h4&gt;主要发现&lt;/h4&gt;报告呈现了每个挑战的结果，突出了表现最佳的解决方案，并提供了社区取得进展的见解。&lt;h4&gt;结论&lt;/h4&gt;SoccerNet挑战赛继续成为推动可复现、开放研究的驱动力，促进计算机视觉、人工智能和体育领域的交叉研究。&lt;h4&gt;翻译&lt;/h4&gt;SoccerNet 2025挑战赛标志着SoccerNet开放基准测试活动的第五个年度版本，致力于推进足球视频理解中的计算机视觉研究。今年的挑战涵盖四个视觉任务：(1)团队球类动作识别，专注于检测足球广播中的球相关动作并将动作分配给各队伍；(2)单目深度估计，旨在通过单摄像头广播片段的相对深度估计来恢复场景几何；(3)多视角犯规识别，需要分析多个同步摄像头视图以分类犯规及其严重程度；(4)比赛状态重建，旨在从广播视频中定位并识别所有球员，重建球场2D俯视图上的比赛状态。在所有任务中，参与者都获得了大规模标注数据集、统一评估协议和强基线作为起点。本报告呈现了每个挑战的结果，突出了表现最佳的解决方案，并提供了社区取得进展的见解。SoccerNet挑战赛继续成为推动计算机视觉、人工智能和体育交叉领域可复现、开放研究的驱动力。有关任务、挑战和排行榜的详细信息可在https://www.soccer-net.org找到，基线和开发工具包可在https://github.com/SoccerNet获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文报告了SoccerNet 2025挑战赛的结果，旨在通过四个视觉任务推动足球视频理解的计算机视觉研究：1)团队球类动作定位，2)单目深度估计，3)多视角犯规识别，4)游戏状态重建。这些问题在现实中对体育广播增强、裁判辅助系统、球队分析和战术制定有重要价值，在研究中则促进了计算机视觉在时空推理、多模态学习和体育视频理解领域的进步，提供了一个开放、可复现的研究平台。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 这篇论文是挑战赛结果报告，而非提出新方法。SoccerNet挑战赛设计借鉴了前几年成功经验，保留了核心任务但增加了复杂度（如团队归属判断），引入了新任务（如单目深度估计），并扩展了现有基准。参赛团队方法基于现有计算机视觉技术（如预训练模型Depth Anything V2、TAdaFormer等），针对足球场景特点进行定制优化，结合领域知识和时空上下文，体现了对现有工作的继承与创新。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过四个互补任务全面评估足球视频理解进展：1)识别球类动作并确定执行团队；2)从单摄像头估计相对深度图；3)利用多视角分类犯规类型和严重程度；4)重建游戏状态（玩家位置、角色和团队）。整体流程包括：收集标注足球比赛视频；定义任务目标和评估指标；开发基线方法；团队提交解决方案；使用标准评估指标评估排名；分析最佳方法特点和整体趋势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)任务创新（增加团队归属判断、引入单目深度估计、扩展犯规分类、改进游戏状态重建）；2)数据创新（更大规模密集注释数据集、新数据模态、使用合成数据）；3)评估创新（专门评估指标、处理类别不平衡的指标、特定任务评估指标）；4)方法创新（鼓励预训练模型、领域特定优化、多模态学习应用）。相比之前工作，2025年挑战增加了任务复杂度，引入新任务领域，提供更丰富数据集，推动跨领域技术应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过SoccerNet 2025挑战赛的四个任务展示了足球视频理解的最新进展，推动了开放、可复现的体育视频分析研究，并为未来研究和应用提供了新的基准和方向。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The SoccerNet 2025 Challenges mark the fifth annual edition of the SoccerNetopen benchmarking effort, dedicated to advancing computer vision research infootball video understanding. This year's challenges span four vision-basedtasks: (1) Team Ball Action Spotting, focused on detecting ball-related actionsin football broadcasts and assigning actions to teams; (2) Monocular DepthEstimation, targeting the recovery of scene geometry from single-camerabroadcast clips through relative depth estimation for each pixel; (3)Multi-View Foul Recognition, requiring the analysis of multiple synchronizedcamera views to classify fouls and their severity; and (4) Game StateReconstruction, aimed at localizing and identifying all players from abroadcast video to reconstruct the game state on a 2D top-view of the field.Across all tasks, participants were provided with large-scale annotateddatasets, unified evaluation protocols, and strong baselines as startingpoints. This report presents the results of each challenge, highlights thetop-performing solutions, and provides insights into the progress made by thecommunity. The SoccerNet Challenges continue to serve as a driving force forreproducible, open research at the intersection of computer vision, artificialintelligence, and sports. Detailed information about the tasks, challenges, andleaderboards can be found at https://www.soccer-net.org, with baselines anddevelopment kits available at https://github.com/SoccerNet.</description>
      <author>example@mail.com (Silvio Giancola, Anthony Cioppa, Marc Gutiérrez-Pérez, Jan Held, Carlos Hinojosa, Victor Joos, Arnaud Leduc, Floriane Magera, Karen Sanchez, Vladimir Somers, Artur Xarles, Antonio Agudo, Alexandre Alahi, Olivier Barnich, Albert Clapés, Christophe De Vleeschouwer, Sergio Escalera, Bernard Ghanem, Thomas B. Moeslund, Marc Van Droogenbroeck, Tomoki Abe, Saad Alotaibi, Faisal Altawijri, Steven Araujo, Xiang Bai, Xiaoyang Bi, Jiawang Cao, Vanyi Chao, Kamil Czarnogórski, Fabian Deuser, Mingyang Du, Tianrui Feng, Patrick Frenzel, Mirco Fuchs, Jorge García, Konrad Habel, Takaya Hashiguchi, Sadao Hirose, Xinting Hu, Yewon Hwang, Ririko Inoue, Riku Itsuji, Kazuto Iwai, Hongwei Ji, Yangguang Ji, Licheng Jiao, Yuto Kageyama, Yuta Kamikawa, Yuuki Kanasugi, Hyungjung Kim, Jinwook Kim, Takuya Kurihara, Bozheng Li, Lingling Li, Xian Li, Youxing Lian, Dingkang Liang, Hongkai Lin, Jiadong Lin, Jian Liu, Liang Liu, Shuaikun Liu, Zhaohong Liu, Yi Lu, Federico Méndez, Huadong Ma, Wenping Ma, Jacek Maksymiuk, Henry Mantilla, Ismail Mathkour, Daniel Matthes, Ayaha Motomochi, Amrulloh Robbani Muhammad, Haruto Nakayama, Joohyung Oh, Yin May Oo, Marcelo Ortega, Norbert Oswald, Rintaro Otsubo, Fabian Perez, Mengshi Qi, Cristian Rey, Abel Reyes-Angulo, Oliver Rose, Hoover Rueda-Chacón, Hideo Saito, Jose Sarmiento, Kanta Sawafuji, Atom Scott, Xi Shen, Pragyan Shrestha, Jae-Young Sim, Long Sun, Yuyang Sun, Tomohiro Suzuki, Licheng Tang, Masato Tonouchi, Ikuma Uchida, Henry O. Velesaca, Tiancheng Wang, Rio Watanabe, Jay Wu, Yongliang Wu, Shunzo Yamagishi, Di Yang, Xu Yang, Yuxin Yang, Hao Ye, Xinyu Ye, Calvin Yeung, Xuanlong Yu, Chao Zhang, Dingyuan Zhang, Kexing Zhang, Zhe Zhao, Xin Zhou, Wenbo Zhu, Julian Ziegler)</author>
      <guid isPermaLink="false">2508.19182v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>HierCVAE: Hierarchical Attention-Driven Conditional Variational Autoencoders for Multi-Scale Temporal Modeling</title>
      <link>http://arxiv.org/abs/2508.18922v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出HierCVAE架构，结合分层注意机制和条件变分自编码器，用于复杂系统的时间建模，在能源消耗预测任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;复杂系统中的时间建模需要捕捉多时间尺度的依赖关系，同时处理固有的不确定性。&lt;h4&gt;目的&lt;/h4&gt;提出HierCVAE，一种结合了分层注意机制与条件变分自编码器的新架构，以应对时间建模中的挑战。&lt;h4&gt;方法&lt;/h4&gt;HierCVAE采用三层注意结构（局部、全局、跨时间）结合多模态条件编码，捕捉时间、统计和趋势信息；在潜在空间中融入ResFormer块，并通过预测头提供明确的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;在能源消耗数据集上的评估显示，HierCVAE相比最先进方法实现了15-40%的预测精度提升，并在不确定性校准方面表现优越，特别擅长长期预测和复杂的多变量依赖关系。&lt;h4&gt;结论&lt;/h4&gt;HierCVAE是一种有效的时间建模方法，能够处理多时间尺度的依赖关系并提供不确定性量化。&lt;h4&gt;翻译&lt;/h4&gt;复杂系统中的时间建模需要捕捉多时间尺度的依赖关系，同时处理固有的不确定性。我们提出了HierCVAE，一种结合分层注意机制与条件变分自编码器的新架构来应对这些挑战。HierCVAE采用三层注意结构（局部、全局、跨时间）结合多模态条件编码，以捕捉时间、统计和趋势信息。该方法在潜在空间中融入ResFormer块，并通过预测头提供明确的不确定性量化。通过在能源消耗数据集上的评估，HierCVAE相比最先进方法实现了15-40%的预测精度提升，并在不确定性校准方面表现优越，特别擅长长期预测和复杂的多变量依赖关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporal modeling in complex systems requires capturing dependencies acrossmultiple time scales while managing inherent uncertainties. We proposeHierCVAE, a novel architecture that integrates hierarchical attentionmechanisms with conditional variational autoencoders to address thesechallenges. HierCVAE employs a three-tier attention structure (local, global,cross-temporal) combined with multi-modal condition encoding to capturetemporal, statistical, and trend information. The approach incorporatesResFormer blocks in the latent space and provides explicit uncertaintyquantification via prediction heads. Through evaluations on energy consumptiondatasets, HierCVAE demonstrates a 15-40% improvement in prediction accuracy andsuperior uncertainty calibration compared to state-of-the-art methods,excelling in long-term forecasting and complex multi-variate dependencies.</description>
      <author>example@mail.com (Yao Wu)</author>
      <guid isPermaLink="false">2508.18922v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>OwlCap: Harmonizing Motion-Detail for Video Captioning via HMD-270K and Caption Set Equivalence Reward</title>
      <link>http://arxiv.org/abs/2508.18634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为OwlCap的视频描述多模态大语言模型，通过解决运动-细节不平衡问题，显著提升了视频描述的完整性和准确性。&lt;h4&gt;背景&lt;/h4&gt;视频描述旨在生成全面且连贯的视频内容描述，但现有方法常存在运动-细节不平衡问题，导致描述不完整，影响视频理解和生成的一致性。&lt;h4&gt;目的&lt;/h4&gt;解决视频描述中的运动-细节不平衡问题，提高描述的完整性和准确性，增强视频理解和生成的一致性。&lt;h4&gt;方法&lt;/h4&gt;1) 构建HMD-270K数据集，通过Motion-Detail Fusion和Fine-Grained Examination两阶段流程；2) 引入基于GRPO的CSER奖励机制，通过单元到集合匹配和双向验证增强运动和细节捕捉；3) 基于HMD-270K监督微调和GRPO后训练开发OwlCap模型。&lt;h4&gt;主要发现&lt;/h4&gt;OwlCap在VDC基准测试上准确率提升4.2，在DREAM-1K基准测试上F1分数提升4.6，显著优于基线模型。&lt;h4&gt;结论&lt;/h4&gt;OwlCap通过平衡运动和细节描述有效提高了视频描述质量，研究团队将公开数据集和模型以促进研究社区发展。&lt;h4&gt;翻译&lt;/h4&gt;视频描述旨在生成对视频内容的全面且连贯的描述，有助于视频理解和生成的进步。然而，现有方法常常受到运动-细节不平衡的困扰，因为模型倾向于过度强调一个方面而忽视另一个方面。这种不平衡导致描述不完整，进而导致视频理解和生成的一致性缺乏。为了解决这个问题，我们从两个方面提出解决方案：1）数据方面：我们通过两阶段流程（Motion-Detail Fusion和Fine-Grained Examination）构建了Harmonizing Motion-Detail 270K (HMD-270K)数据集。2）优化方面：我们引入了基于Group Relative Policy Optimization (GRPO)的Caption Set Equivalence Reward (CSER)。CSER通过单元到集合匹配和双向验证，增强了对运动和细节捕捉的完整性和准确性。基于HMD-270K监督微调和使用CSER的GRPO后训练，我们开发了OwlCap，这是一个具有运动-细节平衡功能的强大视频描述多模态大语言模型(MLLM)。实验结果表明，与基线模型相比，OwlCap在两个基准测试上取得了显著改进：注重细节的VDC（+4.2准确率）和注重运动的DREAM-1K（+4.6 F1）。HMD-270K数据集和OwlCap模型将被公开发布，以促进视频描述研究社区的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video captioning aims to generate comprehensive and coherent descriptions ofthe video content, contributing to the advancement of both video understandingand generation. However, existing methods often suffer from motion-detailimbalance, as models tend to overemphasize one aspect while neglecting theother. This imbalance results in incomplete captions, which in turn leads to alack of consistency in video understanding and generation. To address thisissue, we propose solutions from two aspects: 1) Data aspect: We constructedthe Harmonizing Motion-Detail 270K (HMD-270K) dataset through a two-stagepipeline: Motion-Detail Fusion (MDF) and Fine-Grained Examination (FGE). 2)Optimization aspect: We introduce the Caption Set Equivalence Reward (CSER)based on Group Relative Policy Optimization (GRPO). CSER enhances completenessand accuracy in capturing both motion and details through unit-to-set matchingand bidirectional validation. Based on the HMD-270K supervised fine-tuning andGRPO post-training with CSER, we developed OwlCap, a powerful video captioningmulti-modal large language model (MLLM) with motion-detail balance.Experimental results demonstrate that OwlCap achieves significant improvementscompared to baseline models on two benchmarks: the detail-focused VDC (+4.2Acc) and the motion-focused DREAM-1K (+4.6 F1). The HMD-270K dataset and OwlCapmodel will be publicly released to facilitate video captioning researchcommunity advancements.</description>
      <author>example@mail.com (Chunlin Zhong, Qiuxia Hou, Zhangjun Zhou, Shuang Hao, Haonan Lu, Yanhao Zhang, He Tang, Xiang Bai)</author>
      <guid isPermaLink="false">2508.18634v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Context-Aware Zero-Shot Anomaly Detection in Surveillance Using Contrastive and Predictive Spatiotemporal Modeling</title>
      <link>http://arxiv.org/abs/2508.18463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的上下文感知零样本异常检测框架，结合TimeSformer、DPC和CLIP模型，能够在不接触异常样本的情况下识别监控视频中的异常事件。&lt;h4&gt;背景&lt;/h4&gt;在监控视频中检测异常具有挑战性，因为异常具有不可预测性和上下文依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在训练过程中不接触异常样本的情况下识别异常事件的零样本异常检测方法。&lt;h4&gt;方法&lt;/h4&gt;提出的混合架构结合TimeSformer、DPC和CLIP，其中TimeSformer作为视觉主干提取时空特征，DPC预测未来表示识别时间偏差，CLIP-based语义流通过文本提示实现概念级异常检测。使用InfoNCE和CPC损失进行联合训练，并通过上下文门控机制增强决策能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过整合预测建模与视觉语言理解，系统能够在复杂环境中推广到未见过的行为。&lt;h4&gt;结论&lt;/h4&gt;该框架弥合了零样本异常检测中时间推理和语义上下文之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;在监控视频中检测异常本质上具有挑战性，因为它们的性质是不可预测和依赖于上下文的。这项工作引入了一种新颖的上下文感知零样本异常检测框架，该框架可以在训练过程中不接触异常样本的情况下识别异常事件。提出的混合架构结合了TimeSformer、DPC和CLIP来建模时空动态和语义上下文。TimeSformer作为视觉主干提取丰富的时空特征，而DPC预测未来表示以识别时间偏差。此外，基于CLIP的语义流通过特定上下文文本提示实现概念级异常检测。这些组件使用InfoNCE和CPC损失进行联合训练，将视觉输入与它们的时间和语义表示对齐。上下文门控机制通过使用场景感知线索或全局视频特征来调节预测，进一步增强了决策能力。通过整合预测建模与视觉语言理解，系统可以在复杂环境中推广到未见过的行为。该框架弥合了零样本异常检测中时间推理和语义上下文之间的差距。本研究的代码已在https://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting anomalies in surveillance footage is inherently challenging due totheir unpredictable and context-dependent nature. This work introduces a novelcontext-aware zero-shot anomaly detection framework that identifies abnormalevents without exposure to anomaly examples during training. The proposedhybrid architecture combines TimeSformer, DPC, and CLIP to model spatiotemporaldynamics and semantic context. TimeSformer serves as the vision backbone toextract rich spatial-temporal features, while DPC forecasts futurerepresentations to identify temporal deviations. Furthermore, a CLIP-basedsemantic stream enables concept-level anomaly detection throughcontext-specific text prompts. These components are jointly trained usingInfoNCE and CPC losses, aligning visual inputs with their temporal and semanticrepresentations. A context-gating mechanism further enhances decision-making bymodulating predictions with scene-aware cues or global video features. Byintegrating predictive modeling with vision-language understanding, the systemcan generalize to previously unseen behaviors in complex environments. Thisframework bridges the gap between temporal reasoning and semantic context inzero-shot anomaly detection for surveillance. The code for this research hasbeen made available athttps://github.com/NK-II/Context-Aware-ZeroShot-Anomaly-Detection-in-Surveillance.</description>
      <author>example@mail.com (Md. Rashid Shahriar Khan, Md. Abrar Hasan, Mohammod Tareq Aziz Justice)</author>
      <guid isPermaLink="false">2508.18463v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>FedProtoKD: Dual Knowledge Distillation with Adaptive Class-wise Prototype Margin for Heterogeneous Federated Learning</title>
      <link>http://arxiv.org/abs/2508.19009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FedProtoKD方法，通过增强的双知识蒸馏机制和对比学习解决异构联邦学习中的原型边缘收缩问题，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;异构联邦学习(HFL)因能适应不同客户端的多样化模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的方案出现，但当前方法在服务器上使用加权平均聚合原型会导致次优全局知识和原型收缩问题。&lt;h4&gt;目的&lt;/h4&gt;提出FedProtoKD方法，利用增强的双知识蒸馏机制提高异构联邦学习系统性能，解决原型边缘收缩问题，并通过评估公共样本重要性增强学习效果。&lt;h4&gt;方法&lt;/h4&gt;使用基于对比学习的可训练服务器原型，利用类别的自适应原型边缘，通过样本原型与其类别代表性原型的接近程度评估公共样本重要性，从而提升整体学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;FedProtoKD在各种设置下实现了1.13%到34.13%的平均准确率改进，显著优于现有的最先进HFL方法，特别是在模型异构且数据分布极度非IID的场景下表现出色。&lt;h4&gt;结论&lt;/h4&gt;FedProtoKD通过解决原型边缘收缩问题和改进知识聚合过程，有效提高了异构联邦学习环境中的性能，为HFL研究提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;异构联邦学习(HFL)因能够适应不同客户端的多样化模型和异构数据而受到关注。基于原型的HFL方法作为解决统计异构性和隐私挑战的有前途的解决方案出现。这种方法专注于在异构客户端之间仅共享类别代表性原型。然而，这些原型通常在服务器上使用加权平均进行聚合，导致次优的全局知识；这会导致聚合原型的收缩，当模型异构且数据分布极度非IID时，对模型性能产生负面影响。我们在异构联邦学习环境中提出FedProtoKD，使用增强的双知识蒸馏机制利用客户端的logits和原型特征表示来提高系统性能。我们旨在利用基于对比学习的可训练服务器原型和类别自适应原型边缘来解决原型边缘收缩问题。此外，我们通过样本原型与其类别代表性原型的接近程度评估公共样本的重要性，这增强了学习性能。FedProtoKD在各种设置下实现了1.13%到34.13%的平均准确率改进，并显著优于现有的最先进HFL方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous Federated Learning (HFL) has gained attention for its abilityto accommodate diverse models and heterogeneous data across clients.Prototype-based HFL methods emerge as a promising solution to addressstatistical heterogeneity and privacy challenges, paving the way for newadvancements in HFL research. This method focuses on sharing onlyclass-representative prototypes among heterogeneous clients. However, theseprototypes are often aggregated on the server using weighted averaging, leadingto sub-optimal global knowledge; these cause the shrinking of aggregatedprototypes, which negatively affects the model performance in scenarios whenmodels are heterogeneous and data distributions are extremely non-IID. Wepropose FedProtoKD in a Heterogeneous Federated Learning setting, using anenhanced dual-knowledge distillation mechanism to improve the systemperformance with clients' logits and prototype feature representation. We aimto resolve the prototype margin-shrinking problem using a contrastivelearning-based trainable server prototype by leveraging a class-wise adaptiveprototype margin. Furthermore, we assess the importance of public samples usingthe closeness of the sample's prototype to its class representative prototypes,which enhances learning performance. FedProtoKD achieved average improvementsof 1.13% up to 34.13% accuracy across various settings and significantlyoutperforms existing state-of-the-art HFL methods.</description>
      <author>example@mail.com (Md Anwar Hossen, Fatema Siddika, Wensheng Zhang, Anuj Sharma, Ali Jannesari)</author>
      <guid isPermaLink="false">2508.19009v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.18687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究揭示了当前医学视觉语言模型在面对语义等价的问题重新表述时表现出的脆弱性，并提出了一致性和对比学习方法(CCL)来提高模型的一致性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在高风险医疗应用中，对多样化问题表述保持一致的回答对可靠诊断至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决医学视觉语言模型在医学视觉问答中面对语义等价的问题重新表述时答案波动的问题，提高模型的一致性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;构建了RoMed数据集(包含144k个问题，涵盖词级、句级和语义级的扰动)，并提出了CCL方法，包括基于知识的一致性学习和偏见感知的对比学习两个关键组件。&lt;h4&gt;主要发现&lt;/h4&gt;在RoMed上评估最先进模型时观察到性能显著下降(例如，召回率下降40%)，而CCL在三个流行的VQA基准测试上实现了最先进的性能，并在RoMed测试集上将答案一致性提高了50%。&lt;h4&gt;结论&lt;/h4&gt;CCL显著提高了医学视觉语言模型的鲁棒性和一致性，代码将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;在高风险医疗应用中，对多样化问题表述保持一致的回答对可靠诊断至关重要。然而，我们发现当前的医学视觉语言模型在医学视觉问答中表现出明显的脆弱性，当面对医学问题的语义等价重新表述时，它们的答案会显著波动。我们将此归因于两个局限：(1)医学概念对齐不足，导致推理模式分歧；(2)训练数据中存在的隐藏偏见，优先考虑句法捷径而非语义理解。为解决这些挑战，我们构建了RoMed数据集，基于原始VQA数据集，包含144k个问题，涵盖词级、句级和语义级的扰动。在RoMed上评估最先进模型(如LLaVA-Med)时，我们观察到令人担忧的性能下降(例如，召回率下降40%)，暴露了关键的鲁棒性差距。为弥合这一差距，我们提出了一致性和对比学习(CCL)，它整合了两个关键组件：(1)基于知识的一致性学习，将医学视觉语言模型与医学知识而非浅层特征模式对齐；(2)偏见感知的对比学习，通过判别性表示细化减轻数据特定先验。CCL在三个流行的VQA基准测试上实现了最先进的性能，并在具有挑战性的RoMed测试集上将答案一致性提高了50%，显著提高了鲁棒性。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In high-stakes medical applications, consistent answering across diversequestion phrasings is essential for reliable diagnosis. However, we reveal thatcurrent Medical Vision-Language Models (Med-VLMs) exhibit concerning fragilityin Medical Visual Question Answering, as their answers fluctuate significantlywhen faced with semantically equivalent rephrasings of medical questions. Weattribute this to two limitations: (1) insufficient alignment of medicalconcepts, leading to divergent reasoning patterns, and (2) hidden biases intraining data that prioritize syntactic shortcuts over semantic understanding.To address these challenges, we construct RoMed, a dataset built upon originalVQA datasets containing 144k questions with variations spanning word-level,sentence-level, and semantic-level perturbations. When evaluatingstate-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarmingperformance drops (e.g., a 40\% decline in Recall) compared to original VQAbenchmarks, exposing critical robustness gaps. To bridge this gap, we proposeConsistency and Contrastive Learning (CCL), which integrates two keycomponents: (1) knowledge-anchored consistency learning, aligning Med-VLMs withmedical knowledge rather than shallow feature patterns, and (2) bias-awarecontrastive learning, mitigating data-specific priors through discriminativerepresentation refinement. CCL achieves SOTA performance on three popular VQAbenchmarks and notably improves answer consistency by 50\% on the challengingRoMed test set, demonstrating significantly enhanced robustness. Code will bereleased.</description>
      <author>example@mail.com (Songtao Jiang, Yuxi Chen, Sibo Song, Yan Zhang, Yeying Jin, Yang Feng, Jian Wu, Zuozhu Liu)</author>
      <guid isPermaLink="false">2508.18687v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>Structures Meet Semantics: Multimodal Fusion via Graph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.18322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages,7 figures,conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个名为结构-语义统一器(SSU)的新框架，用于多模态情感分析。该框架通过整合模态特定的结构信息和跨模态语义基础，有效解决了现有方法中忽视模态特定结构依赖性和语义错位的问题。SSU在多个基准数据集上实现了最先进的性能，同时提高了可解释性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;多模态情感分析旨在通过整合文本、声学和视觉模态来推断情感状态。尽管已有显著进展，但现有的多模态融合方法通常忽略了模态特定的结构依赖性和语义错位，限制了模型的质量、可解释性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够系统性地整合模态特定结构信息和跨模态语义基础的新框架，以增强多模态表示，提高情感分析的准确性、可解释性和计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出结构-语义统一器(SSU)框架，包括：1)利用语言语法为文本构建模态特定图，使用轻量级文本引导注意力机制为声学和视觉模态构建图；2)引入源自全局文本语义的语义锚点作为跨模态对齐中心；3)开发多视图对比学习目标，促进模态内和模态间视图的判别性、语义一致性和结构连贯性。&lt;h4&gt;主要发现&lt;/h4&gt;1)在CMU-MOSI和CMU-MOSEI两个基准数据集上，SSU始终实现了最先进的性能；2)与先前方法相比，SSU显著降低了计算开销；3)定性分析验证了SSU的可解释性和捕捉细微情感模式的能力。&lt;h4&gt;结论&lt;/h4&gt;结构-语义统一器(SSU)框架通过有效整合模态特定结构和跨模态语义信息，解决了现有多模态情感分析方法中的局限性，在保持高性能的同时提高了可解释性和计算效率，为多模态情感分析提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态情感分析旨在通过有效整合文本、声学和视觉模态来推断情感状态。尽管取得了显著进展，但现有的多模态融合方法通常忽略了模态特定的结构依赖性和语义错位，限制了它们的质量、可解释性和鲁棒性。为应对这些挑战，我们提出了一种名为结构-语义统一器(SSU)的新颖框架，该框架系统性地整合了模态特定的结构信息和跨模态语义基础，以增强多模态表示。具体而言，SSU利用语言语法为文本动态构建模态特定图，并为声学和视觉模态使用轻量级的文本引导注意力机制，从而捕获详细的模态内关系和语义交互。我们进一步引入了一个源自全局文本语义的语义锚点，作为跨模态对齐中心，有效调和了不同模态之间的异构语义空间。此外，我们开发了一个多视图对比学习目标，促进了模态内和模态间视图的判别性、语义一致性和结构连贯性。在两个广泛使用的基准数据集CMU-MOSI和CMU-MOSEI上的广泛评估表明，SSU始终实现了最先进的性能，同时显著降低了与先前方法相比的计算开销。全面的定性分析进一步验证了SSU的可解释性及其通过语义基础交互捕捉细微情感模式的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal sentiment analysis (MSA) aims to infer emotional states byeffectively integrating textual, acoustic, and visual modalities. Despitenotable progress, existing multimodal fusion methods often neglectmodality-specific structural dependencies and semantic misalignment, limitingtheir quality, interpretability, and robustness. To address these challenges,we propose a novel framework called the Structural-Semantic Unifier (SSU),which systematically integrates modality-specific structural information andcross-modal semantic grounding for enhanced multimodal representations.Specifically, SSU dynamically constructs modality-specific graphs by leveraginglinguistic syntax for text and a lightweight, text-guided attention mechanismfor acoustic and visual modalities, thus capturing detailed intra-modalrelationships and semantic interactions. We further introduce a semanticanchor, derived from global textual semantics, that serves as a cross-modalalignment hub, effectively harmonizing heterogeneous semantic spaces acrossmodalities. Additionally, we develop a multiview contrastive learning objectivethat promotes discriminability, semantic consistency, and structural coherenceacross intra- and inter-modal views. Extensive evaluations on two widely usedbenchmark datasets, CMU-MOSI and CMU-MOSEI, demonstrate that SSU consistentlyachieves state-of-the-art performance while significantly reducingcomputational overhead compared to prior methods. Comprehensive qualitativeanalyses further validate SSU's interpretability and its ability to capturenuanced emotional patterns through semantically grounded interactions.</description>
      <author>example@mail.com (Jiangfeng Sun, Sihao He, Zhonghong Ou, Meina Song)</author>
      <guid isPermaLink="false">2508.18322v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    <item>
      <title>scI2CL: Effectively Integrating Single-cell Multi-omics by Intra- and Inter-omics Contrastive Learning</title>
      <link>http://arxiv.org/abs/2508.18304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了scI2CL，一种基于组内和组间对比学习的单细胞多组学融合框架，能够从互补的多组学数据中学习全面且具有辨别力的细胞表示，有效支持各种下游分析任务。&lt;h4&gt;背景&lt;/h4&gt;单细胞多组学数据包含大量细胞状态信息，分析这些数据可揭示细胞异质性、疾病和生物过程的宝贵见解。然而，细胞分化与发育是连续且动态的过程，基于单细胞多组学数据计算建模和推断细胞相互作用模式仍具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的单细胞多组学融合框架，从互补的多组学数据中学习全面且具有辨别力的细胞表示，用于各种下游任务。&lt;h4&gt;方法&lt;/h4&gt;scI2CL是一种基于组内和组间对比学习的单细胞多组学融合框架，旨在学习全面且具有辨别力的细胞表示。&lt;h4&gt;主要发现&lt;/h4&gt;在细胞聚类任务中，scI2CL在四个真实数据集上超越了八种最先进方法；在细胞亚型分析中，发现了三种现有方法未能识别的单细胞亚群；是唯一能够正确构建从造血干细胞到记忆B细胞发育轨迹的方法；解决了CD4+ T细胞亚群间的细胞类型误分类问题。&lt;h4&gt;结论&lt;/h4&gt;scI2CL能够准确表征细胞间的跨组学关系，有效融合多组学数据并学习具有辨别力的细胞表示，支持各种下游分析任务。&lt;h4&gt;翻译&lt;/h4&gt;单细胞多组学数据包含大量细胞状态信息，分析这些数据可以揭示关于细胞异质性、疾病和生物过程的宝贵见解。然而，由于细胞分化与发育是一个连续且动态的过程，基于单细胞多组学数据计算建模和推断细胞相互作用模式仍然具有挑战性。本文提出了scI2CL，一种基于组内和组间对比学习的新型单细胞多组学融合框架，从互补的多组学数据中学习全面且具有辨别力的细胞表示，用于各种下游任务。四个下游任务的广泛实验验证了scI2CL的有效性及其优于现有方法的性能。具体而言，在细胞聚类中，scI2CL在四个广泛使用的真实数据集上超越了八种最先进的方法。在细胞亚型分析中，scI2CL有效区分了三种潜在的单细胞亚群，这些亚群是现有方法未能发现的。同时，scI2CL是唯一能够正确构建从造血干细胞和祖细胞到记忆B细胞的细胞发育轨迹的方法。此外，scI2CL解决了两个CD4+ T细胞亚群之间的细胞类型误分类问题，而现有方法无法精确区分这些混合细胞。总之，scI2CL能够准确表征细胞之间的跨组学关系，从而有效融合多组学数据并学习具有辨别力的细胞表示，支持各种下游分析任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-08-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell multi-omics data contain huge information of cellular states, andanalyzing these data can reveal valuable insights into cellular heterogeneity,diseases, and biological processes. However, as cell differentiation \&amp;development is a continuous and dynamic process, it remains challenging tocomputationally model and infer cell interaction patterns based on single-cellmulti-omics data. This paper presents scI2CL, a new single-cell multi-omicsfusion framework based on intra- and inter-omics contrastive learning, to learncomprehensive and discriminative cellular representations from complementarymulti-omics data for various downstream tasks. Extensive experiments of fourdownstream tasks validate the effectiveness of scI2CL and its superiority overexisting peers. Concretely, in cell clustering, scI2CL surpasses eightstate-of-the-art methods on four widely-used real-world datasets. In cellsubtyping, scI2CL effectively distinguishes three latent monocyte cellsubpopulations, which are not discovered by existing methods. Simultaneously,scI2CL is the only method that correctly constructs the cell developmentaltrajectory from hematopoietic stem and progenitor cells to Memory B cells. Inaddition, scI2CL resolves the misclassification of cell types between twosubpopulations of CD4+ T cells, while existing methods fail to preciselydistinguish the mixed cells. In summary, scI2CL can accurately characterizecross-omics relationships among cells, thus effectively fuses multi-omics dataand learns discriminative cellular representations to support variousdownstream analysis tasks.</description>
      <author>example@mail.com (Wuchao Liu, Han Peng, Wengen Li, Yichao Zhang, Jihong Guan, Shuigeng Zhou)</author>
      <guid isPermaLink="false">2508.18304v1</guid>
      <pubDate>Wed, 27 Aug 2025 15:27:17 +0800</pubDate>
    </item>
    </channel>
</rss>