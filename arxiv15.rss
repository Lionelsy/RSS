<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Thu, 06 Nov 2025 15:25:40 +0800</lastBuildDate>
    <item>
      <title>Leveraging Compact Satellite Embeddings and Graph Neural Networks for Large-Scale Poverty Mapping</title>
      <link>http://arxiv.org/abs/2511.01408v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于图的方法，利用卫星嵌入来预测撒哈拉以南非洲的集群级别财富指数，改善了贫困地图的绘制质量。&lt;h4&gt;背景&lt;/h4&gt;全球南方国家缺乏精确、细粒度的贫困地图。人口与健康调查(DHS)虽然提供高质量的社会经济数据，但空间覆盖有限，且报告的坐标被随机位移以保护隐私，进一步降低了数据质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于图的方法，利用低维AlphaEarth卫星嵌入来预测撒哈拉以南非洲的集群级别财富指数。&lt;h4&gt;方法&lt;/h4&gt;通过建模已调查位置和未标记位置之间的空间关系，并引入概率'模糊标签'损失函数来解释坐标位移，改进了财富预测的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;在37个DHS数据集(2017-2023)上的实验表明，与'仅图像'基线相比，结合图结构略微提高了准确性，展示了紧凑的地球观测嵌入在大规模社会经济绘图中的潜力。&lt;h4&gt;结论&lt;/h4&gt;基于图的方法结合卫星嵌入可以改善贫困地图的绘制，特别是在处理数据稀疏和坐标位移问题方面。&lt;h4&gt;翻译&lt;/h4&gt;精确的、细粒度的贫困地图在大多数全球南方国家仍然稀缺。虽然人口与健康调查(DHS)提供高质量的社会经济数据，但它们的空间覆盖有限，且报告的坐标被随机位移以保护隐私，进一步降低了数据质量。我们提出了一种基于图的方法，利用低维AlphaEarth卫星嵌入来预测撒哈拉以南非洲的集群级别财富指数。通过建模已调查和未标记位置之间的空间关系，并通过引入概率'模糊标签'损失函数来解释坐标位移，我们改进了财富预测在现有调查之外的泛化能力。我们在37个DHS数据集(2017-2023)上的实验表明，与'仅图像'基线相比，结合图结构略微提高了准确性，展示了紧凑的地球观测嵌入在大规模社会经济绘图中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate, fine-grained poverty maps remain scarce across much of the GlobalSouth. While Demographic and Health Surveys (DHS) provide high-qualitysocioeconomic data, their spatial coverage is limited and reported coordinatesare randomly displaced for privacy, further reducing their quality. We proposea graph-based approach leveraging low-dimensional AlphaEarth satelliteembeddings to predict cluster-level wealth indices across Sub-Saharan Africa.By modeling spatial relations between surveyed and unlabeled locations, and byintroducing a probabilistic "fuzzy label" loss to account for coordinatedisplacement, we improve the generalization of wealth predictions beyondexisting surveys. Our experiments on 37 DHS datasets (2017-2023) show thatincorporating graph structure slightly improves accuracy compared to"image-only" baselines, demonstrating the potential of compact EO embeddingsfor large-scale socioeconomic mapping.</description>
      <author>example@mail.com (Markus B. Pettersson, Adel Daoud)</author>
      <guid isPermaLink="false">2511.01408v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
  <item>
      <title>Machine Learning for RNA Secondary Structure Prediction: a review of current methods and challenges</title>
      <link>http://arxiv.org/abs/2511.02622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述讨论了RNA二级结构预测领域从热力学方法到数据驱动方法的演变，包括单序列模型、基于进化的模型和混合模型，以及该领域面临的'泛化危机'和RNA基础模型的兴起。同时，文章展望了未来需要解决的主要挑战，包括复杂基序的准确预测、扩展到千碱基长度转录本、整合修饰核苷酸的化学多样性，以及从静态结构转向动态集合。&lt;h4&gt;背景&lt;/h4&gt;RNA二级结构预测是计算生物学的核心挑战，对于理解分子功能和设计新型治疗药物至关重要。该领域已经从基础但准确度有限的热力学方法演变为由机器学习和深度学习主导的新数据驱动范式。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在概述现代RNA二级结构预测方法，评估当前进展，并展望未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;现代方法包括单序列模型、基于进化的模型以及将机器学习与生物物理学相结合的混合模型。此外，为应对数据稀缺问题，RNA基础模型应运而生，它们从大量未标记的序列语料库中学习以提高泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;领域面临'泛化危机'，强大的模型在新的RNA家族上表现不佳，这促使整个社区转向更严格、同源感知的基准测试。RNA基础模型通过从未标记的大规模序列数据中学习，提高了泛化能力。&lt;h4&gt;结论&lt;/h4&gt;未来需要解决的主要挑战包括准确预测假结等复杂基序、扩展到千碱基长度转录本、整合修饰核苷酸的化学多样性，以及将预测目标从静态结构转向更好地捕捉生物功能的动态集合。同时，需要标准化的前瞻性基准测试系统，以确保无偏验证并加速进展。&lt;h4&gt;翻译&lt;/h4&gt;预测RNA的二级结构是计算生物学的核心挑战，对于理解分子功能和设计新型治疗药物至关重要。该领域已经从基础但准确度有限的热力学方法演变为由机器学习和深度学习主导的新数据驱动范式。这些模型直接从数据中学习折叠模式，带来了显著的性能提升。这篇综述概述了这些现代方法的现状，涵盖单序列模型、基于进化的模型以及将机器学习与生物物理学相结合的混合模型。一个中心主题是该领域的'泛化危机'，强大的模型被发现在新RNA家族上表现不佳，促使整个社区转向更严格、同源感知的基准测试。为应对潜在的数据稀缺挑战，RNA基础模型已经出现，它们从大量未标记的序列语料库中学习以提高泛化能力。最后，我们展望下一组主要障碍——包括准确预测假结等复杂基序、扩展到千碱基长度转录本、整合修饰核苷酸的化学多样性，以及将预测目标从静态结构转向更好地捕捉生物功能的动态集合。我们还强调了需要标准化的前瞻性基准测试系统，以确保无偏验证并加速进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting the secondary structure of RNA is a core challenge incomputational biology, essential for understanding molecular function anddesigning novel therapeutics. The field has evolved from foundational butaccuracy-limited thermodynamic approaches to a new data-driven paradigmdominated by machine learning and deep learning. These models learn foldingpatterns directly from data, leading to significant performance gains. Thisreview surveys the modern landscape of these methods, covering single-sequence,evolutionary-based, and hybrid models that blend machine learning withbiophysics. A central theme is the field's "generalization crisis," wherepowerful models were found to fail on new RNA families, prompting acommunity-wide shift to stricter, homology-aware benchmarking. In response tothe underlying challenge of data scarcity, RNA foundation models have emerged,learning from massive, unlabeled sequence corpora to improve generalization.Finally, we look ahead to the next set of major hurdles-including the accurateprediction of complex motifs like pseudoknots, scaling to kilobase-lengthtranscripts, incorporating the chemical diversity of modified nucleotides, andshifting the prediction target from static structures to the dynamic ensemblesthat better capture biological function. We also highlight the need for astandardized, prospective benchmarking system to ensure unbiased validation andaccelerate progress.</description>
      <author>example@mail.com (Giuseppe Sacco, Giovanni Bussi, Guido Sanguinetti)</author>
      <guid isPermaLink="false">2511.02622v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Adapting General-Purpose Foundation Models for X-ray Ptychography in Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2511.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了PtychoBench基准，用于比较高级显微镜工作流程自动化中两种模型专业化策略（监督微调SFT和上下文学习ICL）的效果，发现最优策略取决于任务类型，为科学AI系统开发提供了框架。&lt;h4&gt;背景&lt;/h4&gt;高级显微镜工作流程自动化是关键目标，基础模型如LLMs和VLMs显示巨大潜力，但将这些通用模型适应专业科学任务的最佳策略尚不明确。&lt;h4&gt;目的&lt;/h4&gt;通过引入PtychoBench基准，系统比较SFT和ICL两种专业化策略在显微镜分析任务上的效果，确定最优专业化路径。&lt;h4&gt;方法&lt;/h4&gt;创建多模态、多任务PtychoBench基准，使用VLMs进行视觉伪影检测任务，使用LLMs进行文本参数推荐任务，评估SFT和ICL策略在数据稀缺环境下的表现。&lt;h4&gt;主要发现&lt;/h4&gt;最优专业化路径取决于任务类型；视觉任务中SFT和ICL互补效果最佳；文本任务中在大基础模型上使用ICL更优；上下文感知提示具有优越性；微调模型中存在上下文干扰现象。&lt;h4&gt;结论&lt;/h4&gt;科学AI系统的最优专业化路径取决于任务模态，为开发更有效的科学智能系统提供了明确框架。&lt;h4&gt;翻译&lt;/h4&gt;高级显微镜工作流程的自动化是一个关键目标，其中像语言模型（LLMs）和视觉-语言模型（VLMs）这样的基础模型显示出巨大潜力。然而，将这些通用模型适应专业科学任务是至关重要的，最佳领域适应策略通常不明确。为此，我们引入了PtychoBench，一个用于衍射分析的新多模态、多任务基准。使用此基准，我们系统性地比较了两种专业化策略：监督微调（SFT）和上下文学习（ICL）。我们在数据稀缺环境下，使用VLMs评估视觉伪影检测任务，使用LLMs评估文本参数推荐任务。我们的研究结果表明，最优的专业化路径取决于任务类型。对于视觉任务，SFT和ICL高度互补，由上下文感知示例引导的微调模型实现了最高的平均性能（Micro-F1为0.728）。相反，对于文本任务，在大基础模型上使用ICL是更优策略，达到峰值Micro-F1为0.847，优于强大的'超级专家'SFT模型（0-shot Micro-F1为0.839）。我们还确认了上下文感知提示的优越性，并在微调模型中发现了持续的上下文干扰现象。这些结果与包括GPT-4o和基于DINOv3的分类器在内的强大基线进行了基准测试，为科学AI提供了关键观察：在我们的基准中，最优专业化路径取决于任务模态，为开发更有效的科学智能系统提供了明确的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The automation of workflows in advanced microscopy is a key goal wherefoundation models like Language Models (LLMs) and Vision-Language Models (VLMs)show great potential. However, adapting these general-purpose models forspecialized scientific tasks is critical, and the optimal domain adaptationstrategy is often unclear. To address this, we introduce PtychoBench, a newmulti-modal, multi-task benchmark for ptychographic analysis. Using thisbenchmark, we systematically compare two specialization strategies: SupervisedFine-Tuning (SFT) and In-Context Learning (ICL). We evaluate these strategieson a visual artifact detection task with VLMs and a textual parameterrecommendation task with LLMs in a data-scarce regime. Our findings reveal thatthe optimal specialization pathway is task-dependent. For the visual task, SFTand ICL are highly complementary, with a fine-tuned model guided bycontext-aware examples achieving the highest mean performance (Micro-F1 of0.728). Conversely, for the textual task, ICL on a large base model is thesuperior strategy, reaching a peak Micro-F1 of 0.847 and outperforming apowerful "super-expert" SFT model (0-shot Micro-F1 of 0.839). We also confirmthe superiority of context-aware prompting and identify a consistent contextualinterference phenomenon in fine-tuned models. These results, benchmarkedagainst strong baselines including GPT-4o and a DINOv3-based classifier, offerkey observations for AI in science: the optimal specialization path in ourbenchmark is dependent on the task modality, offering a clear framework fordeveloping more effective science-based agentic systems.</description>
      <author>example@mail.com (Robinson Umeike, Neil Getty, Yin Xiangyu, Yi Jiang)</author>
      <guid isPermaLink="false">2511.02503v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Model order reduction via Lie groups</title>
      <link>http://arxiv.org/abs/2511.03520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MORLie的新型基于李群的模型降阶框架，该框架能够将流形上的高维动力系统近似为李群上的低维动力系统，特别适用于处理实际应用中常见的非等变动力学问题。&lt;h4&gt;背景&lt;/h4&gt;李群及其作用在物理系统的描述中无处不在，模型降阶(MOR)是处理高维系统的重要方法。&lt;h4&gt;目的&lt;/h4&gt;探索李群在模型降阶中的影响，并提出一种新的基于李群的模型降阶框架。&lt;h4&gt;方法&lt;/h4&gt;提出MORLie框架，在流形上高维动力系统与李群上低维动力系统之间建立近似关系，提供基于几何公式的新非侵入式MOR方法，能够处理非等变动力学。&lt;h4&gt;主要发现&lt;/h4&gt;MORLie的误差边界低于Kolmogorov N-宽度，限制了线性子空间方法；在三个应用案例中表现优异：1)变形体建模中优于POD方法；2)肝脏运动重建接近最先进水平且训练时间大幅减少；3)解析例证显示方法的通用性。&lt;h4&gt;结论&lt;/h4&gt;MORLi是一种有效的模型降阶方法，能够处理非等变动力学，在多个实际应用中展现出色性能，包括变形体建模和医学图像处理等。&lt;h4&gt;翻译&lt;/h4&gt;李群及其作用在物理系统的描述中无处不在，我们探索了在模型降阶(MOR)设置中的影响。我们提出了一个名为MORLie的基于李群的新型模型降阶框架，其中流形上的高维动力系统被李群上的低维动力系统近似。与其他李群方法相比，我们能够处理实际应用中常见的非等变动力学，并基于提出的几何公式提供了新的非侵入式MOR方法。我们还通过数值计算强调，MORLie的误差边界低于限制线性子空间方法的Kolmogorov N-宽度。该方法应用于各种示例：1. 对遵循剪切运动的噪声点云数据建模的变形体简化模型，其中MORLie在准确性和降维方面优于简单的POD方法；2. 通过超声扫描边缘检测数据重建呼吸期间的肝脏运动，MORLi的性能接近最先进水平，同时将训练时间从计算集群上的几小时减少到移动工作站上的几分钟；3. 一个解析例子，显示冻结方法作为特例被解析恢复，表明了几何框架的通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lie groups and their actions are ubiquitous in the description of physicalsystems, and we explore implications in the setting of model order reduction(MOR). We present a novel framework of MOR via Lie groups, called MORLie, inwhich high-dimensional dynamical systems on manifolds are approximated bylow-dimensional dynamical systems on Lie groups. In comparison to other Liegroup methods we are able to attack non-equivariant dynamics, which arefrequent in practical applications, and we provide new non-intrusive MORmethods based on the presented geometric formulation. We also highlightnumerically that MORLie has a lower error bound than the Kolmogorov $N$-width,which limits linear-subspace methods. The method is applied to variousexamples: 1. MOR of a simplified deforming body modeled by a noisy point clouddata following a sheering motion, where MORLie outperforms a naive POD approachin terms of accuracy and dimensionality reduction. 2. Reconstructing livermotion during respiration with data from edge detection in ultrasound scans,where MORLie reaches performance approaching the state of the art, whilereducing the training time from hours on a computing cluster to minutes on amobile workstation. 3. An analytic example showing that the method of freezingis analytically recovered as a special case, showing the generality of thegeometric framework.</description>
      <author>example@mail.com (Yannik P. Wotte, Patrick Buchfink, Silke Glas, Federico Califano, Stefano Stramigioli)</author>
      <guid isPermaLink="false">2511.03520v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>IEC3D-AD: A 3D Dataset of Industrial Equipment Components for Unsupervised Point Cloud Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.03267v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一个针对真实工业场景的点云异常检测数据集(IEC3D-AD)和新的3D异常检测范式(GMANet)，解决了现有数据集无法捕捉真实工业环境复杂性的问题。&lt;h4&gt;背景&lt;/h4&gt;3D异常检测在工业制造中至关重要，特别是对核心设备组件的可靠性和安全。现有数据集如Real3D-AD和MVTec3D-AD无法捕捉真实工业环境中的复杂性和细微缺陷，限制了工业设备组件(如轴承、环和螺栓)的精确异常检测研究。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对真实工业场景的点云异常检测数据集，直接从实际生产线收集，确保高保真度和相关性，以支持更严格的异常检测任务。&lt;h4&gt;方法&lt;/h4&gt;创建IEC3D-AD数据集，具有改进的点云分辨率和缺陷注释粒度；引入GMANet范式，基于几何形态分析生成合成点云样本，通过空间差异优化减少正常和异常点级特征之间的边界并增加重叠。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，所提出的方法在IEC3D-AD和其他数据集上均表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;新开发的数据集和方法有效解决了工业3D异常检测中的挑战，提高了异常检测的精确度。&lt;h4&gt;翻译&lt;/h4&gt;三维异常检测在工业制造中发挥着关键作用，特别是在确保核心设备组件的可靠性和安全性方面。尽管现有的三维数据集如Real3D-AD和MVTec3D-AD提供了广泛的应用支持，但它们无法捕捉真实工业环境中存在的复杂性和细微缺陷。这一限制阻碍了精确异常检测研究，特别是对于轴承、环和螺栓等工业设备组件。为了应对这一挑战，我们开发了一个针对真实工业场景的点云异常检测数据集(IEC3D-AD)。该数据集直接从实际生产线收集，确保了高保真度和相关性。与现有数据集相比，IEC3D-AD具有显著改进的点云分辨率和缺陷注释粒度，支持更严格的异常检测任务。此外，受生成式二维异常检测方法的启发，我们在IEC3D-AD上引入了一种新的三维异常检测范式(GMANet)。该范式基于几何形态分析生成合成点云样本，然后通过空间差异优化减少正常和异常点级特征之间的边界并增加重叠。大量实验证明了我们的方法在IEC3D-AD和其他数据集上的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决现有3D异常检测数据集缺乏真实工业场景样本的问题，以及无法同时平衡空间覆盖率和数据密度的局限性。这个问题在现实中非常重要，因为工业设备组件是工业基础和产业链现代化的重要连接点，确保其可靠性和安全对工业生产至关重要；在研究中，现有数据集多来自模具和玩具而非真实工业环境，导致算法在实际应用中性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别到现有数据集缺乏真实工业场景数据，因此构建了专门针对真实工业场景的数据集IEC3D-AD。方法设计上，作者受生成式2D异常检测方法的启发，引入了基于几何形态分析的合成点云生成(SPCG)模块，借鉴了教师-学生网络的思想使用专家域和学徒域两个编码器，并参考了焦点损失来设计权重优化特征分布差异。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过几何形态分析生成合成异常样本，然后利用空间差异优化减少正常和异常特征边界，增加特征重叠。整体流程分为训练和测试两个阶段：训练阶段使用正常样本，通过SPCG生成合成异常样本，双编码器提取特征，计算差异并优化；测试阶段输入真实样本，使用训练好的编码器提取特征并计算异常分数识别异常点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了IEC3D-AD数据集，直接从工业生产线收集，实现360度全覆盖和高点云密度；2)提出了GMANet方法，包含SPCG和SDO两个创新模块；3)提供了全面的基准测试。相比之前工作，不同之处在于：数据来源更真实(工业生产线vs模具玩具)，同时实现了高覆盖率和密度，缺陷比例更低(0.78%-2.28%)，且包含功能拓扑结构和微观几何畸变等真实工业特征。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一个专门针对工业设备组件的高质量3D点云异常检测数据集IEC3D-AD，以及一种基于几何形态分析和空间差异优化的无监督异常检测方法GMANet，显著提升了在真实工业场景中检测微小缺陷的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D anomaly detection (3D-AD) plays a critical role in industrialmanufacturing, particularly in ensuring the reliability and safety of coreequipment components. Although existing 3D datasets like Real3D-AD and MVTec3D-AD offer broad application support, they fall short in capturing thecomplexities and subtle defects found in real industrial environments. Thislimitation hampers precise anomaly detection research, especially forindustrial equipment components (IEC) such as bearings, rings, and bolts. Toaddress this challenge, we have developed a point cloud anomaly detectiondataset (IEC3D-AD) specific to real industrial scenarios. This dataset isdirectly collected from actual production lines, ensuring high fidelity andrelevance. Compared to existing datasets, IEC3D-AD features significantlyimproved point cloud resolution and defect annotation granularity, facilitatingmore demanding anomaly detection tasks. Furthermore, inspired by generative2D-AD methods, we introduce a novel 3D-AD paradigm (GMANet) on IEC3D-AD. Thisparadigm generates synthetic point cloud samples based on geometricmorphological analysis, then reduces the margin and increases the overlapbetween normal and abnormal point-level features through spatial discrepancyoptimization. Extensive experiments demonstrate the effectiveness of our methodon both IEC3D-AD and other datasets.</description>
      <author>example@mail.com (Bingyang Guo, Hongjie Li, Ruiyun Yu, Hanzhe Liang, Jinbao Wang)</author>
      <guid isPermaLink="false">2511.03267v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Scheduling the Off-Diagonal Weingarten Loss of Neural SDFs for CAD Models</title>
      <link>http://arxiv.org/abs/2511.03147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Lecture Notes in Computer Science (LNCS), 20th International  Symposium on Visual Computing 2025, 12 pages, 4 figures, preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进神经符号距离函数(SDFs)在CAD表面重建中的方法，通过引入时变调度策略优化非对角魏恩加滕(ODW)损失，显著提升了重建质量。&lt;h4&gt;背景&lt;/h4&gt;神经符号距离函数已成为从点云进行几何重建的强大表示方法，但通常需要基于梯度和曲率的正则化来抑制伪影并保持结构保真度。FlatCAD虽然引入了高效的ODW损失作为二阶先验，但使用固定权重存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发ODW损失的调度策略，在高初始权重稳定优化的同时，逐渐衰减权重以允许后期细尺度细节恢复，从而提升CAD重建质量。&lt;h4&gt;方法&lt;/h4&gt;研究并实现了多种调度策略，包括常数、线性、五次和步长插值调度，以及增加的预热变体，并在ABC CAD数据集上进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;时变调度策略显著优于固定权重方法，与FlatCAD基线相比，在Chamfer距离上实现了高达35%的性能提升。&lt;h4&gt;结论&lt;/h4&gt;将调度作为曲率正则化的简单而有效的扩展，能够显著提升神经符号距离函数在CAD重建中的鲁棒性和质量。&lt;h4&gt;翻译&lt;/h4&gt;神经符号距离函数(SDFs)已成为从点云进行几何重建的强大表示方法，但它们通常需要基于梯度和曲率的正则化来抑制伪影并保持结构保真度。FlatCAD引入了非对角魏恩加滕(ODW)损失作为CAD表面的高效二阶先验，以大约一半的计算成本近似完整Hessian正则化。然而，FlatCAD在整个训练过程中应用固定的ODW权重，这是次优的：强正则化稳定了早期优化，但在后期阶段抑制了细节恢复。我们提出了ODW损失的调度策略，分配高初始权重以稳定优化，并逐渐衰减它以允许细尺度细化。我们研究了常数、线性、五次和步长插值调度，以及一个增加的预热变体。在ABC CAD数据集上的实验表明，时变调度始终优于固定权重。我们的方法在Chamfer距离上比FlatCAD基线实现了高达35%的改进，确立了调度作为曲率正则化的简单而有效的扩展，用于鲁棒的CAD重建。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的问题是神经符号距离函数在重建CAD模型时使用固定的Off-Diagonal Weingarten损失权重导致的问题。固定权重在训练早期能稳定优化但会抑制后期细节恢复。这个问题重要是因为CAD模型通常由简单几何形状组成，需要平衡结构稳定性和细节精度，而固定权重无法满足训练不同阶段的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到固定权重无法平衡训练早期的稳定性和后期的细节恢复，因此借鉴了课程学习和多任务学习中的动态权重调整思想。他们设计了多种调度策略，包括常量、线性、五次多项式和阶跃插值，并进行了系统比较。他们还受到Neuralangelo的粗到细策略启发，但将其应用于曲率正则化领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是采用'强开始-衰减'策略：训练初期使用高权重ODW损失稳定优化并抑制伪影，随着训练进行逐渐降低权重，允许网络恢复几何细节。实现流程是：定义一组关键点指定训练进度和对应权重；根据当前训练进度确定所在区间；在区间内根据选择的调度策略计算当前权重；使用动态权重更新总损失函数进行网络优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次系统研究ODW损失调度策略；提出多种调度方法；引入'强开始-衰减'训练范式；对比衰减和预热策略。相比之前工作，本文解决了固定权重无法适应训练不同阶段需求的问题，首次系统研究了ODW权重调度，证明了动态权重比固定权重效果更好，最多提升35%的Chamfer距离。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种针对神经符号距离函数中Off-Diagonal Weingarten损失的动态权重调度策略，通过初期高权重稳定优化、后期降低权重允许细节恢复，显著提高了CAD模型的重建质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural signed distance functions (SDFs) have become a powerful representationfor geometric reconstruction from point clouds, yet they often require bothgradient- and curvature-based regularization to suppress spurious warp andpreserve structural fidelity. FlatCAD introduced the Off-Diagonal Weingarten(ODW) loss as an efficient second-order prior for CAD surfaces, approximatingfull-Hessian regularization at roughly half the computational cost. However,FlatCAD applies a fixed ODW weight throughout training, which is suboptimal:strong regularization stabilizes early optimization but suppresses detailrecovery in later stages. We present scheduling strategies for the ODW lossthat assign a high initial weight to stabilize optimization and progressivelydecay it to permit fine-scale refinement. We investigate constant, linear,quintic, and step interpolation schedules, as well as an increasing warm-upvariant. Experiments on the ABC CAD dataset demonstrate that time-varyingschedules consistently outperform fixed weights. Our method achieves up to a35% improvement in Chamfer Distance over the FlatCAD baseline, establishingscheduling as a simple yet effective extension of curvature regularization forrobust CAD reconstruction.</description>
      <author>example@mail.com (Haotian Yin, Przemyslaw Musialski)</author>
      <guid isPermaLink="false">2511.03147v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>DentalSplat: Dental Occlusion Novel View Synthesis from Sparse Intra-Oral Photographs</title>
      <link>http://arxiv.org/abs/2511.03099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DentalSplat的框架，用于从稀疏正畸图像进行3D重建，解决了传统3D高斯溅射技术在正畸远程医疗应用中的局限性。&lt;h4&gt;背景&lt;/h4&gt;在正畸治疗特别是远程医疗背景下，从多角度观察患者咬合情况有助于及时临床决策。传统3D高斯溅射技术需要密集多视角输入和精确相机姿态，但正畸病例通常只有三张稀疏图像（正面和双侧颊视图），使重建极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效框架，能够从稀疏正畸图像进行高质量3D重建，支持远程正畸诊断。&lt;h4&gt;方法&lt;/h4&gt;DentalSplat框架利用先验引导的密集立体重建模型初始化点云，采用尺度自适应剪枝策略提高3DGS训练效率和重建质量，在极度稀疏视角情况下结合光流作为几何约束和梯度正则化来提高渲染保真度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能有效处理稀疏输入场景，在咬合可视化方面实现卓越的新视角合成质量，优于现有最先进技术。&lt;h4&gt;结论&lt;/h4&gt;DentalSplat成功解决了正畸治疗中从稀疏图像进行3D重建的挑战，为远程正畸诊断提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;在正畸治疗中，特别是在远程医疗背景下，从多角度观察患者的咬合情况有助于及时的临床决策。最近的3D高斯溅射（3DGS）技术在3D重建和新视角合成方面显示出强大潜力。然而，传统的3DGS流程通常依赖于密集捕获的多视角输入和精确初始化的相机姿态，限制了其实用性。正畸病例通常只有三张稀疏图像，即正面视图和双侧颊视图，使重建任务特别具有挑战性。输入视图的极度稀疏会严重降低重建质量，而相机姿态信息的缺失进一步复杂化了这一过程。为了克服这些限制，我们提出了DentalSplat，一个从稀疏正畸图像进行3D重建的有效框架。我们的方法利用先验引导的密集立体重建模型初始化点云，随后采用尺度自适应剪枝策略提高3DGS的训练效率和重建质量。在极度稀疏视角的情况下，我们进一步结合光流作为几何约束，并结合梯度正则化来提高渲染保真度。我们在一个包含950个临床病例的大型数据集上验证了我们的方法，以及一个额外的基于视频的测试集，包含195个病例，用于模拟现实世界远程正畸成像条件。实验结果表明，我们的方法能有效处理稀疏输入场景，并在咬合可视化方面实现卓越的新视角合成质量，优于最先进的技术。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从稀疏口腔内照片中合成新视角以观察牙齿咬合的问题。在正畸治疗中，特别是远程医疗场景下，这有助于及时临床决策。传统方法如CBCT和IOS需要专业设备，限制远程使用；而现有AI系统主要依赖单视图图像，无法全面评估咬合关系。此问题对提高远程正畸治疗效果、降低专业设备依赖具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统3DGS需要密集多视角输入和精确相机姿态；其他方法如MVSplat、Nope-NeRF假设有重叠视图，不适合真正稀疏场景；DUSt3R虽解决稀疏输入问题但在正畸应用中面临设备差异、牙齿反射、运动模糊等挑战。作者借鉴了3DGS和DUSt3R技术，并针对正畸场景特点设计了专门改进：尺度自适应修剪策略处理密集点云，集成光学流约束和梯度正则化提高渲染质量。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合DUSt3R和3DGS优势，解决正畸场景中稀疏输入和未知相机姿态挑战。流程包括：1)用DUSt3R生成初始点云和相机姿态；2)应用尺度自适应修剪策略处理点云；3)在3DGS优化中结合光学流约束确保几何一致性；4)使用梯度约束增强密集化过程；5)通过联合优化相机姿态和3D高斯原语实现高质量渲染。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)尺度自适应修剪(SAP)策略，减少点云大小同时保持质量；2)增强的差分高斯光栅化模块，集成光学流约束和梯度权重计算；3)专门针对正畸场景的优化，处理反射、模糊等问题。不同之处：专为三张稀疏图像设计，而其他方法假设更多输入；解决DUSt3R密集点云导致的计算效率问题；结合多种约束提高复杂牙齿结构渲染质量；无需相机姿态信息也能工作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DentalSplat首次实现了从稀疏、无姿态的口腔内图像中快速（一分钟内）生成高质量牙齿咬合3D重建和新视角合成的方法，显著优于现有技术，为远程正畸治疗提供了新的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In orthodontic treatment, particularly within telemedicine contexts,observing patients' dental occlusion from multiple viewpoints facilitatestimely clinical decision-making. Recent advances in 3D Gaussian Splatting(3DGS) have shown strong potential in 3D reconstruction and novel viewsynthesis. However, conventional 3DGS pipelines typically rely on denselycaptured multi-view inputs and precisely initialized camera poses, limitingtheir practicality. Orthodontic cases, in contrast, often comprise only threesparse images, specifically, the anterior view and bilateral buccal views,rendering the reconstruction task especially challenging. The extreme sparsityof input views severely degrades reconstruction quality, while the absence ofcamera pose information further complicates the process. To overcome theselimitations, we propose DentalSplat, an effective framework for 3Dreconstruction from sparse orthodontic imagery. Our method leverages aprior-guided dense stereo reconstruction model to initialize the point cloud,followed by a scale-adaptive pruning strategy to improve the trainingefficiency and reconstruction quality of 3DGS. In scenarios with extremelysparse viewpoints, we further incorporate optical flow as a geometricconstraint, coupled with gradient regularization, to enhance renderingfidelity. We validate our approach on a large-scale dataset comprising 950clinical cases and an additional video-based test set of 195 cases designed tosimulate real-world remote orthodontic imaging conditions. Experimental resultsdemonstrate that our method effectively handles sparse input scenarios andachieves superior novel view synthesis quality for dental occlusionvisualization, outperforming state-of-the-art techniques.</description>
      <author>example@mail.com (Yiyi Miao, Taoyu Wu, Tong Chen, Sihao Li, Ji Jiang, Youpeng Yang, Angelos Stefanidis, Limin Yu, Jionglong Su)</author>
      <guid isPermaLink="false">2511.03099v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>From Propagation to Prediction: Point-level Uncertainty Evaluation of MLS Point Clouds under Limited Ground Truth</title>
      <link>http://arxiv.org/abs/2511.03053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于学习的MLS点云不确定性评估框架，结合最优邻域估计和几何特征提取，实验证明其可行且高效。&lt;h4&gt;背景&lt;/h4&gt;移动激光扫描点云在许多高精度应用（如扫描到建筑信息模型、变形分析和三维建模）中的可靠使用依赖于不确定性评估。然而，在许多实际应用中，获取用于评估的地面真实值通常成本高昂且不可行。&lt;h4&gt;目的&lt;/h4&gt;减少不确定性评估研究中对地面真实值的长期依赖，提出一种基于学习的框架用于MLS点云的不确定性评估。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于学习的框架，结合最优邻域估计和几何特征提取，使用XGBoost模型进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架可行；XGBoost模型与随机森林相比具有完全相当的准确性，同时效率更高（快约3倍）；几何特征可用于预测由点到点距离量化的点级不确定性。&lt;h4&gt;结论&lt;/h4&gt;MLS点云的不确定性是可以学习的，为不确定性评估研究提供了新的基于学习的视角。&lt;h4&gt;翻译&lt;/h4&gt;评估不确定性对于移动激光扫描点云在许多高精度应用（如扫描到建筑信息模型、变形分析和三维建模）中的可靠使用至关重要。然而，在许多实际应用中，获取用于评估的地面真实值通常成本高昂且不可行。为了减少不确定性评估研究中对地面真实值的长期依赖，本研究提出了一个MLS点云的基于学习框架，结合了最优邻域估计和几何特征提取。在真实世界数据集上的实验表明，所提出的框架是可行的，XGBoost模型与随机森林相比具有完全相当的准确性，同时实现了更高的效率（快约3倍），初步证明了几何特征可用于预测由点到点距离量化的点级不确定性。总之，这项研究表明MLS点云的不确定性是可以学习的，为不确定性评估研究提供了新的基于学习的视角。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决MLS点云不确定性评估中过度依赖地面真实值(GT)的问题。这个问题很重要，因为在高精度应用如Scan-to-BIM、变形分析和3D建模中，不仅需要准确几何信息，还需要可靠的不确定性估计；而传统方法要么难以全面建模所有误差源，要么获取GT成本过高，限制了MLS技术的广泛应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统前向和后向建模方法的局限性，特别是后向建模对GT的依赖问题。然后转向学习-based方法，将不确定性评估转化为监督学习问题，学习点的误差与局部几何特征间的关系。方法设计借鉴了TLS测量中使用集成方法的研究成果，采用了点云分类中表现良好的几何特征提取方法，以及最优邻域估计策略，但将其首次应用于真实世界的MLS数据集。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是点级不确定性可以通过局部几何特征学习和预测，无需为每个新场景都获取GT。流程包括：1)数据准备(使用C2C距离量化不确定性，筛选C2C&lt;80mm的点)；2)特征工程(估计最优邻域，提取26种几何特征)；3)模型训练(Random Forest和XGBoost两种集成学习模型)；4)模型测试(使用多种评估指标和可视化)；5)结果分析(比较模型性能，进行特征重要性分析)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个针对MLS点云的学习-based不确定性评估框架；2)集成最优邻域估计与几何特征提取；3)证明XGBoost在保持与RF相当精度的同时效率更高；4)通过SHAP和排列重要性分析揭示关键几何特征。相比之前工作，本文从TLS扩展到复杂MLS场景，从实验室条件扩展到真实世界，使用C2C距离提供更敏感的局部误差表征，并减少了GT需求，提高了实用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文首次提出了一种基于机器学习的MLS点云点级不确定性评估框架，通过几何特征预测不确定性，减少了对地面真实值的依赖，同时证明了XGBoost模型在保持与随机森林相当精度的同时具有更高的计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluating uncertainty is critical for reliable use of Mobile Laser Scanning(MLS) point clouds in many high-precision applications such as Scan-to-BIM,deformation analysis, and 3D modeling. However, obtaining the ground truth (GT)for evaluation is often costly and infeasible in many real-world applications.To reduce this long-standing reliance on GT in uncertainty evaluation research,this study presents a learning-based framework for MLS point clouds thatintegrates optimal neighborhood estimation with geometric feature extraction.Experiments on a real-world dataset show that the proposed framework isfeasible and the XGBoost model delivers fully comparable accuracy to RandomForest while achieving substantially higher efficiency (about 3 times faster),providing initial evidence that geometric features can be used to predictpoint-level uncertainty quantified by the C2C distance. In summary, this studyshows that MLS point clouds' uncertainty is learnable, offering a novellearning-based viewpoint towards uncertainty evaluation research.</description>
      <author>example@mail.com (Ziyang Xu, Olaf Wysocki, Christoph Holst)</author>
      <guid isPermaLink="false">2511.03053v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Curvature of high-dimensional data</title>
      <link>http://arxiv.org/abs/2511.02873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了从带噪声的样本数据中估计曲率的问题，探讨了高维情况下曲率估计的偏差问题，并提出了一种改进的概率框架来构建更准确的曲率估计器。&lt;h4&gt;背景&lt;/h4&gt;对于维度大于一的流形，存在多种局部曲率的定义，每种定义对应不同的估计过程。最近的研究证明了'局部点云曲率'估计会随着点云密度趋近于无限而收敛到相关的局部曲率光滑概念。&lt;h4&gt;目的&lt;/h4&gt;研究收敛定理的实际局限性，分析曲率估计中偏差的显著影响，特别是在高维情况下，并提出构建更准确曲率估计器的方法。&lt;h4&gt;方法&lt;/h4&gt;提出一个概率框架，为任意噪声模型构建更准确的曲率估计器，并在高达十二维的球体上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;偏差在高维情况下会急剧增加，在高维情况下，朴素曲率估计落在真实曲率附近小区间内的概率可能接近于零；提出的概率框架能够构建更准确的曲率估计器。&lt;h4&gt;结论&lt;/h4&gt;在高维流形中，曲率估计面临显著的偏差挑战，但通过提出的概率框架可以有效提高估计的准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑估计曲率的问题，其中数据可以被视为来自基础流形的噪声样本。对于维度大于一的流形，存在多种局部曲率的定义，每种定义对给定数据集提出了不同的估计过程。最近，在证明'局部点云曲率'估计随着点云密度趋近于无限而收敛到相关的局部曲率光滑概念方面取得了进展。在此，我们研究了这些收敛定理的实际局限性，并讨论了最近文献中报道的此类估计中偏差的显著影响。我们提供了理论论证，证明偏差在高维情况下急剧增加，以至于在高维情况下，朴素曲率估计落在真实曲率附近小区间内的概率可能接近于零。我们提出了一个概率框架，能够为任意噪声模型构建更准确的曲率估计器。我们在高达十二维的球体上的实验支持了我们技术的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of estimating curvature where the data can be viewedas a noisy sample from an underlying manifold. For manifolds of dimensiongreater than one there are multiple definitions of local curvature, eachsuggesting a different estimation process for a given data set. Recently, therehas been progress in proving that estimates of ``local point cloud curvature"converge to the related smooth notion of local curvature as the density of thepoint cloud approaches infinity. Herein we investigate practical limitations ofsuch convergence theorems and discuss the significant impact of bias in suchestimates as reported in recent literature. We provide theoretical argumentsfor the fact that bias increases drastically in higher dimensions, so much sothat in high dimensions, the probability that a naive curvature estimate liesin a small interval near the true curvature could be near zero. We present aprobabilistic framework that enables the construction of more accurateestimators of curvature for arbitrary noise models. The efficacy of ourtechnique is supported with experiments on spheres of dimension as large astwelve.</description>
      <author>example@mail.com (Jiayi Chen, Mohammad Javad Latifi Jebelli, Daniel N. Rockmore)</author>
      <guid isPermaLink="false">2511.02873v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>POEMS: Product of Experts for Interpretable Multi-omic Integration using Sparse Decoding</title>
      <link>http://arxiv.org/abs/2511.03464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POEMS是一种新的无监督概率框架，用于整合多组学数据，通过稀疏解码和专家乘积模型，在保持预测性能的同时提供可解释性，实现了生物标志物发现和跨组学关联。&lt;h4&gt;背景&lt;/h4&gt;整合不同分子层（即多组学数据）对于理解疾病复杂性至关重要；然而，大多数深度生成模型要么优先考虑预测性能而牺牲可解释性，要么通过线性化解码器来强制可解释性，从而削弱了网络的非线性表达能力。&lt;h4&gt;目的&lt;/h4&gt;克服预测性能和可解释性之间的权衡，开发一种能够同时保持预测性能并提供可解释性的无监督概率框架。&lt;h4&gt;方法&lt;/h4&gt;引入POEMS框架，通过以下方式提供可解释性而不需要线性化网络的任何部分：1)使用稀疏连接将特征映射到潜在因子，直接转化为生物标志物发现；2)使用专家乘积模型通过共享潜在空间实现跨组学关联；3)通过门控网络报告每个组学的贡献，该网络自适应地计算它们在表示学习中的影响；此外，还提出了一种高效的稀疏解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在癌症亚型分型案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了一套新的解释方法，证明基于生物标志物的洞察力和预测准确性可以在多组学表示学习中共存。&lt;h4&gt;结论&lt;/h4&gt;POEMS框架成功整合了预测性能和可解释性，证明在多组学表示学习中，生物标志物洞察力和预测准确性可以共存。&lt;h4&gt;翻译&lt;/h4&gt;整合不同的分子层，即多组学数据，对于揭示疾病复杂性至关重要；然而，大多数深度生成模型要么优先考虑预测性能而牺牲可解释性，要么通过线性化解码器来强制可解释性，从而削弱了网络的非线性表达能力。为了克服这种权衡，我们引入了POEMS：使用稀疏解码的可解释多组学集成的专家乘积模型，这是一个无监督概率框架，在提供可解释性的同时保持预测性能。POEMS通过以下方式在不线性化网络任何部分的情况下提供可解释性：1)使用稀疏连接将特征映射到潜在因子，直接转化为生物标志物发现；2)通过专家乘积模型使用共享潜在空间实现跨组学关联；3)通过门控网络报告每个组学的贡献，该网络自适应地计算它们在表示学习中的影响。此外，我们还提出了一种高效的稀疏解码器。在癌症亚型分型案例研究中，POEMS实现了具有竞争力的聚类和分类性能，同时提供了一套新的解释方法，证明了基于生物标志物的洞察力和预测准确性可以在多组学表示学习中共存。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating different molecular layers, i.e., multiomics data, is crucial forunraveling the complexity of diseases; yet, most deep generative models eitherprioritize predictive performance at the expense of interpretability or enforceinterpretability by linearizing the decoder, thereby weakening the network'snonlinear expressiveness. To overcome this tradeoff, we introduce POEMS:Product Of Experts for Interpretable Multiomics Integration using SparseDecoding, an unsupervised probabilistic framework that preserves predictiveperformance while providing interpretability. POEMS provides interpretabilitywithout linearizing any part of the network by 1) mapping features to latentfactors using sparse connections, which directly translates to biomarkerdiscovery, 2) allowing for cross-omic associations through a shared latentspace using product of experts model, and 3) reporting contributions of eachomic by a gating network that adaptively computes their influence in therepresentation learning. Additionally, we present an efficient sparse decoder.In a cancer subtyping case study, POEMS achieves competitive clustering andclassification performance while offering our novel set of interpretations,demonstrating that biomarker based insight and predictive accuracy can coexistin multiomics representation learning.</description>
      <author>example@mail.com (Mihriban Kocak Balik, Pekka Marttinen, Negar Safinianaini)</author>
      <guid isPermaLink="false">2511.03464v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>ProM3E: Probabilistic Masked MultiModal Embedding Model for Ecology</title>
      <link>http://arxiv.org/abs/2511.02946v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究介绍了一个名为ProM3E的概率掩码多模态嵌入模型，用于生态学中的多模态表示生成。该模型基于嵌入空间中的掩码模态重建，支持模态反转，能够分析模态融合的可行性，并提出了新的跨模态检索方法，展示了优越的表示学习能力。&lt;h4&gt;背景&lt;/h4&gt;生态学研究需要处理多种模态的数据，但现有方法可能无法有效处理任意模态间的转换和融合。需要一种能够处理多模态数据并支持任意模态间转换的模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够支持生态学多模态表示任意生成的模型，学习推断缺失的模态，分析模态融合的可行性，并提高跨模态检索性能。&lt;h4&gt;方法&lt;/h4&gt;提出ProM3E模型，基于嵌入空间中的掩码模态重建；支持模态反转功能；利用概率性质分析模态融合的可行性；提出结合模态间和模态内相似性的跨模态检索方法；使用隐藏表示进行线性探测任务。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的跨模态检索方法在所有检索任务中实现了优越的性能；模型展示了卓越的表示学习能力；能够有效分析不同模态融合的可行性。&lt;h4&gt;结论&lt;/h4&gt;ProM3E模型为生态学多模态数据的处理提供了有效解决方案，支持任意模态间的转换和融合，并在跨模态检索和表示学习任务中表现出色。研究团队将公开代码、数据集和模型以促进进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了ProM3E，一个用于生态学多模态表示的任意生成概率掩码多模态嵌入模型。ProM3E基于嵌入空间中的掩码模态重建，学习在给定少量上下文模态的情况下推断缺失的模态。根据设计，我们的模型支持嵌入空间中的模态反转。我们模型的概率性质使我们能够分析融合各种模态以用于给定下游任务的可行性，本质上学习融合什么。利用我们模型的这些特性，我们提出了一种新的跨模态检索方法，该方法结合了模态间和模态内相似性，以在所有检索任务中实现卓越的性能。我们进一步利用我们模型的隐藏表示来执行线性探测任务，并展示了我们模型卓越的表示学习能力。我们所有的代码、数据集和模型将在https://vishu26.github.io/prom3e上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ProM3E, a probabilistic masked multimodal embedding model forany-to-any generation of multimodal representations for ecology. ProM3E isbased on masked modality reconstruction in the embedding space, learning toinfer missing modalities given a few context modalities. By design, our modelsupports modality inversion in the embedding space. The probabilistic nature ofour model allows us to analyse the feasibility of fusing various modalities forgiven downstream tasks, essentially learning what to fuse. Using these featuresof our model, we propose a novel cross-modal retrieval approach that mixesinter-modal and intra-modal similarities to achieve superior performance acrossall retrieval tasks. We further leverage the hidden representation from ourmodel to perform linear probing tasks and demonstrate the superiorrepresentation learning capability of our model. All our code, datasets andmodel will be released at https://vishu26.github.io/prom3e.</description>
      <author>example@mail.com (Srikumar Sastry, Subash Khanal, Aayush Dhakal, Jiayu Lin, Dan Cher, Phoenix Jarosz, Nathan Jacobs)</author>
      <guid isPermaLink="false">2511.02946v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>XR-1: Towards Versatile Vision-Language-Action Models via Learning Unified Vision-Motion Representations</title>
      <link>http://arxiv.org/abs/2511.02776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了XR-1框架，通过统一视觉运动编码(UVMC)和三阶段训练范式解决了视觉语言动作模型面临的两个基本挑战：从高维观测中产生精确低级动作和弥合跨异构数据源的领域差距。&lt;h4&gt;背景&lt;/h4&gt;大规模机器人数据集和视觉语言模型的发展推动了VLA模型研究，但现有模型面临两个挑战：从高维观测产生精确低级动作，以及弥合不同机器人形态和人类演示数据之间的领域差距。现有方法未能充分利用大规模异构数据集中的互补多模态知识。&lt;h4&gt;目的&lt;/h4&gt;开发一个多功能可扩展的VLA学习框架(XR-1)，能够在多样化机器人、任务和环境上有效工作。&lt;h4&gt;方法&lt;/h4&gt;引入统一视觉运动编码(UVMC)，一种通过双分支VQ-VAE学习的离散潜在表示，同时编码视觉动力学和机器人运动。采用三阶段训练范式：自监督的UVMC学习、UVMC引导的大规模跨形态机器人数据集预训练、以及任务特定的后训练。&lt;h4&gt;主要发现&lt;/h4&gt;在六种不同机器人形态上进行了超过14,000次滚动的真实世界实验，涵盖120多种操作任务。XR-1持续优于π₀.₅、π₀、RDT、UniVLA和GR00T-N1.5等基线方法，并对新物体、背景变化、干扰物和光照变化展现出强大泛化能力。&lt;h4&gt;结论&lt;/h4&gt;XR-1通过UVMC和三阶段训练范式成功解决了VLA模型的关键挑战，在多样化场景中实现了优越的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;最近大规模机器人数据集和视觉语言模型(VLMs)的进展推动了视觉语言动作(VLA)模型的研究。然而，现有VLA模型仍面临两个基本挑战：(i)从高维观测中产生精确的低级动作，(ii)弥合跨异构数据源的领域差距，包括多样化的机器人形态和人类演示。现有方法通常从视觉动力学或机器人动作中编码潜在变量来指导策略学习，但它们未能充分利用大规模、异构数据集中存在的互补多模态知识。在这项工作中，我们提出了X机器人模型1(XR-1)，一个适用于多样化机器人、任务和环境的多功能可扩展VLA学习框架。XR-1引入了统一视觉运动编码(UVMC)，一种通过双分支VQ-VAE学习的离散潜在表示，可同时编码视觉动力学和机器人运动。UVMC通过(i)作为观测和动作之间的中间表示，和(ii)对齐来自异构数据源的多模态动态信息以捕获互补知识来解决这些挑战。为了有效利用UVMC，我们提出了三阶段训练范式：(i)自监督的UVMC学习，(ii)在大型跨形态机器人数据集上进行UVMC引导的预训练，和(iii)任务特定的后训练。我们通过在六种不同机器人形态上进行超过14,000次滚动的广泛真实世界实验验证了XR-1，涵盖120多种不同的操作任务。XR-1在性能上持续优于最先进的基线方法，如π₀.₅、π₀、RDT、UniVLA和GR00T-N1.5，同时展现出对新物体、背景变化、干扰物和光照变化的强大泛化能力。我们的项目网址是https://xr-1-vla.github.io/。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决两个问题：1) 从高维观测生成精确的低级行动困难；2) 跨形态数据集利用受到形态异质性阻碍。这些问题在现实中很重要，因为精确的低级行动对机器人执行实际任务至关重要，特别是在需要精确操作的场景；而有效利用跨形态数据集可以提高机器人学习的数据效率和泛化能力，推动机器人向更通用、适应性强的方向发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有方法要么只编码视觉动态，要么只编码机器人动作，没有充分利用多模态知识。受人类认知启发——人类自然将异构感官输入整合成跨模态代码，作者设计了双分支VQ-VAE架构，将视觉动态和机器人运动编码到共享的离散潜在空间中，并添加了对齐损失强制视觉代码与运动代码保持一致。该方法借鉴了VQ-VAE架构、大规模预训练数据集和视觉-语言模型，但创新性地将它们整合用于统一的视觉-运动表示学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过学习统一的视觉-运动表示(UVMC)实现跨模态对齐，作为观测和动作之间的中间表示。整体实现流程分为三阶段：1) 第一阶段学习UVMC，使用双分支VQ-VAE分别编码视觉动态和机器人运动到共享潜在空间，并添加跨模态对齐损失；2) 第二阶段UVMC指导预训练，将UVMC作为监督信号注入VLM；3) 第三阶段任务特定后训练，微调VLA策略提高特定任务性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的视觉-运动表示(UVMC)同时编码视觉和运动信息；2) 三阶段训练范式增加UVMC学习阶段；3) 跨模态对齐损失强制视觉与运动代码一致；4) 模型无关设计可灵活应用于不同VLA架构。相比之前工作，XR-1不仅利用视觉信息还整合机器人运动信息，不需要大量标记的机器人动作数据，能从人类演示视频中学习，并通过增加UVMC学习阶段更有效利用异构数据源。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; XR-1通过学习统一的视觉-运动表示，实现了跨数据利用、跨模态对齐和跨形态控制的通用视觉-语言-动作模型，显著提高了机器人在多样化任务和环境中的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent progress in large-scale robotic datasets and vision-language models(VLMs) has advanced research on vision-language-action (VLA) models. However,existing VLA models still face two fundamental challenges: (i) producingprecise low-level actions from high-dimensional observations, (ii) bridgingdomain gaps across heterogeneous data sources, including diverse robotembodiments and human demonstrations. Existing methods often encode latentvariables from either visual dynamics or robotic actions to guide policylearning, but they fail to fully exploit the complementary multi-modalknowledge present in large-scale, heterogeneous datasets. In this work, wepresent X Robotic Model 1 (XR-1), a novel framework for versatile and scalableVLA learning across diverse robots, tasks, and environments. XR-1 introducesthe \emph{Unified Vision-Motion Codes (UVMC)}, a discrete latent representationlearned via a dual-branch VQ-VAE that jointly encodes visual dynamics androbotic motion. UVMC addresses these challenges by (i) serving as anintermediate representation between the observations and actions, and (ii)aligning multimodal dynamic information from heterogeneous data sources tocapture complementary knowledge. To effectively exploit UVMC, we propose athree-stage training paradigm: (i) self-supervised UVMC learning, (ii)UVMC-guided pretraining on large-scale cross-embodiment robotic datasets, and(iii) task-specific post-training. We validate XR-1 through extensivereal-world experiments with more than 14,000 rollouts on six different robotembodiments, spanning over 120 diverse manipulation tasks. XR-1 consistentlyoutperforms state-of-the-art baselines such as $\pi_{0.5}$, $\pi_0$, RDT,UniVLA, and GR00T-N1.5 while demonstrating strong generalization to novelobjects, background variations, distractors, and illumination changes. Ourproject is at https://xr-1-vla.github.io/.</description>
      <author>example@mail.com (Shichao Fan, Kun Wu, Zhengping Che, Xinhua Wang, Di Wu, Fei Liao, Ning Liu, Yixue Zhang, Zhen Zhao, Zhiyuan Xu, Meng Li, Qingjie Liu, Shanghang Zhang, Min Wan, Jian Tang)</author>
      <guid isPermaLink="false">2511.02776v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>DANIEL: A Distributed and Scalable Approach for Global Representation Learning with EHR Applications</title>
      <link>http://arxiv.org/abs/2511.02754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究重新审视了Ising模型，开发了一个分布式框架，使具有内在低秩结构的大规模二元数据的可扩展和隐私保护表示学习成为可能。通过双因子梯度下降优化非凸替代损失函数，在多机构电子健康记录数据集上验证了算法的优越性，突显了在高维环境中进行统计推断的潜力。&lt;h4&gt;背景&lt;/h4&gt;传统的概率图模型在现代数据环境中面临基本挑战，这些数据环境具有高维度、源异构性和严格的数据共享限制。&lt;h4&gt;目的&lt;/h4&gt;重新审视Ising模型，并开发一个分布式框架，使具有内在低秩结构的大规模二元数据的可扩展和隐私保护表示学习成为可能。&lt;h4&gt;方法&lt;/h4&gt;通过双因子梯度下降优化非凸替代损失函数，与传统的凸方法相比，提供了显著的计算和通信优势。&lt;h4&gt;主要发现&lt;/h4&gt;在匹兹堡大学医学中心和马萨诸塞州总医院的58,248名患者的多机构电子健康记录数据集上评估了算法，在全局表示学习和下游临床任务（包括关系检测、患者表型和患者聚类）中表现出优越性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了在联邦、高维环境中进行统计推断的更广泛潜力，同时解决了数据复杂性和多机构整合的实际挑战。&lt;h4&gt;翻译&lt;/h4&gt;传统的概率图模型在现代数据环境中面临基本挑战，这些数据环境具有高维度、源异构性和严格的数据共享限制。在本工作中，我们重新审视了Ising模型，它是马尔可夫随机场家族中一个成熟的成员，并开发了一个分布式框架，使具有内在低秩结构的大规模二元数据的可扩展和隐私保护表示学习成为可能。我们的方法通过双因子梯度下降优化非凸替代损失函数，与传统的凸方法相比，提供了显著的计算和通信优势。我们在匹兹堡大学医学中心和马萨诸塞州总医院的58,248名患者的多机构电子健康记录数据集上评估了我们的算法，在全局表示学习和下游临床任务（包括关系检测、患者表型和患者聚类）中表现出优越性能。这些结果突显了在联邦、高维环境中进行统计推断的更广泛潜力，同时解决了数据复杂性和多机构整合的实际挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classical probabilistic graphical models face fundamental challenges inmodern data environments, which are characterized by high dimensionality,source heterogeneity, and stringent data-sharing constraints. In this work, werevisit the Ising model, a well-established member of the Markov Random Field(MRF) family, and develop a distributed framework that enables scalable andprivacy-preserving representation learning from large-scale binary data withinherent low-rank structure. Our approach optimizes a non-convex surrogate lossfunction via bi-factored gradient descent, offering substantial computationaland communication advantages over conventional convex approaches. We evaluateour algorithm on multi-institutional electronic health record (EHR) datasetsfrom 58,248 patients across the University of Pittsburgh Medical Center (UPMC)and Mass General Brigham (MGB), demonstrating superior performance in globalrepresentation learning and downstream clinical tasks, including relationshipdetection, patient phenotyping, and patient clustering. These results highlighta broader potential for statistical inference in federated, high-dimensionalsettings while addressing the practical challenges of data complexity andmulti-institutional integration.</description>
      <author>example@mail.com (Zebin Wang, Ziming Gan, Weijing Tang, Zongqi Xia, Tianrun Cai, Tianxi Cai, Junwei Lu)</author>
      <guid isPermaLink="false">2511.02754v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Modality-Transition Representation Learning for Visible-Infrared Person Re-Identification</title>
      <link>http://arxiv.org/abs/2511.02685v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为模态转换表示学习（MTRL）的新型VI-ReID框架，通过中间生成的图像作为模态转换的桥梁，有效对齐可见光和红外模态的特征，无需额外参数即可提升性能。&lt;h4&gt;背景&lt;/h4&gt;可见光-红外行人重识别（VI-ReID）技术能够在背景光照变化场景中关联不同模态的行人图像，但可见光和红外模态间存在本质差距。现有方法主要依赖中间表示来对齐跨模态特征，但这些方法要么通过生成中间图像（数据增强），要么融合中间特征（参数多，可解释性差），且未能充分利用中间特征。&lt;h4&gt;目的&lt;/h4&gt;解决可见光和红外模态间的差距问题，改进现有的VI-ReID方法，使其能够更有效地对齐跨模态特征，同时保持推理速度不增加额外参数。&lt;h4&gt;方法&lt;/h4&gt;提出模态转换表示学习（MTRL）框架，使用中间生成的图像作为从可见光到红外模态的传输器，这些图像与原始可见光图像完全对齐且与红外模态相似。采用模态转换对比损失和模态查询正则化损失进行训练，实现更有效的跨模态特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的MTRL框架不需要额外参数，保持与骨干网络相同的推理速度，同时在VI-ReID任务上性能得到提升。在三个典型VI-ReID数据集上的实验结果表明，该方法显著且一致地优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;通过模态转换表示学习框架，可以有效解决可见光和红外模态间的差距问题，提升VI-ReID性能，且不增加计算负担，为跨模态行人重识别提供了新的有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可见光-红外行人重识别（VI-ReID）技术能够在实际场景中关联可见光和红外模态的行人图像，特别是在背景光照变化的情况下。然而，这两种模态之间本质上存在显著差距。此外，现有方法主要依靠中间表示来对齐同一人的跨模态特征。这些中间特征表示通常是通过生成中间图像（一种数据增强）或融合中间特征（参数更多，可解释性差）来创建的，并且它们没有很好地利用中间特征。因此，我们提出了一种通过模态转换表示学习（MTRL）的新型VI-ReID框架，使用中间生成的图像作为从可见光到红外模态的传输器，这些图像与原始可见光图像完全对齐，并且与红外模态相似。之后，使用模态转换对比损失和模态查询正则化损失进行训练，可以更有效地对齐跨模态特征。值得注意的是，我们提出的框架不需要任何额外参数，在保持与骨干网络相同推理速度的同时，提高了其在VI-ReID任务上的性能。大量实验结果表明，在三个典型的VI-ReID数据集上，我们的模型显著且一致地优于现有的最先进方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visible-infrared person re-identification (VI-ReID) technique could associatethe pedestrian images across visible and infrared modalities in the practicalscenarios of background illumination changes. However, a substantial gapinherently exists between these two modalities. Besides, existing methodsprimarily rely on intermediate representations to align cross-modal features ofthe same person. The intermediate feature representations are usually create bygenerating intermediate images (kind of data enhancement), or fusingintermediate features (more parameters, lack of interpretability), and they donot make good use of the intermediate features. Thus, we propose a novelVI-ReID framework via Modality-Transition Representation Learning (MTRL) with amiddle generated image as a transmitter from visible to infrared modals, whichare fully aligned with the original visible images and similar to the infraredmodality. After that, using a modality-transition contrastive loss and amodality-query regularization loss for training, which could align thecross-modal features more effectively. Notably, our proposed framework does notneed any additional parameters, which achieves the same inference speed to thebackbone while improving its performance on VI-ReID task. Extensiveexperimental results illustrate that our model significantly and consistentlyoutperforms existing SOTAs on three typical VI-ReID datasets.</description>
      <author>example@mail.com (Chao Yuan, Zanwu Liu, Guiwei Zhang, Haoxuan Xu, Yujian Zhao, Guanglin Niu, Bo Li)</author>
      <guid isPermaLink="false">2511.02685v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Using Deep Learning for Robust Classification of Fast Radio Bursts</title>
      <link>http://arxiv.org/abs/2511.02634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures, 9 tables. Comments are welcome&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用深度学习方法对快速射电暴进行分类并分析其潜在结构模式，通过监督变分自编码器模型实现了高分类准确率，并揭示了重复发射器与非重复发射器之间的特征差异。&lt;h4&gt;背景&lt;/h4&gt;快速射电暴的本质仍然未知，但群体层面的分析可以阐明这些信号中的潜在结构。研究使用了第一个CHIME目录中的数据。&lt;h4&gt;目的&lt;/h4&gt;使用深度学习方法对FRBs进行分类，并分析从CHIME目录中学习的潜在空间中的结构模式。&lt;h4&gt;方法&lt;/h4&gt;采用监督变分自编码器(sVAE)架构，结合变分自编码器的表示学习能力和监督分类任务，构建学习到的潜在空间并进行进一步降维以寻找数据中的潜在结构。&lt;h4&gt;主要发现&lt;/h4&gt;sVAE模型对FRB重复发射器实现了高分类准确率，揭示了重复发射器和非重复发射器群体之间的分离。色散测量过剩、光谱指数和光谱运行是区分重复发射器和非重复发射器的主导特征。研究还确定了四个非重复FRBs作为重复发射器候选者，其中两个在先前研究中已被独立标记。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法可以有效地对FRBs进行分类并揭示其潜在结构，重复发射器和非重复发射器之间存在可区分的特征差异。&lt;h4&gt;翻译&lt;/h4&gt;尽管快速射电暴的性质仍然未知，但群体层面的分析可以阐明这些信号中的潜在结构。在本研究中，我们采用深度学习方法来对FRBs进行分类，并分析从第一个CHIME目录中学习的潜在空间中的结构模式。我们采用监督变分自编码器架构，该架构结合了变分自编码器的表示学习能力和监督分类任务，从而提高了分类性能和潜在空间的可解释性。我们在构建的潜在空间中执行进一步的降维，以寻找数据中的潜在结构。我们的结果表明，sVAE模型对FRB重复发射器实现了高分类准确率，并揭示了重复发射器与非重复发射器群体之间的分离。通过对潜在空间的进一步分析，我们观察到色散测量过剩、光谱指数和光谱运行是区分重复发射器与非重复发射器的主导特征。我们还确定了四个非重复FRBs作为重复发射器候选者，其中两个在先前研究中已被独立标记。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While the nature of fast radio bursts (FRBs) remains unknown,population-level analyses can elucidate underlying structure in these signals.In this study, we employ deep learning methods to both classify FRBs andanalyze structural patterns in the latent space learned from the first CHIMEcatalog. We adopt a Supervised Variational Autoencoder (sVAE) architecturewhich combines the representational learning capabilities of VariationalAutoencoders (VAEs) with a supervised classification task, thereby improvingboth classification performance and the interpretability of the latent space.We construct a learned latent space in which we perform further dimensionalityreduction to find underlying structure in the data. Our results demonstratethat the sVAE model achieves high classification accuracy for FRB repeaters andreveals separation between repeater and non-repeater populations. Upon furtheranalysis of the latent space, we observe that dispersion measure excess,spectral index, and spectral running are the dominant features distinguishingrepeaters from non-repeaters. We also identify four non-repeating FRBs asrepeater candidates, two of which have been independently flagged in previousstudies.</description>
      <author>example@mail.com (Rohan Arni, Carlos Blanco, Anirudh Prabhu)</author>
      <guid isPermaLink="false">2511.02634v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SKGE: Spherical Knowledge Graph Embedding with Geometric Regularization</title>
      <link>http://arxiv.org/abs/2511.02460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了球面知识图谱嵌入(SKGE)模型，通过将实体表示限制在超球面上，克服了传统欧几里得空间KGE模型的局限性。实验证明SKGE在多个基准测试上优于TransE模型，特别是在大规模数据集上表现出色。研究表明几何约束作为正则化器可以提高性能，且球面几何创造了更好的负采样环境。&lt;h4&gt;背景&lt;/h4&gt;知识图谱嵌入(KGE)已成为多关系数据表示学习的基本技术。许多经典模型(如TransE)在无界欧几里得空间中操作，这种方法在建模复杂关系时存在固有局限性，可能导致训练效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型来挑战现有范式，通过将实体表示限制在紧凑流形(超球面)上，解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了球面知识图谱嵌入(SKGE)模型，使用可学习的非线性球化层将实体映射到球面上，并将关系解释为混合平移-投影变换。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集(FB15k-237, CoDEx-S, CoDEx-M)上的广泛实验表明，SKGE一致且显著优于TransE模型，特别是在大型基准测试上表现更佳。球面几何先验的有效性得到验证，几何约束作为一种强大的正则化器，导致所有关系类型的性能全面提升。球面几何创造了'内在困难负采样'环境，自然消除平凡负样本，迫使模型学习更强大和语义一致的表示。&lt;h4&gt;结论&lt;/h4&gt;流形的选择不仅仅是实现细节，而是基本设计原则。倡议将几何先验作为设计下一代强大且稳定的KGE模型的基础。&lt;h4&gt;翻译&lt;/h4&gt;知识图谱嵌入(KGE)已成为多关系数据表示学习的基本技术。许多开创性模型，如TransE，在无界欧几里得空间中运行，这在建模复杂关系时存在固有局限性，并可能导致训练效率低下。在本文中，我们提出了球面知识图谱嵌入(SKGE)模型，通过将实体表示限制在紧凑流形(超球面)上，挑战了这一范式。SKGE使用可学习的非线性球化层将实体映射到球面上，并将关系解释为混合平移-投影变换。通过对三个基准数据集FB15k-237、CoDEx-S和CoDEx-M的广泛实验，我们证明SKGE一致且显著地优于其强大的欧几里得对应模型TransE，特别是在FB15k-237和CoDEx-M等大规模基准测试上，证明了球面几何先验的有效性。我们提供了深入分析以揭示这种优势的来源，表明这种几何约束作为一种强大的正则化器，导致所有关系类型的全面性能提升。更根本的是，我们证明了球面几何创造了'内在困难负采样'环境，自然消除平凡负样本，迫使模型学习更强大和语义一致的表示。我们的发现有力地证明了流形的选择不仅仅是实现细节，而是基本设计原则，倡导将几何先验作为设计下一代强大且稳定的KGE模型的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge graph embedding (KGE) has become a fundamental technique forrepresentation learning on multi-relational data. Many seminal models, such asTransE, operate in an unbounded Euclidean space, which presents inherentlimitations in modeling complex relations and can lead to inefficient training.In this paper, we propose Spherical Knowledge Graph Embedding (SKGE), a modelthat challenges this paradigm by constraining entity representations to acompact manifold: a hypersphere. SKGE employs a learnable, non-linearSpherization Layer to map entities onto the sphere and interprets relations asa hybrid translate-then-project transformation. Through extensive experimentson three benchmark datasets, FB15k-237, CoDEx-S, and CoDEx-M, we demonstratethat SKGE consistently and significantly outperforms its strong Euclideancounterpart, TransE, particularly on large-scale benchmarks such as FB15k-237and CoDEx-M, demonstrating the efficacy of the spherical geometric prior. Weprovide an in-depth analysis to reveal the sources of this advantage, showingthat this geometric constraint acts as a powerful regularizer, leading tocomprehensive performance gains across all relation types. More fundamentally,we prove that the spherical geometry creates an "inherently hard negativesampling" environment, naturally eliminating trivial negatives and forcing themodel to learn more robust and semantically coherent representations. Ourfindings compellingly demonstrate that the choice of manifold is not merely animplementation detail but a fundamental design principle, advocating forgeometric priors as a cornerstone for designing the next generation of powerfuland stable KGE models.</description>
      <author>example@mail.com (Xuan-Truong Quan, Xuan-Son Quan, Duc Do Minh, Vinh Nguyen Van)</author>
      <guid isPermaLink="false">2511.02460v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Measuring the Intrinsic Dimension of Earth Representations</title>
      <link>http://arxiv.org/abs/2511.02101v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Pre-print. 27 pages, 11 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究首次探讨了地理隐式神经表示(INRs)的内在维度特性，发现其内在维度在2到10之间，且与下游任务性能相关，为无监督评估模型提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;地理隐式神经表示(INRs)在地球观测学习中将低维位置输入(经度、纬度)嵌入到高维表示中，基于地理参考的卫星、图像或文本数据训练，但缺乏对其信息含量和分布的理解。&lt;h4&gt;目的&lt;/h4&gt;探究地理INRs的内在维度，了解这些表示中包含多少信息以及信息集中在哪里，为模型评估提供新方法。&lt;h4&gt;方法&lt;/h4&gt;分析内在维度在256到512之间的INRs，研究内在维度如何捕获数据局部变化，评估内在维度与下游任务性能的关系。&lt;h4&gt;主要发现&lt;/h4&gt;地理INRs的内在维度大致在2到10之间；对INR预训练过程中变化的空间分辨率和输入模态敏感；与下游任务性能相关；能够捕获空间伪影，促进模型评估和诊断。&lt;h4&gt;结论&lt;/h4&gt;提供了一种与架构无关、无标签的信息内容度量方法，可以在INRs之间实现无监督评估、模型选择和预训练设计。&lt;h4&gt;翻译&lt;/h4&gt;在地球观测表征学习的背景下，地理隐式神经表示(INRs)将低维位置输入(经度、纬度)嵌入到高维嵌入中，这些模型基于地理参考的卫星、图像或文本数据训练。尽管地理INRs的共同目标是将地球数据提炼成紧凑、易于学习的表示，但我们缺乏对这些地球表示中包含多少信息以及这些信息集中在哪里理解。数据集的内在维度衡量了捕获其局部变化所需的自由度数量，无论它嵌入的高维空间如何。这项工作提供了对地理INRs内在维度的首次研究。分析ambent维度在256到512之间的INRs，我们发现它们的内在维度大致在2到10之间，并且对INR预训练过程中变化的空间分辨率和输入模态敏感。此外，我们表明地理INRs的内在维度与下游任务性能相关，并且可以捕获空间伪影，促进模型评估和诊断。更广泛地说，我们的工作提供了一种与架构无关、无标签的信息内容度量方法，可以在INRs之间实现无监督评估、模型选择和预训练设计。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何量化地理隐式神经表示(Geographic INRs)的信息含量及信息集中位置的问题。这个问题很重要，因为目前地球表示模型的质量主要通过特定下游任务的有监督性能评估，缺乏对模型基本表示能力的理解，而理解地球表示的信息含量对于评估和改进地理表示学习模型至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从地理INRs将低维位置输入映射到高维嵌入但实际信息含量可能远低于环境维度这一观察出发，借鉴了内在维度(ID)测量这一已有概念，设计了全局ID和局部ID两种测量方法。作者借鉴了基于距离的估计器(如MLE、TwoNN)和基于角度的估计器(如FisherS)等现有ID估计方法，以及SatCLIP、GeoCLIP等地理INRs的现有工作，将其应用于地球表示这一特定领域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过测量地理INRs嵌入的内在维度(ID)来量化地球表示的信息丰富度和信息集中位置。整体流程包括：1)使用预训练位置编码器生成地理嵌入；2)通过角度估计器计算全局ID，通过基于距离的估计器计算局部ID生成ID地图；3)在下游任务激活空间中测量ID评估任务对齐；4)分析ID与下游性能的关系及分辨率和输入模态的影响；5)解释ID如何反映模型表示能力和任务对齐。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次研究地理INRs的内在维度；提出ID作为无监督评估指标；区分表示性和任务对齐；揭示空间伪影；建立ID与下游性能的关系。相比之前工作，不同之处在于：评估方法从有监督转向无监督；评估深度从'学习友好性'深入到信息含量和表示能力；应用范围从特定任务扩展到一般INRs；增加了诊断模型空间异质性和伪影的能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次系统地测量了地理隐式神经表示的内在维度，发现其远低于环境维度但与下游任务性能相关，为地球表示学习提供了一种新的无监督评估方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Within the context of representation learning for Earth observation,geographic Implicit Neural Representations (INRs) embed low-dimensionallocation inputs (longitude, latitude) into high-dimensional embeddings, throughmodels trained on geo-referenced satellite, image or text data. Despite thecommon aim of geographic INRs to distill Earth's data into compact,learning-friendly representations, we lack an understanding of how muchinformation is contained in these Earth representations, and where thatinformation is concentrated. The intrinsic dimension of a dataset measures thenumber of degrees of freedom required to capture its local variability,regardless of the ambient high-dimensional space in which it is embedded. Thiswork provides the first study of the intrinsic dimensionality of geographicINRs. Analyzing INRs with ambient dimension between 256 and 512, we find thattheir intrinsic dimensions fall roughly between 2 and 10 and are sensitive tochanging spatial resolution and input modalities during INR pre-training.Furthermore, we show that the intrinsic dimension of a geographic INRcorrelates with downstream task performance and can capture spatial artifacts,facilitating model evaluation and diagnostics. More broadly, our work offers anarchitecture-agnostic, label-free metric of information content that can enableunsupervised evaluation, model selection, and pre-training design across INRs.</description>
      <author>example@mail.com (Arjun Rao, Marc Rußwurm, Konstantin Klemmer, Esther Rolf)</author>
      <guid isPermaLink="false">2511.02101v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Lifelong and Multi-Task Representation Learning via Algorithm and Complexity Measure</title>
      <link>http://arxiv.org/abs/2511.01847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究终身学习中的表示学习问题，提出了一种基于多任务经验风险最小化的算法，并基于任务回避维度建立了样本复杂度界限。&lt;h4&gt;背景&lt;/h4&gt;终身学习中，学习者面临一系列具有共享结构的任务，需要识别并利用这些结构来加速学习。与多任务学习不同，终身学习要求学习者利用现有知识，同时在线方式持续收集部分信息。&lt;h4&gt;目的&lt;/h4&gt;研究一个广义的终身表示学习框架，开发一种简单算法来处理在线方式下的持续学习，并建立样本复杂度界限。&lt;h4&gt;方法&lt;/h4&gt;提出一种使用多任务经验风险最小化作为子程序的算法，并基于引入的任务回避维度概念建立样本复杂度界限。&lt;h4&gt;主要发现&lt;/h4&gt;基于任务回避维度建立了样本复杂度界限，该结果适用于涉及一般函数类的广泛学习问题，并在噪声下的分类和回归任务中得到了具体应用。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法和理论框架为终身学习提供了有效的表示学习方法，适用于多种学习场景。&lt;h4&gt;翻译&lt;/h4&gt;在终身学习中，学习者面临一系列具有共享结构的任务，旨在识别并利用这些结构来加速学习。我们研究了一种结构通过数据的共同表示来捕捉的场景。与多任务学习或学习-to-learn不同（在这些方法中任务一开始就可用以学习表示），终身学习要求学习者利用现有知识，同时在线方式持续收集部分信息。在本文中，我们考虑了一个广义的终身表示学习框架。我们提出了一种使用多任务经验风险最小化作为子程序的简单算法，并基于我们引入的新概念——任务回避维度，建立了样本复杂度界限。我们的结果适用于涉及一般函数类的广泛学习问题。作为具体例子，我们在噪声下的分类和回归任务中实例化了我们的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In lifelong learning, a learner faces a sequence of tasks with sharedstructure and aims to identify and leverage it to accelerate learning. We studythe setting where such structure is captured by a common representation ofdata. Unlike multi-task learning or learning-to-learn, where tasks areavailable upfront to learn the representation, lifelong learning requires thelearner to make use of its existing knowledge while continually gatheringpartial information in an online fashion. In this paper, we consider ageneralized framework of lifelong representation learning. We propose a simplealgorithm that uses multi-task empirical risk minimization as a subroutine andestablish a sample complexity bound based on a new notion we introduce--thetask-eluder dimension. Our result applies to a wide range of learning problemsinvolving general function classes. As concrete examples, we instantiate ourresult on classification and regression tasks under noise.</description>
      <author>example@mail.com (Zhi Wang, Chicheng Zhang, Ramya Korlakai Vinayak)</author>
      <guid isPermaLink="false">2511.01847v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>From Pixels to Cooperation Multi Agent Reinforcement Learning based on Multimodal World Models</title>
      <link>http://arxiv.org/abs/2511.01310v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于共享生成多模态世界模型（MWM）的新框架，用于解决从高维多模态感官输入学习合作多智能体策略的样本效率问题。&lt;h4&gt;背景&lt;/h4&gt;从高维、多模态感官输入（如像素和音频）直接学习合作多智能体策略存在样本效率低的问题。无模型多智能体强化学习算法面临表示学习、部分可观察性和信用分配的联合挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于共享生成多模态世界模型（MWM）的新框架，提高合作多智能体强化学习的样本效率，解决表示学习、部分可观察性和信用分配的联合挑战。&lt;h4&gt;方法&lt;/h4&gt;提出多模态世界模型（MWM），使用基于注意力的机制融合所有智能体的分布式多模态观测，学习环境动力学的压缩潜在表示；利用MWM作为快速'想象'模拟器，在潜在空间内训练合作MARL策略（如MAPPO），将表示学习与策略学习解耦；引入基于3D物理模拟器的多模态、多智能体基准测试集。&lt;h4&gt;主要发现&lt;/h4&gt;MWM-MARL框架与最先进的无模型MARL基线相比实现了数量级更高的样本效率；在感觉不对称环境中，所提出的多模态融合对任务成功至关重要；架构对传感器脱落具有更好的鲁棒性，这对实际部署至关重要。&lt;h4&gt;结论&lt;/h4&gt;基于共享生成多模态世界模型的框架能够有效解决从高维多模态输入学习合作多智能体策略的样本效率问题，并在感觉不对称环境和传感器脱落情况下表现出优越的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;从高维、多模态感官输入（如像素和音频）直接学习合作多智能体策略存在众所周知的样本效率低下问题。无模型多智能体强化学习算法难以应对表示学习、部分可观察性和信用分配的联合挑战。为此，我们提出了一种基于共享生成式多模态世界模型（MWM）的新框架。我们的MWM通过使用可扩展的基于注意力的机制融合所有智能体的分布式多模态观测，学习环境动力学的压缩潜在表示。随后，我们利用这个学习的MWM作为快速的'想象'模拟器，在其潜在空间内完全训练合作MARL策略（如MAPPO），将表示学习与策略学习解耦。我们引入了一组基于3D物理模拟器的新挑战性多模态、多智能体基准测试。我们的实验证明，与最先进的无模型MARL基线相比，我们的MWM-MARL框架实现了数量级更高的样本效率。我们进一步表明，在感觉不对称的环境中，我们提出的多模态融合对任务成功至关重要，并且我们的架构对传感器脱落提供了更强的鲁棒性，这是实际部署的关键特性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决从高维多模态感官输入(如像素和音频)直接学习合作多智能体策略时样本效率低下的问题。这个问题在现实中很重要，因为它关系到开发能在物理世界协作的智能体系统(如机器人团队、自动驾驶车辆等)；在研究中也很重要，因为现有的模型无关MARL算法难以同时处理高维感官输入、部分可观察环境和复杂社会推理的联合挑战，限制了智能体系统在复杂环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到模型无关MARL算法在高维多模态输入上的样本效率低下问题，然后借鉴了单智能体'World Models'范式的成功经验，意识到需要将其扩展到多智能体多模态环境。作者还考虑了计算效率和可扩展性挑战。该方法借鉴了多个领域的工作：单智能体世界模型(如Dreamer系列)、MARL的价值分解方法和CTDE范式、多模态学习(如CLIP、Vision Transformer)以及高效大规模系统(如MoE架构、联邦学习原则)。作者设计了一个基于共享多模态世界模型(MWM)的框架，使用注意力机制融合多智能体信息，并在潜在空间中训练策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1)学习一个共享的多模态世界模型(MWM)来统一表示环境；2)将表示学习与策略学习解耦，先学习环境模型再在潜在空间中训练策略；3)使用分层注意力机制动态融合多智能体多模态信息；4)利用MWM作为'想象'模拟器在潜在空间中训练策略。整体流程分为两个阶段：第一阶段学习MWM，包括多模态观察编码、分层融合、循环潜在动力学建模和训练；第二阶段在潜在空间中训练合作策略，包括使用MWM生成'梦想'轨迹、计算优势并更新策略、最终实现分散执行。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将单智能体世界模型扩展到多智能体多模态环境；2)提出基于注意力的分层多模态融合机制，实现动态信息路由；3)在潜在空间中完全训练合作MARL策略，解耦表示学习与策略学习；4)基于RSSM的概率性质提高对传感器故障的鲁棒性；5)引入新的多智能体多模态基准测试。相比之前工作，本文不仅扩展了世界模型到多智能体环境，还引入了多模态融合机制；与传统MARL不同，不是直接从原始数据学习而是先学习环境模型；相比多模态学习，不仅学习表示还学习环境动力学；相比联邦学习，将高效通信原则应用于多智能体协作场景。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多模态世界模型的多智能体强化学习框架，通过解耦表示学习与策略学习，实现了从高维多模态感官输入直接学习合作策略的样本效率大幅提升，并显著提高了对传感器故障的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning cooperative multi-agent policies directly from high-dimensional,multimodal sensory inputs like pixels and audio (from pixels) is notoriouslysample-inefficient. Model-free Multi-Agent Reinforcement Learning (MARL)algorithms struggle with the joint challenge of representation learning,partial observability, and credit assignment. To address this, we propose anovel framework based on a shared, generative Multimodal World Model (MWM). OurMWM is trained to learn a compressed latent representation of the environment'sdynamics by fusing distributed, multimodal observations from all agents using ascalable attention-based mechanism. Subsequently, we leverage this learned MWMas a fast, "imagined" simulator to train cooperative MARL policies (e.g.,MAPPO) entirely within its latent space, decoupling representation learningfrom policy learning. We introduce a new set of challenging multimodal,multi-agent benchmarks built on a 3D physics simulator. Our experimentsdemonstrate that our MWM-MARL framework achieves orders-of-magnitude greatersample efficiency compared to state-of-the-art model-free MARL baselines. Wefurther show that our proposed multimodal fusion is essential for task successin environments with sensory asymmetry and that our architecture providessuperior robustness to sensor-dropout, a critical feature for real-worlddeployment.</description>
      <author>example@mail.com (Sureyya Akin, Kavita Srivastava, Prateek B. Kapoor, Pradeep G. Sethi, Sunita Q. Patel, Rahu Srivastava)</author>
      <guid isPermaLink="false">2511.01310v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Influence-aware Causal Autoencoder Network for Node Importance Ranking in Complex Networks</title>
      <link>http://arxiv.org/abs/2511.01228v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ICAN（影响感知因果自编码网络）的新框架，通过因果表示学习获取稳健、不变的节点嵌入，用于跨网络排序任务。ICAN在合成网络上训练，但能有效应用于真实网络，解决了现有方法依赖目标网络拓扑结构导致的隐私问题和泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;节点重要性排序是图数据分析中的基础问题。现有方法通常依赖于传统中心性度量或先进图表示学习方法，这些方法直接依赖目标网络的拓扑结构，引发隐私问题，且在不同网络间泛化能力差。&lt;h4&gt;目的&lt;/h4&gt;设计一个仅在合成网络上训练，但能有效应用于真实网络的节点重要性排序模型，消除对目标网络拓扑的依赖，提高实用性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出影响感知因果自编码网络（ICAN），在自编码器架构中引入影响感知因果表示学习模块，提取与节点重要性因果相关的节点嵌入；引入因果排序损失，设计统一优化框架，联合优化重建和排序目标，实现节点表示学习和排序优化的相互强化。&lt;h4&gt;主要发现&lt;/h4&gt;ICAN在合成网络上训练，能有效泛化到多样化的真实图；在多个基准数据集上的实验表明，ICAN在排序准确性和泛化能力方面持续优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;ICAN成功解决了在合成网络上训练并应用于真实网络的问题，通过因果表示学习提高了模型的泛化能力和实用性。&lt;h4&gt;翻译&lt;/h4&gt;节点重要性排序是图数据分析中的一个基础问题。现有方法通常依赖于从传统中心性度量或先进的图表示学习方法中推导出的节点特征，这些方法直接依赖于目标网络的拓扑结构。然而，这种对结构信息的依赖引发了隐私问题，并且通常在不同网络间的泛化能力较差。在这项工作中，我们解决了一个关键问题：我们能否设计一个仅在合成网络上训练的节点重要性排序模型，并有效应用于真实网络，消除对目标网络拓扑的依赖，同时提高实用性和泛化能力？我们通过提出影响感知因果自编码网络（ICAN）对此问题给予了肯定的回答，这是一个利用因果表示学习获取稳健、不变的节点嵌入的新框架，用于跨网络排序任务。首先，ICAN在自编码器架构中引入了影响感知因果表示学习模块，提取与节点重要性因果相关的节点嵌入。此外，我们引入了因果排序损失，并设计了一个统一优化框架，联合优化重建和排序目标，使节点表示学习和排序优化能够相互强化。这种设计使得ICAN在合成网络上训练后，能够有效泛化到多样化的真实图。在多个基准数据集上的广泛实验表明，ICAN在排序准确性和泛化能力方面持续优于最先进的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Node importance ranking is a fundamental problem in graph data analysis.Existing approaches typically rely on node features derived from eithertraditional centrality measures or advanced graph representation learningmethods, which depend directly on the target network's topology. However, thisreliance on structural information raises privacy concerns and often leads topoor generalization across different networks. In this work, we address a keyquestion: Can we design a node importance ranking model trained exclusively onsynthetic networks that is effectively appliable to real-world networks,eliminating the need to rely on the topology of target networks and improvingboth practicality and generalizability? We answer this question affirmativelyby proposing the Influence-aware Causal Autoencoder Network (ICAN), a novelframework that leverages causal representation learning to get robust,invariant node embeddings for cross-network ranking tasks. Firstly, ICANintroduces an influence-aware causal representation learning module within anautoencoder architecture to extract node embeddings that are causally relatedto node importance. Moreover, we introduce a causal ranking loss and design aunified optimization framework that jointly optimizes the reconstruction andranking objectives, enabling mutual reinforcement between node representationlearning and ranking optimization. This design allows ICAN, trained onsynthetic networks, to generalize effectively across diverse real-world graphs.Extensive experiments on multiple benchmark datasets demonstrate that ICANconsistently outperforms state-of-the-art baselines in terms of both rankingaccuracy and generalization capability.</description>
      <author>example@mail.com (Jiahui Gao, Kuang Zhou, Yuchen Zhu)</author>
      <guid isPermaLink="false">2511.01228v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Anatomically Constrained Transformers for Echocardiogram Analysis</title>
      <link>http://arxiv.org/abs/2511.01109v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Video Anatomically Constrained Transformer (ViACT)，一种将解剖先验直接整合到变换器架构中的新框架，用于超声心动图分析。&lt;h4&gt;背景&lt;/h4&gt;视频变换器在超声心动图分析中显示出强大潜力，但与其他视频模型一样，它们容易从非诊断区域（如图像背景）学习到虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;克服现有视频变换器模型的局限性，通过整合解剖先验来提高模型在医学图像分析中的表现和可解释性。&lt;h4&gt;方法&lt;/h4&gt;ViACT将变形的解剖结构表示为点集，将其空间几何和相应的图像块编码为变换器token；在预训练过程中采用掩码自编码策略，仅掩码和重建解剖区域；预训练模型可针对该区域的任务进行微调；专注于心肌应用，并在左心室射血分数回归和心脏淀粉样变性检测等任务上展示框架。&lt;h4&gt;主要发现&lt;/h4&gt;解剖约束将变换器的注意力集中在心肌区域，产生与已知病理区域对齐的可解释注意力图；ViACT能够推广到心肌点跟踪，无需专门跟踪网络中的特定任务组件。&lt;h4&gt;结论&lt;/h4&gt;ViACT通过整合解剖先验，解决了视频变换器在医学图像分析中学习非相关区域的问题，提高了模型在超声心动图分析任务中的性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;视频变换器最近在超声心动图分析中显示出强大的潜力，利用自监督预训练和灵活适应不同任务的能力。然而，与其他视频模型一样，它们容易从非诊断区域（如图像背景）学习到虚假相关性。为克服这一局限，我们提出了视频解剖约束变换器（ViACT），一种将解剖先验直接整合到变换器架构的新框架。ViACT将变形的解剖结构表示为点集，并将其空间几何和相应的图像块编码为变换器token。在预训练过程中，ViACT遵循掩码自编码策略，仅掩码和重建解剖区域，强制表示学习集中在解剖区域。预训练模型随后可以针对该区域的任务进行微调。本文中我们专注于心肌，在左心室射血分数回归和心脏淀粉样变性检测等超声分析任务上展示了该框架。解剖约束将变换器的注意力集中在心肌区域，产生与已知CA病理区域对齐的可解释注意力图。此外，ViACT能够推广到心肌点跟踪，而无需专门跟踪网络中使用的特定任务组件，如相关体积。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video transformers have recently demonstrated strong potential forechocardiogram (echo) analysis, leveraging self-supervised pre-training andflexible adaptation across diverse tasks. However, like other models operatingon videos, they are prone to learning spurious correlations from non-diagnosticregions such as image backgrounds. To overcome this limitation, we propose theVideo Anatomically Constrained Transformer (ViACT), a novel framework thatintegrates anatomical priors directly into the transformer architecture. ViACTrepresents a deforming anatomical structure as a point set and encodes both itsspatial geometry and corresponding image patches into transformer tokens.During pre-training, ViACT follows a masked autoencoding strategy that masksand reconstructs only anatomical patches, enforcing that representationlearning is focused on the anatomical region. The pre-trained model can then befine-tuned for tasks localized to this region. In this work we focus on themyocardium, demonstrating the framework on echo analysis tasks such as leftventricular ejection fraction (EF) regression and cardiac amyloidosis (CA)detection. The anatomical constraint focuses transformer attention within themyocardium, yielding interpretable attention maps aligned with regions of knownCA pathology. Moreover, ViACT generalizes to myocardium point tracking withoutrequiring task-specific components such as correlation volumes used inspecialized tracking networks.</description>
      <author>example@mail.com (Alexander Thorley, Agis Chartsias, Jordan Strom, Jeremy Slivnick, Dipak Kotecha, Alberto Gomez, Jinming Duan)</author>
      <guid isPermaLink="false">2511.01109v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>The Geometry of Grokking: Norm Minimization on the Zero-Loss Manifold</title>
      <link>http://arxiv.org/abs/2511.01938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了神经网络中的'Grokking'现象，即神经网络在完全记忆训练数据后，经过显著延迟才出现完全泛化的现象。论文提出通过约束优化视角理解记忆后的学习过程，证明梯度下降实际上是在零损失流形上最小化权重范数。&lt;h4&gt;背景&lt;/h4&gt;先前研究将延迟泛化归因于权重衰减驱动的表示学习，但精确的潜在动态机制仍然不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过约束优化的视角理解神经网络记忆后的学习过程。&lt;h4&gt;方法&lt;/h4&gt;论文在无限小学习率和权重衰减系数的极限下形式化证明梯度下降在零损失流形上最小化权重范数；引入近似方法解耦参数学习动态；推导两层网络第一层记忆后动力学的闭式表达式。&lt;h4&gt;主要发现&lt;/h4&gt;梯度下降在零损失流形上最小化权重范数；通过近似方法可解耦参数学习动态；推导出两层网络第一层记忆后动力学的闭式表达式。&lt;h4&gt;结论&lt;/h4&gt;实验证实使用预测梯度模拟训练过程能重现 grokking 的延迟泛化和表示学习特征。&lt;h4&gt;翻译&lt;/h4&gt;Grokking 是神经网络中的一种令人费解的现象，其中完全泛化仅在完全记忆训练数据后经过显著延迟才发生。先前的研究将这种延迟泛化与权重衰减驱动的表示学习联系起来，但潜在的精确动态机制仍然不清楚。在本文中，我们认为记忆后的学习可以通过约束优化的视角来理解：梯度下降有效地在零损失流形上最小化权重范数。我们在学习率和权重衰减系数无限小的极限情况下形式化证明了这一点。为了进一步剖析这一机制，我们引入了一个近似方法，将网络中一部分参数的学习动态与其他参数解耦。应用这一框架，我们推导出两层网络第一层记忆后动力学的闭式表达式。实验证实，使用我们预测的梯度模拟训练过程能够重现 grokking 的延迟泛化和表示学习特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grokking is a puzzling phenomenon in neural networks where fullgeneralization occurs only after a substantial delay following the completememorization of the training data. Previous research has linked this delayedgeneralization to representation learning driven by weight decay, but theprecise underlying dynamics remain elusive. In this paper, we argue thatpost-memorization learning can be understood through the lens of constrainedoptimization: gradient descent effectively minimizes the weight norm on thezero-loss manifold. We formally prove this in the limit of infinitesimallysmall learning rates and weight decay coefficients. To further dissect thisregime, we introduce an approximation that decouples the learning dynamics of asubset of parameters from the rest of the network. Applying this framework, wederive a closed-form expression for the post-memorization dynamics of the firstlayer in a two-layer network. Experiments confirm that simulating the trainingprocess using our predicted gradients reproduces both the delayedgeneralization and representation learning characteristic of grokking.</description>
      <author>example@mail.com (Tiberiu Musat)</author>
      <guid isPermaLink="false">2511.01938v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>TRISKELION-1: Unified Descriptive-Predictive-Generative AI</title>
      <link>http://arxiv.org/abs/2511.00711v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 18 figures, submitted to arXiv (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TRISKELION-1是一种统一架构，在一个编码器-解码器框架中集成了描述性、预测性和生成性功能，能够同时实现描述性重建、预测分类和生成采样。&lt;h4&gt;背景&lt;/h4&gt;当前人工智能领域需要能够同时处理描述性、预测性和生成性任务的统一架构。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够联合优化描述性表示学习、预测推理和生成合成的统一模型框架。&lt;h4&gt;方法&lt;/h4&gt;TRISKELION-1架构在一个编码器-解码器框架中集成了统计、机制和生成推理，使用变分目标进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;在MNIST数据集上的实验表明，描述性重建、预测分类和生成采样可以在一个模型中稳定共存。&lt;h4&gt;结论&lt;/h4&gt;该框架为连接可解释性、准确性和创造力的通用智能架构提供了蓝图。&lt;h4&gt;翻译&lt;/h4&gt;TRISKELION-1是一种统一的描述性-预测性-生成性架构，它在单个编码器-解码器框架中集成了统计、机制和生成推理。该模型展示了如何使用变分目标联合优化描述性表示学习、预测推理和生成合成。在MNIST上的实验验证了描述性重建、预测分类和生成采样可以在一个模型中稳定共存。该框架为连接可解释性、准确性和创造力的通用智能架构提供了蓝图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; TRISKELION-1 is a unified descriptive-predictive-generative architecture thatintegrates statistical, mechanistic, and generative reasoning within a singleencoder-decoder framework. The model demonstrates how descriptiverepresentation learning, predictive inference, and generative synthesis can bejointly optimized using variational objectives. Experiments on MNIST validatethat descriptive reconstruction, predictive classification, and generativesampling can coexist stably within one model. The framework provides ablueprint toward universal intelligence architectures that connectinterpretability, accuracy, and creativity.</description>
      <author>example@mail.com (Nardeep Kumar, Arun Kanwar)</author>
      <guid isPermaLink="false">2511.00711v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Improving Robustness to Out-of-Distribution States in Imitation Learning via Deep Koopman-Boosted Diffusion Policy</title>
      <link>http://arxiv.org/abs/2511.00555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE T-RO&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DeepKoopman增强的双分支扩散策略(D3P)的算法，用于解决机器人操作模仿学习中现有扩散模型难以捕捉多步骤间强时间依赖性的问题，特别是在整合本体感受输入时。D3P通过双分支架构解耦不同感官模态的作用，并利用Deep Koopman Operator模块增强视觉表示学习，显著提高了任务执行效果。&lt;h4&gt;背景&lt;/h4&gt;将生成模型与动作块结合在机器人操作模仿学习中显示出巨大潜力，但现有的基于扩散的方法往往难以捕捉多步骤间的强时间依赖性，特别是在整合本体感受输入时，这会导致任务失败，策略过度适应本体感受线索而忽略视觉特征。&lt;h4&gt;目的&lt;/h4&gt;克服现有扩散模型在捕捉多步骤间强时间依赖性方面的局限性，特别是当整合本体感受输入时，防止策略过度适应本体感受线索而忽略视觉特征，从而提高机器人操作任务的执行效果。&lt;h4&gt;方法&lt;/h4&gt;提出D3P算法，采用双分支架构解耦不同感官模态组合的作用：视觉分支编码视觉观察以指示任务进展，融合分支整合视觉和本体感受输入实现精确操作。当机器人无法完成中间目标时，策略可动态切换到视觉分支生成的动作块。同时，集成Deep Koopman Operator模块捕捉视觉输入中的结构化时间动态，并利用测试时损失作为置信信号引导预测动作块的聚合。&lt;h4&gt;主要发现&lt;/h4&gt;在六个RLBench桌面任务的模拟实验中，D3P比最先进的扩散策略平均高出14.6%。在三个真实世界机器人操作任务中，实现了15.0%的改进。代码已公开在GitHub上。&lt;h4&gt;结论&lt;/h4&gt;D3P算法通过双分支架构和Deep Koopman Operator模块有效解决了现有扩散模型在捕捉时间依赖性和整合多感官输入方面的局限性，显著提高了机器人操作任务的执行效果和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;将生成模型与动作块整合在机器人操作模仿学习中显示出巨大潜力。然而，现有的基于扩散的范式往往难以捕捉多步骤间的强时间依赖性，特别是在整合本体感受输入时。这种局限性会导致任务失败，策略过度适应本体感受线索而牺牲对任务中视觉派生特征的捕捉。为克服这一挑战，我们提出了DeepKoopman增强的双分支扩散策略(D3P)算法。D3P引入双分支架构来解耦不同感官模态组合的作用。视觉分支编码视觉观察以指示任务进展，而融合分支整合视觉和本体感受输入以实现精确操作。在此架构中，当机器人无法完成中间目标（如抓取抽屉把手）时，策略可动态切换到执行由视觉分支生成的动作块，允许恢复到先前观察的状态并促进任务重试。为进一步增强视觉表示学习，我们集成了Deep Koopman Operator模块，从视觉输入中捕捉结构化时间动态。在推理过程中，我们使用生成模型的测试时损失作为置信信号来指导时间重叠预测动作块的聚合，从而提高策略执行的可靠性。在六个RLBench桌面任务的模拟实验中，D3P比最先进的扩散策略平均高出14.6%。在三个真实世界机器人操作任务中，实现了15.0%的改进。代码：https://github.com/dianyeHuang/D3P。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating generative models with action chunking has shown significantpromise in imitation learning for robotic manipulation. However, the existingdiffusion-based paradigm often struggles to capture strong temporaldependencies across multiple steps, particularly when incorporatingproprioceptive input. This limitation can lead to task failures, where thepolicy overfits to proprioceptive cues at the expense of capturing the visuallyderived features of the task. To overcome this challenge, we propose the DeepKoopman-boosted Dual-branch Diffusion Policy (D3P) algorithm. D3P introduces adual-branch architecture to decouple the roles of different sensory modalitycombinations. The visual branch encodes the visual observations to indicatetask progression, while the fused branch integrates both visual andproprioceptive inputs for precise manipulation. Within this architecture, whenthe robot fails to accomplish intermediate goals, such as grasping a drawerhandle, the policy can dynamically switch to execute action chunks generated bythe visual branch, allowing recovery to previously observed states andfacilitating retrial of the task. To further enhance visual representationlearning, we incorporate a Deep Koopman Operator module that capturesstructured temporal dynamics from visual inputs. During inference, we use thetest-time loss of the generative model as a confidence signal to guide theaggregation of the temporally overlapping predicted action chunks, therebyenhancing the reliability of policy execution. In simulation experiments acrosssix RLBench tabletop tasks, D3P outperforms the state-of-the-art diffusionpolicy by an average of 14.6\%. On three real-world robotic manipulation tasks,it achieves a 15.0\% improvement. Code: https://github.com/dianyeHuang/D3P.</description>
      <author>example@mail.com (Dianye Huang, Nassir Navab, Zhongliang Jiang)</author>
      <guid isPermaLink="false">2511.00555v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>FedMGP: Personalized Federated Learning with Multi-Group Text-Visual Prompts</title>
      <link>http://arxiv.org/abs/2511.00480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FedMGP是一种用于视觉语言模型中个性化联邦提示学习的新范式，通过多组配对的文本和视觉提示捕捉多样化语义，采用动态提示聚合策略平衡全局与本地特征，实现高效且高性能的联邦学习。&lt;h4&gt;背景&lt;/h4&gt;在联邦学习环境中，视觉语言模型需要同时考虑全局知识的共享和客户端个性化特征的保留，而传统的联邦提示学习方法在平衡这两方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的联邦提示学习范式，能够有效捕捉多样化、细粒度的语义和实例级线索，平衡通用知识保留与客户端特定特征，同时保持参数效率。&lt;h4&gt;方法&lt;/h4&gt;FedMGP为每个客户端配备多组配对的文本和视觉提示，引入多样性损失促使各组提示专注于不同语义方面，采用基于相似度引导概率采样的动态提示聚合策略进行通信，通过softmax加权分布选择最相关的提示组。&lt;h4&gt;主要发现&lt;/h4&gt;动态聚合策略通过强化共享语义同时抑制客户端特定噪声，促进鲁棒的全局表征学习；FedMGP在所有联邦提示学习方法中实现了最低的通信参数，同时达到最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FedMGP在多样化的联邦视觉语言基准测试中，在个性化和领域泛化方面均始终优于先前的方法，为联邦视觉语言模型提供了一种高效且有效的个性化学习框架。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了FedMGP，一种用于视觉语言模型中个性化联邦提示学习的新范式。FedMGP为每个客户端配备多组配对的文本和视觉提示，使模型能够捕捉多样化、细粒度的语义和实例级线索。引入了多样性损失促使每组提示专注于不同且互补的语义方面，确保各组共同覆盖更广泛的本地特征。在通信过程中，FedMGP采用基于相似度引导概率采样的动态提示聚合策略：每个客户端计算其提示组与上一轮全局提示之间的余弦相似度，然后通过softmax加权分布采样s个组。这种软选择机制优先聚合语义对齐的知识，同时有效探索代表性不足的模式，平衡了通用知识的保留与客户端特定特征。值得注意的是，FedMGP通过在多个组之间重新分配固定的提示容量，在所有联邦提示学习方法中实现了最低的通信参数，同时达到最先进的性能。理论分析表明，我们的动态聚合策略通过强化共享语义同时抑制客户端特定噪声，促进鲁棒的全局表征学习。大量实验证明，在多样化的联邦视觉语言基准测试中，FedMGP在个性化和领域泛化方面均始终优于先前的方法。代码将在https://github.com/weihao-bo/FedMGP.git上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce FedMGP, a new paradigm for personalized federatedprompt learning in vision-language models. FedMGP equips each client withmultiple groups of paired textual and visual prompts, enabling the model tocapture diverse, fine-grained semantic and instance-level cues. A diversityloss is introduced to drive each prompt group to specialize in distinct andcomplementary semantic aspects, ensuring that the groups collectively cover abroader range of local characteristics. During communication, FedMGP employs adynamic prompt aggregation strategy based on similarity-guided probabilisticsampling: each client computes the cosine similarity between its prompt groupsand the global prompts from the previous round, then samples s groups via asoftmax-weighted distribution. This soft selection mechanism preferentiallyaggregates semantically aligned knowledge while still enabling exploration ofunderrepresented patterns effectively balancing the preservation of commonknowledge with client-specific features. Notably, FedMGP maintains parameterefficiency by redistributing a fixed prompt capacity across multiple groups,achieving state-of-the-art performance with the lowest communication parametersamong all federated prompt learning methods. Theoretical analysis shows thatour dynamic aggregation strategy promotes robust global representation learningby reinforcing shared semantics while suppressing client-specific noise.Extensive experiments demonstrate that FedMGP consistently outperforms priorapproaches in both personalization and domain generalization across diversefederated vision-language benchmarks. The code will be released onhttps://github.com/weihao-bo/FedMGP.git.</description>
      <author>example@mail.com (Weihao Bo, Yanpeng Sun, Yu Wang, Xinyu Zhang, Zechao Li)</author>
      <guid isPermaLink="false">2511.00480v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SHIELD: Securing Healthcare IoT with Efficient Machine Learning Techniques for Anomaly Detection</title>
      <link>http://arxiv.org/abs/2511.03661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种机器学习驱动的框架，用于检测物联网医疗环境中的恶意网络攻击和设备异常，通过评估多种机器学习模型，确定了最优解决方案并提升了医疗设备安全性。&lt;h4&gt;背景&lt;/h4&gt;物联网设备在医疗领域的整合引入了显著的安全性和可靠性挑战，增加了系统对网络威胁和操作异常的易感性。&lt;h4&gt;目的&lt;/h4&gt;开发一种机器学习框架，用于检测恶意网络攻击和识别故障设备异常，从而提高物联网医疗环境的安全性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;利用包含20万条记录的数据集，评估了八种机器学习模型，包括监督学习(XGBoost、K-NN)、半监督学习(GAN、VAE)和无监督学习(One-Class SVM、Isolation Forest、GNN、LSTM自编码器)方法，通过F1分数、精确率、召回率、准确率、ROC-AUC和计算效率等指标进行综合评估。&lt;h4&gt;主要发现&lt;/h4&gt;异常检测方面，XGBoost表现最优(99%准确率，0.04秒计算时间)，隔离森林有效平衡了精确率和召回率，LSTM自编码器表现较差；攻击检测方面，KNN实现接近完美的指标且计算成本最低(0.05秒)，VAE准确率达97%，GAN计算成本最高但准确率和ROC-AUC最低。&lt;h4&gt;结论&lt;/h4&gt;该框架通过有效的异常检测策略增强了物联网医疗安全性，能够早期检测网络威胁和设备故障，防止数据泄露，最小化系统停机时间，确保医疗设备持续安全运行，最终保护患者健康和对物联网医疗解决方案的信任。&lt;h4&gt;翻译&lt;/h4&gt;物联网设备在医疗领域的整合引入了显著的安全性和可靠性挑战，增加了对网络威胁和操作异常的易感性。本研究提出了一种机器学习驱动的框架，用于(1)检测恶意网络攻击和(2)识别故障设备异常，利用包含20万条记录的数据集。评估了八种机器学习模型，涵盖三种学习方法：监督学习(XGBoost、K-近邻)、半监督学习(生成对抗网络、变分自编码器)和无监督学习(一类支持向量机、隔离森林、图神经网络和长短期记忆自编码器)。通过F1分数、精确率、召回率、准确率、ROC-AUC和计算效率等多个指标进行综合评估。XGBoost以99%的准确率和最小的计算开销(0.04秒)实现了异常检测，隔离森林有效地平衡了精确率和召回率。LSTM自编码器表现较差，准确率较低且延迟较高。在攻击检测方面，KNN以最低的计算成本(0.05秒)实现了接近完美的精确率、召回率和F1分数，其次是VAE，准确率为97%。GAN显示出最高的计算成本，但准确率和ROC-AUC最低。这些发现通过有效的异常检测策略增强了物联网医疗安全性。通过提高网络威胁和设备故障的早期检测，该框架有可能防止数据泄露，最小化系统停机时间，并确保医疗设备的持续安全运行，最终保护患者健康和对物联网驱动医疗解决方案的信任。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/AIIoT65859.2025.11105287&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of IoT devices in healthcare introduces significant securityand reliability challenges, increasing susceptibility to cyber threats andoperational anomalies. This study proposes a machine learning-driven frameworkfor (1) detecting malicious cyberattacks and (2) identifying faulty deviceanomalies, leveraging a dataset of 200,000 records. Eight machine learningmodels are evaluated across three learning approaches: supervised learning(XGBoost, K-Nearest Neighbors (K- NN)), semi-supervised learning (GenerativeAdversarial Networks (GAN), Variational Autoencoders (VAE)), and unsupervisedlearning (One-Class Support Vector Machine (SVM), Isolation Forest, GraphNeural Networks (GNN), and Long Short-Term Memory (LSTM) Autoencoders). Thecomprehensive evaluation was conducted across multiple metrics like F1-score,precision, recall, accuracy, ROC-AUC, computational efficiency. XGBoostachieved 99\% accuracy with minimal computational overhead (0.04s) for anomalydetection, while Isolation Forest balanced precision and recall effectively.LSTM Autoencoders underperformed with lower accuracy and higher latency. Forattack detection, KNN achieved near-perfect precision, recall, and F1-scorewith the lowest computational cost (0.05s), followed by VAE at 97% accuracy.GAN showed the highest computational cost with lowest accuracy and ROC-AUC.These findings enhance IoT-enabled healthcare security through effectiveanomaly detection strategies. By improving early detection of cyber threats anddevice failures, this framework has the potential to prevent data breaches,minimize system downtime, and ensure the continuous and safe operation ofmedical devices, ultimately safeguarding patient health and trust in IoT-drivenhealthcare solutions.</description>
      <author>example@mail.com (Mahek Desai, Apoorva Rumale, Marjan Asadinia)</author>
      <guid isPermaLink="false">2511.03661v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>NABench: Large-Scale Benchmarks of Nucleotide Foundation Models for Fitness Prediction</title>
      <link>http://arxiv.org/abs/2511.02888v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NABench是一个用于核酸适应性预测的大规模、系统化基准测试平台，汇集了162个高通量测定和260万个突变序列，涵盖多种DNA和RNA家族。研究团队评估了29个代表性基础模型在不同场景下的表现，建立了强大且可重现的基准线，并公开了代码以促进核酸建模和相关应用。&lt;h4&gt;背景&lt;/h4&gt;核苷酸序列变异可能导致功能适应性的显著变化。最近的核苷酸基础模型可以直接从序列预测这些适应性效应，但数据集的不一致性和预处理的不统一使得难以公平地比较不同DNA和RNA家族的方法。&lt;h4&gt;目的&lt;/h4&gt;介绍NABench，一个用于核酸适应性预测的大规模、系统化基准测试平台，以解决现有方法比较困难的问题。&lt;h4&gt;方法&lt;/h4&gt;NABench汇集了162个高通量测定，整理了260万个突变序列，涵盖了多种DNA和RNA家族，并具有标准化的分割和丰富的元数据。研究者在统一的评估套件下，严格评估了29个代表性基础模型，包括零样本预测、少样本预测、迁移学习和监督设置等场景。&lt;h4&gt;主要发现&lt;/h4&gt;NABench在规模、多样性和数据质量上超越了先前的核苷酸适应性基准测试；评估结果量化了不同任务和核酸类型之间的性能异质性；展示了不同建模选择的明显优势和失败模式；建立了强大且可重现的基准线。&lt;h4&gt;结论&lt;/h4&gt;研究团队发布了NABench以促进核酸建模，支持RNA/DNA设计、合成生物学和生物化学等下游应用。代码已在GitHub上提供。&lt;h4&gt;翻译&lt;/h4&gt;核苷酸序列变异可能引起功能适应性的显著变化。最近的核苷酸基础模型有望直接从序列预测这些适应性效应，然而异构数据集和不一致的预处理使得难以在DNA和RNA家族间公平地比较方法。在此，我们引入NABench，一个用于核酸适应性预测的大规模、系统化基准。NABench汇集了162个高通量测定，整理了260万个突变序列，涵盖多样化的DNA和RNA家族，具有标准化的分割和丰富的元数据。我们证明NABench在规模、多样性和数据质量上超越了先前的核苷酸适应性基准测试。在统一的评估套件下，我们严格评估了29个代表性基础模型，包括零样本预测、少样本预测、迁移学习和监督设置。结果量化了不同任务和核酸类型之间的性能异质性，展示了不同建模选择的明显优势和失败模式，并建立了强大、可重现的基准线。我们发布NABench以推进核酸建模，支持RNA/DNA设计、合成生物学和生物化学的下游应用。我们的代码可在https://github.com/mrzzmrzz/NABench获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nucleotide sequence variation can induce significant shifts in functionalfitness. Recent nucleotide foundation models promise to predict such fitnesseffects directly from sequence, yet heterogeneous datasets and inconsistentpreprocessing make it difficult to compare methods fairly across DNA and RNAfamilies. Here we introduce NABench, a large-scale, systematic benchmark fornucleic acid fitness prediction. NABench aggregates 162 high-throughput assaysand curates 2.6 million mutated sequences spanning diverse DNA and RNAfamilies, with standardized splits and rich metadata. We show that NABenchsurpasses prior nucleotide fitness benchmarks in scale, diversity, and dataquality. Under a unified evaluation suite, we rigorously assess 29representative foundation models across zero-shot, few-shot prediction,transfer learning, and supervised settings. The results quantify performanceheterogeneity across tasks and nucleic-acid types, demonstrating clearstrengths and failure modes for different modeling choices and establishingstrong, reproducible baselines. We release NABench to advance nucleic acidmodeling, supporting downstream applications in RNA/DNA design, syntheticbiology, and biochemistry. Our code is available athttps://github.com/mrzzmrzz/NABench.</description>
      <author>example@mail.com (Zhongmin Li, Runze Ma, Jiahao Tan, Chengzi Tan, Shuangjia Zheng)</author>
      <guid isPermaLink="false">2511.02888v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural AI with Temporal Dynamics for Comprehensive Anomaly Detection in Microservices</title>
      <link>http://arxiv.org/abs/2511.03285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络与时间建模的统一框架，用于微服务架构中的异常检测和根因追踪。该框架将微服务调用链抽象为有向图，通过图卷积聚合特征并建模依赖关系，同时使用门控循环单元建模时间演化，最终实现从局部异常检测到全局调用链追踪的统一建模。&lt;h4&gt;背景&lt;/h4&gt;微服务架构中的异常检测和根因追踪问题&lt;h4&gt;目的&lt;/h4&gt;提出一个统一框架来解决微服务架构中的异常检测和根因追踪问题&lt;h4&gt;方法&lt;/h4&gt;结合图神经网络与时间建模的统一框架：将微服务调用链抽象为有向图，使用节点和边的多维特征构建服务拓扑表示，应用图卷积聚合节点特征并建模依赖关系，引入门控循环单元(GRU)建模调用链的时间演化，使用多层堆叠和连接操作联合获取结构和时间表示，定义节点和路径级别的异常评分函数&lt;h4&gt;主要发现&lt;/h4&gt;所提出的框架在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性&lt;h4&gt;结论&lt;/h4&gt;该研究不仅为微服务异常检测提供了新的技术路径，也为分布式系统的智能运维奠定了方法论基础&lt;h4&gt;翻译&lt;/h4&gt;本研究解决了微服务架构中的异常检测和根因追踪问题，并提出了一种结合图神经网络与时间建模的统一框架。微服务调用链被抽象为有向图，其中节点和边的多维特征用于构建服务拓扑表示，并应用图卷积来聚合节点特征并建模依赖关系，捕捉服务间的复杂结构关系。在此基础上，引入门控循环单元来建模调用链的时间演化，并使用多层堆叠和连接操作联合获取结构和时间表示，提高识别异常模式的能力。此外，定义了节点和路径级别的异常评分函数，实现从局部异常检测到全局调用链追踪的统一建模，从而能够识别异常服务节点并重建潜在的异常传播路径。随后从超参数、环境干扰和数据分布等多个维度设计了敏感性实验来评估该框架，结果表明其在AUC、ACC、Recall和F1-Score等关键指标上优于基线方法，在动态拓扑和复杂环境下保持高准确性和稳定性。这项研究不仅为微服务异常检测提供了新的技术路径，也为分布式系统的智能运维奠定了方法论基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the problem of anomaly detection and root cause tracingin microservice architectures and proposes a unified framework that combinesgraph neural networks with temporal modeling. The microservice call chain isabstracted as a directed graph, where multidimensional features of nodes andedges are used to construct a service topology representation, and graphconvolution is applied to aggregate features across nodes and modeldependencies, capturing complex structural relationships among services. Onthis basis, gated recurrent units are introduced to model the temporalevolution of call chains, and multi-layer stacking and concatenation operationsare used to jointly obtain structural and temporal representations, improvingthe ability to identify anomaly patterns. Furthermore, anomaly scoringfunctions at both the node and path levels are defined to achieve unifiedmodeling from local anomaly detection to global call chain tracing, whichenables the identification of abnormal service nodes and the reconstruction ofpotential anomaly propagation paths. Sensitivity experiments are then designedfrom multiple dimensions, including hyperparameters, environmentaldisturbances, and data distribution, to evaluate the framework, and resultsshow that it outperforms baseline methods in key metrics such as AUC, ACC,Recall, and F1-Score, maintaining high accuracy and stability under dynamictopologies and complex environments. This research not only provides a newtechnical path for anomaly detection in microservices but also lays amethodological foundation for intelligent operations in distributed systems.</description>
      <author>example@mail.com (Qingyuan Zhang, Ning Lyu, Le Liu, Yuxi Wang, Ziyu Cheng, Cancan Hua)</author>
      <guid isPermaLink="false">2511.03285v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SurgAnt-ViVQA: Learning to Anticipate Surgical Events through GRU-Driven Temporal Cross-Attention</title>
      <link>http://arxiv.org/abs/2511.03178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了PitVQA-Anticipation数据集和SurgAnt-ViVQA模型，用于解决经鼻蝶垂体手术中的前瞻性手术推理问题。通过结合时间感知编码器和细粒度门控交叉注意力，系统能够从回顾性描述转向主动预测，为手术实时辅助提供支持。&lt;h4&gt;背景&lt;/h4&gt;在经鼻蝶垂体手术中，视野有限且工作流程变化迅速，预测即将发生的手术事件对实时辅助至关重要。现有视觉问答系统基于孤立帧进行静态推理，对预测下一步或器械需求支持有限，且现有数据集关注当前场景而非近期未来。&lt;h4&gt;目的&lt;/h4&gt;创建第一个用于前瞻性手术推理的VQA数据集，并提出一种能够预测手术未来阶段、步骤、器械需求和剩余时间的视频语言模型。&lt;h4&gt;方法&lt;/h4&gt;构建了包含33.5小时手术视频和734,769个问答对的PitVQA-Anticipation数据集，涵盖四个预测任务。提出SurgAnt-ViVQA模型，使用双向GRU编码帧动态，通过自适应门将视觉上下文注入语言流，并采用参数高效微调定制语言主干。&lt;h4&gt;主要发现&lt;/h4&gt;SurgAnt-ViVQA在PitVQA-Anticipation和EndoVis数据集上超越了现有基线。消融研究表明时间循环和门控 fusion带来主要性能提升。帧预算研究显示8帧最大化流畅性，32帧略微降低BLEU但改进数值时间估计。&lt;h4&gt;结论&lt;/h4&gt;SurgAnt-ViVQA将手术VQA从回顾性描述推进到主动预测，PitVQA-Anticipation为此提供了全面基准，强调了针对时间建模对可靠、未来感知型手术辅助的重要性。&lt;h4&gt;翻译&lt;/h4&gt;预测即将发生的手术事件对于经鼻蝶垂体手术中的实时辅助至关重要，在这些手术中视野有限且工作流程变化迅速。大多数视觉问答系统基于孤立帧进行推理，使用静态视觉语言对齐，对预测下一步或器械需求提供很少支持。现有的手术VQA数据集同样关注当前场景而非近期未来。我们引入了PitVQA-Anticipation，这是第一个用于前瞻性手术推理的VQA数据集。它包含33.5小时手术视频和734,769个问答对，这些问答对基于时间分组剪辑和专家注释构建，涵盖四个任务：预测未来阶段、下一步、即将使用的器械和剩余时间。我们进一步提出了SurgAnt-ViVQA，这是一个视频语言模型，使用GRU门控时间交叉注意力模块来适应大语言模型。双向GRU编码帧到帧的动态，同时自适应门将视觉上下文注入到令牌级别的语言流中。参数高效微调将语言主干定制到手术领域。SurgAnt-ViVQA在PitVQA-Anticipation和EndoVis数据集上测试，超越了强大的基于图像和视频的基线。消融研究表明，时间循环和门控融合带来了大部分性能提升。帧预算研究表明存在权衡：8帧最大化流畅性，而32帧略微降低BLEU但改进了数值时间估计。通过将时间感知编码器与细粒度门控交叉注意力相结合，SurgAnt-ViVQA推动手术VQA从回顾性描述发展到主动预测。PitVQA-Anticipation为此设置提供了全面的基准，并强调了针对时间建模对可靠、未来感知型手术辅助的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anticipating forthcoming surgical events is vital for real-time assistance inendonasal transsphenoidal pituitary surgery, where visibility is limited andworkflow changes rapidly. Most visual question answering (VQA) systems reasonon isolated frames with static vision language alignment, providing littlesupport for forecasting next steps or instrument needs. Existing surgical VQAdatasets likewise center on the current scene rather than the near future. Weintroduce PitVQA-Anticipation, the first VQA dataset designed for forwardlooking surgical reasoning. It comprises 33.5 hours of operative video and734,769 question answer pairs built from temporally grouped clips and expertannotations across four tasks: predicting the future phase, next step, upcominginstrument, and remaining duration. We further propose SurgAnt-ViVQA, a videolanguage model that adapts a large language model using a GRU Gated TemporalCross-Attention module. A bidirectional GRU encodes frame to frame dynamics,while an adaptive gate injects visual context into the language stream at thetoken level. Parameter efficient fine tuning customizes the language backboneto the surgical domain. SurgAnt-ViVQA tested upon on PitVQA-Anticipation andEndoVis datasets, surpassing strong image and video based baselines. Ablationsshow that temporal recurrence and gated fusion drive most of the gains. A framebudget study indicates a trade-off: 8 frames maximize fluency, whereas 32frames slightly reduce BLEU but improve numeric time estimation. By pairing atemporally aware encoder with fine grained gated cross-attention, SurgAnt-ViVQAadvances surgical VQA from retrospective description to proactive anticipation.PitVQA-Anticipation offers a comprehensive benchmark for this setting andhighlights the importance of targeted temporal modeling for reliable, futureaware surgical assistance.</description>
      <author>example@mail.com (Shreyas C. Dhake, Jiayuan Huang, Runlong He, Danyal Z. Khan, Evangelos B. Mazomenos, Sophia Bano, Hani J. Marcus, Danail Stoyanov, Matthew J. Clarkson, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03178v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26241v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究评估了视觉-语言模型(VLMs)对视频中时间信息的理解能力，发现当前模型在判断时间方向方面表现接近随机水平，远低于人类准确度。&lt;h4&gt;背景&lt;/h4&gt;现代视觉-语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的掌握仍然薄弱，且这一点尚未得到充分评估。&lt;h4&gt;目的&lt;/h4&gt;通过判断时间方向(AoT)的挑战来探测VLMs在时间理解方面的差距，即判断短片是正向播放还是反向播放。&lt;h4&gt;方法&lt;/h4&gt;引入AoT-PsyPhyBENCH基准测试，这是一个经过心理物理学验证的评估工具，使用与人类相同的刺激和行为基线来测试VLMs推断自然视频中时间方向的能力。&lt;h4&gt;主要发现&lt;/h4&gt;对各类VLMs的全面评估显示，大多数模型表现接近随机水平，即使在物理不可逆过程和因果手动操作上，表现最好的模型也远低于人类准确度，而人类能几乎立即识别这些过程。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了当前多模态系统的基本差距：虽然模型能捕捉丰富的视觉-语义相关性，但缺乏时间连续性和因果理解所需的归纳偏差。&lt;h4&gt;翻译&lt;/h4&gt;现代视觉-语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的掌握仍然薄弱，并且这一点尚未得到充分评估。我们通过一个看似简单但具有揭示性的挑战来探测这一差距：判断时间方向(AoT)——即判断短片是正向播放还是反向播放。我们引入了AoT-PsyPhyBENCH，这是一个经过心理物理学验证的基准测试，用于测试VLMs是否能推断自然视频中的时间方向，使用与人类相同的刺激和行为基线。对开源和专有、推理和非推理VLMs的全面评估表明，大多数模型的表现接近随机水平，即使在物理不可逆过程(如自由落体、扩散/爆炸)和因果手动操作(分割/添加)上，表现最好的模型也远低于人类的准确度，而这些过程人类几乎可以立即识别。这些结果突显了当前多模态系统中的一个基本差距：虽然它们捕捉了丰富的视觉-语义相关性，但缺乏时间连续性和因果理解所需的归纳偏差。我们发布了AoT-PsyPhyBENCH的代码和数据，以鼓励VLMs在物理和时间推理能力方面的进一步进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern vision-language models (VLMs) excel at many multimodal tasks, yettheir grasp of temporal information in video remains weak and, crucially,under-evaluated. We probe this gap with a deceptively simple but revealingchallenge: judging the arrow of time (AoT)-whether a short clip is playedforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validatedbenchmark that tests whether VLMs can infer temporal direction in naturalvideos using the same stimuli and behavioral baselines established for humans.Our comprehensive evaluation of open-weight and proprietary, reasoning andnon-reasoning VLMs reveals that most models perform near chance, and even thebest lag far behind human accuracy on physically irreversible processes (e.g.,free fall, diffusion/explosion) and causal manual actions (division/addition)that humans recognize almost instantly. These results highlight a fundamentalgap in current multimodal systems: while they capture rich visual-semanticcorrelations, they lack the inductive biases required for temporal continuityand causal understanding. We release the code and data for AoT-PsyPhyBENCH toencourage further progress in the physical and temporal reasoning capabilitiesof VLMs.</description>
      <author>example@mail.com (Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa)</author>
      <guid isPermaLink="false">2510.26241v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SVG Decomposition for Enhancing Large Multimodal Models Visualization Comprehension: A Study with Floor Plans</title>
      <link>http://arxiv.org/abs/2511.03478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了使用可缩放矢量图形(SVG)作为分解策略，以提高大型多模态模型(LMMs)对平面图理解能力的效果。研究分析了三种LMM模型在75个平面图上的表现，发现SVG与光栅输入结合可提高空间理解任务表现，但可能阻碍空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;大型多模态模型(LMMs)虽然越来越能够解释可视化，但在空间推理方面仍然存在困难。平面图是一个有价值的测试平台，因为它们结合了几何、拓扑和语义，且其可靠理解对盲人和低视力人士的无障碍服务有实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;研究可缩放矢量图形(SVG)作为一种分解策略，以提高LMMs对平面图理解的效果。&lt;h4&gt;方法&lt;/h4&gt;进行了一项探索性研究，使用了三个LMMs（GPT-4o、Claude 3.7 Sonnet和Llama 3.2 11B Vision Instruct），分析了75个平面图的表现。&lt;h4&gt;主要发现&lt;/h4&gt;将SVG与光栅输入结合(SVG+PNG)可以提高空间理解任务的表现，但往往阻碍空间推理，特别是在路径查找方面。&lt;h4&gt;结论&lt;/h4&gt;这些发现突显了分解策略在推进空间可视化理解方面的潜力和局限性。&lt;h4&gt;翻译&lt;/h4&gt;大型多模态模型(LMMs)越来越能够解释可视化，但在空间推理方面仍然存在困难。一种提出的策略是分解，将复杂的可视化分解为结构化组件。在这项工作中，我们研究了可缩放矢量图形(SVG)作为一种分解策略的有效性，以提高LMMs对平面图理解的能力。平面图是一个有价值的测试平台，因为它们结合了几何、拓扑和语义，且其可靠理解有实际应用价值，例如为盲人和低视力人士提供无障碍服务。我们对三种LMMs（GPT-4o、Claude 3.7 Sonnet和Llama 3.2 11B Vision Instruct）进行了探索性研究，分析了75个平面图。结果表明，将SVG与光栅输入结合(SVG+PNG)可以提高空间理解任务的表现，但往往阻碍空间推理，特别是在路径查找方面。这些发现突显了分解策略在推进空间可视化理解方面的潜力和局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large multimodal models (LMMs) are increasingly capable of interpretingvisualizations, yet they continue to struggle with spatial reasoning. Oneproposed strategy is decomposition, which breaks down complex visualizationsinto structured components. In this work, we examine the efficacy of scalablevector graphics (SVGs) as a decomposition strategy for improving LMMs'performance on floor plans comprehension. Floor plans serve as a valuabletestbed because they combine geometry, topology, and semantics, and theirreliable comprehension has real-world applications, such as accessibility forblind and low-vision individuals. We conducted an exploratory study with threeLMMs (GPT-4o, Claude 3.7 Sonnet, and Llama 3.2 11B Vision Instruct) across 75floor plans. Results show that combining SVG with raster input (SVG+PNG)improves performance on spatial understanding tasks but often hinders spatialreasoning, particularly in pathfinding. These findings highlight both thepromise and limitations of decomposition as a strategy for advancing spatialvisualization comprehension.</description>
      <author>example@mail.com (Jeongah Lee, Ali Sarvghad)</author>
      <guid isPermaLink="false">2511.03478v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SurgViVQA: Temporally-Grounded Video Question Answering for Surgical Scene Understanding</title>
      <link>http://arxiv.org/abs/2511.03325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了SurgViVQA，一个专门用于手术领域的视频问答模型，能够捕捉时间连贯的事件而非孤立图像。该模型使用掩码视频-文本编码器融合视频和问题特征，捕捉运动和工具-组织交互等时间线索，由微调的大语言模型解码为连贯答案。作者还创建了REAL-Colon-VQA数据集进行评估，实验表明该模型在关键词准确性上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;当前手术领域的视频问答方法局限于静态图像特征，可用的数据集通常缺乏时间标注，忽略了准确解读手术程序所需的关键动态信息。手术视频问答旨在通过AI模型对时间连贯事件进行推理来增强手术中的理解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从静态图像扩展到动态手术场景的视频问答模型，捕捉运动和工具-组织交互等时间线索，提高AI模型对动态手术程序的解读能力。&lt;h4&gt;方法&lt;/h4&gt;提出SurgViVQA模型，使用掩码视频-文本编码器融合视频和问题特征，捕捉时间线索；创建REAL-Colon-VQA数据集，包括与运动相关的问题和诊断属性，以及重新表述或语义改变的问题形式来评估模型鲁棒性；在REAL-Colon-VQA和EndoVis18-VQA数据集上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SurgViVQA在关键词准确性上显著优于现有基于图像的VQA基准模型，在REAL-Colon-VQA上比PitVQA提高11%，在EndoVis18-VQA上提高9%。对问题的扰动研究证实了模型对问题表述变化的泛化能力和鲁棒性得到改善。&lt;h4&gt;结论&lt;/h4&gt;SurgViVQA和REAL-Colon-VQA数据集为手术视频问答中的时间感知理解提供了框架，使AI模型能够更有效地解读动态手术程序上下文。&lt;h4&gt;翻译&lt;/h4&gt;手术领域的视频问答旨在通过使AI模型能够对时间上连贯的事件进行推理，而不是孤立的帧，来增强手术中的理解。当前的方法局限于静态图像特征，可用的数据集通常缺乏时间标注，忽略了准确解读手术程序所需的关键动态信息。我们提出了SurgViVQA，一个手术视频问答模型，它将视觉推理从静态图像扩展到动态手术场景。它使用掩码视频-文本编码器来融合视频和问题特征，捕捉运动和工具-组织交互等时间线索，然后由微调的大语言模型解码为连贯的答案。为了评估其性能，我们创建了REAL-Colon-VQA，一个结肠镜视频数据集，包括与运动相关的问题和诊断属性，以及重新表述或语义改变的问题形式来评估模型的鲁棒性。在REAL-Colon-VQA和公共的EndoVis18-VQA数据集上的实验验证表明，SurgViVQA优于现有的基于图像的VQA基准模型，特别是在关键词准确性方面，在REAL-Colon-VQA上比PitVQA提高了11%，在EndoVis18-VQA上提高了9%。对问题的扰动研究进一步证实了模型对问题表述变化的泛化能力和鲁棒性得到了改善。SurgViVQA和REAL-Colon-VQA数据集为手术视频问答中的时间感知理解提供了框架，使AI模型能够更有效地解读动态手术程序上下文。代码和数据集可在https://github.com/madratak/SurgViVQA获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Question Answering (VideoQA) in the surgical domain aims to enhanceintraoperative understanding by enabling AI models to reason over temporallycoherent events rather than isolated frames. Current approaches are limited tostatic image features, and available datasets often lack temporal annotations,ignoring the dynamics critical for accurate procedural interpretation. Wepropose SurgViVQA, a surgical VideoQA model that extends visual reasoning fromstatic images to dynamic surgical scenes. It uses a Masked Video--Text Encoderto fuse video and question features, capturing temporal cues such as motion andtool--tissue interactions, which a fine-tuned large language model (LLM) thendecodes into coherent answers. To evaluate its performance, we curatedREAL-Colon-VQA, a colonoscopic video dataset that includes motion-relatedquestions and diagnostic attributes, as well as out-of-template questions withrephrased or semantically altered formulations to assess model robustness.Experimental validation on REAL-Colon-VQA and the public EndoVis18-VQA datasetshows that SurgViVQA outperforms existing image-based VQA benchmark models,particularly in keyword accuracy, improving over PitVQA by +11\% onREAL-Colon-VQA and +9\% on EndoVis18-VQA. A perturbation study on the questionsfurther confirms improved generalizability and robustness to variations inquestion phrasing. SurgViVQA and the REAL-Colon-VQA dataset provide a frameworkfor temporally-aware understanding in surgical VideoQA, enabling AI models tointerpret dynamic procedural contexts more effectively. Code and datasetavailable at https://github.com/madratak/SurgViVQA.</description>
      <author>example@mail.com (Mauro Orazio Drago, Luca Carlini, Pelinsu Celebi Balyemez, Dennis Pierantozzi, Chiara Lena, Cesare Hassan, Danail Stoyanov, Elena De Momi, Sophia Bano, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03325v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>MME-CC: A Challenging Multi-Modal Evaluation Benchmark of Cognitive Capacity</title>
      <link>http://arxiv.org/abs/2511.03146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MME-CC基准测试，用于评估多模态大语言模型在视觉信息处理方面的认知能力，通过16个模型的实验发现闭源模型整体领先，但空间和几何推理能力普遍较弱，并确定了常见错误模式。&lt;h4&gt;背景&lt;/h4&gt;随着推理模型的迅速扩展，多模态在人类认知中的重要作用日益凸显，但现有多模态基准测试要么过度强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致对MLLMs的认知能力评估不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态基准测试的局限性，引入MME-CC基准测试，系统评估MLLMs在视觉信息处理方面的认知能力。&lt;h4&gt;方法&lt;/h4&gt;创建MME-CC基准测试，将11个代表性推理任务组织为空间、几何和基于知识的三个基本类别，并对16个代表性MLLMs进行广泛实验。&lt;h4&gt;主要发现&lt;/h4&gt;闭源模型目前整体领先（如Gemini-2.5-Pro得分为42.66，GLM-4.5V为30.45）；空间和几何推理能力普遍较弱（≤30%）；常见错误包括方向错误、脆弱的跨视图身份持久性和对反事实指令的遵循不良；思维链通常遵循提取→推理→验证的三阶段过程，且严重依赖视觉提取。&lt;h4&gt;结论&lt;/h4&gt;希望这项工作能促使将MLLMs的认知能力作为评估和模型设计的中心。&lt;h4&gt;翻译&lt;/h4&gt;随着推理模型的迅速扩展，多模态在人类认知中的重要作用日益凸显，促使人们需要探索以视觉为中心的认知行为。然而，现有的多模态基准测试要么过度强调文本推理，要么未能系统地捕捉以视觉为中心的认知行为，导致对MLLMs的认知能力评估不足。为了解决这一局限性，我们引入了MME-CC（认知能力多模态评估基准），这是一个以视觉为基础的基准测试，将11个代表性的推理任务组织为三个基本类别：空间、几何和基于知识的推理，并提供了MLLMs在这些维度上认知能力的细粒度分析。基于MME-CC，我们对16个代表性的MLLMs进行了广泛的实验。我们的研究表明，闭源模型目前整体领先（例如，Gemini-2.5-Pro得分为42.66，而GLM-4.5V得分为30.45），而空间和几何推理能力普遍较弱（小于或等于30%）。我们进一步确定了常见的错误模式，包括方向错误、脆弱的跨视图身份持久性以及未能很好地遵循反事实指令，并观察到思维链通常遵循三阶段过程（提取→推理→验证），且严重依赖视觉提取。我们希望这项工作能够促使将MLLMs的认知能力作为评估和模型设计的中心。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As reasoning models scale rapidly, the essential role of multimodality inhuman cognition has come into sharp relief, driving a growing need to probevision-centric cognitive behaviors. Yet, existing multimodal benchmarks eitheroveremphasize textual reasoning or fall short of systematically capturingvision-centric cognitive behaviors, leaving the cognitive capacity of MLLMsinsufficiently assessed. To address this limitation, we introduce MME-CC(Multi-Modal Evaluation benchmark of Cognitive Capacity), a vision-groundedbenchmark that organizes 11 representative reasoning tasks into threefundamental categories of visual information: spatial, geometric, andknowledge-based reasoning, and provides fine-grained analyses of MLLMs'cognitive capacity across these dimensions. Based on MME-CC, we conductextensive experiments over 16 representative MLLMs. Our study reveals thatclosed-source models currently lead overall (e.g., 42.66 for Gemini-2.5-Pro vs.30.45 for GLM-4.5V), while spatial and geometric reasoning remain broadly weak(less than or equal to 30%). We further identify common error patterns,including orientation mistakes, fragile cross-view identity persistence, andpoor adherence to counterfactual instructions, and observe thatChain-of-Thought typically follows a three-stage process (extract -&gt; reason -&gt;verify) with heavy reliance on visual extraction. We hope this work catalyzes ashift toward treating the cognitive capacity of MLLMs as central to bothevaluation and model design.</description>
      <author>example@mail.com (Kaiyuan Zhang, Chenghao Yang, Zhoufutu Wen, Sihang Yuan, Qiuyue Wang, Chaoyi Huang, Guosheng Zhu, He Wang, Huawenyu Lu, Jianing Wen, Jianpeng Jiao, Lishu Luo, Longxiang Liu, Sijin Wu, Xiaolei Zhu, Xuanliang Zhang, Ge Zhang, Yi Lin, Guang Shi, Chaoyou Fu, Wenhao Huang)</author>
      <guid isPermaLink="false">2511.03146v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>nanoTabPFN: A Lightweight and Educational Reimplementation of TabPFN</title>
      <link>http://arxiv.org/abs/2511.03634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;nanoTabPFN是一个简化的TabPFN v2架构实现，使表格基础模型对学生和研究人员更易访问，在小数据场景下性能与传统机器学习基线相当，且预训练速度快160,000倍。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型如TabPFN已革新表格数据的预测机器学习，但现有开源实现复杂（超过10,000行代码），缺乏架构文档和代码质量，难以理解和适应新实验。&lt;h4&gt;目的&lt;/h4&gt;解决现有表格基础模型实现复杂、难以理解、对初学者不友好的问题，使表格基础模型更易于教育和研究使用。&lt;h4&gt;方法&lt;/h4&gt;引入nanoTabPFN，作为TabPFN v2架构的简化轻量级实现，并使用预生成的训练数据实现相应的训练循环。&lt;h4&gt;主要发现&lt;/h4&gt;在小数据场景下，nanoTabPFN在单个GPU上进行一分钟预训练后性能与传统机器学习基线相当，预训练速度比TabPFN v2快160,000倍，消除了对大型计算资源的需求。&lt;h4&gt;结论&lt;/h4&gt;nanoTabPFN使表格基础模型更易于访问，其代码已在GitHub上提供（https://github.com/automl/nanoTabPFN）。&lt;h4&gt;翻译&lt;/h4&gt;表格基础模型如TabPFN已经革新了表格数据的预测机器学习。同时，这一革命的驱动因素难以理解。现有的开源表格基础模型在复杂的管道中实现，拥有超过10,000行代码，缺乏架构文档或代码质量。简而言之，这些实现难以理解，对初学者不友好，且难以适应新实验。我们引入了nanoTabPFN，这是TabPFN v2架构的简化和轻量级实现，以及使用预生成训练数据的相应训练循环。nanoTabPFN使表格基础模型对学生和研究人员都更加易于访问。例如，限制在小数据场景下，它在单个GPU上进行一分钟预训练后，实现了与传统机器学习基线相当的性能（比TabPFN v2预训练快160,000倍）。这种对大型计算资源需求的消除使表格基础模型的预训练可用于教育目的。我们的代码可在https://github.com/automl/nanoTabPFN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundation models such as TabPFN have revolutionized predictivemachine learning for tabular data. At the same time, the driving factors ofthis revolution are hard to understand. Existing open-source tabular foundationmodels are implemented in complicated pipelines boasting over 10,000 lines ofcode, lack architecture documentation or code quality. In short, theimplementations are hard to understand, not beginner-friendly, and complicatedto adapt for new experiments. We introduce nanoTabPFN, a simplified andlightweight implementation of the TabPFN v2 architecture and a correspondingtraining loop that uses pre-generated training data. nanoTabPFN makes tabularfoundation models more accessible to students and researchers alike. Forexample, restricted to a small data setting it achieves a performancecomparable to traditional machine learning baselines within one minute ofpre-training on a single GPU (160,000x faster than TabPFN v2 pretraining). Thiseliminated requirement of large computational resources makes pre-trainingtabular foundation models accessible for educational purposes. Our code isavailable at https://github.com/automl/nanoTabPFN.</description>
      <author>example@mail.com (Alexander Pfefferle, Johannes Hog, Lennart Purucker, Frank Hutter)</author>
      <guid isPermaLink="false">2511.03634v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>GUIDES: Guidance Using Instructor-Distilled Embeddings for Pre-trained Robot Policy Enhancement</title>
      <link>http://arxiv.org/abs/2511.03400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, Accepted by IEEE IROS 2025 Workshop WIR-M&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GUIDES是一个轻量级框架，通过注入基础模型的语义指导来增强预训练的机器人策略，无需重新设计架构。该方法结合了微调的视觉语言模型生成上下文指令，并通过辅助模块编码为引导嵌入，注入到策略的潜在空间中。此外，基于大语言模型的Reflector模块在推理时监控指导模型的置信度，并在置信度低时启动推理循环以增强鲁棒性。实验表明GUIDES能显著提高任务成功率并增强运动精度。&lt;h4&gt;背景&lt;/h4&gt;预训练的机器人策略是许多已验证的机器人系统的基础，包含大量具身知识。然而，它们通常缺乏基础模型特有的语义感知能力，而完全替换这些策略在许多情况下是不切实际的，因为成本高昂且会损失积累的知识。&lt;h4&gt;目的&lt;/h4&gt;解决预训练机器人策略缺乏语义感知能力的问题，同时避免完全替换这些策略带来的高成本和知识损失，提供一种实用且资源高效的升级途径。&lt;h4&gt;方法&lt;/h4&gt;GUIDES框架通过以下步骤实现：1)使用微调的视觉语言模型(Instructor)生成上下文指令；2)通过辅助模块将指令编码为引导嵌入；3)将嵌入注入到策略的潜在空间中；4)通过短暂微调使传统模型适应新语义输入；5)使用基于大语言模型的Reflector模块监控置信度；6)当置信度低时启动推理循环分析历史并优化后续行动。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboCasa仿真环境中对多种策略架构进行的广泛验证显示，GUIDES在任务成功率方面有一致且显著的提高。在UR5机器人上的实际部署进一步表明，GUIDES增强了抓取等关键子任务的运动精度。&lt;h4&gt;结论&lt;/h4&gt;GUIDES提供了一种实用且资源高效的途径来升级而非替换已验证的机器人策略，使传统策略能够获得基础模型的语义感知能力，同时保留其积累的知识和经验。&lt;h4&gt;翻译&lt;/h4&gt;预训练的机器人策略是许多已验证的机器人系统的基础，它们封装了大量具身知识。然而，它们通常缺乏基础模型特有的语义感知能力，并且在许多情况下完全替换它们是不切实际的，因为成本高昂且会损失积累的知识。为了解决这一差距，我们引入了GUIDES，这是一个轻量级框架，通过基础模型的语义指导增强预训练策略，而无需重新设计架构。GUIDES使用微调的视觉语言模型(Instructor)生成上下文指令，这些指令由辅助模块编码为引导嵌入。这些嵌入被注入到策略的潜在空间中，使传统模型能够通过短暂、有针对性的微调来适应这种新的语义输入。为了提高推理时的鲁棒性，基于大语言模型的Reflector模块监控Instructor的置信度，当置信度低时，启动一个推理循环，分析执行历史，检索相关示例，并增强VLM的上下文以细化后续行动。在RoboCasa仿真环境中对多种策略架构进行的广泛验证显示，任务成功率有一致且显著的提高。在UR5机器人上的实际部署进一步表明，GUIDES增强了抓取等关键子任务的运动精度。总体而言，GUIDES提供了一种实用且资源高效的途径来升级而非替换已验证的机器人策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained robot policies serve as the foundation of many validated roboticsystems, which encapsulate extensive embodied knowledge. However, they oftenlack the semantic awareness characteristic of foundation models, and replacingthem entirely is impractical in many situations due to high costs and the lossof accumulated knowledge. To address this gap, we introduce GUIDES, alightweight framework that augments pre-trained policies with semantic guidancefrom foundation models without requiring architectural redesign. GUIDES employsa fine-tuned vision-language model (Instructor) to generate contextualinstructions, which are encoded by an auxiliary module into guidanceembeddings. These embeddings are injected into the policy's latent space,allowing the legacy model to adapt to this new semantic input through brief,targeted fine-tuning. For inference-time robustness, a large languagemodel-based Reflector monitors the Instructor's confidence and, when confidenceis low, initiates a reasoning loop that analyzes execution history, retrievesrelevant examples, and augments the VLM's context to refine subsequent actions.Extensive validation in the RoboCasa simulation environment across diversepolicy architectures shows consistent and substantial improvements in tasksuccess rates. Real-world deployment on a UR5 robot further demonstrates thatGUIDES enhances motion precision for critical sub-tasks such as grasping.Overall, GUIDES offers a practical and resource-efficient pathway to upgrade,rather than replace, validated robot policies.</description>
      <author>example@mail.com (Minquan Gao, Xinyi Li, Qing Yan, Xiaojian Sun, Xiaopan Zhang, Chien-Ming Huang, Jiachen Li)</author>
      <guid isPermaLink="false">2511.03400v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>GMoPE:A Prompt-Expert Mixture Framework for Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2511.03251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出GMoPE框架，结合专家混合架构与基于提示的图学习，提高图神经网络跨领域泛化能力并降低适应成本&lt;h4&gt;背景&lt;/h4&gt;图神经网络在特定任务上表现优异，但跨领域和任务泛化能力有限，现有方法存在负迁移、可扩展性问题和高适应成本&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高图神经网络的泛化能力和效率&lt;h4&gt;方法&lt;/h4&gt;提出GMoPE框架，利用专家特定提示向量和结构感知的MoE路由，引入提示向量间软正交约束促进专家多样性，采用仅提示微调策略降低时空复杂度&lt;h4&gt;主要发现&lt;/h4&gt;实验表明GMoPE优于最先进基线，性能接近完整参数微调，但适应开销显著降低&lt;h4&gt;结论&lt;/h4&gt;GMoPE为推进可泛化和高效的图基础模型提供了有原则且可扩展的框架&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在特定任务基准上表现出色，但它们跨不同领域和任务的泛化能力仍然有限。现有方法通常难以处理负迁移、可扩展性问题和高适应成本。为应对这些挑战，我们提出了GMoPE（图专家混合提示），一种将专家混合架构与基于提示的图学习无缝集成的新框架。GMoPE利用专家特定的提示向量和结构感知的MoE路由，使每个专家能够专注于不同的子域，并动态贡献预测。为了促进多样性和防止专家坍塌，我们在提示向量之间引入了软正交约束，鼓励专家专业化并促进更平衡的专家利用。此外，我们采用仅提示微调策略，显著减少了迁移过程中的时空复杂度。我们通过各种预训练策略和多个下游任务的广泛实验验证了GMoPE。结果表明，GMoPE始终优于最先进的基线方法，并且实现了与完整参数微调相当的性能，同时只需要一小部分的适应开销。我们的工作为推进可泛化和高效的图基础模型提供了一个有原则且可扩展的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated impressive performance ontask-specific benchmarks, yet their ability to generalize across diversedomains and tasks remains limited. Existing approaches often struggle withnegative transfer, scalability issues, and high adaptation costs. To addressthese challenges, we propose GMoPE (Graph Mixture of Prompt-Experts), a novelframework that seamlessly integrates the Mixture-of-Experts (MoE) architecturewith prompt-based learning for graphs. GMoPE leverages expert-specific promptvectors and structure-aware MoE routing to enable each expert to specialize indistinct subdomains and dynamically contribute to predictions. To promotediversity and prevent expert collapse, we introduce a soft orthogonalityconstraint across prompt vectors, encouraging expert specialization andfacilitating a more balanced expert utilization. Additionally, we adopt aprompt-only fine-tuning strategy that significantly reduces spatiotemporalcomplexity during transfer. We validate GMoPE through extensive experimentsunder various pretraining strategies and multiple downstream tasks. Resultsshow that GMoPE consistently outperforms state-of-the-art baselines andachieves performance comparable to full parameter fine-tuning-while requiringonly a fraction of the adaptation overhead. Our work provides a principled andscalable framework for advancing generalizable and efficient graph foundationmodels.</description>
      <author>example@mail.com (Zhibin Wang, Zhixing Zhang, Shuqi Wang, Xuanting Xie, Zhao Kang)</author>
      <guid isPermaLink="false">2511.03251v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>SENT Map - Semantically Enhanced Topological Maps with Foundation Models</title>
      <link>http://arxiv.org/abs/2511.03165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025 Workshop on Foundation Models and  Neuro-Symbolic AI for Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SENT-Map是一种语义增强的拓扑地图，用于表示室内环境，通过基础模型支持机器人的自主导航和操作。&lt;h4&gt;背景&lt;/h4&gt;室内环境的表示对机器人自主导航和操作至关重要，基础模型的发展为环境表示和规划提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合语义信息的基础模型友好的室内环境表示方法，使机器人能够在规划过程中避免不可行状态。&lt;h4&gt;方法&lt;/h4&gt;采用两阶段方法：首先使用视觉基础模型与操作员一起映射环境；然后使用SENT-Map表示和自然语言查询在基础模型中进行规划。SENT-Map以JSON文本格式表示环境，使人类和基础模型都能理解并编辑语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;语义增强使即使是小型本地部署的基础模型也能够成功规划室内环境。&lt;h4&gt;结论&lt;/h4&gt;SENT-Map通过JSON文本格式表示环境，支持语义信息的添加和编辑，同时帮助机器人在规划过程中避免不可行状态。&lt;h4&gt;翻译&lt;/h4&gt;我们引入SENT-Map，一种用于表示室内环境的语义增强拓扑地图，旨在通过利用基础模型的进步来支持自主导航和操作。通过以JSON文本格式表示环境，我们能够以人类和基础模型都能理解的方式添加和编辑语义信息，同时在规划过程中将机器人与现有节点绑定，以避免部署过程中的不可行状态。我们提出的框架采用两阶段方法，首先使用视觉基础模型与操作员一起映射环境，然后使用SENT-Map表示和自然语言查询在基础模型中进行规划。我们的实验结果表明，语义增强使即使是小型本地部署的基础模型也能够成功规划室内环境。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让机器人在复杂人类环境中进行自主导航和操作的问题，同时利用基础模型的能力但避免其幻觉和虚假置信度等风险。这个问题很重要，因为基础模型虽提供了强大的语义理解和规划能力，但在实际应用中存在可靠性问题，而将基础模型与现实世界位置相结合并通过拓扑地图表示，可以让机器人更安全、可靠地执行任务。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过认识到基础模型的潜力与风险，设计了一个两阶段框架：映射阶段和规划/执行阶段。在映射阶段，人类引导机器人构建环境地图并标记语义节点；在规划阶段，利用基础模型生成任务计划。作者借鉴了现有工作如SLAM、视觉语言模型和对象中心映射方法，但解决了这些方法的高计算需求、缺乏操作推理、3D重建困难以及地图不可编辑等问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一种语义增强的拓扑地图(SENT-Map)，结合基础模型能力与人类可编辑的JSON表示，使机器人能理解环境语义并可靠执行任务。实现流程分两阶段：1)映射阶段：人类引导机器人探索环境，机器人拍摄关键位置图像，基础模型生成JSON格式的语义节点，人类可编辑完善；2)规划阶段：基础模型根据SENT-Map、技能描述和自然语言命令生成可执行计划，机器人执行这些计划完成导航和操作任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)SENT-Map：一种人类可编辑的JSON格式语义增强拓扑地图；2)使用基础模型构建和规划SENT-Map的框架；3)实验证明SENT-Map能提高基础模型规划成功率。相比之前工作，SENT-Map解决了高计算需求、缺乏操作保证、3D重建困难、地图不可验证等问题，通过JSON格式使地图可由人类验证和编辑，同时支持开放集语义增强和自然语言任务规范。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SENT-Map通过结合人类引导的基础模型和可编辑的JSON表示，使机器人能够在复杂环境中更可靠地理解和执行自然语言指令的导航和操作任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce SENT-Map, a semantically enhanced topological map forrepresenting indoor environments, designed to support autonomous navigation andmanipulation by leveraging advancements in foundational models (FMs). Throughrepresenting the environment in a JSON text format, we enable semanticinformation to be added and edited in a format that both humans and FMsunderstand, while grounding the robot to existing nodes during planning toavoid infeasible states during deployment. Our proposed framework employs a twostage approach, first mapping the environment alongside an operator with aVision-FM, then using the SENT-Map representation alongside a natural-languagequery within an FM for planning. Our experimental results show thatsemantic-enhancement enables even small locally-deployable FMs to successfullyplan over indoor environments.</description>
      <author>example@mail.com (Raj Surya Rajendran Kathirvel, Zach A Chavis, Stephen J. Guy, Karthik Desingh)</author>
      <guid isPermaLink="false">2511.03165v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Subsampled Randomized Fourier GaLore for Adapting Foundation Models in Depth-Driven Liver Landmark Segmentation</title>
      <link>http://arxiv.org/abs/2511.03163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种深度引导的肝脏地标分割框架，通过视觉基础编码器整合语义和几何线索，并引入SRFT-GaLore方法高效适应大型视觉模型，在腹腔镜肝脏手术中实现了精确的解剖结构检测和分割。&lt;h4&gt;背景&lt;/h4&gt;医学影像中解剖结构的精确检测和描绘对计算机辅助手术至关重要，尤其在腹腔镜肝脏手术中，2D视频流限制了深度感知，使地标定位复杂化。现有研究利用单目深度线索增强地标检测，但在融合RGB和深度特征以及高效调整大规模视觉模型方面仍有挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种深度引导的肝脏地标分割框架，整合语义和几何线索，高效适应大型视觉模型，并评估其跨数据集泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用Segment Anything Model V2 (SAM2)编码器提取RGB特征，Depth Anything V2 (DA2)编码器提取深度感知特征；引入SRFT-GaLore低秩梯度投影方法替代计算昂贵的SVD；实现交叉注意力融合模块整合RGB和深度线索；构建新的腹腔镜肝脏手术数据集(LLSD)作为外部验证基准。&lt;h4&gt;主要发现&lt;/h4&gt;在公共L3D数据集上，Dice相似系数提高4.85%，平均对称表面距离减少11.78个百分点；在LLSD数据集上，模型保持竞争性性能，显著优于基于SAM的基线，展示了强大的跨数据集鲁棒性和对未见手术环境的适应性。&lt;h4&gt;结论&lt;/h4&gt;SRFT-GaLore增强的双编码器框架能够在实时、深度受限的手术设置中实现可扩展和精确的分割。&lt;h4&gt;翻译&lt;/h4&gt;医学影像中解剖结构的精确检测和描绘对计算机辅助手术至关重要，特别是在腹腔镜肝脏手术中，2D视频流限制了深度感知并使地标定位复杂化。虽然最近的工作已利用单目深度线索增强地标检测，但在融合RGB和深度特征以及高效调整大规模视觉模型以适应手术领域方面仍存在挑战。我们提出了一种深度引导的肝脏地标分割框架，通过视觉基础编码器整合语义和几何线索。我们采用Segment Anything Model V2 (SAM2)编码器提取RGB特征，Depth Anything V2 (DA2)编码器提取深度感知特征。为高效适应SAM2，我们引入了SRFT-GaLore，一种新颖的低秩梯度投影方法，用子采样随机傅里叶变换(SRFT)替代计算昂贵的SVD。这能够在不牺牲表征能力的情况下高效微调高维注意力层。交叉注意力融合模块进一步整合RGB和深度线索。为评估跨数据集泛化能力，我们还构建了一个新的腹腔镜肝脏手术数据集(LLSD)作为外部验证基准。在公共L3D数据集上，我们的方法比D2GPLand提高了4.85%的Dice相似系数，并将平均对称表面距离降低了11.78个百分点。为进一步评估泛化能力，我们在LLSD数据集上评估了我们的模型。我们的模型保持竞争性性能，并显著优于基于SAM的基线，展示了强大的跨数据集鲁棒性和对未见手术环境的适应性。这些结果表明，我们的SRFT-GaLore增强的双编码器框架能够在实时、深度受限的手术设置中实现可扩展和精确的分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate detection and delineation of anatomical structures in medicalimaging are critical for computer-assisted interventions, particularly inlaparoscopic liver surgery where 2D video streams limit depth perception andcomplicate landmark localization. While recent works have leveraged monoculardepth cues for enhanced landmark detection, challenges remain in fusing RGB anddepth features and in efficiently adapting large-scale vision models tosurgical domains. We propose a depth-guided liver landmark segmentationframework integrating semantic and geometric cues via vision foundationencoders. We employ Segment Anything Model V2 (SAM2) encoder to extract RGBfeatures and Depth Anything V2 (DA2) encoder to extract depth-aware features.To efficiently adapt SAM2, we introduce SRFT-GaLore, a novel low-rank gradientprojection method that replaces the computationally expensive SVD with aSubsampled Randomized Fourier Transform (SRFT). This enables efficientfine-tuning of high-dimensional attention layers without sacrificingrepresentational power. A cross-attention fusion module further integrates RGBand depth cues. To assess cross-dataset generalization, we also construct a newLaparoscopic Liver Surgical Dataset (LLSD) as an external validation benchmark.On the public L3D dataset, our method achieves a 4.85% improvement in DiceSimilarity Coefficient and a 11.78-point reduction in Average Symmetric SurfaceDistance compared to the D2GPLand. To further assess generalization capability,we evaluate our model on LLSD dataset. Our model maintains competitiveperformance and significantly outperforms SAM-based baselines, demonstratingstrong cross-dataset robustness and adaptability to unseen surgicalenvironments. These results demonstrate that our SRFT-GaLore-enhanceddual-encoder framework enables scalable and precise segmentation underreal-time, depth-constrained surgical settings.</description>
      <author>example@mail.com (Yun-Chen Lin, Jiayuan Huang, Hanyuan Zhang, Sergi Kavtaradze, Matthew J. Clarkson, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.03163v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Forecast2Anomaly (F2A): Adapting Multivariate Time Series Foundation Models for Anomaly Prediction</title>
      <link>http://arxiv.org/abs/2511.03149v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Forecast2Anomaly (F2A)的新框架，使时间序列基础模型(TSFMs)具备异常预测能力，通过联合预测-异常损失和检索增强生成(RAG)模块实现，能够在不更新模型的情况下跟踪演变的异常。&lt;h4&gt;背景&lt;/h4&gt;在不同现实世界、动态和复杂系统中预测多变量时间序列中的异常对于预防关键故障至关重要。现有方法仅适用于特定系统，无法随时间演变泛化到异常模式。尽管预训练的时间序列基础模型展示了强大的泛化和零样本预测能力，但其异常预测潜力尚未被开发。&lt;h4&gt;目的&lt;/h4&gt;开发一个新框架，使预训练的时间序列基础模型(TSFMs)具备异常预测能力，弥合TSFM零样本预测和零样本异常预测之间的差距。&lt;h4&gt;方法&lt;/h4&gt;F2A框架包含两个关键创新：1) 提出联合预测-异常损失，微调TSFMs以准确预测异常时间点的未来信号；2) 引入检索增强生成(RAG)模块，检索历史上相关的范围并基于它们进行条件预测，使模型在推理时能动态适应分布变化。&lt;h4&gt;主要发现&lt;/h4&gt;在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A持续优于最先进的方法，提供了可扩展的零样本异常预测解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过结合目标微调和动态检索，F2A成功将TSFM的零样本预测能力扩展到零样本异常预测，为实际应用提供了一种有效的异常预测解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要翻译：来自不同现实世界、动态和复杂系统的多变量时间序列中的异常预测（异常预测）对于预防关键故障至关重要，能够显著降低运营成本和人工劳动。然而，现有方法仅适用于特定系统，无法随时间演变泛化到异常模式。相比之下，预训练的时间序列基础模型(TSFMs)最近展示了强大的泛化和零样本预测能力。然而，它们在异常预测方面的潜力尚未被开发，因为异常预测与预测正常行为的任务根本不同。因此，我们提出了Forecast2Anomaly (F2A)，一个新颖的框架，通过两个关键创新使TSFMs具备异常预测能力。首先，我们提出了一种联合预测-异常损失，微调TSFMs以准确预测异常时间点的未来信号。其次，我们引入了检索增强生成(RAG)模块，检索历史上相关的范围并基于它们进行条件预测。该组件在推理时动态适应分布变化，使F2A能够在不更新模型的情况下跟踪演变的异常。通过结合目标微调和动态检索，F2A弥合了健壮的TSFM零样本预测和零样本异常预测之间的差距。在16个不同数据集和多个TSFM骨干网络上的广泛实验表明，F2A持续优于最先进的方法，为实际应用提供了可扩展的零样本异常预测解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting anomalies (anomaly prediction) in multivariate time series fromdifferent real-world, dynamic, and complex systems is vital for preemptingcritical failures, leading to a substantial minimization in operational costsand human labor. Yet, existing methods are limited to specific systems whilefailing to generalize to evolving anomaly patterns over time. In contrast,pretrained Time Series Foundation Models (TSFMs) have recently demonstratedstrong generalization and zero-shot forecasting capabilities. However, theirpotential remains untapped for anomaly prediction, a task fundamentallydifferent from forecasting normal behavior. Thus, we present Forecast2Anomaly(F2A), a novel framework that empowers TSFMs with anomaly prediction abilitiesthrough two key innovations. First, we propose a joint forecast-anomaly lossthat fine-tunes TSFMs to accurately forecast future signals even at anomaloustime points. Second, we introduce a Retrieval-Augmented Generation (RAG) modulethat retrieves historically relevant horizons and conditions predictions onthem. This component dynamically adapts to distributional shifts at inferencetime, enabling F2A to track evolving anomalies without requiring model updates.By combining targeted fine-tuning with dynamic retrieval, F2A bridges the gapbetween robust TSFM zero-shot forecasting and zero-shot anomaly prediction.Extensive experiments across 16 diverse datasets and multiple TSFM backbonesshow that F2A consistently outperforms state-of-the-art methods, offering ascalable, zero-shot anomaly prediction solution for real-world applications.</description>
      <author>example@mail.com (Atif Hassan, Tarun Kumar, Ashish Mishra, Sergey Serebryakov, Satish Kumar Mopur, Phanidhar Koganti, Murthy Chelankuri, Ramanagopal Vogety, Suparna Bhattacharya, Martin Foltin)</author>
      <guid isPermaLink="false">2511.03149v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>A Foundation Model for Brain MRI with Dynamic Modality Integration</title>
      <link>http://arxiv.org/abs/2511.03014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary work; results ongoing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种脑部MRI基础模型，能够处理不同成像序列的组合，使用单一编码器架构，通过可学习模态嵌入和条件层归一化技术，实现了对缺失模态的自适应处理，无需为每种模态单独建模。&lt;h4&gt;背景&lt;/h4&gt;传统脑部MRI分析方法通常需要为不同成像序列分别训练模型，当某些模态缺失或未见时，模型性能会显著下降，限制了临床应用的实际价值。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够灵活处理不同成像序列组合的脑部MRI基础模型，使其能够在模态缺失或未见的情况下保持良好性能，提高模型的实用性和适应性。&lt;h4&gt;方法&lt;/h4&gt;使用单个编码器架构，配备可学习模态嵌入和条件层归一化技术；采用考虑缺失模态的掩码自编码目标函数；应用方差-协方差正则化器稳定特征学习；在约60,000多中心MRI数据上通过自监督重建和模态插值进行训练；利用可学习模态嵌入引导特征提取。&lt;h4&gt;主要发现&lt;/h4&gt;模型能够有效处理不同模态组合；在序列缺失或未见时能够自适应调整；方差-协方差正则化器有助于稳定特征学习并提高表示多样性；初步结果显示该方法可行。&lt;h4&gt;结论&lt;/h4&gt;所提出的脑部MRI基础模型能够灵活处理不同成像序列组合，无需为每种模态单独建模，在模态缺失情况下仍能保持良好性能，代码和预训练模型已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种脑部MRI基础模型，它可以处理不同成像序列的组合。该模型使用一个带有可学习模态嵌入的编码器、条件层归一化，以及一个考虑缺失模态的掩码自编码目标函数。应用了方差-协方差正则化器来稳定特征学习并提高表示多样性。这种设计消除了为每种模态单独建模的需要，并允许网络在某些序列缺失或未见时进行自适应调整。模型在约60,000多中心MRI上通过自监督重建和模态插值进行训练，以学习灵活的表示。可学习的模态嵌入引导特征提取，使编码器能够适应不同的输入。我们描述了计划在脑肿瘤和多发性硬化症分割以及病变分类方面的评估，在各种模态设置下进行。初步结果显示该方法可行，并计划进行更多实验以更详细地研究其性能。所有代码和预训练模型可在https://github.com/BrainFM/brainfm获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决脑部MRI分析中不同成像序列组合不一致的问题。现实中，不同医院使用的MRI协议不同，导致可用的成像序列（如T1、T2、FLAIR等）存在差异，现有方法通常需要为每种模态组合训练单独的模型，效率低下且难以处理缺失或未见过的新模态组合。这个问题限制了医学影像分析的泛化能力和实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了多个现有工作：自然语言处理和计算机视觉中的掩码自编码器、医学影像的自监督学习方法（如Models Genesis）以及多模态MRI处理技术（如AMAES、M4oE、MoME和mmFormer）。作者首先分析了现有方法的局限性，然后设计了一个单一编码器结合可学习模态嵌入的架构，通过条件层归一化和掩码自编码目标使模型能够适应不同模态组合，并应用方差-协方差正则化来稳定特征学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用单一编码器处理不同模态组合的MRI数据，通过可学习的模态嵌入和条件层归一化使模型能够自适应不同输入。整体流程包括：1)数据准备与预处理，将MRI调整为统一大小并应用数据增强；2)模型架构，将MRI分割为3D块并添加模态和位置嵌入；3)使用条件层归一化的Transformer编码器处理可见块，轻量级解码器重建被掩盖块；4)训练目标结合掩码重建和方差-协方差正则化；5)下游任务适配时丢弃解码器，保留编码器。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)动态模态集成，可处理任意MRI序列组合；2)单一编码器架构，无需为每种模态组合创建单独模型；3)可学习模态嵌入，能泛化到未见过序列；4)条件层归一化，使编码器自适应不同模态；5)模态感知掩码重建，提高对缺失数据的鲁棒性；6)方差-协方差正则化，稳定特征学习。相比之前工作，它超越了AMAES的单模态限制，避免了MoME和M4oE需要专家网络的复杂性，比mmFormer更专注于广泛下游应用，且能处理未见过的模态组合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了BrainFM-MRI，一个能够动态适应不同MRI模态组合的基础模型，通过单一编码器和可学习模态嵌入解决了医疗成像中模态不一致和缺失的问题，为脑部MRI分析提供了更加灵活和鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a foundation model for brain MRI that can work with differentcombinations of imaging sequences. The model uses one encoder with learnablemodality embeddings, conditional layer normalization, and a masked autoencodingobjective that accounts for missing modalities. A variance-covarianceregularizer is applied to stabilize feature learning and improve representationdiversity. This design removes the need for separate models for each modalityand allows the network to adapt when some sequences are missing or unseen. Itis trained on about 60,000 multi-center MRIs using self-supervisedreconstruction and modality imputation to learn flexible representations. Alearnable modality embedding guides feature extraction so the encoder canadjust to different inputs. We describe our planned evaluation on brain tumorand multiple sclerosis segmentation, as well as lesion classification, undervarious modality settings. Preliminary results show that the method worksfeasibly, and further experiments are planned to study its performance in moredetail. All code and pretrained models are available athttps://github.com/BrainFM/brainfm</description>
      <author>example@mail.com (Minh Sao Khue Luu, Bair N. Tuchinov)</author>
      <guid isPermaLink="false">2511.03014v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Zero-shot data citation function classification using transformer-based large language models (LLMs)</title>
      <link>http://arxiv.org/abs/2511.02936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究应用开源大型语言模型Llama 3.1-405B为引用特定基因组数据集的出版物生成结构化的数据使用案例标签，并引入新的评估框架验证方法有效性。结果显示模型在零样本分类任务上达到0.674的F1分数，但面临数据可用性、提示过拟合等挑战。&lt;h4&gt;背景&lt;/h4&gt;近年来，确定特定数据集与包含这些数据集的科学文献之间关联的努力有所增加。当已知某出版物引用了某数据集后，探索该数据如何或为何被使用成为下一步逻辑。基于预训练转换器的大型语言模型进步为扩展文献中数据使用案例描述提供了新途径，避免了传统机器学习系统需要昂贵的手动标注和训练数据集开发。&lt;h4&gt;目的&lt;/h4&gt;应用开源LLM Llama 3.1-405B为已知包含特定基因组数据集的出版物生成结构化数据使用案例标签，并引入新的评估框架确定方法有效性。&lt;h4&gt;方法&lt;/h4&gt;使用开源大型语言模型Llama 3.1-405B生成结构化数据使用案例标签，针对引用特定基因组数据集的出版物。同时引入创新评估框架验证方法效果。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在没有预先定义类别的零样本数据引用分类任务上达到0.674的F1分数，表明使用大型语言模型描述数据使用案例具有良好前景。&lt;h4&gt;结论&lt;/h4&gt;尽管结果很有希望，但研究受到数据可用性、提示过拟合、计算基础设施和负责任性能评估成本等因素的限制，这些挑战需要在实际应用中加以解决。&lt;h4&gt;翻译&lt;/h4&gt;近年来，确定特定数据集与包含这些数据集的科学文献之间关联的努力有所增加。已知某出版物引用了某数据集后，下一步逻辑就是探索该数据是如何或为何被使用的。近年来基于预训练、转换器的大型语言模型（LLMs）的进步，为扩展已发表文献中数据使用案例的描述提供了潜在手段。这避免了传统机器学习系统需要昂贵的手动标记和训练数据集开发。在本研究中，我们应用开源LLM Llama 3.1-405B，为已知包含特定基因组数据集的出版物生成结构化的数据使用案例标签。我们还引入了一种新的评估框架来确定我们方法的有效性。我们的结果表明，基础模型在没有预先定义类别的零样本数据引用分类任务上可以达到0.674的F1分数。虽然结果很有前景，但我们的结果受到与数据可用性、提示过拟合、计算基础设施和进行负责任的性能评估所需成本相关的限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efforts have increased in recent years to identify associations betweenspecific datasets and the scientific literature that incorporates them. Knowingthat a given publication cites a given dataset, the next logical step is toexplore how or why that data was used. Advances in recent years withpretrained, transformer-based large language models (LLMs) offer potentialmeans for scaling the description of data use cases in the publishedliterature. This avoids expensive manual labeling and the development oftraining datasets for classical machine-learning (ML) systems. In this work weapply an open-source LLM, Llama 3.1-405B, to generate structured data use caselabels for publications known to incorporate specific genomic datasets. We alsointroduce a novel evaluation framework for determining the efficacy of ourmethods. Our results demonstrate that the stock model can achieve an F1 scoreof .674 on a zero-shot data citation classification task with no previouslydefined categories. While promising, our results are qualified by barriersrelated to data availability, prompt overfitting, computational infrastructure,and the expense required to conduct responsible performance evaluation.</description>
      <author>example@mail.com (Neil Byers, Ali Zaidi, Valerie Skye, Chris Beecroft, Kjiersten Fagnan)</author>
      <guid isPermaLink="false">2511.02936v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
      <link>http://arxiv.org/abs/2511.02834v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, 14 tables. Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Agent-Omni框架通过主代理系统协调现有基础模型，实现灵活的多模态推理而无需重新训练，在多种模态和跨模态推理任务上取得最先进性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)目前只能处理固定模态对，需要大量对齐数据进行昂贵的微调，构建完全全能的模型(能整合文本、图像、音频和视频)仍然不切实际且缺乏强大的推理支持。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够灵活处理多种模态推理的框架，无需重新训练模型，并能整合现有基础模型的能力。&lt;h4&gt;方法&lt;/h4&gt;提出Agent-Omni框架，通过主代理系统协调现有基础模型，主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。&lt;h4&gt;主要发现&lt;/h4&gt;Agent-Omni在文本、图像、音频、视频和全能基准测试中持续取得最先进的性能，特别是在需要复杂跨模态推理的任务上表现出色；基于代理的设计实现了专业基础模型的无缝集成，确保对不同输入的适应性，同时保持透明性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;Agent-Omni框架是模块化和可扩展的，允许随着更强大模型的可用性进行未来改进，为多模态推理提供了灵活且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已展现出强大的能力，但仍局限于固定的模态对，并且需要使用大型对齐数据集进行昂贵的微调。构建能够整合文本、图像、音频和视频的完全全能模型仍然不切实际，且缺乏强大的推理支持。在本文中，我们提出了一个Agent-Omni框架，通过主代理系统协调现有基础模型，实现灵活的多模态推理而无需重新训练。主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。在文本、图像、音频、视频和全能基准测试中的大量实验表明，Agent-Omni持续取得最先进的性能，特别是在需要复杂跨模态推理的任务上。其基于代理的设计实现了专业基础模型的无缝集成，确保对不同输入的适应性，同时保持透明性和可解释性。此外，该框架是模块化和可扩展的，允许随着更强模型的可用性进行未来改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong capabilities butremain limited to fixed modality pairs and require costly fine-tuning withlarge aligned datasets. Building fully omni-capable models that can integratetext, images, audio, and video remains impractical and lacks robust reasoningsupport. In this paper, we propose an Agent-Omni framework that coordinatesexisting foundation models through a master-agent system, enabling flexiblemultimodal reasoning without retraining. The master agent interprets userintent, delegates subtasks to modality-specific agents, and integrates theiroutputs into coherent responses. Extensive experiments across text, image,audio, video, and omni benchmarks show that Agent-Omni consistently achievesstate-of-the-art performance, particularly on tasks requiring complexcross-modal reasoning. Its agent-based design enables seamless integration ofspecialized foundation models, ensuring adaptability to diverse inputs whilemaintaining transparency and interpretability. In addition, the framework ismodular and easily extensible, allowing future improvements as stronger modelsbecome available.</description>
      <author>example@mail.com (Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh)</author>
      <guid isPermaLink="false">2511.02834v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>PLUTO-4: Frontier Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02826v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PLUTO-4是病理学基础模型的下一代版本，包含PLUTO-4S和PLUTO-4G两种互补架构，在多种病理学任务上实现了最先进的性能，具有实际应用潜力。&lt;h4&gt;背景&lt;/h4&gt;基于大规模病理图像语料库训练的基础模型已显示出在多样组织病理学任务中的强大迁移能力，PLUTO是病理学通用转换器模型。&lt;h4&gt;目的&lt;/h4&gt;介绍PLUTO-4，扩展PLUTO到前沿规模，并提供两种互补的视觉转换器架构。&lt;h4&gt;方法&lt;/h4&gt;PLUTO-4S使用FlexiViT设置进行多尺度部署，采用2D-RoPE嵌入；PLUTO-4G使用单一补丁大小训练；两者均使用基于DINOv2的自监督目标在包含551,164个WSI的多机构语料库上预训练；在公共和内部基准上评估性能。&lt;h4&gt;主要发现&lt;/h4&gt;PLUTO-4在补丁级分类、分割和幻灯片级诊断等任务上实现最先进性能；PLUTO-4S提供高吞吐量和稳健性能；PLUTO-4G在多个病理学基准上建立新性能前沿，皮肤病理学诊断提高11%。&lt;h4&gt;结论&lt;/h4&gt;PLUTO-4的多样化改进强调了其作为转化研究和诊断用例主干，改变现实世界应用的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在大型病理图像语料库上训练的基础模型已在多种组织病理学任务中展现出强大的迁移能力。在此基础上，我们介绍了PLUTO-4，这是我们的病理学基础模型下一代版本，将病理学通用转换器(PLUTO)扩展到前沿规模。我们在PLUTO-4家族中分享了两种互补的视觉转换器架构：紧凑高效的PLUTO-4S模型，使用FlexiViT设置进行多尺度部署，采用2D-RoPE嵌入；以及前沿规模的PLUTO-4G模型，使用单一补丁大小训练以最大化表示能力和稳定性。两种模型都使用从DINOv2衍生的自监督目标进行预训练，在包含来自137,144名患者的551,164个WSI的多机构语料库上训练，涵盖50多个机构、60多种疾病类型和100多种染色方法。在公共和内部基准上的全面评估表明，PLUTO-4在需要不同空间和生物学背景的任务上实现了最先进的性能，包括补丁级分类、分割和幻灯片级诊断。紧凑的PLUTO-4S为实际部署提供高吞吐量和稳健性能，而PLUTO-4G在多个病理学基准上建立了新的性能前沿，包括在皮肤病理学诊断上提高11%。这些多样化的改进强调了PLUTO-4作为转化研究和诊断用例主干，改变现实世界应用的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale pathology image corpora havedemonstrated strong transfer capabilities across diverse histopathology tasks.Building on this progress, we introduce PLUTO-4, our next generation ofpathology foundation models that extend the Pathology-Universal Transformer(PLUTO) to frontier scale. We share two complementary Vision Transformerarchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S modeloptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPEembeddings, and a frontier-scale PLUTO-4G model trained with a single patchsize to maximize representation capacity and stability. Both models arepretrained using a self-supervised objective derived from DINOv2 on a largemulti-institutional corpus containing 551,164 WSIs from 137,144 patients acrossover 50 institutions, spanning over 60 disease types and over 100 stains.Comprehensive evaluation across public and internal benchmarks demonstratesthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varyingspatial and biological context, including patch-level classification,segmentation, and slide-level diagnosis. The compact PLUTO-4S provideshigh-throughput and robust performance for practical deployment, while PLUTO-4Gestablishes new performance frontiers across multiple pathology benchmarks,including an 11% improvement in dermatopathology diagnosis. These diverseimprovements underscore PLUTO-4's potential to transform real-worldapplications as a backbone for translational research and diagnostic use cases.</description>
      <author>example@mail.com (Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed)</author>
      <guid isPermaLink="false">2511.02826v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02802v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The library is open source and available at  https://github.com/Lexsi-Labs/TabTune&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TabTune是一个统一库，旨在标准化表格基础模型的完整工作流程，通过单一接口提供一致的服务，解决了表格基础模型采用中的主要障碍，包括异构预处理管道、碎片化API、不一致微调程序和缺乏标准化评估等问题。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型是结构化数据学习中的一个新兴范式，将大规模预训练的好处扩展到表格领域，但其采用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的库来标准化表格基础模型的完整工作流程，通过单一接口提供一致的服务。&lt;h4&gt;方法&lt;/h4&gt;提出TabTune库，提供对七种最先进模型的一致访问，支持零样本推理、元学习、监督微调和参数高效微调等多种适应策略；框架自动化模型感知预处理，内部管理架构异构性，并集成性能、校准和公平性的评估模块。&lt;h4&gt;主要发现&lt;/h4&gt;表格基础模型的采用受到异构预处理管道、碎片化API、不一致微调程序和缺乏针对校准和公平性等部署导向指标的标准化评估的限制。&lt;h4&gt;结论&lt;/h4&gt;TabTune通过提供可扩展且可重现的框架，能够对表格基础模型的适应策略进行一致的基准测试，解决了表格基础模型采用中的主要障碍。&lt;h4&gt;翻译&lt;/h4&gt;表格基础模型代表了结构化数据学习中的一个不断增长的范式，将大规模预训练的好处扩展到表格领域。然而，由于异构预处理管道、碎片化API、不一致的微调程序以及缺乏针对校准和公平性等部署导向指标的标准化评估，它们的采用仍然有限。我们提出了TabTune，这是一个通过单一界面标准化表格基础模型完整工作流程的统一库。TabTune提供对七种最先进模型的一致访问，支持多种适应策略，包括零样本推理、元学习、监督微调(SFT)和参数高效微调(PEFT)。该框架自动化模型感知预处理，内部管理架构异构性，并集成性能、校准和公平性的评估模块。TabTune设计为可扩展和可重现，能够对表格基础模型的适应策略进行一致的基准测试。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundation models represent a growing paradigm in structured datalearning, extending the benefits of large-scale pretraining to tabular domains.However, their adoption remains limited due to heterogeneous preprocessingpipelines, fragmented APIs, inconsistent fine-tuning procedures, and theabsence of standardized evaluation for deployment-oriented metrics such ascalibration and fairness. We present TabTune, a unified library thatstandardizes the complete workflow for tabular foundation models through asingle interface. TabTune provides consistent access to seven state-of-the-artmodels supporting multiple adaptation strategies, including zero-shotinference, meta-learning, supervised fine-tuning (SFT), and parameter-efficientfine-tuning (PEFT). The framework automates model-aware preprocessing, managesarchitectural heterogeneity internally, and integrates evaluation modules forperformance, calibration, and fairness. Designed for extensibility andreproducibility, TabTune enables consistent benchmarking of adaptationstrategies of tabular foundation models.</description>
      <author>example@mail.com (Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu)</author>
      <guid isPermaLink="false">2511.02802v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Discourse-Aware Scientific Paper Recommendation via QA-Style Summarization and Multi-Level Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.03330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了OMRC-MR框架，一种结合问答式OMRC摘要、多级对比学习和结构感知重排序的层次化方法，用于科学论文推荐，显著提高了推荐的精确度和召回率。&lt;h4&gt;背景&lt;/h4&gt;开放获取出版物的快速增长使识别相关科学论文更具挑战性。由于隐私限制和用户交互数据有限访问，研究转向基于内容的推荐方法，但这些方法通常将论文视为非结构化文本，忽略了话语组织结构，限制了语义完整性和可解释性。&lt;h4&gt;目的&lt;/h4&gt;开发能够捕捉论文结构化信息的内容推荐方法，提高科学论文推荐的精确度和召回率，同时增强推荐结果的可解释性。&lt;h4&gt;方法&lt;/h4&gt;OMRC-MR框架包含三个主要模块：问答式OMRC摘要模块将原始论文转换为结构化表示；多级对比学习目标在元数据、章节和文档级别对齐语义表示；结构感知重排序阶段通过上下文相似度校准优化检索精确度。&lt;h4&gt;主要发现&lt;/h4&gt;在DBLP、S2ORC和Sci-OMRC数据集上的实验表明，OMRC-MR在Precision@10和Recall@10上分别实现了高达7.2%和3.8%的改进。问答式摘要产生了更连贯和事实完整的表示。&lt;h4&gt;结论&lt;/h4&gt;OMRC-MR为科学论文推荐提供了一个统一且可解释的内容范式，推进了可信和隐私感知的学术信息检索。&lt;h4&gt;翻译&lt;/h4&gt;开放获取出版物的快速增长加剧了识别相关科学论文的挑战。由于隐私限制和用户交互数据的有限访问，近期努力转向了基于内容的推荐，这完全依赖于文本信息。然而，现有模型通常将论文视为非结构化文本，忽略了它们的话语组织，从而限制了语义完整性和可解释性。为了解决这些限制，我们提出了OMRC-MR，这是一个层次化框架，集成了问答式OMRC摘要、多级对比学习和结构感知重排序用于学术推荐。问答式摘要模块将原始论文转换为结构化和话语一致的表示，而多级对比目标在元数据、章节和文档级别对齐语义表示。最终的重排序阶段通过上下文相似度校准进一步优化检索精确度。在DBLP、S2ORC和新构建的Sci-OMRC数据集上的实验表明，OMRC-MR始终超越最先进的基线，在Precision@10和Recall@10上分别实现了高达7.2%和3.8%的改进。额外的评估确认问答式摘要产生了更连贯和事实完整的表示。总体而言，OMRC-MR为科学论文推荐提供了一个统一且可解释的内容范式，推进了可信和隐私感知的学术信息检索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of open-access (OA) publications has intensified thechallenge of identifying relevant scientific papers. Due to privacy constraintsand limited access to user interaction data, recent efforts have shifted towardcontent-based recommendation, which relies solely on textual information.However, existing models typically treat papers as unstructured text,neglecting their discourse organization and thereby limiting semanticcompleteness and interpretability. To address these limitations, we proposeOMRC-MR, a hierarchical framework that integrates QA-style OMRC (Objective,Method, Result, Conclusion) summarization, multi-level contrastive learning,and structure-aware re-ranking for scholarly recommendation. The QA-stylesummarization module converts raw papers into structured anddiscourse-consistent representations, while multi-level contrastive objectivesalign semantic representations across metadata, section, and document levels.The final re-ranking stage further refines retrieval precision throughcontextual similarity calibration. Experiments on DBLP, S2ORC, and the newlyconstructed Sci-OMRC dataset demonstrate that OMRC-MR consistently surpassesstate-of-the-art baselines, achieving up to 7.2% and 3.8% improvements inPrecision@10 and Recall@10, respectively. Additional evaluations confirm thatQA-style summarization produces more coherent and factually completerepresentations. Overall, OMRC-MR provides a unified and interpretablecontent-based paradigm for scientific paper recommendation, advancingtrustworthy and privacy-aware scholarly information retrieval.</description>
      <author>example@mail.com (Shenghua Wang, Zhen Yin)</author>
      <guid isPermaLink="false">2511.03330v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>An Augmentation Overlap Theory of Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.03114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了自监督对比学习的底层工作机制，提出了基于增强重叠的理论框架，并开发了与下游性能高度一致的无监督评估指标。&lt;h4&gt;背景&lt;/h4&gt;自监督对比学习在各种任务上取得了巨大成功，但其底层工作机制尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;探索对比学习的底层工作机制，特别是放松条件独立假设，研究更实际的增强重叠假设对下游性能的影响。&lt;h4&gt;方法&lt;/h4&gt;基于条件独立假设提供最紧的界限；将条件独立假设放松到增强重叠假设，推导下游性能的渐近闭合界限；提出增强重叠理论；开发用于对比学习表示评估的无监督指标。&lt;h4&gt;主要发现&lt;/h4&gt;不同类内样本的支撑集在激进数据增强下会变得更加重叠，简单对齐正样本可使对比学习将类内样本聚集在一起。&lt;h4&gt;结论&lt;/h4&gt;所提出的增强重叠理论解释了对比学习的机制，开发的无监督评估指标与下游性能高度一致，几乎不需要额外模块。&lt;h4&gt;翻译&lt;/h4&gt;最近，自监督对比学习在各种任务上取得了巨大成功。然而，其底层工作机制尚不清楚。在本文中，我们首先基于广泛采用的条件独立假设提供最紧的界限。进一步，我们将条件独立假设放松到更实际的增强重叠假设，并推导出下游性能的渐近闭合界限。我们提出的增强重叠理论基于这样的洞察：不同类内样本的支撑集在激进的数据增强下会变得更加重叠，因此简单地对齐正样本（同一样本的增强视图）可以使对比学习将类内样本聚集在一起。此外，从新推导的增强重叠角度，我们开发了一种用于对比学习表示评估的无监督指标，它与下游性能几乎完美一致，且几乎不需要依赖额外模块。代码可在https://github.com/PKU-ML/GARC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, self-supervised contrastive learning has achieved great success onvarious tasks. However, its underlying working mechanism is yet unclear. Inthis paper, we first provide the tightest bounds based on the widely adoptedassumption of conditional independence. Further, we relax the conditionalindependence assumption to a more practical assumption of augmentation overlapand derive the asymptotically closed bounds for the downstream performance. Ourproposed augmentation overlap theory hinges on the insight that the support ofdifferent intra-class samples will become more overlapped under aggressive dataaugmentations, thus simply aligning the positive samples (augmented views ofthe same sample) could make contrastive learning cluster intra-class samplestogether. Moreover, from the newly derived augmentation overlap perspective, wedevelop an unsupervised metric for the representation evaluation of contrastivelearning, which aligns well with the downstream performance almost withoutrelying on additional modules. Code is available athttps://github.com/PKU-ML/GARC.</description>
      <author>example@mail.com (Qi Zhang, Yifei Wang, Yisen Wang)</author>
      <guid isPermaLink="false">2511.03114v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Stochastic Deep Graph Clustering for Practical Group Formation</title>
      <link>http://arxiv.org/abs/2511.02879v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了DeepForm框架，解决群组推荐系统中的动态群组形成问题，满足高阶用户信息整合、实时群组形成和动态群组数量调整三个关键需求。&lt;h4&gt;背景&lt;/h4&gt;现有群组推荐系统研究主要关注提高推荐准确性，但多数方法假设群组静态或预定义，不适合动态、真实场景。&lt;h4&gt;目的&lt;/h4&gt;重新将群组形成视为群组推荐系统的核心挑战，提出满足三个关键操作要求的框架：整合高阶用户信息、实时群组形成和动态调整群组数量。&lt;h4&gt;方法&lt;/h4&gt;DeepForm采用轻量级GCN架构捕获高阶结构信号，通过随机聚类学习实现无需重新训练的自适应群组重新配置，利用对比学习在动态条件下优化群组。&lt;h4&gt;主要发现&lt;/h4&gt;多个数据集实验表明，DeepForm在群组形成质量、效率和推荐准确性方面均优于各种基线方法。&lt;h4&gt;结论&lt;/h4&gt;DeepForm为动态场景下的群组形成提供了有效解决方案，同时保持了高质量的推荐性能。&lt;h4&gt;翻译&lt;/h4&gt;虽然先前关于群组推荐系统(GRSs)的研究主要集中在提高推荐准确性上，但大多数方法假设群组是静态或预定义的，使它们不适合动态、真实的场景。我们将群组形成重新定义为GRSs中的核心挑战，并提出了DeepForm（用于实际群组形成的随机深度图聚类），这是一个旨在满足三个关键操作要求的框架：(1)整合高阶用户信息，(2)实时群组形成，(3)动态调整群组数量。DeepForm采用轻量级GCN架构，有效捕获高阶结构信号。随机聚类学习使群组能够自适应重新配置而无需重新训练，同时对比学习在动态条件下优化群组。在多个数据集上的实验表明，与各种基线相比，DeepForm在群组形成质量、效率和推荐准确性方面都表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While prior work on group recommender systems (GRSs) has primarily focused onimproving recommendation accuracy, most approaches assume static or predefinedgroups, making them unsuitable for dynamic, real-world scenarios. We reframegroup formation as a core challenge in GRSs and propose DeepForm (StochasticDeep Graph Clustering for Practical Group Formation), a framework designed tomeet three key operational requirements: (1) the incorporation of high-orderuser information, (2) real-time group formation, and (3) dynamic adjustment ofthe number of groups. DeepForm employs a lightweight GCN architecture thateffectively captures high-order structural signals. Stochastic cluster learningenables adaptive group reconfiguration without retraining, while contrastivelearning refines groups under dynamic conditions. Experiments on multipledatasets demonstrate that DeepForm achieves superior group formation quality,efficiency, and recommendation accuracy compared with various baselines.</description>
      <author>example@mail.com (Junhyung Park, Hyungjin Kim, Seokho Ahn, Young-Duk Seo)</author>
      <guid isPermaLink="false">2511.02879v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Graph Cuts</title>
      <link>http://arxiv.org/abs/2511.02272v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;作者提出了一种统一的概率框架，作为谱聚类的可微分替代方法，涵盖广泛的图切类型，包括Normalized Cut。该框架提供了紧密的解析上界，具有闭合形式的前向和反向传播，为可扩展的、可微分的图分割提供了严格且数值稳定的基础。&lt;h4&gt;背景&lt;/h4&gt;概率松弛的图切作为谱聚类的可微分替代方法，可以在不进行特征分解的情况下实现端到端和在线学习。然而，先前的工作主要集中在RatioCut上，缺乏通用保证和有原则的梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的概率框架，涵盖广泛的图切类型，包括Normalized Cut，并提供紧密的解析上界和具有闭合形式的前向和反向传播。&lt;h4&gt;方法&lt;/h4&gt;构建统一的概率框架，通过积分表示和具有闭合形式前向和反向传播的高斯超几何函数，为期望离散切提供紧密的解析上界。&lt;h4&gt;主要发现&lt;/h4&gt;该框架提供了紧密的解析上界，具有闭合形式的前向和反向传播，使得算法在数值上更加稳定。&lt;h4&gt;结论&lt;/h4&gt;这些结果为可扩展的、可微分的图分割提供了严格且数值稳定的基础，涵盖了广泛的聚类和对比学习目标。&lt;h4&gt;翻译&lt;/h4&gt;概率松弛的图切为谱聚类提供了可微分的替代方案，无需特征分解即可实现端到端和在线学习，但先前的工作主要集中在RatioCut上，缺乏通用保证和有原则的梯度。我们提出了一个统一的概率框架，涵盖了广泛的图切类型，包括Normalized Cut。我们的框架通过积分表示和具有闭合形式前向和反向传播的高斯超几何函数，为期望离散切提供了紧密的解析上界。这些结果共同为可扩展的、可微分的图分割提供了严格且数值稳定的基础，涵盖了广泛的聚类和对比学习目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic relaxations of graph cuts offer a differentiable alternative tospectral clustering, enabling end-to-end and online learning withouteigendecompositions, yet prior work centered on RatioCut and lacked generalguarantees and principled gradients. We present a unified probabilisticframework that covers a wide class of cuts, including Normalized Cut. Ourframework provides tight analytic upper bounds on expected discrete cuts viaintegral representations and Gauss hypergeometric functions with closed-formforward and backward. Together, these results deliver a rigorous, numericallystable foundation for scalable, differentiable graph partitioning covering awide range of clustering and contrastive learning objectives.</description>
      <author>example@mail.com (Ayoub Ghriss)</author>
      <guid isPermaLink="false">2511.02272v2</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing composition-based materials property prediction by cross-modal knowledge transfer</title>
      <link>http://arxiv.org/abs/2511.03371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;晶体图神经网络在建模实验合成的化合物和具有未知合成可能性的假设材料方面具有广泛应用。相比之下，结构不可知预测算法可以探索化学空间中以前无法访问的领域。本研究提出了一种通过跨模态知识转移来增强基于成分的材料属性预测的通用方法。&lt;h4&gt;背景&lt;/h4&gt;晶体图神经网络广泛用于建模实验合成的化合物和未知合成可能性的假设材料。结构不可知预测算法则允许探索化学空间中以前无法访问的领域。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用方法，通过跨模态知识转移来增强基于成分的材料属性预测。&lt;h4&gt;方法&lt;/h4&gt;提出了两种公式：隐式转移涉及在多模态嵌入上预训练化学语言模型，而显式转移建议生成晶体结构并实现结构感知预测器。这些方法在LLM4Mat-Bench和MatBench任务上进行了基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在32个案例中的25个案例中达到了最先进的性能。此外，展示了化学语言模型的另一个建模方面——可解释性——如何通过应用博弈论方法受益，该方法能够纳入高阶特征交互。&lt;h4&gt;结论&lt;/h4&gt;通过跨模态知识转移的方法可以显著提高基于成分的材料属性预测性能，化学语言模型的可解释性可以通过博弈论方法得到增强。&lt;h4&gt;翻译&lt;/h4&gt;晶体图神经网络在建模实验合成的化合物和具有未知合成可能性的假设材料方面具有广泛应用。相比之下，结构不可知预测算法可以探索化学空间中以前无法访问的领域。在此，我们提出了一种通过跨模态知识转移来增强基于成分的材料属性预测的通用方法。提出了两种公式：隐式转移涉及在多模态嵌入上预训练化学语言模型，而显式转移建议生成晶体结构并实现结构感知预测器。所提出的方法在LLM4Mat-Bench和MatBench任务上进行了基准测试，在32个案例中的25个案例中取得了最先进的性能。此外，我们展示了化学语言模型的另一个建模方面——可解释性——如何通过应用博弈论方法受益，该方法能够纳入高阶特征交互。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crystal graph neural networks are widely applicable in modelingexperimentally synthesized compounds and hypothetical materials with unknownsynthesizability. In contrast, structure-agnostic predictive algorithms allowexploring previously inaccessible domains of chemical space. Here we present auniversal approach for enhancing composition-based materials propertyprediction by means of cross-modal knowledge transfer. Two formulations areproposed: implicit transfer involves pretraining chemical language models onmultimodal embeddings, whereas explicit transfer suggests generating crystalstructures and implementing structure-aware predictors. The proposed approacheswere benchmarked on LLM4Mat-Bench and MatBench tasks, achievingstate-of-the-art performance in 25 out of 32 cases. In addition, wedemonstrated how another modeling aspect of chemical language models -interpretability - benefits from applying a game-theoretic approach, which isable to incorporate high-order feature interactions.</description>
      <author>example@mail.com (Ivan Rubtsov, Ivan Dudakov, Yuri Kuratov, Vadim Korolev)</author>
      <guid isPermaLink="false">2511.03371v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>GraphCliff: Short-Long Range Gating for Subtle Differences but Critical Changes</title>
      <link>http://arxiv.org/abs/2511.03170v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GraphCliff的新模型，通过门控机制整合短程和长程信息，解决了分子图嵌入在区分结构相似但功能不同分子时表现不佳的问题，从而提高了在活性悬崖和非悬崖化合物上的预测性能。&lt;h4&gt;背景&lt;/h4&gt;定量构效关系(QSAR)假设分子结构与生物活性之间存在平滑关系，但活性悬崖(结构相似但效力差异大的化合物对)会破坏这种连续性。最近的基准测试表明，具有扩展连接性指纹的经典机器学习模型在处理活性悬崖时优于图神经网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够保留分子图结构表示并有效区分结构相似但功能不同分子的模型，以提高在活性悬崖和非悬崖化合物上的预测性能。&lt;h4&gt;方法&lt;/h4&gt;提出名为GraphCliff的新模型，通过门控机制整合短程和长程信息，保留分子作为图的结构表示。&lt;h4&gt;主要发现&lt;/h4&gt;GraphCliff在非悬崖和悬崖化合物上都一致提高了性能；分层节点嵌入分析显示与强基线图模型相比，减少了过平滑并增强了判别能力。&lt;h4&gt;结论&lt;/h4&gt;GraphCliff成功解决了分子图嵌入在区分结构相似但功能不同分子时表现不佳的问题，同时保留了分子图结构的表达力，为处理活性悬崖问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;定量构效关系假设分子结构和生物活性之间存在平滑关系。然而，活性悬崖被定义为结构相似的化合物对，其效力差异很大，这破坏了这种连续性。最近针对活性悬崖的基准测试表明，具有扩展连接性指纹的经典机器学习模型优于图神经网络。我们的分析表明，图嵌入无法在嵌入空间中充分分离结构相似的分子，这使得难以区分结构相似但功能不同的分子。尽管存在这一限制，分子图结构本质上具有表达力且吸引人，因为它们保留了分子拓扑结构。为了保留分子作为图的结构表示，我们提出了一个新模型GraphCliff，它通过门控机制整合短程和长程信息。实验结果表明，GraphCliff在非悬崖和悬崖化合物上都一致提高了性能。此外，分层节点嵌入分析显示与强基线图模型相比，减少了过平滑并增强了判别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantitative structure-activity relationship assumes a smooth relationshipbetween molecular structure and biological activity. However, activity cliffsdefined as pairs of structurally similar compounds with large potencydifferences break this continuity. Recent benchmarks targeting activity cliffshave revealed that classical machine learning models with extended connectivityfingerprints outperform graph neural networks. Our analysis shows that graphembeddings fail to adequately separate structurally similar molecules in theembedding space, making it difficult to distinguish between structurallysimilar but functionally different molecules. Despite this limitation,molecular graph structures are inherently expressive and attractive, as theypreserve molecular topology. To preserve the structural representation ofmolecules as graphs, we propose a new model, GraphCliff, which integratesshort- and long-range information through a gating mechanism. Experimentalresults demonstrate that GraphCliff consistently improves performance on bothnon-cliff and cliff compounds. Furthermore, layer-wise node embedding analysesreveal reduced over-smoothing and enhanced discriminative power relative tostrong baseline graph models.</description>
      <author>example@mail.com (Hajung Kim, Jueon Park, Junseok Choe, Sheunheun Baek, Hyeon Hwang, Jaewoo Kang)</author>
      <guid isPermaLink="false">2511.03170v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Homomorphism distortion: A metric to distinguish them all and in the latent space bind them</title>
      <link>http://arxiv.org/abs/2511.03068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图相似性度量方法——图同态失真(graph homomorphism distortion)，它能够完全表征图，是一种完整的图嵌入。通过采样方法可以有效计算这一度量，并从中获得一个度量标准。实证表明，该方法能区分传统方法无法区分的图，并在特定数据集上优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;长期以来，图神经网络的表达能力仅通过组合性质来衡量，缺乏对图相似性的原则性测量方法。&lt;h4&gt;目的&lt;/h4&gt;提供一种测量顶点属性图相似性的原则性方法，并开发一种新的图相似性度量。&lt;h4&gt;方法&lt;/h4&gt;引入图同态失真(graph homomorphism distortion)作为相似性度量，证明它可以完全表征图，因此也是一种完整的图嵌入。为解决图规范化问题，通过采样方法有效计算这一度量，并从中获得一个度量标准。&lt;h4&gt;主要发现&lt;/h4&gt;图同态失真能够：(1)完全区分BREC数据集中的图，包括那些通过4-WL无法区分的图；(2)在ZINC-12k数据集上，优于之前受同态启发的方法。&lt;h4&gt;结论&lt;/h4&gt;这些理论和实证结果为未来图的表征铺平了道路，将图论传统扩展到新的前沿领域。&lt;h4&gt;翻译&lt;/h4&gt;长期以来，图神经网络的表达能力仅通过组合性质来衡量。本文打破了这一传统，提供了一种测量顶点属性图相似性的原则性方法，我们将其称为图同态失真(graph homomorphism distortion)。我们证明它可以完全表征图，因此也是一种完整的图嵌入。然而，在研究过程中，我们遇到了图规范化问题。为克服这一障碍，我们设计了通过采样来有效计算这一度量的方法，期望上保证了完整性。此外，我们还发现可以从这一度量中获得一个度量标准。我们通过实证验证了我们的主张，发现图同态失真：(1)能完全区分BREC数据集中的图，包括那些通过4-WL无法区分的图；(2)在ZINC-12k数据集上，优于之前受同态启发的方法。这些理论结果（及其实证验证）为未来图的表征铺平了道路，将图论传统扩展到新的前沿领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For far too long, expressivity of graph neural networks has been measured\emph{only} in terms of combinatorial properties. In this work we stray awayfrom this tradition and provide a principled way to measure similarity betweenvertex attributed graphs. We denote this measure as the \emph{graphhomomorphism distortion}. We show it can \emph{completely characterize} graphsand thus is also a \emph{complete graph embedding}. However, somewhere alongthe road, we run into the graph canonization problem. To circumvent thisobstacle, we devise to efficiently compute this measure via sampling, which inexpectation ensures \emph{completeness}. Additionally, we also discovered thatwe can obtain a metric from this measure. We validate our claims empiricallyand find that the \emph{graph homomorphism distortion}: (1.) fullydistinguishes the \texttt{BREC} dataset with up to $4$-WL non-distinguishablegraphs, and (2.) \emph{outperforms} previous methods inspired in homomorphismsunder the \texttt{ZINC-12k} dataset.  These theoretical results, (and their empirical validation), pave the way forfuture characterization of graphs, extending the graph theoretic tradition tonew frontiers.</description>
      <author>example@mail.com (Martin Carrasco, Olga Zaghen, Erik Bekkers, Bastian Rieck)</author>
      <guid isPermaLink="false">2511.03068v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Digital Twin-Driven Pavement Health Monitoring and Maintenance Optimization Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合数字孪生和图神经网络的创新方法，用于路面基础设施的智能监测和维护，解决了传统路面管理系统被动响应的问题，实现了主动维护和预测性规划。&lt;h4&gt;背景&lt;/h4&gt;路面基础设施监测面临复杂的空间依赖性、变化的环境条件和道路网络上的非线性退化等挑战。传统的路面管理系统(PMS)主要是被动的，缺乏故障预防和最优维护规划的实时智能。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的数字孪生(DT)和图神经网络(GNN)框架，用于可扩展、数据驱动的路面健康监测和预测性维护。&lt;h4&gt;方法&lt;/h4&gt;将路段和空间关系建模为图的节点和边，实时无人机、传感器和LiDAR数据流入数字孪生系统。归纳式GNN从图结构输入中学习退化模式以预测损坏。开发了交互式仪表板和强化学习模块用于模拟、可视化和自适应维护规划。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了0.3798的R²值，优于基线回归器，并有效捕获了非线性退化模式。&lt;h4&gt;结论&lt;/h4&gt;DT-GNN集成提高了预测精度并建立了持续改进的闭环反馈系统，为主动、智能和可持续的路面管理奠定了基础，未来将向实际部署、多智能体协调和智慧城市集成扩展。&lt;h4&gt;翻译&lt;/h4&gt;路面基础设施监测面临复杂的空间依赖性、变化的环境条件和道路网络上的非线性退化等挑战。传统的路面管理系统(PMS)主要是被动的，缺乏故障预防和最优维护规划的实时智能。为解决这一问题，我们提出了一个统一的数字孪生(DT)和图神经网络(GNN)框架，用于可扩展、数据驱动的路面健康监测和预测性维护。路段和空间关系被建模为图的节点和边，而实时无人机、传感器和LiDAR数据流入数字孪生系统。归纳式GNN从图结构输入中学习退化模式，以预测损坏并实现主动干预。在具有路段属性和动态连接的真实世界启发的数据集上训练，我们的模型实现了0.3798的R²值，优于基线回归器，并有效捕获了非线性退化。我们还开发了交互式仪表板和强化学习模块，用于模拟、可视化和自适应维护规划。这种DT-GNN集成提高了预测精度，并建立了持续改进的闭环反馈系统，将该方法定位为主动、智能和可持续路面管理的基础，未来将向实际部署、多智能体协调和智慧城市集成扩展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统路面管理系统(PMS)的被动维护问题，这些系统只在路面出现故障时才进行干预，缺乏实时智能来预防故障和优化维护计划。这个问题很重要，因为路面是现代交通系统的支柱，恶化路面会导致旅行时间延长、燃料消耗增加、车辆运营成本提高和交通事故风险增加。传统固定时间表的维护方法可能与实际退化轨迹不匹配，造成过早的结构性故障或过高的维护成本。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统路面管理系统的局限性，然后注意到数字孪生(DT)技术可以提供物理资产的详细实时虚拟表示，图神经网络(GNN)能够处理复杂的时空数据。作者借鉴了DT技术在多个领域的应用经验，以及GNN在分析相互连接系统方面的能力，将两者结合创建了一个统一框架。他们整合了现有的数据采集技术，如实时UAV、传感器和LiDAR数据，并利用了已有的路面监测和预测方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将数字孪生(DT)和图神经网络(GNN)集成到一个统一框架中，使用图结构表示路面段和空间关系，通过实时数据更新DT，并利用GNN学习退化模式。整体实现流程包括：1)数据合成与集成层，收集和预处理多源数据；2)图构建模块，将数据转换为动态图表示；3)模拟和分析引擎，使用有限元建模、无人机和LiDAR评估以及GNN预测分析；4)交互式维护和可视化系统，提供决策支持和可视化界面。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)集成数字孪生框架实现实时动态监测；2)开发基于GNN的预测模型捕获路面段间的时空依赖关系；3)DT-GNN集成支持交互式模拟和假设场景分析；4)全面的比较评估证明系统性能优于传统方法。相比之前工作，不同之处在于：之前研究主要关注孤立的DT应用或独立的GNN模型，而本文将两者统一集成，实现了从被动到主动维护的范式转变，建立了闭环反馈系统，提高了预测精度并促进了连续监测。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过集成数字孪生和图神经网络技术，建立了一个实时、数据驱动的路面健康监测和维护优化框架，实现了从被动到主动维护的范式转变，提高了预测准确性并延长了路面使用寿命。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pavement infrastructure monitoring is challenged by complex spatialdependencies, changing environmental conditions, and non-linear deteriorationacross road networks. Traditional Pavement Management Systems (PMS) remainlargely reactive, lacking real-time intelligence for failure prevention andoptimal maintenance planning. To address this, we propose a unified DigitalTwin (DT) and Graph Neural Network (GNN) framework for scalable, data-drivenpavement health monitoring and predictive maintenance. Pavement segments andspatial relations are modeled as graph nodes and edges, while real-time UAV,sensor, and LiDAR data stream into the DT. The inductive GNN learnsdeterioration patterns from graph-structured inputs to forecast distress andenable proactive interventions. Trained on a real-world-inspired dataset withsegment attributes and dynamic connectivity, our model achieves an R2 of0.3798, outperforming baseline regressors and effectively capturing non-lineardegradation. We also develop an interactive dashboard and reinforcementlearning module for simulation, visualization, and adaptive maintenanceplanning. This DT-GNN integration enhances forecasting precision andestablishes a closed feedback loop for continuous improvement, positioning theapproach as a foundation for proactive, intelligent, and sustainable pavementmanagement, with future extensions toward real-world deployment, multi-agentcoordination, and smart-city integration.</description>
      <author>example@mail.com (Mohsin Mahmud Topu, Mahfuz Ahmed Anik, Azmine Toushik Wasi, Md Manjurul Ahsan)</author>
      <guid isPermaLink="false">2511.02957v1</guid>
      <pubDate>Thu, 06 Nov 2025 15:25:40 +0800</pubDate>
    </item>
    <item>
      <title>Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 Pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个名为eval的框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。研究发现当前的数据过滤方法可能不太有效，被排除的知识可以通过微调恢复，且双重使用信号可能已经存在于预训练表示中。&lt;h4&gt;背景&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。它们虽然有望加速科学研究和药物开发，但也可能被恶意行为者用于开发更致命的生物武器。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为eval的框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;eval通过三种视角评估模型对病毒的理解：序列建模、突变效应预测和毒力预测。&lt;h4&gt;主要发现&lt;/h4&gt;当前过滤实践可能不太有效；在某些情况下，被排除的知识可以通过微调快速恢复；在序列建模中表现出更广泛的泛化能力；双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来提取。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了数据过滤作为独立程序的挑战，突显了对开放权重生物基础模型进行稳健安全和安全策略进一步研究的必要性。&lt;h4&gt;翻译&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。虽然它们在加速科学研究和药物开发方面具有巨大潜力，但也可能被恶意行为者用于开发更致命的生物武器。为了减轻这些模型带来的风险，当前的方法集中在预训练期间过滤生物危害数据。然而，这种方法的有效性仍然不清楚，特别是针对那些可能对这些模型进行微调以进行恶意使用的坚定行为者。为了解决这一差距，我们提出了eval，一个用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性的框架。eval通过三种视角评估模型对病毒的理解，包括序列建模、突变效应预测和毒力预测。我们的结果表明，当前的过滤实践可能不是特别有效的：在某些情况下，被排除的知识可以通过微调快速恢复，并且在序列建模中表现出更广泛的泛化能力。此外，双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来提取。这些发现强调了数据过滤作为独立程序的挑战，突显了对开放权重生物基础模型进行稳健安全和安全策略进一步研究的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-weight bio-foundation models present a dual-use dilemma. While holdinggreat promise for accelerating scientific research and drug development, theycould also enable bad actors to develop more deadly bioweapons. To mitigate therisk posed by these models, current approaches focus on filtering biohazardousdata during pre-training. However, the effectiveness of such an approachremains unclear, particularly against determined actors who might fine-tunethese models for malicious use. To address this gap, we propose \eval, aframework to evaluate the robustness of procedures that are intended to reducethe dual-use capabilities of bio-foundation models. \eval assesses models'virus understanding through three lenses, including sequence modeling,mutational effects prediction, and virulence prediction. Our results show thatcurrent filtering practices may not be particularly effective: Excludedknowledge can be rapidly recovered in some cases via fine-tuning, and exhibitsbroader generalizability in sequence modeling. Furthermore, dual-use signalsmay already reside in the pretrained representations, and can be elicited viasimple linear probing. These findings highlight the challenges of datafiltering as a standalone procedure, underscoring the need for further researchinto robust safety and security strategies for open-weight bio-foundationmodels.</description>
      <author>example@mail.com (Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika)</author>
      <guid isPermaLink="false">2510.27629v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
  <item>
      <title>Dynamic Reflections: Probing Video Representations with Text Alignment</title>
      <link>http://arxiv.org/abs/2511.02767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究进行了首次全面的视频-文本表征对齐研究，探索现代视频和语言编码器的能力，并提出了参数化的测试时缩放定律。&lt;h4&gt;背景&lt;/h4&gt;多模态表征对齐已被证明可以提供不同编码器在跨数据类型结构相似性和下游能力方面的见解。虽然图像与文本的对齐已取得显著进展，但视频数据的时序特性在这一背景下尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;进行首次全面的视频-文本表征对齐研究，探究现代视频和语言编码器的性能和能力。&lt;h4&gt;方法&lt;/h4&gt;研究视频-文本表征对齐，探究不同视觉和文本数据丰富度对跨模态对齐的影响，分析语义对齐与下游任务性能的关联，以及时序推理与跨模态对齐的关系。&lt;h4&gt;主要发现&lt;/h4&gt;1) 跨模态对齐高度依赖于测试时提供的视觉和文本数据的丰富程度；2) 提出的参数化测试时缩放定律具有显著的预测能力；3) 与文本编码器的强对齐可能与通用视频表征和理解相关；4) 时序推理与跨模态对齐的关联为视觉和语言模型提供了挑战性测试平台。&lt;h4&gt;结论&lt;/h4&gt;将视频-文本对齐引入为一种信息丰富的零样本方法，用于探测不同编码器在时空数据上的表征能力。&lt;h4&gt;翻译&lt;/h4&gt;不同模态表征的对齐最近已被证明能够提供关于不同编码器在跨数据类型结构相似性和下游能力方面的见解。虽然在对齐图像与文本方面已取得重大进展，但视频数据的时序特性在此背景下仍 largely 未经探索。在本工作中，我们进行了首次全面的视频-文本表征对齐研究，探究现代视频和语言编码器的能力。我们的发现揭示了几个关键见解。首先，我们证明跨模态对齐高度依赖于测试时提供的视觉（静态图像与多帧视频）和文本（单一标题与集合）数据的丰富程度，特别是使用最先进的视频编码器时。我们提出了捕捉这种行为的参数化测试时缩放定律，并显示出与经验观察相比显著的预测能力。其次，我们研究了语义对齐与语义和非语义下游任务性能之间的相关性，提供了初步证据，表明与文本编码器的强对齐可能与通用视频表征和理解相关。最后，我们将时序推理与跨模态对齐相关联，为视觉和语言模型提供了具有挑战性的测试平台。总体而言，我们的工作将视频-文本对齐引入为一种信息丰富的零样本方法，用于探测不同编码器在时空数据上的表征能力。项目页面可在 https://video-prh.github.io/ 找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The alignment of representations from different modalities has recently beenshown to provide insights on the structural similarities and downstreamcapabilities of different encoders across diverse data types. While significantprogress has been made in aligning images with text, the temporal nature ofvideo data remains largely unexplored in this context. In this work, we conductthe first comprehensive study of video-text representation alignment, probingthe capabilities of modern video and language encoders. Our findings revealseveral key insights. First, we demonstrate that cross-modal alignment highlydepends on the richness of both visual (static images vs. multi-frame videos)and text (single caption vs. a collection) data provided at test time,especially when using state-of-the-art video encoders. We propose parametrictest-time scaling laws that capture this behavior and show remarkablepredictive power against empirical observations. Secondly, we investigate thecorrelation between semantic alignment and performance on both semantic andnon-semantic downstream tasks, providing initial evidence that strong alignmentagainst text encoders may be linked to general-purpose video representation andunderstanding. Finally, we correlate temporal reasoning with cross-modalalignment providing a challenging test-bed for vision and language models.Overall, our work introduces video-text alignment as an informative zero-shotway to probe the representation power of different encoders for spatio-temporaldata. Project page can be found at https://video-prh.github.io/</description>
      <author>example@mail.com (Tyler Zhu, Tengda Han, Leonidas Guibas, Viorica Pătrăucean, Maks Ovsjanikov)</author>
      <guid isPermaLink="false">2511.02767v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>From the Laboratory to Real-World Application: Evaluating Zero-Shot Scene Interpretation on Edge Devices for Mobile Robotics</title>
      <link>http://arxiv.org/abs/2511.02427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI  International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18  DECEMBER 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究调查了先进的视觉语言模型(VLMs)在场景解释和动作识别任务上的能力，特别关注适合在移动机器人背景下部署到边缘设备的小型VLMs。&lt;h4&gt;背景&lt;/h4&gt;视频理解、场景解释和常识推理是使智能体能够解释视觉信息、感知环境、互动并做出决策的关键任务。大型语言模型和视觉语言模型在这些领域取得了显著进展，支持特定领域应用和零样本开放词汇任务，但计算复杂性限制了它们在边缘设备和移动机器人中的应用。&lt;h4&gt;目的&lt;/h4&gt;调查最先进的VLMs在场景解释和动作识别任务上的能力，特别关注能在移动机器人背景下部署到边缘设备的小型VLMs。&lt;h4&gt;方法&lt;/h4&gt;提出一个评估管道，并在多样化数据集上进行测试，包括各种真实世界城市景观、校园和室内场景。&lt;h4&gt;主要发现&lt;/h4&gt;讨论了小型模型在边缘设备上的潜力，特别强调了挑战、弱点、固有模型偏差以及获取信息的应用。&lt;h4&gt;结论&lt;/h4&gt;小型VLMs在边缘设备上部署具有潜力，但仍面临计算效率、准确性和推理时间之间的权衡挑战。&lt;h4&gt;翻译&lt;/h4&gt;视频理解、场景解释和常识推理是非常具有挑战性的任务，它们使智能体能够解释视觉信息，允许智能体在其环境中感知、互动并做出理性决策。大型语言模型和视觉语言模型近年来在这些领域显示出显著的进步，使特定领域的应用以及零样本开放词汇任务成为可能，并能够结合多个领域。然而，所需的计算复杂性对它们在边缘设备和移动机器人环境中的应用提出了挑战，特别是在考虑准确性和推理时间之间的权衡时。在本文中，我们调查了最先进的VLMs在场景解释和动作识别任务上的能力，特别关注能够在移动机器人背景下部署到边缘设备的小型VLMs。所提出的管道在各种包含不同真实世界城市景观、校园和室内场景的多样化数据集上进行了评估。实验评估讨论了这些小型模型在边缘设备上的潜力，特别强调了挑战、弱点、固有模型偏差以及获取信息的应用。补充材料可通过以下存储库获取：https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在边缘设备上部署小型视觉语言模型(VLMs)进行零样本场景解释的问题，特别是在移动机器人应用中。这个问题很重要，因为移动机器人需要在动态环境中自主运行，而视觉常识推理对机器人理解环境和做出决策至关重要。边缘设备上的本地解决方案对于无法保证外部服务可用性的场景特别重要，同时零样本能力允许开放域使用，不受限于预定义的动作集合，更接近真实世界场景的复杂性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了视觉常识推理的不同方法，特别是利用大型语言模型(LLMs)和视觉语言模型(VLMs)的优势。他们研究了各种模型，特别关注适用于边缘设备的小型模型(sVLMs)。作者借鉴了现有工作如ViCor(结合LLMs和VLMs的优势)、VLMaps(结合视觉语言特征与3D重建)等，设计了一个混合架构，结合本地边缘设备处理和云支持的优势，以平衡计算能力、隐私保护和实时性需求。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是在边缘设备上使用小型VLM进行场景解释，同时保持隐私并利用零样本能力处理开放词汇场景。整体实现流程是：1)边缘设备上的小型VLM生成最近时间间隔内图像序列的文本描述；2)生成的描述被分解为名词，用于提示零样本分割和跟踪；3)使用Grounded DINO和SAM进行零样本目标检测和分割；4)生成的描述可用于各种下游任务，如与本地或云端LLMs进行进一步推理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)专注于在边缘设备上部署小型VLMs进行零样本场景解释；2)提出结合本地边缘设备和云支持的混合架构；3)在多样化的真实世界数据集上评估小型VLMs的能力；4)研究不同场景域(校园室内、校园室外和城市)的性能差异；5)提出语义引导的分割方法，专注于描述中重要的元素。相比之前的工作，这种方法不依赖外部服务器进行所有处理，保护隐私；使用小型模型而非大型模型，更适合边缘计算环境；关注真实世界场景而非受控环境；提供了在移动机器人背景下应用VLMs的全面评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文评估并展示了小型视觉语言模型在边缘设备上进行零样本场景解释的可行性，为移动机器人在真实世界环境中提供了一种隐私保护的本地解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video Understanding, Scene Interpretation and Commonsense Reasoning arehighly challenging tasks enabling the interpretation of visual information,allowing agents to perceive, interact with and make rational decisions in itsenvironment. Large Language Models (LLMs) and Visual Language Models (VLMs)have shown remarkable advancements in these areas in recent years, enablingdomain-specific applications as well as zero-shot open vocabulary tasks,combining multiple domains. However, the required computational complexityposes challenges for their application on edge devices and in the context ofMobile Robotics, especially considering the trade-off between accuracy andinference time. In this paper, we investigate the capabilities ofstate-of-the-art VLMs for the task of Scene Interpretation and ActionRecognition, with special regard to small VLMs capable of being deployed toedge devices in the context of Mobile Robotics. The proposed pipeline isevaluated on a diverse dataset consisting of various real-world cityscape,on-campus and indoor scenarios. The experimental evaluation discusses thepotential of these small models on edge devices, with particular emphasis onchallenges, weaknesses, inherent model biases and the application of the gainedinformation. Supplementary material is provided via the following repository:https://datahub.rz.rptu.de/hstr-csrl-public/publications/scene-interpretation-on-edge-devices/</description>
      <author>example@mail.com (Nicolas Schuler, Lea Dewald, Nick Baldig, Jürgen Graf)</author>
      <guid isPermaLink="false">2511.02427v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>M3PD Dataset: Dual-view Photoplethysmography (PPG) Using Front-and-rear Cameras of Smartphones in Lab and Clinical Settings</title>
      <link>http://arxiv.org/abs/2511.02349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于智能手机的双视图光电容积描记术方法，通过融合面部和指尖视频数据，提高了心率监测的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;便携式生理监测对心血管疾病的早期检测和管理至关重要，但当前方法通常需要专业设备限制了可及性，或者要求患者保持不切实际的姿势。基于智能手机的视频光电容积描记术虽提供了便捷的无创替代方案，但仍面临运动伪影、光照变化和单视图限制等可靠性挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，引入首个公开可用的双视图移动光电容积描记术数据集，并提出一种融合双视图数据的新方法，提高心血管患者心率监测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;构建M3PD数据集，包含60名参与者（47名心血管患者）通过前置和后置智能手机摄像头同时采集的面部和指尖同步视频。基于此双视图设置，提出F3Mamba模型，通过基于Mamba的时间建模融合面部和指尖视图。&lt;h4&gt;主要发现&lt;/h4&gt;F3Mamba模型将心率误差比现有单视图基线降低了21.9%至30.2%，同时在具有挑战性的现实场景中提高了监测的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;双视图移动光电容积描记术结合先进的融合算法可有效提高心血管患者心率监测的准确性和可靠性，为便携式心血管健康监测提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;便携式生理监测对心血管疾病的早期检测和管理至关重要，但当前方法通常需要专业设备限制了可及性，或者要求患者保持不切实际的姿势。基于智能手机的视频光电容积描记术提供了便捷的无创替代方案，但仍面临由运动伪影、光照变化和单视图限制引起的可靠性挑战。很少有研究证明其在心血管患者中的可靠应用，且缺乏广泛使用的开放数据集用于跨设备准确性评估。为解决这些限制，我们引入了M3PD数据集，这是首个公开可用的双视图移动光电容积描记术数据集，包含通过前置和后置智能手机摄像头同时采集的60名参与者（包括47名心血管患者）的面部和指尖同步视频。基于这种双视图设置，我们进一步提出F3Mamba，通过基于Mamba的时间建模融合面部和指尖视图。该模型将心率误差比现有单视图基线降低了21.9%至30.2%，同时在具有挑战性的现实场景中提高了鲁棒性。数据和代码：https://github.com/Health-HCI-Group/F3Mamba。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Portable physiological monitoring is essential for early detection andmanagement of cardiovascular disease, but current methods often requirespecialized equipment that limits accessibility or impose impractical posturesthat patients cannot maintain. Video-based photoplethysmography on smartphonesoffers a convenient noninvasive alternative, yet it still faces reliabilitychallenges caused by motion artifacts, lighting variations, and single-viewconstraints. Few studies have demonstrated reliable application tocardiovascular patients, and no widely used open datasets exist forcross-device accuracy. To address these limitations, we introduce the M3PDdataset, the first publicly available dual-view mobile photoplethysmographydataset, comprising synchronized facial and fingertip videos capturedsimultaneously via front and rear smartphone cameras from 60 participants(including 47 cardiovascular patients). Building on this dual-view setting, wefurther propose F3Mamba, which fuses the facial and fingertip views throughMamba-based temporal modeling. The model reduces heart-rate error by 21.9 to30.2 percent over existing single-view baselines while improving robustness inchallenging real-world scenarios. Data and code:https://github.com/Health-HCI-Group/F3Mamba.</description>
      <author>example@mail.com (Jiankai Tang, Tao Zhang, Jia Li, Yiru Zhang, Mingyu Zhang, Kegang Wang, Yuming Hao, Bolin Wang, Haiyang Li, Xingyao Wang, Yuanchun Shi, Yuntao Wang, Sichong Qian)</author>
      <guid isPermaLink="false">2511.02349v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>LiCoMemory: Lightweight and Cognitive Agentic Memory for Efficient Long-Term Reasoning</title>
      <link>http://arxiv.org/abs/2511.01448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LiCoMemory是一种端到端代理记忆框架，通过引入CogniGraph轻量级分层图解决大型语言模型记忆限制问题，在长期对话任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型(LLM)代理具有出色的对话和推理能力，但受限于上下文窗口小和缺乏持久性记忆。&lt;h4&gt;目的&lt;/h4&gt;解决现有外部记忆架构中扁平、纠缠结构导致的冗余表示、非结构化检索以及效率和准确性下降问题。&lt;h4&gt;方法&lt;/h4&gt;提出LiCoMemory框架，引入CogniGraph轻量级分层图，利用实体和关系作为语义索引层，采用时间和层次感知搜索与集成重排序进行自适应知识检索。&lt;h4&gt;主要发现&lt;/h4&gt;在LoCoMo和LongMemEval基准测试上，LiCoMemory在时间推理、多会话一致性和检索效率方面优于基线模型，并显著降低了更新延迟。&lt;h4&gt;结论&lt;/h4&gt;LiCoMemory有效解决了大型语言模型的记忆限制问题，提高了长期对话任务中的性能和效率。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLM)代理展现出卓越的对话和推理能力，但仍然受限于有限的上下文窗口和持久性记忆的缺乏。最近的工作通过外部记忆架构解决这些限制，通常采用基于图的表示，但大多数采用扁平、纠缠的结构，将语义与拓扑交织在一起，导致冗余表示、非结构化检索以及效率和准确性的下降。为解决这些问题，我们提出了LiCoMemory，一个用于实时更新和检索的端到端代理记忆框架，它引入了CogniGraph，一种利用实体和关系作为语义索引层的轻量级分层图，并采用时间和层次感知搜索与集成重排序进行自适应和连贯的知识检索。在长期对话基准LoCoMo和LongMemEval上的实验表明，LiCoMemory不仅在时间推理、多会话一致性和检索效率方面优于既定的基线，而且显著降低了更新延迟。我们的官方代码和数据可在https://github.com/EverM0re/LiCoMemory获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Model (LLM) agents exhibit remarkable conversational andreasoning capabilities but remain constrained by limited context windows andthe lack of persistent memory. Recent efforts address these limitations viaexternal memory architectures, often employing graph-based representations, yetmost adopt flat, entangled structures that intertwine semantics with topology,leading to redundant representations, unstructured retrieval, and degradedefficiency and accuracy. To resolve these issues, we propose LiCoMemory, anend-to-end agentic memory framework for real-time updating and retrieval, whichintroduces CogniGraph, a lightweight hierarchical graph that utilizes entitiesand relations as semantic indexing layers, and employs temporal andhierarchy-aware search with integrated reranking for adaptive and coherentknowledge retrieval. Experiments on long-term dialogue benchmarks, LoCoMo andLongMemEval, show that LiCoMemory not only outperforms established baselines intemporal reasoning, multi-session consistency, and retrieval efficiency, butalso notably reduces update latency. Our official code and data are availableat https://github.com/EverM0re/LiCoMemory.</description>
      <author>example@mail.com (Zhengjun Huang, Zhoujin Tian, Qintian Guo, Fangyuan Zhang, Yingli Zhou, Di Jiang, Xiaofang Zhou)</author>
      <guid isPermaLink="false">2511.01448v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>DeepSpecs: Expert-Level Questions Answering in 5G</title>
      <link>http://arxiv.org/abs/2511.01305v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DeepSpecs是一个通过结构化和时间推理增强的RAG系统，通过三个元数据库（SpecDB、ChangeDB和TDocDB）解决5G标准文档中的交叉引用和演变问题，显著提高了回答5G规范专业级问题的能力。&lt;h4&gt;背景&lt;/h4&gt;5G技术为数十亿用户提供移动互联网接入，回答关于5G规范的专业级问题需要浏览数千页交叉引用的标准文档。现有的检索增强生成(RAG)框架依赖语义相似性，无法可靠地解决交叉引用或对规范演变进行推理。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理5G标准文档复杂性的系统，解决现有RAG框架在处理交叉引用和规范演变方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出DeepSpecs系统，使用三个元数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档）。通过元数据查找递归检索引用条款解决交叉引用，通过挖掘变化并链接到变更请求来跟踪规范演变。整理了两个5G问答数据集：573条专家注释的真实世界问题和350条演变问题。&lt;h4&gt;主要发现&lt;/h4&gt;DeepSpecs在多个LLM后端上优于基础模型和最先进的电信RAG系统。消融研究证实明确的交叉引用解决和演变感知检索显著提高了答案质量，强调了建模5G标准的结构和时间特性的价值。&lt;h4&gt;结论&lt;/h4&gt;DeepSpecs通过结构化和时间推理有效解决了5G标准文档的复杂性，显著提高了回答关于5G规范的专业级问题的能力。&lt;h4&gt;翻译&lt;/h4&gt;5G技术为数十亿用户提供了移动互联网接入。回答关于5G规范的专业级问题需要浏览数千页交叉引用的标准文档，这些标准在不同版本中不断演变。现有的检索增强生成(RAG)框架，包括电信特定方法，依赖语义相似性，无法可靠地解决交叉引用或对规范演变进行推理。我们提出了DeepSpecs，一个通过结构化和时间推理增强的RAG系统，通过三个丰富的元数据库：SpecDB（条款对齐的规范文本）、ChangeDB（行级版本差异）和TDocDB（标准化会议文档）。DeepSpecs通过元数据查找递归检索引用的条款，明确解决交叉引用，并通过挖掘变化并将它们链接到记录设计原理的变更请求来跟踪规范演变。我们整理了两个5G问答数据集：573条来自从业者论坛和教育资源的专家注释的真实世界问题，以及350条从已批准变更请求中演变而来的问题。在多个LLM后端上，DeepSpecs优于基础模型和最先进的电信RAG系统；消融研究证实明确的交叉引用解决和演变感知检索显著提高了答案质量，强调了建模5G标准的结构和时间特性的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 5G technology enables mobile Internet access for billions of users. Answeringexpert-level questions about 5G specifications requires navigating thousands ofpages of cross-referenced standards that evolve across releases. Existingretrieval-augmented generation (RAG) frameworks, including telecom-specificapproaches, rely on semantic similarity and cannot reliably resolvecross-references or reason about specification evolution. We present DeepSpecs,a RAG system enhanced by structural and temporal reasoning via threemetadata-rich databases: SpecDB (clause-aligned specification text), ChangeDB(line-level version diffs), and TDocDB (standardization meeting documents).DeepSpecs explicitly resolves cross-references by recursively retrievingreferenced clauses through metadata lookup, and traces specification evolutionby mining changes and linking them to Change Requests that document designrationale. We curate two 5G QA datasets: 573 expert-annotated real-worldquestions from practitioner forums and educational resources, and 350evolution-focused questions derived from approved Change Requests. Acrossmultiple LLM backends, DeepSpecs outperforms base models and state-of-the-arttelecom RAG systems; ablations confirm that explicit cross-reference resolutionand evolution-aware retrieval substantially improve answer quality,underscoring the value of modeling the structural and temporal properties of 5Gstandards.</description>
      <author>example@mail.com (Aman Ganapathy Manvattira, Yifei Xu, Ziyue Dang, Songwu Lu)</author>
      <guid isPermaLink="false">2511.01305v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>KAT-GNN: A Knowledge-Augmented Temporal Graph Neural Network for Risk Prediction in Electronic Health Records</title>
      <link>http://arxiv.org/abs/2511.01249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了KAT-GNN（知识增强型时序图神经网络）框架，用于基于电子健康记录的临床风险预测，通过整合临床知识和时序动态，在冠状动脉疾病预测和住院死亡率预测任务中取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;使用电子健康记录进行临床风险预测对于及时干预和临床决策支持至关重要。然而，建模异构和不规则的时序EHR数据存在重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效整合临床知识和时序动态的图神经网络框架，用于提高临床风险预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;KAT-GNN首先从EHR中构建特定模态的患者图，然后使用两种知识来源增强这些图：（1）来自SNOMED CT的本体驱动边；（2）从EHR中提取的共现先验。随后，采用时间感知transformer来捕捉图编码的患者表示中的纵向动态。&lt;h4&gt;主要发现&lt;/h4&gt;KAT-GNN在冠状动脉疾病预测中达到最先进性能（AUROC: 0.9269 ± 0.0029），在MIMIC-III（AUROC: 0.9230 ± 0.0070）和MIMIC-IV（AUROC: 0.8849 ± 0.0089）的死亡率预测中也表现出色，持续优于GRASP和RETAIN等基线模型。消融研究证实，基于知识的增强和时序建模组件都是性能提升的重要贡献者。&lt;h4&gt;结论&lt;/h4&gt;将临床知识整合到图表示中，结合时间感知注意力机制，为跨不同临床任务和数据集的风险预测提供了一种有效且可推广的方法。&lt;h4&gt;翻译&lt;/h4&gt;使用电子健康记录进行临床风险预测对于促进及时干预和临床决策支持至关重要。然而，建模异构和不规则的时序EHR数据存在重大挑战。我们提出了KAT-GNN（知识增强型时序图神经网络），一种基于图的框架，整合临床知识和时序动态用于风险预测。KAT-GNN首先从EHR中构建特定模态的患者图，然后使用两种知识来源增强这些图：（1）来自SNOMED CT的本体驱动边；（2）从EHR中提取的共现先验。随后，采用时间感知transformer来捕捉图编码的患者表示中的纵向动态。KAT-GNN在三个不同的数据集和任务上进行了评估：使用长庚研究数据库进行冠状动脉疾病预测，以及使用MIMIC-III和MIMIC-IV数据集进行住院死亡率预测。KAT-GNN在CAD预测中达到最先进性能，在MIMIC-III和MIMIC-IV的死亡率预测中表现出色，持续优于GRASP和RETAIN等基线模型。消融研究证实，基于知识的增强和时序建模组件都是性能提升的重要贡献者。这些发现表明，将临床知识整合到图表示中，结合时间感知注意力机制，为跨不同临床任务和数据集的风险预测提供了一种有效且可推广的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clinical risk prediction using electronic health records (EHRs) is vital tofacilitate timely interventions and clinical decision support. However,modeling heterogeneous and irregular temporal EHR data presents significantchallenges. We propose \textbf{KAT-GNN} (Knowledge-Augmented Temporal GraphNeural Network), a graph-based framework that integrates clinical knowledge andtemporal dynamics for risk prediction. KAT-GNN first constructsmodality-specific patient graphs from EHRs. These graphs are then augmentedusing two knowledge sources: (1) ontology-driven edges derived from SNOMED CTand (2) co-occurrence priors extracted from EHRs. Subsequently, a time-awaretransformer is employed to capture longitudinal dynamics from the graph-encodedpatient representations. KAT-GNN is evaluated on three distinct datasets andtasks: coronary artery disease (CAD) prediction using the Chang Gung ResearchDatabase (CGRD) and in-hospital mortality prediction using the MIMIC-III andMIMIC-IV datasets. KAT-GNN achieves state-of-the-art performance in CADprediction (AUROC: 0.9269 $\pm$ 0.0029) and demonstrated strong results inmortality prediction in MIMIC-III (AUROC: 0.9230 $\pm$ 0.0070) and MIMIC-IV(AUROC: 0.8849 $\pm$ 0.0089), consistently outperforming established baselinessuch as GRASP and RETAIN. Ablation studies confirm that both knowledge-basedaugmentation and the temporal modeling component are significant contributorsto performance gains. These findings demonstrate that the integration ofclinical knowledge into graph representations, coupled with a time-awareattention mechanism, provides an effective and generalizable approach for riskprediction across diverse clinical tasks and datasets.</description>
      <author>example@mail.com (Kun-Wei Lin, Yu-Chen Kuo, Hsin-Yao Wang, Yi-Ju Tseng)</author>
      <guid isPermaLink="false">2511.01249v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Fleming-VL: Towards Universal Medical Visual Reasoning with Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2511.00916v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Fleming-VL是一个统一的端到端框架，用于跨异构模态的综合医学视觉理解。通过三种关键策略解决了医学数据异质性和格式不一致的挑战，并在多个基准测试上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在通用领域表现出色，研究人员正致力于赋予其医学对话能力。然而，医学数据具有异质性，包含2D图像、3D体积扫描和时序视频序列等多种模态，这些模态间的领域差距和数据格式不一致阻碍了统一医学MLLMs的发展。&lt;h4&gt;目的&lt;/h4&gt;解决医学数据异质性和格式不一致的挑战，开发一个统一的端到端框架，用于跨异构模态的综合医学视觉理解。&lt;h4&gt;方法&lt;/h4&gt;从数据角度出发通过三种策略：(1)整合自然域和医学特定领域的长上下文数据扩大预训练；(2)补充稀有医学数据（包括整体视频分析和代表性不足的2D模态）；(3)扩展评估框架，纳入3D体积和视频理解基准。通过监督微调(SFT)和组相对策略优化(GRPO)开发了多种模型规模的Fleming-VL。&lt;h4&gt;主要发现&lt;/h4&gt;Fleming-VL在多个基准测试上取得了最先进的性能，包括医学VQA、视频问答和3D医学图像理解。&lt;h4&gt;结论&lt;/h4&gt;Fleming-VL成功解决了医学数据异质性和格式不一致的挑战，为跨模态医学视觉理解提供了统一框架。作者公开发布了Fleming-VL以促进医学AI的透明、可复现和可审计的进展。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已在各种通用领域场景中展现出显著的有效性，如视觉问答和图像描述。最近，研究人员越来越专注于赋予MLLMs医学对话能力，这对临床应用具有重要前景。然而，医学数据由于其异质性而呈现独特挑战——包含多种模态，包括2D图像、3D体积扫描和时序视频序列。这些模态之间的显著领域差距和数据格式不一致阻碍了统一医学MLLMs的发展。为应对这些挑战，我们提出了Fleming-VL，一个用于跨异构模态综合医学视觉理解的统一端到端框架。Fleming-VL从数据角度通过三种关键策略解决这个问题：(1)通过整合自然域和医学特定领域的长上下文数据扩大预训练；(2)通过补充稀有医学数据（包括整体视频分析和代表性不足的2D模态，如超声和皮肤镜图像）来完善微调；(3)扩展现有评估框架，纳入3D体积和视频理解基准。通过监督微调(SFT)和组相对策略优化(GRPO)，我们开发了多种模型规模的Fleming-VL。大量实验表明，Fleming-VL在多个基准测试上取得了最先进的性能，包括医学VQA、视频问答和3D医学图像理解。我们公开发布Fleming-VL，以促进医学AI的透明、可复现和可审计的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal Large Language Models (MLLMs) have demonstrated remarkableeffectiveness in various general-domain scenarios, such as visual questionanswering and image captioning. Recently, researchers have increasingly focusedon empowering MLLMs with medical conversational abilities, which holdsignificant promise for clinical applications. However, medical data presentsunique challenges due to its heterogeneous nature -- encompassing diversemodalities including 2D images, 3D volumetric scans, and temporal videosequences. The substantial domain gap and data format inconsistencies acrossthese modalities have hindered the development of unified medical MLLMs. Toaddress these challenges, we propose Fleming-VL, a unified end-to-end frameworkfor comprehensive medical visual understanding across heterogeneous modalities.Fleming-VL tackles this problem from a data-centric perspective through threekey strategies: (1) scaling up pretraining by integrating long-context datafrom both natural and medical-specific domains; (2) complementing fine-tuningwith rare medical data, including holistic video analysis and underrepresented2D modalities such as ultrasound and dermoscopy images; (3) extending existingevaluation frameworks to incorporate 3D volumetric and video understandingbenchmarks. Through supervised fine-tuning (SFT) and group relative policyoptimization (GRPO), we develop Fleming-VL in multiple model scales. Extensiveexperiments demonstrate that Fleming-VL achieves state-of-the-art performanceacross multiple benchmarks, including medical VQA, video QA, and 3D medicalimage understanding. We publicly release Fleming-VL to promote transparent,reproducible, and auditable progress in medical AI.</description>
      <author>example@mail.com (Yan Shu, Chi Liu, Robin Chen, Derek Li, Bryan Dai)</author>
      <guid isPermaLink="false">2511.00916v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Systematic Review of Spatio-Temporal Statistical Models: Theory, Structure, and Applications</title>
      <link>http://arxiv.org/abs/2511.00422v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一项关于时空数据统计模型的系统文献综述，提出了时空模型结构的分类方案，并分析了不同领域中的应用情况和模型特点。&lt;h4&gt;背景&lt;/h4&gt;具有时空属性的数据在许多研究领域普遍存在，分析时空关系的统计模型被广泛应用。现有的综述要么专注于特定领域，要么专注于特定模型类型，缺乏全面的、跨学科的综合概述。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有综述的局限性，作者旨在提出时空模型结构的分类方案，并突出它们在常见领域的应用。&lt;h4&gt;方法&lt;/h4&gt;作者遵循PRISMA指南进行了系统文献综述，搜索了两个数据库，时间跨度为2021-2025年，确定了83篇符合标准的出版物。&lt;h4&gt;主要发现&lt;/h4&gt;层次模型是最常使用的模型；大多数模型包含加性成分以考虑时空依赖性；不同应用领域的首选模型结构不同；研究工作主要集中在少数特定学科，尽管时空数据具有更广泛的相关性；可重复性仍然有限。&lt;h4&gt;结论&lt;/h4&gt;作者的综述不仅为跨学科比较模型结构提供了灵感，还强调了提高透明度、可访问性和跨领域知识转移的机会。&lt;h4&gt;翻译&lt;/h4&gt;具有时空属性的数据在许多研究领域普遍存在，分析时空关系的统计模型被广泛应用。现有的综述要么专注于特定领域，要么专注于特定模型类型，造成缺乏全面的、跨学科综合概述的空白。为解决这一问题，我们遵循PRISMA指南进行了系统文献综述，搜索了两个数据库中2021-2025年的文献，确定了83篇符合我们标准的出版物。我们提出了时空模型结构的分类方案，并突出了它们在常见领域中的应用：流行病学、生态学、公共卫生、经济学和犯罪学。尽管不同领域的任务有所不同，但许多模型具有相似之处。我们发现层次模型是最常使用的，大多数模型包含加性成分以考虑时空依赖性。应用领域的首选模型结构各不相同。我们还注意到，尽管时空数据具有更广泛的相关性，但研究工作主要集中在少数特定学科。此外，我们发现可重复性仍然有限。因此，我们的综述不仅为跨学科比较模型结构提供了灵感，还强调了提高透明度、可访问性和跨领域知识转移的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data with spatial-temporal attributes are prevalent across many researchfields, and statistical models for analyzing spatio-temporal relationships arewidely used. Existing reviews focus either on specific domains or model types,creating a gap in comprehensive, cross-disciplinary overviews. To address this,we conducted a systematic literature review following the PRISMA guidelines,searched two databases for the years 2021-2025, and identified 83 publicationsthat met our criteria. We propose a classification scheme for spatio-temporalmodel structures and highlight their application in the most common fields:epidemiology, ecology, public health, economics, and criminology. Althoughtasks vary by domain, many models share similarities. We found thathierarchical models are the most frequently used, and most models incorporateadditive components to account for spatial-temporal dependencies. The preferredmodel structures differ among fields of application. We also observe thatresearch efforts are concentrated in only a few specific disciplines, despitethe broader relevance of spatio-temporal data. Furthermore, we notice thatreproducibility remains limited. Our review, therefore, not only offersinspiration for comparing model structures in an interdisciplinary manner butalso highlights opportunities for greater transparency, accessibility, andcross-domain knowledge transfer.</description>
      <author>example@mail.com (Isabella Habereder, Thomas Kneib, Isao Echizen, Timo Spinde)</author>
      <guid isPermaLink="false">2511.00422v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>LongCat-Flash-Omni Technical Report</title>
      <link>http://arxiv.org/abs/2511.00279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LongCat-Flash-Omni是一个最先进的开源多模态模型，具有5600亿参数，擅长实时音频视觉交互。它采用课程启发的渐进式训练策略，从简单到复杂的模态序列建模任务过渡，在保持强大单模态能力的同时获得全面的多模态能力。&lt;h4&gt;背景&lt;/h4&gt;基于LongCat-Flash模型，该模型采用高性能的捷径连接专家混合架构，具有零计算专家。LongCat-Flash-Omni集成了高效的多模态感知和语音重建模块。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够实现低延迟实时音频视觉交互的大型多模态模型，同时保持高性能和效率。&lt;h4&gt;方法&lt;/h4&gt;采用课程启发的渐进式训练策略；基于LongCat-Flash的捷径连接MoE架构；集成高效多模态感知和语音重建模块；开发模态解耦并行化方案来管理大规模多模态训练中的数据和模型异质性。&lt;h4&gt;主要发现&lt;/h4&gt;尽管模型庞大（5600亿参数，其中270亿被激活），仍能实现低延迟实时音频视觉交互；模态解耦并行化方案效率高，能维持文本-only训练超过90%的吞吐量；在多模态基准测试中取得开源模型的最先进性能；在文本、图像、视频理解以及音频理解和生成等多种模态特定任务上具有高度竞争力。&lt;h4&gt;结论&lt;/h4&gt;LongCat-Flash-Omni是一个高效的大型多模态模型，通过创新的训练策略和架构设计，实现了实时音频视觉交互，并在各种任务上取得了优异性能。研究团队开源了该模型，以促进未来的研究和社区发展。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了LongCat-Flash-Omni，这是一个最先进的开源多模态模型，具有5600亿参数，擅长实时音频视觉交互。通过采用课程启发的渐进式训练策略，从简单到复杂的模态序列建模任务过渡，LongCat-Flash-Omni在保持强大单模态能力的同时获得了全面的多模态能力。基于采用高性能捷径连接专家混合架构且具有零计算专家的LongCat-Flash，LongCat-Flash-Omni集成了高效的多模态感知和语音重建模块。尽管其庞大的5600亿参数（其中270亿被激活），LongCat-Flash-Omni仍实现了低延迟的实时音频视觉交互。在训练基础设施方面，我们开发了一种模态解耦并行化方案，专门用于管理大规模多模态训练中固有的数据和模型异质性。这种创新方法通过维持文本-only训练超过90%的吞吐量，展示了卓越的效率。广泛的评估表明，LongCat-Flash-Omni在开源模型的多模态基准测试中取得了最先进的性能。此外，它在广泛的模态特定任务上提供了高度竞争性的结果，包括文本、图像和视频理解，以及音频理解和生成。我们全面概述了模型架构设计、训练流程和数据策略，并开源了该模型，以促进社区未来的研究和开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce LongCat-Flash-Omni, a state-of-the-art open-source omni-modalmodel with 560 billion parameters, excelling at real-time audio-visualinteraction. By adopting a curriculum-inspired progressive training strategythat transitions from simpler to increasingly complex modality sequencemodeling tasks, LongCat-Flash-Omni attains comprehensive multimodalcapabilities while maintaining strong unimodal capability. Building uponLongCat-Flash, which adopts a high-performance Shortcut-connectedMixture-of-Experts (MoE) architecture with zero-computation experts,LongCat-Flash-Omni integrates efficient multimodal perception and speechreconstruction modules. Despite its immense size of 560B parameters (with 27Bactivated), LongCat-Flash-Omni achieves low-latency real-time audio-visualinteraction. For training infrastructure, we developed a modality-decoupledparallelism scheme specifically designed to manage the data and modelheterogeneity inherent in large-scale multimodal training. This innovativeapproach demonstrates exceptional efficiency by sustaining over 90% of thethroughput achieved by text-only training. Extensive evaluations show thatLongCat-Flash-Omni achieves state-of-the-art performance on omni-modalbenchmarks among open-source models. Furthermore, it delivers highlycompetitive results across a wide range of modality-specific tasks, includingtext, image, and video understanding, as well as audio understanding andgeneration. We provide a comprehensive overview of the model architecturedesign, training procedures, and data strategies, and open-source the model tofoster future research and development in the community.</description>
      <author>example@mail.com (Meituan LongCat Team, Bairui Wang, Bayan, Bin Xiao, Bo Zhang, Bolin Rong, Borun Chen, Chang Wan, Chao Zhang, Chen Huang, Chen Chen, Chen Chen, Chengxu Yang, Chengzuo Yang, Cong Han, Dandan Peng, Delian Ruan, Detai Xin, Disong Wang, Dongchao Yang, Fanfan Liu, Fengjiao Chen, Fengyu Yang, Gan Dong, Gang Huang, Gang Xu, Guanglu Wan, Guoqiang Tan, Guoqiao Yu, Haibo Qiu, Hao Lu, Hongbo Liu, Hongyu Xiang, Jiaheng Wu, Jian Yang, Jiaxing Liu, Jing Huang, Jingang Wang, Jinrui Ding, Juchao Jiang, Jun Kuang, Jun Wang, Junhui Mei, Ke Ding, Kefeng Zhang, Lei Chen, Liang Shi, Limeng Qiao, Liming Zheng, Lin Ma, Liuyang Guo, Liya Ma, Luying Sun, Man Gao, Mengshen Zhu, Miao Cao, Minliang Lin, Nuo Xu, Peng Shi, Qi Zhang, Qian Fang, Qian Wang, Qian Yang, Quanxiu Wang, Rongxiang Weng, Rongxin Guo, Ruoxuan Liang, Senbin Yang, Shanbo Xu, Shanglin Lei, Shengze Ye, Shimin Chen, Shuaiqi Chen, Shujie Hu, Shuo Li, Siqi Yang, Siyu Xu, Siyu Ren, Song Li, Songxiang Liu, Tianhao Bai, Tianye Dai, Wei Hong, Wei Wang, Weixiao Zhao, Wengang Cao, Wenlong Zhu, Wenlong He, Xi Su, Xi Nan, Xiaohan Zhao, Xiaohao Wang, Xiaoyu Zhao, Xiaoyu Wang, Xiaoyu Li, Xin Pan, Xin Chen, Xiusong Sun, Xu Xiang, Xudong Xing, Xuezhi Cao, Xunliang Cai, Yang Yang, Yanli Tan, Yao Yao, Yerui Sun, Yi Chen, Yifan Lu, Yin Gong, Yining Zhang, Yitian Chen, Yiyang Gan, Yuchen Tang, Yuchen Xie, Yueqian Wang, Yuewen Zheng, Yufei Zhang, Yufeng Zhong, Yulei Qian, Yuqi Peng, Yuwei Jiang, Zeyang Hu, Zheng Zhang, Zhengkun Tian, Zhiqing Hong, Zhixiong Zeng, Zhuqi Mi, Ziran Li, Ziwen Wang, Ziyi Zhao, Ziyuan Zhuang, Zizhe Zhao)</author>
      <guid isPermaLink="false">2511.00279v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>FLoC: Facility Location-Based Efficient Visual Token Compression for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2511.00141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FLoC框架，一种基于设施定位函数的高效视觉标记压缩方法，用于解决长视频理解中视觉标记过多导致的扩展性问题，在保证接近最优性能的同时显著减少视觉标记数量。&lt;h4&gt;背景&lt;/h4&gt;最近的长视频理解研究利用了大型多模态模型(LMMs)先进的视觉-语言推理能力，推动了专门处理长视频序列的视频-LMMs的发展。然而，这些模型的扩展性受到长视频序列产生的大量视觉标记的严重限制。&lt;h4&gt;目的&lt;/h4&gt;解决长视频理解中视觉标记过多导致的模型扩展性问题，开发一种高效的方法来压缩视觉标记，同时保持模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出FLoC框架，基于设施定位函数的视觉标记压缩方法，通过懒贪婪算法快速选择紧凑且具有代表性和多样性的视觉标记子集，在预定义的视觉标记数量预算内工作。该方法无需训练，与模型和查询无关。&lt;h4&gt;主要发现&lt;/h4&gt;在Video-MME、MLVU和LongVideoBench等大规模基准测试上的广泛评估表明，该框架始终优于最近的压缩技术，展示了其在解决长视频理解关键挑战方面的有效性和稳健性，以及处理速度方面的效率。&lt;h4&gt;结论&lt;/h4&gt;FLoC框架为长视频理解提供了一个通用的解决方案，能够无缝集成到各种视频-LLMs和现有工作流程中，显著减少视觉标记数量而不牺牲性能。&lt;h4&gt;翻译&lt;/h4&gt;最近的长视频理解研究利用了大型多模态模型(LMMs)先进的视觉-语言推理能力，推动了专门处理长视频序列的视频-LMMs的发展。然而，这些模型的扩展性受到长视频序列产生的大量视觉标记的严重限制。为应对这一挑战，本文提出了FLoC，一种基于设施定位函数的高效视觉标记压缩框架，这是一种在视觉标记数量预定义预算内快速选择紧凑但高度代表性和多样性视觉标记子集的原则性方法。通过集成懒贪婪算法，我们的方法通过快速选择紧凑的标记子集实现了显著的效率提升，在保证接近最优性能的同时大幅减少了视觉标记数量。值得注意的是，我们的方法无需训练，与模型和查询无关，提供了一个通用的解决方案，可以无缝集成到各种视频-LLMs和现有工作流程中。在Video-MME、MLVU和LongVideoBench等大规模基准上的广泛评估表明，我们的框架始终优于最近的压缩技术，这不仅突显了其在解决长视频理解关键挑战方面的有效性和稳健性，也展示了其在处理速度方面的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies in long video understanding have harnessed the advancedvisual-language reasoning capabilities of Large Multimodal Models (LMMs),driving the evolution of video-LMMs specialized for processing extended videosequences. However, the scalability of these models is severely limited by theoverwhelming volume of visual tokens generated from extended video sequences.To address this challenge, this paper proposes FLoC, an efficient visual tokencompression framework based on the facility location function, a principledapproach that swiftly selects a compact yet highly representative and diversesubset of visual tokens within a predefined budget on the number of visualtokens. By integrating the lazy greedy algorithm, our method achievesremarkable efficiency gains by swiftly selecting a compact subset of tokens,drastically reducing the number of visual tokens while guaranteeingnear-optimal performance. Notably, our approach is training-free,model-agnostic, and query-agnostic, providing a versatile solution thatseamlessly integrates with diverse video-LLMs and existing workflows. Extensiveevaluations on large-scale benchmarks, such as Video-MME, MLVU, andLongVideoBench, demonstrate that our framework consistently surpasses recentcompression techniques, highlighting not only its effectiveness and robustnessin addressing the critical challenges of long video understanding, but also itsefficiency in processing speed.</description>
      <author>example@mail.com (Janghoon Cho, Jungsoo Lee, Munawar Hayat, Kyuwoong Hwang, Fatih Porikli, Sungha Choi)</author>
      <guid isPermaLink="false">2511.00141v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>AI Powered High Quality Text to Video Generation with Enhanced Temporal Consistency</title>
      <link>http://arxiv.org/abs/2511.00107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MOVAI框架，解决了文本到视频生成中的时间一致性、组合理解和精细控制问题，通过创新的场景解析、注意力机制和视频细化模块实现了高质量视频生成。&lt;h4&gt;背景&lt;/h4&gt;文本到视频生成是生成式人工智能的关键前沿，但现有方法在保持时间一致性、组合理解和精细控制视觉叙事方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个名为MOVAI的新型分层框架，用于高保真文本到视频合成，结合组合场景理解和时间感知扩散模型。&lt;h4&gt;方法&lt;/h4&gt;MOVAI框架包含三个关键创新：(1)组合场景解析器(CSP)将文本描述分解为带有时间注释的分层场景图；(2)时间-空间注意力机制(TSAM)确保帧间连贯运动动态同时保持空间细节；(3)渐进式视频细化(PVR)模块通过多尺度时间推理迭代提高视频质量。&lt;h4&gt;主要发现&lt;/h4&gt;在标准基准上的实验表明，MOVAI实现了最先进的性能，相比现有方法，LPIPS指标提高15.3%，FVD指标提高12.7%，用户偏好研究提高18.9%。&lt;h4&gt;结论&lt;/h4&gt;MOVAI框架在生成具有真实时间动态和精细语义控制的多对象复杂场景方面表现出特别优势。&lt;h4&gt;翻译&lt;/h4&gt;文本到视频生成已成为生成式人工智能的关键前沿，然而现有方法在保持时间一致性、组合理解和精细控制视觉叙事方面存在困难。我们提出了MOVAI（多模态原始视频AI），这是一种创新的分层框架，将组合场景理解与时间感知扩散模型相结合，用于高保真文本到视频合成。我们的方法引入了三个关键创新：(1)组合场景解析器(CSP)，将文本描述分解为带有时间注释的分层场景图；(2)时间-空间注意力机制(TSAM)，确保帧间连贯的运动动态同时保持空间细节；(3)渐进式视频细化(PVR)模块，通过多尺度时间推理迭代提高视频质量。在标准基准上的广泛实验表明，MOVAI实现了最先进的性能，与现有方法相比，在LPIPS指标上提高15.3%，在FVD指标上提高12.7%，在用户偏好研究中提高18.9%。我们的框架在生成具有真实时间动态和精细语义控制的多对象复杂场景方面表现出特别优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决文本到视频生成中的时间一致性、组合理解和精细控制问题。这些问题很重要，因为视频不仅是图像集合，而是复杂的时间叙事，需要确保物体在帧间保持一致、动作流畅自然，同时能准确理解文本描述中的复杂场景关系。现有方法常出现闪烁、物体变形、控制不足等问题，限制了视频生成技术的实际应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到视频生成比图像生成更具挑战性，需要从根本上设计时间建模，而非简单地在图像生成基础上添加时间维度。他们借鉴了文本到图像生成中的扩散模型技术（如Stable Diffusion），视频生成中的3D卷积和Transformer架构（如CogVideo），以及组合理解中的场景图生成技术。基于这些现有工作，作者设计了MOVAI框架，从基础层面构建专门用于时间视觉叙事的系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将文本描述分解为层次化场景图，同时建模帧内空间关系和跨帧时间依赖，通过多尺度时间推理渐进式提高视频质量。整体流程分为四个阶段：1)文本输入处理：使用BERT编码器将文本转换为密集嵌入；2)场景理解：CSP模块通过图神经网络生成包含对象、关系和时间约束的场景图；3)注意力处理：TSAM模块通过空间、时间和跨模态三种注意力机制确保生成一致性；4)视频生成：PVR模块通过三个分辨率级别迭代细化视频质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三个：1)组合场景解析器(CSP)：将文本分解为层次化场景图，提供精细控制；2)时间-空间注意力机制(TSAM)：统一建模空间和时间关系，保持物体一致性并确保流畅运动；3)渐进式视频细化(PVR)：多阶段细化过程提高生成稳定性。相比之前工作，MOVAI不是简单地将图像生成方法扩展到视频，而是专为时间视觉叙事设计的系统，解决了现有方法在复杂场景中表现不佳的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MOVAI通过创新的组合场景解析、时间-空间注意力和渐进式视频细化框架，显著提高了文本到视频生成的时间一致性和视觉质量，为复杂场景中的高质量视频生成提供了新方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text to video generation has emerged as a critical frontier in generativeartificial intelligence, yet existing approaches struggle with maintainingtemporal consistency, compositional understanding, and fine grained controlover visual narratives. We present MOVAI (Multimodal Original Video AI), anovel hierarchical framework that integrates compositional scene understandingwith temporal aware diffusion models for high fidelity text to video synthesis.Our approach introduces three key innovations: (1) a Compositional Scene Parser(CSP) that decomposes textual descriptions into hierarchical scene graphs withtemporal annotations, (2) a Temporal-Spatial Attention Mechanism (TSAM) thatensures coherent motion dynamics across frames while preserving spatialdetails, and (3) a Progressive Video Refinement (PVR) module that iterativelyenhances video quality through multi-scale temporal reasoning. Extensiveexperiments on standard benchmarks demonstrate that MOVAI achievesstate-of-the-art performance, improving video quality metrics by 15.3% inLPIPS, 12.7% in FVD, and 18.9% in user preference studies compared to existingmethods. Our framework shows particular strength in generating complexmulti-object scenes with realistic temporal dynamics and fine-grained semanticcontrol.</description>
      <author>example@mail.com (Piyushkumar Patel)</author>
      <guid isPermaLink="false">2511.00107v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>MaGNet: A Mamba Dual-Hypergraph Network for Stock Prediction via Temporal-Causal and Global Relational Learning</title>
      <link>http://arxiv.org/abs/2511.00085v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MaGNet，一种基于Mamba双超图网络的股票趋势预测方法，通过三个关键创新解决了现有方法在捕捉时间依赖性和动态股票间互动方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;股票趋势预测对盈利交易策略和投资组合管理至关重要，但由于市场波动性、复杂时间动态和股票间多维关系的存在，这一任务极具挑战性。现有方法难以有效捕捉时间依赖性和动态股票间互动，常忽略市场横截面影响，依赖静态相关性，对节点和边采用统一处理，并混淆多样化关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型股票预测模型，有效捕捉时间依赖性和动态股票间互动，提高预测准确性和投资回报。&lt;h4&gt;方法&lt;/h4&gt;MaGNet包含三个关键创新：(1) MAGE块：利用双向Mamba和自适应门控机制进行上下文时间建模，集成稀疏专家混合层动态适应不同市场条件，使用多头注意力捕获全局依赖；(2) 特征级和股票级二维时空注意力模块：实现多元特征精确融合和跨股票依赖关系，桥接时间建模与关系推理；(3) 双超图框架：包含时间约束超图(TCH)捕获细粒度因果依赖，以及全局概率超图(GPH)建模市场范围模式，实现多尺度关系学习。&lt;h4&gt;主要发现&lt;/h4&gt;在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进方法，且具有出色的风险管理能力。&lt;h4&gt;结论&lt;/h4&gt;MaGNet通过创新的架构设计有效解决了股票趋势预测中的关键挑战，能够处理市场波动性、复杂时间动态和股票间多维关系，为交易策略和投资组合管理提供了更可靠的预测工具。&lt;h4&gt;翻译&lt;/h4&gt;股票趋势预测对盈利交易策略和投资组合管理至关重要，但由于市场波动性、复杂的时间动态和股票间的多维关系，这一任务仍然具有挑战性。现有方法难以有效捕捉时间依赖性和动态的股票间互动，常常忽略市场横截面影响，依赖静态相关性，对节点和边采用统一处理，并混淆多样化关系。这项工作引入了MaGNet，一种用于股票预测的新型Mamba双超图网络，整合了三个关键创新：(1) MAGE块，利用双向Mamba和自适应门控机制进行上下文时间建模，并集成稀疏专家混合层以动态适应不同市场条件，同时使用多头注意力捕获全局依赖；(2) 特征级和股票级二维时空注意力模块实现多元特征的精确融合和跨股票依赖关系，有效增强信息量同时保留内在数据结构，桥接时间建模与关系推理；(3) 双超图框架包括时间约束超图(TCH)，通过时间约束捕获细粒度因果依赖，以及全局概率超图(GPH)，通过软超边分配和Jensen-Shannon散度加权机制建模市场范围模式，共同分离局部时间影响与瞬时全局结构，实现多尺度关系学习。在六个主要股票指数上的广泛实验表明，MaGNet在预测性能和投资回报方面均优于最先进方法，并具有出色的风险管理能力。代码可在https://github.com/PeilinTime/MaGNet获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stock trend prediction is crucial for profitable trading strategies andportfolio management yet remains challenging due to market volatility, complextemporal dynamics and multifaceted inter-stock relationships. Existing methodsstruggle to effectively capture temporal dependencies and dynamic inter-stockinteractions, often neglecting cross-sectional market influences, relying onstatic correlations, employing uniform treatments of nodes and edges, andconflating diverse relationships. This work introduces MaGNet, a novel Mambadual-hyperGraph Network for stock prediction, integrating three keyinnovations: (1) a MAGE block, which leverages bidirectional Mamba withadaptive gating mechanisms for contextual temporal modeling and integrates asparse Mixture-of-Experts layer to enable dynamic adaptation to diverse marketconditions, alongside multi-head attention for capturing global dependencies;(2) Feature-wise and Stock-wise 2D Spatiotemporal Attention modules enableprecise fusion of multivariate features and cross-stock dependencies,effectively enhancing informativeness while preserving intrinsic datastructures, bridging temporal modeling with relational reasoning; and (3) adual hypergraph framework consisting of the Temporal-Causal Hypergraph (TCH)that captures fine-grained causal dependencies with temporal constraints, andGlobal Probabilistic Hypergraph (GPH) that models market-wide patterns throughsoft hyperedge assignments and Jensen-Shannon Divergence weighting mechanism,jointly disentangling localized temporal influences from instantaneous globalstructures for multi-scale relational learning. Extensive experiments on sixmajor stock indices demonstrate MaGNet outperforms state-of-the-art methods inboth superior predictive performance and exceptional investment returns withrobust risk management capabilities. Codes available at:https://github.com/PeilinTime/MaGNet.</description>
      <author>example@mail.com (Peilin Tan, Chuanqi Shi, Dian Tu, Liang Xie)</author>
      <guid isPermaLink="false">2511.00085v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>UniLION: Towards Unified Autonomous Driving Model with Linear Group RNNs</title>
      <link>http://arxiv.org/abs/2511.01768v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个名为UniLION的统一自动驾驶模型，能够高效处理大规模LiDAR点云、高分辨率多视角图像和事件时间序列数据，基于线性群组RNN算子。该模型作为单一通用架构，可无缝支持多种专业变体，并在多项核心任务上取得具有竞争力的甚至最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;Transformer模型在各个领域表现出色，但其二次注意力机制在处理长序列数据时引入了显著的计算开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一的多模态自动驾驶模型，能够处理多种数据类型，并在3D感知、预测和规划等核心任务上保持高性能，同时简化多模态和多任务自动驾驶系统的设计。&lt;h4&gt;方法&lt;/h4&gt;提出了UniLION模型，基于线性群组RNN算子，能够高效处理大规模LiDAR点云、高分辨率多视角图像和事件时间序列。该模型作为单一架构支持多种专业变体（仅LiDAR、时间LiDAR、多模态、多模态时间融合配置），无需显式的时间或多模态融合模块。&lt;h4&gt;主要发现&lt;/h4&gt;UniLION在3D感知（3D目标检测、3D目标跟踪、3D占用预测、BEV地图分割）、预测（运动预测）和规划（端到端规划）等广泛核心任务中持续提供具有竞争力和最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;这种统一范式自然地简化了多模态和多任务自动驾驶系统的设计，同时保持卓越性能，为自动驾驶中3D基础模型的发展提供了新视角。&lt;h4&gt;翻译&lt;/h4&gt;尽管Transformer已在各个领域展现出卓越的能力，但其二次注意力机制在处理长序列数据时引入了显著的计算开销。在本文中，我们提出了一个统一的自动驾驶模型UniLION，它基于线性群组RNN算子（即对分组特征执行线性RNN），能够高效处理大规模LiDAR点云、高分辨率多视角图像和事件时间序列。值得注意的是，UniLION作为一个单一的多功能架构，可以无缝支持多种专业变体（即仅LiDAR、时间LiDAR、多模态和多模态时间融合配置），而无需显式的时间或多模态融合模块。此外，UniLION在广泛的核心任务中持续提供具有竞争力和最先进的性能，包括3D感知（例如3D目标检测、3D目标跟踪、3D占用预测、BEV地图分割）、预测（例如运动预测）和规划（例如端到端规划）。这种统一范式自然地简化了多模态和多任务自动驾驶系统的设计，同时保持卓越性能。最终，我们希望UniLION能为自动驾驶中3D基础模型的发展提供新的视角。代码可在https://github.com/happinesslz/UniLION获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶系统中多模态数据（激光雷达点云和摄像头图像）和时间序列信息的统一处理问题。现实中，自动驾驶需要同时处理来自不同传感器的异构数据和时间维度的信息，而现有方法通常需要复杂的融合模块和顺序依赖的架构，导致系统复杂、计算效率低。解决这个问题可以简化自动驾驶系统设计，提高计算效率，增强系统鲁棒性和适应性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到Transformer的二次方复杂度不适合处理长序列数据，而线性RNN具有线性计算复杂度的优势。他们借鉴了之前的工作LION（基于线性RNN的3D目标检测），并扩展到更广泛的自动驾驶任务。作者还参考了BEV表示方法和多模态融合技术，但通过线性组RNN的创新应用，消除了对显式融合模块的需求，实现了更简洁高效的统一架构。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用线性组RNN作为统一架构的基础，通过直接标记级连接将多模态（激光雷达和图像）和时间信息统一处理，消除显式的融合模块，生成紧凑的BEV特征表示。整体流程包括：1) 编码阶段处理激光雷达点云和多视角图像；2) 使用3D稀疏窗口分区将输入体素分组；3) 通过UniLION块进行特征交互；4) 使用自回归体素生成策略增强特征；5) 通过BEV主干网络生成统一表示；6) 并行执行各种下游任务（检测、跟踪等）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一异构输入，通过直接标记连接集成多模态和时间信息；2) 统一模型架构，实现不同输入格式的参数共享；3) 统一输出表示，将多模态时间信息压缩为紧凑BEV特征；4) 在多种任务上实现竞争性或最先进性能。相比之前工作，UniLION消除了显式融合模块，使用线性RNN降低计算复杂度，支持单一模型处理多种配置，实现并行多任务学习而非顺序依赖，提高了系统灵活性和故障容错能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UniLION提出了一种基于线性组RNN的统一自动驾驶模型框架，能够高效处理多模态和时间序列数据，在保持高性能的同时显著简化了系统架构并提高了计算效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although transformers have demonstrated remarkable capabilities acrossvarious domains, their quadratic attention mechanisms introduce significantcomputational overhead when processing long-sequence data. In this paper, wepresent a unified autonomous driving model, UniLION, which efficiently handleslarge-scale LiDAR point clouds, high-resolution multi-view images, and eventemporal sequences based on the linear group RNN operator (i.e., performslinear RNN for grouped features). Remarkably, UniLION serves as a singleversatile architecture that can seamlessly support multiple specializedvariants (i.e., LiDAR-only, temporal LiDAR, multi-modal, and multi-modaltemporal fusion configurations) without requiring explicit temporal ormulti-modal fusion modules. Moreover, UniLION consistently delivers competitiveand even state-of-the-art performance across a wide range of core tasks,including 3D perception (e.g., 3D object detection, 3D object tracking, 3Doccupancy prediction, BEV map segmentation), prediction (e.g., motionprediction), and planning (e.g., end-to-end planning). This unified paradigmnaturally simplifies the design of multi-modal and multi-task autonomousdriving systems while maintaining superior performance. Ultimately, we hopeUniLION offers a fresh perspective on the development of 3D foundation modelsin autonomous driving. Code is available athttps://github.com/happinesslz/UniLION</description>
      <author>example@mail.com (Zhe Liu, Jinghua Hou, Xiaoqing Ye, Jingdong Wang, Hengshuang Zhao, Xiang Bai)</author>
      <guid isPermaLink="false">2511.01768v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Urban-MAS: Human-Centered Urban Prediction with LLM-Based Multi-Agent System</title>
      <link>http://arxiv.org/abs/2511.00096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to The 3rd ACM SIGSPATIAL International Workshop on Advances  in Urban AI (UrbanAI'25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Urban-MAS是一个基于大语言模型的多智能体系统框架，通过三种专门设计的智能体提高城市预测的准确性和可靠性，在零样本设置下进行以人为中心的城市预测任务。&lt;h4&gt;背景&lt;/h4&gt;Urban AI在以人为中心的城市任务方面取得进展，大语言模型虽能整合多模态输入处理城市异构数据，但在特定领域任务上表现欠佳。&lt;h4&gt;目的&lt;/h4&gt;介绍Urban-MAS框架，用于在零样本设置下进行以人为中心的城市预测，解决单一大语言模型在城市特定任务上的局限性。&lt;h4&gt;方法&lt;/h4&gt;Urban-MAS包含三种智能体：预测因素引导智能体（优先考虑关键预测因素指导知识提取）、可靠城市信息提取智能体（通过比较输出、验证一致性提高鲁棒性）、多城市信息推理智能体（跨维度整合多源信息进行预测）。&lt;h4&gt;主要发现&lt;/h4&gt;在东京、米兰和西雅图的实验中，Urban-MAS相比单-LLM基线显著减少预测误差；消融研究表明预测因素引导智能体对提高预测性能最为关键。&lt;h4&gt;结论&lt;/h4&gt;Urban-MAS被定位为可扩展的以人为中心的城市AI预测范式，代码已在GitHub开源。&lt;h4&gt;翻译&lt;/h4&gt;城市人工智能（Urban AI）已推进了以人为中心的城市任务，如感知预测和人类动态。大语言模型可以整合多模态输入以处理复杂城市系统中的异构数据，但在特定领域任务上往往表现不佳。Urban-MAS是一个基于大语言模型的多智能体系统（MAS）框架，用于在零样本设置下进行以人为中心的城市预测。它包括三种智能体类型：预测因素引导智能体，优先考虑关键预测因素以指导知识提取，增强压缩城市知识在大语言模型中的有效性；可靠城市信息提取智能体，通过比较多个输出、验证一致性并在发生冲突时重新提取来提高鲁棒性；多城市信息推理智能体，跨维度整合提取的多源信息进行预测。在东京、米兰和西雅图的运行量预测和城市感知实验表明，Urban-MAS与单-LLM基线相比显著减少了误差。消融研究表明预测因素引导智能体对提高预测性能最为关键，使Urban-MAS成为以人为中心的城市AI预测的可扩展范式。代码可在项目网站获取：https://github.com/THETUREHOOHA/UrbanMAS&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban Artificial Intelligence (Urban AI) has advanced human-centered urbantasks such as perception prediction and human dynamics. Large Language Models(LLMs) can integrate multimodal inputs to address heterogeneous data in complexurban systems but often underperform on domain-specific tasks. Urban-MAS, anLLM-based Multi-Agent System (MAS) framework, is introduced for human-centeredurban prediction under zero-shot settings. It includes three agent types:Predictive Factor Guidance Agents, which prioritize key predictive factors toguide knowledge extraction and enhance the effectiveness of compressed urbanknowledge in LLMs; Reliable UrbanInfo Extraction Agents, which improverobustness by comparing multiple outputs, validating consistency, andre-extracting when conflicts occur; and Multi-UrbanInfo Inference Agents, whichintegrate extracted multi-source information across dimensions for prediction.Experiments on running-amount prediction and urban perception across Tokyo,Milan, and Seattle demonstrate that Urban-MAS substantially reduces errorscompared to single-LLM baselines. Ablation studies indicate that PredictiveFactor Guidance Agents are most critical for enhancing predictive performance,positioning Urban-MAS as a scalable paradigm for human-centered urban AIprediction. Code is available on the projectwebsite:https://github.com/THETUREHOOHA/UrbanMAS</description>
      <author>example@mail.com (Shangyu Lou)</author>
      <guid isPermaLink="false">2511.00096v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>OmniField: Conditioned Neural Fields for Robust Multimodal Spatiotemporal Learning</title>
      <link>http://arxiv.org/abs/2511.02205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 12 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OmniField是一种连续感知框架，能够处理真实世界实验数据的多模态时空学习挑战，通过学习基于可用模态条件化的连续神经场并迭代融合跨模态上下文，实现统一的重建、插值、预测和跨模态预测功能。&lt;h4&gt;背景&lt;/h4&gt;真实世界实验数据的多模态时空学习面临两个主要挑战：单模态测量数据稀疏、不规则且带有噪声，但跨模态之间存在相关性；可用模态集合随空间和时间变化，导致可用记录减少。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够适应任意模态子集并处理稀疏、不规则、带噪声数据的框架，用于多模态时空学习。&lt;h4&gt;方法&lt;/h4&gt;提出OmniField框架，该框架学习基于可用模态条件化的连续神经场，并通过多模态串扰块架构与迭代跨模态细化相结合，在解码器前对齐信号，无需网格化或代理预处理即可实现统一功能。&lt;h4&gt;主要发现&lt;/h4&gt;OmniField在评估中始终优于八个强大的多模态时空基线；即使在严重的模拟传感器噪声下，性能仍接近清洁输入水平，显示出对损坏测量的强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;OmniField是一种有效解决多模态时空学习中数据稀疏、不规则、带噪声以及模态集合变化等挑战的框架。&lt;h4&gt;翻译&lt;/h4&gt;真实世界实验数据的多模态时空学习受两个挑战限制：单模态测量稀疏、不规则且带有噪声(QA/QC伪影)，但跨模态相关；可用模态集合随空间和时间变化，除非模型能够在训练和测试时适应任意子集，否则会缩小可用记录。我们提出了OmniField，一种连续感知框架，学习基于可用模态条件化的连续神经场，并迭代融合跨模态上下文。多模态串扰块架构与迭代跨模态细化相结合，在解码器前对齐信号，无需网格化或代理预处理即可实现统一的重建、插值、预测和跨模态预测。广泛评估显示，OmniField始终优于八个强大的多模态时空基线。在严重的模拟传感器噪声下，性能仍接近清洁输入水平，突显了对损坏测量的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态时空学习中的两个关键挑战：1) 同一模态的测量数据稀疏、不规则且含噪声，但不同模态间相互关联；2) 可用模态集在空间和时间上变化，缩小可用记录规模。这些问题在气候科学、空气污染研究、材料科学等多个领域都至关重要，因为现有方法要么依赖数据预处理引入系统副作用，要么使用模型方法但假设固定观测算子，难以处理真实世界中的传感器数据不完整性和噪声问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：数据预处理引入平滑偏差和不确定性崩溃，而模型方法假设固定观测算子在实际情况中不成立。基于此，作者设计了一个连续感知的框架OmniField，它基于条件神经场(CNFs)原理，扩展了SCENT的工作。借鉴了多模态融合(MIA)、神经场表示(NeRF)和算子学习(FNO)等现有方法，但针对科学数据的特殊性进行了改进，特别是处理稀疏、不规则、噪声数据的能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是学习一个连续感知的多模态条件神经场，能够处理稀疏、不规则、噪声数据，并适应不同模态的可用性。整体流程采用编码器-处理器-解码器架构：1) 编码器将不规则观测转换为固定长度表示；2) 处理器融合坐标编码与上下文摘要，形成条件神经场；3) 解码器生成各模态预测。关键组件包括高斯傅里叶特征(GFF)和正弦初始化解决低频偏差，多模态串扰(MCT)块实现跨模态信息交换，迭代跨模态精炼(ICMR)渐进对齐信号，以及灵活模态 fusion处理模态缺失情况。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 连续感知的多模态条件神经场框架；2) 高斯傅里叶特征和正弦初始化解决低频偏差；3) 多模态串扰(MCT)块实现跨模态信息交换；4) 迭代跨模态精炼(ICMR)渐进信号对齐；5) 灵活模态融合处理模态缺失。相比之前工作，OmniField扩展了SCENT的多模态能力，不同于PROSE-FD的PDE算子学习，区别于MIA的双层优化方法，且无需网格化预处理，能更好处理科学数据中的稀疏性和噪声问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OmniField通过引入连续感知的多模态条件神经场、多模态串扰块和迭代跨模态精炼机制，有效解决了科学实验中多模态时空学习的稀疏性、不规则性和噪声挑战，实现了在无需网格化或代理预处理的情况下，对多种任务的鲁棒统一处理。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal spatiotemporal learning on real-world experimental data isconstrained by two challenges: within-modality measurements are sparse,irregular, and noisy (QA/QC artifacts) but cross-modally correlated; the set ofavailable modalities varies across space and time, shrinking the usable recordunless models can adapt to arbitrary subsets at train and test time. We proposeOmniField, a continuity-aware framework that learns a continuous neural fieldconditioned on available modalities and iteratively fuses cross-modal context.A multimodal crosstalk block architecture paired with iterative cross-modalrefinement aligns signals prior to the decoder, enabling unifiedreconstruction, interpolation, forecasting, and cross-modal prediction withoutgridding or surrogate preprocessing. Extensive evaluations show that OmniFieldconsistently outperforms eight strong multimodal spatiotemporal baselines.Under heavy simulated sensor noise, performance remains close to clean-inputlevels, highlighting robustness to corrupted measurements.</description>
      <author>example@mail.com (Kevin Valencia, Thilina Balasooriya, Xihaier Luo, Shinjae Yoo, David Keetae Park)</author>
      <guid isPermaLink="false">2511.02205v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian full waveform inversion with learned prior using deep convolutional autoencoder</title>
      <link>http://arxiv.org/abs/2511.02737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 19 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合深度卷积自编码器和贝叶斯全波形反演的方法，通过降低模型维度和优化计算过程，实现了更高效的速度模型重建和不确定性评估。&lt;h4&gt;背景&lt;/h4&gt;全波形反演(FWI)可以用贝叶斯框架表达，其中相关的不确定性由后验概率分布(PPD)捕获。然而，使用基于采样的方法如马尔可夫链蒙特卡洛(MCMC)解决贝叶斯FWI在计算上非常困难，因为模型空间的维度极高。&lt;h4&gt;目的&lt;/h4&gt;为了缓解计算困难，作者开发了一种深度卷积自编码器(CAE)作为反演的学习先验，以提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;1) 使用CAE将详细的地下速度模型压缩为低维潜在表示；2) 采用自适应梯度MCMC算法，通过基于自动微分的FWI在潜在空间中高效计算梯度；3) 实现迁移学习策略，通过反演过程中的在线微调，使框架能够适应原始训练集中未表示的速度结构。&lt;h4&gt;主要发现&lt;/h4&gt;使用合成数据的数值实验表明，与传统MCMC方法相比，该方法能以更高的效率重建速度模型并评估不确定性。&lt;h4&gt;结论&lt;/h4&gt;结合深度学习和贝叶斯方法的创新框架有效解决了高维模型空间中的计算挑战，实现了更高效的全波形反演。&lt;h4&gt;翻译&lt;/h4&gt;全波形反演(FWI)可以用贝叶斯框架表达，其中相关的不确定性由后验概率分布(PPD)捕获。实际上，使用基于采样的方法如马尔可夫链蒙特卡洛(MCMC)解决贝叶斯FWI在计算上非常困难，因为模型空间的维度极高。为了缓解这一困难，我们开发了一种深度卷积自编码器(CAE)，作为反演的学习先验。CAE将详细的地下速度模型压缩为低维潜在表示，实现了比传统降维方法更有效且地质一致性的模型简化。反演过程采用自适应梯度MCMC算法，通过基于自动微分的FWI在潜在空间中高效计算梯度。此外，我们通过反演过程中的在线微调实现了迁移学习策略，使框架能够适应原始训练集中未表示的速度结构。使用合成数据的数值实验表明，与传统MCMC方法相比，该方法能以更高的效率重建速度模型并评估不确定性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决贝叶斯全波形反演(Bayesian FWI)的计算效率问题。传统基于采样的贝叶斯方法(如MCMC)因模型空间维度极高而计算需求巨大，难以实际应用。这个问题很重要，因为贝叶斯框架能自然包含不确定性，提供更全面的地下结构成像结果，而传统方法只给出单一最优解，无法评估不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到贝叶斯FWI面临维度灾难问题后，借鉴了深度学习中的自编码器技术，特别是卷积自编码器(CAE)的降维能力。他们结合了贝叶斯推理框架、MCMC采样方法、自动微分技术和迁移学习策略，提出在低维潜在空间中进行采样以提高效率。该方法确实借鉴了大量现有工作，包括贝叶斯理论基础、自编码器应用、MCMC方法和自动微分技术等。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用深度卷积自编码器将高维地下速度模型压缩到低维潜在空间，在这个低维空间中进行贝叶斯MCMC采样，解决维度灾难问题。流程包括：1)训练阶段：收集地下速度模型数据，训练CAE；2)反演阶段：将初始模型映射到潜在空间，执行自适应梯度MCMC采样，使用自动微分计算梯度；3)对于分布外情况，先进行解码器在线微调，再进行采样；4)结果分析：收集后验样本，计算统计量和评估不确定性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用深度卷积自编码器作为学习先验，优于传统参数化方法；2)在潜在空间中进行自适应梯度MCMC采样，大幅降低计算复杂度；3)提出在线微调的迁移学习策略，解决分布外反演问题；4)使用正弦激活函数提高性能。相比之前工作，本文更紧密地结合自编码器与贝叶斯框架，提出完整流程，证明了CAE在保持地质特征方面的优势，并解决了分布外反演挑战。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种结合深度卷积自编码器与贝叶斯全波形反演的创新方法，通过在低维潜在空间中进行高效采样，显著提高了地下结构成像的计算效率，同时能够量化反演结果的不确定性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Full waveform inversion (FWI) can be expressed in a Bayesian framework, wherethe associated uncertainties are captured by the posterior probabilitydistribution (PPD). In practice, solving Bayesian FWI with sampling-basedmethods such as Markov chain Monte Carlo (MCMC) is computationally demandingbecause of the extremely high dimensionality of the model space. To alleviatethis difficulty, we develop a deep convolutional autoencoder (CAE) that servesas a learned prior for the inversion. The CAE compresses detailed subsurfacevelocity models into a low-dimensional latent representation, achieving moreeffective and geologically consistent model reduction than conventionaldimension reduction approaches. The inversion procedure employs an adaptivegradient-based MCMC algorithm enhanced by automatic differentiation-based FWIto compute gradients efficiently in the latent space. In addition, we implementa transfer learning strategy through online fine-tuning during inversion,enabling the framework to adapt to velocity structures not represented in theoriginal training set. Numerical experiments with synthetic data show that themethod can reconstruct velocity models and assess uncertainty with improvedefficiency compared to traditional MCMC methods.</description>
      <author>example@mail.com (Shuhua Hu, Mrinal K Sen, Zeyu Zhao, Abdelrahman Elmeliegy, Shuo Zhang)</author>
      <guid isPermaLink="false">2511.02737v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Learning for Industrial Defect Detection: A Case Study on Shearographic Data</title>
      <link>http://arxiv.org/abs/2511.02541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures, 1 table; accepted for AI-2025 Forty-fifth SGAI  International Conference on Artificial Intelligence CAMBRIDGE, ENGLAND 16-18  DECEMBER 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了无监督学习方法在剪切散斑图像自动异常检测中的应用，比较了三种深度学习模型，发现师生特征匹配方法在分类鲁棒性和缺陷定位方面表现最佳，为工业环境中的高效无损检测提供了新思路。&lt;h4&gt;背景&lt;/h4&gt;剪切散斑测量是一种高灵敏度、全场检测能力的无损检测方法，可用于检测表面下缺陷。然而，由于其需要专家解读，在工业应用中的推广受到限制。&lt;h4&gt;目的&lt;/h4&gt;减少对标记数据和人工评估的依赖，探索无监督学习方法用于剪切散斑图像中的自动异常检测，提高工业应用中的检测效率和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;评估了三种架构：全连接自编码器、卷积自编码器和师生特征匹配模型。所有模型仅使用无缺陷数据进行训练。开发了一个使用具有可重复缺陷模式的自定义试样的受控数据集，定义了两个训练子集：一个只包含无畸变、无缺陷样本，另一个额外包含全局变形但无缺陷的数据。评估包括二元分类和空间缺陷定位。&lt;h4&gt;主要发现&lt;/h4&gt;师生特征匹配方法实现了卓越的分类鲁棒性和精确的定位能力。与自编码器模型相比，它表现出更好的特征表示可分性，通过t-SNE嵌入可视化。使用YOLOv8作为参考基准验证了定位质量。&lt;h4&gt;结论&lt;/h4&gt;无监督深度学习在工业环境中具有可扩展性和标签效率，为剪切散斑检测提供了有前景的解决方案，特别是在减少对专家解读的依赖方面。&lt;h4&gt;翻译&lt;/h4&gt;剪切散斑测量是一种用于检测表面下缺陷的无损检测方法，具有高灵敏性和全场检测能力。然而，由于其需要专家解读，在工业应用中的推广仍然有限。为了减少对标记数据和人工评估的依赖，本研究探索了无监督学习方法用于剪切散斑图像中的自动异常检测。评估了三种架构：全连接自编码器、卷积自编码器和师生特征匹配模型。所有模型仅使用无缺陷数据进行训练。开发了一个使用具有可重复缺陷模式的自定义试样的受控数据集，实现了在理想和现实变形条件下系统获取剪切散斑测量。定义了两个训练子集：一个只包含无畸变、无缺陷样本，另一个额外包含全局变形但无缺陷的数据。后者通过包含可能掩盖局部异常的变形引起的条纹图案来模拟实际检测条件。从二元分类和师生模型的缺陷定位方面评估了模型。结果表明，师生方法实现了卓越的分类鲁棒性和精确的定位能力。与基于自编码器的模型相比，它表现出更好的特征表示可分性，通过t-SNE嵌入可视化。此外，在标记缺陷数据上训练的YOLOv8模型作为定位质量的参考基准。这项研究强调了无监督深度学习在工业环境中可扩展、标签高效的剪切散斑检测的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Shearography is a non-destructive testing method for detecting subsurfacedefects, offering high sensitivity and full-field inspection capabilities.However, its industrial adoption remains limited due to the need for expertinterpretation. To reduce reliance on labeled data and manual evaluation, thisstudy explores unsupervised learning methods for automated anomaly detection inshearographic images. Three architectures are evaluated: a fully connectedautoencoder, a convolutional autoencoder, and a student-teacher featurematching model. All models are trained solely on defect-free data. A controlleddataset was developed using a custom specimen with reproducible defectpatterns, enabling systematic acquisition of shearographic measurements underboth ideal and realistic deformation conditions. Two training subsets weredefined: one containing only undistorted, defect-free samples, and oneadditionally including globally deformed, yet defect-free, data. The lattersimulates practical inspection conditions by incorporating deformation-inducedfringe patterns that may obscure localized anomalies. The models are evaluatedin terms of binary classification and, for the student-teacher model, spatialdefect localization. Results show that the student-teacher approach achievessuperior classification robustness and enables precise localization. Comparedto the autoencoder-based models, it demonstrates improved separability offeature representations, as visualized through t-SNE embeddings. Additionally,a YOLOv8 model trained on labeled defect data serves as a reference tobenchmark localization quality. This study underscores the potential ofunsupervised deep learning for scalable, label-efficient shearographicinspection in industrial environments.</description>
      <author>example@mail.com (Jessica Plassmann, Nicolas Schuler, Georg von Freymann, Michael Schuth)</author>
      <guid isPermaLink="false">2511.02541v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Phenotype Discovery in Electronic Health Records through Prior Knowledge-Guided Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2511.02102v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to JAMIA; preprint is the author's original version. Github  repo: https://github.com/mm4963/prior-guided-EHR-phenotyping/tree/main&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种结合领域特定知识的贝叶斯潜在类别框架，用于改进基于电子健康记录的无监督学习表型发现方法。通过信息先验引导聚类向临床相关亚组发展，在哮喘患者数据中成功识别出与2型炎症特征相关的'T2高炎症'亚型。&lt;h4&gt;背景&lt;/h4&gt;传统的基于电子健康记录的无监督学习方法在表型发现方面显示出前景，但这些方法通常忽略了现有的临床信息，限制了结果的可解释性。在缺乏明确表型定义的异质性疾病研究中，需要能够整合临床知识的方法。&lt;h4&gt;目的&lt;/h4&gt;旨在通过整合领域特定知识来提高EHR衍生表型的临床意义，并展示该方法在识别与2型炎症特征相关的哮喘亚型方面的效用。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，通过信息先验将临床知识整合到贝叶斯潜在类别模型中，引导无监督聚类向临床相关的亚组方向发展。该方法能够建模缺失值，考虑潜在的随机缺失模式，并提供患者级别的表型分配概率及其不确定性。研究者在包含44,642名成年哮喘患者的大型哮喘EHR队列中应用了该模型，为2型炎症相关特征指定了信息先验，为其他临床变量指定了弱信息先验。&lt;h4&gt;主要发现&lt;/h4&gt;使用2017年1月至2024年2月的就诊数据，研究者发现了表型分配的双峰后验分布，表明存在明显的类别分离。T2炎症信息类(38.7%)的特征是嗜酸性粒细胞水平和过敏标志物升高，以及高医疗利用率和药物使用，尽管后者的变量只有弱信息先验。这些模式表明存在一个'未控制的T2高'亚型。&lt;h4&gt;结论&lt;/h4&gt;贝叶斯潜在类别建模方法支持在缺乏明确表型定义的异质性疾病研究中进行假设生成和队列识别，展示了如何通过整合临床知识提高EHR数据分析的临床相关性。&lt;h4&gt;翻译&lt;/h4&gt;目标：利用电子健康记录数据进行无监督学习在表型发现方面显示出前景，但通常方法忽略了现有的临床信息，限制了可解释性。我们将贝叶斯潜在类别框架具体化用于表型分析，整合领域特定知识以提高EHR衍生表型的临床意义，并通过识别与2型炎症特征相关的哮喘亚型来说明其效用。材料与方法：我们通过信息先验将临床知识整合到贝叶斯潜在类别模型中，引导无监督聚类向临床相关的亚组方向发展。该方法对缺失值进行建模，考虑潜在的随机缺失模式，并提供患者级别的表型分配概率及其不确定性。使用可重用且灵活的代码，我们将该模型应用于大型哮喘EHR队列，为T2炎症相关特征指定信息先验，为其他临床变量指定弱信息先验，让数据 informing 后验分布。结果与结论：使用2017年1月至2024年2月间44,642名成年哮喘患者的就诊数据，我们发现表型分配的后验分布呈双峰，表明存在明显的类别分离。T2炎症信息类(38.7%)的特征是嗜酸性粒细胞水平和过敏标志物升高，以及高医疗利用率和药物使用，尽管后者的变量只有弱信息先验。这些模式表明存在一个'未控制的T2高'亚型。这展示了我们的贝叶斯潜在类别建模方法如何支持在缺乏明确表型定义的异质性疾病EHR研究中进行假设生成和队列识别。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Objectives: Unsupervised learning with electronic health record (EHR) datahas shown promise for phenotype discovery, but approaches typically disregardexisting clinical information, limiting interpretability. We operationalize aBayesian latent class framework for phenotyping that incorporatesdomain-specific knowledge to improve clinical meaningfulness of EHR-derivedphenotypes and illustrate its utility by identifying an asthma sub-phenotypeinformed by features of Type 2 (T2) inflammation.  Materials and methods: We illustrate a framework for incorporating clinicalknowledge into a Bayesian latent class model via informative priors to guideunsupervised clustering toward clinically relevant subgroups. This approachmodels missingness, accounting for potential missing-not-at-random patterns,and provides patient-level probabilities for phenotype assignment withuncertainty. Using reusable and flexible code, we applied the model to a largeasthma EHR cohort, specifying informative priors for T2 inflammation-relatedfeatures and weakly informative priors for other clinical variables, allowingthe data to inform posterior distributions.  Results and Conclusion: Using encounter data from January 2017 to February2024 for 44,642 adult asthma patients, we found a bimodal posteriordistribution of phenotype assignment, indicating clear class separation. The T2inflammation-informed class (38.7%) was characterized by elevated eosinophillevels and allergy markers, plus high healthcare utilization and medicationuse, despite weakly informative priors on the latter variables. These patternssuggest an "uncontrolled T2-high" sub-phenotype. This demonstrates how ourBayesian latent class modeling approach supports hypothesis generation andcohort identification in EHR-based studies of heterogeneous diseases withoutwell-established phenotype definitions.</description>
      <author>example@mail.com (Melanie Mayer, Kimberly Lactaoen, Gary E. Weissman, Blanca E. Himes, Rebecca A. Hubbard)</author>
      <guid isPermaLink="false">2511.02102v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Machine and Deep Learning for Indoor UWB Jammer Localization</title>
      <link>http://arxiv.org/abs/2511.01819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the 20th International Conference on Risks and Security  of Internet and Systems (CRiSIS 2025, Gatineau-Canada,  https://crisis2025.uqo.ca/). The paper will soon be published as  post-proceedings in Springer's LNCS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了超宽带(UWB)定位系统易受干扰攻击的问题，提出了一种域对抗ConvNeXt自编码器(A-CNT)方法，能够在室内布局变化的情况下实现鲁棒的干扰器定位。&lt;h4&gt;背景&lt;/h4&gt;超宽带(UWB)定位能提供厘米级精度，但容易受到干扰攻击，对智能建筑中的资产跟踪和入侵检测构成安全风险。尽管机器学习和深度学习方法已改进标签定位，但在单个房间内定位恶意干扰器以及应对变化的室内布局方面研究不足。&lt;h4&gt;目的&lt;/h4&gt;研究如何在室内布局变化的情况下实现鲁棒的干扰器定位，解决域偏移问题。&lt;h4&gt;方法&lt;/h4&gt;引入两个新的UWB数据集（原始和修改后的房间配置），建立全面的ML/DL基线，使用多种分类和回归指标评估性能。提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐跨域的CIR衍生特征。&lt;h4&gt;主要发现&lt;/h4&gt;在源数据集上，随机森林达到最高的F1-macro分数0.95，XGBoost达到最低的平均欧几里得误差20.16厘米。但在修改后的房间布局中，XGBoost的平均误差增加十倍至207.99厘米。A-CNT框架将平均误差降低到34.67厘米，比非对抗性迁移学习提高77%，比最佳基线提高83%，使30厘米内的样本比例恢复到0.56。&lt;h4&gt;结论&lt;/h4&gt;对抗特征对齐使得尽管环境变化，室内干扰器定位能够保持鲁棒性和可转移性。&lt;h4&gt;翻译&lt;/h4&gt;超宽带(UWB)定位提供厘米级精度但容易受到干扰攻击，对智能建筑中的资产跟踪和入侵检测构成安全风险。虽然机器学习(ML)和深度学习(DL)方法已改进标签定位，但在单个房间内定位恶意干扰器以及应对变化的室内布局方面 largely unexplored。研究引入两个新的UWB数据集，分别在原始和修改后的房间配置下收集，建立全面的ML/DL基线。使用多种分类和回归指标严格评估性能。在源数据集上，随机森林达到最高的F1-macro分数0.95，XGBoost达到最低的平均欧几里得误差20.16厘米。但在修改后的房间布局中部署源训练的模型导致性能严重下降，XGBoost的平均误差增加十倍至207.99厘米，显示出显著的域偏移。为缓解这种退化，提出域对抗ConvNeXt自编码器(A-CNT)，利用梯度反转层对齐跨域的CIR衍生特征。A-CNT框架通过将平均误差降低到34.67厘米恢复定位性能，比非对抗性迁移学习提高77%，比最佳基线提高83%，使30厘米内的样本比例恢复到0.56。总体结果表明，尽管环境变化，对抗特征对齐 enables robust and transferable indoor jammer localization。代码和数据集可在 https://github.com/afbf4c8996f/Jammer-Loc 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ultra-wideband (UWB) localization delivers centimeter-scale accuracy but isvulnerable to jamming attacks, creating security risks for asset tracking andintrusion detection in smart buildings. Although machine learning (ML) and deeplearning (DL) methods have improved tag localization, localizing maliciousjammers within a single room and across changing indoor layouts remains largelyunexplored. Two novel UWB datasets, collected under original and modified roomconfigurations, are introduced to establish comprehensive ML/DL baselines.Performance is rigorously evaluated using a variety of classification andregression metrics. On the source dataset with the collected UWB features,Random Forest achieves the highest F1-macro score of 0.95 and XGBoost achievesthe lowest mean Euclidean error of 20.16 cm. However, deploying thesesource-trained models in the modified room layout led to severe performancedegradation, with XGBoost's mean Euclidean error increasing tenfold to 207.99cm, demonstrating significant domain shift. To mitigate this degradation, adomain-adversarial ConvNeXt autoencoder (A-CNT) is proposed that leverages agradient-reversal layer to align CIR-derived features across domains. The A-CNTframework restores localization performance by reducing the mean Euclideanerror to 34.67 cm. This represents a 77 percent improvement overnon-adversarial transfer learning and an 83 percent improvement over the bestbaseline, restoring the fraction of samples within 30 cm to 0.56. Overall, theresults demonstrate that adversarial feature alignment enables robust andtransferable indoor jammer localization despite environmental changes. Code anddataset available at https://github.com/afbf4c8996f/Jammer-Loc</description>
      <author>example@mail.com (Hamed Fard, Mahsa Kholghi, Benedikt Groß, Gerhard Wunder)</author>
      <guid isPermaLink="false">2511.01819v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>An Open-Access Benchmark of Statistical and Machine-Learning Anomaly Detection Methods for Battery Applications</title>
      <link>http://arxiv.org/abs/2511.01745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OSBAD是一个开源基准测试平台，用于电池应用中的异常检测框架，通过测试15种不同算法实现异常检测方法的系统性比较，提出的特征转换工作流程和贝叶斯优化管道提高了异常检测性能，具有跨化学泛化能力，为电池安全分析提供了重要工具和方法。&lt;h4&gt;背景&lt;/h4&gt;电池安全在从消费电子产品到电动汽车和飞机的各种应用中至关重要，未被发现的异常可能引发安全隐患或导致昂贵的停机时间。&lt;h4&gt;目的&lt;/h4&gt;提出一个名为OSBAD的开源基准测试平台，用于电池应用中的异常检测框架，实现不同数据集上异常检测方法的系统性比较。&lt;h4&gt;方法&lt;/h4&gt;对15种不同的算法进行基准测试，包括统计方法、基于距离的方法和无监督机器学习方法；展示基于物理和统计信息的特征转换工作流程，通过将集体异常分解为点异常提高异常可分离性；提出贝叶斯优化管道，基于迁移学习和回归代理实现自动超参数调优。&lt;h4&gt;主要发现&lt;/h4&gt;通过涵盖液态和固态化学成分的数据集验证，证明了OSBAD的跨化学泛化能力，能够识别不同电化学系统中的异常情况；物理和统计信息驱动的特征工程以及概率超参数调优的模型选择对推进关键能源系统的可信数据驱动诊断具有重要意义。&lt;h4&gt;结论&lt;/h4&gt;通过向社区提供开源可复现的异常检测工作流程的基准测试数据库，OSBAD为开发安全、可扩展和可转移的电池分析异常检测工具建立了统一基础；强调了物理和统计信息驱动的特征工程以及概率超参数调优的模型选择在推进关键能源系统可信数据驱动诊断中的重要性。&lt;h4&gt;翻译&lt;/h4&gt;电池安全在从消费电子产品到电动汽车和飞机的各种应用中至关重要，未被发现的异常可能引发安全隐患或导致昂贵的停机时间。在本研究中，我们提出了OSBAD作为电池应用中异常检测框架的开源基准。通过对涵盖统计、基于距离和无监督机器学习方法在内的15种不同算法进行基准测试，OSBAD实现了跨异构数据集异常检测方法的系统性比较。此外，我们展示了如何通过基于物理和统计信息的特征转换工作流程，通过将集体异常分解为点异常来提高异常可分离性。为解决无监督异常检测中因标签不完整导致的主要瓶颈，我们提出了一种基于迁移学习和回归代理的贝叶斯优化管道，实现自动超参数调优。通过在涵盖液态和固态化学成分的数据集上进行验证，我们进一步证明了OSBAD的跨化学泛化能力，能够识别不同电化学系统中的不规则性。通过向社区提供包含开源可复现异常检测工作流程的基准测试数据库，OSBAD为开发电池分析中安全、可扩展和可转移的异常检测工具建立了统一基础。这项研究强调了物理和统计信息驱动的特征工程以及概率超参数调优的模型选择在推进关键能源系统可信数据驱动诊断中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Battery safety is critical in applications ranging from consumer electronicsto electric vehicles and aircraft, where undetected anomalies could triggersafety hazards or costly downtime. In this study, we present OSBAD as anopen-source benchmark for anomaly detection frameworks in battery applications.By benchmarking 15 diverse algorithms encompassing statistical, distance-based,and unsupervised machine-learning methods, OSBAD enables a systematiccomparison of anomaly detection methods across heterogeneous datasets. Inaddition, we demonstrate how a physics- and statistics-informed featuretransformation workflow enhances anomaly separability by decomposing collectiveanomalies into point anomalies. To address a major bottleneck in unsupervisedanomaly detection due to incomplete labels, we propose a Bayesian optimizationpipeline that facilitates automated hyperparameter tuning based ontransfer-learning and regression proxies. Through validation on datasetscovering both liquid and solid-state chemistries, we further demonstrate thecross-chemistry generalization capability of OSBAD to identify irregularitiesacross different electrochemical systems. By making benchmarking database withopen-source reproducible anomaly detection workflows available to thecommunity, OSBAD establishes a unified foundation for developing safe,scalable, and transferable anomaly detection tools in battery analytics. Thisresearch underscores the significance of physics- and statistics-informedfeature engineering as well as model selection with probabilistichyperparameter tuning, in advancing trustworthy, data-driven diagnostics forsafety-critical energy systems.</description>
      <author>example@mail.com (Mei-Chin Pang, Suraj Adhikari, Takuma Kasahara, Nagihiro Haba, Saneyuki Ohno)</author>
      <guid isPermaLink="false">2511.01745v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Discriminately Treating Motion Components Evolves Joint Depth and Ego-Motion Learning</title>
      <link>http://arxiv.org/abs/2511.01502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DiMoDE的深度和自我运动联合学习框架，通过区分性处理运动组件，利用各自刚性流的几何规律来改善深度和自我运动估计，在多个数据集上取得了最先进的表现。&lt;h4&gt;背景&lt;/h4&gt;无监督学习深度和自我运动这两个基础3D感知任务近年来取得了显著进展。然而，大多数方法将自我运动视为辅助任务，要么混合所有运动类型，要么在监督中排除与深度无关的旋转运动，限制了强几何约束的融入，降低了在多样化条件下的可靠性和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;引入对运动组件的区分性处理，利用各自刚性流的几何规律来改善深度和自我运动估计，提高系统在多样化条件下的可靠性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;给定连续视频帧，网络首先对齐源相机和目标相机的光轴和成像平面。帧之间的光流通过这些对齐进行变换，并量化偏差以对每个自我运动组件单独施加几何约束。这些对齐进一步将联合学习过程重新表述为共轴和共面形式，其中深度和每个平移分量可以通过闭合形式的几何关系相互推导。&lt;h4&gt;主要发现&lt;/h4&gt;通过区分性处理运动组件并利用各自刚性流的几何规律，DiMoDE框架在多个公共数据集和新收集的多样化真实世界数据集上实现了最先进的表现，特别是在具有挑战性的条件下表现优异。&lt;h4&gt;结论&lt;/h4&gt;DiMoDE是一种通用的深度和自我运动联合学习框架，通过区分性处理运动组件和利用几何约束，显著提高了深度和自我运动估计的准确性和鲁棒性，特别是在具有挑战性的条件下。&lt;h4&gt;翻译&lt;/h4&gt;无监督学习深度和自我运动这两个基础3D感知任务近年来取得了显著进展。然而，大多数方法将自我运动视为辅助任务，要么混合所有运动类型，要么在监督中排除与深度无关的旋转运动。这种设计限制了强几何约束的融入，降低了在多样化条件下的可靠性和鲁棒性。本研究引入了对运动组件的区分性处理，利用各自刚性流的几何规律来改善深度和自我运动估计。给定连续视频帧，网络首先对齐源相机和目标相机的光轴和成像平面。帧之间的光流通过这些对齐进行变换，并量化偏差以对每个自我运动组件单独施加几何约束，实现更精细的优化。这些对齐进一步将联合学习过程重新表述为共轴和共面形式，其中深度和每个平移分量可以通过闭合形式的几何关系相互推导，引入互补约束以提高深度鲁棒性。DiMoDE是一种融合了这些设计的通用深度和自我运动联合学习框架，在多个公共数据集和新收集的多样化真实世界数据集上取得了最先进的表现，特别是在具有挑战性的条件下。我们的源代码将在发表后于mias.group/DiMoDE公开。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决无监督学习深度和自运动估计中运动组件处理不当的问题。现有方法要么不加区分地混合所有运动类型，要么排除旋转运动，阻碍了强几何约束的融入，限制了模型在复杂环境下的可靠性。这个问题很重要，因为深度和自运动估计是3D感知的基础，在自动驾驶、机器人导航等领域有广泛应用，而更准确鲁棒的估计能提高系统在各种实际环境中的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析不同运动类型（旋转、切向平移、径向平移）产生的刚性流动差异，发现旋转产生不规则深度无关流动，而平移产生规则但不同的深度相关流动。现有方法混合这些流动导致监督信号质量下降。作者借鉴了现有无监督学习方法（如SfMLearner、Monodepth2）使用光流作为监督信号的思想，以及相机校准和几何变换原理，但创新性地应用于区分不同运动组件的处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是区分性处理运动组件，利用每种组件固有的几何规律性：旋转产生不规则流动，切向平移产生平行流动，径向平移产生朝向/远离主点的流动。实现流程包括：1)将自运动分解为旋转、切向平移和径向平移；2)用旋转对齐光轴消除不规则流动；3)用平移对齐成像平面生成规则流动；4)施加双重几何约束（优化PoseNet和DepthNet）；5)整合所有约束进行联合训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次明确区分三种运动组件并分别处理；2)通过光轴和成像平面对齐将学习重新表述为同轴和共面形式；3)引入双重几何约束分别优化PoseNet和DepthNet；4)提出兼容多种架构的DiMoDE统一框架。相比之前工作，本文不混合处理所有运动类型或完全排除旋转，而是区分切向和径向平移，引入更高层次的几何约束而非仅依赖像素级光度一致性，显著提高了模型在复杂环境下的鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过区分性处理运动组件并引入双重几何约束，提出了一种改进的无监督深度和自运动联合学习框架DiMoDE，显著提高了模型在不同环境条件下的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised learning of depth and ego-motion, two fundamental 3D perceptiontasks, has made significant strides in recent years. However, most methodstreat ego-motion as an auxiliary task, either mixing all motion types orexcluding depth-independent rotational motions in supervision. Such designslimit the incorporation of strong geometric constraints, reducing reliabilityand robustness under diverse conditions. This study introduces a discriminativetreatment of motion components, leveraging the geometric regularities of theirrespective rigid flows to benefit both depth and ego-motion estimation. Givenconsecutive video frames, network outputs first align the optical axes andimaging planes of the source and target cameras. Optical flows between framesare transformed through these alignments, and deviations are quantified toimpose geometric constraints individually on each ego-motion component,enabling more targeted refinement. These alignments further reformulate thejoint learning process into coaxial and coplanar forms, where depth and eachtranslation component can be mutually derived through closed-form geometricrelationships, introducing complementary constraints that improve depthrobustness. DiMoDE, a general depth and ego-motion joint learning frameworkincorporating these designs, achieves state-of-the-art performance on multiplepublic datasets and a newly collected diverse real-world dataset, particularlyunder challenging conditions. Our source code will be publicly available atmias.group/DiMoDE upon publication.</description>
      <author>example@mail.com (Mengtan Zhang, Zizhan Guo, Hongbo Zhao, Yi Feng, Zuyi Xiong, Yue Wang, Shaoyi Du, Hanli Wang, Rui Fan)</author>
      <guid isPermaLink="false">2511.01502v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Soft-partitioned Semi-supervised Collaborative Transfer Learning Approach for Multi-Domain Recommendation</title>
      <link>http://arxiv.org/abs/2511.01404v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CIKM'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对多领域推荐系统中的数据不平衡问题提出了解决方案，通过软分区半监督协同迁移学习(SSCTL)方法显著提升了推荐效果。&lt;h4&gt;背景&lt;/h4&gt;多领域推荐(MDR)在工业实践中至关重要，共享-特定架构被广泛用于捕获共享和独特属性，但不同领域间数据不平衡导致模型性能问题。&lt;h4&gt;目的&lt;/h4&gt;解决多领域推荐中因数据不平衡导致的两个关键问题：主导领域数据过多导致模型偏向，以及非主导领域数据稀疏导致过拟合。&lt;h4&gt;方法&lt;/h4&gt;提出软分区半监督协同迁移学习(SSCTL)方法，通过生成动态参数解决主导问题，利用主导领域实例的带权伪标签增强非主导领域数据以对抗过拟合。&lt;h4&gt;主要发现&lt;/h4&gt;在线实验表明，该方法在各个领域均取得显著改进，GMV增长0.54%至2.90%，CTR提升0.22%至1.69%。&lt;h4&gt;结论&lt;/h4&gt;SSCTL方法有效解决了多领域推荐中的数据不平衡问题，提升了非主导领域的推荐性能，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;在工业实践中，多领域推荐(MDR)起着关键作用。共享-特定架构在工业解决方案中被广泛使用，通过共享和特定参数来捕获共享和独特属性。然而，由于不同领域间数据不平衡，这些模型面临两个关键问题：(1)主导问题：主导领域的数据使模型性能偏向，忽略了非主导领域。(2)过拟合问题：非主导领域的数据稀疏导致特定参数过拟合。为应对这些挑战，我们提出了用于多领域推荐的软分区半监督协同迁移学习(SSCTL)。SSCTL生成动态参数来解决主导问题，从而将重点转向非主导领域的样本。为了对抗过拟合，它利用主导领域实例的带权伪标签来增强非主导领域数据。我们进行了全面的在线和离线实验来验证所提出方法的有效性。在线测试在各种领域都取得了显著改进，GMV增长了0.54%至2.90%，CTR提升了0.22%至1.69%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In industrial practice, Multi-domain Recommendation (MDR) plays a crucialrole. Shared-specific architectures are widely used in industrial solutions tocapture shared and unique attributes via shared and specific parameters.However, with imbalanced data across different domains, these models face twokey issues: (1) Overwhelming: Dominant domain data skews model performance,neglecting non-dominant domains. (2) Overfitting: Sparse data in non-dominantdomains leads to overfitting in specific parameters. To tackle thesechallenges, we propose Soft-partitioned Semi-supervised Collaborative TransferLearning (SSCTL) for multi-domain recommendation. SSCTL generates dynamicparameters to address the overwhelming issue, thus shifting focus towardssamples from non-dominant domains. To combat overfitting, it leveragespseudo-labels with weights from dominant domain instances to enhancenon-dominant domain data. We conduct comprehensive experiments, both online andoffline, to validate the efficacy of our proposed method. Online tests yieldedsignificant improvements across various domains, with increases in GMV rangingfrom 0.54% to 2.90% and enhancements in CTR ranging from 0.22% to 1.69%.</description>
      <author>example@mail.com (Xiaoyu Liu, Yiqing Wu, Ruidong Han, Fuzhen Zhuang, Xiang Li, Wei Lin)</author>
      <guid isPermaLink="false">2511.01404v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Embodiment Transfer Learning for Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2511.01224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ET-VLA的新型框架，通过合成继续预训练(SCP)和具身思维图技术，解决了VLA模型在多机器人协作方面的挑战，显著提升了模型在多embodiment环境中的性能。&lt;h4&gt;背景&lt;/h4&gt;Vision-language-action (VLA)模型已在机器人学习领域取得显著进展，能够在大规模、跨embodiment数据上训练并针对特定机器人进行微调。然而，最先进的自回归VLA模型在多机器人协作方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效有效的框架，将预训练的VLA模型转移到多机器人系统，解决多机器人协作问题。&lt;h4&gt;方法&lt;/h4&gt;ET-VLA框架的核心是合成继续预训练(SCP)，使用合成生成的数据使模型适应新的embodiment，避免真实人类演示，降低数据收集成本。SCP使模型学习正确动作和精确动作令牌数量，随后在目标embodiment数据上微调。此外，提出具身思维图技术，将子任务表述为节点，使VLA模型在任务执行中区分各embodiment的功能和角色。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟基准测试和三种不同双臂机器人的真实机器人上验证了方法有效性。ET-VLA在六个真实世界任务上的表现比OpenVLA高出53.2%。&lt;h4&gt;结论&lt;/h4&gt;将开源所有代码，支持社区推进用于机器人学习的VLA模型发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作(VLA)模型显著推进了机器人学习，使能够在大规模、跨embodiment数据上训练并为特定机器人进行微调。然而，最先进的自回归VLA模型在多机器人协作方面存在困难。我们引入了embodiment迁移学习，称为ET-VLA，这是一种新型框架，用于高效有效地将预训练的VLA模型转移到多机器人系统。ET-VLA的核心是合成继续预训练(SCP)，它使用合成的生成数据来使模型适应新的embodiment，绕过对真实人类演示的需求，降低数据收集成本。SCP使模型能够学习正确的动作和精确的动作令牌数量。在SCP之后，模型在目标embodiment数据上进行微调。为了进一步提高模型在多embodiment上的性能，我们提出了具身思维图技术，一种新颖的方法，将每个子任务表述为一个节点，使VLA模型能够在任务执行过程中区分每个embodiment的功能和角色。我们的研究考虑了双臂机器人，作为多机器人的一个简单版本来验证我们的方法。我们在模拟基准测试和覆盖三种不同双臂embodiment的真实机器人上验证了我们方法的有效性。特别是，我们提出的ET-VLA在六个真实世界任务上可以比OpenVLA高出53.2%。我们将开源所有代码，支持社区推进用于机器人学习的VLA模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action (VLA) models have significantly advanced roboticlearning, enabling training on large-scale, cross-embodiment data andfine-tuning for specific robots. However, state-of-the-art autoregressive VLAsstruggle with multi-robot collaboration. We introduce embodiment transferlearning, denoted as ET-VLA, a novel framework for efficient and effectivetransfer of pre-trained VLAs to multi-robot. ET-VLA's core is SyntheticContinued Pretraining (SCP), which uses synthetically generated data to warm upthe model for the new embodiment, bypassing the need for real humandemonstrations and reducing data collection costs. SCP enables the model tolearn correct actions and precise action token numbers. Following SCP, themodel is fine-tuned on target embodiment data. To further enhance the modelperformance on multi-embodiment, we present the Embodied Graph-of-Thoughttechnique, a novel approach that formulates each sub-task as a node, thatallows the VLA model to distinguish the functionalities and roles of eachembodiment during task execution. Our work considers bimanual robots, a simpleversion of multi-robot to verify our approaches. We validate the effectivenessof our method on both simulation benchmarks and real robots covering threedifferent bimanual embodiments. In particular, our proposed ET-VLA \space canoutperform OpenVLA on six real-world tasks over 53.2%. We will open-source allcodes to support the community in advancing VLA models for robot learning.</description>
      <author>example@mail.com (Chengmeng Li, Yaxin Peng)</author>
      <guid isPermaLink="false">2511.01224v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>STELLAR-koff: A Transfer Learning Model for Protein-Ligand Dissociation Rate Constant Prediction Based on Interaction Landscape</title>
      <link>http://arxiv.org/abs/2511.01171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种名为STELLAR-koff的图神经网络模型，用于预测蛋白质-配体解离速率常数，通过迁移学习将多个配体构象转化为相互作用景观，扩展了数据集，并在测试中表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;成功的药物设计关键在于正确理解蛋白质-配体相互作用，目前已有许多预测热力学性质的深度学习模型，但缺乏成熟的预测动力学性质的模型，主要原因是缺乏动力学数据。&lt;h4&gt;目的&lt;/h4&gt;开发一个预测蛋白质-配体解离速率常数的模型，解决缺乏动力学数据的问题。&lt;h4&gt;方法&lt;/h4&gt;开发名为STELLAR-koff的图神经网络模型，使用迁移学习将蛋白质内多个配体构象转化为蛋白质-配体相互作用景观，扩展PDBbind koff数据集从680到1197个条目，通过五折交叉验证和外部集测试模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;STELLAR-koff在五折交叉验证中达到0.729的皮尔逊相关系数，性能超越或与大多数已发表的预测方法相当；在外部集上对未见蛋白质的预测表现出强大性能，特别是在粘着斑激酶上达到0.838的皮尔逊相关系数；在周期依赖性激酶上的实验验证证明了其在真实药物发现场景中的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究为预测蛋白质-配体解离速率常数提供了有效工具，为该领域的未来发展提供了新的见解。&lt;h4&gt;翻译&lt;/h4&gt;成功药物设计的关键在于正确理解蛋白质-配体相互作用。在当前知识框架下，这些相互作用可以从热力学和动力学两个角度进行描述。近年来，许多深度学习模型 emerged 用于预测蛋白质-配体相互作用的热力学性质。然而，目前缺乏成熟的预测动力学性质的模型，主要原因是缺乏动力学数据。为解决这个问题，我们开发了一个名为STELLAR-koff（基于结构的迁移学习用于配体活性回归）的图神经网络模型来预测蛋白质-配体解离速率常数。与传统的蛋白质-配体性质预测模型不同，后者通常使用单一复合物构象作为输入，STELLAR-koff采用迁移学习将蛋白质内多个配体构象转化为蛋白质-配体相互作用景观，并将这种景观作为模型的主要输入。此外，我们将PDBbind koff数据集从680个扩展到1197个条目，并使用增强的数据集进行模型训练和测试。通过五折交叉验证测试时，STELLAR-koff达到0.729的皮尔逊相关系数，性能超越或与大多数已发表的预测方法相当。在外部集测试中，STELLAR-koff在对未见蛋白质的预测中表现出强大性能，特别是在粘着斑激酶上达到了0.838的皮尔逊相关系数。在周期依赖性激酶上的实验验证也证明了STELLAR-koff在真实药物发现场景中的有效性。我们相信这项研究为预测蛋白质-配体解离速率常数提供了有效工具，并为该领域的未来发展提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The key to successful drug design lies in the correct comprehension ofprotein-ligand interactions. Within the current knowledge paragm, theseinteractions can be described from both thermodynamic and kinetic perspectives.In recent years, many deep learning models have emerged for predicting thethermodynamic properties of protein-ligand interactions. However, there iscurrently no mature model for predicting kinetic properties, primarily due tolack of kinetic data. To tackle this problem, we have developed a graph neuralnetwork model called STELLAR-koff (Structure-based TransfEr Learning for LigandActivity Regression) to predict protein-ligand dissociation rate constant.Unlike traditional protein-ligand property prediction models, which typicallyuse a single complex conformation as input, STELLAR-koff employs transferlearning to transform multiple ligand conformations within the protein into aprotein ligand interaction landscape, and uses this landscape as the primaryinput for the model. In addition, we expanded the PDBbind koff dataset from 680to 1,197 entries and employed the augmented dataset for model training andtesting. When tested through five-fold cross-validation, STELLAR-koff achievedPearson correlation coefficient of 0.729 surpassing or being on pair with mostof the published prediction methods. Tested on external set, STELLAR-koffdemonstrated strong predictive performance on unseen protein, achieving aPearson of 0.838 on the focal adhesion kinase in particular. Experimentalvalidation on cyclin-dependent kinase also demonstrated the effectiveness ofSTELLAR-koff in real drug discovering scenarios. We believe this study providesan effective tool for predicting protein-ligand dissociation rate constant andoffers new insight for the future development of this field.</description>
      <author>example@mail.com (Jingyuan Li)</author>
      <guid isPermaLink="false">2511.01171v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot Multimodal Medical Imaging: A Theoretical Framework</title>
      <link>http://arxiv.org/abs/2511.01140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 Pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的理论框架，用于描述低资源医学影像条件下的学习和推理，旨在解决医学影像领域数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;医学影像依赖于大型标记数据集，但在临床环境中这些数据集不易获取，从业者面临数据有限、数据系统碎片化和数据集不平衡等结构障碍。&lt;h4&gt;目的&lt;/h4&gt;为数据稀缺情况下医学影像学习方法提供坚实的理论基础，解释为什么某些方法成功或失败。&lt;h4&gt;方法&lt;/h4&gt;在少样本条件下形式化学习目标并计算样本复杂度约束；基于PAC学习和PAC-Bayesian理论解释多模态集成如何促进泛化和量化不确定性；提出解释稳定性形式化度量。&lt;h4&gt;主要发现&lt;/h4&gt;多模态集成可以在数据稀缺条件下促进泛化和量化不确定性；解释稳定性度量可以为低数据条件下的可解释性提供保证。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架通过统一描述样本效率、不确定量化和可解释性，为构建可靠、数据高效的医学影像诊断系统奠定了理论基础。&lt;h4&gt;翻译&lt;/h4&gt;医学成像在很大程度上依赖于大型标记数据集。但不幸的是，在临床环境中它们并不总是容易获取。此外，许多从业者经常面临各种结构障碍，如数据可用性有限、数据系统碎片化和数据集不平衡。这些障碍通常导致诊断不确定性增加、某些疾病表现不足、模型鲁棒性降低和诊断决策有偏见。为应对这些挑战，转移学习、元学习和多模态融合等方法已取得长足进展。然而，在数据稀缺的情况下，它们成功或失败的原因仍缺乏坚实的理论依据。为解决这一差距，我们提出了一个统一的理论框架，用于描述低资源医学影像条件下的学习和推理。我们首先在少样本条件下形式化学习目标，并计算样本复杂度约束，以估计实现临床可靠精度所需的最小数据量。然后基于PAC学习和PAC-Bayesian理论的思想，我们解释多模态集成如何促进泛化，以及在稀疏监督下量化不确定性。我们进一步提出了解释稳定性的形式化度量，为低数据条件下的可解释性提供保证。总之，所提出的框架通过在统一理论设定中共同描述样本效率、不确定量化和可解释性，为构建可靠、数据高效的诊断系统奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging relies heavily on large, labeled datasets. But,unfortunately, they are not always easily accessible in clinical settings.Additionally, many practitioners often face various structural obstacles likelimited data availability, fragmented data systems, and unbalanced datasets.These barriers often lead to the increased diagnostic uncertainty,underrepresentation of certain conditions, reduced model robustness, and biaseddiagnostic decisions. In response to these challenges, approaches such astransfer learning, meta-learning, and multimodal fusion have made greatstrides. However, they still need a solid theoretical justification for whythey succeed or fail in situations where data is scarce. To address this gap,we propose a unified theoretical framework that characterizes learning andinference under low-resource medical imaging conditions. We first formalize thelearning objective under few-shot conditions and compute sample complexityconstraints to estimate the smallest quantity of data needed to achieveclinically reliable accuracy. Then based on ideas from PAC-learning andPAC-Bayesian theory, we explain how multimodal integration encouragesgeneralization and quantifies uncertainty under sparse supervision. We furtherpropose a formal metric for explanation stability, offering interpretabilityguarantees under low-data conditions. Taken together, the proposed frameworkestablishes a principled foundation for constructing dependable, data-efficientdiagnostic systems by jointly characterizing sample efficiency, uncertaintyquantification, and interpretability in a unified theoretical setting.</description>
      <author>example@mail.com (Md Talha Mohsin, Ismail Abdulrashid)</author>
      <guid isPermaLink="false">2511.01140v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Learning an Efficient Optimizer via Hybrid-Policy Sub-Trajectory Balance</title>
      <link>http://arxiv.org/abs/2511.00543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Lo-Hp的解耦两阶段权重生成框架，解决了生成式模型在神经网络权重生成中的过耦合和长时程问题，提高了灵活性和效率。&lt;h4&gt;背景&lt;/h4&gt;生成式模型的最新进展使得神经网络能够在不依赖基于梯度优化的情况下生成权重，但当前方法受限于过耦合和长时程问题。过耦合将权重生成与特定任务目标紧密绑定，限制了学习优化器的灵活性；长时程问题因缺乏局部约束导致推理效率低下和准确性不足。&lt;h4&gt;目的&lt;/h4&gt;解决现有权重生成方法的过耦合和长时程问题，提高框架的灵活性和推理效率，特别是在需要频繁权重更新的任务中。&lt;h4&gt;方法&lt;/h4&gt;提出Lo-Hp框架，一个解耦的两阶段权重生成方法，通过学习各种优化策略增强灵活性。采用混合策略子轨迹平衡目标，结合在线学习和离线学习来捕获局部优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明了仅学习局部优化策略可以解决长时程问题，同时增强全局最优权重的生成。在需要频繁权重更新的任务中验证了Lo-Hp的优越准确性和推理效率。&lt;h4&gt;结论&lt;/h4&gt;Lo-Hp在迁移学习、小样本学习、领域泛化和大型语言模型适应等需要频繁权重更新的任务中表现出色，具有更高的准确性和推理效率。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模的最新进展使神经网络能够在不依赖基于梯度优化的情况下生成权重。然而，当前方法受限于过耦合和长时程问题。前者将权重生成与特定任务目标紧密绑定，从而限制了学习优化器的灵活性。后者因缺乏局部约束导致推理效率低下和准确性不足。在本文中，我们提出了Lo-Hp，一个解耦的两阶段权重生成框架，通过学习各种优化策略来提高灵活性。它采用混合策略子轨迹平衡目标，结合在线学习和离线学习来捕获局部优化策略。理论上，我们证明了仅学习局部优化策略可以解决长时程问题，同时增强全局最优权重的生成。此外，我们在需要频繁权重更新的任务（如迁移学习、小样本学习、领域泛化和大型语言模型适应）中验证了Lo-Hp的优越准确性和推理效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in generative modeling enable neural networks to generateweights without relying on gradient-based optimization. However, currentmethods are limited by issues of over-coupling and long-horizon. The formertightly binds weight generation with task-specific objectives, thereby limitingthe flexibility of the learned optimizer. The latter leads to inefficiency andlow accuracy during inference, caused by the lack of local constraints. In thispaper, we propose Lo-Hp, a decoupled two-stage weight generation framework thatenhances flexibility through learning various optimization policies. It adoptsa hybrid-policy sub-trajectory balance objective, which integrates on-policyand off-policy learning to capture local optimization policies. Theoretically,we demonstrate that learning solely local optimization policies can address thelong-horizon issue while enhancing the generation of global optimal weights. Inaddition, we validate Lo-Hp's superior accuracy and inference efficiency intasks that require frequent weight updates, such as transfer learning, few-shotlearning, domain generalization, and large language model adaptation.</description>
      <author>example@mail.com (Yunchuan Guan, Yu Liu, Ke Zhou, Hui Li, Sen Jia, Zhiqi Shen, Ziyang Wang, Xinglin Zhang, Tao Chen, Jenq-Neng Hwang, Lei Li)</author>
      <guid isPermaLink="false">2511.00543v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Onboard Cloud Segmentation in Thermal Earth Observation: From Landsat to a CubeSat Constellation</title>
      <link>http://arxiv.org/abs/2511.00357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work was presented at the TerraBytes Workshop at the 42nd  International Conference on Machine Learning. This version is not part of the  official ICML proceedings&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了CubeSat任务中热红外云分割的挑战，通过迁移学习和轻量级架构实现了准确、高效的在轨云分割，支持实时决策。&lt;h4&gt;背景&lt;/h4&gt;CubeSat任务在热红外地球观测中的云分割是一个关键但研究不足的任务。这些卫星受限于硬件，通常只能依赖单一热红外波段，且缺乏足够的标记数据，使得传统云掩膜技术不可行。&lt;h4&gt;目的&lt;/h4&gt;解决CubeSat任务中热红外云分割的挑战，通过迁移学习方法实现准确、高效的热红外云分割，支持实时决策。&lt;h4&gt;方法&lt;/h4&gt;将迁移学习应用于FOREST-2 CubeSat的热红外云分割任务，使用带有轻量级MobileNet编码器的UNet模型。在公共的Landsat-7云覆盖评估数据集上预训练模型，然后在联合训练设置中使用少量任务特定样本进行微调。最后将模型转换为TensorRT引擎，在NVIDIA Jetson Nano上实现全图像推理。&lt;h4&gt;主要发现&lt;/h4&gt;通过迁移学习方法，宏F1分数从仅使用FOREST-2基线的0.850提高到0.877。利用公共数据集和轻量级架构可以在轨上实现准确、高效的热红外云分割，全图像推理时间不到5秒。&lt;h4&gt;结论&lt;/h4&gt;利用公共数据集和轻量级架构可以在轨上实现准确、高效的热红外云分割，支持数据受限地球观测任务中的实时决策。&lt;h4&gt;翻译&lt;/h4&gt;机载云分割是热红外地球观测中的一个关键但研究不足的任务，特别是对于受限于有限硬件和光谱信息的CubeSat任务。CubeSat通常依赖单一热红外波段，且缺乏足够的标记数据，这使得传统的云掩膜技术不可行。这项工作通过将迁移学习应用于FOREST-2 CubeSat的热红外云分割来解决这些挑战，使用带有轻量级MobileNet编码器的UNet模型。我们在公共的Landsat-7云覆盖评估数据集上预训练模型，然后在联合训练设置中使用少量任务特定样本进行微调，使宏F1分数从仅使用FOREST-2基线的0.850提高到0.877。我们将模型转换为TensorRT引擎，并在NVIDIA Jetson Nano上演示了全图像推理，时间不到5秒。这些结果表明，利用公共数据集和轻量级架构可以在轨上实现准确、高效的热红外云分割，支持数据受限地球观测任务中的实时决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Onboard cloud segmentation is a critical yet underexplored task in thermalEarth observation (EO), particularly for CubeSat missions constrained bylimited hardware and spectral information. CubeSats often rely on a singlethermal band and lack sufficient labeled data, making conventional cloudmasking techniques infeasible. This work addresses these challenges by applyingtransfer learning to thermal cloud segmentation for the FOREST-2 CubeSat, usinga UNet with a lightweight MobileNet encoder. We pretrain the model on thepublic Landsat-7 Cloud Cover Assessment Dataset and fine-tune it with a smallset of mission-specific samples in a joint-training setup, improving the macroF1 from 0.850 to 0.877 over FOREST-2-only baselines. We convert the model to aTensorRT engine and demonstrate full-image inference in under 5 seconds on anNVIDIA Jetson Nano. These results show that leveraging public datasets andlightweight architectures can enable accurate, efficient thermal-only cloudmasking on-orbit, supporting real-time decision-making in data-limited EOmissions.</description>
      <author>example@mail.com (Niklas Wölki, Lukas Kondmann, Christian Mollière, Martin Langer, Julia Gottfriedsen, Martin Werner)</author>
      <guid isPermaLink="false">2511.00357v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>FedReplay: A Feature Replay Assisted Federated Transfer Learning Framework for Efficient and Privacy-Preserving Smart Agriculture</title>
      <link>http://arxiv.org/abs/2511.00269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合冻结的CLIP视觉变换器和轻量级变换器分类器的联邦学习框架，用于解决智能农业中的准确分类问题，同时处理隐私保护和非IID数据分布的挑战。&lt;h4&gt;背景&lt;/h4&gt;准确分类在智能农业中扮演关键角色，但传统集中式训练引发隐私问题，标准联邦学习难以处理非IID数据且通信成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种联邦学习框架，解决数据隐私和非IID数据分布问题，同时降低通信成本。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的CLIP ViT特征提取能力，避免从头训练大规模模型；将联邦更新限制在紧凑分类器上减少传输开销；共享1%的CLIP特征表示对齐不同参与者的类别表示，同时保护隐私。&lt;h4&gt;主要发现&lt;/h4&gt;在农业分类任务上达到86.6%的准确率，比基线联邦学习方法高出4倍以上。&lt;h4&gt;结论&lt;/h4&gt;结合视觉-语言模型特征与联邦学习能有效实现隐私保护和可扩展的农业智能。&lt;h4&gt;翻译&lt;/h4&gt;准确的分类在智能农业中起着关键作用，支持作物监测、果实识别和病虫害检测等应用。然而，传统的集中式训练通常需要大规模数据收集，这引发了隐私问题，而标准的联邦学习难以处理非独立同分布数据并产生高通信成本。为解决这些挑战，我们提出了一种结合冻结的对比语言-图像预训练视觉变换器和轻量级变换器分类器的联邦学习框架。通过利用预训练的CLIP视觉变换器的强大特征提取能力，该框架避免了从头开始训练大规模模型，并将联邦更新限制在紧凑的分类器上，从而显著减少了传输开销。此外，为了减轻由非IID数据分布导致的性能下降，从所有类别中共享一小部分(1%)的CLIP提取的特征表示。这些共享的特征无法逆向还原为原始图像，确保了隐私保护，同时使不同参与者的类别表示保持一致。在农业分类任务上的实验结果表明，所提出的方法达到了86.6%的准确率，比基线联邦学习方法高出4倍以上。这证明了将视觉-语言模型特征与联邦学习相结合的有效性和效率，为隐私保护和可扩展的农业智能提供了新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate classification plays a pivotal role in smart agriculture, enablingapplications such as crop monitoring, fruit recognition, and pest detection.However, conventional centralized training often requires large-scale datacollection, which raises privacy concerns, while standard federated learningstruggles with non-independent and identically distributed (non-IID) data andincurs high communication costs. To address these challenges, we propose afederated learning framework that integrates a frozen ContrastiveLanguage-Image Pre-training (CLIP) vision transformer (ViT) with a lightweighttransformer classifier. By leveraging the strong feature extraction capabilityof the pre-trained CLIP ViT, the framework avoids training large-scale modelsfrom scratch and restricts federated updates to a compact classifier, therebyreducing transmission overhead significantly. Furthermore, to mitigateperformance degradation caused by non-IID data distribution, a small subset(1%) of CLIP-extracted feature representations from all classes is sharedacross clients. These shared features are non-reversible to raw images,ensuring privacy preservation while aligning class representation acrossparticipants. Experimental results on agricultural classification tasks showthat the proposed method achieve 86.6% accuracy, which is more than 4 timeshigher compared to baseline federated learning approaches. This demonstratesthe effectiveness and efficiency of combining vision-language model featureswith federated learning for privacy-preserving and scalable agriculturalintelligence.</description>
      <author>example@mail.com (Long Li, Jiajia Li, Dong Chen, Lina Pu, Haibo Yao, Yanbo Huang)</author>
      <guid isPermaLink="false">2511.00269v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Melanoma Classification Through Deep Ensemble Learning and Explainable AI</title>
      <link>http://arxiv.org/abs/2511.00246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Publisher-formatted version provided under CC BY-NC-ND 4.0 license.  Original source produced by SciTePress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合可解释人工智能技术的机器学习模型，用于黑色素瘤的早期检测，以提高诊断的可靠性和信任度。&lt;h4&gt;背景&lt;/h4&gt;黑色素瘤是最具侵袭性和致命性的皮肤癌之一，如果在早期不被发现和治疗会导致死亡。人工智能技术，特别是深度学习已被开发用于帮助皮肤科医生早期检测黑色素瘤，并取得了高准确率。然而，深度学习模型的黑盒操作导致缺乏可靠性和信任度。&lt;h4&gt;目的&lt;/h4&gt;开发一种可靠的机器学习模型，通过集成学习和可解释人工智能技术来提高黑色素瘤早期检测的准确性和可信度。&lt;h4&gt;方法&lt;/h4&gt;提出了一种机器学习模型，使用三种最先进的深度迁移学习网络的集成学习，并结合可解释人工智能技术来解释预测的基础。&lt;h4&gt;主要发现&lt;/h4&gt;通过集成三种最先进的深度迁移学习网络和可解释人工智能技术，可以提高黑色素瘤检测的准确性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;可解释人工智能技术可以解决深度学习模型黑盒操作导致的可靠性和信任度问题，为医疗诊断领域提供更可靠的AI辅助诊断工具。&lt;h4&gt;翻译&lt;/h4&gt;黑色素瘤是最具侵袭性和致命性的皮肤癌之一，如果在早期不被发现和治疗会导致死亡。人工智能技术最近已被开发用于帮助皮肤科医生早期检测黑色素瘤，基于深度学习的系统能够以高准确率检测这些病变。然而，整个社区必须克服可解释性的限制，才能在医疗诊断领域从深度学习中获得最大收益。由于深度学习模型决策中的黑盒操作缺陷，结果缺乏可靠性和信任度。然而，可解释人工智能(XAI)可以通过解释AI系统的预测来解决这个问题。本文提出了一种机器学习模型，使用三种最先进的深度迁移学习网络的集成学习，并利用XAI技术解释预测的基础，确保预测的可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5220/0012575400003657&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Melanoma is one of the most aggressive and deadliest skin cancers, leading tomortality if not detected and treated in the early stages. Artificialintelligence techniques have recently been developed to help dermatologists inthe early detection of melanoma, and systems based on deep learning (DL) havebeen able to detect these lesions with high accuracy. However, the entirecommunity must overcome the explainability limit to get the maximum benefitfrom DL for diagnostics in the healthcare domain. Because of the black boxoperation's shortcomings in DL models' decisions, there is a lack ofreliability and trust in the outcomes. However, Explainable ArtificialIntelligence (XAI) can solve this problem by interpreting the predictions of AIsystems. This paper proposes a machine learning model using ensemble learningof three state-of-the-art deep transfer Learning networks, along with anapproach to ensure the reliability of the predictions by utilizing XAItechniques to explain the basis of the predictions.</description>
      <author>example@mail.com (Wadduwage Shanika Perera, ABM Islam, Van Vung Pham, Min Kyung An)</author>
      <guid isPermaLink="false">2511.00246v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient and Generalizable Transfer Learning Method for Weather Condition Detection on Ground Terminals</title>
      <link>http://arxiv.org/abs/2511.00211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种高效的迁移学习方法，用于检测地面终端组件上的天气相关条件，如积雪和潮湿等，以提高低地球轨道卫星互联网系统在恶劣天气条件下的性能和可靠性。该方法在检测多种天气条件下表现出色，且具有很好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;随着低地球轨道卫星星座在卫星互联网中的广泛应用，为农村和偏远地区提供了无处不在的连接能力。然而，天气事件对卫星互联网的性能和可靠性有显著影响。雪、雨等不良天气会严重干扰卫星天线等关键地面终端组件的性能，破坏LEO卫星与地面站之间的空间-地面链路条件。&lt;h4&gt;目的&lt;/h4&gt;研究需要基于地区的天气预报以及对地面终端组件上精细化天气条件的检测能力，以支持卫星互联网的故障诊断和缓解，确保系统的可靠性。然而，目前缺乏有效的解决方案，且在实际部署中需要考虑解决方案的有效性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;论文讨论了一种高效的迁移学习(TL)方法，使地面组件能够本地检测代表性的天气相关条件。该方法可以检测由不良和典型天气事件引起的积雪、潮湿等条件。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的迁移学习方法在检测多种天气条件下表现出色，与典型的深度学习方法如YOLOv7、YOLOv9、Faster R-CNN和R-YOLO相比具有优越性能。此外，该方法还显示出能够泛化到各种场景的优势。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习方法为解决天气对卫星互联网地面终端组件的影响提供了有效解决方案，能够提高卫星互联网在恶劣天气条件下的可靠性和性能，且具有良好的泛化能力，适用于各种实际部署场景。&lt;h4&gt;翻译&lt;/h4&gt;随着低地球轨道(LEO)卫星星座在卫星互联网中的日益普及，为农村和偏远地区提供了无处不在的连接能力。然而，天气事件对卫星互联网的性能和可靠性有重大影响。雪、雨等不良天气事件会显著干扰卫星互联网关键地面终端组件（如卫星天线）的性能和运行，严重破坏LEO卫星与地面站之间的空间-地面链路条件。这一挑战不仅需要基于地区的天气预报，还需要对地面终端组件上的精细化天气条件进行精细检测。这种能力可以帮助卫星互联网进行故障诊断和缓解，确保可靠性，但相应的解决方案仍然缺乏，更不用说在实际部署中必不可少的有效性和泛化能力。本文讨论了一种高效的迁移学习(TL)方法，使地面组件能够本地检测代表性的天气相关条件。所提出的方法可以检测由不良和典型天气事件引起的积雪、潮湿等条件，与典型的深度学习方法（如YOLOv7、YOLOv9、Faster R-CNN和R-YOLO）相比表现出优越性能。我们的迁移学习方法还显示出能够泛化到各种场景的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TAES.2024.3496857&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing adoption of satellite Internet with low-Earth-orbit (LEO)satellites in mega-constellations allows ubiquitous connectivity to rural andremote areas. However, weather events have a significant impact on theperformance and reliability of satellite Internet. Adverse weather events suchas snow and rain can disturb the performance and operations of satelliteInternet's essential ground terminal components, such as satellite antennas,significantly disrupting the space-ground link conditions between LEOsatellites and ground stations. This challenge calls for not only region-basedweather forecasts but also fine-grained detection capability on ground terminalcomponents of fine-grained weather conditions. Such a capability can assist infault diagnostics and mitigation for reliable satellite Internet, but itssolutions are lacking, not to mention the effectiveness and generalization thatare essential in real-world deployments. This paper discusses an efficienttransfer learning (TL) method that can enable a ground component to locallydetect representative weather-related conditions. The proposed method candetect snow, wet, and other conditions resulting from adverse and typicalweather events and shows superior performance compared to the typical deeplearning methods, such as YOLOv7, YOLOv9, Faster R-CNN, and R-YOLO. Our TLmethod also shows the advantage of being generalizable to various scenarios.</description>
      <author>example@mail.com (Wenxuan Zhang, Peng Hu)</author>
      <guid isPermaLink="false">2511.00211v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Transfer learning discovery of molecular modulators for perovskite solar cells</title>
      <link>http://arxiv.org/abs/2511.00204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种基于预训练深度神经网络的化学信息迁移学习框架，用于预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响，并通过实验验证了该方法的有效性，实现了26.91%的高效率。&lt;h4&gt;背景&lt;/h4&gt;钙钛矿太阳能电池的有效分子调节剂发现对推进其发展至关重要，但化学空间的庞大以及耗时昂贵的实验筛选阻碍了研究进程。同时，机器学习在加速材料发现方面有潜力，但由于数据稀缺和传统定量结构-性质关系模型的限制，将其应用于钙钛矿太阳能电池仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效预测分子调节剂对钙钛矿太阳能电池性能影响的机器学习方法，以加速新型分子调节剂的发现过程。&lt;h4&gt;方法&lt;/h4&gt;应用基于预训练深度神经网络的化学信息迁移学习框架，通过系统性地对多种分子表示进行基准测试，建立预测模型，并利用可解释性技术可视化学习到的化学表示，最后对筛选出的候选分子进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够低成本、高通量地筛选79,043种商业可用分子，并准确预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响。实验验证表明，该框架确定的前分子调节剂显著提高了电池性能，实现了26.91%的冠军功率转换效率。&lt;h4&gt;结论&lt;/h4&gt;基于预训练深度神经网络的化学信息迁移学习框架为钙钛矿太阳能电池分子调节剂的发现提供了有效工具，克服了传统方法中的数据稀缺和模型限制问题，显著加速了材料发现进程。&lt;h4&gt;翻译&lt;/h4&gt;有效分子调节剂的发现对推进钙钛矿太阳能电池的发展至关重要，但研究过程受到化学空间庞大以及耗时昂贵的实验筛选的阻碍。同时，机器学习在加速材料发现方面具有巨大潜力。然而，由于数据稀缺和传统定量结构-性质关系模型的局限性，将机器学习应用于钙钛矿太阳能电池仍是一个重大挑战。在此，我们应用了一种基于预训练深度神经网络的化学信息迁移学习框架，能够高精度地预测分子调节剂对钙钛矿太阳能电池功率转换效率的影响。通过对多种分子表示进行系统性基准测试建立了该框架，能够以低成本和高通量方式对79,043种商业可用分子进行虚拟筛选。此外，我们利用可解释性技术可视化学习到的化学表示，并对得到的调节剂-钙钛矿相互作用进行实验表征。该框架确定的前分子调节剂随后通过实验验证，在钙钛矿太阳能电池中实现了显著提高的冠军功率转换效率26.91%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery of effective molecular modulators is essential for advancingperovskite solar cells (PSCs), but the research process is hindered by thevastness of chemical space and the time-consuming and expensive trial-and-errorexperimental screening. Concurrently, machine learning (ML) offers significantpotential for accelerating materials discovery. However, applying ML to PSCsremains a major challenge due to data scarcity and limitations of traditionalquantitative structure-property relationship (QSPR) models. Here, we apply achemical informed transfer learning framework based on pre-trained deep neuralnetworks, which achieves high accuracy in predicting the molecular modulator'seffect on the power conversion efficiency (PCE) of PSCs. This framework isestablished through systematical benchmarking of diverse molecularrepresentations, enabling lowcost and high-throughput virtual screening over79,043 commercially available molecules. Furthermore, we leverageinterpretability techniques to visualize the learned chemical representationand experimentally characterize the resulting modulator-perovskiteinteractions. The top molecular modulators identified by the framework aresubsequently validated experimentally, delivering a remarkably improvedchampion PCE of 26.91% in PSCs.</description>
      <author>example@mail.com (Haoming Yan, Xinyu Chen, Yanran Wang, Zhengchao Luo, Weizheng Huang, Hongshuai Wang, Peng Chen, Yuzhi Zhang, Weijie Sun, Jinzhuo Wang, Qihuang Gong, Rui Zhu, Lichen Zhao)</author>
      <guid isPermaLink="false">2511.00204v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval-Augmented Multimodal Depression Detection</title>
      <link>http://arxiv.org/abs/2511.01892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE EMBC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新颖的检索增强生成（RAG）框架，用于抑郁症检测，通过情感提示作为辅助模态提高情感表示和可解释性。&lt;h4&gt;背景&lt;/h4&gt;多模态深度学习通过整合文本、音频和视频信号在抑郁症检测中显示出潜力。然而，现有利用情感分析增强情感理解的方法存在计算成本高、领域不匹配和静态知识限制的问题。&lt;h4&gt;目的&lt;/h4&gt;解决现有抑郁症检测方法中计算成本高、领域不匹配和静态知识限制的问题。&lt;h4&gt;方法&lt;/h4&gt;提出检索增强生成（RAG）框架，给定抑郁症相关文本，从情感数据集中检索语义相关的情感内容，使用大型语言模型生成情感提示作为辅助模态，以丰富情感表示并提高可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;在AVEC 2019数据集上的实验表明，该方法实现了最先进的性能，CCC达到0.593，MAE达到3.95，超过了之前的迁移学习和多任务学习基线。&lt;h4&gt;结论&lt;/h4&gt;检索增强生成框架能有效解决现有方法在抑郁症检测中的问题，提高性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;多模态深度学习通过整合文本、音频和视频信号在抑郁症检测中显示出潜力。最近的研究利用情感分析来增强情感理解，但存在计算成本高、领域不匹配和静态知识限制的问题。为解决这些问题，我们提出了一种新颖的检索增强生成（RAG）框架。给定一个与抑郁症相关的文本，我们的方法从情感数据集中检索语义上相关的情感内容，并使用大型语言模型生成情感提示作为辅助模态。这个提示丰富了情感表示并提高了可解释性。在AVEC 2019数据集上的实验表明，我们的方法实现了最先进的性能，CCC达到0.593，MAE达到3.95，超过了之前的迁移学习和多任务学习基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal deep learning has shown promise in depression detection byintegrating text, audio, and video signals. Recent work leverages sentimentanalysis to enhance emotional understanding, yet suffers from highcomputational cost, domain mismatch, and static knowledge limitations. Toaddress these issues, we propose a novel Retrieval-Augmented Generation (RAG)framework. Given a depression-related text, our method retrieves semanticallyrelevant emotional content from a sentiment dataset and uses a Large LanguageModel (LLM) to generate an Emotion Prompt as an auxiliary modality. This promptenriches emotional representation and improves interpretability. Experiments onthe AVEC 2019 dataset show our approach achieves state-of-the-art performancewith CCC of 0.593 and MAE of 3.95, surpassing previous transfer learning andmulti-task learning baselines.</description>
      <author>example@mail.com (Ruibo Hou, Shiyu Teng, Jiaqing Liu, Shurong Chai, Yinhao Li, Lanfen Lin, Yen-Wei Chen)</author>
      <guid isPermaLink="false">2511.01892v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Mapcher: Loop Closure Detection-Free Heterogeneous LiDAR Multi-Session SLAM Leveraging Outlier-Robust Registration for Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2511.00635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Multi-Mapcher的新型多会话同步定位与地图构建框架，通过大规模地图到地图配准实现会话间初始对齐，克服了传统方法对回环检测的依赖，实验证明该方法在各种LiDAR传感器条件下表现优异且速度更快。&lt;h4&gt;背景&lt;/h4&gt;随着各种3D光探测和测距（LiDAR）传感器被引入市场，使用异构LiDAR传感器进行多会话同步定位与地图构建的研究已经积极展开。现有的MSS方法大多依赖于回环检测来实现会话间的对齐；然而，由于不同会话中使用的传感器在密度和视场（FoV）上存在差异，回环检测的性能可能会降低。&lt;h4&gt;目的&lt;/h4&gt;挑战现有方法严重依赖检测模块的范式，提出一种使用大规模地图到地图配准实现会话间初始对齐的新型MSS框架，这通常被认为不可行。&lt;h4&gt;方法&lt;/h4&gt;利用抗异常值的3D点云配准技术进行会话间初始对齐，然后在假设初始对齐足够精确的情况下，通过基于半径搜索发现会话间的回环，最后采用基于锚节点的鲁棒姿态图优化来构建一致的全局地图。&lt;h4&gt;主要发现&lt;/h4&gt;使用各种LiDAR传感器捕获会话时，新方法表现出显著更好的MSS性能，并且比最先进的方法更快。&lt;h4&gt;结论&lt;/h4&gt;Multi-Mapcher框架通过创新地使用大规模地图到地图配准，有效解决了异构LiDAR传感器在多会话同步定位与地图构建中的挑战，代码已公开在https://github.com/url-kaist/multi-mapcher。&lt;h4&gt;翻译&lt;/h4&gt;随着各种3D光探测和测距（LiDAR）传感器被引入市场，使用异构LiDAR传感器进行多会话同步定位与地图构建（MSS）的研究已经积极展开。现有的MSS方法大多依赖于回环检测来实现会话间的对齐；然而，由于不同会话中使用的传感器在密度和视场（FoV）上存在差异，回环检测的性能可能会降低。本研究挑战了现有方法严重依赖检测模块的范式，提出了一种名为Multi-Mapcher的新型MSS框架，该框架采用大规模地图到地图配准来实现会话间初始对齐，这通常被认为不可行，方法是利用抗异常值的3D点云配准。在假设会话间初始对齐足够精确的情况下，通过基于半径搜索来发现会话间的回环，然后采用基于锚节点的鲁棒姿态图优化来构建一致的全局地图。如我们的实验所示，我们的方法在使用各种LiDAR传感器捕获会话时表现出显著更好的MSS性能，并且比最先进的方法更快。我们的代码可在https://github.com/url-kaist/multi-mapcher获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多会话SLAM中，当使用不同类型激光雷达传感器时，现有方法依赖的回环检测模块性能下降导致会话间对齐失败的问题。这个问题很重要，因为随着市场上各种激光雷达传感器的出现，自动驾驶车辆和机器人可能配备不同类型的传感器，而现有方法在处理异构传感器数据时表现不佳，导致无法构建一致的全局地图，限制了长期地图管理和多机器人协作的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有MSS方法在异构激光雷达传感器上表现不佳，挑战了依赖回环检测的范式。他们借鉴了异常值鲁棒的点云注册方法（如Quatro），使用快速点特征直方图（FPFH）建立对应关系，并引入锚节点概念解决多会话轨迹参考帧不固定的问题。作者还借鉴了基于因子图的姿态图优化方法，但进行了改进以适应多会话场景，实现了地图到地图和扫描到扫描两个级别的鲁棒注册。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是不依赖传统回环检测模块，而是使用异常值鲁棒的点云注册实现会话间对齐。整体流程包括：1) 会话内SLAM处理；2) 地图到地图级别的会话间初始对齐，使用FPFH特征和鲁棒注册估计变换矩阵；3) 扫描到扫描级别的会话间回环闭合，通过半径搜索和截断均方误差过滤错误候选；4) 基于锚节点的姿态图优化；5) 构建一致的全局地图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 不依赖回环检测的LCD-free方法；2) 实现大规模地图到地图注册；3) 在地图和扫描两个级别使用鲁棒注册；4) 提出截断均方误差处理视场差异；5) 支持异构激光雷达传感器。相比之前工作，Multi-Mapcher在处理异构传感器数据时表现更好，不会因LCD性能下降导致失败；将2D地图合并扩展到3D；对部分重叠和环境变化更鲁棒；速度比现有方法快5-9倍。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Multi-Mapcher通过不依赖回环检测的鲁棒注册方法，实现了对异构激光雷达传感器捕获的多会话SLAM的精确对齐，构建了一致的全局地图，同时比现有方法更快且更鲁棒。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As various 3D light detection and ranging (LiDAR) sensors have beenintroduced to the market, research on multi-session simultaneous localizationand mapping (MSS) using heterogeneous LiDAR sensors has been activelyconducted. Existing MSS methods mostly rely on loop closure detection forinter-session alignment; however, the performance of loop closure detection canbe potentially degraded owing to the differences in the density and field ofview (FoV) of the sensors used in different sessions. In this study, wechallenge the existing paradigm that relies heavily on loop detection modulesand propose a novel MSS framework, called Multi-Mapcher, that employslarge-scale map-to-map registration to perform inter-session initial alignment,which is commonly assumed to be infeasible, by leveraging outlier-robust 3Dpoint cloud registration. Next, after finding inter-session loops by radiussearch based on the assumption that the inter-session initial alignment issufficiently precise, anchor node-based robust pose graph optimization isemployed to build a consistent global map. As demonstrated in our experiments,our approach shows substantially better MSS performance for various LiDARsensors used to capture the sessions and is faster than state-of-the-artapproaches. Our code is available athttps://github.com/url-kaist/multi-mapcher.</description>
      <author>example@mail.com (Hyungtae Lim, Daebeom Kim, Hyun Myung)</author>
      <guid isPermaLink="false">2511.00635v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>MambaNetLK: Enhancing Colonoscopy Point Cloud Registration with Mamba</title>
      <link>http://arxiv.org/abs/2511.00260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 4 figures, 3 tables, IPCAI conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MambaNetLK的新型3D点云配准方法，以及一个名为C3VD-Raycasting-10k的大规模临床数据集，用于解决结肠镜引导中生物组织特征退化和术前术后域差异导致的配准稳定性问题。&lt;h4&gt;背景&lt;/h4&gt;准确的3D点云配准是可靠的图像引导结肠镜检查的基础，直接影响病变定位、边缘评估和导航安全性。然而，生物组织具有重复纹理和局部均匀几何特征，导致特征退化，同时术前解剖和术中观察之间的显著域差异进一步降低了配准稳定性。&lt;h4&gt;目的&lt;/h4&gt;解决临床关键挑战，引入一种针对内镜导航的新型3D配准方法，并创建高质量、临床基础的数据集以支持严格且可复现的基准测试。&lt;h4&gt;方法&lt;/h4&gt;引入C3VD-Raycasting-10k数据集，包含10,014对从临床CT数据派生的几何对齐点云；提出MambaNetLK框架，通过将Mamba状态空间模型作为跨模态特征提取器集成到PointNetLK架构中增强其能力，有效捕获长程依赖关系，并使用Lucas-Kanade算法迭代实现对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在临床数据集C3VD-Raycasting-10k上，MambaNetLK实现了最先进的性能，与第二好的方法相比，中值旋转误差降低了56.04%，RMSE平移误差降低了26.19%；该模型在ModelNet40上表现出强大的泛化能力，并对初始姿态扰动具有优异的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MambaNetLK为外科导航中的3D配准提供了强大基础，基于SSM的全局表达特征提取器与大规模临床数据集的结合，使微创手术如结肠镜检查中的引导系统更加准确和可靠。&lt;h4&gt;翻译&lt;/h4&gt;准确的3D点云配准是可靠的图像引导结肠镜检查的基础，直接影响病变定位、边缘评估和导航安全性。然而，生物组织具有重复纹理和局部均匀几何特征，导致特征退化，同时术前解剖和术中观察之间的显著域差异进一步降低了配准稳定性。为解决这些临床关键挑战，我们引入了一种针对内镜导航的新型3D配准方法和高质量、临床基础的数据集以支持严格且可复现的基准测试。我们引入了C3VD-Raycasting-10k，这是一个包含10,014对从临床CT数据派生的几何对齐点云的大规模基准数据集。我们提出了MambaNetLK，一种新的无对应点配准框架，通过将Mamba状态空间模型作为跨模态特征提取器集成到PointNetLK架构中来增强它。因此，所提出的框架以线性时间复杂度有效捕获长程依赖关系。对齐是通过使用Lucas-Kanade算法迭代实现的。在临床数据集C3VD-Raycasting-10k上，MambaNetLK与最先进的方法相比实现了最佳性能，中值旋转误差比第二好的方法降低了56.04%，RMSE平移误差降低了26.19%。该模型在ModelNet40上也表现出强大的泛化能力，并对初始姿态扰动具有优异的鲁棒性。MambaNetLK为外科导航中的3D配准提供了强大的基础。基于SSM的全局表达特征提取器与大规模临床数据集的结合，使结肠镜等微创手术中的引导系统更加准确和可靠。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决结肠镜检查中的3D点云配准问题，即如何准确地将手术中的实时内窥镜数据与术前的CT扫描模型进行对齐。这个问题非常重要，因为准确的配准直接关系到病变定位、边界评估和导航安全，而生物组织的重复纹理和局部均匀几何特性会导致特征退化，同时术前与术中数据间的域偏移会进一步降低配准稳定性，影响临床诊断和治疗效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者基于对现有方法局限性的分析进行设计：基于对应关系的方法在平滑、无纹理的器官表面存在特征退化；PointNetLK等无对应关系方法依赖MLP特征提取器，难以捕获长距离几何依赖；Transformer虽有长距离建模能力但在手术应用中受限。作者借鉴了PointNetLK的框架，但用Mamba状态空间模型替代了MLP特征提取器，并创建了新的临床数据集C3VD-Raycasting-10k来解决基准数据缺乏的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将Mamba状态空间模型整合到Lucas-Kanade配准流程中，通过将点云视为序列来有效捕获全局几何结构，避免显式点对应关系。整体流程：1)输入点云并进行序列化和位置编码；2)使用Mamba块处理位置感知特征序列，选择性传播或遗忘信息；3)通过MLP层和最大池化生成全局特征描述符；4)使用Lucas-Kanade算法迭代最小化源点和目标点云特征差异直至收敛；5)利用相机姿态生成匹配的源点和目标点云对。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MambaNetLK框架，结合Mamba SSM和IC-LK实现长距离依赖建模；2)C3VD-Raycasting-10k临床数据集，提供10,014个视点匹配的点云对；3)高效捕获全局几何结构，克服MLP特征提取器局限；4)线性时间复杂度的长距离依赖建模。相比之前工作：避免了基于对应关系方法中的特征退化问题；比PointNetLK更好地捕获长距离几何依赖；比Transformer更高效且更适合手术应用；解决了缺乏3D配准基准数据集的问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MambaNetLK通过整合Mamba状态空间模型和创建C3VD-Raycasting-10k临床数据集，显著提高了结肠镜检查中3D点云配准的准确性和鲁棒性，为微创手术中的实时临床导航系统奠定了坚实基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D point cloud registration underpins reliable image-guidedcolonoscopy, directly affecting lesion localization, margin assessment, andnavigation safety. However, biological tissue exhibits repetitive textures andlocally homogeneous geometry that cause feature degeneracy, while substantialdomain shifts between pre-operative anatomy and intra-operative observationsfurther degrade alignment stability. To address these clinically criticalchallenges, we introduce a novel 3D registration method tailored for endoscopicnavigation and a high-quality, clinically grounded dataset to support rigorousand reproducible benchmarking. We introduce C3VD-Raycasting-10k, a large-scalebenchmark dataset with 10,014 geometrically aligned point cloud pairs derivedfrom clinical CT data. We propose MambaNetLK, a novel correspondence-freeregistration framework, which enhances the PointNetLK architecture byintegrating a Mamba State Space Model (SSM) as a cross-modal feature extractor.As a result, the proposed framework efficiently captures long-rangedependencies with linear-time complexity. The alignment is achieved iterativelyusing the Lucas-Kanade algorithm. On the clinical dataset, C3VD-Raycasting-10k,MambaNetLK achieves the best performance compared with the state-of-the-artmethods, reducing median rotation error by 56.04% and RMSE translation error by26.19% over the second-best method. The model also demonstrates stronggeneralization on ModelNet40 and superior robustness to initial poseperturbations. MambaNetLK provides a robust foundation for 3D registration insurgical navigation. The combination of a globally expressive SSM-based featureextractor and a large-scale clinical dataset enables more accurate and reliableguidance systems in minimally invasive procedures like colonoscopy.</description>
      <author>example@mail.com (Linzhe Jiang, Jiayuan Huang, Sophia Bano, Matthew J. Clarkson, Zhehua Mao, Mobarak I. Hoque)</author>
      <guid isPermaLink="false">2511.00260v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Grouping-Based Hybrid Color Correction Algorithm for Color Point Clouds</title>
      <link>http://arxiv.org/abs/2511.02397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于分组的混合颜色校正算法用于彩色点云，根据点云重叠率自适应分组，并针对不同组别采用不同的颜色校正方法，通过大量测试验证了算法的有效性。&lt;h4&gt;背景&lt;/h4&gt;彩色点云的颜色一致性校正是3D渲染和压缩应用中的基础且重要任务，而以往的颜色校正方法主要针对彩色图像，而非点云数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于分组的混合颜色校正算法，专门用于彩色点云的颜色一致性校正。&lt;h4&gt;方法&lt;/h4&gt;1) 估计对齐后的源点云和目标点云之间的重叠率；2) 根据重叠率高低，将目标点分为两组(Gcl和Gmod)或三组(Gcl、Gmod和Gdist)；3) 对Gcl组使用K近邻双边插值方法；4) 对Gmod组使用结合KBI和直方图均衡化的方法；5) 对Gdist组使用直方图均衡化方法；6) 讨论算法的分组效应特性和消融研究。&lt;h4&gt;主要发现&lt;/h4&gt;算法的颜色一致性校正效果已通过1086对测试彩色点云与最先进方法进行了验证，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的基于分组的混合颜色校正算法能够有效实现彩色点云的颜色一致性校正，相关C++源代码已在GitHub平台公开。&lt;h4&gt;翻译&lt;/h4&gt;彩色点云的颜色一致性校正是3D渲染和压缩应用中的一个基础而重要的任务。过去，大多数先前的颜色校正方法旨在校正彩色图像的颜色。本文的目的是提出一种基于分组的混合颜色校正算法用于彩色点云。我们的算法首先估计对齐后的源点云和目标点云之间的重叠率，然后根据估计的重叠率高低，自适应地将目标点分为两组，即近距离组Gcl和中距离组Gmod，或三组，即Gcl、Gmod和远距离组Gdist。为了校正Gcl中目标点的颜色，提出了一种基于K近邻的双边插值(KBI)方法。为了校正Gmod中目标点的颜色，提出了一种结合KBI和直方图均衡化(JKHE)的方法。对于Gdist中的目标点，提出了一种直方图均衡化(HE)方法进行颜色校正。最后，我们讨论了算法中无分组效应特性和消融研究。我们的算法在1086对测试彩色点云上与最先进方法进行了比较，证明了期望的颜色一致性校正效果。本算法的C++源代码可以从网站https://github.com/ivpml84079/Point-cloud-color-correction获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决彩色点云对齐后的颜色一致性问题。当两个彩色点云对齐时，由于采集设备参数、光照条件等因素差异，常会出现颜色不一致现象，导致视觉不协调。这个问题在3D视觉、自动驾驶、虚拟现实等领域非常重要，因为颜色不一致会影响点云压缩、人体姿态估计和点云渲染等应用的质量，最终影响合成3D场景的真实感和美观度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有颜色校正方法（如最近邻法、K近邻法、直方图匹配法等）的优缺点，发现这些方法主要针对彩色图像而非点云。作者借鉴了这些方法的基本思想，但针对点云特性进行了改进。作者观察到点云中点与周围点的关系不同于图像像素，因此考虑了点之间的空间距离关系，提出了基于重叠率的分组策略，并针对不同距离的组设计了专门的校正方法，从而形成了这个混合算法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是根据源点云和目标点云之间的重叠率，自适应地将目标点分为不同组，然后对不同组的点采用最适合的颜色校正方法。整体流程为：1)计算点云对齐的重叠率；2)根据重叠率决定将目标点分为两组或三组；3)对近距离组使用K近邻双边插值法校正颜色；4)对中等距离组使用联合K近邻双边插值法和直方图均衡化法校正颜色；5)对远距离组使用直方图均衡化法校正颜色；6)输出颜色校正后的结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出基于重叠率的自适应分组策略；2)针对不同距离组设计专门的校正方法；3)证明算法具有分组效应自由性质，确保组间边界颜色平滑；4)在大量点云对上验证了算法性能。相比之前工作，本文专注于点云而非图像，采用分组策略而非单一方法处理所有点，并在中等距离组中引入了动态权重调整机制，显著提高了颜色校正效果和视觉质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于分组混合的颜色校正算法，通过自适应分组和针对性校正方法，有效解决了彩色点云对齐后的颜色一致性问题，显著提升了合成3D场景的视觉质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Color consistency correction for color point clouds is a fundamental yetimportant task in 3D rendering and compression applications. In the past, mostprevious color correction methods aimed at correcting color for color images.The purpose of this paper is to propose a grouping-based hybrid colorcorrection algorithm for color point clouds. Our algorithm begins by estimatingthe overlapping rate between the aligned source and target point clouds, andthen adaptively partitions the target points into two groups, namely the closeproximity group Gcl and the moderate proximity group Gmod, or three groups,namely Gcl, Gmod, and the distant proximity group Gdist, when the estimatedoverlapping rate is low or high, respectively. To correct color for targetpoints in Gcl, a K-nearest neighbors based bilateral interpolation (KBI) methodis proposed. To correct color for target points in Gmod, a joint KBI and thehistogram equalization (JKHE) method is proposed. For target points in Gdist, ahistogram equalization (HE) method is proposed for color correction. Finally,we discuss the grouping-effect free property and the ablation study in ouralgorithm. The desired color consistency correction benefit of our algorithmhas been justified through 1086 testing color point cloud pairs against thestate-of-the-art methods. The C++ source code of our algorithm can be accessedfrom the website: https://github.com/ivpml84079/Point-cloud-color-correction.</description>
      <author>example@mail.com (Kuo-Liang Chung, Ting-Chung Tang)</author>
      <guid isPermaLink="false">2511.02397v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Moving Object Segmentation of Sparse and Noisy Radar Point Clouds</title>
      <link>http://arxiv.org/abs/2511.02395v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at IEEE International Conference on  Intelligent Transportation Systems (ITSC 2025), 8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种自监督学习方法，用于稀疏和有噪声的雷达点云的运动目标分割。该方法采用两步式方法：首先使用基于聚类的对比自监督表示学习进行预训练，然后使用有限的标注数据进行监督微调。作者提出了一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类优化，使网络能够生成雷达数据的运动感知表示。通过自监督预训练，该方法在微调后提高了标签效率，并有效提升了最先进性能。&lt;h4&gt;背景&lt;/h4&gt;运动目标分割对自动驾驶等自主移动系统的安全和可靠性至关重要，可以提高后续任务（如SLAM或路径规划）的可靠性和鲁棒性。虽然相机或LiDAR数据的分割已被广泛研究并取得良好成果，但通常需要累积时间序列来获得必要的时间上下文，从而增加了延迟。雷达传感器通过提供点的多普勒速度的直接测量值解决了单扫描运动目标分割的问题。然而，雷达点云通常稀疏且有噪声，使得在监督学习中使用的数据标注非常繁琐、耗时且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;解决雷达点云数据标注困难的问题，提出一种自监督方法，用于稀疏和有噪声的雷达点云的运动目标分割，减少对大量标注数据的依赖，提高分割效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;采用两步方法：1) 对比自监督表示学习：提出一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类优化，预训练网络以生成雷达数据的运动感知表示；2) 监督微调：使用有限的标注数据进行监督微调。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在微调后提高了标签效率，通过自监督预训练有效提升了最先进性能。&lt;h4&gt;结论&lt;/h4&gt;自监督学习方法可以有效解决雷达点云运动目标分割中标注数据不足的问题，通过预训练和微调的两步策略，能够在有限标注数据的情况下实现高性能的分割结果。&lt;h4&gt;翻译&lt;/h4&gt;运动目标分割对于自动驾驶等自主移动系统的安全和可靠性至关重要，可以提高后续任务（如SLAM或路径规划）的可靠性和鲁棒性。虽然相机或LiDAR数据的分割已被广泛研究并取得良好成果，但通常需要累积时间序列来获得必要的时间上下文，从而增加了延迟。雷达传感器通过提供点的多普勒速度的直接测量值解决了这个问题，可用于单扫描运动目标分割。然而，雷达点云通常稀疏且有噪声，使得在监督学习中使用的数据标注非常繁琐、耗时且成本高昂。为解决这个问题，我们解决了稀疏和有噪声的雷达点云的自监督运动目标分割任务。我们采用对比自监督表示学习与后续使用有限标注数据的监督微调的两步方法。我们提出了一种新颖的基于聚类的对比损失函数，通过动态点移除进行聚类优化，预训练网络以生成雷达数据的运动感知表示。我们的方法在微调后提高了标签效率，通过自监督预训练有效提升了最先进性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决稀疏和有噪声的雷达点云上的移动物体分割问题。这个问题在现实中非常重要，因为移动物体分割对自动驾驶汽车等自主移动系统的安全和可靠性至关重要，能够提高后续任务如SLAM或路径规划的可靠性和鲁棒性。虽然相机和LiDAR数据的分割研究广泛，但它们需要积累时间序列数据，增加了系统延迟。雷达传感器可以通过多普勒速度直接测量实现单次扫描的移动物体分割，但雷达点云的稀疏性和噪声性使得数据标注非常困难、耗时且成本高昂。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的问题：相机和LiDAR需要时间序列增加延迟，雷达虽能单次扫描但数据质量差导致标注困难。作者借鉴了自监督学习思想，采用两步方法：先进行对比自监督表征学习预训练，再用有限标注数据监督微调。设计上采用学生-教师框架（自监督学习中常用），并基于雷达实例变换器（RIT）架构进行修改。核心创新是设计了新的对比损失函数，结合聚类和动态点移除（DPR）算法来生成伪标签，使网络学习运动感知的表征。作者还借鉴了LiDAR领域的自监督表征学习方法，但针对雷达数据的特性进行了调整。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用自监督学习减少对标注数据的依赖，设计专门的对比损失函数使网络学习能区分运动和静止物体的表征，结合空间信息和运动信息提高分割性能。整体流程包括：1)数据准备：使用包含坐标、多普勒速度和雷达横截面积的雷达点云；2)自监督预训练：构建学生-教师框架，使用HDBSCAN聚类，通过DPR算法细化聚类分离运动和静止点，计算对比损失拉近同类聚类、推远异类聚类；3)监督微调：添加MLP生成分割掩码，在少量标注数据上微调；4)评估：在View-of-Delft和RadarScenes数据集上用IoU指标评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)新的聚类对比损失函数，专门针对雷达数据设计，关注运动信息而非语义特征；2)自监督预训练框架首次应用于雷达点云的移动物体分割；3)两步训练方法提高标签效率。相比之前工作的不同：与相机/LiDAR方法不需时间序列减少延迟；与RaFlow等自监督方法相比在少量标注数据上表现更好；与现有监督方法在相同标注量下性能更高；与现有对比损失函数不同，专注于运动信息而非语义特征；结合聚类和算法方法生成伪标签而非依赖数据标注。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种新的自监督学习方法，通过设计专门的聚类对比损失函数，在少量标注数据的情况下显著提高了稀疏和噪声雷达点云上的移动物体分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Moving object segmentation is a crucial task for safe and reliable autonomousmobile systems like self-driving cars, improving the reliability and robustnessof subsequent tasks like SLAM or path planning. While the segmentation ofcamera or LiDAR data is widely researched and achieves great results, it oftenintroduces an increased latency by requiring the accumulation of temporalsequences to gain the necessary temporal context. Radar sensors overcome thisproblem with their ability to provide a direct measurement of a point's Dopplervelocity, which can be exploited for single-scan moving object segmentation.However, radar point clouds are often sparse and noisy, making data annotationfor use in supervised learning very tedious, time-consuming, andcost-intensive. To overcome this problem, we address the task ofself-supervised moving object segmentation of sparse and noisy radar pointclouds. We follow a two-step approach of contrastive self-supervisedrepresentation learning with subsequent supervised fine-tuning using limitedamounts of annotated data. We propose a novel clustering-based contrastive lossfunction with cluster refinement based on dynamic points removal to pretrainthe network to produce motion-aware representations of the radar data. Ourmethod improves label efficiency after fine-tuning, effectively boostingstate-of-the-art performance by self-supervised pretraining.</description>
      <author>example@mail.com (Leon Schwarzer, Matthias Zeller, Daniel Casado Herraez, Simon Dierl, Michael Heidingsfeld, Cyrill Stachniss)</author>
      <guid isPermaLink="false">2511.02395v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>3D Point Cloud Object Detection on Edge Devices for Split Computing</title>
      <link>http://arxiv.org/abs/2511.02293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages. This version includes minor lstlisting configuration  adjustments for successful compilation. No changes to content or layout.  Originally published at ACM/IEEE RAGE 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用分割计算技术解决了自动驾驶领域中深度学习模型在边缘设备上计算负担重、处理时间长和功耗高的问题，实验结果表明该方法能有效减少推理时间和边缘设备执行时间。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶技术领域正在快速发展，深度学习是其中的关键组成部分。特别是在感知领域，利用LiDAR收集的3D点云数据来运行深度神经网络模型进行3D物体检测。&lt;h4&gt;目的&lt;/h4&gt;解决最先进的深度学习模型在边缘设备上处理时间长和功耗高的问题，通过分割计算减轻边缘设备的计算负担。&lt;h4&gt;方法&lt;/h4&gt;采用分割计算，一种分布式机器学习推理方法，将计算任务分割处理，只传输深度神经网络模型的中间数据，从而减少边缘设备的计算负担和数据泄露风险。&lt;h4&gt;主要发现&lt;/h4&gt;在体素化后分割，推理时间减少70.8%，边缘设备执行时间减少90.0%；在网络内分割，推理时间减少高达57.1%，边缘设备执行时间减少高达69.5%。&lt;h4&gt;结论&lt;/h4&gt;分割计算能有效解决自动驾驶技术中边缘设备计算负担重的问题，显著减少处理时间和功耗，同时提高数据安全性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶技术领域正在快速发展，深度学习是其关键组成部分。特别是在感知领域，利用LiDAR收集的3D点云数据来运行深度神经网络模型进行3D物体检测。然而，这些最先进的模型复杂度高，导致边缘设备处理时间长和功耗增加。本研究旨在通过利用分割计算（一种分布式机器学习推理方法）来解决这些问题。分割计算旨在减轻边缘设备的计算负担，从而减少处理时间和功耗。此外，它仅传输深度神经网络模型的中间数据，从而最小化数据泄露风险。实验结果表明，在体素化后分割可将推理时间减少70.8%，边缘设备执行时间减少90.0%。在网络内分割时，推理时间可减少高达57.1%，边缘设备执行时间可减少高达69.5%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决边缘设备进行3D点云目标检测时计算负担过重的问题。在自动驾驶领域，LiDAR收集的3D点云数据需要通过复杂深度神经网络处理，但边缘设备计算能力有限，导致处理时间长、功耗高，影响实时性能和安全。这一问题重要是因为它关系到自动驾驶系统的可靠性和实用性，计算效率不足可能导致安全隐患，而轻量级模型又会降低检测准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了边缘设备处理复杂模型的局限性，考虑了轻量级模型和直接传输数据到服务器两种方案，但发现它们分别存在准确性和隐私问题。因此，作者借鉴了Split Computing这一分布式机器学习方法，将其应用于3D点云目标检测场景。作者使用了OpenPCDet作为检测工具箱，并参考了BottleFit等现有SC方法，同时选择了Voxel R-CNN作为基础模型，通过实验确定最佳分割点。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将深度神经网络模型在中间分割成两部分(head model和tail model)，head模型在边缘设备运行，tail模型在边缘服务器运行，只传输中间处理结果而非原始数据。实现流程为：1)边缘设备接收点云数据并预处理；2)模型在预设分割点处分割；3)边缘设备运行head模型；4)将head模型输出传输到边缘服务器；5)边缘服务器运行tail模型生成预测结果；6)将结果返回给边缘设备。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将Split Computing应用于3D点云目标检测；2)系统研究了不同分割点对性能的影响；3)提出选择分割点的两个关键标准(早期分割和最小输出数据大小)；4)通过实验验证了方法的有效性。相比之前工作，这种方法在保持高检测准确性的同时显著降低了边缘设备计算负担和执行时间，相比直接传输原始数据保护了隐私，相比轻量级模型维持了更好的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于Split Computing的3D点云目标检测方法，通过将深度学习模型分割并在边缘设备和边缘服务器间协同计算，显著降低了边缘设备的计算负担和执行时间，同时保护了数据隐私。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/RAGE62451.2024.00009&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The field of autonomous driving technology is rapidly advancing, with deeplearning being a key component. Particularly in the field of sensing, 3D pointcloud data collected by LiDAR is utilized to run deep neural network models for3D object detection. However, these state-of-the-art models are complex,leading to longer processing times and increased power consumption on edgedevices. The objective of this study is to address these issues by leveragingSplit Computing, a distributed machine learning inference method. SplitComputing aims to lessen the computational burden on edge devices, therebyreducing processing time and power consumption. Furthermore, it minimizes therisk of data breaches by only transmitting intermediate data from the deepneural network model. Experimental results show that splitting aftervoxelization reduces the inference time by 70.8% and the edge device executiontime by 90.0%. When splitting within the network, the inference time is reducedby up to 57.1%, and the edge device execution time is reduced by up to 69.5%.</description>
      <author>example@mail.com (Taisuke Noguchi, Takuya Azumi)</author>
      <guid isPermaLink="false">2511.02293v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR-VGGT: Cross-Modal Coarse-to-Fine Fusion for Globally Consistent and Metric-Scale Dense Mapping</title>
      <link>http://arxiv.org/abs/2511.01186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为LiDAR-VGGT的新型框架，通过两阶段融合流程将激光雷达惯性里程计与VGGT模型紧密结合，实现了大规模彩色点云的有效重建，解决了现有方法在可扩展性和度量尺度方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;大规模彩色点云重建是机器人学中的重要任务，支持感知、导航和场景理解。然而，现有的激光雷达惯性视觉里程计(LIVO)对外部校准高度敏感，而3D视觉基础模型如VGGT在大规模环境中可扩展性有限且缺乏度量尺度。&lt;h4&gt;目的&lt;/h4&gt;克服现有技术的局限性，提出一种能够生成密集、全局一致的彩色点云的新型框架。&lt;h4&gt;方法&lt;/h4&gt;提出LiDAR-VGGT框架，通过两阶段由粗到细的融合流程紧密耦合激光雷达惯性里程计与VGGT模型。第一阶段采用预融合模块进行鲁棒的初始化细化，有效估计VGGT姿态和具有粗略度量尺度的点云；第二阶段通过后融合模块增强跨模态3D相似性变换，使用基于边界框的正则化减少传感器间视场不一致导致的尺度失真。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，性能优于基于VGGT的方法和LIVO基线。&lt;h4&gt;结论&lt;/h4&gt;提出的LiDAR-VGGT框架有效解决了现有技术在点云重建中的局限性，并将新型彩色点云评估工具包实现为开源软件。&lt;h4&gt;翻译&lt;/h4&gt;重建大规模彩色点云是机器人学中的重要任务，支持感知、导航和场景理解。尽管激光雷达惯性视觉里程计(LIVO)有所进步，但其性能对外部校准高度敏感。同时，3D视觉基础模型如VGGT在大规模环境中可扩展性有限且缺乏度量尺度。为克服这些限制，我们提出了LiDAR-VGGT，一种通过两阶段由粗到细的融合流程紧密耦合激光雷达惯性里程计与最先进的VGGT模型的新型框架：首先，具有鲁棒初始化细化的预融合模块有效估计了每个会话内具有粗略度量尺度的VGGT姿态和点云；然后，后融合模块增强跨模态3D相似性变换，使用基于边界框的正则化减少激光雷达和相机传感器之间视场不一致导致的尺度失真。在多个数据集上的大量实验表明，LiDAR-VGGT实现了密集、全局一致的彩色点云，性能优于基于VGGT的方法和LIVO基线。我们提出的新型彩色点云评估工具包实现将作为开源软件发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决两个问题：1) 传统LiDAR惯性视觉里程计(LIVO)方法对传感器外参校准敏感且点云稀疏；2) 3D视觉基础模型(如VGGT)在大环境中缺乏全局一致性和度量尺度。这个问题在机器人领域非常重要，因为准确的彩色点云重建是机器人感知、导航和场景理解的基础，对自主导航、多机器人协作和自动驾驶等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的优缺点：LIVO提供精确定位但对校准敏感且点云稀疏，VGGT能生成密集彩色点云但缺乏全局一致性和度量尺度。作者借鉴了LIVO的LiDAR-IMU融合获取真实尺度参考，借鉴VGGT的视觉几何变换生成密集重建，借鉴SLAM中的位图优化确保全局一致性。在此基础上，作者设计了新的两阶段粗到细融合框架：预融合模块使用LIVO初始化和校准VGGT，后融合模块通过增强的跨模态配准进一步优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过将LiDAR-IMU视觉里程计与VGGT模型紧密耦合，利用LiDAR提供真实世界尺度信息解决VGGT缺乏度量尺度的问题，同时利用VGGT生成密集彩色点云的能力克服LiDAR点云稀疏的局限性。整体流程分为：1) 预融合模块：将长图像序列分成多个会话，独立使用VGGT处理，通过线性验证和尺度RANSAC精炼VGGT位姿，转换到世界坐标系；2) 后融合模块：使用基于边界框正则化的增强跨模态Sim(3)配准，将VGGT点云与LiDAR点云对齐，应用全局位图优化确保全局一致性；3) 彩色地图评估：提出四种评估指标评估重建质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 提出LiDAR-VGGT框架，首次将LiDAR与VGGT融合；2) 设计预融合模块，通过线性验证和尺度RANSAC精炼VGGT位姿；3) 引入基于边界框正则化的跨模态Sim(3)配准，解决视场不一致导致的尺度失真；4) 提出新的彩色点云评估工具。相比之前工作：1) 比纯LIVO方法生成更密集点云且对校准误差不那么敏感；2) 比纯VGGT方法提供真实度量尺度和更好全局一致性；3) 比其他融合方法直接处理RGB图像，效率更高；4) 专门评估彩色点云质量，不仅关注几何质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LiDAR-VGGT通过创新的粗到细跨模态融合方法，成功将LiDAR的真实世界度量信息与VGGT的密集彩色重建能力相结合，实现了大规模、全局一致且具有准确度量尺度的彩色点云地图重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing large-scale colored point clouds is an important task inrobotics, supporting perception, navigation, and scene understanding. Despiteadvances in LiDAR inertial visual odometry (LIVO), its performance remainshighly sensitive to extrinsic calibration. Meanwhile, 3D vision foundationmodels, such as VGGT, suffer from limited scalability in large environments andinherently lack metric scale. To overcome these limitations, we proposeLiDAR-VGGT, a novel framework that tightly couples LiDAR inertial odometry withthe state-of-the-art VGGT model through a two-stage coarse- to-fine fusionpipeline: First, a pre-fusion module with robust initialization refinementefficiently estimates VGGT poses and point clouds with coarse metric scalewithin each session. Then, a post-fusion module enhances cross-modal 3Dsimilarity transformation, using bounding-box-based regularization to reducescale distortions caused by inconsistent FOVs between LiDAR and camera sensors.Extensive experiments across multiple datasets demonstrate that LiDAR-VGGTachieves dense, globally consistent colored point clouds and outperforms bothVGGT-based methods and LIVO baselines. The implementation of our proposed novelcolor point cloud evaluation toolkit will be released as open source.</description>
      <author>example@mail.com (Lijie Wang, Lianjie Guo, Ziyi Xu, Qianhao Wang, Fei Gao, Xieyuanli Chen)</author>
      <guid isPermaLink="false">2511.01186v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>GauDP: Reinventing Multi-Agent Collaboration through Gaussian-Image Synergy in Diffusion Policies</title>
      <link>http://arxiv.org/abs/2511.00998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Project page:  https://ziyeeee.github.io/gaudp.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GauDP，一种新的高斯图像协同表示方法，用于在多智能体协作系统中实现可扩展的、感知感知的模仿学习。该方法通过从分散的RGB观测构建全局一致的3D高斯场，并动态将3D高斯属性重新分配给各智能体的局部视角，使智能体能够保持各自视角的同时从共享场景表示中查询任务关键特征。&lt;h4&gt;背景&lt;/h4&gt;在具身多智能体系统中实现有效协调仍然是一个基本挑战，特别是在智能体必须平衡个体视角与全局环境感知的场景中。现有方法往往难以平衡细粒度的局部控制和全面的场景理解，导致可扩展性有限且协作质量受损。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的表示方法，使多智能体系统能够同时实现细粒度控制和全局连贯的行为，而无需额外的感知模式（如3D点云）。&lt;h4&gt;方法&lt;/h4&gt;GauDP方法包括：1) 从分散的RGB观测构建全局一致的3D高斯场；2) 动态将3D高斯属性重新分配给每个智能体的局部视角；3) 使所有智能体能够从共享场景表示中自适应查询任务关键特征，同时保持各自的视角。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboFactory基准测试（包括多种多臂操作任务）上评估，GauDP方法比现有基于图像的方法表现出优越的性能，接近点云驱动方法的有效性，同时随着智能体数量的增加保持强大的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;GauDP提供了一种新的表示方法，能够在多智能体协作系统中实现细粒度控制和全局连贯的行为，无需额外的感知模式，且具有良好的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;最近，在具身多智能体系统中实现有效协调仍然是一个基本挑战，特别是在智能体必须平衡个体视角与全局环境感知的场景中。现有方法往往难以平衡细粒度的局部控制和全面的场景理解，导致可扩展性有限且协作质量受损。在本文中，我们提出了GauDP，一种新的高斯图像协同表示方法，用于在多智能体协作系统中实现可扩展的、感知感知的模仿学习。具体来说，GauDP从分散的RGB观测构建全局一致的3D高斯场，然后动态将3D高斯属性重新分配给每个智能体的局部视角。这使得所有智能体能够在保持各自视角的同时从共享场景表示中自适应查询任务关键特征。这种设计实现了细粒度控制和全局连贯的行为，而无需额外的感知模式（如3D点云）。我们在RoboFactory基准测试上评估了GauDP，该测试包括多种多臂操作任务。我们的方法比现有基于图像的方法表现出优越的性能，接近点云驱动方法的有效性，同时随着智能体数量的增加保持强大的可扩展性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决具身多智能体系统中的有效协调问题，特别是在智能体需要平衡个体视角与全局环境感知的场景中。这个问题在现实中非常重要，因为许多实际应用（如工业装配、手术机器人和辅助家务任务）需要多个智能体协调工作。如果智能体之间不能有效协调，可能会导致碰撞或任务中断等灾难性失败。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了多智能体控制的两种主要范式：聚合所有智能体的局部观察或使用全局环境观察。发现第一种方法无法捕捉联合协作状态，第二种方法缺乏高分辨率细节。作者借鉴了3D高斯溅射技术用于3D场景重建，以及扩散策略框架用于动作生成。他们设计了一个统一的图像-高斯表示框架，通过3D高斯场构建全局一致表示，然后动态分配给各智能体。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过3D高斯表示融合局部和全局观察，使智能体在保持个体视角的同时，能从共享场景中查询任务关键特征。流程包括：1) 从各智能体的RGB图像构建全局3D高斯场；2) 动态将高斯属性重新分配给各智能体；3) 智能体从共享高斯表示中提取任务特征；4) 使用扩散策略基于融合特征预测动作。具体实现使用Noposplat网络重建3D高斯，通过交叉视图ViT解码器融合信息，并引入深度监督提高保真度。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的图像-高斯表示框架；2) 动态表示选择机制；3) 选择性全局上下文分发；4) 像素级协同策略。相比之前工作，GauDP的不同之处在于：仅使用RGB输入就能达到接近点云方法的性能；能自然扩展到更多智能体；同时提供精细控制和全局一致行为；在RoboFactory基准测试中显著优于现有图像方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GauDP通过3D高斯-图像协同表示，使多智能体系统在仅使用RGB输入的情况下就能实现接近点云方法的协作性能，同时保持良好的可扩展性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, effective coordination in embodied multi-agent systems has remaineda fundamental challenge, particularly in scenarios where agents must balanceindividual perspectives with global environmental awareness. Existingapproaches often struggle to balance fine-grained local control withcomprehensive scene understanding, resulting in limited scalability andcompromised collaboration quality. In this paper, we present GauDP, a novelGaussian-image synergistic representation that facilitates scalable,perception-aware imitation learning in multi-agent collaborative systems.Specifically, GauDP constructs a globally consistent 3D Gaussian field fromdecentralized RGB observations, then dynamically redistributes 3D Gaussianattributes to each agent's local perspective. This enables all agents toadaptively query task-critical features from the shared scene representationwhile maintaining their individual viewpoints. This design facilitates bothfine-grained control and globally coherent behavior without requiringadditional sensing modalities (e.g., 3D point cloud). We evaluate GauDP on theRoboFactory benchmark, which includes diverse multi-arm manipulation tasks. Ourmethod achieves superior performance over existing image-based methods andapproaches the effectiveness of point-cloud-driven methods, while maintainingstrong scalability as the number of agents increases.</description>
      <author>example@mail.com (Ziye Wang, Li Kang, Yiran Qin, Jiahua Ma, Zhanglin Peng, Lei Bai, Ruimao Zhang)</author>
      <guid isPermaLink="false">2511.00998v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow</title>
      <link>http://arxiv.org/abs/2511.00977v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 15 figures, to appear in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种名为NicheFlow的基于流的生成模型，用于推断细胞微环境在时空数据中的演化轨迹。&lt;h4&gt;背景&lt;/h4&gt;理解时空数据中细胞微环境的演化对于解析组织发育和疾病进展至关重要。当前模拟细胞进化的方法在单细胞水平上操作，忽略了组织中细胞状态的协调发育。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够推断细胞微环境在连续空间切片上的时间轨迹的方法，克服现有单细胞水平方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;NicheFlow是一种基于流的生成模型，通过将局部细胞邻域表示为点云，使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。&lt;h4&gt;主要发现&lt;/h4&gt;NicheFlow成功从多样化的时空数据集中恢复了全局空间架构和局部微环境组成，从胚胎发育到大脑发育。&lt;h4&gt;结论&lt;/h4&gt;NicheFlow能够有效地建模细胞微环境的时空演化，为理解组织发育和疾病进展提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;理解时空数据中细胞微环境的演化对于解析组织发育和疾病进展至关重要。虽然像空间转录组学这样的实验技术现在能够在时空上实现组织组织的高分辨率映射，但当前模拟细胞进化的方法在单细胞水平上操作，忽略了组织中细胞状态的协调发育。我们介绍了NicheFlow，一种基于流的生成模型，用于推断细胞微环境在连续空间切片上的时间轨迹。通过将局部细胞邻域表示为点云，NicheFlow使用最优传输和变分流匹配联合建模细胞状态和空间坐标的演化。我们的方法成功从多样化的时空数据集中恢复了全局空间架构和局部微环境组成，从胚胎发育到大脑发育。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the evolution of cellular microenvironments in spatiotemporaldata is essential for deciphering tissue development and disease progression.While experimental techniques like spatial transcriptomics now enablehigh-resolution mapping of tissue organization across space and time, currentmethods that model cellular evolution operate at the single-cell level,overlooking the coordinated development of cellular states in a tissue. Weintroduce NicheFlow, a flow-based generative model that infers the temporaltrajectory of cellular microenvironments across sequential spatial slides. Byrepresenting local cell neighborhoods as point clouds, NicheFlow jointly modelsthe evolution of cell states and spatial coordinates using optimal transportand Variational Flow Matching. Our approach successfully recovers both globalspatial architecture and local microenvironment composition across diversespatiotemporal datasets, from embryonic to brain development.</description>
      <author>example@mail.com (Kristiyan Sakalyan, Alessandro Palma, Filippo Guerranti, Fabian J. Theis, Stephan Günnemann)</author>
      <guid isPermaLink="false">2511.00977v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>URDF-Anything: Constructing Articulated Objects with 3D Multimodal Language Model</title>
      <link>http://arxiv.org/abs/2511.00940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the 39th Conference on Neural Information Processing  Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为URDF-Anything的端到端自动重建框架，基于3D多模态大语言模型，用于构建关节物体的精确数字孪生，显著提高了几何分割、运动学参数预测和物理执行能力。&lt;h4&gt;背景&lt;/h4&gt;构建关节物体的精确数字孪生对于机器人模拟训练和具身AI世界模型构建至关重要，但传统方法需要繁琐的手动建模或多阶段流程。&lt;h4&gt;目的&lt;/h4&gt;开发一种端到端的自动重建框架，简化关节物体数字孪生的构建过程，提高其准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出URDF-Anything框架，基于3D多模态大语言模型，利用点云和文本多模态输入的自回归预测框架联合优化几何分割和运动学参数预测，实现专门的[SEG]令牌机制与点云特征交互。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在几何分割(mIoU提高17%)、运动学参数预测(平均误差减少29%)和物理执行能力(比基线提高50%)方面显著优于现有方法，且表现出优秀的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;URDF-Anything为构建机器人模拟的数字孪生提供了高效解决方案，显著提高了模拟到现实的迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;构建关节物体的精确数字孪生对于机器人模拟训练和具身AI世界模型构建至关重要，但传统上需要繁琐的手动建模或多阶段流程。在这项工作中，我们提出了URDF-Anything，一种基于3D多模态大语言模型的端到端自动重建框架。URDF-Anything利用基于点云和文本多模态输入的自回归预测框架，联合优化几何分割和运动学参数预测。它实现了一个专门的[SEG]令牌机制，直接与点云特征交互，实现细粒度的部件级分割，同时保持与运动学参数预测的一致性。在模拟和真实世界数据集上的实验表明，我们的方法在几何分割(mIoU提高17%)、运动学参数预测(平均误差减少29%)和物理执行能力(比基线提高50%)方面显著优于现有方法。值得注意的是，我们的方法表现出优秀的泛化能力，即使在训练集外的物体上也能良好表现。这项工作为构建机器人模拟的数字孪生提供了高效解决方案，显著提高了模拟到现实的迁移能力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决铰接物体（如门、抽屉、剪刀等具有内部连接结构的物体）的数字孪生自动重建问题。传统方法需要繁琐的手动建模或多阶段处理流程，而本文提出的方法可以直接从视觉输入（单视图或多视图图像）自动生成功能性的URDF模型。这个问题在机器人仿真训练、具身AI世界模型构建、自动驾驶和交互式虚拟/增强现实环境中至关重要，因为这些应用需要精确的物体表示来进行准确的物理模拟和交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：之前的铰接物体重建方法要么依赖给定的网格资产库，要么涉及单独的部件分割阶段，无法实现端到端的处理。作者设计了一个基于3D多模态大语言模型（MLLM）的框架，利用其处理多模态输入的能力、大规模预训练获取的3D形状先验以及直接理解空间关系的能力。该方法借鉴了ShapeLLM作为骨干网络，并创新性地应用了LISA中的[SEG]标记机制来实现符号铰接结构与几何分割的同步预测。在点云生成方面，作者使用了DUSt3R（多视图）和LGM（单视图）等现有方法，但在整体框架上进行了创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用3D多模态大语言模型的能力，通过特殊的[SEG]标记机制实现符号铰接结构与几何分割的联合预测，确保预测的运动学与重建几何之间的一致性。整体流程包括：1)输入表示：将视觉输入转换为3D点云；2)多模态铰接解析：3D MLLM联合预测部件分割和运动学参数；3)几何分割：通过[SEG]标记机制对点云进行精细分割；4)网格转换和URDF生成：将分割结果和运动学参数整合为标准URDF文件。这种方法实现了从原始视觉输入到可直接用于物理模拟的完整URDF模型的端到端处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个用于铰接物体重建的端到端3D MLLM框架；2)[SEG]标记机制实现几何分割和运动学参数的深度耦合与联合预测；3)端到端训练确保几何与运动学之间的一致性；4)强大的泛化能力，在训练集外物体上表现优异。相比之前的工作，本文直接使用原始3D点云作为输入而非简化表示（如OBB），采用端到端处理而非多阶段流水线，同时预测几何和运动学参数而非分别处理，并通过创新机制确保两者之间的一致性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了URDF-Anything，一种基于3D多模态大语言模型的端到端框架，能够从视觉输入自动重建铰接物体的功能URDF数字孪生，实现了几何分割和运动学参数的高精度联合预测，显著提升了模拟到现实转换的能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Constructing accurate digital twins of articulated objects is essential forrobotic simulation training and embodied AI world model building, yethistorically requires painstaking manual modeling or multi-stage pipelines. Inthis work, we propose \textbf{URDF-Anything}, an end-to-end automaticreconstruction framework based on a 3D multimodal large language model (MLLM).URDF-Anything utilizes an autoregressive prediction framework based onpoint-cloud and text multimodal input to jointly optimize geometricsegmentation and kinematic parameter prediction. It implements a specialized$[SEG]$ token mechanism that interacts directly with point cloud features,enabling fine-grained part-level segmentation while maintaining consistencywith the kinematic parameter predictions. Experiments on both simulated andreal-world datasets demonstrate that our method significantly outperformsexisting approaches regarding geometric segmentation (mIoU 17\% improvement),kinematic parameter prediction (average error reduction of 29\%), and physicalexecutability (surpassing baselines by 50\%). Notably, our method exhibitsexcellent generalization ability, performing well even on objects outside thetraining set. This work provides an efficient solution for constructing digitaltwins for robotic simulation, significantly enhancing the sim-to-real transfercapability.</description>
      <author>example@mail.com (Zhe Li, Xiang Bai, Jieyu Zhang, Zhuangzhe Wu, Che Xu, Ying Li, Chengkai Hou, Shanghang Zhang)</author>
      <guid isPermaLink="false">2511.00940v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Persistence-Based Statistics for Detecting Structural Changes in High-Dimensional Point Clouds</title>
      <link>http://arxiv.org/abs/2511.00938v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  42 pages, 3 figures, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了持久性统计在分布变化下的概率行为，并提出了一种用于检测高维随机点云结构变化的新非参数框架。&lt;h4&gt;背景&lt;/h4&gt;持久性统计在拓扑数据分析中用于研究点云数据的结构特性，但在分布变化下的行为尚需深入研究。&lt;h4&gt;目的&lt;/h4&gt;建立持久性统计在一般分布下的理论性质，并开发一种能够检测高维点云结构变化的统计方法。&lt;h4&gt;方法&lt;/h4&gt;建立经典持久性统计量（总持久性和最大持久性）的矩界和紧性结果，引入基于持久性景观与Jensen-Shannon散度的标准化统计量，并证明其Hölder连续性。&lt;h4&gt;主要发现&lt;/h4&gt;持久性统计在高斯混合模型中具有明确的尺度行为，所提出的统计量具有稳定性、尺度和位移不变性，能够通过置换测试进行非参数推断。&lt;h4&gt;结论&lt;/h4&gt;该方法能够捕获制度转变和演化的几何复杂性，为随机持久性的理论理解和复杂高维系统中拓扑变化的检测提供了严格的统计基础。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了持久性统计在分布变化下的概率行为，并提出了一种用于检测高维随机点云结构变化的新非参数框架。我们首先在一般分布下建立了经典持久性统计量（总持久性和最大持久性）的矩界和紧性结果，并为高斯混合模型推导了明确的尺度行为。基于这些理论基础，我们引入了一种结合持久性景观和Jensen-Shannon散度的标准化统计量，并证明了它相对于输入点云扰动的Hölder连续性。所得的测度是稳定的、尺度和位移不变的，并通过置换测试适合非参数推断。使用去中心化治理数据的动态属性向量进行的数值说明展示了所提出的方法如何能够捕获制度转变和演化的几何复杂性。我们的结果为随机持久性的理论理解做出了贡献，并为复杂高维系统中拓扑变化的检测提供了严格的统计基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study the probabilistic behavior of persistence statistics underdistributional variability and propose a novel nonparametric framework fordetecting structural changes in high-dimensional random point clouds. We firstestablish moment bounds and tightness results for classical persistencestatistics - total and maximum persistence - under general distributions, withexplicit scaling behavior derived for Gaussian mixture models. Building onthese theoretical foundations, we introduce a normalized statistic based onpersistence landscapes combined with the Jensen-Shannon divergence, and weprove its Holder continuity with respect to perturbations of input pointclouds. The resulting measure is stable, scale- and shift-invariant, andsuitable for nonparametric inference via permutation testing. A numericalillustration using dynamic attribute vectors from decentralized governance datademonstrates how the proposed method can capture regime shifts and evolvinggeometric complexity. Our results contribute to the theoretical understandingof random persistence and provide a rigorous statistical foundation fortopological change-point detection in complex, high-dimensional systems.</description>
      <author>example@mail.com (Toshiyuki Nakayama)</author>
      <guid isPermaLink="false">2511.00938v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Neural Green's Functions</title>
      <link>http://arxiv.org/abs/2511.01924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为'Neural Green's Function'的神经网络解决方案算子，用于求解具有特征分解的线性偏微分方程。&lt;h4&gt;背景&lt;/h4&gt;Green函数是线性偏微分方程的解算子，它们仅依赖于域的几何形状。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够模仿Green函数行为的神经网络，实现在不同不规则几何形状和源函数及边界条件下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;Neural Green's Function从表示问题域的体积点云中提取逐点特征，并使用它们来预测解算子的分解，然后通过数值积分评估解。&lt;h4&gt;主要发现&lt;/h4&gt;该框架对训练中使用的特定函数不敏感，能够实现稳健高效的泛化。在MCB数据集上对机械零件几何形状的稳态热分析中，Neural Green's Function优于最先进的神经算子，在五个形状类别上平均误差减少了13.9%，比需要计算密集网格化的数值求解器快350倍。&lt;h4&gt;结论&lt;/h4&gt;Neural Green's Function是一种有效的神经网络解决方案算子，能够处理线性偏微分方程，并在不同几何形状和条件下实现良好的泛化性能。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了神经格林函数，一种用于线性偏微分方程的神经网络解算子，其微分算子允许特征分解。受格林函数的启发，线性偏微分方程的解算子仅依赖于域的几何形状，我们设计了神经格林函数来模仿它们的行为，实现在各种不规则几何形状、源函数和边界函数上的优越泛化能力。具体而言，神经格林函数从表示问题域的体积点云中提取逐点特征，并使用它们来预测解算子的分解，随后通过数值积分应用来评估解。与最近基于学习的解算子不同，这些解算子通常难以泛化到未见过的源函数或边界函数，我们的框架在设计上对训练中使用的特定函数不敏感，能够实现稳健和高效的泛化。在MCB数据集中机械零件几何形状的稳态热分析中，神经格林函数优于最先进的神经算子，在五个形状类别上平均误差减少了13.9%，而比需要计算密集网格化的数值求解器快350倍。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决线性偏微分方程(PDEs)求解的问题，特别是那些微分算子可以进行特征分解的线性PDEs（如泊松方程和双调和方程）。这个问题在现实和研究中非常重要，因为PDE在科学和工程领域有广泛应用，包括热分析、静电学、流体动力学和弹性力学等。传统数值求解方法（如有限元法）依赖于计算密集型的网格生成过程，这限制了在工程设计早期阶段的快速迭代评估。现有学习求解器虽然不需要网格，但往往难以推广到未见过的源函数和边界函数，当问题域、源函数和边界函数同时变化时表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到Green函数的启发，Green函数是线性PDE的解算子，仅依赖于问题域的几何形状而不依赖于特定的源函数或边界函数。作者基于线性PDE解的数学表达式，利用微分算子的特征分解性质，将解算子表示为特征向量和特征值的乘积。设计神经网络仅从域几何中提取特征，而不依赖于特定的源函数或边界函数。作者借鉴了Green函数理论、神经算子（如GNO、FNO）以及Transolver的网络架构，同时扩展了先前学习Green函数的工作（如Boullé和Teng等人的工作），使其能够处理更复杂的几何形状。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是设计一个仅从问题域几何中提取特征的神经网络，使用这些特征近似Green函数的特征分解，预测必要的微分量（如质量矩阵）以进行数值积分，从而实现解的计算。整体实现流程如下：1) 输入表示问题域几何的查询点；2) 使用神经网络（基于Transolver架构）从查询点坐标提取特征；3) 使用提取的特征构造神经Green函数，作为真实Green函数的近似；4) 预测每个顶点的质量值和算子的子矩阵；5) 通过基于Green函数的数值积分公式计算解；6) 通过最小化预测解与真实解之间的误差以及质量预测的正则化项来优化网络参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 仅从域几何提取特征，不将源函数和边界函数作为输入，使模型能够推广到未见过的函数；2) 利用Green函数的特征分解性质设计更有效的表示；3) 预测必要的微分量（如质量矩阵），使方法能够处理复杂几何形状；4) 专注于可以特征分解的线性PDE，如泊松方程和双调和方程。相比之前的工作，该方法比传统数值求解器快350倍（不需要网格生成），比PINNs不需要为每个问题实例重新训练，比神经算子（如GNO、FNO）能够更好地推广到未见过的源函数和边界函数，比先前学习Green函数的工作能够处理更复杂的几何形状且不需要为每个域重新训练。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种神经Green函数方法，通过仅从问题域几何中提取特征并近似Green函数的特征分解，实现了对线性偏微分方程的高效求解，能够推广到各种不规则几何形状和未见过的源函数、边界函数，且比传统数值求解器快350倍。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Neural Green's Function, a neural solution operator for linearpartial differential equations (PDEs) whose differential operators admiteigendecompositions. Inspired by Green's functions, the solution operators oflinear PDEs that depend exclusively on the domain geometry, we design NeuralGreen's Function to imitate their behavior, achieving superior generalizationacross diverse irregular geometries and source and boundary functions.Specifically, Neural Green's Function extracts per-point features from avolumetric point cloud representing the problem domain and uses them to predicta decomposition of the solution operator, which is subsequently applied toevaluate solutions via numerical integration. Unlike recent learning-basedsolution operators, which often struggle to generalize to unseen source orboundary functions, our framework is, by design, agnostic to the specificfunctions used during training, enabling robust and efficient generalization.In the steady-state thermal analysis of mechanical part geometries from the MCBdataset, Neural Green's Function outperforms state-of-the-art neural operators,achieving an average error reduction of 13.9\% across five shape categories,while being up to 350 times faster than a numerical solver that requirescomputationally expensive meshing.</description>
      <author>example@mail.com (Seungwoo Yoo, Kyeongmin Yeo, Jisung Hwang, Minhyuk Sung)</author>
      <guid isPermaLink="false">2511.01924v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking individual tree segmentation using multispectral airborne laser scanning data: the FGI-EMIT dataset</title>
      <link>http://arxiv.org/abs/2511.00653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了FGI-EMIT，首个用于单木分割的大规模多谱段机载激光扫描基准数据集，并比较了传统无监督算法与深度学习方法在树木分割任务上的性能表现。&lt;h4&gt;背景&lt;/h4&gt;单木分割(LiDAR点云)是森林资源清查、碳监测和生物多样性评估的基础应用。传统方法采用无监督几何算法，近期转向监督深度学习。过去因缺乏大规模基准数据集限制了方法发展，尽管多光谱反射率能提高分割准确性，但多光谱LiDAR数据格式仍然有限。&lt;h4&gt;目的&lt;/h4&gt;创建首个用于单木分割的大规模多谱段机载激光扫描基准数据集，并全面评估不同算法的性能。&lt;h4&gt;方法&lt;/h4&gt;FGI-EMIT数据集在532、905和1,550 nm波长处捕获，包含1,561个手动标注的树木，特别关注小林下树木。研究评估了四种传统无监督算法和四种监督深度学习方法，其中无监督方法使用贝叶斯优化超参数，深度学习模型从头训练。&lt;h4&gt;主要发现&lt;/h4&gt;无监督方法中Treeiso表现最佳，F1分数为52.7%；深度学习方法整体显著更好，最佳模型ForestFormer3D达到73.3%的F1分数。林下树木性能差异最显著，ForestFormer3D比Treeiso高出25.9个百分点。当前深度学习方法未能有效利用多谱段反射率信息，但单通道反射率可略微提高准确性，特别是对林下树木。即使点密度低至10点/m²，深度学习方法仍优于无监督算法。&lt;h4&gt;结论&lt;/h4&gt;深度学习方法在树木分割任务上显著优于传统无监督方法，多光谱数据有提高分割准确性的潜力，但当前深度学习模型未能充分利用这些信息。即使在低点密度条件下，深度学习方法也保持优势。&lt;h4&gt;翻译&lt;/h4&gt;从激光雷达点云中进行单木分割(ITS)对于森林资源清查、碳监测和生物多样性评估等应用至关重要。传统上，ITS通过无监督的几何算法实现，而最近的进展已转向监督深度学习(DL)。过去，由于缺乏大规模基准数据集，方法开发进展受限，尽管有证据表明多光谱(MS)反射率可以提高ITS的准确性，但新型数据格式(特别是多光谱激光雷达)的可用性至今仍然有限。本研究引入了FGI-EMIT，这是首个用于ITS的大规模多谱段机载激光扫描基准数据集。该数据集在532、905和1,550 nm波长处捕获，包含1,561个手动标注的树木，特别关注小林下树木。利用FGI-EMIT，我们全面评估了四种传统无监督算法和四种监督深度学习方法。无监督方法的超参数使用贝叶斯方法优化，而深度学习模型从头开始训练。在无监督方法中，Treeiso实现了最高的测试集F1分数，为52.7%。深度学习方法整体表现显著更好，最佳模型ForestFormer3D达到了73.3%的F1分数。林下树木观察到最显著的差异，ForestFormer3D比Treeiso高出25.9个百分点。消融研究表明，当多谱段反射率作为额外输入特征提供时，当前的基于深度学习的方法通常无法有效利用这些信息，尽管单通道反射率可以略微提高准确性，特别是对于林下树木。在不同点密度下的性能分析进一步表明，即使点密度低至10点/m²，深度学习方法仍然始终优于无监督算法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决个体树木分割(ITS)方法的评估和比较问题，特别是缺乏大规模多光谱激光扫描基准数据集的挑战。这个问题在现实中很重要，因为准确的树木分割是林业调查、碳监测和生物多样性评估的基础应用，而缺乏标准化的评估框架使得研究人员和从业者难以选择最适合特定应用的方法。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有数据集的局限性，如缺乏多光谱数据和高质量3D标注，然后设计了FGI-EMIT数据集。他们借鉴了FOR-Instance等数据集的经验，但增加了多光谱数据和更详细的标注；在评估方法上借鉴了计算机视觉领域的3D IoU指标；在超参数优化上使用了贝叶斯优化；在数据分割上采用了分层随机采样确保代表性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、高质量的多光谱激光扫描数据集，用于系统性地评估传统无监督算法和深度学习方法。整体流程包括：1)使用多波长激光扫描仪采集数据；2)预处理和合并数据；3)手动标注树木实例和语义信息；4)计算树木位置和高度；5)将树木分为四种类别；6)分割数据集为训练集和测试集；7)使用3D IoU指标评估多种方法的性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)创建了第一个大规模多光谱激光扫描个体树木分割基准数据集FGI-EMIT；2)进行了全面的性能比较，评估了四种传统方法和四种深度学习方法；3)首次研究了多光谱信息在深度学习方法中的利用。相比之前的工作，FGI-EMIT是第一个包含城市环境人工结构的多光谱数据集，并且提供了系统性的超参数优化和多种点云密度下的性能评估。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建首个大规模多光谱激光扫描个体树木分割基准数据集并进行全面的性能比较，证明了深度学习方法显著优于传统无监督算法，同时揭示了当前深度学习方法未能充分利用多光谱信息的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Individual tree segmentation (ITS) from LiDAR point clouds is fundamental forapplications such as forest inventory, carbon monitoring and biodiversityassessment. Traditionally, ITS has been achieved with unsupervisedgeometry-based algorithms, while more recent advances have shifted towardsupervised deep learning (DL). In the past, progress in method development washindered by the lack of large-scale benchmark datasets, and the availability ofnovel data formats, particularly multispectral (MS) LiDAR, remains limited tothis day, despite evidence that MS reflectance can improve the accuracy of ITS.This study introduces FGI-EMIT, the first large-scale MS airborne laserscanning benchmark dataset for ITS. Captured at wavelengths 532, 905, and 1,550nm, the dataset consists of 1,561 manually annotated trees, with a particularfocus on small understory trees. Using FGI-EMIT, we comprehensively benchmarkedfour conventional unsupervised algorithms and four supervised DL approaches.Hyperparameters of unsupervised methods were optimized using a Bayesianapproach, while DL models were trained from scratch. Among the unsupervisedmethods, Treeiso achieved the highest test set F1-score of 52.7%. The DLapproaches performed significantly better overall, with the best model,ForestFormer3D, attaining an F1-score of 73.3%. The most significant differencewas observed in understory trees, where ForestFormer3D exceeded Treeiso by 25.9percentage points. An ablation study demonstrated that current DL-basedapproaches generally fail to leverage MS reflectance information when it isprovided as additional input features, although single channel reflectance canimprove accuracy marginally, especially for understory trees. A performanceanalysis across point densities further showed that DL methods consistentlyremain superior to unsupervised algorithms, even at densities as low as 10points/m$^2$.</description>
      <author>example@mail.com (Lassi Ruoppa, Tarmo Hietala, Verneri Seppänen, Josef Taher, Teemu Hakala, Xiaowei Yu, Antero Kukko, Harri Kaartinen, Juha Hyyppä)</author>
      <guid isPermaLink="false">2511.00653v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Been There, Scanned That: Nostalgia-Driven LiDAR Compression for Self-Driving Cars</title>
      <link>http://arxiv.org/abs/2511.00652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DejaView的3D点云数据压缩方法，通过利用自动驾驶车辆在大时间尺度上的数据冗余，实现了高效的数据压缩。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶车辆每天可产生数TB传感器数据，其中大量是由LiDAR等深度传感器生成的3D点云数据。这些数据需传输到云端用于机器学习模型训练或事故分析，但网络和存储成本高昂。&lt;h4&gt;目的&lt;/h4&gt;减少自动驾驶车辆3D点云数据的网络传输和存储成本，提高数据压缩效率。&lt;h4&gt;方法&lt;/h4&gt;DejaView利用自动驾驶车辆活动区域有限且主要行驶固定路线的特点，在更大的时间尺度（天和月）上寻找数据冗余，而非传统的帧间冗余。其核心是一个diff操作，将点云紧凑地表示为相对于过去3D数据的增量。&lt;h4&gt;主要发现&lt;/h4&gt;使用两个月LiDAR数据的测试表明，DejaView的端到端实现可将点云压缩210倍，同时保持仅15厘米的重构误差。&lt;h4&gt;结论&lt;/h4&gt;DejaView是一种有效的3D点云数据压缩方法，特别适用于自动驾驶车辆场景，能够显著降低数据存储和传输成本。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶车辆每天可以产生数TB的传感器数据。其中很大一部分是由深度传感器（如LiDAR）产生的3D点云数据。这些数据必须传输到云存储，用于训练机器学习模型或进行分析，例如在发生事故时进行取证调查。为了减少网络和存储成本，本文介绍了DejaView。尽管先前的工作使用帧间冗余来压缩数据，但DejaView在更大的时间尺度（天和月）上搜索并利用冗余，以实现更有效的压缩。我们基于自动驾驶车辆的活动区域有限且主要每天行驶相同路线的洞察设计了DejaView。因此，车辆每天收集的3D数据可能与过去捕获的数据相似。为此，DejaView的核心是一个diff操作，将点云紧凑地表示为相对于过去3D数据的增量。使用两个月的LiDAR数据，DejaView的端到端实现可以在仅15厘米的重构误差下将点云压缩210倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; An autonomous vehicle can generate several terabytes of sensor data per day.A significant portion of this data consists of 3D point clouds produced bydepth sensors such as LiDARs. This data must be transferred to cloud storage,where it is utilized for training machine learning models or conductinganalyses, such as forensic investigations in the event of an accident. Toreduce network and storage costs, this paper introduces DejaView. Althoughprior work uses interframe redundancies to compress data, DejaView searches forand uses redundancies on larger temporal scales (days and months) for moreeffective compression. We designed DejaView with the insight that the operatingarea of autonomous vehicles is limited and that vehicles mostly traverse thesame routes daily. Consequently, the 3D data they collect daily is likelysimilar to the data they have captured in the past. To capture this, the coreof DejaView is a diff operation that compactly represents point clouds as deltaw.r.t. 3D data from the past. Using two months of LiDAR data, an end-to-endimplementation of DejaView can compress point clouds by a factor of 210 at areconstruction error of only 15 cm.</description>
      <author>example@mail.com (Ali Khalid, Jaiaid Mobin, Sumanth Rao Appala, Avinash Maurya, Stephany Berrio Perez, M. Mustafa Rafique, Fawad Ahmad)</author>
      <guid isPermaLink="false">2511.00652v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Three-dimensional narrow volume reconstruction method with unconditional stability based on a phase-field Lagrange multiplier approach</title>
      <link>http://arxiv.org/abs/2511.00508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint, 30+ pages; multiple figures and tables; code and data:  https://github.com/cfdyang521/C-3PO/tree/main; intended for submission to a  computational mathematics journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于Allen-Cahn模型的有效点云重建算法，采用拉格朗日乘子法，通过增强的边缘检测函数重建窄壳结构。&lt;h4&gt;背景&lt;/h4&gt;从点云重建物体在假肢、医学成像、计算机视觉等领域非常重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的基于Allen-Cahn模型的重建算法。&lt;h4&gt;方法&lt;/h4&gt;采用拉格朗日乘子法，利用物体的散乱数据点，通过求解增强有边缘检测函数的控制方程来重建窄壳；边缘检测函数基于无符号距离函数设计；使用Crank-Nicolson时间离散化和有限差分法近似空间运算。&lt;h4&gt;主要发现&lt;/h4&gt;算法可以稳定和解耦地更新解；全离散方案被证明是无条件稳定的；复杂3D体积重建实验验证了算法的准确性、稳定性和有效性。&lt;h4&gt;结论&lt;/h4&gt;分析了特定参数选择如何影响重建体积的细节和精细度；分享了计算代码和数据以便其他研究者理解和使用该算法。&lt;h4&gt;翻译&lt;/h4&gt;从点云重建物体在假肢、医学成像、计算机视觉等领域至关重要。我们提出了一种基于Allen-Cahn模型的有效重建算法，采用拉格朗日乘子法。利用物体的散乱数据点，我们通过求解增强有从无符号距离函数导出的边缘检测函数的控制方程来重建窄壳。特别设计的边缘检测函数确保了能量稳定性。通过拉格朗日乘子技术重新表述控制方程并实施Crank-Nicolson时间离散化，我们可以以稳定和解耦的方式更新解。空间运算使用有限差分法近似，我们通过分析证明了全离散方案的无条件稳定性。包括重建《星球大战》中的字符等复杂3D体积在内的全面数值实验，验证了算法的准确性、稳定性和有效性。此外，我们分析了特定参数选择如何影响重建体积的细节和精细度。为了便于感兴趣的读者理解我们的算法，我们在https://github.com/cfdyang521/C-3PO/tree/main分享了计算代码和数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstruction of an object from points cloud is essential in prosthetics,medical imaging, computer vision, etc. We present an effective algorithm for anAllen--Cahn-type model of reconstruction, employing the Lagrange multiplierapproach. Utilizing scattered data points from an object, we reconstruct anarrow shell by solving the governing equation enhanced with an edge detectionfunction derived from the unsigned distance function. The specifically designededge detection function ensures the energy stability. By reformulating thegoverning equation through the Lagrange multiplier technique and implementing aCrank--Nicolson time discretization, we can update the solutions in a stableand decoupled manner. The spatial operations are approximated using the finitedifference method, and we analytically demonstrate the unconditional stabilityof the fully discrete scheme. Comprehensive numerical experiments, includingreconstructions of complex 3D volumes such as characters from \textit{StarWars}, validate the algorithm's accuracy, stability, and effectiveness.Additionally, we analyze how specific parameter selections influence the levelof detail and refinement in the reconstructed volumes. To facilitate theinterested readers to understand our algorithm, we share the computationalcodes and data in https://github.com/cfdyang521/C-3PO/tree/main.</description>
      <author>example@mail.com (Renjun Gao, Xiangjie Kong, Dongting Cai, Boyi Fu, Junxiang Yang)</author>
      <guid isPermaLink="false">2511.00508v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Dataset for Indoor Radio Mapping with 3D Point Clouds and RSSI</title>
      <link>http://arxiv.org/abs/2511.00494v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures, 3 tables, under review to Nature Scientific Data&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一个多模态数据集，结合高分辨率3D激光雷达扫描与Wi-Fi接收信号强度测量，用于研究室内无线信号传播特性，特别是在不同AP配置和人员存在情况下的动态环境影响。&lt;h4&gt;背景&lt;/h4&gt;随着支持带宽密集型和延迟敏感型应用的智能设备增多，室内环境需要可靠的无线连接。准确的无线电环境图(REMs)估计对自适应网络规划和接入点优化至关重要。&lt;h4&gt;目的&lt;/h4&gt;克服室内空间复杂性导致的真实REMs生成挑战，为数据驱动的无线建模研究提供资源，特别是在IEEE 802.11be(Wi-Fi 7)等新兴高频标准的背景下，促进高容量室内通信系统发展。&lt;h4&gt;方法&lt;/h4&gt;创建并展示了一个多模态数据集，整合了高分辨率3D激光雷达扫描与Wi-Fi RSSI测量数据，在多房间室内环境中，20种不同AP配置下收集，包含无人和有人两种场景的测量数据。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集支持研究动态环境（如人员存在）对无线信号传播的影响，为室内无线通信系统优化提供了基础数据支持。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据集作为研究资源，有助于推动数据驱动的室内无线建模，特别是在高频标准下的通信系统优化，为开发稳健、高容量的室内通信系统奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;随着支持带宽密集型和延迟敏感型应用（如实时视频分析、智能感知和扩展现实XR）的智能设备数量不断增加，室内环境需要可靠的无线连接。在此，准确的无线电环境图(REMs)估计能够支持自适应无线网络规划和接入点(AP)部署优化。然而，由于室内空间的复杂性，生成真实的REMs仍然具有挑战性。为克服这一挑战，本文引入了一个多模态数据集，该数据集集成了高分辨率3D激光雷达扫描与在多房间室内环境中20种不同AP配置下收集的Wi-Fi接收信号强度指示器(RSSI)测量。该数据集捕获了两种测量场景：第一种是环境中无人存在的情况，第二种是有人存在的情况。因此，所提供的数据集支持研究动态环境对无线信号传播的影响。该资源旨在促进数据驱动的无线建模研究，特别是在IEEE 802.11be(Wi-Fi 7)等新兴高频标准的背景下，旨在推动稳健、高容量室内通信系统的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The growing number of smart devices supporting bandwidth-intensive andlatency-sensitive applications, such as real-time video analytics, smartsensing, and Extended Reality (XR), necessitates reliable wireless connectivityin indoor environments. Therein, accurate estimation of Radio Environment Maps(REMs) enables adaptive wireless network planning and optimization of AccessPoint (AP) placement. However, generating realistic REMs remains challengingdue to the complexity of indoor spaces. To overcome this challenge, this paperintroduces a multimodal dataset that integrates high-resolution 3D LiDAR scanswith Wi-Fi Received Signal Strength Indicator (RSSI) measurements collectedunder 20 distinct AP configurations in a multi-room indoor environment. Thedataset captures two measurement scenarios: the first without human presence inthe environment, and the second with human presence. Thus, the presenteddataset supports the study of dynamic environmental effects on wireless signalpropagation. This resource is designed to facilitate research in data-drivenwireless modeling, particularly in the context of emerging high-frequencystandards such as IEEE 802.11be (Wi-Fi 7), and aims to advance the developmentof robust, high-capacity indoor communication systems.</description>
      <author>example@mail.com (Ljupcho Milosheski, Kuon Akiyama, Blaž Bertalanič, Jernej Hribar, Ryoichi Shinkuma)</author>
      <guid isPermaLink="false">2511.00494v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>VLM6D: VLM based 6Dof Pose Estimation based on RGB-D Images</title>
      <link>http://arxiv.org/abs/2511.00120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted to IEIE( The Institute Of Electronics  and Information Engineering, South Korea) Fall,2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VLM6D是一种新颖的双流架构，利用RGB-D输入中的视觉和几何数据实现鲁棒且精确的6D物体姿态估计，在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉中精确计算6D物体姿态的主要挑战在于当前方法在从合成数据到真实环境的泛化方面存在困难，特别是在光照变化、无纹理物体和严重遮挡的情况下表现脆弱。&lt;h4&gt;目的&lt;/h4&gt;提出VLM6D，一种新颖的双流架构，利用RGB-D输入中的视觉和几何数据的优势，实现鲁棒且精确的姿态估计。&lt;h4&gt;方法&lt;/h4&gt;VLM6D框架集成了两个专门的编码器：一个强大的自监督视觉Transformer（DINOv2）处理RGB模态，利用其丰富的预训练视觉理解能力；一个PointNet++编码器处理深度数据衍生的3D点云，实现鲁棒的几何推理；这两个互补的特征流被有效融合，用于多任务预测头。&lt;h4&gt;主要发现&lt;/h4&gt;通过全面实验，VLM6D在具有挑战性的Occluded-LineMOD数据集上取得了新的SOTA性能，验证了其优越的鲁棒性和准确性。&lt;h4&gt;结论&lt;/h4&gt;VLM6D通过结合视觉和几何信息，成功解决了计算机视觉中6D物体姿态估计的挑战，特别是在处理复杂环境（如光照变化、无纹理物体和严重遮挡）时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉中的主要挑战是精确计算6D物体的姿态，然而许多当前方法仍然脆弱且难以从合成数据泛化到具有变化光照、无纹理物体和严重遮挡的真实世界情况。为解决这些限制，VLM6D是一种新颖的双流架构，它利用RGB-D输入中视觉和几何数据的独特优势进行鲁棒且精确的姿态估计。我们的框架独特地集成了两个专门的编码器：一个强大的自监督视觉Transformer（DINOv2）处理RGB模态，利用其丰富、预训练的视觉语法理解能力，实现对纹理和光照变化的显著抵抗力。同时，一个PointNet++编码器处理从深度数据衍生的3D点云，实现鲁棒的几何推理，即使在严重遮挡情况下典型的稀疏、碎片化数据中也能表现出色。这些互补的特征流被有效融合，为多任务预测头提供信息。我们通过全面实验证明，VLM6D在具有挑战性的Occluded-LineMOD上获得了新的SOTA性能，验证了其优越的鲁棒性和准确性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决计算机视觉中6D物体姿态估计的挑战，特别是处理真实世界中常见的问题如光照变化、无纹理物体和严重遮挡。这个问题非常重要，因为准确的6D姿态估计是机器人抓取、自动驾驶、增强现实和自动化装配等应用的基础能力，使机器能够感知、理解和与周围环境互动。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性（在真实世界复杂情况下表现脆弱）设计了VLM6D。他们借鉴了双流架构思想，分别处理RGB和深度数据，但进行了创新：使用DINOv2处理RGB数据（利用其强大的视觉理解和泛化能力），使用PointNet++处理深度数据（处理几何信息）。作者还借鉴了密集对应点的思想，但通过双流架构和特征融合策略改进了计算效率和准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用双流架构，分别从RGB图像和深度数据中提取互补的特征信息（RGB提供视觉信息，深度提供几何信息），然后融合这些特征进行6D姿态估计。实现流程包括：1)输入处理（RGB图像调整归一化，深度图像转为点云）；2)双流特征提取（DINOv2处理RGB，PointNet++处理点云）；3)特征融合（连接特征向量并通过MLP处理）；4)多任务预测（旋转、平移、置信度和物体分类）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)双流架构设计（结合DINOv2和PointNet++）；2)特征融合策略（晚期融合）；3)多任务预测头。相比之前工作（如RDPN6D），VLM6D具有更强的泛化能力（使用自监督学习）、更高效的计算（不需要预测密集坐标图）、更好的鲁棒性（在反射表面、无纹理物体和极端遮挡条件下表现更好），并且能够处理高达80%的物体遮挡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VLM6D通过创新性地结合自监督视觉Transformer和点云处理网络，实现了在复杂场景下（严重遮挡、光照变化、无纹理物体）更加鲁棒和准确的6D物体姿态估计。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The primary challenge in computer vision is precisely calculating the pose of6D objects, however many current approaches are still fragile and have troublegeneralizing from synthetic data to real-world situations with fluctuatinglighting, textureless objects, and significant occlusions. To address theselimitations, VLM6D, a novel dual-stream architecture that leverages thedistinct strengths of visual and geometric data from RGB-D input for robust andprecise pose estimation. Our framework uniquely integrates two specializedencoders: a powerful, self-supervised Vision Transformer (DINOv2) processes theRGB modality, harnessing its rich, pre-trained understanding of visual grammarto achieve remarkable resilience against texture and lighting variations.Concurrently, a PointNet++ encoder processes the 3D point cloud derived fromdepth data, enabling robust geometric reasoning that excels even with thesparse, fragmented data typical of severe occlusion. These complementaryfeature streams are effectively fused to inform a multi task prediction head.We demonstrate through comprehensive experiments that VLM6D obtained new SOTAperformance on the challenging Occluded-LineMOD, validating its superiorrobustness and accuracy.</description>
      <author>example@mail.com (Md Selim Sarowar, Sungho Kim)</author>
      <guid isPermaLink="false">2511.00120v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>D$^2$GS: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2510.25173v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出D²GS，一个无LiDAR的城市场景重建框架，能够产生比LiDAR更密集、更准确的几何先验，实验表明其性能优于现有方法，甚至超过使用真实LiDAR数据的方法。&lt;h4&gt;背景&lt;/h4&gt;高斯散射在自动驾驶城市场景重建中潜力巨大，但当前方法依赖LiDAR和图像等多模态传感器。LiDAR提供的几何先验虽可减轻重建不适定性，但实践中获取准确LiDAR数据面临挑战：需要精确时空校准且不同传感器位置会导致重投影误差。&lt;h4&gt;目的&lt;/h4&gt;避免获取准确LiDAR深度的困难，开发一种无LiDAR的城市场景重建框架，获得与LiDAR一样有效但更密集、更准确的几何先验。&lt;h4&gt;方法&lt;/h4&gt;D²GS框架包含三个主要部分：首先，通过反向投影多视图度量深度预测初始化密集点云，并用渐进修剪策略优化以提高全局一致性；其次，通过深度增强器联合优化高斯几何和预测深度，利用深度基础模型的扩散先验增强高斯渲染的深度图；最后，约束道路区域内高斯的形状和法线属性以提高地面几何准确性。&lt;h4&gt;主要发现&lt;/h4&gt;Waymo数据集上的大量实验表明，该方法持续优于最先进方法，产生更准确的几何，即使与使用真实LiDAR数据的方法相比也是如此。&lt;h4&gt;结论&lt;/h4&gt;D²GS成功实现了无LiDAR的城市场景重建，能够产生比传统LiDAR方法更密集、更准确的几何先验，性能超越现有最先进方法。&lt;h4&gt;翻译&lt;/h4&gt;最近，高斯散射在自动驾驶领域的城市场景重建中显示出巨大潜力。然而，当前的城市场景重建方法通常依赖于多模态传感器作为输入，即LiDAR和图像。尽管LiDAR点云提供的几何先验可以大大减轻重建中的不适定性，但在实践中获取准确的LiDAR数据仍然具有挑战性：i)需要LiDAR与其他传感器之间的精确时空校准，因为它们可能无法同时捕获数据；ii)当LiDAR和相机安装在不同位置时，空间错位会导致重投影误差。为了避免获取准确LiDAR深度的困难，我们提出了D²GS，一个无LiDAR的城市场景重建框架。在这项工作中，我们获得了与LiDAR一样有效但更密集、更准确的几何先验。首先，我们通过反向投影多视图度量深度预测来初始化密集点云。然后通过渐进修剪策略优化该点云以提高全局一致性。其次，我们通过深度增强器联合优化高斯几何和预测的密集度量深度。具体来说，我们利用来自深度基础模型的扩散先验来增强由高斯渲染的深度图。反过来，增强的深度在高斯训练期间提供更强的几何约束。最后，我们通过约束道路区域内高斯的形状和法线属性来提高地面几何的准确性。在Waymo数据集上的大量实验表明，我们的方法持续优于最先进的方法，产生更准确的几何，即使与使用真实LiDAR数据的方法相比也是如此。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶领域城市场景重建中对LiDAR传感器的依赖问题。这个问题很重要，因为获取LiDAR数据需要昂贵设备、专业车辆，且传感器间精确校准困难，同时LiDAR和相机安装在不同位置会导致重投影误差，这些都限制了实际应用的可扩展性和成本效益。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了LiDAR依赖带来的校准困难和数据获取成本问题，然后意识到需要替代LiDAR的密集几何先验。方法设计上，他们使用多视图深度预测初始化点云，通过渐进式修剪策略优化，借鉴了3DGS的高效特性、扩散模型的深度生成先验以及场景图表示方法来组织不同类型的Gaussian，并结合道路区域的强几何先验知识。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过仅使用相机输入创建不依赖LiDAR的城市街道场景重建框架，利用图像衍生的几何先验替代LiDAR点云。实现流程包括：1)使用多视图深度估计和渐进式修剪初始化紧凑Gaussian表示；2)使用基于扩散的深度增强器进行深度和Gaussian的联合优化；3)在场景图中引入专门的道路节点利用强几何先验显式建模地面平面；4)迭代优化Gaussian参数和深度表示。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)完全LiDAR-free的重建框架；2)渐进式修剪策略从密集点云获得紧凑表示；3)基于扩散的深度增强器实现深度和Gaussian的联合优化；4)专门的道路节点利用强几何先验。相比之前工作，D2GS消除了对LiDAR的依赖和校准误差，避免了单目深度估计的尺度模糊和多视图深度估计不适合动态场景的问题，并通过迭代优化提供了更密集、准确的深度监督。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; D2GS提出了一种不依赖LiDAR的城市街道场景重建框架，通过渐进式修剪、深度增强和道路节点优化，实现了比使用LiDAR数据更准确的几何重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Gaussian Splatting (GS) has shown great potential for urban scenereconstruction in the field of autonomous driving. However, current urban scenereconstruction methods often depend on multimodal sensors as inputs,\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDARpoint clouds can largely mitigate ill-posedness in reconstruction, acquiringsuch accurate LiDAR data is still challenging in practice: i) precisespatiotemporal calibration between LiDAR and other sensors is required, as theymay not capture data simultaneously; ii) reprojection errors arise from spatialmisalignment when LiDAR and cameras are mounted at different locations. Toavoid the difficulty of acquiring accurate LiDAR depth, we propose D$^2$GS, aLiDAR-free urban scene reconstruction framework. In this work, we obtaingeometry priors that are as effective as LiDAR while being denser and moreaccurate. $\textbf{First}$, we initialize a dense point cloud byback-projecting multi-view metric depth predictions. This point cloud is thenoptimized by a Progressive Pruning strategy to improve the global consistency.$\textbf{Second}$, we jointly refine Gaussian geometry and predicted densemetric depth via a Depth Enhancer. Specifically, we leverage diffusion priorsfrom a depth foundation model to enhance the depth maps rendered by Gaussians.In turn, the enhanced depths provide stronger geometric constraints duringGaussian training. $\textbf{Finally}$, we improve the accuracy of groundgeometry by constraining the shape and normal attributes of Gaussians withinroad regions. Extensive experiments on the Waymo dataset demonstrate that ourmethod consistently outperforms state-of-the-art methods, producing moreaccurate geometry even when compared with those using ground-truth LiDAR data.</description>
      <author>example@mail.com (Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang)</author>
      <guid isPermaLink="false">2510.25173v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Which LiDAR scanning pattern is better for roadside perception: Repetitive or Non-repetitive?</title>
      <link>http://arxiv.org/abs/2511.00060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了不同LiDAR扫描模式对路边感知性能的影响，创建了'InfraLiDARs' Benchmark'数据集，比较了重复式和非重复式扫描LiDAR的性能，发现两者检测性能相当，非重复式LiDAR虽然感知范围有限但成本效益高&lt;h4&gt;背景&lt;/h4&gt;基于LiDAR的路边感知是智能交通系统的基石，现有研究多关注LiDAR的最佳放置位置，而不同扫描模式对感知性能的影响研究不足&lt;h4&gt;目的&lt;/h4&gt;系统研究基础设施背景下不同LiDAR扫描模式的差异，评估这些模式对3D目标检测算法性能的影响&lt;h4&gt;方法&lt;/h4&gt;在CARLA仿真环境中创建'InfraLiDARs' Benchmark'数据集，使用同时运行的重复式和非重复式扫描LiDAR进行数据收集，进行统计分析并评估多种3D目标检测算法的性能&lt;h4&gt;主要发现&lt;/h4&gt;非重复扫描LiDAR和128线重复扫描LiDAR在各种场景中检测性能相当；尽管非重复LiDAR感知范围有限，但因其价格低廉而具有成本效益；不同扫描模式产生不同点云分布，影响目标检测和环境理解效果&lt;h4&gt;结论&lt;/h4&gt;为设置具有最佳LiDAR扫描模式和兼容算法的路边感知系统提供见解，适应不同路边应用需求，并公开数据集促进进一步研究&lt;h4&gt;翻译&lt;/h4&gt;基于LiDAR的路边感知是先进智能交通系统(ITS)的基石。虽然已有大量研究解决了基础设施LiDAR的最佳放置问题，但不同LiDAR扫描模式对感知性能的深远影响尚未得到充分研究。各种扫描模式的固有特性——如传统重复式（机械/固态）与新兴的非重复式（如基于棱镜的系统）——导致在不同距离下产生不同的点云分布，这直接影响目标检测和整体环境理解的效果。为了系统性地研究基础设施背景下的这些差异，我们引入了'InfraLiDARs' Benchmark'，这是一个在CARLA仿真环境中精心收集的新数据集，使用了同时运行的基础设施LiDAR，展示了两种扫描模式。利用这个基准，我们对各种LiDAR扫描能力进行了全面的统计分析，并评估了这些不同模式对各种领先3D目标检测算法性能的影响。我们的研究揭示，非重复扫描LiDAR和128线重复扫描LiDAR在各种场景中表现出相当的检测性能。尽管非重复LiDAR的感知范围有限，但考虑到其低廉的价格，它是一种经济有效的选择。最终，这项研究为设置具有最佳LiDAR扫描模式和兼容算法的路边感知系统提供了见解，以适应不同的路边应用需求，并公开发布了'InfraLiDARs' Benchmark'数据集，以促进进一步的研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要想解决的问题是：对于路侧感知系统，重复扫描模式（repetitive）和非重复扫描模式（non-repetitive）的激光雷达（LiDAR）哪种性能更好？这个问题在现实中非常重要，因为LiDAR路侧感知是智能交通系统的基础，直接影响交通安全性、流量管理和自动驾驶能力；在研究中也很重要，因为虽然已有大量研究关注LiDAR部署位置，但不同扫描模式对感知性能的影响研究相对不足，而不同的扫描模式会导致不同距离下的点云分布差异，直接影响物体检测和环境理解的效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了研究空白：虽然已有车载LiDAR研究，但路侧部署的不同扫描模式比较不足。然后设计了综合评估框架，包括统计基准和性能基准。他们在CARLA仿真环境中创建了专用数据集'InfraLiDARs' Benchmark'，收集了不同LiDAR扫描模式的数据。作者借鉴了现有的3D物体检测算法（如PointRCNN、PointPillars、PV-RCNN和DSVT），并参考了已有的LiDAR分类框架，但基于扫描模式而非传统分类方法。他们还参考了路侧感知研究，但专注于扫描模式而非传感器放置策略。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统比较重复扫描和非重复扫描两种LiDAR模式在路侧感知中的性能，为实际部署提供数据驱动的建议，并考虑成本效益因素。整体实现流程包括：1）在CARLA仿真环境中创建三种场景（高速公路、十字路口、弯道），使用四种LiDAR在相同位置和方向部署；2）进行统计基准测试，分析点云质量和不同距离下的检测能力；3）使用多种3D物体检测算法进行性能基准测试，采用整体AP分析、距离分段AP分析和高质量检测区域分析；4）综合比较结果，提出针对不同应用场景的最优配置建议。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）创建了'InfraLiDARs' Benchmark'数据集，专为路侧感知设计；2）首次系统比较了路侧部署中重复与非重复扫描LiDAR的性能；3）提出了全面的评估框架，结合统计基准和性能基准；4）发现了非重复扫描LiDAR在远距离检测中的优势。相比之前的工作，本文的研究焦点不同（关注扫描模式而非车载LiDAR或传感器放置），使用的数据集不同（专为路侧感知设计），评估维度更全面（考虑距离分布和高质量检测区域），并引入了成本效益分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过创建专用数据集和综合评估框架，系统比较了重复扫描与非重复扫描LiDAR在路侧感知中的性能，发现非重复扫描LiDAR与128线重复扫描LiDAR具有相当的检测性能，且在远距离检测中表现更佳，为路侧感知系统的优化部署提供了数据驱动的建议。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR-based roadside perception is a cornerstone of advanced IntelligentTransportation Systems (ITS). While considerable research has addressed optimalLiDAR placement for infrastructure, the profound impact of differing LiDARscanning patterns on perceptual performance remains comparativelyunder-investigated. The inherent nature of various scanning modes - such astraditional repetitive (mechanical/solid-state) versus emerging non-repetitive(e.g. prism-based) systems - leads to distinct point cloud distributions atvarying distances, critically dictating the efficacy of object detection andoverall environmental understanding. To systematically investigate thesedifferences in infrastructure-based contexts, we introduce the "InfraLiDARs'Benchmark," a novel dataset meticulously collected in the CARLA simulationenvironment using concurrently operating infrastructure-based LiDARs exhibitingboth scanning paradigms. Leveraging this benchmark, we conduct a comprehensivestatistical analysis of the respective LiDAR scanning abilities and evaluatethe impact of these distinct patterns on the performance of various leading 3Dobject detection algorithms. Our findings reveal that non-repetitive scanningLiDAR and the 128-line repetitive LiDAR were found to exhibit comparabledetection performance across various scenarios. Despite non-repetitive LiDAR'slimited perception range, it's a cost-effective option considering its lowprice. Ultimately, this study provides insights for setting up roadsideperception system with optimal LiDAR scanning patterns and compatiblealgorithms for diverse roadside applications, and publicly releases the"InfraLiDARs' Benchmark" dataset to foster further research.</description>
      <author>example@mail.com (Zhiqi Qi, Runxin Zhao, Hanyang Zhuang, Chunxiang Wang, Ming Yang)</author>
      <guid isPermaLink="false">2511.00060v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为UniField的通用流场表示模型，通过整合多个子领域的空气动力学数据进行联合训练，解决了神经网络在空气动力学模拟中面临的数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算流体动力学(CFD)模拟的高效替代方案，但数据稀缺性仍然是一个基本挑战，限制了神经网络的应用。&lt;h4&gt;目的&lt;/h4&gt;为了解决数据稀缺的限制，作者提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的场域表示。&lt;h4&gt;方法&lt;/h4&gt;作者整合了五个不同的数据集，涵盖汽车、火车、飞机和一般形状等多个领域。面对不同领域之间的显著数据差异，他们提出了UniField，它采用领域无关的Transformer模块提取通用的点云特征，并定制领域特定的流条件适配器来适应不同子领域的流信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据通常受不同方程支配，但作者发现，在所有数据上联合训练的模型通常比在单独数据集上分别训练的模型表现更好。这表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了UniField作为通用流场表示模型的潜力，并为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算上昂贵的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据并进行联合训练，以学习更通用的场域表示。我们整合了五个不同的数据集，涵盖各种领域，包括汽车、火车、飞机和一般形状。面对不同领域之间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用的点云特征，并定制领域特定的流条件适配器来适应不同子领域的流信息。尽管不同子领域的空气动力学数据通常受不同方程支配，但我们比较了在所有数据上联合训练的模型与在单独数据集上分别训练的模型，发现联合训练的模型通常表现出更好的性能。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，并为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v3</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>A Cognitive Process-Inspired Architecture for Subject-Agnostic Brain Visual Decoding</title>
      <link>http://arxiv.org/abs/2511.02565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages main text with 6 figures (excluding references),  supplementary material included&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为视觉皮层流架构（VCFlow）的新型层次解码框架，能够无需针对特定受试者训练的情况下，从fMRI数据重建连续视觉体验，解决了跨受试者泛化困难和大脑信号复杂性的挑战。&lt;h4&gt;背景&lt;/h4&gt;主题无关的脑解码技术在临床应用方面有很大潜力，但由于跨受试者泛化困难和大脑信号的复杂性，这一方向仍处于探索阶段。传统方法需要每个受试者超过12小时的数据和大量计算资源。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的解码框架，能够高效地重建视觉体验，减少对受试者特定数据的依赖，并提高计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出视觉皮层流架构（VCFlow），这是一种层次解码框架，明确模拟人类视觉系统的腹侧-背侧架构。通过解离和利用早期视觉皮层、腹侧和背侧流中的特征，捕获视觉重建所需的多样化和互补认知信息。同时引入特征级对比学习策略，增强提取受试者不变的语义表征，提高对未见受试者的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;VCFlow相比传统方法仅损失7%的准确率，但无需重新训练，每10秒即可生成重建的视频，提供了一种快速且临床可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;VCFlow为视觉脑解码领域提供了一个高效、实用的解决方案，有望在临床应用中发挥作用。&lt;h4&gt;翻译&lt;/h4&gt;主题无关的脑解码旨在无需针对特定受试者训练的情况下，从fMRI数据重建连续视觉体验，在临床应用方面有很大潜力。然而，由于跨受试者泛化困难和大脑信号的复杂性，这一方向仍处于探索阶段。在本工作中，我们提出了视觉皮层流架构（VCFlow），一种新的层次解码框架，明确模拟人类视觉系统的腹侧-背侧架构，以学习多维表征。通过解离和利用来自早期视觉皮层、腹侧和背侧流的特征，VCFlow捕获了视觉重建所需的多样化和互补认知信息。此外，我们引入了特征级对比学习策略，以增强提取受试者不变的语义表征，从而增强对未见受试者的主题无关适用性。与需要每个受试者超过12小时数据和大量计算的传统方法不同，VCFlow平均仅损失7%的准确率，无需重新训练，每10秒即可生成每个重建视频，提供了快速且临床可扩展的解决方案。论文接受后将发布源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Subject-agnostic brain decoding, which aims to reconstruct continuous visualexperiences from fMRI without subject-specific training, holds great potentialfor clinical applications. However, this direction remains underexplored due tochallenges in cross-subject generalization and the complex nature of brainsignals. In this work, we propose Visual Cortex Flow Architecture (VCFlow), anovel hierarchical decoding framework that explicitly models the ventral-dorsalarchitecture of the human visual system to learn multi-dimensionalrepresentations. By disentangling and leveraging features from early visualcortex, ventral, and dorsal streams, VCFlow captures diverse and complementarycognitive information essential for visual reconstruction. Furthermore, weintroduce a feature-level contrastive learning strategy to enhance theextraction of subject-invariant semantic representations, thereby enhancingsubject-agnostic applicability to previously unseen subjects. Unlikeconventional pipelines that need more than 12 hours of per-subject data andheavy computation, VCFlow sacrifices only 7\% accuracy on average yet generateseach reconstructed video in 10 seconds without any retraining, offering a fastand clinically scalable solution. The source code will be released uponacceptance of the paper.</description>
      <author>example@mail.com (Jingyu Lu, Haonan Wang, Qixiang Zhang, Xiaomeng Li)</author>
      <guid isPermaLink="false">2511.02565v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Across Time and Views: Multi-Temporal Cross-View Learning for Robust Video Person Re-Identification</title>
      <link>http://arxiv.org/abs/2511.02564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了MTF-CVReID，一个针对视频跨视角行人重识别的参数高效框架，通过七个专门设计的模块解决了视角变化大、尺度差异和时间不一致性带来的挑战，在保持实时效率的同时实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;视频跨视角行人重识别(如空中-地面监控)由于极端视角变化、尺度差异和时间不一致性仍然是一个开放问题。&lt;h4&gt;目的&lt;/h4&gt;开发一个参数高效的框架，解决跨视角视频行人重识别中的挑战，同时保持实时计算效率。&lt;h4&gt;方法&lt;/h4&gt;在ViT-B/16骨干网络上引入七个互补模块：CSFN(校正相机和视角偏差)、MRFH(实现跨高度的尺度稳定)、IAMM(强化持久的身份特征)、TDM(用于感知运动的短期时间编码)、IVFA(实现视角不变的表征对齐)、HTPL(捕获多尺度时间规律)和MVICL(使用对比学习范式强制跨视角身份一致性)。&lt;h4&gt;主要发现&lt;/h4&gt;尽管只增加约200万个参数和0.7 GFLOPs，MTF-CVReID保持了实时效率(189 FPS)，在AG-VPReID基准测试的所有高度级别上实现了最先进性能，并在G2A-VReID和MARS数据集上表现出强大的跨数据集泛化能力。&lt;h4&gt;结论&lt;/h4&gt;精心设计的基于适配器的模块可以在不牺牲计算效率的情况下显著增强跨视角鲁棒性和时间一致性。&lt;h4&gt;翻译&lt;/h4&gt;基于视频的跨域视角行人重识别(例如，空中-地面监控)由于极端视角变化、尺度差异和时间不一致性仍然是一个开放问题。为了解决这些挑战，我们提出了MTF-CVReID，一个参数高效的框架，在ViT-B/16骨干网络上引入了七个互补模块。具体来说，我们包括：(1)跨流特征归一化(CSFN)来校正相机和视角偏差；(2)多分辨率特征调和(MRFH)用于跨高度的尺度稳定；(3)身份感知记忆模块(IAMM)来强化持久的身份特征；(4)时间动态建模(TDM)用于感知运动的短期时间编码；(5)跨视角特征对齐(IVFA)实现视角不变的表征对齐；(6)分层时间模式学习(HTPL)捕获多尺度时间规律；以及(7)多视角身份一致性学习(MVICL)，使用对比学习范式强制跨视角身份一致性。尽管比基线模型只增加了约200万个参数和0.7 GFLOPs，MTF-CVReID保持了实时效率(189 FPS)，并在AG-VPReID基准测试的所有高度级别上实现了最先进性能，同时在G2A-VReID和MARS数据集上具有强大的跨数据集泛化能力。这些结果表明，精心设计的基于适配器的模块可以在不牺牲计算效率的情况下显著增强跨视角鲁棒性和时间一致性。源代码可在https://github.com/MdRashidunnabi/MTF-CVReID获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video-based person re-identification (ReID) in cross-view domains (forexample, aerial-ground surveillance) remains an open problem because of extremeviewpoint shifts, scale disparities, and temporal inconsistencies. To addressthese challenges, we propose MTF-CVReID, a parameter-efficient framework thatintroduces seven complementary modules over a ViT-B/16 backbone. Specifically,we include: (1) Cross-Stream Feature Normalization (CSFN) to correct camera andview biases; (2) Multi-Resolution Feature Harmonization (MRFH) for scalestabilization across altitudes; (3) Identity-Aware Memory Module (IAMM) toreinforce persistent identity traits; (4) Temporal Dynamics Modeling (TDM) formotion-aware short-term temporal encoding; (5) Inter-View Feature Alignment(IVFA) for perspective-invariant representation alignment; (6) HierarchicalTemporal Pattern Learning (HTPL) to capture multi-scale temporal regularities;and (7) Multi-View Identity Consistency Learning (MVICL) that enforcescross-view identity coherence using a contrastive learning paradigm. Despiteadding only about 2 million parameters and 0.7 GFLOPs over the baseline,MTF-CVReID maintains real-time efficiency (189 FPS) and achievesstate-of-the-art performance on the AG-VPReID benchmark across all altitudelevels, with strong cross-dataset generalization to G2A-VReID and MARSdatasets. These results show that carefully designed adapter-based modules cansubstantially enhance cross-view robustness and temporal consistency withoutcompromising computational efficiency. The source code is available athttps://github.com/MdRashidunnabi/MTF-CVReID</description>
      <author>example@mail.com (Md Rashidunnabi, Kailash A. Hambarde, Vasco Lopes, Joao C. Neves, Hugo Proenca)</author>
      <guid isPermaLink="false">2511.02564v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>CoCoVa: Chain of Continuous Vision-Language Thought for Latent Space Reasoning</title>
      <link>http://arxiv.org/abs/2511.02360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoCoVa是一种新的视觉-语言模型框架，通过连续跨模态推理解决传统模型在处理高维视觉感知方面的局限性，在多个基准测试中表现出色。&lt;h4&gt;背景&lt;/h4&gt;人类认知中存在难以用言语表达的隐性思维过程，使人类能以多种方式理解世界；而当代视觉-语言模型局限于离散的语言标记空间推理，限制了视觉感知的丰富高维特性。&lt;h4&gt;目的&lt;/h4&gt;弥合人类隐性思维与当前视觉-语言模型之间的差距，提出利用连续跨模态推理处理多样化视觉-语言任务的新框架。&lt;h4&gt;方法&lt;/h4&gt;CoCoVa的核心是迭代推理循环，使用潜在Q-Former作为动态推理引擎，通过跨模态融合优化潜在思维向量链；实现令牌选择机制识别显著视觉区域；结合对比学习和基于扩散的重构进行多任务训练，确保潜在表示与视觉和文本模态对齐。&lt;h4&gt;主要发现&lt;/h4&gt;CoCoVa在准确率和令牌效率上优于强基线模型；1.5B主干模型在几乎所有基准测试中与7B-9B模型竞争或超越；扩展到7B LLM主干模型时仍保持竞争力；学习到的潜在空间捕捉了可解释和结构化的推理模式。&lt;h4&gt;结论&lt;/h4&gt;CoCoVa成功弥合了离散语言处理与视觉理解连续性之间的表征差距，展示了桥接这一差距的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在人类认知中，存在许多难以言表且超越言语表达的思维过程，使我们能够以多种方式理解和与世界互动。然而，当代视觉-语言模型仍然局限于在离散且刚性的语言标记空间中进行推理，从而限制了视觉感知的丰富高维特性。为了弥合这一差距，我们提出了CoCoVa（连续视觉-语言思维链），一种利用连续跨模态推理处理多样化视觉-语言任务的新框架。CoCoVa的核心是一个迭代推理循环，其中一种新型的潜在Q-Former作为动态推理引擎，通过跨模态融合迭代优化潜在思维向量链。为了聚焦这一过程，令牌选择机制动态识别显著的视觉区域，模拟注意力焦点。为确保这些潜在思维保持基础，我们使用结合对比学习和基于扩散的重构的多任务目标训练模型，强制潜在表示与视觉和文本模态保持对齐。评估显示，CoCoVa在准确率和令牌效率方面优于强基线模型。使用1.5B主干模型时，它在几乎所有基准测试中与更大的7B-9B模型竞争或超越。当扩展到7B LLM主干模型时，它仍能与最先进模型保持竞争力。定性分析验证了学习到的潜在空间捕捉了可解释和结构化的推理模式，突显了CoCoVa弥合离散语言处理与视觉理解连续性之间表征差距的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In human cognition, there exist numerous thought processes that are tacit andbeyond verbal expression, enabling us to understand and interact with the worldin multiple ways. However, contemporary Vision-Language Models (VLMs) remainconstrained to reasoning within the discrete and rigid space of linguistictokens, thereby bottlenecking the rich, high-dimensional nature of visualperception. To bridge this gap, we propose CoCoVa (Chain of ContinuousVision-Language Thought), a novel framework for vision-language model thatleverages continuous cross-modal reasoning for diverse vision-language tasks.The core of CoCoVa is an iterative reasoning cycle, where a novel LatentQ-Former (LQ-Former) acts as a dynamic reasoning engine, iteratively refining achain of latent thought vectors through cross-modal fusion. To focus thisprocess, a token selection mechanism dynamically identifies salient visualregions, mimicking attentional focus. To ensure these latent thoughts remaingrounded, we train the model with a multi-task objective that combinescontrastive learning and diffusion-based reconstruction, enforcing alignmentbetween latent representations and both visual and textual modalities.Evaluations show CoCoVa improves accuracy and token efficiency over strongbaselines. With a 1.5B backbone, it competes with or surpasses larger 7B-9Bmodels on almost all benchmarks. When scaled to 7B LLM backbones, it remainscompetitive with state-of-the-art models. Qualitative analysis validates thatlearned latent space captures interpretable and structured reasoning patterns,highlighting the potential of CoCoVa to bridge the representational gap betweendiscrete language processing and the continuous nature of visual understanding.</description>
      <author>example@mail.com (Jizheng Ma, Xiaofei Zhou, Yanlong Song, Han Yan)</author>
      <guid isPermaLink="false">2511.02360v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Probabilistic Graph Cuts</title>
      <link>http://arxiv.org/abs/2511.02272v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的概率框架，用于图割的概率松弛，作为谱聚类的可微分替代方案，无需特征分解即可实现端到端和在线学习，为可扩展、可微分的图划分提供了严谨的数值稳定基础。&lt;h4&gt;背景&lt;/h4&gt;现有的图割概率松弛方法主要集中在RatioCut上，缺乏通用保证和原则性梯度，限制了其在各种聚类和对比学习任务中的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个覆盖广泛割类型（包括Normalized Cut）的统一概率框架，提供期望离散割的紧密解析上界，并建立可扩展、可微分图划分的严谨数值稳定基础。&lt;h4&gt;方法&lt;/h4&gt;通过积分表示和高斯超几何函数构建统一的概率框架，提供具有闭式前向和反向传播的解析上界，实现可微分图划分。&lt;h4&gt;主要发现&lt;/h4&gt;提出的统一框架覆盖了广泛的割类型，通过积分表示和高斯超几何函数提供了期望离散割的紧密解析上界，并具有闭式前向和反向传播。&lt;h4&gt;结论&lt;/h4&gt;该研究为可扩展、可微分的图划分提供了严谨、数值稳定的基础，能够支持广泛的聚类和对比学习目标，克服了现有方法的局限性。&lt;h4&gt;翻译&lt;/h4&gt;图割的概率松弛为谱聚类提供了不同的可微分替代方案，能够在不进行特征分解的情况下实现端到端和在线学习，但先前的工作主要集中在RatioCut上，缺乏通用保证和原则性梯度。我们提出了一个统一的概率框架，涵盖了广泛的割类型，包括Normalized Cut。我们的框架通过积分表示和具有闭式前向和反向传播的高斯超几何函数，为期望离散割提供了紧密的解析上界。这些结果共同为可扩展、可微分的图划分提供了一个严谨的、数值稳定的基础，涵盖了广泛的聚类和对比学习目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic relaxations of graph cuts offer a differentiable alternative tospectral clustering, enabling end-to-end and online learning withouteigendecompositions, yet prior work centered on RatioCut and lacked generalguarantees and principled gradients. We present a unified probabilisticframework that covers a wide class of cuts, including Normalized Cut. Ourframework provides tight analytic upper bounds on expected discrete cuts viaintegral representations and Gauss hypergeometric functions with closed-formforward and backward. Together, these results deliver a rigorous, numericallystable foundation for scalable, differentiable graph partitioning covering awide range of clustering and contrastive learning objectives.</description>
      <author>example@mail.com (Ayoub Ghriss)</author>
      <guid isPermaLink="false">2511.02272v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>NSYNC: Negative Synthetic Image Generation for Contrastive Training to Improve Stylized Text-To-Image Translation</title>
      <link>http://arxiv.org/abs/2511.01517v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为NSYNC的新型对比学习框架，通过生成负合成数据集与正真实图像结合进行对比训练，以提高大型文本到图像扩散模型的风格化能力。&lt;h4&gt;背景&lt;/h4&gt;当前文本条件图像生成方法能生成逼真图像但无法捕捉特定风格，即使在目标风格数据集上微调也难以掌握风格特征。&lt;h4&gt;目的&lt;/h4&gt;提高大型文本到图像扩散模型的风格化能力，使其能够更好地捕捉特定风格特征。&lt;h4&gt;方法&lt;/h4&gt;NSYNC框架专注于生成负合成数据集，与正真实图像一起用于对比训练。同时处理负数据和正数据获得对应梯度，通过从正梯度中减去其在负梯度上的投影得到正交分量，基于此更新参数，消除正负数据中都存在的平凡属性，引导模型捕捉独特风格。&lt;h4&gt;主要发现&lt;/h4&gt;在各种画家和插画师风格的实验中，NSYNC在定量和定性评估上都优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;NSYNC框架能有效提升文本到图像扩散模型的风格化能力，通过对比学习方法和负合成数据集的生成，使模型能够更好地捕捉特定风格特征。&lt;h4&gt;翻译&lt;/h4&gt;当前文本条件图像生成方法输出看起来真实的图像，但它们无法捕捉特定风格。简单地在目标风格数据集上微调仍然难以掌握风格特征。在这项工作中，我们提出了一种新颖的对比学习框架来提高大型文本到图像扩散模型的风格化能力。受图像生成模型惊人进展的启发，我们在方法中利用了合成图像生成。通常，生成的合成数据依赖于任务，大多数情况下用于扩大可用的真实训练数据集。有了NSYNC，我们专注于生成负合成数据集，与真实正图像一起用于新颖的对比训练方案。在我们提出的训练设置中，我们将负数据与正数据一起前向传播，分别获得负梯度和正梯度。然后我们通过从正梯度中减去其在负梯度上的投影来获得正交分量，基于此更新参数。这个正交分量消除了正负数据中都存在的平凡属性，并引导模型捕捉更独特的风格。在各种画家和插画师风格上的实验表明，我们的方法在定量和定性上都优于基线方法。我们的代码可在https://github.com/giddyyupp/NSYNC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current text conditioned image generation methods output realistic lookingimages, but they fail to capture specific styles. Simply finetuning them on thetarget style datasets still struggles to grasp the style features. In thiswork, we present a novel contrastive learning framework to improve thestylization capability of large text-to-image diffusion models. Motivated bythe astonishing advance in image generation models that makes synthetic data anintrinsic part of model training in various computer vision tasks, we exploitsynthetic image generation in our approach. Usually, the generated syntheticdata is dependent on the task, and most of the time it is used to enlarge theavailable real training dataset. With NSYNC, alternatively, we focus ongenerating negative synthetic sets to be used in a novel contrastive trainingscheme along with real positive images. In our proposed training setup, weforward negative data along with positive data and obtain negative and positivegradients, respectively. We then refine the positive gradient by subtractingits projection onto the negative gradient to get the orthogonal component,based on which the parameters are updated. This orthogonal component eliminatesthe trivial attributes that are present in both positive and negative data anddirects the model towards capturing a more unique style. Experiments on variousstyles of painters and illustrators show that our approach improves theperformance over the baseline methods both quantitatively and qualitatively.Our code is available at https://github.com/giddyyupp/NSYNC.</description>
      <author>example@mail.com (Serkan Ozturk, Samet Hicsonmez, Pinar Duygulu)</author>
      <guid isPermaLink="false">2511.01517v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Cognition Augmented End2End Autonomous Driving</title>
      <link>http://arxiv.org/abs/2511.01334v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages,4 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为E³AD的新范式，通过在视觉特征提取网络和通用脑电图大模型之间进行对比学习，学习潜在的人类驾驶认知以增强端到端自动驾驶规划性能。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的端到端自动驾驶已成为新范式，但现有方法通常依赖于标签监督下训练的视觉特征提取网络，这种有限监督框架限制了驾驶模型的通用性和适用性。&lt;h4&gt;目的&lt;/h4&gt;提出E³AD范式，通过对比学习整合人类驾驶认知，以增强端到端自动驾驶的规划性能。&lt;h4&gt;方法&lt;/h4&gt;收集认知数据集用于对比学习；研究使用人类驾驶认知增强端到端规划的方法和机制；在公开数据集上使用流行驾驶模型作为基线；进行开环和闭环测试全面评估规划性能。&lt;h4&gt;主要发现&lt;/h4&gt;E³AD范式显著增强了基线模型的端到端规划性能；消融研究验证了驾驶认知的贡献和对比学习过程的有效性。&lt;h4&gt;结论&lt;/h4&gt;这是首个将人类驾驶认知整合到端到端自动驾驶规划中的工作；是具身认知数据融入端到端自动驾驶的初步尝试；为脑启发自动驾驶系统提供了有价值的见解；代码将在Github上提供。&lt;h4&gt;翻译&lt;/h4&gt;近年来，基于视觉的端到端自动驾驶已成为一种新范式。然而，流行的端到端方法通常依赖于在标签监督下训练的视觉特征提取网络。这种有限监督框架限制了驾驶模型的通用性和适用性。在本文中，我们提出了一种称为E³AD的新范式，主张在视觉特征提取网络和通用脑电图大模型之间进行对比学习，以学习潜在的人类驾驶认知，从而增强端到端规划。在这项工作中，我们收集了用于上述对比学习过程的数据集。随后，我们研究了使用人类驾驶认知增强端到端规划的方法和潜在机制，在公开可用的自动驾驶数据集上使用流行的驾驶模型作为基线。进行了开环和闭环测试，以全面评估规划性能。实验结果表明，E³AD范式显著增强了基线模型的端到端规划性能。消融研究进一步验证了驾驶认知的贡献和对比学习过程的有效性。据我们所知，这是第一个将人类驾驶认知整合用于改进端到端自动驾驶规划的工作。它代表了将具身认知数据融入端到端自动驾驶的初步尝试，为未来的脑启发自动驾驶系统提供了有价值的见解。我们的代码将在Github上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, vision-based end-to-end autonomous driving has emerged as anew paradigm. However, popular end-to-end approaches typically rely on visualfeature extraction networks trained under label supervision. This limitedsupervision framework restricts the generality and applicability of drivingmodels. In this paper, we propose a novel paradigm termed $E^{3}AD$, whichadvocates for comparative learning between visual feature extraction networksand the general EEG large model, in order to learn latent human drivingcognition for enhancing end-to-end planning. In this work, we collected acognitive dataset for the mentioned contrastive learning process. Subsequently,we investigated the methods and potential mechanisms for enhancing end-to-endplanning with human driving cognition, using popular driving models asbaselines on publicly available autonomous driving datasets. Both open-loop andclosed-loop tests are conducted for a comprehensive evaluation of planningperformance. Experimental results demonstrate that the $E^{3}AD$ paradigmsignificantly enhances the end-to-end planning performance of baseline models.Ablation studies further validate the contribution of driving cognition and theeffectiveness of comparative learning process. To the best of our knowledge,this is the first work to integrate human driving cognition for improvingend-to-end autonomous driving planning. It represents an initial attempt toincorporate embodied cognitive data into end-to-end autonomous driving,providing valuable insights for future brain-inspired autonomous drivingsystems. Our code will be made available at Github</description>
      <author>example@mail.com (Ling Niu, Xiaoji Zheng, Han Wang, Chen Zheng, Ziyuan Yang, Bokui Chen, Jiangtao Gong)</author>
      <guid isPermaLink="false">2511.01334v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>ColMate: Contrastive Late Interaction and Masked Text for Multimodal Document Retrieval</title>
      <link>http://arxiv.org/abs/2511.00903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了ColMate，一个专门针对多模态文档检索的模型，通过OCR预训练、自监督掩码对比学习和后期交互评分机制，改进了现有的文档检索方法，在基准测试中取得了更好的性能。&lt;h4&gt;背景&lt;/h4&gt;检索增强生成在模型需要专业知识或最新数据访问时已被证明具有实用性。然而，现有的多模态文档检索方法通常复制仅为文本检索开发的技术，无论是在文档编码方式、训练目标定义还是相似度分数计算方面。&lt;h4&gt;目的&lt;/h4&gt;解决现有多模态文档检索方法的局限性，弥合多模态表示学习与文档检索之间的差距。&lt;h4&gt;方法&lt;/h4&gt;提出ColMate模型，它利用三种关键技术：基于OCR的预训练目标、自监督掩码对比学习目标，以及与多模态文档结构和视觉特征更相关的后期交互评分机制。&lt;h4&gt;主要发现&lt;/h4&gt;ColMate在ViDoRe V2基准测试上比现有检索模型提高了3.61%，并且显示出对域外基准测试的更强泛化能力。&lt;h4&gt;结论&lt;/h4&gt;ColMate是一个有效的文档检索模型，它专门针对多模态文档的特点进行了优化，能够提供比现有方法更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;检索增强生成已被证明在模型需要专业知识或访问最新数据时具有实用性。然而，现有的多模态文档检索方法通常复制仅为文本检索开发的技术，无论是在文档编码方式、训练目标定义还是相似度分数计算方面。为解决这些局限性，我们提出了ColMate，一个弥合多模态表示学习与文档检索之间差距的文档检索模型。ColMate利用了基于OCR的预训练目标、自监督掩码对比学习目标，以及一种与多模态文档结构和视觉特征更相关的后期交互评分机制。ColMate在ViDoRe V2基准测试上比现有检索模型提高了3.61%，显示出对域外基准测试的更强泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval-augmented generation has proven practical when models requirespecialized knowledge or access to the latest data. However, existing methodsfor multimodal document retrieval often replicate techniques developed fortext-only retrieval, whether in how they encode documents, define trainingobjectives, or compute similarity scores. To address these limitations, wepresent ColMate, a document retrieval model that bridges the gap betweenmultimodal representation learning and document retrieval. ColMate utilizes anovel OCR-based pretraining objective, a self-supervised masked contrastivelearning objective, and a late interaction scoring mechanism more relevant tomultimodal document structures and visual characteristics. ColMate obtains3.61% improvements over existing retrieval models on the ViDoRe V2 benchmark,demonstrating stronger generalization to out-of-domain benchmarks.</description>
      <author>example@mail.com (Ahmed Masry, Megh Thakkar, Patrice Bechard, Sathwik Tejaswi Madhusudhan, Rabiul Awal, Shambhavi Mishra, Akshay Kalkunte Suresh, Srivatsava Daruru, Enamul Hoque, Spandana Gella, Torsten Scholak, Sai Rajeswar)</author>
      <guid isPermaLink="false">2511.00903v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>TriCon-Fair: Triplet Contrastive Learning for Mitigating Social Bias in Pre-trained Language Models</title>
      <link>http://arxiv.org/abs/2511.00854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为TriCon-Fair的对比学习框架，用于解决大型语言模型中的社会偏见传播问题。该方法通过解耦损失函数，结合三元组和语言建模项，消除正负耦合，减少歧视性输出，同时保持强大的下游性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的广泛应用引发了关于社会偏见传播的严重担忧，这可能导致有害和不公平的结果。现有的去偏见方法独立处理有偏见和无偏见的样本，忽略了它们之间的相互关系。&lt;h4&gt;目的&lt;/h4&gt;解决现有去偏见方法中存在的隐藏负-正耦合问题，即对一组的改进无意中损害另一组，导致残余社会偏见持续存在。&lt;h4&gt;方法&lt;/h4&gt;提出TriCon-Fair，一种对比学习框架，采用解耦损失函数，结合三元组和语言建模项，消除正负耦合。该方法为每个锚点分配明确的有偏见负样本和无偏见正样本，解耦推-拉动态，避免正负耦合，并联合优化语言建模目标以保持通用能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TriCon-Fair能够超越现有的去偏见基线，减少歧视性输出，同时保持强大的下游性能。&lt;h4&gt;结论&lt;/h4&gt;TriCon-Fair为敏感的自然语言处理应用提供了一种实用且合乎伦理的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型日益增多的应用引发了关于社会偏见传播的严重担忧，这可能导致有害和不公平的结果。然而，现有的去偏见方法独立处理有偏见和无偏见的样本，从而忽略了它们之间的相互关系。这种疏忽导致了一种隐藏的负-正耦合，即对一组的改进无意中损害另一组，使残余社会偏见得以持续。在本文中，我们介绍了TriCon-Fair，一种对比学习框架，采用结合三元组和语言建模项的解耦损失函数来消除负-正耦合。我们的TriCon-Fair为每个锚点分配明确的有偏见负样本和无偏见正样本，解耦推-拉动态，避免负-正耦合，并联合优化语言建模(LM)目标以保持通用能力。实验结果表明，TriCon-Fair超越了现有的去偏见基线，减少了歧视性输出，同时保持强大的下游性能。这表明我们提出的TriCon-Fair为敏感的自然语言处理应用提供了一种实用且合乎伦理的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing utilization of large language models raises significantconcerns about the propagation of social biases, which may result in harmfuland unfair outcomes. However, existing debiasing methods treat the biased andunbiased samples independently, thus ignoring their mutual relationship. Thisoversight enables a hidden negative-positive coupling, where improvements forone group inadvertently compromise the other, allowing residual social bias topersist. In this paper, we introduce TriCon-Fair, a contrastive learningframework that employs a decoupled loss that combines triplet and languagemodeling terms to eliminate positive-negative coupling. Our TriCon-Fair assignseach anchor an explicitly biased negative and an unbiased positive, decouplingthe push-pull dynamics and avoiding positive-negative coupling, and jointlyoptimizes a language modeling (LM) objective to preserve general capability.Experimental results demonstrate that TriCon-Fair reduces discriminatory outputbeyond existing debiasing baselines while maintaining strong downstreamperformance. This suggests that our proposed TriCon-Fair offers a practical andethical solution for sensitive NLP applications.</description>
      <author>example@mail.com (Chong Lyu, Lin Li, Shiqing Wu, Jingling Yuan)</author>
      <guid isPermaLink="false">2511.00854v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:53 +0800</pubDate>
    </item>
    <item>
      <title>Towards classification-based representation learning for place recognition on LiDAR scans</title>
      <link>http://arxiv.org/abs/2511.00738v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将地点识别作为多类分类问题的替代方法，通过为LiDAR扫描分配离散位置标签并训练编码器-解码器模型来直接分类位置，在NuScenes数据集上验证了其与对比学习方法相当的竞争性能，同时具有更高的训练效率和稳定性。&lt;h4&gt;背景&lt;/h4&gt;地点识别是自动驾驶中的关键任务，允许车辆使用传感器数据确定自身位置。现有方法大多依赖于对比学习。&lt;h4&gt;目的&lt;/h4&gt;探索一种替代对比学习的方法，将地点识别作为多类分类问题处理。&lt;h4&gt;方法&lt;/h4&gt;为LiDAR扫描分配离散位置标签，训练编码器-解码器模型直接分类每个扫描的位置。&lt;h4&gt;主要发现&lt;/h4&gt;在NuScenes数据集上评估显示，该方法与基于对比学习的方法具有竞争性能，同时在训练效率和稳定性方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;将地点识别作为多类分类问题是一种有效的替代方法，具有与对比学习方法相当的性能，并且在训练效率和稳定性方面具有优势。&lt;h4&gt;翻译&lt;/h4&gt;地点识别是自动驾驶中的一个关键任务，它允许车辆使用传感器数据来确定自己的位置。虽然大多数现有方法依赖于对比学习，但我们通过将地点识别构建为一个多类分类问题来探索一种替代方法。我们的方法为LiDAR扫描分配离散的位置标签，并训练一个编码器-解码器模型来直接分类每个扫描的位置。我们在NuScenes数据集上评估了这种方法，并表明它与基于对比学习的方法相比具有竞争力的性能，同时在训练效率和稳定性方面提供优势。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于激光雷达(LiDAR)扫描的地点识别问题，即根据传感器数据确定车辆位置。这个问题在现实中非常重要，因为在GPS信号不可靠的环境（如城市峡谷、隧道或恶劣天气）中，准确的定位对自动驾驶车辆的导航、地图绘制和安全决策至关重要。在研究中，这个问题也很重要，因为现有方法大多基于对比学习，需要复杂的负样本挖掘策略且训练效率低、稳定性差。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到人脸识别领域分类方法的启发，思考是否可以将地点识别问题重新定义为分类任务。作者注意到地点不像标准分类任务中的对象那样有明确边界，因此通过将连续位置离散化为网格单元来解决这一问题。他们借鉴了PointNet++作为骨干网络处理点云，采用了两塔(two-tower)架构分离索引构建和查询服务，并参考了LCPR的数据集划分策略。作者设计了掩码交叉熵损失函数，避免惩罚预测到正确位置相邻网格的预测，解决了空间相关位置之间的梯度冲突问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将地点识别问题从传统的对比学习框架重新定义为多类别分类问题，通过将连续位置离散化为网格单元，训练模型直接预测激光扫描对应的离散位置类别。整体流程包括：1)数据准备和划分；2)位置离散化，将连续坐标转换为网格坐标并分配唯一类别标签；3)构建编码器-解码器模型，使用PointNet++处理点云并添加分类头；4)使用掩码交叉熵损失进行训练，避免惩罚相邻位置的预测；5)评估时使用KNN搜索在预构建的嵌入数据库中查找最相似的样本，计算召回率指标。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将地点识别重新定义为多类别分类问题；2)提出新颖的位置离散化方法，使用3元组表示离散位置；3)设计掩码交叉熵损失解决空间相关位置的梯度冲突；4)避免对比学习中的复杂负样本挖掘，提高训练效率；5)通过整合多地图数据展示大规模训练可行性。相比之前的工作，不同之处在于训练目标（分类vs度量学习）、负样本处理（无需复杂挖掘）、模型架构（添加分类头）和训练稳定性（分类方法更稳定）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于分类的新型地点识别方法，通过位置离散化和掩码损失函数，为激光雷达扫描的地点识别提供了更高效、更稳定的训练框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Place recognition is a crucial task in autonomous driving, allowing vehiclesto determine their position using sensor data. While most existing methods relyon contrastive learning, we explore an alternative approach by framing placerecognition as a multi-class classification problem. Our method assignsdiscrete location labels to LiDAR scans and trains an encoder-decoder model toclassify each scan's position directly. We evaluate this approach on theNuScenes dataset and show that it achieves competitive performance compared tocontrastive learning-based methods while offering advantages in trainingefficiency and stability.</description>
      <author>example@mail.com (Maksim Konoplia, Dmitrii Khizbullin)</author>
      <guid isPermaLink="false">2511.00738v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Reasoning Planning for Language Models</title>
      <link>http://arxiv.org/abs/2511.00521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EPIC的集成规划与对比学习框架，用于解决语言模型生成中选择合适推理方法的问题，通过理论分析和创新框架实现了准确性和计算效率的平衡。&lt;h4&gt;背景&lt;/h4&gt;在语言模型生成中，为给定查询选择合适的推理方法仍然是一个关键挑战。现有方法通常生成多个候选响应并使用聚合策略选择输出答案，且往往假设更多的候选答案会带来更高的准确性。&lt;h4&gt;目的&lt;/h4&gt;重新审视'更多候选答案意味着更高准确性'这一假设，通过严谨的理论分析，推导固定生成分布和候选大小下标准聚合方法的准确性界限。&lt;h4&gt;方法&lt;/h4&gt;提出EPIC（Ensemble Planning with Contrastive learning）框架，学习一个共享的表示空间来捕捉模型推理能力和查询方法兼容性，并将概率界限作为正则化项纳入效用驱动的优化中，平衡准确性和计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析对现有假设有了更深入的理解；EPIC能够在各种数学推理任务中一致地选择最优推理方法；在提高准确性的同时减少了计算开销。&lt;h4&gt;结论&lt;/h4&gt;EPIC框架有效地解决了语言模型生成中选择合适推理方法的挑战，通过理论分析和创新框架实现了准确性和计算效率的平衡。&lt;h4&gt;翻译&lt;/h4&gt;为给定查询选择合适的推理方法在语言模型生成中仍然是一个关键挑战。现有方法通常生成多个候选响应并使用聚合策略来选择输出答案，常常假设更多的候选答案会带来更高的准确性。我们通过严谨的理论分析重新审视这一假设，推导了固定生成分布和候选大小下标准聚合方法的准确性界限。基于这些见解，我们引入了EPIC，一种集成规划与对比学习框架，用于学习一个共享的表示空间，该空间能够捕捉模型推理能力和查询方法兼容性。EPIC将我们的概率界限作为正则化项纳入到效用驱动的优化中，平衡准确性和计算成本。在各种数学推理任务上的实验表明，EPIC能够一致地选择最优推理方法，在提高准确性的同时减少计算开销。我们的代码可以在https://github.com/nguyenngocbaocmt02/EPIC找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Selecting an appropriate reasoning method for a given query remains a keychallenge in language model generation. Existing approaches typically generatemultiple candidate responses and use an aggregation strategy to select theoutput answer, often assuming that more candidate answers yield higheraccuracy. We revisit this assumption through a rigorous theoretical analysis,deriving accuracy bounds for standard aggregation methods under fixedgeneration distributions and candidate sizes. Building on these insights, weintroduce EPIC, an Ensemble Planning with Contrastive learning framework tolearn a shared representation space that captures both model reasoningabilities and query-method compatibility. EPIC incorporates our probabilitybounds as a regularizer in a utility-driven optimization that balances accuracyand computational cost. Experiments on diverse mathematical reasoning tasksshow that EPIC consistently selects optimal reasoning methods, improvingaccuracy while reducing computational overhead. Our code can be found athttps://github.com/nguyenngocbaocmt02/EPIC.</description>
      <author>example@mail.com (Bao Nguyen, Hieu Trung Nguyen, Ruifeng She, Xiaojin Fu, Viet Anh Nguyen)</author>
      <guid isPermaLink="false">2511.00521v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>ToxicTextCLIP: Text-Based Poisoning and Backdoor Attacks on CLIP Pre-training</title>
      <link>http://arxiv.org/abs/2511.00446v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ToxicTextCLIP的框架，用于生成高质量的对抗性文本来攻击CLIP模型在预训练阶段。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型通过自监督对比学习对齐大规模网络数据中的图像-文本对推动了视觉-语言建模，但它依赖未筛选的互联网数据，面临数据投毒和后门风险。现有研究主要关注基于图像的攻击，而同样对CLIP训练至关重要的文本模态尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种针对CLIP预训练阶段的高质量对抗文本生成框架，解决背景不一致导致的语义错位和背景一致文本稀缺性问题。&lt;h4&gt;方法&lt;/h4&gt;ToxicTextCLIP框架迭代应用两种技术：1) 背景感知选择器，优先选择与目标类别背景内容对齐的文本；2) 背景驱动增强器，生成语义连贯且多样化的投毒样本。&lt;h4&gt;主要发现&lt;/h4&gt;在分类和检索任务上的实验表明，ToxicTextCLIP实现了高达95.83%的投毒成功率和98.68%的后门Hit@1，同时成功绕过了RoCLIP、CleanCLIP和SafeCLIP防御。&lt;h4&gt;结论&lt;/h4&gt;ToxicTextCLIP是一种有效的针对CLIP预训练阶段的文本投毒框架，能够高效生成高质量的对抗文本。&lt;h4&gt;翻译&lt;/h4&gt;对比语言图像预训练（CLIP）模型通过自监督对比学习对齐大规模网络数据中的图像-文本对，显著推动了视觉-语言建模的发展。然而，它对未筛选的互联网来源数据的依赖使其面临数据投毒和后门风险。虽然现有研究主要调查基于图像的攻击，但对CLIP训练同样至关重要的文本模态仍未被充分探索。在这项工作中，我们引入了ToxicTextCLIP，一个用于在预训练阶段针对CLIP生成高质量对抗文本的框架。该框架解决了两个关键挑战：由背景与目标类别不一致导致的语义错位，以及背景一致文本的稀缺性。为此，ToxicTextCLIP迭代应用：1) 一个背景感知选择器，优先选择与目标类别背景内容对齐的文本；2) 一个背景驱动增强器，生成语义连贯且多样化的投毒样本。在分类和检索任务上的广泛实验表明，ToxicTextCLIP实现了高达95.83%的投毒成功率和98.68%的后门Hit@1，同时绕过了RoCLIP、CleanCLIP和SafeCLIP防御。源代码可通过https://github.com/xinyaocse/ToxicTextCLIP/访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Contrastive Language-Image Pretraining (CLIP) model has significantlyadvanced vision-language modeling by aligning image-text pairs from large-scaleweb data through self-supervised contrastive learning. Yet, its reliance onuncurated Internet-sourced data exposes it to data poisoning and backdoorrisks. While existing studies primarily investigate image-based attacks, thetext modality, which is equally central to CLIP's training, remainsunderexplored. In this work, we introduce ToxicTextCLIP, a framework forgenerating high-quality adversarial texts that target CLIP during thepre-training phase. The framework addresses two key challenges: semanticmisalignment caused by background inconsistency with the target class, and thescarcity of background-consistent texts. To this end, ToxicTextCLIP iterativelyapplies: 1) a background-aware selector that prioritizes texts with backgroundcontent aligned to the target class, and 2) a background-driven augmenter thatgenerates semantically coherent and diverse poisoned samples. Extensiveexperiments on classification and retrieval tasks show that ToxicTextCLIPachieves up to 95.83% poisoning success and 98.68% backdoor Hit@1, whilebypassing RoCLIP, CleanCLIP and SafeCLIP defenses. The source code can beaccessed via https://github.com/xinyaocse/ToxicTextCLIP/.</description>
      <author>example@mail.com (Xin Yao, Haiyang Zhao, Yimin Chen, Jiawei Guo, Kecheng Huang, Ming Zhao)</author>
      <guid isPermaLink="false">2511.00446v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Simple and Behavior-Driven Augmentation for Recommendation with Rich Collaborative Signals</title>
      <link>http://arxiv.org/abs/2511.00436v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages. This paper is accepted at IEEE BigData 2025 (Short)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SCAR(Simple Collaborative Augmentation for Recommendation)的简单协作增强方法，用于改进图协同过滤中的对比学习效果。该方法通过生成伪交互而非删除信息来增强数据视图，在四个基准数据集上表现出色，尤其在稀疏数据场景中效果显著。&lt;h4&gt;背景&lt;/h4&gt;对比学习(CI)已被广泛用于增强图协同过滤(GCF)的性能以实现个性化推荐。数据增强在对比学习的成功中起着关键作用，先前的工作设计了去除用户和项目之间噪声交互的增强方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而直观的增强方法SCAR，旨在最大化对比学习对图协同过滤的有效性，同时避免复杂增强模块的缺点。&lt;h4&gt;方法&lt;/h4&gt;SCAR不删除信息，而是利用从用户-项目交互中提取的协作信号生成伪交互，然后将这些伪交互添加到现有交互中或用来替换现有交互，从而生成更鲁棒的数据表示。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上的实验表明，SCAR在关键评估指标上优于之前的基于对比学习的图协同过滤方法以及其他最先进的自监督学习方法。SCAR在不同的超参数设置下表现出强大的鲁棒性，并且在稀疏数据场景中特别有效。&lt;h4&gt;结论&lt;/h4&gt;通过避免定义噪声的模糊性问题，SCAR能够保留核心信息并生成更可靠的数据视图，同时避免了过于复杂的增强模块，是一种更有效且直观的数据增强方法。&lt;h4&gt;翻译&lt;/h4&gt;对比学习(CI)已被广泛用于增强图协同过滤(GCF)的性能以实现个性化推荐。由于数据增强在对比学习的成功中起着关键作用，先前的工作设计了去除用户和项目之间噪声交互的增强方法，以生成有效的增强视图。然而，定义'噪声'的模糊性持续存在丢失核心信息和生成不可靠数据视图的风险，同时增加了增强的整体复杂性。在本文中，我们提出了用于推荐的简单协作增强(SCAR)，这是一种新颖而直观的增强方法，旨在最大化对比学习对图协同过滤的有效性。SCAR不删除信息，而是利用从用户-项目交互中提取的协作信号生成伪交互，然后将这些伪交互添加到现有交互中或用来替换现有交互。这产生了更鲁棒的表示，同时避免了过于复杂的增强模块的缺陷。我们在四个基准数据集上进行了实验，表明SCAR在关键评估指标上优于之前的基于对比学习的图协同过滤方法以及其他最先进的自监督学习方法。SCAR在不同的超参数设置下表现出强大的鲁棒性，并且在稀疏数据场景中特别有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning (CL) has been widely used for enhancing the performanceof graph collaborative filtering (GCF) for personalized recommendation. Sincedata augmentation plays a crucial role in the success of CL, previous workshave designed augmentation methods to remove noisy interactions between usersand items in order to generate effective augmented views. However, theambiguity in defining ''noisiness'' presents a persistent risk of losing coreinformation and generating unreliable data views, while increasing the overallcomplexity of augmentation. In this paper, we propose Simple CollaborativeAugmentation for Recommendation (SCAR), a novel and intuitive augmentationmethod designed to maximize the effectiveness of CL for GCF. Instead ofremoving information, SCAR leverages collaborative signals extracted fromuser-item interactions to generate pseudo-interactions, which are then eitheradded to or used to replace existing interactions. This results in more robustrepresentations while avoiding the pitfalls of overly complex augmentationmodules. We conduct experiments on four benchmark datasets and show that SCARoutperforms previous CL-based GCF methods as well as other state-of-the-artself-supervised learning approaches across key evaluation metrics. SCARexhibits strong robustness across different hyperparameter settings and isparticularly effective in sparse data scenarios.</description>
      <author>example@mail.com (Doyun Choi, Cheonwoo Lee, Jaemin Yoo)</author>
      <guid isPermaLink="false">2511.00436v2</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Mutual Information guided Visual Contrastive Learning</title>
      <link>http://arxiv.org/abs/2511.00028v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech Report - Undergraduate Thesis - 2023&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于互信息的数据选择方法，以提高表征学习在开放环境中的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;使用InfoNCE损失的表征学习方法能够有效减少人工标注，但数据选择和增强仍依赖人工假设或工程方法，可能不是最优的。例如，对比学习中的数据增强主要关注颜色抖动，旨在模拟真实世界的光照变化。&lt;h4&gt;目的&lt;/h4&gt;研究基于从实际分布计算互信息来选择训练数据的潜力，使学习到的特征在开放环境中具有更好的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;考虑在自然扰动（如颜色变化和运动）下表现出高互信息的场景补丁作为对比损失学习的正样本，提出了一种基于互信息的数据增强方法。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试上评估了所提出的互信息感知数据增强方法，证明了其有效性。&lt;h4&gt;结论&lt;/h4&gt;基于互信息的数据选择方法是一个有前途的未来研究方向。&lt;h4&gt;翻译&lt;/h4&gt;使用InfoNCE损失的表征学习方法已证明通过训练不变性神经网络特征提取器能够显著减少人工标注工作量。尽管不同变体的训练目标遵循数据与学习特征之间的信息最大化原则，但数据选择和增强仍然依赖人类假设或工程方法，这可能不是最优的。例如，对比学习中的数据增强主要关注颜色抖动，旨在模拟真实世界的光照变化。在本工作中，我们研究了基于从实际分布计算的互信息来选择训练数据的潜力，原则上，这应该使学习到的特征在开放环境中应用时具有更好的泛化能力。具体而言，我们将具有自然扰动（如颜色变化和运动）下表现出高互信息的场景补丁视为使用对比损失学习的正样本。我们在多个最先进的表征学习框架的几个基准上评估了所提出的互信息感知数据增强方法，证明了其有效性，并确立了其作为未来研究的有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning methods utilizing the InfoNCE loss have demonstratedconsiderable capacity in reducing human annotation effort by training invariantneural feature extractors. Although different variants of the trainingobjective adhere to the information maximization principle between the data andlearned features, data selection and augmentation still rely on humanhypotheses or engineering, which may be suboptimal. For instance, dataaugmentation in contrastive learning primarily focuses on color jittering,aiming to emulate real-world illumination changes. In this work, we investigatethe potential of selecting training data based on their mutual informationcomputed from real-world distributions, which, in principle, should endow thelearned features with better generalization when applied in open environments.Specifically, we consider patches attached to scenes that exhibit high mutualinformation under natural perturbations, such as color changes and motion, aspositive samples for learning with contrastive loss. We evaluate the proposedmutual-information-informed data augmentation method on several benchmarksacross multiple state-of-the-art representation learning frameworks,demonstrating its effectiveness and establishing it as a promising directionfor future research.</description>
      <author>example@mail.com (Hanyang Chen, Yanchao Yang)</author>
      <guid isPermaLink="false">2511.00028v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Causal Graph Neural Networks for Healthcare</title>
      <link>http://arxiv.org/abs/2511.02531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;医疗人工智能系统在不同机构部署时经常失败，表现为性能下降和历史数据中歧视模式的延续。这种脆弱性部分源于学习统计关联而非因果机制。因果图神经网络通过结合生物医学数据的图表示和因果推理原理，解决了分布转移、歧视和不可解释性的三重危机，学习不变的机制而非虚假相关性。&lt;h4&gt;背景&lt;/h4&gt;医疗人工智能系统在不同机构部署时经常失败，有记录显示性能下降和延续了历史数据中的歧视模式。这种脆弱性部分源于学习统计关联而非因果机制。&lt;h4&gt;目的&lt;/h4&gt;解决医疗人工智能系统的分布转移、歧视和不可解释性三重危机，通过学习不变的机制而非虚假相关性来提高系统性能和公平性。&lt;h4&gt;方法&lt;/h4&gt;因果图神经网络，结合结构因果模型、解纠缠的因果表征学习，以及图上的干预预测和反事实推理技术。&lt;h4&gt;主要发现&lt;/h4&gt;因果图神经网络已应用于精神疾病诊断（脑网络分析）、癌症亚型分类（多组学因果整合）、连续生理监测（机械解释）和药物推荐（纠正处方偏见）。这些进展为患者特异性因果数字孪生奠定基础，结合大型语言模型进行假设生成和因果图神经网络进行机械验证。&lt;h4&gt;结论&lt;/h4&gt;仍存在重大障碍，包括计算需求阻碍实时部署、验证挑战需要多模态证据三角测量、以及因果清洗风险。提出分层框架区分受因果启发的架构和因果验证的发现，并确定关键研究优先事项，以做出因果而非纯粹关联的声明。&lt;h4&gt;翻译&lt;/h4&gt;医疗人工智能系统在不同机构部署时经常失败，有记录显示性能下降和延续了历史数据中的歧视模式。这种脆弱性部分源于学习统计关联而非因果机制。因果图神经网络通过结合生物医学数据的图表示和因果推理原理，解决了分布转移、歧视和不可解释性的三重危机，学习不变的机制而非虚假相关性。本综述审视了方法论基础，包括结构因果模型、解纠缠的因果表征学习，以及图上的干预预测和反事实推理技术。我们分析了具有临床价值的应用，包括通过脑网络分析的精神疾病诊断、通过多组学因果整合的癌症亚型分类、具有机械解释的连续生理监测，以及纠正处方偏见的药物推荐。这些进展为患者特异性因果数字孪生奠定基础，使计算机内临床实验成为可能，并整合大型语言模型进行假设生成和因果图神经网络进行机械验证。仍然存在重大障碍，包括计算需求阻碍实时部署、验证挑战需要超越交叉验证的多模态证据三角测量，以及因果清洗的风险（方法使用因果术语但缺乏严格的证据支持）。我们提出分层框架区分受因果启发的架构和因果验证的发现，并确定关键研究优先事项，以做出因果而非纯粹关联的声明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Healthcare artificial intelligence systems routinely fail when deployedacross institutions, with documented performance drops and perpetuation ofdiscriminatory patterns embedded in historical data. This brittleness stems, inpart, from learning statistical associations rather than causal mechanisms.Causal graph neural networks address this triple crisis of distribution shift,discrimination, and inscrutability by combining graph-based representations ofbiomedical data with causal inference principles to learn invariant mechanismsrather than spurious correlations. This Review examines methodologicalfoundations spanning structural causal models, disentangled causalrepresentation learning, and techniques for interventional prediction andcounterfactual reasoning on graphs. We analyse applications demonstratingclinical value across psychiatric diagnosis through brain network analysis,cancer subtyping via multi-omics causal integration, continuous physiologicalmonitoring with mechanistic interpretation, and drug recommendation correctingprescription bias. These advances establish foundations for patient-specificCausal Digital Twins, enabling in silico clinical experimentation, withintegration of large language models for hypothesis generation and causal graphneural networks for mechanistic validation. Substantial barriers remain,including computational requirements precluding real-time deployment,validation challenges demanding multi-modal evidence triangulation beyondcross-validation, and risks of causal-washing where methods employ causalterminology without rigorous evidentiary support. We propose tiered frameworksdistinguishing causally-inspired architectures from causally-validateddiscoveries and identify critical research priorities making causal rather thanpurely associational claims.</description>
      <author>example@mail.com (Munib Mesinovic, Max Buhlan, Tingting Zhu)</author>
      <guid isPermaLink="false">2511.02531v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Object Detection as an Optional Basis: A Graph Matching Network for Cross-View UAV Localization</title>
      <link>http://arxiv.org/abs/2511.02489v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, Submitted to IEEE TIM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于目标检测的跨视图无人机定位框架，通过图神经网络处理图像间和图像内节点关系，有效解决了GNSS受限区域的定位问题，并在异构航空图像匹配中表现出色。&lt;h4&gt;背景&lt;/h4&gt;随着低空经济的快速发展，无人机在巡逻系统中的测量和跟踪变得至关重要。然而，在GNSS受限区域，基于卫星的定位方法容易失效，需要新的定位解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一个跨视图无人机定位框架，通过目标检测进行地图匹配，有效解决跨时间、跨视图、异构航空图像匹配问题，提高无人机在无卫星信号环境下的定位能力。&lt;h4&gt;方法&lt;/h4&gt;利用现代目标检测从无人机和卫星图像中提取显著实例，集成图神经网络推理图像间和图像内节点关系，采用细粒度的基于图的节点相似度度量方法实现检索和定位。与传统图像检索方法和分类任务方法相比，避免了极坐标重投影、透视变换或生成对抗网络可能带来的错位、内容损失和真实性有限问题。&lt;h4&gt;主要发现&lt;/h4&gt;在公共和真实世界数据集上的广泛实验表明，该方法能有效处理异构外观差异，具有良好泛化能力，适用于具有更大模态差距的场景，如红外-可见光图像匹配。&lt;h4&gt;结论&lt;/h4&gt;该框架为GNSS受限区域的无人机定位提供了有效解决方案，相关数据集将在https://github.com/liutao23/ODGNNLoc.git公开，为后续研究提供支持。&lt;h4&gt;翻译&lt;/h4&gt;随着低空经济的快速增长，无人机已成为巡逻系统中测量和跟踪的关键工具。然而，在GNSS受限区域，基于卫星的定位方法容易失效。本文提出了一种跨视图无人机定位框架，通过目标检测执行地图匹配，旨在有效解决跨时间、跨视图、异构航空图像匹配问题。在典型流程中，无人机视觉定位被表述为图像检索问题：提取特征构建定位地图，并通过将查询图像与具有已知姿态的参考数据库匹配来估计其姿态。由于公开的无人机定位数据集有限，许多方法将定位重新表述为分类任务，并依赖这些数据集中的场景标签来确保准确性。其他方法使用极坐标重投影、透视变换或生成对抗网络来减少跨域差异；然而，它们可能存在错位、内容损失和真实性有限的问题。相比之下，我们利用现代目标检测从无人机和卫星图像中准确提取显著实例，并集成图神经网络来推理图像间和图像内节点关系。使用细粒度的、基于图的节点相似度度量方法，我们的方法实现了强大的检索和定位性能。在公共和真实世界数据集上的广泛实验表明，我们的方法能有效处理异构外观差异，并具有良好的泛化能力，使其适用于具有更大模态差距的场景，如红外-可见光图像匹配。我们的数据集将在以下网址公开：https://github.com/liutao23/ODGNNLoc.git。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid growth of the low-altitude economy, UAVs have become crucialfor measurement and tracking in patrol systems. However, in GNSS-denied areas,satellite-based localization methods are prone to failure. This paper presentsa cross-view UAV localization framework that performs map matching via objectdetection, aimed at effectively addressing cross-temporal, cross-view,heterogeneous aerial image matching. In typical pipelines, UAV visuallocalization is formulated as an image-retrieval problem: features areextracted to build a localization map, and the pose of a query image isestimated by matching it to a reference database with known poses. Becausepublicly available UAV localization datasets are limited, many approachesrecast localization as a classification task and rely on scene labels in thesedatasets to ensure accuracy. Other methods seek to reduce cross-domaindifferences using polar-coordinate reprojection, perspective transformations,or generative adversarial networks; however, they can suffer from misalignment,content loss, and limited realism. In contrast, we leverage modern objectdetection to accurately extract salient instances from UAV and satelliteimages, and integrate a graph neural network to reason about inter-image andintra-image node relationships. Using a fine-grained, graph-basednode-similarity metric, our method achieves strong retrieval and localizationperformance. Extensive experiments on public and real-world datasets show thatour approach handles heterogeneous appearance differences effectively andgeneralizes well, making it applicable to scenarios with larger modality gaps,such as infrared-visible image matching. Our dataset will be publicly availableat the following URL: https://github.com/liutao23/ODGNNLoc.git.</description>
      <author>example@mail.com (Tao Liu, Kan Ren, Qian Chen)</author>
      <guid isPermaLink="false">2511.02489v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Using ensemble learning with hybrid graph neural networks and transformers to predict traffic in cities</title>
      <link>http://arxiv.org/abs/2511.02484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为HybridST的混合架构，用于提高城市交通预测的准确性，特别是在复杂的多模式交通环境中。&lt;h4&gt;背景&lt;/h4&gt;智能交通系统(ITS)在城市交通预测方面仍然存在困难，特别是在具有复杂时空动态的大规模多模式交通环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确捕捉空间依赖性、长期时间模式和外部信号的新型交通预测模型。&lt;h4&gt;方法&lt;/h4&gt;提出HybridST混合架构，整合图神经网络(GNNs)、多头时间序列Transformer和监督集成学习方法(XGBoost或RandomForest)，以综合捕捉空间依赖性、长期时间模式和包括天气、日历或控制状态在内的外部信号。&lt;h4&gt;主要发现&lt;/h4&gt;在METR-LA、PEMS-BAY和Seattle Loop tree三个公开基准数据集上的实验表明，HybridST在MAE和RMSE等重要指标上始终优于经典基线模型(LSTM, GCN, DCRNN, PDFormer)，同时保持良好的可扩展性和易于理解的特点。&lt;h4&gt;结论&lt;/h4&gt;HybridST框架为实时城市交通规划、能源优化和缓解拥堵策略提供了有前景的途径，特别适用于智慧城市和2030年世界杯等重大活动的交通管理。&lt;h4&gt;翻译&lt;/h4&gt;智能交通系统(ITS)在城市交通预测方面仍然面临挑战，特别是在具有复杂时空动态的大规模多模式交通环境中。本文提出了HybridST，一种混合架构，整合了图神经网络(GNNs)、多头时间序列Transformer和监督集成学习方法(XGBoost或RandomForest)，以共同捕捉空间依赖性、长期时间模式和外部信号，包括天气、日历或控制状态。我们在METR-LA、PEMS-BAY和Seattle Loop tree三个公开基准数据集上测试了我们的模型。这些数据集涵盖了从高速公路传感器网络到车路协同感知的各种场景。实验结果表明，HybridST在MAE和RMSE等重要指标上始终优于经典基线模型(LSTM, GCN, DCRNN, PDFormer)，同时仍然保持高度可扩展性和易于理解的特点。所提出的框架为实时城市交通规划、能源优化和缓解拥堵策略提供了有前景的途径，特别是在智慧城市和2030年世界杯等重大活动的框架内。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5281/zenodo.17521951&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligent transportation systems (ITS) still have a hard time accuratelypredicting traffic in cities, especially in big, multimodal settings withcomplicated spatiotemporal dynamics. This paper presents HybridST, a hybridarchitecture that integrates Graph Neural Networks (GNNs), multi-head temporalTransformers, and supervised ensemble learning methods (XGBoost or RandomForest) to collectively capture spatial dependencies, long-range temporalpatterns, and exogenous signals, including weather, calendar, or controlstates. We test our model on the METR-LA, PEMS-BAY, and Seattle Loop treepublic benchmark datasets. These datasets include situations ranging fromfreeway sensor networks to vehicle-infrastructure cooperative perception.Experimental results show that HybridST consistently beats classical baselines(LSTM, GCN, DCRNN, PDFormer) on important metrics like MAE and RMSE, whilestill being very scalable and easy to understand. The proposed frameworkpresents a promising avenue for real-time urban mobility planning, energyoptimization, and congestion alleviation strategies, especially within theframework of smart cities and significant events such as the 2030 FIFA WorldCup.</description>
      <author>example@mail.com (Ismail Zrigui, Samira Khoulji, Mohamed Larbi Kerkeb)</author>
      <guid isPermaLink="false">2511.02484v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Evolving Graph Learning for Out-of-Distribution Generalization in Non-stationary Environments</title>
      <link>http://arxiv.org/abs/2511.02354v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为EvoOOD的新型演化图学习框架，通过环境感知不变模式识别来解决动态图在分布偏移下的泛化能力问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在动态图的空间和时间模式利用方面表现出色，但现有GNN在分布偏移下泛化能力差，这在动态场景中是不可避免的。&lt;h4&gt;目的&lt;/h4&gt;探索动态图生成在潜在非平稳环境中对分布外(OOD)泛化的影响，并提出一种用于OOD泛化的新型演化图学习框架。&lt;h4&gt;方法&lt;/h4&gt;设计环境序列变分自编码器建模环境演化并推断环境分布；引入环境感知不变模式识别机制解决环境多样化问题；使用混合实例化环境样本对单个节点进行细粒度因果干预。&lt;h4&gt;主要发现&lt;/h4&gt;该方法有助于区分用于OOD预测的时空不变模式，特别是在非平稳环境中；实验证明了EvoOOD在真实世界和合成动态数据集分布偏移下的优越性。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，这是首次从环境演化角度研究动态图OOD泛化问题。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络在利用动态图上的空间和时间模式方面表现出色。然而，现有的GNN在分布偏移下表现出较差的泛化能力，这在动态场景中是不可避免的。随着动态图生成在潜在非平稳环境中不断推进，探索它们对分布外(OOD)泛化的影响至关重要。本文通过环境感知不变模式识别，提出了一种用于OOD泛化的新型演化图学习框架(EvoOOD)。具体来说，我们首先设计了一个环境序列变分自编码器来建模环境演化并推断潜在环境分布。然后，我们引入了一种环境感知不变模式识别机制，通过推断的分布来解决环境多样化问题。最后，我们使用混合实例化环境样本对单个节点进行细粒度因果干预。这种方法有助于区分用于OOD预测的时空不变模式，特别是在非平稳环境中。实验结果证明了EvoOOD在真实世界和合成动态数据集分布偏移下的优越性。据我们所知，这是首次从环境演化角度研究动态图OOD泛化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have shown remarkable success in exploiting the spatialand temporal patterns on dynamic graphs. However, existing GNNs exhibit poorgeneralization ability under distribution shifts, which is inevitable indynamic scenarios. As dynamic graph generation progresses amid evolving latentnon-stationary environments, it is imperative to explore their effects onout-of-distribution (OOD) generalization. This paper proposes a novel EvolvingGraph Learning framework for OOD generalization (EvoOOD) by environment-awareinvariant pattern recognition. Specifically, we first design an environmentsequential variational auto-encoder to model environment evolution and inferthe underlying environment distribution. Then, we introduce a mechanism forenvironment-aware invariant pattern recognition, tailored to addressenvironmental diversification through inferred distributions. Finally, weconduct fine-grained causal interventions on individual nodes using a mixtureof instantiated environment samples. This approach helps to distinguishspatio-temporal invariant patterns for OOD prediction, especially innon-stationary environments. Experimental results demonstrate the superiorityof EvoGOOD on both real-world and synthetic dynamic datasets under distributionshifts. To the best of our knowledge, it is the first attempt to study thedynamic graph OOD generalization problem from the environment evolutionperspective.</description>
      <author>example@mail.com (Qingyun Sun, Jiayi Luo, Haonan Yuan, Xingcheng Fu, Hao Peng, Jianxin Li, Philip S. Yu)</author>
      <guid isPermaLink="false">2511.02354v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Link prediction Graph Neural Networks for structure recognition of Handwritten Mathematical Expressions</title>
      <link>http://arxiv.org/abs/2511.02288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted for ICDAR2025-WML&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的手写数学表达式识别方法，通过将数学表达式建模为图结构，结合深度BLSTM网络和图神经网络进行识别和结构优化。&lt;h4&gt;背景&lt;/h4&gt;手写数学表达式识别是一个具有挑战性的任务，需要同时识别符号并理解它们之间的空间关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确识别手写数学表达式并正确理解其结构的方法。&lt;h4&gt;方法&lt;/h4&gt;将手写数学表达式建模为图，其中节点代表符号，边代表空间依赖关系；使用深度BLSTM网络进行符号分割、识别和空间关系分类，形成初始原始图；使用2D-CFG解析器生成所有可能的空间关系；应用基于GNN的链接预测模型优化结构，移除不必要的连接，形成最终的符号标记图。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在手写数学表达式结构识别方面具有良好的性能。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的方法在处理手写数学表达式识别任务中是有效的，能够准确识别符号并正确理解它们之间的空间关系。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图神经网络的手写数学表达式识别方法，通过将手写数学表达式建模为图来表示，其中节点代表符号，边捕获空间依赖关系。使用深度BLSTM网络进行符号分割、识别和空间关系分类，形成初始原始图。然后，2D-CFG解析器生成所有可能的空间关系，而基于GNN的链接预测模型通过移除不必要的连接来优化结构，最终形成符号标记图。实验结果证明了我们方法的有效性，在手写数学表达式结构识别方面显示出良好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a Graph Neural Network (GNN)-based approach for HandwrittenMathematical Expression (HME) recognition by modeling HMEs as graphs, wherenodes represent symbols and edges capture spatial dependencies. A deep BLSTMnetwork is used for symbol segmentation, recognition, and spatial relationclassification, forming an initial primitive graph. A 2D-CFG parser thengenerates all possible spatial relations, while the GNN-based link predictionmodel refines the structure by removing unnecessary connections, ultimatelyforming the Symbol Label Graph. Experimental results demonstrate theeffectiveness of our approach, showing promising performance in HME structurerecognition.</description>
      <author>example@mail.com (Cuong Tuan Nguyen, Ngoc Tuan Nguyen, Triet Hoang Minh Dao, Huy Minh Nhat, Huy Truong Dinh)</author>
      <guid isPermaLink="false">2511.02288v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>PrivGNN: High-Performance Secure Inference for Cryptographic Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02185v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to FC'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究设计并实现了一种名为SysName的轻量级密码学方案，用于在云环境中进行安全图神经网络推断，通过混合加法和函数秘密共享技术，显著提升了计算效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是分析和学习图结构数据的强大工具，可在多种服务中应用。但在隐私关键型云环境中部署这些服务需要开发安全推断协议来保护敏感的图结构数据。现有解决方案主要关注图像和文本数据的卷积模型，而保护图神经网络和图结构数据的安全挑战相对未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级密码学方案，用于在云环境中安全地进行图神经网络推断，保护敏感图结构数据的同时保持高效性和准确性。&lt;h4&gt;方法&lt;/h4&gt;设计并实现SysName方案，在安全两方计算框架下混合使用加法秘密共享和函数秘密共享，基于一系列新颖的交互协议，优化了线性层和非线性层的计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;SysName与最先进解决方案相比，线性层速度提升1.5倍至1.7倍，非线性层速度提升2倍至15倍。在四个数据集上的实验表明，安全预测速度提升1.3倍至4.7倍，同时保持与明文图属性推断相当的准确性。&lt;h4&gt;结论&lt;/h4&gt;SysName是一种高效、安全的图神经网络推断方案，能够在保护数据隐私的同时提供与明文计算相当的准确性和显著更快的计算速度。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络是分析和学习图结构数据的强大工具，促进广泛的服务应用。在隐私关键的云环境中部署此类服务需要开发安全推断协议以保护敏感的图结构数据。然而，现有的安全推断解决方案主要关注图像和文本数据的卷积模型，而保护图神经网络和图结构数据的挑战相对未被充分探索。在本工作中，我们设计、实现并评估了SysName，这是一种用于云中以图为中心推断的轻量级密码学方案。通过在安全两方计算中混合加法秘密共享和函数秘密共享，SysName基于一系列新颖的交互协议精心设计，与最先进解决方案相比，线性层速度提升1.5倍至1.7倍，非线性层速度提升2倍至15倍。提供了彻底的理论分析以证明SysName的正确性、安全性和轻量级特性。在四个数据集上的广泛实验表明，SysName具有卓越的效率，安全预测速度提升1.3倍至4.7倍，同时保持与明文图属性推断相当的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are powerful tools for analyzing and learningfrom graph-structured (GS) data, facilitating a wide range of services.Deploying such services in privacy-critical cloud environments necessitates thedevelopment of secure inference (SI) protocols that safeguard sensitive GSdata. However, existing SI solutions largely focus on convolutional models forimage and text data, leaving the challenge of securing GNNs and GS datarelatively underexplored. In this work, we design, implement, and evaluate$\sysname$, a lightweight cryptographic scheme for graph-centric inference inthe cloud. By hybridizing additive and function secret sharings within securetwo-party computation (2PC), $\sysname$ is carefully designed based on a seriesof novel 2PC interactive protocols that achieve $1.5\times \sim 1.7\times$speedups for linear layers and $2\times \sim 15\times$ for non-linear layersover state-of-the-art (SotA) solutions. A thorough theoretical analysis isprovided to prove $\sysname$'s correctness, security, and lightweight nature.Extensive experiments across four datasets demonstrate $\sysname$'s superiorefficiency with $1.3\times \sim 4.7\times$ faster secure predictions whilemaintaining accuracy comparable to plaintext graph property inference.</description>
      <author>example@mail.com (Fuyi Wang, Zekai Chen, Mingyuan Fan, Jianying Zhou, Lei Pan, Leo Yu Zhang)</author>
      <guid isPermaLink="false">2511.02185v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking LLM Human Simulation: When a Graph is What You Need</title>
      <link>http://arxiv.org/abs/2511.02135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/schang-lab/gems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GEMS的基于图的轻量级模型，用于人类模拟任务，在保持与大型语言模型相当或更好准确性的同时，显著提高了效率、可解释性和透明度。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型越来越多地被用来模拟人类，应用范围从调查预测到决策制定。然而，这些模型通常体积庞大，计算资源需求高。&lt;h4&gt;目的&lt;/h4&gt;探究在人类模拟任务中，是否可以使用更小、更专业的模型替代大型语言模型，特别是在个体在离散选项中做出选择的场景下。&lt;h4&gt;方法&lt;/h4&gt;提出Graph-basEd Models for human Simulation (GEMS)框架，将离散选择模拟任务转化为图上的链接预测问题，利用关系知识并仅在需要时融入语言表示。使用图神经网络作为基础架构。&lt;h4&gt;主要发现&lt;/h4&gt;在三个模拟数据集上的三种关键设置中评估显示，尽管图神经网络模型比大型语言模型小三个数量级，但它能够匹配或超越强大的大型语言模型基线模型的性能。GEMS在准确性、效率、可解释性和透明度方面均表现出色。&lt;h4&gt;结论&lt;/h4&gt;基于图的建模作为大型语言模型用于人类模拟的轻量级替代方案具有显著前景，特别是在需要高效、可解释和透明的模拟系统的场景中。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型正越来越多地被用来模拟人类，应用范围从调查预测到决策制定。然而，大型语言模型是否严格必要，还是更小、领域专用的模型就足够了？我们确定了一类模拟问题，即个体在离散选项中做出选择的问题，在这类问题中，图神经网络可以匹配甚至超越强大的大型语言模型基线模型，尽管小三个数量级。我们提出了基于图的人类模拟模型，它将离散选择模拟任务作为图上的链接预测问题，利用关系知识，仅在需要时才融入语言表示。在三个模拟数据集上的三种关键设置中的评估表明，GEMS实现了与大型语言模型相当或更好的准确性，同时具有更高的效率、可解释性和透明度，突显了基于图的建模作为大型语言模型用于人类模拟的轻量级替代方案的潜力。我们的代码可在https://github.com/schang-lab/gems获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are increasingly used to simulate humans, withapplications ranging from survey prediction to decision-making. However, areLLMs strictly necessary, or can smaller, domain-grounded models suffice? Weidentify a large class of simulation problems in which individuals make choicesamong discrete options, where a graph neural network (GNN) can match or surpassstrong LLM baselines despite being three orders of magnitude smaller. Weintroduce Graph-basEd Models for human Simulation (GEMS), which casts discretechoice simulation tasks as a link prediction problem on graphs, leveragingrelational knowledge while incorporating language representations only whenneeded. Evaluations across three key settings on three simulation datasets showthat GEMS achieves comparable or better accuracy than LLMs, with far greaterefficiency, interpretability, and transparency, highlighting the promise ofgraph-based modeling as a lightweight alternative to LLMs for human simulation.Our code is available at https://github.com/schang-lab/gems.</description>
      <author>example@mail.com (Joseph Suh, Suhong Moon, Serina Chang)</author>
      <guid isPermaLink="false">2511.02135v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Predicting Microbial Interactions Using Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.02038v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures, NeurIPS 2025 Workshop New Perspectives in Graph  Machine Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用图神经网络(GNNs)预测微生物间的相互作用类型，包括二元相互作用(正/负)和复杂相互作用类型(如互利共生、竞争和寄生)，取得了80.44%的F1分数，显著优于传统XGBoost模型的72.76%。&lt;h4&gt;背景&lt;/h4&gt;预测物种间相互作用是微生物生态学中的一个关键挑战，因为这些相互作用对微生物群落的结构和活性起着决定性作用。&lt;h4&gt;目的&lt;/h4&gt;利用单培养生长能力、与其他物种的相互作用以及系统发育数据来预测微生物相互作用的负向或正向效应。&lt;h4&gt;方法&lt;/h4&gt;使用包含超过7,500个相互作用的数据集(涉及两个分类群的20个物种在40种不同碳条件下的共培养)训练模型；构建微生物相互作用的边图，利用图神经网络(GNNs)预测相互作用模式。&lt;h4&gt;主要发现&lt;/h4&gt;模型不仅能预测二元相互作用，还能分类更复杂的相互作用类型；初始结果F1分数达80.44%，显著优于文献中可比的传统XGBoost模型(72.76%)。&lt;h4&gt;结论&lt;/h4&gt;图神经网络是预测微生物相互作用的强大工具，为理解微生物群落结构和功能提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;预测物种间相互作用是微生物生态学中的一个关键挑战，因为这些相互作用对微生物群落的结构和活性至关重要。在本工作中，我们使用单培养生长能力、与其他物种的相互作用以及系统发育数据来预测相互作用的负向或正向效应。更准确地说，我们使用了最大的可用成对相互作用数据集之一来训练我们的模型，包含在40种不同碳条件下共培养的两个分类群的20个物种之间的7,500多个相互作用，主要关注Nestor等人[28]的工作。在本工作中，我们提出图神经网络(GNNs)作为预测效应方向的有力分类器。我们构建成对微生物相互作用的边图，以利用单个共培养实验中的共享信息，并使用GNNs预测相互作用模式。我们的模型不仅可以预测二元相互作用(正/负)，还可以分类更复杂的相互作用类型，如互利共生、竞争和寄生。我们的初步结果令人鼓舞，F1分数达到80.44%。这显著优于文献中可比的方法，包括传统的极端梯度提升(XGBoost)模型，后者报告的F1分数为72.76%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting interspecies interactions is a key challenge in microbial ecology,as these interactions are critical to determining the structure and activity ofmicrobial communities. In this work, we used data on monoculture growthcapabilities, interactions with other species, and phylogeny to predict anegative or positive effect of interactions. More precisely, we used one of thelargest available pairwise interaction datasets to train our models, comprisingover 7,500 interactions be- tween 20 species from two taxonomic groupsco-cultured under 40 distinct carbon conditions, with a primary focus on thework of Nestor et al.[28 ]. In this work, we propose Graph Neural Networks(GNNs) as a powerful classifier to predict the direction of the effect. Weconstruct edge-graphs of pairwise microbial interactions in order to leverageshared information across individual co-culture experiments, and use GNNs topredict modes of interaction. Our model can not only predict binaryinteractions (positive/negative) but also classify more complex interactiontypes such as mutualism, competition, and parasitism. Our initial results wereencouraging, achieving an F1-score of 80.44%. This significantly outperformscomparable methods in the literature, including conventional Extreme GradientBoosting (XGBoost) models, which reported an F1-score of 72.76%.</description>
      <author>example@mail.com (Elham Gholamzadeh, Kajal Singla, Nico Scherf)</author>
      <guid isPermaLink="false">2511.02038v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>HyperNQ: A Hypergraph Neural Network Decoder for Quantum LDPC Codes</title>
      <link>http://arxiv.org/abs/2511.01741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 4 figures, Submitted to the IEEE International Conference on  Communications (ICC 2026). Preprint version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HyperNQ是一种基于超图神经网络的量子低密度奇偶校验码解码器，通过利用超边捕获高阶稳定器约束，显著提高了量子错误纠正性能。&lt;h4&gt;背景&lt;/h4&gt;量子计算需要有效的错误纠正策略来缓解噪声和退相干问题。量子低密度奇偶校验码通过支持恒定速率编码和稀疏奇偶校验结构，已成为可扩展量子错误纠正应用的有前途的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕获高阶稳定器约束的QLDPC解码器，以解决传统解码方法如置信传播在短循环存在时收敛性差的问题，以及图神经网络仅限于成对交互而无法捕获高阶相关性的限制。&lt;h4&gt;方法&lt;/h4&gt;提出HyperNQ，第一个基于超图神经网络的QLDPC解码器，利用超边捕获高阶稳定器约束，实现高度表达性和紧凑的解码。采用两阶段消息传递方案，并在伪阈值区域评估解码器性能。&lt;h4&gt;主要发现&lt;/h4&gt;在伪阈值标记以下，HyperNQ将逻辑错误率比置信传播提高了最多84%，比基于图神经网络的策略提高了50%，展示了优于现有最先进解码器的性能。&lt;h4&gt;结论&lt;/h4&gt;HyperNQ通过超图神经网络有效解决了传统QLDPC解码方法的局限性，为量子错误纠正提供了一种创新且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;量子计算需要有效的错误纠正策略来缓解噪声和退相干。量子低密度奇偶校验码通过支持恒定速率编码和稀疏奇偶校验结构，已成为可扩展量子错误纠正应用的有前途的解决方案。然而，通过置信传播等传统方法解码QLDPC码在存在短循环时收敛性差。图神经网络等机器学习技术利用节点特征进行学习消息传递，但它们仅限于Tanner图上的成对交互，限制了捕获高阶相关性的能力。在这项工作中，我们提出了HyperNQ，这是第一个基于超图神经网络的QLDPC解码器，通过利用超边捕获高阶稳定器约束，从而实现高度表达性和紧凑的解码。我们使用两阶段消息传递方案，并在伪阈值区域评估解码器。在伪阈值标记以下，HyperNQ将逻辑错误率比BP提高了最多84%，比基于GNN的策略提高了50%，展示了优于现有最先进解码器的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum computing requires effective error correction strategies to mitigatenoise and decoherence. Quantum Low-Density Parity-Check (QLDPC) codes haveemerged as a promising solution for scalable Quantum Error Correction (QEC)applications by supporting constant-rate encoding and a sparse parity-checkstructure. However, decoding QLDPC codes via traditional approaches such asBelief Propagation (BP) suffers from poor convergence in the presence of shortcycles. Machine learning techniques like Graph Neural Networks (GNNs) utilizelearned message passing over their node features; however, they are restrictedto pairwise interactions on Tanner graphs, which limits their ability tocapture higher-order correlations. In this work, we propose HyperNQ, the firstHypergraph Neural Network (HGNN)- based QLDPC decoder that captureshigher-order stabilizer constraints by utilizing hyperedges-thus enablinghighly expressive and compact decoding. We use a two-stage message passingscheme and evaluate the decoder over the pseudo-threshold region. Below thepseudo-threshold mark, HyperNQ improves the Logical Error Rate (LER) up to 84%over BP and 50% over GNN-based strategies, demonstrating enhanced performanceover the existing state-of-the-art decoders.</description>
      <author>example@mail.com (Ameya S. Bhave, Navnil Choudhury, Kanad Basu)</author>
      <guid isPermaLink="false">2511.01741v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Panther: A Cost-Effective Privacy-Preserving Framework for GNN Training and Inference Services in Cloud Environments</title>
      <link>http://arxiv.org/abs/2511.01654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Transactions on Services Computing  (TSC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Panther，一个在云环境中用于图神经网络训练和推理的经济有效的隐私保护框架，能够在保护隐私的同时显著降低计算和通信成本。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在多个领域有重大影响，随着用户向云计算迁移，在云环境中保护GNN隐私成为关键问题。现有隐私保护技术虽然存在，但计算和通信开销较高，导致财务成本高，限制了其广泛采用。&lt;h4&gt;目的&lt;/h4&gt;保护GNN隐私同时降低额外经济成本，开发一个适用于云环境的经济有效的隐私保护框架。&lt;h4&gt;方法&lt;/h4&gt;Panther利用四方计算异步执行安全数组访问协议，并随机填充GNN节点的邻居信息来保护隐私。&lt;h4&gt;主要发现&lt;/h4&gt;与最先进方法相比，Panther将训练和推理时间分别平均减少75.28%和82.80%，通信开销分别减少52.61%和50.26%，在Google Cloud平台上估计为GNN训练和推理分别节省55.05%和59.00%的财务成本。&lt;h4&gt;结论&lt;/h4&gt;Panther是一个有效的隐私保护框架，能够在保护GNN隐私的同时显著降低计算和通信成本，有望促进隐私保护GNN技术在云环境中的广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在交通状态预测、社交推荐、知识感知问答等领域已产生重大影响。随着越来越多用户转向云计算，在云环境中释放GNN能力的同时保护隐私已成为关键问题。具体而言，GNN的训练数据和推理数据需要防止被外部对手窃取。同时，云计算的经济成本是用户的另一个主要关注点。因此，尽管现有研究已提出云环境中GNN的隐私保护技术，但其额外的计算和通信开销仍然相对较高，导致高额财务成本，限制了用户中的广泛采用。为了在保护GNN隐私的同时降低额外财务成本，我们引入了Panther，这是一个在云环境中用于GNN训练和推理服务的经济有效的隐私保护框架。技术上，Panther利用四方计算异步执行安全数组访问协议，并随机填充GNN节点的邻居信息。我们证明了Panther可以保护GNN模型训练和推理的隐私。我们的评估显示，与最先进的方法相比，Panther分别将训练和推理时间平均减少75.28%和82.80%，通信开销平均减少52.61%和50.26%，这估计在Google Cloud平台上为GNN训练和推理过程分别节省55.05%和59.00%的财务成本（基于按需定价模型）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have marked significant impact in traffic stateprediction, social recommendation, knowledge-aware question answering and soon. As more and more users move towards cloud computing, it has become acritical issue to unleash the power of GNNs while protecting the privacy incloud environments. Specifically, the training data and inference data for GNNsneed to be protected from being stolen by external adversaries. Meanwhile, thefinancial cost of cloud computing is another primary concern for users.Therefore, although existing studies have proposed privacy-preservingtechniques for GNNs in cloud environments, their additional computational andcommunication overhead remain relatively high, causing high financial coststhat limit their widespread adoption among users.  To protect GNN privacy while lowering the additional financial costs, weintroduce Panther, a cost-effective privacy-preserving framework for GNNtraining and inference services in cloud environments. Technically, Pantherleverages four-party computation to asynchronously executing the secure arrayaccess protocol, and randomly pads the neighbor information of GNN nodes. Weprove that Panther can protect privacy for both training and inference of GNNmodels. Our evaluation shows that Panther reduces the training and inferencetime by an average of 75.28% and 82.80%, respectively, and communicationoverhead by an average of 52.61% and 50.26% compared with the state-of-the-art,which is estimated to save an average of 55.05% and 59.00% in financial costs(based on on-demand pricing model) for the GNN training and inference processon Google Cloud Platform.</description>
      <author>example@mail.com (Congcong Chen, Xinyu Liu, Kaifeng Huang, Lifei Wei, Yang Shi)</author>
      <guid isPermaLink="false">2511.01654v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>IVGAE-TAMA-BO: A novel temporal dynamic variational graph model for link prediction in global food trade networks with momentum structural memory and Bayesian optimization</title>
      <link>http://arxiv.org/abs/2511.01639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26pages,6figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究开发了一种创新的动态图神经网络模型IVGAE-TAMA-BO，用于预测全球粮食贸易网络中的未来链接。该模型通过捕捉时间演变和结构依赖关系，显著提高了预测准确性，结合贝叶斯优化的模型在各种贸易场景下表现优异。&lt;h4&gt;背景&lt;/h4&gt;全球粮食贸易在确保粮食安全和维持供应链稳定方面发挥着关键作用。然而，粮食贸易网络结构在政治、经济和环境因素影响下动态演变，使得建模和预测未来的贸易链接具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;有效捕捉粮食贸易网络中的时间模式，提高链接预测的准确性和稳健性。&lt;h4&gt;方法&lt;/h4&gt;提出了IVGAE-TAMA-BO，一种新型的动态图神经网络。基于原始IVGAE框架，引入了Trade-Aware Momentum Aggregator (TAMA)来捕捉贸易网络的时间演变，联合建模短期波动和长期结构依赖关系。使用基于动量的结构记忆机制提高预测的稳定性和性能，并使用贝叶斯优化自动调整关键超参数。&lt;h4&gt;主要发现&lt;/h4&gt;这是首次将动态图神经网络应用于粮食贸易网络领域，显著提高了预测性能。在五种作物特定数据集上的实验表明，IVGAE-TAMA明显优于静态IVGAE和其他动态基线，通过有效建模时间依赖性，贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的框架是全球贸易网络结构预测的稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;全球粮食贸易在确保粮食安全和维持供应链稳定方面发挥着关键作用。然而，其网络结构在政治、经济和环境因素的影响下动态演变，使得建模和预测未来的贸易链接具有挑战性。因此，有效捕捉粮食贸易网络中的时间模式对于提高链接预测的准确性和稳健性至关重要。本研究引入了IVGAE-TAMA-BO，一种专为模拟不断演变的贸易结构和预测全球粮食贸易网络中未来链接而设计的新型动态图神经网络。据我们所知，这是首次将动态图神经网络应用于该领域，显著提高了预测性能。基于原始IVGAE框架，所提出的模型纳入了一个贸易感知动量聚合器(TAMA)来捕捉贸易网络的时间演变，联合建模短期波动和长期结构依赖关系。基于动量的结构记忆机制进一步提高了预测的稳定性和性能。此外，使用贝叶斯优化来自动调整关键超参数，增强在不同贸易场景下的泛化能力。在五种作物特定数据集上的广泛实验表明，IVGAE-TAMA通过有效建模时间依赖性，明显优于静态IVGAE和其他动态基线，而贝叶斯优化进一步提升了IVGAE-TAMA-BO的性能。这些结果表明，所提出的框架是全球贸易网络结构预测的稳健且可扩展的解决方案，在粮食安全监测和政策决策支持方面具有强大的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global food trade plays a crucial role in ensuring food security andmaintaining supply chain stability. However, its network structure evolvesdynamically under the influence of geopolitical, economic, and environmentalfactors, making it challenging to model and predict future trade links.Effectively capturing temporal patterns in food trade networks is thereforeessential for improving the accuracy and robustness of link prediction. Thisstudy introduces IVGAE-TAMA-BO, a novel dynamic graph neural network designedto model evolving trade structures and predict future links in global foodtrade networks. To the best of our knowledge, this is the first work to applydynamic graph neural networks to this domain, significantly enhancingpredictive performance. Building upon the original IVGAE framework, theproposed model incorporates a Trade-Aware Momentum Aggregator (TAMA) to capturethe temporal evolution of trade networks, jointly modeling short-termfluctuations and long-term structural dependencies. A momentum-based structuralmemory mechanism further improves predictive stability and performance. Inaddition, Bayesian optimization is used to automatically tune keyhyperparameters, enhancing generalization across diverse trade scenarios.Extensive experiments on five crop-specific datasets demonstrate thatIVGAE-TAMA substantially outperforms the static IVGAE and other dynamicbaselines by effectively modeling temporal dependencies, while Bayesianoptimization further boosts performance in IVGAE-TAMA-BO. These resultshighlight the proposed framework as a robust and scalable solution forstructural prediction in global trade networks, with strong potential forapplications in food security monitoring and policy decision support.</description>
      <author>example@mail.com (Sicheng Wang, Shuhao Chen, Jingran Zhou, Chengyi Tu)</author>
      <guid isPermaLink="false">2511.01639v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Gated Fusion Enhanced Multi-Scale Hierarchical Graph Convolutional Network for Stock Movement Prediction</title>
      <link>http://arxiv.org/abs/2511.01570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MS-HGFN(多尺度分层图融合网络)的新型模型，用于解决股票市场预测中的挑战，通过分层GNN模块和自上而下的门控方法，有效捕捉股票间的复杂关系和多尺度特征，实验表明该模型在预测准确性和稳定性方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;准确预测股票市场走势由于股票固有的波动性和股票间复杂的相互依赖关系仍然是一个巨大的挑战。&lt;h4&gt;目的&lt;/h4&gt;克服现有多尺度图神经网络在股票预测中忽视的两个关键点：每个股票内部的细微属性模式以及多尺度采样中对粗粒度和细粒度特征的偏见注意力。&lt;h4&gt;方法&lt;/h4&gt;提出MS-HGFN模型，包含一个分层GNN模块，通过在不同时间尺度上学习内部属性模式和外部属性特征形成动态图，并采用自上而下的门控方法促进多尺度时空特征的融合。&lt;h4&gt;主要发现&lt;/h4&gt;使用美国和中国股票市场的真实数据集进行实验，MS-HGFN优于传统和先进模型，预测准确性提高了高达1.4%，在回报模拟中增强了稳定性。&lt;h4&gt;结论&lt;/h4&gt;MS-HGFN通过有效捕捉股票间的复杂关系和多尺度特征，显著提升了股票市场预测的准确性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;准确预测股票市场走势由于股票固有的波动性和股票间复杂的相互依赖关系仍然是一个巨大的挑战。虽然多尺度图神经网络在建模这些关系方面具有潜力，但它们经常忽视两个关键点：每个股票内部影响股票间相关性的细微的内部属性模式，以及在多尺度采样过程中对粗粒度和细粒度特征的偏见注意力。为了克服这些挑战，我们引入了MS-HGFN(多尺度分层图融合网络)。该模型具有一个分层GNN模块，通过在不同时间尺度上学习内部属性模式和外部属性特征来形成动态图，从而全面捕捉时空依赖关系。此外，自上而下的门控方法促进多尺度时空特征的整合，保留了关键的粗粒度和细粒度特征而不过多干扰。利用美国和中国股票市场的真实数据集进行的实验表明，MS-HGFN优于传统和先进模型，预测准确性提高了高达1.4%，并在回报模拟中增强了稳定性。代码可在https://anonymous.4open.science/r/MS-HGFN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting stock market movements remains a formidable challengedue to the inherent volatility and complex interdependencies among stocks.Although multi-scale Graph Neural Networks (GNNs) hold potential for modelingthese relationships, they frequently neglect two key points: the subtleintra-attribute patterns within each stock affecting inter-stock correlation,and the biased attention to coarse- and fine-grained features duringmulti-scale sampling. To overcome these challenges, we introduce MS-HGFN(Multi-Scale Hierarchical Graph Fusion Network). The model features ahierarchical GNN module that forms dynamic graphs by learning patterns fromintra-attributes and features from inter-attributes over different time scales,thus comprehensively capturing spatio-temporal dependencies. Additionally, atop-down gating approach facilitates the integration of multi-scalespatio-temporal features, preserving critical coarse- and fine-grained featureswithout too much interference. Experiments utilizing real-world datasets fromU.S. and Chinese stock markets demonstrate that MS-HGFN outperforms bothtraditional and advanced models, yielding up to a 1.4% improvement inprediction accuracy and enhanced stability in return simulations. The code isavailable at https://anonymous.4open.science/r/MS-HGFN.</description>
      <author>example@mail.com (Xiaosha Xue, Peibo Duan, Zhipeng Liu, Qi Chu, Changsheng Zhang, Bin zhang)</author>
      <guid isPermaLink="false">2511.01570v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Curvature-aware Graph Network</title>
      <link>http://arxiv.org/abs/2511.01443v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为有效电阻曲率的新型图曲率度量方法，解决了现有Ollivier-Ricci曲率计算复杂度高的问题，在保持相当几何表达能力的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;图曲率能为图神经网络提供几何先验，增强其建模复杂图结构的能力，特别是在结构感知、鲁棒性和理论可解释性方面。现有Ollivier-Ricci曲率虽具有强几何可解释性，但计算复杂度极高，限制了其在大型图数据集上的应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算效率更高且保持相当几何表达能力的图曲率度量方法，以替代计算复杂度高的Ollivier-Ricci曲率。&lt;h4&gt;方法&lt;/h4&gt;提出有效电阻曲率，使用节点对之间的有效电阻而非最优传输距离来量化沿图边的消息传递难易度。&lt;h4&gt;主要发现&lt;/h4&gt;有效电阻曲率显著优于Ollivier-Ricci曲率在计算效率方面，同时保持了相当的几何表达能力；理论证明了其低计算复杂度和对Ollivier-Ricci曲率的可替代性；实验表明其在多种GNN任务上实现了竞争性性能，同时大幅降低计算开销。&lt;h4&gt;结论&lt;/h4&gt;有效电阻曲率为图神经网络提供了一种高效且具有竞争力的几何先验方法，解决了Ollivier-Ricci曲率在大规模图数据集上的应用限制。&lt;h4&gt;翻译&lt;/h4&gt;图曲率为图神经网络(GNNs)提供几何先验，增强其建模复杂图结构的能力，特别是在结构感知、鲁棒性和理论可解释性方面。在现有方法中，Ollivier-Ricci曲率因其强几何可解释性而被广泛研究，能有效表征节点间的局部几何分布。然而，其极高的计算复杂度限制了其在大型图数据集上的适用性。为应对这一挑战，我们提出了一种新的图曲率度量方法——有效电阻曲率，它使用节点对之间的有效电阻而非最优传输距离来量化沿图边的消息传递难易度。该方法在计算效率上显著优于Ollivier-Ricci曲率，同时保持了相当的几何表达能力。理论上，我们证明了有效电阻曲率的低计算复杂度，并建立了其对Ollivier-Ricci曲率的可替代性。此外，在多种GNN任务上的广泛实验表明，我们的方法在大幅降低计算开销的同时，实现了与Ollivier-Ricci曲率相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph curvature provides geometric priors for Graph Neural Networks (GNNs),enhancing their ability to model complex graph structures, particularly interms of structural awareness, robustness, and theoretical interpretability.Among existing methods, Ollivier-Ricci curvature has been extensively studieddue to its strong geometric interpretability, effectively characterizing thelocal geometric distribution between nodes. However, its prohibitively highcomputational complexity limits its applicability to large-scale graphdatasets. To address this challenge, we propose a novel graph curvaturemeasure--Effective Resistance Curvature--which quantifies the ease of messagepassing along graph edges using the effective resistance between node pairs,instead of the optimal transport distance. This method significantlyoutperforms Ollivier-Ricci curvature in computational efficiency whilepreserving comparable geometric expressiveness. Theoretically, we prove the lowcomputational complexity of effective resistance curvature and establish itssubstitutability for Ollivier-Ricci curvature. Furthermore, extensiveexperiments on diverse GNN tasks demonstrate that our method achievescompetitive performance with Ollivier-Ricci curvature while drasticallyreducing computational overhead.</description>
      <author>example@mail.com (Chaoqun Fei, Tinglve Zhou, Tianyong Hao, Yangyang Li)</author>
      <guid isPermaLink="false">2511.01443v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based Solver for CNF Placement on the Cloud-Continuum</title>
      <link>http://arxiv.org/abs/2511.01343v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures. Presented at PE-WASUN'25 (IEEE International  Symposium on Performance Evaluation of Wireless Ad Hoc, Sensor, and  Ubiquitous Networks)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于去噪扩散概率模型(DDPM)的新理论框架，用于解决云原生网络功能(CNFs)在云连续体中的部署问题，实现了比传统方法更快且更有效的解决方案。&lt;h4&gt;背景&lt;/h4&gt;云原生网络功能(CNFs)在云连续体(Cloud-Continuum)中的部署是当前5G和未来6G网络编排的核心挑战。这个过程涉及将互依赖的计算任务(结构化为服务功能链)放置在分布式云基础设施上，同时满足严格的资源、带宽和延迟约束。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法(包括混合整数非线性规划、启发式和强化学习)在可扩展性、约束处理和泛化能力方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出基于去噪扩散概率模型(DDPM)的新理论框架，将部署问题重新概念化为生成图到分配任务，将部署问题编码为异构图，训练图神经网络去噪器迭代细化有噪的CNF到云分配矩阵，并将特定约束的损失直接整合到损失函数中。&lt;h4&gt;主要发现&lt;/h4&gt;在各种拓扑结构上进行了广泛评估，证实该模型始终产生可行解决方案，推理速度比MINLP求解器快几个数量级。&lt;h4&gt;结论&lt;/h4&gt;基于扩散的生成模型在受约束的网络嵌入问题中显示出潜力，对分布式云原生网络功能的实际、可扩展编排产生影响。&lt;h4&gt;翻译&lt;/h4&gt;将云原生网络功能(CNFs)跨云连续体(Cloud-Continuum)的部署代表当前5G和未来6G网络编排中的一个核心挑战。该过程涉及将互依赖的计算任务(结构化为服务功能链)放置在分布式云基础设施上，同时满足严格的资源、带宽和延迟约束。公认的是，包括混合整数非线性规划、启发式和强化学习在内的传统方法在可扩展性、约束处理和泛化能力方面存在局限性。在本研究中，提出了一种基于去噪扩散概率模型(DDPM)的新理论框架用于CNF部署。当前方法将重新概念化部署为生成图到分配任务，其中部署问题被编码为异构图，并且训练图神经网络去噪器来迭代细化有噪的CNF到云分配矩阵。该模型将特定约束的损失直接整合到损失函数中，从而使其能够学习可行的解空间。通过严谨和系统的方法实现了DDPM公式与结构组合约束的集成。已在各种拓扑结构上进行了广泛评估，证实该模型始终产生可行解决方案，推理速度比MINLP求解器快几个数量级。获得的结果证明了基于扩散的生成模型在受约束的网络嵌入问题中的潜力，对分布式云原生网络功能的实际、可扩展编排产生影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The placement of Cloud-Native Network Functions (CNFs) across theCloud-Continuum represents a core challenge in the orchestration of current 5Gand future 6G networks. The process involves the placement of interdependentcomputing tasks, structured as Service Function Chains, over distributed cloudinfrastructures. This is achieved while satisfying strict resource, bandwidthand latency constraints. It is acknowledged that classical approaches,including mixed-integer nonlinear programming, heuristics and reinforcementlearning are limited in terms of scalability, constraint handling andgeneralisation capacity. In the present study, a novel theoretical framework isproposed, which is based on Denoising Diffusion Probabilistic Models (DDPM) forCNF placement. The present approach proposes a reconceptualisation of placementas a generative graph to assignment task, where the placement problem isencoded as a heterogeneous graph, and a Graph Neural Network denoiser istrained to iteratively refine noisy CNF-to-cloud assignment matrices. The modelincorporates constraint-specific losses directly into the loss function,thereby allowing it to learn feasible solution spaces. The integration of theDDPM formulation with structured combinatorial constraints is achieved througha rigorous and systematic approach. Extensive evaluations across diversetopologies have been conducted, which have confirmed that the modelconsistently produces feasible solutions with orders of magnitude fasterinference than MINLP solvers. The results obtained demonstrate the potential ofdiffusion-based generative modelling for constrained network embeddingproblems, making an impact towards the practical, scalable orchestration ofdistributed Cloud-Native Network Functions.</description>
      <author>example@mail.com (Álvaro Vázquez Rodríguez, Manuel Fernández-Veiga, Carlos Giraldo-Rodríguez)</author>
      <guid isPermaLink="false">2511.01343v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network-Based Semi-Supervised Open-Set Fault Diagnosis for Marine Machinery Systems</title>
      <link>http://arxiv.org/abs/2511.01258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种半监督开集故障诊断(SOFD)框架，用于解决海洋机械系统中未知故障类型的检测问题，增强了深度学习模型在实际工业环境中的适用性。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的海洋机械系统故障诊断方法在航运业受到关注，但现有研究假设训练和测试数据集的故障类别一致且已知，在受控环境下表现良好。然而实际应用中可能出现训练期间未见的故障类型，导致现有方法失效。&lt;h4&gt;目的&lt;/h4&gt;解决未知故障类型导致现有故障诊断方法失效的问题，增强和扩展深度学习模型在开集故障诊断场景中的适用性。&lt;h4&gt;方法&lt;/h4&gt;提出半监督开集故障诊断(SOFD)框架，包含可靠性子集构建过程，使用监督特征学习模型提取的多层融合特征表示选择未标记测试子集，将标记训练集和伪标记测试子集输入半监督诊断模型，学习判别性特征以实现已知故障分类和未知样本检测。&lt;h4&gt;主要发现&lt;/h4&gt;在公共海事基准数据集上的实验结果证明所提出的SOFD框架具有有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;SOFD框架能够有效处理海洋机械系统中的开集故障诊断问题，在工业应用中具有潜力，可应对实际环境中出现的未知故障类型。&lt;h4&gt;翻译&lt;/h4&gt;最近，基于深度学习模型的海洋机械系统故障诊断方法在航运业引起了广泛关注。大多数现有研究假设训练和测试数据集中的故障类别是一致且已知的，这些方法在受控环境下表现良好。然而，在实践中，可能会出现先前未见或未知的故障类型（即训练期间不存在的分布外或开集观测），导致这些方法失效，并对它们在工业中的广泛部署构成重大挑战。为应对这一挑战，本文提出了一种半监督开集故障诊断(SOFD)框架，增强了深度学习模型在开集故障诊断场景中的适用性。该框架包括一个可靠性子集构建过程，使用监督特征学习模型提取的多层融合特征表示来选择未标记的测试子集。然后，将标记的训练集和伪标记的测试子集输入半监督诊断模型，学习每个类的判别性特征，实现对已知故障的准确分类和未知样本的有效检测。在公共海事基准数据集上的实验结果证明了所提出的SOFD框架的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, fault diagnosis methods for marine machinery systems based on deeplearning models have attracted considerable attention in the shipping industry.Most existing studies assume fault classes are consistent and known between thetraining and test datasets, and these methods perform well under controlledenvironment. In practice, however, previously unseen or unknown fault types(i.e., out-of-distribution or open-set observations not present duringtraining) can occur, causing such methods to fail and posing a significantchallenge to their widespread industrial deployment. To address this challenge,this paper proposes a semi-supervised open-set fault diagnosis (SOFD) frameworkthat enhances and extends the applicability of deep learning models in open-setfault diagnosis scenarios. The framework includes a reliability subsetconstruction process, which uses a multi-layer fusion feature representationextracted by a supervised feature learning model to select an unlabeled testsubset. The labeled training set and pseudo-labeled test subset are then fedinto a semi-supervised diagnosis model to learn discriminative features foreach class, enabling accurate classification of known faults and effectivedetection of unknown samples. Experimental results on a public maritimebenchmark dataset demonstrate the effectiveness and superiority of the proposedSOFD framework.</description>
      <author>example@mail.com (Chuyue Lou, M. Amine Atoui)</author>
      <guid isPermaLink="false">2511.01258v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>WindMiL: Equivariant Graph Learning for Wind Loading Prediction</title>
      <link>http://arxiv.org/abs/2511.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;WindMiL是一种新的机器学习框架，通过结合系统数据集生成与对称感知的图神经网络，实现了建筑物风荷载的高效、可扩展和准确预测，解决了传统方法成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;准确预测建筑物风荷载对结构安全和可持续设计至关重要，但传统方法如风洞测试和大涡模拟(LES)成本过高，每个LES案例需要至少24小时计算时间，使得全面参数研究不可行。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、准确预测建筑物风荷载的方法，降低计算成本，使大规模参数研究成为可能。&lt;h4&gt;方法&lt;/h4&gt;构建大规模风荷载数据集，通过符号距离函数插值处理屋面几何形状，模拟462个不同形状和风向的案例；开发反射等变性图神经网络，确保在镜像几何形状下的物理一致性预测。&lt;h4&gt;主要发现&lt;/h4&gt;WindMiL在插值和外推评估中取得高精度，表面压力系数的平均值和标准差的误差小于等于0.02；在反射测试评估中保持96%以上的命中率，比非等变性基线模型提高10%以上。&lt;h4&gt;结论&lt;/h4&gt;WindMiL通过将系统数据集与等变性代理模型配对，实现了建筑物风荷载的高效、可扩展和准确预测，为结构安全和可持续设计提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;准确预测建筑物上的风荷载对于结构安全和可持续设计至关重要，然而传统方法如风洞测试和大涡模拟(LES)对于大规模探索来说成本过高。每个LES案例通常需要至少24小时的计算时间，这使得全面的参数研究不可行。我们提出了WindMiL，这是一种新的机器学习框架，结合了系统数据集生成与对称感知的图神经网络(GNNs)。首先，我们通过对屋面几何形状应用符号距离函数插值，并在不同形状和风向条件下模拟462个案例，构建了一个关于低层建筑物风荷载的大规模数据集。其次，我们开发了一种反射等变性GNN，确保在镜像几何形状下物理预测的一致性。在插值和外推评估中，WindMiL在表面压力系数的平均值和标准差方面都取得了高精度，并且在反射测试评估中保持准确，命中率保持在96%以上，而非等变性基线模型的命中率下降了10%以上。通过将系统数据集与等变性代理模型配对，WindMiL能够实现建筑物风荷载的高效、可扩展和准确的预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate prediction of wind loading on buildings is crucial for structuralsafety and sustainable design, yet conventional approaches such as wind tunneltesting and large-eddy simulation (LES) are prohibitively expensive forlarge-scale exploration. Each LES case typically requires at least 24 hours ofcomputation, making comprehensive parametric studies infeasible. We introduceWindMiL, a new machine learning framework that combines systematic datasetgeneration with symmetry-aware graph neural networks (GNNs). First, weintroduce a large-scale dataset of wind loads on low-rise buildings by applyingsigned distance function interpolation to roof geometries and simulating 462cases with LES across varying shapes and wind directions. Second, we develop areflection-equivariant GNN that guarantees physically consistent predictionsunder mirrored geometries. Across interpolation and extrapolation evaluations,WindMiL achieves high accuracy for both the mean and the standard deviation ofsurface pressure coefficients (e.g., RMSE $\leq 0.02$ for mean $C_p$) andremains accurate under reflected-test evaluation, maintaining hit rates above$96\%$ where the non-equivariant baseline model drops by more than $10\%$. Bypairing a systematic dataset with an equivariant surrogate, WindMiL enablesefficient, scalable, and accurate predictions of wind loads on buildings.</description>
      <author>example@mail.com (Themistoklis Vargiemezis, Charilaos Kanatsoulis, Catherine Gorlé)</author>
      <guid isPermaLink="false">2511.01226v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>An Interdisciplinary and Cross-Task Review on Missing Data Imputation</title>
      <link>http://arxiv.org/abs/2511.01196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于缺失数据插补方法的综合性综述，连接了统计基础与现代机器学习进展，涵盖了从传统到现代的各种插补方法，特别关注复杂数据类型和与下游任务的集成，并提出了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;缺失数据是数据科学中的基本挑战，在医疗保健、生物信息学、社会科学、电子商务和工业监控等多个领域显著阻碍了分析和决策。尽管有数十年的研究和多种插补方法，但文献仍然分散在不同领域，缺乏将统计基础与现代机器学习进展联系起来的全面综合。&lt;h4&gt;目的&lt;/h4&gt;系统性地回顾缺失数据的核心概念，包括缺失机制、单次与多次插补以及不同的插补目标；检查各领域的问题特征；提供插补方法的全面分类；研究插补与下游任务的集成；评估理论保证、基准资源和评估指标；确定关键挑战和未来方向。&lt;h4&gt;方法&lt;/h4&gt;从经典技术（如回归、EM算法）到现代方法（如低秩和高秩矩阵补全、深度学习模型、大型语言模型）进行全面分类；特别关注复杂数据类型（张量、时间序列、流数据、图结构数据、分类数据和多模态数据）的插补方法；研究插补与下游任务（分类、聚类、异常检测）的集成方式，包括顺序管道和联合优化框架；评估理论保证、基准资源和评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;提供了从传统到现代的插补方法全面分类；特别关注了复杂数据类型的插补方法；探讨了插补与下游任务的集成方式；强调了模型选择和超参数优化的重要性；指出了隐私保护插补的日益重要性；提出了追求可泛化模型的方向。&lt;h4&gt;结论&lt;/h4&gt;确定了关键挑战和未来方向，包括模型选择和超参数优化的重要性；通过联邦学习进行隐私保护插补的日益重要性；追求能够跨领域和数据类型适应的可泛化模型；为未来研究勾勒出路线图。&lt;h4&gt;翻译&lt;/h4&gt;缺失数据是数据科学中的一个基本挑战，在医疗保健、生物信息学、社会科学、电子商务和工业监控等广泛领域显著阻碍了分析和决策。尽管有数十年的研究和众多的插补方法，但文献仍然分散在不同领域，迫切需要将统计基础与现代机器学习进展联系起来的全面综合。本工作系统性地回顾了核心概念，包括缺失机制、单次与多次插补以及不同的插补目标，并检查了各领域的问题特征。它提供了插补方法的全面分类，涵盖从经典技术（如回归、EM算法）到现代方法，如低秩和高秩矩阵补全、深度学习模型（自编码器、GAN、扩散模型、图神经网络）和大型语言模型。特别关注了复杂数据类型的方法，如张量、时间序列、流数据、图结构数据、分类数据和多模态数据。除了方法论，我们还研究了插补与下游任务（如分类、聚类和异常检测）的关键集成，检查了顺序管道和联合优化框架。该综述还评估了理论保证、基准资源和评估指标。最后，我们确定了关键挑战和未来方向，强调模型选择和超参数优化的重要性、通过联邦学习进行隐私保护插补的日益重要性，以及追求能够跨领域和数据类型适应的可泛化模型，从而为未来研究勾勒出路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Missing data is a fundamental challenge in data science, significantlyhindering analysis and decision-making across a wide range of disciplines,including healthcare, bioinformatics, social science, e-commerce, andindustrial monitoring. Despite decades of research and numerous imputationmethods, the literature remains fragmented across fields, creating a criticalneed for a comprehensive synthesis that connects statistical foundations withmodern machine learning advances. This work systematically reviews coreconcepts-including missingness mechanisms, single versus multiple imputation,and different imputation goals-and examines problem characteristics acrossvarious domains. It provides a thorough categorization of imputation methods,spanning classical techniques (e.g., regression, the EM algorithm) to modernapproaches like low-rank and high-rank matrix completion, deep learning models(autoencoders, GANs, diffusion models, graph neural networks), and largelanguage models. Special attention is given to methods for complex data types,such as tensors, time series, streaming data, graph-structured data,categorical data, and multimodal data. Beyond methodology, we investigate thecrucial integration of imputation with downstream tasks like classification,clustering, and anomaly detection, examining both sequential pipelines andjoint optimization frameworks. The review also assesses theoretical guarantees,benchmarking resources, and evaluation metrics. Finally, we identify criticalchallenges and future directions, emphasizing model selection andhyperparameter optimization, the growing importance of privacy-preservingimputation via federated learning, and the pursuit of generalizable models thatcan adapt across domains and data types, thereby outlining a roadmap for futureresearch.</description>
      <author>example@mail.com (Jicong Fan)</author>
      <guid isPermaLink="false">2511.01196v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2511.00908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GraphGeo，一种基于异构图神经网络的多代理辩论框架，用于视觉地理定位任务，通过结构化辩论将代理间的认知冲突转化为增强的定位精度。&lt;h4&gt;背景&lt;/h4&gt;视觉地理定位需要广泛地理知识和复杂推理来确定无GPS元数据的图像位置；传统检索方法受限于数据库覆盖和质量；大型视觉-语言模型虽能直接从图像内容推理位置，但单个模型难以处理多样化地理区域和复杂场景；现有多代理系统统一处理所有代理交互，缺乏有效处理冲突预测的机制。&lt;h4&gt;目的&lt;/h4&gt;提出GraphGeo框架，利用异构图神经网络和结构化辩论机制提高视觉地理定位的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用类型化边建模多样化辩论关系，区分支持性协作、竞争性论证和知识转移；引入双重辩论机制，结合节点级细化和边级论证建模；采用跨层拓扑细化策略，实现图结构与代理表示的共同演化。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准测试中，GraphGeo显著优于最先进的方法，有效将代理间的认知冲突转化为增强的地理定位精度。&lt;h4&gt;结论&lt;/h4&gt;通过结构化辩论机制，GraphGeo能够有效处理代理间的认知冲突，显著提升视觉地理定位的性能，为多代理系统在复杂推理任务中的应用提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;视觉地理定位需要广泛的地理知识和复杂的推理来确定没有GPS元数据的图像位置。传统检索方法受限于数据库覆盖范围和质量。最近的大型视觉-语言模型(LVLMs)能够直接从图像内容进行位置推理，但单个模型难以处理多样化的地理区域和复杂场景。现有的多代理系统通过模型协作提高性能，但统一处理所有代理交互，缺乏有效处理冲突预测的机制。我们提出GraphGeo，一个使用异构图神经网络进行视觉地理定位的多代理辩论框架。我们的方法通过类型化边建模多样化的辩论关系，区分支持性协作、竞争性论证和知识转移。我们引入了双重辩论机制，结合节点级细化和边级论证建模。跨层拓扑细化策略实现了图结构与代理表示的共同演化。在多个基准测试中的实验表明，GraphGeo显著优于最先进的方法。我们的框架通过结构化辩论将代理之间的认知冲突转化为增强的地理定位精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual geo-localization requires extensive geographic knowledge andsophisticated reasoning to determine image locations without GPS metadata.Traditional retrieval methods are constrained by database coverage and quality.Recent Large Vision-Language Models (LVLMs) enable direct location reasoningfrom image content, yet individual models struggle with diverse geographicregions and complex scenes. Existing multi-agent systems improve performancethrough model collaboration but treat all agent interactions uniformly. Theylack mechanisms to handle conflicting predictions effectively. We propose\textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graphneural networks for visual geo-localization. Our approach models diverse debaterelationships through typed edges, distinguishing supportive collaboration,competitive argumentation, and knowledge transfer. We introduce a dual-leveldebate mechanism combining node-level refinement and edge-level argumentationmodeling. A cross-level topology refinement strategy enables co-evolutionbetween graph structure and agent representations. Experiments on multiplebenchmarks demonstrate GraphGeo significantly outperforms state-of-the-artmethods. Our framework transforms cognitive conflicts between agents intoenhanced geo-localization accuracy through structured debate.</description>
      <author>example@mail.com (Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang)</author>
      <guid isPermaLink="false">2511.00908v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>IL-PCSR: Legal Corpus for Prior Case and Statute Retrieval</title>
      <link>http://arxiv.org/abs/2511.00268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at EMNLP 2025 (Main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了IL-PCR语料库，用于法规检索和先例检索两项任务的共同开发，并利用LLM重新排序方法取得了最佳性能。&lt;h4&gt;背景&lt;/h4&gt;法律从业者经常需要识别和检索相关法规和先例案例，但研究人员至今独立处理这两项任务，忽略了它们之间的内在联系。&lt;h4&gt;目的&lt;/h4&gt;解决法规检索和先例检索任务之间的研究缺口，开发可以利用两项任务依赖关系的模型。&lt;h4&gt;方法&lt;/h4&gt;提出IL-PCR语料库作为共同测试平台，使用词汇模型、语义模型和基于GNN的集成模型进行实验，并开发基于LLM的重新排序方法。&lt;h4&gt;主要发现&lt;/h4&gt;基于LLM的重新排序方法能够有效利用法规检索和先例检索任务之间的依赖关系，取得了最佳性能。&lt;h4&gt;结论&lt;/h4&gt;通过IL-PCR语料库和基于LLM的重新排序方法，可以更好地同时处理法规检索和先例检索任务，提高法律检索的效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;识别/检索给定法律情况下的相关法规和先例/判例是法律从业者常见的任务。迄今为止，研究人员独立处理这两项任务，为每个任务开发了完全不同的数据集和模型；然而，这两项检索任务本质上是相关的，例如，相似的案例往往会引用相似的法规（由于相似的事实情况）。在本文中，我们解决了这一研究缺口。我们提出了IL-PCR（印度法律先例和法规检索语料库），这是一个独特的语料库，为开发法规检索和先例检索两项任务的模型提供了共同测试平台，可以利用两项任务之间的依赖关系。我们使用多种基线模型对这两项任务进行了广泛实验，包括词汇模型、语义模型和基于GNN的集成模型。此外，为了利用两项任务之间的依赖关系，我们开发了一种基于LLM的重新排序方法，取得了最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying/retrieving relevant statutes and prior cases/precedents for agiven legal situation are common tasks exercised by law practitioners.Researchers to date have addressed the two tasks independently, thus developingcompletely different datasets and models for each task; however, both retrievaltasks are inherently related, e.g., similar cases tend to cite similar statutes(due to similar factual situation). In this paper, we address this gap. Wepropose IL-PCR (Indian Legal corpus for Prior Case and Statute Retrieval),which is a unique corpus that provides a common testbed for developing modelsfor both the tasks (Statute Retrieval and Precedent Retrieval) that can exploitthe dependence between the two. We experiment extensively with several baselinemodels on the tasks, including lexical models, semantic models and ensemblebased on GNNs. Further, to exploit the dependence between the two tasks, wedevelop an LLM-based re-ranking approach that gives the best performance.</description>
      <author>example@mail.com (Shounak Paul, Dhananjay Ghumare, Pawan Goyal, Saptarshi Ghosh, Ashutosh Modi)</author>
      <guid isPermaLink="false">2511.00268v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>MeixnerNet: Adaptive and Robust Spectral Graph Neural Networks with Discrete Orthogonal Polynomials</title>
      <link>http://arxiv.org/abs/2511.00113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MeixnerNet是一种新的谱图神经网络架构，使用离散正交多项式而非连续正交多项式，解决了连续滤波器应用于离散图结构的理论不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;Spectral GNNs通过在谱域定义图卷积实现了最先进的结果，而ChebyNet使用基于连续正交多项式的滤波器是一种常见方法。&lt;h4&gt;目的&lt;/h4&gt;解决连续域滤波器应用于离散图结构造成的不匹配问题，提高模型性能并增强对超参数设置的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;引入MeixnerNet，使用离散Meixner多项式作为滤波器基础，使多项式参数可学习，并通过结合Laplacian缩放和LayerNorm的技术解决数值不稳定性问题。&lt;h4&gt;主要发现&lt;/h4&gt;在最佳设置下，MeixnerNet在三个基准测试中胜过ChebyNet两个；更重要的是，MeixnerNet对多项式次数的变化异常稳健，而ChebyNet对此超参数非常脆弱。&lt;h4&gt;结论&lt;/h4&gt;使用离散正交多项式而非连续正交多项式可以解决理论不匹配问题，提高模型性能和对超参数的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;谱图神经网络通过在谱域定义图卷积取得了最先进的结果。ChebyNet推广的一种常见方法是使用基于连续正交多项式（如Chebyshev）的多项式滤波器。这造成了理论上的脱节，因为这些连续域滤波器被应用于本质上离散的图结构。我们假设这种不匹配可能导致次优性能和对超参数设置的脆弱性。在本文中，我们介绍了MeixnerNet，一种新的谱GNN架构，它采用离散正交多项式——特别是Meixner多项式。我们的模型使多项式的两个关键形状参数可学习，允许滤波器根据给定图的特定谱属性调整其多项式基。我们通过引入一种结合Laplacian缩放和每个基的LayerNorm的新稳定技术，克服了这些多项式显著的数值不稳定性。实验证明，在最佳设置下，MeixnerNet与强大的ChebyNet基线相比具有竞争性到优越的性能（在3个基准测试中赢得2个）。更重要的是，我们表明MeixnerNet对多项式次数的变化异常稳健，而ChebyNet对此超参数证明是非常脆弱的，在性能崩溃的地方MeixnerNet保持稳定。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral Graph Neural Networks (GNNs) have achieved state-of-the-art resultsby defining graph convolutions in the spectral domain. A common approach,popularized by ChebyNet, is to use polynomial filters based on continuousorthogonal polynomials (e.g., Chebyshev). This creates a theoreticaldisconnect, as these continuous-domain filters are applied to inherentlydiscrete graph structures. We hypothesize this mismatch can lead to suboptimalperformance and fragility to hyperparameter settings.  In this paper, we introduce MeixnerNet, a novel spectral GNN architecturethat employs discrete orthogonal polynomials -- specifically, the Meixnerpolynomials $M_k(x; \beta, c)$. Our model makes the two key shape parameters ofthe polynomial, beta and c, learnable, allowing the filter to adapt itspolynomial basis to the specific spectral properties of a given graph. Weovercome the significant numerical instability of these polynomials byintroducing a novel stabilization technique that combines Laplacian scalingwith per-basis LayerNorm.  We demonstrate experimentally that MeixnerNet achievescompetitive-to-superior performance against the strong ChebyNet baseline at theoptimal K = 2 setting (winning on 2 out of 3 benchmarks). More critically, weshow that MeixnerNet is exceptionally robust to variations in the polynomialdegree K, a hyperparameter to which ChebyNet proves to be highly fragile,collapsing in performance where MeixnerNet remains stable.</description>
      <author>example@mail.com (Huseyin Goksu)</author>
      <guid isPermaLink="false">2511.00113v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Agent-Omni: Test-Time Multimodal Reasoning via Model Coordination for Understanding Anything</title>
      <link>http://arxiv.org/abs/2511.02834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 7 figures, 14 tables. Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Agent-Omni框架通过主代理系统协调现有基础模型，实现了无需重新训练的灵活多模态推理，在多种模态和全能任务上取得了最先进性能，特别是复杂跨模态推理任务，且具有模块化、可扩展和透明的特点。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型（MLLMs）已展现出强大的能力，但仍局限于固定的模态对，并且需要使用大型对齐数据集进行昂贵的微调。构建能够完全整合文本、图像、音频和视频的全能模型仍然不切实际，且缺乏强大的推理支持。&lt;h4&gt;目的&lt;/h4&gt;提出一个Agent-Omni框架，通过主代理系统协调现有基础模型，实现灵活的多模态推理，无需重新训练。&lt;h4&gt;方法&lt;/h4&gt;主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。&lt;h4&gt;主要发现&lt;/h4&gt;在文本、图像、音频、视频和全能基准上的广泛实验表明，Agent-Omni始终取得了最先进的性能，特别是在需要复杂跨模态推理的任务上。其基于代理的设计实现了专业基础模型的无缝集成，确保了对多样化输入的适应性，同时保持透明性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;该框架是模块化的且易于扩展，允许随着更强大模型的可用而进行未来改进。作者发布了开源实现，以支持对可扩展和可靠的全模态推理的持续研究。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型（MLLMs）已展现出强大的能力，但仍局限于固定的模态对，并且需要使用大型对齐数据集进行昂贵的微调。构建能够完全整合文本、图像、音频和视频的全能模型仍然不切实际，且缺乏强大的推理支持。在本文中，我们提出了一个Agent-Omni框架，通过主代理系统协调现有基础模型，实现了无需重新训练的灵活多模态推理。主代理解释用户意图，将子任务委托给特定模态的代理，并将它们的输出整合成连贯的响应。在文本、图像、音频、视频和全能基准上的广泛实验表明，Agent-Omni始终取得了最先进的性能，特别是在需要复杂跨模态推理的任务上。其基于代理的设计实现了专业基础模型的无缝集成，确保了对多样化输入的适应性，同时保持透明性和可解释性。此外，该框架是模块化的且易于扩展，允许随着更强大模型的可用而进行未来改进。我们发布了开源实现，以支持对可扩展和可靠的全模态推理的持续研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have shown strong capabilities butremain limited to fixed modality pairs and require costly fine-tuning withlarge aligned datasets. Building fully omni-capable models that can integratetext, images, audio, and video remains impractical and lacks robust reasoningsupport. In this paper, we propose an Agent-Omni framework that coordinatesexisting foundation models through a master-agent system, enabling flexiblemultimodal reasoning without retraining. The master agent interprets userintent, delegates subtasks to modality-specific agents, and integrates theiroutputs into coherent responses. Extensive experiments across text, image,audio, video, and omni benchmarks show that Agent-Omni consistently achievesstate-of-the-art performance, particularly on tasks requiring complexcross-modal reasoning. Its agent-based design enables seamless integration ofspecialized foundation models, ensuring adaptability to diverse inputs whilemaintaining transparency and interpretability. In addition, the framework ismodular and easily extensible, allowing future improvements as stronger modelsbecome available. %We release an open-source implementation to supportcontinued research on scalable and reliable omni-modal reasoning.</description>
      <author>example@mail.com (Huawei Lin, Yunzhi Shi, Tong Geng, Weijie Zhao, Wei Wang, Ravender Pal Singh)</author>
      <guid isPermaLink="false">2511.02834v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>GeoCrossBench: Cross-Band Generalization for Remote Sensing</title>
      <link>http://arxiv.org/abs/2511.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GeoCrossBench基准测试，用于评估遥感基础模型对新卫星的泛化能力，并开发了ChiViT模型来改善跨卫星性能。研究发现现有模型在分布内表现不佳，对无波段重叠卫星的泛化能力显著下降，但对额外波段有一定适应能力。仅微调最后一层线性层即可获得一致性能，表明该基准远未饱和。&lt;h4&gt;背景&lt;/h4&gt;遥感卫星的数量和多样性随时间增长，而大多数标记数据来自较老的卫星。随着地球观测基础模型的扩展，支持新卫星的训练成本增加，模型对新卫星的泛化能力变得尤为重要。&lt;h4&gt;目的&lt;/h4&gt;介绍GeoCrossBench基准测试，扩展流行的GeoBench基准，并开发新的评估协议来测试模型对无波段重叠卫星和具有额外波段卫星的泛化能力。同时开发ChiViT模型以改善跨卫星性能。&lt;h4&gt;方法&lt;/h4&gt;创建GeoCrossBench基准测试，开发ChannelViT的自监督扩展ChiViT，并进行多项实验：评估分布内性能、评估对无波段重叠卫星的泛化能力、评估对具有额外波段卫星的泛化能力，以及测试仅微调最后一层线性层的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1) 即使最好的遥感基础模型在分布内设置下也不如通用模型如DINOv3；2) 当泛化到无波段重叠的新卫星时，所有模型性能下降2-4倍，ChiViT显著优于第二名DINOv3；3) 当测试时提供额外波段，所有测试模型性能平均下降5-25%；4) 仅使用所有波段标签微调最后一层线性层，可在所有卫星上获得相对一致的性能。&lt;h4&gt;结论&lt;/h4&gt;该基准测试远未达到饱和。作者公开发布了代码和数据集，以鼓励开发具有更强跨卫星泛化能力的更面向未来的遥感模型。&lt;h4&gt;翻译&lt;/h4&gt;随着遥感卫星的数量和多样性随时间增长，而绝大多数标记数据来自较老的卫星。随着地球观测基础模型的扩展，支持新卫星的(重新)训练成本也在增加，因此模型对新卫星的泛化能力变得越来越重要。在这项工作中，我们介绍了GeoCrossBench，这是流行的GeoBench基准的扩展，具有新的评估协议：它测试分布内性能；对无波段重叠卫星的泛化能力；以及对具有训练集之外额外波段卫星的泛化能力。我们还开发了ChannelViT的自监督扩展ChiViT，以改善其跨卫星性能。首先，我们表明即使最好的遥感基础模型(DOFA, TerraFM)在分布内设置下也不如通用模型如DINOv3。其次，当泛化到无波段重叠的新卫星时，所有模型性能下降2-4倍，而ChiViT显著优于第二名DINOv3。第三，当测试时提供额外波段，所有测试模型的性能平均下降5-25%。最后，我们表明仅使用所有波段标签微调这些模型的最后一层线性层，可以在所有卫星上获得相对一致的性能，这突显出该基准远未饱和。我们公开发布了代码和数据集，以鼓励开发具有更强跨卫星泛化能力的更面向未来的遥感模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The number and diversity of remote sensing satellites grows over time, whilethe vast majority of labeled data comes from older satellites. As thefoundation models for Earth observation scale up, the cost of (re-)training tosupport new satellites grows too, so the generalization capabilities of themodels towards new satellites become increasingly important. In this work weintroduce GeoCrossBench, an extension of the popular GeoBench benchmark with anew evaluation protocol: it tests the in-distribution performance;generalization to satellites with no band overlap; and generalization tosatellites with additional bands with respect to the training set. We alsodevelop a self-supervised extension of ChannelViT, ChiViT, to improve itscross-satellite performance. First, we show that even the best foundationmodels for remote sensing (DOFA, TerraFM) do not outperform general purposemodels like DINOv3 in the in-distribution setting. Second, when generalizing tonew satellites with no band overlap, all models suffer 2-4x drop inperformance, and ChiViT significantly outperforms the runner-up DINOv3. Third,the performance of all tested models drops on average by 5-25\% when givenadditional bands during test time. Finally, we show that fine-tuning just thelast linear layer of these models using oracle labels from all bands can getrelatively consistent performance across all satellites, highlighting that thebenchmark is far from being saturated. We publicly release the code and thedatasets to encourage the development of more future-proof remote sensingmodels with stronger cross-satellite generalization.</description>
      <author>example@mail.com (Hakob Tamazyan, Ani Vanyan, Alvard Barseghyan, Anna Khosrovyan, Evan Shelhamer, Hrant Khachatrian)</author>
      <guid isPermaLink="false">2511.02831v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>PLUTO-4: Frontier Pathology Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究人员介绍了PLUTO-4，这是新一代病理基础模型，扩展了Pathology-Universal Transformer到前沿规模。该模型家族包含两种互补的Vision Transformer架构：PLUTO-4S（紧凑高效，适合多规模部署）和PLUTO-4G（前沿规模，最大化表示能力）。模型在大型多机构数据集上预训练，并在多种病理学任务上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型在大型病理图像语料库上的训练显示出在多种组织病理学任务中的强大迁移能力。&lt;h4&gt;目的&lt;/h4&gt;开发下一代病理基础模型PLUTO-4，扩展Pathology-Universal Transformer到前沿规模，提升病理图像分析能力。&lt;h4&gt;方法&lt;/h4&gt;提供两种互补的Vision Transformer架构：PLUTO-4S（使用FlexiViT设置和2D-RoPE嵌入）和PLUTO-4G（使用单一补丁大小训练）。模型使用基于DINOv2的自监督目标进行预训练，训练数据包含来自137,144名患者的551,164例WSI，跨越50个机构，覆盖60多种疾病类型和100多种染色方法。&lt;h4&gt;主要发现&lt;/h4&gt;PLUTO-4在需要不同空间和生物学背景的任务上达到最先进性能，包括补丁级分类、分割和幻灯片级诊断。PLUTO-4S提供高吞吐量和稳健性能，适合实际部署；PLUTO-4G在多个病理学基准上建立新性能前沿，在皮肤病理学诊断中提高11%性能。&lt;h4&gt;结论&lt;/h4&gt;PLUTO-4的多样化改进凸显了其作为转化研究和诊断用例骨干的潜力，能够改变现实世界应用。&lt;h4&gt;翻译&lt;/h4&gt;在大型病理图像语料库上训练的基础模型已显示出在多种组织病理学任务中的强大迁移能力。基于这一进展，我们介绍了PLUTO-4，我们的下一代病理基础模型，将病理学通用转换器(PLUTO)扩展到前沿规模。我们在PLUTO-4家族中分享了两种互补的Vision Transformer架构：一个紧凑高效的PLUTO-4S模型，使用带有2D-RoPE嵌入的FlexiViT设置进行优化，适用于多规模部署；以及一个前沿规模的PLUTO-4G模型，使用单一补丁大小训练以最大化表示能力和稳定性。两个模型都使用从DINOv2衍生的自监督目标进行预训练，在包含来自137,144名患者的551,164例WSI的大型多机构语料库上训练，跨越50个机构，涵盖60多种疾病类型和100多种染色方法。在公共和内部基准上的全面评估表明，PLUTO-4在需要不同空间和生物学背景的任务上实现了最先进的性能，包括补丁级分类、分割和幻灯片级诊断。紧凑的PLUTO-4S为实际部署提供高吞吐量和稳健性能，而PLUTO-4G在多个病理学基准上建立了新的性能前沿，包括在皮肤病理学诊断中提高11%的性能。这些多样化的改进强调了PLUTO-4作为转化研究和诊断用例骨干的潜力，可以改变现实世界应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models trained on large-scale pathology image corpora havedemonstrated strong transfer capabilities across diverse histopathology tasks.Building on this progress, we introduce PLUTO-4, our next generation ofpathology foundation models that extend the Pathology-Universal Transformer(PLUTO) to frontier scale. We share two complementary Vision Transformerarchitectures in the PLUTO-4 family: a compact and efficient PLUTO-4S modeloptimized for multi-scale deployment using a FlexiViT setup with 2D-RoPEembeddings, and a frontier-scale PLUTO-4G model trained with a single patchsize to maximize representation capacity and stability. Both models arepretrained using a self-supervised objective derived from DINOv2 on a largemulti-institutional corpus containing 551,164 WSIs from 137,144 patients acrossover 50 institutions, spanning over 60 disease types and over 100 stains.Comprehensive evaluation across public and internal benchmarks demonstratesthat PLUTO-4 achieves state-of-the-art performance on tasks requiring varyingspatial and biological context, including patch-level classification,segmentation, and slide-level diagnosis. The compact PLUTO-4S provideshigh-throughput and robust performance for practical deployment, while PLUTO-4Gestablishes new performance frontiers across multiple pathology benchmarks,including an 11% improvement in dermatopathology diagnosis. These diverseimprovements underscore PLUTO-4's potential to transform real-worldapplications as a backbone for translational research and diagnostic use cases.</description>
      <author>example@mail.com (Harshith Padigela, Shima Nofallah, Atchuth Naveen Chilaparasetti, Ryun Han, Andrew Walker, Judy Shen, Chintan Shah, Blake Martin, Aashish Sood, Elliot Miller, Ben Glass, Andy Beck, Harsha Pokkalla, Syed Ashar Javed)</author>
      <guid isPermaLink="false">2511.02826v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>TabTune: A Unified Library for Inference and Fine-Tuning Tabular Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TabTune是一个统一的开源库，通过单一接口标准化了表格基础模型的完整工作流程，解决了当前表格基础模型采用受限的问题。&lt;h4&gt;背景&lt;/h4&gt;表格基础模型在结构化数据学习中是一个不断发展的范式，但由于异构预处理管道、碎片化API、不一致的微调程序以及缺乏针对部署导向指标的标准化评估，其采用仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出TabTune库，标准化表格基础模型的完整工作流程，提供一致访问和评估能力。&lt;h4&gt;方法&lt;/h4&gt;TabTune提供对七种最先进模型的一致访问，支持零样本推理、元学习、监督微调(SFT)和参数高效微调(PEFT)等多种适应策略，自动化模型感知预处理，管理架构异构性，并集成性能、校准和公平性评估模块。&lt;h4&gt;主要发现&lt;/h4&gt;TabTune框架支持一致的基准测试，表格基础模型可以通过多种适应策略进行有效应用。&lt;h4&gt;结论&lt;/h4&gt;TabTune库为表格基础模型的可扩展性和可重复性提供了解决方案，使研究人员和实践者能够一致地评估和部署这些模型。&lt;h4&gt;翻译&lt;/h4&gt;表格基础模型代表了结构化数据学习中不断增长的范式，将大规模预训练的好处扩展到表格领域。然而，由于异构预处理管道、碎片化的API、不一致的微调程序以及缺乏针对部署导向指标(如校准和公平性)的标准化评估，其采用仍然有限。我们提出了TabTune，一个通过单一接口标准化表格基础模型完整工作流程的统一库。TabTune提供对七种最先进模型的一致访问，支持多种适应策略，包括零样本推理、元学习、监督微调(SFT)和参数高效微调(PEFT)。该框架自动化模型感知的预处理，在内部管理架构异构性，并集成了用于性能、校准和公平性的评估模块。TabTune是为可扩展性和可重复性而设计的，它能够对表格基础模型的适应策略进行一致的基准测试。该库是开源的，可在https://github.com/Lexsi-Labs/TabTune获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular foundation models represent a growing paradigm in structured datalearning, extending the benefits of large-scale pretraining to tabular domains.However, their adoption remains limited due to heterogeneous preprocessingpipelines, fragmented APIs, inconsistent fine-tuning procedures, and theabsence of standardized evaluation for deployment-oriented metrics such ascalibration and fairness. We present TabTune, a unified library thatstandardizes the complete workflow for tabular foundation models through asingle interface. TabTune provides consistent access to seven state-of-the-artmodels supporting multiple adaptation strategies, including zero-shotinference, meta-learning, supervised fine-tuning (SFT), and parameter-efficientfine-tuning (PEFT). The framework automates model-aware preprocessing, managesarchitectural heterogeneity internally, and integrates evaluation modules forperformance, calibration, and fairness. Designed for extensibility andreproducibility, TabTune enables consistent benchmarking of adaptationstrategies of tabular foundation models. The library is open source andavailable at https://github.com/Lexsi-Labs/TabTune .</description>
      <author>example@mail.com (Aditya Tanna, Pratinav Seth, Mohamed Bouadi, Utsav Avaiya, Vinay Kumar Sankarapu)</author>
      <guid isPermaLink="false">2511.02802v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>When One Modality Sabotages the Others: A Diagnostic Lens on Multimodal Reasoning</title>
      <link>http://arxiv.org/abs/2511.02794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Multimodal Algorithmic Reasoning (MAR) Workshop,  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种诊断多模态大语言模型推理过程的新方法，通过'模态破坏'概念分析模态间的交互关系，并开发了一种轻量级评估框架来识别贡献模态和破坏模态。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)虽快速发展，但其推理过程仍不透明：不清楚哪个模态驱动预测，冲突如何解决，或何时一个信息流占主导地位。&lt;h4&gt;目的&lt;/h4&gt;分析模态间动态交互关系，特别是高置信度单模态错误如何覆盖其他证据并误导融合结果，为多模态推理提供诊断支架。&lt;h4&gt;方法&lt;/h4&gt;提出轻量级、模型无关的评估层，将每个模态视为独立代理产生候选标签和自我评估，通过简单融合机制聚合输出，区分贡献者(支持正确结果的模态)和破坏者(误导的模态)。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态情感识别基准案例研究中应用该方法，揭示了基础模型的系统性可靠性特征，帮助区分失败源于数据集伪影还是模型内在限制。&lt;h4&gt;结论&lt;/h4&gt;该框架为多模态推理提供诊断支架，支持对融合动力学的原则性审计，为可能的模型改进干预措施提供指导。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型(MLLMs)迅速发展，它们的推理过程仍然不透明：通常不清楚哪个模态驱动预测，冲突如何解决，或者何时一个信息流占主导地位。在本文中，我们引入了模态破坏，这是一种诊断性失效模式，其中高置信度的单模态错误会覆盖其他证据并误导融合结果。为了分析此类动态过程，我们提出了一种轻量级、模型无关的评估层，将每个模态视为一个代理，产生候选标签和用于审计的简短自我评估。一个简单的融合机制聚合这些输出，暴露贡献者(支持正确结果的模态)和破坏者(误导的模态)。在基础模型的多模态情感识别基准案例研究中应用我们的诊断层，揭示了系统性的可靠性特征，提供了关于失败可能源于数据集伪影还是模型限制的见解。更广泛地说，我们的框架为多模态推理提供了诊断支架，支持对融合动力学的原则性审计，并为可能的干预措施提供信息。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite rapid growth in multimodal large language models (MLLMs), theirreasoning traces remain opaque: it is often unclear which modality drives aprediction, how conflicts are resolved, or when one stream dominates. In thispaper, we introduce modality sabotage, a diagnostic failure mode in which ahigh-confidence unimodal error overrides other evidence and misleads the fusedresult. To analyze such dynamics, we propose a lightweight, model-agnosticevaluation layer that treats each modality as an agent, producing candidatelabels and a brief self-assessment used for auditing. A simple fusion mechanismaggregates these outputs, exposing contributors (modalities supporting correctoutcomes) and saboteurs (modalities that mislead). Applying our diagnosticlayer in a case study on multimodal emotion recognition benchmarks withfoundation models revealed systematic reliability profiles, providing insightinto whether failures may arise from dataset artifacts or model limitations.More broadly, our framework offers a diagnostic scaffold for multimodalreasoning, supporting principled auditing of fusion dynamics and informingpossible interventions.</description>
      <author>example@mail.com (Chenyu Zhang, Minsol Kim, Shohreh Ghorbani, Jingyao Wu, Rosalind Picard, Patricia Maes, Paul Pu Liang)</author>
      <guid isPermaLink="false">2511.02794v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>VidEmo: Affective-Tree Reasoning for Emotion-Centric Video Foundation Models</title>
      <link>http://arxiv.org/abs/2511.02712v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages, 26 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新颖的情感线索引导推理框架和视频情感基础模型(VidEmo)，解决了视频中情感理解和预测的挑战，通过建立Emo-CFG数据集和两阶段调优方法，在15个面部感知任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;理解和预测视频中的情绪在近期研究中引起了广泛关注，得益于视频大型语言模型(VideoLLMs)的进步。尽管先进方法在视频情绪分析方面取得了进展，但情绪的本质特性（动态和线索依赖）仍带来重大挑战，使得难以以合理的理由理解复杂且不断变化的情绪状态。&lt;h4&gt;目的&lt;/h4&gt;提出一种情感线索引导推理框架，统一基本属性感知、表达分析和高级情感理解；设计专门用于情感推理和指令跟随的视频情感基础模型(VidEmo)；建立情感理解任务的基础数据基础设施。&lt;h4&gt;方法&lt;/h4&gt;提出情感线索引导推理框架，分阶段统一基本属性感知、表达分析和高级情感理解；开发视频情感基础模型(VidEmo)家族；采用两阶段调优过程：课程情感学习注入情感知识，情感树强化学习进行情感推理；建立包含210万多样化指令样本的情感细粒度数据集(Emo-CFG)，包括可解释的情感问答、细粒度字幕和相关理由。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在15个面部感知任务中取得了具有竞争力的性能，树立了新的里程碑。Emo-CFG数据集为情感理解任务提供了必要资源，包括可解释的情感问答、细粒度字幕和相关理由。&lt;h4&gt;结论&lt;/h4&gt;通过统一的框架和专门设计的模型，有效解决了视频中情感理解和预测的挑战。建立的数据基础设施和数据集为情感理解任务提供了重要资源，推动了该领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解和预测视频中的情绪在近期研究中引起了广泛关注，这得益于视频大型语言模型(VideoLLMs)的进步。尽管先进方法在视频情绪分析方面取得了进展，但情绪的本质特性仍带来重大挑战。情绪具有动态和线索依赖的特性，使得难以以合理的理由理解复杂且不断变化的情绪状态。为应对这些挑战，我们提出了一种新颖的情感线索引导推理框架，以分阶段的方式统一基本属性感知、表达分析和高级情感理解。我们方法的核心是一系列专为情感推理和指令跟随而设计的视频情感基础模型(VidEmo)。这些模型经历两阶段调优过程：首先进行课程情感学习以注入情感知识，然后进行情感树强化学习以进行情感推理。此外，我们建立了基础数据基础设施，并引入了一个以情感为中心的细粒度数据集(Emo-CFG)，包含210万多样化的基于指令的样本。Emo-CFG包括可解释的情感问答、细粒度字幕和相关理由，为推进情感理解任务提供了必要资源。实验结果表明，我们的方法取得了具有竞争力的性能，在15个面部感知任务中树立了新的里程碑。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and predicting emotion from videos has gathered significantattention in recent studies, driven by advancements in video large languagemodels (VideoLLMs). While advanced methods have made progress in video emotionanalysis, the intrinsic nature of emotions poses significant challenges.Emotions are characterized by dynamic and cues-dependent properties, making itdifficult to understand complex and evolving emotional states with reasonablerationale. To tackle these challenges, we propose a novel affective cues-guidedreasoning framework that unifies fundamental attribute perception, expressionanalysis, and high-level emotional understanding in a stage-wise manner. At thecore of our approach is a family of video emotion foundation models (VidEmo),specifically designed for emotion reasoning and instruction-following. Thesemodels undergo a two-stage tuning process: first, curriculum emotion learningfor injecting emotion knowledge, followed by affective-tree reinforcementlearning for emotion reasoning. Moreover, we establish a foundational datainfrastructure and introduce a emotion-centric fine-grained dataset (Emo-CFG)consisting of 2.1M diverse instruction-based samples. Emo-CFG includesexplainable emotional question-answering, fine-grained captions, and associatedrationales, providing essential resources for advancing emotion understandingtasks. Experimental results demonstrate that our approach achieves competitiveperformance, setting a new milestone across 15 face perception tasks.</description>
      <author>example@mail.com (Zhicheng Zhang, Weicheng Wang, Yongjie Zhu, Wenyu Qin, Pengfei Wan, Di Zhang, Jufeng Yang)</author>
      <guid isPermaLink="false">2511.02712v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Apriel-H1: Towards Efficient Enterprise Reasoning Models</title>
      <link>http://arxiv.org/abs/2511.02651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合transformer注意力和状态空间模型(SSM)的混合架构，实现了比传统transformer模型更高的推理效率，同时保持了良好的推理性能。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型通过transformer架构和注意力机制实现了显著的推理能力，但transformers在注意力模块中具有二次时间和内存复杂度，且需要缓存键值状态，这严重限制了吞吐量和可扩展性。高推理吞吐量对智能体任务、长上下文推理等应用至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合LLM架构，结合transformer注意力和SSM序列混合器，实现高效的推理能力，同时保持较高的推理吞吐量。&lt;h4&gt;方法&lt;/h4&gt;提出Apriel-H1系列混合LLMs，通过增量蒸馏从预训练推理transformer(Apriel-Nemotron-15B-Thinker)获得，逐步用线性Mamba块替换不关键的注意力层。发布了多种后蒸馏变体，分析了不同SSM与MHA比例对推理性能的影响，并在vLLM环境中测试了30/50混合变体的性能。&lt;h4&gt;主要发现&lt;/h4&gt;蒸馏后的混合SSM-Transformer架构在生产环境中实现了超过2倍的更高推理吞吐量，同时推理性能仅最小程度下降。随着更多Mamba层替换MHA，推理性能会逐渐下降，但效率显著提升。&lt;h4&gt;结论&lt;/h4&gt;混合SSM-Transformer架构能够在不显著损害推理质量的情况下，比预训练的transformer等效模型提供实质性的效率提升，为大型语言模型提供了一种有前途的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型(LLMs)通过具有注意力机制的transformer架构实现了显著的推理能力。然而，transformers在注意力模块(MHA)中具有二次时间和内存复杂度，并且在推理过程中需要缓存键值状态，这严重限制了吞吐量和可扩展性。高推理吞吐量对于智能体任务、长上下文推理、高请求负载下的高效部署以及更高效的测试时计算缩放至关重要。状态空间模型(SSMs)如Mamba通过具有固定大小隐藏状态的循环计算提供了具有线性推理复杂度和常量内存占用的有前途的替代方案。在本技术报告中，我们引入了Apriel-H1系列混合LLMs，结合了transformer注意力和SSM序列混合器，在150亿模型规模下实现高效推理。这些模型通过从预训练推理transformer(Apriel-Nemotron-15B-Thinker)进行增量蒸馏获得，逐步用线性Mamba块替换不太关键的注意力层。我们发布了多种后蒸馏变体，具有不同的SSM与MHA比例，并分析了当更多Mamba层替换MHA时推理性能如何下降。此外，我们发布了30/50混合变体，在推理轨迹的监督数据集上进一步微调，在生产就绪的vLLM环境中实现了超过2倍的更高推理吞吐量，且推理性能最小程度下降。这表明，蒸馏后的混合SSM-Transformer架构可以在不显著损害推理质量的情况下，比预训练的transformer等效模型提供实质性的效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) achieve remarkable reasoning capabilitiesthrough transformer architectures with attention mechanisms. However,transformers suffer from quadratic time and memory complexity in the attentionmodule (MHA) and require caching key-value states during inference, whichseverely limits throughput and scalability. High inference throughput iscritical for agentic tasks, long-context reasoning, efficient deployment underhigh request loads, and more efficient test-time compute scaling.  State Space Models (SSMs) such as Mamba offer a promising alternative withlinear inference complexity and a constant memory footprint via recurrentcomputation with fixed-size hidden states. In this technical report weintroduce the Apriel-H1 family of hybrid LLMs that combine transformerattention and SSM sequence mixers for efficient reasoning at 15B model size.These models are obtained through incremental distillation from a pretrainedreasoning transformer, Apriel-Nemotron-15B-Thinker, progressively replacingless critical attention layers with linear Mamba blocks.  We release multiple post-distillation variants of Apriel-H1-15B-Thinker withdifferent SSM-to-MHA ratios and analyse how reasoning performance degrades asmore Mamba layers replace MHA. Additionally, we release a 30/50 hybrid variantof Apriel-H1, further fine-tuned on a supervised dataset of reasoning traces,achieving over 2x higher inference throughput when deployed in theproduction-ready vLLM environment, with minimal degradation in reasoningperformance. This shows that distilled hybrid SSM-Transformer architectures candeliver substantial efficiency gains over the pretrained transformer equivalentwithout substantially compromising the reasoning quality.</description>
      <author>example@mail.com (Oleksiy Ostapenko, Luke Kumar, Raymond Li, Denis Kocetkov, Joel Lamy-Poirier, Shruthan Radhakrishna, Soham Parikh, Shambhavi Mishra, Sebastien Paquet, Srinivas Sunkara, Valérie Bécaert, Sathwik Tejaswi Madhusudhan, Torsten Scholak)</author>
      <guid isPermaLink="false">2511.02651v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Zero-Shot Multi-Animal Tracking in the Wild</title>
      <link>http://arxiv.org/abs/2511.02591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于视觉基础模型的多动物跟踪框架，无需重新训练即可应用于新数据集，在多种物种和环境中表现出强大且一致的性能。&lt;h4&gt;背景&lt;/h4&gt;多动物跟踪对于理解动物生态和行为至关重要，但由于栖息地、运动模式和物种外观的差异，这仍然是一个具有挑战性的任务。传统方法通常需要对每个应用场景进行大量的模型微调和启发式设计。&lt;h4&gt;目的&lt;/h4&gt;探索最近的视觉基础模型在零样本多动物跟踪中的潜力，开发一个无需重新训练或超参数调整的通用跟踪框架。&lt;h4&gt;方法&lt;/h4&gt;结合Grounding Dino目标检测器和Segment Anything Model 2 (SAM 2)跟踪器，以及精心设计的启发式方法，构建了一个可应用于新数据集的跟踪框架。&lt;h4&gt;主要发现&lt;/h4&gt;在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上的评估表明，该框架在多样物种和环境中表现出强大且一致的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于视觉基础模型的跟踪框架能够在不进行重新训练的情况下适应新的数据集和应用场景，为多动物跟踪提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多动物跟踪对于理解动物生态和行为至关重要。然而，由于栖息地、运动模式和物种外观的差异，这仍然是一项具有挑战性的任务。传统方法通常需要对每个应用场景进行大量的模型微调和启发式设计。在这项工作中，我们探索了最近的视觉基础模型在零样本多动物跟踪中的潜力。通过结合Grounding Dino目标检测器和Segment Anything Model 2 (SAM 2)跟踪器以及精心设计的启发式方法，我们开发了一个跟踪框架，可以应用于新数据集而无需重新训练或超参数调整。在ChimpAct、Bird Flock Tracking、AnimalTrack和GMOT-40子集上的评估表明，该框架在多样物种和环境中表现出强大且一致的性能。代码可在https://github.com/ecker-lab/SAM2-Animal-Tracking获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-animal tracking is crucial for understanding animal ecology andbehavior. However, it remains a challenging task due to variations in habitat,motion patterns, and species appearance. Traditional approaches typicallyrequire extensive model fine-tuning and heuristic design for each applicationscenario. In this work, we explore the potential of recent vision foundationmodels for zero-shot multi-animal tracking. By combining a Grounding Dinoobject detector with the Segment Anything Model 2 (SAM 2) tracker and carefullydesigned heuristics, we develop a tracking framework that can be applied to newdatasets without any retraining or hyperparameter adaptation. Evaluations onChimpAct, Bird Flock Tracking, AnimalTrack, and a subset of GMOT-40 demonstratestrong and consistent performance across diverse species and environments. Thecode is available at https://github.com/ecker-lab/SAM2-Animal-Tracking.</description>
      <author>example@mail.com (Jan Frederik Meier, Timo Lüddecke)</author>
      <guid isPermaLink="false">2511.02591v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Resource-efficient Automatic Refinement of Segmentations via Weak Supervision from Light Feedback</title>
      <link>http://arxiv.org/abs/2511.02576v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SCORE（基于区域评估的分割校正）框架，一种弱监督方法，用于改进医学图像分割结果。SCORE仅需轻量级反馈即可学习改进分割预测，无需密集的训练图像标注，在肱骨CT扫描上显著提升了初始分割性能。&lt;h4&gt;背景&lt;/h4&gt;手动分割医学图像解剖区域虽然准确但劳动强度大且易变，推动了自动化方法的发展。虽然基础模型可实现多种解剖结构和成像模态的自动分割，但可能不总能达到临床准确性标准。现有分割改进方法要么需要大量用户交互，要么需要完全监督的分割进行训练。&lt;h4&gt;目的&lt;/h4&gt;开发一种弱监督框架，能够仅使用轻量级反馈来改进分割预测，减少对密集训练标注的依赖。&lt;h4&gt;方法&lt;/h4&gt;SCORE框架引入了一种新的损失函数，利用区域质量分数和过分割/欠分割错误标签，而不是依赖密集的训练图像标注。该方法在肱骨CT扫描上进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;SCORE在肱骨CT扫描上显著改进了TotalSegmentator的初始预测，实现了与现有改进方法相当的性能，同时大大减少了监督需求和标注时间。&lt;h4&gt;结论&lt;/h4&gt;SCORE提供了一种有效的弱监督分割改进方法，能够在不牺牲性能的情况下，显著减少对大量标注数据的依赖。&lt;h4&gt;翻译&lt;/h4&gt;在医学图像分析中，解剖区域的划分是一项关键任务。手动分割虽然能实现高精度，但劳动强度大且容易产生变化，这促使了自动化方法的发展。最近，大量基础模型使得在多种解剖结构和成像模态上实现自动分割成为可能，但这些模型并不总能达到临床准确性标准。虽然分割改进策略可以提高性能，但当前方法依赖于大量用户交互或需要完全监督的分割进行训练。在此，我们提出了SCORE（基于区域评估的分割校正），一种弱监督框架，它仅使用训练过程中的轻量级反馈来学习改进掩膜预测。具体而言，SCORE不依赖密集的训练图像标注，而是引入了一种新的损失函数，利用区域质量分数和过分割/欠分割错误标签。我们在肱骨CT扫描上验证了SCORE，它显著改进了TotalSegmentator的初始预测，并实现了与现有改进方法相当的性能，同时大大减少了它们的监督需求和标注时间。我们的代码可在以下网址获取：https://gitlab.inria.fr/adelangl/SCORE。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何高效改进医学图像分割模型的预测结果问题。医学图像分割是临床诊断的关键任务，手动分割准确但耗时费力，而自动化基础模型虽提高了效率，但其预测结果往往达不到临床应用标准。现有的改进方法要么需要大量人工交互，要么需要完全标注的训练数据，限制了它们在临床环境中的广泛应用。因此，开发一种既能提高分割准确性又能减少标注负担的方法对推动医学图像分析的临床应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有分割改进方法的局限性：半自动方法需要大量人工交互，全自动方法则需要完全标注的地面真实数据。基于这一分析，作者提出使用'轻量反馈'（区域质量评分和错误类型标签）作为弱监督信号来训练改进网络。他们借鉴了3D U-Net网络架构、TotalSegmentator基础模型和Otsu阈值计算等方法，但创新性地设计了一个形态学启发的三部分损失函数，将区域级别的反馈转换为体素级别的校正指导。同时，作者还开发了专门的数据增强策略，提高模型对不同强度分割误差的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用'轻量反馈'（区域质量评分和错误类型标签）作为弱监督信号来训练分割改进网络，而不需要完全标注的地面真实数据。整体流程包括：1)输入3D医学图像、基础模型生成的初始分割和结构轮廓概率图；2)使用3D U-Net作为改进网络；3)通过三部分损失函数进行训练：稳定性损失保持正确区域内部不变，扩张损失针对欠分割区域，收缩损失针对过分割区域；4)训练好的网络接收输入图像、初始分割和边界概率图，输出改进后的分割结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)弱监督训练策略，仅使用轻量反馈而非完全标注数据；2)形态学启发的三部分损失函数，将区域级反馈转换为体素级校正；3)边界先验整合，指导网络在解剖学合理位置进行校正；4)专门的数据增强策略，提高模型鲁棒性。相比之前的工作，SCORE完全自动化且不需要专家在推理过程中交互，训练标注时间减少了约95%，同时在多个测试集上达到了与完全监督方法相当的性能，为医学图像分割的临床应用提供了更实用的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SCORE提出了一种创新的弱监督框架，仅通过轻量反馈就能高效训练分割改进模型，显著减少了标注需求同时达到了与完全监督方法相当的性能，为医学图像分割的临床应用提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Delineating anatomical regions is a key task in medical image analysis.Manual segmentation achieves high accuracy but is labor-intensive and prone tovariability, thus prompting the development of automated approaches. Recently,a breadth of foundation models has enabled automated segmentations acrossdiverse anatomies and imaging modalities, but these may not always meet theclinical accuracy standards. While segmentation refinement strategies canimprove performance, current methods depend on heavy user interactions orrequire fully supervised segmentations for training. Here, we present SCORE(Segmentation COrrection from Regional Evaluations), a weakly supervisedframework that learns to refine mask predictions only using light feedbackduring training. Specifically, instead of relying on dense training imageannotations, SCORE introduces a novel loss that leverages region-wise qualityscores and over/under-segmentation error labels. We demonstrate SCORE onhumerus CT scans, where it considerably improves initial predictions fromTotalSegmentator, and achieves performance on par with existing refinementmethods, while greatly reducing their supervision requirements and annotationtime. Our code is available at: https://gitlab.inria.fr/adelangl/SCORE.</description>
      <author>example@mail.com (Alix de Langlais, Benjamin Billot, Théo Aguilar Vidal, Marc-Olivier Gauci, Hervé Delingette)</author>
      <guid isPermaLink="false">2511.02576v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Can Foundation Models Revolutionize Mobile AR Sparse Sensing?</title>
      <link>http://arxiv.org/abs/2511.02215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了基础模型如何改变移动稀疏感知的格局，通过真实移动AR数据评估，发现基础模型在几何感知图像变换方面提供显著改进，证明了基于基础模型的稀疏感知的可扩展性及其在3D场景重建中的领先性能。&lt;h4&gt;背景&lt;/h4&gt;移动感知系统长期以来在感知质量和效率之间面临基本权衡，这种权衡源于计算能力、功率和其他限制。稀疏感知作为一种关键策略，旨在获取和处理仅一部分传感器数据，以在这些限制下维持性能。&lt;h4&gt;目的&lt;/h4&gt;探究基础模型是否能改变移动稀疏感知的格局。&lt;h4&gt;方法&lt;/h4&gt;使用真实的移动AR数据进行评估，研究基础模型在几何感知图像变换方面的表现，这是实现准确重用跨帧信息的关键技术。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在几何感知图像变换方面提供了显著改进；基于基础模型的稀疏感知具有可扩展性；在3D场景重建方面表现领先。&lt;h4&gt;结论&lt;/h4&gt;研究揭示了将基础模型集成到移动稀疏感知系统中的前景和开放挑战的关键方面。&lt;h4&gt;翻译&lt;/h4&gt;移动感知系统长期以来由于计算、功率和其他限制，在感知质量和效率之间面临基本权衡。稀疏感知作为一种旨在获取和处理仅一部分传感器数据的关键策略，一直是在这些限制下维持性能的重要方法。然而，现有的稀疏感知方法通常面临准确性降低的问题，因为缺失的空间和时间信息给许多感知系统带来了不确定性。在本研究中，我们探究基础模型是否能改变移动稀疏感知的格局。使用真实的移动AR数据进行评估，我们的研究表明基础模型在几何感知图像变换方面提供了显著改进，这是实现准确重用跨帧信息的关键技术。此外，我们的研究证明了基于基础模型的稀疏感知的可扩展性，并显示了其在3D场景重建中的领先性能。总体而言，我们的研究揭示了将基础模型集成到移动稀疏感知系统中的前景和开放挑战的关键方面。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动增强现实系统中的感知质量和效率之间的权衡问题。由于移动设备在计算能力、功耗等方面的限制，传统稀疏感知方法虽然减少了数据采集和处理量，但往往导致准确性下降。这个问题在现实中很重要，因为连续感知会消耗大量移动设备能源，影响设备续航和用户体验，而随着AR应用普及，如何在资源受限的移动设备上实现高效准确的感知变得至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到移动AR系统面临感知质量和效率的权衡问题，发现传统稀疏感知方法存在准确性下降的缺陷。他们注意到基础模型具有大规模预训练和强大泛化能力，可能从稀疏输入中提取有意义信息，从而解决传统稀疏感知问题。作者借鉴了现有稀疏感知技术、基础模型(如DINOv3和Metric3DV2)、几何感知图像变形技术和3D重建方法(如Poisson表面重建和ICP)，但将它们创新性地结合应用于移动AR稀疏感知场景。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用基础模型增强移动AR中的稀疏感知能力，通过基础模型从稀疏传感器数据中提取有意义信息，在减少感知频率的同时保持或提高感知质量。整体流程包括：1)使用基础模型从RGB图像估计深度图替代传统LiDAR；2)利用估计的深度图进行几何感知图像变形，实现跨帧信息重用；3)使用基础模型估计的深度图进行3D环境重建；4)分析不同稀疏感知策略下的信息重叠，优化控制策略。实验使用ScanNet++数据集，通过比较基础模型与LiDAR在图像变形和3D重建中的表现来验证方法效果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将基础模型应用于移动AR稀疏感知；2)利用基础模型估计深度图显著提高几何感知图像变形准确性；3)展示基础模型在低帧率条件下也能实现高质量3D重建；4)分析时空稀疏感知策略下的信息重叠，提出混合策略思路。相比之前工作，本文方法减少了对硬件传感器的依赖，通过更精确的深度估计提高了跨帧信息重用效果，并提出了考虑时间和空间两个维度的混合稀疏感知策略，而非传统基于时间间隔或单一维度的控制策略。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文展示了基础模型如何通过更精确的深度估计和跨帧信息重用，显著提升移动AR系统中的稀疏感知能力，实现了在减少感知频率的同时甚至提高3D重建质量的效果。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile sensing systems have long faced a fundamental trade-off betweensensing quality and efficiency due to constraints in computation, power, andother limitations. Sparse sensing, which aims to acquire and process only asubset of sensor data, has been a key strategy for maintaining performanceunder such constraints. However, existing sparse sensing methods often sufferfrom reduced accuracy, as missing information across space and time introducesuncertainty into many sensing systems. In this work, we investigate whetherfoundation models can change the landscape of mobile sparse sensing. Usingreal-world mobile AR data, our evaluations demonstrate that foundation modelsoffer significant improvements in geometry-aware image warping, a centraltechnique for enabling accurate reuse of cross-frame information. Furthermore,our study demonstrates the scalability of foundation model-based sparse sensingand shows its leading performance in 3D scene reconstruction. Collectively, ourstudy reveals critical aspects of the promises and the open challenges ofintegrating foundation models into mobile sparse sensing systems.</description>
      <author>example@mail.com (Yiqin Zhao, Tian Guo)</author>
      <guid isPermaLink="false">2511.02215v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Automated Reward Design for Gran Turismo</title>
      <link>http://arxiv.org/abs/2511.02094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了如何利用基础模型在奖励函数空间中进行搜索，仅基于文本指令生成期望的强化学习代理，特别是在Gran Turismo 7赛车游戏中的应用。&lt;h4&gt;背景&lt;/h4&gt;在强化学习代理设计中，设计师通过定义奖励函数传达期望行为，但将期望行为映射到奖励函数是一个困难的过程，特别是在自动驾驶赛车等复杂环境中。&lt;h4&gt;目的&lt;/h4&gt;展示如何利用当前基础模型有效地搜索奖励函数空间，仅基于文本指令生成期望的强化学习代理。&lt;h4&gt;方法&lt;/h4&gt;结合基于大型语言模型(LLM)的奖励生成、基于视觉语言模型(VLM)的偏好评估和人类反馈的系统。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够生成与GT Sophy(冠军级强化学习赛车代理)具有竞争力的赛车代理，并能生成新颖的行为。&lt;h4&gt;结论&lt;/h4&gt;为实际应用中的自动化奖励设计铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;在设计强化学习(RL)代理时，设计师通过定义奖励函数来传达期望的代理行为——作为对代理行动的奖励或惩罚而给予的数值反馈。然而，将期望行为映射到奖励函数可能是一个困难的过程，特别是在自动驾驶赛车等复杂环境中。在本文中，我们展示了当前基础模型如何能够有效地搜索奖励函数空间，仅基于文本指令为Gran Turismo 7赛车游戏生成期望的强化学习代理。通过结合基于大型语言模型的奖励生成、基于视觉语言模型的偏好评估和人类反馈，我们展示了如何使用我们的系统生成与GT Sophy(冠军级强化学习赛车代理)具有竞争力的赛车代理，以及生成新颖的行为，为实际应用中的自动化奖励设计铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When designing reinforcement learning (RL) agents, a designer communicatesthe desired agent behavior through the definition of reward functions -numerical feedback given to the agent as reward or punishment for its actions.However, mapping desired behaviors to reward functions can be a difficultprocess, especially in complex environments such as autonomous racing. In thispaper, we demonstrate how current foundation models can effectively search overa space of reward functions to produce desirable RL agents for the Gran Turismo7 racing game, given only text-based instructions. Through a combination ofLLM-based reward generation, VLM preference-based evaluation, and humanfeedback we demonstrate how our system can be used to produce racing agentscompetitive with GT Sophy, a champion-level RL racing agent, as well asgenerate novel behaviors, paving the way for practical automated reward designin real world applications.</description>
      <author>example@mail.com (Michel Ma, Takuma Seno, Kaushik Subramanian, Peter R. Wurman, Peter Stone, Craig Sherstan)</author>
      <guid isPermaLink="false">2511.02094v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Text-VQA Aug: Pipelined Harnessing of Large Multimodal Models for Automated Synthesis</title>
      <link>http://arxiv.org/abs/2511.02046v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First two authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种自动化方法，用于基于场景文本创建大规模视觉问答数据集，整合多种模型和技术避免了繁琐的人工标注过程。&lt;h4&gt;背景&lt;/h4&gt;创建大规模视觉问答(text-VQA)数据库需要熟练的人工标注，这既繁琐又具有挑战性。随着处理视觉和语言模态的基础模型的出现以及OCR系统的成熟，为解决这个问题提供了新的可能。&lt;h4&gt;目的&lt;/h4&gt;建立一个端到端的管道，能够根据给定图像中的场景文本自动合成问答(QA)对，实现text-VQA数据集的自动化合成。&lt;h4&gt;方法&lt;/h4&gt;提出一个自动化合成text-VQA数据集的管道，整合多种模型和算法，包括OCR检测和识别(文本定位)、感兴趣区域(ROI)检测、标题生成和问题生成，将这些组件整合成一个连贯的管道以自动合成和验证QA对。&lt;h4&gt;主要发现&lt;/h4&gt;该管道能够生成可靠的QA对，并能根据场景文本数据的可用性进行扩展，成功创建了包含约72K个QA对、基于约44K张图像的大规模text-VQA数据集。&lt;h4&gt;结论&lt;/h4&gt;据我们所知，这是第一个提出的管道，可以自动合成和验证大规模text-VQA数据集，为视觉问答领域提供了新的数据集构建方法。&lt;h4&gt;翻译&lt;/h4&gt;为视觉问答任务创建大规模数据库，特别是针对场景中的文本数据(text-VQA)，需要熟练的人工标注，这既繁琐又具有挑战性。随着处理视觉和语言模态的基础模型的出现以及OCR系统的成熟，现在有必要建立一个端到端的管道，能够根据给定图像中的场景文本合成问答(QA)对。我们提出了一个用于text-VQA数据集自动合成的管道，可以生成可靠的QA对，并能根据场景文本数据的可用性进行扩展。我们提出的方法利用了多种模型和算法的能力，包括OCR检测和识别(文本定位)、感兴趣区域(ROI)检测、标题生成和问题生成。这些组件被整合成一个连贯的管道，以自动合成和验证QA对。据我们所知，这是第一个提出的管道，可以自动合成和验证包含约72K个QA对、基于约44K张图像的大规模text-VQA数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Creation of large-scale databases for Visual Question Answering taskspertaining to the text data in a scene (text-VQA) involves skilful humanannotation, which is tedious and challenging. With the advent of foundationmodels that handle vision and language modalities, and with the maturity of OCRsystems, it is the need of the hour to establish an end-to-end pipeline thatcan synthesize Question-Answer (QA) pairs based on scene-text from a givenimage. We propose a pipeline for automated synthesis for text-VQA dataset thatcan produce faithful QA pairs, and which scales up with the availability ofscene text data. Our proposed method harnesses the capabilities of multiplemodels and algorithms involving OCR detection and recognition (text spotting),region of interest (ROI) detection, caption generation, and questiongeneration. These components are streamlined into a cohesive pipeline toautomate the synthesis and validation of QA pairs. To the best of ourknowledge, this is the first pipeline proposed to automatically synthesize andvalidate a large-scale text-VQA dataset comprising around 72K QA pairs based onaround 44K images.</description>
      <author>example@mail.com (Soham Joshi, Shwet Kamal Mishra, Viswanath Gopalakrishnan)</author>
      <guid isPermaLink="false">2511.02046v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Assessing the value of Geo-Foundational Models for Flood Inundation Mapping: Benchmarking models for Sentinel-1, Sentinel-2, and Planetscope for end-users</title>
      <link>http://arxiv.org/abs/2511.01990v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究对多种地理基础模型(GFMs)与传统模型在洪水淹没制图方面进行了系统比较，发现GFMs特别是Clay模型在性能和效率方面均优于传统模型，即使在数据有限的情况下也能保持良好表现。&lt;h4&gt;背景&lt;/h4&gt;地理基础模型(GFMs)能够快速可靠地从卫星影像中提取时空信息，通过利用位置和时间嵌入改进洪水淹没制图。然而，尚不清楚GFMs是否优于传统模型如U-Net，且缺乏对不同传感器和数据可用性场景的系统比较。&lt;h4&gt;目的&lt;/h4&gt;评估三种GFMs(Prithvi 2.0、Clay V1.5、DOFA和UViT)与TransNorm、U-Net和Attention U-Net在PlanetScope、Sentinel-1和Sentinel-2上的表现，为用户提供模型选择指导。&lt;h4&gt;方法&lt;/h4&gt;使用多种传感器数据进行模型比较，进行区域外交叉验证(跨五个区域)，进行少样本实验评估少量训练数据下的表现，并比较不同模型的计算时间和模型大小。&lt;h4&gt;主要发现&lt;/h4&gt;1) 所有GFMs表现相当，不同传感器上最佳和最差模型间仅2-5%差异；2) Clay在PlanetScope和Sentinel-2上表现最佳，Prithvi在Sentinel-1上领先；3) 在交叉验证中，Clay在所有传感器上表现略优于其他模型；4) Clay在保留细节方面具有优势；5) 仅用五张训练图像，Clay表现优于其他模型；6) Clay计算效率更高，模型较小(2600万参数)，比Prithvi快约3倍，比DOFA快2倍。&lt;h4&gt;结论&lt;/h4&gt;与先前发现相反，研究结果表明GFMs在洪水制图准确性方面比传统U-Net有小到中等程度的提升，同时计算成本和标注工作量更低。&lt;h4&gt;翻译&lt;/h4&gt;地理基础模型(GFMs)能够快速可靠地从卫星影像中提取时空信息，通过利用位置和时间嵌入改进洪水淹没制图。尽管它们有潜力，但尚不清楚GFMs是否优于传统模型如U-Net。对不同传感器和数据可用性场景的系统比较仍然缺乏，这是指导用户选择模型的重要步骤。为此，我们评估了三种GFMs(Prithvi 2.0、Clay V1.5、DOFA和Prithvi的变体UViT)与TransNorm、U-Net和Attention U-Net在PlanetScope、Sentinel-1和Sentinel-2上的表现。我们观察到所有GFMs都具有竞争力，不同传感器上最佳和最差模型间仅2-5%的差异。Clay在PlanetScope(0.79 mIoU)和Sentinel-2(0.70)上表现最佳，而Prithvi在Sentinel-1上领先(0.57)。在跨五个区域的外交叉验证中，Clay在所有传感器上表现略好(PlanetScope: 0.72(0.04), Sentinel-2: 0.66(0.07), Sentinel-1: 0.51(0.08))，优于Prithvi和DOFA。在所有19个站点的外交叉验证中，Clay比U-Net提高了4%的准确率。视觉检查显示Clay在保留细节方面具有优势。少样本实验显示，仅用五张训练图像，Clay在PlanetScope上达到0.64 mIoU，优于Prithvi(0.24)和DOFA(0.35)。在计算时间方面，由于模型较小(2600万参数)，Clay是更好的选择，比Prithvi(6.5亿参数)快约3倍，比DOFA(4.1亿参数)快2倍。与先前发现相反，我们的研究结果表明，与传统U-Net相比，GFMs在洪水制图准确性方面有小到中等程度的提升，同时计算成本和标注工作量更低。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geo-Foundational Models (GFMs) enable fast and reliable extraction ofspatiotemporal information from satellite imagery, improving flood inundationmapping by leveraging location and time embeddings. Despite their potential, itremains unclear whether GFMs outperform traditional models like U-Net. Asystematic comparison across sensors and data availability scenarios is stilllacking, which is an essential step to guide end-users in model selection. Toaddress this, we evaluate three GFMs, Prithvi 2.0, Clay V1.5, DOFA, and UViT (aPrithvi variant), against TransNorm, U-Net, and Attention U-Net usingPlanetScope, Sentinel-1, and Sentinel-2. We observe competitive performanceamong all GFMs, with only 2-5% variation between the best and worst modelsacross sensors. Clay outperforms others on PlanetScope (0.79 mIoU) andSentinel-2 (0.70), while Prithvi leads on Sentinel-1 (0.57). Inleave-one-region-out cross-validation across five regions, Clay shows slightlybetter performance across all sensors (mIoU: 0.72(0.04), 0.66(0.07),0.51(0.08)) compared to Prithvi (0.70(0.05), 0.64(0.09), 0.49(0.13)) and DOFA(0.67(0.07), 0.64(0.04), 0.49(0.09)) for PlanetScope, Sentinel-2, andSentinel-1, respectively. Across all 19 sites, leave-one-region-outcross-validation reveals a 4% improvement by Clay compared to U-Net. Visualinspection highlights Clay's superior ability to retain fine details. Few-shotexperiments show Clay achieves 0.64 mIoU on PlanetScope with just five trainingimages, outperforming Prithvi (0.24) and DOFA (0.35). In terms of computationaltime, Clay is a better choice due to its smaller model size (26M parameters),making it ~3x faster than Prithvi (650M) and 2x faster than DOFA (410M).Contrary to previous findings, our results suggest GFMs offer small to moderateimprovements in flood mapping accuracy at lower computational cost and labelingeffort compared to traditional U-Net.</description>
      <author>example@mail.com (Saurabh Kaushik, Lalit Maurya, Elizabeth Tellman, ZhiJie Zhang)</author>
      <guid isPermaLink="false">2511.01990v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Robust Mathematical Reasoning</title>
      <link>http://arxiv.org/abs/2511.01846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (main conference),  https://aclanthology.org/2025.emnlp-main.1794/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IMO-Bench，一个针对国际数学奥林匹克竞赛(IMO)级别的高级推理评估基准套件，包含IMO-AnswerBench和IMO-Proof Bench两部分，用于评估基础模型的数学推理能力。该基准在Gemini Deep Think模型上取得了显著成果，并在IMO 2025上获得金牌表现。&lt;h4&gt;背景&lt;/h4&gt;现有基础模型的数学推理能力评估存在局限性，要么过于简单，要么只关注获取简短正确答案，缺乏对高级数学推理能力的有效评估。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对国际数学奥林匹克竞赛(IMO)级别的高级推理评估基准，以更准确地评估基础模型的数学推理能力。&lt;h4&gt;方法&lt;/h4&gt;构建了IMO-Bench评估套件，包括IMO-AnswerBench(测试400个多样化奥林匹克问题)和IMO-Proof Bench(评估证明写作能力，包含基础和高级IMO级别问题及详细评分指南)。此外，还构建了IMO-GradingBench，包含1000个人类评分的证明。&lt;h4&gt;主要发现&lt;/h4&gt;Gemini Deep Think模型在IMO-AnswerBench上达到80.0%的准确率，在高级IMO-Proof Bench上达到65.7%，分别领先其他最佳非Gemini模型6.9%和42.4%。基于Gemini推理能力的自动评分器与人工评估有很好的相关性。&lt;h4&gt;结论&lt;/h4&gt;IMO-Bench为评估高级数学推理能力提供了有效工具，IMO-GradingBench促进了长答案自动评估的发展，这些工具将帮助社区推进强大的数学推理能力。&lt;h4&gt;翻译&lt;/h4&gt;找到正确的北极星指标对于推进基础模型的数学推理能力至关重要，特别是考虑到现有评估要么太简单，要么只关注获取正确的简短答案。为解决这些问题，我们提出了IMO-Bench，一个由顶级专家评审的高级推理基准套件，专门针对国际数学奥林匹克竞赛(IMO)的水平，这是年轻数学家最负盛名的平台。IMO-AnswerBench首先测试模型在400个多样化奥林匹克问题上的表现，这些问题有可验证的简短答案。IMO-Proof Bench是下一级别的证明写作能力评估，包含基础和高级IMO级别问题以及详细的评分指南，以促进自动评分。这些基准在我们的Gemini Deep Think在IMO 2025上取得历史性金牌表现(Luong和Lockhart, 2025)中发挥了关键作用。我们的模型在IMO-AnswerBench上达到80.0%，在高级IMO-Proof Bench上达到65.7%，分别大幅领先最佳非Gemini模型6.9%和42.4%。我们还表明，使用Gemini推理能力构建的自动评分器与人工评估有很好的相关性，并构建了IMO-GradingBench，包含1000个证明的人类评分，以促进长答案自动评估的进一步发展。我们希望IMO-Bench将帮助社区推进强大的数学推理能力，并在https://imobench.github.io/上发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Finding the right north-star metrics is highly critical for advancing themathematical reasoning capabilities of foundation models, especially given thatexisting evaluations are either too easy or only focus on getting correct shortanswers. To address these issues, we present IMO-Bench, a suite of advancedreasoning benchmarks, vetted by a panel of top specialists and thatspecifically targets the level of the International Mathematical Olympiad(IMO), the most prestigious venue for young mathematicians. IMO-AnswerBenchfirst tests models on 400 diverse Olympiad problems with verifiable shortanswers. IMO-Proof Bench is the next-level evaluation for proof-writingcapabilities, which includes both basic and advanced IMO level problems as wellas detailed grading guidelines to facilitate automatic grading. Thesebenchmarks played a crucial role in our historic achievement of the gold-levelperformance at IMO 2025 with Gemini Deep Think (Luong and Lockhart, 2025). Ourmodel achieved 80.0% on IMO-AnswerBench and 65.7% on the advanced IMO-ProofBench, surpassing the best non-Gemini models by large margins of 6.9% and 42.4%respectively. We also showed that autograders built with Gemini reasoningcorrelate well with human evaluations and construct IMO-GradingBench, with 1000human gradings on proofs, to enable further progress in automatic evaluation oflong-form answers. We hope that IMO-Bench will help the community towardsadvancing robust mathematical reasoning and release it athttps://imobench.github.io/.</description>
      <author>example@mail.com (Thang Luong, Dawsen Hwang, Hoang H. Nguyen, Golnaz Ghiasi, Yuri Chervonyi, Insuk Seo, Junsu Kim, Garrett Bingham, Jonathan Lee, Swaroop Mishra, Alex Zhai, Clara Huiyi Hu, Henryk Michalewski, Jimin Kim, Jeonghyun Ahn, Junhwi Bae, Xingyou Song, Trieu H. Trinh, Quoc V. Le, Junehyuk Jung)</author>
      <guid isPermaLink="false">2511.01846v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>How Far Are Surgeons from Surgical World Models? A Pilot Study on Zero-shot Surgical Video Generation with Expert Assessment</title>
      <link>http://arxiv.org/abs/2511.01775v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了SurgVeo基准测试和手术合理性金字塔(SPP)框架，用于评估手术视频生成模型。研究发现先进模型在视觉层面表现良好，但在理解手术操作、环境反馈和手术意图等深层知识方面存在明显不足。&lt;h4&gt;背景&lt;/h4&gt;基础模型在视频生成领域展现出作为物理世界模拟模型的潜力，但在手术等高风险领域需要深度、专业的因果知识，而非通用物理规则，这一领域仍存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;系统解决手术视频生成模型评估的挑战，提出首个专家策划的手术视频生成模型评估基准SurgVeo，以及专门用于评估模型输出的四层框架SPP。&lt;h4&gt;方法&lt;/h4&gt;基于SurgVeo基准，让先进Veo-3模型对腹腔镜和神经外科手术片段进行零样本预测，由四位认证外科医生根据SPP框架评估生成视频。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了明显的'合理性差距'：Veo-3在视觉感知合理性方面表现出色，但在器械操作合理性、环境反馈合理性和手术意图合理性等更高层次评估中严重失败。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了手术AI中视觉模仿与因果理解之间鸿沟的首次定量证据，为开发能够处理专业医疗领域复杂性的未来模型奠定了基础和路线图。&lt;h4&gt;翻译&lt;/h4&gt;视频生成领域的基础模型作为模拟物理世界的潜在世界模型展现出卓越能力。然而，在手术等高风险领域的应用仍是一个关键未探索的空白，这些领域需要深度、专业的因果知识而非通用物理规则。为系统解决这一挑战，我们提出了SurgVeo，这是首个用于手术视频生成模型评估的专家策划基准，以及手术合理性金字塔(SPP)，这是一个新颖的四层框架，专门用于从基本外观到复杂手术策略评估模型输出。基于SurgVeo基准，我们让先进的Veo-3模型对来自腹腔镜和神经外科手术片段进行零样本预测任务。由四位认证外科医生组成的评估小组根据SPP评估生成的视频。我们的研究结果揭示了一个明显的'合理性差距'：虽然Veo-3在视觉感知合理性方面取得了卓越成就，但在SPP的更高层次上却严重失败，包括器械操作合理性、环境反馈合理性和手术意图合理性。这项工作提供了手术AI中视觉上令人信服的模仿与因果理解之间鸿沟的首次定量证据。我们从SurgVeo和SPP中获得的研究结果为开发能够处理专业、现实医疗领域复杂性的未来模型奠定了重要基础和路线图。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in video generation are demonstrating remarkablecapabilities as potential world models for simulating the physical world.However, their application in high-stakes domains like surgery, which demanddeep, specialized causal knowledge rather than general physical rules, remainsa critical unexplored gap. To systematically address this challenge, we presentSurgVeo, the first expert-curated benchmark for video generation modelevaluation in surgery, and the Surgical Plausibility Pyramid (SPP), a novel,four-tiered framework tailored to assess model outputs from basic appearance tocomplex surgical strategy. On the basis of the SurgVeo benchmark, we task theadvanced Veo-3 model with a zero-shot prediction task on surgical clips fromlaparoscopic and neurosurgical procedures. A panel of four board-certifiedsurgeons evaluates the generated videos according to the SPP. Our resultsreveal a distinct "plausibility gap": while Veo-3 achieves exceptional VisualPerceptual Plausibility, it fails critically at higher levels of the SPP,including Instrument Operation Plausibility, Environment Feedback Plausibility,and Surgical Intent Plausibility. This work provides the first quantitativeevidence of the chasm between visually convincing mimicry and causalunderstanding in surgical AI. Our findings from SurgVeo and the SPP establish acrucial foundation and roadmap for developing future models capable ofnavigating the complexities of specialized, real-world healthcare domains.</description>
      <author>example@mail.com (Zhen Chen, Qing Xu, Jinlin Wu, Biao Yang, Yuhao Zhai, Geng Guo, Jing Zhang, Yinlu Ding, Nassir Navab, Jiebo Luo)</author>
      <guid isPermaLink="false">2511.01775v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>AnyPPG: An ECG-Guided PPG Foundation Model Trained on Over 100,000 Hours of Recordings for Holistic Health Profiling</title>
      <link>http://arxiv.org/abs/2511.01747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了AnyPPG，一个在大型多源同步PPG-ECG数据上预训练的PPG基础模型，通过与ECG表示对齐学习有生理意义的特征，在多项生理分析和多器官疾病诊断任务中取得了最先进的性能，展示了PPG作为全面健康评估模式的潜力。&lt;h4&gt;背景&lt;/h4&gt;PPG是一种非侵入式且易于获取的健康监测方式，可在临床环境外使用。然而，现有研究受限于标记数据的规模和多样性，这限制了模型的准确性、泛化能力以及更广泛应用的探索。&lt;h4&gt;目的&lt;/h4&gt;研究通过整合基础模型技术，探索PPG进行全面健康档案构建的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出了AnyPPG，一个在大型、多源同步PPG-ECG数据上预训练的PPG基础模型。通过在共享空间中对齐PPG和ECG表示，AnyPPG从未标记的信号中学习有生理意义的特征。在多样化的下游任务中评估了其能力，包括传统的生理分析和全面的多器官疾病诊断。&lt;h4&gt;主要发现&lt;/h4&gt;在跨越六个独立数据集的十一个生理分析任务中，AnyPPG取得了最先进的性能，与次优模型相比，回归任务平均提高了12.8%，分类任务平均提高了9.1%。在多器官疾病诊断中，AnyPPG展示了广泛的跨系统诊断潜力，在1,014个ICD-10三位数疾病类别中，13个达到0.8以上的AUC，137个超过0.7。除了在心血管疾病方面表现强劲外，AnyPPG在帕金森病和慢性肾病等非心血管状况中也显示出实质性的诊断价值。&lt;h4&gt;结论&lt;/h4&gt;AnyPPG证明，通过与ECG进行生理对齐训练的PPG基础模型可以产生准确且稳健的信号表示。基于此能力，它强调了PPG作为评估全身和多器官健康模式的潜力。&lt;h4&gt;翻译&lt;/h4&gt;背景：光电容积脉搏波描记术提供了一种非侵入式且易于获取的健康监测方式，可用于临床环境之外的健康监测。然而，现有研究受限于标记数据的规模和多样性，这限制了模型的准确性、泛化能力以及更广泛应用的探索。本研究通过整合基础模型技术，调查了PPG进行全面健康档案构建的潜力。方法：我们提出了AnyPPG，一个在大型、多源同步PPG-ECG数据上预训练的PPG基础模型。通过在共享空间中对齐PPG和ECG表示，AnyPPG从未标记的信号中学习有生理意义的特征。其能力在多样化的下游任务中得到了进一步评估，涵盖传统的生理分析和全面的多器官疾病诊断。结果：在跨越六个独立数据集的十一个生理分析任务中，AnyPPG取得了最先进的性能，与次优模型相比，回归任务平均提高了12.8%，分类任务平均提高了9.1%。在多器官疾病诊断中，AnyPPG展示了广泛的跨系统诊断潜力。在1,014个ICD-10三位数疾病类别中，13个达到0.8以上的AUC，137个超过0.7。除了在心力衰竭、瓣膜疾病和高血压等心血管疾病方面表现强劲外，AnyPPG在帕金森病和慢性肾病等非心血管状况中也显示出实质性的诊断价值。结论：AnyPPG证明，通过与ECG进行生理对齐训练的PPG基础模型可以产生准确且稳健的信号表示。基于此能力，它强调了PPG作为评估全身和多器官健康模式的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: Photoplethysmography (PPG) offers a noninvasive and accessiblemodality for health monitoring beyond clinical settings. However, existingstudies are limited by the scale and diversity of labeled data, constrainingmodel accuracy, generalizability, and the exploration of broader applications.This study investigates the potential of PPG for holistic health profilingthrough the integration of foundation model techniques.  Methods: We present AnyPPG, a PPG foundation model pretrained on large-scale,multi-source synchronized PPG-ECG data. By aligning PPG and ECG representationswithin a shared space, AnyPPG learns physiologically meaningful features fromunlabeled signals. Its capability was further evaluated across a diverse set ofdownstream tasks, encompassing both conventional physiological analysis andcomprehensive multi-organ disease diagnosis.  Results: Across eleven physiological analysis tasks spanning six independentdatasets, AnyPPG achieved state-of-the-art performance, with averageimprovements of 12.8% in regression and 9.1% in classification tasks over thenext-best model. In multi-organ disease diagnosis, AnyPPG demonstrated broadcross-system diagnostic potential. Among 1,014 ICD-10 three-digit diseasecategories, 13 achieved an AUC above 0.8 and 137 exceeded 0.7. Beyond strongperformance in cardiovascular diseases such as heart failure, valvulardisorders, and hypertension, AnyPPG also showed substantial diagnostic valuefor non-cardiovascular conditions, exemplified by Parkinson's disease (AUC =0.78) and chronic kidney disease (AUC = 0.74).  Conclusions: AnyPPG demonstrates that a PPG foundation model trained throughphysiological alignment with ECG can produce accurate and robust signalrepresentations. Building on this capability, it underscores the potential ofPPG as a modality for comprehensive assessment of systemic and multi-organhealth.</description>
      <author>example@mail.com (Guangkun Nie, Gongzheng Tang, Yujie Xiao, Jun Li, Shun Huang, Deyun Zhang, Qinghao Zhao, Shenda Hong)</author>
      <guid isPermaLink="false">2511.01747v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>DINO-MX: A Modular &amp; Flexible Framework for Self-Supervised Learning</title>
      <link>http://arxiv.org/abs/2511.01610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DINO-MX是一个模块化、可扩展的视觉基础模型训练框架，结合了DINO、DINOv2和DINOv3的核心原则，支持多种Transformer架构和Hugging Face生态系统，通过多种训练策略和分布式训练方法实现了高效的自监督视觉模型训练。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉基础模型训练流程通常不够灵活、局限于特定领域或计算成本高，限制了它们在不同领域和资源环境中的可用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个模块化、可扩展的训练框架，结合DINO系列模型的核心原则，支持各种基于Transformer的架构，并与Hugging Face生态系统完全兼容，提高视觉基础模型的可用性和适应性。&lt;h4&gt;方法&lt;/h4&gt;DINO-MX是一个统一配置驱动的训练框架，支持低秩自适应(LoRA)、层冻结、知识蒸馏等多种训练策略，通过分布式数据并行(DDP)和全分片数据并行(FSDP)支持分布式训练，适用于自然和专门的数据类型，包括单通道和多通道图像。&lt;h4&gt;主要发现&lt;/h4&gt;在不同数据集上的实验结果表明，DINO-MX在显著降低计算成本的同时实现了具有竞争力的性能。此外，它提供了可解释性工具和标签引导的数据增强方法，可以改进基于注意力的定位，无需额外的检测或分割头。&lt;h4&gt;结论&lt;/h4&gt;DINO-MX为开发、适应和基准测试自监督视觉模型提供了一个可重现且可扩展的基础，适用于各种研究和实际应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型(VFMs)通过自监督方法推动了表征学习的进步。然而，现有的训练流程通常不够灵活、局限于特定领域或计算成本高，这限制了它们在不同领域和资源环境中的可用性。DINO-MX是一个模块化且可扩展的训练框架，在统一的配置驱动系统中结合了DINO、DINOv2和DINOv3的核心原则。它支持各种基于Transformer的架构，并与Hugging Face生态系统完全兼容。该框架包括多种训练策略，如低秩自适应(LoRA)、层冻结和知识蒸馏，同时通过分布式数据并行(DDP)和全分片数据并行(FSDP)支持分布式训练。DINO-MX设计用于处理自然和专门的数据类型，包括单通道和多通道图像。在不同数据集上的实验结果表明，DINO-MX在显著降低计算成本的同时实现了具有竞争力的性能。此外，它提供了解释性工具和一种标签引导的数据增强方法，可以改进基于注意力的定位，而无需额外的检测或分割头。DINO-MX为开发、适应和基准测试自监督视觉模型提供了一个可重现且可扩展的基础，适用于各种研究和实际应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision Foundation Models (VFMs) have advanced representation learning throughself-supervised methods. However, existing training pipelines are ofteninflexible, domain-specific, or computationally expensive, which limits theirusability across different domains and resource settings. DINO-MX is a modularand extensible training framework that combines the core principles of DINO,DINOv2 and DINOv3 within a unified configuration-driven system. It supports avariety of transformer-based architectures and is fully compatible with theHugging Face ecosystem. The framework includes multiple training strategiessuch as low-rank adaptation (LoRA), layer freezing, and knowledge distillation,along with support for distributed training through both Distributed DataParallel (DDP) and Fully Sharded Data Parallel (FSDP). DINO-MX is designed towork with both natural and specialized data types, including single- andmulti-channel images. Experimental results on diverse datasets show thatDINO-MX achieves competitive performance while significantly reducingcomputational costs. Additionally, it offers interpretability tools and alabel-guided data augmentation method that improves attention-basedlocalization without the need for extra detection or segmentation heads.DINO-MX provides a reproducible and scalable foundation for developing,adapting, and benchmarking self-supervised vision models across a range ofresearch and real-world applications.</description>
      <author>example@mail.com (Mahmut Selman Gokmen, Cody Bumgardner)</author>
      <guid isPermaLink="false">2511.01610v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Analyzing Sustainability Messaging in Large-Scale Corporate Social Media</title>
      <link>http://arxiv.org/abs/2511.01550v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个多模态分析流程，利用视觉和语言领域的大型基础模型来分析企业社交媒体内容，特别是与可持续发展相关的传播。&lt;h4&gt;背景&lt;/h4&gt;企业在X平台(前Twitter)上的信息在不断变化、多模态且往往模糊不清，这给分析带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;分析企业社交媒体内容，揭示不同行业在可持续发展目标参与度上的差异、时间趋势以及企业信息、环境、社会、治理风险和消费者参与之间的关联。&lt;h4&gt;方法&lt;/h4&gt;使用大型语言模型集合标注企业推文与可持续发展目标的一致性，并利用视觉语言模型在语义簇框架内分析视觉可持续性传播模式。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了不同行业在可持续发展目标参与度上的差异、时间趋势以及企业信息、环境、社会、治理风险和消费者参与之间的关联。&lt;h4&gt;结论&lt;/h4&gt;自动标签生成和语义视觉聚类方法可广泛适用于其他领域，为大规模社交媒体分析提供了灵活框架。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们介绍了一个多模态分析流程，它利用视觉和语言领域的大型基础模型来分析企业社交媒体内容，重点关注与可持续发展相关的传播。针对企业在X平台(前Twitter)等平台上不断变化、多模态且往往模糊不清的企业信息传播所面临的挑战，我们采用大型语言模型集合来标注大量企业推文，使其与17个可持续发展目标的主题保持一致。这种方法避免了昂贵的、特定任务的标注需求，并探索了此类模型作为社交媒体数据的临时标注者的潜力，能够以可扩展的方式高效捕捉对可持续性主题的显性和隐性引用。作为文本分析的补充，我们在一个使用语义簇的视觉理解框架内利用视觉语言模型，揭示视觉可持续性传播中的模式。这种综合方法揭示了不同行业在可持续发展目标参与度上的差异、时间趋势以及企业信息、环境、社会、治理风险和消费者参与之间的关联。我们的方法——自动标签生成和语义视觉聚类——可广泛适用于其他领域，并为大规模社交媒体分析提供了一个灵活的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we introduce a multimodal analysis pipeline that leverageslarge foundation models in vision and language to analyze corporate socialmedia content, with a focus on sustainability-related communication. Addressingthe challenges of evolving, multimodal, and often ambiguous corporate messagingon platforms such as X (formerly Twitter), we employ an ensemble of largelanguage models (LLMs) to annotate a large corpus of corporate tweets on theirtopical alignment with the 17 Sustainable Development Goals (SDGs). Thisapproach avoids the need for costly, task-specific annotations and explores thepotential of such models as ad-hoc annotators for social media data that canefficiently capture both explicit and implicit references to sustainabilitythemes in a scalable manner. Complementing this textual analysis, we utilizevision-language models (VLMs), within a visual understanding framework thatuses semantic clusters to uncover patterns in visual sustainabilitycommunication. This integrated approach reveals sectoral differences in SDGengagement, temporal trends, and associations between corporate messaging,environmental, social, governance (ESG) risks, and consumer engagement. Ourmethods-automatic label generation and semantic visual clustering-are broadlyapplicable to other domains and offer a flexible framework for large-scalesocial media analysis.</description>
      <author>example@mail.com (Ujjwal Sharma, Stevan Rudinac, Ana Mićković, Willemijn van Dolen, Marcel Worring)</author>
      <guid isPermaLink="false">2511.01550v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Driving scenario generation and evaluation using a structured layer representation and foundational models</title>
      <link>http://arxiv.org/abs/2511.01541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结构化的五层模型，用于改进罕见驾驶场景的评估和生成，并结合大型基础模型使用数据增强策略生成新的驾驶场景。该模型为场景中的每个代理引入子类和特征，使用特定于层模型的嵌入进行比较，并评估了合成数据集的相关性。&lt;h4&gt;背景&lt;/h4&gt;罕见且具有挑战性的驾驶场景对自动驾驶车辆的发展至关重要。由于这些场景难以遇到，使用生成模型模拟或生成它们是一种流行的方法。之前的研究已经尝试在层模型中结构化驾驶场景表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种结构化的五层模型，以提高罕见场景的评估和生成能力。研究并调整两个指标来评估合成数据集在结构化表示背景下的相关性。&lt;h4&gt;方法&lt;/h4&gt;1. 提出结构化的五层模型改进驾驶场景表示；2. 结合大型基础模型采用数据增强策略生成新场景；3. 为每个代理引入子类和特征；4. 使用特定于层模型的嵌入进行比较；5. 研究多样性分数和原创性分数两个评估指标；6. 在不同生成设置下展示指标并进行合成视频的定性评估。&lt;h4&gt;主要发现&lt;/h4&gt;论文展示了在不同生成设置下多样性和原创性分数的应用，以及从结构化场景描述生成的合成视频的定性评估结果。代码和扩展结果可在提供的GitHub链接获取。&lt;h4&gt;结论&lt;/h4&gt;提出的结构化五层模型能够有效改进罕见驾驶场景的评估和生成，结合大型基础模型和数据增强策略可以生成高质量的合成驾驶场景。&lt;h4&gt;翻译&lt;/h4&gt;罕见且具有挑战性的驾驶场景对自动驾驶车辆的发展至关重要。由于它们难以遇到，使用生成模型模拟或生成它们是一种流行方法。在之前尝试在层模型中结构化驾驶场景表示的基础上，我们提出了一种结构化的五层模型来改进罕见场景的评估和生成。我们使用该模型与大型基础模型结合，采用数据增强策略生成新的驾驶场景。与之前的表示方法不同，我们的结构为场景中的每个代理引入了子类和特征，使我们能够使用特定于我们层模型的嵌入来比较它们。我们研究并调整了两个指标来评估合成数据集在结构化表示背景下的相关性：多样性分数估计数据集中场景之间的差异程度，而原创性分数计算合成数据集与真实参考集的相似程度。本文展示了在不同生成设置下的这两个指标，以及对从结构化场景描述生成的合成视频的定性评估。代码和扩展结果可在https://github.com/Valgiz/5LMSG找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成和评估罕见或具有挑战性的驾驶场景（Edge Cases）的问题。这个问题在现实中非常重要，因为这类场景对自动驾驶系统的开发至关重要，但它们在真实驾驶数据中非常稀缺，难以遇到。通过模拟或生成这些罕见场景，研究人员可以测试和改进自动驾驶系统应对极端情况的能力，从而提高系统的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到罕见驾驶场景对自动驾驶开发的重要性，但同时也意识到这些场景难以获取。他们借鉴了Scholtes等人提出的6层模型(6LM)，但将其简化为5层模型(5LM)，去掉了与数字信息相关的第6层。作者利用大型语言模型(LLM)和基础模型来生成新的驾驶场景，采用数据增强策略。他们还研究并调整了两个指标来评估合成数据集的相关性：多样性分数和原创性分数。整个设计过程是基于对现有工作的分析和对自动驾驶场景生成需求的深入理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用结构化的五层模型(5LM)来表示驾驶场景，以便更好地生成和评估罕见场景，并利用大型语言模型和基础模型来增强现有真实场景数据集。整体实现流程包括：1)将驾驶场景分解为5个层次（道路结构、周围建筑、临时变化、动态对象、环境条件）；2)使用多模态大语言模型分析真实驾驶数据并根据5层模型格式化；3)通过两种策略生成新场景（非结构化编辑和结构化层编辑）；4)使用专门的评估指标（原创性分数和多样性分数）来评估生成场景的质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)结构化的五层模型(5LM)提供标准化的驾驶场景表示，引入子类和特征使每个代理都能被详细描述；2)新的合成场景生成策略，通过编辑现有真实场景的特定组件创建边缘案例；3)文本评估方法，提出原创性分数和多样性分数来评估生成场景质量；4)结合视觉和文本生成，先生成结构化场景描述再转化为视觉表示。相比之前的工作，这种方法提供了更精细的场景控制、更全面的评估指标，并确保了生成场景的语义合理性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于结构化五层模型和基础模型的驾驶场景生成与评估方法，通过数据增强策略创建多样化且原创的罕见驾驶场景，为自动驾驶系统开发和测试提供了新的工具和评估框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rare and challenging driving scenarios are critical for autonomous vehicledevelopment. Since they are difficult to encounter, simulating or generatingthem using generative models is a popular approach. Following previous effortsto structure driving scenario representations in a layer model, we propose astructured five-layer model to improve the evaluation and generation of rarescenarios. We use this model alongside large foundational models to generatenew driving scenarios using a data augmentation strategy. Unlike previousrepresentations, our structure introduces subclasses and characteristics forevery agent of the scenario, allowing us to compare them using an embeddingspecific to our layer-model. We study and adapt two metrics to evaluate therelevance of a synthetic dataset in the context of a structured representation:the diversity score estimates how different the scenarios of a dataset are fromone another, while the originality score calculates how similar a syntheticdataset is from a real reference set. This paper showcases both metrics indifferent generation setup, as well as a qualitative evaluation of syntheticvideos generated from structured scenario descriptions. The code and extendedresults can be found at https://github.com/Valgiz/5LMSG.</description>
      <author>example@mail.com (Arthur Hubert, Gamal Elghazaly, Raphaël Frank)</author>
      <guid isPermaLink="false">2511.01541v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>HMVLM: Human Motion-Vision-Lanuage Model via MoE LoRA</title>
      <link>http://arxiv.org/abs/2511.01463v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5figures. The Thirty-Ninth Annual Conference on Neural  Information Processing Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了HMVLM模型，基于MoE LoRA策略的统一框架，用于解决人类运动与语言模型整合中的模态差异和灾难性遗忘问题，同时改进姿态表示方法。&lt;h4&gt;背景&lt;/h4&gt;指令调优数据的扩展使基础语言模型能够表现出改进的指令遵循能力和在多样化下游任务上的卓越性能。语义丰富的3D人体运动正逐渐与这些基础模型集成，以增强多模态理解和跨模态生成能力。然而，人体运动和文本之间的模态差异引发了关于这种整合过程中灾难性遗忘的未解决问题。此外，开发能够在异构下游任务中保持泛化能力的自回归兼容姿态表示仍然是一个关键的技术障碍。&lt;h4&gt;目的&lt;/h4&gt;解决人体运动与文本之间的模态差异导致的灾难性遗忘问题，以及开发能够在多样化下游任务中保持泛化能力的自回归兼容姿态表示。&lt;h4&gt;方法&lt;/h4&gt;提出HMVLM（人类运动-视觉-语言模型），这是一个基于专家混合低秩适应（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配LoRA专家权重，实现多任务的同步微调。为减轻指令调优过程中的灾难性遗忘，引入了一种新型零专家，用于保留预训练参数以处理一般语言任务。对于姿态表示，通过将人体划分为不同的关节组，实现了身体部位特定的标记化，提高了表示的空间分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法有效减轻了指令调优过程中的知识遗忘，并在多样化的人体运动下游任务上取得了卓越的性能。&lt;h4&gt;结论&lt;/h4&gt;HMVLM模型通过MoE LoRA策略和零专家机制，成功解决了人体运动与语言模型整合中的灾难性遗忘问题，同时通过身体部位特定的标记化改进了姿态表示，为多模态理解和生成提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;指令调优数据的扩展使基础语言模型能够在多样化的下游任务上表现出改进的指令遵循能力和卓越性能。语义丰富的3D人体运动正逐渐与这些基础模型集成，以增强多模态理解和跨模态生成能力。然而，人体运动和文本之间的模态差异引发了关于这种整合过程中灾难性遗忘的未解决问题。此外，开发能够在异构下游任务中保持泛化能力的自回归兼容姿态表示仍然是一个关键的技术障碍。为解决这些问题，我们提出了HMVLM（人类运动-视觉-语言模型），这是一个基于专家混合低秩适应（MoE LoRA）策略的统一框架。该框架利用门控网络根据输入提示动态分配LoRA专家权重，实现多任务的同步微调。为减轻指令调优过程中的灾难性遗忘，我们引入了一种新型零专家，用于保留预训练参数以处理一般语言任务。对于姿态表示，我们通过将人体划分为不同的关节组，实现了身体部位特定的标记化，提高了表示的空间分辨率。实验表明，我们的方法有效减轻了指令调优过程中的知识遗忘，并在多样化的人体运动下游任务上取得了卓越的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决两个关键问题：1) 当基础语言模型集成人类运动模态时出现的灾难性遗忘问题，即模型在训练过程中遗忘原有的世界知识和语言能力；2) 如何开发能保留跨任务泛化能力的自回归兼容姿态表示问题。这些问题在现实中很重要，因为随着大型基础模型在多模态理解中的应用日益广泛，将人类运动（包含丰富语义和情感）集成到这些模型中变得至关重要，而遗忘原有知识会导致模型变成对话能力有限的任务特定系统，姿态表示的局限性则会限制虚拟现实、具身智能等多个领域的应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到监督指令微调会过度关注新标记导致遗忘原有知识，因此借鉴了MoE和LoRA技术设计MoE LoRA框架，并引入零专家来保留预训练参数。针对姿态表示问题，作者受图像处理中基于块标记化的启发，将人体划分为不同肢体部分分别编码。作者借鉴了多项现有工作：MoE架构和LoRA技术用于多任务微调，基于块的图像编码用于空间建模，VQ-VAE架构用于离散化运动序列，以及Transformer架构用于自回归生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想包括：1) 使用MoE LoRA框架通过门控网络动态分配专家权重实现多任务微调；2) 引入零专家保留预训练参数减轻灾难性遗忘；3) 采用基于身体部位的标记化提高表示空间分辨率。整体流程：首先处理输入指令和提示，通过门控网络分配专家权重；然后对模态特定输入进行投影对齐；接着根据权重动态组合LoRA专家；同时使用身体部位标记器将姿态和运动离散化为标记；最后基础模型和加权专家共同生成输出。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) MoE LoRA框架引入零专家机制保留基础模型知识；2) 基于身体部位的标记化方法提高姿态表示的空间分辨率；3) 统一框架支持多种人体相关下游任务。相比之前工作不同：与MotionGPT等模型相比，HMVLM通过MoE LoRA和零专家更好保留了对话能力；与传统运动标记化相比，同时考虑空间信息提高表示精细度；与单任务模型相比，作为统一多任务框架在多任务场景下仍保持良好性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了HMVLM框架，通过MoE LoRA和基于身体部位的标记化方法，有效解决了人类运动与语言模型集成中的灾难性遗忘和表示问题，同时支持多种人体相关任务的高效执行。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The expansion of instruction-tuning data has enabled foundation languagemodels to exhibit improved instruction adherence and superior performanceacross diverse downstream tasks. Semantically-rich 3D human motion is beingprogressively integrated with these foundation models to enhance multimodalunderstanding and cross-modal generation capabilities. However, the modalitygap between human motion and text raises unresolved concerns about catastrophicforgetting during this integration. In addition, developingautoregressive-compatible pose representations that preserve generalizabilityacross heterogeneous downstream tasks remains a critical technical barrier. Toaddress these issues, we propose the Human Motion-Vision-Language Model(HMVLM), a unified framework based on the Mixture of Expert Low-RankAdaption(MoE LoRA) strategy. The framework leverages the gating network todynamically allocate LoRA expert weights based on the input prompt, enablingsynchronized fine-tuning of multiple tasks. To mitigate catastrophic forgettingduring instruction-tuning, we introduce a novel zero expert that preserves thepre-trained parameters for general linguistic tasks. For pose representation,we implement body-part-specific tokenization by partitioning the human bodyinto different joint groups, enhancing the spatial resolution of therepresentation. Experiments show that our method effectively alleviatesknowledge forgetting during instruction-tuning and achieves remarkableperformance across diverse human motion downstream tasks.</description>
      <author>example@mail.com (Lei Hu, Yongjing Ye, Shihong Xia)</author>
      <guid isPermaLink="false">2511.01463v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>From Passive to Proactive: A Multi-Agent System with Dynamic Task Orchestration for Intelligent Medical Pre-Consultation</title>
      <link>http://arxiv.org/abs/2511.01445v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14pages, 7 figures, 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种分层多智能体框架，将被动医疗AI系统转变为主动询问智能体，通过自主任务编排优化预咨询流程，在提高效率的同时保证了临床质量。&lt;h4&gt;背景&lt;/h4&gt;全球医疗系统面临患者数量增加和咨询时间有限的挑战，许多国家初级保健咨询平均不足5分钟。现有预咨询流程受限于被动交互模式和AI系统的上下文管理挑战。&lt;h4&gt;目的&lt;/h4&gt;引入分层多智能体框架，将被动医疗AI系统转变为主动询问智能体，通过自主任务编排提升预咨询效率和质量。&lt;h4&gt;方法&lt;/h4&gt;开发八智能体架构，具有集中控制机制，将预咨询分解为四个主要任务（分诊、现病史采集、既往史采集、主诉生成）及13个子任务。在1,372个中国医疗平台电子健康记录上评估，使用GPT-OSS 20B、Qwen3-8B、Phi4-14B等基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;框架在初级科室分诊准确率达87.0%，二级科室分类达80.5%；智能体驱动调度任务完成率98.2%，高于顺序处理的93.1%；临床质量评分平均为：主诉4.56分、现病史4.48分、既往史4.69分（5分制）；T2在12.7轮内完成，T3在16.9轮内完成。&lt;h4&gt;结论&lt;/h4&gt;模型无关架构在不同基础模型上保持高性能，通过本地部署保护数据隐私，展示了自主AI系统增强临床预咨询效率和质量的可能性。&lt;h4&gt;翻译&lt;/h4&gt;全球医疗系统正面临患者数量增加和咨询时间有限的严峻挑战，在许多国家，初级保健咨询的平均时间不足5分钟。虽然涵盖分诊和结构化病史采集的预咨询流程提供了潜在解决方案，但它们仍受限于现有AI系统中的被动交互模式和上下文管理挑战。本研究引入了一种分层多智能体框架，通过自主任务编排将被动的医疗AI系统转变为主动的询问智能体。我们开发了一个具有集中控制机制的八智能体架构，将预咨询分解为四个主要任务：分诊、现病史采集、既往史采集和主诉生成，其中T1-T3进一步细分为13个特定领域的子任务。在中国医疗平台的1,372个经过验证的电子健康记录上，使用多个基础模型（GPT-OSS 20B、Qwen3-8B、Phi4-14B）评估，该框架在初级科室分诊上达到87.0%的准确率，在二级科室分类上达到80.5%的准确率，使用智能体驱动的调度任务完成率达到98.2%，而顺序处理为93.1%。18名医师的临床质量评分平均为：主诉4.56分、现病史4.48分、既往史4.69分（5分制），T2在12.7轮内完成，T3在16.9轮内完成。与模型无关的架构在不同基础模型上保持了高性能，同时通过本地部署保护数据隐私，展示了自主AI系统增强临床环境中预咨询效率和质量的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Global healthcare systems face critical challenges from increasing patientvolumes and limited consultation times, with primary care visits averagingunder 5 minutes in many countries. While pre-consultation processesencompassing triage and structured history-taking offer potential solutions,they remain limited by passive interaction paradigms and context managementchallenges in existing AI systems. This study introduces a hierarchicalmulti-agent framework that transforms passive medical AI systems into proactiveinquiry agents through autonomous task orchestration. We developed aneight-agent architecture with centralized control mechanisms that decomposespre-consultation into four primary tasks: Triage ($T_1$), History of PresentIllness collection ($T_2$), Past History collection ($T_3$), and ChiefComplaint generation ($T_4$), with $T_1$--$T_3$ further divided into 13domain-specific subtasks. Evaluated on 1,372 validated electronic healthrecords from a Chinese medical platform across multiple foundation models(GPT-OSS 20B, Qwen3-8B, Phi4-14B), the framework achieved 87.0% accuracy forprimary department triage and 80.5% for secondary department classification,with task completion rates reaching 98.2% using agent-driven scheduling versus93.1% with sequential processing. Clinical quality scores from 18 physiciansaveraged 4.56 for Chief Complaints, 4.48 for History of Present Illness, and4.69 for Past History on a 5-point scale, with consultations completed within12.7 rounds for $T_2$ and 16.9 rounds for $T_3$. The model-agnosticarchitecture maintained high performance across different foundation modelswhile preserving data privacy through local deployment, demonstrating thepotential for autonomous AI systems to enhance pre-consultation efficiency andquality in clinical settings.</description>
      <author>example@mail.com (ChengZhang Yu, YingRu He, Hongyan Cheng, nuo Cheng, Zhixing Liu, Dongxu Mu, Zhangrui Shen, Zhanpeng Jin)</author>
      <guid isPermaLink="false">2511.01445v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards General Auditory Intelligence: Large Multimodal Models for Machine Listening and Speaking</title>
      <link>http://arxiv.org/abs/2511.01299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文是一篇综述，探讨了在大语言模型和通用人工智能时代，计算机听觉如何超越传统范式，充分利用基础模型的能力，朝着更全面的理解、更自然的生成和更类人的交互方向发展。&lt;h4&gt;背景&lt;/h4&gt;在大语言模型和通用人工智能时代，计算机听觉需要发展以适应新的技术环境。音频作为一种包含丰富语义、情感和上下文线索的模式，在实现自然化和具身机器智能方面发挥着关键作用。&lt;h4&gt;目的&lt;/h4&gt;这篇综述旨在全面回顾近期将音频整合到LLMs中的进展，分析LLMs如何重塑音频感知和推理能力，探索音频和视觉模式的融合如何增强情境感知和跨模态推理，并确定构建音频原生AGI系统的关键挑战和未来方向。&lt;h4&gt;方法&lt;/h4&gt;论文采用综述方法，分析近期在音频与LLMs整合方面的研究进展，特别关注四个关键领域：音频理解、音频生成、基于语音的交互和音频-视觉理解。论文分析了LLMs如何改变音频感知和推理，探索了多模态智能的边界。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs使系统能够在更深层次的语义水平上理解声音，生成富有表现力的音频输出，进行类人的口语交互，并且音频和视觉模式的融合增强了情境感知和跨模态推理，推动了多模态智能的边界。&lt;h4&gt;结论&lt;/h4&gt;这篇综述不仅综合了现有研究，还确定了构建能够像人类一样通过声音感知、理解和交互的音频原生AGI系统的关键挑战和未来方向。&lt;h4&gt;翻译&lt;/h4&gt;在大语言模型和通用人工智能时代，计算机听觉必须超越传统范式，充分利用基础模型的能力，朝着更全面的理解、更自然的生成和更类人的交互方向发展。音频作为一种富含语义、情感和上下文线索的模式，在实现自然化和具身机器智能方面发挥着至关重要的作用。这篇综述全面回顾了近期将音频整合到LLMs中的进展，重点关注四个关键领域：音频理解、音频生成、基于语音的交互和音频-视觉理解。我们分析了LLMs如何重塑音频感知和推理，使系统能够在更深层次的语义水平上理解声音，生成富有表现力的音频输出，并进行类人的口语交互。此外，我们还探索了音频和视觉模式的融合如何增强情境感知和跨模态推理，推动多模态智能的边界。这篇综述不仅综合了现有研究，还确定了构建能够像人类一样自然地通过声音感知、理解和交互的音频原生AGI系统的关键挑战和未来方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the era of large language models (LLMs) and artificial generalintelligence (AGI), computer audition must evolve beyond traditional paradigmsto fully leverage the capabilities of foundation models, towards morecomprehensive understanding, more natural generation and more human-likeinteraction. Audio, as a modality rich in semantic, emotional, and contextualcues, plays a vital role in achieving naturalistic and embodied machineintelligence. This survey provides a comprehensive review of recent progress inintegrating audio into LLMs, with a focus on four key areas: audiocomprehension, audio generation, speech-based interaction, and audio-visualunderstanding. We analyze how LLMs are reshaping audio perception andreasoning, enabling systems to understand sound at a deeper semantic level,generate expressive audio outputs, and engage in human-like spoken interaction.Furthermore, we explore how the fusion of audio and visual modalities enhancessituational awareness and cross-modal reasoning, pushing the boundaries ofmultimodal intelligence. This survey not only synthesizes existing research butalso identifies critical challenges and future directions for buildingaudio-native AGI systems capable of perceiving, understanding, and interactingthrough sound as naturally as humans do.</description>
      <author>example@mail.com (Siyin Wang, Zengrui Jin, Changli Tang, Qiujia Li, Bo Li, Chen Chen, Yuchen Hu, Wenyi Yu, Yixuan Li, Jimin Zhuang, Yudong Yang, Mingqiu Wang, Michael Han, Yifan Ding, Junwen Bai, Tom Ouyang, Shuo-yiin Chang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Lu Lu, Guangzhi Sun, Zhehuai Chen, Ji Wu, Bowen Zhou, Yuxuan Wang, Tara Sainath, Yonghui Wu, Chao Zhang)</author>
      <guid isPermaLink="false">2511.01299v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Optimal Attention Temperature Enhances In-Context Learning under Distribution Shift</title>
      <link>http://arxiv.org/abs/2511.01292v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在分布偏移条件下注意力温度对预训练Transformer模型上下文学习性能的影响，首次提供了理论和实证研究，证明了最优注意力温度可以最小化分布偏移导致的误差，提高了ICL的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;预训练Transformer模型在上下文学习方面表现出色，但当预训练和测试数据间存在分布偏移时，ICL性能会急剧下降，这种情况在实际部署中越来越常见。虽然调整注意力温度可以提高Transformer性能，但在分布偏移条件下注意力温度对ICL的影响尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;提供关于在分布偏移条件下ICL注意力温度的首个理论和实证研究，探索最优注意力温度对提高ICL鲁棒性的作用。&lt;h4&gt;方法&lt;/h4&gt;使用'线性化softmax'框架推导闭式泛化误差表达式，证明输入协方差变化和标签噪声对ICL的影响，并通过线性回归任务的模拟和GPT-2、LLaMA2-7B在问答基准上的大规模实验验证理论预测。&lt;h4&gt;主要发现&lt;/h4&gt;输入协方差的变化或标签噪声会显著损害ICL性能；存在最优注意力温度可以最小化分布偏移条件下的误差；注意力温度是提高预训练Transformer中ICL鲁棒性的有效机制。&lt;h4&gt;结论&lt;/h4&gt;注意力温度是提高预训练Transformer中ICL鲁棒性的原则性和强大机制，研究推进了理论理解，并为实践中选择注意力温度提供了可行的指导。&lt;h4&gt;翻译&lt;/h4&gt;预训练Transformer在上下文学习方面表现出色，仅从少量例子中就能推断新任务。然而，当预训练和测试数据之间存在分布偏移时，它们的ICL性能可能会急剧下降，这种情况在实际部署中越来越常见。虽然最近的实证研究表明调整softmax中的注意力温度可以提高Transformer性能，但在分布偏移条件下，注意力温度在ICL中的作用仍未被探索。本文首次提供了关于在分布偏移条件下ICL注意力温度的理论和实证研究。使用简化但富有表现力的'线性化softmax'框架，我们推导出闭式泛化误差表达式，并证明输入协方差的变化或标签噪声会显著损害ICL，但存在最优注意力温度可以最小化这种误差。然后，我们通过线性回归任务的广泛模拟和GPT-2及LLaMA2-7B在问答基准上的大规模实验验证了我们的预测。我们的研究结果表明，注意力温度是提高预训练Transformer中ICL鲁棒性的原则性和强大机制，推进了理论理解，并为实践中选择注意力温度提供了可行的指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained Transformers excel at in-context learning (ICL), inferring newtasks from only a handful of examples. Yet, their ICL performance can degradesharply under distribution shift between pretraining and test data, a regimeincreasingly common in real-world deployments. While recent empirical workhints that adjusting the attention temperature in the softmax can enhanceTransformer performance, the attention temperature's role in ICL underdistribution shift remains unexplored. This paper provides the firsttheoretical and empirical study of attention temperature for ICL underdistribution shift. Using a simplified but expressive "linearized softmax"framework, we derive closed-form generalization error expressions and provethat shifts in input covariance or label noise substantially impair ICL, butthat an optimal attention temperature exists which minimizes this error. Wethen validate our predictions through extensive simulations on linearregression tasks and large-scale experiments with GPT-2 and LLaMA2-7B onquestion-answering benchmarks. Our results establish attention temperature as aprincipled and powerful mechanism for improving the robustness of ICL inpretrained Transformers, advancing theoretical understanding and providingactionable guidance for selecting attention temperature in practice.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2511.01292v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Adaptation of Foundation Models for Medical Image Analysis: Strategies, Challenges, and Future Directions</title>
      <link>http://arxiv.org/abs/2511.01284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于Foundation models在医学图像分析中应用的综述文章，评估了将FMs适应医学成像特定需求的多种策略，包括监督微调、领域特定预训练等方法，并指出了新兴研究方向如持续学习、隐私保护方法等，为开发适应性、可信且临床整合的FMs提供了路线图。&lt;h4&gt;背景&lt;/h4&gt;Foundation models已成为医学图像分析中的一种变革性范式，能够为广泛的临床任务和成像模式提供可泛化、任务无关的解决方案。它们从大规模数据中学习可迁移表示的能力，有望解决传统特定任务模型的局限性。然而，将FMs适应真实临床实践仍面临关键挑战，包括域偏移、高质量标注数据有限、计算需求大以及严格的隐私要求。&lt;h4&gt;目的&lt;/h4&gt;本文对将FMs适应医学成像特定需求的策略进行了全面评估。&lt;h4&gt;方法&lt;/h4&gt;检查了监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态或跨模态框架等适应FMs的方法，并对每种方法评估了报告的性能提升、临床适用性和局限性，同时确定了先前综述经常忽视的权衡和未解决的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;新兴研究方向包括：实现动态部署的持续学习；保护敏感数据的联邦和隐私保护方法；提高数据效率的混合自监督学习；结合合成生成与人工验证循环的数据中心管道；以及评估在真实临床变异性下的鲁棒泛化的系统基准测试。&lt;h4&gt;结论&lt;/h4&gt;通过概述这些策略和相关研究差距，本综述为开发适应性、可信且临床整合的FMs提供了路线图，使其能够满足真实医学成像的需求。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已成为医学图像分析中的一种变革性范式，能够为广泛的临床任务和成像模式提供可泛化、任务无关的解决方案。它们从大规模数据中学习可迁移表示的能力，有望解决传统特定任务模型的局限性。然而，将FMs适应真实临床实践仍面临关键挑战，包括域偏移、高质量标注数据有限、计算需求大以及严格的隐私要求。本文对将FMs适应医学成像特定需求的策略进行了全面评估。我们检查了监督微调、领域特定预训练、参数高效微调、自监督学习、混合方法以及多模态或跨模态框架等方法。对于每种方法，我们评估了报告的性能提升、临床适用性和局限性，同时确定了先前综述经常忽视的权衡和未解决的挑战。除了这些已建立的技术外，我们还强调了旨在解决当前差距的新兴方向，包括实现动态部署的持续学习、保护敏感数据的联邦和隐私保护方法、提高数据效率的混合自监督学习、结合合成生成与人工验证循环的数据中心管道，以及评估在真实临床变异性下的鲁棒泛化的系统基准测试。通过概述这些策略和相关研究差距，本综述为开发适应性、可信且临床整合的FMs提供了路线图，使其能够满足真实医学成像的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) have emerged as a transformative paradigm in medicalimage analysis, offering the potential to provide generalizable, task-agnosticsolutions across a wide range of clinical tasks and imaging modalities. Theircapacity to learn transferable representations from large-scale data has thepotential to address the limitations of conventional task-specific models.However, adaptation of FMs to real-world clinical practice remains constrainedby key challenges, including domain shifts, limited availability ofhigh-quality annotated data, substantial computational demands, and strictprivacy requirements. This review presents a comprehensive assessment ofstrategies for adapting FMs to the specific demands of medical imaging. Weexamine approaches such as supervised fine-tuning, domain-specific pretraining,parameter-efficient fine-tuning, self-supervised learning, hybrid methods, andmultimodal or cross-modal frameworks. For each, we evaluate reportedperformance gains, clinical applicability, and limitations, while identifyingtrade-offs and unresolved challenges that prior reviews have often overlooked.Beyond these established techniques, we also highlight emerging directionsaimed at addressing current gaps. These include continual learning to enabledynamic deployment, federated and privacy-preserving approaches to safeguardsensitive data, hybrid self-supervised learning to enhance data efficiency,data-centric pipelines that combine synthetic generation with human-in-the-loopvalidation, and systematic benchmarking to assess robust generalization underreal-world clinical variability. By outlining these strategies and associatedresearch gaps, this review provides a roadmap for developing adaptive,trustworthy, and clinically integrated FMs capable of meeting the demands ofreal-world medical imaging.</description>
      <author>example@mail.com (Karma Phuntsho, Abdullah, Kyungmi Lee, Ickjai Lee, Euijoon Ahn)</author>
      <guid isPermaLink="false">2511.01284v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Speech-DRAME: A Framework for Human-Aligned Benchmarks in Speech Role-Play</title>
      <link>http://arxiv.org/abs/2511.01261v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  67 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Speech-DRAME是一个统一的框架，用于评估语音角色扮演系统，包含评估基准、微调评估模型和语音角色扮演基准三个层面贡献，通过原型评估和真实性评估两种互补策略提供全面的评估基础。&lt;h4&gt;背景&lt;/h4&gt;角色扮演已成为生成模型的关键测试平台，从纯文本对话扩展到多模态交互。将角色扮演扩展到语音可以捕捉韵律、情感和表达方式，但也带来了新的评估挑战。&lt;h4&gt;目的&lt;/h4&gt;提出Speech-DRAME框架，解决当前语音角色扮演评估中存在的问题，为语音角色扮演提供全面、可复制的评估基础。&lt;h4&gt;方法&lt;/h4&gt;Speech-DRAME框架在三个层面做出贡献：(i)Speech-DRAME-EvalBench评估基准，包含双语人工注释数据和用于训练测试语音评估模型的协议，(ii)DRAME-Eval微调评估模型，显著优于零样本和少样本音频大语言模型，(iii)Speech-DRAME-RoleBench语音角色扮演基准，利用DRAME-Eval作为自动评估者。同时区分了原型评估和真实性评估两种互补的评估策略。&lt;h4&gt;主要发现&lt;/h4&gt;与零样本音频大语言模型评估者相比，DRAME-Eval与人类评分的一致性更强，原型评估中的相关系数从0.480提高到0.629，真实性评估中从0.390提高到0.625。&lt;h4&gt;结论&lt;/h4&gt;通过整合透明的基准资源、建模方法和系统级评估，Speech-DRAME为评估语音角色扮演提供了首个全面、可复制的基础。&lt;h4&gt;翻译&lt;/h4&gt;角色扮演已成为生成模型的关键测试平台，从纯文本对话扩展到多模态交互。将角色扮演扩展到语音可以捕捉韵律、情感和表达方式，但也带来了新的评估挑战。当前的评估流程通常使用音频大语言模型作为零样本评估者，这些模型会忽略副语言线索，将多个方面合并为粗略的分数，并依赖无法反映现实世界角色的合成语音参考。我们提出了Speech-DRAME，一个统一框架，在三个层面做出贡献：(i)Speech-DRAME-EvalBench，一个包含双语人工注释数据和用于训练测试语音评估模型的协议的评估基准，(ii)DRAME-Eval，一个微调的评估模型，显著优于零样本和少样本音频大语言模型，(iii)Speech-DRAME-RoleBench，一个利用DRAME-Eval作为自动评估者来比较语音基础模型的语音角色扮演基准。Speech-DRAME区分了两种互补的评估策略：原型评估，一种自上而下的方法，衡量对广泛角色原型的遵循程度；真实性评估，一种基于真实人类语音的自下而上方法，强调细微的角色质量。与零样本音频大语言模型评估者相比，DRAME-Eval与人类评分的一致性更强（原型评估中的相关系数从0.480提高到0.629，真实性评估中从0.390提高到0.625）。通过整合透明的基准资源、建模方法和系统级评估，Speech-DRAME为评估语音角色扮演提供了首个全面、可复制的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Role-play has become a key testbed for generative models, expanding fromtext-only dialogue to multimodal interaction. Extending role-play to speechcaptures prosody, emotion, and delivery, but also poses new evaluationchallenges. Current pipelines often use audio large language models (ALLMs) aszero-shot judges, which miss paralinguistic cues, collapse multiple aspectsinto coarse scores, and rely on synthetic speech references that fail toreflect real-world roles. We present Speech-DRAME, a unified framework thatcontributes at three levels: (i) Speech-DRAME-EvalBench, an evaluationbenchmark with bilingual human-annotated data and protocols for training andtesting speech evaluation models (SEMs), (ii) DRAME-Eval, a fine-tunedevaluation model, which substantially outperforms zero-shot and few-shot ALLMs,and (iii) Speech-DRAME-RoleBench, a speech role-play benchmark that leveragesDRAME-Eval as an automatic judge to compare speech foundation models (SFMs).Speech-DRAME distinguishes between two complementary evaluation strategies:Archetype Evaluation, a top-down approach measuring adherence to broad rolearchetypes, and Realism Evaluation, a bottom-up approach grounded in real humanspeech that emphasizes nuanced role quality. Compared to zero-shot ALLM judges,DRAME-Eval achieves stronger agreement with human ratings (Pearson correlationfrom 0.480 to 0.629 in archetypes, and 0.390 to 0.625 in realism). Byintegrating transparent benchmark resources, modeling approaches, andsystem-level evaluation, Speech-DRAME provides the first comprehensive,reproducible foundation for assessing spoken role-play.</description>
      <author>example@mail.com (Jiatong Shi, Jionghao Han, Yichen Lu, Santiago Pascual, Pengfei Wu, Chenye Cui, Shinji Watanabe, Chao Weng, Cong Zhou)</author>
      <guid isPermaLink="false">2511.01261v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>VesSAM: Efficient Multi-Prompting for Segmenting Complex Vessel</title>
      <link>http://arxiv.org/abs/2511.00981v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VesSAM是一个专门针对2D血管分割的强大高效框架，通过结合卷积适配器、多提示编码器和轻量级解码器，解决了血管分割中的挑战，在各种成像模态上表现优异，且在分布外数据上具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;准确的血管分割对临床应用（如疾病诊断和手术规划）至关重要，但由于血管具有细小、分支结构和低纹理对比度，血管分割仍然具有挑战性。基础模型如Segment Anything Model (SAM)在通用分割方面显示出前景，但在血管结构上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对2D血管分割的强大而高效的框架，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;VesSAM框架包含三个关键组件：(1)卷积适配器增强局部纹理特征，(2)多提示编码器通过分层交叉注意力融合解剖学提示（包括骨架、分叉点和线段中点），(3)轻量级掩码解码器减少锯齿伪影。此外，还引入了自动化流程生成结构化多提示标注，并整理了包含5种成像模态8个数据集的多样化基准数据集。&lt;h4&gt;主要发现&lt;/h4&gt;VesSAM在Dice和IoU指标上比最先进的基于PEFT的SAM变体高出10%和13%，与完全微调的方法相比实现了具有竞争力的性能，且参数显著减少。VesSAM在分布外（OoD）设置中表现良好，在平均OoD Dice和IoU上优于所有基线。&lt;h4&gt;结论&lt;/h4&gt;VesSAM是一个专门为血管分割设计的有效框架，通过结合创新架构和提示机制，显著提升了血管分割的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;准确的血管分割对临床应用（如疾病诊断和手术规划）至关重要，但由于血管细小、分支结构和低纹理对比度，仍然具有挑战性。虽然像Segment Anything Model (SAM)这样的基础模型在通用分割方面显示出前景，但在血管结构上表现不佳。在这项工作中，我们提出了VesSAM，一个专门针对2D血管分割的强大而高效的框架。VesSAM集成了(1)卷积适配器增强局部纹理特征，(2)多提示编码器通过分层交叉注意力融合解剖学提示，包括骨架、分叉点和线段中点，以及(3)轻量级掩码解码器减少锯齿伪影。我们还引入了自动化流程生成结构化多提示标注，并整理了一个包含5种成像模态8个数据集的多样化基准数据集。实验结果表明，VesSAM在Dice和IoU指标上持续比最先进的基于PEFT的SAM变体高出10%和13%，与完全微调的方法相比实现了具有竞争力的性能，且参数显著减少。VesSAM在分布外（OoD）设置中泛化良好，在平均OoD Dice和IoU上优于所有基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate vessel segmentation is critical for clinical applications such asdisease diagnosis and surgical planning, yet remains challenging due to thin,branching structures and low texture contrast. While foundation models like theSegment Anything Model (SAM) have shown promise in generic segmentation, theyperform sub-optimally on vascular structures. In this work, we present VesSAM,a powerful and efficient framework tailored for 2D vessel segmentation. VesSAMintegrates (1) a convolutional adapter to enhance local texture features, (2) amulti-prompt encoder that fuses anatomical prompts, including skeletons,bifurcation points, and segment midpoints, via hierarchical cross-attention,and (3) a lightweight mask decoder to reduce jagged artifacts. We alsointroduce an automated pipeline to generate structured multi-promptannotations, and curate a diverse benchmark dataset spanning 8 datasets across5 imaging modalities. Experimental results demonstrate that VesSAM consistentlyoutperforms state-of-the-art PEFT-based SAM variants by over 10% Dice and 13%IoU, and achieves competitive performance compared to fully fine-tuned methods,with significantly fewer parameters. VesSAM also generalizes well toout-of-distribution (OoD) settings, outperforming all baselines in average OoDDice and IoU.</description>
      <author>example@mail.com (Suzhong Fu, Rui Sun, Xuan Ding, Jingqi Dong, Yiming Yang, Yao Zhu, Min Chang Jordan Ren, Delin Deng, Angelica Aviles-Rivero, Shuguang Cui, Zhen Li)</author>
      <guid isPermaLink="false">2511.00981v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Actial: Activate Spatial Reasoning Ability of Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2511.01618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过引入视角学习任务和Viewpoint-100K数据集，采用两阶段微调策略和混合冷启动初始化方法，有效提升了多模态大语言模型在3D推理任务中的空间推理能力，为未来机器人、自主系统和3D场景理解等领域的发展提供了支持。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在2D视觉理解方面取得了显著进展，引起了人们对将其应用于复杂3D推理任务的兴趣。然而，尚不清楚这些模型是否能有效捕捉稳健现实世界性能所需的详细空间信息，特别是跨视图一致性，这是准确3D推理的关键要求。&lt;h4&gt;目的&lt;/h4&gt;引入视角学习任务，旨在评估和改进多模态大语言模型的空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;提出Viewpoint-100K数据集，包含10万个以对象为中心的图像对，具有多样化的视角和相应的问题-答案对；采用两阶段微调策略：首先通过监督微调将基础知识注入基础模型，然后通过强化学习增强泛化能力；引入混合冷启动初始化方法，同时学习视角表示并保持连贯的推理思维。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著激活了多模态大语言模型的空间推理能力，提高了在领域内和领域外推理任务上的性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了在多模态大语言模型中开发基础空间技能的价值，支持机器人、自主系统和3D场景理解方面的未来进步。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型的最新进展显著提高了二维视觉理解能力，促使人们对其在复杂三维推理任务中的应用产生兴趣。然而，这些模型是否能有效捕捉稳健现实世界性能所需的详细空间信息，特别是跨视图一致性，这一准确三维推理的关键要求，仍不清楚。考虑到这一问题，我们引入了视角学习，这是一个旨在评估和改进多模态大语言模型空间推理能力的任务。我们提出了Viewpoint-100K数据集，包含10万个以对象为中心的图像对，具有多样化的视角和相应的问题-答案对。我们的方法采用两阶段微调策略：首先，通过在Viewpoint-100K上进行监督微调将基础知识注入基础多模态大语言模型，在多个任务上取得显著改进；其次，通过在更广泛的问题集上使用组相对策略优化算法的强化学习来增强泛化能力。此外，我们引入了一种混合冷启动初始化方法，旨在同时学习视角表示并保持连贯的推理思维。实验结果表明，我们的方法显著激活了多模态大语言模型的空间推理能力，提高了在领域内和领域外推理任务上的性能。我们的研究结果强调了在多模态大语言模型中开发基础空间技能的价值，支持机器人、自主系统和三维场景理解方面的未来发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态大语言模型(MLLMs)无法有效捕捉3D空间推理所需的详细空间信息，特别是跨视图一致性的问题。这个问题很重要，因为虽然MLLMs在2D视觉理解方面进步显著，但在需要准确3D推理的现实应用中表现不佳，限制了机器人在复杂环境中的导航、自主系统的空间感知以及3D场景理解等关键应用的发展。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到当前MLLMs主要在强调2D连续性的视频数据上训练，导致它们难以理解3D空间一致性。他们从计算机视觉领域的相机校准和立体匹配方法中获得启发，设计了简化的视角学习任务，将复杂的3D问题分解为简单的多选题。作者借鉴了参考帧(FoR)概念、Group Relative Policy Optimization算法、冷启动初始化和思维链(CoT)等现有技术，但将其创新性地应用于激活MLLMs的空间推理能力上。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过专门的视角学习任务激活MLLMs的空间推理能力，采用两阶段微调策略：先注入基础知识，再增强泛化能力。整体流程包括：1)创建Viewpoint-100K数据集，包含10万个对象中心的图像对和问答对；2)第一阶段使用监督微调(SFT)和混合冷启动初始化(90%真实数据+10%伪思维链)注入基础知识；3)第二阶段使用强化学习GRPO算法在SAT数据集上增强泛化能力，鼓励模型生成自己的推理链并应用已学空间知识。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出视角学习任务评估和改进MLLMs空间推理能力；2)创建Viewpoint-100K数据集；3)采用两阶段微调策略；4)设计混合冷启动初始化方法；5)将复杂3D问题简化为多选题。相比之前工作，本文更关注空间推理的基础任务(如视角估计)而非高级推理；不依赖额外3D信息而是激活模型内在能力；不仅提高特定任务性能，还增强领域外泛化能力；强调基础的3D感知能力而非仅关注高层次的推理。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过视角学习任务和两阶段微调策略，成功激活了多模态大语言模型的空间推理能力，显著提升了模型在视觉和空间推理任务中的性能，特别是在领域外任务中的泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in Multimodal Large Language Models (MLLMs) havesignificantly improved 2D visual understanding, prompting interest in theirapplication to complex 3D reasoning tasks. However, it remains unclear whetherthese models can effectively capture the detailed spatial information requiredfor robust real-world performance, especially cross-view consistency, a keyrequirement for accurate 3D reasoning. Considering this issue, we introduceViewpoint Learning, a task designed to evaluate and improve the spatialreasoning capabilities of MLLMs. We present the Viewpoint-100K dataset,consisting of 100K object-centric image pairs with diverse viewpoints andcorresponding question-answer pairs. Our approach employs a two-stagefine-tuning strategy: first, foundational knowledge is injected to the baselineMLLM via Supervised Fine-Tuning (SFT) on Viewpoint-100K, resulting insignificant improvements across multiple tasks; second, generalization isenhanced through Reinforcement Learning using the Group Relative PolicyOptimization (GRPO) algorithm on a broader set of questions. Additionally, weintroduce a hybrid cold-start initialization method designed to simultaneouslylearn viewpoint representations and maintain coherent reasoning thinking.Experimental results show that our approach significantly activates the spatialreasoning ability of MLLM, improving performance on both in-domain andout-of-domain reasoning tasks. Our findings highlight the value of developingfoundational spatial skills in MLLMs, supporting future progress in robotics,autonomous systems, and 3D scene understanding.</description>
      <author>example@mail.com (Xiaoyu Zhan, Wenxuan Huang, Hao Sun, Xinyu Fu, Changfeng Ma, Shaosheng Cao, Bohan Jia, Shaohui Lin, Zhenfei Yin, Lei Bai, Wanli Ouyang, Yuanqi Li, Jie Guo, Yanwen Guo)</author>
      <guid isPermaLink="false">2511.01618v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>PixelVLA: Advancing Pixel-level Understanding in Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2511.01571v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17pages,7 figures, 5 tabels&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PixelVLA是一种新型Vision-Language-Action模型，支持像素级推理和多模态提示，通过两阶段自动注释流程生成Pixel-160K数据集，实验显示其比OpenVLA提高操作成功率10.1%-17.8%，同时仅需1.5%的预训练成本。&lt;h4&gt;背景&lt;/h4&gt;Vision-Language-Action models (VLAs)是学习通用视觉运动控制策略的有力工具，但当前VLAs主要在大规模图像-文本-动作数据上训练，存在两个关键限制：难以进行像素级场景理解，以及严重依赖文本提示，降低了在现实世界环境中的灵活性。&lt;h4&gt;目的&lt;/h4&gt;解决当前VLAs在像素级场景理解和现实世界应用灵活性方面的限制，引入支持像素级推理和多模态提示（文本和视觉输入）的VLA模型。&lt;h4&gt;方法&lt;/h4&gt;基于新的视觉运动指令调整框架，集成多尺度像素感知编码器和视觉提示编码器，提出两阶段自动注释流程生成Pixel-160K数据集，该数据集具有从现有机器人数据派生的像素级注释。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标准VLA基准测试和两个VLA模型变体上的实验表明，PixelVLA比OpenVLA提高操作成功率10.1%-17.8%，同时仅需OpenVLA 1.5%的预训练成本。&lt;h4&gt;结论&lt;/h4&gt;PixelVLA可以集成到现有VLAs中，在复杂环境中实现更准确、高效和多功能的机器人控制，数据集和代码将作为开源发布。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-动作模型（VLAs）正成为学习通用视觉运动控制策略的有力工具。然而，当前的VLAs主要在大规模的图像-文本-动作数据上训练，并在两个方面存在局限：（i）它们难以进行像素级场景理解，（ii）它们严重依赖文本提示，这降低了它们在现实世界环境中的灵活性。为应对这些挑战，我们引入了PixelVLA，这是第一个支持像素级推理和多模态提示（文本和视觉输入）的VLA模型。我们的方法建立在一种新的视觉运动指令调整框架上，该框架集成了多尺度像素感知编码器和视觉提示编码器。为了有效训练PixelVLA，我们进一步提出了一种两阶段自动注释流程，生成Pixel-160K，这是一个从现有机器人数据派生的大型像素级注释数据集。在三个标准VLA基准测试和两个VLA模型变体上的实验表明，PixelVLA比OpenVLA提高操作成功率10.1%-17.8%，同时仅需其1.5%的预训练成本。这些结果表明，PixelVLA可以集成到现有VLAs中，在复杂环境中实现更准确、高效和多功能的机器人控制。数据集和代码将作为开源发布。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有视觉-语言-动作模型(VLAs)的两个关键限制：1)缺乏像素级场景理解能力，2)过度依赖文本提示而缺乏对视觉提示的灵活处理。这些问题在现实中很重要，因为像素级理解对于机器人在复杂环境中进行精确操作至关重要，而多模态提示能力则能增强人机交互的灵活性和适应性，使机器人能更好地应对现实世界中的多样化任务和场景变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有VLAs的局限性，然后借鉴了视觉指令调优在视觉语言模型中的成功经验。具体设计上，作者参考了OpenVLA和π0等VLA模型的基本架构，利用SAM模型进行图像分割，并采用LoRA技术进行模型微调。通过引入多尺度像素感知编码器、视觉提示编码器和连续动作解码器这三个核心组件，以及设计两阶段自动注释管道来生成高质量数据集，作者构建了PixelVLA模型，实现了像素级理解和多模态提示能力。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过像素级理解和多模态提示能力增强VLAs在复杂环境中的空间感知和操作精度。整体实现流程分为三部分：1)架构设计，包含视觉编码器、视觉提示编码器、多尺度像素感知编码器、LLM骨干和连续动作解码器；2)数据生成，通过两阶段自动注释管道(夹爪感知区域提案阶段和多模态对象分割阶段)创建Pixel-160K数据集；3)训练流程，包括连续动作训练阶段和像素级理解增强阶段，后者使用LoRA适配进行微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多尺度像素感知编码器，实现像素级场景理解；2)视觉提示编码器，支持点、线、区域和掩码等多种视觉提示；3)两阶段自动注释管道，生成高质量像素级标注数据集Pixel-160K；4)连续动作解码器，直接预测连续动作表示。相比之前工作，PixelVLA突破了传统VLAs仅处理图像级别信息和依赖文本提示的限制，实现了更精细的空间理解和更灵活的人机交互方式。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PixelVLA通过引入像素级理解和多模态提示能力，显著提升了机器人在复杂环境中的操作精度和泛化能力，同时仅需1.5%的预训练成本就能实现10.1%~28.7%的操作成功率提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action models (VLAs) are emerging as powerful tools forlearning generalizable visuomotor control policies. However, current VLAs aremostly trained on large-scale image-text-action data and remain limited in twokey ways: (i) they struggle with pixel-level scene understanding, and (ii) theyrely heavily on textual prompts, which reduces their flexibility in real-worldsettings. To address these challenges, we introduce PixelVLA, the first VLAmodel designed to support both pixel-level reasoning and multimodal promptingwith text and visual inputs. Our approach is built on a new visuomotorinstruction tuning framework that integrates a multiscale pixel-aware encoderwith a visual prompting encoder. To train PixelVLA effectively, we furtherpropose a two-stage automated annotation pipeline that generates Pixel-160K, alarge-scale dataset with pixel-level annotations derived from existing robotdata. Experiments on three standard VLA benchmarks and two VLA model variantsshow that PixelVLA improves manipulation success rates by 10.1%-17.8% overOpenVLA, while requiring only 1.5% of its pretraining cost. These resultsdemonstrate that PixelVLA can be integrated into existing VLAs to enable moreaccurate, efficient, and versatile robot control in complex environments. Thedataset and code will be released as open source.</description>
      <author>example@mail.com (Wenqi Liang, Gan Sun, Yao He, Jiahua Dong, Suyan Dai, Ivan Laptev, Salman Khan, Yang Cong)</author>
      <guid isPermaLink="false">2511.01571v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Grounding Surgical Action Triplets with Instrument Instance Segmentation: A Dataset and Target-Aware Fusion Approach</title>
      <link>http://arxiv.org/abs/2511.00643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新的手术动作三元组空间定位方法，通过器械实例分割来实现空间定位的&lt;器械、动词、目标&gt;输出，并构建了相应的大规模数据集和评估基准。&lt;h4&gt;背景&lt;/h4&gt;现有的手术动作三元组识别方法仅限于帧级分类学习，无法可靠地将动作与特定器械实例关联。之前的空间定位方法主要依赖类激活图，缺乏精确性和鲁棒性，无法满足详细的器械-组织交互分析需求。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在手术动作三元组空间定位上的局限性，提出一种能够将动作与特定器械实例空间关联的统一框架。&lt;h4&gt;方法&lt;/h4&gt;提出了'三元组分割'任务，构建了CholecTriplet-Seg大规模数据集（包含30,000+标注帧），并设计了TargetFusionNet架构，通过目标感知融合机制扩展Mask2Former，融合弱解剖先验与器械实例查询以提高解剖目标预测准确性。&lt;h4&gt;主要发现&lt;/h4&gt;TargetFusionNet在识别、检测和三元组分割指标上均优于现有基线，证明强实例监督与弱目标先验相结合能显著提高手术动作理解的准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;三元组分割为手术动作三元组的空间定位建立了统一框架，所提出的基准和架构为更可解释的手术场景理解铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;理解手术器械-组织交互不仅需要识别哪个器械在哪个解剖目标上执行什么动作，还需要将这些交互在手术场景中空间定位。现有的手术动作三元组识别方法仅限于从帧级分类学习，无法可靠地将动作与特定的器械实例关联起来。之前的空间定位尝试主要依赖于类激活图，但这些方法缺乏详细的器械-组织交互分析所需的精确性和鲁棒性。为解决这一差距，我们提出了通过器械实例分割来定位手术动作三元组，简称为三元组分割，这是一个新的统一任务，可以产生空间定位的&lt;器械、动词、目标&gt;输出。我们首先介绍了CholecTriplet-Seg数据集，这是一个包含超过30,000个标注帧的大规模数据集，将器械实例掩码与动作动词和解剖目标标注相关联，并建立了首个用于强监督、实例级三元组定位和评估的基准。为了学习三元组分割，我们提出了TargetFusionNet，这是一种新颖的架构，它通过目标感知融合机制扩展了Mask2Former，以解决通过将弱解剖先验与器械实例查询融合来准确预测解剖目标的挑战。在识别、检测和三元组分割指标上的评估表明，TargetFusionNet始终优于现有基线，证明强实例监督与弱目标先验相结合显著提高了手术动作理解的准确性和鲁棒性。三元组分割为空间定位手术动作三元组建立了统一框架。所提出的基准和架构为更可解释的手术场景理解铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-11-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding surgical instrument-tissue interactions requires not onlyidentifying which instrument performs which action on which anatomical target,but also grounding these interactions spatially within the surgical scene.Existing surgical action triplet recognition methods are limited to learningfrom frame-level classification, failing to reliably link actions to specificinstrument instances.Previous attempts at spatial grounding have primarilyrelied on class activation maps, which lack the precision and robustnessrequired for detailed instrument-tissue interaction analysis.To address thisgap, we propose grounding surgical action triplets with instrument instancesegmentation, or triplet segmentation for short, a new unified task whichproduces spatially grounded &lt;instrument, verb, target&gt; outputs.We start bypresenting CholecTriplet-Seg, a large-scale dataset containing over 30,000annotated frames, linking instrument instance masks with action verb andanatomical target annotations, and establishing the first benchmark forstrongly supervised, instance-level triplet grounding and evaluation.To learntriplet segmentation, we propose TargetFusionNet, a novel architecture thatextends Mask2Former with a target-aware fusion mechanism to address thechallenge of accurate anatomical target prediction by fusing weak anatomypriors with instrument instance queries.Evaluated across recognition,detection, and triplet segmentation metrics, TargetFusionNet consistentlyimproves performance over existing baselines, demonstrating that stronginstance supervision combined with weak target priors significantly enhancesthe accuracy and robustness of surgical action understanding.Tripletsegmentation establishes a unified framework for spatially grounding surgicalaction triplets. The proposed benchmark and architecture pave the way for moreinterpretable, surgical scene understanding.</description>
      <author>example@mail.com (Oluwatosin Alabi, Meng Wei, Charlie Budd, Tom Vercauteren, Miaojing Shi)</author>
      <guid isPermaLink="false">2511.00643v1</guid>
      <pubDate>Wed, 05 Nov 2025 17:09:54 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid-Task Meta-Learning: A GNN Approach for Scalable and Transferable Bandwidth Allocation</title>
      <link>http://arxiv.org/abs/2401.10253v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于深度学习的带宽分配策略，具有可扩展性和可转移性特点。通过使用图神经网络和混合任务元学习算法，实现了在不同通信场景下的高效带宽分配。&lt;h4&gt;背景&lt;/h4&gt;随着用户数量和通信场景的多样化，传统的带宽分配方法面临可扩展性和泛化能力不足的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种既可随用户数量扩展，又能适应不同通信场景（如非平稳无线信道、不同服务质量要求和动态可用资源）的带宽分配策略。&lt;h4&gt;方法&lt;/h4&gt;1. 使用图神经网络(GNN)表示带宽分配策略，确保参数数量不随用户数量变化；2. 开发混合任务元学习(HML)算法，在元训练阶段使用不同通信场景训练GNN初始参数；3. 在元测试阶段使用少量样本对未见过的通信场景进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;1. HML方法比现有基准提高初始性能8.79%，样本效率提高73%；2. 微调后的GNN策略以更低推理复杂度获得接近最优策略的奖励；3. HML比最优迭代算法减少约200到2000倍的计算时间。&lt;h4&gt;结论&lt;/h4&gt;基于GNN和HML的带宽分配策略在性能、效率和计算复杂度方面均优于现有方法，为实际通信系统中的资源分配提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们开发了一种基于深度学习的带宽分配策略，该策略：1)随用户数量可扩展；2)可转移到不同的通信场景，如非平稳无线信道、不同的服务质量要求和动态可用资源。为了支持可扩展性，带宽分配策略由图神经网络(GNN)表示，其训练参数数量不随用户数量变化。为了实现GNN的泛化能力，我们开发了一种混合任务元学习(HML)算法，在元训练期间使用不同的通信场景训练GNN的初始参数。接下来，在元测试期间，使用少量样本对未见过的通信场景微调GNN。仿真结果表明，与现有基准相比，我们的HML方法可以提高初始性能8.79%，样本效率提高73%。微调后，我们的次优GNN策略与使用迭代优化获得的最优策略相比，可以以低得多的推理复杂度获得几乎相同的奖励。数值结果验证，与最优迭代算法相比，我们的HML可以减少约200到2000倍的计算时间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2023-12-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we develop a deep learning-based bandwidth allocation policythat is: 1) scalable with the number of users and 2) transferable to differentcommunication scenarios, such as non-stationary wireless channels, differentquality-of-service (QoS) requirements, and dynamically available resources. Tosupport scalability, the bandwidth allocation policy is represented by a graphneural network (GNN), with which the number of training parameters does notchange with the number of users. To enable the generalization of the GNN, wedevelop a hybrid-task meta-learning (HML) algorithm that trains the initialparameters of the GNN with different communication scenarios duringmeta-training. Next, during meta-testing, a few samples are used to fine-tunethe GNN with unseen communication scenarios. Simulation results demonstratethat our HML approach can improve the initial performance by 8.79%, and sampleefficiency by 73%, compared with existing benchmarks. After fine-tuning, ournear-optimal GNN-based policy can achieve close to the same reward with muchlower inference complexity compared to the optimal policy obtained usingiterative optimization. Numerical results validate that our HML can reduce thecomputation time by approximately 200 to 2000 times than the optimal iterativealgorithm.</description>
      <author>example@mail.com (Xin Hao, Changyang She, Phee Lep Yeoh, Yuhong Liu, Branka Vucetic, Yonghui Li)</author>
      <guid isPermaLink="false">2401.10253v3</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
  <item>
      <title>Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</title>
      <link>http://arxiv.org/abs/2510.25760v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文对大型模型在多模态空间推理任务方面进行了全面综述，分类了多模态大型语言模型的最新进展，并介绍了开放基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;人类具有通过视觉和声音等多模态观察理解空间的空间推理能力。大型多模态推理模型通过学习感知和推理，在多样化的空间任务中展现出有前景的性能，但系统综述和公开可用的基准仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供对大型模型多模态空间推理任务的全面回顾，分类多模态大型语言模型(MLLMs)的最新进展，并介绍用于评估的开放基准。&lt;h4&gt;方法&lt;/h4&gt;从概述一般空间推理开始，重点关注训练后技术、可解释性和架构。研究空间关系推理、场景和布局理解、3D空间中的视觉问答和定位。回顾具身AI进展，包括视觉语言导航和动作模型。考虑音频和第一人称视频等新兴模态。&lt;h4&gt;主要发现&lt;/h4&gt;多模态大型模型在空间推理任务中展现出有前景的性能，但仍需要更多系统研究和公开基准来评估这些模型的能力。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为多模态空间推理这一不断发展的领域奠定了坚实基础并提供了见解。相关更新信息、代码和开放基准的实现可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;人类具有空间推理能力，使他们能够通过视觉和声音等多模态观察理解空间。大型多模态推理模型通过学习感知和推理扩展了这些能力，在多样化的空间任务中展现出有前景的性能。然而，这些模型的系统综述和公开可用的基准仍然有限。在这篇综述中，我们提供了对大型模型多模态空间推理任务的全面回顾，分类了多模态大型语言模型(MLLMs)的最新进展，并介绍了用于评估的开放基准。我们首先概述一般空间推理，重点关注训练后技术、可解释性和架构。除了传统的2D任务外，我们还研究了空间关系推理、场景和布局理解，以及3D空间中的视觉问答和定位。我们还回顾了具身AI的进展，包括视觉语言导航和动作模型。此外，我们还考虑了音频和第一人称视频等新兴模态，这些模态通过新型传感器促进新的空间理解。我们相信这篇综述为多模态空间推理这一不断发展的领域奠定了坚实基础并提供了见解。关于这篇综述的更新信息、代码和开放基准的实现可在https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning上找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态空间推理领域缺乏系统性回顾和公开可用基准的问题。这个问题很重要，因为人类具有通过视觉、声音等多模态感知理解空间的能力，而大型多模态模型虽已展现出色性能，但缺乏系统评估和比较标准，阻碍了该领域的快速发展。空间推理对导航、物体关系理解和复杂场景交互等实际应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过构建分类框架（如图2所示）组织多模态空间推理研究，从一般多模态空间推理到3D空间推理，再到具身AI和新兴模态。作者借鉴了多模态模型、空间推理和具身AI等领域的现有工作，同时发现前人研究存在空白，如Wang等人专注于单模态任务，Ke等人未深入多模态空间推理，Bi等人未提供系统评估框架。作者通过系统性文献回顾和基准构建填补了这些空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统回顾和基准测试促进多模态空间推理研究发展。流程包括：1)明确定义多模态空间推理任务和评估协议；2)构建分类框架涵盖2D到3D、静态到动态、传统到新兴模态；3)全面回顾文献，包括后训练技术、可解释性、架构设计等；4)开发开放基准评估模型性能；5)提供代码和实现资源促进进一步研究。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提供多模态空间推理的系统性全面回顾；2)构建详细分类框架（图2）覆盖广泛任务；3)引入开放基准标准化评估；4)整合空间推理与具身AI；5)提供资源促进研究。相比前人工作，本文更全面系统，不仅涵盖2D到3D任务，还整合新兴模态和具身AI，并提供实用评估基准，而非仅关注单一领域或理论实现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性的文献回顾、分类框架构建和开放基准引入，为多模态空间推理领域提供了坚实基础和评估标准，促进了该领域的研究发展和实际应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess spatial reasoning abilities that enable them to understandspaces through multimodal observations, such as vision and sound. Largemultimodal reasoning models extend these abilities by learning to perceive andreason, showing promising performance across diverse spatial tasks. However,systematic reviews and publicly available benchmarks for these models remainlimited. In this survey, we provide a comprehensive review of multimodalspatial reasoning tasks with large models, categorizing recent progress inmultimodal large language models (MLLMs) and introducing open benchmarks forevaluation. We begin by outlining general spatial reasoning, focusing onpost-training techniques, explainability, and architecture. Beyond classical 2Dtasks, we examine spatial relationship reasoning, scene and layoutunderstanding, as well as visual question answering and grounding in 3D space.We also review advances in embodied AI, including vision-language navigationand action models. Additionally, we consider emerging modalities such as audioand egocentric video, which contribute to novel spatial understanding throughnew sensors. We believe this survey establishes a solid foundation and offersinsights into the growing field of multimodal spatial reasoning. Updatedinformation about this survey, codes and implementation of the open benchmarkscan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</description>
      <author>example@mail.com (Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu)</author>
      <guid isPermaLink="false">2510.25760v2</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
    <item>
      <title>Best Practices for Biorisk Evaluations on Open-Weight Bio-Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27629v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 Pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为eval的框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。研究通过序列建模、突变效应预测和毒力预测三个角度评估模型对病毒的理解能力，发现当前的数据过滤方法可能不够有效，被排除的知识可以通过微调恢复，且双重使用信号可能已存在于预训练表示中。&lt;h4&gt;背景&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。这些模型有加速科学研究和药物开发的巨大潜力，但也可能被恶意行为者用于开发更致命的生物武器。当前的方法主要关注在预训练过程中过滤生物危害数据，但其有效性尚不明确。&lt;h4&gt;目的&lt;/h4&gt;解决当前过滤生物危害数据方法的有效性不明确问题，特别是针对可能微调这些模型进行恶意使用的有决心的行为者。提出一个框架来评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了名为eval的框架，通过三个角度评估模型的病毒理解能力：序列建模、突变效应预测和毒力预测。&lt;h4&gt;主要发现&lt;/h4&gt;当前过滤实践可能不是特别有效；在某些情况下，被排除的知识可以通过微调快速恢复，并在序列建模中表现出更广泛的泛化能力；双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来引出。&lt;h4&gt;结论&lt;/h4&gt;数据过滤作为独立程序面临挑战，强调需要对开放权重生物基础模型进行更深入的安全和安保策略研究。&lt;h4&gt;翻译&lt;/h4&gt;开放权重生物基础模型呈现双重使用困境。虽然这些模型在加速科学研究和药物开发方面展现出巨大潜力，但也可能被恶意行为者用于开发更致命的生物武器。为了减轻这些模型带来的风险，当前的方法主要关注在预训练过程中过滤生物危害数据。然而，这种方法的有效性仍不明确，特别是针对可能微调这些模型进行恶意使用的有决心的行为者。为了解决这一空白，我们提出了eval框架，用于评估旨在减少生物基础模型双重使用能力的程序的鲁棒性。eval通过序列建模、突变效应预测和毒力预测三个角度评估模型对病毒的理解能力。我们的结果表明，当前的过滤实践可能不是特别有效：在某些情况下，被排除的知识可以通过微调快速恢复，并在序列建模中表现出更广泛的泛化能力。此外，双重使用信号可能已经存在于预训练表示中，可以通过简单的线性探测来引出。这些发现强调了数据过滤作为独立程序所面临的挑战，突显了对开放权重生物基础模型进行更深入的安全和安保策略研究的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Open-weight bio-foundation models present a dual-use dilemma. While holdinggreat promise for accelerating scientific research and drug development, theycould also enable bad actors to develop more deadly bioweapons. To mitigate therisk posed by these models, current approaches focus on filtering biohazardousdata during pre-training. However, the effectiveness of such an approachremains unclear, particularly against determined actors who might fine-tunethese models for malicious use. To address this gap, we propose \eval, aframework to evaluate the robustness of procedures that are intended to reducethe dual-use capabilities of bio-foundation models. \eval assesses models'virus understanding through three lenses, including sequence modeling,mutational effects prediction, and virulence prediction. Our results show thatcurrent filtering practices may not be particularly effective: Excludedknowledge can be rapidly recovered in some cases via fine-tuning, and exhibitsbroader generalizability in sequence modeling. Furthermore, dual-use signalsmay already reside in the pretrained representations, and can be elicited viasimple linear probing. These findings highlight the challenges of datafiltering as a standalone procedure, underscoring the need for further researchinto robust safety and security strategies for open-weight bio-foundationmodels.</description>
      <author>example@mail.com (Boyi Wei, Zora Che, Nathaniel Li, Udari Madhushani Sehwag, Jasper Götting, Samira Nedungadi, Julian Michael, Summer Yue, Dan Hendrycks, Peter Henderson, Zifan Wang, Seth Donoughe, Mantas Mazeika)</author>
      <guid isPermaLink="false">2510.27629v2</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
    <item>
      <title>Image Hashing via Cross-View Code Alignment in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27584v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CroVCA（Cross-View Code Alignment），一种简单统一的原则，用于学习在不同语义对齐视图中保持一致的二进制码。通过HashCoder实现，该方法仅需5个训练周期就能达到最先进的结果，在16位时表现尤为出色，具有高效率、适应性和广泛适用性。&lt;h4&gt;背景&lt;/h4&gt;大规模检索需要紧凑且具有判别性的表示。基础模型提供强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高。哈希提供了一种高效的替代方案，但现有方法通常依赖复杂流程、多目标项、针对单一学习范式设计以及长时间训练。&lt;h4&gt;目的&lt;/h4&gt;引入CroVCA，一种简单统一的原则，用于学习在不同语义对齐视图中保持一致的二进制码，实现高效、快速、准确的哈希方法。&lt;h4&gt;方法&lt;/h4&gt;使用单个二元交叉熵损失强制对齐，编码率最大化作为反崩溃正则化器促进平衡和多样化的码。设计了HashCoder，一个轻量级的MLP哈希网络，带有最终的批归一化层以强制平衡的码。HashCoder可用作冻结嵌入上的探测头，或通过LoRA微调有效地适配编码器。&lt;h4&gt;主要发现&lt;/h4&gt;CroVCA在基准测试中取得了最先进的结果，只需5个训练周期。在16位时表现特别好，例如在COCO上的无监督哈希在不到2分钟内完成，在ImageNet100上的监督哈希在单个GPU上约3分钟内完成。&lt;h4&gt;结论&lt;/h4&gt;CroVCA展示了其效率、适应性和广泛的适用性。&lt;h4&gt;翻译&lt;/h4&gt;高效的大规模检索需要既紧凑又具有判别性的表示。基础模型提供了强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高昂。哈希通过使用二进制码实现快速汉明距离搜索，提供了一种高效的替代方案，然而现有方法通常依赖复杂流程、多目标项、针对单一学习范式的设计以及长时间的训练。我们引入了CroVCA（Cross-View Code Alignment），这是一种学习在不同语义对齐视图中保持一致的二进制码的简单统一原则。单个二元交叉熵损失强制对齐，而编码率最大化作为反崩溃正则化器促进平衡和多样化的码。为此，我们设计了HashCoder，一个带有最终批归一化层以强制平衡码的轻量级MLP哈希网络。HashCoder可以用作冻结嵌入上的探测头，或通过LoRA微调有效地适配编码器。在基准测试中，CroVCA仅需5个训练周期就取得了最先进的结果。在16位时，它表现特别好——例如，在单个GPU上，COCO上的无监督哈希在不到2分钟内完成，ImageNet100上的监督哈希在约3分钟内完成。这些结果凸显了CroVCA的效率、适应性和广泛的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient large-scale retrieval requires representations that are bothcompact and discriminative. Foundation models provide powerful visual andmultimodal embeddings, but nearest neighbor search in these high-dimensionalspaces is computationally expensive. Hashing offers an efficient alternative byenabling fast Hamming distance search with binary codes, yet existingapproaches often rely on complex pipelines, multi-term objectives, designsspecialized for a single learning paradigm, and long training times. Weintroduce CroVCA (Cross-View Code Alignment), a simple and unified principlefor learning binary codes that remain consistent across semantically alignedviews. A single binary cross-entropy loss enforces alignment, while coding-ratemaximization serves as an anti-collapse regularizer to promote balanced anddiverse codes. To implement this, we design HashCoder, a lightweight MLPhashing network with a final batch normalization layer to enforce balancedcodes. HashCoder can be used as a probing head on frozen embeddings or to adaptencoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achievesstate-of-the-art results in just 5 training epochs. At 16 bits, it particularlywell-for instance, unsupervised hashing on COCO completes in under 2 minutesand supervised hashing on ImageNet100 in about 3 minutes on a single GPU. Theseresults highlight CroVCA's efficiency, adaptability, and broad applicability.</description>
      <author>example@mail.com (Ilyass Moummad, Kawtar Zaher, Hervé Goëau, Alexis Joly)</author>
      <guid isPermaLink="false">2510.27584v2</guid>
      <pubDate>Tue, 04 Nov 2025 14:08:49 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Genomics into Multimodal EHR Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23639v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录（EHR）基础模型，整合多基因风险评分（PRS）作为基础数据模态，超越了传统EHR-only方法，构建更全面的健康档案。该多模态框架利用'全民研究计划'的多样化数据，学习临床数据和遗传易感性间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统电子健康记录方法仅使用临床数据，未充分利用遗传信息，需要整合多种数据类型构建更全面的健康档案。&lt;h4&gt;目的&lt;/h4&gt;开发创新的EHR基础模型，整合PRS作为基础数据模态，构建更全面的健康档案，学习临床数据和遗传易感性间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;利用'全民研究计划'的广泛多样化数据，开发多模态框架整合PRS和EHR数据，将生成式人工智能进展应用于EHR基础模型领域，使用迁移学习进行定制分类任务，并在AoU数据上评估模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;模型对多种疾病（特别是2型糖尿病）的发作具有预测价值；展示了PRS与EHR数据间的相互作用；架构具有多样性和效率，适用于迁移学习；能解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解。&lt;h4&gt;结论&lt;/h4&gt;这种整合PRS的EHR基础模型方法超越了传统方法，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定基础，对疾病预测和个性化治疗具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录（EHR）基础模型，该模型将多基因风险评分（PRS）整合为基础数据模态，超越了传统的仅基于EHR的方法，以构建更全面的健康档案。利用'全民研究计划'（AoU）的广泛多样化数据，这种多模态框架旨在学习临床数据和遗传易感性之间的复杂关系。该方法将生成式人工智能的进展扩展到EHR基础模型领域，增强了预测能力和可解释性。在AoU数据上的评估展示了该模型对多种疾病（特别是2型糖尿病）发作的预测价值，并说明了PRS与EHR数据之间的相互作用。该研究还探讨了针对定制分类任务的迁移学习，展示了架构的多样性和效率。这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an innovative Electronic Health Record (EHR) foundationmodel that integrates Polygenic Risk Scores (PRS) as a foundational datamodality, moving beyond traditional EHR-only approaches to build more holistichealth profiles. Leveraging the extensive and diverse data from the All of Us(AoU) Research Program, this multimodal framework aims to learn complexrelationships between clinical data and genetic predispositions. Themethodology extends advancements in generative AI to the EHR foundation modelspace, enhancing predictive capabilities and interpretability. Evaluation onAoU data demonstrates the model's predictive value for the onset of variousconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplaybetween PRS and EHR data. The work also explores transfer learning for customclassification tasks, showcasing the architecture's versatility and efficiency.This approach is pivotal for unlocking new insights into disease prediction,proactive health management, risk stratification, and personalized treatmentstrategies, laying the groundwork for more personalized, equitable, andactionable real-world evidence generation in healthcare.</description>
      <author>example@mail.com (Jonathan Amar, Edward Liu, Alessandra Breschi, Liangliang Zhang, Pouya Kheradpour, Sylvia Li, Lisa Soleymani Lehmann, Alessandro Giulianelli, Matt Edwards, Yugang Jia, David Nola, Raghav Mani, Pankaj Vats, Jesse Tetreault, T. J. Chen, Cory Y. McLean)</author>
      <guid isPermaLink="false">2510.23639v2</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
  <item>
      <title>Validity Is What You Need</title>
      <link>http://arxiv.org/abs/2510.27628v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文讨论了Agentic AI的定义、特性和验证方法，提出Agentic AI是一种软件交付机制，类似于SaaS，使应用程序能够在复杂企业环境中自主工作。研究指出Agentic AI系统主要是应用程序而非基础模型，其成功依赖于用户验证，并强调在良好验证措施下，可用更简单模型替代复杂基础模型。&lt;h4&gt;背景&lt;/h4&gt;AI代理在计算机科学领域早已被讨论和研究，但当前的Agentic AI系统是新的发展。大型语言模型(LLMs)等基础模型的进步推动了Agentic AI的发展。&lt;h4&gt;目的&lt;/h4&gt;考虑其他Agentic AI的定义并提出一个新的现实主义定义。&lt;h4&gt;方法&lt;/h4&gt;将Agentic AI定义为软件交付机制，类似于软件即服务(SaaS)，使应用程序能够在复杂的企业环境中自主工作。&lt;h4&gt;主要发现&lt;/h4&gt;Agentic AI系统主要是应用程序而非基础模型，其成功依赖于最终用户和主要利益相关者的验证；验证应用程序的工具与技术评估基础模型的技术不同；在有良好验证措施的情况下，基础模型通常可以用更简单、更快、更可解释的模型替代。&lt;h4&gt;结论&lt;/h4&gt;对于Agentic AI，有效性是关键需求，LLMs是可能实现这一目标的一种选择。&lt;h4&gt;翻译&lt;/h4&gt;虽然AI代理在计算机科学领域早已被讨论和研究，但今天的Agentic AI系统是全新的。我们考虑了其他Agentic AI的定义，并提出了一个新的现实主义定义。Agentic AI是一种软件交付机制，类似于软件即服务(SaaS)，它使应用程序能够在复杂的企业环境中自主工作。大型语言模型(LLMs)等基础模型的最新进展推动了Agentic AI的发展。然而，我们注意到Agentic AI系统主要是应用程序，而非基础模型，因此其成功依赖于最终用户和主要利益相关者的验证。主要用户验证其应用程序所需的工具与技术用于评估基础模型的技术有很大不同。讽刺的是，在有良好验证措施的情况下，在许多情况下基础模型可以用更简单、更快、更可解释的模型来替代，这些模型处理核心逻辑。当涉及到Agentic AI时，有效性是您所需要的。LLMs是实现这一目标的一种选择。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While AI agents have long been discussed and studied in computer science,today's Agentic AI systems are something new. We consider other definitions ofAgentic AI and propose a new realist definition. Agentic AI is a softwaredelivery mechanism, comparable to software as a service (SaaS), which puts anapplication to work autonomously in a complex enterprise setting. Recentadvances in large language models (LLMs) as foundation models have drivenexcitement in Agentic AI. We note, however, that Agentic AI systems areprimarily applications, not foundations, and so their success depends onvalidation by end users and principal stakeholders. The tools and techniquesneeded by the principal users to validate their applications are quitedifferent from the tools and techniques used to evaluate foundation models.Ironically, with good validation measures in place, in many cases thefoundation models can be replaced with much simpler, faster, and moreinterpretable models that handle core logic. When it comes to Agentic AI,validity is what you need. LLMs are one option that might achieve it.</description>
      <author>example@mail.com (Sebastian Benthall, Andrew Clark)</author>
      <guid isPermaLink="false">2510.27628v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Image Hashing via Cross-View Code Alignment in the Age of Foundation Models</title>
      <link>http://arxiv.org/abs/2510.27584v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了CroVCA（Cross-View Code Alignment）方法，一种简单统一的原则，用于学习在语义对齐视图中保持一致的二进制码，用于高效的大规模检索。&lt;h4&gt;背景&lt;/h4&gt;高效的大规模检索需要既紧凑又有区分度的表示。基础模型提供强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高。现有哈希方法通常依赖复杂的流程、多目标函数、专为单一学习范式设计且训练时间长。&lt;h4&gt;目的&lt;/h4&gt;开发一种简单、高效且适应性强的方法来学习二进制码，用于快速的大规模检索，同时减少训练时间和计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;引入CroVCA原则，使用单个二元交叉熵损失强制语义对齐视图之间的码对齐，并采用编码率最大化作为防坍塌正则化子。设计了HashCoder，一种轻量级的MLP哈希网络，带有最终的批量归一化层以强制平衡的码。HashCoder可作为冻结嵌入上的探测头或通过LoRA微调适应编码器。&lt;h4&gt;主要发现&lt;/h4&gt;CroVCA在基准测试中仅需5个训练周期就取得了最先进的结果。在16位时表现特别优异，例如在COCO上的无监督哈希在单GPU上不到2分钟完成，在ImageNet100上的监督哈希约3分钟完成。&lt;h4&gt;结论&lt;/h4&gt;CroVCA提供了一种高效、适应性强的哈希方法，具有广泛的适用性，显著减少了训练时间和计算资源需求，同时保持了高质量的检索性能。&lt;h4&gt;翻译&lt;/h4&gt;高效的大规模检索需要既紧凑又有区分度的表示。基础模型提供强大的视觉和多模态嵌入，但这些高维空间中的最近邻搜索计算成本高。哈希通过使用二进制码实现快速的汉明距离搜索，提供了一种高效的替代方案。然而，现有方法通常依赖复杂的流程、多目标函数、专为单一学习范式设计且训练时间长。我们介绍了CroVCA（Cross-View Code Alignment），一种简单统一的原则，用于学习在语义对齐视图中保持一致的二进制码。单个二元交叉熵损失强制对齐，而编码率最大化作为防坍塌正则化子促进平衡和多样化的码。为此，我们设计了HashCoder，一种轻量级的MLP哈希网络，带有最终的批量归一化层以强制平衡的码。HashCoder可用作冻结嵌入上的探测头，或通过LoRA微调高效适应编码器。在基准测试中，CroVCA仅需5个训练周期就取得了最先进的结果。在16位时，性能特别好，例如在COCO上的无监督哈希在单GPU上不到2分钟完成，在ImageNet100上的监督哈希约3分钟完成。这些结果突显了CroVCA的高效性、适应性和广泛适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient large-scale retrieval requires representations that are bothcompact and discriminative. Foundation models provide powerful visual andmultimodal embeddings, but nearest neighbor search in these high-dimensionalspaces is computationally expensive. Hashing offers an efficient alternative byenabling fast Hamming distance search with binary codes, yet existingapproaches often rely on complex pipelines, multi-term objectives, designsspecialized for a single learning paradigm, and long training times. Weintroduce CroVCA (Cross-View Code Alignment), a simple and unified principlefor learning binary codes that remain consistent across semantically alignedviews. A single binary cross-entropy loss enforces alignment, while coding-ratemaximization serves as an anti-collapse regularizer to promote balanced anddiverse codes. To implement this, we design HashCoder, a lightweight MLPhashing network with a final batch normalization layer to enforce balancedcodes. HashCoder can be used as a probing head on frozen embeddings or to adaptencoders efficiently via LoRA fine-tuning. Across benchmarks, CroVCA achievesstate-of-the-art results in just 5 training epochs. At 16 bits, it particularlywell-for instance, unsupervised hashing on COCO completes in under 2 minutesand supervised hashing on ImageNet100 in about 3 minutes on a single GPU. Theseresults highlight CroVCA's efficiency, adaptability, and broad applicability.</description>
      <author>example@mail.com (Ilyass Moummad, Kawtar Zaher, Hervé Goëau, Alexis Joly)</author>
      <guid isPermaLink="false">2510.27584v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Toward Accurate Long-Horizon Robotic Manipulation: Language-to-Action with Foundation Models via Scene Graphs</title>
      <link>http://arxiv.org/abs/2510.27558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种无需领域特定训练的机器人操作框架，该框架利用预训练基础模型进行机器人操作，通过集成现成模型、多模态感知和通用推理模型实现稳健任务排序，并通过动态场景图提供空间感知和环境一致性推理能力。&lt;h4&gt;背景&lt;/h4&gt;机器人操作领域通常需要领域特定的训练，而预训练基础模型在多个领域已显示出强大能力，但如何有效利用这些模型进行机器人操作尚不明确。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需领域特定训练的机器人操作框架，有效利用预训练基础模型进行机器人操作任务。&lt;h4&gt;方法&lt;/h4&gt;集成现成的预训练基础模型，结合多模态感知和通用推理模型，动态维护场景图提供空间感知，通过场景图实现环境一致性推理，并在桌面机器人操作实验中评估框架性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列桌面机器人操作实验评估，该框架展示了在现成基础模型之上直接构建机器人操作系统的潜力。&lt;h4&gt;结论&lt;/h4&gt;该框架证明了利用预训练基础模型进行机器人操作的可行性，无需领域特定训练，为构建机器人操作系统提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一个框架，该框架利用预训练的基础模型进行机器人操作，无需领域特定的训练。该框架集成现成的模型，结合来自基础模型的多模态感知和能够进行稳健任务排序的通用推理模型。在框架内动态维护的场景图提供了空间感知能力，并使对环境的一致推理成为可能。通过一系列桌面机器人操作实验对该框架进行了评估，结果凸显了在现成基础模型之上直接构建机器人操作系统的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a framework that leverages pre-trained foundation modelsfor robotic manipulation without domain-specific training. The frameworkintegrates off-the-shelf models, combining multimodal perception fromfoundation models with a general-purpose reasoning model capable of robust tasksequencing. Scene graphs, dynamically maintained within the framework, providespatial awareness and enable consistent reasoning about the environment. Theframework is evaluated through a series of tabletop robotic manipulationexperiments, and the results highlight its potential for building roboticmanipulation systems directly on top of off-the-shelf foundation models.</description>
      <author>example@mail.com (Sushil Samuel Dinesh, Shinkyu Park)</author>
      <guid isPermaLink="false">2510.27558v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MapSAM2: Adapting SAM2 for Automatic Segmentation of Historical Map Images and Time Series</title>
      <link>http://arxiv.org/abs/2510.27547v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MapSAM2是一个基于视觉基础模型的统一框架，用于自动分割历史地图图像和时间序列，通过将图像和时间序列视为视频处理，有效解决了历史地图分析中的风格多样性和数据稀缺性问题。&lt;h4&gt;背景&lt;/h4&gt;历史地图是记录不同时间段地理特征的有价值档案，但由于其风格多样性和标注训练数据稀缺，自动分析面临挑战；从历史地图时间序列构建链接时空数据集更为耗时耗力，需要综合多张地图信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架自动分割历史地图图像和时间序列，支持建筑物年代测定、道路网络和聚落发展分析、环境变化研究等多种应用。&lt;h4&gt;方法&lt;/h4&gt;MapSAM2基于视觉基础模型，使用少样本微调适应不同分割任务；关键创新是将历史地图图像和时间序列都视为视频处理，对于图像将图块组作为视频处理使内存注意力机制整合上下文线索，对于时间序列引入Siegfried建筑时间序列数据集并提出通过模拟时间变换从单年地图生成伪时间序列以减少标注成本。&lt;h4&gt;主要发现&lt;/h4&gt;MapSAM2能有效学习时间关联，在有限监督或使用伪视频的情况下可准确分割和时间序列中的建筑物，提高了几何准确性，特别是对于区域特征。&lt;h4&gt;结论&lt;/h4&gt;MapSAM2框架成功解决了历史地图图像和时间序列的自动分割问题，作者将发布数据集和代码以支持未来研究。&lt;h4&gt;翻译&lt;/h4&gt;历史地图是记录不同时间段地理特征独特且有价值的档案。然而，由于其风格多样性和标注训练数据的稀缺性，历史地图图像的自动分析仍然是一个重大挑战。从历史地图时间序列构建链接时空数据集更加耗时耗力，因为它需要综合多张地图的信息。这类数据集对于建筑物年代测定、道路网络和聚落发展分析、环境变化研究等应用至关重要。我们提出了MapSAM2，一个用于自动分割历史地图图像和时间序列的统一框架。MapSAM2基于视觉基础模型，使用少样本微调适应不同的分割任务。我们的关键创新是将历史地图图像和时间序列都视为视频处理。对于图像，我们将一组图块作为视频处理，使内存注意力机制能够整合来自相似图块的上下文线索，从而提高了几何准确性，特别是对于区域特征。对于时间序列，我们引入了标注的Siegfried建筑时间序列数据集，并为了减少标注成本，提出了通过模拟常见的时间变换从单年地图生成伪时间序列。实验结果表明，MapSAM2能有效学习时间关联，并在有限监督或使用伪视频的情况下准确分割和时间序列中的建筑物。我们将发布我们的数据集和代码以支持未来研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Historical maps are unique and valuable archives that document geographicfeatures across different time periods. However, automated analysis ofhistorical map images remains a significant challenge due to their widestylistic variability and the scarcity of annotated training data. Constructinglinked spatio-temporal datasets from historical map time series is even moretime-consuming and labor-intensive, as it requires synthesizing informationfrom multiple maps. Such datasets are essential for applications such as datingbuildings, analyzing the development of road networks and settlements, studyingenvironmental changes etc. We present MapSAM2, a unified framework forautomatically segmenting both historical map images and time series. Built on avisual foundation model, MapSAM2 adapts to diverse segmentation tasks withfew-shot fine-tuning. Our key innovation is to treat both historical map imagesand time series as videos. For images, we process a set of tiles as a video,enabling the memory attention mechanism to incorporate contextual cues fromsimilar tiles, leading to improved geometric accuracy, particularly for arealfeatures. For time series, we introduce the annotated Siegfried Building TimeSeries Dataset and, to reduce annotation costs, propose generating pseudo timeseries from single-year maps by simulating common temporal transformations.Experimental results show that MapSAM2 learns temporal associations effectivelyand can accurately segment and link buildings in time series under limitedsupervision or using pseudo videos. We will release both our dataset and codeto support future research.</description>
      <author>example@mail.com (Xue Xia, Randall Balestriero, Tao Zhang, Yixin Zhou, Andrew Ding, Dev Saini, Lorenz Hurni)</author>
      <guid isPermaLink="false">2510.27547v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Generic Time Series Foundation Models for EEG Classification</title>
      <link>http://arxiv.org/abs/2510.27522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作探索了时间序列基础模型在脑电图(EEG)任务中的应用潜力，发现即使是在非神经数据或合成信号上预训练的通用模型也能有效地迁移到EEG任务，并且性能超过特定于EEG的模型。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型正在成为强大的通用主干网络，但这些模型在特定领域生物医学信号（如脑电图EEG）方面的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;研究最近提出的时间序列分类基础模型在不同EEG任务上的适用性，包括运动想象分类和睡眠阶段预测。&lt;h4&gt;方法&lt;/h4&gt;测试两种预训练方案：(a)在来自多个领域的异构真实世界时间序列上进行预训练，(b)在纯合成数据上进行预训练。&lt;h4&gt;主要发现&lt;/h4&gt;两种变体都表现出强大的性能，一致优于广泛使用的卷积基线EEGNet和最新的特定于EEG的基础模型CBraMod，表明通用时间序列基础模型能有效迁移到EEG任务。&lt;h4&gt;结论&lt;/h4&gt;利用跨领域预训练模型进行脑信号分析具有前景，EEG可能会从更广泛的时间序列文献的进步中受益。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型正在作为强大的通用主干网络出现，然而它们在特定领域生物医学信号（如脑电图EEG）方面的潜力仍然相当未被探索。在这项工作中，我们研究了一种最近提出的时间序列分类基础模型在不同EEG任务（如运动想象分类和睡眠阶段预测）上的适用性。我们测试了两种预训练方案：(a)在来自多个领域的异构真实世界时间序列上预训练，(b)在纯合成数据上预训练。我们发现两种变体都表现出强大的性能，一致优于广泛使用的卷积基线EEGNet和最新的特定于EEG的基础模型CBraMod。这些结果表明，通用时间序列基础模型，即使是在非神经起源数据或合成信号上预训练的，也能有效地迁移到EEG。我们的发现强调了利用跨领域预训练模型进行脑信号分析的前景，表明EEG可能会从更广泛的时间序列文献的进步中受益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for time series are emerging as powerful general-purposebackbones, yet their potential for domain-specific biomedical signals such aselectroencephalography (EEG) remains rather unexplored. In this work, weinvestigate the applicability a recently proposed time series classificationfoundation model, to a different EEG tasks such as motor imagery classificationand sleep stage prediction. We test two pretraining regimes: (a) pretraining onheterogeneous real-world time series from multiple domains, and (b) pretrainingon purely synthetic data. We find that both variants yield strong performance,consistently outperforming EEGNet, a widely used convolutional baseline, andCBraMod, the most recent EEG-specific foundation model. These results suggestthat generalist time series foundation models, even when pretrained on data ofnon-neural origin or on synthetic signals, can transfer effectively to EEG. Ourfindings highlight the promise of leveraging cross-domain pretrained models forbrain signal analysis, suggesting that EEG may benefit from advances in thebroader time series literature.</description>
      <author>example@mail.com (Théo Gnassounou, Yessin Moakher, Shifeng Xie, Vasilii Feofanov, Ievgen Redko)</author>
      <guid isPermaLink="false">2510.27522v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Semantic Collapse in Partially Relevant Video Retrieval</title>
      <link>http://arxiv.org/abs/2510.27432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accpeted to NeurIPS 2025. Code is available at  https://github.com/admins97/MSC_PRVR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决部分相关视频检索中语义崩塌问题的框架，通过文本相关性保持学习和跨分支视频对齐方法，显著提高了检索准确性。&lt;h4&gt;背景&lt;/h4&gt;部分相关视频检索旨在检索与文本查询部分内容匹配的视频。现有方法将每个标注的文本-视频对视为正例，其他视为负例，忽略了单个视频内部和不同视频之间的丰富语义变化，导致同一视频中不同事件的嵌入空间压缩在一起，而不同视频中语义相似的查询和片段被分开。&lt;h4&gt;目的&lt;/h4&gt;解决文本和视频嵌入空间中的语义崩塌问题，提高部分相关视频检索的准确性，特别是在处理包含多个不同事件的视频时。&lt;h4&gt;方法&lt;/h4&gt;1) 文本相关性保持学习，保留基础模型编码的文本查询之间的语义关系；2) 跨分支视频对齐，一种对比对齐方法，解耦时间尺度上的分层视频表示；3) 保留顺序的令牌合并和自适应CBVA，生成内部连贯且相互区别的视频片段以增强对齐效果。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架有效防止了语义崩塌，并在PRVR基准测试上显著提高了检索准确性。&lt;h4&gt;结论&lt;/h4&gt;通过解决语义崩塌问题，该研究改进了部分相关视频检索的性能，特别是在处理包含多个不同事件的视频时。&lt;h4&gt;翻译&lt;/h4&gt;部分相关视频检索(PRVPR)旨在检索与文本查询部分内容匹配的视频。现有方法将每个标注的文本-视频对视为正例，其他视为负例，忽略了单个视频内部和不同视频之间的丰富语义变化。因此，同一视频中不同事件的查询及其对应的视频片段段的嵌入空间压缩在一起，而不同视频中语义相似的查询和片段的嵌入空间被分开。这限制了视频包含多个、多样事件时的检索性能。本文解决了上述问题，称为文本和视频嵌入空间中的语义崩塌。我们首先引入文本相关性保持学习，保留基础模型编码的文本查询之间的语义关系。为解决视频嵌入中的崩塌问题，我们提出了跨分支视频对齐(CBVA)，一种对比对齐方法，解耦时间尺度上的分层视频表示。随后，我们引入保留顺序的令牌合并和自适应CBVA，通过生成内部连贯且相互区别的视频片段来增强对齐效果。在PRVR基准测试上的大量实验表明，我们的框架有效防止了语义崩塌并显著提高了检索准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Partially Relevant Video Retrieval (PRVR) seeks videos where only part of thecontent matches a text query. Existing methods treat every annotated text-videopair as a positive and all others as negatives, ignoring the rich semanticvariation both within a single video and across different videos. Consequently,embeddings of both queries and their corresponding video-clip segments fordistinct events within the same video collapse together, while embeddings ofsemantically similar queries and segments from different videos are drivenapart. This limits retrieval performance when videos contain multiple, diverseevents. This paper addresses the aforementioned problems, termed as semanticcollapse, in both the text and video embedding spaces. We first introduce TextCorrelation Preservation Learning, which preserves the semantic relationshipsencoded by the foundation model across text queries. To address collapse invideo embeddings, we propose Cross-Branch Video Alignment (CBVA), a contrastivealignment method that disentangles hierarchical video representations acrosstemporal scales. Subsequently, we introduce order-preserving token merging andadaptive CBVA to enhance alignment by producing video segments that areinternally coherent yet mutually distinctive. Extensive experiments on PRVRbenchmarks demonstrate that our framework effectively prevents semanticcollapse and substantially improves retrieval accuracy.</description>
      <author>example@mail.com (WonJun Moon, MinSeok Jung, Gilhan Park, Tae-Young Kim, Cheol-Ho Cho, Woojin Jun, Jae-Pil Heo)</author>
      <guid isPermaLink="false">2510.27432v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>A Sensing Whole Brain Zebrafish Foundation Model for Neuron Dynamics and Behavior</title>
      <link>http://arxiv.org/abs/2510.27366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种名为SBM的稀疏注意力全脑基础模型，用于斑马鱼幼虫，能够基于感觉刺激预测神经元放电概率，并将大脑状态与行为联系起来。该模型支持对复杂神经现象的快速、基于行为的探索。&lt;h4&gt;背景&lt;/h4&gt;神经动力学支撑着从记忆到睡眠的各种行为，但识别高阶现象（如社交互动）的机制在实验上具有挑战性。现有的全脑模型往往无法扩展到单神经元分辨率，忽略了行为读出，或依赖于PCA/卷积管道，这些方法会错过长程、非线性相互作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够预测神经元放电概率并将大脑状态与行为联系起来的全脑模型，同时保持全脑规模和可解释性。&lt;h4&gt;方法&lt;/h4&gt;研究团队引入了一种稀疏注意力全脑基础模型（SBM），该模型在神经元和时间上分解注意力，从而实现全脑规模和可解释性。模型与一个排列不变的行为头相结合，能够基于梯度合成引发目标行为的神经模式。&lt;h4&gt;主要发现&lt;/h4&gt;在保留主体上，该模型实现了平均绝对误差小于0.02的校准预测和稳定的自回归滚动。通过排列不变的行为头，SBM能够基于梯度合成引发目标行为的神经模式。&lt;h4&gt;结论&lt;/h4&gt;该框架支持对复杂神经现象的快速、基于行为的探索，为研究高阶神经现象提供了新的工具。&lt;h4&gt;翻译&lt;/h4&gt;神经动力学支撑着从记忆到睡眠的各种行为，但识别高阶现象（如社交互动）的机制在实验上具有挑战性。现有的全脑模型往往无法扩展到单神经元分辨率，忽略行为读出，或依赖于PCA/卷积管道，这些方法会错过长程、非线性相互作用。我们引入了一种用于斑马鱼幼虫的稀疏注意力全脑基础模型（SBM），该模型基于感觉刺激预测神经元放电概率，并将大脑状态与行为联系起来。SBM在神经元和时间上分解注意力，从而实现全脑规模和可解释性。在保留主体上，它实现了平均绝对误差小于0.02的校准预测和稳定的自回归滚动。与一个排列不变的行为头相结合，SBM能够基于梯度合成引发目标行为的神经模式。该框架支持对复杂神经现象的快速、基于行为的探索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural dynamics underlie behaviors from memory to sleep, yet identifyingmechanisms for higher-order phenomena (e.g., social interaction) isexperimentally challenging. Existing whole-brain models often fail to scale tosingle-neuron resolution, omit behavioral readouts, or rely on PCA/convpipelines that miss long-range, non-linear interactions. We introduce asparse-attention whole-brain foundation model (SBM) for larval zebrafish thatforecasts neuron spike probabilities conditioned on sensory stimuli and linksbrain state to behavior. SBM factorizes attention across neurons and alongtime, enabling whole-brain scale and interpretability. On a held-out subject,it achieves mean absolute error &lt;0.02 with calibrated predictions and stableautoregressive rollouts. Coupled to a permutation-invariant behavior head, SBMenables gradient-based synthesis of neural patterns that elicit targetbehaviors. This framework supports rapid, behavior-grounded exploration ofcomplex neural phenomena.</description>
      <author>example@mail.com (Sam Fatehmanesh Vegas, Matt Thomson, James Gornet, David Prober)</author>
      <guid isPermaLink="false">2510.27366v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Band Channel Impulse Response Prediction: Leveraging 3.5 GHz Channels for Upper Mid-Band</title>
      <link>http://arxiv.org/abs/2510.27349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures, 4 tables, this work has been submitted to IEEE  International Conference on Communications (ICC) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CIR-UNext的深度学习框架，用于6G网络中的跨频带信道预测，解决了中高频段信道预测的计算复杂性和数据采集成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;6G网络中，特别是在中高频段（FR3，7-24GHz），穿透损耗和阻塞问题严重。射线追踪方法虽然能提供高保真建模，但计算量大且高频数据采集成本高。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可扩展的跨频带信道预测方法，利用低频段数据预测高频段信道特性。&lt;h4&gt;方法&lt;/h4&gt;提出CIR-UNext深度学习框架，利用丰富的3.5GHz信道脉冲响应（CIR）来预测7GHz的CIR。该框架结合基于射线追踪的数据集流程和注意力U-Net（AU-Net）变体进行增益和相位预测。&lt;h4&gt;主要发现&lt;/h4&gt;AU-Net-Aux模型在未见过的复杂环境中实现了0.58dB的中值增益误差和0.27rad的相位预测误差。扩展的Channel2ComMap基础模型在MIMO-OFDM系统中的吞吐量预测表现优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;CIR-UNext为跨频带预测提供了高效且可扩展的解决方案，可应用于6G网络中的定位、波束管理、数字孪生和智能资源分配等领域。&lt;h4&gt;翻译&lt;/h4&gt;精确的跨频带信道预测对6G网络至关重要，特别是在中高频段（FR3，7-24GHz），该频段的穿透损耗和阻塞问题严重。虽然射线追踪（RT）能提供高保真建模，但计算量大，且高频数据采集成本高。为应对这些挑战，我们提出了CIR-UNext，这是一个深度学习框架，旨在利用丰富的3.5GHz信道脉冲响应（CIR）来预测7GHz的CIR。该框架将基于RT的数据集流程与用于增益和相位预测的注意力U-Net（AU-Net）变体相结合。提出的AU-Net-Aux模型在未见过的复杂环境中实现了0.58dB的中值增益误差和0.27rad的相位预测误差。此外，我们将CIR-UNext扩展为一个基础模型Channel2ComMap，用于MIMO-OFDM系统中的吞吐量预测，显示出与现有方法相比的优越性能。总体而言，CIR-UNext为跨频带预测提供了高效且可扩展的解决方案，使定位、波束管理、数字孪生和智能资源分配等应用在6G网络中成为可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate cross-band channel prediction is essential for 6G networks,particularly in the upper mid-band (FR3, 7--24 GHz), where penetration loss andblockage are severe. Although ray tracing (RT) provides high-fidelity modeling,it remains computationally intensive, and high-frequency data acquisition iscostly. To address these challenges, we propose CIR-UNext, a deep learningframework designed to predict 7 GHz channel impulse responses (CIRs) byleveraging abundant 3.5 GHz CIRs. The framework integrates an RT-based datasetpipeline with attention U-Net (AU-Net) variants for gain and phase prediction.The proposed AU-Net-Aux model achieves a median gain error of 0.58 dB and aphase prediction error of 0.27 rad on unseen complex environments. Furthermore,we extend CIR-UNext into a foundation model, Channel2ComMap, for throughputprediction in MIMO-OFDM systems, demonstrating superior performance comparedwith existing approaches. Overall, CIR-UNext provides an efficient and scalablesolution for cross-band prediction, enabling applications such as localization,beam management, digital twins, and intelligent resource allocation in 6Gnetworks.</description>
      <author>example@mail.com (Fan-Hao Lin, Chi-Jui Sung, Chu-Hsiang Huang, Hui Chen, Chao-Kai Wen, Henk Wymeersch)</author>
      <guid isPermaLink="false">2510.27349v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Implicit User Intention via Reasoning with Large Language Model for Image Editing</title>
      <link>http://arxiv.org/abs/2510.27335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CIELR的新方法，通过将复杂用户指令转换为简单明确的编辑动作，避免了联合微调大型语言模型和扩散模型的高计算成本，实现了高效的复杂图像编辑。&lt;h4&gt;背景&lt;/h4&gt;现有图像编辑方法在处理简单编辑指令时表现良好，但处理复杂指令时通常需要联合微调大型语言模型(LLMs)和扩散模型(DMs)，这涉及极高的计算复杂性和训练成本。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法在处理复杂图像编辑指令时的高计算成本问题，提出一种无需联合微调LLMs和DMs的新方法。&lt;h4&gt;方法&lt;/h4&gt;CIELR方法首先利用基础模型构建输入图像的结构化语义表示，然后引入迭代更新机制逐步细化这一表示，获得图像场景的细粒度视觉表示，从而能够执行复杂且灵活的图像编辑任务。&lt;h4&gt;主要发现&lt;/h4&gt;在SmartEdit Reasoning Scenario Set上的实验显示，该方法在PSNR指标上比之前的最先进方法高出9.955 dB，表明其在保持应保持一致区域方面的优越性。此外，作者还构建了包含86个图像样本的CIEBench基准，CIELR在该基准上也优于之前的方法。&lt;h4&gt;结论&lt;/h4&gt;CIELR方法成功避免了联合微调LLMs和DMs的需要，通过将复杂指令分解为简单动作，实现了高效且高质量的复杂图像编辑，代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;现有的图像编辑方法可以很好地处理简单的编辑指令。为了处理复杂的编辑指令，它们通常需要联合微调大型语言模型(LLMs)和扩散模型(DMs)，这涉及非常高的计算复杂性和训练成本。为了解决这个问题，我们提出了一种新方法，称为CIELR(Complex Image Editing via LLM Reasoning)，它将复杂的用户指令转换为一组简单明确的编辑动作，消除了联合微调大型语言模型和扩散模型的需要。具体来说，我们首先使用基础模型构建输入图像的结构化语义表示。然后，我们引入一个迭代更新机制，可以逐步细化这一表示，获得图像场景的细粒度视觉表示。这使我们能够执行复杂且灵活的图像编辑任务。在SmartEdit Reasoning Scenario Set上的大量实验表明，我们的方法在PSNR指标上比之前的最先进方法高出9.955 dB，表明其在保持应保持一致区域方面的优越性。由于公共数据集中复杂推理图像编辑的样本有限，我们构建了一个名为CIEBench的基准，包含86个图像样本，以及一个专门用于基于推理的图像编辑的指标。CIELR在这个基准上也优于之前的方法。代码和数据集可在https://github.com/Jia-shao/Reasoning-Editing获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing image editing methods can handle simple editing instructions verywell. To deal with complex editing instructions, they often need to jointlyfine-tune the large language models (LLMs) and diffusion models (DMs), whichinvolves very high computational complexity and training cost. To address thisissue, we propose a new method, called \textbf{C}omplex \textbf{I}mage\textbf{E}diting via \textbf{L}LM \textbf{R}easoning (CIELR), which converts acomplex user instruction into a set of simple and explicit editing actions,eliminating the need for jointly fine-tuning the large language models anddiffusion models. Specifically, we first construct a structured semanticrepresentation of the input image using foundation models. Then, we introducean iterative update mechanism that can progressively refine thisrepresentation, obtaining a fine-grained visual representation of the imagescene. This allows us to perform complex and flexible image editing tasks.Extensive experiments on the SmartEdit Reasoning Scenario Set show that ourmethod surpasses the previous state-of-the-art by 9.955 dB in PSNR, indicatingits superior preservation of regions that should remain consistent. Due to thelimited number of samples of public datasets of complex image editing withreasoning, we construct a benchmark named CIEBench, containing 86 imagesamples, together with a metric specifically for reasoning-based image editing.CIELR also outperforms previous methods on this benchmark. The code and datasetare available at\href{https://github.com/Jia-shao/Reasoning-Editing}{https://github.com/Jia-shao/Reasoning-Editing}.</description>
      <author>example@mail.com (Yijia Wang, Yiqing Shen, Weiming Chen, Zhihai He)</author>
      <guid isPermaLink="false">2510.27335v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Fusion of Heterogeneous Pathology Foundation Models for Whole Slide Image Analysis</title>
      <link>http://arxiv.org/abs/2510.27237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FuseCPath的新型框架，用于融合异质性病理学基础模型，提升模型整体性能。该框架通过多视图聚类过滤代表性补丁、集群级重新嵌入策略融合补丁级模型，以及协作蒸馏策略融合幻灯片级模型，在多种癌症数据集上实现了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;全幻灯片图像分析已成为计算病理学中日益重要的技术。最近的病理学基础模型在从WSI中提取有意义的补丁级或幻灯片级特征方面显示出显著优势。然而，当前病理学基础模型由于多样化的私有训练数据集和不同网络架构表现出显著异质性，导致使用不同基础模型提取的特征进行下游任务时性能不稳定。&lt;h4&gt;目的&lt;/h4&gt;为了充分利用多种基础模型的优势，本研究旨在提出一种新型框架来融合异质性病理学基础模型，从而获得具有优越集成性能的模型。&lt;h4&gt;方法&lt;/h4&gt;FuseCPath框架包含三个主要贡献：(i) 基于多视图聚类的方法，通过多个基础模型的嵌入来过滤出具有代表性的训练补丁；(ii) 集群级重新嵌入策略，在线捕获补丁级局部特征，以有效融合异质性补丁级基础模型；(iii) 协作蒸馏策略，探索基础模型之间的连接，以有效融合异质性幻灯片级基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;在癌症基因组图谱的肺癌、膀胱癌和结直肠癌数据集上进行的大量实验表明，所提出的FuseCPath在这些公共数据集的多个任务上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FuseCPath框架通过有效融合异质性病理学基础模型，解决了当前病理学基础模型存在的异质性问题，为全幻灯片图像分析提供了更强大的工具，在多种癌症数据集上取得了优异的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;全幻灯片图像分析已成为计算病理学中日益重要的技术。最近的病理学基础模型进展在从全幻灯片图像中提取有意义的补丁级或幻灯片级特征表示方面显示出显著优势。然而，当前的病理学基础模型由于多样化的私有训练数据集和不同的网络架构表现出显著的异质性。这种异质性在使用不同基础模型提取的特征进行下游任务时会导致性能变化。为了充分利用多种基础模型的优势，在本工作中，我们提出了一种用于融合异质性病理学基础模型的新型框架，称为FuseCPath，产生了一个具有优越集成性能的模型。我们框架的主要贡献可以总结如下：(i) 为了保证训练补丁的代表性，我们提出了一种基于多视图聚类的方法，通过多个基础模型的嵌入来过滤出判别性补丁；(ii) 为了有效融合异质性补丁级基础模型，我们设计了一种集群级重新嵌入策略，在线捕获补丁级局部特征；(iii) 为了有效融合异质性幻灯片级基础模型，我们设计了一种协作蒸馏策略，探索幻灯片级基础模型之间的连接。在癌症基因组图谱的肺癌、膀胱癌和结直肠癌数据集上进行的大量实验表明，所提出的FuseCPath在这些公共数据集的多个任务上实现了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Whole slide image (WSI) analysis has emerged as an increasingly essentialtechnique in computational pathology. Recent advances in the pathologicalfoundation models (FMs) have demonstrated significant advantages in derivingmeaningful patch-level or slide-level feature representations from WSIs.However, current pathological FMs have exhibited substantial heterogeneitycaused by diverse private training datasets and different networkarchitectures. This heterogeneity introduces performance variability when weutilize the extracted features from different FMs in the downstream tasks. Tofully explore the advantage of multiple FMs effectively, in this work, wepropose a novel framework for the fusion of heterogeneous pathological FMs,called FuseCPath, yielding a model with a superior ensemble performance. Themain contributions of our framework can be summarized as follows: (i) Toguarantee the representativeness of the training patches, we propose amulti-view clustering-based method to filter out the discriminative patches viamultiple FMs' embeddings. (ii) To effectively fuse the heterogeneouspatch-level FMs, we devise a cluster-level re-embedding strategy to onlinecapture patch-level local features. (iii) To effectively fuse the heterogeneousslide-level FMs, we devise a collaborative distillation strategy to explore theconnections between slide-level FMs. Extensive experiments conducted on lungcancer, bladder cancer, and colorectal cancer datasets from The Cancer GenomeAtlas (TCGA) have demonstrated that the proposed FuseCPath achievesstate-of-the-art performance across multiple tasks on these public datasets.</description>
      <author>example@mail.com (Zhidong Yang, Xiuhui Shi, Wei Ba, Zhigang Song, Haijing Luan, Taiyuan Hu, Senlin Lin, Jiguang Wang, Shaohua Kevin Zhou, Rui Yan)</author>
      <guid isPermaLink="false">2510.27237v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MoRE: 3D Visual Geometry Reconstruction Meets Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2510.27234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: https://g-1nonly.github.io/MoRE_Website/, Code:  https://github.com/alibaba/Taobao3D&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了MoRE，一种基于专家混合(MoE)架构的密集3D视觉基础模型，通过动态路由特征到任务特定专家，解决了3D模型扩展的挑战，实现了在各种几何任务中的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;语言和视觉领域的最新进展表明增加模型容量可提高任务表现；3D视觉几何重建中大规模训练也被证明有效，但由于几何监督复杂性和3D数据多样性，进一步扩展3D模型面临挑战。&lt;h4&gt;目的&lt;/h4&gt;克服3D模型扩展的限制，提出一种提高可扩展性和适应性的密集3D视觉基础模型。&lt;h4&gt;方法&lt;/h4&gt;提出MoRE模型，采用专家混合架构动态路由特征到任务特定专家；包含基于置信度的深度细化模块以稳定几何估计；集成密集语义特征与全局对齐的3D主干表示进行表面法线预测；使用定制的损失函数确保鲁棒学习。&lt;h4&gt;主要发现&lt;/h4&gt;MoRE在多个基准测试中实现最先进性能，支持有效的下游应用且无需额外计算。&lt;h4&gt;结论&lt;/h4&gt;MoRE通过专家混合架构解决了3D模型扩展挑战，提高了模型的可扩展性和适应性，并在真实世界条件下增强了鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;近期语言和视觉领域的进展表明，扩展模型容量可以持续提高各种任务的表现。在3D视觉几何重建中，大规模训练同样被证明对学习通用表示有效。然而，由于几何监督的复杂性和3D数据的多样性，进一步扩展3D模型具有挑战性。为克服这些限制，我们提出了MoRE，一种基于专家混合(MoE)架构的密集3D视觉基础模型，它动态地将特征路由到任务特定的专家，使它们能够专业化于互补的数据方面，提高可扩展性和适应性。为提高真实世界条件下的鲁棒性，MoRE包含一个基于置信度的深度细化模块，用于稳定和细化几何估计。此外，它将密集语义特征与全局对齐的3D主干表示集成，用于高保真表面法线预测。MoRE还使用定制的损失函数进行优化，确保对各种输入和多种几何任务的鲁棒学习。大量实验表明，MoRE在多个基准测试中实现了最先进的性能，并支持有效的下游应用，无需额外计算。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D视觉几何重建领域的可扩展性问题。传统方法依赖场景特定优化，缺乏灵活性，难以在AR/VR、游戏内容创建、机器人和自动驾驶等需要强大几何先验和跨场景一致性的现实应用中发挥作用。随着模型和数据规模的扩大，3D重建面临几何监督复杂性和数据多样性的挑战，限制了性能提升。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到3D重建需要像语言和视觉领域那样的可扩展基础模型，但直接扩展3D模型面临特殊挑战。他们受混合专家模型(MoE)框架启发，该框架在大型语言模型中已证明能有效扩展神经网络。作者借鉴了VGGT作为基础架构，参考了DINOv2的特征提取技术，并受MoGe等单目几何估计模型的启发。他们设计了一个结合MoE的3D重建框架，通过动态路由特征到专家，实现不同场景的自适应表示学习。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将混合专家(MoE)框架引入3D视觉几何重建，创建一个密集的3D视觉基础模型，使模型能够动态路由特征到任务特定专家，学习不同场景的自适应和互补表示。整体流程包括：1)使用密集视觉Transformer骨干网络提取特征；2)实现MoE层，动态路由特征到专家；3)设计基于置信度的深度细化模块过滤噪声；4)融合全局3D特征与局部语义特征增强法线预测；5)使用多任务训练目标和自适应损失策略稳定训练。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将MoE框架系统应用于3D视觉几何重建；2)基于置信度的深度细化模块提高深度估计准确性；3)密集语义特征融合增强表面法线预测；4)多任务训练策略确保稳定学习。相比之前工作，MoRE通过MoE架构实现了更好的可扩展性和适应性，能处理各种3D场景，而传统方法通常针对特定场景优化；与其他前馈3D重建方法相比，MoRE能保持跨场景的高性能，并通过融合语义特征提高了细节表现。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoRE通过将混合专家框架引入3D视觉几何重建，创建了一个可扩展且自适应的基础模型，能够通过动态路由特征到任务特定专家，实现高质量、鲁棒的3D几何预测，并融合语义特征提高表面法线估计的准确性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in language and vision have demonstrated that scaling upmodel capacity consistently improves performance across diverse tasks. In 3Dvisual geometry reconstruction, large-scale training has likewise proveneffective for learning versatile representations. However, further scaling of3D models is challenging due to the complexity of geometric supervision and thediversity of 3D data. To overcome these limitations, we propose MoRE, a dense3D visual foundation model based on a Mixture-of-Experts (MoE) architecturethat dynamically routes features to task-specific experts, allowing them tospecialize in complementary data aspects and enhance both scalability andadaptability. Aiming to improve robustness under real-world conditions, MoREincorporates a confidence-based depth refinement module that stabilizes andrefines geometric estimation. In addition, it integrates dense semanticfeatures with globally aligned 3D backbone representations for high-fidelitysurface normal prediction. MoRE is further optimized with tailored lossfunctions to ensure robust learning across diverse inputs and multiplegeometric tasks. Extensive experiments demonstrate that MoRE achievesstate-of-the-art performance across multiple benchmarks and supports effectivedownstream applications without extra computation.</description>
      <author>example@mail.com (Jingnan Gao, Zhe Wang, Xianze Fang, Xingyu Ren, Zhuo Chen, Shengqi Liu, Yuhao Cheng, Jiangjing Lyu, Xiaokang Yang, Yichao Yan)</author>
      <guid isPermaLink="false">2510.27234v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>SpecAware: A Spectral-Content Aware Foundation Model for Unifying Multi-Sensor Learning in Hyperspectral Remote Sensing Mapping</title>
      <link>http://arxiv.org/abs/2510.27219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SpecAware，一种新型的高光谱光谱内容感知基础模型，用于统一多传感器学习以进行高光谱成像映射。作者还构建了Hyper-400K数据集，包含超过40万个图像块。SpecAware采用两步超网络驱动编码过程，能够感知和解释不同场景和传感器中的空间-光谱特征，实验证明其在多个任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像(HSI)是精细土地利用和土地覆盖(LULC)制图的重要工具，但其数据的内在异质性一直是开发通用模型的主要障碍。现有HSI基础模型忽略了传感器元属性的关键指导作用，且难以进行多传感器训练，限制了可迁移性。&lt;h4&gt;目的&lt;/h4&gt;解决HSI基础模型中传感器元属性被忽略以及多传感器训练困难的问题，开发一个能够统一多传感器学习的高光谱成像映射基础模型。&lt;h4&gt;方法&lt;/h4&gt;1. 提出SpecAware模型；2. 构建Hyper-400K数据集；3. 设计两步超网络驱动编码过程：元内容感知模块融合传感器元属性和图像内容生成条件输入，超嵌入模块通过样本条件超网络动态生成矩阵因子对进行通道编码。&lt;h4&gt;主要发现&lt;/h4&gt;SpecAware能够感知和解释不同场景和传感器中的空间-光谱特征，自适应处理可变数量的光谱通道，建立统一的联合预训练框架。在六个数据集上的实验表明，SpecAware在土地覆盖语义分割分类、变化检测和场景分类方面表现优异。&lt;h4&gt;结论&lt;/h4&gt;SpecAware通过考虑传感器元属性和采用两步超网络驱动编码过程，成功解决了HSI基础模型在多传感器训练和可迁移性方面的挑战，为高光谱成像映射提供了统一且高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高光谱成像(HSI)是精细土地利用和土地覆盖(LULC)制图的重要工具。然而，HSI数据的内在异质性长期以来一直是通过联合训练开发通用模型的主要障碍。尽管HSI基础模型在不同下游任务中显示出潜力，但现有方法通常忽略了传感器元属性的关键指导作用，并且难以进行多传感器训练，限制了它们的可迁移性。为了解决这些挑战，我们提出了SpecAware，这是一种新型的高光谱光谱内容感知基础模型，用于统一多传感器学习以进行HSI映射。我们还构建了Hyper-400K数据集来促进这项研究，这是一个新的包含超过40万个来自不同机载AVIRIS传感器图像块的大规模高质量基准数据集。SpecAware的核心是一个两步超网络驱动的HSI数据编码过程。首先，我们设计了一个元内容感知模块，通过融合传感器元属性和其自身的图像内容，为每个HSI块生成一个独特的条件输入，针对每个样本的每个光谱波段定制。其次，我们设计了超嵌入模块，其中样本条件超网络动态生成一对用于通道编码的矩阵因子，包括自适应空间模式提取和潜在语义特征重投影。因此，SpecAware获得了感知和解释不同场景和传感器中空间-光谱特征的能力。这反过来又使SpecAware能够自适应处理可变数量的光谱通道，建立统一的联合预训练框架。在六个数据集上的广泛实验表明，SpecAware能够学习优越的特征表示，在土地覆盖语义分割分类、变化检测和场景分类方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging (HSI) is a vital tool for fine-grained land-use andland-cover (LULC) mapping. However, the inherent heterogeneity of HSI data haslong posed a major barrier to developing generalized models via joint training.Although HSI foundation models have shown promise for different downstreamtasks, the existing approaches typically overlook the critical guiding role ofsensor meta-attributes, and struggle with multi-sensor training, limiting theirtransferability. To address these challenges, we propose SpecAware, which is anovel hyperspectral spectral-content aware foundation model for unifyingmulti-sensor learning for HSI mapping. We also constructed the Hyper-400Kdataset to facilitate this research, which is a new large-scale, high-qualitybenchmark dataset with over 400k image patches from diverse airborne AVIRISsensors. The core of SpecAware is a two-step hypernetwork-driven encodingprocess for HSI data. Firstly, we designed a meta-content aware module togenerate a unique conditional input for each HSI patch, tailored to eachspectral band of every sample by fusing the sensor meta-attributes and its ownimage content. Secondly, we designed the HyperEmbedding module, where asample-conditioned hypernetwork dynamically generates a pair of matrix factorsfor channel-wise encoding, consisting of adaptive spatial pattern extractionand latent semantic feature re-projection. Thus, SpecAware gains the ability toperceive and interpret spatial-spectral features across diverse scenes andsensors. This, in turn, allows SpecAware to adaptively process a variablenumber of spectral channels, establishing a unified framework for jointpre-training. Extensive experiments on six datasets demonstrate that SpecAwarecan learn superior feature representations, excelling in land-cover semanticsegmentation classification, change detection, and scene classification.</description>
      <author>example@mail.com (Renjie Ji, Xue Wang, Chao Niu, Wen Zhang, Yong Mei, Kun Tan)</author>
      <guid isPermaLink="false">2510.27219v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>FMint-SDE: A Multimodal Foundation Model for Accelerating Numerical Simulation of SDEs via Error Correction</title>
      <link>http://arxiv.org/abs/2510.27173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FMint-SDE的多模态基础模型，用于快速准确地模拟动态系统，解决了传统数值积分器在准确性和效率之间的权衡问题，以及现有神经网络方法需要为每个案例单独训练模型的限制。&lt;h4&gt;背景&lt;/h4&gt;快速准确地模拟动态系统是科学和工程领域的基本挑战。传统数值积分器通常在准确性和计算效率之间面临权衡，而现有的基于神经网络的方法通常需要为每种情况单独训练一个模型。&lt;h4&gt;目的&lt;/h4&gt;克服传统方法和现有神经网络方法的局限性，引入一种新的多模态基础模型用于大规模微分方程模拟。&lt;h4&gt;方法&lt;/h4&gt;提出了FMint-SDE（基于初始化的随机微分方程基础模型），它基于仅解码器的transformer架构，具有上下文学习能力。该模型利用数值和文本模态学习通用误差校正方案，使用传统求解器生成的粗解序列进行提示训练，从而实现对不同系统的广泛泛化。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖分子动力学、机械系统、金融和生物学应用的一系列具有挑战性的SDE基准测试上，实验结果表明FMint-SDE相比经典求解器实现了更优的准确性-效率权衡。&lt;h4&gt;结论&lt;/h4&gt;FMint-SDE作为动态系统通用仿真工具具有巨大潜力，能够实现快速准确的模拟。&lt;h4&gt;翻译&lt;/h4&gt;快速准确地模拟动态系统是科学和工程领域的基本挑战。传统数值积分器通常在准确性和计算效率之间面临权衡，而现有的基于神经网络的方法通常需要为每种情况单独训练一个模型。为了克服这些限制，我们引入了一种新颖的多模态基础模型FMint-SDE（基于初始化的随机微分方程基础模型），用于大规模微分方程模拟。基于具有上下文学习能力的仅解码器transformer，FMint-SDE利用数值和文本模态学习通用误差校正方案。它使用传统求解器生成的粗解序列进行提示训练，从而实现对不同系统的广泛泛化。我们在一系列具有挑战性的SDE基准测试上评估了我们的模型，这些测试涵盖了分子动力学、机械系统、金融和生物学应用。实验结果表明，我们的方法相比经典求解器实现了更优的准确性-效率权衡，凸显了FMint-SDE作为动态系统通用仿真工具的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fast and accurate simulation of dynamical systems is a fundamental challengeacross scientific and engineering domains. Traditional numerical integratorsoften face a trade-off between accuracy and computational efficiency, whileexisting neural network-based approaches typically require training a separatemodel for each case. To overcome these limitations, we introduce a novelmulti-modal foundation model for large-scale simulations of differentialequations: FMint-SDE (Foundation Model based on Initialization for stochasticdifferential equations). Based on a decoder-only transformer with in-contextlearning, FMint-SDE leverages numerical and textual modalities to learn auniversal error-correction scheme. It is trained using prompted sequences ofcoarse solutions generated by conventional solvers, enabling broadgeneralization across diverse systems. We evaluate our models on a suite ofchallenging SDE benchmarks spanning applications in molecular dynamics,mechanical systems, finance, and biology. Experimental results show that ourapproach achieves a superior accuracy-efficiency tradeoff compared to classicalsolvers, underscoring the potential of FMint-SDE as a general-purposesimulation tool for dynamical systems.</description>
      <author>example@mail.com (Jiaxin Yuan, Haizhao Yang, Maria Cameron)</author>
      <guid isPermaLink="false">2510.27173v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>AD-SAM: Fine-Tuning the Segment Anything Vision Foundation Model for Autonomous Driving Perception</title>
      <link>http://arxiv.org/abs/2510.27047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE Transactions on Intelligent Transportation Systems  (IEEE T-ITS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AD-SAM模型，这是一个针对自动驾驶领域语义分割任务优化的视觉基础模型。该模型通过双编码器和可变形解码器扩展了SAM模型，在道路场景的复杂空间和几何特性上表现出色。实验证明AD-SAM在多个基准测试中超越了现有模型，具有更好的分割精度、跨域泛化能力和数据效率。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶需要精确的语义分割来理解复杂的道路场景。现有的基础模型如SAM在自动驾驶领域的应用可能无法充分满足对空间和几何复杂性的需求。此外，自动驾驶领域需要模型能够高效学习并减少标注成本。&lt;h4&gt;目的&lt;/h4&gt;开发一个专门针对自动驾驶场景优化的语义分割模型，提高分割精度、边界精确度，同时实现更好的泛化能力和数据效率，减少对大量标注数据的依赖。&lt;h4&gt;方法&lt;/h4&gt;1. 双编码器结构：结合SAM预训练的Vision Transformer (ViT-H)的全局语义上下文和可训练的卷积深度学习主干网络（ResNet-50）的局部空间细节，生成多尺度融合表示。2. 可变形融合模块：对齐不同尺度和对象几何形状的异构特征。3. 可变形注意力解码器：执行渐进式多阶段细化。4. 混合损失函数：结合Focal、Dice、Lovasz-Softmax和Surface损失，提高语义类别平衡性、边界精确度和优化稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 在Cityscapes和BDD100K基准测试中，AD-SAM的分割精度分别达到68.1 mIoU和59.5 mIoU，显著优于SAM、G-SAM和DeepLabV3。2. 在结构化和多样化的道路场景中分别领先最多22.9和19.2 mIoU。3. AD-SAM表现出强大的跨域泛化能力，保留得分为0.87（而SAM为0.76）。4. 学习速度更快且更稳定，在30-40个周期内收敛，是基准模型学习速度的两倍。5. 仅使用1000个样本，AD-SAM仍能保持0.607 mIoU，显示出高数据效率。&lt;h4&gt;结论&lt;/h4&gt;针对基础模型进行有针对性的架构和优化增强，能够实现可靠且可扩展的自动驾驶感知。AD-SAM通过结合全局语义和局部空间信息，以及使用可变形注意力机制，显著提高了自动驾驶场景中的语义分割性能，同时减少了训练数据需求。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了自动驾驶分割一切模型（AD-SAM），这是一个针对自动驾驶（AD）中语义分割任务微调的视觉基础模型。AD-SAM通过双编码器和可变形解码器扩展了分割一切模型（SAM），以适应道路场景的空间和几何复杂性。双编码器通过结合SAM预训练的Vision Transformer (ViT-H)的全局语义上下文和可训练的卷积深度学习主干网络（即ResNet-50）的局部空间细节，产生多尺度融合表示。可变形融合模块对齐了不同尺度和对象几何形状之间的异构特征。解码器使用可变形注意力执行渐进式多阶段细化。训练由混合损失指导，该损失整合了Focal、Dice、Lovasz-Softmax和Surface损失，提高了语义类别平衡性、边界精确度和优化稳定性。在Cityscapes和伯克利深度驾驶100K（BDD100K）基准测试中的实验表明，AD-SAM在分割精度上超越了SAM、广义SAM（G-SAM）和深度学习基线（DeepLabV3）。它在Cityscapes上达到68.1平均交并比（mIoU），在BDD100K上达到59.5 mIoU，在结构化和多样化的道路场景中分别领先SAM、G-SAM和DeepLabV3最多22.9和19.2 mIoU。AD-SAM表现出强大的跨域泛化能力，保留得分为0.87（SAM为0.76），并且学习速度更快、更稳定，在30-40个周期内收敛，是基准模型学习速度的两倍。仅使用1000个样本，AD-SAM仍能保持0.607 mIoU，表明数据效率对于降低标注成本至关重要。这些结果证实，对基础模型进行有针对性的架构和优化增强能够实现可靠且可扩展的AD感知。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Autonomous Driving Segment Anything Model (AD-SAM), afine-tuned vision foundation model for semantic segmentation in autonomousdriving (AD). AD-SAM extends the Segment Anything Model (SAM) with adual-encoder and deformable decoder tailored to spatial and geometriccomplexity of road scenes. The dual-encoder produces multi-scale fusedrepresentations by combining global semantic context from SAM's pretrainedVision Transformer (ViT-H) with local spatial detail from a trainableconvolutional deep learning backbone (i.e., ResNet-50). A deformable fusionmodule aligns heterogeneous features across scales and object geometries. Thedecoder performs progressive multi-stage refinement using deformable attention.Training is guided by a hybrid loss that integrates Focal, Dice,Lovasz-Softmax, and Surface losses, improving semantic class balance, boundaryprecision, and optimization stability. Experiments on the Cityscapes andBerkeley DeepDrive 100K (BDD100K) benchmarks show that AD-SAM surpasses SAM,Generalized SAM (G-SAM), and a deep learning baseline (DeepLabV3) insegmentation accuracy. It achieves 68.1 mean Intersection over Union (mIoU) onCityscapes and 59.5 mIoU on BDD100K, outperforming SAM, G-SAM, and DeepLabV3 bymargins of up to +22.9 and +19.2 mIoU in structured and diverse road scenes,respectively. AD-SAM demonstrates strong cross-domain generalization with a0.87 retention score (vs. 0.76 for SAM), and faster, more stable learningdynamics, converging within 30-40 epochs, enjoying double the learning speed ofbenchmark models. It maintains 0.607 mIoU with only 1000 samples, suggestingdata efficiency critical for reducing annotation costs. These results confirmthat targeted architectural and optimization enhancements to foundation modelsenable reliable and scalable AD perception.</description>
      <author>example@mail.com (Mario Camarena, Het Patel, Fatemeh Nazari, Evangelos Papalexakis, Mohamadhossein Noruzoliaee, Jia Chen)</author>
      <guid isPermaLink="false">2510.27047v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>GeoPep: A geometry-aware masked language model for protein-peptide binding site prediction</title>
      <link>http://arxiv.org/abs/2510.27040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GeoPep是一种创新的肽结合位点预测框架，通过迁移学习从ESM3蛋白质基础模型获取知识，有效解决了蛋白质-肽相互作用预测中的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态方法整合蛋白质结构和序列在蛋白质-蛋白质界面预测中已取得显著成功，但由于肽的内在构象灵活性和结构数据有限，这些方法难以扩展到蛋白质-肽相互作用领域。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服肽构象灵活性和结构数据有限性问题的蛋白质-肽结合位点预测方法。&lt;h4&gt;方法&lt;/h4&gt;GeoPep框架利用ESM3多模态蛋白质基础模型的迁移学习，微调其预学习的蛋白质-蛋白质结合表示，并与参数高效的神经网络架构集成，使用基于距离的损失函数训练模型，利用3D结构信息增强预测能力。&lt;h4&gt;主要发现&lt;/h4&gt;全面评估表明GeoPep能够有效捕获稀疏和异质的结合模式，在蛋白质-肽结合位点预测方面显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;GeoPep为蛋白质-肽相互作用预测提供了一种有效的解决方案，成功克服了该领域的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;整合蛋白质结构和序列的多模态方法在蛋白质-蛋白质界面预测中取得了显著成功。然而，由于肽的内在构象灵活性和结构数据有限，阻碍了结构感知模型的直接训练，将这些方法扩展到蛋白质-肽相互作用仍然具有挑战性。为解决这些局限性，我们引入了GeoPep，这是一种新颖的肽结合位点预测框架，它利用从ESM3（多模态蛋白质基础模型）的迁移学习。GeoPep微调ESM3从蛋白质-蛋白质结合中预学习的丰富表示，以解决蛋白质-肽结合数据有限的问题。微调的模型进一步与能够从稀疏数据中学习复杂模式的参数高效神经网络架构集成。此外，模型使用基于距离的损失函数进行训练，利用3D结构信息增强结合位点预测。全面评估表明，GeoPep通过有效捕获稀疏和异质的结合模式，在蛋白质-肽结合位点预测方面显著优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal approaches that integrate protein structure and sequence haveachieved remarkable success in protein-protein interface prediction. However,extending these methods to protein-peptide interactions remains challenging dueto the inherent conformational flexibility of peptides and the limitedavailability of structural data that hinder direct training of structure-awaremodels. To address these limitations, we introduce GeoPep, a novel frameworkfor peptide binding site prediction that leverages transfer learning from ESM3,a multimodal protein foundation model. GeoPep fine-tunes ESM3's richpre-learned representations from protein-protein binding to address the limitedavailability of protein-peptide binding data. The fine-tuned model is furtherintegrated with a parameter-efficient neural network architecture capable oflearning complex patterns from sparse data. Furthermore, the model is trainedusing distance-based loss functions that exploit 3D structural information toenhance binding site prediction. Comprehensive evaluations demonstrate thatGeoPep significantly outperforms existing methods in protein-peptide bindingsite prediction by effectively capturing sparse and heterogeneous bindingpatterns.</description>
      <author>example@mail.com (Dian Chen, Yunkai Chen, Tong Lin, Sijie Chen, Xiaolin Cheng)</author>
      <guid isPermaLink="false">2510.27040v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MoME: Mixture of Visual Language Medical Experts for Medical Imaging Segmentation</title>
      <link>http://arxiv.org/abs/2510.26996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MoME是一种用于医学图像分割的视觉语言专家混合模型，结合了多尺度视觉特征和文本嵌入，通过动态专家选择实现高性能的医学图像分割。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型中广泛使用的专家混合范式在医学视觉语言任务中的应用尚不充分，医学图像分析需要更有效的处理方法。&lt;h4&gt;目的&lt;/h4&gt;将成功的MoE范式应用于医学图像分割任务，探索视觉语言模型在医学领域的新型集成方法，实现稳健的医学图像分析。&lt;h4&gt;方法&lt;/h4&gt;提出MoME架构，有效利用针对医学图像复杂性定制的多尺度视觉特征，并结合文本嵌入实现动态专家选择。使用包含3410个CT扫描的10个数据集进行训练和测试。&lt;h4&gt;主要发现&lt;/h4&gt;MoME在全面的医学图像分割基准测试中表现出强大的性能，在多个数据集上展示了具有竞争力的精确度。通过整合文本信息，模型性能得到显著提升。&lt;h4&gt;结论&lt;/h4&gt;MoME探索了一种用于实现稳健医学图像分析结果的新型架构，证明了将基础模型和MoE范式应用于医学图像分割的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，我们提出了MoME，一种用于医学图像分割的视觉语言医学专家混合模型。MoME借鉴了大型语言模型中广泛使用的专家混合范式，应用于医学视觉语言任务。该架构通过有效利用针对医学图像复杂性定制的多尺度视觉特征，并结合文本嵌入，实现了动态专家选择。这项工作探索了视觉语言模型在该领域的新型集成方法。利用包含3410个CT扫描的10个数据集，MoME在全面的医学图像分割基准测试中表现出强大的性能。我们的方法探索了基础模型在医学图像中的应用，受益于MoE通过整合文本信息提高模型性能的已证实有效性。MoME在多个数据集上展示了具有竞争力的精确度，探索了一种用于实现稳健医学图像分析结果的新型架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we propose MoME, a Mixture of Visual Language Medical Experts,for Medical Image Segmentation. MoME adapts the successful Mixture of Experts(MoE) paradigm, widely used in Large Language Models (LLMs), for medicalvision-language tasks. The architecture enables dynamic expert selection byeffectively utilizing multi-scale visual features tailored to the intricaciesof medical imagery, enriched with textual embeddings. This work explores anovel integration of vision-language models for this domain. Utilizing anassembly of 10 datasets, encompassing 3,410 CT scans, MoME demonstrates strongperformance on a comprehensive medical imaging segmentation benchmark. Ourapproach explores the integration of foundation models for medical imaging,benefiting from the established efficacy of MoE in boosting model performanceby incorporating textual information. Demonstrating competitive precisionacross multiple datasets, MoME explores a novel architecture for achievingrobust results in medical image analysis.</description>
      <author>example@mail.com (Arghavan Rezvani, Xiangyi Yan, Anthony T. Wu, Kun Han, Pooya Khosravi, Xiaohui Xie)</author>
      <guid isPermaLink="false">2510.26996v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>NaviTrace: Evaluating Embodied Navigation of Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26909v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures, under review at IEEE conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NaviTrace，一个高质量的视觉问答基准测试，用于评估视觉语言模型在机器人导航任务中的表现。该基准测试包含1000多个场景和3000多条专家轨迹，使用语义感知轨迹分数评估了八个最先进的VLM，发现模型在空间定位和目标定位方面与人类性能存在差距。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多种任务和场景中展现出前所未有的性能和泛化能力，将其集成到机器人导航系统中有助于构建通用机器人。然而，评估这些模型导航能力受到现实世界试验成本高、模拟过于简单以及基准有限等限制。&lt;h4&gt;目的&lt;/h4&gt;开发一个高质量的基准测试来评估视觉语言模型在机器人导航任务中的能力，解决当前评估方法存在的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NaviTrace基准测试，模型接收指令和实体类型（人类、腿式机器人、轮式机器人、自行车），在图像空间输出二维导航轨迹。使用语义感知轨迹分数（结合动态时间规整距离、目标端点误差和基于逐像素语义的实体条件惩罚）进行评估，并与人类偏好进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;八个最先进的VLM在机器人导航任务中与人类性能存在持续差距，主要问题在于空间定位和目标定位能力不足。NaviTrace为现实世界机器人导航提供了可扩展且可重现的基准测试。&lt;h4&gt;结论&lt;/h4&gt;NaviTrace基准测试填补了视觉语言模型在机器人导航评估方面的空白，为未来研究提供了可靠的评估工具，有助于推动通用机器人的发展。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在各种任务和场景中展现出前所未有的性能和泛化能力。将这些基础模型集成到机器人导航系统中，为构建通用机器人开辟了途径。然而，评估这些模型的导航能力仍然受到现实世界试验成本高、模拟过于简单以及基准有限等限制。我们引入了NaviTrace，这是一个高质量的视觉问答基准测试，模型接收指令和实体类型（人类、腿式机器人、轮式机器人、自行车），必须在图像空间输出二维导航轨迹。在1000多个场景和3000多条专家轨迹中，我们使用新引入的语义感知轨迹分数系统地评估了八个最先进的VLM。该指标结合了动态时间规整距离、目标端点误差以及基于逐像素语义的实体条件惩罚，并与人类偏好相关。我们的评估显示，由于空间定位和目标定位能力差，模型与人类性能之间存在持续差距。NaviTrace为现实世界机器人导航建立了可扩展且可重现的基准测试。基准测试和排行榜可以在https://leggedrobotics.github.io/navitrace_webpage/找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models demonstrate unprecedented performance andgeneralization across a wide range of tasks and scenarios. Integrating thesefoundation models into robotic navigation systems opens pathways towardbuilding general-purpose robots. Yet, evaluating these models' navigationcapabilities remains constrained by costly real-world trials, overly simplifiedsimulations, and limited benchmarks. We introduce NaviTrace, a high-qualityVisual Question Answering benchmark where a model receives an instruction andembodiment type (human, legged robot, wheeled robot, bicycle) and must output a2D navigation trace in image space. Across 1000 scenarios and more than 3000expert traces, we systematically evaluate eight state-of-the-art VLMs using anewly introduced semantic-aware trace score. This metric combines Dynamic TimeWarping distance, goal endpoint error, and embodiment-conditioned penaltiesderived from per-pixel semantics and correlates with human preferences. Ourevaluation reveals consistent gap to human performance caused by poor spatialgrounding and goal localization. NaviTrace establishes a scalable andreproducible benchmark for real-world robotic navigation. The benchmark andleaderboard can be found athttps://leggedrobotics.github.io/navitrace_webpage/.</description>
      <author>example@mail.com (Tim Windecker, Manthan Patel, Moritz Reuss, Richard Schwarzkopf, Cesar Cadena, Rudolf Lioutikov, Marco Hutter, Jonas Frey)</author>
      <guid isPermaLink="false">2510.26909v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Cognition Envelopes for Bounded AI Reasoning in Autonomous UAS Operations</title>
      <link>http://arxiv.org/abs/2510.26905v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10.5 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了认知包络(Cognition Envelopes)的概念，用于约束AI模型在网络物理系统中的决策，以应对模型引入的新类型错误。&lt;h4&gt;背景&lt;/h4&gt;网络物理系统越来越多地依赖基础模型如大型语言模型(LLMs)和视觉语言模型(VLMs)，通过增强感知、推理和规划来提高自主性。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型引入的新类型错误(如幻觉、过度泛化和上下文不匹配)导致的错误决策问题。&lt;h4&gt;方法&lt;/h4&gt;引入认知包络概念，建立推理边界来约束AI生成的决策，同时补充元认知和传统安全包络的使用，并为其制定实际指南和系统化流程进行定义、验证和保证。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体研究发现。&lt;h4&gt;结论&lt;/h4&gt;认知包络是管理AI模型决策风险的一种有效方法，需要系统化的流程来确保其有效性。&lt;h4&gt;翻译&lt;/h4&gt;网络物理系统越来越多地依赖基础模型，如大型语言模型(LLMs)和视觉语言模型(VLMs)，通过增强感知、推理和规划来提高自主性。然而，这些模型也引入了新的错误类型，如幻觉、过度泛化和上下文不匹配，导致错误和有缺陷的决策。为了解决这个问题，我们引入了认知包络的概念，旨在建立推理边界，以约束AI生成的决策，同时补充元认知和传统安全包络的使用。与安全包络一样，认知包络需要其实际指南和系统化的流程来进行定义、验证和保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cyber-physical systems increasingly rely on Foundational Models such as LargeLanguage Models (LLMs) and Vision-Language Models (VLMs) to increase autonomythrough enhanced perception, inference, and planning. However, these modelsalso introduce new types of errors, such as hallucinations,overgeneralizations, and context misalignments, resulting in incorrect andflawed decisions. To address this, we introduce the concept of CognitionEnvelopes, designed to establish reasoning boundaries that constrainAI-generated decisions while complementing the use of meta-cognition andtraditional safety envelopes. As with safety envelopes, Cognition Envelopesrequire practical guidelines and systematic processes for their definition,validation, and assurance.</description>
      <author>example@mail.com (Pedro Antonio Alarcón Granadeno, Arturo Miguel Bernal Russell, Sofia Nelson, Demetrius Hernandez, Maureen Petterson, Michael Murphy, Walter J. Scheirer, Jane Cleland-Huang)</author>
      <guid isPermaLink="false">2510.26905v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Pre-trained Forecasting Models: Strong Zero-Shot Feature Extractors for Time Series Classification</title>
      <link>http://arxiv.org/abs/2510.26777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 Workshop on Recent Advances in Time Series Foundation  Models (BERT2S)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了预训练的时间序列预测模型在分类任务上的表示有效性，挑战了任务特定预训练的必要性假设。&lt;h4&gt;背景&lt;/h4&gt;最近关于时间序列基础模型的研究主要集中在预测任务上，不清楚这些模型学习到的表示有多大的通用性。&lt;h4&gt;目的&lt;/h4&gt;研究冻结预训练的预测模型是否能为分类任务提供有效的表示，以及预测性能与分类性能之间的关系。&lt;h4&gt;方法&lt;/h4&gt;比较不同的表示提取策略，并引入两种与模型无关的嵌入增强方法，通过实验评估预测模型在分类任务上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;最好的预测模型在分类任务上实现的准确率与或甚至超过了专门为分类预训练的最先进模型，且预测性能和分类性能之间存在正相关。&lt;h4&gt;结论&lt;/h4&gt;学习预测可能是构建通用时间序列基础模型的有力途径，挑战了任务特定预训练的必要性假设。&lt;h4&gt;翻译&lt;/h4&gt;最近关于时间序列基础模型的研究主要集中在预测上，这使得它们学习到的表示的通用性尚不清楚。在本研究中，我们考察冻结预训练的预测模型是否能为分类提供有效表示。为此，我们比较了不同的表示提取策略，并引入了两种与模型无关的嵌入增强方法。我们的实验表明，最好的预测模型实现的分类准确率与甚至超过了专门为分类预训练的最先进模型。此外，我们观察到预测性能和分类性能之间存在正相关。这些发现挑战了任务特定预训练的必要性假设，并表明学习预测可能是构建通用时间序列基础模型的有力途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research on time series foundation models has primarily focused onforecasting, leaving it unclear how generalizable their learned representationsare. In this study, we examine whether frozen pre-trained forecasting modelscan provide effective representations for classification. To this end, wecompare different representation extraction strategies and introduce twomodel-agnostic embedding augmentations. Our experiments show that the bestforecasting models achieve classification accuracy that matches or evensurpasses that of state-of-the-art models pre-trained specifically forclassification. Moreover, we observe a positive correlation between forecastingand classification performance. These findings challenge the assumption thattask-specific pre-training is necessary, and suggest that learning to forecastmay provide a powerful route toward constructing general-purpose time seriesfoundation models.</description>
      <author>example@mail.com (Andreas Auer, Daniel Klotz, Sebastinan Böck, Sepp Hochreiter)</author>
      <guid isPermaLink="false">2510.26777v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Platform Evaluation of Reasoning Capabilities in Foundation Models</title>
      <link>http://arxiv.org/abs/2510.26732v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文对当代基础模型的推理能力进行了全面的跨平台评估，建立了不依赖特定基础设施的基准测试，涵盖高性能计算超级计算机、云平台和大学集群三种计算范式。&lt;h4&gt;背景&lt;/h4&gt;当前基础模型在不同平台上的推理能力表现尚不明确，需要建立一个统一的评估框架。&lt;h4&gt;目的&lt;/h4&gt;建立基础设施无关的基准测试，评估15个基础模型在8个学术领域79个问题上的推理能力，并探索不同计算环境下的性能差异。&lt;h4&gt;方法&lt;/h4&gt;通过三个实验阶段进行评估：(1)基线建立：在MareNostrum 5上使用6个模型评估19个问题；(2)基础设施验证：在大学集群和Nebius AI Studio上重复基准测试；(3)扩展评估：在两个平台上对完整的79个问题进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;挑战了传统的扩展假设，确立了训练数据质量比模型大小更重要，为不同环境中的模型选择提供了可操作的指导原则。&lt;h4&gt;结论&lt;/h4&gt;三种基础设施的方法和79个问题的基准测试使能够随着基础模型的演变进行推理能力的纵向跟踪。&lt;h4&gt;翻译&lt;/h4&gt;本文对当代基础模型的推理能力进行了全面的跨平台评估，建立了一个不依赖特定基础设施的基准测试，涵盖高性能计算超级计算机、云平台和大学集群三种计算范式。我们评估了15个基础模型，涵盖物理、数学、化学、经济学、生物学、统计学、微积分和优化八个学术领域的79个问题。通过三个实验阶段：(1)基线建立：在MareNostrum 5上使用6个模型评估19个问题，建立方法和参考性能；(2)基础设施验证：在大学集群和Nebius AI Studio上重复19个问题的基准测试，确认与基础设施无关的可重复性；(3)扩展评估：在两个平台上对完整的79个问题进行全面评估，探索架构多样性方面的规模化泛化能力。研究挑战了传统的扩展假设，确立了训练数据质量比模型大小更重要，为教育、生产和研究环境中的模型选择提供了可操作的指导原则。三种基础设施的方法和79个问题的基准测试使能够随着基础模型的演变进行推理能力的纵向跟踪。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a comprehensive cross-platform evaluation of reasoningcapabilities in contemporary foundation models, establishing aninfrastructure-agnostic benchmark across three computational paradigms: HPCsupercomputing (MareNostrum 5), cloud platforms (Nebius AI Studio), anduniversity clusters (a node with eight H200 GPUs).  We evaluate 15 foundation models across 79 problems spanning eight academicdomains (Physics, Mathematics, Chemistry, Economics, Biology, Statistics,Calculus, and Optimization) through three experimental phases: (1) Baselineestablishment: Six models (Mixtral-8x7B, Phi-3, LLaMA 3.1-8B, Gemma-2-9b,Mistral-7B, OLMo-7B) evaluated on 19 problems using MareNostrum 5, establishingmethodology and reference performance; (2) Infrastructure validation: The19-problem benchmark repeated on university cluster (seven models includingFalcon-Mamba state-space architecture) and Nebius AI Studio (ninestate-of-the-art models: Hermes-4 70B/405B, LLaMA 3.1-405B/3.3-70B, Qwen330B/235B, DeepSeek-R1, GPT-OSS 20B/120B) to confirm infrastructure-agnosticreproducibility; (3) Extended evaluation: Full 79-problem assessment on bothuniversity cluster and Nebius platforms, probing generalization at scale acrossarchitectural diversity.  The findings challenge conventional scaling assumptions, establish trainingdata quality as more critical than model size, and provide actionableguidelines for model selection across educational, production, and researchcontexts. The tri-infrastructure methodology and 79-problem benchmark enablelongitudinal tracking of reasoning capabilities as foundation models evolve.</description>
      <author>example@mail.com (J. de Curtò, I. de Zarzà, Pablo García, Jordi Cabot)</author>
      <guid isPermaLink="false">2510.26732v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>LSM-MS2: A Foundation Model Bridging Spectral Identification and Biological Interpretation</title>
      <link>http://arxiv.org/abs/2510.26715v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LSM-MS2是一个大规模深度学习基础模型，通过学习语义化学空间，显著提高了光谱识别的性能，特别是在异构化合物识别、复杂生物样本分析和低浓度条件下，并能直接进行生物学解释和临床预测。&lt;h4&gt;背景&lt;/h4&gt;大多数质谱数据仍未被表征，导致大量生物和化学信息未被利用。机器学习的最新进展开始解决这个问题，特别是在串联质谱数据的光谱识别任务中。&lt;h4&gt;目的&lt;/h4&gt;介绍LSM-MS2，一个大规模深度学习基础模型，旨在学习语义化学空间并解决质谱数据表征不足的问题。&lt;h4&gt;方法&lt;/h4&gt;LSM-MS2是在数百万张光谱上训练的大规模深度学习基础模型，它学习语义化学空间，能够生成丰富的光谱嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;LSM-MS2在光谱识别方面取得了最先进的性能：在识别具有挑战性的异构化合物方面比现有方法提高30%的准确率；在复杂生物样本中产生42%的正确识别；在低浓度条件下保持稳健性；产生的光谱嵌入可直接从最少的下游数据进行生物学解释；成功区分不同的疾病状态并预测各种转化应用中的临床结果。&lt;h4&gt;结论&lt;/h4&gt;LSM-MS2模型有效地解决了质谱数据表征不足的问题，为生物和化学信息的利用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;绝大多数质谱数据仍未被表征，导致其大量生物和化学信息未被利用。机器学习的最新进展开始解决这个问题，特别是在串联质谱数据的光谱识别任务方面。在此，我们介绍了LSM-MS2的最新一代，这是一个在数百万张光谱上训练的大规模深度学习基础模型，用于学习语义化学空间。LSM-MS2在光谱识别方面取得了最先进的性能，在识别具有挑战性的异构化合物方面比现有方法提高30%的准确率，在复杂生物样本中产生42%的正确识别，并在低浓度条件下保持稳健性。此外，LSM-MS2产生丰富的光谱嵌入，能够直接从最少的下游数据进行生物学解释，成功区分不同的疾病状态并预测各种转化应用中的临床结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A vast majority of mass spectrometry data remains uncharacterized, leavingmuch of its biological and chemical information untapped. Recent advances inmachine learning have begun to address this gap, particularly for tasks such asspectral identification in tandem mass spectrometry data. Here, we present thelatest generation of LSM-MS2, a large-scale deep learning foundation modeltrained on millions of spectra to learn a semantic chemical space. LSM-MS2achieves state-of-the-art performance in spectral identification, improving onexisting methods by 30% in accuracy of identifying challenging isomericcompounds, yielding 42% more correct identifications in complex biologicalsamples, and maintaining robustness under low-concentration conditions.Furthermore, LSM-MS2 produces rich spectral embeddings that enable directbiological interpretation from minimal downstream data, successfullydifferentiating disease states and predicting clinical outcomes across diversetranslational applications.</description>
      <author>example@mail.com (Gabriel Asher, Devesh Shah, Amy A. Caudy, Luke Ferro, Lea Amar, Ana S. H. Costa, Thomas Patton, Niall O'Connor, Jennifer M. Campbell, Jack Geremia)</author>
      <guid isPermaLink="false">2510.26715v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>ProstNFound+: A Prospective Study using Medical Foundation Models for Prostate Cancer Detection</title>
      <link>http://arxiv.org/abs/2510.26703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了ProstNFound+，一种基于医学基础模型的前列腺癌检测系统，并通过前瞻性验证了其在临床微超声图像中的有效性和实用性。&lt;h4&gt;背景&lt;/h4&gt;医学基础模型为构建高性能诊断系统提供了新途径，但在前列腺癌微超声检测领域的临床应用尚未得到验证。&lt;h4&gt;目的&lt;/h4&gt;开发并验证ProstNFound+系统，将医学基础模型应用于前列腺癌的微超声检测，并进行临床前瞻性验证。&lt;h4&gt;方法&lt;/h4&gt;ProstNFound+整合了医学基础模型、适配器调优和嵌入前列腺癌特异性临床生物标志物的自定义提示编码器，生成癌症热图和风险评分。模型在多中心回顾性数据上训练，并在五年后新临床站点获取的数据上进行前瞻性评估，与标准临床评分方案(PRI-MUS和PI-RADS)进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;ProstNFound+在前瞻性数据上表现出强大的泛化能力，性能未出现下降；与临床评分高度一致；生成的热图具有可解释性，与活检证实的病变一致。&lt;h4&gt;结论&lt;/h4&gt;ProstNFound+具有临床部署潜力，为专家驱动协议提供了可扩展且可解释的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;目的：医学基础模型为构建高性能诊断系统提供了一种途径。然而，这些模型在前列腺癌微超声检测中的临床应用尚未得到验证。我们提出了ProstNFound+，这是一个针对前列腺癌微超声检测的基础模型适应版本，并进行了首次前瞻性验证。方法：ProstNFound+整合了医学基础模型、适配器调优和嵌入前列腺癌特异性临床生物标志物的自定义提示编码器。该模型生成癌症热图和临床显著前列腺癌的风险评分。在多中心回顾性数据上训练后，模型在五年后从新临床站点获取的数据上进行前瞻性评估。模型预测与标准临床评分方案(PRI-MUS和PI-RADS)进行基准测试。结果：ProstNFound+在前瞻性数据上显示出强大的泛化能力，与回顾性评估相比没有性能下降。与临床评分高度一致，生成与活检证实的病变一致的可解释热图。结论：结果突显了ProstNFound+在临床部署方面的潜力，提供了可扩展且可解释的替代方案，替代专家驱动的协议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Medical foundation models (FMs) offer a path to buildhigh-performance diagnostic systems. However, their application to prostatecancer (PCa) detection from micro-ultrasound ({\mu}US) remains untested inclinical settings. We present ProstNFound+, an adaptation of FMs for PCadetection from {\mu}US, along with its first prospective validation. Methods:ProstNFound+ incorporates a medical FM, adapter tuning, and a custom promptencoder that embeds PCa-specific clinical biomarkers. The model generates acancer heatmap and a risk score for clinically significant PCa. Followingtraining on multi-center retrospective data, the model is prospectivelyevaluated on data acquired five years later from a new clinical site. Modelpredictions are benchmarked against standard clinical scoring protocols(PRI-MUS and PI-RADS). Results: ProstNFound+ shows strong generalization to theprospective data, with no performance degradation compared to retrospectiveevaluation. It aligns closely with clinical scores and produces interpretableheatmaps consistent with biopsy-confirmed lesions. Conclusion: The resultshighlight its potential for clinical deployment, offering a scalable andinterpretable alternative to expert-driven protocols.</description>
      <author>example@mail.com (Paul F. R. Wilson, Mohamed Harmanani, Minh Nguyen Nhat To, Amoon Jamzad, Tarek Elghareb, Zhuoxin Guo, Adam Kinnaird, Brian Wodlinger, Purang Abolmaesumi, Parvin Mousavi)</author>
      <guid isPermaLink="false">2510.26703v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Aeolus: A Multi-structural Flight Delay Dataset</title>
      <link>http://arxiv.org/abs/2510.26616v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍Aeolus，一个大规模多模态航班延误数据集，用于推进航班延误预测研究和支持表格数据基础模型的开发。&lt;h4&gt;背景&lt;/h4&gt;现有领域数据集通常限于扁平表格结构，无法捕捉航班延误传播中固有的时空动态特性。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的局限性，提供能够捕捉航班延误传播特性的多模态数据集。&lt;h4&gt;方法&lt;/h4&gt;提供三种对齐模态：(i) 包含丰富运营、气象和机场级别特征的表格数据集，涵盖超过500万次航班；(ii) 航班链模块，模拟沿连续航班航段的延误传播，捕捉上下游依赖关系；(iii) 航班网络图，编码共享的飞机、机组人员和机场资源连接，实现跨航班关系推理。&lt;h4&gt;主要发现&lt;/h4&gt;数据集通过时间分割、全面特征和严格的防泄漏措施精心构建，支持真实且可复现的机器学习评估。&lt;h4&gt;结论&lt;/h4&gt;Aeolus支持多种任务，包括回归、分类、时间结构建模和图学习，可作为表格、序列和图模态的统一基准，填补了领域特定建模和通用结构数据研究的关键空白。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍Aeolus，这是一个大规模多模态航班延误数据集，旨在推进航班延误预测研究并支持表格数据基础模型的开发。该领域现有数据集通常限于扁平表格结构，无法捕捉延误传播中固有的时空动态特性。Aeolus通过提供三种对齐模态解决了这一局限：(i) 包含丰富运营、气象和机场级别特征的表格数据集，涵盖超过500万次航班；(ii) 航班链模块，用于模拟沿连续航班航段的延误传播，捕捉上游和下游依赖关系；(iii) 航班网络图，编码共享的飞机、机组人员和机场资源连接，实现跨航班关系推理。该数据集通过时间分割、全面特征和严格的防泄漏措施精心构建，支持真实且可复现的机器学习评估。Aeolus支持多种任务，包括回归、分类、时间结构建模和图学习，可作为表格、序列和图模态的统一基准。我们发布了基线实验和预处理工具以促进采用。Aeolus填补了领域特定建模和通用结构数据研究的关键空白。我们的源代码和数据可在https://github.com/Flnny/Delay-data获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Aeolus, a large-scale Multi-modal Flight Delay Dataset designedto advance research on flight delay prediction and support the development offoundation models for tabular data. Existing datasets in this domain aretypically limited to flat tabular structures and fail to capture thespatiotemporal dynamics inherent in delay propagation. Aeolus addresses thislimitation by providing three aligned modalities: (i) a tabular dataset withrich operational, meteorological, and airportlevel features for over 50 millionflights; (ii) a flight chain module that models delay propagation alongsequential flight legs, capturing upstream and downstream dependencies; and(iii) a flight network graph that encodes shared aircraft, crew, and airportresource connections, enabling cross-flight relational reasoning. The datasetis carefully constructed with temporal splits, comprehensive features, andstrict leakage prevention to support realistic and reproducible machinelearning evaluation. Aeolus supports a broad range of tasks, includingregression, classification, temporal structure modeling, and graph learning,serving as a unified benchmark across tabular, sequential, and graphmodalities. We release baseline experiments and preprocessing tools tofacilitate adoption. Aeolus fills a key gap for both domain-specific modelingand general-purpose structured data research.Our source code and data can beaccessed at https://github.com/Flnny/Delay-data</description>
      <author>example@mail.com (Lin Xu, Xinyun Yuan, Yuxuan Liang, Suwan Yin, Yuankai Wu)</author>
      <guid isPermaLink="false">2510.26616v2</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Foundation Models for Enhancing Robot Perception and Action</title>
      <link>http://arxiv.org/abs/2510.26855v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Doctoral thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何系统性地利用基础模型来增强机器人能力，特别是在非结构化环境中实现更有效的定位、交互和操作。研究围绕四个核心问题展开，共同构建了一个语义感知的机器人智能框架。&lt;h4&gt;背景&lt;/h4&gt;基础模型在机器人领域的应用是一个新兴的研究方向，特别是在处理非结构化环境中的挑战时。&lt;h4&gt;目的&lt;/h4&gt;探索基础模型如何被系统性地应用于机器人领域，以增强机器人在非结构化环境中的定位、交互和操作能力。&lt;h4&gt;方法&lt;/h4&gt;研究围绕四个核心问题展开，每个问题解决机器人领域的一个基本挑战，共同构建一个语义感知的机器人智能框架。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型可以被系统性地利用来增强机器人在非结构化环境中的能力，包括更有效的定位、交互和操作。&lt;h4&gt;结论&lt;/h4&gt;基础模型为增强机器人能力提供了新的途径，特别是在处理非结构化环境中的复杂任务时。&lt;h4&gt;翻译&lt;/h4&gt;本论文研究了如何系统性地利用基础模型来增强机器人能力，使机器人在非结构化环境中能够实现更有效的定位、交互和操作。这项工作围绕四个核心问题展开，每个问题都解决了机器人领域的一个基本挑战，同时共同构建了一个语义感知的机器人智能框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何系统性地将基础模型（Foundation Models）集成到机器人系统中，以增强机器人在非结构化环境中的感知和动作能力。这个问题很重要，因为传统机器人系统依赖于特定任务模型，难以在真实世界的复杂、动态环境中泛化和适应。基础模型具有强大的语义理解和泛化能力，可以帮助机器人更好地解释复杂场景、适应新任务，并在变化的环境中灵活响应，最终实现更接近人类的灵活推理和行为。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统机器人系统在真实环境中的局限性，然后发现了基础模型（如GPT-3、CLIP等）的潜力。作者设计方法时借鉴了现有工作：在定位方面借鉴了语义信息增强方法，但使用了更强大的基础模型；在抓取方面借鉴了语言引导方法，但结合了大型语言模型和视觉语言模型；在分类方面借鉴了知识蒸馏技术；在视觉鲁棒性方面借鉴了视觉抽象方法。作者基于四个核心研究问题（定位、抓取、分类和操作）设计了四个方法，每个方法都利用基础模型的特定能力，并通过不同技术（如零样本推理、模型蒸馏）解决机器人面临的挑战。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 这篇论文提出了四个主要方法，每个针对不同机器人任务：1) FM-Loc使用基础模型生成语义图像描述符，通过CLIP检测对象、GPT-3生成房间标签、再次使用CLIP选择最可能的房间标签，最后计算查询与参考图像的语义相似度；2) Lan-grasp利用大型语言模型确定可抓取部分，视觉语言模型定位这些部分，并引入反馈机制动态调整抓取策略；3) VLM-Vac使用知识蒸馏将视觉语言模型能力转移到轻量级模型，并通过语言引导的经验回放实现持续学习；4) ARRO使用开放词汇分割和对象检测隔离任务相关组件，投影到虚拟背景上，减少视觉域变化的影响。整体流程都是先利用基础模型进行语义理解，然后根据具体任务设计相应的推理或决策机制。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) FM-Loc使用基础模型构建语义描述符，无需额外数据收集或微调；2) Lan-grasp结合语言模型和视觉模型进行语义抓取，引入反馈机制；3) VLM-Vac使用知识蒸馏和经验回放实现持续学习；4) ARRO提出视觉抽象方法提高视觉运动策略鲁棒性。相比之前工作，这些创新更系统地利用基础模型能力，注重语义理解和上下文感知，采用零样本推理等技术解决机器人挑战，并在真实环境中广泛评估，展示了实用性和有效性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统性地将基础模型集成到机器人核心任务中，利用零样本推理、模型蒸馏和视觉抽象等技术，显著提高了机器人在非结构化环境中的语义理解、泛化能力和鲁棒性，为开发能够与人类在动态环境中可靠协作的自主机器人提供了新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This thesis investigates how foundation models can be systematicallyleveraged to enhance robotic capabilities, enabling more effectivelocalization, interaction, and manipulation in unstructured environments. Thework is structured around four core lines of inquiry, each addressing afundamental challenge in robotics while collectively contributing to a cohesiveframework for semantics-aware robotic intelligence.</description>
      <author>example@mail.com (Reihaneh Mirjalili)</author>
      <guid isPermaLink="false">2510.26855v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>CYPRESS: Crop Yield Prediction via Regression on Prithvi's Encoder for Satellite Sensing</title>
      <link>http://arxiv.org/abs/2510.26609v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CYPRESS，一种基于深度学习的油菜籽产量预测模型，通过利用预训练的地理空间基础模型，将多时相卫星图像转换为高分辨率的像素级产量图，为精准农业提供了更实用的工具。&lt;h4&gt;背景&lt;/h4&gt;准确及时的作物产量预测对全球粮食安全和现代农业管理至关重要，但传统方法缺乏精准农业所需的可扩展性和粒度。&lt;h4&gt;目的&lt;/h4&gt;开发一种名为CYPRESS的深度学习模型，用于高分辨率、田间级别的油菜籽产量预测。&lt;h4&gt;方法&lt;/h4&gt;CYPRESS利用预训练的大规模地理空间基础模型(Prithvi-EO-2.0-600M)，将其适配为连续回归任务，将多时相卫星图像转换为密集的像素级产量图。&lt;h4&gt;主要发现&lt;/h4&gt;在加拿大草原综合数据集上评估，CYPRESS表现优于现有的基于深度学习的产量预测模型，证明了微调基础模型用于专业农业应用的有效性。&lt;h4&gt;结论&lt;/h4&gt;CYPRESS提供连续、高分辨率的输出，比传统方法更实用；该工作弥合了大规模地球观测和农场决策之间的差距，为详细的农业监测提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确及时的作物产量预测对全球粮食安全和现代农业管理至关重要。传统方法往往缺乏精准农业所需的可扩展性和粒度。本文介绍了CYPRESS（通过卫星传感的Prithvi编码器回归进行作物产量预测），这是一种专为高分辨率、田间级油菜籽产量预测设计的深度学习模型。CYPRESS利用预训练的大规模地理空间基础模型（Prithvi-EO-2.0-600M），并对其进行适配以用于连续回归任务，将多时相卫星图像转换为密集的像素级产量图。在加拿大草原综合数据集上的评估表明，CYPRESS优于现有的基于深度学习的产量预测模型，突显了微调基础模型用于专业农业应用的有效性。通过提供连续、高分辨率的输出，CYPRESS比传统的分类或县级聚合方法为精准农业提供了更实用的工具。这项工作验证了一种新颖的方法，弥合了大规模地球观测和农场决策之间的差距，为详细的农业监测提供了可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and timely crop yield prediction is crucial for global food securityand modern agricultural management. Traditional methods often lack thescalability and granularity required for precision farming. This paperintroduces CYPRESS (Crop Yield Prediction via Regression on Prithvi's Encoderfor Satellite Sensing), a deep learning model designed for high-resolution,intra-field canola yield prediction. CYPRESS leverages a pre-trained,large-scale geospatial foundation model (Prithvi-EO-2.0-600M) and adapts it fora continuous regression task, transforming multi-temporal satellite imageryinto dense, pixel-level yield maps. Evaluated on a comprehensive dataset fromthe Canadian Prairies, CYPRESS demonstrates superior performance over existingdeep learning-based yield prediction models, highlighting the effectiveness offine-tuning foundation models for specialized agricultural applications. Byproviding a continuous, high-resolution output, CYPRESS offers a moreactionable tool for precision agriculture than conventional classification orcounty-level aggregation methods. This work validates a novel approach thatbridges the gap between large-scale Earth observation and on-farmdecision-making, offering a scalable solution for detailed agriculturalmonitoring.</description>
      <author>example@mail.com (Shayan Nejadshamsi, Yuanyuan Zhang, Shadi Zaki, Brock Porth, Lysa Porth, Vahab Khoshdel)</author>
      <guid isPermaLink="false">2510.26609v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Stop Wasting Your Tokens: Towards Efficient Runtime Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2510.26585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SupervisorAgent是一个轻量级和模块化的框架，用于运行时自适应监督，无需更改基础智能体的架构。它通过无LLM的自适应过滤器触发，在关键时刻进行干预，主动纠正错误，指导低效行为，并净化观测信息，从而显著减少多智能体系统中的令牌消耗，同时保持成功率。&lt;h4&gt;背景&lt;/h4&gt;多智能体系统在处理复杂任务时表现出色，但随着操作复杂性的增加，它们的自主性增强会导致关键效率问题，如过度消耗令牌和因错误信息导致的失败。现有方法主要关注事后故障归因，缺乏主动的、实时干预来增强鲁棒性和效率。&lt;h4&gt;目的&lt;/h4&gt;引入SupervisorAgent，一个轻量级和模块化的框架，用于运行时自适应监督，无需更改基础智能体的架构，以解决多智能体系统中的效率问题。&lt;h4&gt;方法&lt;/h4&gt;SupervisorAgent由一个无LLM的自适应过滤器触发，在关键时刻进行干预，主动纠正错误，指导低效行为，并净化观测信息。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的GAIA基准测试中，SupervisorAgent将Smolagent框架的令牌消耗平均减少了29.45%，同时不损害其成功率。在五个额外的基准测试（数学推理、代码生成和问答）和各种最先进的基础模型上的广泛实验验证了我们方法的广泛适用性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;SupervisorAgent是一个有效的解决方案，可以解决多智能体系统中的效率问题，同时保持其性能，代码已开源。&lt;h4&gt;翻译&lt;/h4&gt;虽然多智能体系统在复杂任务中表现出色，但随着操作复杂性的增加，它们的自主性增强常常导致关键效率问题，如过度消耗令牌和因错误信息导致的失败。现有方法主要关注事后故障归因，缺乏主动的、实时干预来增强鲁棒性和效率。为此，我们引入了SupervisorAgent，一个轻量级和模块化的框架，用于运行时自适应监督，无需更改基础智能体的架构。由无LLM的自适应过滤器触发，SupervisorAgent在关键时刻进行干预，主动纠正错误，指导低效行为，并净化观测信息。在具有挑战性的GAIA基准测试中，SupervisorAgent将Smolagent框架的令牌消耗平均减少了29.45%，同时不损害其成功率。在五个额外的基准测试（数学推理、代码生成和问答）和各种最先进的基础模型上的广泛实验验证了我们方法的广泛适用性和鲁棒性。代码可在https://github.com/LINs-lab/SupervisorAgent获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While Multi-Agent Systems (MAS) excel at complex tasks, their growingautonomy with operational complexity often leads to critical inefficiencies,such as excessive token consumption and failures arising from misinformation.Existing methods primarily focus on post-hoc failure attribution, lackingproactive, real-time interventions to enhance robustness and efficiency. Tothis end, we introduce SupervisorAgent, a lightweight and modular framework forruntime, adaptive supervision that operates without altering the base agent'sarchitecture. Triggered by an LLM-free adaptive filter, SupervisorAgentintervenes at critical junctures to proactively correct errors, guideinefficient behaviors, and purify observations. On the challenging GAIAbenchmark, SupervisorAgent reduces the token consumption of the Smolagentframework by an average of 29.45% without compromising its success rate.Extensive experiments across five additional benchmarks (math reasoning, codegeneration, and question answering) and various SoTA foundation models validatethe broad applicability and robustness of our approach. The code is availableat https://github.com/LINs-lab/SupervisorAgent.</description>
      <author>example@mail.com (Fulin Lin, Shaowen Chen, Ruishan Fang, Hongwei Wang, Tao Lin)</author>
      <guid isPermaLink="false">2510.26585v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MedSAE: Dissecting MedCLIP Representations with Sparse Autoencoders</title>
      <link>http://arxiv.org/abs/2510.26411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过应用医学稀疏自编码器(MedSAEs)提高医疗视觉中的机制可解释性，在保持高性能的同时增加了AI模型的透明度。&lt;h4&gt;背景&lt;/h4&gt;人工智能在医疗保健领域需要既准确又可解释的模型，特别是在医疗视觉分析方面。&lt;h4&gt;目的&lt;/h4&gt;通过将MedSAEs应用于MedCLIP的潜在空间来提高医疗视觉中的机制可解释性，MedCLIP是一种在胸部X光片和报告上训练的视觉-语言模型。&lt;h4&gt;方法&lt;/h4&gt;提出一个结合相关性指标、熵分析和通过MedGEMMA基础模型进行自动神经元命名的评估框架，并在CheXpert数据集上进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;MedSAE神经元比原始MedCLIP特征实现了更高的单语义性和可解释性。&lt;h4&gt;结论&lt;/h4&gt;研究结果连接了高性能医疗AI和透明度，为发展临床可靠表示提供了可扩展的步骤。&lt;h4&gt;翻译&lt;/h4&gt;医疗保健中的人工智能需要准确且可解释的模型。我们通过将医学稀疏自编码器(MedSAEs)应用于MedCLIP的潜在空间，推进了医疗视觉中的机制可解释性，MedCLIP是一种在胸部X光片和报告上训练的视觉-语言模型。为了量化可解释性，我们提出一个结合相关性指标、熵分析和通过MedGEMMA基础模型进行自动神经元命名的评估框架。在CheXpert数据集上的实验表明，MedSAE神经元比原始MedCLIP特征具有更高的单语义性和可解释性。我们的研究结果连接了高性能医疗AI和透明度，为向临床可靠表示发展提供了可扩展的步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence in healthcare requires models that are accurate andinterpretable. We advance mechanistic interpretability in medical vision byapplying Medical Sparse Autoencoders (MedSAEs) to the latent space of MedCLIP,a vision-language model trained on chest radiographs and reports. To quantifyinterpretability, we propose an evaluation framework that combines correlationmetrics, entropy analyzes, and automated neuron naming via the MedGEMMAfoundation model. Experiments on the CheXpert dataset show that MedSAE neuronsachieve higher monosemanticity and interpretability than raw MedCLIP features.Our findings bridge high-performing medical AI and transparency, offering ascalable step toward clinically reliable representations.</description>
      <author>example@mail.com (Riccardo Renzulli, Colas Lepoutre, Enrico Cassano, Marco Grangetto)</author>
      <guid isPermaLink="false">2510.26411v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards Explainable and Reliable AI in Finance</title>
      <link>http://arxiv.org/abs/2510.26353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了金融领域可解释和可靠AI的三种方法，包括Time-LLM模型使用提示避免错误预测、结合基础模型与可靠性估计器过滤不可靠预测，以及使用符号推理编码领域规则提供透明解释。&lt;h4&gt;背景&lt;/h4&gt;金融预测越来越多地使用大型神经网络模型，但这些模型的透明度低，对信任和监管合规性提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;提出几种金融领域可解释和可靠AI的方法，以解决模型的透明度和可靠性问题。&lt;h4&gt;方法&lt;/h4&gt;1) 描述Time-LLM（时间序列基础模型）如何使用提示来避免错误的方向性预测；2) 展示将时间序列预测的基础模型与可靠性估计器结合可以过滤不可靠的预测；3) 主张使用符号推理来编码领域规则，以提供透明的解释。&lt;h4&gt;主要发现&lt;/h4&gt;这些方法强调只执行既可靠又可解释的预测；在股票和加密货币数据上的实验表明，该架构减少了误报并支持选择性执行。&lt;h4&gt;结论&lt;/h4&gt;通过整合预测性能、可靠性估计和基于规则的推理，该框架推进了透明和可审计的金融AI系统。&lt;h4&gt;翻译&lt;/h4&gt;金融预测越来越多地使用大型神经网络模型，但其不透明性对信任和监管合规性提出了挑战。我们提出了几种金融领域可解释和可靠AI的方法。首先，我们描述了Time-LLM（时间序列基础模型）如何使用提示来避免错误的方向性预测。其次，我们展示了将时间序列预测的基础模型与可靠性估计器结合可以过滤不可靠的预测。第三，我们主张使用符号推理来编码领域规则以提供透明的解释。这些方法强调只执行既可靠又可解释的预测。在股票和加密货币数据上的实验表明，该架构减少了误报并支持选择性执行。通过整合预测性能、可靠性估计和基于规则的推理，我们的框架推进了透明和可审计的金融AI系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Financial forecasting increasingly uses large neural network models, buttheir opacity raises challenges for trust and regulatory compliance. We presentseveral approaches to explainable and reliable AI in finance. \emph{First}, wedescribe how Time-LLM, a time series foundation model, uses a prompt to avoid awrong directional forecast. \emph{Second}, we show that combining foundationmodels for time series forecasting with a reliability estimator can filter ourunreliable predictions. \emph{Third}, we argue for symbolic reasoning encodingdomain rules for transparent justification. These approaches shift emphasizeexecuting only forecasts that are both reliable and explainable. Experiments onequity and cryptocurrency data show that the architecture reduces falsepositives and supports selective execution. By integrating predictiveperformance with reliability estimation and rule-based reasoning, our frameworkadvances transparent and auditable financial AI systems.</description>
      <author>example@mail.com (Albi Isufaj, Pablo Mollá, Helmut Prendinger)</author>
      <guid isPermaLink="false">2510.26353v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>ConceptScope: Characterizing Dataset Bias via Disentangled Visual Concepts</title>
      <link>http://arxiv.org/abs/2510.26186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Thirty-Ninth Conference on Neural Information  Processing Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ConceptScope框架，用于通过发现和量化人类可解释的概念来分析视觉数据集中的偏差。&lt;h4&gt;背景&lt;/h4&gt;数据集偏差在机器学习数据集中普遍存在，但系统性识别这些偏差具有挑战性，因为需要昂贵的、细粒度的属性标注。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展且自动化的框架来分析视觉数据集，通过发现和量化人类可解释的概念来识别和量化数据集偏差。&lt;h4&gt;方法&lt;/h4&gt;ConceptScope使用在视觉基础模型表示上训练的稀疏自编码器来发现和量化人类可解释的概念。根据概念与类标签的语义相关性和统计相关性，将概念分为目标、上下文和偏差类型，从而实现类级别的数据集特征描述、偏差识别和鲁棒性评估。&lt;h4&gt;主要发现&lt;/h4&gt;ConceptScope能够捕获广泛的视觉概念，包括物体、纹理、背景、面部属性、情绪和动作。概念激活产生的空间归因与语义上有意义的图像区域一致。该框架能够可靠地检测已知偏差（如Waterbirds中的背景偏差）并发现先前未标注的偏差（如ImageNet中共现的物体）。&lt;h4&gt;结论&lt;/h4&gt;ConceptScope为数据集审计和模型诊断提供了实用的工具。&lt;h4&gt;翻译&lt;/h4&gt;数据集偏差在机器学习数据集无处不在，其中数据点偏向于某些概念。然而，在没有昂贵的细粒度属性标注的情况下，系统性地识别这些偏差具有挑战性。我们提出了ConceptScope，这是一个可扩展且自动化的框架，用于通过使用在视觉基础模型表示上训练的稀疏自编码器发现和量化人类可解释的概念来分析视觉数据集。ConceptScope根据概念与类标签的语义相关性和统计相关性将概念分为目标、上下文和偏差类型，从而通过基于概念的子分组实现类级别的数据集特征描述、偏差识别和鲁棒性评估。通过与标注数据集的比较，我们验证了ConceptScope能够捕获广泛的视觉概念，包括物体、纹理、背景、面部属性、情绪和动作。此外，我们表明概念激活产生的空间归因与语义上有意义的图像区域一致。ConceptScope可靠地检测了已知偏差（例如Waterbirds中的背景偏差）并发现了先前未标注的偏差（例如ImageNet中共现的物体），为数据集审计和模型诊断提供了实用的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dataset bias, where data points are skewed to certain concepts, is ubiquitousin machine learning datasets. Yet, systematically identifying these biases ischallenging without costly, fine-grained attribute annotations. We presentConceptScope, a scalable and automated framework for analyzing visual datasetsby discovering and quantifying human-interpretable concepts using SparseAutoencoders trained on representations from vision foundation models.ConceptScope categorizes concepts into target, context, and bias types based ontheir semantic relevance and statistical correlation to class labels, enablingclass-level dataset characterization, bias identification, and robustnessevaluation through concept-based subgrouping. We validate that ConceptScopecaptures a wide range of visual concepts, including objects, textures,backgrounds, facial attributes, emotions, and actions, through comparisons withannotated datasets. Furthermore, we show that concept activations producespatial attributions that align with semantically meaningful image regions.ConceptScope reliably detects known biases (e.g., background bias inWaterbirds) and uncovers previously unannotated ones (e.g, co-occurring objectsin ImageNet), offering a practical tool for dataset auditing and modeldiagnostics.</description>
      <author>example@mail.com (Jinho Choi, Hyesu Lim, Steffen Schneider, Jaegul Choo)</author>
      <guid isPermaLink="false">2510.26186v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Deep Neural Watermarking for Robust Copyright Protection in 3D Point Clouds</title>
      <link>http://arxiv.org/abs/2510.27533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于3D点云版权保护和所有权验证的鲁棒深度神经水印框架，通过奇异值分解将二进制水印嵌入到3D点云块中，并利用PointNet++神经网络架构实现水印的可靠提取。&lt;h4&gt;背景&lt;/h4&gt;随着数字媒体中三维内容的快速增长，知识产权保护变得至关重要。与传统的图像或视频不同，3D点云在版权执行方面面临独特挑战，因为它们特别容易受到各种几何和非几何攻击的影响，这些攻击可以轻易降低或移除传统水印信号。&lt;h4&gt;目的&lt;/h4&gt;解决3D点云在版权保护中的挑战，提出一种能够抵抗各种攻击的鲁棒深度神经水印框架，用于3D点云的版权保护和所有权验证。&lt;h4&gt;方法&lt;/h4&gt;使用奇异值分解(SVD)将二进制水印嵌入到3D点云块的奇异值中，并利用PointNet++神经网络架构的深度学习提取能力。训练网络以在数据经过旋转、缩放、噪声、裁剪和信号失真等各种攻击后仍能可靠提取水印。&lt;h4&gt;主要发现&lt;/h4&gt;在ModelNet40数据集上的验证表明，深度学习提取方法在具有挑战性的条件下显著优于传统的SVD技术。对于实验中最严重的几何失真——裁剪(70%)攻击，深度学习方法实现了0.83的位准确度和0.80的交并比(IoU)，而传统SVD方法仅实现了0.58的位准确度和0.26的IoU。&lt;h4&gt;结论&lt;/h4&gt;该方法即使在严重失真条件下也能实现卓越的水印恢复并保持高保真度，证明了其在实际应用中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;随着数字媒体中三维内容的快速增长，知识产权保护变得至关重要。与传统的图像或视频不同，3D点云在版权执行方面面临独特挑战，因为它们特别容易受到各种几何和非几何攻击的影响，这些攻击可以轻易降低或移除传统水印信号。在本文中，我们通过提出一种用于3D点云版权保护和所有权验证的鲁棒深度神经水印框架来解决这些挑战。我们的方法使用奇异值分解(SVD)将二进制水印嵌入到3D点云块的奇异值中，并利用PointNet++神经网络架构的深度学习提取能力。网络经过训练，即使在数据经过旋转、缩放、噪声、裁剪和信号失真等各种攻击后也能可靠提取水印。我们使用公开的ModelNet40数据集验证了我们的方法，证明在具有挑战性的条件下，基于深度学习的提取显著优于传统的SVD技术。我们的实验评估表明，基于深度学习的提取方法显著优于现有的SVD方法，深度学习在裁剪(70%)攻击(实验中最严重的几何失真)下实现了0.83的位准确度和0.80的交并比(IoU)，而SVD仅实现了0.58的位准确度和0.26的IoU。这证明了我们的方法即使在严重失真条件下也能实现卓越的水印恢复并保持高保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.33166/AETiC.2025.05.002&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The protection of intellectual property has become critical due to the rapidgrowth of three-dimensional content in digital media. Unlike traditional imagesor videos, 3D point clouds present unique challenges for copyright enforcement,as they are especially vulnerable to a range of geometric and non-geometricattacks that can easily degrade or remove conventional watermark signals. Inthis paper, we address these challenges by proposing a robust deep neuralwatermarking framework for 3D point cloud copyright protection and ownershipverification. Our approach embeds binary watermarks into the singular values of3D point cloud blocks using spectral decomposition, i.e. Singular ValueDecomposition (SVD), and leverages the extraction capabilities of Deep Learningusing PointNet++ neural network architecture. The network is trained toreliably extract watermarks even after the data undergoes various attacks suchas rotation, scaling, noise, cropping and signal distortions. We validated ourmethod using the publicly available ModelNet40 dataset, demonstrating that deeplearning-based extraction significantly outperforms traditional SVD-basedtechniques under challenging conditions. Our experimental evaluationdemonstrates that the deep learning-based extraction approach significantlyoutperforms existing SVD-based methods with deep learning achieving bitwiseaccuracy up to 0.83 and Intersection over Union (IoU) of 0.80, compared to SVDachieving a bitwise accuracy of 0.58 and IoU of 0.26 for the Crop (70%) attack,which is the most severe geometric distortion in our experiment. Thisdemonstrates our method's ability to achieve superior watermark recovery andmaintain high fidelity even under severe distortions.</description>
      <author>example@mail.com (Khandoker Ashik Uz Zaman, Mohammad Zahangir Alam, Mohammed N. M. Ali, Mahdi H. Miraz)</author>
      <guid isPermaLink="false">2510.27533v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Modal Neuro-Symbolic Approach for Spatial Reasoning-Based Visual Grounding in Robotics</title>
      <link>http://arxiv.org/abs/2510.27033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种整合全景图像和3D点云信息的神经符号框架，结合神经感知与符号推理，用于解决视觉推理中的细粒度空间推理问题，在拥挤的人造环境中表现出优越性能和可靠性，同时保持轻量级设计。&lt;h4&gt;背景&lt;/h4&gt;视觉推理，特别是空间推理，是一个具有挑战性的认知任务，需要理解物体关系及其在复杂环境中的交互。现有的视觉语言模型在感知任务上表现出色，但在细粒度空间推理方面存在困难，因为它们采用隐式、基于相关性的推理且仅依赖图像。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经符号框架，整合全景图像和3D点云信息，结合神经感知与符号推理，以明确建模空间和逻辑关系。&lt;h4&gt;方法&lt;/h4&gt;框架由感知模块(用于检测实体和提取属性)和推理模块(构建结构化的场景图以支持精确、可解释的查询)组成。&lt;h4&gt;主要发现&lt;/h4&gt;在JRDB-Reasoning数据集上评估，该方法在拥挤的人造环境中表现出优越的性能和可靠性，同时保持轻量级设计。&lt;h4&gt;结论&lt;/h4&gt;该框架适用于机器人和具身AI应用。&lt;h4&gt;翻译&lt;/h4&gt;视觉推理，特别是空间推理，是一项具有挑战性的认知任务，需要理解物体关系及其在复杂环境中的交互，尤其是在机器人领域。现有的视觉语言模型在感知任务上表现出色，但由于其隐式、基于相关性的推理和仅依赖图像的能力，在细粒度空间推理方面存在困难。我们提出了一种新的神经符号框架，整合全景图像和3D点云信息，结合神经感知与符号推理，以明确建模空间和逻辑关系。我们的框架由感知模块(用于检测实体和提取属性)和推理模块(构建结构化的场景图以支持精确、可解释的查询)组成。在JRDB-Reasoning数据集上的评估表明，该方法在拥挤的人造环境中表现出优越的性能和可靠性，同时保持轻量级设计，适用于机器人和具身AI应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人在复杂环境中进行细粒度空间推理的问题，特别是在理解物体和人类之间精确空间关系方面的挑战。这个问题很重要，因为空间推理是机器人导航和交互的基础能力，在人类建造的拥挤环境中，机器人需要准确理解多个实体之间的空间关系来完成各种任务，而现有模型在这方面存在明显不足，导致在机器人、导航和具身AI应用中可靠性不高。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有视觉-语言模型(VLMs)在空间推理方面的局限性，包括其隐式推理方式、仅依赖2D图像而忽略3D信息、以及在相对人类定位等任务中的表现不佳。基于这些分析，作者设计了一个结合神经感知与符号推理的框架，借鉴了基础视觉-语言主干网络用于特征提取，以及场景图概念来表示实体关系。通过整合全景图像和3D点云信息，作者构建了一个能够进行显式几何和逻辑结构推理的系统，从而克服了现有模型的不足。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合神经感知与符号推理，利用全景图像和3D点云信息进行更准确、可解释的空间推理，通过构建场景图作为中间推理层来支持精确查询。整体流程分为两个主要部分：感知部分和推理部分。感知部分包括特征提取模块(检测实体并提取属性)和投影模块(整合语义特征与几何关系)；推理部分是图搜索模块，包含句子解析(将查询转换为结构化表示)和搜索算法(在场景图上查找答案)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多模态轻量级框架，整合全景图像和3D点云信息，参数少(1.3B)且适合机器人应用；2) 新颖的显式符号推理，通过构建场景图减少推理错误；3) 在拥挤环境中表现出优越的性能和可靠性。相比之前的工作，不同之处在于：结合了2D图像和3D点云信息而非仅依赖2D图像；使用显式符号推理结构而非隐式统计相关性；轻量级设计同时实现高级推理能力；特别擅长处理细粒度空间关系和复杂查询。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种轻量级多模态神经符号框架，通过整合全景图像和3D点云信息，结合神经感知与符号推理，显著提高了机器人在复杂环境中进行细粒度空间推理的能力，同时保持了模型的轻量化和可解释性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual reasoning, particularly spatial reasoning, is a challenging cognitivetask that requires understanding object relationships and their interactionswithin complex environments, especially in robotics domain. Existingvision_language models (VLMs) excel at perception tasks but struggle withfine-grained spatial reasoning due to their implicit, correlation-drivenreasoning and reliance solely on images. We propose a novel neuro_symbolicframework that integrates both panoramic-image and 3D point cloud information,combining neural perception with symbolic reasoning to explicitly model spatialand logical relationships. Our framework consists of a perception module fordetecting entities and extracting attributes, and a reasoning module thatconstructs a structured scene graph to support precise, interpretable queries.Evaluated on the JRDB-Reasoning dataset, our approach demonstrates superiorperformance and reliability in crowded, human_built environments whilemaintaining a lightweight design suitable for robotics and embodied AIapplications.</description>
      <author>example@mail.com (Simindokht Jahangard, Mehrzad Mohammadi, Abhinav Dhall, Hamid Rezatofighi)</author>
      <guid isPermaLink="false">2510.27033v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Active transfer learning for structural health monitoring</title>
      <link>http://arxiv.org/abs/2510.27525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种贝叶斯框架用于基于群体的结构健康监测中的域适应，结合主动采样策略提高数据效率，减少标记数据需求，降低结构运营成本。&lt;h4&gt;背景&lt;/h4&gt;用于训练结构健康监测系统的数据通常昂贵且难以获取，特别是标记数据。基于群体的结构健康监测试图通过利用多个结构的数据解决这一问题，但不同结构的数据分布差异可能导致传统机器学习方法产生较大泛化误差。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用有限标记目标数据改进无监督域适应映射的贝叶斯框架，并将其与主动采样策略集成，指导检查选择最有信息量的观察结果进行标记。&lt;h4&gt;方法&lt;/h4&gt;使用域适应技术对齐数据分布，提出贝叶斯框架用于群体结构健康监测中的域适应，集成主动采样策略指导检查，并在实验桥梁群体上评估该方法的有效性，包括多种损伤状态和环境条件的数据。&lt;h4&gt;主要发现&lt;/h4&gt;结合迁移学习和主动学习可以在标记稀缺场景中提高学习分类模型的数据效率，对数据驱动的结构运营和维护有重要影响。&lt;h4&gt;结论&lt;/h4&gt;通过采用所提出的方法，可以减少结构运营寿命内的检查次数，从而降低运营成本，同时保持有效的结构健康监测。&lt;h4&gt;翻译&lt;/h4&gt;用于训练结构健康监测系统的数据通常昂贵且/或不切实际，特别是对于标记数据。基于群体的结构健康监测旨在通过利用来自多个结构的数据来解决这一限制。然而，来自不同结构的数据将遵循不同的分布，可能导致通过传统机器学习方法学习的模型产生较大的泛化误差。为了解决这个问题，可以采用迁移学习--以域适应的形式--来对齐数据分布。大多数先前的方法只考虑了无监督域适应，其中没有可用的标记目标数据；它们没有考虑如何将这些技术整合到在线框架中--随着在整个监测过程中获得标签而进行更新。本文提出了用于群体结构健康监测中域适应的贝叶斯框架，可以使用有限数量的标记目标数据改进无监督域适应映射。此外，该模型被集成到主动采样策略中，指导检查选择最有信息量的观察结果进行标记--从而进一步减少学习目标分类器所需的标记数据。该方法的有效性在一组实验桥梁群体上进行了评估。具体而言，该群体包括对应于多种损伤状态的数据，以及一套全面的环境条件。研究发现，结合迁移学习和主动学习可以在标记稀缺场景中提高学习分类模型时的数据效率。这一结果对数据驱动的结构运营和维护有影响，表明可以在结构的运营寿命内减少检查次数--从而降低运营成本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.ymssp.2025.113260&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data for training structural health monitoring (SHM) systems are oftenexpensive and/or impractical to obtain, particularly for labelled data.Population-based SHM (PBSHM) aims to address this limitation by leveraging datafrom multiple structures. However, data from different structures will followdistinct distributions, potentially leading to large generalisation errors formodels learnt via conventional machine learning methods. To address this issue,transfer learning -- in the form of domain adaptation (DA) -- can be used toalign the data distributions. Most previous approaches have only considered\emph{unsupervised} DA, where no labelled target data are available; they donot consider how to incorporate these technologies in an online framework --updating as labels are obtained throughout the monitoring campaign. This paperproposes a Bayesian framework for DA in PBSHM, that can improve unsupervised DAmappings using a limited quantity of labelled target data. In addition, thismodel is integrated into an active sampling strategy to guide inspections toselect the most informative observations to label -- leading to furtherreductions in the required labelled data to learn a target classifier. Theeffectiveness of this methodology is evaluated on a population of experimentalbridges. Specifically, this population includes data corresponding to severaldamage states, as well as, a comprehensive set of environmental conditions. Itis found that combining transfer learning and active learning can improve dataefficiency when learning classification models in label-scarce scenarios. Thisresult has implications for data-informed operation and maintenance ofstructures, suggesting a reduction in inspections over the operational lifetimeof a structure -- and therefore a reduction in operational costs -- can beachieved.</description>
      <author>example@mail.com (J. Poole, N. Dervilis, K. Worden, P. Gardner, V. Giglioni, R. S. Mills, A. J. Hughes)</author>
      <guid isPermaLink="false">2510.27525v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>pDANSE: Particle-based Data-driven Nonlinear State Estimation from Nonlinear Measurements</title>
      <link>http://arxiv.org/abs/2510.27503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 10 figures, under review at IEEE Transactions on Signal  Processing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种粒子数据驱动非线性状态估计方法(pDANSE)，用于处理状态转换模型未知的模型自由过程，通过循环神经网络和基于重参数化技巧的粒子采样方法处理非线性测量系统，实现了高效的状态估计。&lt;h4&gt;背景&lt;/h4&gt;传统数据驱动非线性状态估计方法(DANSE)在处理线性测量系统时可获得状态后验的闭式解，但面对非线性测量系统时这种方法不再适用，需要开发新的解决方案。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理非线性测量系统的状态估计方法，避免使用计算密集的顺序蒙特卡洛(SMC)和祖先采样，并支持在有标签和无标签数据下分别进行半监督和无监督学习。&lt;h4&gt;方法&lt;/h4&gt;使用循环神经网络(RNN)提供高斯先验参数描述模型自由过程状态，采用基于重参数化技巧的粒子采样方法处理非线性测量，估计状态后验的二阶统计量，并在随机Lorenz-63系统上验证性能。&lt;h4&gt;主要发现&lt;/h4&gt;pDANSE能有效利用顺序测量避免计算密集方法，在立方非线性、相机模型非线性(无监督学习)以及半波整流非线性、笛卡尔到球面非线性(半监督学习)四种测量系统上均表现良好，性能与具有完整状态转换模型知识的粒子滤波器相当。&lt;h4&gt;结论&lt;/h4&gt;pDANSE方法成功解决了模型自由过程的非线性测量状态估计问题，通过创新方法实现了高效准确的状态估计，为未知模型系统的状态估计提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑设计一种数据驱动的非线性状态估计方法，该方法使用（带噪声的）非线性测量值来处理底层状态转换模型未知的过程。这样的过程被称为无模型过程。循环神经网络提供高斯先验的参数，该参数使用给定时间点的所有先前测量值来描述无模型过程的状态。在DANSE的情况下，测量系统是线性的，从而得到状态后验的闭式解。然而，非线性测量系统的存在使得闭式解不可行。相反，使用在时间点观察到的非线性测量值来计算状态后验的二阶统计量。我们使用基于重参数化技巧的粒子采样方法处理非线性测量，并估计状态后验的二阶统计量。所提出的方法被称为基于粒子的DANSE(pDANSE)。pDANSE的RNN有效利用顺序测量，避免了使用计算密集的顺序蒙特卡洛和/或祖先采样。我们描述了pDANSE的半监督学习方法，在没有标签数据的情况下过渡到无监督学习。使用随机Lorenz-63系统作为基准过程，我们实验性地展示了四种非线性测量系统的状态估计性能。我们探索了立方非线性和相机模型非线性，这里使用无监督学习；然后我们探索了半波整流非线性和笛卡尔到球面非线性，这里使用半监督学习。状态估计的性能被证明与具有完整Lorenz-63系统状态转换模型知识的粒子滤波器相比具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the problem of designing a data-driven nonlinear state estimation(DANSE) method that uses (noisy) nonlinear measurements of a process whoseunderlying state transition model (STM) is unknown. Such a process is referredto as a model-free process. A recurrent neural network (RNN) providesparameters of a Gaussian prior that characterize the state of the model-freeprocess, using all previous measurements at a given time point. In the case ofDANSE, the measurement system was linear, leading to a closed-form solution forthe state posterior. However, the presence of a nonlinear measurement systemrenders a closed-form solution infeasible. Instead, the second-order statisticsof the state posterior are computed using the nonlinear measurements observedat the time point. We address the nonlinear measurements using areparameterization trick-based particle sampling approach, and estimate thesecond-order statistics of the state posterior. The proposed method is referredto as particle-based DANSE (pDANSE). The RNN of pDANSE uses sequentialmeasurements efficiently and avoids the use of computationally intensivesequential Monte-Carlo (SMC) and/or ancestral sampling. We describe thesemi-supervised learning method for pDANSE, which transitions to unsupervisedlearning in the absence of labeled data. Using a stochastic Lorenz-$63$ systemas a benchmark process, we experimentally demonstrate the state estimationperformance for four nonlinear measurement systems. We explore cubicnonlinearity and a camera-model nonlinearity where unsupervised learning isused; then we explore half-wave rectification nonlinearity andCartesian-to-spherical nonlinearity where semi-supervised learning is used. Theperformance of state estimation is shown to be competitive vis-\`a-vis particlefilters that have complete knowledge of the STM of the Lorenz-$63$ system.</description>
      <author>example@mail.com (Anubhab Ghosh, Yonina C. Eldar, Saikat Chatterjee)</author>
      <guid isPermaLink="false">2510.27503v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>UNILocPro: Unified Localization Integrating Model-Based Geometry and Channel Charting</title>
      <link>http://arxiv.org/abs/2510.27394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UNILocPro的统一定位框架，结合基于模型的定位和信道映射方法，用于处理混合视距/非视距场景。该框架通过自适应激活两种方法并使用多种损失函数进行无监督学习，显著提高了定位精度。同时提出了低复杂度的UNILoc实现，大幅降低训练复杂度而性能仅有轻微下降。&lt;h4&gt;背景&lt;/h4&gt;在混合视距(LoS)/非视距(NLoS)场景中，单一的定位方法难以满足高精度需求。基于模型的定位方法和信道映射(Channel Charting, CC)方法各有优势，但单独使用时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种统一框架，有效结合基于模型的定位和信道映射方法，提高混合LoS/NLoS场景下的定位精度，同时降低训练复杂度。&lt;h4&gt;方法&lt;/h4&gt;1. 提出UNILocPro框架，根据LoS/NLoS识别自适应激活基于模型和基于CC的方法；2. 利用基于模型方法的信息训练CC模型；3. 使用多种损失函数：成对距离损失、三元组损失(如果有时间戳)、基于LoS的损失和基于最优传输(OT)的损失；4. 提出低复杂度实现UNILoc，使用自生成标签训练CC模型，避免迭代Sinkhorn更新。&lt;h4&gt;主要发现&lt;/h4&gt;1. 统一框架比单独的基于模型和基于CC的方法显著提高了定位精度；2. 带有时间戳的UNILocPro性能与完全监督的指纹识别相当，无需标记训练数据；3. UNILoc大幅降低训练复杂度，性能仅有轻微下降。&lt;h4&gt;结论&lt;/h4&gt;UNILocPro框架有效结合了两种定位方法的优势，在混合LoS/NLoS场景中实现了高精度定位。UNLoc作为低复杂度实现，在实际应用中具有更好的实用性。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种统一的定位框架（称为UNILocPro），该框架集成了基于模型的定位和信道映射（CC）方法，用于处理混合视距（LoS）/非视距（NLoS）场景。具体而言，基于LoS/NLoS识别，在基于模型和基于CC的方法之间进行自适应激活。针对无监督学习，利用基于模型方法获得的信息来训练CC模型，联合使用成对距离损失（涉及新的不相似度度量设计）、三元组损失（如果有时间戳）、基于LoS的损失和基于最优传输（OT）的损失，以保持全局几何结构。为了减少UNILocPro的训练复杂度，我们提出了一种低复杂度实现（称为UNILoc），其中CC模型使用通过单个预训练OT转换生成的自生成标签进行训练，避免了OT损失计算中涉及的迭代Sinkhorn更新。大量的数值实验表明，所提出的统一框架比基于模型和基于CC的方法显著提高了定位精度。值得注意的是，带有时间戳的UNILocPro性能与完全监督的指纹识别相当，尽管它不使用标记的训练数据。研究还表明，低复杂度的UNLoc可以显著减少训练复杂度，而性能仅有轻微下降。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a unified localization framework (called UNILocPro)that integrates model-based localization and channel charting (CC) for mixedline-of-sight (LoS)/non-line-of-sight (NLoS) scenarios. Specifically, based onLoS/NLoS identification, an adaptive activation between the model-based andCC-based methods is conducted. Aiming for unsupervised learning, informationobtained from the model-based method is utilized to train the CC model, where apairwise distance loss (involving a new dissimilarity metric design), a tripletloss (if timestamps are available), a LoS-based loss, and an optimal transport(OT)-based loss are jointly employed such that the global geometry can be wellpreserved. To reduce the training complexity of UNILocPro, we propose alow-complexity implementation (called UNILoc), where the CC model is trainedwith self-generated labels produced by a single pre-training OT transformation,which avoids iterative Sinkhorn updates involved in the OT-based losscomputation. Extensive numerical experiments demonstrate that the proposedunified frameworks achieve significantly improved positioning accuracy comparedto both model-based and CC-based methods. Notably, UNILocPro with timestampsattains performance on par with fully-supervised fingerprinting despiteoperating without labelled training data. It is also shown that thelow-complexity UNILoc can substantially reduce training complexity with onlymarginal performance degradation.</description>
      <author>example@mail.com (Yuhao Zhang, Guangjin Pan, Musa Furkan Keskin, Ossi Kaltiokallio, Mikko Valkama, Henk Wymeersch)</author>
      <guid isPermaLink="false">2510.27394v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Soft Task-Aware Routing of Experts for Equivariant Representation Learning</title>
      <link>http://arxiv.org/abs/2510.27222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了软任务感知路由（STAR）策略，用于解决等变表示学习和不变表示学习联合训练中的冗余特征学习问题。STAR通过将投影头建模为专家，促使它们专门捕捉共享信息或任务特定信息，从而减少冗余并提高模型效率。&lt;h4&gt;背景&lt;/h4&gt;等变表示学习旨在捕捉输入变换在表示空间中引起的变异，而不变表示学习通过忽略这些变换来编码语义信息。最近研究表明，联合学习这两种表示对下游任务有益，通常通过使用单独的投影头实现。&lt;h4&gt;目的&lt;/h4&gt;解决不变学习和等变学习联合训练中信息共享被忽略的问题，减少冗余特征学习，提高模型容量的利用效率。&lt;h4&gt;方法&lt;/h4&gt;引入软任务感知路由（STAR）策略，将投影头建模为专家，促使它们专门捕捉共享信息或任务特定信息。&lt;h4&gt;主要发现&lt;/h4&gt;STAR策略使不变嵌入和等变嵌入之间的标准相关性降低，减少了冗余特征学习。实验结果表明，STAR在各种迁移学习任务中实现了持续改进。&lt;h4&gt;结论&lt;/h4&gt;STAR通过有效的任务感知路由策略，解决了等变和不变表示学习中的冗余问题，提高了模型效率，在各种迁移学习任务中展现了优越性能。&lt;h4&gt;翻译&lt;/h4&gt;等变表示学习旨在捕捉输入变换在表示空间中引起的变异，而不变表示学习通过忽略这些变换来编码语义信息。最近研究表明，联合学习这两种表示通常对下游任务有益，通常通过使用单独的投影头实现。然而，这种设计忽略了不变学习和等变学习之间共享的信息，导致冗余的特征学习和模型容量的低效利用。为此，我们引入了软任务感知路由（STAR），这是一种针对投影头的路由策略，将它们建模为专家。STAR促使专家专门捕捉共享信息或任务特定信息，从而减少冗余的特征学习。我们通过观察不变嵌入和等变嵌入之间较低的标准相关性来验证这一效果。实验结果表明在各种迁移学习任务中都有持续改进。代码可在https://github.com/YonseiML/star获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决不变表示学习和等变表示学习之间的冗余特征学习问题。当使用两个独立投影头分别处理这两个任务时，它们会冗余地捕获共享信息，导致模型容量使用效率低下。这个问题很重要，因为不变和等变学习实际上是相互依赖的，而非完全独立，传统方法忽略这种依赖关系会导致模型性能受限，同时保留语义内容(不变性)和变换相关信息(等变性)对许多视觉任务至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过'陨石坑错觉'例子识别了不变和等变学习间的相互依赖关系，意识到传统双分支方法会导致冗余特征学习。为此，他们设计了两种形式的Soft Task-Aware Routing (STAR)：单一共享投影头(添加一个共享投影头提供共同嵌入)和MMoE投影(采用多门控混合专家架构动态分配专家)。作者借鉴了现有的混合专家框架，特别是MMoE架构，但创新性地将其仅限制在预训练阶段的投影头中，解决了传统MoE模型在跨任务转移中的可转移性问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过专家路由显式协调共享信息和任务特定信息，减少冗余特征学习。整体流程为：1)为输入图像生成两个增强视图；2)用共享编码器提取潜在表示；3)通过STAR投影模块生成不变和等变嵌入——SS版本使用三个专家(不变、等变和共享)输出相加，MMoE版本使用共享专家和任务特定路由器动态分配权重；4)等变学习通过预测器预测变换后的嵌入；5)使用对比损失训练模型；预训练后仅保留编码器用于下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)揭示不变和等变学习间的内在依赖性；2)提出STAR策略协调共享与任务特定信息；3)设计STAR的两种实现形式(单一共享投影和MMoE投影)；4)创新性地将MMoE限制在预训练阶段。相比EquiMod等传统方法，STAR显式建模任务间共享信息，减少冗余特征学习，能根据输入动态分配专家而非使用固定投影头，并在多种下游任务上取得更好性能，同时降低了不变和等变嵌入间的典型相关性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了Soft Task-Aware Routing策略，通过专家路由显式协调不变和等变表示学习中的共享与任务特定信息，有效减少了冗余特征学习，并在多种下游任务上提升了表示学习的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Equivariant representation learning aims to capture variations induced byinput transformations in the representation space, whereas invariantrepresentation learning encodes semantic information by disregarding suchtransformations. Recent studies have shown that jointly learning both types ofrepresentations is often beneficial for downstream tasks, typically byemploying separate projection heads. However, this design overlooks informationshared between invariant and equivariant learning, which leads to redundantfeature learning and inefficient use of model capacity. To address this, weintroduce Soft Task-Aware Routing (STAR), a routing strategy for projectionheads that models them as experts. STAR induces the experts to specialize incapturing either shared or task-specific information, thereby reducingredundant feature learning. We validate this effect by observing lowercanonical correlations between invariant and equivariant embeddings.Experimental results show consistent improvements across diverse transferlearning tasks. The code is available at https://github.com/YonseiML/star.</description>
      <author>example@mail.com (Jaebyeong Jeon, Hyeonseo Jang, Jy-yong Sohn, Kibok Lee)</author>
      <guid isPermaLink="false">2510.27222v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Functional Analysis of Loss-development Patterns in P&amp;C Insurance</title>
      <link>http://arxiv.org/abs/2510.27204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages. Keywords: loss development; loss reserving; incremental  loss ratios; unsupervised learning; functional data; functional depth;  outlier detection; IBNR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文使用函数数据分析方法分析NAIC P计划损失三角形的损失发展，提出了一种基于偏最小二乘回归和函数自举的概率预测框架，能够提供更准确的函数预测区间。&lt;h4&gt;背景&lt;/h4&gt;NAIC P计划损失三角形是保险业常用的损失发展分析工具，传统方法如链梯法在处理不确定性方面存在局限。&lt;h4&gt;目的&lt;/h4&gt;研究增量损失比率的发展模式，识别异常曲线，并开发一种更准确的概率预测方法来估计未来损失发展。&lt;h4&gt;方法&lt;/h4&gt;采用函数数据分析方法，将数据视为3300多条增量损失比率曲线；使用函数数据深度研究发展模式；提出基于偏最小二乘回归的函数模型来完成部分发展的曲线；结合函数自举量化不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;基于公司特定协变量可以识别发展模式的相似性和差异；能够识别异常的增量损失比率曲线；所提出的方法相比链梯法具有更好的概率评分。&lt;h4&gt;结论&lt;/h4&gt;函数数据分析方法为损失发展分析提供了新的视角，所提出的概率预测框架能够提供更准确的函数预测区间，有助于保险公司更好地评估未来损失发展。&lt;h4&gt;翻译&lt;/h4&gt;我们使用函数数据分析方法分析NAIC P计划损失三角形的损失发展。采用函数观点，我们的数据集包含24个事故年份中工人赔偿险种的3300多条增量损失比率曲线。依赖函数数据深度，我们首先基于公司特定协变量研究发展模式的相似性和差异，并识别异常的增量损失比率曲线。探索性发现激励了论文后半部分发展的概率预测框架。我们提出了一种函数模型，基于主成分分析得分的偏最小二乘回归来完成部分发展的增量损失比率曲线。结合上述方法与函数自举，使我们能够量化所有未来滞后的未来增量损失比率的不确定性。我们证明，与链梯法相比，我们的方法具有更好的概率评分，特别是能够提供准确的函数预测区间。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We analyze loss development in NAIC Schedule P loss triangles usingfunctional data analysis methods. Adopting the functional viewpoint, ourdataset comprises 3300+ curves of incremental loss ratios (ILR) of workers'compensation lines over 24 accident years. Relying on functional data depth, wefirst study similarities and differences in development patterns based oncompany-specific covariates, as well as identify anomalous ILR curves.  The exploratory findings motivate the probabilistic forecasting frameworkdeveloped in the second half of the paper. We propose a functional model tocomplete partially developed ILR curves based on partial least squaresregression of PCA scores. Coupling the above with functional bootstrappingallows us to quantify future ILR uncertainty jointly across all future lags. Wedemonstrate that our method has much better probabilistic scores relative toChain Ladder and in particular can provide accurate functional predictiveintervals.</description>
      <author>example@mail.com (Arthur Charpentier, Qiheng Guo, Mike Ludkovski)</author>
      <guid isPermaLink="false">2510.27204v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry</title>
      <link>http://arxiv.org/abs/2510.26008v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, submitted to nsdi 26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Reveal系统，一种仅依赖硬件信号进行机器学习工作负载优化的方法，成功识别系统问题并加速模型性能。&lt;h4&gt;背景&lt;/h4&gt;现代机器学习已发展为紧密结合的全栈生态系统，用户依赖云提供商获取弹性资源，但这些平台即服务使用虚拟化，导致运营商对用户工作负载了解有限。&lt;h4&gt;目的&lt;/h4&gt;论证工作负载知识对于系统级优化不是必需的，提出一种仅依赖硬件信号的优化方法。&lt;h4&gt;方法&lt;/h4&gt;提出Reveal系统，采用以硬件为中心的方法，使用从系统收集的低级信号，通过无监督学习流程检测异常。该流程基于30多种流行ML模型在各种硬件平台上的分析开发，确保对新兴工作负载的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;使用Reveal成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%的性能。&lt;h4&gt;结论&lt;/h4&gt;工作负载知识对于系统级优化不是必需的，通过仅依赖硬件信号，运营商可以实现有效的系统优化。&lt;h4&gt;翻译&lt;/h4&gt;现代机器学习(ML)已发展为紧密结合的全栈生态系统，结合了硬件、软件、网络和应用。许多用户依赖云提供商提供弹性、隔离和成本高效的资源。不幸的是，这些平台即服务使用虚拟化，这意味着运营商对用户的工作负载了解有限。这阻碍了运营商进行资源优化，而资源优化对确保成本效率和最小化执行时间至关重要。在本文中，我们认为工作负载知识对于系统级优化不是必需的。我们提出了Reveal，它采用以硬件为中心的方法，仅依赖硬件信号-运营商完全可以访问。使用从系统收集的低级信号，Reveal通过无监督学习流程检测异常。该流程是通过分析各种硬件平台上超过30种流行ML模型开发的，确保对新兴工作负载和未知部署模式的适应性。使用Reveal，我们成功识别了网络和系统配置问题，将DeepSeek模型加速了5.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern machine learning (ML) has grown into a tightly coupled, full-stackecosystem that combines hardware, software, network, and applications. Manyusers rely on cloud providers for elastic, isolated, and cost-efficientresources. Unfortunately, these platforms as a service use virtualization,which means operators have little insight into the users' workloads. Thishinders resource optimizations by the operator, which is essential to ensurecost efficiency and minimize execution time. In this paper, we argue thatworkload knowledge is unnecessary for system-level optimization. We proposeReveal, which takes a hardware-centric approach, relying only on hardwaresignals - fully accessible by operators. Using low-level signals collected fromthe system, Reveal detects anomalies through an unsupervised learning pipeline.The pipeline is developed by analyzing over 30 popular ML models on varioushardware platforms, ensuring adaptability to emerging workloads and unknowndeployment patterns. Using Reveal, we successfully identified both network andsystem configuration issues, accelerating the DeepSeek model by 5.97%.</description>
      <author>example@mail.com (Ziji Chen, Steven W. D. Chien, Peng Qian, Noa Zilberman)</author>
      <guid isPermaLink="false">2510.26008v2</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>ANCHOR: Integrating Adversarial Training with Hard-mined Supervised Contrastive Learning for Robust Representation Learning</title>
      <link>http://arxiv.org/abs/2510.27599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ANCHOR的框架，通过结合监督对比学习和硬正样本挖掘，提高神经网络模型对抗对抗性攻击的鲁棒性，同时保持较高的准确率。&lt;h4&gt;背景&lt;/h4&gt;神经网络通过遵循梯度学习，逐步调整参数识别数据中的模式，但这种学习机制也使模型容易受到对抗性攻击，即通过微小、不可察觉的输入变化导致模型做出错误判断。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够学习更稳定、更有意义的模式表示的方法，使模型对对抗性攻击更加鲁棒，同时保持高准确率。&lt;h4&gt;方法&lt;/h4&gt;提出ANCHOR框架，利用监督对比学习和显式硬正样本挖掘，使图像、其增强版本和扰动版本在嵌入空间中与同类图像聚类，同时与其他类别图像分离，从而专注于稳定模式而非脆弱梯度线索。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10数据集上，ANCHOR在干净和PGD-20攻击下的鲁棒准确性均优于标准对抗训练方法，表明结合对抗性指导与硬挖掘的对比监督有助于模型学习更结构化和鲁棒性的表示。&lt;h4&gt;结论&lt;/h4&gt;结合对抗性指导和硬挖掘的对比监督可以有效缩小模型准确性和鲁棒性之间的差距，使神经网络能够更好地抵抗对抗性攻击。&lt;h4&gt;翻译&lt;/h4&gt;神经网络改变了机器解释世界的方式。从根本上说，它们通过遵循梯度学习，逐步调整参数，直到识别出数据中最具判别性的模式。这一过程赋予了它们力量，但也打开了一个隐藏缺陷的大门。正是这些帮助模型学习的梯度，也可以用来产生微小、不可察觉的调整，导致模型完全改变其决策。这种调整被称为对抗性攻击。这些攻击通过向图像添加微小、不可察觉的变化来利用这一漏洞，这些变化虽然对人类眼睛来说是相同的，但会导致模型做出错误预测。在这项工作中，我们提出了对抗训练的对比性硬挖掘用于优化鲁棒性（ANCHOR）框架，该框架利用监督对比学习的力量，结合显式的硬正样本挖掘，使模型能够学习图像的表示，使图像、其增强版本和扰动版本在嵌入空间中与同一类别的其他图像聚类在一起，同时与其他类别的图像分离。这种对齐帮助模型专注于稳定、有意义的模式，而不是脆弱的梯度线索。在CIFAR-10上，我们的方法在干净和鲁棒准确性方面都取得了令人印象深刻的结果，在PGD-20（epsilon = 0.031）攻击下优于标准的对抗训练方法。我们的结果表明，将对抗性指导与硬挖掘的对比监督相结合，有助于模型学习更有结构和鲁棒性的表示，缩小了准确性和鲁棒性之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural networks have changed the way machines interpret the world. At theircore, they learn by following gradients, adjusting their parameters step bystep until they identify the most discriminant patterns in the data. Thisprocess gives them their strength, yet it also opens the door to a hidden flaw.The very gradients that help a model learn can also be used to produce small,imperceptible tweaks that cause the model to completely alter its decision.Such tweaks are called adversarial attacks. These attacks exploit thisvulnerability by adding tiny, imperceptible changes to images that, whileleaving them identical to the human eye, cause the model to make wrongpredictions. In this work, we propose Adversarially-trained ContrastiveHard-mining for Optimized Robustness (ANCHOR), a framework that leverages thepower of supervised contrastive learning with explicit hard positive mining toenable the model to learn representations for images such that the embeddingsfor the images, their augmentations, and their perturbed versions clustertogether in the embedding space along with those for other images of the sameclass while being separated from images of other classes. This alignment helpsthe model focus on stable, meaningful patterns rather than fragile gradientcues. On CIFAR-10, our approach achieves impressive results for both clean androbust accuracy under PGD-20 (epsilon = 0.031), outperforming standardadversarial training methods. Our results indicate that combining adversarialguidance with hard-mined contrastive supervision helps models learn morestructured and robust representations, narrowing the gap between accuracy androbustness.</description>
      <author>example@mail.com (Samarup Bhattacharya, Anubhab Bhattacharya, Abir Chakraborty)</author>
      <guid isPermaLink="false">2510.27599v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>C-LEAD: Contrastive Learning for Enhanced Adversarial Defense</title>
      <link>http://arxiv.org/abs/2510.27249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用对比学习进行对抗防御的新方法，通过同时优化模型参数和扰动，使网络学习鲁棒表示，实验结果表明该方法显著提高了模型对抗各种对抗性扰动的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络在计算机视觉任务中取得了显著成功，但它们容易受到对抗性攻击，这种攻击只需对输入图像进行微小扰动就能导致错误预测。&lt;h4&gt;目的&lt;/h4&gt;解决对抗性攻击问题，以便部署稳健的深度学习系统。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的方法，利用对比学习进行对抗防御。该方法利用对比损失函数，通过同时使用干净和对抗性扰动的图像来增强分类模型的鲁棒性。通过同时优化模型参数和扰动，使网络学习对对抗攻击不太敏感的鲁棒表示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，模型对各种类型的对抗性扰动有显著改进。这表明对比损失有助于提取更具信息性和弹性的特征，有助于深度学习的对抗鲁棒性领域。&lt;h4&gt;结论&lt;/h4&gt;对比学习在对抗防御方面是一个有前景的方向，能够提高模型对对抗攻击的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;深度神经网络在图像分类、分割和目标检测等计算机视觉任务中取得了显著成功。然而，它们容易受到对抗性攻击，这种攻击只需对输入图像进行微小扰动就能导致错误预测。解决这个问题对于部署稳健的深度学习系统至关重要。本文提出了一种新颖的方法，利用对比学习进行对抗防御，这是一个先前未被探索的领域。我们的方法利用对比损失函数，通过同时使用干净和对抗性扰动的图像来增强分类模型的鲁棒性。通过同时优化模型参数和扰动，我们的方法使网络学习对对抗攻击不太敏感的鲁棒表示。实验结果表明，模型对各种类型的对抗性扰动的鲁棒性有显著提高。这表明对比损失有助于提取更具信息性和弹性的特征，为深度学习的对抗鲁棒性领域做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks (DNNs) have achieved remarkable success in computervision tasks such as image classification, segmentation, and object detection.However, they are vulnerable to adversarial attacks, which can cause incorrectpredictions with small perturbations in input images. Addressing this issue iscrucial for deploying robust deep-learning systems. This paper presents a novelapproach that utilizes contrastive learning for adversarial defense, apreviously unexplored area. Our method leverages the contrastive loss functionto enhance the robustness of classification models by training them with bothclean and adversarially perturbed images. By optimizing the model's parametersalongside the perturbations, our approach enables the network to learn robustrepresentations that are less susceptible to adversarial attacks. Experimentalresults show significant improvements in the model's robustness against varioustypes of adversarial perturbations. This suggests that contrastive loss helpsextract more informative and resilient features, contributing to the field ofadversarial robustness in deep learning.</description>
      <author>example@mail.com (Suklav Ghosh, Sonal Kumar, Arijit Sur)</author>
      <guid isPermaLink="false">2510.27249v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。&lt;h4&gt;背景&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型用于低级3D重建，并将高级空间理解孤立处理，忽视了这两个方面的关键互动，导致泛化能力有限和下游任务表现不佳。最近的尝试通过简单对齐3D模型与特定语言模型来缓解此问题，但限制了感知能力和下游任务适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一空间重建和实例级上下文理解知识的端到端大型统一transformer模型。&lt;h4&gt;方法&lt;/h4&gt;提出Instance Grounded Geometry Transformer (IGGT)，设计3D一致的对比学习策略，指导模型通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示，支持将2D输入提升为具有明确不同对象实例的连贯3D场景。同时构建InsScene-15K数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解，可以改善3D场景分析的性能；仅通过2D视觉输入就能实现有效的3D重建和实例理解。&lt;h4&gt;结论&lt;/h4&gt;IGGT模型能够有效统一几何结构和语义理解，通过3D一致的对比学习策略，仅从2D视觉输入就能生成具有明确对象实例的连贯3D场景。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型用于低级3D重建，并将高级空间理解孤立处理，忽视了这两个3D场景分析基本方面之间的关键互动，从而限制了泛化能力，导致在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解此问题，从而限制了感知能力并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致的对比学习策略，指导IGGT通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示。这种表示支持将2D视觉输入一致地提升到具有明确不同对象实例的连贯3D场景。为促进此任务，我们进一步构建了InsScene-15K，一个通过新颖数据整理流程构建的大规模数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D场景重建中几何结构和语义理解的分离问题。人类自然将几何结构和语义内容交织理解，而现有方法将这两者孤立处理，导致模型泛化能力差，在下游3D理解任务中表现不佳。这个问题在机器人操作、AR/VR和空间规划等应用中至关重要，这些应用需要同时理解场景的精确几何结构和丰富语义内容。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：分离处理几何重建和语义理解，或简单将3D模型与特定语言模型对齐，限制了模型适应性和性能。作者认为应通过联合训练将几何结构和实例级语义耦合，让模型自主学习两者关系。设计上借鉴了VGGT的统一Transformer架构，使用DINOv2提取特征，采用DPT-like架构进行密集预测，并利用SAM2进行数据标注。核心创新在于设计了3D一致的对比学习策略，确保跨视图的实例一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何结构和实例级语义耦合，实现相互提升，并使用实例掩码作为桥接连接统一表示与各种视觉语言模型。整体流程：1) 构建InsScene-15K数据集；2) 使用大型统一变换器编码多视图图像为统一场景表示；3) 通过几何头部和实例头部分别预测几何结构和实例特征；4) 应用跨模态融合块增强实例特征的细粒度空间感知；5) 使用3D一致的对比学习策略训练模型；6) 通过实例掩码桥接策略支持下游任务如实例跟踪、开放词汇分割和QA场景定位。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 实例级几何-语义统一表示，通过联合训练实现相互提升；2) 3D一致的对比学习策略，确保跨视图实例一致性；3) 实例掩码桥接策略，实现与各种VLMs和LMMs的即插即用集成；4) 构建InsScene-15K大规模高质量数据集。相比之前工作：不同于分离处理几何和语义的方法，IGGT实现两者统一；不同于简单对齐特定语言模型的方法，IGT通过联合训练自主学习关系；不同于仅支持类别级特征的方法，IGT能区分同一类别中的不同实例；不绑定特定VLM，而是通过实例掩码灵活集成各种模型。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过统一几何重建和实例级语义理解，并引入实例掩码桥接策略，实现了高质量的语义3D重建和灵活的下游任务支持，显著提升了3D场景理解的性能和泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v3</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-SSRL: Enhancing Spatial Understanding via Self-Supervised Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.27606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Spatial-SSRL是一种自监督强化学习范式，通过从普通RGB或RGB-D图像中提取可验证信号，设计了五个捕捉2D和3D空间结构的预训练任务，显著提升了大型视觉语言模型的空间理解能力，无需昂贵的监督或专业工具。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型(LVLMs)在空间理解方面存在弱点，现有的监督微调(SFT)和可验证奖励强化学习(RLVR)方法依赖昂贵的监督、专业工具或受限环境，限制了规模扩展。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需昂贵监督和专业工具的自强化学习范式，提升LVLMs的空间理解能力，同时保持通用视觉能力。&lt;h4&gt;方法&lt;/h4&gt;提出Spatial-SSRL，一种自监督RL范式，自动设计五个预训练任务：打乱块重排序、翻转块识别、裁剪块修复、区域深度排序和相对3D位置预测，这些任务提供易于验证的真实答案，无需人工或LVLM注释。&lt;h4&gt;主要发现&lt;/h4&gt;在七个空间理解基准测试中，Spatial-SSRL相比Qwen2.5-VL基线实现了显著提升：3B模型平均准确率提高4.63%，7B模型提高3.89%，同时保留了通用视觉能力。&lt;h4&gt;结论&lt;/h4&gt;简单、内在的监督使大规模RLVR成为可能，为LVLMs提供更强的空间智能的实用途径，无需依赖昂贵的监督或专业工具。&lt;h4&gt;翻译&lt;/h4&gt;空间理解仍然是大型视觉语言模型(LVLMs)的弱点。现有的监督微调(SFT)和最近的可验证奖励强化学习(RLVR)流程依赖于昂贵的监督、专业工具或受限环境，限制了规模扩展。我们引入了Spatial-SSRL，一种自监督RL范式，直接从普通RGB或RGB-D图像中派生可验证信号。Spatial-SSRL自动设计了五个捕捉2D和3D空间结构的预训练任务：打乱块重排序、翻转块识别、裁剪块修复、区域深度排序和相对3D位置预测。这些任务提供易于验证的真实答案，不需要人工或LVLM注释。在我们的任务上训练显著提高了空间推理能力，同时保留了通用视觉能力。在图像和视频设置下的七个空间理解基准测试中，Spatial-SSRL相比Qwen2.5-VL基线实现了平均准确率提升：3B模型提升4.63%，7B模型提升3.89%。我们的结果表明，简单、内在的监督使大规模RLVR成为可能，并为LVLMs提供更强的空间智能的实用途径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决大型视觉语言模型（LVLMs）在空间理解方面的不足问题。空间理解对LVLMs分析复杂真实世界场景至关重要，能够推理深度、距离、方位和相对物体位置，实现3D环境重建，并支持自动驾驶、机器人操作和具身导航等应用。尽管LVLMs在其他任务上表现优异，但其空间理解能力仍远低于人类水平，限制了它们在现实世界中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从现有方法的局限性出发，认识到监督微调（SFT）和可验证奖励强化学习（RLVR）依赖昂贵监督和专业工具的问题。他们借鉴了视觉自监督学习（SSL）的理念，认为普通RGB或RGB-D图像中固有的内在一致性信号可以自然地监督空间理解。作者设计了自监督任务作为可验证奖励函数，并使用组相对策略优化（GRPO）进行强化学习训练。这种方法结合了SSL的无监督特性和RLVR的优化优势，但创新性地将其应用于空间理解任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用图像内在结构作为自监督信号，生成可验证的奖励，通过强化学习优化LVLM的空间理解能力，无需人工标注或专业工具。整体流程包括：1) 设计五类自监督任务（三类无深度任务：打乱块重排、翻转块识别、裁剪块修复；两类基于深度任务：区域深度排序、相对3D位置预测）；2) 从COCO等数据集收集原始图像，自动构建Spatial-SSRL-81k数据集；3) 采用SFT冷启动后，使用GRPO进行强化学习训练，结合准确度奖励和格式奖励优化模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 提出Spatial-SSRL自监督强化学习新范式；2) 设计覆盖2D和3D空间结构的五类互补pretext任务；3) 构建完全自动生成的Spatial-SSRL-81k数据集；4) 结合SFT冷启动和GRPO优化。相比之前工作，这种方法不依赖昂贵标注或专业工具，避免了SFT的过拟合问题和RLVR的环境限制，同时将SSL从表示学习转移到行为优化，实现了更好的泛化能力和可扩展性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Spatial-SSRL通过图像内在结构自监督和强化学习，显著提升了LVLM的空间理解能力，同时保持通用视觉能力，且成本更低、可扩展性更强。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial understanding remains a weakness of Large Vision-Language Models(LVLMs). Existing supervised fine-tuning (SFT) and recent reinforcementlearning with verifiable rewards (RLVR) pipelines depend on costly supervision,specialized tools, or constrained environments that limit scale. We introduceSpatial-SSRL, a self-supervised RL paradigm that derives verifiable signalsdirectly from ordinary RGB or RGB-D images. Spatial-SSRL automaticallyformulates five pretext tasks that capture 2D and 3D spatial structure:shuffled patch reordering, flipped patch recognition, cropped patch inpainting,regional depth ordering, and relative 3D position prediction. These tasksprovide ground-truth answers that are easy to verify and require no human orLVLM annotation. Training on our tasks substantially improves spatial reasoningwhile preserving general visual capabilities. On seven spatial understandingbenchmarks in both image and video settings, Spatial-SSRL delivers averageaccuracy gains of 4.63% (3B) and 3.89% (7B) over the Qwen2.5-VL baselines. Ourresults show that simple, intrinsic supervision enables RLVR at scale andprovides a practical route to stronger spatial intelligence in LVLMs.</description>
      <author>example@mail.com (Yuhong Liu, Beichen Zhang, Yuhang Zang, Yuhang Cao, Long Xing, Xiaoyi Dong, Haodong Duan, Dahua Lin, Jiaqi Wang)</author>
      <guid isPermaLink="false">2510.27606v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>NAUTILUS: A Large Multimodal Model for Underwater Scene Understanding</title>
      <link>http://arxiv.org/abs/2510.27481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Data and models are available at  https://github.com/H-EmbodVis/NAUTILUS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究构建了NautData数据集并提出视觉特征增强模块，开发了名为NAUTILUS的水下大语言模型，有效提高了水下场景理解能力。&lt;h4&gt;背景&lt;/h4&gt;水下探索对了解地球和资源勘探、国家安全等应用至关重要，但水下场景理解需要多任务感知能力，而目前缺乏大规模水下多任务指令调整数据集。&lt;h4&gt;目的&lt;/h4&gt;构建支持八种水下场景理解任务的大规模数据集，并开发能够提高水下场景理解鲁棒性的方法，解决水下图像退化挑战。&lt;h4&gt;方法&lt;/h4&gt;构建包含145万张图像-文本对的NautData数据集；引入基于水下成像模型的物理先验，提出即插即用的视觉特征增强模块；将模块集成到LLaVA-1.5和Qwen2.5-VL基线模型中，构建NAUTILUS模型。&lt;h4&gt;主要发现&lt;/h4&gt;VFE模块在NautData和公共水下数据集上实验证明有效，能够持续提高基线模型在大多数支持任务上的性能，确保了NAUTILUS在水下场景理解领域的优越性。&lt;h4&gt;结论&lt;/h4&gt;通过大规模数据集构建和视觉特征增强模块提出，成功提高了水下场景理解性能，NAUTILUS模型在水下场景理解任务中表现优越。&lt;h4&gt;翻译&lt;/h4&gt;水下探索为我们提供了了解地球的关键见解，并在资源勘探、国家安全等方面吸引越来越多的关注。我们研究水下场景理解方法，旨在实现水下探索的自动化。水下场景理解任务需要从多个粒度进行多任务感知。然而，缺乏大规模水下多任务指令调整数据集阻碍了这项研究的进展。为了弥补这一差距，我们构建了NautData，这是一个包含145万张图像-文本对的数据集，支持八种水下场景理解任务。它使水下场景理解模型的发展和全面评估成为可能。水下图像退化是一个广泛认可的问题，它干扰水下任务。为了提高水下场景理解的鲁棒性，我们引入了基于水下成像模型的物理先验，并提出了一种即插即用的视觉特征增强模块，该模块明确恢复了清晰的水下信息。我们将此模块集成到著名的基线模型LLaVA-1.5和Qwen2.5-VL中，构建了我们的水下大语言模型NAUTILUS。在NautData和公共水下数据集上进行的实验证明了VFE模块的有效性，持续提高了基线模型在大多数支持任务上的性能，从而确保了NAUTILUS在水下场景理解领域的优越性。数据和模型可在https://github.com/H-EmbodVis/NAUTILUS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater exploration offers critical insights into our planet and attractsincreasing attention for its broader applications in resource exploration,national security, etc. We study the underwater scene understanding methods,which aim to achieve automated underwater exploration. The underwater sceneunderstanding task demands multi-task perceptions from multiple granularities.However, the absence of large-scale underwater multi-task instruction-tuningdatasets hinders the progress of this research. To bridge this gap, weconstruct NautData, a dataset containing 1.45 M image-text pairs supportingeight underwater scene understanding tasks. It enables the development andthorough evaluation of the underwater scene understanding models. Underwaterimage degradation is a widely recognized challenge that interferes withunderwater tasks. To improve the robustness of underwater scene understanding,we introduce physical priors derived from underwater imaging models and proposea plug-and-play vision feature enhancement (VFE) module, which explicitlyrestores clear underwater information. We integrate this module into renownedbaselines LLaVA-1.5 and Qwen2.5-VL and build our underwater LMM, NAUTILUS.Experiments conducted on the NautData and public underwater datasetsdemonstrate the effectiveness of the VFE module, consistently improving theperformance of both baselines on the majority of supported tasks, thus ensuringthe superiority of NAUTILUS in the underwater scene understanding area. Dataand models are available at https://github.com/H-EmbodVis/NAUTILUS.</description>
      <author>example@mail.com (Wei Xu, Cheng Wang, Dingkang Liang, Zongchuang Zhao, Xingyu Jiang, Peng Zhang, Xiang Bai)</author>
      <guid isPermaLink="false">2510.27481v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>GeoFM: Enhancing Geometric Reasoning of MLLMs via Synthetic Data Generation through Formal Language</title>
      <link>http://arxiv.org/abs/2510.27448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为GeoFM的新方法，用于合成高质量的几何数据，解决了多模态大语言模型在几何推理中的挑战。通过形式化语言探索度量空间内条件的组合，GeoFM能够生成多样化且正确的几何问题，实验证明其显著提升了模型在几何任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型在学术界和工业界因其处理多模态任务的能力而受到广泛关注。然而，由于高质量几何数据的稀缺，这些模型在数学几何推理方面面临挑战。&lt;h4&gt;目的&lt;/h4&gt;解决多模态大语言模型在数学几何推理中的挑战，通过开发一种新的合成几何数据方法来提升模型性能。&lt;h4&gt;方法&lt;/h4&gt;提出GeoFM方法，使用形式化语言探索度量空间内条件的组合，生成高保真度的几何问题，并通过符号引擎确保正确性。&lt;h4&gt;主要发现&lt;/h4&gt;使用GeoFM合成数据训练的模型在MathVista几何问题解决任务上超越专有GPT-4o模型18.7%，在GeoQA上超越16.5%；在MathVista上超越领先开源模型5.7%，在GeoQA上超越2.7%。&lt;h4&gt;结论&lt;/h4&gt;GeoFM方法能够生成高质量、多样化的几何数据，有效提升了模型在几何推理任务上的性能，显著优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型在学术界和工业界因其处理多模态任务的能力而受到广泛关注。然而，由于高质量几何数据的稀缺，这些模型在数学几何推理方面面临挑战。为了解决这个问题，合成几何数据已成为一种必要策略。当前生成合成几何数据的方法包括重新表述或扩展现有问题，以及使用预定义规则和模板创建几何图像和问题。然而，这些方法往往产生的数据多样性不足或容易引入噪声。此外，现有方法合成的几何图像变化有限，与真实几何图差异显著。为了克服这些限制，我们提出了GeoFM，一种新的合成几何数据方法。GeoFM使用形式化语言探索度量空间内条件的组合，生成与原始问题不同但保持高保真度的几何问题，并通过符号引擎确保正确性。实验结果表明，我们的合成数据显著优于现有方法。使用我们数据训练的模型在MathVista的几何问题解决任务上超越专有GPT-4o模型18.7%，在GeoQA上超越16.5%。此外，它在MathVista上超越领先开源模型5.7%，在GeoQA上超越2.7%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal Large Language Models (MLLMs) have gained significant attentionin both academia and industry for their capabilities in handling multi-modaltasks. However, these models face challenges in mathematical geometricreasoning due to the scarcity of high-quality geometric data. To address thisissue, synthetic geometric data has become an essential strategy. Currentmethods for generating synthetic geometric data involve rephrasing or expandingexisting problems and utilizing predefined rules and templates to creategeometric images and problems. However, these approaches often produce datathat lacks diversity or is prone to noise. Additionally, the geometric imagessynthesized by existing methods tend to exhibit limited variation and deviatesignificantly from authentic geometric diagrams. To overcome these limitations,we propose GeoFM, a novel method for synthesizing geometric data. GeoFM usesformal languages to explore combinations of conditions within metric space,generating high-fidelity geometric problems that differ from the originalswhile ensuring correctness through a symbolic engine. Experimental results showthat our synthetic data significantly outperforms existing methods. The modeltrained with our data surpass the proprietary GPT-4o model by 18.7\% ongeometry problem-solving tasks in MathVista and by 16.5\% on GeoQA.Additionally, it exceeds the performance of a leading open-source model by5.7\% on MathVista and by 2.7\% on GeoQA.</description>
      <author>example@mail.com (Yuhao Zhang, Dingxin Hu, Tinghao Yu, Hao Liu, Yiting Liu)</author>
      <guid isPermaLink="false">2510.27448v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning Sparse Approximate Inverse Preconditioners for Conjugate Gradient Solvers on GPUs</title>
      <link>http://arxiv.org/abs/2510.27517v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, poster&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的稀疏近似逆(SPAI)预处理器方法，用于解决共轭梯度求解器在GPU上的并行化和长距离依赖问题，显著提高了求解速度和性能。&lt;h4&gt;背景&lt;/h4&gt;共轭梯度(CG)求解器是求解对称正定线性系统Ax=b的常用方法，有效预处理器对快速收敛至关重要。传统预处理器依赖预设算法提供理论保证但限制数据优化能力，现有基于学习的方法利用GNN提高性能，但依赖不完全分解导致GPU并行化困难和长距离依赖建模挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种GPU友好的学习型预处理器，特别是使用GNN构建稀疏近似逆(SPAI)预处理器，避免三角求解并减少每个CG步骤的计算量。&lt;h4&gt;方法&lt;/h4&gt;使用GNN构建稀疏近似逆(SPAI)预处理器，每个CG步骤只需两次矩阵-向量乘积；利用矩阵-向量乘积的局部性与GNN局部传播机制的兼容性；引入基于统计的尺度不变损失函数，匹配CG收敛率取决于条件数而非绝对尺度的特性。&lt;h4&gt;主要发现&lt;/h4&gt;在三个PDE导出数据集和一个合成数据集上评估，该方法在GPU上优于标准预处理器(对角线、IC和传统SPAI)及之前的基于学习预处理器；减少40%-53%求解时间(快68%-113%)，具有更好条件数和泛化性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的GNN构建的SPAI预处理器成功解决了现有方法在GPU并行化和长距离依赖建模方面的挑战，在GPU上实现了更快的求解时间和更好的性能，适用于广泛的应用场景。&lt;h4&gt;翻译&lt;/h4&gt;共轭梯度求解器(CG)是求解对称正定线性系统Ax=b的常用方法，其中有效的预处理器对快速收敛至关重要。传统预处理器依赖于预设算法来提供严格的理论保证，同时限制了利用数据优化的能力。现有的基于学习的方法通常利用图神经网络(GNN)来提高性能和加速构建过程。然而，它们对不完全分解的依赖导致了重大挑战：相关的三角求解在实践中阻碍了GPU并行化，并引入了难以被GNN建模的长距离依赖关系。为解决这些问题，我们提出了一种基于学习的方法来生成GPU友好的预处理器，特别是使用GNN构建稀疏近似逆(SPAI)预处理器，避免了三角求解，每个CG步骤只需要两次矩阵-向量乘积。矩阵-向量乘积的局部性与GNN的局部传播机制兼容。GNN的灵活性也使我们的方法可以应用于广泛场景。此外，我们引入了一种基于统计的尺度不变损失函数，其设计匹配了CG的性质——收敛率取决于条件数，而不是A的绝对尺度，从而提高了学习到的预处理器的性能。在三个从PDE导出的数据集和一个合成数据集上的评估表明，我们的方法在GPU上优于标准预处理器(对角线、IC和传统SPAI)以及之前的基于学习的预处理器。我们在GPU上减少了40%-53%的求解时间(快68%-113%)，同时具有更好的条件数和优异的泛化性能。源代码可在https://github.com/Adversarr/LearningSparsePreconditioner4GPU获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The conjugate gradient solver (CG) is a prevalent method for solvingsymmetric and positive definite linear systems Ax=b, where effectivepreconditioners are crucial for fast convergence. Traditional preconditionersrely on prescribed algorithms to offer rigorous theoretical guarantees, whilelimiting their ability to exploit optimization from data. Existinglearning-based methods often utilize Graph Neural Networks (GNNs) to improvethe performance and speed up the construction. However, their reliance onincomplete factorization leads to significant challenges: the associatedtriangular solve hinders GPU parallelization in practice, and introduceslong-range dependencies which are difficult for GNNs to model. To address theseissues, we propose a learning-based method to generate GPU-friendlypreconditioners, particularly using GNNs to construct Sparse ApproximateInverse (SPAI) preconditioners, which avoids triangular solves and requiresonly two matrix-vector products at each CG step. The locality of matrix-vectorproduct is compatible with the local propagation mechanism of GNNs. Theflexibility of GNNs also allows our approach to be applied in a wide range ofscenarios. Furthermore, we introduce a statistics-based scale-invariant lossfunction. Its design matches CG's property that the convergence rate depends onthe condition number, rather than the absolute scale of A, leading to improvedperformance of the learned preconditioner. Evaluations on three PDE-deriveddatasets and one synthetic dataset demonstrate that our method outperformsstandard preconditioners (Diagonal, IC, and traditional SPAI) and previouslearning-based preconditioners on GPUs. We reduce solution time on GPUs by40%-53% (68%-113% faster), along with better condition numbers and superiorgeneralization performance. Source code available athttps://github.com/Adversarr/LearningSparsePreconditioner4GPU</description>
      <author>example@mail.com (Zherui Yang, Zhehao Li, Kangbo Lyu, Yixuan Li, Tao Du, Ligang Liu)</author>
      <guid isPermaLink="false">2510.27517v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Neural Graph Sparsification</title>
      <link>http://arxiv.org/abs/2510.27474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的图表示学习框架——谱保持网络，通过生成简化的图作为原始图的忠实代理，使下游任务能够在降低计算成本的情况下进行。&lt;h4&gt;背景&lt;/h4&gt;图是建模复杂系统的核心工具，应用于社交网络、分子化学和神经科学等领域。图神经网络已成为图学习的标准工具，但仍受限于固定结构的依赖和过平滑问题。&lt;h4&gt;目的&lt;/h4&gt;提出谱保持网络框架，生成简化的图作为原始图的忠实代理，使社区检测、影响传播和信息扩散等下游任务能够在降低计算成本的情况下进行。&lt;h4&gt;方法&lt;/h4&gt;谱保持网络引入两个关键组件：联合图进化层，同时变换图拓扑和节点特征矩阵，使结构和属性在层间自适应演化；谱一致性损失，通过强制保持图的谱特性和节点特征向量的一致性来正则化这些变换。&lt;h4&gt;主要发现&lt;/h4&gt;通过节点级稀化分析评估了谱保持网络的有效性，使用成熟指标并与最先进方法进行基准测试，实验结果表明该方法具有优越的性能和明显优势。&lt;h4&gt;结论&lt;/h4&gt;谱保持网络是一种有效的图表示学习方法，能够生成简化的图同时保持原始图的关键特性，在降低计算成本的同时，在下游任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图是建模复杂系统的核心工具，应用于社交网络、分子化学和神经科学等领域。虽然图神经网络，特别是图卷积网络已成为图学习的标准工具，但它们仍然受限于对固定结构的依赖和过平滑问题。我们提出了谱保持网络，这是一种用于图表示学习的新框架，它生成简化的图作为原始图的忠实代理，使社区检测、影响传播和信息扩散等下游任务能够以降低的计算成本进行。谱保持网络引入了两个关键组件：联合图进化层和谱一致性损失。前者同时变换图拓扑和节点特征矩阵，使结构和属性在层间自适应演化，克服了静态邻域聚合的刚性。后者通过强制保持图的谱特性和节点特征向量的一致性来正则化这些变换。我们通过分析成熟的指标和与最先进方法进行基准测试，评估了谱保持网络在节点级稀化方面的有效性。实验结果表明我们的方法具有优越的性能和明显的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are central to modeling complex systems in domains such as socialnetworks, molecular chemistry, and neuroscience. While Graph Neural Networks,particularly Graph Convolutional Networks, have become standard tools for graphlearning, they remain constrained by reliance on fixed structures andsusceptibility to over-smoothing. We propose the Spectral Preservation Network,a new framework for graph representation learning that generates reduced graphsserving as faithful proxies of the original, enabling downstream tasks such ascommunity detection, influence propagation, and information diffusion at areduced computational cost. The Spectral Preservation Network introduces twokey components: the Joint Graph Evolution layer and the Spectral Concordanceloss. The former jointly transforms both the graph topology and the nodefeature matrix, allowing the structure and attributes to evolve adaptivelyacross layers and overcoming the rigidity of static neighborhood aggregation.The latter regularizes these transformations by enforcing consistency in boththe spectral properties of the graph and the feature vectors of the nodes. Weevaluate the effectiveness of Spectral Preservation Network on node-levelsparsification by analyzing well-established metrics and benchmarking againststate-of-the-art methods. The experimental results demonstrate the superiorperformance and clear advantages of our approach.</description>
      <author>example@mail.com (Angelica Liguori, Ettore Ritacco, Pietro Sabatino, Annalisa Socievole)</author>
      <guid isPermaLink="false">2510.27474v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Modal Feature Fusion for Spatial Morphology Analysis of Traditional Villages via Hierarchical Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.27208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种分层图神经网络模型，整合多源数据对村庄空间形态进行分析，解决了现有研究中的局限性，在多模态融合和分类任务上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;村庄区域在人地关系研究中具有重要性，但随着城市化发展，村庄空间特征逐渐消失，景观同质化问题突出。现有研究主要采用单一学科视角，过度依赖定性分析方法，且受限于数字基础设施不足和数据缺乏。&lt;h4&gt;目的&lt;/h4&gt;为解决当前研究局限性，提出一种分层图神经网络模型，整合多源数据对村庄空间形态进行深入分析。&lt;h4&gt;方法&lt;/h4&gt;提出包含两种节点（输入节点和通信节点）和两种边（静态输入边和动态通信边）的分层图神经网络模型。通过结合图卷积网络和图注意力网络，在两阶段特征更新机制下融合多模态特征，并引入关系池化机制，对17个子类型实施联合训练策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多模态融合和分类任务上比现有方法取得显著性能提升。所有子类型的联合优化使平均准确率/F1值从独立模型的0.71/0.83提升到0.82/0.90，其中地块任务带来了6%的提升。&lt;h4&gt;结论&lt;/h4&gt;该方法为探索村庄空间格局和生成逻辑提供了科学依据。&lt;h4&gt;翻译&lt;/h4&gt;村庄区域在研究人地关系中具有重要性。然而，随着城市化的推进，空间特征的逐渐消失和景观的同质化已成为突出问题。现有研究主要采用单一学科视角分析村庄空间形态及其影响因素，过度依赖定性分析方法。这些研究常受限于数字基础设施不足和数据缺乏。为解决当前研究局限性，本文提出了一种整合多源数据的分层图神经网络模型，对村庄空间形态进行深入分析。该框架包含两种节点类型（输入节点和通信节点）和两种边类型（静态输入边和动态通信边）。通过结合图卷积网络和图注意力网络，所提出的模型在两阶段特征更新机制下高效融合多模态特征。此外，基于现有的村庄空间形态分类原则，本文引入了关系池化机制，并对17个子类型实施了联合训练策略。实验结果表明，该方法在多模态融合和分类任务上比现有方法取得了显著的性能提升。此外，所有子类型的联合优化使平均准确率/F1值从独立模型的0.71/0.83提升到0.82/0.90，其中地块任务带来了6%的提升。我们的方法为探索村庄空间格局和生成逻辑提供了科学证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Villages areas hold significant importance in the study of human-landrelationships. However, with the advancement of urbanization, the gradualdisappearance of spatial characteristics and the homogenization of landscapeshave emerged as prominent issues. Existing studies primarily adopt asingle-disciplinary perspective to analyze villages spatial morphology and itsinfluencing factors, relying heavily on qualitative analysis methods. Theseefforts are often constrained by the lack of digital infrastructure andinsufficient data. To address the current research limitations, this paperproposes a Hierarchical Graph Neural Network (HGNN) model that integratesmulti-source data to conduct an in-depth analysis of villages spatialmorphology. The framework includes two types of nodes-input nodes andcommunication nodes-and two types of edges-static input edges and dynamiccommunication edges. By combining Graph Convolutional Networks (GCN) and GraphAttention Networks (GAT), the proposed model efficiently integrates multimodalfeatures under a two-stage feature update mechanism. Additionally, based onexisting principles for classifying villages spatial morphology, the paperintroduces a relational pooling mechanism and implements a joint trainingstrategy across 17 subtypes. Experimental results demonstrate that this methodachieves significant performance improvements over existing approaches inmultimodal fusion and classification tasks. Additionally, the proposed jointoptimization of all sub-types lifts mean accuracy/F1 from 0.71/0.83(independent models) to 0.82/0.90, driven by a 6% gain for parcel tasks. Ourmethod provides scientific evidence for exploring villages spatial patterns andgenerative logic.</description>
      <author>example@mail.com (Jiaxin Zhang, Zehong Zhu, Junye Deng, Yunqin Li, and Bowen Wang)</author>
      <guid isPermaLink="false">2510.27208v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MDAS-GNN: Multi-Dimensional Spatiotemporal GNN with Spatial Diffusion for Urban Traffic Risk Forecasting</title>
      <link>http://arxiv.org/abs/2510.27197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于多维注意力的空间扩散图神经网络(MDAS-GNN)，用于交通事故预测，整合了交通安全、基础设施和环境三个核心风险维度，在多个城市区域的数据集上表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;交通事故是全球严重的公共健康挑战，每年导致超过135万人死亡。传统事故预测模型将道路段独立处理，无法捕捉城市交通网络中的复杂空间关系和时间依赖性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕捉城市交通网络中复杂空间关系和时间依赖性的交通事故预测模型，为交通基础设施设计和城市规划提供数据支持。&lt;h4&gt;方法&lt;/h4&gt;构建了MDAS-GNN模型，整合三个核心风险维度：交通安全、基础设施和环境风险。采用特定特征的空间扩散机制和多头时间注意力来捕捉不同时间跨度的依赖关系，并在英国交通部提供的事故数据上进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;MDAS-GNN相比既定的基线方法表现出优越性能，在短期、中期和长期预测中都保持了较低的预测误差，特别是在长期预测方面表现出色。消融研究表明，集成的多维特征比单特征方法更有效，可将预测误差降低高达40%。&lt;h4&gt;结论&lt;/h4&gt;该框架为土木工程师和城市规划者提供了先进的预测能力，支持交通基础设施设计，使决策者能够基于数据进行路网优化、基础设施资源改进和城市发展项目中的战略安全干预。&lt;h4&gt;翻译&lt;/h4&gt;交通事故代表着一项关键的公共卫生挑战，每年在全球范围内造成超过135万人死亡。传统的事故预测模型将道路段独立处理，无法捕捉城市交通网络中复杂的空间关系和时间依赖性。本研究开发了MDAS-GNN，一种基于多维注意力的空间扩散图神经网络，整合了三个核心风险维度：交通安全、基础设施和环境风险。该框架采用特定特征的空间扩散机制和多头时间注意力来捕捉不同时间跨度的依赖关系。在英国交通部提供的伦敦中部、曼彻斯特南部和伯明翰东南部的事故数据评估中，MDAS-GNN相比既定的基线方法取得了优越的性能。该模型在短期、中期和长期预测中都保持了一致的低预测误差，特别是在长期预测方面具有特别优势。消融研究证实，集成的多维特征优于单特征方法，可将预测误差降低高达40%。该框架为土木工程师和城市规划者提供了先进的预测能力，用于交通基础设施设计，使他们能够为路网优化、基础设施资源改进和城市发展项目中的战略安全干预提供数据驱动的决策。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic accidents represent a critical public health challenge, claiming over1.35 million lives annually worldwide. Traditional accident prediction modelstreat road segments independently, failing to capture complex spatialrelationships and temporal dependencies in urban transportation networks. Thisstudy develops MDAS-GNN, a Multi-Dimensional Attention-based Spatial-diffusionGraph Neural Network integrating three core risk dimensions: traffic safety,infrastructure, and environmental risk. The framework employs feature-specificspatial diffusion mechanisms and multi-head temporal attention to capturedependencies across different time horizons. Evaluated on UK Department forTransport accident data across Central London, South Manchester, and SEBirmingham, MDASGNN achieves superior performance compared to establishedbaseline methods. The model maintains consistently low prediction errors acrossshort, medium, and long-term periods, with particular strength in long-termforecasting. Ablation studies confirm that integrated multi-dimensionalfeatures outperform singlefeature approaches, reducing prediction errors by upto 40%. This framework provides civil engineers and urban planners withadvanced predictive capabilities for transportation infrastructure design,enabling data-driven decisions for road network optimization, infrastructureresource improvements, and strategic safety interventions in urban developmentprojects.</description>
      <author>example@mail.com (Ziyuan Gao)</author>
      <guid isPermaLink="false">2510.27197v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Relation-Aware Bayesian Optimization of DBMS Configurations Guided by Affinity Scores</title>
      <link>http://arxiv.org/abs/2510.27145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了RelTune框架，通过关系图表示参数依赖关系，并引入混合评分引导的贝叶斯优化方法，解决了现有DBMS参数自动调优方法的局限性，在多个DBMS和工作负载上实现了更快的收敛和更高的优化效率。&lt;h4&gt;背景&lt;/h4&gt;数据库管理系统(DBMSs)对于管理大规模异构数据至关重要，其性能受配置参数影响。有效的参数调优对于适应不同工作负载、最大化吞吐量同时最小化延迟至关重要。&lt;h4&gt;目的&lt;/h4&gt;解决现有自动配置优化方法的关键局限性，包括忽略参数间依赖关系、仅选择部分参数进行优化、贝叶斯优化依赖代理模型导致的预测不稳定和探索效率低下等问题。&lt;h4&gt;方法&lt;/h4&gt;提出RelTune框架，将参数依赖关系表示为关系图，学习基于图神经网络(GNN)的潜在嵌入编码性能相关语义；引入混合评分引导的贝叶斯优化(HBO)，结合代理预测与亲和性评分，测量与先前高性能配置的接近度。&lt;h4&gt;主要发现&lt;/h4&gt;在多个DBMS和工作负载上的实验结果表明，RelTune比传统基于贝叶斯优化的方法实现更快的收敛和更高的优化效率，在所有评估场景中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;RelTune框架有效解决了现有DBMS参数自动调优方法的关键局限性，通过考虑参数依赖关系和更全面的参数优化，实现了更优的性能。&lt;h4&gt;翻译&lt;/h4&gt;数据库管理系统(DBMSs)是管理大规模异构数据的基础，其性能受配置参数的影响。有效调整这些参数对于适应不同的工作负载、最大化吞吐量同时最小化延迟至关重要。最近的研究集中在使用机器学习进行自动配置优化；然而，现有方法仍存在几个关键局限性。大多数调优框架忽略了参数之间的依赖关系，假设每个参数独立运作。这种简化限制了优化器利用参数间的关联效应，限制了其捕捉性能敏感交互的能力。此外，为降低高维搜索空间的复杂性，先前工作通常只选择前几个参数进行优化，忽略了其他对性能有重要贡献的参数。作为自动调优最常用的方法，贝叶斯优化(BO)也受限于其对代理模型的依赖，这可能导致预测不稳定和探索效率低下。为克服这些局限性，我们提出了RelTune，一种新框架，它将参数依赖关系表示为关系图，并学习基于图神经网络的潜在嵌入，编码性能相关的语义。RelTune进一步引入了混合评分引导的贝叶斯优化(HBO)，结合代理预测与亲和性评分，测量与先前高性能配置的接近度。在多个DBMS和工作负载上的实验结果表明，RelTune比传统的基于BO的方法实现更快的收敛和更高的优化效率，在所有评估场景中取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Database Management Systems (DBMSs) are fundamental for managing large-scaleand heterogeneous data, and their performance is critically influenced byconfiguration parameters. Effective tuning of these parameters is essential foradapting to diverse workloads and maximizing throughput while minimizinglatency. Recent research has focused on automated configuration optimizationusing machine learning; however, existing approaches still exhibit several keylimitations. Most tuning frameworks disregard the dependencies amongparameters, assuming that each operates independently. This simplificationprevents optimizers from leveraging relational effects across parameters,limiting their capacity to capture performancesensitive interactions. Moreover,to reduce the complexity of the high-dimensional search space, prior work oftenselects only the top few parameters for optimization, overlooking others thatcontribute meaningfully to performance. Bayesian Optimization (BO), the mostcommon method for automatic tuning, is also constrained by its reliance onsurrogate models, which can lead to unstable predictions and inefficientexploration. To overcome these limitations, we propose RelTune, a novelframework that represents parameter dependencies as a Relational Graph andlearns GNN-based latent embeddings that encode performancerelevant semantics.RelTune further introduces Hybrid-Score-Guided Bayesian Optimization (HBO),which combines surrogate predictions with an Affinity Score measuring proximityto previously high-performing configurations. Experimental results on multipleDBMSs and workloads demonstrate that RelTune achieves faster convergence andhigher optimization efficiency than conventional BO-based methods, achievingstate-of-the-art performance across all evaluated scenarios.</description>
      <author>example@mail.com (Sein Kwon, Seulgi Baek, Hyunseo Yang, Youngwan Jo, Sanghyun Park)</author>
      <guid isPermaLink="false">2510.27145v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>A Cloud-Based Spatio-Temporal GNN-Transformer Hybrid Model for Traffic Flow Forecasting with External Feature Integration</title>
      <link>http://arxiv.org/abs/2510.27039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于云的混合模型，结合时空图神经网络和Transformer架构，用于交通流量预测，该模型能有效捕捉复杂时空依赖关系并整合外部特征，在云平台上部署实现了可扩展性和实时适应性。&lt;h4&gt;背景&lt;/h4&gt;准确的交通流量预测对智能交通系统的发展至关重要，支持交通信号优化、拥堵管理和路线规划等任务。传统模型往往无法有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日和交通事故等外部因素的影响下。&lt;h4&gt;目的&lt;/h4&gt;解决传统模型在捕捉大规模路网复杂时空依赖关系方面的不足，特别是在外部因素影响下的预测问题，开发一种能够有效整合空间和时间信息的混合模型，提高交通流量预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于云的混合模型，结合时空图神经网络和Transformer架构。该模型利用GNN在建模路网空间相关性方面的优势以及Transformer捕捉长期时间依赖关系的能力。通过特征融合整合外部上下文特征以提高预测准确性。模型部署在云计算平台上以实现可扩展性和实时适应性。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估显示，该模型在数据集上的表现优于基线方法，RMSE仅为17.92，MAE仅为10.53。这些发现表明混合GNN-Transformer方法为基于云的ITS应用提供了有效且可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;混合GNN-Transformer方法为交通流量预测提供了方法论进步，并为拥堵缓解提供了实际应用价值，是一种有效且可扩展的基于云的ITS应用解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的交通流量预测对智能交通系统的发展至关重要，支持交通信号优化、拥堵管理和路线规划等任务。传统模型往往无法有效捕捉大规模路网中的复杂时空依赖关系，特别是在天气、节假日和交通事故等外部因素的影响下。为应对这一挑战，本文提出了一种基于云的混合模型，结合时空图神经网络和Transformer架构进行交通流量预测。该模型利用了GNN在建模路网空间相关性方面的优势以及Transformer捕捉长期时间依赖关系的能力。通过特征融合整合外部上下文特征以提高预测准确性。所提出的模型部署在云计算平台上以实现可扩展性和实时适应性。对数据集的实验评估显示，我们的模型优于基线方法，RMSE仅为17.92，MAE仅为10.53。这些发现表明，混合GNN-Transformer方法为基于云的ITS应用提供了有效且可扩展的解决方案，为交通流量预测提供了方法论进步，并为拥堵缓解提供了实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate traffic flow forecasting is essential for the development ofintelligent transportation systems (ITS), supporting tasks such as trafficsignal optimization, congestion management, and route planning. Traditionalmodels often fail to effectively capture complex spatial-temporal dependenciesin large-scale road networks, especially under the influence of externalfactors such as weather, holidays, and traffic accidents. To address thischallenge, this paper proposes a cloud-based hybrid model that integratesSpatio-Temporal Graph Neural Networks (ST-GNN) with a Transformer architecturefor traffic flow prediction. The model leverages the strengths of GNNs inmodeling spatial correlations across road networks and the Transformers'ability to capture long-term temporal dependencies. External contextualfeatures are incorporated via feature fusion to enhance predictive accuracy.The proposed model is deployed on a cloud computing platform to achievescalability and real-time adaptability. Experimental evaluation of the datasetshows that our model outperforms baseline methods (LSTM, TCN, GCN, pureTransformer) with an RMSE of only 17.92 and a MAE of only 10.53. These findingssuggest that the hybrid GNN-Transformer approach provides an effective andscalable solution for cloud-based ITS applications, offering methodologicaladvancements for traffic flow forecasting and practical implications forcongestion mitigation.</description>
      <author>example@mail.com (Zhuo Zheng, Lingran Meng, Ziyu Lin)</author>
      <guid isPermaLink="false">2510.27039v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Oral Tradition-Encoded NanyinHGNN: Integrating Nanyin Music Preservation and Generation through a Pipa-Centric Dataset</title>
      <link>http://arxiv.org/abs/2510.26817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NanyinHGNN，一个用于生成南音乐器音乐的异构图网络模型。该模型通过构建以琵琶为中心的MIDI数据集和专门的标记化方法，将装饰音生成为异构图中的节点，结合图神经网络和规则系统生成真实的南音乐曲。&lt;h4&gt;背景&lt;/h4&gt;南音是联合国教科文组织认可的无形文化遗产，它是一种以琵琶为中心的异声传统，核心旋律以传统记谱法记谱，而装饰音则通过口头传承，这对保存和当代创新都提出了挑战。&lt;h4&gt;目的&lt;/h4&gt;解决南音音乐保存和创新中的挑战，特别是处理传统记谱与口头传承装饰音之间的差异。&lt;h4&gt;方法&lt;/h4&gt;构建以琵琶为中心的MIDI数据集，开发NanyinTok作为专门的标记化方法，使用图转换器将符号序列转换为图结构，将装饰音生成重新定义为异构图中装饰音节点的创建，结合图神经网络生成旋律轮廓，并使用基于南音表演实践的规则系统完善装饰音。&lt;h4&gt;主要发现&lt;/h4&gt;该模型成功生成了包含四种传统乐器的真实异声合奏，证明了将领域特定知识整合到模型架构中可以有效缓解民族音乐计算学中的数据稀缺挑战。&lt;h4&gt;结论&lt;/h4&gt;整合领域特定知识到模型架构中可以有效解决民族音乐计算学中的数据稀缺问题，为南音等传统音乐的保存和创新提供了新方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了NanyinHGNN，一个用于生成南音乐器音乐的异构图网络模型。作为联合国教科文组织认可的无形文化遗产，南音遵循一种以琵琶为中心的异声传统，核心旋律以传统记谱法记谱，而装饰音则通过口头传承，这对保存和当代创新都提出了挑战。为解决这一问题，我们构建了一个以琵琶为中心的MIDI数据集，开发了NanyinTok作为专门的标记化方法，并使用图转换器将符号序列转换为图结构，以确保保留关键音乐特征。我们的主要创新是将装饰音生成为异构图中装饰音节点的创建。首先，图神经网络生成针对装饰音优化的旋律轮廓；然后，一个由南音表演实践指导的规则系统将这些轮廓完善为完整的装饰音，而在训练期间不需要明确的装饰音注释。实验结果表明，我们的模型成功生成了包含四种传统乐器的真实异声合奏。这些发现证明将领域特定知识整合到模型架构中可以有效缓解民族音乐计算学中的数据稀缺挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose NanyinHGNN, a heterogeneous graph network model for generatingNanyin instrumental music. As a UNESCO-recognized intangible cultural heritage,Nanyin follows a heterophonic tradition centered around the pipa, where coremelodies are notated in traditional notation while ornamentations are passeddown orally, presenting challenges for both preservation and contemporaryinnovation. To address this, we construct a Pipa-Centric MIDI dataset, developNanyinTok as a specialized tokenization method, and convert symbolic sequencesinto graph structures using a Graph Converter to ensure that key musicalfeatures are preserved. Our key innovation reformulates ornamentationgeneration as the creation of ornamentation nodes within a heterogeneous graph.First, a graph neural network generates melodic outlines optimized forornamentations. Then, a rule-guided system informed by Nanyin performancepractices refines these outlines into complete ornamentations without requiringexplicit ornamentation annotations during training. Experimental resultsdemonstrate that our model successfully generates authentic heterophonicensembles featuring four traditional instruments. These findings validate thatintegrating domain-specific knowledge into model architecture can effectivelymitigate data scarcity challenges in computational ethnomusicology.</description>
      <author>example@mail.com (Jianbing Xiahou, Weixi Zhai, Xu Cui)</author>
      <guid isPermaLink="false">2510.26817v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Effect of Domain Generalization Techniques in Low Resource Systems</title>
      <link>http://arxiv.org/abs/2510.27512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了资源有限情况下自然语言任务中的两种因果领域泛化技术，评估了它们在处理分布偏移问题上的有效性。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型通常假设训练和测试数据遵循相同分布，但现实中的分布偏移常导致这一假设失效。在资源有限的环境中，数据稀缺和领域多样性不足进一步阻碍了模型的稳健泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究两种不同的因果领域泛化技术，提高资源有限自然语言任务中模型对分布偏移的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1) 因果数据增强(CDA)方法：自动生成反事实例子应用于NaijaSenti Twitter语料库的情感分类，通过添加语义等效的释义模拟受控分布偏移；2) 不变因果表征学习(ICRL)方法：使用DINER框架，将其适配到多语言情感分析任务中。&lt;h4&gt;主要发现&lt;/h4&gt;两种方法都提高了对未见领域的鲁棒性：反事实数据增强在情感分类中带来了一致的跨域准确率提升；而使用DINER的因果表征学习在多语言情感分析中改善了分布外性能，尽管不同语言间的提升程度有所不同。&lt;h4&gt;结论&lt;/h4&gt;因果领域泛化技术能有效提高资源有限情况下自然语言处理任务的跨域泛化能力，但不同方法在不同任务和语言中的效果存在差异。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型通常假设训练和测试数据遵循相同分布，这一假设在现实场景中常因分布偏移而失效。这一问题在资源有限的环境中尤为突出，因为数据稀缺和有限的领域多样性阻碍了稳健的泛化。领域泛化(DG)方法通过学习跨领域保持不变的特征来解决这一挑战，通常使用因果机制提高模型的鲁棒性。在本研究中，我们考察了资源有限的自然语言任务中的两种不同因果DG技术。首先，我们研究了一种因果数据增强(CDA)方法，自动生成反事实例子以提高对虚假相关性的鲁棒性。我们将此方法应用于NaijaSenti Twitter语料库上的情感分类，通过添加语义等效的释义来模拟受控的分布偏移。其次，我们探索了使用DINER框架的不变因果表征学习(ICRL)方法，该方法最初用于消除基于方面的情感分析的偏见。我们将DINER适配到多语言环境中。我们的研究结果表明，两种方法都提高了对未见领域的鲁棒性：反事实数据增强在情感分类中带来了一致的跨域准确率提升，而使用DINER的因果表征学习在多语言情感分析中改善了分布外性能，尽管不同语言间的提升程度有所不同。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning models typically assume that training and test data followthe same distribution, an assumption that often fails in real-world scenariosdue to distribution shifts. This issue is especially pronounced in low-resourcesettings, where data scarcity and limited domain diversity hinder robustgeneralization. Domain generalization (DG) approaches address this challenge bylearning features that remain invariant across domains, often using causalmechanisms to improve model robustness. In this study, we examine two distinctcausal DG techniques in low-resource natural language tasks. First, weinvestigate a causal data augmentation (CDA) approach that automaticallygenerates counterfactual examples to improve robustness to spuriouscorrelations. We apply this method to sentiment classification on theNaijaSenti Twitter corpus, expanding the training data with semanticallyequivalent paraphrases to simulate controlled distribution shifts. Second, weexplore an invariant causal representation learning (ICRL) approach using theDINER framework, originally proposed for debiasing aspect-based sentimentanalysis. We adapt DINER to a multilingual setting. Our findings demonstratethat both approaches enhance robustness to unseen domains: counterfactual dataaugmentation yields consistent cross-domain accuracy gains in sentimentclassification, while causal representation learning with DINER improvesout-of-distribution performance in multilingual sentiment analysis, albeit withvarying gains across languages.</description>
      <author>example@mail.com (Mahi Aminu, Chisom Chibuike, Fatimo Adebanjo, Omokolade Awosanya, Samuel Oyeneye)</author>
      <guid isPermaLink="false">2510.27512v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Model-Aware Counterfactual Explanations for Random Forest</title>
      <link>http://arxiv.org/abs/2510.27397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at XAI-FIN-2025: International Joint Workshop on  Explainable AI in Finance: Achieving Trustworthy Financial Decision-Making;  November 15, 2025; Singapore&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于随机森林的反事实解释方法，用于解决机器学习模型在受监管行业中解释性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型尽管具有强大的预测能力，但在受监管行业（如金融）中应用受限，因为它们提供解释的能力有限。现有的模型无关框架（如Shapley值）通常与所寻求的因果解释不一致。&lt;h4&gt;目的&lt;/h4&gt;解决反事实案例搜索和解释的问题，生成更直观、可行且稀疏有用的解释。&lt;h4&gt;方法&lt;/h4&gt;将反事实搜索和解释问题表述为相似性学习问题，利用随机森林预测模型本身学习的表示。找到反事实后，计算解释的特征重要性作为从原始实例到达反事实所穿越的随机森林分区的函数。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在MNIST手写数字数据集和德国信用数据集上生成的解释比Shapley值更稀疏和有用。&lt;h4&gt;结论&lt;/h4&gt;基于随机森林的反事实解释方法能够提供更符合监管行业需求的解释性结果。&lt;h4&gt;翻译&lt;/h4&gt;尽管机器学习模型具有巨大的预测能力，但由于它们提供解释的能力有限，通常不适合在金融等受监管行业应用。虽然像Shapley值这样的模型无关框架已被证明是方便且流行的，但它们很少符合通常所寻求的因果解释类型。反事实案例解释，即告知个人哪些情况需要不同才能导致结果变化，可能更直观和可行。然而，寻找合适的反事实案例是一个开放的挑战，同样解释哪些特征对结果变化最为关键也是如此。在这里，我们将反事实搜索和解释问题表述为相似性学习，利用随机森林预测模型本身学习的表示。一旦找到反事实，解释的特征重要性被计算为从原始实例到达它所穿越的随机森林分区的函数。我们在MNIST手写数字数据集和德国信用数据集上展示了这种方法，发现它生成的解释比Shapley值更稀疏和有用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite their enormous predictive power, machine learning models are oftenunsuitable for applications in regulated industries such as finance, due totheir limited capacity to provide explanations. While model-agnostic frameworkssuch as Shapley values have proved to be convenient and popular, they rarelyalign with the kinds of causal explanations that are typically sought after.Counterfactual case-based explanations, where an individual is informed ofwhich circumstances would need to be different to cause a change in outcome,may be more intuitive and actionable. However, finding appropriatecounterfactual cases is an open challenge, as is interpreting which featuresare most critical for the change in outcome. Here, we pose the question ofcounterfactual search and interpretation in terms of similarity learning,exploiting the representation learned by the random forest predictive modelitself. Once a counterfactual is found, the feature importance of theexplanation is computed as a function of which random forest partitions arecrossed in order to reach it from the original instance. We demonstrate thismethod on both the MNIST hand-drawn digit dataset and the German creditdataset, finding that it generates explanations that are sparser and moreuseful than Shapley values.</description>
      <author>example@mail.com (Joshua S. Harvey, Guanchao Feng, Sai Anusha Meesala, Tina Zhao, Dhagash Mehta)</author>
      <guid isPermaLink="false">2510.27397v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Functional embeddings enable Aggregation of multi-area SEEG recordings over subjects and sessions</title>
      <link>http://arxiv.org/abs/2510.27090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICLR 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种可扩展的表示学习框架，用于解决跨受试者颅内记录数据聚合的挑战。该框架通过学习电极的功能身份和建模区域间关系，实现了受试者无关的神经数据处理，为大规模跨受试者数据分析和预训练提供了新途径。&lt;h4&gt;背景&lt;/h4&gt;跨受试者颅内记录数据聚合面临电极数量、位置和覆盖区域差异大的挑战。传统空间归一化方法（如MNI坐标）虽然提供了解剖参考，但往往无法捕捉真正的功能相似性，尤其在定位不精确时，即使匹配解剖坐标，不同个体的目标脑区域和神经动力学也可能存在显著差异。&lt;h4&gt;目的&lt;/h4&gt;开发一种可扩展的表示学习框架，学习受试者无关的电极功能身份，并建模区域间关系，以实现跨受试者颅内神经数据的有效聚合和分析，特别是在缺乏严格任务结构和均匀传感器布局的情况下。&lt;h4&gt;方法&lt;/h4&gt;研究提出了一种双阶段框架：(1) 使用具有对比目标的孪生编码器，从多区域局部场电位中学习每个电极的受试者无关功能身份，诱导对区域特定神经签名具有局部敏感性的嵌入几何结构；(2) 将这些嵌入标记化，用于变换器模型，使用可变数量的通道建模区域间关系。研究在包含20名受试者的基底节-丘脑区域数据集上评估了该方法，这些数据集来自灵活的休息/运动记录会话，具有异构的电极布局。&lt;h4&gt;主要发现&lt;/h4&gt;学习到的功能空间支持准确的受试者内辨别，形成清晰、区域一致的聚类，并能零样本迁移到未见过的通道。变换器在功能标记上操作，无需受试者特定的头部或监督，能够捕获跨区域依赖关系并重建被屏蔽的通道，为下游解码提供了受试者无关的主干。&lt;h4&gt;结论&lt;/h4&gt;该研究为颅内神经数据在缺乏严格任务结构和均匀传感器布局情况下的跨受试者聚合和预训练提供了一条新途径，有望促进大规模神经数据分析的发展。&lt;h4&gt;翻译&lt;/h4&gt;跨受试者颅内记录数据聚合具有挑战性，因为电极数量、放置位置和覆盖区域差异很大。像MNI坐标这样的空间归一化方法提供了共享的解剖参考，但往往无法捕捉真正的功能相似性，尤其是在定位不精确时；即使在匹配的解剖坐标上，不同个体之间的目标脑区域和潜在的神经动力学也可能存在显著差异。我们提出了一种可扩展的表示学习框架，该框架(i)使用具有对比目标的孪生编码器，从多区域局部场电位中学习每个电极的受试者无关功能身份，诱导一种对区域特定神经签名具有局部敏感性的嵌入几何结构，以及(ii)将这些嵌入标记化，用于一个变换器，该变换器使用可变数量的通道建模区域间关系。我们在一个包含20名受试者的数据集上评估了该框架，这些数据集涵盖了基底节-丘脑区域，是在灵活的休息/运动记录会话期间收集的，具有异构的电极布局。学习到的功能空间支持准确的受试者内辨别，并形成清晰、区域一致的聚类；它可以零样本迁移到未见过的通道。变换器在功能标记上操作，无需受试者特定的头部或监督，能够捕获跨区域依赖关系，并重建被屏蔽的通道，为下游解码提供了受试者无关的主干。这些结果表明，在严格的任务结构和均匀传感器布局不可用的情况下，为颅内神经数据实现大规模、跨受试者聚合和预训练提供了一条途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aggregating intracranial recordings across subjects is challenging sinceelectrode count, placement, and covered regions vary widely. Spatialnormalization methods like MNI coordinates offer a shared anatomical reference,but often fail to capture true functional similarity, particularly whenlocalization is imprecise; even at matched anatomical coordinates, the targetedbrain region and underlying neural dynamics can differ substantially betweenindividuals. We propose a scalable representation-learning framework that (i)learns a subject-agnostic functional identity for each electrode frommulti-region local field potentials using a Siamese encoder with contrastiveobjectives, inducing an embedding geometry that is locality-sensitive toregion-specific neural signatures, and (ii) tokenizes these embeddings for atransformer that models inter-regional relationships with a variable number ofchannels. We evaluate this framework on a 20-subject dataset spanning basalganglia-thalamic regions collected during flexible rest/movement recordingsessions with heterogeneous electrode layouts. The learned functional spacesupports accurate within-subject discrimination and forms clear,region-consistent clusters; it transfers zero-shot to unseen channels. Thetransformer, operating on functional tokens without subject-specific heads orsupervision, captures cross-region dependencies and enables reconstruction ofmasked channels, providing a subject-agnostic backbone for downstream decoding.Together, these results indicate a path toward large-scale, cross-subjectaggregation and pretraining for intracranial neural data where strict taskstructure and uniform sensor placement are unavailable.</description>
      <author>example@mail.com (Sina Javadzadeh, Rahil Soroushmojdehi, S. Alireza Seyyed Mousavi, Mehrnaz Asadi, Sumiko Abe, Terence D. Sanger)</author>
      <guid isPermaLink="false">2510.27090v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Incremental Human-Object Interaction Detection with Invariant Relation Representation Learning</title>
      <link>http://arxiv.org/abs/2510.27020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一个名为'无样本增量关系蒸馏'(IRD)的框架，用于解决开放环境中人类-物体交互(HOI)的增量检测问题，以应对动态环境中的交互漂移和零样本HOI组合检测挑战。&lt;h4&gt;背景&lt;/h4&gt;在开放世界环境中，人类-物体交互(HOI)不断演变，这挑战了传统的封闭世界HOI检测模型。人类具有渐进式获取知识的能力，而现有的增量学习模型面临灾难性遗忘问题，以及交互漂移和零样本HOI组合检测的特殊挑战。&lt;h4&gt;目的&lt;/h4&gt;开发能够识别动态环境中人类-物体关系的智能体，即增量HOI检测(IHOID)，以解决增量学习中的灾难性遗忘问题，以及交互漂移和零样本HOI组合检测的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个无样本增量关系蒸馏(IRD)框架。IRD将物体和关系的学习解耦，并引入了两种独特的蒸馏损失，用于学习在不同HOI组合中共享相同关系的不变关系特征。&lt;h4&gt;主要发现&lt;/h4&gt;在HICO-DET和V-COCO数据集上的大量实验表明，该方法在减轻遗忘、增强对交互漂移的鲁棒性以及零样本HOI的泛化能力方面优于最先进的基线方法。&lt;h4&gt;结论&lt;/h4&gt;IRD框架有效地解决了开放世界环境中HOI检测的增量学习挑战，特别是在处理动态交互和零样本场景方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;在开放世界环境中，人类-物体交互(HOI)不断演变，这对传统的封闭世界HOI检测模型提出了挑战。受人类渐进式获取知识能力的启发，我们探索了增量HOI检测(IHOID)，以开发能够识别此类动态环境中人类-物体关系的智能体。这种设置不仅面临增量学习中常见的灾难性遗忘问题，还面临由交互漂移和检测具有连续到达数据的零样本HOI组合带来的特殊挑战。因此，我们提出了一个新颖的无样本增量关系蒸馏(IRD)框架。IRD将物体和关系的学习解耦，并引入了两种独特的蒸馏损失，用于学习在不同HOI组合中共享相同关系的不变关系特征。在HICO-DET和V-COCO数据集上的大量实验证明了我们的方法在减轻遗忘、增强对交互漂移的鲁棒性以及零样本HOI的泛化能力方面优于最先进的基线方法。代码可在以下网址获取：https://github.com/weiyana/ContinualHOI&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In open-world environments, human-object interactions (HOIs) evolvecontinuously, challenging conventional closed-world HOI detection models.Inspired by humans' ability to progressively acquire knowledge, we exploreincremental HOI detection (IHOID) to develop agents capable of discerninghuman-object relations in such dynamic environments. This setup confronts notonly the common issue of catastrophic forgetting in incremental learning butalso distinct challenges posed by interaction drift and detecting zero-shot HOIcombinations with sequentially arriving data. Therefore, we propose a novelexemplar-free incremental relation distillation (IRD) framework. IRD decouplesthe learning of objects and relations, and introduces two unique distillationlosses for learning invariant relation features across different HOIcombinations that share the same relation. Extensive experiments on HICO-DETand V-COCO datasets demonstrate the superiority of our method overstate-of-the-art baselines in mitigating forgetting, strengthening robustnessagainst interaction drift, and generalization on zero-shot HOIs. Code isavailable at \href{https://github.com/weiyana/ContinualHOI}{this HTTP URL}</description>
      <author>example@mail.com (Yana Wei, Zeen Chi, Chongyu Wang, Yu Wu, Shipeng Yan, Yongfei Liu, Xuming He)</author>
      <guid isPermaLink="false">2510.27020v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>FOCUS: Efficient Keyframe Selection for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.27280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为FOCUS的关键帧选择方法，用于解决多模态大语言模型处理长视频时的标记预算问题，在处理不到2%视频帧的情况下实现了显著的准确性提升。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型将图像和视频帧表示为视觉标记，但扩展到长视频时标记预算会远超实际限制。现有方法要么均匀采样，要么使用小模型进行关键帧选择，但这些方法依赖预过滤且可能错过重要信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练、与模型无关的关键帧选择模块，在严格标记预算下选择与查询相关的帧，避免错过重要信息。&lt;h4&gt;方法&lt;/h4&gt;FOCUS将关键帧选择表述为多臂老虎机中的组合纯探索问题，将短时间片段视为臂，使用经验均值和伯恩斯坦置信半径识别信息区域，同时保留对不确定区域的探索。采用两阶段探索-利用程序，先识别高价值时间区域，再在每个区域内选择最高分帧。&lt;h4&gt;主要发现&lt;/h4&gt;在两个长视频问答基准测试中，FOCUS处理不到2%的视频帧实现了显著的准确性提升。对于超过20分钟的视频，在LongVideoBench上实现了11.9%的准确率提升。&lt;h4&gt;结论&lt;/h4&gt;FOCUS作为关键帧选择方法的有效性得到验证，为MLLMs可扩展的长视频理解提供了一种简单通用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)将图像和视频帧表示为视觉标记。然而，从单图像扩展到数小时长的视频会使标记预算远超实际限制。因此，常用方法要么均匀下采样，要么使用较小的视觉语言模型进行基于检索评分的关键帧选择。然而，这些关键帧选择方法仍然依赖选择前的预过滤来降低推理成本，并可能错过最具信息量的时刻。我们提出FOCUS，即Frame-Optimistic Confidence Upper-bound Selection，这是一种无需训练、与模型无关的关键帧选择模块，在严格的标记预算下选择与查询相关的帧。FOCUS将关键帧选择表述为多臂老虎机中的组合纯探索问题：它将短时间片段视为臂，并使用经验均值和伯恩斯坦置信半径来识别信息丰富的区域，同时保留对不确定区域的探索。 resulting两阶段探索-利用程序从具有理论保证的顺序策略简化而来，首先识别高价值时间区域，然后在每个区域内选择得分最高的帧。在两个长视频问答基准测试中，FOCUS在处理不到2%的视频帧的同时提供了显著的准确性提升。对于超过20分钟的视频，它在LongVideoBench上实现了11.9%的准确率提升，证明了其作为关键帧选择方法的有效性，并为MLLMs可扩展的长视频理解提供了一种简单通用的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) represent images and video frames asvisual tokens. Scaling from single images to hour-long videos, however,inflates the token budget far beyond practical limits. Popular pipelinestherefore either uniformly subsample or apply keyframe selection withretrieval-style scoring using smaller vision-language models. However, thesekeyframe selection methods still rely on pre-filtering before selection toreduce the inference cost and can miss the most informative moments.  We propose FOCUS, Frame-Optimistic Confidence Upper-bound Selection, atraining-free, model-agnostic keyframe selection module that selectsquery-relevant frames under a strict token budget. FOCUS formulates keyframeselection as a combinatorial pure-exploration (CPE) problem in multi-armedbandits: it treats short temporal clips as arms, and uses empirical means andBernstein confidence radius to identify informative regions while preservingexploration of uncertain areas. The resulting two-stageexploration-exploitation procedure reduces from a sequential policy withtheoretical guarantees, first identifying high-value temporal regions, thenselecting top-scoring frames within each region On two long-videoquestion-answering benchmarks, FOCUS delivers substantial accuracy improvementswhile processing less than 2% of video frames. For videos longer than 20minutes, it achieves an 11.9% gain in accuracy on LongVideoBench, demonstratingits effectiveness as a keyframe selection method and providing a simple andgeneral solution for scalable long-video understanding with MLLMs.</description>
      <author>example@mail.com (Zirui Zhu, Hailun Xu, Yang Luo, Yong Liu, Kanchan Sarkar, Zhenheng Yang, Yang You)</author>
      <guid isPermaLink="false">2510.27280v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>Adapting Large Language Models to Emerging Cybersecurity using Retrieval Augmented Generation</title>
      <link>http://arxiv.org/abs/2510.27080v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究引入了一个基于检索增强生成(RAG)的框架，用于增强大型语言模型在网络安全任务中的适应性和可靠性，特别是在知识保留和时间推理方面。&lt;h4&gt;背景&lt;/h4&gt;安全应用越来越多地依赖大型语言模型进行网络威胁检测，但其不透明的推理限制了信任，特别是在需要特定领域知识的决策中。安全威胁迅速演变，要求模型不仅回忆历史事件，还要适应新漏洞和攻击模式。RAG在一般LLM应用中已显示出有效性，但在网络安全领域的潜力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一个基于RAG的框架，使网络安全数据情境化，并增强大型语言模型在知识保留和时间推理方面的准确性。&lt;h4&gt;方法&lt;/h4&gt;使用外部数据集和Llama-3-8B-Instruct模型，评估基准RAG和优化的混合检索方法，并在多个性能指标上进行比较分析。&lt;h4&gt;主要发现&lt;/h4&gt;混合检索方法在增强大型语言模型对网络安全任务的适应性和可靠性方面显示出显著前景。&lt;h4&gt;结论&lt;/h4&gt;基于RAG的框架可以有效提升大型语言模型在网络安全领域的应用能力，特别是在处理不断演变的安全威胁时。&lt;h4&gt;翻译&lt;/h4&gt;安全应用越来越多地依赖大型语言模型(LLMs)进行网络威胁检测；然而，它们的不透明推理常常限制了信任，特别是在需要特定领域网络安全知识的决策中。由于安全威胁迅速演变，LLM不仅要回忆历史事件，还要适应新兴的漏洞和攻击模式。检索增强生成(RAG)已在一般LLM应用中显示出有效性，但其在网络安全领域的潜力仍未得到充分探索。在这项工作中，我们引入了一个基于RAG的框架，用于使网络安全数据情境化，并增强LLM在知识保留和时间推理方面的准确性。使用外部数据集和Llama-3-8B-Instruct模型，我们评估了基准RAG和优化的混合检索方法，并在多个性能指标上进行了比较分析。我们的研究结果强调了混合检索在增强LLM对网络安全任务的适应性和可靠性方面的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Security applications are increasingly relying on large language models(LLMs) for cyber threat detection; however, their opaque reasoning often limitstrust, particularly in decisions that require domain-specific cybersecurityknowledge. Because security threats evolve rapidly, LLMs must not only recallhistorical incidents but also adapt to emerging vulnerabilities and attackpatterns. Retrieval-Augmented Generation (RAG) has demonstrated effectivenessin general LLM applications, but its potential for cybersecurity remainsunderexplored. In this work, we introduce a RAG-based framework designed tocontextualize cybersecurity data and enhance LLM accuracy in knowledgeretention and temporal reasoning. Using external datasets and theLlama-3-8B-Instruct model, we evaluate baseline RAG, an optimized hybridretrieval approach, and conduct a comparative analysis across multipleperformance metrics. Our findings highlight the promise of hybrid retrieval instrengthening the adaptability and reliability of LLMs for cybersecurity tasks.</description>
      <author>example@mail.com (Arnabh Borah, Md Tanvirul Alam, Nidhi Rastogi)</author>
      <guid isPermaLink="false">2510.27080v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>M^3Detection: Multi-Frame Multi-Level Feature Fusion for Multi-Modal 3D Object Detection with Camera and 4D Imaging Radar</title>
      <link>http://arxiv.org/abs/2510.27166v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;M^3Detection是一个统一的多帧3D物体检测框架，通过融合相机和4D成像雷达数据，实现多级特征融合，解决了单帧融合信息不完整的问题，在多帧检测中取得了最先进的效果。&lt;h4&gt;背景&lt;/h4&gt;4D成像雷达在恶劣天气条件下提供稳健感知，相机传感器提供密集语义信息，两者互补融合对3D感知具有潜力。然而现有方法多限于单帧输入，无法捕捉完整场景，且图像退化和雷达稀疏性影响检测性能。&lt;h4&gt;目的&lt;/h4&gt;解决多帧融合面临的两个挑战：实现跨帧和跨模态的稳健有效物体特征融合，以及减少冗余特征提取的计算成本。&lt;h4&gt;方法&lt;/h4&gt;提出M^3Detection框架，在多模态数据上进行多级特征融合；利用基线检测器的中间特征和跟踪器生成参考轨迹；设计雷达信息引导的全局级间对象特征聚合模块对候选提案进行全局特征对齐；设计局部级间网格特征聚合模块扩展局部特征；使用轨迹级多帧时空推理模块编码跨帧交互并增强时间表示。&lt;h4&gt;主要发现&lt;/h4&gt;在VoD和TJ4DRadSet数据集上的实验表明，M^3Detection实现了最先进的3D检测性能，验证了其在多帧检测与相机-4D成像雷达融合方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;M^3Detection通过多级特征融合和时空推理，有效解决了相机-雷达融合中的信息不完整问题，提高了检测性能，为多模态多帧3D感知提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近的4D成像雷达进展使得在恶劣天气条件下能够实现稳健的感知，而相机传感器则提供密集的语义信息。融合这些互补的模态在成本效益高的3D感知方面具有巨大潜力。然而，大多数现有的相机-雷达融合方法仅限于单帧输入，只能捕捉场景的部分视图。不完整的场景信息，加上图像退化和4D雷达的稀疏性，阻碍了整体检测性能。相比之下，多帧融合提供了更丰富的时空信息，但面临两个挑战：实现跨帧和跨模态的稳健有效物体特征融合，以及减少冗余特征提取的计算成本。因此，我们提出了M^3Detection，一个统一的多帧3D物体检测框架，在来自相机和4D成像雷达的多模态数据上进行多级特征融合。我们的框架利用基线检测器的中间特征并使用跟踪器生成参考轨迹，提高计算效率并为第二阶段提供更丰富的信息。在第二阶段，我们设计了一个雷达信息引导的全局级间对象特征聚合模块，对候选提案进行全局特征对齐，以及一个局部级间网格特征聚合模块，沿着参考轨迹扩展局部特征以增强细粒度物体表示。然后，聚合的特征通过轨迹级多帧时空推理模块进行处理，以编码跨帧交互并增强时间表示。在VoD和TJ4DRadSet数据集上的大量实验表明，M^3Detection实现了最先进的3D检测性能，验证了其在多帧检测与相机-4D成像雷达融合方面的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决单帧相机-雷达融合3D物体检测的局限性，即单帧输入只能捕捉场景部分视图，导致不完整场景信息、图像退化和4D雷达稀疏性影响检测性能。这个问题在自动驾驶领域至关重要，因为3D物体检测是自动驾驶的基础技术，而多帧信息能提供更丰富的时空上下文，提高检测准确性和鲁棒性，同时解决计算效率问题对实时系统至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了单帧检测的局限性及多帧融合的潜力与挑战，然后设计了两阶段框架：第一阶段利用基线检测器提取特征并生成初始检测结果和参考轨迹；第二阶段进行多帧多级特征融合。作者借鉴了现有BEV特征融合检测器（如BEVFusion）、两阶段检测框架（如MPPNet）、注意力机制和跟踪算法（如Immortal），但创新性地设计了多级特征融合策略来解决多模态多帧检测问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多帧多级特征融合提升相机和4D雷达多模态3D物体检测性能，利用丰富时空信息同时避免冗余特征提取。流程分两阶段：第一阶段使用基线检测器提取特征并生成轨迹；第二阶段包括三个模块：全局级间物体特征聚合(GOA)减轻跟踪不确定性，局部级间网格特征聚合(LGA)增强细粒度表示，轨迹级多帧时空推理(MSTR)建模时间特征交互，最终融合全局和局部特征进行检测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的多帧3D物体检测框架M3Detection；2)多帧多级特征融合策略(GOA、LGA和MSTR模块)。相比之前工作，不同之处在于：避免了传统多帧方法的冗余特征提取；采用多级特征融合同时关注全局上下文和局部细节；专门针对相机和4D雷达多模态融合优化；通过多假设策略减轻跟踪不确定性影响；不仅进行场景级时间聚合，还进行物体级精细建模。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; M3Detection通过创新的多帧多级特征融合框架，有效整合了相机和4D成像雷达的互补信息，在保持计算效率的同时显著提高了3D物体检测的准确性和鲁棒性，为自动驾驶感知系统提供了更可靠的解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in 4D imaging radar have enabled robust perception in adverseweather, while camera sensors provide dense semantic information. Fusing thethese complementary modalities has great potential for cost-effective 3Dperception. However, most existing camera-radar fusion methods are limited tosingle-frame inputs, capturing only a partial view of the scene. The incompletescene information, compounded by image degradation and 4D radar sparsity,hinders overall detection performance. In contrast, multi-frame fusion offersricher spatiotemporal information but faces two challenges: achieving robustand effective object feature fusion across frames and modalities, andmitigating the computational cost of redundant feature extraction.Consequently, we propose M^3Detection, a unified multi-frame 3D objectdetection framework that performs multi-level feature fusion on multi-modaldata from camera and 4D imaging radar. Our framework leverages intermediatefeatures from the baseline detector and employs the tracker to producereference trajectories, improving computational efficiency and providing richerinformation for second-stage. In the second stage, we design a global-levelinter-object feature aggregation module guided by radar information to alignglobal features across candidate proposals and a local-level inter-grid featureaggregation module that expands local features along the reference trajectoriesto enhance fine-grained object representation. The aggregated features are thenprocessed by a trajectory-level multi-frame spatiotemporal reasoning module toencode cross-frame interactions and enhance temporal representation. Extensiveexperiments on the VoD and TJ4DRadSet datasets demonstrate that M^3Detectionachieves state-of-the-art 3D detection performance, validating itseffectiveness in multi-frame detection with camera-4D imaging radar fusion.</description>
      <author>example@mail.com (Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang)</author>
      <guid isPermaLink="false">2510.27166v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>MLPerf Automotive</title>
      <link>http://arxiv.org/abs/2510.27065v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 5 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MLPerf Automotive是首个用于评估汽车系统中AI加速部署的机器学习系统的标准化公共基准测试。&lt;h4&gt;背景&lt;/h4&gt;现有的基准测试套件无法用于汽车系统，因为汽车工作负载具有独特的约束，包括安全性和实时处理能力，这些特性使它们区别于之前引入基准测试所针对的领域。&lt;h4&gt;目的&lt;/h4&gt;解决汽车机器学习系统需要标准化性能评估方法的问题。&lt;h4&gt;方法&lt;/h4&gt;通过MLCommons和自动驾驶计算联盟之间的合作开发，提供延迟和准确性指标以及评估协议，使不同硬件平台和软件实现之间能够进行一致且可重复的性能比较。&lt;h4&gt;主要发现&lt;/h4&gt;第一版基准测试包括2D目标检测、2D语义分割和3D目标检测等汽车感知任务。描述了基准设计的方法论，包括任务选择、参考模型和提交规则。讨论了第一轮基准提交以及获取数据集和开发参考实现所涉及的挑战。&lt;h4&gt;结论&lt;/h4&gt;MLPerf Automotive基准测试为汽车AI系统提供了标准化的评估方法，使不同平台和实现之间的性能比较成为可能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了MLPerf Automotive，这是首个用于评估部署在汽车系统中用于AI加速的机器学习系统的标准化公共基准测试。该基准测试由MLCommons和自动驾驶计算联盟通过合作开发，解决了汽车机器学习系统需要标准化性能评估方法的需求。现有的基准测试套件无法用于这些系统，因为汽车工作负载具有独特的约束，包括安全性和实时处理能力，这些特性使它们区别于之前引入基准测试所针对的领域。我们的基准测试框架提供了延迟和准确性指标以及评估协议，使不同硬件平台和软件实现之间能够进行一致且可重复的性能比较。第一版基准测试包括2D目标检测、2D语义分割和3D目标检测等汽车感知任务。我们描述了基准设计背后的方法论，包括任务选择、参考模型和提交规则。我们还讨论了第一轮基准提交以及获取数据集和开发参考实现所涉及的挑战。我们的基准测试代码可在https://github.com/mlcommons/mlperf_automotive获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-31&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present MLPerf Automotive, the first standardized public benchmark forevaluating Machine Learning systems that are deployed for AI acceleration inautomotive systems. Developed through a collaborative partnership betweenMLCommons and the Autonomous Vehicle Computing Consortium, this benchmarkaddresses the need for standardized performance evaluation methodologies inautomotive machine learning systems. Existing benchmark suites cannot beutilized for these systems since automotive workloads have unique constraintsincluding safety and real-time processing that distinguish them from thedomains that previously introduced benchmarks target. Our benchmarkingframework provides latency and accuracy metrics along with evaluation protocolsthat enable consistent and reproducible performance comparisons acrossdifferent hardware platforms and software implementations. The first iterationof the benchmark consists of automotive perception tasks in 2D objectdetection, 2D semantic segmentation, and 3D object detection. We describe themethodology behind the benchmark design including the task selection, referencemodels, and submission rules. We also discuss the first round of benchmarksubmissions and the challenges involved in acquiring the datasets and theengineering efforts to develop the reference implementations. Our benchmarkcode is available at https://github.com/mlcommons/mlperf_automotive.</description>
      <author>example@mail.com (Radoyeh Shojaei, Predrag Djurdjevic, Mostafa El-Khamy, James Goel, Kasper Mecklenburg, John Owens, Pınar Muyan-Özçelik, Tom St. John, Jinho Suh, Arjun Suresh)</author>
      <guid isPermaLink="false">2510.27065v1</guid>
      <pubDate>Mon, 03 Nov 2025 15:37:44 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniField的方法，通过整合多个子领域的空气动力学数据进行联合训练，解决了数据稀缺性问题，实现了更通用的流场表示。&lt;h4&gt;背景&lt;/h4&gt;表面压力场的空气动力学模拟对许多工程问题至关重要。深度神经网络已成为传统计算流体力学(CFD)模拟的高效替代方案，但数据稀缺性限制了神经网络的应用。&lt;h4&gt;目的&lt;/h4&gt;整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示，解决数据稀缺性问题。&lt;h4&gt;方法&lt;/h4&gt;整合五个涵盖汽车、火车、飞机和一般形状等不同领域的数据集。提出UniField方法，采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流条件适配器以适应不同子领域的流信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据遵循不同方程，但联合训练的模型比单独训练的模型表现更好，表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;UniField作为通用流场表示模型具有潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算成本高昂的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺性仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示。我们整合了五个涵盖不同领域的数据集，包括汽车、火车、飞机和一般形状。面对不同领域间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流条件适配器以适应不同子领域的流信息。尽管不同子领域的空气动力学数据通常遵循不同的方程，但我们比较了在所有数据上联合训练的模型与在单个数据集上分别训练的模型，发现联合训练的模型通常表现更好。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Yueqing Wang, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v2</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
  <item>
      <title>Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</title>
      <link>http://arxiv.org/abs/2510.22033v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种线性最优传输(LOT)框架，用于处理单细胞技术生成的高维点云数据，实现了预测准确性、可解释性和生成建模的统一。&lt;h4&gt;背景&lt;/h4&gt;单细胞技术生成高维点云数据，能详细表征复杂患者状态和治疗反应。每个患者由不规则点云而非简单向量表示，难以直接量化和比较个体间生物学差异。现有非线性方法虽准确但如同黑箱，生物学可解释性差。&lt;h4&gt;目的&lt;/h4&gt;开发一种既能保持预测准确性又具有生物学可解释性的方法，解决单细胞点云数据分析中的挑战。&lt;h4&gt;方法&lt;/h4&gt;适配线性最优传输(LOT)框架，将不规则点云嵌入固定维度欧几里得空间，同时保留分布结构。这种嵌入提供有原则的线性表示，形成患者间的配准，支持直接比较细胞分布。&lt;h4&gt;主要发现&lt;/h4&gt;LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，分类器权重映射回特定标志物和空间区域；(ii) 患者来源类器官的合成数据生成。LOT形心产生平均细胞谱，支持药物相互作用测试。&lt;h4&gt;结论&lt;/h4&gt;LOT作为统一框架连接了预测性能、可解释性和生成建模，通过将异质点云转换为结构化嵌入，为理解高维生物系统中的免疫变异和治疗效应开辟新机会。&lt;h4&gt;翻译&lt;/h4&gt;单细胞技术生成细胞的高维点云，能够详细表征复杂的患者状态和治疗反应。然而每个患者由不规则点云而非简单向量表示，使得难以直接量化和比较个体间的生物学差异。诸如核方法和神经网络等非线性方法能达到预测准确性，但如同黑箱，提供很少的生物学可解释性。为解决这些局限性，我们将线性最优传输(LOT)框架适配到这一场景，将不规则点云嵌入到固定维度的欧几里得空间，同时保留分布结构。这种嵌入提供了有原则的线性表示，保留了最优传输几何，同时支持下游分析。它还形成了任意两个患者之间的配准，使其能够直接比较细胞分布。在此空间中，LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，其中分类器权重映射回驱动预测的特定标志物和空间区域；(ii) 患者来源类器官的合成数据生成，利用LOT嵌入的线性性。LOT形心产生平均细胞谱，代表组合条件或样本，支持药物相互作用测试。这些结果共同确立了LOT作为连接预测性能、可解释性和生成建模的统一框架。通过将异质点云转换为可直接追溯到原始数据的结构化嵌入，LOT为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell technologies generate high-dimensional point clouds of cells,enabling detailed characterization of complex patient states and treatmentresponses. Yet each patient is represented by an irregular point cloud ratherthan a simple vector, making it difficult to directly quantify and comparebiological differences between individuals. Nonlinear methods such as kernelsand neural networks achieve predictive accuracy but act as black boxes,offering little biological interpretability.  To address these limitations, we adapt the Linear Optimal Transport (LOT)framework to this setting, embedding irregular point clouds into afixed-dimensional Euclidean space while preserving distributional structure.This embedding provides a principled linear representation that preservesoptimal transport geometry while enabling downstream analysis. It also forms aregistration between any two patients, enabling direct comparison of theircellular distributions. Within this space, LOT enables: (i) \textbf{accurateand interpretable classification} of COVID-19 patient states, where classifierweights map back to specific markers and spatial regions driving predictions;and (ii) \textbf{synthetic data generation} for patient-derived organoids,exploiting the linearity of the LOT embedding. LOT barycenters yield averagedcellular profiles representing combined conditions or samples, supporting druginteraction testing.  Together, these results establish LOT as a unified framework that bridgespredictive performance, interpretability, and generative modeling. Bytransforming heterogeneous point clouds into structured embeddings directlytraceable to the original data, LOT opens new opportunities for understandingimmune variation and treatment effects in high-dimensional biological systems.</description>
      <author>example@mail.com (Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger)</author>
      <guid isPermaLink="false">2510.22033v2</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Clone Deterministic 3D Worlds with Geometrically-Regularized World Models</title>
      <link>http://arxiv.org/abs/2510.26782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种几何正则化世界模型(GRWM)，通过改进表示学习来提高世界模型的性能，特别是在长期预测任务中表现出更好的保真度和稳定性。&lt;h4&gt;背景&lt;/h4&gt;世界模型是一种内部模型，用于模拟世界的发展，基于过去的观察和行动预测智能体及其环境的未来。准确的世界模型对智能体在复杂环境中有效思考、规划和推理至关重要。然而，当前世界模型在长期范围内表现脆弱且会退化。&lt;h4&gt;目的&lt;/h4&gt;研究仅通过改进表示学习是否能显著提高世界模型的性能，并构建一个能够完全克隆并拟合确定性3D世界的模型。&lt;h4&gt;方法&lt;/h4&gt;提出几何正则化世界模型(GRWM)，强制自然感觉轨迹上的连续点在潜在表示空间中保持接近，从而学习与环境真实拓扑紧密对齐的潜在表示。&lt;h4&gt;主要发现&lt;/h4&gt;GRWM显著提高了确定性3D环境和长期预测任务中的滚动保真度和稳定性，其优势源于学习具有优越几何结构的潜在流形。GRWM是即插即用的，只需最小架构修改，可随轨迹长度扩展，且兼容各种潜在生成主干网络。&lt;h4&gt;结论&lt;/h4&gt;改进表示学习是构建健壮世界模型的直接且有用的途径，无需扩大动态模块即可提供可靠的长期预测。&lt;h4&gt;翻译&lt;/h4&gt;世界模型是一种内部模型，用于模拟世界的发展。基于过去的观察和行动，它预测智能体及其环境的未来。准确的世界模型对于智能体在复杂动态环境中有效思考、规划和推理至关重要。尽管进展迅速，但当前世界模型仍然脆弱，且在长期范围内会退化。我们认为，一个主要原因是表示质量：外部输入（如图像）是高维度的，且有损或纠缠的潜在表示使动态学习变得不必要地困难。因此，我们研究仅通过改进表示学习是否能显著提高世界模型的性能。在本文中，我们通过解决一个基本但尚未解决的问题，朝着构建真正准确的世界模型迈出了一步：构建一个能够完全克隆并拟合确定性3D世界的模型。我们提出了几何正则化世界模型(GRWM)，强制自然感觉轨迹上的连续点在潜在表示空间中保持接近。这种方法产生了显著改进的潜在表示，与环境真实拓扑紧密对齐。GRWM是即插即用的，只需要最小的架构修改，可随轨迹长度扩展，并且兼容各种潜在生成主干网络。在确定性3D环境和长期预测任务中，GRWM显著提高了滚动保真度和稳定性。分析表明，其优势源于学习具有优越几何结构的潜在流形。这些发现支持一个明确的结论：改进表示学习是构建健壮世界模型的直接且有用的途径，无需扩大动态模块即可提供可靠的长期预测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的是如何构建能够准确克隆确定性3D世界的世界模型，特别是解决长期预测中误差累积导致轨迹偏离现实的问题。这个问题在现实中非常重要，因为世界模型是强化学习、机器人规划和游戏内容生成等应用的核心工具，而当前世界模型在长期预测方面表现脆弱，无法满足需要精确模拟环境的实际应用需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过实验发现，当世界模型直接使用真实底层状态时预测效果极佳，但使用标准VAE的潜在空间时性能急剧下降，这使他们认识到表示质量是主要瓶颈。他们借鉴了对比学习(特别是时序对比学习)和几何正则化在3D物体表示学习中的应用，将其扩展到3D环境建模领域。具体设计上，他们提出了结合时序上下文架构和时序对比正则化的GRWM方法，确保潜在空间结构与环境的真实状态流形一致。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过改进表示学习来提升世界模型的性能，使潜在空间结构与环境的真实状态流形保持一致。整体实现流程包括：1)使用因果编码器处理连续观察序列生成潜在表示；2)结合重构损失、KL散度项、时序缓慢损失(确保连续状态在潜在空间中接近)和潜在均匀性损失(防止特征坍塌)进行训练；3)将训练好的潜在表示作为输入提供给各种动力学模型进行状态转换学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)重新定义问题，从开放世界生成转向确定性环境的精确复制；2)提出'表示质量优先'的观点，与当前侧重改进动力学模型的主流思路不同；3)引入几何正则化方法，确保潜在空间结构与环境的真实拓扑一致；4)设计插件式组件，可无缝集成到现有模型中；5)显著提高长期轨迹预测的稳定性和准确性。相比之前工作，GRWM更注重表示质量而非复杂动力学模型，结合了时序上下文和几何正则化，且完全无监督，不需要真实状态标签。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过几何正则化改进潜在表示学习，GRWM显著提高了确定性3D世界模型的长期预测准确性和稳定性，证明了表示质量对构建可靠世界模型的关键作用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A world model is an internal model that simulates how the world evolves.Given past observations and actions, it predicts the future of both theembodied agent and its environment. Accurate world models are essential forenabling agents to think, plan, and reason effectively in complex, dynamicsettings. Despite rapid progress, current world models remain brittle anddegrade over long horizons. We argue that a central cause is representationquality: exteroceptive inputs (e.g., images) are high-dimensional, and lossy orentangled latents make dynamics learning unnecessarily hard. We therefore askwhether improving representation learning alone can substantially improveworld-model performance. In this work, we take a step toward building a trulyaccurate world model by addressing a fundamental yet open problem: constructinga model that can fully clone and overfit to a deterministic 3D world. Wepropose Geometrically-Regularized World Models (GRWM), which enforces thatconsecutive points along a natural sensory trajectory remain close in latentrepresentation space. This approach yields significantly improved latentrepresentations that align closely with the true topology of the environment.GRWM is plug-and-play, requires only minimal architectural modification, scaleswith trajectory length, and is compatible with diverse latent generativebackbones. Across deterministic 3D settings and long-horizon prediction tasks,GRWM significantly increases rollout fidelity and stability. Analyses show thatits benefits stem from learning a latent manifold with superior geometricstructure. These findings support a clear takeaway: improving representationlearning is a direct and useful path to robust world models, deliveringreliable long-horizon predictions without enlarging the dynamics module.</description>
      <author>example@mail.com (Zaishuo Xia, Yukuan Lu, Xinyi Li, Yifan Xu, Yubei Chen)</author>
      <guid isPermaLink="false">2510.26782v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>UniTok-Audio: A Unified Audio Generation Framework via Generative Modeling on Discrete Codec Tokens</title>
      <link>http://arxiv.org/abs/2510.26372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UniTok-Audio框架，解决了音频生成模型在质量和泛化能力方面的挑战，实现了统一的音频生成任务处理。&lt;h4&gt;背景&lt;/h4&gt;生成式建模在文本、图像和音频领域取得显著成功，但音频生成模型仍面临音频质量和跨任务泛化能力的挑战，导致开发冗余、性能不一致和扩展性有限。&lt;h4&gt;目的&lt;/h4&gt;提出UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架，解决现有音频生成模型的碎片化问题。&lt;h4&gt;方法&lt;/h4&gt;1) 提取条件的连续特征，以自回归方式生成目标音频的离散令牌；2) 使用特殊任务标识符令牌统一不同任务的学习模式；3) 开发包含声学和语义分支的双流音频编解码器实现高保真波形重构。&lt;h4&gt;主要发现&lt;/h4&gt;UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）上与最先进的特定任务或多任务系统相比具有竞争力的性能。&lt;h4&gt;结论&lt;/h4&gt;UniTok-Audio提供了一个统一的音频生成框架，解决了现有模型的碎片化问题，作者将开源代码库以促进未来研究。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模最近在文本、图像和音频领域取得了显著成功，展示了统一表征学习的强大能力。然而，音频生成模型在音频质量和跨任务泛化能力方面仍面临挑战。这种碎片化导致了冗余的开发工作、不一致的性能和有限的扩展性。为了解决这些问题，我们提出了UniTok-Audio，一个可扩展且可扩展的统一音频生成任务框架。具体而言，1) UniTok-Audio以自回归方式提取条件的连续特征，生成目标音频的离散令牌；2) 特殊的任务标识符令牌在单一框架中统一了多种任务的学习模式；3) 开发了包含声学和语义分支的双流音频编解码器，用于高保真波形重构。实验结果表明，UniTok-Audio在五个时间对齐任务（语音恢复、目标说话人提取、语音分离、语音转换和语言查询音频源分离）上与最先进的特定任务或多任务系统相比具有竞争力的性能。为了促进未来的研究，我们将开源我们的代码库。我们工作的演示页面可以在以下网址找到：https://alibaba.github.io/unified-audio。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has recently achieved remarkable success across text,image, and audio domains, demonstrating powerful capabilities for unifiedrepresentation learning. However, audio generation models still face challengesin terms of audio quality and generalization ability across tasks. Thisfragmentation results in redundant development efforts, inconsistentperformance, and limited extensibility. To address these issues, we propose\textbf{UniTok-Audio}, a scalable and extensible framework for unified audiogeneration tasks. Specifically, 1) UniTok-Audio extracts continuous feature ofconditions to generates discrete tokens of target audio in an autoregressivemanner; 2) a special task identifier token unifies different learning patternsof multiple tasks in a single framework; 3) a dual-stream audio codec involvingacoustic and semantic branch is developed for high-fidelity waveformreconstruction. Experimental results demonstrate that UniTok-Audio achievescompetitive performance in comparation with state-of-the-art task-specific ormulti-task systems across five time-aligned tasks: speech restoration, targetspeaker extraction, speech separation, voice conversion, and language-queriedaudio source separation. To foster future research, we will open-source ourcodebase. The demo page of our work can be found here:https://alibaba.github.io/unified-audio.</description>
      <author>example@mail.com (Chengwei Liu, Haoyin Yan, Shaofei Xue, Xiaotao Liang, Yinghao Liu, Zheng Xue, Gang Song, Boyang Zhou)</author>
      <guid isPermaLink="false">2510.26372v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Understanding Hardness of Vision-Language Compositionality from A Token-level Causal Lens</title>
      <link>http://arxiv.org/abs/2510.26302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对CLIP模型在组合推理方面的局限性提出了一种基于标记的因果表示学习框架，揭示了CLIP在处理对象、属性和关系组合时的脆弱性根源，并为改进模型提供了理论指导。&lt;h4&gt;背景&lt;/h4&gt;CLIP模型通过在共享嵌入空间中对齐图像和文本实现了强大的跨模态泛化能力，但在处理对象、属性和关系的组合推理方面持续失败，其行为类似于词袋匹配器。先前的因果解释通常将文本建模为单个向量，掩盖了标记级别的结构，无法解释提示敏感性和对困难样本失败等核心现象。&lt;h4&gt;目的&lt;/h4&gt;解决现有CLIP模型在组合推理方面的局限性，提供对标记级别结构的更好理解，并解释模型在提示敏感性和困难样本处理上的失败原因。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于标记的因果表示学习（CRL）框架，该框架基于顺序的语言标记结构因果模型（SCM）。将块可识别性理论扩展到标记化文本，证明CLIP的对比目标可以在句子级和标记级SCM下恢复模态不变潜在变量。&lt;h4&gt;主要发现&lt;/h4&gt;揭示了CLIP的组合脆弱性源于'组合不可识别性'现象。存在伪最优文本编码器，它们可以实现完美的模态不变对齐，但对SWAP、REPLACE和ADD操作在原子概念上明显不敏感，因此无法区分正确的标题和困难样本。分析还表明语言侧的不可识别性与视觉侧的失败通过模态差距相联系，迭代组合运算会增加问题难度。&lt;h4&gt;结论&lt;/h4&gt;标记级别的因果表示学习框架能够解释CLIP在组合推理方面的失败，这些发现为改进负样本挖掘策略和提高模型组合推理能力提供了理论依据。&lt;h4&gt;翻译&lt;/h4&gt;对比语言-图像预训练（CLIP）通过在共享嵌入空间中对齐图像和文本，提供了强大的跨模态泛化能力，但在处理对象、属性和关系的组合推理方面持续失败，其行为常常类似于词袋匹配器。先前的因果解释通常将文本建模为单个向量，掩盖了标记级别的结构，无法解释提示敏感性和对困难样本失败等核心现象。我们通过基于标记的因果表示学习（CRL）框架解决了这一差距，该框架基于顺序的语言标记结构因果模型（SCM）。我们的理论将块可识别性扩展到标记化文本，证明CLIP的对比目标可以在句子级和标记级SCM下恢复模态不变的潜在变量。关键是，标记粒度为CLIP的组合脆弱性提供了第一个原则性解释：组合不可识别性。我们展示了伪最优文本编码器的存在，这些编码器可以实现完美的模态不变对齐，但对SWAP、REPLACE和ADD操作在原子概念上明显不敏感，因此尽管与真正最优编码器优化相同的训练目标，却无法区分正确的标题和困难样本。分析进一步将语言侧的不可识别性与视觉侧的失败通过模态差距联系起来，并展示了迭代组合运算如何增加难度，从而改进了负样本挖掘策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-training (CLIP) delivers strong cross modalgeneralization by aligning images and texts in a shared embedding space, yet itpersistently fails at compositional reasoning over objects, attributes, andrelations often behaving like a bag-of-words matcher. Prior causal accountstypically model text as a single vector, obscuring token-level structure andleaving core phenomena-such as prompt sensitivity and failures on hardnegatives unexplained. We address this gap with a token-aware causalrepresentation learning (CRL) framework grounded in a sequential,language-token SCM. Our theory extends block identifiability to tokenized text,proving that CLIP's contrastive objective can recover the modal-invariantlatent variable under both sentence-level and token-level SCMs. Crucially,token granularity yields the first principled explanation of CLIP'scompositional brittleness: composition nonidentifiability. We show theexistence of pseudo-optimal text encoders that achieve perfect modal-invariantalignment yet are provably insensitive to SWAP, REPLACE, and ADD operationsover atomic concepts, thereby failing to distinguish correct captions from hardnegatives despite optimizing the same training objective as true-optimalencoders. The analysis further links language-side nonidentifiability tovisual-side failures via the modality gap and shows how iterated compositionoperators compound hardness, motivating improved negative mining strategies.</description>
      <author>example@mail.com (Ziliang Chen, Tianang Xiao, Jusheng Zhang, Yongsen Zheng, Xipeng Chen)</author>
      <guid isPermaLink="false">2510.26302v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>ReaKase-8B: Legal Case Retrieval via Knowledge and Reasoning Representations with LLMs</title>
      <link>http://arxiv.org/abs/2510.26178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了ReaKase-8B框架，通过整合法律事实、法律问题、法律关系三元组和法律推理，显著提高了法律案例检索的性能。&lt;h4&gt;背景&lt;/h4&gt;法律案例检索(LCR)是现实世界法律决策的基石，现有方法主要依赖传统的词汇模型和预训练语言模型编码法律文本，但忽略了法律实体间的关系以及法律事实和法律问题如何导致司法决策的推理过程。&lt;h4&gt;目的&lt;/h4&gt;将法律关系信息和关键推理过程整合到精确的案例嵌入中，以提高案例检索的准确性，并提出ReaKase-8B框架有效利用这些信息进行法律案例检索。&lt;h4&gt;方法&lt;/h4&gt;ReaKase-8B设计了上下文法律案例表示学习范式，使用微调的大语言模型，整合提取的法律事实、法律问题、法律关系三元组和法律推理信息。&lt;h4&gt;主要发现&lt;/h4&gt;在COLIEE 2022和COLIEE 2023两个基准数据集上的实验表明，知识和推理增强的嵌入显著提高了检索性能，超越了基线模型。&lt;h4&gt;结论&lt;/h4&gt;集成法律推理到法律案例检索系统中具有巨大潜力，ReaKase-8B框架展示了这种整合的有效性，代码已在GitHub发布。&lt;h4&gt;翻译&lt;/h4&gt;该摘要已按要求翻译为中文，提取了论文的核心要素，包括背景、目的、方法、发现和结论，并以JSON格式组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Legal case retrieval (LCR) is a cornerstone of real-world legal decisionmaking, as it enables practitioners to identify precedents for a given querycase. Existing approaches mainly rely on traditional lexical models andpretrained language models to encode the texts of legal cases. Yet there arerich information in the relations among different legal entities as well as thecrucial reasoning process that uncovers how legal facts and legal issues canlead to judicial decisions. Such relational reasoning process reflects thedistinctive characteristics of each case that can distinguish one from another,mirroring the real-world judicial process. Naturally, incorporating suchinformation into the precise case embedding could further enhance the accuracyof case retrieval. In this paper, a novel ReaKase-8B framework is proposed toleverage extracted legal facts, legal issues, legal relation triplets and legalreasoning for effective legal case retrieval. ReaKase-8B designs an in-contextlegal case representation learning paradigm with a fine-tuned large languagemodel. Extensive experiments on two benchmark datasets from COLIEE 2022 andCOLIEE 2023 demonstrate that our knowledge and reasoning augmented embeddingssubstantially improve retrieval performance over baseline models, highlightingthe potential of integrating legal reasoning into legal case retrieval systems.The code has been released on https://github.com/yanran-tang/ReaKase-8B.</description>
      <author>example@mail.com (Yanran Tang, Ruihong Qiu, Xue Li, Zi Huang)</author>
      <guid isPermaLink="false">2510.26178v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Gap Between Molecule and Textual Descriptions via Substructure-aware Alignment</title>
      <link>http://arxiv.org/abs/2510.26157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025 (main)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MolBridge，一种基于亚结构感知对齐的新型分子-文本学习框架，通过增强分子亚结构和化学短语之间的细粒度对齐，有效提升了分子表示学习的性能。&lt;h4&gt;背景&lt;/h4&gt;分子和文本表示学习越来越受到关注，因为它有潜力增强对化学信息的理解。然而，现有模型往往难以捕捉分子及其描述之间的细微差异。&lt;h4&gt;目的&lt;/h4&gt;解决现有模型缺乏学习分子亚结构和化学短语之间细粒度对齐能力的问题，提高分子表示学习的准确性。&lt;h4&gt;方法&lt;/h4&gt;通过从分子亚结构和化学短语中衍生的额外对齐信号来增强原始分子-描述对，采用亚结构感知对比学习，并结合自我完善机制来过滤有噪声的对齐信号。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MolBridge能够有效捕获细粒度对应关系，并在广泛的分子基准测试中优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;亚结构感知对齐在分子-文本学习中具有重要意义，MolBridge框架为分子表示学习提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;分子和文本表示学习因其增强化学信息理解的潜力而日益受到关注。然而，现有模型往往难以捕捉分子及其描述之间的细微差异，因为它们缺乏学习分子亚结构和化学短语之间细粒度对齐的能力。为解决这一局限性，我们引入了MolBridge，一种基于亚结构感知对齐的新型分子-文本学习框架。具体而言，我们通过从分子亚结构和化学短语中衍生的额外对齐信号来增强原始分子-描述对。为了有效学习这些丰富的对齐信息，MolBridge采用亚结构感知对比学习，并结合一种自我完善机制来过滤有噪声的对齐信号。实验结果表明，MolBridge能够有效捕获细粒度对应关系，并在广泛的分子基准测试中优于最先进的基线模型，突显了亚结构感知对齐在分子-文本学习中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecule and text representation learning has gained increasing interest dueto its potential for enhancing the understanding of chemical information.However, existing models often struggle to capture subtle differences betweenmolecules and their descriptions, as they lack the ability to learnfine-grained alignments between molecular substructures and chemical phrases.To address this limitation, we introduce MolBridge, a novel molecule-textlearning framework based on substructure-aware alignments. Specifically, weaugment the original molecule-description pairs with additional alignmentsignals derived from molecular substructures and chemical phrases. Toeffectively learn from these enriched alignments, MolBridge employssubstructure-aware contrastive learning, coupled with a self-refinementmechanism that filters out noisy alignment signals. Experimental results showthat MolBridge effectively captures fine-grained correspondences andoutperforms state-of-the-art baselines on a wide range of molecular benchmarks,highlighting the significance of substructure-aware alignment in molecule-textlearning.</description>
      <author>example@mail.com (Hyuntae Park, Yeachan Kim, SangKeun Lee)</author>
      <guid isPermaLink="false">2510.26157v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Learning Geometry: A Framework for Building Adaptive Manifold Models through Metric Optimization</title>
      <link>http://arxiv.org/abs/2510.26068v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新颖的机器学习范式，超越了传统的参数优化方法。它将模型本身视为可变形的几何实体，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。&lt;h4&gt;背景&lt;/h4&gt;传统的机器学习方法在固定的几何空间内搜索最优参数，而本文提出了一种不同的思路。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的机器学习范式，通过优化度量张量场来动态调整模型的几何结构，从而提高模型的表示能力和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;构建了一个变分框架，其损失函数平衡了数据保真度和流形的内在几何复杂度。为解决这个无限维优化问题的计算挑战，引入了一种基于离散微分几何的实用方法，将连续流形离散化为三角形网格，并通过边长参数化度量张量。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析揭示了该框架与广义相对论中的爱因斯坦-希尔伯特作用之间的深刻类比，为'数据驱动几何'概念提供了优雅的物理解释。即使拓扑结构固定，度量优化也比固定几何的模型具有更强的表达能力。&lt;h4&gt;结论&lt;/h4&gt;这项工作为构建能够自主进化其几何和拓扑结构的完全动态'元学习器'奠定了坚实基础，并在科学模型发现和鲁棒表示学习等领域具有广阔的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种超越传统参数优化的机器学习新范式。与在固定几何空间内搜索最优参数的传统方法不同，我们的核心思想是将模型本身视为可变形的几何实体。具体而言，我们在具有预定义拓扑的流形上优化度量张量场，从而动态塑造模型空间的几何结构。为此，我们构建了一个变分框架，其损失函数仔细平衡了数据保真度与流形的内在几何复杂性。前者确保模型能有效解释观测数据，而后者则作为正则化项，对过度弯曲或不规则的几何结构进行惩罚，以鼓励更简单的模型并防止过拟合。为解决这个无限维优化问题的计算挑战，我们引入了一种基于离散微分几何的实用方法：将连续流形离散化为三角形网格，并通过边长参数化度量张量，从而能够使用自动微分工具进行高效优化。理论分析揭示了我们的框架与广义相对论中爱因斯坦-希尔伯特作用之间的深刻类比，为'数据驱动几何'概念提供了优雅的物理解释。我们进一步认为，即使拓扑结构固定，度量优化也比具有固定几何的模型具有更强的表达能力。这项工作为构建能够自主进化其几何和拓扑结构的完全动态'元学习器'奠定了坚实基础，并指向了科学模型发现和鲁棒表示学习等领域的广泛应用前景。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决传统机器学习方法的局限性：模型在固定的几何空间中寻找最优参数，而非优化模型空间本身的几何结构。当数据的本质几何结构是非欧几里得、弯曲或具有复杂拓扑时，传统方法可能导致效率低下、泛化能力弱或难以解释。这个问题的重要性在于，它限制了模型对数据内在结构的捕捉能力，阻碍了机器学习在处理复杂数据结构时的表现，同时也限制了我们对学习过程本质的理解。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者的思考始于对传统机器学习局限性的反思，提出不仅应优化参数点，还应优化塑造空间的几何结构本身。他借鉴了信息几何的启发，将参数化的概率分布视为微分流形，但超越了传统信息几何中静态度量的观念。作者设计了在固定拓扑流形上优化度量张量场的理论框架，构建了平衡数据保真度和几何复杂性的变分框架，并引入基于离散微分几何的实用计算方法。这项工作借鉴了信息几何、几何深度学习、生成模型和离散微分几何等领域的研究成果，但提出了全新的整合视角。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将机器学习模型视为可塑的几何实体，其学习过程表现为流形自身度量结构的自我优化，通过优化流形上的度量张量场来动态塑造模型空间的几何结构。实现流程包括：1)问题形式化，给定固定拓扑流形和观测数据集；2)设计变分框架，包含数据保真度项和几何复杂性项；3)将连续流形离散化为三角形网格，通过边长参数化度量张量；4)使用基于自动微分的优化算法进行迭代优化，通过投影梯度更新确保满足三角形不等式约束。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)从参数学习到结构学习的范式转变；2)将流形上的度量张量场本身作为优化目标；3)构建平衡数据保真度和几何复杂性的变分框架；4)基于离散微分几何的实用计算方法；5)揭示框架与广义相对论中爱因斯坦-希尔伯特作用的深刻联系。相比之前的工作，本文超越了传统信息几何中静态度量的观念，不同于几何深度学习中的固定几何假设，区别于传统生成模型中预设的简单潜在空间，也不同于传统非线性降维中固定的描述性流形，使几何结构成为学习过程的核心部分。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种机器学习新范式，通过优化流形上的度量张量场使模型能够动态塑造自身的几何结构，为构建完全自适应的'元学习器'奠定了理论基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a novel paradigm for machine learning that moves beyondtraditional parameter optimization. Unlike conventional approaches that searchfor optimal parameters within a fixed geometric space, our core idea is totreat the model itself as a malleable geometric entity. Specifically, weoptimize the metric tensor field on a manifold with a predefined topology,thereby dynamically shaping the geometric structure of the model space. Toachieve this, we construct a variational framework whose loss functioncarefully balances data fidelity against the intrinsic geometric complexity ofthe manifold. The former ensures the model effectively explains observed data,while the latter acts as a regularizer, penalizing overly curved or irregulargeometries to encourage simpler models and prevent overfitting. To address thecomputational challenges of this infinite-dimensional optimization problem, weintroduce a practical method based on discrete differential geometry: thecontinuous manifold is discretized into a triangular mesh, and the metrictensor is parameterized by edge lengths, enabling efficient optimization usingautomatic differentiation tools. Theoretical analysis reveals a profoundanalogy between our framework and the Einstein-Hilbert action in generalrelativity, providing an elegant physical interpretation for the concept of"data-driven geometry". We further argue that even with fixed topology, metricoptimization offers significantly greater expressive power than models withfixed geometry. This work lays a solid foundation for constructing fullydynamic "meta-learners" capable of autonomously evolving their geometry andtopology, and it points to broad application prospects in areas such asscientific model discovery and robust representation learning.</description>
      <author>example@mail.com (Di Zhang)</author>
      <guid isPermaLink="false">2510.26068v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Dual Mixture-of-Experts Framework for Discrete-Time Survival Analysis</title>
      <link>http://arxiv.org/abs/2510.26014v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 workshop Learning from Time Series for  Health (TS4H)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于离散时间生存分析的双重专家混合框架，有效解决了患者异质性建模和风险预测适应性的挑战，在乳腺癌数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;生存分析是建模直到感兴趣事件发生的时间的任务，广泛应用于临床和生物医学研究。主要挑战是如何建模患者异质性，同时使风险预测适应个体特征和时间动态性。&lt;h4&gt;目的&lt;/h4&gt;开发一个双重专家混合框架，用于离散时间生存分析，以有效处理患者异质性和时间动态性。&lt;h4&gt;方法&lt;/h4&gt;提出双重专家混合框架，结合特征编码器专家混合用于亚组感知的表示学习，以及风险专家混合利用患者特征和时间嵌入来捕获时间动态性。该设计可灵活集成到现有的基于深度学习的生存分析管道中。&lt;h4&gt;主要发现&lt;/h4&gt;在METABRIC和GBSG乳腺癌数据集上，该方法持续提高了性能，将时间依赖的C-index提升了最多0.04（在测试集上），当整合到Consurv框架中时获得了进一步的提升。&lt;h4&gt;结论&lt;/h4&gt;双重MoE框架能够有效处理患者异质性并适应时间动态性，在乳腺癌数据集上表现优异，且可以与现有框架结合使用。&lt;h4&gt;翻译&lt;/h4&gt;生存分析是一项建模直到感兴趣事件发生的时间的任务，广泛应用于临床和生物医学研究。一个关键挑战是建模患者异质性，同时使风险预测适应个体特征和时间动态性。我们提出了一个用于离散时间生存分析的双重专家混合框架。我们的方法结合了一个特征编码器专家混合，用于亚组感知的表示学习，以及一个风险专家混合，利用患者特征和时间嵌入来捕获时间动态性。这种双重MoE设计可以灵活地与现有的基于深度学习的生存分析管道集成。在METABRIC和GBSG乳腺癌数据集上，我们的方法持续提高了性能，将时间依赖的C-index提升了最多0.04（在测试集上），当整合到Consurv框架中时获得了进一步的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Survival analysis is a task to model the time until an event of interestoccurs, widely used in clinical and biomedical research. A key challenge is tomodel patient heterogeneity while also adapting risk predictions to bothindividual characteristics and temporal dynamics. We propose a dualmixture-of-experts (MoE) framework for discrete-time survival analysis. Ourapproach combines a feature-encoder MoE for subgroup-aware representationlearning with a hazard MoE that leverages patient features and time embeddingsto capture temporal dynamics. This dual-MoE design flexibly integrates withexisting deep learning based survival pipelines. On METABRIC and GBSG breastcancer datasets, our method consistently improves performance, boosting thetime-dependent C-index up to 0.04 on the test sets, and yields further gainswhen incorporated into the Consurv framework.</description>
      <author>example@mail.com (Hyeonjun Lee, Hyungseob Shin, Gunhee Nam, Hyeonsoo Lee)</author>
      <guid isPermaLink="false">2510.26014v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Predictive Coding Done Right for Mutual Information Estimation</title>
      <link>http://arxiv.org/abs/2510.25983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文指出InfoNCE作为互信息估计器的局限性，并提出改进方法InfoNCE-anchor，通过引入辅助锚点类实现更准确的互信息估计。研究还揭示了对比表示学习的真正价值在于学习结构化密度比，而非准确估计互信息。&lt;h4&gt;背景&lt;/h4&gt;InfoNCE目标最初用于对比表示学习，已成为互信息(MI)估计的流行选择，尽管它与MI的联系是间接的。&lt;h4&gt;目的&lt;/h4&gt;证明为什么InfoNCE不应被视为有效的MI估计器，并引入一个称为InfoNCE-anchor的简单修改，用于准确的MI估计。&lt;h4&gt;方法&lt;/h4&gt;通过引入一个辅助锚点类来修改InfoNCE，实现了一致的密度比估计，并产生了一个偏差显著减少的即插即用MI估计器。此外，使用适当的评分规则将框架推广，当使用对数评分时，InfoNCE-anchor作为特例被恢复。这个公式将一系列对比目标（包括NCE、InfoNCE和f-散度变体）统一在一个单一的原则性框架下。&lt;h4&gt;主要发现&lt;/h4&gt;使用对数评分的InfoNCE-anchor能实现最准确的MI估计；然而，在自监督表示学习实验中，锚点并未提高下游任务性能。&lt;h4&gt;结论&lt;/h4&gt;对比表示学习受益的不是准确的MI估计本身，而是结构化密度比的学习。&lt;h4&gt;翻译&lt;/h4&gt;InfoNCE目标最初引入用于对比表示学习，尽管它与互信息(MI)的联系是间接的，但已成为MI估计的流行选择。在本文中，我们证明了为什么不应将InfoNCE视为有效的MI估计器，并介绍了一个简单的修改，我们称之为InfoNCE-anchor，用于准确的MI估计。我们的修改引入了一个辅助锚点类，实现了一致的密度比估计，并产生了一个偏差显著减少的即插即用MI估计器。除此之外，我们使用适当的评分规则推广了我们的框架，当使用对数评分时，InfoNCE-anchor作为特例被恢复。这个公式在单一原则性框架下统一了广泛的对比目标，包括NCE、InfoNCE和f-散度变体。从经验上看，我们发现使用对数评分的InfoNCE-anchor实现了最准确的MI估计；然而，在自监督表示学习实验中，我们发现锚点并未提高下游任务性能。这些发现证实了对比表示学习受益的不是准确的MI估计本身，而是结构化密度比的学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The InfoNCE objective, originally introduced for contrastive representationlearning, has become a popular choice for mutual information (MI) estimation,despite its indirect connection to MI. In this paper, we demonstrate whyInfoNCE should not be regarded as a valid MI estimator, and we introduce asimple modification, which we refer to as InfoNCE-anchor, for accurate MIestimation. Our modification introduces an auxiliary anchor class, enablingconsistent density ratio estimation and yielding a plug-in MI estimator withsignificantly reduced bias. Beyond this, we generalize our framework usingproper scoring rules, which recover InfoNCE-anchor as a special case when thelog score is employed. This formulation unifies a broad spectrum of contrastiveobjectives, including NCE, InfoNCE, and $f$-divergence variants, under a singleprincipled framework. Empirically, we find that InfoNCE-anchor with the logscore achieves the most accurate MI estimates; however, in self-supervisedrepresentation learning experiments, we find that the anchor does not improvethe downstream task performance. These findings corroborate that contrastiverepresentation learning benefits not from accurate MI estimation per se, butfrom the learning of structured density ratios.</description>
      <author>example@mail.com (J. Jon Ryu, Pavan Yeddanapudi, Xiangxiang Xu, Gregory W. Wornell)</author>
      <guid isPermaLink="false">2510.25983v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>HiMAE: Hierarchical Masked Autoencoders Discover Resolution-Specific Structure in Wearable Time Series</title>
      <link>http://arxiv.org/abs/2510.25785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为HiMAE的自监督学习方法，用于分析可穿戴传感器产生的生理时间序列数据，探索时间分辨率对预测效用的影响。&lt;h4&gt;背景&lt;/h4&gt;可穿戴传感器提供了丰富的生理时间序列数据，但支配这些数据预测效用的基本原理尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;测试时间分辨率作为表征学习基本轴的假设，即不同的临床和行为结果依赖于不同尺度的结构。&lt;h4&gt;方法&lt;/h4&gt;引入HiMAE（分层掩码自编码器），一种结合掩码自编码和分层卷积编码器-解码器的自监督框架。&lt;h4&gt;主要发现&lt;/h4&gt;HiMAE能产生多分辨率嵌入，系统评估哪些时间尺度携带预测信号；在分类、回归和生成基准测试中优于最先进模型；体积小几个数量级；可在智能手表上实现亚毫秒级推理，支持真正的边缘计算。&lt;h4&gt;结论&lt;/h4&gt;HiMAE既是高效的自监督学习方法，也是发现可穿戴健康数据中尺度敏感结构的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感器提供了丰富的生理时间序列，但支配其预测效用的原理仍然不清楚。我们假设时间分辨率是表征学习的基本轴，不同的临床和行为结果依赖于不同尺度的结构。为了测试这一分辨率假设，我们引入了HiMAE（分层掩码自编码器），这是一种结合掩码自编码和分层卷积编码器-解码器的自监督框架。HiMAE产生多分辨率嵌入，能够系统评估哪些时间尺度携带预测信号，将分辨率从超参数转变为可解释性的探针。在分类、回归和生成基准测试中，HiMAE始终优于将尺度压缩的最先进基础模型，同时体积小几个数量级。HiMAE是一种高效的表征学习器，足够紧凑，可以在手表上完全运行，在智能手表类CPU上实现亚毫秒级推理，实现真正的边缘推理。总之，这些贡献使HiMAE既成为高效的自监督学习方法，也成为发现可穿戴健康中尺度敏感结构的发现工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensors provide abundant physiological time series, yet theprinciples governing their predictive utility remain unclear. We hypothesizethat temporal resolution is a fundamental axis of representation learning, withdifferent clinical and behavioral outcomes relying on structure at distinctscales. To test this resolution hypothesis, we introduce HiMAE (HierarchicalMasked Autoencoder), a self supervised framework that combines maskedautoencoding with a hierarchical convolutional encoder decoder. HiMAE producesmulti resolution embeddings that enable systematic evaluation of which temporalscales carry predictive signal, transforming resolution from a hyperparameterinto a probe for interpretability. Across classification, regression, andgenerative benchmarks, HiMAE consistently outperforms state of the artfoundation models that collapse scale, while being orders of magnitude smaller.HiMAE is an efficient representation learner compact enough to run entirely onwatch, achieving sub millisecond inference on smartwatch class CPUs for trueedge inference. Together, these contributions position HiMAE as both anefficient self supervised learning method and a discovery tool for scalesensitive structure in wearable health.</description>
      <author>example@mail.com (Simon A. Lee, Cyrus Tanade, Hao Zhou, Juhyeon Lee, Megha Thukral, Minji Han, Rachel Choi, Md Sazzad Hissain Khan, Baiying Lu, Migyeong Gwak, Mehrab Bin Morshed, Viswam Nathan, Md Mahbubur Rahman, Li Zhu, Subramaniam Venkatraman, Sharanya Arcot Desai)</author>
      <guid isPermaLink="false">2510.25785v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Autograder+: A Multi-Faceted AI Framework for Rich Pedagogical Feedback in Programming Education</title>
      <link>http://arxiv.org/abs/2510.26402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Autograder+是一个创新的自动评分系统，通过AI驱动的反馈、语义聚类和交互式可视化，将自动评分从纯总结性过程转变为形成性学习体验，减轻教师工作量的同时支持有针对性的教学并促进更强的学习成果。&lt;h4&gt;背景&lt;/h4&gt;编程教育的快速增长已经超过了传统评估工具的发展，使教师难以提供有意义、可扩展的反馈。传统的自动评分器虽然高效，但作为黑盒系统仅返回通过/失败结果，很少提供关于学生思维或学习需求的见解。&lt;h4&gt;目的&lt;/h4&gt;将自动评分从纯总结性过程转变为形成性学习体验，为教师提供更有效的评估工具，同时为学生提供更有价值的反馈。&lt;h4&gt;方法&lt;/h4&gt;引入两个关键功能：1) 使用微调的大语言模型自动生成反馈；2) 可视化学生代码提交以发现学习模式。模型经过精心筛选的学生代码和专家反馈进行微调，确保教育对齐和上下文感知的指导。系统支持提示池，允许教师通过选择的提示模板指导反馈风格。&lt;h4&gt;主要发现&lt;/h4&gt;在来自多个编程任务的600多个学生提交的评估中，系统生成的反馈与教师评论具有很强的语义一致性。基于1000个带注释的提交训练的对比学习代码嵌入能够基于功能和方法的相似性将解决方案分组为有意义的集群。&lt;h4&gt;结论&lt;/h4&gt;通过整合AI驱动的反馈、语义聚类和交互式可视化，Autograder+减轻了教师的工作量，同时支持有针对性的教学并促进更强的学习成果。&lt;h4&gt;翻译&lt;/h4&gt;编程教育的快速增长已经超过了传统评估工具的发展，使教师难以提供有意义、可扩展的反馈。传统的自动评分器虽然高效，但作为黑盒系统仅返回通过/失败结果，很少提供关于学生思维或学习需求的见解。Autograder+旨在将自动评分从纯总结性过程转变为形成性学习体验。它引入了两个关键功能：使用微调的大语言模型自动生成反馈，以及可视化学生代码提交以发现学习模式。该模型经过精心筛选的学生代码和专家反馈进行微调，确保教育对齐、上下文感知的指导。在来自多个编程任务的600多个学生提交的评估中，系统生成的反馈与教师评论具有很强的语义一致性。对于可视化功能，基于1000个带注释的提交训练的对比学习代码嵌入能够基于功能和方法的相似性将解决方案分组为有意义的集群。该系统还支持提示池，允许教师通过选择的提示模板指导反馈风格。通过整合AI驱动的反馈、语义聚类和交互式可视化，Autograder+减轻了教师的工作量，同时支持有针对性的教学并促进更强的学习成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of programming education has outpaced traditional assessmenttools, leaving faculty with limited means to provide meaningful, scalablefeedback. Conventional autograders, while efficient, act as black-box systemsthat simply return pass/fail results, offering little insight into studentthinking or learning needs.  Autograder+ is designed to shift autograding from a purely summative processto a formative learning experience. It introduces two key capabilities:automated feedback generation using a fine-tuned Large Language Model, andvisualization of student code submissions to uncover learning patterns. Themodel is fine-tuned on curated student code and expert feedback to ensurepedagogically aligned, context-aware guidance.  In evaluation across 600 student submissions from multiple programming tasks,the system produced feedback with strong semantic alignment to instructorcomments. For visualization, contrastively learned code embeddings trained on1,000 annotated submissions enable grouping solutions into meaningful clustersbased on functionality and approach. The system also supports prompt-pooling,allowing instructors to guide feedback style through selected prompt templates.  By integrating AI-driven feedback, semantic clustering, and interactivevisualization, Autograder+ reduces instructor workload while supportingtargeted instruction and promoting stronger learning outcomes.</description>
      <author>example@mail.com (Vikrant Sahu, Gagan Raj Gupta, Raghav Borikar, Nitin Mane)</author>
      <guid isPermaLink="false">2510.26402v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Which Way Does Time Flow? A Psychophysics-Grounded Evaluation for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.26241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究揭示了当前视觉语言模型在视频时间信息理解上的显著缺陷，提出了一个简单但有效的评估方法——时间方向判断(AoT)。&lt;h4&gt;背景&lt;/h4&gt;现代视觉语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的理解仍然薄弱，并且这一点尚未得到充分评估。&lt;h4&gt;目的&lt;/h4&gt;探究视觉语言模型在理解视频时间信息方面的差距，通过判断视频播放方向(正向或反向)的挑战来评估其时间推理能力。&lt;h4&gt;方法&lt;/h4&gt;引入AoT-PsyPhyBENCH基准测试，使用经过心理物理学验证的刺激和行为基线，测试VLMs能否推断自然视频中的时间方向，并对开源和专有模型进行全面评估。&lt;h4&gt;主要发现&lt;/h4&gt;大多数模型表现接近随机水平，即使在物理不可逆过程和因果手动操作上表现最好的模型也远低于人类准确度，而人类几乎能立即识别这些内容。&lt;h4&gt;结论&lt;/h4&gt;当前多模态系统存在基本差距：虽然捕捉了丰富的视觉语义相关性，但缺乏时间连续性和因果理解所需的归纳偏置。&lt;h4&gt;翻译&lt;/h4&gt;现代视觉语言模型在许多多模态任务中表现出色，但它们对视频中时间信息的理解仍然薄弱，且这一点尚未得到充分评估。我们通过一个看似简单但具有揭示性的挑战来探究这一差距：判断时间方向(AoT)，即判断短视频片段是正向播放还是反向播放。我们引入了AoT-PsyPhyBENCH，这是一个经过心理物理学验证的基准测试，用于测试VLMs能否使用与人类相同的刺激和行为基线来推断自然视频中的时间方向。我们对开源和专有的、推理和非推理的VLMs进行了全面评估，发现大多数模型表现接近随机水平，即使在物理不可逆过程(如自由落体、扩散/爆炸)和因果手动操作(分割/添加)上表现最好的模型也远低于人类的准确度，而人类几乎能立即识别这些内容。这些结果突显了当前多模态系统的一个基本差距：虽然它们捕捉了丰富的视觉语义相关性，但缺乏时间连续性和因果理解所需的归纳偏置。我们发布了AoT-PsyPhyBENCH的代码和数据，以鼓励VLMs在物理和时间推理能力方面的进一步发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern vision-language models (VLMs) excel at many multimodal tasks, yettheir grasp of temporal information in video remains weak and, crucially,under-evaluated. We probe this gap with a deceptively simple but revealingchallenge: judging the arrow of time (AoT)-whether a short clip is playedforward or backward. We introduce AoT-PsyPhyBENCH, a psychophysically validatedbenchmark that tests whether VLMs can infer temporal direction in naturalvideos using the same stimuli and behavioral baselines established for humans.Our comprehensive evaluation of open-weight and proprietary, reasoning andnon-reasoning VLMs reveals that most models perform near chance, and even thebest lag far behind human accuracy on physically irreversible processes (e.g.,free fall, diffusion/explosion) and causal manual actions (division/addition)that humans recognize almost instantly. These results highlight a fundamentalgap in current multimodal systems: while they capture rich visual-semanticcorrelations, they lack the inductive biases required for temporal continuityand causal understanding. We release the code and data for AoT-PsyPhyBENCH toencourage further progress in the physical and temporal reasoning capabilitiesof VLMs.</description>
      <author>example@mail.com (Shiho Matta, Lis Kanashiro Pereira, Peitao Han, Fei Cheng, Shigeru Kitazawa)</author>
      <guid isPermaLink="false">2510.26241v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>STAR: A Privacy-Preserving, Energy-Efficient Edge AI Framework for Human Activity Recognition via Wi-Fi CSI in Mobile and Pervasive Computing Environments</title>
      <link>http://arxiv.org/abs/2510.26148v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为STAR的边缘AI优化框架，用于在低功耗嵌入式设备上实现实时、节能的人类活动识别(HAR)系统。该系统通过简化的神经网络、自适应信号处理和硬件感知优化，在保持高准确率的同时显著提高了计算效率。&lt;h4&gt;背景&lt;/h4&gt;人类活动识别(HAR)通过Wi-Fi信道状态信息(CSI)提供了一种保护隐私、无需接触的感知方法，适用于智能家居、健康监测和移动物联网系统。然而，现有方法存在计算效率低下、高延迟和资源受限环境中可行性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;开发STAR(Sensing Technology for Activity Recognition)框架，在低功耗嵌入式设备上实现实时、节能的HAR，解决现有方法在资源受限环境中的局限性。&lt;h4&gt;方法&lt;/h4&gt;集成轻量级神经网络架构、自适应信号处理和硬件感知联合优化；使用简化的基于门控循环单元(GRU)的循环神经网络，比传统LSTM减少33%参数；采用多阶段预处理管道(中值滤波、8阶巴特沃斯低通滤波和经验模态分解)；在配备NPU的Rockchip RV1126处理器上实现，并与ESP32-S3 CSI采集模块接口。&lt;h4&gt;主要发现&lt;/h4&gt;在七类活动上的平均识别准确率达93.52%，人体存在检测准确率达99.11%；使用仅97.6k参数的紧凑模型；INT8量化推理以33 MHz速度运行，仅占用8% CPU利用率，比CPU执行快六倍；系统具有亚秒级响应延迟和低功耗。&lt;h4&gt;结论&lt;/h4&gt;STAR系统确保了实时、隐私保护的HAR，为移动和普适计算环境提供了实用、可扩展的解决方案，有效解决了传统方法在资源受限嵌入式环境中的局限性。&lt;h4&gt;翻译&lt;/h4&gt;人类活动识别(HAR)通过Wi-Fi信道状态信息(CSI)提供了一种保护隐私、无需接触的感知方法，适用于智能家居、健康监测和移动物联网系统。然而，现有方法常遇到计算效率低下、高延迟以及在资源受限的嵌入式移动边缘环境中可行性有限的问题。本文提出了STAR(Sensing Technology for Activity Recognition)，这是一个边缘AI优化的框架，集成了轻量级神经网络架构、自适应信号处理和硬件感知的联合优化，以在低功耗嵌入式设备上实现实时、节能的HAR。STAR采用简化的基于门控循环单元(GRU)的循环神经网络，比传统LSTM模型减少33%的模型参数，同时保持有效的时间建模能力。采用结合中值滤波、8阶巴特沃斯低通滤波和经验模态分解(EMD)的多阶段预处理管道，用于去噪CSI振幅数据并提取时空特征。对于设备上部署，STAR在配备嵌入式神经处理单元(NPU)的Rockchip RV1126处理器上实现，与基于ESP32-S3的CSI采集模块接口。实验结果显示，在七类活动上的平均识别准确率为93.52%，人体存在检测为99.11%，使用紧凑的97.6k参数模型。INT8量化推理以33 MHz的处理速度运行，仅占用8%的CPU利用率，比基于CPU的执行速度快六倍。凭借亚秒级响应延迟和低功耗，该系统确保了实时、隐私保护的HAR，为移动和普适计算环境提供了实用、可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human Activity Recognition (HAR) via Wi-Fi Channel State Information (CSI)presents a privacy-preserving, contactless sensing approach suitable for smarthomes, healthcare monitoring, and mobile IoT systems. However, existing methodsoften encounter computational inefficiency, high latency, and limitedfeasibility within resource-constrained, embedded mobile edge environments.This paper proposes STAR (Sensing Technology for Activity Recognition), anedge-AI-optimized framework that integrates a lightweight neural architecture,adaptive signal processing, and hardware-aware co-optimization to enablereal-time, energy-efficient HAR on low-power embedded devices. STARincorporates a streamlined Gated Recurrent Unit (GRU)-based recurrent neuralnetwork, reducing model parameters by 33% compared to conventional LSTM modelswhile maintaining effective temporal modeling capability. A multi-stagepre-processing pipeline combining median filtering, 8th-order Butterworthlow-pass filtering, and Empirical Mode Decomposition (EMD) is employed todenoise CSI amplitude data and extract spatial-temporal features. For on-devicedeployment, STAR is implemented on a Rockchip RV1126 processor equipped with anembedded Neural Processing Unit (NPU), interfaced with an ESP32-S3-based CSIacquisition module. Experimental results demonstrate a mean recognitionaccuracy of 93.52% across seven activity classes and 99.11% for human presencedetection, utilizing a compact 97.6k-parameter model. INT8 quantized inferenceachieves a processing speed of 33 MHz with just 8% CPU utilization, deliveringsixfold speed improvements over CPU-based execution. With sub-second responselatency and low power consumption, the system ensures real-time,privacy-preserving HAR, offering a practical, scalable solution for mobile andpervasive computing environments.</description>
      <author>example@mail.com (Kexing Liu)</author>
      <guid isPermaLink="false">2510.26148v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>EgoExo-Con: Exploring View-Invariant Video Temporal Understanding</title>
      <link>http://arxiv.org/abs/2510.26113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  project page:  \url{https://minjoong507.github.io/projects/EgoExo-Con/}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视频大语言模型在不同视角下捕捉同一事件时的时间理解一致性问题，提出了EgoExo-Con基准测试和View-GRPO强化学习框架来解决现有模型的局限性。&lt;h4&gt;背景&lt;/h4&gt;视频大语言模型在不同视角下捕捉同一事件时可能存在时间理解不一致的问题。&lt;h4&gt;目的&lt;/h4&gt;研究视频大语言模型在不同视角下捕捉同一事件时是否能保持一致的时间理解能力，并提出改进方法。&lt;h4&gt;方法&lt;/h4&gt;引入EgoExo-Con基准测试，包含全面同步的第一人称和第三人称视角视频对及人类优化的自然语言查询，强调时间验证和时间定位两个任务，并提出View-GRPO强化学习框架来增强特定视角的时间推理并促进跨视角一致性理解。&lt;h4&gt;主要发现&lt;/h4&gt;现有Video-LLMs存在两个关键限制：(1)模型通常无法保持一致性，结果远差于单视角表现；(2)当使用双视角同步视频微调时，模型显示出一致性改进，但表现往往不如单视角训练的模型。&lt;h4&gt;结论&lt;/h4&gt;View-GRPO方法在提高跨视角一致性方面优于简单的SFT和GRPO，所有研究资源将公开可用。&lt;h4&gt;翻译&lt;/h4&gt;当视频从不同视角捕捉同一事件时，视频大语言模型能否实现一致的时间理解？为研究此问题，我们引入了EgoExo-Con(一致性)基准，该基准包含全面同步的第一人称和第三人称视频对以及人类优化的自然语言查询。EgoExo-Con强调两个时间理解任务：时间验证和时间定位。它不仅评估正确性，还评估跨视角的一致性。我们的分析揭示了现有Video-LLMs的两个关键局限：(1)模型通常无法保持一致性，结果远差于其单视角表现。(2)当使用双视角同步视频进行微调时，模型显示出一致性改进，但往往表现不如单视角训练的模型。为改进，我们提出了View-GRPO，一种新的强化学习框架，能有效增强特定视角的时间推理，同时鼓励跨视角的一致性理解。我们的方法在提高跨视角一致性方面优于简单的SFT和GRPO。所有资源将公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Can Video-LLMs achieve consistent temporal understanding when videos capturethe same event from different viewpoints? To study this, we introduceEgoExo-Con (Consistency), a benchmark of comprehensively synchronizedegocentric and exocentric video pairs with human-refined queries in naturallanguage. EgoExo-Con emphasizes two temporal understanding tasks: TemporalVerification and Temporal Grounding. It evaluates not only correctness butconsistency across viewpoints. Our analysis reveals two critical limitations ofexisting Video-LLMs: (1) models often fail to maintain consistency, withresults far worse than their single-view performances. (2) When naivelyfinetuned with synchronized videos of both viewpoints, the models show improvedconsistency but often underperform those trained on a single view. Forimprovements, we propose View-GRPO, a novel reinforcement learning frameworkthat effectively strengthens view-specific temporal reasoning while encouragingconsistent comprehension across viewpoints. Our method demonstrates itssuperiority over naive SFT and GRPO, especially for improving cross-viewconsistency. All resources will be made publicly available.</description>
      <author>example@mail.com (Minjoon Jung, Junbin Xiao, Junghyun Kim, Byoung-Tak Zhang, Angela Yao)</author>
      <guid isPermaLink="false">2510.26113v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Temporal Understanding in Video-LLMs through Stacked Temporal Attention in Vision Encoders</title>
      <link>http://arxiv.org/abs/2510.26027v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种改进的视频大语言模型架构，通过在视觉编码器中引入堆叠时序注意力模块，解决了当前模型在理解视频时序动态方面的局限性，显著提升了时序推理能力和动作识别性能。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型(MLLMs)已取得显著进展，但理解视频中复杂的时序动态仍然是一个主要挑战。当前视频大语言模型(Video-LLM)架构在时序理解方面存在关键局限性，难以处理需要详细理解动作序列和时间进展的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的Video-LLM架构，增强模型对视频时序动态的理解能力，特别是在动作序列和时间进展方面的理解。&lt;h4&gt;方法&lt;/h4&gt;在视觉编码器中直接引入堆叠的时序注意力模块，在视觉编码器中融入时序注意力，使模型能够更好地捕捉动作进展和帧之间的关系，在将视觉令牌传递给LLM之前增强时序理解。&lt;h4&gt;主要发现&lt;/h4&gt;该方法显著提高了时序推理能力，在视频问答任务中优于现有模型，特别是在动作识别方面。在VITATECS、MVBench和Video-MME等基准测试中，性能提升了高达5.5%。&lt;h4&gt;结论&lt;/h4&gt;通过增强视觉编码器的时序结构，解决了Video-LLMs在视频理解方面的关键差距，为视频时序理解提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型(MLLMs)取得了显著进展，但理解视频中复杂的时序动态仍然是一个主要挑战。我们的实验表明，当前视频大语言模型(Video-LLM)架构在时序理解方面存在关键局限性，难以处理需要详细理解动作序列和时间进展的任务。在这项工作中，我们提出了一种Video-LLM架构，在视觉编码器中直接引入堆叠的时序注意力模块。这种设计在视觉编码器中融入了时序注意力，使模型能够在将视觉令牌传递给LLM之前更好地捕捉动作进展和帧之间的关系。我们的结果表明，这种方法显著提高了时序推理能力，并在视频问答任务中优于现有模型，特别是在动作识别方面。我们在VITATECS、MVBench和Video-MME等基准测试中提高了高达5.5%。通过增强视觉编码器的时序结构，我们解决了Video-LLMs在视频理解方面的关键差距。项目页面和代码可在以下网址获取：https://alirasekh.github.io/STAVEQ2/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite significant advances in Multimodal Large Language Models (MLLMs),understanding complex temporal dynamics in videos remains a major challenge.Our experiments show that current Video Large Language Model (Video-LLM)architectures have critical limitations in temporal understanding, strugglingwith tasks that require detailed comprehension of action sequences and temporalprogression. In this work, we propose a Video-LLM architecture that introducesstacked temporal attention modules directly within the vision encoder. Thisdesign incorporates a temporal attention in vision encoder, enabling the modelto better capture the progression of actions and the relationships betweenframes before passing visual tokens to the LLM. Our results show that thisapproach significantly improves temporal reasoning and outperforms existingmodels in video question answering tasks, specifically in action recognition.We improve on benchmarks including VITATECS, MVBench, and Video-MME by up to+5.5%. By enhancing the vision encoder with temporal structure, we address acritical gap in video understanding for Video-LLMs. Project page and code areavailable at: https://alirasekh.github.io/STAVEQ2/.</description>
      <author>example@mail.com (Ali Rasekh, Erfan Bagheri Soula, Omid Daliran, Simon Gottschalk, Mohsen Fayyaz)</author>
      <guid isPermaLink="false">2510.26027v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>From Queries to Insights: Agentic LLM Pipelines for Spatio-Temporal Text-to-SQL</title>
      <link>http://arxiv.org/abs/2510.25997v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, GeoGenAgent'25 - ACM SIGSPATIAL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于代理的管道系统，用于处理复杂的空间和时间自然语言查询，显著提高了查询准确性和用户友好性。&lt;h4&gt;背景&lt;/h4&gt;现有的自然语言到SQL系统在处理真实空间和时间查询时存在困难，需要将模糊的用户表述与特定模式类别匹配、处理时间推理并选择适当输出。&lt;h4&gt;目的&lt;/h4&gt;开发能够处理复杂空间和时间查询的系统，支持缺乏SQL专业知识、详细模式知识或提示技能的用户。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于代理的管道，通过基于Mistral的ReAct代理对基础文本到SQL模型(llama-3-sqlcoder-8b)进行编排，使代理能够通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。&lt;h4&gt;主要发现&lt;/h4&gt;在纽约和东京签到数据集的35个自然语言查询评估中，代理系统准确率达到91.4%，而基础模型仅为28.6%，并通过地图、图表和结构化的自然语言摘要显著增强了可用性。&lt;h4&gt;结论&lt;/h4&gt;代理编排而非更强的SQL生成器本身是构建交互式地理空间助手的有前途的基础。&lt;h4&gt;翻译&lt;/h4&gt;自然语言到SQL系统有望使结构化数据访问民主化，使用户无需学习SQL即可查询数据库。然而，现有系统在处理现实空间时间查询方面存在困难，成功需要将模糊的用户表述与特定模式类别对齐、处理时间推理并选择适当输出。我们提出了一种基于代理的管道，通过基于Mistral的ReAct代理的编排，扩展了一个基础文本到SQL模型(llama-3-sqlcoder-8b)。该代理可以通过模式检查、SQL生成、执行和可视化工具来规划、分解和调整查询。我们在纽约和东京签到数据集上的35个自然语言查询进行了评估，涵盖了空间、时间和多数据集推理。代理的准确率显著高于基础模型，达到91.4%对28.6%，并通过地图、图表和结构化的自然语言摘要增强了可用性。关键的是，我们的设计支持了更自然的人机数据库交互，支持缺乏SQL专业知识、详细模式知识或提示技能的用户。我们得出结论，代理编排而非更强的SQL生成器本身，是交互式地理空间助手的有前途的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3764915.3770724&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Natural-language-to-SQL (NL-to-SQL) systems hold promise for democratizingaccess to structured data, allowing users to query databases without learningSQL. Yet existing systems struggle with realistic spatio-temporal queries,where success requires aligning vague user phrasing with schema-specificcategories, handling temporal reasoning, and choosing appropriate outputs. Wepresent an agentic pipeline that extends a naive text-to-SQL baseline(llama-3-sqlcoder-8b) with orchestration by a Mistral-based ReAct agent. Theagent can plan, decompose, and adapt queries through schema inspection, SQLgeneration, execution, and visualization tools. We evaluate on 35natural-language queries over the NYC and Tokyo check-in dataset, coveringspatial, temporal, and multi-dataset reasoning. The agent achievessubstantially higher accuracy than the naive baseline 91.4% vs. 28.6% andenhances usability through maps, plots, and structured natural-languagesummaries. Crucially, our design enables more natural human-databaseinteraction, supporting users who lack SQL expertise, detailed schemaknowledge, or prompting skill. We conclude that agentic orchestration, ratherthan stronger SQL generators alone, is a promising foundation for interactivegeospatial assistants.</description>
      <author>example@mail.com (Manu Redd, Tao Zhe, Dongjie Wang)</author>
      <guid isPermaLink="false">2510.25997v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Underwater Object Detection through Spatio-Temporal Analysis and Spatial Attention Networks</title>
      <link>http://arxiv.org/abs/2510.25797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究评估了时空建模和空间注意力机制在水下物体检测中的有效性，比较了标准YOLOv5、T-YOLOv5及其与CBAM结合的变体性能。&lt;h4&gt;背景&lt;/h4&gt;水下物体检测在动态海洋环境中面临挑战，如突然运动、部分遮挡和逐渐运动等。&lt;h4&gt;目的&lt;/h4&gt;研究时空建模和空间注意力机制如何提高水下物体检测的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;分两个阶段进行，第一阶段评估T-YOLOv5与标准YOLOv5的性能比较；第二阶段开发添加了卷积块注意力模块(CBAM)的T-YOLOv5增强版本。&lt;h4&gt;主要发现&lt;/h4&gt;T-YOLOv5和T-YOLOv5与CBAM的变体在mAP@50-95指标上分别达到0.813和0.811，显著优于标准YOLOv5的0.563。&lt;h4&gt;结论&lt;/h4&gt;T-YOLOv5相比标准模型显著提高了检测可靠性，而T-YOLOv5与CBAM在具有挑战性的场景中进一步提高了性能，但在简单场景中会损失一些准确性。&lt;h4&gt;翻译&lt;/h4&gt;该研究检验了时空建模和空间注意力机制在深度学习模型中用于水下物体检测的有效性。具体而言，在第一阶段，评估了增强时序的YOLOv5变体T-YOLOv5与标准YOLOv5的性能比较。在第二阶段，通过添加卷积块注意力模块(CBAM)开发了T-YOLOv5的增强版本。研究表明，CBAM如何通过时序建模提高了在动态海洋环境中的检测准确性，特别是在突然运动、部分遮挡和逐渐运动的条件下。测试结果显示，YOLOv5达到了0.563的mAP@50-95，而T-YOLOv5和带有CBAM的T-YOLOv5分别以0.813和0.811的mAP@50-95分数表现更优，突显了它们在检测复杂物体方面的卓越准确性和泛化能力。研究结果表明，与标准模型相比，T-YOLOv5显著提高了检测可靠性，而带有CBAM的T-YOLOv5在具有挑战性的场景中进一步提高了性能，尽管在简单场景中会损失一些准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study examines the effectiveness of spatio-temporal modeling and theintegration of spatial attention mechanisms in deep learning models forunderwater object detection. Specifically, in the first phase, the performanceof temporal-enhanced YOLOv5 variant T-YOLOv5 is evaluated, in comparison withthe standard YOLOv5. For the second phase, an augmented version of T-YOLOv5 isdeveloped, through the addition of a Convolutional Block Attention Module(CBAM). By examining the effectiveness of the already pre-existing YOLOv5 andT-YOLOv5 models and of the newly developed T-YOLOv5 with CBAM. With CBAM, theresearch highlights how temporal modeling improves detection accuracy indynamic marine environments, particularly under conditions of sudden movements,partial occlusions, and gradual motion. The testing results showed that YOLOv5achieved a mAP@50-95 of 0.563, while T-YOLOv5 and T-YOLOv5 with CBAMoutperformed with mAP@50-95 scores of 0.813 and 0.811, respectively,highlighting their superior accuracy and generalization in detecting complexobjects. The findings demonstrate that T-YOLOv5 significantly enhancesdetection reliability compared to the standard model, while T-YOLOv5 with CBAMfurther improves performance in challenging scenarios, although there is a lossof accuracy when it comes to simpler scenarios.</description>
      <author>example@mail.com (Sai Likhith Karri, Ansh Saxena)</author>
      <guid isPermaLink="false">2510.25797v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Context-Aware Scene Reasoning Using Vision-Language Alignment in Zero-Shot Real-World Scenarios</title>
      <link>http://arxiv.org/abs/2510.26580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint under review at IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种动态上下文感知场景推理框架，利用视觉语言对齐解决零样本现实世界场景问题，通过结合视觉Transformer和大语言模型显著提高了复杂环境中的场景理解准确性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界环境中，AI系统经常面临没有标记数据的陌生场景，这给传统场景理解模型带来重大挑战。无法在未见过的上下文中进行泛化限制了基于视觉的应用程序在动态、非结构化环境中的部署。&lt;h4&gt;目的&lt;/h4&gt;使智能系统能够在没有特定任务先验训练的情况下推断并适应新环境。&lt;h4&gt;方法&lt;/h4&gt;提出的方法集成了预训练的视觉Transformer和大语言模型，将视觉语义与自然语言描述对齐增强上下文理解能力。动态推理模块通过结合全局场景线索和由语言先验引导的对象级交互来优化预测。&lt;h4&gt;主要发现&lt;/h4&gt;在COCO、Visual Genome和Open Images等零样本基准上的实验表明，在复杂且未见过的环境中，场景理解准确性比基线模型提高了高达18%。由于视觉和语言的协同融合，系统在模糊或杂乱的场景中表现出强大的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架为上下文感知推理提供了一种可扩展和可解释的方法，推动了动态现实世界环境中的零样本泛化。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界环境中，AI系统经常面临没有标记数据的陌生场景，这给传统的场景理解模型带来了重大挑战。无法在未见过的上下文中进行泛化限制了基于视觉的应用程序在动态、非结构化环境中的部署。这项工作引入了一种动态上下文感知场景推理框架，利用视觉语言对齐来解决零样本现实世界场景问题。目标是使智能系统能够在没有特定任务先验训练的情况下推断并适应新环境。提出的方法集成了预训练的视觉Transformer和大语言模型，将视觉语义与自然语言描述对齐，增强上下文理解能力。动态推理模块通过结合全局场景线索和由语言先验引导的对象级交互来优化预测。在COCO、Visual Genome和Open Images等零样本基准上的广泛实验表明，在复杂且未见过的环境中，场景理解准确性比基线模型提高了高达18%。结果还显示，由于视觉和语言的协同融合，在模糊或杂乱的场景中表现出强大的性能。该框架为上下文感知推理提供了一种可扩展和可解释的方法，推动了动态现实世界环境中的零样本泛化。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决AI系统在零样本(real-world)场景下进行动态上下文感知场景推理的问题。传统场景理解模型在面对没有标记数据的新环境时无法有效泛化，这限制了视觉应用在自动驾驶、机器人导航、监控等动态、非结构化环境中的部署。这个问题在现实中非常重要，因为真实世界环境往往是复杂、多变且缺乏标记数据的，AI系统需要能够理解和适应从未见过的新场景而不需要针对每个特定任务重新训练。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过整合预训练的视觉转换器(visual transformers)和大语言模型(large language models)来设计这个方法。他们借鉴了多项现有工作，包括Context VLM用于自动驾驶安全、结合开放世界检测器和大视觉语言模型的零样本目标识别、基于图的视觉语言模型调整方法、利用CLIP模型的动态场景恢复框架等。作者在这些工作的基础上，提出了一个动态推理模块，该模块利用全局场景线索和对象级交互，由语言先验指导，从而实现更有效的场景理解。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过视觉-语言对齐来实现动态上下文感知的场景推理，使AI系统能够在没有任务特定监督的情况下理解和适应新环境。整体实现流程包括：1)使用视觉编码器(如CLIP-ViT)提取丰富的视觉特征；2)使用语言编码器(如GPT或BERT变体)建模语义先验；3)采用基于注意力的跨模态融合机制对齐视觉和语言；4)引入上下文精炼单元，建模对象级交互和全局场景语义；5)在零样本场景下评估模型性能，通过计算视觉和文本嵌入的余弦相似度来进行场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出动态上下文感知场景推理框架，整合预训练视觉转换器和大语言模型；2)引入动态推理模块，利用全局场景线索和对象级交互；3)在多个零样本基准数据集上证明模型泛化和适应性；4)在模糊或杂乱场景中表现出强大性能；5)提供可扩展和可解释的上下文感知推理方法。相比之前的工作，这个方法的主要不同在于：结合了动态推理能力和视觉-语言对齐；不依赖任务特定监督；能处理复杂、模糊场景；具有更好的泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种创新的动态上下文感知场景推理框架，通过视觉-语言对齐实现了在没有任务特定监督的情况下理解和适应新环境的能力，显著提升了AI系统在复杂、模糊和杂乱场景中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world environments, AI systems often face unfamiliar scenarioswithout labeled data, creating a major challenge for conventional sceneunderstanding models. The inability to generalize across unseen contexts limitsthe deployment of vision-based applications in dynamic, unstructured settings.This work introduces a Dynamic Context-Aware Scene Reasoning framework thatleverages Vision-Language Alignment to address zero-shot real-world scenarios.The goal is to enable intelligent systems to infer and adapt to newenvironments without prior task-specific training. The proposed approachintegrates pre-trained vision transformers and large language models to alignvisual semantics with natural language descriptions, enhancing contextualcomprehension. A dynamic reasoning module refines predictions by combiningglobal scene cues and object-level interactions guided by linguistic priors.Extensive experiments on zero-shot benchmarks such as COCO, Visual Genome, andOpen Images demonstrate up to 18% improvement in scene understanding accuracyover baseline models in complex and unseen environments. Results also showrobust performance in ambiguous or cluttered scenes due to the synergisticfusion of vision and language. This framework offers a scalable andinterpretable approach for context-aware reasoning, advancing zero-shotgeneralization in dynamic real-world settings.</description>
      <author>example@mail.com (Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi)</author>
      <guid isPermaLink="false">2510.26580v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>AgriGS-SLAM: Orchard Mapping Across Seasons via Multi-View Gaussian Splatting SLAM</title>
      <link>http://arxiv.org/abs/2510.26358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgriGS-SLAM是一种结合视觉和激光雷达的SLAM框架，利用多相机3D高斯溅射技术实现果园环境的实时3D场景理解，克服了重复几何结构、季节变化和风吹 foliage 运动的挑战。&lt;h4&gt;背景&lt;/h4&gt;果园中的自主机器人需要实时3D场景理解，尽管存在重复的行列几何结构、季节性外观变化和风吹 foliage 运动。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理果园环境特殊挑战的实时3D场景理解系统。&lt;h4&gt;方法&lt;/h4&gt;结合直接激光雷达里程计和闭环检测与多相机3D高斯溅射渲染；通过互补视角的批量栅格化恢复遮挡下的果园结构；在关键帧之间执行统一的梯度驱动地图生命周期；基于概率激光雷达深度一致性项进行姿态优化并加强几何-外观耦合。&lt;h4&gt;主要发现&lt;/h4&gt;在苹果和梨果园的休眠期、开花期和收获期测试；跨季节和站点提供比先进3DGS-SLAM基线更清晰、更稳定的重建和轨迹；在拖拉机上保持实时性能。&lt;h4&gt;结论&lt;/h4&gt;虽然演示于果园监测，但该方法可应用于需要鲁棒多模态感知的其他户外领域。&lt;h4&gt;翻译&lt;/h4&gt;果园中的自主机器人需要实时3D场景理解，尽管存在重复的行列几何结构、季节性外观变化和风吹 foliage 运动。我们提出了AgriGS-SLAM，这是一种结合直接激光雷达里程计和闭环检测与多相机3D高斯溅射渲染的视觉-激光雷达SLAM框架。通过互补视角的批量栅格化恢复遮挡下的果园结构，同时在关键帧之间执行统一的梯度驱动地图生命周期以保留精细细节并限制内存使用。姿态优化由基于概率激光雷达的深度一致性项引导，通过相机投影反向传播以加强几何-外观耦合。我们在苹果和梨果园的休眠期、开花期和收获期使用标准化轨迹协议部署了该系统，评估训练视图和新颖视图合成以减少3DGS评估中的过拟合。跨季节和站点，AgriGS-SLAM比最近的先进3DGS-SLAM基线提供更清晰、更稳定的重建和更稳定的轨迹，同时在拖拉机上保持实时性能。虽然演示于果园监测，但该方法可应用于需要鲁棒多模态感知的其他户外领域。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决果园环境中自主机器人的实时3D场景理解问题，特别是在面对季节性外观变化、重复的行几何结构和风吹引起的叶片运动等挑战时的SLAM系统适应性。这个问题很重要，因为全球人口增长和劳动力短缺增加了对自主农业技术的需求，果园机器人需要准确的3D重建能力来执行导航、收获、喷洒和修剪等任务，同时农民需要即时反馈来调整田间操作，农业数字孪子也需要物理世界与数字表示之间的持续同步。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有SLAM系统在农业环境中的局限性：视觉-only方法在重复作物模式和植被运动下会失败，而激光雷达-only系统在几何稀疏性方面受限。他们借鉴了神经渲染的最新进展，特别是3D高斯泼溅(3DGS)的显式点表示和高效光栅化特性，适合大型场景SLAM的增量特性。方法设计上结合了直接激光雷达里程计(DLO)作为前端，因子图作为后端，并扩展了3DGS到多视图设置以处理果园遮挡，同时参考了现有的多视图优化和闭环检测技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合视觉和激光雷达两种传感器的优势，使用多摄像头系统提供互补视角解决遮挡问题，利用3D高斯泼溅进行实时场景表示，设计梯度驱动的地图生命周期管理，以及使用多模态损失函数联合优化场景表示和机器人定位。整体流程包括：SLAM前端使用直接激光雷达里程计估计运动；SLAM后端维护关键帧姿势的因子图；多视图3D高斯泼溅部分进行增量映射、内存管理和泼溅生命周期操作；优化循环调度各种操作；最后通过多模态损失函数结合光度一致性和几何一致性进行优化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 实时视觉-激光雷达3DGS-SLAM系统；2) 跨季节适用性基准评估方法；3) 统一的梯度驱动的3DGS-SLAM地图和定位优化；4) 支持多摄像头设置的户外3DGS-SLAM框架。相比之前工作，该方法专门针对果园环境设计，使用多摄像头系统而非单摄像头，结合激光雷达里程计和闭环检测而非仅依赖视觉或激光雷达，设计了特定的地图生命周期管理策略，使用多模态损失函数结合光度和几何一致性，并在真实果园的不同生长阶段进行了广泛测试。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AgriGS-SLAM通过结合多视图3D高斯泼溅与激光雷达里程计和闭环检测，实现了果园环境中的实时、跨季节精确地图构建和机器人定位，解决了季节性变化、重复结构和遮挡带来的挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robots in orchards require real-time 3D scene understandingdespite repetitive row geometry, seasonal appearance changes, and wind-drivenfoliage motion. We present AgriGS-SLAM, a Visual--LiDAR SLAM framework thatcouples direct LiDAR odometry and loop closures with multi-camera 3D GaussianSplatting (3DGS) rendering. Batch rasterization across complementary viewpointsrecovers orchard structure under occlusions, while a unified gradient-drivenmap lifecycle executed between keyframes preserves fine details and boundsmemory. Pose refinement is guided by a probabilistic LiDAR-based depthconsistency term, back-propagated through the camera projection to tightengeometry-appearance coupling. We deploy the system on a field platform in appleand pear orchards across dormancy, flowering, and harvesting, using astandardized trajectory protocol that evaluates both training-view andnovel-view synthesis to reduce 3DGS overfitting in evaluation. Across seasonsand sites, AgriGS-SLAM delivers sharper, more stable reconstructions andsteadier trajectories than recent state-of-the-art 3DGS-SLAM baselines whilemaintaining real-time performance on-tractor. While demonstrated in orchardmonitoring, the approach can be applied to other outdoor domains requiringrobust multimodal perception.</description>
      <author>example@mail.com (Mirko Usuelli, David Rapado-Rincon, Gert Kootstra, Matteo Matteucci)</author>
      <guid isPermaLink="false">2510.26358v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>A Three-Stage Bayesian Transfer Learning Framework to Improve Predictions in Data-Scarce Domains</title>
      <link>http://arxiv.org/abs/2510.26541v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Engineering Applications of Artificial Intelligence&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种全监督的三阶段框架（staged B-DANN），结合参数迁移和共享潜在空间适应，用于解决数据稀缺领域的机器学习问题。该方法通过确定性特征提取、对抗性优化和贝叶斯微调三个阶段，提高了预测准确性和泛化能力，同时提供不确定性估计。&lt;h4&gt;背景&lt;/h4&gt;机器学习在工程领域的应用持续增长，深度神经网络因其性能和可访问性被广泛采用，但需要大量高质量数据集。实验数据通常稀疏、嘈杂或不足以构建稳健的数据驱动模型。迁移学习利用数据丰富的源领域来辅助数据稀缺目标领域的学习，但传统参数迁移在领域差异较大时性能下降，领域对抗神经网络(DANNs)在半监督设置下可以处理更大的领域偏移，但训练不稳定且缺乏不确定性量化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理领域差异、提供不确定性估计并提高数据稀缺领域预测准确性和泛化能力的机器学习方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种全监督的三阶段框架：阶段性贝叶斯领域对抗神经网络(staged B-DANN)。第一阶段在源领域训练确定性特征提取器；第二阶段使用DANN对抗性地优化特征提取器；第三阶段在适应的特征提取器上构建贝叶斯神经网络，用于在目标领域微调，处理条件偏移并提供校准的不确定性估计。&lt;h4&gt;主要发现&lt;/h4&gt;在合成基准测试中，该方法显著优于标准迁移技术；应用于预测矩形通道中的临界热通量时，利用管状实验数据作为源领域，结果显示staged B-DANN方法可以提高预测准确性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;staged B-DANN方法可以改进预测准确性和泛化能力，并提供不确定性估计，可能有助于核工程等其他领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;机器学习在工程领域的应用持续增长，以支持广泛的应用。在这些方法中，深度神经网络因其性能和可访问性被广泛采用，但它们需要大量高质量的数据集。实验数据通常稀疏、嘈杂或不足以构建稳健的数据驱动模型。迁移学习利用数据丰富的相关源领域来辅助数据稀缺目标领域的学习，已显示出有效性。参数迁移（重用预训练权重）很常见，但在大的领域偏移下性能会下降。领域对抗神经网络(DANNs)通过学习领域不变的表示来解决这个问题，从而在半监督设置下提高较大领域偏移的迁移效果。然而，DANNs在训练过程中可能不稳定，且缺乏原生的不确定性量化手段。本研究引入了一种全监督的三阶段框架——阶段性贝叶斯领域对抗神经网络(staged B-DANN)，它结合了参数迁移和共享潜在空间适应。在第一阶段，在源领域训练确定性特征提取器。然后在第二阶段使用DANN对抗性地优化该特征提取器。在第三阶段，在适应的特征提取器上构建贝叶斯神经网络，用于在目标领域进行微调，以处理条件偏移并产生校准的不确定性估计。首先在合成基准测试中验证了这种阶段性B-DANN方法，结果表明它显著优于标准迁移技术。然后将其应用于预测矩形通道中的临界热通量任务，利用管状实验数据作为源领域。本研究结果表明，阶段性B-DANN方法可以提高预测准确性和泛化能力，可能有助于核工程的其他领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of ML in engineering has grown steadily to support a wide array ofapplications. Among these methods, deep neural networks have been widelyadopted due to their performance and accessibility, but they require large,high-quality datasets. Experimental data are often sparse, noisy, orinsufficient to build resilient data-driven models. Transfer learning, whichleverages relevant data-abundant source domains to assist learning indata-scarce target domains, has shown efficacy. Parameter transfer, wherepretrained weights are reused, is common but degrades under large domainshifts. Domain-adversarial neural networks (DANNs) help address this issue bylearning domain-invariant representations, thereby improving transfer undergreater domain shifts in a semi-supervised setting. However, DANNs can beunstable during training and lack a native means for uncertaintyquantification. This study introduces a fully-supervised three-stage framework,the staged Bayesian domain-adversarial neural network (staged B-DANN), thatcombines parameter transfer and shared latent space adaptation. In Stage 1, adeterministic feature extractor is trained on the source domain. This featureextractor is then adversarially refined using a DANN in Stage 2. In Stage 3, aBayesian neural network is built on the adapted feature extractor forfine-tuning on the target domain to handle conditional shifts and yieldcalibrated uncertainty estimates. This staged B-DANN approach was firstvalidated on a synthetic benchmark, where it was shown to significantlyoutperform standard transfer techniques. It was then applied to the task ofpredicting critical heat flux in rectangular channels, leveraging data fromtube experiments as the source domain. The results of this study show that thestaged B-DANN method can improve predictive accuracy and generalization,potentially assisting other domains in nuclear engineering.</description>
      <author>example@mail.com (Aidan Furlong, Robert Salko, Xingang Zhao, Xu Wu)</author>
      <guid isPermaLink="false">2510.26541v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Analysis of the Robustness of an Edge Detector Based on Cellular Automata Optimized by Particle Swarm</title>
      <link>http://arxiv.org/abs/2510.26509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于二维元胞自动机的可适应边缘检测器，通过元启发式方法和迁移学习技术进行优化。研究分析了优化阶段搜索空间扩展的影响以及检测器在不同图像集上的适应性。&lt;h4&gt;背景&lt;/h4&gt;边缘检测是图像处理中提取相关信息的重要任务，但现有检测器存在难以检测松散边缘和缺乏上下文信息等问题。&lt;h4&gt;目的&lt;/h4&gt;分析扩大优化阶段搜索空间的影响，以及检测器在识别自然图像集及其专门子集边缘时的适应性稳健性。&lt;h4&gt;方法&lt;/h4&gt;开发了一种由二维元胞自动机描述并通过元启发式方法结合迁移学习技术优化的可适应检测器。&lt;h4&gt;主要发现&lt;/h4&gt;扩大优化阶段的搜索空间对所选图像集并不有效；模型能够适应不同输入，但迁移学习技术未带来显著改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的检测器具有良好的适应性，但扩大搜索空间和迁移学习技术未能显著提高性能。&lt;h4&gt;翻译&lt;/h4&gt;边缘检测任务在图像处理中至关重要，旨在从图像中提取相关信息。此任务中存在一个反复出现的问题，即某些检测器的弱点，例如难以检测松散边缘以及缺乏从特定问题中提取相关信息的上下文。为解决这些弱点并使检测器适应图像特性，研究人员开发了一种由二维元胞自动机描述并通过元启发式方法结合迁移学习技术优化的可适应检测器。本研究旨在分析扩大优化阶段搜索空间的影响，以及检测器在识别自然图像集及其从同一图像集中提取的专门子集边缘时的适应性稳健性。获得的结果证明，对于所选图像集，扩大优化阶段的搜索空间并不有效。研究还通过一系列实验和验证技术分析了模型的适应性，发现无论采用何种验证方法，模型都能适应输入，并且应用于模型的迁移学习技术未显示出显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The edge detection task is essential in image processing aiming to extractrelevant information from an image. One recurring problem in this task is theweaknesses found in some detectors, such as the difficulty in detecting looseedges and the lack of context to extract relevant information from specificproblems. To address these weaknesses and adapt the detector to the propertiesof an image, an adaptable detector described by two-dimensional cellularautomaton and optimized by meta-heuristic combined with transfer learningtechniques was developed. This study aims to analyze the impact of expandingthe search space of the optimization phase and the robustness of theadaptability of the detector in identifying edges of a set of natural imagesand specialized subsets extracted from the same image set. The results obtainedprove that expanding the search space of the optimization phase was noteffective for the chosen image set. The study also analyzed the adaptability ofthe model through a series of experiments and validation techniques and foundthat, regardless of the validation, the model was able to adapt to the inputand the transfer learning techniques applied to the model showed no significantimprovements.</description>
      <author>example@mail.com (Vinícius Ferraria, Eurico Ruivo)</author>
      <guid isPermaLink="false">2510.26509v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Applications of Machine Learning in Polymer Materials: Property Prediction, Material Design, and Systematic Processes</title>
      <link>http://arxiv.org/abs/2510.26100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  55 pages, 6 tables, 9 figures, a systematic review on the research  progress and application prospects of machine learning in polymer materials&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统综述了机器学习技术在聚合物材料领域的研究进展和应用前景，介绍了基本技术、应用方法、当前挑战和未来趋势。&lt;h4&gt;背景&lt;/h4&gt;机器学习技术在聚合物材料领域发展迅速，显著加速了材料预测和设计，但其复杂性也给传统领域研究者带来理解和应用困难。聚合物材料研发面临结构复杂性和传统试错方法局限性等挑战。&lt;h4&gt;目的&lt;/h4&gt;应对聚合物材料研发中的挑战，解决机器学习方法的复杂性给传统领域研究者带来的理解和应用困难，促进机器学习技术在聚合物材料领域的有效应用。&lt;h4&gt;方法&lt;/h4&gt;分析聚合物材料研发中的固有挑战；介绍分子描述符、特征表示、数据标准化和清洗等关键技术；记录高质量聚合物数据库；构建高可靠性机器学习模型；实施实验验证、模型评估和优化方法。&lt;h4&gt;主要发现&lt;/h4&gt;机器学习在聚合物性能预测和材料设计中发挥关键作用；传统机器学习、深度学习和迁移学习等算法有具体应用；数据驱动设计策略包括反向设计、高通量虚拟筛选和多目标优化。&lt;h4&gt;结论&lt;/h4&gt;当前研究面临数据质量和模型泛化能力等技术挑战；未来发展趋势包括多尺度建模、物理信息机器学习、标准化数据共享和可解释机器学习。&lt;h4&gt;翻译&lt;/h4&gt;本文系统综述了机器学习技术在聚合物材料领域的研究进展和应用前景。目前，机器学习方法在聚合物材料研究中发展迅速；尽管它们显著加速了材料预测和设计，但其复杂性也给传统领域研究者的理解和应用带来了困难。针对上述问题，本文首先分析了聚合物材料研发中的固有挑战，包括结构复杂性和传统试错方法的局限性。为解决这些问题，它重点介绍了分子描述符和特征表示、数据标准化和清洗等关键基础技术，并记录了多个高质量的聚合物数据库。随后，它详细阐述了机器学习在聚合物性能预测和材料设计中的关键作用，涵盖了传统机器学习、深度学习和迁移学习等算法的具体应用；进一步深入阐述了数据驱动设计策略，如反向设计、高通量虚拟筛选和多目标优化。本文还系统介绍了构建高可靠性机器学习模型的完整过程，总结了有效的实验验证、模型评估和优化方法。最后，它总结了当前研究中的技术挑战，如数据质量和模型泛化能力，并展望了包括多尺度建模、物理信息机器学习、标准化数据共享和可解释机器学习在内的未来发展趋势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper systematically reviews the research progress and applicationprospects of machine learning technologies in the field of polymer materials.Currently, machine learning methods are developing rapidly in polymer materialresearch; although they have significantly accelerated material prediction anddesign, their complexity has also caused difficulties in understanding andapplication for researchers in traditional fields. In response to the aboveissues, this paper first analyzes the inherent challenges in the research anddevelopment of polymer materials, including structural complexity and thelimitations of traditional trial-and-error methods. To address these problems,it focuses on introducing key basic technologies such as molecular descriptorsand feature representation, data standardization and cleaning, and records anumber of high-quality polymer databases. Subsequently, it elaborates on thekey role of machine learning in polymer property prediction and materialdesign, covering the specific applications of algorithms such as traditionalmachine learning, deep learning, and transfer learning; further, it deeplyexpounds on data-driven design strategies, such as reverse design,high-throughput virtual screening, and multi-objective optimization. The paperalso systematically introduces the complete process of constructinghigh-reliability machine learning models and summarizes effective experimentalverification, model evaluation, and optimization methods. Finally, itsummarizes the current technical challenges in research, such as data qualityand model generalization ability, and looks forward to future developmenttrends including multi-scale modeling, physics-informed machine learning,standardized data sharing, and interpretable machine learning.</description>
      <author>example@mail.com (Hongtao Guo Shuai Li Shu Li)</author>
      <guid isPermaLink="false">2510.26100v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Anomalies in Machine Learning Infrastructure via Hardware Telemetry</title>
      <link>http://arxiv.org/abs/2510.26008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, submitted to nsdi 26&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为System-X的系统级优化方法，它采用以硬件为中心的思路，仅依赖硬件信号而非工作负载知识进行优化，成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%。&lt;h4&gt;背景&lt;/h4&gt;现代机器学习已成为一个紧密结合的全栈生态系统，许多用户依赖云提供商提供弹性、隔离和成本高效的资源。然而，这些平台即服务使用虚拟化，导致运营商对用户的工作负载了解有限，阻碍了资源优化。&lt;h4&gt;目的&lt;/h4&gt;论证工作负载知识对系统级优化不是必需的，并提出一种仅依赖硬件信号的优化方法。&lt;h4&gt;方法&lt;/h4&gt;提出System-X系统，采用以硬件为中心的方法，仅依赖运营商完全可访问的硬件信号。通过从系统收集低级信号，使用无监督学习管道检测异常。该管道通过分析各种硬件平台上30多种流行的ML模型开发，确保能够适应新兴工作负载和未知部署模式。&lt;h4&gt;主要发现&lt;/h4&gt;使用System-X成功识别了网络和系统配置问题，加速了DeepSeek模型5.97%。&lt;h4&gt;结论&lt;/h4&gt;系统级优化可以通过仅依赖硬件信号来实现，无需了解具体的工作负载细节。&lt;h4&gt;翻译&lt;/h4&gt;现代机器学习已发展成为一个紧密结合的全栈生态系统，结合了硬件、软件、网络和应用。许多用户依赖云提供商提供弹性、隔离和成本高效的资源。不幸的是，这些平台即服务使用虚拟化，这意味着运营商对用户的工作负载了解有限。这阻碍了运营商的资源优化，而这对确保成本效率和最小化执行时间至关重要。在本文中，我们认为工作负载知识对系统级优化不是必需的。我们提出了System-X，它采用以硬件为中心的方法，仅依赖硬件信号——这些信号完全可被运营商访问。使用从系统收集的低级信号，System-X通过无监督学习管道检测异常。该管道是通过分析各种硬件平台上30多种流行的ML模型开发的，确保了对新兴工作负载和未知部署模式的适应性。使用System-X，我们成功识别了网络和系统配置问题，将DeepSeek模型加速了5.97%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern machine learning (ML) has grown into a tightly coupled, full-stackecosystem that combines hardware, software, network, and applications. Manyusers rely on cloud providers for elastic, isolated, and cost-efficientresources. Unfortunately, these platforms as a service use virtualization,which means operators have little insight into the users' workloads. Thishinders resource optimizations by the operator, which is essential to ensurecost efficiency and minimize execution time. In this paper, we argue thatworkload knowledge is unnecessary for system-level optimization. We proposeSystem-X, which takes a \emph{hardware-centric} approach, relying only onhardware signals -- fully accessible by operators. Using low-level signalscollected from the system, System-X detects anomalies through an unsupervisedlearning pipeline. The pipeline is developed by analyzing over 30 popular MLmodels on various hardware platforms, ensuring adaptability to emergingworkloads and unknown deployment patterns. Using System-X, we successfullyidentified both network and system configuration issues, accelerating theDeepSeek model by 5.97%.</description>
      <author>example@mail.com (Ziji Chen, Steven Chien, Peng Qian, Noa Zilberman)</author>
      <guid isPermaLink="false">2510.26008v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised local learning based on voltage-dependent synaptic plasticity for resistive and ferroelectric synapses</title>
      <link>http://arxiv.org/abs/2510.25787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于忆阻器件的电压依赖性突触可塑性(VDSP)学习方法，用于解决边缘计算设备上AI部署的能耗问题。该方法实现了低功耗的无监督学习，在MNIST模式识别任务上取得了超过83%的准确率，并针对不同类型的忆阻器件进行了适应性调整和鲁棒性优化。&lt;h4&gt;背景&lt;/h4&gt;边缘计算设备上部署人工智能面临显著的能耗和功能性挑战。这些设备需要低功耗且能够实时适应的学习机制。基于纳米尺度电阻存储器的内存计算技术可能在这些边缘设备上执行AI工作负载方面发挥关键作用。&lt;h4&gt;目的&lt;/h4&gt;研究旨在开发一种高效的无监督和局部学习方法，基于赫布原理在忆阻突触中实现，使AI能够在边缘设备上低功耗运行，同时保持高性能。&lt;h4&gt;方法&lt;/h4&gt;研究引入了电压依赖性突触可塑性(VDSP)作为基于赫布原理的忆阻突触无监督和局部学习方法。这种方法无需复杂脉冲整形电路即可实现在线学习，展示了如何将VDSP适应到三种具有不同开关特性的忆阻器件：TiO₂、基于HfO₂的金属氧化物丝状突触和基于HfZrO₄的铁电隧道结(FTJ)。通过系统级模拟验证了这些器件在脉冲神经网络中的无监督学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 所有测试的忆阻器件在使用200个神经元的情况下，在基于MNIST的模式识别任务上实现了超过83%的准确率，达到最先进性能。2. VDSP方法成功适应了三种不同类型的忆阻器件，证明了其通用性。3. 研究评估了器件变异性(如开关阈值和高低电阻状态水平比率)对性能的影响，并提出了增强系统鲁棒性的缓解策略。&lt;h4&gt;结论&lt;/h4&gt;VDSP方法为边缘计算设备上的AI部署提供了一种高效、低功耗的学习解决方案，无需复杂电路即可实现无监督学习。该方法在不同类型的忆阻器件上表现出色，并通过针对器件变异性的缓解策略增强了系统鲁棒性，为边缘AI应用提供了实用可行的技术路径。&lt;h4&gt;翻译&lt;/h4&gt;在边缘计算设备上部署人工智能面临与能耗和功能相关的重大挑战。这些设备可以从大脑启发的学习机制中极大受益，允许在低功耗条件下进行实时适应。使用纳米尺度电阻存储器的内存计算可能在使这些边缘设备上执行AI工作负载方面发挥关键作用。在本研究中，我们引入了电压依赖性突触可塑性(VDSP)作为一种基于赫布原理的忆阻突触中无监督和局部学习的高效方法。这种方法无需脉冲时间依赖可塑性(STDP)通常需要的复杂脉冲整形电路即可实现在线学习。我们展示了如何将VDSP advantageous地适应到三种具有不同开关特性的忆阻器件：TiO₂、基于HfO₂的金属氧化物丝状突触和基于HfZrO₄的铁电隧道结(FTJ)。进行了包含这些器件的脉冲神经网络系统级模拟，以验证基于MNIST的模式识别任务上的无监督学习，达到了最先进的性能。结果表明所有设备在使用200个神经元的情况下都实现了超过83%的准确率。此外，我们评估了器件变异性的影响，如开关阈值和高低电阻状态水平比率，并提出了增强鲁棒性的缓解策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of AI on edge computing devices faces significant challengesrelated to energy consumption and functionality. These devices could greatlybenefit from brain-inspired learning mechanisms, allowing for real-timeadaptation while using low-power. In-memory computing with nanoscale resistivememories may play a crucial role in enabling the execution of AI workloads onthese edge devices. In this study, we introduce voltage-dependent synapticplasticity (VDSP) as an efficient approach for unsupervised and local learningin memristive synapses based on Hebbian principles. This method enables onlinelearning without requiring complex pulse-shaping circuits typically necessaryfor spike-timing-dependent plasticity (STDP). We show how VDSP can beadvantageously adapted to three types of memristive devices (TiO$_2$,HfO$_2$-based metal-oxide filamentary synapses, and HfZrO$_4$-basedferroelectric tunnel junctions (FTJ)) with disctinctive switchingcharacteristics. System-level simulations of spiking neural networksincorporating these devices were conducted to validate unsupervised learning onMNIST-based pattern recognition tasks, achieving state-of-the-art performance.The results demonstrated over 83% accuracy across all devices using 200neurons. Additionally, we assessed the impact of device variability, such asswitching thresholds and ratios between high and low resistance state levels,and proposed mitigation strategies to enhance robustness.</description>
      <author>example@mail.com (Nikhil Garg, Ismael Balafrej, Joao Henrique Quintino Palhares, Laura Bégon-Lours, Davide Florini, Donato Francesco Falcone, Tommaso Stecconi, Valeria Bragaglia, Bert Jan Offrein, Jean-Michel Portal, Damien Querlioz, Yann Beilliard, Dominique Drouin, Fabien Alibart)</author>
      <guid isPermaLink="false">2510.25787v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>HEIR: Learning Graph-Based Motion Hierarchies</title>
      <link>http://arxiv.org/abs/2510.26786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code link: https://github.com/princeton-computational-imaging/HEIR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种通用的层次运动建模方法，通过图神经网络直接从数据中学习结构化的、可解释的运动关系，克服了传统方法依赖手动定义层次结构和固定运动基元的局限性，在多种运动类型上展现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;运动的层次结构存在于计算机视觉、图形学和机器人学等多个研究领域，复杂动力学通常源于简单运动组件之间的协调相互作用。现有方法通常依赖于手动定义的或启发式的层次结构，具有固定的运动基元，限制了它们在不同任务间的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的层次运动建模方法，直接从数据中学习结构化的、可解释的运动关系，适用于广泛的以运动为中心的任务。&lt;h4&gt;方法&lt;/h4&gt;使用基于图的层次结构表示观察到的运动，将全局绝对运动分解为父继承模式和局部运动残差；将层次推断制定为可微的图学习问题，其中顶点表示基本运动，有向边通过图神经网络捕获学习的父子依赖关系；在一维平移运动、二维旋转运动和动态三维场景变形三个示例上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在一维和二维情况下成功重建了内在的运动层次结构；与基线相比，在动态3D高斯飞溅场景中产生了更真实、可解释的变形效果。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种适应性强、数据驱动的层次建模范式，适用于广泛的以运动为中心的任务，通过学习而非预设层次结构实现了更好的泛化能力和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;运动层次结构存在于包括计算机视觉、图形学和机器人学在内的研究领域，其中复杂动力学通常源于简单运动组件之间的协调相互作用。现有方法对这类动力学进行建模通常依赖于手动定义的或启发式的层次结构，具有固定的运动基元，限制了它们在不同任务间的泛化能力。在本工作中，我们提出了一种通用的层次运动建模方法，直接从数据中学习结构化的、可解释的运动关系。我们的方法使用基于图的层次结构来表示观察到的运动，明确地将全局绝对运动分解为父继承模式和局部运动残差。我们将层次推断制定为可微的图学习问题，其中顶点表示基本运动，有向边通过图神经网络捕获学习的父子依赖关系。我们在三个示例上评估了我们的层次重建方法：一维平移运动、二维旋转运动和通过高斯飞溅的动态三维场景变形。实验结果表明，我们的方法在一维和二维情况下重建了内在的运动层次结构，与基线相比，在动态3D高斯飞溅场景中产生了更真实、可解释的变形。通过提供一种适应性强、数据驱动的层次建模范式，我们的方法适用于广泛的以运动为中心的任务。项目页面：https://light.princeton.edu/HEIR/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决运动层次结构的自动建模问题，现有方法通常依赖手动定义的层次结构或固定的运动基元，限制了跨任务泛化能力。这个问题在计算机视觉、图形学和机器人学等多个领域都很重要，因为复杂运动往往源于简单运动组件间的协调，层次结构能帮助理解、生成、预测和控制运动，解决多尺度依赖关系和组合爆炸问题。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者认识到现有方法（手动定义模板或非可解释神经模块）的局限性，注意到不同研究领域面临共同挑战，需要自适应选择合适抽象层次的方法。他们借鉴了图神经网络思想用于学习边权重和父子关系，使用Gumbel-Softmax技巧处理离散层次结构的可微分采样，还参考了层次运动表示（如骨骼定义关系）和3D场景变形（如NeMF、MovingParts）等领域的现有工作。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用基于图的层次结构表示运动，将全局绝对运动分解为父继承模式和局部运动残差，将层次推断转化为可微分图学习问题。流程包括：1)构建邻近有向图，顶点表示运动元素；2)通过图注意力层计算边权重；3)使用Gumbel-Softmax采样层次结构；4)沿层次结构累积相对速度重建绝对运动；5)通过重建损失和正则化项训练模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新包括：直接从数据学习可解释运动关系；使用图结构显式分解运动为父继承和局部残差；将层次推断转化为可微分图学习问题；适用于多种运动类型。相比之前工作，不依赖手动定义层次或固定基元，提供可解释结构，不假设特定领域或维度，在3D场景变形中产生更真实结果，具有更好泛化能力和可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于图的层次运动学习方法HEIR，能够直接从数据中学习可解释的运动层次结构，有效分解复杂运动为父继承模式和局部残差，并在多种运动建模任务中展现出优越的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hierarchical structures of motion exist across research fields, includingcomputer vision, graphics, and robotics, where complex dynamics typically arisefrom coordinated interactions among simpler motion components. Existing methodsto model such dynamics typically rely on manually-defined or heuristichierarchies with fixed motion primitives, limiting their generalizabilityacross different tasks. In this work, we propose a general hierarchical motionmodeling method that learns structured, interpretable motion relationshipsdirectly from data. Our method represents observed motions using graph-basedhierarchies, explicitly decomposing global absolute motions intoparent-inherited patterns and local motion residuals. We formulate hierarchyinference as a differentiable graph learning problem, where vertices representelemental motions and directed edges capture learned parent-child dependenciesthrough graph neural networks. We evaluate our hierarchical reconstructionapproach on three examples: 1D translational motion, 2D rotational motion, anddynamic 3D scene deformation via Gaussian splatting. Experimental results showthat our method reconstructs the intrinsic motion hierarchy in 1D and 2D cases,and produces more realistic and interpretable deformations compared to thebaseline on dynamic 3D Gaussian splatting scenes. By providing an adaptable,data-driven hierarchical modeling paradigm, our method offers a formulationapplicable to a broad range of motion-centric tasks. Project Page:https://light.princeton.edu/HEIR/</description>
      <author>example@mail.com (Cheng Zheng, William Koch, Baiang Li, Felix Heide)</author>
      <guid isPermaLink="false">2510.26786v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Graph Guided Modulo Recovery of EEG Signals</title>
      <link>http://arxiv.org/abs/2510.26756v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的GraphUnwrapNet方法，用于解决脑电图(EEG)信号模数采样恢复问题。通过将EEG信号表示为有组织的图结构，并引入预估计引导的特征注入模块，有效提升了在信号折叠边界处的恢复稳定性。实验结果表明，该方法优于传统优化技术，并与当前深度学习模型具有竞争性。&lt;h4&gt;背景&lt;/h4&gt;脑电图(EEG)在不同人之间常表现出显著变异性，这种波动会干扰可靠信号采集并可能导致信号失真或削波。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的方法从模数采样的折叠观测中恢复原始EEG信号，解决这一高度不适定问题。&lt;h4&gt;方法&lt;/h4&gt;提出GraphUnwrapNet，一种基于图神经网络的方法，将EEG信号表示为有组织的图结构，并引入预估计引导的特征注入模块，提供粗略的折叠指示器以增强恢复稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;在STEW数据集上的实验表明，与传统优化技术相比有持续提升，与当前深度学习模型相比具有竞争性的准确性。&lt;h4&gt;结论&lt;/h4&gt;基于图的方法在鲁棒模数EEG恢复方面具有显著潜力。&lt;h4&gt;翻译&lt;/h4&gt;脑电图(EEG)通常表现出显著的个体间变异性。这种波动会干扰可靠的信号采集并可能导致失真或削波。模数采样现在是解决这个问题的有前途的方法，通过折叠信号而不是使它们饱和。从折叠观测中恢复原始波形是一个高度不适定的问题。在本工作中，我们提出了一种基于图神经网络的方法，称为GraphUnwrapNet，用于EEG信号的模数恢复。我们的核心思想是将EEG信号表示为一个有组织的图，其通道和时间连接建立了潜在的相互依赖关系。我们的一个关键贡献是引入了一个预估计引导的特征注入模块，提供粗略的折叠指示器，增强在折叠边界处的恢复稳定性。这种设计将结构信息与折叠先验集成到一个统一的框架中。我们在同时任务脑电图工作负荷(STEW)数据集上进行了全面的实验。结果表明与传统优化技术相比有持续的提升，与当前深度学习模型相比具有竞争性的准确性。我们的发现强调了基于图的方法在鲁棒模数EEG恢复方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electroencephalography (EEG) often shows significant variability amongpeople. This fluctuation disrupts reliable acquisition and may result indistortion or clipping. Modulo sampling is now a promising solution to thisproblem, by folding signals instead of saturating them. Recovery of theoriginal waveform from folded observations is a highly ill-posed problem. Inthis work, we propose a method based on a graph neural network, referred to asGraphUnwrapNet, for the modulo recovery of EEG signals. Our core idea is torepresent an EEG signal as an organized graph whose channels and temporalconnections establish underlying interdependence. One of our key contributionsis in introducing a pre-estimation guided feature injection module to providecoarse folding indicators that enhance stability during recovery at wrapboundaries. This design integrates structural information with folding priorsinto an integrated framework. We performed comprehensive experiments on theSimultaneous Task EEG Workload (STEW) dataset. The results demonstrateconsistent enhancements over traditional optimization techniques andcompetitive accuracy relative to current deep learning models. Our findingsemphasize the potential of graph-based methodology for robust modulo EEGrecovery.</description>
      <author>example@mail.com (Soujanya Hazra, Sanjay Ghosh)</author>
      <guid isPermaLink="false">2510.26756v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Spiking Patches: Asynchronous, Sparse, and Efficient Tokens for Event Cameras</title>
      <link>http://arxiv.org/abs/2510.26614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对事件相机的tokenization方法，称为Spiking Patches，能够保留事件流的异步性和空间稀疏性特性，同时保持高准确性，并且推理速度比传统方法更快。&lt;h4&gt;背景&lt;/h4&gt;现有的事件表示方法（如帧或体素）虽然是同步的且降低了空间稀疏性，但能产生高准确性。&lt;h4&gt;目的&lt;/h4&gt;发现一种能够保留事件相机独特属性的事件表示方法。&lt;h4&gt;方法&lt;/h4&gt;提出Spiking Patches tokenizer，专门为事件相机设计，能够保留事件流的异步性和空间稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;使用GNN、PCN和Transformer在手势识别和物体检测任务上评估，Spiking Patches的token比基于体素的token推理速度快3.4倍，比基于帧的token推理速度快10.4倍，在保持相同准确性的同时，在某些情况下甚至超越它们，手势识别绝对改进最高达3.8，物体检测绝对改进最高达1.4。&lt;h4&gt;结论&lt;/h4&gt;tokenization是事件视觉领域的新方向，标志着保留事件相机属性方法的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出事件的tokenization并展示了一个tokenizer，Spiking Patches，专门为事件相机设计。给定异步和空间稀疏的事件流，我们的目标是发现保留这些属性的事件表示。先前的工作将事件表示为帧或体素。然而，虽然这些表示能产生高准确性，但帧和体素都是同步的，降低了空间稀疏性。Spiking Patches提供了保留事件相机独特属性的方法，我们在实验中证明这不会牺牲准确性。我们使用GNN、PCN和Transformer在手势识别和物体检测任务上评估我们的tokenizer。来自Spiking Patches的token比基于体素的token推理速度快3.4倍，比基于帧的token推理速度快10.4倍。我们在保持相同准确性的同时实现了这一点，在某些情况下甚至超越它们，手势识别绝对改进最高达3.8，物体检测绝对改进最高达1.4。因此，tokenization构成了事件视觉领域的一个新方向，标志着保留事件相机属性方法的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose tokenization of events and present a tokenizer, Spiking Patches,specifically designed for event cameras. Given a stream of asynchronous andspatially sparse events, our goal is to discover an event representation thatpreserves these properties. Prior works have represented events as frames or asvoxels. However, while these representations yield high accuracy, both framesand voxels are synchronous and decrease the spatial sparsity. Spiking Patchesgives the means to preserve the unique properties of event cameras and we showin our experiments that this comes without sacrificing accuracy. We evaluateour tokenizer using a GNN, PCN, and a Transformer on gesture recognition andobject detection. Tokens from Spiking Patches yield inference times that are upto 3.4x faster than voxel-based tokens and up to 10.4x faster than frames. Weachieve this while matching their accuracy and even surpassing in some caseswith absolute improvements up to 3.8 for gesture recognition and up to 1.4 forobject detection. Thus, tokenization constitutes a novel direction inevent-based vision and marks a step towards methods that preserve theproperties of event cameras.</description>
      <author>example@mail.com (Christoffer Koo Øhrstrøm, Ronja Güldenring, Lazaros Nalpantidis)</author>
      <guid isPermaLink="false">2510.26614v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>UnifiedFL: A Dynamic Unified Learning Framework for Equitable Federation</title>
      <link>http://arxiv.org/abs/2510.26350v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出UnifiedFL，一种动态联邦学习框架，用于处理具有不同神经网络架构和非相同分布数据的客户端之间的协作训练，通过图神经网络优化异构本地网络，实验证明在多个基准测试中表现优越。&lt;h4&gt;背景&lt;/h4&gt;联邦学习作为关键范式，允许多个客户端在不共享原始数据的情况下协作训练模型，支持隐私保护应用。然而，关于具有不同神经网络架构和非相同分布数据集的客户端之间的协作训练研究仍然很少。&lt;h4&gt;目的&lt;/h4&gt;解决现有联邦学习框架在支持根本不同架构客户端、处理数据统计异质性和领域断裂问题上的局限性，提高模型在不同测试域上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出UnifiedFL框架，将异构本地网络表示为有向模型图中的节点和边，通过共享图神经网络优化。引入通用GNN参数化所有架构、基于客户端参数之间欧几里得距离的距离驱动聚类，以及平衡收敛性和多样性的两层聚合策略。&lt;h4&gt;主要发现&lt;/h4&gt;现有联邦学习方法只能支持单一模型家族内的变体，假设共享全局架构，无法适应不同网络类型；现有方法通常只处理统计异质性，忽略领域断裂问题；当客户端使用不同架构、具有非相同分布数据并遇到不同测试域时，当前方法表现不佳。&lt;h4&gt;结论&lt;/h4&gt;UnifiedFL在MedMNIST分类和海马体分割基准测试中表现出优越性能，代码和数据可在https://github.com/basiralab/UnifiedFL获取。&lt;h4&gt;翻译&lt;/h4&gt;联邦学习（FL）已成为一种关键范式，使多个客户端能够在不共享原始数据的情况下协作训练模型，在放射学和病理学等领域支持隐私保护应用。然而，关于具有根本不同神经网络架构和非相同分布数据集的客户端之间的协作训练研究仍然很少。现有的联邦学习框架面临几个局限性。尽管声称支持架构异构性，但大多数联邦学习方法只容忍单一模型家族内的变体（例如，更浅、更深或更宽的CNN），仍然假设共享全局架构，无法适应客户端部署不同网络类型（例如，CNN、GNN、MLP）的联邦。此外，现有方法通常只处理统计异质性，而忽略了领域断裂问题，即每个客户端的数据分布与测试时面临的数据分布明显不同，从而削弱了模型的泛化能力。当客户端使用不同架构、具有非相同分布数据并遇到不同的测试域时，当前方法表现不佳。为解决这些挑战，我们提出UnifiedFL，一种动态联邦学习框架，将异构本地网络表示为有向模型图中的节点和边，并通过共享的图神经网络（GNN）进行优化。UnifiedFL引入了（i）通用GNN参数化所有架构，（ii）基于客户端参数之间欧几里得距离的距离驱动聚类，以及（iii）平衡收敛性和多样性的两层聚合策略。在MedMNIST分类和海马体分割基准测试中进行的实验证明了UnifiedFL的优越性能。代码和数据：https://github.com/basiralab/UnifiedFL&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated learning (FL) has emerged as a key paradigm for collaborative modeltraining across multiple clients without sharing raw data, enablingprivacy-preserving applications in areas such as radiology and pathology.However, works on collaborative training across clients with fundamentallydifferent neural architectures and non-identically distributed datasets remainscarce. Existing FL frameworks face several limitations. Despite claiming tosupport architectural heterogeneity, most recent FL methods only toleratevariants within a single model family (e.g., shallower, deeper, or wider CNNs),still presuming a shared global architecture and failing to accommodatefederations where clients deploy fundamentally different network types (e.g.,CNNs, GNNs, MLPs). Moreover, existing approaches often address only statisticalheterogeneity while overlooking the domain-fracture problem, where eachclient's data distribution differs markedly from that faced at testing time,undermining model generalizability. When clients use different architectures,have non-identically distributed data, and encounter distinct test domains,current methods perform poorly. To address these challenges, we proposeUnifiedFL, a dynamic federated learning framework that represents heterogeneouslocal networks as nodes and edges in a directed model graph optimized by ashared graph neural network (GNN). UnifiedFL introduces (i) a common GNN toparameterize all architectures, (ii) distance-driven clustering via Euclideandistances between clients' parameters, and (iii) a two-tier aggregation policybalancing convergence and diversity. Experiments on MedMNIST classification andhippocampus segmentation benchmarks demonstrate UnifiedFL's superiorperformance. Code and data: https://github.com/basiralab/UnifiedFL</description>
      <author>example@mail.com (Furkan Pala, Islem Rekik)</author>
      <guid isPermaLink="false">2510.26350v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>From Embedding to Control: Representations for Stochastic Multi-Object Systems</title>
      <link>http://arxiv.org/abs/2510.26344v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出图可控嵌入（GCE）框架，用于学习线性控制下的随机多对象动力学系统。该框架基于希尔伯特空间嵌入，将受控随机动力学的概率分布嵌入到再生核希尔伯特空间中，保留非线性表达能力的同时支持线性操作。GCE采用平均场近似技术捕获对象间依赖关系，并通过图神经网络构建适应动态交互模式的核特征，能够推广到未见过的拓扑结构。&lt;h4&gt;背景&lt;/h4&gt;具有多个相互作用的随机非线性动力学系统的精确建模和控制是一个具有挑战性的问题。非均匀交互和随机拓扑结构使得这一任务更加困难，需要开发新的方法来处理这些复杂情况。&lt;h4&gt;目的&lt;/h4&gt;研究如何实现具有多个相互作用的随机非线性动力学系统的精确建模和有效控制，特别是应对非均匀交互和随机拓扑带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了图可控嵌入（GCE）框架，这是一种基于希尔伯特空间嵌入的通用方法。GCE将受控随机动力学的概率分布直接嵌入到再生核希尔伯特空间（RKHS）中，允许在保留非线性表达能力的同时进行线性操作。该方法采用平均场近似技术来捕获对象间依赖关系，并通过整合图神经网络构建数据相关的核特征，使其能够适应动态交互模式并推广到未见过的拓扑结构。&lt;h4&gt;主要发现&lt;/h4&gt;1. GCE提供了关于存在性、收敛性和适用性的理论保证；2. 平均场近似技术能够有效捕获对象间依赖关系，实现低样本复杂度；3. 构建的核特征能够适应动态交互模式，仅用有限训练实例即可推广到未见拓扑；4. GCE可无缝扩展到不同大小和拓扑的多对象系统；5. 希尔伯特空间的线性支持简单而有效的控制算法来合成最优序列。&lt;h4&gt;结论&lt;/h4&gt;图可控嵌入（GCE）框架为随机多对象动力学系统的建模和控制提供了一种有效方法。通过结合希尔伯特空间嵌入、平均场近似和图神经网络，GCE能够处理非均匀交互和随机拓扑带来的挑战，并在物理系统、机器人和电力系统实验中展现出优越性能，特别是在分布内和少样本测试中优于其他嵌入方法。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了如何在具有多个相互作用的随机非线性动力学系统中实现精确建模和有效控制。然而，非均匀交互和随机拓扑使这一任务具有挑战性。我们通过提出图可控嵌入（GCE）来解决这些挑战，这是一个用于学习线性控制下随机多对象动力学的通用框架。具体来说，GCE建立在希尔伯特空间嵌入的基础上，允许将受控随机动力学的概率分布直接嵌入到再生核希尔伯特空间（RKHS）中，这使其RKHS中的线性操作能够保留非线性表达能力。我们提供了关于GCE的存在性、收敛性和适用性的理论保证。值得注意的是，采用平均场近似技术来有效捕获对象间依赖关系，并实现可证明的低样本复杂度。通过整合图神经网络，我们构建了能够适应动态交互模式的数据相关核特征，并且仅用有限的训练实例就能推广到未见过的拓扑结构。GCE可以无缝扩展到不同大小和拓扑的多对象系统。利用希尔伯特空间的线性，GCE还支持简单而有效的控制算法来合成最优序列。在物理系统、机器人和电力系统上的实验验证了GCE的有效性，并在分布内和少样本测试中，相比各种有竞争力的嵌入方法都展现出一致的性能提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper studies how to achieve accurate modeling and effective control instochastic nonlinear dynamics with multiple interacting objects. However,non-uniform interactions and random topologies make this task challenging. Weaddress these challenges by proposing \textit{Graph Controllable Embeddings}(GCE), a general framework to learn stochastic multi-object dynamics for linearcontrol. Specifically, GCE is built on Hilbert space embeddings, allowingdirect embedding of probability distributions of controlled stochastic dynamicsinto a reproducing kernel Hilbert space (RKHS), which enables linear operationsin its RKHS while retaining nonlinear expressiveness. We provide theoreticalguarantees on the existence, convergence, and applicability of GCE. Notably, amean field approximation technique is adopted to efficiently captureinter-object dependencies and achieve provably low sample complexity. Byintegrating graph neural networks, we construct data-dependent kernel featuresthat are capable of adapting to dynamic interaction patterns and generalizingto even unseen topologies with only limited training instances. GCE scalesseamlessly to multi-object systems of varying sizes and topologies. Leveragingthe linearity of Hilbert spaces, GCE also supports simple yet effective controlalgorithms for synthesizing optimal sequences. Experiments on physical systems,robotics, and power grids validate GCE and demonstrate consistent performanceimprovement over various competitive embedding methods in both in-distributionand few-shot tests</description>
      <author>example@mail.com (Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Chenyang Yuan, Zhuo Sun, Yukun Hu)</author>
      <guid isPermaLink="false">2510.26344v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Heterogeneous Graph Neural Networks for Cybersecurity Anomaly Detection</title>
      <link>http://arxiv.org/abs/2510.26307v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 4 figures, 86 references. Submitted to Journal of Computer  Security (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了对网络安全中基于异构图神经网络(HGNN)的异常检测方法的全面综述，建立了分类法，分析了代表性模型，回顾了基准数据集和评估指标，并确定了未来研究方向。&lt;h4&gt;背景&lt;/h4&gt;异常检测在网络安全中是关键任务，需要识别内部威胁、访问违规和协调攻击。基于图的方法在建模实体交互方面变得越来越重要，但大多数依赖于同质和静态结构，这限制了它们捕捉现实环境中异构性和时间演化的能力。&lt;h4&gt;目的&lt;/h4&gt;解决基于HGNN的异常检测研究分散、缺乏比较评估和标准化基准的问题，为该领域建立结构化的基础。&lt;h4&gt;方法&lt;/h4&gt;引入按异常类型和图动力学对方法进行分类的分类法，分析代表性模型并将其映射到关键网络安全应用，回顾常用基准数据集和评估指标，强调其优缺点。&lt;h4&gt;主要发现&lt;/h4&gt;确定了与建模、数据和部署相关的主要开放挑战，概述了未来研究的有希望的方向。&lt;h4&gt;结论&lt;/h4&gt;该综述旨在为推进基于HGNN的异常检测建立结构化的基础，使其可扩展、可解释且实际可部署。&lt;h4&gt;翻译&lt;/h4&gt;异常检测是网络安全中的关键任务，其中识别内部威胁、访问违规和协调攻击对于确保系统弹性至关重要。基于图的方法在建模实体交互方面变得越来越重要，但大多数依赖于同质和静态结构，这限制了它们捕捉现实环境中异构性和时间演化的能力。异构图神经网络已成为一种有前景的异常检测范式，通过整合类型感知变换和关系敏感聚合，能够对复杂的网络数据进行更具表现力的建模。然而，当前关于基于HGNN的异常检测研究仍然分散，建模策略多样，比较评估有限，且缺乏标准化基准。为了解决这一差距，我们对网络安全中基于HGNN的异常检测方法进行了全面综述。我们引入了一个按异常类型和图动力学对方法进行分类的分类法，分析了代表性模型，并将它们映射到关键的网络安全应用。我们还回顾了常用的基准数据集和评估指标，强调了它们的优缺点。最后，我们确定了与建模、数据和部署相关的主要开放挑战，并概述了未来研究的有希望的方向。本综述旨在为推进基于HGNN的异常检测建立结构化的基础，朝着可扩展、可解释和实际可部署的解决方案发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection is a critical task in cybersecurity, where identifyinginsider threats, access violations, and coordinated attacks is essential forensuring system resilience. Graph-based approaches have become increasinglyimportant for modeling entity interactions, yet most rely on homogeneous andstatic structures, which limits their ability to capture the heterogeneity andtemporal evolution of real-world environments. Heterogeneous Graph NeuralNetworks (HGNNs) have emerged as a promising paradigm for anomaly detection byincorporating type-aware transformations and relation-sensitive aggregation,enabling more expressive modeling of complex cyber data. However, currentresearch on HGNN-based anomaly detection remains fragmented, with diversemodeling strategies, limited comparative evaluation, and an absence ofstandardized benchmarks. To address this gap, we provide a comprehensive surveyof HGNN-based anomaly detection methods in cybersecurity. We introduce ataxonomy that classifies approaches by anomaly type and graph dynamics, analyzerepresentative models, and map them to key cybersecurity applications. We alsoreview commonly used benchmark datasets and evaluation metrics, highlightingtheir strengths and limitations. Finally, we identify key open challengesrelated to modeling, data, and deployment, and outline promising directions forfuture research. This survey aims to establish a structured foundation foradvancing HGNN-based anomaly detection toward scalable, interpretable, andpractically deployable solutions.</description>
      <author>example@mail.com (Laura Jiang, Reza Ryan, Qian Li, Nasim Ferdosian)</author>
      <guid isPermaLink="false">2510.26307v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Morphology-Aware Graph Reinforcement Learning for Tensegrity Robot Locomotion</title>
      <link>http://arxiv.org/abs/2510.26067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种形态感知的强化学习框架，通过将图神经网络集成到Soft Actor-Critic算法中，解决了张力完整性机器人的运动控制问题。&lt;h4&gt;背景&lt;/h4&gt;张力完整性机器人结合刚性杆和弹性缆索，具有高弹性和可部署性，但因其欠驱动和高度耦合的动力学特性，在运动控制方面面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种强化学习方法，利用机器人的结构先验知识，提高张力完整性机器人的运动控制性能。&lt;h4&gt;方法&lt;/h4&gt;将机器人的物理拓扑表示为图，使用基于图神经网络(GNN)的策略捕捉组件间的耦合关系，并将其集成到Soft Actor-Critic (SAC)算法中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在物理三杆张力完整性机器人上得到验证，在直线跟踪和双向转向等运动任务中表现出优异的样本效率、对噪声和刚度变化的鲁棒性以及改进的轨迹精度；学习到的策略可以直接从模拟转移到硬件，无需微调。&lt;h4&gt;结论&lt;/h4&gt;将结构先验知识整合到强化学习中对于张力完整性机器人控制具有显著优势，能够实现更高效、更稳定的控制策略。&lt;h4&gt;翻译&lt;/h4&gt;张力完整性机器人结合刚性杆和弹性缆索，提供高弹性和可部署性，但由于其欠驱动和高度耦合的动力学特性，给运动控制带来了重大挑战。本文引入了一种形态感知的强化学习框架，将图神经网络(GNN)集成到Soft Actor-Critic (SAC)算法中。通过将机器人的物理拓扑表示为图，所提出的基于GNN的策略捕捉了组件之间的耦合关系，实现了比传统多层感知器(MLP)策略更快且更稳定的学习。该方法在物理三杆张力完整性机器人上得到了验证，包括直线跟踪和双向转向在内的三种运动原语。它显示出优异的样本效率、对噪声和刚度变化的鲁棒性，以及改进的轨迹精度。值得注意的是，学习到的策略可以直接从模拟转移到硬件而无需微调，实现了稳定的真实世界运动。这些结果表明，将结构先验知识整合到强化学习中对于张力完整性机器人控制具有优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tensegrity robots combine rigid rods and elastic cables, offering highresilience and deployability but posing major challenges for locomotion controldue to their underactuated and highly coupled dynamics. This paper introduces amorphology-aware reinforcement learning framework that integrates a graphneural network (GNN) into the Soft Actor-Critic (SAC) algorithm. Byrepresenting the robot's physical topology as a graph, the proposed GNN-basedpolicy captures coupling among components, enabling faster and more stablelearning than conventional multilayer perceptron (MLP) policies. The method isvalidated on a physical 3-bar tensegrity robot across three locomotionprimitives, including straight-line tracking and bidirectional turning. Itshows superior sample efficiency, robustness to noise and stiffness variations,and improved trajectory accuracy. Notably, the learned policies transferdirectly from simulation to hardware without fine-tuning, achieving stablereal-world locomotion. These results demonstrate the advantages ofincorporating structural priors into reinforcement learning for tensegrityrobot control.</description>
      <author>example@mail.com (Chi Zhang, Mingrui Li, Wenzhe Tong, Xiaonan Huang)</author>
      <guid isPermaLink="false">2510.26067v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Data-driven Projection Generation for Efficiently Solving Heterogeneous Quadratic Programming Problems</title>
      <link>http://arxiv.org/abs/2510.26061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一种数据驱动的框架，通过针对特定实例的投影减少高维二次规划问题的变量数量，使用图神经网络生成定制化投影，高效解决二次规划问题。&lt;h4&gt;背景&lt;/h4&gt;二次规划问题在高维情况下求解复杂度高，传统方法面临计算挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效解决高维二次规划问题的方法，通过减少变量数量降低计算复杂度，同时保证解的质量。&lt;h4&gt;方法&lt;/h4&gt;设计基于图神经网络的模型生成定制化投影；使用双层优化训练模型，内层优化在给定投影下解决QP问题，外层优化更新模型参数；开发高效算法计算参数梯度，无需通过求解器反向传播；提供神经网络生成投影矩阵解决QP问题的泛化能力理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;方法能产生高质量可行解并减少计算时间；即使对未见过的QP问题也能生成高质量解决方案；实验结果优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据驱动框架通过针对特定实例的投影和图神经网络模型，能高效解决高维二次规划问题，在保证解质量的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种数据驱动的框架，通过使用针对特定实例的投影来减少高维二次规划问题中的变量数量，从而高效解决二次规划问题。我们设计了一个基于图神经网络的模型，为每个二次规划实例生成定制化投影，使我们能够即使对于未见过的也能产生高质量解。该模型在异构QP上进行训练，以最小化在投影解上评估的期望目标值。这被表述为一个双层优化问题；内层优化在给定投影下使用QP求解器解决QP问题，而外层优化更新模型参数。我们开发了一种高效算法来解决这个双层优化问题，计算参数梯度时无需通过求解器进行反向传播。我们提供了使用神经网络生成的投影矩阵解决QP问题的泛化能力理论分析。实验结果表明，我们的方法产生了高质量可行解并减少了计算时间，优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-30&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a data-driven framework for efficiently solving quadraticprogramming (QP) problems by reducing the number of variables inhigh-dimensional QPs using instance-specific projection. A graph neuralnetwork-based model is designed to generate projections tailored to each QPinstance, enabling us to produce high-quality solutions even for previouslyunseen problems. The model is trained on heterogeneous QPs to minimize theexpected objective value evaluated on the projected solutions. This isformulated as a bilevel optimization problem; the inner optimization solves theQP under a given projection using a QP solver, while the outer optimizationupdates the model parameters. We develop an efficient algorithm to solve thisbilevel optimization problem, which computes parameter gradients withoutbackpropagating through the solver. We provide a theoretical analysis of thegeneralization ability of solving QPs with projection matrices generated byneural networks. Experimental results demonstrate that our method produceshigh-quality feasible solutions with reduced computation time, outperformingexisting methods.</description>
      <author>example@mail.com (Tomoharu Iwata, Futoshi Futami)</author>
      <guid isPermaLink="false">2510.26061v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Robust GNN Watermarking via Implicit Perception of Topological Invariants</title>
      <link>http://arxiv.org/abs/2510.25934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InvGNN-WM的图神经网络水印技术，它不依赖后门触发器，而是将所有权与模型对图不变性的隐式感知联系起来，实现了黑盒验证且对任务影响微小的水印方法。&lt;h4&gt;背景&lt;/h4&gt;图神经网络是有价值的知识产权，但现有水印技术大多依赖后门触发器，这些触发器在常见模型编辑下会被破坏，导致所有权模糊。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需触发器、支持黑盒验证且对任务影响微小的图神经网络水印技术，以解决现有水印技术的局限性。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级头在所有者私有的载体集上预测归一化代数连通性；使用敏感解码器输出比特；使用校准阈值控制误报率。&lt;h4&gt;主要发现&lt;/h4&gt;在多种节点和图分类数据集和主干网络上，InvGNN-WM保持了清洁准确率，同时比基线方法产生更高的水印准确率；该方法在非结构化剪枝、微调和后训练量化条件下保持强健性；纯知识蒸馏会削弱水印，而带有水印损失的知识蒸馏可以恢复水印。&lt;h4&gt;结论&lt;/h4&gt;InvGNN-WM提供了不可感知性和鲁棒性的保证，精确移除该水印被证明是NP完全问题。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是有价值的知识产权，但许多水印依赖于后门触发器，这些触发器在常见的模型编辑下会被破坏并导致所有权模糊。我们提出了InvGNN-WM，它将所有权与模型对图不变性的隐式感知联系起来，实现了无需触发器、黑盒验证且对任务影响微小的水印。轻量级头在所有者私有的载体集上预测归一化代数连通性；敏感解码器输出比特，校准阈值控制误报率。在多样化的节点和图分类数据集及主干网络上，InvGNN-WM匹配清洁准确率，同时比基于触发器和压缩的基线产生更高的水印准确率。它在非结构化剪枝、微调和后训练量化下保持强健性；普通知识蒸馏(KD)会削弱水印，而带有水印损失的KD(KD+WM)可恢复它。我们提供了不可感知性和鲁棒性的保证，并证明精确移除是NP完全的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are valuable intellectual property, yet manywatermarks rely on backdoor triggers that break under common model edits andcreate ownership ambiguity. We present InvGNN-WM, which ties ownership to amodel's implicit perception of a graph invariant, enabling trigger-free,black-box verification with negligible task impact. A lightweight head predictsnormalized algebraic connectivity on an owner-private carrier set; asign-sensitive decoder outputs bits, and a calibrated threshold controls thefalse-positive rate. Across diverse node and graph classification datasets andbackbones, InvGNN-WM matches clean accuracy while yielding higher watermarkaccuracy than trigger- and compression-based baselines. It remains strong underunstructured pruning, fine-tuning, and post-training quantization; plainknowledge distillation (KD) weakens the mark, while KD with a watermark loss(KD+WM) restores it. We provide guarantees for imperceptibility and robustness,and we prove that exact removal is NP-complete.</description>
      <author>example@mail.com (Jipeng Li, Yannning Shen)</author>
      <guid isPermaLink="false">2510.25934v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>Attention Augmented GNN RNN-Attention Models for Advanced Cybersecurity Intrusion Detection</title>
      <link>http://arxiv.org/abs/2510.25802v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型混合深度学习架构，结合图神经网络、循环神经网络和多头注意力机制，显著提升了网络安全入侵检测能力。&lt;h4&gt;背景&lt;/h4&gt;现代网络安全环境需要实时入侵检测系统，且需要将计算资源集中在高影响安全事件上。UNSW-NB15数据集包含多样的网络流量模式，为研究提供了基础。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效捕获网络流量中的空间依赖性和时间动态性的入侵检测系统，提高检测复杂攻击模式的能力。&lt;h4&gt;方法&lt;/h4&gt;提出混合深度学习架构，结合图神经网络(GNNs)捕获空间依赖性、循环神经网络(RNNs)进行序列分析，以及多头注意力机制提高模型可解释性和特征选择。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，与传统机器学习方法和独立深度学习模型相比，该混合模型在准确率、精确率、召回率和F1分数等多个评估指标上表现更优。特别是在检测高级持续性威胁(APTs)、分布式拒绝服务(DDoS)攻击和零日漏洞等复杂攻击模式方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;该混合模型是复杂网络环境中下一代网络安全应用的有前景解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种新型混合深度学习架构，协同结合图神经网络(GNNs)、循环神经网络(RNNs)和多头注意力机制，显著提升了网络安全入侵检测能力。通过利用包含多样化网络流量模式的UNSW-NB15综合数据集，我们的方法有效地通过图结构关系捕获空间依赖性，并通过网络事件的序列分析捕获时间动态性。集成的注意力机制提供了提高模型可解释性和增强特征选择的双重好处，使网络安全分析师能够将计算资源集中在高影响安全事件上——这是现代实时入侵检测系统的关键要求。我们广泛的实验评估表明，与传统机器学习方法和独立的深度学习模型相比，所提出的混合模型在多个评估指标上实现了优越的性能，包括准确率、精确率、召回率和F1分数。该模型在检测高级持续性威胁(APTs)、分布式拒绝服务(DDoS)攻击和零日漏洞等复杂攻击模式方面表现出特别强的性能，使其成为复杂网络环境中下一代网络安全应用的有前景解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a novel hybrid deep learning architecture thatsynergistically combines Graph Neural Networks (GNNs), Recurrent NeuralNetworks (RNNs), and multi-head attention mechanisms to significantly enhancecybersecurity intrusion detection capabilities. By leveraging the comprehensiveUNSW-NB15 dataset containing diverse network traffic patterns, our approacheffectively captures both spatial dependencies through graph structuralrelationships and temporal dynamics through sequential analysis of networkevents. The integrated attention mechanism provides dual benefits of improvedmodel interpretability and enhanced feature selection, enabling cybersecurityanalysts to focus computational resources on high-impact security events -- acritical requirement in modern real-time intrusion detection systems. Ourextensive experimental evaluation demonstrates that the proposed hybrid modelachieves superior performance compared to traditional machine learningapproaches and standalone deep learning models across multiple evaluationmetrics, including accuracy, precision, recall, and F1-score. The modelachieves particularly strong performance in detecting sophisticated attackpatterns such as Advanced Persistent Threats (APTs), Distributed Denial ofService (DDoS) attacks, and zero-day exploits, making it a promising solutionfor next-generation cybersecurity applications in complex network environments.</description>
      <author>example@mail.com (Jayant Biradar, Smit Shah, Tanmay Naik)</author>
      <guid isPermaLink="false">2510.25802v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>SHA-256 Infused Embedding-Driven Generative Modeling of High-Energy Molecules in Low-Data Regimes</title>
      <link>http://arxiv.org/abs/2510.25788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种结合LSTM网络和注意力GNN的新方法用于高能分子生成和属性预测，通过创新的嵌入空间构建策略实现了67.5%的有效性和37.5%的新颖性，成功识别出37种新型超爆炸物。&lt;h4&gt;背景&lt;/h4&gt;高能材料在推进和防御领域至关重要，但其发现受限于实验数据和测试设施的有限获取。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来发现高能分子，特别是高能爆炸材料。&lt;h4&gt;方法&lt;/h4&gt;结合长短期记忆网络(LSTM)进行分子生成，使用注意力图神经网络(GNN)进行属性预测，提出了一种创新的嵌入空间构建策略，整合固定的SHA-256嵌入和部分可训练表示，在学习开始前重塑分子输入空间，不依赖预训练。&lt;h4&gt;主要发现&lt;/h4&gt;生成器达到67.5%的有效性和37.5%的新颖性；生成的库相对于训练集的平均Tanimoto系数为0.214，表明框架能够生成多样化的化学空间；识别出37种新型超爆炸物，预测爆速超过9公里/秒。&lt;h4&gt;结论&lt;/h4&gt;这种新方法能够有效发现新型高能材料，特别是超爆炸物，为高能材料的研究提供了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;高能材料(HEMs)对于推进和防御领域至关重要，但其发现受限于实验数据和测试设施的有限获取。这项工作通过结合用于分子生成的长短期记忆网络(LSTM)和用于属性预测的注意力图神经网络(GNN)，提出了一种针对高能分子的新方法。我们提出了一种变革性的嵌入空间构建策略，整合固定的SHA-256嵌入和部分可训练表示。与传统的正则化技术不同，这改变了表示基础本身，在学习开始前重塑了分子输入空间。无需依赖预训练，生成器实现了67.5%的有效性和37.5%的新颖性。生成的库相对于训练集的平均Tanimoto系数为0.214，表明该框架能够生成多样化的化学空间。我们识别出37种新型超爆炸物，预测爆速超过9公里/秒。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-energy materials (HEMs) are critical for propulsion and defense domains,yet their discovery remains constrained by experimental data and restrictedaccess to testing facilities. This work presents a novel approach towardhigh-energy molecules by combining Long Short-Term Memory (LSTM) networks formolecular generation and Attentive Graph Neural Networks (GNN) for propertypredictions. We propose a transformative embedding space construction strategythat integrates fixed SHA-256 embeddings with partially trainablerepresentations. Unlike conventional regularization techniques, this changesthe representational basis itself, reshaping the molecular input space beforelearning begins. Without recourse to pretraining, the generator achieves 67.5%validity and 37.5% novelty. The generated library exhibits a mean Tanimotocoefficient of 0.214 relative to training set signifying the ability offramework to generate a diverse chemical space. We identified 37 new superexplosives higher than 9 km/s predicted detonation velocity.</description>
      <author>example@mail.com (Siddharth Verma, Alankar Alankar)</author>
      <guid isPermaLink="false">2510.25788v1</guid>
      <pubDate>Fri, 31 Oct 2025 14:58:35 +0800</pubDate>
    </item>
    <item>
      <title>4-Doodle: Text to 3D Sketches that Move!</title>
      <link>http://arxiv.org/abs/2510.25319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了4-Doodle框架，一个从文本描述生成动态3D草图动画的无需训练方法，通过双空间蒸馏方案解决文本到3D草图动画任务中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;现有3D内容生成方法主要关注逼真内容，忽视了稀疏、风格化的3D矢量草图这一轻量级媒介。该任务面临三大挑战：缺乏配对数据集、结构抽象难以建模、动画需要时间一致性和多视角一致性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需训练的框架，从文本生成动态、时间一致且多视角一致的3D矢量草图动画，实现结构稳定的动画效果，包括翻转、旋转和关节运动等。&lt;h4&gt;方法&lt;/h4&gt;4-Doodle采用双空间蒸馏方案：一个空间使用可微贝塞尔曲线捕获多视角一致的几何形状；另一个通过时间感知先验编码运动动态。采用多视图优化确保结构对齐，并引入结构感知的运动模块将保持形状的轨迹与变形感知的变化分开。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，4-Doodle能生成时间逼真且结构稳定的3D草图动画，在保真度和可控性方面优于现有基线。多视图优化确保结构对齐，结构感知运动模块实现富有表现力的动画效果。&lt;h4&gt;结论&lt;/h4&gt;4-Doodle为文本到动态3D草图动画提供了有效解决方案，使4D内容创作更加直观和易于访问，填补了相关研究空白，为视觉交流和原型设计提供新可能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一个新任务：文本到3D草图动画，旨在让自由形式的草图在动态3D空间中'活起来'。与专注于生成逼真内容的前期工作不同，我们目标是稀疏的、风格化的和视角一致的3D矢量草图，这是一种轻量级且可解释的媒介，非常适合视觉交流和原型设计。然而，这项任务非常具有挑战性：(i) 没有文本和3D（或4D）草图的配对数据集；(ii) 草图需要结构抽象，难以用传统的3D表示建模；(iii) 为这样的草图添加动画需要时间一致性和多视角一致性，而当前的处理流程无法解决这个问题。因此，我们提出了4-Doodle，这是第一个从文本生成动态3D草图的无需训练的框架。它通过双空间蒸馏方案利用预训练的图像和视频扩散模型：一个空间使用可微的贝塞尔曲线捕获多视角一致的几何形状，而另一个通过时间感知先验编码运动动态。与之前的工作不同，后者每步从单一视图进行优化，而我们的多视图优化确保了结构对齐并避免了视图模糊性，这对稀疏草图至关重要。此外，我们引入了一个结构感知的运动模块，该模块将保持形状的轨迹与变形感知的变化分开，实现翻转、旋转和关节运动等富有表现力的动作。大量实验表明，我们的方法能够生成时间上逼真且结构稳定的3D草图动画，在保真度和可控性方面都优于现有的基线。我们希望这项工作能推动更加直观和易于访问的4D内容创作发展。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从文本描述生成动态3D矢量草图的问题。这个问题很重要，因为随着空间计算平台（如Apple Vision Pro和Meta Quest）的兴起，创建和动画3D草图成为沉浸式内容创建的基础。传统的3D内容生成方法主要关注照片真实感内容，而3D草图作为一种轻量级、可解释的媒介，非常适合设计原型、视觉叙事和空间用户界面等应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将问题分解为两个互补阶段：构建一致的3D草图结构和添加动画。他们借鉴了现有工作，如DreamFusion中的Score Distillation Sampling技术、3Doodle中的贝塞尔曲线表示、LiveSketch中的运动先验蒸馏，以及MVDream等多视角扩散模型。作者的核心创新在于设计了一个双空间知识蒸馏框架，利用预训练的图像和视频扩散模型，通过多视角优化和基于贝塞尔曲线的表示来生成动态3D草图，避免了需要成对的文本-4D草图训练数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是双空间知识蒸馏框架，利用预训练的图像和视频扩散模型将3D结构和运动动力学的知识转移过来，无需特定训练数据。整体流程分为两个阶段：第一阶段是多视角3D草图生成，通过随机初始化贝塞尔曲线，从多个视角（前、后、左、右）渲染并使用Score Distillation Sampling优化曲线参数；第二阶段是运动场学习，将3D场景投影到前视图和侧视图，利用视频扩散模型预测位移序列，然后重建3D位移向量，最后添加时间平滑确保动画流畅。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首个文本到3D草图动画框架；2）基于可微分贝塞尔曲线的双空间架构；3）多视角优化策略减少歧义并确保结构对齐；4）结构感知的运动生成模块；5）投影-重建策略使视频扩散模型能在3D空间中合成运动。相比之前工作，4-Doodle不仅处理静态3D草图（如SketchDream、Sketch2NeRF），还能生成动态内容；不需要手动绘制运动轨迹（如Sketch2Anim）；专注于结构抽象和草图感知（如Animate3D、3DTopia）；支持动态草图抽象和跨视图时间一致性（如CLAY）。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 4-Doodle首次实现了从文本描述直接生成动态、空间一致且富有表现力的3D矢量草图动画，通过双空间知识蒸馏框架和基于贝塞尔曲线的可微表示，解决了草图动画中的结构一致性和时间连贯性挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel task: text-to-3D sketch animation, which aims to bringfreeform sketches to life in dynamic 3D space. Unlike prior works focused onphotorealistic content generation, we target sparse, stylized, andview-consistent 3D vector sketches, a lightweight and interpretable mediumwell-suited for visual communication and prototyping. However, this task isvery challenging: (i) no paired dataset exists for text and 3D (or 4D)sketches; (ii) sketches require structural abstraction that is difficult tomodel with conventional 3D representations like NeRFs or point clouds; and(iii) animating such sketches demands temporal coherence and multi-viewconsistency, which current pipelines do not address. Therefore, we propose4-Doodle, the first training-free framework for generating dynamic 3D sketchesfrom text. It leverages pretrained image and video diffusion models through adual-space distillation scheme: one space captures multi-view-consistentgeometry using differentiable B\'ezier curves, while the other encodes motiondynamics via temporally-aware priors. Unlike prior work (e.g., DreamFusion),which optimizes from a single view per step, our multi-view optimizationensures structural alignment and avoids view ambiguity, critical for sparsesketches. Furthermore, we introduce a structure-aware motion module thatseparates shape-preserving trajectories from deformation-aware changes,enabling expressive motion such as flipping, rotation, and articulatedmovement. Extensive experiments show that our method produces temporallyrealistic and structurally stable 3D sketch animations, outperforming existingbaselines in both fidelity and controllability. We hope this work serves as astep toward more intuitive and accessible 4D content creation.</description>
      <author>example@mail.com (Hao Chen, Jiaqi Wang, Yonggang Qi, Ke Li, Kaiyue Pang, Yi-Zhe Song)</author>
      <guid isPermaLink="false">2510.25319v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
  <item>
      <title>SynHLMA:Synthesizing Hand Language Manipulation for Articulated Object with Discrete Human Object Interaction Representation</title>
      <link>http://arxiv.org/abs/2510.25268v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SynHLMA框架，用于生成关节物体的手部语言操作序列，实现了HAOI生成、预测和插值三种任务，在HAOI-lang数据集上展示了优越性能，并可用于机器人抓取应用。&lt;h4&gt;背景&lt;/h4&gt;生成手部抓取动作是具身AI和VR/AR应用中的广泛研究课题。当涉及关节物体交互时，手部抓取合成需要同时考虑物体功能性和物体变形过程中的长期操作序列。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的HAOI序列生成框架SynHLMA，用于合成关节物体的手部语言操作。&lt;h4&gt;方法&lt;/h4&gt;给定关节物体的完整点云，使用离散的HAOI表示建模每个手部物体交互帧；结合自然语言嵌入，通过HAOI操作语言模型训练这些表示，在共享表示空间中对齐抓取过程与语言描述；采用关节感知损失确保手部抓取遵循关节物体的动态变化。&lt;h4&gt;主要发现&lt;/h4&gt;SynHLMA实现了关节物体的三种典型手部操作任务：HAOI生成、HAOI预测和HAOI插值；在HAOI-lang数据集上的评估结果显示，与最先进方法相比具有优越的手部抓取序列生成性能；通过使用SynHLMA提供的操作序列，机器人可以实现灵巧抓取的模仿学习。&lt;h4&gt;结论&lt;/h4&gt;SynHLMA框架在关节物体的手部抓取序列生成方面表现优越，代码和数据集将公开可用，为具身AI和VR/AR应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;通过语言指令生成手部抓取是一个广泛研究的课题，受益于具身AI和VR/AR应用。当转化为关节物体交互时，手部抓取合成不仅需要物体功能性，还需要考虑物体变形过程中的长期操作序列。本文提出了一个新的HAOI序列生成框架SynHLMA，用于合成关节物体的手部语言操作。给定关节物体的完整点云，我们使用离散的HAOI表示来建模每个手部物体交互帧。结合自然语言嵌入，通过HAOI操作语言模型训练这些表示，在共享表示空间中对齐抓取过程与语言描述。采用关节感知损失来确保手部抓取遵循关节物体的动态变化。通过这种方式，我们的SynHLMA实现了关节物体的三种典型手部操作任务：HAOI生成、HAOI预测和HAOI插值。我们在构建的HAOI-lang数据集上评估SynHLMA，实验结果展示了与最先进方法相比的优越手部抓取序列生成性能。我们还展示了机器抓取应用，通过使用SynHLMA提供的操作序列，使模仿学习能够执行灵巧抓取。我们的代码和数据集将公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating hand grasps with language instructions is a widely studied topicthat benefits from embodied AI and VR/AR applications. While transferring intohand articulatied object interaction (HAOI), the hand grasps synthesis requiresnot only object functionality but also long-term manipulation sequence alongthe object deformation. This paper proposes a novel HAOI sequence generationframework SynHLMA, to synthesize hand language manipulation for articulatedobjects. Given a complete point cloud of an articulated object, we utilize adiscrete HAOI representation to model each hand object interaction frame. Alongwith the natural language embeddings, the representations are trained by anHAOI manipulation language model to align the grasping process with itslanguage description in a shared representation space. A joint-aware loss isemployed to ensure hand grasps follow the dynamic variations of articulatedobject joints. In this way, our SynHLMA achieves three typical handmanipulation tasks for articulated objects of HAOI generation, HAOI predictionand HAOI interpolation. We evaluate SynHLMA on our built HAOI-lang dataset andexperimental results demonstrate the superior hand grasp sequence generationperformance comparing with state-of-the-art. We also show a robotics graspapplication that enables dexterous grasps execution from imitation learningusing the manipulation sequence provided by our SynHLMA. Our codes and datasetswill be made publicly available.</description>
      <author>example@mail.com (Wang zhi, Yuyan Liu, Liu Liu, Li Zhang, Ruixuan Lu, Dan Guo)</author>
      <guid isPermaLink="false">2510.25268v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>U-CAN: Unsupervised Point Cloud Denoising with Consistency-Aware Noise2Noise Matching</title>
      <link>http://arxiv.org/abs/2510.25210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025. Project page:  https://gloriasze.github.io/U-CAN/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为U-CAN的无监督点云去噪框架，采用一致性感知的Noise2Noise匹配方法，通过神经网络推断多步去噪路径，并引入几何一致性约束，无需大量人工标注数据即可达到与监督方法相当的去噪效果。&lt;h4&gt;背景&lt;/h4&gt;扫描传感器捕获的点云数据通常受到噪声干扰，这对下游任务（如表面重建和形状理解）有负面影响。先前的工作大多使用含噪-清洁点云对训练神经网络来学习去噪先验，这需要大量的人工努力。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需人工标注数据的无监督点云去噪方法，以减少对大量含噪-清洁点云对的依赖。&lt;h4&gt;方法&lt;/h4&gt;U-CAN框架利用神经网络推断每个点或场景的多步去噪路径，通过噪声到噪声匹配方案实现。通过一种新的损失函数，使模型能够对多个含噪点云观测进行统计推理。引入一种去噪后几何一致性约束，以学习一致性感知的去噪模式。该约束不仅限于3D领域，还可以贡献于2D图像去噪领域。&lt;h4&gt;主要发现&lt;/h4&gt;在点云去噪、上采样和图像去噪的广泛基准测试中，U-CAN比最先进的无监督方法有显著改进，并且产生的结果与监督方法相当。&lt;h4&gt;结论&lt;/h4&gt;U-CAN是一种有效的无监督点云去噪方法，不需要大量人工标注的数据，同时能够达到与监督方法相当的性能，为点云去噪领域提供了一种新的无监督解决方案。&lt;h4&gt;翻译&lt;/h4&gt;扫描传感器捕获的点云数据常常受到噪声干扰，这对下游任务（例如表面重建和形状理解）有严重的负面影响。先前的工作主要集中在使用含噪-清洁点云对训练神经网络来学习去噪先验，这需要大量的人工努力。在本工作中，我们引入了U-CAN，一种基于一致性感知的Noise2Noise匹配的无监督点云去噪框架。具体来说，我们利用神经网络推断形状或场景中每个点的多步去噪路径，采用噪声到噪声匹配方案。我们通过一种新的损失函数实现这一点，该损失函数能够在多个含噪点云观测上进行统计推理。我们进一步引入了一种对去噪后几何一致性的新约束，以学习一致性感知的去噪模式。我们证明所提出的约束是一个通用术语，不仅限于3D领域，还可以贡献于2D图像去噪领域。在点云去噪、上采样和图像去噪的广泛基准测试中，我们的评估显示比最先进的无监督方法有显著改进，其中U-CAN也产生了与监督方法相当的结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决无监督点云去噪问题。点云数据（如激光雷达扫描获取的三维点数据）通常包含噪声，影响下游任务如表面重建和形状理解。现有方法需要成对的'带噪-干净'点云数据训练，需要大量人工标注，成本高。在现实中，自动驾驶汽车、手机等设备每天都在产生大量带噪点云，而干净数据获取困难。解决这个问题能减少对人工标注的依赖，使去噪技术更容易应用于实际场景，提升自动驾驶、增强现实和机器人等领域的性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到尽管干净点云有限，但带噪点云数据每天都在快速增长。借鉴了2D图像的Noise2Noise方法，但发现点云无序不规则，没有像素间的一对一对应关系，不能直接应用。现有无监督方法如TotalDenoising只使用全局约束，难以保持局部几何结构。因此，作者设计多步去噪框架，通过神经网络为每个点推断去噪路径；提出点对点噪声到噪声匹配，使用地球移动距离建立点间对应关系；引入一致性感知约束确保不同噪声观测的去噪预测保持一致。借鉴了PointNet、PointNet++等点云处理架构和TotalDenoising的全局约束思想。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是噪声到噪声匹配和一致性感知约束。通过学习从一个带噪点云到另一个带噪点云的映射，利用统计推理从多个带噪观测中揭示干净结构；同时确保不同噪声观测的去噪预测间保持几何一致性，解决缺乏真实表面位置信息导致的收敛不稳定问题。整体流程：输入带噪点云→多步去噪框架（每步包含特征提取和路径预测）→噪声到噪声匹配（使用地球移动距离建立点间对应）→一致性感知约束（最小化不同去噪预测间的几何差异）→输出去噪后点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) U-CAN无监督框架，利用噪声到噪声匹配和一致性感知约束；2) 点对点噪声匹配方案，使用地球移动距离建立点云对应关系；3) 去噪几何一致性约束，确保不同噪声观测的去噪预测一致；4) 证明该约束不仅限于3D领域，也可用于2D图像去噪；5) 可用于无监督点云上采样任务。不同之处：相比监督方法，无需干净数据；相比TotalDenoising等，不仅用全局约束，还引入局部约束；相比直接应用Noise2Noise，解决了点云对应关系缺失问题；相比其他无监督方法，引入一致性约束解决收敛不稳定问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; U-CAN通过创新的噪声到噪声匹配和一致性感知约束，实现了仅使用带噪点云数据就能达到与监督方法相当的去噪效果，无需人工标注的干净数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds captured by scanning sensors are often perturbed by noise, whichhave a highly negative impact on downstream tasks (e.g. surface reconstructionand shape understanding). Previous works mostly focus on training neuralnetworks with noisy-clean point cloud pairs for learning denoising priors,which requires extensively manual efforts. In this work, we introduce U-CAN, anUnsupervised framework for point cloud denoising with Consistency-AwareNoise2Noise matching. Specifically, we leverage a neural network to infer amulti-step denoising path for each point of a shape or scene with a noise tonoise matching scheme. We achieve this by a novel loss which enablesstatistical reasoning on multiple noisy point cloud observations. We furtherintroduce a novel constraint on the denoised geometry consistency for learningconsistency-aware denoising patterns. We justify that the proposed constraintis a general term which is not limited to 3D domain and can also contribute tothe area of 2D image denoising. Our evaluations under the widely usedbenchmarks in point cloud denoising, upsampling and image denoising showsignificant improvement over the state-of-the-art unsupervised methods, whereU-CAN also produces comparable results with the supervised methods.</description>
      <author>example@mail.com (Junsheng Zhou, Xingyu Shi, Haichuan Song, Yi Fang, Yu-Shen Liu, Zhizhong Han)</author>
      <guid isPermaLink="false">2510.25210v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>$D^2GS$: Dense Depth Regularization for LiDAR-free Urban Scene Reconstruction</title>
      <link>http://arxiv.org/abs/2510.25173v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;D²GS是一种无LiDAR的城市场景重建框架，通过多视图深度预测和优化技术，实现了比使用LiDAR的方法更准确的几何重建。&lt;h4&gt;背景&lt;/h4&gt;高斯溅射(GS)在自动驾驶城市场景重建中显示出潜力，但现有方法依赖多模态传感器(如LiDAR和图像)，而LiDAR数据获取存在时空校准困难和空间不对齐问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需LiDAR数据的城市场景重建方法，避免获取准确LiDAR深度的困难，同时保持或提高重建质量。&lt;h4&gt;方法&lt;/h4&gt;1) 通过反向投影多视图度量深度预测初始化密集点云，并用渐进修剪策略优化；2) 利用深度基础模型的扩散先验增强高斯渲染的深度图，提供更强几何约束；3) 约束道路区域内高斯的形状和法线属性提高地面几何准确性。&lt;h4&gt;主要发现&lt;/h4&gt;在Waymo数据集上的实验表明，D²GS方法始终优于最先进方法，即使与使用真实LiDAR数据的方法相比，也能产生更准确的几何。&lt;h4&gt;结论&lt;/h4&gt;D²GS框架成功实现了无LiDAR的城市场景重建，获得了比LiDAR方法更密集、更准确的几何先验，证明了深度先验和优化策略的有效性。&lt;h4&gt;翻译&lt;/h4&gt;最近，高斯溅射(GS)在自动驾驶领域的城市场景重建中显示出巨大潜力。然而，当前的城市场景重建方法通常依赖于多模态传感器作为输入，即LiDAR和图像。虽然LiDAR点云提供的几何先验可以大大减轻重建中的不适定性，但在实践中获取准确的LiDAR数据仍然具有挑战性：i)需要LiDAR和其他传感器之间精确的时空校准，因为它们可能不会同时捕获数据；ii)当LiDAR和相机安装在不同位置时，空间不对齐会导致重投影误差。为了避免获取准确LiDAR深度的困难，我们提出了D²GS，一个无LiDAR的城市场景重建框架。在这项工作中，我们获得了与LiDAR一样有效但更密集、更准确的几何先验。首先，我们通过反向投影多视图度量深度预测来初始化密集点云。然后通过渐进修剪策略优化该点云以提高全局一致性。其次，我们通过深度增强器联合优化高斯几何和预测的密集度量深度。具体来说，我们利用来自深度基础模型的扩散先验来增强由高斯渲染的深度图。反过来，增强的深度在高斯训练期间提供更强的几何约束。最后，我们通过约束道路区域内高斯的形状和法线属性来提高地面几何的准确性。在Waymo数据集上的大量实验表明，我们的方法始终优于最先进的方法，即使与使用真实LiDAR数据的方法相比，也能产生更准确的几何。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶领域城市场景重建中对LiDAR（激光雷达）数据的依赖问题。这个问题很重要，因为获取准确的LiDAR数据在实际应用中面临诸多挑战：需要专业设备和车辆进行数据收集、传感器间需要精确的时空校准、LiDAR与相机安装在不同位置会导致重投影误差，此外LiDAR数据成本高昂且难以扩展，限制了大规模应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法对LiDAR依赖的问题，探索了替代方案。他们借鉴了多视图深度估计网络、3D Gaussian Splatting框架、扩散先验模型（如Marigold）、场景图表示方法以及道路几何先验知识。在此基础上，设计了三个关键组件：利用渐进式剪枝策略管理密集点云、通过深度增强模块迭代优化深度和高斯表示、在场景图中引入专门的道路节点约束。这些设计既吸收了现有工作的优点，又针对LiDAR-free场景进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过有效利用从图像中推导的几何先验，消除对LiDAR数据的依赖，创建一个仅使用相机输入的动态城市街道场景重建框架。整体流程分为：1)初始化阶段：使用多视图深度估计预测深度图，反投影得到点云，通过渐进式剪枝获得代表性点集；2)优化阶段：创建道路节点约束，实施联合优化策略，使用深度增强模块利用扩散先验细化深度；3)训练阶段：迭代更新高斯参数和深度估计，利用置信度图指导深度增强；4)评估阶段：在Waymo数据集上评估性能，与使用LiDAR的方法进行比较。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LiDAR-free框架，消除对LiDAR数据的需求和校准误差；2)渐进式剪枝策略，有效管理密集点云；3)基于扩散的深度增强联合优化策略，提供密集度量深度监督；4)道路节点约束，利用地面平面先验提高道路重建精度。相比之前工作，不同之处在于：不需要LiDAR数据避免校准问题；不依赖单目深度估计避免尺度模糊；能处理动态场景而多视图深度估计不能；将生成深度先验直接集成到3DGS优化循环中；提供比LiDAR更密集、更准确的几何先验。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; D²GS提出了一种无需LiDAR的城市场景重建框架，通过渐进式剪枝、深度增强和道路节点约束，仅使用相机输入就能实现比使用LiDAR数据更准确的动态城市街道场景重建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, Gaussian Splatting (GS) has shown great potential for urban scenereconstruction in the field of autonomous driving. However, current urban scenereconstruction methods often depend on multimodal sensors as inputs,\textit{i.e.} LiDAR and images. Though the geometry prior provided by LiDARpoint clouds can largely mitigate ill-posedness in reconstruction, acquiringsuch accurate LiDAR data is still challenging in practice: i) precisespatiotemporal calibration between LiDAR and other sensors is required, as theymay not capture data simultaneously; ii) reprojection errors arise from spatialmisalignment when LiDAR and cameras are mounted at different locations. Toavoid the difficulty of acquiring accurate LiDAR depth, we propose $D^2GS$, aLiDAR-free urban scene reconstruction framework. In this work, we obtaingeometry priors that are as effective as LiDAR while being denser and moreaccurate. $\textbf{First}$, we initialize a dense point cloud byback-projecting multi-view metric depth predictions. This point cloud is thenoptimized by a Progressive Pruning strategy to improve the global consistency.$\textbf{Second}$, we jointly refine Gaussian geometry and predicted densemetric depth via a Depth Enhancer. Specifically, we leverage diffusion priorsfrom a depth foundation model to enhance the depth maps rendered by Gaussians.In turn, the enhanced depths provide stronger geometric constraints duringGaussian training. $\textbf{Finally}$, we improve the accuracy of groundgeometry by constraining the shape and normal attributes of Gaussians withinroad regions. Extensive experiments on the Waymo dataset demonstrate that ourmethod consistently outperforms state-of-the-art methods, producing moreaccurate geometry even when compared with those using ground-truth LiDAR data.</description>
      <author>example@mail.com (Kejing Xia, Jidong Jia, Ke Jin, Yucai Bai, Li Sun, Dacheng Tao, Youjian Zhang)</author>
      <guid isPermaLink="false">2510.25173v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Point-level Uncertainty Evaluation of Mobile Laser Scanning Point Clouds</title>
      <link>http://arxiv.org/abs/2510.24773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于机器学习的框架，用于评估移动激光扫描点云的点级别不确定性，无需依赖高精度参考数据。&lt;h4&gt;背景&lt;/h4&gt;移动激光扫描点云中的不确定性可靠量化对3D制图、建模和变化分析等下游应用的准确性和可信度至关重要，而传统方法高度依赖难以获取的高精度参考数据。&lt;h4&gt;目的&lt;/h4&gt;解决传统不确定性建模方法依赖高精度参考数据的问题，开发一种不依赖此类数据的点级别不确定性评估方法。&lt;h4&gt;方法&lt;/h4&gt;提出基于机器学习的框架，学习局部几何特征与点级别误差之间的关系，使用随机森林和XGBoost两种集成学习模型，并在空间分区化的真实世界数据集上训练验证以避免数据泄露。&lt;h4&gt;主要发现&lt;/h4&gt;两种模型能有效捕捉几何特征与不确定性间的非线性关系，平均ROC-AUC值超过0.87；描述高程变化、点密度和局部结构复杂性的几何特征在预测不确定性中起主导作用。&lt;h4&gt;结论&lt;/h4&gt;该框架为不确定性评估提供了数据驱动的方法，为大规模点云的质量控制和误差分析提供了可扩展且适应性强的基础。&lt;h4&gt;翻译&lt;/h4&gt;移动激光扫描点云中不确定性的可靠量化对于确保3D制图、建模和变化分析等下游应用的准确性和可信度至关重要。传统的不确定性建模方法高度依赖于高精度参考数据，而这些数据在大规模情况下通常成本高昂或难以获取。为解决这一问题，本研究提出了一种基于机器学习的点级别不确定性评估框架，学习局部几何特征与点级别误差之间的关系。该框架使用随机森林和XGBoost两种集成学习模型实现，在空间分区化的真实世界数据集上进行训练和验证以避免数据泄露。实验结果表明，两种模型都能有效捕捉几何特征与不确定性之间的非线性关系，平均ROC-AUC值超过0.87。分析进一步表明，描述高程变化、点密度和局部结构复杂性的几何特征在预测不确定性中起主导作用。所提出的框架为不确定性评估提供了数据驱动的方法，为未来大规模点云的质量控制和误差分析提供了可扩展且适应性强的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决移动激光扫描点云的点级不确定性评估问题，特别是减少对高精度参考数据的依赖。这个问题很重要，因为可靠的不确定性量化对3D建模、变化分析等下游应用的准确性和可信度至关重要，而不充分评估点云质量会影响高精度应用如导航和变化分析，不仅降低可靠性，还会浪费时间和资源。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了MLS系统中的不确定性来源，评估了现有方法（前向建模和后向建模）的局限性，特别是后向建模对参考数据的依赖和高成本问题。然后提出用机器学习替代方案，建立点云特征与不确定性关系。该方法借鉴了现有工作，如使用C2C距离作为不确定性度量、基于KNN的邻域定义策略，以及采用随机森林和XGBoost等集成学习方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过机器学习学习局部几何特征与点级误差之间的关系，将不确定性评估转化为二分类问题。整体流程包括：1)使用C2C距离定义不确定性度量；2)基于KNN提取每个点的局部几何特征；3)采用随机森林和XGBoost模型进行二分类训练；4)使用多种指标和空间分区5折交叉验证评估模型性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：提出基于机器学习的框架减少对参考数据依赖；将不确定性评估转化为二分类问题；使用局部几何特征预测不确定性；采用互补的集成学习方法验证；通过特征重要性分析提供误差源新见解。相比传统方法，本研究采用数据驱动方式，训练后不再需要参考数据，提供了更可扩展和适应性强的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本研究提出了一种基于机器学习的框架，能够通过学习点云的局部几何特征与点级误差之间的关系，实现对移动激光扫描点云的点级不确定性预测，减少了对高精度参考数据的依赖，为大规模点云质量评估提供了新的数据驱动方法。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable quantification of uncertainty in Mobile Laser Scanning (MLS) pointclouds is essential for ensuring the accuracy and credibility of downstreamapplications such as 3D mapping, modeling, and change analysis. Traditionalbackward uncertainty modeling heavily rely on high-precision reference data,which are often costly or infeasible to obtain at large scales. To address thisissue, this study proposes a machine learning-based framework for point-leveluncertainty evaluation that learns the relationship between local geometricfeatures and point-level errors. The framework is implemented using twoensemble learning models, Random Forest (RF) and XGBoost, which are trained andvalidated on a spatially partitioned real-world dataset to avoid data leakage.Experimental results demonstrate that both models can effectively capture thenonlinear relationships between geometric characteristics and uncertainty,achieving mean ROC-AUC values above 0.87. The analysis further reveals thatgeometric features describing elevation variation, point density, and localstructural complexity play a dominant role in predicting uncertainty. Theproposed framework offers a data-driven perspective of uncertainty evaluation,providing a scalable and adaptable foundation for future quality control anderror analysis of large-scale point clouds.</description>
      <author>example@mail.com (Ziyang Xu, Olaf Wysocki, Christoph Holst)</author>
      <guid isPermaLink="false">2510.24773v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Controlling Contrastive Self-Supervised Learning with Knowledge-Driven Multiple Hypothesis: Application to Beat Tracking</title>
      <link>http://arxiv.org/abs/2510.25560v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种对比自监督预训练方法，利用多种可能的正样本假设来解决数据模糊性问题，在音乐节拍跟踪任务上取得了优于现有方法的性能。&lt;h4&gt;背景&lt;/h4&gt;数据中的模糊性和问题约束的多样性会导致机器学习任务产生多种同样合理的不同结果。例如在节拍和强拍跟踪中，不同听众可能采用各种节奏解释，这些解释都不一定是错误的。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理数据模糊性的方法，通过利用多种可能的正样本假设来提高机器学习模型的性能，特别是在音乐表示学习领域。&lt;h4&gt;方法&lt;/h4&gt;提出一种对比自监督预训练方法，模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。&lt;h4&gt;主要发现&lt;/h4&gt;在有标签数据上进行微调时，该方法在标准基准测试上优于现有方法，证明了将领域知识与多假设选择相结合的有效性。&lt;h4&gt;结论&lt;/h4&gt;将领域知识与多假设选择相结合在音乐表示学习中具有显著优势，能够有效处理数据中的模糊性问题并提高模型性能。&lt;h4&gt;翻译&lt;/h4&gt;数据中的模糊性和问题约束可能导致机器学习任务产生多种同样合理的不同结果。例如在节拍和强拍跟踪中，不同听众可能采用各种节奏解释，这些解释都不一定是错误的。为此，我们提出了一种对比自监督预训练方法，利用数据中可能的正样本的多种假设。我们的模型被训练为学习与不同假设兼容的表示，这些假设通过基于知识的评分函数选择，以保留最合理的假设。在有标签数据上进行微调时，我们的模型在标准基准测试上优于现有方法，展示了将领域知识与多假设选择相结合在音乐表示学习中的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ambiguities in data and problem constraints can lead to diverse, equallyplausible outcomes for a machine learning task. In beat and downbeat tracking,for instance, different listeners may adopt various rhythmic interpretations,none of which would necessarily be incorrect. To address this, we propose acontrastive self-supervised pre-training approach that leverages multiplehypotheses about possible positive samples in the data. Our model is trained tolearn representations compatible with different such hypotheses, which areselected with a knowledge-based scoring function to retain the most plausibleones. When fine-tuned on labeled data, our model outperforms existing methodson standard benchmarks, showcasing the advantages of integrating domainknowledge with multi-hypothesis selection in music representation learning inparticular.</description>
      <author>example@mail.com (Antonin Gagnere, Slim Essid, Geoffroy Peeters)</author>
      <guid isPermaLink="false">2510.25560v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>IBNorm: Information-Bottleneck Inspired Normalization for Representation Learning</title>
      <link>http://arxiv.org/abs/2510.25262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于信息瓶颈原理的新归一化方法IBNorm，通过有界压缩操作鼓励嵌入保留预测信息同时抑制无用变异性，在大规模语言模型和视觉模型上均优于传统归一化方法。&lt;h4&gt;背景&lt;/h4&gt;归一化是深度学习的基础，但现有方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差稳定训练，但没有控制表示如何捕获任务相关信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的归一化方法，能够鼓励表示保留预测信息同时抑制无用变异性，从而产生更具信息量的表示。&lt;h4&gt;方法&lt;/h4&gt;提出IB-Inspired Normalization (IBNorm)，一种基于信息瓶颈原理的简单而强大的方法系列，引入有界压缩操作来优化信息表示。&lt;h4&gt;主要发现&lt;/h4&gt;理论上证明IBNorm比方差中心方法获得更高的IB值和更紧的泛化边界；实验上在大型语言模型和视觉模型上一致优于传统归一化方法，互信息分析证实了其优越的信息瓶颈行为。&lt;h4&gt;结论&lt;/h4&gt;IBNorm能够产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性，是一种优于传统归一化方法的新方法。&lt;h4&gt;翻译&lt;/h4&gt;归一化是深度学习的基础，但现有的方法如BatchNorm、LayerNorm和RMSNorm都是方差中心的，通过强制零均值和单位方差来稳定训练，而没有控制表示如何捕获任务相关信息。我们提出了受信息瓶颈原理启发的归一化方法（IBNorm），这是一种简单而强大的方法系列。IBNorm引入了有界压缩操作，鼓励嵌入保留预测信息同时抑制无用变异性，从而产生更具信息量的表示，同时保持标准归一化的稳定性和兼容性。理论上，我们证明IBNorm比方差中心方法获得更高的IB值和更紧的泛化边界。实验上，IBNorm在大型语言模型（LLaMA、GPT-2）和视觉模型（ResNet、ViT）上一致优于BatchNorm、LayerNorm和RMSNorm，互信息分析证实了其优越的信息瓶颈行为。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Normalization is fundamental to deep learning, but existing approaches suchas BatchNorm, LayerNorm, and RMSNorm are variance-centric by enforcing zeromean and unit variance, stabilizing training without controlling howrepresentations capture task-relevant information. We propose IB-InspiredNormalization (IBNorm), a simple yet powerful family of methods grounded in theInformation Bottleneck principle. IBNorm introduces bounded compressionoperations that encourage embeddings to preserve predictive information whilesuppressing nuisance variability, yielding more informative representationswhile retaining the stability and compatibility of standard normalization.Theoretically, we prove that IBNorm achieves a higher IB value and tightergeneralization bounds than variance-centric methods. Empirically, IBNormconsistently outperforms BatchNorm, LayerNorm, and RMSNorm across large-scalelanguage models (LLaMA, GPT-2) and vision models (ResNet, ViT), with mutualinformation analysis confirming superior information bottleneck behavior. Codewill be released publicly.</description>
      <author>example@mail.com (Xiandong Zou, Pan Zhou)</author>
      <guid isPermaLink="false">2510.25262v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Improving time series estimation and prediction via transfer learning</title>
      <link>http://arxiv.org/abs/2510.25236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表示的迁移学习框架，用于解决高维度但样本量有限的时间序列数据集的估计和预测问题，通过利用相关源数据集的丰富观测信息提高估计效率。&lt;h4&gt;背景&lt;/h4&gt;许多时间序列数据集（如宏观经济变量）具有高维度但样本量有限，仅使用这些数据集本身几乎无法获得有效的估计和准确的预测。&lt;h4&gt;目的&lt;/h4&gt;引入一种基于表示的迁移学习框架用于向量自回归模型，利用相关源数据集的丰富观测信息，通过表示学习提高估计效率。&lt;h4&gt;方法&lt;/h4&gt;提出一种具有良好非渐近性质的两阶段正则化估计程序，并建议使用交替更新算法来寻找估计值。该框架能够处理具有不同样本量和异步开始/结束时间点的时间序列，灵活整合来自不同数据集的信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验评估了所提出方法的有限样本性能，并通过对日本和其他20个宏观经济变量的实证分析证明了该方法的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;该迁移学习框架解决了高维度但样本量有限的时间序列分析问题，通过利用相关源数据集的信息提高了估计效率和预测准确性。&lt;h4&gt;翻译&lt;/h4&gt;现有文献中存在许多高维度但样本量有限的时间序列，如宏观经济变量，仅使用相应的数据集本身几乎不可能获得有效的估计和准确的预测。本文通过引入一种基于表示的迁移学习框架来填补这一空白，该框架用于向量自回归模型，可以通过表示学习利用来自相关源数据集的丰富观测信息来提高估计效率。提出了一种具有良好建立的非渐近性质的两阶段正则化估计程序，并建议使用交替更新算法来寻找估计值。我们的迁移学习框架可以处理具有不同样本量和异步开始/结束时间点的时间序列，从而在整合来自不同数据集的信息方面提供了显著的灵活性。进行了模拟实验来评估所提出方法的有限样本性能，并通过分析日本和其他20个宏观经济变量的实证分析证明了其有用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There are many time series in the literature with high dimension yet limitedsample sizes, such as macroeconomic variables, and it is almost impossible toobtain efficient estimation and accurate prediction by using the correspondingdatasets themselves. This paper fills the gap by introducing a novelrepresentation-based transfer learning framework for vector autoregressivemodels, and information from related source datasets with rich observations canbe leveraged to enhance estimation efficiency through representation learning.A two-stage regularized estimation procedure is proposed with well establishednon-asymptotic properties, and algorithms with alternating updates aresuggested to search for the estimates. Our transfer learning framework canhandle time series with varying sample sizes and asynchronous starting and/orending time points, thereby offering remarkable flexibility in integratinginformation from diverse datasets. Simulation experiments are conducted toevaluate the finite-sample performance of the proposed methodology, and itsusefulness is demonstrated by an empirical analysis on 20 macroeconomicvariables from Japan and another nine countries.</description>
      <author>example@mail.com (Yuchang Lin, Qianqian Zhu, Guodong Li)</author>
      <guid isPermaLink="false">2510.25236v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Learning Fair Graph Representations with Multi-view Information Bottleneck</title>
      <link>http://arxiv.org/abs/2510.25096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FairMIB是一种多视图信息瓶颈框架，通过分解图为特征、结构和扩散视图，结合对比学习和逆概率加权邻域校正，有效减轻图神经网络中的偏见传播，在保持高任务效用的同时提高公平性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理关系数据时表现优秀，但会放大训练数据中的偏见，将歧视性属性和结构不平衡传播到不公平的结果中。现有公平性方法将偏见视为单一来源，忽略了不同的属性和结构效应，导致公平性和实用性之间的次优权衡。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时考虑属性和结构效应的框架，以减轻图神经网络中的偏见传播，实现更好的公平性和实用性权衡。&lt;h4&gt;方法&lt;/h4&gt;FairMIB是一种多视图信息瓶颈框架，将图分解为特征、结构和扩散三个视图。它使用对比学习最大化跨视图互信息实现无偏见表示学习，整合多视角条件信息瓶颈目标平衡任务效用和公平性，并在扩散视图中引入逆概率加权邻域校正减少偏见传播。&lt;h4&gt;主要发现&lt;/h4&gt;FairMIB能够有效分解并处理不同类型的偏见，通过多视图方法实现了比现有方法更好的公平性和实用性权衡。实验表明它在五个真实世界基准数据集上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;FairMIB通过多视图信息瓶颈框架和创新的偏见缓解技术，成功解决了图神经网络中的偏见问题，在不牺牲任务效用的前提下显著提高了公平性。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过在节点特征和结构上传递消息，在关系数据上表现出色，但它们会放大训练数据中的偏见，将歧视性属性和结构不平衡传播到不公平的结果中。许多公平性方法将偏见视为单一来源，忽略了不同的属性和结构效应，导致公平性和实用性之间的次优权衡。为了克服这一挑战，我们提出了FairMIB，一种多视图信息瓶颈框架，旨在将图分解为特征、结构和扩散视图，以减轻图神经网络中的复杂度偏见。特别是，所提出的FairMIB采用对比学习来最大化跨视图互信息，实现无偏见的表示学习。它进一步整合多视角条件信息瓶颈目标，通过最小化与敏感属性的互信息来平衡任务效用和公平性。此外，FairMIB在扩散视图中引入了逆概率加权邻域校正，减少了消息传递过程中偏见的传播。在五个真实世界基准数据集上的实验表明，FairMIB在效用和公平性指标上都达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) excel on relational data by passing messagesover node features and structure, but they can amplify training data biases,propagating discriminatory attributes and structural imbalances into unfairoutcomes. Many fairness methods treat bias as a single source, ignoringdistinct attribute and structure effects and leading to suboptimal fairness andutility trade-offs. To overcome this challenge, we propose FairMIB, amulti-view information bottleneck framework designed to decompose graphs intofeature, structural, and diffusion views for mitigating complexity biases inGNNs. Especially, the proposed FairMIB employs contrastive learning to maximizecross-view mutual information for bias-free representation learning. It furtherintegrates multi-perspective conditional information bottleneck objectives tobalance task utility and fairness by minimizing mutual information withsensitive attributes. Additionally, FairMIB introduces an inverseprobability-weighted (IPW) adjacency correction in the diffusion view, whichreduces the spread of bias propagation during message passing. Experiments onfive real-world benchmark datasets demonstrate that FairMIB achievesstate-of-the-art performance across both utility and fairness metrics.</description>
      <author>example@mail.com (Chuxun Liu, Debo Cheng, Qingfeng Chen, Jiangzhang Gan, Jiuyong Li, Lin Liu)</author>
      <guid isPermaLink="false">2510.25096v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Topic Analysis with Side Information: A Neural-Augmented LDA Approach</title>
      <link>http://arxiv.org/abs/2510.24918v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;nnLDA是一种创新的神经增强概率主题模型，通过神经先验机制动态整合辅助信息，解决了传统主题模型难以融入元数据、用户属性或文档标签等辅助信息的局限性，在多个基准数据集上表现出色。&lt;h4&gt;背景&lt;/h4&gt;传统主题模型如LDA被广泛用于揭示文本语料库中的潜在结构，但这些模型往往难以整合辅助信息如元数据、用户属性或文档标签，限制了它们的表现力、个性化和可解释性。&lt;h4&gt;目的&lt;/h4&gt;提出nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息，以克服传统主题模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成。这种设计使模型能够捕获辅助信息和主题分布之间复杂的非线性交互，并开发了随机变分期望最大化算法来联合优化神经和概率组件。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面持续优于LDA和Dirichlet-Multinomial Regression。&lt;h4&gt;结论&lt;/h4&gt;当辅助信息可用时，结合神经表示学习和概率主题建模能够带来显著优势，nnLDA证明了这种混合方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;传统的主题模型如潜在狄利克雷分配（LDA）已被广泛用于揭示文本语料库中的潜在结构，但它们往往难以整合辅助信息，如元数据、用户属性或文档标签。这些局限性限制了它们的表现力、个性化和可解释性。为此，我们提出了nnLDA，一种神经增强的概率主题模型，通过神经先验机制动态整合辅助信息。nnLDA将每个文档建模为潜在主题的混合，其中主题比例的先验由基于辅助特征的神经网络生成。这种设计使模型能够捕获辅助信息和主题分布之间复杂的非线性交互，这是静态狄利克雷先验无法表示的。我们开发了一种随机变分期望最大化算法来联合优化神经和概率组件。在多个基准数据集上，nnLDA在主题一致性、困惑度和下游分类方面持续优于LDA和狄利克雷-多项式回归。这些结果强调了在辅助信息可用的情况下，结合神经表示学习和概率主题建模的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional topic models such as Latent Dirichlet Allocation (LDA) have beenwidely used to uncover latent structures in text corpora, but they oftenstruggle to integrate auxiliary information such as metadata, user attributes,or document labels. These limitations restrict their expressiveness,personalization, and interpretability. To address this, we propose nnLDA, aneural-augmented probabilistic topic model that dynamically incorporates sideinformation through a neural prior mechanism. nnLDA models each document as amixture of latent topics, where the prior over topic proportions is generatedby a neural network conditioned on auxiliary features. This design allows themodel to capture complex nonlinear interactions between side information andtopic distributions that static Dirichlet priors cannot represent. We develop astochastic variational Expectation-Maximization algorithm to jointly optimizethe neural and probabilistic components. Across multiple benchmark datasets,nnLDA consistently outperforms LDA and Dirichlet-Multinomial Regression intopic coherence, perplexity, and downstream classification. These resultshighlight the benefits of combining neural representation learning withprobabilistic topic modeling in settings where side information is available.</description>
      <author>example@mail.com (Biyi Fang, Kripa Rajshekhar, Truong Vo, Diego Klabjan)</author>
      <guid isPermaLink="false">2510.24918v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Transformers from Compressed Representations</title>
      <link>http://arxiv.org/abs/2510.23665v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TEMPEST是一种利用压缩文件字节流结构进行表示学习的方法，通过紧凑编码实现高效语义表示，同时保持与最先进方法相当的准确性。&lt;h4&gt;背景&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但其在表示学习方面的潜力尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一种能够利用压缩文件固有字节流结构进行有效标记化和编码策略的方法，直接从压缩数据流中学习语义表示。&lt;h4&gt;方法&lt;/h4&gt;TEMPEST（TransformErs froM comPressed rEpreSenTations）利用压缩文件的固有字节流结构设计标记化和编码策略，使标准transformer可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完整媒体解码的需要。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPEST显著减少了语义分类所需的标记数量，降低了计算复杂性和内存使用；在多个数据集、编码方案和模态的实验中，实现了与最先进方法相当的准确性，同时在内存和计算方面提高了效率。&lt;h4&gt;结论&lt;/h4&gt;TEMPEST是一种有效的方法，可以从压缩数据中学习语义表示，在保持准确性的同时提高了效率。&lt;h4&gt;翻译&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但其在表示学习方面的潜力仍未被充分探索。我们引入了TEMPEST（一种基于压缩表示的transformer方法），它利用压缩文件的固有字节流结构来设计有效的标记化和编码策略。通过利用这种紧凑编码，标准的transformer可以直接从压缩数据流中学习语义表示，绕过了原始字节级处理或完整媒体解码的需要。我们的提议显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的广泛实验，我们表明TEMPEST实现了与最先进方法相当的准确性，同时在内存和计算方面带来了效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compressed file formats are the corner stone of efficient data storage andtransmission, yet their potential for representation learning remains largelyunderexplored. We introduce TEMPEST (TransformErs froM comPressedrEpreSenTations), a method that exploits the inherent byte-stream structure ofcompressed files to design an effective tokenization and encoding strategy. Byleveraging this compact encoding, a standard transformer can directly learnsemantic representations from compressed data streams, bypassing the need forraw byte-level processing or full media decoding. Our proposal substantiallyreduces the number of tokens required for semantic classification, therebylowering both computational complexity and memory usage. Through extensiveexperiments across diverse datasets, coding schemes, and modalities, we showthat TEMPEST achieves accuracy competitive wit the state-of-the-art whiledelivering efficiency gains in memory and compute.</description>
      <author>example@mail.com (Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem)</author>
      <guid isPermaLink="false">2510.23665v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Enhanced Multimodal Fusion of Eye-Tracking and Facial Features for Alzheimer's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2510.24777v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 8 figures, and 7 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种多模态交叉增强融合框架，通过整合眼动追踪和面部特征进行阿尔茨海默病诊断，并在包含25名AD患者和25名健康对照者的数据集上实现了95.11%的分类准确率。&lt;h4&gt;背景&lt;/h4&gt;准确诊断阿尔茨海默病对及时干预和减缓疾病进展至关重要。多模态诊断方法通过整合行为和感知领域的互补信息显示出巨大潜力，而眼动追踪和面部特征是认知功能的重要指标，但很少有研究探索它们的联合集成用于辅助AD诊断。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够协同利用眼动追踪和面部特征进行AD检测的多模态交叉增强融合框架，提高诊断性能的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出一个包含两个关键模块的多模态框架：(a)交叉增强融合注意力模块(CEFAM)，通过交叉注意力和全局增强建模模态间交互；(b)方向感知卷积模块(DACM)，通过水平-垂直感受野捕获细粒度方向性面部特征。同时构建了一个同步多模态数据集，包括AD患者和健康对照者在视觉记忆搜索范式中的面部视频和眼动追踪序列。&lt;h4&gt;主要发现&lt;/h4&gt;在构建的数据集上，该框架优于传统的后期融合和特征连接方法，在区分AD和健康对照者方面实现了95.11%的分类准确率，通过明确建模模态间依赖性和模态特定贡献，显示出卓越的鲁棒性和诊断性能。&lt;h4&gt;结论&lt;/h4&gt;多模态交叉增强融合框架通过协同整合眼动追踪和面部特征，能够有效提高阿尔茨海默病的诊断准确性和鲁棒性，为AD的辅助诊断提供了新的方法。&lt;h4&gt;翻译&lt;/h4&gt;阿尔茨海默病(AD)的准确诊断对于实现及时干预和减缓疾病进展至关重要。多模态诊断方法通过整合行为和感知领域的互补信息显示出巨大潜力。特别是，眼动追踪和面部特征是认知功能的重要指标，反映了注意力分布和神经认知状态。然而，很少有研究探索它们的联合集成用于辅助AD诊断。在本研究中，我们提出了一种多模态交叉增强融合框架，通过协同利用眼动追踪和面部特征进行AD检测。该框架包含两个关键模块：(a)交叉增强融合注意力模块(CEFAM)，通过交叉注意力和全局增强建模模态间交互；(b)方向感知卷积模块(DACM)，通过水平-垂直感受野捕获细粒度方向性面部特征。这些模块共同实现了自适应和判别性多模态表示学习。为支持这项工作，我们构建了一个同步多模态数据集，包括25名AD患者和25名健康对照者(HC)，通过在视觉记忆搜索范式期间记录对齐的面部视频和眼动追踪序列，为评估集成策略提供了生态有效资源。在该数据集上的大量实验表明，我们的框架优于传统的后期融合和特征连接方法，在区分AD和HC方面实现了95.11%的分类准确率，通过明确建模模态间依赖性和模态特定贡献，突显了卓越的鲁棒性和诊断性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate diagnosis of Alzheimer's disease (AD) is essential for enablingtimely intervention and slowing disease progression. Multimodal diagnosticapproaches offer considerable promise by integrating complementary informationacross behavioral and perceptual domains. Eye-tracking and facial features, inparticular, are important indicators of cognitive function, reflectingattentional distribution and neurocognitive state. However, few studies haveexplored their joint integration for auxiliary AD diagnosis. In this study, wepropose a multimodal cross-enhanced fusion framework that synergisticallyleverages eye-tracking and facial features for AD detection. The frameworkincorporates two key modules: (a) a Cross-Enhanced Fusion Attention Module(CEFAM), which models inter-modal interactions through cross-attention andglobal enhancement, and (b) a Direction-Aware Convolution Module (DACM), whichcaptures fine-grained directional facial features via horizontal-verticalreceptive fields. Together, these modules enable adaptive and discriminativemultimodal representation learning. To support this work, we constructed asynchronized multimodal dataset, including 25 patients with AD and 25 healthycontrols (HC), by recording aligned facial video and eye-tracking sequencesduring a visual memory-search paradigm, providing an ecologically validresource for evaluating integration strategies. Extensive experiments on thisdataset demonstrate that our framework outperforms traditional late fusion andfeature concatenation methods, achieving a classification accuracy of 95.11% indistinguishing AD from HC, highlighting superior robustness and diagnosticperformance by explicitly modeling inter-modal dependencies andmodality-specific contributions.</description>
      <author>example@mail.com (Yujie Nie, Jianzhang Ni, Yonglong Ye, Yuan-Ting Zhang, Yun Kwok Wing, Xiangqing Xu, Xin Ma, Lizhou Fan)</author>
      <guid isPermaLink="false">2510.24777v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>WBT-BGRL: A Non-Contrastive Weighted Bipartite Link Prediction Model for Inductive Learning</title>
      <link>http://arxiv.org/abs/2510.24927v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, submitted to the 12th International Conference on Soft  Computing and Machine Intelligence (ISCMI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了WBT-BGRL框架，一种用于二分图链接预测的加权非对比学习方法，通过三元损失中的加权机制增强自举学习，在真实数据集上展示了有竞争力的性能。&lt;h4&gt;背景&lt;/h4&gt;二分图链接预测对推荐系统和故障检测等应用至关重要，但研究较少；对比方法在负采样上效率低且偏差大，非对比方法仅依赖正样本；现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中效果未验证。&lt;h4&gt;目的&lt;/h4&gt;解决现有二分图链接预测方法的局限性，特别是在归纳、加权和二分场景中的有效性问题。&lt;h4&gt;方法&lt;/h4&gt;提出加权二分图三元自举图潜在表示(WBT-BGRL)，采用非对比框架，通过三元损失中的新加权机制增强自举学习；使用具有双GCN编码器的二分架构；与适配的最先进模型(T-BGRL, BGRL, GBT, CCA-SSG)进行比较评估。&lt;h4&gt;主要发现&lt;/h4&gt;在工业和电子商务真实数据集上，WBT-BGRL展现出有竞争力的性能，特别是在预训练过程中应用加权时效果更佳，突显了加权的非对比学习在二分图归纳链接预测中的价值。&lt;h4&gt;结论&lt;/h4&gt;加权的非对比学习对于二分图中的归纳链接预测具有重要价值，WBT-BGRL框架为此提供了有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;二分图中的链接预测对于推荐系统和故障检测等应用至关重要，但相比单分图的研究较少。对比方法在负采样方面效率低下且存在偏差，而非对比方法仅依赖正样本。现有模型在直推式设置中表现良好，但在归纳式、加权和二分场景中的有效性尚未得到验证。为解决这一问题，我们提出了加权二分图三元自举图潜在表示(WBT-BGRL)，这是一种非对比框架，通过三元损失中的新加权机制增强自举学习。使用具有双GCN编码器的二分架构，将WBT-BGRL与适配的最先进模型(T-BGRL, BGRL, GBT, CCA-SSG)进行比较评估。在工业和电子商务真实世界数据集上的结果显示了具有竞争力的性能，特别是在预训练过程中应用加权时，突显了加权的非对比学习在二分图归纳链接预测中的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction in bipartite graphs is crucial for applications likerecommendation systems and failure detection, yet it is less studied than inmonopartite graphs. Contrastive methods struggle with inefficient and biasednegative sampling, while non-contrastive approaches rely solely on positivesamples. Existing models perform well in transductive settings, but theireffectiveness in inductive, weighted, and bipartite scenarios remains untested.To address this, we propose Weighted Bipartite Triplet-Bootstrapped GraphLatents (WBT-BGRL), a non-contrastive framework that enhances bootstrappedlearning with a novel weighting mechanism in the triplet loss. Using abipartite architecture with dual GCN encoders, WBT-BGRL is evaluated againstadapted state-of-the-art models (T-BGRL, BGRL, GBT, CCA-SSG). Results onreal-world datasets (Industry and E-commerce) show competitive performance,especially when weighting is applied during pretraining-highlighting the valueof weighted, non-contrastive learning for inductive link prediction inbipartite graphs.</description>
      <author>example@mail.com (Joel Frank Huarayo Quispe, Lilian Berton, Didier Vega-Oliveros)</author>
      <guid isPermaLink="false">2510.24927v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Improving Temporal Consistency and Fidelity at Inference-time in Perceptual Video Restoration by Zero-shot Image-based Diffusion Models</title>
      <link>http://arxiv.org/abs/2510.25420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出两种无需训练的推理时策略，以改善基于零样本图像扩散模型的时间一致性视频修复，通过感知直线引导(PSG)和多路径集成采样(MPES)技术，实现时间稳定的高保真感知视频修复。&lt;h4&gt;背景&lt;/h4&gt;扩散模型已成为单图像修复的强大先验，但由于采样的随机性和整合显式时间建模的复杂性，将其应用于零样本视频修复时存在时间一致性问题。&lt;h4&gt;目的&lt;/h4&gt;在不重新训练或修改预训练扩散模型架构的情况下，提高视频修复中的时间一致性，实现时间稳定的高保真感知视频修复。&lt;h4&gt;方法&lt;/h4&gt;提出两种互补的推理时策略：(1)感知直线引导(PSG)：基于神经科学启发的感知直线假设，通过在感知空间中引入曲率惩罚，引导扩散去噪过程向更平滑的时间演化发展；(2)多路径集成采样(MPES)：通过集成多个扩散轨迹来减少随机变化，提高保真度分数而不牺牲清晰度。&lt;h4&gt;主要发现&lt;/h4&gt;PSG增强了时间自然性，特别是在时间模糊的情况下；MPES在所有任务中一致提高了保真度和时空感知-失真权衡。这两种策略无需重新训练或修改模型架构即可实现显著改进。&lt;h4&gt;结论&lt;/h4&gt;这些无需训练的技术为使用大型预训练扩散模型实现时间稳定的高保真感知视频修复提供了实用路径，通过结合PSG和MPES可以同时改善时间自然性和保真度。&lt;h4&gt;翻译&lt;/h4&gt;扩散模型已成为单图像修复的强大先验，但将其应用于零样本视频修复时，由于采样的随机性和整合显式时间建模的复杂性，存在时间一致性问题。在本工作中，我们解决了在不重新训练或修改其架构的情况下，使用零样本基于图像的扩散模型提高视频修复时间一致性的挑战。我们提出了两种互补的推理时策略：(1)基于神经科学启发的感知直线假设的感知直线引导(PSG)，通过在感知空间中引入曲率惩罚来引导扩散去噪过程向更平滑的时间演化发展，以改善时间感知分数，如Fréchet视频距离(FVD)和感知直线度；(2)多路径集成采样(MPES)，旨在通过集成多个扩散轨迹来减少随机变化，提高保真度(失真)分数，如PSNR和SSIM，而不牺牲清晰度。这些无需训练的技术共同为使用大型预训练扩散模型实现时间稳定的高保真感知视频修复提供了实用路径。我们在多个数据集和退化类型上进行了广泛实验，系统评估了每种策略以了解其优势和局限性。我们的结果表明，虽然PSG增强了时间自然性，特别是在时间模糊的情况下，但MPES在所有任务中一致提高了保真度和时空感知-失真权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models have emerged as powerful priors for single-imagerestoration, but their application to zero-shot video restoration suffers fromtemporal inconsistencies due to the stochastic nature of sampling andcomplexity of incorporating explicit temporal modeling. In this work, weaddress the challenge of improving temporal coherence in video restorationusing zero-shot image-based diffusion models without retraining or modifyingtheir architecture. We propose two complementary inference-time strategies: (1)Perceptual Straightening Guidance (PSG) based on the neuroscience-inspiredperceptual straightening hypothesis, which steers the diffusion denoisingprocess towards smoother temporal evolution by incorporating a curvaturepenalty in a perceptual space to improve temporal perceptual scores, such asFr\'echet Video Distance (FVD) and perceptual straightness; and (2) Multi-PathEnsemble Sampling (MPES), which aims at reducing stochastic variation byensembling multiple diffusion trajectories to improve fidelity (distortion)scores, such as PSNR and SSIM, without sacrificing sharpness. Together, thesetraining-free techniques provide a practical path toward temporally stablehigh-fidelity perceptual video restoration using large pretrained diffusionmodels. We performed extensive experiments over multiple datasets anddegradation types, systematically evaluating each strategy to understand theirstrengths and limitations. Our results show that while PSG enhances temporalnaturalness, particularly in case of temporal blur, MPES consistently improvesfidelity and spatio-temporal perception--distortion trade-off across all tasks.</description>
      <author>example@mail.com (Nasrin Rahimi, A. Murat Tekalp)</author>
      <guid isPermaLink="false">2510.25420v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>StreamingCoT: A Dataset for Temporal Dynamics and Multimodal Chain-of-Thought Reasoning in Streaming VideoQA</title>
      <link>http://arxiv.org/abs/2510.25332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;StreamingCoT是首个专为流媒体视频问答中的时间演化和多模态思维链任务设计的数据集，解决了现有VideoQA数据集无法捕捉时间动态和缺少明确推理过程标注的问题。&lt;h4&gt;背景&lt;/h4&gt;流媒体视频应用的快速增长需要具有增强时间动态理解和复杂推理能力多模态模型，但当前VideoQA数据集存在静态标注机制无法捕捉视频流中答案的演变性质，以及缺少明确推理过程标注两大关键限制。&lt;h4&gt;目的&lt;/h4&gt;解决现有VideoQA数据集的两个关键限制：1)静态标注机制无法捕捉时间视频流中答案的演变性质；2)缺少明确的推理过程标注，限制了模型的可解释性和逻辑推理能力。&lt;h4&gt;方法&lt;/h4&gt;建立动态分层标注架构，生成每秒密集描述并通过相似性融合构建时间依赖的语义段，加入受时间演化模式约束的问题-答案集；提出明确推理链生成范式，通过关键帧语义提取时空对象，使用大语言模型基于对象状态转换推导推理路径，并通过人工验证确保逻辑一致性。&lt;h4&gt;主要发现&lt;/h4&gt;通过StreamingCoT数据集的构建，为推进流媒体视频理解、复杂时间推理和多模态推理研究奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;StreamingCoT数据集及其构建工具包为解决流媒体视频理解中的时间动态和复杂推理问题提供了有效支持，相关资源已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;流媒体视频应用的快速增长需要具有增强时间动态理解和复杂推理能力多模态模型。然而，当前视频问答(VideoQA)数据集存在两个关键限制：1)静态标注机制无法捕捉时间视频流中答案的演变性质；2)缺少明确的推理过程标注，限制了模型的可解释性和逻辑推理能力。为解决这些挑战，我们引入了StreamingCoT，这是首个专为流媒体视频问答中的时间演化推理和多模态思维链(CoT)任务设计的数据集。我们的框架首先建立了动态分层标注架构，生成每秒密集描述并通过相似性融合构建时间依赖的语义段，同时加入受时间演化模式约束的问题-答案集。我们进一步提出了明确的推理链生成范式，通过关键帧语义对齐提取时空对象，使用大语言模型基于对象状态转换推导推理路径，并通过人工验证确保逻辑一致性。该数据集为推进流媒体视频理解、复杂时间推理和多模态推理研究奠定了基础。我们的StreamingCoT及其构建工具包可在https://github.com/Fleeting-hyh/StreamingCoT获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758311&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of streaming video applications demands multimodal modelswith enhanced capabilities for temporal dynamics understanding and complexreasoning. However, current Video Question Answering (VideoQA) datasets sufferfrom two critical limitations: 1) Static annotation mechanisms fail to capturethe evolving nature of answers in temporal video streams, and 2) The absence ofexplicit reasoning process annotations restricts model interpretability andlogical deduction capabilities. To address these challenges, We introduceStreamingCoT, the first dataset explicitly designed for temporally evolvingreasoning in streaming VideoQA and multimodal Chain-of-Thought (CoT) tasks. Ourframework first establishes a dynamic hierarchical annotation architecture thatgenerates per-second dense descriptions and constructs temporally-dependentsemantic segments through similarity fusion, paired with question-answer setsconstrained by temporal evolution patterns. We further propose an explicitreasoning chain generation paradigm that extracts spatiotemporal objects viakeyframe semantic alignment, derives object state transition-based reasoningpaths using large language models, and ensures logical coherence throughhuman-verified validation. This dataset establishes a foundation for advancingresearch in streaming video understanding, complex temporal reasoning, andmultimodal inference. Our StreamingCoT and its construction toolkit can beaccessed at https://github.com/Fleeting-hyh/StreamingCoT.</description>
      <author>example@mail.com (Yuhang Hu, Zhenyu Yang, Shihan Wang, Shengsheng Qian, Bin Wen, Fan Yang, Tingting Gao, Changsheng Xu)</author>
      <guid isPermaLink="false">2510.25332v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks</title>
      <link>http://arxiv.org/abs/2510.25760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述文章全面回顾了大型多模态空间推理模型在各类任务中的进展，分类了多模态大语言模型的最新研究，并引入了开放基准进行评估。&lt;h4&gt;背景&lt;/h4&gt;人类具有空间推理能力，能够通过视觉和声音等多模态观察来理解空间。大型多模态推理模型扩展了这些能力，在各种空间任务中展现出有希望的性能，但系统性综述和公开可用的基准仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提供对大型多模态空间推理任务的全面回顾，分类多模态大语言模型的进展，并引入用于评估的开放基准。&lt;h4&gt;方法&lt;/h4&gt;文章首先概述通用空间推理，重点关注训练后技术、可解释性和架构。研究内容包括空间关系推理、场景和布局理解、3D空间中的视觉问答和定位，以及具身AI（如视觉语言导航和动作模型）。此外还探讨了音频和第一人称视频等新兴模态对空间理解的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;多模态空间推理模型在2D和3D空间理解、视觉问答、具身AI等任务中展现出有希望的性能。音频和第一人称视频等新兴模态通过新传感器为空间理解提供了新的视角。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有空间推理能力，使他们能够通过多模态观察（如视觉和声音）来理解空间。大型多模态推理模型通过学习感知和推理扩展了这些能力，在各种空间任务中展现出有希望的性能。然而，对这些模型的系统性综述和公开可用的基准仍然有限。在本综述中，我们对大型多模态空间推理任务进行了全面回顾，分类了多模态大语言模型的最新进展，并引入了用于评估的开放基准。我们首先概述了通用的空间推理，重点关注训练后技术、可解释性和架构。除了传统的2D任务外，我们还研究了空间关系推理、场景和布局理解，以及3D空间中的视觉问答和定位。我们还回顾了具身AI的进展，包括视觉语言导航和动作模型。此外，我们还考虑了音频和第一人称视频等新兴模态，这些模态通过新传感器为空间理解做出贡献。我们相信本综述为多模态空间推理这一不断发展的领域奠定了坚实的基础，并提供了有价值的见解。关于本综述的更新信息、开放基准的代码和实现可以在https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning找到。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决多模态空间推理在大型模型时代缺乏系统综述和公开基准测试的问题。这个问题很重要，因为人类通过视觉、声音等多模态输入理解空间的能力是基础性的，而大型语言模型虽然文本处理能力强，但空间推理能力有限。整合多模态信息增强空间推理对机器人导航、增强现实、自动驾驶等现实应用至关重要，同时缺乏系统评估阻碍了该领域的标准化发展和比较研究。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性回顾和分析现有文献构建了这篇综述。他们首先定义多模态空间推理，然后分类各类空间任务（从2D到3D，从静态到动态），分析技术进展（测试时扩展、后训练方法、架构修改等），最后引入评估基准。作者确实借鉴了大量现有工作，如Wang等人的小型推理模型研究、Ke等人的推理扩展分析、Zha等人的3D能力研究等，但指出这些工作要么未深入多模态空间推理，要么缺乏系统评估框架，因此他们的综述填补了这一空白。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提供首个多模态空间推理在大型模型时代的全面综述，建立系统分类框架，并引入开放基准。整体流程：1)定义多模态空间推理并概述评估维度；2)分析一般多模态空间推理技术（测试时扩展、后训练、架构修改等）；3)探讨3D空间中的核心任务（视觉定位、场景推理、3D生成）；4)讨论具身AI中的空间推理；5)考虑音频和第一人称视频等新兴模态；6)提供开放基准和评估框架。作者还通过GitHub仓库提供代码和最新信息，方便研究实践。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)构建首个专门针对多模态空间推理的系统综述框架；2)建立详细任务分类体系，涵盖从2D到3D、静态到动态、视觉到其他模态的广泛任务；3)引入开放基准标准化评估；4)整合音频和第一人称视频等新兴模态；5)提供跨领域视角连接传统2D理解与3D推理、具身AI等。相比之前工作，本文专注多模态空间推理而非一般推理，提供系统性评估框架而非单一任务分析，引入开放基准而非仅文献回顾，并通过GitHub提供实用资源，更全面且实用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文为多模态空间推理在大型模型时代提供了首个全面的综述框架，系统性地分类了各类空间任务，引入了开放评估基准，并通过整合新兴模态为该领域的研究和实践奠定了坚实基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess spatial reasoning abilities that enable them to understandspaces through multimodal observations, such as vision and sound. Largemultimodal reasoning models extend these abilities by learning to perceive andreason, showing promising performance across diverse spatial tasks. However,systematic reviews and publicly available benchmarks for these models remainlimited. In this survey, we provide a comprehensive review of multimodalspatial reasoning tasks with large models, categorizing recent progress inmultimodal large language models (MLLMs) and introducing open benchmarks forevaluation. We begin by outlining general spatial reasoning, focusing onpost-training techniques, explainability, and architecture. Beyond classical 2Dtasks, we examine spatial relationship reasoning, scene and layoutunderstanding, as well as visual question answering and grounding in 3D space.We also review advances in embodied AI, including vision-language navigationand action models. Additionally, we consider emerging modalities such as audioand egocentric video, which contribute to novel spatial understanding throughnew sensors. We believe this survey establishes a solid foundation and offersinsights into the growing field of multimodal spatial reasoning. Updatedinformation about this survey, codes and implementation of the open benchmarkscan be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.</description>
      <author>example@mail.com (Xu Zheng, Zihao Dongfang, Lutao Jiang, Boyuan Zheng, Yulong Guo, Zhenquan Zhang, Giuliano Albanese, Runyi Yang, Mengjiao Ma, Zixin Zhang, Chenfei Liao, Dingcheng Zhen, Yuanhuiyi Lyu, Yuqian Fu, Bin Ren, Linfeng Zhang, Danda Pani Paudel, Nicu Sebe, Luc Van Gool, Xuming Hu)</author>
      <guid isPermaLink="false">2510.25760v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>EA3D: Online Open-World 3D Object Extraction from Streaming Videos</title>
      <link>http://arxiv.org/abs/2510.25146v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The Thirty-Ninth Annual Conference on Neural Information Processing  Systems(NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ExtractAnything3D (EA3D)，一个统一的在线开放世界3D物体提取框架，能够同时实现几何重建和整体场景理解。&lt;h4&gt;背景&lt;/h4&gt;当前3D场景理解方法受限于离线收集的多视图数据或预构建的3D几何形状。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的在线框架，用于开放世界的3D物体提取，同时实现几何重建和整体场景理解。&lt;h4&gt;方法&lt;/h4&gt;EA3D通过视觉语言和2D视觉基础编码器动态解释每个视频帧提取物体级知识，使用前馈在线更新策略将知识集成到高斯特征图中，从历史帧迭代估计视觉里程计并增量更新高斯特征，通过循环联合优化模块引导模型关注感兴趣区域。&lt;h4&gt;主要发现&lt;/h4&gt;EA3D在多样化基准测试和任务中表现出色，包括照片级真实感渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成。&lt;h4&gt;结论&lt;/h4&gt;EA3D建立了统一的、高效的框架，用于联合在线3D重建和整体场景理解，能够支持广泛的下游任务。&lt;h4&gt;翻译&lt;/h4&gt;当前的3D场景理解方法受限于离线收集的多视图数据或预构建的3D几何形状。在本文中，我们提出了ExtractAnything3D (EA3D)，一个统一的在线开放世界3D物体提取框架，能够同时实现几何重建和整体场景理解。给定流式视频，EA3D使用视觉语言和2D视觉基础编码器动态解释每个帧，提取物体级知识。这些知识通过前馈在线更新策略被集成并嵌入到高斯特征图中。然后我们从历史帧迭代估计视觉里程计，并用新观察增量更新在线高斯特征。循环联合优化模块引导模型关注感兴趣区域，同时增强几何重建和语义理解。在多样化的基准测试和任务中的大量实验，包括照片级真实感渲染、语义和实例分割、3D边界框和语义占用估计以及3D网格生成，证明了EA3D的有效性。我们的方法为联合在线3D重建和整体场景理解建立了统一且高效的框架，能够支持广泛的下游任务。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何从流式视频中实时提取和理解开放世界中的3D物体问题。这个问题在现实中非常重要，因为自主智能体（如机器人）需要在陌生环境中实时理解和重建周围环境，而现实世界中的场景是开放的，物体种类和数量未知，需要同时处理流式视频输入并理解物体的几何结构和语义信息。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到现有的视觉语言模型在2D开放世界理解上表现出色，但在3D领域存在视角不一致和几何错位问题。他们发现直接将2D模型提升到3D的方法需要预构建的几何和标注数据，而现有的可微分渲染框架又需要完整的多视图图像。受人类感知启发，作者设计了EA3D，使其能像人类一样进入环境时立即开始处理视觉输入。该方法借鉴了视觉语言模型进行开放世界解释，利用视觉基础模型提取特征，基于高斯泼溅构建在线表示，并参考了在线视觉里程计和高斯更新的方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是建立一个统一的在线开放世界3D物体提取框架，能同时进行几何重建和场景理解，无需几何或姿态先验。整体流程包括：1)知识提取与集成：使用VLMs识别物体，维护语义缓存，利用VFMs提取特征并嵌入高斯表示；2)在线3D物体提取：通过在线视觉里程计估计相机姿态，利用在线高斯更新重建几何和理解语义；3)循环联合优化：设计语义感知正则化，联合优化高斯特征和相机姿态，结合多种损失函数提升重建和理解质量。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的在线开放世界3D物体提取框架，能同时进行重建和理解；2)在线特征高斯表示，结合在线视觉里程计和高斯更新；3)循环联合优化策略，动态引导模型注意力；4)支持多种下游任务。相比之前的工作，EA3D的不同之处在于：它能在线处理流式视频而非依赖完整多视图；能处理开放世界中的未知物体类别；无需预构建几何或姿态先验；提供支持多种任务的统一框架而非专注于单一任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; EA3D提出了一种统一的在线框架，能够从流式视频中实时提取开放世界中的3D物体，同时进行几何重建和语义理解，无需任何几何或姿态先验，支持多种下游3D感知任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current 3D scene understanding methods are limited by offline-collectedmulti-view data or pre-constructed 3D geometry. In this paper, we presentExtractAnything3D (EA3D), a unified online framework for open-world 3D objectextraction that enables simultaneous geometric reconstruction and holisticscene understanding. Given a streaming video, EA3D dynamically interprets eachframe using vision-language and 2D vision foundation encoders to extractobject-level knowledge. This knowledge is integrated and embedded into aGaussian feature map via a feed-forward online update strategy. We theniteratively estimate visual odometry from historical frames and incrementallyupdate online Gaussian features with new observations. A recurrent jointoptimization module directs the model's attention to regions of interest,simultaneously enhancing both geometric reconstruction and semanticunderstanding. Extensive experiments across diverse benchmarks and tasks,including photo-realistic rendering, semantic and instance segmentation, 3Dbounding box and semantic occupancy estimation, and 3D mesh generation,demonstrate the effectiveness of EA3D. Our method establishes a unified andefficient framework for joint online 3D reconstruction and holistic sceneunderstanding, enabling a broad range of downstream tasks.</description>
      <author>example@mail.com (Xiaoyu Zhou, Jingqi Wang, Yuang Jia, Yongtao Wang, Deqing Sun, Ming-Hsuan Yang)</author>
      <guid isPermaLink="false">2510.25146v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Vision-Language Integration for Zero-Shot Scene Understanding in Real-World Environments</title>
      <link>http://arxiv.org/abs/2510.25070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint under review at IEEE Transactions on Pattern Analysis and  Machine Intelligence (TPAMI), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种视觉-语言集成框架，通过统一预训练视觉编码器和大语言模型，实现零样本场景理解，在多个数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;真实世界环境中的零样本场景理解面临重大挑战，由于自然场景的复杂性和可变性，模型必须在没有先前标记示例的情况下识别新对象、动作和上下文。&lt;h4&gt;目的&lt;/h4&gt;实现稳健的零样本场景理解，利用自然语言作为桥梁，推广到未见过的类别和上下文。&lt;h4&gt;方法&lt;/h4&gt;提出视觉-语言集成框架，统一预训练视觉编码器（如CLIP、ViT）和大语言模型（如基于GPT的架构），开发将视觉输入和文本提示嵌入共享空间的统一模型，并使用多模态融合和推理层进行上下文解释。&lt;h4&gt;主要发现&lt;/h4&gt;在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，与最先进的零样本模型相比，在对象识别、活动检测和场景字幕生成方面有显著提升，top-1准确率提高高达18%，语义一致性指标也有显著提升。&lt;h4&gt;结论&lt;/h4&gt;跨模态对齐和语言锚定在增强真实世界场景理解的泛化能力方面非常有效。&lt;h4&gt;翻译&lt;/h4&gt;真实世界环境中的零样本场景理解由于自然场景的复杂性和可变性而面临重大挑战，模型必须在没有先前标记示例的情况下识别新对象、动作和上下文。这项工作提出了一种视觉-语言集成框架，统一了预训练的视觉编码器（如CLIP、ViT）和大语言模型（如基于GPT的架构），以实现视觉和文本模态之间的语义对齐。目标是利用自然语言作为桥梁，推广到未见过的类别和上下文，实现稳健的零样本场景理解。我们的方法开发了一个统一模型，将视觉输入和文本提示嵌入到共享空间，然后使用多模态融合和推理层进行上下文解释。在Visual Genome、COCO、ADE20K和自定义真实世界数据集上的实验表明，与最先进的零样本模型相比，在对象识别、活动检测和场景字幕生成方面有显著提升。提出的系统在top-1准确率上提高了高达18%，在语义一致性指标方面也有显著提升，突显了跨模态对齐和语言锚定在增强真实世界场景理解泛化能力方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot scene understanding in real-world settings presents majorchallenges due to the complexity and variability of natural scenes, wheremodels must recognize new objects, actions, and contexts without prior labeledexamples. This work proposes a vision-language integration framework thatunifies pre-trained visual encoders (e.g., CLIP, ViT) and large language models(e.g., GPT-based architectures) to achieve semantic alignment between visualand textual modalities. The goal is to enable robust zero-shot comprehension ofscenes by leveraging natural language as a bridge to generalize over unseencategories and contexts. Our approach develops a unified model that embedsvisual inputs and textual prompts into a shared space, followed by multimodalfusion and reasoning layers for contextual interpretation. Experiments onVisual Genome, COCO, ADE20K, and custom real-world datasets demonstratesignificant gains over state-of-the-art zero-shot models in object recognition,activity detection, and scene captioning. The proposed system achieves up to18% improvement in top-1 accuracy and notable gains in semantic coherencemetrics, highlighting the effectiveness of cross-modal alignment and languagegrounding in enhancing generalization for real-world scene understanding.</description>
      <author>example@mail.com (Manjunath Prasad Holenarasipura Rajiv, B. M. Vidyavathi)</author>
      <guid isPermaLink="false">2510.25070v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>PISA-Bench: The PISA Index as a Multilingual and Multimodal Metric for the Evaluation of Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.24792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 11 tables and figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PISA-Bench是一个基于PISA测试的多语言多模态推理基准，包含六种语言的平行数据集，用于评估视觉语言模型在不同语言和推理任务上的表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在多模态推理方面取得显著进展，但现有基准测试在高质量、人工验证的例子方面有限，且大多数数据集仅限于英语，翻译样本的质量保证既耗时又昂贵。&lt;h4&gt;目的&lt;/h4&gt;填补高质量多语言多模态推理基准的空白，提供一个包含多种语言的人工验证数据集。&lt;h4&gt;方法&lt;/h4&gt;从专家创建的PISA测试英语例子中衍生出PISA-Bench，包含人工提取的指令、问题、答案选项和图像，并增加问题类型分类；将这些内容从英语翻译成西班牙语、德语、中文、法语和意大利语，形成完全平行的六语言语料库。&lt;h4&gt;主要发现&lt;/h4&gt;小型视觉语言模型(&lt;20B参数)在PISA-Bench上无法获得高分；模型在非英语部分的性能显著下降；当处理空间和几何推理任务时，模型错误率高。&lt;h4&gt;结论&lt;/h4&gt;通过发布PISA-Bench数据集和评估框架，为推进多模态多语言推理研究提供了重要资源。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要为英文，上述内容已将其核心信息翻译并结构化为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) have demonstrated remarkable progress inmultimodal reasoning. However, existing benchmarks remain limited in terms ofhigh-quality, human-verified examples. Many current datasets rely onsynthetically generated content by large language models (LLMs). Furthermore,most datasets are limited to English, as manual quality assurance of translatedsamples is time-consuming and costly. To fill this gap, we introducePISA-Bench, a multilingual benchmark derived from English examples of theexpert-created PISA tests, a unified framework for the assessment of studentcompetencies in over eighty countries. Each example consists of human-extractedinstructions, questions, answer options, and images, enriched with questiontype categories, and has been translated from English into five additionallanguages (Spanish, German, Chinese, French, and Italian), resulting in a fullyparallel corpus covering six languages. We evaluate state-of-the-artvision-language models on PISA-Bench and find that especially small models(&lt;20B parameters) fail to achieve high test scores. We further find substantialperformance degradation on non-English splits as well as high error-rates whenmodels are tasked with spatial and geometric reasoning. By releasing thedataset and evaluation framework, we provide a resource for advancing researchon multilingual multimodal reasoning.</description>
      <author>example@mail.com (Patrick Haller, Fabio Barth, Jonas Golde, Georg Rehm, Alan Akbik)</author>
      <guid isPermaLink="false">2510.24792v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>How Data Mixing Shapes In-Context Learning: Asymptotic Equivalence for Transformers with MLPs</title>
      <link>http://arxiv.org/abs/2510.25753v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, 24 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练Transformer模型中的上下文学习（ICL）能力，特别是在具有非线性MLP头部的模型上，从多个异构数据源学习的非线性任务。作者通过理论分析和实验验证，证明了非线性MLP相比线性基线能显著提升ICL性能，特别是在非线性任务上，并确定了高质量数据源的关键特性和特征学习的条件。&lt;h4&gt;背景&lt;/h4&gt;预训练Transformer模型展现出显著的上下文学习能力，使其能够在无需参数更新的情况下从演示中适应新任务。然而，理论研究通常依赖于简化的架构（如省略MLP）、数据模型（如具有各向同性输入的线性回归）和单源训练，这限制了它们与现实设置的相关性。&lt;h4&gt;目的&lt;/h4&gt;研究具有非线性MLP头部的预训练Transformer在从多个具有异构输入、任务和噪声分布的数据源获取的非线性任务上的ICL能力，分析数据混合效应，并提供关于架构和数据在ICL中作用的可操作见解。&lt;h4&gt;方法&lt;/h4&gt;分析一个包含两层的MLP模型，其中第一层通过单次梯度步骤训练，第二层完全优化。在高维渐近条件下，利用高斯普适性和正交多项式理论，证明这类模型的ICL误差等价于结构化多项式预测器。在各种激活函数、模型大小和数据分布上进行经验验证，并在多语言情感分析的真实场景中进行实验。&lt;h4&gt;主要发现&lt;/h4&gt;非线性MLP能显著提升ICL性能，特别是在非线性任务上；高质量数据源具有低噪声和结构化协方差的关键特性；只有当任务协方差具有足够结构时，特征学习才会出现；这些发现在各种激活函数、模型大小和数据分布上得到了经验验证；多语言情感分析实验表明这些发现可以扩展到真实世界案例。&lt;h4&gt;结论&lt;/h4&gt;这项工作推进了Transformer中ICL的理论基础，并提供了关于架构和数据在ICL中作用的可操作见解，特别是在非线性任务和异构数据源设置下。&lt;h4&gt;翻译&lt;/h4&gt;预训练的Transformer模型展现出显著的上下文学习（ICL）能力，使它们能够在无需参数更新的情况下从演示中适应新任务。然而，理论研究通常依赖于简化的架构（例如，省略MLP）、数据模型（例如，具有各向同性输入的线性回归）和单源训练，限制了它们与现实设置的相关性。在这项工作中，我们研究了具有非线性MLP头部的预训练Transformer的ICL能力，这些模型在从多个具有异构输入、任务和噪声分布的数据源中获取的非线性任务上表现。我们分析了一个模型，其中MLP包含两层，第一层通过单次梯度步骤训练，第二层完全优化。在高维渐近条件下，我们证明这类模型的ICL误差等价于结构化多项式预测器，利用了高斯普适性和正交多项式理论的结果。这种等价性表明非线性MLP相比线性基线能显著提升ICL性能，特别是在非线性任务上。它还使数据分析混合效应的精确分析成为可能：我们确定了高质量数据源的关键特性（低噪声、结构化协方差），并表明只有当任务协方差具有足够结构时，特征学习才会出现。这些发现在各种激活函数、模型大小和数据分布上得到了经验验证。最后，我们在一个涉及多语言情感分析的真实场景中进行了实验，每种语言被视为不同的数据源。这个案例的实验结果说明了我们的发现如何扩展到真实世界案例。总体而言，我们的工作推进了Transformer中ICL的理论基础，并提供了关于架构和数据在ICL中作用的可操作见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pretrained Transformers demonstrate remarkable in-context learning (ICL)capabilities, enabling them to adapt to new tasks from demonstrations withoutparameter updates. However, theoretical studies often rely on simplifiedarchitectures (e.g., omitting MLPs), data models (e.g., linear regression withisotropic inputs), and single-source training, limiting their relevance torealistic settings. In this work, we study ICL in pretrained Transformers withnonlinear MLP heads on nonlinear tasks drawn from multiple data sources withheterogeneous input, task, and noise distributions. We analyze a model wherethe MLP comprises two layers, with the first layer trained via a singlegradient step and the second layer fully optimized. Under high-dimensionalasymptotics, we prove that such models are equivalent in ICL error tostructured polynomial predictors, leveraging results from the theory ofGaussian universality and orthogonal polynomials. This equivalence reveals thatnonlinear MLPs meaningfully enhance ICL performance, particularly on nonlineartasks, compared to linear baselines. It also enables a precise analysis of datamixing effects: we identify key properties of high-quality data sources (lownoise, structured covariances) and show that feature learning emerges only whenthe task covariance exhibits sufficient structure. These results are validatedempirically across various activation functions, model sizes, and datadistributions. Finally, we experiment with a real-world scenario involvingmultilingual sentiment analysis where each language is treated as a differentsource. Our experimental results for this case exemplify how our findingsextend to real-world cases. Overall, our work advances the theoreticalfoundations of ICL in Transformers and provides actionable insight into therole of architecture and data in ICL.</description>
      <author>example@mail.com (Samet Demir, Zafer Dogan)</author>
      <guid isPermaLink="false">2510.25753v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Lost in Phonation: Voice Quality Variation as an Evaluation Dimension for Speech Foundation Models</title>
      <link>http://arxiv.org/abs/2510.25577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures, 4 tables, submitted to LREC 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了语音基础模型(SFMs)如何处理语音中的副语言变化，特别是音质(如嘶哑和气声)对模型行为的影响。作者通过开放式生成任务和语音情感识别评估模型对不同音质输入的一致性反应，并引入了新的平行数据集来评估SFMs对音质的敏感性。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型(SFMs)可直接处理原始音频中的口语，绕过文本表示，因此能接触到语音信号中的副语言变化。音质是副语言变化中未被充分探索的维度，包括嘶哑和气声等发声类型，这些类型影响听众对情感状态、立场和社会意义的推断。现有语音理解基准主要依赖多项选择题格式，容易失败，难以捕捉副语言特征对模型行为的微妙影响。&lt;h4&gt;目的&lt;/h4&gt;通过开放式生成任务和语音情感识别探测SFMs，评估模型行为在不同音质输入下是否一致；引入包含音质合成修改的平行数据集，评估SFMs对嘶哑和气声的反应；提供对SFMs对这些特定语音感知非词汇方面敏感性的首次检验。&lt;h4&gt;方法&lt;/h4&gt;使用开放式生成任务探测SFMs；通过语音情感识别评估模型反应；引入包含音质合成修改的平行数据集；评估SFMs对嘶哑和气声的反应。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体实验结果，主要介绍了研究方法和数据集的构建。&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及具体结论，主要介绍了研究的创新点和贡献，即首次检验了SFMs对语音中特定非词汇方面的敏感性。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型(SFMs)的最新进展使得可以直接处理原始音频中的口语，绕过中间的文本表示。这种能力使SFMs能够接触到输入语音信号中嵌入的丰富副语言变化，并可能对这些变化做出响应。副语言变化的一个未被充分探索的维度是音质，包括嘶哑和气声等发声类型。这些发声类型已知会影响听众如何推断语音中的情感状态、立场和社会意义。现有的语音理解基准测试主要依赖多项选择题问答(MCQA)格式，这些格式容易失败，因此在捕捉副语言特征如何微妙影响模型行为方面并不可靠。在本文中，我们通过开放式生成任务和语音情感识别来探测SFMs，评估模型行为在不同音质输入下是否一致。我们引入了一个新的平行数据集，其中包含对音质的合成修改，旨在评估SFMs对嘶哑和气声的反应。我们的工作首次检验了SFMs对这些特定语音感知非词汇方面的敏感性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in speech foundation models (SFMs) have enabled the directprocessing of spoken language from raw audio, bypassing intermediate textualrepresentations. This capability allows SFMs to be exposed to, and potentiallyrespond to, rich paralinguistic variations embedded in the input speech signal.One under-explored dimension of paralinguistic variation is voice quality,encompassing phonation types such as creaky and breathy voice. These phonationtypes are known to influence how listeners infer affective state, stance andsocial meaning in speech. Existing benchmarks for speech understanding largelyrely on multiple-choice question answering (MCQA) formats, which are prone tofailure and therefore unreliable in capturing the nuanced ways paralinguisticfeatures influence model behaviour. In this paper, we probe SFMs throughopen-ended generation tasks and speech emotion recognition, evaluating whethermodel behaviours are consistent across different phonation inputs. We introducea new parallel dataset featuring synthesized modifications to voice quality,designed to evaluate SFM responses to creaky and breathy voice. Our workprovides the first examination of SFM sensitivity to these particularnon-lexical aspects of speech perception.</description>
      <author>example@mail.com (Harm Lameris, Shree Harsha Bokkahalli Satish, Joakim Gustafson, Éva Székely)</author>
      <guid isPermaLink="false">2510.25577v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging an Atmospheric Foundational Model for Subregional Sea Surface Temperature Forecasting</title>
      <link>http://arxiv.org/abs/2510.25563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究将大气预报深度学习模型Aurora适应化用于海洋预测，通过微调实现了高精度的海表温度预测，同时降低了计算成本，为数据驱动的海洋预报提供了新方法。&lt;h4&gt;背景&lt;/h4&gt;准确的海洋变量预测对理解气候变化、管理海洋资源和优化海洋活动至关重要。传统海洋预报依赖数值模型，但面临计算成本高和可扩展性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;将Aurora深度学习模型（原为大气预报设计）适应化，用于预测加那利上升流系统的海表温度(SST)，探索深度学习在海洋预报中的应用潜力。&lt;h4&gt;方法&lt;/h4&gt;使用高分辨率海洋再分析数据对模型进行分阶段微调，结合纬度加权误差指标，优化超参数以实现高效学习，减少计算需求。&lt;h4&gt;主要发现&lt;/h4&gt;模型实现了0.119K的低均方根误差，异常相关系数高达0.997，成功重现大尺度SST结构，但在捕捉沿海地区精细细节方面存在挑战。&lt;h4&gt;结论&lt;/h4&gt;研究证明使用在不同领域预训练的深度学习模型进行海洋应用具有可行性，未来改进方向包括整合更多海洋变量、提高空间分辨率和探索物理信息神经网络。&lt;h4&gt;翻译&lt;/h4&gt;准确的海洋变量预测对于理解气候变化、管理海洋资源和优化海洋活动至关重要。传统的海洋预报依赖于数值模型；然而，这些方法在计算成本和可扩展性方面存在局限性。在本研究中，我们将Aurora（一种最初为大气预报设计的基础深度学习模型）适应化，用于预测加那利上升流系统的海表温度(SST)。通过使用高分辨率的海洋再分析数据对模型进行微调，我们展示了其捕捉复杂时空模式的能力，同时减少了计算需求。我们的方法包括分阶段微调过程，结合纬度加权误差指标，并优化超参数以实现高效学习。实验结果显示，模型实现了0.119K的低均方根误差，并保持高的异常相关系数(ACC≈0.997)。模型成功重现了大尺度SST结构，但在捕捉沿海地区更精细的细节方面面临挑战。这项工作通过证明使用在不同领域预训练的深度学习模型进行海洋应用的可行性，为数据驱动的海洋预报领域做出了贡献。未来改进包括整合额外的海洋变量、提高空间分辨率，以及探索物理信息神经网络以增强可解释性和理解。这些进步可以改善气候建模和海洋预测精度，支持环境和经济部门的决策制定。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The accurate prediction of oceanographic variables is crucial forunderstanding climate change, managing marine resources, and optimizingmaritime activities. Traditional ocean forecasting relies on numerical models;however, these approaches face limitations in terms of computational cost andscalability. In this study, we adapt Aurora, a foundational deep learning modeloriginally designed for atmospheric forecasting, to predict sea surfacetemperature (SST) in the Canary Upwelling System. By fine-tuning this modelwith high-resolution oceanographic reanalysis data, we demonstrate its abilityto capture complex spatiotemporal patterns while reducing computationaldemands. Our methodology involves a staged fine-tuning process, incorporatinglatitude-weighted error metrics and optimizing hyperparameters for efficientlearning. The experimental results show that the model achieves a low RMSE of0.119K, maintaining high anomaly correlation coefficients (ACC $\approx0.997$). The model successfully reproduces large-scale SST structures but faceschallenges in capturing finer details in coastal regions. This work contributesto the field of data-driven ocean forecasting by demonstrating the feasibilityof using deep learning models pre-trained in different domains for oceanicapplications. Future improvements include integrating additional oceanographicvariables, increasing spatial resolution, and exploring physics-informed neuralnetworks to enhance interpretability and understanding. These advancements canimprove climate modeling and ocean prediction accuracy, supportingdecision-making in environmental and economic sectors.</description>
      <author>example@mail.com (Víctor Medina, Giovanny A. Cuervo-Londoño, Javier Sánchez)</author>
      <guid isPermaLink="false">2510.25563v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>FaCT: Faithful Concept Traces for Explaining Neural Network Decisions</title>
      <link>http://arxiv.org/abs/2510.25512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025; Code is available at  https://github.com/m-parchami/FaCT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种具有模型内在机制概念解释的新模型，强调概念化解释的忠实性，并引入了概念一致性度量C²-Score来评估概念化方法。&lt;h4&gt;背景&lt;/h4&gt;深度网络在各种任务上表现出色，但要全面理解其工作机制仍然是一个挑战。现有的后验概念化方法在解释模型时并不总是忠实于模型本身，且对模型学习的概念做出了严格的假设。&lt;h4&gt;目的&lt;/h4&gt;强调概念化解释的忠实性，提出一种具有模型内在机制概念解释的新模型，并开发一种新的概念一致性度量标准来评估概念化方法。&lt;h4&gt;方法&lt;/h4&gt;提出的新模型的概念跨类别共享，可以从任何层追踪其对logit的贡献和输入可视化。利用基础模型提出了一种新的概念一致性度量标准C²-Score，用于评估概念化方法。&lt;h4&gt;主要发现&lt;/h4&gt;与先前的工作相比，提出的模型在定量上更加一致，用户发现其概念更具可解释性，同时保持了有竞争力的ImageNet性能。&lt;h4&gt;结论&lt;/h4&gt;通过强调概念化解释的忠实性和提出新的度量标准，该研究为理解深度网络的工作机制提供了更有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;深度网络在广泛任务中表现出色，但要全面理解其工作机制仍然是一个关键挑战。许多后验概念化方法被引入以解释其工作原理，但它们并不总是忠实于模型。此外，它们对模型学习的概念做出了严格的假设，如类别特异性、小的空间范围或符合人类预期。在本工作中，我们强调此类概念化解释的忠实性，并提出了一种具有模型内在机制概念解释的新模型。我们的概念跨类别共享，并且可以从任何层追踪其对logit的贡献及其输入可视化。我们还利用基础模型提出了一个新的概念一致性度量标准C²-Score，可用于评估概念化方法。我们表明，与先前的工作相比，我们的概念在定量上更加一致，用户发现我们的概念更具可解释性，同时保持了有竞争力的ImageNet性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep networks have shown remarkable performance across a wide range of tasks,yet getting a global concept-level understanding of how they function remains akey challenge. Many post-hoc concept-based approaches have been introduced tounderstand their workings, yet they are not always faithful to the model.Further, they make restrictive assumptions on the concepts a model learns, suchas class-specificity, small spatial extent, or alignment to human expectations.In this work, we put emphasis on the faithfulness of such concept-basedexplanations and propose a new model with model-inherent mechanisticconcept-explanations. Our concepts are shared across classes and, from anylayer, their contribution to the logit and their input-visualization can befaithfully traced. We also leverage foundation models to propose a newconcept-consistency metric, C$^2$-Score, that can be used to evaluateconcept-based methods. We show that, compared to prior work, our concepts arequantitatively more consistent and users find our concepts to be moreinterpretable, all while retaining competitive ImageNet performance.</description>
      <author>example@mail.com (Amin Parchami-Araghi, Sukrut Rao, Jonas Fischer, Bernt Schiele)</author>
      <guid isPermaLink="false">2510.25512v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>TempoPFN: Synthetic Pre-training of Linear RNNs for Zero-shot Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.25502v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 18 figures, 13 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了TempoPFN，一种基于线性循环神经网络的单变量时间序列基础模型，仅在合成数据上预训练，解决了零样本时间序列预测中的长期预测效率和可重现性问题。&lt;h4&gt;背景&lt;/h4&gt;零样本时间序列预测的基础模型面临长期预测效率低和可重现性差的挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效且可重现的时间序列基础模型，用于零样本预测，超越现有仅使用合成数据的方法的性能。&lt;h4&gt;方法&lt;/h4&gt;TempoPFN采用GatedDeltaProduct架构和状态编织技术，实现跨序列长度的完全并行化训练，消除对窗口化或摘要技术的需求。综合合成数据管道统一了随机微分方程、高斯过程和音频合成等多种生成器，并引入新颖的数据增强技术。&lt;h4&gt;主要发现&lt;/h4&gt;在Gift-Eval基准测试中，TempoPFN达到顶尖竞争性能，超越所有现有的仅使用合成数据的方法，并超过绝大多数在真实数据上训练的模型。同时，通过完全并行化的训练和推理，比现有基线更高效。&lt;h4&gt;结论&lt;/h4&gt;开源完整的数据生成管道和训练代码，为未来研究提供可重现的基础，推动零样本时间序列预测领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;零样本时间序列预测的基础模型面临长期预测效率低和可重现性差的挑战，现有的仅使用合成数据的方法在具有挑战性的基准测试中表现不佳。本文提出了TempoPFN，一种基于线性循环神经网络的单变量时间序列基础模型，该模型仅在合成数据上进行预训练。该模型采用GatedDeltaProduct架构和状态编织技术，实现跨序列长度的完全并行化训练，消除对窗口化或摘要技术的需求，同时保持强大的时间状态跟踪能力。我们的综合合成数据管道统一了多种生成器，包括随机微分方程、高斯过程和音频合成，并引入了新颖的数据增强技术。在Gift-Eval基准的零样本评估中，TempoPFN达到了顶尖的竞争性能，超越了所有现有的仅使用合成数据的方法，并超过了绝大多数在真实数据上训练的模型，同时通过利用完全并行化的训练和推理，比现有基线更高效。我们开源了完整的数据生成管道和训练代码，为未来研究提供可重现的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for zero-shot time series forecasting face challenges inefficient long-horizon prediction and reproducibility, with existingsynthetic-only approaches underperforming on challenging benchmarks. This paperpresents TempoPFN, a univariate time series foundation model based on linearRecurrent Neural Networks (RNNs) pre-trained exclusively on synthetic data. Themodel uses a GatedDeltaProduct architecture with state-weaving for fullyparallelizable training across sequence lengths, eliminating the need forwindowing or summarization techniques while maintaining robust temporalstate-tracking. Our comprehensive synthetic data pipeline unifies diversegenerators, including stochastic differential equations, Gaussian processes,and audio synthesis, with novel augmentations. In zero-shot evaluations on theGift-Eval benchmark, TempoPFN achieves top-tier competitive performance,outperforming all existing synthetic-only approaches and surpassing the vastmajority of models trained on real-world data, while being more efficient thanexisting baselines by leveraging fully parallelizable training and inference.We open-source our complete data generation pipeline and training code,providing a reproducible foundation for future research.</description>
      <author>example@mail.com (Vladyslav Moroshan, Julien Siems, Arber Zela, Timur Carstensen, Frank Hutter)</author>
      <guid isPermaLink="false">2510.25502v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Position: Biology is the Challenge Physics-Informed ML Needs to Evolve</title>
      <link>http://arxiv.org/abs/2510.25368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将物理信息机器学习(PIML)扩展到生物学领域，称为生物学信息机器学习(BIML)，以应对生物建模的独特挑战。&lt;h4&gt;背景&lt;/h4&gt;物理信息机器学习已成功将机理理解整合到机器学习中，特别是在受已知物理定律支配的领域，这一成功促使人们尝试将其应用于生物学领域。&lt;h4&gt;目的&lt;/h4&gt;将PIML的原则性方法扩展到生物学，创建BIML框架，使其能够适应生物学的实际现实，而非视为障碍。&lt;h4&gt;方法&lt;/h4&gt;重新调整PIML的方法，使其能够在更软性、概率形式的先验知识下运行，提出四大基础支柱：不确定性量化、上下文化、受限潜在结构推断和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;生物建模面临的挑战（多方面且不确定的先验知识、异构且嘈杂的数据、部分可观察性以及复杂的高维网络）不应被视为PIML的障碍，而应被视为其进化的催化剂。&lt;h4&gt;结论&lt;/h4&gt;基础模型和大语言模型将成为关键推动因素，将人类专业知识与计算建模结合，构建BIML生态系统，并将PIML启发的创新引向具有高度科学和社会相关性的挑战。&lt;h4&gt;翻译&lt;/h4&gt;物理信息机器学习(PIML)已成功将机理理解整合到机器学习中，特别是在受已知物理定律支配的领域。这一成功促使人们尝试将PIML应用于生物学，这是一个充满动态系统但受不同约束塑造的领域。然而，生物建模面临独特挑战：多方面且不确定的先验知识、异构且嘈杂的数据、部分可观察性以及复杂的高维网络。在这篇立场论文中，我们认为这些挑战不应被视为PIML的障碍，而应是其进化的催化剂。我们提出了生物学信息机器学习(BIML)：PIML的原则性扩展，保留了其结构基础，同时适应生物学的实际现实。BIML不是取代PIML，而是重新调整其方法，使其能够在更软性、概率形式的先验知识下运行。我们概述了四个基础支柱作为这一转变的路线图：不确定性量化、上下文化、受限潜在结构推断和可扩展性。基础模型和大语言模型将成为关键推动因素，将人类专业知识与计算建模结合起来。最后，我们提出具体建议，以构建BIML生态系统，并将PIML启发的创新引向具有高度科学和社会相关性的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physics-Informed Machine Learning (PIML) has successfully integratedmechanistic understanding into machine learning, particularly in domainsgoverned by well-known physical laws. This success has motivated efforts toapply PIML to biology, a field rich in dynamical systems but shaped bydifferent constraints. Biological modeling, however, presents uniquechallenges: multi-faceted and uncertain prior knowledge, heterogeneous andnoisy data, partial observability, and complex, high-dimensional networks. Inthis position paper, we argue that these challenges should not be seen asobstacles to PIML, but as catalysts for its evolution. We proposeBiology-Informed Machine Learning (BIML): a principled extension of PIML thatretains its structural grounding while adapting to the practical realities ofbiology. Rather than replacing PIML, BIML retools its methods to operate undersofter, probabilistic forms of prior knowledge. We outline four foundationalpillars as a roadmap for this transition: uncertainty quantification,contextualization, constrained latent structure inference, and scalability.Foundation Models and Large Language Models will be key enablers, bridginghuman expertise with computational modeling. We conclude with concreterecommendations to build the BIML ecosystem and channel PIML-inspiredinnovation toward challenges of high scientific and societal relevance.</description>
      <author>example@mail.com (Julien Martinelli)</author>
      <guid isPermaLink="false">2510.25368v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>3D CT-Based Coronary Calcium Assessment: A Feature-Driven Machine Learning Framework</title>
      <link>http://arxiv.org/abs/2510.25347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 Figures, MICCAI AMAI 2025 workshop, to be published in  Volume 16206 of the Lecture Notes in Computer Science series&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于放射组学的流程，利用伪标记生成训练标签，解决了非对比冠状动脉计算机断层血管造影(CCTA)扫描中冠状动脉钙化(CAC)评分标记数据有限的问题。&lt;h4&gt;背景&lt;/h4&gt;冠状动脉钙化(CAC)评分在冠状动脉疾病(CAD)的早期检测和风险分层中起着关键作用。非对比冠状动脉计算机断层血管造影(CCTA)扫描在临床中常用于早期钙化检测。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于放射组学的流程，利用伪标记生成训练标签，避免需要专家定义的分割，并探索预训练基础模型在特征提取中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出基于放射组学的流程，利用伪标记生成训练标签；探索使用预训练基础模型(CT-FM和RadImageNet)提取图像特征并与传统分类器结合；比较深度学习特征与放射组学特征性能；在包含182名患者的临床CCTA数据集上评估，将患者分为零钙化评分组和非零钙化评分组；研究在非对比数据集与对比+非对比数据集上训练的影响。&lt;h4&gt;主要发现&lt;/h4&gt;基于放射组学的模型显著优于来自基础模型的CNN嵌入，达到84%的准确率(p&lt;0.05)，尽管没有专家标注可用。&lt;h4&gt;结论&lt;/h4&gt;基于放射组学的方法在冠状动脉钙化检测中表现出色，即使在没有专家标注的情况下也能达到高准确率。&lt;h4&gt;翻译&lt;/h4&gt;冠状动脉钙化(CAC)评分在冠状动脉疾病(CAD)的早期检测和风险分层中起着关键作用。在本研究中，我们关注非对比冠状动脉计算机断层血管造影(CCTA)扫描，这些扫描在临床中常用于早期钙化检测。为解决标记数据有限这一挑战，我们提出了一种基于放射组学的流程，利用伪标记生成训练标签，从而消除对专家定义分割的需求。此外，我们探索了使用预训练基础模型(特别是CT-FM和RadImageNet)提取图像特征，然后与传统分类器一起使用。我们将这些深度学习特征与放射组学特征的性能进行了比较。评估在包含182名患者的临床CCTA数据集上进行，个体被分为两组：零钙化评分组与非零钙化评分组。我们进一步研究了在非对比数据集与对比+非对比数据集上训练的影响，测试仅在非对比扫描上进行。结果表明，尽管没有专家标注可用，但基于放射组学的模型显著优于来自基础模型的CNN嵌入(达到84%的准确率和p&lt;0.05)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Coronary artery calcium (CAC) scoring plays a crucial role in the earlydetection and risk stratification of coronary artery disease (CAD). In thisstudy, we focus on non-contrast coronary computed tomography angiography (CCTA)scans, which are commonly used for early calcification detection in clinicalsettings. To address the challenge of limited annotated data, we propose aradiomics-based pipeline that leverages pseudo-labeling to generate traininglabels, thereby eliminating the need for expert-defined segmentations.Additionally, we explore the use of pretrained foundation models, specificallyCT-FM and RadImageNet, to extract image features, which are then used withtraditional classifiers. We compare the performance of these deep learningfeatures with that of radiomics features. Evaluation is conducted on a clinicalCCTA dataset comprising 182 patients, where individuals are classified into twogroups: zero versus non-zero calcium scores. We further investigate the impactof training on non-contrast datasets versus combined contrast and non-contrastdatasets, with testing performed only on non contrast scans. Results show thatradiomics-based models significantly outperform CNN-derived embeddings fromfoundation models (achieving 84% accuracy and p&lt;0.05), despite theunavailability of expert annotations.</description>
      <author>example@mail.com (Ayman Abaid, Gianpiero Guidone, Sara Alsubai, Foziyah Alquahtani, Talha Iqbal, Ruth Sharif, Hesham Elzomor, Emiliano Bianchini, Naeif Almagal, Michael G. Madden, Faisal Sharif, Ihsan Ullah)</author>
      <guid isPermaLink="false">2510.25347v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>GAP: Graph-Based Agent Planning with Parallel Tool Use and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.25320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了基于图的代理规划(GAP)框架，通过建模任务间依赖关系实现工具的并行和顺序执行，解决了现有顺序推理范式的效率问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型驱动的自主代理在工具操作方面显示出强大能力，但现有范式(如ReAct)依赖顺序推理和执行，无法利用独立子任务之间的内在并行性。&lt;h4&gt;目的&lt;/h4&gt;解决顺序推理瓶颈导致的工具利用效率低下和多步推理场景中表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;提出基于图的代理规划(GAP)框架，训练基础模型将复杂任务分解为依赖感知的子任务图，自主确定工具的并行或顺序执行方式；采用两阶段训练策略：监督微调(SFT)和强化学习(RL)。&lt;h4&gt;主要发现&lt;/h4&gt;GAP在MHQA数据集上显著优于传统ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。&lt;h4&gt;结论&lt;/h4&gt;依赖感知的任务编排在执行效率和任务准确性方面都取得了实质性改进。&lt;h4&gt;翻译&lt;/h4&gt;由大型语言模型(LLM)驱动的自主代理在工具操作方面展现出解决复杂任务的强大能力。然而，现有的ReAct等范式依赖顺序推理和执行，无法利用独立子任务之间的内在并行性。这种顺序瓶颈导致工具利用效率低下，以及在多步推理场景中表现不佳。我们引入了基于图的代理规划(GAP)，这是一个新框架，通过基于图的规划明确建模任务间依赖关系，实现自适应并行和顺序工具执行。我们的方法训练基础模型将复杂任务分解为依赖感知的子任务图，自主确定哪些工具可以并行执行，哪些必须遵循顺序依赖。这种依赖感知的编排在执行效率和任务准确性方面都取得了实质性改进。为了训练GAP，我们从多跳问答(MHQA)基准构建了高质量的基于图的规划轨迹数据集。我们采用两阶段训练策略：首先在整理的数据集上进行监督微调(SFT)，然后在基于正确性奖励函数的强化学习(RL)阶段，在战略采样的查询上进行训练。在MHQA数据集上的实验结果表明，GAP显著优于传统的ReAct基线，特别是在多步检索任务上，同时通过智能并行化实现了工具调用效率的显著提升。项目页面可在以下网址访问：https://github.com/WJQ7777/Graph-Agent-Planning。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous agents powered by large language models (LLMs) have shownimpressive capabilities in tool manipulation for complex task-solving. However,existing paradigms such as ReAct rely on sequential reasoning and execution,failing to exploit the inherent parallelism among independent sub-tasks. Thissequential bottleneck leads to inefficient tool utilization and suboptimalperformance in multi-step reasoning scenarios. We introduce Graph-based AgentPlanning (GAP), a novel framework that explicitly models inter-taskdependencies through graph-based planning to enable adaptive parallel andserial tool execution. Our approach trains agent foundation models to decomposecomplex tasks into dependency-aware sub-task graphs, autonomously determiningwhich tools can be executed in parallel and which must follow sequentialdependencies. This dependency-aware orchestration achieves substantialimprovements in both execution efficiency and task accuracy. To train GAP, weconstruct a high-quality dataset of graph-based planning traces derived fromthe Multi-Hop Question Answering (MHQA) benchmark. We employ a two-stagetraining strategy: supervised fine-tuning (SFT) on the curated dataset,followed by reinforcement learning (RL) with a correctness-based rewardfunction on strategically sampled queries where tool-based reasoning providesmaximum value. Experimental results on MHQA datasets demonstrate that GAPsignificantly outperforms traditional ReAct baselines, particularly onmulti-step retrieval tasks, while achieving dramatic improvements in toolinvocation efficiency through intelligent parallelization. The project page isavailable at: https://github.com/WJQ7777/Graph-Agent-Planning.</description>
      <author>example@mail.com (Jiaqi Wu, Qinlao Zhao, Zefeng Chen, Kai Qin, Yifei Zhao, Xueqian Wang, Yuhang Yao)</author>
      <guid isPermaLink="false">2510.25320v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>RT-DETRv4: Painlessly Furthering Real-Time Object Detection with Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2510.25257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种成本效益高且适应性强的蒸馏框架，利用视觉基础模型(VFMs)增强轻量级目标检测器，解决了高速度推理与特征表示能力之间的矛盾。&lt;h4&gt;背景&lt;/h4&gt;实时目标检测通过精心设计的架构和优化策略取得了显著进展，但轻量级网络设计追求高速推理往往导致特征表示能力下降，阻碍了性能提升和实际设备部署。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用视觉基础模型(VFMs)能力增强轻量级目标检测器的成本效益高且适应性强的蒸馏框架，解决VFM与资源受限检测器之间架构和学习目标差异导致的语义传输挑战。&lt;h4&gt;方法&lt;/h4&gt;引入深度语义注入器(DSI)模块促进VFM高级表示与检测器深层集成；设计基于梯度的自适应调制(GAM)策略，根据梯度范数比率动态调整语义传输强度。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在不增加部署和推理开销的情况下，为各种基于DETR的模型带来显著且一致的性能提升；新模型RT-DETRv4在COCO上取得最先进结果，在273/169/124/78 FPS速度下分别达到49.7/53.5/55.4/57.0的AP分数。&lt;h4&gt;结论&lt;/h4&gt;该方法强调了其在实时检测中的实际效用，为实时目标检测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;实时目标检测通过精心设计的架构和优化策略取得了实质性进展。然而，通过轻量级网络设计追求高速推理通常会导致特征表示能力下降，这阻碍了性能的进一步改进和实际设备部署。在本文中，我们提出了一种具有成本效益且高度适应性的蒸馏框架，利用视觉基础模型(VFMs)的快速发展能力来增强轻量级目标检测器。鉴于VFM与资源受限检测器之间存在显著的架构和学习目标差异，实现稳定且任务对齐的语义传输具有挑战性。为此，一方面，我们引入了深度语义注入器(DSI)模块，促进VFM的高级表示与检测器深层层的集成；另一方面，我们设计了基于梯度的自适应调制(GAM)策略，根据梯度范数比率动态调整语义传输强度。在不增加部署和推理开销的情况下，我们的方法在各种基于DETR的模型上轻松实现了显著且一致的性能提升，凸显了其在实时检测中的实际效用。我们的新模型系列RT-DETRv4在COCO上取得了最先进的结果，在相应速度为273/169/124/78 FPS时分别达到49.7/53.5/55.4/57.0的AP分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-time object detection has achieved substantial progress throughmeticulously designed architectures and optimization strategies. However, thepursuit of high-speed inference via lightweight network designs often leads todegraded feature representation, which hinders further performance improvementsand practical on-device deployment. In this paper, we propose a cost-effectiveand highly adaptable distillation framework that harnesses the rapidly evolvingcapabilities of Vision Foundation Models (VFMs) to enhance lightweight objectdetectors. Given the significant architectural and learning objectivedisparities between VFMs and resource-constrained detectors, achieving stableand task-aligned semantic transfer is challenging. To address this, on onehand, we introduce a Deep Semantic Injector (DSI) module that facilitates theintegration of high-level representations from VFMs into the deep layers of thedetector. On the other hand, we devise a Gradient-guided Adaptive Modulation(GAM) strategy, which dynamically adjusts the intensity of semantic transferbased on gradient norm ratios. Without increasing deployment and inferenceoverhead, our approach painlessly delivers striking and consistent performancegains across diverse DETR-based models, underscoring its practical utility forreal-time detection. Our new model family, RT-DETRv4, achieves state-of-the-artresults on COCO, attaining AP scores of 49.7/53.5/55.4/57.0 at correspondingspeeds of 273/169/124/78 FPS.</description>
      <author>example@mail.com (Zijun Liao, Yian Zhao, Xin Shan, Yu Yan, Chang Liu, Lei Lu, Xiangyang Ji, Jie Chen)</author>
      <guid isPermaLink="false">2510.25257v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Test-Time Adaptive Object Detection with Foundation Model</title>
      <link>http://arxiv.org/abs/2510.25175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于基础模型的测试时自适应目标检测方法，消除了对源数据的完全需求，克服了传统封闭集限制，实现了高效的跨域和跨类别适应。&lt;h4&gt;背景&lt;/h4&gt;测试时自适应目标检测近年来受到越来越多的关注，因为它在在线领域适应方面具有独特优势，更接近实际应用场景。然而，现有方法严重依赖于源域统计特征，并假设源域和目标域共享相同的类别空间。&lt;h4&gt;目的&lt;/h4&gt;提出第一个基于基础模型的测试时自适应目标检测方法，消除对源数据的完全需求，克服传统封闭集限制，实现任意跨域和跨类别的目标数据适应。&lt;h4&gt;方法&lt;/h4&gt;设计了一个多模态提示的Mean-Teacher框架，结合文本和视觉提示调整，以参数高效的方式适应语言和视觉表示空间；提出了针对视觉提示的测试时预热策略；维护实例动态内存模块存储高质量伪标签；并提出了内存增强和内存幻觉两种新策略。&lt;h4&gt;主要发现&lt;/h4&gt;在跨损坏和跨数据集基准上的广泛实验表明，该方法持续优于之前的最先进方法，能够适应任意跨域和跨类别的目标数据。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的方法在测试时自适应目标检测方面表现出色，代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;近年来，测试时自适应目标检测因其在线领域适应中的独特优势而受到越来越多的关注，这更接近实际应用场景。然而，现有方法严重依赖于源域统计特征，同时做出源域和目标域共享相同类别空间的强假设。本文提出了第一个基于基础模型的测试时自适应目标检测方法，完全消除了对源数据的需求并克服了传统封闭集限制。具体而言，我们设计了一个多模态提示的Mean-Teacher框架，用于视觉-语言检测器驱动的测试时适应，结合文本和视觉提示调整，以参数高效的方式在测试数据上适应语言和视觉表示空间。相应地，我们提出了针对视觉提示定制的测试时预热策略，以有效保持视觉分支的表示能力。此外，为保证每个测试批次的高质量伪标签，我们维护了一个存储来自先前测试样本的高质量伪标签的实例动态内存模块，并提出了两种新策略——内存增强和内存幻觉，分别利用IDM的高质量实例来增强原始预测和对没有可用伪标签的图像进行幻觉处理。在跨损坏和跨数据集基准上的广泛实验表明，我们的方法持续优于之前的最先进方法，并能适应任意跨域和跨类别的目标数据。代码可在https://github.com/gaoyingjay/ttaod_foundation获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, test-time adaptive object detection has attracted increasingattention due to its unique advantages in online domain adaptation, whichaligns more closely with real-world application scenarios. However, existingapproaches heavily rely on source-derived statistical characteristics whilemaking the strong assumption that the source and target domains share anidentical category space. In this paper, we propose the first foundationmodel-powered test-time adaptive object detection method that eliminates theneed for source data entirely and overcomes traditional closed-set limitations.Specifically, we design a Multi-modal Prompt-based Mean-Teacher framework forvision-language detector-driven test-time adaptation, which incorporates textand visual prompt tuning to adapt both language and vision representationspaces on the test data in a parameter-efficient manner. Correspondingly, wepropose a Test-time Warm-start strategy tailored for the visual prompts toeffectively preserve the representation capability of the vision branch.Furthermore, to guarantee high-quality pseudo-labels in every test batch, wemaintain an Instance Dynamic Memory (IDM) module that stores high-qualitypseudo-labels from previous test samples, and propose two novelstrategies-Memory Enhancement and Memory Hallucination-to leverage IDM'shigh-quality instances for enhancing original predictions and hallucinatingimages without available pseudo-labels, respectively. Extensive experiments oncross-corruption and cross-dataset benchmarks demonstrate that our methodconsistently outperforms previous state-of-the-art methods, and can adapt toarbitrary cross-domain and cross-category target data. Code is available athttps://github.com/gaoyingjay/ttaod_foundation.</description>
      <author>example@mail.com (Yingjie Gao, Yanan Zhang, Zhi Cai, Di Huang)</author>
      <guid isPermaLink="false">2510.25175v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>TabMGP: Martingale Posterior with TabPFN</title>
      <link>http://arxiv.org/abs/2510.25154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages (+3 reference, +22 appendix). Extra plots in  https://drive.google.com/drive/folders/1ct_effOoTEGpiWUf0_1xI3VqLWHtJY16&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于基础转换器的鞅后验方法（TabMGP），用于解决贝叶斯推断中的不确定性量化问题。&lt;h4&gt;背景&lt;/h4&gt;贝叶斯推断在不确定性量化方面具有优势，但面临先验设定、似然误设和计算负担等挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种有效的预测规则，用于构建高质量的鞅后验方法，提高不确定性量化的准确性。&lt;h4&gt;方法&lt;/h4&gt;利用基础转换器（特别是TabPFN，一种表格数据领域的最先进模型）构建鞅后验（TabMGP），通过自回归生成模拟前向数据生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;TabMGP产生的可信集具有接近名义覆盖范围的性能，并且通常优于现有的MGP结构和标准贝叶斯方法。&lt;h4&gt;结论&lt;/h4&gt;基于基础转换器的鞅后验方法（TabMGP）在表格数据的不确定性量化方面表现优异，为贝叶斯推断提供了有效替代方案。&lt;h4&gt;翻译&lt;/h4&gt;贝叶斯推断提供了有原则的不确定性量化，但常常受到先验设定、似然误设和计算负担的限制。鞅后验（MGP，Fong等人，2023年）提供了一种替代方案，用预测规则（即一步前向预测分布序列）替代先验-似然设定，用于前向数据生成。MGP的有效性取决于预测规则的选择，但文献中很少有令人信服的例子。基础转换器在这里非常适合，因为它们的自回归生成模拟了这种前向模拟，并且它们的通用设计能够实现丰富的预测建模。我们介绍了TabMGP，这是一种基于TabPFN构建的MGP，TabPFN是表格数据当前最先进的基础模型。TabMGP产生的可信集具有接近名义覆盖范围的性能，并且通常优于现有的MGP结构和标准贝叶斯方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bayesian inference provides principled uncertainty quantification but isoften limited by challenges of prior elicitation, likelihood misspecification,and computational burden. The martingale posterior (MGP, Fong et al., 2023)offers an alternative, replacing prior-likelihood elicitation with a predictiverule - namely, a sequence of one-step-ahead predictive distributions - forforward data generation. The utility of MGPs depends on the choice ofpredictive rule, yet the literature has offered few compelling examples.Foundation transformers are well-suited here, as their autoregressivegeneration mirrors this forward simulation and their general-purpose designenables rich predictive modeling. We introduce TabMGP, an MGP built on TabPFN,a transformer foundation model that is currently state-of-the-art for tabulardata. TabMGP produces credible sets with near-nominal coverage and oftenoutperforms both existing MGP constructions and standard Bayes.</description>
      <author>example@mail.com (Kenyon Ng, Edwin Fong, David T. Frazier, Jeremias Knoblauch, Susan Wei)</author>
      <guid isPermaLink="false">2510.25154v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>POWSM: A Phonetic Open Whisper-Style Speech Foundation Model</title>
      <link>http://arxiv.org/abs/2510.24992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;POWSM是一个统一的语音处理框架，能够同时执行多个音位相关任务，性能与专业模型相当，且支持多种转换功能。&lt;h4&gt;背景&lt;/h4&gt;语音处理领域在自动语音识别(ASR)、音位识别(PR)、字形到音位转换(G2P)和音位到字形转换(P2G)等音位任务上取得了显著进展，但这些任务大多是孤立研究的，每个任务都依赖于特定的架构和数据集。&lt;h4&gt;目的&lt;/h4&gt;引入POWSM（Phonetic Open Whisper-style Speech Model），创建第一个能够同时执行多个与音位相关任务的统一框架。&lt;h4&gt;方法&lt;/h4&gt;POWSM模型实现了音频、文本（字形）和音位之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;POWSM在性能上与类似大小的专业PR模型（Wav2Vec2Phoneme和ZIPA）相当或更优，同时支持G2P、P2G和ASR。&lt;h4&gt;结论&lt;/h4&gt;训练数据、代码和模型已公开发布，以促进开放科学。&lt;h4&gt;翻译&lt;/h4&gt;最近语音处理方面的进展已在语音识别、音位识别、字形到音位转换和音位到字形转换等音位任务中取得实质性进展。尽管这些任务在概念上相似，但它们大多被孤立研究，每个任务都依赖于特定的架构和数据集。在本文中，我们引入了POWSM（Phonetic Open Whisper-style Speech Model），这是第一个能够同时执行多个音位相关任务的统一框架。POWSM实现了音频、文本（字形）和音位之间的无缝转换，为通用和低资源语音处理开辟了新的可能性。我们的模型在性能上与类似大小的专业PR模型（Wav2Vec2Phoneme和ZIPA）相当或更优，同时支持G2P、P2G和ASR。我们的训练数据、代码和模型已公开发布，以促进开放科学。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in spoken language processing have led to substantialprogress in phonetic tasks such as automatic speech recognition (ASR), phonerecognition (PR), grapheme-to-phoneme conversion (G2P), and phoneme-to-graphemeconversion (P2G). Despite their conceptual similarity, these tasks have largelybeen studied in isolation, each relying on task-specific architectures anddatasets. In this paper, we introduce POWSM (Phonetic Open Whisper-style SpeechModel), the first unified framework capable of jointly performing multiplephone-related tasks. POWSM enables seamless conversion between audio, text(graphemes), and phones, opening up new possibilities for universal andlow-resource speech processing. Our model outperforms or matches specialized PRmodels of similar size (Wav2Vec2Phoneme and ZIPA) while jointly supporting G2P,P2G, and ASR. Our training data, code and models are released to foster openscience.</description>
      <author>example@mail.com (Chin-Jou Li, Kalvin Chang, Shikhar Bharadwaj, Eunjung Yeo, Kwanghee Choi, Jian Zhu, David Mortensen, Shinji Watanabe)</author>
      <guid isPermaLink="false">2510.24992v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Pearl: A Foundation Model for Placing Every Atom in the Right Location</title>
      <link>http://arxiv.org/abs/2510.24670v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  technical report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Pearl是一个用于蛋白质-配体共同折叠的基础模型，通过创新的训练方法、架构设计和推理控制，显著提高了蛋白质-配体复合物结构预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构是计算药物发现中的基本挑战，限制了治疗设计的速度和成功。深度学习方法虽有潜力，但受限于实验数据稀缺、架构效率低下、物理无效构象以及辅助信息利用能力有限等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够克服当前蛋白质-配体结构预测局限性的基础模型，提高预测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;Pearl通过三个关键创新解决挑战：(1)包含大规模合成数据的训练配方，克服数据稀缺；(2)融合SO(3)-等变扩散模块的架构，尊重3D旋转对称性，提高泛化能力；(3)可控推理系统，支持蛋白质和非聚合物组分以及无条件/条件模式。&lt;h4&gt;主要发现&lt;/h4&gt;Pearl在蛋白质-配体共同折叠方面建立了新性能标准，在公共基准测试中超越AlphaFold3和其他开源模型14%以上；在口袋条件共同折叠模式下，对真实世界药物目标实现了3.6倍的改进；模型性能与训练中使用的合成数据集大小直接相关。&lt;h4&gt;结论&lt;/h4&gt;Pearl通过创新的训练方法、架构设计和推理控制，显著提高了蛋白质-配体复合物结构预测的准确性，为药物设计提供了更强大的工具。&lt;h4&gt;翻译&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构仍然是计算药物发现中的一个基本挑战，它限制了治疗设计的速度和成功率。深度学习方法最近作为结构预测工具显示出强大的潜力，在各种生物分子系统中取得了有希望的准确性。然而，它们的性能和实用性受到实验数据稀缺、架构效率低下、物理无效构象以及在推理过程中利用辅助信息能力有限等因素的制约。为解决这些问题，我们引入了Pearl（Placing Every Atom in the Right Location），一个用于大规模蛋白质-配体共同折叠的基础模型。Pearl通过三个关键创新来解决这些挑战：(1)包含大规模合成数据的训练配方，以克服数据稀缺；(2)融合SO(3)-等变扩散模块的架构， inherently尊重3D旋转对称性，提高泛化能力和样本效率；(3)可控推理，包括支持蛋白质和非聚合物组分以及无条件/条件模式的双链通用模板系统。Pearl在蛋白质-配体共同折叠方面建立了新的最先进性能。在生成准确（RMSD &lt; 2Å）和物理有效构象的关键指标上，Pearl在公共Runs N' Poses和PoseBusters基准测试中超越了AlphaFold3和其他开源基线，比次优模型分别提高了14.5%和14.2%。在口袋条件共同折叠模式下，Pearl在一个具有挑战性的真实世界药物目标专有集上，在更严格的RMSD &lt; 1Å阈值下实现了3.6倍的改进。最后，我们证明了模型性能与训练中使用的合成数据集大小直接相关。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the three-dimensional structures of protein-ligandcomplexes remains a fundamental challenge in computational drug discovery thatlimits the pace and success of therapeutic design. Deep learning methods haverecently shown strong potential as structural prediction tools, achievingpromising accuracy across diverse biomolecular systems. However, theirperformance and utility are constrained by scarce experimental data,inefficient architectures, physically invalid poses, and the limited ability toexploit auxiliary information available at inference. To address these issues,we introduce Pearl (Placing Every Atom in the Right Location), a foundationmodel for protein-ligand cofolding at scale. Pearl addresses these challengeswith three key innovations: (1) training recipes that include large-scalesynthetic data to overcome data scarcity; (2) architectures that incorporate anSO(3)-equivariant diffusion module to inherently respect 3D rotationalsymmetries, improving generalization and sample efficiency, and (3)controllable inference, including a generalized multi-chain templating systemsupporting both protein and non-polymeric components as well as dualunconditional/conditional modes. Pearl establishes a new state-of-the-artperformance in protein-ligand cofolding. On the key metric of generatingaccurate (RMSD &lt; 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold3 and other open source baselines on the public Runs N' Poses and PoseBustersbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over thenext best model. In the pocket-conditional cofolding regime, Pearl delivers$3.6\times$ improvement on a proprietary set of challenging, real-world drugtargets at the more rigorous RMSD &lt; 1 \r{A} threshold. Finally, we demonstratethat model performance correlates directly with synthetic dataset size used intraining.</description>
      <author>example@mail.com (Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Roy Tal, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan)</author>
      <guid isPermaLink="false">2510.24670v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Compiler.next: A Search-Based Compiler to Power the AI-Native Future of Software Engineering</title>
      <link>http://arxiv.org/abs/2510.24799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 5 figures, submitted to ACM Transactions on Software  Engineering and Methodology&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出Compiler.next，一种基于搜索的新型编译器，旨在解决AI辅助软件工程中的认知过载、工具集成效率低和AI副驾驶功能有限等问题，实现AI原生软件系统的无缝演进。&lt;h4&gt;背景&lt;/h4&gt;AI辅助软件工程快速发展，但现有工具和范式受认知过载、工具集成效率低下和AI副驾驶功能有限等因素的限制。&lt;h4&gt;目的&lt;/h4&gt;提出Compiler.next作为软件工程3.0时代的一部分，实现AI原生软件系统的无缝演进，降低非专家技术门槛，实现可扩展、适应性强和可靠的AI驱动软件。&lt;h4&gt;方法&lt;/h4&gt;Compiler.next接受人类编写的意图，通过搜索最优解决方案自动生成工作软件，涉及认知架构及其组成部分的动态优化，在准确性、成本和延迟等多个目标间找到最佳平衡。&lt;h4&gt;主要发现&lt;/h4&gt;Compiler.next的架构设计使其能够作为降低非专家技术门槛、实现可扩展、适应性强和可靠的AI驱动软件的基石。&lt;h4&gt;结论&lt;/h4&gt;Compiler.next为完全自动化、搜索驱动的软件开发奠定了基础，促进了更快创新和更高效的AI驱动系统，解决了意图编译的核心挑战。&lt;h4&gt;翻译&lt;/h4&gt;AI辅助软件工程的快速发展为软件工程领域带来了变革潜力，但现有工具和范式仍然受到认知过载、工具集成效率低下以及AI副驾驶功能有限等因素的限制。为此，我们提出了Compiler.next，一种基于搜索的新型编译器，作为新兴软件工程3.0时代的一部分，旨在实现AI原生软件系统的无缝演进。与传统的静态编译器不同，Compiler.next接受人类编写的意图，并通过搜索最优解决方案来自动生成工作软件。这一过程涉及认知架构及其组成部分（如提示、基础模型配置和系统参数）的动态优化，同时找到准确性、成本和延迟等多个目标之间的最佳权衡。本文概述了Compiler.next的架构，并将其定位为降低非专家技术门槛、实现可扩展、适应性强和可靠的AI驱动软件的基石。我们提出了一个路线图来解决意图编译中的核心挑战，包括开发高质量编程构造、有效搜索启发式方法、可复现性以及编译器之间的互操作性。我们的愿景为完全自动化、搜索驱动的软件开发奠定了基础，促进了更快创新和更高效的AI驱动系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancement of AI-assisted software engineering has broughttransformative potential to the field of software engineering, but existingtools and paradigms remain limited by cognitive overload, inefficient toolintegration, and the narrow capabilities of AI copilots. In response, wepropose Compiler.next, a novel search-based compiler designed to enable theseamless evolution of AI-native software systems as part of the emergingSoftware Engineering 3.0 era. Unlike traditional static compilers,Compiler.next takes human-written intents and automatically generates workingsoftware by searching for an optimal solution. This process involves dynamicoptimization of cognitive architectures and their constituents (e.g., prompts,foundation model configurations, and system parameters) while finding theoptimal trade-off between several objectives, such as accuracy, cost, andlatency. This paper outlines the architecture of Compiler.next and positions itas a cornerstone in democratizing software development by lowering thetechnical barrier for non-experts, enabling scalable, adaptable, and reliableAI-powered software. We present a roadmap to address the core challenges inintent compilation, including developing quality programming constructs,effective search heuristics, reproducibility, and interoperability betweencompilers. Our vision lays the groundwork for fully automated, search-drivensoftware development, fostering faster innovation and more efficient AI-drivensystems.</description>
      <author>example@mail.com (Filipe R. Cogo, Gustavo A. Oliva, Ahmed E. Hassan)</author>
      <guid isPermaLink="false">2510.24799v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Why Foundation Models in Pathology Are Failing</title>
      <link>http://arxiv.org/abs/2510.23807v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了基础模型在计算病理学应用中的不足，指出其存在诊断准确率低、鲁棒性差等问题，并分析了这些问题背后的七个相互关联原因，认为当前病理学基础模型在概念上与组织形态学本质不匹配，需要范式重构。&lt;h4&gt;背景&lt;/h4&gt;在非医疗领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理，人们预期其在计算病理学领域也能取得类似突破。&lt;h4&gt;目的&lt;/h4&gt;检查基础模型在计算病理学应用中的不足，分析这些缺点背后的根本原因。&lt;h4&gt;方法&lt;/h4&gt;通过系统评估方法检查基础模型的缺点，并分析这些缺点背后的原因。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型存在诊断准确率低、鲁棒性差、几何不稳定性、计算需求量大以及安全漏洞等问题；这些问题源于七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。&lt;h4&gt;结论&lt;/h4&gt;当前病理学基础模型在概念上与组织形态学本质不匹配，需要对范式本身进行根本性的重新思考。&lt;h4&gt;翻译&lt;/h4&gt;在非医疗领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理。因此，计算病理学领域对这类模型的快速应用预期能在癌症诊断、预后判断和多模态检索方面带来类似的突破。然而，最近的系统评估揭示了根本性弱点：低诊断准确率、差鲁棒性、几何不稳定性、高计算需求以及令人担忧的安全漏洞。这篇短文检查了这些不足，并认为它们源于主流人工智能中通用基础建模的基本假设与人体组织内在复杂性之间的更深层次概念不匹配。确定了七个相互关联的原因：生物复杂性、无效的自监督、过度泛化、过度的架构复杂性、缺乏领域特定创新、数据不足以及与组织块大小相关的基本设计缺陷。这些发现表明，当前的病理学基础模型在概念上仍与组织形态学的本质不匹配，需要对范式本身进行根本性的重新思考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In non-medical domains, foundation models (FMs) have revolutionized computervision and language processing through large-scale self-supervised andmultimodal learning. Consequently, their rapid adoption in computationalpathology was expected to deliver comparable breakthroughs in cancer diagnosis,prognostication, and multimodal retrieval. However, recent systematicevaluations reveal fundamental weaknesses: low diagnostic accuracy, poorrobustness, geometric instability, heavy computational demands, and concerningsafety vulnerabilities. This short paper examines these shortcomings and arguesthat they stem from deeper conceptual mismatches between the assumptionsunderlying generic foundation modeling in mainstream AI and the intrinsiccomplexity of human tissue. Seven interrelated causes are identified:biological complexity, ineffective self-supervision, overgeneralization,excessive architectural complexity, lack of domain-specific innovation,insufficient data, and a fundamental design flaw related to tissue patch size.These findings suggest that current pathology foundation models remainconceptually misaligned with the nature of tissue morphology and call for afundamental rethinking of the paradigm itself.</description>
      <author>example@mail.com (Hamid R. Tizhoosh)</author>
      <guid isPermaLink="false">2510.23807v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出动态人格完善框架(DPRF)，用于提高大型语言模型角色扮演代理的行为与目标个体的一致性，通过迭代识别认知分歧并优化个人资料实现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型角色扮演代理旨在模拟个体人类行为，但手动创建的个人资料(如精心挑选的信息和个性特征)未经验证与目标个体的对齐度，导致人格保真度受损。&lt;h4&gt;目的&lt;/h4&gt;优化LLM RPAs的行为与目标个体行为的一致性，提高角色扮演的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;DPRF通过迭代识别生成行为与人类真实认知之间的分歧(无论是自由形式还是基于理论的结构化分析)，并完善个人资料以减轻这些分歧。&lt;h4&gt;主要发现&lt;/h4&gt;在五个大型语言模型和四种多样的行为预测场景(正式辩论、涉及心理健康问题的社交媒体帖子、公开采访和电影评论)中，DPRF能够显著提高行为一致性，并且跨模型和场景具有通用性。&lt;h4&gt;结论&lt;/h4&gt;该工作为创建高保真度个人资料和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;h4&gt;翻译&lt;/h4&gt;新兴的大型语言模型角色扮演代理旨在模拟个体人类行为，但人格保真度常因手动创建的个人资料(如精心挑选的信息和个性特征)未经验证与目标个体的对齐度而受损。为解决这一局限，我们的工作引入了动态人格完善框架(DPRF)。DPRF旨在通过迭代识别生成行为与人类真实认知之间的分歧(无论是自由形式还是基于理论的结构化分析)，并完善个人资料以减轻这些分歧，从而优化LLM RPAs的行为与目标个体行为的一致性。我们在五个大型语言模型和四种多样的行为预测场景(正式辩论、涉及心理健康问题的社交媒体帖子、公开采访和电影评论)中评估了DPRF。DPRF能够显著提高行为一致性，并且跨模型和场景具有通用性。我们的工作为创建高保真度个人资料和增强下游应用(如用户模拟、社会研究和个性化AI)的有效性提供了稳健的方法论。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF). DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences. We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews. DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios. Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v3</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Graph Network-based Structural Simulator: Graph Neural Networks for Structural Dynamics</title>
      <link>http://arxiv.org/abs/2510.25683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 14 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为GNSS的图神经网络框架，用于动态结构问题的代理建模，通过三个关键特征解决了现有方法的局限性，在案例研究中表现出色，实现了比传统方法更快的推理速度。&lt;h4&gt;背景&lt;/h4&gt;图神经网络作为数值模拟的代理模型已在计算流体动力学领域有所研究，但在结构问题特别是动态情况中的应用相对较少，存在研究空白。&lt;h4&gt;目的&lt;/h4&gt;为了填补动态结构问题中图神经网络应用的空白，作者开发了GNSS框架，专门用于动态结构问题的代理建模。&lt;h4&gt;方法&lt;/h4&gt;GNSS遵循编码-处理-解码范式，具有三个关键特征：在节点固定的局部框架中表达节点运动学；采用符号感知回归损失减少相位误差；使用波长感知的连接半径优化图结构构建。&lt;h4&gt;主要发现&lt;/h4&gt;GNSS在50kHz汉宁调制脉冲激励梁的案例研究中，能够准确复现物理特性并泛化到未见过的加载条件，而现有GNN方法无法收敛。与有限元方法相比，GNSS实现了显著的推理加速同时保持空间和时间保真度。&lt;h4&gt;结论&lt;/h4&gt;具有物理一致性更新规则且保持局部性的图神经网络是动态、波主导结构模拟的有竞争力的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络最近被探索作为数值模拟的代理模型。虽然它们在计算流体动力学中的应用已被研究，但很少被应用于结构问题，特别是动态情况。为了解决这一研究空白，我们引入了基于图网络的结构模拟器，这是一个用于动态结构问题代理建模的图神经网络框架。GNSS遵循基于GNN的机器学习模型的典型编码-处理-解码范式，其设计使其特别适合动态模拟，这得益于三个关键特征：在节点固定的局部框架中表达节点运动学，避免有限差分速度中的灾难性取消；采用符号感知回归损失，减少长期rollout中的相位误差；使用波长感知的连接半径，优化图结构构建。我们在一个涉及由50kHz汉宁调制脉冲激励的梁的案例研究中评估了GNSS。结果表明GNSS能够在数百个时间步长内准确复现问题的物理特性，并能泛化到未见过的加载条件，而现有的GNN方法无法收敛或提供有意义的预测。与显式有限元基线方法相比，GNSS在保持空间和时间保真度的同时实现了显著的推理加速。这些发现表明，具有物理一致性更新规则且保持局部性的GNN是动态、波主导结构模拟的有竞争力的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently been explored as surrogate modelsfor numerical simulations. While their applications in computational fluiddynamics have been investigated, little attention has been given to structuralproblems, especially for dynamic cases. To address this gap, we introduce theGraph Network-based Structural Simulator (GNSS), a GNN framework for surrogatemodeling of dynamic structural problems.  GNSS follows the encode-process-decode paradigm typical of GNN-based machinelearning models, and its design makes it particularly suited for dynamicsimulations thanks to three key features: (i) expressing node kinematics innode-fixed local frames, which avoids catastrophic cancellation infinite-difference velocities; (ii) employing a sign-aware regression loss,which reduces phase errors in long rollouts; and (iii) using awavelength-informed connectivity radius, which optimizes graph construction.  We evaluate GNSS on a case study involving a beam excited by a 50kHzHanning-modulated pulse. The results show that GNSS accurately reproduces thephysics of the problem over hundreds of timesteps and generalizes to unseenloading conditions, where existing GNNs fail to converge or deliver meaningfulpredictions.  Compared with explicit finite element baselines, GNSS achieves substantialinference speedups while preserving spatial and temporal fidelity. Thesefindings demonstrate that locality-preserving GNNs with physics-consistentupdate rules are a competitive alternative for dynamic, wave-dominatedstructural simulations.</description>
      <author>example@mail.com (Alessandro Lucchetti, Francesco Cadini, Marco Giglio, Luca Lomazzi)</author>
      <guid isPermaLink="false">2510.25683v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>Bridging the Divide: End-to-End Sequence-Graph Learning</title>
      <link>http://arxiv.org/abs/2510.25126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BRIDGE是一种统一的端到端架构，能够联合学习序列和图信息，在友谊预测和欺诈检测任务上表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;现实世界的数据集通常是序列性和关系性的，每个节点携带事件序列，边编码交互。现有方法往往只考虑一种模态而忽略另一种。&lt;h4&gt;目的&lt;/h4&gt;作者认为序列和图是同一数据集的互补方面，应该联合学习，而不是作为独立问题处理。&lt;h4&gt;方法&lt;/h4&gt;BRIDGE将序列编码器与图神经网络(GNN)耦合在单一目标下，允许梯度在两个模块间流动。添加了TOKENXATTN标记级交叉注意力层，实现邻居间细粒度的标记级消息传递。&lt;h4&gt;主要发现&lt;/h4&gt;在友谊预测(Brightkite)和欺诈检测(Amazon)两种场景下，BRIDGE在排序和分类指标上始终优于静态GNN、时图方法和仅基于序列的基线。&lt;h4&gt;结论&lt;/h4&gt;BRIDGE通过联合学习序列和图信息，能够学习任务对齐的表示，在各种任务上表现优异。&lt;h4&gt;翻译&lt;/h4&gt;许多现实世界的数据集既是序列性的又是关系性的：每个节点携带事件序列，而边则编码交互。现有的序列建模和图建模方法通常忽略了一种或另一种模态。我们认为序列和图不是独立的问题，而是同一数据集的互补方面，应该联合学习。我们引入了BRIDGE，一个统一的端到端架构，将序列编码器与GNN在单一目标下耦合，允许梯度在两个模块间流动，并学习任务对齐的表示。为了实现邻居间细粒度的标记级消息传递，我们添加了TOKENXATTN，一个标记级交叉注意力层，用于在相邻序列的事件之间传递消息。在友谊预测(Brightkite)和欺诈检测(Amazon)两种设置下，BRIDGE在排序和分类指标上始终优于静态GNN、时图方法和仅基于序列的基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many real-world datasets are both sequential and relational: each nodecarries an event sequence while edges encode interactions. Existing methods insequence modeling and graph modeling often neglect one modality or the other.We argue that sequences and graphs are not separate problems but complementaryfacets of the same dataset, and should be learned jointly. We introduce BRIDGE,a unified end-to-end architecture that couples a sequence encoder with a GNNunder a single objective, allowing gradients to flow across both modules andlearning task-aligned representations. To enable fine-grained token-levelmessage passing among neighbors, we add TOKENXATTN, a token-levelcross-attention layer that passes messages between events in neighboringsequences. Across two settings, friendship prediction (Brightkite) and frauddetection (Amazon), BRIDGE consistently outperforms static GNNs, temporal graphmethods, and sequence-only baselines on ranking and classification metrics.</description>
      <author>example@mail.com (Yuen Chen, Yulun Wu, Samuel Sharpe, Igor Melnyk, Nam H. Nguyen, Furong Huang, C. Bayan Bruss, Rizal Fathony)</author>
      <guid isPermaLink="false">2510.25126v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>The Underappreciated Power of Vision Models for Graph Structural Understanding</title>
      <link>http://arxiv.org/abs/2510.24788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了图神经网络与人类视觉感知的差异，发现视觉模型在图理解任务中具有与GNNs相当的性能但展现出不同的学习模式。作者提出了新的基准测试GraphAbstract，评估模型对全局图属性的理解能力，结果表明视觉模型在需要整体结构理解的任务上优于GNNs。&lt;h4&gt;背景&lt;/h4&gt;图神经网络通过自下而上的消息传递机制工作，这与人类视觉感知先捕捉全局结构的直觉方式有根本不同。现有基准测试将领域特征与拓扑理解混为一谈，无法有效评估模型对图全局结构的理解能力。&lt;h4&gt;目的&lt;/h4&gt;研究视觉模型在图理解方面的潜力，评估它们与GNNs在性能和学习模式上的差异，并开发新的基准测试来评估模型对全局图属性的理解能力。&lt;h4&gt;方法&lt;/h4&gt;提出了名为GraphAbstract的新基准测试，评估模型识别组织原型、检测对称性、感知连接强度和识别关键元素的能力，这些能力与人类对图的全局理解方式相似。&lt;h4&gt;主要发现&lt;/h4&gt;视觉模型在需要整体结构理解的任务上显著优于GNNs；视觉模型在不同图规模上保持泛化能力；GNNs在全局模式抽象方面存在困难，且随着图规模增大性能下降；视觉模型具有显著的但未被充分利用的图结构理解能力。&lt;h4&gt;结论&lt;/h4&gt;视觉模型在需要全局拓扑意识和尺度不变推理的问题上具有显著的能力，这些发现为开发更有效的图基础模型开辟了新途径，特别是对于那些由整体模式识别主导的任务。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络通过自下而上的消息传递运行，这与人类视觉感知有根本不同，人类视觉直觉上首先捕捉全局结构。我们研究了视觉模型在图理解方面的未被充分认识到的潜力，发现它们在既定基准上实现了与GNNs相当的性能，同时表现出明显不同的学习模式。这些不同的行为，加上现有基准将领域特征与拓扑理解混为一谈的限制，促使我们引入GraphAbstract。这个基准评估模型像人类一样感知全局图属性的能力：识别组织原型、检测对称性、感知连接强度和识别关键元素。我们的结果显示，在需要整体结构理解的任务上，视觉模型显著优于GNNs，并且在不同图规模上保持泛化能力，而GNNs在全局模式抽象方面存在困难，且随着图规模增大性能下降。这项工作表明，视觉模型具有显著的但未被充分利用的图结构理解能力，特别是对于需要全局拓扑意识和尺度不变推理的问题。这些发现为利用这种未被充分认识到的潜力开发更有效的图基础模型开辟了新途径，这些任务主要由整体模式识别主导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks operate through bottom-up message-passing,fundamentally differing from human visual perception, which intuitivelycaptures global structures first. We investigate the underappreciated potentialof vision models for graph understanding, finding they achieve performancecomparable to GNNs on established benchmarks while exhibiting distinctlydifferent learning patterns. These divergent behaviors, combined withlimitations of existing benchmarks that conflate domain features withtopological understanding, motivate our introduction of GraphAbstract. Thisbenchmark evaluates models' ability to perceive global graph properties ashumans do: recognizing organizational archetypes, detecting symmetry, sensingconnectivity strength, and identifying critical elements. Our results revealthat vision models significantly outperform GNNs on tasks requiring holisticstructural understanding and maintain generalizability across varying graphscales, while GNNs struggle with global pattern abstraction and degrade withincreasing graph size. This work demonstrates that vision models possessremarkable yet underutilized capabilities for graph structural understanding,particularly for problems requiring global topological awareness andscale-invariant reasoning. These findings open new avenues to leverage thisunderappreciated potential for developing more effective graph foundationmodels for tasks dominated by holistic pattern recognition.</description>
      <author>example@mail.com (Xinjian Zhao, Wei Pang, Zhongkai Xue, Xiangru Jian, Lei Zhang, Yaoyao Xu, Xiaozhuang Song, Shu Wu, Tianshu Yu)</author>
      <guid isPermaLink="false">2510.24788v1</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>FastJAM: a Fast Joint Alignment Model for Images</title>
      <link>http://arxiv.org/abs/2510.22842v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31  are Supplemental Material. FastJAM website -  https://bgu-cs-vil.github.io/FastJAM/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为FastJAM的快速图像联合对齐方法，显著降低计算复杂度，实现高质量对齐效果。&lt;h4&gt;背景&lt;/h4&gt;现有图像联合对齐方法通常需要长时间训练、大容量模型和大量超参数调整，计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的图像联合对齐方法，减少计算时间并提高对齐质量。&lt;h4&gt;方法&lt;/h4&gt;FastJAM基于图方法，利用现成图像匹配器计算pairwise匹配，结合快速非参数聚类构建表示图像内和图像间关键点关系的图。通过图神经网络传播和聚合对应关系，利用图像级池化预测单应性参数。采用逆组合损失消除正则化项需求，避免超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FastJAM在多个基准测试上实现了优于现有现代JA方法的对齐质量，同时将计算时间从小时或分钟级减少到几秒钟。&lt;h4&gt;结论&lt;/h4&gt;FastJAM通过创新的图神经网络和逆组合损失方法，实现了快速、高效的图像联合对齐，为图像处理领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图像联合对齐（JA）旨在将一组图像对齐到统一的坐标系中，使语义相似的特征出现在对应的空间位置。大多数现有方法通常需要长时间训练、大容量模型和大量超参数调整。我们引入了FastJAM，一种快速的基于图的方法，显著降低了联合对齐任务的计算复杂度。FastJAM利用现成的图像匹配器计算的pairwise匹配，结合快速非参数聚类，构建表示图像内和图像间关键点关系的图。图神经网络传播和聚合这些对应关系，通过图像级池化有效预测每个图像的单应性参数。利用逆组合损失，消除了对预测变换的正则化项的需求（因此也避免了与这些项相关的超参数调整），FastJAM能够快速有效地执行图像JA。在几个基准测试上的实验结果表明，FastJAM在对齐质量方面优于现有的现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。我们的代码可在项目网页获取：https://bgu-cs-vil.github.io/FastJAM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Alignment (JA) of images aims to align a collection of images into aunified coordinate frame, such that semantically-similar features appear atcorresponding spatial locations. Most existing approaches often require longtraining times, large-capacity models, and extensive hyperparameter tuning. Weintroduce FastJAM, a rapid, graph-based method that drastically reduces thecomputational complexity of joint alignment tasks. FastJAM leverages pairwisematches computed by an off-the-shelf image matcher, together with a rapidnonparametric clustering, to construct a graph representing intra- andinter-image keypoint relations. A graph neural network propagates andaggregates these correspondences, efficiently predicting per-image homographyparameters via image-level pooling. Utilizing an inverse-compositional loss,that eliminates the need for a regularization term over the predictedtransformations (and thus also obviates the hyperparameter tuning associatedwith such terms), FastJAM performs image JA quickly and effectively.Experimental results on several benchmarks demonstrate that FastJAM achievesresults better than existing modern JA methods in terms of alignment quality,while reducing computation time from hours or minutes to mere seconds. Our codeis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</description>
      <author>example@mail.com (Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld)</author>
      <guid isPermaLink="false">2510.22842v2</guid>
      <pubDate>Thu, 30 Oct 2025 14:52:15 +0800</pubDate>
    </item>
    <item>
      <title>JiuTian Chuanliu: A Large Spatiotemporal Model for General-purpose Dynamic Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.23662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为通用动态人类移动性嵌入(GDHME)的框架，用于处理大规模人类移动性数据，发现移动行为背后的潜在语义，并支持各种城市感知任务。&lt;h4&gt;背景&lt;/h4&gt;人类移动性作为城市感知的窗口，包含丰富的时空信息，反映了居民行为偏好和城市区域功能。现有方法通常从特定角度处理特定任务，导致对人类移动性建模不足，所学知识在下游应用中适用性有限。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，通过时空模型处理大量人类移动性数据，发现潜在语义，支持多种城市感知任务。&lt;h4&gt;方法&lt;/h4&gt;GDHME框架遵循自监督学习思想，包含两个阶段：第一阶段将人和区域视为动态图中的节点，统一为人-区域-时间交互，使用连续时间编码器计算演化节点表示，并设计自回归自监督任务引导学习；第二阶段利用这些表示支持各种任务。通过基站系统收集大规模移动数据，并构建多任务城市感知基准进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;离线实验证明GDHME能从大量数据中自动学习有价值的节点特征。该框架已成功部署九天川流大模型，该系统在2023年中国移动全球合作伙伴大会上展示。&lt;h4&gt;结论&lt;/h4&gt;GDHME框架能有效处理人类移动性数据，提取有价值特征，支持多种城市感知任务，具有广泛的适用性和实用价值。&lt;h4&gt;翻译&lt;/h4&gt;作为城市感知的窗口，人类移动性包含丰富的时空信息，反映了居民的行为偏好和城市区域的功能。人类移动性分析吸引了众多研究者的关注。然而，现有方法通常从特定角度处理特定任务，导致对人类移动性建模不足，所学知识在各种下游应用中适用性有限。为解决这些挑战，本文提出将大量人类移动性数据输入时空模型，发现移动行为背后的潜在语义，并支持各种城市感知任务。具体来说，通过无处不在的基站系统收集大规模、广泛覆盖的人类移动性数据，并引入了一个名为通用动态人类移动性嵌入(GDHME)的城市感知框架。该框架遵循自监督学习思想，包含两个主要阶段。第一阶段，GDHME将人和区域视为动态图中的节点，将人类移动性数据统一为人-区域-时间交互。在连续时间运行的编码器动态计算演化的节点表示，捕捉人和区域的动态状态。此外，专门设计了自回归自监督任务来引导通用节点嵌入的学习。第二阶段，利用这些表示来支持各种任务。为评估GDHME框架的有效性，作者构建了一个多任务城市感知基准。离线实验证明了GDHME能够从大量数据中自动学习有价值的节点特征。此外，该框架被用于部署九天川流大模型，该系统已在2023年中国移动全球合作伙伴大会上展示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a window for urban sensing, human mobility contains rich spatiotemporalinformation that reflects both residents' behavior preferences and thefunctions of urban areas. The analysis of human mobility has attracted theattention of many researchers. However, existing methods often address specifictasks from a particular perspective, leading to insufficient modeling of humanmobility and limited applicability of the learned knowledge in variousdownstream applications. To address these challenges, this paper proposes topush massive amounts of human mobility data into a spatiotemporal model,discover latent semantics behind mobility behavior and support various urbansensing tasks. Specifically, a large-scale and widely covering human mobilitydata is collected through the ubiquitous base station system and a frameworknamed General-purpose and Dynamic Human Mobility Embedding (GDHME) for urbansensing is introduced. The framework follows the self-supervised learning ideaand contains two major stages. In stage 1, GDHME treats people and regions asnodes within a dynamic graph, unifying human mobility data aspeople-region-time interactions. An encoder operating in continuous-timedynamically computes evolving node representations, capturing dynamic statesfor both people and regions. Moreover, an autoregressive self-supervised taskis specially designed to guide the learning of the general-purpose nodeembeddings. In stage 2, these representations are utilized to support varioustasks. To evaluate the effectiveness of our GDHME framework, we furtherconstruct a multi-task urban sensing benchmark. Offline experiments demonstrateGDHME's ability to automatically learn valuable node features from vast amountsof data. Furthermore, our framework is used to deploy the JiuTian ChuanLiu BigModel, a system that has been presented at the 2023 China Mobile WorldwidePartner Conference.</description>
      <author>example@mail.com (Liangzhe Han, Leilei Sun, Tongyu Zhu, Tao Tao, Jibin Wang, Weifeng Lv)</author>
      <guid isPermaLink="false">2510.23662v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
  <item>
      <title>Pearl: A Foundation Model for Placing Every Atom in the Right Location</title>
      <link>http://arxiv.org/abs/2510.24670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了Pearl，一种用于蛋白质-配体协同折叠的基础模型，通过三个关键创新解决了深度学习方法在结构预测中的局限性，实现了最先进的性能表现。&lt;h4&gt;背景&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构是计算药物发现中的基本挑战，限制了治疗设计的速度和成功率。虽然深度学习方法显示出潜力，但其性能受限于实验数据稀少、架构效率低下、物理无效构象以及无法充分利用可用辅助信息等因素。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够克服数据稀缺、提高架构效率、确保物理有效性并充分利用辅助信息的蛋白质-配体结构预测模型。&lt;h4&gt;方法&lt;/h4&gt;作者提出了Pearl（Placing Every Atom in the Right Location）模型，包含三个关键创新：(1) 使用大规模合成数据的训练方法以克服数据稀缺；(2) 融入SO(3)-等变扩散模块的架构，尊重3D旋转对称性；(3) 支持蛋白质和非聚合物组分的通用多链模板系统，以及无条件/条件双模式的可控推理。&lt;h4&gt;主要发现&lt;/h4&gt;Pearl在蛋白质-配体协同折叠方面建立了新的性能标准，在公共基准测试中超越了AlphaFold3和其他开源基线模型，准确构象生成比次优模型分别提高了14.5%和14.2%。在口袋条件协同折叠模式下，Pearl在严格标准下实现了3.6倍的改进，且模型性能与训练中使用的合成数据集大小直接相关。&lt;h4&gt;结论&lt;/h4&gt;Pearl通过创新的训练方法、架构设计和推理机制，显著提高了蛋白质-配体复合物结构预测的准确性，合成数据的使用对模型性能有直接积极影响。&lt;h4&gt;翻译&lt;/h4&gt;准确预测蛋白质-配体复合物的三维结构仍然是计算药物发现中的一个基本挑战，它限制了治疗设计的速度和成功率。深度学习方法最近显示出作为结构预测工具的强大潜力，在多样化的生物分子系统中取得了有希望的准确性。然而，它们的性能和效用受到实验数据稀少、架构效率低下、物理无效构象以及在推理阶段利用可用辅助信息的能力有限等因素的制约。为了解决这些问题，我们引入了Pearl（Placing Every Atom in the Right Location），一种用于大规模蛋白质-配体协同折叠的基础模型。Pearl通过三个关键创新解决了这些挑战：(1) 包括大规模合成数据的训练方法，以克服数据稀缺；(2) 融入SO(3)-等变扩散模块的架构，本质上尊重3D旋转对称性，提高泛化能力和样本效率；(3) 可控推理，包括支持蛋白质和非聚合物组分的通用多链模板系统，以及无条件/条件双模式。Pearl在蛋白质-配体协同折叠方面建立了新的最先进性能。在生成准确和物理有效构象的关键指标上，Pearl在公共基准测试中超越了AlphaFold3和其他开源基线模型，比次优模型分别提高了14.5%和14.2%。在口袋条件协同折叠模式下，Pearl在更严格的标准下实现了3.6倍的改进。最后，我们研究表明模型性能与训练中使用的合成数据集大小直接相关。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately predicting the three-dimensional structures of protein-ligandcomplexes remains a fundamental challenge in computational drug discovery thatlimits the pace and success of therapeutic design. Deep learning methods haverecently shown strong potential as structural prediction tools, achievingpromising accuracy across diverse biomolecular systems. However, theirperformance and utility are constrained by scarce experimental data,inefficient architectures, physically invalid poses, and the limited ability toexploit auxiliary information available at inference. To address these issues,we introduce Pearl (Placing Every Atom in the Right Location), a foundationmodel for protein-ligand cofolding at scale. Pearl addresses these challengeswith three key innovations: (1) training recipes that include large-scalesynthetic data to overcome data scarcity; (2) architectures that incorporate anSO(3)-equivariant diffusion module to inherently respect 3D rotationalsymmetries, improving generalization and sample efficiency, and (3)controllable inference, including a generalized multi-chain templating systemsupporting both protein and non-polymeric components as well as dualunconditional/conditional modes. Pearl establishes a new state-of-the-artperformance in protein-ligand cofolding. On the key metric of generatingaccurate (RMSD &lt; 2 \r{A}) and physically valid poses, Pearl surpasses AlphaFold3 and other open source baselines on the public Runs N' Poses and PoseBustersbenchmarks, delivering 14.5% and 14.2% improvements, respectively, over thenext best model. In the pocket-conditional cofolding regime, Pearl delivers$3.6\times$ improvement on a proprietary set of challenging, real-world drugtargets at the more rigorous RMSD &lt; 1 \r{A} threshold. Finally, we demonstratethat model performance correlates directly with synthetic dataset size used intraining.</description>
      <author>example@mail.com (Genesis Research Team, Alejandro Dobles, Nina Jovic, Kenneth Leidal, Pranav Murugan, David C. Williams, Drausin Wulsin, Nate Gruver, Christina X. Ji, Korrawat Pruegsanusak, Gianluca Scarpellini, Ansh Sharma, Wojciech Swiderski, Andrea Bootsma, Richard Strong Bowen, Charlotte Chen, Jamin Chen, Marc André Dämgen, Roy Tal Dew, Benjamin DiFrancesco, J. D. Fishman, Alla Ivanova, Zach Kagin, David Li-Bland, Zuli Liu, Igor Morozov, Jeffrey Ouyang-Zhang, Frank C. Pickard IV, Kushal S. Shah, Ben Shor, Gabriel Monteiro da Silva, Maxx Tessmer, Carl Tilbury, Cyr Vetcher, Daniel Zeng, Maruan Al-Shedivat, Aleksandra Faust, Evan N. Feinberg, Michael V. LeVine, Matteus Pan)</author>
      <guid isPermaLink="false">2510.24670v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Advancing site-specific disease and pest management in precision agriculture: From reasoning-driven foundation models to adaptive, feedback-based learning</title>
      <link>http://arxiv.org/abs/2510.24650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 8 figures, and 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该综述探讨了基础模型(FMs)在农业特定地点疾病管理(SSDM)中的应用，重点关注大型语言模型(LLMs)和视觉语言模型(VLMs)的发展及其在自适应学习、强化学习和数字孪生框架中的作用。&lt;h4&gt;背景&lt;/h4&gt;农业特定地点疾病管理通过机器学习和深度学习在实时计算机视觉方面取得快速进展，研究从手工特征提取发展到大规模自动化特征学习，基础模型为作物疾病数据处理带来全新方式。&lt;h4&gt;目的&lt;/h4&gt;筛选约40篇关于基础模型在SSDM中应用的论文，讨论其在自适应学习、强化学习和数字孪生框架中的作用，并分析当前发展状况和挑战。&lt;h4&gt;方法&lt;/h4&gt;分析基础模型如何整合视觉和文本数据，解释症状文本，推理症状-管理关系，支持交互式问答，以及机器人和自适应学习如何支持基于现场的疾病管理。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在2023-24年文献激增；视觉语言模型发表数量比大型语言模型多5-10倍；强化学习和自适应学习在智能喷洒方面仍处于起步阶段；数字孪生可虚拟模拟目标喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍有限；具有实时反馈的多模态基础模型将推动下一代SSDM。&lt;h4&gt;结论&lt;/h4&gt;基础模型特别是视觉语言模型在农业特定地点疾病管理中展现出巨大潜力，但仍需解决模拟到现实的差距和人机协作的局限性等挑战。&lt;h4&gt;翻译&lt;/h4&gt;作物特定地点疾病管理通过机器学习和深度学习在实时计算机视觉方面取得了快速进展。研究从手工特征提取发展到大规模自动化特征学习。随着基础模型的出现，作物疾病数据现在以全新的方式被处理。与传统神经网络不同，基础模型整合视觉和文本数据，解释文本中的症状，推理症状-管理关系，并为种植者和教育者支持交互式问答。机器人和自适应学习进一步支持基于现场的疾病管理。本综述筛选了约40篇关于基础模型在特定地点疾病管理中应用的论文，重点关注大型语言模型和视觉语言模型，并讨论它们在自适应学习、强化学习和用于目标喷洒的数字孪生框架中的作用。主要发现：基础模型在2023-24年文献激增中越来越受欢迎；视觉语言模型的发表数量比大型语言模型多5-10倍；强化学习和自适应学习在智能喷洒方面仍处于起步阶段；带有强化学习的数字孪生可以虚拟模拟目标喷洒；解决模拟到现实的差距对实际部署至关重要；人机协作仍然有限，特别是在人机交互方法中，机器人检测早期症状，人类验证不确定情况；具有实时反馈的多模态基础模型将推动下一代特定地点疾病管理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Site-specific disease management (SSDM) in crops has advanced rapidly throughmachine and deep learning (ML and DL) for real-time computer vision. Researchevolved from handcrafted feature extraction to large-scale automated featurelearning. With foundation models (FMs), crop disease datasets are now processedin fundamentally new ways. Unlike traditional neural networks, FMs integratevisual and textual data, interpret symptoms in text, reason aboutsymptom-management relationships, and support interactive QA for growers andeducators. Adaptive and imitation learning in robotics further enablesfield-based disease management. This review screened approx. 40 articles on FMapplications for SSDM, focusing on large-language models (LLMs) andvision-language models (VLMs), and discussing their role in adaptive learning(AL), reinforcement learning (RL), and digital twin frameworks for targetedspraying. Key findings: (a) FMs are gaining traction with surging literature in2023-24; (b) VLMs outpace LLMs, with a 5-10x increase in publications; (c) RLand AL are still nascent for smart spraying; (d) digital twins with RL cansimulate targeted spraying virtually; (e) addressing the sim-to-real gap iscritical for real-world deployment; (f) human-robot collaboration remainslimited, especially in human-in-the-loop approaches where robots detect earlysymptoms and humans validate uncertain cases; (g) multi-modal FMs withreal-time feedback will drive next-gen SSDM. For updates, resources, andcontributions, visit, https://github.com/nitin-dominic/AgriPathogenDatabase, tosubmit papers, code, or datasets.</description>
      <author>example@mail.com (Nitin Rai, Daeun, Choi, Nathan S. Boyd, Arnold W. Schumann)</author>
      <guid isPermaLink="false">2510.24650v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Generative AI for Healthcare: Fundamentals, Challenges, and Perspectives</title>
      <link>http://arxiv.org/abs/2510.24551v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种以数据为中心的医疗保健生成式人工智能系统设计范式，通过构建医疗数据生态系统作为基础支撑，实现高质量、有效的医疗保健服务。&lt;h4&gt;背景&lt;/h4&gt;生成式人工智能(GenAI)正在全球范围内迅速发展，为医疗保健领域带来变革性机会。从大型语言模型用于临床笔记合成和对话式辅助，到整合医学影像、电子健康记录和基因组数据的多模态系统用于决策支持，GenAI正在改变医学实践和医疗保健提供方式。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据中心的范式，用于医疗保健领域生成式人工智能系统的设计和部署，解决GenAI在医疗保健应用中的挑战。&lt;h4&gt;方法&lt;/h4&gt;重新定位数据生命周期，将医疗数据生态系统作为生成式医疗保健系统的基础支撑。该生态系统支持多样化医疗数据和知识的集成、表示和检索，通过语义向量搜索和上下文查询等数据处理管道，为上游模型组件和下游临床应用提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;生成式人工智能在医疗保健领域部署需要深入了解医疗保健任务以及可以实现和不能实现的目标，医疗数据生态系统是解决这一挑战的关键。&lt;h4&gt;结论&lt;/h4&gt;通过构建可持续的医疗数据生态系统，不仅能为基础模型提供高质量、多模态数据用于大规模预训练和领域特定微调，还能作为知识检索后端支持特定任务推理，使GenAI能够高质量、有效地部署医疗保健服务。&lt;h4&gt;翻译&lt;/h4&gt;生成式人工智能(GenAI)正在席卷全球。它承诺为推进和颠覆现有实践（包括医疗保健）带来变革性机会。从用于临床笔记合成和对话式辅助的大型语言模型(LLMs)，到整合医学影像、电子健康记录和基因组数据用于决策支持的多模态系统，GenAI正在改变医学实践和医疗保健的提供方式，如诊断和个性化治疗，有潜力减轻临床医生的认知负担，从而改善整体医疗保健服务。然而，GenAI在医疗保健领域的部署需要对医疗保健任务以及可实现和不可实现的目标有深入理解。在本文中，我们提出了一个以数据为中心的范式，用于医疗保健领域生成式人工智能系统的设计和部署。具体而言，我们通过将医疗数据生态系统作为生成式医疗保健系统的基础支撑物，重新定位了数据生命周期。该生态系统旨在可持续地支持多样化医疗数据和知识的集成、表示和检索。通过有效和高效的数据处理管道，如语义向量搜索和上下文查询，它使生成式人工智能能够为上游模型组件和下游临床应用提供支持。最终，它不仅为基础模型提供高质量、多模态数据用于大规模预训练和领域特定微调，还充当知识检索后端，通过智能层支持特定任务的推理。该生态系统使生成式人工智能能够高质量、有效地部署医疗保健服务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Artificial Intelligence (GenAI) is taking the world by storm. Itpromises transformative opportunities for advancing and disrupting existingpractices, including healthcare. From large language models (LLMs) for clinicalnote synthesis and conversational assistance to multimodal systems thatintegrate medical imaging, electronic health records, and genomic data fordecision support, GenAI is transforming the practice of medicine and thedelivery of healthcare, such as diagnosis and personalized treatments, withgreat potential in reducing the cognitive burden on clinicians, therebyimproving overall healthcare delivery. However, GenAI deployment in healthcarerequires an in-depth understanding of healthcare tasks and what can and cannotbe achieved. In this paper, we propose a data-centric paradigm in the designand deployment of GenAI systems for healthcare. Specifically, we reposition thedata life cycle by making the medical data ecosystem as the foundationalsubstrate for generative healthcare systems. This ecosystem is designed tosustainably support the integration, representation, and retrieval of diversemedical data and knowledge. With effective and efficient data processingpipelines, such as semantic vector search and contextual querying, it enablesGenAI-powered operations for upstream model components and downstream clinicalapplications. Ultimately, it not only supplies foundation models withhigh-quality, multimodal data for large-scale pretraining and domain-specificfine-tuning, but also serves as a knowledge retrieval backend to supporttask-specific inference via the agentic layer. The ecosystem enables thedeployment of GenAI for high-quality and effective healthcare delivery.</description>
      <author>example@mail.com (Gang Chen, Changshuo Liu, Gene Anne Ooi, Marcus Tan, Zhongle Xie, Jianwei Yin, James Wei Luen Yip, Wenqiao Zhang, Jiaqi Zhu, Beng Chin Ooi)</author>
      <guid isPermaLink="false">2510.24551v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Affordance Representation and Recognition for Autonomous Agents</title>
      <link>http://arxiv.org/abs/2510.24459v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种从结构化数据构建世界模型的新方法，解决了软件代理在适应不断变化的Web环境时面临的两个关键挑战。&lt;h4&gt;背景&lt;/h4&gt;软件代理的自主性依赖于其从结构化数据（如网页DOM和Web服务语义描述）构建内部世界模型的能力。然而，原始HTML的冗长性和硬编码API集成的静态性构成了重大障碍。&lt;h4&gt;目的&lt;/h4&gt;开发一种模式语言，使软件代理能够高效构建和维护准确的世界模型，从而实现跨Web及其扩展资源的可扩展、自适应和互操作性自动化。&lt;h4&gt;方法&lt;/h4&gt;提出两种互补的架构模式：1) DOM转换模式，将冗长的原始DOM提炼为紧凑的、任务相关的表示；2) 超媒体功能识别模式，通过解析语义描述动态发现和集成未知Web服务的能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过结合这两种模式，软件代理能够克服数据冗余和服务动态变化的挑战，构建和维护准确的世界模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的模式语言为构建能够高效适应不断演化的数字环境的智能代理提供了强大框架，增强了自动化系统的可扩展性、适应性和互操作性。&lt;h4&gt;翻译&lt;/h4&gt;软件代理的自主性根本上取决于其从定义其数字环境的结构化数据（如网页的文档对象模型和Web服务的语义描述）构建可行的内部世界模型的能力。然而，从原始结构化数据构建此世界模型存在两个关键挑战：原始HTML的冗长性使其在计算上难以被基础模型直接使用，而硬编码API集成的静态性质阻止了代理适应不断发展的服务。本文介绍了一种从结构化数据进行世界建模的模式语言，提出了两种互补的架构模式。DOM转换模式通过将冗长的原始DOM提炼为紧凑的、任务相关的表示或为代理推理核心优化的世界模型，解决了网页复杂性的挑战。同时，超媒体功能识别模式使代理能够通过解析标准化的语义描述来动态丰富其世界模型，从而在运行时发现和集成未知Web服务的能力。这些模式共同提供了一个强大的框架，用于构建能够高效构建和维护准确世界模型的代理，从而实现Web及其扩展资源的可扩展、自适应和互操作性自动化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The autonomy of software agents is fundamentally dependent on their abilityto construct an actionable internal world model from the structured data thatdefines their digital environment, such as the Document Object Model (DOM) ofweb pages and the semantic descriptions of web services. However, constructingthis world model from raw structured data presents two critical challenges: theverbosity of raw HTML makes it computationally intractable for direct use byfoundation models, while the static nature of hardcoded API integrationsprevents agents from adapting to evolving services.  This paper introduces a pattern language for world modeling from structureddata, presenting two complementary architectural patterns. The DOM TransductionPattern addresses the challenge of web page complexity by distilling} averbose, raw DOM into a compact, task-relevant representation or world modeloptimized for an agent's reasoning core. Concurrently, the HypermediaAffordances Recognition Pattern enables the agent to dynamically enrich itsworld model by parsing standardized semantic descriptions to discover andintegrate the capabilities of unknown web services at runtime. Together, thesepatterns provide a robust framework for engineering agents that can efficientlyconstruct and maintain an accurate world model, enabling scalable, adaptive,and interoperable automation across the web and its extended resources.</description>
      <author>example@mail.com (Habtom Kahsay Gidey, Niklas Huber, Alexander Lenz, Alois Knoll)</author>
      <guid isPermaLink="false">2510.24459v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Visual Intelligence: Insights from Video Pretraining</title>
      <link>http://arxiv.org/abs/2510.24448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Updated version from preprint arXiv:2506.07280 (Gen2Gen) focused on  visual intelligence. This work can be considered as v2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究表明视频扩散模型在视觉任务上比语言模型更高效，视频预训练提供的归纳偏置有助于构建视觉基础模型。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在大规模预训练后能够在语言领域快速适应新问题，但这种成功在视觉领域并未同样有效，包括LLMs在内的模型在组合理解、样本效率和通用问题解决方面仍然存在困难。&lt;h4&gt;目的&lt;/h4&gt;研究视频扩散模型作为弥合语言模型与视觉模型差距的有前途方向，测试视频预训练是否能提供支持广泛任务适应性的归纳偏置。&lt;h4&gt;方法&lt;/h4&gt;在时空数据上预训练视频扩散模型，使其具有结构和动态性的强归纳偏置；设计对照评估，让预训练的LLM和VDM都配备轻量级适配器，并以自然方式呈现任务；在多个基准测试中评估，包括ARC-AGI、ConceptARC、视觉游戏、路线规划和细胞自动机。&lt;h4&gt;主要发现&lt;/h4&gt;VDMs比语言对应模型具有更高的数据效率；视频预训练提供的归纳偏置支持视觉基础模型的进展。&lt;h4&gt;结论&lt;/h4&gt;视频预训练提供了归纳偏置，支持向视觉基础模型进展。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型已经证明，大规模预训练使系统能够在语言领域以少量监督快速适应新问题。然而，这种成功并没有同样有效地转化为视觉领域，包括LLMs在内的模型在组合理解、样本效率和通用问题解决方面仍然存在困难。我们研究视频扩散模型作为弥合这一差距的有前途方向。在时空数据上的预训练赋予这些模型结构和动态性的强归纳偏置，我们假设这可以支持广泛的任务适应性。为了验证这一点，我们设计了一个对照评估，让预训练的LLM和预训练的VDM都配备轻量级适配器，并以它们自然的方式呈现任务。在包括ARC-AGI、ConceptARC、视觉游戏、路线规划和细胞自动机在内的基准测试中，VDMs比其语言对应模型表现出更高的数据效率。综上所述，我们的结果表明视频预训练提供了归纳偏置，支持向视觉基础模型的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated that large-scale pretrainingenables systems to adapt rapidly to new problems with little supervision in thelanguage domain. This success, however, has not translated as effectively tothe visual domain, where models, including LLMs, continue to struggle withcompositional understanding, sample efficiency, and general-purposeproblem-solving. We investigate Video Diffusion Models (VDMs) as a promisingdirection for bridging this gap. Pretraining on spatiotemporal data endowsthese models with strong inductive biases for structure and dynamics, which wehypothesize can support broad task adaptability. To test this, we design acontrolled evaluation in which both a pretrained LLM and a pretrained VDM areequipped with lightweight adapters and presented with tasks in their naturalmodalities. Across benchmarks including ARC-AGI, ConceptARC, visual games,route planning, and cellular automata, VDMs demonstrate higher data efficiencythan their language counterparts. Taken together, our results indicate thatvideo pretraining offers inductive biases that support progress toward visualfoundation models.</description>
      <author>example@mail.com (Pablo Acuaviva, Aram Davtyan, Mariam Hassan, Sebastian Stapf, Ahmad Rahimi, Alexandre Alahi, Paolo Favaro)</author>
      <guid isPermaLink="false">2510.24448v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Geometric Space Bridging AI Models and the Human Brain</title>
      <link>http://arxiv.org/abs/2510.24342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了'类脑空间'概念，通过映射AI模型的内在空间注意力拓扑结构到人类功能性脑网络，实现了跨模态AI模型的统一比较框架，揭示了机器与大脑之间的深层组织原则。&lt;h4&gt;背景&lt;/h4&gt;几十年来，神经科学家和计算机科学家一直致力于理解并构建智能。现代人工神经网络在语言、感知和推理方面已可与人类媲美，但这些系统是否像大脑一样组织信息仍然在很大程度上未知。现有的大脑-AI对齐研究虽展示了两个系统间的对应关系，但比较局限于特定输入和任务，缺乏跨模态AI模型内在组织的共同比较基础。&lt;h4&gt;目的&lt;/h4&gt;引入'类脑空间'概念，这是一个统一的几何空间，无论输入模态、任务或感觉域如何，都能通过将AI模型内在的空间注意力拓扑结构映射到典型人类功能性脑网络上，实现对AI模型的精确定位和比较。&lt;h4&gt;方法&lt;/h4&gt;对151个基于Transformer的模型进行广泛分析，涵盖最先进的大型视觉模型、大型语言模型和大型多模态模型。&lt;h4&gt;主要发现&lt;/h4&gt;在类脑空间中存在连续的弧形几何结构，反映类脑性的逐渐增加；不同模型表现出不同的分布模式，与不同程度的类脑性相关，这种模式不仅受模态影响，还受预训练范式是否强调全局语义抽象以及位置编码方案是否促进不同模态间深度融合的影响；此外，模型的类脑程度和其下游任务表现并非完全相同。&lt;h4&gt;结论&lt;/h4&gt;类脑空间提供了第一个跨领域的定位、量化和比较智能的统一框架，揭示了连接机器和大脑的深层组织原则。&lt;h4&gt;翻译&lt;/h4&gt;几十年来，神经科学家和计算机科学家一直怀有共同的志向：理解智能并构建它。现代人工神经网络在语言、感知和推理方面现在可以与人类相媲美，但这些人工系统是否像大脑一样组织信息仍然在很大程度上未知。现有的大脑-AI对齐研究已经展示了两个系统之间的惊人对应关系，但这样的比较仍然局限于特定的输入和任务，没有提供共同的基础来比较不同模态（视觉、语言或多模态）的AI模型是如何内在组织的。在这里，我们引入了一个突破性的概念：类脑空间：一个统一的几何空间，通过将AI模型内在的空间注意力拓扑组织映射到典型的人类功能性脑网络上，无论输入模态、任务或感觉域如何，每个AI模型都可以在这个空间中被精确定位和比较。我们对151个基于Transformer的模型进行了广泛分析，这些模型涵盖了最先进的大型视觉模型、大型语言模型和大型多模态模型，在这个空间中发现了一个连续的弧形几何结构，反映了类脑性的逐渐增加；不同的模型在这个几何结构中表现出不同的分布模式，与不同程度的类脑性相关，这种模式不仅受模态影响，还受预训练范式是否强调全局语义抽象以及位置编码方案是否促进不同模态间的深度融合的影响。此外，模型的类脑程度和其下游任务表现并非'完全相同'。类脑空间提供了第一个跨领域的定位、量化和比较智能的统一框架，揭示了连接机器和大脑的深层组织原则。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For decades, neuroscientists and computer scientists have pursued a sharedambition: to understand intelligence and build it. Modern artificial neuralnetworks now rival humans in language, perception, and reasoning, yet it isstill largely unknown whether these artificial systems organize information asthe brain does. Existing brain-AI alignment studies have shown the strikingcorrespondence between the two systems, but such comparisons remain bound tospecific inputs and tasks, offering no common ground for comparing how AImodels with different kinds of modalities-vision, language, or multimodal-areintrinsically organized. Here we introduce a groundbreaking concept ofBrain-like Space: a unified geometric space in which every AI model can beprecisely situated and compared by mapping its intrinsic spatial attentiontopological organization onto canonical human functional brain networks,regardless of input modality, task, or sensory domain. Our extensive analysisof 151 Transformer-based models spanning state-of-the-art large vision models,large language models, and large multimodal models uncovers a continuousarc-shaped geometry within this space, reflecting a gradual increase ofbrain-likeness; different models exhibit distinct distribution patterns withinthis geometry associated with different degrees of brain-likeness, shaped notmerely by their modality but by whether the pretraining paradigm emphasizesglobal semantic abstraction and whether the positional encoding schemefacilitates deep fusion across different modalities. Moreover, the degree ofbrain-likeness for a model and its downstream task performance are not"identical twins". The Brain-like Space provides the first unified frameworkfor situating, quantifying, and comparing intelligence across domains,revealing the deep organizational principles that bridge machines and thebrain.</description>
      <author>example@mail.com (Silin Chen, Yuzhong Chen, Zifan Wang, Junhao Wang, Zifeng Jia, Keith M Kendrick, Tuo Zhang, Lin Zhao, Dezhong Yao, Tianming Liu, Xi Jiang)</author>
      <guid isPermaLink="false">2510.24342v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Vanish into Thin Air: Cross-prompt Universal Adversarial Attacks for SAM2</title>
      <link>http://arxiv.org/abs/2510.24195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了UAP-SAM2，第一个针对SAM2的跨提示通用对抗攻击，通过双语义偏差驱动，有效解决了SAM2架构差异带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;图像分割基础模型SAM对对抗样本存在脆弱性，其后续模型SAM2在视频分割方面表现出强大的泛化能力，但其鲁棒性尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;分析现有攻击在SAM和SAM2之间的性能差距，并提出一种有效的对抗攻击方法针对SAM2。&lt;h4&gt;方法&lt;/h4&gt;提出UAP-SAM2方法，包括：1) 设计目标扫描策略，将每帧划分为k个区域，每个区域随机分配提示，减少优化过程中的提示依赖性；2) 设计双语义偏差框架，通过扭曲当前帧内语义和破坏连续帧间语义一致性来优化UAP。&lt;h4&gt;主要发现&lt;/h4&gt;现有攻击在SAM和SAM2之间存在性能差距，主要由于SAM2架构差异带来的两个关键挑战：来自提示的方向性指导和连续帧之间的语义纠缠。&lt;h4&gt;结论&lt;/h4&gt;UAP-SAM2在两个分割任务上的六个数据集实验中表现出有效性，以较大优势显著优于最先进的攻击方法。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究揭示了图像分割基础模型SAM对对抗样本的脆弱性。其后续模型SAM2因其强大的视频分割泛化能力而受到广泛关注。然而，其鲁棒性尚未被探索，目前还不清楚现有的针对SAM的攻击是否可以直接转移到SAM2上。在本文中，我们首先分析了现有攻击在SAM和SAM2之间的性能差距，并指出了它们架构差异带来的两个关键挑战：来自提示的方向性指导和连续帧之间的语义纠缠。为解决这些问题，我们提出了UAP-SAM2，这是第一个由双语义偏差驱动的针对SAM2的跨提示通用对抗攻击。为实现跨提示可转移性，我们首先设计了一个目标扫描策略，将每帧划分为k个区域，每个区域随机分配一个提示，以减少优化过程中的提示依赖性。为提高有效性，我们设计了一个双语义偏差框架，通过扭曲当前帧内的语义和破坏连续帧之间的语义一致性来优化UAP。在两个分割任务上的六个数据集进行的广泛实验证明了所提方法对SAM2的有效性。比较结果显示，UAP-SAM2以较大优势显著优于最先进的(SOTA)攻击。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies reveal the vulnerability of the image segmentation foundationmodel SAM to adversarial examples. Its successor, SAM2, has attractedsignificant attention due to its strong generalization capability in videosegmentation. However, its robustness remains unexplored, and it is unclearwhether existing attacks on SAM can be directly transferred to SAM2. In thispaper, we first analyze the performance gap of existing attacks between SAM andSAM2 and highlight two key challenges arising from their architecturaldifferences: directional guidance from the prompt and semantic entanglementacross consecutive frames. To address these issues, we propose UAP-SAM2, thefirst cross-prompt universal adversarial attack against SAM2 driven by dualsemantic deviation. For cross-prompt transferability, we begin by designing atarget-scanning strategy that divides each frame into k regions, each randomlyassigned a prompt, to reduce prompt dependency during optimization. Foreffectiveness, we design a dual semantic deviation framework that optimizes aUAP by distorting the semantics within the current frame and disrupting thesemantic consistency across consecutive frames. Extensive experiments on sixdatasets across two segmentation tasks demonstrate the effectiveness of theproposed method for SAM2. The comparative results show that UAP-SAM2significantly outperforms state-of-the-art (SOTA) attacks by a large margin.</description>
      <author>example@mail.com (Ziqi Zhou, Yifan Hu, Yufei Song, Zijing Li, Shengshan Hu, Leo Yu Zhang, Dezhong Yao, Long Zheng, Hai Jin)</author>
      <guid isPermaLink="false">2510.24195v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Blindfolded Experts Generalize Better: Insights from Robotic Manipulation and Videogames</title>
      <link>http://arxiv.org/abs/2510.24194v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;行为克隆是一种从演示中学习序列决策的简单而有效技术，本文提出让演示者在部分信息不足的情况下进行演示，发现这种'蒙眼'专家的克隆方法在泛化到未见任务时表现更好。&lt;h4&gt;背景&lt;/h4&gt;行为克隆已成为物理世界基础模型的核心，实现泛化需要无数任务的演示。通常，具有完整任务信息的人类专家会演示最优行为。&lt;h4&gt;目的&lt;/h4&gt;研究向演示者隐藏部分任务信息是否能提高行为克隆的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出'蒙眼专家'方法，即向任务演示者隐藏部分信息，迫使其使用非平凡的探索策略来解决任务。在真实世界机器人插销任务和Procgen基准视频游戏上进行实验，并进行理论分析。&lt;h4&gt;主要发现&lt;/h4&gt;克隆'蒙眼专家'的行为比克隆完全信息专家的行为在未见任务上泛化能力更好。理论分析表明泛化误差与演示者可用的任务信息量和演示任务数量有关，使用更少演示任务时克隆蒙眼专家能实现更好泛化。&lt;h4&gt;结论&lt;/h4&gt;理论和实践均表明，使用更少的演示任务，克隆蒙眼专家能够实现更好的泛化效果。&lt;h4&gt;翻译&lt;/h4&gt;行为克隆是一种简单而有效的技术，用于从演示中学习序列决策。最近，它已成为物理世界基础模型的核心，其中实现泛化需要无数任务的演示。通常，具有任务完整信息的人类专家会演示（几乎）最优的行为。在本文中，我们提出向演示者隐藏任务的部分信息。这种'蒙眼'专家被迫采用非平凡的探索来解决任务。我们证明，克隆蒙眼专家比完全信息的专家在未见任务上泛化能力更好。我们在有限人类演示下的真实世界机器人插销任务以及Procgen基准的视频游戏上进行了实验。此外，我们通过理论分析支持了这一发现，理论和实践都表明，用更少的演示任务克隆蒙眼专家能更好地泛化。项目页面包含视频和代码：https://sites.google.com/view/blindfoldedexperts/home&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavioral cloning is a simple yet effective technique for learningsequential decision-making from demonstrations. Recently, it has gainedprominence as the core of foundation models for the physical world, whereachieving generalization requires countless demonstrations of a multitude oftasks. Typically, a human expert with full information on the task demonstratesa (nearly) optimal behavior. In this paper, we propose to hide some of thetask's information from the demonstrator. This ``blindfolded'' expert iscompelled to employ non-trivial exploration to solve the task. We show thatcloning the blindfolded expert generalizes better to unseen tasks than itsfully-informed counterpart. We conduct experiments of real-world robot peginsertion tasks with (limited) human demonstrations, alongside videogames fromthe Procgen benchmark. Additionally, we support our findings with theoreticalanalysis, which confirms that the generalization error scales with$\sqrt{I/m}$, where $I$ measures the amount of task information available tothe demonstrator, and $m$ is the number of demonstrated tasks. Both theory andpractice indicate that cloning blindfolded experts generalizes better withfewer demonstrated tasks. Project page with videos and code:https://sites.google.com/view/blindfoldedexperts/home</description>
      <author>example@mail.com (Ev Zisselman, Mirco Mutti, Shelly Francis-Meretzki, Elisei Shafer, Aviv Tamar)</author>
      <guid isPermaLink="false">2510.24194v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>BLM$_1$: A Boundless Large Model for Cross-Space, Cross-Task, and Cross-Embodiment Learning</title>
      <link>http://arxiv.org/abs/2510.24161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Boundless Large Model (BLM₁)，一个多模态空间基础模型，能够在数字和物理空间无缝操作，实现跨具身和任务泛化。通过两阶段训练范式，BLM₁整合了跨空间迁移、跨任务学习和跨具身泛化能力，在数字和物理基准测试中超越了多种模型家族。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)在视觉-语言推理方面取得进展并应用于具身智能体，但仍存在显著局限性：MLLMs在数字-物理空间和具身形式间泛化能力差；视觉-语言-动作模型(VLAs)产生低级行动但缺乏稳健的高层次具身推理；大多数具身大语言模型(ELLMs)局限于数字空间，对物理世界泛化能力差。缺乏能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型。&lt;h4&gt;方法&lt;/h4&gt;提出Boundless Large Model (BLM₁)，一个多模态空间基础模型，保留指令跟随和推理能力，整合具身知识，支持稳健的跨具身控制。通过两阶段训练范式整合三种关键能力：跨空间迁移、跨任务学习和跨具身泛化。第一阶段通过精心挑选的数字语料库将具身知识注入MLLM，同时保持语言能力；第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高层语义来指导控制，无需微调MLLM主干。使用自收集的跨具身演示套件，涵盖四种机器人具身和六种渐进式挑战任务。&lt;h4&gt;主要发现&lt;/h4&gt;在数字和物理基准测试中评估显示，单个BLM₁实例优于四种模型家族（MLLMs、ELLMs、VLAs和GMLMs），在数字任务中实现约6%的提升，在物理任务中实现约3%的提升。&lt;h4&gt;结论&lt;/h4&gt;BLM₁是一个有效的多模态空间基础模型，能够整合具身知识并实现跨空间、跨任务和跨具身的泛化能力，为具身智能体提供了新的发展方向。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)已推进视觉-语言推理，并越来越多地部署在具身智能体中。然而，仍存在显著局限性：MLLMs在数字-物理空间和具身形式间泛化能力差；视觉-语言-动作模型(VLAs)产生低级行动但缺乏稳健的高层次具身推理；大多数具身大语言模型(ELLMs)局限于数字空间，对物理世界泛化能力差。因此，能够在数字和物理空间无缝操作、跨具身和任务泛化的统一模型仍然缺失。我们引入了Boundless Large Model (BLM₁)，一个多模态空间基础模型，保留了指令跟随和推理能力，整合了具身知识，并支持稳健的跨具身控制。BLM₁通过两阶段训练范式整合了三种关键能力——跨空间迁移、跨任务学习和跨具身泛化。第一阶段通过精心挑选的数字语料库将具身知识注入MLLM，同时保持语言能力。第二阶段通过意图桥接接口训练策略模块，从MLLM中提取高层语义来指导控制，无需微调MLLM主干。这一过程得到了一个自收集的跨具身演示套件的支持，涵盖四种机器人具身和六种渐进式挑战任务。在数字和物理基准测试中的评估显示，单个BLM₁实例优于四种模型家族——MLLMs、ELLMs、VLAs和GMLMs，在数字任务中实现约6%的提升，在物理任务中实现约3%的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) have advanced vision-languagereasoning and are increasingly deployed in embodied agents. However,significant limitations remain: MLLMs generalize poorly across digital-physicalspaces and embodiments; vision-language-action models (VLAs) produce low-levelactions yet lack robust high-level embodied reasoning; and most embodied largelanguage models (ELLMs) are constrained to digital-space with poorgeneralization to the physical world. Thus, unified models that operateseamlessly across digital and physical spaces while generalizing acrossembodiments and tasks remain absent. We introduce the \textbf{Boundless LargeModel (BLM$_1$)}, a multimodal spatial foundation model that preservesinstruction following and reasoning, incorporates embodied knowledge, andsupports robust cross-embodiment control. BLM$_1$ integrates three keycapabilities -- \textit{cross-space transfer, cross-task learning, andcross-embodiment generalization} -- via a two-stage training paradigm. Stage Iinjects embodied knowledge into the MLLM through curated digital corpora whilemaintaining language competence. Stage II trains a policy module through anintent-bridging interface that extracts high-level semantics from the MLLM toguide control, without fine-tuning the MLLM backbone. This process is supportedby a self-collected cross-embodiment demonstration suite spanning four robotembodiments and six progressively challenging tasks. Evaluations across digitaland physical benchmarks show that a single BLM$_1$ instance outperforms fourmodel families -- MLLMs, ELLMs, VLAs, and GMLMs -- achieving$\sim\!\textbf{6%}$ gains in digital tasks and $\sim\!\textbf{3%}$ in physicaltasks.</description>
      <author>example@mail.com (Wentao Tan, Bowen Wang, Heng Zhi, Chenyu Liu, Zhe Li, Jian Liu, Zengrong Lin, Yukun Dai, Yipeng Chen, Wenjie Yang, Enci Xie, Hao Xue, Baixu Ji, Chen Xu, Zhibin Wang, Tianshi Wang, Lei Zhu, Heng Tao Shen)</author>
      <guid isPermaLink="false">2510.24161v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Global Chlorophyll-\textit{a} Retrieval algorithm from Sentinel 2 Using Residual Deep Learning and Novel Machine Learning Water Classification</title>
      <link>http://arxiv.org/abs/2510.24124v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为全球水分类器(GWC)的监督式机器学习分类器，用于全球范围内水体识别和叶绿素-a浓度反演，并通过残差CNN校正提高了反演精度。&lt;h4&gt;背景&lt;/h4&gt;传统的水体识别和叶绿素-a浓度反演面临多种干扰因素，如云、太阳耀斑、雪、冰、水生植被、陆地和沉积物等，需要一种能够全球应用且稳健的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够全球范围内准确识别水体并反演叶绿素-a浓度的方法，克服地理和气候条件的限制。&lt;h4&gt;方法&lt;/h4&gt;1. 使用Sen2Cor校正的Sentinel-2地表反射率数据训练全球水分类器(GWC)2. 基于近100个全球分布的内陆水体样本进行训练3. 使用XGBoost回归器进行叶绿素-a浓度反演4. 添加残差CNN(RCNN)校正阶段提高反演精度5. 在867个水体上进行测试验证&lt;h4&gt;主要发现&lt;/h4&gt;1. GWC能够有效区分不同叶绿素-a水平的水体与非水体光谱2. GWC表现出地理位置稳定的性能3. GWC正标记的场景产生的叶绿素-a反演值更准确4. 残差CNN校正阶段显著提高了反演精度5. 最终算法在测试中表现出稳健性、可扩展性和全球可转移性&lt;h4&gt;结论&lt;/h4&gt;全球水分类器结合残差CNN校正的方法能够准确、稳健地进行全球水体识别和叶绿素-a浓度反演，无需针对不同地区进行额外调优，具有很高的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了全球水分类器(GWC)，一种监督式、地理范围广泛的机器学习分类器，基于Sen2Cor校正的Sentinel-2地表反射率数据训练。使用近100个全球分布的内陆水体，GWC能够区分不同叶绿素-a水平的水体光谱与非水体光谱(云、太阳耀斑、雪、冰、水生植被、陆地和沉积物)，并表现出地理位置稳定的性能。在此基础模型上，我们使用匹配的Sentinel-2反射率数据与美国地质调查局(USGS)AquaMatch现场数据集进行叶绿素-a反演，覆盖了多样的地理和水文条件。我们在13626个匹配点上训练了一个XGBoost回归器。GWC正标记的场景持续优于负标记场景，并产生更准确的叶绿素-a反演值，这证实了分类器在减少各种干扰方面的优势。接下来，回归预测的残差分析揭示了结构化误差，促使我们添加了残差CNN(RCNN)校正阶段。我们添加了一个基于归一化残差训练的CNN残差阶段，取得了显著改进。我们的算法在867个水体上进行了测试，超过2000个预测，叶绿素-a值高达1000毫克每立方米，实现了R² = 0.79，平均绝对误差 = 13.52毫克每立方米，斜率 = 0.91，证明了其稳健、可扩展且全球可转移的性能，无需额外调优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present the Global Water Classifier (GWC), a supervised, geospatiallyextensive Machine Learning (ML) classifier trained on Sen2Cor correctedSentinel-2 surface reflectance data. Using nearly 100 globally distributedinland water bodies, GWC distinguishes water across Chlorophyll-a (Chla) levelsfrom non-water spectra (clouds, sun glint, snow, ice, aquatic vegetation, landand sediments) and shows geographically stable performance.  Building on this foundation model, we perform Chla retrieval based on amatchup Sentinel-2 reflectance data with the United States Geological Survey(USGS) AquaMatch in-situ dataset, covering diverse geographical andhydrological conditions.  We train an XGBoost regressor on 13626 matchup points. The positive labeledscenes by the GWC consistently outperform the negatives and produce moreaccurate Chla retrieval values, which confirms the classifiers advantage inreducing various interferences.  Next, residual analysis of the regression predictions revealed structurederrors, motivating a residual CNN (RCNN) correction stage. We add a CNNresidual stage trained on normalized residuals, which yield substantialimprovement. Our algorithm was tested on 867 water bodies with over 2,000predictions and Chla values up to 1000~mg$/m^{3}$, achieving $R^2$ = 0.79, MAE= 13.52~mg$/m^{3}$, and slope = 0.91, demonstrating robust, scalable, andglobally transferable performance without additional tuning.</description>
      <author>example@mail.com (Yotam Sherf, Bar Efrati, Gabriel Rozman, Moshe Harel)</author>
      <guid isPermaLink="false">2510.24124v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>OmniLearned: A Foundation Model Framework for All Tasks Involving Jet Physics</title>
      <link>http://arxiv.org/abs/2510.24066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了OmniLearn基础模型的重大升级，形成了OmniLearned框架。该框架包含模型架构和训练的更新、使用超过十亿个喷注进行训练，以及提供完善的软件访问工具。通过三个代表性任务展示了该框架的性能，结果表明它在所有任务中都是最先进的，显著增强了粒子物理实验的发现潜力。&lt;h4&gt;背景&lt;/h4&gt;基础模型利用大型数据集构建有效的数据表示，可应用于多样化的下游任务。之前开发的OmniLearn基础模型针对粒子物理喷注，利用了粒子物理的独特性质，能够显著增强对撞机实验的发现潜力。&lt;h4&gt;目的&lt;/h4&gt;对现有的OmniLearn基础模型进行重大升级，创建OmniLearned框架，进一步提升其在粒子物理实验中的性能和可用性，扩展过去、当前和未来对撞机实验的发现潜力。&lt;h4&gt;方法&lt;/h4&gt;开发OmniLearned框架，包含三个新元素：更新模型架构和训练方法、使用超过十亿个喷注进行训练、提供完善的软件用于访问所有数据集和模型。通过三个代表性任务进行验证：top夸克喷注标记、b标记和异常检测。&lt;h4&gt;主要发现&lt;/h4&gt;在三个代表性任务（top夸克喷注标记、b标记和异常检测）中，OmniLearned均达到了最先进的性能水平。该框架能够显著增强对撞机实验的发现潜力，包括过去、当前和未来的实验。&lt;h4&gt;结论&lt;/h4&gt;OmniLearned框架代表了基础模型在粒子物理领域的重要进展，通过架构更新、大规模训练和完善的软件工具，显著提升了模型性能，为粒子物理研究提供了更强大的分析工具。&lt;h4&gt;翻译&lt;/h4&gt;基础模型使用大型数据集构建有效的数据表示，可部署在多样化的下游任务中。先前的研究开发了用于粒子物理喷注的OmniLearn基础模型，利用了粒子物理的独特性质，并表明它可以显著增强对撞机实验的发现潜力。本文介绍了一个重大升级，结果是OmniLearned框架。该框架有三个新元素：(1)对模型架构和训练的更新，(2)使用超过十亿个喷注进行训练，(3)提供完善的软件用于访问所有数据集和模型。我们通过三个代表性任务展示了OmniLearned：使用社区Delphes基准数据集进行top夸克喷注标记，使用ATLAS全模拟进行b标记，以及使用CMS实验数据进行异常检测。在每种情况下，OmniLearned都是最先进的，进一步扩展了过去、当前和未来对撞机实验的发现潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models use large datasets to build an effective representation ofdata that can be deployed on diverse downstream tasks. Previous researchdeveloped the OmniLearn foundation model for jet physics, using uniqueproperties of particle physics, and showed that it could significantly advancediscovery potential across collider experiments. This paper introduces a majorupgrade, resulting in the OmniLearned framework. This framework has three newelements: (1) updates to the model architecture and training, (2) using overone billion jets used for training, and (3) providing well-documented softwarefor accessing all datasets and models. We demonstrate OmniLearned with threerepresentative tasks: top-quark jet tagging with the community Delphes-basedbenchmark dataset, b-tagging with ATLAS full simulation, and anomaly detectionwith CMS experimental data. In each case, OmniLearned is the state of the art,further expanding the discovery potential of past, current, and future colliderexperiments.</description>
      <author>example@mail.com (Wahid Bhimji, Chris Harris, Vinicius Mikuni, Benjamin Nachman)</author>
      <guid isPermaLink="false">2510.24066v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Human Machine Social Hybrid Intelligence:A Collaborative Decision Making Framework for Large Model Agent Groups and Human Experts</title>
      <link>http://arxiv.org/abs/2510.24030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新型的人机社会混合智能(HMS-HI)框架，通过共享认知空间、动态角色任务分配和跨物种信任校准三个核心组件，实现了人类专家和AI代理之间的深度协作决策，在应急响应模拟中显著降低了伤亡和认知负荷。&lt;h4&gt;背景&lt;/h4&gt;大型基础模型和多智能体系统快速发展提供了前所未有的能力，但当前人机协同(HiTL)范式未能充分整合人类专业知识，在复杂、高风险环境中常导致认知过载和决策瓶颈。&lt;h4&gt;目的&lt;/h4&gt;设计一种新型架构，用于人类专家群体和基于大语言模型的AI代理之间的深度协作决策，解决现有人机协同方法的不足。&lt;h4&gt;方法&lt;/h4&gt;HMS-HI建立在三个核心支柱上：(1)共享认知空间(SCS)用于统一的多模态态势感知和结构化世界建模；(2)动态角色和任务分配(DRTA)模块根据能力和工作负载将任务自适应分配给最适合的代理；(3)跨物种信任校准(CSTC)协议通过可解释声明和结构化反馈促进透明度、责任和相互适应。&lt;h4&gt;主要发现&lt;/h4&gt;在高保真城市应急响应模拟中，HMS-HI相比传统HiTL方法将平民伤亡减少72%，认知负荷减少70%，证明了卓越的决策质量、效率和人类-AI信任。消融研究确认了每个模块的关键贡献，表明工程化的信任和共享背景是可扩展的人机协作基础。&lt;h4&gt;结论&lt;/h4&gt;HMS-HI框架通过三个核心组件的整合，在复杂、高风险环境中实现了更有效的人机协作决策，显著提高了决策质量和效率，同时减轻了人类认知负担。&lt;h4&gt;翻译&lt;/h4&gt;大型基础模型和多智能体系统的快速发展提供了前所未有的能力，但当前人机协同(HiTL)范式未能充分整合人类专业知识，在复杂、高风险环境中常常导致认知过载和决策瓶颈。我们提出了'人机社会混合智能'(HMS-HI)框架，这是一种专为人类专家群体和由大语言模型驱动的AI代理之间的深度协作决策而设计的新型架构。HMS-HI建立在三个核心支柱上：(1)共享认知空间(SCS)用于统一、多模态的态势感知和结构化世界建模；(2)动态角色和任务分配(DRTA)模块，根据能力和工作负载将任务自适应地分配给最适合的代理(人类或AI)；(3)跨物种信任校准(CSTC)协议，通过可解释声明和结构化反馈促进透明度、责任和相互适应。在高保真的城市应急响应模拟中验证，HMS-HI与传统HiTL方法相比，平民伤亡减少了72%，认知负荷减少了70%，证明了卓越的决策质量、效率和人类-AI信任。消融研究确认了每个模块的关键贡献，强调工程化的信任和共享背景是可扩展的、协同的人机协作的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid advancements in large foundation models and multi-agent systemsoffer unprecedented capabilities, yet current Human-in-the-Loop (HiTL)paradigms inadequately integrate human expertise, often leading to cognitiveoverload and decision-making bottlenecks in complex, high-stakes environments.We propose the "Human-Machine Social Hybrid Intelligence" (HMS-HI) framework, anovel architecture designed for deep, collaborative decision-making betweengroups of human experts and LLM-powered AI agents. HMS-HI is built upon threecore pillars: (1) a \textbf{Shared Cognitive Space (SCS)} for unified,multi-modal situational awareness and structured world modeling; (2) a\textbf{Dynamic Role and Task Allocation (DRTA)} module that adaptively assignstasks to the most suitable agent (human or AI) based on capabilities andworkload; and (3) a \textbf{Cross-Species Trust Calibration (CSTC)} protocolthat fosters transparency, accountability, and mutual adaptation throughexplainable declarations and structured feedback. Validated in a high-fidelityurban emergency response simulation, HMS-HI significantly reduced civiliancasualties by 72\% and cognitive load by 70\% compared to traditional HiTLapproaches, demonstrating superior decision quality, efficiency, and human-AItrust. An ablation study confirms the critical contribution of each module,highlighting that engineered trust and shared context are foundational forscalable, synergistic human-AI collaboration.</description>
      <author>example@mail.com (Ahmet Akkaya Melih, Yamuna Singh, Kunal L. Agarwal, Priya Mukherjee, Kiran Pattnaik, Hanuman Bhatia)</author>
      <guid isPermaLink="false">2510.24030v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Mars-Bench: A Benchmark for Evaluating Foundation Models for Mars Science Tasks</title>
      <link>http://arxiv.org/abs/2510.24010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Mars-Bench，这是第一个用于系统评估火星相关任务的基准，包含20个数据集，涵盖分类、分割和目标检测，专注于关键地质特征。研究结果表明火星特定的基础模型可能比通用领域模型具有优势，为火星科学领域的机器学习模型开发提供了标准化基础。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模无标签数据预训练在许多专业领域取得了快速进展，表现出强大的泛化能力。然而，这类模型在火星科学领域的应用仍然有限，主要原因是火星科学缺乏标准化基准和评估框架，限制了火星任务基础模型的发展。&lt;h4&gt;目的&lt;/h4&gt;引入Mars-Bench，第一个基准，旨在系统评估使用轨道和表面图像的广泛火星相关任务模型，为火星科学领域的机器学习模型开发提供标准化基础。&lt;h4&gt;方法&lt;/h4&gt;Mars-Bench包含20个数据集，涵盖分类、分割和目标检测，专注于陨石坑、锥体、巨石和霜等关键地质特征。提供标准化、即用型数据集和基线评估，使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;所有分析结果表明，火星特定的基础模型可能比通用领域模型具有优势，这激励了对领域自适应预训练的进一步探索。&lt;h4&gt;结论&lt;/h4&gt;Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化基础，其数据、模型和代码已公开可用。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模无标签数据预训练在许多专业领域取得了快速进展，显示出对各种下游任务的强大泛化能力。尽管这类模型在地球观测等领域受到广泛关注，但在火星科学领域的应用仍然有限。其他领域取得进展的一个关键因素是标准化基准的可用性，这些基准支持系统评估。相比之下，火星科学缺乏此类基准和标准化评估框架，这限制了火星任务基础模型的发展。为解决这一差距，我们引入了Mars-Bench，这是第一个基准，旨在使用轨道和表面图像系统评估广泛火星相关任务的模型。Mars-Bench包含20个数据集，涵盖分类、分割和目标检测，专注于陨石坑、锥体、巨石和霜等关键地质特征。我们提供了标准化、即用型数据集和基线评估，使用在自然图像、地球卫星数据和最先进的视觉语言模型上预训练的模型。所有分析的结果表明，火星特定的基础模型可能比通用领域对应模型具有优势，这激励了对领域自适应预训练的进一步探索。Mars-Bench旨在为开发和比较火星科学的机器学习模型建立标准化基础。我们的数据、模型和代码可在 https://mars-bench.github.io/ 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have enabled rapid progress across many specialized domainsby leveraging large-scale pre-training on unlabeled data, demonstrating stronggeneralization to a variety of downstream tasks. While such models have gainedsignificant attention in fields like Earth Observation, their application toMars science remains limited. A key enabler of progress in other domains hasbeen the availability of standardized benchmarks that support systematicevaluation. In contrast, Mars science lacks such benchmarks and standardizedevaluation frameworks, which have limited progress toward developing foundationmodels for Martian tasks. To address this gap, we introduce Mars-Bench, thefirst benchmark designed to systematically evaluate models across a broad rangeof Mars-related tasks using both orbital and surface imagery. Mars-Benchcomprises 20 datasets spanning classification, segmentation, and objectdetection, focused on key geologic features such as craters, cones, boulders,and frost. We provide standardized, ready-to-use datasets and baselineevaluations using models pre-trained on natural images, Earth satellite data,and state-of-the-art vision-language models. Results from all analyses suggestthat Mars-specific foundation models may offer advantages over general-domaincounterparts, motivating further exploration of domain-adapted pre-training.Mars-Bench aims to establish a standardized foundation for developing andcomparing machine learning models for Mars science. Our data, models, and codeare available at: https://mars-bench.github.io/.</description>
      <author>example@mail.com (Mirali Purohit, Bimal Gajera, Vatsal Malaviya, Irish Mehta, Kunal Kasodekar, Jacob Adler, Steven Lu, Umaa Rebbapragada, Hannah Kerner)</author>
      <guid isPermaLink="false">2510.24010v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>Why Foundation Models in Pathology Are Failing</title>
      <link>http://arxiv.org/abs/2510.23807v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;基础模型在非医学领域取得成功，但在计算病理学应用中存在根本性概念不匹配，需要重新思考建模范式&lt;h4&gt;背景&lt;/h4&gt;在非医学领域，基础模型通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理，预期计算病理学中也会迅速采用这些模型&lt;h4&gt;目的&lt;/h4&gt;检查基础模型在计算病理学中的缺点，论证这些缺点源于通用基础建模假设与人体组织复杂性之间的概念性不匹配&lt;h4&gt;方法&lt;/h4&gt;进行系统评估，识别导致基础模型在计算病理学中失效的七个相互关联原因&lt;h4&gt;主要发现&lt;/h4&gt;基础模型在计算病理学中存在低诊断准确性、鲁棒性差、几何不稳定性、计算需求量大以及安全漏洞等基本弱点&lt;h4&gt;结论&lt;/h4&gt;当前病理学基础模型在概念上与组织形态学性质不一致，需要对范式本身进行根本性重新思考&lt;h4&gt;翻译&lt;/h4&gt;在非医学领域，基础模型(FMs)通过大规模自监督和多模态学习彻底改变了计算机视觉和语言处理。因此，预期计算病理学中会迅速采用这些模型，并在癌症诊断、预后和多模态检索方面取得类似突破。然而，最近的系统评估揭示了基本弱点：低诊断准确性、鲁棒性差、几何不稳定性、计算需求量大，以及令人担忧的安全漏洞。这篇简短论文检查了这些缺点，并论证它们源于主流人工智能中通用基础建模的假设与人体组织内在复杂性之间的更深层次的概念性不匹配。确定了七个相互关联的原因：生物复杂性、无效的自监督、过度概括、过度的架构复杂性、缺乏领域特定创新、数据不足，以及与组织块大小相关的基本设计缺陷。这些发现表明，当前病理学基础模型在概念上仍然与组织形态学的性质不一致，需要对这一范式本身进行根本性的重新思考。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In non-medical domains, foundation models (FMs) have revolutionized computervision and language processing through large-scale self-supervised andmultimodal learning. Consequently, their rapid adoption in computationalpathology was expected to deliver comparable breakthroughs in cancer diagnosis,prognostication, and multimodal retrieval. However, recent systematicevaluations reveal fundamental weaknesses: low diagnostic accuracy, poorrobustness, geometric instability, heavy computational demands, and concerningsafety vulnerabilities. This short paper examines these shortcomings and arguesthat they stem from deeper conceptual mismatches between the assumptionsunderlying generic foundation modeling in mainstream AI and the intrinsiccomplexity of human tissue. Seven interrelated causes are identified:biological complexity, ineffective self-supervision, overgeneralization,excessive architectural complexity, lack of domain-specific innovation,insufficient data, and a fundamental design flaw related to tissue patch size.These findings suggest that current pathology foundation models remainconceptually misaligned with the nature of tissue morphology and call for afundamental rethinking of the paradigm itself.</description>
      <author>example@mail.com (Hamid R. Tizhoosh)</author>
      <guid isPermaLink="false">2510.23807v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:09 +0800</pubDate>
    </item>
    <item>
      <title>CountFormer: A Transformer Framework for Learning Visual Repetition and Structure in Class-Agnostic Object Counting</title>
      <link>http://arxiv.org/abs/2510.23785v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 2 tables, 6 figures. Submitted to IEEE 5th International  Conference on Electrical, Computer and Telecommunication Engineering (ICECTE  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了CountFormer，一个基于transformer的框架，用于学习识别重复和结构一致性，实现类别无关的物体计数。该模型使用DINOv2作为视觉编码器，并融入位置嵌入融合，在FSC-147数据集上实现了与当前最先进方法相当的性能，同时在结构复杂或密集场景中表现更优。&lt;h4&gt;背景&lt;/h4&gt;人类能够通过感知视觉重复和结构关系而非依赖类别身份来计数多样化物体，但大多数现有计数模型在物体具有复杂形状、内部对称性或重叠组件时经常计数错误。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够像人类一样通过感知视觉重复和结构关系来进行计数的模型，实现类别无关的物体计数。&lt;h4&gt;方法&lt;/h4&gt;基于CounTR架构，用自监督基础模型DINOv2替换视觉编码器以获得更丰富的特征表示，融入位置嵌入融合保留几何关系，并通过轻量级卷积解码器将特征解码为密度图。&lt;h4&gt;主要发现&lt;/h4&gt;在FSC-147数据集上，CountFormer实现了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出更高的准确性。&lt;h4&gt;结论&lt;/h4&gt;集成基础模型如DINOv2可以使计数系统接近人类的结构感知能力，朝着真正通用和无样本范例的计数范式迈进。&lt;h4&gt;翻译&lt;/h4&gt;人类可以通过感知视觉重复和结构关系而不是依赖类别身份来轻松计数多样化的物体。然而，大多数现有的计数模型无法复制这种能力；当物体表现出复杂形状、内部对称性或重叠组件时，它们经常计数错误。在这项工作中，我们引入了CountFormer，一个基于transformer的框架，用于学习识别重复和结构一致性，实现类别无关的物体计数。基于CounTR架构，我们的模型用自监督基础模型DINOv2替换了其视觉编码器，DINOv2产生更丰富和空间一致的特征表示。我们进一步融合位置嵌入，在通过轻量级卷积解码器将这些特征解码为密度图之前保留几何关系。在FSC-147数据集上评估，我们的模型实现了与当前最先进方法相当的性能，同时在结构复杂或密集堆积的场景中表现出更高的准确性。我们的研究结果表明，集成基础模型如DINOv2可以使计数系统接近人类的结构感知能力，朝着真正通用和无样本范例的计数范式迈进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans can effortlessly count diverse objects by perceiving visual repetitionand structural relationships rather than relying on class identity. However,most existing counting models fail to replicate this ability; they oftenmiscount when objects exhibit complex shapes, internal symmetry, or overlappingcomponents. In this work, we introduce CountFormer, a transformer-basedframework that learns to recognize repetition and structural coherence forclass-agnostic object counting. Built upon the CounTR architecture, our modelreplaces its visual encoder with the self-supervised foundation model DINOv2,which produces richer and spatially consistent feature representations. Wefurther incorporate positional embedding fusion to preserve geometricrelationships before decoding these features into density maps through alightweight convolutional decoder. Evaluated on the FSC-147 dataset, our modelachieves performance comparable to current state-of-the-art methods whiledemonstrating superior accuracy on structurally intricate or densely packedscenes. Our findings indicate that integrating foundation models such as DINOv2enables counting systems to approach human-like structural perception,advancing toward a truly general and exemplar-free counting paradigm.</description>
      <author>example@mail.com (Md Tanvir Hossain, Akif Islam, Mohd Ruhul Ameen)</author>
      <guid isPermaLink="false">2510.23785v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating Long-Term Memory for Long-Context Question Answering</title>
      <link>http://arxiv.org/abs/2510.23730v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages including appendix, 3 figures. Submitted to October ARR and  to Metacognition in Generative AI EurIPS workshop (under review for both)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究系统评估了不同类型的记忆增强方法，发现记忆架构复杂度应与模型能力相匹配，不同类型模型适合不同记忆方法，情景记忆可帮助大语言模型识别自身知识局限性。&lt;h4&gt;背景&lt;/h4&gt;大语言模型需要记忆功能实现真正的对话连续性和经验学习，但研究虽已聚焦复杂记忆系统开发，仍不清楚哪种记忆类型对长上下文对话任务最有效。&lt;h4&gt;目的&lt;/h4&gt;使用LoCoMo基准（一个需要多种推理策略的问答任务合成长上下文对话基准）系统评估增强记忆的方法。&lt;h4&gt;方法&lt;/h4&gt;分析四种记忆增强方法：全上下文提示、通过检索增强生成和智能体记忆实现的语义记忆、通过上下文学习实现的情景记忆、通过提示优化的程序记忆。&lt;h4&gt;主要发现&lt;/h4&gt;增强记忆方法可减少90%以上的token使用量同时保持有竞争力准确性；小型基础模型从RAG中获益最多；强大指令微调推理模型通过反思获得情景学习好处并受益于更复杂智能体语义记忆。&lt;h4&gt;结论&lt;/h4&gt;记忆架构复杂度应与模型能力相匹配，情景记忆可以帮助大语言模型识别自身知识的局限性。&lt;h4&gt;翻译&lt;/h4&gt;为了让大语言模型实现真正的对话连续性和从经验学习中受益，它们需要记忆功能。虽然研究已集中在复杂记忆系统的开发上，但目前尚不清楚哪种类型的记忆对长上下文对话任务最有效。我们使用LoCoMo（一个为需要多种推理策略的问答任务标注的合成长上下文对话基准）对增强记忆的方法进行了系统评估。我们分析了全上下文提示、通过检索增强生成和智能体记忆实现的语义记忆、通过上下文学习实现的情景记忆，以及通过提示优化的程序记忆。我们的研究结果表明，增强记忆的方法在保持有竞争力的准确性的同时，可减少90%以上的token使用量。记忆架构的复杂度应与模型能力相匹配，小型基础模型从RAG中获益最多，而强大的指令微调推理模型通过反思获得情景学习的好处，并受益于更复杂的智能体语义记忆。特别是，情景记忆可以帮助大语言模型识别自身知识的局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order for large language models to achieve true conversational continuityand benefit from experiential learning, they need memory. While research hasfocused on the development of complex memory systems, it remains unclear whichtypes of memory are most effective for long-context conversational tasks. Wepresent a systematic evaluation of memory-augmented methods using LoCoMo, abenchmark of synthetic long-context dialogues annotated for question-answeringtasks that require diverse reasoning strategies. We analyse full-contextprompting, semantic memory through retrieval-augmented generation and agenticmemory, episodic memory through in-context learning, and procedural memorythrough prompt optimization. Our findings show that memory-augmented approachesreduce token usage by over 90% while maintaining competitive accuracy. Memoryarchitecture complexity should scale with model capability, with smallfoundation models benefitting most from RAG, and strong instruction-tunedreasoning model gaining from episodic learning through reflections and morecomplex agentic semantic memory. In particular, episodic memory can help LLMsrecognise the limits of their own knowledge.</description>
      <author>example@mail.com (Alessandra Terranova, Björn Ross, Alexandra Birch)</author>
      <guid isPermaLink="false">2510.23730v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents</title>
      <link>http://arxiv.org/abs/2510.23691v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Game-TARS是一种基于人类对齐的原生键盘鼠标输入的通用游戏代理，使用统一的可扩展动作空间进行训练，在多种游戏任务上表现出色&lt;h4&gt;背景&lt;/h4&gt;与API或GUI方法不同，需要能够在异构领域(如OS、网络和模拟游戏)进行大规模持续预训练的游戏代理&lt;h4&gt;目的&lt;/h4&gt;开发一种通用游戏代理，通过简单的可扩展动作表示与大规模预训练相结合，实现广泛的计算机使用能力&lt;h4&gt;方法&lt;/h4&gt;Game-TARS在超过500B tokens的多样化轨迹和多模态数据上进行预训练，采用衰减持续损失减少因果混淆，以及高效的稀疏思考策略平衡推理深度和推理成本&lt;h4&gt;主要发现&lt;/h4&gt;在开放世界Minecraft任务上成功率比前SOTA模型高约2倍；在未见过的网络3D游戏中通用性接近新鲜人类；在FPS基准测试中优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet；统一动作空间在扩展到跨游戏和多模态数据时能持续改进&lt;h4&gt;结论&lt;/h4&gt;简单的可扩展动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用代理提供了有前途的发展路径&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Game-TARS，一种通用游戏代理，通过统一的、可扩展的动作空间进行训练，该动作空间锚定在人类对齐的原生键盘鼠标输入上。与基于API或GUI的方法不同，这种范式能够在包括操作系统、网络和模拟游戏在内的异构领域进行大规模持续预训练。Game-TARS在超过500B tokens的多样化轨迹和多模态数据上进行预训练。关键技术包括减少因果混淆的衰减持续损失，以及平衡推理深度和推理成本的高效稀疏思考策略。实验表明，Game-TARS在开放世界Minecraft任务上的成功率比之前的SOTA模型高出约2倍，在未见过的网络3D游戏中接近新鲜人类的通用性，并在FPS基准测试中优于GPT-5、Gemini-2.5-Pro和Claude-4-Sonnet。训练时间和测试时间的扩展结果证实，统一动作空间在扩展到跨游戏和多模态数据时能够持续改进。我们的结果表明，简单的可扩展动作表示与大规模预训练相结合，为具有广泛计算机使用能力的通用代理提供了一条有前途的道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何创建真正可扩展且具有广泛泛化能力的游戏智能体问题。这个问题很重要，因为构建能够与复杂动态数字环境无缝交互的通用人工智能体是实现AGI的关键路径，而视频游戏因其多样化的任务目标和丰富的视觉信息，成为训练和评估此类智能体的理想平台。现有方法在创建具有真正泛化能力的智能体方面仍面临重大挑战，限制了AI系统在开放世界环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统智能体的局限性，认识到动作空间与特定环境紧密耦合限制了泛化能力。他们提出将动作空间抽象到更低层次，直接锚定到人类交互的基本输入设备——键盘和鼠标。设计过程借鉴了ReAct范式将推理和动作统一输出，采用Deitke等人的在线思考协议(think-aloud protocol)收集高质量轨迹，使用视觉锚点方法解决多模态数据对齐问题，并在后训练阶段借鉴拒绝采样优化推理-动作链。这些方法基于对现有工作的理解，但进行了创新性整合和改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用统一锚定于键盘-鼠标输入的动作空间，结合稀疏思考策略和大规模持续预训练，实现跨领域的泛化能力。整体流程分为两个主要阶段：1)持续预训练阶段：使用统一动作空间收集游戏轨迹，通过在线思考协议收集稀疏ReAct轨迹，使用视觉锚点对齐多模态数据，采用衰减损失函数处理动作分布不平衡，在500B+ token上预训练；2)后训练阶段：通过指令遵循、多模态提示、稀疏思考优化、双层记忆架构和跨域数据整合，提升智能体的核心能力。最终在Minecraft、FPS游戏等未见环境中进行评估验证。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一动作空间：锚定到底层键盘-鼠标输入而非高层API，实现跨环境泛化；2)稀疏思考策略：只在关键决策点推理，通过在线思考协议收集高质量轨迹；3)衰减持续损失：解决动作分布不平衡导致的因果混淆；4)双层记忆架构：结合短期上下文和长期摘要记忆；5)跨域数据整合：将游戏数据与其他领域智能体轨迹结合。相比之前工作，传统API/GUI方法使用定制化动作集与环境紧密耦合，而Game-TARS的统一动作空间具有更好的泛化性；现有游戏智能体通常专注于特定环境，而Game-TARS通过大规模预训练实现了真正的泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Game-TARS通过统一锚定于键盘-鼠标输入的动作空间和大规模持续预训练，实现了在多样化游戏和环境中表现卓越的通用游戏智能体，相比之前的方法展现出显著的泛化能力和性能提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Game-TARS, a generalist game agent trained with a unified,scalable action space anchored to human-aligned native keyboard-mouse inputs.Unlike API- or GUI-based approaches, this paradigm enables large-scalecontinual pre-training across heterogeneous domains, including OS, web, andsimulation games. Game-TARS is pre-trained on over 500B tokens with diversetrajectories and multimodal data. Key techniques include a decaying continualloss to reduce causal confusion and an efficient Sparse-Thinking strategy thatbalances reasoning depth and inference cost. Experiments show that Game-TARSachieves about 2 times the success rate over the previous sota model onopen-world Minecraft tasks, is close to the generality of fresh humans inunseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnetin FPS benchmarks. Scaling results on training-time and test-time confirm thatthe unified action space sustains improvements when scaled to cross-game andmultimodal data. Our results demonstrate that simple, scalable actionrepresentations combined with large-scale pre-training provide a promising pathtoward generalist agents with broad computer-use abilities.</description>
      <author>example@mail.com (Zihao Wang, Xujing Li, Yining Ye, Junjie Fang, Haoming Wang, Longxiang Liu, Shihao Liang, Junting Lu, Zhiyong Wu, Jiazhan Feng, Wanjun Zhong, Zili Li, Yu Wang, Yu Miao, Bo Zhou, Yuanfan Li, Hao Wang, Zhongkai Zhao, Faming Wu, Zhengxuan Jiang, Weihao Tan, Heyuan Yao, Shi Yan, Xiangyang Li, Yitao Liang, Yujia Qin, Guang Shi)</author>
      <guid isPermaLink="false">2510.23691v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Explicit Memory through Online 3D Gaussian Splatting Improves Class-Agnostic Video Segmentation</title>
      <link>http://arxiv.org/abs/2510.23521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Robotics and Automation Letters September 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用显式3D记忆增强视频分割模型的方法，通过在线3D高斯溅射技术存储预测的对象级片段，显著提高了预测的准确性和一致性。&lt;h4&gt;背景&lt;/h4&gt;现有视频分割算法通常不使用对象级记忆（如FastSAM）或仅使用循环神经网络特征的隐式记忆（如SAM2），而记住过去预测的对象片段位置对提高分割质量至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种显式3D记忆方法来增强现有分割模型，使增强后的模型具有更准确和一致的预测能力。&lt;h4&gt;方法&lt;/h4&gt;开发在线3D高斯溅射（3DGS）技术存储视频过程中生成的预测对象级片段，并基于此开发融合技术FastSAM-Splat和SAM2-Splat，利用显式3DGS记忆改进各自基础模型预测。&lt;h4&gt;主要发现&lt;/h4&gt;消融实验验证了所提技术和超参数设置的有效性；真实世界和模拟基准实验结果表明，使用显式3D记忆的模型比无记忆或仅使用隐式神经网络记忆的模型产生更准确和一致的预测。&lt;h4&gt;结论&lt;/h4&gt;显式3D记忆技术可以显著改善视频分割模型的性能，提高预测的准确性和一致性。&lt;h4&gt;翻译&lt;/h4&gt;记住过去预测的对象片段位置对提高无类别视频分割算法的准确性和一致性是有用的。现有的视频分割算法通常使用不使用对象级记忆（例如FastSAM）或使用循环神经网络特征的隐式记忆（例如SAM2）。在本文中，我们使用显式3D记忆增强这两种分割模型，并证明 resulting 模型具有更准确和一致的预测。为此，我们开发了一种在线3D高斯溅射（3DGS）技术来存储在整个视频持续时间内生成的预测对象级片段。基于这种3DGS表示，开发了一系列融合技术，分别命名为FastSAM-Splat和SAM2-Splat，它们使用显式3DGS记忆来改进各自基础模型的预测。使用消融实验来验证所提技术的设计和超参数设置。来自真实世界和模拟基准实验的结果表明，使用显式3D记忆的模型比不使用记忆或仅使用隐式神经网络记忆的模型产生更准确和一致的预测。项目页面：https://topipari.com/projects/FastSAM-Splat/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决类无关视频分割(class-agnostic video segmentation)的准确性和一致性问题。这个问题在机器人应用中非常重要，因为机器人需要在人类家庭环境中构建有用的语义地图，必须能够检测和分割任何类别的物体（包括部署前未知的物体）。现实环境中的遮挡、低光照、重复和动态物体等因素使得这一挑战更加复杂，而现有的视频分割算法要么不使用物体级记忆，要么使用隐式记忆，导致分割结果在时间维度上不一致，影响机器人对环境的理解和交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到记住物体分割在过去的预测位置对提高视频分割一致性很有用，并假设模型如果能访问过去预测的密集3D记忆将会受益。他们借鉴了3D高斯溅射(3DGS)技术，这是一种用于密集3D场景重建的强大表示方法。作者还受到在线3DGS技术的启发，这些技术可以从视频输入中实时构建环境3D地图。他们扩展了3DGS表示，将每个高斯与一个段ID特征向量关联，用于存储语义记忆，并基于对比学习优化段ID码本，确保不同物体段的ID向量之间有足够距离。这种方法结合了现有的视频分割模型(如FastSAM和SAM2)和3D重建技术，创造性地将显式3D记忆引入视频分割任务。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用显式的3D记忆来增强视频分割模型，以提高分割的一致性和准确性。通过在线3D高斯溅射(3DGS)技术存储物体分割的历史信息，形成一个3D记忆系统，利用这个记忆来指导当前帧的分割预测。整体实现流程包括：1)构建3DGS表示，每个高斯参数包括位置、方向、缩放、不透明度、颜色和段ID特征；2)创建段ID码本，使用对比损失优化确保不同段ID间有足够距离；3)对于FastSAM-Splat，将渲染的3DGS段与FastSAM预测的图像段匹配并融合；4)对于SAM2-Splat，使用SAM2预测的跟踪ID与3DGS段关联，识别不一致并通过重新提示SAM2来纠正错误；5)更新3DGS记忆以对齐当前帧的预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)FastSAM-Splat模型，扩展了原本没有时间记忆的FastSAM，通过集成3DGS记忆提高分割一致性；2)SAM2-Splat模型，开发了基于3DGS的重新提示策略，通过整合显式3D记忆减少SAM2的不一致预测；3)通过实验验证显式3D记忆的优势。相比之前的工作，这种方法使用显式的3D记忆而非无物体级记忆或隐式记忆(如循环神经网络特征)；使用3D高斯溅射存储和表示物体分割历史，这是一种更密集和结构化的表示；专注于机器人应用场景，利用深度和相机姿态信息构建3D记忆；针对不同类型的分割模型(FastSAM和SAM2)设计了不同的融合策略；实验表明在处理遮挡和物体重新出现等挑战性场景时性能更优。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文通过引入基于在线3D高斯溅射的显式记忆机制，显著提升了类无关视频分割的准确性和时间一致性，为机器人感知任务提供了更可靠的分割解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3619783&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Remembering where object segments were predicted in the past is useful forimproving the accuracy and consistency of class-agnostic video segmentationalgorithms. Existing video segmentation algorithms typically use either noobject-level memory (e.g. FastSAM) or they use implicit memories in the form ofrecurrent neural network features (e.g. SAM2). In this paper, we augment bothtypes of segmentation models using an explicit 3D memory and show that theresulting models have more accurate and consistent predictions. For this, wedevelop an online 3D Gaussian Splatting (3DGS) technique to store predictedobject-level segments generated throughout the duration of a video. Based onthis 3DGS representation, a set of fusion techniques are developed, namedFastSAM-Splat and SAM2-Splat, that use the explicit 3DGS memory to improvetheir respective foundation models' predictions. Ablation experiments are usedto validate the proposed techniques' design and hyperparameter settings.Results from both real-world and simulated benchmarking experiments show thatmodels which use explicit 3D memories result in more accurate and consistentpredictions than those which use no memory or only implicit neural networkmemories. Project Page: https://topipari.com/projects/FastSAM-Splat/</description>
      <author>example@mail.com (Anthony Opipari, Aravindhan K Krishnan, Shreekant Gayaka, Min Sun, Cheng-Hao Kuo, Arnie Sen, Odest Chadwicke Jenkins)</author>
      <guid isPermaLink="false">2510.23521v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Dexbotic: Open-Source Vision-Language-Action Toolbox</title>
      <link>http://arxiv.org/abs/2510.23511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Authors are listed in alphabetical order. The official website is  located at https://dexbotic.com/. Code is available at  https://github.com/Dexmal/dexbotic&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Dexbotic，一个基于PyTorch的开源视觉-语言-行动模型工具箱，为具身智能研究提供一站式服务。&lt;h4&gt;背景&lt;/h4&gt;具身智能领域需要有效的工具来支持视觉-语言-行动模型的研究和开发，现有工具可能缺乏统一性和易用性。&lt;h4&gt;目的&lt;/h4&gt;提供一个开源的、统一的VLA模型工具箱，使研究人员能够轻松复现各种VLA方法，快速开发新实验，并利用更强大的预训练模型提升性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一个基于PyTorch的Dexbotic工具箱，支持多种主流VLA策略，提供实验为中心的开发环境，并开发更强大的预训练模型。&lt;h4&gt;主要发现&lt;/h4&gt;该工具箱能够支持多种VLA策略的统一实现，通过简单的环境设置即可复现各种方法；通过修改Exp脚本可以快速开发新实验；使用更强大的预训练模型可以显著提升最先进VLA策略的性能。&lt;h4&gt;结论&lt;/h4&gt;Dexbotic作为一个开源工具箱，有效简化了VLA模型的研究流程，提高了研究效率，并将持续更新以包含最新的预训练模型和前沿VLA模型。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了Dexbotic，一个基于PyTorch的开源视觉-语言-行动模型工具箱。它旨在为具身智能领域的专业人士提供一站式VLA研究服务。它提供了一个代码库，同时支持多种主流VLA策略，使用户只需通过单一环境设置即可重现各种VLA方法。该工具箱以实验为中心，用户只需修改Exp脚本即可快速开发新的VLA实验。此外，我们提供了更强大的预训练模型，以实现最先进的VLA策略的性能提升。Dexbotic将不断更新，以包含更多最新的预训练基础模型和行业前沿的VLA模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present Dexbotic, an open-source Vision-Language-Action(VLA) model toolbox based on PyTorch. It aims to provide a one-stop VLAresearch service for professionals in the field of embodied intelligence. Itoffers a codebase that supports multiple mainstream VLA policiessimultaneously, allowing users to reproduce various VLA methods with just asingle environment setup. The toolbox is experiment-centric, where the userscan quickly develop new VLA experiments by simply modifying the Exp script.Moreover, we provide much stronger pretrained models to achieve greatperformance improvements for state-of-the-art VLA policies. Dexbotic willcontinuously update to include more of the latest pre-trained foundation modelsand cutting-edge VLA models in the industry.</description>
      <author>example@mail.com (Bin Xie, Erjin Zhou, Fan Jia, Hao Shi, Haoqiang Fan, Haowei Zhang, Hebei Li, Jianjian Sun, Jie Bin, Junwen Huang, Kai Liu, Kaixin Liu, Kefan Gu, Lin Sun, Meng Zhang, Peilong Han, Ruitao Hao, Ruitao Zhang, Saike Huang, Songhan Xie, Tiancai Wang, Tianle Liu, Wenbin Tang, Wenqi Zhu, Yang Chen, Yingfei Liu, Yizhuang Zhou, Yu Liu, Yucheng Zhao, Yunchao Ma, Yunfei Wei, Yuxiang Chen, Ze Chen, Zeming Li, Zhao Wu, Ziheng Zhang, Ziming Liu, Ziwei Yan, Ziyu Zhang)</author>
      <guid isPermaLink="false">2510.23511v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Vulnerability in AI Industry</title>
      <link>http://arxiv.org/abs/2510.23421v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary Draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种合成AI脆弱性指数(AIVI)，评估Foundation Models(FMs)生产上游价值链的脆弱性，重点关注计算、数据、人才、资本和能源五个关键输入因素。&lt;h4&gt;背景&lt;/h4&gt;Foundation Models(FMs)因Transformer架构的快速发展而推动当前AI生态系统。这些大型模型通过大规模训练和下游适应性获得广泛采用，形成由平台经济学和激烈投资塑造的动荡市场。&lt;h4&gt;目的&lt;/h4&gt;由于数据限制，评估快速发展的AI行业脆弱性具有挑战性。本研究旨在提出一种专注于FM生产上游价值链的合成AI脆弱性指数(AIVI)，优先考虑公开可用数据。&lt;h4&gt;方法&lt;/h4&gt;将FM输出建模为五个输入的函数：计算(Compute)、数据(Data)、人才(Talent)、资本(Capital)和能源(Energy)，假设任何输入的供应脆弱性都会威胁整个行业。使用加权几何平均数聚合子指数，并使用理论或经验基准进行归一化。&lt;h4&gt;主要发现&lt;/h4&gt;关键脆弱性包括：计算集中、数据稀缺和法律风险、人才瓶颈、资本密集度和战略依赖性，以及不断增长的能源需求。&lt;h4&gt;结论&lt;/h4&gt;尽管存在局限性和改进空间，但这一初步指数旨在量化AI核心生产引擎中的系统性风险，并间接揭示下游价值链的风险。&lt;h4&gt;翻译&lt;/h4&gt;Foundation Models(FMs)的快速发展，得益于Transformer架构，推动了当前的AI生态系统。这些大型模型以大规模训练和下游适应性为特征（如GPT系列），已获得广泛采用，促成了由平台经济学和激烈投资塑造的动荡市场。由于数据限制，评估这个快速发展的行业的脆弱性至关重要且具有挑战性。本文提出了一种专注于FM生产上游价值链的合成AI脆弱性指数(AIVI)，优先考虑公开可用数据。我们将FM输出建模为五个输入的函数：计算、数据、人才、资本和能源，并假设任何输入的供应脆弱性都会威胁整个行业。主要脆弱性包括计算集中、数据稀缺和法律风险、人才瓶颈、资本密集度和战略依赖性，以及不断增长的能源需求。考虑到输入的不完全可替代性，我们提出使用加权几何平均数来聚合子指数，并使用理论或经验基准进行归一化。尽管存在局限性和改进空间，但这一初步指数旨在量化AI核心生产引擎中的系统性风险，并间接揭示了下游价值链的风险。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid ascent of Foundation Models (FMs), enabled by the Transformerarchitecture, drives the current AI ecosystem. Characterized by large-scaletraining and downstream adaptability, FMs (as GPT family) have achieved massivepublic adoption, fueling a turbulent market shaped by platform economics andintense investment. Assessing the vulnerability of this fast-evolving industryis critical yet challenging due to data limitations. This paper proposes asynthetic AI Vulnerability Index (AIVI) focusing on the upstream value chainfor FM production, prioritizing publicly available data. We model FM output asa function of five inputs: Compute, Data, Talent, Capital, and Energy,hypothesizing that supply vulnerability in any input threatens the industry.Key vulnerabilities include compute concentration, data scarcity and legalrisks, talent bottlenecks, capital intensity and strategic dependencies, aswell as escalating energy demands. Acknowledging imperfect inputsubstitutability, we propose a weighted geometrical average of aggregatesubindexes, normalized using theoretical or empirical benchmarks. Despitelimitations and room for improvement, this preliminary index aims to quantifysystemic risks in AI's core production engine, and implicitly shed a light onthe risks for downstream value chain.</description>
      <author>example@mail.com (Claudio Pirrone, Stefano Fricano, Gioacchino Fazio)</author>
      <guid isPermaLink="false">2510.23421v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Towards Generalisable Foundation Models for 3D Brain MRI</title>
      <link>http://arxiv.org/abs/2510.23415v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;BrainFound是一种自监督基础模型，通过扩展DINO-v2视觉转换器构建，专为脑部MRI设计，能够处理3D脑部解剖信息，支持单模态和多模态输入，在标签稀缺和多对比度环境下表现优异，提高了诊断准确性并减少了对专家标注的依赖。&lt;h4&gt;背景&lt;/h4&gt;人工智能基础模型通过大规模无标签数据集实现通用特征学习，正在改变医学影像领域。&lt;h4&gt;目的&lt;/h4&gt;开发一个针对脑部MRI的自监督基础模型，能够处理3D脑部解剖信息，支持多种下游任务。&lt;h4&gt;方法&lt;/h4&gt;通过扩展DINO-v2视觉转换器构建BrainFound，整合连续MRI切片的体积信息来建模完整3D脑部解剖结构，支持单模态和多模态输入，适用于多种MRI模态（如T1、T2、FLAIR）。&lt;h4&gt;主要发现&lt;/h4&gt;BrainFound在性能上始终优于现有的自监督预训练策略和监督基线，特别是在标签稀缺和多对比度设置下；通过整合多种3D MRI模态信息，提高了诊断准确性，减少了对大量专家标注的依赖。&lt;h4&gt;结论&lt;/h4&gt;BrainFound的灵活性使其成为3D神经影像流程的可扩展且实用的解决方案，具有在临床部署和研究创新方面的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;人工智能（AI）中的基础模型正在通过大规模无标签数据集实现通用特征学习，从而改变医学影像。在这项工作中，我们介绍了BrainFound，这是一个用于脑部MRI的自监督基础模型，通过扩展DINO-v2（一种最初为2D自然图像设计的视觉转换器）构建。BrainFound通过整合连续MRI切片的体积信息来适应DINO-v2，以建模完整的3D脑部解剖结构，超越了传统的单切片范式。它支持单模态和多模态输入，能够实现广泛的下游任务，包括疾病检测和图像分割，同时能够在不同的成像协议和临床场景中泛化。我们证明BrainFound在性能上始终优于现有的自监督预训练策略和监督基线，特别是在标签稀缺和多对比度设置下。通过整合多种3D MRI模态（如T1、T2、FLAIR）的信息，它提高了诊断准确性，减少了对大量专家标注的依赖。这种灵活性使BrainFound成为3D神经影像流程的可扩展且实用的解决方案，在临床部署和研究创新方面具有巨大潜力。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何构建通用的基础模型用于3D脑部MRI分析的问题。这个问题在现实中非常重要，因为放射科医生面临巨大工作压力（平均每3-4秒需解读一张图像），导致诊断延迟和错误；深度学习在放射学领域虽潜力巨大，但成功依赖于大量昂贵耗时的标记数据；现有监督模型难以跨领域泛化；而大多数基础模型是为2D自然图像设计，无法有效处理3D医学影像数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者借鉴了DINO-v2框架（一种为2D自然图像设计的视觉变换器）和自监督学习方法，特别是对比学习和知识蒸馏。设计思路是将DINO-v2从2D扩展到3D，通过处理3D扫描作为2D切片序列；设计支持单模态和多模态MRI输入的架构；将T1、T2和FLAIR扫描堆叠为通道输入（类似RGB图像）；采用多尺度裁剪策略捕获全局和局部脑结构；使用双域预训练（先在自然图像上预训练，再在脑部MRI上微调）。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将3D脑部MRI作为2D切片序列处理，利用自监督学习在大规模未标记数据上学习通用特征表示，结合自然图像和脑部MRI的双域预训练，并支持多模态输入整合不同MRI对比度的互补信息。整体流程包括：1)收集10,000个体积脑部MRI图像并进行标准化预处理；2)基于DINO-v2构建支持多模态输入的Vision Transformer架构；3)使用多尺度裁剪和自监督知识蒸馏方法进行预训练；4)在下游任务（疾病检测和图像分割）上进行微调应用。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)将DINO-v2从2D扩展到3D脑部MRI处理；2)支持可变体积深度，提高对不同MRI任务的适应性；3)设计能处理单模态和多模态MRI输入的架构，将不同MRI模态堆叠为通道输入；4)采用双域预训练策略，结合自然图像和脑部MRI的优势；5)统一处理疾病检测和图像分割任务。相比之前的工作，BrainFound在多种任务上表现优于仅使用自然图像预训的模型、仅在脑部图像上从头训练的自监督模型以及其他自监督方法，其多模态设计也提供了比单模态模型更强的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; BrainFound通过将DINO-v2框架扩展到3D脑部MRI并采用双域预训练策略，创建了一个强大的自监督基础模型，能够有效处理多模态输入并在疾病检测和图像分割任务上实现卓越的泛化性能，减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models in artificial intelligence (AI) are transforming medicalimaging by enabling general-purpose feature learning from large-scale,unlabeled datasets. In this work, we introduce BrainFound, a self-supervisedfoundation model for brain MRI, built by extending DINO-v2, a visiontransformer originally designed for 2D natural images. BrainFound adaptsDINO-v2 to model full 3D brain anatomy by incorporating volumetric informationfrom sequential MRI slices, moving beyond conventional single-slice paradigms.It supports both single- and multimodal inputs, enabling a broad range ofdownstream tasks, including disease detection and image segmentation, whilegeneralising across varied imaging protocols and clinical scenarios. We showthat BrainFound consistently outperforms existing self-supervised pretrainingstrategies and supervised baselines, particularly in label-scarce andmulti-contrast settings. By integrating information from diverse 3D MRImodalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reducesdependency on extensive expert annotations. This flexibility makes BrainFound ascalable and practical solution for 3D neuroimaging pipelines, with significantpotential for clinical deployment and research innovation.</description>
      <author>example@mail.com (Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander)</author>
      <guid isPermaLink="false">2510.23415v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Bid2X: Revealing Dynamics of Bidding Environment in Online Advertising from A Foundation Model Lens</title>
      <link>http://arxiv.org/abs/2510.23410v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Bid2X出价基础模型，通过统一的函数估计特定出价下的效果，解决了传统出价模型在不同场景间泛化能力有限的问题。该模型结合了序列嵌入、双注意力机制和零膨胀投影模块，在淘宝广告平台部署后显著提升了广告效果。&lt;h4&gt;背景&lt;/h4&gt;自动出价对在线广告至关重要，但现有出价模型通常针对特定场景设计，在不同环境间的泛化能力有限。&lt;h4&gt;目的&lt;/h4&gt;探索场景无关的出价原则，提出一个统一的函数来估计特定出价下的效果，如预算消耗、商品交易总额(GMV)和页面浏览量等。&lt;h4&gt;方法&lt;/h4&gt;提出Bid2X出价基础模型，构建在统一序列嵌入之上，通过定制嵌入方法编码异构数据；提出两种注意力机制分别处理不同变量和不同时间的嵌入；使用变量感知融合模块进行自适应出价结果预测；设计零膨胀投影模块将估计的非零概率纳入值预测，形成包含分类和回归的联合优化目标。&lt;h4&gt;主要发现&lt;/h4&gt;模型已在淘宝广告平台部署；在八个数据集上的离线评估显示Bid2X优于各种基线模型且在不同场景中具有通用性；在线A/B测试中GMV增加4.65%，ROI增加2.44%。&lt;h4&gt;结论&lt;/h4&gt;Bid2X为计算广告中的出价基础模型铺平了道路，展示了基础模型在广告出价领域的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;自动出价对于通过为广告商自动提供出价来促进在线广告至关重要。虽然之前的工作在建模出价环境以获得更好的广告效果方面做出了巨大努力，但这些模型通常针对特定的出价场景定制，在不同环境间的泛化能力存在局限性。为此，我们通过一个统一的函数来探索场景无关的原则，该函数估计特定出价下的效果，如预算消耗、商品交易总额(GMV)、页面浏览量等。然后，我们提出了Bid2X出价基础模型，从各种场景的数据中学习这个基本函数。我们的Bid2X构建在统一序列嵌入之上，通过定制的嵌入方法编码异构数据。为了捕捉出价数据中复杂的变量间动态和时间依赖关系，我们提出了两种注意力机制，分别将不同变量和不同时间的嵌入作为注意力令牌进行表示学习。在学习到的变量和时间表示之上，使用变量感知融合模块进行自适应出价结果预测。为了建模独特的出价数据分布，我们设计了一个零膨胀投影模块，将估计的非零概率纳入其值预测，这构成了一个包含分类和回归的联合优化目标。该目标被证明可以收敛到零膨胀分布。我们的模型已部署在淘宝广告平台上，这是世界上最大的电子商务平台之一。在八个数据集上的离线评估显示，与各种基线相比，Bid2X具有优越性，并且在不同场景中具有通用性。Bid2X在线A/B测试中使GMV增加了4.65%，ROI增加了2.44%，为计算广告中的出价基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auto-bidding is crucial in facilitating online advertising by automaticallyproviding bids for advertisers. While previous work has made great efforts tomodel bidding environments for better ad performance, it has limitations ingeneralizability across environments since these models are typically tailoredfor specific bidding scenarios. To this end, we approach thescenario-independent principles through a unified function that estimates theachieved effect under specific bids, such as budget consumption, grossmerchandise volume (GMV), page views, etc. Then, we propose a biddingfoundation model Bid2X to learn this fundamental function from data in variousscenarios. Our Bid2X is built over uniform series embeddings that encodeheterogeneous data through tailored embedding methods. To capture complexinter-variable and dynamic temporal dependencies in bidding data, we proposetwo attention mechanisms separately treating embeddings of different variablesand embeddings at different times as attention tokens for representationlearning. On top of the learned variable and temporal representations, avariable-aware fusion module is used to perform adaptive bidding outcomeprediction. To model the unique bidding data distribution, we devise azero-inflated projection module to incorporate the estimated non-zeroprobability into its value prediction, which makes up a joint optimizationobjective containing classification and regression. The objective is proven toconverge to the zero-inflated distribution. Our model has been deployed on thead platform in Taobao, one of the world's largest e-commerce platforms. Offlineevaluation on eight datasets exhibits Bid2X's superiority compared to variousbaselines and its generality across different scenarios. Bid2X increased GMV by4.65% and ROI by 2.44% in online A/B tests, paving the way for biddingfoundation model in computational advertising.</description>
      <author>example@mail.com (Jiahao Ji, Tianyu Wang, Yeshu Li, Yushen Huo, Zhilin Zhang, Chuan Yu, Jian Xu, Bo Zheng)</author>
      <guid isPermaLink="false">2510.23410v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Solar flare forecasting with foundational transformer models across image, video, and time-series modalities</title>
      <link>http://arxiv.org/abs/2510.23400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究比较了基于Transformer的架构在利用异构数据模态（包括图像、视频序列和时间序列观测）进行太阳耀斑预测方面的性能&lt;h4&gt;背景&lt;/h4&gt;太阳活动预测对于空间天气预警至关重要，但需要处理多种类型的数据&lt;h4&gt;目的&lt;/h4&gt;评估Transformer主干架构在太阳活动的空间和时间表示方面的优势和局限性&lt;h4&gt;方法&lt;/h4&gt;使用三种预训练模型（SigLIP2用于图像编码，VideoMAE用于时空视频表示，Moirai2用于多元时间序列预测）处理来自SDO/HMI任务的太阳磁图和GOES卫星的软X射线通量数据，并采用多种损失函数和训练平衡策略来处理类别不平衡问题&lt;h4&gt;主要发现&lt;/h4&gt;虽然SigLIP2和VideoMAE在图像和视频数据上达到典型性能（真实技能统计约0.60-0.65），但仅基于辐照度时间演化的Moirai2时间序列模型达到了优越的预测技能（真实技能统计约0.74）&lt;h4&gt;结论&lt;/h4&gt;预训练Transformer架构和跨模态学习对推进业务空间天气预报具有潜力，为整合视觉和时间信息的统一多模态模型铺平了道路&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一项基于Transformer架构的太阳耀斑预测的比较研究，该研究使用异构数据模态，包括图像、视频序列和时间序列观测。我们的分析评估了三个最近的基础模型 - 用于图像编码的SigLIP2，用于时空视频表示的VideoMAE，以及用于多元时间序列预测的Moirai2 - 应用于来自SDO/HMI任务的太阳磁图公共数据集和GOES卫星获取的软X射线通量。所有模型在一致的数据分割和评估标准下进行训练和验证，旨在评估Transformer主干架构在太阳活动的空间和时间表示方面的优势和局限性。我们研究了多种损失公式（加权BCE、focal和分数导向的）和训练平衡策略，以减轻耀斑数据集中典型的类别不平衡。结果表明，虽然SigLIP2和VideoMAE在图像和视频数据上达到典型性能（真实技能统计约0.60-0.65），但时间序列模型Moirai2仅基于辐照度时间演化就达到了优越的预测技能（真实技能统计约0.74）。这些发现突显了预训练Transformer架构和跨模态学习在推进业务空间天气预报方面的潜力，为整合视觉和时间信息的统一多模态模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a comparative study of transformer-based architectures for solarflare forecasting using heterogeneous data modalities, including images, videosequences, and time-series observations. Our analysis evaluates three recentfoundational models - SigLIP2 for image encoding, VideoMAE for spatio-temporalvideo representation, and Moirai2 for multivariate time-series forecasting -applied to publicly available datasets of solar magnetograms from the SDO/HMImission and soft X-ray fluxes acquired by GOES satellites. All models aretrained and validated under consistent data splits and evaluation criteria,with the goal of assessing the strengths and limitations of transformerbackbones across spatial and temporal representations of solar activity. Weinvestigate multiple loss formulations (weighted BCE, focal, andscore-oriented) and training balance strategies to mitigate class imbalancetypical of flare datasets. Results show that while both SigLIP2 and VideoMAEachieve typical performance on image and video data (True Skill StatisticTSS~0.60-0.65), the time-series model Moirai2 reaches superior forecastingskill (TSS~0.74) using irradiance-based temporal evolution alone. Thesefindings highlight the potential of pretrained transformer architectures andcross-modal learning for advancing operational space weather forecasting,paving the way toward unified multimodal models that integrate visual andtemporal information.</description>
      <author>example@mail.com (S. Riggi, P. Romano, A. Pilzer, U. Becciani)</author>
      <guid isPermaLink="false">2510.23400v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ZeroFlood: A Geospatial Foundation Model for Data-Efficient Flood Susceptibility Mapping</title>
      <link>http://arxiv.org/abs/2510.23364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint submitted to EUSAR 2026 (under review)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZeroFlood是一种地理空间基础模型框架，通过思维模态推理实现数据高效的洪水易感性映射，能够在数据稀缺地区从基本地球观测数据进行洪水预测。&lt;h4&gt;背景&lt;/h4&gt;洪水易感性映射对于灾害预防至关重要，但在数据稀缺地区面临挑战，因为传统水动力模型需要密集的地球物理输入数据。&lt;h4&gt;目的&lt;/h4&gt;开发ZeroFlood框架，解决数据稀缺地区洪水易感性映射的问题，提供一种数据高效的解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用思维模态(TiM)推理微调地理空间基础模型(GFMs)，从Sentinel-1或Sentinel-2等基本地球观测数据进行洪水预测；利用数据丰富地区的成对地球观测和模拟洪水地图，通过跨模态表示学习弥合数据差距；使用TerraMind和Prithvi GFMs进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;TiM推理增强了模型的鲁棒性，TerraMind-Large配置实现了67.21的F1分数。&lt;h4&gt;结论&lt;/h4&gt;基于基础模型的FSM是一种可扩展且数据高效的洪水风险管理解决方案，适用于数据稀缺地区。&lt;h4&gt;翻译&lt;/h4&gt;洪水易感性映射(FSM)对灾害预防至关重要，但在需要密集地球物理输入的水动力模型难以应用的稀缺数据地区仍然具有挑战性。本文介绍了ZeroFlood，一种用于数据高效FSM的地理空间基础模型框架。该方法通过思维模态(TiM)推理微调地理空间基础模型(GFMs)，能够从Sentinel-1或Sentinel-2等基本地球观测数据进行洪水预测。利用数据丰富地区的成对地球观测和模拟洪水地图，ZeroFlood通过跨模态表示学习弥合了数据可用性差距。使用TerraMind和Prithvi GFMs的实验表明，TiM增强了模型鲁棒性，其中TerraMind-Large配置实现了67.21的F1分数。结果证明了基于基础模型的FSM作为可扩展和数据高效的洪水风险管理解决方案的可行性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flood susceptibility mapping (FSM) is vital for disaster prevention butremains challenging in data-scarce regions where hydrodynamic models requiredense geophysical inputs. This work introduces ZeroFlood, a geospatialfoundation model framework for data-efficient FSM. The approach fine-tunesGeospatial Foundation Models (GFMs) with Thinking-in-Modality (TiM) reasoning,enabling flood prediction from basic Earth observation data such as Sentinel-1or Sentinel-2 imagery. Using paired EO and simulated flood maps from data-richregions, ZeroFlood bridges data availability gaps through cross-modalrepresentation learning. Experiments with TerraMind and Prithvi GFMs show thatTiM enhances model robustness, with the TerraMind-Large configuration achievingan F1 score of 67.21. The results demonstrate the feasibility offoundation-model-based FSM as a scalable and data-efficient solution for floodrisk management.</description>
      <author>example@mail.com (Hyeongkyun Kim, Orestis Oikonomou)</author>
      <guid isPermaLink="false">2510.23364v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Provable test-time adaptivity and distributional robustness of in-context learning</title>
      <link>http://arxiv.org/abs/2510.23254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文研究了预训练Transformer在不同难度任务上的性能表现，证明其能够达到与任务难度相对应的最优收敛速率，并且对分布偏移具有鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;研究上下文学习问题，其中Transformer在从混合分布中抽取的任务上进行预训练，该混合分布由不同难度级别的任务分布组成。&lt;h4&gt;目的&lt;/h4&gt;理解预训练Transformer在不同于预训练分布的测试分布上的性能，特别是当测试分布与预训练分布中对应难度级别的分布存在卡方散度限制的偏移时。&lt;h4&gt;方法&lt;/h4&gt;考虑非参数回归问题和多指标模型，分析大型预训练Transformer在这些模型上的收敛性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练Transformer能够达到与任务难度级别相对应的最优收敛速率，并且在卡方散度球内的测试分布上是一致的；Transformer在较容易任务上收敛更快，且对分布偏移具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;预训练Transformer即使面对分布偏移也能保持最优性能，其性能优于理论上能够访问测试分布的估计器，提供了比最小最大下界更合适的最优性保证。&lt;h4&gt;翻译&lt;/h4&gt;我们研究上下文学习问题，其中Transformer在从混合分布中抽取的任务上进行预训练，称为预训练先验，其中每个混合分量是针对特定难度级别的任务分布。我们的目标是理解预训练Transformer在不同于测试分布上的性能表现，该测试分布由固定难度的任务组成，并且相对于对应难度级别的分布可能存在分布偏移，但卡方散度至多为某个常数。特别是，我们考虑具有随机光滑性的非参数回归问题，以及具有随机光滑性和随机有效维度的多指标模型。我们证明，在足够数据上预训练的大型Transformer能够达到与难度级别相对应的最优收敛速率，并且在卡方散度球内的测试分布上是一致的。因此，预训练的Transformer能够在较容易的任务上实现更快的收敛速率，并且对测试时的分布偏移具有鲁棒性。最后，我们证明即使估计器能够访问测试分布，其在测试分布上的期望风险的收敛速率也不会比预训练的Transformer更快，从而提供了比最小最大下界更合适的最优性保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study in-context learning problems where a Transformer is pretrained ontasks drawn from a mixture distribution $\pi=\sum_{\alpha\in\mathcal{A}}\lambda_{\alpha} \pi_{\alpha}$, called the pretraining prior, in which eachmixture component $\pi_{\alpha}$ is a distribution on tasks of a specificdifficulty level indexed by $\alpha$. Our goal is to understand the performanceof the pretrained Transformer when evaluated on a different test distribution$\mu$, consisting of tasks of fixed difficulty $\beta\in\mathcal{A}$, and withpotential distribution shift relative to $\pi_\beta$, subject to thechi-squared divergence $\chi^2(\mu,\pi_{\beta})$ being at most $\kappa$. Inparticular, we consider nonparametric regression problems with randomsmoothness, and multi-index models with random smoothness as well as randomeffective dimension. We prove that a large Transformer pretrained on sufficientdata achieves the optimal rate of convergence corresponding to the difficultylevel $\beta$, uniformly over test distributions $\mu$ in the chi-squareddivergence ball. Thus, the pretrained Transformer is able to achieve fasterrates of convergence on easier tasks and is robust to distribution shift attest time. Finally, we prove that even if an estimator had access to the testdistribution $\mu$, the convergence rate of its expected risk over $\mu$ couldnot be faster than that of our pretrained Transformers, thereby providing amore appropriate optimality guarantee than minimax lower bounds.</description>
      <author>example@mail.com (Tianyi Ma, Tengyao Wang, Richard J. Samworth)</author>
      <guid isPermaLink="false">2510.23254v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Are ASR foundation models generalized enough to capture features of regional dialects for low-resource languages?</title>
      <link>http://arxiv.org/abs/2510.23252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript contains 11 pages, 5 tables and 16 figures This was  accepted at International Joint Conference on Natural Language Processing &amp;  Asia-Pacific Chapter of the Association for Computational Linguistics  (IJCNLP-AACL) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了名为Ben-10的孟加拉语音转文本语料库，研究了方言变化对自动语音识别(ASR)的影响，发现语音基础模型在区域方言ASR中表现不佳，但方言特定模型训练可缓解此问题。&lt;h4&gt;背景&lt;/h4&gt;传统语音识别研究大多使用标准形式处理低资源语言，而区域方言的自动语音识别(ASR)被视为微调任务。&lt;h4&gt;目的&lt;/h4&gt;研究方言变化对自动语音识别(ASR)的影响。&lt;h4&gt;方法&lt;/h4&gt;开发了一个78小时标注的孟加拉语音转文本(STT)语料库，命名为Ben-10，并从语言学和数据驱动角度进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;语音基础模型在区域方言ASR中表现不佳，无论是零样本还是微调设置；所有深度学习方法都难以在方言变化条件下建模语音数据，但方言特定的模型训练可以缓解这一问题。&lt;h4&gt;结论&lt;/h4&gt;该数据集可作为ASR算法在资源受限条件下建模的分布外(OOD)资源。&lt;h4&gt;翻译&lt;/h4&gt;传统语音识别建模研究大多依赖标准形式处理大多数低资源语言，而区域方言的自动语音识别(ASR)被视为微调任务。为研究对方言变化对ASR的影响，我们开发了一个名为Ben-10的78小时标注的孟加拉语音转文本(STT)语料库。从语言学和数据驱动角度的研究表明，语音基础模型在区域方言ASR中表现严重不佳，无论是在零样本还是微调设置下。我们观察到所有深度学习方法都难以在方言变化条件下建模语音数据，但方言特定的模型训练可以缓解这一问题。我们的数据集也可作为ASR算法在资源受限条件下建模的分布外(OOD)资源。该项目开发的数据集和代码已公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional research on speech recognition modeling relies on the canonicalform for most low-resource languages while automatic speech recognition (ASR)for regional dialects is treated as a fine-tuning task. To investigate theeffects of dialectal variations on ASR we develop a 78-hour annotated BengaliSpeech-to-Text (STT) corpus named Ben-10. Investigation from linguistic anddata-driven perspectives shows that speech foundation models struggle heavilyin regional dialect ASR, both in zero-shot and fine-tuned settings. We observethat all deep learning methods struggle to model speech data under dialectalvariations but dialect specific model training alleviates the issue. Ourdataset also serves as a out of-distribution (OOD) resource for ASR modelingunder constrained resources in ASR algorithms. The dataset and code developedfor this project are publicly available</description>
      <author>example@mail.com (Tawsif Tashwar Dipto, Azmol Hossain, Rubayet Sabbir Faruque, Md. Rezuwan Hassan, Kanij Fatema, Tanmoy Shome, Ruwad Naswan, Md. Foriduzzaman Zihad, Mohaymen Ul Anam, Nazia Tasnim, Hasan Mahmud, Md Kamrul Hasan, Md. Mehedi Hasan Shawon, Farig Sadeque, Tahsin Reasat)</author>
      <guid isPermaLink="false">2510.23252v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Finding 3D Scene Analogies with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to FM4RoboPlan workshop at RSS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比，通过混合神经表示实现复杂场景间的准确对应关系，支持轨迹和航路点转移。&lt;h4&gt;背景&lt;/h4&gt;将当前观察与先验经验连接有助于机器人在新3D环境中适应和规划。3D场景类比可作为平滑映射对齐具有共同空间关系的场景区域，支持轨迹或航路点转移，可用于模仿学习示范转移或跨场景任务规划。&lt;h4&gt;目的&lt;/h4&gt;提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比，避免现有方法需要的额外训练和固定物体词汇表限制。&lt;h4&gt;方法&lt;/h4&gt;采用混合神经表示场景，包括基于视觉语言模型特征的稀疏图和来自3D形状基础模型的特征场。通过粗到细方式寻找3D场景类比，首先对齐图，然后用特征场细化对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够建立复杂场景之间的准确对应关系，并成功应用于轨迹和航路点转移。&lt;h4&gt;结论&lt;/h4&gt;使用多模态基础模型可以在无需额外训练和固定词汇表的情况下实现3D场景类比，为机器人在新环境中的适应和规划提供了有效方法。&lt;h4&gt;翻译&lt;/h4&gt;将当前观察与先验经验连接起来有助于机器人在新的、未见过的3D环境中进行适应和规划。最近，3D场景类比被提出用于连接两个3D场景，这些是平滑的映射，能够对齐具有共同空间关系的场景区域。这些映射可以支持轨迹或航路点的详细转移，可能支持模仿学习的示范转移或跨场景的任务规划转移。然而，现有方法需要额外的训练和固定的物体词汇表。在本文中，我们提出使用多模态基础模型在零样本、开放词汇设置下寻找3D场景类比。我们方法的核心是场景的混合神经表示，包括基于视觉语言模型特征的稀疏图和来自3D形状基础模型的特征场。3D场景类比通过粗到细的方式找到，首先对齐图，然后使用特征场细化对应关系。我们的方法能够建立复杂场景之间的准确对应关系，我们展示了在轨迹和航路点转移中的应用。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决的是如何在没有额外训练和固定词汇表限制的情况下，找到3D场景之间的类比关系。这个问题很重要，因为它能帮助机器人将新环境与已知经验联系起来，从而在未知环境中更好地进行规划和行动。3D场景类比可以创建场景间的平滑映射，支持轨迹或路径点的转移，可用于模仿学习或跨场景的任务规划。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者注意到现有方法要么需要特定领域训练（神经描述符场），要么依赖语义标签（场景图匹配），限制了泛化能力。因此，作者转向利用已在大量多模态数据上训练的基础模型。方法借鉴了视觉语言模型（CLIP）提取对象特征、3D形状模型（PartField）构建特征场、图匹配技术、DBSCAN聚类和薄板样条拟合等现有技术，但将它们创新地组合成一个新的框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用多模态基础模型在零样本、开放词汇表设置下寻找3D场景类比，通过混合神经表示（稀疏图+密集特征场）实现从粗到细的场景类比估计。流程包括：1)构建场景图（对象节点+CLIP特征）和特征场（PartField特征）；2)图匹配获得粗粒度对象关联；3)DBSCAN聚类并拟合仿射映射；4)基于特征场优化局部位移；5)用薄板样条拟合得到最终映射。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)使用多模态基础模型实现零样本3D场景类比；2)提出混合神经表示方法结合稀疏图和密集场；3)采用从粗到细的估计策略；4)支持开放词汇表场景。相比之前工作，不同之处在于：无需特定领域训练（优于神经场景图方法）、不需要预知语义标签（优于场景图匹配方法）、能处理复杂场景且映射准确性更高。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于多模态基础模型的3D场景类比方法，通过结合视觉语言和3D形状特征的混合表示，实现了零样本、开放词汇表场景下的高精度场景映射，为机器人规划和模仿学习提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Connecting current observations with prior experiences helps robots adapt andplan in new, unseen 3D environments. Recently, 3D scene analogies have beenproposed to connect two 3D scenes, which are smooth maps that align sceneregions with common spatial relationships. These maps enable detailed transferof trajectories or waypoints, potentially supporting demonstration transfer forimitation learning or task plan transfer across scenes. However, existingmethods for the task require additional training and fixed object vocabularies.In this work, we propose to use multimodal foundation models for finding 3Dscene analogies in a zero-shot, open-vocabulary setting. Central to ourapproach is a hybrid neural representation of scenes that consists of a sparsegraph based on vision-language model features and a feature field derived from3D shape foundation models. 3D scene analogies are then found in acoarse-to-fine manner, by first aligning the graph and refining thecorrespondence with feature fields. Our method can establish accuratecorrespondences between complex scenes, and we showcase applications intrajectory and waypoint transfer.</description>
      <author>example@mail.com (Junho Kim, Young Min Kim)</author>
      <guid isPermaLink="false">2510.23184v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Guiding Skill Discovery with Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为Foundation model Guided (FoG)的技能发现方法，通过基础模型将人类意图融入技能发现过程，解决了现有方法只关注技能多样性而忽略人类偏好导致不理想行为的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的技能发现方法仅专注于最大化技能多样性，不考虑人类偏好，这会导致不理想甚至危险的行为。例如，使用先前方法训练的猎豹机器人学会向各个方向翻滚以最大化技能多样性，而非我们期望的奔跑行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将人类意图融入技能发现过程的方法，从而学习符合人类偏好的多样化技能，避免不理想和危险行为。&lt;h4&gt;方法&lt;/h4&gt;FoG方法从基础模型中提取评分函数，根据人类意图评估状态，为理想状态赋予更高值，不理想状态赋予更低值。然后使用这些分数重新加权技能发现算法的奖励，通过优化重新加权的奖励来指导技能发现过程。&lt;h4&gt;主要发现&lt;/h4&gt;FoG成功消除了不理想行为，如翻转或翻滚，并在基于状态和基于像素的任务中有效避免了危险区域。此外，该方法能够发现涉及难以定义的行为的技能。&lt;h4&gt;结论&lt;/h4&gt;FoG方法通过将人类意图融入技能发现过程，解决了现有方法只关注多样性而忽略人类偏好的问题，使强化学习能够学习更符合人类期望的技能，从而加速下游任务的强化学习过程。&lt;h4&gt;翻译&lt;/h4&gt;无需手工设计的奖励函数即可学习多样化技能，可以加速下游任务中的强化学习。然而，现有的技能发现方法只专注于最大化技能多样性，而没有考虑人类偏好，这会导致不理想的行为甚至危险技能。例如，使用先前方法训练的猎豹机器人学会向各个方向翻滚以最大化技能多样性，而我们更希望它能够奔跑而不翻转或进入危险区域。在这项工作中，我们提出了一种基础模型引导(FoG)的技能发现方法，通过基础模型将人类意图融入技能发现。具体来说，FoG从基础模型中提取评分函数，根据人类意图评估状态，为理想状态赋予更高值，为不理想状态赋予更低值。然后使用这些分数重新加权技能发现算法的奖励。通过优化重新加权的技能发现奖励，FoG成功消除了不理想行为，如翻转或翻滚，并在基于状态和基于像素的任务中避免了危险区域。有趣的是，我们表明FoG可以发现涉及难以定义的行为的技能。交互式可视化可通过https://sites.google.com/view/submission-fog获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning diverse skills without hand-crafted reward functions couldaccelerate reinforcement learning in downstream tasks. However, existing skilldiscovery methods focus solely on maximizing the diversity of skills withoutconsidering human preferences, which leads to undesirable behaviors andpossibly dangerous skills. For instance, a cheetah robot trained using previousmethods learns to roll in all directions to maximize skill diversity, whereaswe would prefer it to run without flipping or entering hazardous areas. In thiswork, we propose a Foundation model Guided (FoG) skill discovery method, whichincorporates human intentions into skill discovery through foundation models.Specifically, FoG extracts a score function from foundation models to evaluatestates based on human intentions, assigning higher values to desirable statesand lower to undesirable ones. These scores are then used to re-weight therewards of skill discovery algorithms. By optimizing the re-weighted skilldiscovery rewards, FoG successfully learns to eliminate undesirable behaviors,such as flipping or rolling, and to avoid hazardous areas in both state-basedand pixel-based tasks. Interestingly, we show that FoG can discover skillsinvolving behaviors that are difficult to define. Interactive visualisationsare available from https://sites.google.com/view/submission-fog.</description>
      <author>example@mail.com (Zhao Yang, Thomas M. Moerland, Mike Preuss, Aske Plaat, Vincent François-Lavet, Edward S. Hu)</author>
      <guid isPermaLink="false">2510.23167v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Modeling for Transferability Estimation of Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23145v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为隐式可迁移性建模(ITM)的新框架，用于评估预训练模型对下游任务的适用性，无需进行完整的微调过程。&lt;h4&gt;背景&lt;/h4&gt;可迁移性估计能够识别最适合下游任务的预训练模型，避免完整微调的高计算成本，促进模型部署和预训练-微调范式发展。然而，现有方法在评估具有多样化架构、训练策略和任务对齐的新兴预训练模型时，准确性有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确评估各种类型预训练模型可迁移性的新方法，使其能够在更广泛的模型和下游任务上实现泛化。&lt;h4&gt;方法&lt;/h4&gt;提出隐式可迁移性建模(ITM)框架，隐式建模每个模型的内在可迁移性，并结合分治变分近似(DVA)策略来有效近似嵌入空间的演化过程。&lt;h4&gt;主要发现&lt;/h4&gt;在涵盖多种训练策略和模型类型的综合基准测试中，ITM在稳定性、有效性和效率方面持续优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;ITM框架为评估新兴预训练模型的可迁移性提供了更准确、更高效的解决方案，有助于推动预训练和微调范式的发展。&lt;h4&gt;翻译&lt;/h4&gt;可迁移性估计能够识别最适合下游任务的预训练模型，而无需承担完整微调的高计算成本。这种能力促进了部署并推动了预训练和微调范式的发展。然而，现有方法在评估具有多样化架构、训练策略和任务对齐的新兴预训练模型的可迁移性时，往往难以准确评估。在这项工作中，我们提出了隐式可迁移性建模(ITM)，这是一个新颖的框架，它隐式地建模每个模型的内在可迁移性，并结合分治变分近似(DVA)策略来有效近似嵌入空间的演化。这种设计使模型能够在更广泛的模型和下游任务上实现泛化。在涵盖广泛训练策略和更多样化模型类型的综合基准上的大量实验表明，ITM在稳定性、有效性和效率方面持续优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferability estimation identifies the best pre-trained models fordownstream tasks without incurring the high computational cost of fullfine-tuning. This capability facilitates deployment and advances thepre-training and fine-tuning paradigm. However, existing methods often struggleto accurately assess transferability for emerging pre-trained models withdiverse architectures, training strategies, and task alignments. In this work,we propose Implicit Transferability Modeling (ITM), a novel framework thatimplicitly models each model's intrinsic transferability, coupled with aDivide-and-Conquer Variational Approximation (DVA) strategy to efficientlyapproximate embedding space evolution. This design enables generalizationacross a broader range of models and downstream tasks. Extensive experiments ona comprehensive benchmark--spanning extensive training regimes and a widervariety of model types--demonstrate that ITM consistently outperforms existingmethods in terms of stability, effectiveness, and efficiency.</description>
      <author>example@mail.com (Yaoyan Zheng, Huiqun Wang, Nan Zhou, Di Huang)</author>
      <guid isPermaLink="false">2510.23145v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>OmniDexGrasp: Generalizable Dexterous Grasping via Foundation Model and Force Feedback</title>
      <link>http://arxiv.org/abs/2510.23119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://isee-laboratory.github.io/OmniDexGrasp/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了OmniDexGrasp框架，结合基础模型与转移控制策略，实现了机器人根据人类命令灵巧抓取和操作物体的通用能力。&lt;h4&gt;背景&lt;/h4&gt;让机器人根据人类命令灵巧地抓取和操作物体是机器人学的一个有前景的方向，但现有方法由于语义灵巧抓取数据集规模有限，难以在不同物体或任务上泛化。&lt;h4&gt;目的&lt;/h4&gt;解决基础模型与物理机器人执行之间的差距问题，开发一个能在用户提示、灵巧抓取和抓取任务方面实现全能力的通用框架。&lt;h4&gt;方法&lt;/h4&gt;OmniDexGrasp集成了三个关键模块：使用基础模型生成人类抓取图像增强泛化能力；人类图像到机器人行动的转移策略实现全灵巧抓取；力感知自适应抓取策略确保稳健稳定的抓取执行。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实机器人上的实验验证了OmniDexGrasp在不同用户提示、抓取任务和灵巧手方面的有效性，且可扩展到灵巧操作任务。&lt;h4&gt;结论&lt;/h4&gt;OmniDexGrasp通过结合基础模型与转移控制策略，显著提升了机器人灵巧抓取和操作能力，具有广泛的适用性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;使机器人能够根据人类命令灵巧地抓取和操作物体是机器人学中的一个有前景的方向。然而，由于语义灵巧抓取数据集的规模有限，现有方法难以在多样化的物体或任务上泛化。基础模型提供了一种增强泛化的新方法，但由于抽象模型知识与物理机器人执行之间的差距，直接利用它们生成可行的机器人行动仍然具有挑战性。为了解决这些挑战，我们提出了OmniDexGrasp，一个通用框架，通过结合基础模型与转移和控制策略，在用户提示、灵巧抓取和抓取任务方面实现全能力。OmniDexGrasp集成了三个关键模块：(i) 使用基础模型生成人类抓取图像，增强泛化能力，支持用户提示和任务的全能力；(ii) 人类图像到机器人行动的转移策略将人类演示转化为可执行的机器人行动，实现全灵巧抓取；(iii) 力感知自适应抓取策略确保稳健和稳定的抓取执行。在模拟和真实机器人上的实验验证了OmniDexGrasp在不同用户提示、抓取任务和灵巧手方面的有效性，进一步的结果显示其可扩展到灵巧操作任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to dexterously grasp and manipulate objects based on humancommands is a promising direction in robotics. However, existing approaches arechallenging to generalize across diverse objects or tasks due to the limitedscale of semantic dexterous grasp datasets. Foundation models offer a new wayto enhance generalization, yet directly leveraging them to generate feasiblerobotic actions remains challenging due to the gap between abstract modelknowledge and physical robot execution. To address these challenges, we proposeOmniDexGrasp, a generalizable framework that achieves omni-capabilities in userprompting, dexterous embodiment, and grasping tasks by combining foundationmodels with the transfer and control strategies. OmniDexGrasp integrates threekey modules: (i) foundation models are used to enhance generalization bygenerating human grasp images supporting omni-capability of user prompt andtask; (ii) a human-image-to-robot-action transfer strategy converts humandemonstrations into executable robot actions, enabling omni dexterousembodiment; (iii) force-aware adaptive grasp strategy ensures robust and stablegrasp execution. Experiments in simulation and on real robots validate theeffectiveness of OmniDexGrasp on diverse user prompts, grasp task and dexteroushands, and further results show its extensibility to dexterous manipulationtasks.</description>
      <author>example@mail.com (Yi-Lin Wei, Zhexi Luo, Yuhao Lin, Mu Lin, Zhizhao Liang, Shuoyu Chen, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2510.23119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Eigenfunction Extraction for Ordered Representation Learning</title>
      <link>http://arxiv.org/abs/2510.24672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种通用框架，用于提取有序且可识别的特征函数，解决了现有表示学习方法只能恢复核的前几个特征函数线性张成空间的问题。&lt;h4&gt;背景&lt;/h4&gt;表示学习的最新进展表明，广泛使用的目标（如对比和非对比方法）隐式地对由输入与其上下文之间的关系诱导的上下文核执行谱分解。&lt;h4&gt;目的&lt;/h4&gt;开发一个能提取有序且可识别特征函数的通用框架，满足与上下文核兼容和可扩展到现代设置的需求。&lt;h4&gt;方法&lt;/h4&gt;展示低秩近似和瑞利商优化两种主要方法论范式如何与特征函数提取框架对齐，基于模块化构建块设计。&lt;h4&gt;主要发现&lt;/h4&gt;恢复的特征值可作为特征选择的有效重要性分数，通过自适应维度表示实现原则性的效率-准确性权衡。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在合成核和真实图像数据集上均得到验证，能够提供更精确的特征提取和理解，有助于特征选择和模型效率与准确性的平衡。&lt;h4&gt;翻译&lt;/h4&gt;表示学习的最新进展表明，广泛使用的目标（如对比和非对比方法）隐式地对由输入与其上下文之间的关系诱导的上下文核执行谱分解。然而，这些方法只能恢复核的前几个特征函数的线性张成空间，而精确的谱分解对于理解特征排序和重要性至关重要。在本研究中，我们提出一个通用框架来提取有序且可识别的特征函数，基于满足关键需求的模块化构建块设计，包括与上下文核的兼容性和可扩展到现代设置的能力。然后，我们展示了两种主要方法论范式（低秩近似和瑞利商优化）如何与这一特征函数提取框架对齐。最后，我们在合成核上验证了我们的方法，并在真实图像数据集上证明恢复的特征值可作为特征选择的有效重要性分数，通过自适应维度表示实现原则性的效率-准确性权衡。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in representation learning reveal that widely usedobjectives, such as contrastive and non-contrastive, implicitly performspectral decomposition of a contextual kernel, induced by the relationshipbetween inputs and their contexts. Yet, these methods recover only the linearspan of top eigenfunctions of the kernel, whereas exact spectral decompositionis essential for understanding feature ordering and importance. In this work,we propose a general framework to extract ordered and identifiableeigenfunctions, based on modular building blocks designed to satisfy keydesiderata, including compatibility with the contextual kernel and scalabilityto modern settings. We then show how two main methodological paradigms,low-rank approximation and Rayleigh quotient optimization, align with thisframework for eigenfunction extraction. Finally, we validate our approach onsynthetic kernels and demonstrate on real-world image datasets that therecovered eigenvalues act as effective importance scores for feature selection,enabling principled efficiency-accuracy tradeoffs via adaptive-dimensionalrepresentations.</description>
      <author>example@mail.com (Burak Varıcı, Che-Ping Tsai, Ritabrata Ray, Nicholas M. Boffi, Pradeep Ravikumar)</author>
      <guid isPermaLink="false">2510.24672v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Perception Learning: A Formal Separation of Sensory Representation Learning from Decision Learning</title>
      <link>http://arxiv.org/abs/2510.24356v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出感知学习(PeL)范式，使用任务无关信号优化智能体的感官接口，与下游决策学习解耦。PeL直接针对无标签的感知属性，通过表示不变的指标评估，形式化了感知与决策的分离，并证明PeL更新与贝叶斯任务风险梯度正交。&lt;h4&gt;背景&lt;/h4&gt;传统学习中感知和决策通常紧密耦合，限制了智能体发展通用感知能力，因为学习过于关注特定任务表现而忽略基本感知属性。&lt;h4&gt;目的&lt;/h4&gt;提出PeL范式解耦感知与决策；优化智能体感官接口；定义独立于任务目标的感知属性；提供评估感知质量的指标。&lt;h4&gt;方法&lt;/h4&gt;使用任务无关信号优化感官接口；将感知学习与下游决策学习解耦；针对稳定性、信息量、几何结构等感知属性优化；使用表示不变的指标评估；形式化感知与决策分离；证明PeL更新与贝叶斯任务风险梯度正交；提供任务无关评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;感知与决策可成功分离；可定义独立于目标或重新参数化的感知属性；保持不变量的PeL更新与贝叶斯任务风险梯度正交；提供有效评估指标认证感知质量。&lt;h4&gt;结论&lt;/h4&gt;PeL范式为智能体提供新感知学习框架，通过解耦感知与决策，发展更通用、鲁棒的感知能力，提高任务表现并增强环境理解和适应能力。&lt;h4&gt;翻译&lt;/h4&gt;我们引入感知学习(PeL)，一种使用任务无关信号优化智能体感官接口的范式，与下游决策学习解耦。PeL直接针对无标签的感知属性，如对扰动的稳定性、信息量而不崩溃、受控几何结构等，通过表示不变的指标进行评估。我们形式化了感知与决策的分离，定义了独立于目标或重新参数化的感知属性，并证明了保持足够不变量的PeL更新与贝叶斯任务风险梯度正交。此外，我们还提供了一套任务无关的评估指标来认证感知质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Perception Learning (PeL), a paradigm that optimizes an agent'ssensory interface $f_\phi:\mathcal{X}\to\mathcal{Z}$ using task-agnosticsignals, decoupled from downstream decision learning$g_\theta:\mathcal{Z}\to\mathcal{Y}$. PeL directly targets label-freeperceptual properties, such as stability to nuisances, informativeness withoutcollapse, and controlled geometry, assessed via objectiverepresentation-invariant metrics. We formalize the separation of perception anddecision, define perceptual properties independent of objectives orreparameterizations, and prove that PeL updates preserving sufficientinvariants are orthogonal to Bayes task-risk gradients. Additionally, weprovide a suite of task-agnostic evaluation metrics to certify perceptualquality.</description>
      <author>example@mail.com (Suman Sanyal)</author>
      <guid isPermaLink="false">2510.24356v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaRend: Learning 3D Dynamics via Masked Future Rendering for Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.24261v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出DynaRend框架，通过可微分体积渲染学习3D感知和动态信息的三平面特征，统一捕获空间几何、未来动态和任务语义，有效提升机器人操作任务的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;由于缺乏多样化的真实世界训练数据，学习可泛化的机器人操作策略仍面临挑战。现有方法要么依赖2D视觉预训练范式关注静态语义或几何，要么利用视频预测模型强调2D动态，无法同时学习操作所需的几何、语义和动态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够联合学习几何、语义和动态的表示学习框架，解决机器人操作任务中数据稀缺和泛化能力不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DynaRend表示学习框架，通过掩码重建和未来预测学习三平面特征，在多视图RGB-D视频数据上进行预训练，并通过动作价值图预测将学习到的表示转移到下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;在RLBench和Colosseum基准测试及真实世界实验中，DynaRend在策略成功率、环境扰动泛化能力和多样化任务实际适用性方面均有显著提升。&lt;h4&gt;结论&lt;/h4&gt;DynaRend成功解决了现有方法的局限性，能够有效联合学习几何、语义和动态信息，大幅提高机器人操作策略的泛化能力和实际应用效果。&lt;h4&gt;翻译&lt;/h4&gt;由于缺乏多样化的真实世界训练数据，学习可泛化的机器人操作策略仍然是一个关键挑战。尽管最近的方法尝试通过自监督表示学习来缓解这一问题，但大多数方法要么依赖于2D视觉预训练范式如掩码图像建模，主要关注静态语义或场景几何，要么利用大规模视频预测模型强调2D动态，因此无法有效操作所需的几何、语义和动态的联合学习。在本文中，我们提出DynaRend，一个表示学习框架，通过可微分体积渲染进行掩码重建和未来预测，学习3D感知和动态信息的三平面特征。通过在多视图RGB-D视频数据上预训练，DynaRend能够在统一的三平面表示中同时捕获空间几何、未来动态和任务语义。学习到的表示可以通过动作价值图预测有效地转移到下游机器人操作任务中。我们在两个具有挑战性的基准测试RLBench和Colosseum以及真实世界机器人实验中评估DynaRend，证明了其在策略成功率、对环境扰动的泛化能力以及在不同操作任务中的实际适用性方面都有显著改进。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作策略难以泛化的问题，原因是缺乏多样化的真实世界训练数据。现有方法要么依赖2D视觉预训练（关注静态语义或几何），要么使用视频预测模型（强调2D动态），无法同时学习有效操作所需的几何、语义和动态信息。这个问题很重要，因为机器人操作需要理解3D环境变化，真实世界数据收集成本高，且有效的操作需要综合理解场景结构、物体语义和动态变化。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：2D预训练缺乏3D感知，3D方法结构复杂难以扩展，渲染方法需要密集相机设置不实用。作者设计了一个统一框架，通过掩码重建和未来预测学习3D感知和动态信息的三平面特征。借鉴了掩码图像建模、神经渲染、视频预测和三平面表示等技术，但创新性地结合了这些方法，并引入目标视图增强技术提高真实世界适用性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过'掩码未来渲染'学习3D感知和动态信息的三平面特征，同时捕获场景几何、语义信息和未来动态变化。流程包括：1)从多视图RGB-D重建点云并投影为三平面特征；2)随机掩码部分特征，通过重建网络恢复当前场景，通过预测网络生成未来场景；3)对重建和预测结果进行体积渲染，用RGB、语义和深度损失进行监督；4)利用预训练模型合成新视图增强监督；5)预训练后添加动作解码器，在下游任务上微调预测动作值图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D表示学习框架，联合学习几何、动态和语义；2)掩码未来渲染技术，结合重建和预测两个互补目标；3)目标视图增强技术，减少对密集相机设置的依赖；4)多任务渲染损失，同时优化RGB、语义和深度。相比之前工作，DynaRend具有明确的3D空间感知能力，能捕获3D动态而非2D，使用更高效简洁的三平面表示，且通过目标视图增强提高了真实世界适用性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DynaRend通过掩码未来渲染技术，首次在统一的三平面表示中联合学习空间几何、未来动态和任务语义，显著提高了机器人操作策略的泛化能力和环境适应性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning generalizable robotic manipulation policies remains a key challengedue to the scarcity of diverse real-world training data. While recentapproaches have attempted to mitigate this through self-supervisedrepresentation learning, most either rely on 2D vision pretraining paradigmssuch as masked image modeling, which primarily focus on static semantics orscene geometry, or utilize large-scale video prediction models that emphasize2D dynamics, thus failing to jointly learn the geometry, semantics, anddynamics required for effective manipulation. In this paper, we presentDynaRend, a representation learning framework that learns 3D-aware anddynamics-informed triplane features via masked reconstruction and futureprediction using differentiable volumetric rendering. By pretraining onmulti-view RGB-D video data, DynaRend jointly captures spatial geometry, futuredynamics, and task semantics in a unified triplane representation. The learnedrepresentations can be effectively transferred to downstream roboticmanipulation tasks via action value map prediction. We evaluate DynaRend on twochallenging benchmarks, RLBench and Colosseum, as well as in real-world roboticexperiments, demonstrating substantial improvements in policy success rate,generalization to environmental perturbations, and real-world applicabilityacross diverse manipulation tasks.</description>
      <author>example@mail.com (Jingyi Tian, Le Wang, Sanping Zhou, Sen Wang, Jiayi Li, Gang Hua)</author>
      <guid isPermaLink="false">2510.24261v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Debiasing Reward Models by Representation Learning with Guarantees</title>
      <link>http://arxiv.org/abs/2510.23751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种减轻大型语言模型对齐过程中奖励模型偏见的新框架，通过识别和利用非虚假潜在变量来提高模型的稳健性。&lt;h4&gt;背景&lt;/h4&gt;近期基于人类反馈的强化学习等对齐技术被广泛采用，但这些模型常常利用虚假相关性，如响应长度、歧视性、奉承和概念偏见等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个有原则的框架，减轻奖励模型中的偏见，同时保留反映预期潜在偏好的因素。&lt;h4&gt;方法&lt;/h4&gt;提供数据生成过程的公式化，假设观测数据由虚假和非虚假潜在变量生成；使用变分推断来恢复这些变量并利用它们训练奖励模型。&lt;h4&gt;主要发现&lt;/h4&gt;非虚假潜在变量理论上可以从数据中识别出来，无论虚假潜在变量的代理变量是否可用。&lt;h4&gt;结论&lt;/h4&gt;在合成和真实世界数据集上的实验表明，该方法有效减轻了虚假相关性问题，并产生了更稳健的奖励模型。&lt;h4&gt;翻译&lt;/h4&gt;近期的对齐技术，如基于人类反馈的强化学习，已被广泛采用，通过学习和利用奖励模型使大型语言模型与人类偏好保持一致。在实践中，这些模型常常利用虚假相关性，例如响应长度、歧视性、奉承和概念偏见，这个问题已受到越来越多的关注。在这项工作中，我们提出了一个有原则的框架，在减轻奖励模型中这些偏见的同时，保留反映预期潜在偏好的因素。我们首先提供了数据生成过程的公式化，假设观测数据是由虚假和非虚假的潜在变量生成的。我们有趣地表明，这些非虚假的潜在变量理论上可以从数据中识别出来，无论虚假潜在变量的代理变量是否可用。这进一步启发了一种使用变分推断来恢复这些变量并利用它们训练奖励模型的实用方法。在合成和真实世界数据集上的实验表明，我们的方法有效减轻了虚假相关性问题，并产生了更稳健的奖励模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent alignment techniques, such as reinforcement learning from humanfeedback, have been widely adopted to align large language models with humanpreferences by learning and leveraging reward models. In practice, these modelsoften exploit spurious correlations, involving, e.g., response length,discrimination, sycophancy, and conceptual bias, which is a problem that hasreceived increasing attention. In this work, we propose a principled frameworkthat mitigates these biases in reward models while preserving the underlyingfactors that reflect intended preferences. We first provide a formulation ofthe data-generating process, assuming that the observed data (e.g., text) isgenerated from both spurious and non-spurious latent variables. We show that,interestingly, these non-spurious latent variables can be theoreticallyidentified from data, regardless of whether a surrogate for the spurious latentvariables is available. This further inspires a practical method that usesvariational inference to recover these variables and leverages them to trainreward models. Experiments on synthetic and real-world datasets demonstratethat our method effectively mitigates spurious correlation issues and yieldsmore robust reward models.</description>
      <author>example@mail.com (Ignavier Ng, Patrick Blöbaum, Siddharth Bhandari, Kun Zhang, Shiva Kasiviswanathan)</author>
      <guid isPermaLink="false">2510.23751v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Manifold Approximation leads to Robust Kernel Alignment</title>
      <link>http://arxiv.org/abs/2510.22953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures + supplementary&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的流形近似核对齐(MKA)方法，用于改进现有的中心化核对齐(CKA)度量方法，使其能够考虑底层流形几何，从而提供更稳健的表征测量基础。&lt;h4&gt;背景&lt;/h4&gt;中心化核对齐(CKA)是一种流行的度量方法，广泛应用于比较表征、确定网络等价性和神经科学研究。然而，CKA存在局限性，它没有考虑底层流形，并且依赖于许多启发式方法，导致在不同数据尺度下表现不一致。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够考虑流形几何的核对齐方法，以克服CKA的局限性，提供更稳健的表征测量基础。&lt;h4&gt;方法&lt;/h4&gt;提出流形近似核对齐(MKA)方法，将流形几何整合到对齐任务中，并推导了MKA的理论框架。在合成数据集和真实世界示例上进行经验评估，以表征和比较MKA及其当代方法。&lt;h4&gt;主要发现&lt;/h4&gt;考虑流形的核对齐为测量表征提供了更稳健的基础，在表征学习中有潜在应用价值。&lt;h4&gt;结论&lt;/h4&gt;MKA方法通过考虑底层流形，克服了CKA的局限性，为表征比较提供了更可靠的理论和实践基础。&lt;h4&gt;翻译&lt;/h4&gt;中心化核对齐(CKA)是一种流行的度量方法，用于比较表征、确定网络等价性和神经科学研究。然而，CKA没有考虑底层流形，并且依赖于许多启发式方法，导致它在不同数据尺度下表现不同。在这项工作中，我们提出了流形近似核对齐(MKA)，将流形几何整合到对齐任务中。我们推导了MKA的理论框架。我们在合成数据集和真实世界示例上进行经验评估，以表征和比较MKA及其当代方法。我们的研究结果表明，考虑流形的核对齐为测量表征提供了更稳健的基础，在表征学习中有潜在应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Centered kernel alignment (CKA) is a popular metric for comparingrepresentations, determining equivalence of networks, and neuroscienceresearch. However, CKA does not account for the underlying manifold and relieson numerous heuristics that cause it to behave differently at different scalesof data. In this work, we propose Manifold approximated Kernel Alignment (MKA),which incorporates manifold geometry into the alignment task. We derive atheoretical framework for MKA. We perform empirical evaluations on syntheticdatasets and real-world examples to characterize and compare MKA to itscontemporaries. Our findings suggest that manifold-aware kernel alignmentprovides a more robust foundation for measuring representations, with potentialapplications in representation learning.</description>
      <author>example@mail.com (Mohammad Tariqul Islam, Du Liu, Deblina Sarkar)</author>
      <guid isPermaLink="false">2510.22953v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Switchable Token-Specific Codebook Quantization For Face Image Compression</title>
      <link>http://arxiv.org/abs/2510.22943v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种可切换的特定token码本量化方法，用于面部图像压缩，通过为不同类别图像学习不同码本组并为每个token分配独立码本，提高了低比特率下的重建性能。&lt;h4&gt;背景&lt;/h4&gt;随着视觉数据量不断增加，高效无损的传输及其解释理解成为现代信息系统的关键瓶颈。现有的基于码本的解决方案使用全局共享码本，忽视了面部图像内部的类别相关性和token间的语义差异，导致低比特率下性能不佳。&lt;h4&gt;目的&lt;/h4&gt;解决全局码本策略在面部图像压缩中的局限性，特别是在低比特率(bpp)情况下的性能问题，提高重建图像的质量和识别准确率。&lt;h4&gt;方法&lt;/h4&gt;提出可切换的特定token码本量化方法，为不同图像类别学习不同的码本组，为每个token分配独立码本，并用少量比特记录每个token所属的码本组，以减少码本组减小时造成的损失。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用少量比特记录token所属的码本组，可以在较低总bpp下拥有更多码本总数，增强表达能力并提高重建性能。该方法在面部识别数据集上表现出色，0.05 bpp下重建图像的平均准确率达到93.51%。&lt;h4&gt;结论&lt;/h4&gt;所提出的可切换特定token码本量化方法有效解决了全局码本策略在面部图像压缩中的局限性，特别是在低比特率情况下，且具有良好的通用性，可集成到现有基于码本的表示学习方法中。&lt;h4&gt;翻译&lt;/h4&gt;随着视觉数据量的不断增加，高效无损的传输及其随后的解释理解已成为现代信息系统的关键瓶颈。新兴的基于码本的解决方案利用全局共享码本对每个token进行量化和反量化，通过调整token数量或码本大小来控制bpp。然而，对于富含属性的面部图像，这种全局码本策略忽视了图像内部的类别特定相关性以及token之间的语义差异，导致性能不佳，特别是在低bpp情况下。受这些观察的启发，我们提出了一种用于面部图像压缩的可切换特定token码本量化方法，该方法为不同图像类别学习不同的码本组，并为每个token分配独立的码本。通过用少量比特记录每个token所属的码本组，我们的方法可以减少减小每个码本组大小时造成的损失。这使得在较低的总bpp下可以拥有更多的码本总数，从而增强表达能力并提高重建性能。由于其通用设计，我们的方法可以集成到任何现有的基于码本的表示学习方法中，并在面部识别数据集上证明了其有效性，在0.05 bpp下重建图像的平均准确率达到93.51%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the ever-increasing volume of visual data, the efficient and losslesstransmission, along with its subsequent interpretation and understanding, hasbecome a critical bottleneck in modern information systems. The emergedcodebook-based solution utilize a globally shared codebook to quantize anddequantize each token, controlling the bpp by adjusting the number of tokens orthe codebook size. However, for facial images, which are rich in attributes,such global codebook strategies overlook both the category-specificcorrelations within images and the semantic differences among tokens, resultingin suboptimal performance, especially at low bpp. Motivated by theseobservations, we propose a Switchable Token-Specific Codebook Quantization forface image compression, which learns distinct codebook groups for differentimage categories and assigns an independent codebook to each token. Byrecording the codebook group to which each token belongs with a small number ofbits, our method can reduce the loss incurred when decreasing the size of eachcodebook group. This enables a larger total number of codebooks under a loweroverall bpp, thereby enhancing the expressive capability and improvingreconstruction performance. Owing to its generalizable design, our method canbe integrated into any existing codebook-based representation learning approachand has demonstrated its effectiveness on face recognition datasets, achievingan average accuracy of 93.51% for reconstructed images at 0.05 bpp.</description>
      <author>example@mail.com (Yongbo Wang, Haonan Wang, Guodong Mu, Ruixin Zhang, Jiaqi Chen, Jingyun Zhang, Jun Wang, Yuan Xie, Zhizhong Zhang, Shouhong Ding)</author>
      <guid isPermaLink="false">2510.22943v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Transformers from Compressed Representations</title>
      <link>http://arxiv.org/abs/2510.23665v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了TEMPEST方法，它利用压缩文件的字节流结构设计了一种有效的标记化和编码策略，使标准变换器可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完全媒体解码的需求。&lt;h4&gt;背景&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但它们在表示学习方面的潜力在很大程度上尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法，能够直接从压缩数据中学习语义表示，同时减少计算复杂性和内存使用。&lt;h4&gt;方法&lt;/h4&gt;TEMPEST（TransformErs froM comPressed rEpreSenTations）利用压缩文件固有的字节流结构设计有效的标记化和编码策略，使标准变换器可以直接从压缩数据流中学习语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;TEMPEST显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的大量实验，TEMPEST实现了与最先进技术相当的准确性，同时在内存和计算方面提供了效率提升。&lt;h4&gt;结论&lt;/h4&gt;TEMPEST是一种有效的方法，可以直接从压缩数据中学习语义表示，同时保持高准确性和提高效率。&lt;h4&gt;翻译&lt;/h4&gt;压缩文件格式是高效数据存储和传输的基石，但它们在表示学习方面的潜力在很大程度上尚未被探索。我们介绍了TEMPEST（来自压缩表示的变换器），这是一种利用压缩文件固有字节流结构设计有效标记化和编码策略的方法。通过利用这种紧凑编码，标准变换器可以直接从压缩数据流中学习语义表示，绕过原始字节级处理或完全媒体解码的需求。我们的提议显著减少了语义分类所需的标记数量，从而降低了计算复杂性和内存使用。通过在不同数据集、编码方案和模态上的大量实验，我们表明TEMPEST实现了与最先进技术相当的准确性，同时在内存和计算方面提供了效率提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compressed file formats are the corner stone of efficient data storage andtransmission, yet their potential for representation learning remains largelyunderexplored. We introduce TEMPEST (TransformErs froM comPressedrEpreSenTations), a method that exploits the inherent byte-stream structure ofcompressed files to design an effective tokenization and encoding strategy. Byleveraging this compact encoding, a standard transformer can directly learnsemantic representations from compressed data streams, bypassing the need forraw byte-level processing or full media decoding. Our proposal substantiallyreduces the number of tokens required for semantic classification, therebylowering both computational complexity and memory usage. Through extensiveexperiments across diverse datasets, coding schemes, and modalities, we showthat TEMPEST achieves accuracy competitive wit the state-of-the-art whiledelivering efficiency gains in memory and compute.</description>
      <author>example@mail.com (Juan C. Leon Alcazar, Mattia Soldan, Mohammad Saatialsoruji, Alejandro Pardo, Hani Itani, Juan Camilo Perez, Bernard Ghanem)</author>
      <guid isPermaLink="false">2510.23665v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Learning Without Augmenting: Unsupervised Time Series Representation Learning via Frame Projections</title>
      <link>http://arxiv.org/abs/2510.22655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at the Conference on Neural Information Processing Systems  (NeurIPS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用正交基和过完备帧替代传统数据增强的自监督学习方法，通过联合利用不同流形的互补几何特性，在多个时间序列任务上实现了高达15-20%的性能提升。&lt;h4&gt;背景&lt;/h4&gt;自监督学习(SSL)是一种无需标记数据即可学习表征的强大范式，但大多数SSL方法依赖于强大、成熟、手工制作的数据增强技术，这需要领域特定知识并可能限制模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种无监督表征学习方法，用正交基和过完备帧生成视图来替代传统的数据增强技术，避免其带来的限制。&lt;h4&gt;方法&lt;/h4&gt;使用正交基和过完备帧生成视图替代传统数据增强，联合利用从正交和过完备空间学习到的不同流形上的互补几何特性。&lt;h4&gt;主要发现&lt;/h4&gt;从正交和过完备空间学习到的嵌入位于由不同空间中表示样本所引入的几何偏差形成的不同流形上，通过联合利用这些不同流形的互补几何特性，可以在不通过强增强人为增加数据多样性的情况下实现优越性能。&lt;h4&gt;结论&lt;/h4&gt;在五个时间序列任务上的九个数据集上证明了该方法的有效性，在这些信号特性使得数据增强具有挑战性的任务上，不依赖于增强引起的多样性，实现了高达15-20%的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习(SSL)已成为一种无需标记数据即可学习表征的强大范式。大多数SSL方法依赖于强大、成熟、手工制作的数据增强来为表征学习生成多样化视图。然而，设计此类增强需要领域特定知识，并隐式地对模型施加表征不变性，这可能限制泛化能力。在这项工作中，我们提出了一种无监督表征学习方法，使用正交基和过完备帧生成视图来替代数据增强。我们表明，从正交和过完备空间学习到的嵌入位于不同的流形上，这些流形由在不同空间中表示样本所引入的几何偏差形成。通过联合利用这些不同流形的互补几何，我们的方法在不通过强增强人为增加数据多样性的情况下实现了优越性能。我们在五个时间序列任务上的九个数据集上证明了该方法的有效性，在这些任务中，信号特定特性使得数据增强特别具有挑战性。在不依赖于增强引起的多样性的情况下，我们的方法相比现有自监督方法实现了高达15-20%的性能提升。源代码：https://github.com/eth-siplab/Learning-with-FrameProjections&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning (SSL) has emerged as a powerful paradigm forlearning representations without labeled data. Most SSL approaches rely onstrong, well-established, handcrafted data augmentations to generate diverseviews for representation learning. However, designing such augmentationsrequires domain-specific knowledge and implicitly imposes representationalinvariances on the model, which can limit generalization. In this work, wepropose an unsupervised representation learning method that replacesaugmentations by generating views using orthonormal bases and overcompleteframes. We show that embeddings learned from orthonormal and overcompletespaces reside on distinct manifolds, shaped by the geometric biases introducedby representing samples in different spaces. By jointly leveraging thecomplementary geometry of these distinct manifolds, our approach achievessuperior performance without artificially increasing data diversity throughstrong augmentations. We demonstrate the effectiveness of our method on ninedatasets across five temporal sequence tasks, where signal-specificcharacteristics make data augmentations particularly challenging. Withoutrelying on augmentation-induced diversity, our method achieves performancegains of up to 15--20\% over existing self-supervised approaches. Source code:https://github.com/eth-siplab/Learning-with-FrameProjections</description>
      <author>example@mail.com (Berken Utku Demirel, Christian Holz)</author>
      <guid isPermaLink="false">2510.22655v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaCausal: Dynamic Causality-Aware Root Cause Analysis for Distributed Microservices</title>
      <link>http://arxiv.org/abs/2510.22613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DynaCausal是一种动态因果感知框架，用于分布式微服务系统的根本原因分析，通过统一多模态动态信号和动态对比机制，有效解决了级联故障传播建模不足、噪声干扰和概念漂移以及过度依赖服务偏离强度等挑战。&lt;h4&gt;背景&lt;/h4&gt;云原生微服务支持快速迭代和可扩展部署，但创建了复杂且快速演化的依赖关系，对可靠诊断构成挑战。现有的根本原因分析方法在捕捉动态行为和变化的服务关系方面有限。&lt;h4&gt;目的&lt;/h4&gt;解决三个关键挑战：级联故障传播建模不足、噪声干扰和概念漂移影响、以及过度依赖服务偏离强度掩盖真正根本原因的问题。&lt;h4&gt;方法&lt;/h4&gt;提出DynaCausal框架，统一多模态动态信号通过交互感知表征学习捕捉时空依赖关系，引入动态对比机制分离故障指标与噪声，采用因果优先的成对排序目标优化因果归因。&lt;h4&gt;主要发现&lt;/h4&gt;在公共基准上的评估显示，DynaCausal持续超越最先进方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。&lt;h4&gt;结论&lt;/h4&gt;DynaCausal通过动态因果感知的方法有效解决了微服务系统故障诊断中的关键挑战，实现了更准确和可解释的根本原因分析。&lt;h4&gt;翻译&lt;/h4&gt;云原生微服务支持快速迭代和可扩展部署，但也创建了复杂且快速演化的依赖关系，对可靠诊断构成挑战。现有的根本原因分析方法，即使融合了日志、追踪和指标等多模态数据，在捕捉动态行为和变化的服务关系方面仍然有限。三个关键挑战仍然存在：(i)级联故障传播建模不足，(ii)容易受到正常服务行为中噪声干扰和概念漂移的影响，(iii)过度依赖服务偏离强度掩盖了真正的根本原因。为解决这些挑战，我们提出了DynaCausal，一种用于分布式微服务系统RCA的动态因果感知框架。DynaCausal统一多模态动态信号，通过交互感知的表征学习捕捉时空依赖关系。它进一步引入了动态对比机制，将真正的故障指标与上下文噪声分离，并采用因果优先的成对排序目标，明确优化因果归因。在公共基准上的全面评估表明，DynaCausal持续超越最先进的方法，平均AC@1达到0.63，绝对增益从0.25到0.46，在高度动态的微服务环境中提供准确且可解释的诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cloud-native microservices enable rapid iteration and scalable deployment butalso create complex, fast-evolving dependencies that challenge reliablediagnosis. Existing root cause analysis (RCA) approaches, even with multi-modalfusion of logs, traces, and metrics, remain limited in capturing dynamicbehaviors and shifting service relationships. Three critical challengespersist: (i) inadequate modeling of cascading fault propagation, (ii)vulnerability to noise interference and concept drift in normal servicebehavior, and (iii) over-reliance on service deviation intensity that obscurestrue root causes. To address these challenges, we propose DynaCausal, a dynamiccausality-aware framework for RCA in distributed microservice systems.DynaCausal unifies multi-modal dynamic signals to capture time-varyingspatio-temporal dependencies through interaction-aware representation learning.It further introduces a dynamic contrastive mechanism to disentangle true faultindicators from contextual noise and adopts a causal-prioritized pairwiseranking objective to explicitly optimize causal attribution. Comprehensiveevaluations on public benchmarks demonstrate that DynaCausal consistentlysurpasses state-of-the-art methods, attaining an average AC@1 of 0.63 withabsolute gains from 0.25 to 0.46, and delivering both accurate andinterpretable diagnoses in highly dynamic microservice environments.</description>
      <author>example@mail.com (Songhan Zhang, Aoyang Fang, Yifan Yang, Ruiyi Cheng, Xiaoying Tang, Pinjia He)</author>
      <guid isPermaLink="false">2510.22613v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Random Search Neural Networks for Efficient and Expressive Graph Learning</title>
      <link>http://arxiv.org/abs/2510.22520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NEURIPS 2025; version with full appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的随机搜索神经网络(RSNNs)方法，用于解决随机游走神经网络(RWNNs)在图表示学习中的局限性。RSNNs通过保证完全节点覆盖的随机搜索，显著降低了采样复杂度，并在理论和实验上都表现出优越性能。&lt;h4&gt;背景&lt;/h4&gt;随机游走神经网络(RWNNs)已成为图表示学习的一种有前景的方法，利用序列模型处理随机游走。然而，在现实采样约束下，RWNNs往往无法捕捉全局结构，即使在小型图中也是如此，这是因为节点和边覆盖不完整，限制了其表达能力。&lt;h4&gt;目的&lt;/h4&gt;解决RWNNs在捕捉全局结构方面的局限性，提高图表示学习的效率和表达能力，特别是在稀疏图上的表现。&lt;h4&gt;方法&lt;/h4&gt;提出随机搜索神经网络(RSNNs)，在保证完全节点覆盖的随机搜索上运行。理论上证明在稀疏图中，只需O(log |V|)次搜索即可实现完全边覆盖，显著低于RWNNs所需的O(|V|)次游走。RSNNs与通用序列模型配对时是通用近似器，且对图同构具有概率不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在稀疏图中，RSNNs只需O(log |V|)次搜索就能实现完全边覆盖，比RWNNs的O(|V|)次游走大幅降低采样复杂度。RSNNs是通用近似器，且对图同构具有概率不变性。实验表明，RSNNs在分子和蛋白质基准测试中持续优于RWNNs，使用最多减少16倍的采样序列就能达到相当或更好的性能。&lt;h4&gt;结论&lt;/h4&gt;RSNNs弥合了基于随机游走方法在理论和实践上的差距，为稀疏图上的学习提供了一种高效且具有表达力的框架，在保持高性能的同时显著降低了计算复杂度。&lt;h4&gt;翻译&lt;/h4&gt;随机游走神经网络(RWNNs)已成为图表示学习的一种有前景的方法，利用序列模型的最新进展来处理随机游走。然而，在现实采样约束下，RWNNs往往无法捕捉全局结构，即使在小型图中也是如此，这是由于节点和边覆盖不完整，限制了其表达能力。为解决这一问题，我们提出了随机搜索神经网络(RSNNs)，它在随机搜索上运行，每个搜索都能保证完全的节点覆盖。理论上，我们证明了在稀疏图中，只需O(log |V|)次搜索就能实现完全边覆盖，与RWNNs所需的O(|V|)次游走相比，显著降低了采样复杂度（假设游走长度随图大小缩放）。此外，当与通用序列模型配对时，RSNNs是通用近似器。最后，我们证明了RSNNs对图同构具有概率不变性，确保其期望是同构不变的图函数。实验上，RSNNs在分子和蛋白质基准测试中持续优于RWNNs，使用最多减少16倍的采样序列就能实现相当或更好的性能。我们的工作弥合了基于随机游走方法在理论和实践上的进展，为在稀疏图上的学习提供了一种高效且具有表达力的框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Random walk neural networks (RWNNs) have emerged as a promising approach forgraph representation learning, leveraging recent advances in sequence models toprocess random walks. However, under realistic sampling constraints, RWNNsoften fail to capture global structure even in small graphs due to incompletenode and edge coverage, limiting their expressivity. To address this, wepropose \textit{random search neural networks} (RSNNs), which operate on randomsearches, each of which guarantees full node coverage. Theoretically, wedemonstrate that in sparse graphs, only $O(\log |V|)$ searches are needed toachieve full edge coverage, substantially reducing sampling complexity comparedto the $O(|V|)$ walks required by RWNNs (assuming walk lengths scale with graphsize). Furthermore, when paired with universal sequence models, RSNNs areuniversal approximators. We lastly show RSNNs are probabilistically invariantto graph isomorphisms, ensuring their expectation is an isomorphism-invariantgraph function. Empirically, RSNNs consistently outperform RWNNs on molecularand protein benchmarks, achieving comparable or superior performance with up to16$\times$ fewer sampled sequences. Our work bridges theoretical and practicaladvances in random walk based approaches, offering an efficient and expressiveframework for learning on sparse graphs.</description>
      <author>example@mail.com (Michael Ito, Danai Koutra, Jenna Wiens)</author>
      <guid isPermaLink="false">2510.22520v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MAGIC-Flow: Multiscale Adaptive Conditional Flows for Generation and Interpretable Classification</title>
      <link>http://arxiv.org/abs/2510.22070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内同时执行生成和分类任务，解决了生成式建模在医学影像等困难领域应用时缺乏任务对齐的问题。&lt;h4&gt;背景&lt;/h4&gt;生成式建模已成为表示学习的强大范式，但其在医学影像等困难领域的直接应用仍然有限，仅进行生成而不考虑任务对齐，无法为临床应用提供稳健的基础。&lt;h4&gt;目的&lt;/h4&gt;提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内执行生成和分类，以解决生成式建模在临床应用中的局限性。&lt;h4&gt;方法&lt;/h4&gt;MAGIC-Flow构建为可逆和可微双射的层次结构，其中雅可比行列式在子变换中因子化，确保了似然的精确计算和稳定的优化。通过基于类标签的条件化，支持可控样本合成和原则性类概率估计，同时可逆性使得样本似然的显式可视化成为可能，为模型推理提供了可解释的视角。&lt;h4&gt;主要发现&lt;/h4&gt;MAGIC-Flow在相似性、保真度和多样性指标上与顶级基线相当，在多个数据集上解决了扫描噪声下的生成和分类问题，以及模态特定的合成和识别。结果显示MAGIC-Flow创建了真实、多样的样本并改进了分类性能。&lt;h4&gt;结论&lt;/h4&gt;MAGIC-Flow是数据有限领域中生成和分类的有效策略，对隐私保护增强、鲁棒泛化和可信医疗AI有直接益处。&lt;h4&gt;翻译&lt;/h4&gt;生成式建模已成为表示学习的强大范式，但其在医学影像等困难领域的直接应用仍然有限：仅进行生成而不考虑任务对齐，无法为临床应用提供稳健的基础。我们提出MAGIC-Flow，一种条件多尺度归一化流架构，在单一模块化框架内执行生成和分类。该模型构建为可逆和可微双射的层次结构，其中雅可比行列式在子变换中因子化。我们展示了这如何确保似然的精确计算和稳定的优化，同时可逆性使得样本似然的显式可视化成为可能，为模型推理提供了可解释的视角。通过基于类标签的条件化，MAGIC-Flow支持可控样本合成和原则性类概率估计，有效地辅助生成性和判别性目标。我们使用相似性、保真度和多样性指标将MAGIC-Flow与顶级基线进行比较。在多个数据集上，它解决了扫描噪声下的生成和分类，以及模态特定的合成和识别。结果表明MAGIC-Flow创建了真实、多样的样本并改进了分类。MAGIC-Flow是数据有限领域中生成和分类的有效策略，对隐私保护增强、鲁棒泛化和可信医疗AI有直接益处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative modeling has emerged as a powerful paradigm for representationlearning, but its direct applicability to challenging fields like medicalimaging remains limited: mere generation, without task alignment, fails toprovide a robust foundation for clinical use. We propose MAGIC-Flow, aconditional multiscale normalizing flow architecture that performs generationand classification within a single modular framework. The model is built as ahierarchy of invertible and differentiable bijections, where the Jacobiandeterminant factorizes across sub-transformations. We show how this ensuresexact likelihood computation and stable optimization, while invertibilityenables explicit visualization of sample likelihoods, providing aninterpretable lens into the model's reasoning. By conditioning on class labels,MAGIC-Flow supports controllable sample synthesis and principledclass-probability estimation, effectively aiding both generative anddiscriminative objectives. We evaluate MAGIC-Flow against top baselines usingmetrics for similarity, fidelity, and diversity. Across multiple datasets, itaddresses generation and classification under scanner noise, andmodality-specific synthesis and identification. Results show MAGIC-Flow createsrealistic, diverse samples and improves classification. MAGIC-Flow is aneffective strategy for generation and classification in data-limited domains,with direct benefits for privacy-preserving augmentation, robustgeneralization, and trustworthy medical AI.</description>
      <author>example@mail.com (Luca Caldera, Giacomo Bottacini, Lara Cavinato)</author>
      <guid isPermaLink="false">2510.22070v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Predictive Coding Enhances Meta-RL To Achieve Interpretable Bayes-Optimal Belief Representation Under Partial Observability</title>
      <link>http://arxiv.org/abs/2510.22039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to Annual Conference on Neural Information Processing  Systems (NeurIPS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了在部分可观测环境中，通过整合预测编码模块到元强化学习中，以学习更紧凑、可解释的贝叶斯最优表示，从而提高代理的适应性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在部分可观测环境中，学习历史信息的紧凑表示对规划和泛化至关重要。现有元强化学习代理虽能接近贝叶斯最优策略，但往往无法学习到紧凑且可解释的贝叶斯最优信念状态，这种表示效率低下限制了代理的适应性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;研究将自监督预测编码模块整合到元强化学习中，是否能够促进贝叶斯最优表示的学习，从而提高代理在部分可观测环境中的表现。&lt;h4&gt;方法&lt;/h4&gt;受神经科学中预测编码和深度强化学习中辅助预测目标的启发，作者将预测编码模块整合到元强化学习框架中，并通过状态机模拟进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;带有预测模块的元强化学习能够生成更可解释的表示，更好地近似贝叶斯最优信念状态；在需要主动信息获取的挑战性任务中，只有带有预测模块的元强化学习成功学习了最优表示和策略；更好的表示学习能够提高泛化能力。&lt;h4&gt;结论&lt;/h4&gt;预测学习作为代理在部分可观测环境中有效表示学习的指导原则具有重要价值，能够显著提升代理的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;学习历史信息的紧凑表示对于部分可观测环境中的规划和泛化至关重要。虽然元强化学习代理能够获得接近贝叶斯最优的策略，但它们往往无法学习到紧凑、可解释的贝叶斯最优信念状态。这种表示效率低下可能限制了代理的适应性和泛化能力。受神经科学中预测编码的启发——它表明大脑预测感觉输入是贝叶斯推断的神经实现——以及深度强化学习中的辅助预测目标，我们研究了将自监督预测编码模块整合到元强化学习中是否有助于学习贝叶斯最优表示。通过状态机模拟，我们表明带有预测模块的元强化学习能够生成更可解释的表示，更好地近似贝叶斯最优信念状态，与传统的元强化学习相比，在多种任务中表现一致，即使两者都实现了最优策略。在需要主动信息获取的挑战性任务中，只有带有预测模块的元强化学习成功学习了最优表示和策略，而传统元强化学习在表示学习方面表现不足。最后，我们证明了更好的表示学习能够提高泛化能力。我们的结果强烈表明，预测学习作为代理在部分可观测环境中有效表示学习的指导原则具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning a compact representation of history is critical for planning andgeneralization in partially observable environments. While meta-reinforcementlearning (RL) agents can attain near Bayes-optimal policies, they often fail tolearn the compact, interpretable Bayes-optimal belief states. Thisrepresentational inefficiency potentially limits the agent's adaptability andgeneralization capacity. Inspired by predictive coding in neuroscience--whichsuggests that the brain predicts sensory inputs as a neural implementation ofBayesian inference--and by auxiliary predictive objectives in deep RL, weinvestigate whether integrating self-supervised predictive coding modules intometa-RL can facilitate learning of Bayes-optimal representations. Through statemachine simulation, we show that meta-RL with predictive modules consistentlygenerates more interpretable representations that better approximateBayes-optimal belief states compared to conventional meta-RL across a widevariety of tasks, even when both achieve optimal policies. In challenging tasksrequiring active information seeking, only meta-RL with predictive modulessuccessfully learns optimal representations and policies, whereas conventionalmeta-RL struggles with inadequate representation learning. Finally, wedemonstrate that better representation learning leads to improvedgeneralization. Our results strongly suggest the role of predictive learning asa guiding principle for effective representation learning in agents navigatingpartial observability.</description>
      <author>example@mail.com (Po-Chen Kuo, Han Hou, Will Dabney, Edgar Y. Walker)</author>
      <guid isPermaLink="false">2510.22039v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Orbital Minimization Method for Neural Operator Decomposition</title>
      <link>http://arxiv.org/abs/2510.21952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 8 figures. To appear at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究重新审视了轨道最小化方法(OMM)，这是一种来自计算物理学的经典优化框架，用于训练神经网络分解线性算子，展示了其在现代机器学习中的实用性和优势。&lt;h4&gt;背景&lt;/h4&gt;谱分解线性算子在机器学习和科学计算中扮演核心角色。近期工作探索了训练神经网络来近似这些算子的特征函数，为表示学习、动力系统和偏微分方程提供了可扩展的方法。&lt;h4&gt;目的&lt;/h4&gt;证明轨道最小化方法(OMM)在现代学习流程中的更广泛应用性，并将其调整为训练神经网络分解正半定算子的框架。&lt;h4&gt;方法&lt;/h4&gt;重新审视轨道最小化方法(OMM)，提供OMM目标一致性的简单线性代数证明，揭示此方法与其他领域独立出现的思想之间的联系，并调整该框架用于训练神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;轨道最小化方法在一系列基准任务中展示了实际优势，通过现代理论和计算的角度重新审视经典数值方法，可以为数值模拟中部署神经网络提供原则性方法，同时为机器学习提供有效且可扩展的工具。&lt;h4&gt;结论&lt;/h4&gt;重新审视经典数值方法可以为机器学习和科学计算提供新的视角和实用工具，扩展了神经网络在数值模拟和机器学习中的应用范围。&lt;h4&gt;翻译&lt;/h4&gt;线性算子的谱分解在机器学习和科学计算的许多领域中扮演着核心角色。近期工作探索了训练神经网络来近似这些算子的特征函数，使表示学习、动力系统和偏微分方程(PDEs)的方法具有可扩展性。在本文中，我们重新审视了来自计算物理学文献的一个经典优化框架，称为轨道最小化方法(OMM)，最初在1990年代提出用于计算化学中的特征值问题。我们提供了OMM目标一致性的简单线性代数证明，并揭示了此方法与不同领域独立出现的几种思想之间的联系。我们的主要目标是证明它在现代学习流程中的更广泛应用性。我们将这一框架调整为训练神经网络来分解正半定算子，并在一系列基准任务中展示了其实际优势。我们的结果强调了如何通过现代理论和计算的角度重新审视经典数值方法，不仅可以为在数值模拟中部署神经网络提供原则性方法，还可以为机器学习提供有效且可扩展的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spectral decomposition of linear operators plays a central role in many areasof machine learning and scientific computing. Recent work has explored trainingneural networks to approximate eigenfunctions of such operators, enablingscalable approaches to representation learning, dynamical systems, and partialdifferential equations (PDEs). In this paper, we revisit a classicaloptimization framework from the computational physics literature known as the\emph{orbital minimization method} (OMM), originally proposed in the 1990s forsolving eigenvalue problems in computational chemistry. We provide a simplelinear-algebraic proof of the consistency of the OMM objective, and revealconnections between this method and several ideas that have appearedindependently across different domains. Our primary goal is to justify itsbroader applicability in modern learning pipelines. We adapt this framework totrain neural networks to decompose positive semidefinite operators, anddemonstrate its practical advantages across a range of benchmark tasks. Ourresults highlight how revisiting classical numerical methods through the lensof modern theory and computation can provide not only a principled approach fordeploying neural networks in numerical simulation, but also effective andscalable tools for machine learning.</description>
      <author>example@mail.com (J. Jon Ryu, Samuel Zhou, Gregory W. Wornell)</author>
      <guid isPermaLink="false">2510.21952v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Structure-Aware Fusion with Progressive Injection for Multimodal Molecular Representation Learning</title>
      <link>http://arxiv.org/abs/2510.23640v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MuMo，一种结构化多模态融合框架，解决了分子表示中的3D构象不稳定性和模态崩溃问题，提高了模型的鲁棒性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;多模态分子模型通常受到3D构象不可靠性和模态崩溃的限制，这影响了它们的鲁棒性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;设计MuMo框架，解决分子表示中的3D构象不稳定性和模态崩溃问题，提高分子表示的质量。&lt;h4&gt;方法&lt;/h4&gt;1) 设计结构化融合管道（SFP），将2D拓扑和3D几何结合成统一且稳定的结构先验；2) 引入渐进注入（PI）机制，非对称地将先验整合到序列流中，保留模态特定建模同时实现跨模态增强；3) 基于状态空间主干构建，支持长程依赖建模和鲁棒信息传播。&lt;h4&gt;主要发现&lt;/h4&gt;在TDC和MoleculeNet的29个基准任务上，MuMo平均比最佳基线提高2.7%，在22个任务中排名第一，包括在LD50任务上提高27%，验证了模型对3D构象噪声的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MuMo框架通过结构化多模态融合有效解决了分子表示中的3D构象不稳定性和模态崩溃问题，在各种任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;多模态分子模型通常受到3D构象不可靠性和模态崩溃的限制，限制了它们的鲁棒性和泛化能力。我们提出了MuMo，一种结构化多模态融合框架，通过两个关键策略解决了分子表示中的这些挑战。为了减少构象依赖融合的不稳定性，我们设计了一个结构化融合管道（SFP），将2D拓扑和3D几何结合成统一且稳定的结构先验。为了缓解朴素融合引起的模态崩溃，我们引入了渐进注入（PI）机制，非对称地将此先验整合到序列流中，同时保留模态特定建模并实现跨模态增强。基于状态空间主干构建，MuMo支持长程依赖建模和鲁棒信息传播。在来自Therapeutics Data Commons（TDC）和MoleculeNet的29个基准任务上，MuMo在每个任务上平均比最佳基线提高2.7%，在22个任务中排名第一，包括在LD50任务上提高27%。这些结果验证了它对3D构象噪声的鲁棒性以及多模态融合在分子表示中的有效性。代码可在github.com/selmiss/MuMo获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal molecular models often suffer from 3D conformer unreliability andmodality collapse, limiting their robustness and generalization. We proposeMuMo, a structured multimodal fusion framework that addresses these challengesin molecular representation through two key strategies. To reduce theinstability of conformer-dependent fusion, we design a Structured FusionPipeline (SFP) that combines 2D topology and 3D geometry into a unified andstable structural prior. To mitigate modality collapse caused by naive fusion,we introduce a Progressive Injection (PI) mechanism that asymmetricallyintegrates this prior into the sequence stream, preserving modality-specificmodeling while enabling cross-modal enrichment. Built on a state spacebackbone, MuMo supports long-range dependency modeling and robust informationpropagation. Across 29 benchmark tasks from Therapeutics Data Commons (TDC) andMoleculeNet, MuMo achieves an average improvement of 2.7% over thebest-performing baseline on each task, ranking first on 22 of them, including a27% improvement on the LD50 task. These results validate its robustness to 3Dconformer noise and the effectiveness of multimodal fusion in molecularrepresentation. The code is available at: github.com/selmiss/MuMo.</description>
      <author>example@mail.com (Zihao Jing, Yan Sun, Yan Yi Li, Sugitha Janarthanan, Alana Deng, Pingzhao Hu)</author>
      <guid isPermaLink="false">2510.23640v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models in Dermatopathology: Skin Tissue Classification</title>
      <link>http://arxiv.org/abs/2510.21664v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究比较了两种基础模型(UNI和Virchow2)在皮肤病理学全切片图像分类中的表现，发现Virchow2特征提取器表现略优，使用逻辑回归分类器可达到90%的准确率。研究还探索了数据增强和图像归一化技术，并证实平均聚合方法能有效生成切片级特征表示。&lt;h4&gt;背景&lt;/h4&gt;皮肤病理学中全切片图像(WSI)的快速生成需要自动化方法进行高效处理和准确分类。&lt;h4&gt;目的&lt;/h4&gt;评估两种基础模型(UNI和Virchow2)作为特征提取器，用于将WSI分类为三种诊断类别：黑素细胞性、基底样性和鳞状病变。&lt;h4&gt;方法&lt;/h4&gt;使用平均聚合策略将块级嵌入聚合成切片级特征；训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型；使用精确度、召回率、真正例率、假正例率和AUROC评估性能；探索数据增强技术和图像归一化；使用WandB.ai跟踪和可视化实验结果。&lt;h4&gt;主要发现&lt;/h4&gt;使用Virchow2提取的块级特征在大多数切片级分类器上优于通过UNI提取的特征；使用Virchow2的逻辑回归模型达到了最高的准确率(90%)，但差异不具有统计学意义；平均聚合方法提供了可靠的切片级特征表示。&lt;h4&gt;结论&lt;/h4&gt;这项研究强调了基础模型在自动化WSI分类中的潜力；为皮肤病理学诊断提供了一种可扩展且有效的方法；为切片级表示学习的未来进展铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;皮肤病理学中全切片图像(WSI)的快速生成需要自动化方法进行高效处理和准确分类。本研究评估了两种基础模型UNI和Virchow2作为特征提取器的性能，用于将WSI分类为三种诊断类别：黑素细胞性、基底样性和鳞状病变。使用平均聚合策略将块级嵌入聚合成切片级特征，并随后用于训练多种机器学习分类器，包括逻辑回归、梯度提升树和随机森林模型。使用精确度、召回率、真正例率、假正例率和接收者操作特征曲线下面积(AUROC)在测试集上评估性能。结果表明，使用Virchow2提取的块级特征在大多数切片级分类器上优于通过UNI提取的特征，其中使用Virchow2的逻辑回归达到了最高的准确率(90%)，尽管差异不具有统计学意义。该研究还探索了数据增强技术和图像归一化，以提高模型的鲁棒性和泛化能力。平均聚合方法提供了可靠的切片级特征表示。所有实验结果和指标都使用WandB.ai进行跟踪和可视化，促进了可重复性和可解释性。这项研究强调了基础模型在自动化WSI分类中的潜力，为皮肤病理学诊断提供了一种可扩展且有效的方法，同时为切片级表示学习的未来进展铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid generation of whole-slide images (WSIs) in dermatopathologynecessitates automated methods for efficient processing and accurateclassification. This study evaluates the performance of two foundation models,UNI and Virchow2, as feature extractors for classifying WSIs into threediagnostic categories: melanocytic, basaloid, and squamous lesions. Patch-levelembeddings were aggregated into slide-level features using a mean-aggregationstrategy and subsequently used to train multiple machine learning classifiers,including logistic regression, gradient-boosted trees, and random forestmodels. Performance was assessed using precision, recall, true positive rate,false positive rate, and the area under the receiver operating characteristiccurve (AUROC) on the test set. Results demonstrate that patch-level featuresextracted using Virchow2 outperformed those extracted via UNI across mostslide-level classifiers, with logistic regression achieving the highestaccuracy (90%) for Virchow2, though the difference was not statisticallysignificant. The study also explored data augmentation techniques and imagenormalization to enhance model robustness and generalizability. Themean-aggregation approach provided reliable slide-level featurerepresentations. All experimental results and metrics were tracked andvisualized using WandB.ai, facilitating reproducibility and interpretability.This research highlights the potential of foundation models for automated WSIclassification, providing a scalable and effective approach fordermatopathological diagnosis while paving the way for future advancements inslide-level representation learning.</description>
      <author>example@mail.com (Riya Gupta, Yiwei Zong, Dennis H. Murphree)</author>
      <guid isPermaLink="false">2510.21664v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Causality Meets Locality: Provably Generalizable and Scalable Policy Learning for Networked Systems</title>
      <link>http://arxiv.org/abs/2510.21427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出GSAC框架，结合因果表示学习和元actor-critic学习，解决了大规模网络系统（如交通、电力和无线网格）中强化学习的规模和环境变化挑战。&lt;h4&gt;背景&lt;/h4&gt;大规模网络系统（如交通、电力和无线网格）对强化学习代理提出了规模和环境变化的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够同时实现可扩展性和领域泛化的框架，解决大规模网络系统中的强化学习挑战。&lt;h4&gt;方法&lt;/h4&gt;提出GSAC（Generalizable and Scalable Actor-Critic）框架，每个代理学习稀疏局部因果掩码识别关键变量，生成状态和领域因素的紧凑表示（ACRs），元actor-critic训练跨多个源领域的共享策略，测试时通过少量轨迹估计新领域因素并部署自适应策略。&lt;h4&gt;主要发现&lt;/h4&gt;建立了因果恢复、actor-critic收敛和自适应差距的有限样本保证，GSAC能够快速适应并显著优于传统方法。&lt;h4&gt;结论&lt;/h4&gt;GSAC框架有效解决了大规模网络系统中的强化学习挑战，实现了可扩展性和领域泛化。&lt;h4&gt;翻译&lt;/h4&gt;大规模网络系统，如交通、电力和无线网格，对强化学习代理提出了规模和环境变化的挑战。为应对这些挑战，我们提出了GSAC（Generalizable and Scalable Actor-Critic）框架，该框架结合因果表示学习和元actor-critic学习，以实现可扩展性和领域泛化。每个代理首先学习一个稀疏的局部因果掩码，可识别影响其动态的最小邻域变量，生成状态和领域因素的指数紧致近似紧凑表示（ACRs）。这些ACRs限制了将值函数截断到κ-跳邻域的误差，实现在图上的高效学习。元actor-critic则在多个源领域上训练共享策略，同时基于紧凑的领域因素进行条件化；在测试时，只需少量轨迹即可估计新的领域因素并部署自适应策略。我们建立了因果恢复、actor-critic收敛和自适应差距的有限样本保证，并表明GSAC能够快速适应且显著优于从头学习和传统自适应基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale networked systems, such as traffic, power, and wireless grids,challenge reinforcement-learning agents with both scale and environment shifts.To address these challenges, we propose GSAC (Generalizable and ScalableActor-Critic), a framework that couples causal representation learning withmeta actor-critic learning to achieve both scalability and domaingeneralization. Each agent first learns a sparse local causal mask thatprovably identifies the minimal neighborhood variables influencing itsdynamics, yielding exponentially tight approximately compact representations(ACRs) of state and domain factors. These ACRs bound the error of truncatingvalue functions to $\kappa$-hop neighborhoods, enabling efficient learning ongraphs. A meta actor-critic then trains a shared policy across multiple sourcedomains while conditioning on the compact domain factors; at test time, a fewtrajectories suffice to estimate the new domain factor and deploy the adaptedpolicy. We establish finite-sample guarantees on causal recovery, actor-criticconvergence, and adaptation gap, and show that GSAC adapts rapidly andsignificantly outperforms learning-from-scratch and conventional adaptationbaselines.</description>
      <author>example@mail.com (Hao Liang, Shuqing Shi, Yudi Zhang, Biwei Huang, Yali Du)</author>
      <guid isPermaLink="false">2510.21427v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Disentangled Representation Learning via Modular Compositional Bias</title>
      <link>http://arxiv.org/abs/2510.21402v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种组合偏置方法，用于解决解耦表示学习中的归纳偏置问题。通过随机混合潜在变量并根据特定规则重组，该方法能够在不修改架构或目标的情况下实现属性、对象甚至两者的解耦。&lt;h4&gt;背景&lt;/h4&gt;当前的解耦表示学习方法主要依赖于特定策略的归纳偏置，包括为属性学习特定目标或为对象设计特定架构。这种依赖性在新的变化因素与先验假设不匹配或多个因素共存时会导致显著开销。&lt;h4&gt;目的&lt;/h4&gt;提出一种组合偏置，一种与目标和架构都解耦的模块化归纳偏置，解决当多个因素共存时需要重新设计架构或目标的问题。&lt;h4&gt;方法&lt;/h4&gt;根据不同因素在数据分布中的不同重组规则（全局属性互斥，对象共享公共支持），随机混合潜在变量。通过两个互补目标强制编码器发现混合策略反映的因素结构：(i)先验损失确保每个混合解码为真实图像；(ii)组合一致性损失将复合图像与其对应的复合潜在变量对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在通用框架下，只需调整混合策略即可实现属性、对象甚至两者的解耦，无需修改目标或架构。实验表明该方法在属性和对象解耦方面具有竞争性性能，且唯一实现了全局风格和对象的联合解耦。&lt;h4&gt;结论&lt;/h4&gt;提出的组合偏置框架提供了一种灵活的方法来处理不同类型的解耦表示学习任务，只需调整混合策略而无需修改架构或目标。&lt;h4&gt;翻译&lt;/h4&gt;最近的解耦表示学习方法严重依赖于特定因素策略-无论是为属性学习目标还是为对象设计架构-以嵌入归纳偏置。这种不同的方法在新的变化因素与先验假设（如统计独立性或空间排他性）不匹配或多个因素共存时会导致显著开销，因为从业者必须重新设计架构或目标。为此，我们提出了一种组合偏置，一种与目标和架构都解耦的模块化归纳偏置。我们的关键见解是，不同因素在数据分布中遵循不同的重组规则：全局属性是互斥的，例如一张脸只有一个鼻子，而对象共享公共支持（任何对象子集都可以共存）。因此，我们根据特定规则随机混合潜在变量，即混合策略，并通过两个互补目标强制编码器发现混合策略反映的任何因素结构：(i)确保每个混合解码为真实图像的先验损失；(ii)Wiedemer等人(arXiv:2310.05327)引入的组合一致性损失，它将每个复合图像与其对应的复合潜在变量对齐。在这一通用框架下，只需调整混合策略即可实现属性、对象甚至两者的解耦，而无需修改目标或架构。大量实验表明，我们的方法在属性和对象解耦方面都表现出竞争性性能，并且唯一实现了全局风格和对象的联合解耦。代码可在https://github.com/whieya/Compositional-DRL获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent disentangled representation learning (DRL) methods heavily rely onfactor specific strategies-either learning objectives for attributes or modelarchitectures for objects-to embed inductive biases. Such divergent approachesresult in significant overhead when novel factors of variation do not alignwith prior assumptions, such as statistical independence or spatialexclusivity, or when multiple factors coexist, as practitioners must redesignarchitectures or objectives. To address this, we propose a compositional bias,a modular inductive bias decoupled from both objectives and architectures. Ourkey insight is that different factors obey distinct recombination rules in thedata distribution: global attributes are mutually exclusive, e.g., a face hasone nose, while objects share a common support (any subset of objects canco-exist). We therefore randomly remix latents according to factor-specificrules, i.e., a mixing strategy, and force the encoder to discover whicheverfactor structure the mixing strategy reflects through two complementaryobjectives: (i) a prior loss that ensures every remix decodes into a realisticimage, and (ii) the compositional consistency loss introduced by Wiedemer etal. (arXiv:2310.05327), which aligns each composite image with itscorresponding composite latent. Under this general framework, simply adjustingthe mixing strategy enables disentanglement of attributes, objects, and evenboth, without modifying the objectives or architectures. Extensive experimentsdemonstrate that our method shows competitive performance in both attribute andobject disentanglement, and uniquely achieves joint disentanglement of globalstyle and objects. Code is available athttps://github.com/whieya/Compositional-DRL.</description>
      <author>example@mail.com (Whie Jung, Dong Hoon Lee, Seunghoon Hong)</author>
      <guid isPermaLink="false">2510.21402v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models Meet Text-Attributed Graphs: A Survey of Integration Frameworks and Applications</title>
      <link>http://arxiv.org/abs/2510.21131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Surveys and overviews; Natural language processing; Knowledge  representation and reasoning; Graph algorithms&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从编排的角度系统地回顾了大型语言模型和文本属性图的集成研究，展示了两者结合带来的互补优势。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面取得了显著成功，但它们的黑盒性质限制了结构化和多跳推理能力；文本属性图提供了明确的文本关系结构，但往往缺乏语义深度。&lt;h4&gt;目的&lt;/h4&gt;首次从编排的角度系统地回顾LLM和TAG的集成，总结现有方法，并指出未来在语言和图学习交叉领域的研究方向。&lt;h4&gt;方法&lt;/h4&gt;引入新的分类法涵盖两个基本方向：LLM用于TAG（丰富基于图的任务）和TAG用于LLM（改进LLM推理）；将编排策略分为顺序、并行和多模块框架；讨论TAG特定的预训练、提示和参数高效微调的进展。&lt;h4&gt;主要发现&lt;/h4&gt;结合LLMs和TAGs可以产生互补的好处：增强TAG表示学习和提高LLMs的推理能力和可解释性。&lt;h4&gt;结论&lt;/h4&gt;总结了经验性见解，整理了可用数据集，强调了在推荐系统、生物医学分析和知识密集型问答等领域的应用，并指出了开放的挑战和有希望的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型通过强大的语义理解和生成在自然语言处理方面取得了显著成功。然而，它们的黑盒性质限制了结构化和多跳推理能力。相比之下，文本属性图提供了丰富的文本关系结构，但往往缺乏语义深度。最近的研究表明，结合LLMs和TAGs可以带来互补的好处：增强TAG表示学习并提高LLMs的推理能力和可解释性。这篇综述从编排的角度首次系统地回顾了LLM和TAG的集成。我们介绍了一种新的分类法，涵盖两个基本方向：LLM用于TAG，即LLMs丰富基于图的任务；以及TAG用于LLM，即结构化图改进LLM推理。我们将编排策略分为顺序、并行和多模块框架，并讨论了TAG特定的预训练、提示和参数高效微调的进展。除了方法论，我们还总结了经验性见解，整理了可用数据集，并强调了在推荐系统、生物医学分析和知识密集型问答等领域的多样化应用。最后，我们指出了开放的挑战和有希望的研究方向，旨在指导未来在语言和图学习交叉领域的工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) have achieved remarkable success in naturallanguage processing through strong semantic understanding and generation.However, their black-box nature limits structured and multi-hop reasoning. Incontrast, Text-Attributed Graphs (TAGs) provide explicit relational structuresenriched with textual context, yet often lack semantic depth. Recent researchshows that combining LLMs and TAGs yields complementary benefits: enhancing TAGrepresentation learning and improving the reasoning and interpretability ofLLMs. This survey provides the first systematic review of LLM--TAG integrationfrom an orchestration perspective. We introduce a novel taxonomy covering twofundamental directions: LLM for TAG, where LLMs enrich graph-based tasks, andTAG for LLM, where structured graphs improve LLM reasoning. We categorizeorchestration strategies into sequential, parallel, and multi-moduleframeworks, and discuss advances in TAG-specific pretraining, prompting, andparameter-efficient fine-tuning. Beyond methodology, we summarize empiricalinsights, curate available datasets, and highlight diverse applications acrossrecommendation systems, biomedical analysis, and knowledge-intensive questionanswering. Finally, we outline open challenges and promising researchdirections, aiming to guide future work at the intersection of language andgraph learning.</description>
      <author>example@mail.com (Guangxin Su, Hanchen Wang, Jianwei Wang, Wenjie Zhang, Ying Zhang, Jian Pei)</author>
      <guid isPermaLink="false">2510.21131v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging semantic similarity for experimentation with AI-generated treatments</title>
      <link>http://arxiv.org/abs/2510.21119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种双核表示学习方法，用于处理大型语言模型生成的高维数字实验处理，并通过学习低维表示捕捉处理的基本结构，应用于指导生成模型产生有意义的处理变体和促进在线实验的自适应分配。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）使数字实验能够以新的形式进行，其中处理方式结合了人类和模型生成的内容，且方式日益复杂。在这种环境下，主要的方法论挑战是如何表示这些高维处理而不丢失其语义含义或使分析变得不可行。&lt;h4&gt;目的&lt;/h4&gt;解决高维处理的表示问题，学习能够捕捉此类处理基本结构的低维表示，并应用于下游任务如指导生成模型和促进在线实验的自适应分配。&lt;h4&gt;方法&lt;/h4&gt;提出双核表示学习方法，通过处理和用户协变量的基于核的表示的内积来建模因果效应。开发了一种交替最小化算法，能够从数据中高效学习这些表示，并在低秩因子模型下提供收敛保证。引入了一种在线实验的自适应设计策略作为该框架的应用。&lt;h4&gt;主要发现&lt;/h4&gt;双核表示学习方法能够有效地从数据中学习处理的高维表示，并在低秩因子模型下保证收敛。该方法在在线实验的自适应设计中表现出有效性。&lt;h4&gt;结论&lt;/h4&gt;通过学习低维表示来捕捉高维处理的基本结构，可以有效地解决大型语言模型数字实验中的表示挑战，并应用于多种下游任务。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）使数字实验能够以新形式进行，其中处理方式结合了人类和模型生成的内容，且方式日益复杂。这种环境下的主要方法论挑战是如何表示这些高维处理而不丢失其语义含义或使分析变得不可行。在此，我们通过专注于学习能够捕捉此类处理基本结构的低维表示来解决这一问题。这些表示使下游应用成为可能，例如指导生成模型产生有意义的处理变体和促进在线实验中的自适应分配。我们提出了双核表示学习方法，通过处理和用户协变量的基于核的表示的内积来建模因果效应。我们开发了一种交替最小化算法，能够从数据中高效学习这些表示，并在低秩因子模型下提供收敛保证。作为该框架的应用，我们引入了一种在线实验的自适应设计策略，并通过数值实验证明了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) enable a new form of digital experimentationwhere treatments combine human and model-generated content in increasinglysophisticated ways. The main methodological challenge in this setting isrepresenting these high-dimensional treatments without losing their semanticmeaning or rendering analysis intractable. Here, we address this problem byfocusing on learning low-dimensional representations that capture theunderlying structure of such treatments. These representations enabledownstream applications such as guiding generative models to produce meaningfultreatment variants and facilitating adaptive assignment in online experiments.We propose double kernel representation learning, which models the causaleffect through the inner product of kernel-based representations of treatmentsand user covariates. We develop an alternating-minimization algorithm thatlearns these representations efficiently from data and provides convergenceguarantees under a low-rank factor model. As an application of this framework,we introduce an adaptive design strategy for online experimentation anddemonstrate the method's effectiveness through numerical experiments.</description>
      <author>example@mail.com (Lei Shi, David Arbour, Raghavendra Addanki, Ritwik Sinha, Avi Feller)</author>
      <guid isPermaLink="false">2510.21119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Fair Representation Learning with Controllable High Confidence Guarantees via Adversarial Inference</title>
      <link>http://arxiv.org/abs/2510.21017v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了FRG框架，用于学习具有高置信度公平性保证的表示，确保在下游任务中人口统计差异被限制在用户定义的阈值内。&lt;h4&gt;背景&lt;/h4&gt;表示学习被广泛应用于生成能在多个下游任务中泛化的表示，但确保其公平性对于防止对特定人口统计群体产生不公平至关重要。&lt;h4&gt;目的&lt;/h4&gt;正式引入学习高置信度公平性表示的任务，保证每个下游预测中的人口统计差异被限制在用户定义的错误阈值内，并以可控的高概率实现。&lt;h4&gt;方法&lt;/h4&gt;提出FRG(Fair Representation learning with high-confidence Guarantees)框架，通过利用优化的对抗模型提供高置信度公平性保证。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界数据集上的实证评估表明，FRG相比六种最先进的公平表示学习方法，能够在一系列下游模型和任务中持续限制不公平性。&lt;h4&gt;结论&lt;/h4&gt;FRG框架能有效提供高置信度的公平性保证，确保在多种下游任务中公平性得到控制。&lt;h4&gt;翻译&lt;/h4&gt;表示学习越来越多地被应用于生成能够在多个下游任务中泛化的表示。确保表示学习中的公平性保证对于防止在下游任务中对特定人口统计群体产生不公平至关重要。在这项工作中，我们正式引入了学习能够实现高置信度公平性表示的任务。我们旨在保证每个下游预测中的人口统计差异被限制在用户定义的错误阈值内，并以可控的高概率实现。为此，我们提出了FRG框架，该框架通过利用优化的对抗模型提供这些高置信度公平性保证。我们在三个真实世界数据集上对FRG进行了实证评估，将其性能与六种最先进的公平表示学习方法进行比较。我们的结果表明FRG能够在一系列下游模型和任务中持续限制不公平性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representation learning is increasingly applied to generate representationsthat generalize well across multiple downstream tasks. Ensuring fairnessguarantees in representation learning is crucial to prevent unfairness towardspecific demographic groups in downstream tasks. In this work, we formallyintroduce the task of learning representations that achieve high-confidencefairness. We aim to guarantee that demographic disparity in every downstreamprediction remains bounded by a *user-defined* error threshold $\epsilon$, with*controllable* high probability. To this end, we propose the ***F**air**R**epresentation learning with high-confidence **G**uarantees (FRG)*framework, which provides these high-confidence fairness guarantees byleveraging an optimized adversarial model. We empirically evaluate FRG on threereal-world datasets, comparing its performance to six state-of-the-art fairrepresentation learning methods. Our results demonstrate that FRG consistentlybounds unfairness across a range of downstream models and tasks.</description>
      <author>example@mail.com (Yuhong Luo, Austin Hoag, Xintong Wang, Philip S. Thomas, Przemyslaw A. Grabowicz)</author>
      <guid isPermaLink="false">2510.21017v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>L^2M^3OF: A Large Language Multimodal Model for Metal-Organic Frameworks</title>
      <link>http://arxiv.org/abs/2510.20976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了L2M3OF，首个专门用于金属有机框架设计的多模态大语言模型，通过整合晶体表示学习与语言理解能力，在材料设计领域取得了突破性进展。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在自然语言任务中展现出强大推理能力，但在科学发现方面的突破有限。理解复杂物理现象需要超越语言的多方面表示，MOFs设计面临巨大三维原子排列空间和严格网状规则的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理MOFs设计的多模态大语言模型，克服仅基于文本表示的局限性，减少对人类专家经验的依赖。&lt;h4&gt;方法&lt;/h4&gt;L2M3OF整合晶体表示学习与语言理解，联合处理结构、文本和知识模态；使用预训练的晶体编码器和轻量级投影层压缩结构信息；构建晶体材料的结构-性质-知识数据库；与GPT-5、Gemini-2.5-Pro和DeepSeek-R1等闭源LLM进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;L2M3OF在性质预测和知识生成任务上优于领先的基于文本的闭源LLM，尽管使用的参数少得多。&lt;h4&gt;结论&lt;/h4&gt;多模态方法对于多孔材料理解的重要性，L2M3OF成为材料发现领域下一代AI系统的基础。&lt;h4&gt;翻译&lt;/h4&gt;大语言模型已在各种自然语言任务中展现出卓越的推理能力。然而，在科学发现方面的可比突破更为有限，因为理解复杂的物理现象需要超越语言本身的多方面表示。一个有力的例子是功能材料如金属有机框架的设计，这些材料对于碳捕获和氢储存等一系列重要应用至关重要。由于其可能存在的三维原子排列数量众多以及配位几何和拓扑的严格网状规则，在基于语言的、可被大语言模型解释的表示中导航其巨大而复杂的设计空间具有挑战性。尽管在更简单的材料系统中，大语言模型辅助的发现已显示出有希望的结果，但MOF设计仍然严重依赖于很少仅以文本信息编码的隐性人类专业知识。为了克服这一障碍，我们引入了L2M3OF，这是首个用于MOFs的多模态大语言模型。L2M3OF整合了晶体表示学习与语言理解，以联合处理结构、文本和知识模态。L2M3OF采用预训练的晶体编码器和轻量级投影层将结构信息压缩到标记空间，从而实现与语言指令的有效对齐。为了促进训练和评估，我们整理了一个包含晶体材料的结构-性质-知识数据库，并将L2M3OF与最先进的闭源大语言模型（如GPT-5、Gemini-2.5-Pro和DeepSeek-R1）进行基准测试。实验表明，尽管使用的参数少得多，L2M3OF在性质预测和知识生成任务上优于领先的基于文本的闭源大语言模型。这些结果突显了多模态方法对于多孔材料理解的重要性，并将L2M3OF确立为材料发现领域下一代AI系统的基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何利用大型语言模型有效理解和设计金属有机框架（MOFs）材料的问题。这个问题很重要，因为MOFs是一类多孔晶体材料，在碳捕获、氢储存、水收集和药物输送等领域有广泛应用潜力。然而，MOFs的三维复杂结构难以用纯文本表示，现有方法无法有效捕捉其三维对称性、周期性和长程结构相关性，限制了材料设计的效率和准确性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有LLMs在材料科学中的局限性，特别是它们难以处理MOFs的三维结构信息。他们借鉴了晶体表示学习（如PMTransformer）和指令微调范式，设计了L2M3OF模型。该方法结合了预训练的晶体编码器与轻量级投影层，将结构信息压缩到标记空间，实现与语言指令的高效对齐。同时，作者还采用了分组训练策略增强上下文多样性。这些设计基于对现有工作的深入分析，并针对MOFs的特殊性进行了创新改进。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过多模态学习，将MOFs的三维结构信息与文本知识相结合，使模型能够同时理解和处理材料的结构、属性和应用知识。整体流程包括：1) 构建MOF-SPK数据库，包含超过10万种MOFs的结构、属性和知识；2) 使用PMTransformer作为晶体结构编码器，将三维结构转换为潜在表示；3) 通过多模态投影桥将结构信息压缩并投影到语言模型空间；4) 使用指令微调范式训练模型，采用分组训练策略增强上下文多样性；5) 在属性预测、结构提取、描述生成和问答等任务上评估性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首个专门为MOFs设计的多模态大型语言模型；2) 构建了首个包含MOFs结构、属性和领域知识的综合数据库MOF-SPK；3) 设计了高效的多模态对齐方法，使用轻量级投影层实现结构信息与语言指令的融合；4) 提出了分组训练策略增强模型性能。相比之前的工作，L2M3OF能够处理复杂的三维MOF结构，超越了纯文本表示的局限性；同时能够同时处理多种任务（属性预测、结构提取、描述生成和问答），在性能上超越了参数更多的商业LLMs，尽管使用了更少的参数。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; L2M3OF通过整合三维晶体结构表示与领域知识，开创了多模态大型语言模型在金属有机框架材料设计中的应用，实现了超越纯文本模型的性能，为材料科学发现提供了新的AI辅助工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have demonstrated remarkable reasoning capabilitiesacross diverse natural language tasks. However, comparable breakthroughs inscientific discovery are more limited, because understanding complex physicalphenomena demands multifaceted representations far beyond language alone. Acompelling example is the design of functional materials such as MOFs-criticalfor a range of impactful applications like carbon capture and hydrogen storage.Navigating their vast and intricate design space in language-basedrepresentations interpretable by LLMs is challenging due to the numerouspossible three-dimensional atomic arrangements and strict reticular rules ofcoordination geometry and topology. Despite promising early results inLLM-assisted discovery for simpler materials systems, MOF design remainsheavily reliant on tacit human expertise rarely codified in textual informationalone. To overcome this barrier, we introduce L2M3OF, the first multimodal LLMfor MOFs. L2M3OF integrates crystal representation learning with languageunderstanding to process structural, textual, and knowledge modalities jointly.L2M3OF employs a pre-trained crystal encoder with a lightweight projectionlayer to compress structural information into a token space, enabling efficientalignment with language instructions. To facilitate training and evaluation, wecurate a structure-property-knowledge database of crystalline materials andbenchmark L2M3OF against state-of-the-art closed-source LLMs such as GPT-5,Gemini-2.5-Pro and DeepSeek-R1. Experiments show that L2M3OF outperformsleading text-based closed-source LLMs in property prediction and knowledgegeneration tasks, despite using far fewer parameters. These results highlightthe importance of multimodal approaches for porous material understanding andestablish L2M3OF as a foundation for next-generation AI systems in materialsdiscovery.</description>
      <author>example@mail.com (Jiyu Cui, Fang Wu, Haokai Zhao, Minggao Feng, Xenophon Evangelopoulos, Andrew I. Cooper, Yejin Choi)</author>
      <guid isPermaLink="false">2510.20976v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Irish-BLiMP: A Linguistic Benchmark for Evaluating Human and Language Model Performance in a Low-Resource Setting</title>
      <link>http://arxiv.org/abs/2510.20957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Irish-BLiMP是首个专门为评估爱尔兰语语言能力而设计的数据集和框架，研究对比了人类和大型语言模型在爱尔兰语语法知识上的表现。&lt;h4&gt;背景&lt;/h4&gt;爱尔兰语是一种濒危语言，缺乏专门的语言能力评估工具和框架。&lt;h4&gt;目的&lt;/h4&gt;评估现有大型语言模型和流利人类参与者在爱尔兰语语法知识方面的表现，并分析两者之间的差异。&lt;h4&gt;方法&lt;/h4&gt;基于多种语言学文献和参考资料，由流利的爱尔兰语使用者团队手动构建和审查了1020个最小对比对，涵盖11个语言学特征的分类法。&lt;h4&gt;主要发现&lt;/h4&gt;人类在所有语言学特征上的表现都优于所有模型，平均准确率高出16.6%；开源和闭源大型语言模型之间存在18.1%的性能差距；最强模型(gpt-5)准确率为73.5%，人类为90.1%；人类和模型在不同语法方面存在不同困难。&lt;h4&gt;结论&lt;/h4&gt;Irish-BLiMP为评估LLMs在爱尔兰语中的语法能力提供了首个系统性框架，为低资源语言理解研究提供了有价值的基准。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了Irish-BLiMP(爱尔兰语语言能力最小对比数据集)，这是第一个专门为评估爱尔兰语(一种濒危语言)语言能力而设计和构建的数据集和框架。借鉴多种语言学文献和语法参考资料，我们由流利的爱尔兰语使用者团队手动构建并审查了涵盖11个语言学特征分类法的1020个最小对比对。我们评估了现有大型语言模型和流利人类参与者在爱尔兰语语法知识方面的表现。我们的发现显示，人类在所有语言学特征上的表现都优于所有模型，平均准确率高出16.6%。此外，开源和闭源大型语言模型之间存在18.1%的显著性能差距，即使是最强的模型也仅达到73.5%的准确率，而人类达到90.1%。有趣的是，人类参与者和模型在爱尔兰语语法的不同方面存在困难，这突显了模型学习表征的差异。总体而言，Irish-BLiMP为评估大型语言模型在爱尔兰语中的语法能力提供了首个系统性框架，并为推进低资源语言理解研究提供了有价值的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Irish-BLiMP (Irish Benchmark of Linguistic Minimal Pairs), thefirst dataset and framework designed for fine-grained evaluation of linguisticcompetence in the Irish language, an endangered language. Drawing on a varietyof linguistic literature and grammar reference works, we manually constructedand reviewed 1020 minimal pairs across a taxonomy of 11 linguistic features,through a team of fluent Irish speakers. We evaluate both existing LargeLanguage Models (LLMs) and fluent human participants on their syntacticknowledge of Irish. Our findings show that humans outperform all models acrossall linguistic features, achieving 16.6% higher accuracy on average. Moreover,a substantial performance gap of 18.1% persists between open- and closed-sourceLLMs, with even the strongest model (gpt-5) reaching only 73.5% accuracycompared to 90.1% by human. Interestingly, human participants and modelsstruggle on different aspects of Irish grammar, thus highlighting a differencein representation learned by the models. Overall, Irish-BLiMP provides thefirst systematic framework for evaluating the grammatical competence of LLMs inIrish and offers a valuable benchmark for advancing research on linguisticunderstanding in low-resource languages.</description>
      <author>example@mail.com (Josh McGiff, Khanh-Tung Tran, William Mulcahy, Dáibhidh Ó Luinín, Jake Dalzell, Róisín Ní Bhroin, Adam Burke, Barry O'Sullivan, Hoang D. Nguyen, Nikola S. Nikolov)</author>
      <guid isPermaLink="false">2510.20957v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ROPES: Robotic Pose Estimation via Score-Based Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2510.20884v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A preliminary version of this paper appeared at NeurIPS 2025 Workshop  on Embodied World Models for Decision Making&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ROPES方法，将因果表征学习应用于机器人姿态估计，这是一个无监督框架，能够解离潜在生成因素并识别可通过致动直接控制的变量。&lt;h4&gt;背景&lt;/h4&gt;因果表征学习(CRL)是一种强大的无监督框架，能够解离高维数据下的潜在生成因素，并学习解离变量之间的因果关系。尽管在可识别性和实际应用方面取得了进展，但理论与实践之间仍存在显著差距。&lt;h4&gt;目的&lt;/h4&gt;将CRL引入机器人领域，以缩小理论与实践之间的差距。具体解决机器人姿态估计问题，即从原始图像中恢复位置和方向。&lt;h4&gt;方法&lt;/h4&gt;提出ROPES（基于分数的CRL的机器人姿态估计）框架。这是一个无监督框架，通过识别被致动的生成因素来体现干预性CRL的本质。图像由内在和外在潜在因素生成（如关节角度、手臂/肢体几何形状、光照、背景和相机配置），目标是解离和恢复可控制的潜在变量。&lt;h4&gt;主要发现&lt;/h4&gt;在半合成机械臂实验中的经验评估表明，ROPES能够高保真度地解离潜在生成因素。这是仅通过利用分布变化实现的，而没有使用任何标记数据。论文还包括了与基于半监督框架的基线方法的比较。&lt;h4&gt;结论&lt;/h4&gt;将机器人姿态定位为CRL的近实用测试平台。&lt;h4&gt;翻译&lt;/h4&gt;因果表征学习（CRL）已成为一种强大的无监督框架，它（i）解离高维数据下的潜在生成因素，以及（ii）学习解离变量之间的因果关系。尽管最近在可识别性方面取得了广泛进展并有一些实践进展，但理论与实践之间仍然存在巨大差距。本文通过将CRL引入机器人领域（这一领域推动了CRL的发展）来缩小这一差距。具体而言，本文通过引入基于分数的CRL的机器人姿态估计（ROPES）来解决明确的机器人姿态估计问题——从原始图像中恢复位置和方向。作为一个无监督框架，ROPES通过识别被致动的生成因素体现了干预性CRL的本质：图像由内在和外在潜在因素（例如，关节角度、手臂/肢体几何形状、光照、背景和相机配置）生成，目标是解离和恢复可控制的潜在变量，即那些可以通过致动直接操作（干预）的变量。干预性CRL理论表明，通过干预经历变化的变量可以被识别。在机器人领域，这种干预通过命令各种关节的致动器并在不同控制下记录图像自然产生。在半合成机械臂实验中的经验评估表明，ROPES能够高保真度地解离潜在生成因素。关键的是，这是仅通过利用分布变化实现的，而没有使用任何标记数据。本文还包括了与最近提出的半监督框架的基线方法的比较。本文最后将机器人姿态定位为CRL的近实用测试平台。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Causal representation learning (CRL) has emerged as a powerful unsupervisedframework that (i) disentangles the latent generative factors underlyinghigh-dimensional data, and (ii) learns the cause-and-effect interactions amongthe disentangled variables. Despite extensive recent advances inidentifiability and some practical progress, a substantial gap remains betweentheory and real-world practice. This paper takes a step toward closing that gapby bringing CRL to robotics, a domain that has motivated CRL. Specifically,this paper addresses the well-defined robot pose estimation -- the recovery ofposition and orientation from raw images -- by introducing Robotic PoseEstimation via Score-Based CRL (ROPES). Being an unsupervised framework, ROPESembodies the essence of interventional CRL by identifying those generativefactors that are actuated: images are generated by intrinsic and extrinsiclatent factors (e.g., joint angles, arm/limb geometry, lighting, background,and camera configuration) and the objective is to disentangle and recover thecontrollable latent variables, i.e., those that can be directly manipulated(intervened upon) through actuation. Interventional CRL theory shows thatvariables that undergo variations via interventions can be identified. Inrobotics, such interventions arise naturally by commanding actuators of variousjoints and recording images under varied controls. Empirical evaluations insemi-synthetic manipulator experiments demonstrate that ROPES successfullydisentangles latent generative factors with high fidelity with respect to theground truth. Crucially, this is achieved by leveraging only distributionalchanges, without using any labeled data. The paper also includes a comparisonwith a baseline based on a recently proposed semi-supervised framework. Thispaper concludes by positioning robot pose estimation as a near-practicaltestbed for CRL.</description>
      <author>example@mail.com (Pranamya Kulkarni, Puranjay Datta, Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Ali Tajer)</author>
      <guid isPermaLink="false">2510.20884v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Machine-Learning-Assisted Comparison of Regression Functions</title>
      <link>http://arxiv.org/abs/2510.24714v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新审视了比较回归函数的经典问题，提出了一种基于核的条件均值依赖性的广义概念，并开发了两种新的检验方法，利用现代机器学习方法进行灵活估计。&lt;h4&gt;背景&lt;/h4&gt;比较回归函数是统计推断中的一个基本问题，与数据集成、迁移学习和因果推断等现代应用密切相关。现有方法通常依赖于平滑技术，因此受到维度诅咒的限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来比较回归函数，克服现有方法在维度诅咒下的局限性，并减少对分布假设的依赖。&lt;h4&gt;方法&lt;/h4&gt;提出基于核的条件均值依赖性的广义概念，为回归函数相等的零假设提供新表征，并基于此重新表述开发两种新的检验方法，利用现代机器学习方法进行灵活估计。&lt;h4&gt;主要发现&lt;/h4&gt;建立了检验统计量的渐近性质，这些性质在固定维度和高维度情况下都成立；与需要严格分布假设的现有方法不同，该框架仅施加温和的矩条件。&lt;h4&gt;结论&lt;/h4&gt;所提出的检验方法在广泛的数值研究中证明了其有效性，为比较回归函数提供了更灵活和实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;我们重新审视了比较回归函数的经典问题，这是统计推断中的一个基本问题，与数据集成、迁移学习和因果推断等现代应用密切相关。现有方法通常依赖于平滑技术，因此受到维度诅咒的限制。我们提出了一种基于核的条件均值依赖性的广义概念，为回归函数相等的零假设提供了新的表征。基于这一重新表述，我们开发了两种新的检验方法，利用现代机器学习方法进行灵活估计。我们建立了检验统计量的渐近性质，这些性质在固定维度和高维度情况下都成立。与通常需要严格分布假设的现有方法不同，我们的框架仅施加温和的矩条件。所提出检验方法的有效性通过大量的数值研究得到了证明。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit the classical problem of comparing regression functions, afundamental question in statistical inference with broad relevance to modernapplications such as data integration, transfer learning, and causal inference.Existing approaches typically rely on smoothing techniques and are thushindered by the curse of dimensionality. We propose a generalized notion ofkernel-based conditional mean dependence that provides a new characterizationof the null hypothesis of equal regression functions. Building on thisreformulation, we develop two novel tests that leverage modern machine learningmethods for flexible estimation. We establish the asymptotic properties of thetest statistics, which hold under both fixed- and high-dimensional regimes.Unlike existing methods that often require restrictive distributionalassumptions, our framework only imposes mild moment conditions. The efficacy ofthe proposed tests is demonstrated through extensive numerical studies.</description>
      <author>example@mail.com (Jian Yan, Zhuoxi Li, Yang Ning, Yong Chen)</author>
      <guid isPermaLink="false">2510.24714v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Cluster Dose Prediction in Carbon Ion Therapy: Using Transfer Learning from a Pretrained Dose Prediction U-Net</title>
      <link>http://arxiv.org/abs/2510.24703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究使用神经网络预测簇剂量分布，以替代计算密集型模拟，并通过迁移学习技术优化了U-Net架构，实现了快速且准确的簇剂量估计。&lt;h4&gt;背景&lt;/h4&gt;簇剂量概念为基于放射生物学效应(RBE)的模型提供了替代方案，用于描述辐射诱导的生物效应。&lt;h4&gt;目的&lt;/h4&gt;研究应用神经网络预测簇剂量分布，以替代当前需要的计算密集型模拟。&lt;h4&gt;方法&lt;/h4&gt;使用U-Net架构预测簇剂量分布，该网络首先在常规剂量分布上预训练，然后通过迁移学习技术对解码器路径进行适应；训练和预训练数据集包括来自多个患者头颈区域的不同能量和位置的碳离子束；使用蒙特卡洛模拟生成真实簇剂量分布作为基准。&lt;h4&gt;主要发现&lt;/h4&gt;U-Net能够在使用图形处理单元(GPU)的情况下，在几毫秒内完成单笔束的簇剂量估计；预测的簇剂量分布与真实值的偏差小于0.35%。&lt;h4&gt;结论&lt;/h4&gt;该原理验证研究证明了使用机器学习在临床可接受的计算时间内准确估计簇剂量的可行性；通过利用预训练神经网络和应用迁移学习技术，显著减少了对大规模、计算成本高昂的训练数据的需求。&lt;h4&gt;翻译&lt;/h4&gt;簇剂量概念为基于放射生物学效应(RBE)的模型提供了替代方案，用于描述辐射诱导的生物效应。本研究探讨了应用神经网络预测簇剂量分布的可能性，旨在替代当前需要的计算密集型模拟。使用最初在常规剂量分布上预训练的U-Net来预测簇剂量分布，并通过迁移学习技术对解码器路径进行适应以用于簇剂量估计。训练和预训练数据集包括来自多个患者头颈区域的不同能量和位置的碳离子束。使用蒙特卡洛(MC)模拟生成真实簇剂量分布。U-Net能够在使用图形处理单元(GPU)的情况下，在几毫秒内完成单笔束的簇剂量估计。预测的簇剂量分布与真实值的偏差小于0.35%。这项原理验证研究证明了使用机器学习(ML)在临床可接受的计算时间内准确估计簇剂量的可行性。通过利用预训练神经网络和应用迁移学习技术，该方法显著减少了对大规模、计算成本高昂的训练数据的需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cluster dose concept offers an alternative to the radiobiologicaleffectiveness (RBE)-based model for describing radiation-induced biologicaleffects. This study examines the application of a neural network to predictcluster dose distributions, with the goal of replacing the computationallyintensive simulations currently required. Cluster dose distributions arepredicted using a U-Net that was initially pretrained on conventional dosedistributions. Using transfer learning techniques, the decoder path is adaptedfor cluster dose estimation. Both the training and pretraining datasets includehead and neck regions from multiple patients and carbon ion beams of varyingenergies and positions. Monte Carlo (MC) simulations were used to generate theground truth cluster dose distributions. The U-Net enables cluster doseestimation for a single pencil beam within milliseconds using a graphicsprocessing unit (GPU). The predicted cluster dose distributions deviate fromthe ground truth by less than 0.35%. This proof-of-principle study demonstratesthe feasibility of accurately estimating cluster doses within clinicallyacceptable computation times using machine learning (ML). By leveraging apretrained neural network and applying transfer learning techniques, theapproach significantly reduces the need for large-scale, computationallyexpensive training data.</description>
      <author>example@mail.com (Miriam Schwarze, Hui Khee Looe, Björn Poppe, Leo Thomas, Hans Rabus)</author>
      <guid isPermaLink="false">2510.24703v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Semi-supervised and unsupervised learning for health indicator extraction from guided waves in aerospace composite structures</title>
      <link>http://arxiv.org/abs/2510.24614v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种综合数据驱动框架，通过结合两种学习方法与多域信号处理来提取航空航天复合材料结构的健康指标，解决了健康指标提取中的挑战。&lt;h4&gt;背景&lt;/h4&gt;健康指标对于诊断和预测航空航天复合材料结构的状况至关重要，有助于高效维护和操作安全。然而，由于材料特性的变异性、损伤演变的随机性和多样化的损伤模式，提取可靠的健康指标具有挑战性。制造缺陷和服役期间的事故进一步增加了复杂性。&lt;h4&gt;目的&lt;/h4&gt;开发一种综合数据驱动框架，通过两种学习方法与多域信号处理相结合来学习健康指标，由于缺乏真实健康指标，提出半监督和无监督方法来解决这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出两种学习方法：多样性深度半监督异常检测(Diversity-DeepSAD)方法，使用连续辅助标签作为假设损伤代理；退化趋势约束变分自编码器(DTC-VAE)，通过显式趋势约束嵌入单调性准则。使用多种激励频率的导波监测单加筋复合材料结构，探索时间、频率和时间频域表示，并通过无监督集成融合各频率健康指标。&lt;h4&gt;主要发现&lt;/h4&gt;使用快速傅里叶变换特征，增强的Diversity-DeepSAD模型达到81.6%的性能，DTC-VAE提供最一致的健康指标，达到92.3%的性能，优于现有基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的数据驱动框架，特别是DTC-VAE方法，能够有效提取航空航天复合材料结构的健康指标，为结构健康监测提供了可靠解决方案。&lt;h4&gt;翻译&lt;/h4&gt;健康指标对于诊断和预测航空航天复合材料结构的状况至关重要，能够实现高效维护和操作安全。然而，由于材料特性的变异性、损伤演变的随机性和多样化的损伤模式，提取可靠的健康指标仍然具有挑战性。制造缺陷（如脱粘）和服役期间的事故（如鸟撞）进一步使这一过程复杂化。本研究提出了一种综合数据驱动框架，通过结合两种学习方法与多域信号处理来学习健康指标。由于缺乏真实健康指标，提出了半监督和无监督方法：(i)多样性深度半监督异常检测方法，使用连续辅助标签作为假设损伤代理，克服了仅区分健康和故障状态的二元标签的局限性；(ii)退化趋势约束变分自编码器，其中单调性准则通过显式趋势约束嵌入。使用多种激励频率的导波来监测在疲劳载荷下的单加筋复合材料结构，并通过无监督集成融合各频率的健康指标，以减少频率依赖性和方差。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Health indicators (HIs) are central to diagnosing and prognosing thecondition of aerospace composite structures, enabling efficient maintenance andoperational safety. However, extracting reliable HIs remains challenging due tovariability in material properties, stochastic damage evolution, and diversedamage modes. Manufacturing defects (e.g., disbonds) and in-service incidents(e.g., bird strikes) further complicate this process. This study presents acomprehensive data-driven framework that learns HIs via two learning approachesintegrated with multi-domain signal processing. Because ground-truth HIs areunavailable, a semi-supervised and an unsupervised approach are proposed: (i) adiversity deep semi-supervised anomaly detection (Diversity-DeepSAD) approachaugmented with continuous auxiliary labels used as hypothetical damage proxies,which overcomes the limitation of prior binary labels that only distinguishhealthy and failed states while neglecting intermediate degradation, and (ii) adegradation-trend-constrained variational autoencoder (DTC-VAE), in which themonotonicity criterion is embedded via an explicit trend constraint. Guidedwaves with multiple excitation frequencies are used to monitor single-stiffenercomposite structures under fatigue loading. Time, frequency, and time-frequencyrepresentations are explored, and per-frequency HIs are fused via unsupervisedensemble learning to mitigate frequency dependence and reduce variance. Usingfast Fourier transform features, the augmented Diversity-DeepSAD model achieved81.6% performance, while DTC-VAE delivered the most consistent HIs with 92.3%performance, outperforming existing baselines.</description>
      <author>example@mail.com (James Josep Perry, Pablo Garcia-Conde Ortiz, George Konstantinou, Cornelie Vergouwen, Edlyn Santha Kumaran, Morteza Moradi)</author>
      <guid isPermaLink="false">2510.24614v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised learning for variability detection with Gaia DR3 photometry. The main sequence-white dwarf valley</title>
      <link>http://arxiv.org/abs/2510.23776v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in Astronomy &amp; Astrophysics (A&amp;A); 10 pages,  9 figures, 1 appendix (7 additional figures, 2 tables)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究利用无监督学习方法从Gaia DR3数据中识别变星和特殊系统，成功发现了包括热亚矮星、激变变星、食双星等多种天体类型，并证实该方法在大规模恒星群体分析中的有效性。&lt;h4&gt;背景&lt;/h4&gt;来自空间和地面望远镜的空前数量和质量的数据为机器学习提供了机会，使其能够识别传统方法可能忽略的新变星类别和特殊系统。之前已有相关方法学工作。&lt;h4&gt;目的&lt;/h4&gt;研究无监督学习方法在大恒星群体（包括拥挤场中的天体）上的扩展潜力，无需预先选择的目录，专注于从Gaia DR3中选出的13405个源。&lt;h4&gt;方法&lt;/h4&gt;使用基于从Gaia DR3时代测光中提取的统计特征的无监督聚类技术，采用t-SNE算法识别变星类别、子类型和仪器效应引起的虚假变异性。&lt;h4&gt;主要发现&lt;/h4&gt;聚类结果显示了不同组别，包括热亚矮星、激变变星、食双星和仙女座场中的拥挤场天体；发现了潜在的恒星子类型；被标记为RR Lyrae的天体出现在CMD的意外区域，可能由于不可靠的天体测量或替代演化途径。&lt;h4&gt;结论&lt;/h4&gt;所提出方法在寻找Gaia CMD大区域中可变天体（包括可变热亚矮星和激变变星）具有稳健性，展示了检测扩展恒星群体中变异性的效率，该无监督学习框架可扩展到大型数据集并在识别恒星子类方面有前景。&lt;h4&gt;翻译&lt;/h4&gt;来自空间和地面望远镜的空前数量和质量的数据为机器学习提供了识别新类别变星和可能被传统方法忽视的特殊系统的机会。在先前方法学研究的基础上，本研究探讨了无监督学习方法在大恒星群体（包括拥挤场中的天体）上的扩展潜力，无需预先选择的目录，特别专注于从Gaia DR3中选出的13405个源，位于选定CMD区域。我们的方法主要基于从Gaia DR3时代测光中提取的统计特征，采用无监督聚类技术。我们使用t-SNE算法来识别变星类别、其子类型以及由仪器效应引起的虚假变异性。聚类结果显示了不同的组别，包括热亚矮星、激变变星、食双星以及拥挤场中的天体，如仙女座(M31)场中的天体。在这些集群中还出现了几种潜在的恒星子类型。值得注意的是，先前被标记为RR Lyrae的天体在CMD的意外区域被发现，可能是由于不可靠的天体测量（如双星性）或替代的演化途径。本研究强调了所提出方法在寻找Gaia CMD大区域中可变天体的稳健性，包括可变热亚矮星和激变变星，同时展示了其在检测扩展恒星群体中变异性的效率。所提出的无监督学习框架可扩展到大型数据集，并在识别恒星子类方面有前景的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unprecedented volume and quality of data from space- and ground-basedtelescopes present an opportunity for machine learning to identify new classesof variable stars and peculiar systems that may have been overlooked bytraditional methods. Extending prior methodological work, this studyinvestigates the potential of an unsupervised learning approach to scaleeffectively to larger stellar populations, including objects in crowded fields,and without the need for pre-selected catalogues, specifically focusing on 13405 sources selected from Gaia DR3 and lying in the selected region of the CMD.Our methodology incorporates unsupervised clustering techniques based primarilyon statistical features extracted from Gaia DR3 epoch photometry. We used thet-distributed stochastic neighbour embedding (t-SNE) algorithm to identifyvariability classes, their subtypes, and spurious variability induced byinstrumental effects. The clustering results revealed distinct groups,including hot subdwarfs, cataclysmic variables (CVs), eclipsing binaries, andobjects in crowded fields, such as those in the Andromeda (M31) field. Severalpotential stellar subtypes also emerged within these clusters. Notably, objectspreviously labelled as RR Lyrae were found in an unexpected region of the CMD,potentially due to either unreliable astrometric measurements (e.g., due tobinarity) or alternative evolutionary pathways. This study emphasises therobustness of the proposed method in finding variable objects in a large regionof the Gaia CMD, including variable hot subdwarfs and CVs, while demonstratingits efficiency in detecting variability in extended stellar populations. Theproposed unsupervised learning framework demonstrates scalability to largedatasets and yields promising results in identifying stellar subclasses.</description>
      <author>example@mail.com (P. Ranaivomanana, C. Johnston, G. Iorio, P. J. Groot, M. Uzundag, T. Kupfer, C. Aerts)</author>
      <guid isPermaLink="false">2510.23776v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Genomics into Multimodal EHR Foundation Models</title>
      <link>http://arxiv.org/abs/2510.23639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种创新的电子健康记录(EHR)基础模型，整合多基因风险评分(PRS)作为基础数据模态，超越传统EHR-only方法，构建更全面的健康档案。&lt;h4&gt;背景&lt;/h4&gt;传统EHR模型仅使用临床数据，忽略了遗传因素对健康的影响。All of Us (AoU)研究项目提供了广泛而多样的数据资源。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态框架，学习临床数据和遗传倾向之间的复杂关系，增强预测能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;将生成式AI的进步扩展到EHR基础模型空间，利用AoU研究项目的数据进行训练，并探索迁移学习用于定制分类任务。&lt;h4&gt;主要发现&lt;/h4&gt;在AoU数据上的评估表明，该模型对多种疾病发作(特别是2型糖尿病)具有预测价值，并展示了PRS和EHR数据之间的相互作用。&lt;h4&gt;结论&lt;/h4&gt;这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种创新的电子健康记录(EHR)基础模型，将多基因风险评分(PRS)作为基础数据模态整合其中，超越了传统的仅使用EHR的方法，以构建更全面的健康档案。利用All of Us (AoU)研究项目的广泛而多样的数据，这个多模态框架旨在学习临床数据和遗传倾向之间的复杂关系。该方法将生成式AI的进步扩展到EHR基础模型空间，增强了预测能力和可解释性。在AoU数据上的评估证明了该模型对多种疾病发作(特别是2型糖尿病)的预测价值，并说明了PRS和EHR数据之间的相互作用。该研究还探索了迁移学习用于定制分类任务，展示了架构的多功能性和效率。这种方法对于解锁疾病预测、主动健康管理、风险分层和个性化治疗策略的新见解至关重要，为医疗保健中更个性化、公平和可行的真实世界证据生成奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces an innovative Electronic Health Record (EHR) foundationmodel that integrates Polygenic Risk Scores (PRS) as a foundational datamodality, moving beyond traditional EHR-only approaches to build more holistichealth profiles. Leveraging the extensive and diverse data from the All of Us(AoU) Research Program, this multimodal framework aims to learn complexrelationships between clinical data and genetic predispositions. Themethodology extends advancements in generative AI to the EHR foundation modelspace, enhancing predictive capabilities and interpretability. Evaluation onAoU data demonstrates the model's predictive value for the onset of variousconditions, particularly Type 2 Diabetes (T2D), and illustrates the interplaybetween PRS and EHR data. The work also explores transfer learning for customclassification tasks, showcasing the architecture's versatility and efficiency.This approach is pivotal for unlocking new insights into disease prediction,proactive health management, risk stratification, and personalized treatmentstrategies, laying the groundwork for more personalized, equitable, andactionable real-world evidence generation in healthcare.</description>
      <author>example@mail.com (Jonathan Amar, Edward Liu, Alessandra Breschi, Liangliang Zhang, Pouya Kheradpour, Sylvia Li, Lisa Soleymani Lehmann, Alessandro Giulianelli, Matt Edwards, Yugang Jia, David Nola, Raghav Mani, Pankaj Vats, Jesse Tetreault, T. J. Chen, Cory Y. McLean)</author>
      <guid isPermaLink="false">2510.23639v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>An unsupervised tour through the hidden pathways of deep neural networks</title>
      <link>http://arxiv.org/abs/2510.21582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文旨在提高对深度人工神经网络创建有意义表示并能泛化的内部机制的理解，专注于使用无监督学习工具表征隐藏表示的语义内容。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络创建有意义表示和泛化的内部机制尚不完全清楚，需要工具来表征隐藏表示的语义内容并利用数据的低维结构。&lt;h4&gt;目的&lt;/h4&gt;改进对深度神经网络如何创建有意义表示并能泛化的内部机制的理解，开发无监督学习工具来表征隐藏表示的语义内容。&lt;h4&gt;方法&lt;/h4&gt;开发无监督学习工具利用数据的低维结构；介绍Gride方法估计数据内在维度作为尺度的显式函数；研究深度神经网络隐藏层概率密度的演变；分析深度神经网络中的泛化问题。&lt;h4&gt;主要发现&lt;/h4&gt;初始层产生单模态概率密度，消除与分类无关的结构；后续层中密度峰以分层方式出现，反映概念语义层次；输出层概率密度的峰地形可重建类别语义关系；宽神经网络学习冗余表示而非对虚假相关性过拟合；冗余神经元只在网络被正则化且训练误差为零时出现。&lt;h4&gt;结论&lt;/h4&gt;深度神经网络通过分层方式构建语义层次结构；增加参数到插值训练数据的网络会改善泛化性能，与经典偏差-方差权衡相悖；宽神经网络学习冗余表示而非过拟合。&lt;h4&gt;翻译&lt;/h4&gt;本论文的目标是提高我们对深度人工神经网络创建有意义表示并能泛化的内部机制的理解。我们专注于使用无监督学习工具表征隐藏表示的语义内容，这些工具由我们部分开发并在本论文中描述，它们能够利用数据的低维结构。第二章介绍了Gride，一种方法，允许将数据的内在维度估计为尺度的显式函数，而无需对数据集进行任何降采样。我们的方法基于严格的分布结果，能够量化估计的不确定性。此外，我们的方法简单且计算高效，因为它仅依赖于最近数据点之间的距离。在第三章中，我们研究了最先进深度神经网络中隐藏层概率密度的演变。我们发现初始层产生单模态概率密度，消除任何与分类无关的结构。在后续层中，密度峰以分层方式出现，反映了概念的语义层次结构。这个过程在输出层的概率密度中留下了痕迹，其中峰的地形允许重建类别的语义关系。在第四章中，我们研究了深度神经网络中的泛化问题：向插值训练数据的网络添加参数通常会改善其泛化性能，这与经典的偏差-方差权衡相悖。我们证明宽神经网络学习冗余表示而不是对虚假相关性过拟合，并且只有当网络被正则化且训练误差为零时，冗余神经元才会出现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this thesis is to improve our understanding of the internalmechanisms by which deep artificial neural networks create meaningfulrepresentations and are able to generalize. We focus on the challenge ofcharacterizing the semantic content of the hidden representations withunsupervised learning tools, partially developed by us and described in thisthesis, which allow harnessing the low-dimensional structure of the data.Chapter 2. introduces Gride, a method that allows estimating the intrinsicdimension of the data as an explicit function of the scale without performingany decimation of the data set. Our approach is based on rigorousdistributional results that enable the quantification of uncertainty of theestimates. Moreover, our method is simple and computationally efficient sinceit relies only on the distances among nearest data points. In Chapter 3, westudy the evolution of the probability density across the hidden layers in somestate-of-the-art deep neural networks. We find that the initial layers generatea unimodal probability density getting rid of any structure irrelevant toclassification. In subsequent layers, density peaks arise in a hierarchicalfashion that mirrors the semantic hierarchy of the concepts. This processleaves a footprint in the probability density of the output layer, where thetopography of the peaks allows reconstructing the semantic relationships of thecategories. In Chapter 4, we study the problem of generalization in deep neuralnetworks: adding parameters to a network that interpolates its training datawill typically improve its generalization performance, at odds with theclassical bias-variance trade-off. We show that wide neural networks learnredundant representations instead of overfitting to spurious correlation andthat redundant neurons appear only if the network is regularized and thetraining error is zero.</description>
      <author>example@mail.com (Diego Doimo)</author>
      <guid isPermaLink="false">2510.21582v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MFiSP: A Multimodal Fire Spread Prediction Framework</title>
      <link>http://arxiv.org/abs/2510.23934v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测以提高预测准确性，评估结果显示该方法优于传统火灾预测方法。&lt;h4&gt;背景&lt;/h4&gt;2019-2020年澳大利亚黑色夏季山火摧毁了1900万公顷土地，3000栋房屋，持续七个月，显示了野火威胁的规模和紧迫性，需要更好的预测来有效应对。&lt;h4&gt;目的&lt;/h4&gt;开发一种更准确的火灾蔓延预测方法，以应对日益严重的野火威胁，提高应急响应效率。&lt;h4&gt;方法&lt;/h4&gt;提出多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测，通过调整燃料图操纵策略动态调整火灾行为预测，使其与观察到的蔓延速率保持一致。&lt;h4&gt;主要发现&lt;/h4&gt;整合多模态数据的MFiSP可以提高火灾蔓延预测的准确性，超越依赖消防行为分析师专业知识和静态输入的传统方法。&lt;h4&gt;结论&lt;/h4&gt;新兴数据源如NASA的FIRMS卫星图像和自愿地理信息，结合多模态数据整合方法，能够显著改善火灾蔓延预测，为应对日益严重的野火威胁提供有效工具。&lt;h4&gt;翻译&lt;/h4&gt;2019-2020年澳大利亚黑色夏季山火摧毁了1900万公顷土地，3000栋房屋，持续七个月，显示了野火威胁规模和紧迫性的升级，需要更好的预测以实现有效应对。传统火灾建模依赖于消防行为分析师(FBAns)的手动解读和静态环境数据，常常导致不准确和操作限制。新兴数据源，如NASA的FIRMS卫星图像和自愿地理信息，通过实现动态火灾蔓延预测，提供了改进的可能性。本研究提出了一个多模态火灾蔓延预测框架(MFiSP)，整合社交媒体数据和遥感观测以提高预测准确性。通过在同化周期之间调整燃料图操纵策略，该框架动态调整火灾行为预测，以与观察到的蔓延速率保持一致。我们使用在不同场景中合成的火灾事件多边形评估MFiSP的有效性，分析个体和组合对预测边界的影响。结果表明，整合多模态数据的MFiSP可以提高火灾蔓延预测，超越依赖FBAn专业知识和静态输入的传统方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The 2019-2020 Black Summer bushfires in Australia devastated 19 millionhectares, destroyed 3,000 homes, and lasted seven months, demonstrating theescalating scale and urgency of wildfire threats requiring better forecastingfor effective response. Traditional fire modeling relies on manualinterpretation by Fire Behaviour Analysts (FBAns) and static environmentaldata, often leading to inaccuracies and operational limitations. Emerging datasources, such as NASA's FIRMS satellite imagery and Volunteered GeographicInformation, offer potential improvements by enabling dynamic fire spreadprediction. This study proposes a Multimodal Fire Spread Prediction Framework(MFiSP) that integrates social media data and remote sensing observations toenhance forecast accuracy. By adapting fuel map manipulation strategies betweenassimilation cycles, the framework dynamically adjusts fire behaviorpredictions to align with the observed rate of spread. We evaluate the efficacyof MFiSP using synthetically generated fire event polygons across multiplescenarios, analyzing individual and combined impacts on forecast perimeters.Results suggest that our MFiSP integrating multimodal data can improve firespread prediction beyond conventional methods reliant on FBAn expertise andstatic inputs.</description>
      <author>example@mail.com (Alec Sathiyamoorthy, Wenhao Zhou, Xiangmin Zhou, Xiaodong Li, Iqbal Gondal)</author>
      <guid isPermaLink="false">2510.23934v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DPRF: A Generalizable Dynamic Persona Refinement Framework for Optimizing Behavior Alignment Between Personalized LLM Role-Playing Agents and Humans</title>
      <link>http://arxiv.org/abs/2510.14205v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;动态人格完善框架（DPRF）通过迭代识别和解决认知差异，提高了LLM RPAs与目标个体行为的一致性&lt;h4&gt;背景&lt;/h4&gt;大语言模型角色扮演代理（LLM RPAs）旨在模拟个人人类行为，但人格保真度常因手动创建的档案（如精心挑选的信息和人格特征）而受损，这些档案未经与目标个体一致性的验证&lt;h4&gt;目的&lt;/h4&gt;解决上述限制，引入动态人格完善框架（DPRF），优化LLM RPAs行为与目标个体行为的一致性&lt;h4&gt;方法&lt;/h4&gt;DPRF通过迭代识别认知差异来优化LLM RPAs与目标个体行为的对齐，这些认知差异可以通过自由形式或基于理论的、结构化的分析来识别生成行为与人类真实情况之间的差异，并完善人格档案以减轻这些差异&lt;h4&gt;主要发现&lt;/h4&gt;在五个大语言模型和四种多样的行为预测场景上评估了DPRF，这些场景包括正式辩论、涉及心理健康问题的社交媒体帖子、公开访谈和电影评论，DPRF能够持续显著提高行为对齐度，优于基线人格，且能够跨模型和场景泛化&lt;h4&gt;结论&lt;/h4&gt;提供了一种创建高保真人格档案的稳健方法，提高了下游应用的有效性，如用户模拟、社会研究和个性化AI&lt;h4&gt;翻译&lt;/h4&gt;新兴的大语言模型角色扮演代理（LLM RPAs）旨在模拟个人人类行为，但人格保真度常因手动创建的档案（如精心挑选的信息和人格特征）而受损，这些档案未经与目标个体一致性的验证。为解决这一限制，我们的工作引入了动态人格完善框架（DPRF）。DPRF旨在通过迭代识别生成行为与人类真实情况之间的认知差异（无论是通过自由形式还是基于理论的、结构化的分析）来优化LLM RPAs行为与目标个体行为的一致性，并完善人格档案以减轻这些差异。我们在四个多样的行为预测场景中用五个大语言模型评估了DPRF：正式辩论、涉及心理健康问题的社交媒体帖子、公开访谈和电影评论。DPRF能够持续显著提高行为对齐度，优于基线人格，并且能够跨模型和场景泛化。我们的工作为创建高保真人格档案提供了一种稳健方法，并增强了下游应用的有效性，如用户模拟、社会研究和个性化AI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging large language model role-playing agents (LLM RPAs) aim tosimulate individual human behaviors, but the persona fidelity is oftenundermined by manually-created profiles (e.g., cherry-picked information andpersonality characteristics) without validating the alignment with the targetindividuals. To address this limitation, our work introduces the DynamicPersona Refinement Framework (DPRF).DPRF aims to optimize the alignment of LLMRPAs' behaviors with those of target individuals by iteratively identifying thecognitive divergence, either through free-form or theory-grounded, structuredanalysis, between generated behaviors and human ground truth, and refining thepersona profile to mitigate these divergences.We evaluate DPRF with five LLMson four diverse behavior-prediction scenarios: formal debates, social mediaposts with mental health issues, public interviews, and movie reviews.DPRF canconsistently improve behavioral alignment considerably over baseline personasand generalizes across models and scenarios.Our work provides a robustmethodology for creating high-fidelity persona profiles and enhancing thevalidity of downstream applications, such as user simulation, social studies,and personalized AI.</description>
      <author>example@mail.com (Bingsheng Yao, Bo Sun, Yuanzhe Dong, Yuxuan Lu, Dakuo Wang)</author>
      <guid isPermaLink="false">2510.14205v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ComboBench: Can LLMs Manipulate Physical Devices to Play Virtual Reality Games?</title>
      <link>http://arxiv.org/abs/2510.24706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文引入了ComboBench基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力，发现即使是顶级模型在程序推理和空间理解方面仍与人类存在差距。&lt;h4&gt;背景&lt;/h4&gt;虚拟现实游戏需要玩家将高级语义动作转换为使用控制器和头戴显示器的精确设备操作，而人类基于常识和具身理解直观地执行这种转换，但大型语言模型是否能有效复制这种能力尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一个名为ComboBench的基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力。&lt;h4&gt;方法&lt;/h4&gt;从四个流行的VR游戏(《半衰期：爱莉克斯》、《Into the Radius》、《Moss：Book II》和《Vivecraft》)的262个场景中评估七个大型语言模型(GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash)，并与标注的真实基线和人类表现进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;表现最佳的模型(如Gemini-1.5-Pro)展示了强大的任务分解能力，但在程序推理和空间理解方面仍与人类存在差距；不同游戏之间的性能差异显著，表明对交互复杂性的敏感性；少样本示例显著提高了性能，表明有潜力针对性地增强大型语言模型的VR操作能力。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型在VR设备操作序列生成方面仍有改进空间，特别是在程序推理和空间理解方面。&lt;h4&gt;翻译&lt;/h4&gt;虚拟现实游戏要求玩家使用控制器和头戴显示器将高级语义动作转换为精确的设备操作。虽然人类基于常识和具身理解直观地执行这种转换，但大型语言模型是否能有效复制这种能力尚未得到充分探索。本文引入了ComboBench基准测试，评估大型语言模型将语义动作转换为VR设备操作序列的能力，涵盖来自四个流行VR游戏(半衰期：爱莉克斯、Into the Radius、Moss：Book II和Vivecraft)的262个场景。我们评估了七个大型语言模型，包括GPT-3.5、GPT-4、GPT-4o、Gemini-1.5-Pro、LLaMA-3-8B、Mixtral-8x7B和GLM-4-Flash，并与标注的真实基线和人类表现进行比较。结果表明，尽管表现最佳的模型(如Gemini-1.5-Pro)展示了强大的任务分解能力，但在程序推理和空间理解方面仍与人类存在差距。不同游戏之间的性能差异显著，表明对交互复杂性的敏感性。少样本示例显著提高了性能，表明有潜力针对性地增强大型语言模型的VR操作能力。我们在https://sites.google.com/view/combobench发布了所有材料。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文研究大型语言模型（LLMs）是否能够有效地将高级语义动作（如'投降'、'驯服马匹'）转化为精确的VR设备操作序列（如'按X键'、'将头显朝向苦力怕'）。这个问题重要是因为VR游戏需要玩家将抽象意图转化为具体物理操作，这种能力是人类智能的关键组成部分，但目前尚不清楚LLMs是否具备这种具身认知能力，这对开发更智能的虚拟代理和游戏AI具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先通过认知科学专家访谈确定了VR交互所需的六种核心认知能力（任务分解、程序推理等），然后系统性地选择了四款代表性VR游戏，提取了262个语义动作场景，并由经验VR玩家进行精细的设备操作序列标注。他们借鉴了机器人系统（如SayCan）、虚拟环境智能体（如Voyager）和多步骤规划策略（如Chain-of-Thought）的工作，但专注于VR设备操作的精确映射，而非代码生成或机器人控制。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过构建全面基准测试ComboBench评估LLMs将语义动作转化为VR设备操作的能力，并采用多维度认知框架分析其表现。整体流程包括：1)构建基准：确定认知能力、选择游戏、提取场景、标注操作序列和认知能力；2)模型评估：在多个模型和少样本设置下测试；3)性能分析：比较LLMs与人类表现、分析认知能力差异、研究少样本影响和游戏复杂度关系。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个专门评估LLMs将语义动作转化为VR设备操作能力的基准测试；2)建立六种核心认知能力框架并实现步骤级别标注；3)设计多维度评估指标全面分析性能；4)收集四款不同类型VR游戏的262个场景提供多样化测试环境；5)通过与人类对比揭示LLMs在具身认知方面的优势和不足。相比之前工作，ComboBench专注于VR环境中的物理设备操作而非代码生成、机器人控制或平面界面操作。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了ComboBench基准测试，首次系统评估了大型语言模型将高级语义动作转化为VR设备操作的能力，揭示了当前LLMs在具身认知方面的优势与局限，为开发更智能的VR交互AI提供了重要指导。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Virtual Reality (VR) games require players to translate high-level semanticactions into precise device manipulations using controllers and head-mounteddisplays (HMDs). While humans intuitively perform this translation based oncommon sense and embodied understanding, whether Large Language Models (LLMs)can effectively replicate this ability remains underexplored. This paperintroduces a benchmark, ComboBench, evaluating LLMs' capability to translatesemantic actions into VR device manipulation sequences across 262 scenariosfrom four popular VR games: Half-Life: Alyx, Into the Radius, Moss: Book II,and Vivecraft. We evaluate seven LLMs, including GPT-3.5, GPT-4, GPT-4o,Gemini-1.5-Pro, LLaMA-3-8B, Mixtral-8x7B, and GLM-4-Flash, compared againstannotated ground truth and human performance. Our results reveal that whiletop-performing models like Gemini-1.5-Pro demonstrate strong task decompositioncapabilities, they still struggle with procedural reasoning and spatialunderstanding compared to humans. Performance varies significantly acrossgames, suggesting sensitivity to interaction complexity. Few-shot examplessubstantially improve performance, indicating potential for targetedenhancement of LLMs' VR manipulation capabilities. We release all materials athttps://sites.google.com/view/combobench.</description>
      <author>example@mail.com (Shuqing Li, Jiayi Yan, Chenyu Niu, Jen-tse Huang, Yun Peng, Wenxuan Wang, Yepang Liu, Michael R. Lyu)</author>
      <guid isPermaLink="false">2510.24706v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Sound Source Localization for Spatial Mapping of Surgical Actions in Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2510.24332v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种创新方法，通过整合3D声学信息和视觉数据，增强了手术场景的理解。该方法能够将声学事件在3D空间中定位并与视觉元素关联，实验证明在真实手术室环境中表现良好。&lt;h4&gt;背景&lt;/h4&gt;手术场景理解对于推进计算机辅助和智能手术系统至关重要。当前方法主要依赖视觉数据或端到端学习，这限制了细粒度上下文建模。&lt;h4&gt;目的&lt;/h4&gt;通过整合3D声学信息来增强手术场景表示，实现对手术环境在时间和空间上的多模态理解。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的框架，用于生成手术场景的4D视听表示。通过将相控麦克风阵列的声学定位信息投影到RGB-D相机的动态点云上，并使用基于Transformer的声学事件检测模块识别包含工具-组织相互作用的相关时间片段。在专家执行的模拟手术程序期间，在真实的手术室设置中进行了实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法成功地将手术声学事件在3D空间中定位，并与视觉场景元素关联。实验评估证明了精确的空间声音定位和多模态数据的稳健融合，提供了手术活动的全面、动态表示。&lt;h4&gt;结论&lt;/h4&gt;这项工作首次引入了动态手术场景中的空间声音定位方法，向多模态手术场景表示迈出了重要一步。通过整合声学和视觉数据，所提出的框架实现了更丰富的上下文理解，为未来的智能和自主手术系统奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;目的：手术场景理解对于推进计算机辅助和智能手术系统至关重要。当前方法主要依赖视觉数据或端到端学习，这限制了细粒度上下文建模。这项工作旨在通过整合3D声学信息来增强手术场景表示，实现对手术环境在时间和空间上的多模态理解。方法：我们提出了一种新颖的框架，通过将相控麦克风阵列的声学定位信息投影到RGB-D相机的动态点云上，生成手术场景的4D视听表示。基于Transformer的声学事件检测模块识别包含工具-组织相互作用的相关时间片段，这些片段在视听场景表示中进行空间定位。系统在专家执行的模拟手术程序期间，在真实的手术室设置中进行了实验评估。结果：所提出的方法成功地将手术声学事件在3D空间中定位，并将它们与视觉场景元素关联。实验评估证明了精确的空间声音定位和多模态数据的稳健融合，提供了手术活动的全面、动态表示。结论：这项工作首次引入了动态手术场景中的空间声音定位方法，向多模态手术场景表示迈出了重要一步。通过整合声学和视觉数据，所提出的框架实现了更丰富的上下文理解，为未来的智能和自主手术系统奠定了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决手术场景中的声学事件定位问题，目的是通过整合声学信息来增强手术场景的数字表示。这个问题很重要，因为当前手术场景理解主要依赖视觉数据，无法捕捉工具-组织相互作用的细粒度信息，而声学信息可以提供视觉无法获取的关键细节，对于开发智能手术系统和提高手术安全性与效率至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有手术场景理解方法的局限性，然后提出多模态融合的思路，认为结合声学和视觉信息可以提供更全面的手术场景理解。他们借鉴了AudioSpectrogramTransformer模型进行声学事件检测，利用现有的声学波束形成技术生成2D声学热图，并参考了点云处理和3D定位技术。整个系统设计围绕如何有效融合声学和视觉信息，创建时空一致的4D手术场景表示。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过融合声学和视觉信息创建更全面的4D（3D空间+时间）手术场景表示，利用声学事件补充视觉信息，提供工具-组织相互作用的上下文。整体流程包括：1)使用相控麦克风阵列和RGB-D相机采集多模态数据；2)通过波束形成生成2D声学热图并投影到3D点云上；3)使用transformer模型检测手术声学事件；4)通过聚类算法定位声源并生成3D边界框；5)评估系统性能。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在动态手术场景中进行空间声音定位；2)提出4D音频-视觉手术场景表示的新概念；3)基于transformer的声学事件检测方法；4)有效的多模态融合方法。相比之前工作，本文不仅整合了声学和视觉两种模态信息，还创建了时空一致的4D表示，专注于细粒度的声学事件检测和空间定位，而非高层概念预测，提供了更好的可解释性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次通过融合声学和视觉信息创建了4D动态手术场景表示，实现了手术声学事件的空间定位，为开发更智能、更全面的手术理解系统奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Purpose: Surgical scene understanding is key to advancing computer-aided andintelligent surgical systems. Current approaches predominantly rely on visualdata or end-to-end learning, which limits fine-grained contextual modeling.This work aims to enhance surgical scene representations by integrating 3Dacoustic information, enabling temporally and spatially aware multimodalunderstanding of surgical environments.  Methods: We propose a novel framework for generating 4D audio-visualrepresentations of surgical scenes by projecting acoustic localizationinformation from a phased microphone array onto dynamic point clouds from anRGB-D camera. A transformer-based acoustic event detection module identifiesrelevant temporal segments containing tool-tissue interactions which arespatially localized in the audio-visual scene representation. The system wasexperimentally evaluated in a realistic operating room setup during simulatedsurgical procedures performed by experts.  Results: The proposed method successfully localizes surgical acoustic eventsin 3D space and associates them with visual scene elements. Experimentalevaluation demonstrates accurate spatial sound localization and robust fusionof multimodal data, providing a comprehensive, dynamic representation ofsurgical activity.  Conclusion: This work introduces the first approach for spatial soundlocalization in dynamic surgical scenes, marking a significant advancementtoward multimodal surgical scene representations. By integrating acoustic andvisual data, the proposed framework enables richer contextual understanding andprovides a foundation for future intelligent and autonomous surgical systems.</description>
      <author>example@mail.com (Jonas Hein, Lazaros Vlachopoulos, Maurits Geert Laurent Olthof, Bastian Sigrist, Philipp Fürnstahl, Matthias Seibold)</author>
      <guid isPermaLink="false">2510.24332v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Vision-Language Models for Autonomous Driving through Task-Specific Prompting and Spatial Reasoning</title>
      <link>http://arxiv.org/abs/2510.24152v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  RoboSense Challenge with IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个系统性框架，用于提高视觉语言模型在自动驾驶场景理解任务中的性能，通过四个核心组件实现问题分类、任务特定提示设计、视觉信息组装和模型参数优化。&lt;h4&gt;背景&lt;/h4&gt;IROS 2025 RoboSense Challenge评估视觉语言模型在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和损坏检测四个任务领域。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效的框架，提升视觉语言模型在安全关键型自动驾驶任务中的表现，特别是在处理干净数据和损坏数据时的准确率。&lt;h4&gt;方法&lt;/h4&gt;构建了一个四组件框架：1)混合提示路由器分类并分派问题；2)特定任务提示嵌入坐标系、空间推理规则等；3)视觉组装模块组合多视图图像；4)按任务配置模型推理参数。&lt;h4&gt;主要发现&lt;/h4&gt;在Qwen2.5-VL-72B模型上实现，该方法在第一阶段(干净数据)达到70.87%平均准确率，在第二阶段(损坏数据)达到72.85%准确率。&lt;h4&gt;结论&lt;/h4&gt;结构化提示和空间接地能显著提升视觉语言模型在安全关键型自动驾驶任务中的性能。&lt;h4&gt;翻译&lt;/h4&gt;本技术报告介绍了我们在IROS 2025 RoboSense Challenge上的解决方案，该方案评估视觉语言模型在自动驾驶场景理解方面的能力，涵盖感知、预测、规划和损坏检测任务。我们提出了一个基于四个核心组件构建的系统性框架。首先，混合提示路由器对问题进行分类并将其分派给特定任务的专家提示，消除了不同问题类型之间的干扰。其次，特定任务提示嵌入明确的坐标系、空间推理规则、角色扮演、思维链/思维树推理以及为每个任务定制的小样本示例。第三，视觉组装模块根据问题要求组合多视图图像、对象裁剪、洋红色标记和自适应历史帧。第四，我们按任务配置模型推理参数(温度、top-p、消息角色)以优化输出质量。在Qwen2.5-VL-72B上实现，我们的方法在第一阶段(干净数据)上平均准确率达到70.87%，在第二阶段(损坏数据)上达到72.85%，证明结构化提示和空间接地显著提高了VLM在安全关键型自动驾驶任务上的性能。代码和提示可在https://github.com/wuaodi/UCAS-CSU-phase2获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决视觉-语言模型(VLMs)在自动驾驶场景理解中面临的三个关键挑战：多视角场景中的空间推理困难(如混淆左右方向、错误判断物体位置)、不同任务类型之间的提示干扰(单一通用提示难以同时优化感知、预测、规划等不同任务)、以及时间上下文集成不当(添加历史帧可能引入噪声而非有用信息)。这些问题在现实中非常重要，因为自动驾驶系统需要准确的场景理解才能做出安全决策，解决这些问题能显著提高VLMs在安全关键自动驾驶任务中的可靠性和性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过三个关键设计原则来解决问题：消除提示干扰(使用专家提示而非通用提示)、增强空间推理(明确定义坐标系统和约束)、以及自适应时间上下文(根据问题类型选择适当历史帧)。作者借鉴了多项现有工作，包括Mixture-of-Prompts(使用多个专家提示)、Role-playing(为模型分配特定角色)、Chain-of-Thought/Tree-of-Thought推理(逐步推理和探索多种可能性)、In-context learning(通过示例引导模型)以及Visual prompting(视觉注意力引导)。作者将这些现有技术组合并调整，以适应自动驾驶场景的特殊需求，特别是空间推理和时间上下文处理。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过系统化的提示工程增强VLM在自动驾驶场景理解中的性能，特别关注空间推理和任务特定提示设计。整体实现流程包含四个核心组件：1)路由器：分类测试查询并分配到适当的任务专家提示；2)任务特定提示：包含坐标系统、空间规则、角色扮演、链式/树式思维推理和少样本示例；3)视觉组装模块：根据问题需求组合多视角图像、物体裁剪、标记和自适应历史帧；4)模型选择和推理参数：使用Qwen2.5-VL-72B并根据任务类型调整推理参数。流程是：路由器分类问题→选择任务特定提示→组合视觉输入→使用特定推理参数调用VLM生成答案。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)Mixture-of-Prompts路由器，消除不同任务类型间的提示干扰；2)明确的坐标系统和空间规则，增强多视角空间定位能力；3)自适应视觉组装，根据问题类型组合视觉输入；4)任务特定的推理参数，优化不同任务类型的输出质量；5)结合多种推理技术，为不同任务定制推理策略。相比之前工作，本文的主要不同在于不是通过微调模型来增强性能，而是通过提示级别的设计(明确空间定位、自适应时间证据和任务特定路由)来提高可靠性，这种方法在保持基础模型不变的情况下显著提升了性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于任务特定提示和空间推理的系统化框架，显著提升了视觉-语言模型在自动驾驶场景理解任务中的性能，特别是在多视角空间定位和不同任务类型处理方面。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This technical report presents our solution for the RoboSense Challenge atIROS 2025, which evaluates Vision-Language Models (VLMs) on autonomous drivingscene understanding across perception, prediction, planning, and corruptiondetection tasks. We propose a systematic framework built on four corecomponents. First, a Mixture-of-Prompts router classifies questions anddispatches them to task-specific expert prompts, eliminating interferenceacross diverse question types. Second, task-specific prompts embed explicitcoordinate systems, spatial reasoning rules, role-playing,Chain-of-Thought/Tree-of-Thought reasoning, and few-shot examples tailored toeach task. Third, a visual assembly module composes multi-view images withobject crops, magenta markers, and adaptive historical frames based on questionrequirements. Fourth, we configure model inference parameters (temperature,top-p, message roles) per task to optimize output quality. Implemented onQwen2.5-VL-72B, our approach achieves 70.87% average accuracy on Phase-1 (cleandata) and 72.85% on Phase-2 (corrupted data), demonstrating that structuredprompting and spatial grounding substantially enhance VLM performance onsafety-critical autonomous driving tasks. Code and prompt are available athttps://github.com/wuaodi/UCAS-CSU-phase2.</description>
      <author>example@mail.com (Aodi Wu, Xubo Luo)</author>
      <guid isPermaLink="false">2510.24152v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Concerto: Joint 2D-3D Self-Supervised Learning Emerges Spatial Representations</title>
      <link>http://arxiv.org/abs/2510.23607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025, produced by Pointcept, project page:  https://pointcept.github.io/Concerto&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Concerto是一个模拟人类概念学习的模型，通过结合3D模态内自蒸馏和2D-3D跨模态联合嵌入，学习空间认知中的抽象概念，表现出优越的性能。&lt;h4&gt;背景&lt;/h4&gt;人类通过多感官协同学习抽象概念，一旦形成，可以从单一感官回忆这些表示。&lt;h4&gt;目的&lt;/h4&gt;受人类学习原理启发，开发一个用于空间认知的概念学习模型。&lt;h4&gt;方法&lt;/h4&gt;Concerto结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入的方法。&lt;h4&gt;主要发现&lt;/h4&gt;Concerto学习到更连贯和信息丰富的空间特征；在零样本可视化中表现出色；在线性探测中分别比最先进的2D和3D自监督模型高出14.2%和4.8%；完全微调后在多个场景理解基准测试中设置了新的最先进结果；提出了针对视频提升点云空间理解的变体；开发了将表示投影到CLIP语言空间的翻译器，实现开放世界感知。&lt;h4&gt;结论&lt;/h4&gt;Concerto产生了具有优越细粒度几何和语义一致性的空间表示。&lt;h4&gt;翻译&lt;/h4&gt;人类通过多感官协同学习抽象概念，一旦形成，这样的表示通常可以从单一感官回忆。受这一原理启发，我们引入了Concerto，这是一个用于空间认知的人类概念学习的极简模拟，结合了3D模态内自蒸馏和2D-3D跨模态联合嵌入。尽管简单，但Concerto学习到更连贯和信息丰富的空间特征，如零样本可视化所示。在3D场景感知的线性探测中，它分别比独立的最新2D和3D自监督模型高出14.2%和4.8%，也优于它们的特征连接。通过完全微调，Concerto在多个场景理解基准测试中设置了新的最新结果（例如在ScanNet上达到80.7% mIoU）。我们进一步提出了一个针对视频提升点云空间理解的Concerto变体，以及一个将Concerto表示线性投影到CLIP语言空间的翻译器，实现开放世界感知。这些结果表明，Concerto产生了具有优越细粒度几何和语义一致性的空间表示。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何通过联合2D图像和3D点云的自监督学习，学习更丰富、更一致的空间表示问题。这个问题在现实中很重要，因为空间认知是自动驾驶、混合现实和机器人等应用的基础，而多模态学习可以提供更全面的空间理解，减少对标注数据的依赖，使模型能够从大量无标签数据中学习更强大的表示。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受人类多感官协同学习抽象过程的启发，认识到人类可以通过不同感官（如视觉和触觉）形成统一概念，并能从单一感官唤起完整体验。他们首先进行了初步研究，验证了简单拼接2D和3D特征优于单一模态，进而设计了更复杂的框架。该方法借鉴了Sonata框架用于3D点云表示学习的单模态自蒸馏技术，以及基于LeCun的联合嵌入预测架构(JEPA)的跨模态对齐方法，将两者结合形成Concerto框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过模拟人类多感官协同学习的方式，结合2D图像和3D点云的自监督学习，学习更丰富、更一致的空间表示，使得模型能够从单一模态中唤出完整的空间概念。整体实现流程包括：1) 单模态自蒸馏：使用Point Transformer V3作为点云编码器，通过教师-学生范式训练，使用在线聚类目标函数增强一致性；2) 跨模态联合嵌入预测：使用预训练图像编码器提取特征，建立点云点和图像像素对应关系，预测点云特征以匹配图像特征，使用余弦相似度作为损失；3) 协同训练：结合两个目标函数，适当平衡权重，训练出能从单一模态唤出丰富空间表示的模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 多模态协同学习框架，通过跨模态预测而非简单特征融合学习统一表示；2) 自监督点云Transformer，结合单模态自蒸馏和跨模态联合嵌入；3) 视频感知变体，利用前馈重建从视频中生成点云数据；4) 语言桥接，将表示映射到CLIP语言空间实现开放词汇感知。相比之前的工作，Concerto不仅整合了2D图像和3D点云信息，还通过联合学习产生了比单一模态或简单特征拼接更丰富、更一致的表示空间，在多个场景理解任务上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Concerto通过模拟人类多感官协同学习的方式，联合2D图像和3D点云的自监督学习，学习到了比单一模态更丰富、更一致的空间表示，并在多个场景理解任务上取得了最先进的性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans learn abstract concepts through multisensory synergy, and once formed,such representations can often be recalled from a single modality. Inspired bythis principle, we introduce Concerto, a minimalist simulation of human conceptlearning for spatial cognition, combining 3D intra-modal self-distillation with2D-3D cross-modal joint embedding. Despite its simplicity, Concerto learns morecoherent and informative spatial features, as demonstrated by zero-shotvisualizations. It outperforms both standalone SOTA 2D and 3D self-supervisedmodels by 14.2% and 4.8%, respectively, as well as their feature concatenation,in linear probing for 3D scene perception. With full fine-tuning, Concerto setsnew SOTA results across multiple scene understanding benchmarks (e.g., 80.7%mIoU on ScanNet). We further present a variant of Concerto tailored forvideo-lifted point cloud spatial understanding, and a translator that linearlyprojects Concerto representations into CLIP's language space, enablingopen-world perception. These results highlight that Concerto emerges spatialrepresentations with superior fine-grained geometric and semantic consistency.</description>
      <author>example@mail.com (Yujia Zhang, Xiaoyang Wu, Yixing Lao, Chengyao Wang, Zhuotao Tian, Naiyan Wang, Hengshuang Zhao)</author>
      <guid isPermaLink="false">2510.23607v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Localising under the drape: proprioception in the era of distributed surgical robotic system</title>
      <link>http://arxiv.org/abs/2510.23512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种无需标记的手术机器人本体感觉方法，通过轻量级立体RGB摄像头和基于Transformer的深度学习模型，实现了在无菌遮挡情况下的精确定位，提高了手术场景的可见性和追踪能力。&lt;h4&gt;背景&lt;/h4&gt;手术机器人虽然机械精密，但对周围环境缺乏感知能力，导致碰撞、系统恢复和工作流程中断等问题。现有的追踪系统依赖笨重的红外摄像头和反射标记，只能提供有限视角并增加手术室硬件负担。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需标记的本体感觉方法，使手术机器人在无菌遮挡情况下能够精确定位，提高手术场景的可见性和追踪能力，减少硬件负担，提高手术安全性。&lt;h4&gt;方法&lt;/h4&gt;使用轻量级立体RGB摄像头和基于Transformer的新型深度学习模型。基于最大的多中心空间机器人手术数据集（140万张来自人体尸体和临床前体内研究的自注释图像），通过跟踪整个机器人和手术场景而非单个标记来实现定位。&lt;h4&gt;主要发现&lt;/h4&gt;该方法提供对遮挡具有鲁棒性的整体视图，支持手术场景理解和上下文感知控制。在体内呼吸补偿中展示了临床应用潜力，可获取组织动力学；在多机器人系统中实现精确定位。与现有系统相比，消除了标记并将追踪可见性提高了25%。&lt;h4&gt;结论&lt;/h4&gt;这是首次展示完全覆盖的手术机器人的无标记本体感觉，降低了设置复杂性，提高了安全性，并为模块化和自主机器人手术铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;尽管手术机器人具有机械精密性，但它们仍然无法感知周围环境。这种空间意识的缺乏导致碰撞、系统恢复和工作流程中断等问题，随着具有独立交互臂的分布式机器人的引入，这些问题将加剧。现有的追踪系统依赖笨重的红外摄像头和反射标记，仅提供手术场景的有限视角，并在拥挤的手术室中增加硬件负担。我们提出了一种无需标记的本体感觉方法，使手术机器人在无菌遮挡的情况下能够精确定位，尽管视觉线索被遮挡。我们的方法仅依靠轻量级立体RGB摄像头和基于Transformer的新型深度学习模型。它基于迄今为止最大的多中心空间机器人手术数据集（来自人体尸体和临床前体内研究的140万张自注释图像）。通过跟踪整个机器人和手术场景，而不是单个标记，我们的方法提供了对遮挡具有鲁棒性的整体视图，支持手术场景理解和上下文感知控制。我们展示了体内呼吸补偿的潜在临床应用示例，可以获取最先进追踪技术无法观察到的组织动力学，并在多机器人系统中精确定位以支持未来的智能交互。此外，与现有系统相比，我们的方法消除了标记并将追踪可见性提高了25%。据我们所知，这是首次展示完全覆盖的手术机器人的无标记本体感觉，降低了设置复杂性，提高了安全性，并为模块化和自主机器人手术铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决手术机器人在被无菌布覆盖后无法精确定位的问题。这个问题很重要，因为当前手术机器人缺乏环境感知能力，会导致碰撞、系统恢复和工作流程中断，随着分布式多臂机器人系统的普及，这些问题会更加严重。现有的红外跟踪系统笨重、容易被遮挡、需要严格校准，且难以扩展到多机器人环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有红外跟踪系统的局限性，然后设计了基于轻量级立体RGB相机和Transformer深度学习模型的解决方案。他们借鉴了工业机器人中的无标记定位方法，但进行了修改以适应外科环境中的无菌布遮挡问题。方法核心是立体可微分渲染技术，结合了粒子群优化算法进行初始估计，并通过'上下文先验'方法迭代改进分割结果。作者还构建了140万张图像的大型多中心数据集来训练模型。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用轻量级立体RGB相机和深度学习模型，通过立体可微分渲染技术实现对被覆盖手术机器人的精确定位，无需传统反射标记。整体流程包括：1)收集和预处理多中心数据集；2)训练能够处理遮挡的分割模型；3)使用相机群优化进行初始估计；4)应用立体可微分渲染优化姿态估计；5)使用'上下文先验'方法迭代改进结果；6)在临床前和临床环境中验证方法。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次实现完全覆盖的无标记手术机器人定位；2)开发立体可微分渲染技术处理遮挡；3)构建最大规模的多中心手术机器人数据集；4)开发遮挡不变的分割方法；5)提出'上下文先验'方法改进分割；6)支持多机器人设置。相比之前工作，本文方法无需标记、硬件更轻量(轻13倍、体积小3倍)、提供更完整的场景理解、能捕获传统系统不可见的组织动态，并在真实临床条件下验证了亚毫米级定位精度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于轻量级立体RGB相机和深度学习的无标记定位方法，实现了对被无菌布覆盖的手术机器人的精确定位，提高了手术场景可见性并揭示了传统系统无法观察到的组织动态，为模块化和自主机器人手术铺平了道路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite their mechanical sophistication, surgical robots remain blind totheir surroundings. This lack of spatial awareness causes collisions, systemrecoveries, and workflow disruptions, issues that will intensify with theintroduction of distributed robots with independent interacting arms. Existingtracking systems rely on bulky infrared cameras and reflective markers,providing only limited views of the surgical scene and adding hardware burdenin crowded operating rooms. We present a marker-free proprioception method thatenables precise localisation of surgical robots under their sterile drapingdespite associated obstruction of visual cues. Our method solely relies onlightweight stereo-RGB cameras and novel transformer-based deep learningmodels. It builds on the largest multi-centre spatial robotic surgery datasetto date (1.4M self-annotated images from human cadaveric and preclinical invivo studies). By tracking the entire robot and surgical scene, rather thanindividual markers, our approach provides a holistic view robust to occlusions,supporting surgical scene understanding and context-aware control. Wedemonstrate an example of potential clinical benefits during in vivo breathingcompensation with access to tissue dynamics, unobservable under state of theart tracking, and accurately locate in multi-robot systems for futureintelligent interaction. In addition, and compared with existing systems, ourmethod eliminates markers and improves tracking visibility by 25%. To ourknowledge, this is the first demonstration of marker-free proprioception forfully draped surgical robots, reducing setup complexity, enhancing safety, andpaving the way toward modular and autonomous robotic surgery.</description>
      <author>example@mail.com (Martin Huber, Nicola A. Cavalcanti, Ayoob Davoodi, Ruixuan Li, Christopher E. Mower, Fabio Carrillo, Christoph J. Laux, Francois Teyssere, Thibault Chandanson, Antoine Harlé, Elie Saghbiny, Mazda Farshad, Guillaume Morel, Emmanuel Vander Poorten, Philipp Fürnstahl, Sébastien Ourselin, Christos Bergeles, Tom Vercauteren)</author>
      <guid isPermaLink="false">2510.23512v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UrbanIng-V2X: A Large-Scale Multi-Vehicle, Multi-Infrastructure Dataset Across Multiple Intersections for Cooperative Perception</title>
      <link>http://arxiv.org/abs/2510.23478v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Including supplemental material. For code  and dataset, see https://github.com/thi-ad/UrbanIng-V2X&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UrbanIng-V2X是首个大规模、多模态数据集，支持德国Ingolstadt三个城市交叉路口的车辆和基础设施传感器之间的合作感知，包含34个20秒的传感器序列，提供多种传感器数据，并以10Hz频率标注3D边界框。&lt;h4&gt;背景&lt;/h4&gt;现有合作感知数据集在智能移动应用中起关键作用，但真实世界数据集通常仅限于单个交叉路口或单辆车，缺乏多连接车辆和基础设施传感器跨越多个交叉路口的全面感知数据集，限制了算法在多样化交通环境中的基准测试。&lt;h4&gt;目的&lt;/h4&gt;解决现有数据集的局限性，引入首个大规模、多模态合作感知数据集UrbanIng-V2X，支持车辆和基础设施传感器之间的合作感知。&lt;h4&gt;方法&lt;/h4&gt;在德国Ingolstadt的三个城市交叉路口部署传感器，收集34个时间对齐和空间校准的传感器序列(每个20秒)，涉及两辆车和最多三个基础设施传感器杆，使用12个车载RGB摄像头、2个车载LiDAR、17个基础设施热成像摄像头和12个基础设施LiDAR，以10Hz频率标注13个类别的3D边界框。&lt;h4&gt;主要发现&lt;/h4&gt;提供使用最先进的合作感知方法的全面评估，验证了数据集的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;公开提供代码库、数据集、高清地图和完整数据收集环境的数字孪生，促进合作感知领域的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;近期的合作感知数据集通过促进智能体之间的信息交换，在推进智能移动应用方面发挥了关键作用，帮助克服遮挡等挑战，并提高整体场景理解能力。虽然一些现有的真实世界数据集同时包含车辆对车辆和车辆对基础设施的交互，但它们通常仅限于单个交叉路口或单辆车。一个包含多个连接车辆和基础设施传感器跨越多个交叉路口的全面感知数据集仍然不可用，限制了算法在多样化交通环境中的基准测试。因此，可能会发生过拟合，模型可能由于相似的交叉路口布局和交通参与者行为而表现出误导性的高性能。为解决这一差距，我们引入了UrbanIng-V2X，这是首个大规模、多模态数据集，支持在德国Ingolstadt三个城市交叉路口部署的车辆和基础设施传感器之间的合作感知。UrbanIng-V2X包含34个时间对齐和空间校准的传感器序列，每个持续20秒。所有序列包含三个交叉路口中一个的记录，涉及两辆车和最多三个基础设施安装的传感器杆，在协调场景中运行。总的来说，UrbanIng-V2X提供来自12个车载RGB摄像头、2个车载LiDAR、17个基础设施热成像摄像头和12个基础设施LiDAR的数据。所有序列以10Hz的频率标注3D边界框，涵盖13个对象类别，导致整个数据集大约有712k个标注实例。我们使用最先进的合作感知方法提供了全面评估，并公开提供了代码库、数据集、高清地图和完整数据收集环境的数字孪生。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是缺乏一个大规模、多车辆、多基础设施、多交叉路口的合作感知数据集。这个问题在现实中很重要，因为城市交叉路口是自动驾驶中最复杂的场景之一，单一智能体的感知系统常因视野受限和遮挡而无法检测关键物体，而合作感知可以克服这些限制。缺乏多样性的数据集会导致算法过拟合，模型可能因相似场景而表现出误导性的高性能，限制了真实世界应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有数据集的不足（如只包含单个交叉路口或车辆、缺乏多种传感器类型）来设计方法。他们精心设计了传感器部署（两辆车各配备6个RGB摄像头和1个激光雷达，三个交叉路口配备热成像摄像头和激光雷达系统），实现了精确的传感器同步和校准，并从8小时数据中挑选34个代表性场景进行标注。作者借鉴了现有工作如V2V4Real、DAIR-V2X-C等数据集的经验，同时采用了类似nuScenes的标注方法和OpenCOOD的格式转换工具。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建一个大规模、多模态、多交叉路口的合作感知数据集，使研究人员能够开发和评估在复杂城市环境中能有效协作的感知算法。整体流程包括：1)传感器部署（车载和基础设施）；2)数据采集（三个交叉路口34个20秒序列）；3)传感器同步与校准（UTC时钟同步、精确校准）；4)场景选择与标注（多样化光照条件、10Hz频率标注13个物体类别）；5)数据发布与工具提供（代码库、高清地图、数字孪生）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个多车辆、多基础设施、多交叉路口的合作感知数据集；2)引入最多的合作传感器和热成像相机等多模态传感器；3)支持多种合作感知基准任务；4)提供综合基准评估；5)提供开发者工具包和数字孪生。相比之前工作，UrbanIng-V2X同时支持多车辆和多基础设施，覆盖多个交叉路口，提供更丰富的传感器组合和更全面的标注（13个类别、712k实例），还提供了数字孪生环境和新的数据集分割策略以评估泛化能力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; UrbanIng-V2X数据集通过提供首个大规模、多车辆、多基础设施、多交叉路口的合作感知数据集，克服了现有数据集的局限性，为开发和评估在复杂城市环境中能有效协作的感知算法提供了坚实基础，同时揭示了模型在未见环境中的泛化挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent cooperative perception datasets have played a crucial role inadvancing smart mobility applications by enabling information exchange betweenintelligent agents, helping to overcome challenges such as occlusions andimproving overall scene understanding. While some existing real-world datasetsincorporate both vehicle-to-vehicle and vehicle-to-infrastructure interactions,they are typically limited to a single intersection or a single vehicle. Acomprehensive perception dataset featuring multiple connected vehicles andinfrastructure sensors across several intersections remains unavailable,limiting the benchmarking of algorithms in diverse traffic environments.Consequently, overfitting can occur, and models may demonstrate misleadinglyhigh performance due to similar intersection layouts and traffic participantbehavior. To address this gap, we introduce UrbanIng-V2X, the firstlarge-scale, multi-modal dataset supporting cooperative perception involvingvehicles and infrastructure sensors deployed across three urban intersectionsin Ingolstadt, Germany. UrbanIng-V2X consists of 34 temporally aligned andspatially calibrated sensor sequences, each lasting 20 seconds. All sequencescontain recordings from one of three intersections, involving two vehicles andup to three infrastructure-mounted sensor poles operating in coordinatedscenarios. In total, UrbanIng-V2X provides data from 12 vehicle-mounted RGBcameras, 2 vehicle LiDARs, 17 infrastructure thermal cameras, and 12infrastructure LiDARs. All sequences are annotated at a frequency of 10 Hz with3D bounding boxes spanning 13 object classes, resulting in approximately 712kannotated instances across the dataset. We provide comprehensive evaluationsusing state-of-the-art cooperative perception methods and publicly release thecodebase, dataset, HD map, and a digital twin of the complete data collectionenvironment.</description>
      <author>example@mail.com (Karthikeyan Chandra Sekaran, Markus Geisler, Dominik Rößle, Adithya Mohan, Daniel Cremers, Wolfgang Utschick, Michael Botsch, Werner Huber, Torsten Schön)</author>
      <guid isPermaLink="false">2510.23478v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>VEHME: A Vision-Language Model For Evaluating Handwritten Mathematics Expressions</title>
      <link>http://arxiv.org/abs/2510.22798v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  EMNLP 2025. Project Website: https://vehme.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍VEHME，一种用于评估手写数学表达式的视觉语言模型，能够以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。&lt;h4&gt;背景&lt;/h4&gt;自动评估手写数学解题是教育技术中的重要问题，具有实际应用，但由于学生作业的多样格式、非结构化布局和符号复杂性，这仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;开发VEHME模型，以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。&lt;h4&gt;方法&lt;/h4&gt;VEHME采用两阶段训练管道：使用结构化推理数据进行监督微调；通过强化学习使模型输出与多维度评分目标（正确性、推理深度和错误定位）保持一致；提出表达式感知的视觉提示模块，在合成的多行数学表达式数据集上训练，以在视觉异构输入中稳健地引导注意力。&lt;h4&gt;主要发现&lt;/h4&gt;在AIHub和FERMAT数据集上评估，VEHME在开源模型中取得了最先进的性能，并接近专有系统的准确性。&lt;h4&gt;结论&lt;/h4&gt;VEHME展示了其作为可扩展且可访问的自动数学评估工具的潜力，训练和实验代码已在GitHub公开存储库中可用。&lt;h4&gt;翻译&lt;/h4&gt;自动评估手写数学解题是教育技术中的一个重要问题，具有实际应用，但由于学生作业的多样格式、非结构化布局和符号复杂性，这仍然是一个重大挑战。为应对这一挑战，我们介绍了VEHME——一种用于评估手写数学表达式的视觉语言模型——旨在以高准确性和可解释的推理痕迹评估开放形式的手写数学答案。VEHME集成了一个两阶段训练管道：(i) 使用结构化推理数据进行监督微调，(ii) 强化学习使模型输出与多维度评分目标（包括正确性、推理深度和错误定位）保持一致。为增强空间理解，我们提出了一个表达式感知的视觉提示模块，在我们合成的多行数学表达式数据集上训练，以在视觉异构输入中稳健地引导注意力。在AIHub和FERMAT数据集上评估，VEHME在开源模型中取得了最先进的性能，并接近专有系统的准确性，展示了其作为可扩展且可访问的自动数学评估工具的潜力。我们的训练和实验代码已在GitHub公开存储库中提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automatically assessing handwritten mathematical solutions is an importantproblem in educational technology with practical applications, but it remains asignificant challenge due to the diverse formats, unstructured layouts, andsymbolic complexity of student work. To address this challenge, we introduceVEHME-a Vision-Language Model for Evaluating Handwritten MathematicsExpressions-designed to assess open-form handwritten math responses with highaccuracy and interpretable reasoning traces. VEHME integrates a two-phasetraining pipeline: (i) supervised fine-tuning using structured reasoning data,and (ii) reinforcement learning that aligns model outputs withmulti-dimensional grading objectives, including correctness, reasoning depth,and error localization. To enhance spatial understanding, we propose anExpression-Aware Visual Prompting Module, trained on our synthesized multi-linemath expressions dataset to robustly guide attention in visually heterogeneousinputs. Evaluated on AIHub and FERMAT datasets, VEHME achieves state-of-the-artperformance among open-source models and approaches the accuracy of proprietarysystems, demonstrating its potential as a scalable and accessible tool forautomated math assessment. Our training and experiment code is publiclyavailable at our GitHub repository.</description>
      <author>example@mail.com (Thu Phuong Nguyen, Duc M. Nguyen, Hyotaek Jeon, Hyunwook Lee, Hyunmin Song, Sungahn Ko, Taehwan Kim)</author>
      <guid isPermaLink="false">2510.22798v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了InstanceGrounded Geometry Transformer (IGGT)，一种端到端的大型统一transformer，用于统一3D场景的空间重建和实例级上下文理解。&lt;h4&gt;背景&lt;/h4&gt;人类自然将3D世界的几何结构和语义内容作为相互交织的维度感知，但先前方法优先训练几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了两者间的相互作用，限制了泛化能力和下游任务表现。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，同时处理3D场景的几何结构和语义理解，提高3D场景分析的准确性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出IGGT模型，设计3D一致性对比学习策略，通过仅2D视觉输入编码具有几何结构和实例聚类的统一表示，并构建InsScene-15K数据集，包含高质量RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解的方法，可以改善3D场景的理解和重建效果，实现从2D输入到连贯3D场景的有效转换。&lt;h4&gt;结论&lt;/h4&gt;所提出的IGGT方法和3D一致性对比学习策略能够有效地将2D视觉输入转换为连贯的3D场景，并明确区分对象实例，为3D场景分析提供了新的统一框架。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容作为相互交织的维度来感知，从而能够连贯准确地理解复杂场景。然而，先前的方法优先训练大型几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了这两个基本方面之间的相互作用，从而限制了泛化能力并在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解这一问题，从而限制了感知能力，并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致性对比学习策略，指导IGGT仅通过2D视觉输入来编码具有几何结构和实例聚类的统一表示。该表示支持将2D视觉输入一致提升为具有明确不同对象实例的连贯3D场景。为了促进这一任务，我们进一步构建了InsScene-15K，这是一个大规模数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释，采用新颖的数据整理流程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D场景几何重建与高层次语义理解相分离的问题。人类自然地将3D世界的几何结构和语义内容视为交织在一起的维度，而现有方法通常将这两个方面视为独立任务，导致它们无法相互增强，限制了模型在下游任务中的泛化能力和性能。这个问题很重要，因为统一的几何-语义表示对于机器人操作、增强现实/虚拟现实和空间规划等应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法的局限性：几何重建和语义理解被分离处理，或简单地将3D模型与特定语言模型对齐，导致感知能力受限和适应性差。他们设计了一个端到端的统一框架，通过联合训练耦合几何和语义特征，让模型自主学习3D实例级语义与几何结构的关系。作者借鉴了VGGT的结构，使用DINOv2进行特征提取，采用DPT架构进行密集预测，并利用SAM2进行数据标注。他们还创新性地设计了3D一致的对比学习策略来增强模型性能。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何和实例级语义特征耦合，实现上下文理解和几何重建的相互改进。整体流程包括：1)接收多视图图像输入；2)使用大型统一变换器提取统一表示；3)通过几何头和实例头分别预测几何结构和实例特征；4)应用跨模态融合块增强实例特征的细粒度空间感知；5)使用3D一致的对比监督确保多视图一致性；6)通过聚类生成实例掩码，用于下游任务如空间跟踪、开放词汇分割和场景理解。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一的3D重建与理解框架IGGT；2)3D一致的对比学习策略；3)InsScene-15K大规模数据集；4)实例级场景理解范式。相比之前的工作，不同之处在于：不是简单对齐几何与语言特征，而是通过联合训练实现相互增强；不与特定视觉语言模型紧密耦合，可以集成更强大的基础模型；能够区分同一语义类别内的不同对象，支持更复杂的下游应用；实现了即插即用的方式与各种视觉语言模型和大型多模态模型集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过提出统一的实例级几何变换器框架和3D一致的对比学习策略，实现了几何重建与语义理解的深度融合，显著提升了3D场景重建与理解的质量和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>BLIP-FusePPO: A Vision-Language Deep Reinforcement Learning Framework for Lane Keeping in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2510.22370v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Amin-A96/BLIP-FusePPO-A-Vision-Language-Deep-Reinforcement-Learning-Framework-for-Lane-Keeping-in-Autonomous.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了BLIP-FusePPO，一种用于自动驾驶车道保持的新型多模态强化学习框架，将视觉语言模型生成的语义嵌入与几何状态、LiDAR观测和PID控制反馈相融合。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶中的车道保持需要结合高级语义理解和低级控制信号，而现有方法可能仅使用语义模型来塑造奖励或未充分利用多模态信息。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够学习具有环境意识且易于理解的驾驶规则的多模态强化学习框架，结合视觉语言模型的高级场景理解与低级控制和空间信号。&lt;h4&gt;方法&lt;/h4&gt;提出BLIP-FusePPO框架，将视觉语言模型生成的语义嵌入直接融合到代理观测空间中的几何状态、LiDAR观测和PID控制反馈，结合语义、几何和控制感知表示，并使用包含语义对齐、车道保持准确性、障碍物避让和速度调节的混合奖励函数。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在车道保持的稳定性和适应性方面优于最佳视觉和多模态强化学习基线，在各种困难的驾驶情况下表现良好，且直接嵌入语义特征减少了昂贵的运行时推理，确保语义指导始终可用。&lt;h4&gt;结论&lt;/h4&gt;BLIP-FusePPO是一个有效的多模态强化学习框架，能够提高自动驾驶车道保持任务的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了基于引导语言-图像预训练的融合状态表示近端策略优化（BLIP-FusePPO），这是一种用于自动驾驶车道保持的新型多模态强化学习框架，其中视觉语言模型生成的语义嵌入直接与几何状态、LiDAR观测和基于比例-积分-微分的控制反馈在代理观测空间内融合。所提出的方法通过结合视觉语言模型的高级场景理解与低级控制和空间信号，使代理能够学习具有环境意识且易于理解的驾驶规则。我们的架构将语义、几何和控制感知表示结合在一起，使策略学习更加稳健。包含语义对齐、车道保持准确性、障碍物避让和速度调节的混合奖励函数有助于学习更加高效和可泛化。我们的方法不同于仅使用语义模型来塑造奖励的方法，而是直接将语义特征嵌入到状态表示中。这减少了昂贵的运行时推理，并确保语义指导始终可用。仿真结果表明，在广泛的困难驾驶情况下，所提出的模型在车道保持的稳定性和适应性方面优于最佳视觉和多模态强化学习基线。我们公开提供代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶车辆车道保持任务中语义感知与控制感知融合不足的问题。这一问题在现实中很重要，因为现有系统在复杂环境（如车道标记磨损、不同光照条件或被遮挡车道）中表现有限，而缺乏对场景语义理解的系统难以适应多变路况，影响自动驾驶的安全性和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统视觉方法缺乏语义理解、多模态RL方法仅用VLM塑造奖励而非融入状态、传统控制器缺乏可解释性。作者借鉴了BLIP视觉语言模型用于语义提取、PPO算法用于稳定策略学习、PID控制器提供控制反馈等现有工作。创新点在于将语义特征与几何状态、LiDAR观测和PID控制反馈直接融合到状态表示中，而非仅用于奖励塑造。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将语义感知（通过BLIP提取）与控制感知（通过PID获取）直接融合到强化学习智能体的状态表示中，使智能体同时理解场景语义上下文和执行精确控制。整体流程包括：1)混合状态表示（RGB视觉、LiDAR数据、PID反馈、语义嵌入）；2)预处理和特征提取；3)数据增强（水平镜像）；4)连续动作空间设计（转向和速度控制）；5)混合奖励函数（车道保持、障碍物避免、速度匹配等）；6)使用PPO算法训练策略网络。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)新颖架构将BLIP语义嵌入和PID信号直接注入状态表示；2)基于PID控制的状态增强技术提高学习稳定性；3)新型混合奖励函数整合语义对齐和几何遵循；4)直接语义特征嵌入而非仅用于奖励塑造。相比之前工作，不同之处在于：传统方法缺乏语义理解、现有多模态RL方法仅用VLM塑造奖励、传统控制器缺乏可解释性、基于VLM的RL系统计算开销大。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了BLIP-FusePPO框架，通过融合语义感知与控制感知到状态表示中，显著提高了自动驾驶车道保持的稳定性和适应性，同时降低了计算开销，为更安全鲁棒的自动驾驶系统提供了新思路。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose Bootstrapped Language-Image Pretraining-drivenFused State Representation in Proximal Policy Optimization (BLIP-FusePPO), anovel multimodal reinforcement learning (RL) framework for autonomouslane-keeping (LK), in which semantic embeddings generated by a vision-languagemodel (VLM) are directly fused with geometric states, LiDAR observations, andProportional-Integral-Derivative-based (PID) control feedback within the agentobservation space. The proposed method lets the agent learn driving rules thatare aware of their surroundings and easy to understand by combining high-levelscene understanding from the VLM with low-level control and spatial signals.Our architecture brings together semantic, geometric, and control-awarerepresentations to make policy learning more robust. A hybrid reward functionthat includes semantic alignment, LK accuracy, obstacle avoidance, and speedregulation helps learning to be more efficient and generalizable. Our method isdifferent from the approaches that only use semantic models to shape rewards.Instead, it directly embeds semantic features into the state representation.This cuts down on expensive runtime inference and makes sure that semanticguidance is always available. The simulation results show that the proposedmodel is better at LK stability and adaptability than the best vision-based andmultimodal RL baselines in a wide range of difficult driving situations. Wemake our code publicly available.</description>
      <author>example@mail.com (Seyed Ahmad Hosseini Miangoleh, Amin Jalal Aghdasian, Farzaneh Abdollahi)</author>
      <guid isPermaLink="false">2510.22370v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MOGRAS: Human Motion with Grasping in 3D Scenes</title>
      <link>http://arxiv.org/abs/2510.22199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  British Machine Vision Conference Workshop - From Scene Understanding  to Human Modeling&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了MOGRAS数据集和一种简单有效的方法，用于解决在3D场景中生成物理合理的全身抓取运动的挑战，通过定量和定性实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;生成与物体交互的真实全身运动对机器人技术、虚拟现实和人机交互应用至关重要，但现有方法要么缺乏精细任务的保真度，要么忽略了周围3D场景。&lt;h4&gt;目的&lt;/h4&gt;弥合现有方法在生成全身抓取运动方面的局限性，提供能够在3D场景中生成物理合理全身抓取运动的解决方案。&lt;h4&gt;方法&lt;/h4&gt;引入MOGRAS（3D场景中的人体抓取运动）数据集，提供预抓取全身行走运动和最终抓取姿态；利用该数据集基准测试现有方法；提出一种简单有效的方法使现有方法能在3D场景中无缝工作。&lt;h4&gt;主要发现&lt;/h4&gt;现有全身抓握方法在场景感知生成方面存在局限性；所提出的方法在全身抓取运动生成方面取得了显著改进；通过大量定量和定性实验验证了数据集的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究为更真实的人体-场景交互铺平了道路，展示了在3D场景中生成物理合理全身抓取运动的可行性。&lt;h4&gt;翻译&lt;/h4&gt;生成与物体交互的真实全身运动对机器人技术、虚拟现实和人机交互应用至关重要。虽然现有方法可以生成3D场景中的全身运动，但它们通常缺乏精细任务（如物体抓取）的保真度。相反，生成精确抓取运动的方法通常忽略了周围的3D场景。在3D场景中生成物理上合理的全身抓取运动仍然是一个重大挑战。为解决这一问题，我们引入了MOGRAS（3D场景中的人体抓取运动），这是一个弥合这一差距的大规模数据集。MOGRAS在丰富的3D室内场景标注中提供了预抓取的全身行走运动和最终抓取姿态。我们利用MOGRAS对现有的全身抓取方法进行基准测试，展示了它们在场景感知生成方面的局限性。此外，我们提出了一种简单而有效的方法，使现有方法能够在3D场景中无缝工作。通过大量的定量和定性实验，我们验证了数据集的有效性，并突显了我们提出方法所取得的显著改进，为更真实的人体-场景交互铺平了道路。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决在3D场景中生成物理合理的全身抓取运动的问题。现有方法要么能生成全身运动但缺乏精细抓保真度，要么能生成精确抓取但忽略周围3D场景。这个问题对机器人、虚拟现实和人机交互等领域至关重要，因为准确建模人-物体交互能支持行为分析、智能机器人系统开发和逼真虚拟环境创建。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别现有研究的差距：全身运动方法缺乏精细抓取能力，抓取方法忽略场景上下文。考虑到手动捕获此类数据成本高昂，他们设计了自动化数据生成框架。作者借鉴了HUMANISE的运动对齐方法、AMASS的行走序列、ScanNetv2的3D环境、BABEL的运动标签、GRAB的抓取物体，并改进了FLEX的抓取姿势生成和PriorMDM的运动填充技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是创建MOGRAS数据集，提供3D室内场景中的全身抓取运动序列，包括预抓取行走和最终抓取姿势。实现流程分五步：1)行走运动对齐和物体放置；2)改进ScanNet场景的地板对齐；3)使用改进的FLEX生成抓取姿势；4)用PriorMDM生成从行走到抓取的平滑过渡；5)确保数据集规模和质量，通过自动过滤和人类评估保证物理合理性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)MOGRAS数据集：首个结合全身运动、精细抓取和3D场景的大规模合成数据集；2)GNet++方法：通过场景编码和穿透损失实现场景感知抓取；3)场景处理改进：解决地板不对齐问题。相比之前工作，MOGRAS是首个同时包含三种元素(3D场景、精细抓取、全身运动)的数据集，而GNet++显式考虑环境约束，而之前方法如GOAL和SAGA忽略了场景上下文。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 通过引入首个大规模场景感知全身抓取数据集MOGRAS和相应生成方法GNet++，论文弥合了3D场景中全身运动生成与精细物体抓取之间的差距，为更真实的人-场景交互研究奠定了基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic full-body motion interacting with objects is criticalfor applications in robotics, virtual reality, and human-computer interaction.While existing methods can generate full-body motion within 3D scenes, theyoften lack the fidelity for fine-grained tasks like object grasping.Conversely, methods that generate precise grasping motions typically ignore thesurrounding 3D scene. This gap, generating full-body grasping motions that arephysically plausible within a 3D scene, remains a significant challenge. Toaddress this, we introduce MOGRAS (Human MOtion with GRAsping in 3D Scenes), alarge-scale dataset that bridges this gap. MOGRAS provides pre-graspingfull-body walking motions and final grasping poses within richly annotated 3Dindoor scenes. We leverage MOGRAS to benchmark existing full-body graspingmethods and demonstrate their limitations in scene-aware generation.Furthermore, we propose a simple yet effective method to adapt existingapproaches to work seamlessly within 3D scenes. Through extensive quantitativeand qualitative experiments, we validate the effectiveness of our dataset andhighlight the significant improvements our proposed method achieves, paving theway for more realistic human-scene interactions.</description>
      <author>example@mail.com (Kunal Bhosikar, Siddharth Katageri, Vivek Madhavaram, Kai Han, Charu Sharma)</author>
      <guid isPermaLink="false">2510.22199v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>CogStereo: Neural Stereo Matching with Implicit Spatial Cognition Embedding</title>
      <link>http://arxiv.org/abs/2510.22119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为CogStereo的新型立体匹配框架，通过引入空间认知机制来改进立体匹配性能，特别是在处理遮挡或弱纹理等挑战性区域时表现出色，同时提高了跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;深度立体匹配通过微调在基准数据集上取得了显著进展，但在零样本泛化方面不如其他视觉任务中的基础模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种不依赖数据集特定先验的框架，解决立体匹配中的挑战性问题，提高跨域泛化能力，并将立体视觉转向认知驱动的方法。&lt;h4&gt;方法&lt;/h4&gt;CogStereo通过使用单目深度特征作为先验，将隐式空间认知嵌入到细化过程中，捕获超越局部对应的全场景理解。该方法采用双条件细化机制，结合逐像素不确定性和认知引导特征，实现对不匹配的一致性全局校正。&lt;h4&gt;主要发现&lt;/h4&gt;CogStereo在Scene Flow、KITTI、Middlebury、ETH3D、EuRoc和真实世界等多个数据集上实现了最先进的结果，并在跨域泛化方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;CogStereo成功解决了传统立体匹配方法在处理挑战性区域时的局限性，通过引入空间认知机制提高了立体视觉系统的泛化能力，推动了立体视觉向认知驱动方向发展。&lt;h4&gt;翻译&lt;/h4&gt;深度立体匹配通过微调在基准数据集上取得了显著进展，但在零样本泛化方面不如其他视觉任务中的基础模型。我们引入了CogStereo，一种新颖的框架，它解决了遮挡或弱纹理等挑战性区域，而不依赖于数据集特定的先验。CogStereo通过使用单目深度特征作为先验，将隐式空间认知嵌入到细化过程中，捕获超越局部对应的全局场景理解。这种方法确保了结构一致性的视差估计，即使在仅靠几何不足的区域。CogStereo采用双条件细化机制，结合逐像素不确定性和认知引导特征，实现对不匹配的一致性全局校正。在Scene Flow、KITTI、Middlebury、ETH3D、EuRoc和真实世界上的大量实验表明，CogStereo不仅取得了最先进的结果，还在跨域泛化方面表现出色，将立体视觉转向认知驱动的方法。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决深度立体匹配方法在零样本泛化能力上的不足问题。当前方法虽然在基准数据集上表现良好，但在遮挡区域、弱纹理等困难区域表现不佳，且缺乏强大的零样本泛化能力。这个问题在自动驾驶、机器人等应用中至关重要，因为这些应用需要在各种不同环境下保持一致的深度估计性能，而现有方法过度依赖局部几何对应，缺乏全局场景理解能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过观察立体匹配与其他视觉任务基础模型的差距，引入了'空间认知'概念，借鉴单目深度估计的成功经验，特别是Depth Anything v2捕获的物体级几何和全局场景理解能力。作者还借鉴了条件控制思想，设计了双条件修正机制。创新之处在于将不确定性估计提前到成本体积阶段，而非视差回归之后，并设计了不确定性引导的空间认知注意力机制、低不确定性区域的KNN对齐策略和突然深度差异感知梯度损失等组件。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将隐式空间认知嵌入到立体匹配过程中，利用单目深度特征作为先验，超越局部对应关系，捕获整体场景理解。整体流程分为两阶段：第一阶段是成本体积不确定性估计预训练，使用标准立体匹配骨干网络提取特征，构建三维成本体积，并预测每个像素的不确定性；第二阶段是通过空间认知的双条件修正，整合不确定性先验与空间认知特征，使用注意力机制进行视差修正，并通过KNN对齐防止度量漂移，最后应用特殊损失函数确保视差图的平滑性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次将隐式空间认知概念引入立体匹配；2)设计双条件修正机制结合不确定性和认知特征；3)在成本体积阶段而非视差回归后进行不确定性估计；4)引入低不确定性区域的KNN对齐策略防止度量漂移；5)设计突然深度差异感知梯度损失。相比之前工作，CogStereo超越了纯几何匹配，实现了强大的零样本泛化，改变了不确定性处理方式，创新性地利用深度基础模型的中间特征而非原始深度预测，并针对遮挡、弱纹理等困难区域提供了更鲁棒的解决方案。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; CogStereo通过引入隐式空间认知嵌入到立体匹配过程中，结合像素级不确定性与认知引导特征，显著提升了在遮挡、弱纹理等困难区域的零样本泛化能力和视差估计准确性，实现了从纯几何匹配向认知驱动立体视觉的转变。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep stereo matching has advanced significantly on benchmark datasets throughfine-tuning but falls short of the zero-shot generalization seen in foundationmodels in other vision tasks. We introduce CogStereo, a novel framework thataddresses challenging regions, such as occlusions or weak textures, withoutrelying on dataset-specific priors. CogStereo embeds implicit spatial cognitioninto the refinement process by using monocular depth features as priors,capturing holistic scene understanding beyond local correspondences. Thisapproach ensures structurally coherent disparity estimation, even in areaswhere geometry alone is inadequate. CogStereo employs a dual-conditionalrefinement mechanism that combines pixel-wise uncertainty with cognition-guidedfeatures for consistent global correction of mismatches. Extensive experimentson Scene Flow, KITTI, Middlebury, ETH3D, EuRoc, and real-world demonstrate thatCogStereo not only achieves state-of-the-art results but also excels incross-domain generalization, shifting stereo vision towards a cognition-drivenapproach.</description>
      <author>example@mail.com (Lihuang Fang, Xiao Hu, Yuchen Zou, Hong Zhang)</author>
      <guid isPermaLink="false">2510.22119v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>GeoThought: A Dataset for Enhancing Mathematical Geometry Reasoning in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对大型语言模型在几何视觉推理任务中的性能下降问题，开发了一个名为GeoThoughts的几何推理数据集和一个名为GeoThought-MLLM的多模态数学推理模型，通过链式思维训练显著提升了模型在几何问题上的表现。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在文本数学问题解决中表现出强大的推理能力，但在视觉推理任务，特别是几何问题解决中，其性能显著下降。这是因为几何问题具有独特挑战：一是几何本身的复杂性需要详细的图像理解和多步推理；二是现有数据集规模不足、多样性有限且缺乏明确的推理痕迹，阻碍了有效模型训练。&lt;h4&gt;目的&lt;/h4&gt;开发一个全面的几何推理数据集和一个多模态数学推理模型，以解决大型语言模型在几何问题解决中的性能下降问题。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了GeoThoughts数据集，包含两个子集：Geo-Thought-6K（6,243个样本）和Geo-Thought-Augmented-10K（10,834个样本）。2. 每个数据条目包括视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。3. 基于此数据集开发了GeoThought-MLLM，一个在问题解决过程中生成详细思考过程的多模态数学推理模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 使用链式思维数据集训练的GeoThought-MLLM在几何任务上优于现有基准。2. 训练显著提升了模型在领域内和领域外几何推理能力。3. 错误主要源于数学概念错误解释或空间判断失误。4. 通过调用链式思维纠正这些错误，模型能够产生正确答案。&lt;h4&gt;结论&lt;/h4&gt;GeoThoughts数据集和GeoThought-MLLM模型有效解决了大型语言模型在几何问题解决中的性能下降问题，为几何视觉推理任务提供了新的解决方案和见解。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在基于文本的数学问题解决中表现出强大的推理能力；然而，当适应到视觉推理任务，特别是几何问题解决时，它们的性能大幅下降，因为几何问题带来了独特的挑战。具体来说，这些挑战源于两个关键因素：首先，几何本身的复杂性需要详细的图像理解和多步推理；其次，现有数据集的规模、多样性和明确的推理痕迹不足，从而阻碍了有效的模型训练。为应对这些挑战，我们开发了GeoThoughts数据集，这是一个全面的几何推理语料库，包含两个子集：包含6,243个样本的Geo-Thought-6K及其增强版本Geo-Thought-Augmented-10K，包含10,834个样本。每个条目包括视觉描述、分步解决方案、明确的推理链、反思步骤和最终答案。使用此数据集，我们开发了GeoThought-MLLM，一个在问题解决过程中生成详细思考过程的数学推理多模态模型。我们的模型在几何任务上优于现有基准，证明使用我们的链式思维数据集进行训练可以提升领域内和领域外设置的几何推理能力。最后，我们分析了失败案例，发现错误主要源于数学概念错误解释或空间判断失误。通过调用链式思维纠正这些错误，模型产生了正确答案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong reasoning capabilitiesin text-based mathematical problem solving; however, when adapted to visualreasoning tasks, particularly geometric problem solving, their performancesubstantially declines because geometric problems present unique challenges.Specifically, these challenges stem from two key factors: first, the intrinsiccomplexity of geometry requiring detailed image comprehension and multi-stepreasoning, and second, the limitations of existing datasets which lacksufficient scale, diversity, and explicit reasoning traces, consequentlyhindering effective model training. To address these challenges, we developedthe GeoThoughts dataset, a comprehensive geometric reasoning corpus with twosubsets: Geo-Thought-6K with 6,243 samples and its augmented versionGeo-Thought-Augmented-10K containing 10,834 samples. Each entry includes visualdescriptions, step-by-step solutions, explicit reasoning chains, reflectionsteps, and final answers. Using this dataset, we developed GeoThought-MLLM, amathematical reasoning multimodal model that generates detailed thinkingprocesses during problem-solving. Our model outperforms existing benchmarks ingeometric tasks, demonstrating that training with our Chain-of-Thought datasetimproves geometric reasoning capabilities across both in-domain andout-of-domain settings. Finally, we analyze failure cases and observe thaterrors primarily arise from incorrect interpretation of mathematical conceptsor spatial misjudgment. By invoking CoT to correct these mistakes, the modelproduces correct answers.</description>
      <author>example@mail.com (Nannan Shi, Chuanyu Qin, Shipeng Song, Man Luo)</author>
      <guid isPermaLink="false">2510.21881v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>GRAPHIA: Harnessing Social Graph Data to Enhance LLM-Based Social Simulation</title>
      <link>http://arxiv.org/abs/2510.24251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Graphia是一种基于大型语言模型的社会图模拟框架，利用图数据作为监督信号通过强化学习对LLM进行后训练，能够预测互动对象和互动方式，在微观和宏观层面都显示出显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在模拟类人社会行为方面展现出潜力，但社会图作为包含局部交互和全局网络结构的高质量监督信号，在LLM训练中仍未得到充分利用。&lt;h4&gt;目的&lt;/h4&gt;提出Graphia框架，利用图数据作为监督信号，通过强化学习对LLM进行后训练，以缩小代理行为与基于LLM的模拟中网络动力学之间的差距。&lt;h4&gt;方法&lt;/h4&gt;Graphia训练专门的代理来预测与谁互动（目标选择）和如何互动（边生成），使用基于GNN的结构性奖励，并设计了图生成流程。在归因动态图生成（TDGG）和归纳动态图生成（IDGG）两种设置下进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实世界网络上，Graphia相比最强基线模型，微观对齐方面：综合目标选择分数提高6.1%，边分类准确率提高12%，边内容BERTScore提高27.9%；宏观对齐方面：结构相似性提高41.11%，社会现象（如幂律和回音室）复制能力提高32.98%。Graphia还支持反事实模拟，能在平台激励下生成合理的行为转变。&lt;h4&gt;结论&lt;/h4&gt;社会图可以作为LLM后训练的高质量监督信号，有效缩小基于LLM的模拟中代理行为与网络动力学之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型在模拟类人社会行为方面展现出潜力。社会图提供了高质量监督信号，编码了局部交互和全局网络结构，但这些信号在LLM训练中仍未得到充分利用。为解决这一差距，我们提出了Graphia，这是第一个基于LLM的社会图模拟通用框架，它利用图数据作为监督信号，通过强化学习对LLM进行后训练。基于GNN的结构性奖励，Graphia训练专门的代理来预测与谁互动（目标选择）和如何互动（边生成），然后使用设计的图生成流程。我们在两种设置下评估Graphia：归因动态图生成（TDGG），这是使用我们提出的节点级交互对齐指标的微观任务；以及归纳动态图生成（IDGG），这是使用我们提出的指标对齐涌现网络属性的宏观任务。在三个真实世界网络上，Graphia相比最强基线模型，在微观对齐方面提高了6.1%的综合目标选择分数，12%的边分类准确率和27.9%的边内容BERTScore。对于宏观对齐，它实现了41.11%更高的结构相似性和32.98%更好的社会现象（如幂律和回音室）复制能力。Graphia还支持反事实模拟，在平台激励下生成合理的行为转变。我们的结果表明，社会图可以作为LLM后训练的高质量监督信号，缩小基于LLM的模拟中代理行为与网络动力学之间的差距。代码可在https://github.com/Ji-Cather/Graphia.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have shown promise in simulating human-likesocial behaviors. Social graphs provide high-quality supervision signals thatencode both local interactions and global network structure, yet they remainunderutilized for LLM training. To address this gap, we propose Graphia, thefirst general LLM-based social graph simulation framework that leverages graphdata as supervision for LLM post-training via reinforcement learning. WithGNN-based structural rewards, Graphia trains specialized agents to predict whomto interact with (destination selection) and how to interact (edge generation),followed by designed graph generation pipelines. We evaluate Graphia under twosettings: Transductive Dynamic Graph Generation (TDGG), a micro-level task withour proposed node-wise interaction alignment metrics; and Inductive DynamicGraph Generation (IDGG), a macro-level task with our proposed metrics foraligning emergent network properties. On three real-world networks, Graphiaimproves micro-level alignment by 6.1% in the composite destination selectionscore, 12% in edge classification accuracy, and 27.9% in edge content BERTScoreover the strongest baseline. For macro-level alignment, it achieves 41.11%higher structural similarity and 32.98% better replication of social phenomenasuch as power laws and echo chambers. Graphia also supports counterfactualsimulation, generating plausible behavioral shifts under platform incentives.Our results show that social graphs can serve as high-quality supervisionsignals for LLM post-training, closing the gap between agent behaviors andnetwork dynamics for LLM-based simulation. Code is available athttps://github.com/Ji-Cather/Graphia.git.</description>
      <author>example@mail.com (Jiarui Ji, Zehua Zhang, Zhewei Wei, Bin Tong, Guan Wang, Bo Zheng)</author>
      <guid isPermaLink="false">2510.24251v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MAGNET: A Multi-Graph Attentional Network for Code Clone Detection</title>
      <link>http://arxiv.org/abs/2510.24241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MAGNET是一种多图注意力框架，通过联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征，实现了代码克隆检测的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;代码克隆检测是软件工程中的基础任务，支持重构、调试、剽窃检测和漏洞分析。现有方法通常依赖单一表示（如AST、CFG、DFG），只能捕捉代码语义的部分方面，而混合方法的融合策略通常是手工设计的且效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MAGNET框架，一种多图注意力框架，联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征。&lt;h4&gt;方法&lt;/h4&gt;MAGNET结合残差图神经网络与节点级自注意力学习局部和长距离依赖关系，引入门控交叉注意力机制用于细粒度的图间交互，采用Set2Set池化将多图嵌入融合为统一的程序级表示。&lt;h4&gt;主要发现&lt;/h4&gt;在BigCloneBench和Google Code Jam上的实验表明，MAGNET分别达到96.5%和99.2%的总体F1分数，实现了最先进的性能。消融研究证实了多图融合和每个注意力组件的关键贡献。&lt;h4&gt;结论&lt;/h4&gt;MAGNET通过多图表示和注意力机制实现了高效的代码克隆检测，代码已开源于https://github.com/ZixianReid/Multigraph_match。&lt;h4&gt;翻译&lt;/h4&gt;代码克隆检测是软件工程中的一个基础任务，它支持重构、调试、剽窃检测和漏洞分析。现有方法通常依赖于单一表示，如抽象语法树（AST）、控制流图（CFG）和数据流图（DFG），这些表示只能捕捉代码语义的部分方面。混合方法已经出现，但它们的融合策略通常是手工设计的且效果不佳。在本研究中，我们提出了MAGNET，一种多图注意力框架，它联合利用AST、CFG和DFG表示来捕获源代码的语法和语义特征。MAGNET将残差图神经网络与节点级自注意力相结合，学习局部和长距离依赖关系，引入门控交叉注意力机制用于细粒度的图间交互，并采用Set2Set池化将多图嵌入融合为统一的程序级表示。在BigCloneBench和Google Code Jam上的大量实验表明，MAGNET在两个数据集上分别实现了96.5%和99.2%的总体F1分数，达到了最先进的性能。消融研究证实了多图融合和每个注意力组件的关键贡献。我们的代码可在https://github.com/ZixianReid/Multigraph_match获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Code clone detection is a fundamental task in software engineering thatunderpins refactoring, debugging, plagiarism detection, and vulnerabilityanalysis. Existing methods often rely on singular representations such asabstract syntax trees (ASTs), control flow graphs (CFGs), and data flow graphs(DFGs), which capture only partial aspects of code semantics. Hybrid approacheshave emerged, but their fusion strategies are typically handcrafted andineffective. In this study, we propose MAGNET, a multi-graph attentionalframework that jointly leverages AST, CFG, and DFG representations to capturesyntactic and semantic features of source code. MAGNET integrates residualgraph neural networks with node-level self-attention to learn both local andlong-range dependencies, introduces a gated cross-attention mechanism forfine-grained inter-graph interactions, and employs Set2Set pooling to fusemulti-graph embeddings into unified program-level representations. Extensiveexperiments on BigCloneBench and Google Code Jam demonstrate that MAGNETachieves state-of-the-art performance with an overall F1 score of 96.5\% and99.2\% on the two datasets, respectively. Ablation studies confirm the criticalcontributions of multi-graph fusion and each attentional component. Our code isavailable at https://github.com/ZixianReid/Multigraph_match</description>
      <author>example@mail.com (Zixian Zhang, Takfarinas Saber)</author>
      <guid isPermaLink="false">2510.24241v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>HyperGraphX: Graph Transductive Learning with Hyperdimensional Computing and Message Passing</title>
      <link>http://arxiv.org/abs/2510.23980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为hdgc的新型算法，该算法结合了图卷积与高维计算中的绑定和捆绑操作，用于归纳图学习。&lt;h4&gt;背景&lt;/h4&gt;在图学习领域，图神经网络和高维计算是两种重要的方法，各有优势和局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时利用图神经网络和高维计算优势的新算法，提高图学习的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;hdgc算法将图卷积操作与高维计算中的绑定和捆绑操作相结合，主要在二进制向量上进行学习操作。&lt;h4&gt;主要发现&lt;/h4&gt;hdgc在预测准确性上优于主流图神经网络实现和最先进的高维计算实现，适用于同质图和异质图；在相同GPU平台上，hdgc比gcnii图神经网络实现平均快9561倍，比hdgl高维计算实现平均快144.5倍。&lt;h4&gt;结论&lt;/h4&gt;hdgc算法在多种图类型上表现出色，由于主要操作在二进制向量上进行，预期在神经形态和新兴存储器处理设备上具有出色的能源性能。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为hdgc的新颖算法，该算法将图卷积与高维计算中的绑定和捆绑操作相结合，用于归纳图学习。在预测准确性方面，hdgc优于主要的和流行的图神经网络实现以及最先进的高维计算实现，适用于一系列同质图和异质图。与我们测试的最准确的学习方法相比，在相同的目标GPU平台上，hdgc比图神经网络实现gcnii平均快9561.0倍，比高维计算实现hdgl平均快144.5倍。由于大部分学习操作在二进制向量上进行，我们期望hdgc在神经形态和新兴存储器处理设备上具有出色的能源性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel algorithm, \hdgc, that marries graph convolution withbinding and bundling operations in hyperdimensional computing for transductivegraph learning. For prediction accuracy \hdgc outperforms major and populargraph neural network implementations as well as state-of-the-arthyperdimensional computing implementations for a collection of homophilicgraphs and heterophilic graphs. Compared with the most accurate learningmethodologies we have tested, on the same target GPU platform, \hdgc is onaverage 9561.0 and 144.5 times faster than \gcnii, a graph neural networkimplementation and HDGL, a hyperdimensional computing implementation,respectively. As the majority of the learning operates on binary vectors, weexpect outstanding energy performance of \hdgc on neuromorphic and emergingprocess-in-memory devices.</description>
      <author>example@mail.com (Guojing Cong, Tom Potok, Hamed Poursiami, Maryam Parsa)</author>
      <guid isPermaLink="false">2510.23980v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Exploring an image-based $b$-jet tagging method using convolution neural networks</title>
      <link>http://arxiv.org/abs/2510.23962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages, 17 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于图像的喷流味道识别方法，利用主顶点周围的图像和喷流锥内的带电粒子，通过卷积神经网络进行分析，在b-喷流识别中实现了80-90%的效率，有望提高高能核物理实验的准确性。&lt;h4&gt;背景&lt;/h4&gt;喷流味道识别（识别起源于c夸克、b夸克和其他夸克（轻夸克和胶子）的喷流）在高能重离子物理中至关重要，因为它能够研究重离子碰撞产生的热密核介质中的味道依赖响应。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的喷流味道识别方法，提高在高能核物理实验中的准确性。&lt;h4&gt;方法&lt;/h4&gt;基于主顶点周围的图像，利用喷流锥内的带电粒子（可通过硅跟踪系统测量），使用卷积神经网络进行分析。研究假设跟踪系统具有理想性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于图像的味道识别方法在横向动量范围从20到100 GeV/c的喷流中，实现了80-90%的b-喷流识别效率。&lt;h4&gt;结论&lt;/h4&gt;这种基于图像的喷流味道识别方法有潜力显著提高高能核物理实验中喷流味道识别的准确性。&lt;h4&gt;翻译&lt;/h4&gt;喷流味道识别，即识别起源于c夸克、b夸克和其他夸克（轻夸克和胶子）的喷流，是高能重离子物理中的关键任务，因为它能够研究重离子碰撞产生的热密核介质中的味道依赖响应。最近，基于深度学习技术（如深度神经网络和图神经网络）的几种方法已被开发。这些基于深度学习的方法相比依赖轨迹影响参数和次级顶点的传统方法表现出显著改进的性能。在识别算法中，使用了喷流和组成带电粒子的各种属性作为输入参数。我们探索了一种基于主顶点周围图像的新方法，利用喷流锥内的带电粒子，这些粒子可以通过硅跟踪系统测量。对于这项初步实验研究，我们假设跟踪系统具有理想性能。为了分析这些图像，我们采用了卷积神经网络。基于图像的味道识别方法在横向动量范围从20到100 GeV/c的喷流中显示了80-90%的b-喷流识别效率。这种方法有潜力显著提高高能核物理实验中喷流味道识别的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s40042-025-01506-3&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Jet flavor tagging, the identification of jets originating from $c$-quarks,$b$-quarks, and other quarks (light quarks and gluons), is a crucial task inhigh-energy heavy-ion physics, as it enables the investigation offlavor-dependent responses within the hot and dense nuclear medium produced inheavy-ion collisions. Recently, several methods based on deep learningtechniques, such as deep neural networks and graph neural networks, have beendeveloped. These deep-learning-based methods demonstrate significantly improvedperformance compared to traditional methods that rely on track impactparameters and secondary vertices. In the tagging algorithms, variousproperties of jets and constituent charged particles are used as inputparameters. We explore a new method based on images surrounding the primaryvertex, utilizing charged particles within the jet cone, which can be measuredusing a silicon tracking system. For this initial experimental study, we assumethe ideal performance of the tracking system. To analyze these images, weemployed convolutional neural networks. The image-based flavor tagging methodshows an 80-90% $b$-jet tagging efficiency for jets in the transverse momentumrange from 20 to 100 GeV/$c$. This approach has the potential to significantlyimprove the accuracy of jet flavor tagging in high-energy nuclear physicsexperiments.</description>
      <author>example@mail.com (Hangil Jang, Sanghoon Lim)</author>
      <guid isPermaLink="false">2510.23962v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
      <link>http://arxiv.org/abs/2510.22839v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器，用于结构参数优化，以克服传统数值方法计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;优化结构参数(如质量m、刚度k和阻尼系数c)对于设计高效、有韧性和稳定的结构至关重要。传统的数值方法，如有限元法(FEM)和计算流体动力学(CFD)模拟，虽然能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。&lt;h4&gt;目的&lt;/h4&gt;开发一种计算效率更高的方法来优化结构参数，避免传统数值方法的计算负担，实现自动化和智能化的结构设计。&lt;h4&gt;方法&lt;/h4&gt;提出混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器；使用GNN学习结构参数与动态位移响应之间的非线性映射；使用Newmark Beta方法生成单自由度(SDOF)系统响应数据集；GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和GA框架实现了强收敛性和鲁棒泛化能力；与传统模拟相比，显著降低了计算成本；结合机器学习代理和进化优化的方法对于自动化和智能结构设计是有效的。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法克服了传统数值优化方法的计算效率问题，为结构参数优化提供了一种有效途径，展示了机器学习代理与进化优化结合在自动化和智能结构设计中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;结构参数(如质量m、刚度k和阻尼系数c)的优化对于设计高效、有韧性和稳定的结构至关重要。传统的数值方法，包括有限元法(FEM)和计算流体动力学(CFD)模拟，虽然能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器来克服这些挑战。GNN被训练以准确学习结构参数与动态位移响应之间的非线性映射，实现无需重复求解系统方程的快速预测。使用Newmark Beta方法生成单自由度(SDOF)系统响应数据集，涵盖多种质量、刚度和阻尼配置。然后，GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。结果表明，与传统模拟相比，GNN和GA框架实现了强收敛性、鲁棒泛化能力和显著降低的计算成本。这种方法突显了将机器学习代理与进化优化相结合用于自动化和智能结构设计的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of structural parameters, such as mass(m), stiffness(k), anddamping coefficient(c), is critical for designing efficient, resilient, andstable structures. Conventional numerical approaches, including Finite ElementMethod (FEM) and Computational Fluid Dynamics (CFD) simulations, providehigh-fidelity results but are computationally expensive for iterativeoptimization tasks, as each evaluation requires solving the governing equationsfor every parameter combination. This study proposes a hybrid data-drivenframework that integrates a Graph Neural Network (GNN) surrogate model with aGenetic Algorithm (GA) optimizer to overcome these challenges. The GNN istrained to accurately learn the nonlinear mapping between structural parametersand dynamic displacement responses, enabling rapid predictions withoutrepeatedly solving the system equations. A dataset of single-degree-of-freedom(SDOF) system responses is generated using the Newmark Beta method acrossdiverse mass, stiffness, and damping configurations. The GA then searches forglobally optimal parameter sets by minimizing predicted displacements andenhancing dynamic stability. Results demonstrate that the GNN and GA frameworkachieves strong convergence, robust generalization, and significantly reducedcomputational cost compared to conventional simulations. This approachhighlights the effectiveness of combining machine learning surrogates withevolutionary optimization for automated and intelligent structural design.</description>
      <author>example@mail.com (Sagnik Mukherjee)</author>
      <guid isPermaLink="false">2510.22839v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MIC-BEV: Multi-Infrastructure Camera Bird's-Eye-View Transformer with Relation-Aware Fusion for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2510.24688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于基础设施的感知在智能交通系统中起着关键作用，提供全局态势感知并支持协作自动驾驶。然而，现有基于摄像头的检测模型在多视图基础设施设置、多样化摄像头配置、降级视觉输入和各种道路布局等场景下表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出MIC-BEV，一种基于Transformer的鸟瞰图(BEV)感知框架，用于基于基础设施的多摄像头3D物体检测。&lt;h4&gt;方法&lt;/h4&gt;MIC-BEV灵活支持具有异构内参和外参的变量摄像头数量，在传感器降级情况下表现出强大的鲁棒性。提出的图增强融合模块通过利用摄像头和BEV单元之间的几何关系以及潜在视觉线索，将多视图图像特征集成到BEV空间。同时引入M2I数据集，用于基于基础设施的物体检测，具有多样化的摄像头配置、道路布局和环境条件。&lt;h4&gt;主要发现&lt;/h4&gt;在M2I和真实世界数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测方面实现了最先进的性能，在极端天气和传感器降级等具有挑战性的条件下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;MIC-BEV的结果突显了其在现实世界部署的潜力，数据集和源代码可在GitHub链接获取。&lt;h4&gt;翻译&lt;/h4&gt;基于基础设施的感知在智能交通系统中起着关键作用，提供全局态势感知并支持协作自动驾驶。然而，现有基于摄像头的检测模型在多视图基础设施设置、多样化摄像头配置、降级视觉输入和各种道路布局等场景下表现不佳。我们提出MIC-BEV，一种基于Transformer的鸟瞰图(BEV)感知框架，用于基于基础设施的多摄像头3D物体检测。MIC-BEV灵活支持具有异构内参和外参的变量摄像头数量，在传感器降级情况下表现出强大的鲁棒性。MIC-BEV中提出的图增强融合模块通过利用摄像头和BEV单元之间的几何关系以及潜在视觉线索，将多视图图像特征集成到BEV空间。为支持训练和评估，我们引入M2I数据集，用于基于基础设施的物体检测，具有多样化的摄像头配置、道路布局和环境条件。在M2I和真实世界数据集RoScenes上的大量实验表明，MIC-BEV在3D物体检测方面实现了最先进的性能，在极端天气和传感器降级等具有挑战性的条件下保持鲁棒性。这些结果突显了MIC-BEV在现实世界部署的潜力。数据集和源代码可在以下链接获取：https://github.com/HandsomeYun/MIC-BEV。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决基于基础设施的多摄像头3D目标检测问题，特别是在多视图基础设施设置、多样化摄像头配置、退化视觉输入和复杂道路布局等挑战场景下的性能提升。这个问题在现实中很重要，因为基于基础设施的感知是智能交通系统的关键组成部分，能提供全局态势感知和协同自主能力；同时，相比昂贵的激光雷达方案，摄像头更实惠、可扩展且提供丰富的语义信息，但现有摄像头检测模型在这些复杂场景中表现不佳。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了基础设施感知与车载感知的区别，指出基础设施摄像头空间分布广泛，具有异构参数，这给现有BEV方法带来挑战。他们设计MIC-BEV框架时借鉴了现有BEV感知方法（如BEVFormer）的Transformer架构，但针对基础设施场景进行了创新。具体来说，他们引入了图增强融合模块，利用摄像头和BEV单元间的几何关系进行特征融合；借鉴了可变形注意力机制进行特征提取；采用DETR风格解码器进行目标检测；并使用图注意力网络(GAT)来学习融合权重，使模型能够自适应地处理不同摄像头配置。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过图神经网络建模摄像头和BEV单元之间的几何关系，实现关系感知的多视图特征融合，使模型能够根据每个摄像头与BEV单元的几何相关性动态分配权重。整体流程包括：1)处理可变数量的摄像头输入；2)使用ResNet-101和FPN提取多尺度特征；3)初始化可学习的BEV查询；4)通过Transformer编码进行特征处理，包括时间自注意力和关系增强空间交叉注意力(ReSCA)；5)使用任务特定解码器进行3D目标检测和BEV语义分割。其中ReSCA模块是关键，它为每个BEV查询生成3D参考点，投影到摄像头视图，使用可变形注意力提取特征，并通过GAT学习融合权重。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)MIC-BEV框架，专门设计用于处理基础设施多摄像头系统的异构配置；2)关系增强的空间交叉注意力(ReSCA)，利用图神经网络建模几何关系；3)M2I数据集，提供多样化的摄像头配置、道路布局和环境条件；4)摄像头掩码策略，提高对传感器降级的鲁棒性。相比之前工作，MIC-BEV能适应不同数量、方向、高度和视场的摄像头；使用显式的关系建模而非隐式融合；考虑摄像头和BEV单元间的几何关系；通过多任务学习（3D检测和BEV分割）增强空间理解；并在训练中模拟传感器故障以提高鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MIC-BEV通过引入关系感知的图增强融合机制和M2I多样化数据集，显著提升了基础设施多摄像头系统在复杂场景下的3D目标检测性能和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Infrastructure-based perception plays a crucial role in intelligenttransportation systems, offering global situational awareness and enablingcooperative autonomy. However, existing camera-based detection models oftenunderperform in such scenarios due to challenges such as multi-viewinfrastructure setup, diverse camera configurations, degraded visual inputs,and various road layouts. We introduce MIC-BEV, a Transformer-basedbird's-eye-view (BEV) perception framework for infrastructure-basedmulti-camera 3D object detection. MIC-BEV flexibly supports a variable numberof cameras with heterogeneous intrinsic and extrinsic parameters anddemonstrates strong robustness under sensor degradation. The proposedgraph-enhanced fusion module in MIC-BEV integrates multi-view image featuresinto the BEV space by exploiting geometric relationships between cameras andBEV cells alongside latent visual cues. To support training and evaluation, weintroduce M2I, a synthetic dataset for infrastructure-based object detection,featuring diverse camera configurations, road layouts, and environmentalconditions. Extensive experiments on both M2I and the real-world datasetRoScenes demonstrate that MIC-BEV achieves state-of-the-art performance in 3Dobject detection. It also remains robust under challenging conditions,including extreme weather and sensor degradation. These results highlight thepotential of MIC-BEV for real-world deployment. The dataset and source code areavailable at: https://github.com/HandsomeYun/MIC-BEV.</description>
      <author>example@mail.com (Yun Zhang, Zhaoliang Zheng, Johnson Liu, Zhiyu Huang, Zewei Zhou, Zonglin Meng, Tianhui Cai, Jiaqi Ma)</author>
      <guid isPermaLink="false">2510.24688v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Optimizing Retrieval for RAG via Reinforced Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.24652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;R3是一种通过试验和反馈强化对比学习为RAG优化的检索框架，能够在RAG环境中动态探索和优化相关性，实验表明其性能优于原始检索器和最先进检索器，且高效实用。&lt;h4&gt;背景&lt;/h4&gt;随着检索增强生成(RAG)的普及，信息检索(IR)的角色正从为人类用户检索信息转变为为AI系统检索上下文知识，这使得相关性难以预先定义或标注。&lt;h4&gt;目的&lt;/h4&gt;提出一种解决方案，使检索器能够在RAG环境中动态探索和优化相关性，无需依赖预先标注的数据。&lt;h4&gt;方法&lt;/h4&gt;R3框架通过检索结果与环境交互产生对比信号，自动引导检索器的自我改进，不同于依赖标注或合成数据进行监督微调的先前方法。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明R3比原始检索器提高RAG性能5.2%，超越最先进检索器4.9%，同时与LLM增强检索和基于后训练或指令调整LLM的RAG系统性能相当。&lt;h4&gt;结论&lt;/h4&gt;R3是一种高效实用的解决方案，解决了在RAG环境中定义和优化相关性的挑战，只需4个GPU一天内完成训练。&lt;h4&gt;翻译&lt;/h4&gt;随着检索增强生成(RAG)变得越来越普遍，信息检索(IR)的角色正从为人类用户检索信息转变为为人工智能(AI)系统检索上下文知识，这使得相关性变得难以预先定义或标注。为应对这一挑战，我们提出了R3，一种通过试验和反馈强化对比学习为RAG优化的检索框架。与依赖标注或合成数据进行监督微调的先前方法不同，R3使检索器能够在RAG环境中动态探索和优化相关性。在训练过程中，检索结果与环境交互以产生对比信号，自动引导检索器的自我改进。在各种不同任务上的大量实验表明，R3比原始检索器提高RAG性能5.2%，超越最先进检索器4.9%，同时与LLM增强检索和基于后训练或指令调整LLM构建的RAG系统相当。R3既高效又实用，只需4个GPU，并在一天内完成训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As retrieval-augmented generation (RAG) becomes increasingly widespread, therole of information retrieval (IR) is shifting from retrieving information forhuman users to retrieving contextual knowledge for artificial intelligence (AI)systems, where relevance becomes difficult to define or annotate beforehand. Toaddress this challenge, we propose R3, a Retrieval framework optimized for RAGthrough trialand-feedback Reinforced contrastive learning. Unlike priorapproaches that rely on annotated or synthetic data for supervised fine-tuning,R3 enables the retriever to dynamically explore and optimize relevance withinthe RAG environment. During training, the retrieved results interact with theenvironment to produce contrastive signals that automatically guide theretriever's self-improvement. Extensive experiments across diverse tasksdemonstrate that R3 improves RAG performance by 5.2% over the originalretriever and surpasses state-of-the-art retrievers by 4.9%, while achievingcomparable results to LLM-augmented retrieval and RAG systems built onpost-trained or instruction-tuned LLMs. It is both efficient and practical,requiring only 4 GPUs and completing training within a single day.</description>
      <author>example@mail.com (Jiawei Zhou, Lei Chen)</author>
      <guid isPermaLink="false">2510.24652v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DeshadowMamba: Deshadowing as 1D Sequential Similarity</title>
      <link>http://arxiv.org/abs/2510.24260v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于Mamba模型的图像阴影去除方法，通过引入CrossGate方向调制机制和ColorShift正则化技术，有效解决了现有方法中阴影结构扭曲和颜色不一致的问题，实现了高质量的阴影去除效果。&lt;h4&gt;背景&lt;/h4&gt;当前基于深度学习的图像阴影去除方法通常依赖于基于注意力的架构来捕获长距离依赖关系，但这些固定的注意模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。&lt;h4&gt;目的&lt;/h4&gt;重新审视阴影去除问题，从序列建模的角度探索更有效的解决方案，解决现有方法中阴影结构扭曲和颜色不一致的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 从序列建模角度探索使用Mamba（选择性状态空间模型）来传播全局上下文；2. 提出CrossGate方向调制机制，将阴影感知相似性注入Mamba的输入门；3. 引入ColorShift正则化，通过合成结构化的信息负样本引导模型抑制颜色污染并实现稳健的颜色恢复。&lt;h4&gt;主要发现&lt;/h4&gt;1. Mamba模型能通过方向状态转换实现有效的全局感受野同时保持位置连续性；2. 直接将Mamba应用于图像数据缺乏阴影-非阴影语义意识且易受颜色干扰；3. 所提出的CrossGate和ColorShift正则化技术能有效解决这些问题；4. DeshadowMamba在公共基准测试上实现了最先进的视觉质量和强大的定量性能。&lt;h4&gt;结论&lt;/h4&gt;通过将序列建模适应于阴影去除所需的完整结构和色度一致性，所提出的方法DeshadowMamba在图像阴影去除任务中取得了显著效果，为解决阴影去除中的结构完整性和色度一致性问题提供了有效途径。&lt;h4&gt;翻译&lt;/h4&gt;最近的图像阴影去除深度模型通常依赖于基于注意力的架构来捕获长距离依赖关系。然而，它们的固定注意模式往往会混合来自不相关区域的照明线索，导致结构扭曲和颜色不一致。在这项工作中，我们从序列建模的角度重新审视阴影去除问题，并探索使用Mamba（一种选择性状态空间模型）来通过方向状态转换传播全局上下文。这些转换产生有效的全局感受野，同时保持位置连续性。尽管有潜力，但直接将Mamba应用于图像数据并非最佳选择，因为它缺乏阴影-非阴影语义意识，并且仍然容易受到附近区域颜色干扰的干扰。为了解决这些局限性，我们提出了CrossGate，一种方向调制机制，将阴影感知相似性注入Mamba的输入门，允许沿过渡轴选择性地集成相关上下文。为了进一步确保外观保真度，我们引入了ColorShift正则化，这是一种由全局颜色统计驱动的对比学习目标。通过合成结构化的信息负样本，它引导模型抑制颜色污染并实现稳健的颜色恢复。这些组件共同将序列建模适应于阴影去除所需的完整结构和色度一致性。在公共基准测试上的大量实验表明，DeshadowMamba实现了最先进的视觉质量和强大的定量性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent deep models for image shadow removal often rely on attention-basedarchitectures to capture long-range dependencies. However, their fixedattention patterns tend to mix illumination cues from irrelevant regions,leading to distorted structures and inconsistent colors. In this work, werevisit shadow removal from a sequence modeling perspective and explore the useof Mamba, a selective state space model that propagates global context throughdirectional state transitions. These transitions yield an efficient globalreceptive field while preserving positional continuity. Despite its potential,directly applying Mamba to image data is suboptimal, since it lacks awarenessof shadow-non-shadow semantics and remains susceptible to color interferencefrom nearby regions. To address these limitations, we propose CrossGate, adirectional modulation mechanism that injects shadow-aware similarity intoMamba's input gate, allowing selective integration of relevant context alongtransition axes. To further ensure appearance fidelity, we introduce ColorShiftregularization, a contrastive learning objective driven by global colorstatistics. By synthesizing structured informative negatives, it guides themodel to suppress color contamination and achieve robust color restoration.Together, these components adapt sequence modeling to the structural integrityand chromatic consistency required for shadow removal. Extensive experiments onpublic benchmarks demonstrate that DeshadowMamba achieves state-of-the-artvisual quality and strong quantitative performance.</description>
      <author>example@mail.com (Zhaotong Yang, Yi Chen, Yanying Li, Shengfeng He, Yangyang Xu, Junyu Dong, Jian Yang, Yong Du)</author>
      <guid isPermaLink="false">2510.24260v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>MATCH: Task-Driven Code Evaluation through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.23169v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了MATCH，一种新颖的无参考代码评估指标，用于解决AI生成代码与开发者意图匹配度评估的挑战。&lt;h4&gt;背景&lt;/h4&gt;AI代码生成越来越普遍，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配程度仍然是一个重大挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决无参考代码评估的空白问题，除了ICE-Score等少数替代方案外，引入MATCH作为一种新颖的无参考代码指标。&lt;h4&gt;方法&lt;/h4&gt;MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码执行任务程度的相似性评分。&lt;h4&gt;主要发现&lt;/h4&gt;MATCH在多种编程语言上，与功能正确性和人类偏好相比，比现有指标实现了更强的相关性。&lt;h4&gt;结论&lt;/h4&gt;MATCH是一种有效的无参考代码评估指标，能够更好地评估生成代码与开发者意图的匹配程度。&lt;h4&gt;翻译&lt;/h4&gt;基于AI的代码生成日益普及，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配程度仍然是一个重大挑战。传统评估方法，如单元测试，通常难以扩展且成本高昂。语法相似性指标（如BLEU、ROUGE）无法捕捉代码功能，而像CodeBERTScore这样的指标需要参考代码，但参考代码并不总是可用的。为了解决无参考代码评估的空白问题，除了ICE-Score等少数替代方案外，本文引入了MATCH，一种新颖的无参考代码指标。MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码执行任务程度的相似性评分。我们表明，MATCH在多种编程语言上，与功能正确性和人类偏好相比，比现有指标实现了更强的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-based code generation is increasingly prevalent, with GitHub Copilotestimated to generate 46% of the code on GitHub. Accurately evaluating how wellgenerated code aligns with developer intent remains a critical challenge.Traditional evaluation methods, such as unit tests, are often unscalable andcostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture codefunctionality, and metrics like CodeBERTScore require reference code, which isnot always available. To address the gap in reference-free evaluation, with fewalternatives such as ICE-Score, this paper introduces MATCH, a novelreference-free metric. MATCH uses Contrastive Learning to generate meaningfulembeddings for code and natural language task descriptions, enabling similarityscoring that reflects how well generated code implements the task. We show thatMATCH achieves stronger correlations with functional correctness and humanpreference than existing metrics across multiple programming languages.</description>
      <author>example@mail.com (Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moshkovitz, Dotan Di Castro)</author>
      <guid isPermaLink="false">2510.23169v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UniField: Joint Multi-Domain Training for Universal Surface Pressure Modeling</title>
      <link>http://arxiv.org/abs/2510.24106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为UniField的方法，通过整合多个子领域的空气动力学数据进行联合训练，解决了数据稀缺问题，实现了更好的流场表示。&lt;h4&gt;背景&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。深度神经网络已成为传统计算流体力学(CFD)模拟的高效替代方案，但数据稀缺限制了其应用。&lt;h4&gt;目的&lt;/h4&gt;解决空气动力学数据稀缺问题，通过整合多个子领域的数据进行联合训练，学习更通用的流场表示。&lt;h4&gt;方法&lt;/h4&gt;提出UniField方法，整合五个不同数据集（涵盖汽车、火车、飞机和一般形状）。该方法采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流动条件适配器来适应不同子领域的流动信息。&lt;h4&gt;主要发现&lt;/h4&gt;尽管不同子领域的空气动力学数据通常遵循不同方程，但联合训练的模型通常比单独训练的模型表现更好，表明这些数据相互补充，帮助模型学习更好的流场表示。&lt;h4&gt;结论&lt;/h4&gt;UniField作为通用流场表示模型具有潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;物体表面压力场的空气动力学模拟对许多工程问题至关重要。近年来，深度神经网络已成为传统计算成本高昂的CFD模拟的高效替代方案，用于建模表面压力场。然而，数据稀缺仍然是一个基本挑战，限制了神经网络的应用。为了解决这一限制，我们提出整合多个子领域的空气动力学数据进行联合训练，以学习更通用的流场表示。我们整合了涵盖不同领域的五个不同数据集，包括汽车、火车、飞机和一般形状。面对不同领域间的显著数据差异，我们提出了UniField，它采用领域无关的Transformer模块提取通用点云特征，并定制领域特定的流动条件适配器来适应不同子领域的流动信息。尽管不同子领域的空气动力学数据通常遵循不同的方程，但我们比较了在所有数据上联合训练的模型与在单个数据集上单独训练的模型，发现联合训练的模型通常表现出更好的性能。这表明这些数据相互补充，帮助模型学习更好的流场表示。这些结果突显了UniField作为通用流场表示模型的潜力，为神经网络在空气动力学分析中的更广泛应用奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aerodynamic simulation of the surface pressure field around objects iscrucial for many engineering problems. In recent years, deep neural networkshave emerged as an efficient alternative to traditional, computationallyexpensive CFD simulations for modeling surface pressure fields. However, datascarcity remains a fundamental challenge, limiting the application of neuralnetworks. To address this limitation, we propose to integrate aerodynamic datafrom multiple subfields and conduct joint training to learn more general fieldrepresentations. We consolidate five different datasets covering variousfields, including automobiles, trains, aircraft, and general shapes. Facingsignificant data differences across different domains, we propose UniField,which employs a domain-agnostic Transformer module to extract general pointcloud features and customizes domain-specific flow-conditioned adapters toadapt to the flow information in different subfields. Despite the fact thataerodynamic data from different subfields are typically governed by differentequations, we compare models trained jointly on all data with those trainedseparately on individual datasets and find that the jointly-trained modelcommonly demonstrates better performance. This indicates that these datacomplement each other to help the model learn better flow fieldrepresentations. These results highlight the potential of UniField as auniversal flow field representation model and lay the foundation for broaderapplications of neural networks in aerodynamic analysis.</description>
      <author>example@mail.com (Junhong Zou, Zhenxu Sun, Yueqing Wang, Wei Qiu, Zhaoxiang Zhang, Zhen Lei, Xiangyu Zhu)</author>
      <guid isPermaLink="false">2510.24106v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DPGLA: Bridging the Gap between Synthetic and Real Data for Unsupervised Domain Adaptation in 3D LiDAR Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2510.23525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication at the 2025 IEEE/RSJ  International Conference on Intelligent Robots and Systems (IROS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种动态伪标签过滤(DPLF)方案和先导引导的数据增强流水线(PG-DAP)，用于增强点云无监督域适应语义分割中真实数据的利用，提高合成到真实点云语义分割的性能。&lt;h4&gt;背景&lt;/h4&gt;为智能自主系统标注真实世界的LiDAR点云成本很高，现有基于自训练的无监督域适应方法在利用合成点云数据时未能有效利用未标记数据。&lt;h4&gt;目的&lt;/h4&gt;提高点云语义分割性能，更有效地利用未标记数据，克服现有方法依赖预定义或固定置信度阈值导致的性能限制。&lt;h4&gt;方法&lt;/h4&gt;提出动态伪标签过滤(DPLF)方案增强真实数据利用，设计先导引导的数据增强流水线(PG-DAP)减轻域偏移，并使用数据混合一致性损失推动模型学习上下文无关表示。&lt;h4&gt;主要发现&lt;/h4&gt;在两个具有挑战性的合成到真实点云语义分割任务上，该方法取得了优越的性能，消融研究证实了DPLF和PG-DAP模块的有效性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效解决现有点云UDA语义分割中未标记数据利用不足的问题，显著提高了合成到真实点云语义分割的性能。&lt;h4&gt;翻译&lt;/h4&gt;为智能自主系统使用而标注真实世界的LiDAR点云成本很高。为克服这一限制，基于自训练的无监督域适应(UDA)已被广泛用于利用合成点云数据提高点云语义分割。然而，我们认为现有方法没有有效利用未标记数据，因为它们要么依赖于预定义或固定的置信度阈值，导致性能不佳。在本文中，我们提出了一种动态伪标签过滤(DPLF)方案，以增强点云UDA语义分割中真实数据的利用。此外，我们设计了一个简单高效的先导引导的数据增强流水线(PG-DAP)，以减轻合成和真实世界点云之间的域偏移。最后，我们使用数据混合一致性损失来推动模型学习上下文无关的表示。我们通过最先进方法的广泛比较实施并彻底评估了我们的方法。在两个具有挑战性的合成到真实点云语义分割任务上的实验表明，我们的方法取得了优越的性能。消融研究证实了DPLF和PG-DAP模块的有效性。我们在本文中发布了我们方法的代码。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决3D激光雷达点云语义分割中的无监督域适应问题，具体是如何有效利用带有自动标签的合成数据来改善对真实世界点云的语义分割，而无需对真实世界数据进行昂贵的标注。这个问题很重要，因为对真实世界的激光雷达点云进行密集标注成本高昂且耗时，而智能自主系统（如自动驾驶汽车）需要在动态真实环境中工作。虽然合成数据可以自动生成标签，但合成环境和真实世界之间存在域分布差异，导致在合成数据上训练的模型在真实数据上性能下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，特别是发现固定置信度阈值导致真实数据利用效率低下和类别不平衡问题。然后设计了解决方案：1) 动态伪标签过滤(DPLF)方案，自适应调整置信度阈值；2) 先验引导的数据增强管道(PG-DAP)，缓解域差异；3) 数据混合一致性损失，学习上下文无关表示。作者借鉴了Mean Teacher模型架构、LaserMix数据混合框架、局部和全局仿射变换以及指数移动平均(EMA)更新机制，但进行了创新性改进以解决特定问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过动态调整伪标签过滤策略和利用先验知识进行高效数据增强，解决合成到真实数据的域适应问题。整体流程：1) 使用Mean Teacher架构，预训练教师网络；2) 教师网络生成目标域伪标签；3) 通过DPLF进行伪标签过滤(距离加权、分层过滤、动态阈值更新)；4) 使用PG-DAP进行数据增强(DAS、DAJ、HAJ)；5) 应用LaserMix进行数据混合；6) 计算分割损失和数据混合一致性损失；7) 更新学生网络参数，使用EMA更新教师网络参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 动态伪标签过滤(DPLF)：自适应调整全局和类别特定置信度阈值，使用距离权重和EMA动态更新，解决固定阈值导致的类别不平衡；2) 先验引导数据增强(PG-DAP)：包含DAS(密度感知采样)、DAJ(距离感知抖动)和HAJ(高度感知抖动)，基于先验知识无需额外学习；3) 数据混合一致性损失：推动模型学习上下文无关表示。相比之前工作，不同之处在于：不使用固定置信度阈值，避免类别不平衡；不依赖计算资源昂贵的GAN进行域转换；结合了动态伪标签过滤和高效数据增强，通过一致性损失进一步改善特征表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DPGLA通过动态伪标签过滤和先验引导数据增强，有效解决了3D激光雷达点云语义分割中合成到真实数据的无监督域适应问题，显著提升了模型在真实场景中的分割性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Annotating real-world LiDAR point clouds for use in intelligent autonomoussystems is costly. To overcome this limitation, self-training-basedUnsupervised Domain Adaptation (UDA) has been widely used to improve pointcloud semantic segmentation by leveraging synthetic point cloud data. However,we argue that existing methods do not effectively utilize unlabeled data, asthey either rely on predefined or fixed confidence thresholds, resulting insuboptimal performance. In this paper, we propose a Dynamic Pseudo-LabelFiltering (DPLF) scheme to enhance real data utilization in point cloud UDAsemantic segmentation. Additionally, we design a simple and efficientPrior-Guided Data Augmentation Pipeline (PG-DAP) to mitigate domain shiftbetween synthetic and real-world point clouds. Finally, we utilize data mixingconsistency loss to push the model to learn context-free representations. Weimplement and thoroughly evaluate our approach through extensive comparisonswith state-of-the-art methods. Experiments on two challenging synthetic-to-realpoint cloud semantic segmentation tasks demonstrate that our approach achievessuperior performance. Ablation studies confirm the effectiveness of the DPLFand PG-DAP modules. We release the code of our method in this paper.</description>
      <author>example@mail.com (Wanmeng Li, Simone Mosco, Daniel Fusaro, Alberto Pretto)</author>
      <guid isPermaLink="false">2510.23525v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Quality-controlled registration of urban MLS point clouds reducing drift effects by adaptive fragmentation</title>
      <link>http://arxiv.org/abs/2510.23416v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures. This manuscript is currently under review at the  International Journal of Applied Earth Observation and Geoinformation  (Elsevier). A preprint version will also be available on SSRN (Elsevier  Preprints) with a DOI once processed. This is the original preprint version  submitted for peer review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种新型工作流程，用于高效准确地在大规模城市街道场景中将移动激光扫描(MLS)点云配准到目标模型点云。&lt;h4&gt;背景&lt;/h4&gt;城市环境中的点云配准面临复杂挑战，包括点云密度差异、噪声特性和遮挡场景，这些在城市中心尤为常见。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够应对城市环境复杂性的工作流程，实现大规模MLS点云与目标模型点云的高效准确配准。&lt;h4&gt;方法&lt;/h4&gt;提出两种方法创新：1) 半球检查(SSC)预处理技术，通过识别相互正交的平面表面分割MLS轨迹数据，减少MLS漂移影响；2) 平面体素广义最近点迭代算法(PV-GICP)，在体素分区中选择性使用平面表面进行精细配准。&lt;h4&gt;主要发现&lt;/h4&gt;慕尼黑市中心真实数据集实验表明，该工作流程实现了平均亚0.01米的配准精度，同时比传统点对平面ICP方法减少50%以上的计算时间。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够推动自动化三维城市建模和更新，在城市规划、基础设施管理和动态城市监测中有直接应用。&lt;h4&gt;翻译&lt;/h4&gt;本研究提出了一种新型工作流程，旨在高效准确地在大规模移动激光扫描(MLS)点云与城市街道场景中的目标模型点云之间进行配准。该工作流程专门针对城市环境中固有的复杂性，巧妙地解决了点云密度、噪声特性和遮挡场景差异带来的挑战，这些挑战在城市中心中普遍存在。研究引入了两种方法创新。首先，提出的半球检查(SSC)预处理技术通过识别相互正交的平面表面，最优地分割MLS轨迹数据。这一步骤减少了MLS漂移对整个点云配准精度的影响，同时确保每个片段内有足够的几何特征以避免局部最小值。其次，我们提出了平面体素广义最近点迭代算法(PV-GICP)，一种在体素分区中选择性使用平面表面的精细配准方法。这种预处理策略不仅提高了配准精度，而且比传统的点对平面ICP方法减少了50%以上的计算时间。在慕尼黑市中心的真实数据集实验中，我们的工作流程实现了亚0.01米的平均配准精度，同时显著缩短了处理时间。研究结果强调了所提出方法在推进自动化三维城市建模和更新方面的潜力，可直接应用于城市规划、基础设施管理和动态城市监测。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决城市环境中大规模移动激光扫描(MLS)点云与目标模型点云的高效准确配准问题，特别是处理城市环境中的复杂性和挑战，包括不同密度、噪声特性和遮挡场景的点云集成，以及MLS长时间扫描过程中产生的漂移效应。这个问题在现实中非常重要，因为城市环境变化迅速，需要频繁更新3D城市模型用于城市规划、基础设施管理和城市发展监测等应用，而传统更新方法劳动密集且容易出错。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别了现有方法的局限性，包括传统点云配准技术在处理大规模城市数据时效率低下，单个变换矩阵可能不足以实现精确对齐，以及固定分割方法在某些缺乏特征的片段中可能失败。在此基础上，作者借鉴了现有的点云分割技术(如等时间间隔分割)、特征匹配方法(如RANSAC)和ICP算法，但进行了创新改进，提出了自适应分割策略确保每个片段包含足够几何特征，以及专门针对城市环境的配准流程。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应分割将大型MLS点云分成包含足够几何特征的小片段减少漂移影响，只在稳定的平面区域进行精细配提高效率和准确性，并利用变换参数分析MLS漂移效应。整体流程包括：1)数据预处理(重采样、去噪、语义分类)；2)数据分割(初始分割后通过半球检查验证)；3)粗配准(特征检测、匹配和异常值去除)；4)精细配准(识别平面区域并执行GICP)；5)漂移分析(评估变换参数)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)半球检查(SSC)自适应分割技术，确保每个片段包含足够的相互正交平面表面；2)基于平面体素的广义ICP(PV-GICP)，选择性使用平面区域提高配准精度并减少50%以上的计算时间；3)漂移分析策略，通过分割过程识别和减少漂移误差。相比之前的工作，传统分割方法使用固定时间间隔可能导致某些片段缺乏特征，传统ICP方法在整个点云上运行计算量大且对非刚性区域敏感，而本文方法通过自适应分割和选择性平面配准解决了这些问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一种基于自适应分割和选择性平面配准的城市MLS点云高质量配准方法，显著提高了配准精度并减少了计算时间，同时有效量化了MLS漂移效应，为自动化3D城市模型更新提供了实用解决方案。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study presents a novel workflow designed to efficiently and accuratelyregister large-scale mobile laser scanning (MLS) point clouds to a target modelpoint cloud in urban street scenarios. This workflow specifically targets thecomplexities inherent in urban environments and adeptly addresses thechallenges of integrating point clouds that vary in density, noisecharacteristics, and occlusion scenarios, which are common in bustling citycenters. Two methodological advancements are introduced. First, the proposedSemi-sphere Check (SSC) preprocessing technique optimally fragments MLStrajectory data by identifying mutually orthogonal planar surfaces. This stepreduces the impact of MLS drift on the accuracy of the entire point cloudregistration, while ensuring sufficient geometric features within each fragmentto avoid local minima. Second, we propose Planar Voxel-based GeneralizedIterative Closest Point (PV-GICP), a fine registration method that selectivelyutilizes planar surfaces within voxel partitions. This pre-process strategy notonly improves registration accuracy but also reduces computation time by morethan 50% compared to conventional point-to-plane ICP methods. Experiments onreal-world datasets from Munich's inner city demonstrate that our workflowachieves sub-0.01 m average registration accuracy while significantlyshortening processing times. The results underscore the potential of theproposed methods to advance automated 3D urban modeling and updating, withdirect applications in urban planning, infrastructure management, and dynamiccity monitoring.</description>
      <author>example@mail.com (Marco Antonio Ortiz Rincon, Yihui Yang, Christoph Holst)</author>
      <guid isPermaLink="false">2510.23416v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Symmetria: A Synthetic Dataset for Learning in Point Clouds</title>
      <link>http://arxiv.org/abs/2510.23414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  40 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Symmetria是一种公式驱动的点云数据集，通过利用对称性概念克服了点云学习中数据稀缺的问题，能够按任意规模生成，提供精确的地面真值，促进数据高效实验，并支持广泛的泛化和扩展。&lt;h4&gt;背景&lt;/h4&gt;与图像或文本领域受益于大量大型数据集不同，点云学习技术经常由于缺乏大规模数据集而遇到限制，这成为研究中的一个主要挑战。&lt;h4&gt;目的&lt;/h4&gt;克服点云数据集稀缺的限制，提出一种可按任意规模生成的公式驱动数据集，为点云学习提供充足且高质量的数据支持。&lt;h4&gt;方法&lt;/h4&gt;利用对称性概念创建具有已知结构和高度可变性的形状，确保精确地面真值的绝对可用性，设计数据集以促进数据高效的实验，实现跨不同几何设置的广泛泛化，并为新任务和模态提供易于扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;该数据集对点云自监督预训练非常有效，训练的模型在分类和分割等下游任务中表现出强大的性能，同时也显示出良好的少样本学习能力；该数据集还可用于真实世界物体的分类，展示了方法的实用价值；作者还引入了一个具有挑战性的对称检测任务，并为基线比较提供了基准。&lt;h4&gt;结论&lt;/h4&gt;Symmetria数据集和相关代码的公开可用性，以及能够生成非常大的数据集集合的能力，为点云学习领域的进一步研究和创新提供了重要基础。&lt;h4&gt;翻译&lt;/h4&gt;与图像或文本领域受益于大量大型数据集不同，点云学习技术经常由于缺乏大规模数据集而遇到限制。为了克服这一限制，我们提出了Symmetria，一种可按任意规模生成的公式驱动数据集。通过构造，它确保精确地面真值的绝对可用性，通过需要更少的样本促进数据高效的实验，实现跨不同几何设置的广泛泛化，并为新任务和模态提供易于扩展性。利用对称性的概念，我们创建了具有已知结构和高度可变性的形状，使神经网络能够有效地学习点云特征。我们的结果表明，该数据集对于点云自监督预训练非常有效，产生的模型在分类和分割等下游任务中表现出强大的性能，同时也显示出良好的少样本学习能力。此外，我们的数据集可以支持将模型微调以分类真实世界物体，突显了我们方法的实用性和应用价值。我们还引入了一个具有挑战性的对称检测任务，并为基线比较提供了基准。我们方法的一个显著优势是数据集、配套代码的公开可用性，以及生成非常大集合的能力，促进了点云学习的进一步研究和创新。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云学习领域中的数据稀缺问题。与图像或文本领域有大量大规模数据集不同，点云学习技术经常因缺乏大规模数据集而受到限制。这个问题很重要，因为点云是3D视觉的重要表示形式，广泛应用于机器人、自动驾驶等领域；缺乏大规模数据集限制了点云深度学习模型的发展；现有的3D数据集存在版权、隐私和标注成本等问题；真实世界的3D数据获取成本高、耗时长，难以满足机器学习所需的规模。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者观察到图像领域有公式驱动的数据集表现出了有趣性能，甚至在某些任务上超过了ImageNet，这启发了他们在3D点云领域采用类似方法。他们借鉴了SHREC23数据集的工作，但进行了扩展和改进。作者基于对称性概念设计数据集，因为几乎现实世界中的每个物体都表现出某种对称性。他们设计数据集时遵循了几个原则：大规模探索、数据效率、可用的真实标签、隐私保护和通用可扩展性。从平面参数曲线开始生成3D形状，这些曲线具有已知的几何特性，然后通过挤压或旋转操作将它们转换为3D表面。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用对称性作为生成合成点云数据集的基础，通过参数化平面曲线生成具有已知对称结构的形状，然后通过几何变换转换为3D表面，并添加各种扰动增加数据集的多样性和挑战性。整体实现流程包括：1)从具有对称性的参数化平面曲线开始；2)通过挤压（大多数曲线）或旋转（贝塞尔曲线）将曲线转换为3D表面；3)确保3D形状保持原始平面曲线的对称性；4)应用各种变换增加数据集多样性；5)以一定概率应用随机平移和/或旋转；6)将生成的点云组织成不同复杂度的子数据集；7)为每个点云提供其对称性的真实标签信息。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于对称性的合成数据集；2)公式驱动的生成方法；3)精确的真实标签；4)数据效率；5)多样化的几何变换；6)可扩展性；7)多任务支持。相比之前的工作（如ShapeNet、ModelNet、SHREC23等）的不同之处：1)数据来源不同，Symmetria是完全合成的，避免了版权和隐私问题；2)生成方式不同，使用了更丰富的参数化平面曲线库；3)数据规模可按需生成任意规模；4)提供了更全面的真实标签注释；5)不仅验证了在传统任务上的有效性，还专门验证了对称性检测这一特定任务上的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了Symmetria，一个基于对称性的大规模合成点云数据集，通过程序化生成方法解决了3D点云学习中的数据稀缺问题，同时提供了精确的真实标签和多样化的几何变换，有效支持了自监督预训练和对称性检测等任务。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unlike image or text domains that benefit from an abundance of large-scaledatasets, point cloud learning techniques frequently encounter limitations dueto the scarcity of extensive datasets. To overcome this limitation, we presentSymmetria, a formula-driven dataset that can be generated at any arbitraryscale. By construction, it ensures the absolute availability of precise groundtruth, promotes data-efficient experimentation by requiring fewer samples,enables broad generalization across diverse geometric settings, and offers easyextensibility to new tasks and modalities. Using the concept of symmetry, wecreate shapes with known structure and high variability, enabling neuralnetworks to learn point cloud features effectively. Our results demonstratethat this dataset is highly effective for point cloud self-supervisedpre-training, yielding models with strong performance in downstream tasks suchas classification and segmentation, which also show good few-shot learningcapabilities. Additionally, our dataset can support fine-tuning models toclassify real-world objects, highlighting our approach's practical utility andapplication. We also introduce a challenging task for symmetry detection andprovide a benchmark for baseline comparisons. A significant advantage of ourapproach is the public availability of the dataset, the accompanying code, andthe ability to generate very large collections, promoting further research andinnovation in point cloud learning.</description>
      <author>example@mail.com (Ivan Sipiran, Gustavo Santelices, Lucas Oyarzún, Andrea Ranieri, Chiara Romanengo, Silvia Biasotti, Bianca Falcidieno)</author>
      <guid isPermaLink="false">2510.23414v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Workspace Registration and Collision Detection for Industrial Robotics Applications</title>
      <link>http://arxiv.org/abs/2510.23227v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文主要研究机器人运动规划中的环境建模和碰撞检测方法&lt;h4&gt;背景&lt;/h4&gt;机器人运动规划依赖于对环境的精确知识，需要定义受限区域并考虑碰撞物体&lt;h4&gt;目的&lt;/h4&gt;比较不同传感器，说明从检测到完成碰撞环境的过程，以及检测机器人与环境的碰撞&lt;h4&gt;方法&lt;/h4&gt;使用各种传感器获取环境点云，通过区域增长分割和VCCS算法识别碰撞物体，并对点簇进行近似处理&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体发现&lt;h4&gt;结论&lt;/h4&gt;摘要中未明确提及结论&lt;h4&gt;翻译&lt;/h4&gt;机器人运动规划依赖于对环境的精确知识，以便能够定义受限区域并考虑碰撞物体。为了捕获工作空间，使用各种传感器获取环境的点云。碰撞物体通过区域增长分割和VCCS算法进行识别。随后对点簇进行近似处理。本文的目的是比较不同的传感器，说明从检测到完成碰撞环境的过程，并检测机器人与该环境之间的碰撞。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决工业机器人在复杂生产环境中进行精确环境感知和碰撞检测的问题。这个问题在现实中非常重要，因为它关系到机器人能否安全高效地工作，避免与周围环境发生碰撞，同时最大化利用可用工作空间，确保生产过程的顺利进行和人员设备的安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先考虑了机器人操作环境需要精确建模的需求，然后设计了一套从环境感知到碰撞检测的完整流程。他们借鉴了多项现有工作：使用了Point Cloud Library (PCL)中的方法和算法；应用了区域增长分割算法来识别物体；采用了Voxel Cloud Connectivity Segmentation (VCCS)算法进行更精细的分割；并使用了分离轴定理和基于Minkowski差的碰撞检测方法。作者的主要贡献在于将这些技术整合成一个完整的工业机器人应用系统，并进行了系统性的比较和优化。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过传感器获取环境点云数据，经过处理和分割后将环境近似为简单的几何形状，然后应用碰撞检测算法确保机器人安全运行。整体流程包括：1)环境描述：将机器人操作环境表示为点云数据；2)检测和特征提取：使用3D传感器获取环境数据，进行预处理和去噪，通过分割算法识别和聚类物体；3)边界框生成：将聚类近似为立方体边界框，并与物体协方差矩阵主轴对齐以减少空间损失；4)碰撞检测：使用分离轴定理或基于Minkowski差的方法检测机器人与环境的碰撞。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)系统比较了不同传感器(TOF、主动立体视觉)在工业机器人环境感知中的性能差异；2)提出了一套从环境检测到完成碰撞环境的完整流程；3)研究了不同的物体近似方法及其对可用空间的影响；4)比较了不同碰撞检测方法在可用空间和约束数量方面的表现。相比之前的工作，这篇论文不仅关注单一算法，而是关注整个系统流程；不仅评估算法性能，还考虑了实际可用工作空间；通过物体对齐方法和基于Minkowski差的碰撞检测，显著减少了约束数量，提高了路径规划效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 该论文提出了一套完整的工业机器人环境感知与碰撞检测方法，通过系统比较不同传感器和碰撞检测算法，优化了机器人工作空间利用率和路径规划效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning for robotic manipulators relies on precise knowledge of theenvironment in order to be able to define restricted areas and to takecollision objects into account. To capture the workspace, point clouds of theenvironment are acquired using various sensors. The collision objects areidentified by region growing segmentation and VCCS algorithm. Subsequently thepoint clusters are approximated. The aim of the present paper is to comparedifferent sensors, to illustrate the process from detection to the finishedcollision environment and to detect collisions between the robot and thisenvironment.</description>
      <author>example@mail.com (Klaus Zauner, Josef El Dib, Hubert Gattringer, Andreas Mueller)</author>
      <guid isPermaLink="false">2510.23227v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>UGAE: Unified Geometry and Attribute Enhancement for G-PCC Compressed Point Clouds</title>
      <link>http://arxiv.org/abs/2510.23009v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种统一的几何和属性增强(UGAE)框架，通过三个核心组件(PoGE、PAE和PoAE)有效解决了点云有损压缩导致的几何结构和属性信息失真问题，在多个基准数据集上显著优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;点云的有损压缩可以减少存储和传输成本，但不可避免地导致几何结构和属性信息中的不可逆失真。&lt;h4&gt;目的&lt;/h4&gt;解决有损压缩导致的几何结构和属性信息失真问题，提高压缩后的点云质量。&lt;h4&gt;方法&lt;/h4&gt;提出统一的几何和属性增强(UGAE)框架，包含三个核心组件：1)后几何增强(PoGE)使用基于Transformer的稀疏卷积U-Net重建几何结构；2)预属性增强(PAE)引入增强几何引导的重新着色策略，使用DA-KNN方法保留高频细节；3)后属性增强(PoAE)使用带W-MSE损失的属性残差预测网络增强高频区域质量。&lt;h4&gt;主要发现&lt;/h4&gt;UGAE在8iVFB、Owlii和MVUB三个基准数据集上显著优于现有方法；与G-PCC测试模型相比，几何部分平均BD-PSNR增益9.98 dB，BD-比特率节省90.98%；属性部分BD-PSNR提高3.67 dB，BD-比特率节省56.88%；显著改善了感知质量。&lt;h4&gt;结论&lt;/h4&gt;UGAE框架能有效解决点云有损压缩导致的失真问题，在多个指标上表现优异，具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;有损压缩点云减少了存储和传输成本；然而，它不可避免地导致几何结构和属性信息中的不可逆失真。为解决这些问题，我们提出了统一的几何和属性增强(UGAE)框架，包含三个核心组件：后几何增强(PoGE)、预属性增强(PAE)和后属性增强(PoAE)。在PoGE中，使用基于Transformer的稀疏卷积U-Net通过预测体素占用概率高精度重建几何结构。基于改进的几何结构，PAE引入创新的增强几何引导重新着色策略，使用细节感知的K-近邻(DA-KNN)方法实现精确重新着色，并在属性压缩前有效保留高频细节。最后，在解码器端，PoAE使用带加权均方误差(W-MSE)损失的属性残差预测网络，增强高频区域质量，同时保持低频区域的保真度。UGAE在三个基准数据集上显著优于现有方法：8iVFB、Owlii和MVUB。与最新的G-PCC测试模型(TMC13v29)相比，UGAE在D1指标下几何部分平均BD-PSNR增益9.98 dB，BD-比特率节省90.98%，属性部分在Y分量上BD-PSNR提高3.67 dB，BD-比特率节省56.88%。此外，它显著改善了感知质量。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云压缩过程中几何结构和属性信息不可避免产生的不可逆失真问题。这个问题在现实中非常重要，因为点云数据广泛应用于自动驾驶、文化遗产保护和虚拟现实等领域，高精度点云数据量大，存储和传输成本高，而现有压缩方法在减少数据量的同时会引入失真，影响3D应用的效率和用户体验。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云压缩方法的局限性，发现它们主要分别优化几何或属性，忽略了两者间的耦合关系。现有联合增强方法仅在解码器端进行增强，无法充分利用几何增强对属性压缩的好处。作者借鉴了点云上采样方法（如PU-Net、PUFA-GAN）、稀疏卷积方法（如PU-Dense、GRNet）以及网络架构（Transformer和U-Net）和损失函数（BCE和MSE），设计了包含PoGE、PAE和PoAE三个核心组件的UGAE框架，在整个压缩过程中协同优化几何和属性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是联合优化几何和属性，通过编码器-解码器协同增强来保留高频细节，分阶段解决压缩失真问题。整体流程是：编码器端，PoGE接收有损几何并生成增强几何结构；PAE使用增强几何和原始属性通过DA-KNN重新着色生成中间属性。传输有损几何比特流和重新着色后的属性比特流。解码器端，PoGE重建相同的增强几何；PoAE使用W-MSE损失函数专注于重建属性残差，特别是在高频区域。最终输出联合增强的点云。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 统一的几何和属性增强框架(UGAE)；2) 后几何增强(PoGE)结合Transformer和U-Net架构，使用密集连接并解决GPU随机性问题；3) 前属性增强(PAE)引入DA-KNN算法保留高频细节；4) 后属性增强(PoAE)使用W-MSE损失函数专注于高频区域。相比之前工作，UGAE同时处理几何和属性失真，考虑两者耦合关系，在编码器和解码器端都进行增强，而G-PCC++等现有方法仅在解码器端增强，且基于有损几何进行重新着色。UGAE在三个基准数据集上性能显著优于现有方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了UGAE框架，通过在编码器和解码器端协同优化几何和属性增强，显著提高了点云压缩质量，特别是在保留高频细节方面表现优异。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lossy compression of point clouds reduces storage and transmission costs;however, it inevitably leads to irreversible distortion in geometry structureand attribute information. To address these issues, we propose a unifiedgeometry and attribute enhancement (UGAE) framework, which consists of threecore components: post-geometry enhancement (PoGE), pre-attribute enhancement(PAE), and post-attribute enhancement (PoAE). In PoGE, a Transformer-basedsparse convolutional U-Net is used to reconstruct the geometry structure withhigh precision by predicting voxel occupancy probabilities. Building on therefined geometry structure, PAE introduces an innovative enhancedgeometry-guided recoloring strategy, which uses a detail-aware K-NearestNeighbors (DA-KNN) method to achieve accurate recoloring and effectivelypreserve high-frequency details before attribute compression. Finally, at thedecoder side, PoAE uses an attribute residual prediction network with aweighted mean squared error (W-MSE) loss to enhance the quality ofhigh-frequency regions while maintaining the fidelity of low-frequency regions.UGAE significantly outperformed existing methods on three benchmark datasets:8iVFB, Owlii, and MVUB. Compared to the latest G-PCC test model (TMC13v29),UGAE achieved an average BD-PSNR gain of 9.98 dB and 90.98% BD-bitrate savingsfor geometry under the D1 metric, as well as a 3.67 dB BD-PSNR improvement with56.88% BD-bitrate savings for attributes on the Y component. Additionally, itimproved perceptual quality significantly.</description>
      <author>example@mail.com (Pan Zhao, Hui Yuan, Chongzhen Tian, Tian Guo, Raouf Hamzaoui, Zhigeng Pan)</author>
      <guid isPermaLink="false">2510.23009v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Scaling Up Occupancy-centric Driving Scene Generation: Dataset and Method</title>
      <link>http://arxiv.org/abs/2510.22973v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的占据中心驾驶场景生成方法，通过创建大规模语义占据数据集Nuplan-Occ和开发统一框架，解决了占据中心方法对标注数据依赖的问题，实现了高质量语义占据、多视图视频和LiDAR点云的联合生成。&lt;h4&gt;背景&lt;/h4&gt;场景生成是自动驾驶的关键领域，占据中心方法最近取得了最先进的结果，但这些方法严重依赖于标注的占据数据，而这类数据仍然稀缺。&lt;h4&gt;目的&lt;/h4&gt;克服占据中心方法对标注数据的依赖限制，创建大规模语义占据数据集，并开发统一框架实现多模态场景生成。&lt;h4&gt;方法&lt;/h4&gt;创建Nuplan-Occ数据集，开发统一框架联合生成语义占据、多视图视频和LiDAR点云，采用时空解耦架构支持4D动态占据的扩展和预测，提出基于高斯飞溅的稀疏点图渲染策略和传感器感知嵌入策略。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在生成保真度和可扩展性方面优于现有方法，在下游任务中验证了其实用价值。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的统一占据中心驾驶场景生成方法在自动驾驶场景生成方面表现出色，有助于感知和规划评估等下游应用。&lt;h4&gt;翻译&lt;/h4&gt;场景生成是自动驾驶的关键领域，使包括感知和规划评估在内的下游应用成为可能。占据中心方法最近通过提供跨帧和模态的一致条件取得了最先进的结果；然而，它们的性能严重依赖于标注的占据数据，而这种数据仍然稀缺。为了克服这一限制，我们整理了Nuplan-Occ，这是迄今为止最大的语义占据数据集，由广泛使用的Nuplan基准构建而成。其规模和多样性不仅促进了大规模生成建模，也促进了自动驾驶的下游应用。基于此数据集，我们开发了一个统一框架，联合合成高质量语义占据、多视图视频和LiDAR点云。我们的方法采用时空解耦架构，支持4D动态占据的高保真空间扩展和时间预测。为了弥合模态差距，我们进一步提出了两种新颖技术：一种基于高斯飞溅的稀疏点图渲染策略，增强多视图视频生成；一种传感器感知嵌入策略，明确建模LiDAR传感器特性，以实现真实的多LiDAR模拟。大量实验表明，与现有方法相比，我们的方法在生成保真度和可扩展性方面取得了优越的性能，并在下游任务中验证了其实际价值。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决自动驾驶场景生成中的规模限制问题。现有占用中心方法虽先进，但受限于标注数据稀缺，无法实现大规模训练；同时，多模态生成（语义占用、视频、LiDAR）存在模态差距，导致生成质量受限。这一问题对自动驾驶领域至关重要，因为高质量场景生成能支持感知和规划评估，降低开发成本，提高系统鲁棒性和安全性，同时支持算法在多样化环境中训练，提升泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到现有方法受限于数据稀缺性，无法实现大规模训练；然后意识到需要构建更大规模数据集；观察到多模态生成存在模态差距，需要新技术弥合；提出时空解耦架构分解4D占用生成；为解决视频生成中的传感器校准问题，引入高斯飞溅稀疏点图渲染；为实现真实LiDAR模拟，提出传感器感知嵌入策略。该方法借鉴了UniScene的前期工作，利用了扩散模型、3D高斯表示、CogVideoX的3D因果VAE和体积渲染技术等现有成果。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是以语义占用为中心构建统一框架，通过时空解耦架构将4D动态占用生成分解为空间扩展和时间预测，利用大规模数据集支持训练，并通过专门技术弥合模态差距。整体流程：1)构建Nuplan-Occ数据集，使用前景-背景分离聚合策略；2)4D占用生成，使用VAE和DiT编码解码，时空解耦处理；3)视频生成，将占用转为3D高斯基元渲染成稀疏点图，用视频扩散Transformer生成；4)LiDAR生成，使用传感器感知嵌入和稀疏UNet，应用射线平滑正则化。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)可扩展的统一4D动态场景生成框架，联合扩展模型架构和数据；2)4D占用的时空解耦建模，分离空间扩展和时间预测；3)多传感器真实性的模态桥接策略，包括稀疏点图渲染和传感器感知嵌入；4)构建Nuplan-Occ最大语义占用数据集。相比之前工作，本文数据规模更大（比Nuscenes-Occupancy大19倍），采用时空解耦架构而非混合处理，使用分层生成策略和稀疏渲染技术，在多个任务上取得了最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了UniScenev2，一个基于大规模Nuplan-Occ数据集的统一占用中心框架，通过时空解耦架构和模态桥接技术，实现了高质量语义占用、多视角视频和LiDAR点云的联合生成，显著提升了自动驾驶场景生成的规模和质量。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driving scene generation is a critical domain for autonomous driving,enabling downstream applications, including perception and planning evaluation.Occupancy-centric methods have recently achieved state-of-the-art results byoffering consistent conditioning across frames and modalities; however, theirperformance heavily depends on annotated occupancy data, which still remainsscarce. To overcome this limitation, we curate Nuplan-Occ, the largest semanticoccupancy dataset to date, constructed from the widely used Nuplan benchmark.Its scale and diversity facilitate not only large-scale generative modeling butalso autonomous driving downstream applications. Based on this dataset, wedevelop a unified framework that jointly synthesizes high-quality semanticoccupancy, multi-view videos, and LiDAR point clouds. Our approach incorporatesa spatio-temporal disentangled architecture to support high-fidelity spatialexpansion and temporal forecasting of 4D dynamic occupancy. To bridge modalgaps, we further propose two novel techniques: a Gaussian splatting-basedsparse point map rendering strategy that enhances multi-view video generation,and a sensor-aware embedding strategy that explicitly models LiDAR sensorproperties for realistic multi-LiDAR simulation. Extensive experimentsdemonstrate that our method achieves superior generation fidelity andscalability compared to existing approaches, and validates its practical valuein downstream tasks. Repo:https://github.com/Arlo0o/UniScene-Unified-Occupancy-centric-Driving-Scene-Generation/tree/v2</description>
      <author>example@mail.com (Bohan Li, Xin Jin, Hu Zhu, Hongsi Liu, Ruikai Li, Jiazhe Guo, Kaiwen Cai, Chao Ma, Yueming Jin, Hao Zhao, Xiaokang Yang, Wenjun Zeng)</author>
      <guid isPermaLink="false">2510.22973v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>TWC-SLAM: Multi-Agent Cooperative SLAM with Text Semantics and WiFi Features Integration for Similar Indoor Environments</title>
      <link>http://arxiv.org/abs/2510.22754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the IEEE/RSJ International Conference on Intelligent  Robots and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了TWC-SLAM，一种多智能体协作SLAM框架，通过整合文本语义和WiFi信号特征来增强位置识别和回环检测，以提高在具有重复结构的相似室内环境中的协作SLAM性能。&lt;h4&gt;背景&lt;/h4&gt;多智能体协作SLAM在具有重复结构的相似室内环境中（如走廊和房间）常面临挑战。当使用基于点云的技术时，这些挑战会导致共享位置识别出现显著不准确。&lt;h4&gt;目的&lt;/h4&gt;减轻多智能体协作SLAM在相似室内环境中的挑战，提高共享位置识别的准确性。&lt;h4&gt;方法&lt;/h4&gt;TWC-SLAM框架包括基于FAST-LIO2的单智能体前端里程计模块、利用文本语义和WiFi特征的位置识别和回环检测模块以及全局映射模块。智能体配备了能够捕获文本信息和检测WiFi信号的传感器。通过关联这些数据源，TWC-SLAM建立共同位置，促进不同智能体地图之间的点云对齐，并采用回环检测和优化模块实现全局优化和一致性映射。&lt;h4&gt;主要发现&lt;/h4&gt;使用具有相似走廊、房间和文本标志的室内数据集评估的结果表明，TWC-SLAM显著提高了在具有重复建筑特征的复杂环境中协作SLAM系统的性能。&lt;h4&gt;结论&lt;/h4&gt;整合文本语义和WiFi信号特征可以有效提高多智能体协作SLAM在具有重复结构的相似室内环境中的性能，特别是在位置识别和回环检测方面。&lt;h4&gt;翻译&lt;/h4&gt;多智能体协作SLAM常在具有重复结构的相似室内环境（如走廊和房间）中遇到挑战。当采用基于点云的技术时，这些挑战可能导致共享位置识别出现显著不准确。为缓解这些问题，我们引入了TWC-SLAM，一种多智能体协作SLAM框架，它整合文本语义和WiFi信号特征以增强位置识别和回环检测。TWC-SLAM包括基于FAST-LIO2的单智能体前端里程计模块、利用文本语义和WiFi特征的位置识别和回环检测模块以及全局映射模块。智能体配备了能够捕获文本信息和检测WiFi信号的传感器。通过关联这些数据源，TWC-SLAM建立共同位置，促进不同智能体地图之间的点云对齐。此外，系统采用回环检测和优化模块来实现全局优化和一致性映射。我们使用具有相似走廊、房间和文本标志的室内数据集评估了我们的方法。结果表明，TWC-SLAM显著提高了在具有重复建筑特征的复杂环境中协作SLAM系统的性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多智能体协同SLAM在具有重复结构的相似室内环境（如走廊和相似房间）中面临的位置识别不准确问题。这个问题在现实中很重要，因为许多室内环境（如办公楼、医院、学校）都有相似结构，传统基于点云的技术容易在这些环境中产生错误匹配，导致地图不一致和定位错误，影响多智能体系统在检查、救援、物流等领域的实际应用效果。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法在相似环境中的局限性，然后借鉴了现有工作如FAST-LIO2作为前端里程计模块，参考了TextSLAM等文本识别方法和SpotFi等WiFi定位技术。作者设计了多模态融合思路，结合文本语义（提供明确标识但可能重复）和WiFi特征（提供环境特定信号但区分度有限）两种互补信息，设计了四个主要组件：多智能体前端里程计、文本语义匹配、WiFi特征匹配和全局映射模块，通过算法1实现多模态位置识别，确保位置识别的准确性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过整合文本语义和WiFi特征这两种互补的模态信息，解决相似室内环境中多智能体协同SLAM的位置识别挑战。整体流程包括：1)多智能体使用FAST-LIO2计算里程和生成点云地图；2)通过OCR提取文本语义，使用Levenshtein距离计算文本相似度进行匹配；3)收集WiFi数据，计算MAC地址相似度和RSS值相似度进行验证；4)基于匹配结果识别相同位置，使用点云匹配方法计算坐标变换，执行回环检测和全局优化，生成一致的全局地图。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首创性地整合文本语义与WiFi特征进行多智能体协同SLAM；2)提出新颖的回环检测和位置识别方法，协同利用两种模态信息；3)构建专门的多智能体数据集。相比之前工作，TWC-SLAM比传统点云方法精度高88%，比纯文本方法高82%，比纯WiFi方法高92%。它解决了单一模态方法在相似环境中的局限性，通过多模态融合提高了位置识别的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; TWC-SLAM通过创新性地整合文本语义与WiFi特征，显著提高了多智能体协同SLAM系统在具有重复结构的相似室内环境中的定位精度和地图一致性，解决了传统方法在复杂环境中易出现错误匹配的关键挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent cooperative SLAM often encounters challenges in similar indoorenvironments characterized by repetitive structures, such as corridors androoms. These challenges can lead to significant inaccuracies in shared locationidentification when employing point cloud-based techniques. To mitigate theseissues, we introduce TWC-SLAM, a multi-agent cooperative SLAM framework thatintegrates text semantics and WiFi signal features to enhance locationidentification and loop closure detection. TWC-SLAM comprises a single-agentfront-end odometry module based on FAST-LIO2, a location identification andloop closure detection module that leverages text semantics and WiFi features,and a global mapping module. The agents are equipped with sensors capable ofcapturing textual information and detecting WiFi signals. By correlating thesedata sources, TWC-SLAM establishes a common location, facilitating point cloudalignment across different agents' maps. Furthermore, the system employs loopclosure detection and optimization modules to achieve global optimization andcohesive mapping. We evaluated our approach using an indoor dataset featuringsimilar corridors, rooms, and text signs. The results demonstrate that TWC-SLAMsignificantly improves the performance of cooperative SLAM systems in complexenvironments with repetitive architectural features.</description>
      <author>example@mail.com (Chunyu Li, Shoubin Chen, Dong Li, Weixing Xue, Qingquan Li)</author>
      <guid isPermaLink="false">2510.22754v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Continuum Robot Shape under External Loading using Spatiotemporal Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 IEEE/RSJ International Conference on Intelligent Robots and  Systems (IROS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的方法，用于准确估计受外部负载的柔性连续体机器人的3D形状。该方法通过时空神经网络架构融合多模态输入，生成点云表示机器人变形配置，并通过拟合贝塞尔曲线实现连续3D形状重建。实验验证显示该方法具有高精度，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;柔性连续体机器人在外部负载下的3D形状估计是一个挑战性问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于学习的方法来准确估计柔性连续体机器人在外部负载下的3D形状。&lt;h4&gt;方法&lt;/h4&gt;提出时空神经网络架构，融合多模态输入（当前和历史肌腱位移数据以及RGB图像），生成点云表示机器人变形配置。网络集成了循环神经模块进行时间特征提取，编码模块进行空间特征提取，以及多模态融合模块结合视觉数据的空间特征和历史执行器输入的时间依赖性。通过将贝塞尔曲线拟合到预测点云上实现连续3D形状重建。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证显示该方法具有高精度，无负载时平均形状估计误差为0.08毫米，负载时为0.22毫米，优于最先进的TDCRs形状传感方法。&lt;h4&gt;结论&lt;/h4&gt;基于深度学习的时空数据融合在负载条件下能有效实现精确的形状估计。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于学习的方法，用于准确估计受外部负载的柔性连续体机器人的3D形状。提出的方法引入了一种时空神经网络架构，融合多模态输入，包括当前和历史肌腱位移数据以及RGB图像，生成代表机器人变形配置的点云。网络集成了循环神经模块进行时间特征提取，编码模块进行空间特征提取，以及多模态融合模块来结合从视觉数据中提取的空间特征和来自历史执行器输入的时间依赖性。通过将贝塞尔曲线拟合到预测的点云上实现连续3D形状重建。实验验证表明，我们的方法实现了高精度，无负载时平均形状估计误差为0.08毫米，负载时为0.22毫米，优于TDCRs形状传感的最先进方法。结果证明了基于深度学习的时空数据融合在负载条件下精确形状估计的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何准确估计在外部负载作用下的柔性连续机器人的3D形状问题。这个问题很重要，因为连续机器人在医疗手术、工业检测等领域有广泛应用，而外部负载会改变它们的变形行为，使得准确预测形状变得复杂。精确的形状估计对机器人控制至关重要，能帮助它们动态调整配置以优化任务执行和环境交互。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括传感器方法的精度限制和易损坏问题，模型方法难以处理材料非线性和未知外部负载，以及数据驱动方法在负载下鲁棒性有限。他们设计了一种时空神经网络架构，融合多模态输入数据。该方法借鉴了U-Net架构用于视觉特征提取，LSTM网络捕捉时间依赖性，以及空间注意力机制进行特征融合。同时创新性地将这些技术组合，形成了一个能够处理外部负载条件下的形状估计系统。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用时空神经网络融合视觉数据（RGB图像）和肌腱位移数据，建立这些多模态输入与连续机器人3D形状之间的非线性映射关系。整体流程包括：1)输入当前和历史肌腱位移数据及RGB图像；2)通过空间特征提取器从图像中提取空间特征；3)利用时间特征提取器处理肌腱位移序列；4)通过注意力机制融合时空特征；5)预测代表机器人形状的3D点云；6)使用贝塞尔曲线拟合点云生成连续3D形状。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)多模态数据融合，同时利用视觉和肌腱位移数据；2)时空神经网络架构，结合卷积和循环神经网络；3)在外部负载下实现高精度形状估计；4)端到端学习框架。相比之前工作，该方法结合了视觉和本体感觉数据，提高了精度和鲁棒性；使用更先进的神经网络架构；在各种负载条件下表现出更好的一致性；无需显式机械建模，能处理材料非线性和未知外部负载。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于时空神经网络的创新方法，通过融合视觉和肌腱位移数据，实现了在外部负载条件下高精度估计连续机器人3D形状的目标，相比现有方法在精度和鲁棒性上均有显著提升。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a learning-based approach for accurately estimating the3D shape of flexible continuum robots subjected to external loads. The proposedmethod introduces a spatiotemporal neural network architecture that fusesmulti-modal inputs, including current and historical tendon displacement dataand RGB images, to generate point clouds representing the robot's deformedconfiguration. The network integrates a recurrent neural module for temporalfeature extraction, an encoding module for spatial feature extraction, and amulti-modal fusion module to combine spatial features extracted from visualdata with temporal dependencies from historical actuator inputs. Continuous 3Dshape reconstruction is achieved by fitting B\'ezier curves to the predictedpoint clouds. Experimental validation demonstrates that our approach achieveshigh precision, with mean shape estimation errors of 0.08 mm (unloaded) and0.22 mm (loaded), outperforming state-of-the-art methods in shape sensing forTDCRs. The results validate the efficacy of deep learning-based spatiotemporaldata fusion for precise shape estimation under loading conditions.</description>
      <author>example@mail.com (Enyi Wang, Zhen Deng, Chuanchuan Pan, Bingwei He, Jianwei Zhang)</author>
      <guid isPermaLink="false">2510.22339v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Breaking the Static Assumption: A Dynamic-Aware LIO Framework Via Spatio-Temporal Normal Analysis</title>
      <link>http://arxiv.org/abs/2510.22313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, Accepted to IEEE Robotics and Automation Letters  (RA-L)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种解决动态环境中激光雷达-惯性里程计(LIO)挑战的新方法，通过将动态感知直接集成到点云配准过程中，打破了静态特征识别和姿态估计之间的循环依赖关系。&lt;h4&gt;背景&lt;/h4&gt;传统LIO算法基于静态世界假设，在动态环境中表现不佳，特别是在动态物体主导场景和几何稀疏环境中。当前动态LIO方法面临根本性挑战：准确的定位需要可靠识别静态特征，而区分动态物体又需要精确的姿态估计。&lt;h4&gt;目的&lt;/h4&gt;解决动态环境中的LIO挑战，打破静态特征识别和姿态估计之间的循环依赖关系。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并配以高效的空间一致性验证方法来增强静态地图构建。&lt;h4&gt;主要发现&lt;/h4&gt;在具有有限几何结构的挑战性动态环境中，与最先进的LIO系统相比，性能有显著提升。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效解决了动态环境中的LIO问题，代码和数据集已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;本文解决了动态环境中激光雷达-惯性里程计(LIO)的挑战，传统方法由于其静态世界假设经常失败。当动态物体主导场景，特别是在几何稀疏环境中时，传统LIO算法表现不佳。当前动态LIO方法面临一个基本挑战：准确的定位需要可靠识别静态特征，而区分动态物体又需要精确的姿态估计。我们的解决方案通过将动态感知直接集成到点云配准过程中，打破了这种循环依赖。我们引入了一种新颖的动态感知迭代最近点算法，利用时空法线分析，并辅以高效的空间一致性验证方法来增强静态地图构建。实验评估表明，在具有有限几何结构的挑战性动态环境中，与最先进的LIO系统相比，性能有显著提升。代码和数据集可在https://github.com/thisparticle/btsa获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决激光雷达-惯性里程计（LIO）在动态环境中的定位和地图构建问题。传统LIO系统假设环境是静态的，当场景中存在大量移动物体（如行人、车辆）时会导致严重的定位误差。这个问题在现实中非常重要，因为真实世界环境通常是动态的，特别是在几何特征稀疏的环境中，动态物体可能主导场景，使传统系统完全失效。此外，现有方法存在循环依赖问题：准确的定位需要可靠的静态特征识别，而有效的动态物体检测又需要精确的位姿估计。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：传统方法将动态物体检测作为预处理步骤而非解决配准算法的根本问题；学习技术只能检测预定义物体类别且需要大量训练数据；几何方法依赖于特定假设且在复杂场景中表现不佳。作者借鉴了时空法线分析的概念，但创新性地将其直接集成到点云配准过程中，而不是作为后处理。同时，作者采用了双地图架构（时间滑动窗口地图用于时空法线计算，长期体素地图提供全局一致性），并扩展了点对点ICP算法的异常值剔除步骤。通过这种方式，作者打破了状态估计和动态物体检测之间的循环依赖。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过时空法线分析直接将动态感知集成到点云配准过程中，同时解决状态估计和动态点分类问题，打破两者之间的循环依赖。整体流程包括：1）输入数据预处理（IMU预积分和点云畸变校正）；2）点云下采样；3）动态感知点云配准（计算时空法线、分类稳定/不稳定点、迭代优化位姿）；4）静态地图构建（不稳定点上采样、DBSCAN聚类、空间一致性检查）。这一过程在每个迭代中同时评估点的动态性和优化位姿估计，而不是分步处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）动态感知ICP算法，通过时空法线分析直接集成动态感知到配准过程；2）双地图架构平衡时空计算需求；3）高效的空间一致性验证方法改进静态地图构建；4）在真正具有挑战性的动态环境中进行验证。相比之前工作的不同：传统方法将动态检测作为预处理步骤，而本文直接集成到配准中；现有方法要么依赖学习技术（需大量训练数据且泛化有限），要么依赖几何假设（在复杂场景中表现不佳）；大多数方法假设已知精确位姿，而本文同时优化位姿和动态点分类；评估不仅限于几何丰富的城市环境，还包括几何退化和动态物体主导的环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种通过时空法线分析直接集成动态感知到激光雷达-惯性里程计配准过程中的新框架，打破了状态估计和动态物体检测之间的循环依赖，在具有挑战性的动态环境中实现了更准确和鲁棒的定位与地图构建。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of Lidar-Inertial Odometry (LIO) indynamic environments, where conventional methods often fail due to theirstatic-world assumptions. Traditional LIO algorithms perform poorly whendynamic objects dominate the scenes, particularly in geometrically sparseenvironments. Current approaches to dynamic LIO face a fundamental challenge:accurate localization requires a reliable identification of static features,yet distinguishing dynamic objects necessitates precise pose estimation. Oursolution breaks this circular dependency by integrating dynamic awarenessdirectly into the point cloud registration process. We introduce a noveldynamic-aware iterative closest point algorithm that leverages spatio-temporalnormal analysis, complemented by an efficient spatial consistency verificationmethod to enhance static map construction. Experimental evaluations demonstratesignificant performance improvements over state-of-the-art LIO systems inchallenging dynamic environments with limited geometric structure. The code anddataset are available at https://github.com/thisparticle/btsa.</description>
      <author>example@mail.com (Chen Zhiqiang, Le Gentil Cedric, Lin Fuling, Lu Minghao, Qiao Qiyuan, Xu Bowen, Qi Yuhua, Lu Peng)</author>
      <guid isPermaLink="false">2510.22313v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Linearized Optimal Transport for Analysis of High-Dimensional Point-Cloud and Single-Cell Data</title>
      <link>http://arxiv.org/abs/2510.22033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于线性最优传输(LOT)的框架，用于处理单细胞技术生成的高维点云数据，解决了不规则点云难以直接量化和比较的问题，同时兼顾了预测准确性和生物学可解释性。&lt;h4&gt;背景&lt;/h4&gt;单细胞技术生成高维点云数据，能够详细表征复杂的患者状态和治疗反应，但每个患者由不规则点云表示，难以直接量化和比较个体间的生物学差异。非线性方法虽能达到预测准确性，但作为黑盒模型，生物学可解释性差。&lt;h4&gt;目的&lt;/h4&gt;将不规则点云嵌入到固定维度的欧几里得空间中，同时保留分布结构，提供一种有原则性的线性表示，保留最优传输几何结构，同时支持下游分析。&lt;h4&gt;方法&lt;/h4&gt;适应线性最优传输(LOT)框架到单细胞数据分析场景，将不规则点云嵌入到固定维度的欧几里得空间中，保留分布结构，形成任意两个患者之间的配准，使其能够直接比较细胞分布。&lt;h4&gt;主要发现&lt;/h4&gt;LOT实现了COVID-19患者状态的准确且可解释的分类，分类器权重映射回驱动预测的特定标记物和空间区域；同时实现了患者来源类器官的合成数据生成，利用LOT嵌入的线性特性；LOT重心产生表示组合条件或样本的平均细胞谱，支持药物相互作用测试。&lt;h4&gt;结论&lt;/h4&gt;LOT作为一个统一框架，连接了预测性能、可解释性和生成建模，通过将异质点云转换为可直接追踪到原始数据的结构化嵌入，为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;h4&gt;翻译&lt;/h4&gt;单细胞技术生成细胞的高维点云，能够详细表征复杂的患者状态和治疗反应。然而每个患者由不规则点云而非简单向量表示，使得难以直接量化和比较个体间的生物学差异。非线性方法如核方法和神经网络能实现预测准确性，但作为黑箱模型，提供的生物学可解释性有限。为解决这些限制，我们将线性最优传输(LOT)框架适应到这一场景，将不规则点云嵌入到固定维度的欧几里得空间中，同时保留分布结构。这种嵌入提供了有原则性的线性表示，保留最优传输几何结构，同时支持下游分析。它还形成了任意两个患者之间的配准，使其能够直接比较细胞分布。在此空间中，LOT实现了：(i) COVID-19患者状态的准确且可解释的分类，其中分类器权重映射回驱动预测的特定标记物和空间区域；(ii) 患者来源类器官的合成数据生成，利用LOT嵌入的线性特性。LOT重心产生表示组合条件或样本的平均细胞谱，支持药物相互作用测试。这些结果共同确立了LOT作为连接预测性能、可解释性和生成建模的统一框架。通过将异质点云转换为可直接追踪到原始数据的结构化嵌入，LOT为理解高维生物系统中的免疫变异和治疗效应开辟了新机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell technologies generate high-dimensional point clouds of cells,enabling detailed characterization of complex patient states and treatmentresponses. Yet each patient is represented by an irregular point cloud ratherthan a simple vector, making it difficult to directly quantify and comparebiological differences between individuals. Nonlinear methods such as kernelsand neural networks achieve predictive accuracy but act as black boxes,offering little biological interpretability.  To address these limitations, we adapt the Linear Optimal Transport (LOT)framework to this setting, embedding irregular point clouds into afixed-dimensional Euclidean space while preserving distributional structure.This embedding provides a principled linear representation that preservesoptimal transport geometry while enabling downstream analysis. It also forms aregistration between any two patients, enabling direct comparison of theircellular distributions. Within this space, LOT enables: (i) \textbf{accurateand interpretable classification} of COVID-19 patient states, where classifierweights map back to specific markers and spatial regions driving predictions;and (ii) \textbf{synthetic data generation} for patient-derived organoids,exploiting the linearity of the LOT embedding. LOT barycenters yield averagedcellular profiles representing combined conditions or samples, supporting druginteraction testing.  Together, these results establish LOT as a unified framework that bridgespredictive performance, interpretability, and generative modeling. Bytransforming heterogeneous point clouds into structured embeddings directlytraceable to the original data, LOT opens new opportunities for understandingimmune variation and treatment effects in high-dimensional biological systems.</description>
      <author>example@mail.com (Tianxiang Wang, Yingtong Ke, Dhananjay Bhaskar, Smita Krishnaswamy, Alexander Cloninger)</author>
      <guid isPermaLink="false">2510.22033v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Spatially Aware Linear Transformer (SAL-T) for Particle Jet Tagging</title>
      <link>http://arxiv.org/abs/2510.23641v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为空间感知线性变换器(SAL-T)的新型架构，解决了在高能物理高数据吞吐量环境中部署Transformer模型时的资源消耗和延迟问题。&lt;h4&gt;背景&lt;/h4&gt;Transformers在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但在高数据吞吐量环境(如CERN LHC)中部署面临挑战，其二次复杂度需要大量资源并增加推理延迟。&lt;h4&gt;目的&lt;/h4&gt;开发一种资源高效且低延迟的Transformer变体，能够在保持高性能的同时适应高能物理等高数据吞吐量环境。&lt;h4&gt;方法&lt;/h4&gt;SAL-T基于linformer架构，结合了空间感知粒子分区(基于运动学特征)和卷积层(捕获局部相关性)，实现了物理意义显著的区域间注意力计算。&lt;h4&gt;主要发现&lt;/h4&gt;SAL-T在喷流分类任务中优于标准linformer，实现了与全注意力Transformer相当的结果，同时显著减少了资源使用和推理延迟；在ModelNet10点云分类数据集上验证了这一优势。&lt;h4&gt;结论&lt;/h4&gt;SAL-T是一种高效解决方案，能够在保持高性能的同时显著降低计算需求和延迟，特别适合高能物理等资源受限环境。&lt;h4&gt;翻译&lt;/h4&gt;Transformers在捕获高能粒子碰撞中的全局和局部相关性方面非常有效，但在高数据吞吐量环境中部署时面临挑战，例如CERN LHC。Transformer模型的二次复杂度需要大量资源并增加推理时的延迟。为了解决这些问题，我们引入了空间感知线性变换器(SAL-T)，这是对linformer架构的物理启发式增强，保持了线性注意力。我们的方法基于运动学特征对粒子进行空间感知分区，从而计算物理意义显著的区域之间的注意力。此外，我们使用卷积层来捕获局部相关性，这些见解来自喷流物理学。除了在喷流分类任务中优于标准linformer外，SAL-T还实现了与全注意力Transformer相当的分类结果，同时在推理过程中使用更少的资源和更低的延迟。在通用点云分类数据集(ModelNet10)上的实验进一步证实了这一趋势。我们的代码可在https://github.com/aaronw5/SAL-T4HEP获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决传统Transformer模型在粒子物理应用中的计算复杂度问题。传统Transformer具有二次方复杂度(O(n²))，导致在处理高能物理数据时资源需求大、推理延迟高。这个问题在现实中非常重要，因为CERN大型强子对撞机每秒产生4000万次碰撞事件，需要实时过滤系统(触发系统)来筛选数据，而传统Transformer无法满足这种低延迟、高吞吐量的需求。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有高效Transformer变体(如Longformer、Linformer)在粒子物理中等长度输入(约100个token)上的局限性，然后结合粒子物理专业知识设计了SAL-T。方法借鉴了Linformer的线性注意力机制作为基础，并融入了粒子物理中的kT排序概念(用于粒子聚类算法)和卷积层设计来捕获局部相关性。作者通过三种主要修改来增强Linformer：基于kT指标的空间感知排序、分区注意力机制和卷积增强注意力，创造出一种既高效又能捕捉物理相关空间信息的新方法。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; SAL-T的核心思想是通过空间感知的分区和卷积增强来改进Linformer架构，使其能够更有效地处理粒子喷射分类任务，同时保持线性计算复杂度。具体实现流程包括：1)使用kT=pTΔR指标对粒子进行排序，确保物理相关的邻近粒子在序列中彼此接近；2)将排序后的键和值向量分区，使每个投影头只关注自己的粒子子集；3)在每个头的原始注意力分数上应用小型深度2D卷积，以融入局部邻居交互；4)通过线性分区粒子多头注意力(LPP-MHA)计算注意力并聚合值表示；5)对注意力输出进行最大聚合并传递到分类层。整个流程保持了线性计算复杂度(O(np))，同时捕获了局部喷射子结构。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)空间感知分区：基于kT指标对粒子排序并分区，使每个投影头关注物理相关的粒子子集；2)卷积增强注意力：在注意力分数上应用2D卷积，捕获局部邻居交互；3)物理启发的特征排序：使用kT而非传统pT排序，更好地保留空间结构；4)在保持线性复杂度的同时实现与全注意力Transformer相当的性能。相比之前工作，SAL-T克服了标准Linformer不编码空间信息的局限，实现了与全注意力Transformer相当的性能但计算复杂度显著降低，同时比特定于粒子物理的Transformer变体更适合资源受限的触发系统。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SAL-T通过空间感知的分区和卷积增强，在保持线性计算复杂度的同时，实现了与全注意力Transformer相当的粒子喷射分类性能，使其成为资源受限的粒子物理触发系统的可行选择。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers are very effective in capturing both global and localcorrelations within high-energy particle collisions, but they presentdeployment challenges in high-data-throughput environments, such as the CERNLHC. The quadratic complexity of transformer models demands substantialresources and increases latency during inference. In order to address theseissues, we introduce the Spatially Aware Linear Transformer (SAL-T), aphysics-inspired enhancement of the linformer architecture that maintainslinear attention. Our method incorporates spatially aware partitioning ofparticles based on kinematic features, thereby computing attention betweenregions of physical significance. Additionally, we employ convolutional layersto capture local correlations, informed by insights from jet physics. Inaddition to outperforming the standard linformer in jet classification tasks,SAL-T also achieves classification results comparable to full-attentiontransformers, while using considerably fewer resources with lower latencyduring inference. Experiments on a generic point cloud classification dataset(ModelNet10) further confirm this trend. Our code is available athttps://github.com/aaronw5/SAL-T4HEP.</description>
      <author>example@mail.com (Aaron Wang, Zihan Zhao, Subash Katel, Vivekanand Gyanchand Sahu, Elham E Khoda, Abhijith Gandrakota, Jennifer Ngadiuba, Richard Cavanaugh, Javier Duarte)</author>
      <guid isPermaLink="false">2510.23641v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization</title>
      <link>http://arxiv.org/abs/2510.20974v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PCA点云(PPC)的标准化框架，用于解决点云强化学习中对相机姿态不匹配的敏感性问题，提高了算法在现实环境中的鲁棒性和可靠性。&lt;h4&gt;背景&lt;/h4&gt;强化学习从原始视觉输入中取得了显著成功，但对分布外变化(如光照、颜色和视点变化)仍然很脆弱。点云强化学习减轻了基于外观的脆弱性，但仍然受相机姿态不匹配的影响。&lt;h4&gt;目的&lt;/h4&gt;解决点云强化学习对相机姿态不匹配的敏感性，提高在现实环境中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出PCA点云(PPC)框架，将任意刚体变换下的点云映射到唯一的规范姿态，使观测与一致框架对齐，减少视点引起的不一致性。&lt;h4&gt;主要发现&lt;/h4&gt;PPC提高了对具有挑战性的机器人任务中未见过的相机姿态的鲁棒性，为领域随机化提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PPC框架有效解决了点云强化学习中的视点不一致性问题，提高了算法在现实环境中的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;从原始视觉输入的强化学习(RL)近年来取得了显著的成功，但它对分布外变化(如光照、颜色和视点变化)仍然很脆弱。点云强化学习(PC-RL)通过减轻基于外观的脆弱性提供了一种有前途的替代方案，但它对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。为了解决这一挑战，我们提出了PCA点云(PPC)，这是一种专门为下游机器人控制设计的标准化框架。PPC将任意刚体变换下的点云映射到唯一的规范姿态，使观测与一致框架对齐，从而显著减少视点引起的不一致性。在我们的实验中，我们表明PPC提高了对具有挑战性的机器人任务中未见过的相机姿态的鲁棒性，为领域随机化提供了有原则的替代方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云强化学习（PC-RL）中的视角敏感性问题。点云通常在相机局部坐标系中表示，即使微小的视角变化也会导致场景表示显著改变，影响算法在实际环境中的可靠性。这个问题很重要，因为视角变化是现实世界中的常见现象，而现有方法如域随机化往往导致样本效率低下且理论保证较弱。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出PC-RL虽然减轻了外观相关的脆弱性，但仍存在视角敏感性问题。然后分析了域随机化方法的局限性，借鉴了旋转不变点云分析领域的进展，特别是基于PCA的标准化方法。作者识别出PCA方法存在符号歧义问题，设计了新颖的几何驱动消歧步骤。该方法借鉴了PointNet、PointNet++等点云处理方法，以及PointPatch RL作为基础强化学习框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过基于PCA的标准化方法，将点云映射到唯一的规范姿态，实现对刚体变换（平移和旋转）的不变性。具体流程包括：1)点云下采样（体素下采样+最远点采样）；2)中心化处理（减去质心）；3)PCA对齐（与特征向量对齐）；4)符号消歧（使用几何驱动分数函数解决PCA符号歧义）；5)生成规范表示（在消歧后的坐标系中重新表达点云）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出PCA点云（PPC）标准化框架；2)设计几何驱动的消歧方法解决PCA符号歧义；3)提供理论保证证明对刚体变换的不变性；4)模块化设计可集成到任何PC-RL算法中。相比之前工作，PPC比域随机化更高效且有理论保证；比现有旋转不变分析方法解决符号歧义问题；比传统PCA方法确保确定性输出；比其他点云方法更适合强化学习任务。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种基于PCA的点云标准化方法（PPC），通过解决PCA的符号歧义问题，实现了点云表示对相机视角变化的鲁棒性，显著提高了点云强化学习在未见视角下的零样本泛化能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) from raw visual input has achieved impressivesuccesses in recent years, yet it remains fragile to out-of-distributionvariations such as changes in lighting, color, and viewpoint. Point CloudReinforcement Learning (PC-RL) offers a promising alternative by mitigatingappearance-based brittleness, but its sensitivity to camera pose mismatchescontinues to undermine reliability in realistic settings. To address thischallenge, we propose PCA Point Cloud (PPC), a canonicalization frameworkspecifically tailored for downstream robotic control. PPC maps point cloudsunder arbitrary rigid-body transformations to a unique canonical pose, aligningobservations to a consistent frame, thereby substantially decreasingviewpoint-induced inconsistencies. In our experiments, we show that PPCimproves robustness to unseen camera poses across challenging robotic tasks,providing a principled alternative to domain randomization.</description>
      <author>example@mail.com (Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi)</author>
      <guid isPermaLink="false">2510.20974v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence</title>
      <link>http://arxiv.org/abs/2510.24693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Homepage: https://internlm.github.io/StarBench/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了STAR-Bench，一个新的音频基准测试，用于评估模型对声音在时间和三维空间中的动态推理能力。与现有基准测试不同，STAR-Bench专注于文本描述难以捕捉的精细感知推理。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态大语言模型和大型音频语言模型发展迅速，但现有的音频基准测试主要测试可以从文本标题中恢复的语义，掩盖了在精细感知推理方面的不足。&lt;h4&gt;目的&lt;/h4&gt;正式定义音频4D智能（即对时间和三维空间中声音动态的推理），并引入STAR-Bench基准测试来衡量这种能力，揭示当前模型在理解物理世界方面的不足。&lt;h4&gt;方法&lt;/h4&gt;STAR-Bench结合基础声学感知设置（六个属性）和整体时空推理设置（包括段重排序和空间任务）。数据收集使用程序合成和物理模拟的音频，以及包括人工注释的四阶段流程确保高质量样本。&lt;h4&gt;主要发现&lt;/h4&gt;对19个模型的评估显示与人类存在显著差距，闭源模型受限于精细感知，而开源模型在感知、知识和推理方面都落后。STAR-Bench导致的准确性下降远大于先前基准测试（时间维度-31.5%，空间维度-35.2%）。&lt;h4&gt;结论&lt;/h4&gt;STAR-Bench为开发具有更强大物理世界理解能力的未来模型提供了关键见解和明确的发展路径。&lt;h4&gt;翻译&lt;/h4&gt;尽管多模态大语言模型和大型音频语言模型取得了快速进展，但现有的音频基准测试主要测试可以从文本标题中恢复的语义，掩盖了在精细感知推理方面的不足。我们将音频4D智能正式定义为对时间和三维空间中声音动态的推理，并引入STAR-Bench来衡量它。STAR-Bench结合了基础声学感知设置（绝对和相对条件下的六个属性）和整体时空推理设置，包括连续和离散过程的段重排序以及跨越静态定位、多源关系和动态轨迹的空间任务。我们的数据收集流程使用两种方法确保高质量样本。对于基础任务，我们使用程序合成和物理模拟的音频。对于整体数据，我们遵循包括人工注释和基于人类表现的最终选择在内的四阶段流程。与先前仅通过标题回答略微降低准确性的基准测试不同，STAR-Bench导致更大的下降（时间维度-31.5%，空间维度-35.2%），证明了其对语言难以描述线索的关注。对19个模型的评估显示与人类存在显著差距，并揭示了能力层次结构：闭源模型受限于精细感知，而开源模型在感知、知识和推理方面都落后。我们的STAR-Bench为开发具有更强大物理世界理解能力的未来模型提供了关键见解和明确的发展路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite rapid progress in Multi-modal Large Language Models and LargeAudio-Language Models, existing audio benchmarks largely test semantics thatcan be recovered from text captions, masking deficits in fine-grainedperceptual reasoning. We formalize audio 4D intelligence that is defined asreasoning over sound dynamics in time and 3D space, and introduce STAR-Bench tomeasure it. STAR-Bench combines a Foundational Acoustic Perception setting (sixattributes under absolute and relative regimes) with a Holistic Spatio-TemporalReasoning setting that includes segment reordering for continuous and discreteprocesses and spatial tasks spanning static localization, multi-sourcerelations, and dynamic trajectories. Our data curation pipeline uses twomethods to ensure high-quality samples. For foundational tasks, we useprocedurally synthesized and physics-simulated audio. For holistic data, wefollow a four-stage process that includes human annotation and final selectionbased on human performance. Unlike prior benchmarks where caption-onlyanswering reduces accuracy slightly, STAR-Bench induces far larger drops(-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguisticallyhard-to-describe cues. Evaluating 19 models reveals substantial gaps comparedwith humans and a capability hierarchy: closed-source models are bottleneckedby fine-grained perception, while open-source models lag across perception,knowledge, and reasoning. Our STAR-Bench provides critical insights and a clearpath forward for developing future models with a more robust understanding ofthe physical world.</description>
      <author>example@mail.com (Zihan Liu, Zhikang Niu, Qiuyang Xiao, Zhisheng Zheng, Ruoqi Yuan, Yuhang Zang, Yuhang Cao, Xiaoyi Dong, Jianze Liang, Xie Chen, Leilei Sun, Dahua Lin, Jiaqi Wang)</author>
      <guid isPermaLink="false">2510.24693v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>DynaStride: Dynamic Stride Windowing with MMCoT for Instructional Multi-Scene Captioning</title>
      <link>http://arxiv.org/abs/2510.23907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 15 figures, 5 Tables, submitted to AAAI AI4ED Workshop 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DynaStride的管道方法，用于生成教学视频中场景级别的连贯字幕，无需手动场景分割。该方法通过自适应帧采样和多模态窗口捕获关键转换，采用多模态思维链过程生成动作-对象对，并使用动态步长窗口选择算法进行融合，最终生成整合视觉语义和时间推理的场景级字幕。&lt;h4&gt;背景&lt;/h4&gt;教学视频中的场景级字幕通过理解视觉线索和时间结构来增强学习。将视觉线索与文本指导相结合支持程序学习和多模态推理，为技能获取提供丰富上下文。然而，未能捕捉这种结构的字幕可能缺乏连贯性和质量，造成混淆并破坏视频的教育意图。&lt;h4&gt;目的&lt;/h4&gt;解决现有字幕生成方法无法有效捕捉教学视频中时间结构和视觉语义的问题，开发一种能够生成连贯、高质量场景级字幕的方法，而无需手动场景分割。&lt;h4&gt;方法&lt;/h4&gt;作者提出了DynaStride管道，使用YouCookII数据集的场景注释，执行自适应帧采样和多模态窗口化来捕获每个场景内的关键转换。然后采用多模态思维链过程产生多个动作-对象对，并使用动态步长窗口选择算法进行精炼和融合，该算法自适应地平衡时间上下文和冗余。最终的场景级字幕将视觉语义和时间推理整合在一个教学字幕中。&lt;h4&gt;主要发现&lt;/h4&gt;与包括VLLaMA3和GPT-4o在内的强大基线相比，DynaStride在基于N-gram的指标(BLEU, METEOR)和语义相似性度量(BERTScore, CLIPScore)上均表现出一致的性能提升。定性分析进一步表明，DynaStride生成的字幕在时间连贯性和信息性方面更优。&lt;h4&gt;结论&lt;/h4&gt;DynaStride为改进AI驱动的教学内容生成提供了有希望的方向，能够生成更连贯、信息更丰富的场景级字幕，有助于提高教学视频的学习效果。&lt;h4&gt;翻译&lt;/h4&gt;教学视频中的场景级字幕可以通过要求理解视觉线索和时间结构来增强学习。通过将视觉线索与文本指导相一致，这种理解支持程序学习和多模态推理，为技能获取提供更丰富的上下文。然而，未能捕捉这种结构的字幕可能缺乏连贯性和质量，这可能造成混淆并破坏视频的教育意图。为了解决这一差距，我们引入了DynaStride，一个无需手动场景分割即可生成连贯场景级字幕的管道。使用YouCookII数据集的场景注释，DynaStride执行自适应帧采样和多模态窗口化来捕获每个场景内的关键转换。然后，它采用多模态思维链过程产生多个动作-对象对，这些对使用动态步长窗口选择算法进行精炼和融合，该算法自适应地平衡时间上下文和冗余。最终的场景级字幕将视觉语义和时间推理整合在一个教学字幕中。与包括VLLaMA3和GPT-4o在内的强大基线的经验评估表明，在基于N-gram的指标(BLEU, METEOR)和语义相似性度量(BERTScore, CLIPScore)上均表现出一致的提升。定性分析进一步表明，DynaStride生成的字幕在时间连贯性和信息性方面更优，这表明改进AI驱动的教学内容生成是一个有希望的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene-level captioning in instructional videos can enhance learning byrequiring an understanding of both visual cues and temporal structure. Byaligning visual cues with textual guidance, this understanding supportsprocedural learning and multimodal reasoning, providing a richer context forskill acquisition. However, captions that fail to capture this structure maylack coherence and quality, which can create confusion and undermine thevideo's educational intent. To address this gap, we introduce DynaStride, apipeline to generate coherent, scene-level captions without requiring manualscene segmentation. Using the YouCookII dataset's scene annotations, DynaStrideperforms adaptive frame sampling and multimodal windowing to capture keytransitions within each scene. It then employs a multimodal chain-of-thoughtprocess to produce multiple action-object pairs, which are refined and fusedusing a dynamic stride window selection algorithm that adaptively balancestemporal context and redundancy. The final scene-level caption integratesvisual semantics and temporal reasoning in a single instructional caption.Empirical evaluations against strong baselines, including VLLaMA3 and GPT-4o,demonstrate consistent gains on both N-gram-based metrics (BLEU, METEOR) andsemantic similarity measures (BERTScore, CLIPScore). Qualitative analysesfurther show that DynaStride produces captions that are more temporallycoherent and informative, suggesting a promising direction for improvingAI-powered instructional content generation.</description>
      <author>example@mail.com (Eddison Pham, Prisha Priyadarshini, Adrian Maliackel, Kanishk Bandi, Cristian Meo, Kevin Zhu)</author>
      <guid isPermaLink="false">2510.23907v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>VideoTG-R1: Boosting Video Temporal Grounding via Curriculum Reinforcement Learning on Reflected Boundary Annotations</title>
      <link>http://arxiv.org/abs/2510.23397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;VideoTG-R1是一种新型课程强化学习框架，通过反射边界标注解决视频时间定位中的训练样本质量和难度问题，实现了数据高效训练。&lt;h4&gt;背景&lt;/h4&gt;视频时间定位(VTG)是根据语言查询在视频中定位精确片段的基础挑战。多模态大型语言模型(MLLMs)通过强化学习(RL)在VTG方面显示出潜力，但忽视了训练样本质量和难度带来的挑战。&lt;h4&gt;目的&lt;/h4&gt;解决VTG训练中部分标注样本和难以定位样本带来的问题，提高训练效率。&lt;h4&gt;方法&lt;/h4&gt;提出VideoTG-R1框架，包含边界反射代理(识别并过滤部分标注样本)和难度估计代理(评估样本难度并设计课程RL策略)，实现数据高效训练。&lt;h4&gt;主要发现&lt;/h4&gt;仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化(GRPO)和监督微调(SFT)下都优于全数据对应方法。&lt;h4&gt;结论&lt;/h4&gt;VideoTG-R1通过解决训练样本质量和难度问题，实现了在VTG和基于视频的问答任务上的有效性能提升。&lt;h4&gt;翻译&lt;/h4&gt;视频时间定位(VTG)旨在根据语言查询在视频中定位精确片段，这是视频理解中的一个基础挑战。虽然最近的多模态大型语言模型(MLLMs)通过强化学习(RL)在解决VTG方面显示出潜力，但它们忽视了训练样本质量和难度带来的挑战。(1)部分标注样本：许多样本包含超出标注区间的相关片段，引入了模糊监督。(2)难以定位的样本：零样本性能差的样本在RL训练中产生持续低且不可区分的奖励，在多个输出中没有明显偏好，从而阻碍学习效率。为解决这些挑战，我们提出VideoTG-R1，一个具有反射边界标注的新型课程RL框架，实现数据高效训练。具体来说，我们提出边界反射代理，利用MLLMs预测标注区间外的查询相关时间戳，使我们能够识别并过滤部分标注样本，从而减少模糊性。此外，我们引入难度估计代理来评估每个样本的训练难度，并设计课程RL策略，根据训练步骤动态掩盖难以定位样本的视频，降低训练难度并提供更清晰的偏好。在VTG和基于视频的问答任务上的实验证明了我们方法的有效性。值得注意的是，仅使用10%的训练样本和21%的计算预算，VideoTG-R1在组相对策略优化(GRPO)和监督微调(SFT)下都优于全数据对应方法。代码可在https://github.com/ldong1111/VideoTG-R1获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video temporal grounding (VTG) aims to locate precise segments in videosbased on language queries, which is a fundamental challenge in videounderstanding. While recent Multimodal Large Language Models (MLLMs) have shownpromise in tackling VTG through reinforcement learning (RL), they overlook thechallenges arising from both the quality and difficulty of training samples.(1) Partially annotated samples. Many samples contain relevant segments beyondthe annotated interval, introducing ambiguous supervision. (2) Hard-to-groundsamples. Samples with poor zero-shot performance produce consistently low andindistinguishable rewards during RL training, exhibiting no clear preferenceamong multiple outputs and thus hindering learning efficiency. To address thesechallenges, we propose VideoTG-R1, a novel curriculum RL framework withreflected boundary annotations, enabling data-efficient training. Specifically,we propose a Boundary Reflection Agent that utilizes MLLMs to predictquery-relevant timestamps outside the annotated intervals, allowing us toidentify and filter out partially annotated samples, thereby reducingambiguity. Furthermore, we introduce a Difficulty Estimation Agent to assessthe training difficulty of each sample and design a curriculum RL strategy thatdynamically masks the videos of hard-to-ground samples according to thetraining steps, easing the training difficulty and providing clearerpreference. Experiments on the VTG and grounded VideoQA tasks demonstrate theeffectiveness of our method. Remarkably, with only 10% of the training samplesand 21% of the computational budget, VideoTG-R1 outperforms full-datacounterparts under both group relative policy optimization (GRPO) andsupervised fine-tuning (SFT). The code is available athttps://github.com/ldong1111/VideoTG-R1.</description>
      <author>example@mail.com (Lu Dong, Haiyu Zhang, Han Lin, Ziang Yan, Xiangyu Zeng, Hongjie Zhang, Yifei Huang, Yi Wang, Zhen-Hua Ling, Limin Wang, Yali Wang)</author>
      <guid isPermaLink="false">2510.23397v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Evaluation of Vision-LLMs in Surveillance Video</title>
      <link>http://arxiv.org/abs/2510.23190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as poster in the NeurIPS 2025 Workshop on Space in Vision,  Language, and Embodied AI&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了视觉语言模型在异常动作识别中的空间推理能力，将其作为零样本、语言基础的任务，解决了从稀疏2D视频中解释动态3D场景的具身感知挑战。&lt;h4&gt;背景&lt;/h4&gt;社会中摄像机的广泛应用产生了大量视频数据，远远超出人工监控能力，这对公共安全构成重大挑战，因为及时检测异常或犯罪事件对有效预防和响应至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究视觉语言模型（VLMs）的空间推理能力，探索小型预训练视觉-语言模型作为空间基础的零样本异常检测器的可行性，并评估其在提示和隐私保护条件下的表现。&lt;h4&gt;方法&lt;/h4&gt;将视频转换为文本描述并通过文本蕴含对标签进行评分，在UCF-Crime和RWF-2000数据集上评估四个开放模型，研究少样本示例和隐私过滤器对模型性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;少样本示例可以提高某些模型的准确性但可能增加误报；隐私过滤器（尤其是全身GAN变换）会引入不一致性降低准确性；当前视觉-语言模型在简单、空间显著事件上表现良好，但在处理嘈杂空间线索和身份模糊时表现不佳。&lt;h4&gt;结论&lt;/h4&gt;提出了加强空间基础的具体路径，包括结构感知提示、跨片段轻量级空间记忆、描述过程中的场景图或3D姿态先验，以及保留动作相关几何形状的隐私方法，使零样本、语言基础的管道成为具身、真实世界视频理解的适应性构建块。&lt;h4&gt;翻译&lt;/h4&gt;我们社会中摄像机的广泛应用产生了大量视频数据，远远超出了人工监控的能力。这对公共安全和安全构成了关键挑战，因为及时检测异常或犯罪事件对于有效预防和应对至关重要。具身代理识别意外事件的能力根本上与其空间推理能力相关。本文通过将异常动作识别构架为零样本、语言基础的任务，研究了视觉语言模型（VLMs）的空间推理，解决了从稀疏2D视频中解释动态3D场景的具身感知挑战。具体来说，我们调查了小型预训练视觉-语言模型是否可以通过将视频转换为文本描述并通过文本蕴含对标签进行评分，作为空间基础的零样本异常检测器。我们在提示和隐私保护条件下，在UCF-Crime和RWF-2000数据集上评估了四个开放模型。少样本示例可以提高某些模型的准确性，但可能增加误报，而隐私过滤器——尤其是全身GAN变换——会引入不一致性，降低准确性。这些结果展示了当前视觉-语言模型在哪些方面成功（简单、空间显著事件）和哪些方面失败（嘈杂的空间线索、身份模糊）。展望未来，我们概述了加强空间基础的具体路径，无需任务特定训练：结构感知提示、跨片段轻量级空间记忆、描述过程中的场景图或3D姿态先验，以及保留动作相关几何形状的隐私方法。这使零样本、语言基础的管道成为具身、真实世界视频理解的适应性构建块。我们用于评估VLMs的实现已在以下公开可用：https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何评估视觉-语言模型在监控视频中进行零样本异常行为识别的问题。这个问题很重要，因为社会上有大量摄像头产生的视频数据远远超过人类监控能力，及时检测异常或犯罪事件对公共安全和有效预防至关重要。此外，现有的公共异常行为识别数据集有限，仅在这些数据集上训练的模型可能无法很好地泛化到新的异常类型。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者思考过程是认识到传统视频异常检测依赖监督学习，需要大量带注释的数据集，成本高且难以识别新异常。因此他们设计了一个零样本框架，将异常分类重新构建为语言基础推理任务而非像素到标签映射。该方法借鉴了现有VLM的语义推理和世界知识，但不同于之前的AnomalyCLIP、LAVAD等方法，它不需要任务特定微调，而是专注于纯零样本异常检测。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将异常分类转化为语言基础推理任务，利用大型预训练视觉-语言模型的语义推理能力。整体流程分为两步：1)文本描述生成：视觉-LLM处理视频输入，基于视觉和提示生成描述性文本；2)零样本分类：使用预训练的NLI分类器评估文本与候选异常类别的逻辑蕴含程度，选择最高分数的类别作为结果。整个过程无需对模型进行梯度更新，实现了真正的零样本学习。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：系统评估小型视觉-LLMs在零样本异常识别中的能力；设计多种提示策略实验；研究隐私保护变换对模型性能的影响；提出改进空间推理的具体方法。与之前工作不同，该方法不需要大量标注数据，专注于纯零样本检测，首次系统评估了隐私保护变换对模型的影响，并揭示了提示技术与隐私过滤器之间的关键权衡。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过系统评估视觉-LLMs在零样本异常检测中的能力，揭示了提示技术和隐私过滤器之间的关键权衡，为设计更安全、更有效的视频理解系统提供了实用建议。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread use of cameras in our society has created an overwhelmingamount of video data, far exceeding the capacity for human monitoring. Thispresents a critical challenge for public safety and security, as the timelydetection of anomalous or criminal events is crucial for effective response andprevention. The ability for an embodied agent to recognize unexpected events isfundamentally tied to its capacity for spatial reasoning. This paperinvestigates the spatial reasoning of vision-language models (VLMs) by framinganomalous action recognition as a zero-shot, language-grounded task, addressingthe embodied perception challenge of interpreting dynamic 3D scenes from sparse2D video. Specifically, we investigate whether small, pre-trained vision--LLMscan act as spatially-grounded, zero-shot anomaly detectors by converting videointo text descriptions and scoring labels via textual entailment. We evaluatefour open models on UCF-Crime and RWF-2000 under prompting andprivacy-preserving conditions. Few-shot exemplars can improve accuracy for somemodels, but may increase false positives, and privacy filters -- especiallyfull-body GAN transforms -- introduce inconsistencies that degrade accuracy.These results chart where current vision--LLMs succeed (simple, spatiallysalient events) and where they falter (noisy spatial cues, identityobfuscation). Looking forward, we outline concrete paths to strengthen spatialgrounding without task-specific training: structure-aware prompts, lightweightspatial memory across clips, scene-graph or 3D-pose priors during description,and privacy methods that preserve action-relevant geometry. This positionszero-shot, language-grounded pipelines as adaptable building blocks forembodied, real-world video understanding. Our implementation for evaluatingVLMs is publicly available at:https://github.com/pascalbenschopTU/VLLM_AnomalyRecognition</description>
      <author>example@mail.com (Pascal Benschop, Cristian Meo, Justin Dauwels, Jelte P. Mense)</author>
      <guid isPermaLink="false">2510.23190v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Large-Model AI for Near Field Beam Prediction: A CNN-GPT2 Framework for 6G XL-MIMO</title>
      <link>http://arxiv.org/abs/2510.22557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于CNN-GPT2的新型近场波束预测框架，用于解决毫米波通信中极大规模天线阵列在高移动性场景下的近场波束预测挑战。&lt;h4&gt;背景&lt;/h4&gt;毫米波通信中极大规模天线阵列在高移动性场景下的应用凸显了近场波束预测的重要性。与传统远场假设不同，近场波束预测需要在角度和距离域联合采样码本，导致导频开销大幅增加。此外，最优近场波束指数表现出突发的非线性动态特性，对时间建模构成挑战。&lt;h4&gt;目的&lt;/h4&gt;解决近场波束预测中的挑战，设计一个有效的近场波束预测框架，以应对导频开销增加和波束指数非线性动态特性问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于CNN-GPT2的新型近场波束预测框架。具体包括：设计上行链路导频传输策略，通过宽波束模拟预编码和频率变化数字预编码实现高效信道探测；接收的导频信号经过预处理后，通过基于CNN的特征提取器；然后通过GPT-2模型捕获多个帧之间的时间依赖性，以端到端方式直接预测近场波束指数。&lt;h4&gt;主要发现&lt;/h4&gt;CNN-GPT2框架能够有效处理近场波束预测的挑战，所提出的导频传输策略实现了高效信道探测，该方法能够捕获时间依赖性并直接预测近场波束指数。&lt;h4&gt;结论&lt;/h4&gt;基于CNN-GPT2的近场波束预测框架为解决毫米波通信中极大规模天线阵列在高移动性场景下的近场波束预测问题提供了有效方案。&lt;h4&gt;翻译&lt;/h4&gt;毫米波通信中极大规模天线阵列的出现，尤其是在高移动性场景下，凸显了近场波束预测的重要性。与传统远场假设不同，近场波束预测需要同时在角度和距离域采样的码本，这导致导频开销大幅增加。此外，与远场情况下最优波束演化的时间平滑性不同，最优近场波束指数由于其同时依赖于用户角度和距离而表现出突发和非线性动态特性，给时间建模带来了重大挑战。为应对这些挑战，我们提出了一种新颖的基于卷积神经网络-生成预训练Transformer 2（CNN-GPT2）的近场波束预测框架。具体而言，设计了一种上行链路导频传输策略，通过宽波束模拟预编码和频率变化数字预编码实现高效信道探测。接收的导频信号经过预处理后，通过基于CNN的特征提取器，然后由GPT-2模型捕获多个帧之间的时间依赖性，并以端到端方式直接预测近场波束指数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of extremely large-scale antenna arrays (ELAA) inmillimeter-wave (mmWave) communications, particularly in high-mobilityscenarios, highlights the importance of near-field beam prediction. Unlike theconventional far-field assumption, near-field beam prediction requirescodebooks that jointly sample the angular and distance domains, which leads toa dramatic increase in pilot overhead. Moreover, unlike the far-field casewhere the optimal beam evolution is temporally smooth, the optimal near-fieldbeam index exhibits abrupt and nonlinear dynamics due to its joint dependenceon user angle and distance, posing significant challenges for temporalmodeling. To address these challenges, we propose a novel Convolutional NeuralNetwork-Generative Pre-trained Transformer 2 (CNN-GPT2) based near-field beamprediction framework. Specifically, an uplink pilot transmission strategy isdesigned to enable efficient channel probing through widebeam analog precodingand frequency-varying digital precoding. The received pilot signals arepreprocessed and passed through a CNN-based feature extractor, followed by aGPT-2 model that captures temporal dependencies across multiple frames anddirectly predicts the near-field beam index in an end-to-end manner.</description>
      <author>example@mail.com (Wang Liu, Cunhua Pan, Hong Ren, Wei Zhang, Cheng-Xiang Wang, Jiangzhou Wang)</author>
      <guid isPermaLink="false">2510.22557v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>TERRA: A Transformer-Enabled Recursive R-learner for Longitudinal Heterogeneous Treatment Effect Estimation</title>
      <link>http://arxiv.org/abs/2510.22407v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为TERRA的新方法，用于解决纵向数据中异质性处理效应估计的挑战，特别是在时变干预情况下。&lt;h4&gt;背景&lt;/h4&gt;在纵向数据中准确估计异质性处理效应对医疗保健、公共政策、教育和数字营销等领域的个性化决策至关重要。然而，时变干预带来了延续效应、时变异质性和处理后偏差等独特挑战，这些挑战无法通过标准HTE方法解决。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理时变干预带来的独特挑战（延续效应、时变异质性和处理后偏差）的方法，以准确估计纵向数据中的异质性处理效应。&lt;h4&gt;方法&lt;/h4&gt;引入TERRA（Transformer-Enabled Recursive R-learner），包含两个主要组件：1）使用Transformer架构编码完整的处理特征历史，捕捉长期时间依赖性和延续效应；2）开发递归残差学习公式，将经典结构嵌套均值模型推广到参数规范之外，解决处理后偏差问题。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和数据应用中，TERRA在HTE估计的准确性和稳定性方面始终优于强大的基线方法。&lt;h4&gt;结论&lt;/h4&gt;将有原则的因果结构与高容量的序列模型相结合，对于纵向异质性处理效应估计具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;在纵向环境中准确估计异质性处理效应(HTE)对于医疗保健、公共政策、教育和数字营销等领域的个性化决策至关重要。然而，时变干预带来了许多独特挑战，如延续效应、时变异质性和处理后偏差，这些问题标准HTE方法无法解决。为应对这些挑战，我们引入了TERRA（Transformer-Enabled Recursive R-learner），它促进具有灵活时间建模和学习的纵向HTE估计。TERRA有两个组件。首先，我们使用Transformer架构编码完整的处理特征历史，使表示长期时间依赖性和延续效应成为可能，从而更全面地捕捉个体和特定时间的处理效应变化。其次，我们开发了一种递归残差学习公式，将经典结构嵌套均值模型(SNMMs)推广到参数规范之外，解决了处理后偏差问题，同时减少了对功能假设的依赖。在模拟和数据应用中，TERRA在HTE估计的准确性和稳定性方面始终优于强大的基线方法，突显了将原则性因果结构与高容量序列模型相结合对纵向HTE的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately estimating heterogeneous treatment effects (HTE) in longitudinalsettings is essential for personalized decision-making across healthcare,public policy, education, and digital marketing. However, time-varyinginterventions introduce many unique challenges, such as carryover effects,time-varying heterogeneity, and post-treatment bias, which are not addressed bystandard HTE methods. To address these challenges, we introduce TERRA(Transformer-Enabled Recursive R-learner), which facilitates longitudinal HTEestimation with flexible temporal modeling and learning. TERRA has twocomponents. First, we use a Transformer architecture to encode fulltreatment-feature histories, enabling the representation of long-range temporaldependencies and carryover effects, hence capturing individual- andtime-specific treatment effect variation more comprehensively. Second, wedevelop a recursive residual-learning formulation that generalizes theclassical structural nested mean models (SNMMs) beyond parametricspecifications, addressing post-treatment bias while reducing reliance onfunctional assumptions. In simulations and data applications, TERRAconsistently outperforms strong baselines in HTE estimation in both accuracyand stability, highlighting the value of combining principled causal structurewith high-capacity sequence models for longitudinal HTE.</description>
      <author>example@mail.com (Lei Shi, Sizhu Lu, Qiuran Lyu, Peng Ding, Nikos Vlassis)</author>
      <guid isPermaLink="false">2510.22407v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Human-Centric Anomaly Detection in Surveillance Videos Using YOLO-World and Spatio-Temporal Deep Learning</title>
      <link>http://arxiv.org/abs/2510.22056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种稳健的深度学习框架，通过结合以人为中心的预处理和时空建模来解决监控视频异常检测中的挑战，在UCF-Crime数据集上实现了92.41%的平均测试准确率。&lt;h4&gt;背景&lt;/h4&gt;监控视频中的异常检测面临异常事件多样性、类别不平衡和场景依赖的视觉混乱等挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个稳健的深度学习框架，整合以人为中心的预处理与时空建模，用于多类异常分类。&lt;h4&gt;方法&lt;/h4&gt;使用YOLO-World识别人体实例，ByteTrack进行身份感知跟踪，通过高斯模糊抑制背景区域，使用InceptionV3进行空间特征提取，并用双向LSTM捕获时间动态进行序列级分类。&lt;h4&gt;主要发现&lt;/h4&gt;在UCF-Crime五类子集上评估，三次独立试验中平均测试准确率达92.41%，每类F1分数均超过0.85，展示了对类别不平衡的强鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;前景聚焦的预处理显著增强了现实监控场景中的异常辨别能力。&lt;h4&gt;翻译&lt;/h4&gt;监控视频中的异常检测由于异常事件的多样性、类别不平衡和场景依赖的视觉混乱而仍然是一项具有挑战性的任务。为了解决这些问题，我们提出了一个稳健的深度学习框架，该框架整合了以人为中心的预处理与时空建模，用于多类异常分类。我们的流程首先应用YOLO-World（一种开放词汇的视觉语言检测器）来识别原始视频片段中的人体实例，然后使用ByteTrack进行一致的身份感知跟踪。通过高斯模糊抑制检测边界框外的背景区域，有效减少场景特定的干扰，使模型专注于行为相关的前景内容。然后，经过精炼的帧由在ImageNet上预训练的InceptionV3网络处理进行空间特征提取，并使用双向LSTM（BiLSTM）捕获时间动态，进行序列级分类。在UCF-Crime数据集的五类子集（正常、入室盗窃、打架、纵火、爆炸）上评估，我们的方法在三次独立试验中平均测试准确率达到92.41%，每个类别的F1分数均超过0.85。全面的评估指标，包括混淆矩阵、ROC曲线和宏/加权平均值，展示了强大的泛化能力和对类别不平衡的鲁棒性。结果证实，前景聚焦的预处理显著增强了现实监控场景中的异常辨别能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in surveillance videos remains a challenging task due tothe diversity of abnormal events, class imbalance, and scene-dependent visualclutter. To address these issues, we propose a robust deep learning frameworkthat integrates human-centric preprocessing with spatio-temporal modeling formulti-class anomaly classification. Our pipeline begins by applying YOLO-World- an open-vocabulary vision-language detector - to identify human instances inraw video clips, followed by ByteTrack for consistent identity-aware tracking.Background regions outside detected bounding boxes are suppressed via Gaussianblurring, effectively reducing scene-specific distractions and focusing themodel on behaviorally relevant foreground content. The refined frames are thenprocessed by an ImageNet-pretrained InceptionV3 network for spatial featureextraction, and temporal dynamics are captured using a bidirectional LSTM(BiLSTM) for sequence-level classification. Evaluated on a five-class subset ofthe UCF-Crime dataset (Normal, Burglary, Fighting, Arson, Explosion), ourmethod achieves a mean test accuracy of 92.41% across three independent trials,with per-class F1-scores consistently exceeding 0.85. Comprehensive evaluationmetrics - including confusion matrices, ROC curves, and macro/weighted averages- demonstrate strong generalization and resilience to class imbalance. Theresults confirm that foreground-focused preprocessing significantly enhancesanomaly discrimination in real-world surveillance scenarios.</description>
      <author>example@mail.com (Mohammad Ali Etemadi Naeen, Hoda Mohammadzade, Saeed Bagheri Shouraki)</author>
      <guid isPermaLink="false">2510.22056v1</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>ViBED-Net: Video Based Engagement Detection Network Using Face-Aware and Scene-Aware Spatiotemporal Cues</title>
      <link>http://arxiv.org/abs/2510.18016v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ViBED-Net的新型深度学习框架，用于从视频数据中检测学生在在线学习环境中的参与度。该模型通过结合面部表情和全场景上下文信息，利用双流架构和EfficientNetV2进行特征提取，并使用LSTM或Transformer进行时间建模。&lt;h4&gt;背景&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。存在一个名为DAiSEE的大规模基准数据集，用于电子学习中情感状态识别。&lt;h4&gt;目的&lt;/h4&gt;提出ViBED-Net（基于视频的参与度检测网络），一种新的深度学习框架，设计用于从视频数据评估学生参与度。&lt;h4&gt;方法&lt;/h4&gt;采用双流架构，使用EfficientNetV2进行空间特征提取，处理面部裁剪和完整视频帧来捕捉面部表情和全场景上下文。使用两种时间建模策略分析特征：长短期记忆（LSTM）网络和Transformer编码器。在DAiSEE数据集上评估模型，并应用有针对性的数据增强技术提高在代表性不足的参与度类别上的性能。&lt;h4&gt;主要发现&lt;/h4&gt;ViBED-Net与LSTM的组合达到73.43%的准确率，优于现有的最先进方法。结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。&lt;h4&gt;结论&lt;/h4&gt;ViBED-Net的模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。该工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了基于视频的情感计算发展。项目源代码可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;在线学习环境中的参与度检测对于提高学生成果和个性化教学至关重要。我们提出了ViBED-Net（基于视频的参与度检测网络），一种新的深度学习框架，旨在通过双流架构从视频数据评估学生参与度。ViBED-Net通过EfficientNetV2处理面部裁剪和完整视频帧来捕捉面部表情和全场景上下文，用于空间特征提取。然后使用两种时间建模策略分析这些特征：长短期记忆（LSTM）网络和Transformer编码器。我们的模型在DAiSEE数据集上进行了评估，这是一个电子学习中情感状态识别的大规模基准。为了提高在代表性不足的参与度类别上的性能，我们应用了有针对性的数据增强技术。在测试的变体中，ViBED-Net与LSTM结合实现了73.43%的准确率，优于现有的最先进方法。ViBED-Net证明，结合面部感知和场景感知的时空线索显著提高了参与度检测的准确性。其模块化设计使其具有灵活性，可应用于教育、用户体验研究和内容个性化。这项工作通过为现实世界的参与度分析提供可扩展的高性能解决方案，推动了基于视频的情感计算发展。该项目的源代码可在https://github.com/prateek-gothwal/ViBED-Net获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Engagement detection in online learning environments is vital for improvingstudent outcomes and personalizing instruction. We present ViBED-Net(Video-Based Engagement Detection Network), a novel deep learning frameworkdesigned to assess student engagement from video data using a dual-streamarchitecture. ViBED-Net captures both facial expressions and full-scene contextby processing facial crops and entire video frames through EfficientNetV2 forspatial feature extraction. These features are then analyzed over time usingtwo temporal modeling strategies: Long Short-Term Memory (LSTM) networks andTransformer encoders. Our model is evaluated on the DAiSEE dataset, alarge-scale benchmark for affective state recognition in e-learning. To enhanceperformance on underrepresented engagement classes, we apply targeted dataaugmentation techniques. Among the tested variants, ViBED-Net with LSTMachieves 73.43\% accuracy, outperforming existing state-of-the-art approaches.ViBED-Net demonstrates that combining face-aware and scene-aware spatiotemporalcues significantly improves engagement detection accuracy. Its modular designallows flexibility for application across education, user experience research,and content personalization. This work advances video-based affective computingby offering a scalable, high-performing solution for real-world engagementanalysis. The source code for this project is available onhttps://github.com/prateek-gothwal/ViBED-Net .</description>
      <author>example@mail.com (Prateek Gothwal, Deeptimaan Banerjee, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.18016v2</guid>
      <pubDate>Wed, 29 Oct 2025 16:32:10 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and Scalable Multimodal Pathology Retrieval via Attentive Vision-Language Alignment</title>
      <link>http://arxiv.org/abs/2510.23224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PathSearch是一个结合细粒度注意力马赛克表示与全局幻灯片嵌入的检索框架，通过视觉语言对比学习对齐，在数字病理学中实现了准确和灵活的幻灯片检索，提高了诊断准确性和观察者一致性。&lt;h4&gt;背景&lt;/h4&gt;病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新可能。基于内容的幻灯片检索使病理学家能够识别形态学和语义上相似的病例，支持精确诊断、增强观察者间一致性并协助基于案例的教育。然而，全幻灯片图像的有效检索仍具挑战性，因其具有千兆像素规模且在大量无关内容中捕捉细微语义差异困难。&lt;h4&gt;目的&lt;/h4&gt;开发一个检索框架来克服全幻灯片图像检索的挑战，实现准确、高效的幻灯片检索功能。&lt;h4&gt;方法&lt;/h4&gt;提出PathSearch检索框架，统一细粒度注意力马赛克表示与全局幻灯片嵌入，通过视觉语言对比学习对齐。在6,926个幻灯片-报告对语料库上训练，支持两种关键功能：(1)基于马赛克的图像到图像检索；(2)多模态检索，文本查询可直接检索相关幻灯片。&lt;h4&gt;主要发现&lt;/h4&gt;在四个公共病理数据集和三个内部队列上进行了严格评估，涵盖解剖部位检索、肿瘤亚型分类、肿瘤与非肿瘤鉴别及跨多个器官的分级任务。外部结果显示PathSearch优于传统图像到图像检索框架。多中心读者研究证明在真实临床场景中提高了诊断准确性，增强了信心，并提高了观察者间一致性。&lt;h4&gt;结论&lt;/h4&gt;PathSearch被确立为数字病理学中可扩展和通用的检索解决方案。&lt;h4&gt;翻译&lt;/h4&gt;病理切片的快速数字化为临床和研究工作流程中的计算工具开辟了新的可能性。在这些工具中，基于内容的幻灯片检索脱颖而出，使病理学家能够识别形态学和语义上相似的病例，从而支持精确诊断，增强观察者间的一致性，并协助基于案例的教育。然而，由于全幻灯片图像的千兆像素规模以及在大量无关内容中捕捉细微语义差异的困难，全幻灯片图像的有效检索仍然具有挑战性。为了克服这些挑战，我们提出了PathSearch，这是一个检索框架，统一了细粒度注意力马赛克表示与全局幻灯片嵌入，通过视觉语言对比学习对齐。在包含6,926个幻灯片-报告对的语料库上训练，PathSearch捕捉了细粒度形态学线索和高级语义模式，以实现准确和灵活的检索。该框架支持两个关键功能：(1)基于马赛克的图像到图像检索，确保准确和高效的幻灯片研究；(2)多模态检索，文本查询可以直接检索相关幻灯片。PathSearch在四个公共病理数据集和三个内部队列上进行了严格评估，涵盖了解剖部位检索、肿瘤亚型分类、肿瘤与非肿瘤鉴别以及跨乳腺、肺、肾、肝和胃等多种器官的分级任务。外部结果显示，PathSearch优于传统的图像到图像检索框架。多中心读者研究进一步证明，在真实临床场景中，PathSearch提高了病理学家的诊断准确性，增强了信心，并提高了观察者间的一致性。这些结果确立了PathSearch作为数字病理学中可扩展和通用的检索解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid digitization of histopathology slides has opened up newpossibilities for computational tools in clinical and research workflows. Amongthese, content-based slide retrieval stands out, enabling pathologists toidentify morphologically and semantically similar cases, thereby supportingprecise diagnoses, enhancing consistency across observers, and assistingexample-based education. However, effective retrieval of whole slide images(WSIs) remains challenging due to their gigapixel scale and the difficulty ofcapturing subtle semantic differences amid abundant irrelevant content. Toovercome these challenges, we present PathSearch, a retrieval framework thatunifies fine-grained attentive mosaic representations with global-wise slideembeddings aligned through vision-language contrastive learning. Trained on acorpus of 6,926 slide-report pairs, PathSearch captures both fine-grainedmorphological cues and high-level semantic patterns to enable accurate andflexible retrieval. The framework supports two key functionalities: (1)mosaic-based image-to-image retrieval, ensuring accurate and efficient slideresearch; and (2) multi-modal retrieval, where text queries can directlyretrieve relevant slides. PathSearch was rigorously evaluated on four publicpathology datasets and three in-house cohorts, covering tasks includinganatomical site retrieval, tumor subtyping, tumor vs. non-tumor discrimination,and grading across diverse organs such as breast, lung, kidney, liver, andstomach. External results show that PathSearch outperforms traditionalimage-to-image retrieval frameworks. A multi-center reader study furtherdemonstrates that PathSearch improves diagnostic accuracy, boosts confidence,and enhances inter-observer agreement among pathologists in real clinicalscenarios. These results establish PathSearch as a scalable and generalizableretrieval solution for digital pathology.</description>
      <author>example@mail.com (Hongyi Wang, Zhengjie Zhu, Jiabo Ma, Fang Wang, Yue Shi, Bo Luo, Jili Wang, Qiuyu Cai, Xiuming Zhang, Yen-Wei Chen, Lanfen Lin, Hao Chen)</author>
      <guid isPermaLink="false">2510.23224v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
  <item>
      <title>MATCH: Task-Driven Code Evaluation through Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.23169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MATCH的新型无参考代码评估指标，使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现相似性评分。&lt;h4&gt;背景&lt;/h4&gt;AI代码生成日益普及，GitHub Copilot估计生成GitHub上46%的代码。准确评估生成代码与开发者意图的匹配度是重大挑战，传统方法如单元测试难以扩展且成本高，语法相似性指标无法捕捉代码功能，而需要参考代码的指标如CodeBERTScore并不总是可用。&lt;h4&gt;目的&lt;/h4&gt;解决无参考代码评估的空白，提供一种不依赖参考代码的代码生成质量评估方法。&lt;h4&gt;方法&lt;/h4&gt;引入MATCH指标，使用对比学习技术为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码实现任务程度的相似性评分。&lt;h4&gt;主要发现&lt;/h4&gt;MATCH在多种编程语言上实现了比现有指标与功能正确性和人类偏好更强的相关性。&lt;h4&gt;结论&lt;/h4&gt;MATCH是一种有效的无参考代码评估指标，能够更好地评估生成代码与任务意图的匹配度。&lt;h4&gt;翻译&lt;/h4&gt;基于AI的代码生成日益普及，GitHub Copilot估计生成了GitHub上46%的代码。准确评估生成代码与开发者意图的匹配度仍然是一个重大挑战。传统的评估方法，如单元测试，通常难以扩展且成本高昂。语法相似性指标（如BLEU、ROUGE）无法捕捉代码功能，而像CodeBERTScore这样的指标需要参考代码，但参考代码并不总是可用的。为了解决无参考评估的空白，除了ICE-Score等少数替代方案外，本文引入了MATCH，一种新型的无参考指标。MATCH使用对比学习为代码和自然语言任务描述生成有意义的嵌入，实现反映生成代码实现任务程度的相似性评分。我们证明，在多种编程语言上，MATCH比现有指标实现了与功能正确性和人类偏好更强的相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; AI-based code generation is increasingly prevalent, with GitHub Copilotestimated to generate 46% of the code on GitHub. Accurately evaluating how wellgenerated code aligns with developer intent remains a critical challenge.Traditional evaluation methods, such as unit tests, are often unscalable andcostly. Syntactic similarity metrics (e.g., BLEU, ROUGE) fail to capture codefunctionality, and metrics like CodeBERTScore require reference code, which isnot always available. To address the gap in reference-free evaluation, with fewalternatives such as ICE-Score, this paper introduces MATCH, a novelreference-free metric. MATCH uses Contrastive Learning to generate meaningfulembeddings for code and natural language task descriptions, enabling similarityscoring that reflects how well generated code implements the task. We show thatMATCH achieves stronger correlations with functional correctness and humanpreference than existing metrics across multiple programming languages.</description>
      <author>example@mail.com (Marah Ghoummaid, Vladimir Tchuiev, Ofek Glick, Michal Moschkovitz, Dotan Di Castro)</author>
      <guid isPermaLink="false">2510.23169v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>VALA: Learning Latent Anchors for Training-Free and Temporally Consistent</title>
      <link>http://arxiv.org/abs/2510.22970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了VALA（Variational Alignment for Latent Anchors）变分对齐模块，用于解决现有无需训练的视频编辑方法中帧选择和时间一致性的问题。&lt;h4&gt;背景&lt;/h4&gt;无需训练的视频编辑技术最近取得了进展，利用预训练的文本到图像扩散模型实现了轻量级和精确的跨帧生成。然而，现有方法通常依赖启发式帧选择来维持DDIM反演过程中的时间一致性，这引入了人工偏差并降低了端到端推理的可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应选择关键帧并将它们的潜在特征压缩为语义锚点的方法，以实现一致的视频编辑。&lt;h4&gt;方法&lt;/h4&gt;VALA采用具有对比学习目标的变分框架，将跨帧潜在表示转换为保留内容和时间一致性的压缩潜在锚点。该方法可以完全集成到无需训练的基于文本到图像的视频编辑模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界视频编辑基准上的大量实验表明，VALA在反演保真度、编辑质量和时间一致性方面达到了最先进的性能，同时相比之前的方法提供了更高的效率。&lt;h4&gt;结论&lt;/h4&gt;VALA是一种有效的变分对齐模块，能够解决现有视频编辑方法中帧选择和时间一致性的挑战，提高了视频编辑的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;最近无需训练的视频编辑技术的进步使得利用预训练的文本到图像扩散模型实现了轻量级和精确的跨帧生成。然而，现有方法通常依赖启发式帧选择来维持DDIM反演过程中的时间一致性，这引入了人工偏差并降低了端到端推理的可扩展性。在本文中，我们提出了VALA（变分锚点对齐），这是一种变分对齐模块，可以自适应选择关键帧并将它们的潜在特征压缩为语义锚点，以实现一致的视频编辑。为了学习有意义的分配，VALA提出了一个具有对比学习目标的变分框架。因此，它可以将跨帧潜在表示转换为保留内容和时间一致性的压缩潜在锚点。我们的方法可以完全集成到无需训练的基于文本到图像的视频编辑模型中。在真实世界视频编辑基准上的大量实验表明，VALA在反演保真度、编辑质量和时间一致性方面达到了最先进的性能，同时相比之前的方法提供了更高的效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in training-free video editing have enabled lightweight andprecise cross-frame generation by leveraging pre-trained text-to-imagediffusion models. However, existing methods often rely on heuristic frameselection to maintain temporal consistency during DDIM inversion, whichintroduces manual bias and reduces the scalability of end-to-end inference. Inthis paper, we propose~\textbf{VALA} (\textbf{V}ariational \textbf{A}lignmentfor \textbf{L}atent \textbf{A}nchors), a variational alignment module thatadaptively selects key frames and compresses their latent features intosemantic anchors for consistent video editing. To learn meaningful assignments,VALA propose a variational framework with a contrastive learning objective.Therefore, it can transform cross-frame latent representations into compressedlatent anchors that preserve both content and temporal coherence. Our methodcan be fully integrated into training-free text-to-image based video editingmodels. Extensive experiments on real-world video editing benchmarks show thatVALA achieves state-of-the-art performance in inversion fidelity, editingquality, and temporal consistency, while offering improved efficiency overprior methods.</description>
      <author>example@mail.com (Zhangkai Wu, Xuhui Fan, Zhongyuan Xie, Kaize Shi, Longbing Cao)</author>
      <guid isPermaLink="false">2510.22970v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Lingual Sponsored Search via Dual-Encoder and Graph Neural Networks for Context-Aware Query Translation in Advertising Platforms</title>
      <link>http://arxiv.org/abs/2510.22957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AdGraphTrans是一种结合图神经网络的双编码器框架，用于广告中的上下文感知查询翻译，显著提高了跨语言搜索广告的效果。&lt;h4&gt;背景&lt;/h4&gt;跨语言搜索广告对全球广告平台至关重要，但传统机器翻译方法无法捕捉查询特定的上下文线索，导致语义歧义，影响点击率和转化率。&lt;h4&gt;目的&lt;/h4&gt;解决传统翻译方法在广告搜索中的局限性，提高跨语言搜索广告的效果。&lt;h4&gt;方法&lt;/h4&gt;提出AdGraphTrans框架，使用多语言Transformer编码器独立编码用户查询和广告内容，将上下文关系建模为异构图，应用图注意力网络改进嵌入，并通过对比学习对齐嵌入以减少翻译歧义。&lt;h4&gt;主要发现&lt;/h4&gt;AdGraphTrans在EN-ZH、EN-ES、EN-FR语言对上实现BLEU得分38.9和语义相似度0.83，优于mBERT和M2M-100基线方法；在下游广告检索任务中提高4.67%点击率和1.72%转化率。&lt;h4&gt;结论&lt;/h4&gt;将基于图的上下文信号与双编码器翻译相结合，为增强广告平台中的跨语言搜索广告提供了强大的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;跨语言搜索广告对全球广告平台至关重要，用户来自不同语言背景并与多语言广告互动。传统机器翻译方法往往无法捕捉查询特定的上下文线索，导致语义歧义，对点击率和转化率产生负面影响。为应对这一挑战，我们提出了AdGraphTrans，一种结合图神经网络的双编码器新框架，用于广告中的上下文感知查询翻译。具体而言，使用多语言Transformer编码器独立编码用户查询和广告内容，并将上下文关系（如共同点击的广告、用户搜索会话和查询-广告共现）建模为异构图。然后应用图注意力网络利用语义和行为上下文改进嵌入。这些嵌入通过对比学习对齐，以减少翻译歧义。在从Google Ads和Amazon Ads收集的跨语言搜索广告数据集（EN-ZH、EN-ES、EN-FR对）上进行的实验表明，AdGraphTrans显著提高了查询翻译质量，实现BLEU得分38.9和语义相似度0.83，优于mBERT和M2M-100等强基线方法。此外，在下游广告检索任务中，AdGraphTrans比基线方法提高了4.67%的点击率和1.72%的转化率。这些结果证实，将基于图的上下文信号与双编码器翻译相结合，为增强广告平台中的跨语言搜索广告提供了强大的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-lingual sponsored search is crucial for global advertising platforms,where users from different language backgrounds interact with multilingual ads.Traditional machine translation methods often fail to capture query-specificcontextual cues, leading to semantic ambiguities that negatively impactclick-through rates (CTR) and conversion rates (CVR). To address thischallenge, we propose AdGraphTrans, a novel dual-encoder framework enhancedwith graph neural networks (GNNs) for context-aware query translation inadvertising. Specifically, user queries and ad contents are independentlyencoded using multilingual Transformer-based encoders (mBERT/XLM-R), andcontextual relations-such as co-clicked ads, user search sessions, and query-adco-occurrence-are modeled as a heterogeneous graph. A graph attention network(GAT) is then applied to refine embeddings by leveraging semantic andbehavioral context. These embeddings are aligned via contrastive learning toreduce translation ambiguity. Experiments conducted on a cross-lingualsponsored search dataset collected from Google Ads and Amazon Ads (EN-ZH,EN-ES, EN-FR pairs) demonstrate that AdGraphTrans significantly improves querytranslation quality, achieving a BLEU score of 38.9 and semantic similarity(cosine score) of 0.83, outperforming strong baselines such as mBERT andM2M-100. Moreover, in downstream ad retrieval tasks, AdGraphTrans yields +4.67%CTR and +1.72% CVR improvements over baseline methods. These results confirmthat incorporating graph-based contextual signals with dual-encoder translationprovides a robust solution for enhancing cross-lingual sponsored search inadvertising platforms.</description>
      <author>example@mail.com (Ziyang Gao, Yuanliang Qu, Yi Han)</author>
      <guid isPermaLink="false">2510.22957v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Bi-Encoder Contrastive Learning for Fingerprint and Iris Biometrics</title>
      <link>http://arxiv.org/abs/2510.22937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究挑战了生物特征统计学上不相关的传统假设，证明同一个体的生物特征（特别是虹膜）实际上存在相关性，并使用双编码器网络和深度学习模型验证了这一发现。&lt;h4&gt;背景&lt;/h4&gt;历史上一直假设个体的生物特征在统计学上是不相关的，这一假设需要被检验。&lt;h4&gt;目的&lt;/h4&gt;测试个体生物特征统计学上不相关的假设，通过训练双编码器网络进行三种生物特征验证任务。&lt;h4&gt;方法&lt;/h4&gt;在274名受试者上训练双编码器网络（约10万张指纹图像和7千张虹膜图像），进行三种匹配任务：指纹到指纹匹配、虹膜到虹膜匹配、跨模态指纹到虹膜匹配。使用ResNet-50和Vision Transformer骨干网络构建双编码器架构，最小化来自同一个体的图像之间的对比损失。&lt;h4&gt;主要发现&lt;/h4&gt;虹膜ResNet架构在虹膜到虹膜匹配中达到91的ROC AUC分数，证明个体的左右虹膜是相关的；指纹模型重现了先前工作提出的正样本内相关性；这是首次尝试使用Vision Transformer进行此类匹配；跨模态匹配仅略高于随机水平。&lt;h4&gt;结论&lt;/h4&gt;这些发现继续挑战生物特征的独立性假设，研究计划将这项工作扩展到其他生物特征。&lt;h4&gt;翻译&lt;/h4&gt;历史上一直假设个体的生物特征在统计学上是不相关的。我们通过在三种验证任务上训练双编码器网络来测试这一假设，包括指纹到指纹匹配、虹膜到虹膜匹配，以及使用274名受试者（约10万张指纹和7千张虹膜图像）进行的跨模态指纹到虹膜匹配。我们在双编码器架构中训练了ResNet-50和Vision Transformer骨干网络，以最小化来自同一个体的图像之间的对比损失。虹膜ResNet架构在虹膜到虹膜匹配中达到91的ROC AUC分数，提供了个体左右虹膜相关的明确证据。指纹模型重现了该领域先前工作所提出的正样本内相关性。这是首次尝试使用Vision Transformer进行此类匹配。跨模态匹配仅略高于随机水平，这表明需要更多数据和更复杂的管道才能获得令人信服的结果。这些发现继续挑战生物特征的独立性假设，我们计划将这项工作扩展到其他生物特征。代码可用：https://github.com/MatthewSo/bio_fingerprints_iris。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There has been a historic assumption that the biometrics of an individual arestatistically uncorrelated. We test this assumption by training Bi-Encodernetworks on three verification tasks, including fingerprint-to-fingerprintmatching, iris-to-iris matching, and cross-modal fingerprint-to-iris matchingusing 274 subjects with $\sim$100k fingerprints and 7k iris images. We trainedResNet-50 and Vision Transformer backbones in Bi-Encoder architectures suchthat the contrastive loss between images sampled from the same individual isminimized. The iris ResNet architecture reaches 91 ROC AUC score foriris-to-iris matching, providing clear evidence that the left and right irisesof an individual are correlated. Fingerprint models reproduce the positiveintra-subject suggested by prior work in this space. This is the first workattempting to use Vision Transformers for this matching. Cross-modal matchingrises only slightly above chance, which suggests that more data and a moresophisticated pipeline is needed to obtain compelling results. These findingscontinue challenge independence assumptions of biometrics and we plan to extendthis work to other biometrics in the future. Code available:https://github.com/MatthewSo/bio_fingerprints_iris.</description>
      <author>example@mail.com (Matthew So, Judah Goldfeder, Mark Lis, Hod Lipson)</author>
      <guid isPermaLink="false">2510.22937v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-Preserving Cross-Style Visual Reasoning for Robust Multi-Modal Understanding in Large Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.22838v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SP-CSVR的新型框架，用于解决大型视觉-语言模型中的'风格陷阱'问题，实现稳定语义理解和跨风格视觉推理。&lt;h4&gt;背景&lt;/h4&gt;'风格陷阱'阻碍了大型视觉-语言模型在不同视觉风格下的稳健语义理解，特别是在上下文学习中。现有方法难以有效解耦风格与内容，限制了模型的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现稳定语义理解和自适应跨风格视觉推理的框架，以克服风格陷阱问题。&lt;h4&gt;方法&lt;/h4&gt;提出语义保持跨风格视觉推理器(SP-CSVR)，包含三个核心组件：跨风格特征编码器(CSFE)用于风格-内容解耦；语义对齐的上下文解码器(SAICD)用于高效的少样本风格适应；自适应语义一致性模块(ASCM)采用多任务对比学习强制跨风格语义不变性。&lt;h4&gt;主要发现&lt;/h4&gt;在具有挑战性的多风格数据集上，SP-CSVR在视觉描述、视觉问答和上下文风格适应方面达到了最先进的性能。消融研究和泛化分析证实了该方法在增强稳健性、泛化能力和效率方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;SP-CSVR成功解决了大型视觉-语言模型中的风格陷阱问题，实现了跨风格的稳定语义理解和高效推理。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为'语义保持跨风格视觉推理器'(SP-CSVR)的新型框架，旨在解决大型视觉-语言模型(LVLMs)中的'风格陷阱'问题，从而实现稳定的语义理解和自适应的跨风格视觉推理。SP-CSVR集成了跨风格特征编码器(CSFE)用于风格-内容解耦，语义对齐的上下文解码器(SAICD)用于高效的少样本风格适应，以及采用多任务对比学习的自适应语义一致性模块(ASCM)来强制跨风格语义不变性。在具有挑战性的多风格数据集上的广泛实验表明，SP-CSVR在视觉描述、视觉问答和上下文风格适应方面达到了最先进的性能。包括消融研究和泛化分析在内的全面评估证实了SP-CSVR在增强稳健性、泛化能力和效率方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The "style trap" poses a significant challenge for Large Vision-LanguageModels (LVLMs), hindering robust semantic understanding across diverse visualstyles, especially in in-context learning (ICL). Existing methods often fail toeffectively decouple style from content, hindering generalization. To addressthis, we propose the Semantic-Preserving Cross-Style Visual Reasoner (SP-CSVR),a novel framework for stable semantic understanding and adaptive cross-stylevisual reasoning. SP-CSVR integrates a Cross-Style Feature Encoder (CSFE) forstyle-content disentanglement, a Semantic-Aligned In-Context Decoder (SAICD)for efficient few-shot style adaptation, and an Adaptive Semantic ConsistencyModule (ASCM) employing multi-task contrastive learning to enforce cross-stylesemantic invariance. Extensive experiments on a challenging multi-style datasetdemonstrate SP-CSVR's state-of-the-art performance across visual captioning,visual question answering, and in-context style adaptation. Comprehensiveevaluations, including ablation studies and generalization analysis, confirmSP-CSVR's efficacy in enhancing robustness, generalization, and efficiencyacross diverse visual styles.</description>
      <author>example@mail.com (Aya Nakayama, Brian Wong, Yuji Nishimura, Kaito Tanaka)</author>
      <guid isPermaLink="false">2510.22838v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>IGGT: Instance-Grounded Geometry Transformer for Semantic 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2510.22706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://github.com/lifuguan/IGGT_official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识，并通过3D一致的对比学习策略仅使用2D视觉输入编码具有几何结构和基于实例聚类的统一表示。&lt;h4&gt;背景&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为相互交织的维度，但大多数先前方法优先训练大型几何模型进行低级3D重建，将高级空间理解孤立处理，忽视了这两个基本方面间的关键相互作用，导致下游3D理解任务表现不佳。最近的尝试通过简单对齐3D模型与特定语言模型来缓解问题，但限制了感知能力和下游任务适应性。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够统一几何结构和语义理解的模型，改善3D场景的理解和重建能力，提高在下游任务中的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;设计了IGGT模型和3D一致的对比学习策略，指导模型仅通过2D视觉输入编码具有几何结构和基于实例聚类的统一表示。同时构建了InsScene-15K数据集，包含高质量的RGB图像、姿态、深度图和3D一致的实例级掩码注释。&lt;h4&gt;主要发现&lt;/h4&gt;通过统一几何结构和语义理解，可以有效地将2D视觉输入一致提升到具有明显不同对象实例的连贯3D场景，改善3D场景的理解和重建。&lt;h4&gt;结论&lt;/h4&gt;IGGT模型和InsScene-15K数据集能够有效解决3D场景分析中几何结构和语义理解分离的问题，提高下游3D理解任务的性能。&lt;h4&gt;翻译&lt;/h4&gt;人类自然地将3D世界的几何结构和语义内容视为交织的维度，使能够连贯准确地理解复杂场景。然而，大多数先前方法优先训练大型几何模型进行低级3D重建，并将高级空间理解孤立处理，忽视了这两个3D场景分析基本方面之间的关键相互作用，从而限制了泛化能力，导致在下游3D理解任务中表现不佳。最近的尝试通过简单地将3D模型与特定语言模型对齐来缓解这一问题，但这限制了感知能力，并限制了下游任务的适应性。在本文中，我们提出了InstanceGrounded Geometry Transformer (IGGT)，一个端到端的大型统一transformer，用于统一空间重建和实例级上下文理解的知识。具体来说，我们设计了一种3D一致的对比学习策略，指导IGGT仅通过2D视觉输入编码具有几何结构和基于实例聚类的统一表示。这种表示支持将2D视觉输入一致提升到具有明显不同对象实例的连贯3D场景。为了促进这一任务，我们进一步构建了InsScene-15K，一个包含高质量RGB图像、姿态、深度图和3D一致的实例级掩码注释的大规模数据集，采用了新颖的数据整理流程。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D几何重建和高级语义理解被分离处理的问题。人类自然将几何结构和语义内容作为交织维度理解3D世界，但当前方法要么优先处理低级几何重建而忽视高级语义理解，要么简单将几何与特定语言模型对齐。这种分离限制了模型泛化能力，导致在下游3D理解任务中表现不佳。解决这个问题对实现接近人类的空间智能理解至关重要，对机器人操作、AR/VR和空间规划等应用具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到人类自然地将几何结构和语义内容作为交织维度理解3D世界，然后指出当前方法的局限性：将几何重建和语义理解分离，或者简单地将几何与语言模型对齐。他们设计了一种新思路：通过联合训练将几何和实例级语义特征耦合，让模型自主学习3D实例级语义与其几何结构之间的关系。该方法借鉴了VGGT的Transformer架构、DINOv2的图像特征提取、DPT的密集预测架构以及SAM2的视频对象分割技术，并在实例空间跟踪中受到SAMPart3D的启发。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过联合训练将几何结构和实例级语义特征耦合，使模型能够自主学习3D实例级语义与其几何结构之间的关系，并使用实例掩码作为桥梁连接统一表示与各种视觉语言模型。整体流程为：1)输入多张RGB图像；2)使用大型统一Transformer编码为统一标记表示；3)通过几何头部和实例头部解码生成几何点图和实例聚类场；4)使用跨模态融合块增强实例特征的空间感知；5)应用3D一致的对比学习策略确保跨视图一致性；6)使用无监督聚类生成实例掩码；7)这些掩码引导视觉语言模型执行下游任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出IGGT框架，统一几何重建和语义理解；2)设计3D一致的对比学习策略；3)构建InsScene-15K数据集；4)提出实例级场景理解范式。相比之前工作，不同之处在于：不再将几何和语义分离处理，而是通过联合训练实现相互增强；不再绑定特定语言模型，而是通过实例掩码灵活集成各种视觉语言模型；能区分同一类别内的不同对象实例；实现更好的多视图一致性，特别是在大视角变化和复杂场景中。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; IGGT通过统一几何重建和语义理解，并引入实例级场景理解范式，实现了高质量、一致的语义3D重建，并能灵活支持多种下游应用。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally perceive the geometric structure and semantic content of a3D world as intertwined dimensions, enabling coherent and accurateunderstanding of complex scenes. However, most prior approaches prioritizetraining large geometry models for low-level 3D reconstruction and treathigh-level spatial understanding in isolation, overlooking the crucialinterplay between these two fundamental aspects of 3D-scene analysis, therebylimiting generalization and leading to poor performance in downstream 3Dunderstanding tasks. Recent attempts have mitigated this issue by simplyaligning 3D models with specific language models, thus restricting perceptionto the aligned model's capacity and limiting adaptability to downstream tasks.In this paper, we propose InstanceGrounded Geometry Transformer (IGGT), anend-to-end large unified transformer to unify the knowledge for both spatialreconstruction and instance-level contextual understanding. Specifically, wedesign a 3D-Consistent Contrastive Learning strategy that guides IGGT to encodea unified representation with geometric structures and instance-groundedclustering through only 2D visual inputs. This representation supportsconsistent lifting of 2D visual inputs into a coherent 3D scene with explicitlydistinct object instances. To facilitate this task, we further constructInsScene-15K, a large-scale dataset with high-quality RGB images, poses, depthmaps, and 3D-consistent instance-level mask annotations with a novel datacuration pipeline.</description>
      <author>example@mail.com (Hao Li, Zhengyu Zou, Fangfu Liu, Xuanyang Zhang, Fangzhou Hong, Yukang Cao, Yushi Lan, Manyuan Zhang, Gang Yu, Dingwen Zhang, Ziwei Liu)</author>
      <guid isPermaLink="false">2510.22706v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CLEANet: Robust and Efficient Anomaly Detection in Contaminated Multivariate Time Series</title>
      <link>http://arxiv.org/abs/2510.22619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为CLEANet的稳健高效的多变量时间序列异常检测框架，用于解决训练数据污染和模型推理效率低下的问题。CLEANet通过抗污染训练框架和轻量级共轭MLP设计，显著提高了检测性能并降低了计算成本。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列异常检测对于维护工业系统可靠性至关重要，但现实部署面临两大挑战：训练数据污染（噪声和隐藏异常）和低效的模型推理。现有无监督方法假设训练数据干净，但污染会扭曲学习模式并降低检测准确性；同时，复杂深度模型容易过度拟合到污染数据上且延迟高，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种稳健且高效的多变量时间序列异常检测框架，能够有效处理训练数据污染问题，避免模型过度拟合，并提高计算效率，从而提升异常检测的准确性和实用性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了CLEANet框架，包含两个核心组件：1) 抗污染训练框架(CRTF)，通过自适应重建权重策略结合聚类引导的对比学习减轻污染样本影响；2) 轻量级共轭MLP，用于分离时间跨特征依赖关系，避免过度拟合并提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;在五个公共数据集上，CLEANet比十个最先进的基线方法实现了高达73.04%的F1提升和81.28%的运行时间减少。此外，将CRTF集成到三个先进模型中平均获得5.35%的F1提升，证明了其良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CLEANet框架有效解决了多变量时间序列异常检测中的训练数据污染和模型推理效率问题，通过抗污染训练策略和轻量级模型设计显著提升了检测性能和计算效率，具有良好的实用价值和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列异常检测对于维护工业系统可靠性至关重要，但现实部署受到两个关键挑战的阻碍：训练数据污染（噪声和隐藏异常）和低效的模型推理。现有无监督方法假设训练数据干净，但污染会扭曲学习模式并降低检测准确性。同时，复杂深度模型往往过度拟合到污染数据上并遭受高延迟，限制了实际应用。为解决这些挑战，我们提出了CLEANet，一个在受污染多变量时间序列中稳健且高效的异常检测框架。CLEANet引入了抗污染训练框架(CRTF)，通过自适应重建权重策略结合聚类引导的对比学习减轻污染样本的影响，从而增强稳健性。为进一步避免在污染数据上过度拟合并提高计算效率，我们设计了一个轻量级共轭MLP，用于分离时间跨特征依赖关系。在五个公共数据集上，CLEANet比十个最先进的基线方法实现了高达73.04%的F1提升和81.28%的运行时间减少。此外，将CRTF集成到三个先进模型中平均获得5.35%的F1提升，证实了其强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series (MTS) anomaly detection is essential for maintainingthe reliability of industrial systems, yet real-world deployment is hindered bytwo critical challenges: training data contamination (noises and hiddenanomalies) and inefficient model inference. Existing unsupervised methodsassume clean training data, but contamination distorts learned patterns anddegrades detection accuracy. Meanwhile, complex deep models often overfit tocontamination and suffer from high latency, limiting practical use. To addressthese challenges, we propose CLEANet, a robust and efficient anomaly detectionframework in contaminated multivariate time series. CLEANet introduces aContamination-Resilient Training Framework (CRTF) that mitigates the impact ofcorrupted samples through an adaptive reconstruction weighting strategycombined with clustering-guided contrastive learning, thereby enhancingrobustness. To further avoid overfitting on contaminated data and improvecomputational efficiency, we design a lightweight conjugate MLP thatdisentangles temporal and cross-feature dependencies. Across five publicdatasets, CLEANet achieves up to 73.04% higher F1 and 81.28% lower runtimecompared with ten state-of-the-art baselines. Furthermore, integrating CRTFinto three advanced models yields an average 5.35% F1 gain, confirming itsstrong generalizability.</description>
      <author>example@mail.com (Songhan Zhang, Yuanhao Lai, Pengfei Zheng, Boxi Yu, Xiaoying Tang, Qiuai Fu, Pinjia He)</author>
      <guid isPermaLink="false">2510.22619v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers</title>
      <link>http://arxiv.org/abs/2510.22555v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了CP-GBA(Cross-Paradigm Graph Backdoor Attacks with Promptable Subgraph Triggers)，一种新的可转移图后门攻击方法，通过图提示学习训练通用子图触发器，实现了跨学习范式的有效攻击。&lt;h4&gt;背景&lt;/h4&gt;图神经网络易受后门攻击，现有触发器生成器结构简单，过度依赖特定特征，局限于单一图学习范式（如图监督学习、图对比学习或图提示学习），导致跨范式转移性差，无法充分利用图数据的复杂结构信息和节点多样性。&lt;h4&gt;目的&lt;/h4&gt;解决现有触发器生成器的局限性，提高攻击成功率，开发一种能在多种学习范式中有效工作的可转移图后门攻击方法。&lt;h4&gt;方法&lt;/h4&gt;提出CP-GBA方法，首先从目标图中提炼出紧凑且具有表达力的触发器集合，通过联合强制类感知性、特征丰富度和结构保真度来实现；其次探索了GPL在基于提示的目标下训练这些触发器的理论可转移性，使其能泛化到多样化和未见过的测试时范式。&lt;h4&gt;主要发现&lt;/h4&gt;CP-GBA在多个真实数据集和防御场景中实现了最先进的攻击成功率，证明了其在不同学习范式间的有效泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CP-GBA通过利用图提示学习训练通用子图触发器，解决了现有触发器生成器的局限性，提高了攻击成功率，为图神经网络的后门攻击研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)容易受到后门攻击，攻击者植入恶意触发器来操纵模型预测。现有的触发器生成器通常结构简单，过度依赖特定特征，将其限制在单一图学习范式中。这种专门化设计导致触发器在应用于其他学习范式时转移性差。此外，这些简单生成器通常无法利用图数据中的复杂结构信息或节点多样性，限制了攻击成功率。因此，我们提出了CP-GBA，采用图提示学习训练一组通用子图触发器，通过提炼紧凑且具有表达力的触发器集合和探索理论可转移性，实现了在多种学习范式中的有效攻击，并在多个数据集和防御场景中取得了最先进的攻击成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks(GNNs) are vulnerable to backdoor attacks, whereadversaries implant malicious triggers to manipulate model predictions.  Existing trigger generators are often simplistic in structure and overlyreliant on specific features, confining them to a single graph learningparadigm, such as graph supervised learning, graph contrastive learning, orgraph prompt learning.  This specialized design, which aligns the trigger with one learningobjective, results in poor transferability when applied to other learningparadigms.  For instance, triggers generated for the graph supervised learning paradigmperform poorly when tested within graph contrastive learning or graph promptlearning environments.  Furthermore, these simple generators often fail to utilize complex structuralinformation or node diversity within the graph data.  These constraints limit the attack success rates of such methods in generaltesting scenarios.  Therefore, to address these limitations, we propose Cross-Paradigm GraphBackdoor Attacks with Promptable Subgraph Triggers(CP-GBA), a new transferablegraph backdoor attack that employs graph prompt learning(GPL) to train a set ofuniversal subgraph triggers.  First, we distill a compact yet expressive trigger set from target graphs,which is structured as a queryable repository, by jointly enforcingclass-awareness, feature richness, and structural fidelity.  Second, we conduct the first exploration of the theoretical transferabilityof GPL to train these triggers under prompt-based objectives, enablingeffective generalization to diverse and unseen test-time paradigms.  Extensive experiments across multiple real-world datasets and defensescenarios show that CP-GBA achieves state-of-the-art attack success rates.</description>
      <author>example@mail.com (Dongyi Liu, Jiangtong Li, Dawei Cheng, Changjun Jiang)</author>
      <guid isPermaLink="false">2510.22555v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding</title>
      <link>http://arxiv.org/abs/2510.22264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了PatenTEB，一个全面的专利文本嵌入基准测试，包含15个任务，跨越检索、分类、释义和聚类领域，共206万个示例。同时开发了patembed模型家族，通过多任务训练，参数量从67M到344M不等，上下文长度最高可达4096个token。外部验证显示patembed-base在MTEB BigPatentClustering.v2上达到最先进水平（0.494 V-measure vs. 之前的0.445最佳），而patembed-large在DAPFAM上达到0.377 NDCG@100。&lt;h4&gt;背景&lt;/h4&gt;专利文本嵌入能够实现现有技术搜索、技术景观分析和专利分析，但现有的基准测试无法充分捕捉专利特有的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够更好地捕捉专利特定挑战的基准测试和模型，以提高专利文本嵌入的性能和适用性。&lt;h4&gt;方法&lt;/h4&gt;1. 创建PatenTEB基准测试，包含15个任务，跨越检索、分类、释义和聚类领域；2. 使用领域分层分割、领域特定硬负挖掘和系统覆盖不对称片段到文档匹配场景；3. 开发patembed模型家族，通过多任务训练，参数量从67M到344M；4. 使用领域预训练初始化。&lt;h4&gt;主要发现&lt;/h4&gt;1. 多任务训练提高了外部泛化能力，尽管对基准测试有轻微影响；2. 领域预训练初始化在任务家族中提供了持续的优势；3. patembed-base在MTEB BigPatentClustering.v2上达到最先进水平（0.494 V-measure）；4. patembed-large在DAPFAM上达到0.377 NDCG@100。&lt;h4&gt;结论&lt;/h4&gt;PatenTEB基准测试和patembed模型家族能够有效解决专利文本嵌入中的特定挑战，并通过多任务训练和领域预训练初始化实现了更好的性能和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;专利检索、句子嵌入、多任务学习、不对称检索、基准测试评估、对比学习&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Patent text embeddings enable prior art search, technology landscaping, andpatent analysis, yet existing benchmarks inadequately capture patent-specificchallenges. We introduce PatenTEB, a comprehensive benchmark comprising 15tasks across retrieval, classification, paraphrase, and clustering, with 2.06million examples. PatenTEB employs domain-stratified splits, domain specifichard negative mining, and systematic coverage of asymmetricfragment-to-document matching scenarios absent from general embeddingbenchmarks. We develop the patembed model family through multi-task training,spanning 67M to 344M parameters with context lengths up to 4096 tokens.External validation shows strong generalization: patembed-base achievesstate-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.Systematic ablations reveal that multi-task training improves externalgeneralization despite minor benchmark costs, and that domain-pretrainedinitialization provides consistent advantages across task families. Allresources will be made available at https://github.com/iliass-y/patenteb.Keywords: patent retrieval, sentence embeddings, multi-task learning,asymmetric retrieval, benchmark evaluation, contrastive learning.</description>
      <author>example@mail.com (Iliass Ayaou, Denis Cavallucci)</author>
      <guid isPermaLink="false">2510.22264v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Attention Residual Fusion Network with Contrast for Source-free Domain Adaptation</title>
      <link>http://arxiv.org/abs/2510.22142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于对比学习的注意力残差融合网络(ARFNet)框架，用于解决源域无适应(SFDA)中的负迁移和域偏移问题。&lt;h4&gt;背景&lt;/h4&gt;源域无适应(SFDA)是在源域训练模型后应用于相关目标域，但适应过程中无法访问源数据和标签的任务。场景信息复杂和缺乏源域数据使SFDA成为一项困难任务。现有研究虽取得一定成果，但许多方法只关注域偏移而忽略负迁移的影响。&lt;h4&gt;目的&lt;/h4&gt;解决SFDA中的负迁移和域偏移问题，提高模型在适应过程中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出ARFNet框架，利用三种技术：1)注意力机制捕获目标对象的判别区域；2)将注意力特征分解为空间和通道注意力，实现跨层注意力残差融合和自蒸馏；3)对比全局和局部表示，提高类别感知能力；4)动态质心评估策略评估可信质心和标签，用于自监督自蒸馏，减轻域偏移。&lt;h4&gt;主要发现&lt;/h4&gt;在五个不同规模的基准测试上进行的综合实验表明，该方法优于其他技术，在SFDA基准测试中取得了优越的性能。&lt;h4&gt;结论&lt;/h4&gt;ARFNet框架有效解决了SFDA中的负迁移和域偏移问题，在多个基准测试上证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;源域无适应(SFDA)涉及在源域训练模型，然后将其应用于相关目标域，但在适应过程中无法访问源数据和标签。场景信息复杂和缺乏源域数据使SFDA成为一项困难任务。最近研究显示出有希望的结果，但许多域适应方法集中在域偏移上，而忽略了负迁移的影响，这可能阻碍模型在适应过程中的性能提升。在本文中，针对这一问题，我们提出了一个基于对比学习的注意力残差融合网络(ARFNet)框架，用于SFDA，以减轻适应过程中的负迁移和域偏移，其中利用了注意力残差融合、全局-局部注意力对比和动态质心评估。具体来说，首先利用注意力机制捕获目标对象的判别区域。然后，在每个块中，注意力特征被分解为空间注意力和通道注意力，以逐步实现跨层注意力残差融合和自蒸馏。在适应过程中，我们对比全局和局部表示，以提高不同类别的感知能力，使模型能够区分类内和类间变化。最后，利用动态质心评估策略评估可信质心和标签，用于自监督自蒸馏，旨在准确近似源域中心和伪标签，以减轻域偏移。为了验证有效性，我们在五个不同规模的基准上进行了综合实验。实验结果表明，我们的方法优于其他技术，在SFDA基准测试中取得了优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/TCSVT.2025.3626247&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Source-free domain adaptation (SFDA) involves training a model on sourcedomain and then applying it to a related target domain without access to thesource data and labels during adaptation. The complexity of scene informationand lack of the source domain make SFDA a difficult task. Recent studies haveshown promising results, but many approaches to domain adaptation concentrateon domain shift and neglect the effects of negative transfer, which may impedeenhancements of model performance during adaptation. n this paper, addressingthis issue, we propose a novel framework of Attention Residual Fusion Network(ARFNet) based on contrast learning for SFDA to alleviate negative transfer anddomain shift during the progress of adaptation, in which attention residualfusion, global-local attention contrast, and dynamic centroid evaluation areexploited. Concretely, the attention mechanism is first exploited to capturethe discriminative region of the target object. Then, in each block, attentionfeatures are decomposed into spatial-wise and channel-wise attentions toachieve the cross-layer attention residual fusion progressively andself-distillation. During adaptation progress, we contrast global and localrepresentations to improve the perceptual capabilities of different categories,which enables the model to discriminate variations between inner-class andintra-class. Finally, a dynamic centroid evaluation strategy is exploited toevaluate the trustworthy centroids and labels for self-supervisedself-distillation, which aims to accurately approximate the center of thesource domain and pseudo-labels to mitigate domain shift. To validate theefficacy, we execute comprehensive experiments on five benchmarks of varyingscales. Experimental outcomes indicate that our method surpasses othertechniques, attaining superior performance across SFDA benchmarks.</description>
      <author>example@mail.com (Renrong Shao, Wei Zhang, Jun Wang)</author>
      <guid isPermaLink="false">2510.22142v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LOC: A General Language-Guided Framework for Open-Set 3D Occupancy Prediction</title>
      <link>http://arxiv.org/abs/2510.22141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了LOC框架，一种用于3D场景理解的视觉语言模型方法，通过密集对比学习增强开放集识别能力。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型在开放集挑战中取得了显著进展，但3D数据集的有限可用性限制了它们在3D场景理解中的有效应用。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用的语言引导框架，适应各种占据网络，支持监督和自监督学习范式，以改善VLMs在3D场景理解中的应用。&lt;h4&gt;方法&lt;/h4&gt;提出LOC框架，融合多帧LiDAR点，使用泊松重建填补空洞，通过KNN分配体素语义，引入DCL缓解特征过度同质化，预测嵌入CLIP特征空间的密集体素特征。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的实验表明，该方法对已知类别实现了高精度预测，能够区分未知类别而无需额外训练数据。&lt;h4&gt;结论&lt;/h4&gt;LOC框架有效解决了VLMs在3D场景理解中的应用限制，通过密集对比学习增强了开放集识别能力，同时支持监督和自监督学习。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在开放集挑战中已显示出显著进展。然而，3D数据集的有限可用性阻碍了它们在3D场景理解中的有效应用。我们提出了LOC，一个通用的语言引导框架，可适应各种占据网络，支持监督和自监督学习范式。对于自监督任务，我们采用了一种融合多帧LiDAR点以处理动态/静态场景的策略，使用泊松重建填补空洞，并通过K近邻为体素分配语义，以获得全面的体素表示。为了缓解直接高维特征蒸馏导致的特征过度同质化问题，我们引入了密集对比学习。DCL利用密集体素语义信息和预定义的文本提示，有效增强了开放集识别能力，无需密集像素级监督，我们的框架还可以利用现有真实数据进一步改善性能。我们的模型预测嵌入在CLIP特征空间中的密集体素特征，整合文本和图像像素信息，并基于文本和语义相似性进行分类。在nuScenes数据集上的实验证明了该方法的优越性能，对已知类别实现了高精度预测，并能区分未知类别而无需额外训练数据。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决开放集3D占用预测问题，即让模型能够识别训练数据中未包含的新物体类别。这个问题在自动驾驶等领域非常重要，因为现实世界中物体种类繁多，训练数据无法覆盖所有可能的物体类别，系统需要能够识别未知物体以确保安全。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有3D占用预测方法的局限性，即它们只能识别已知类别。然后借鉴了视觉语言模型(如CLIP)的知识，利用它们在大量图像-文本对上训练的优势。同时采用了多帧LiDAR点云融合、Poisson重建和KNN等技术来处理3D数据的稀疏性问题。最终设计了LOC框架，结合了监督学习和自监督学习，并通过密集对比学习解决了特征过度同质化的问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的视觉语言模型的丰富语义知识，通过密集对比学习将2D空间的知识转移到3D空间，增强模型对未知类别的识别能力。整体流程包括：1)将2D图像特征投影到3D体素空间；2)使用占用头预测体素状态；3)通过语言头将体素特征映射到文本嵌入空间；4)应用鲁棒密集化策略生成密集3D表示；5)使用密集对比学习对齐体素特征与文本提示；6)结合两个头的输出实现开放集预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)LOC框架，首个用于开放集3D占用预测的语言引导框架；2)密集对比学习(DCL)，有效增强开放集识别能力并避免特征过度同质化；3)鲁棒密集化策略，生成高质量密集3D占用表示。相比之前的工作，LOC能够同时处理已知和未知类别，而传统方法只能识别已知类别；LOC避免了直接高维特征蒸馏的问题；LOC支持监督和自监督学习，能更好地利用有限标注数据。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; LOC是一个通用的语言引导框架，通过密集对比学习将2D视觉语言模型的知识有效转移到3D空间，实现了对已知类别的高精度预测和对未知类别的有效区分，无需额外训练数据。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Models (VLMs) have shown significant progress in open-setchallenges. However, the limited availability of 3D datasets hinders theireffective application in 3D scene understanding. We propose LOC, a generallanguage-guided framework adaptable to various occupancy networks, supportingboth supervised and self-supervised learning paradigms. For self-supervisedtasks, we employ a strategy that fuses multi-frame LiDAR points fordynamic/static scenes, using Poisson reconstruction to fill voids, andassigning semantics to voxels via K-Nearest Neighbor (KNN) to obtaincomprehensive voxel representations. To mitigate feature over-homogenizationcaused by direct high-dimensional feature distillation, we introduce DenselyContrastive Learning (DCL). DCL leverages dense voxel semantic information andpredefined textual prompts. This efficiently enhances open-set recognitionwithout dense pixel-level supervision, and our framework can also leverageexisting ground truth to further improve performance. Our model predicts densevoxel features embedded in the CLIP feature space, integrating textual andimage pixel information, and classifies based on text and semantic similarity.Experiments on the nuScenes dataset demonstrate the method's superiorperformance, achieving high-precision predictions for known classes anddistinguishing unknown classes without additional training data.</description>
      <author>example@mail.com (Yuhang Gao, Xiang Xiang, Sheng Zhong, Guoyou Wang)</author>
      <guid isPermaLink="false">2510.22141v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Towards Low-Latency and Adaptive Ransomware Detection Using Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.21957v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper was accepted in the 2025 IEEE International Conference on  Computer Design (ICCD)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合自监督对比学习和神经架构搜索的框架，用于解决勒索软件检测中的三大局限性，实现了更高的检测准确性和更快的响应时间。&lt;h4&gt;背景&lt;/h4&gt;勒索软件已成为网络安全的严重威胁，因其快速演变、需要早期检测且多样性增加，对传统检测方法构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;解决现有AI勒索软件检测方法的三大局限性：特征依赖性、响应延迟和对未知变种的适应性有限。&lt;h4&gt;方法&lt;/h4&gt;提出一个结合自监督对比学习和神经架构搜索的框架，具体包括：(1)设计结合硬件性能计数器的对比学习框架分析勒索软件运行时行为；(2)引入自定义损失函数实现早期检测减少延迟；(3)部署神经架构搜索框架自动构建自适应模型架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有方法相比，提出的方法在检测准确性上提升高达16.1%，响应时间改善高达6倍，并在规避攻击下保持鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在检测准确性、响应时间和鲁棒性方面均优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;勒索软件由于其快速演变、早期检测的必要性和日益增长的多样性，已成为网络安全的关键威胁，对传统检测方法构成了重大挑战。虽然先前的研究提出了基于人工智能的方法来辅助勒索软件检测，但现有方法存在三个主要局限性：特定的特征依赖性、响应延迟以及对未见变种的适应性有限。在本文中，我们提出了一种结合自监督对比学习和神经架构搜索(NAS)的框架来解决这些挑战。具体来说，本文提供了三个重要贡献：(1)我们设计了一个结合硬件性能计数器(HPC)的对比学习框架，用于分析目标勒索软件的运行时行为。(2)我们引入了一个自定义的损失函数，鼓励对恶意活动的早期检测，并显著减少了检测延迟。(3)我们部署了一个神经架构搜索(NAS)框架，自动构建自适应的模型架构，使检测器能够灵活地与未见的勒索软件变种保持一致。实验结果表明，与现有方法相比，我们提出的方法在检测准确性(高达16.1%)和响应时间(高达6倍)方面都有显著提高，同时在规避攻击下保持鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ransomware has become a critical threat to cybersecurity due to its rapidevolution, the necessity for early detection, and growing diversity, posingsignificant challenges to traditional detection methods. While AI-basedapproaches had been proposed by prior works to assist ransomware detection,existing methods suffer from three major limitations, ad-hoc featuredependencies, delayed response, and limited adaptability to unseen variants. Inthis paper, we propose a framework that integrates self-supervised contrastivelearning with neural architecture search (NAS) to address these challenges.Specifically, this paper offers three important contributions. (1) We design acontrastive learning framework that incorporates hardware performance counters(HPC) to analyze the runtime behavior of target ransomware. (2) We introduce acustomized loss function that encourages early-stage detection of maliciousactivity, and significantly reduces the detection latency. (3) We deploy aneural architecture search (NAS) framework to automatically construct adaptivemodel architectures, allowing the detector to flexibly align with unseenransomware variants. Experimental results show that our proposed methodachieves significant improvements in both detection accuracy (up to 16.1%) andresponse time (up to 6x) compared to existing approaches while maintainingrobustness under evasive attacks.</description>
      <author>example@mail.com (Zhixin Pan, Ziyu Shu, Amberbir Alemayoh)</author>
      <guid isPermaLink="false">2510.21957v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Learning Neural Observer-Predictor Models for Limb-level Sampling-based Locomotion Planning</title>
      <link>http://arxiv.org/abs/2510.22789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于学习的观察器-预测器框架，用于准确预测足式机器人的全身运动，解决了简化运动学模型无法捕捉复杂闭环动力学的问题。该系统通过神经观察器提供可靠状态估计，并使用高效预测器评估潜在轨迹，在四足机器人上成功实现了肢体感知的运动规划。&lt;h4&gt;背景&lt;/h4&gt;准确的全身运动预测对足式机器人的安全自主导航至关重要，特别是在复杂环境中进行肢体级碰撞检查。简化的运动学模型往往无法捕捉机器人和其底层控制器的复杂闭环动力学，导致预测仅限于简单的平面运动。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够准确预测足式机器人复杂全身运动的框架，克服传统简化模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出一个基于学习的观察器-预测器框架，包含：1)具有可证明UUB保证的神经观察器，从本体感觉测量历史中提供可靠的潜在状态估计；2)计算高效的预测器，能够快速并行评估数千条潜在轨迹；3)将系统集成到基于MPPI的规划器中，并在Vision 60四足机器人上进行了硬件实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;硬件实验成功展示了系统在具有挑战性的狭窄通道和小物体上的有效肢体感知运动规划能力，证明该系统为动态机器人平台上的高性能、碰撞感知规划提供了稳健基础。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于学习的观察器-预测器框架能够准确预测足式机器人的全身运动，为复杂环境中的安全自主导航和碰撞感知规划提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;准确的全身运动预测对足式机器人的安全自主导航至关重要，能够实现如杂乱环境中肢体级碰撞检查等关键功能。简化的运动学模型往往无法捕捉机器人和其底层控制器的复杂闭环动力学，限制了它们仅能预测简单的平面运动。为此，我们提出了一种基于学习的观察器-预测器框架，能够准确预测这种运动。我们的方法特点是一个具有可证明UUB保证的神经观察器，它从本体感觉测量历史中提供可靠的潜在状态估计。这个稳定的估计初始化了一个计算高效的预测器，专为现代采样规划器所需的大量潜在轨迹的快速并行评估而设计。我们通过将神经预测器集成到Vision 60四足机器人的基于MPPI的规划器中验证了该系统。硬件实验成功展示了在具有挑战性的狭窄通道和小物体上的有效肢体感知运动规划，突显了我们系统为动态机器人平台上的高性能、碰撞感知规划提供稳健基础的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate full-body motion prediction is essential for the safe, autonomousnavigation of legged robots, enabling critical capabilities like limb-levelcollision checking in cluttered environments. Simplified kinematic models oftenfail to capture the complex, closed-loop dynamics of the robot and itslow-level controller, limiting their predictions to simple planar motion. Toaddress this, we present a learning-based observer-predictor framework thataccurately predicts this motion. Our method features a neural observer withprovable UUB guarantees that provides a reliable latent state estimate from ahistory of proprioceptive measurements. This stable estimate initializes acomputationally efficient predictor, designed for the rapid, parallelevaluation of thousands of potential trajectories required by modernsampling-based planners. We validated the system by integrating our neuralpredictor into an MPPI-based planner on a Vision 60 quadruped. Hardwareexperiments successfully demonstrated effective, limb-aware motion planning ina challenging, narrow passage and over small objects, highlighting our system'sability to provide a robust foundation for high-performance, collision-awareplanning on dynamic robotic platforms.</description>
      <author>example@mail.com (Abhijeet M. Kulkarni, Ioannis Poulakakis, Guoquan Huang)</author>
      <guid isPermaLink="false">2510.22789v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.19661v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgentSense是一种混合的、无需训练的框架，将大型语言模型集成到参与式城市感知中，通过多智能体进化系统适应动态城市条件，提供自然语言解释以提高透明度。&lt;h4&gt;背景&lt;/h4&gt;基于网络的参与式城市感知已成为现代城市管理的重要方法，利用移动个体作为分布式传感器。然而，现有系统在跨不同城市场景的泛化能力有限，且在决策过程中可解释性差。&lt;h4&gt;目的&lt;/h4&gt;解决现有城市感知系统的局限性，提高系统的泛化能力和可解释性。&lt;h4&gt;方法&lt;/h4&gt;AgentSense框架将大型语言模型集成到参与式城市感知中，通过多智能体进化系统实现。它首先使用经典规划器生成基线解决方案，然后迭代改进这些解决方案以适应动态城市条件和异构工人偏好，同时生成自然语言解释以提高透明度和信任度。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模移动数据集和七种动态干扰的广泛实验中，AgentSense在适应性和可解释性方面明显优于传统方法。与单一智能体LLM基线相比，该方法在性能和鲁棒性方面表现更好，并提供更合理和透明的解释。&lt;h4&gt;结论&lt;/h4&gt;AgentSense是部署自适应和可解释的网络城市感知系统的重要进展。&lt;h4&gt;翻译&lt;/h4&gt;基于网络的参与式城市感知已通过利用移动个体作为分布式传感器，成为现代城市管理的重要方法。然而，现有的城市感知系统在跨不同城市场景的泛化能力有限，且在决策过程中的可解释性较差。在这项工作中，我们介绍了AgentSense，这是一种混合的、无需训练的框架，通过多智能体进化系统将大型语言模型（LLMs）集成到参与式城市感知中。AgentSense首先使用经典规划器生成基线解决方案，然后迭代改进这些解决方案，以使感知任务分配适应动态城市条件和异构工人偏好，同时生成自然语言解释以提高透明度和信任度。在两个大规模移动数据集和七种动态干扰的广泛实验中证明，与传统方法相比，AgentSense在适应性和可解释性方面具有明显优势。此外，与单一智能体LLM基线相比，我们的方法在性能和鲁棒性方面都更优，并提供更合理和透明的解释。这些结果表明，AgentSense是向网络部署自适应和可解释的城市感知系统迈进的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Web-based participatory urban sensing has emerged as a vital approach formodern urban management by leveraging mobile individuals as distributedsensors. However, existing urban sensing systems struggle with limitedgeneralization across diverse urban scenarios and poor interpretability indecision-making. In this work, we introduce AgentSense, a hybrid, training-freeframework that integrates large language models (LLMs) into participatory urbansensing through a multi-agent evolution system. AgentSense initially employsclassical planner to generate baseline solutions and then iteratively refinesthem to adapt sensing task assignments to dynamic urban conditions andheterogeneous worker preferences, while producing natural language explanationsthat enhance transparency and trust. Extensive experiments across twolarge-scale mobility datasets and seven types of dynamic disturbancesdemonstrate that AgentSense offers distinct advantages in adaptivity andexplainability over traditional methods. Furthermore, compared to single-agentLLM baselines, our approach outperforms in both performance and robustness,while delivering more reasonable and transparent explanations. These resultsposition AgentSense as a significant advancement towards deploying adaptive andexplainable urban sensing systems on the web.</description>
      <author>example@mail.com (Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.19661v2</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CURVETE: Curriculum Learning and Progressive Self-supervised Training for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.23442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CURVETE是一种创新的深度卷积神经网络，通过课程学习和类别分解方法解决了医学图像分析中的样本有限和类别分布不规则的挑战，在各种医学图像数据集上表现出优越的分类性能。&lt;h4&gt;背景&lt;/h4&gt;在医学图像分析中，识别高质量且易于获取的标注样本是一个显著挑战。迁移学习技术利用预训练数据为这一问题提供了灵活的解决方案。然而，当数据集在类别间呈现不规则分布时，微调的效果会减弱。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为课程学习和渐进式自监督训练(CURVETE)的新型深度卷积神经网络，解决与样本有限相关的挑战，增强模型泛化能力，并提高整体分类性能。&lt;h4&gt;方法&lt;/h4&gt;CURVETE采用基于样本分解粒度的课程学习策略，在训练通用未标记样本时使用；在下游任务中整合类别分解方法，解决类别分布不规则的挑战；在脑肿瘤、数字膝盖X光和Mini-DDSM三个医学图像数据集上进行评估，研究了使用通用自监督样本分解方法的分类性能，包括和不包括课程学习组件。&lt;h4&gt;主要发现&lt;/h4&gt;CURVETE模型在测试集上实现了优越的性能，使用基线ResNet-50在脑肿瘤数据集上达到96.60%的准确率，在数字膝盖X光数据集上达到75.60%，在Mini-DDSM数据集上达到93.35%；使用基线DenseNet-121，在三个数据集上分别达到95.77%、80.36%和93.22%的准确率，优于其他训练策略。&lt;h4&gt;结论&lt;/h4&gt;CURVETE模型能够有效解决医学图像分析中的样本有限和类别分布不规则的挑战，通过课程学习和渐进式自监督训练，显著提高了分类性能。&lt;h4&gt;翻译&lt;/h4&gt;在医学图像分析中，识别高质量且易于获取的标注样本是一个显著挑战。迁移学习技术利用预训练数据为这一问题提供了灵活的解决方案。然而，当数据集在类别间呈现不规则分布时，微调的效果会减弱。本文提出了一种名为课程学习和渐进式自监督训练(CURVETE)的新型深度卷积神经网络。CURVETE通过在训练通用未标记样本时采用基于样本分解粒度的课程学习策略，解决了与样本有限相关的挑战，增强了模型泛化能力，并提高了整体分类性能。此外，CURVETE通过在下游任务中整合类别分解方法，解决了类别分布不规则的挑战。该方法在三个不同的医学图像数据集上进行了评估：脑肿瘤、数字膝盖X光和Mini-DDSM数据集。我们研究了使用通用自监督样本分解方法进行分类性能，包括和不包括在训练预任务中使用课程学习组件。实验结果表明，CURVETE模型在测试集上实现了优越的性能，使用基线ResNet-50在脑肿瘤数据集上达到96.60%的准确率，在数字膝盖X光数据集上达到75.60%，在Mini-DDSM数据集上达到93.35%。此外，使用基线DenseNet-121，在脑肿瘤、数字膝盖X光和Mini-DDSM数据集上分别达到95.77%、80.36%和93.22%的准确率，优于其他训练策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Identifying high-quality and easily accessible annotated samples poses anotable challenge in medical image analysis. Transfer learning techniques,leveraging pre-training data, offer a flexible solution to this issue. However,the impact of fine-tuning diminishes when the dataset exhibits an irregulardistribution between classes. This paper introduces a novel deep convolutionalneural network, named Curriculum Learning and Progressive Self-supervisedTraining (CURVETE). CURVETE addresses challenges related to limited samples,enhances model generalisability, and improves overall classificationperformance. It achieves this by employing a curriculum learning strategy basedon the granularity of sample decomposition during the training of genericunlabelled samples. Moreover, CURVETE address the challenge of irregular classdistribution by incorporating a class decomposition approach in the downstreamtask. The proposed method undergoes evaluation on three distinct medical imagedatasets: brain tumour, digital knee x-ray, and Mini-DDSM datasets. Weinvestigate the classification performance using a generic self-supervisedsample decomposition approach with and without the curriculum learningcomponent in training the pretext task. Experimental results demonstrate thatthe CURVETE model achieves superior performance on test sets with an accuracyof 96.60% on the brain tumour dataset, 75.60% on the digital knee x-raydataset, and 93.35% on the Mini-DDSM dataset using the baseline ResNet-50.Furthermore, with the baseline DenseNet-121, it achieved accuracies of 95.77%,80.36%, and 93.22% on the brain tumour, digital knee x-ray, and Mini-DDSMdatasets, respectively, outperforming other training strategies.</description>
      <author>example@mail.com (Asmaa Abbas, Mohamed Gaber, Mohammed M. Abdelsamea)</author>
      <guid isPermaLink="false">2510.23442v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>DREaM: Drug-Drug Relation Extraction via Transfer Learning Method</title>
      <link>http://arxiv.org/abs/2510.23189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为DREAM的方法，用于药物关系抽取，通过结合关系抽取模型和大型语言模型构建药物关系本体并验证结果。&lt;h4&gt;背景&lt;/h4&gt;药物关系抽取对识别药物相互作用和预测副作用至关重要。机器学习方法和大型医学文本数据库的发展降低了关系抽取成本，但目前缺乏专门针对药物关系抽取的数据集。&lt;h4&gt;目的&lt;/h4&gt;由于缺乏专业数据集，需要采用迁移学习方法来应用机器学习技术进行药物关系抽取，并构建药物关系本体。&lt;h4&gt;方法&lt;/h4&gt;DREAM方法首先使用训练好的关系抽取模型发现实体间关系，然后将模型应用于医学文本语料库构建药物关系本体，最后使用大型语言模型验证抽取的关系。&lt;h4&gt;主要发现&lt;/h4&gt;定量结果显示，大型语言模型同意从PubMed摘要子集中提取的71个关系。定性分析表明该方法能揭示医学领域的模糊性，突显了关系抽取的挑战。&lt;h4&gt;结论&lt;/h4&gt;通过迁移学习和大型语言模型验证，DREAM方法能有效提取药物关系并构建药物关系本体，同时揭示了医学领域中关系抽取的固有挑战。&lt;h4&gt;翻译&lt;/h4&gt;药物之间的关系抽取在识别药物-药物相互作用和预测副作用方面起着至关重要的作用。机器学习方法在关系抽取方面的进步，以及大型医学文本数据库的发展，使得与其他通常需要专业知识的方法相比，这种关系的提取成本更低。然而，据我们所知，目前专门用于药物关系抽取的数据集非常有限。因此，采用迁移学习成为在该领域应用机器学习方法的必要手段。在本研究中，我们提出了DREAM方法，该方法首先使用训练好的关系抽取模型发现实体间的关系，然后将该模型应用于医学文本语料库以构建药物关系本体。随后使用大型语言模型验证抽取的关系。定量结果表明，大型语言模型同意从PubMed摘要子集中提取的71个关系。此外，我们的定性分析表明，这种方法可以揭示医学领域中的模糊性，突显了该领域关系抽取的固有挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relation extraction between drugs plays a crucial role in identifying drugdrug interactions and predicting side effects. The advancement of machinelearning methods in relation extraction, along with the development of largemedical text databases, has enabled the low cost extraction of such relationscompared to other approaches that typically require expert knowledge. However,to the best of our knowledge, there are limited datasets specifically designedfor drug drug relation extraction currently available. Therefore, employingtransfer learning becomes necessary to apply machine learning methods in thisdomain. In this study, we propose DREAM, a method that first employs a trainedrelation extraction model to discover relations between entities and thenapplies this model to a corpus of medical texts to construct an ontology ofdrug relationships. The extracted relations are subsequently validated using alarge language model. Quantitative results indicate that the LLM agreed with 71of the relations extracted from a subset of PubMed abstracts. Furthermore, ourqualitative analysis indicates that this approach can uncover ambiguities inthe medical domain, highlighting the challenges inherent in relation extractionin this field.</description>
      <author>example@mail.com (Ali Fata, Hossein Rahmani, Parinaz Soltanzadeh, Amirhossein Derakhshan, Behrouz Minaei Bidgoli)</author>
      <guid isPermaLink="false">2510.23189v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LightPFP: A Lightweight Route to Ab Initio Accuracy at Scale</title>
      <link>http://arxiv.org/abs/2510.23064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LightPFP是一种数据高效的知识蒸馏框架，利用通用机器学习原子势(u-MLIP)生成针对特定材料的高质量训练数据，结合预训练轻量级MLIP提高效率，实现了比传统DFT方法快三个数量级的模型开发速度，同时保持与第一性原理相当的准确性，且生成的特定任务MLIP(ts-MLIP)在保持高精度的同时实现了1-2个数量级的推理速度提升。&lt;h4&gt;背景&lt;/h4&gt;原子模拟方法已从量子力学发展到密度泛函理论(DFT)，再到机器学习原子势(MLIPs)。通用MLIPs(u-MLIPs)具有良好的可转移性但计算开销大，限制了大规模应用；特定任务MLIPs(ts-MLIPs)效率更高但为每个材料系统生成DFT数据的成本极高。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据高效的知识蒸馏框架LightPFP，解决传统方法中DFT计算成本高的问题，实现快速开发高精度、高效的特定任务MLIPs。&lt;h4&gt;方法&lt;/h4&gt;LightPFP框架利用u-MLIP生成针对特定材料的高质量训练数据，并使用预训练的轻量级MLIP进一步提高数据效率，通过知识蒸馏技术生成ts-MLIP，同时支持高效的精度迁移学习。&lt;h4&gt;主要发现&lt;/h4&gt;LightPFP比传统基于DFT的方法快三个数量级的模型开发速度，同时保持与第一性原理预测相当的准确性；蒸馏出的ts-MLIP比u-MLIP快1-2个数量级的推理速度；仅需10个高精度DFT数据点即可校正u-MLIP的系统误差。&lt;h4&gt;结论&lt;/h4&gt;这种u-MLIP驱动的蒸馏方法能够为材料科学应用快速开发高保真度、高效的MLIPs。&lt;h4&gt;翻译&lt;/h4&gt;原子模拟方法已经通过连续的计算层级逐步发展，每个层级都建立在更基础的方法之上：从量子力学到密度泛函理论(DFT)，随后发展到机器学习原子势(MLIPs)。虽然通用MLIPs(u-MLIPs)具有广泛的可转移性，但其计算开销限制了大规模应用。特定任务MLIPs(ts-MLIPs)实现了更高的效率，但为每个材料系统生成DFT数据的成本高得令人望而却步。在本文中，我们提出了LightPFP，一种数据高效的知识蒸馏框架。LightPFP不使用昂贵的DFT计算，而是利用u-MLIP生成针对特定材料定制的高质量训练数据，并使用预训练的轻量级MLIP进一步提高数据效率，从而生成蒸馏的ts-MLIP。在包括固态电解质、高熵合金和反应离子系统在内的广泛材料范围内，LightPFP比传统基于DFT的方法快三个数量级的模型开发速度，同时保持与第一性原理预测相当的准确性。此外，蒸馏出的ts-MLs进一步维持了大规模分子动力学计算所必需的计算效率，比u-MLIPs快1-2个数量级的推理速度。该框架还支持高效的精度迁移学习，其中可以使用少至10个高精度DFT数据点来校正u-MLIP的系统误差，如在MgO熔点预测中所演示的。这种u-MLIP驱动的蒸馏方法能够为材料科学应用快速开发高保真度、高效的MLIPs。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Atomistic simulation methods have evolved through successive computationallevels, each building upon more fundamental approaches: from quantum mechanicsto density functional theory (DFT), and subsequently, to machine learninginteratomic potentials (MLIPs). While universal MLIPs (u-MLIPs) offer broadtransferability, their computational overhead limits large-scale applications.Task-specific MLIPs (ts-MLIPs) achieve superior efficiency but requireprohibitively expensive DFT data generation for each material system. In thispaper, we propose LightPFP, a data-efficient knowledge distillation framework.Instead of using costly DFT calculations, LightPFP generates a distilledts-MLIP by leveraging u-MLIP to generate high-quality training data tailoredfor specific materials and utilizing a pre-trained light-weight MLIP to furtherenhance data efficiency. Across a broad spectrum of materials, includingsolid-state electrolytes, high-entropy alloys, and reactive ionic systems,LightPFP delivers three orders of magnitude faster model development thanconventional DFT-based methods, while maintaining accuracy on par withfirst-principles predictions. Moreover, the distilled ts-MLIPs further sustainthe computational efficiency essential for large-scale molecular dynamics,achieving 1-2 orders of magnitude faster inference than u-MLIPs. The frameworkfurther enables efficient precision transfer learning, where systematic errorsfrom the u-MLIP can be corrected using as few as 10 high-accuracy DFT datapoints, as demonstrated for MgO melting point prediction. This u-MLIP-drivendistillation approach enables rapid development of high-fidelity, efficientMLIPs for materials science applications.</description>
      <author>example@mail.com (Wenwen Li, Nontawat Charoenphakdee, Yong-Bin Zhuang, Ryuhei Okuno, Yuta Tsuboi, So Takamoto, Junichi Ishida, Ju Li)</author>
      <guid isPermaLink="false">2510.23064v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>TLCD: A Deep Transfer Learning Framework for Cross-Disciplinary Cognitive Diagnosis</title>
      <link>http://arxiv.org/abs/2510.23062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种创新的跨学科认知诊断方法(TLCD)，结合深度学习和迁移学习策略，解决了跨学科领域中认知诊断面临的挑战，提高了对学生学习情况评估的准确性。&lt;h4&gt;背景&lt;/h4&gt;在线教育模式已成为教育产业的重要组成部分。认知诊断技术可利用学生学习数据评估其能力水平，但跨学科领域存在特征提取复杂性和学科数据稀缺性问题，传统认知诊断方法面临挑战。&lt;h4&gt;目的&lt;/h4&gt;针对不同学科间知识系统、认知结构和数据特征的差异，研究神经网络认知诊断和知识关联神经网络认知诊断，提出创新的跨学科认知诊断方法。&lt;h4&gt;方法&lt;/h4&gt;提出跨学科认知诊断方法(TLCD)，结合深度学习技术和迁移学习策略，通过利用主学科的共同特征来提高模型在目标学科中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能够更准确地评估学生的学习情况。&lt;h4&gt;结论&lt;/h4&gt;跨学科认知诊断方法(TLCD)有效解决了跨学科认知诊断中的挑战，提高了诊断的准确性和性能，对智能教育领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;受智能教育和人工智能技术的双重驱动，在线教育模式已迅速成为教育产业的重要组成部分。认知诊断技术可以利用教育评估中学生学习的数据和反馈信息，准确评估他们在知识层面的能力水平。然而，大量信息虽然提供了丰富的数据资源，但也带来了特征提取的复杂性和学科数据的稀缺性。在跨学科领域，传统的认知诊断方法仍面临许多挑战。鉴于不同学科之间知识系统、认知结构和数据特征的差异，本文对神经网络认知诊断和知识关联神经网络认知诊断进行了深入研究，并提出了一种创新的跨学科认知诊断方法(TLCD)。该方法结合了深度学习技术和迁移学习策略，通过利用主学科的共同特征来提高模型在目标学科中的性能。实验结果表明，基于深度学习的跨学科认知诊断模型在跨学科认知诊断任务中表现优于基础模型，能够更准确地评估学生的学习情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driven by the dual principles of smart education and artificial intelligencetechnology, the online education model has rapidly emerged as an importantcomponent of the education industry. Cognitive diagnostic technology canutilize students' learning data and feedback information in educationalevaluation to accurately assess their ability level at the knowledge level.However, while massive amounts of information provide abundant data resources,they also bring about complexity in feature extraction and scarcity ofdisciplinary data. In cross-disciplinary fields, traditional cognitivediagnostic methods still face many challenges. Given the differences inknowledge systems, cognitive structures, and data characteristics betweendifferent disciplines, this paper conducts in-depth research on neural networkcognitive diagnosis and knowledge association neural network cognitivediagnosis, and proposes an innovative cross-disciplinary cognitive diagnosismethod (TLCD). This method combines deep learning techniques and transferlearning strategies to enhance the performance of the model in the targetdiscipline by utilizing the common features of the main discipline. Theexperimental results show that the cross-disciplinary cognitive diagnosis modelbased on deep learning performs better than the basic model incross-disciplinary cognitive diagnosis tasks, and can more accurately evaluatestudents' learning situation.</description>
      <author>example@mail.com (Zhifeng Wang, Meixin Su, Yang Yang, Chunyan Zeng, Lizhi Ye)</author>
      <guid isPermaLink="false">2510.23062v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Survey of Multimodal Geospatial Foundation Models: Techniques, Applications, and Challenges</title>
      <link>http://arxiv.org/abs/2510.22964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述从模态驱动视角对多模态地理空间基础模型(GFMs)进行全面回顾，涵盖五种核心视觉和视觉-语言模态，分析其在遥感图像分析中的应用、挑战和未来发展方向。&lt;h4&gt;背景&lt;/h4&gt;基础模型已改变自然语言处理和计算机视觉领域，其影响正在重塑遥感图像分析。基础模型强大的泛化和迁移学习能力与遥感数据的多模态、多分辨率和多时态特性自然契合。&lt;h4&gt;目的&lt;/h4&gt;解决遥感领域的独特挑战，通过多模态地理空间基础模型(GFMs)这一专门研究前沿，提供从模态驱动视角的全面回顾，并分析关键技术、评估模型性能和应用场景。&lt;h4&gt;方法&lt;/h4&gt;涵盖五种核心视觉和视觉-语言模态，检查成像物理和数据表示差异如何塑造交互设计，分析对齐、集成和知识转移的关键技术，系统评估训练范式、架构和适应策略进展，在十个下游任务上评估代表性模型，并通过真实案例研究展示应用潜力。&lt;h4&gt;主要发现&lt;/h4&gt;多模态GFMs在土地覆盖制图、农业监测、灾害响应、气候研究和地理空间情报等领域展现实际应用潜力，不同模型在架构、性能和应用场景上存在差异，需要针对模态异构性、分布偏移和语义差距进行优化。&lt;h4&gt;结论&lt;/h4&gt;领域泛化、可解释性、效率和隐私是GFMs发展面临的紧迫挑战，未来研究需要在这些方面探索有前途的方向，进一步提升模型性能和应用范围。&lt;h4&gt;翻译&lt;/h4&gt;基础模型已经改变了自然语言处理和计算机视觉，它们的影响现在正在重塑遥感图像分析。凭借强大的泛化和迁移学习能力，它们与遥感数据的多模态、多分辨率和多时态特性自然契合。为解决该领域的独特挑战，多模态地理空间基础模型(GFMs)已成为专门的研究前沿。这篇综述从模态驱动视角对多模态GFMs进行全面回顾，涵盖五种核心视觉和视觉-语言模态。我们检查成像物理和数据表示差异如何塑造交互设计，并分析对齐、集成和知识转移的关键技术，以处理模态异构性、分布偏移和语义差距。系统评估了训练范式、架构和任务特定适应策略的进展，以及大量新兴基准。在十个下游任务上评估了代表性的多模态视觉和视觉-语言GFMs，深入了解它们的架构、性能和应用场景。真实案例研究，涵盖土地覆盖制图、农业监测、灾害响应、气候研究和地理空间情报，展示了GFMs的实际潜力。最后，我们概述了领域泛化、可解释性、效率和隐私方面的紧迫挑战，并为未来研究规划了有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have transformed natural language processing and computervision, and their impact is now reshaping remote sensing image analysis. Withpowerful generalization and transfer learning capabilities, they alignnaturally with the multimodal, multi-resolution, and multi-temporalcharacteristics of remote sensing data. To address unique challenges in thefield, multimodal geospatial foundation models (GFMs) have emerged as adedicated research frontier. This survey delivers a comprehensive review ofmultimodal GFMs from a modality-driven perspective, covering five core visualand vision-language modalities. We examine how differences in imaging physicsand data representation shape interaction design, and we analyze key techniquesfor alignment, integration, and knowledge transfer to tackle modalityheterogeneity, distribution shifts, and semantic gaps. Advances in trainingparadigms, architectures, and task-specific adaptation strategies aresystematically assessed alongside a wealth of emerging benchmarks.Representative multimodal visual and vision-language GFMs are evaluated acrossten downstream tasks, with insights into their architectures, performance, andapplication scenarios. Real-world case studies, spanning land cover mapping,agricultural monitoring, disaster response, climate studies, and geospatialintelligence, demonstrate the practical potential of GFMs. Finally, we outlinepressing challenges in domain generalization, interpretability, efficiency, andprivacy, and chart promising avenues for future research.</description>
      <author>example@mail.com (Liling Yang, Ning Chen, Jun Yue, Yidan Liu, Jiayi Ma, Pedram Ghamisi, Antonio Plaza, Leyuan Fang)</author>
      <guid isPermaLink="false">2510.22964v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Inductive Transfer Learning for Graph-Based Recommenders</title>
      <link>http://arxiv.org/abs/2510.22799v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the New Perspectives in Graph Machine Learning Workshop  at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NBF-Rec，一种支持跨不同数据集进行归纳迁移学习的图推荐模型，能够在不重新训练的情况下处理新用户、新项目或新数据集。&lt;h4&gt;背景&lt;/h4&gt;图推荐系统通常在归纳设置下训练，这限制了它们对新用户、新项目或新数据集的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种支持在不同数据集上进行归纳迁移学习的图推荐模型，解决传统方法需要为每个领域重新训练的问题。&lt;h4&gt;方法&lt;/h4&gt;提出NBF-Rec模型，一种基于图的推荐模型，可以在用户和项目集合不相交的数据集之间进行归纳迁移学习。与传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入，无需为每个领域重新训练。&lt;h4&gt;主要发现&lt;/h4&gt;NBF-Rec在七个真实世界数据集（涵盖电影、音乐、电子商务和地点签到等领域）上进行了评估，在零样本设置下（不使用目标域数据进行训练）取得了具有竞争力的性能，并通过轻量级微调进一步提高了性能。&lt;h4&gt;结论&lt;/h4&gt;归纳迁移在图推荐中是可行的，交互级别的消息传递支持跨数据集的泛化，而无需对齐用户或项目。&lt;h4&gt;翻译&lt;/h4&gt;基于图的推荐系统通常在归纳设置下进行训练，这限制了它们对新用户、新项目或新数据集的适用性。我们提出了NBF-Rec，一种基于图的推荐模型，支持在不同用户和项目集合不相交的数据集上进行归纳迁移学习。与需要为每个领域重新训练的传统基于嵌入的方法不同，NBF-Rec在推理时动态计算节点嵌入。我们在七个涵盖电影、音乐、电子商务和地点签到的真实世界数据集上评估了该方法。NBF-Rec在零样本设置下（不使用目标域数据进行训练）取得了具有竞争力的性能，并通过轻量级微调展示了进一步的改进。这些结果表明，归纳迁移在图推荐中是可行的，并且交互级别的消息传递支持跨数据集的泛化，而无需对齐用户或项目。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based recommender systems are commonly trained in transductivesettings, which limits their applicability to new users, items, or datasets. Wepropose NBF-Rec, a graph-based recommendation model that supports inductivetransfer learning across datasets with disjoint user and item sets. Unlikeconventional embedding-based methods that require retraining for each domain,NBF-Rec computes node embeddings dynamically at inference time. We evaluate themethod on seven real-world datasets spanning movies, music, e-commerce, andlocation check-ins. NBF-Rec achieves competitive performance in zero-shotsettings, where no target domain data is used for training, and demonstratesfurther improvements through lightweight fine-tuning. These results show thatinductive transfer is feasible in graph-based recommendation and thatinteraction-level message passing supports generalization across datasetswithout requiring aligned users or items.</description>
      <author>example@mail.com (Florian Grötschla, Elia Trachsel, Luca A. Lanzendörfer, Roger Wattenhofer)</author>
      <guid isPermaLink="false">2510.22799v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Qlustering: Harnessing Network-Based Quantum Transport for Data Clustering</title>
      <link>http://arxiv.org/abs/2510.22727v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Qlustering，一种受量子启发的无监督学习算法，利用基于网络的量子传输进行数据聚类，在多种数据集上表现出与经典方法相当或更优的性能，特别是在处理非凸或高维数据时。&lt;h4&gt;背景&lt;/h4&gt;传统聚类方法主要基于距离度量，而量子计算提供了新的计算范式，可以解决传统方法难以处理的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的量子启发式聚类算法，能够有效处理非凸或高维数据，并具有计算效率和物理可实现性。&lt;h4&gt;方法&lt;/h4&gt;Qlustering将数据编码为紧束缚哈密顿量框架中的输入状态，通过量子粒子在网络中的传播动力学进行计算，聚类分配从终端节点的稳态输出电流中产生，算法通过迭代优化网络哈密顿量和随机更新实现收敛。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据集、定位问题、QM9分子数据库和Iris数据集上，Qlustering与k-means等经典方法相比具有竞争力或更优的性能，特别是在处理非凸或高维数据时表现出色。&lt;h4&gt;结论&lt;/h4&gt;Qlustering具有内在的鲁棒性、低计算复杂性和与光子实现的兼容性，为构建物理可实现的、量子原生的聚类架构提供了有前途的途径。&lt;h4&gt;翻译&lt;/h4&gt;我们引入Qlustering，一种用于无监督学习的受量子启发的算法，它利用基于网络的量子传输来执行数据聚类。与传统的基于距离的方法不同，Qlustering将量子粒子通过网络传播的稳态动力学视为计算资源。数据被编码为由Lindblad主方程控制的紧束缚哈密顿量框架中的输入状态，聚类分配从终端节点的稳态输出电流中产生。该算法迭代地优化网络的哈密顿量以最小化物理动机的成本函数，通过随机更新实现收敛。我们在合成数据集、定位问题以及真实的化学和生物数据（即QM9分子数据库和Iris数据集的子集）上对Qlustering进行了基准测试。在这些多样化的任务中，Qlustering展示了与k-means等经典方法相比具有竞争力或更优的性能，特别是对于非凸或高维数据。其内在的鲁棒性、低计算复杂性和与光子实现的兼容性表明，其有望实现物理可实现的、量子原生的聚类架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Qlustering, a quantum-inspired algorithm for unsupervisedlearning that leverages network-based quantum transport to perform dataclustering. In contrast to traditional distance-based methods, Qlusteringtreats the steady-state dynamics of quantum particles propagating through anetwork as a computational resource. Data are encoded as input states in atight-binding Hamiltonian framework governed by the Lindblad master equation,and cluster assignments emerge from steady-state output currents at terminalnodes. The algorithm iteratively optimizes the network's Hamiltonian tominimize a physically motivated cost function, achieving convergence throughstochastic updates. We benchmark Qlustering on synthetic datasets, alocalization problem, and real-world chemical and biological data, namelysubsets of the QM9 molecular database and the Iris dataset. Across thesediverse tasks, Qlustering demonstrates competitive or superior performancecompared with classical methods such as k-means, particularly for non-convex orhigh-dimensional data. Its intrinsic robustness, low computational complexity,and compatibility with photonic implementations suggest a promising routetoward physically realizable, quantum-native clustering architectures.</description>
      <author>example@mail.com (Shmuel Lorber, Yonatan Dubi)</author>
      <guid isPermaLink="false">2510.22727v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Species Transfer Learning in Agricultural AI: Evaluating ZebraPose Adaptation for Dairy Cattle Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.22618v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 11 figures, 6 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究评估了使用在斑马图像上训练的ZebraPose模型进行奶牛姿态估计的跨物种迁移学习潜力，发现在不同环境间存在显著泛化挑战。&lt;h4&gt;背景&lt;/h4&gt;姿态估计是计算机视觉的核心技术，用于理解动物姿态、行为和福利，但农业应用受限于缺乏大型标注的牲畜数据集，特别是奶牛数据集。&lt;h4&gt;目的&lt;/h4&gt;评估跨物种迁移学习的潜力和局限性，通过将ZebraPose模型适应于谷仓条件下奶牛的27个关键点检测。&lt;h4&gt;方法&lt;/h4&gt;使用三种配置评估模型：自定义农场数据集（375张图像）、APT-36K基准数据集的子集以及它们的组合，系统评估了模型在不同环境中的准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;组合模型在分布内数据上表现良好，但在未见过的谷仓和奶牛群体上出现显著泛化失败，表明合成到真实域差距是农业AI部署的主要障碍，物种形态相似性不足以实现跨域迁移。&lt;h4&gt;结论&lt;/h4&gt;研究强调了数据集多样性、环境变化性和计算约束对现实世界部署的影响，呼吁以农业为先的AI设计，优先考虑农场级真实性、跨环境鲁棒性和开放基准数据集。&lt;h4&gt;翻译&lt;/h4&gt;姿态估计作为计算机视觉的基石，用于理解动物姿态、行为和福利。然而，农业应用仍然受限于大型标注牲畜数据集的稀缺，特别是奶牛。本研究通过将ZebraPose（一种基于视觉变换器的模型，在合成斑马图像上训练）适应于谷仓条件下奶牛的27个关键点检测，评估了跨物种迁移学习的潜力和局限性。使用三种配置——自定义农场数据集（375张图像，加拿大新不伦瑞克州苏塞克斯）、APT-36K基准数据集的子集以及它们的组合，我们系统评估了模型在不同环境中的准确性和泛化能力。虽然组合模型在分布内数据上取得了有希望的性能，但当应用于未见过的谷仓和奶牛群体时，出现了显著的泛化失败。这些发现揭示了合成到真实域差距是农业AI部署的主要障碍，并强调物种间的形态相似性不足以进行跨域迁移。研究提供了关于数据集多样性、环境变化性和计算约束影响现实世界部署的实践见解。我们呼吁以农业为先的AI设计，优先考虑农场级真实性、跨环境鲁棒性和开放基准数据集，以推进可信和可扩展的以动物为中心的技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pose estimation serves as a cornerstone of computer vision for understandinganimal posture, behavior, and welfare. Yet, agricultural applications remainconstrained by the scarcity of large, annotated datasets for livestock,especially dairy cattle. This study evaluates the potential and limitations ofcross-species transfer learning by adapting ZebraPose - a visiontransformer-based model trained on synthetic zebra imagery - for 27-keypointdetection in dairy cows under real barn conditions. Using three configurations- a custom on-farm dataset (375 images, Sussex, New Brunswick, Canada), asubset of the APT-36K benchmark dataset, and their combination, wesystematically assessed model accuracy and generalization across environments.While the combined model achieved promising performance (AP = 0.86, AR = 0.87,PCK 0.5 = 0.869) on in-distribution data, substantial generalization failuresoccurred when applied to unseen barns and cow populations. These findingsexpose the synthetic-to-real domain gap as a major obstacle to agricultural AIdeployment and emphasize that morphological similarity between species isinsufficient for cross-domain transfer. The study provides practical insightsinto dataset diversity, environmental variability, and computationalconstraints that influence real-world deployment of livestock monitoringsystems. We conclude with a call for agriculture-first AI design, prioritizingfarm-level realism, cross-environment robustness, and open benchmark datasetsto advance trustworthy and scalable animal-centric technologies.</description>
      <author>example@mail.com (Mackenzie Tapp, Sibi Chakravarthy Parivendan, Kashfia Sailunaz, Suresh Neethirajan)</author>
      <guid isPermaLink="false">2510.22618v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A roadmap for curvature-based geometric data analysis and learning</title>
      <link>http://arxiv.org/abs/2510.22599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提供了对离散曲率模型的首次全面综述，涵盖了数学基础、计算公式以及在数据分析和学习中的实际应用。文章从黎曼几何和度量几何角度讨论离散曲率，并提出了一种曲率驱动的数据分析系统流程。&lt;h4&gt;背景&lt;/h4&gt;几何数据分析和学习已成为一个独特且快速发展的研究领域，因其跨领域的有效性而日益受到认可。曲率是该领域的核心概念，它能够捕捉内在几何结构并支持从社区检测到几何深度学习的众多任务。针对图、单纯复形、立方体复形和流形采样点云等多种数据表示，已经提出了广泛的离散曲率模型。&lt;h4&gt;目的&lt;/h4&gt;这篇论文旨在对现有的离散曲率模型进行首次全面综述，涵盖其数学基础、计算公式以及在数据分析和学习中的实际应用。&lt;h4&gt;方法&lt;/h4&gt;作者从黎曼几何和度量几何两个角度讨论离散曲率，并提出了一种曲率驱动的数据分析系统流程。他们还检查了不同数据表示下的相应计算算法，提供了详细的比较和见解。&lt;h4&gt;主要发现&lt;/h4&gt;离散曲率模型不仅为数据几何提供了有效的表征，而且构成了几何学习框架的基本组成部分。这些模型在各种数据表示上都有应用，并在监督和无监督学习中取得了最先进的应用效果。&lt;h4&gt;结论&lt;/h4&gt;这篇综述为研究人员提供了一个概念性和实践性的路线图，帮助他们更好地理解离散曲率作为几何理解和学习的基本工具。&lt;h4&gt;翻译&lt;/h4&gt;几何数据分析和学习已成为一个独特且快速发展的研究领域，其有效性在多样化的应用中日益得到认可。该领域的核心是曲率，一个强大且可解释的概念，它捕捉内在的几何结构并支撑着从社区检测到几何深度学习的众多任务。针对图、单纯复形、立方体复形和从流形采样的点云等多种数据表示，已经提出了广泛的离散曲率模型。这些模型不仅为数据几何提供了有效的表征，而且构成了几何学习框架的基本组成部分。在本文中，我们首次对现有的离散曲率模型进行了全面综述，涵盖了它们的数学基础、计算公式以及数据分析和学习中的实际应用。特别是，我们从黎曼几何和度量几何的角度讨论了离散曲率，并提出了一个曲率驱动的数据分析系统流程。我们进一步检查了不同数据表示下的相应计算算法，提供了详细的比较和见解。最后，我们回顾了曲率在监督和无监督学习中的最先进应用。本综述为研究人员提供了一个概念性和实践性的路线图，使他们能够更好地理解离散曲率作为几何理解和学习的基本工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决的问题是离散曲率模型的系统综述和整合。目前存在多种离散曲率模型（如Forman-Ricci、Ollivier-Ricci等），它们基于不同数学原理，应用于不同数据表示，但缺乏统一框架和比较。这个问题的重要性在于几何数据分析和学习已成为快速发展的研究领域，曲率是理解数据内在几何结构的关键概念，从社区检测到几何深度学习都有重要应用，而离散曲率模型为数据几何提供了高效表征，并构成几何学习框架的基本组成部分。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过系统性地回顾现有文献来构建他们的方法。首先介绍几何数据分析和学习的背景，强调曲率的重要性；然后回顾黎曼几何和度量几何中曲率的数学基础；接着介绍多种离散曲率模型的定义和计算方法；最后提出一个三步流程用于基于曲率的数据分析。作者借鉴了大量现有工作，包括Forman基于Bochner-Weitzenböck公式的组合曲率方法、Ollivier基于最优输运的粗糙Ricci曲率、Bakry-Émery的Ricci曲率下界、Joharinad和Jost的sectional曲率，以及Menger曲率、Haantjes曲率和电阻曲率等网络分析中的定义。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是曲率作为理解数据内在几何结构的基本工具，可以用于分析和学习各种非欧几里得数据。整体实现流程是三步流程：1）数据表示：根据应用领域提取合适的拓扑表示（如图、单纯复形、立方体复形或超图）；2）离散曲率计算：在提取的拓扑表示上应用适当的曲率定义（如Forman-Ricci、Ollivier-Ricci等）；3）特征提取：从计算出的曲率中提取有意义的几何特征，如边基曲率特征化边或连接，顶点基曲率特征化顶点或数据点。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次全面综述离散曲率模型；系统化分类曲率模型；跨领域整合不同领域的曲率概念；提出实用三步流程；提供各种曲率模型在不同数据表示上的具体计算方法。相比之前的工作，这篇论文的不同之处在于：之前的文献通常专注于单一曲率模型或特定应用，而本文提供了全面视角，比较了不同模型的优缺点；不仅关注理论，还关注实际计算和应用；强调了曲率在不同数据表示上的通用性，展示了其在几何深度学习中的广泛应用潜力。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文首次提供了离散曲率模型的全面综述，建立了从理论到实践的系统性框架，使研究者能够理解和应用曲率作为几何数据分析和学习的基本工具。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric data analysis and learning has emerged as a distinct and rapidlydeveloping research area, increasingly recognized for its effectiveness acrossdiverse applications. At the heart of this field lies curvature, a powerful andinterpretable concept that captures intrinsic geometric structure and underpinsnumerous tasks, from community detection to geometric deep learning. A widerange of discrete curvature models have been proposed for various datarepresentations, including graphs, simplicial complexes, cubical complexes, andpoint clouds sampled from manifolds. These models not only provide efficientcharacterizations of data geometry but also constitute essential components ingeometric learning frameworks. In this paper, we present the firstcomprehensive review of existing discrete curvature models, covering theirmathematical foundations, computational formulations, and practicalapplications in data analysis and learning. In particular, we discuss discretecurvature from both Riemannian and metric geometry perspectives and propose asystematic pipeline for curvature-driven data analysis. We further examine thecorresponding computational algorithms across different data representations,offering detailed comparisons and insights. Finally, we review state-of-the-artapplications of curvature in both supervised and unsupervised learning. Thissurvey provides a conceptual and practical roadmap for researchers to gain abetter understanding of discrete curvature as a fundamental tool for geometricunderstanding and learning.</description>
      <author>example@mail.com (Yasharth Yadav, Kelin Xia)</author>
      <guid isPermaLink="false">2510.22599v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AnyECG-Lab: An Exploration Study of Fine-tuning an ECG Foundation Model to Estimate Laboratory Values from Single-Lead ECG Signals</title>
      <link>http://arxiv.org/abs/2510.22301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了利用心电图(ECG)通过深度学习技术估计实验室值的可行性，提供了一种非侵入性、快速的临床决策支持方法。&lt;h4&gt;背景&lt;/h4&gt;当前实验室检测依赖于侵入性静脉采样，存在延迟问题。心电图作为无创且广泛可用的信号，为快速估计实验室值提供了有前景的途径，但现有模型受限于低信噪比、个体间变异性大、数据多样性有限以及泛化能力不足。&lt;h4&gt;目的&lt;/h4&gt;探索使用迁移学习微调大规模预训练的ECG基础模型(ECGFounder)，实现从ECG信号中估计实验室值，并建立实时、无创估计实验室值的可行性范围。&lt;h4&gt;方法&lt;/h4&gt;利用斯坦福大学的多模式临床监测急诊数据集(MC-MED)进行探索性研究，使用迁移学习技术对ECGFounder大型预训练模型进行微调，并生成了超过2000万个标准化的10秒ECG片段以增强对细微生化相关性的敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;在内部验证中，模型对33项实验室指标表现出强的预测性能(曲线下面积高于0.65)，对59项指标表现出中等性能(曲线下面积在0.55到0.65之间)，对16项指标表现有限(曲线下面积低于0.55)。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效的人工智能驱动解决方案，并建立了实时、无创估计实验室值的可行性范围。&lt;h4&gt;翻译&lt;/h4&gt;及时获取实验室值对临床决策至关重要，但当前方法依赖于侵入性静脉采样且本质上存在延迟。心电图作为一种无创且广泛可用的信号，为快速估计实验室值提供了有前景的方式。深度学习的最新进展使得从心电图中提取潜在的血液学特征成为可能。然而，现有模型受限于低信噪比、显著的个体间变异性、有限的数据多样性和次优的泛化能力，特别是在适配到导联数较少的可穿戴设备时。在本工作中，我们进行了一项探索性研究，利用迁移学习技术在斯坦福大学的多模式临床监测急诊数据集(MC-MED)上微调ECGFounder——一个大规模预训练的心电基础模型。我们生成了超过2000万个标准化的十秒心电片段，以增强对细微生化相关性的敏感性。在内部验证中，该模型对三十三项实验室指标表现出强的预测性能(曲线下面积高于0.65)，对五十九项指标表现出中等性能(在0.55和0.65之间)，对十六项指标表现有限(低于0.55)。该研究提供了一种高效的人工智能驱动解决方案，并建立了实时、无创估计实验室值的可行性范围。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timely access to laboratory values is critical for clinical decision-making,yet current approaches rely on invasive venous sampling and are intrinsicallydelayed. Electrocardiography (ECG), as a non-invasive and widely availablesignal, offers a promising modality for rapid laboratory estimation. Recentprogress in deep learning has enabled the extraction of latent hematologicalsignatures from ECGs. However, existing models are constrained by lowsignal-to-noise ratios, substantial inter-individual variability, limited datadiversity, and suboptimal generalization, especially when adapted to low-leadwearable devices. In this work, we conduct an exploratory study leveragingtransfer learning to fine-tune ECGFounder, a large-scale pre-trained ECGfoundation model, on the Multimodal Clinical Monitoring in the EmergencyDepartment (MC-MED) dataset from Stanford. We generated a corpus of more than20 million standardized ten-second ECG segments to enhance sensitivity tosubtle biochemical correlates. On internal validation, the model demonstratedstrong predictive performance (area under the curve above 0.65) forthirty-three laboratory indicators, moderate performance (between 0.55 and0.65) for fifty-nine indicators, and limited performance (below 0.55) forsixteen indicators. This study provides an efficient artificial-intelligencedriven solution and establishes the feasibility scope for real-time,non-invasive estimation of laboratory values.</description>
      <author>example@mail.com (Yujie Xiao, Gongzhen Tang, Wenhui Liu, Jun Li, Guangkun Nie, Zhuoran Kan, Deyun Zhang, Qinghao Zhao, Shenda Hong)</author>
      <guid isPermaLink="false">2510.22301v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Synthetic-to-Real Transfer Learning for Chromatin-Sensitive PWS Microscopy</title>
      <link>http://arxiv.org/abs/2510.22239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 5 figures and 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出CFU Net，一种分层分割架构，使用三阶段课程在合成多模态数据上训练，实现了近乎完美的细胞核分割性能，应用于超过一万个细胞核的自动分析，提取了区分正常与癌前组织的染色质生物标志物，为专业显微镜中的合成到真实迁移学习提供了通用框架。&lt;h4&gt;背景&lt;/h4&gt;染色质敏感部分波谱(csPWS)显微镜技术能够无标记检测发生在可见细胞转化之前的纳米级染色质包装变化，但手动细胞核分割限制了群体规模分析，且缺乏注释的csPWS成像数据阻碍了标准深度学习方法的使用。&lt;h4&gt;目的&lt;/h4&gt;解决手动细胞核分割限制群体规模分析的问题，克服缺乏注释csPWS成像数据的挑战，开发能够自动分析染色质包装变化的方法，用于早期癌症检测中的生物标志物发现。&lt;h4&gt;方法&lt;/h4&gt;提出CFU Net分层分割架构，使用三阶段课程在合成多模态数据上训练；采用基于物理的渲染，结合染色质包装统计、Mie散射模型和模态特定噪声；整合五种架构元素：ConvNeXt骨干网络、特征金字塔网络、UNet++密集连接、双注意力和深度监督；实现INT8量化以提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;在合成测试数据上实现近乎完美性能（Dice 0.9879，IoU 0.9895）；与基础UNet相比Dice提高8.3%；通过量化实现240倍吞吐量增益；提取的染色质生物标志物区分正常与癌前组织效应量显著（Cohen's d在1.31到2.98之间）；分类准确率达94%。&lt;h4&gt;结论&lt;/h4&gt;该工作为专业显微镜中的合成到真实迁移学习提供了通用框架，并提供了社区在临床标本上进行验证的开放资源，有效应用于早期癌症检测中的生物标志物发现。&lt;h4&gt;翻译&lt;/h4&gt;染色质敏感部分波谱(csPWS)显微镜技术能够无标记检测发生在可见细胞转化之前的纳米级染色质包装变化。然而，手动细胞核分割限制了早期癌症检测中生物标志物发现所需的群体规模分析。缺乏注释的csPWS成像数据阻碍了标准深度学习方法的直接使用。我们提出了CFU Net，一种使用三阶段课程在合成多模态数据上训练的分层分割架构。CFU Net在代表多样化光谱成像条件的保留合成测试数据上实现了近乎完美的性能，无需手动注释（Dice 0.9879，IoU 0.9895）。我们的方法使用基于物理的渲染，结合经验支持的染色质包装统计、Mie散射模型和模态特定噪声，并结合一个从对抗性RGB预训练进展到光谱微调和组织学验证的课程。CFU Net整合了五种架构元素（ConvNeXt骨干网络、特征金字塔网络、UNet++密集连接、双注意力和深度监督），这些元素共同使Dice比基础UNet提高了8.3%。我们展示了可部署的INT8量化，压缩率为74.9%，推理时间为0.15秒，比手动分析提高了240倍的吞吐量。应用于来自合成测试数据的超过一万个自动分割的细胞核，该流程提取了区分正常与癌前组织的染色质生物标志物，具有大的效应量（Cohen's d在1.31到2.98之间），达到94%的分类准确率。这项工作为专业显微镜中的合成到真实迁移学习提供了通用框架，并为社区在临床标本上进行验证提供了开放资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Chromatin sensitive partial wave spectroscopic (csPWS) microscopy enableslabel free detection of nanoscale chromatin packing alterations that occurbefore visible cellular transformation. However, manual nuclear segmentationlimits population scale analysis needed for biomarker discovery in early cancerdetection. The lack of annotated csPWS imaging data prevents direct use ofstandard deep learning methods. We present CFU Net, a hierarchical segmentationarchitecture trained with a three stage curriculum on synthetic multimodaldata. CFU Net achieves near perfect performance on held out synthetic test datathat represent diverse spectroscopic imaging conditions without manualannotations (Dice 0.9879, IoU 0.9895). Our approach uses physics basedrendering that incorporates empirically supported chromatin packing statistics,Mie scattering models, and modality specific noise, combined with a curriculumthat progresses from adversarial RGB pretraining to spectroscopic fine tuningand histology validation. CFU Net integrates five architectural elements(ConvNeXt backbone, Feature Pyramid Network, UNet plus plus dense connections,dual attention, and deep supervision) that together improve Dice over abaseline UNet by 8.3 percent. We demonstrate deployment ready INT8 quantizationwith 74.9 percent compression and 0.15 second inference, giving a 240 timesthroughput gain over manual analysis. Applied to more than ten thousandautomatically segmented nuclei from synthetic test data, the pipeline extractschromatin biomarkers that distinguish normal from pre cancerous tissue withlarge effect sizes (Cohens d between 1.31 and 2.98), reaching 94 percentclassification accuracy. This work provides a general framework for syntheticto real transfer learning in specialized microscopy and open resources forcommunity validation on clinical specimens.</description>
      <author>example@mail.com (Jahidul Arafat, Sanjaya Poudel)</author>
      <guid isPermaLink="false">2510.22239v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Automatic Assessment of Students' Classroom Engagement with Bias Mitigated Multi-task Model</title>
      <link>http://arxiv.org/abs/2510.22057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 12 figures, and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究开发了一种自动化系统来检测在线学习期间的学生参与度，同时确保模型不依赖性别等敏感特征进行预测，提高了模型的公平性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要方面，但传统评估方法可能不直接适用于虚拟环境。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化系统来检测在线学习期间学生的参与度水平，解决传统方法在虚拟环境中不适用的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的训练方法，应用属性正则正交化技术到分割模型分类器中，并使用多种迁移学习策略，以阻止模型利用敏感特征如性别进行预测。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法不仅有助于执行道德标准，还能增强模型预测的可解释性；通过该方法，预测敏感群体的分布差异从未缓解模型的皮尔逊相关系数0.897降低到缓解模型的0.999。&lt;h4&gt;结论&lt;/h4&gt;成功开发了一个能够检测在线学习中学生参与度的自动化系统，同时确保了模型的公平性和可解释性，源代码已在GitHub上公开。&lt;h4&gt;翻译&lt;/h4&gt;随着在线和虚拟学习的兴起，监控和提升学生参与度已成为有效教育的重要方面。评估学生参与度的传统方法可能不直接适用于虚拟环境。在本研究中，我们关注这一问题，致力于开发一个自动化系统来检测在线学习期间学生的参与度水平。我们提出了一种新的训练方法，可以阻止模型利用性别等敏感特征进行预测。所提出的方法不仅在执行道德标准方面有益，还能增强模型预测的可解释性。我们将属性正则正交化技术应用于分割模型分类器，该分类器使用多种迁移学习策略，在减少预测敏感群体的分布差异方面取得了有效成果，从未缓解模型的皮尔逊相关系数0.897降低到缓解模型的0.999。该项目的源代码可在https://github.com/ashiskb/elearning-engagement-study获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of online and virtual learning, monitoring and enhancingstudent engagement have become an important aspect of effective education.Traditional methods of assessing a student's involvement might not beapplicable directly to virtual environments. In this study, we focused on thisproblem and addressed the need to develop an automated system to detect studentengagement levels during online learning. We proposed a novel training methodwhich can discourage a model from leveraging sensitive features like gender forits predictions. The proposed method offers benefits not only in theenforcement of ethical standards, but also to enhance interpretability of themodel predictions. We applied an attribute-orthogonal regularization techniqueto a split-model classifier, which uses multiple transfer learning strategiesto achieve effective results in reducing disparity in the distribution ofprediction for sensitivity groups from a Pearson correlation coefficient of0.897 for the unmitigated model, to 0.999 for the mitigated model. The sourcecode for this project is available onhttps://github.com/ashiskb/elearning-engagement-study .</description>
      <author>example@mail.com (James Thiering, Tarun Sethupat Radha Krishna, Dylan Zelkin, Ashis Kumer Biswas)</author>
      <guid isPermaLink="false">2510.22057v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>LiteDiff</title>
      <link>http://arxiv.org/abs/2510.22004v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Lite-Diff，一种轻量级扩散模型适应方法，通过将轻量适应层集成到冻结的扩散U-Net中，结合潜在形态自编码器和像素级判别器，显著降低了计算成本并减少了过拟合，即使在数据有限的情况下也能高效工作。&lt;h4&gt;背景&lt;/h4&gt;扩散模型在高保真图像合成方面取得了显著成功，但在特定领域（如医学成像）微调这些模型仍然具有挑战性，原因是领域特定数据有限和完整模型适应的高计算成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效微调方法，使扩散模型能够在特定领域（如医学成像）有效适应，同时降低计算成本并减少过拟合风险。&lt;h4&gt;方法&lt;/h4&gt;Lite-Diff将轻量级适应层集成到冻结的扩散U-Net中，同时使用潜在形态自编码器（用于领域特定潜在一致性）和像素级判别器（用于对抗对齐）来增强训练。通过冻结基础模型权重并仅优化小型残差适配器模块实现轻量化。&lt;h4&gt;主要发现&lt;/h4&gt;选择性在不同U-Net块中集成适应层可以找到效率与性能的最佳平衡。在三个胸部X光数据集（Kaggle Chest X-Ray Pneumonia、NIH Chest X-ray14和VinBigData Chest X_ray）上的实验表明，Lite-Diff相比传统完整微调实现了更好的适应效率。&lt;h4&gt;结论&lt;/h4&gt;Lite-Diff框架为扩散模型的迁移学习提供了有希望的方向，促进了它们在多样化低数据领域中的部署。&lt;h4&gt;翻译&lt;/h4&gt;近年来，扩散模型在高保真图像合成方面表现出色。然而，由于领域特定数据有限和完整模型适应的高计算成本，将这些模型微调到专业领域（如医学成像）仍然具有挑战性。在本文中，我们引入了Lite-Diff（轻量级扩散模型适应），一种新的微调方法，它将轻量级适应层集成到冻结的扩散U-Net中，同时使用潜在形态自编码器（用于领域特定潜在一致性）和像素级判别器（用于对抗对齐）来增强训练。通过冻结基础模型的权重并仅优化小型残差适配器模块，Lite-Diff显著降低了计算开销并减轻了过拟合，即使在数据有限的情况下也是如此。此外，我们进行了消融研究，分析了在不同U-Net块中选择性集成适应层的效果，揭示了效率与性能之间的最佳平衡。在三个胸部X光数据集 - (1) Kaggle胸部X光肺炎、(2) NIH胸部X光14和(3) VinBigData胸部X光上的实验表明，Lite-Diff相比传统完整微调实现了更好的适应效率。我们的框架为扩散模型的迁移学习提供了有希望的方向，促进了它们在多样化低数据领域中的部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, diffusion models have demonstrated remarkable success inhigh-fidelity image synthesis. However, fine-tuning these models forspecialized domains, such as medical imaging, remains challenging due tolimited domain-specific data and the high computational cost of full modeladaptation. In this paper, we introduce Lite-Diff (Lightweight Diffusion ModelAdaptation), a novel finetuning approach that integrates lightweight adaptationlayers into a frozen diffusion U-Net while enhancing training with a latentmorphological autoencoder (for domain-specific latent consistency) and a pixellevel discriminator(for adversarial alignment). By freezing weights of the basemodel and optimizing only small residual adapter modules, LiteDiffsignificantly reduces the computational overhead and mitigates overfitting,even in minimal-data settings. Additionally, we conduct ablation studies toanalyze the effects of selectively integrating adaptation layers in differentU-Net blocks, revealing an optimal balance between efficiency and performance.Experiments on three chest X-ray datasets - (1) Kaggle Chest X-Ray Pneumonia,(2) NIH Chest X-ray14 and (3) VinBigData Chest X_ray demonstrate that LiteDiffachieves superior adaptation efficiency compared to naive full fine-tuning. Ourframework provides a promising direction for transfer learning in diffusionmodels, facilitating their deployment in diverse low data domains.</description>
      <author>example@mail.com (Ruchir Namjoshi, Nagasai Thadishetty, Vignesh Kumar, Hemanth Venkateshwara)</author>
      <guid isPermaLink="false">2510.22004v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Split-MMD Training for Small-Sample Cross-Dataset P300 EEG Classification</title>
      <link>http://arxiv.org/abs/2510.21969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures. Submitted to IEEE BIBM 2025 Workshop on Machine  Learning for EEG Signal Processing (MLESP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种自适应分割最大均值差异训练(AS-MMD)方法，用于解决从脑电图(EEG)中检测单次试验P300时数据量有限的问题，特别是在跨数据集迁移学习中的分布偏移挑战。&lt;h4&gt;背景&lt;/h4&gt;当只有少量标记试验可用时，从脑电图(EEG)中检测单次试验P300是困难的。当尝试通过迁移学习用大型源数据集增强小型目标集时，会出现跨数据集偏移问题。&lt;h4&gt;目的&lt;/h4&gt;研究两个公共视觉oddball ERP数据集之间的迁移学习，解决在小样本设置下(目标:每个受试者10次试验；源:每个受试者80次试验)的跨数据集分布不一致问题。&lt;h4&gt;方法&lt;/h4&gt;提出自适应分割最大均值差异训练(AS-MMD)，结合了三种技术：(1)与源/目标大小比值相关的目标加权损失和预热；(2)具有共享参数和每域统计的分割批量归一化；(3)使用中带带宽启发式的无参数对数级RBF核最大均值差异项。该方法在EEG Conformer上实现，与主干网络无关且保持推理模型不变。&lt;h4&gt;主要发现&lt;/h4&gt;在两种迁移方向上，AS-MMD均优于仅目标训练和联合训练(Active Visual Oddball: 准确率/AUC为0.66/0.74；ERP CORE P3: 0.61/0.65)，与联合训练相比的增益在统计上显著。消融研究表明所有三个组件都对性能提升有贡献。&lt;h4&gt;结论&lt;/h4&gt;AS-MMD方法有效解决了小样本条件下EEG信号P300检测中的跨数据集迁移学习挑战，通过结合三种创新技术显著提高了检测性能。&lt;h4&gt;翻译&lt;/h4&gt;当只有少量标记试验可用时，从脑电图(EEG)中检测单次试验P300是困难的。当尝试通过迁移学习用大型源数据集增强小型目标集时，会出现跨数据集偏移。为应对这一挑战，我们在严格的小样本设置下(目标:每个受试者10次试验；源:每个受试者80次试验)，研究了使用五个共享电极(Fz, Pz, P3, P4, Oz)在两个公共视觉oddball ERP数据集之间的迁移学习。我们引入了自适应分割最大均值差异训练(AS-MMD)，它结合了(i)与源/目标大小比值的平方根相关的目标加权损失和预热，(ii)具有共享仿射参数和每域运行统计的分割批量归一化(Split-BN)，以及(iii)使用中带带宽启发式的无参数对数级径向基函数核最大均值差异(RBF-MMD)项。在EEG Conformer上实现后，AS-MMD与主干网络无关且保持推理时模型不变。在两种迁移方向上，它都优于仅目标训练和联合训练(Active Visual Oddball: 准确率/AUC为0.66/0.74；ERP CORE P3: 0.61/0.65)，与联合训练相比的增益在校正后的配对t检验下显著。消融研究将改进归因于所有三个组件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting single-trial P300 from EEG is difficult when only a few labeledtrials are available. When attempting to boost a small target set with a largesource dataset through transfer learning, cross-dataset shift arises. Toaddress this challenge, we study transfer between two public visual-oddball ERPdatasets using five shared electrodes (Fz, Pz, P3, P4, Oz) under a strictsmall-sample regime (target: 10 trials/subject; source: 80 trials/subject). Weintroduce Adaptive Split Maximum Mean Discrepancy Training (AS-MMD), whichcombines (i) a target-weighted loss with warm-up tied to the square root of thesource/target size ratio, (ii) Split Batch Normalization (Split-BN) with sharedaffine parameters and per-domain running statistics, and (iii) a parameter-freelogit-level Radial Basis Function kernel Maximum Mean Discrepancy (RBF-MMD)term using the median-bandwidth heuristic. Implemented on an EEG Conformer,AS-MMD is backbone-agnostic and leaves the inference-time model unchanged.Across both transfer directions, it outperforms target-only and pooled training(Active Visual Oddball: accuracy/AUC 0.66/0.74; ERP CORE P3: 0.61/0.65), withgains over pooling significant under corrected paired t-tests. Ablationsattribute improvements to all three components.</description>
      <author>example@mail.com (Weiyu Chen, Arnaud Delorme)</author>
      <guid isPermaLink="false">2510.21969v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>An unsupervised tour through the hidden pathways of deep neural networks</title>
      <link>http://arxiv.org/abs/2510.21582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  PhD thesis&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究旨在深入理解深度人工神经网络创建有意义表示并实现泛化的内部机制。研究重点关注使用无监督学习工具描述隐藏表示的语义内容，并利用数据的低维结构。论文介绍了Gride方法用于估计数据内在维度，研究了深度神经网络中隐藏层概率密度的演变，以及探讨了深度神经网络中的泛化问题。&lt;h4&gt;背景&lt;/h4&gt;深度神经网络虽然取得了显著成功，但其内部工作机制和泛化能力仍不完全清楚。理解神经网络如何创建有意义的表示以及它们如何能够泛化到未见数据是深度学习领域的关键挑战。&lt;h4&gt;目的&lt;/h4&gt;提高对深度人工神经网络创建有意义表示和泛化能力的内部机制的理解。重点在于使用无监督学习工具描述隐藏表示的语义内容，并利用数据的低维结构。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了Gride方法，用于估计数据内在维度作为尺度的显式函数，无需降采样数据集。2. 研究了最先进深度神经网络中隐藏层概率密度的演变。3. 研究了深度神经网络中的泛化问题，特别是添加参数如何提高泛化性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. Gride方法基于严格的分布结果，能够量化估计的不确定性，且计算效率高。2. 深度神经网络的初始层生成单模态概率密度，消除与分类无关的结构；后续层中密度峰以分层方式出现，反映概念的语义层次。3. 宽神经网络学习冗余表示而非对虚假相关性过拟合，冗余神经元仅在网络被正则化且训练误差为零时出现。&lt;h4&gt;结论&lt;/h4&gt;深度神经网络通过分层结构创建有意义的表示，初始层消除无关结构，后续层建立语义层次。网络的泛化能力与冗余表示学习相关，而非传统的偏差-方差权衡。Gride方法为分析数据内在结构提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文的目的是提高我们对深度人工神经网络创建有意义表示并能够泛化的内部机制的理解。我们专注于使用无监督学习工具描述隐藏表示的语义内容的挑战，这些工具部分由我们开发并在本论文中描述，它们允许利用数据的低维结构。第2章介绍了Gride，一种允许将数据的内在维度估计为尺度的显式函数的方法，而无需对数据集进行任何降采样。我们的方法基于严格的分布结果，能够量化估计的不确定性。此外，我们的方法简单且计算高效，因为它仅依赖于最近数据点之间的距离。在第3章中，我们研究了一些最先进的深度神经网络中隐藏层概率密度的演变。我们发现初始层生成单模态概率密度，消除任何与分类无关的结构。在后续层中，密度峰以分层方式出现，反映概念的语义层次结构。这个过程在输出层的概率密度中留下了足迹，其中峰的地形可以重建类别的语义关系。在第4章中，我们研究了深度神经网络中的泛化问题：向插值其训练数据的网络添加参数通常会提高其泛化性能，这与经典的偏差-方差权衡相悖。我们证明宽神经网络学习冗余表示，而不是对虚假相关性过拟合，并且只有当网络被正则化且训练误差为零时，冗余神经元才会出现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of this thesis is to improve our understanding of the internalmechanisms by which deep artificial neural networks create meaningfulrepresentations and are able to generalize. We focus on the challenge ofcharacterizing the semantic content of the hidden representations withunsupervised learning tools, partially developed by us and described in thisthesis, which allow harnessing the low-dimensional structure of the data.Chapter 2. introduces Gride, a method that allows estimating the intrinsicdimension of the data as an explicit function of the scale without performingany decimation of the data set. Our approach is based on rigorousdistributional results that enable the quantification of uncertainty of theestimates. Moreover, our method is simple and computationally efficient sinceit relies only on the distances among nearest data points. In Chapter 3, westudy the evolution of the probability density across the hidden layers in somestate-of-the-art deep neural networks. We find that the initial layers generatea unimodal probability density getting rid of any structure irrelevant toclassification. In subsequent layers, density peaks arise in a hierarchicalfashion that mirrors the semantic hierarchy of the concepts. This processleaves a footprint in the probability density of the output layer, where thetopography of the peaks allows reconstructing the semantic relationships of thecategories. In Chapter 4, we study the problem of generalization in deep neuralnetworks: adding parameters to a network that interpolates its training datawill typically improve its generalization performance, at odds with theclassical bias-variance trade-off. We show that wide neural networks learnredundant representations instead of overfitting to spurious correlation andthat redundant neurons appear only if the network is regularized and thetraining error is zero.</description>
      <author>example@mail.com (Diego Doimo)</author>
      <guid isPermaLink="false">2510.21582v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Cost-Sensitive Freeze-thaw Bayesian Optimization for Efficient Hyperparameter Tuning</title>
      <link>http://arxiv.org/abs/2510.21379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于冻结-解冻贝叶斯优化的成本敏感超参数优化方法，通过引入效用函数、新的获取函数和停止准则，实现了在成本和性能之间的动态权衡，并通过迁移学习提高了样本效率。&lt;h4&gt;背景&lt;/h4&gt;研究基于冻结-解冻贝叶斯优化的成本敏感超参数优化问题，关注用户在预期性能改进相对于额外计算成本不够满意时提前停止HPO过程的场景。&lt;h4&gt;目的&lt;/h4&gt;引入描述成本与性能之间权衡的效用函数，结合新的获取函数和停止准则，动态选择最优配置并自动停止HPO过程，同时通过迁移学习提高样本效率。&lt;h4&gt;方法&lt;/h4&gt;提出成本敏感HPO方法，引入效用函数，设计新的获取函数和停止准则，使用迁移学习开发专门的代理模型，提高冻结-解冻方法的样本效率。&lt;h4&gt;主要发现&lt;/h4&gt;在多保真度HPO基准测试上验证了算法性能，优于所有考虑的冻结-解冻BO和迁移-BO基线方法，实现了成本和性能之间显著更好的权衡。&lt;h4&gt;结论&lt;/h4&gt;所提方法在成本敏感HPO问题上表现出色，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们解决了基于冻结-解冻贝叶斯优化（BO）的成本敏感超参数优化（HPO）问题。具体而言，我们假设一种场景，即当预期性能改进相对于额外计算成本不够令人满意时，用户希望提前停止HPO过程。受此场景启发，我们在冻结-解冻框架中引入了'效用'，这是一个描述成本与性能之间权衡的函数，可以从用户偏好数据中估计。这个效用函数结合我们新的获取函数和停止准则，使我们能够动态继续训练我们预期未来效用最大化的配置，并在效用最大值附近自动停止HPO过程。此外，我们通过迁移学习改进了现有冻结-解冻方法的样本效率，为成本敏感HPO问题开发了专门的代理模型。我们在既定的多保真度HPO基准上验证了我们的算法，并表明它优于我们考虑的所有先前冻结-解冻BO和迁移-BO基线方法，同时实现了成本和性能之间显著更好的权衡。我们的代码已在https://github.com/db-Lee/CFBO公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we address the problem of \emph{cost-sensitive} hyperparameteroptimization (HPO) built upon freeze-thaw Bayesian optimization (BO).Specifically, we assume a scenario where users want to early-stop the HPOprocess when the expected performance improvement is not satisfactory withrespect to the additional computational cost. Motivated by this scenario, weintroduce \emph{utility} in the freeze-thaw framework, a function describingthe trade-off between the cost and performance that can be estimated from theuser's preference data. This utility function, combined with our novelacquisition function and stopping criterion, allows us to dynamically continuetraining the configuration that we expect to maximally improve the utility inthe future, and also automatically stop the HPO process around the maximumutility. Further, we improve the sample efficiency of existing freeze-thawmethods with transfer learning to develop a specialized surrogate model for thecost-sensitive HPO problem. We validate our algorithm on establishedmulti-fidelity HPO benchmarks and show that it outperforms all the previousfreeze-thaw BO and transfer-BO baselines we consider, while achieving asignificantly better trade-off between the cost and performance. Our code ispublicly available at https://github.com/db-Lee/CFBO.</description>
      <author>example@mail.com (Dong Bok Lee, Aoxuan Silvia Zhang, Byungjoo Kim, Junhyeon Park, Steven Adriaensen, Juho Lee, Sung Ju Hwang, Hae Beom Lee)</author>
      <guid isPermaLink="false">2510.21379v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>$α$-LoRA: Effective Fine-Tuning via Base Model Rescaling</title>
      <link>http://arxiv.org/abs/2510.21345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一类新的用于迁移学习的重参数化方法，旨在提高微调模型的泛化能力，并通过理论分析和实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;微调被证明是使预训练模型在少量数据样本上在新任务上表现更好的有效方法，其中重参数化方法是最广泛使用的方法之一。&lt;h4&gt;目的&lt;/h4&gt;设计一类新的重参数化方法，以增强微调模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一类新的重参数化方法，在高维二分类设置中使用随机矩阵理论工具建立其有效性，并通过微调大型语言模型等实验进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的重参数化方法在高维二分类任务中表现有效，且通过微调LLMs的实验进一步验证了理论发现。&lt;h4&gt;结论&lt;/h4&gt;新提出的重参数化方法能够有效提高微调模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;微调已被证明是使预训练模型在少量数据样本上在新任务上表现更好的有效方法。其中最广泛使用的方法是重参数化方法，它们通过添加一个额外的可训练权重矩阵来更新目标模块的冻结权重矩阵。最突出的例子是低秩适应(LoRA)，近年来受到了广泛关注。在本文中，我们介绍了一类用于迁移学习的新型重参数化方法，旨在提高微调模型的泛化能力。我们使用随机矩阵理论工具在高维二分类设置中建立了该方法的有效性，并通过更真实的实验（如微调大型语言模型）进一步验证了我们的理论发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning has proven to be highly effective in adapting pre-trained modelsto perform better on new desired tasks with minimal data samples. Among themost widely used approaches are reparameterization methods, which update atarget module by augmenting its frozen weight matrix with an additionaltrainable weight matrix. The most prominent example is Low Rank Adaption(LoRA), which gained significant attention in recent years. In this paper, weintroduce a new class of reparameterization methods for transfer learning,designed to enhance the generalization ability of fine-tuned models. Weestablish the effectiveness of our approach in a high-dimensional binaryclassification setting using tools from Random Matrix Theory, and furthervalidate our theoretical findings through more realistic experiments, such asfine-tuning LLMs.</description>
      <author>example@mail.com (Aymane El Firdoussi, El Mahdi Chayti, Mohamed El Amine Seddik, Martin Jaggi)</author>
      <guid isPermaLink="false">2510.21345v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Graph Mixture of Residual Experts: Unsupervised Learning on Diverse Graphs with Heterogeneous Specialization</title>
      <link>http://arxiv.org/abs/2510.21207v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ADaMoRE框架，解决了图神经网络在适应多样化图结构方面的挑战，通过无监督训练实现了异构专家的有效组合，在各种任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;图神经网络面临基本适应性挑战：固定的消息传递架构难以应对现实世界图的巨大多样性，最优计算策略因局部结构和任务而异。现有图专家混合方法依赖监督信号且训练异构专家时存在不稳定性。&lt;h4&gt;目的&lt;/h4&gt;引入ADaMoRE框架，实现在图上进行异构专家混合的稳健、完全无监督训练。&lt;h4&gt;方法&lt;/h4&gt;ADaMoRE采用骨干-残差专家架构，基础编码器提供稳定性，残差专家捕获不同计算模式；结构感知门控网络执行细粒度节点路由；通过统一无监督目标进行端到端训练，结合重建任务和信息论多样性正则化器强制专家功能专业化。&lt;h4&gt;主要发现&lt;/h4&gt;在16个基准测试上验证了ADaMoRE在无监督节点分类和少样本学习方面的最先进性能，以及优越的泛化能力、训练效率和更快收敛速度。&lt;h4&gt;结论&lt;/h4&gt;ADaMoRE框架通过无监督训练有效解决了图神经网络适应性问题，在多样化图和任务上展现出卓越性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)面临一个基本的适应性挑战：它们固定的消息传递架构难以应对现实世界图的巨大多样性，而最优的计算策略因局部结构和任务的不同而异。尽管专家混合(MoE)为适应性提供了一条有前景的路径，但现有的图MoE方法仍然依赖于监督信号，并且在训练异构专家时存在不稳定性。我们引入ADaMoRE(Adaptive Mixture of Residual Experts)，这是一个原则性框架，能够在图上实现异构MoE的稳健、完全无监督训练。ADaMoRE采用骨干-残差专家架构，其中基础编码器提供稳定性，而专门的残差专家捕获不同的计算模式。一个结构感知的门控网络执行细粒度的节点路由。整个架构通过统一的无监督目标进行端到端训练，该目标结合了主要的重建任务和信息论多样性正则化器，以明确强制专家之间的功能专业化。理论分析证实了他们的设计提高了数据效率和训练稳定性。在16个基准测试上的广泛评估验证了ADaMoRE在无监督节点分类和少样本学习方面的最先进性能，以及在多样化图和任务上的优越泛化能力、训练效率和更快收敛速度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) face a fundamental adaptability challenge: theirfixed message-passing architectures struggle with the immense diversity ofreal-world graphs, where optimal computational strategies vary by localstructure and task. While Mixture-of-Experts (MoE) offers a promising pathwayto adaptability, existing graph MoE methods remain constrained by theirreliance on supervised signals and instability when training heterogeneousexperts. We introduce ADaMoRE (Adaptive Mixture of Residual Experts), aprincipled framework that enables robust, fully unsupervised training ofheterogeneous MoE on graphs. ADaMoRE employs a backbone-residual expertarchitecture where foundational encoders provide stability while specializedresidual experts capture diverse computational patterns. A structurally-awaregating network performs fine-grained node routing. The entire architecture istrained end-to-end using a unified unsupervised objective, which integrates aprimary reconstruction task with an information-theoretic diversity regularizerto explicitly enforce functional specialization among the experts. Theoreticalanalysis confirms our design improves data efficiency and training stability.Extensive evaluation across 16 benchmarks validates ADaMoRE's state-of-the-artperformance in unsupervised node classification and few-shot learning,alongside superior generalization, training efficiency, and faster convergenceon diverse graphs and tasks.</description>
      <author>example@mail.com (Yunlong Chu, Minglai Shao, Zengyi Wo, Bing Hao, Yuhang Liu, Ruijie Wang, Jianxin Li)</author>
      <guid isPermaLink="false">2510.21207v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>CIPHER: Scalable Time Series Analysis for Physical Sciences with Application to Solar Wind Phenomena</title>
      <link>http://arxiv.org/abs/2510.21022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 2 figures, Machine Learning and the Physical Sciences  Workshop @ NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为CIPHER的框架，用于加速物理学中复杂时间序列的大规模标注。该框架结合了可索引符号聚合近似、基于密度的聚类和人类专家验证，解决了物理科学中时间序列标注稀缺、成本高且不一致的问题。&lt;h4&gt;背景&lt;/h4&gt;在物理科学中，时间序列的标注或分类是一个持续的挑战。专家标注稀缺、成本高且往往不一致，但稳健的标注对于启用机器学习模型进行理解、预测和预测至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计一个框架来加速物理学中复杂时间序列的大规模标注，解决专家标注稀缺的问题。&lt;h4&gt;方法&lt;/h4&gt;CIPHER框架集成了以下组件：1. 可索引符号聚合近似用于可解释的压缩和索引；2. 基于密度的聚类来分组重复出现的现象；3. 人类在环中的步骤用于高效的专家验证。领域科学家对代表性样本进行标注，然后将这些标注传播到整个集群中，产生系统化的、可扩展的分类。&lt;h4&gt;主要发现&lt;/h4&gt;作者在OMNI数据中分类太阳风现象的任务上评估了CIPHER，这是空间天气研究中的一个核心挑战。结果表明，该框架能够识别有意义的现象，如日冕物质抛射和流相互作用区域。&lt;h4&gt;结论&lt;/h4&gt;CIPHER展示了一种结合符号表示、无监督学习和专业知识的通用策略，以解决物理科学中时间序列标注稀缺的问题。研究所用的代码和配置文件是公开的，以支持可重复性。&lt;h4&gt;翻译&lt;/h4&gt;时间序列的标注或分类在物理科学中是一个持续的挑战，其中专家标注稀缺、成本高昂且往往不一致。然而，稳健的标注对于启用机器学习模型进行理解、预测和预测至关重要。我们提出了'聚类与索引管道及人类评估用于识别'，这是一个旨在加速物理学中复杂时间序列大规模标注的框架。CIPHER集成了可索引符号聚合近似用于可解释的压缩和索引，基于密度的聚类来分组重复出现的现象，以及一个人机交互的步骤用于高效的专家验证。代表性样本由领域科学家标注，这些标注被传播到整个集群中，产生系统化、可扩展的分类。我们在OMNI数据中分类太阳风现象的任务上评估了CIPHER，这是空间天气研究中的一个核心挑战，结果表明该框架能够识别有意义的现象，如日冕物质抛射和流相互作用区域。除了这个案例研究，CIPHER强调了一种结合符号表示、无监督学习和专业知识的通用策略，以解决物理科学中时间序列的标注稀缺问题。本研究使用的代码和配置文件是公开的，以支持可重复性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Labeling or classifying time series is a persistent challenge in the physicalsciences, where expert annotations are scarce, costly, and often inconsistent.Yet robust labeling is essential to enable machine learning models forunderstanding, prediction, and forecasting. We present the \textit{Clusteringand Indexation Pipeline with Human Evaluation for Recognition} (CIPHER), aframework designed to accelerate large-scale labeling of complex time series inphysics. CIPHER integrates \textit{indexable Symbolic Aggregate approXimation}(iSAX) for interpretable compression and indexing, density-based clustering(HDBSCAN) to group recurring phenomena, and a human-in-the-loop step forefficient expert validation. Representative samples are labeled by domainscientists, and these annotations are propagated across clusters to yieldsystematic, scalable classifications. We evaluate CIPHER on the task ofclassifying solar wind phenomena in OMNI data, a central challenge in spaceweather research, showing that the framework recovers meaningful phenomena suchas coronal mass ejections and stream interaction regions. Beyond this casestudy, CIPHER highlights a general strategy for combining symbolicrepresentations, unsupervised learning, and expert knowledge to address labelscarcity in time series across the physical sciences. The code andconfiguration files used in this study are publicly available to supportreproducibility.</description>
      <author>example@mail.com (Jasmine R. Kobayashi, Daniela Martin, Valmir P Moraes Filho, Connor O'Brien, Jinsu Hong, Sudeshna Boro Saikia, Hala Lamdouar, Nathan D. Miles, Marcella Scoczynski, Mavis Stone, Sairam Sundaresan, Anna Jungbluth, Andrés Muñoz-Jaramillo, Evangelia Samara, Joseph Gallego)</author>
      <guid isPermaLink="false">2510.21022v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Memory Constrained Dynamic Subnetwork Update for Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.20979v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为MeDyate的框架，用于解决在设备上神经网络训练面临的内存限制问题，通过动态子网络适应方法实现了在严格内存预算下的有效微调。&lt;h4&gt;背景&lt;/h4&gt;在设备上的神经网络训练面临严重的内存限制，这些限制阻碍了预训练模型对下游任务的适应。&lt;h4&gt;目的&lt;/h4&gt;提出一个有理论依据的框架，用于内存受限的动态子网络适应，实现在严格内存预算下的有效微调。&lt;h4&gt;方法&lt;/h4&gt;MeDyate框架包含两个关键创新：LaRa（Layer Ranking）作为改进的层重要性度量实现有原则的层预选择，以及动态通道采样策略利用微调过程中通道重要性分布的时间稳定性；根据重要性加权概率在周期之间动态重新采样通道，确保在尊重内存预算的同时全面探索参数空间。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛的任务和架构上进行的大量评估表明，MeDyate在极端内存限制下实现了最先进的性能，一致优于现有的静态和动态方法，同时保持高计算效率。&lt;h4&gt;结论&lt;/h4&gt;该方法代表了推动设备上高效学习的重要一步，证明了在内存预算低至几百KB RAM的情况下进行有效微调的可能性。&lt;h4&gt;翻译&lt;/h4&gt;设备上的神经网络训练面临关键的内存限制，这些限制阻碍了预训练模型对下游任务的适应。我们提出了MeDyate，一个有理论依据的框架，用于内存受限的动态子网络适应。我们的方法引入了两个关键创新：LaRa（Layer Ranking），一种改进的层重要性度量，能够实现有原则的层预选择，以及动态通道采样策略，利用微调过程中通道重要性分布的时间稳定性。MeDyate根据重要性加权概率在周期之间动态重新采样通道，确保在尊重严格内存预算的同时全面探索参数空间。在广泛的任务和架构上进行的大量评估表明，MeDyate在极端内存限制下实现了最先进的性能，一致优于现有的静态和动态方法，同时保持高计算效率。我们的方法代表了推动设备上高效学习的重要一步，证明了在内存预算低至几百KB RAM的情况下进行有效微调的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; On-device neural network training faces critical memory constraints thatlimit the adaptation of pre-trained models to downstream tasks. We presentMeDyate, a theoretically-grounded framework for memory-constrained dynamicsubnetwork adaptation. Our approach introduces two key innovations: LaRa (LayerRanking), an improved layer importance metric that enables principled layerpre-selection, and a dynamic channel sampling strategy that exploits thetemporal stability of channel importance distributions during fine-tuning.MeDyate dynamically resamples channels between epochs according toimportance-weighted probabilities, ensuring comprehensive parameter spaceexploration while respecting strict memory budgets. Extensive evaluation acrossa large panel of tasks and architectures demonstrates that MeDyate achievesstate-of-the-art performance under extreme memory constraints, consistentlyoutperforming existing static and dynamic approaches while maintaining highcomputational efficiency. Our method represents a significant step towardsenabling efficient on-device learning by demonstrating effective fine-tuningwith memory budgets as low as a few hundred kB of RAM.</description>
      <author>example@mail.com (Aël Quélennec, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2510.20979v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>AG-Fusion: adaptive gated multimodal fusion for 3d object detection in complex scenes</title>
      <link>http://arxiv.org/abs/2510.23151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种自适应门控融合方法，通过选择性整合跨模态知识，在复杂场景中实现了更鲁棒的3D目标检测。&lt;h4&gt;背景&lt;/h4&gt;多模态相机-激光雷达融合技术在3D目标检测中应用广泛，但在传感器退化或环境干扰等具有挑战性的场景中，现有方法性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自适应门控融合方法，通过识别可靠的模式来选择性地整合跨模态知识，以实现复杂场景中的鲁棒检测。&lt;h4&gt;方法&lt;/h4&gt;将每个模态的特征投影到统一的BEV空间并使用基于窗口的注意力机制增强特征，然后设计基于跨模态注意力的自适应门控融合模块来整合这些特征，同时构建了一个名为Excavator3D（E3D）的新数据集，专注于具有挑战性的挖掘机操作场景。&lt;h4&gt;主要发现&lt;/h4&gt;在标准的KITTI数据集上达到93.92%的准确率，在具有挑战性的E3D数据集上比基线方法高出24.88%，证明了在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的AG-Fusion方法在复杂场景中表现优异，特别是在处理不可靠模态信息时具有更强的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;多模态相机-激光雷达融合技术在3D目标检测中已得到广泛应用，并显示出令人鼓舞的性能表现。然而，在传感器退化或环境干扰等具有挑战性的场景中，现有方法表现出显著的性能下降。我们提出了一种新颖的自适应门控融合方法，通过识别可靠的模式来选择性地整合跨模态知识，从而在复杂场景中实现鲁棒检测。具体而言，我们首先将每个模态的特征投影到统一的BEV空间，并使用基于窗口的注意力机制增强这些特征。随后，我们设计了一个基于跨模态注意力的自适应门控融合模块，将这些特征整合为可靠的BEV表示，以应对具有挑战性的环境。此外，我们构建了一个名为Excavator3D（E3D）的新数据集，专注于具有挑战性的挖掘机操作场景，以在复杂条件下评估性能。我们的方法不仅在标准的KITTI数据集上实现了93.92%的准确率，具有竞争力的性能，而且在具有挑战性的E3D数据集上比基线方法高出24.88%，证明了在复杂工业场景中对不可靠模态信息具有优越的鲁棒性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决多模态相机-激光雷达融合技术在复杂场景（如挖掘机操作环境）中性能显著下降的问题。这些问题在现实中很重要，因为灰尘、光照变化导致图像退化，机械部件遮挡和金属表面反射干扰点云数据，这些挑战限制了自动驾驶和工业自动化技术在真实世界中的应用，现有方法在标准数据集上表现良好但在复杂场景中性能大幅下降。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有BEV融合技术的局限性，指出它们主要依赖卷积操作，无法自适应建模跨模态交互。针对工业场景中的挑战，作者在BEVFusion基础上进行改进，借鉴了Swin Transformer的窗口自注意力机制设计SA-E模块增强特征，并引入双向交叉注意力和自适应门控机制实现更智能的融合。整体设计思路是根据场景特点动态调整不同模态的贡献权重。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过自适应选择和融合来自相机和激光雷达的可靠信息，提高在复杂场景中的3D目标检测性能。方法首先使用窗口自注意力增强每个模态的特征；然后通过双向交叉注意力和自适应门控机制融合这些特征，根据场景特点动态调整不同模态的贡献权重；最后将所有特征流集成到统一的BEV表示中进行3D检测。整体流程包括特征提取、增强特征提取、跨模态门控融合和多级特征聚合四个主要步骤。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 自适应门控多模态融合(AG-Fusion)框架，结合双向交叉注意力和空间自适应门控机制；2) 基于窗口的自注意力增强(SA-E)模块，有效降低计算复杂度；3) 构建了专门的挖掘机3D检测数据集(E3D)。相比之前的工作，该方法不再依赖静态或局部约束的特征聚合，能够处理遮挡和传感器噪声，在复杂工业场景中表现显著优于现有方法，在E3D数据集上比基线方法提升24.88%的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种自适应门控多模态融合方法，通过动态整合相机和激光雷达的可靠信息，显著提升了在复杂工业场景中的3D目标检测性能，并构建了专门的挖掘机3D检测数据集验证方法的有效性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal camera-LiDAR fusion technology has found extensive application in3D object detection, demonstrating encouraging performance. However, existingmethods exhibit significant performance degradation in challenging scenarioscharacterized by sensor degradation or environmental disturbances. We propose anovel Adaptive Gated Fusion (AG-Fusion) approach that selectively integratescross-modal knowledge by identifying reliable patterns for robust detection incomplex scenes. Specifically, we first project features from each modality intoa unified BEV space and enhance them using a window-based attention mechanism.Subsequently, an adaptive gated fusion module based on cross-modal attention isdesigned to integrate these features into reliable BEV representations robustto challenging environments. Furthermore, we construct a new dataset namedExcavator3D (E3D) focusing on challenging excavator operation scenarios tobenchmark performance in complex conditions. Our method not only achievescompetitive performance on the standard KITTI dataset with 93.92% accuracy, butalso significantly outperforms the baseline by 24.88% on the challenging E3Ddataset, demonstrating superior robustness to unreliable modal information incomplex industrial scenes.</description>
      <author>example@mail.com (Sixian Liu, Chen Xu, Qiang Wang, Donghai Shi, Yiwen Li)</author>
      <guid isPermaLink="false">2510.23151v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>DQ3D: Depth-guided Query for Transformer-Based 3D Object Detection in Traffic Scenarios</title>
      <link>http://arxiv.org/abs/2510.23144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个名为DQ3D的深度引导查询生成器，用于解决3D目标检测中的参考点采样问题，并通过混合注意力机制处理部分遮挡目标，在nuScenes数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;多视角图像中的3D目标检测在交通场景中近年来受到广泛关注。现有方法依赖于从3D参考点生成的目标查询来定位物体。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法中参考点可能远离目标物体导致误检的问题，并处理当前帧中部分遮挡的目标物体。&lt;h4&gt;方法&lt;/h4&gt;提出了深度引导查询生成器(DQ3D)，利用深度信息和2D检测确保参考点从物体表面或内部采样；引入混合注意力机制，将历史检测结果与深度引导查询融合，形成混合查询。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上的评估表明，该方法在平均精度(mAP)上比基线提高了6.3%，在NuScenes检测分数(NDS)上提高了4.3%。&lt;h4&gt;结论&lt;/h4&gt;深度引导查询生成器和混合注意力机制能有效提高3D目标检测的性能，特别是在处理参考点采样和遮挡物体方面。&lt;h4&gt;翻译&lt;/h4&gt;近年来，交通场景中基于多视角图像的3D目标检测受到了广泛关注。许多现有方法依赖于从3D参考点生成的目标查询来定位物体。然而，这些方法的一个局限性是，一些参考点通常远离目标物体，这可能导致误检。在本文中，我们提出了一个用于3D目标检测的深度引导查询生成器(DQ3D)，它利用深度信息和2D检测确保参考点从物体表面或内部采样。此外，为了解决当前帧中部分遮挡的物体，我们引入了一种混合注意力机制，将历史检测结果与深度引导查询融合，从而形成混合查询。在nuScenes数据集上的评估表明，我们的方法在平均精度(mAP)上比基线提高了6.3%，在NuScenes检测分数(NDS)上提高了4.3%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection from multi-view images in traffic scenarios has garneredsignificant attention in recent years. Many existing approaches rely on objectqueries that are generated from 3D reference points to localize objects.However, a limitation of these methods is that some reference points are oftenfar from the target object, which can lead to false positive detections. Inthis paper, we propose a depth-guided query generator for 3D object detection(DQ3D) that leverages depth information and 2D detections to ensure thatreference points are sampled from the surface or interior of the object.Furthermore, to address partially occluded objects in current frame, weintroduce a hybrid attention mechanism that fuses historical detection resultswith depth-guided queries, thereby forming hybrid queries. Evaluation on thenuScenes dataset demonstrates that our method outperforms the baseline by 6.3\%in terms of mean Average Precision (mAP) and 4.3\% in the NuScenes DetectionScore (NDS).</description>
      <author>example@mail.com (Ziyu Wang, Wenhao Li, Ji Wu)</author>
      <guid isPermaLink="false">2510.23144v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>RLGF: Reinforcement Learning with Geometric Feedback for Autonomous Driving Video Generation</title>
      <link>http://arxiv.org/abs/2509.16500v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究解决了自动驾驶系统中合成视频数据的几何失真问题，提出了强化学习与几何反馈(RLGF)方法，显著提高了合成数据的几何准确性和3D目标检测性能。&lt;h4&gt;背景&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉上逼真，却存在微妙的几何失真，限制了其在下游感知任务中的应用。研究显示，使用合成数据与真实数据进行3D目标检测时存在显著性能差距。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来减少合成视频数据中的几何失真，提高其在自动驾驶感知任务中的效用，缩小合成数据与真实数据之间的性能差距。&lt;h4&gt;方法&lt;/h4&gt;研究引入了'带有几何反馈的强化学习'(RLGF)，该方法通过整合来自专用潜在空间自动驾驶感知模型的奖励来优化视频扩散模型。其核心组件包括：1) 潜在空间窗口优化技术，用于在扩散过程中提供针对性反馈；2) 分层几何奖励(HGR)系统，为点线面对齐和场景占用一致性提供多级奖励。研究还提出了GeoScores来量化几何失真。&lt;h4&gt;主要发现&lt;/h4&gt;应用RLGF到DiVE模型上，在nuScenes数据集上显著减少了几何误差（例如：消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测mAP达12.7%，缩小了与真实数据性能的差距。&lt;h4&gt;结论&lt;/h4&gt;RLGF为自动驾驶开发提供了一种即插即用的解决方案，能够生成几何准确可靠的合成视频，有助于推进自动驾驶系统的训练和测试。&lt;h4&gt;翻译&lt;/h4&gt;合成数据对推进自动驾驶系统至关重要，但当前最先进的视频生成模型尽管视觉上逼真，却存在微妙的几何失真，限制了其在下游感知任务中的应用。研究确定了并量化了这一关键问题，展示了使用合成数据与真实数据进行3D目标检测时的显著性能差距。为此，研究引入了'带有几何反馈的强化学习'(RLGF)。RLGF通过整合来自专用潜在空间自动驾驶感知模型的奖励，独特地优化了视频扩散模型。其核心组件包括：1) 用于在扩散过程中提供针对性反馈的高效潜在空间窗口优化技术；2) 提供点线面对齐和场景占用一致性多级奖励的分层几何奖励(HGR)系统。为了量化这些失真，研究提出了GeoScores。将RLGF应用于nuScenes上的DiVE等模型，显著减少了几何误差（例如：消失点误差降低21%，深度误差降低57%），并大幅提高了3D目标检测mAP达12.7%，缩小了与真实数据性能的差距。RLGF为生成几何准确可靠的合成视频用于自动驾驶开发提供了一种即插即用的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶视频生成中存在的几何失真问题。虽然当前视频生成模型在视觉上看起来很真实，但它们生成的视频中存在微妙的几何扭曲，这些扭曲限制了它们在下游感知任务中的实用性。这个问题很重要，因为自动驾驶系统需要大量高质量的合成数据进行训练和验证，而这些几何扭曲会导致基于合成数据训练的3D目标检测等下游任务性能显著下降，影响自动驾驶系统的可靠性和安全性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别并量化了当前视频生成模型中的几何失真问题，提出了GeoScores指标来评估这些失真。通过实验发现当前模型虽然保留了2D外观但无法捕捉准确的3D场景结构，主要原因是潜在的几何不一致性。作者借鉴了强化学习从人类反馈的成功经验（如LLMs中的PPO或DPO）、视频扩散模型的研究以及自动驾驶感知模型的设计。但作者指出现有方法主要依赖像素级对齐，无法显式强制遵守复杂的底层几何原理，因此设计了RLGF框架，利用专门的预训练自动驾驶感知模型作为奖励提供者，确保几何保真度。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过引入基于感知模型的几何反馈来增强视频扩散模型的几何完整性，使生成的自动驾驶场景视频不仅在视觉上逼真，而且在几何结构上准确可靠。整体流程包括：1)预训练两个专门的感知模型（潜在几何感知模型Pgeo和潜在占用预测模型Pocc）；2)设计分层几何奖励（HGR）系统，提供点线面几何反馈和场景级占用反馈；3)实现潜在空间窗口化优化，在扩散过程的中间步骤提供反馈；4)使用强化学习微调预训练的视频扩散模型，将感知模型作为奖励提供者，通过LoRA高效更新模型参数。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次系统量化了自动驾驶视频生成中的几何失真问题，提出GeoScores评估指标；2)引入强化学习与几何反馈（RLGF）框架，将感知模型驱动的几何空间约束直接注入视频生成过程；3)提出潜在空间窗口化优化技术，在扩散过程的中间步骤而非仅最终输出提供反馈；4)设计分层几何奖励（HGR）系统，结合点线面几何反馈和场景级占用反馈。相比之前的工作，RLGF专注于几何完整性而非仅像素级视觉保真度，使用具体的、可解释的几何约束而非人类偏好或高层次奖励信号，并且是即插即用的解决方案，可与现有视频扩散模型集成。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了RLGF框架，通过引入基于感知模型的几何反馈强化学习，有效解决了自动驾驶视频生成中的几何失真问题，显著提升了合成数据的3D感知任务性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-09-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Synthetic data is crucial for advancing autonomous driving (AD) systems, yetcurrent state-of-the-art video generation models, despite their visual realism,suffer from subtle geometric distortions that limit their utility fordownstream perception tasks. We identify and quantify this critical issue,demonstrating a significant performance gap in 3D object detection when usingsynthetic versus real data. To address this, we introduce ReinforcementLearning with Geometric Feedback (RLGF), RLGF uniquely refines video diffusionmodels by incorporating rewards from specialized latent-space AD perceptionmodels. Its core components include an efficient Latent-Space WindowingOptimization technique for targeted feedback during diffusion, and aHierarchical Geometric Reward (HGR) system providing multi-level rewards forpoint-line-plane alignment, and scene occupancy coherence. To quantify thesedistortions, we propose GeoScores. Applied to models like DiVE on nuScenes,RLGF substantially reduces geometric errors (e.g., VP error by 21\%, Deptherror by 57\%) and dramatically improves 3D object detection mAP by 12.7\%,narrowing the gap to real-data performance. RLGF offers a plug-and-playsolution for generating geometrically sound and reliable synthetic videos forAD development.</description>
      <author>example@mail.com (Tianyi Yan, Wencheng Han, Xia Zhou, Xueyang Zhang, Kun Zhan, Cheng-zhong Xu, Jianbing Shen)</author>
      <guid isPermaLink="false">2509.16500v2</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>When No Paths Lead to Rome: Benchmarking Systematic Neural Relational Reasoning</title>
      <link>http://arxiv.org/abs/2510.23532v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted at NeurIPS 2025 D&amp;B track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了NoRA新基准测试，用于评估系统关系推理模型的泛化能力，突破了现有基于路径组合的简化假设。&lt;h4&gt;背景&lt;/h4&gt;设计能够系统化学习的模型是重要挑战，近年已提出多种解决方案，包括神经符号方法、Transformer变体和图神经网络，但现有基准测试过于简化。&lt;h4&gt;目的&lt;/h4&gt;支持神经网络在系统关系推理领域的进一步发展，引入更全面的评估基准。&lt;h4&gt;方法&lt;/h4&gt;开发NoRA基准测试，增加多个难度级别，要求模型超越简单的路径组合推理。&lt;h4&gt;主要发现&lt;/h4&gt;现有基准测试基于推理可简化为关系路径组合的假设，导致模型在这些基准上表现良好但难以泛化到其他场景。&lt;h4&gt;结论&lt;/h4&gt;需要NoRA这样的新基准来更全面地评估模型的真实系统推理能力，推动领域发展。&lt;h4&gt;翻译&lt;/h4&gt;设计能够以系统化方式学习的模型是一个重要且长期存在的挑战。近年来，针对系统关系推理的特定案例提出了多种解决方案，包括神经符号方法、Transformer架构的变体和专门的图神经网络。然而，现有的系统关系推理基准测试基于过于简化的设置，基于推理可以简化为组合关系路径的假设。事实上，这个假设被嵌入到几个最新模型的架构中，导致这些方法在现有基准上表现良好但难以推广到其他设置。为了支持神经网络在系统关系推理领域的进一步发展，我们引入NoRA，一个新的基准测试，它增加了多个难度级别，要求模型超越基于路径的推理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing models that can learn to reason in a systematic way is an importantand long-standing challenge. In recent years, a wide range of solutions havebeen proposed for the specific case of systematic relational reasoning,including Neuro-Symbolic approaches, variants of the Transformer architecture,and specialised Graph Neural Networks. However, existing benchmarks forsystematic relational reasoning focus on an overly simplified setting, based onthe assumption that reasoning can be reduced to composing relational paths. Infact, this assumption is hard-baked into the architecture of several recentmodels, leading to approaches that can perform well on existing benchmarks butare difficult to generalise to other settings. To support further progress inthe field of systematic relational reasoning with neural networks, we introduceNoRA, a new benchmark which adds several levels of difficulty and requiresmodels to go beyond path-based reasoning.</description>
      <author>example@mail.com (Anirban Das, Irtaza Khalid, Rafael Peñaloza, Steven Schockaert)</author>
      <guid isPermaLink="false">2510.23532v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>iPac: Incorporating Intra-image Patch Context into Graph Neural Networks for Medical Image Classification</title>
      <link>http://arxiv.org/abs/2510.23504v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in the proceedings of ICONIP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;iPac是一种创新的图神经网络方法，通过引入图像的新型图表示来改进图像分类性能，特别在医学图像分类中表现出色，比基线方法平均提高5%的准确率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为图像处理的一种有前景的范式，但在图像分类任务中的表现受到限制，因为它们对视觉实体之间的底层结构和关系考虑不足。&lt;h4&gt;目的&lt;/h4&gt;提出iPac方法，引入图像的新型图表示，增强图神经网络在医学图像分类中的性能，通过认识到底层结构和关系在医学图像分类中的重要性。&lt;h4&gt;方法&lt;/h4&gt;iPac集成了多个阶段，包括patch分区、特征提取、聚类、图构建和基于图的学习，将这些阶段整合到一个统一的网络中，通过捕获相关特征并将它们组织成簇，构建有意义的图表示，有效封装图像的语义。&lt;h4&gt;主要发现&lt;/h4&gt;在多种医学图像数据集上的实验评估证明了iPac的有效性，与基线方法相比，平均准确率提高了高达5%。&lt;h4&gt;结论&lt;/h4&gt;该方法为图像分类提供了一种通用且灵活的解决方案，特别是在医学图像领域，通过利用图表示并考虑视觉实体之间的固有结构和关系。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为图像处理的一种有前景的范式，但它们在图像分类任务中的表现受到对视觉实体之间底层结构和关系考虑不足的限制。本文提出了iPac，一种通过引入图像的新型图表示来增强图神经网络图像分类的新方法，通过认识到底层结构和关系在医学图像分类中的重要性。iPac将多个阶段（包括patch分区、特征提取、聚类、图构建和基于图的学习）整合到一个统一的网络中，以推进图神经网络图像分类。通过捕获相关特征并将它们组织成簇，我们构建了一个有意义的图表示，有效地封装了图像的语义。在多种医学图像数据集上的实验评估证明了iPac的有效性，与基线方法相比，平均准确率提高了高达5%。我们的方法通过利用图表示并考虑视觉实体之间的固有结构和关系，为图像分类提供了一种通用且灵活的解决方案，特别是在医学图像领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks have emerged as a promising paradigm for imageprocessing, yet their performance in image classification tasks is hindered bya limited consideration of the underlying structure and relationships amongvisual entities. This work presents iPac, a novel approach to introduce a newgraph representation of images to enhance graph neural network imageclassification by recognizing the importance of underlying structure andrelationships in medical image classification. iPac integrates various stages,including patch partitioning, feature extraction, clustering, graphconstruction, and graph-based learning, into a unified network to advance graphneural network image classification. By capturing relevant features andorganising them into clusters, we construct a meaningful graph representationthat effectively encapsulates the semantics of the image. Experimentalevaluation on diverse medical image datasets demonstrates the efficacy of iPac,exhibiting an average accuracy improvement of up to 5% over baseline methods.Our approach offers a versatile and generic solution for image classification,particularly in the realm of medical images, by leveraging the graphrepresentation and accounting for the inherent structure and relationshipsamong visual entities.</description>
      <author>example@mail.com (Usama Zidan, Mohamed Gaber, Mohammed M. Abdelsamea)</author>
      <guid isPermaLink="false">2510.23504v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Dual Prompting: Hierarchical Debiasing for Fairness-aware Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.23469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，通过在无标签图数据上进行自监督学习来预训练图神经网络已成为图学习中的广泛采用范式。虽然这种范式对预训练强大的GNN模型有效，但预训练和下游任务之间通常存在目标差距。图提示通过额外的可学习提示来调整预训练的GNN模型以适应特定下游任务，同时保持预训练的GNN模型冻结。&lt;h4&gt;目的&lt;/h4&gt;解决现有图提示方法在设计提示时往往忽视公平性的问题。预训练的GNN模型会在不同人口统计子群中产生有区别性的节点表示，因为下游图数据在节点属性和图结构中固有地包含偏见。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应双提示(ADPrompt)框架，用于增强预训练GNN模型适应下游任务的公平性。设计了自适应特征校正模块，学习自定义的属性提示，在输入层抑制敏感信息，从源头减少偏见。提出了自适应消息校准模块，在每一层生成结构提示，调整来自邻居节点的消息，实现信息流的动态和软校准。联合优化两个提示模块，以适应预训练的GNN同时增强公平性。&lt;h4&gt;主要发现&lt;/h4&gt;在四个数据集上使用四种预训练策略进行了广泛的实验来评估ADPrompt的性能。结果表明，ADPrompt在节点分类任务上优于七种基线方法。&lt;h4&gt;结论&lt;/h4&gt;ADPrompt框架能够有效提升预训练GNN模型在下游任务中的公平性和性能。&lt;h4&gt;翻译&lt;/h4&gt;近年来，通过在无标签图数据上进行自监督学习来预训练图神经网络已成为图学习中的广泛采用范式。虽然这种范式对预训练强大的GNN模型有效，但预训练和下游任务之间通常存在目标差距。为了弥合这一差距，图提示通过额外的可学习提示来调整预训练的GNN模型以适应特定的下游任务，同时保持预训练的GNN模型冻结。由于最近的图提示方法主要关注增强模型在下游任务上的效用，它们在设计提示进行适应时往往忽视了公平性问题。实际上，预训练的GNN模型会在不同人口统计子群中产生有区别性的节点表示，因为下游图数据在节点属性和图结构中固有地包含偏见。为了解决这个问题，我们提出了一个自适应双提示(ADPrompt)框架，用于增强预训练GNN模型适应下游任务的公平性。为了减轻属性偏见，我们设计了一个自适应特征校正模块，学习自定义的属性提示，在输入层抑制敏感信息，从源头减少偏见。之后，我们提出了一个自适应消息校准模块，在每一层生成结构提示，调整来自邻居节点的消息，实现信息流的动态和软校准。最后，ADPrompt联合优化两个提示模块，以适应预训练的GNN同时增强公平性。我们在四个数据集上使用四种预训练策略进行了广泛的实验来评估ADPrompt的性能。结果表明，我们提出的ADPrompt在节点分类任务上优于七种基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, pre-training Graph Neural Networks (GNNs) throughself-supervised learning on unlabeled graph data has emerged as a widelyadopted paradigm in graph learning. Although the paradigm is effective forpre-training powerful GNN models, the objective gap often exists betweenpre-training and downstream tasks. To bridge this gap, graph prompting adaptspre-trained GNN models to specific downstream tasks with extra learnableprompts while keeping the pre-trained GNN models frozen. As recent graphprompting methods largely focus on enhancing model utility on downstream tasks,they often overlook fairness concerns when designing prompts for adaptation. Infact, pre-trained GNN models will produce discriminative node representationsacross demographic subgroups, as downstream graph data inherently containsbiases in both node attributes and graph structures. To address this issue, wepropose an Adaptive Dual Prompting (ADPrompt) framework that enhances fairnessfor adapting pre-trained GNN models to downstream tasks. To mitigate attributebias, we design an Adaptive Feature Rectification module that learns customizedattribute prompts to suppress sensitive information at the input layer,reducing bias at the source. Afterward, we propose an Adaptive MessageCalibration module that generates structure prompts at each layer, which adjustthe message from neighboring nodes to enable dynamic and soft calibration ofthe information flow. Finally, ADPrompt jointly optimizes the two promptingmodules to adapt the pre-trained GNN while enhancing fairness. We conductextensive experiments on four datasets with four pre-training strategies toevaluate the performance of ADPrompt. The results demonstrate that our proposedADPrompt outperforms seven baseline methods on node classification tasks.</description>
      <author>example@mail.com (Yuhan Yang, Xingbo Fu, Jundong Li)</author>
      <guid isPermaLink="false">2510.23469v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Improving Predictions of Molecular Properties with Graph Featurisation and Heterogeneous Ensemble Models</title>
      <link>http://arxiv.org/abs/2510.23428v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络学习描述符与传统分子描述符的混合方法，通过MetaModel框架整合多种机器学习模型，以提高分子性质预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;分子性质预测是化学信息学和药物发现中的关键任务，现有的方法通常专注于单一类型的特征或模型架构，可能无法充分利用不同方法的优势。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够结合多种描述符和机器学习模型的'最佳结合'方法，以提高分子性质预测的准确性，特别是在广泛的回归和分类任务中。&lt;h4&gt;方法&lt;/h4&gt;1. 引入MetaModel框架聚合来自多样化领先ML模型的预测；2. 设计特征化方案结合任务特定的GNN衍生特征与传统分子描述符；3. 使用图神经网络(GNN)学习分子描述符；4. 结合通用描述符和混合机器学习模型集成。&lt;h4&gt;主要发现&lt;/h4&gt;在所有测试的回归数据集上优于最先进的ChemProp模型；在9个分类数据集中的6个上优于ChemProp模型；包含从ChemProp衍生的GNN特征可以提升集成模型在多个数据集上的性能；该方法在多种类型的分子性质预测任务中表现优异。&lt;h4&gt;结论&lt;/h4&gt;为了在广泛问题上实现最佳性能，结合通用描述符与任务特定的学习特征，并使用多样化的机器学习模型进行预测至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们通过结合图神经网络(GNN)学习到的分子描述符与通用描述符以及混合的机器学习模型集成，探索了一种用于建模分子性质的'最佳结合'方法。我们引入了一个MetaModel框架来聚合来自多样化领先ML模型的预测。我们提出了一种特征化方案，用于结合任务特定的GNN衍生特征与传统分子描述符。我们证明，在所有测试的回归数据集上以及在9个分类数据集中的6个上，我们的框架优于最先进的ChemProp模型。我们进一步表明，包含从ChemProp衍生的GNN特征可以提升集成模型在多个数据集上的性能，否则这些数据集上的性能会较差。我们得出结论，为了在广泛问题上实现最佳性能，结合通用描述符与任务特定的学习特征，并使用多样化的ML模型进行预测至关重要。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1021/acs.jcim.5c01844&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore a "best-of-both" approach to modelling molecular properties bycombining learned molecular descriptors from a graph neural network (GNN) withgeneral-purpose descriptors and a mixed ensemble of machine learning (ML)models. We introduce a MetaModel framework to aggregate predictions from adiverse set of leading ML models. We present a featurisation scheme forcombining task-specific GNN-derived features with conventional moleculardescriptors.  We demonstrate that our framework outperforms the cutting-edge ChemProp modelon all regression datasets tested and 6 of 9 classification datasets. Wefurther show that including the GNN features derived from ChemProp boosts theensemble model's performance on several datasets where it otherwise would haveunderperformed. We conclude that to achieve optimal performance across a wideset of problems, it is vital to combine general-purpose descriptors withtask-specific learned features and use a diverse set of ML models to make thepredictions.</description>
      <author>example@mail.com (Michael L. Parker, Samar Mahmoud, Bailey Montefiore, Mario Öeren, Himani Tandon, Charlotte Wharrick, Matthew D. Segall)</author>
      <guid isPermaLink="false">2510.23428v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Framework for Multi-Modal Protein Representation Learning</title>
      <link>http://arxiv.org/abs/2510.23273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DAMPE是一个统一框架，通过最优传输(OT)表示对齐和条件图生成(CGG)信息融合解决了蛋白质功能预测中的跨模态异质性和嘈杂数据问题，在标准基准上取得了优于或匹配最先进方法的性能。&lt;h4&gt;背景&lt;/h4&gt;准确的蛋白质功能预测需要整合异构的内在信号(如序列和结构)与嘈杂的外部上下文(如蛋白质相互作用和GO术语注释)。然而，两个关键挑战阻碍了有效融合：(i)预训练的内在编码器产生的嵌入之间的跨模态分布不匹配；(ii)外部数据的嘈杂关系图降低了基于GNN的信息聚合效果。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架来解决蛋白质功能预测中的跨模态分布不匹配和嘈杂数据关系图问题，提高蛋白质功能预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了DAMPE(Diffused and Aligned Multi-modal Protein Embedding)框架，包含两个核心机制：(1)基于最优传输(OT)的表示对齐，建立不同模态内在嵌入空间之间的对应关系，缓解跨模态异质性；(2)基于条件图生成(CGG)的信息融合方法，条件编码器融合对齐的内在嵌入为图重建提供信息提示。理论分析表明CGG目标驱动条件编码器将图感知知识吸收到蛋白质表示中。&lt;h4&gt;主要发现&lt;/h4&gt;DAMPE在标准GO基准测试上优于或匹配了DPFunc等最先进方法，实现了0.002-0.013 pp的AUPR增益和0.004-0.007 pp的Fmax增益。消融研究表明基于OT的对齐贡献了0.043-0.064 pp的AUPR，基于CGG的融合增加了0.005-0.111 pp的Fmax。&lt;h4&gt;结论&lt;/h4&gt;DAMPE为稳健的多模态蛋白质表示学习提供了一种可扩展且理论上有依据的方法，显著提高了蛋白质功能预测的准确性。&lt;h4&gt;翻译&lt;/h4&gt;准确的蛋白质功能预测需要整合异构的内在信号(如序列和结构)与嘈杂的外部上下文(如蛋白质相互作用和GO术语注释)。然而，两个关键挑战阻碍了有效融合：(i)由预训练的内在编码器产生的嵌入之间的跨模态分布不匹配，以及(ii)外部数据的嘈杂关系图降低了基于GNN的信息聚合效果。我们提出了DAMPE(扩散和对齐的多模态蛋白质嵌入)，一个通过两个核心机制解决这些问题的统一框架。首先，我们提出了基于最优传输(OT)的表示对齐，建立了不同模态内在嵌入空间之间的对应关系，有效缓解了跨模态异质性。其次，我们开发了基于条件图生成(CGG)的信息融合方法，其中条件编码器融合对齐的内在嵌入，为图重建提供信息提示。同时，我们的理论分析表明CGG目标驱动条件编码器将其产生的蛋白质表示吸收图感知知识。经验上，DAMPE在标准GO基准测试上优于或匹配了DPFunc等最先进方法，实现了0.002-0.013 pp的AUPR增益和0.004-0.007 pp的Fmax增益。消融研究进一步表明，基于OT的对齐贡献了0.043-0.064 pp的AUPR，而基于CGG的融合增加了0.005-0.111 pp的Fmax。总体而言，DAMPE为稳健的多模态蛋白质表示学习提供了一种可扩展且理论上有依据的方法，显著提高了蛋白质功能预测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate protein function prediction requires integrating heterogeneousintrinsic signals (e.g., sequence and structure) with noisy extrinsic contexts(e.g., protein-protein interactions and GO term annotations). However, two keychallenges hinder effective fusion: (i) cross-modal distributional mismatchamong embeddings produced by pre-trained intrinsic encoders, and (ii) noisyrelational graphs of extrinsic data that degrade GNN-based informationaggregation. We propose Diffused and Aligned Multi-modal Protein Embedding(DAMPE), a unified framework that addresses these through two core mechanisms.First, we propose Optimal Transport (OT)-based representation alignment thatestablishes correspondence between intrinsic embedding spaces of differentmodalities, effectively mitigating cross-modal heterogeneity. Second, wedevelop a Conditional Graph Generation (CGG)-based information fusion method,where a condition encoder fuses the aligned intrinsic embeddings to provideinformative cues for graph reconstruction. Meanwhile, our theoretical analysisimplies that the CGG objective drives this condition encoder to absorbgraph-aware knowledge into its produced protein representations. Empirically,DAMPE outperforms or matches state-of-the-art methods such as DPFunc onstandard GO benchmarks, achieving AUPR gains of 0.002-0.013 pp and Fmax gains0.004-0.007 pp. Ablation studies further show that OT-based alignmentcontributes 0.043-0.064 pp AUPR, while CGG-based fusion adds 0.005-0.111 ppFmax. Overall, DAMPE offers a scalable and theoretically grounded approach forrobust multi-modal protein representation learning, substantially enhancingprotein function prediction.</description>
      <author>example@mail.com (Runjie Zheng, Zhen Wang, Anjie Qiao, Jiancong Xie, Jiahua Rao, Yuedong Yang)</author>
      <guid isPermaLink="false">2510.23273v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>GTR-Mamba: Geometry-to-Tangent Routing for Hyperbolic POI Recommendation</title>
      <link>http://arxiv.org/abs/2510.22942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 8 figures, 4 tables, submitted to ICDE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GTR-Mamba是一种创新的下一个兴趣点推荐框架，通过结合双曲几何和欧几里得切线空间的优势，解决了现有模型难以同时捕捉空间层次结构和时间动态性的问题。&lt;h4&gt;背景&lt;/h4&gt;下一个兴趣点推荐是现代位置社交网络中的关键任务，现有模型主要基于图神经网络和序列模型，但存在基本局限性。&lt;h4&gt;目的&lt;/h4&gt;克服现有模型的限制，能够同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态变化，提供更精准的个性化推荐。&lt;h4&gt;方法&lt;/h4&gt;提出GTR-Mamba框架，利用双曲几何建模静态树状偏好层次结构，在欧几里得切线空间中通过Mamba层处理动态序列更新，并通过跨流形通道融合时空信息引导状态空间模型。&lt;h4&gt;主要发现&lt;/h4&gt;在三个真实数据集上的实验表明，GTR-Mamba在下一个POI推荐任务上持续优于最先进的基线模型。&lt;h4&gt;结论&lt;/h4&gt;GTR-Mamba框架有效解决了现有模型在捕捉空间层次结构和时间动态性方面的局限性，提升了推荐的准确性。&lt;h4&gt;翻译&lt;/h4&gt;下一个兴趣点推荐是现代位置社交网络中的关键任务，旨在对人类移动的复杂决策过程进行建模，为用户的下一个签到位置提供个性化推荐。现有的POI推荐模型主要基于图神经网络和序列模型，已得到广泛研究。然而，这些模型面临一个基本限制：它们难以同时捕捉空间选择的内在层次结构和用户特定时间上下文的动态及不规则变化。为克服这一限制，我们提出了GTR-Mamba，一个用于跨流形条件和路由的新框架。GTR-Mamba利用不同数学空间的不同优势处理不同任务：它在双曲几何中建模静态的树状偏好层次结构，同时将动态序列更新路由到计算稳定且高效的欧几里得切线空间中的新型Mamba层。这一过程由跨流形通道协调，该通道融合时空信息以明确引导状态空间模型，实现对上下文变化的灵活适应。在三个真实数据集上的大量实验表明，GTR-Mamba在下一个POI推荐方面持续优于最先进的基线模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next Point-of-Interest (POI) recommendation is a critical task in modernLocation-Based Social Networks (LBSNs), aiming to model the complexdecision-making process of human mobility to provide personalizedrecommendations for a user's next check-in location. Existing POIrecommendation models, predominantly based on Graph Neural Networks andsequential models, have been extensively studied. However, these models face afundamental limitation: they struggle to simultaneously capture the inherenthierarchical structure of spatial choices and the dynamics and irregular shiftsof user-specific temporal contexts. To overcome this limitation, we proposeGTR-Mamba, a novel framework for cross-manifold conditioning and routing.GTR-Mamba leverages the distinct advantages of different mathematical spacesfor different tasks: it models the static, tree-like preference hierarchies inhyperbolic geometry, while routing the dynamic sequence updates to a novelMamba layer in the computationally stable and efficient Euclidean tangentspace. This process is coordinated by a cross-manifold channel that fusesspatio-temporal information to explicitly steer the State Space Model (SSM),enabling flexible adaptation to contextual changes. Extensive experiments onthree real-world datasets demonstrate that GTR-Mamba consistently outperformsstate-of-the-art baseline models in next POI recommendation.</description>
      <author>example@mail.com (Zhuoxuan Li, Jieyuan Pei, Tangwei Ye, Zhongyuan Lai, Zihan Liu, Fengyuan Xu, Qi Zhang, Liang Hu)</author>
      <guid isPermaLink="false">2510.22942v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Diffuse to Detect: A Generalizable Framework for Anomaly Detection with Diffusion Models Applications to UAVs and Beyond</title>
      <link>http://arxiv.org/abs/2510.22928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DTD是一种创新的异常检测框架，通过适应扩散模型并采用单步过程实现快速精确的异常识别，结合图神经网络捕捉时空异常，并通过双分支架构平衡可扩展性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;复杂、高维数据（如无人机传感器读数）中的异常检测对于操作安全至关重要，但现有方法在敏感性、可扩展性和捕捉复杂依赖关系方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种创新的异常检测方法，解决现有方法的局限性，实现快速、精确的异常识别。&lt;h4&gt;方法&lt;/h4&gt;提出Diffuse to Detect (DTD)框架，采用单步扩散过程预测噪声模式；集成图神经网络建模传感器关系为动态图；使用双分支架构（参数化神经网络能量评分和非参数统计方法）平衡可扩展性和可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;DTD能够在不产生重构错误的情况下快速精确识别异常；在无人机传感器数据、多元时间序列和图像上的评估表明DTD优于现有方法；DTD具有跨不同数据模态的通用性。&lt;h4&gt;结论&lt;/h4&gt;DTD因其多功能性和适应性，成为安全关键应用（包括工业监测等）的变革性解决方案。&lt;h4&gt;翻译&lt;/h4&gt;复杂、高维数据（如无人机传感器读数）中的异常检测对于操作安全至关重要，但由于现有方法的敏感性、可扩展性有限且无法捕捉复杂依赖关系，这仍然是一个挑战。我们提出了Diffuse to Detect (DTD)框架，这是一种创新的方法，将扩散模型适应于异常检测，不同于其在具有高推理时间的生成任务中的常规使用。相比之下，DTD采用单步扩散过程来预测噪声模式，能够快速精确地识别异常而不会产生重构错误。这种方法基于稳健的理论基础，将噪声预测与数据分布的得分函数联系起来，确保可靠的偏差检测。通过集成图神经网络将传感器关系建模为动态图，DTD有效捕捉了空间（传感器间）和时间异常。其双分支架构采用基于参数化神经网络的能量评分实现可扩展性，非参数统计方法提供可解释性，在计算效率和透明度之间提供了灵活的权衡。在无人机传感器数据、多元时间序列和图像上的广泛评估证明了DTD优于现有方法的性能，强调了其在不同数据模态上的通用性。这种多功能性及其适应性使DTD成为安全关键应用的变革性解决方案，包括工业监测等。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection in complex, high-dimensional data, such as UAV sensorreadings, is essential for operational safety but challenging for existingmethods due to their limited sensitivity, scalability, and inability to captureintricate dependencies. We propose the Diffuse to Detect (DTD) framework, anovel approach that innovatively adapts diffusion models for anomaly detection,diverging from their conventional use in generative tasks with high inferencetime. By comparison, DTD employs a single-step diffusion process to predictnoise patterns, enabling rapid and precise identification of anomalies withoutreconstruction errors. This approach is grounded in robust theoreticalfoundations that link noise prediction to the data distribution's scorefunction, ensuring reliable deviation detection. By integrating Graph NeuralNetworks to model sensor relationships as dynamic graphs, DTD effectivelycaptures spatial (inter-sensor) and temporal anomalies. Its two-brancharchitecture, with parametric neural network-based energy scoring forscalability and nonparametric statistical methods for interpretability,provides flexible trade-offs between computational efficiency and transparency.Extensive evaluations on UAV sensor data, multivariate time series, and imagesdemonstrate DTD's superior performance over existing methods, underscoring itsgenerality across diverse data modalities. This versatility, combined with itsadaptability, positions DTD as a transformative solution for safety-criticalapplications, including industrial monitoring and beyond.</description>
      <author>example@mail.com (Mingze Gong, Juan Du, Jianbang You)</author>
      <guid isPermaLink="false">2510.22928v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>FastJAM: a Fast Joint Alignment Model for Images</title>
      <link>http://arxiv.org/abs/2510.22842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025. Pages 1-10 are the Main Paper. Pages 23-31  are Supplemental Material. FastJAM website -  https://bgu-cs-vil.github.io/FastJAM/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FastJAM是一种快速的基于图的图像联合对齐方法，能够显著降低计算复杂度，实现从小时或分钟到秒级的速度提升，同时保持或提高对齐质量。&lt;h4&gt;背景&lt;/h4&gt;图像联合对齐(JA)旨在将一组图像对齐到统一坐标系，使语义相似特征出现在对应空间位置。现有方法通常需要长时间训练、大容量模型和大量超参数调整。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、高效的图像联合对齐方法，减少计算时间和资源需求，同时保持或提高对齐质量。&lt;h4&gt;方法&lt;/h4&gt;FastJAM利用现成的图像匹配器计算的对和快速非参数聚类构建图，表示图像内和图像间关键点关系。通过图神经网络传播和聚合这些对应关系，使用图像级池化预测单应性参数。采用逆组合损失消除正则化项需求，避免相关超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FastJAM在对齐质量方面优于现有现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。&lt;h4&gt;结论&lt;/h4&gt;FastJAM是一种高效、快速的图像联合对齐方法，能够在保持高质量对齐的同时显著减少计算时间。&lt;h4&gt;翻译&lt;/h4&gt;图像联合对齐(JA)旨在将一组图像对齐到统一的坐标系中，使语义相似的特征出现在对应的空间位置。大多数现有方法通常需要长时间的训练、大容量模型和大量的超参数调整。我们引入了FastJAM，一种快速的基于图的方法，显著降低了联合对齐任务的计算复杂度。FastJAM利用现成的图像匹配器计算的对，以及快速的非参数聚类，来构建表示图像内和图像间关键点关系的图。图神经网络传播和聚合这些对应关系，通过图像级池化有效地预测每个图像的单应性参数。利用逆组合损失消除了对预测变换的正则化项的需求（因此也避免了与这些项相关的超参数调整），FastJAM能够快速有效地执行图像联合对齐。在几个基准测试上的实验结果表明，FastJAM在对齐质量方面优于现有的现代JA方法，同时将计算时间从小时或分钟减少到几秒钟。我们的代码可在项目网页获取，https://bgu-cs-vil.github.io/FastJAM/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Alignment (JA) of images aims to align a collection of images into aunified coordinate frame, such that semantically-similar features appear atcorresponding spatial locations. Most existing approaches often require longtraining times, large-capacity models, and extensive hyperparameter tuning. Weintroduce FastJAM, a rapid, graph-based method that drastically reduces thecomputational complexity of joint alignment tasks. FastJAM leverages pairwisematches computed by an off-the-shelf image matcher, together with a rapidnonparametric clustering, to construct a graph representing intra- andinter-image keypoint relations. A graph neural network propagates andaggregates these correspondences, efficiently predicting per-image homographyparameters via image-level pooling. Utilizing an inverse-compositional loss,that eliminates the need for a regularization term over the predictedtransformations (and thus also obviates the hyperparameter tuning associatedwith such terms), FastJAM performs image JA quickly and effectively.Experimental results on several benchmarks demonstrate that FastJAM achievesresults better than existing modern JA methods in terms of alignment quality,while reducing computation time from hours or minutes to mere seconds. Our codeis available at our project webpage, https://bgu-cs-vil.github.io/FastJAM/</description>
      <author>example@mail.com (Omri Hirsch, Ron Shapira Weber, Shira Ifergane, Oren Freifeld)</author>
      <guid isPermaLink="false">2510.22842v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Network Assisted Genetic Algorithm for Structural Dynamic Response and Parameter Optimization</title>
      <link>http://arxiv.org/abs/2510.22839v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种结合图神经网络(GNN)代理模型和遗传算法(GA)优化器的混合数据驱动框架，用于优化结构参数（质量、刚度和阻尼系数）。该方法克服了传统数值方法在迭代优化任务中计算成本高的问题，实现了强收敛性、良好泛化能力和显著降低的计算成本，为自动化和智能结构设计提供了有效途径。&lt;h4&gt;背景&lt;/h4&gt;结构参数（质量m、刚度k和阻尼系数c）的优化对设计高效、有韧性和稳定的结构至关重要。传统的数值方法如有限元法(FEM)和计算流体动力学(CFD)模拟虽能提供高精度结果，但在迭代优化任务中计算成本高昂，因为每次评估都需要为每个参数组合求解控制方程。&lt;h4&gt;目的&lt;/h4&gt;开发一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器，以克服传统数值方法在结构参数优化中的计算成本高问题，实现更高效、准确的参数优化。&lt;h4&gt;方法&lt;/h4&gt;研究采用了一种混合数据驱动框架，包括：1) 使用图神经网络(GNN)作为代理模型，学习结构参数与动态位移响应之间的非线性映射；2) 使用Newmark Beta方法生成具有不同质量、刚度和阻尼配置的单自由度(SDOF)系统响应数据集；3) 应用遗传算法(GA)通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。&lt;h4&gt;主要发现&lt;/h4&gt;GNN和GA框架实现了强收敛性、良好的泛化能力，并显著降低了计算成本，相比传统模拟方法具有明显优势。该方法能够准确学习结构参数与动态响应之间的复杂关系，实现快速预测和优化。&lt;h4&gt;结论&lt;/h4&gt;结合机器学习代理与进化优化的方法在自动化和智能结构设计中具有显著有效性。该框架为结构参数优化提供了一种高效、准确的解决方案，克服了传统数值方法的计算瓶颈。&lt;h4&gt;翻译&lt;/h4&gt;结构参数（如质量m、刚度k和阻尼系数c）的优化对设计高效、有韧性和稳定的结构至关重要。传统的数值方法，包括有限元法(FEM)和计算流体动力学(CFD)模拟，能提供高精度结果，但在迭代优化任务中计算成本高，因为每次评估都需要为每个参数组合求解控制方程。本研究提出了一种混合数据驱动框架，结合图神经网络(GNN)代理模型和遗传算法(GA)优化器来克服这些挑战。GNN被训练来准确学习结构参数和动态位移响应之间的非线性映射，从而能够快速预测而无需重复求解系统方程。使用Newmark Beta方法生成了具有不同质量、刚度和阻尼配置的单自由度(SDOF)系统响应数据集。然后，GA通过最小化预测位移和提高动态稳定性来搜索全局最优参数集。结果表明，与传统模拟相比，GNN和GA框架实现了强收敛性、良好的泛化能力和显著降低的计算成本。这种方法强调了将机器学习代理与进化优化相结合在自动化和智能结构设计中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The optimization of structural parameters, such as mass(m), stiffness(k), anddamping coefficient(c), is critical for designing efficient, resilient, andstable structures. Conventional numerical approaches, including Finite ElementMethod (FEM) and Computational Fluid Dynamics (CFD) simulations, providehigh-fidelity results but are computationally expensive for iterativeoptimization tasks, as each evaluation requires solving the governing equationsfor every parameter combination. This study proposes a hybrid data-drivenframework that integrates a Graph Neural Network (GNN) surrogate model with aGenetic Algorithm (GA) optimizer to overcome these challenges. The GNN istrained to accurately learn the nonlinear mapping between structural parametersand dynamic displacement responses, enabling rapid predictions withoutrepeatedly solving the system equations. A dataset of single-degree-of-freedom(SDOF) system responses is generated using the Newmark Beta method acrossdiverse mass, stiffness, and damping configurations. The GA then searches forglobally optimal parameter sets by minimizing predicted displacements andenhancing dynamic stability. Results demonstrate that the GNN and GA frameworkachieves strong convergence, robust generalization, and significantly reducedcomputational cost compared to conventional simulations. This approachhighlights the effectiveness of combining machine learning surrogates withevolutionary optimization for automated and intelligent structural design.</description>
      <author>example@mail.com (Sagnik Mukherjee)</author>
      <guid isPermaLink="false">2510.22839v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Policies over Poses: Reinforcement Learning based Distributed Pose-Graph Optimization for Multi-Robot SLAM</title>
      <link>http://arxiv.org/abs/2510.22740v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE International Symposium on Multi-Robot &amp; Multi-Agent Systems  (MRS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于多智能体强化学习的分布式姿态图优化框架，解决了传统方法收敛到局部最小值的问题，显著提高了轨迹估计的准确性和计算效率。&lt;h4&gt;背景&lt;/h4&gt;分布式姿态图优化（PGO）是多机器人同时定位与地图构建（SLAM）中精确轨迹估计的基础。传统迭代方法将高度非凸优化目标线性化，需要重复求解正规方程，通常收敛到局部最小值，从而产生次优估计。&lt;h4&gt;目的&lt;/h4&gt;开发一个可扩展、抗异常值的分布式平面PGO框架，利用多智能体强化学习（MARL）来提高轨迹估计的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;将分布式PGO建模为部分可观察马尔可夫博弈，每个机器人运行带有自适应边缘门控的循环边缘条件图神经网络（GNN）编码器来去噪，通过混合策略利用先验动作记忆和图嵌入优化姿态，最后使用一致性方案解决机器人间的不一致。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和真实世界数据集上的评估表明，该方法比最先进的分布式PGO框架平均减少37.5%的全局目标，同时将推理效率提高至少6倍；单个学习策略无需重新训练即可扩展到更大的机器人团队。&lt;h4&gt;结论&lt;/h4&gt;基于MARL的分布式PGO框架在提高轨迹估计精度的同时显著增强了计算效率，具有良好的可扩展性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;我们考虑分布式姿态图优化（PGO）问题，这是多机器人同时定位与地图构建（SLAM）中精确轨迹估计的基础。传统迭代方法将高度非凸优化目标线性化，需要重复求解正规方程，通常收敛到局部最小值，从而产生次优估计。我们提出了一种使用多智能体强化学习（MARL）的可扩展、抗异常值的分布式平面PGO框架。我们将分布式PGO构建为定义在局部姿态图上的部分可观察马尔可夫博弈，其中每个动作优化单个边的姿态估计。图分区器分解全局姿态图，每个机器人运行带有自适应边缘门控的循环边缘条件图神经网络（GNN）编码器来去噪噪声边缘。机器人通过利用先验动作记忆和图嵌入的混合策略顺序优化姿态。在局部图校正后，使用一致性方案解决机器人间的不一致，产生全局一致的估计。我们在综合的合成和真实世界数据集上的广泛评估表明，我们学习的基于MARL的智能体比最先进的分布式PGO框架平均减少37.5%的全局目标，同时将推理效率提高至少6倍。我们还证明了智能体复制允许单个学习策略无需重新训练即可轻松扩展到更大的机器人团队。代码可在https://github.com/herolab-uga/policies-over-poses公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We consider the distributed pose-graph optimization (PGO) problem, which isfundamental in accurate trajectory estimation in multi-robot simultaneouslocalization and mapping (SLAM). Conventional iterative approaches linearize ahighly non-convex optimization objective, requiring repeated solving of normalequations, which often converge to local minima and thus produce suboptimalestimates. We propose a scalable, outlier-robust distributed planar PGOframework using Multi-Agent Reinforcement Learning (MARL). We cast distributedPGO as a partially observable Markov game defined on local pose-graphs, whereeach action refines a single edge's pose estimate. A graph partitionerdecomposes the global pose graph, and each robot runs a recurrentedge-conditioned Graph Neural Network (GNN) encoder with adaptive edge-gatingto denoise noisy edges. Robots sequentially refine poses through a hybridpolicy that utilizes prior action memory and graph embeddings. After localgraph correction, a consensus scheme reconciles inter-robot disagreements toproduce a globally consistent estimate. Our extensive evaluations on acomprehensive suite of synthetic and real-world datasets demonstrate that ourlearned MARL-based actors reduce the global objective by an average of 37.5%more than the state-of-the-art distributed PGO framework, while enhancinginference efficiency by at least 6X. We also demonstrate that actor replicationallows a single learned policy to scale effortlessly to substantially largerrobot teams without any retraining. Code is publicly available athttps://github.com/herolab-uga/policies-over-poses.</description>
      <author>example@mail.com (Sai Krishna Ghanta, Ramviyas Parasuraman)</author>
      <guid isPermaLink="false">2510.22740v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>SpoofTrackBench: Interpretable AI for Spoof-Aware UAV Tracking and Benchmarking</title>
      <link>http://arxiv.org/abs/2510.22726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SpoofTrackBench是一个可重现、模块化的基准测试，用于评估雷达欺骗下的实时定位和跟踪系统的对抗鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;雷达欺骗攻击对实时定位和跟踪系统构成威胁，需要有效的评估方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个基准测试框架，评估不同跟踪架构在雷达欺骗攻击下的性能和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用Hampton University Skyler Radar Sensor数据集，模拟漂移、幽灵和镜像类型的欺骗攻击，并使用联合概率数据关联(JPDA)和全局最近邻(GNN)架构评估跟踪器性能。框架分离干净和欺骗的检测流，可视化轨迹偏离，并量化分配错误。&lt;h4&gt;主要发现&lt;/h4&gt;通过聚类叠加、注入感知时间线和场景自适应可视化，实现了不同欺骗类型和配置下的结果可解释性。评估图表和日志自动导出，确保可重现性。&lt;h4&gt;结论&lt;/h4&gt;SpoofTrackBench为开放、合乎道德的欺骗感知跟踪管道基准测试设定了新标准，实现了严格的跨架构分析和社区验证。&lt;h4&gt;翻译&lt;/h4&gt;SpoofTrackBench是一个可重现、模块化的基准测试，用于评估雷达欺骗下的实时定位和跟踪(RTLS)系统的对抗鲁棒性。利用Hampton University Skyler雷达传感器数据集，我们模拟了漂移、幽灵和镜像类型的欺骗攻击，并使用联合概率数据关联(JPDA)和全局最近邻(GNN)架构评估跟踪器性能。我们的框架分离干净和欺骗的检测流，可视化欺骗引起的轨迹偏离，并通过直接偏离真相的指标量化分配错误。聚类叠加、注入感知时间线和场景自适应可视化使不同欺骗类型和配置下的结果可解释。评估图表和日志自动导出，用于可重现的比较。SpoofTrackBench为开放、合乎道德的欺骗感知跟踪管道基准测试设定了新标准，实现了严格的跨架构分析和社区验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SpoofTrackBench is a reproducible, modular benchmark for evaluatingadversarial robustness in real-time localization and tracking (RTLS) systemsunder radar spoofing. Leveraging the Hampton University Skyler Radar Sensordataset, we simulate drift, ghost, and mirror-type spoofing attacks andevaluate tracker performance using both Joint Probabilistic Data Association(JPDA) and Global Nearest Neighbor (GNN) architectures. Our framework separatesclean and spoofed detection streams, visualizes spoof-induced trajectorydivergence, and quantifies assignment errors via direct drift-from-truthmetrics. Clustering overlays, injection-aware timelines, and scenario-adaptivevisualizations enable interpretability across spoof types and configurations.Evaluation figures and logs are auto-exported for reproducible comparison.SpoofTrackBench sets a new standard for open, ethical benchmarking ofspoof-aware tracking pipelines, enabling rigorous cross-architecture analysisand community validation.</description>
      <author>example@mail.com (Van Le, Tan Le)</author>
      <guid isPermaLink="false">2510.22726v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>If You Want to Be Robust, Be Wary of Initialization</title>
      <link>http://arxiv.org/abs/2510.22652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络(GNNs)的权重初始化和超参数对对抗性鲁棒性的影响，发现适当的初始化方法可显著提升模型防御能力。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在各种图相关任务中表现出色，但容易受到对抗性扰动的攻击。现有防御策略主要关注预处理技术和自适应消息传递方案，而忽略了权重初始化的影响。&lt;h4&gt;目的&lt;/h4&gt;探索权重初始化和相关超参数(如训练周期)对模型鲁棒性的影响，建立初始化策略与网络抗扰动能力之间的理论联系。&lt;h4&gt;方法&lt;/h4&gt;引入连接初始化策略和网络鲁棒性的理论框架，分析初始权重、训练周期与模型脆弱性的关系，并将框架扩展到深度神经网络。通过多种模型和真实数据集的对抗性攻击实验验证理论发现。&lt;h4&gt;主要发现&lt;/h4&gt;适当的权重初始化不仅能保证模型在干净数据集上的性能，还能显著提升对抗性防御能力，与其他初始化方法相比性能差距可达50%。&lt;h4&gt;结论&lt;/h4&gt;权重初始化是提升图神经网络对抗鲁棒性的重要因素，为防御对抗性攻击提供了超越传统机制的新视角。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在各种图相关任务中表现出色，然而人们对它们容易受到对抗性扰动的担忧依然存在。虽然主流防御策略主要关注预处理技术和自适应消息传递方案，但本研究探讨了一个尚未充分研究的维度：权重初始化及相关超参数(如训练周期)对模型鲁棒性的影响。我们引入了一个理论框架，连接初始化策略与网络对抗扰动的恢复能力。我们的分析揭示了初始权重、训练周期数量与模型脆弱性之间的直接关系，为对抗性鲁棒性提供了超越传统防御机制的新见解。虽然我们的主要关注点是图神经网络，但我们扩展了理论框架，提供了一个适用于深度神经网络的通用上界。跨越多种模型和真实世界数据集的广泛实验( subjected to various adversarial attacks)验证了我们的发现。我们说明，选择适当的初始化不仅能确保在干净数据集上的性能，还能增强模型对抗扰动的鲁棒性，与其他初始化方法相比观察到高达50%的性能差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable performance acrossa spectrum of graph-related tasks, however concerns persist regarding theirvulnerability to adversarial perturbations. While prevailing defense strategiesfocus primarily on pre-processing techniques and adaptive message-passingschemes, this study delves into an under-explored dimension: the impact ofweight initialization and associated hyper-parameters, such as training epochs,on a model's robustness. We introduce a theoretical framework bridging theconnection between initialization strategies and a network's resilience toadversarial perturbations. Our analysis reveals a direct relationship betweeninitial weights, number of training epochs and the model's vulnerability,offering new insights into adversarial robustness beyond conventional defensemechanisms. While our primary focus is on GNNs, we extend our theoreticalframework, providing a general upper-bound applicable to Deep Neural Networks.Extensive experiments, spanning diverse models and real-world datasetssubjected to various adversarial attacks, validate our findings. We illustratethat selecting appropriate initialization not only ensures performance on cleandatasets but also enhances model robustness against adversarial perturbations,with observed gaps of up to 50\% compared to alternative initializationapproaches.</description>
      <author>example@mail.com (Sofiane Ennadir, Johannes F. Lutzeyer, Michalis Vazirgiannis, El Houcine Bergou)</author>
      <guid isPermaLink="false">2510.22652v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph Classification Robustness with Singular Pooling</title>
      <link>http://arxiv.org/abs/2510.22643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at Neurips 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了图神经网络在图分类任务中的对抗鲁棒性问题，特别关注了池化操作在塑造鲁棒性方面的作用，并提出了一种名为鲁棒奇异池化(RS-Pool)的新策略。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在图表示学习任务中表现出色，但其在图分类任务中的对抗鲁棒性研究相对较少，尤其是在与节点分类相比时。大多数现有防御方法集中在消息传递组件上，而忽略了池化操作的作用。&lt;h4&gt;目的&lt;/h4&gt;研究池化操作在图神经网络对抗鲁棒性中的角色，并开发一种能够提高图分类任务鲁棒性的新池化策略。&lt;h4&gt;方法&lt;/h4&gt;对标准扁平池化方法(求和、平均和最大值)进行理论分析，推导对抗风险上限，确定不同攻击场景和图结构下的脆弱性。基于这些见解，提出利用节点嵌入矩阵主奇异向量构建鲁棒图级表示的RS-Pool策略。&lt;h4&gt;主要发现&lt;/h4&gt;RS-Pool在遭受最先进对抗攻击时，比其他池化方法提供更好的鲁棒性，同时保持有竞争力的干净准确率。该策略与模型无关，可通过幂迭代有效实现，理论分析表明其具有良好的鲁棒性特性。&lt;h4&gt;结论&lt;/h4&gt;池化操作在图神经网络的对抗鲁棒性中扮演着重要角色，所提出的RS-Pool方法能够有效提高图分类任务的鲁棒性，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在一系列图表示学习任务中已取得强大性能，然而与节点分类相比，其在图分类中的对抗鲁棒性研究仍然不足。虽然大多数现有防御方法集中在消息传递组件上，但本研究调查了池化操作在塑造鲁棒性中被忽视的作用。我们对标准扁平池化方法(求和、平均和最大值)进行了理论分析，推导了它们对抗风险的上限，并确定了它们在不同攻击场景和图结构下的脆弱性。受这些见解启发，我们提出了鲁棒奇异池化(RS-Pool)，这是一种新颖的池化策略，利用节点嵌入矩阵的主奇异向量构建鲁棒的图级表示。我们从理论上研究了RS-Pool的鲁棒性，并解释了所得界限，从而加深对我们提出的池化算子的理解。虽然我们的分析集中在图卷积网络(GCNs)上，但RS-Pool是与模型无关的，可以通过幂迭代有效实现。真实世界基准测试的实证结果表明，当遭受最先进的对抗攻击时，RS-Pool比考虑的池化方法提供更好的鲁棒性，同时保持有竞争力的干净准确率。我们的代码已在GitHub公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have achieved strong performance across a rangeof graph representation learning tasks, yet their adversarial robustness ingraph classification remains underexplored compared to node classification.While most existing defenses focus on the message-passing component, this workinvestigates the overlooked role of pooling operations in shaping robustness.We present a theoretical analysis of standard flat pooling methods (sum,average and max), deriving upper bounds on their adversarial risk andidentifying their vulnerabilities under different attack scenarios and graphstructures. Motivated by these insights, we propose \textit{Robust SingularPooling (RS-Pool)}, a novel pooling strategy that leverages the dominantsingular vector of the node embedding matrix to construct a robust graph-levelrepresentation. We theoretically investigate the robustness of RS-Pool andinterpret the resulting bound leading to improved understanding of our proposedpooling operator. While our analysis centers on Graph Convolutional Networks(GCNs), RS-Pool is model-agnostic and can be implemented efficiently via poweriteration. Empirical results on real-world benchmarks show that RS-Poolprovides better robustness than the considered pooling methods when subject tostate-of-the-art adversarial attacks while maintaining competitive cleanaccuracy. Our code is publicly availableat:\href{https://github.com/king/rs-pool}{https://github.com/king/rs-pool}.</description>
      <author>example@mail.com (Sofiane Ennadir, Oleg Smirnov, Yassine Abbahaddou, Lele Cao, Johannes F. Lutzeyer)</author>
      <guid isPermaLink="false">2510.22643v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Iteratively Refined Early Interaction Alignment for Subgraph Matching based Graph Retrieval</title>
      <link>http://arxiv.org/abs/2510.22538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了IsoNet++，一种基于子图同构的改进图检索方法，通过技术创新显著提升了检索性能。&lt;h4&gt;背景&lt;/h4&gt;基于子图同构的图检索在场景图检索、分子指纹检测和电路设计等领域有广泛应用。Roy等人提出的IsoNet是一种后期交互模型，用于子图匹配，它先独立计算每对图的节点和边嵌入，再计算可训练的对齐映射。&lt;h4&gt;目的&lt;/h4&gt;开发IsoNet++，一种早期交互图神经网络，通过技术创新改进现有的子图匹配方法，提高图检索的准确性。&lt;h4&gt;方法&lt;/h4&gt;IsoNet++包含三个技术创新：1)通过在两个输入图内部和之间传递消息计算所有节点嵌入，由节点间的单射对齐引导；2)以惰性方式在多轮中更新对齐，每轮基于当前对齐状态从头运行逐层GNN；3)引入节点对伙伴交互概念，将节点对而非单个节点视为潜在伙伴。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，随着轮次增加，对齐 progressively 得到细化，检索性能显著优于现有方法，且所有三个创新都对提升准确性有贡献。&lt;h4&gt;结论&lt;/h4&gt;IsoNet++通过三个技术创新显著提高了图检索性能，代码和数据集已公开在https://github.com/structlearning/isonetpp。&lt;h4&gt;翻译&lt;/h4&gt;基于子图同构的图检索在场景图检索、分子指纹检测和电路设计等现实世界应用中有多种应用。Roy等人提出了IsoNet，一种用于子图匹配的后期交互模型，它首先独立计算每对图中节点和边的嵌入，然后计算可训练的对齐映射。本文提出了IsoNet++，一种早期交互图神经网络(GNN)，基于几项技术创新。首先，我们通过在两个输入图内部和之间传递消息来计算所有节点的嵌入，这些消息由节点之间的单射对齐引导。其次，我们在多轮中以惰性方式更新对齐。每轮中，我们基于当前对齐状态从头开始运行逐层GNN。一轮GNN完成后，我们使用最后一层嵌入更新对齐，然后进入下一轮。第三，IsoNet++引入了节点对伙伴交互的新概念。传统早期交互计算节点与另一图中潜在伙伴之间的注意力，注意力控制跨图传递的消息。相比之下，我们将节点对（而非单个节点）视为潜在伙伴。一个图中节点间存在边而另一图中不存在，提供了细化对齐的重要信号。我们在多个数据集上的实验表明，对齐随着轮次的推进逐步细化，检索性能显著优于现有方法。我们证明了所有三个创新都对提高准确性做出了贡献。我们的代码和数据集已在https://github.com/structlearning/isonetpp公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph retrieval based on subgraph isomorphism has several real-worldapplications such as scene graph retrieval, molecular fingerprint detection andcircuit design. Roy et al. [35] proposed IsoNet, a late interaction model forsubgraph matching, which first computes the node and edge embeddings of eachgraph independently of paired graph and then computes a trainable alignmentmap. Here, we present IsoNet++, an early interaction graph neural network(GNN), based on several technical innovations. First, we compute embeddings ofall nodes by passing messages within and across the two input graphs, guided byan injective alignment between their nodes. Second, we update this alignment ina lazy fashion over multiple rounds. Within each round, we run a layerwise GNNfrom scratch, based on the current state of the alignment. After the completionof one round of GNN, we use the last-layer embeddings to update the alignments,and proceed to the next round. Third, IsoNet++ incorporates a novel notion ofnode-pair partner interaction. Traditional early interaction computes attentionbetween a node and its potential partners in the other graph, the attentionthen controlling messages passed across graphs. In contrast, we consider nodepairs (not single nodes) as potential partners. Existence of an edge betweenthe nodes in one graph and non-existence in the other provide vital signals forrefining the alignment. Our experiments on several datasets show that thealignments get progressively refined with successive rounds, resulting insignificantly better retrieval performance than existing methods. Wedemonstrate that all three innovations contribute to the enhanced accuracy. Ourcode and datasets are publicly available athttps://github.com/structlearning/isonetpp.</description>
      <author>example@mail.com (Ashwin Ramachandran, Vaibhav Raj, Indrayumna Roy, Soumen Chakrabarti, Abir De)</author>
      <guid isPermaLink="false">2510.22538v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Toward Robust Signed Graph Learning through Joint Input-Target Denoising</title>
      <link>http://arxiv.org/abs/2510.22513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM MM 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了RIDGE框架，一种通过联合去噪图输入和监督目标来实现的鲁棒符号图学习方法，有效提高了SGNN在噪声环境下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;符号图神经网络（SGNNs）被广泛用于分析包含正负链接的符号图中的复杂模式。鉴于现实世界连接的噪声特性，SGNN的鲁棒性也已成为一个关键研究领域。在经验属性监督下，图结构学习已在符号图表示学习中显示出其鲁棒性，然而，缺乏理论指导的鲁棒SGNN研究仍然较少。&lt;h4&gt;目的&lt;/h4&gt;受图信息瓶颈（GIB）在信息提取中的成功启发，提出一种有理论指导的鲁棒SGNN框架，通过联合去噪图输入和监督目标来提高鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;作者提出了RIDGE框架，扩展了GIB理论以支持目标空间去噪，因为输入和目标空间都存在噪声。通过重参数化机制和变分近似产生的可处理目标函数，RIDGE有效清理输入数据和监督目标。&lt;h4&gt;主要发现&lt;/h4&gt;在四个常用的符号图数据集上的广泛验证表明，RIDGE在各种噪声水平下显著提高了流行SGNN模型的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;RIDGE框架能够有效提高SGNN在噪声环境下的鲁棒性，为符号图分析提供了新的理论指导和实践方法。&lt;h4&gt;翻译&lt;/h4&gt;符号图神经网络（SGNNs）被广泛用于分析包含正负链接的符号图中的复杂模式。鉴于现实世界连接的噪声特性，SGNN的鲁棒性也已成为一个关键研究领域。在经验属性监督下，图结构学习已在符号图表示学习中显示出其鲁棒性，然而，缺乏理论指导的鲁棒SGNN研究仍然较少。受图信息瓶颈（GIB）在信息提取中的成功启发，我们提出了RIDGE，一种通过联合去噪图输入和监督目标来实现鲁棒符号图学习的新框架。与基本GIB不同，我们扩展了GIB理论，使其能够对目标空间进行去噪，因为输入和目标空间都存在噪声。在实例化中，RIDGE通过重参数化机制和变分近似产生的可处理目标函数有效清理输入数据和监督目标。我们在四个常用的符号图数据集上广泛验证了我们的方法，结果表明，在各种噪声水平下，RIDGE显著提高了流行SGNN模型的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Signed Graph Neural Networks (SGNNs) are widely adopted to analyze complexpatterns in signed graphs with both positive and negative links. Given thenoisy nature of real-world connections, the robustness of SGNN has also emergedas a pivotal research area. Under the supervision of empirical properties,graph structure learning has shown its robustness on signed graphrepresentation learning, however, there remains a paucity of researchinvestigating a robust SGNN with theoretical guidance. Inspired by the successof graph information bottleneck (GIB) in information extraction, we proposeRIDGE, a novel framework for Robust sI gned graph learning through jointDenoising of Graph inputs and supervision targEts. Different from the basicGIB, we extend the GIB theory with the capability of target space denoising asthe co-existence of noise in both input and target spaces. In instantiation,RIDGE effectively cleanses input data and supervision targets via a tractableobjective function produced by reparameterization mechanism and variationalapproximation. We extensively validate our method on four prevalent signedgraph datasets, and the results show that RIDGE clearly improves the robustnessof popular SGNN models under various levels of noise.</description>
      <author>example@mail.com (Junran Wu, Beng Chin Ooi, Ke Xu)</author>
      <guid isPermaLink="false">2510.22513v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>GraphTOP: Graph Topology-Oriented Prompting for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by the 39 Annual Conference on Neural Information Processing  Systems (NeurIPS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了GraphTOP框架，开创性地探索了面向图拓扑的提示方法，通过修改图拓扑而非仅修改节点特征来适配预训练的图神经网络模型。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)通过大规模图数据学习强大的图表示，'预训练、适配'方案是训练强大GNN的常见模式。在适配阶段，图提示是一种有效策略，可修改输入图数据同时保持预训练GNN模型冻结。&lt;h4&gt;目的&lt;/h4&gt;进行图提示在图拓扑方面的开创性研究，提出第一个图拓扑导向提示(GraphTOP)框架，有效适配预训练GNN模型用于下游任务。&lt;h4&gt;方法&lt;/h4&gt;将拓扑导向提示表述为多跳局部子图中的边重连问题，通过重参数化将其松弛到连续概率空间，同时确保紧密松弛并保持图的稀疏性。&lt;h4&gt;主要发现&lt;/h4&gt;在四种预训练策略下的五个图数据集上进行大量实验，GraphTOP在多个节点分类数据集上优于六个基线方法。&lt;h4&gt;结论&lt;/h4&gt;GraphTOP框架在图提示领域，特别是面向拓扑的提示方面表现出色。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)通过大规模图数据学习强大的图表示，革新了图学习领域。作为训练强大GNN的常见模式，'预训练、适配'方案首先在无标签图数据上预训练GNN，然后将其适配到特定下游任务。在适配阶段，图提示是一种有效策略，即可学习提示修改输入图数据，同时保持预训练GNN模型冻结。通常，现有图提示研究主要关注面向特征的方法，将图提示应用于节点特征或隐藏表示。然而，这些研究表现次优，因为它们持续忽视了面向拓扑提示的潜力，后者通过修改图拓扑来适配预训练GNN。在本研究中，我们从图拓扑角度对图提示进行了开创性研究。我们提出了第一个图拓扑导向提示(GraphTOP)框架，有效适配预训练GNN模型用于下游任务。更具体地说，我们将拓扑导向提示表述为多跳局部子图中的边重连问题，并通过重参数化将其松弛到连续概率空间，同时确保紧密松弛并保持图的稀疏性。在四种预训练策略下的五个图数据集上进行的大量实验表明，我们提出的GraphTOP在多个节点分类数据集上优于六个基线方法。我们的代码可在https://github.com/xbfu/GraphTOP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have revolutionized the field of graph learningby learning expressive graph representations from massive graph data. As acommon pattern to train powerful GNNs, the "pre-training, adaptation" schemefirst pre-trains GNNs over unlabeled graph data and subsequently adapts them tospecific downstream tasks. In the adaptation phase, graph prompting is aneffective strategy that modifies input graph data with learnable prompts whilekeeping pre-trained GNN models frozen. Typically, existing graph promptingstudies mainly focus on *feature-oriented* methods that apply graph prompts tonode features or hidden representations. However, these studies often achievesuboptimal performance, as they consistently overlook the potential of*topology-oriented* prompting, which adapts pre-trained GNNs by modifying thegraph topology. In this study, we conduct a pioneering investigation of graphprompting in terms of graph topology. We propose the first **Graph****T**opology-**O**riented **P**rompting (GraphTOP) framework to effectivelyadapt pre-trained GNN models for downstream tasks. More specifically, wereformulate topology-oriented prompting as an edge rewiring problem withinmulti-hop local subgraphs and relax it into the continuous probability spacethrough reparameterization while ensuring tight relaxation and preserving graphsparsity. Extensive experiments on five graph datasets under four pre-trainingstrategies demonstrate that our proposed GraphTOP outshines six baselines onmultiple node classification datasets. Our code is available athttps://github.com/xbfu/GraphTOP.</description>
      <author>example@mail.com (Xingbo Fu, Zhenyu Lei, Zihan Chen, Binchi Zhang, Chuxu Zhang, Jundong Li)</author>
      <guid isPermaLink="false">2510.22451v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Augmentation: Leveraging Inter-Instance Relation in Self-Supervised Representation Learning</title>
      <link>http://arxiv.org/abs/2510.22322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in IEEE Signal Processing Letters, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将图论与自监督表示学习相结合的新方法，通过构建k近邻图并利用图神经网络进行表示学习，显著提升了模型性能。&lt;h4&gt;背景&lt;/h4&gt;传统自监督表示学习方法主要关注通过数据增强技术生成的实例内变化，但往往忽略了实例间的重要关系信息。&lt;h4&gt;目的&lt;/h4&gt;在保留实例内属性的同时，有效捕获实例间关系，并通过图神经网络实现更广泛的上下文集成，提升表示学习效果。&lt;h4&gt;方法&lt;/h4&gt;在预训练阶段为教师和学生流构建k近邻(KNN)图，其中节点表示样本及其潜在表示，边编码实例间的相似性；在表示细化阶段，使用图神经网络在多个跃点间传播消息，实现更广泛的上下文整合。&lt;h4&gt;主要发现&lt;/h4&gt;在CIFAR-10、ImageNet-100和ImageNet-1K三个数据集上，分别实现了7.3%、3.2%和1.0%的准确率提升，显著优于现有最先进方法。&lt;h4&gt;结论&lt;/h4&gt;基于图的机制在自监督表示学习中具有显著优势，能够有效提升模型性能，代码已公开可获取。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了一种将图论整合到自监督表示学习中的新方法。传统方法专注于应用增强技术生成的实例内变化。然而，它们常常忽略了重要的实例间关系。虽然我们的方法保留了实例内属性，但通过在预训练期间为教师和学生流构建k近邻(KNN)图，进一步捕获了实例间关系。在这些图中，节点表示样本及其潜在表示，边编码实例之间的相似性。预训练后，执行表示细化阶段。在此阶段，图神经网络不仅可以在直接邻居之间传播消息，还可以跨越多个跃点，从而实现更广泛的上下文集成。在CIFAR-10、ImageNet-100和ImageNet-1K上的实验结果分别比最先进的方法提高了7.3%、3.2%和1.0%的准确率。这些结果突显了所提出的基于图机制的有效性。代码可在https://github.com/alijavidani/SSL-GraphNNCLR公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LSP.2025.3610549&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel approach that integrates graph theory intoself-supervised representation learning. Traditional methods focus onintra-instance variations generated by applying augmentations. However, theyoften overlook important inter-instance relationships. While our method retainsthe intra-instance property, it further captures inter-instance relationshipsby constructing k-nearest neighbor (KNN) graphs for both teacher and studentstreams during pretraining. In these graphs, nodes represent samples along withtheir latent representations. Edges encode the similarity between instances.Following pretraining, a representation refinement phase is performed. In thisphase, Graph Neural Networks (GNNs) propagate messages not only among immediateneighbors but also across multiple hops, thereby enabling broader contextualintegration. Experimental results on CIFAR-10, ImageNet-100, and ImageNet-1Kdemonstrate accuracy improvements of 7.3%, 3.2%, and 1.0%, respectively, overstate-of-the-art methods. These results highlight the effectiveness of theproposed graph based mechanism. The code is publicly available athttps://github.com/alijavidani/SSL-GraphNNCLR.</description>
      <author>example@mail.com (Ali Javidani, Babak Nadjar Araabi, Mohammad Amin Sadeghi)</author>
      <guid isPermaLink="false">2510.22322v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Does Homophily Help in Robust Test-time Node Classification?</title>
      <link>http://arxiv.org/abs/2510.22289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了一种名为GrapHoST的测试时图结构转换方法，通过调整测试图中的同质性来提高预训练图神经网络在节点分类任务上的鲁棒性和性能，无需重新训练模型。&lt;h4&gt;背景&lt;/h4&gt;同质性是现实世界图的基本属性，但现有方法主要关注训练图的学习。然而，测试图常面临数据质量问题和分布偏移，如社交网络中不同地区用户的领域偏移和引文网络中的时间演化偏移，这些因素会降低预训练模型的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提高预训练GNN模型在面对测试时数据质量问题和分布偏移情况下的鲁棒性和性能。&lt;h4&gt;方法&lt;/h4&gt;提出GrapHoST方法，开发同质性预测器来区分测试边，通过预测同质性得分的置信度实现自适应的测试时图结构转换。&lt;h4&gt;主要发现&lt;/h4&gt;通过增加同质性图中的同质性或减少异质性图中的同质性来转换测试图结构，可以显著提高预训练GNN在节点分类任务上的鲁棒性和性能，无需模型训练或更新。&lt;h4&gt;结论&lt;/h4&gt;在九个基准数据集上的实验表明，GrapHoST在各种测试时数据质量问题下始终实现了最先进的性能，最高提升达10.92%，代码已公开。&lt;h4&gt;翻译&lt;/h4&gt;同质性，即同一类别节点倾向于连接的特性，是现实世界图的基本属性，支撑着引文网络和社会网络等领域中的结构和语义模式。现有方法通过设计同质性感知的GNN架构或图结构学习策略来利用同质性，但它们主要关注训练图的GNN学习。然而，在现实场景中，测试图常常面临数据质量问题和分布偏移，如社交网络中不同地区用户之间的领域偏移，以及在不同时间段收集的引文网络图中的时间演化偏移。这些因素显著降低了预训练模型的鲁棒性，导致测试时性能下降。通过实证观察和理论分析，我们揭示出通过转换测试图结构——在同质性图中增加同质性或在异质性图中减少同质性——可以显著提高预训练GNN在节点分类任务上的鲁棒性和性能，无需模型训练或更新。基于这些见解，我们提出了一种基于同质性的新颖测试时图结构转换方法，名为GrapHoST。具体来说，开发了一个同质性预测器来区分测试边，通过预测同质性得分的置信度实现自适应的测试时图结构转换。在九个基准数据集上针对多种测试时数据质量问题的广泛实验表明，GrapHoST始终实现了最先进的性能，最高提升达10.92%。我们的代码已在https://github.com/YanJiangJerry/GrapHoST发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Homophily, the tendency of nodes from the same class to connect, is afundamental property of real-world graphs, underpinning structural and semanticpatterns in domains such as citation networks and social networks. Existingmethods exploit homophily through designing homophily-aware GNN architecturesor graph structure learning strategies, yet they primarily focus on GNNlearning with training graphs. However, in real-world scenarios, test graphsoften suffer from data quality issues and distribution shifts, such as domainshifts across users from different regions in social networks and temporalevolution shifts in citation network graphs collected over varying timeperiods. These factors significantly compromise the pre-trained model'srobustness, resulting in degraded test-time performance. With empiricalobservations and theoretical analysis, we reveal that transforming the testgraph structure by increasing homophily in homophilic graphs or decreasing itin heterophilic graphs can significantly improve the robustness and performanceof pre-trained GNNs on node classifications, without requiring model trainingor update. Motivated by these insights, a novel test-time graph structuraltransformation method grounded in homophily, named GrapHoST, is proposed.Specifically, a homophily predictor is developed to discriminate test edges,facilitating adaptive test-time graph structural transformation by theconfidence of predicted homophily scores. Extensive experiments on ninebenchmark datasets under a range of test-time data quality issues demonstratethat GrapHoST consistently achieves state-of-the-art performance, withimprovements of up to 10.92%. Our code has been released athttps://github.com/YanJiangJerry/GrapHoST.</description>
      <author>example@mail.com (Yan Jiang, Ruihong Qiu, Zi Huang)</author>
      <guid isPermaLink="false">2510.22289v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic Graph Neural Network for Data-Driven Physiologically Based Pharmacokinetic Modeling</title>
      <link>http://arxiv.org/abs/2510.22096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了使用深度学习替代传统生理药代动力学建模方法，提出了一种动态图神经网络(Dynamic GNN)来模拟器官间的相互作用，实现了更高的预测性能。&lt;h4&gt;背景&lt;/h4&gt;生理药代动力学(PBPK)建模在药物开发中通过预测器官间药物浓度动态发挥关键作用。传统方法依赖常微分方程和强简化假设，限制了其对非线性生理相互作用的适应性。&lt;h4&gt;目的&lt;/h4&gt;探索使用深度学习进行PBPK预测的数据驱动替代方法，以提高预测准确性和适应性。&lt;h4&gt;方法&lt;/h4&gt;实现两种基线架构：多层感知器(MLP)和长短期记忆(LSTM)网络，分别用于捕捉分子和时间依赖性。提出动态图神经网络(Dynamic GNN)将生理连接建模为器官间的递归消息传递过程。&lt;h4&gt;主要发现&lt;/h4&gt;动态GNN在所有模型中表现最佳，预测性能最高，R平方值为0.9342，均方根误差为0.0159，平均绝对误差为0.0116。相比之下，MLP基线获得R平方值0.8705，LSTM获得0.8059。明确建模器官相互作用的时空依赖性可实现更准确和可推广的药物浓度预测。&lt;h4&gt;结论&lt;/h4&gt;动态GNN为传统PBPK公式提供了可扩展的、无方程的替代方案，在临床前和临床研究中，数据驱动的药代动力学建模展现出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;生理药代动力学(PBPK)建模通过预测器官间药物浓度动态在药物开发中发挥关键作用。传统方法依赖常微分方程和强简化假设，限制了其对非线性生理相互作用的适应性。本研究探索了使用深度学习进行PBPK预测的数据驱动替代方法。实现了两种基线架构：多层感知器(MLP)和长短期记忆(LSTM)网络，分别用于捕捉分子和时间依赖性。为了整合器官间相互作用，我们提出了一种动态图神经网络(Dynamic GNN)，将生理连接建模为器官间的递归消息传递过程。实验结果表明，所提出的动态GNN在所有模型中实现了最高的预测性能，R平方值为0.9342，均方根误差为0.0159，平均绝对误差为0.0116。相比之下，MLP基线获得R平方值0.8705，LSTM获得0.8059。这些结果表明，明确建模器官相互作用的时空依赖性可实现更准确和可推广的药物浓度预测。动态GNN为传统PBPK公式提供了可扩展的、无方程的替代方案，并在临床前和临床研究中展现出数据驱动药代动力学建模的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physiologically Based Pharmacokinetic (PBPK) modeling plays a critical rolein drug development by predicting drug concentration dynamics across organs.Traditional approaches rely on ordinary differential equations with strongsimplifying assumptions, which limit their adaptability to nonlinearphysiological interactions. In this study, we explore data-driven alternativesfor PBPK prediction using deep learning. Two baseline architectures - amultilayer perceptron (MLP) and a long short-term memory (LSTM) network - areimplemented to capture molecular and temporal dependencies, respectively. Toincorporate inter-organ interactions, we propose a Dynamic Graph Neural Network(Dynamic GNN) that models physiological connections as recurrentmessage-passing processes between organs. Experimental results demonstrate thatthe proposed Dynamic GNN achieves the highest predictive performance among allmodels, with an R^2 of 0.9342, an RMSE of 0.0159, and an MAE of 0.0116. Incomparison, the MLP baseline obtains an R^2 of 0.8705 and the LSTM achieves0.8059. These results highlight that explicitly modeling the spatial andtemporal dependencies of organ interactions enables more accurate andgeneralizable drug concentration prediction. The Dynamic GNN provides ascalable, equation-free alternative to traditional PBPK formulations anddemonstrates strong potential for data-driven pharmacokinetic modeling inpreclinical and clinical research.</description>
      <author>example@mail.com (Su Liu, Xin Hu, Shurong Wen, Jiaqi Liu, Jiexi Xu, Lanruo Wang)</author>
      <guid isPermaLink="false">2510.22096v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Graph Networks for Accurate Weather Forecasting via Lightweight Training</title>
      <link>http://arxiv.org/abs/2510.22094v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了HiFlowCast和HiAntFlow两种层级图神经网络模型，通过创新机制提高气候事件预测准确性，同时降低训练成本和环境影响。&lt;h4&gt;背景&lt;/h4&gt;气候事件由复杂的全球尺度驱动因素导致的多元动态过程产生，对食物、能源和基础设施有深远影响。然而，由于物理过程跨越不同的时空尺度，固定分辨率方法无法捕捉这些过程，导致准确天气预测仍然困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够捕捉多尺度物理过程的气候预测方法，提高预测准确性，特别是对于极端事件。&lt;h4&gt;方法&lt;/h4&gt;提出HiFlowCast和其集合变体HiAntFlow，这是一种层级图神经网络，将物理学嵌入多尺度预测框架。创新点包括：1)潜在记忆保留机制，在向下遍历过程中保持全局趋势；2)从潜在到物理的分支，整合不同尺度上的偏微分方程解场。&lt;h4&gt;主要发现&lt;/h4&gt;在13天提前期的预测中，模型将误差减少了5%以上；在第一和第九十九百分位极端情况下，误差减少了5-8%，提高了罕见事件的可靠性。利用预训练模型权重，模型在一个周期内收敛，显著降低了训练成本和碳足迹。&lt;h4&gt;结论&lt;/h4&gt;提高预测效率对于应对机器学习规模增长带来的可持续性挑战和研究可及性限制至关重要，代码和模型权重见补充材料。&lt;h4&gt;翻译&lt;/h4&gt;气候事件源于由全球尺度驱动的复杂多变量动态过程，深刻影响食物、能源和基础设施。然而，由于物理过程跨越多样的时空尺度展开，固定分辨率方法无法捕捉，准确的天气预测仍然难以实现。层级图神经网络提供多尺度表示，但非线性向下映射通常会抹去全局趋势，削弱物理学与预测的整合。我们引入HiFlowCast及其集合变体HiAntFlow，这些图神经网络将物理学嵌入多尺度预测框架。两个创新支撑了它们的设计：潜在记忆保留机制在向下遍历过程中保持全局趋势，以及从潜在到物理的分支整合不同尺度上的偏微分方程解场。我们的模型在13天提前期将误差减少5%以上，在第一和第九十九百分位极端情况下减少5-8%，提高了罕见事件的可靠性。利用预训练模型权重，它们在一个周期内收敛，降低了训练成本和碳足迹。这种效率至关重要，因为机器学习规模的不断增长挑战可持续性并限制研究可及性。代码和模型权重见补充材料。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Climate events arise from intricate, multivariate dynamics governed byglobal-scale drivers, profoundly impacting food, energy, and infrastructure.Yet, accurate weather prediction remains elusive due to physical processesunfolding across diverse spatio-temporal scales, which fixed-resolution methodscannot capture. Hierarchical Graph Neural Networks (HGNNs) offer a multiscalerepresentation, but nonlinear downward mappings often erase global trends,weakening the integration of physics into forecasts. We introduce HiFlowCastand its ensemble variant HiAntFlow, HGNNs that embed physics within amultiscale prediction framework. Two innovations underpin their design: aLatent-Memory-Retention mechanism that preserves global trends during downwardtraversal, and a Latent-to-Physics branch that integrates PDE solution fieldsacross diverse scales. Our Flow models cut errors by over 5% at 13-day leadtimes and by 5-8% under 1st and 99th quantile extremes, improving reliabilityfor rare events. Leveraging pretrained model weights, they converge within asingle epoch, reducing training cost and their carbon footprint. Suchefficiency is vital as the growing scale of machine learning challengessustainability and limits research accessibility. Code and model weights are inthe supplementary materials.</description>
      <author>example@mail.com (Thomas Bailie, S. Karthik Mukkavilli, Varvara Vetrova, Yun Sing Koh)</author>
      <guid isPermaLink="false">2510.22094v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Pruning and Quantization Impact on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.22058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了神经网络压缩方法(剪枝和量化)对图神经网络(GNNs)的影响，发现非结构化细粒度和全局剪枝可显著减小模型大小(50%)同时保持或提高精度，而不同量化方法在不同数据集上有不同影响。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在图结构数据学习上具有高准确性，但面临高计算和资源成本问题。&lt;h4&gt;目的&lt;/h4&gt;研究神经网络压缩方法(剪枝和量化)如何减小GNN模型大小同时保持合理准确性。&lt;h4&gt;方法&lt;/h4&gt;在三个图数据集(Cora, Proteins, BBBBP)上评估三种剪枝方法和三种量化方法对三种GNN任务(图分类、节点分类和链接预测)的影响。&lt;h4&gt;主要发现&lt;/h4&gt;非结构化细粒度和全局剪枝可显著减小模型大小(50%)，同时在微调后保持甚至提高精度；不同量化方法对GNN的准确性、推理时间和模型大小在不同数据集上有不同影响。&lt;h4&gt;结论&lt;/h4&gt;神经网络压缩技术(特别是剪枝)可以有效减少GNN模型大小而不牺牲性能。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在从图结构数据学习方面以高精度著称，但它们面临高计算和资源成本的问题。神经网络压缩方法用于减小模型大小同时保持合理准确性。两种常见的神经网络压缩技术包括剪枝和量化。在这项研究中，我们实证检验了三种剪枝方法和三种量化方法对不同GNN模型的影响，包括图分类任务、节点分类任务和链接预测。我们在三个图数据集上进行了所有实验，包括Cora、Proteins和BBBP。我们的研究结果表明，非结构化细粒度和全局剪枝可以显著减小模型大小(50%)，同时在微调剪枝后的模型后保持甚至提高精度。对GNN上不同量化方法的评估显示，在不同数据集上，这些方法对准确性、推理时间和模型大小有不同影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are known to operate with high accuracy onlearning from graph-structured data, but they suffer from high computationaland resource costs. Neural network compression methods are used to reduce themodel size while maintaining reasonable accuracy. Two of the common neuralnetwork compression techniques include pruning and quantization. In thisresearch, we empirically examine the effects of three pruning methods and threequantization methods on different GNN models, including graph classificationtasks, node classification tasks, and link prediction. We conducted allexperiments on three graph datasets, including Cora, Proteins, and BBBP. Ourfindings demonstrate that unstructured fine-grained and global pruning cansignificantly reduce the model's size(50\%) while maintaining or even improvingprecision after fine-tuning the pruned model. The evaluation of differentquantization methods on GNN shows diverse impacts on accuracy, inference time,and model size across different datasets.</description>
      <author>example@mail.com (Khatoon Khedri, Reza Rawassizadeh, Qifu Wen, Mehdi Hosseinzadeh)</author>
      <guid isPermaLink="false">2510.22058v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>PF$Δ$: A Benchmark Dataset for Power Flow under Load, Generation, and Topology Variations</title>
      <link>http://arxiv.org/abs/2510.22048v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages, 14 figures. Accepted at NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了PFΔ，一个用于潮流计算的基准数据集，旨在解决电力系统操作中的计算瓶颈和不确定性挑战。&lt;h4&gt;背景&lt;/h4&gt;潮流计算是实时电网操作的基础，但存在计算瓶颈问题。可再生能源整合和气候引起的极端天气增加了电力系统操作的不确定性，需要能够准确高效模拟各种场景的工具。&lt;h4&gt;目的&lt;/h4&gt;引入一个能够捕捉负荷、发电和拓扑多样变化的潮流计算基准数据集PFΔ，以评估现有方法并确定未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;构建包含859,800个已解决潮流计算实例的数据集，涵盖六种不同总线系统规模，包含三种应急场景（N、N-1和N-2），以及接近稳态电压稳定性极限的案例。&lt;h4&gt;主要发现&lt;/h4&gt;评估了传统求解器和基于GNN的方法，突出了现有方法面临的挑战领域，并确定了未来研究的开放性问题。&lt;h4&gt;结论&lt;/h4&gt;PFΔ数据集和相关代码已公开发布，为电力系统潮流计算研究提供了新资源。&lt;h4&gt;翻译&lt;/h4&gt;潮流计算是实时电网操作的基础，贯穿于工作流程中，如contingency analysis（重复PF评估评估停电情况下的电网安全）和拓扑优化（涉及基于PF的组合式大动作空间搜索）。在操作时间尺度上运行这些计算或在大评估空间中运行仍然是主要的计算瓶颈。此外，可再生能源整合和气候引起的极端天气导致的电力系统操作中不断增加的不确定性，也需要能够准确高效地模拟各种场景和操作条件的工具。机器学习方法相比传统求解器提供了潜在的速度提升，但它们在捕捉现实世界变异性的基准上尚未得到系统性评估。本文引入了PFΔ，一个潮流计算的基准数据集，捕捉了负荷、发电和拓扑的多样变化。PFΔ包含859,800个已解决的潮流计算实例，涵盖六种不同总线系统规模，捕获三种类型的应急场景（N、N-1和N-2），并包括接近稳态电压稳定性极限的接近不可行案例。我们评估了传统求解器和基于GNN的方法，突出了现有方法遇到困难的领域，并确定了未来研究的开放性问题。我们的数据集可在https://huggingface.co/datasets/pfdelta/pfdelta/tree/main获取，我们的代码包含数据生成脚本和模型实现，位于https://github.com/MOSSLab-MIT/pfdelta。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Power flow (PF) calculations are the backbone of real-time grid operations,across workflows such as contingency analysis (where repeated PF evaluationsassess grid security under outages) and topology optimization (which involvesPF-based searches over combinatorially large action spaces). Running thesecalculations at operational timescales or across large evaluation spacesremains a major computational bottleneck. Additionally, growing uncertainty inpower system operations from the integration of renewables and climate-inducedextreme weather also calls for tools that can accurately and efficientlysimulate a wide range of scenarios and operating conditions. Machine learningmethods offer a potential speedup over traditional solvers, but theirperformance has not been systematically assessed on benchmarks that capturereal-world variability. This paper introduces PF$\Delta$, a benchmark datasetfor power flow that captures diverse variations in load, generation, andtopology. PF$\Delta$ contains 859,800 solved power flow instances spanning sixdifferent bus system sizes, capturing three types of contingency scenarios (N ,N -1, and N -2), and including close-to-infeasible cases near steady-statevoltage stability limits. We evaluate traditional solvers and GNN-basedmethods, highlighting key areas where existing approaches struggle, andidentifying open problems for future research. Our dataset is available athttps://huggingface.co/datasets/pfdelta/pfdelta/tree/main and our code withdata generation scripts and model implementations is athttps://github.com/MOSSLab-MIT/pfdelta.</description>
      <author>example@mail.com (Ana K. Rivera, Anvita Bhagavathula, Alvaro Carbonero, Priya Donti)</author>
      <guid isPermaLink="false">2510.22048v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Hybrid GNN-LSE Method for Fast, Robust, and Physically-Consistent AC Power Flow</title>
      <link>http://arxiv.org/abs/2510.22020v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合物理信息图神经网络(GNN)和线性状态估计(LSE)的两阶段混合方法，用于解决传统交流潮流求解器在大规模电力系统中的计算和收敛挑战。&lt;h4&gt;背景&lt;/h4&gt;传统的交流潮流(ACPF)求解器如牛顿-拉夫森法(NR)在现代大规模电力系统中面临显著的计算和收敛挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速且物理一致的电力系统求解方法，适用于实时操作和分析。&lt;h4&gt;方法&lt;/h4&gt;结合物理信息图神经网络与迭代线性状态估计：使用物理信息损失函数训练GNN预测高质量初始系统状态，然后通过LSE细化步骤解决线性方程以强制执行物理定律，绕过传统求解器的非线性和收敛问题。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在IEEE 33节点、69节点和118节点系统上得到验证；GNN变体比牛顿-拉夫森法快高达8400倍；LSE细化能快速获得物理一致解；重载压力测试和N-1 contingencies证明了方法的可靠性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该框架成功连接了快速数据驱动模型与电力系统物理约束，为实时电力系统操作和分析提供了实用工具。&lt;h4&gt;翻译&lt;/h4&gt;传统的交流潮流(ACPF)求解器如牛顿-拉夫森法(NR)在现代大规模电力系统中面临显著的计算和收敛挑战。本文提出了一种新颖的两阶段混合方法，结合物理信息图神经网络(GNN)和稳健的迭代线性状态估计(LSE)细化步骤，以产生快速且物理一致的解。使用具有高效动态加权方案的物理信息损失函数训练的GNN可快速预测高质量的初始系统状态。然后使用受状态估计技术启发的迭代直接线性求解器进行细化。LSE细化步骤解决一系列线性方程以强制执行物理定律，有效绕过传统求解器的非线性和收敛问题。所提出的GNN-LSE框架在从小的辐射状配电网(IEEE 33节点、69节点)到大型网状输电系统(IEEE 118节点)的各种系统上得到了全面验证。结果表明，我们的GNN变体比NR快高达8400倍。LSE细化提供了一条快速获得物理一致解的途径，而重载压力测试(标称值的120%-150%)和N-1 contingencies展示了该方法的可靠性和泛化能力。这项工作提出了一个强大而灵活的框架，用于连接快速的数据驱动模型与电力系统物理的严格约束，为实时操作和分析提供了实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional AC Power Flow (ACPF) solvers like Newton-Raphson (NR) facesignificant computational and convergence challenges in modern, large-scalepower systems. This paper proposes a novel, two-stage hybrid method thatintegrates a Physics-Informed Graph Neural Network (GNN) with a robust,iterative Linear State Estimation (LSE) refinement step to produce fast andphysically-consistent solutions. The GNN, trained with a physics-informed lossfunction featuring an efficient dynamic weighting scheme, rapidly predicts ahigh-quality initial system state. This prediction is then refined using aniterative, direct linear solver inspired by state estimation techniques. ThisLSE refinement step solves a series of linear equations to enforce physicallaws, effectively bypassing the non-linearities and convergence issues oftraditional solvers. The proposed GNN-LSE framework is comprehensivelyvalidated on systems ranging from small radial distribution networks (IEEE33-bus, 69-bus) to a large, meshed transmission system (IEEE 118-bus). Resultsshow that our GNN variants are up to $8.4 \times 10^3$ times faster than NR.The LSE refinement provides a fast route to a physically-consistent solution,while heavy-loading stress tests (120%-150% of nominal) and N-1 contingenciesdemonstrate the method's reliability and generalization. This work presents apowerful and flexible framework for bridging fast, data-driven models with therigorous constraints of power system physics, offering a practical tool forreal-time operations and analysis.</description>
      <author>example@mail.com (Mohamed Shamseldein)</author>
      <guid isPermaLink="false">2510.22020v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>A Multimodal Human Protein Embeddings Database: DeepDrug Protein Embeddings Bank (DPEB)</title>
      <link>http://arxiv.org/abs/2510.22008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DPEB是一个整合了四种蛋白质嵌入类型的数据集，用于提高蛋白质-蛋白质相互作用(PPI)预测的准确性，并在多种蛋白质分类任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;计算预测蛋白质-蛋白质相互作用(PPI)具有挑战性，主要原因是缺乏整合的多模态蛋白质表示方法。&lt;h4&gt;目的&lt;/h4&gt;创建一个整合多种蛋白质嵌入类型的数据集，填补AlphaFold2内部神经网络嵌入不可用的空白，为计算建模提供支持。&lt;h4&gt;方法&lt;/h4&gt;DPEB是一个包含22,043个人类蛋白质的精选集合，整合了四种嵌入类型：结构嵌入(AlphaFold2)、基于transformer的序列嵌入(BioEmbeddings)、上下文氨基酸模式(ESM-2)和基于序列的n-gram统计(ProtVec)。&lt;h4&gt;主要发现&lt;/h4&gt;GraphSAGE与BioEmbedding结合实现了最高的PPI预测性能(87.37% AUROC, 79.16%准确率)；该框架在酶分类任务上达到77.42%的准确率；在蛋白质家族分类任务上达到86.04%的准确率。&lt;h4&gt;结论&lt;/h4&gt;DPEB支持多种图神经网络方法进行PPI预测，可应用于系统生物学、药物靶点识别、通路分析和疾病机制研究。&lt;h4&gt;翻译&lt;/h4&gt;计算预测蛋白质-蛋白质相互作用(PPI)具有挑战性，由于缺乏整合的多模态蛋白质表示。DPEB是一个包含22,043个人类蛋白质的精选集合，整合了四种嵌入类型：结构(AlphaFold2)、基于transformer的序列(BioEmbeddings)、上下文氨基酸模式(ESM-2: Evolutionary Scale Modeling)和基于序列的n-gram统计(ProtVec)。AlphaFold2蛋白质结构可通过公共数据库(如AlphaFold2蛋白质结构数据库)获取，但内部神经网络嵌入不可用。DPEB通过提供AlphaFold2衍生的嵌入用于计算建模来填补这一空白。我们的基准评估显示，GraphSAGE与BioEmbedding结合实现了最高的PPI预测性能(87.37% AUROC, 79.16%准确率)。该框架在酶分类上实现了77.42%的准确率，在蛋白质家族分类上实现了86.04%的准确率。DPEB支持多种图神经网络方法进行PPI预测，能够在系统生物学、药物靶点识别、通路分析和疾病机制研究中应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computationally predicting protein-protein interactions (PPIs) is challengingdue to the lack of integrated, multimodal protein representations. DPEB is acurated collection of 22,043 human proteins that integrates four embeddingtypes: structural (AlphaFold2), transformer-based sequence (BioEmbeddings),contextual amino acid patterns (ESM-2: Evolutionary Scale Modeling), andsequence-based n-gram statistics (ProtVec]). AlphaFold2 protein structures areavailable through public databases (e.g., AlphaFold2 Protein StructureDatabase), but the internal neural network embeddings are not. DPEB addressesthis gap by providing AlphaFold2-derived embeddings for computational modeling.Our benchmark evaluations show GraphSAGE with BioEmbedding achieved the highestPPI prediction performance (87.37% AUROC, 79.16% accuracy). The framework alsoachieved 77.42% accuracy for enzyme classification and 86.04% accuracy forprotein family classification. DPEB supports multiple graph neural networkmethods for PPI prediction, enabling applications in systems biology, drugtarget identification, pathway analysis, and disease mechanism studies.</description>
      <author>example@mail.com (Md Saiful Islam Sajol, Magesh Rajasekaran, Hayden Gemeinhardt, Adam Bess, Chris Alvin, Supratik Mukhopadhyay)</author>
      <guid isPermaLink="false">2510.22008v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning on Real-World Graphs</title>
      <link>http://arxiv.org/abs/2510.21994v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The thesis was submitted for the degree of Doctor of Philosophy in  Computing at Imperial College London (February 2024), under the supervision  of Prof. Michael M. Bronstein. It includes work published at ICML, ICLR,  NeurIPS, and the Learning on Graphs Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一系列图神经网络模型，解决了GNNs在实际应用中的关键挑战，包括可扩展性、时间性、方向性、数据不完整性和结构不确定性等问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已成为学习图结构数据的核心工具，但在实际系统中的应用受到可扩展性、时间性、方向性、数据不完整性和结构不确定性等关键挑战的限制。&lt;h4&gt;目的&lt;/h4&gt;解决GNNs在实际应用中的限制，使其能够应用于工业规模的图数据。&lt;h4&gt;方法&lt;/h4&gt;作者提出了五个模型：SIGN用于可扩展图学习，TGN用于时间图，Dir-GNN用于有向和异质网络，Feature Propagation用于处理缺失节点特征，NuGget用于博弈论结构推断。&lt;h4&gt;主要发现&lt;/h4&gt;这些模型共同弥合了学术基准和工业规模图之间的差距，使GNNs能够在社交系统和推荐系统等领域使用。&lt;h4&gt;结论&lt;/h4&gt;通过这些创新模型，GNNs的实际应用限制得到了解决，使其能够在真实世界系统中有效应用。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络已成为学习图结构数据的核心工具，但它们在实际系统中的应用受到可扩展性、时间性、方向性、数据不完整性和结构不确定性等关键挑战的限制。本论文引入了一系列解决这些限制的模型：SIGN用于可扩展图学习，TGN用于时间图，Dir-GNN用于有向和异质网络，Feature Propagation (FP)用于学习缺失节点特征，NuGget用于博弈论结构推断。这些贡献共同弥合了学术基准和工业规模图之间的差距，使GNNs能够在社交和推荐系统等领域使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.25560/112863&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have become a central tool for learning ongraph-structured data, yet their applicability to real-world systems remainslimited by key challenges such as scalability, temporality, directionality,data incompleteness, and structural uncertainty. This thesis introduces aseries of models addressing these limitations: SIGN for scalable graphlearning, TGN for temporal graphs, Dir-GNN for directed and heterophilicnetworks, Feature Propagation (FP) for learning with missing node features, andNuGget for game-theoretic structural inference. Together, these contributionsbridge the gap between academic benchmarks and industrial-scale graphs,enabling the use of GNNs in domains such as social and recommender systems.</description>
      <author>example@mail.com (Emanuele Rossi)</author>
      <guid isPermaLink="false">2510.21994v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging Classical Algorithms for Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.21574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探索了通过在经典算法上预训练图神经网络(GNNs)来提升其在分子属性预测任务上的性能。研究证明将经典算法的先验知识嵌入到GNNs中能够提供有用的归纳偏置，从而提升在复杂真实世界图数据上的表现。&lt;h4&gt;背景&lt;/h4&gt;神经网络在处理非结构化数据方面表现出色，但通常无法分布外泛化；而经典算法虽然保证正确性但缺乏灵活性。&lt;h4&gt;目的&lt;/h4&gt;探索通过在经典算法上预训练图神经网络(GNNs)来改善其在Open Graph Benchmark上的分子属性预测任务中的性能，包括ogbg-molhiv(HIV抑制)和ogbg-molclintox(临床毒性)任务。&lt;h4&gt;方法&lt;/h4&gt;使用从CLRS算法推理基准中的24个经典算法训练的GNNs，来初始化并冻结第二GNN的选定层，用于分子预测任务。&lt;h4&gt;主要发现&lt;/h4&gt;与随机初始化的基线相比，预训练模型取得了一致的胜利或平局。其中，基于Segments Intersect算法的预训练在ogbg-molhiv上取得了6%的绝对增益，基于Dijkstra的预训练在ogbg-molclintox上取得了3%的增益。&lt;h4&gt;结论&lt;/h4&gt;将经典算法的先验知识嵌入到GNNs中可以提供有用的归纳偏置，提高在复杂、真实世界图数据上的性能。&lt;h4&gt;翻译&lt;/h4&gt;神经网络在处理非结构化数据方面表现出色，但通常无法分布外泛化，而经典算法虽然保证正确性但缺乏灵活性。我们探索了通过在经典算法上预训练图神经网络(GNNs)来改善其在Open Graph Benchmark上的分子属性预测任务中的性能，包括ogbg-molhiv(HIV抑制)和ogbg-molclintox(临床毒性)任务。使用从CLRS算法推理基准中的24个经典算法训练的GNNs，来初始化并冻结第二GNN的选定层用于分子预测。与随机初始化的基线相比，预训练模型取得了一致的胜利或平局，其中基于Segments Intersect算法的预训练在ogbg-molhiv上取得了6%的绝对增益，基于Dijkstra的预训练在ogbg-molclintox上取得了3%的增益。这些结果表明将经典算法的先验知识嵌入到GNNs中可以提供有用的归纳偏置，提高在复杂、真实世界图数据上的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neural networks excel at processing unstructured data but often fail togeneralise out-of-distribution, whereas classical algorithms guaranteecorrectness but lack flexibility. We explore whether pretraining Graph NeuralNetworks (GNNs) on classical algorithms can improve their performance onmolecular property prediction tasks from the Open Graph Benchmark: ogbg-molhiv(HIV inhibition) and ogbg-molclintox (clinical toxicity). GNNs trained on 24classical algorithms from the CLRS Algorithmic Reasoning Benchmark are used toinitialise and freeze selected layers of a second GNN for molecular prediction.Compared to a randomly initialised baseline, the pretrained models achieveconsistent wins or ties, with the Segments Intersect algorithm pretrainingyielding a 6% absolute gain on ogbg-molhiv and Dijkstra pretraining achieving a3% gain on ogbg-molclintox. These results demonstrate embedding classicalalgorithmic priors into GNNs provides useful inductive biases, boostingperformance on complex, real-world graph data.</description>
      <author>example@mail.com (Jason Wu, Petar Veličković)</author>
      <guid isPermaLink="false">2510.21574v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>HollowFlow: Efficient Sample Likelihood Evaluation using Hollow Message Passing</title>
      <link>http://arxiv.org/abs/2510.21542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为HollowFlow的流模型，利用非回溯图神经网络解决大规模系统中的计算瓶颈问题，实现了高达O(n²)的加速，使基于流的生成模型能够应用于更大规模的科学问题。&lt;h4&gt;背景&lt;/h4&gt;流和扩散模型已成为科学应用的强大工具，特别适用于采样非归一化概率分布，如玻尔兹曼生成器(BGs)。然而，这些模型在实际部署时面临关键挑战：它们依赖于样本似然计算，而这种计算的系统规模n呈指数级增长，使得大规模问题难以处理。&lt;h4&gt;目的&lt;/h4&gt;为了解决流模型在大规模系统中的计算效率问题，作者引入了HollowFlow，旨在显著提高似然评估速度，使BGs能够扩展到更大的系统。&lt;h4&gt;方法&lt;/h4&gt;作者提出了HollowFlow，一种利用新型非回溯图神经网络(NoBGNN)的基于流的生成模型。通过强制块对角雅可比结构，HollowFlow的似然评估可以在n中用常数次反向传播完成。该框架具有普适性，任何等变GNN或基于注意力的架构都可以被适配为NoBGNN。&lt;h4&gt;主要发现&lt;/h4&gt;作者通过在两个不同规模的系统上训练BGs验证了HollowFlow。对于这两个系统，采样和似然评估时间都显著减少，遵循了理论上的缩放规律。对于较大的系统，作者获得了100倍的加速，展示了基于HollowFlow的方法在高维科学问题上的潜力。&lt;h4&gt;结论&lt;/h4&gt;HollowFlow为基于流的生成模型在大规模科学问题中的应用提供了有效解决方案，通过创新的图神经网络架构显著提高了计算效率，使得以前因计算限制而无法处理的高维问题现在变得可行。&lt;h4&gt;翻译&lt;/h4&gt;流和扩散模型已成为科学应用的强大工具，特别适用于采样非归一化概率分布，如玻尔兹曼生成器(BGs)。部署这些模型的一个关键挑战是它们依赖于样本似然计算，而这种计算的系统规模n呈指数级增长，通常使得大规模问题变得不可行。为了解决这个问题，我们引入了HollowFlow，这是一种基于流的生成模型，利用了一种新型的非回溯图神经网络(NoBGNN)。通过强制块对角雅可比结构，HollowFlow的似然评估可以在n中用常数次反向传播完成，实现高达O(n²)的加速：这是将BGs扩展到更大系统的重要一步。重要的是，我们的框架具有普适性：任何等变GNN或基于注意力的架构都可以被适配为NoBGNN。我们通过在两个不同规模的系统上训练BGs来验证HollowFlow。对于这两个系统，采样和似然评估时间都显著减少，遵循了理论上的缩放规律。对于较大的系统，我们获得了100倍的加速，清楚地展示了基于HollowFlow的方法在高维科学问题上的潜力，这些问题以前因计算瓶颈而受到阻碍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Flow and diffusion-based models have emerged as powerful tools for scientificapplications, particularly for sampling non-normalized probabilitydistributions, as exemplified by Boltzmann Generators (BGs). A criticalchallenge in deploying these models is their reliance on sample likelihoodcomputations, which scale prohibitively with system size $n$, often renderingthem infeasible for large-scale problems. To address this, we introduce$\textit{HollowFlow}$, a flow-based generative model leveraging a novelnon-backtracking graph neural network (NoBGNN). By enforcing a block-diagonalJacobian structure, HollowFlow likelihoods are evaluated with a constant numberof backward passes in $n$, yielding speed-ups of up to $\mathcal{O}(n^2)$: asignificant step towards scaling BGs to larger systems. Crucially, ourframework generalizes: $\textbf{any equivariant GNN or attention-basedarchitecture}$ can be adapted into a NoBGNN. We validate HollowFlow by trainingBGs on two different systems of increasing size. For both systems, the samplingand likelihood evaluation time decreases dramatically, following ourtheoretical scaling laws. For the larger system we obtain a $10^2\times$speed-up, clearly illustrating the potential of HollowFlow-based approaches forhigh-dimensional scientific problems previously hindered by computationalbottlenecks.</description>
      <author>example@mail.com (Johann Flemming Gloy, Simon Olsson)</author>
      <guid isPermaLink="false">2510.21542v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Estimating Treatment Effects in Networks using Domain Adversarial Training</title>
      <link>http://arxiv.org/abs/2510.21457v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了HINet方法，通过结合图神经网络和领域对抗训练，解决了网络环境中估计异质治疗效应时面临的干扰、未知暴露映射和网络层面协变量偏移等问题。&lt;h4&gt;背景&lt;/h4&gt;在网络环境中估计异质治疗效应受到干扰困扰，一个实例的结果可能受到他人治疗状态的影响。现有方法通常假设已知的暴露映射，这往往不现实。同质性与治疗分配机制的相互作用可能导致网络层面的协变量偏移，进而导致治疗效应估计不准确，这种现象尚未被明确研究。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在未知暴露映射下估计治疗效应的方法，同时减轻网络层面协变量偏移的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了HINet，一种新颖的方法，结合了图神经网络和领域对抗训练。这种组合允许在未知暴露映射下估计治疗效应，同时减轻网络层面协变量偏移的影响。&lt;h4&gt;主要发现&lt;/h4&gt;在合成和半合成网络数据集上的广泛实证评估证明了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;HINet方法成功解决了网络环境中估计异质治疗效应的挑战。&lt;h4&gt;翻译&lt;/h4&gt;在网络环境中估计异质治疗效应因干扰而复杂化，这意味着一个实例的结果可能受到他人治疗状态的影响。现有的因果机器学习方法通常假设已知的暴露映射，该映射总结了给定实例的结果如何受他人治疗的影响，这是一种简化的假设，通常不切实际。此外，同质性——相似实例倾向于连接——与治疗分配机制之间的相互作用可能引发网络层面的协变量偏移，可能导致不准确的治疗效应估计，这种现象尚未被明确研究。为了应对这些挑战，我们提出了HINet，一种将图神经网络与领域对抗训练相结合的新颖方法。这种组合允许在未知暴露映射的情况下估计治疗效应，同时减轻（网络层面）协变量偏移的影响。在合成和半合成网络数据集上的广泛实证评估证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating heterogeneous treatment effects in network settings is complicatedby interference, meaning that the outcome of an instance can be influenced bythe treatment status of others. Existing causal machine learning approachesusually assume a known exposure mapping that summarizes how the outcome of agiven instance is influenced by others' treatment, a simplification that isoften unrealistic. Furthermore, the interaction between homophily -- thetendency of similar instances to connect -- and the treatment assignmentmechanism can induce a network-level covariate shift that may lead toinaccurate treatment effect estimates, a phenomenon that has not yet beenexplicitly studied. To address these challenges, we propose HINet, a novelmethod that integrates graph neural networks with domain adversarial training.This combination allows estimating treatment effects under unknown exposuremappings while mitigating the impact of (network-level) covariate shift. Anextensive empirical evaluation on synthetic and semi-synthetic network datasetsdemonstrates the effectiveness of our approach.</description>
      <author>example@mail.com (Daan Caljon, Jente Van Belle, Wouter Verbeke)</author>
      <guid isPermaLink="false">2510.21457v1</guid>
      <pubDate>Tue, 28 Oct 2025 15:34:08 +0800</pubDate>
    </item>
    <item>
      <title>Modest-Align: Data-Efficient Alignment for Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Modest-Align是一个轻量级跨模态对齐框架，通过随机扰动和嵌入平滑两种策略解决资源受限场景下的过度自信问题，在保持高性能的同时大幅减少训练数据和计算资源需求。&lt;h4&gt;背景&lt;/h4&gt;跨模态对齐旨在将异构模态映射到共享潜在空间，CLIP等模型通过大规模预训练获得强大识别能力，但在资源受限、数据有限或质量低的情况下，这些模型常因模糊或弱相关的图像-文本对而出现过度自信和性能下降。&lt;h4&gt;目的&lt;/h4&gt;设计一个轻量级的对齐框架，提高在资源受限场景下的鲁棒性和效率，解决模型的过度自信问题。&lt;h4&gt;方法&lt;/h4&gt;提出Modest-Align框架，采用两种互补策略：随机扰动引入受控噪声模拟不确定性，嵌入平滑校准嵌入空间中的相似度分布，共同减少过度自信并提高对噪声或弱对齐样本的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在多个基准数据集上的实验表明，Modest-Align在检索任务中优于最先进方法，使用超过100倍少的训练数据和600倍少的GPU时间达到与CLIP竞争的结果。&lt;h4&gt;结论&lt;/h4&gt;Modest-Align为现实世界中资源受限的跨模态对齐问题提供了实用且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;跨模态对齐旨在将异构模态映射到共享的潜在空间，如CLIP等模型所示，这些模型通过大规模图像-文本预训练获得强大的识别能力。然而，在资源受限、数据有限或质量低的环境中，由于模糊或弱相关的图像-文本对普遍存在，这些模型常常过度自信且性能下降。当前依赖单一正样本对的对比学习方法进一步加剧了这一问题，通过强化对不确定样本的过度自信。为应对这些挑战，我们提出了Modest-Align，一个为鲁棒性和效率而设计的轻量级对齐框架。我们的方法采用两种互补策略——随机扰动，引入受控噪声来模拟不确定性；以及嵌入平滑，校准嵌入空间中的相似度分布。这些机制共同减少过度自信并提高对噪声或弱对齐样本的性能。在多个基准数据集上的广泛实验表明，Modest-Align在检索任务中优于最先进方法，使用超过100倍少的训练数据和600倍少的GPU时间达到与CLIP竞争的结果。我们的方法为现实世界中资源受限的跨模态对齐问题提供了实用且可扩展的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-modal alignment aims to map heterogeneous modalities into a sharedlatent space, as exemplified by models like CLIP, which benefit fromlarge-scale image-text pretraining for strong recognition capabilities.However, when operating in resource-constrained settings with limited orlow-quality data, these models often suffer from overconfidence and degradedperformance due to the prevalence of ambiguous or weakly correlated image-textpairs. Current contrastive learning approaches, which rely on single positivepairs, further exacerbate this issue by reinforcing overconfidence on uncertainsamples. To address these challenges, we propose Modest-Align, a lightweightalignment framework designed for robustness and efficiency. Our approachleverages two complementary strategies -- Random Perturbation, which introducescontrolled noise to simulate uncertainty, and Embedding Smoothing, whichcalibrates similarity distributions in the embedding space. These mechanismscollectively reduce overconfidence and improve performance on noisy or weaklyaligned samples. Extensive experiments across multiple benchmark datasetsdemonstrate that Modest-Align outperforms state-of-the-art methods in retrievaltasks, achieving competitive results with over 100x less training data and 600xless GPU time than CLIP. Our method offers a practical and scalable solutionfor cross-modal alignment in real-world, low-resource scenarios.</description>
      <author>example@mail.com (Jiaxiang Liu, Yuan Wang, Jiawei Du, Joey Tianyi Zhou, Mingkun Xu, Zuozhu Liu)</author>
      <guid isPermaLink="false">2510.21606v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
  <item>
      <title>Visual Diffusion Models are Geometric Solvers</title>
      <link>http://arxiv.org/abs/2510.21697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://kariander1.github.io/visual-geo-solver/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究展示了视觉扩散模型可以作为有效的几何求解器，通过在像素空间工作直接解决几何问题。研究者将这种方法应用于三个著名的几何难题：内接正方形问题、斯坦纳树问题和简单多边形问题。&lt;h4&gt;背景&lt;/h4&gt;几何问题求解一直是挑战性问题，尤其是内接正方形问题（询问每个约旦曲线是否包含四个点形成正方形）等长期未解决的难题。前期工作需要专门的架构和领域特定的适应来将扩散应用于参数化几何表示。&lt;h4&gt;目的&lt;/h4&gt;展示视觉扩散模型作为几何求解器的有效性，探索在像素空间直接推理几何问题的可能性，并开发一种通用框架来近似解决著名的几何难题。&lt;h4&gt;方法&lt;/h4&gt;将每个问题实例视为图像，训练标准视觉扩散模型将高斯噪声转换为代表有效近似解的图像。模型学习将嘈杂的几何结构转换为正确配置，将几何推理重新表述为图像生成过程。&lt;h4&gt;主要发现&lt;/h4&gt;视觉扩散模型能够有效解决内接正方形问题、斯坦纳树问题和简单多边形问题；模型能够将嘈杂的几何结构转换为正确配置；生成模型与几何问题解决之间存在联系；在图像空间操作提供了一种通用框架来近似解决著名难题。&lt;h4&gt;结论&lt;/h4&gt;视觉扩散模型可以作为有效的几何求解器，在图像空间操作为近似解决著名难题提供了通用且实用的框架，为解决更广泛的挑战性几何任务开辟了新途径。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们展示了视觉扩散模型可以作为有效的几何求解器：它们通过在像素空间工作，能够直接推理几何问题。我们首先在内接正方形问题上证明了这一点，这是几何学中的一个长期未解决的问题，询问每个约旦曲线是否包含四个点形成正方形。然后我们将这种方法扩展到其他两个著名的难解几何问题：斯坦纳树问题和简单多边形问题。我们的方法将每个问题实例视为图像，并训练一个标准的视觉扩散模型，该模型将高斯噪声转换为代表有效近似解的图像，该解与精确解非常匹配。模型学习将嘈杂的几何结构转换为正确配置，有效地将几何推理重新表述为图像生成。与之前需要专门架构和领域特定适应的工作不同，我们使用在问题视觉表示上操作的标准视觉扩散模型。这种简单性突显了生成模型与几何问题解决之间令人惊讶的联系。除了这里研究的具体问题外，我们的结果指向一个更广泛的范式：在图像空间操作为近似解决著名难题提供了通用且实用的框架，并为解决更广泛的挑战性几何任务开辟了新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we show that visual diffusion models can serve as effectivegeometric solvers: they can directly reason about geometric problems by workingin pixel space. We first demonstrate this on the Inscribed Square Problem, along-standing problem in geometry that asks whether every Jordan curve containsfour points forming a square. We then extend the approach to two otherwell-known hard geometric problems: the Steiner Tree Problem and the SimplePolygon Problem.  Our method treats each problem instance as an image and trains a standardvisual diffusion model that transforms Gaussian noise into an imagerepresenting a valid approximate solution that closely matches the exact one.The model learns to transform noisy geometric structures into correctconfigurations, effectively recasting geometric reasoning as image generation.  Unlike prior work that necessitates specialized architectures anddomain-specific adaptations when applying diffusion to parametric geometricrepresentations, we employ a standard visual diffusion model that operates onthe visual representation of the problem. This simplicity highlights asurprising bridge between generative modeling and geometric problem solving.Beyond the specific problems studied here, our results point toward a broaderparadigm: operating in image space provides a general and practical frameworkfor approximating notoriously hard problems, and opens the door to tackling afar wider class of challenging geometric tasks.</description>
      <author>example@mail.com (Nir Goren, Shai Yehezkel, Omer Dahary, Andrey Voynov, Or Patashnik, Daniel Cohen-Or)</author>
      <guid isPermaLink="false">2510.21697v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>OpenHype: Hyperbolic Embeddings for Hierarchical Open-Vocabulary Radiance Fields</title>
      <link>http://arxiv.org/abs/2510.21441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出OpenHype方法，使用连续双曲潜在空间表示3D场景层次结构，实现了更高效的3D场景理解，优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;建模3D对象和3D场景的内在层次结构对自主代理理解环境至关重要，但使用隐式表示如神经辐射场实现这一目标仍面临挑战。现有显式建模层次结构的方法存在局限性：要么需要多次渲染增加推理时间，要么依赖预定义的封闭集离散层次结构，难以泛化到真实世界的多样化结构。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效表示3D场景层次结构的方法，解决现有方法在推理效率和泛化能力方面的局限性，实现对3D场景更全面、高效的理解。&lt;h4&gt;方法&lt;/h4&gt;提出OpenHype，使用连续双曲潜在空间表示场景层次结构。利用双曲几何特性自然编码多尺度关系，通过潜在空间中的测地线路径实现层次结构的平滑遍历。&lt;h4&gt;主要发现&lt;/h4&gt;OpenHype在标准基准测试中优于最先进的方法，展示了在3D场景理解方面卓越的效率和适应性。&lt;h4&gt;结论&lt;/h4&gt;通过利用双曲几何性质，OpenHype提供了表示和探索3D场景层次结构的有效方式，解决了现有方法的效率和泛化局限性，为自主代理的环境理解提供了更强大工具。&lt;h4&gt;翻译&lt;/h4&gt;建模3D对象和3D场景的内在层次结构是非常可取的，因为它能够使自主代理更全面地理解环境。使用隐式表示（如神经辐射场）来实现这一点仍然是一个未被探索的挑战。明确建模层次结构的现有方法通常面临显著限制：它们要么需要多次渲染传递来捕获不同粒度级别的嵌入，显著增加了推理时间；要么依赖于预定义的封闭集离散层次结构，难以泛化到代理在真实世界中遇到的多样化且细微的结构。为解决这些挑战，我们提出了OpenHype，一种使用连续双曲潜在空间表示场景层次结构的新方法。通过利用双曲几何的特性，OpenHype自然编码了多尺度关系，并能够通过潜在空间中的测地线路径实现层次结构的平滑遍历。我们的方法在标准基准测试中优于最先进的方法，展示了在3D场景理解方面卓越的效率和适应性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效表示3D场景中的层次结构问题，特别是在使用神经辐射场(NeRF)等隐式表示时。这个问题很重要，因为理解3D场景的层次结构对自主代理全面理解环境至关重要，例如物体由多个部分组成，也可以在更高层次上语义分组，这种层次组织对于语义分割、场景重建和物体检测等应用非常关键。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有方法的局限性进行思考，发现现有方法要么需要多次渲染增加推理时间，要么依赖预定义的离散层次结构泛化能力差。他们借鉴了双曲几何的思想，因为双曲空间的指数扩展特性能够自然编码多尺度关系。方法借鉴了现有工作如使用CLIP提取语言特征、使用中性词减少噪声等，但创新性地将双曲几何应用于3D场景层次表示，实现了连续层次遍历。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用双曲空间的几何特性自然编码3D场景的层次结构，实现连续的多尺度关系表示。整体流程包括：1)双曲自编码器训练：将语言对齐特征转换为双曲空间表示，高层对象靠近原点，低层对象靠近边界；2)NeRF训练：监督模型预测双曲特征，使用双曲距离作为损失函数；3)层次遍历：通过沿测地线路径连续遍历层次，解码特征并计算与文本提示的相似度，使用softmax加权聚合结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次在3D场景理解中应用双曲空间自然编码层次结构；2)实现连续层次遍历，只需一次渲染而非多次；3)提出特征外推技术解决多视图一致性问题；4)改进的softmax加权聚合方法处理复杂查询。相比之前工作，OpenHype无需预定义离散层次或多次渲染，能连续遍历层次结构，在处理组合查询时表现更好，解决了现有方法的'词袋效应'问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; OpenHype通过双曲空间几何特性实现了在神经辐射场中连续、高效地表示和遍历3D场景层次结构，显著提升了开放词汇3D场景理解能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling the inherent hierarchical structure of 3D objects and 3D scenes ishighly desirable, as it enables a more holistic understanding of environmentsfor autonomous agents. Accomplishing this with implicit representations, suchas Neural Radiance Fields, remains an unexplored challenge. Existing methodsthat explicitly model hierarchical structures often face significantlimitations: they either require multiple rendering passes to captureembeddings at different levels of granularity, significantly increasinginference time, or rely on predefined, closed-set discrete hierarchies thatgeneralize poorly to the diverse and nuanced structures encountered by agentsin the real world. To address these challenges, we propose OpenHype, a novelapproach that represents scene hierarchies using a continuous hyperbolic latentspace. By leveraging the properties of hyperbolic geometry, OpenHype naturallyencodes multi-scale relationships and enables smooth traversal of hierarchiesthrough geodesic paths in latent space. Our method outperforms state-of-the-artapproaches on standard benchmarks, demonstrating superior efficiency andadaptability in 3D scene understanding.</description>
      <author>example@mail.com (Lisa Weijler, Sebastian Koch, Fabio Poiesi, Timo Ropinski, Pedro Hermosilla)</author>
      <guid isPermaLink="false">2510.21441v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>ZING-3D: Zero-shot Incremental 3D Scene Graphs via Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.21069v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ZING-3D是一个创新的框架，能够生成丰富语义表示的3D场景图，支持增量更新和几何基础，适用于机器人应用。&lt;h4&gt;背景&lt;/h4&gt;理解和推理复杂的3D环境需要结构化的场景表示，捕获物体及其语义和空间关系。现有3D场景图生成工作利用预训练VLMs但存在局限：局限于单视图设置、不支持增量更新、缺乏3D空间几何基础。&lt;h4&gt;目的&lt;/h4&gt;提出ZING-3D框架，利用预训练基础模型实现开放词汇识别，零样本生成丰富场景语义表示，支持增量更新和3D空间几何基础，适用于下游机器人应用。&lt;h4&gt;方法&lt;/h4&gt;利用VLM推理生成丰富2D场景图，使用深度信息与3D空间关联；节点表示开放词汇对象(含特征、3D位置、语义上下文)，边捕获空间和语义关系(含对象间距离)。&lt;h4&gt;主要发现&lt;/h4&gt;在Replica和HM3D数据集上的实验表明，ZING-3D能有效捕获空间和关系知识，无需特定任务训练。&lt;h4&gt;结论&lt;/h4&gt;ZING-3D解决了现有方法的局限性，适用于下游机器人应用。&lt;h4&gt;翻译&lt;/h4&gt;理解和推理复杂的3D环境需要结构化的场景表示，这些表示不仅要捕获物体，还要捕获它们的语义和空间关系。虽然最近关于3D场景图生成的工作利用了没有针对特定任务微调的预训练VLMs，但它们主要局限于单视图设置，无法支持随着新观察到来时的增量更新，并且缺乏在3D空间中的明确几何基础，所有这些对于具身场景都是必不可少的。在本文中，我们提出了ZING-3D框架，它利用预训练基础模型的丰富知识，实现开放词汇识别，并以零样本方式生成丰富的场景语义表示，同时支持在3D空间中进行增量更新和几何基础，使其适用于下游机器人应用。我们的方法利用VLM推理生成丰富的2D场景图，并使用深度信息将其与3D关联。节点表示具有特征、3D位置和语义上下文的开放词汇对象，而边捕获具有对象间距离的空间和语义关系。我们在来自Replica和HM3D数据集的场景上的实验表明，ZING-3D能够在无需特定任务训练的情况下有效地捕获空间和关系知识。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决现有3D场景图生成方法的三个限制：依赖特定词汇表、只能在单张2D图像上操作、无法增量更新。这个问题在现实中很重要，因为机器人需要在动态环境中在线构建和更新对3D环境的理解，而现有方法无法处理现实世界中的新物体或关系，也无法捕捉不同视角间的空间一致性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者从嵌入式AI代理在环境中探索的角度设计系统，考虑如何随着新观测的加入逐步构建场景理解。他们借鉴了Vision-Language Models(VLMs)的进步，特别是像Open-World SGG和Pixels-to-Graphs等利用预训练模型进行零样本关系推理的工作。同时，他们结合了深度信息实现3D几何定位，并使用Grounded-SAM2提供精确的物体分割掩码，将2D场景表示提升到3D空间。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练视觉-语言模型的广泛知识，实现零样本开放词汇表识别，同时支持场景图的增量更新和3D空间中的几何定位。整体流程包括：1)使用VLM进行开放词汇表物体检测；2)构建2D场景图，捕获物体间的空间和语义关系；3)使用Grounded-SAM2获取精确分割掩码，结合深度信息将物体投影到3D空间；4)随着机器人探索，增量更新全局3D场景图；5)根据导航任务需求进行场景图剪枝。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)零样本嵌入式场景图生成，无需任务特定训练；2)丰富的语义信息，节点包含物体特征、3D位置和房间类型，边表示精确的空间关系；3)支持增量更新，场景图随探索过程动态演进。相比之前工作，ZING-3D的独特之处在于结合了2D视觉推理与3D几何信息，支持增量更新，实现了真正的开放词汇表识别，并提供了任务导向的场景图剪枝功能，更适合机器人实际应用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ZING-3D通过结合视觉-语言模型与3D几何信息，实现了零样本增量式3D场景图生成，为机器人在复杂环境中的导航和交互提供了结构化的语义-空间表示。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding and reasoning about complex 3D environments requires structuredscene representations that capture not only objects but also their semantic andspatial relationships. While recent works on 3D scene graph generation haveleveraged pretrained VLMs without task-specific fine-tuning, they are largelyconfined to single-view settings, fail to support incremental updates as newobservations arrive and lack explicit geometric grounding in 3D space, all ofwhich are essential for embodied scenarios. In this paper, we propose, ZING-3D,a framework that leverages the vast knowledge of pretrained foundation modelsto enable open-vocabulary recognition and generate a rich semanticrepresentation of the scene in a zero-shot manner while also enablingincremental updates and geometric grounding in 3D space, making it suitable fordownstream robotics applications. Our approach leverages VLM reasoning togenerate a rich 2D scene graph, which is grounded in 3D using depthinformation. Nodes represent open-vocabulary objects with features, 3Dlocations, and semantic context, while edges capture spatial and semanticrelations with inter-object distances. Our experiments on scenes from theReplica and HM3D dataset show that ZING-3D is effective at capturing spatialand relational knowledge without the need of task-specific training.</description>
      <author>example@mail.com (Pranav Saxena, Jimmy Chiun)</author>
      <guid isPermaLink="false">2510.21069v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Stuck in the Matrix: Probing Spatial Reasoning in Large Language Models</title>
      <link>http://arxiv.org/abs/2510.20198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 24 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过五项任务测试了大型语言模型在文本输入上的空间推理能力，发现模型在简单任务中表现中等，但随着复杂性和规模增加，性能显著下降，平均准确率损失42.7%，最高达84%，表明LLMs在空间推理方面存在明显局限性。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在自然语言处理方面表现出色，但其空间推理能力尚未被充分研究。&lt;h4&gt;目的&lt;/h4&gt;探究大型语言模型在文本输入上的空间理解和计算能力，识别其在空间推理方面的优势和局限。&lt;h4&gt;方法&lt;/h4&gt;设计并实施了五项空间推理任务：象限识别、几何变换、距离评估、单词搜索和滑块拼图。这些任务在结构化网格环境中进行，通过增加网格尺寸来提高复杂性，要求模型从简单模式识别扩展到抽象空间推理。&lt;h4&gt;主要发现&lt;/h4&gt;1) 模型在复杂性和规模较小的任务中表现中等；2) 随着规模增加，性能迅速下降，准确率平均损失42.7%，最高达84%；3) 所有初始准确率超过50%的测试显示至少48%的性能损失；4) 模型在扩展复杂性方面的挣扎暗示其底层架构中缺乏强大的空间表示。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型在语言推理和空间推理之间存在明显差距，本研究揭示了其当前局限性，并为未来在语言和几何交叉领域的集成基准研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;本文通过一套五项任务，探究了大型语言模型对文本输入的空间推理能力，旨在测试它们的空间理解和计算能力。模型在基于结构化网格环境中的基本空间推理和多步问题解决方面接受了测试，使用了象限识别、几何变换、距离评估、单词搜索和滑块拼图等任务。每个任务通过增加网格尺寸来提高复杂性，要求模型超越简单的模式识别，进入抽象空间推理。我们的结果显示，虽然大型语言模型在复杂性和规模较小的所有任务中表现出中等成功，但随着规模增加，性能迅速下降，准确率平均损失42.7%，最高达到84%。所有初始准确率超过50%的测试都显示出至少48%的损失，说明了性能下降的一致性。此外，模型在扩展复杂性方面的挣扎暗示其底层架构中缺乏强大的空间表示。本文强调了大型语言模型中语言推理和空间推理之间的差距，提供了对其当前局限性的见解，并为未来在语言和几何交叉领域的集成基准研究奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the spatial reasoning capability of large language models(LLMs) over textual input through a suite of five tasks aimed at probing theirspatial understanding and computational abilities. The models were tested onboth fundamental spatial reasoning and multi-step problem-solving withinstructured grid-based environments using tasks such as quadrant identification,geometric transformations, distance evaluation, word searches, and tilesliding. Each task was scaled in complexity through increasing grid dimensions,requiring models to extend beyond simple pattern recognition into abstractspatial reasoning. Our results reveal that while LLMs demonstrate moderatesuccess in all tasks with small complexity and size, performance drops offrapidly as scale increases, with an average loss in accuracy of 42.7%, andreaching as high as 84%. Every test that began with over 50% accuracy showed aloss of at least 48%, illustrating the consistent nature of the deterioration.Furthermore, their struggles with scaling complexity hint at a lack of robustspatial representations in their underlying architectures. This paperunderscores the gap between linguistic and spatial reasoning in LLMs, offeringinsights into their current limitations, and laying the groundwork for futureintegrative benchmarks at the intersection of language and geometry.</description>
      <author>example@mail.com (Maggie Bai, Ava Kim Cohen, Eleanor Koss, Charlie Lichtenbaum)</author>
      <guid isPermaLink="false">2510.20198v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Uncertainty evaluation of segmentation models for Earth observation</title>
      <link>http://arxiv.org/abs/2510.19586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了从卫星影像中估计语义分割预测不确定性的方法，针对遥感地球观测应用对现有方法进行了基准测试，评估了不确定性度量的实际效用，并提出了实用建议。&lt;h4&gt;背景&lt;/h4&gt;与标准图像分类相比，分割中的不确定性估计面临独特挑战，需要可扩展的方法来生成逐像素估计。大多数相关研究集中在场景理解或医学影像领域。&lt;h4&gt;目的&lt;/h4&gt;专门针对遥感地球观测应用对不确定性估计方法进行基准测试，评估不确定性度量的实际效用，测试它们识别预测错误和噪声损坏的输入图像区域的能力。&lt;h4&gt;方法&lt;/h4&gt;在两个遥感数据集PASTIS和ForTy上进行实验，这些数据集在规模、地理覆盖范围和标签置信度方面存在差异。评估包括多种模型（如随机分割网络和集成方法）与多种神经网络架构和不确定性度量相结合的广泛评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验评估了不同不确定性估计方法在遥感应用中的表现，确定了哪些方法更适合识别预测错误和噪声损坏区域。&lt;h4&gt;结论&lt;/h4&gt;基于研究结果提出了若干实用建议，为遥感影像语义分割中的不确定性估计提供了指导。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了从卫星影像中估计语义分割预测不确定性的方法。与标准图像分类相比，分割中的不确定性估计面临独特挑战，需要可扩展的方法来生成逐像素估计。虽然大多数关于此主题的研究集中在场景理解或医学影像上，但这项工作专门针对遥感地球观测应用对现有方法进行了基准测试。我们的评估侧重于不确定性度量的实际效用，测试它们识别预测错误和噪声损坏的输入图像区域的能力。实验在两个遥感数据集PASTIS和ForTy上进行，这两个数据集在规模、地理覆盖范围和标签置信度方面存在差异。我们进行了广泛的评估，结合了多种模型（如随机分割网络和集成方法）与多种神经网络架构和不确定性度量。根据我们的发现，我们提出了若干实用建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper investigates methods for estimating uncertainty in semanticsegmentation predictions derived from satellite imagery. Estimating uncertaintyfor segmentation presents unique challenges compared to standard imageclassification, requiring scalable methods producing per-pixel estimates. Whilemost research on this topic has focused on scene understanding or medicalimaging, this work benchmarks existing methods specifically for remote sensingand Earth observation applications. Our evaluation focuses on the practicalutility of uncertainty measures, testing their ability to identify predictionerrors and noise-corrupted input image regions. Experiments are conducted ontwo remote sensing datasets, PASTIS and ForTy, selected for their differencesin scale, geographic coverage, and label confidence. We perform an extensiveevaluation featuring several models, such as Stochastic Segmentation Networksand ensembles, in combination with a number of neural architectures anduncertainty metrics. We make a number of practical recommendations based on ourfindings.</description>
      <author>example@mail.com (Melanie Rey, Andriy Mnih, Maxim Neumann, Matt Overlan, Drew Purves)</author>
      <guid isPermaLink="false">2510.19586v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Seeing Across Views: Benchmarking Spatial Reasoning of Vision-Language Models in Robotic Scenes</title>
      <link>http://arxiv.org/abs/2510.19400v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The project and benchmark are publicly available at  https://github.com/microsoft/MV-RoboBench&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出MV-RoboBench基准测试，用于评估视觉语言模型在机器人操作中的多视图空间推理能力。研究显示当前最先进模型表现远低于人类水平，并发现空间智能与机器人任务执行呈正相关，但单视图基准表现不能可靠预测多视图机器人任务表现。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型对具身人工智能至关重要，是视觉语言动作模型的基础。然而大多数VLM评估集中在单视图设置，对多视图信息整合能力的探索不足。多摄像头设置在机器人平台上越来越标准，能提供互补视角以缓解遮挡和深度模糊问题。&lt;h4&gt;目的&lt;/h4&gt;填补VLMs多视图空间推理能力评估的空白，专门设计一个基准测试来评估VLMs在机器人操作中的多视图空间推理能力。&lt;h4&gt;方法&lt;/h4&gt;创建MV-RoboBench基准测试，包含8个子任务中的1.7k个手动筛选的问答项目，分为空间理解和机器人执行两个主要类别。评估多种现有VLMs（包括开源和闭源模型）以及采用CoT启发技术的增强版本。&lt;h4&gt;主要发现&lt;/h4&gt;(i)在多视图机器人场景中，空间智能和机器人任务执行呈正相关；(ii)在现有通用单视图空间理解基准上的良好表现并不能可靠地转化为在机器人空间任务中的成功。&lt;h4&gt;结论&lt;/h4&gt;当前最先进的VLMs在多视图机器人感知方面仍面临重大挑战。作者发布MV-RoboBench作为开放资源，旨在促进空间感知VLMs和VLAs的进步，提供数据和多视图具身推理的标准化评估协议。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型对具身人工智能至关重要，使机器人能够感知、推理并在复杂环境中行动。它们也是最近视觉语言动作模型的基础。然而，大多数VLM评估集中在单视图设置，对其整合多视图信息的能力探索不足。与此同时，多摄像头设置在机器人平台上越来越标准，因为它们提供互补视角以缓解遮挡和深度模糊问题。因此，VLMs是否能有效利用此类多视图输入进行机器人推理仍然是一个开放问题。为填补这一空白，我们引入MV-RoboBench，一个专门设计用于评估VLMs在机器人操作中多视图空间推理能力的基准测试。MV-RoboBench包含8个子任务中的1.7k个手动筛选的问答项目，分为两个主要类别：空间理解和机器人执行。我们评估了多种现有的VLMs，包括开源和闭源模型，以及采用CoT启发技术的增强版本。结果显示，最先进的模型表现远低于人类水平，突显了VLMs在多视图机器人感知方面面临的重大挑战。此外，我们的分析揭示了两个关键发现：(i)在多视图机器人场景中，空间智能和机器人任务执行呈正相关；(ii)在现有通用单视图空间理解基准上的良好表现并不能可靠地转化为在我们基准评估的机器人空间任务中的成功。我们发布MV-RoboBench作为开放资源，旨在促进空间感知VLMs和VLAs的进步，不仅提供数据，还提供多视图具身推理的标准化评估协议。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的问题是评估视觉语言模型（VLMs）在机器人场景中的多视图空间推理能力。这个问题很重要，因为现有的VLM评估大多集中在单视图设置，而机器人平台越来越多地采用多摄像头系统来提供互补视角以克服遮挡和深度模糊问题。理解VLM能否有效整合这些多视图信息对提升机器人在复杂环境中的感知和决策能力至关重要，也是实现先进视觉语言动作（VLA）模型的基础。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有空间推理基准的局限性，发现它们大多专注于单视图数据或非具身任务，而机器人操作场景需要多视图感知能力。他们借鉴了ShareRobot（具身机器人任务但无多视图）、All-Angles Bench和Ego3D-Bench（多视图但仅限导航或照片对齐）等工作，设计了MV-RoboBench，一个专门针对机器人操作场景中多视图空间推理的基准。作者构建了多阶段管道：数据收集（从AgiWorld和BridgeV2数据集筛选）、问答生成（为八个子任务设计模板）和人工质保审查，确保基准质量和多样性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是评估VLMs能否有效整合多个摄像头视图的互补信息，支持机器人在现实世界中的决策。基准包含1700多个人工策划的问答项目，分为空间理解（跨视图匹配、距离判断、视角识别、3D空间一致性）和机器人执行（动作规划、步骤执行、轨迹选择、功能识别）两大类。实现流程包括：1)数据收集（规则过滤+GPT-4.1辅助筛选+人工验证）；2)问答生成（任务特定模板+五选一问答对构建）；3)人工质保审查（迭代审查+内容修正+答案分布平衡）；4)模型评估（统一零样本提示+准确率作为指标）。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个整合空间和机器人推理与多视图输入的机器人操作基准；2)系统性评估VLMs整合多视图信息的能力；3)发现空间智能与机器人执行在多视图场景中正相关；4)揭示单视图基准性能不能可靠转移到多视图机器人任务。相比之前工作，MV-RoboBench专注于具身多视图推理而非抽象任务；使用真实机器人演示而非模板生成；同时评估空间理解和机器人执行；强调多视图互补信息整合而非单一视角分析。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MV-RoboBench是首个专门针对机器人操作场景中多视图空间推理能力的基准测试，通过系统评估现有视觉语言模型的表现，揭示了它们在整合多视图信息进行机器人决策方面的显著不足，并为未来具身人工智能和多视图感知研究提供了新的评估标准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models (VLMs) are essential to Embodied AI, enabling robotsto perceive, reason, and act in complex environments. They also serve as thefoundation for the recent Vision-Language-Action (VLA) models. Yet mostevaluations of VLMs focus on single-view settings, leaving their ability tointegrate multi-view information underexplored. At the same time, multi-camerasetups are increasingly standard in robotic platforms, as they providecomplementary perspectives to mitigate occlusion and depth ambiguity. WhetherVLMs can effectively leverage such multi-view inputs for robotic reasoningtherefore remains an open question. To bridge this gap, we introduceMV-RoboBench, a benchmark specifically designed to evaluate the multi-viewspatial reasoning capabilities of VLMs in robotic manipulation. MV-RoboBenchconsists of 1.7k manually curated QA items across eight subtasks, divided intotwo primary categories: spatial understanding and robotic execution. Weevaluate a diverse set of existing VLMs, including both open-source andclosed-source models, along with enhanced versions incorporating CoT-inspiredtechniques. The results show that state-of-the-art models remain far belowhuman performance, underscoring the substantial challenges VLMs face inmulti-view robotic perception. Additionally, our analysis uncovers two keyfindings: (i) spatial intelligence and robotic task execution are positivelycorrelated in multi-view robotic scenarios; and (ii) strong performance onexisting general-purpose single-view spatial understanding benchmarks does notreliably translate to success in the robotic spatial tasks assessed by ourbenchmark. We release MV-RoboBench as an open resource to foster progress inspatially grounded VLMs and VLAs, providing not only data but also astandardized evaluation protocol for multi-view embodied reasoning.</description>
      <author>example@mail.com (Zhiyuan Feng, Zhaolu Kang, Qijie Wang, Zhiying Du, Jiongrui Yan, Shubin Shi, Chengbo Yuan, Huizhi Liang, Yu Deng, Qixiu Li, Rushuai Yang, Arctanx An, Leqi Zheng, Weijie Wang, Shawn Chen, Sicheng Xu, Yaobo Liang, Jiaolong Yang, Baining Guo)</author>
      <guid isPermaLink="false">2510.19400v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Scale Shift in Crowd Localization under the Context of Domain Generalization</title>
      <link>http://arxiv.org/abs/2510.19330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人群定位中的尺度偏移问题及其在域泛化场景下的影响，提出了Catto算法来减轻尺度偏移的影响，并建立了ScaleBench基准测试。&lt;h4&gt;背景&lt;/h4&gt;人群定位在视觉场景理解中扮演关键角色，但现有方法因训练和测试数据之间头部尺度分布差异（尺度偏移）导致性能显著下降，这一问题被称为域泛化挑战。&lt;h4&gt;目的&lt;/h4&gt;理解人群定位模型在域泛化背景下尺度偏移的本质，解决四个关键问题：尺度偏移如何影响人群定位、如何量化这种影响、产生原因以及如何减轻影响。&lt;h4&gt;方法&lt;/h4&gt;系统检查不同尺度偏移水平下人群定位性能变化；建立ScaleBench基准测试，重现20种先进域泛化算法；提供尺度偏移的严格理论分析；提出因果特征分解和各向异性处理（Catto）算法。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验展示了现有算法的局限性；强调了尺度偏移的重要性和复杂性；提供了四个对未来研究有重要意义的见解。&lt;h4&gt;结论&lt;/h4&gt;强调了'尺度偏移域泛化'这一新颖且适用的研究方向的重要性。&lt;h4&gt;翻译&lt;/h4&gt;人群定位在视觉场景理解中扮演关键角色，用于预测人群中每个行人的位置，因此适用于各种下游任务。然而，由于训练和测试数据之间头部尺度分布的差异（尺度偏移），现有方法性能显著下降，这一挑战被称为域泛化（DG）。本文旨在理解在人群定位模型的域泛化背景下尺度偏移的本质。为此，我们解决了四个关键问题：(i) 尺度偏移如何在域泛化场景中影响人群定位？(ii) 如何量化这种影响？(iii) 产生这种影响的原因是什么？(iv) 如何减轻这种影响？首先，我们系统地检查了人群定位性能如何随不同水平的尺度偏移而变化。然后，我们建立了一个基准ScaleBench，重现了20种先进的域泛化算法来量化这种影响。通过大量实验，我们展示了现有算法的局限性，并强调了尺度偏移的重要性和复杂性，这是一个尚未充分探索的主题。为了加深理解，我们对尺度偏移提供了严格的理论分析。基于这些见解，我们进一步提出了一种名为因果特征分解和各向异性处理（Catto）的有效算法，以减轻域泛化设置中尺度偏移的影响。随后，我们还提供了大量的分析实验，揭示了四个对未来研究有重要意义的见解。我们的结果强调了这一新颖且适用的研究方向的重要性，我们称之为尺度偏移域泛化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crowd localization plays a crucial role in visual scene understanding towardspredicting each pedestrian location in a crowd, thus being applicable tovarious downstream tasks. However, existing approaches suffer from significantperformance degradation due to discrepancies in head scale distributions (scaleshift) between training and testing data, a challenge known as domaingeneralization (DG). This paper aims to comprehend the nature of scale shiftwithin the context of domain generalization for crowd localization models. Tothis end, we address four critical questions: (i) How does scale shiftinfluence crowd localization in a DG scenario? (ii) How can we quantify thisinfluence? (iii) What causes this influence? (iv) How to mitigate theinfluence? Initially, we conduct a systematic examination of how crowdlocalization performance varies with different levels of scale shift. Then, weestablish a benchmark, ScaleBench, and reproduce 20 advanced DG algorithms toquantify the influence. Through extensive experiments, we demonstrate thelimitations of existing algorithms and underscore the importance and complexityof scale shift, a topic that remains insufficiently explored. To deepen ourunderstanding, we provide a rigorous theoretical analysis on scale shift.Building on these insights, we further propose an effective algorithm calledCausal Feature Decomposition and Anisotropic Processing (Catto) to mitigate theinfluence of scale shift in DG settings. Later, we also provide extensiveanalytical experiments, revealing four significant insights for futureresearch. Our results emphasize the importance of this novel and applicableresearch direction, which we term Scale Shift Domain Generalization.</description>
      <author>example@mail.com (Juncheng Wang, Lei Shang, Ziqi Liu, Wang Lu, Xixu Hu, Zhe Hu, Jindong Wang, Shujun Wang)</author>
      <guid isPermaLink="false">2510.19330v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>MoTVLA: A Vision-Language-Action Model with Unified Fast-Slow Reasoning</title>
      <link>http://arxiv.org/abs/2510.18337v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MoTVLA是一种基于混合变压器的视觉-语言-动作模型，结合了快速-慢速统一推理与行为策略学习，解决了现有方法中语言控制能力有限和推理延迟显著的问题。&lt;h4&gt;背景&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中是增强机器人开放世界泛化能力的热门研究方向。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够平衡语言控制能力和执行效率的模型，解决现有方法中的两个主要挑战。&lt;h4&gt;方法&lt;/h4&gt;MoTVLA模型保留了预训练视觉语言模型的通用智能，同时引入一个与预训练模型共享知识的领域专家transformer，生成领域特定的快速推理，并将动作专家基于分解的运动指令进行条件化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛评估，MoTVLA在快速-慢速推理和操作任务性能方面表现出优越性，能够学习多样化行为并显著提高语言控制能力。&lt;h4&gt;结论&lt;/h4&gt;MoTVLA成功整合了快速-慢速统一推理与行为策略学习，有效解决了现有方法中的局限性，为机器人学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将视觉语言指令整合到视觉运动策略中正在增强机器人学习的开放世界泛化能力方面获得动力。尽管有 promising 的进展，现有方法面临两个挑战：在不使用生成推理作为条件时，语言控制能力有限，或者在整合推理时推理延迟显著。在这项工作中，我们引入了MoTVLA，一种基于混合变压器(MoT)的视觉-语言-动作(VLA)模型，它整合了快速-慢速统一推理与行为策略学习。MoTVLA保留了预训练VLMs的通用智能（作为通用者）用于感知、场景理解和语义规划等任务，同时整合了一个领域专家（第二个transformer），它与预训练VLM共享知识，以生成领域特定的快速推理（例如机器人运动分解），从而提高策略执行效率。通过将动作专家基于分解的运动指令进行条件化，MoTVLA能够学习多样化行为并显著提高语言控制能力。在自然语言处理基准、机器人仿真环境和真实世界实验中的广泛评估证实了MoTVLA在快速-慢速推理和操作任务性能方面的优越性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决在机器人学习中整合视觉语言指令时面临的两个挑战：一是当不使用生成的推理作为条件时语言控制能力有限，二是当整合推理时推理延迟显著。这个问题很重要，因为它限制了机器人在开放世界中的泛化能力和实时应用，影响了机器人在需要快速响应和精确控制的环境中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有方法的局限性：传统视觉语言动作模型在连续动作表示上存在问题，扩散策略虽然适合连续动作空间但语言控制能力有限。他们提出通过'分解-组合-再分解'的混合变换器架构统一快速和慢速推理。该方法借鉴了混合变换器架构、预训练视觉语言模型、扩散策略等现有工作，并参考了BAGEL模型中的Vision Transformer和Qwen2.5 LLM的文本分词器。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过混合变换器架构统一快速和慢速推理，在一个模型中同时保留通用智能和领域特定知识，使用'分解-组合-再分解'流程实现知识共享。整体流程包括：输入空间设计(处理语言、RGB图像和可学习查询)；推理骨干设计(通用专家负责慢速推理，领域专家负责快速推理)；推理输出设计(统一在文本空间但分为两种功能)；动作专家设计(使用扩散变换器生成动作)；训练流程(领域专家微调和动作专家扩散策略训练)；推理流程(支持对话模式和动作模式两种交互方式)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)统一快速-慢速推理的MoT架构；2)基于分解运动条件的策略学习；3)支持对话和动作的双模式操作。相比之前的工作，MoTVLA解决了连续动作表示的精度问题，显式生成推理内容提高语言控制能力，显著降低推理延迟，并通过知识共享避免了灾难性遗忘，实现了更好的知识保留和执行效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MoTVLA通过混合变换器架构统一了快速和慢速推理，在保持通用视觉语言模型智能的同时，实现了高效、可解释的机器人操作策略学习，显著提升了语言控制能力和任务执行效率。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Integrating visual-language instructions into visuomotor policies is gainingmomentum in robot learning for enhancing open-world generalization. Despitepromising advances, existing approaches face two challenges: limited languagesteerability when no generated reasoning is used as a condition, or significantinference latency when reasoning is incorporated. In this work, we introduceMoTVLA, a mixture-of-transformers (MoT)-based vision-language-action (VLA)model that integrates fast-slow unified reasoning with behavior policylearning. MoTVLA preserves the general intelligence of pre-trained VLMs(serving as the generalist) for tasks such as perception, scene understanding,and semantic planning, while incorporating a domain expert, a secondtransformer that shares knowledge with the pretrained VLM, to generatedomain-specific fast reasoning (e.g., robot motion decomposition), therebyimproving policy execution efficiency. By conditioning the action expert ondecomposed motion instructions, MoTVLA can learn diverse behaviors andsubstantially improve language steerability. Extensive evaluations acrossnatural language processing benchmarks, robotic simulation environments, andreal-world experiments confirm the superiority of MoTVLA in both fast-slowreasoning and manipulation task performance.</description>
      <author>example@mail.com (Wenhui Huang, Changhe Chen, Han Qi, Chen Lv, Yilun Du, Heng Yang)</author>
      <guid isPermaLink="false">2510.18337v3</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>RoboGPT-R1: Enhancing Robot Planning with Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.14828v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为RoboGPT-R1的两阶段微调框架，用于提高具身智能体在长期操作任务中的推理能力，显著提升了模型在EmbodiedBench基准测试上的表现。&lt;h4&gt;背景&lt;/h4&gt;提高具身智能体的推理能力对机器人在长期操作任务中成功完成复杂人类指令至关重要。尽管基于监督微调的大语言模型和视觉语言模型在规划任务中取得成功，但在复杂真实环境中的长期操作任务仍面临挑战，这是由于它们有限的常识和推理能力。&lt;h4&gt;目的&lt;/h4&gt;解决通用视觉语言模型通过监督微调对机器人规划任务的泛化能力差和物理理解不足的问题。&lt;h4&gt;方法&lt;/h4&gt;提出RoboGPT-R1框架，第一阶段通过监督训练使用专家序列获取基础知识，第二阶段使用强化学习解决模型在视觉空间理解和推理方面的不足。设计基于规则的奖励函数，同时考虑长期性能和环境中的动作约束，以实现多步推理任务中的物理理解和动作序列一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，提高了21.33%，超越了在Qwen2.5-VL-7B上训练的其他工作，提高了20.33%。&lt;h4&gt;结论&lt;/h4&gt;RoboGPT-R1框架有效提高了具身智能体的推理能力和长期操作任务表现。&lt;h4&gt;翻译&lt;/h4&gt;提高具身智能体的推理能力对于机器人在长期操作任务中成功完成复杂的人类指令至关重要。尽管基于监督微调的大语言模型和视觉语言模型在规划任务中取得了成功，但由于其有限的常识和推理能力，它们在复杂真实环境中执行长期操作任务时仍面临挑战。考虑到通过监督微调将通用视觉语言模型与机器人规划任务对齐存在泛化能力差和物理理解不足的问题，我们提出了RoboGPT-R1，一个用于具身规划的两阶段微调框架。在该框架中，监督训练通过专家序列获取基础知识，随后使用强化学习来解决模型在视觉空间理解和推理方面的不足。为了在多步推理任务中实现物理理解和动作序列一致性，我们设计了一个基于规则的奖励函数，同时考虑长期性能和环境中的动作约束。在Qwen2.5-VL-3B上训练的推理模型在EmbodiedBench基准测试上显著优于更大规模的GPT-4o-mini模型，提高了21.33%，并超越了在Qwen2.5-VL-7B上训练的其他工作，提高了20.33%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improving the reasoning capabilities of embodied agents is crucial for robotsto complete complex human instructions in long-view manipulation taskssuccessfully. Despite the success of large language models and vision languagemodels based on Supervised Fine-Tuning (SFT) in planning tasks, they continuefacing challenges in performing long-horizon manipulation tasks in complexreal-world environments, owing to their restricted common sense and reasoningcapabilities. Considering that aligning general-purpose vision language modelsto robotic planning tasks via supervised fine-tuning suffers from poorgeneralization and insufficient physical understanding, we propose RoboGPT-R1,a two-stage fine-tuning framework for embodied planning. In this framework,supervised training acquires foundational knowledge through expert sequences,followed by RL to address the model's shortcomings in visual-spatialunderstanding and reasoning. To achieve physical understanding and actionsequence consistency in multi-step reasoning tasks, we design a rule-basedreward function that simultaneously considers long-horizon performance andaction constraint in the environment. The reasoning model, trained onQwen2.5-VL-3B, significantly outperforms the larger-scale model, GPT-4o-mini,by 21.33% and surpasses other work trained on Qwen2.5-VL-7B by 20.33% on theEmbodiedBench benchmark.</description>
      <author>example@mail.com (Jinrui Liu, Bingyan Nie, Boyu Li, Yaran Chen, Yuze Wang, Shunsen He, Haoran Li)</author>
      <guid isPermaLink="false">2510.14828v2</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>DAP-MAE: Domain-Adaptive Point Cloud Masked Autoencoder for Effective Cross-Domain Learning</title>
      <link>http://arxiv.org/abs/2510.21635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DAP-MAE的领域自适应点云掩码自编码器方法，用于解决跨领域点云数据整合问题，提高下游3D点云分析任务性能。&lt;h4&gt;背景&lt;/h4&gt;与2D数据相比，可用于训练的点云数据在不同领域中规模有限，研究人员尝试结合不同领域数据进行MAE预训练以缓解数据稀缺问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够自适应整合跨领域数据集知识的方法，以改善通用点云分析任务性能。&lt;h4&gt;方法&lt;/h4&gt;设计了异构领域适配器，在预训练阶段使用适配模式学习跨领域点云信息，在微调阶段采用融合模式增强特征；同时引入领域特征生成器指导点云特征适应下游任务。&lt;h4&gt;主要发现&lt;/h4&gt;仅通过一次预训练，DAP-MAE在四种点云分析任务上表现优异，在ScanObjectNN上的目标分类达到95.18%，在Bosphorus上的面部表情识别达到88.45%。&lt;h4&gt;结论&lt;/h4&gt;DAP-MAE有效解决了跨领域点云数据整合问题，提高了下游任务性能，为点云分析提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;与2D数据相比，可用于训练的点云数据在不同领域的规模相当有限。研究人员一直在尝试结合这些不同领域的数据进行掩码自编码器预训练，以利用这种数据稀缺问题。然而，从混合领域学到的先验知识可能与下游3D点云分析任务不太匹配，导致性能下降。为解决这一问题，我们提出了领域自适应点云掩码自编码器，这是一种MAE预训练方法，可以自适应地整合跨领域数据集的知识，用于通用点云分析。在DAP-MAE中，我们设计了一个异构领域适配器，在预训练期间使用适配模式，使模型能够全面学习来自不同领域点云的信息，同时在微调阶段采用融合模式以增强点云特征。同时，DAP-MAE集成了一个领域特征生成器，指导点云特征适应各种下游任务。仅通过一次预训练，DAP-MAE在四种不同的点云分析任务上取得了优异的性能，在ScanObjectNN上的目标分类达到95.18%，在Bosphorus上的面部表情识别达到88.45%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决点云数据在不同领域（物体、人脸、场景）之间的迁移学习问题。在现实中，3D点云数据的收集和标注需要大量资源，导致各领域可用数据有限。现有方法通常只在单一领域内进行预训练，当应用于不同领域任务时性能显著下降。解决这个问题对于实现通用3D点云分析至关重要，可应用于自动驾驶、机器人、增强/虚拟现实等领域，有效利用有限的数据资源并提高模型泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有点云MAE方法的局限性：单一领域预训练导致跨领域性能下降，简单组合多领域数据也会因信息误解为噪声而降低性能。基于此，他们设计了DAP-MAE框架，包含异构领域适配器(HDA)和领域特征生成器(DFG)两个核心组件。该方法借鉴了掩码自编码器(MAE)的自监督学习框架、Transformer架构、PointNet的点云处理方法以及对比学习技术，但创新性地将其应用于跨领域点云学习场景，实现了单模态一次预训练适应多任务的目标。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过异构领域适配器和领域特征生成器，使模型能够协作学习来自不同领域的点云数据，实现一次预训练适应多种下游任务。整体流程分为两阶段：1)预训练阶段：使用来自物体、人脸、场景三个领域的数据，通过HDA的适应模式分别处理各领域数据，使用Transformer编码器-解码器架构进行掩码重建，同时DFG提取领域特征并通过对比损失训练；2)微调阶段：针对特定下游任务，使用HDA的融合模式整合多领域信息，DFG提取领域和类别特征，输入任务头进行训练。这种方法既保留了各领域的特性，又实现了跨领域知识的有效迁移。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首次提出DAP-MAE框架，实现单模态一次预训练适应多任务；2)设计异构领域适配器(HDA)，预训练时使用适应模式分别处理不同领域，微调时使用融合模式整合信息；3)引入领域特征生成器(DFG)提取多样化领域特征指导下游任务。相比之前工作：与传统MAE不同，DAP-MAE能跨领域学习；与简单组合多领域数据的方法不同，它避免将跨域信息误解为噪声；与跨模态方法不同，它专注于单模态点云数据降低训练成本；在多个下游任务上实现了优于其他自监督方法的性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; DAP-MAE通过异构领域适配器和领域特征生成器实现了跨领域点云数据的有效协作学习，仅需一次预训练就能在多种3D点云分析任务上达到顶尖性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compared to 2D data, the scale of point cloud data in different domainsavailable for training, is quite limited. Researchers have been trying tocombine these data of different domains for masked autoencoder (MAE)pre-training to leverage such a data scarcity issue. However, the priorknowledge learned from mixed domains may not align well with the downstream 3Dpoint cloud analysis tasks, leading to degraded performance. To address such anissue, we propose the Domain-Adaptive Point Cloud Masked Autoencoder (DAP-MAE),an MAE pre-training method, to adaptively integrate the knowledge ofcross-domain datasets for general point cloud analysis. In DAP-MAE, we design aheterogeneous domain adapter that utilizes an adaptation mode duringpre-training, enabling the model to comprehensively learn information frompoint clouds across different domains, while employing a fusion mode in thefine-tuning to enhance point cloud features. Meanwhile, DAP-MAE incorporates adomain feature generator to guide the adaptation of point cloud features tovarious downstream tasks. With only one pre-training, DAP-MAE achievesexcellent performance across four different point cloud analysis tasks,reaching 95.18% in object classification on ScanObjectNN and 88.45% in facialexpression recognition on Bosphorus.</description>
      <author>example@mail.com (Ziqi Gao, Qiufu Li, Linlin Shen)</author>
      <guid isPermaLink="false">2510.21635v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Robust Point Cloud Reinforcement Learning via PCA-Based Canonicalization</title>
      <link>http://arxiv.org/abs/2510.20974v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PCA点云(PPC)框架，用于解决点云强化学习中的相机姿态不匹配问题，通过将点云映射到规范姿态，显著提高了对视角变化的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;从原始视觉输入的强化学习近年来取得了显著成功，但它对分布外变化(如光照、颜色和视角变化)仍然很脆弱。点云强化学习提供了一种有前景的替代方案，减轻了基于外观的脆弱性，但其对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;解决点云强化学习对相机姿态不匹配敏感的挑战，提高在现实场景中的可靠性。&lt;h4&gt;方法&lt;/h4&gt;提出PCA点云(PPC)框架，这是一个专门为下游机器人控制设计的规范化框架，它将任意刚体变换下的点云映射到唯一的规范姿态，将观测结果对齐到一致的坐标系。&lt;h4&gt;主要发现&lt;/h4&gt;PPC显著减少了视角引起的不一致性，在实验中提高了在具有挑战性的机器人任务中对未见过的相机姿态的鲁棒性，为域随机化提供了有原则的替代方案。&lt;h4&gt;结论&lt;/h4&gt;PPC框架有效地解决了点云强化学习中的相机姿态不匹配问题，提高了在现实场景中的鲁棒性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;从原始视觉输入的强化学习近年来取得了显著成功，但它对分布外变化(如光照、颜色和视角变化)仍然很脆弱。点云强化学习提供了一种有前景的替代方案，减轻了基于外观的脆弱性，但其对相机姿态不匹配的敏感性继续削弱了在现实环境中的可靠性。为应对这一挑战，我们提出了PCA点云(PPC)，这是一个专门为下游机器人控制设计的规范化框架。PPC将任意刚体变换下的点云映射到唯一的规范姿态，将观测结果对齐到一致的坐标系，从而显著减少了视角引起的不一致性。在我们的实验中，我们证明了PPC提高了在具有挑战性的机器人任务中对未见过的相机姿态的鲁棒性，为域随机化提供了有原则的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) from raw visual input has achieved impressivesuccesses in recent years, yet it remains fragile to out-of-distributionvariations such as changes in lighting, color, and viewpoint. Point CloudReinforcement Learning (PC-RL) offers a promising alternative by mitigatingappearance-based brittleness, but its sensitivity to camera pose mismatchescontinues to undermine reliability in realistic settings. To address thischallenge, we propose PCA Point Cloud (PPC), a canonicalization frameworkspecifically tailored for downstream robotic control. PPC maps point cloudsunder arbitrary rigid-body transformations to a unique canonical pose, aligningobservations to a consistent frame, thereby substantially decreasingviewpoint-induced inconsistencies. In our experiments, we show that PPCimproves robustness to unseen camera poses across challenging robotic tasks,providing a principled alternative to domain randomization.</description>
      <author>example@mail.com (Michael Bezick, Vittorio Giammarino, Ahmed H. Qureshi)</author>
      <guid isPermaLink="false">2510.20974v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Fractional harmonic transform on point cloud manifolds</title>
      <link>http://arxiv.org/abs/2510.20842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ICASSP 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数扩展了传统的点云流形谐波变换(PMHT)，构建了空间域和频率域之间可连续调节的中间分数阶谱域，实现了更灵活的变换和滤波操作。&lt;h4&gt;背景&lt;/h4&gt;点云可被视为光滑流形的离散样本，可使用拉普拉斯-贝尔特拉米算子进行谱分析。然而，传统的PMHT受限于固定基函数和单一谱表示，难以捕获复杂几何特征。&lt;h4&gt;目的&lt;/h4&gt;提出PMFHT来克服传统PMHT的局限性，通过引入分数阶参数构建连续可调的中间谱域。&lt;h4&gt;方法&lt;/h4&gt;引入分数阶参数，构建空间域和频率域之间的中间分数阶谱域，支持更灵活的变换和滤波操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，选择不同的变换顺序可以丰富点云的谱表示，在滤波和特征增强等任务中取得优异结果。&lt;h4&gt;结论&lt;/h4&gt;PMFHT扩展了点云谱分析的理论框架，为流形几何处理提供了强大的新工具。&lt;h4&gt;翻译&lt;/h4&gt;三维点云可以被视为光滑流形的离散样本，允许使用拉普拉斯-贝尔特拉米算子进行谱分析。然而，传统的点云流形谐波变换(PMHT)受其固定基函数和单一谱表示的限制，限制了其捕获复杂几何特征的能力。本文提出了一种点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数推广了PMHT，并构建了空间域和频率域之间可连续调节的中间分数阶谱域。这种分数阶框架支持更灵活的变换和滤波操作。实验表明，选择不同的变换顺序可以丰富点云的谱表示，并在滤波和特征增强等任务中取得优异结果。因此，PMFHT不仅扩展了点云谱分析的理论框架，还为流形几何处理提供了强大的新工具。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文解决传统点云流形谐波变换(PMHT)的局限性，即其固定的基函数和单一频谱表示无法充分捕捉复杂几何特征。这一问题很重要，因为三维点云是3D场景最常见的数据表示形式之一，广泛应用于LiDAR、结构光扫描和立体重建等领域，有效的几何特征提取对点云处理至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到传统PMHT的局限性，然后从信号处理中的分数傅里叶变换(FRFT)获得启发，后者通过引入分数阶参数解决了类似限制，提供了时域和频域之间的连续中间表示。作者借鉴了PMHT的基础框架、FRFT的分数阶参数思想以及LBO在点云上的离散化方法，将流形谐波扩展为分数阶形式，通过非线性缩放LBO特征值创建连续可调的中间频谱域。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入分数阶参数α，将传统PMHT扩展为分数阶形式(PMFHT)，创建空间域和频率域之间可连续调整的中间分数阶频谱域。实现流程包括：1)构建离散拉普拉斯-贝尔特拉米算子；2)求解广义特征值问题获得点云流形谐波基；3)定义分数阶傅里叶矩阵和点云流形分数阶谐波变换；4)应用变换进行不同类型的滤波操作，如特征增强或平滑处理。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)构建点云流形分数阶谐波变换的统一框架；2)提供简单高效的PMFHT分数幂公式；3)证明PMFHT能提供更丰富的频谱表示并在点云处理任务中表现出色。相比传统PMHT，PMFHT引入分数阶参数提供连续可调的中间频谱域，能捕捉多尺度几何特征，通过选择不同变换阶数丰富频谱表示，为流形几何处理提供新工具。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了点云流形分数阶谐波变换(PMFHT)，通过引入分数阶参数扩展了传统点云流形谐波变换，提供了空间域和频率域之间的连续可调中间表示，丰富了点云的频谱分析能力，并在点云处理任务中展现出优越性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Three-dimensional point clouds can be viewed as discrete samples of smoothmanifolds, allowing spectral analysis using the Laplace-Beltrami operator(LBO). However, the traditional point cloud manifold harmonic transform (PMHT)is limited by its fixed basis functions and single spectral representation,which restricts its ability to capture complex geometric features. This paperproposes a point cloud manifold fractional harmonic transform (PMFHT), whichgeneralizes PMHT by introducing fractional-order parameters and constructs acontinuously adjustable intermediate fractional-order spectral domain betweenthe spatial domain and the frequency domain. This fractional-order frameworksupports more flexible transformation and filtering operations. Experimentsshow that choosing different transformation orders can enrich the spectralrepresentation of point clouds and achieve excellent results in tasks such asfiltering and feature enhancement. Therefore, PMFHT not only expands thetheoretical framework of point cloud spectral analysis, but also provides apowerful new tool for manifold geometry processing.</description>
      <author>example@mail.com (Jiamian Li, Bing-Zhao Li)</author>
      <guid isPermaLink="false">2510.20842v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>REVE: A Foundation Model for EEG -- Adapting to Any Setup with Large-Scale Pretraining on 25,000 Subjects</title>
      <link>http://arxiv.org/abs/2510.21585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code available at: https://brain-bzh.github.io/reve/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了REVE模型，一个专为EEG信号设计的基础模型，能够处理不同长度和电极排列的EEG信号，并在多种下游任务中取得了最先进的结果。&lt;h4&gt;背景&lt;/h4&gt;基础模型通过大规模预训练减少了任务特定数据的依赖，在语言和视觉领域取得成功，但在EEG领域的应用滞后。公共EEG数据集的异质性（不同协议、设备和电极配置）导致现有EEG基础模型难以泛化，现有模型通常限制在单一设置下预训练，导致次优性能，特别是在线性探测下。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够跨多样化EEG信号泛化的预训练模型REVE（Representation for EEG with Versatile Embeddings）。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新颖的4D位置编码方案，使其能够处理任意长度和电极排列的信号；使用掩码自编码目标函数进行预训练；在来自92个数据集、25,000名受试者的超过60,000小时EEG数据上预训练REVE，这是迄今为止最大的EEG预训练工作。&lt;h4&gt;主要发现&lt;/h4&gt;REVE在10个下游EEG任务上取得了最先进的结果，包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情绪识别；几乎不需要微调的情况下，REVE展示了强大的泛化能力和细致的时空建模能力。&lt;h4&gt;结论&lt;/h4&gt;REVE为EEG信号处理提供了新的基础模型，能够处理多样化的EEG数据；研究团队发布了代码、预训练权重和教程，以支持标准化的EEG研究并加速临床神经科学的进展。&lt;h4&gt;翻译&lt;/h4&gt;基础模型通过大规模预训练减少了对任务特定数据的依赖，从而改变了人工智能领域。尽管在语言和视觉领域取得了成功，但由于公共数据集的异质性（收集于不同的协议、设备和电极配置下），它们在EEG中的应用一直滞后。现有的EEG基础模型难以在这些变化中泛化，通常将预训练限制在单一设置中，导致次优性能，特别是在线性探测下。我们提出了REVE（Representation for EEG with Versatile Embeddings），一个明确设计为能够泛化到多样化EEG信号的预训练模型。REVE引入了一种新颖的4D位置编码方案，使其能够处理任意长度和电极排列的信号。使用掩码自编码目标函数，我们在来自92个数据集、25,000名受试者的超过60,000小时EEG数据上预训练了REVE，这是迄今为止最大的EEG预训练工作。REVE在10个下游EEG任务上取得了最先进的结果，包括运动想象分类、癫痫检测、睡眠分期、认知负荷估计和情绪识别。几乎不需要微调的情况下，它展示了强大的泛化能力和细致的时空建模能力。我们发布了代码、预训练权重和教程，以支持标准化的EEG研究并加速临床神经科学的进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models have transformed AI by reducing reliance on task-specificdata through large-scale pretraining. While successful in language and vision,their adoption in EEG has lagged due to the heterogeneity of public datasets,which are collected under varying protocols, devices, and electrodeconfigurations. Existing EEG foundation models struggle to generalize acrossthese variations, often restricting pretraining to a single setup, resulting insuboptimal performance, in particular under linear probing. We present REVE(Representation for EEG with Versatile Embeddings), a pretrained modelexplicitly designed to generalize across diverse EEG signals. REVE introduces anovel 4D positional encoding scheme that enables it to process signals ofarbitrary length and electrode arrangement. Using a masked autoencodingobjective, we pretrain REVE on over 60,000 hours of EEG data from 92 datasetsspanning 25,000 subjects, representing the largest EEG pretraining effort todate. REVE achieves state-of-the-art results on 10 downstream EEG tasks,including motor imagery classification, seizure detection, sleep staging,cognitive load estimation, and emotion recognition. With little to nofine-tuning, it demonstrates strong generalization, and nuanced spatio-temporalmodeling. We release code, pretrained weights, and tutorials to supportstandardized EEG research and accelerate progress in clinical neuroscience.</description>
      <author>example@mail.com (Yassine El Ouahidi, Jonathan Lys, Philipp Thölke, Nicolas Farrugia, Bastien Pasdeloup, Vincent Gripon, Karim Jerbi, Giulia Lioi)</author>
      <guid isPermaLink="false">2510.21585v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>MUVR: A Multi-Modal Untrimmed Video Retrieval Benchmark with Multi-Level Visual Correspondence</title>
      <link>http://arxiv.org/abs/2510.21406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025 D&amp;B Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了多模态未修剪视频检索(MUVR)任务及相应基准数据集，旨在推进长视频平台上的视频检索技术。该研究构建了实用的检索范式、多层次视觉对应和全面的评估标准，并对多种先进模型进行了评估，揭示了当前方法在处理未修剪视频和多模态查询方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;随着长视频平台的普及，视频检索技术面临新的挑战。现有的视频检索方法主要针对修剪过的短视频，而长视频平台上的视频通常包含多个相关片段，需要更灵活的检索方式来满足用户需求。&lt;h4&gt;目的&lt;/h4&gt;提出并构建一个专门针对长视频平台的多模态未修剪视频检索任务和基准数据集，以促进该领域的研究和发展，并评估现有方法在这一新任务上的表现。&lt;h4&gt;方法&lt;/h4&gt;设计了MUVR基准数据集，包含53K个来自Bilibili的未修剪视频、1,050个多模态查询和84K个匹配。构建了以视频为中心的多模态查询支持长文本描述、视频标签提示和掩码提示。建立了六个级别的多层次视觉对应标准（副本、事件、场景、实例、动作和其他）。开发了三个版本的评估基准（基础版、过滤版、问答版），并提出了重新排序分数评估指标。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果显示，当前的视频检索模型在处理未修剪视频和多模态查询方面存在明显局限性；MLLMs在多视频理解和重新排序能力上也表现出不足，这为未来研究指明了方向。&lt;h4&gt;结论&lt;/h4&gt;MUVR基准为长视频平台上的视频检索研究提供了新的评估框架，揭示了现有方法的不足，并为未来改进提供了方向。该研究有助于推动多模态未修剪视频检索领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了多模态未修剪视频检索任务，并创建了一个新的基准(MUVR)以推进长视频平台的视频检索。MUVR旨在使用多模态查询检索包含相关片段的未修剪视频。它具有以下特点：1)实用的检索范式：MUVR支持以视频为中心的多模态查询，通过长文本描述、视频标签提示和掩码提示表达细粒度检索需求。它采用一对多检索范式，专注于未修剪视频，专为长视频平台应用定制。2)多层次视觉对应：为了涵盖常见的视频类别（如新闻、旅行、舞蹈）并精确定义检索匹配标准，我们基于用户感兴趣且想要检索的核心视频内容（如新闻事件、旅行地点、舞蹈动作）构建了多层次视觉对应。它涵盖六个级别：副本、事件、场景、实例、动作和其他。3)全面的评估标准：我们开发了3个版本的MUVR（即基础版、过滤版、问答版）。MUVR-Base/Filter评估检索模型，而MUVR-QA以问答格式评估MLLMs。我们还提出了重新排序分数来评估MLLMs的重新排序能力。MUVR包含来自Bilibili视频平台的53K个未修剪视频，有1,050个多模态查询和84K个匹配。我们对3个最先进的视频检索模型、6个基于图像的VLMs和10个MLLMs进行了广泛评估。MUVR揭示了检索方法在处理未修剪视频和多模态查询方面的局限性，以及MLLMs在多视频理解和重新排序方面的局限性。我们的代码和基准可在https://github.com/debby-0527/MUVR获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose the Multi-modal Untrimmed Video Retrieval task, along with a newbenchmark (MUVR) to advance video retrieval for long-video platforms. MUVR aimsto retrieve untrimmed videos containing relevant segments using multi-modalqueries. It has the following features: 1) Practical retrieval paradigm: MUVRsupports video-centric multi-modal queries, expressing fine-grained retrievalneeds through long text descriptions, video tag prompts, and mask prompts. Itadopts a one-to-many retrieval paradigm and focuses on untrimmed videos,tailored for long-video platform applications. 2) Multi-level visualcorrespondence: To cover common video categories (e.g., news, travel, dance)and precisely define retrieval matching criteria, we construct multi-levelvisual correspondence based on core video content (e.g., news events, travellocations, dance moves) which users are interested in and want to retrieve. Itcovers six levels: copy, event, scene, instance, action, and others. 3)Comprehensive evaluation criteria: We develop 3 versions of MUVR (i.e., Base,Filter, QA). MUVR-Base/Filter evaluates retrieval models, while MUVR-QAassesses MLLMs in a question-answering format. We also propose a RerankingScore to evaluate the reranking ability of MLLMs. MUVR consists of 53Kuntrimmed videos from the video platform Bilibili, with 1,050 multi-modalqueries and 84K matches. Extensive evaluations of 3 state-of-the-art videoretrieval models, 6 image-based VLMs, and 10 MLLMs are conducted. MUVR revealsthe limitations of retrieval methods in processing untrimmed videos andmulti-modal queries, as well as MLLMs in multi-video understanding andreranking. Our code and benchmark is available athttps://github.com/debby-0527/MUVR.</description>
      <author>example@mail.com (Yue Feng, Jinwei Hu, Qijia Lu, Jiawei Niu, Li Tan, Shuo Yuan, Ziyi Yan, Yizhen Jia, Qingzhi He, Shiping Ge, Ethan Q. Chen, Wentong Li, Limin Wang, Jie Qin)</author>
      <guid isPermaLink="false">2510.21406v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>HRT1: One-Shot Human-to-Robot Trajectory Transfer for Mobile Manipulation</title>
      <link>http://arxiv.org/abs/2510.21026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 11 figures and 3 tables. Project page is available at  \url{https://irvlutd.github.io/HRT1/}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一个新颖的人机轨迹传递系统，使机器人能够通过学习人类示范视频来操作物体，系统包含四个模块，实现了机器人观看一次人类示范后就能在不同环境中重复相同操作任务的能力。&lt;h4&gt;背景&lt;/h4&gt;机器人操作任务通常需要精确的编程和大量调整，而人类能够直观地通过示范学习操作任务。如何让机器人从人类示范中学习操作技能是一个重要研究方向。&lt;h4&gt;目的&lt;/h4&gt;开发一个系统，使机器人能够通过观看人类示范视频来学习操作任务，并能在不同环境中重复这些任务，即使物体放置方式与示范不同。&lt;h4&gt;方法&lt;/h4&gt;系统包含四个模块：1)使用AR头戴设备从机器人视角收集人类示范视频的数据收集模块；2)从示范视频中检测物体并提取3D人类手部轨迹的视频理解模块；3)将人类手部轨迹转换为机器人末端执行器参考轨迹的轨迹传递模块；4)利用轨迹优化算法解决机器人配置空间中轨迹问题的轨迹优化模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明该系统能够使移动机械臂观看一次人类示范视频后，就能在不同的环境中重复相同的移动操作任务，即使物体放置方式与示范不同。&lt;h4&gt;结论&lt;/h4&gt;该系统有效地实现了从人类示范到机器人操作的技能传递，为机器人学习人类操作任务提供了一种直观、灵活的方法。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种新颖的人机轨迹传递系统，使机器人能够通过学习人类示范视频来操作物体。该系统由四个模块组成。第一个模块是数据收集模块，旨在使用AR头戴设备从机器人视角收集人类示范视频。第二个模块是视频理解模块，从示范视频中检测物体并提取3D人类手部轨迹。第三个模块将人类手部轨迹转换为3D空间中机器人末端执行器的参考轨迹。最后一个模块利用轨迹优化算法解决机器人配置空间中的轨迹问题，使其能够遵循从人类示范传递而来的末端执行器轨迹。因此，这些模块使机器人能够观看一次人类示范视频，然后在不同环境中重复相同的移动操作任务，即使物体放置方式与示范不同。我们在移动机械臂上进行了不同操作任务的实验，以验证我们系统的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何让机器人通过观看一次人类演示视频，就能在不同环境中重复执行相同的移动操作任务。这个问题在现实中很重要，因为传统的机器人操作需要大量编程和调参，而现有的基于学习的方法通常需要大量机器人遥操作数据或多次演示，收集成本高。此外，现有方法在物体被手部遮挡时表现不佳，且大多不支持移动操作，限制了机器人在日常环境中的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有基于人类演示的机器人操作方法，发现模仿学习方法需要多次演示，强化学习方法需要构建任务空间的数字孪生，而训练免费方法在物体遮挡或噪声处理方面存在局限。作者借鉴了多个现有工作：使用AR头显收集数据类似iTeach框架；使用GroundingDINO和SAMv2进行物体检测；使用HaMeR进行手部姿态估计；使用统一夹持器坐标系空间(UGCS)进行抓取转移；使用BundleSDF进行物体姿态估计。基于这些分析，作者设计了HRT1系统，专注于手部轨迹转移而非物体轨迹，并加入了轨迹优化算法以提高鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过分析人类演示视频中的手部动作，将其转换为机器人的执行轨迹，使机器人能够一次性学习并执行相同的操作任务，即使在不同环境和物体摆放情况下也能成功。整体流程分为四个模块：1)数据收集模块：使用HoloLens 2从机器人视角收集人类演示视频；2)视频理解模块：检测物体并提取3D人类手部轨迹；3)人类到机器人抓取转移模块：使用UGCS表示将人类手部轨迹转换为机器人夹持器轨迹；4)任务执行的轨迹对齐模块：使用BundleSDF估计物体姿态变换，并通过两阶段轨迹优化算法(先优化机器人基座位置，再优化关节空间轨迹)使机器人精确执行任务。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)基于手部轨迹而非物体轨迹的转移，在物体被遮挡时更可靠；2)使用统一夹持器坐标系空间(UGCS)进行抓取转移，支持不同类型夹持器；3)两阶段轨迹优化算法，能处理转移轨迹中的噪声；4)支持移动操作，是首个支持移动的训练免费方法；5)改进的3D手部姿态估计，提高深度准确性。相比之前工作，HRT1不依赖物体姿态估计，使用轨迹优化而非简单逆运动学，支持移动操作，且只需一次演示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; HRT1通过分析人类演示视频中的手部动作并转换为机器人执行轨迹，实现了机器人仅需观看一次演示就能在不同环境中成功执行移动操作任务的能力，显著提高了轨迹转移的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel system for human-to-robot trajectory transfer thatenables robots to manipulate objects by learning from human demonstrationvideos. The system consists of four modules. The first module is a datacollection module that is designed to collect human demonstration videos fromthe point of view of a robot using an AR headset. The second module is a videounderstanding module that detects objects and extracts 3D human-handtrajectories from demonstration videos. The third module transfers a human-handtrajectory into a reference trajectory of a robot end-effector in 3D space. Thelast module utilizes a trajectory optimization algorithm to solve a trajectoryin the robot configuration space that can follow the end-effector trajectorytransferred from the human demonstration. Consequently, these modules enable arobot to watch a human demonstration video once and then repeat the same mobilemanipulation task in different environments, even when objects are placeddifferently from the demonstrations. Experiments of different manipulationtasks are conducted on a mobile manipulator to verify the effectiveness of oursystem</description>
      <author>example@mail.com (Sai Haneesh Allu, Jishnu Jaykumar P, Ninad Khargonkar, Tyler Summers, Jian Yao, Yu Xiang)</author>
      <guid isPermaLink="false">2510.21026v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Integrated Bayesian State Space Models for Multimodal Time-Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.20952v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了LLM集成的贝叶斯状态空间模型(LBS)，一种用于多模态时间预测的新概率框架，解决了现有方法在架构上的限制，实现了灵活的时间窗口、不确定性量化和改进的时间泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现实世界预测需要整合结构化时间序列数据与非结构化文本信息，但现有方法受固定输入/输出时间跨度限制，无法建模或量化不确定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时处理数值和文本信息的时间预测方法，提供灵活的时间窗口、不确定性量化和更好的时间泛化能力。&lt;h4&gt;方法&lt;/h4&gt;LBS包含两个主要组件：(1)状态空间模型(SSM)骨干，捕获生成数值和文本观测的潜在状态的时间动态；(2)预训练大型语言模型(LLM)，用于编码文本输入进行后验状态估计和解码文本预测。&lt;h4&gt;主要发现&lt;/h4&gt;LBS在TextTimeCorpus基准测试上比之前的最先进方法提高13.20%，同时为每个预测提供可读的人类摘要，实现了灵活的回看和预测窗口、原则性不确定性量化和改进的时间泛化。&lt;h4&gt;结论&lt;/h4&gt;该研究首次统一LLM和SSM进行数值和文本联合预测，为多模态时间推理提供了新的基础框架。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中的预测需要整合结构化的时间序列数据和非结构化的文本信息，但现有方法在架构上受到固定输入/输出时间跨度的限制，无法建模或量化不确定性。我们通过引入LLM集成的贝叶斯状态空间模型(LBS)来解决这一挑战，这是一种用于多模态时间预测的新概率框架。总体而言，LBS包含两个组件：(1)状态空间模型(SSM)骨干，捕获生成数值和文本观测的潜在状态的时间动态；(2)预训练的大型语言模型(LLM)，调整后用于编码文本输入进行后验状态估计和解码与潜在轨迹一致的文本预测。这种设计能够提供灵活的回看和预测窗口，原则性的不确定性量化，并改善时间泛化能力。在TextTimeCorpus基准测试上的实验表明，LBS比之前的最先进方法提高13.20%，同时为每个预测提供可读的人类摘要。我们的工作是首次统一LLM和SSM进行数值和文本联合预测，为多模态时间推理提供了新的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Forecasting in the real world requires integrating structured time-seriesdata with unstructured textual information, but existing methods arearchitecturally limited by fixed input/output horizons and are unable to modelor quantify uncertainty. We address this challenge by introducingLLM-integrated Bayesian State space models (LBS), a novel probabilisticframework for multimodal temporal forecasting. At a high level, LBS consists oftwo components: (1) a state space model (SSM) backbone that captures thetemporal dynamics of latent states from which both numerical and textualobservations are generated and (2) a pretrained large language model (LLM) thatis adapted to encode textual inputs for posterior state estimation and decodetextual forecasts consistent with the latent trajectory. This design enablesflexible lookback and forecast windows, principled uncertainty quantification,and improved temporal generalization thanks to the well-suited inductive biasof SSMs toward modeling dynamical systems. Experiments on the TextTimeCorpusbenchmark demonstrate that LBS improves the previous state-of-the-art by 13.20%while providing human-readable summaries of each forecast. Our work is thefirst to unify LLMs and SSMs for joint numerical and textual prediction,offering a novel foundation for multimodal temporal reasoning.</description>
      <author>example@mail.com (Sungjun Cho, Changho Shin, Suenggwan Jo, Xinya Yan, Shourjo Aditya Chaudhuri, Frederic Sala)</author>
      <guid isPermaLink="false">2510.20952v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>SeViCES: Unifying Semantic-Visual Evidence Consensus for Long Video Understanding</title>
      <link>http://arxiv.org/abs/2510.20622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SeViCES框架，通过语义-视觉共识证据选择方法解决长视频理解中的挑战，在准确性和鲁棒性方面超越了现有方法。&lt;h4&gt;背景&lt;/h4&gt;长视频理解因其复杂、多样且时间分散的内容而具有挑战性。现有的Video-LLMs可处理数十分钟的视频，但应用于真正长序列时计算成本高，且推理往往不聚焦或不一致。&lt;h4&gt;目的&lt;/h4&gt;开发一个有效可靠的长视频理解框架，解决现有方法中忽略时间依赖性和依赖单模态证据的局限性。&lt;h4&gt;方法&lt;/h4&gt;SeViCES是一个无需训练且与模型无关的框架，包含两个关键组件：(1)语义-视觉共识帧选择(SVCFS)模块，通过时间感知语义分支和聚类引导视觉分支选择信息量最大的帧；(2)答案共识精炼(ACR)模块，通过融合证据和约束答案空间解决语义和视觉预测间的不一致。&lt;h4&gt;主要发现&lt;/h4&gt;在长视频理解基准上的大量实验表明，SeViCES在准确性和鲁棒性方面均持续优于最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;共识驱动的证据选择对Video-LLMs的长视频理解能力至关重要。&lt;h4&gt;翻译&lt;/h4&gt;长视频理解由于其复杂、多样且时间分散的内容而仍然具有挑战性。尽管视频大语言模型(Video-LLMs)可以处理长达数十分钟的视频，但将它们应用于真正长序列在计算上是禁止的，并且往往导致不聚焦或不一致的推理。一个有希望的解决方案是只选择信息量最大的帧，然而现有方法通常忽略时间依赖性或依赖单模态证据，限制了它们提供完整且与查询相关上下文的能力。我们提出了一个用于有效可靠长视频理解的语义-视觉共识证据选择(SeViCES)框架。SeViCES无需训练且与模型无关，并引入了两个关键组件。语义-视觉共识帧选择(SVCFS)模块通过(1)利用LLM对字幕进行推理的时间感知语义分支，和(2)通过互信息将嵌入与语义分数对齐的聚类引导视觉分支来选择帧。答案共识精炼(ACR)模块通过融合证据和约束答案空间，进一步解决基于语义和视觉的预测之间的一致性问题。在长视频理解基准上的大量实验表明，SeViCES在准确性和鲁棒性方面均持续优于最先进的方法，证明了共识驱动的证据选择对Video-LLMs的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long video understanding remains challenging due to its complex, diverse, andtemporally scattered content. Although video large language models (Video-LLMs)can process videos lasting tens of minutes, applying them to truly longsequences is computationally prohibitive and often leads to unfocused orinconsistent reasoning. A promising solution is to select only the mostinformative frames, yet existing approaches typically ignore temporaldependencies or rely on unimodal evidence, limiting their ability to providecomplete and query-relevant context. We propose a Semantic-Visual ConsensusEvidence Selection (SeViCES) framework for effective and reliable long videounderstanding. SeViCES is training-free and model-agnostic, and introduces twokey components. The Semantic-Visual Consensus Frame Selection (SVCFS) moduleselects frames through (1) a temporal-aware semantic branch that leverages LLMreasoning over captions, and (2) a cluster-guided visual branch that alignsembeddings with semantic scores via mutual information. The Answer ConsensusRefinement (ACR) module further resolves inconsistencies between semantic- andvisual-based predictions by fusing evidence and constraining the answer space.Extensive experiments on long video understanding benchmarks show that SeViCESconsistently outperforms state-of-the-art methods in both accuracy androbustness, demonstrating the importance of consensus-driven evidence selectionfor Video-LLMs.</description>
      <author>example@mail.com (Yuan Sheng, Yanbin Hao, Chenxu Li, Shuo Wang, Xiangnan He)</author>
      <guid isPermaLink="false">2510.20622v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Open-o3 Video: Grounded Video Reasoning with Explicit Spatio-Temporal Evidence</title>
      <link>http://arxiv.org/abs/2510.20579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Open-o3 Video是一个非智能体框架，将显式时空证据整合到视频推理中，通过专门的数据集和训练策略实现了在多个视频理解基准测试上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;大多数视频推理模型只生成文本推理轨迹而不指示关键证据出现的时间和位置。将图像证据中心推理扩展到视频更具挑战性，因为它需要在动态场景中联合时空跟踪和定位。&lt;h4&gt;目的&lt;/h4&gt;引入Open-o3 Video框架，解决视频推理中时空证据整合的挑战，通过收集训练数据和设计训练策略来提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;创建了两个高质量数据集STGR-CoT-30k用于SFT和STGR-RL-36k用于RL，包含精心构建的时空注释；采用冷启动强化学习策略，使用多种专门设计的奖励来鼓励答案准确性、时间对齐和空间精度。&lt;h4&gt;主要发现&lt;/h4&gt;在V-STAR基准测试上，Open-o3 Video实现了最先进性能，相比Qwen2.5-VL基线，mAM提高14.4%，mLGM提高24.2%；在VideoMME、WorldSense、VideoMMMU和TVGBench等多个视频理解基准测试上观察到一致改进。&lt;h4&gt;结论&lt;/h4&gt;Open-o3 Video的推理轨迹为测试时扩展提供了有价值的信号，支持置信感知的验证，提高答案可靠性。&lt;h4&gt;翻译&lt;/h4&gt;大多数视频推理模型只生成文本推理轨迹而不指示关键证据出现的时间和位置。最近的模型如OpenAI-o3在图像证据中心推理方面引起了广泛兴趣，但将这种能力扩展到视频更具挑战性，因为它需要在动态场景中联合时空跟踪和定位。我们引入了Open-o3 Video，一个将显式时空证据整合到视频推理中的非智能体框架，并仔细收集训练数据和设计训练策略来解决上述挑战。模型在答案旁边突出显示关键时间戳、对象和边界框，使推理能够基于具体的视觉观察。为实现这一功能，我们首先策划并构建了两个高质量数据集：用于SFT的STGR-CoT-30k和用于RL的STGR-RL-36k，包含精心构建的时间和空间注释，因为大多数现有数据集只提供视频的时间跨度或图像的空间框，缺乏统一的时空监督和推理轨迹。然后，我们采用冷启动强化学习策略，使用多种专门设计的奖励，共同鼓励答案准确性、时间对齐和空间精度。在V-STAR基准测试上，Open-o3 Video实现了最先进的性能，相比Qwen2.5-VL基线，mAM提高了14.4%，mLGM提高了24.2%。在广泛的视频理解基准测试上，包括VideoMME、WorldSense、VideoMMMU和TVGBench，也观察到一致改进。除了准确性，Open-o3 Video产生的推理轨迹还为测试时扩展提供了有价值的信号，使置信感知的验证成为可能，并提高答案可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most video reasoning models only generate textual reasoning traces withoutindicating when and where key evidence appears. Recent models such as OpenAI-o3have sparked wide interest in evidence-centered reasoning for images, yetextending this ability to videos is more challenging, as it requires jointtemporal tracking and spatial localization across dynamic scenes. We introduceOpen-o3 Video, a non-agent framework that integrates explicit spatio-temporalevidence into video reasoning, and carefully collect training data and designtraining strategies to address the aforementioned challenges. The modelhighlights key timestamps, objects, and bounding boxes alongside its answers,allowing reasoning to be grounded in concrete visual observations. To enablethis functionality, we first curate and build two high-quality datasets,STGR-CoT-30k for SFT and STGR-RL-36k for RL, with carefully constructedtemporal and spatial annotations, since most existing datasets offer eithertemporal spans for videos or spatial boxes on images, lacking unifiedspatio-temporal supervision and reasoning traces. Then, we adopt a cold-startreinforcement learning strategy with multiple specially designed rewards thatjointly encourage answer accuracy, temporal alignment, and spatial precision.On V-STAR benchmark, Open-o3 Video achieves state-of-the-art performance,raising mAM by 14.4% and mLGM by 24.2% on the Qwen2.5-VL baseline. Consistentimprovements are also observed on a broad range of video understandingbenchmarks, including VideoMME, WorldSense, VideoMMMU, and TVGBench. Beyondaccuracy, the reasoning traces produced by Open-o3 Video also provide valuablesignals for test-time scaling, enabling confidence-aware verification andimproving answer reliability.</description>
      <author>example@mail.com (Jiahao Meng, Xiangtai Li, Haochen Wang, Yue Tan, Tao Zhang, Lingdong Kong, Yunhai Tong, Anran Wang, Zhiyang Teng, Yujing Wang, Zhuochen Wang)</author>
      <guid isPermaLink="false">2510.20579v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Conan: Progressive Learning to Reason Like a Detective over Multi-Scale Visual Evidence</title>
      <link>http://arxiv.org/abs/2510.20470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Conan，一个基于证据的多步视频推理框架，通过识别上下文和证据帧、跨帧线索推理以及自适应决策机制，解决了现有视频推理方法的局限性。&lt;h4&gt;背景&lt;/h4&gt;视频推理需要跨帧多步推理，对多模态大语言模型(MLLMs)仍是主要挑战。基于强化学习的方法依赖文本链导致结论缺乏基础，帧检索方法则难以准确进行证据定位。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够有效进行多步视频推理的框架，解决现有方法在推理准确性和证据定位方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;构建Conan-91K数据集，包含自动生成的推理轨迹；设计多阶段渐进式冷启动策略和识别-推理-行动(AIR)RLVR训练框架，共同增强多步视觉推理能力。&lt;h4&gt;主要发现&lt;/h4&gt;在六个多步推理基准测试上，Conan的准确性平均超过基线Qwen2.5-VL-7B-Instruct模型10%以上，达到最先进性能；且能有效泛化到长视频理解任务，展示出良好的可扩展性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;Conan框架通过结合证据基础和多步推理，有效解决了视频推理中的挑战，实现了更准确可靠的视频理解。&lt;h4&gt;翻译&lt;/h4&gt;视频推理需要跨帧多步推理，这对多模态大语言模型(MLLMs)仍然是一个主要挑战。虽然基于强化学习(RL)的方法增强了推理能力，但它们通常只依赖文本链，导致结论缺乏基础或产生幻觉。相反，帧检索方法引入了视觉基础，但仍然难以准确进行证据定位。为了解决这些挑战，我们提出了Conan，一个用于基于证据的多步视频推理框架。Conan能够识别上下文和证据帧，跨帧线索进行推理，并自适应地决定何时得出结论或进一步探索。为此，我们(1)构建了Conan-91K，这是一个大规模的自动生成推理轨迹数据集，包括帧识别、证据推理和行动决策，以及(2)设计了一个多阶段渐进式冷启动策略，结合识别-推理-行动(AIR)RLVR训练框架，共同增强多步视觉推理。在六个多步推理基准测试上的广泛实验表明，Conan在准确性上平均超过基线Qwen2.5-VL-7B-Instruct模型10%以上，达到了最先进的性能。此外，Conan能有效地泛化到长视频理解任务，验证了其强大的可扩展性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video reasoning, which requires multi-step deduction across frames, remains amajor challenge for multimodal large language models (MLLMs). Whilereinforcement learning (RL)-based methods enhance reasoning capabilities, theyoften rely on text-only chains that yield ungrounded or hallucinatedconclusions. Conversely, frame-retrieval approaches introduce visual groundingbut still struggle with inaccurate evidence localization. To address thesechallenges, we present Conan, a framework for evidence-grounded multi-stepvideo reasoning. Conan identifies contextual and evidence frames, reasons overcross-frame clues, and adaptively decides when to conclude or explore further.To achieve this, we (1) construct Conan-91K, a large-scale dataset ofautomatically generated reasoning traces that includes frame identification,evidence reasoning, and action decision, and (2) design a multi-stageprogressive cold-start strategy combined with anIdentification-Reasoning-Action (AIR) RLVR training framework to jointlyenhance multi-step visual reasoning. Extensive experiments on six multi-stepreasoning benchmarks demonstrate that Conan surpasses the baselineQwen2.5-VL-7B-Instruct by an average of over 10% in accuracy, achievingstate-of-the-art performance. Furthermore, Conan generalizes effectively tolong-video understanding tasks, validating its strong scalability androbustness.</description>
      <author>example@mail.com (Kun Ouyang, Yuanxin Liu, Linli Yao, Yishuo Cai, Hao Zhou, Jie Zhou, Fandong Meng, Xu Sun)</author>
      <guid isPermaLink="false">2510.20470v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>InvDec: Inverted Decoder for Multivariate Time Series Forecasting with Separated Temporal and Variate Modeling</title>
      <link>http://arxiv.org/abs/2510.20302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为InvDec的混合架构，用于多元时间序列预测，有效结合了时间建模和跨变量依赖关系建模，特别在高维数据集上表现优异。&lt;h4&gt;背景&lt;/h4&gt;多元时间序列预测需要同时建模时间模式和跨变量依赖关系。现有方法存在局限性：通道独立方法如PatchTST擅长时间建模但忽略变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合架构，实现时间编码和变量级解码的原则性分离，以同时捕捉时间模式和跨变量依赖关系。&lt;h4&gt;方法&lt;/h4&gt;提出InvDec架构，结合基于补丁的时间编码器和通过变量级自注意力操作的倒置解码器；引入延迟变量嵌入，在时间编码后才丰富变量特定表示；采用自适应残差融合机制动态平衡时间信息和变量信息；将InvDec与PatchTST结合形成InvDec-PatchTST。&lt;h4&gt;主要发现&lt;/h4&gt;在七个基准测试上，InvDec-PatchTST在高维数据集上表现显著：Electricity数据集（321个变量）MSE降低20.9%，Weather数据集提升4.3%，Traffic数据集提升2.7%；在低维ETT数据集上保持竞争力；消融研究验证了各组件有效性；InvDec的优势随数据集维度增长而增长。&lt;h4&gt;结论&lt;/h4&gt;InvDec有效地结合了时间建模和跨变量相关性建模，特别适合高维数据集，随着变量数量增加，跨变量建模变得至关重要。&lt;h4&gt;翻译&lt;/h4&gt;多元时间序列预测需要同时建模时间模式和跨变量依赖关系。通道独立方法如PatchTST擅长时间建模但忽略了变量相关性，而纯变量注意力方法如iTransformer牺牲了时间编码。我们提出了InvDec（倒置解码器），一种混合架构，实现了时间编码和变量级解码的原则性分离。InvDec结合了基于补丁的时间编码器和一个通过变量级自注意力操作在变量维度上运行的倒置解码器。我们引入了延迟变量嵌入，仅在时间编码后丰富变量特定表示，保持时间特征完整性。自适应残差融合机制动态平衡不同维度数据集的时间信息和变量信息。将InvDec与PatchTST实例化得到InvDec-PatchTST。在七个基准测试上的广泛实验表明，在高维数据集上取得了显著提升：Electricity（321个变量）上MSE降低20.9%，Weather上提升4.3%，Traffic上提升2.7%，同时在低维ETT数据集上保持竞争力。消融研究验证了每个组件，分析显示InvDec的优势随数据集维度增长而增长，证实了随着变量数量增加，跨变量建模变得关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate time series forecasting requires simultaneously modelingtemporal patterns and cross-variate dependencies. Channel-independent methodssuch as PatchTST excel at temporal modeling but ignore variable correlations,while pure variate-attention approaches such as iTransformer sacrifice temporalencoding. We proposeInvDec (Inverted Decoder), a hybrid architecture thatachieves principled separation between temporal encoding and variate-leveldecoding. InvDec combines a patch-based temporal encoder with an inverteddecoder operating on the variate dimension through variate-wise self-attention.We introduce delayed variate embeddings that enrich variable-specificrepresentations only after temporal encoding, preserving temporal featureintegrity. An adaptive residual fusion mechanism dynamically balances temporaland variate information across datasets of varying dimensions. InstantiatingInvDec with PatchTST yields InvDec-PatchTST. Extensive experiments on sevenbenchmarks demonstrate significant gains on high-dimensional datasets: 20.9%MSE reduction on Electricity (321 variables), 4.3% improvement on Weather, and2.7% gain on Traffic compared to PatchTST, while maintaining competitiveperformance on low-dimensional ETT datasets. Ablation studies validate eachcomponent, and analysis reveals that InvDec's advantage grows with datasetdimensionality, confirming that cross-variate modeling becomes critical as thenumber of variables increases.</description>
      <author>example@mail.com (Yuhang Wang)</author>
      <guid isPermaLink="false">2510.20302v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>DMC$^3$: Dual-Modal Counterfactual Contrastive Construction for Egocentric Video Question Answering</title>
      <link>http://arxiv.org/abs/2510.20285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种双模态反事实对比构建（DMC³）框架，用于解决以第一人称视角视频为基础的问题回答任务中的独特挑战，如理解多个事件和识别手部物体交互。&lt;h4&gt;背景&lt;/h4&gt;以第一人称视角视频为基础的问题回答（Egocentric VideoQA）在以第一人称视频理解中扮演着重要角色。现有的预训练和微调方法忽略了第一人称视角带来的独特挑战，如理解多个事件和识别手部物体交互。&lt;h4&gt;目的&lt;/h4&gt;为了解决第一人称视角视频理解中的独特挑战，特别是理解多个事件和识别手部物体交互，作者提出了一种新的框架DMC³。&lt;h4&gt;方法&lt;/h4&gt;DMC³框架包含三个主要部分：开发一个反事实样本构建模块，通过事件描述重述和核心交互挖掘分别为文本和视觉模态生成正负样本；将这些样本与原始样本一起输入基线模型；在反事实样本参与的对比优化模块中应用对比损失，最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在EgoTaskQA数据集的normal和indirect分割上分别达到了52.51%和46.04%的性能，在QAEGO4D上达到了13.2%的性能，均达到了最先进的水平。&lt;h4&gt;结论&lt;/h4&gt;通过提出DMC³框架，有效解决了第一人称视角视频理解中的独特挑战，特别是在理解多个事件和识别手部物体交互方面取得了显著进展，并在多个基准测试中达到了最先进的性能。&lt;h4&gt;翻译&lt;/h4&gt;以第一人称视频问答（Egocentric VideoQA）在以第一人称视频理解中发挥着重要作用，它指的是基于第一人称视频回答问题。尽管现有方法通过预训练和微调的范式已经取得了进展，但它们忽略了第一人称视角带来的独特挑战，如理解多个事件和识别手部物体交互。为了应对这些挑战，我们提出了一个双模态反事实对比构建（DMC³）框架，该框架包含一个以第一人称视频问答的基线模型、一个反事实样本构建模块和一个反事实样本参与的对比优化。具体来说，我们首先开发了一个反事实样本构建模块，通过事件描述重述和核心交互挖掘分别为文本和视觉模态生成正负样本。然后，我们将这些样本与原始样本一起输入基线模型。最后，在反事实样本参与的对比优化模块中，我们应用对比损失来最小化原始样本特征与正样本特征之间的距离，同时最大化与负样本的距离。实验表明，我们的方法在EgoTaskQA的normal和indirect分割上分别达到了52.51%和46.04%，在QAEGO4D上达到了13.2%，均达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3755085&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Egocentric Video Question Answering (Egocentric VideoQA) plays an importantrole in egocentric video understanding, which refers to answering questionsbased on first-person videos. Although existing methods have made progressthrough the paradigm of pre-training and fine-tuning, they ignore the uniquechallenges posed by the first-person perspective, such as understandingmultiple events and recognizing hand-object interactions. To deal with thesechallenges, we propose a Dual-Modal Counterfactual Contrastive Construction(DMC$^3$) framework, which contains an egocentric videoqa baseline, acounterfactual sample construction module and a counterfactual sample-involvedcontrastive optimization. Specifically, We first develop a counterfactualsample construction module to generate positive and negative samples fortextual and visual modalities through event description paraphrasing and coreinteraction mining, respectively. Then, We feed these samples together with theoriginal samples into the baseline. Finally, in the counterfactualsample-involved contrastive optimization module, we apply contrastive loss tominimize the distance between the original sample features and the positivesample features, while maximizing the distance from the negative samples.Experiments show that our method achieve 52.51\% and 46.04\% on the\textit{normal} and \textit{indirect} splits of EgoTaskQA, and 13.2\% onQAEGO4D, both reaching the state-of-the-art performance.</description>
      <author>example@mail.com (Jiayi Zou, Chaofan Chen, Bing-Kun Bao, Changsheng Xu)</author>
      <guid isPermaLink="false">2510.20285v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>PPMStereo: Pick-and-Play Memory Construction for Consistent Dynamic Stereo Matching</title>
      <link>http://arxiv.org/abs/2510.20178v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为PPMStereo的新方法，通过引入内存缓冲区和两阶段决策过程（选择和播放），实现了从立体视频中估计时间上一致的深度信息，在保持计算效率的同时提高了时空一致性。&lt;h4&gt;背景&lt;/h4&gt;从立体视频中估计时间上一致的深度信息对增强现实等实际应用至关重要，因为深度估计的不一致会破坏用户沉浸感。然而，这项任务具有挑战性，因为很难以计算高效的方式建模长期的时间一致性。之前的方法在时空建模的广度和计算效率之间存在权衡。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在解决立体视频中时间一致深度估计的挑战，特别是如何在保持计算效率的同时建模长程时空一致性，开发一种能够实现高效信息聚合且保持深度估计时间一致性的方法。&lt;h4&gt;方法&lt;/h4&gt;作者提出了一种名为PPMStereo的方法，引入了内存缓冲区用于建模长程时空一致性。受人类两阶段决策过程的启发，PPMStereo包含一个'选择'过程（识别最相关的帧）和一个'播放'过程（为时空聚合自适应地加权所选帧），形成一种两阶段协作过程。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验验证了PPMStereo的有效性，表明其在准确性和时间一致性方面达到了最先进的性能。在Sintel clean/final数据集上，PPMStereo实现了0.62/1.11 TEPE的性能，比BiDAStereo分别提高了17.3%和9.02%，且计算成本更低。&lt;h4&gt;结论&lt;/h4&gt;PPMStereo通过创新的内存缓冲区和两阶段决策过程，成功解决了立体视频中时间一致深度估计的挑战，在保持计算效率的同时提高了时空一致性，为增强现实等实际应用提供了更可靠的深度估计技术。&lt;h4&gt;翻译&lt;/h4&gt;从立体视频中估计时间上一致的深度信息对于增强现实等实际应用至关重要，因为不一致的深度估计会破坏用户的沉浸感。尽管如此，由于难以以计算高效的方式建模长期时间一致性，这项任务仍然具有挑战性。先前的方法试图通过聚合时空信息来解决这一问题，但面临一个基本的权衡：有限的时空建模只能带来适度的提升，而捕捉长程依赖关系则会显著增加计算成本。为了解决这一限制，我们引入了一个内存缓冲区，用于建模长程时空一致性，同时实现高效的动态立体匹配。受人类两阶段决策过程的启发，我们提出了一种用于动态立体匹配的选择并播放记忆(PPM)构建模块，称为PPMStereo。PPM包括一个'选择'过程，用于识别最相关的帧，以及一个'播放'过程，用于为时空聚合自适应地加权所选帧。这种两阶段协作过程保持了一个紧凑但信息丰富的内存缓冲区，同时实现了时间上一致的信息聚合。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决动态场景中的立体匹配问题，即在视频序列中保持深度估计的时间一致性，避免出现闪烁和模糊等不一致现象。这个问题在现实中非常重要，因为像增强现实、自动驾驶和机器人等应用需要时间一致的深度估计来提供稳定可靠的用户体验，而不一致的深度估计会严重影响用户体验和系统性能。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：一些方法使用小时间窗口限制了信息传播，而扩大窗口则计算成本过高且不考虑帧可靠性差异。作者借鉴了人类决策的两阶段过程（'选择'和'播放'），并参考了视频任务中的记忆模型（如XMem和RMem），但针对立体匹配任务进行了专门改进。作者设计了一个质量评估模块来评估帧的价值，并采用动态选择机制来构建高效的记忆缓冲区。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是引入记忆缓冲区来建模长期时空一致性，同时保持计算效率。方法包含两个关键过程：1）'选择'过程：使用质量评估模块识别最相关的K帧，评估标准包括置信度、冗余性和相似性；2）'播放'过程：通过动态记忆调制机制自适应加权选定帧的特征，并使用注意力机制读取记忆缓冲区。整体流程包括特征提取、成本体积构建、上下文编码、记忆缓冲区更新和迭代细化等步骤，通过GRU模块逐步优化视差估计。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）首次将记忆缓冲区引入动态立体匹配任务，实现高效长期建模；2）提出'选择和播放'记忆构建方法，动态选择并加权关键帧；3）引入质量评估模块联合评估帧的置信度、冗余性和相似性；4）设计动态记忆调制机制自适应调整特征权重。相比之前工作，PPMStereo不再使用固定窗口或平等对待所有帧，而是根据帧质量和相关性动态选择和加权，在保持计算效率的同时显著提高了时间一致性和准确性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PPMStereo通过创新的'选择和播放'记忆构建机制，实现了在计算高效的同时保持时间一致性的动态立体匹配，显著提高了深度估计的准确性和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Temporally consistent depth estimation from stereo video is critical forreal-world applications such as augmented reality, where inconsistent depthestimation disrupts the immersion of users. Despite its importance, this taskremains challenging due to the difficulty in modeling long-term temporalconsistency in a computationally efficient manner. Previous methods attempt toaddress this by aggregating spatio-temporal information but face a fundamentaltrade-off: limited temporal modeling provides only modest gains, whereascapturing long-range dependencies significantly increases computational cost.To address this limitation, we introduce a memory buffer for modelinglong-range spatio-temporal consistency while achieving efficient dynamic stereomatching. Inspired by the two-stage decision-making process in humans, wepropose a \textbf{P}ick-and-\textbf{P}lay \textbf{M}emory (PPM) constructionmodule for dynamic \textbf{Stereo} matching, dubbed as \textbf{PPMStereo}. PPMconsists of a `pick' process that identifies the most relevant frames and a`play' process that weights the selected frames adaptively for spatio-temporalaggregation. This two-stage collaborative process maintains a compact yethighly informative memory buffer while achieving temporally consistentinformation aggregation. Extensive experiments validate the effectiveness ofPPMStereo, demonstrating state-of-the-art performance in both accuracy andtemporal consistency. % Notably, PPMStereo achieves 0.62/1.11 TEPE on theSintel clean/final (17.3\% \&amp; 9.02\% improvements over BiDAStereo) with fewercomputational costs. Codes are available at\textcolor{blue}{https://github.com/cocowy1/PPMStereo}.</description>
      <author>example@mail.com (Yun Wang, Junjie Hu, Qiaole Dong, Yongjian Zhang, Yanwei Fu, Tin Lun Lam, Dapeng Wu)</author>
      <guid isPermaLink="false">2510.20178v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>Abstain Mask Retain Core: Time Series Prediction by Adaptive Masking Loss with Representation Consistency</title>
      <link>http://arxiv.org/abs/2510.19980v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 4 figures. Accepted as Spotlight poster in NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究针对时间序列预测中的冗余特征学习问题，提出了一种名为AMRC的创新解决方案，通过动态掩码损失和表示一致性约束提高了预测性能，挑战了传统的时间序列建模假设。&lt;h4&gt;背景&lt;/h4&gt;时间序列预测在能源管理和金融市场等关键领域发挥着重要作用。尽管基于深度学习的方法（如MLP、RNN、Transformer）已取得显著进展，但现有的'长序列信息增益假设'存在固有局限性。&lt;h4&gt;目的&lt;/h4&gt;研究旨在解决现有模型在训练过程中学习大量冗余特征（如噪声或不相关波动）的问题，从而影响有效信号提取，提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为'具有表示一致性的自适应掩码损失'（AMRC）的创新解决方案，包含两个核心组件：1) 动态掩码损失，自适应识别高判别性时间段以指导梯度下降；2) 表示一致性约束，稳定输入、标签和预测之间的映射关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过系统实验发现了一个反直觉现象：适当截断历史数据可以 paradoxically 提高预测准确性，表明现有模型在训练过程中学习了大量冗余特征，损害了有效信号的提取。&lt;h4&gt;结论&lt;/h4&gt;AMRC能有效抑制冗余特征学习，同时显著提高模型性能。这项工作不仅挑战了时间建模中的传统假设，还为开发高效和稳健的预测模型提供了新的理论见解和方法突破。&lt;h4&gt;翻译&lt;/h4&gt;时间序列预测在能源管理和金融市场等关键领域发挥着关键作用。尽管基于深度学习的方法（如MLP、RNN、Transformer）已取得显著进展，但现有的'长序列信息增益假设'存在固有局限性。通过系统实验，本研究揭示了一个反直觉现象：适当截断历史数据可以 paradoxically 提高预测准确性，表明现有模型在训练过程中学习了大量冗余特征（如噪声或不相关波动），从而损害了有效信号的提取。基于信息瓶颈理论，我们提出了一种名为'具有表示一致性的自适应掩码损失'（AMRC）的创新解决方案，包含两个核心组件：1) 动态掩码损失，在模型训练过程中自适应识别高判别性时间段以指导梯度下降；2) 表示一致性约束，稳定输入、标签和预测之间的映射关系。实验结果表明，AMRC能有效抑制冗余特征学习，同时显著提高模型性能。这项工作不仅挑战了时间建模中的传统假设，还为开发高效和稳健的预测模型提供了新的理论见解和方法突破。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series forecasting plays a pivotal role in critical domains such asenergy management and financial markets. Although deep learning-basedapproaches (e.g., MLP, RNN, Transformer) have achieved remarkable progress, theprevailing "long-sequence information gain hypothesis" exhibits inherentlimitations. Through systematic experimentation, this study reveals acounterintuitive phenomenon: appropriately truncating historical data canparadoxically enhance prediction accuracy, indicating that existing modelslearn substantial redundant features (e.g., noise or irrelevant fluctuations)during training, thereby compromising effective signal extraction. Buildingupon information bottleneck theory, we propose an innovative solution termedAdaptive Masking Loss with Representation Consistency (AMRC), which featurestwo core components: 1) Dynamic masking loss, which adaptively identifiedhighly discriminative temporal segments to guide gradient descent during modeltraining; 2) Representation consistency constraint, which stabilized themapping relationships among inputs, labels, and predictions. Experimentalresults demonstrate that AMRC effectively suppresses redundant feature learningwhile significantly improving model performance. This work not only challengesconventional assumptions in temporal modeling but also provides noveltheoretical insights and methodological breakthroughs for developing efficientand robust forecasting models.</description>
      <author>example@mail.com (Renzhao Liang, Sizhe Xu, Chenggang Xie, Jingru Chen, Feiyang Ren, Shu Yang, Takahiro Yabe)</author>
      <guid isPermaLink="false">2510.19980v1</guid>
      <pubDate>Mon, 27 Oct 2025 14:45:35 +0800</pubDate>
    </item>
    <item>
      <title>A Scalable, Causal, and Energy Efficient Framework for Neural Decoding with Spiking Neural Networks</title>
      <link>http://arxiv.org/abs/2510.20683v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Spikachu是一种基于脉冲神经网络的脑机接口解码框架，具有可扩展性、因果性和高能效性，解决了现有方法在实时应用和能源效率方面的限制。&lt;h4&gt;背景&lt;/h4&gt;脑机接口对神经运动障碍人群具有重要意义，但现有解码方法要么简单但缺乏泛化能力，要么复杂但难以实时应用，且都依赖于高能耗的人工神经网络，难以集成到资源有限的现实系统中。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于脉冲神经网络的、可扩展、因果且节能的神经解码框架，以克服现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;Spikachu直接处理分箱脉冲，将其投影到共享潜在空间，利用适应输入时序的脉冲模块提取特征，然后整合这些潜在表示并解码生成行为预测。研究在6只非人灵长类动物的113个记录会话（总计43小时）上评估该方法。&lt;h4&gt;主要发现&lt;/h4&gt;与因果基线相比，Spikachu在使用少2.26到418.81倍能源的情况下表现更好；将训练扩展到多个会话和受试者可提高性能，并实现向未见过的会话、受试者和任务的少样本迁移。&lt;h4&gt;结论&lt;/h4&gt;Spikachu引入了一种基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗低几个数量级。&lt;h4&gt;翻译&lt;/h4&gt;脑机接口(BCIs)有望为神经运动障碍个体实现言语和假肢控制等关键功能。其成功的关键是神经解码器，即将神经活动映射到预期行为的模型。当前基于学习的解码方法分为两类：简单但缺乏泛化能力的因果模型，或复杂但难以实时应用的非因果模型。两者都面临一个共同挑战，它们依赖于能耗高的人工神经网络骨干，这使得集成到现实世界的资源有限系统中变得困难。脉冲神经网络(SNNs)提供了一种有前景的替代方案。由于它们以因果方式运行，这些模型适合实时使用，并且它们的低能耗需求使其成为电池受限环境的理想选择。为此，我们引入了Spikachu：一种基于SNN的可扩展、因果且节能的神经解码框架。我们的方法通过将分箱脉冲直接投影到共享潜在空间来处理它们，在该空间中，适应输入时序的脉冲模块提取相关特征；然后将这些潜在表示整合和解码以生成行为预测。我们在6只非人灵长类动物的113个记录会话上评估了我们的方法，总计43小时的记录。与因果基线相比，我们的方法在使用少2.26到418.81倍能源的情况下表现更好。此外，我们证明将训练扩展到多个会话和受试者可以提高性能，并实现向未见过的会话、受试者和任务的少样本迁移。总体而言，Spikachu引入了一种基于SNN的可扩展、在线兼容的神经解码框架，其性能与最先进模型相当，同时能耗低几个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Brain-computer interfaces (BCIs) promise to enable vital functions, such asspeech and prosthetic control, for individuals with neuromotor impairments.Central to their success are neural decoders, models that map neural activityto intended behavior. Current learning-based decoding approaches fall into twoclasses: simple, causal models that lack generalization, or complex, non-causalmodels that generalize and scale offline but struggle in real-time settings.Both face a common challenge, their reliance on power-hungry artificial neuralnetwork backbones, which makes integration into real-world, resource-limitedsystems difficult. Spiking neural networks (SNNs) offer a promisingalternative. Because they operate causally these models are suitable forreal-time use, and their low energy demands make them ideal forbattery-constrained environments. To this end, we introduce Spikachu: ascalable, causal, and energy-efficient neural decoding framework based on SNNs.Our approach processes binned spikes directly by projecting them into a sharedlatent space, where spiking modules, adapted to the timing of the input,extract relevant features; these latent representations are then integrated anddecoded to generate behavioral predictions. We evaluate our approach on 113recording sessions from 6 non-human primates, totaling 43 hours of recordings.Our method outperforms causal baselines when trained on single sessions usingbetween 2.26 and 418.81 times less energy. Furthermore, we demonstrate thatscaling up training to multiple sessions and subjects improves performance andenables few-shot transfer to unseen sessions, subjects, and tasks. Overall,Spikachu introduces a scalable, online-compatible neural decoding frameworkbased on SNNs, whose performance is competitive relative to state-of-the-artmodels while consuming orders of magnitude less energy.</description>
      <author>example@mail.com (Georgios Mentzelopoulos, Ioannis Asmanis, Konrad P. Kording, Eva L. Dyer, Kostas Daniilidis, Flavia Vitale)</author>
      <guid isPermaLink="false">2510.20683v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
  <item>
      <title>Monocular Visual 8D Pose Estimation for Articulated Bicycles and Cyclists</title>
      <link>http://arxiv.org/abs/2510.20158v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对关节式自行车和骑行者的类别级8D姿态估计方法，能够从单张RGB图像估计自行车的3D平移、旋转以及转向手柄和踏板相对于车身的旋转，从而提供更细粒度的自行车姿态状态和行驶方向估计。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶中，骑行者属于安全关键类弱势道路使用者(VRU)，准确估计其姿态对过马路意图分类、行为预测和碰撞避免至关重要。与刚性物体不同，关节式自行车由通过关节连接的可移动刚性部件组成，6D姿态方法在自行车转向/踏板角度变化时变得不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够估计自行车完整关节状态的方法，包括3D位置、旋转以及转向手柄和踏板的相对旋转，以提供更准确的骑行者行为预测和碰撞避免能力。&lt;h4&gt;方法&lt;/h4&gt;提出联合估计关节式自行车8D姿态和3D关键点的模型，使用合成和真实图像数据的混合进行训练，以在真实图像上实现良好的泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的8D姿态估计方法能够提供更细粒度的自行车姿态状态和行驶方向估计，与使用刚性规范对象模板的最先进6D姿态估计器相比具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该方法在处理关节式自行车姿态变化方面表现出色，能够更好地理解骑行者的实际行驶意图，为自动驾驶系统提供更可靠的骑行者行为预测能力。&lt;h4&gt;翻译&lt;/h4&gt;在自动驾驶中，骑行者属于安全关键类弱势道路使用者(VRU)，准确估计他们的姿态对于骑行者过马路意图分类、行为预测和碰撞避免至关重要。与刚性物体不同，关节式自行车由通过关节连接的可移动刚性部件组成，并受运动学结构约束。6D姿态方法可以估计刚性自行车的3D旋转和平移，但当自行车的转向/踏板角度变化时，6D方法变得不足。这是因为：1)自行车关节姿态的变化会导致其3D边界框也发生变化，2)3D框的方向不一定与决定实际预期行驶方向的转向方向对齐。在这项工作中，我们介绍了一种针对关节式自行车和骑行者的类别级8D姿态估计方法，可以从单张RGB图像进行估计。除了能够从单张图像估计自行车的3D平移和旋转外，我们的方法还估计其转向手柄和踏板相对于自行车车身的旋转。这两个新参数能够估计更细粒度的自行车姿态状态和行驶方向。我们提出的模型联合估计关节式自行车的8D姿态和3D关键点，并使用合成和真实图像数据的混合进行训练，以在真实图像上泛化。我们包含了一个评估部分，评估了估计的8D姿态参数的准确性，与使用刚性规范对象模板进行匹配的最先进类别级6D姿态估计器相比，我们的方法取得了具有竞争力的分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In Autonomous Driving, cyclists belong to the safety-critical class ofVulnerable Road Users (VRU), and accurate estimation of their pose is criticalfor cyclist crossing intention classification, behavior prediction, andcollision avoidance. Unlike rigid objects, articulated bicycles are composed ofmovable rigid parts linked by joints and constrained by a kinematic structure.6D pose methods can estimate the 3D rotation and translation of rigid bicycles,but 6D becomes insufficient when the steering/pedals angles of the bicyclevary. That is because: 1) varying the articulated pose of the bicycle causesits 3D bounding box to vary as well, and 2) the 3D box orientation is notnecessarily aligned to the orientation of the steering which determines theactual intended travel direction. In this work, we introduce a method forcategory-level 8D pose estimation for articulated bicycles and cyclists from asingle RGB image. Besides being able to estimate the 3D translation androtation of a bicycle from a single image, our method also estimates therotations of its steering handles and pedals with respect to the bicycle bodyframe. These two new parameters enable the estimation of a more fine-grainedbicycle pose state and travel direction. Our proposed model jointly estimatesthe 8D pose and the 3D Keypoints of articulated bicycles, and trains with a mixof synthetic and real image data to generalize on real images. We include anevaluation section where we evaluate the accuracy of our estimated 8D poseparameters, and our method shows promising results by achieving competitivescores when compared against state-of-the-art category-level 6D pose estimatorsthat use rigid canonical object templates for matching.</description>
      <author>example@mail.com (Eduardo R. Corral-Soto, Yang Liu, Yuan Ren, Bai Dongfeng, Liu Bingbing)</author>
      <guid isPermaLink="false">2510.20158v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>OmniMotion-X: Versatile Multimodal Whole-Body Motion Generation</title>
      <link>http://arxiv.org/abs/2510.19789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了OmniMotion-X，一个用于全身人体运动生成的多模态框架，采用自回归扩散变换器以统一的序列到序列方式工作。该框架支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势以及全局时空控制场景，并能灵活组合这些任务。&lt;h4&gt;背景&lt;/h4&gt;人体运动生成领域需要能够处理多种模态输入并生成连贯、可控运动的系统。现有方法在处理多模态任务组合和保持生成内容一致性方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一的多模态框架，能够处理多种运动生成任务，提高生成内容的一致性、风格和时序动态，并实现长时间的真实、连贯、可控运动生成。&lt;h4&gt;方法&lt;/h4&gt;使用自回归扩散变换器作为核心架构，引入参考运动作为条件信号增强一致性，采用渐进式弱到强混合条件训练策略处理多模态冲突，并构建了OmniMoCap-X数据集，整合28个公开MoCap来源，使用GPT-4o自动生成结构化字幕。&lt;h4&gt;主要发现&lt;/h4&gt;OmniMotion-X在多个多模态任务上显著超越现有方法，实现了最先进的性能，能够生成交互式的真实、连贯、可控的长时间运动。&lt;h4&gt;结论&lt;/h4&gt;OmniMotion-X通过统一的多模态框架和创新的条件训练策略，解决了人体运动生成中的多模态整合问题，为动画制作和人机交互提供了有力工具。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了OmniMotion-X，一个用于全身人体运动生成的多模态框架，利用自回归扩散变换器以统一的序列到序列方式工作。OmniMotion-X有效支持多种多模态任务，包括文本到运动、音乐到舞蹈、语音到手势和全局时空控制场景（如运动预测、中间帧生成、补全和关节/轨迹引导合成），以及这些任务的灵活组合。具体而言，我们提出使用参考运动作为新的条件信号，显著提高了生成内容、风格和时序动态的一致性，这对真实动画至关重要。为处理多模态冲突，我们引入了渐进式弱到强混合条件训练策略。为支持高质量多模态训练，我们构建了OmniMoCap-X，这是迄今为止最大的统一多模态运动数据集，整合了10个不同任务中的28个公开MoCap来源，标准化为30fps的SMPL-X格式。为确保详细且一致的标注，我们将序列渲染为视频并使用GPT-4o自动生成结构化和层次化字幕，捕捉低级行动和高层语义。大量实验评估证实，OmniMotion-X显著超越现有方法，在多个多模态任务上展示了最先进的性能，并能实现真实、连贯、可控的长时间运动的交互式生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces OmniMotion-X, a versatile multimodal framework forwhole-body human motion generation, leveraging an autoregressive diffusiontransformer in a unified sequence-to-sequence manner. OmniMotion-X efficientlysupports diverse multimodal tasks, including text-to-motion, music-to-dance,speech-to-gesture, and global spatial-temporal control scenarios (e.g., motionprediction, in-betweening, completion, and joint/trajectory-guided synthesis),as well as flexible combinations of these tasks. Specifically, we propose theuse of reference motion as a novel conditioning signal, substantially enhancingthe consistency of generated content, style, and temporal dynamics crucial forrealistic animations. To handle multimodal conflicts, we introduce aprogressive weak-to-strong mixed-condition training strategy. To enablehigh-quality multimodal training, we construct OmniMoCap-X, the largest unifiedmultimodal motion dataset to date, integrating 28 publicly available MoCapsources across 10 distinct tasks, standardized to the SMPL-X format at 30 fps.To ensure detailed and consistent annotations, we render sequences into videosand use GPT-4o to automatically generate structured and hierarchical captions,capturing both low-level actions and high-level semantics. Extensiveexperimental evaluations confirm that OmniMotion-X significantly surpassesexisting methods, demonstrating state-of-the-art performance across multiplemultimodal tasks and enabling the interactive generation of realistic,coherent, and controllable long-duration motions.</description>
      <author>example@mail.com (Guowei Xu, Yuxuan Bian, Ailing Zeng, Mingyi Shi, Shaoli Huang, Wen Li, Lixin Duan, Qiang Xu)</author>
      <guid isPermaLink="false">2510.19789v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ProTerrain: Probabilistic Physics-Informed Rough Terrain World Modeling</title>
      <link>http://arxiv.org/abs/2510.19364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is submitted to IEEE International Conference on Robotics  and Automation (ICRA) 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种有效的概率框架，用于处理非结构化越野环境中机器人运动预测的不确定性，通过建模地形参数的空间相关偶然不确定性并传播到轨迹预测中，显著提高了预测的准确性。&lt;h4&gt;背景&lt;/h4&gt;不确定性感知的机器人运动预测对于非结构化越野环境中的下游可通行性估计和自主导航至关重要，因为在这种环境中地形是异构的且感知不确定性很高。&lt;h4&gt;目的&lt;/h4&gt;引入一个有效的概率框架，明确地对地形参数的空间相关偶然不确定性建模为概率世界模型，并通过可微分物理引擎传播这种不确定性，实现概率轨迹预测。&lt;h4&gt;方法&lt;/h4&gt;利用结构化卷积算子，提供高分辨率多变量预测，同时保持可管理的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;在公开数据集上的实验评估显示，与偶然不确定性估计基线相比，该方法的不确定性估计和轨迹预测准确性显著提高。&lt;h4&gt;结论&lt;/h4&gt;通过明确建模和传播空间相关的偶然不确定性，该方法能够提供更可靠的机器人轨迹预测，适用于复杂的越野环境。&lt;h4&gt;翻译&lt;/h4&gt;不确定性感知的机器人运动预测对于非结构化越野环境中的下游可通行性估计和安全自主导航至关重要，在这种环境中地形是异构的且感知不确定性很高。大多数现有方法假设确定性或空间独立的地面不确定性，忽略了3D空间数据的固有局部相关性，并且通常产生不可靠的预测。在这项工作中，我们引入了一个有效的概率框架，明确地将地形参数的空间相关偶然不确定性建模为概率世界模型，并通过可微分物理引擎传播这种不确定性以实现概率轨迹预测。通过利用结构化卷积算子，我们的方法在可管理的计算成本下提供了高分辨率的多变量预测。在公开可用数据集上的实验评估表明，与偶然不确定性估计基线相比，不确定性估计和轨迹预测准确性显著提高。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人越野导航中不确定性感知的运动预测问题。具体来说，现有方法大多假设地形是确定性的或空间独立的不确定性，忽略了3D空间数据的固有局部相关性，导致预测不可靠。这个问题在现实中非常重要，因为越野环境地形异构、感知不确定性高，不确定性感知的运动预测对可通行性评估和安全的自主导航至关重要，而传统方法忽略了关键的物理地形参数、空间相关性和环境不确定性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性，包括确定性方法、空间独立的不确定性估计方法和物理信息方法。他们借鉴了MonoForce的物理信息世界建模框架，但扩展了它以包含空间相关的不确定性。方法设计考虑了计算效率，通过结构化卷积算子避免显式构造高维协方差矩阵，结合深度学习与可微分物理引擎，实现了从感知到轨迹预测的端到端不确定性传播。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是提出一个端到端的概率框架，显式建模地形参数上的空间相关偶然不确定性，并通过可微分物理引擎传播这种不确定性用于概率轨迹预测，利用结构化卷积算子以可管理的计算成本提供高分辨率多变量预测。整体流程：1)输入车载摄像头图像；2)使用Lift-Splat-Shoot将特征投影到鸟瞰图网格；3)通过卷积层预测地形参数的平均图和方差图；4)将方差图与固定高斯核卷积形成结构化多元高斯分布；5)从概率世界模型中采样；6)通过可微分物理引擎进行轨迹预测。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)具有空间相关偶然不确定性的地形端到端概率世界建模；2)基于卷积的结构化协方差估计的可扩展损失公式；3)通过可微分物理引擎进行不确定性感知的轨迹预测；4)在真实世界数据集上的广泛验证。相比之前工作，不同之处在于：与确定性方法相比显式建模了不确定性；与空间独立方法相比考虑了空间相关性；与现有物理信息方法相比在模型中明确包含空间相关不确定性；与高斯过程等方法相比通过结构化卷积提高了计算效率。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ProTerrain通过引入基于结构化卷积的高效概率框架，解决了越野导航中地形参数空间相关不确定性建模的挑战，实现了从感知到轨迹预测的端到端不确定性传播，显著提高了机器人在复杂环境中的导航安全性和可靠性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Uncertainty-aware robot motion prediction is crucial for downstreamtraversability estimation and safe autonomous navigation in unstructured,off-road environments, where terrain is heterogeneous and perceptualuncertainty is high. Most existing methods assume deterministic or spatiallyindependent terrain uncertainties, ignoring the inherent local correlations of3D spatial data and often producing unreliable predictions. In this work, weintroduce an efficient probabilistic framework that explicitly models spatiallycorrelated aleatoric uncertainty over terrain parameters as a probabilisticworld model and propagates this uncertainty through a differentiable physicsengine for probabilistic trajectory forecasting. By leveraging structuredconvolutional operators, our approach provides high-resolution multivariatepredictions at manageable computational cost. Experimental evaluation on apublicly available dataset shows significantly improved uncertainty estimationand trajectory prediction accuracy over aleatoric uncertainty estimationbaselines.</description>
      <author>example@mail.com (Golnaz Raja, Ruslan Agishev, Miloš Prágr, Joni Pajarinen, Karel Zimmermann, Arun Kumar Singh, Reza Ghabcheloo)</author>
      <guid isPermaLink="false">2510.19364v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>HumanCM: One Step Human Motion Prediction</title>
      <link>http://arxiv.org/abs/2510.16709v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;HumanCM是一种基于一致性模型的人体运动预测框架，能够实现高效的单步生成，无需依赖多步去噪过程。&lt;h4&gt;背景&lt;/h4&gt;现有基于扩散模型的人体运动预测方法通常需要多步去噪过程，计算效率较低。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的单步人体运动预测框架，在减少计算步骤的同时保持或提高预测准确性。&lt;h4&gt;方法&lt;/h4&gt;HumanCM学习嘈杂和清洁运动状态之间的自一致映射，采用基于Transformer的时空架构，使用时间嵌入来建模长程依赖并保持运动一致性。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和HumanEva-I数据集上的实验表明，HumanCM能够达到与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少最多两个数量级。&lt;h4&gt;结论&lt;/h4&gt;HumanCM是一种高效的人体运动预测方法，能够在单步生成中实现高准确性。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了HumanCM，一种基于一致性模型构建的单步人体运动预测框架。HumanCM不依赖于扩散模型中的多步去噪，而是通过学习嘈杂和清洁运动状态之间的自一致映射，执行高效的单步生成。该框架采用基于Transformer的时空架构，使用时间嵌入来建模长程依赖并保持运动一致性。在Human3.6M和HumanEva-I上的实验表明，HumanCM能够达到与最先进的扩散模型相当或更好的准确性，同时将推理步骤减少最多两个数量级。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present HumanCM, a one-step human motion prediction framework built uponconsistency models. Instead of relying on multi-step denoising as indiffusion-based methods, HumanCM performs efficient single-step generation bylearning a self-consistent mapping between noisy and clean motion states. Theframework adopts a Transformer-based spatiotemporal architecture with temporalembeddings to model long-range dependencies and preserve motion coherence.Experiments on Human3.6M and HumanEva-I demonstrate that HumanCM achievescomparable or superior accuracy to state-of-the-art diffusion models whilereducing inference steps by up to two orders of magnitude.</description>
      <author>example@mail.com (Liu Haojie, Gao Suixiang)</author>
      <guid isPermaLink="false">2510.16709v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Bayesian Inference of Primordial Magnetic Field Parameters from CMB with Spherical Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2510.20795v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 6 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于贝叶斯图深度学习的框架，用于从宇宙微波背景图中估算原始磁场宇宙学参数，并结合贝叶斯神经网络实现不确定性量化。&lt;h4&gt;背景&lt;/h4&gt;深度学习已成为现代宇宙学中的变革性方法，为从复杂数据集中提取物理信息提供了强大工具。&lt;h4&gt;目的&lt;/h4&gt;实现一种新颖的贝叶斯图深度学习框架，用于从模拟的宇宙微波背景(CMB)图中直接估算原始磁场(PMF)宇宙学中的关键宇宙学参数。&lt;h4&gt;方法&lt;/h4&gt;使用DeepSphere球形卷积神经网络架构，通过HEALPix像素化尊重CMB数据的球形几何特性，并集成贝叶斯神经网络(BNNs)来捕获偶然性和认知性不确定性，实现稳健的不确定性量化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法表现出卓越的性能，在磁场参数估计中实现了超过0.89的R²分数，并通过方差缩放和GPNormal后训练技术获得了校准良好的不确定性估计。&lt;h4&gt;结论&lt;/h4&gt;集成的DeepSphere-BNNs框架不仅提供了来自带有PMF贡献的CMB图的准确参数估计，还提供了可靠的不确定性量化，为精确宇宙学时代的稳健宇宙学推理提供了必要工具。&lt;h4&gt;翻译&lt;/h4&gt;深度学习已成为现代宇宙学中的变革性方法，为从复杂数据集中提取有意义的物理信息提供了强大工具。本文实现了一种新颖的贝叶斯图深度学习框架，用于从模拟的宇宙微波背景(CMB)图中直接估算原始磁场(PMF)宇宙学中的关键宇宙学参数。我们的方法利用了DeepSphere，这是一种专门设计用于通过HEALPix像素化尊重CMB数据球形几何形状的球形卷积神经网络架构。为了超越确定性点估计并实现稳健的不确定性量化，我们将贝叶斯神经网络(BNNs)集成到框架中，捕获反映模型对其预测置信度的偶然性和认知性不确定性。所提出的方法表现出卓越的性能，在磁场参数估计中实现了超过0.89的R²分数。我们通过方差缩放和GPNormal等后训练技术获得了校准良好的不确定性估计。这种集成的DeepSphere-BNNs框架不仅提供了来自带有PMF贡献的CMB图的准确参数估计，还提供了可靠的不确定性量化，为精确宇宙学时代的稳健宇宙学推理提供了必要工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has emerged as a transformative methodology in moderncosmology, providing powerful tools to extract meaningful physical informationfrom complex astronomical datasets. This paper implements a novel Bayesiangraph deep learning framework for estimating key cosmological parameters in aprimordial magnetic field (PMF) cosmology directly from simulated CosmicMicrowave Background (CMB) maps. Our methodology utilizes DeepSphere, aspherical convolutional neural network architecture specifically designed torespect the spherical geometry of CMB data through HEALPix pixelization. Toadvance beyond deterministic point estimates and enable robust uncertaintyquantification, we integrate Bayesian Neural Networks (BNNs) into theframework, capturing aleatoric and epistemic uncertainties that reflect themodel confidence in its predictions. The proposed approach demonstratesexceptional performance, achieving $R^{2}$ scores exceeding 0.89 for themagnetic parameter estimation. We further obtain well-calibrated uncertaintyestimates through post-hoc training techniques including Variance Scaling andGPNormal. This integrated DeepSphere-BNNs framework not only delivers accurateparameter estimation from CMB maps with PMF contributions but also providesreliable uncertainty quantification, providing the necessary tools for robustcosmological inference in the era of precision cosmology.</description>
      <author>example@mail.com (Juan Alejandro Pinto Castro, Héctor J. Hortúa, Jorge Enrique García-Farieta, Roger Anderson Hurtado)</author>
      <guid isPermaLink="false">2510.20795v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Triage Taint Flows Reported by Dynamic Program Analysis in Node.js Packages</title>
      <link>http://arxiv.org/abs/2510.20739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何使用机器学习技术优先处理程序分析工具报告的漏洞，通过在Node.js包上的实验表明，机器学习模型能有效减少人工审查工作量，同时保持高准确率。&lt;h4&gt;背景&lt;/h4&gt;程序分析工具会产生大量候选漏洞报告，需要昂贵的人工审查，这给安全分析师带来了如何优先处理最可能是真实漏洞的报告的实际挑战。&lt;h4&gt;目的&lt;/h4&gt;研究机器学习是否可以应用于优先处理程序分析工具报告的漏洞，以减轻安全分析师的工作负担。&lt;h4&gt;方法&lt;/h4&gt;收集了1,883个Node.js包的基准测试数据集，每个包包含一个报告的ACE或ACI漏洞；评估了多种机器学习方法，包括经典模型、图神经网络、大型语言模型以及混合模型；所有模型都基于动态程序分析工具的输出数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;顶级大型语言模型达到F1分数0.915，最佳图神经网络和经典机器学习模型达到F1分数0.904；在低于7%的假阴性率下，领先模型可消除66.9%的良性包无需人工审查，每个包处理时间约60毫秒；当模型调整为0.8精度水平时，可检测99.2%的可利用污染流，仅遗漏0.8%。&lt;h4&gt;结论&lt;/h4&gt;该机器学习方法在现实世界漏洞分类中显示出强大的潜力，能显著减少人工审查工作量，同时保持高检测率。&lt;h4&gt;翻译&lt;/h4&gt;程序分析工具经常产生大量候选漏洞报告，需要昂贵的人工审查，这带来了一个实际挑战：安全分析师如何优先处理最可能是真实漏洞的报告？本文研究了机器学习是否可以应用于优先处理程序分析工具报告的漏洞。我们专注于Node.js包，收集了1,883个Node.js包的基准测试，每个包包含一个报告的ACE或ACI漏洞。我们评估了多种机器学习方法，包括经典模型、图神经网络、大型语言模型以及结合GNN和LLMs的混合模型，这些模型都基于动态程序分析工具的输出数据进行训练。顶级LLM达到F1分数0.915，而最佳GNN和经典ML模型达到F1分数0.904。在低于7%的假阴性率下，领先模型消除了66.9%需要人工审查的良性包，每个包处理时间约60毫秒。如果将最佳模型调整为在0.8的精度水平下运行（即在所有警告中允许20%的假阳性），我们的方法可以检测99.2%的可利用污染流，仅遗漏0.8%，这表明在现实世界漏洞分类方面具有强大的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Program analysis tools often produce large volumes of candidate vulnerabilityreports that require costly manual review, creating a practical challenge: howcan security analysts prioritize the reports most likely to be truevulnerabilities?  This paper investigates whether machine learning can be applied toprioritizing vulnerabilities reported by program analysis tools. We focus onNode.js packages and collect a benchmark of 1,883 Node.js packages, eachcontaining one reported ACE or ACI vulnerability. We evaluate a variety ofmachine learning approaches, including classical models, graph neural networks(GNNs), large language models (LLMs), and hybrid models that combine GNN andLLMs, trained on data based on a dynamic program analysis tool's output. Thetop LLM achieves $F_{1} {=} 0.915$, while the best GNN and classical ML modelsreaching $F_{1} {=} 0.904$. At a less than 7% false-negative rate, the leadingmodel eliminates 66.9% of benign packages from manual review, taking around 60ms per package. If the best model is tuned to operate at a precision level of0.8 (i.e., allowing 20% false positives amongst all warnings), our approach candetect 99.2% of exploitable taint flows while missing only 0.8%, demonstratingstrong potential for real-world vulnerability triage.</description>
      <author>example@mail.com (Ronghao Ni, Aidan Z. H. Yang, Min-Chien Hsu, Nuno Sabino, Limin Jia, Ruben Martins, Darion Cassel, Kevin Cheang)</author>
      <guid isPermaLink="false">2510.20739v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Anomaly Prediction with N-BEATS and Graph Neural Network in Multi-variate Semiconductor Process Time Series</title>
      <link>http://arxiv.org/abs/2510.20718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 27 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了半导体制造中的异常预测问题，提出了两种新方法：基于N-BEATS的单变量预测模型和基于图神经网络(GNN)的多变量关系模型。研究显示GNN在性能和效率上均优于N-BEATS模型，为制造环境中的在线异常预测提供了有前景的解决方案。&lt;h4&gt;背景&lt;/h4&gt;半导体制造是极其复杂且精度要求高的过程，涉及数千个相互依赖的参数。多变量时间序列分析在此类环境中至关重要，但异常预测面临传感器数据高维度、真实故障稀有导致的类别不平衡，以及变量间复杂相互依赖关系等挑战。&lt;h4&gt;目的&lt;/h4&gt;推动半导体制造领域从异常检测向异常预测发展，为实现实时工艺校正和主动故障预防提供技术支持。&lt;h4&gt;方法&lt;/h4&gt;提出包含两个阶段的异常预测框架：首先在假设无异常的数据集上训练预测模型，然后对未见时间序列数据进行预测。两种方法差异在于：第一种使用N-BEATS模型进行单变量时间序列预测，假设变量间独立；第二种使用图神经网络捕捉变量间关系，取消独立性假设。&lt;h4&gt;主要发现&lt;/h4&gt;两种模型在20个时间点的预测范围内表现出色，并在50个时间点范围内保持稳定的异常预测能力。GNN持续优于N-BEATS模型，同时需要更少的可训练参数和更低的计算成本。&lt;h4&gt;结论&lt;/h4&gt;图神经网络(GNN)作为在线异常预测解决方案具有显著优势，适合在制造环境中部署，为半导体制造业的实时监控和故障预防提供了有效工具。&lt;h4&gt;翻译&lt;/h4&gt;半导体制造是一个极其复杂且精度要求高的过程，其特点是在各种工具和工艺步骤中收集数千个相互依赖的参数。多变量时间序列分析已成为这类环境中实时监控和故障检测的关键领域。然而，半导体制造中的异常预测面临几个关键挑战，包括传感器数据的高维度和由于真实故障的稀有性导致的严重类别不平衡。此外，变量之间复杂的相互依赖关系使异常预测和根本原因分析都变得复杂。本文提出了两种新颖的方法，推动该领域从异常检测向异常预测发展，这是实现实时工艺校正和主动故障预防的关键步骤。提出的异常预测框架包含两个主要阶段：在假设不含异常的数据集上训练预测模型，然后对未见的时间序列数据进行预测。将预测结果与训练信号的预测进行比较，超出预定阈值的偏差被标记为异常。这两种方法在采用的预测模型上有所不同。第一种利用N-BEATS模型进行单变量时间序列预测，假设变量间相互独立。第二种利用图神经网络捕捉变量间关系，取消了这一假设。两种模型在长达20个时间点的预测范围内都表现出强大的预测性能，并在长达50个时间点的范围内保持稳定的异常预测能力。图神经网络始终优于N-BEATS模型，同时需要显著更少的可训练参数和更低的计算成本。这些结果将图神经网络定位为一种有前景的在线异常预测解决方案，适合在制造环境中部署。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semiconductor manufacturing is an extremely complex and precision-drivenprocess, characterized by thousands of interdependent parameters collectedacross diverse tools and process steps. Multi-variate time-series analysis hasemerged as a critical field for real-time monitoring and fault detection insuch environments. However, anomaly prediction in semiconductor fabricationpresents several critical challenges, including high dimensionality of sensordata and severe class imbalance due to the rarity of true faults. Furthermore,the complex interdependencies between variables complicate both anomalyprediction and root-cause-analysis. This paper proposes two novel approaches toadvance the field from anomaly detection to anomaly prediction, an essentialstep toward enabling real-time process correction and proactive faultprevention. The proposed anomaly prediction framework contains two main stages:(a) training a forecasting model on a dataset assumed to contain no anomalies,and (b) performing forecast on unseen time series data. The forecast iscompared with the forecast of the trained signal. Deviations beyond apredefined threshold are flagged as anomalies. The two approaches differ in theforecasting model employed. The first assumes independence between variables byutilizing the N-BEATS model for univariate time series forecasting. The secondlifts this assumption by utilizing a Graph Neural Network (GNN) to captureinter-variable relationships. Both models demonstrate strong forecastingperformance up to a horizon of 20 time points and maintain stable anomalyprediction up to 50 time points. The GNN consistently outperforms the N-BEATSmodel while requiring significantly fewer trainable parameters and lowercomputational cost. These results position the GNN as promising solution foronline anomaly forecasting to be deployed in manufacturing environments.</description>
      <author>example@mail.com (Daniel Sorensen, Bappaditya Dey, Minjin Hwang, Sandip Halder)</author>
      <guid isPermaLink="false">2510.20718v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GRACE: GRaph-based Addiction Care prEdiction</title>
      <link>http://arxiv.org/abs/2510.20671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为GRACE的图神经网络框架，用于自动确定成瘾患者的适当护理场所，解决了传统方法中存在的类别不平衡问题，并在真实数据中取得显著改进。&lt;h4&gt;背景&lt;/h4&gt;确定成瘾患者的适当护理场所是影响治疗效果和资源利用的关键临床决策。由于专科治疗资源不足，且现有决策方法在成瘾数据集中存在严重类别不平衡问题，需要开发自动化框架。&lt;h4&gt;目的&lt;/h4&gt;开发一个自动化框架来确定成瘾患者的适当护理场所，并解决成瘾数据集中的类别不平衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出新的图神经网络框架GRACE，将护理场所预测形式化为结构化学习问题；进行广泛特征工程；提出获取无偏元图的新方法训练图神经网络以克服类别不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据上的实验结果表明，与竞争基线相比，少数类别的F1分数提高了11-35%。&lt;h4&gt;结论&lt;/h4&gt;GRACE框架在解决成瘾患者护理场所预测问题上有效，特别是在处理类别不平衡问题时表现优异。&lt;h4&gt;翻译&lt;/h4&gt;确定成瘾患者的适当护理场所是影响患者治疗效果和资源有效利用的最关键临床决策之一。由于缺乏足够的专科治疗资源，如住院床位或人员，开发自动化框架的需求尚未得到满足。当前的决策方法在成瘾数据集中存在严重的类别不平衡问题。为解决这一限制，我们提出了一个新的图神经网络框架，将护理场所预测形式化为一个结构化学习问题。此外，我们进行了广泛的特征工程，并提出了一种获得无偏元图的新方法来训练图神经网络，以克服类别不平衡问题。真实世界数据的实验结果表明，与竞争基线相比，少数类别的F1分数提高了11-35%。代码和注释嵌入可在https://anonymous.4open.science/r/GRACE-F8E1/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Determining the appropriate locus of care for addiction patients is one ofthe most critical clinical decisions that affects patient treatment outcomesand effective use of resources. With a lack of sufficient specialized treatmentresources, such as inpatient beds or staff, there is an unmet need to developan automated framework for the same. Current decision-making approaches sufferfrom severe class imbalances in addiction datasets. To address this limitation,we propose a novel graph neural network (GRACE) framework that formalizes locusof care prediction as a structured learning problem. Further, we performextensive feature engineering and propose a new approach of obtaining anunbiased meta-graph to train a GNN to overcome the class imbalance problem.Experimental results in real-world data show an improvement of 11-35% in termsof the F1 score of the minority class over competitive baselines. The codes andnote embeddings are available at https://anonymous.4open.science/r/GRACE-F8E1/.</description>
      <author>example@mail.com (Subham Kumar, Prakrithi Shivaprakash, Koustav Rudra, Lekhansh Shukla, Animesh Mukherjee)</author>
      <guid isPermaLink="false">2510.20671v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Graph Learning for Transmission Congestion Management via Busbar Splitting</title>
      <link>http://arxiv.org/abs/2510.20591v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络(GNN)的电网拓扑优化(NTO)加速方法，用于解决输电网拥堵管理问题。该方法通过母线分裂技术，能够实现大规模系统的近实时优化，并具有良好的泛化能力和跨系统可转移性。&lt;h4&gt;背景&lt;/h4&gt;电网拓扑优化(NTO)通过母线分裂可以缓解输电网拥堵并减少重新调度成本。然而，对于大规模系统，使用现有求解器解决这种混合整数非线性问题在近实时情况下目前是不可行的。虽然机器学习方法是一种有前景的替代方案，但它们对未见的拓扑、变化的运行条件和不同系统的泛化能力有限，限制了其实际应用。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在开发一种能够实现大规模系统近实时电网拓扑优化的方法，同时解决现有方法在泛化能力和跨系统可转移性方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;本文考虑线性化交流潮流(AC PF)，将NTO公式化为拥堵管理问题，并提出了一种图神经网络(GNN)加速方法。研究人员开发了一种异构边缘感知消息传递神经网络，以预测有效的母线分裂行为作为候选NTO解决方案。该方法能够捕获局部流模式，实现对未见的拓扑变化的泛化，并提高跨系统的可转移性。&lt;h4&gt;主要发现&lt;/h4&gt;案例研究显示，所提出的GNN方法在速度上提高了4个数量级，能够在GOC 2000母线系统上一分钟内提供交流可行解，优化间隙为2.3%。这表明该方法在近实时优化方面取得了显著进展。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，所提出的GNN方法在具有拓扑和跨系统泛化能力的大规模系统近实时NTO方面取得了重要进展，为解决电网拥堵管理问题提供了新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;电网拓扑优化(NTO)通过母线分裂可以缓解输电网拥堵并减少重新调度成本。然而，使用现有求解器解决大规模系统的这种混合整数非线性问题在近实时情况下目前是不可行的。机器学习方法已成为一种有前景的替代方案，但它们对未见的拓扑、变化的运行条件和不同系统的泛化能力有限，这限制了它们的实际应用。本文考虑线性化交流潮流(AC PF)，将NTO公式化为拥堵管理问题，并提出了一种图神经网络(GNN)加速方法。我们开发了一种异构边缘感知消息传递神经网络，以预测有效的母线分裂行为作为候选NTO解决方案。所提出的GNN捕获局部流模式，实现对未见的拓扑变化的泛化，并提高了跨系统的可转移性。案例研究显示速度提高了4个数量级，在GOC 2000母线系统上在一分钟内提供交流可行解，优化间隙为2.3%。这些结果表明，在具有拓扑和跨系统泛化能力的大规模系统近实时NTO方面取得了重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network topology optimization (NTO) via busbar splitting can mitigatetransmission grid congestion and reduce redispatch costs. However, solving thismixed-integer non-linear problem for large-scale systems in near-real-time iscurrently intractable with existing solvers. Machine learning (ML) approacheshave emerged as a promising alternative, but they have limited generalizationto unseen topologies, varying operating conditions, and different systems,which limits their practical applicability. This paper formulates NTO forcongestion management problem considering linearized AC PF, and proposes agraph neural network (GNN)-accelerated approach. We develop a heterogeneousedge-aware message passing NN to predict effective busbar splitting actions ascandidate NTO solutions. The proposed GNN captures local flow patterns,achieves generalization to unseen topology changes, and improvestransferability across systems. Case studies show up to 4 orders-of-magnitudespeed-up, delivering AC-feasible solutions within one minute and a 2.3%optimality gap on the GOC 2000-bus system. These results demonstrate asignificant step toward near-real-time NTO for large-scale systems withtopology and cross-system generalization.</description>
      <author>example@mail.com (Ali Rajaei, Peter Palensky, Jochen L. Cremer)</author>
      <guid isPermaLink="false">2510.20591v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Structural Invariance Matters: Rethinking Graph Rewiring through Graph Metrics</title>
      <link>http://arxiv.org/abs/2510.20556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 5 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;图重连是减轻图神经网络和图变换器中过度压缩的关键技术，通过修改图拓扑改善信息流动，但会改变图结构，可能扭曲重要拓扑信号。本研究首次系统分析了重连对图结构指标的影响及其与下游任务性能的关系。&lt;h4&gt;背景&lt;/h4&gt;图重连已成为减轻图神经网络（GNNs）和图变换器（Graph Transformers）中过度压缩的关键技术，通过修改图拓扑来改善信息流动。然而，重连会改变图的结构，有扭曲重要拓扑相关信号的风险。&lt;h4&gt;目的&lt;/h4&gt;提供关于重连如何影响各种图结构指标的系统性分析，以及这些变化如何与下游任务性能相关联，明确需要保留哪些结构属性以确保性能提升和结构保真度。&lt;h4&gt;方法&lt;/h4&gt;研究七种不同的重连策略，并将局部和全局图属性的变化与节点分类准确性进行关联分析。&lt;h4&gt;主要发现&lt;/h4&gt;成功的重连方法倾向于保留局部结构，同时在全局连接方面保持灵活性。这一模式在研究中呈现出一致性。&lt;h4&gt;结论&lt;/h4&gt;这些发现为设计有效的重连策略提供了新的见解，弥合了图理论与实际GNN优化之间的差距。&lt;h4&gt;翻译&lt;/h4&gt;图重连已成为减轻图神经网络（GNNs）和图变换器（Graph Transformers）中过度压缩的关键技术，通过修改图拓扑来改善信息流动。虽然有效，但重连本质上改变了图的结构，带来了扭曲重要拓扑相关信号的风险。然而，尽管重连的使用日益增多，人们尚不清楚需要保留哪些结构属性以确保性能提升和结构保真度。在本工作中，我们首次提供了关于重连如何影响各种图结构指标的系统性分析，以及这些变化如何与下游任务性能相关联。我们研究了七种不同的重连策略，并将局部和全局图属性的变化与节点分类准确性进行关联。我们的结果揭示了一个一致的规律：成功的重连方法倾向于保留局部结构，同时在全局连接方面保持灵活性。这些发现为设计有效的重连策略提供了新的见解，弥合了图理论与实际GNN优化之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph rewiring has emerged as a key technique to alleviate over-squashing inGraph Neural Networks (GNNs) and Graph Transformers by modifying the graphtopology to improve information flow. While effective, rewiring inherentlyalters the graph's structure, raising the risk of distorting importanttopology-dependent signals. Yet, despite the growing use of rewiring, little isknown about which structural properties must be preserved to ensure bothperformance gains and structural fidelity. In this work, we provide the firstsystematic analysis of how rewiring affects a range of graph structuralmetrics, and how these changes relate to downstream task performance. We studyseven diverse rewiring strategies and correlate changes in local and globalgraph properties with node classification accuracy. Our results reveal aconsistent pattern: successful rewiring methods tend to preserve localstructure while allowing for flexibility in global connectivity. These findingsoffer new insights into the design of effective rewiring strategies, bridgingthe gap between graph theory and practical GNN optimization.</description>
      <author>example@mail.com (Alexandre Benoit, Catherine Aitken, Yu He)</author>
      <guid isPermaLink="false">2510.20556v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Intransitive Player Dominance and Market Inefficiency in Tennis Forecasting: A Graph Neural Network Approach</title>
      <link>http://arxiv.org/abs/2510.20454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络的网球比赛预测方法，专门处理非传递性玩家优势现象，并发现博彩市场在此类比赛中存在效率低下的问题。&lt;h4&gt;背景&lt;/h4&gt;非传递性玩家优势（即A击败B，B击败C，但C击败A）在网球比赛中很常见，但很少有预测方法能处理这种复杂关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够建模和利用非传递性关系的预测方法，以识别和利用博彩市场中的效率低下点。&lt;h4&gt;方法&lt;/h4&gt;使用图神经网络方法，通过时序有向图建模玩家关系，其中玩家作为节点，历史比赛结果作为有向边。&lt;h4&gt;主要发现&lt;/h4&gt;博彩公司Pinnacle Sports在处理高非传递性复杂度的比赛时表现不佳；基于图的方法能有效捕捉这些场景中的关系动态；在1903次投注中，使用Kelly下注策略获得了3.26%的显著正回报率。&lt;h4&gt;结论&lt;/h4&gt;博彩市场在处理非传递性比赛时存在效率低下的问题，而所提出的图神经网络方法能够成功利用这种市场效率低下问题。&lt;h4&gt;翻译&lt;/h4&gt;非传递性玩家优势，即玩家A击败B，B击败C，但C击败A，在竞争性网球比赛中很常见。然而，很少有已知的方法尝试将其纳入预测方法中。我们通过图神经网络方法解决了这个问题，该方法通过时序有向图明确建模这些非传递性关系，玩家作为节点，他们的历史比赛结果作为有向边。我们发现博彩公司Pinnacle Sports在处理高非传递性复杂度的比赛时表现不佳，并认为我们的基于图的方法能够捕捉这些场景中的关系动态。当有选择地使用我们的模型对高非传递性比赛进行下注时（准确率65.7%，0.215 Brier分数），在1903次投注中，使用Kelly下注策略获得了3.26%的显著正回报率，表明市场在处理非传递性比赛时存在效率低下的问题，而我们的方法成功地利用了这一点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intransitive player dominance, where player A beats B, B beats C, but C beatsA, is common in competitive tennis. Yet, there are few known attempts toincorporate it within forecasting methods. We address this problem with a graphneural network approach that explicitly models these intransitive relationshipsthrough temporal directed graphs, with players as nodes and their historicalmatch outcomes as directed edges. We find the bookmaker Pinnacle Sports poorlyhandles matches with high intransitive complexity and posit that ourgraph-based approach is uniquely positioned to capture relational dynamics inthese scenarios. When selectively betting on higher intransitivity matchupswith our model (65.7% accuracy, 0.215 Brier Score), we achieve significantpositive returns of 3.26% ROI with Kelly staking over 1903 bets, suggesting amarket inefficiency in handling intransitive matchups that our approachsuccessfully exploits.</description>
      <author>example@mail.com (Lawrence Clegg, John Cartlidge)</author>
      <guid isPermaLink="false">2510.20454v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Distributional Invariance in Causal Subgraph for IRM-Free Graph Generalization</title>
      <link>http://arxiv.org/abs/2510.20295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种不需要不变风险最小化框架的方法，用于捕获因果子图，解决分布偏移下图神经网络的分布外泛化挑战。通过识别因果子图在不同环境中具有较小分布变化的特性，建立了不变分布标准，并基于此提出了范数引导的不变分布目标方法，实验证明该方法优于现有最先进方法。&lt;h4&gt;背景&lt;/h4&gt;分布偏移下图神经网络的分布外泛化仍然是一个关键挑战。现有方法通常采用不变风险最小化框架，需要昂贵的环境注释或启发式生成的合成分割。&lt;h4&gt;目的&lt;/h4&gt;开发一种不需要不变风险最小化框架的方法，用于捕获因果子图，以克服现有方法的限制。&lt;h4&gt;方法&lt;/h4&gt;基于不变分布标准，系统地揭示分布偏移与表示范数之间的定量关系，用于识别因果子图；提出一种范数引导的不变分布目标方法，用于因果子图发现和预测。&lt;h4&gt;主要发现&lt;/h4&gt;因果子图在不同环境中表现出比非因果成分小得多的分布变化，这被形式化为不变分布标准，并从理论上得到了证明。&lt;h4&gt;结论&lt;/h4&gt;在两个广泛使用的基准测试上，所提出的方法在图泛化任务中始终优于现有的最先进方法，证明了该方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在分布偏移下的分布外泛化对于图神经网络仍然是一个关键挑战。现有方法通常采用不变风险最小化框架，需要昂贵的环境注释或启发式生成的合成分割。为避免这些限制，本文旨在开发一种不需要不变风险最小化的方法来捕获因果子图。我们首先确定因果子图在不同环境中表现出比非因果成分小得多的分布变化，这被形式化为不变分布标准，并在本文中从理论上进行了证明。基于这一标准，我们系统地揭示了分布偏移与表示范数之间的定量关系，用于识别因果子图，并深入研究了其潜在机制。最后，我们通过引入一个基于范数的不变分布目标，提出了一种不需要不变风险最小化的方法，用于因果子图发现和预测。在两个广泛使用的基准测试上的大量实验表明，我们的方法在图泛化方面始终优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution generalization under distributional shifts remains acritical challenge for graph neural networks. Existing methods generally adoptthe Invariant Risk Minimization (IRM) framework, requiring costly environmentannotations or heuristically generated synthetic splits. To circumvent theselimitations, in this work, we aim to develop an IRM-free method for capturingcausal subgraphs. We first identify that causal subgraphs exhibit substantiallysmaller distributional variations than non-causal components across diverseenvironments, which we formalize as the Invariant Distribution Criterion andtheoretically prove in this paper. Building on this criterion, wesystematically uncover the quantitative relationship between distributionalshift and representation norm for identifying the causal subgraph, andinvestigate its underlying mechanisms in depth. Finally, we propose an IRM-freemethod by introducing a norm-guided invariant distribution objective for causalsubgraph discovery and prediction. Extensive experiments on two widely usedbenchmarks demonstrate that our method consistently outperformsstate-of-the-art methods in graph generalization.</description>
      <author>example@mail.com (Yang Qiu, Yixiong Zou, Jun Wang, Wei Liu, Xiangyu Fu, Ruixuan Li)</author>
      <guid isPermaLink="false">2510.20295v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Layer-to-Layer Knowledge Mixing in Graph Neural Network for Chemical Property Prediction</title>
      <link>http://arxiv.org/abs/2510.20236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为层到层知识混合(LKM)的自知识蒸馏方法，能够在不显著增加计算复杂性的情况下提高图神经网络(GNNs)预测分子性质的准确性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)是目前预测分子性质最有效的方法，但仍需要更准确的模型。增加模型复杂度可以提高准确性，但也会增加训练和推理过程中的计算成本和内存需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种提高GNN准确性的方法，同时不显著增加计算复杂性和内存需求。&lt;h4&gt;方法&lt;/h4&gt;提出层到层知识混合(LKM)方法，通过最小化GNN层现有隐藏嵌入之间的平均绝对距离，有效聚合多跳和多尺度信息，改进局部和全局分子特征的表示。&lt;h4&gt;主要发现&lt;/h4&gt;LKM方法在不显著增加训练和推理复杂性的情况下，提高了最先进GNN的准确性。使用三种不同的GNN架构(DimeNet++、MXMNet和PAMNet)和三个数据集(QM9、MD17和Chignolin)进行评估，LKM将量子化学和生物物理性质预测的平均绝对误差分别降低了最高9.8%(QM9)、45.3%(MD17能量)和22.9%(Chignolin)。&lt;h4&gt;结论&lt;/h4&gt;LKM有潜力显著提高GNN预测化学性质的准确性，而不会显著增加训练和推理成本。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)是目前预测分子性质最有效的方法，但仍需要更准确的模型。通过增加模型复杂度可以提高GNN的准确性，但这也会增加训练和推理过程中的计算成本和内存需求。在本研究中，我们开发了层到层知识混合(LKM)，一种新颖的自知识蒸馏方法，它在提高最先进GNN准确性的同时，在训练和推理过程中只增加了微不足道的计算复杂度。通过最小化GNN层现有隐藏嵌入之间的平均绝对距离，LKM有效地聚合了多跳和多尺度信息，实现了对局部和全局分子特征的改进表示。我们使用三种不同的GNN架构(DimeNet++、MXMNet和PAMNet)和量子化学性质数据集(QM9、MD17和Chignolin)评估了LKM。我们发现LKM方法将量子化学和生物物理性质预测的平均绝对误差分别降低了高达9.8%(QM9)、45.3%(MD17能量)和22.9%(Chignolin)。这项工作证明了LKM在不显著增加训练和推理成本的情况下，显著提高GNN化学性质预测准确性的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are the currently most effective methods forpredicting molecular properties but there remains a need for more accuratemodels. GNN accuracy can be improved by increasing the model complexity butthis also increases the computational cost and memory requirement duringtraining and inference. In this study, we develop Layer-to-Layer KnowledgeMixing (LKM), a novel self-knowledge distillation method that increases theaccuracy of state-of-the-art GNNs while adding negligible computationalcomplexity during training and inference. By minimizing the mean absolutedistance between pre-existing hidden embeddings of GNN layers, LKM efficientlyaggregates multi-hop and multi-scale information, enabling improvedrepresentation of both local and global molecular features. We evaluated LKMusing three diverse GNN architectures (DimeNet++, MXMNet, and PAMNet) usingdatasets of quantum chemical properties (QM9, MD17 and Chignolin). We foundthat the LKM method effectively reduces the mean absolute error of quantumchemical and biophysical property predictions by up to 9.8% (QM9), 45.3% (MD17Energy), and 22.9% (Chignolin). This work demonstrates the potential of LKM tosignificantly improve the accuracy of GNNs for chemical property predictionwithout any substantial increase in training and inference cost.</description>
      <author>example@mail.com (Teng Jiek See, Daokun Zhang, Mario Boley, David K. Chalmers)</author>
      <guid isPermaLink="false">2510.20236v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Extending machine learning model for implicit solvation to free energy calculations</title>
      <link>http://arxiv.org/abs/2510.20103v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种基于图神经网络的隐式溶剂模型LSNN，通过结合力匹配和化学变量导数匹配，实现了与显式溶剂模型相当的自由能预测准确性，同时提高了计算效率，为药物发现应用奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;隐式溶剂方法在分子模拟中计算效率高，但与显式溶剂模型相比准确性不足，限制了其在精确热力学计算中的应用。基于机器学习的方法目前主要依靠力匹配，可能导致能量预测相差任意常数，不适合绝对自由能比较。&lt;h4&gt;目的&lt;/h4&gt;开发更精确的隐式溶剂势能，解决当前基于机器学习的方法仅依靠力匹配的缺点，确保不同化学物种的溶剂化自由能可以进行有意义的比较。&lt;h4&gt;方法&lt;/h4&gt;引入基于图神经网络(GNN)的隐式溶剂模型Lambda Solvation Neural Network (LSNN)，除了力匹配外，还训练网络匹配化学变量的导数。在约30万个小分子的数据集上进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;LSNN实现了与显式溶剂化学模拟相当的自由能预测准确性，同时提供了计算速度提升，为药物发现中的未来应用建立了基础框架。&lt;h4&gt;结论&lt;/h4&gt;LSNN克服了传统隐式溶剂方法的局限性，通过结合力匹配和化学变量导数匹配，确保了不同化学物种的溶剂化自由能可以进行有意义的比较。&lt;h4&gt;翻译&lt;/h4&gt;隐式溶剂方法为分子模拟中的溶剂化效应建模提供了计算效率高的框架。然而，与显式溶剂模型相比，其准确性往往不足，限制了其在精确热力学计算中的应用。机器学习(ML)的最新进展提供了一种克服这些局限性的机会，通过利用神经网络为各种应用开发更精确的隐式溶剂势能。当前基于ML方法的一个主要缺点是其仅依靠力匹配，这可能导致能量预测相差任意常数，因此不适合绝对自由能比较。在此，我们介绍了一种新颖的方法，即基于图神经网络(GNN)的隐式溶剂模型，称为Lambda Solvation Neural Network (LSNN)。除了力匹配外，该网络还被训练以匹配化学变量的导数，确保不同化学物种的溶剂化自由能可以进行有意义的比较。在约30万个小分子的数据集上训练后，LSNN实现了与显式溶剂化学模拟相当的自由能预测准确性，同时提供了计算加速，并为药物发现中的未来应用建立了基础框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The implicit solvent approach offers a computationally efficient framework tomodel solvation effects in molecular simulations. However, its accuracy oftenfalls short compared to explicit solvent models, limiting its use in precisethermodynamic calculations. Recent advancements in machine learning (ML)present an opportunity to overcome these limitations by leveraging neuralnetworks to develop more precise implicit solvent potentials for diverseapplications. A major drawback of current ML-based methods is their reliance onforce-matching alone, which can lead to energy predictions that differ by anarbitrary constant and are therefore unsuitable for absolute free energycomparisons. Here, we introduce a novel methodology with a graph neural network(GNN)-based implicit solvent model, dubbed Lambda Solvation Neural Network(LSNN). In addition to force-matching, this network was trained to match thederivatives of alchemical variables, ensuring that solvation free energies canbe meaningfully compared across chemical species.. Trained on a dataset ofapproximately 300,000 small molecules, LSNN achieves free energy predictionswith accuracy comparable to explicit-solvent alchemical simulations, whileoffering a computational speedup and establishing a foundational framework forfuture applications in drug discovery.</description>
      <author>example@mail.com (Rishabh Dey, Michael Brocidiacono, Kushal Koirala, Alexander Tropsha, Konstantin I. Popov)</author>
      <guid isPermaLink="false">2510.20103v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>RELATE: A Schema-Agnostic Perceiver Encoder for Multimodal Relational Graphs</title>
      <link>http://arxiv.org/abs/2510.19954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了RELATE，一种与模式无关的特征编码器，用于处理关系型多表数据，能与任何通用图神经网络一起使用，在保持性能的同时大幅减少参数数量。&lt;h4&gt;背景&lt;/h4&gt;关系型多表数据在电子商务、医疗健康和科学研究等领域很常见，可表示为具有多模态节点属性的异质时间图。现有图神经网络依赖特定模式的特征编码器，需为每种节点类型和特征列设计单独模块，限制了可扩展性和参数共享。&lt;h4&gt;目的&lt;/h4&gt;开发一种与模式无关、即插即用的特征编码器，能与任何通用图神经网络配合使用，解决现有方法的可扩展性和参数共享问题。&lt;h4&gt;方法&lt;/h4&gt;RELATE采用针对分类、数值、文本和时间属性的共享模态特定编码器，后接Perceiver风格的交叉注意力模块，将特征聚合成固定大小、排列不变的节点表示。&lt;h4&gt;主要发现&lt;/h4&gt;在RelBench基准测试中，RELATE实现了与特定模式编码器相当的性能（差距在3%以内），同时将参数数量减少了高达5倍。&lt;h4&gt;结论&lt;/h4&gt;RELATE设计支持变化的数据模式，并支持通用图神经网络的多数据集预训练，为关系图数据的基础模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;关系型多表数据在电子商务、医疗健康和科学研究等领域很常见，可以自然地表示为具有多模态节点属性的异质时间图。现有的图神经网络依赖于特定模式的特征编码器，需要为每种节点类型和特征列设计单独的模块，这阻碍了可扩展性和参数共享。我们引入了RELATE（关系型实体潜在聚合编码器），这是一种与模式无关的、即插即用的特征编码器，可以与任何通用图神经网络一起使用。RELATE采用针对分类、数值、文本和时间属性的共享模态特定编码器，然后是一个Perceiver风格的交叉注意力模块，将特征聚合成固定大小、排列不变的节点表示。我们在RelBench基准测试的ReLGNN和HGT上评估了RELATE，结果显示它实现了与特定模式编码器相当的性能（差距在3%以内），同时将参数数量减少了高达5倍。这种设计支持变化的数据模式，并支持通用图神经网络的多数据集预训练，为关系图数据的基础模型铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Relational multi-table data is common in domains such as e-commerce,healthcare, and scientific research, and can be naturally represented asheterogeneous temporal graphs with multi-modal node attributes. Existing graphneural networks (GNNs) rely on schema-specific feature encoders, requiringseparate modules for each node type and feature column, which hindersscalability and parameter sharing. We introduce RELATE (Relational Encoder forLatent Aggregation of Typed Entities), a schema-agnostic, plug-and-play featureencoder that can be used with any general purpose GNN. RELATE employs sharedmodality-specific encoders for categorical, numerical, textual, and temporalattributes, followed by a Perceiver-style cross-attention module thataggregates features into a fixed-size, permutation-invariant noderepresentation. We evaluate RELATE on ReLGNN and HGT in the RelBench benchmark,where it achieves performance within 3% of schema-specific encoders whilereducing parameter counts by up to 5x. This design supports varying schemas andenables multi-dataset pretraining for general-purpose GNNs, paving the waytoward foundation models for relational graph data.</description>
      <author>example@mail.com (Joseph Meyer, Divyansha Lachi, Reza Mohammadi, Roshan Reddy Upendra, Eva L. Dyer, Mark Li, Tom Palczewski)</author>
      <guid isPermaLink="false">2510.19954v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>FnRGNN: Distribution-aware Fairness in Graph Neural Network</title>
      <link>http://arxiv.org/abs/2510.19257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了FnRGNN，一种用于图神经网络节点回归的公平性感知处理框架，通过在结构、表示和预测三个层面进行干预，有效减少了组间差异而不牺牲性能。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在结构化数据学习中表现出色，但在回归任务中的公平性研究仍然不足。&lt;h4&gt;目的&lt;/h4&gt;解决图神经网络在节点级回归任务中的公平性问题，特别是处理现有方法无法解决的连续特性挑战。&lt;h4&gt;方法&lt;/h4&gt;FnRGNN框架采用三级干预策略：结构层面的边重加权、表示层面的MMD对齐、以及预测层面的Sinkhorn分布匹配归一化。&lt;h4&gt;主要发现&lt;/h4&gt;在四个真实世界数据集上的实验表明，FnRGNN能够有效减少组间差异，同时保持模型性能。&lt;h4&gt;结论&lt;/h4&gt;多层级干预策略确保了FnRGNN在复杂图拓扑结构下的鲁棒公平性，为图神经网络回归任务中的公平性问题提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)在结构化数据学习中表现出色，但在回归任务中的公平性研究仍然不足。现有方法主要针对分类任务和表示层面的去偏见，无法完全处理节点级回归的连续特性。我们提出了FnRGNN，一个基于GNN的节点回归的公平性感知处理框架，在三个层面进行干预：(i)结构层面的边重加权，(ii)表示层面的MMD对齐，(iii)预测层面的Sinkhorn分布匹配归一化。这种多层级策略确保了在复杂图拓扑结构下的鲁棒公平性。在四个真实世界数据集上的实验表明，FnRGNN减少了组间差异而不牺牲性能。代码可在https://github.com/sybeam27/FnRGNN获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746252.3760796&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) excel at learning from structured data, yetfairness in regression tasks remains underexplored. Existing approaches mainlytarget classification and representation-level debiasing, which cannot fullyaddress the continuous nature of node-level regression. We propose FnRGNN, afairness-aware in-processing framework for GNN-based node regression thatapplies interventions at three levels: (i) structure-level edge reweighting,(ii) representation-level alignment via MMD, and (iii) prediction-levelnormalization through Sinkhorn-based distribution matching. This multi-levelstrategy ensures robust fairness under complex graph topologies. Experiments onfour real-world datasets demonstrate that FnRGNN reduces group disparitieswithout sacrificing performance. Code is available athttps://github.com/sybeam27/FnRGNN.</description>
      <author>example@mail.com (Soyoung Park, Sungsu Lim)</author>
      <guid isPermaLink="false">2510.19257v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Graph Neural Networks: A Mutual Learning Approach</title>
      <link>http://arxiv.org/abs/2510.19223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种图神经网络之间的协作学习框架，在没有预训练教师模型的情况下，让简单浅层的GNN架构相互教学，从而提升模型在推理时的性能，特别是在处理多个任务时。&lt;h4&gt;背景&lt;/h4&gt;知识蒸馏技术是将复杂教师模型专业知识转移到轻量级学生模型的有效工具，特别适合在资源受限设备上部署高性能模型。该方法已成功应用于图神经网络，利用其表达能力生成捕获结构和特征相关信息的节点嵌入。&lt;h4&gt;目的&lt;/h4&gt;探索GNNs之间协作学习的潜力，使相对简单和浅层的GNN架构能够协同学习高效模型，在推理时表现更好，特别是在处理多个任务时。&lt;h4&gt;方法&lt;/h4&gt;提出一个协作学习框架，其中学生GNN集合在整个训练过程中相互教学；引入自适应logit加权单元促进模型间的高效知识交换；采用熵增强技术改进相互学习；这些组件动态赋能模型在训练过程中调整学习策略，优化下游任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;简单浅层的GNN架构能够协同学习高效模型；这些模型在推理时表现更好，特别是在处理多个任务时；提出的自适应logit加权单元和熵增强技术有效促进了模型间的知识交换和相互学习。&lt;h4&gt;结论&lt;/h4&gt;通过协作学习框架，学生GNN能够在没有预训练教师模型的情况下相互教学；提出的方法在节点分类和图分类任务上表现出色；为资源受限环境中的高效模型部署提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;知识蒸馏技术已成为一种强大的工具，用于将复杂教师模型的专业知识转移到轻量级学生模型中，特别适合在资源受限设备上部署高性能模型。这种方法已成功应用于图神经网络，利用其表达能力生成捕获结构和特征相关信息的节点嵌入。在本研究中，我们通过探索GNNs之间协作学习的潜力，偏离了传统的KD方法。在没有预训练教师模型的情况下，我们证明相对简单和浅层的GNN架构能够协同学习高效模型，在推理时表现更好，特别是在处理多个任务时。我们提出了一个协作学习框架，其中学生GNN集合在整个训练过程中相互教学。我们引入了自适应logit加权单元以促进模型间的高效知识交换，以及熵增强技术以改进相互学习。这些组件动态赋能模型在训练过程中调整学习策略，优化下游任务性能。在三个节点分类和图分类数据集上进行的广泛实验证明了我们方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Knowledge distillation (KD) techniques have emerged as a powerful tool fortransferring expertise from complex teacher models to lightweight studentmodels, particularly beneficial for deploying high-performance models inresource-constrained devices. This approach has been successfully applied tograph neural networks (GNNs), harnessing their expressive capabilities togenerate node embeddings that capture structural and feature-relatedinformation. In this study, we depart from the conventional KD approach byexploring the potential of collaborative learning among GNNs. In the absence ofa pre-trained teacher model, we show that relatively simple and shallow GNNarchitectures can synergetically learn efficient models capable of performingbetter during inference, particularly in tackling multiple tasks. We propose acollaborative learning framework where ensembles of student GNNs mutually teacheach other throughout the training process. We introduce an adaptive logitweighting unit to facilitate efficient knowledge exchange among models and anentropy enhancement technique to improve mutual learning. These componentsdynamically empower the models to adapt their learning strategies duringtraining, optimizing their performance for downstream tasks. Extensiveexperiments conducted on three datasets each for node and graph classificationdemonstrate the effectiveness of our approach.</description>
      <author>example@mail.com (Paul Agbaje, Akajyoti Mitra, Afia Anjum, Pranali Khose, Ebelechukwu Nwafor, Habeeb Olufowobi)</author>
      <guid isPermaLink="false">2510.19223v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>An Active Diffusion Neural Network for Graphs</title>
      <link>http://arxiv.org/abs/2510.19202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为ADGNN的新型图神经网络，通过整合多个外部信息源实现主动扩散，解决了传统扩散GNN的过平滑问题，使节点能保留独特特征同时获取全局图结构信息。&lt;h4&gt;背景&lt;/h4&gt;热扩散类比促进了图信息流理解和图神经网络发展，但大多数扩散GNN模拟被动热扩散，存在过平滑问题，限制了捕获全局图信息的能力。这类似于宇宙热寂理论，即封闭系统中能量分布随时间变得均匀，导致节点表示收敛到相同特征向量。&lt;h4&gt;目的&lt;/h4&gt;解决传统扩散GNN中的过平滑问题，使节点能保留独特特征同时有效获取图全局结构信息。&lt;h4&gt;方法&lt;/h4&gt;提出ADGNN（主动扩散图神经网络），通过整合多个外部信息源实现主动扩散，动态影响扩散过程克服过平滑问题。通过直接计算主动扩散迭代公式的闭式解，实现真正的无限扩散。&lt;h4&gt;主要发现&lt;/h4&gt;ADGNN在多种图任务上与最先进GNN模型相比，显著提高了准确性和效率，有效捕获全局图信息并保持节点独特性。&lt;h4&gt;结论&lt;/h4&gt;ADGNN通过主动扩散机制解决了传统扩散GNN的过平滑问题，使节点既能保持独特特征又能获取全局图结构信息，在多种图任务上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;热扩散的类比增强了对图中信息流的理解，并启发了图神经网络(GNNs)的发展。然而，大多数基于扩散的GNN模拟被动热扩散，仍然存在过平滑问题，限制了它们捕获全局图信息的能力。受宇宙热寂理论的启发，该理论认为在封闭系统中能量分布随时间变得均匀，我们认识到，在没有外部输入的情况下，节点表示会随着扩散过程收敛到相同的特征向量。为解决这个问题，我们提出了主动扩散图神经网络(ADGNN)。ADGNN通过整合多个外部信息源实现主动扩散，这些信息源动态影响扩散过程，有效克服了过平滑问题。此外，我们的方法通过直接计算主动扩散迭代公式的闭式解，实现了真正的无限扩散。这使得节点能够保留其独特特征，同时有效地获取对图全局结构的全面理解。我们在各种图任务上将ADGNN与几个最先进的GNN模型进行了评估。结果表明，ADGNN显著提高了准确性和效率，突显了其在捕获全局图信息和保持节点独特性方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The analogy to heat diffusion has enhanced our understanding of informationflow in graphs and inspired the development of Graph Neural Networks (GNNs).However, most diffusion-based GNNs emulate passive heat diffusion, which stillsuffers from over-smoothing and limits their ability to capture global graphinformation. Inspired by the heat death of the universe, which posits thatenergy distribution becomes uniform over time in a closed system, we recognizethat, without external input, node representations in a graph converge toidentical feature vectors as diffusion progresses. To address this issue, wepropose the Active Diffusion-based Graph Neural Network (ADGNN). ADGNN achievesactive diffusion by integrating multiple external information sources thatdynamically influence the diffusion process, effectively overcoming theover-smoothing problem. Furthermore, our approach realizes true infinitediffusion by directly calculating the closed-form solution of the activediffusion iterative formula. This allows nodes to preserve their uniquecharacteristics while efficiently gaining comprehensive insights into thegraph's global structure. We evaluate ADGNN against several state-of-the-artGNN models across various graph tasks. The results demonstrate that ADGNNsignificantly improves both accuracy and efficiency, highlighting itseffectiveness in capturing global graph information and maintaining nodedistinctiveness.</description>
      <author>example@mail.com (Mengying Jiang)</author>
      <guid isPermaLink="false">2510.19202v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Learning noisy tissue dynamics across time scales</title>
      <link>http://arxiv.org/abs/2510.19090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究团队开发了一种仿生机器学习框架，能够直接从实验电影中推断噪声多细胞动力学，成功应用于上皮组织、果蝇翅膀发育和ERK波介导的细胞信号过程。&lt;h4&gt;背景&lt;/h4&gt;组织动力学在从伤口愈合到形态发生的生物过程中起着关键作用，但这些噪声多细胞动力学难以预测。&lt;h4&gt;目的&lt;/h4&gt;引入一个能够直接从实验电影中推断噪声多细胞动力学的仿生机器学习框架。&lt;h4&gt;方法&lt;/h4&gt;该生成模型结合了图神经网络、归一化流和WaveNet算法，将组织表示为神经随机微分方程，其中细胞是 evolving graph 的边。&lt;h4&gt;主要发现&lt;/h4&gt;该机器学习架构反映了底层生物组织的架构，大大减少了训练所需的数据量；该模型能捕捉随机细胞运动并预测细胞在分裂周期中状态的演变；可以准确生成发育系统和细胞信号过程的实验动力学。&lt;h4&gt;结论&lt;/h4&gt;该方法为在生物工程和临床环境中作为数字孪生使用铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;组织动力学在从伤口愈合到形态发生的生物过程中起着关键作用。然而，这些噪声多细胞动力学难以预测。在此，我们引入了一种仿生机器学习框架，能够直接从实验电影中推断噪声多细胞动力学。该生成模型结合了图神经网络、归一化流和WaveNet算法，将组织表示为神经随机微分方程，其中细胞是 evolving graph 的边。与卷积或全连接神经网络相比，该机器学习架构反映了底层生物组织的架构，大大减少了训练所需的数据量。以上皮组织实验为例，我们表明该模型不仅能捕捉随机细胞运动，还能预测细胞在分裂周期中状态的演变。最后，我们证明了该方法可以准确生成发育系统（如果蝇翅膀）和由随机ERK波介导的细胞信号过程的实验动力学，为在生物工程和临床环境中作为数字孪生使用铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tissue dynamics play a crucial role in biological processes ranging fromwound healing to morphogenesis. However, these noisy multicellular dynamics arenotoriously hard to predict. Here, we introduce a biomimetic machine learningframework capable of inferring noisy multicellular dynamics directly fromexperimental movies. This generative model combines graph neural networks,normalizing flows and WaveNet algorithms to represent tissues as neuralstochastic differential equations where cells are edges of an evolving graph.This machine learning architecture reflects the architecture of the underlyingbiological tissues, substantially reducing the amount of data needed to trainit compared to convolutional or fully-connected neural networks. Takingepithelial tissue experiments as a case study, we show that our model not onlycaptures stochastic cell motion but also predicts the evolution of cell statesin their division cycle. Finally, we demonstrate that our method can accuratelygenerate the experimental dynamics of developmental systems, such as the flywing, and cell signaling processes mediated by stochastic ERK waves, paving theway for its use as a digital twin in bioengineering and clinical contexts.</description>
      <author>example@mail.com (Ming Han, John Devany, Michel Fruchart, Margaret L. Gardel, Vincenzo Vitelli)</author>
      <guid isPermaLink="false">2510.19090v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Committors without Descriptors</title>
      <link>http://arxiv.org/abs/2510.18018v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于committor的增强采样方法，用于解决原子模拟中的稀有事件研究挑战。通过结合图神经网络技术，实现了对系统亚稳态之间频繁转换的促进，并对过程过渡态集合进行了广泛采样。&lt;h4&gt;背景&lt;/h4&gt;稀有事件研究是原子模拟中的主要挑战之一，已经提出了几种增强采样方法来解决这一问题。最近有研究建议使用committor（提供对稀有事件的精确形式化描述）来解决这一挑战。&lt;h4&gt;目的&lt;/h4&gt;进一步自动化基于committor的方法，通过将其与图神经网络的强大表达能力相结合，使方法能够直接处理原子坐标而非描述符，并展示基于图的方法在描述溶剂分子在离子对解离或配体结合等系统中作用方面的优势。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于committor的方法，该方法促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。该方法的特点是自洽和半自动，利用变分准则迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入。进一步通过将先前方法与图神经网络的强大表达能力相结合，直接处理原子坐标而非描述符。&lt;h4&gt;主要发现&lt;/h4&gt;基于committor的方法能够促进系统亚稳态之间的频繁转换；该方法允许对过程过渡态集合进行广泛采样；结合图神经网络使方法更加自动化；基于图的方法在描述溶剂分子在离子对解离或配体结合等系统中作用方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过将基于committor的方法与图神经网络相结合，研究成功实现了方法的进一步自动化，并展示了基于图的方法在描述溶剂分子在特定系统中的角色方面的优势，为原子模拟中稀有事件的研究提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;稀有事件研究是原子模拟中的主要挑战之一，已经提出了几种解决这一问题的增强采样方法。最近有研究建议使用committor（提供对稀有事件的精确形式化描述）来解决这一挑战。我们最近跟进这一建议，提出了一种基于committor的方法，该方法促进系统亚稳态之间的频繁转换，并允许对过程过渡态集合进行广泛采样。我们方法的优势之一是自洽和半自动，利用变分准则迭代优化基于神经网络的committor参数化，使用一组物理描述符作为输入。在这里，我们通过将先前方法与图神经网络的强大表达能力相结合，进一步自动化了这一过程，图神经网络可以直接处理原子坐标而非描述符。除了在基准系统上的应用外，我们强调了基于图的方法在描述离子对解离或配体结合等系统中溶剂分子作用方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of rare events is one of the major challenges in atomisticsimulations, and several enhanced sampling methods towards its solution havebeen proposed. Recently, it has been suggested that the use of the committor,which provides a precise formal description of rare events, could be of use inthis context. We have recently followed up on this suggestion and proposed acommittor-based method that promotes frequent transitions between themetastable states of the system and allows extensive sampling of the processtransition state ensemble. One of the strengths of our approach is beingself-consistent and semi-automatic, exploiting a variational criterion toiteratively optimize a neural-network-based parametrization of the committor,which uses a set of physical descriptors as input. Here, we further automatethis procedure by combining our previous method with the expressive power ofgraph neural networks, which can directly process atomic coordinates ratherthan descriptors. Besides applications on benchmark systems, we highlight theadvantages of a graph-based approach in describing the role of solventmolecules in systems, such as ion pair dissociation or ligand binding.</description>
      <author>example@mail.com (Peilin Kang, Jintu Zhang, Enrico Trizio, TingJun Hou, Michele Parrinello)</author>
      <guid isPermaLink="false">2510.18018v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AutoScape: Geometry-Consistent Long-Horizon Scene Generation</title>
      <link>http://arxiv.org/abs/2510.20726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICCV 2025. Project page: https://auto-scape.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AutoScape是一个长时程驾驶场景生成框架，通过创新的RGB-D扩散模型生成几何一致的关键帧，并使用视频扩散模型生成连贯的驾驶视频。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶和场景生成领域，需要生成长时间、几何一致的驾驶场景，这是一个具有挑战性的任务。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够生成长时间（超过20秒）、真实且几何一致的驾驶视频的框架，解决现有方法在长时程场景生成中的局限性。&lt;h4&gt;方法&lt;/h4&gt;AutoScape框架包含一个RGB-D扩散模型，该模型在共享潜在空间中联合处理图像和深度，基于先前生成的关键帧场景几何条件，并使用一致的引导来引导采样过程。给定高质量RGB-D关键帧后，视频扩散模型在关键帧之间进行插值。&lt;h4&gt;主要发现&lt;/h4&gt;AutoScape能够生成超过20秒的真实且几何一致的驾驶视频，相比之前的最先进方法，在长时程FID和FVD评分上分别提高了48.6%和43.0%。&lt;h4&gt;结论&lt;/h4&gt;AutoScape通过创新的RGB-D扩散模型和视频插值方法，显著提高了长时程驾驶场景生成的质量和一致性，为自动驾驶模拟和训练提供了更真实的数据来源。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了AutoScape，一个长时程驾驶场景生成框架。其核心是一个创新的RGB-D扩散模型，迭代生成稀疏的、几何一致的关键帧，作为场景外观和几何的可靠锚点。为了保持长距离几何一致性，模型1)在共享潜在空间中联合处理图像和深度，2)显式地基于先前生成的关键帧的场景几何（即渲染的点云）进行条件化，3)使用一致的引导来引导采样过程。给定高质量的RGB-D关键帧后，视频扩散模型在它们之间进行插值，生成密集且连贯的视频帧。AutoScape生成超过20秒的真实且几何一致的驾驶视频，相比之前的最先进方法，长时程FID和FVD评分分别提高了48.6%和43.0%。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决如何生成长时间（20秒以上）且保持3D几何一致性的驾驶场景视频问题。这个问题在自动驾驶领域至关重要，因为自动驾驶系统需要大量高质量、长时间一致的场景数据进行安全可靠的测试和验证，而当前方法在长时间生成时难以保持几何一致性，限制了实际应用价值。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将长时间场景生成问题分解为稀疏关键帧生成和密集帧插值两个子问题，采用层次化方法解决。核心洞察是几何一致性退化是长时间生成的关键瓶颈，因此需要显式建模几何信息。方法借鉴了扩散模型在图像生成中的成功应用、RGB-D联合建模、ControlNet的条件控制机制以及视频扩散模型，但创新性地设计了RGB-D扩散模型、几何条件机制和Warp Consistent Guidance来提高几何一致性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过层次化方法解决长时间场景生成：先生成稀疏但几何一致的关键帧作为场景锚点，然后在关键帧间插值生成密集视频帧。整体流程分为两个阶段：1)关键帧生成阶段：从输入图像反投影为3D点云，投影到下一视角生成渲染点和掩码，使用RGB-D扩散模型生成新关键帧，迭代添加到点云集合，并用Warp Consistent Guidance提高一致性；2)插值阶段：使用视频扩散模型在关键帧间生成密集视频帧，条件于从关键帧渲染的3D点云确保几何一致性。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)层次化框架，分解为关键帧生成和插值；2)RGB-D扩散模型，联合建模颜色和深度；3)显式几何条件，将渲染点云作为条件输入；4)Warp Consistent Guidance，减少误差累积；5)两阶段训练策略。相比WonderJourney和Vista等之前工作，AutoScape专门设计的RGB-D扩散模型具有更好的几何感知能力，通过关键帧锚点确保长期一致性，并显式利用几何信息而非仅依赖时间一致性模块。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AutoScape提出了一种创新的层次化框架，通过RGB-D扩散模型生成几何一致的关键帧，并结合视频插值实现了长达20秒的高质量、3D一致的驾驶场景视频生成，显著提升了长时间场景生成的质量和一致性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes AutoScape, a long-horizon driving scene generationframework. At its core is a novel RGB-D diffusion model that iterativelygenerates sparse, geometrically consistent keyframes, serving as reliableanchors for the scene's appearance and geometry. To maintain long-rangegeometric consistency, the model 1) jointly handles image and depth in a sharedlatent space, 2) explicitly conditions on the existing scene geometry (i.e.,rendered point clouds) from previously generated keyframes, and 3) steers thesampling process with a warp-consistent guidance. Given high-quality RGB-Dkeyframes, a video diffusion model then interpolates between them to producedense and coherent video frames. AutoScape generates realistic andgeometrically consistent driving videos of over 20 seconds, improving thelong-horizon FID and FVD scores over the prior state-of-the-art by 48.6\% and43.0\%, respectively.</description>
      <author>example@mail.com (Jiacheng Chen, Ziyu Jiang, Mingfu Liang, Bingbing Zhuang, Jong-Chyi Su, Sparsh Garg, Ying Wu, Manmohan Chandraker)</author>
      <guid isPermaLink="false">2510.20726v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ALICE-LRI: A General Method for Lossless Range Image Generation for Spinning LiDAR Sensors without Calibration Metadata</title>
      <link>http://arxiv.org/abs/2510.20708v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为ALICE-LRI的新型方法，实现了从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件。该方法通过自动推断LiDAR传感器的内几何参数，实现了零点损失的点云投影和重建。&lt;h4&gt;背景&lt;/h4&gt;3D LiDAR传感器在自主导航、环境监测和遥感应用中至关重要。为了高效处理这些传感器产生的大量点云数据，LiDAR数据通常被投影到2D距离图像中，这些图像根据点的角度位置和距离来组织点。然而，传统的投影方法存在基本的几何不一致性，导致不可逆的信息丢失，影响高保真应用。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用的、与传感器无关的方法，能够从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件，并保持几何精度在传感器精度范围内。&lt;h4&gt;方法&lt;/h4&gt;ALICE-LRI是一种自动LiDAR内标定估计方法，能够自动逆向工程任何旋转LiDAR传感器的内几何。该方法通过推断关键参数，包括激光束配置、角度分布和每束校准校正，实现无损投影和完整的点云重建，零点损失。&lt;h4&gt;主要发现&lt;/h4&gt;在完整的KITTI和DurLAR数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，所有点云中都没有点丢失。几何精度保持在传感器精度范围内，建立了具有实时性能的几何无损性。压缩案例研究验证了显著的下游效益，展示了实际应用中的显著质量改进。&lt;h4&gt;结论&lt;/h4&gt;从近似到无损LiDAR投影的范式转变，为需要完整几何保存的高精度遥感应用开辟了新的可能性。ALICE-LRI方法代表了LiDAR数据处理领域的重要进展，能够在不损失信息的情况下实现高效处理。&lt;h4&gt;翻译&lt;/h4&gt;3D LiDAR传感器对于自主导航、环境监测和遥感应用中的精密制图至关重要。为了高效处理这些传感器产生的大量点云数据，LiDAR数据通常被投影到2D距离图像中，这些图像根据点的角度位置和距离来组织点。虽然这些距离图像表示能够实现高效处理，但传统的投影方法存在基本的几何不一致性，导致不可逆的信息丢失，影响高保真应用。我们提出了ALICE-LRI（无损距离图像的自动LiDAR内标定估计），这是第一个通用的、与传感器无关的方法，能够从旋转LiDAR点云生成无损距离图像，无需制造商元数据或校准文件。我们的算法通过推断关键参数（包括激光束配置、角度分布和每束校准校正）来自动逆向工程任何旋转LiDAR传感器的内几何，实现无损投影和零点损失的完整点云重建。在完整的KITTI和DurLAR数据集上的全面评估表明，ALICE-LRI实现了完美的点保留，所有点云中都没有点丢失。几何精度保持在传感器精度范围内，建立了具有实时性能的几何无损性。我们还介绍了压缩案例研究，验证了显著的下游效益，展示了实际应用中的显著质量改进。从近似到无损LiDAR投影的范式转变，为需要完整几何保存的高精度遥感应用开辟了新的可能性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决的是如何在不依赖制造商提供的校准元数据的情况下，从旋转式激光雷达(LiDAR)点云生成无损的2D距离图像。这个问题很重要，因为LiDAR传感器在自动驾驶、环境监测和遥感等领域至关重要，而传统投影方法存在几何不一致性导致信息损失，会影响高精度应用的质量。许多实际场景中我们只有已校准的点云数据，而没有原始传感器数据或校准文件，限制了现有方法的应用。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了理想传感器模型和实际传感器模型之间的差异，认识到需要推断出厂校准的几何参数。他们设计了ALICE-LRI方法，包含参数估计和无损投影两个主要阶段。参数估计又分为垂直和水平参数估计，使用Hough变换识别候选扫描线，加权最小二乘法进行拟合，以及冲突解决机制确保一致性。作者借鉴了Hough变换用于特征提取的技术和加权最小二乘法处理异方差噪声，但解决了现有校准研究的逆问题——从已校准点云推断参数而非从原始数据估计参数。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是自动反向推断LiDAR传感器的内在几何结构，无需制造商元数据，直接从点云数据推断关键参数，生成与传感器几何完全一致的距离图像，实现点云完全重建。整体流程分为：1)垂直参数估计：使用Hough变换识别候选扫描线参数，选择一致点，加权最小二乘拟合，解决冲突；2)水平参数估计：对每束进行分辨率穷举搜索，计算水平和方位角偏移，对点数不足的扫描线使用启发式方法；3)距离图像生成：使用估计参数将点云投影到2D图像，确保双射性和完全重建。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)首个通用方法实现从已校准点云生成无损距离图像，无需元数据；2)自动推断传感器内在几何参数；3)实现完全零点损失和几何无损；4)具有实时性能；5)提供开源实现。相比之前工作，传统方法依赖制造商提供的查找表或数据包信息，现有校准研究处理正向问题而非逆问题，大多数方法接受轻微几何失真，而ALICE-LRI实现了完全无损且通用。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ALICE-LRI首次实现了无需传感器元数据即可从旋转式LiDAR点云生成完全无损的距离图像，通过自动推断传感器内在几何参数解决了传统投影方法中的信息损失问题，为高精度遥感应用提供了新的可能性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D LiDAR sensors are essential for autonomous navigation, environmentalmonitoring, and precision mapping in remote sensing applications. Toefficiently process the massive point clouds generated by these sensors, LiDARdata is often projected into 2D range images that organize points by theirangular positions and distances. While these range image representations enableefficient processing, conventional projection methods suffer from fundamentalgeometric inconsistencies that cause irreversible information loss,compromising high-fidelity applications. We present ALICE-LRI (Automatic LiDARIntrinsic Calibration Estimation for Lossless Range Images), the first general,sensor-agnostic method that achieves lossless range image generation fromspinning LiDAR point clouds without requiring manufacturer metadata orcalibration files. Our algorithm automatically reverse-engineers the intrinsicgeometry of any spinning LiDAR sensor by inferring critical parametersincluding laser beam configuration, angular distributions, and per-beamcalibration corrections, enabling lossless projection and complete point cloudreconstruction with zero point loss. Comprehensive evaluation across thecomplete KITTI and DurLAR datasets demonstrates that ALICE-LRI achieves perfectpoint preservation, with zero points lost across all point clouds. Geometricaccuracy is maintained well within sensor precision limits, establishinggeometric losslessness with real-time performance. We also present acompression case study that validates substantial downstream benefits,demonstrating significant quality improvements in practical applications. Thisparadigm shift from approximate to lossless LiDAR projections opens newpossibilities for high-precision remote sensing applications requiring completegeometric preservation.</description>
      <author>example@mail.com (Samuel Soutullo, Miguel Yermo, David L. Vilariño, Óscar G. Lorenzo, José C. Cabaleiro, Francisco F. Rivera)</author>
      <guid isPermaLink="false">2510.20708v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>PointMapPolicy: Structured Point Cloud Processing for Multi-Modal Imitation Learning</title>
      <link>http://arxiv.org/abs/2510.20406v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了PointMapPolicy，一种新颖的机器人操作方法，通过将点云组织成结构化网格并结合RGB数据，实现了高效的多模态感知，在多种操作任务中达到了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;机器人操作系统受益于互补的传感模态，其中每种模态提供独特的环境信息。点云能捕获详细的几何结构，而RGB图像提供丰富的语义上下文。然而，当前点云方法难以捕获细粒度细节，特别是对于复杂任务；而RGB方法缺乏几何意识，限制了其精度和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型方法，结合点云和RGB图像的优势，解决现有方法在几何细节和语义理解方面的局限性，提高机器人操作系统的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;作者提出了PointMapPolicy，一种将扩散策略基于未下采样的结构化点网格的新方法。这种方法创建的数据类型更容易从观测中提取形状和空间关系，并且可以在参考帧之间转换。由于点网格的结构规整，可以直接使用成熟的计算机视觉技术处理3D数据。模型使用xLSTM作为骨干网络，高效融合点图与RGB数据以增强多模态感知。&lt;h4&gt;主要发现&lt;/h4&gt;在RoboCasa和CALVIN基准测试以及真实机器人评估上的大量实验表明，该方法在各种操作任务中实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;PointMapPolicy有效结合了点云和RGB数据的优势，通过结构化点网格和多模态融合，显著提升了机器人操作系统的性能，特别是在需要精细几何理解和语义上下文的复杂任务中。&lt;h4&gt;翻译&lt;/h4&gt;机器人操作系统受益于互补的传感模态，每种模态提供独特的环境信息。点云捕获详细的几何结构，而RGB图像提供丰富的语义上下文。当前点云方法难以捕获细粒度细节，特别是对于复杂任务，而RGB方法缺乏几何意识，这限制了它们的精度和泛化能力。我们引入了PointMapPolicy，一种新颖的方法，将扩散策略基于未下采样的结构化点网格。产生的数据类型更容易从观测中提取形状和空间关系，并且可以在参考帧之间转换。但由于它们在规则网格中的结构，我们能够直接使用成熟的计算机视觉技术处理3D数据。使用xLSTM作为骨干网络，我们的模型高效地将点图与RGB数据融合以增强多模态感知。在RoboCasa和CALVIN基准测试以及真实机器人评估的大量实验中，我们证明我们的方法在各种操作任务中实现了最先进的性能。概述和演示可在我们的项目页面查看：https://point-map.github.io/Point-Map/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决机器人系统中多模态感知的挑战，即如何同时利用点云提供的几何信息和RGB图像提供的语义信息。这个问题很重要，因为机器人执行复杂任务时需要同时理解场景的几何结构和语义内容，而现有的方法要么缺乏几何细节（仅用RGB图像），要么难以处理精细细节（仅用点云），限制了机器人在复杂环境中的精确操作能力和泛化能力。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者受到计算机视觉社区在立体重建方面的最新进展启发，提出将点云转换为结构化的点图表示。他们借鉴了立体重建技术中的点图方法，将其应用于机器人模仿学习领域。设计过程中，他们考虑了点云和RGB图像的优缺点，创建了可以与标准视觉架构兼容的点图表示，并探索了多种融合策略来结合几何和语义信息。同时，他们借鉴了xLSTM架构作为骨干网络，平衡了时间建模能力和计算效率。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将无结构的点云转换为结构化的点图表示，使其能够与标准视觉架构兼容，同时融合点图的几何信息和RGB图像的语义信息。整体实现流程包括：1)将深度图转换为结构化的点图表示，编码为2D网格中的XYZ坐标；2)使用预训练的视觉编码器处理RGB图像和点图；3)采用晚期融合策略(如拼接)来结合多模态信息；4)使用xLSTM作为骨干网络处理多模态输入；5)基于EDM框架应用扩散策略生成动作序列；6)通过少量去噪步骤(4步)生成最终动作。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出点图(point maps)这一新的观察模态，首次在基于扩散的模仿学习中使用；2)将点云结构化为规则的2D网格，可直接应用标准视觉架构，无需KNN和FPS等昂贵操作；3)设计高效的多模态融合策略，平衡几何精度和语义理解；4)使用xLSTM作为骨干网络，相比Transformer具有更高的计算效率。相比之前的工作，PointMapPolicy不需要复杂的点云处理步骤，能同时利用几何和语义信息，且计算效率更高，在多个基准测试中取得了最先进性能。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PointMapPolicy通过结构化点云处理和多模态融合，在机器人模仿学习中实现了几何精度与语义理解的平衡，并在多个基准测试中取得了最先进性能。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulation systems benefit from complementary sensing modalities,where each provides unique environmental information. Point clouds capturedetailed geometric structure, while RGB images provide rich semantic context.Current point cloud methods struggle to capture fine-grained detail, especiallyfor complex tasks, which RGB methods lack geometric awareness, which hinderstheir precision and generalization. We introduce PointMapPolicy, a novelapproach that conditions diffusion policies on structured grids of pointswithout downsampling. The resulting data type makes it easier to extract shapeand spatial relationships from observations, and can be transformed betweenreference frames. Yet due to their structure in a regular grid, we enable theuse of established computer vision techniques directly to 3D data. Using xLSTMas a backbone, our model efficiently fuses the point maps with RGB data forenhanced multi-modal perception. Through extensive experiments on the RoboCasaand CALVIN benchmarks and real robot evaluations, we demonstrate that ourmethod achieves state-of-the-art performance across diverse manipulation tasks.The overview and demos are available on our project page:https://point-map.github.io/Point-Map/</description>
      <author>example@mail.com (Xiaogang Jia, Qian Wang, Anrui Wang, Han A. Wang, Balázs Gyenes, Emiliyan Gospodinov, Xinkai Jiang, Ge Li, Hongyi Zhou, Weiran Liao, Xi Huang, Maximilian Beck, Moritz Reuss, Rudolf Lioutikov, Gerhard Neumann)</author>
      <guid isPermaLink="false">2510.20406v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>NeuralTouch: Neural Descriptors for Precise Sim-to-Real Tactile Robot Control</title>
      <link>http://arxiv.org/abs/2510.20390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了NeuralTouch，一个集成神经描述场（NDF）和触觉传感的多模态框架，通过轻柔的物理交互实现准确、可推广的机器人抓取。&lt;h4&gt;背景&lt;/h4&gt;抓取精度对精确物体操作至关重要，通常需要机器人手与物体仔细对齐。NDF是一种基于视觉的生成抓取姿势的方法，但单独使用时可能因相机校准不完美、点云不完整和物体变化而产生不准确姿势。触觉传感虽能实现更精确接触，但现有方法通常仅限于简单、预定义的接触几何形状。&lt;h4&gt;目的&lt;/h4&gt;开发一个集成NDF和触觉传感的框架，通过轻柔的物理交互实现准确、可推广的抓取，解决视觉方法在精确抓取方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;利用NDF隐式表示目标接触几何形状，训练深度强化学习策略使用触觉反馈来优化抓取。该策略以神经描述符为条件，无需明确指定接触类型。通过模拟中的消融研究和零样本迁移到现实世界任务（如销钉插入孔和瓶盖开启）进行验证，无需额外微调。&lt;h4&gt;主要发现&lt;/h4&gt;NeuralTouch显著提高了抓取精度和鲁棒性，优于基线方法，为精确、接触丰富的机器人操作提供了一个通用框架。&lt;h4&gt;结论&lt;/h4&gt;通过结合视觉（NDF）和触觉传感，NeuralTouch实现了准确、可推广的抓取，为需要精确接触的机器人操作任务提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;抓取精度是精确物体操作的关键前提，通常需要机器人手与物体之间的仔细对齐。神经描述场（NDF）提供了一种有前景的基于视觉的方法，可生成跨物体类别的抓取姿势。然而，仅靠NDF可能因不完美的相机校准、不完整的点云和物体变化而产生不准确姿势。同时，触觉传感能实现更精确接触，但现有方法通常学习限于简单、预定义接触几何形状的策略。在这项工作中，我们介绍了NeuralTouch，一个集成NDF和触觉传感的多模态框架，通过轻柔的物理交互实现准确、可推广的抓取。我们的方法利用NDF隐式表示目标接触几何形状，基于此训练深度强化学习策略，使用触觉反馈来优化抓取。该策略以神经描述符为条件，不需要明确指定接触类型。我们通过模拟中的消融研究和零样本迁移到现实世界操作任务（如销钉插入孔和瓶盖开启）来验证NeuralTouch，无需额外微调。结果表明，NeuralTouch显著提高了抓取精度和鲁棒性，优于基线方法，为精确、接触丰富的机器人操作提供了一个通用框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Grasping accuracy is a critical prerequisite for precise object manipulation,often requiring careful alignment between the robot hand and object. NeuralDescriptor Fields (NDF) offer a promising vision-based method to generategrasping poses that generalize across object categories. However, NDF alone canproduce inaccurate poses due to imperfect camera calibration, incomplete pointclouds, and object variability. Meanwhile, tactile sensing enables more precisecontact, but existing approaches typically learn policies limited to simple,predefined contact geometries. In this work, we introduce NeuralTouch, amultimodal framework that integrates NDF and tactile sensing to enableaccurate, generalizable grasping through gentle physical interaction. Ourapproach leverages NDF to implicitly represent the target contact geometry,from which a deep reinforcement learning (RL) policy is trained to refine thegrasp using tactile feedback. This policy is conditioned on the neuraldescriptors and does not require explicit specification of contact types. Wevalidate NeuralTouch through ablation studies in simulation and zero-shottransfer to real-world manipulation tasks--such as peg-out-in-hole and bottlelid opening--without additional fine-tuning. Results show that NeuralTouchsignificantly improves grasping accuracy and robustness over baseline methods,offering a general framework for precise, contact-rich robotic manipulation.</description>
      <author>example@mail.com (Yijiong Lin, Bowen Deng, Chenghua Lu, Max Yang, Efi Psomopoulou, Nathan F. Lepora)</author>
      <guid isPermaLink="false">2510.20390v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AnyPcc: Compressing Any Point Cloud with a Single Universal Model</title>
      <link>http://arxiv.org/abs/2510.20331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为AnyPcc的通用点云压缩框架，解决了深度学习点云几何压缩中的泛化性问题。该框架通过通用上下文模型和实例自适应微调策略，有效处理了分布外数据，并在15个数据集上实现了最先进的压缩性能。&lt;h4&gt;背景&lt;/h4&gt;基于深度学习的点云几何压缩面临泛化性挑战，主要原因是缺乏鲁棒的上下文模型和对分布外数据的低效处理。&lt;h4&gt;目的&lt;/h4&gt;开发一个通用点云压缩框架AnyPcc，以解决点云几何压缩中的泛化性问题，特别是上下文建模和分布外数据处理方面的挑战。&lt;h4&gt;方法&lt;/h4&gt;AnyPcc包含两个主要组件：1) 通用上下文模型，利用空间和通道分组的先验信息捕获鲁棒的上下文依赖关系；2) 实例自适应微调策略，通过协同显式和隐式压缩范式处理分布外数据，为每个实例微调一小部分网络权重并整合到位流中。&lt;h4&gt;主要发现&lt;/h4&gt;在15个不同数据集上的广泛实验表明，AnyPcc在点云压缩方面设立了新的最先进水平，证明了其有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;AnyPcc成功解决了点云几何压缩中的泛化性问题，通过创新的通用上下文模型和实例自适应微调策略，实现了更好的压缩性能，为点云压缩领域提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;泛化性是基于深度学习的点云几何压缩的一个关键挑战。我们认为这源于两个关键限制：缺乏鲁棒的上下文模型和对分布外数据的低效处理。为解决这两个问题，我们引入了AnyPcc，一个通用的点云压缩框架。AnyPcc首先采用通用上下文模型，利用空间和通道分组的先验信息来捕获鲁棒的上下文依赖关系。其次，我们新颖的实例自适应微调策略通过协同显式和隐式压缩范式来处理分布外数据。它为每个实例微调一小部分网络权重，并将它们整合到位流中，其中权重的边际位成本远小于几何压缩带来的节省。在15个不同数据集组成的基准测试上的大量实验证实，AnyPcc在点云压缩方面设立了新的最先进水平。我们的代码和数据集将被发布以鼓励可重复研究。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决点云压缩中的泛化挑战。现有方法通常针对特定点云密度设计，无法在真实世界中各种密度的点云上保持稳定性能，并且在遇到分布外数据时压缩效率急剧下降。这个问题很重要，因为随着自动驾驶和虚拟现实等应用中3D内容的普及，点云已成为主要数据表示形式，对高效压缩算法有迫切需求，而真实世界的点云数据具有广泛的多样性，许多关键类型（如医学扫描、3D高斯溅射）通常没有专用的训练数据。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先识别出现有方法的局限性：空间上下文模型在密集数据上表现好但在稀疏场景中性能差，而通道级模型虽能处理稀疏输入但牺牲了空间信息。为了解决这一权衡，他们设计了通用上下文模型(UCM)协同整合空间和通道先验。对于分布外数据问题，他们借鉴了参数高效微调技术，结合了隐式神经表示的优点和预训练模型的效率。作者借鉴了图像压缩中的上下文建模技术，但将其应用于几何占用码，还参考了参数高效微调方法，但这些技术在点云压缩领域尚未被探索。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合通用强大的预训练模型与快速的实例特定适应。整体流程包括：1)UCM采用从粗到细的层次结构，通过空间-通道上下文分解处理点云数据；2)使用3D棋盘模式将体素分为两组(G1和G2)，并在每组内将8位占用码分解为两个4位子符号；3)通过协同特征聚合增强上下文；4)IAFT策略只微调网络的一小部分参数(最终预测头)，实现快速实例适应；5)最终压缩位流包含微调后的权重和几何组件两部分；6)解码时先重建模型参数，再对几何数据进行解码。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)AnyPcc框架是首个使用单一统一模型实现高压缩率和跨多样化点云类型鲁棒性能的方法；2)UCM通过创新的棋盘分组策略协同整合空间和通道先验，建立跨越所有点云密度的鲁棒上下文建模；3)IAFT策略通过只微调预训练模型的一小部分参数，实现快速实例特定适应；4)实现了统一的无损和有损压缩。相比之前的工作，AnyPcc解决了类别特定方法的局限性，避免了Unicorn等泛化尝试的非统一架构问题，克服了隐式压缩的高计算成本，并解决了现有上下文模型在密集和稀疏数据之间的权衡问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; AnyPcc通过引入通用上下文模型和实例自适应微调策略，首次实现了使用单一统一模型在各种点云类型上达到最先进的压缩性能，解决了点云压缩中长期存在的泛化挑战。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalization remains a critical challenge for deep learning-based pointcloud geometry compression. We argue this stems from two key limitations: thelack of robust context models and the inefficient handling ofout-of-distribution (OOD) data. To address both, we introduce AnyPcc, auniversal point cloud compression framework. AnyPcc first employs a UniversalContext Model that leverages priors from both spatial and channel-wise groupingto capture robust contextual dependencies. Second, our novel Instance-AdaptiveFine-Tuning (IAFT) strategy tackles OOD data by synergizing explicit andimplicit compression paradigms. It fine-tunes a small subset of network weightsfor each instance and incorporates them into the bitstream, where the marginalbit cost of the weights is dwarfed by the resulting savings in geometrycompression. Extensive experiments on a benchmark of 15 diverse datasetsconfirm that AnyPcc sets a new state-of-the-art in point cloud compression. Ourcode and datasets will be released to encourage reproducible research.</description>
      <author>example@mail.com (Kangli Wang, Qianxi Yi, Yuqi Ye, Shihao Li, Wei Gao)</author>
      <guid isPermaLink="false">2510.20331v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种纯视觉单视图扩散策略学习方法(VO-DP)，通过融合预训练视觉基础模型的语义和几何特征，在机器人操作任务中取得了优异性能，特别是在真实世界任务中明显优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习领域，基于视觉运动的扩散策略学习是机器人操作的主要方向。现有方法多依赖点云作为输入，通过点云特征学习构建场景表示，但缺乏对纯视觉解决方案的深入探索，而纯视觉方案具有显著潜力。&lt;h4&gt;目的&lt;/h4&gt;提出一种纯视觉且单视图的扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合，以提高机器人操作的性能。&lt;h4&gt;方法&lt;/h4&gt;利用VGGT的中间特征，结合来自DINOv2的语义特征和来自交替注意力块的几何特征，通过交叉注意力融合特征，并使用CNN进行空间压缩，形成策略头的输入。&lt;h4&gt;主要发现&lt;/h4&gt;VO-DP在仿真任务中达到64.6%的平均成功率，与基于点云的方法DP3(64.0%)相当，远高于纯视觉基线DP(34.8%)；在真实世界任务中达到87.9%的成功率，显著优于DP3(67.5%)和DP(11.2%)。鲁棒性评估表明VO-DP在各种变化条件下保持高度稳定。&lt;h4&gt;结论&lt;/h4&gt;提出的VO-DP方法在机器人操作任务中表现出色，特别是在真实世界任务中。研究团队还开源了一个支持多机器和多GPU并行训练的机器人操作训练库，兼容多种视觉运动策略。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习的背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数方法依赖点云作为观察输入，通过点云特征学习构建场景表示，从而实现显著的准确性。然而，现有文献缺乏对具有显著潜力的纯视觉解决方案的深入探索。在本文中，我们提出了一种纯视觉且单视图的扩散策略学习方法(VO-DP)，利用预训练的视觉基础模型实现语义和几何特征的有效融合。我们利用VGGT的中间特征，结合来自DINOv2的语义特征和来自交替注意力块的几何特征。特征通过交叉注意力融合，并使用CNN进行空间压缩，形成策略头的输入。大量实验证明，VO-DP不仅显著优于纯视觉基线DP，而且与基于点云的方法DP3相比表现出明显的性能趋势：在仿真任务中，VO-DP的平均成功率为64.6%，与DP3的64.0%相当，远高于DP的34.8%；而在真实世界任务中，它达到87.9%，明显优于DP3的67.5%和DP的11.2%。进一步的鲁棒性评估证实，VO-DP在颜色、大小、背景和光照等变化条件下保持高度稳定。最后，我们开源了一个机器人操作训练库。该库基于Accelerate构建，支持多机器和多GPU并行训练以及混合精度训练。它与DP、DP3和VO-DP等视觉运动策略兼容，并支持RoboTwin模拟器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决的是如何仅使用RGB图像（vision-only）作为输入，实现与基于点云的方法相媲美的机器人操作性能。这个问题重要是因为当前主流机器人操作方法依赖昂贵的深度传感器（如深度相机或LiDAR），导致硬件成本高、系统复杂（需要多传感器校准），且在复杂场景中表现受限。相比之下，仅使用RGB图像的方法成本低得多、实用性高，但现有研究显示其性能通常不如基于点云的方法。提升vision-only方法的性能具有显著的实际应用价值，可以大幅降低机器人系统的部署成本和复杂度。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了当前机器人操作领域的研究现状，指出vision-only方法成本低但性能不足，而基于点云的方法性能好但成本高。他们评估了现有方法，发现vision-only方法的瓶颈主要在于表示学习模块不够发达。作者认识到视觉基础模型（如VGGT、DINOv2）的潜力，这些模型可以直接从RGB图像中提取几何信息。因此，他们提出将语义特征和几何特征进行有效融合的思路，并在不依赖3D传感器的情况下获得丰富的场景理解。该方法借鉴了多个现有工作：使用预训练的VGGT模型作为视觉编码器，利用DINOv2进行语义特征提取，采用Alternating Attention网络进行几何特征提取，并借鉴了扩散模型（Diffusion Policy）的思想进行动作生成。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过预训练的视觉基础模型，从单视角RGB图像中同时提取语义和几何特征，并将这些特征自适应融合，从而在不依赖3D传感器的情况下实现高性能的机器人操作。整体实现流程包括四个主要步骤：1) 视觉编码：使用DINOv2提取语义特征，通过VGGT模型的Alternating Attention网络提取几何特征；2) 特征融合：使用残差交叉注意力机制将语义和几何特征融合，并通过前馈网络进一步处理；3) 场景表示压缩：使用轻量级ResNet对融合特征进行下采样和空间压缩，然后将压缩后的空间特征与本体感受信息连接，形成紧凑的场景表示；4) 动作生成：使用基于去噪扩散概率模型（DDPM）的策略头，以压缩后的场景表示为条件，通过迭代去噪过程预测动作轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次证明了仅使用RGB图像的vision-only方法可以达到与基于点云方法相媲美的性能水平；2) 设计了基于交叉注意力的特征融合模块，能够根据任务需求自适应地融合语义和几何特征；3) 通过空间特征压缩模块，从高维特征中提取关键信息，实现高效的单视角场景表示；4) 开源了DRRM训练库，支持多机多GPU并行训练和混合精度训练。相比之前的工作，VO-DP不再依赖点云或RGB-D等3D输入，仅使用RGB图像；利用预训练的视觉基础模型提取更丰富的特征；设计了专门的特征融合机制；在保持高性能的同时，显著降低了硬件成本和系统复杂度。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VO-DP通过创新性地融合预训练视觉模型的语义和几何特征，首次实现了仅使用低成本RGB图像输入的机器人操作方法达到与基于昂贵3D传感器方法相媲美的性能，同时显著提升了在真实世界环境中的鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v3</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>On Optimal Hyperparameters for Differentially Private Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2510.20616v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 30 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了差分隐私迁移学习中的关键超参数选择问题，特别关注裁剪边界C和批量大小B，揭示了理论理解与实际结果之间的不匹配，并提出改进方法。&lt;h4&gt;背景&lt;/h4&gt;差分隐私迁移学习（即在私有数据上微调预训练模型）是当前在隐私约束下训练大型模型的最先进方法。然而，在关键超参数选择方面存在理论与实践的脱节。&lt;h4&gt;目的&lt;/h4&gt;研究差分隐私迁移学习中两个关键超参数（裁剪边界C和批量大小B）的最优选择方法，解决当前理论理解与实证结果之间的不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;分析裁剪边界C和批量大小B对模型性能的影响，考察梯度分布的变化，在固定计算预算（固定周期）下评估现有启发式方法，分析累积DP噪声对批量大小选择的影响，研究跨任务使用单一(C,B)设置的效果，并分析裁剪作为梯度重加权形式和累积DP噪声的作用。&lt;h4&gt;主要发现&lt;/h4&gt;1. 当前关于如何选择最优C的理论理解（更强的隐私需要更小的C）与实证结果（更强的隐私下更大的C表现更好）之间存在明显不匹配，这是由梯度分布变化引起的。2. 在计算预算有限的情况下，现有的调整批量大小B的启发式方法不适用，而累积DP噪声能更好地解释较小或较大批量的表现差异。3. 跨任务使用单一(C,B)设置会导致次优性能，特别是在从宽松隐私转向严格隐私以及从充足计算转向有限计算的情况下。&lt;h4&gt;结论&lt;/h4&gt;差分隐私迁移学习中的超参数选择需要考虑梯度分布变化和累积DP噪声的影响，不应简单地跨任务应用相同的超参数设置，而应根据隐私需求和计算资源进行针对性调整。&lt;h4&gt;翻译&lt;/h4&gt;差分隐私（DP）迁移学习，即在私有数据上微调预训练模型，是当前在隐私约束下训练大型模型的最先进方法。我们关注此设置中的两个关键超参数：裁剪边界C和批量大小B。我们展示了当前关于如何选择最优C的理论理解（更强的隐私需要更小的C）与实证结果（更强的隐私下更大的C表现更好）之间的明显不匹配，这是由梯度分布变化引起的。假设计算预算有限（固定周期），我们证明了现有的调整B的启发式方法不适用，而累积DP噪声能更好地解释较小或较大批量的表现差异。我们还强调了跨任务使用单一(C,B)设置的常见做法可能导致次优性能。我们发现，当在宽松与严格隐私之间转换以及在充足与有限计算之间转换时，性能下降特别明显，我们通过分析裁剪作为梯度重加权形式和检查累积DP噪声来解释这一现象。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Differentially private (DP) transfer learning, i.e., fine-tuning a pretrainedmodel on private data, is the current state-of-the-art approach for traininglarge models under privacy constraints. We focus on two key hyperparameters inthis setting: the clipping bound $C$ and batch size $B$. We show a clearmismatch between the current theoretical understanding of how to choose anoptimal $C$ (stronger privacy requires smaller $C$) and empirical outcomes(larger $C$ performs better under strong privacy), caused by changes in thegradient distributions. Assuming a limited compute budget (fixed epochs), wedemonstrate that the existing heuristics for tuning $B$ do not work, whilecumulative DP noise better explains whether smaller or larger batches performbetter. We also highlight how the common practice of using a single $(C,B)$setting across tasks can lead to suboptimal performance. We find thatperformance drops especially when moving between loose and tight privacy andbetween plentiful and limited compute, which we explain by analyzing clippingas a form of gradient re-weighting and examining cumulative DP noise.</description>
      <author>example@mail.com (Aki Rehn, Linzh Zhao, Mikko A. Heikkilä, Antti Honkela)</author>
      <guid isPermaLink="false">2510.20616v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Reproducible Demographic Inference for Fairness in Face Analysis</title>
      <link>http://arxiv.org/abs/2510.20482v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个模块化迁移学习方法的人口统计属性推断流水线，以提高面部分析系统公平性评估的可靠性。该方法通过整合预训练的人脸识别编码器与非线性分类头，在性别和种族推断任务上超越了基线方法，特别是在更具挑战性的种族属性上表现优异。研究还引入了通过身份一致性定义的鲁棒性指标，适用于任何人口统计分割方案。&lt;h4&gt;背景&lt;/h4&gt;面部分析系统中的公平性评估通常依赖于自动人口统计属性推断，而人口统计属性推断又依赖于预定义的人口统计分割。公平性审计的有效性取决于DAI过程的可靠性，但这一问题在以往研究中未得到充分重视。&lt;h4&gt;目的&lt;/h4&gt;提高DAI的可靠性，从而获得更少偏差和更低方差的面部分析系统公平性估计；提出一个完全可复现的DAI流水线；为公平审计中的人口统计推断提供可靠基础。&lt;h4&gt;方法&lt;/h4&gt;用模块化迁移学习方法替代传统的端到端训练；整合预训练的人脸识别编码器与非线性分类头；从准确性、公平性和新引入的鲁棒性（通过身份一致性定义）三个维度评估流水线；在多个数据集和训练设置上对性别和种族推断进行基准测试。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模块化迁移学习方法在性别和种族推断上优于强大的基线方法；在更具挑战性的种族属性上表现尤其出色；新引入的鲁棒性指标适用于任何人口统计分割方案。&lt;h4&gt;结论&lt;/h4&gt;该工作为公平审计中的人口统计推断提供了可靠的基础；通过公开训练数据集元数据、完整代码库、预训练模型和评估工具包，促进了研究的透明度和可复现性。&lt;h4&gt;翻译&lt;/h4&gt;面部分析系统中的公平性评估通常依赖于自动人口统计属性推断，而人口统计属性推断本身又依赖于预定义的人口统计分割。然而，公平性审计的有效性取决于DAI过程的可靠性。我们首先提供了这种依赖关系的理论动机，表明提高DAI可靠性可以减少偏差并降低面部分析系统公平性估计的方差。为此，我们提出了一个完全可复现的DAI流水线，用模块化迁移学习方法替代传统的端到端训练。我们的设计整合了预训练的人脸识别编码器与非线性分类头。我们从三个维度评估了这个流水线：准确性、公平性，以及通过身份一致性定义的新引入的鲁棒性概念。所提出的鲁棒性指标适用于任何人口统计分割方案。我们在多个数据集和训练设置上对性别和种族推断进行了基准测试。我们的结果表明，所提出的方法优于强大的基线方法，特别是在更具挑战性的种族属性上。为了促进透明度和可复现性，我们将公开训练数据集元数据、完整代码库、预训练模型和评估工具包。这项工作为公平审计中的人口统计推断提供了可靠的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fairness evaluation in face analysis systems (FAS) typically depends onautomatic demographic attribute inference (DAI), which itself relies onpredefined demographic segmentation. However, the validity of fairness auditinghinges on the reliability of the DAI process. We begin by providing atheoretical motivation for this dependency, showing that improved DAIreliability leads to less biased and lower-variance estimates of FAS fairness.To address this, we propose a fully reproducible DAI pipeline that replacesconventional end-to-end training with a modular transfer learning approach. Ourdesign integrates pretrained face recognition encoders with non-linearclassification heads. We audit this pipeline across three dimensions: accuracy,fairness, and a newly introduced notion of robustness, defined viaintra-identity consistency. The proposed robustness metric is applicable to anydemographic segmentation scheme. We benchmark the pipeline on gender andethnicity inference across multiple datasets and training setups. Our resultsshow that the proposed method outperforms strong baselines, particularly onethnicity, which is the more challenging attribute. To promote transparency andreproducibility, we will publicly release the training dataset metadata, fullcodebase, pretrained models, and evaluation toolkit. This work contributes areliable foundation for demographic inference in fairness auditing.</description>
      <author>example@mail.com (Alexandre Fournier-Montgieux, Hervé Le Borgne, Adrian Popescu, Bertrand Luvison)</author>
      <guid isPermaLink="false">2510.20482v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Machine learning identification of fractional-order vortex beam diffraction process</title>
      <link>http://arxiv.org/abs/2510.20245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于ResNet的深度学习方法，用于在衍射条件下准确识别分数阶涡旋光束的传播距离和拓扑荷，考虑了大气湍流的影响，实现了高精度的识别。&lt;h4&gt;背景&lt;/h4&gt;分数阶涡旋光束具有分数阶轨道角动量(FOAM)模式，理论上可以无限增加传输容量，在测量、光通信和微粒操纵等领域有重要应用前景。然而，当分数阶涡旋光束在自由空间传播时，其螺旋相位的连续性使其在实际应用中容易受到衍射的影响，从而影响OAM模式识别的准确性，严重限制了基于FOAM的光通信的使用。&lt;h4&gt;目的&lt;/h4&gt;实现衍射条件下分数阶涡旋光束的机器学习识别，解决目前尚未报道的紧急问题。&lt;h4&gt;方法&lt;/h4&gt;基于ResNet，提出了一种深度学习方法。利用实验测量和数值模拟的强度分布，创建了大气湍流环境中涡旋光束衍射强度模式的数据集。采用基于迁移学习的改进101层ResNet结构，实现不同传播距离下FOAM模型的准确高效识别。&lt;h4&gt;主要发现&lt;/h4&gt;该方法可以在湍流条件下准确识别传播距离为100厘米、间距为5厘米、模式间距为0.1的FOAM模式，准确率达到99.69%。该方法考虑了空间传输过程中大气湍流的影响，使得识别方案即使在特殊环境中也能实现高精度。它具有区分超细FOAM模式和传播距离的能力，这是传统方法无法实现的。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法解决了分数阶涡旋光束在衍射条件下识别的难题，特别是在考虑大气湍流影响的情况下，实现了高精度的FOAM模式识别，为实际应用提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;分数阶涡旋光束具有分数阶轨道角动量(FOAM)模式，理论上具有无限增加传输容量的潜力。因此，它们在测量、光通信和微粒操纵领域具有重要的应用前景。然而，当分数阶涡旋光束在自由空间传播时，螺旋相位的连续性使它们在实际应用中容易受到衍射的影响，从而影响OAM模式识别的准确性，严重限制了基于FOAM的光通信的使用。实现衍射条件下分数阶涡旋光束的机器学习识别目前是一个紧迫且尚未报道的问题。在本工作中，基于ResNet，提出了一种深度学习(DL)方法，用于准确识别分数阶涡旋光束衍射过程中的传播距离和拓扑荷。利用实验测量和数值模拟的强度分布，创建了大气湍流环境中涡旋光束衍射强度模式的数据集。采用基于迁移学习的改进101层ResNet结构，实现不同传播距离下FOAM模型的准确高效识别。实验结果表明，所提出的方法可以在湍流条件下准确识别传播距离为100厘米、间距为5厘米、模式间距为0.1的FOAM模式，准确率为99.69%。该方法考虑了空间传输过程中大气湍流的影响，使得识别方案即使在特殊环境中也能实现高精度。它具有区分超细FOAM模式和传播距离的能力，这是传统方法无法实现的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.7498/aps.74.20241458&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fractional-order vortex beams possess fractional orbital angular momentum(FOAM) modes, which theoretically have the potential to increase transmissioncapacity infinitely. Therefore, they have significant application prospects inthe fields of measurement, optical communication and micro-particlemanipulation. However, when fractional-order vortex beams propagate in freespace, the discontinuity of the helical phase makes them susceptible todiffraction in practical applications, thereby affecting the accuracy of OAMmode recognition and severely limiting the use of FOAM-based opticalcommunication. Achieving machine learning recognition of fractional-ordervortex beams under diffraction conditions is currently an urgent and unreportedissue. Based on ResNet, a deep learning (DL) method of accurately recognizingthe propagation distance and topological charge of fractional-order vortex beamdiffraction process is proposed in this work. Utilizing both experimentallymeasured and numerically simulated intensity distributions, a dataset of vortexbeam diffraction intensity patterns in atmospheric turbulence environments iscreated. An improved 101-layer ResNet structure based on transfer learning isemployed to achieve accurate and efficient recognition of the FOAM model atdifferent propagation distances. Experimental results show that the proposedmethod can accurately recognize FOAM modes with a propagation distance of 100cm, a spacing of 5 cm, and a mode spacing of 0.1 under turbulent conditions,with an accuracy of 99.69%. This method considers the effect of atmosphericturbulence during spatial transmission, allowing the recognition scheme toachieve high accuracy even in special environments. It has the ability todistinguish ultra-fine FOAM modes and propagation distances, which cannot beachieved by traditional methods.</description>
      <author>example@mail.com (Yan Guo, Heng Lyu, Chunling Ding, Chenzhi Yuan, Ruibo Jin)</author>
      <guid isPermaLink="false">2510.20245v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Transfer Learning for Sequence Labeling Tasks by Adapting Pre-trained Neural Language Models</title>
      <link>http://arxiv.org/abs/2510.20033v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇博士论文提出了三种改进序列标注任务迁移学习的方法，通过调整预训练神经语言模型来提高性能&lt;h4&gt;背景&lt;/h4&gt;序列标注任务的迁移学习需要更有效的预训练语言模型适应方法&lt;h4&gt;目的&lt;/h4&gt;改进序列标注任务的迁移学习方法，使预训练神经语言模型能够更好地适应特定任务&lt;h4&gt;方法&lt;/h4&gt;提出了三种改进方法：1) 引入额外信号的多任务模型；2) 修改自回归大语言模型架构以实现层间双向信息流；3) 利用监督上下文微调结合响应导向适应策略的序列标注框架&lt;h4&gt;主要发现&lt;/h4&gt;预训练神经语言模型在有针对性的迁移学习范式下，在序列标注任务上能够达到最佳性能&lt;h4&gt;结论&lt;/h4&gt;通过有针对性的迁移学习范式调整预训练神经语言模型，可以显著提高其在序列标注任务上的性能&lt;h4&gt;翻译&lt;/h4&gt;这篇博士论文通过调整预训练的神经语言模型，改进了序列标注任务的迁移学习。所提出的迁移学习改进包括引入一个额外信号的多任务模型、基于自回归大语言模型架构修改的方法，以及利用监督上下文微调结合响应导向适应策略的自回归大语言模型序列标注框架。第一个改进是在事件触发检测任务的领域迁移背景下提出的，通过将独立于领域的文本处理系统获得的额外信号整合到多任务模型中来改进领域迁移。第二个改进涉及修改模型架构，为此提出了一个方法，使自回归大语言模型的层之间能够实现双向信息流。第三个改进利用自回归大语言模型作为文本生成器，通过生成式监督上下文微调框架实现。所提出的模型、方法和框架表明，当通过有针对性的迁移学习范式进行调整时，预训练的神经语言模型在序列标注任务上能够达到最佳性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This doctoral thesis improves the transfer learning for sequence labelingtasks by adapting pre-trained neural language models. The proposed improvementsin transfer learning involve introducing a multi-task model that incorporatesan additional signal, a method based on architectural modifications inautoregressive large language models, and a sequence labeling framework forautoregressive large language models utilizing supervised in-contextfine-tuning combined with response-oriented adaptation strategies. The firstimprovement is given in the context of domain transfer for the event triggerdetection task. The domain transfer of the event trigger detection task can beimproved by incorporating an additional signal obtained from adomain-independent text processing system into a multi-task model. The secondimprovement involves modifying the model's architecture. For that purpose, amethod is proposed to enable bidirectional information flow across layers ofautoregressive large language models. The third improvement utilizesautoregressive large language models as text generators through a generativesupervised in-context fine-tuning framework. The proposed model, method, andframework demonstrate that pre-trained neural language models achieve theirbest performance on sequence labeling tasks when adapted through targetedtransfer learning paradigms.</description>
      <author>example@mail.com (David Dukić)</author>
      <guid isPermaLink="false">2510.20033v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Novel Class Discovery for Point Cloud Segmentation via Joint Learning of Causal Representation and Reasoning</title>
      <link>http://arxiv.org/abs/2510.13307v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于因果表示和推理的联合学习方法，用于解决点云分割中的新类别发现问题&lt;h4&gt;背景&lt;/h4&gt;在点云分割任务中，需要学习一个模型，仅使用已标记的基础类别监督信息，来分割未标记的新类别点云&lt;h4&gt;目的&lt;/h4&gt;建立点表示与基础类别标签之间的精确相关性，以及基础类别与新类别点之间的表示相关性&lt;h4&gt;方法&lt;/h4&gt;引入结构因果模型(SCM)重新形式化3D-NCD问题，分析基础类别表示中的隐藏混杂因素，设计消除混杂因素的因果表示原型，并使用图结构建模基础类别与新类别之间的因果关系&lt;h4&gt;主要发现&lt;/h4&gt;粗略或统计相关性学习可能导致新类别推理的混淆，而施加因果关系作为强相关约束可以准确揭示对应类别的本质点云表示&lt;h4&gt;结论&lt;/h4&gt;在3D和2D NCD语义分割任务上的大量实验和可视化结果证明了该方法的优势&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们专注于点云分割的新类别发现(3D-NCD)，旨在学习一个模型，仅使用已标记的基础3D类别的监督信息，来分割未标记的新3D类别。此任务的关键在于建立点表示与其基础类别标签之间的精确相关性，以及基础类别与新类别点之间的表示相关性。粗略或统计相关性学习可能导致新类别推理的混淆。如果我们将因果关系作为强相关约束强加于学习过程，应该能够准确揭示对应于类别的本质点云表示。为此，我们引入结构因果模型(SCM)重新形式化3D-NCD问题，并提出一种新方法，即因果表示和推理的联合学习。具体而言，我们首先通过SCM分析基础类别表示中的隐藏混杂因素以及基础类别与新类别之间的因果关系。我们设计了一个消除混杂因素的因果表示原型，以捕获基础类别的因果表示。然后使用图结构建模基础类别的因果表示原型与新类别原型之间的因果关系，实现从基础类别到新类别的因果推理。在3D和2D NCD语义分割上的大量实验和可视化结果证明了我们方法的优势&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文解决点云分割中的新类别发现问题，即如何仅使用已标记的基础类别数据来分割未标记的新类别。这个问题在自动驾驶、机器人感知等实际应用中非常重要，因为这些场景中环境是动态开放的，可能会突然出现新的物体类别，而传统方法无法处理这些未预先定义的类别，限制了系统在真实世界中的实用性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了传统方法的局限性：它们本质上是统计模型，倾向于学习'捷径特征'而非本质特征，且难以处理新类别。因此，作者引入结构因果模型(SCM)重新形式化问题，识别出混杂因素对基础类别学习的干扰，以及基础到新类别的因果路径。方法设计借鉴了点云分割领域常用的MinkowskiUNet架构、因果学习理论中的独立因果机制原则、对抗训练思想以及图神经网络技术，将它们创新性地结合来解决3D-NCD问题。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过因果表示学习识别和去除点云中的非因果特征(混杂因素)，学习到能准确对应类别的本质表示，并利用基础类别的因果表示通过图结构建模基础到新类别的因果关系，实现因果推理。整体流程包括：1)因果表示原型学习，通过对抗训练去除混杂因素，提取基础类别的因果表示并生成原型；2)构建因果推理图，使用自注意力机制调整边权重，应用因果剪枝和推理方向一致性约束；3)基于图卷积网络生成高质量伪标签，实现新类别的分割。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点有三：1)首次将因果学习引入3D-NCD领域；2)提出因果表示原型学习方法，通过对抗机制消除混杂因素；3)设计基于图的因果推理框架，显式建模类别间因果关系。相比之前工作，不同之处在于：传统方法依赖统计相似性且易受捷径特征干扰，而本文方法通过因果学习提取本质特征；传统方法直接测量类别相似性，而本文使用图结构建模复杂的高阶依赖关系；实验表明本文方法在多个数据集上显著优于现有方法，特别是在新类别分割任务上。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文提出了一种结合因果表示学习和推理的新方法，通过识别和去除点云中的非因果特征并建模基础到新类别的因果关系，显著提升了点云分割中新类别的发现和分割能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on Novel Class Discovery for Point Cloud Segmentation(3D-NCD), aiming to learn a model that can segment unlabeled (novel) 3D classesusing only the supervision from labeled (base) 3D classes. The key to this taskis to setup the exact correlations between the point representations and theirbase class labels, as well as the representation correlations between thepoints from base and novel classes. A coarse or statistical correlationlearning may lead to the confusion in novel class inference. lf we impose acausal relationship as a strong correlated constraint upon the learningprocess, the essential point cloud representations that accurately correspondto the classes should be uncovered. To this end, we introduce a structuralcausal model (SCM) to re-formalize the 3D-NCD problem and propose a new method,i.e., Joint Learning of Causal Representation and Reasoning. Specifically, wefirst analyze hidden confounders in the base class representations and thecausal relationships between the base and novel classes through SCM. We devisea causal representation prototype that eliminates confounders to capture thecausal representations of base classes. A graph structure is then used to modelthe causal relationships between the base classes' causal representationprototypes and the novel class prototypes, enabling causal reasoning from baseto novel classes. Extensive experiments and visualization results on 3D and 2DNCD semantic segmentation demonstrate the superiorities of our method.</description>
      <author>example@mail.com (Yang Li, Aming Wu, Zihao Zhang, Yahong Han)</author>
      <guid isPermaLink="false">2510.13307v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Amplifying Prominent Representations in Multimodal Learning via Variational Dirichlet Process</title>
      <link>http://arxiv.org/abs/2510.20736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeruIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于狄利克雷过程(DP)的多模态学习框架，通过DP的'富者愈富'特性自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡，有效解决了多模态融合中保持特征表达能力和学习跨模态交互的挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态融合在医疗保健和金融等现实世界场景中变得越来越重要，关键挑战是如何在保持每个模态特征表达能力的同时学习跨模态交互。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多模态学习方法，避免过度强调模态边缘分布对齐带来的问题，同时保持每个模态内的有意义表示。&lt;h4&gt;方法&lt;/h4&gt;提出DP驱动的多模态学习框架，假设每个模态遵循多元高斯分布的混合，并采用狄利克雷过程计算所有组件的混合权重，利用其'富者愈富'特性动态分配特征贡献并选择最突出的特征。&lt;h4&gt;主要发现&lt;/h4&gt;在多个多模态数据集上的实验表明，该模型优于其他竞争方法；消融分析验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;DP驱动的多模态学习框架能够自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡，有效解决了多模态融合中的关键挑战。&lt;h4&gt;翻译&lt;/h4&gt;开发有效的多模态融合方法在医疗保健和金融等现实世界场景中变得越来越重要。关键挑战是如何在保持每个模态特征表达能力的同时学习跨模态交互。先前的方法主要关注跨模态对齐，但过度强调模态边缘分布的对齐可能会施加过度的正则化，并阻碍每个模态内的有意义表示。狄利克雷过程(DP)混合模型是一种强大的贝叶斯非参数方法，可以通过其'富者愈富'特性放大最突出的特征，为它们分配不断增加的权重。受DP这一独特特性的启发，我们提出了一种新的DP驱动的多模态学习框架，自动实现显著的模态内表示学习和跨模态对齐之间的最佳平衡。具体而言，我们假设每个模态遵循多元高斯分布的混合，并进一步采用DP计算所有组件的混合权重。这种范式允许DP动态分配特征的贡献并选择最突出的特征，利用其'富者愈富'特性，从而促进多模态特征融合。在多个多模态数据集上的广泛实验证明了我们的模型优于其他竞争模型。消融分析进一步验证了DP在模态分布对齐中的有效性及其对关键超参数变化的鲁棒性。代码已在https://github.com/HKU-MedAI/DPMM.git匿名提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing effective multimodal fusion approaches has become increasinglyessential in many real-world scenarios, such as health care and finance. Thekey challenge is how to preserve the feature expressiveness in each modalitywhile learning cross-modal interactions. Previous approaches primarily focus onthe cross-modal alignment, while over-emphasis on the alignment of marginaldistributions of modalities may impose excess regularization and obstructmeaningful representations within each modality. The Dirichlet process (DP)mixture model is a powerful Bayesian non-parametric method that can amplify themost prominent features by its richer-gets-richer property, which allocatesincreasing weights to them. Inspired by this unique characteristic of DP, wepropose a new DP-driven multimodal learning framework that automaticallyachieves an optimal balance between prominent intra-modal representationlearning and cross-modal alignment. Specifically, we assume that each modalityfollows a mixture of multivariate Gaussian distributions and further adopt DPto calculate the mixture weights for all the components. This paradigm allowsDP to dynamically allocate the contributions of features and select the mostprominent ones, leveraging its richer-gets-richer property, thus facilitatingmultimodal feature fusion. Extensive experiments on several multimodal datasetsdemonstrate the superior performance of our model over other competitors.Ablation analysis further validates the effectiveness of DP in aligningmodality distributions and its robustness to changes in key hyperparameters.Code is anonymously available at https://github.com/HKU-MedAI/DPMM.git</description>
      <author>example@mail.com (Tsai Hor Chan, Feng Wu, Yihang Chen, Guosheng Yin, Lequan Yu)</author>
      <guid isPermaLink="false">2510.20736v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>From Masks to Worlds: A Hitchhiker's Guide to World Models</title>
      <link>http://arxiv.org/abs/2510.20668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Github: https://github.com/M-E-AGI-Lab/Awesome-World-Models&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文不是典型的世界模型综述，而是面向世界构建者的指南，聚焦于从早期掩码模型到记忆增强系统的世界模型发展路径&lt;h4&gt;背景&lt;/h4&gt;现有关于世界模型的文献分散且缺乏系统性，许多论文仅提及'世界模型'概念但未深入探讨&lt;h4&gt;目的&lt;/h4&gt;提供一条清晰的世界模型发展路径，专注于核心组件而非列举所有相关研究&lt;h4&gt;方法&lt;/h4&gt;遵循从跨模态表示学习的掩码模型，到统一架构，再到交互式生成模型，最后到记忆增强系统的发展脉络&lt;h4&gt;主要发现&lt;/h4&gt;世界模型的核心在于三个关键组件：生成核心、交互循环和记忆系统&lt;h4&gt;结论&lt;/h4&gt;通过关注这三个核心组件构建的系统是实现真正世界模型的最有前途路径&lt;h4&gt;翻译&lt;/h4&gt;这不是一篇典型的世界模型综述；这是一份面向那些想要构建世界的人的指南。我们的目标不是罗列所有曾经提及'世界模型'的论文。相反，我们遵循一条清晰的道路：从早期跨模态统一表示学习的掩码模型，到共享单一范式的统一架构，再到闭合动作-感知循环的交互式生成模型，最后到随时间保持一致世界的记忆增强系统。我们绕过了松散相关的分支，专注于核心：生成核心、交互循环和记忆系统。我们表明这是实现真正世界模型的最有前途的路径。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何构建真正的世界模型的问题。尽管有数百篇相关论文，但对于如何实际构建一个真正的世界模型还没有明确共识。这个问题很重要，因为真正的世界模型可以模拟整个环境，用于强化学习、智能体规划、大型语言模型模拟社会等多个领域。它能从预测引擎转变为'活的世界'，具有持久性、代理性和涌现性，对理解复杂系统、进行科学实验和创造交互体验具有重要意义。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析世界模型的历史发展，提出了一条'狭窄的道路'，将世界模型发展分为五个阶段：基于掩码的模型、统一模型、交互式生成模型、记忆与一致性系统，以及真正的世界模型。论文大量借鉴了现有工作，每个阶段都列举了代表性模型和方法，如BERT、MAE等（第一阶段），EMU3、Chameleon等（第二阶段），Genie系列等（第三阶段），RETRO、MemGPT等（第四阶段）。作者通过分析这些工作的演进，提出了构建真正世界模型的路径。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：真正的世界模型不是单一实体，而是由三个核心子系统合成的系统：生成核心（产生世界状态）、交互循环（实时关闭行动-感知循环）和持久记忆系统（随时间维持一致的世界）。整体实现流程遵循五个阶段：首先建立基于掩码的模型，为跨模态表示学习提供通用范式；然后统一架构，使单一架构能处理和生成多种模态；接着引入交互循环，将静态生成器转变为实时模拟器；然后添加记忆系统，使模拟能随时间持续；最后将这些组件合成为自主整体，实现真正的世界模型。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）提出了清晰的世界模型五阶段发展路线图；2）定义了真正世界模型的三个核心子系统；3）指出了构建真正世界模型的三个基本挑战：一致性问题、压缩问题和对齐问题；4）提出了真正世界模型的三个定义属性：持久性、代理性和涌现性。与之前工作相比，不同之处在于：它不是简单罗列论文，而是提供清晰发展路径；不仅关注技术细节，还关注哲学意义和潜在影响；将世界模型视为综合系统而非孤立组件；提出了构建真正世界模型的具体挑战和未来方向。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提供了一个从基于掩码的模型到真正世界模型的五阶段发展路线图，定义了真正世界模型的三个核心子系统和三个关键属性，并指出了构建真正世界模型的三个基本挑战，为构建能够持久、交互和涌现的'活的世界'提供了清晰的指南。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is not a typical survey of world models; it is a guide for those whowant to build worlds. We do not aim to catalog every paper that has evermentioned a ``world model". Instead, we follow one clear road: from earlymasked models that unified representation learning across modalities, tounified architectures that share a single paradigm, then to interactivegenerative models that close the action-perception loop, and finally tomemory-augmented systems that sustain consistent worlds over time. We bypassloosely related branches to focus on the core: the generative heart, theinteractive loop, and the memory system. We show that this is the mostpromising path towards true world models.</description>
      <author>example@mail.com (Jinbin Bai, Yu Lei, Hecong Wu, Yuchen Zhu, Shufan Li, Yi Xin, Xiangtai Li, Molei Tao, Aditya Grover, Ming-Hsuan Yang)</author>
      <guid isPermaLink="false">2510.20668v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Connecting Jensen-Shannon and Kullback-Leibler Divergences: A New Bound for Representation Learning</title>
      <link>http://arxiv.org/abs/2510.20644v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at NeurIPS 2025. Code available at  https://github.com/ReubenDo/JSDlowerbound/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了互信息(MI)与Jensen-Shannon散度(JSD)之间的关系，通过推导一个新的紧密且可处理的Kullback-Leibler散度(KLD)下界作为JSD的函数，建立了两者之间的理论联系。研究证明最大化基于JSD的信息会增加对互信息的保证下界，并通过实验验证了该方法的有效性和实用性，为在基于MI的表示学习中使用判别学习提供了理论依据和实证支持。&lt;h4&gt;背景&lt;/h4&gt;互信息(MI)是表示学习中广泛使用的基本统计依赖性度量。然而，直接通过其定义为Kullback-Leibler散度(KLD)来优化MI通常是不可行的。因此，最近的方法转而最大化替代的依赖性度量，特别是Jensen-Shannon散度(JSD)作为判别损失。但这些替代目标与MI之间的联系尚未被充分理解。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在填补替代目标(特别是基于JSD的目标)与互信息之间理论理解的空白，通过建立它们之间的严格数学关系，为在表示学习中使用判别学习提供理论依据。&lt;h4&gt;方法&lt;/h4&gt;研究通过推导一个新的、紧密且可处理的KLD下界作为JSD的函数来建立MI与JSD之间的理论联系。通过将这一边界专门应用于联合分布和边缘分布，证明了最大化基于JSD的信息会增加对互信息的保证下界。此外，研究重新审视了基于JSD目标的实际实现，并观察到最小化二元分类器的交叉熵损失可以恢复JSD的已知变分下界。&lt;h4&gt;主要发现&lt;/h4&gt;1. 推导出了一个新的、紧密且可处理的KLD下界作为JSD的函数；2. 证明了最大化基于JSD的信息会增加对互信息的保证下界；3. 最小化区分联合分布与边缘分布对的二元分类器的交叉熵损失可以恢复JSD的已知变分下界；4. 实验表明该下界应用于MI估计时是紧密的；5. 与最先进的神经变分下界估计器相比，该下界估计器提供了稳定的低方差估计；6. 在信息瓶颈框架中展示了该方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究的结果为在基于互信息的表示学习中使用判别学习提供了新的理论依据和强有力的实证证据。所提出的下界估计器能够提供对互信息的稳定、低方差估计，并且在信息瓶颈框架中具有实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;互信息(MI)是表示学习中广泛使用的基本统计依赖性度量。虽然直接通过其定义为Kullback-Leibler散度(KLD)来优化MI通常是不可行的，但最近的方法转而最大化替代的依赖性度量，特别是通过判别损失来最大化联合分布与边缘分布乘积之间的Jensen-Shannon散度(JSD)。然而，这些替代目标与MI之间的联系尚未被充分理解。在本工作中，我们通过推导一个新的、紧密且可处理的KLD下界作为JSD的函数来填补这一空白。通过将这一边界专门应用于联合分布和边缘分布，我们证明了最大化基于JSD的信息会增加对互信息的保证下界。此外，我们重新审视了基于JSD目标的实际实现，并观察到最小化训练以区分联合分布与边缘分布对的二元分类器的交叉熵损失可以恢复JSD的已知变分下界。广泛的实验表明该下界应用于MI估计时是紧密的。我们将该下界与一系列既定参考场景中最先进的神经变分下界估计器进行了比较。我们的下界估计器一致地提供了对互信息紧密下界的稳定、低方差估计。我们还展示了其在信息瓶颈框架中的实际实用性。综上所述，我们的结果为在基于MI的表示学习中使用判别学习提供了新的理论依据和强有力的实证证据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mutual Information (MI) is a fundamental measure of statistical dependencewidely used in representation learning. While direct optimization of MI via itsdefinition as a Kullback-Leibler divergence (KLD) is often intractable, manyrecent methods have instead maximized alternative dependence measures, mostnotably, the Jensen-Shannon divergence (JSD) between joint and product ofmarginal distributions via discriminative losses. However, the connectionbetween these surrogate objectives and MI remains poorly understood. In thiswork, we bridge this gap by deriving a new, tight, and tractable lower bound onKLD as a function of JSD in the general case. By specializing this bound tojoint and marginal distributions, we demonstrate that maximizing the JSD-basedinformation increases a guaranteed lower bound on mutual information.Furthermore, we revisit the practical implementation of JSD-based objectivesand observe that minimizing the cross-entropy loss of a binary classifiertrained to distinguish joint from marginal pairs recovers a known variationallower bound on the JSD. Extensive experiments demonstrate that our lower boundis tight when applied to MI estimation. We compared our lower bound tostate-of-the-art neural estimators of variational lower bound across a range ofestablished reference scenarios. Our lower bound estimator consistentlyprovides a stable, low-variance estimate of a tight lower bound on MI. We alsodemonstrate its practical usefulness in the context of the InformationBottleneck framework. Taken together, our results provide new theoreticaljustifications and strong empirical evidence for using discriminative learningin MI-based representation learning.</description>
      <author>example@mail.com (Reuben Dorent, Polina Golland, William Wells III)</author>
      <guid isPermaLink="false">2510.20644v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion Autoencoders with Perceivers for Long, Irregular and Multimodal Astronomical Sequences</title>
      <link>http://arxiv.org/abs/2510.20595v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为Diffusion Autoencoder with Perceivers (daep)的新架构，用于处理科学领域中不规则、多模态序列数据。该架构通过将异构测量值标记化，使用Perceiver编码器压缩，并使用Perceiver-IO扩散解码器重建，实现了在不同数据设置中的可扩展学习。研究还建立了maep作为基线模型，实验表明daep在重建误差、判别性潜在空间保存和精细结构保留方面均优于VAE和maep基线模型。&lt;h4&gt;背景&lt;/h4&gt;自监督学习已成为表征学习的中心策略，但大多数用于编码数据的架构仅在规则采样的输入（如图像、音频和视频）上得到验证。然而，在许多科学领域中，数据是以长序列、不规则和多模态的形式出现的。&lt;h4&gt;目的&lt;/h4&gt;为了从这些不规则、多模态序列数据中提取语义信息，作者提出了daep架构，并建立了一个强基线模型maep，以评估daep的性能。&lt;h4&gt;方法&lt;/h4&gt;daep架构通过以下步骤工作：将异构测量值标记化，使用Perceiver编码器进行压缩，使用Perceiver-IO扩散解码器进行重建。为了评估daep，作者将掩码自编码器调整为Perceiver编码器/解码器设计，建立了maep基线模型。&lt;h4&gt;主要发现&lt;/h4&gt;在多样化的光谱和光度天文数据集上，daep实现了比VAE和maep基线模型更低的重建误差，产生了更具判别性的潜在空间，并更好地保留了精细结构。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明daep是处理科学领域中不规则、异构序列数据的有效框架。&lt;h4&gt;翻译&lt;/h4&gt;自监督学习已成为表征学习的中心策略，但大多数用于编码数据的架构仅在规则采样的输入（如图像、音频和视频）上得到验证。在许多科学领域中，数据实际上是以长序列、不规则和多模态的形式出现的。为了从这些数据中提取语义信息，我们引入了带有Perceiver的扩散自编码器（daep）。daep将异构测量值标记化，使用Perceiver编码器压缩它们，并使用Perceiver-IO扩散解码器重建它们，从而能够在各种数据设置中实现可扩展学习。为了对daep架构进行基准测试，我们将掩码自编码器调整为Perceiver编码器/解码器设计，并在与daep相同的架构家族中建立了强基线（maep）。在多样化的光谱和光度天文数据集上，daep实现了比VAE和maep基线模型更低的重建误差，产生更具判别性的潜在空间，并更好地保留了精细结构。这些结果确立了daep作为科学领域中数据以不规则、异构序列形式出现的有效框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has become a central strategy for representationlearning, but the majority of architectures used for encoding data have onlybeen validated on regularly-sampled inputs such as images, audios. and videos.In many scientific domains, data instead arrive as long, irregular, andmultimodal sequences. To extract semantic information from these data, weintroduce the Diffusion Autoencoder with Perceivers (daep). daep tokenizesheterogeneous measurements, compresses them with a Perceiver encoder, andreconstructs them with a Perceiver-IO diffusion decoder, enabling scalablelearning in diverse data settings. To benchmark the daep architecture, we adaptthe masked autoencoder to a Perceiver encoder/decoder design, and establish astrong baseline (maep) in the same architectural family as daep. Across diversespectroscopic and photometric astronomical datasets, daep achieves lowerreconstruction errors, produces more discriminative latent spaces, and betterpreserves fine-scale structure than both VAE and maep baselines. These resultsestablish daep as an effective framework for scientific domains where dataarrives as irregular, heterogeneous sequences.</description>
      <author>example@mail.com (Yunyi Shen, Alexander Gagliano)</author>
      <guid isPermaLink="false">2510.20595v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Cross-modal Representation Bias for Multicultural Image-to-Recipe Retrieval</title>
      <link>http://arxiv.org/abs/2510.20393v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACM Multimedia 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的因果方法，用于解决图像到食谱检索中的表示偏差问题，通过预测并注入图像中可能被忽视的烹饪元素，提高了检索性能。&lt;h4&gt;背景&lt;/h4&gt;现有图像到食谱检索方法假设食物图像能完全捕捉食谱细节，但实际上图像只反映烹饪结果而非过程。这导致跨模态表示学习忽略视觉上不明显但对检索关键的细节，使表示偏向主要视觉元素，难以区分相似食谱。当训练数据包含不同菜系时，这种偏差更严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种因果方法，预测图像中可能被忽视的烹饪元素，并将这些元素明确注入跨模态表示学习中，以减轻偏差。&lt;h4&gt;方法&lt;/h4&gt;采用因果方法预测图像中可能被忽视的烹饪元素，并将这些元素注入跨模态表示学习过程。在标准单语Recipe1M数据集和新策划的多语言多文化菜系数据集上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;提出的因果表示学习能够揭示细微的成分和烹饪动作，在单语和多语言多文化数据集上都取得了令人印象深刻的检索性能。&lt;h4&gt;结论&lt;/h4&gt;通过因果方法预测并注入图像中可能被忽视的烹饪元素，可以有效减轻跨模态表示学习中的偏差，特别是在处理不同菜系的图像和食谱时效果显著。&lt;h4&gt;翻译&lt;/h4&gt;现有的图像到食谱检索方法隐含假设食物图像可以完全捕捉其食谱中文本记录的细节。然而，食物图像只反映了烹饪菜肴的视觉结果，而不是底层的烹饪过程。因此，学习跨模态表示来弥合图像和食谱之间的模态差距时，往往会忽略那些在视觉上不明显但对食谱检索至关重要的细微、特定于食谱的细节。具体来说，这些表示偏向于捕捉主要的视觉元素，导致难以对在使用成分和烹饪方法上有细微差异的相似食谱进行排序。当训练数据混合来自不同菜系的图像和食谱时，表示学习中的偏差预计会更严重。本文提出了一种新的因果方法，预测图像中可能被忽视的烹饪元素，同时明确地将这些元素注入跨模态表示学习中以减轻偏差。实验在标准的单语Recipe1M数据集和一个新策划的多语言多文化菜系数据集上进行。结果表明，提出的因果表示学习能够揭示细微的成分和烹饪动作，并在单语和多语言多文化数据集上都取得了令人印象深刻的检索性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing approaches for image-to-recipe retrieval have the implicitassumption that a food image can fully capture the details textually documentedin its recipe. However, a food image only reflects the visual outcome of acooked dish and not the underlying cooking process. Consequently, learningcross-modal representations to bridge the modality gap between images andrecipes tends to ignore subtle, recipe-specific details that are not visuallyapparent but are crucial for recipe retrieval. Specifically, therepresentations are biased to capture the dominant visual elements, resultingin difficulty in ranking similar recipes with subtle differences in use ofingredients and cooking methods. The bias in representation learning isexpected to be more severe when the training data is mixed of images andrecipes sourced from different cuisines. This paper proposes a novel causalapproach that predicts the culinary elements potentially overlooked in images,while explicitly injecting these elements into cross-modal representationlearning to mitigate biases. Experiments are conducted on the standardmonolingual Recipe1M dataset and a newly curated multilingual multiculturalcuisine dataset. The results indicate that the proposed causal representationlearning is capable of uncovering subtle ingredients and cooking actions andachieves impressive retrieval performance on both monolingual and multilingualmulticultural datasets.</description>
      <author>example@mail.com (Qing Wang, Chong-Wah Ngo, Yu Cao, Ee-Peng Lim)</author>
      <guid isPermaLink="false">2510.20393v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GUSL-Dehaze: A Green U-Shaped Learning Approach to Image Dehazing</title>
      <link>http://arxiv.org/abs/2510.20266v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GUSL-Dehaze是一种绿色U型学习方法的图像去雾技术，结合了基于物理的模型与绿色学习框架，避免了深度学习的高计算成本和大参数量问题。&lt;h4&gt;背景&lt;/h4&gt;图像去雾是恢复清晰图像的任务，传统方法依赖统计先验和物理模型，而最先进的方法主要基于深度学习，但这些方法计算成本高且参数量大，不适合资源受限设备。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级、透明的图像去雾方法，避免深度学习的高计算成本，同时保持与最先进方法相当的性能。&lt;h4&gt;方法&lt;/h4&gt;GUSL-Dehaze采用改进的暗通道先验进行初始去雾，然后通过U型架构实现绿色学习流程，使用无监督表示学习进行特征提取，并结合相关特征测试和最小二乘归一化变换等特征工程技术，最后通过透明的监督学习策略获得去雾图像。&lt;h4&gt;主要发现&lt;/h4&gt;GUSL-Dehaze显著减少了参数数量，同时确保了数学可解释性，并取得了与最先进深度学习模型相当的性能。&lt;h4&gt;结论&lt;/h4&gt;GUSL-Dehaze为图像去雾提供了一种轻量级、透明的替代方案，避免了深度学习的计算负担，同时保持了高性能和可解释性。&lt;h4&gt;翻译&lt;/h4&gt;图像去雾是一项恢复任务，旨在从单幅有雾输入中恢复清晰图像。传统方法依赖于统计先验和基于物理的大气散射模型来重建无雾图像。虽然最近最先进的方法主要基于深度学习架构，但这些模型通常涉及高计算成本和大参数量，使其不适合资源受限设备。在本工作中，我们提出了GUSL-Dehaze，一种绿色U型学习方法的图像去雾技术。我们的方法将基于物理的模型与绿色学习框架相结合，提供了比传统深度学习技术更轻量、更透明的替代方案。与基于神经网络的解决方案不同，GUSL-Dehaze完全避免了深度学习。相反，我们首先使用改进的暗通道先验进行初始去雾步骤，然后通过U型架构实现绿色学习流程。该架构采用无监督表示学习进行有效特征提取，并结合相关特征测试和最小二乘归一化变换等特征工程技术来保持紧凑的模型大小。最后，通过透明的监督学习策略获得去雾图像。GUSL-Dehaze显著减少了参数数量，同时确保了数学可解释性，并取得了与最先进深度学习模型相当的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image dehazing is a restoration task that aims to recover a clear image froma single hazy input. Traditional approaches rely on statistical priors and thephysics-based atmospheric scattering model to reconstruct the haze-free image.While recent state-of-the-art methods are predominantly based on deep learningarchitectures, these models often involve high computational costs and largeparameter sizes, making them unsuitable for resource-constrained devices. Inthis work, we propose GUSL-Dehaze, a Green U-Shaped Learning approach to imagedehazing. Our method integrates a physics-based model with a green learning(GL) framework, offering a lightweight, transparent alternative to conventionaldeep learning techniques. Unlike neural network-based solutions, GUSL-Dehazecompletely avoids deep learning. Instead, we begin with an initial dehazingstep using a modified Dark Channel Prior (DCP), which is followed by a greenlearning pipeline implemented through a U-shaped architecture. Thisarchitecture employs unsupervised representation learning for effective featureextraction, together with feature-engineering techniques such as the RelevantFeature Test (RFT) and the Least-Squares Normal Transform (LNT) to maintain acompact model size. Finally, the dehazed image is obtained via a transparentsupervised learning strategy. GUSL-Dehaze significantly reduces parameter countwhile ensuring mathematical interpretability and achieving performance on parwith state-of-the-art deep learning models.</description>
      <author>example@mail.com (Mahtab Movaheddrad, Laurence Palmer, C. -C. Jay Kuo)</author>
      <guid isPermaLink="false">2510.20266v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Towards Objective Obstetric Ultrasound Assessment: Contrastive Representation Learning for Fetal Movement Detection</title>
      <link>http://arxiv.org/abs/2510.20214v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is the preprint version of the manuscript submitted to IEEE  Journal of Biomedical and Health Informatics (JBHI) for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CURL的新型自监督学习框架，用于从胎儿超声视频中准确检测胎儿运动，解决了传统方法的主观性和准确性有限的问题。&lt;h4&gt;背景&lt;/h4&gt;准确的胎儿运动检测对于评估产前健康至关重要，异常的运动模式可能表明存在潜在的并发症，如胎盘功能障碍或胎儿窘迫。传统方法包括母亲感知和胎心宫缩监护图，但这些方法存在主观性和准确性有限的问题。&lt;h4&gt;目的&lt;/h4&gt;为了解决传统胎儿运动检测方法的挑战，研究人员提出了一种新型自监督学习框架CURL，用于从延长的胎儿超声视频记录中检测胎儿运动。&lt;h4&gt;方法&lt;/h4&gt;CURL方法利用双重对比损失，结合空间和时间对比学习，来学习鲁棒的运动表示。此外，研究还引入了一种特定任务的采样策略，确保在自监督训练过程中有效分离运动和非运动段，同时通过概率微调方法实现对任意长度的超声记录的灵活推断。&lt;h4&gt;主要发现&lt;/h4&gt;在包含92名受试者（每人进行30分钟超声检查）的内部数据集上评估，CURL达到了78.01%的敏感性和81.60%的AUROC，证明了其在胎儿运动分析方面的可靠性和客观性。&lt;h4&gt;结论&lt;/h4&gt;这些结果突显了自监督对比学习在胎儿运动分析中的潜力，为改进产前监测和临床决策铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;准确的胎儿运动检测对于评估产前健康至关重要，因为异常的运动模式可能表明存在潜在的并发症，如胎盘功能障碍或胎儿窘迫。传统方法，包括母亲感知和胎心宫缩监护图，存在主观性和准确性有限的问题。为了解决这些挑战，我们提出了对比超声视频表示学习，这是一种新颖的自监督学习框架，用于从延长的胎儿超声视频记录中检测胎儿运动。我们的方法利用双重对比损失，结合空间和时间对比学习，来学习鲁棒的运动表示。此外，我们引入了一种特定任务的采样策略，确保在自监督训练过程中有效分离运动和非运动段，同时通过概率微调方法实现对任意长度的超声记录的灵活推断。在包含92名受试者（每人进行30分钟超声检查）的内部数据集上评估，CURL达到了78.01%的敏感性和81.60%的AUROC，证明了其在胎儿运动分析方面的可靠性和客观性。这些结果突显了自监督对比学习在胎儿运动分析中的潜力，为改进产前监测和临床决策铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate fetal movement (FM) detection is essential for assessing prenatalhealth, as abnormal movement patterns can indicate underlying complicationssuch as placental dysfunction or fetal distress. Traditional methods, includingmaternal perception and cardiotocography (CTG), suffer from subjectivity andlimited accuracy. To address these challenges, we propose ContrastiveUltrasound Video Representation Learning (CURL), a novel self-supervisedlearning framework for FM detection from extended fetal ultrasound videorecordings. Our approach leverages a dual-contrastive loss, incorporating bothspatial and temporal contrastive learning, to learn robust motionrepresentations. Additionally, we introduce a task-specific sampling strategy,ensuring the effective separation of movement and non-movement segments duringself-supervised training, while enabling flexible inference on arbitrarily longultrasound recordings through a probabilistic fine-tuning approach. Evaluatedon an in-house dataset of 92 subjects, each with 30-minute ultrasound sessions,CURL achieves a sensitivity of 78.01% and an AUROC of 81.60%, demonstrating itspotential for reliable and objective FM analysis. These results highlight thepotential of self-supervised contrastive learning for fetal movement analysis,paving the way for improved prenatal monitoring and clinical decision-making.</description>
      <author>example@mail.com (Talha Ilyas, Duong Nhu, Allison Thomas, Arie Levin, Lim Wei Yap, Shu Gong, David Vera Anaya, Yiwen Jiang, Deval Mehta, Ritesh Warty, Vinayak Smith, Maya Reddy, Euan Wallace, Wenlong Cheng, Zongyuan Ge, Faezeh Marzbanrad)</author>
      <guid isPermaLink="false">2510.20214v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>A Structured Review and Quantitative Profiling of Public Brain MRI Datasets for Foundation Model Development</title>
      <link>http://arxiv.org/abs/2510.20196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了54个公开可用的脑部MRI数据集，评估了数据规模、多样性和一致性对基础模型开发的影响，发现数据集间存在显著不平衡和异质性，预处理无法完全消除数据集间的偏差，强调了设计脑部MRI基础模型时需要考虑预处理感知和领域自适应策略的必要性。&lt;h4&gt;背景&lt;/h4&gt;脑部MRI基础模型的发展依赖于可用数据的规模、多样性和一致性，然而对这些因素的系统评估仍然很少见。&lt;h4&gt;目的&lt;/h4&gt;分析54个公开可用的脑部MRI数据集（包含超过538,031个样本），为脑部MRI基础模型开发提供结构化、多层次的概述。&lt;h4&gt;方法&lt;/h4&gt;在数据集层面分析模态组成、疾病覆盖范围和数据集规模；在图像层面量化15个代表性数据集中的体素间距、方向和强度分布；评估预处理步骤（强度归一化、偏置场校正、颅骨剥离、空间配准和插值）对体素统计和几何形状的影响；使用3D DenseNet121进行特征空间案例研究，评估标准化预处理后的协变量偏移。&lt;h4&gt;主要发现&lt;/h4&gt;数据集层面存在大型健康队列与较小临床人群之间的严重不平衡；图像层面存在显著的异质性，可能影响表示学习；预处理步骤虽提高了数据集内部一致性，但数据集间的残余差异仍然存在；标准化预处理后仍可测量到残余协变量偏移，确认仅靠调和无法消除数据集间的偏差。&lt;h4&gt;结论&lt;/h4&gt;这些分析提供了公共脑部MRI资源中变异性的统一表征，强调在设计可推广的脑部MRI基础模型时需要考虑预处理感知和领域自适应策略。&lt;h4&gt;翻译&lt;/h4&gt;脑部MRI基础模型的发展在很大程度上取决于可用数据的规模、多样性和一致性，然而对这些因素的系统评估仍然很少。在本研究中，我们分析了54个公开可用的脑部MRI数据集，包含超过538,031个样本，为基础模型开发提供了结构化、多层次的概述。在数据集层面，我们表征了模态组成、疾病覆盖范围和数据集规模，揭示了大型健康队列与较小临床人群之间的严重不平衡。在图像层面，我们量化了15个代表性数据集中的体素间距、方向和强度分布，证明了可能影响表示学习的显著异质性。然后，我们对预处理变异性进行了定量评估，检查了强度归一化、偏置场校正、颅骨剥离、空间配准和插值如何改变体素统计和几何形状。虽然这些步骤提高了数据集内部的一致性，但数据集之间的残余差异仍然存在。最后，使用3D DenseNet121进行的特征空间案例研究显示，在标准化预处理后仍可测量到残余协变量偏移，确认仅靠调和无法消除数据集间的偏差。总之，这些分析提供了公共脑部MRI资源中变异性的统一表征，并强调了在设计可推广的脑部MRI基础模型时需要考虑预处理感知和领域自适应策略的必要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of foundation models for brain MRI depends critically on thescale, diversity, and consistency of available data, yet systematic assessmentsof these factors remain scarce. In this study, we analyze 54 publiclyaccessible brain MRI datasets encompassing over 538,031 to provide astructured, multi-level overview tailored to foundation model development. Atthe dataset level, we characterize modality composition, disease coverage, anddataset scale, revealing strong imbalances between large healthy cohorts andsmaller clinical populations. At the image level, we quantify voxel spacing,orientation, and intensity distributions across 15 representative datasets,demonstrating substantial heterogeneity that can influence representationlearning. We then perform a quantitative evaluation of preprocessingvariability, examining how intensity normalization, bias field correction,skull stripping, spatial registration, and interpolation alter voxel statisticsand geometry. While these steps improve within-dataset consistency, residualdifferences persist between datasets. Finally, feature-space case study using a3D DenseNet121 shows measurable residual covariate shift after standardizedpreprocessing, confirming that harmonization alone cannot eliminateinter-dataset bias. Together, these analyses provide a unified characterizationof variability in public brain MRI resources and emphasize the need forpreprocessing-aware and domain-adaptive strategies in the design ofgeneralizable brain MRI foundation models.</description>
      <author>example@mail.com (Minh Sao Khue Luu, Margaret V. Benedichuk, Ekaterina I. Roppert, Roman M. Kenzhin, Bair N. Tuchinov)</author>
      <guid isPermaLink="false">2510.20196v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>IB-GAN: Disentangled Representation Learning with Information Bottleneck Generative Adversarial Networks</title>
      <link>http://arxiv.org/abs/2510.20165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Proceedings of the Thirty Fifth AAAI Conference on  Artificial Intelligence (AAAI 2021), paper number 7926&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于GAN的无监督解纠缠表征学习模型IB-GAN，利用信息瓶颈框架优化GAN，通过生成器的中间层约束输入与生成输出之间的互信息，实现了对潜在空间的解纠缠和可解释性利用。&lt;h4&gt;背景&lt;/h4&gt;解纠缠表征学习是机器学习领域的重要研究方向，现有的方法如InfoGAN和β-VAEs存在一定局限性，需要改进模型架构以获得更好的解纠缠能力和样本质量。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于GAN的无监督解纠缠表征学习模型IB-GAN，利用信息瓶颈框架优化GAN，实现更好的解纠缠能力和样本质量。&lt;h4&gt;方法&lt;/h4&gt;IB-GAN架构与InfoGAN部分相似但有关键差异：利用生成器的中间层约束输入与生成输出之间的互信息；中间随机层可作为可学习的潜在分布，与生成器端到端联合训练；使生成器能够以解纠缠和可解释的方式利用潜在空间。&lt;h4&gt;主要发现&lt;/h4&gt;在dSprites和Color-dSprites数据集上的实验表明，IB-GAN实现了与最先进的β-VAEs相当的解纠缠分数，并优于InfoGAN；在CelebA和3D Chairs数据集上，IB-GAN在FID分数方面通常比β-VAEs和Info-GAN生成的样本具有更好的视觉质量和多样性。&lt;h4&gt;结论&lt;/h4&gt;IB-GAN通过利用信息瓶颈框架优化GAN，有效提升了模型的解纠缠能力和样本生成质量，是一种有效的无监督解纠缠表征学习方法。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的基于GAN的无监督解纠缠表征学习模型。这一新模型是在尝试将信息瓶颈框架应用于GAN优化的过程中发现的，因此命名为IB-GAN。IB-GAN的架构与InfoGAN部分相似，但有一个关键区别：利用生成器的中间层来约束输入与生成输出之间的互信息。中间随机层可以作为可学习的潜在分布，与生成器以端到端的方式联合训练。因此，IB-GAN的生成器能够以解纠缠和可解释的方式利用潜在空间。在dSprites和Color-dSprites数据集上的实验表明，IB-GAN实现了与最先进的β-VAEs相当的解纠缠分数，并优于InfoGAN。此外，在CelebA和3D Chairs数据集上，IB-GAN生成的样本在FID分数方面通常比β-VAEs和Info-GAN具有更好的视觉质量和多样性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1609/aaai.v35i9.16967&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new GAN-based unsupervised model for disentangled representationlearning. The new model is discovered in an attempt to utilize the InformationBottleneck (IB) framework to the optimization of GAN, thereby named IB-GAN. Thearchitecture of IB-GAN is partially similar to that of InfoGAN but has acritical difference; an intermediate layer of the generator is leveraged toconstrain the mutual information between the input and the generated output.The intermediate stochastic layer can serve as a learnable latent distributionthat is trained with the generator jointly in an end-to-end fashion. As aresult, the generator of IB-GAN can harness the latent space in a disentangledand interpretable manner. With the experiments on dSprites and Color-dSpritesdataset, we demonstrate that IB-GAN achieves competitive disentanglement scoresto those of state-of-the-art \b{eta}-VAEs and outperforms InfoGAN. Moreover,the visual quality and the diversity of samples generated by IB-GAN are oftenbetter than those by \b{eta}-VAEs and Info-GAN in terms of FID score on CelebAand 3D Chairs dataset.</description>
      <author>example@mail.com (Insu Jeon, Wonkwang Lee, Myeongjang Pyeon, Gunhee Kim)</author>
      <guid isPermaLink="false">2510.20165v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>TOMCAT: Test-time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning</title>
      <link>http://arxiv.org/abs/2510.20162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的组合零样本学习方法，通过无监督数据积累多模态知识并更新原型，解决了测试时分布偏移的问题，在多个基准数据集上取得了最先进的性能。&lt;h4&gt;背景&lt;/h4&gt;组合零样本学习旨在基于已学习知识识别新的属性-对象组合。现有方法在测试时由于标签空间分布偏移导致性能下降，这种偏移源于包含了从未见过的属性和对象重新组合的样本。&lt;h4&gt;目的&lt;/h4&gt;克服测试时标签空间分布偏移带来的挑战，提出一种方法来更新多模态原型，使模型能够灵活适应测试时的分布偏移。&lt;h4&gt;方法&lt;/h4&gt;提出一种新方法，通过无监督数据积累文本和视觉模态的综合知识；设计自适应更新权重控制原型调整程度；引入动态优先队列存储高置信度图像，从历史图像获取视觉知识；通过多模态协同表示学习对齐文本和视觉原型。&lt;h4&gt;主要发现&lt;/h4&gt;在四个基准数据集上，无论是在封闭世界还是开放世界设置下，该方法都达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该方法通过更新多模态原型和自适应权重，有效解决了组合零样本学习中的分布偏移问题，代码将在https://github.com/xud-yan/TOMCAT上提供。&lt;h4&gt;翻译&lt;/h4&gt;组合零样本学习旨在基于已学习知识识别新的属性-对象组合。现有方法在测试时由于标签空间分布偏移导致性能下降，这源于包含了从未见过的属性和对象重新组合的样本。为克服这一挑战，我们提出了一种新方法，通过无监督数据在文本和视觉模态中积累综合知识，以在测试时更新多模态原型。基于此，我们进一步设计了自适应更新权重来控制原型调整程度，使模型能够在测试过程中灵活适应分布偏移。此外，我们引入了动态优先队列，存储高置信度图像，以便从历史图像获取视觉知识进行推理。考虑到多模态知识的语义一致性，我们通过多模态协同表示学习对齐文本和视觉原型。大量实验表明，我们的方法在四个基准数据集上，无论是在封闭世界还是开放世界设置下，都达到了最先进的性能。代码将在https://github.com/xud-yan/TOMCAT上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Compositional Zero-Shot Learning (CZSL) aims to recognize novelattribute-object compositions based on the knowledge learned from seen ones.Existing methods suffer from performance degradation caused by the distributionshift of label space at test time, which stems from the inclusion of unseencompositions recombined from attributes and objects. To overcome the challenge,we propose a novel approach that accumulates comprehensive knowledge in bothtextual and visual modalities from unsupervised data to update multimodalprototypes at test time. Building on this, we further design an adaptive updateweight to control the degree of prototype adjustment, enabling the model toflexibly adapt to distribution shift during testing. Moreover, a dynamicpriority queue is introduced that stores high-confidence images to acquirevisual knowledge from historical images for inference. Considering the semanticconsistency of multimodal knowledge, we align textual and visual prototypes bymultimodal collaborative representation learning. Extensive experimentsindicate that our approach achieves state-of-the-art performance on fourbenchmark datasets under both closed-world and open-world settings. Code willbe available at https://github.com/xud-yan/TOMCAT .</description>
      <author>example@mail.com (Xudong Yan, Songhe Feng)</author>
      <guid isPermaLink="false">2510.20162v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Improving Predictive Confidence in Medical Imaging via Online Label Smoothing</title>
      <link>http://arxiv.org/abs/2510.20011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted and presented in International Conference on Advancing  Science and Technologies in Health Science&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探索了在线标签平滑(OLS)在医学图像分类中的应用，结果显示OLS能提高分类准确率并改善模型校准性。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型，特别是卷积神经网络，在医学图像分类中取得了显著成果，但这些模型经常产生过度自信的预测，影响在关键医疗环境中的可靠性。&lt;h4&gt;目的&lt;/h4&gt;研究使用在线标签平滑(OLS)，一种基于模型自身预测模式动态调整软标签的方法，以提高医学图像分类模型的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;在大型RadImageNet数据集上使用三种架构评估OLS：ResNet-50、MobileNetV2和VGG-19，并与标准训练方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;OLS相比标准训练方法持续提高了Top-1和Top-5分类准确率，并产生更紧凑和良好分离的特征嵌入，表明表示学习得到改善。&lt;h4&gt;结论&lt;/h4&gt;OLS不仅增强了预测性能，还提高了校准性，使其成为医学成像领域开发可信AI系统的实用有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型，特别是卷积神经网络，在医学图像分类中已取得了令人印象深刻的结果。然而，这些模型通常会产生过度自信的预测，这可能削弱它们在关键医疗环境中的可靠性。虽然传统的标签平滑提供了一种减少这种过度自信的简单方法，但它未能考虑类别之间的关系，将所有非目标类别同等对待。在本研究中，我们探索了在线标签平滑(OLS)的使用，这是一种动态方法，基于模型自身的预测模式在训练过程中调整软标签。我们在大型RadImageNet数据集上使用三种广泛使用的架构评估了OLS：ResNet-50、MobileNetV2和VGG-19。我们的结果表明，与标准训练方法（包括硬标签、传统标签平滑和无教师知识蒸馏）相比，OLS持续提高了Top-1和Top-5分类准确率。除了准确率的提升外，OLS还导致更紧凑和良好分离的特征嵌入，表明表示学习得到改善。这些发现表明，OLS不仅增强了预测性能，还提高了校准性，使其成为医学成像领域开发可信AI系统的实用有效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models, especially convolutional neural networks, have achievedimpressive results in medical image classification. However, these models oftenproduce overconfident predictions, which can undermine their reliability incritical healthcare settings. While traditional label smoothing offers a simpleway to reduce such overconfidence, it fails to consider relationships betweenclasses by treating all non-target classes equally. In this study, we explorethe use of Online Label Smoothing (OLS), a dynamic approach that adjusts softlabels throughout training based on the model's own prediction patterns. Weevaluate OLS on the large-scale RadImageNet dataset using three widely usedarchitectures: ResNet-50, MobileNetV2, and VGG-19. Our results show that OLSconsistently improves both Top-1 and Top-5 classification accuracy compared tostandard training methods, including hard labels, conventional label smoothing,and teacher-free knowledge distillation. In addition to accuracy gains, OLSleads to more compact and well-separated feature embeddings, indicatingimproved representation learning. These findings suggest that OLS not onlystrengthens predictive performance but also enhances calibration, making it apractical and effective solution for developing trustworthy AI systems in themedical imaging domain.</description>
      <author>example@mail.com (Kushan Choudhury, Shubhrodeep Roy, Ankur Chanda, Shubhajit Biswas, Somenath Kuiry)</author>
      <guid isPermaLink="false">2510.20011v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Transformed Multi-view 3D Shape Features with Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.19955v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了3D形状特征表示学习的挑战，通过将Vision Transformers架构与对比学习目标相结合，在3D形状理解方面取得了良好效果&lt;h4&gt;背景&lt;/h4&gt;计算机视觉方法在从2D图像识别3D物体方面存在困难，通常需要大量标记数据，且依赖的卷积神经网络可能会忽略关键的形状关系&lt;h4&gt;目的&lt;/h4&gt;解决3D形状特征表示学习中的挑战，探索减少对大量标记数据依赖的方法，克服CNNs在捕获关键形状关系方面的局限性&lt;h4&gt;方法&lt;/h4&gt;使用Vision Transformers (ViTs)架构与现代对比目标相结合，进行多视图3D分析，结合ViTs理解整体形状的能力和对比学习的有效性&lt;h4&gt;主要发现&lt;/h4&gt;监督对比损失在ModelNet10上达到了约90.6%的准确率；ViTs能够捕获全局形状语义，而对比优化能够完善局部判别特征&lt;h4&gt;结论&lt;/h4&gt;通过结合ViTs与对比目标，成功实现了3D表示学习，这种方法基于大量实验评估，证明了其有效性&lt;h4&gt;翻译&lt;/h4&gt;这篇论文通过研究最先进的骨干网络与对比监督和自监督学习目标的组合，解决了3D形状特征表示学习中的挑战。计算机视觉方法在从2D图像识别3D物体方面存在困难，通常需要大量标记数据，并依赖于卷积神经网络(CNNs)，而这些网络可能会忽略关键的形状关系。我们的研究表明，当Vision Transformers (ViTs)架构与现代对比目标配对时，在我们的下游任务中多视图3D分析取得了有希望的结果，统一了对比学习和3D形状理解的流程。例如，监督对比损失在ModelNet10上达到了约90.6%的准确率。ViTs和对比学习的应用，利用了ViTs理解整体形状的能力和对比学习的有效性，克服了对大量标记数据的需求以及CNNs在捕获关键形状关系方面的局限性。成功的原因在于通过ViTs捕获全局形状语义，并通过对比优化完善局部判别特征。重要的是，我们的方法是经验性的，因为它基于大量的实验评估来验证将ViTs与对比目标相结合用于3D表示学习的有效性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决3D形状特征表示学习中的挑战，特别是计算机视觉方法从2D图像识别3D物体的困难。这个问题很重要，因为3D形状理解对机器人、虚拟现实等应用至关重要，而当前方法需要大量标记数据且依赖CNN，这些网络可能忽略关键的形状关系，限制了模型在真实世界应用中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到CNN在3D形状理解中的局限性，注意到ViT架构在视觉识别任务中表现优异，并观察到对比学习在利用未标记数据方面的潜力。他们借鉴了MVCNN(首次使用CNN进行3D形状理解)、ViT架构以及多种对比学习方法(如InfoNCE、SimCLR、SupCon)，创新性地将这些技术结合应用于3D多视图形状理解，这是之前研究较少探索的组合。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用Vision Transformer的注意力机制捕获全局形状语义，通过对比学习优化细化局部判别特征，统一对比学习和3D形状理解流程。整体流程分为两个阶段：第一阶段是多视图渲染和对比学习，从3D网格生成12个视图图像，使用多种对比损失函数训练ViT和CNN骨干；第二阶段是下游任务评估，包括分类(线性评估、k-NN分类、t-SNE可视化)和检索任务(基于余弦相似度排序，使用mAP评估)。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：首次将ViT与对比学习结合用于3D多视图形状理解；系统评估四种ViT骨干和五种对比损失函数；统一对比学习和3D形状理解流程；在多个下游任务进行全面评估。相比之前工作，本文主要使用ViT而非CNN骨干；将对比学习从2D扩展到3D多视图数据；提供更全面的评估；在ModelNet10上达到90.6%分类准确率和95.5%的mAP，超越之前方法。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过将Vision Transformer与先进的对比学习目标相结合，显著提升了3D多视图形状理解的性能，同时减少了对大量标记数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenges in representation learning of 3D shapefeatures by investigating state-of-the-art backbones paired with bothcontrastive supervised and self-supervised learning objectives. Computer visionmethods struggle with recognizing 3D objects from 2D images, often requiringextensive labeled data and relying on Convolutional Neural Networks (CNNs) thatmay overlook crucial shape relationships. Our work demonstrates that VisionTransformers (ViTs) based architectures, when paired with modern contrastiveobjectives, achieve promising results in multi-view 3D analysis on ourdownstream tasks, unifying contrastive and 3D shape understanding pipelines.For example, supervised contrastive losses reached about 90.6% accuracy onModelNet10. The use of ViTs and contrastive learning, leveraging ViTs' abilityto understand overall shapes and contrastive learning's effectiveness,overcomes the need for extensive labeled data and the limitations of CNNs incapturing crucial shape relationships. The success stems from capturing globalshape semantics via ViTs and refining local discriminative features throughcontrastive optimization. Importantly, our approach is empirical, as it isgrounded on extensive experimental evaluation to validate the effectiveness ofcombining ViTs with contrastive objectives for 3D representation learning.</description>
      <author>example@mail.com (Márcus Vinícius Lobo Costa, Sherlon Almeida da Silva, Bárbara Caroline Benato, Leo Sampaio Ferraz Ribeiro, Moacir Antonelli Ponti)</author>
      <guid isPermaLink="false">2510.19955v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Domain Adaptation via Similarity-based Prototypes for Cross-Modality Segmentation</title>
      <link>http://arxiv.org/abs/2510.20596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2021&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于相似性原型的跨模态分割新框架，通过在嵌入空间中学习类别原型并引入相似性约束，以及使用字典存储不同图像中提取的原型，解决了深度学习模型在未见数据上性能下降的问题，实验证明该方法优于其他最先进方法。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型在各种视觉挑战中取得了巨大成功，但训练好的模型在应用于未见过的数据时性能会急剧下降，模型对域偏移敏感。&lt;h4&gt;目的&lt;/h4&gt;减少域差距，避免对未见域的昂贵标注，提高模型在跨模态分割任务中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出一种基于相似性原型的跨模态分割框架，在嵌入空间中学习类别的代表性原型，引入相似性约束使原型对每个语义类别具有代表性且不同类别间可分离，使用字典存储从不同图像中提取的原型防止类别缺失问题，实现原型的对比学习以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过原型学习和对比学习的方法可以有效解决域偏移问题，提高跨模态分割性能。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于相似性原型的跨模态分割框架比其他最先进方法取得了更好的结果。&lt;h4&gt;翻译&lt;/h4&gt;深度学习模型在各种视觉挑战中取得了巨大成功，但训练好的模型在应用于未见过的数据时性能会急剧下降。由于模型对域偏移敏感，无监督域适应尝试减少域差距并避免对未见域的昂贵标注。本文提出了一种基于相似性原型的跨模态分割新框架。具体来说，我们在嵌入空间中学习类别的原型，然后引入相似性约束，使这些原型对每个语义类别具有代表性，同时不同类别之间可分离。此外，我们使用字典存储从不同图像中提取的原型，这可以防止类别缺失问题，并实现原型的对比学习，进一步提高性能。大量实验表明，我们的方法比其他最先进方法取得了更好的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning models have achieved great success on various visionchallenges, but a well-trained model would face drastic performance degradationwhen applied to unseen data. Since the model is sensitive to domain shift,unsupervised domain adaptation attempts to reduce the domain gap and avoidcostly annotation of unseen domains. This paper proposes a novel framework forcross-modality segmentation via similarity-based prototypes. In specific, welearn class-wise prototypes within an embedding space, then introduce asimilarity constraint to make these prototypes representative for each semanticclass while separable from different classes. Moreover, we use dictionaries tostore prototypes extracted from different images, which prevents theclass-missing problem and enables the contrastive learning of prototypes, andfurther improves performance. Extensive experiments show that our methodachieves better results than other state-of-the-art methods.</description>
      <author>example@mail.com (Ziyu Ye, Chen Ju, Chaofan Ma, Xiaoyun Zhang)</author>
      <guid isPermaLink="false">2510.20596v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>SheafAlign: A Sheaf-theoretic Framework for Decentralized Multimodal Alignment</title>
      <link>http://arxiv.org/abs/2510.20540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SheafAlign是一种基于层理论的多模态对齐框架，适用于分布式场景，不要求所有模态相互冗余，能有效保留共享和独特信息，并在多个方面表现优于现有方法。&lt;h4&gt;背景&lt;/h4&gt;传统多模态对齐方法假设所有模态之间存在相互冗余，这一假设在现实世界的分布式场景中并不成立。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为SheafAlign的框架，用于去中心化的多模态对齐，用多个比较空间替代单一空间对齐。&lt;h4&gt;方法&lt;/h4&gt;使用层理论框架，通过层结构建模成对模态关系，并利用基于去中心化对比学习的目标进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;Sheaf克服了先前方法的局限性，不需要所有模态之间存在相互冗余；在多模态传感数据集上表现出优越的零样本泛化能力、优秀的跨模态对齐能力、对缺失模态具有鲁棒性，与最先进的基线相比，通信成本降低50%。&lt;h4&gt;结论&lt;/h4&gt;SheafAlign是一种有效的去中心化多模态对齐方法，能够在分布式场景中更好地处理模态间关系。&lt;h4&gt;翻译&lt;/h4&gt;传统多模态对齐方法假设所有模态之间存在相互冗余，这一假设在现实世界的分布式场景中并不成立。我们提出了SheafAlign，一种基于层理论的去中心化多模态对齐框架，它用多个比较空间替代了单一空间对齐。这种方法通过层结构建模成对模态关系，并利用基于去中心化对比学习的目标进行训练。SheafAlign通过不要求所有模态之间存在相互冗余，克服了先前方法的局限性，同时保留了共享信息和独特信息。在多模态传感数据集上的实验显示，它在零样本泛化、跨模态对齐和对缺失模态的鲁棒性方面具有优越性，并且与最先进的基线相比，通信成本降低了50%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional multimodal alignment methods assume mutual redundancy across allmodalities, an assumption that fails in real-world distributed scenarios. Wepropose SheafAlign, a sheaf-theoretic framework for decentralized multimodalalignment that replaces single-space alignment with multiple comparison spaces.This approach models pairwise modality relations through sheaf structures andleverages decentralized contrastive learning-based objectives for training.SheafAlign overcomes the limitations of prior methods by not requiring mutualredundancy among all modalities, preserving both shared and unique information.Experiments on multimodal sensing datasets show superior zero-shotgeneralization, cross-modal alignment, and robustness to missing modalities,with 50\% lower communication cost than state-of-the-art baselines.</description>
      <author>example@mail.com (Abdulmomen Ghalkha, Zhuojun Tian, Chaouki Ben Issaid, Mehdi Bennis)</author>
      <guid isPermaLink="false">2510.20540v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>ViTacGen: Robotic Pushing with Vision-to-Touch Generation</title>
      <link>http://arxiv.org/abs/2510.14117v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ViTacGen是一种新颖的机器人操作框架，通过视觉到触觉生成在强化学习中消除对高分辨率真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署，在模拟和真实世界实验中展现出高达86%的成功率。&lt;h4&gt;背景&lt;/h4&gt;机器人推动是一种需要触觉反馈来捕捉末端执行器与物体间细微接触力和动力学的基本操作任务。真实触觉传感器面临高成本、脆弱性、校准和传感器差异等挑战，而仅依赖视觉的策略性能有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从视觉推断触觉状态的机器人操作框架，减少对昂贵且脆弱的真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署。&lt;h4&gt;方法&lt;/h4&gt;ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化触觉表示），以及一个使用对比学习融合视觉-触觉数据的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界实验中验证了ViTacGen的有效性，其性能优于传统方法，成功率达到86%。&lt;h4&gt;结论&lt;/h4&gt;ViTacGen成功实现了从视觉到触觉的生成，使仅视觉的机器人系统能够在没有真实触觉传感器的情况下执行有效的机器人推动任务。&lt;h4&gt;翻译&lt;/h4&gt;机器人推动是一种基本操作任务，需要触觉反馈来捕捉末端执行器与物体之间的细微接触力和动力学特性。然而，真实触觉传感器通常面临高成本和脆弱性等硬件限制，以及涉及校准和不同传感器间差异的部署挑战，而仅视觉的策略难以获得令人满意的性能。受人类从视觉推断触觉状态能力的启发，我们提出了ViTacGen，一种专为视觉机器人推动设计的新颖机器人操作框架，在强化学习中使用视觉到触觉生成，消除对高分辨率真实触觉传感器的依赖，实现仅视觉机器人系统上的有效零样本部署。具体来说，ViTacGen包含一个编码器-解码器视觉到触觉生成网络，直接从视觉图像序列生成接触深度图像（标准化的触觉表示），随后是一个基于视觉和生成触觉观察使用对比学习融合视觉-触觉数据的强化学习策略。我们在模拟和真实世界实验中都验证了我们方法的有效性，展示了其优越的性能，成功率高达86%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic pushing is a fundamental manipulation task that requires tactilefeedback to capture subtle contact forces and dynamics between the end-effectorand the object. However, real tactile sensors often face hardware limitationssuch as high costs and fragility, and deployment challenges involvingcalibration and variations between different sensors, while vision-onlypolicies struggle with satisfactory performance. Inspired by humans' ability toinfer tactile states from vision, we propose ViTacGen, a novel robotmanipulation framework designed for visual robotic pushing with vision-to-touchgeneration in reinforcement learning to eliminate the reliance onhigh-resolution real tactile sensors, enabling effective zero-shot deploymenton visual-only robotic systems. Specifically, ViTacGen consists of anencoder-decoder vision-to-touch generation network that generates contact depthimages, a standardized tactile representation, directly from visual imagesequence, followed by a reinforcement learning policy that fuses visual-tactiledata with contrastive learning based on visual and generated tactileobservations. We validate the effectiveness of our approach in both simulationand real world experiments, demonstrating its superior performance andachieving a success rate of up to 86\%.</description>
      <author>example@mail.com (Zhiyuan Wu, Yijiong Lin, Yongqiang Zhao, Xuyang Zhang, Zhuo Chen, Nathan Lepora, Shan Luo)</author>
      <guid isPermaLink="false">2510.14117v2</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Real Deep Research for AI, Robotics and Beyond</title>
      <link>http://arxiv.org/abs/2510.20809v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  website: https://realdeepresearch.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Real Deep Research (RDR)的全面框架，用于系统分析AI和机器人研究领域，帮助研究人员识别新兴趋势、发现跨领域机会并为新研究提供起点。&lt;h4&gt;背景&lt;/h4&gt;AI和机器人研究快速增长，每年发表超过10,000篇论文，使得研究人员难以跟上最新发展。快速发展的趋势、跨学科工作的兴起以及需要探索专业领域之外的知识都构成了这一挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个通用流程，能够系统分析任何研究领域，识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。&lt;h4&gt;方法&lt;/h4&gt;开发了Real Deep Research (RDR)全面框架，应用于AI和机器人领域，特别关注基础模型和机器人进展，同时简要扩展到其他科学领域。主论文详细介绍了RDR流程的构建，附录提供了各分析主题的广泛结果。&lt;h4&gt;主要发现&lt;/h4&gt;摘要中未明确提及具体的研究发现。&lt;h4&gt;结论&lt;/h4&gt;希望这项工作能为AI领域及更广泛领域的研究人员提供启示，帮助他们应对信息过载的挑战。&lt;h4&gt;翻译&lt;/h4&gt;随着AI和机器人研究的快速增长，现在每年产生超过10,000篇论文，研究人员越来越难以跟上最新发展。快速发展的趋势、跨学科工作的兴起以及探索专业领域之外知识的需求都构成了这一挑战。为解决这些问题，我们提出了一种能够系统分析任何研究领域的通用流程：识别新兴趋势，发现跨领域机会，并为新研究提供具体起点。在本工作中，我们介绍了Real Deep Research (RDR)，这是一个应用于AI和机器人领域的全面框架，特别关注基础模型和机器人进展。我们还简要扩展了对其他科学领域的分析。主论文详细介绍了RDR流程的构建，附录提供了每个分析主题的广泛结果。我们希望这项工作能为AI领域及其他领域的研究人员提供启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid growth of research in AI and robotics now producing over10,000 papers annually it has become increasingly difficult for researchers tostay up to date. Fast evolving trends, the rise of interdisciplinary work, andthe need to explore domains beyond one's expertise all contribute to thischallenge. To address these issues, we propose a generalizable pipeline capableof systematically analyzing any research area: identifying emerging trends,uncovering cross domain opportunities, and offering concrete starting pointsfor new inquiry. In this work, we present Real Deep Research (RDR) acomprehensive framework applied to the domains of AI and robotics, with aparticular focus on foundation models and robotics advancements. We alsobriefly extend our analysis to other areas of science. The main paper detailsthe construction of the RDR pipeline, while the appendix provides extensiveresults across each analyzed topic. We hope this work sheds light forresearchers working in the field of AI and beyond.</description>
      <author>example@mail.com (Xueyan Zou, Jianglong Ye, Hao Zhang, Xiaoyu Xiang, Mingyu Ding, Zhaojing Yang, Yong Jae Lee, Zhuowen Tu, Sifei Liu, Xiaolong Wang)</author>
      <guid isPermaLink="false">2510.20809v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>EmbodiedBrain: Expanding Performance Boundaries of Task Planning for Embodied Intelligence</title>
      <link>http://arxiv.org/abs/2510.20578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了EmbodiedBrain，一种新型的视觉语言基础模型，解决了当前具身AI模型在模型设计、实时性能评估和离线指标方面的局限性，实现了在所有评估指标上的最先进性能。&lt;h4&gt;背景&lt;/h4&gt;实现通用人工智能(AGI)需要能够在物理环境中进行稳健的空间感知、有效任务规划和自适应执行的具身AI智能体。然而，当前用于具身任务的大型语言模型和多模态大型语言模型存在关键局限性。&lt;h4&gt;目的&lt;/h4&gt;解决当前具身AI模型的局限性，提出一种新的视觉语言基础模型，提高具身智能体在物理环境中的感知、规划和执行能力。&lt;h4&gt;方法&lt;/h4&gt;开发了EmbodiedBrain模型(7B和32B参数规模)，采用与智能体对齐的数据结构，结合大规模监督微调(SFT)和步骤增强组相对策略优化(Step-GRPO)训练方法，引入包含生成奖励模型(GRM)的全面奖励系统，并建立三部分评估体系(通用、规划和端到端模拟基准测试)。&lt;h4&gt;主要发现&lt;/h4&gt;EmbodiedBrain在所有评估指标上实现了卓越性能，为具身基础模型建立了新的最先进水平，有效解决了模型设计与智能体需求之间的差距、实时延迟与性能的权衡问题，以及不真实的离线评估指标问题。&lt;h4&gt;结论&lt;/h4&gt;EmbodiedBrain为下一代通用具身智能体的发展铺平了道路，所有数据、模型权重和评估方法均已开源，可供研究社区使用。&lt;h4&gt;翻译&lt;/h4&gt;实现通用人工智能(AGI)需要能够在物理环境中进行稳健的空间感知、有效任务规划和自适应执行的具身AI智能体。然而，当前用于具身任务的大型语言模型(LLMs)和多模态大型语言模型(MLLMs)存在关键局限性，包括模型设计与智能体需求之间的显著差距、实时延迟与性能之间的不可避免权衡，以及使用不真实的离线评估指标。为解决这些挑战，我们提出了EmbodiedBrain，一种有7B和32B两种参数规模的新型视觉语言基础模型。我们的框架具有与智能体对齐的数据结构，采用结合大规模监督微调(SFT)和步骤增强组相对策略优化(Step-GRPO)的强大训练方法，通过将前序步骤整合为引导前体来提高长距离任务成功率。此外，我们引入了包含在基础设施层面加速的生成奖励模型(GRM)的全面奖励系统，以提高训练效率。为进行彻底验证，我们建立了包含通用、规划和端到端模拟基准测试的三部分评估体系，并提出了一个具有挑战性的新模拟环境并开源。实验结果表明，EmbodiedBrain在所有指标上都实现了卓越性能，为具身基础模型建立了新的最先进水平。为铺平下一代通用具身智能体的发展道路，我们开源了所有数据、模型权重和评估方法，可在https://zterobot.github.io/EmbodiedBrain.github.io获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The realization of Artificial General Intelligence (AGI) necessitatesEmbodied AI agents capable of robust spatial perception, effective taskplanning, and adaptive execution in physical environments. However, currentlarge language models (LLMs) and multimodal LLMs (MLLMs) for embodied taskssuffer from key limitations, including a significant gap between model designand agent requirements, an unavoidable trade-off between real-time latency andperformance, and the use of unauthentic, offline evaluation metrics. To addressthese challenges, we propose EmbodiedBrain, a novel vision-language foundationmodel available in both 7B and 32B parameter sizes. Our framework features anagent-aligned data structure and employs a powerful training methodology thatintegrates large-scale Supervised Fine-Tuning (SFT) with Step-Augumented GroupRelative Policy Optimization (Step-GRPO), which boosts long-horizon tasksuccess by integrating preceding steps as Guided Precursors. Furthermore, weincorporate a comprehensive reward system, including a Generative Reward Model(GRM) accelerated at the infrastructure level, to improve training efficiency.For enable thorough validation, we establish a three-part evaluation systemencompassing General, Planning, and End-to-End Simulation Benchmarks,highlighted by the proposal and open-sourcing of a novel, challengingsimulation environment. Experimental results demonstrate that EmbodiedBrainachieves superior performance across all metrics, establishing a newstate-of-the-art for embodied foundation models. Towards paving the way for thenext generation of generalist embodied agents, we open-source all of our data,model weight, and evaluating methods, which are available athttps://zterobot.github.io/EmbodiedBrain.github.io.</description>
      <author>example@mail.com (Ding Zou, Feifan Wang, Mengyu Ge, Siyuan Fan, Zongbing Zhang, Wei Chen, Lingfeng Wang, Zhongyou Hu, Wenrui Yan, Zhengwei Gao, Hao Wang, Weizhao Jin, Yu Zhang, Hainan Zhao, Mingliang Zhang, Xianxian Xi, Yaru Zhang, Wenyuan Li, Zhengguang Gao, Yurui Zhu)</author>
      <guid isPermaLink="false">2510.20578v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>A Unified Framework for Zero-Shot Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2510.20542v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了零样本强化学习的第一个统一框架，引入了一致的符号和分类法，将现有方法组织为直接表示和组合表示两大类，为该领域提供了原则性基础和研究方向。&lt;h4&gt;背景&lt;/h4&gt;零样本强化学习允许代理在不监督的情况下开发通用能力，无需额外训练或规划即可解决下游任务。与传统RL优化固定奖励不同，零样本RL需要代理编码足够丰富的表示以立即适应任何目标，类似于视觉和语言基础模型。尽管该领域兴趣增长，但缺乏共同的分析视角。&lt;h4&gt;目的&lt;/h4&gt;提出零样本强化学习的第一个统一框架，引入一致的符号和分类法，组织现有方法并允许直接比较不同方法，为该领域提供原则性基础。&lt;h4&gt;方法&lt;/h4&gt;框架将算法分为两类：直接表示（学习从奖励到策略的端到端映射）和组合表示（利用值函数的子结构分解表示）。在此框架内，突出方法的共同原则和关键差异，为后续特征方法推导扩展界限，提供其在零样本环境中的新视角。&lt;h4&gt;主要发现&lt;/h4&gt;通过共同视角整合了现有工作，揭示了不同方法间的共享原则和关键差异，为后续特征方法提供了新的理论视角，表明组合表示可能更适合零样本场景。&lt;h4&gt;结论&lt;/h4&gt;该框架为零样本强化学习的未来研究提供了原则性基础，并指明了开发更通用代理的明确路径，有助于推动通用人工智能代理的发展。&lt;h4&gt;翻译&lt;/h4&gt;零样本强化学习(RL)已成为一种在不监督情况下开发通用代理的设置，能够在测试时无需额外训练或规划的情况下解决下游任务。与传统优化固定奖励的RL不同，零样本RL需要代理编码足够丰富的表示以支持立即适应任何目标，这与视觉和语言基础模型相类似。尽管兴趣日益增长，该领域仍缺乏共同的分析视角。我们提出了零样本RL的第一个统一框架，我们的引入了一致的符号和分类法，组织了现有方法并允许直接比较它们。我们框架的核心是将算法分为两个家族：直接表示，学习从奖励到策略的端到端映射；以及组合表示，利用值函数的子结构分解表示。在此框架内，我们突出了跨方法的共同原则和关键差异，并为后续特征方法推导了扩展界限，提供了它们在零样本环境中性能的新视角。通过在共同视角下整合现有工作，我们的框架为未来零样本RL研究提供了原则性基础，并概述了开发更通用代理的明确路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Zero-shot reinforcement learning (RL) has emerged as a setting for developinggeneral agents in an unsupervised manner, capable of solving downstream taskswithout additional training or planning at test-time. Unlike conventional RL,which optimizes policies for a fixed reward, zero-shot RL requires agents toencode representations rich enough to support immediate adaptation to anyobjective, drawing parallels to vision and language foundation models. Despitegrowing interest, the field lacks a common analytical lens.  We present the first unified framework for zero-shot RL. Our formulationintroduces a consistent notation and taxonomy that organizes existingapproaches and allows direct comparison between them. Central to our frameworkis the classification of algorithms into two families: direct representations,which learn end-to-end mappings from rewards to policies, and compositionalrepresentations, which decompose the representation leveraging the substructureof the value function. Within this framework, we highlight shared principlesand key differences across methods, and we derive an extended bound forsuccessor-feature methods, offering a new perspective on their performance inthe zero-shot regime. By consolidating existing work under a common lens, ourframework provides a principled foundation for future research in zero-shot RLand outlines a clear path toward developing more general agents.</description>
      <author>example@mail.com (Jacopo Di Ventura, Jan Felix Kleuker, Aske Plaat, Thomas Moerland)</author>
      <guid isPermaLink="false">2510.20542v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Dino-Diffusion Modular Designs Bridge the Cross-Domain Gap in Autonomous Parking</title>
      <link>http://arxiv.org/abs/2510.20335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code is at  https://github.com/ChampagneAndfragrance/Dino_Diffusion_Parking_Official&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了Dino-Diffusion Parking (DDP)，一种结合视觉基础模型与基于扩散规划的自动停车流水线，解决了在不同环境条件下停车的鲁棒性问题。&lt;h4&gt;背景&lt;/h4&gt;停车是驾驶安全的关键支柱，尽管端到端方法在领域内取得了良好结果，但在天气和光照变化等条件下的鲁棒性仍是主要挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种领域无关的自动停车流水线，实现分布变化下的通用感知和鲁棒运动规划。&lt;h4&gt;方法&lt;/h4&gt;提出Dino-Diffusion Parking (DDP)，将视觉基础模型与基于扩散的规划相结合，在CARLA常规设置中训练，然后以零样本方式转移到更具挑战性的环境。&lt;h4&gt;主要发现&lt;/h4&gt;模型在所有测试的分布外场景中停车成功率 consistently保持在90%以上，消融研究证实网络架构和算法设计显著提高了跨域性能，在3D高斯飞溅环境中的测试显示了有希望的模拟到现实迁移能力。&lt;h4&gt;结论&lt;/h4&gt;所提出的DDP方法在不同环境条件下都能实现高成功率的自动停车，并且具有从模拟到现实的迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;停车是驾驶安全的关键支柱。尽管最近的端到端方法在领域内取得了有希望的结果，但在领域变化（如天气和光照变化）下的鲁棒性仍然是一个关键挑战。我们提出Dino-Diffusion Parking (DDP)，一个领域无关的自动停车流水线，它将视觉基础模型与基于扩散的规划相结合，以实现分布变化下的通用感知和鲁棒运动规划。我们在CARLA的常规设置中训练我们的流水线，并以零样本方式将其转移到更具挑战性的设置中。我们的模型在所有测试的分布外场景中停车成功率始终保持在90%以上，消融研究证实，网络架构和算法设计都显著提高了跨域性能，优于现有基线。此外，在从真实停车场重建的3D高斯飞溅环境中的测试显示了有希望的模拟到现实迁移。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决自动驾驶停车系统在不同环境条件下的跨域适应问题。当系统从训练环境（如晴天）转移到部署环境（如雨天、雾天或不同光照条件）时，性能会显著下降。这个问题很重要，因为停车占美国车辆事故的20%，且91%与倒车操作相关，准确的感知、规划和控制对安全至关重要。此外，传统解决方案需要大量收集不同条件下的数据，成本高昂，而本文方法无需额外数据就能适应各种环境。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者将自动驾驶停车问题分解为感知、规划和控制三个模块，而非使用单一端到端模型。他们借鉴了多个现有工作：使用DINOv2视觉基础模型实现鲁棒感知，参考'Lift, Splat, Shoot'和'BEVFormer'进行BEV转换，借鉴机器人领域的扩散模型进行运动规划，并采用经典的Stanley控制器进行轨迹跟踪。作者特别设计了'后视目标重标记'的数据增强策略，通过人工扰动目标位置增强数据多样性，提高目标识别的鲁棒性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是使用模块化设计解耦感知和规划，利用视觉基础模型实现跨环境鲁棒感知，通过扩散模型减少运动规划中的累积误差，并结合经典控制器实现精确跟踪。整体流程：1)使用DINOv2处理摄像头图像生成鲁棒特征；2)将特征转换为鸟瞰图(BEV)表示；3)通过交叉注意力和FiLM结构将目标位置信息融合到BEV特征；4)扩散模型基于融合特征和目标位置预测轨迹；5)Stanley控制器根据预测轨迹生成控制命令执行停车。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1)首次研究跨域自动驾驶停车问题，实现零样本迁移；2)模块化设计解耦感知和规划，避免过拟合；3)目标重标记数据增强技术提高目标识别鲁棒性；4)在SE(2)空间进行扩散运动规划，包含位置和方向信息；5)在3D高斯飞溅环境中验证模拟到真实世界的迁移能力。相比之前工作，本文方法在跨域场景中表现更好，不需要额外收集不同条件的数据，结合了模块化设计和扩散模型优势，并针对停车任务进行了专门优化。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文提出了一种基于视觉基础模型和扩散模型的模块化自动驾驶停车框架，实现了在无需额外数据的情况下跨环境零样本迁移的能力，显著提升了自动驾驶系统在不同天气、光照条件下的停车鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parking is a critical pillar of driving safety. While recent end-to-end (E2E)approaches have achieved promising in-domain results, robustness under domainshifts (e.g., weather and lighting changes) remains a key challenge. Ratherthan relying on additional data, in this paper, we propose Dino-DiffusionParking (DDP), a domain-agnostic autonomous parking pipeline that integratesvisual foundation models with diffusion-based planning to enable generalizedperception and robust motion planning under distribution shifts. We train ourpipeline in CARLA at regular setting and transfer it to more adversarialsettings in a zero-shot fashion. Our model consistently achieves a parkingsuccess rate above 90% across all tested out-of-distribution (OOD) scenarios,with ablation studies confirming that both the network architecture andalgorithmic design significantly enhance cross-domain performance over existingbaselines. Furthermore, testing in a 3D Gaussian splatting (3DGS) environmentreconstructed from a real-world parking lot demonstrates promising sim-to-realtransfer.</description>
      <author>example@mail.com (Zixuan Wu, Hengyuan Zhang, Ting-Hsuan Chen, Yuliang Guo, David Paz, Xinyu Huang, Liu Ren)</author>
      <guid isPermaLink="false">2510.20335v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Breakdance Video classification in the age of Generative AI</title>
      <link>http://arxiv.org/abs/2510.20287v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究分析了现代视频基础模型在霹雳舞这一小众但流行的舞蹈体育中的应用性，发现视频编码器模型在预测任务上表现优于最先进的视频语言模型。&lt;h4&gt;背景&lt;/h4&gt;大型视觉语言模型已在多个体育用例中得到广泛应用，但大多数研究仅针对足球、板球、篮球等流行体育项目，专注于视觉问答和精彩片段生成等生成任务。&lt;h4&gt;目的&lt;/h4&gt;分析现代视频基础模型（包括编码器和解码器）在霹雳舞这一非常小众但极受欢迎的舞蹈体育中的应用性。&lt;h4&gt;方法&lt;/h4&gt;评估视频编码器模型和视频语言模型在霹雳舞视频分类任务上的表现，并提供编码器模型选择和微调解码器模型分析的见解。&lt;h4&gt;主要发现&lt;/h4&gt;视频编码器模型在预测任务上继续优于最先进的视频语言模型，研究提供了如何选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作机制进行了详细分析。&lt;h4&gt;结论&lt;/h4&gt;视频编码器模型在特定体育应用（如霹雳舞）中可能比视频语言模型更有效。&lt;h4&gt;翻译&lt;/h4&gt;大型视觉语言模型最近在多个体育用例中得到了广泛应用。这些工作大多针对足球、板球、篮球等流行体育项目的一个有限子集，专注于视觉问答、精彩片段生成等生成任务。这项工作分析了现代视频基础模型（包括编码器和解码器）在霹雳舞这种非常小众但极受欢迎的舞蹈体育中的应用性。我们的结果表明，视频编码器模型在预测任务上继续优于最先进的视频语言模型。我们提供了如何选择编码器模型的见解，并对微调后的解码器模型在霹雳舞视频分类中的工作机制进行了详细分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Vision Language models have seen huge application in several sportsuse-cases recently. Most of these works have been targeted towards a limitedsubset of popular sports like soccer, cricket, basketball etc; focusing ongenerative tasks like visual question answering, highlight generation. Thiswork analyzes the applicability of the modern video foundation models (bothencoder and decoder) for a very niche but hugely popular dance sports -breakdance. Our results show that Video Encoder models continue to outperformstate-of-the-art Video Language Models for prediction tasks. We provideinsights on how to choose the encoder model and provide a thorough analysisinto the workings of a finetuned decoder model for breakdance videoclassification.</description>
      <author>example@mail.com (Sauptik Dhar, Naveen Ramakrishnan, Michelle Munson)</author>
      <guid isPermaLink="false">2510.20287v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Optimistic Task Inference for Behavior Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OpTI-BFM是一种改进的行为基础模型方法，通过直接对奖励函数不确定性建模和乐观决策，实现了在测试时仅通过环境交互来高效推断和优化奖励函数，显著减少了数据需求。&lt;h4&gt;背景&lt;/h4&gt;行为基础模型(BFMs)能够在测试时直接检索针对任何指定奖励函数的高性能策略，实现零样本强化学习。尽管这种方法在计算上高效，但在数据方面效率较低，因为它通常需要在非平凡的推断数据集上计算奖励，假设可以访问奖励的功能形式或需要大量标注工作。&lt;h4&gt;目的&lt;/h4&gt;解决BFMs在数据效率方面的问题，使模型能够通过在测试时仅与环境交互来进行任务推断，避免对奖励函数功能形式的依赖或大量标注工作。&lt;h4&gt;方法&lt;/h4&gt;提出OpTI-BFM，一种乐观决策标准，直接对奖励函数的不确定性进行建模，并指导BFMs进行任务推断的数据收集。通过与线性bandit的上置信度算法的直接连接，为训练良好的BFMs提供了遗憾界限。&lt;h4&gt;主要发现&lt;/h4&gt;在既定的零样本基准上评估OpTI-BFM后，观察到它使基于后继特征的BFMs能够在少量回合中识别和优化未见过的奖励函数，且计算开销最小。&lt;h4&gt;结论&lt;/h4&gt;OpTI-BFM解决了传统BFMs在数据效率方面的限制，使其能够在测试时仅通过环境交互来推断和优化任务，显著减少了数据需求。&lt;h4&gt;翻译&lt;/h4&gt;行为基础模型(BFMs)能够检索针对任何在测试时直接指定的奖励函数的高性能策略，通常被称为零样本强化学习(RL)。虽然这在计算方面是一个非常高效的过程，但在数据方面可能效率较低：作为标准假设，BFMs需要在非平凡的推断数据集上计算奖励，假设可以访问奖励的功能形式或需要大量的标注工作。为了减轻这些限制，我们解决了在测试时仅通过与环境交互来进行任务推断的问题。我们提出了OpTI-BFM，一种乐观决策标准，直接对奖励函数的不确定性进行建模，并指导BFMs进行任务推断的数据收集。形式上，我们通过与线性bandit的上置信度算法的直接连接，为训练良好的BFMs提供了遗憾界限。经验上，我们在既定的零样本基准上评估了OpTI-BFM，并观察到它使基于后继特征的BFMs能够在少量回合中识别和优化未见过的奖励函数，且计算开销最小。代码可在https://github.com/ThomasRupf/opti-bfm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Behavior Foundation Models (BFMs) are capable of retrieving high-performingpolicy for any reward function specified directly at test-time, commonlyreferred to as zero-shot reinforcement learning (RL). While this is a veryefficient process in terms of compute, it can be less so in terms of data: as astandard assumption, BFMs require computing rewards over a non-negligibleinference dataset, assuming either access to a functional form of rewards, orsignificant labeling efforts. To alleviate these limitations, we tackle theproblem of task inference purely through interaction with the environment attest-time. We propose OpTI-BFM, an optimistic decision criterion that directlymodels uncertainty over reward functions and guides BFMs in data collection fortask inference. Formally, we provide a regret bound for well-trained BFMsthrough a direct connection to upper-confidence algorithms for linear bandits.Empirically, we evaluate OpTI-BFM on established zero-shot benchmarks, andobserve that it enables successor-features-based BFMs to identify and optimizean unseen reward function in a handful of episodes with minimal computeoverhead. Code is available at https://github.com/ThomasRupf/opti-bfm.</description>
      <author>example@mail.com (Thomas Rupf, Marco Bagatella, Marin Vlastelica, Andreas Krause)</author>
      <guid isPermaLink="false">2510.20264v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>There is No "apple" in Timeseries: Rethinking TSFM through the Lens of Invariance</title>
      <link>http://arxiv.org/abs/2510.20119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)与轻量级监督基线模型和经典模型性能相当，差距源于简单导入NLP或CV流程。时间序列数据不像图像和文本那样直接捕捉人类概念，因此'在线抓取一切'的范式不适用。进步需要从机会性聚合转向原则性设计，构建系统跨越保持时间语义不变性空间的数据集，并基于第一原理构建时间序列不变性本体论，以确保表示完整性，使TSFMs实现泛化、推理和真正涌现行为所需的对齐结构。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)数量不断增加，但轻量级监督基线模型甚至经典模型通常与它们表现相当。这种差距源于简单导入NLP或CV的流程。&lt;h4&gt;目的&lt;/h4&gt;提出需要从机会性聚合转向原则性设计：构建数据集，系统性地跨越保持时间语义的不变性空间。&lt;h4&gt;方法&lt;/h4&gt;建议基于第一原理构建时间序列不变性的本体论，通过不变性覆盖确保表示的完整性。&lt;h4&gt;主要发现&lt;/h4&gt;在语言和视觉领域，大规模网络语料库密集捕捉人类概念，但时间序列数据没有直接对应的概念，因此'在线抓取一切'的范式对时间序列不适用。&lt;h4&gt;结论&lt;/h4&gt;只有通过不变性覆盖确保表示的完整性，TSFMs才能实现泛化、推理和真正涌现行为所需的对齐结构。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)不断增加，然而轻量级监督基线和甚至经典模型常常与它们匹敌。我们认为这种差距源于简单导入NLP或CV流程。在语言和视觉中，大规模网络语料库密集捕捉人类概念，即有无数的苹果图像和文本。相比之下，时间序列数据设计用来补充图像和文本模态。没有包含'苹果'概念的时间序列数据集。因此，'在线抓取一切'的范式对时间序列不适用。我们认为进步需要从机会性转向原则性设计：构建数据集，系统性地跨越保持时间语义的不变性空间。为此，我们建议时间序列不变性的本体论应基于第一原理构建。只有通过不变性覆盖确保表示的完整性，TSFMs才能实现泛化、推理和真正涌现行为所需的对齐结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Timeseries foundation models (TSFMs) have multiplied, yet lightweightsupervised baselines and even classical models often match them. We argue thisgap stems from the naive importation of NLP or CV pipelines. In language andvision, large web-scale corpora densely capture human concepts i.e. there arecountless images and text of apples. In contrast, timeseries data is built tocomplement the image and text modalities. There are no timeseries dataset thatcontains the concept apple. As a result, the scrape-everything-online paradigmfails for TS. We posit that progress demands a shift from opportunisticaggregation to principled design: constructing datasets that systematicallyspan the space of invariance that preserve temporal semantics. To this end, wesuggest that the ontology of timeseries invariances should be built based onfirst principles. Only by ensuring representational completeness throughinvariance coverage can TSFMs achieve the aligned structure necessary forgeneralisation, reasoning, and truly emergent behaviour.</description>
      <author>example@mail.com (Arian Prabowo, Flora D. Salim)</author>
      <guid isPermaLink="false">2510.20119v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>BIOCAP: Exploiting Synthetic Captions Beyond Labels in Biological Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20095v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://imageomics.github.io/biocap/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究探讨了描述性字幕作为生物多模态基础模型的额外监督来源，通过使用多模态大语言模型生成合成字幕，训练出BIOCAP模型，在物种分类和文本图像检索方面表现优异。&lt;h4&gt;背景&lt;/h4&gt;图像和字幕可以被视为物种潜在形态空间中的互补样本，各自捕捉特定的生物学特征。在训练中加入字幕可以促进与共享潜在结构的对齐，强调可能有诊断价值的特征，同时抑制虚假相关性。&lt;h4&gt;目的&lt;/h4&gt;解决生物有机体生物学中大规模获取忠实、实例特定字幕的挑战，以充分利用自然语言监督在生物多模态基础模型中的应用。&lt;h4&gt;方法&lt;/h4&gt;使用多模态大语言模型(MLLMs)生成合成字幕，这些字幕由维基百科衍生的视觉信息和特定于分类群的格式示例指导，以减少幻觉并产生准确的描述性字幕。然后使用这些字幕训练BIOCAP(即带有字幕的BIOCLIP)模型。&lt;h4&gt;主要发现&lt;/h4&gt;BIOCAP模型能够捕捉丰富的语义，并在物种分类和文本图像检索方面取得强大的性能。&lt;h4&gt;结论&lt;/h4&gt;描述性字幕在连接生物图像与多模态基础模型方面具有超越标签的价值。&lt;h4&gt;翻译&lt;/h4&gt;本研究探讨了描述性字幕作为生物多模态基础模型的额外监督来源。图像和字幕可以被视为物种潜在形态空间中的互补样本，每种都捕捉了特定的生物学特征。在训练中加入字幕可以鼓励与这种共享潜在结构的对齐，强调可能有诊断价值的特征，同时抑制虚假相关性。然而，主要挑战在于大规模获取忠实、实例特定的字幕。这一要求限制了自然语言监督在生物有机体生物学中的应用，与其他许多科学领域相比。我们通过使用多模态大语言模型(MLLMs)生成合成字幕来弥补这一差距，这些字幕由维基百科衍生的视觉信息和特定于分类群的格式示例指导。这些特定领域的上下文有助于减少幻觉，并产生准确、基于实例的描述性字幕。使用这些字幕，我们训练了BIOCAP(即带有字幕的BIOCLIP)，这是一个能够捕捉丰富语义并在物种分类和文本图像检索方面取得强大性能的生物基础模型。这些结果证明了描述性字幕在连接生物图像与多模态基础模型方面超越标签的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work investigates descriptive captions as an additional source ofsupervision for biological multimodal foundation models. Images and captionscan be viewed as complementary samples from the latent morphospace of aspecies, each capturing certain biological traits. Incorporating captionsduring training encourages alignment with this shared latent structure,emphasizing potentially diagnostic characters while suppressing spuriouscorrelations. The main challenge, however, lies in obtaining faithful,instance-specific captions at scale. This requirement has limited theutilization of natural language supervision in organismal biology compared withmany other scientific domains. We complement this gap by generating syntheticcaptions with multimodal large language models (MLLMs), guided byWikipedia-derived visual information and taxon-tailored format examples. Thesedomain-specific contexts help reduce hallucination and yield accurate,instance-based descriptive captions. Using these captions, we train BIOCAP(i.e., BIOCLIP with Captions), a biological foundation model that captures richsemantics and achieves strong performance in species classification andtext-image retrieval. These results demonstrate the value of descriptivecaptions beyond labels in bridging biological images with multimodal foundationmodels.</description>
      <author>example@mail.com (Ziheng Zhang, Xinyue Ma, Arpita Chowdhury, Elizabeth G. Campolongo, Matthew J. Thompson, Net Zhang, Samuel Stevens, Hilmar Lapp, Tanya Berger-Wolf, Yu Su, Wei-Lun Chao, Jianyang Gu)</author>
      <guid isPermaLink="false">2510.20095v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Surfer 2: The Next Generation of Cross-Platform Computer Use Agents</title>
      <link>http://arxiv.org/abs/2510.19949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 9 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Surfer 2是一个统一的架构，仅从视觉观察操作，在Web、桌面和移动三种环境中实现了最先进的性能，无需任务特定微调即可超越人类表现。&lt;h4&gt;背景&lt;/h4&gt;构建能够在网络、桌面和移动环境中通用的代理仍然是一个开放的挑战，因为之前的系统依赖于特定环境的接口，限制了跨平台部署。&lt;h4&gt;目的&lt;/h4&gt;介绍Surfer 2，一个统一的架构，仅从视觉观察操作，在所有三种环境中实现最先进的性能。&lt;h4&gt;方法&lt;/h4&gt;Surfer 2集成了分层上下文管理、解耦的规划和执行，以及自适应恢复的自验证， enabling可靠操作在长任务范围内。&lt;h4&gt;主要发现&lt;/h4&gt;在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%的准确率，在OSWorld上达到60.1%的准确率，在AndroidWorld上达到87.1%的准确率，超越了所有之前的系统，无需任务特定的微调，多次尝试后，Surfer 2在所有基准测试中超过了人类性能。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，系统编排增强了基础模型的能力，仅通过视觉交互实现了通用计算机控制，同时呼吁新一代视觉语言模型以实现帕累托最优的成本效益。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在网络、桌面和移动环境中通用的代理仍然是一个开放的挑战，因为之前的系统依赖于特定环境的接口，这限制了跨平台部署。我们介绍了Surfer 2，一个统一的架构，仅从视觉观察操作，在所有三种环境中实现最先进的性能。Surfer 2集成了分层上下文管理、解耦的规划和执行，以及自适应恢复的自验证， enabling可靠操作在长任务范围内。我们的系统在WebVoyager上达到97.1%的准确率，在WebArena上达到69.6%，在OSWorld上达到60.1%，在AndroidWorld上达到87.1%，超越了所有之前的系统，无需任务特定的微调。多次尝试后，Surfer 2在所有基准测试中超过了人类性能。这些结果表明，系统编排增强了基础模型的能力，仅通过视觉交互实现了通用计算机控制，同时呼吁新一代视觉语言模型以实现帕累托最优的成本效益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building agents that generalize across web, desktop, and mobile environmentsremains an open challenge, as prior systems rely on environment-specificinterfaces that limit cross-platform deployment. We introduce Surfer 2, aunified architecture operating purely from visual observations that achievesstate-of-the-art performance across all three environments. Surfer 2 integrateshierarchical context management, decoupled planning and execution, andself-verification with adaptive recovery, enabling reliable operation over longtask horizons. Our system achieves 97.1% accuracy on WebVoyager, 69.6% onWebArena, 60.1% on OSWorld, and 87.1% on AndroidWorld, outperforming all priorsystems without task-specific fine-tuning. With multiple attempts, Surfer 2exceeds human performance on all benchmarks. These results demonstrate thatsystematic orchestration amplifies foundation model capabilities and enablesgeneral-purpose computer control through visual interaction alone, whilecalling for a next-generation vision language model to achieve Pareto-optimalcost-efficiency.</description>
      <author>example@mail.com (Mathieu Andreux, Märt Bakler, Yanael Barbier, Hamza Ben Chekroun, Emilien Biré, Antoine Bonnet, Riaz Bordie, Nathan Bout, Matthias Brunel, Aleix Cambray, Pierre-Louis Cedoz, Antoine Chassang, Gautier Cloix, Ethan Connelly, Alexandra Constantinou, Ramzi De Coster, Hubert de la Jonquiere, Aurélien Delfosse, Maxime Delpit, Alexis Deprez, Augustin Derupti, Mathieu Diaz, Shannon D'Souza, Julie Dujardin, Abai Edmund, Michael Eickenberg, Armand Fatalot, Wissem Felissi, Isaac Herring, Xavier Koegler, Erwan Le Jumeau de Kergaradec, Aurélien Lac, Maxime Langevin, Corentin Lauverjat, Antonio Loison, Avshalom Manevich, Axel Moyal, Axel Nguyen Kerbel, Marinela Parovic, Julien Revelle, Guillaume Richard, Mats Richter, Ronan Riochet, María Santos, Romain Savidan, Laurent Sifre, Maxime Theillard, Marc Thibault, Ivan Valentini, Tony Wu, Laura Yie, Kai Yuan, Jevgenij Zubovskij)</author>
      <guid isPermaLink="false">2510.19949v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Seed3D 1.0: From Images to High-Fidelity Simulation-Ready 3D Assets</title>
      <link>http://arxiv.org/abs/2510.19944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Seed3D 1.0 Technical Report; Official Page on  https://seed.bytedance.com/seed3d&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Seed3D 1.0是一个基础模型，可以从单张图像生成可用于仿真的3D资产，解决了具身AI代理训练环境中的可扩展性问题，同时保持物理准确性。&lt;h4&gt;背景&lt;/h4&gt;开发具身AI代理需要平衡内容多样性和物理准确性的可扩展训练环境。现有世界模拟器存在局限：基于视频的方法内容多样但缺乏实时物理反馈，基于物理的引擎物理准确但受限于昂贵的手动资产创建。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够从单张图像生成仿真就绪3D资产的基础模型，解决可扩展性挑战，同时保持物理严谨性。&lt;h4&gt;方法&lt;/h4&gt;提出Seed3D 1.0基础模型，生成具有准确几何、良好对齐纹理和真实物理材质的3D资产，可直接集成到物理引擎中，支持机器人操作和仿真训练，并能扩展到完整场景生成。&lt;h4&gt;主要发现&lt;/h4&gt;Seed3D 1.0生成的3D资产具有准确的几何结构、对齐良好的纹理和真实的物理材质，可直接用于物理引擎，支持机器人操作和仿真训练，并能扩展到完整场景生成。&lt;h4&gt;结论&lt;/h4&gt;Seed3D 1.0通过实现可扩展的仿真就绪内容创建，为推进基于物理的世界模拟器提供了基础，现已可在指定网址获取。&lt;h4&gt;翻译&lt;/h4&gt;开发具身AI代理需要可扩展的训练环境，这些环境需要在内容多样性和物理准确性之间取得平衡。世界模拟器提供了这样的环境，但面临不同的限制：基于视频的方法可以生成多样化的内容，但缺乏实时物理反馈以支持交互式学习；而基于物理的引擎能提供准确的动力学，但由于昂贵的手动资产创建而面临可扩展性限制。我们提出了Seed3D 1.0，这是一个基础模型，可以从单张图像生成仿真就绪的3D资产，解决了可扩展性挑战，同时保持物理严谨性。与现有的3D生成模型不同，我们的系统生成具有准确几何、对齐良好的纹理和真实物理材质的资产。这些资产可以直接集成到物理引擎中，只需最少的配置，支持在机器人操作和仿真训练中部署。除了单个对象外，系统还能通过将对象组装成连贯的环境来扩展到完整场景生成。通过实现可扩展的仿真就绪内容创建，Seed3D 1.0为推进基于物理的世界模拟器提供了基础。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何从单张图像生成高质量的、可直接用于物理仿真的3D资产的问题。这个问题重要是因为开发具身AI代理需要可扩展的训练环境，平衡内容多样性和物理准确性，而现有世界模拟器面临根本性权衡：基于视频的方法缺乏实时物理反馈，基于物理的引擎则受限于手动资产创建的可扩展性，制约了训练环境的多样性和规模。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有世界模拟器的局限性，认识到需要结合生成多样性和物理严谨性，设计了Seed3D 1.0基础模型。系统借鉴了现有工作：几何生成部分采用VAE和基于修正流的扩散Transformer架构；使用DINOv2和RADIO作为图像编码器；纹理生成部分借鉴多模态扩散Transformer；数据预处理参考3DShape2VecSet设计；训练基础设施采用FlashAttention和混合分片数据并行等技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过结合生成多样性和物理严谨性，解决3D资产创建的可扩展性问题，生成可直接用于物理仿真的高质量3D资产。整体流程包括：1)几何生成：使用Seed3D-VAE学习紧凑潜在表示，Seed3D-DiT合成3D形状；2)纹理生成：Seed3D-MV生成多视图图像，Seed3D-PBR分解为PBR材质图，Seed3D-UV补全UV纹理；3)数据处理：自动化预处理管道、格式标准化和质量过滤；4)训练和推理：采用渐进式策略训练模型，通过多阶段处理生成最终3D资产。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)高质量资产生成：生成具有精确几何、高分辨率纹理和真实物理材质的3D资产；2)物理引擎兼容性：资产可直接集成到物理引擎中；3)可扩展场景合成：从室内到城市环境实现连贯场景；4)技术创新：开发了Seed3D-VAE、Seed3D-DiT、Seed3D-MV、Seed3D-PBR和Seed3D-UV五个核心组件。相比之前工作，解决了几何伪影和纹理错位问题，通过UV纹理补全解决自遮挡，采用混合架构平衡跨模态学习和模态特定处理，使用长度感知时间步长维持生成质量。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Seed3D 1.0通过从单张图像生成高质量的、物理兼容的3D资产，解决了具身AI训练环境中内容多样性和物理准确性之间的权衡问题，为物理驱动的世界模拟器提供了可扩展的基础。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing embodied AI agents requires scalable training environments thatbalance content diversity with physics accuracy. World simulators provide suchenvironments but face distinct limitations: video-based methods generatediverse content but lack real-time physics feedback for interactive learning,while physics-based engines provide accurate dynamics but face scalabilitylimitations from costly manual asset creation. We present Seed3D 1.0, afoundation model that generates simulation-ready 3D assets from single images,addressing the scalability challenge while maintaining physics rigor. Unlikeexisting 3D generation models, our system produces assets with accurategeometry, well-aligned textures, and realistic physically-based materials.These assets can be directly integrated into physics engines with minimalconfiguration, enabling deployment in robotic manipulation and simulationtraining. Beyond individual objects, the system scales to complete scenegeneration through assembling objects into coherent environments. By enablingscalable simulation-ready content creation, Seed3D 1.0 provides a foundationfor advancing physics-based world simulators. Seed3D 1.0 is now available onhttps://console.volcengine.com/ark/region:ark+cn-beijing/experience/vision?modelId=doubao-seed3d-1-0-250928&amp;tab=Gen3D</description>
      <author>example@mail.com (Jiashi Feng, Xiu Li, Jing Lin, Jiahang Liu, Gaohong Liu, Weiqiang Lou, Su Ma, Guang Shi, Qinlong Wang, Jun Wang, Zhongcong Xu, Xuanyu Yi, Zihao Yu, Jianfeng Zhang, Yifan Zhu, Rui Chen, Jinxin Chi, Zixian Du, Li Han, Lixin Huang, Kaihua Jiang, Yuhan Li, Guan Luo, Shuguang Wang, Qianyi Wu, Fan Yang, Junyang Zhang, Xuanmeng Zhang)</author>
      <guid isPermaLink="false">2510.19944v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>FairGRPO: Fair Reinforcement Learning for Equitable Clinical Reasoning</title>
      <link>http://arxiv.org/abs/2510.19893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as Oral on NeurIPS 2025 GenAI4Health Workshop&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为FairGRPO的分层强化学习方法，用于提高医学人工智能系统在不同人口统计群体中的诊断公平性，通过自适应重要性加权和无监督聚类处理缺失标签问题，实验表明该方法显著提高了预测平等性和F1分数。&lt;h4&gt;背景&lt;/h4&gt;医学人工智能系统在诊断方面取得显著成就，但在不同人口统计群体中表现出明显的性能差异，对代表性不足人群造成实际伤害。多模态推理基础模型推动了临床诊断，但通过强化学习进行的推理训练继承了并放大了训练数据集中的偏见。&lt;h4&gt;目的&lt;/h4&gt;提出一种促进跨异质临床人群公平学习的方法，解决临床领域常见的缺乏人口统计标签的问题。&lt;h4&gt;方法&lt;/h4&gt;引入Fairness-aware Group Relative Policy Optimization (FairGRPO)，一种分层强化学习方法，采用基于代表性、任务难度和数据源的自适应优势重要性加权。采用无监督聚类来处理缺失的人口统计标签，当标签不可用时自动发现潜在的人口统计群体。&lt;h4&gt;主要发现&lt;/h4&gt;在7个涵盖5种临床模态的临床诊断数据集上，FairGRPO与所有普通和偏见缓解的强化学习基线相比，将预测平等性提高了27.2%，同时F1分数提高了12.49%。训练动态分析显示，FairGRPO在整个优化过程中逐步改善公平性，而基线强化学习方法在训练过程中表现出公平性恶化。基于FairGRPO，发布了FairMedGemma-4B，一个公平感知的临床VLLM，在实现最先进性能的同时显著减少了不同人口统计群体之间的差异。&lt;h4&gt;结论&lt;/h4&gt;FairGRPO是一种有效的医学人工智能系统公平性提升方法，能够在不牺牲性能的情况下提高跨人群的诊断公平性，解决了临床数据中缺乏人口统计标签的常见问题。&lt;h4&gt;翻译&lt;/h4&gt;医学人工智能系统已取得显著的诊断能力，然而它们在不同人口统计群体中持续表现出性能差异，对代表性不足的人群造成实际伤害。虽然最近的多模态推理基础模型通过整合分析多样化的医疗数据推动了临床诊断的发展，但通过强化学习进行的推理训练继承了主导多数人群的训练数据集中存在的偏见，并往往放大这些偏见。我们引入了公平感知的群体相对策略优化（FairGRPO），这是一种分层强化学习方法，促进跨异质临床人群的公平学习。FairGRPO基于代表性、任务难度和数据源采用自适应的优势重要性加权。为解决临床领域中常见的人口统计标签缺失问题，我们进一步采用无监督聚类，当标签不可用时自动发现潜在的人口统计群体。在跨越X光、CT扫描、皮肤镜检查、乳腺X光检查和超声波5种临床模态的7个临床诊断数据集上进行综合实验，我们证明FairGRPO与所有普通和偏见缓解的强化学习基线相比，将预测平等性提高了27.2%，同时F1分数提高了12.49%。此外，训练动态分析显示，FairGRPO在整个优化过程中逐步改善公平性，而基线强化学习方法在训练过程中表现出公平性恶化。基于FairGRPO，我们发布了FairMedGemma-4B，一个公平感知的临床VLLM，在实现最先进性能的同时，显著减少了不同人口统计群体之间的差异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical artificial intelligence systems have achieved remarkable diagnosticcapabilities, yet they consistently exhibit performance disparities acrossdemographic groups, causing real-world harm to underrepresented populations.While recent multimodal reasoning foundation models have advanced clinicaldiagnosis through integrated analysis of diverse medical data, reasoningtrainings via reinforcement learning inherit and often amplify biases presentin training datasets dominated by majority populations. We introduceFairness-aware Group Relative Policy Optimization (FairGRPO), a hierarchicalreinforcement learning approach that promotes equitable learning acrossheterogeneous clinical populations. FairGRPO employs adaptive importanceweighting of advantages based on representation, task difficulty, and datasource. To address the common issue of missing demographic labels in theclinical domain, we further employ unsupervised clustering, which automaticallydiscovers latent demographic groups when labels are unavailable. Throughcomprehensive experiments across 7 clinical diagnostic datasets spanning 5clinical modalities across X-ray, CT scan, dermoscropy, mammography andultrasound, we demonstrate that FairGRPO reduces predictive parity by 27.2%against all vanilla and bias mitigated RL baselines, while improving F1 scoreby 12.49%. Furthermore, training dynamics analysis reveals that FairGRPOprogressively improves fairness throughout optimization, while baseline RLmethods exhibit deteriorating fairness as training progresses. Based onFairGRPO, we release FairMedGemma-4B, a fairness-aware clinical VLLM thatachieves state-of-the-art performance while demonstrating significantly reduceddisparities across demographic groups.</description>
      <author>example@mail.com (Shiqi Dai, Wei Dai, Jiaee Cheong, Paul Pu Liang)</author>
      <guid isPermaLink="false">2510.19893v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>SEMPO: Lightweight Foundation Models for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2510.19710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by NeurIPS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SEMPO是一种新型轻量级时间序列预测基础模型，通过两个创新模块在减少预训练数据规模和模型大小的同时实现强大的预测性能。&lt;h4&gt;背景&lt;/h4&gt;现有的时间序列基础模型虽然性能出色，但网络架构庞大，需要大规模数据集进行预训练，难以在资源受限环境中部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种多功能且经济实惠的时间序列基础模型，解决现有模型在多功能性和可负担性之间的矛盾。&lt;h4&gt;方法&lt;/h4&gt;SEMPO包含两个关键模块：(1)能量感知的频谱分解模块，同时建模高能量和低能量但信息丰富的频率信号；(2)基于提示混合的Transformer，通过小型数据集特定提示学习异构时间模式，实现参数高效模型适应。&lt;h4&gt;主要发现&lt;/h4&gt;SEMPO在两个大规模基准测试(包含16个数据集)上的实验表明，与最先进方法相比，它在零样本和少样本预测场景中表现出优越性能，同时显著减少了预训练数据规模和模型大小。&lt;h4&gt;结论&lt;/h4&gt;SEMPO成功实现了时间序列预测领域多功能性和可负担性的平衡，为资源受限环境中的时间序列预测提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近大型预训练模型的兴起见证了在时间序列预测领域开发基础模型的显著成功。尽管在各种下游预测任务中表现出令人印象深刻的性能，但现有时间序列基础模型拥有庞大的网络架构，需要在大规模数据集上进行大量预训练，这严重阻碍了它们在资源受限环境中的部署。为了应对多功能性和可负担性之间日益加剧的矛盾，我们提出了SEMPO，一种新型轻量级基础模型，它只需要在相对小规模的数据上进行预训练，却表现出强大的通用时间序列预测能力。具体而言，SEMPO包含两个关键模块：1)能量感知的频谱分解模块，通过不仅建模高能量频率信号，还建模当前方法中被忽略的低能量但信息丰富的频率信号，显著提高了预训练数据的利用率；以及2)基于提示混合的Transformer，通过小型数据集特定的提示学习异构时间模式，并将时间序列标记自适应路由到基于提示的专家，实现跨不同数据集和领域的参数高效模型适应。配备这些模块后，SEMPO显著减少了预训练数据规模和模型大小，同时实现了强大的泛化能力。在覆盖16个数据集的两个大规模基准测试上进行的广泛实验表明，与最先进的方法相比，SEMPO在零样本和少样本预测场景中表现出优越性能。代码和数据可在https://github.com/mala-lab/SEMPO获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The recent boom of large pre-trained models witnesses remarkable success indeveloping foundation models (FMs) for time series forecasting. Despiteimpressive performance across diverse downstream forecasting tasks, existingtime series FMs possess massive network architectures and require substantialpre-training on large-scale datasets, which significantly hinders theirdeployment in resource-constrained environments. In response to this growingtension between versatility and affordability, we propose SEMPO, a novellightweight foundation model that requires pretraining on relativelysmall-scale data, yet exhibits strong general time series forecasting.Concretely, SEMPO comprises two key modules: 1) energy-aware SpEctraldecomposition module, that substantially improves the utilization ofpre-training data by modeling not only the high-energy frequency signals butalso the low-energy yet informative frequency signals that are ignored incurrent methods; and 2) Mixture-of-PrOmpts enabled Transformer, that learnsheterogeneous temporal patterns through small dataset-specific prompts andadaptively routes time series tokens to prompt-based experts forparameter-efficient model adaptation across different datasets and domains.Equipped with these modules, SEMPO significantly reduces both pre-training datascale and model size, while achieving strong generalization. Extensiveexperiments on two large-scale benchmarks covering 16 datasets demonstrate thesuperior performance of SEMPO in both zero-shot and few-shot forecastingscenarios compared with state-of-the-art methods. Code and data are availableat https://github.com/mala-lab/SEMPO.</description>
      <author>example@mail.com (Hui He, Kun Yi, Yuanchi Ma, Qi Zhang, Zhendong Niu, Guansong Pang)</author>
      <guid isPermaLink="false">2510.19710v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Latin in Historical Books with Large Language Models: A Multimodal Benchmark</title>
      <link>http://arxiv.org/abs/2510.19585v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review. Both the dataset and code will be published&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一项从混合语言历史文档中提取拉丁语片段的新任务，并评估了大型基础模型在此任务上的性能。&lt;h4&gt;背景&lt;/h4&gt;历史文档通常包含多种语言和不同的布局，从中提取特定语言片段具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;评估大型基础模型在从混合语言历史文档中提取拉丁语片段任务上的能力和局限性。&lt;h4&gt;方法&lt;/h4&gt;使用包含724个标注页面的多模态数据集，对大型基础模型进行了基准测试和性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;当代模型能够可靠地检测和提取拉丁语片段。&lt;h4&gt;结论&lt;/h4&gt;该研究首次全面分析了大型基础模型在从混合语言历史文档中提取拉丁语片段任务上的能力和局限性。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一项从混合语言历史文档中提取拉丁语片段的新任务，这些文档具有不同的布局。我们使用一个包含724个标注页面的多模态数据集，对大型基础模型进行了基准测试和性能评估。结果表明，使用当代模型进行可靠的拉丁语检测是可行的。我们的研究首次对这些模型在此任务上的能力和局限性进行了全面分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel task of extracting Latin fragments frommixed-language historical documents with varied layouts. We benchmark andevaluate the performance of large foundation models against a multimodaldataset of 724 annotated pages. The results demonstrate that reliable Latindetection with contemporary models is achievable. Our study provides the firstcomprehensive analysis of these models' capabilities and limits for this task.</description>
      <author>example@mail.com (Yu Wu, Ke Shu, Jonas Fischer, Lidia Pivovarova, David Rosson, Eetu Mäkelä, Mikko Tolonen)</author>
      <guid isPermaLink="false">2510.19585v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>GigaBrain-0: A World Model-Powered Vision-Language-Action Model</title>
      <link>http://arxiv.org/abs/2510.19430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  https://gigabrain0.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了GigaBrain-0，一个利用世界模型生成数据的新型视觉-语言-动作(VLA)基础模型，减少了对真实机器人数据的依赖，提高了跨任务泛化能力和策略鲁棒性，在灵巧操作、长视野和移动操作任务中实现了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;为通用机器人训练视觉-语言-动作(VLA)模型通常需要大规模的真实世界机器人数据，这些数据的收集既昂贵又耗时。物理数据收集的低效严重限制了当前VLA系统的可扩展性和泛化能力。&lt;h4&gt;目的&lt;/h4&gt;解决物理数据收集低效的问题，减少对真实机器人数据的依赖，同时提高VLA系统的跨任务泛化能力和策略鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;1. 引入GigaBrain-0，由世界模型生成数据(如视频生成、真实到真实转移、人类转移、视角转移、仿真到真实转移数据)赋能的新型VLA基础模型；2. 利用世界模型大规模生成多样化数据；3. 通过RGBD输入建模和具身思维链(CoT)监督提高策略鲁棒性；4. 开发了GigaBrain-0-Small，一个优化的轻量级变体，可在NVIDIA Jetson AGX Orin等设备上高效运行。&lt;h4&gt;主要发现&lt;/h4&gt;1. GigaBrain-0在灵巧操作、长视野和移动操作任务中实现了显著的性能提升；2. 广泛的实验证明GigaBrain-0在外观(如纹理、颜色)、物体放置和摄像机视点变化方面具有优越的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用世界模型生成数据，GigaBrain-0显著减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力和策略鲁棒性，为通用机器人提供了一个更高效、更可扩展的VLA解决方案。&lt;h4&gt;翻译&lt;/h4&gt;为通用机器人训练视觉-语言-动作(VLA)模型通常需要大规模的真实世界机器人数据，这些数据的收集既昂贵又耗时。物理数据收集的低效严重限制了当前VLA系统的可扩展性和泛化能力。为应对这一挑战，我们引入了GigaBrain-0，一个由世界模型生成数据(如视频生成、真实到真实转移、人类转移、视角转移、仿真到真实转移数据)赋能的新型VLA基础模型。通过利用世界模型大规模生成多样化数据，GigaBrain-0显著减少了对真实机器人数据的依赖，同时提高了跨任务泛化能力。我们的方法通过RGBD输入建模和具身思维链(CoT)监督进一步提高了策略鲁棒性，使模型能够在任务执行过程中推理空间几何、物体状态和长视野依赖关系。这导致在灵巧操作、长视野和移动操作任务中的实际性能显著提升。广泛的实验证明，GigaBrain-0在外观(如纹理、颜色)、物体放置和摄像机视点变化方面实现了优越的泛化能力。此外，我们提出了GigaBrain-0-Small，一个优化的轻量级变体，设计用于在NVIDIA Jetson AGX Orin等设备上高效运行。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决训练视觉-语言-行动（VLA）模型需要大规模真实世界机器人数据的问题，而收集这些数据既昂贵又耗时。这个问题很重要，因为它严重限制了当前VLA系统的可扩展性和泛化能力，阻碍了通用机器人在多样化环境中的应用。缺乏足够多样性的训练数据导致模型在现实世界中表现不佳，限制了机器人技术的实际部署。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到真实世界机器人数据收集的局限性，然后提出利用世界模型生成多样化训练数据的解决方案。他们设计了混合transformer架构，结合预训练视觉语言模型和动作扩散变换器，并引入RGB-D输入和具身思维链机制。作者借鉴了多项现有工作，包括π0等VLA模型架构、世界模型作为数据生成器、视觉语言模型如PaliGemma2、扩散模型用于视频生成，以及思维链推理和知识隔离技术。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用世界模型生成多样化、逼真的合成训练数据，减少对真实世界机器人数据的依赖，并通过RGB-D输入和具身思维链增强模型的感知和推理能力。整体流程包括：1）收集真实世界数据并利用GigaWorld生成多种合成数据（Real2Real转移、视图转移等）；2）采用混合transformer架构处理RGB-D输入和语言指令；3）训练模型生成具身思维链作为中间表示；4）基于思维链输出连续动作序列；5）提供轻量级版本适配边缘设备。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）整合多种世界模型生成的数据源，增强数据多样性；2）RGB-D输入建模提升3D空间理解；3）具身思维链监督机制改善推理能力；4）混合架构与知识隔离技术提高训练效率；5）高效数据生成与质量评估。相比之前工作，GigaBrain-0利用了更多样化的数据源（包括视图转移和Real2Real转移），具有更强的3D空间理解能力，能显式生成中间推理步骤，训练效率更高，在变化条件下泛化能力更强。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; GigaBrain-0通过创新性地整合世界模型生成的多样化训练数据和具身思维链推理机制，显著提升了视觉-语言-行动模型在真实世界任务中的泛化能力和执行效率，同时大幅减少了对昂贵真实世界机器人数据的依赖。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training Vision-Language-Action (VLA) models for generalist robots typicallyrequires large-scale real-world robot data, which is expensive andtime-consuming to collect. The inefficiency of physical data collectionseverely limits the scalability, and generalization capacity of current VLAsystems. To address this challenge, we introduce GigaBrain-0, a novel VLAfoundation model empowered by world model-generated data (e.g., videogeneration, real2real transfer, human transfer, view transfer, sim2realtransfer data). By leveraging world models to generate diverse data at scale,GigaBrain-0 significantly reduces reliance on real robot data while improvingcross-task generalization. Our approach further improves policy robustnessthrough RGBD input modeling and embodied Chain-of-Thought (CoT) supervision,enabling the model to reason about spatial geometry, object states, andlong-horizon dependencies during task execution. This leads to substantialgains in real-world performance on dexterous, long-horizon, and mobilemanipulation tasks. Extensive experiments demonstrate that GigaBrain-0 achievessuperior generalization across variations in appearances (e.g., textures,colors), object placements, and camera viewpoints. Additionally, we presentGigaBrain-0-Small, an optimized lightweight variant designed to run efficientlyon devices such as the NVIDIA Jetson AGX Orin.</description>
      <author>example@mail.com (GigaBrain Team, Angen Ye, Boyuan Wang, Chaojun Ni, Guan Huang, Guosheng Zhao, Haoyun Li, Jie Li, Jiagang Zhu, Lv Feng, Peng Li, Qiuping Deng, Runqi Ouyang, Wenkang Qin, Xinze Chen, Xiaofeng Wang, Yang Wang, Yifan Li, Yilong Li, Yiran Ding, Yuan Xu, Yun Ye, Yukun Zhou, Zhehao Dong, Zhenan Wang, Zhichao Liu, Zheng Zhu)</author>
      <guid isPermaLink="false">2510.19430v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Using Temperature Sampling to Effectively Train Robot Learning Policies on Imbalanced Datasets</title>
      <link>http://arxiv.org/abs/2510.19373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种简单的策略训练采样方法，用于缓解机器人任务数据集中物理动作序列的不平衡问题。&lt;h4&gt;背景&lt;/h4&gt;随着越来越多的机器人动作和感官观测数据集被收集用于训练大型神经网络，发现许多基于不同描述的任务实际上涉及非常相似的身体动作序列，导致数据集在物理机器人动作方面存在严重不平衡。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单的采样策略来缓解机器人任务数据集中的动作不平衡问题，提高模型在多任务场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出一种简单的策略训练采样方法，只需几行代码即可集成到现有代码库中，并在预训练小型模型和微调大型基础模型上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与之前最先进的方法相比，该方法在低资源任务上取得了显著改进，同时没有降低高资源任务上的性能，使得模型容量能够更有效地用于多任务策略。&lt;h4&gt;结论&lt;/h4&gt;在Franka Panda机械臂上的多样化任务中进一步验证了该方法的有效性，证明了其在实际应用中的可行性。&lt;h4&gt;翻译&lt;/h4&gt;越来越多的机器人动作和感官观测数据集被收集起来，用于训练日益庞大的神经网络。这些数据集是基于任务收集的，尽管这些任务在描述上可能不同，但许多任务涉及非常相似的身体动作序列（例如，'拿起苹果'与'拿起橙子'）。因此，许多机器人任务数据集在所代表的物理机器人动作方面存在严重不平衡。在这项工作中，我们提出了一种简单的策略训练采样方法来缓解这种不平衡。我们的方法只需要几行代码就可以集成到现有代码库中，并提高了泛化能力。我们在预训练小型模型和微调大型基础模型上都评估了我们的方法。结果表明，与之前最先进的方法相比，在低资源任务上取得了显著改进，同时没有降低高资源任务上的性能。这使得模型容量能够更有效地用于多任务策略。我们还进一步在Franka Panda机械臂上的多样化任务设置中验证了我们的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increasingly large datasets of robot actions and sensory observations arebeing collected to train ever-larger neural networks. These datasets arecollected based on tasks and while these tasks may be distinct in theirdescriptions, many involve very similar physical action sequences (e.g., 'pickup an apple' versus 'pick up an orange'). As a result, many datasets of robotictasks are substantially imbalanced in terms of the physical robotic actionsthey represent. In this work, we propose a simple sampling strategy for policytraining that mitigates this imbalance. Our method requires only a few lines ofcode to integrate into existing codebases and improves generalization. Weevaluate our method in both pre-training small models and fine-tuning largefoundational models. Our results show substantial improvements on low-resourcetasks compared to prior state-of-the-art methods, without degrading performanceon high-resource tasks. This enables more effective use of model capacity formulti-task policies. We also further validate our approach in a real-worldsetup on a Franka Panda robot arm across a diverse set of tasks.</description>
      <author>example@mail.com (Basavasagar Patil, Sydney Belt, Jayjun Lee, Nima Fazeli, Bernadette Bucher)</author>
      <guid isPermaLink="false">2510.19373v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>AMAuT: A Flexible and Efficient Multiview Audio Transformer Framework Trained from Scratch</title>
      <link>http://arxiv.org/abs/2510.19368v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了AMAuT框架，一个无需预训练权重且支持任意采样率和音频长度的音频模型，在多个基准测试中达到高准确度同时大幅减少计算资源消耗。&lt;h4&gt;背景&lt;/h4&gt;最近的SSAST、EAT、HuBERT、Qwen-Audio和AudioFlamingo等基础模型在标准音频基准测试中表现优异，但受限于固定的输入速率和持续时间，影响了它们的重用性。&lt;h4&gt;目的&lt;/h4&gt;开发一个无需依赖预训练权重、支持任意采样率和音频长度的音频分类框架，提高模型的灵活性和效率。&lt;h4&gt;方法&lt;/h4&gt;AMAuT集成了四个关键组件：增强驱动的多视图学习提高鲁棒性；conv1+conv7+conv1一维CNN瓶颈实现稳定的时间编码；双CLS+TAL令牌进行双向上下文表示；测试时自适应/增强(TTA²)提高推理可靠性。&lt;h4&gt;主要发现&lt;/h4&gt;在AudioMNIST、SpeechCommands V1&amp;V2、VocalSound和CochlScene五个公共基准测试上，AMAuT准确度高达99.8%，同时消耗的GPU小时数不到可比预训练模型的3%。&lt;h4&gt;结论&lt;/h4&gt;AMAuT为大型预训练模型提供了一个高效且灵活的替代方案，使最先进的音频分类在计算受限环境中变得可行。&lt;h4&gt;翻译&lt;/h4&gt;最近的SSAST、EAT、HuBERT、Qwen-Audio和AudioFlamingo等基础模型在标准音频基准测试中取得了顶尖结果，但受限于固定的输入速率和持续时间，阻碍了它们的重用性。本文引入了增强驱动多视图音频变换器(AMAuT)，这是一个从头开始训练的框架，消除对预训练权重的依赖，同时支持任意采样率和音频长度。AMAuT集成了四个关键组件：(1)增强驱动的多视图学习，提高鲁棒性；(2)conv1+conv7+conv1一维CNN瓶颈，用于稳定的时间编码；(3)双CLS+TAL令牌，用于双向上下文表示；(4)测试时自适应/增强(TTA²)，提高推理可靠性。在AudioMNIST、SpeechCommands V1&amp;V2、VocalSound和CochlScene五个公共基准测试上的实验表明，AMAuT准确度高达99.8%，同时消耗的GPU小时数不到可比预训练模型的3%。因此，AMAuT为大型预训练模型提供了一个高效且灵活的替代方案，使最先进的音频分类在计算受限环境中变得可行。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent foundational models, SSAST, EAT, HuBERT, Qwen-Audio, and AudioFlamingo, achieve top-tier results across standard audio benchmarks but arelimited by fixed input rates and durations, hindering their reusability. Thispaper introduces the Augmentation-driven Multiview Audio Transformer (AMAuT), atraining-from-scratch framework that eliminates the dependency on pre-trainedweights while supporting arbitrary sample rates and audio lengths. AMAuTintegrates four key components: (1) augmentation-driven multiview learning forrobustness, (2) a conv1 + conv7 + conv1 one-dimensional CNN bottleneck forstable temporal encoding, (3) dual CLS + TAL tokens for bidirectional contextrepresentation, and (4) test-time adaptation/augmentation (TTA^2) to improveinference reliability. Experiments on five public benchmarks, AudioMNIST,SpeechCommands V1 &amp; V2, VocalSound, and CochlScene, show that AMAuT achievesaccuracies up to 99.8% while consuming less than 3% of the GPU hours requiredby comparable pre-trained models. Thus, AMAuT presents a highly efficient andflexible alternative to large pre-trained models, making state-of-the-art audioclassification accessible in computationally constrained settings.</description>
      <author>example@mail.com (Weichuang Shao, Iman Yi Liao, Tomas Henrique Bode Maul, Tissa Chandesa)</author>
      <guid isPermaLink="false">2510.19368v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Model Forecasts: Form and Function</title>
      <link>http://arxiv.org/abs/2510.19345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列基础模型(TSFMs)虽然预测准确性高，但预测形式（点预测、分位数预测、参数化预测或轨迹集合）决定了其实际应用价值。研究发现大多数TSFMs只能提供点或参数化预测，而实际操作任务常需要保留时间依赖性的轨迹集合。研究确定了预测类型间的转换条件，证明边际分布无法确定路径相关事件概率，并将六个基本预测任务映射到最小充分预测类型，表明预测类型而非准确性才是区分模型实用价值的关键。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)在预测准确性方面表现出色，但准确性并不完全决定其实际价值。&lt;h4&gt;目的&lt;/h4&gt;研究不同预测形式对实际操作任务的支持能力，确定预测类型间的转换条件，并提供任务对齐的评估框架。&lt;h4&gt;方法&lt;/h4&gt;分析现有TSFMs的预测类型，研究预测类型之间的转换条件，证明边际分布与联合分布的关系，并将基本预测任务映射到最小充分预测类型。&lt;h4&gt;主要发现&lt;/h4&gt;1. 三分之二的TSFMs只产生点或参数化预测，而许多操作任务需要保留时间依赖性的轨迹集合；2. 轨迹集合可通过边际化转换为简单形式，但反向转换需要额外方法；3. 边际分布无法确定路径相关事件概率，无限多联合分布可具有相同边际分布但给出不同操作答案；4. 六个基本预测任务可映射到最小充分预测类型。&lt;h4&gt;结论&lt;/h4&gt;在实际应用中，预测类型而非准确性是区分模型实用价值的关键因素。选择适当的预测形式对于支持特定操作任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)实现了强大的预测准确性，然而准确性本身并不决定实际价值。预测的形式——点预测、分位数预测、参数化预测或轨迹集合——从根本上限制了它能够支持的操作任务。我们调查了最近的TSFMs，发现三分之二只产生点预测或参数化预测，而许多操作任务需要保留时间依赖性的轨迹集合。我们确定了预测类型何时可以转换、何时不可以转换：轨迹集合可以通过边际化转换为更简单的形式而无需额外假设，但反向转换则需要通过copulas或conformal方法施加时间依赖性。我们证明了边际分布无法确定路径相关事件概率——无限多的联合分布具有相同的边际分布，但对操作问题给出不同的答案。我们将六个基本预测任务映射到最小充分预测类型，并提供了任务对齐的评估框架。我们的分析阐明了当预测类型而非准确性区分实用价值时的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time-series foundation models (TSFMs) achieve strong forecast accuracy, yetaccuracy alone does not determine practical value. The form of a forecast --point, quantile, parametric, or trajectory ensemble -- fundamentally constrainswhich operational tasks it can support. We survey recent TSFMs and find thattwo-thirds produce only point or parametric forecasts, while many operationaltasks require trajectory ensembles that preserve temporal dependence. Weestablish when forecast types can be converted and when they cannot: trajectoryensembles convert to simpler forms via marginalization without additionalassumptions, but the reverse requires imposing temporal dependence throughcopulas or conformal methods. We prove that marginals cannot determinepath-dependent event probabilities -- infinitely many joint distributions shareidentical marginals but yield different answers to operational questions. Wemap six fundamental forecasting tasks to minimal sufficient forecast types andprovide a task-aligned evaluation framework. Our analysis clarifies whenforecast type, not accuracy, differentiates practical utility.</description>
      <author>example@mail.com (Alvaro Perez-Diaz, James C. Loach, Danielle E. Toutoungi, Lee Middleton)</author>
      <guid isPermaLink="false">2510.19345v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Slot Filling as a Reasoning Task for SpeechLLMs</title>
      <link>http://arxiv.org/abs/2510.19326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出将推理能力整合到语音大语言模型中用于端到端槽填充任务，通过链式思维框架分解任务并创建推理数据集，实验表明混合语音LLM结合直接和推理模式表现最佳。&lt;h4&gt;背景&lt;/h4&gt;受到最近推理大语言模型发展的启发，研究者尝试将推理能力引入语音大语言模型。&lt;h4&gt;目的&lt;/h4&gt;通过链式思维框架将槽填充任务分解为多个推理步骤，创建推理数据集，并应用监督微调策略到语音大语言模型中。&lt;h4&gt;方法&lt;/h4&gt;区分常规和推理语音大语言模型，实验不同类型和大小的LLM作为文本基础模型，通过引入推理步骤展示性能改进。&lt;h4&gt;主要发现&lt;/h4&gt;引入推理步骤可提升性能；主要为数学、逻辑和编码领域开发的推理文本LLM作为基础模型时表现不佳；混合语音LLM结合直接和推理操作模式比单一模式微调的模型性能更好。&lt;h4&gt;结论&lt;/h4&gt;混合语音LLM（结合直接和推理模式）在性能上优于仅使用一种模式的模型，是更优的选择。&lt;h4&gt;翻译&lt;/h4&gt;我们提出将推理整合到语音大语言模型中用于端到端槽填充任务。受推理大语言模型最近发展的启发，我们使用链式思维框架将槽填充任务分解为多个推理步骤，创建推理数据集，并应用监督微调策略到语音大语言模型中。我们区分常规和推理语音大语言模型，并实验不同类型和大小的LLM作为它们的文本基础模型。我们通过引入推理（中间）步骤展示了性能改进。然而，我们表明主要为数学、逻辑和编码领域开发的推理文本LLM作为推理语音LLM的基础模型时可能表现不佳。我们进一步表明，构建在混合文本基础LLM上并微调以保留直接和推理操作模式的混合语音LLM，比仅使用一种操作模式微调的模型有更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose integration of reasoning into speech large language models(speechLLMs) for the end-to-end slot-filling task. Inspired by the recentdevelopment of reasoning LLMs, we use a chain-of-thought framework to decomposethe slot-filling task into multiple reasoning steps, create a reasoning datasetand apply the supervised fine-tuning strategy to a speechLLM. We distinguishbetween regular and reasoning speechLLMs and experiment with different typesand sizes of LLMs as their text foundation models. We demonstrate performanceimprovements by introducing reasoning (intermediate) steps. However, we showthat a reasoning textual LLM developed mainly for math, logic and codingdomains might be inferior as a foundation model for a reasoning speechLLM. Wefurther show that hybrid speechLLMs, built on a hybrid text foundation LLM andfine-tuned to preserve both direct and reasoning modes of operation, havebetter performance than those fine-tuned employing only one mode of operation.</description>
      <author>example@mail.com (Kadri Hacioglu, Manjunath K E, Andreas Stolcke)</author>
      <guid isPermaLink="false">2510.19326v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Balancing Rewards in Text Summarization: Multi-Objective Reinforcement Learning via HyperVolume Optimization</title>
      <link>http://arxiv.org/abs/2510.19325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为超体积优化(HVO)的新策略，用于解决大型语言模型在文本摘要任务中的多目标优化问题，通过动态调整奖励过程中的分数，使模型逐步逼近帕累托前沿，生成在多个维度上平衡的摘要。&lt;h4&gt;背景&lt;/h4&gt;文本摘要需要同时优化一致性、连贯性、相关性和流畅性等多个目标，这带来了很大挑战。虽然大型语言模型通过强化学习已展现出卓越性能，但很少有研究关注基于LLMs通过RL优化摘要的多目标问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的优化策略，用于解决基于大型语言模型的文本摘要任务中的多目标优化问题，生成在多个维度上更加平衡的摘要。&lt;h4&gt;方法&lt;/h4&gt;提出超体积优化(HVO)方法，在强化学习的奖励过程中使用超体积方法动态调整组之间的分数，引导模型优化逐步逼近帕累托前沿，从而在多个目标上生成平衡的摘要。&lt;h4&gt;主要发现&lt;/h4&gt;在多个代表性摘要数据集上的实验表明，HVO在总体得分上优于组相对策略优化(GRPO)，且在不同维度上表现更平衡。通过HVO增强的7B基础模型在摘要任务中表现与GPT-4相当，同时保持更短的生成长度。&lt;h4&gt;结论&lt;/h4&gt;HVO是一种有效的多目标优化方法，能够生成在多个维度上平衡的摘要，代码已在GitHub公开。&lt;h4&gt;翻译&lt;/h4&gt;文本摘要是一项关键任务，需要同时优化一致性、连贯性、相关性和流畅性等多个目标，这带来了相当大的挑战。尽管大型语言模型已经展示了卓越的性能，并通过强化学习得到了增强，但很少有研究关注基于LLMs通过RL优化摘要的多目标问题。在本文中，我们引入了超体积优化(HVO)，一种新颖的优化策略，通过使用超体积方法在强化学习的奖励过程中动态调整组之间的分数。这种方法引导模型的优化逐步逼近帕累托前沿，从而在多个目标上生成平衡的摘要。在几个代表性摘要数据集上的实验结果表明，我们的方法在总体得分上优于组相对策略优化(GRPO)，并在不同维度上表现出更平衡的性能。此外，通过HVO增强的7B基础模型在摘要任务中表现与GPT-4相当，同时保持更短的生成长度。我们的代码已在https://github.com/ai4business-LiAuto/HVO.git公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text summarization is a crucial task that requires the simultaneousoptimization of multiple objectives, including consistency, coherence,relevance, and fluency, which presents considerable challenges. Although largelanguage models (LLMs) have demonstrated remarkable performance, enhanced byreinforcement learning (RL), few studies have focused on optimizing themulti-objective problem of summarization through RL based on LLMs. In thispaper, we introduce hypervolume optimization (HVO), a novel optimizationstrategy that dynamically adjusts the scores between groups during the rewardprocess in RL by using the hypervolume method. This method guides the model'soptimization to progressively approximate the pareto front, thereby generatingbalanced summaries across multiple objectives. Experimental results on severalrepresentative summarization datasets demonstrate that our method outperformsgroup relative policy optimization (GRPO) in overall scores and shows morebalanced performance across different dimensions. Moreover, a 7B foundationmodel enhanced by HVO performs comparably to GPT-4 in the summarization task,while maintaining a shorter generation length. Our code is publicly availableat https://github.com/ai4business-LiAuto/HVO.git</description>
      <author>example@mail.com (Junjie Song, Yiwen Liu, Dapeng Li, Yin Sun, Shukun Fu, Siqi Chen, Yuji Cao)</author>
      <guid isPermaLink="false">2510.19325v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Advances in 4D Representation: Geometry, Motion, and Interaction</title>
      <link>http://arxiv.org/abs/2510.19255v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages. Project Page: https://mingrui-zhao.github.io/4DRep-GMI/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这是一篇关于4D生成和重建的调查论文，从4D表示的独特视角出发，帮助读者了解如何选择和定制适合自己任务的4D表示方法。&lt;h4&gt;背景&lt;/h4&gt;4D生成和重建是计算机图形学中一个快速发展的子领域，其发展受到神经场、几何和深度学习以及3D生成人工智能(GenAI)最近进展的推动。&lt;h4&gt;目的&lt;/h4&gt;帮助读者了解如何选择和定制适合自己任务的4D表示方法，以建模随时间演变的3D几何并展示运动和交互。&lt;h4&gt;方法&lt;/h4&gt;采用选择性方法，重点关注代表性工作，以突出不同计算、应用和数据场景下每种表示的理想特性和随之而来的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;将4D表示基于几何、运动和交互三个关键支柱进行分类；涵盖当前流行的表示方法如NeRFs和3DGS，以及相对未被充分探索的表示；讨论大型语言模型和视频基础模型在4D应用中的作用及其局限性；分析当前可用的4D数据集及推动领域发展所需的更多数据集。&lt;h4&gt;结论&lt;/h4&gt;选择和定制适当的4D表示对于完成特定任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了关于4D生成和重建的调查，这是计算机图形学中一个快速发展的子领域，其发展受到神经场、几何和深度学习以及3D生成人工智能(GenAI)最近进展的推动。虽然我们的调查不是首创，但我们从4D表示的独特视角构建了该领域的覆盖范围，用于建模随时间演变的3D几何并展示运动和交互。具体而言，我们没有提供大量工作的详尽列举，而是采用更选择性的方法，重点关注代表性工作，以突出不同计算、应用和数据场景下每种表示的理想特性和随之而来的挑战。我们旨在传达给读者的主要信息是如何为他们的任务选择和定制适当的4D表示。在组织上，我们基于三个关键支柱来区分4D表示：几何、运动和交互。我们的讨论不仅将涵盖当今最受欢迎的表示，如神经辐射场(NeRFs)和3D高斯溅射(3DGS)，还将引起对4D背景下相对未被充分探索的表示的关注，如结构化模型和长程运动。在整个调查中，我们将回顾大型语言模型(LLMs)和视频基础模型(VFMs)在多种4D应用中的作用，同时引导讨论它们当前的局限性以及如何解决这些局限性。我们还专门介绍了当前可用的4D数据集，以及推动该子领域发展所缺乏的数据集。项目页面：https://mingrui-zhao.github.io/4DRep-GMI/&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D表示（随时间变化的3D几何形状）的系统分类和分析问题。这个问题在现实中非常重要，因为随着计算机图形学应用扩展到电影视觉效果、虚拟现实、自主机器人、医学成像和电子商务等领域，能够捕获、表示和操作4D内容已成为连接图形学、视觉和机器学习的基本挑战。4D表示技术能够帮助我们理解和建模动态世界，为各种应用提供基础支撑。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者采用以表示为中心的独特视角，而非像之前综述那样按应用或方法分类。他们借鉴了Cao等人、Fan等人、Miao等人的工作，但认为这些综述未能充分涵盖所有相关表示方法，特别是结构化表示、运动和交互方面。作者通过三个关键支柱（几何、运动和交互）构建分析框架，并在几何部分进一步区分结构化和非结构化表示，从而提供了一个更全面、更有条理的分析视角。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 作为一篇综述论文，其核心思想是从表示角度系统分类和分析4D表示方法，帮助研究人员理解不同表示的特性、优势和局限性。整体流程分为六个部分：1）引言介绍背景和问题；2）几何建模分析非结构化表示（网格、点云、NeRF、3D高斯飞溅）和结构化表示（模板、部件、图）；3）运动建模分析不同运动类型与表示的交互；4）交互建模讨论多实体交互表示；5）数据集、评估指标和基准测试；6）整体分析和未来方向。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1）采用表示中心视角而非应用或方法分类；2）提出几何、运动、交互三支柱框架；3）明确区分结构化与非结构化表示；4）全面分析不同运动类型与表示选择的相互作用；5）专门讨论交互表示问题；6）探讨大型语言模型和视频基础模型在4D应用中的作用。相比之前工作，这篇论文提供了更全面的表示分类，更深入分析表示方法的优缺点和适用场景，并提供了如何为特定任务选择表示的实用指导。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 这篇论文通过从几何、运动和交互三个关键支柱对4D表示方法进行系统分类和分析，为研究人员提供了如何为特定4D任务选择和定制适当表示的全面指导框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a survey on 4D generation and reconstruction, a fast-evolvingsubfield of computer graphics whose developments have been propelled by recentadvances in neural fields, geometric and motion deep learning, as well 3Dgenerative artificial intelligence (GenAI). While our survey is not the firstof its kind, we build our coverage of the domain from a unique and distinctiveperspective of 4D representations\/}, to model 3D geometry evolving over timewhile exhibiting motion and interaction. Specifically, instead of offering anexhaustive enumeration of many works, we take a more selective approach byfocusing on representative works to highlight both the desirable properties andensuing challenges of each representation under different computation,application, and data scenarios. The main take-away message we aim to convey tothe readers is on how to select and then customize the appropriate 4Drepresentations for their tasks. Organizationally, we separate the 4Drepresentations based on three key pillars: geometry, motion, and interaction.Our discourse will not only encompass the most popular representations oftoday, such as neural radiance fields (NeRFs) and 3D Gaussian Splatting (3DGS),but also bring attention to relatively under-explored representations in the 4Dcontext, such as structured models and long-range motions. Throughout oursurvey, we will reprise the role of large language models (LLMs) and videofoundational models (VFMs) in a variety of 4D applications, while steering ourdiscussion towards their current limitations and how they can be addressed. Wealso provide a dedicated coverage on what 4D datasets are currently available,as well as what is lacking, in driving the subfield forward. Projectpage:https://mingrui-zhao.github.io/4DRep-GMI/</description>
      <author>example@mail.com (Mingrui Zhao, Sauradip Nag, Kai Wang, Aditya Vora, Guangda Ji, Peter Chun, Ali Mahdavi-Amiri, Hao Zhang)</author>
      <guid isPermaLink="false">2510.19255v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>TinyUSFM: Towards Compact and Efficient Ultrasound Foundation Models</title>
      <link>http://arxiv.org/abs/2510.19239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submit to JBHI, 14 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TinyUSLM是一种通过知识蒸馏技术开发的轻量级超声基础模型，能够在保持优异性能的同时显著减少计算资源需求，使其适用于资源有限的临床环境。&lt;h4&gt;背景&lt;/h4&gt;医学成像的基础模型在多样化的解剖结构和临床应用中表现出优越的泛化能力，但其出色的性能依赖于大量计算资源，限制了在资源有限的临床环境中的部署。&lt;h4&gt;目的&lt;/h4&gt;开发一种轻量级超声基础模型，能够在保持大规模超声基础模型(USFM)的优异器官多样性和任务适应性的同时，实现显著的计算效率。&lt;h4&gt;方法&lt;/h4&gt;提出特征梯度驱动的核心集选择策略筛选高质量训练数据；开发域分离的掩码图像建模辅助一致性驱动的动态蒸馏保留空间和频域特性；建立包含8个分类和10个分割数据集的UniUS-Bench超声基准。&lt;h4&gt;主要发现&lt;/h4&gt;TinyUSLM仅使用20万张图像进行蒸馏，就能以仅6.36%的参数和6.40%的GFLOPs达到与USFM相当的性能；在分类和分割任务上分别比普通模型高出9.45%和7.72%；超越了所有最先进的轻量级模型；实现了84.91%的平均分类准确率和85.78%的平均分割Dice分数。&lt;h4&gt;结论&lt;/h4&gt;TinyUSLM成功实现了轻量级超声基础模型的开发，在保持优异性能的同时显著降低了计算资源需求，使其适用于资源有限的临床环境，为医学成像领域提供了实用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;医学成像基础模型在多样化的解剖结构和临床应用中表现出优越的泛化能力。它们的出色性能依赖于大量计算资源，限制了在资源有限的临床环境中的部署。本文提出了TinyUSLM，这是第一个轻量级超声基础模型，通过使用精心筛选的小型数据集进行知识蒸馏，保持了我们的大规模超声基础模型(USFM)的卓越器官多样性和任务适应性，在不牺牲性能的情况下提供了显著的计算效率。考虑到轻量级模型的有限容量和表示能力，我们提出了一个特征梯度驱动的核心集选择策略，用于筛选高质量的紧凑训练数据，避免因低质量冗余图像导致的训练退化。为了在知识转移过程中保留基本的空间和频域特性，我们开发了域分离的掩码图像建模辅助一致性驱动的动态蒸馏。这个新颖的框架通过利用教师模型在不同域掩码上的一致性，自适应地从大型基础模型转移知识，专门针对超声解释进行定制。为了评估，我们建立了UniUS-Bench，这是最大的公开可用超声基准，包含跨15个器官的8个分类和10个分割数据集。仅使用20万张图像进行蒸馏，TinyUSLM就能以仅6.36%的参数和6.40%的GFLOPs达到USLM的性能。TinyUSLM在分类和分割任务上分别比普通模型高出9.45%和7.72%，超越了所有最先进的轻量级模型，并在各种医疗设备和中心实现了84.91%的平均分类准确率和85.78%的平均分割Dice分数。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for medical imaging demonstrate superior generalizationcapabilities across diverse anatomical structures and clinical applications.Their outstanding performance relies on substantial computational resources,limiting deployment in resource-constrained clinical environments. This paperpresents TinyUSFM, the first lightweight ultrasound foundation model thatmaintains superior organ versatility and task adaptability of our large-scaleUltrasound Foundation Model (USFM) through knowledge distillation withstrategically curated small datasets, delivering significant computationalefficiency without sacrificing performance. Considering the limited capacityand representation ability of lightweight models, we propose a feature-gradientdriven coreset selection strategy to curate high-quality compact training data,avoiding training degradation from low-quality redundant images. To preservethe essential spatial and frequency domain characteristics during knowledgetransfer, we develop domain-separated masked image modeling assistedconsistency-driven dynamic distillation. This novel framework adaptivelytransfers knowledge from large foundation models by leveraging teacher modelconsistency across different domain masks, specifically tailored for ultrasoundinterpretation. For evaluation, we establish the UniUS-Bench, the largestpublicly available ultrasound benchmark comprising 8 classification and 10segmentation datasets across 15 organs. Using only 200K images in distillation,TinyUSFM matches USFM's performance with just 6.36% of parameters and 6.40% ofGFLOPs. TinyUSFM significantly outperforms the vanilla model by 9.45% inclassification and 7.72% in segmentation, surpassing all state-of-the-artlightweight models, and achieving 84.91% average classification accuracy and85.78% average segmentation Dice score across diverse medical devices andcenters.</description>
      <author>example@mail.com (Chen Ma, Jing Jiao, Shuyu Liang, Junhu Fu, Qin Wang, Zeju Li, Yuanyuan Wang, Yi Guo)</author>
      <guid isPermaLink="false">2510.19239v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Implicit Biases of Design Choices for Time Series Foundation Models</title>
      <link>http://arxiv.org/abs/2510.19236v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过理论和实证方法分析了时间序列基础模型(TSFMs)中设计选择的影响，揭示了不同设计如何导致模型中的隐式偏置，以及这些偏置如何影响模型行为，研究结果对于理解和改进未来TSFMs的发展具有重要意义。&lt;h4&gt;背景&lt;/h4&gt;时间序列基础模型(TSFMs)是一类强大的通用工具，用于时间序列预测和相关时间任务，但这些模型的行为受到其设计中微妙归纳偏置的强烈影响。&lt;h4&gt;目的&lt;/h4&gt;理解训练过程中的各种'旋钮'如何影响模型质量，而非开发一个声称比现有TSFMs更好的新模型；探讨设计选择如何导致模型基本属性中的隐式偏置。&lt;h4&gt;方法&lt;/h4&gt;使用理论和受控经验评估相结合的方法；识别几种设计选择(如patch大小、嵌入选择、训练目标等)；研究这些设计选择如何影响模型的基本属性。&lt;h4&gt;主要发现&lt;/h4&gt;不同的设计选择会导致模型基本属性中的隐式偏置；这些偏置可能是直观的或非常违反直觉的，取决于模型和数据的特性；在异常值处理的案例研究中，展示了多种偏置如何以复杂方式相互作用；讨论了研究结果对学习'苦涩教训'和构建TSFMs的启示。&lt;h4&gt;结论&lt;/h4&gt;理解设计选择对模型行为的影响对于构建有效的TSFMs至关重要；模型中的隐式偏置可以是有益的，但也可能导致意想不到的行为。&lt;h4&gt;翻译&lt;/h4&gt;时间序列基础模型(TSFMs)是一类潜在的强大通用工具，用于时间序列预测和相关时间任务，但它们的行为受到其设计中微妙归纳偏置的强烈影响。我们不是开发一个新模型并声称它比现有的TSFMs更好，例如通过在现有成熟的基准测试中获胜，我们的目标是理解训练过程中的各种'旋钮'如何影响模型质量。结合理论和受控经验评估，我们确定了几个设计选择(补丁大小、嵌入选择、训练目标等)，并展示了它们如何导致模型基本属性中的隐式偏置(时间行为、几何结构、模型回归到均值的激进程度等)；我们展示了这些偏置如何可能是直观的或非常违反直觉的，这取决于模型和数据的特性。我们还在异常值处理的案例研究中说明了多种偏置如何以复杂方式相互作用；我们讨论了我们的结果对学习苦涩教训和构建TSFMs的启示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series foundation models (TSFMs) are a class of potentially powerful,general-purpose tools for time series forecasting and related temporal tasks,but their behavior is strongly shaped by subtle inductive biases in theirdesign. Rather than developing a new model and claiming that it is better thanexisting TSFMs, e.g., by winning on existing well-established benchmarks, ourobjective is to understand how the various ``knobs'' of the training processaffect model quality. Using a mix of theory and controlled empiricalevaluation, we identify several design choices (patch size, embedding choice,training objective, etc.) and show how they lead to implicit biases infundamental model properties (temporal behavior, geometric structure, howaggressively or not the model regresses to the mean, etc.); and we show howthese biases can be intuitive or very counterintuitive, depending on propertiesof the model and data. We also illustrate in a case study on outlier handlinghow multiple biases can interact in complex ways; and we discuss implicationsof our results for learning the bitter lesson and building TSFMs.</description>
      <author>example@mail.com (Annan Yu, Danielle C. Maddix, Boran Han, Xiyuan Zhang, Abdul Fatir Ansari, Oleksandr Shchur, Christos Faloutsos, Andrew Gordon Wilson, Michael W. Mahoney, Yuyang Wang)</author>
      <guid isPermaLink="false">2510.19236v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>PoSh: Using Scene Graphs To Guide LLMs-as-a-Judge For Detailed Image Descriptions</title>
      <link>http://arxiv.org/abs/2510.19060v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 9 figures. Metric/benchmark available at  https://github.com/amith-ananthram/posh&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PoSh的新型评估指标，用于评估视觉语言模型生成的详细图像描述，并引入了DOCENT数据集作为基准测试。PoSh使用场景图作为结构化评分指南，能够更好地模拟人类评分行为，并且在多个方面优于现有评估方法。&lt;h4&gt;背景&lt;/h4&gt;视觉语言模型(VLMs)已发展到能够生成详细的图像描述，但评估这些描述仍然面临挑战。现有标准评估指标(如CIDEr、SPICE)是为短文本设计的，主要针对现在已不常见的错误类型(如对象识别错误)进行调整，无法有效评估长文本中属性和关系的连接性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够评估详细图像描述的新指标，特别关注长文本中属性和关系的连接性，并能将错误定位到特定文本跨度。同时创建一个具有挑战性的新数据集来验证该指标的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出PoSh评估指标，利用场景图作为结构化评分指南指导大语言模型作为评判者，产生基于细粒度错误的聚合分数。同时创建DOCENT数据集，包含艺术品、专家参考描述和模型生成描述，配有艺术史学生的质量评估。通过PoSh评估开放和封闭模型在描述绘画、素描和雕像方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;PoSh在DOCENT上与人类判断的相关性比最佳开源替代方案更强，对图像类型具有鲁棒性，且作为奖励函数优于标准监督微调。研究发现基础模型难以实现对具有丰富场景动态的图像的完整、无错误覆盖，确立了评估VLM进展的新任务标准。&lt;h4&gt;结论&lt;/h4&gt;PoSh和DOCENT为评估详细图像描述提供了新工具，有望促进辅助文本生成等重要领域的进步，为视觉语言模型的发展提供更准确的评估方法。&lt;h4&gt;翻译&lt;/h4&gt;虽然视觉语言模型已经发展到能够进行详细的图像描述，但评估仍然是一个挑战。标准指标是为短文本设计的，并且调整为识别现在不常见的错误，如对象识别错误。相比之下，长文本需要对属性和关系连接的敏感性，以及将错误定位到特定文本跨度的评分。在这项工作中，我们介绍了PoSh，一种用于详细图像描述的指标，它使用场景图作为结构化评分指南来指导大语言模型作为评判者，产生基于细粒度错误的聚合分数。PoSh是可复制的、可解释的，并且比现有指标更好地模拟人类评分者的行为。为了验证PoSh，我们引入了一个具有挑战性的新数据集DOCENT。这个新的基准包含艺术品，配以专家撰写的参考文本和模型生成的描述，并附有艺术史学生对它们质量的细致和粗略判断。因此，DOCENT能够在一个具有挑战性的新领域评估详细的图像描述指标和详细的图像描述本身。我们表明，PoSh在DOCENT上与人类判断的相关性比最佳开源替代方案更强，对图像类型具有鲁棒性，并且是一个有效的奖励函数，优于标准的监督微调。然后，使用PoSh，我们描述了开放和封闭模型在描述绘画、素描和雕像方面的性能，发现基础模型难以实现对具有丰富场景动态的图像的完整、无错误的覆盖，从而确立了一个具有挑战性的新任务来衡量VLM的进展。通过PoSh和DOCENT，我们希望能够在辅助文本生成等重要领域取得进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While vision-language models (VLMs) have advanced into detailed imagedescription, evaluation remains a challenge. Standard metrics (e.g. CIDEr,SPICE) were designed for short texts and tuned to recognize errors that are nowuncommon, such as object misidentification. In contrast, long texts requiresensitivity to attribute and relation attachments and scores that localizeerrors to particular text spans. In this work, we introduce PoSh, a metric fordetailed image description that uses scene graphs as structured rubrics toguide LLMs-as-a-Judge, producing aggregate scores grounded in fine-grainederrors (e.g. mistakes in compositional understanding). PoSh is replicable,interpretable and a better proxy for human raters than existing metrics(including GPT4o-as-a-Judge). To validate PoSh, we introduce a challenging newdataset, DOCENT. This novel benchmark contains artwork, paired withexpert-written references, and model-generated descriptions, augmented withgranular and coarse judgments of their quality from art history students. Thus,DOCENT enables evaluating both detailed image description metrics and detailedimage description itself in a challenging new domain. We show that PoShachieves stronger correlations (+0.05 Spearman $\rho$) with the human judgmentsin DOCENT than the best open-weight alternatives, is robust to image type(using CapArena, an existing dataset of web imagery) and is a capable rewardfunction, outperforming standard supervised fine-tuning. Then, using PoSh, wecharacterize the performance of open and closed models in describing thepaintings, sketches and statues in DOCENT and find that foundation modelsstruggle to achieve full, error-free coverage of images with rich scenedynamics, establishing a demanding new task to gauge VLM progress. Through bothPoSh and DOCENT, we hope to enable advances in important areas such asassistive text generation.</description>
      <author>example@mail.com (Amith Ananthram, Elias Stengel-Eskin, Lorena A. Bradford, Julia Demarest, Adam Purvis, Keith Krut, Robert Stein, Rina Elster Pantalony, Mohit Bansal, Kathleen McKeown)</author>
      <guid isPermaLink="false">2510.19060v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>QKCV Attention: Enhancing Time Series Forecasting with Static Categorical Embeddings for Both Lightweight and Pre-trained Foundation Models</title>
      <link>http://arxiv.org/abs/2510.20222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为QKCV的注意力机制，通过融入静态类别嵌入来增强传统QKV框架，提高时间序列预测准确性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界的时间序列预测任务中，类别信息在捕捉固有数据模式方面起着关键作用。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效利用类别信息提高预测准确性的注意力机制。&lt;h4&gt;方法&lt;/h4&gt;引入QKCV（Query-Key-Category-Value）注意力机制，作为传统QKV框架的扩展，融入静态类别嵌入C来强调特定类别的信息。&lt;h4&gt;主要发现&lt;/h4&gt;QKCV作为即插即用模块能增强多种基于注意力的模型在现实数据集上的预测准确性；在微调单变量时间序列基础模型时，只需更新静态嵌入C，保留预训练权重，减少计算开销并提高微调性能。&lt;h4&gt;结论&lt;/h4&gt;QKCV注意力机制能有效利用类别信息提高时间序列预测的准确性，具有良好的适应性和计算效率。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的时间序列预测任务中，类别信息在捕捉固有数据模式方面起着关键作用。本文引入了QKCV（查询-键-类别-值）注意力，这是传统QKV框架的扩展，融入了静态类别嵌入C来强调特定类别的信息。作为一个通用的即插即用模块，QKCV增强了基于注意力的模型（如普通Transformer、Informer、PatchTST、TFT）在各种现实世界数据集上的预测准确性。此外，QKCV在通过仅更新静态嵌入C同时保留预训练权重来微调单变量时间序列基础模型时表现出显著的适应性，从而减少了计算开销并实现了更好的微调性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world time series forecasting tasks, category information plays apivotal role in capturing inherent data patterns. This paper introduces QKCV(Query-Key-Category-Value) attention, an extension of the traditional QKVframework that incorporates a static categorical embedding C to emphasizecategory-specific information. As a versatile plug-in module, QKCV enhances theforecasting accuracy of attention-based models (e.g., Vanilla Transformer,Informer, PatchTST, TFT) across diverse real-world datasets. Furthermore, QKCVdemonstrates remarkable adaptability in fine-tuning univariate time seriesfoundation model by solely updating the static embedding C while preservingpretrained weights, thereby reducing computational overhead and achievingsuperior fine-tuning performance.</description>
      <author>example@mail.com (Hao Wang, Baojun Ma)</author>
      <guid isPermaLink="false">2510.20222v1</guid>
      <pubDate>Fri, 24 Oct 2025 15:29:54 +0800</pubDate>
    </item>
    <item>
      <title>MobiAct: Efficient MAV Action Recognition Using MobileNetV4 with Contrastive Learning and Knowledge Distillation</title>
      <link>http://arxiv.org/abs/2510.19273v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个轻量级的MAV动作识别框架MobiAct，实现了高精度与低计算成本的平衡，在保持92.12%平均识别准确率的同时，仅消耗136.16 pJ能量并以每秒8.84个动作的速度处理，解码速度比领先方法快2倍。&lt;h4&gt;背景&lt;/h4&gt;微型飞行器(MAV)的精确高效运动识别对于自主空中群体的实时感知和协调至关重要。然而，现有方法大多依赖于大型、计算密集型模型，不适合资源有限的MAV平台，导致识别精度和推理速度之间的权衡。&lt;h4&gt;目的&lt;/h4&gt;提出一种轻量级的MAV动作识别框架MobiAct，旨在以低计算成本实现高精度的MAV动作识别。&lt;h4&gt;方法&lt;/h4&gt;采用MobileNetV4作为骨干网络；引入分阶段正交知识蒸馏(SOKD)策略将教师网络(ResNet18)的MAV运动特征有效转移到学生网络；集成无参数注意力机制提高识别精度而不增加模型复杂度；开发混合损失训练策略结合多个损失目标确保训练过程的稳定和鲁棒优化。&lt;h4&gt;主要发现&lt;/h4&gt;MobiAct实现了低能耗、低计算的MAV动作识别；在所有三个自收集数据集上，平均识别准确率达到92.12%；仅消耗136.16 pJ能量，处理速度为每秒8.84个动作；动作解码速度比领先方法快2倍，同时保持高度相当的识别精度。&lt;h4&gt;结论&lt;/h4&gt;MobiAct在MAV动作识别方面展现出卓越的效率，成功解决了识别精度与计算资源消耗之间的权衡问题。&lt;h4&gt;翻译&lt;/h4&gt;微型飞行器(MAV)运动的精确高效识别对于自主空中群体的实时感知和协调至关重要。然而，大多数现有方法依赖于大型、计算密集型模型，不适合资源有限的MAV平台，这导致了识别精度和推理速度之间的权衡。为解决这些挑战，本文提出了一个轻量级的MAV动作识别框架MobiAct，旨在以低计算成本实现高精度。具体而言，MobiAct采用MobileNetV4作为骨干网络，并引入分阶段正交知识蒸馏(SOKD)策略，将MAV运动特征从教师网络(ResNet18)有效转移到学生网络，从而提高知识转移效率。此外，架构中集成了无参数注意力机制，在不增加模型复杂度的情况下提高识别精度。此外，还开发了混合损失训练策略，结合多个损失目标，确保训练过程中的稳定和鲁棒优化。实验结果表明，所提出的MobiAct实现了低能耗、低计算的MAV动作识别，同时在比较的方法中保持最快的动作解码速度。在所有三个自收集数据集上，MobiAct平均识别准确率达到92.12%，而仅消耗136.16 pJ的能量，并以每秒8.84个动作的速度进行识别。值得注意的是，MobiAct的动作解码速度比领先方法快2倍，同时具有高度相当的识别精度，突显了其在MAV动作识别方面的卓越效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and efficient recognition of Micro Air Vehicle (MAV) motion isessential for enabling real-time perception and coordination in autonomousaerial swarm. However, most existing approaches rely on large, computationallyintensive models that are unsuitable for resource-limited MAV platforms, whichresults in a trade-off between recognition accuracy and inference speed. Toaddress these challenges, this paper proposes a lightweight MAV actionrecognition framework, MobiAct, designed to achieve high accuracy with lowcomputational cost. Specifically, MobiAct adopts MobileNetV4 as the backbonenetwork and introduces a Stage-wise Orthogonal Knowledge Distillation (SOKD)strategy to effectively transfer MAV motion features from a teacher network(ResNet18) to a student network, thereby enhancing knowledge transferefficiency. Furthermore, a parameter-free attention mechanism is integratedinto the architecture to improve recognition accuracy without increasing modelcomplexity. In addition, a hybrid loss training strategy is developed tocombine multiple loss objectives, which ensures stable and robust optimizationduring training. Experimental results demonstrate that the proposed MobiActachieves low-energy and low-computation MAV action recognition, whilemaintaining the fastest action decoding speed among compared methods. Acrossall three self-collected datasets, MobiAct achieves an average recognitionaccuracy of 92.12%, while consuming only 136.16 pJ of energy and processingrecognition at a rate of 8.84 actions per second. Notably, MobiAct decodesactions up to 2 times faster than the leading method, with highly comparablerecognition accuracy, highlighting its superior efficiency in MAV actionrecognition.</description>
      <author>example@mail.com (Zhang Nengbo, Ho Hann Woei)</author>
      <guid isPermaLink="false">2510.19273v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
  <item>
      <title>X-Ego: Acquiring Team-Level Tactical Situational Awareness via Cross-Egocentric Contrastive Video Representation Learning</title>
      <link>http://arxiv.org/abs/2510.19150v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究引入了X-Ego-CS基准数据集和交叉自我中心对比学习(CECL)方法，用于研究电竞游戏中的多智能体决策和团队战术学习。&lt;h4&gt;背景&lt;/h4&gt;人类团队战术源于个人视角及其预测、解释和适应队友意图的能力。现有视频理解研究虽改善了体育中团队互动建模，但大多依赖第三方广播视角，忽视了多智能体学习的同步、自我中心特性。&lt;h4&gt;目的&lt;/h4&gt;引入X-Ego-CS基准数据集，促进复杂3D环境中多智能体决策的研究，并提供交叉自我中心视角来捕捉团队互动。&lt;h4&gt;方法&lt;/h4&gt;X-Ego-CS数据集包含45场专业级《反恐精英2》比赛的124小时游戏录像，提供所有玩家的同步第一人称视角和状态-行动轨迹。提出CECL方法，对齐队友的自我中心视觉流，培养团队战术情境意识。&lt;h4&gt;主要发现&lt;/h4&gt;CECL能有效增强智能体从单一第一人称视图推断队友和对手位置的能力，使用最先进的视频编码器实现了有效性能。&lt;h4&gt;结论&lt;/h4&gt;X-Ego-CS和CECL为电竞中的交叉自我中心多智能体基准测试奠定基础，将游戏理解定位为多智能体建模和战术学习的测试平台，对虚拟和现实领域中的时空推理和人类-AI团队协作具有启示意义。&lt;h4&gt;翻译&lt;/h4&gt;人类团队战术源于每个球员的个人视角及其预测、解释和适应队友意图的能力。尽管视频理解方面的进展已改善了体育中团队互动的建模，但大多数现有工作依赖第三方广播视角，并忽视了多智能体学习的同步、自我中心特性。我们引入X-Ego-CS基准数据集，包含来自45场专业级流行电竞游戏《反恐精英2》的124小时游戏录像，旨在促进复杂3D环境中多智能体决策的研究。X-Ego-CS提供交叉自我中心视频流，同步捕捉所有玩家的第一人称视角以及状态-行动轨迹。基于此资源，我们提出交叉自我中心对比学习(CECL)，对齐队友的自我中心视觉流，从个人视角培养团队层面的战术情境意识。我们在队友-对手位置预测任务上评估CECL，证明了其有效性，能够增强智能体使用最先进的视频编码器从单一第一人称视图推断队友和对手位置的能力。X-Ego-CS和CECL共同为电竞中的交叉自我中心多智能体基准测试奠定基础。更广泛地说，我们的工作将游戏理解定位为多智能体建模和战术学习的测试平台，对虚拟和现实领域中的时空推理以及人类-AI团队协作具有启示意义。代码和数据集可在https://github.com/HATS-ICT/x-ego获取。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让AI系统从团队成员的第一人称视角中获得团队层面的战术态势感知能力。这个问题在现实中很重要，因为真实世界的团队协作（如体育竞技、军事行动、应急响应等）需要参与者能够根据队友和对手的意图来协调行动，而现有方法大多依赖第三人称视角，无法捕捉个体感知和协调的第一人称特性，限制了智能体在部分可观测环境中的表现。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有团队行为建模方法的局限性，特别是在处理部分可观测环境中的团队协调时。他们借鉴了体育理解中的第三人称分析方法、游戏理解中的人-AI协作研究以及对比学习在计算机视觉和多智能体学习中的应用。作者选择使用第一人称射击游戏（反恐精英）作为研究平台，因为它提供了丰富的游戏状态和决策复杂性。基于此，他们创建了X-Ego-CS数据集并设计了跨自我中心对比学习（CECL）方法，通过对比学习对齐队友的第一人称视觉表征。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是通过对比学习对齐队友的第一人称视觉表征，使模型能够从有限的第一人称视角中推断团队层面的战术态势。整体流程包括：1) 收集和处理专业级反恐精英比赛数据，提取第一人称视频流和状态-动作轨迹；2) 使用时空视频编码器处理每个玩家的视角；3) 应用对比学习目标函数，使同一时间点的队友视角产生相似表征；4) 设计下游任务（队友和对手位置预测）来评估模型性能；5) 结合对比损失和分类损失进行训练，使模型能够从单个视角推断团队态势。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) X-Ego-CS数据集：首个包含同步第一人称视频流和结构化状态-动作轨迹的专业电子竞技数据集；2) 跨自我中心对比学习（CECL）方法：通过对比学习对齐队友视角，实现团队态势感知；3) 队友-对手位置预测任务：为评估团队理解能力提供标准化基准。相比之前工作，本文方法使用同步第一人称视角而非第三人称视角，提供完整的第一人称视频流和精确轨迹数据，并通过对比学习模拟人类心智理论能力，在复杂3D环境中验证而非简化环境。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; 本文贡献了一个包含同步第一人称视频流的专业电子竞技数据集和一个通过对比学习对齐队友视角的方法，使AI系统能够从有限的第一人称视角中获取团队层面的战术态势感知能力，为多智能体系统中的团队协作研究建立了新的基准。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human team tactics emerge from each player's individual perspective and theirability to anticipate, interpret, and adapt to teammates' intentions. Whileadvances in video understanding have improved the modeling of team interactionsin sports, most existing work relies on third-person broadcast views andoverlooks the synchronous, egocentric nature of multi-agent learning. Weintroduce X-Ego-CS, a benchmark dataset consisting of 124 hours of gameplayfootage from 45 professional-level matches of the popular e-sports gameCounter-Strike 2, designed to facilitate research on multi-agentdecision-making in complex 3D environments. X-Ego-CS provides cross-egocentricvideo streams that synchronously capture all players' first-person perspectivesalong with state-action trajectories. Building on this resource, we proposeCross-Ego Contrastive Learning (CECL), which aligns teammates' egocentricvisual streams to foster team-level tactical situational awareness from anindividual's perspective. We evaluate CECL on a teammate-opponent locationprediction task, demonstrating its effectiveness in enhancing an agent'sability to infer both teammate and opponent positions from a singlefirst-person view using state-of-the-art video encoders. Together, X-Ego-CS andCECL establish a foundation for cross-egocentric multi-agent benchmarking inesports. More broadly, our work positions gameplay understanding as a testbedfor multi-agent modeling and tactical learning, with implications forspatiotemporal reasoning and human-AI teaming in both virtual and real-worlddomains. Code and dataset are available at https://github.com/HATS-ICT/x-ego.</description>
      <author>example@mail.com (Yunzhe Wang, Soham Hans, Volkan Ustun)</author>
      <guid isPermaLink="false">2510.19150v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>UniHPR: Unified Human Pose Representation via Singular Value Contrastive Learning</title>
      <link>http://arxiv.org/abs/2510.19078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为UniHPR的统一人体姿态表示学习管道，通过创新的基于奇异值的对比学习损失函数，实现了图像、2D和3D人体姿态表示的有效对齐，并在人体姿态估计和检索任务中取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，开发有效的对齐管道以从不同模态生成统一表示受到越来越多的关注。人体姿态表示作为以人为中心应用的关键组成部分，在人体姿态估计、动作识别、人机交互、目标跟踪等下游任务中至关重要。然而，目前很少有研究使用对比范式清晰研究多种人体姿态表示之间的相关性。&lt;h4&gt;目的&lt;/h4&gt;提出UniHPR，一个统一的人体姿态表示学习管道，用于对齐来自图像、2D和3D人体姿态的人体姿态嵌入。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的基于奇异值的对比学习损失函数，用于同时对齐超过两种数据表示，更好地对齐不同模态并进一步提高性能。选择2D和3D人体姿态估计作为评估任务，以验证对齐表示的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE性能指标，在3DPW数据集上实现了跨域评估的51.6mm PA-MPJPE性能指标。此外，在Human3.6M数据集上，使用统一的人体姿态表示实现了2D和3D姿态检索，检索误差为9.24mm的MPJPE。&lt;h4&gt;结论&lt;/h4&gt;UniHPR能够有效对齐不同模态的人体姿态表示，并在多种下游任务中展现出优异的性能，为多模态人体姿态表示的学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;近年来，人们越来越关注开发有效的对齐管道，从不同模态生成统一表示，用于多模态融合和生成。作为以人为中心应用的重要组成部分，人体姿态表示在许多下游任务中至关重要，如人体姿态估计、动作识别、人机交互、目标跟踪等。人体姿态表示或嵌入可以从图像、2D关键点、3D骨架、网格模型等多种模态中提取。然而，使用对比范式清晰研究所有这些表示之间相关性的实例有限。在本文中，我们提出UniHPR，一个统一的人体姿态表示学习管道，用于对齐来自图像、2D和3D人体姿态的人体姿态嵌入。为了同时对齐超过两种数据表示，我们提出了一种新颖的基于奇异值的对比学习损失函数，更好地对齐不同模态并进一步提高性能。为了评估对齐表示的有效性，我们选择2D和3D人体姿态估计(HPE)作为评估任务。在我们的评估中，使用简单的3D人体姿态解码器，UniHPR在Human3.6M数据集上实现了49.9mm的MPJPE性能指标，在3DPW数据集上实现了跨域评估的51.6mm的PA-MPJPE性能指标。同时，我们能够在Human3.6M数据集上使用统一的人体姿态表示实现2D和3D姿态检索，检索误差为9.24mm的MPJPE。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been a growing interest in developing effectivealignment pipelines to generate unified representations from differentmodalities for multi-modal fusion and generation. As an important component ofHuman-Centric applications, Human Pose representations are critical in manydownstream tasks, such as Human Pose Estimation, Action Recognition,Human-Computer Interaction, Object tracking, etc. Human Pose representations orembeddings can be extracted from images, 2D keypoints, 3D skeletons, meshmodels, and lots of other modalities. Yet, there are limited instances wherethe correlation among all of those representations has been clearly researchedusing a contrastive paradigm. In this paper, we propose UniHPR, a unified HumanPose Representation learning pipeline, which aligns Human Pose embeddings fromimages, 2D and 3D human poses. To align more than two data representations atthe same time, we propose a novel singular value-based contrastive learningloss, which better aligns different modalities and further boosts performance.To evaluate the effectiveness of the aligned representation, we choose 2D and3D Human Pose Estimation (HPE) as our evaluation tasks. In our evaluation, witha simple 3D human pose decoder, UniHPR achieves remarkable performance metrics:MPJPE 49.9mm on the Human3.6M dataset and PA-MPJPE 51.6mm on the 3DPW datasetwith cross-domain evaluation. Meanwhile, we are able to achieve 2D and 3D poseretrieval with our unified human pose representations in Human3.6M dataset,where the retrieval error is 9.24mm in MPJPE.</description>
      <author>example@mail.com (Zhongyu Jiang, Wenhao Chai, Lei Li, Zhuoran Zhou, Cheng-Yen Yang, Jenq-Neng Hwang)</author>
      <guid isPermaLink="false">2510.19078v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>ProCLIP: Progressive Vision-Language Alignment via LLM-based Embedder</title>
      <link>http://arxiv.org/abs/2510.18795v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 5 fiugres&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出ProCLIP框架，解决CLIP文本编码器在处理长文本和多语言输入方面的局限性，通过课程学习实现CLIP图像编码器与LLM嵌入器的有效对齐。&lt;h4&gt;背景&lt;/h4&gt;原始CLIP文本编码器受限于77个token的最大输入长度，不支持多语言输入，这些限制显著阻碍了其在更广泛任务中的应用。虽然近期研究尝试用基于LLM的嵌入器替代CLIP文本编码器，但由于LLM和CLIP的表示空间独立预训练且缺乏先验对齐，直接使用对比学习会破坏CLIP图像编码器中固有的视觉-语言对齐。&lt;h4&gt;目的&lt;/h4&gt;开发一种方法来有效对齐CLIP图像编码器与基于LLM的嵌入器，同时保留CLIP的预训练知识，从而增强模型在处理长文本、多语言理解和细粒度语义理解方面的能力。&lt;h4&gt;方法&lt;/h4&gt;ProCLIP采用课程学习的渐进式视觉-语言对齐框架：首先从CLIP文本编码器中蒸馏知识到LLM嵌入器建立初始对齐；然后通过图像-文本对比微调进一步对齐，并使用自蒸馏正则化避免过拟合；在表示继承和对比微调过程中采用实例语义对齐损失和嵌入结构对齐损失以实现更有效的对齐。&lt;h4&gt;主要发现&lt;/h4&gt;直接对齐LLM和CLIP的表示空间会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识利用不足；而ProCLIP框架能够有效对齐两者并保留CLIP的预训练知识。&lt;h4&gt;结论&lt;/h4&gt;ProCLIP通过课程学习的渐进式对齐方法，解决了LLM嵌入器和CLIP图像编码器之间的对齐问题，同时保留了CLIP的预训练知识，显著提升了模型在长文本处理、多语言理解和细粒度语义理解方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;原始的CLIP文本编码器受限于最大77个token的输入长度，这妨碍了它有效处理长文本和进行细粒度语义理解的能力。此外，CLIP文本编码器不支持多语言输入。所有这些限制显著限制了它在更广泛任务中的应用性。最近的研究尝试用基于LLM的嵌入器替换CLIP文本编码器，以增强其处理长文本、多语言理解和细粒度语义理解的能力。然而，由于LLM的表示空间和CLIP的视觉-语言空间是独立预训练且没有对齐先验，直接使用对比学习对齐会破坏CLIP图像编码器中固有的视觉-语言对齐，导致预训练知识利用不足。为解决这一挑战，我们提出ProCLIP，一种基于课程学习的渐进式视觉-语言对齐框架，以有效对齐CLIP图像编码器和基于LLM的嵌入器。具体而言，ProCLIP首先从CLIP的文本编码器中蒸馏知识到基于LLM的嵌入器，利用CLIP丰富的预训练知识，同时建立LLM嵌入器和CLIP图像编码器之间的初始对齐。随后，ProCLIP通过图像-文本对比微调进一步对齐CLIP图像编码器和基于LLM的嵌入器，采用自蒸馏正则化来避免过拟合。为了实现更有效的对齐，在表示继承和对比微调过程中采用了实例语义对齐损失和嵌入结构对齐损失。代码可在https://github.com/VisionXLab/ProCLIP获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The original CLIP text encoder is limited by a maximum input length of 77tokens, which hampers its ability to effectively process long texts and performfine-grained semantic understanding. In addition, the CLIP text encoder lackssupport for multilingual inputs. All these limitations significantly restrictits applicability across a broader range of tasks. Recent studies haveattempted to replace the CLIP text encoder with an LLM-based embedder toenhance its ability in processing long texts, multilingual understanding, andfine-grained semantic comprehension. However, because the representation spacesof LLMs and the vision-language space of CLIP are pretrained independentlywithout alignment priors, direct alignment using contrastive learning candisrupt the intrinsic vision-language alignment in the CLIP image encoder,leading to an underutilization of the knowledge acquired during pre-training.To address this challenge, we propose ProCLIP, a curriculum learning-basedprogressive vision-language alignment framework to effectively align the CLIPimage encoder with an LLM-based embedder. Specifically, ProCLIP first distillsknowledge from CLIP's text encoder into the LLM-based embedder to leverageCLIP's rich pretrained knowledge while establishing initial alignment betweenthe LLM embedder and CLIP image encoder. Subsequently, ProCLIP further alignsthe CLIP image encoder with the LLM-based embedder through image-textcontrastive tuning, employing self-distillation regularization to avoidoverfitting. To achieve a more effective alignment, instance semantic alignmentloss and embedding structure alignment loss are employed during representationinheritance and contrastive tuning. The Code is available athttps://github.com/VisionXLab/ProCLIP.</description>
      <author>example@mail.com (Xiaoxing Hu, Kaicheng Yang, Ziyang Gong, Qi Ming, Zonghao Guo, Xiang An, Ziyong Feng, Junchi Yan, Xue Yang)</author>
      <guid isPermaLink="false">2510.18795v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Decomposed Attention Fusion in MLLMs for Training-Free Video Reasoning Segmentation</title>
      <link>http://arxiv.org/abs/2510.19592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://www.jshyun.me/projects/decaf&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为DecAF的分解注意力融合方法，用于在无需重新训练多模态大语言模型(MLLMs)的情况下实现视频理解与定位，通过改进注意力图实现了与需要训练的方法相当的性能。&lt;h4&gt;背景&lt;/h4&gt;多模态大语言模型(MLLMs)能够通过关注与文本查询相关的视觉标记来展示强大的视频理解能力，但直接将其应用于定位任务存在挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练的方法，将MLLMs的视频理解能力直接适应于视频推理分割任务。&lt;h4&gt;方法&lt;/h4&gt;将视频推理分割视为视频问答任务并通过展开机制提取注意力图；提出DecAF方法，通过对比对象-背景融合和互补视频帧融合两种机制改进原始注意力图；引入注意力引导的SAM2提示获取精细掩码。&lt;h4&gt;主要发现&lt;/h4&gt;DecAF能够抑制不相关的激活并增强对象聚焦的线索，使注意力图可以直接转换为粗略分割掩码；无需训练的方法实现了与需要训练的方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;DecAF优于现有的无需训练的方法，并在指代和推理VOS基准测试上达到了与基于训练方法相当的性能；与现有的将MLLMs与SAM联合训练的方法不同，DecAF完全无需重新训练。&lt;h4&gt;翻译&lt;/h4&gt;多模态大语言模型(MLLMs)通过关注与文本查询相关的视觉标记展示了强大的视频理解能力。为了直接以无需训练的方式将其适应于定位任务，我们将视频推理分割视为视频问答任务，并通过展开机制提取注意力图。然而，原始注意力图嘈杂且与对象区域对齐不良。我们提出了分解注意力融合(DecAF)，通过两种机制改进这些图：(1)对比对象-背景融合和(2)互补视频帧融合。此方法抑制了不相关的激活并增强了对象聚焦的线索，使注意力图可以直接转换为粗略分割掩码。此外，我们引入了注意力引导的SAM2提示来获取精细掩码。与现有的将MLLMs与SAM联合训练的方法不同，我们的方法完全无需重新训练。DecAF优于无需训练的方法，并在指代和推理VOS基准测试上实现了与基于训练方法相当的性能。代码将在https://github.com/HYUNJS/DecAF上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal large language models (MLLMs) demonstrate strong videounderstanding by attending to visual tokens relevant to textual queries. Todirectly adapt this for localization in a training-free manner, we cast videoreasoning segmentation as a video QA task and extract attention maps viarollout mechanism. However, raw attention maps are noisy and poorly alignedwith object regions. We propose Decomposed Attention Fusion (DecAF), whichrefines these maps through two mechanisms: (1) contrastive object-backgroundfusion and (2) complementary video-frame fusion. This method suppressesirrelevant activations and enhances object-focused cues, enabling directconversion of attention maps into coarse segmentation masks. In addition, weintroduce attention-guided SAM2 prompting for obtaining fine-grained masks.Unlike existing methods that jointly train MLLMs with SAM, our method operatesentirely without retraining. DecAF outperforms training-free methods andachieves performance comparable to training-based methods on both referring andreasoning VOS benchmarks. The code will be available athttps://github.com/HYUNJS/DecAF.</description>
      <author>example@mail.com (Su Ho Han, Jeongseok Hyun, Pilhyeon Lee, Minho Shim, Dongyoon Wee, Seon Joo Kim)</author>
      <guid isPermaLink="false">2510.19592v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>A Matter of Time: Revealing the Structure of Time in Vision-Language Models</title>
      <link>http://arxiv.org/abs/2510.19559v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了大规模视觉语言模型的时间感知能力，提出了TIME10k基准数据集，并发现时间信息在VLM嵌入空间中沿低维非线性流形结构化，基于此提出了时间线表示方法，该方法在计算效率高的同时实现了优异的时间推理性能。&lt;h4&gt;背景&lt;/h4&gt;大规模视觉语言模型如CLIP因其可泛化和表达性的多模态表示而受到欢迎。这些模型通过利用具有多样化文本元数据的大规模训练数据，获得了开放词汇能力，能够解决超出其训练范围的任务。&lt;h4&gt;目的&lt;/h4&gt;研究视觉语言模型的时间感知能力，评估它们将视觉内容定位在时间中的能力。&lt;h4&gt;方法&lt;/h4&gt;引入TIME10k基准数据集（包含超过10,000张图像的时间基准数据），通过一种新方法评估37个VLMs的时间感知能力，并基于发现提出从嵌入空间推导显式'时间线'表示的方法。&lt;h4&gt;主要发现&lt;/h4&gt;时间信息在VLM嵌入空间中沿着低维、非线性的流形结构化，基于此可以推导出显式的'时间线'表示。&lt;h4&gt;结论&lt;/h4&gt;提出的时间线表示方法能够模拟时间及其时间进展，促进时间推理任务，在计算效率高的同时，实现了与基于提示的基线相当或更优的准确性。&lt;h4&gt;翻译&lt;/h4&gt;大规模视觉语言模型如CLIP因其可泛化和表达性的多模态表示而受到欢迎。通过利用具有多样化文本元数据的大规模训练数据，VLMs获得了开放词汇能力，能够解决超出其训练范围的任务。本文研究了VLMs的时间感知能力，评估它们将视觉内容定位在时间中的能力。我们引入了TIME10k，一个包含超过10,000张图像的时间基准数据集，并通过一种新方法评估了37个VLMs的时间感知能力。我们的研究揭示，时间信息在VLM嵌入空间中沿着低维、非线性的流形结构化。基于这一见解，我们提出了从嵌入空间推导显式'时间线'表示的方法。这些表示模拟时间及其时间进展，从而促进时间推理任务。我们的时间线方法在计算效率高的同时，实现了与基于提示的基线相当或更优的准确性。所有代码和数据都在https://tekayanidham.github.io/timeline-page/上提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3746027.3758163&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large-scale vision-language models (VLMs) such as CLIP have gained popularityfor their generalizable and expressive multimodal representations. Byleveraging large-scale training data with diverse textual metadata, VLMsacquire open-vocabulary capabilities, solving tasks beyond their trainingscope. This paper investigates the temporal awareness of VLMs, assessing theirability to position visual content in time. We introduce TIME10k, a benchmarkdataset of over 10,000 images with temporal ground truth, and evaluate thetime-awareness of 37 VLMs by a novel methodology. Our investigation revealsthat temporal information is structured along a low-dimensional, non-linearmanifold in the VLM embedding space. Based on this insight, we propose methodsto derive an explicit ``timeline'' representation from the embedding space.These representations model time and its chronological progression and therebyfacilitate temporal reasoning tasks. Our timeline approaches achievecompetitive to superior accuracy compared to a prompt-based baseline whilebeing computationally efficient. All code and data are available athttps://tekayanidham.github.io/timeline-page/.</description>
      <author>example@mail.com (Nidham Tekaya, Manuela Waldner, Matthias Zeppelzauer)</author>
      <guid isPermaLink="false">2510.19559v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>PRGCN: A Graph Memory Network for Cross-Sequence Pattern Reuse in 3D Human Pose Estimation</title>
      <link>http://arxiv.org/abs/2510.19475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  29 pages, 6 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PRGCN的新型框架，通过跨序列模式检索和适应来解决单目3D人体姿态估计中的深度模糊性问题。该方法利用图记忆库存储姿态原型，并通过注意力机制动态检索，结合双流混合架构实现了最先进的性能和跨域泛化能力。&lt;h4&gt;背景&lt;/h4&gt;单目3D人体姿态估计是一个不适定的逆问题，因为从2D到3D的提升中存在固有的深度模糊性。现有基于视频的方法虽然利用时间上下文增强空间推理，但独立处理每个序列，未能充分利用跨序列中人类运动的强结构规律性和重复运动模式。&lt;h4&gt;目的&lt;/h4&gt;解决单目3D人体姿态估计中的深度模糊性问题，突破现有方法仅独立处理每个序列的局限，通过跨序列模式重用机制提升姿态估计的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出PRGCN框架，将姿态估计形式化为模式检索和适应问题；引入图记忆库学习和存储姿态原型；通过注意力机制动态检索提供结构化先验；通过内存驱动的图卷积将先验与解剖约束融合；设计双流混合架构，结合Mamba的局部时间建模和自注意力的全局关系能力。&lt;h4&gt;主要发现&lt;/h4&gt;在Human3.6M和MPI-INF-3DHP基准测试上，PRGCN实现了37.1mm和13.4mm的MPJPE，建立了新的最先进水平，同时表现出增强的跨域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;跨序列模式重用机制对推进人体姿态估计领域至关重要，将研究范式从每序列优化转向累积知识学习。&lt;h4&gt;翻译&lt;/h4&gt;单目3D人体姿态估计由于2D到3D提升中的固有深度模糊性，仍然是一个根本性的不适定逆问题。虽然当代基于视频的方法利用时间上下文来增强空间推理，但它们在关键范式限制下运行：独立处理每个序列，因此未能充分利用跨序列中普遍存在的强结构规律性和重复运动模式。这项工作引入了模式重用图卷积网络，一个将姿态估计形式化为模式检索和适应问题的新型框架。其核心是，PRGCN具有一个图记忆库，学习和存储一组紧凑的姿态原型，编码为关系图，这些原型通过注意力机制动态检索以提供结构化先验。这些先验通过内存驱动的图卷积与硬编码的解剖约束自适应融合，确保几何合理性。为了用鲁棒的空间-时间特征支持这一检索过程，我们设计了一个双流混合架构，协同结合了基于Mamba的状态空间模型的线性复杂度局部时间建模与自注意力的全局关系能力。在Human3.6M和MPI-INF-3DHP基准测试上的广泛评估表明，PRGCN建立了新的最先进水平，分别实现了37.1mm和13.4mm的MPJPE，同时表现出增强的跨域泛化能力。我们的研究认为，长期以来被忽视的跨序列模式重用机制对推进该领域至关重要，将范式从每序列优化转向累积知识学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular 3D human pose estimation remains a fundamentally ill-posed inverseproblem due to the inherent depth ambiguity in 2D-to-3D lifting. Whilecontemporary video-based methods leverage temporal context to enhance spatialreasoning, they operate under a critical paradigm limitation: processing eachsequence in isolation, thereby failing to exploit the strong structuralregularities and repetitive motion patterns that pervade human movement acrosssequences. This work introduces the Pattern Reuse Graph Convolutional Network(PRGCN), a novel framework that formalizes pose estimation as a problem ofpattern retrieval and adaptation. At its core, PRGCN features a graph memorybank that learns and stores a compact set of pose prototypes, encoded asrelational graphs, which are dynamically retrieved via an attention mechanismto provide structured priors. These priors are adaptively fused with hard-codedanatomical constraints through a memory-driven graph convolution, ensuringgeometrical plausibility. To underpin this retrieval process with robustspatiotemporal features, we design a dual-stream hybrid architecture thatsynergistically combines the linear-complexity, local temporal modeling ofMamba-based state-space models with the global relational capacity ofself-attention. Extensive evaluations on Human3.6M and MPI-INF-3DHP benchmarksdemonstrate that PRGCN establishes a new state-of-the-art, achieving an MPJPEof 37.1mm and 13.4mm, respectively, while exhibiting enhanced cross-domaingeneralization capability. Our work posits that the long-overlooked mechanismof cross-sequence pattern reuse is pivotal to advancing the field, shifting theparadigm from per-sequence optimization towards cumulative knowledge learning.</description>
      <author>example@mail.com (Zhuoyang Xie, Yibo Zhao, Hui Huang, Riwei Wang, Zan Gao)</author>
      <guid isPermaLink="false">2510.19475v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>$Δ$t-Mamba3D: A Time-Aware Spatio-Temporal State-Space Model for Breast Cancer Risk Prediction</title>
      <link>http://arxiv.org/abs/2510.19003v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究提出了Time-Aware Δt-Mamba3D，一种新型的状态空间架构，专门用于纵向医学图像分析。该模型能够有效编码不规则访问间隔和丰富的时空上下文，同时保持计算效率，在乳腺癌风险预测任务上表现出色。&lt;h4&gt;背景&lt;/h4&gt;纵向放射图像分析面临一个基本数据挑战：如何有效建模在非规则时间间隔采集的高分辨率图像序列。这种数据结构包含重要的空间和时间线索，但当前方法无法充分利用。现有方法通常要么将空间信息压缩为向量，要么使用计算效率低下且与非均匀时间步不兼容的时空模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效处理不规则时间间隔采集的图像序列的模型，同时充分利用空间和时间信息，并保持计算效率，应用于纵向医学图像分析，特别是乳腺癌风险预测。&lt;h4&gt;方法&lt;/h4&gt;研究者提出了Time-Aware Δt-Mamba3D，一种专为纵向医学成像设计的新的状态空间架构。该模型的核心创新是一个连续时间选择性扫描机制，明确地将检查之间的真实时间差异整合到状态转换中。此外，还采用了多尺度3D邻域融合模块，稳健地捕获时空关系。&lt;h4&gt;主要发现&lt;/h4&gt;在乳腺癌风险预测基准测试中，该模型表现出色，验证c-index提高了2-5个百分点，相比现有的循环、变压器和状态空间模型的变体，实现了更高的1-5年AUC分数。由于具有线性复杂度，该模型能够高效处理长期复杂的患者筛查历史。&lt;h4&gt;结论&lt;/h4&gt;Time-Aware Δt-Mamba3D为纵向图像分析形成了一个新框架，能够有效处理不规则时间间隔采集的图像序列，充分利用时空信息，同时保持计算效率，在医学图像分析任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;纵向连续放射图像分析受到一个基本数据挑战的阻碍：如何有效建模在非规则时间间隔采集的高分辨率图像序列。这种数据结构包含了当前方法无法充分利用的必不可少的空间和时间线索。模型通常要么将空间信息压缩为向量，要么使用计算效率低下且与非均匀时间步不兼容的时空模型。我们通过Time-Aware Δt-Mamba3D解决了这一挑战，这是一种专为纵向医学成像设计的新型状态空间架构。我们的模型同时编码不规则访问间隔和丰富的时空上下文，同时保持计算效率。其核心创新是一个连续时间选择性扫描机制，明确地将检查之间的真实时间差异整合到其状态转换中。这辅以一个多尺度3D邻域融合模块，稳健地捕获时空关系。在使用连续筛查乳腺X光检查的乳腺癌风险预测综合基准中，我们的模型表现出卓越性能，相比现有的循环、变压器和状态空间模型的变体，将验证c-index提高了2-5个百分点，并实现了更高的1-5年AUC分数。由于其线性复杂度，该模型能够高效处理长期复杂的患者筛查历史，为纵向图像分析形成了一个新框架。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何有效建模在不同时间间隔捕获的高分辨率图像序列的问题，特别是在乳腺癌筛查中不规则时间间隔的纵向放射学图像分析。这个问题很重要，因为乳腺癌是全球女性最常见的癌症之一，早期风险预测可以提高筛查效率；现有方法未能充分利用不规则时间间隔这一重要预测因素；医生评估风险时会考虑多次检查的比较，而大多数深度学习系统仍只处理单次检查；开发能够处理不规则时间间隔的高效模型对临床应用至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先认识到乳腺癌筛查是纵向的，患者会定期返回检查，乳房随年龄变化，病变可能逐渐显现。他们指出大多数深度学习系统忽略了时序背景，现有方法要么将空间信息压缩为向量，要么使用计算效率低下的模型，或者无法处理非均匀时间步长。作者分析了各种处理不规则时间序列的方法，发现它们都有局限性，而状态空间模型如Mamba虽能捕获长期依赖关系，但尚未显式编码不规则时间间隔。作者借鉴了Mamba的状态空间架构、3D卷积网络处理空间信息的思想、时间感知模型处理时间间隔的方法，以及视频视觉 transformers处理时空信息的思路，但都进行了改进以适应不规则时间间隔的医学成像数据。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是：1) 时间感知选择性扫描：将真实时间差Δt直接注入选择性扫描中，使模型能根据实际时间间隔调整状态更新；2) 多尺度3D邻域融合：使用深度3D卷积捕获空间和时间依赖关系，同时保持计算效率；3) 线性复杂度：能高效处理长期复杂的患者筛查历史。整体流程：1) 输入处理：每个患者的纵向成像序列，使用Swin-V2处理每个图像并融合特征；2) Δt-Mamba3D块处理：将特征展平为标记序列，运行Mamba选择性扫描，状态更新由真实Δt调制，然后重塑回3D格式并应用3D邻域融合；3) 患者嵌入和风险模块：跨空间和时间聚合特征获得患者嵌入，使用加性危害模型估计未来乳腺癌风险；4) 处理可变长度序列：左填充序列到固定长度，使用掩码处理填充标记。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点：1) 时间感知选择性扫描机制：将真实时间差Δt注入选择性扫描，实现连续时间记忆衰减或累积；2) 多尺度深度3D融合模块：使用深度3D卷积捕获空间和时间依赖关系；3) 线性复杂度设计：能高效处理长期复杂历史。相比之前工作的不同：1) 与时间感知模型(如GRU-D)相比，使用连续时间状态空间模型，能更好地建模观测间的演变风险；2) 与连续时间模型(如Neural ODEs)相比，专为高维医学成像数据设计，处理长时间间隔；3) 与视频视觉transformers相比，明确编码不规则时间间隔，计算效率更高；4) 与视觉状态空间模型相比，引入真实时间间隔信息，结合3D邻域融合；5) 与标准Mamba相比，显式编码真实时间间隔，添加3D邻域融合。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; Time-Aware Δt-Mamba3D通过将真实时间间隔和多尺度3D空间-时间信息整合到高效的状态空间模型中，显著提高了乳腺癌风险预测的准确性，同时保持了线性计算复杂度，为纵向医学图像分析提供了新框架。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Longitudinal analysis of sequential radiological images is hampered by afundamental data challenge: how to effectively model a sequence ofhigh-resolution images captured at irregular time intervals. This datastructure contains indispensable spatial and temporal cues that current methodsfail to fully exploit. Models often compromise by either collapsing spatialinformation into vectors or applying spatio-temporal models that arecomputationally inefficient and incompatible with non-uniform time steps. Weaddress this challenge with Time-Aware $\Delta$t-Mamba3D, a novel state-spacearchitecture adapted for longitudinal medical imaging. Our model simultaneouslyencodes irregular inter-visit intervals and rich spatio-temporal context whileremaining computationally efficient. Its core innovation is a continuous-timeselective scanning mechanism that explicitly integrates the true timedifference between exams into its state transitions. This is complemented by amulti-scale 3D neighborhood fusion module that robustly capturesspatio-temporal relationships. In a comprehensive breast cancer risk predictionbenchmark using sequential screening mammogram exams, our model shows superiorperformance, improving the validation c-index by 2-5 percentage points andachieving higher 1-5 year AUC scores compared to established variants ofrecurrent, transformer, and state-space models. Thanks to its linearcomplexity, the model can efficiently process long and complex patientscreening histories of mammograms, forming a new framework for longitudinalimage analysis.</description>
      <author>example@mail.com (Zhengbo Zhou, Dooman Arefan, Margarita Zuley, Shandong Wu)</author>
      <guid isPermaLink="false">2510.19003v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>SFGFusion: Surface Fitting Guided 3D Object Detection with 4D Radar and Camera Fusion</title>
      <link>http://arxiv.org/abs/2510.19215v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Pattern Recognition&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SFGFusion的新型相机-4D成像雷达检测网络，通过表面拟合引导来解决3D物体检测中的多模态融合问题。&lt;h4&gt;背景&lt;/h4&gt;3D物体检测对自动驾驶至关重要，4D成像雷达作为一种新兴传感器具有低成本、长距离检测和精确速度测量的优势，但其稀疏点云和低分辨率限制了物体的几何表示和跨模态融合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效融合相机和4D成像雷达数据的方法，提高3D物体检测的准确性和可靠性。&lt;h4&gt;方法&lt;/h4&gt;SFGFusion通过估计物体的二次曲面参数增强空间表示和跨模态交互，预测细粒度密集深度用于图像特征转换和伪点云生成，采用基于支柱的方法处理雷达点云，并在BEV空间中进行特征融合和检测。&lt;h4&gt;主要发现&lt;/h4&gt;SFGFusion有效融合了相机和4D雷达特征，在TJ4DRadSet和view-of-delft(VoD)物体检测基准上取得了优越性能。&lt;h4&gt;结论&lt;/h4&gt;基于表面拟合的SFGFusion网络能够有效解决4D成像雷达的稀疏性问题，提升多模态融合效果，提高3D物体检测性能。&lt;h4&gt;翻译&lt;/h4&gt;3D物体检测对自动驾驶至关重要。作为一种新兴传感器，4D成像雷达具有低成本、长距离检测和精确速度测量的优势，使其非常适合物体检测。然而，其稀疏点云和低分辨率限制了物体的几何表示并阻碍了多模态融合。在本研究中，我们引入了SFGFusion，一种基于表面拟合引导的新型相机-4D成像雷达检测网络。通过从图像和雷达数据估计物体的二次曲面参数，显式表面拟合模型增强了空间表示和跨模态交互，实现了对细粒度密集深度更可靠的预测。预测的深度有两个用途：1)在图像分支中引导图像特征从透视视图(PV)转换为统一的鸟瞰图(BEV)用于多模态融合，提高空间映射准确性；2)在表面伪点分支中生成密集伪点云，减轻雷达点稀疏性。原始雷达点云也在单独的雷达分支中编码。这两个点云分支采用基于支柱的方法，然后将特征转换为BEV空间。最后，使用标准的2D主干和检测头从BEV特征预测物体标签和边界框。实验结果表明，SFGFusion有效融合了相机和4D雷达特征，在TJ4DRadSet和view-of-delft(VoD)物体检测基准上取得了优越性能。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决4D成像雷达点云稀疏性和低分辨率导致的物体几何表示不足，以及由此带来的多模态融合困难问题。这个问题在现实中非常重要，因为3D物体检测是自动驾驶的核心技术，而4D成像雷达具有成本低、远距离检测和精确测速的优势，能有效弥补相机在深度信息上的不足。解决这一问题可以提升自动驾驶系统的环境感知能力，增强在复杂场景下的检测精度和可靠性。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者通过分析现有多模态3D检测框架的局限性，特别是图像特征从2D到3D转换过程中因雷达点云稀疏导致的几何约束不足问题，提出了表面拟合模型作为解决方案。作者借鉴了基于图像的3D检测中的特征投影方法、基于点云的3D检测中的柱状处理方法(PointPillars)，以及多模态融合中的BEV特征融合技术，但针对4D成像雷达的特点进行了专门优化和创新设计。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用表面拟合模型估计物体表面深度，通过结合图像语义和雷达几何信息来增强深度预测精度，然后用这些深度信息指导图像特征转换和生成密集伪点云。整体流程包括：1)表面拟合模型融合图像和雷达信息预测物体深度；2)图像分支在深度指导下将特征从透视视图转换为鸟瞰视图；3)雷达分支处理原始4D雷达点云；4)表面伪点分支利用预测深度生成密集伪点云；5)多分支特征融合后通过检测头输出3D物体检测结果。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)提出表面拟合模型增强跨模态交互和深度估计；2)利用拟合深度指导图像特征视图变换提高空间映射精度；3)生成密集伪点云缓解雷达点云稀疏问题；4)设计针对4D雷达特点的多维特征提取方法。相比之前工作，本文专门针对4D成像雷达而非LiDAR进行优化，解决了雷达点云稀疏不规则带来的挑战，同时结合了图像语义和雷达几何信息的优势，实现了更准确的特征对齐和物体表示。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; SFGFusion通过表面拟合模型有效融合相机和4D成像雷达数据，解决了雷达点云稀疏性导致的几何表示不足问题，显著提升了3D物体检测的准确性和鲁棒性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection is essential for autonomous driving. As an emergingsensor, 4D imaging radar offers advantages as low cost, long-range detection,and accurate velocity measurement, making it highly suitable for objectdetection. However, its sparse point clouds and low resolution limit objectgeometric representation and hinder multi-modal fusion. In this study, weintroduce SFGFusion, a novel camera-4D imaging radar detection network guidedby surface fitting. By estimating quadratic surface parameters of objects fromimage and radar data, the explicit surface fitting model enhances spatialrepresentation and cross-modal interaction, enabling more reliable predictionof fine-grained dense depth. The predicted depth serves two purposes: 1) in animage branch to guide the transformation of image features from perspectiveview (PV) to a unified bird's-eye view (BEV) for multi-modal fusion, improvingspatial mapping accuracy; and 2) in a surface pseudo-point branch to generatedense pseudo-point cloud, mitigating the radar point sparsity. The originalradar point cloud is also encoded in a separate radar branch. These two pointcloud branches adopt a pillar-based method and subsequently transform thefeatures into the BEV space. Finally, a standard 2D backbone and detection headare used to predict object labels and bounding boxes from BEV features.Experimental results show that SFGFusion effectively fuses camera and 4D radarfeatures, achieving superior performance on the TJ4DRadSet and view-of-delft(VoD) object detection benchmarks.</description>
      <author>example@mail.com (Xiaozhi Li, Huijun Di, Jian Li, Feng Liu, Wei Liang)</author>
      <guid isPermaLink="false">2510.19215v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>AgentSense: LLMs Empower Generalizable and Explainable Web-Based Participatory Urban Sensing</title>
      <link>http://arxiv.org/abs/2510.19661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;AgentSense是一种混合的、无需训练的框架，通过多智能体进化系统将大型语言模型集成到参与式城市感知中，解决了现有系统在多样化城市场景中泛化能力差和决策解释性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;基于网络的参与式城市感知已成为现代城市管理的重要方法，通过利用移动个体作为分布式传感器收集城市数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够适应动态城市条件和异构工作者偏好的城市感知系统，同时提供自然语言解释以提高透明度和信任度。&lt;h4&gt;方法&lt;/h4&gt;AgentSense首先使用经典规划器生成基线解决方案，然后通过多智能体进化系统迭代优化这些解决方案，使感知任务分配适应动态变化，并生成自然语言解释。&lt;h4&gt;主要发现&lt;/h4&gt;在两个大规模移动数据集和七种动态干扰上的实验表明，AgentSense在适应性和可解释性方面明显优于传统方法，且比单智能体LLM基线在性能和鲁棒性方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;AgentSense代表了在网络上部署自适应和可解释的城市感知系统的重要进展，为现代城市管理提供了更有效的工具。&lt;h4&gt;翻译&lt;/h4&gt;基于网络的参与式城市感知已通过利用移动个体作为分布式传感器成为现代城市管理的重要方法。然而，现有的城市感知系统难以在多样化的城市场景中泛化，并且在决策过程中解释性差。在这项工作中，我们介绍了AgentSense，一个混合的、无需训练的框架，通过多智能体进化系统将大型语言模型集成到参与式城市感知中。AgentSense最初使用经典规划器生成基线解决方案，然后迭代优化它们，使感知任务分配适应动态城市条件和异构工作者偏好，同时产生自然语言解释以提高透明度和信任度。在两个大规模移动数据集和七种动态干扰上的大量实验表明，AgentSense在适应性和可解释性方面比传统方法具有明显优势。此外，与单智能体LLM基线相比，我们的方法在性能和鲁棒性方面表现更好，并提供更合理和透明的解释。这些结果表明AgentSense是在网络上部署自适应和可解释的城市感知系统的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Web-based participatory urban sensing has emerged as a vital approach formodern urban management by leveraging mobile individuals as distributedsensors. However, existing urban sensing systems struggle with limitedgeneralization across diverse urban scenarios and poor interpretability indecision-making. In this work, we introduce AgentSense, a hybrid, training-freeframework that integrates large language models (LLMs) into participatory urbansensing through a multi-agent evolution system. AgentSense initially employsclassical planner to generate baseline solutions and then iteratively refinesthem to adapt sensing task assignments to dynamic urban conditions andheterogeneous worker preferences, while producing natural language explanationsthat enhance transparency and trust. Extensive experiments across twolarge-scale mobility datasets and seven types of dynamic disturbancesdemonstrate that AgentSense offers distinct advantages in adaptivity andexplainability over traditional methods. Furthermore, compared to single-agentLLM baselines, our approach outperforms in both performance and robustness,while delivering more reasonable and transparent explanations. These resultsposition AgentSense as a significant advancement towards deploying adaptive andexplainable urban sensing systems on the web.</description>
      <author>example@mail.com (Xusen Guo, Mingxing Peng, Xixuan Hao, Xingchen Zou, Qiongyan Wang, Sijie Ruan, Yuxuan Liang)</author>
      <guid isPermaLink="false">2510.19661v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Conditions for Catastrophic Forgetting in Multilingual Translation</title>
      <link>http://arxiv.org/abs/2510.19546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Multilingual Representation Learning (MRL) Workshop 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过系统实证研究，探讨了多语言模型微调中的灾难性遗忘现象，发现模型与数据规模比例是遗忘的主要决定因素，指令遵循能力比架构更重要，参数高效微调无明显优势，而跨语言对齐可有效减轻遗忘并促进知识迁移。&lt;h4&gt;背景&lt;/h4&gt;在多语言基础模型上针对特定语言进行微调通常会导致灾难性遗忘，降低在微调中未见语言的性能。虽然这种现象有广泛记录，但文献中关于何时发生遗忘的结果是零散的。&lt;h4&gt;目的&lt;/h4&gt;解决关于灾难性遗忘发生条件的模糊性，进行系统的实证研究，使用机器翻译作为测试平台，以识别多语言微调中触发灾难性遗忘的条件。&lt;h4&gt;方法&lt;/h4&gt;使用机器翻译作为测试平台，进行受控实验，跨越不同的模型架构、数据规模和微调方法。&lt;h4&gt;主要发现&lt;/h4&gt;模型和数据规模之间的相对规模是遗忘的主要决定因素；模型的指令遵循能力对于保留多语言知识比其架构更重要；与假设相反，参数高效微调在减轻遗忘方面没有明显优于完全微调；跨语言对齐可以减轻遗忘，同时促进对未见目标语言的积极迁移。&lt;h4&gt;结论&lt;/h4&gt;跨语言对齐是一种有效的策略，可以减轻灾难性遗忘，并促进知识迁移到未见语言。&lt;h4&gt;翻译&lt;/h4&gt;在多语言基础模型上针对特定语言进行微调通常会导致灾难性遗忘，降低在微调中未见语言的性能。虽然这种现象有广泛记录，但文献中关于何时发生遗忘的结果是零散的。为解决这一模糊性，我们使用机器翻译作为测试平台进行系统实证研究，以识别多语言微调中触发灾难性遗忘的条件。通过跨越不同模型架构、数据规模和微调方法的受控实验，我们揭示了模型与数据规模之间的相对比例是遗忘的主要决定因素。此外，我们证明模型的指令遵循能力对于保留多语言知识比其架构更为关键。与假设相反，参数高效微调在减轻遗忘方面并未显示出比完全微调的明显优势。最后，我们表明跨语言对齐可以减轻遗忘，同时促进对未见目标语言的积极迁移。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning multilingual foundation models on specific languages ofteninduces catastrophic forgetting, degrading performance on languages unseen infine-tuning. While this phenomenon is widely-documented, the literaturepresents fragmented results about when forgetting occurs. To address thisambiguity, we conduct a systematic empirical study using machine translation asa testbed to identify the conditions that trigger catastrophic forgetting inmultilingual fine-tuning. Through controlled experiments across different modelarchitectures, data scales, and fine-tuning approaches, we reveal that therelative scale between model and data size is a primary determinant offorgetting. Moreover, we demonstrate that a model's instruction-followingability is more critical for retaining multilingual knowledge than itsarchitecture. Contrary to assumptions, parameter-efficient fine-tuning offersno clear advantage over full fine-tuning in mitigating forgetting. Lastly, weshow that cross-lingual alignment can mitigate forgetting while alsofacilitating positive transfer to unseen target languages.</description>
      <author>example@mail.com (Danni Liu, Jan Niehues)</author>
      <guid isPermaLink="false">2510.19546v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Which Evaluation for Which Model? A Taxonomy for Speech Model Assessment</title>
      <link>http://arxiv.org/abs/2510.19509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  57 pages (26 main, 25 appendix, 6 references)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种统一的分类法，用于解决语音基础模型评估的匹配问题，通过三个正交轴对现有评估方法进行系统分类，为模型与合适评估方法的匹配提供了原则性框架，并揭示了未来基准设计的优先事项。&lt;h4&gt;背景&lt;/h4&gt;语音基础模型最近在广泛任务中取得了显著能力，但其评估在不同任务和模型类型之间仍然分散，不同模型在语音处理的不同方面表现出色，因此需要不同的评估协议。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的分类法，解决'哪种评估适合哪种模型'的问题，为选择、解释和扩展语音模型评估提供概念基础和实践指南。&lt;h4&gt;方法&lt;/h4&gt;定义三个正交轴：测量的评估方面、尝试任务所需的模型能力、执行任务所需的任务或协议要求，沿着这些轴对现有评估和基准进行分类，涵盖表示学习、语音生成和交互式对话等领域。&lt;h4&gt;主要发现&lt;/h4&gt;通过将每个评估映射到模型展示的能力和方法论需求，该分类法揭示了系统性的差距，如韵律、交互或推理覆盖有限，突显了未来基准设计的优先事项。&lt;h4&gt;结论&lt;/h4&gt;该统一的分类法为模型与合适评估方法的匹配提供了原则性框架，为语音模型评估领域提供了概念基础和实践指导。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型最近在广泛任务中取得了显著能力。然而，它们的评估在不同任务和模型类型之间仍然分散。不同的模型在语音处理的不同方面表现出色，因此需要不同的评估协议。本文提出了一种统一的分类法，解决'哪种评估适合哪种模型'的问题。该分类法定义了三个正交轴：测量的评估方面、尝试任务所需的模型能力、执行任务所需的任务或协议要求。我们沿着这些轴对广泛的现有评估和基准进行分类，涵盖表示学习、语音生成和交互式对话等领域。通过将每个评估映射到模型展示的能力（如语音生成、实时处理）及其方法论需求（如微调数据、人工判断），该分类法为模型与合适评估方法的匹配提供了原则性框架。它还揭示了系统性的差距，如韵律、交互或推理覆盖有限，突显了未来基准设计的优先事项。总体而言，这项工作为选择、解释和扩展语音模型评估提供了概念基础和实践指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models have recently achieved remarkable capabilitiesacross a wide range of tasks. However, their evaluation remains disjointedacross tasks and model types. Different models excel at distinct aspects ofspeech processing and thus require different evaluation protocols. This paperproposes a unified taxonomy that addresses the question: Which evaluation isappropriate for which model? The taxonomy defines three orthogonal axes: the\textbf{evaluation aspect} being measured, the model capabilities required toattempt the task, and the task or protocol requirements needed to perform it.We classify a broad set of existing evaluations and benchmarks along theseaxes, spanning areas such as representation learning, speech generation, andinteractive dialogue. By mapping each evaluation to the capabilities a modelexposes (e.g., speech generation, real-time processing) and to itsmethodological demands (e.g., fine-tuning data, human judgment), the taxonomyprovides a principled framework for aligning models with suitable evaluationmethods. It also reveals systematic gaps, such as limited coverage of prosody,interaction, or reasoning, that highlight priorities for future benchmarkdesign. Overall, this work offers a conceptual foundation and practical guidefor selecting, interpreting, and extending evaluations of speech models.</description>
      <author>example@mail.com (Maureen de Seyssel, Eeshan Gunesh Dhekane)</author>
      <guid isPermaLink="false">2510.19509v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Universal Quantitative Abstraction: Categorical Duality and Logical Completeness for Probabilistic Systems</title>
      <link>http://arxiv.org/abs/2510.19444v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种概率系统的定量抽象统一理论，结合了范畴论、最优传输和定量模态逻辑。核心是一个具有普遍性质的规范ε-商，在所有ε-抽象中最为信息丰富且满足值损失上限。该理论建立了抽象与实现函子之间的伴随关系，揭示了度量结构与逻辑语义的范畴对偶。研究还引入了定量模态μ演算，证明了其在逻辑可表示系统中的表达完整性，并分析了接口细化下的组合性。通过在有限马尔可夫决策过程上的验证，证实了理论的收缩性、值损失界限等性质，为状态聚合和表示学习提供了数学精确的保证。&lt;h4&gt;背景&lt;/h4&gt;概率系统的抽象和近似是计算机科学和人工智能中的重要问题，特别是在处理复杂系统时。现有的抽象方法往往缺乏统一的数学框架，难以保证近似的质量和性质。范畴论提供了描述系统结构和关系的强大工具，最优传输提供了度量概率空间之间距离的方法，而定量模态逻辑则允许对系统的行为进行精确描述。这些理论领域的结合为概率系统的定量抽象提供了新的可能性。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在构建一个统一的概率系统定量抽象理论，该理论能够：1) 提供一个信息丰富且满足值损失上限的抽象方法；2) 建立抽象与实现之间的数学关系；3) 刻画行为伪度量的性质；4) 开发表达完整的定量模态μ演算；5) 分析系统组合性；6) 为状态聚合和表示学习提供数学保证。&lt;h4&gt;方法&lt;/h4&gt;本研究采用了以下方法：1) 构建具有普遍性质的规范ε-商作为核心抽象机制；2) 应用范畴论建立抽象与实现函子之间的伴随关系；3) 使用贝尔曼风格算子刻画行为伪度量并证明其不动点性质；4) 在余代数框架中证明收缩性和利普希茨性质；5) 引入定量模态μ演算并证明其表达完整性；6) 分析接口细化下的组合性质；7) 在有限马尔可夫决策过程上进行实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;研究的主要发现包括：1) 规范ε-商在所有ε-抽象中是最能保留信息且满足值损失上限的；2) 抽象与实现函子之间存在伴随关系，揭示了度量结构与逻辑语义的范畴对偶；3) 行为伪度量是贝尔曼风格算子的唯一不动点，具有收缩性和利普希茨性质；4) 定量模态μ演算在逻辑可表示系统中具有表达完整性，行为距离与最大逻辑偏差一致；5) 在接口细化下，抽象具有组合性质，能够清晰描述系统边界处的交互；6) 通过实验验证，理论具有收缩性、值损失界限、扰动稳定性、对抗区分性和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;本研究提出的概率系统定量抽象统一理论为复杂系统的抽象和近似提供了坚实的数学基础。该理论通过结合范畴论、最优传输和定量模态逻辑，建立了抽象与实现之间的严格关系，并确保了抽象的质量和性质。定量模态μ演算的表达完整性以及行为距离与逻辑偏差的一致性，为系统分析和验证提供了有力工具。实验验证证明了理论的鲁棒性和计算可行性。该框架不仅为状态聚合和表示学习提供了有原则的目标，还在随机域中为值函数近似提供了数学上精确的保证，对概率系统的建模、分析和应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种概率系统的定量抽象统一理论，该理论将范畴论、最优传输和定量模态逻辑联系起来。其核心是一个具有普遍性质的规范ε-商：在所有ε-抽象中，它是最能保留信息且满足规定值损失上限的。这种构造通过特殊伴随函子定理在抽象函子和实现函子之间诱导了一个伴随关系，揭示了度量结构和逻辑语义之间的范畴对偶。行为伪度量被刻画为贝尔曼风格算子的唯一不动点，并在余代数框架中证明了其收缩性和利普希茨性质。引入了定量模态μ演算并证明其在逻辑可表示系统中具有表达完整性，使得行为距离与最大逻辑偏差一致。分析了在接口细化下的组合性，阐明了抽象如何在系统边界处交互。在有限马尔可夫决策过程上的精确验证套件证实了收缩性、值损失界限、扰动下的稳定性、对抗区分性和可扩展性，展示了鲁棒性和计算可行性。由此产生的框架为状态聚合和表示学习提供了有原则的目标，并在随机域中为值函数近似提供了数学上精确的保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A unified theory of quantitative abstraction is presented for probabilisticsystems that links category theory, optimal transport, and quantitative modallogic. At its core is a canonical $ \varepsilon $-quotient endowed with auniversal property: among all $ \varepsilon $-abstractions, it is the mostinformative one that respects a prescribed bound on value loss. Thisconstruction induces an adjunction between abstraction and realization functors$ (Q_{\varepsilon} \dashv R_{\varepsilon}) $, established via the SpecialAdjoint Functor Theorem, revealing a categorical duality between metricstructure and logical semantics. A behavioral pseudometric is characterized asthe unique fixed point of a Bellman-style operator, with contraction andLipschitz properties proved in a coalgebraic setting. A quantitative modal $\mu $-calculus is introduced and shown to be expressively complete forlogically representable systems, so that behavioral distance coincides withmaximal logical deviation. Compositionality under interface refinement isanalyzed, clarifying how abstractions interact across system boundaries. Anexact validation suite on finite Markov decision processes corroborates thecontraction property, value-loss bounds, stability under perturbation,adversarial distinguishability, and scalability, demonstrating both robustnessand computational feasibility. The resulting framework provides principledtargets for state aggregation and representation learning, with mathematicallyprecise guarantees for value-function approximation in stochastic domains.</description>
      <author>example@mail.com (Nivar Anwer)</author>
      <guid isPermaLink="false">2510.19444v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Learning Noise-Resilient and Transferable Graph-Text Alignment via Dynamic Quality Assessment</title>
      <link>http://arxiv.org/abs/2510.19384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了ADAligner，一个动态、质量感知的图文本对齐框架，解决了现有CLIP风格图文本对齐器在处理多对多关系和适应不同数据质量方面的局限性。ADAligner能够根据监督质量在表达性的多对多和保守的一对一目标之间动态调整，在多个任务上表现出色，并具有更强的鲁棒性和更快的预训练速度。&lt;h4&gt;背景&lt;/h4&gt;在文本属性图上预训练图基础模型对于搜索、推荐和知识发现等网络规模应用至关重要。然而，现有的CLIP风格图文本对齐器面临两个关键限制：假设节点和文本之间存在严格的一对一对应关系，忽略了现实世界图中的固有多对多关系；以及依赖于静态对齐目标，无法适应不同的数据质量，在有噪声监督下变得脆弱。&lt;h4&gt;目的&lt;/h4&gt;解决现有图文本对齐器的局限性，提出一个动态、质量感知的图文本对齐框架，能够根据监督质量在表达性的多对多和保守的一对一目标之间动态调整。&lt;h4&gt;方法&lt;/h4&gt;提出了ADAligner框架，实时估计批次级别的对齐可靠性，并相应地调整优化过程：当监督干净时，促进软的、子图级别的多对多对齐；在噪声下，通过动态过滤低置信度配对来强调可靠的一对一对齐。理论上证明了这种动态机制形成一个稳定的负反馈过程，确保收敛性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在九个不同的TAG数据集上的实验表明，ADAligner在零样本/少样本节点分类、链接预测和跨模态检索任务上一致优于先前的图文本对齐器。在有噪声监督下保持强大的鲁棒性，与多模态基线相比，预训练速度加快约2到3倍。&lt;h4&gt;结论&lt;/h4&gt;ADAligner为现实网络环境中的图文本表示学习建立了一个可扩展和可靠的基础。&lt;h4&gt;翻译&lt;/h4&gt;在文本属性图上预训练图基础模型对于搜索、推荐和知识发现等网络规模应用至关重要。然而，现有的CLIP风格图文本对齐器面临两个关键限制：它们假设节点和文本之间存在严格的一对一对应关系，忽略了现实世界图中的固有多对多关系；并且它们依赖于静态对齐目标，无法适应不同的数据质量，在有噪声监督下变得脆弱。总之，这些限制暴露了一个核心困境：拥抱表达性的多对多对齐会放大噪声，而恢复到严格的一对一策略则会牺牲语义多样性，无法处理本质上不匹配的配对。为了应对这些挑战，我们提出了ADAligner，一个动态、质量感知的图文本对齐框架，根据监督质量在表达性的多对多和保守的一对一目标之间动态调整。ADAligner实时估计批次级别的对齐可靠性，并相应地调整其优化过程，在监督干净时促进软的、子图级别的多对多对齐，同时在噪声下通过动态过滤低置信度配对来强调可靠的一对一对齐。理论上，我们证明这种动态机制形成一个稳定的负反馈过程，确保收敛性和鲁棒性。在九个不同的TAG数据集上的综合实验表明，ADAligner在零样本/少样本节点分类、链接预测和跨模态检索任务上一致地优于先前的图文本对齐器。它在有噪声监督下保持强大的鲁棒性，与多模态基线相比，预训练速度加快约2到3倍，为现实网络环境中的图文本表示学习建立了一个可扩展和可靠的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training Graph Foundation Models (GFMs) on text-attributed graphs (TAGs)is central to web-scale applications such as search, recommendation, andknowledge discovery. However, existing CLIP-style graph-text aligners face twokey limitations: they assume strict one-to-one correspondences between nodesand texts, overlooking the inherent many-to-many relations in real-worldgraphs; and they rely on static alignment objectives that cannot adapt tovarying data quality, making them brittle under noisy supervision. Together,these limitations expose a core dilemma: embracing expressive many-to-manyalignment amplifies noise, while reverting to strict one-to-one strategiessacrifices semantic diversity and fails to handle inherently mismatched pairs.To address these challenges, we propose ADAligner, a dynamic, quality-awaregraph-text alignment framework that dynamically adjusts between expressivemany-to-many and conservative one-to-one objectives according to supervisionquality. ADAligner estimates batch-level alignment reliability in real time andadapts its optimization accordingly, promoting soft, subgraph-levelmany-to-many alignment when supervision is clean, while emphasizing reliableone-to-one alignment by dynamically filtering low-confidence pairs under noise.Theoretically, we prove that this dynamic mechanism forms a stable negativefeedback process, ensuring convergence and robustness. Comprehensiveexperiments on nine diverse TAG datasets demonstrate that ADAlignerconsistently outperforms prior graph-text aligners on zero-/few-shot nodeclassification, link prediction and cross-modal retrieval tasks. It maintainsstrong robustness under noisy supervision and accelerates pre-training byapproximately 2 to 3 times compared to multimodal baselines, establishing ascalable and reliable foundation for graph-text representation learning inreal-world web environments.</description>
      <author>example@mail.com (Yuhang Liu, Minglai Shao, Zengyi Wo, Yunlong Chu, Bing Hao, Shengzhong Liu, Ruijie Wang, Jianxin Li)</author>
      <guid isPermaLink="false">2510.19384v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>From Newborn to Impact: Bias-Aware Citation Prediction</title>
      <link>http://arxiv.org/abs/2510.19246v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种偏差感知引用预测框架，通过多智能体特征提取和鲁棒图表示学习，解决了新生论文引用预测中的两个关键研究空白，实验证明其有效性。&lt;h4&gt;背景&lt;/h4&gt;引用动态是获取研究影响的关键，支撑研究评估、学术推荐和知识扩散研究。引用预测对新生论文尤为重要，因为在没有引用信号和高度长尾分布的情况下，必须进行早期评估。&lt;h4&gt;目的&lt;/h4&gt;解决两个关键研究空白：一是对科学影响的隐含因素建模不足，导致依赖粗略代理指标；二是缺乏偏差感知学习，无法在低引用论文上提供稳定预测。&lt;h4&gt;方法&lt;/h4&gt;提出偏差感知引用预测框架，结合多智能体特征提取和鲁棒图表示学习。多智能体图共学习模块从元数据和外部资源中提取细粒度可解释信号，并与异构网络嵌入融合；同时采用鲁棒机制，包括两阶段前向过程、GroupDRO优化和正则化头。&lt;h4&gt;主要发现&lt;/h4&gt;在两个真实世界数据集上的综合实验证明了所提模型的有效性。模型实现了约百分之十三的错误指标降低和百分之五点五的排名指标显著改善。&lt;h4&gt;结论&lt;/h4&gt;提出的偏差感知引用预测框架能够有效解决现有研究空白，提高引用预测的准确性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;作为获取研究影响的关键，引用动态支撑着研究评估、学术推荐和知识扩散研究。引用预测对新生论文尤为重要，因为在没有引用信号和高度长尾分布的情况下，必须进行早期评估。我们确定了两个关键研究空白：一是对科学影响的隐含因素建模不足，导致依赖粗略代理指标；二是缺乏偏差感知学习，无法在低引用论文上提供稳定预测。我们通过提出偏差感知引用预测框架来解决这些空白，该框架结合了多智能体特征提取和鲁棒图表示学习。首先，多智能体图共学习模块从元数据和外部资源中推导出细粒度、可解释的信号，如可重复性、协作网络和文本质量，并将它们与异构网络嵌入融合，即使在缺乏早期引用信号的情况下也能提供丰富的监督。其次，我们加入了一套鲁棒机制：一个将显性因素通过中间曝光估计路由的两阶段前向过程，用于优化跨环境最坏情况组风险的GroupDRO，以及在单调性和平滑性约束下对可控因素执行假设分析的正则化头。在两个真实世界数据集上的综合实验证明了我们提出的模型的有效性。具体而言，我们的模型在错误指标上实现了约百分之十三的降低，在排名指标上比基线方法有显著的百分之五点五的改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a key to accessing research impact, citation dynamics underpins researchevaluation, scholarly recommendation, and the study of knowledge diffusion.Citation prediction is particularly critical for newborn papers, where earlyassessment must be performed without citation signals and under highlylong-tailed distributions. We identify two key research gaps: (i) insufficientmodeling of implicit factors of scientific impact, leading to reliance oncoarse proxies; and (ii) a lack of bias-aware learning that can deliver stablepredictions on lowly cited papers. We address these gaps by proposing aBias-Aware Citation Prediction Framework, which combines multi-agent featureextraction with robust graph representation learning. First, a multi-agent xgraph co-learning module derives fine-grained, interpretable signals, such asreproducibility, collaboration network, and text quality, from metadata andexternal resources, and fuses them with heterogeneous-network embeddings toprovide rich supervision even in the absence of early citation signals. Second,we incorporate a set of robust mechanisms: a two-stage forward process thatroutes explicit factors through an intermediate exposure estimate, GroupDRO tooptimize worst-case group risk across environments, and a regularization headthat performs what-if analyses on controllable factors under monotonicity andsmoothness constraints. Comprehensive experiments on two real-world datasetsdemonstrate the effectiveness of our proposed model. Specifically, our modelachieves around a 13% reduction in error metrics (MALE and RMSLE) and a notable5.5% improvement in the ranking metric (NDCG) over the baseline methods.</description>
      <author>example@mail.com (Mingfei Lu, Mengjia Wu, Jiawei Xu, Weikai Li, Feng Liu, Ying Ding, Yizhou Sun, Jie Lu, Yi Zhang)</author>
      <guid isPermaLink="false">2510.19246v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>No Intelligence Without Statistics: The Invisible Backbone of Artificial Intelligence</title>
      <link>http://arxiv.org/abs/2510.19212v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人工智能的理论和方法基础实际上是统计学，而非仅仅来自计算机科学。统计学为机器学习和现代AI提供了不可或缺的基础。&lt;h4&gt;背景&lt;/h4&gt;人工智能的快速发展通常被描述为来自计算机科学和工程的革命，但这种描述掩盖了一个基本事实：AI的理论和方法核心一直是统计学。&lt;h4&gt;目的&lt;/h4&gt;系统性地论证统计学为机器学习和现代AI提供了不可或缺的基础，并呼吁教育、研究和实践重新拥抱这一统计学基础。&lt;h4&gt;方法&lt;/h4&gt;将AI分解为九个基础支柱（推断、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一），展示每个支柱都建立在百年统计原理之上。&lt;h4&gt;主要发现&lt;/h4&gt;AI的九个基础支柱都建立在统计原理之上；从假设检验和估计的推断框架到聚类和生成式AI的密度估计根源；从启发循环网络的时间序列分析到提供真正理解的因果模型；统计学提供了理论框架、不确定性量化等'大脑'功能，而计算机科学提供了可扩展算法和硬件等'肌肉'功能。&lt;h4&gt;结论&lt;/h4&gt;承认统计学的基础对于开发更强大、可解释和值得信赖的智能系统是必要的步骤。没有统计学习就没有机器学习；没有统计思维就没有人工智能。&lt;h4&gt;翻译&lt;/h4&gt;人工智能的迅速崛起通常被描述为一场源于计算机科学和工程学的革命。然而，这种叙事掩盖了一个基本事实：AI的理论和方法核心，并且一直是，统计学的。本文系统性地论证统计学领域为机器学习和现代AI提供了不可或缺的基础。我们将AI分解为九个基础支柱——推断、密度估计、序列学习、泛化、表示学习、可解释性、因果性、优化和统一——证明每一个都建立在百年统计原理之上。从支撑模型评估的假设检验和估计推断框架，到聚类和生成式AI的密度估计根源；从启发循环网络的时间序列分析到提供真正理解的因果模型，我们追溯了一条不间断的统计谱系。在庆祝推动现代AI的计算引擎的同时，我们认为统计学提供了'大脑'——理论框架、不确定性量化和推断目标——而计算机科学提供了'肌肉'——可扩展算法和硬件。认识到这一统计基础不仅仅是一个学术练习，而是开发更强大、可解释和值得信赖的智能系统的必要步骤。我们呼吁教育、研究和实践重新拥抱这一统计基础。忽视这些根基可能会构建一个脆弱的未来；拥抱它们才是通向真正智能机器的道路。没有统计学习就没有机器学习；没有统计思维就没有人工智能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid ascent of artificial intelligence (AI) is often portrayed as arevolution born from computer science and engineering. This narrative, however,obscures a fundamental truth: the theoretical and methodological core of AI is,and has always been, statistical. This paper systematically argues that thefield of statistics provides the indispensable foundation for machine learningand modern AI. We deconstruct AI into nine foundational pillars-Inference,Density Estimation, Sequential Learning, Generalization, RepresentationLearning, Interpretability, Causality, Optimization, andUnification-demonstrating that each is built upon century-old statisticalprinciples. From the inferential frameworks of hypothesis testing andestimation that underpin model evaluation, to the density estimation roots ofclustering and generative AI; from the time-series analysis inspiring recurrentnetworks to the causal models that promise true understanding, we trace anunbroken statistical lineage. While celebrating the computational engines thatpower modern AI, we contend that statistics provides the brain-the theoreticalframeworks, uncertainty quantification, and inferential goals-while computerscience provides the brawn-the scalable algorithms and hardware. Recognizingthis statistical backbone is not merely an academic exercise, but a necessarystep for developing more robust, interpretable, and trustworthy intelligentsystems. We issue a call to action for education, research, and practice tore-embrace this statistical foundation. Ignoring these roots risks building afragile future; embracing them is the path to truly intelligent machines. Thereis no machine learning without statistical learning; no artificial intelligencewithout statistical thought.</description>
      <author>example@mail.com (Ernest Fokoué)</author>
      <guid isPermaLink="false">2510.19212v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>An Encode-then-Decompose Approach to Unsupervised Time Series Anomaly Detection on Contaminated Training Data--Extended Version</title>
      <link>http://arxiv.org/abs/2510.18998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages. An extended version of "An Encode-then-Decompose Approach  to Unsupervised Time Series Anomaly Detection on Contaminated Training Data"  accepted at ICDE 2026&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的编码-分解范式和基于互信息的度量方法，用于时间序列异常检测，提高了对污染时间序列的鲁棒性，并在多个基准测试上取得了优异性能。&lt;h4&gt;背景&lt;/h4&gt;时间序列异常检测在现代大规模系统中至关重要，应用于多个领域分析和监控系统运行。无监督方法因不需要异常标签而受到广泛关注，避免了高成本并具有更广泛的应用。&lt;h4&gt;目的&lt;/h4&gt;解决自动编码器学习到的表示对训练时间序列中的异常敏感导致准确性降低的问题，提高方法在污染数据上的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出编码-分解范式，将编码表示分解为稳定表示和辅助表示；同时提出基于互信息的新度量方法替代重构误差来识别异常。&lt;h4&gt;主要发现&lt;/h4&gt;在八个常用的多变量和单变量时间序列基准测试上展示了具有竞争力或最先进的性能，对不同污染比例的时间序列表现出鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;新方法通过分解编码表示和使用互信息度量，有效提高了时间序列异常检测的准确性和鲁棒性，特别是在训练数据存在异常污染的情况下。&lt;h4&gt;翻译&lt;/h4&gt;时间序列异常检测在现代大规模系统中很重要，并应用于各种领域以分析和监控不同系统的运行。无监督方法引起了广泛关注，因为它们在训练期间不需要异常标签，从而避免了潜在的高成本并具有更广泛的应用。其中，自动编码器受到了广泛关注。它们使用来自压缩表示的重构误差来定义异常分数。然而，自动编码器学习到的表示对训练时间序列中的异常敏感，导致准确性降低。我们提出了一种新颖的编码-分解范式，将编码表示分解为稳定表示和辅助表示，从而在使用污染时间序列进行训练时增强鲁棒性。此外，我们提出了一种基于互信息的新指标来替代重构误差以识别异常。我们的提案在八个常用的多变量和单变量时间序列基准测试上展示了具有竞争力或最先进的性能，并对具有不同污染比例的时间序列表现出鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series anomaly detection is important in modern large-scale systems andis applied in a variety of domains to analyze and monitor the operation ofdiverse systems. Unsupervised approaches have received widespread interest, asthey do not require anomaly labels during training, thus avoiding potentiallyhigh costs and having wider applications. Among these, autoencoders havereceived extensive attention. They use reconstruction errors from compressedrepresentations to define anomaly scores. However, representations learned byautoencoders are sensitive to anomalies in training time series, causingreduced accuracy. We propose a novel encode-then-decompose paradigm, where wedecompose the encoded representation into stable and auxiliary representations,thereby enhancing the robustness when training with contaminated time series.In addition, we propose a novel mutual information based metric to replace thereconstruction errors for identifying anomalies. Our proposal demonstratescompetitive or state-of-the-art performance on eight commonly used multi- andunivariate time series benchmarks and exhibits robustness to time series withdifferent contamination ratios.</description>
      <author>example@mail.com (Buang Zhang, Tung Kieu, Xiangfei Qiu, Chenjuan Guo, Jilin Hu, Aoying Zhou, Christian S. Jensen, Bin Yang)</author>
      <guid isPermaLink="false">2510.18998v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>SBAN: A Framework \&amp; Multi-Dimensional Dataset for Large Language Model Pre-Training and Software Code Mining</title>
      <link>http://arxiv.org/abs/2510.18936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一个名为SBAN的大规模多维度数据集，用于推进大型语言模型在软件代码分析方面的预训练和评估。&lt;h4&gt;背景&lt;/h4&gt;软件代码分析领域需要大规模、多模态的数据集来支持大型语言模型的训练和评估，特别是在安全分析和软件理解方面。&lt;h4&gt;目的&lt;/h4&gt;创建一个包含源代码、二进制代码、汇编指令和自然语言描述的多维度数据集，以支持跨表示学习、软件语义理解和自动化恶意软件检测等研究。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含超过300万个样本的数据集，其中包括290万个良性样本和672,000个恶意软件样本，每个样本都通过四个互补层表示：二进制代码、汇编指令、自然语言描述和源代码。&lt;h4&gt;主要发现&lt;/h4&gt;这种独特的多模态结构支持跨表示学习研究，并且可以应用于安全分析、代码翻译、代码解释和其他涉及异构数据的软件挖掘任务。&lt;h4&gt;结论&lt;/h4&gt;SBAN数据集通过桥接低级机器表示和高级人类语义，为构建能够推理代码的智能系统提供了坚实的基础，为挖掘软件行为、改进安全分析和增强大型语言模型在软件代码挖掘方面的能力开辟了新的机会。&lt;h4&gt;翻译&lt;/h4&gt;这篇论文介绍了SBAN（源代码、二进制代码、汇编指令和自然语言描述），这是一个大规模、多维度数据集，旨在推进大型语言模型在软件代码分析方面的预训练和评估。SBAN包含超过300万个样本，其中包括290万个良性样本和672,000个恶意软件样本，每个样本都通过四个互补层表示：二进制代码、汇编指令、自然语言描述和源代码。这种独特的多模态结构支持跨表示学习研究、软件语义理解和自动化恶意软件检测。除了安全应用外，SBAN还支持更广泛的任务，如代码翻译、代码解释和其他涉及异构数据的软件挖掘任务。它特别适合深度模型的可扩展训练，包括变压器和其他大型语言模型架构。通过桥接低级机器表示和高级人类语义，SBAN为构建能够推理代码的智能系统提供了坚实的基础。我们相信，这个数据集为挖掘软件行为、改进安全分析和增强大型语言模型在软件代码挖掘的预训练和微调任务方面的能力开辟了新的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces SBAN (Source code, Binary, Assembly, and NaturalLanguage Description), a large-scale, multi-dimensional dataset designed toadvance the pre-training and evaluation of large language models (LLMs) forsoftware code analysis. SBAN comprises more than 3 million samples, including2.9 million benign and 672,000 malware respectively, each represented acrossfour complementary layers: binary code, assembly instructions, natural languagedescriptions, and source code. This unique multimodal structure enablesresearch on cross-representation learning, semantic understanding of software,and automated malware detection. Beyond security applications, SBAN supportsbroader tasks such as code translation, code explanation, and other softwaremining tasks involving heterogeneous data. It is particularly suited forscalable training of deep models, including transformers and other LLMarchitectures. By bridging low-level machine representations and high-levelhuman semantics, SBAN provides a robust foundation for building intelligentsystems that reason about code. We believe that this dataset opens newopportunities for mining software behavior, improving security analytics, andenhancing LLM capabilities in pre-training and fine-tuning tasks for softwarecode mining.</description>
      <author>example@mail.com (Hamed Jelodar, Mohammad Meymani, Samita Bai, Roozbeh Razavi-Far, Ali A. Ghorbani)</author>
      <guid isPermaLink="false">2510.18936v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>A flexible framework for structural plasticity in GPU-accelerated sparse spiking neural networks</title>
      <link>http://arxiv.org/abs/2510.19764v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 9 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的灵活框架，用于实现GPU加速的结构可塑性规则，展示了如何训练高效稀疏的脉冲神经网络分类器并学习拓扑图，稀疏模型可比密集模型训练速度快10倍。&lt;h4&gt;背景&lt;/h4&gt;大多数人工神经网络和生物大脑学习研究集中在突触可塑性上，而生物大脑中的结构可塑性（创建和移除连接）对有效学习、损伤恢复和资源优化同样重要。尽管受此启发，机器学习中常使用剪枝移除弱连接，但现有框架针对密集连接优化，无法降低大型模型的训练成本。&lt;h4&gt;目的&lt;/h4&gt;开发一种支持结构可塑性规则的GPU加速框架，用于训练高效稀疏的SNN分类器，并在无监督学习背景下实现拓扑图形成，探索稀疏性的计算优势。&lt;h4&gt;方法&lt;/h4&gt;基于GeNN模拟器，使用e-prop监督学习规则和DEEP R训练稀疏SNN分类器，然后在无监督学习场景中应用该框架学习拓扑图。&lt;h4&gt;主要发现&lt;/h4&gt;稀疏分类器比基准密集模型训练时间减少高达10倍，同时通过DEEP R重连线保持与原始模型相当的性能；在比实时更快的模拟中成功展示了拓扑图形成，提供了连接演变的见解，并测量了模拟速度与网络规模的关系。&lt;h4&gt;结论&lt;/h4&gt;该框架使研究人员能够探索网络结构和神经通信中的稀疏性维持，以及在各种神经形态应用中稀疏性的计算优势。&lt;h4&gt;翻译&lt;/h4&gt;关于人工神经网络训练和生物大脑学习建模的大多数研究都集中在突触可塑性上，其中学习等同于改变现有连接的强度。然而，在生物大脑中，结构可塑性——创建新连接和移除其他连接——同样重要，不仅对有效学习至关重要，还有助于从损伤中恢复和优化资源使用。受结构可塑性启发，剪枝常用于机器学习以从训练好的模型中移除弱连接，从而降低推理的计算需求。然而，通常用于基于反向传播训练ANN和SNN的机器学习框架针对密集连接进行了优化，这意味着剪枝无法帮助降低不断增长的模型的训练成本。GeNN模拟器已经支持稀疏SNN的高效GPU加速模拟，用于计算神经科学和机器学习。在这里，我们提出了一个新的灵活框架，用于实现GPU加速的结构可塑性规则，并首先使用e-prop监督学习规则和DEEP R训练高效稀疏的SNN分类器，然后在无监督学习背景下学习拓扑图。与基准密集模型相比，我们的稀疏分类器将训练时间减少了高达10倍，而DEEP R重连线使它们能够与原始模型一样好地执行。我们在比实时更快的模拟中展示了拓扑图的形成，提供了连接演变的见解，并测量了模拟速度与网络规模的关系。所提出的框架将使进一步研究能够在网络结构和神经通信中实现和保持稀疏性，以及探索稀疏性在各种神经形态应用中的计算优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The majority of research in both training Artificial Neural Networks (ANNs)and modeling learning in biological brains focuses on synaptic plasticity,where learning equates to changing the strength of existing connections.However, in biological brains, structural plasticity - where new connectionsare created and others removed - is also vital, not only for effective learningbut also for recovery from damage and optimal resource usage. Inspired bystructural plasticity, pruning is often used in machine learning to remove weakconnections from trained models to reduce the computational requirements ofinference. However, the machine learning frameworks typically used forbackpropagation-based training of both ANNs and Spiking Neural Networks (SNNs)are optimized for dense connectivity, meaning that pruning does not help reducethe training costs of ever-larger models. The GeNN simulator already supportsefficient GPU-accelerated simulation of sparse SNNs for computationalneuroscience and machine learning. Here, we present a new flexible frameworkfor implementing GPU-accelerated structural plasticity rules and demonstratethis first using the e-prop supervised learning rule and DEEP R to trainefficient, sparse SNN classifiers and then, in an unsupervised learningcontext, to learn topographic maps. Compared to baseline dense models, oursparse classifiers reduce training time by up to 10x while the DEEP R rewiringenables them to perform as well as the original models. We demonstratetopographic map formation in faster-than-realtime simulations, provide insightsinto the connectivity evolution, and measure simulation speed versus networksize. The proposed framework will enable further research into achieving andmaintaining sparsity in network structure and neural communication, as well asexploring the computational benefits of sparsity in a range of neuromorphicapplications.</description>
      <author>example@mail.com (James C. Knight, Johanna Senk, Thomas Nowotny)</author>
      <guid isPermaLink="false">2510.19764v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Study of Training Dynamics for Memory-Constrained Fine-Tuning</title>
      <link>http://arxiv.org/abs/2510.19675v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了TraDy，一种内存高效的深度神经网络迁移学习方案，通过动态通道选择和层重要性预判实现严格内存约束下的高性能训练。&lt;h4&gt;背景&lt;/h4&gt;随着深度神经网络模型规模不断增大，而部署环境对资源有严格限制，内存高效训练变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在严格内存约束下实现高效训练的深度神经网络训练方法。&lt;h4&gt;方法&lt;/h4&gt;TraDy利用两个关键洞察：更新的层重要性依赖于架构且可预先确定；动态随机通道选择相比静态方法能提供更好的梯度近似。引入动态通道选择方法，在预选层内周期性地随机重新采样通道。&lt;h4&gt;主要发现&lt;/h4&gt;TraDy在各种下游任务和架构上取得最先进性能；实现高达99%的激活稀疏性；实现95%的权重导数稀疏性；权重导数计算的计算量减少97%。&lt;h4&gt;结论&lt;/h4&gt;TraDy是一种有效的内存高效训练方法，能够在保持性能的同时显著减少内存使用和计算需求。&lt;h4&gt;翻译&lt;/h4&gt;随着模型规模扩大而部署环境施加严格的资源限制，深度神经网络的内存高效训练变得越来越重要。我们提出了TraDy，一种利用两个关键洞察的新型迁移学习方案：更新的层重要性依赖于架构且可预先确定，而动态随机通道选择相比静态方法能提供更好的梯度近似。我们引入了一种动态通道选择方法，在预选层内周期性地随机重新采样通道。大量实验表明，TraDy在各种下游任务和架构上取得了最先进性能，同时保持严格的内存约束，实现了高达99%的激活稀疏性、95%的权重导数稀疏性，以及97%的权重导数计算FLOPs减少。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Memory-efficient training of deep neural networks has become increasinglyimportant as models grow larger while deployment environments impose strictresource constraints. We propose TraDy, a novel transfer learning schemeleveraging two key insights: layer importance for updates isarchitecture-dependent and determinable a priori, while dynamic stochasticchannel selection provides superior gradient approximation compared to staticapproaches. We introduce a dynamic channel selection approach thatstochastically resamples channels between epochs within preselected layers.Extensive experiments demonstrate TraDy achieves state-of-the-art performanceacross various downstream tasks and architectures while maintaining strictmemory constraints, achieving up to 99% activation sparsity, 95% weightderivative sparsity, and 97% reduction in FLOPs for weight derivativecomputation.</description>
      <author>example@mail.com (Aël Quélennec, Nour Hezbri, Pavlo Mozharovskyi, Van-Tam Nguyen, Enzo Tartaglione)</author>
      <guid isPermaLink="false">2510.19675v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Beyond the Standard Model</title>
      <link>http://arxiv.org/abs/2510.19168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4+8 pages, 7 figures. Accepted at NeurIPS 2025 Workshop: Machine  Learning and the Physical Sciences&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究展示了如何通过迁移学习减少宇宙学模拟成本，使用标准ΛCDM模型预训练，然后在各种超越ΛCDM场景上微调，发现预训练可以显著减少所需模拟数量，但也存在负迁移风险，特别是当参数间存在强物理简并时。&lt;h4&gt;背景&lt;/h4&gt;机器学习能够实现强大的宇宙学推断，但通常需要大量高保真模拟来覆盖多种宇宙学模型，这带来了高昂的计算成本。&lt;h4&gt;目的&lt;/h4&gt;探索通过迁移学习重用不同宇宙学模型间的知识，以减少模拟成本并提高推断效率。&lt;h4&gt;方法&lt;/h4&gt;在标准ΛCDM宇宙学模型上进行预训练，然后在各种超越ΛCDM场景(包括大质量中微子、修正引力、原始非高斯性)上进行微调，并测试包含瓶颈结构的不同迁移架构。&lt;h4&gt;主要发现&lt;/h4&gt;预训练可以在使用显著更少的超越ΛCDM模拟的情况下实现推断；当ΛCDM和超越ΛCDM参数之间存在强物理简并时，可能会发生负迁移；包含瓶颈结构的迁移架构提供了最佳性能。&lt;h4&gt;结论&lt;/h4&gt;预训练可以加速宇宙学推断过程，但也可能阻碍对新物理的学习，基础模型方法在物理学应用中既带来机会也存在潜在陷阱。&lt;h4&gt;翻译&lt;/h4&gt;机器学习能够实现强大的宇宙学推断，但通常需要覆盖多种宇宙学模型的大量高保真模拟。迁移学习提供了一种通过跨模型重用知识来减少模拟成本的方法。我们展示了在标准宇宙学模型ΛCDM上进行预训练，并在各种超越ΛCDM场景(包括大质量中微子、修正引力和原始非高斯性)上进行微调，可以使用显著更少的超越ΛCDM模拟实现推断。然而，我们也表明当ΛCDM和超越ΛCDM参数之间存在强物理简并时，可能会发生负迁移。我们考虑了各种迁移架构，发现包含瓶颈结构的架构提供最佳性能。我们的研究结果阐明了基础模型方法在物理学中的机会和陷阱：预训练可以加速推断，但也可能阻碍对新物理的学习。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning enables powerful cosmological inference but typicallyrequires many high-fidelity simulations covering many cosmological models.Transfer learning offers a way to reduce the simulation cost by reusingknowledge across models. We show that pre-training on the standard model ofcosmology, $\Lambda$CDM, and fine-tuning on various beyond-$\Lambda$CDMscenarios -- including massive neutrinos, modified gravity, and primordialnon-Gaussianities -- can enable inference with significantly fewerbeyond-$\Lambda$CDM simulations. However, we also show that negative transfercan occur when strong physical degeneracies exist between $\Lambda$CDM andbeyond-$\Lambda$CDM parameters. We consider various transfer architectures,finding that including bottleneck structures provides the best performance. Ourfindings illustrate the opportunities and pitfalls of foundation-modelapproaches in physics: pre-training can accelerate inference, but may alsohinder learning new physics.</description>
      <author>example@mail.com (Veena Krishnaraj, Adrian E. Bayer, Christian Kragh Jespersen, Peter Melchior)</author>
      <guid isPermaLink="false">2510.19168v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Hebbian Principle: Low-Dimensional Structural Projection for Unsupervised Learning</title>
      <link>http://arxiv.org/abs/2510.14810v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为结构投影Hebbian表示（SPHeRe）的新型无监督学习方法，通过整合正交性和结构信息保留，解决了传统Hebbian学习在机器学习中的局限性。实验表明，该方法在图像分类、持续学习、迁移学习和图像重建任务中均表现出色，证明了Hebbian无监督学习在现代深度学习框架中的竞争力和潜力。&lt;h4&gt;背景&lt;/h4&gt;Hebbian学习是一种生物学原理，描述了神经元如何通过重复刺激调整其连接。然而，在机器学习中应用时，由于连接更新不受约束且缺乏反馈介导，它存在严重问题，限制了其在复杂网络架构和任务中的有效扩展。&lt;h4&gt;目的&lt;/h4&gt;解决Hebbian学习在机器学习中的局限性，提出一种能够有效扩展到复杂网络架构和任务的无监督学习方法。&lt;h4&gt;方法&lt;/h4&gt;引入结构投影Hebbian表示（SPHeRe），一种新的无监督学习方法，通过局部的辅助非线性块整合正交性和结构信息保留。结构信息保留的损失通过辅助的轻量级投影反向传播到输入，充当反馈介导，正交性约束则考虑了更新幅度的有界性。&lt;h4&gt;主要发现&lt;/h4&gt;SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试上，在无监督突触可塑性方法中达到了最先进的性能。该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Hebbian无监督学习规则在现代深度学习框架中具有竞争力和潜力，展示了不依赖于严格反向传播的高效和生物启发式学习算法的可能性。&lt;h4&gt;翻译&lt;/h4&gt;Hebbian学习是一种生物学原理，直观地描述了神经元如何通过重复刺激来调整其连接。然而，当应用于机器学习时，由于连接更新不受约束且缺乏反馈介导，它存在严重问题。这些缺点限制了其有效扩展到复杂的网络架构和任务。为此，我们在这里引入了结构投影Hebbian表示（SPHeRe），一种新的无监督学习方法，它通过一个局部的辅助非线性块整合了正交性和结构信息保留。结构信息保留的损失通过一个辅助的轻量级投影反向传播到输入，该投影在概念上充当反馈介导，而正交性约束则考虑了更新幅度的有界性。大量的实验结果表明，SPHeRe在CIFAR-10、CIFAR-100和Tiny-ImageNet等标准图像分类基准测试中，在无监督突触可塑性方法中达到了最先进的性能。此外，该方法在持续学习和迁移学习场景中表现出强大的有效性，图像重建任务显示了所提取特征的鲁棒性和泛化能力。这项工作证明了Hebbian无监督学习规则在现代深度学习框架中的竞争力和潜力，展示了不依赖于严格反向传播的高效和生物启发式学习算法的可能性。我们的代码可在https://github.com/brain-intelligence-lab/SPHeRe获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hebbian learning is a biological principle that intuitively describes howneurons adapt their connections through repeated stimuli. However, when appliedto machine learning, it suffers serious issues due to the unconstrained updatesof the connections and the lack of accounting for feedback mediation. Suchshortcomings limit its effective scaling to complex network architectures andtasks. To this end, here we introduce the Structural Projection HebbianRepresentation (SPHeRe), a novel unsupervised learning method that integratesorthogonality and structural information preservation through a local auxiliarynonlinear block. The loss for structural information preservationbackpropagates to the input through an auxiliary lightweight projection thatconceptually serves as feedback mediation while the orthogonality constraintsaccount for the boundedness of updating magnitude. Extensive experimentalresults show that SPHeRe achieves SOTA performance among unsupervised synapticplasticity approaches on standard image classification benchmarks, includingCIFAR-10, CIFAR-100, and Tiny-ImageNet. Furthermore, the method exhibits strongeffectiveness in continual learning and transfer learning scenarios, and imagereconstruction tasks show the robustness and generalizability of the extractedfeatures. This work demonstrates the competitiveness and potential of Hebbianunsupervised learning rules within modern deep learning frameworks,demonstrating the possibility of efficient and biologically inspired learningalgorithms without the strong dependence on strict backpropagation. Our code isavailable at https://github.com/brain-intelligence-lab/SPHeRe.</description>
      <author>example@mail.com (Shikuang Deng, Jiayuan Zhang, Yuhang Wu, Ting Chen, Shi Gu)</author>
      <guid isPermaLink="false">2510.14810v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>$\nabla$-SDF: Learning Euclidean Signed Distance Functions Online with Gradient-Augmented Octree Interpolation and Neural Residual</title>
      <link>http://arxiv.org/abs/2510.18999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为∇-SDF的混合方法，用于从点云数据估计符号距离函数(SDF)，结合了显式先验和隐式神经残差，实现了高效率、高准确性和可微性的SDF重建。&lt;h4&gt;背景&lt;/h4&gt;从点云数据估计符号距离函数(SDF)对机器人自主能力(如定位、建图、运动规划和控制)有很多好处。现有方法中，支持在线和大规模SDF重建的体积方法会影响SDF估计的连续性和可微性；而神经网络方法虽然能提供高保真度和可微的SDF重建，但效率较低，在大环境中可能面临灾难性遗忘和内存限制，且通常仅限于截断的SDF。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够实现非截断(欧几里得)SDF重建的方法，同时具有体积方法的计算和内存效率以及神经网络方法的可微性和准确性。&lt;h4&gt;方法&lt;/h4&gt;提出∇-SDF，一种混合方法，结合了从梯度增强八叉树插值获得的显式先验和隐式神经残差。&lt;h4&gt;主要发现&lt;/h4&gt;大量实验证明，∇-SDF在准确性和效率方面优于现有最先进的技术，为机器人技术和计算机视觉中的下游任务提供了可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;∇-SDF为机器人自主能力中的SDF估计提供了一个高效、准确且可微的解决方案，克服了现有方法的局限性，为下游任务提供了可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;从点云数据估计符号距离函数(SDF)已被证明有益于许多机器人自主能力，包括定位、建图、运动规划和控制。支持在线和大规模SDF重建的方法往往依赖于离散的体积数据结构，这会影响SDF估计的连续性和可微性。最近，使用隐式特征的神经网络方法展示了高保真度和可微的SDF重建，但它们往往效率较低，在大环境中可能会经历灾难性遗忘和内存限制，并且通常仅限于截断的SDF。本文提出了∇-SDF，一种混合方法，结合了从梯度增强八叉树插值获得的显式先验和隐式神经残差。我们的方法实现了非截断(欧几里得)的SDF重建，计算和内存效率与体积方法相当，可微性和准确性可与神经网络方法相媲美。大量实验证明，∇-SDF在准确性和效率方面优于最先进的技术，为机器人技术和计算机视觉中的下游任务提供了可扩展的解决方案。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决从点云数据在线学习欧几里得符号距离函数（SDF）的问题。这个问题在机器人自主和计算机视觉领域非常重要，因为准确且可微分的几何环境表示对机器人定位、建图、运动规划和控制等关键功能至关重要。快速更新环境模型和获取梯度信息能让机器人更安全、精确地导航和交互环境，而小内存占用对表示在大场景中的可扩展性很重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者分析了现有SDF重建方法的优缺点：体积方法实时性好但不可微；神经网络方法可微分但效率低且易遗忘；高斯过程方法连续但计算复杂。作者借鉴了H2-Mapping的八叉树先验和神经网络残差思想，以及HIO-SDF的全局SDF表示方法。作者设计了一种混合方法，结合显式八叉树先验和隐式神经残差，使用半稀疏八叉树结构和梯度增强插值提高精度，并设计了三种损失函数加速训练，从而克服了现有方法的局限性。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是结合显式八叉树先验和隐式神经残差的混合模型，实现高效、可微分且全局准确的SDF重建。整体流程包括：1)使用半稀疏八叉树存储SDF值和梯度，通过梯度增强插值获得SDF先验；2)使用多分辨率哈希网格编码器和MLP解码器预测SDF残差修正；3)选择关键帧保持训练数据代表性；4)生成表面点、扰动点和自由空间点三种训练样本；5)使用重建损失、Eikonal损失和投影损失训练模型；6)最终SDF预测为八叉树先验与神经网络残差之和。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1)梯度增强的八叉树插值方法，在顶点存储SDF值和梯度，提高先验精度；2)半稀疏八叉树结构，平衡内存和精度；3)混合显式-隐式模型，实现全空间而非仅近表面的SDF学习；4)三种精心设计的损失函数加速收敛。相比H2-Mapping，∇-SDF实现非截断SDF重建；相比HIO-SDF，直接优化八叉参数学习更准确先验；相比体积方法，提供可微SDF；相比纯神经网络方法，解决了大环境中的遗忘问题。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; ∇-SDF通过梯度增强的八叉树插值与神经残差相结合，实现了高效、可微分且全局准确的在线符号距离函数重建，结合了体积方法和神经网络方法的优点，同时克服了它们的局限性。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimation of signed distance functions (SDFs) from point cloud data has beenshown to benefit many robot autonomy capabilities, including localization,mapping, motion planning, and control. Methods that support online andlarge-scale SDF reconstruction tend to rely on discrete volumetric datastructures, which affect the continuity and differentiability of the SDFestimates. Recently, using implicit features, neural network methods havedemonstrated high-fidelity and differentiable SDF reconstruction but they tendto be less efficient, can experience catastrophic forgetting and memorylimitations in large environments, and are often restricted to truncated SDFs.This work proposes $\nabla$-SDF, a hybrid method that combines an explicitprior obtained from gradient-augmented octree interpolation with an implicitneural residual. Our method achieves non-truncated (Euclidean) SDFreconstruction with computational and memory efficiency comparable tovolumetric methods and differentiability and accuracy comparable to neuralnetwork methods. Extensive experiments demonstrate that \methodname{}outperforms the state of the art in terms of accuracy and efficiency, providinga scalable solution for downstream tasks in robotics and computer vision.</description>
      <author>example@mail.com (Zhirui Dai, Qihao Qian, Tianxing Fan, Nikolay Atanasov)</author>
      <guid isPermaLink="false">2510.18999v1</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>PAGE-4D: Disentangled Pose and Geometry Estimation for 4D Perception</title>
      <link>http://arxiv.org/abs/2510.17568v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PAGE-4D是一个扩展到动态场景的前馈模型，解决了现有3D前馈模型在处理动态元素时的局限性，通过动态感知聚合器实现了无需后处理的相机姿态估计、深度预测和点云重建。&lt;h4&gt;背景&lt;/h4&gt;最新的3D前馈模型（如VGGT）在推断静态场景的3D属性方面表现出色，但这些模型通常在静态数据集上训练，因此在涉及移动人类或可变形物体等复杂动态元素的真实场景中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;引入PAGE-4D模型，将VGGT扩展到动态场景，实现相机姿态估计、深度预测和点云重建，无需后处理。&lt;h4&gt;方法&lt;/h4&gt;提出一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息，对于姿态估计抑制运动线索，对于几何重建则增强这些线索，从而解决多任务4D重建中任务间的固有冲突。&lt;h4&gt;主要发现&lt;/h4&gt;PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越的结果。&lt;h4&gt;结论&lt;/h4&gt;PAGE-4D成功解决了多任务4D重建中任务之间的固有冲突，通过动态感知聚合器有效分离了静态和动态信息，在动态场景中表现优异。&lt;h4&gt;翻译&lt;/h4&gt;最近的3D前馈模型，如视觉几何基础变换器（VGGT），在推断静态场景的3D属性方面表现出强大能力。然而，由于它们通常在静态数据集上训练，这些模型在涉及复杂动态元素的真实场景中往往表现不佳，例如移动的人或像伞这样的可变形物体。为解决这一局限性，我们引入了PAGE-4D，一种将VGGT扩展到动态场景的前馈模型，能够实现相机姿态估计、深度预测和点云重建，且无需后处理。多任务4D重建的一个核心挑战是任务之间的内在冲突：准确的相机姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。为解决这一矛盾，我们提出了一种动态感知聚合器，通过预测动态感知掩码来解耦静态和动态信息——在姿态估计中抑制运动线索，而在几何重建中增强它们。大量实验表明，PAGE-4D在动态场景中始终优于原始VGGT，在相机姿态估计、单目和视频深度估计以及密集点图重建方面取得了优越结果。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何在动态场景（包含移动人或可变形物体如伞的场景）中进行准确的3D重建问题。这个问题在现实中非常重要，因为我们的世界本质上是动态的，包含大量移动的物体和人。能够在动态场景中进行准确的3D重建对于机器人导航、增强现实、自动驾驶、视频编辑等多个应用领域至关重要。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先观察到现有的静态3D重建模型在动态场景中表现不佳，尤其是在相机姿态估计和几何重建之间存在冲突：姿态估计需要抑制动态区域，而几何重建则需要建模这些区域。他们借鉴了VGGT作为基础模型，但针对动态场景进行了改进。通过分析VGGT在动态条件下的行为，发现它会忽略动态内容。基于这些观察，作者设计了一个动态感知聚合器，通过预测掩码来分离静态和动态信息，并采用针对性的微调策略，只更新对动态最敏感的中间层。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; PAGE-4D的核心思想是解耦动态区域在不同任务中的作用：在相机姿态估计时抑制动态区域，而在几何重建时利用这些区域的动态信息。整体实现流程包括：1) 使用预训练编码器提取图像特征；2) 通过动态感知聚合器整合空间和时间线索，包括帧间注意、帧内注意和动态感知全局注意；3) 使用轻量级解码器进行深度和点图重建；4) 专门的相机姿态估计解码器。特别的是，PAGE-4D预测一个动态掩码，通过交叉注意机制应用：过滤相机姿态令牌的动态内容，同时为几何令牌强调这些内容。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; PAGE-4D的关键创新点包括：1) 动态感知聚合器，通过预测掩码分离静态和动态信息；2) 针对性的微调策略，只更新对动态最敏感的中间层；3) 任务特定的动态处理，在不同任务中不同方式处理动态区域；4) 统一高效的框架，能在单一前向传递中同时完成多个任务。相比之前的工作，PAGE-4D不需要后处理，运行速度快，在动态场景中表现更好，能产生更密集和准确的点云重建。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; PAGE-4D通过解耦姿态和几何估计中的动态信息处理，首次实现了在单一前向模型中对动态场景的高效准确4D感知，显著超越了之前静态模型在动态环境中的表现。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent 3D feed-forward models, such as the Visual Geometry GroundedTransformer (VGGT), have shown strong capability in inferring 3D attributes ofstatic scenes. However, since they are typically trained on static datasets,these models often struggle in real-world scenarios involving complex dynamicelements, such as moving humans or deformable objects like umbrellas. Toaddress this limitation, we introduce PAGE-4D, a feedforward model that extendsVGGT to dynamic scenes, enabling camera pose estimation, depth prediction, andpoint cloud reconstruction -- all without post-processing. A central challengein multi-task 4D reconstruction is the inherent conflict between tasks:accurate camera pose estimation requires suppressing dynamic regions, whilegeometry reconstruction requires modeling them. To resolve this tension, wepropose a dynamics-aware aggregator that disentangles static and dynamicinformation by predicting a dynamics-aware mask -- suppressing motion cues forpose estimation while amplifying them for geometry reconstruction. Extensiveexperiments show that PAGE-4D consistently outperforms the original VGGT indynamic scenarios, achieving superior results in camera pose estimation,monocular and video depth estimation, and dense point map reconstruction.</description>
      <author>example@mail.com (Kaichen Zhou, Yuhan Wang, Grace Chen, Xinhai Chang, Gaspard Beaudouin, Fangneng Zhan, Paul Pu Liang, Mengyu Wang)</author>
      <guid isPermaLink="false">2510.17568v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    <item>
      <title>VO-DP: Semantic-Geometric Adaptive Diffusion Policy for Vision-Only Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2510.15530v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了一种名为VO-DP的纯视觉单视角扩散策略学习方法，利用预训练视觉基础模型融合语义和几何特征，在模拟和真实世界任务中均表现出色，并开源了机器人操作训练库。&lt;h4&gt;背景&lt;/h4&gt;在模仿学习中，基于视觉运动的扩散策略学习是机器人操作的主要方向。现有方法大多依赖点云作为输入并通过点云特征学习构建场景表示，但对纯视觉解决方案的探索不足。&lt;h4&gt;目的&lt;/h4&gt;探索一种纯视觉且单视角的扩散策略学习方法，以克服对点云输入的依赖，并发挥视觉基础模型在机器人操作中的潜力。&lt;h4&gt;方法&lt;/h4&gt;提出VO-DP方法，利用VGGT的中间特征结合DINOv2的语义特征和交替注意力块的几何特征，通过交叉注意力融合特征，并用CNN空间压缩后输入策略头。同时开源基于Accelerate的机器人操作训练库，支持多GPU并行训练和混合精度训练。&lt;h4&gt;主要发现&lt;/h4&gt;模拟任务中VO-DP成功率达64.6%，与点云方法DP3(64.0%)相当，远高于基线DP(34.8%)；真实世界任务中达到87.9%，显著优于DP3(67.5%)和DP(11.2%)。VO-DP在颜色、尺寸、背景和光照变化条件下保持高度稳定。&lt;h4&gt;结论&lt;/h4&gt;VO-DP证明了纯视觉解决方案在机器人操作中的巨大潜力，特别是在真实世界任务中表现出色。开源的训练库为机器人操作研究提供了有价值的资源。&lt;h4&gt;翻译&lt;/h4&gt;在模仿学习背景下，基于视觉运动的扩散策略学习是机器人操作的主要方向之一。大多数方法依赖点云作为观察输入，通过点云特征学习构建场景表示，实现显著准确性。然而，现有文献缺乏对具有巨大潜力的纯视觉解决方案的深入探索。本文提出纯视觉和单视角扩散策略学习方法(VO-DP)，利用预训练视觉基础模型实现语义和几何特征有效融合。使用VGGT中间特征，结合DINOv2语义特征和交替注意力块几何特征。特征通过交叉注意力融合，用CNN空间压缩后输入策略头。大量实验表明，VO-DP不仅显著优于纯视觉基线DP，且与点云方法DP3表现不同：模拟任务中VO-DP平均成功率达64.6%，与DP3的64.0%相当，远高于DP的34.8%；真实世界任务中达87.9%，显著优于DP3的67.5%和DP的11.2%。进一步鲁棒性评估证实VO-DP在颜色、尺寸、背景和光照变化条件下保持高度稳定。最后开源机器人操作训练库，基于Accelerate构建，支持多机器多GPU并行训练和混合精度训练，兼容DP、DP3和VO-DP等视觉运动策略，支持RoboTwin模拟器。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 这篇论文主要解决机器人操作领域中纯视觉（仅RGB图像）模仿学习方法性能不足的问题。这个问题很重要，因为现有基于点云或RGB-D图像的方法虽然精度高，但依赖昂贵的深度传感器，而纯视觉方法成本低、实用性强，但性能通常不如基于点云的方法。探索纯视觉方法的潜力可以显著降低机器人系统的硬件成本和复杂度，避免多传感器校准问题，并更接近生物感知-行动系统，具有广泛的应用前景。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的优缺点，指出纯视觉方法的性能瓶颈主要在于表示学习模块不完善。他们借鉴了多项现有工作：利用预训练的VGGT模型提取几何信息，使用DINOv2提取语义特征，采用DP中的扩散策略框架，以及Transformer中的cross-attention机制进行特征融合。在此基础上，他们创新设计了语义-几何自适应融合模块和空间特征压缩模块，实现了从单目RGB图像中同时提取和融合语义与几何信息，为下游策略学习提供高质量输入。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是利用预训练的视觉基础模型，从单目RGB图像中同时提取语义和几何特征，并通过自适应融合机制将这些特征有效结合，为下游策略学习提供高质量的输入。整体流程包括：1) 输入处理：接收单视图RGB图像序列；2) 特征提取：使用DINOv2提取语义特征，用VGGT的Alternating Attention网络提取几何特征；3) 特征融合：通过残差交叉注意力机制自适应融合语义和几何特征；4) 场景表示压缩：使用轻量级ResNet压缩融合后的特征，并与机器人关节状态连接形成场景表示；5) 动作生成：基于DDPM的策略头根据场景表示生成动作轨迹。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 关键创新点包括：1) 首次实现纯视觉方法达到点云级别的性能；2) 创新设计语义-几何自适应融合机制；3) 高效的单视图表示学习方法；4) 开源DRRM训练框架。相比之前的工作：1) 相比传统纯视觉方法（如DP），性能显著提升；2) 相比基于点云的方法（如DP3），不需要昂贵深度传感器，在真实世界任务中表现更好；3) 相比其他纯视觉方法，更注重语义和几何特征的融合，在复杂场景中表现更好，对环境变化具有更强鲁棒性。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; VO-DP通过创新性地融合预训练视觉模型的语义和几何特征，首次实现了仅使用RGB图像的机器人操作方法达到与基于点云方法相当的精度，同时大幅降低了硬件成本和系统复杂度。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-10-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the context of imitation learning, visuomotor-based diffusion policylearning is one of the main directions in robotic manipulation. Most of theseapproaches rely on point clouds as observation inputs and construct scenerepresentations through point clouds feature learning, which enables them toachieve remarkable accuracy. However, the existing literature lacks an in-depthexploration of vision-only solutions that have significant potential. In thispaper, we propose a Vision-Only and single-view Diffusion Policy learningmethod (VO-DP) that leverages pretrained visual foundation models to achieveeffective fusion of semantic and geometric features. We utilize intermediatefeatures from VGGT incorporating semantic features from DINOv2 and geometricfeatures from Alternating Attention blocks. Features are fused viacross-attention and spatially compressed with a CNN to form the input to thepolicy head. Extensive experiments demonstrate that VO-DP not only outperformsthe vision-only baseline DP significantly but also exhibits distinctperformance trends against the point cloud-based method DP3: in simulationtasks, VO-DP achieves an average success rate of 64.6% on par with DP3 64.0%and far higher than DP 34.8%, while in real-world tasks, it reaches 87.9%,outperforming both DP3 67.5% and DP 11.2% by a notable margin. Furtherrobustness evaluations confirm that VO-DP remains highly stable under varyingconditions including color, size, background, and lighting. Lastly, weopen-source a training library for robotic manipulation. Built on Accelerate,this library supports multi-machine and multi-GPU parallel training, as well asmixed precision training. It is compatible with visuomotor policies such as DP,DP3 and VO-DP, and also supports the RoboTwin simulator.</description>
      <author>example@mail.com (Zehao Ni, Yonghao He, Lingfeng Qian, Jilei Mao, Fa Fu, Wei Sui, Hu Su, Junran Peng, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2510.15530v2</guid>
      <pubDate>Thu, 23 Oct 2025 14:31:02 +0800</pubDate>
    </item>
    </channel>
</rss>