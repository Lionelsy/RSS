<?xml version='1.0' encoding='utf-8'?>
<rss version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Mon, 10 Mar 2025 17:10:46 +0800</lastBuildDate>
    <item>
      <title>MedFuncta: Modality-Agnostic Representations Based on Efficient Neural Fields</title>
      <link>http://arxiv.org/abs/2502.14401v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page: https://pfriedri.github.io/medfuncta-io/ Code and  Dataset: https://github.com/pfriedri/medfuncta/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种新的医学图像分析的数据表示方式MedFuncta，该方式基于神经场并可以应用于不同的医学信号。研究展示了如何通过利用医疗信号中的冗余信息和使用高效的元学习方法来扩展神经场的规模。&lt;h4&gt;背景介绍&lt;/h4&gt;当前医学影像分析的研究几乎完全集中在网格或体素数据表示上。&lt;h4&gt;主要目的&lt;/h4&gt;引入MedFuncta作为无模态依赖的连续数据表示，挑战传统的数据表示选择，并通过优化神经网络激活函数来解决谱偏置问题。&lt;h4&gt;方法描述&lt;/h4&gt;提出了一种基于SIREN激活函数和ω_0调度策略的方法，以改善重建质量和加速收敛速度。研究还展示了如何从单一实例扩展到大规模数据集的方法。&lt;h4&gt;主要发现&lt;/h4&gt;在多种一维至三维的医学信号（如心电图、胸部X光片、视网膜OCT图像等）上验证了该方法的有效性，并且可以解决下游任务，例如病灶检测和分割。&lt;h4&gt;结论&lt;/h4&gt;成功地展示了MedFuncta能够在不同维度和模态的医疗信号中有效表示信息，解决了传统的数据表示所面临的挑战，并促进了相关领域的研究发展。此外还发布了包含超过550k个注释神经场的大规模数据集以促进该方向的研究。&lt;h4&gt;翻译摘要&lt;/h4&gt;近期基于深度学习的医学影像分析主要关注网格或体素型的数据表示形式。本文通过引入一种基于神经网络字段的模态无关连续数据表示——MedFuncta，对这一普遍选择提出了挑战。本文展示了如何通过利用医疗信号中的冗余信息和使用高效的元学习方法来扩展从单例到大规模数据集的神经网络规模。此外，为了克服常用的SIREN激活函数存在的谱偏差问题，我们引入了ω_0调度策略，从而改进了重建质量和收敛速度。我们在一系列具有不同维度和模态（1D：心电图；2D：胸部X光片、视网膜OCT图像、眼底相机、皮肤镜、结肠组织病理学、细胞显微镜；3D：脑MRI、肺CT）的医学信号上验证了我们提出的方法，并成功展示了在这些表示形式中解决相关下游任务的可能性。此外，为了促进该方向的研究发展，我们还发布了一个包含超过550k个注释神经场的大规模数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/pfriedri/medfuncta&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research in medical image analysis with deep learning almostexclusively focuses on grid- or voxel-based data representations. We challengethis common choice by introducing MedFuncta, a modality-agnostic continuousdata representation based on neural fields. We demonstrate how to scale neuralfields from single instances to large datasets by exploiting redundancy inmedical signals and by applying an efficient meta-learning approach with acontext reduction scheme. We further address the spectral bias in commonly usedSIREN activations, by introducing an $\omega_0$-schedule, improvingreconstruction quality and convergence speed. We validate our proposed approachon a large variety of medical signals of different dimensions and modalities(1D: ECG; 2D: Chest X-ray, Retinal OCT, Fundus Camera, Dermatoscope, ColonHistopathology, Cell Microscopy; 3D: Brain MRI, Lung CT) and successfullydemonstrate that we can solve relevant downstream tasks on theserepresentations. We additionally release a large-scale dataset of &gt; 550kannotated neural fields to promote research in this direction.</description>
      <author>example@mail.com (Paul Friedrich, Florentin Bieder, Philippe C. Cattin)</author>
      <guid isPermaLink="false">2502.14401v2</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
  <item>
      <title>Trial by FIRE: Probing the dark matter density profile of dwarf galaxies with GraphNPE</title>
      <link>http://arxiv.org/abs/2503.03812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MNRAS. Comments welcomed. 20 + 12 pages, 13 + 11  figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究使用GraphNPE框架对FIRE-2 Latte模拟中的卫星矮星系进行暗物质分布分析，并展示了该方法在冷暗物质和自相互作用暗物质场景下的优越性能。&lt;h4&gt;背景&lt;/h4&gt;暗物质的分布为结构形成和粒子性质提供了重要见解，而GraphNPE结合了图神经网络和归一化流以从视线恒星速度中推断出暗物质密度剖面。&lt;h4&gt;目的&lt;/h4&gt;应用GraphNPE框架于FIRE-2 Latte模拟中的卫星矮星系，并在冷暗物质和自相互作用暗物质场景下测试其效果。&lt;h4&gt;方法&lt;/h4&gt;将GraphNPE应用于卫星矮星系，利用该方法从视线速度中推断出暗物质密度剖面。此外，研究了质量和峰值旋转速度参数的约束情况。&lt;h4&gt;主要发现&lt;/h4&gt;与传统Jeans方法相比，GraphNPE在精度上有显著提高，并能在系统具有少于30个追踪器的情况下恢复到95%置信水平；能够可靠地恢复质量和峰值旋转速度参数；准确度分别达到10-20%和0.1-0.4 dex。&lt;h4&gt;结论&lt;/h4&gt;GraphNPE作为一种稳健的工具，在推断矮星系中的暗物质密度剖面方面表现出色，为限制暗物质模型提供了有前景的方法。该框架还能应用于非球形和不平衡模型，展示了基于模拟推理和图学习在天体物理学中的广泛实用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中所述的研究内容包括使用GraphNPE框架进行的FIRE-2 Latte卫星矮星系模拟实验及其对冷暗物质及自相互作用暗物质场景下暗物质分布的分析。研究通过这种方法从视线速度数据推断出暗物质密度剖面，展示了该方法相对于传统方式的优势，并且在质量和旋转速度参数约束方面取得了重要发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Dark Matter (DM) distribution in dwarf galaxies provides crucial insightsinto both structure formation and the particle nature of DM. GraphNPE (GraphNeural Posterior Estimator), first introduced in Nguyen et al. (2023), is anovel simulation-based inference framework that combines graph neural networksand normalizing flows to infer the DM density profile from line-of-sightstellar velocities. Here, we apply GraphNPE to satellite dwarf galaxies in theFIRE-2 Latte simulation suite of Milky Way-mass halos, testing it against bothCold and Self-Interacting DM scenarios. Our method demonstrates superiorprecision compared to conventional Jeans-based approaches, recovering DMdensity profiles to within the 95% confidence level even in systems with as fewas 30 tracers. Moreover, we present the first evaluation of mass modelingmethods in constraining two key parameters from realistic simulations: the peakcircular velocity, $V_\mathrm{max}$, and the peak virial mass,$M_\mathrm{200m}^\mathrm{peak}$. Using only line-of-sight velocities, GraphNPEcan reliably recover both $V_\mathrm{max}$ and $M_\mathrm{200m}^\mathrm{peak}$within our quoted uncertainties, including those experiencing tidal effects($\gtrsim$ 63% of systems are recovered with our 68% confidence intervals and$\gtrsim$ 92% within our 95% confidence intervals). The method achieves 10-20%accuracy in $V_\mathrm{max}$ recovery, while $M_\mathrm{200m}^\mathrm{peak}$ isrecovered to 0.1-0.4 dex accuracy. This work establishes GraphNPE as a robusttool for inferring DM density profiles in dwarf galaxies, offering promisingavenues for constraining DM models. The framework's potential extends beyondthis study, as it can be adapted to non-spherical and disequilibrium models,showcasing the broader utility of simulation-based inference and graph-basedlearning in astrophysics.</description>
      <author>example@mail.com (Tri Nguyen, Justin Read, Lina Necib, Siddharth Mishra-Sharma, Claude-André Faucher-Giguère, Andrew Wetzel)</author>
      <guid isPermaLink="false">2503.03812v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Instrument-Splatting: Controllable Photorealistic Reconstruction of Surgical Instruments Using Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2503.04082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种名为Instrument-Splatting的新型Real2Sim方法，该方法利用3D高斯点云技术实现单目内窥镜视频中手术器械的全三维重建。&lt;h4&gt;背景&lt;/h4&gt;随着外科人工智能和自主性的发展，将真实场景转换为模拟环境（Real2Sim）变得越来越重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的Real2Sim方法来解决单目内窥镜视频中的手术器械全三维重建问题，并确保视觉逼真度和操作性。&lt;h4&gt;方法&lt;/h4&gt;{'技术核心': '利用3D高斯点云技术和Gaussian Splatting实现从单目内窥镜视频中重建手术器械的完整3D模型。', '几何预训练': '引入了几何预训练，通过将具有准确几何先验的Gaussian点云绑定在部件网格上来保持视觉真实度和可操作性。', '前向动力学定义': '定义了一种前向动力学方法来控制Gaussians如同真正的手术器械一样灵活。', '未标记视频处理': '设计了一种利用语义嵌入的高斯点的新颖仪器姿态跟踪方法，用于逐帧精炼未标记视频中的工具姿态和关节状态，确保真实感渲染。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'验证': '在2份公开发布的手术视频及4份采集自离体组织和绿幕背景下的视频上进行了验证。', '效果评估': '定量和定性评价显示了所提出方法的有效性和优越性。'}&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种有效的方法来解决从单目内窥镜视频中重建手术器械的全三维模型的问题，同时保持视觉真实度和操作性。&lt;h4&gt;翻译&lt;/h4&gt;随着外科人工智能和自主性的迅速发展，Real2Sim（将现实场景转换为模拟环境）变得越来越重要。本文提出了一种新的Real2Sim方法——Instrument-Splatting，该方法利用3D高斯点云技术从单目内窥镜视频中提供手术器械的全三维重建。为了保持高度视觉真实度和操作性，我们引入了几何预训练，将准确的几何先验绑定到部件网格上的Gaussian点云，并定义了一种前向动力学来像真正的工具那样灵活地控制高斯分布。为处理未标记视频，设计了一种利用语义嵌入的Gaussians的新颖仪器姿态跟踪方法，在渲染和比较方式下稳健地精炼每个帧中的工具姿态和关节状态，使得我们的工具Gaussian能够精确学习纹理并实现逼真的照片级渲染。我们在2个公开发布的手术视频和4个在离体组织及绿幕背景上收集的视频中验证了该方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real2Sim is becoming increasingly important with the rapid development ofsurgical artificial intelligence (AI) and autonomy. In this work, we propose anovel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3DGaussian Splatting to provide fully controllable 3D reconstruction of surgicalinstruments from monocular surgical videos. To maintain both high visualfidelity and manipulability, we introduce a geometry pre-training to bindGaussian point clouds on part mesh with accurate geometric priors and define aforward kinematics to control the Gaussians as flexible as real instruments.Afterward, to handle unposed videos, we design a novel instrument pose trackingmethod leveraging semantics-embedded Gaussians to robustly refine per-frameinstrument poses and joint states in a render-and-compare manner, which allowsour instrument Gaussian to accurately learn textures and reach photorealisticrendering. We validated our method on 2 publicly released surgical videos and 4videos collected on ex vivo tissues and green screens. Quantitative andqualitative evaluations demonstrate the effectiveness and superiority of theproposed method.</description>
      <author>example@mail.com (Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin)</author>
      <guid isPermaLink="false">2503.04082v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Constrained Monocular Scale Estimation Using Semantic Segmentation for Dynamic Scenes</title>
      <link>http://arxiv.org/abs/2503.04235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一个创新的单目视觉定位策略，结合了深度学习模型与几何约束来解决传统的单目视觉里程计在尺度估计中的挑战。&lt;h4&gt;背景&lt;/h4&gt;单目视觉定位在高级驾驶员辅助系统和自动驾驶中至关重要。然而，传统方法难以处理动态物体并有效管理计算复杂性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合SegNeXt模型的混合方法，用于实时应用中的自我运动估计和地面点选择，并优化尺度恢复过程。&lt;h4&gt;方法&lt;/h4&gt;{'深度学习模型': '使用SegNeXt模型进行实时的应用场景下的自我运动估计和地面点选择', '动态物体处理': '利用动态物体掩码来消除不稳定的特征，提高鲁棒性', '几何约束': '结合道路区域的几何约束恢复尺度信息', 'ORB-SLAM3集成': '将该方法与单目ORB-SLAM3相结合以精确估计路面模型'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能提升': '实验结果表明，相较于现有单目视觉里程计算法及当代尺度恢复方法，所提出的方法在准确性上具有显著优势', '鲁棒性提高': '通过动态物体掩码和几何约束的引入提高了系统的鲁棒性和精确度'}&lt;h4&gt;结论&lt;/h4&gt;该研究为解决单目视觉定位中的挑战提供了新的思路，并且展示了其在实际应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于利用深度学习模型以及几何约束来优化单目视觉里程计的方法的研究，提出了一种结合SegNeXt模型的新方法，在路面模型的准确估计上取得了突破性的进展。实验结果证明该方法在鲁棒性和精确度上有明显优势，并且优于现有的视觉里程计算法和尺度恢复技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular visual localization plays a pivotal role in advanced driverassistance systems and autonomous driving by estimating a vehicle's ego-motionfrom a single pinhole camera. Nevertheless, conventional monocular visualodometry encoun-ters challenges in scale estimation due to the absence of depthinformation during projection. Previous methodologies, whether rooted inphysical constraints or deep learning paradigms, con-tend with issues relatedto computational complexity and the management of dynamic objects. This studyextends our prior research, presenting innovative strategies for ego-motionestima-tion and the selection of ground points. Striving for a nuancedequilibrium between computational efficiency and precision, we propose a hybridmethod that leverages the SegNeXt model for real-time applications,encompassing both ego-motion estimation and ground point selection. Ourmethodology incorporates dy-namic object masks to eliminate unstable featuresand employs ground plane masks for meticulous triangulation. Furthermore, weexploit Geometry-constraint to delineate road regions for scale recovery. Theintegration of this approach with the mo-nocular version of ORB-SLAM3culminates in the accurate esti-mation of a road model, a pivotal component inour scale recov-ery process. Rigorous experiments, conducted on the KITTIda-taset, systematically compare our method with existing monocu-lar visualodometry algorithms and contemporary scale recovery methodologies. The resultsundeniably confirm the superior ef-fectiveness of our approach, surpassingstate-of-the-art visual odometry algorithms. Our source code is available athttps://git hub.com/bFr0zNq/MVOSegScale.</description>
      <author>example@mail.com (Hui Zhang, Zhiyang Wu, Qianqian Shangguan, Kang An)</author>
      <guid isPermaLink="false">2503.04235v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>TrajectoryCrafter: Redirecting Camera Trajectory for Monocular Videos via Diffusion Models</title>
      <link>http://arxiv.org/abs/2503.05638v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://trajectorycrafter.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;TrajectoryCrafter 是一种新颖的方法，用于单目视频的相机轨迹重定向。&lt;h4&gt;背景&lt;/h4&gt;当前方法通常依赖于稀疏的多视角视频来实现相机轨迹的重新定向。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够精确控制用户指定的相机轨迹的新方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的双流条件视频扩散模型，该模型同时将点云渲染和源视频作为条件输入，确保准确的视图转换和连贯的四维内容生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过创新性的双重投影策略，使用结合了网络规模单目视频与静态多视角数据集的混合训练数据集，提高了在各种场景中的鲁棒性泛化能力。&lt;h4&gt;结论&lt;/h4&gt;在多视角和大规模单目视频上的广泛评估表明了我们方法的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们介绍了 TrajectoryCrafter，这是一种用于重定向单目视频中相机轨迹的新颖方法。通过分离确定性的视图转换与随机的内容生成，我们的方法能够实现对用户指定的相机路径的精确控制。我们提出了一种新颖的双流条件视频扩散模型，该模型同时将点云渲染和源视频作为条件输入，确保准确的视图转换和连贯的四维内容生成。通过创新性的双重投影策略，我们没有依赖稀缺的多视角视频，而是构建了一个结合了网络规模单目视频与静态多视角数据集的混合训练数据集，显著增强了在各种场景中的鲁棒性泛化能力。对多视角和大规模单目视频进行广泛的评估表明了我们的方法具有优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present TrajectoryCrafter, a novel approach to redirect cameratrajectories for monocular videos. By disentangling deterministic viewtransformations from stochastic content generation, our method achieves precisecontrol over user-specified camera trajectories. We propose a novel dual-streamconditional video diffusion model that concurrently integrates point cloudrenders and source videos as conditions, ensuring accurate view transformationsand coherent 4D content generation. Instead of leveraging scarce multi-viewvideos, we curate a hybrid training dataset combining web-scale monocularvideos with static multi-view datasets, by our innovative double-reprojectionstrategy, significantly fostering robust generalization across diverse scenes.Extensive evaluations on multi-view and large-scale monocular videosdemonstrate the superior performance of our method.</description>
      <author>example@mail.com (Mark YU, Wenbo Hu, Jinbo Xing, Ying Shan)</author>
      <guid isPermaLink="false">2503.05638v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Removing Geometric Bias in One-Class Anomaly Detection with Adaptive Feature Perturbation</title>
      <link>http://arxiv.org/abs/2503.05520v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in WACV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的异常检测方法，利用预训练模型的特征空间生成伪异常样本，并通过适应性线性特征扰动技术优化噪声分布。&lt;h4&gt;背景&lt;/h4&gt;一类别异常检测旨在识别不属于预先定义正常类别的对象。由于实践中训练数据缺乏真实异常样本，大多数方法依赖于从正常图像中合成的伪异常样本进行训练。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的生成伪异常样本的方法来克服现有技术在一般条件下的限制，并优化模型对正常数据结构的理解。&lt;h4&gt;方法&lt;/h4&gt;利用预训练模型冻结后的特征空间，采用新型适应性线性特征扰动技术生成伪异常特征。该技术根据每个样本调整噪声分布，进行衰减的线性干扰，并通过对比学习目标指导分类过程。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在标准和无几何偏差的数据集上，本文方法优于其他基线方法。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法在克服现有异常检测模型限制方面显示出潜力，并且代码已经在公共仓库中开放访问。&lt;h4&gt;翻译&lt;/h4&gt;单类异常检测的目标是识别不属于预定义正常类别中的对象。实践中训练数据缺乏真实异常样本；因此，最先进的方法被训练来区分正常的和通过合成生成的伪异常数据。大多数方法使用数据增强技术从正常图像模拟异常情况。然而，表现最好的一些隐含利用了基准测试集中的几何偏差，这限制了它们在更一般条件下的适用性。其他则依赖于基本噪声方案，这些可能无法有效捕捉正常数据结构的本质。此外，多数情况下仍倾向于通过仅基于正常类别的端到端模型生成伪异常图像，并忽略信息的丰富表示形式。为克服这些局限性，我们考虑利用预训练模型提供的冻结且丰富的特征空间，并使用一种新颖的适应性线性特征扰动技术创建伪异常特征。该技术根据每一样本调整噪声分布，对特征向量施加衰减线性干扰，并进一步通过对比学习目标引导分类过程。在标准数据集和无几何偏差的数据集上进行的实验评估表明了我们方法相对于可比基线的优势。代码库可通过我们的公共仓库访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One-class anomaly detection aims to detect objects that do not belong to apredefined normal class. In practice training data lack those anomaloussamples; hence state-of-the-art methods are trained to discriminate betweennormal and synthetically-generated pseudo-anomalous data. Most methods use dataaugmentation techniques on normal images to simulate anomalies. However thebest-performing ones implicitly leverage a geometric bias present in thebenchmarking datasets. This limits their usability in more general conditions.Others are relying on basic noising schemes that may be suboptimal in capturingthe underlying structure of normal data. In addition most still favour theimage domain to generate pseudo-anomalies training models end-to-end from onlythe normal class and overlooking richer representations of the information. Toovercome these limitations we consider frozen yet rich feature spaces given bypretrained models and create pseudo-anomalous features with a novel adaptivelinear feature perturbation technique. It adapts the noise distribution to eachsample applies decaying linear perturbations to feature vectors and furtherguides the classification process using a contrastive learning objective.Experimental evaluation conducted on both standard and geometric bias-freedatasets demonstrates the superiority of our approach with respect tocomparable baselines. The codebase is accessible via our public repository.</description>
      <author>example@mail.com (Romain Hermary, Vincent Gaudillière, Abd El Rahman Shabayek, Djamila Aouada)</author>
      <guid isPermaLink="false">2503.05520v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>CACTUS: An Open Dataset and Framework for Automated Cardiac Assessment and Classification of Ultrasound Images Using Deep Transfer Learning</title>
      <link>http://arxiv.org/abs/2503.05604v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文介绍了一种用于自动分类和评估心脏超声图像的深度学习框架，并提出了首个公开评级数据集CACTUS。&lt;h4&gt;背景&lt;/h4&gt;心脏超声扫描是心脏病诊断的重要工具，然而由于医疗数据有限，机器学习在这一领域的应用受限。&lt;h4&gt;目的&lt;/h4&gt;开发一种能辅助医学专业人士进行心脏超声图像分类和评估的技术解决方案。&lt;h4&gt;方法&lt;/h4&gt;{'引入的数据集': 'CACTUS，包含了从心脏模拟器获取的各种视角和不同质量水平的图像。', '构建的框架': '由两个主要组件组成：第一个基于卷积神经网络(CNN)的心脏超声图像视图分类系统；第二个使用迁移学习(TL)，利用前一个组件的知识来创建用于评级和评估心脏图像模型。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能表现': '框架在分类任务中达到了99.43%的准确率，在评级任务中的误差低至0.3067。', '鲁棒性测试': '通过使用新视角的心脏图象进一步微调框架，并将其与现有架构进行比较，展示了其强大的通用性和适应性。', '专家问卷评估': '心脏专家通过对该框架处理实时扫描结果的问卷反馈，证实了它的实用性。'}&lt;h4&gt;结论&lt;/h4&gt;所提出的深度学习框架和CACTUS数据集为自动分类和评估心脏超声图像提供了有效的解决方案，并展示了在心脏病学中的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;心脏超声（US）检查是诊断心脏健康状况的常用技术，因此考虑自动化这些任务并帮助医疗专业人士进行心脏超声图象的分类与评估至关重要。机器学习方法因其成功应用于提升医学领域而被视为解决之道，尤其是在应对超声技师短缺问题上表现突出。然而，由于缺乏医学数据，特别是在心脏超声图像方面，机器学习的应用受到了很大限制。本文通过提出首个公开评级的数据集CACTUS来解决这一挑战，并提供了一个深度学习框架，其中包括两个主要部分：第一部分使用卷积神经网络对心脏视图进行分类；第二部分则采用迁移学习技术微调知识以创建用于评价和评估心脏图像的模型。该框架在分类与评分任务上表现卓越，在实际扫描结果处理上的有效性也得到了心脏病专家的好评。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiac ultrasound (US) scanning is a commonly used techniques in cardiologyto diagnose the health of the heart and its proper functioning. Therefore, itis necessary to consider ways to automate these tasks and assist medicalprofessionals in classifying and assessing cardiac US images. Machine learning(ML) techniques are regarded as a prominent solution due to their success innumerous applications aimed at enhancing the medical field, includingaddressing the shortage of echography technicians. However, the limitedavailability of medical data presents a significant barrier to applying ML incardiology, particularly regarding US images of the heart. This paper addressesthis challenge by introducing the first open graded dataset for CardiacAssessment and ClassificaTion of UltraSound (CACTUS), which is availableonline. This dataset contains images obtained from scanning a CAE Blue Phantomand representing various heart views and different quality levels, exceedingthe conventional cardiac views typically found in the literature. Additionally,the paper introduces a Deep Learning (DL) framework consisting of two maincomponents. The first component classifies cardiac US images based on the heartview using a Convolutional Neural Network (CNN). The second component usesTransfer Learning (TL) to fine-tune the knowledge from the first component andcreate a model for grading and assessing cardiac images. The frameworkdemonstrates high performance in both classification and grading, achieving upto 99.43% accuracy and as low as 0.3067 error, respectively. To showcase itsrobustness, the framework is further fine-tuned using new images representingadditional cardiac views and compared to several other state-of-the-artarchitectures. The framework's outcomes and performance in handling real-timescans were also assessed using a questionnaire answered by cardiac experts.</description>
      <author>example@mail.com (Hanae Elmekki, Ahmed Alagha, Hani Sami, Amanda Spilkin, Antonela Mariel Zanuttini, Ehsan Zakeri, Jamal Bentahar, Lyes Kadem, Wen-Fang Xie, Philippe Pibarot, Rabeb Mizouni, Hadi Otrok, Shakti Singh, Azzam Mourad)</author>
      <guid isPermaLink="false">2503.05604v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Fairness-Aware Low-Rank Adaptation Under Demographic Privacy Constraints</title>
      <link>http://arxiv.org/abs/2503.05684v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了低秩适应（LoRA）在预训练模型中的公平性问题，并提出了一系列无需直接访问敏感属性或其预测器的分布式微调方法。&lt;h4&gt;背景&lt;/h4&gt;现有的公平性感知微调方法依赖于对敏感属性或其预测器的直接访问，但在实践中这些信息往往受到严格的隐私保护，无法用于开发公平模型。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一难题，论文提出了一种基于LoRA的方法集合，允许模型开发者和公平审计员在不共享敏感数据的情况下进行分布式微调，并评估了三种方法的效果。&lt;h4&gt;方法&lt;/h4&gt;研究中使用的是敏感属性遗忘、对抗训练以及正交性损失这三种方法。实验是在CelebA和UTK-Face数据集上利用ImageNet预训练的ViT-Bas模型进行的。&lt;h4&gt;主要发现&lt;/h4&gt;正交性损失在减少偏差的同时保持或提高了模型效用；对抗训练虽然在一些情况下改善了假阳性率平等性和人口统计学公平性，但敏感属性遗忘没有明显的好处。对于存在显著偏见的任务，分布式公平感知微调方法可以在不侵犯消费者隐私的情况下消除偏见，并且大多数情况还能提高模型的实用性。&lt;h4&gt;结论&lt;/h4&gt;通过引入这些新的微调策略，研究展示了如何在保护用户隐私的同时实现更加公平和实用的机器学习模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-trained foundation models can be adapted for specific tasks usingLow-Rank Adaptation (LoRA). However, the fairness properties of these adaptedclassifiers remain underexplored. Existing fairness-aware fine-tuning methodsrely on direct access to sensitive attributes or their predictors, but inpractice, these sensitive attributes are often held under strict consumerprivacy controls, and neither the attributes nor their predictors are availableto model developers, hampering the development of fair models. To address thisissue, we introduce a set of LoRA-based fine-tuning methods that can be trainedin a distributed fashion, where model developers and fairness auditorscollaborate without sharing sensitive attributes or predictors. In this paper,we evaluate three such methods - sensitive unlearning, adversarial training,and orthogonality loss - against a fairness-unaware baseline, using experimentson the CelebA and UTK-Face datasets with an ImageNet pre-trained ViT-Basemodel. We find that orthogonality loss consistently reduces bias whilemaintaining or improving utility, whereas adversarial training improves FalsePositive Rate Parity and Demographic Parity in some cases, and sensitiveunlearning provides no clear benefit. In tasks where significant biases arepresent, distributed fairness-aware fine-tuning methods can effectivelyeliminate bias without compromising consumer privacy and, in most cases,improve model utility.</description>
      <author>example@mail.com (Parameswaran Kamalaruban, Mark Anderson, Stuart Burrell, Maeve Madigan, Piotr Skalski, David Sutton)</author>
      <guid isPermaLink="false">2503.05684v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Joint 3D Point Cloud Segmentation using Real-Sim Loop: From Panels to Trees and Branches</title>
      <link>http://arxiv.org/abs/2503.05630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;现代果园采用结构化行种植，并划分为独立的面板区域，以提高管理水平。准确且高效的Panel到Tree和Branch（P2TB）点云联合分割对于机器人操作至关重要。&lt;h4&gt;背景&lt;/h4&gt;目前大多数分段方法专注于单一实例分割并且依赖于一系列深度网络来执行联任务。这阻碍了嵌入在数据中的层级信息的使用，导致错误累积以及注释和计算成本增加，限制了其应用于实际场景的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法，结合Real2Sim L-TreeGen用于训练数据生成，并设计了一个联合模型（J-P2TB）以完成P2TB任务。该方法旨在解决现有分割技术的局限性，提高效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;采用L-TreeGen进行仿真数据集的生成，基于此训练得到的J-P2TB模型通过零样本学习对真实面板点云进行联合分割。&lt;h4&gt;主要发现&lt;/h4&gt;相比代表性方法，在大多数分割指标上表现更好且参数量减少40%，证明了L-TreeGen在模型训练中的有效性以及J-P2TB模型在联合分割任务中的优异性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的Sim2Real结果强调了所提技术的准确性和效率，对于机器人自动化果园操作的发展和数字孪生技术的进步都有重大影响。&lt;h4&gt;翻译&lt;/h4&gt;现代果园采用结构化行种植，并划分为独立的面板区域，以提高管理水平。准确且高效的Panel到Tree和Branch（P2TB）点云联合分割对于机器人操作至关重要。然而，目前大多数分段方法专注于单一实例分割并且依赖于一系列深度网络来执行联任务。这阻碍了嵌入在数据中的层级信息的使用，导致错误累积以及注释和计算成本增加，限制了其应用于实际场景的能力。在本研究中，我们提出了一种结合Real2Sim L-TreeGen用于训练数据生成的方法，并设计了一个联合模型（J-P2TB）以完成P2TB任务。该方法通过零样本学习对真实面板点云进行联合分割。与代表性方法相比，在大多数分割指标上表现更好且参数量减少40%，证明了L-TreeGen在模型训练中的有效性以及J-P2TB模型在联合分割任务中的优异性能，表明其具有较高的准确度、效率和泛化能力，适用于实际应用。这些改进不仅大大促进了机器人自动化果园操作的发展，而且推动了数字孪生技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern orchards are planted in structured rows with distinct panel divisionsto improve management. Accurate and efficient joint segmentation of point cloudfrom Panel to Tree and Branch (P2TB) is essential for robotic operations.However, most current segmentation methods focus on single instancesegmentation and depend on a sequence of deep networks to perform joint tasks.This strategy hinders the use of hierarchical information embedded in the data,leading to both error accumulation and increased costs for annotation andcomputation, which limits its scalability for real-world applications. In thisstudy, we proposed a novel approach that incorporated a Real2Sim L-TreeGen fortraining data generation and a joint model (J-P2TB) designed for the P2TB task.The J-P2TB model, trained on the generated simulation dataset, was used forjoint segmentation of real-world panel point clouds via zero-shot learning.Compared to representative methods, our model outperformed them in mostsegmentation metrics while using 40% fewer learnable parameters. This Sim2Realresult highlighted the efficacy of L-TreeGen in model training and theperformance of J-P2TB for joint segmentation, demonstrating its strongaccuracy, efficiency, and generalizability for real-world applications. Theseimprovements would not only greatly benefit the development of robots forautomated orchard operations but also advance digital twin technology.</description>
      <author>example@mail.com (Tian Qiu, Ruiming Du, Nikolai Spine, Lailiang Cheng, Yu Jiang)</author>
      <guid isPermaLink="false">2503.05630v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Pretext Task Adversarial Learning for Unpaired Low-field to Ultra High-field MRI Synthesis</title>
      <link>http://arxiv.org/abs/2503.05339v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种从低场MRI合成高质量高场MRI数据的预训练任务对抗学习框架，以解决在缺少大量训练数据情况下的医学图像领域挑战。&lt;h4&gt;背景&lt;/h4&gt;由于高场磁共振成像（MRI）设备的稀缺性和高昂成本，在缺乏足够训练数据的情况下，将低场MRI转换为高场MRI具有重要的潜在价值。然而，这种合成过程中会遇到信号噪声比和空间分辨率的问题以及跨域对齐图像特征的同时保持解剖学准确性的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从低场MRI生成高质量高场MRI的数据增强方法，以提升下游任务（如分割）的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种预训练任务对抗(PTA)学习框架，包括三个关键步骤：(1) 切片级差距感知(SGP)网络通过对比学习对齐低场和高场数据集中的切片不一致；(2) 局部结构修正(LSC)网络通过恢复局部旋转和掩膜图像提取局部位点；(3) 预训练任务引导对抗性培训过程引入额外监督，并结合判别器提升生成图像的真实性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在从低场MRI到超高场MRI的转换实验中表现出色，达到了最先进的性能（FID为16.892，在IS为1.933，MS-SSIM为0.324）。&lt;h4&gt;结论&lt;/h4&gt;所提出的预训练任务对抗学习框架能够在缺少大量高分辨率数据的情况下合成出高质量高场MRI图像，从而促进下游医学影像处理任务的改进。&lt;h4&gt;翻译&lt;/h4&gt;给定高场磁共振成像（MRI）设备的稀缺性和高昂成本，在缺乏足够训练数据情况下从低场MRI合成高场MRI具有重要潜力。然而，这类转换面临挑战，如跨域对齐图像特征的同时保持解剖学准确性以及提升细节。为解决这些问题，我们提出了一种基于预训练任务对抗学习框架的方法来实现从低场MRI到高场MRI的高质量生成数据增强过程。实验验证表明该方法在合成高场MRI方面达到了顶尖水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given the scarcity and cost of high-field MRI, the synthesis of high-fieldMRI from low-field MRI holds significant potential when there is limited datafor training downstream tasks (e.g. segmentation). Low-field MRI often suffersfrom a reduced signal-to-noise ratio (SNR) and spatial resolution compared tohigh-field MRI. However, synthesizing high-field MRI data presents challenges.These involve aligning image features across domains while preservinganatomical accuracy and enhancing fine details. To address these challenges, wepropose a Pretext Task Adversarial (PTA) learning framework for high-field MRIsynthesis from low-field MRI data. The framework comprises three processes: (1)The slice-wise gap perception (SGP) network aligns the slice inconsistencies oflow-field and high-field datasets based on contrastive learning. (2) The localstructure correction (LSC) network extracts local structures by restoring thelocally rotated and masked images. (3) The pretext task-guided adversarialtraining process introduces additional supervision and incorporates adiscriminator to improve image realism. Extensive experiments on low-field toultra high-field task demonstrate the effectiveness of our method, achievingstate-of-the-art performance (16.892 in FID, 1.933 in IS, and 0.324 inMS-SSIM). This enables the generation of high-quality high-field-like MRI datafrom low-field MRI data to augment training datasets for downstream tasks. Thecode is available at:https://github.com/Zhenxuan-Zhang/PTA4Unpaired_HF_MRI_SYN.</description>
      <author>example@mail.com (Zhenxuan Zhang, Peiyuan Jing, Coraline Beitone, Jiahao Huang, Zhifan Gao, Guang Yang, Pete Lally)</author>
      <guid isPermaLink="false">2503.05339v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>From Theory to Application: A Practical Introduction to Neural Operators in Scientific Computing</title>
      <link>http://arxiv.org/abs/2503.05598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  53 pages, 17 figures, Github repository:  https://github.com/CEADpx/neural_operators&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇综述探讨了用于近似参数偏微分方程（PDE）解的神经算子架构，强调高级概念和实际实施策略。&lt;h4&gt;背景&lt;/h4&gt;文章涵盖了基础模型，如深度操作网络（DeepONet）、基于主成分分析的神经网络（PCANet）以及傅立叶神经算子（FNO），并对其核心方法论和性能进行比较研究。&lt;h4&gt;目的&lt;/h4&gt;综述旨在探索这些架构在经典线性参数PDE上的应用，并探讨它们作为贝叶斯推理问题中代理的有效性的加速后验推断的能力。&lt;h4&gt;方法&lt;/h4&gt;该文章讨论了两个经典的线性参数PDE：泊松方程和线性弹性变形，以展示不同神经算子的性能和有效性。&lt;h4&gt;主要发现&lt;/h4&gt;除了解决正向问题外，综述还深入探讨了在贝叶斯推理中应用神经算子作为代理的有效性和准确性保持能力。&lt;h4&gt;结论&lt;/h4&gt;文章讨论了当前面临的挑战，如控制预测准确性和泛化性，并提出了一些新兴策略来应对这些挑战，例如基于残差的误差校正和多级训练。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文综述探索了一系列神经算子架构用于参数偏微分方程（PDE）解的近似，强调高级概念和实际实施策略。研究涵盖了基础模型如深度操作网络（DeepONet）、基于主成分分析的神经网络（PCANet）及傅立叶神经算子（FNO），提供了关于其核心方法论和性能表现的比较见解。这些架构在两个经典线性参数PDE上得到展示：泊松方程和线弹性变形。除了正向问题求解，综述还探讨了将神经算子作为代理应用到贝叶斯推理问题中的有效性，展示了它们加速后验推断的同时保持准确性的能力。文章通过讨论当前挑战，特别是控制预测准确性及泛化性，得出结论，并概述了解决这些问题的新兴策略，如基于残差的误差校正和多级训练方法。本文可作为实施神经算子并将其整合到科学计算工作流程中的全面指南。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This focused review explores a range of neural operator architectures forapproximating solutions to parametric partial differential equations (PDEs),emphasizing high-level concepts and practical implementation strategies. Thestudy covers foundational models such as Deep Operator Networks (DeepONet),Principal Component Analysis-based Neural Networks (PCANet), and Fourier NeuralOperators (FNO), providing comparative insights into their core methodologiesand performance. These architectures are demonstrated on two classical linearparametric PDEs: the Poisson equation and linear elastic deformation. Beyondforward problem-solving, the review delves into applying neural operators assurrogates in Bayesian inference problems, showcasing their effectiveness inaccelerating posterior inference while maintaining accuracy. The paperconcludes by discussing current challenges, particularly in controllingprediction accuracy and generalization. It outlines emerging strategies toaddress these issues, such as residual-based error correction and multi-leveltraining. This review can be seen as a comprehensive guide to implementingneural operators and integrating them into scientific computing workflows.</description>
      <author>example@mail.com (Prashant K. Jha)</author>
      <guid isPermaLink="false">2503.05598v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>opXRD: Open Experimental Powder X-ray Diffraction Database</title>
      <link>http://arxiv.org/abs/2503.05577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Powder X-ray Diffraction (pXRD)实验在材料结构表征中的重要性，并提出了一种通过机器学习实现自动化分析的方法，以解决当前数据分析过程中的瓶颈问题。&lt;h4&gt;背景&lt;/h4&gt;虽然pXRD实验广泛应用于材料科学中，但其结果的自动解析仍然面临许多挑战。现有的机器学习模型通常基于模拟数据训练，这限制了它们在实际实验数据上的表现能力。&lt;h4&gt;目的&lt;/h4&gt;构建一个公开可用的、易于访问的实验性粉末衍射图谱数据库（opXRD），以帮助改进机器学习模型对实验数据的处理性能。&lt;h4&gt;方法&lt;/h4&gt;收集了大量的pXRD图谱，包括2179个标记的数据集，这些数据来自各种材料类别。该数据库旨在为未来的自我驱动实验室提供支持。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用opXRD数据库中的标注和未标注数据，研究人员可以评估模型对实验数据的表现，并改进机器学习算法的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作有望促进机器学习研究向完全自动化的pXRD数据分析方向发展，从而为未来自我驱动材料实验室的发展奠定基础。&lt;h4&gt;翻译&lt;/h4&gt;粉末X射线衍射(pXRD)实验是材料结构表征的基础。尽管它们应用广泛，但分析pXRD衍射图仍然是自动化和高通量发现的瓶颈问题。机器学习有望通过实现自动化的粉末衍射分析来解决这一瓶颈。然而，在这个领域中应用机器学习的一个显著困难是没有足够的实验数据集，这使得研究人员主要依赖于模拟数据进行训练。但是，基于模拟pXRD模式训练出来的模型在实际实验模式上的泛化能力有限，特别是在处理噪声水平高和背景高的低质量实验模式时表现不佳。通过开放实验性粉末X射线衍射数据库（opXRD），我们提供了一个公开可用且易于访问的标记和未标记实验粉末衍射图数据集。标记的opXRD数据可以用于评估模型在实验数据上的性能，而未标记的数据可以帮助改进模型在实验数据上的表现，例如通过迁移学习方法。我们收集了umpatternsdiffractograms，其中2179个被标注，涵盖广泛的材料类别。我们希望这项正在进行的工作能够引导机器学习研究向pXRD数据分析的完全自动化发展，并因此为未来的自我驱动材料实验室提供支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Powder X-ray diffraction (pXRD) experiments are a cornerstone for materialsstructure characterization. Despite their widespread application, analyzingpXRD diffractograms still presents a significant challenge to automation and abottleneck in high-throughput discovery in self-driving labs. Machine learningpromises to resolve this bottleneck by enabling automated powder diffractionanalysis. A notable difficulty in applying machine learning to this domain isthe lack of sufficiently sized experimental datasets, which has constrainedresearchers to train primarily on simulated data. However, models trained onsimulated pXRD patterns showed limited generalization to experimental patterns,particularly for low-quality experimental patterns with high noise levels andelevated backgrounds. With the Open Experimental Powder X-Ray DiffractionDatabase (opXRD), we provide an openly available and easily accessible datasetof labeled and unlabeled experimental powder diffractograms. Labeled opXRD datacan be used to evaluate the performance of models on experimental data andunlabeled opXRD data can help improve the performance of models on experimentaldata, e.g. through transfer learning methods. We collected \numpatternsdiffractograms, 2179 of them labeled, from a wide spectrum of materialsclasses. We hope this ongoing effort can guide machine learning research towardfully automated analysis of pXRD data and thus enable future self-drivingmaterials labs.</description>
      <author>example@mail.com (Daniel Hollarek, Henrik Schopmans, Jona Östreicher, Jonas Teufel, Bin Cao, Adie Alwen, Simon Schweidler, Mriganka Singh, Tim Kodalle, Hanlin Hu, Gregoire Heymans, Maged Abdelsamie, Arthur Hardiagon, Alexander Wieczorek, Siarhei Zhuk, Ruth Schwaiger, Sebastian Siol, François-Xavier Coudert, Moritz Wolf, Carolin M. Sutter-Fella, Ben Breitung, Andrea M. Hodge, Tong-yi Zhang, Pascal Friederich)</author>
      <guid isPermaLink="false">2503.05577v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>FMT:A Multimodal Pneumonia Detection Model Based on Stacking MOE Framework</title>
      <link>http://arxiv.org/abs/2503.05626v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一个灵活的多模态变换器(FMT)，通过结合ResNet-50和BERT进行联合表示学习，并采用动态掩码注意力策略来模拟临床模式丢失，从而提高诊断肺炎的准确性。&lt;h4&gt;背景&lt;/h4&gt;人工智能在医学图像分析中显示出改善肺炎诊断准确性的潜力。然而，传统多模态方法难以应对实际挑战如数据不完整和模式缺失等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型FMT来解决现有问题，并通过小样本多模态肺炎数据集验证其性能。&lt;h4&gt;方法&lt;/h4&gt;使用ResNet-50和BERT进行联合表示学习，应用动态掩码注意力策略模拟临床模式丢失以提高鲁棒性。最后采用序列专家混合(MOE)架构实现多层次决策精炼。&lt;h4&gt;主要发现&lt;/h4&gt;在多模态肺炎数据集上的评估表明FMT取得了94%的准确率、95%的召回率和93%的F1分数，优于单模基线(ResNet: 89%，BERT: 79%)及医学基准CheXMed (90%)。&lt;h4&gt;结论&lt;/h4&gt;该研究为资源受限条件下的多模式肺炎诊断提供了一种可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence has shown the potential to improve diagnosticaccuracy through medical image analysis for pneumonia diagnosis. However,traditional multimodal approaches often fail to address real-world challengessuch as incomplete data and modality loss. In this study, a Flexible MultimodalTransformer (FMT) was proposed, which uses ResNet-50 and BERT for jointrepresentation learning, followed by a dynamic masked attention strategy thatsimulates clinical modality loss to improve robustness; finally, a sequentialmixture of experts (MOE) architecture was used to achieve multi-level decisionrefinement. After evaluation on a small multimodal pneumonia dataset, FMTachieved state-of-the-art performance with 94% accuracy, 95% recall, and 93% F1score, outperforming single-modal baselines (ResNet: 89%; BERT: 79%) and themedical benchmark CheXMed (90%), providing a scalable solution for multimodaldiagnosis of pneumonia in resource-constrained medical settings.</description>
      <author>example@mail.com (Jingyu Xu, Yang Wang)</author>
      <guid isPermaLink="false">2503.05626v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Riemannian Metric Learning: Closer to You than You Imagine</title>
      <link>http://arxiv.org/abs/2503.05321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;黎曼度量学习在机器学习中的新兴应用及其理论基础。&lt;h4&gt;背景&lt;/h4&gt;传统的距离度量学习方法依赖于欧氏空间中的全局距离，但往往无法准确捕捉数据的内在几何结构。为此，引入了基于微分几何的黎曼度量学习方法，它通过利用数据背后的黎曼流形来建模。&lt;h4&gt;目的&lt;/h4&gt;回顾经典度量学习与黎曼几何之间的联系，并提供一种结构化且易于理解的关键方法、应用和近期进展综述。&lt;h4&gt;主要贡献&lt;/h4&gt;论证了黎曼度量学习不仅是一种技术改进，而且是对数据表示方式的基本转变。&lt;h4&gt;应用场景&lt;/h4&gt;['因果推断', '最优传输', '生成模型', '表征学习']&lt;h4&gt;结论&lt;/h4&gt;黎曼度量学习为机器学习提供了新的视角和工具，并展示了其在多个领域的广泛应用前景。鼓励研究者和实践人员探索这一领域，因为它既贴近理论又接近实际应用。&lt;h4&gt;翻译&lt;/h4&gt;黎曼测度学习是机器学习的一个新兴领域，它通过微分几何的方法来建模数据背后的黎曼流形结构，为复杂的数据编码提供了新的途径。与传统的距离度量学习相比，这种方法在捕捉数据内在的几何特性方面表现出了显著的优势，并已在因果推断、最优传输、生成模型和表示学习等多个领域得到了应用。本文综述了经典测度学习与黎曼几何之间的联系，介绍了该领域的关键技术、方法及其最新进展，指出黎曼度量学习不仅仅是技术上的改进，而是对数据表示的一种基本转变，这使得该研究成为希望探索黎曼度量学习的研究者和实践者的宝贵资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Riemannian metric learning is an emerging field in machine learning,unlocking new ways to encode complex data structures beyond traditionaldistance metric learning. While classical approaches rely on global distancesin Euclidean space, they often fall short in capturing intrinsic data geometry.Enter Riemannian metric learning: a powerful generalization that leveragesdifferential geometry to model the data according to their underlyingRiemannian manifold. This approach has demonstrated remarkable success acrossdiverse domains, from causal inference and optimal transport to generativemodeling and representation learning. In this review, we bridge the gap betweenclassical metric learning and Riemannian geometry, providing a structured andaccessible overview of key methods, applications, and recent advances. We arguethat Riemannian metric learning is not merely a technical refinement but afundamental shift in how we think about data representations. Thus, this reviewshould serve as a valuable resource for researchers and practitionersinterested in exploring Riemannian metric learning and convince them that it iscloser to them than they might imagine-both in theory and in practice.</description>
      <author>example@mail.com (Samuel Gruffaz, Josua Sassen)</author>
      <guid isPermaLink="false">2503.05321v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR-enhanced 3D Gaussian Splatting Mapping</title>
      <link>http://arxiv.org/abs/2503.05425v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新型的基于3D Gaussian Splatting（3DGS）的地图构建框架LiGSM，该框架通过整合激光雷达数据提高了三维场景地图绘制的准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统的基于图像的方法在处理传感器偏移变化时不够灵活，并且初始点较少，影响了三维场景重建的效果。&lt;h4&gt;目的&lt;/h4&gt;开发一种结合激光雷达和图像信息来提高3DGS框架性能的新方法。&lt;h4&gt;方法&lt;/h4&gt;LiGSM采用了一种综合损失函数，该函数可以同时利用图像和激光雷达数据估计姿态并优化外参。此外，还使用激光雷达点云对3DGS进行初始化，并在场景渲染时引入深度图监督来确保准确的几何结构和光度特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在公共和自收集的数据集上，LiGSM在姿态跟踪和场景渲染方面都优于比较方法。&lt;h4&gt;结论&lt;/h4&gt;通过将激光雷达数据与图像信息相结合，LiGSM能够提供更精确、鲁棒性更强的三维地图绘制方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting (3DGS) mapping framework that improves the accuracy and robustness of 3D scene mapping by integrating LiDAR data. LiGSM constructs joint loss from images and LiDAR point clouds to estimate poses and optimize their extrinsic parameters, enabling dynamic adaptation to variations in sensor alignment. Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing a denser and more reliable starting points compared to sparse SfM points. In scene rendering, the framework augments standard image-based supervision with depth maps generated from LiDAR projections, ensuring an accurate scene representation in both geometry and photometry. Experiments on public and self-collected datasets demonstrate that LiGSM outperforms comparative methods in pose tracking and scene rendering.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces LiGSM, a novel LiDAR-enhanced 3D Gaussian Splatting(3DGS) mapping framework that improves the accuracy and robustness of 3D scenemapping by integrating LiDAR data. LiGSM constructs joint loss from images andLiDAR point clouds to estimate the poses and optimize their extrinsicparameters, enabling dynamic adaptation to variations in sensor alignment.Furthermore, it leverages LiDAR point clouds to initialize 3DGS, providing adenser and more reliable starting points compared to sparse SfM points. Inscene rendering, the framework augments standard image-based supervision withdepth maps generated from LiDAR projections, ensuring an accurate scenerepresentation in both geometry and photometry. Experiments on public andself-collected datasets demonstrate that LiGSM outperforms comparative methodsin pose tracking and scene rendering.</description>
      <author>example@mail.com (Jian Shen, Huai Yu, Ji Wu, Wen Yang, Gui-Song Xia)</author>
      <guid isPermaLink="false">2503.05425v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Discrete Contrastive Learning for Diffusion Policies in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.05229v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于对比学习的方法，从现有的人类驾驶数据中提取驾驶风格字典，并通过量化离散化这些风格。利用这种方法训练条件扩散策略来模拟人类驾驶员的行为。&lt;h4&gt;背景&lt;/h4&gt;自驾车测试需要准确和丰富的人类驾驶行为的仿真，然而由于人类驾驶风格的高度多样性和变化性，这一任务仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能有效学习人类驾驶行为的方法，以提高自动驾驶车辆的评估和改进技术的有效性和真实性。&lt;h4&gt;方法&lt;/h4&gt;利用对比学习从现有的人类驾驶数据中提取一个驾驶风格字典，并通过量化离散化这些风格。然后使用这些风格来训练条件扩散策略，用以仿真人类驾驶员的行为。&lt;h4&gt;主要发现&lt;/h4&gt;生成的行为比基于机器学习的方法更安全和更具人类特性。&lt;h4&gt;结论&lt;/h4&gt;该方法有望提高自动驾驶车辆评估的现实性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;从数据中学习准确且丰富的模拟人类驾驶行为用于自驾车测试仍然是一个挑战，因为人类驾驶风格具有高度多样性和变异性。我们通过提出一种利用对比学习从现有的人类驾驶数据中提取驾驶风格字典的新方法来解决这一问题。我们将这些风格进行量化离散化，并使用它们来学习条件扩散策略以模拟人类驾驶员的行为。我们的实证评估表明，与基于机器学习的基准方法相比，生成的行为更加安全和人性化。我们认为这将有可能提高自驾车性能评估的真实性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning to perform accurate and rich simulations of human driving behaviorsfrom data for autonomous vehicle testing remains challenging due to humandriving styles' high diversity and variance. We address this challenge byproposing a novel approach that leverages contrastive learning to extract adictionary of driving styles from pre-existing human driving data. Wediscretize these styles with quantization, and the styles are used to learn aconditional diffusion policy for simulating human drivers. Our empiricalevaluation confirms that the behaviors generated by our approach are both saferand more human-like than those of the machine-learning-based baseline methods.We believe this has the potential to enable higher realism and more effectivetechniques for evaluating and improving the performance of autonomous vehicles.</description>
      <author>example@mail.com (Kalle Kujanpää, Daulet Baimukashev, Farzeen Munir, Shoaib Azam, Tomasz Piotr Kucner, Joni Pajarinen, Ville Kyrki)</author>
      <guid isPermaLink="false">2503.05229v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Statistical Deficiency for Task Inclusion Estimation</title>
      <link>http://arxiv.org/abs/2503.05491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  34 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器学习中的任务是评估当前模型能力的最自然对象。目前的趋势是建立能够处理任何任务的一般化模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种理论基础架构来定义任务，并从统计缺陷的角度计算两个任务之间的包含关系。&lt;h4&gt;方法&lt;/h4&gt;使用信息充足性作为可操作代理，估计任务之间包含程度，并在合成数据上验证其有效性。此外，还利用该方法重建了经典的NLP处理流程。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种理论框架来定义机器学习中的“任务”概念，并提供了一种计算两个任务间统计缺陷差异的方法。&lt;h4&gt;结论&lt;/h4&gt;研究为理解和评估机器学习中不同任务之间的关系提供了新的工具和视角，特别是在转移学习和多任务学习领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tasks are central in machine learning, as they are the most natural objectsto assess the capabilities of current models. The trend is to build generalmodels able to address any task. Even though transfer learning and multitasklearning try to leverage the underlying task space, no well-founded tools areavailable to study its structure. This study proposes a theoretically groundedsetup to define the notion of task and to compute the {\bf inclusion} betweentwo tasks from a statistical deficiency point of view. We propose a tractableproxy as information sufficiency to estimate the degree of inclusion betweentasks, show its soundness on synthetic data, and use it to reconstructempirically the classic NLP pipeline.</description>
      <author>example@mail.com (Loïc Fosse, Frédéric Béchet, Benoît Favre, Géraldine Damnati, Gwénolé Lecorvé, Maxime Darrin, Philippe Formont, Pablo Piantanida)</author>
      <guid isPermaLink="false">2503.05491v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Video Music Recommendation with Transformer-Driven Audio-Visual Embeddings</title>
      <link>http://arxiv.org/abs/2503.05008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2024 IEEE 5th International Symposium on the Internet of Sounds  (IS2), Erlangen, Germany, 2024, pp. 1-6&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于自监督学习和对比学习的新方法，用于自动推荐适合视频的音频。&lt;h4&gt;背景&lt;/h4&gt;合适的配乐可以帮助视频更好地传达其内容，并提供更好的沉浸式体验。传统的手动标记过程效率低下且成本高昂。&lt;h4&gt;目的&lt;/h4&gt;通过利用自我监督学习和对比学习，开发一种能够为视频自动生成匹配音频的方法，从而减少人工标注的需求。&lt;h4&gt;方法&lt;/h4&gt;采用双分支交叉模态嵌入模型将音频和视频特征映射到一个公共的低维空间。该方法评估不同音视频对之间的适配性，并通过逆距离度量来实现。此外，还对比分析了多种时间编码方式的有效性，重点突出了Transformer在处理音频视频匹配任务中的时间信息方面的优越性能。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型TIVM（结合了变压器编码器和InfoNCE损失）显著提高了音视频匹配的性能，并超越了传统的方法。该研究强调了Transformers在管理音频-视频配对的时间信息方面的能力，同时展示了使用双分支交叉模态嵌入的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的自监督学习方法为自动推荐适合视频内容的音频提供了一个有效的解决方案，极大地提升了用户体验和效率。&lt;h4&gt;翻译&lt;/h4&gt;A fitting soundtrack can help a video better convey its content and provide abetter immersive experience. This paper introduces a novel approach utilizingself-supervised learning and contrastive learning to automatically recommendaudio for video content, thereby eliminating the need for manual labeling.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IS262782.2024.10704086&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A fitting soundtrack can help a video better convey its content and provide abetter immersive experience. This paper introduces a novel approach utilizingself-supervised learning and contrastive learning to automatically recommendaudio for video content, thereby eliminating the need for manual labeling. Weuse a dual-branch cross-modal embedding model that maps both audio and videofeatures into a common low-dimensional space. The fit of various audio-videopairs can then be mod-eled as inverse distance measure. In addition, acomparative analysis of various temporal encoding methods is presented,emphasizing the effectiveness of transformers in managing the temporalinformation of audio-video matching tasks. Through multiple experiments, wedemonstrate that our model TIVM, which integrates transformer encoders andusing InfoN Celoss, significantly improves the performance of audio-videomatching and surpasses traditional methods.</description>
      <author>example@mail.com (Shimiao Liu, Alexander Lerch)</author>
      <guid isPermaLink="false">2503.05008v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Robust Multimodal Learning for Ophthalmic Disease Grading via Disentangled Representation</title>
      <link>http://arxiv.org/abs/2503.05319v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Essence-Point和Disentangle Representation Learning (EDRL)策略，旨在改进眼科学中多模态数据的诊断准确性。&lt;h4&gt;背景&lt;/h4&gt;眼科医生常依赖多模态数据来提高诊断准确度，但由于医疗设备不足及数据隐私问题，实际应用中难以获得完整的多模态数据。传统深度学习方法通过在潜在空间学习表示来解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;提出EDRL策略以克服现有方法的两个关键限制：冗余信息和重叠表示。&lt;h4&gt;方法&lt;/h4&gt;EDRL策略整合了自蒸馏机制，并将其纳入端到端框架，用于增强特征选择和解耦。Essence-Point模块选取提升疾病分级性能的判别性特征；Disentangled模块将多模态数据分离成通用与独特表征，减少特性缠结。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多模态眼科数据集上，EDRL策略显著优于当前最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的EDRL框架对于提高眼科学中基于多模态数据的诊断准确性和解释性具有潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了眼科医生如何依赖多模态数据以提升诊断准确性。然而，在实际应用中，由于医疗设备限制及数据隐私顾虑，难以获取完整的多模态数据。传统的深度学习方法通常通过在潜在空间学习表示来应对这些挑战，但文章指出这种方法存在两个关键局限：复杂模态中的任务无关冗余信息导致潜在空间表征显著冗余；重叠的多模态表示使得提取每个模态的独特特征变得困难。为克服这些问题，作者提出了Essence-Point和Disentangle Representation Learning (EDRL)策略，该方法结合自蒸馏机制，并将其融入端到端框架中以增强特征选择及解耦能力。实验结果表明，在多模态眼科数据集上，EDRL策略显著优于当前最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper discusses how ophthalmologists often rely on multimodal data toimprove diagnostic accuracy. However, complete multimodal data is rare inreal-world applications due to a lack of medical equipment and concerns aboutdata privacy. Traditional deep learning methods typically address these issuesby learning representations in latent space. However, the paper highlights twokey limitations of these approaches: (i) Task-irrelevant redundant information(e.g., numerous slices) in complex modalities leads to significant redundancyin latent space representations. (ii) Overlapping multimodal representationsmake it difficult to extract unique features for each modality. To overcomethese challenges, the authors propose the Essence-Point and DisentangleRepresentation Learning (EDRL) strategy, which integrates a self-distillationmechanism into an end-to-end framework to enhance feature selection anddisentanglement for more robust multimodal learning. Specifically, theEssence-Point Representation Learning module selects discriminative featuresthat improve disease grading performance. The Disentangled RepresentationLearning module separates multimodal data into modality-common andmodality-unique representations, reducing feature entanglement and enhancingboth robustness and interpretability in ophthalmic disease diagnosis.Experiments on multimodal ophthalmology datasets show that the proposed EDRLstrategy significantly outperforms current state-of-the-art methods.</description>
      <author>example@mail.com (Xinkun Wang, Yifang Wang, Senwei Liang, Feilong Tang, Chengzhi Liu, Ming Hu, Chao Hu, Junjun He, Zongyuan Ge, Imran Razzak)</author>
      <guid isPermaLink="false">2503.05319v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Separability Membrane: 3D Active Contour for Point Cloud Surface Reconstruction</title>
      <link>http://arxiv.org/abs/2503.05217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种名为Separability Membrane的新方法，用于从3D点云对象中提取表面。&lt;h4&gt;背景&lt;/h4&gt;现有的三维物体表面提取技术通常需要训练数据或转换为体积表示，并且在存在噪声或异常值的情况下难以准确重建模糊的边界。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需训练数据和体积表示转换的方法来精确提取并重建3D点云中的对象表面。&lt;h4&gt;方法&lt;/h4&gt;通过最大化基于Fisher比率计算特征（如强度、颜色或局部密度）的类分离度，确定一个三维物体表面作为其内部与外部区域之间差异最大的边界。使用自适应B样条曲面控制3D模型的刚性，并根据本地和全局分离度调整属性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够准确地重建模糊且复杂情况下的对象表面边界（如存在噪声或异常值），展示了高鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;实验在合成数据集和3DNet数据集中验证了Separability Membrane的有效性，证明了其在各种条件下的稳健性能。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种名为分离膜的方法，这是一种用于从三维点云对象中提取表面的鲁棒型三维主动轮廓。该方法定义了一个三维物体的表面为内部与外部区域之间基于Fisher比率计算特征（如强度、颜色或局部密度）的最大类分离度边界。分离膜通过控制自适应B样条曲面来精确识别3D物体的确切表面，这种模型根据本地和全局分离度调整其属性，在无需任何训练数据的情况下准确重建模糊的表面边界，并且不转换为体积表示。通过对合成三维点云数据集和3DNet数据集进行评估，显示了该膜的有效性和鲁棒性，即使在存在不同条件时也是如此。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes Separability Membrane, a robust 3D active contour forextracting a surface from 3D point cloud object. Our approach defines thesurface of a 3D object as the boundary that maximizes the separability of pointfeatures, such as intensity, color, or local density, between its inner andouter regions based on Fisher's ratio. Separability Membrane identifies theexact surface of a 3D object by maximizing class separability while controllingthe rigidity of the 3D surface model with an adaptive B-spline surface thatadjusts its properties based on the local and global separability. A keyadvantage of our method is its ability to accurately reconstruct surfaceboundaries even when they are ambiguous due to noise or outliers, withoutrequiring any training data or conversion to volumetric representation.Evaluations on a synthetic 3D point cloud dataset and the 3DNet datasetdemonstrate the membrane's effectiveness and robustness under diverseconditions.</description>
      <author>example@mail.com (Gulpi Qorik Oktagalu Pratamasunu, Guoqing Hao, Kazuhiro Fukui)</author>
      <guid isPermaLink="false">2503.05217v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Quantum-PEFT: Ultra parameter-efficient fine-tuning</title>
      <link>http://arxiv.org/abs/2503.05431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的量子计算驱动的参数高效微调方法（Quantum-PEFT），该方法与传统的低秩适应法不同，在保持性能的同时提高了参数效率。&lt;h4&gt;背景&lt;/h4&gt;现有的参数高效微调方法，如低秩适应（LoRA），虽然在减少训练参数数量方面有一定效果，但随着模型维度的增长，其优势逐渐减弱。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的量子计算单元化参数调整方式Quantum-PEFT，以实现在保持竞争力性能的同时大幅度提升参数效率。&lt;h4&gt;方法&lt;/h4&gt;利用Pauli参数化实现了量子化的全秩且高效的参数化方式。这种方法的训练参数数量随着环境维度的增长只呈对数增长，而非线性增长。&lt;h4&gt;主要发现&lt;/h4&gt;Quantum-PEFT在各种语言和视觉领域的迁移学习基准测试中展示出显著的参数效率优势，并且可实现比最低秩LoRA更小的训练参数量。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅在保持性能的同时极大提升了模型的参数效率，而且证明了量子计算在解决大规模机器学习问题中的潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces Quantum-PEFT that leverages quantum computations forparameter-efficient fine-tuning (PEFT). Unlike other additive PEFT methods,such as low-rank adaptation (LoRA), Quantum-PEFT exploits an underlyingfull-rank yet surprisingly parameter efficient quantum unitaryparameterization. With the use of Pauli parameterization, the number oftrainable parameters grows only logarithmically with the ambient dimension, asopposed to linearly as in LoRA-based PEFT methods. Quantum-PEFT achievesvanishingly smaller number of trainable parameters than the lowest-rank LoRA asdimensions grow, enhancing parameter efficiency while maintaining a competitiveperformance. We apply Quantum-PEFT to several transfer learning benchmarks inlanguage and vision, demonstrating significant advantages in parameterefficiency.</description>
      <author>example@mail.com (Toshiaki Koike-Akino, Francesco Tonin, Yongtao Wu, Frank Zhengqing Wu, Leyla Naz Candogan, Volkan Cevher)</author>
      <guid isPermaLink="false">2503.05431v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>The Society of HiveMind: Multi-Agent Optimization of Foundation Model Swarms to Unlock the Potential of Collective Intelligence</title>
      <link>http://arxiv.org/abs/2503.05473v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages (excl. appendix)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于多智能体系统的框架，用于解决大型语言模型在可访问性和扩展性方面的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的人工智能基础模型通常由大型语言模型表示，这些模型在处理需要丰富逻辑推理的任务时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够模仿自然界中动物群落行为的多智能体系统框架，以提高任务完成效率和能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个名为“蜂巢心智社会”（Society of HiveMind，SOHM）的新框架。该框架通过模拟现代进化理论中的动物群体行为来协调多个AI基础模型之间的交互。&lt;h4&gt;主要发现&lt;/h4&gt;对于主要是现实世界知识的任务，SOHM提供的益处有限；而对于需要大量逻辑推理的任务，它表现出显著改进。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，结合多种不同的AI基础模型能够形成一种具有自我完善能力的人工群集智能系统。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已由英文翻译成中文并进行了总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent systems address issues of accessibility and scalability ofartificial intelligence (AI) foundation models, which are often represented bylarge language models. We develop a framework - the "Society of HiveMind"(SOHM) - that orchestrates the interaction between multiple AI foundationmodels, imitating the observed behavior of animal swarms in nature by followingmodern evolutionary theories. On the one hand, we find that the SOHM provides anegligible benefit on tasks that mainly require real-world knowledge. On theother hand, we remark a significant improvement on tasks that require intensivelogical reasoning, indicating that multi-agent systems are capable ofincreasing the reasoning capabilities of the collective compared to theindividual agents. Our findings demonstrate the potential of combining amultitude of diverse AI foundation models to form an artificial swarmintelligence capable of self-improvement through interactions with a givenenvironment.</description>
      <author>example@mail.com (Noah Mamie, Susie Xi Rao)</author>
      <guid isPermaLink="false">2503.05473v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Path Pooling: Train-Free Structure Enhancement for Efficient Knowledge Graph Retrieval-Augmented Generation</title>
      <link>http://arxiv.org/abs/2503.05203v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的知识图谱增强的生成模型改进策略——路径池化（Path Pooling），旨在解决大型语言模型在实际应用中的幻觉和知识不足问题。&lt;h4&gt;背景&lt;/h4&gt;尽管大规模语言模型在许多任务上表现出色，但在现实世界的应用中仍面临知识幻觉和知识缺乏的问题。基于知识图谱的检索增强生成方法通过利用结构化信息来提高LLM的质量和可信度，但这些方法难以有效整合结构信息，导致计算成本过高或未能充分利用可用的知识。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单且无需训练的方法，以引入结构化信息，并将其无缝集成到现有的知识图谱-检索增强生成模型中，从而更好地利用丰富的结构信息。&lt;h4&gt;方法&lt;/h4&gt;受图表示学习中平滑操作的启发，提出了路径池化策略。这是一种简单的、无须训练的操作，通过新颖的中心于路径的池化方式引入了结构信息，并且可以无缝集成到现有的KG-RAG方法中使用。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在最先进的KG-RAG模型中加入路径池化策略能显著改善各种设定下的性能表现，同时几乎不会增加额外成本。&lt;h4&gt;结论&lt;/h4&gt;该论文通过提出一种新的技术——路径池化，有效解决了现有知识图谱增强的生成方法在结构信息整合上的难题，并展示了其良好的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;尽管大型语言模型在许多任务上取得了显著成功，但在实际应用中仍存在幻觉和知识不足的问题。基于知识图谱的检索增强生成（KG-RAG）方法通过利用结构化和语义信息来提高LLM的质量和可信度，但这些方法难以有效整合结构信息，要么导致高计算成本，要么未能充分利用可用的知识。受图表示学习中平滑操作的启发，我们提出了一种简单的、无需训练的策略——路径池化（Path Pooling），通过新颖的以路径为中心的池化操作引入了结构信息，并可以无缝集成到现有的KG-RAG方法中使用，实现了更丰富的结构信息利用。广泛的实验表明，在最先进的KG-RAG模型中加入路径池化能够显著提高性能，同时几乎不会增加额外成本。代码即将在https://github.com/hrwang00/path-pooling发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although Large Language Models achieve strong success in many tasks, theystill suffer from hallucinations and knowledge deficiencies in real-worldapplications. Many knowledge graph-based retrieval-augmented generation(KG-RAG) methods enhance the quality and credibility of LLMs by leveragingstructure and semantic information in KGs as external knowledge bases. However,these methods struggle to effectively incorporate structure information, eitherincurring high computational costs or underutilizing available knowledge.Inspired by smoothing operations in graph representation learning, we proposepath pooling, a simple, train-free strategy that introduces structureinformation through a novel path-centric pooling operation. It seamlesslyintegrates into existing KG-RAG methods in a plug-and-play manner, enablingricher structure information utilization. Extensive experiments demonstratethat incorporating the path pooling into the state-of-the-art KG-RAG methodconsistently improves performance across various settings while introducingnegligible additional cost. Code is coming soon athttps://github.com/hrwang00/path-pooling.</description>
      <author>example@mail.com (Hairu Wang, Yuan Feng, Xike Xie, S Kevin Zhou)</author>
      <guid isPermaLink="false">2503.05203v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Spatial Distillation based Distribution Alignment (SDDA) for Cross-Headset EEG Classification</title>
      <link>http://arxiv.org/abs/2503.05349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于空间蒸馏的分布对齐(SDDA)方法，用于非侵入性脑机接口(BCI)中的异构跨头盔转移。&lt;h4&gt;背景&lt;/h4&gt;非侵入性BCI通过EEG信号实现用户与外部设备之间的直接交互。由于不同头盔中电极数量和位置的不同，解码这些信号是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;解决非侵入性BCI中不同头盔之间跨域迁移的问题。&lt;h4&gt;方法&lt;/h4&gt;SDDA利用空间蒸馏充分利用所有电极，并通过输入/特征/输出空间分布对齐来处理源域与目标域之间的显著差异。这是首次在跨头盔传输中使用知识蒸馏的研究。&lt;h4&gt;主要发现&lt;/h4&gt;在六组EEG数据集上的广泛实验表明，SDDA在离线无监督领域适应和在线监督领域适应场景下均表现出优越性能，并且始终优于10种经典和最新的迁移学习算法。&lt;h4&gt;结论&lt;/h4&gt;所提出的SDDA方法证明了其在解决非侵入性BCI中跨头盔传输挑战方面的有效性与鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已按照要求进行了中文总结并转换为JSON格式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A non-invasive brain-computer interface (BCI) enables direct interactionbetween the user and external devices, typically via electroencephalogram (EEG)signals. However, decoding EEG signals across different headsets remains asignificant challenge due to differences in the number and locations of theelectrodes. To address this challenge, we propose a spatial distillation baseddistribution alignment (SDDA) approach for heterogeneous cross-headset transferin non-invasive BCIs. SDDA uses first spatial distillation to make use of thefull set of electrodes, and then input/feature/output space distributionalignments to cope with the significant differences between the source andtarget domains. To our knowledge, this is the first work to use knowledgedistillation in cross-headset transfers. Extensive experiments on six EEGdatasets from two BCI paradigms demonstrated that SDDA achieved superiorperformance in both offline unsupervised domain adaptation and onlinesupervised domain adaptation scenarios, consistently outperforming 10 classicaland state-of-the-art transfer learning algorithms.</description>
      <author>example@mail.com (Dingkun Liu, Siyang Li, Ziwei Wang, Wei Li, Dongrui Wu)</author>
      <guid isPermaLink="false">2503.05349v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>StreamGrid: Streaming Point Cloud Analytics via Compulsory Splitting and Deterministic Termination</title>
      <link>http://arxiv.org/abs/2503.05197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;点云数据在智能应用中的重要性日益增加，但加速器频繁的离片内存访问导致流水线停滞和高能耗。本文提出了两个技术：强制拆分和确定性终止，并在此基础上开发了StreamGrid框架，以实现全流式处理并自动优化片上缓存大小。&lt;h4&gt;背景&lt;/h4&gt;随着点云数据在智能应用中的重要性增加，加速器中频繁的离片内存访问成为瓶颈，导致流水线停滞及高能耗问题。传统的行缓冲技术虽然可以消除离片内存流量，但不适用于点云由于其独特的计算模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的处理方法和框架，以减少能量消耗并提高系统效率，同时尽量保持或接近基准方法的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了两个关键技术：强制拆分（Compulsory Splitting）与确定性终止（Deterministic Termination），并在这些技术的基础上开发了StreamGrid框架。这个框架能够自动优化片上缓冲大小以适应不同的应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;在实验评估中，相较于没有采用新方法的基准模型，StreamGrid减少了61.3%的片上内存使用量，并降低了40.5%的能量消耗；同时，在性能上实现了10.0倍的速度提升和3.9倍的能量效率提高。&lt;h4&gt;结论&lt;/h4&gt;所提出的强制拆分、确定性终止技术和StreamGrid框架成功地解决了点云处理中能量消耗与效率的问题，展示了在智能应用领域的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已给出中文描述&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds are increasingly important in intelligent applications, butfrequent off-chip memory traffic in accelerators causes pipeline stalls andleads to high energy consumption. While conventional line buffer techniques caneliminate off-chip traffic, they cannot be directly applied to point clouds dueto their inherent computation patterns. To address this, we introduce twotechniques: compulsory splitting and deterministic termination, enablingfully-streaming processing. We further propose StreamGrid, a framework thatintegrates these techniques and automatically optimizes on-chip buffer sizes.Our evaluation shows StreamGrid reduces on-chip memory by 61.3\% and energyconsumption by 40.5\% with marginal accuracy loss compared to the baselineswithout our techniques. Additionally, we achieve 10.0$\times$ speedup and3.9$\times$ energy efficiency over state-of-the-art accelerators.</description>
      <author>example@mail.com (Yu Feng, Zheng Liu, Weikai Lin, Zihan Liu, Jingwen Leng, Minyi Guo, Zhezhi He, Jieru Zhao, Yuhao Zhu)</author>
      <guid isPermaLink="false">2503.05197v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts</title>
      <link>http://arxiv.org/abs/2503.05447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical report, 17 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了Linear-MoE，这是一种用于大规模模型的生产级系统，它将线性序列建模（LSM）和混合专家（MoE）集成在一起。&lt;h4&gt;背景&lt;/h4&gt;线性注意、状态空间模型、线性RNN以及混合专家方法在近期被提出作为架构改进。这些技术能够提供线性复杂度的序列建模能力和稀疏激活能力，结合这两者的优点可以提高性能并实现高效训练。&lt;h4&gt;目的&lt;/h4&gt;介绍Linear-MoE系统，该系统旨在利用LSM模块和MoE层的优势来构建大规模模型，并通过高效的训练方式进一步提升模型灵活性与性能。&lt;h4&gt;方法&lt;/h4&gt;Linear-MoE系统主要包含两个子系统：1）建模子系统，它为所有LSM实例提供统一框架；2）训练子系统，该子系统利用各种高级并行技术（特别是针对Linear-MoE模型的序列并行性）来促进高效训练。此外，还探讨了混合模型，这些模型结合了线性MoE层和标准Transformer-MoE层。&lt;h4&gt;主要发现&lt;/h4&gt;在A0.3B-2B系列和A1B-7B系列模型上进行的评估显示，Linear-MoE实现了效率提升，并且在各种基准测试中保持了竞争力的表现。&lt;h4&gt;结论&lt;/h4&gt;结果表明，Linear-MoE架构具有成为下一代基础模型结构的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;线性序列建模（LSM）如线性注意力、状态空间模型和线性RNN以及混合专家(MoE)最近作为重要的架构改进而出现。本文介绍了Linear-MoE，这是一个用于大规模模型的生产级系统，这些模型将LSM与MoE相结合。Linear-MoE利用了LSM模块进行线性复杂度序列建模和MoE层进行稀疏激活的优势，旨在提供高性能并有效地训练。该系统包括：1）建模子系统，为所有LSM实例提供统一框架；2）训练子系统，通过集成各种高级并行技术（特别是针对Linear-MoE模型的序列并行性）促进高效训练。此外，我们还探讨了混合模型，这些模型将Linear-MoE层与标准Transformer-MoE层结合，并利用其序列并行性来进一步增强模型灵活性和性能。在两个模型系列A0.3B-2B和A1B-7B上的评估表明，Linear-MoE实现了效率提升，并且在各种基准测试中保持了竞争力的表现，展示了它作为下一代基础模型结构的巨大潜力。代码：https://github.com/OpenSparseLLMs/Linear-MoE.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linear Sequence Modeling (LSM) like linear attention, state space models andlinear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significantarchitectural improvements. In this paper, we introduce Linear-MoE, aproduction-level system for modeling and training large-scale models thatintegrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modulesfor linear-complexity sequence modeling and MoE layers for sparsely activation,aiming to offer high performance with efficient training. The Linear-MoE systemcomprises: 1) Modeling subsystem, which provides a unified framework supportingall instances of LSM. and 2) Training subsystem, which facilitates efficienttraining by incorporating various advanced parallelism technologies,particularly Sequence Parallelism designed for Linear-MoE models. Additionally,we explore hybrid models that combine Linear-MoE layers with standardTransformer-MoE layers with its Sequence Parallelism to further enhance modelflexibility and performance. Evaluations on two model series, A0.3B-2B andA1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintainingcompetitive performance on various benchmarks, showcasing its potential as anext-generation foundational model architecture. Code:https://github.com/OpenSparseLLMs/Linear-MoE.</description>
      <author>example@mail.com (Weigao Sun, Disen Lan, Tong Zhu, Xiaoye Qu, Yu Cheng)</author>
      <guid isPermaLink="false">2503.05447v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>HexPlane Representation for 3D Semantic Scene Understanding</title>
      <link>http://arxiv.org/abs/2503.05127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于3D语义场景理解的HexPlane表示法，该方法通过设计View Projection Module (VPM)将3D点云投影到六个平面上，并利用2D编码器提取特征。这些特征随后被送入HexPlane Association Module (HAM)，以自适应地融合最具有信息量的信息来生成每个点的融合特征。&lt;h4&gt;背景&lt;/h4&gt;传统的点和体素表示方法在处理稀疏且无序的3D点云时效率低下，难以利用高度优化的2D操作。现有的方法也很难直接应用现成的2D模型、网络权重以及训练配方到3D空间中以实现准确的理解。&lt;h4&gt;目的&lt;/h4&gt;提出HexPlane表示法来提高3D场景理解的效率和准确性，并展示其在ScanNet和SemanticKITTI基准测试中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;1. 设计View Projection Module (VPM)，用于将3D点云投影到六个平面上，保留原始空间信息；2. 使用2D编码器提取六平面的特征；3. 通过HexPlane Association Module (HAM)自适应地融合这些特征以生成最终的预测。&lt;h4&gt;主要发现&lt;/h4&gt;1. HexPlane表示法在ScanNet上的表现优于先前的方法，特别是语义分割任务上获得了77.0 mIoU的成绩，比Point Transformer V2高出1.6mIoU；2. 在室内3D检测任务中也观察到了鼓舞人心的结果。&lt;h4&gt;结论&lt;/h4&gt;HexPlane表示法能够无缝集成到现有的体素、点和范围基线方法中，并且可以带来显著的性能提升。该算法在效率和准确性方面都有明显优势。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们介绍了用于3D语义场景理解的HexPlane表示法。具体而言，首先设计了View Projection Module (VPM)将3D点云投影到六个平面上以尽可能保留原始的空间信息。通过2D编码器从六个平面提取特征，并将其发送给HexPlane Association Module (HAM)，该模块自适应地融合每个点最具有信息量的信息。这些融合后的点特征被进一步馈送到任务头，以产生最终的预测。与流行的点和体素表示相比，HexPlane表示法在处理稀疏且无序的3D点云时更高效，并能利用高度优化的2D操作来提高准确性。它还可以利用现成的2D模型、网络权重以及训练配方，在三维空间中实现准确的理解。在ScanNet和SemanticKITTI基准测试上，我们的算法（称为HexNet3D）与先前的方法相比取得了竞争性的性能表现。特别是在ScanNet 3D分割任务上，我们方法获得了77.0 mIoU的验证集成绩，比Point Transformer V2高出1.6mIoU。我们也观察到了在室内3D检测任务上的令人鼓舞的结果。值得注意的是，我们的方法可以无缝集成到现有的基于体素、点和范围的方法中，并且可以在没有额外功能的情况下带来显著的好处。代码将在发布时提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce the HexPlane representation for 3D semantic sceneunderstanding. Specifically, we first design the View Projection Module (VPM)to project the 3D point cloud into six planes to maximally retain the originalspatial information. Features of six planes are extracted by the 2D encoder andsent to the HexPlane Association Module (HAM) to adaptively fuse the mostinformative information for each point. The fused point features are furtherfed to the task head to yield the ultimate predictions. Compared to the popularpoint and voxel representation, the HexPlane representation is efficient andcan utilize highly optimized 2D operations to process sparse and unordered 3Dpoint clouds. It can also leverage off-the-shelf 2D models, network weights,and training recipes to achieve accurate scene understanding in 3D space. OnScanNet and SemanticKITTI benchmarks, our algorithm, dubbed HexNet3D, achievescompetitive performance with previous algorithms. In particular, on the ScanNet3D segmentation task, our method obtains 77.0 mIoU on the validation set,surpassing Point Transformer V2 by 1.6 mIoU. We also observe encouragingresults in indoor 3D detection tasks. Note that our method can be seamlesslyintegrated into existing voxel-based, point-based, and range-based approachesand brings considerable gains without bells and whistles. The codes will beavailable upon publication.</description>
      <author>example@mail.com (Zeren Chen, Yuenan Hou, Yulin Chen, Li Liu, Xiao Sun, Lu Sheng)</author>
      <guid isPermaLink="false">2503.05127v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>TinyR1-32B-Preview: Boosting Accuracy with Branch-Merge Distillation</title>
      <link>http://arxiv.org/abs/2503.04872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Branch-Merge的知识蒸馏方法，用于提高大型语言模型的压缩效率。该方法通过两个阶段实现：分支阶段和合并阶段。&lt;h4&gt;背景&lt;/h4&gt;减少大型语言模型（LLMs）的大小以保持性能是当前的一个重要挑战，现有的如模型蒸馏、迁移学习等方法在维持高精度方面效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的Branch-Merge知识蒸馏方法，旨在提高模型压缩效率并创建更小、高性能且计算成本更低的大型语言模型。&lt;h4&gt;方法&lt;/h4&gt;该方法包含两个阶段：分支阶段通过领域特定监督微调（SFT）将大教师模型的知识选择性地提取到专业化的学生模型中；合并阶段则使学生模型能够跨域知识转移，提升泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;验证该蒸馏方法使用DeepSeek-R1作为教师模型和DeepSeek-R1-Distill-Qwen-32B作为学生模型。最终得到的TinyR1-32B-Preview模型在多个基准测试（如数学、编程、科学等）中优于其对照组，且接近于DeepSeek-R1在AIME 2024的表现。&lt;h4&gt;结论&lt;/h4&gt;Branch-Merge蒸馏方法提供了一种可扩展的解决方案，用于创建更小、高性能且计算成本较低的大型语言模型。&lt;h4&gt;翻译&lt;/h4&gt;减少大型语言模型（LLMs）的大小以保持性能是当前的一个重要挑战。然而现有的方法如模型蒸馏和迁移学习往往无法实现高精度。为解决此问题，我们提出了Branch-Merge蒸馏法，通过两个阶段增强模型压缩：分支阶段选择性地将大教师模型的知识转移到专业化学生模型中；合并阶段则使这些学生模型能够跨领域知识转移并提升泛化能力。使用DeepSeek-R1作为教师模型和DeepSeek-R1-Distill-Qwen-32B作为学生模型进行验证，得到的TinyR1-32B-Preview在多个基准测试中优于对照组，并且接近于DeepSeek-R1在AIME 2024的表现。此蒸馏方法为创建更小、高性能且计算成本较低的大规模语言模型提供了一种可扩展解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The challenge of reducing the size of Large Language Models (LLMs) whilemaintaining their performance has gained significant attention. However,existing methods, such as model distillation and transfer learning, often failto achieve high accuracy. To address this limitation, we introduce theBranch-Merge distillation approach, which enhances model compression throughtwo phases: (1) the Branch Phase, where knowledge from a large teacher model is\textit{selectively distilled} into specialized student models viadomain-specific supervised fine-tuning (SFT); And (2) the Merge Phase, wherethese student models are merged to enable cross-domain knowledge transfer andimprove generalization. We validate our distillation approach using DeepSeek-R1as the teacher and DeepSeek-R1-Distill-Qwen-32B as the student. The resultingmerged model, TinyR1-32B-Preview, outperforms its counterpartDeepSeek-R1-Distill-Qwen-32B across multiple benchmarks, including Mathematics(+5.5 points), Coding (+4.4 points) and Science (+2.9 points), while achievingnear-equal performance to DeepSeek-R1 on AIME 2024. The Branch-Mergedistillation approach provides a scalable solution for creating smaller,high-performing LLMs with reduced computational cost and time.</description>
      <author>example@mail.com (Lin Sun, Guangxiang Zhao, Xiaoqi Jian, Yuhan Wu, Weihong Lin, Yongfu Zhu, Change Jia, Linglin Zhang, Jinzhu Wu, Junfeng Ran, Sai-er Hu, Zihan Jiang, Junting Zhou, Wenrui Liu, Bin Cui, Tong Yang, Xiangzheng Zhang)</author>
      <guid isPermaLink="false">2503.04872v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>VLMs Play StarCraft II: A Benchmark and Multimodal Decision Method</title>
      <link>http://arxiv.org/abs/2503.05383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了VLM-Attention，这是一个结合了RGB视觉输入和自然语言观察的星际争霸II多模态环境，旨在让人工代理的行为更接近人类的认知过程。&lt;h4&gt;背景&lt;/h4&gt;传统的SMAC等框架依赖于与人类感知相差甚远的抽象状态表示，限制了人工行为的生态有效性。&lt;h4&gt;目的&lt;/h4&gt;通过引入视觉-语言模型、检索增强生成系统和动态角色任务分配系统来解决上述问题，使代理的行为更接近人类玩家。&lt;h4&gt;方法&lt;/h4&gt;{'VLM-Attention框架组成': ['一个强化了专门自我注意力机制的战略单位目标定位和战场评估的视觉-语言模型', '利用领域特定星际争霸II知识进行战术决策的检索增强生成系统', '通过动态角色任务分配实现协调多代理行为的任务分布系统']}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，基于VLM的代理在21个定制场景中可以执行复杂的战术机动而无需显式训练，并达到与传统MARL方法相当的性能。&lt;h4&gt;结论&lt;/h4&gt;这项工作为开发人类一致性的星际争霸II代理人奠定了基础，并推动了多模态游戏AI的研究议程。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了VLM-Attention，这是一个将人工智能代理感知与人机对战体验相匹配的星际争霸II多模态环境。传统框架如SMAC依赖于抽象状态表示，这与人类感知存在显著差异，限制了人工行为的生态有效性。我们的环境通过整合RGB视觉输入和自然语言观察来解决这一问题，这些更接近游戏期间的人类认知过程。VLM-Attention框架包括三个集成组件：（1）一种增强特定自我注意机制的战略单位目标定位和战场评估的视觉-语言模型；（2）一个利用领域特定星际争霸II知识辅助战术决策的检索增强生成系统；以及（3）一个通过动态角色任务分配实现协调多代理行为的任务分布系统。我们在21个定制场景中的实验结果显示，由基础模型驱动的VLM基于智能体可以执行复杂的战术机动而无需显式训练，并且实现了与传统需要大量迭代训练的传统MARL方法相当的性能。这项工作为开发人类一致性的星际争霸II代理人奠定了基础，并推动了多模态游戏AI的研究议程。我们的实现可在https://github.com/camel-ai/VLM-Play-StarCraft2访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce VLM-Attention, a multimodal StarCraft II environment that alignsartificial agent perception with the human gameplay experience. Traditionalframeworks such as SMAC rely on abstract state representations that divergesignificantly from human perception, limiting the ecological validity of agentbehavior. Our environment addresses this limitation by incorporating RGB visualinputs and natural language observations that more closely simulate humancognitive processes during gameplay. The VLM-Attention framework consists ofthree integrated components: (1) a vision-language model enhanced withspecialized self-attention mechanisms for strategic unit targeting andbattlefield assessment, (2) a retrieval-augmented generation system thatleverages domain-specific StarCraft II knowledge to inform tactical decisions,and (3) a dynamic role-based task distribution system that enables coordinatedmulti-agent behavior. Our experimental evaluation across 21 custom scenariosdemonstrates that VLM-based agents powered by foundation models (specificallyQwen-VL and GPT-4o) can execute complex tactical maneuvers without explicittraining, achieving comparable performance to traditional MARL methods thatrequire substantial training iterations. This work establishes a foundation fordeveloping human-aligned StarCraft II agents and advances the broader researchagenda of multimodal game AI. Our implementation is available athttps://github.com/camel-ai/VLM-Play-StarCraft2.</description>
      <author>example@mail.com (Weiyu Ma, Yuqian Fu, Zecheng Zhang, Guohao Li)</author>
      <guid isPermaLink="false">2503.05383v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive-LIO: Enhancing Robustness and Precision through Environmental Adaptation in LiDAR Inertial Odometry</title>
      <link>http://arxiv.org/abs/2503.05077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种松耦合的自适应LiDAR惯性里程计（Adaptive-LIO）方法，该方法解决了现有SLAM系统在面对不同场景时缺乏足够适应性的挑战。&lt;h4&gt;背景&lt;/h4&gt;新兴物联网应用如自动驾驶汽车对高精度定位和导航的需求日益增长。基于激光雷达与惯性测量单元的组合导航技术在机器人领域及自主驾驶中越来越受欢迎。&lt;h4&gt;目的&lt;/h4&gt;为提高LiDAR惯性里程计系统的准确性和鲁棒性，解决点云准确性降低、IMU数据错误累积以及地图分辨率固定导致定位精度下降等问题。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为Adaptive-LIO的自适应松耦合激光雷达惯性里程计方案。该方案通过引入自适应分割提高映射精确度；检测并调整由于IMU饱和带来的运动模式变化；根据距离传感器中心点的距离动态调节地图分辨率。&lt;h4&gt;主要发现&lt;/h4&gt;在多种复杂场景中验证了所提出方法的有效性和优越性能，展示了改进措施对系统效能的显著提升。&lt;h4&gt;结论&lt;/h4&gt;该研究通过解决当前LiDAR惯性里程计面临的关键挑战，大大增强了其适应各种应用场景的能力。开源代码可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;新兴物联网应用如自动驾驶汽车对高精度定位和导航的需求日益增长。目前基于激光雷达与惯性测量单元的组合导航技术在机器人领域及自主驾驶中越来越受欢迎。然而，许多现有的SLAM系统无法足够适应各种场景需求。挑战包括在恒定速度假设下随着帧间隔变长导致点云准确度降低；IMU饱和时错误信息累积的问题；以及在室内外场景切换期间由于使用固定分辨率地图而导致的定位精度下降。为了应对这些难题，我们提出了一种松耦合自适应LiDAR惯性里程计（Adaptive-LIO），该方法利用自适应分割提高映射精确度、通过IMU饱和和故障检测来调整运动模式，并根据激光雷达中心距离动态调节地图分辨率使用多分辨率体素图。我们的方法已在各种具有挑战性的场景中进行了测试，展示了所引入改进的有效性。代码在GitHub上开源：https://github.com/chengwei0427/adaptive_lio&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emerging Internet of Things (IoT) applications, such as driverless cars,have a growing demand for high-precision positioning and navigation. Nowadays,LiDAR inertial odometry becomes increasingly prevalent in robotics andautonomous driving. However, many current SLAM systems lack sufficientadaptability to various scenarios. Challenges include decreased point cloudaccuracy with longer frame intervals under the constant velocity assumption,coupling of erroneous IMU information when IMU saturation occurs, and decreasedlocalization accuracy due to the use of fixed-resolution maps duringindoor-outdoor scene transitions. To address these issues, we propose a looselycoupled adaptive LiDAR-Inertial-Odometry named \textbf{Adaptive-LIO}, whichincorporates adaptive segmentation to enhance mapping accuracy, adapts motionmodality through IMU saturation and fault detection, and adjusts map resolutionadaptively using multi-resolution voxel maps based on the distance from theLiDAR center. Our proposed method has been tested in various challengingscenarios, demonstrating the effectiveness of the improvements we introduce.The code is open-source on GitHub:\href{https://github.com/chengwei0427/adaptive_lio}{Adaptive-LIO}.</description>
      <author>example@mail.com (Chengwei Zhao, Kun Hu, Jie Xu, Lijun Zhao, Baiwen Han, Kaidi Wu, Maoshan Tian, Shenghai Yuan)</author>
      <guid isPermaLink="false">2503.05077v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Informed Mamba for Robust Point Cloud Processing</title>
      <link>http://arxiv.org/abs/2503.04953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了利用Mamba和Masked Autoencoder网络处理点云数据的新方法，适用于监督学习和自监督学习。&lt;h4&gt;背景&lt;/h4&gt;状态空间模型在自然语言处理（NLP）中显示出巨大潜力，并且最近在计算机视觉领域也有所突破。Mamba作为一种新颖的方法，在处理复杂结构的点云时展现出了独特的优势。&lt;h4&gt;目的&lt;/h4&gt;提出三项关键贡献，旨在增强Mamba在网络架构和数据处理能力上的表现。&lt;h4&gt;方法&lt;/h4&gt;{'第一项贡献': '利用图拉普拉斯算子的频谱来捕捉补丁连接性，并定义了一种对于视点变化鲁棒且比传统的3D网格遍历更好的等距不变性遍历顺序，更好地捕获形状流形。', '第二项贡献': '通过基于Laplacian光谱成分的信息递归地分割补丁，实施了更为精细的集成和片段分析策略。', '第三项贡献': '对于Masked Autoencoder中的令牌放置问题，在Mamba中提出了将令牌恢复到其原始位置的方法，以保持必要的顺序并提高学习效率。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在分类、分割以及少样本任务上优于现有的基线模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的新方法能够显著提升点云数据处理的效果，在NLP和计算机视觉领域具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;状态空间模型已在自然语言处理（NLP）中展现出巨大潜力，并且最近在计算机视觉领域也有所突破。本文介绍了一种新的利用Mamba和Masked Autoencoder网络的方法，旨在提高点云数据的监督学习和自监督学习能力。文中提出三项关键贡献以增强Mamba在网络架构和复杂结构处理上的性能：一是采用图拉普拉斯算子频谱捕捉补丁连接性并定义一种鲁棒遍历顺序；二是通过基于Laplacian光谱成分的信息递归分割补丁，进行更精细的片段分析；三是提出Masked Autoencoder中令牌恢复到原始位置的方法以保持学习效果。实验结果证明了该方法在分类、分割及少样本任务上的优势，并超越现有基线模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State space models have shown significant promise in Natural LanguageProcessing (NLP) and, more recently, computer vision. This paper introduces anew methodology leveraging Mamba and Masked Autoencoder networks for pointcloud data in both supervised and self-supervised learning. We propose threekey contributions to enhance Mamba's capability in processing complex pointcloud structures. First, we exploit the spectrum of a graph Laplacian tocapture patch connectivity, defining an isometry-invariant traversal order thatis robust to viewpoints and better captures shape manifolds than traditional 3Dgrid-based traversals. Second, we adapt segmentation via a recursive patchpartitioning strategy informed by Laplacian spectral components, allowing finerintegration and segment analysis. Third, we address token placement in MaskedAutoencoder for Mamba by restoring tokens to their original positions, whichpreserves essential order and improves learning. Extensive experimentsdemonstrate the improvements of our approach in classification, segmentation,and few-shot tasks over state-of-the-art baselines.</description>
      <author>example@mail.com (Ali Bahri, Moslem Yazdanpanah, Mehrdad Noori, Sahar Dastani, Milad Cheraghalikhani, David Osowiechi, Gustavo Adolfo Vargas Hakim, Farzad Beizaee, Ismail Ben Ayed, Christian Desrosiers)</author>
      <guid isPermaLink="false">2503.04953v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Escaping Plato's Cave: Towards the Alignment of 3D and Text Latent Spaces</title>
      <link>http://arxiv.org/abs/2503.05283v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了在大规模训练下，单模态3D编码器与文本特征空间的后训练对齐可能性。研究表明，直接进行特征对齐效果有限，但通过选择合适维度的子空间投影可以显著提高对齐质量。&lt;h4&gt;背景&lt;/h4&gt;以往的工作表明，在大规模训练时，单一模式2D视觉和文本编码器在不同表示下学习到的特征具有相似的结构特性。然而，关于3D编码器与其他模态的关系尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究单模态3D编码器与文本特征空间后训练对齐的可能性，并探索三维数据的独特属性。&lt;h4&gt;方法&lt;/h4&gt;首先展示了直接对单模态文本和3D编码器进行后训练特征对齐效果不佳。然后，专注于提取相应特征空间的子空间，并发现通过投影到精心选择的低维子空间上可以显著提高对齐质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过选择合适的维度子空间进行投影可以极大提升单模态3D和文本特征之间的对齐质量。&lt;h4&gt;结论&lt;/h4&gt;研究首次建立了后训练时期三维数据与文本特征空间对齐的基础，并揭示了两者共享的属性以及三维数据特有的性质。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，当在大规模下训练时，单一模式2D视觉和文本编码器会学习到具有显著结构特性的特性，即使它们起源于不同的表示。然而，3D编码器相对于其他模态的作用仍然未被探索。现有的利用大型数据集的3D基础模型通常与其他表示冻结编码器一起进行明确对齐目标训练。在这项工作中，我们探讨了与基于文本的特征空间相比从单一模式3D编码器中获得的表示后训练对齐的可能性。我们展示了直接在单模态文本和3D编码器上进行特征对齐会导致性能有限。然后，我们将重点放在相应特征空间子空间的提取上，并发现通过将学习到的表示投影到精心选择的低维子空间上可以显著提高对齐质量，从而改善匹配和检索任务中的准确性。我们的分析进一步揭示了这些共享子空间的本质，大致区分了语义和几何数据表示。总体而言，我们是第一个帮助建立3D单模态与文本特征空间后训练对齐基础的工作，并有助于突出三维数据与其他表示相比的共同点和独特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works have shown that, when trained at scale, uni-modal 2D vision andtext encoders converge to learned features that share remarkable structuralproperties, despite arising from different representations. However, the roleof 3D encoders with respect to other modalities remains unexplored.Furthermore, existing 3D foundation models that leverage large datasets aretypically trained with explicit alignment objectives with respect to frozenencoders from other representations. In this work, we investigate thepossibility of a posteriori alignment of representations obtained fromuni-modal 3D encoders compared to text-based feature spaces. We show that naivepost-training feature alignment of uni-modal text and 3D encoders results inlimited performance. We then focus on extracting subspaces of the correspondingfeature spaces and discover that by projecting learned representations ontowell-chosen lower-dimensional subspaces the quality of alignment becomessignificantly higher, leading to improved accuracy on matching and retrievaltasks. Our analysis further sheds light on the nature of these sharedsubspaces, which roughly separate between semantic and geometric datarepresentations. Overall, ours is the first work that helps to establish abaseline for post-training alignment of 3D uni-modal and text feature spaces,and helps to highlight both the shared and unique properties of 3D datacompared to other representations.</description>
      <author>example@mail.com (Souhail Hadgi, Luca Moschella, Andrea Santilli, Diego Gomez, Qixing Huang, Emanuele Rodolà, Simone Melzi, Maks Ovsjanikov)</author>
      <guid isPermaLink="false">2503.05283v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>CAUSAL3D: A Comprehensive Benchmark for Causal Learning from Visual Data</title>
      <link>http://arxiv.org/abs/2503.04852v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;真智能依赖于揭示和利用隐藏的因果关系的能力。尽管在人工智能和计算机视觉领域取得了显著进展，但在评估模型从复杂视觉数据中推断潜在因果性的能力方面仍然缺乏基准。&lt;h4&gt;背景&lt;/h4&gt;目前AI和CV领域的进步尚未提供足够的工具来衡量模型对隐含因果关系的理解和推理能力。&lt;h4&gt;目的&lt;/h4&gt;介绍extsc{extbf{Causal3D}}，一个集成了结构化数据（表格）与其对应的视觉表示（图像）的新型综合性基准测试平台，旨在评估因果推理的能力。&lt;h4&gt;方法&lt;/h4&gt;在系统框架下设计的extsc{extbf{Causal3D}}包含19个不同的3D场景数据集，涵盖了各种因果关系、视角和背景，允许在不同复杂度的情景中进行全面评价。同时评估了多种最先进的方法，包括经典的因果发现、因果表示学习以及大型语言/视觉-语言模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，当没有先验知识时，随着因果结构变得更加复杂，性能显著下降，这强调了即使高级方法在复杂的因果场景下也面临着挑战。&lt;h4&gt;结论&lt;/h4&gt;extsc{extbf{Causal3D}}作为一个重要的资源，在计算机视觉中推动因果推理的发展，并促进关键领域中的可信AI的建立。&lt;h4&gt;翻译&lt;/h4&gt;真正的智能在于发现并利用隐藏的因果关系的能力。尽管人工智能和计算机视觉领域已经取得了显著的进步，但在评估模型从复杂视觉数据中推断潜在因果性能力方面仍然存在不足。这篇论文引入了extsc{extbf{Causal3D}}这一新型且综合性的基准测试平台，它结合了结构化数据（表格）与其对应的图像表示，用于评价因果推理的能力。在系统框架内构建的extsc{extbf{Causal3D}}包括19个不同3D场景的数据集，涵盖了多样的因果关系、视角和背景，支持跨复杂度的情景进行评测。通过评估多种最先进的方法——包括传统的因果发现技术、因果表示学习以及大型/视觉-语言模型（LLMs/VLMs）——实验结果显示，在缺乏先验知识的情况下，随着因果结构变得更为复杂，性能显著下降，这表明即使是先进的方法在复杂的因果情景下也会遇到挑战。extsc{extbf{Causal3D}}为推进计算机视觉中的因果推理以及关键领域的可信AI发展提供了重要的资源。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; True intelligence hinges on the ability to uncover and leverage hidden causalrelations. Despite significant progress in AI and computer vision (CV), thereremains a lack of benchmarks for assessing models' abilities to infer latentcausality from complex visual data. In this paper, we introduce\textsc{\textbf{Causal3D}}, a novel and comprehensive benchmark that integratesstructured data (tables) with corresponding visual representations (images) toevaluate causal reasoning. Designed within a systematic framework, Causal3Dcomprises 19 3D-scene datasets capturing diverse causal relations, views, andbackgrounds, enabling evaluations across scenes of varying complexity. Weassess multiple state-of-the-art methods, including classical causal discovery,causal representation learning, and large/vision-language models (LLMs/VLMs).Our experiments show that as causal structures grow more complex without priorknowledge, performance declines significantly, highlighting the challenges evenadvanced methods face in complex causal scenarios. Causal3D serves as a vitalresource for advancing causal reasoning in CV and fostering trustworthy AI incritical domains.</description>
      <author>example@mail.com (Disheng Liu, Yiran Qiao, Wuche Liu, Yiren Lu, Yunlai Zhou, Tuo Liang, Yu Yin, Jing Ma)</author>
      <guid isPermaLink="false">2503.04852v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Neural Configuration-Space Barriers for Manipulation Planning and Control</title>
      <link>http://arxiv.org/abs/2503.04929v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;高维机器人机械手在复杂和动态环境中进行规划与控制时需要高效的计算能力和可靠的保证安全。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一框架用于运动规划和控制，并通过配置空间距离函数（CDF）障碍物来实现安全性约束。&lt;h4&gt;方法&lt;/h4&gt;开发了一种神经网络学习的分布鲁棒性配置空间距离函数障碍物，以处理建模误差和传感器噪声，同时利用在线传感器观测减少碰撞检测操作。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明所提出的基于神经网络的CDF障碍物形式化能够使机械手在复杂且动态环境中实现高效的规划与可靠的实时安全控制。&lt;h4&gt;结论&lt;/h4&gt;该研究通过学习配置空间距离函数作为机器人体表征的方法，成功实现了高维机器人在复杂环境中的高效和可靠的安全性保障。&lt;h4&gt;翻译&lt;/h4&gt;计划和控制高维度的机器人机械臂，在拥挤、动态的环境中需要计算效率以及稳健的安全保证。受到最近关于使用神经网络学习配置空间距离函数（CDF）作为机器人体表征的研究进展启发，我们提出了一种统一框架用于运动规划和控制，并将安全性约束以CDF障碍物的形式化表示出来。一个CDF障碍物可以近似局部自由配置空间，大大减少了在运动规划过程中进行碰撞检测操作的数量。然而，在线传感器观察中利用神经网络学习的CDF障碍会引入不确定性，必须考虑这些不确定因素来进行控制器综合设计。为了处理这种情况，我们开发了一种分布鲁棒性CDF障碍形式化方法用于控制，明确地处理建模误差和传感器噪声而不需要已知底层分布假设。在6自由度xArm机械臂上的仿真及硬件实验表明：我们的神经网络CDF障碍物形式化能够使高效的规划与可靠的实时安全控制得以实现，在拥挤且动态的环境中仅依靠机载点云观测。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning and control for high-dimensional robot manipulators in cluttered,dynamic environments require both computational efficiency and robust safetyguarantees. Inspired by recent advances in learning configuration-spacedistance functions (CDFs) as robot body representations, we propose a unifiedframework for motion planning and control that formulates safety constraints asCDF barriers. A CDF barrier approximates the local free configuration space,substantially reducing the number of collision-checking operations duringmotion planning. However, learning a CDF barrier with a neural network andrelying on online sensor observations introduce uncertainties that must beconsidered during control synthesis. To address this, we develop adistributionally robust CDF barrier formulation for control that explicitlyaccounts for modeling errors and sensor noise without assuming a knownunderlying distribution. Simulations and hardware experiments on a 6-DoF xArmmanipulator show that our neural CDF barrier formulation enables efficientplanning and robust real-time safe control in cluttered and dynamicenvironments, relying only on onboard point-cloud observations.</description>
      <author>example@mail.com (Kehan Long, Ki Myung Brian Lee, Nikola Raicevic, Niyas Attasseri, Melvin Leok, Nikolay Atanasov)</author>
      <guid isPermaLink="false">2503.04929v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Efficiently parallelizable kernel-based multi-scale algorithm</title>
      <link>http://arxiv.org/abs/2503.04914v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基于核的方法已被证明是处理散乱数据插值问题的一种强大的逼近方法，在计算效率上优于传统的核基插值技术。&lt;h4&gt;目的&lt;/h4&gt;本文旨在提出并分析一种有效且可并行化的实现方案，来解决上述多尺度方法的高效实施问题。&lt;h4&gt;方法&lt;/h4&gt;文中介绍了一种基于点云层次结构和紧支撑径向基函数（如Wendland函数）的单体化计算方法，并对其进行详细分析。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法具有高效的并行处理能力，能够更有效地实现基于核的多尺度逼近技术。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于核的多尺度逼近提供了一种新的高效且可扩展的实施策略。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的研究背景、目的、方法以及主要发现和结论等内容已以中文进行总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The kernel-based multi-scale method has been proven to be a powerfulapproximation method for scattered data approximation problems which iscomputationally superior to conventional kernel-based interpolation techniques.The multi-scale method is based of an hierarchy of point clouds and compactlysupported radial basis functions, typically Wendland functions. There is a richbody of literature concerning the analysis of this method including errorestimates. This article addresses the efficient parallelizable implementationof those methods. To this end, we present and analyse a monolithic approach tocompute the kernel-based multi-scale approximation.</description>
      <author>example@mail.com (Federico Lot, Christian Rieger)</author>
      <guid isPermaLink="false">2503.04914v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>L-FUSION: Laplacian Fetal Ultrasound Segmentation &amp; Uncertainty Estimation</title>
      <link>http://arxiv.org/abs/2503.05245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under Review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;L-FUSION是一种用于胎儿超声图像分析的框架，旨在通过集成不确定性量化和大规模基础模型来提高对正常和异常扫描中胎儿结构分割的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;产前超声波检查对于早期检测发育异常至关重要。然而，操作者依赖和技术限制（例如固有伪影和效果、设置错误）会使图像解释变得复杂，并增加诊断不确定性。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，用于通过无监督学习和大规模基础模型来量化胎儿结构分割中的不确定性和增强超声波影像中病变与正常解剖结构的区分能力。&lt;h4&gt;方法&lt;/h4&gt;提出使用随机分割网络（Stochastic Segmentation Networks）中的aleatoric logit分布和Laplace近似结合快速Hessian估计，以估计仅来自分割头的认知不确定性。同时引入集成Dropout组件，生成增强不确定性和分割反事实图，在超声影像中可靠地区分病变与正常胎儿解剖。&lt;h4&gt;主要发现&lt;/h4&gt;评估结果表明，L-FUSION在多个数据集上实现了优越的分割准确度和一致性的不确定性量化，从而支持现场决策，并为临床环境中胎儿超声波分析的进步提供了一个可扩展的解决方案。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的框架，不仅能够提高对胎儿结构分割的准确性，还能增强不确定性和反事实图以区分病变与正常解剖，从而改进诊断反馈并减少手动疾病标注的需求。此外，L-FUSION在临床环境中为超声波分析提供了可靠的决策支持。&lt;h4&gt;翻译&lt;/h4&gt;准确分析产前超声（US）对于早期检测发育异常至关重要。然而，操作者依赖和技术限制可以复杂化图像解释和诊断不确定性的评估。我们提出了一个框架L-FUSION（基于不确定性量化和大规模基础模型的拉普拉斯胎儿US分割），它通过无监督、规范性学习与大规模基础模型集成来增强正常和病理扫描中胎儿结构的鲁棒分割。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate analysis of prenatal ultrasound (US) is essential for earlydetection of developmental anomalies. However, operator dependency andtechnical limitations (e.g. intrinsic artefacts and effects, setting errors)can complicate image interpretation and the assessment of diagnosticuncertainty. We present L-FUSION (Laplacian Fetal US Segmentation withIntegrated FoundatiON models), a framework that integrates uncertaintyquantification through unsupervised, normative learning and large-scalefoundation models for robust segmentation of fetal structures in normal andpathological scans. We propose to utilise the aleatoric logit distributions ofStochastic Segmentation Networks and Laplace approximations with fast Hessianestimations to estimate epistemic uncertainty only from the segmentation head.This enables us to achieve reliable abnormality quantification for instantdiagnostic feedback. Combined with an integrated Dropout component, L-FUSIONenables reliable differentiation of lesions from normal fetal anatomy withenhanced uncertainty maps and segmentation counterfactuals in US imaging. Itimproves epistemic and aleatoric uncertainty interpretation and removes theneed for manual disease-labelling. Evaluations across multiple datasets showthat L-FUSION achieves superior segmentation accuracy and consistentuncertainty quantification, supporting on-site decision-making and offering ascalable solution for advancing fetal ultrasound analysis in clinical settings.</description>
      <author>example@mail.com (Johanna P. Müller, Robert Wright, Thomas G. Day, Lorenzo Venturini, Samuel F. Budd, Hadrien Reynaud, Joseph V. Hajnal, Reza Razavi, Bernhard Kainz)</author>
      <guid isPermaLink="false">2503.05245v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering</title>
      <link>http://arxiv.org/abs/2502.20224v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures, 5 tables, submitted to The 28th International  Conference on Medical Image Computing and Computer Assisted Intervention  (MICCAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于无监督学习的自动诊断糖尿病性黄斑水肿(DME)系统RURANET++，该系统在提高病变特征提取和图像处理效率的同时，优化了聚类算法。&lt;h4&gt;背景&lt;/h4&gt;DME是糖尿病患者常见的并发症之一，严重威胁视力健康。尽管深度学习在医学影像分析中取得了一定进展，但传统诊断方法依赖大量标注数据和主观眼科医生评估，限制了实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于无监督学习的自动化DME诊断系统RURANET++，旨在减少对专家依赖并提高效率。&lt;h4&gt;方法&lt;/h4&gt;框架采用优化后的U-Net架构，并嵌入空间与通道挤压与激励(SCSE)注意力机制；利用预训练GoogLeNet模型提取视网膜图像深层特征，并通过PCA降维至50维度以提升计算效率；引入一种新的多投影头聚类算法，控制簇多样性并通过动态调整相似度阈值来优化类别内一致性及类别间区分性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明RURANET++在精度、召回率和F1-score等多项指标上均表现优异，达到了0.8411的最高准确率。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效的无监督解决方案用于DME诊断，并具有重要的临床意义。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病性黄斑水肿(DME)，一种在糖尿病患者中普遍存在的并发症，是视力障碍和失明的主要原因。尽管深度学习已经在医学图像分析方面取得了显著的进步，但传统的DME诊断仍然依赖于大量的标注数据以及眼科医生的主观评估，这限制了其实际应用的可能性。为解决这一问题，我们提出了RURANET++，这是一种基于无监督学习的自动化的DME诊断系统。该框架结合了一个优化过的U-Net架构，并嵌入了空间和通道挤压与激励(SCSE)注意力机制以增强病变特征提取能力。在特征处理过程中，预训练好的GoogLeNet模型从视网膜图像中抽取深层特征，然后通过PCA进行基于50维度的降维来提高计算效率。值得注意的是，我们引入了一种新的多投影头聚类算法，在控制簇多样性的同时动态调整相似度阈值以优化类别内一致性和类别间区分性。实验结果表明RURANET++在多个评价指标中表现出色，达到了最高的准确率（0.8411）、精确度（0.8593）、召回率（0.8411）和F1分数（0.8390），且聚类质量卓越。这项工作为DME诊断提供了一种高效的无监督解决方案，并具有重要的临床意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic Macular Edema (DME), a prevalent complication among diabeticpatients, constitutes a major cause of visual impairment and blindness.Although deep learning has achieved remarkable progress in medical imageanalysis, traditional DME diagnosis still relies on extensive annotated dataand subjective ophthalmologist assessments, limiting practical applications. Toaddress this, we present RURANET++, an unsupervised learning-based automatedDME diagnostic system. This framework incorporates an optimized U-Netarchitecture with embedded Spatial and Channel Squeeze &amp; Excitation (SCSE)attention mechanisms to enhance lesion feature extraction. During featureprocessing, a pre-trained GoogLeNet model extracts deep features from retinalimages, followed by PCA-based dimensionality reduction to 50 dimensions forcomputational efficiency. Notably, we introduce a novel clustering algorithmemploying multi-projection heads to explicitly control cluster diversity whiledynamically adjusting similarity thresholds, thereby optimizing intra-classconsistency and inter-class discrimination. Experimental results demonstratesuperior performance across multiple metrics, achieving maximum accuracy(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), withexceptional clustering quality. This work provides an efficient unsupervisedsolution for DME diagnosis with significant clinical implications.</description>
      <author>example@mail.com (Wei Yang, Yiran Zhu, Jiayu Shen, Yuhan Tang, Chengchang Pan, Hui He, Yan Su, Honggang Qi)</author>
      <guid isPermaLink="false">2502.20224v2</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Kaiwu: A Multimodal Manipulation Dataset and Framework for Robot Learning and Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2503.05231v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为Kaiwu的多模态数据集，旨在解决复杂组装场景中同步多模态数据缺乏的问题。&lt;h4&gt;背景&lt;/h4&gt;机器人学习领域的最新技术，如基础模型和人类模仿学习，在大规模高质量数据的需求上存在瓶颈。这些问题在复杂的实际操作环境中尤为突出，尤其是在需要动态信息及其精细标注的情况下。&lt;h4&gt;目的&lt;/h4&gt;提供一个集成的人机环境数据收集框架，解决现有机器人领域中复杂组装场景同步多模态数据不足的问题。&lt;h4&gt;方法&lt;/h4&gt;Kaiwu数据集通过20个参与者和30种交互对象记录了包括手部动作、操作压力、组装过程声音、多视角视频、高精度运动捕捉信息、第一人称视图下的眼动追踪以及肌电图信号在内的多种类型的数据。并且进行了基于绝对时间戳的精细多层次标注。&lt;h4&gt;主要发现&lt;/h4&gt;Kaiwu数据集为机器人学习、灵巧操纵、人类意图研究和人机协作提供了丰富的资源和支持。&lt;h4&gt;结论&lt;/h4&gt;该数据集有望促进相关领域的发展，推动智能机器人技术的进步。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的先进机器人学习技术包括基础模型和从人类模仿学习的技术都对大规模高质量的数据提出了巨大需求，这些构成了通用智能机器人领域的瓶颈之一。本文提出Kaiwu多模态数据集来解决复杂组装场景中存在的同步多模态数据不足的问题，特别是在包含动态信息及其精细标注的情况下。该数据集首先提供了一个集成的人类、环境和机器人的数据收集框架，其中包括20个参与者和30种交互对象，总共产生了11,664次整合的操作实例。对于每次演示，手部动作、操作压力、组装过程的声音、多视角视频、高精度运动捕捉信息、带有第一人称视图的注视跟踪以及肌电图信号都被记录下来。基于绝对时间戳和语义分割标注进行了精细多层次的数据注释。Kaiwu数据集旨在促进机器人学习、灵巧操纵、人类意图研究及人机协作的研究工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cutting-edge robot learning techniques including foundation models andimitation learning from humans all pose huge demands on large-scale andhigh-quality datasets which constitute one of the bottleneck in the generalintelligent robot fields. This paper presents the Kaiwu multimodal dataset toaddress the missing real-world synchronized multimodal data problems in thesophisticated assembling scenario,especially with dynamics information and itsfine-grained labelling. The dataset first provides an integration ofhuman,environment and robot data collection framework with 20 subjects and 30interaction objects resulting in totally 11,664 instances of integratedactions. For each of the demonstration,hand motions,operation pressures,soundsof the assembling process,multi-view videos, high-precision motion captureinformation,eye gaze with first-person videos,electromyography signals are allrecorded. Fine-grained multi-level annotation based on absolute timestamp,andsemantic segmentation labelling are performed. Kaiwu dataset aims to facilitaterobot learning,dexterous manipulation,human intention investigation andhuman-robot collaboration research.</description>
      <author>example@mail.com (Shuo Jiang, Haonan Li, Ruochen Ren, Yanmin Zhou, Zhipeng Wang, Bin He)</author>
      <guid isPermaLink="false">2503.05231v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>ORANSight-2.0: Foundational LLMs for O-RAN</title>
      <link>http://arxiv.org/abs/2503.05200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究介绍了ORANSight-2.0，一个专注于开放无线接入网络（O-RAN）的大型语言模型项目。&lt;h4&gt;背景&lt;/h4&gt;尽管大规模语言模型在医疗、客户服务和商业营销等关键领域产生了变革性影响，但它们与O-RAN的集成仍有限。现有解决方案通常依赖于通用型LLM，这些LLM无法解决O-RAN的独特挑战和技术复杂性。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究提出了ORANSight-2.0，旨在开发专门针对O-RAN的基础语言模型。&lt;h4&gt;方法&lt;/h4&gt;基于18个大型语言模型和五种开源框架构建了ORANSight-2.0。使用QLoRA技术对这些模型进行微调，并采用了RANSTRUCT（一种新型的检索增强生成（RAG）基指令调整框架），通过两个LLM代理创建高质量的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;研究开发了srsRANBench，用于评估代码生成和理解；ORANSight-2.0模型在O-RAN领域基准测试中优于通用型及闭源模型5.421%，在srsRANBench上高出18.465%。&lt;h4&gt;结论&lt;/h4&gt;ORANSight-2.0在性能、计算成本和能源消耗方面都表现出了优越性，并且实验展示了其增强版LLM的能耗特征，包括训练、标准推理以及检索增强生成推理的成本。&lt;h4&gt;翻译&lt;/h4&gt;摘要讨论了大型语言模型（LLMs）对医疗保健、客户服务和商业营销等关键领域的影响，但指出这些技术在开放无线电接入网络（O-RAN）中的应用仍处于初级阶段。研究提出了一种新的解决方案ORANSight-2.0，旨在开发专为O-RAN设计的基础语言模型，并详细介绍了该方案的方法和技术细节以及其性能评估结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the transformative impact of Large Language Models (LLMs) acrosscritical domains such as healthcare, customer service, and business marketing,their integration into Open Radio Access Networks (O-RAN) remains limited. Thisgap is primarily due to the absence of domain-specific foundational models,with existing solutions often relying on general-purpose LLMs that fail toaddress the unique challenges and technical intricacies of O-RAN. To bridgethis gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiativeaimed at developing specialized foundational LLMs tailored for O-RAN. Built on18 LLMs spanning five open-source LLM frameworks, ORANSight-2.0 fine-tunesmodels ranging from 1 to 70B parameters, significantly reducing reliance onproprietary, closed-source models while enhancing performance for O-RAN. At thecore of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation(RAG) based instruction-tuning framework that employs two LLM agents to createhigh-quality instruction-tuning datasets. The generated dataset is then used tofine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluateORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for codegeneration and codebase understanding in the context of srsRAN, a widely used5G O-RAN stack. We also leverage ORANBench13K, an existing benchmark forassessing O-RAN-specific knowledge. Our comprehensive evaluations demonstratethat ORANSight-2.0 models outperform general-purpose and closed-source models,such as ChatGPT-4o and Gemini, by 5.421% on ORANBench and 18.465% onsrsRANBench, achieving superior performance while maintaining lowercomputational and energy costs. We also experiment with RAG-augmented variantsof ORANSight-2.0 LLMs and thoroughly evaluate their energy characteristics,demonstrating costs for training, standard inference, and RAG-augmentedinference.</description>
      <author>example@mail.com (Pranshav Gajjar, Vijay K. Shah)</author>
      <guid isPermaLink="false">2503.05200v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>RTFusion: A depth estimation network based on multimodal fusion in challenging scenarios</title>
      <link>http://arxiv.org/abs/2503.04821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于深度估计的多模态模型RTFusion，结合RGB和THR数据的优点来提高复杂场景下的深度估计准确性。&lt;h4&gt;背景&lt;/h4&gt;单一模式（如可见光或热红外影像）在复杂的现实世界环境中进行深度估计时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新颖的多模态融合机制EGFusion来增强深度估计模型的鲁棒性和精度。&lt;h4&gt;方法&lt;/h4&gt;RTFusion模型利用RGB和THR图像数据，包含Mutual Complementary Attention (MCA)模块和Edge Saliency Enhancement Module (ESEM)模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，在MS2和ViViD++等不同挑战性环境下，该模型能够生成高质量的深度图。&lt;h4&gt;结论&lt;/h4&gt;提出的方法在需要可靠深度估计的应用中展现出潜力，例如自动驾驶、机器人技术和增强现实技术。&lt;h4&gt;翻译&lt;/h4&gt;复杂现实世界场景中的深度估计是一项具有挑战性的任务，尤其是在仅依赖单一模态（如可见光或热红外影像）的情况下。本文提出了一个新颖的多模式深度估计模型RTFusion，通过结合RGB和THR数据的优点来提高深度估计的准确性和鲁棒性。该模型包括Mutual Complementary Attention (MCA)模块用于跨模态特征对齐以及Edge Saliency Enhancement Module (ESEM)以增强边缘细节保存能力。在MS2和ViViD++等数据集上的全面实验表明，所提出的模型能够在夜间、雨天及高反光等各种挑战性环境下持续生成高质量的深度图。实验结果强调了该方法在需要可靠深度估计的应用（如自动驾驶、机器人技术和增强现实）中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depth estimation in complex real-world scenarios is a challenging task,especially when relying solely on a single modality such as visible light orthermal infrared (THR) imagery. This paper proposes a novel multimodal depthestimation model, RTFusion, which enhances depth estimation accuracy androbustness by integrating the complementary strengths of RGB and THR data. TheRGB modality provides rich texture and color information, while the THRmodality captures thermal patterns, ensuring stability under adverse lightingconditions such as extreme illumination. The model incorporates a unique fusionmechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA)module for cross-modal feature alignment and the Edge Saliency EnhancementModule (ESEM) to improve edge detail preservation. Comprehensive experiments onthe MS2 and ViViD++ datasets demonstrate that the proposed model consistentlyproduces high-quality depth maps across various challenging environments,including nighttime, rainy, and high-glare conditions. The experimental resultshighlight the potential of the proposed method in applications requiringreliable depth estimation, such as autonomous driving, robotics, and augmentedreality.</description>
      <author>example@mail.com (Zelin Meng, Takanori Fukao)</author>
      <guid isPermaLink="false">2503.04821v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Dynamic-KGQA: A Scalable Framework for Generating Adaptive Question Answering Datasets</title>
      <link>http://arxiv.org/abs/2503.05049v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Dynamic-KGQA的框架，该框架能够根据知识图谱生成适应性的问题回答数据集。这种方法旨在解决传统静态基准测试可能导致的大规模语言模型记忆问题，并能提供可靠和可重复的结果。&lt;h4&gt;背景&lt;/h4&gt;随着基础模型的发展，对问答系统（QA）进行强大、灵活和大规模评估的需求变得越来越重要。传统的QA基准通常固定且公开，容易被大型语言模型记住并导致过拟合。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够生成适应性数据集的框架，以确保更准确地评估大型语言模型在实际场景中的表现，并减少记忆的风险。&lt;h4&gt;方法&lt;/h4&gt;Dynamic-KGQA使用知识图谱（KG）来生成新的问答数据集变体。这种方法保持底层分布的同时每次运行都会生成不同的数据集，从而提供公平和可重复的评估结果。&lt;h4&gt;主要发现&lt;/h4&gt;该框架支持细粒度控制数据集特性，并且能够为特定领域或主题生成问答数据集。此外，Dynamic-KGQA还能产生紧凑、语义一致的小知识图谱，有助于训练和评价KGQA模型。&lt;h4&gt;结论&lt;/h4&gt;通过引入动态可定制的基准测试方法，Dynamic-KGQA提供了一种更严格和灵活的方式来评估QA系统。&lt;h4&gt;翻译&lt;/h4&gt;随着问答（QA）系统在基础模型快速演变中的进步，需要强大、适应性强且大规模的评估基准变得至关重要。传统的QA基准通常静态且公开获取，使其容易受到数据污染和大型语言模型的记忆风险影响，从而导致对模型泛化能力的高估以及对其真实世界性能的可靠评估困难。本文提出了一种基于知识图谱（KG）生成自适应问答数据集的可扩展框架Dynamic-KGQA，旨在减轻记忆风险的同时保持统计一致性。与固定基准不同，Dynamic-KGQA每次运行都会生成新的数据集变体，同时保持底层分布不变，从而实现公平和可重复的评估。此外，该框架提供对数据集特性的细粒度控制，并支持特定领域或主题的问答数据集生成。动态、自定义化的基准测试范式能够使问答系统的更严谨与适应性评估得以实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As question answering (QA) systems advance alongside the rapid evolution offoundation models, the need for robust, adaptable, and large-scale evaluationbenchmarks becomes increasingly critical. Traditional QA benchmarks are oftenstatic and publicly available, making them susceptible to data contaminationand memorization by large language models (LLMs). Consequently, staticbenchmarks may overestimate model generalization and hinder a reliableassessment of real-world performance. In this work, we introduce Dynamic-KGQA,a scalable framework for generating adaptive QA datasets from knowledge graphs(KGs), designed to mitigate memorization risks while maintaining statisticalconsistency across iterations. Unlike fixed benchmarks, Dynamic-KGQA generatesa new dataset variant on every run while preserving the underlyingdistribution, enabling fair and reproducible evaluations. Furthermore, ourframework provides fine-grained control over dataset characteristics,supporting domain-specific and topic-focused QA dataset generation.Additionally, Dynamic-KGQA produces compact, semantically coherent subgraphsthat facilitate both training and evaluation of KGQA models, enhancing theirability to leverage structured knowledge effectively. To align with existingevaluation protocols, we also provide static large-scale train/test/validationsplits, ensuring comparability with prior methods. By introducing a dynamic,customizable benchmarking paradigm, Dynamic-KGQA enables a more rigorous andadaptable evaluation of QA systems.</description>
      <author>example@mail.com (Preetam Prabhu Srikar Dammu, Himanshu Naidu, Chirag Shah)</author>
      <guid isPermaLink="false">2503.05049v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Continual Pre-training of MoEs: How robust is your router?</title>
      <link>http://arxiv.org/abs/2503.05029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了稀疏激活的专家混合（MoE）架构在大型语言模型中的持续预训练能力，特别是在分布变化和遗忘问题上的表现。&lt;h4&gt;背景&lt;/h4&gt;许多前沿的语言模型已经采用了MoE架构。为了利用大量新收集的数据扩展这些模型的能力，研究人员希望了解如何对现有模型进行增量式的微调而不完全重新训练。&lt;h4&gt;目的&lt;/h4&gt;研究在稀疏激活的专家混合（MoE）解码器独占变压器中，路由算法是否会影响持续预训练的表现，并探讨保持性能所需的策略。&lt;h4&gt;方法&lt;/h4&gt;通过大规模实验（涉及超过20亿参数的Switch和DeepSeek MoE大语言模型，在6000亿标记上进行训练），使用Sinkhorn-Balanced和Z-and-Aux-loss-balanced路由算法评估MoE持续预训练的表现。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现，无论是否采用重放机制（replay），基于Sinkhorn-Balanced和Z-and-Aux-loss-balanced的路由算法在分布变化下表现稳健，并且能够在不完全重新训练的情况下保持样本效率和性能水平。&lt;h4&gt;结论&lt;/h4&gt;研究表明，MoE大语言模型可以以较低的成本持续预训练并匹配完全重新训练后的性能。这意味着，在实际应用中使用MoE架构可能更加高效且具有成本效益。&lt;h4&gt;翻译&lt;/h4&gt;Sparsely-activated Mixture of Experts (MoE) transformers are promising architectures for foundation models. Compared to dense transformers, MoEs offer better sample efficiency during training and stronger performance. The study examines the impact of routing algorithms on continual pre-training in MoE transformers and finds that MoE LLMs maintain their sample efficiency and performance relative to a FLOP-matched dense model even without replay.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sparsely-activated Mixture of Experts (MoE) transformers are promisingarchitectures for foundation models. Compared to dense transformers thatrequire the same amount of floating point operations (FLOPs) per forward pass,MoEs benefit from improved sample efficiency at training time and achieve muchstronger performance. Many closed-source and open-source frontier languagemodels have thus adopted an MoE architecture. Naturally, practitioners willwant to extend the capabilities of these models with large amounts of newlycollected data without completely re-training them. Prior work has shown that asimple combination of replay and learning rate re-warming and re-decaying canenable the continual pre-training (CPT) of dense decoder-only transformers withminimal performance degradation compared to full re-training. In the case ofdecoder-only MoE transformers, however, it is unclear how the routing algorithmwill impact continual pre-training performance: 1) do the MoE transformer'srouters exacerbate forgetting relative to a dense model?; 2) do the routersmaintain a balanced load on previous distributions after CPT?; 3) are the samestrategies applied to dense models sufficient to continually pre-train MoELLMs? In what follows, we conduct a large-scale (&gt;2B parameter switch andDeepSeek MoE LLMs trained for 600B tokens) empirical study across four MoEtransformers to answer these questions. Our results establish a surprisingrobustness to distribution shifts for both Sinkhorn-Balanced andZ-and-Aux-loss-balanced routing algorithms, even in MoEs continuallypre-trained without replay. Moreover, we show that MoE LLMs maintain theirsample efficiency (relative to a FLOP-matched dense model) during CPT and thatthey can match the performance of a fully re-trained MoE at a fraction of thecost.</description>
      <author>example@mail.com (Benjamin Thérien, Charles-Étienne Joseph, Zain Sarwar, Ashwinee Panda, Anirban Das, Shi-Xiong Zhang, Stephen Rawls, Sambit Sahu, Eugene Belilovsky, Irina Rish)</author>
      <guid isPermaLink="false">2503.05029v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>HeTGB: A Comprehensive Benchmark for Heterophilic Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2503.04822v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Heterophilic Text-attributed Graph Benchmark (HeTGB)，这是一个包含五种现实世界异质图数据集的新型基准，这些数据集来自不同的领域，并且节点通过广泛的文本描述得到了增强。&lt;h4&gt;背景&lt;/h4&gt;图形神经网络在同构假设下对关系数据建模取得了成功。然而，在许多实际场景中，链接节点属于不同类别或具有多样属性的情况（即异质性）普遍存在。此外，图中的许多节点都与文本描述相关联，形成了包含异质性的文本属性图。&lt;h4&gt;目的&lt;/h4&gt;为解决目前缺乏全面基准的问题，本文提出了HeTGB，旨在评估图形神经网络、预训练语言模型以及协同训练方法在节点分类任务上的性能，并探讨异质性文本属性图中的挑战及现有模型的局限性。&lt;h4&gt;方法&lt;/h4&gt;HeTGB包括五个来自不同领域的现实世界异质图数据集，节点通过丰富的文本描述进行了增强。该基准允许对GNNs、PLMs和协同训练方法进行系统评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验展示了文本属性在异质图中的作用，并分析了异质性文本属性图所面临的挑战以及现有模型的局限性，还探讨了图形结构与文本属性之间的相互关系。研究提供了基准数据集及其基线实现的公开资源。&lt;h4&gt;结论&lt;/h4&gt;通过HeTGB的研究揭示了当前模型处理异质图和文本属性时存在的问题，并为未来相关领域的进一步研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到，图神经网络在处理同构图形中的关系数据建模中表现出色。然而，在许多现实世界的应用场景中，链接节点可能来自不同的类别或具有不同特征（即异质性）。此外，很多节点还附带了描述性的文本信息，形成了含有异质性的文本属性图。由于缺乏全面的基准测试集，这些异质性图的研究进展缓慢。为此，本文介绍了Heterophilic Text-attributed Graph Benchmark (HeTGB)，该框架包含五个来自不同领域的现实世界异质数据集，并且节点通过广泛的文本描述得到了增强。HeTGB能够用于评估GNNs、预训练的语言模型（PLMs）和协同训练方法在节点分类任务中的表现，同时还能展示文本属性对于处理异质性图的重要性和当前的方法的限制。研究者公开发布了该基准及其基线实现以推动进一步的研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have demonstrated success in modeling relationaldata primarily under the assumption of homophily. However, many real-worldgraphs exhibit heterophily, where linked nodes belong to different categoriesor possess diverse attributes. Additionally, nodes in many domains areassociated with textual descriptions, forming heterophilic text-attributedgraphs (TAGs). Despite their significance, the study of heterophilic TAGsremains underexplored due to the lack of comprehensive benchmarks. To addressthis gap, we introduce the Heterophilic Text-attributed Graph Benchmark(HeTGB), a novel benchmark comprising five real-world heterophilic graphdatasets from diverse domains, with nodes enriched by extensive textualdescriptions. HeTGB enables systematic evaluation of GNNs, pre-trained languagemodels (PLMs) and co-training methods on the node classification task. Throughextensive benchmarking experiments, we showcase the utility of text attributesin heterophilic graphs, analyze the challenges posed by heterophilic TAGs andthe limitations of existing models, and provide insights into the interplaybetween graph structures and textual attributes. We have publicly releasedHeTGB with baseline implementations to facilitate further research in thisfield.</description>
      <author>example@mail.com (Shujie Li, Yuxia Wu, Chuan Shi, Yuan Fang)</author>
      <guid isPermaLink="false">2503.04822v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Incentivizing Multi-Tenant Split Federated Learning for Foundation Models at the Network Edge</title>
      <link>http://arxiv.org/abs/2503.04971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Index Terms: Foundation models, Edge computing, Split federated  learning, Multi-tenant system, Incentive mechanism&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为PRINCE的新型价格激励机制，以促进多租户环境下的分割联邦学习（SFL）过程中的设备参与，并提高基础模型在边缘设备上的高效微调。&lt;h4&gt;背景&lt;/h4&gt;现有的研究主要集中在单一租户的SFL场景上，缺乏针对多种租户环境的定制化激励机制。在实际应用中，边缘网络通常需要支持多样化的下游任务，每个租户有不同的模型类型、性能目标和微调截止日期等要求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的激励机制PRINCE，以促进多租户SFL场景下不同设备的有效参与，并确保各自的需求得到满足。&lt;h4&gt;方法&lt;/h4&gt;首先设计了一种偏倚抵抗的全局SFL模型聚合方案来消除由独立设备微调产生的模型偏差。接着推导出一个严格的SFL收敛边界，用于评估异构设备对基础模型性能提升贡献的程度，从而指导租户制定激励策略。最后通过Stackelberg博弈均衡（SE）分析模型间竞争，并得出每个SFL租户的最优激励策略。&lt;h4&gt;主要发现&lt;/h4&gt;模拟实验表明，在处理四种典型的SFL租户类型（ViT、BERT、Whisper和LLaMA），涵盖文本、图像和音频等多种数据模式的任务时，PRINCE机制相比现有最佳方法可将基础模型微调加速最多3.07倍，并且始终能够满足性能目标。&lt;h4&gt;结论&lt;/h4&gt;提出的PRINCE激励机制能够有效促进多租户环境中SFL的高效执行，提高资源利用效率并确保良好的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) such as GPT-4 exhibit exceptional generativecapabilities across diverse downstream tasks through fine-tuning. SplitFederated Learning (SFL) facilitates privacy-preserving FM fine-tuning onresource-constrained local devices by offloading partial FM computations toedge servers, enabling device-edge synergistic fine-tuning. Practical edgenetworks often host multiple SFL tenants to support diversified downstreamtasks. However, existing research primarily focuses on single-tenant SFLscenarios, and lacks tailored incentive mechanisms for multi-tenant settings,which are essential to effectively coordinate self-interested local devices forparticipation in various downstream tasks, ensuring that each SFL tenant'sdistinct FM fine-tuning requirements (e.g., FM types, performance targets, andfine-tuning deadlines) are met. To address this gap, we propose a novelPrice-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offerstrategic price incentives, which solicit high-quality device participation forefficient FM fine-tuning. Specifically, we first develop a bias-resilientglobal SFL model aggregation scheme to eliminate model biases caused byindependent device participation. We then derive a rigorous SFL convergencebound to evaluate the contributions of heterogeneous devices to FM performanceimprovements, guiding the incentive strategies of SFL tenants. Furthermore, wemodel inter-tenant device competition as a congestion game for Stackelbergequilibrium (SE) analysis, deriving each SFL tenant's optimal incentivestrategy. Extensive simulations involving four representative SFL tenant types(ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images,and audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07xcompared to state-of-the-art approaches, while consistently meeting fine-tuningperformance targets.</description>
      <author>example@mail.com (Songyuan Li, Jia Hu, Geyong Min, Haojun Huang)</author>
      <guid isPermaLink="false">2503.04971v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adapt3R: Adaptive 3D Scene Representation for Domain Transfer in Imitation Learning</title>
      <link>http://arxiv.org/abs/2503.04877v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Videos, code, and data: https://pairlab.github.io/Adapt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;模仿学习(IL)在训练机器人执行复杂多样的操作任务方面非常有效，但当观测数据超出训练分布时，其性能急剧下降。为了提高IL策略的泛化能力，提出了使用3D场景表示的方法，但在跨实体和新相机姿态设置中进行评估后发现改进有限。为解决这些挑战，本文提出了一种通用的3D场景表示方法Adapt3R，它通过新颖的架构将一个或多个RGBD摄像头的数据综合成一个向量，供任意IL算法使用。&lt;h4&gt;背景&lt;/h4&gt;模仿学习(IL)在机器人执行复杂任务中表现出色，但在面对未见过的情境时性能大幅下降。尽管已有研究尝试利用包含标定RGBD相机观测信息的3D场景表示来改进策略泛化能力，但其效果有限。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法Adapt3R以提高模仿学习策略在跨实体和新相机姿态下的零样本迁移能力。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的2D骨干网络提取关于场景的语义信息，并利用3D作为定位该信息相对于末端执行器的方式，合成单一向量供IL算法使用。通过与最新的多任务IL算法结合进行端到端训练来验证其效果。&lt;h4&gt;主要发现&lt;/h4&gt;Adapt3R能够在保持多任务学习能力的同时实现零样本迁移至新实体和相机姿态，并且进行了详尽的消融实验以探索点云观测编码器的设计空间。&lt;h4&gt;结论&lt;/h4&gt;通过引入Adapt3R，模仿学习策略在面对未知情境时的表现得到了显著改善，为未来机器人操作任务的学习提供了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation Learning (IL) has been very effective in training robots to performcomplex and diverse manipulation tasks. However, its performance declinesprecipitously when the observations are out of the training distribution. 3Dscene representations that incorporate observations from calibrated RGBDcameras have been proposed as a way to improve generalizability of IL policies,but our evaluations in cross-embodiment and novel camera pose settings foundthat they show only modest improvement. To address those challenges, we proposeAdaptive 3D Scene Representation (Adapt3R), a general-purpose 3D observationencoder which uses a novel architecture to synthesize data from one or moreRGBD cameras into a single vector that can then be used as conditioning forarbitrary IL algorithms. The key idea is to use a pretrained 2D backbone toextract semantic information about the scene, using 3D only as a medium forlocalizing this semantic information with respect to the end-effector. We showthat when trained end-to-end with several SOTA multi-task IL algorithms,Adapt3R maintains these algorithms' multi-task learning capacity while enablingzero-shot transfer to novel embodiments and camera poses. Furthermore, weprovide a detailed suite of ablation and sensitivity experiments to elucidatethe design space for point cloud observation encoders.</description>
      <author>example@mail.com (Albert Wilcox, Mohamed Ghanem, Masoud Moghani, Pierre Barroso, Benjamin Joffe, Animesh Garg)</author>
      <guid isPermaLink="false">2503.04877v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Invisible Strings: Revealing Latent Dancer-to-Dancer Interactions with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.04816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 10 figures, submitted to ICCC'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了双人舞蹈中的合作关系，并提出了一种基于图神经网络（GNN）的方法来捕捉和解释舞者之间复杂的互动关系。&lt;h4&gt;背景&lt;/h4&gt;在双人舞蹈中，舞者需要密切注意彼此的空间位置、动量以及相互施加的力量。虽然艺术家在表演时对这些细节有深刻的理解，但传统的记录方式无法准确地捕捉到这种细致的关系。&lt;h4&gt;目的&lt;/h4&gt;通过与关注深化合作关系理解的舞蹈艺术家合作，利用GNN来揭示和解释两位舞者之间复杂的联系。&lt;h4&gt;方法&lt;/h4&gt;使用视频到3D姿态提取流水线从当代双人舞蹈视频中提取3D动作，并对数据进行专门预处理以改进重建效果。然后训练GNN预测舞者之间的加权连接关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过可视化和解释预测的两舞者间的关系，展示了基于图的方法能够构建描述双人合作动态的新模型。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一些策略，利用这些见解来指导生成性和共同创造的工作室实践。&lt;h4&gt;翻译&lt;/h4&gt;在双人舞蹈中，舞者需要对彼此的空间位置、动量以及相互施加的力量有高度的理解。虽然艺术家可能在表演时对自己动作与合作伙伴的关系有深刻的认识，但常规的舞蹈记录方法无法捕捉到这种复杂且微妙的互动关系。通过与希望深入了解合作关系的舞蹈艺术家合作，我们利用图神经网络（GNN）来强调并解释两位舞者之间的复杂联系。使用视频到3D姿态提取流水线从精心挑选的当代双人舞蹈视频中提取3D动作，并进行专门预处理以改进重建效果，然后训练GNN预测舞者之间的加权连接关系。通过可视化和解释这些预测的关系，我们展示了基于图的方法能够构建描述双人合作动态的新模型。最后，提供了一些策略示例，说明如何利用这些洞察来指导生成性和共同创造的工作室实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dancing in a duet often requires a heightened attunement to one's partner:their orientation in space, their momentum, and the forces they exert on you.Dance artists who work in partnered settings might have a strong embodiedunderstanding in the moment of how their movements relate to their partner's,but typical documentation of dance fails to capture these varied and subtlerelationships. Working closely with dance artists interested in deepening theirunderstanding of partnering, we leverage Graph Neural Networks (GNNs) tohighlight and interpret the intricate connections shared by two dancers. Usinga video-to-3D-pose extraction pipeline, we extract 3D movements from curatedvideos of contemporary dance duets, apply a dedicated pre-processing to improvethe reconstruction, and train a GNN to predict weighted connections between thedancers. By visualizing and interpreting the predicted relationships betweenthe two movers, we demonstrate the potential for graph-based methods toconstruct alternate models of the collaborative dynamics of duets. Finally, weoffer some example strategies for how to use these insights to inform agenerative and co-creative studio practice.</description>
      <author>example@mail.com (Luis Vitor Zerkowski, Zixuan Wang, Ilya Vidrin, Mariel Pettee)</author>
      <guid isPermaLink="false">2503.04816v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>LLaVE: Large Language and Vision Embedding Models with Hardness-Weighted Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.04812v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了一个新的框架LLaVE，旨在改进现有基于语言多模态（LMM）的嵌入模型在处理困难负样本时的表现。&lt;h4&gt;背景&lt;/h4&gt;当前普遍使用的基于LMM的嵌入模型训练使用标准InfoNCE损失函数时，在正负对相似性分布上存在高度重叠，这使得区分硬负样本变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单而有效的框架，该框架可以根据负面实例在鉴别上的难度动态改善嵌入模型的表现学习。&lt;h4&gt;方法&lt;/h4&gt;训练了一系列新的模型（LLaVE），并在包含4个元任务和36个数据集的MMEB基准上进行了评估。这些新模型旨在提高区分难负样本的能力，并且显示出良好的可扩展性和效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，LLaVE在所有测试中建立了强大的基线并实现了最先进的性能。例如，LLAve-2B超越了之前的7B模型的最佳表现，而LLAVE-7B则进一步提高了6.2分的性能。&lt;h4&gt;结论&lt;/h4&gt;尽管LLaVE是在图像文本数据上训练的，但该模型可以零样本泛化到视频文本检索任务，并表现出强大的性能。这表明LLaVE在其他嵌入任务中具有巨大的潜在迁移能力。&lt;h4&gt;翻译&lt;/h4&gt;通用多模态嵌入模型在交错式图文检索、多模态RAG以及多模态聚类等任务中发挥着关键作用。然而，实证结果显示，现有基于语言的多模态模型使用标准InfoNCE损失训练时，在正负对相似性分布上存在高度重叠，使得区分硬负样本变得困难。为解决此问题，我们提出了一种简单而有效的框架，该框架可以根据负面实例在鉴别上的难度动态改善嵌入模型的表现学习。在此框架内，我们在MMEB基准上训练了一系列新的模型（LLaVE），并对其进行了评估。实验结果表明，LLAVE建立了强大的基线，并实现了最先进的性能表现的同时还展示了极好的可扩展性和效率。具体来说，LLAve-2B超越了之前的7B模型的最佳表现，而LLAVE-7B则进一步提高了6.2分的性能。虽然LLaVE是在图像文本数据上训练的，但该模型可以零样本泛化到视频文本检索任务，并表现出强大的性能，这表明LLAVE在其他嵌入任务中具有巨大的潜在迁移能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Universal multimodal embedding models play a critical role in tasks such asinterleaved image-text retrieval, multimodal RAG, and multimodal clustering.However, our empirical results indicate that existing LMM-based embeddingmodels trained with the standard InfoNCE loss exhibit a high degree of overlapin similarity distribution between positive and negative pairs, making itchallenging to distinguish hard negative pairs effectively. To deal with thisissue, we propose a simple yet effective framework that dynamically improvesthe embedding model's representation learning for negative pairs based on theirdiscriminative difficulty. Within this framework, we train a series of models,named LLaVE, and evaluate them on the MMEB benchmark, which covers 4 meta-tasksand 36 datasets. Experimental results show that LLaVE establishes strongerbaselines that achieve state-of-the-art (SOTA) performance while demonstratingstrong scalability and efficiency. Specifically, LLaVE-2B surpasses theprevious SOTA 7B models, while LLaVE-7B achieves a further performanceimprovement of 6.2 points. Although LLaVE is trained on image-text data, it cangeneralize to text-video retrieval tasks in a zero-shot manner and achievestrong performance, demonstrating its remarkable potential for transfer toother embedding tasks.</description>
      <author>example@mail.com (Zhibin Lan, Liqiang Niu, Fandong Meng, Jie Zhou, Jinsong Su)</author>
      <guid isPermaLink="false">2503.04812v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Spatial regularisation for improved accuracy and interpretability in keypoint-based registration</title>
      <link>http://arxiv.org/abs/2503.04499v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种三重损失函数，用于通过优化固定和移动体素之间的相似性度量来改进无监督注册策略。此方法特别关注基于无监督关键点检测的方法，旨在增强特征的空间分布可解释性。&lt;h4&gt;背景&lt;/h4&gt;无监督配准策略不依赖于地面实况变换或分割标签，并且近年来以一种新类型的无监督关键点检测技术为基础的方案非常有前景，因为它们可以提高算法的透明度和理解度。&lt;h4&gt;目的&lt;/h4&gt;改进基于关键点的注册方法的空间特征分布，使其更加精确并且具有解剖学意义，同时提高解释性。&lt;h4&gt;方法&lt;/h4&gt;通过引入三种损失函数来实现：使用KL散度将特征视为概率型的关键点进行建模；锐化这些特征的空间分布以增加检测到的地标精度；以及在关键点之间引入新的排斥损失以鼓励空间多样性。&lt;h4&gt;主要发现&lt;/h4&gt;三重损失改进了特征的可解释性，使其更精确、更具解剖学意义。它在胎儿刚体运动跟踪和脑MRI仿射配准任务中均优于现有的无监督策略，并接近于有监督方法的表现。&lt;h4&gt;结论&lt;/h4&gt;这种新颖的方法显著提高了基于关键点检测的无监督注册技术的效果，在医学图像处理领域具有潜在的应用价值，代码可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised registration strategies bypass requirements in ground truthtransforms or segmentations by optimising similarity metrics between fixed andmoved volumes. Among these methods, a recent subclass of approaches based onunsupervised keypoint detection stand out as very promising forinterpretability. Specifically, these methods train a network to predictfeature maps for fixed and moving images, from which explainable centres ofmass are computed to obtain point clouds, that are then aligned in closed-form.However, the features returned by the network often yield spatially diffusepatterns that are hard to interpret, thus undermining the purpose ofkeypoint-based registration. Here, we propose a three-fold loss to regularisethe spatial distribution of the features. First, we use the KL divergence tomodel features as point spread functions that we interpret as probabilistickeypoints. Then, we sharpen the spatial distributions of these features toincrease the precision of the detected landmarks. Finally, we introduce a newrepulsive loss across keypoints to encourage spatial diversity. Overall, ourloss considerably improves the interpretability of the features, which nowcorrespond to precise and anatomically meaningful landmarks. We demonstrate ourthree-fold loss in foetal rigid motion tracking and brain MRI affineregistration tasks, where it not only outperforms state-of-the-art unsupervisedstrategies, but also bridges the gap with state-of-the-art supervised methods.Our code is available at https://github.com/BenBillot/spatial_regularisation.</description>
      <author>example@mail.com (Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, P. Ellen Grant, Nicholas Ayache, Hervé Delingette, Polina Golland)</author>
      <guid isPermaLink="false">2503.04499v2</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Parallel Corpora for Machine Translation in Low-resource Indic Languages: A Comprehensive Review</title>
      <link>http://arxiv.org/abs/2503.04797v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文综述了可用于印地语等低资源语言的平行语料库，探讨这些语料库在机器翻译模型训练中的作用和挑战。&lt;h4&gt;背景&lt;/h4&gt;平行语料库对于训练缺乏高质量双语文本数据的低资源语言的机器翻译（MT）模型至关重要。特别是对多种语言族、书写系统和地区变体的印地语等印度语言而言，这种重要性更加突出。&lt;h4&gt;目的&lt;/h4&gt;综述可用的印地语平行语料库，并分类这些语料库为纯文本到文本、代码切换和各种多模式数据集类别，强调它们在开发健壮的跨语言机器翻译系统中的意义。&lt;h4&gt;方法&lt;/h4&gt;除了资源列举之外，还批判性地审视了语料库创建过程中面临的挑战，如语言多样性、书写方式变化、数据稀缺性和非正式文本内容盛行的问题。同时讨论并评估这些语料库的质量和领域代表性等。&lt;h4&gt;主要发现&lt;/h4&gt;指出了开放性的挑战包括印地语之间的数据不平衡、质量与数量的权衡以及嘈杂、非正式和方言化数据对机器翻译性能的影响。&lt;h4&gt;结论&lt;/h4&gt;该论文概述了未来的研究方向，如利用跨语言转移学习来扩大多语言数据集并整合多媒体资源以增强翻译质量。据我们所知，这是首次专门针对低资源印地语的平行语料库进行详尽综述，并将其置于机器翻译上下文中。&lt;h4&gt;翻译&lt;/h4&gt;Parallel corpora play an important role in training machine translation (MT)models, particularly for low-resource languages where high-quality bilingualdata is scarce. This review provides a comprehensive overview of availableparallel corpora for Indic languages, which span diverse linguistic families,scripts, and regional variations.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parallel corpora play an important role in training machine translation (MT)models, particularly for low-resource languages where high-quality bilingualdata is scarce. This review provides a comprehensive overview of availableparallel corpora for Indic languages, which span diverse linguistic families,scripts, and regional variations. We categorize these corpora intotext-to-text, code-switched, and various categories of multimodal datasets,highlighting their significance in the development of robust multilingual MTsystems. Beyond resource enumeration, we critically examine the challengesfaced in corpus creation, including linguistic diversity, script variation,data scarcity, and the prevalence of informal textual content.We also discussand evaluate these corpora in various terms such as alignment quality anddomain representativeness. Furthermore, we address open challenges such as dataimbalance across Indic languages, the trade-off between quality and quantity,and the impact of noisy, informal, and dialectal data on MT performance.Finally, we outline future directions, including leveraging cross-lingualtransfer learning, expanding multilingual datasets, and integrating multimodalresources to enhance translation quality. To the best of our knowledge, thispaper presents the first comprehensive review of parallel corpora specificallytailored for low-resource Indic languages in the context of machinetranslation.</description>
      <author>example@mail.com (Rahul Raja, Arpita Vats)</author>
      <guid isPermaLink="false">2503.04797v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Fidelity Policy Gradient Algorithms</title>
      <link>http://arxiv.org/abs/2503.05696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多保真度策略梯度（MFPG）是一种新的强化学习框架，它利用少量的目标环境数据和大量的低保真度模拟数据来提高训练效率。&lt;h4&gt;背景&lt;/h4&gt;许多强化学习算法需要大量数据，并且在难以频繁与操作系统交互或高保真仿真昂贵或不可用的情况下无法使用。然而，低成本的低保真度模拟器可以为RL训练提供有用的数据。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的强化学习框架MFPG，该框架结合目标环境的小量样本和大量低保真度仿真实验数据来形成无偏估计，减少方差，并提高基于策略的梯度估计准确性。&lt;h4&gt;方法&lt;/h4&gt;开发了两种多保真度策略梯度算法：REINFORCE和近似策略优化（PPO）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在目标环境样本有限的情况下，MFPG可以比只使用高保真数据的基础线方法获得更高的奖励，并且提高了训练的稳定性。即使当基线模型在仿真交互次数提高十倍时，MFPG仍然能够与之匹配或超越。此外，MFPG甚至可以在低保真度环境差异很大时依然有效。&lt;h4&gt;结论&lt;/h4&gt;MFPG不仅为高效的模拟到真实环境转移提供了一种新的范例，而且还提供了管理策略性能和数据收集成本之间权衡的原理性方法。&lt;h4&gt;翻译&lt;/h4&gt;许多强化学习算法需要大量数据，使它们在难以频繁与操作系统交互或高保真仿真昂贵或不可用的应用中无法使用。与此同时，低成本模拟器（如减少阶模型、启发式奖励函数或生成环境）可以便宜地提供对于RL训练有用的少量有用的数据，即使这些模拟器过于粗糙而不能直接进行从仿真的真实环境转移。我们提出了多保真度策略梯度（MFPG），这是一种强化学习框架，它可以将目标环境的小量数据与大量低保真度仿真数据混合，以形成无偏、方差减小的估计（控制变量）用于基于策略的梯度。通过开发REINFORCE和近似策略优化的多保真变体来实例化该框架。一系列模拟机器人基准问题上的实验结果表明，在目标环境样本有限时，MFPG比仅使用高保真数据的基础线方法最高可达3.9倍的奖励，并提高了训练稳定性。此外，即使当基线模型在仿真交互次数提高十倍时，MFPG仍然能够与之匹配或超越。最后，观察到即便低保真度环境差异巨大时，MFPG依然可以有效训练策略。因此，MFPG不仅为高效的从模拟到真实环境转移提供了一种新的范例，而且还提供了管理策略性能和数据收集成本之间权衡的原理性方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many reinforcement learning (RL) algorithms require large amounts of data,prohibiting their use in applications where frequent interactions withoperational systems are infeasible, or high-fidelity simulations are expensiveor unavailable. Meanwhile, low-fidelity simulators--such as reduced-ordermodels, heuristic reward functions, or generative world models--can cheaplyprovide useful data for RL training, even if they are too coarse for directsim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RLframework that mixes a small amount of data from the target environment with alarge volume of low-fidelity simulation data to form unbiased, reduced-varianceestimators (control variates) for on-policy policy gradients. We instantiatethe framework by developing multi-fidelity variants of two policy gradientalgorithms: REINFORCE and proximal policy optimization. Experimental resultsacross a suite of simulated robotics benchmark problems demonstrate that whentarget-environment samples are limited, MFPG achieves up to 3.9x higher rewardand improves training stability when compared to baselines that only usehigh-fidelity data. Moreover, even when the baselines are given morehigh-fidelity samples--up to 10x as many interactions with the targetenvironment--MFPG continues to match or outperform them. Finally, we observethat MFPG is capable of training effective policies even when the low-fidelityenvironment is drastically different from the target environment. MFPG thus notonly offers a novel paradigm for efficient sim-to-real transfer but alsoprovides a principled approach to managing the trade-off between policyperformance and data collection costs.</description>
      <author>example@mail.com (Xinjie Liu, Cyrus Neary, Kushagra Gupta, Christian Ellis, Ufuk Topcu, David Fridovich-Keil)</author>
      <guid isPermaLink="false">2503.05696v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Kinodynamic Model Predictive Control for Energy Efficient Locomotion of Legged Robots with Parallel Elasticity</title>
      <link>http://arxiv.org/abs/2503.05666v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures. Accepted for publication at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用单向并行弹簧（UPS）来提高动态腿足机器人的能量效率的模型预测控制(MPC)框架。&lt;h4&gt;背景&lt;/h4&gt;现有的动态腿足机器人在高速移动时能耗较高，且电机扭矩峰值大。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新的MPC方法，利用UPS减少电机扭矩峰值和站立阶段的能量消耗，从而提高动态跳跃过程中的能量效率。&lt;h4&gt;方法&lt;/h4&gt;采用了分层控制结构，其中简化的动力学模型的MPC解决方案被用来预热非线性中心动力学和运动学约束下的动力学MPC。通过在单足机器人上加入UPS来验证其效果，并进行模拟实验。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果显示，在高速跳跃过程中使用UPS装备的单足机器人，运输成本（CoT）降低了38.8%；初步硬件试验表明能耗减少了14.8%。&lt;h4&gt;结论&lt;/h4&gt;该方法证明了通过采用UPS可以有效地提高腿足机器人的能量效率和动态性能。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们介绍了一种利用单向并行弹簧（UPS）来改善动态腿足机器人能量效率的模型预测控制（MPC）框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we introduce a kinodynamic model predictive control (MPC)framework that exploits unidirectional parallel springs (UPS) to improve theenergy efficiency of dynamic legged robots. The proposed method employs ahierarchical control structure, where the solution of MPC with simplifieddynamic models is used to warm-start the kinodynamic MPC, which accounts fornonlinear centroidal dynamics and kinematic constraints. The proposed approachenables energy efficient dynamic hopping on legged robots by using UPS toreduce peak motor torques and energy consumption during stance phases.Simulation results demonstrated a 38.8% reduction in the cost of transport(CoT) for a monoped robot equipped with UPS during high-speed hopping.Additionally, preliminary hardware experiments show a 14.8% reduction in energyconsumption. Video: https://youtu.be/AF11qMXJD48</description>
      <author>example@mail.com (Yulun Zhuang, Yichen Wang, Yanran Ding)</author>
      <guid isPermaLink="false">2503.05666v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Small-Scale Testbeds for Connected and Automated Vehicles and Robot Swarms: Challenges and a Roadmap</title>
      <link>http://arxiv.org/abs/2503.05656v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;目前在用于连接与自动化车辆(CAVs)及机器人集群的小规模测试平台中存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个路线图以解决这些小规模测试平台中的现有挑战。该路线图为“第1届小型化测试平台研讨会”工作坊的一部分，讨论如何改进CAVs和机器人集群的测试环境。&lt;h4&gt;方法&lt;/h4&gt;{'提高可访问性和多样性': '特别是为代表性不足的社区提供支持', '共享最佳实践': '促进测试平台的发展与维护', '连接抽象层': '通过一个抽象层来支持各测试平台间的合作'}&lt;h4&gt;主要内容&lt;/h4&gt;工作坊包括八位受邀演讲者、四篇论文以及一篇关于测试平台调查的研究报告。&lt;h4&gt;其他信息&lt;/h4&gt;{'在线比较表': '调查报告提供了一个包含超过25个测试床的在线对比表格', '链接': [{'调查报告网址': 'https://bassamlab.github.io/testbeds-survey'}, {'工作坊网站': 'https://cpm-remote.lrt.unibwmuenchen.de/iv24-workshop'}]}&lt;h4&gt;翻译&lt;/h4&gt;该文章提出了一个路线图，旨在解决用于连接与自动化车辆(CAVs)和机器人集群的小规模测试平台中的现有挑战。该路线图为6月2日在韩国济州岛举行的IEEE智能汽车研讨会(IV) 2024的“第1届小型化测试平台研讨会”工作坊的一部分，参与人员共同制定了这一计划。该计划包括三个部分：提高可访问性和多样性（特别是为代表性不足的社区提供支持），共享最佳实践以促进开发和维护测试平台，以及通过抽象层连接各测试平台来支持合作。此次研讨会共有八位受邀演讲者、四篇贡献论文，及一篇关于测试床调查的研究报告展示。研究报告在线提供了超过25个测试平台的对比表（网址：https://bassamlab.github.io/testbeds-survey）。该研讨会网站地址为：https://cpm-remote.lrt.unibwmuenchen.de/iv24-workshop。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This article proposes a roadmap to address the current challenges insmall-scale testbeds for Connected and Automated Vehicles (CAVs) and robotswarms. The roadmap is a joint effort of participants in the workshop "1stWorkshop on Small-Scale Testbeds for Connected and Automated Vehicles and RobotSwarms," held on June 2 at the IEEE Intelligent Vehicles Symposium (IV) 2024 inJeju, South Korea. The roadmap contains three parts: 1) enhancing accessibilityand diversity, especially for underrepresented communities, 2) sharing bestpractices for the development and maintenance of testbeds, and 3) connectingtestbeds through an abstraction layer to support collaboration. The workshopfeatures eight invited speakers, four contributed papers [1]-[4], and apresentation of a survey paper on testbeds [5]. The survey paper provides anonline comparative table of more than 25 testbeds, available athttps://bassamlab.github.io/testbeds-survey. The workshop's own website isavailable at https://cpm-remote.lrt.unibwmuenchen.de/iv24-workshop.</description>
      <author>example@mail.com (Jianye Xu, Bassam Alrifaee, Johannes Betz, Armin Mokhtarian, Archak Mittal, Mengchi Cai, Rahul Mangharam, Omar M. Shehata, Catherine M. Elias, Jan-Nico Zaech, Patrick Scheffe, Felix Jahncke, Sangeet Sankaramangalam Ulhas, Kaj Munhoz Arfvidsson)</author>
      <guid isPermaLink="false">2503.05656v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>BEHAVIOR Robot Suite: Streamlining Real-World Whole-Body Manipulation for Everyday Household Activities</title>
      <link>http://arxiv.org/abs/2503.05652v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://behavior-robot-suite.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的机器人框架BEHAVIOR Robot Suite (BRS)，用于解决移动操作机器人在家庭环境中执行任务时遇到的挑战。&lt;h4&gt;背景&lt;/h4&gt;真实世界中的家务任务对移动操作机器人提出了重大挑战，现有研究显示成功的任务执行依赖于三项关键的整体控制能力：双手协调、稳定且精确的导航以及广泛的末端效应器可达性。实现这些能力需要精心设计硬件，但这也增加了系统复杂度，使得视触觉运动策略学习变得更加困难。&lt;h4&gt;目的&lt;/h4&gt;通过引入BEHAVIOR Robot Suite (BRS)，该框架旨在解决移动操作机器人在家庭任务中遇到的挑战，并通过一种成本效益高的全身遥操作系统接口进行数据收集以及开发新的算法来学习全身视触觉运动策略。&lt;h4&gt;方法&lt;/h4&gt;BRS构建在一个双臂、轮式机器人上，该机器人配备了4自由度躯干。它结合了低成本的整体遥控界面用于数据采集及一个新颖的算法以学习整体视触觉运动策略。&lt;h4&gt;主要发现&lt;/h4&gt;BRS在五个具有挑战性的家庭任务中进行了评估，这些任务不仅强调三项核心能力而且还引入了额外复杂性，例如长距离导航、与活动和可变形物体互动以及在狭小空间内的操作。&lt;h4&gt;结论&lt;/h4&gt;BRS通过其集成的机器人实体，数据收集接口及学习框架，在实现现实世界的全身操纵方面迈出了重要一步，尤其是为了满足日常家庭任务的需求。该套件开源地址为https://behavior-robot-suite.github.io/&lt;h4&gt;翻译&lt;/h4&gt;真实世界中的家务任务对移动操作机器人提出了重大挑战。通过分析现有的机器人基准测试发现，成功的任务执行依赖于三项关键的整体控制能力：双手协调、稳定且精确的导航以及广泛的末端效应器可达性。实现这些能力需要精心设计硬件，但这也增加了系统复杂度，使得视触觉运动策略学习变得更加困难。为解决这些挑战，我们引入了BEHAVIOR Robot Suite (BRS)，这是一个全面的框架，旨在解决多样化的家庭任务中的全身操纵问题。该套件建立在双臂、轮式机器人上，并配备了4自由度躯干。它结合了一种低成本的整体遥控界面用于数据收集以及一个新颖的算法以学习整体视触觉运动策略。我们在五个具有挑战性的家务任务中评估了BRS，这些任务不仅强调三项核心能力而且还引入了额外复杂性，例如长距离导航、与活动和可变形物体互动以及在狭小空间内的操作。我们相信通过其集成的机器人实体、数据收集接口及学习框架，在实现现实世界的全身操纵方面，BRS迈向了一步重要的进展，尤其是为了满足日常家庭任务的需求。BRS开源地址为https://behavior-robot-suite.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Real-world household tasks present significant challenges for mobilemanipulation robots. An analysis of existing robotics benchmarks reveals thatsuccessful task performance hinges on three key whole-body controlcapabilities: bimanual coordination, stable and precise navigation, andextensive end-effector reachability. Achieving these capabilities requirescareful hardware design, but the resulting system complexity furthercomplicates visuomotor policy learning. To address these challenges, weintroduce the BEHAVIOR Robot Suite (BRS), a comprehensive framework forwhole-body manipulation in diverse household tasks. Built on a bimanual,wheeled robot with a 4-DoF torso, BRS integrates a cost-effective whole-bodyteleoperation interface for data collection and a novel algorithm for learningwhole-body visuomotor policies. We evaluate BRS on five challenging householdtasks that not only emphasize the three core capabilities but also introduceadditional complexities, such as long-range navigation, interaction witharticulated and deformable objects, and manipulation in confined spaces. Webelieve that BRS's integrated robotic embodiment, data collection interface,and learning framework mark a significant step toward enabling real-worldwhole-body manipulation for everyday household tasks. BRS is open-sourced athttps://behavior-robot-suite.github.io/</description>
      <author>example@mail.com (Yunfan Jiang, Ruohan Zhang, Josiah Wong, Chen Wang, Yanjie Ze, Hang Yin, Cem Gokmen, Shuran Song, Jiajun Wu, Li Fei-Fei)</author>
      <guid isPermaLink="false">2503.05652v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>dARt Vinci: Egocentric Data Collection for Surgical Robot Learning at Scale</title>
      <link>http://arxiv.org/abs/2503.05646v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种用于外科手术环境中的机器人学习的可扩展数据收集平台dARt Vinci，该平台利用增强现实手部追踪和高保真物理引擎捕捉基础手术任务中的细微操作。&lt;h4&gt;背景&lt;/h4&gt;在机器人学习领域中，特别是在像外科应用这样的安全关键领域，获取高质量的数据一直是个难题。这给研究人员带来了挑战，使他们难以充分利用最近强化学习和模仿学习的进步，这些进步大大提高了泛化能力，并使机器人能够自主执行任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种可扩展数据收集平台dARt Vinci，以解决外科手术环境中缺乏训练数据的问题，同时利用增强现实技术提高数据采集效率。&lt;h4&gt;方法&lt;/h4&gt;该系统使用了增强现实手部追踪技术和高保真物理引擎来捕捉基础手术任务中的细微操作。通过消除对实际机器人设置的需求，并提供时间、空间和硬件资源（如多视图传感器和执行器）的灵活性，特殊的模拟成为一种可行的选择。此外，由于其身体跟踪和内容叠加能力，增强现实使机器人数据收集更加自我中心。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究确认了所提议系统的效率和可用性，在使用广泛使用的基础任务训练达芬奇外科手术机器人的遥操作时。与真实机器人设置相比，所有任务的数据吞吐量平均提高了41%；总实验时间平均减少了10%，任务负载调查中的时间需求得到了改善。这些改进具有统计学意义。&lt;h4&gt;结论&lt;/h4&gt;dARt Vinci平台不仅实现了数据采集效率和使用的提升，还大幅度减少了所需存储空间，并使数据收集频率翻倍。&lt;h4&gt;翻译&lt;/h4&gt;在机器人学习领域中存在一个长期问题：数据稀缺性，在安全至关重要的外科应用等关键领域尤为突出。这使得研究人员难以利用近期关于强化学习和模仿学习的进展来提高泛化能力并让机器人能够自主执行任务。本文介绍了dARt Vinci，这是一种用于手术环境中的可扩展机器人学习的数据采集平台。该系统采用增强现实（AR）手部跟踪技术和高保真物理引擎来捕获基础外科任务中的细微动作：通过消除对实际机器人设备的需求，并在时间和空间以及硬件资源如多视图传感器和执行器方面提供灵活性，特定的模拟成为一种可行的选择。同时，增强现实在数据采集上为机器人提供了更多的自我中心视角，利用其身体跟踪和内容叠加的能力。我们的用户研究表明了所提出的系统的效率和可用性，我们使用广泛使用的基础任务训练达芬奇外科手术机器人的遥操作。与真实机器人设置相比，在所有任务中的数据吞吐量平均提高了41%；总实验时间减少了10%的平均水平，任务负载调查中对时间的需求得到了改善。这些改进具有统计学意义。此外，收集到的数据大小仅为原来的四百分之一，大大减少了存储需求，同时使数据采集频率翻倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Data scarcity has long been an issue in the robot learning community.Particularly, in safety-critical domains like surgical applications, obtaininghigh-quality data can be especially difficult. It poses challenges toresearchers seeking to exploit recent advancements in reinforcement learningand imitation learning, which have greatly improved generalizability andenabled robots to conduct tasks autonomously. We introduce dARt Vinci, ascalable data collection platform for robot learning in surgical settings. Thesystem uses Augmented Reality (AR) hand tracking and a high-fidelity physicsengine to capture subtle maneuvers in primitive surgical tasks: By eliminatingthe need for a physical robot setup and providing flexibility in terms of time,space, and hardware resources-such as multiview sensors andactuators-specialized simulation is a viable alternative. At the same time, ARallows the robot data collection to be more egocentric, supported by its bodytracking and content overlaying capabilities. Our user study confirms theproposed system's efficiency and usability, where we use widely-used primitivetasks for training teleoperation with da Vinci surgical robots. Data throughputimproves across all tasks compared to real robot settings by 41% on average.The total experiment time is reduced by an average of 10%. The temporal demandin the task load survey is improved. These gains are statistically significant.Additionally, the collected data is over 400 times smaller in size, requiringfar less storage while achieving double the frequency.</description>
      <author>example@mail.com (Yihao Liu, Yu-Chun Ku, Jiaming Zhang, Hao Ding, Peter Kazanzides, Mehran Armand)</author>
      <guid isPermaLink="false">2503.05646v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Limits of specifiability for sensor-based robotic planning tasks</title>
      <link>http://arxiv.org/abs/2503.05623v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了复杂机器人任务的可指定性，特别是那些涉及丰富目标和长时间行为的任务。&lt;h4&gt;背景&lt;/h4&gt;已有大量基于形式化方法的技术用于描述和实现复杂的机器人任务。&lt;h4&gt;目的&lt;/h4&gt;研究任务可指定性的边界，并强调具体规范在不同层面上（如状态、动作观察等）对任务可指定性的影响。&lt;h4&gt;方法&lt;/h4&gt;引入新的符号表示处理大类问题，分析不同层面的具体规范如何影响可以提出哪些类型的任务。&lt;h4&gt;主要发现&lt;/h4&gt;某些类型的任务可在不同的具体化组合下被明确指定。&lt;h4&gt;结论&lt;/h4&gt;强调了在特定层面上定义具体规范对机器人任务可指定性的重要作用。&lt;h4&gt;翻译&lt;/h4&gt;目前有许多基于形式化方法的技术用于描述和实现复杂的机器人任务，包括那些涉及各种丰富目标和长时间行为的任务。本文探讨了哪些类型的任务是可明确规定的，并研究了具体规范的准确定义在不同层面上（如机器人的状态、动作和观察结果）对于任务指定是否可行的影响。此前的工作中已有部分对此进行了描述，而我们的贡献则是将这一方面作为核心内容处理：我们引入了一种符号表示来应对大量问题，并探讨了这种具体化如何影响可以提出哪些类型的任务。研究结果表明，在不同的具体化组合下，某些类别的任务是可以被指定的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There is now a large body of techniques, many based on formal methods, fordescribing and realizing complex robotics tasks, including those involving avariety of rich goals and time-extended behavior. This paper explores thelimits of what sorts of tasks are specifiable, examining how the precisegrounding of specifications, that is, whether the specification is given interms of the robot's states, its actions and observations, its knowledge, orsome other information,is crucial to whether a given task can be specified.While prior work included some description of particular choices for thisgrounding, our contribution treats this aspect as a first-class citizen: weintroduce notation to deal with a large class of problems, and examine how thegrounding affects what tasks can be posed. The results demonstrate that certainclasses of tasks are specifiable under different combinations of groundings.</description>
      <author>example@mail.com (Basak Sakcak, Dylan A. Shell, Jason M. O'Kane)</author>
      <guid isPermaLink="false">2503.05623v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Learning and generalization of robotic dual-arm manipulation of boxes from demonstrations via Gaussian Mixture Models (GMMs)</title>
      <link>http://arxiv.org/abs/2503.05619v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于人类演示的学习和泛化方法，旨在改进复杂机器人系统（如双臂机器人）的操作能力。通过使用GMM参数化的策略，并在参数空间中进行直接的泛化过程，该方法能够有效地减少训练成本与时间。&lt;h4&gt;背景&lt;/h4&gt;示教学习(LfD)是让机器人模仿人类操作物体的有效方式之一。然而，在面对具有高负载能力和灵活性的复杂系统（如双臂机器人）时，如何使机器人的动作适应未见的变化仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需昂贵的人机交互就能扩展机器人动作至新情境的方法，并减少所需演示次数以降低训练成本和时间。&lt;h4&gt;方法&lt;/h4&gt;利用GMM参数化策略的特性，在参数空间中进行泛化过程，从而实现低成本、低计算量且高效的算法。该法仅需少数几次人类示教即可开始运行。&lt;h4&gt;主要发现&lt;/h4&gt;通过实际实验验证了所提出的方法的有效性：从仅为单一任务设计的五次演示学习开始，机器人能够成功地将动作扩展到新场景中（包括不同的目标位置、方向及尺寸）&lt;h4&gt;结论&lt;/h4&gt;该方法展示了其在复杂操作中的实用性和可伸缩性，并且减少了传统LfD所需的大量数据收集和昂贵的人机交互环节。&lt;h4&gt;翻译&lt;/h4&gt;学习示教(LfD)是一种有效的让机器人模仿人类移动并操控物体的方法。这种方法对于处理负载能力和灵活性高的复杂系统（例如双臂机器人）尤为有效。然而，如何将所学的动作扩展到未见过的场景中是一个关键挑战。该研究提出了一种基于GMM参数化的策略泛化方法，并通过实验展示了其在实际应用中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning from demonstration (LfD) is an effective method to teach robots tomove and manipulate objects in a human-like manner. This is especially truewhen dealing with complex robotic systems, such as those with dual armsemployed for their improved payload capacity and manipulability. However, a keychallenge is in expanding the robotic movements beyond the learned scenarios toadapt to minor and major variations from the specific demonstrations. In thiswork, we propose a learning and novel generalization approach that adapts thelearned Gaussian Mixture Model (GMM)-parameterized policy derived from humandemonstrations. Our method requires only a small number of human demonstrationsand eliminates the need for a robotic system during the demonstration phase,which can significantly reduce both cost and time. The generalization processtakes place directly in the parameter space, leveraging the lower-dimensionalrepresentation of GMM parameters. With only three parameters per Gaussiancomponent, this process is computationally efficient and yields immediateresults upon request. We validate our approach through real-world experimentsinvolving a dual-arm robotic manipulation of boxes. Starting with just fivedemonstrations for a single task, our approach successfully generalizes to newunseen scenarios, including new target locations, orientations, and box sizes.These results highlight the practical applicability and scalability of ourmethod for complex manipulations.</description>
      <author>example@mail.com (Qian Ying Lee, Suhas Raghavendra Kulkarni, Kenzhi Iskandar Wong, Lin Yang, Bernardo Noronha, Yongjun Wee, Tzu-Yi Hung, Domenico Campolo)</author>
      <guid isPermaLink="false">2503.05619v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Novel Object 6D Pose Estimation with a Single Reference View</title>
      <link>http://arxiv.org/abs/2503.05578v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 12 figures (including supplementary material)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种基于单张参考图像进行6D姿态估计的方法，通过迭代地在相机坐标系统中对齐点位置来处理大姿态差异，并使用RGB和点的SSM捕捉长距离依赖关系和空间信息。&lt;h4&gt;背景&lt;/h4&gt;现有的新物体6D姿态估计方法通常依赖于CAD模型或密集参考视图，但获取这些资源比较困难。仅使用单张参考图像的方法更具可扩展性，但也面临着挑战，如处理大姿态差异以及从单一视角中提取有限的几何和空间信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于单张参考图像的新物体6D姿态估计方法（SinRef-6D），以克服现有技术的局限性，并能够仅使用一张参考图像在不依赖CAD模型的情况下估算出新物体的姿态。&lt;h4&gt;方法&lt;/h4&gt;该方法的核心思想是在相机坐标系统中迭代地进行逐点对齐，利用状态空间模型（SSM）来捕捉长距离依赖关系和从单张视图中的空间信息。具体而言，通过迭代的相机空间逐点对齐可以有效处理大姿态差异，而RGB与点的SSM可以提供线性复杂度以及更优秀的空间建模能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明SinRef-6D在六个流行数据集和现实世界的机器人场景上达到了与基于CAD模型或密集参考视图方法相当的表现水平，在更具挑战性的单张参考图像设置下表现优异。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新颖的基于单张参考图像的新物体6D姿态估计方法，能够直接从一张单一视角获取的信息中精确估算新物体的姿态，同时无需依赖于CAD模型或者大规模训练数据。这种方法为未来的研究提供了新的方向和工具。&lt;h4&gt;翻译&lt;/h4&gt;现有的新技术通常需要CAD模型或密集参考视图来实现新型对象的6D姿态估计，而这两种资源都难以获得。使用单张参考图像的方法更具可扩展性，但面临着处理大姿态差异以及从单一视角中提取有限几何信息的挑战。为解决这些问题，本文提出了一种基于单张参考图像的新物体6D（SinRef-6D）姿态估计方法。该方法的核心在于利用状态空间模型在相机坐标系统下迭代地进行逐点对齐以应对大姿态差异，并通过RGB和点的状态空间模型捕捉长距离依赖关系和单一视角中的空间信息，提供线性复杂度的同时具有优秀的空间建模能力。经过合成数据的预训练后，SinRef-6D能够在不重新训练或使用CAD模型的情况下，仅凭一张参考图像就能估算出新物体的6D姿态。大量实验表明，在六个流行的数据集和现实世界的机器人场景中，该方法的表现与基于CAD模型或密集参考视图的方法相当，即便是在更具挑战性的单一参考设置下也是如此。相关代码将发布在https://github.com/CNJianLiu/SinRef-6D上供研究者使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing novel object 6D pose estimation methods typically rely on CAD modelsor dense reference views, which are both difficult to acquire. Using only asingle reference view is more scalable, but challenging due to large posediscrepancies and limited geometric and spatial information. To address theseissues, we propose a Single-Reference-based novel object 6D (SinRef-6D) poseestimation method. Our key idea is to iteratively establish point-wisealignment in the camera coordinate system based on state space models (SSMs).Specifically, iterative camera-space point-wise alignment can effectivelyhandle large pose discrepancies, while our proposed RGB and Points SSMs cancapture long-range dependencies and spatial information from a single view,offering linear complexity and superior spatial modeling capability. Oncepre-trained on synthetic data, SinRef-6D can estimate the 6D pose of a novelobject using only a single reference view, without requiring retraining or aCAD model. Extensive experiments on six popular datasets and real-world roboticscenes demonstrate that we achieve on-par performance with CAD-based and densereference view-based methods, despite operating in the more challenging singlereference setting. Code will be released athttps://github.com/CNJianLiu/SinRef-6D.</description>
      <author>example@mail.com (Jian Liu, Wei Sun, Kai Zeng, Jin Zheng, Hui Yang, Lin Wang, Hossein Rahmani, Ajmal Mian)</author>
      <guid isPermaLink="false">2503.05578v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>InDRiVE: Intrinsic Disagreement based Reinforcement for Vehicle Exploration through Curiosity Driven Generalized World Model</title>
      <link>http://arxiv.org/abs/2503.05573v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to IROS 2025 and is currently under  review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于模型的强化学习（MBRL）在自动驾驶中展现出巨大潜力，特别是在数据效率和鲁棒性方面。然而，现有的解决方案依赖于精心设计的任务特定外部奖励，限制了对新任务或环境的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;当前大多数自动驾驶中的MBRL方法需要详细定义的任务特定奖励来驱动学习过程，这在面对未知或变化的驾驶情况时效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种称为InDRiVE的方法，在不依赖任何外部任务反馈的情况下，通过利用基于模型世界的一致性奖励进行探索。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一个集成的世界模型训练框架，使得代理可以主动在环境中的不确定性区域进行探索。这种方法产生了任务无关的潜在表示，能够快速地对下游驾驶任务（如车道跟随和碰撞避免）进行零样本或少量样本微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在已见及未见过环境中，InDRiVE的表现优于DreamerV2和DreamerV3基准模型，并且使用显著较少的训练步骤取得了更高的成功率和更少的违规行为。&lt;h4&gt;结论&lt;/h4&gt;纯粹基于内在奖励机制探索的有效性已经被证明可以促进学习稳健车辆控制行为，为开发更加可扩展和适应性强的自动驾驶系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的关键信息已经以中文的形式进行了总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model-based Reinforcement Learning (MBRL) has emerged as a promising paradigmfor autonomous driving, where data efficiency and robustness are critical. Yet,existing solutions often rely on carefully crafted, task specific extrinsicrewards, limiting generalization to new tasks or environments. In this paper,we propose InDRiVE (Intrinsic Disagreement based Reinforcement for VehicleExploration), a method that leverages purely intrinsic, disagreement basedrewards within a Dreamer based MBRL framework. By training an ensemble of worldmodels, the agent actively explores high uncertainty regions of environmentswithout any task specific feedback. This approach yields a task agnostic latentrepresentation, allowing for rapid zero shot or few shot fine tuning ondownstream driving tasks such as lane following and collision avoidance.Experimental results in both seen and unseen environments demonstrate thatInDRiVE achieves higher success rates and fewer infractions compared toDreamerV2 and DreamerV3 baselines despite using significantly fewer trainingsteps. Our findings highlight the effectiveness of purely intrinsic explorationfor learning robust vehicle control behaviors, paving the way for more scalableand adaptable autonomous driving systems.</description>
      <author>example@mail.com (Feeza Khan Khanzada, Jaerock Kwon)</author>
      <guid isPermaLink="false">2503.05573v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A-SEE2.0: Active-Sensing End-Effector for Robotic Ultrasound Systems with Dense Contact Surface Perception Enabled Probe Orientation Adjustment</title>
      <link>http://arxiv.org/abs/2503.05569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, submitted for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要类型&lt;/h4&gt;研究论文&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的机器人超声系统（RUSS），该系统利用双RGB-D深度相机来保持超声探头与皮肤表面垂直，从而提高了成像质量和操作一致性。&lt;h4&gt;背景&lt;/h4&gt;传统的自由手超声成像高度依赖于操作者的技能，导致结果不一致且增加了超声技师的体力负担。机器人超声系统（RUSS）旨在通过提供标准化和自动化的成像解决方案来克服这些限制，在缺乏熟练操作者的情况下尤为有用。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的RUSS系统，该系统利用双RGB-D深度相机在没有预先采集的数据情况下维持探头与皮肤表面的垂直对齐，以提高超声成像的一致性和质量。&lt;h4&gt;方法&lt;/h4&gt;RUSS系统将RGB-D摄像头数据和机器人控制算法相结合，确保在不规则表面上保持探头的正交对准。通过使用仿生模型进行验证测试，并且还在活体前臂超声检查中评估了其性能。&lt;h4&gt;主要发现&lt;/h4&gt;该系统的正常定位精度表现出色，在平面表面定位误差为2.47±1.25度，而在人体模型表面上的估计垂直误差则为12.19±5.81度。这些结果表明RUSS能够生成与手动扫描相当的超声图像。&lt;h4&gt;结论&lt;/h4&gt;A-SEE2.0系统在临床实践中具有潜在的应用价值，特别是在那些难以获取熟练操作员的情况下，可实现高质量和一致性的超声成像。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已经提供为中文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional freehand ultrasound (US) imaging is highly dependent on theskill of the operator, often leading to inconsistent results and increasedphysical demand on sonographers. Robotic Ultrasound Systems (RUSS) aim toaddress these limitations by providing standardized and automated imagingsolutions, especially in environments with limited access to skilled operators.This paper presents the development of a novel RUSS system that employs dualRGB-D depth cameras to maintain the US probe normal to the skin surface, acritical factor for optimal image quality. Our RUSS integrates RGB-D cameradata with robotic control algorithms to maintain orthogonal probe alignment onuneven surfaces without preoperative data. Validation tests using a phantommodel demonstrate that the system achieves robust normal positioning accuracywhile delivering ultrasound images comparable to those obtained through manualscanning. A-SEE2.0 demonstrates 2.47 ${\pm}$ 1.25 degrees error for flatsurface normal-positioning and 12.19 ${\pm}$ 5.81 degrees normal estimationerror on mannequin surface. This work highlights the potential of A-SEE2.0 tobe used in clinical practice by testing its performance during in-vivo forearmultrasound examinations.</description>
      <author>example@mail.com (Yernar Zhetpissov, Xihan Ma, Kehan Yang, Haichong K. Zhang)</author>
      <guid isPermaLink="false">2503.05569v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Riemann$^2$: Learning Riemannian Submanifolds from Riemannian Data</title>
      <link>http://arxiv.org/abs/2503.05540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种学习黎曼流形上几何数据的潜在表示的方法，这种模型能够更好地处理具有内在约束的数据。&lt;h4&gt;背景&lt;/h4&gt;传统的潜在变量模型在处理单位范数向量或对称正定矩阵等具有一些特定几何结构的数据时表现不佳，因为它们忽视了数据的底层几何限制或者无法提供有意义的距离度量方法。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，研究者提出了一种新的学习方式，通过估计由缠绕高斯过程潜在变量模型（Wrapped Gaussian Process Latent Variable Model）诱导的拉回度量来更好地处理这种类型的几何数据。&lt;h4&gt;方法&lt;/h4&gt;该方法基于估计由Wrapped Gaussian Process Latent Variable Model引入的拉回度量，从而可以更准确地定义潜在空间中的距离和最短路径，并确保模型只在数据流形上分配概率质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法，研究者能够处理机器人运动合成和大脑连接体分析等复杂任务。&lt;h4&gt;结论&lt;/h4&gt;这项工作为解决具有内在几何约束的数据提供了一个新的视角，提高了潜在变量模型的适用性和效果。&lt;h4&gt;翻译&lt;/h4&gt;潜变量模型是学习高维数据中的低维流形的强大工具。然而，在处理如单位范数向量或对称正定矩阵等受限数据时，现有的方法忽略了底层的几何限制，或者无法在潜在空间中提供有意义的距离度量。为了解决这些问题，我们提出了学习这些几何数据的黎曼潜表示的方法。为此，我们估计了由缠绕高斯过程潜在变量模型（Wrapped Gaussian Process Latent Variable Model）引入的拉回度量，该方法明确地考虑到了数据的几何结构。这使我们在潜在空间中定义几何感知距离和最短路径的同时，确保我们的模型仅在数据流形上分配概率质量。这项工作推广了先前的工作，并允许我们处理各种领域中的复杂任务，包括机器人运动合成和大脑连接体分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Latent variable models are powerful tools for learning low-dimensionalmanifolds from high-dimensional data. However, when dealing with constraineddata such as unit-norm vectors or symmetric positive-definite matrices,existing approaches ignore the underlying geometric constraints or fail toprovide meaningful metrics in the latent space. To address these limitations,we propose to learn Riemannian latent representations of such geometric data.To do so, we estimate the pullback metric induced by a Wrapped Gaussian ProcessLatent Variable Model, which explicitly accounts for the data geometry. Thisenables us to define geometry-aware notions of distance and shortest paths inthe latent space, while ensuring that our model only assigns probability massto the data manifold. This generalizes previous work and allows us to handlecomplex tasks in various domains, including robot motion synthesis and analysisof brain connectomes.</description>
      <author>example@mail.com (Leonel Rozo, Miguel González-Duque, Noémie Jaquier, Søren Hauberg)</author>
      <guid isPermaLink="false">2503.05540v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating db-A$^\textbf{*}$ for Kinodynamic Motion Planning Using Diffusion</title>
      <link>http://arxiv.org/abs/2503.05539v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的方法，用于利用扩散模型生成适合于每个问题实例的动力学运动原语，通过这种方法可以更快地找到更高质量的解决方案。&lt;h4&gt;背景&lt;/h4&gt;现有的动力学运动规划方法在适应不同机器人动态特性时存在局限性，如二阶单轮车或带有拖车的小汽车。为了解决这些问题，需要一种能够快速生成高质量运动原语的新方法。&lt;h4&gt;目的&lt;/h4&gt;利用扩散模型提出了一种新的方法来为动力学运动规划生成适配于特定问题实例的运动原语，并提高计算时间和解决方案质量。&lt;h4&gt;方法&lt;/h4&gt;采用在随机切割的解决轨迹上训练得到的扩散模型。这些轨迹是通过使用动力学运动规划器解决随机生成的问题实例创建出来的。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在不同的机器人动态特性下，包括二阶单轮车或带有拖车的小汽车等情况下，计算时间和解决方案质量都有显著提高，最高可达30%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过利用扩散模型和问题特定参数提高了动力学运动规划的效率和质量。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的方法，使用扩散模型为动力学运动规划生成运动原语。所提出的方案根据每个问题实例使用问题特有参数进行调整，从而更快地找到更高质量的解决方案。用于该方法中的扩散模型是在随机切割的解决轨迹上训练得到的，这些轨迹是由动力学运动规划器解决随机生成的问题实例创建出来的。实验结果显示，在处理二阶单轮车或带有拖车的小汽车等不同机器人动态特性的情况下，计算时间和解决方案质量都显著提高了30%以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel approach for generating motion primitives for kinodynamicmotion planning using diffusion models. The motions generated by our approachare adapted to each problem instance by utilizing problem-specific parameters,allowing for finding solutions faster and of better quality. The diffusionmodels used in our approach are trained on randomly cut solution trajectories.These trajectories are created by solving randomly generated problem instanceswith a kinodynamic motion planner. Experimental results show significantimprovements up to 30 percent in both computation time and solution qualityacross varying robot dynamics such as second-order unicycle or car withtrailer.</description>
      <author>example@mail.com (Julius Franke, Akmaral Moldagalieva, Pia Hanfeld, Wolfgang Hönig)</author>
      <guid isPermaLink="false">2503.05539v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Design, Dynamic Modeling and Control of a 2-DOF Robotic Wrist Actuated by Twisted and Coiled Actuators</title>
      <link>http://arxiv.org/abs/2503.05508v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的两自由度（2-DOF）机器人腕部设计，该设计使用基于扭曲和卷绕致动器（TCA）的并联机械结构。这种手腕设计旨在通过减轻摩擦问题来提供轻量级结构和优异的运动性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，在工业操纵器和人形机器人的研究中，人工肌肉驱动的执行器越来越受到关注，因为它们提供了高能量密度、轻质构造和紧凑的设计特点。然而，基于动态模型的控制器在机器人手腕的人工肌肉驱动领域的应用往往被忽视。&lt;h4&gt;目的&lt;/h4&gt;本文旨在开发一种新的基于TCA并联机构的2-DOF机器人腕部设计，并建立其拉格朗日动力学模型。&lt;h4&gt;方法&lt;/h4&gt;论文建立了手腕的动力学模型，提出了一种非线性模型预测控制器（NMPC）用于轨迹跟踪任务。此外，还开发了一个原型并通过实验验证了提出的动态模型的有效性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种工作条件下，基于动态模型的控制策略在TCA驱动的机器人手腕运动控制中具有较好的有效性和稳定性。&lt;h4&gt;结论&lt;/h4&gt;新型2-DOF机器人腕部设计能够提供轻量级结构和优异的运动性能，并且所提出的非线性模型预测控制器可以实现更准确和稳定的轨迹跟踪。&lt;h4&gt;翻译&lt;/h4&gt;机器人的腕部在工业操纵器和人形机器人中发挥着重要作用，特别是在抓握任务中。近年来，基于人工肌肉驱动的执行器因其高能量密度、轻质构造和紧凑设计的特点而备受关注。然而，在人工肌肉驱动的机器人腕部的研究中，通常忽视了基于动态模型的控制器的重要性。这项研究提出了一种使用并联机制（3RRRR配置）的新式2-DOF机器人腕部设计方案，并且该方案预计能够提供轻量化结构和优异运动性能同时减少摩擦问题。论文还建立了一个拉格朗日动力学模型，设计了非线性模型预测控制器用于轨迹跟踪任务。一个原型被开发出来并通过广泛的实验验证其出色的运动性能和提出的动态模型的有效性。后续的对比实验表明，在各种操作条件下，基于动态模型的控制策略在TCA驱动的机器人腕部运动控制中显示出更好的有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic wrists play a pivotal role in the functionality of industrialmanipulators and humanoid robots, facilitating manipulation and grasping tasks.In recent years, there has been a growing interest in integrating artificialmuscle-driven actuators for robotic wrists, driven by advancements intechnology offering high energy density, lightweight construction, and compactdesigns. However, in the study of robotic wrists driven by artificial muscles,dynamic model-based controllers are often overlooked, despite their criticalimportance for motion analysis and dynamic control of robots. This paperpresents a novel design of a two-degree-of-freedom (2-DOF) robotic wrist drivenby twisted and coiled actuators (TCA) utilizing a parallel mechanism with a3RRRR configuration. The proposed robotic wrist is expected to featurelightweight structures and superior motion performance while mitigatingfriction issues. The Lagrangian dynamic model of the wrist is established,along with a nonlinear model predictive controller (NMPC) designed fortrajectory tracking tasks. A prototype of the robotic wrist is developed, andextensive experiments are conducted to validate its superior motion performanceand the proposed dynamic model. Subsequently, extensive comparative experimentsbetween NMPC and PID controller were conducted under various operatingconditions. The experimental results demonstrate the effectiveness androbustness of the dynamic model-based controller in the motion control ofTCA-driven robotic wrists.</description>
      <author>example@mail.com (Yunsong Zhang, Xinyu Zhou, Feitian Zhang)</author>
      <guid isPermaLink="false">2503.05508v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Neural Unscented Kalman Filter</title>
      <link>http://arxiv.org/abs/2503.05490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  eight pages, ten figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自适应神经无迹卡尔曼滤波器，用于处理平台操作过程中的时间变化不确定性。&lt;h4&gt;背景&lt;/h4&gt;非线性场景下无迹卡尔曼滤波算法可能因过程噪声协方差的不确定而导致性能下降甚至发散。&lt;h4&gt;目的&lt;/h4&gt;调整过程噪声协方差矩阵以提高滤波估计性能，并适应时间变化中的不确定性。&lt;h4&gt;方法&lt;/h4&gt;设计了ProcessNet，一个简单高效的端到端回归网络，用来自适应地估算过程噪声协方差矩阵。&lt;h4&gt;主要发现&lt;/h4&gt;在自主水下航行器导航中处理非线性惯性传感器和多普勒速度日志融合问题时，展示了滤波性能的优势。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明该自适应神经无迹卡尔曼滤波器优于其他自适应和非自适应的非线性滤波器。&lt;h4&gt;翻译&lt;/h4&gt;无迹卡尔曼滤波是一种能够处理非线性场景的算法。过程噪声协方差中的不确定性可能会降低滤波估计性能，甚至导致其发散。因此，在实时调整过程噪声协方差矩阵是重要的。本文开发了一种自适应神经无迹卡尔曼滤波器以应对平台操作期间的时间变化不确定性。为此设计了ProcessNet网络用于在端到端回归中自适应地估算过程噪声协方差矩阵，特别关注自主水下车辆导航中的非线性惯性传感器和多普勒速度日志融合问题。利用实际记录的自主水下航行器数据集，展示了滤波性能，并显示出相对于其他自适应与不自适应的非线性滤波方法的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The unscented Kalman filter is an algorithm capable of handling nonlinearscenarios. Uncertainty in process noise covariance may decrease the filterestimation performance or even lead to its divergence. Therefore, it isimportant to adjust the process noise covariance matrix in real time. In thispaper, we developed an adaptive neural unscented Kalman filter to cope withtime-varying uncertainties during platform operation. To this end, we devisedProcessNet, a simple yet efficient end-to-end regression network to adaptivelyestimate the process noise covariance matrix. We focused on the nonlinearinertial sensor and Doppler velocity log fusion problem in the case ofautonomous underwater vehicle navigation. Using a real-world recorded datasetfrom an autonomous underwater vehicle, we demonstrated our filter performanceand showed its advantages over other adaptive and non-adaptive nonlinearfilters.</description>
      <author>example@mail.com (Amit Levy, Itzik Klein)</author>
      <guid isPermaLink="false">2503.05490v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>DecoupledGaussian: Object-Scene Decoupling for Physics-Based Interaction</title>
      <link>http://arxiv.org/abs/2503.05484v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR2025 Accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DecoupledGaussian系统提出了一种新颖的方法，用于从野外视频中分离静态对象及其接触面。该方法通过使用联合泊松场修复和扩展高斯分布来支持真实的新牛顿物理仿真。&lt;h4&gt;背景&lt;/h4&gt;当前的技术对于合成数据的处理效果良好，但对真实的、复杂的场景中的物体与表面接触情况处理不足。&lt;h4&gt;目的&lt;/h4&gt;提供一种有效的方法将静态对象与其接触面分离，并实现基于新牛顿力学的真实物理仿真的模拟。&lt;h4&gt;方法&lt;/h4&gt;DecoupledGaussian使用联合泊松场修复和扩展高斯分布以适应位置变化，同时引入多雕刻策略来优化物体的几何形状。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够使用户根据给定的脉冲进行解耦、碰撞以及断裂的真实物理仿真。通过全面的用户研究和定量基准测试验证了DecoupledGaussian的有效性。&lt;h4&gt;结论&lt;/h4&gt;此系统改进了在现实世界环境中与对象和场景之间的数字交互，对VR、机器人技术及自动驾驶等产业具有重要应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种称为DecoupledGaussian的新颖系统，它从野外视频中分离静态物体与其接触面，这是基于新牛顿力学的真实物理仿真的关键前提。与以往专注于合成数据或沿接触表面弹性抖动的方法不同（这使得对象无法完全脱离或独立移动），DecoupledGaussian允许显著的位置变化而不受初始接触面的限制。认识到当前二维修复工具在恢复三维位置方面的局限性，我们的方法提出了一种联合泊松场来修补和扩展物体及其接触场景分离后的高斯分布。通过多雕刻策略进一步完善对象几何形状以提高仿真效果。该系统支持复杂交互的现实物理模拟驱动用户指定脉冲的支持跨多个场景。我们通过全面的用户研究和定量基准测试验证了DecoupledGaussian的有效性，改进了在现实世界环境中与对象和场景之间的数字互动，对VR、机器人技术及自动驾驶等产业具有重要应用价值。项目页面位于：https://wangmiaowei.github.io/DecoupledGaussian.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present DecoupledGaussian, a novel system that decouples static objectsfrom their contacted surfaces captured in-the-wild videos, a key prerequisitefor realistic Newtonian-based physical simulations. Unlike prior methodsfocused on synthetic data or elastic jittering along the contact surface, whichprevent objects from fully detaching or moving independently, DecoupledGaussianallows for significant positional changes without being constrained by theinitial contacted surface. Recognizing the limitations of current 2D inpaintingtools for restoring 3D locations, our approach proposes joint Poisson fields torepair and expand the Gaussians of both objects and contacted scenes afterseparation. This is complemented by a multi-carve strategy to refine theobject's geometry. Our system enables realistic simulations of decouplingmotions, collisions, and fractures driven by user-specified impulses,supporting complex interactions within and across multiple scenes. We validateDecoupledGaussian through a comprehensive user study and quantitativebenchmarks. This system enhances digital interaction with objects and scenes inreal-world environments, benefiting industries such as VR, robotics, andautonomous driving. Our project page is at:https://wangmiaowei.github.io/DecoupledGaussian.github.io/.</description>
      <author>example@mail.com (Miaowei Wang, Yibo Zhang, Rui Ma, Weiwei Xu, Changqing Zou, Daniel Morris)</author>
      <guid isPermaLink="false">2503.05484v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Topology-Driven Trajectory Optimization for Modelling Controllable Interactions Within Multi-Vehicle Scenario</title>
      <link>http://arxiv.org/abs/2503.05471v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于拓扑规划的可微局部同伦不变量度量，用于建模多车辆轨迹优化中的相互作用。&lt;h4&gt;背景&lt;/h4&gt;在多车辆场景中进行轨迹优化面临非线性和非凸性的挑战，并且对初始值敏感，导致难以控制车辆之间的互动。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有技术无法很好地生成可控交互和用户设计的交互模式的问题。&lt;h4&gt;方法&lt;/h4&gt;受到拓扑规划启发，引入了一种可微局部同伦不变量度量作为约束条件纳入多车辆轨迹优化框架中。&lt;h4&gt;主要发现&lt;/h4&gt;通过这种方法能够从同一初始值生成多个互动轨迹，并显示出比现有技术更高的优化效果和效率。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个有效的解决策略来处理复杂环境下的多车辆交互问题，且将开源代码用于推动相关研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;在多车辆场景中进行的轨迹优化因非线性、非凸性质以及初始值敏感特性而面临挑战，使得控制车辆互动变得困难。本文受拓扑规划启发提出了一种可微局部同伦不变量度量，用以建模相互作用。通过将这种拓扑度量作为约束引入到多车辆轨迹优化中，该框架能够从同一初始值生成多个互动轨迹，实现可控互动并支持用户设计的互动模式。大量实验表明其在优化效果和效率上超过现有方法。本文团队计划开放源代码以推动相关研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory optimization in multi-vehicle scenarios faces challenges due toits non-linear, non-convex properties and sensitivity to initial values, makinginteractions between vehicles difficult to control. In this paper, inspired bytopological planning, we propose a differentiable local homotopy invariantmetric to model the interactions. By incorporating this topological metric as aconstraint into multi-vehicle trajectory optimization, our framework is capableof generating multiple interactive trajectories from the same initial values,achieving controllable interactions as well as supporting user-designedinteraction patterns. Extensive experiments demonstrate its superior optimalityand efficiency over existing methods. We will release open-source code toadvance relative research.</description>
      <author>example@mail.com (Changjia Ma, Yi Zhao, Zhongxue Gan, Bingzhao Gao, Wenchao Ding)</author>
      <guid isPermaLink="false">2503.05471v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Self-Modeling Robots by Photographing</title>
      <link>http://arxiv.org/abs/2503.05398v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个高精度、考虑纹理和链级的机器人自建模方法。&lt;h4&gt;背景&lt;/h4&gt;现有自我建模方法在模型质量或数据采集成本方面存在问题，且未涉及机器人的纹理建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的技术来提高机器人自建模的质量，并降低数据采集的成本。同时探索如何将纹理纳入建模中。&lt;h4&gt;方法&lt;/h4&gt;利用三维高斯分布表示静态形态和纹理，并通过聚类构建神经椭球骨，其变形由姿态参数决定；利用一个运动学神经网络生成变换矩阵控制这些骨骼的动态变化；使用关节角度、相机参数和多视角图像作为训练数据，训练3D高斯分布与运动学神经网络。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够以链级精度描述机器人的形态、运动学及纹理，并可用于下游任务如动作规划和逆向运动学问题的解决。&lt;h4&gt;结论&lt;/h4&gt;这项工作为机器人自我建模提供了一种新的有效途径，通过结合三维高斯分布与深度学习模型实现了高质量的自建模。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-modeling enables robots to build task-agnostic models of theirmorphology and kinematics based on data that can be automatically collected,with minimal human intervention and prior information, thereby enhancingmachine intelligence. Recent research has highlighted the potential ofdata-driven technology in modeling the morphology and kinematics of robots.However, existing self-modeling methods suffer from either low modeling qualityor excessive data acquisition costs. Beyond morphology and kinematics, textureis also a crucial component of robots, which is challenging to model andremains unexplored. In this work, a high-quality, texture-aware, and link-levelmethod is proposed for robot self-modeling. We utilize three-dimensional (3D)Gaussians to represent the static morphology and texture of robots, and clusterthe 3D Gaussians to construct neural ellipsoid bones, whose deformations arecontrolled by the transformation matrices generated by a kinematic neuralnetwork. The 3D Gaussians and kinematic neural network are trained using datapairs composed of joint angles, camera parameters and multi-view images withoutdepth information. By feeding the kinematic neural network with joint angles,we can utilize the well-trained model to describe the corresponding morphology,kinematics and texture of robots at the link level, and render robot imagesfrom different perspectives with the aid of 3D Gaussian splatting. Furthermore,we demonstrate that the established model can be exploited to performdownstream tasks such as motion planning and inverse kinematics.</description>
      <author>example@mail.com (Kejun Hu, Peng Yu, Ning Tan)</author>
      <guid isPermaLink="false">2503.05398v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>CoinRobot: Generalized End-to-end Robotic Learning for Physical Intelligence</title>
      <link>http://arxiv.org/abs/2503.05316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;物理智能在推动具身智能的发展中展现出巨大潜力，但现有系统面临诸多挑战。本文提出了一种通用的端到端机器人学习框架来解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;物理智能通过演示使机器人能够获取复杂行为，在推进具身智能方面具有巨大的潜力；然而，为了实现跨不同平台和环境的一般化和迁移，需要精心设计模型架构、训练策略以及数据多样性。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的端到端机器人学习框架来解决现有系统在可扩展性、适应异构硬件能力及真实场景中目标评估方面的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;引入统一架构支持跨平台适应性，实现无缝部署于各种类型的机器人设备；通过集成多任务学习和精简网络设计，在保持与不同传感器配置和动作空间兼容性的前提下实现了比传统方法更稳健的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在七项操纵任务上的广泛实验验证了框架的有效性；尤其值得一提的是，基于扩散模型的方法表现出优于LeRobot框架的性能和泛化能力，跨多种机器人平台和环境条件实现性能改进。&lt;h4&gt;结论&lt;/h4&gt;提出的通用端到端学习框架有助于解决现有物理智能系统中的关键问题，并为未来研究提供了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文提及了物理智能在推动具身智能方面的重要性以及目前存在的挑战。论文介绍了一种新的机器人学习方法，该方法通过统一架构支持跨平台部署，旨在提高机器人的适应性和泛化能力。实验表明这种方法能够显著改善机器人的操作性能和环境适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Physical intelligence holds immense promise for advancing embodiedintelligence, enabling robots to acquire complex behaviors from demonstrations.However, achieving generalization and transfer across diverse robotic platformsand environments requires careful design of model architectures, trainingstrategies, and data diversity. Meanwhile existing systems often struggle withscalability, adaptability to heterogeneous hardware, and objective evaluationin real-world settings. We present a generalized end-to-end robotic learningframework designed to bridge this gap. Our framework introduces a unifiedarchitecture that supports cross-platform adaptability, enabling seamlessdeployment across industrial-grade robots, collaborative arms, and novelembodiments without task-specific modifications. By integrating multi-tasklearning with streamlined network designs, it achieves more robust performancethan conventional approaches, while maintaining compatibility with varyingsensor configurations and action spaces. We validate our framework throughextensive experiments on seven manipulation tasks. Notably, Diffusion-basedmodels trained in our framework demonstrated superior performance andgeneralizability compared to the LeRobot framework, achieving performanceimprovements across diverse robotic platforms and environmental conditions.</description>
      <author>example@mail.com (Yu Zhao, Huxian Liu, Xiang Chen, Jiankai Sun, Jiahuan Yan, Luhui Hu)</author>
      <guid isPermaLink="false">2503.05316v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Security-Aware Sensor Fusion with MATE: the Multi-Agent Trust Estimator</title>
      <link>http://arxiv.org/abs/2503.04954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;智能城市等多代理网络系统中的传感器融合由于缺乏安全意识而容易受到攻击。&lt;h4&gt;目的&lt;/h4&gt;设计一种基于信任估计的感知融合方法来抵御最新的威胁。&lt;h4&gt;方法&lt;/h4&gt;将信任估计视为隐藏马尔可夫模型，并通过映射传感器数据到信任伪测量（PSMs）的方式递归地在贝叶斯框架下更新信任后验概率，从而将信任融入到感知融合中，以增强态势感知的可信度。&lt;h4&gt;主要发现&lt;/h4&gt;开发了一种新的视角估计器、用于将传感器数据转换为PSM的逻辑以及高效贝叶斯更新方法。通过在物理基础的Unreal Engine模拟器CARLA中的案例研究和蒙特卡洛仿真评估了安全意识融合下的攻击情况。&lt;h4&gt;结论&lt;/h4&gt;基于信任的安全感知融合能够在敌对条件下建立可信赖的情境意识，并且这种混合了新型与传统安全相关度量的方法可以有效提高系统安全性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：Lacking security awareness, sensor fusion in systems with multi-agent networks such as smart cities is vulnerable to attacks. To guard against recent threats, we design security-aware sensor fusion that is based on the estimates of distributions over trust. Trust estimation can be cast as a hidden Markov model, and we solve it by mapping sensor data to trust pseudomeasurements (PSMs) that recursively update trust posteriors in a Bayesian context. Trust then feeds sensor fusion to facilitate trust-weighted updates to situational awareness. Essential to security-awareness are a novel field of view estimator, logic to map sensor data into PSMs, and the derivation of efficient Bayesian updates. We evaluate security-aware fusion under attacks on agents using case studies and Monte Carlo simulation in the physics-based Unreal Engine simulator, CARLA. A mix of novel and classical security-relevant metrics show that our security-aware fusion enables building trustworthy situational awareness even in hostile conditions.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lacking security awareness, sensor fusion in systems with multi-agentnetworks such as smart cities is vulnerable to attacks. To guard against recentthreats, we design security-aware sensor fusion that is based on the estimatesof distributions over trust. Trust estimation can be cast as a hidden Markovmodel, and we solve it by mapping sensor data to trust pseudomeasurements(PSMs) that recursively update trust posteriors in a Bayesian context. Trustthen feeds sensor fusion to facilitate trust-weighted updates to situationalawareness. Essential to security-awareness are a novel field of view estimator,logic to map sensor data into PSMs, and the derivation of efficient Bayesianupdates. We evaluate security-aware fusion under attacks on agents using casestudies and Monte Carlo simulation in the physics-based Unreal Enginesimulator, CARLA. A mix of novel and classical security-relevant metrics showthat our security-aware fusion enables building trustworthy situationalawareness even in hostile conditions.</description>
      <author>example@mail.com (R. Spencer Hallyburton, Miroslav Pajic)</author>
      <guid isPermaLink="false">2503.04954v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A Helping (Human) Hand in Kinematic Structure Estimation</title>
      <link>http://arxiv.org/abs/2503.05301v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA25; 8 pages + 7 figures; For supplementary material,  see https://www.tu.berlin/robotics/papers/helpinghands&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种概率实时方法，通过利用人类手部作为先验知识来减少视觉不确定性（如遮挡、缺乏纹理和噪声），从而提高机器人抓取物体时的运动学模型准确性。&lt;h4&gt;背景&lt;/h4&gt;在进行安全的机器人操作过程中，由于诸如遮挡、缺少纹理特征以及噪音等因素的影响，难以获得准确的运动学模型。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的方法来解决视觉不确定性对机器人操控中的运动学建模带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;通过跟踪人类手部在抓取过程中的受限制动作，并且明确地将视觉观测中的不确定因素纳入到模型中，从而在线估计物体的运动学模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法相比两个最近提出的基准方法分别提高了195%和140%，证明了利用合适先验并明确考虑不确定性的重要性。此外，这种方法还能够使机器人安全地操控小物件。&lt;h4&gt;结论&lt;/h4&gt;通过采用人类手部作为先验知识，并且在模型中明确处理视觉观测中的不确定因素，可以显著提高基于视觉的机器人抓取物体时运动学模型的准确性。&lt;h4&gt;翻译&lt;/h4&gt;视觉不确定性（如遮挡、缺少纹理特征和噪音）对安全地获取准确机器人操作所需的运动学模型提出了重大挑战。我们引入了一种概率实时方法，利用人类手部作为先验知识来减轻这些不确定性的影响。通过追踪在抓取过程中的人类手部动作，并明确建模视觉观测中的不确定因素，我们的方法可以在线可靠地估计物体的运动学模型。我们在一个新颖的数据集上验证了该方法的效果，这个数据集中包含的是遮挡严重和结构变化有限的挑战性对象。实验结果显示，通过引入适当的先验知识并考虑不确定性，本方法能产生精确度更高的估计结果，并且优于两种最近基准线的结果195%和140%，这表明我们的方法能够使机器人安全地抓取小物体。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual uncertainties such as occlusions, lack of texture, and noise presentsignificant challenges in obtaining accurate kinematic models for safe roboticmanipulation. We introduce a probabilistic real-time approach that leveragesthe human hand as a prior to mitigate these uncertainties. By tracking theconstrained motion of the human hand during manipulation and explicitlymodeling uncertainties in visual observations, our method reliably estimates anobject's kinematic model online. We validate our approach on a novel datasetfeaturing challenging objects that are occluded during manipulation and offerlimited articulations for perception. The results demonstrate that byincorporating an appropriate prior and explicitly accounting for uncertainties,our method produces accurate estimates, outperforming two recent baselines by195% and 140%, respectively. Furthermore, we demonstrate that our approach'sestimates are precise enough to allow a robot to manipulate even small objectssafely.</description>
      <author>example@mail.com (Adrian Pfisterer, Xing Li, Vito Mengers, Oliver Brock)</author>
      <guid isPermaLink="false">2503.05301v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Evidential Uncertainty Estimation for Multi-Modal Trajectory Prediction</title>
      <link>http://arxiv.org/abs/2503.05274v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于证据深度学习的多模态轨迹预测方法，该方法能够实时估计位置和模式的概率不确定性。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶领域中，准确的轨迹预测至关重要，但由于代理行为的不确定性和感知噪声的存在，这在本质上是具有挑战性的。尽管多模态轨迹预测模型可以生成多个可能的未来路径，并附上相应的概率，但有效地量化这种不确定性仍然是一项尚未解决的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效估计位置和模式不确定性并保持高精度轨迹预测的方法。&lt;h4&gt;方法&lt;/h4&gt;该研究使用基于证据深度学习的新颖多模态轨迹预测方法。此方法利用正态逆伽玛分布处理位置的不确定性，并用狄利克雷分布来量化模式的概率不确定性。与采样法不同，它可以在单一前向传递中推断出两种类型的不确定性，从而大幅提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明该方法能够在Argoverse 1和Argoverse 2数据集上提供可靠且准确的不确定性估计，并通过使用基于不确定性的重要性抽样技术进一步优化训练效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法有效地解决了多模态轨迹预测中量化不确定性的问题，同时保持了很高的轨迹预测精度。&lt;h4&gt;翻译&lt;/h4&gt;精确的轨迹预测对于自动驾驶至关重要，但由于代理行为和感知噪声引起的不确定性使其具有固有的挑战。尽管多模式轨迹预测模型生成多个可能的未来路径，并附上相应概率，但有效量化的不确定性的方法依然是一个开放的问题。在这项工作中，我们提出了一种基于证据深度学习的新颖多模态轨迹预测方法，它实时估计位置和模式的概率不确定性。该方法利用正态逆伽玛分布处理位置的不确定性以及狄利克雷分布处理模式的不确定性。与采样法不同，它可以在单一前向传递中推断出这两种类型的不确定性，显著提高了效率。此外，我们使用基于不确定性的重要性抽样技术来提高训练效率，通过优先考虑高不确定性样本而不是冗余低效样本进行优化。我们在Argoverse 1和2数据集上进行了广泛的评估，结果表明该方法能够提供可靠的不确定性估计同时保持高的轨迹预测准确度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate trajectory prediction is crucial for autonomous driving, yetuncertainty in agent behavior and perception noise makes it inherentlychallenging. While multi-modal trajectory prediction models generate multipleplausible future paths with associated probabilities, effectively quantifyinguncertainty remains an open problem. In this work, we propose a novelmulti-modal trajectory prediction approach based on evidential deep learningthat estimates both positional and mode probability uncertainty in real time.Our approach leverages a Normal Inverse Gamma distribution for positionaluncertainty and a Dirichlet distribution for mode uncertainty. Unlikesampling-based methods, it infers both types of uncertainty in a single forwardpass, significantly improving efficiency. Additionally, we experimented withuncertainty-driven importance sampling to improve training efficiency byprioritizing underrepresented high-uncertainty samples over redundant ones. Weperform extensive evaluations of our method on the Argoverse 1 and Argoverse 2datasets, demonstrating that it provides reliable uncertainty estimates whilemaintaining high trajectory prediction accuracy.</description>
      <author>example@mail.com (Sajad Marvi, Christoph Rist, Julian Schmidt, Julian Jordan, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.05274v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A Map-free Deep Learning-based Framework for Gate-to-Gate Monocular Visual Navigation aboard Miniaturized Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2503.05251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright}2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于单目摄像头的自主纳米无人机系统，该系统采用实时深度学习门检测前端和经典视觉伺服控制后端，在无需地图的情况下实现了高效的自主导航。&lt;h4&gt;背景&lt;/h4&gt;近年来，重量小于50克的掌上自主导航器进入无人机竞速领域。与较大的无人机相比，这些小型无人机在内存和计算能力方面存在三个数量级的差距，需要更加高效且轻量化的基于视觉的方法来提高性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种仅依赖机载资源、使用单目摄像头进行自主导航的小型无人机系统，并通过混合仿真与现实世界的训练，优化其性能。&lt;h4&gt;方法&lt;/h4&gt;本研究从两个最先进的深度学习模型出发，为特定任务进行了适应性调整。经过混合模拟和实际环境的训练后，在纳米无人机上集成并部署这些模型。最佳管道每帧仅消耗24M次乘法累加操作，并实现了30Hz的闭环控制性能。&lt;h4&gt;主要发现&lt;/h4&gt;在约2万个现实世界图像数据集上，门检测根均方误差仅为1.4像素。实验证明了纳米无人机能够在四分钟内成功通过15个关卡，未发生碰撞，总行程约为100米，最高速度达到1.9m/s。&lt;h4&gt;结论&lt;/h4&gt;该系统展示了其在从未见过的环境中的泛化能力，在超过四分钟的时间内导航通过关卡。这种技术为小型无人机自主竞速提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新型的小型无人驾驶飞机，它使用基于单个摄像头的视觉处理和实时深度学习算法来检测赛道上的标志，并能够迅速而准确地绕过障碍物进行竞赛。该系统在计算资源非常有限的情况下表现出色，展示了其在自主导航任务中的潜力和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Palm-sized autonomous nano-drones, i.e., sub-50g in weight, recently enteredthe drone racing scenario, where they are tasked to avoid obstacles andnavigate as fast as possible through gates. However, in contrast with theirbigger counterparts, i.e., kg-scale drones, nano-drones expose three orders ofmagnitude less onboard memory and compute power, demanding more efficient andlightweight vision-based pipelines to win the race. This work presents amap-free vision-based (using only a monocular camera) autonomous nano-dronethat combines a real-time deep learning gate detection front-end with a classicyet elegant and effective visual servoing control back-end, only relying ononboard resources. Starting from two state-of-the-art tiny deep learningmodels, we adapt them for our specific task, and after a mixedsimulator-real-world training, we integrate and deploy them aboard ournano-drone. Our best-performing pipeline costs of only 24M multiply-accumulateoperations per frame, resulting in a closed-loop control performance of 30 Hz,while achieving a gate detection root mean square error of 1.4 pixels, on our~20k real-world image dataset. In-field experiments highlight the capability ofour nano-drone to successfully navigate through 15 gates in 4 min, nevercrashing and covering a total travel distance of ~100m, with a peak flightspeed of 1.9 m/s. Finally, to stress the generalization capability of oursystem, we also test it in a never-seen-before environment, where it navigatesthrough gates for more than 4 min.</description>
      <author>example@mail.com (Lorenzo Scarciglia, Antonio Paolillo, Daniele Palossi)</author>
      <guid isPermaLink="false">2503.05251v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Reward-Centered ReST-MCTS: A Robust Decision-Making Framework for Robotic Manipulation in High Uncertainty Environments</title>
      <link>http://arxiv.org/abs/2503.05226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种增强版的Monte Carlo Tree Search (MCTS)框架，称为Reward-Centered ReST-MCTS。该方法通过在搜索过程中引入中间奖励机制来优化决策路径，并展示了它在机器人操纵任务中的优越性能。&lt;h4&gt;背景&lt;/h4&gt;传统的MCTS方法由于依赖于最终步骤的奖励评估，在不确定性高和数据嘈杂的环境中难以有效工作，无法提供实时反馈进行搜索改进。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MCTS框架Reward-Centered ReST-MCTS，以提高机器人决策任务中面对高度不确定性和噪声环境时的表现。&lt;h4&gt;方法&lt;/h4&gt;通过引入'奖励中心'（Rewarding Center）来动态分配部分奖励，结合规则验证、启发式指导和神经估算机制，在搜索过程中提供中间反馈进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;与基准方法相比，包括Chain-of-Thought (CoT)提示法和原始ReST-MCTS，在不确定环境下处理机器人操纵任务时，Reward-Centered ReST-MCTS提高了2-4%的决策准确性，并且在减少错误路径传播方面更为有效。此外，消融实验显示中间反馈对于搜索路径改进至关重要。&lt;h4&gt;结论&lt;/h4&gt;该方法能够在高不确定性下保持高性能和计算可行性，显示出在复杂机器人任务中的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;蒙特卡洛树搜索（MCTS）作为一种强大的决策工具，在机器人领域得到广泛应用。然而，传统的MCTS方法难以应对高度不确定性和嘈杂数据的挑战，因为它依赖于最终步骤奖励评估缺乏中间反馈优化路径导致次优决策和计算效率低下。本文介绍了一种新的框架Reward-Centered ReST-MCTS，通过引入中间奖励塑造来增强MCTS能力。该框架利用'奖励中心'动态分配部分奖励以改善搜索轨迹，并且在机器人操纵任务中进行了验证，显示出了持续的性能改进并与基准方法相比提高了决策准确性2-4%。此外，在不同不确定性的环境下保持了高性能的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monte Carlo Tree Search (MCTS) has emerged as a powerful tool fordecision-making in robotics, enabling efficient exploration of large searchspaces. However, traditional MCTS methods struggle in environmentscharacterized by high uncertainty and noisy data due to their reliance onfinal-step reward evaluation. The lack of intermediate feedback during searchoften results in suboptimal decision-making and computational inefficiencies.  This paper introduces Reward-Centered ReST-MCTS, a novel framework thatenhances MCTS by incorporating intermediate reward shaping. The core of ourapproach is the Rewarding Center, which refines search trajectories bydynamically assigning partial rewards using rule-based validation, heuristicguidance, and neural estimation. By integrating these mechanisms, our methodenables real-time optimization of search paths, mitigating the effects of errorpropagation.  We evaluate Reward-Centered ReST-MCTS in robotic manipulation tasks underhigh uncertainty, demonstrating consistent improvements in decision accuracy.Compared to baseline methods, including Chain-of-Thought (CoT) prompting andVanilla ReST-MCTS, our framework achieves a 2-4% accuracy improvement whilemaintaining computational feasibility. Ablation studies confirm theeffectiveness of intermediate feedback in search refinement, particularly inpruning incorrect decision paths early. Furthermore, robustness tests show thatour method retains high performance across varying levels of uncertainty.</description>
      <author>example@mail.com (Xibai Wang)</author>
      <guid isPermaLink="false">2503.05226v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Persistent Object Gaussian Splat (POGS) for Tracking Human and Robot Manipulation of Irregularly Shaped Objects</title>
      <link>http://arxiv.org/abs/2503.05189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了一种名为Persistent Object Gaussian Splat (POGS)的新系统，用于在动态环境中跟踪和操作之前未见过的不规则形状物体。&lt;h4&gt;背景&lt;/h4&gt;在制造、组装和物流等机器人应用中，追踪并操纵以前从未见过的不规则形状物体非常重要。尽管最近引入的Gaussian Splats能够有效建模对象几何结构，但它们缺乏面向任务的操作的状态估计。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的方法POGS，该方法结合了语义、自监督视觉特征和物体分组特征，并以紧凑的形式表示，可以持续更新以估算扫描物体的姿态。&lt;h4&gt;方法&lt;/h4&gt;POGS采用单一立体相机在多视角场景捕获和训练阶段之后，将深度估计与自监督视觉编码器功能相结合来进行对象姿态估计。该系统支持抓取、重新定向以及自然语言驱动的操作，并能够进行一系列的人类诱导扰动下的重置操作。&lt;h4&gt;主要发现&lt;/h4&gt;POGS能够在每次重置中连续成功地跟踪和操纵多达12个物体，并能从80%的在握工具干扰中恢复过来。此外，它还能处理高达30度的工具位移错误。&lt;h4&gt;结论&lt;/h4&gt;通过整合自监督视觉特征、语义信息以及物体分组特性，POGS提供了一种强大的方法来实现动态环境下的精确姿态估算和任务操作。&lt;h4&gt;翻译&lt;/h4&gt;追踪并操纵在制造、组装和物流等应用中未见过的不规则形状物体非常重要。最近引入的Gaussian Splats能够有效建模对象几何结构，但缺乏面向任务的操作的状态估计。我们介绍了一种新的系统POGS，结合了语义、自监督视觉特征和物体分组特性，并采用紧凑的形式表示，可以持续更新以估算扫描物体的姿态。在初始多视角场景捕获和训练之后，该系统利用单个立体相机集成了深度估计与自监督视觉编码器功能来进行对象姿态估计。POGS支持抓取、重新定向以及自然语言驱动的操作，并能够处理人类诱导扰动下的工具位移错误至高达30度的范围内。POGS实现了多达12次连续成功的物体重置操作并从80%的在握工具干扰中恢复过来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tracking and manipulating irregularly-shaped, previously unseen objects indynamic environments is important for robotic applications in manufacturing,assembly, and logistics. Recently introduced Gaussian Splats efficiently modelobject geometry, but lack persistent state estimation for task-orientedmanipulation. We present Persistent Object Gaussian Splat (POGS), a system thatembeds semantics, self-supervised visual features, and object grouping featuresinto a compact representation that can be continuously updated to estimate thepose of scanned objects. POGS updates object states without requiring expensiverescanning or prior CAD models of objects. After an initial multi-view scenecapture and training phase, POGS uses a single stereo camera to integrate depthestimates along with self-supervised vision encoder features for object poseestimation. POGS supports grasping, reorientation, and natural language-drivenmanipulation by refining object pose estimates, facilitating sequential objectreset operations with human-induced object perturbations and tool servoing,where robots recover tool pose despite tool perturbations of up to 30{\deg}.POGS achieves up to 12 consecutive successful object resets and recovers from80% of in-grasp tool perturbations.</description>
      <author>example@mail.com (Justin Yu, Kush Hari, Karim El-Refai, Arnav Dalal, Justin Kerr, Chung Min Kim, Richard Cheng, Muhammad Zubair Irshad, Ken Goldberg)</author>
      <guid isPermaLink="false">2503.05189v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Safety-Critical Traffic Simulation with Adversarial Transfer of Driving Intentions</title>
      <link>http://arxiv.org/abs/2503.05180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个新的策略IntSim，用于模拟自动驾驶车辆在复杂和潜在危险场景下的行为。通过将驾驶意图与运动规划解耦，并结合大规模的真实世界数据，该方法能更准确地仿真交通参与者的行为。&lt;h4&gt;背景&lt;/h4&gt;当前的交通仿真技术难以处理动态且对抗性的交互场景，尤其是在利用日志数据进行训练时，这些数据通常只包括常规情况下的车辆行为。&lt;h4&gt;目的&lt;/h4&gt;提出一种创新且有效的策略IntSim，用于生成逼真的安全关键性模拟场景，并通过这种方法提高自动驾驶系统应对复杂和危险场景的能力。&lt;h4&gt;方法&lt;/h4&gt;IntSim策略将驾驶意图的转移形式化为一个优化问题，同时结合深度学习模型来预测基于环境变化的车辆行为。此外，该方法还利用大规模真实世界数据集（如nuScenes和Waymo）进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，通过灵活实现动态对抗性交互并模拟真实的安全关键场景，IntSim在仿真逼真的安全关键性场景方面表现出色，并且对于改善自动驾驶车辆规划者处理此类场景的能力有显著贡献。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种创新的方法来生成和利用复杂的交通场景数据，从而有助于评估和改进自动驾驶系统的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic simulation, complementing real-world data with a long-taildistribution, allows for effective evaluation and enhancement of the ability ofautonomous vehicles to handle accident-prone scenarios. Simulating suchsafety-critical scenarios is nontrivial, however, from log data that aretypically regular scenarios, especially in consideration of dynamic adversarialinteractions between the future motions of autonomous vehicles and surroundingtraffic participants. To address it, this paper proposes an innovative andefficient strategy, termed IntSim, that explicitly decouples the drivingintentions of surrounding actors from their motion planning for realistic andefficient safety-critical simulation. We formulate the adversarial transfer ofdriving intention as an optimization problem, facilitating extensiveexploration of diverse attack behaviors and efficient solution convergence.Simultaneously, intention-conditioned motion planning benefits from powerfuldeep models and large-scale real-world data, permitting the simulation ofrealistic motion behaviors for actors. Specially, through adapting drivingintentions based on environments, IntSim facilitates the flexible realizationof dynamic adversarial interactions with autonomous vehicles. Finally,extensive open-loop and closed-loop experiments on real-world datasets,including nuScenes and Waymo, demonstrate that the proposed IntSim achievesstate-of-the-art performance in simulating realistic safety-critical scenariosand further improves planners in handling such scenarios.</description>
      <author>example@mail.com (Zherui Huang, Xing Gao, Guanjie Zheng, Licheng Wen, Xuemeng Yang, Xiao Sun)</author>
      <guid isPermaLink="false">2503.05180v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>SplatPose: Geometry-Aware 6-DoF Pose Estimation from Single RGB Image via 3D Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2503.05174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SplatPose是一种利用3D高斯点阵和双分支神经网络架构来提高单RGB图像中6-DoF姿态估计精度的框架，解决了现有方法依赖初始姿态预估和旋转模糊的问题。&lt;h4&gt;背景&lt;/h4&gt;在增强现实和机器人技术中的广泛应用下，6-DoF姿态估计是计算机视觉的基本任务。然而，现有的基于单个RGB的方法往往因为对初始姿态估计的依赖以及对抗旋转模糊的能力不足而牺牲精度，同时需要深度传感器或多视角设置的方法则部署成本高昂。&lt;h4&gt;目的&lt;/h4&gt;提出SplatPose框架以解决现有方法在精度和部署成本方面的局限性。&lt;h4&gt;方法&lt;/h4&gt;SplatPose结合了3D高斯点阵（3DGS）技术和一个双分支神经架构，并引入了Dual-Attention Ray Scoring Network(DARS-Net)，通过几何域注意力机制来解耦位置对齐与角度对齐，从而减少旋转模糊。此外，采用了一个粗到细的优化流程逐步精化姿态估计。&lt;h4&gt;主要发现&lt;/h4&gt;在三个基准数据集上的实验表明SplatPose在单RGB设置下的6-DoF姿态估计准确度达到业界领先水平，并且其性能可媲美需要深度信息或多个视角图像的方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的SplatPose框架通过创新的3D高斯点阵技术和精细化的姿态对齐流程，在仅使用单张RGB图像的情况下达到了卓越的6-DoF姿态估计精度，具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;六自由度（6-DoF）姿态估计是计算机视觉中的一个基本任务，并在增强现实和机器人技术中有着广泛的应用。现有的基于单一RGB的方法通常因对初始姿态估计的依赖以及对抗旋转模糊的能力不足而牺牲了精度，而那些需要深度传感器或多视角设置的方法则导致了显著的成本增加。为了解决这些限制，我们引入了一种新的框架SplatPose，该框架结合了3D高斯点阵（3DGS）和双分支神经架构，在仅使用单张RGB图像的情况下实现了高度精确的姿态估计。我们的方法的核心是Dual-Attention Ray Scoring Network (DARS-Net)，它通过几何域注意机制创新性地解耦了位置对齐与角度对齐，明确模型方向依赖关系以减少旋转模糊。此外，一个从粗到细的优化流程逐步精化姿态估计，通过比对查询图像和3DGS合成视图之间的密集2D特征来纠正特征错位及深度误差。在三个基准数据集上的实验显示，在单RGB设置下SplatPose实现了6-DoF姿态估计的最先进精度，与依赖于深度或多视角图像的方法相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 6-DoF pose estimation is a fundamental task in computer vision withwide-ranging applications in augmented reality and robotics. Existing singleRGB-based methods often compromise accuracy due to their reliance on initialpose estimates and susceptibility to rotational ambiguity, while approachesrequiring depth sensors or multi-view setups incur significant deploymentcosts. To address these limitations, we introduce SplatPose, a novel frameworkthat synergizes 3D Gaussian Splatting (3DGS) with a dual-branch neuralarchitecture to achieve high-precision pose estimation using only a single RGBimage. Central to our approach is the Dual-Attention Ray Scoring Network(DARS-Net), which innovatively decouples positional and angular alignmentthrough geometry-domain attention mechanisms, explicitly modeling directionaldependencies to mitigate rotational ambiguity. Additionally, a coarse-to-fineoptimization pipeline progressively refines pose estimates by aligning dense 2Dfeatures between query images and 3DGS-synthesized views, effectivelycorrecting feature misalignment and depth errors from sparse ray sampling.Experiments on three benchmark datasets demonstrate that SplatPose achievesstate-of-the-art 6-DoF pose estimation accuracy in single RGB settings,rivaling approaches that depend on depth or multi-view images.</description>
      <author>example@mail.com (Linqi Yang, Xiongwei Zhao, Qihao Sun, Ke Wang, Ao Chen, Peng Kang)</author>
      <guid isPermaLink="false">2503.05174v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>A Comprehensive LLM-powered Framework for Driving Intelligence Evaluation</title>
      <link>http://arxiv.org/abs/2503.05164v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种用于评估复杂交通环境中自动驾驶行为智能的框架。&lt;h4&gt;背景信息&lt;/h4&gt;目前没有全面的方法来评价自动驾驶系统的智能水平，尤其是在处理复杂的驾驶环境时。&lt;h4&gt;研究目的&lt;/h4&gt;构建一种能够对自动驾驶系统在复杂道路条件下的行为进行客观、综合评价的体系和方法论。&lt;h4&gt;数据来源与构建&lt;/h4&gt;通过自然驾驶实验和驾驶后的行为评估访谈，建立了专业驾驶员和乘客参与的人工智能语言评价数据集。&lt;h4&gt;框架开发&lt;/h4&gt;基于创建的数据集，开发了一种由大模型驱动的自动驾驶评价框架，并在CARLA城市交通模拟器中进行了验证。&lt;h4&gt;人类评估&lt;/h4&gt;该框架的有效性还通过人的直接评估得到了进一步证实。&lt;h4&gt;贡献与影响&lt;/h4&gt;为理解和设计更加智能、人性化的自动驾驶系统提供了有价值的见解和指导。&lt;h4&gt;翻译&lt;/h4&gt;用于评估自主驾驶智能的评价方法对于算法优化至关重要。然而，由于驾驶智能的复杂性，目前尚无全面的评价方法来确定自主驾驶的级别。本文提出了一个复杂的交通环境下的驾驶行为智能评价框架，以填补这一空白。我们通过自然驾驶实验和驾驶后的行为评估访谈构建了一个由专业驾驶员和乘客参与的人工语言评价数据集。基于此数据集，我们开发了一种大模型驱动的驾驶评价框架，并在CARLA城市交通模拟器中进行了验证。此外，还得到了人的直接评估的支持。我们的研究为理解和设计更加智能、人性化的自主驾驶系统提供了有价值的见解。该框架的实现细节和关于数据集的详细信息可在Github上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-07&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Evaluation methods for autonomous driving are crucial for algorithmoptimization. However, due to the complexity of driving intelligence, there iscurrently no comprehensive evaluation method for the level of autonomousdriving intelligence. In this paper, we propose an evaluation framework fordriving behavior intelligence in complex traffic environments, aiming to fillthis gap. We constructed a natural language evaluation dataset of humanprofessional drivers and passengers through naturalistic driving experimentsand post-driving behavior evaluation interviews. Based on this dataset, wedeveloped an LLM-powered driving evaluation framework. The effectiveness ofthis framework was validated through simulated experiments in the CARLA urbantraffic simulator and further corroborated by human assessment. Our researchprovides valuable insights for evaluating and designing more intelligent,human-like autonomous driving agents. The implementation details of theframework and detailed information about the dataset can be found at Github.</description>
      <author>example@mail.com (Shanhe You, Xuewen Luo, Xinhe Liang, Jiashu Yu, Chen Zheng, Jiangtao Gong)</author>
      <guid isPermaLink="false">2503.05164v1</guid>
      <pubDate>Mon, 10 Mar 2025 17:10:46 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Neural Ordinary Differential Equations as Interpretable Healthcare classifiers</title>
      <link>http://arxiv.org/abs/2503.03129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ACL SRW Submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;深度学习在机器学习领域中是一个重要的创新，然而其决策过程的‘黑箱’特性导致了医疗和科研领域的质疑。&lt;h4&gt;背景&lt;/h4&gt;尽管深度学习技术取得了显著进展，但因其复杂性和不可解释性，在如医疗等需要高度透明度的行业中遇到了挑战。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种可解释性的方法——使用神经常微分方程（NODEs）模型来解决深度学习领域的‘黑箱’问题。&lt;h4&gt;方法&lt;/h4&gt;通过利用差分方程动态特性，该研究引入了基于NODEs的新架构，能够连续处理文本数据，并首次展示了此类模型的潜力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的新型神经网络结构不仅具有强大的预测能力，还提供了比传统深度学习模型更高的透明度。&lt;h4&gt;结论&lt;/h4&gt;这项研究为医疗等领域的研究人员提供了一种新的研究方向和可解释性更好的深度学习解决方案。&lt;h4&gt;翻译&lt;/h4&gt;深度学习作为机器学习领域的一项重要创新已经出现。然而，该领域的一个显著局限在于其‘黑箱’决策过程，这在如健康照护和科学社群中引发了对其适用性的质疑。为回应这一挑战，本文介绍了一种可解释的方法，使用神经常微分方程（NODEs）模型，这是一种利用微分方程动态特性进行表示学习的神经网络模型类别。通过借鉴差分方程的基础知识，我们展示了此类模型在连续处理文本数据方面的潜力，这是首个具有这种能力的模型，并为该领域未来的研究提出了一个有前途的方向。这项研究的主要目标是为那些既需要深度学习预测功能又重视NODEs透明度重要性的群体（如医疗健康）提供一种新的架构建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Learning has emerged as one of the most significant innovations inmachine learning. However, a notable limitation of this field lies in the``black box" decision-making processes, which have led to skepticism withingroups like healthcare and scientific communities regarding its applicability.In response, this study introduces a interpretable approach using NeuralOrdinary Differential Equations (NODEs), a category of neural network modelsthat exploit the dynamics of differential equations for representationlearning. Leveraging their foundation in differential equations, we illustratethe capability of these models to continuously process textual data, markingthe first such model of its kind, and thereby proposing a promising directionfor future research in this domain. The primary objective of this research isto propose a novel architecture for groups like healthcare that require thepredictive capabilities of deep learning while emphasizing the importance ofmodel transparency demonstrated in NODEs.</description>
      <author>example@mail.com (Shi Li)</author>
      <guid isPermaLink="false">2503.03129v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
  <item>
      <title>Low-Level Matters: An Efficient Hybrid Architecture for Robust Multi-frame Infrared Small Target Detection</title>
      <link>http://arxiv.org/abs/2503.02220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的混合架构LVNet，用于改进多帧红外小目标检测（IRSTD）的性能。&lt;h4&gt;背景&lt;/h4&gt;多帧红外小目标检测在低空和海上监控中至关重要。结合卷积神经网络（CNN）和变换器的方法显示出增强多帧IRSTD性能的巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;通过重新定义混合框架中的低级特征学习，设计一种简单而强大的LVNet架构来提升多帧IRSTD的效率。&lt;h4&gt;方法&lt;/h4&gt;引入了基于多尺度CNN前端的改进方案，以更好地捕捉对红外小目标至关重要的规模敏感局部特征，并设计了一个U形视频变换器用于跨多个时间空间上下文建模，有效捕获目标运动特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明LVNet在IRDST和NUDT-MIRSDT公共数据集上的性能超越了现有的最先进的方法。相比目前最好的LMAFormer模型，LVNet在nIoU指标上提高了5.63%/18.36%，且使用的参数量仅为后者的1/221，计算成本也大幅减少。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明低级表示学习对于混合架构的重要性，并证明了LVNet的有效性。该代码和训练模型可从https://github.com/ZhihuaShen/LVNet获取。&lt;h4&gt;翻译&lt;/h4&gt;多帧红外小目标检测在低空和海上监控中扮演重要角色，结合CNN和Transformer的混合架构显示出增强性能的巨大潜力。本文提出了一种简单而强大的LVNet架构，通过重新定义低级特征学习来提升多帧IRSTD的效率。实验表明该方法超越了现有的最佳方法，并且其代码和模型已在GitHub上公开。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-frame infrared small target detection (IRSTD) plays a crucial role inlow-altitude and maritime surveillance. The hybrid architecture combining CNNsand Transformers shows great promise for enhancing multi-frame IRSTDperformance. In this paper, we propose LVNet, a simple yet powerful hybridarchitecture that redefines low-level feature learning in hybrid frameworks formulti-frame IRSTD. Our key insight is that the standard linear patch embeddingsin Vision Transformers are insufficient for capturing the scale-sensitive localfeatures critical to infrared small targets. To address this limitation, weintroduce a multi-scale CNN frontend that explicitly models local features byleveraging the local spatial bias of convolution. Additionally, we design aU-shaped video Transformer for multi-frame spatiotemporal context modeling,effectively capturing the motion characteristics of targets. Experiments on thepublicly available datasets IRDST and NUDT-MIRSDT demonstrate that LVNetoutperforms existing state-of-the-art methods. Notably, compared to the currentbest-performing method, LMAFormer, LVNet achieves an improvement of 5.63\% /18.36\% in nIoU, while using only 1/221 of the parameters and 1/92 / 1/21 ofthe computational cost. Ablation studies further validate the importance oflow-level representation learning in hybrid architectures. Our code and trainedmodels are available at https://github.com/ZhihuaShen/LVNet.</description>
      <author>example@mail.com (Zhihua Shen, Siyang Chen, Han Wang, Tongsu Zhang, Xiaohu Zhang, Xiangpeng Xu, Xia Yang)</author>
      <guid isPermaLink="false">2503.02220v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>An Information-theoretic Multi-task Representation Learning Framework for Natural Language Understanding</title>
      <link>http://arxiv.org/abs/2503.04667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, accepted to AAAI 2025 (main conference), the code is  available at https://github.com/zerohd4869/InfoMTL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多任务表示学习框架（InfoMTL），旨在提取对所有任务都适用的噪声不变的充分表示。&lt;h4&gt;背景&lt;/h4&gt;现有的多任务学习方法在处理冗余特征和表示压缩问题时存在不足，尤其是在语言理解领域。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在多任务范式下增强预训练语言模型（PLM）的语言理解能力的新框架。&lt;h4&gt;方法&lt;/h4&gt;{'共享信息最大化原则': '旨在学习对所有目标任务都更充分的共享表示，以避免由于表示压缩而在多任务情况下出现表示不足的问题。', '特定任务的信息最小化原则': '设计用于缓解输入中潜在冗余特征对每个任务造成的负面影响，可以压缩任务无关的冗余信息并保持与目标相关的必要信息。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验结果': '在六个分类基准上进行的实验表明，在相同多任务设置下，该方法优于12种比较性多任务方法，尤其是在数据受限和存在噪声的情况下。', '进一步实验': '广泛的实验表明所学习到的表示更加充分、高效，并且具有更强的鲁棒性。'}&lt;h4&gt;结论&lt;/h4&gt;InfoMTL框架通过确保共享表示对所有任务都是充足的并减少冗余特征的影响，在多任务环境下提高了PLM的语言理解能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的内容（已省略，此处仅用于说明）&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a new principled multi-task representation learningframework (InfoMTL) to extract noise-invariant sufficient representations forall tasks. It ensures sufficiency of shared representations for all tasks andmitigates the negative effect of redundant features, which can enhance languageunderstanding of pre-trained language models (PLMs) under the multi-taskparadigm. Firstly, a shared information maximization principle is proposed tolearn more sufficient shared representations for all target tasks. It can avoidthe insufficiency issue arising from representation compression in themulti-task paradigm. Secondly, a task-specific information minimizationprinciple is designed to mitigate the negative effect of potential redundantfeatures in the input for each task. It can compress task-irrelevant redundantinformation and preserve necessary information relevant to the target formulti-task prediction. Experiments on six classification benchmarks show thatour method outperforms 12 comparative multi-task methods under the samemulti-task settings, especially in data-constrained and noisy scenarios.Extensive experiments demonstrate that the learned representations are moresufficient, data-efficient, and robust.</description>
      <author>example@mail.com (Dou Hu, Lingwei Wei, Wei Zhou, Songlin Hu)</author>
      <guid isPermaLink="false">2503.04667v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>CLDyB: Towards Dynamic Benchmarking for Continual Learning with Pre-trained Models</title>
      <link>http://arxiv.org/abs/2503.04655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了用于评估连续学习(CL)方法的动态基准CLDyB框架，该框架基于马尔可夫决策过程并利用蒙特卡洛树搜索来确定具有挑战性的任务序列。&lt;h4&gt;背景&lt;/h4&gt;基础模型时代的到来激发了将预训练表示用于持续学习的研究兴趣，产生了一系列在标准评估基准上表现优异的方法。然而，人们对预训练阶段可能的数据污染问题日益关注，并且静态的评估基准无法捕捉到现实世界CL场景中的复杂性。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了基于动态基准(CLDB)的一般计算框架来可靠地评估CL方法。&lt;h4&gt;方法&lt;/h4&gt;CLDyB使用马尔可夫决策过程和蒙特卡洛树搜索技术识别给定的持续学习方法中固有的困难任务，并确定具有挑战性的任务顺序。此外，通过这个框架对多个最先进的CL方法进行了联合评估，揭示了现有方法在处理某些特定任务序列时的能力局限性。&lt;h4&gt;主要发现&lt;/h4&gt;发现了现有的连续学习方法在面对通用且具代表性的任务序列时存在明显的性能瓶颈和弱点。&lt;h4&gt;结论&lt;/h4&gt;提供了公开访问的源代码和生成的任务序列链接(https://github.com/szc12153/CLDyB)，以供进一步研究使用，该框架为持续学习领域提供了一个可靠评估新方法的有效工具。&lt;h4&gt;翻译&lt;/h4&gt;基础模型时代的到来引发了利用预训练表示进行连续学习(CL)的研究热潮，产生了在标准评估基准上表现优异的一系列顶级CL方法。然而，人们对预训练阶段潜在数据污染问题的担忧日益增加，并且静态的标准评估基准无法捕捉到现实世界中CL场景的复杂性，导致现有性能达到饱和状态。为了解决这些问题，我们提出了CL on dynamic benchmarks(CLDB)，这是一个基于马尔可夫决策过程来可靠评估CL方法的一般计算框架。该框架可以动态地识别对于给定的CL方法来说固有的困难任务，并通过蒙特卡洛树搜索确定具有挑战性的任务顺序。借助于CLDB，我们首先对多个最先进的CL方法进行了联合评估，揭示了一组普遍具有挑战性和通用性且现有CL方法表现不佳的任务序列。然后分别用CLDB单独评估各个CL方法，发现它们各自的优缺点。源代码和生成的任务序列可以在 https://github.com/szc12153/CLDyB 上公开访问。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The advent of the foundation model era has sparked significant researchinterest in leveraging pre-trained representations for continual learning (CL),yielding a series of top-performing CL methods on standard evaluationbenchmarks. Nonetheless, there are growing concerns regarding potential datacontamination during the pre-training stage. Furthermore, standard evaluationbenchmarks, which are typically static, fail to capture the complexities ofreal-world CL scenarios, resulting in saturated performance. To address theseissues, we describe CL on dynamic benchmarks (CLDyB), a general computationalframework based on Markov decision processes for evaluating CL methodsreliably. CLDyB dynamically identifies inherently difficult andalgorithm-dependent tasks for the given CL methods, and determines challengingtask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct ajoint evaluation of multiple state-of-the-art CL methods, leading to a set ofcommonly challenging and generalizable task sequences where existing CL methodstend to perform poorly. We then conduct separate evaluations of individual CLmethods using CLDyB, discovering their respective strengths and weaknesses. Thesource code and generated task sequences are publicly accessible athttps://github.com/szc12153/CLDyB.</description>
      <author>example@mail.com (Shengzhuang Chen, Yikai Liao, Xiaoxiao Sun, Kede Ma, Ying Wei)</author>
      <guid isPermaLink="false">2503.04655v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Meta Learning not to Learn: Robustly Informing Meta-Learning under Nuisance-Varying Families</title>
      <link>http://arxiv.org/abs/2503.04570v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在存在虚假特征和因果预测因子的情况下，标准神经网络训练时倾向于依赖于这些虚假特征。为了引导网络向具有普适性的假设发展，需要引入额外的归纳偏差。当这种挑战存在于与多个任务相关的场景下（例如来自不同医院的图像扫描用于疾病预后估计）时，情况会变得更加复杂。&lt;h4&gt;背景&lt;/h4&gt;在涉及虚假和因果预测因子同时存在的环境中，标准神经网络倾向于依赖于虚假特征。这导致了在存在多种干扰的情况下进行分布稳健性目标上的学习变得困难，尤其是在相关任务共享这些虚假特征时（例如多医院图像扫描）。&lt;h4&gt;目的&lt;/h4&gt;为了应对这种挑战，需要集成适当的归纳偏差以使模型能够跨不同类型的家族泛化，包括具有干扰的家族和任务家族。为此提出了RIME方法来解决在存在正面和负面归纳偏见的情况下进行元学习的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种理论上的因果框架，解释了现有知识整合方法为何会导致分布稳健性目标下的性能下降，并展示了RIME能够同时集成这两种偏见，在扰动变化家族中的告知元学习设置下达到最佳的性能。&lt;h4&gt;主要发现&lt;/h4&gt;RIME能够在不牺牲正面归纳偏见的情况下避免负面归纳偏见的影响，从而在具有干扰因素的家庭中实现了分布稳健性的目标优化。它为如何有效地解决元学习环境中存在的正负诱导偏差问题提供了新的思路。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一个新的元学习框架（即RIME），用于处理存在多种诱导偏见的情况，并展示了其在复杂环境下的优越性能，这表明了该方法在未来类似问题中的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;原文描述了一种新提出的方法RIME，在涉及虚假特征和因果预测因子同时存在的环境中，可以有效地进行元学习。这种方法不仅可以避免负面归纳偏差的影响，还可以优化正面的偏见，从而在复杂环境下实现更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In settings where both spurious and causal predictors are available, standardneural networks trained under the objective of empirical risk minimization(ERM) with no additional inductive biases tend to have a dependence on aspurious feature. As a result, it is necessary to integrate additionalinductive biases in order to guide the network toward generalizable hypotheses.Often these spurious features are shared across related tasks, such asestimating disease prognoses from image scans coming from different hospitals,making the challenge of generalization more difficult. In these settings, it isimportant that methods are able to integrate the proper inductive biases togeneralize across both nuisance-varying families as well as task families.Motivated by this setting, we present RIME (Robustly Informed Meta lEarning), anew method for meta learning under the presence of both positive and negativeinductive biases (what to learn and what not to learn). We first develop atheoretical causal framework showing why existing approaches at knowledgeintegration can lead to worse performance on distributionally robustobjectives. We then show that RIME is able to simultaneously integrate bothbiases, reaching state of the art performance under distributionally robustobjectives in informed meta-learning settings under nuisance-varying families.</description>
      <author>example@mail.com (Louis McConnell)</author>
      <guid isPermaLink="false">2503.04570v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Joint Masked Reconstruction and Contrastive Learning for Mining Interactions Between Proteins</title>
      <link>http://arxiv.org/abs/2503.04650v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的蛋白质-蛋白质相互作用（PPI）预测方法，称为JmcPPI，该方法结合了掩码重建和对比学习。&lt;h4&gt;背景&lt;/h4&gt;蛋白质-蛋白质相互作用的预测对于阐明细胞操作机制具有重要作用，并对制药开发和临床治疗领域有着重要的实际意义。当前的研究主要集中在氨基酸序列分析上，基于蛋白结构的研究仍处于初步探索阶段。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的PPI预测方法JmcPPI，以推进该领域的研究进展。&lt;h4&gt;方法&lt;/h4&gt;JmcPPI将PPI预测任务分为两个不同阶段：在残基结构编码阶段，设计了两个特征重建任务并采用了图注意力机制来捕捉残基之间的结构信息；在蛋白质相互作用推理阶段，扰动原始的PPI图，并采用多图对比学习策略彻底挖掘新蛋白的外源性交互信息。&lt;h4&gt;主要发现&lt;/h4&gt;在三个常用的PPI数据集上进行的广泛实验表明，JmcPPI超越了现有的最佳基线模型，在各种数据划分方案下表现优异。&lt;h4&gt;结论&lt;/h4&gt;JmcPPI通过结合掩码重建和对比学习的方法，在蛋白质-蛋白质相互作用预测领域取得了显著的进步，并展示了其在实际应用中的潜力。相关代码可从GitHub获得。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein-protein interaction (PPI) prediction is an instrumental means inelucidating the mechanisms underlying cellular operations, holding significantpractical implications for the realms of pharmaceutical development andclinical treatment. Presently, the majority of research methods primarilyconcentrate on the analysis of amino acid sequences, while investigationspredicated on protein structures remain in the nascent stages of exploration.Despite the emergence of several structure-based algorithms in recent years,these are still confronted with inherent challenges: (1) the extraction ofintrinsic structural information of proteins typically necessitates theexpenditure of substantial computational resources; (2) these models are overlyreliant on seen protein data, struggling to effectively unearth interactioncues between unknown proteins. To further propel advancements in this domain,this paper introduces a novel PPI prediction method jointing maskedreconstruction and contrastive learning, termed JmcPPI. This methodologydissects the PPI prediction task into two distinct phases: during the residuestructure encoding phase, JmcPPI devises two feature reconstruction tasks andemploys graph attention mechanism to capture structural information betweenresidues; during the protein interaction inference phase, JmcPPI perturbs theoriginal PPI graph and employs a multi-graph contrastive learning strategy tothoroughly mine extrinsic interaction information of novel proteins. Extensiveexperiments conducted on three widely utilized PPI datasets demonstrate thatJmcPPI surpasses existing optimal baseline models across various data partitionschemes. The associated code can be accessed viahttps://github.com/lijfrank-open/JmcPPI.</description>
      <author>example@mail.com (Jiang Li, Xiaoping Wang)</author>
      <guid isPermaLink="false">2503.04650v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A Modular Pipeline for 3D Object Tracking Using RGB Cameras</title>
      <link>http://arxiv.org/abs/2503.04322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 11 figures, original paper not to be published anywhere else&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的可模块化流水线，用于计算多对象的3D轨迹。该方法适用于多种环境，其中多个同步且固定位置的摄像头记录移动物体，并使用现成的网络摄像头。&lt;h4&gt;背景&lt;/h4&gt;大多数追踪系统受限于二维平面运动并只能追踪单个对象，而这限制了它们在计算机视觉中的应用范围。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够适应不同场景、特别是多摄像机环境下的新型3D轨迹计算方法。&lt;h4&gt;方法&lt;/h4&gt;使用六个RGB网络摄像头跟踪放置餐具过程中的各种对象。该方法需要应对小物体检测、确定相机位置、区分重叠和遮挡的物体以及最终根据平均12456个像素坐标（每三次试验）计算出正确的三维轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;开发了一种稳健的方法，能够生成准确的3D轨迹，并使用x,y,z的位置方差作为置信度指标。此方法能动态处理出现和消失的对象，并即时创建新的扩展卡尔曼滤波器。此外，该方法即使在试验中未知相机位置的情况下也能够大规模应用于数百次餐桌布置实验。&lt;h4&gt;结论&lt;/h4&gt;通过这种方法，在表格设置数据集上进行的测试表明，它可以有效地解决多对象追踪问题并提供精确的结果，同时减少人工注释的需求。&lt;h4&gt;翻译&lt;/h4&gt;物体跟踪是计算机视觉中的关键挑战，具有各种应用，所有这些都需要不同的架构。大多数跟踪系统都存在限制，例如将所有运动约束在二维平面上，并且通常只能跟踪单个对象。在这篇文章中，我们提出了一种新的模块化流水线，用于计算多个对象的3D轨迹。它适用于多种设置，其中多个同步和静止摄像头记录移动物体，并使用现成的网络摄像头进行此操作。我们在表格布置数据集上测试了我们的管道，该数据集中参与者被各种传感器记录下来，当他们用餐桌物品布置桌子时。我们使用六个RGB网络摄像头来跟踪这些操纵的对象。挑战包括：在9,874,699帧中检测小物体、确定相机位置、区分附近和重叠的物体、临时遮挡以及最终计算出正确的3D轨迹，这需要使用平均12456个像素坐标（每三次实验）。我们实现了一种稳健的方法，它生成准确的轨迹，并使用x,y,z的位置方差作为置信度指标。它可以动态处理出现和消失的对象，即时创建新的扩展卡尔曼滤波器。即使在试验中未知相机位置的情况下，这种方法也能够大规模应用于数百次餐桌布置实验，几乎不需要人工注释输入。代码可在https://github.com/LarsBredereke/object_tracking上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object tracking is a key challenge of computer vision with variousapplications that all require different architectures. Most tracking systemshave limitations such as constraining all movement to a 2D plane and they oftentrack only one object. In this paper, we present a new modular pipeline thatcalculates 3D trajectories of multiple objects. It is adaptable to varioussettings where multiple time-synced and stationary cameras record movingobjects, using off the shelf webcams. Our pipeline was tested on the TableSetting Dataset, where participants are recorded with various sensors as theyset a table with tableware objects. We need to track these manipulated objects,using 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699camera frames, determining camera poses, discriminating between nearby andoverlapping objects, temporary occlusions, and finally calculating a 3Dtrajectory using the right subset of an average of 11.12.456 pixel coordinatesper 3-minute trial. We implement a robust pipeline that results in accuratetrajectories with covariance of x,y,z-position as a confidence metric. It dealsdynamically with appearing and disappearing objects, instantiating new ExtendedKalman Filters. It scales to hundreds of table-setting trials with very littlehuman annotation input, even with the camera poses of each trial unknown. Thecode is available at https://github.com/LarsBredereke/object_tracking</description>
      <author>example@mail.com (Lars Bredereke, Yale Hartmann, Tanja Schultz)</author>
      <guid isPermaLink="false">2503.04322v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Wideband User Scheduling and Hybrid Precoding with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.04233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图神经网络（GNN）的架构，用于联合学习宽带多用户多天线系统中的空间频域调度和混合预编码策略。这种架构能够有效地解决资源分配优化问题，并且具有良好的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在宽带多用户多天线系统中，由于用户的组合数量庞大以及各个资源块（RB）之间共享模拟预编码器，空间频域调度与混合预编码的学习尚未被联合研究。因此，在这类系统中有效利用资源面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于GNN的架构来同时学习宽带多用户多天线系统的调度和预编码策略，并探索如何改进这种架构以提高其在不同规模问题中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过重新表述联合优化问题为一个等价的功能性优化问题，设计了一个由两个级联模块组成的GNN架构。此外，针对无线策略定义的集合发现了同参数同决策（SPSD）属性，并据此改进了调度模块的序列GNN结构；同时在预编码模块中开发了一种新的注意力机制。&lt;h4&gt;主要发现&lt;/h4&gt;1. 发现了一种与无线策略相关的同参数同决策（SPSD）属性。                2. 揭示出当用户具有相似信道时，单一GNN难以学习最优调度政策。                 3. 当线性聚合器阻碍大小泛化时，在预编码模块中开发了新的注意力机制来改进信息聚合。&lt;h4&gt;结论&lt;/h4&gt;所提出的架构在短推理时间和低训练复杂度下实现了令人满意的频谱效率，并且对于基站和用户的不同数量、RB数以及天线数具有良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial-frequency scheduling and hybrid precoding in wideband multi-usermulti-antenna systems have never been learned jointly due to the challengesarising from the massive user combinations on resource blocks (RBs) and theshared analog precoder among RBs. In this paper, we strive to jointly learn thescheduling and precoding policies with graph neural networks (GNNs), which haveemerged as a powerful tool for optimizing resource allocation thanks to theirpotential in generalizing across problem scales. By reformulating the jointoptimization problem into an equivalent functional optimization problem for thescheduling and precoding policies, we propose a GNN-based architectureconsisting of two cascaded modules to learn the two policies. We discover asame-parameter same-decision (SPSD) property for wireless policies defined onsets, revealing that a GNN cannot well learn the optimal scheduling policy whenusers have similar channels. This motivates us to develop a sequence of GNNs toenhance the scheduling module. Furthermore, by analyzing the SPSD property, wefind when linear aggregators in GNNs impede size generalization. Based on theobservation, we devise a novel attention mechanism for information aggregationin the precoder module. Simulation results demonstrate that the proposedarchitecture achieves satisfactory spectral efficiency with short inferencetime and low training complexity, and is generalizable to the numbers of users,RBs, and antennas at the base station and users.</description>
      <author>example@mail.com (Shengjie Liu, Chenyang Yang, Shengqian Han)</author>
      <guid isPermaLink="false">2503.04233v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Large Scale Point Cloud Completion for Archaeological Site Restoration</title>
      <link>http://arxiv.org/abs/2503.04030v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种用于恢复大型点云的新颖方法，利用有限且分布不平衡的真实地面标注信息。&lt;h4&gt;背景&lt;/h4&gt;现有的自监督方法在处理具有大面积缺失表面和点分布不均衡的大型物体时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新方法来解决现有自监督方法在修复大型、部分遮挡点云时的不足，特别是针对考古结构等复杂场景。&lt;h4&gt;方法&lt;/h4&gt;使用粗糙边界标注对感兴趣区域进行处理，并将原始点云投影到多中心投影(MCOP)图像中。MCOP图像是五通道（RGB、深度和旋转）图像。该过程转化为在这些MCOP图像上填补缺失像素，最终映射回3D空间以完成结构重建。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种自监督方案来学习在存在结构缺失的情况下填充MCOP图像，并应用特殊损失函数进一步增强生成点云的规则性和一致性。&lt;h4&gt;结论&lt;/h4&gt;实验表明该方法对于恢复600多个不完整且分布不平衡的考古结构（位于秘鲁）具有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;点云完成有助于修复部分遮挡的点云。现有自监督方法在处理大面积缺失表面和点分布不平衡的情况下无法提供高质量的重建结果。本论文提出了一种新方法，利用粗糙边界标注来恢复大型且不完整的结构，在秘鲁600多个考古遗址中取得显著效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion helps restore partial incomplete point cloudssuffering occlusions. Current self-supervised methods fail to give highfidelity completion for large objects with missing surfaces and unbalanceddistribution of available points. In this paper, we present a novel method forrestoring large-scale point clouds with limited and imbalanced ground-truth.Using rough boundary annotations for a region of interest, we project theoriginal point clouds into a multiple-center-of-projection (MCOP) image, wherefragments are projected to images of 5 channels (RGB, depth, and rotation).Completion of the original point cloud is reduced to inpainting the missingpixels in the MCOP images. Due to lack of complete structures and an unbalanceddistribution of existing parts, we develop a self-supervised scheme whichlearns to infill the MCOP image with points resembling existing "complete"patches. Special losses are applied to further enhance the regularity andconsistency of completed MCOP images, which is mapped back to 3D to form finalrestoration. Extensive experiments demonstrate the superiority of our method incompleting 600+ incomplete and unbalanced archaeological structures in Peru.</description>
      <author>example@mail.com (Aocheng Li, James R. Zimmer-Dauphinee, Rajesh Kalyanam, Ian Lindsay, Parker VanValkenburgh, Steven Wernke, Daniel Aliaga)</author>
      <guid isPermaLink="false">2503.04030v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Transferable Foundation Models for Geometric Tasks on Point Cloud Representations: Geometric Neural Operators</title>
      <link>http://arxiv.org/abs/2503.04649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了获取预训练的几何神经算子（GNPs）的方法，这些算子可以作为基础模型用于获得几何特征，并在数据处理管道中用于机器学习任务和数值方法。&lt;h4&gt;背景&lt;/h4&gt;现有的许多任务需要对点云进行几何分析，以提取度量、曲率等形状相关的特性。这需要一种能够稳健地从复杂且可能带有噪声的数据集中学习这些特性的模型。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的基于神经网络的方法来预训练GNPs，以便更有效地处理和理解各种类型的点云数据。&lt;h4&gt;方法&lt;/h4&gt;通过训练GNPs以学习点云的微分几何特征，从而提供度量、曲率和其他形状相关特征的估计。此外，展示如何使用预先训练好的GNPs进行任务如噪声环境下任意形状和拓扑表面几何属性的估算、流形上几何偏微分方程的近似解以及形状变形方程（例如由曲率驱动的流动）的求解。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的GNPs能够在具有挑战性的条件下（包括存在噪声的情况）准确估计点云数据中的关键几何属性，并且可以被集成到现有的和新的数据处理管道中，从而提高效率和准确性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过使用预先训练好的GNPs，可以在各种任务中实现对点云数据的高效和有效的几何分析。这些模型在实际应用中有广泛的应用潜力，例如计算机视觉、机器人技术以及科学计算等领域。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一种方法来获得预训练的几何神经算子（GNPs），它们可以作为基础模型用于获取几何特征，并可被集成到数据处理管道中以服务于机器学习任务和数值方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce methods for obtaining pretrained Geometric Neural Operators(GNPs) that can serve as basal foundation models for use in obtaining geometricfeatures. These can be used within data processing pipelines for machinelearning tasks and numerical methods. We show how our GNPs can be trained tolearn robust latent representations for the differential geometry ofpoint-clouds to provide estimates of metric, curvature, and other shape-relatedfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimatethe geometric properties of surfaces of arbitrary shape and topologies withrobustness in the presence of noise, (ii) to approximate solutions of geometricpartial differential equations (PDEs) on manifolds, and (iii) to solveequations for shape deformations such as curvature driven flows. We alsorelease a package of the codes and weights for using our pre-trained GNPs forprocessing point cloud representations. This allows for incorporating ourpre-trained GNPs as components for reuse within existing and new dataprocessing pipelines. The GNPs also can be used as part of numerical solversinvolving geometry or as part of methods for performing inference and othergeometric tasks.</description>
      <author>example@mail.com (Blaine Quackenbush, Paul J. Atzberger)</author>
      <guid isPermaLink="false">2503.04649v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Diff-Reg v2: Diffusion-Based Matching Matrix Estimation for Image Matching and 3D Registration</title>
      <link>http://arxiv.org/abs/2503.04127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2403.19919&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的匹配矩阵估计方法，利用扩散模型在矩阵空间中的去噪过程来解决图像和点云配准任务中遇到的挑战。&lt;h4&gt;背景&lt;/h4&gt;建立可靠的对应关系对于2D图像配准、3D点云配准以及2D-3D图像到点云配准等注册任务至关重要。然而，这些任务常常面临诸如尺度不一致、对称性和大形变等问题，这些问题会导致匹配模糊。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服传统特征和对应关系方法在复杂场景中的局限性，尤其是在解决局部最小值问题方面。&lt;h4&gt;方法&lt;/h4&gt;该论文引入了一种基于扩散模型的范式，在矩阵空间中进行稳健的匹配矩阵估计。具体而言，它将3D-3D和2D-3D配准任务部署在双随机矩阵空间中，并为2D图像注册任务在应用双重softmax投影正则化的子空间中部署扩散模型。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了提出的基于扩散模型的方法能够有效地处理复杂场景中的匹配问题，优于其他传统方法。同时，该研究还展示了轻量级的去噪模块设计的有效性，并且为不同注册任务提供了自适应匹配矩阵嵌入实现。&lt;h4&gt;结论&lt;/h4&gt;提出的新范式为解决图像和点云配准中的挑战提供了一种有效的方法，能够处理各种复杂场景下的问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Establishing reliable correspondences is crucial for all registration tasks,including 2D image registration, 3D point cloud registration, and 2D-3Dimage-to-point cloud registration. However, these tasks are often complicatedby challenges such as scale inconsistencies, symmetry, and large deformations,which can lead to ambiguous matches. Previous feature-based andcorrespondence-based methods typically rely on geometric or semantic featuresto generate or polish initial potential correspondences. Some methods typicallyleverage specific geometric priors, such as topological preservation, to devisediverse and innovative strategies tailored to a given enhancement goal, whichcannot be exhaustively enumerated. Additionally, many previous approaches relyon a single-step prediction head, which can struggle with local minima incomplex matching scenarios. To address these challenges, we introduce aninnovative paradigm that leverages a diffusion model in matrix space for robustmatching matrix estimation. Our model treats correspondence estimation as adenoising diffusion process in the matching matrix space, gradually refiningthe intermediate matching matrix to the optimal one. Specifically, we apply thediffusion model in the doubly stochastic matrix space for 3D-3D and 2D-3Dregistration tasks. In the 2D image registration task, we deploy the diffusionmodel in a matrix subspace where dual-softmax projection regularization isapplied. For all three registration tasks, we provide adaptive matching matrixembedding implementations tailored to the specific characteristics of each taskwhile maintaining a consistent "match-to-warp" encoding pattern. Furthermore,we adopt a lightweight design for the denoising module. In inference, oncepoints or image features are extracted and fixed, this module performsmulti-step denoising predictions through reverse sampling.</description>
      <author>example@mail.com (Qianliang Wu, Haobo Jiang, Yaqing Ding, Lei Luo, Jin Xie, Jian Yang)</author>
      <guid isPermaLink="false">2503.04127v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning 3D Medical Image Models From Brain Functional Connectivity Network Supervision For Mental Disorder Diagnosis</title>
      <link>http://arxiv.org/abs/2503.04205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架CINP，用于通过对比学习将结构MRI与功能性连接网络相结合以提高精神疾病诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;大多数关于基于MRI的精神疾病的先前研究主要集中在功能连接网络上。然而，由于标注的功能性磁共振成像数据集较小，这限制了它的广泛应用。而常见的3D T1加权MRI（结构MRI）在临床环境中广泛使用且易于获取，但这些方法经常被忽视。&lt;h4&gt;目的&lt;/h4&gt;通过结合功能性与结构性的互补信息来改进诊断准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种框架CINP，该框架利用对比学习技术，在预训练过程中融合了图像掩码建模和网络-图像匹配以增强视觉表示学习和模式对齐。&lt;h4&gt;主要发现&lt;/h4&gt;CINP能够促进从功能性连接网络到结构MRI的知识转移。另外，通过仅使用疑似患者的结构MRI以及少量来自不同患者类别的功能性连接网络，可以进行精神疾病的诊断，这种做法在现实世界的临床环境中是可行的。&lt;h4&gt;结论&lt;/h4&gt;在三个精神疾病诊断任务上的竞争性能表明了CINP框架在整合多模态MRI信息的有效性，并且通过网络提示将结构MRI纳入临床诊断中具有潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;在基于磁共振成像的精神障碍诊断领域，以往的研究大多集中在由功能性磁共振成像（fMRI）得出的功能连接网络（FCN）。然而，由于标注的fMRI数据集较小，这限制了它的广泛应用。同时，在临床环境中广泛使用且易于获取的结构MRIs（如3D T1加权MRI），常常被忽略。为了整合来自功能和结构两方面的互补信息以提高诊断准确性，我们提出了一种框架CINP，该框架采用对比学习方法在sMRI与FCN之间进行对比学习。在预训练阶段中，我们将掩码图像建模和网络-图像匹配集成进来，以增强视觉表示学习和模式对齐。由于CINP能够促进从功能连接网络到结构MRI的知识转移，我们引入了网络提示技术：它仅使用疑似患者的sMRI以及少量来自不同患者类别的FCN来进行精神障碍的诊断，在现实世界的临床环境中是实际可行的。在三个精神疾病诊断任务上的竞争性能表明了CINP框架的有效性，不仅在于整合多模态MRI信息上，而且在于通过网络提示技术将结构MRI融入到临床诊断中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In MRI-based mental disorder diagnosis, most previous studies focus onfunctional connectivity network (FCN) derived from functional MRI (fMRI).However, the small size of annotated fMRI datasets restricts its wideapplication. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w)MRI, which are commonly used and readily accessible in clinical settings, areoften overlooked. To integrate the complementary information from both functionand structure for improved diagnostic accuracy, we propose CINP (ContrastiveImage-Network Pre-training), a framework that employs contrastive learningbetween sMRI and FCN. During pre-training, we incorporate masked image modelingand network-image matching to enhance visual representation learning andmodality alignment. Since the CINP facilitates knowledge transfer from FCN tosMRI, we introduce network prompting. It utilizes only sMRI from suspectedpatients and a small amount of FCNs from different patient classes fordiagnosing mental disorders, which is practical in real-world clinicalscenario. The competitive performance on three mental disorder diagnosis tasksdemonstrate the effectiveness of the CINP in integrating multimodal MRIinformation, as well as the potential of incorporating sMRI into clinicaldiagnosis using network prompting.</description>
      <author>example@mail.com (Xingcan Hu, Wei Wang, Li Xiao)</author>
      <guid isPermaLink="false">2503.04205v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>The JARVIS Infrastructure is All You Need for Materials Design</title>
      <link>http://arxiv.org/abs/2503.04133v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;JARVIS是一个为多尺度、多模态的正向和逆向材料设计提供数据库、工具、教程及基准测试的综合基础设施。&lt;h4&gt;背景信息&lt;/h4&gt;强调开放访问原则和可重复性，集成理论与实验方法如密度泛函理论、量子蒙特卡洛、紧束缚法、经典力场以及机器学习方法包括指纹识别技术、图神经网络和变换模型。&lt;h4&gt;目的陈述&lt;/h4&gt;通过统一的方法和数据集在单一平台上进行整合，以促进材料设计领域的基础发现及实际创新。&lt;h4&gt;研究方法&lt;/h4&gt;收集实验数据涵盖低温学、显微术和衍射，涉及多种材料如金属、半导体等。JARVIS还通过开放数据集、网络应用、可执行脚本以及同行评审的出版物来分发资源。&lt;h4&gt;主要发现&lt;/h4&gt;全球广泛采用，促使了数百万次的数据与工具下载。&lt;h4&gt;结论陈述&lt;/h4&gt;该平台不仅推动基础科学进步也促进了基于数据的方法在材料设计中的实际创新。&lt;h4&gt;涉及材料种类&lt;/h4&gt;包括但不限于金属、半导体、绝缘体、超导体、碳捕获系统、高强度化合物及低维材料、异质结构和缺陷等。&lt;h4&gt;开放性原则&lt;/h4&gt;强调所有资源的开放访问，确保广泛获取性和可重复性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Joint Automated Repository for Various Integrated Simulations (JARVIS) is acomprehensive infrastructure offering databases, tools, tutorials, andbenchmarks for multiscale, multimodal, forward, and inverse materials design.Emphasizing open access principles and reproducibility, it integratestheoretical and experimental methodologies such as density functional theory,quantum Monte Carlo, tight-binding, classical force fields, andmachine-learning approaches-including fingerprinting, graph neural networks,and transformer models. Its experimental data collection spans cryogenics,microscopy, and diffraction, covering materials like metals, semiconductors,insulators, superconductors, carbon capture systems, high-strength compounds,and low-dimensional materials, heterostructures and defects. JARVISdisseminates resources via open datasets, web applications, executable scripts,and peer-reviewed publications, ensuring broad accessibility andreproducibility. Widely adopted worldwide, it has facilitated millions of dataand tool downloads. By unifying diverse methods and data under one platform,JARVIS drives both fundamental discoveries and real-world innovations,advancing conventional and data-driven materials design.</description>
      <author>example@mail.com (Kamal Choudhary)</author>
      <guid isPermaLink="false">2503.04133v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Disorder: Unveiling Cooperativeness in Multidirectional Associative Memories</title>
      <link>http://arxiv.org/abs/2503.04454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过引入统计力学工具来扩展用于异构关联记忆的神经网络架构（称为三向关联存储器TAM），探索监督和非监督学习协议。&lt;h4&gt;背景&lt;/h4&gt;当前对于复杂系统中的统计力学工具未充分利用，特别是在神经网络体系结构的应用方面。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的现象——层协同性，并研究不同层次数据集的熵异质化如何影响记忆检索能力。&lt;h4&gt;方法&lt;/h4&gt;通过向神经网络的不同层级提供具有不同信息量的数据集来探索这种新兴的现象。&lt;h4&gt;主要发现&lt;/h4&gt;观察到训练中使用较少信息数据集的层与那些经历更多信息数据集的层在最终的记忆检索区域内达到相同的幅度，表明了跨层次熵相互作用可以增强整体记忆检索能力。&lt;h4&gt;结论&lt;/h4&gt;提出的协同动力学现象对于理解无序系统中的计算潜力具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;通过利用复杂系统的统计力学工具，本文扩展了一种用于异构关联记忆的神经网络架构（称为三向关联存储器TAM），以探索监督学习和非监督学习协议。研究揭示了数据集熵在不同层次之间的相互作用可以增强这些层次的记忆检索能力，并提出了一个新现象——层协同性，这标志着对于无序系统中计算潜力的理解取得了重大进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By leveraging tools from the statistical mechanics of complex systems, inthese short notes we extend the architecture of a neural network forhetero-associative memory (called three-directional associative memories, TAM)to explore supervised and unsupervised learning protocols. In particular, byproviding entropic-heterogeneous datasets to its various layers, we predict andquantify a new emergent phenomenon -- that we term {\em layer'scooperativeness} -- where the interplay of dataset entropies across network'slayers enhances their retrieval capabilities Beyond those they would havewithout reciprocal influence. Naively we would expect layers trained with lessinformative datasets to develop smaller retrieval regions compared to thosepertaining to layers that experienced more information: this does not happenand all the retrieval regions settle to the same amplitude, allowing foroptimal retrieval performance globally. This cooperative dynamics marks asignificant advancement in understanding emergent computational capabilitieswithin disordered systems.</description>
      <author>example@mail.com (Andrea Alessandrelli, Adriano Barra, Andrea Ladiana, Andrea Lepre, Federico Ricci-Tersenghi)</author>
      <guid isPermaLink="false">2503.04454v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Spatial regularisation for improved accuracy and interpretability in keypoint-based registration</title>
      <link>http://arxiv.org/abs/2503.04499v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种用于改进无监督图像配准中关键点检测的三重损失函数，该方法通过正则化特征的空间分布来提高关键点的解释性和准确性。&lt;h4&gt;背景&lt;/h4&gt;现有的基于无监督关键点检测的方法在实现可解释性方面显示出潜力，但是提取到的关键点往往具有模糊且难以解释的空间模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的三重损失函数来改善这些方法中特征向量的空间分布，提高其空间准确性和解剖学相关性。&lt;h4&gt;方法&lt;/h4&gt;引入了基于KL散度的正则化项，通过将特征视为概率关键点并优化其空间分布，同时加入了排斥损失以鼓励各关键点之间的空间多样性。&lt;h4&gt;主要发现&lt;/h4&gt;应用该三重损失函数在胎儿刚体运动追踪和脑MRI仿射配准任务上均取得了优异表现，并且与现有监督方法的性能差距缩小。&lt;h4&gt;结论&lt;/h4&gt;所提出的正则化策略显著提高了无监督图像配准中关键点检测的质量，使其更加适用于实际医学影像分析任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要讨论了一种用于改进无监督图像配准过程中特征图空间分布的新颖三重损失函数。该方法通过将网络输出的特征视为概率性关键点，并通过优化其空间模式以提高解剖学意义的显著性和精确度，进而增强整体注册过程的可解释性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised registration strategies bypass requirements in ground truthtransforms or segmentations by optimising similarity metrics between fixed andmoved volumes. Among these methods, a recent subclass of approaches based onunsupervised keypoint detection stand out as very promising forinterpretability. Specifically, these methods train a network to predictfeature maps for fixed and moving images, from which explainable centres ofmass are computed to obtain point clouds, that are then aligned in closed-form.However, the features returned by the network often yield spatially diffusepatterns that are hard to interpret, thus undermining the purpose ofkeypoint-based registration. Here, we propose a three-fold loss to regularisethe spatial distribution of the features. First, we use the KL divergence tomodel features as point spread functions that we interpret as probabilistickeypoints. Then, we sharpen the spatial distributions of these features toincrease the precision of the detected landmarks. Finally, we introduce a newrepulsive loss across keypoints to encourage spatial diversity. Overall, ourloss considerably improves the interpretability of the features, which nowcorrespond to precise and anatomically meaningful landmarks. We demonstrate ourthree-fold loss in foetal rigid motion tracking and brain MRI affineregistration tasks, where it not only outperforms state-of-the-art unsupervisedstrategies, but also bridges the gap with state-of-the-art supervised methods.Our code is available at https://github.com/BenBillot/spatial_regularisation.</description>
      <author>example@mail.com (Benjamin Billot, Ramya Muthukrishnan, Esra Abaci-Turk, Ellen P. Grant, Nicholas Ayache, Hervé Delingette, Polina Golland)</author>
      <guid isPermaLink="false">2503.04499v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>UniNet: A Unified Multi-granular Traffic Modeling Framework for Network Security</title>
      <link>http://arxiv.org/abs/2503.04174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 6 figures,15 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现代网络随着各种设备、加密协议和不断变化的安全威胁而变得日益复杂，使得网络流量分析变得至关重要。现有的机器学习模型通常依赖单一的数据表示形式（如包或流），这限制了它们捕捉对全面安全分析至关重要的上下文关系的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种统一框架UniNet以解决现有方法的局限性，并为现代网络安全设定新的基准。&lt;h4&gt;方法&lt;/h4&gt;UniNet引入了一种新颖的多粒度流量表示（T-Matrix）和一个轻量级注意力模型T-Attent，结合会话、流和包级别的特性提供全面的上下文信息。此外，它能高效学习多种安全任务的潜在嵌入，并且能够适应不同数据格式的安全任务。&lt;h4&gt;主要发现&lt;/h4&gt;UniNet在异常检测、攻击分类、物联网设备识别和加密网站指纹鉴定等四个关键网络安全问题上都表现出显著性能提升，优于当前最先进的方法，具有更高的准确性、更低的误报率以及更好的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;通过解决单一层面模型的局限性和统一网络安全分析范式，UniNet为现代网络安全设定了新的标准。&lt;h4&gt;翻译&lt;/h4&gt;随着网络变得越来越复杂，传统的基于单个数据表示形式的方法已经不能满足需求。为此，研究人员提出了UniNet框架来应对这些挑战，并且证明了其在多个关键安全任务中的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As modern networks grow increasingly complex--driven by diverse devices,encrypted protocols, and evolving threats--network traffic analysis has becomecritically important. Existing machine learning models often rely only on asingle representation of packets or flows, limiting their ability to capturethe contextual relationships essential for robust analysis. Furthermore,task-specific architectures for supervised, semi-supervised, and unsupervisedlearning lead to inefficiencies in adapting to varying data formats andsecurity tasks. To address these gaps, we propose UniNet, a unified frameworkthat introduces a novel multi-granular traffic representation (T-Matrix),integrating session, flow, and packet-level features to provide comprehensivecontextual information. Combined with T-Attent, a lightweight attention-basedmodel, UniNet efficiently learns latent embeddings for diverse security tasks.Extensive evaluations across four key network security and privacyproblems--anomaly detection, attack classification, IoT device identification,and encrypted website fingerprinting--demonstrate UniNet's significantperformance gain over state-of-the-art methods, achieving higher accuracy,lower false positive rates, and improved scalability. By addressing thelimitations of single-level models and unifying traffic analysis paradigms,UniNet sets a new benchmark for modern network security.</description>
      <author>example@mail.com (Binghui Wu, Dinil Mon Divakaran, Mohan Gurusamy)</author>
      <guid isPermaLink="false">2503.04174v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>WeakSupCon: Weakly Supervised Contrastive Learning for Encoder Pre-training</title>
      <link>http://arxiv.org/abs/2503.04165v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种新的编码器预训练方法Weakly Supervised Contrastive Learning (WeakSupCon)，用于下游的多实例学习任务，该方法利用了包级标签。', '背景': '弱监督下的多实例学习（MIL）是一个具有挑战性的任务，因为只有包级别标签可用，而每个包通常包含多个实例。在组织病理图像分析中广泛研究此问题，其中标签仅提供于整个滑动片水平，但可将每张滑动片分割成数千个小型图像块进行训练。', '目的': '克服自监督编码器预训练过程中的领域迁移问题，并提高MIL任务的分类性能。', '方法': '通过利用包级标签定义多任务学习和区分不同包标签样本的对比学习损失，实现弱监督下的对比学习。', '主要发现': '实验表明，在三个数据集上使用WeakSupCon生成的特征与自监督方法相比显著提高了MIL分类性能。', '结论': '提出的Weakly Supervised Contrastive Learning (WeakSupCon) 方法可以有效解决当前编码器预训练在MIL任务上的局限性，提高分类精度。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了弱监督下的多实例学习（MIL）的研究背景和挑战，并提出了一种新的方法Weakly Supervised Contrastive Learning (WeakSupCon)，旨在改进现有的自监督编码器预训练技术，通过利用包级标签改善特征的生成质量，从而提升下游MIL任务中的分类性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised multiple instance learning (MIL) is a challenging taskgiven that only bag-level labels are provided, while each bag typicallycontains multiple instances. This topic has been extensively studied inhistopathological image analysis, where labels are usually available only atthe whole slide image (WSI) level, while each whole slide image can be dividedinto thousands of small image patches for training. The dominant MIL approachestake fixed patch features as inputs to address computational constraints andensure model stability. These features are commonly generated by encoderspre-trained on ImageNet, foundation encoders pre-trained on large datasets, orthrough self-supervised learning on local datasets. While the self-supervisedencoder pre-training on the same dataset as downstream MIL tasks helps mitigatedomain shift and generate better features, the bag-level labels are notutilized during the process, and the features of patches from differentcategories may cluster together, reducing classification performance on MILtasks. Recently, pre-training with supervised contrastive learning (SupCon) hasdemonstrated superior performance compared to self-supervised contrastivelearning and even end-to-end training on traditional image classificationtasks. In this paper, we propose a novel encoder pre-training method fordownstream MIL tasks called Weakly Supervised Contrastive Learning (WeakSupCon)that utilizes bag-level labels. In our method, we employ multi-task learningand define distinct contrastive learning losses for samples with different baglabels. Our experiments demonstrate that the features generated usingWeakSupCon significantly enhance MIL classification performance compared toself-supervised approaches across three datasets.</description>
      <author>example@mail.com (Bodong Zhang, Hamid Manoochehri, Beatrice S. Knudsen, Tolga Tasdizen)</author>
      <guid isPermaLink="false">2503.04165v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person Retrieval</title>
      <link>http://arxiv.org/abs/2503.04144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, accepted by AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;文本基础的人脸检索任务得到了广泛关注，由于其与实际应用紧密相关且具有挑战性。针对该领域中的视觉语言预训练知识的应用，提出了将CLIP模型适应于人脸领域的研究方向。&lt;h4&gt;背景&lt;/h4&gt;基于文本的人脸识别（TPR）作为一项细粒度和具有挑战性的任务，在实践中得到广泛应用。利用视觉语言预训练的知识来定制CLIP应用于人脸识别是一个新兴的研究领域。&lt;h4&gt;目的&lt;/h4&gt;解决在TPR中全模型微调计算成本高且易过拟合的问题，以及现有的参数高效迁移学习缺乏对细粒度特征提取的支持问题。&lt;h4&gt;方法&lt;/h4&gt;提出了域感知混合适配器（DM-Adapter）方法，该方法结合了专家混合（MOE）和参数效率的迁移学习技术，以增强细粒度特征表示并保持计算效率。具体而言，在视觉和语言分支中的MLP层旁设计了稀疏混合适配器。&lt;h4&gt;主要发现&lt;/h4&gt;通过建立新的门控函数以及注入可训练的域感知提示符来发展域感知路由机制，促进路由器有效利用领域信息，并缓解路由不平衡问题。实验表明DM-Adapter取得了最先进的性能，显著优于先前的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的新方法能够有效地处理TPR任务中的挑战，在多个基准测试中表现出优越性。&lt;h4&gt;翻译&lt;/h4&gt;文本基础的人脸识别（Text-based Person Retrieval, TPR）已经成为一个精细粒度且具有挑战性的研究领域，并且在实际应用中有紧密的联系。由于视觉语言预训练模型CLIP拥有丰富的知识，将该模型应用于人脸识别的研究方向正在兴起。然而，在TPR任务中进行微调时仍存在一些问题：一是全模型微调的成本高昂并且容易过拟合；二是现有的参数效率迁移学习方法在细粒度特征提取方面仍有不足。为了克服这些问题，我们提出了一种名为域感知混合适配器（Domain-Aware Mixture-of-Adapters, DM-Adapter）的方法。该方法结合了专家混合技术和参数高效迁移学习技术，以增强细粒度特征表示的同时保持计算效率。具体来说，在视觉和语言分支的MLP层旁设计了稀疏混合适配器，并针对不同的专家来处理不同的人脸知识方面，从而更精细地处理特征。为促进路由器有效地利用领域信息并缓解路由不平衡问题，我们开发了一种域感知路由机制，通过建立新的门控函数以及注入可训练的域感知提示符实现。广泛的实验表明，我们的DM-Adapter方法实现了最先进的性能，并且明显优于以前的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-based person retrieval (TPR) has gained significant attention as afine-grained and challenging task that closely aligns with practicalapplications. Tailoring CLIP to person domain is now a emerging research topicdue to the abundant knowledge of vision-language pretraining, but challengesstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR iscomputationally expensive and prone to overfitting.(ii) Existingparameter-efficient transfer learning (PETL) for TPR lacks of fine-grainedfeature extraction. To address these issues, we propose Domain-AwareMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) andPETL to enhance fine-grained feature representations while maintainingefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel toMLP layers in both vision and language branches, where different expertsspecialize in distinct aspects of person knowledge to handle features morefinely. To promote the router to exploit domain information effectively andalleviate the routing imbalance, Domain-Aware Router is then developed bybuilding a novel gating function and injecting learnable domain-aware prompts.Extensive experiments show that our DM-Adapter achieves state-of-the-artperformance, outperforming previous methods by a significant margin.</description>
      <author>example@mail.com (Yating Liu, Zimo Liu, Xiangyuan Lan, Wenming Yang, Yaowei Li, Qingmin Liao)</author>
      <guid isPermaLink="false">2503.04144v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>On the Acquisition of Shared Grammatical Representations in Bilingual Language Models</title>
      <link>http://arxiv.org/abs/2503.03962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文研究了单一语言模型在训练第二语言时的变化，探讨跨语际转移学习中共享的多语言表示形式。通过控制每种语言的数据量和顺序来训练双语小规模模型，并利用结构启动（structural priming）这一方法寻找证据。&lt;h4&gt;背景&lt;/h4&gt;当前的语言模型在多种语言的能力方面依赖于跨语际转移学习，但这种转移发生的机制尚不清晰。&lt;h4&gt;目的&lt;/h4&gt;探究单语语言模型在其开始被另一种语言训练时会发生什么变化。&lt;h4&gt;方法&lt;/h4&gt;控制每种语言的数据量和顺序来训练双语小规模模型，并使用结构启动这一方法研究语法表示形式。首先复制先前的跨语言结构启动结果，然后调整数据量和语言接触模式以观察影响。&lt;h4&gt;主要发现&lt;/h4&gt;在控制了训练数据的数量和语言暴露后，不同语言对之间以及方向上的效果是不对称的；并且对于语系差异较大的语言组合来说，这种交叉语言传输学习和共享表示形式的效果并不稳定且不太可靠。&lt;h4&gt;结论&lt;/h4&gt;研究表明跨语言转移中的不对称性可能会影响关于人类结构启动效应的假设，同时也展示了多语言模型训练中存在的一些潜在限制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While crosslingual transfer is crucial to contemporary language models'multilingual capabilities, how it occurs is not well understood. In this paper,we ask what happens to a monolingual language model when it begins to betrained on a second language. Specifically, we train small bilingual models forwhich we control the amount of data for each language and the order of languageexposure. To find evidence of shared multilingual representations, we turn tostructural priming, a method used to study grammatical representations inhumans. We first replicate previous crosslingual structural priming results andfind that after controlling for training data quantity and language exposure,there are asymmetrical effects across language pairs and directions. We arguethat this asymmetry may shape hypotheses about human structural primingeffects. We also find that structural priming effects are less robust for lesssimilar language pairs, highlighting potential limitations of crosslingualtransfer learning and shared representations for typologically diverselanguages.</description>
      <author>example@mail.com (Catherine Arnett, Tyler A. Chang, James A. Michaelov, Benjamin K. Bergen)</author>
      <guid isPermaLink="false">2503.03962v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical quantum embedding by machine learning for large molecular assemblies</title>
      <link>http://arxiv.org/abs/2503.03928v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  28 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在量子-经典混合模型中，相关结构区域（如反应中心或宿主分子结合口袋）可以由量子模型描述，并嵌入到经典的分子力学环境中。然而，当量子区域变得非常大时，仅能使用近似的电子结构模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种量子内量子嵌入策略并结合机器学习势能来提高大型分子量子-经典混合模型的准确性。&lt;h4&gt;方法&lt;/h4&gt;在量子区域内引入量子核心概念，这些核心因为大小有限而可以采用精确的电子结构模型。例如，Huzinaga型投影基态嵌入技术能够提供准确的电子能量，进而通过转移学习方法提升机器学习势能的精度。&lt;h4&gt;主要发现&lt;/h4&gt;该策略提高了结合自由能计算中的量子描述准确性，特别是对于蛋白质-配体复合物的精确度有显著改进。&lt;h4&gt;结论&lt;/h4&gt;利用此策略可以有效地提高大型分子中特定区域的量子模拟精度，并通过机器学习方法优化整个系统的势能模型。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种量子内嵌入策略结合机器学习势能，以改善大规模分子量子-经典混合模型的准确性。在这样的混合模型中，相关结构区域（例如反应中心或宿主分子结合口袋）可以通过一个量子模型进行描述，并嵌入到经典的分子力学环境中。然而，当这个量子区域变得太大时，只有近似的电子结构模型适用。为了恢复量子描述中的精度，我们在该区域内引入了可以采用精确电子结构模型的量子核心概念，由于它们尺寸较小。例如，Huzinaga型投影基态嵌入技术可以提供准确的电子能量，这是通过先进的电子结构方法获得的。然后，这些总电子能量被输入到一个转移学习方法中，高效地利用更高精度的数据来改进原始量子-经典混合方法所得到的机器学习势能。我们在此策略下探讨了一种经过充分研究的蛋白质-配体复合物中的结合自由能计算能力，使用了化学计量自由能和非平衡切换模拟技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a quantum-in-quantum embedding strategy coupled to machinelearning potentials to improve on the accuracy of quantum-classical hybridmodels for the description of large molecules. In such hybrid models, relevantstructural regions (such as those around reaction centers or pockets forbinding of host molecules) can be described by a quantum model that is thenembedded into a classical molecular-mechanics environment. However, thisquantum region may become so large that only approximate electronic structuremodels are applicable. To then restore accuracy in the quantum description, wehere introduce the concept of quantum cores within the quantum region that areamenable to accurate electronic structure models due to their limited size.Huzinaga-type projection-based embedding, for example, can deliver accurateelectronic energies obtained with advanced electronic structure methods. Theresulting total electronic energies are then fed into a transfer learningapproach that efficiently exploits the higher-accuracy data to improve on amachine learning potential obtained for the original quantum-classical hybridapproach. We explore the potential of this approach in the context of awell-studied protein-ligand complex for which we calculate the free energy ofbinding using alchemical free energy and non-equilibrium switching simulations.</description>
      <author>example@mail.com (Moritz Bensberg, Marco Eckhoff, Raphael T. Husistein, Matthew S. Teynor, Valentina Sora, William Bro-Jørgensen, F. Emil Thomasen, Anders Krogh, Kresten Lindorff-Larsen, Gemma C. Solomon, Thomas Weymuth, Markus Reiher)</author>
      <guid isPermaLink="false">2503.03928v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV Density Images</title>
      <link>http://arxiv.org/abs/2503.04475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于LiDAR的自然森林环境下的地点识别方法ForestLPR，该方法利用不同高度的横截面图像来实现位置再认。&lt;h4&gt;背景&lt;/h4&gt;城市环境中基于激光雷达或相机的地方识别技术已有显著进展，但天然森林等自然环境中的应用尚待深入研究。这些环境具有高自我相似性和随时间变化而产生的植被变化。&lt;h4&gt;目的&lt;/h4&gt;提出一种在自然森林中鲁棒的LiDAR基地方识别方法ForestLPR。&lt;h4&gt;方法&lt;/h4&gt;通过激光雷达点云的不同高度水平切片生成贝叶斯等效视图密度图像作为横截面图像，采用视觉变换器作为共享主干网络以产生局部描述符集合，并引入多BEV交互模块根据不同高度自适应地关注信息。最终聚合层输出一个旋转不变的位置描述符。&lt;h4&gt;主要发现&lt;/h4&gt;在公开基准测试和机器人数据集上的真实世界数据上进行了广泛评估，结果显示ForestLPR在所有评价中的表现优异，在序列内回环闭合检测和序列间重新定位中分别比最近的竞争对手提高了7.38%和9.11%&lt;h4&gt;结论&lt;/h4&gt;实验验证了提出的基于横截面图像的方法对于自然森林环境下的地方识别是有效的。&lt;h4&gt;翻译&lt;/h4&gt;地点识别对大规模定位系统保持全局一致性至关重要。尽管城市环境中使用激光雷达或相机的研究取得了显著进展，但在类似天然森林的自然环境中的应用仍然很大程度上未被探索。此外，由于高度的自我相似性和随时间变化产生的植被变化，森林提出了特殊挑战。在本研究中，我们提出了一种用于自然森林的鲁棒LiDAR基地方识别方法ForestLPR。我们的假设是，在不同高度处生成的森林几何学横截面图像包含重新访问地点所需的信息。这些横截面图像是通过激光雷达点云的不同高度水平切片表示为贝叶斯等效视图密度图像的集合。我们采用视觉变换器作为共享主干网络以产生局部描述符集合，并引入了多BEV交互模块，根据不同高度自适应地关注信息。这之后是一个聚合层，它生成一个旋转不变的位置描述符。我们在公开基准测试和机器人数据集上的真实世界数据上进行了广泛评估，并将我们的方法与最先进（SOTA）的方法进行了比较。结果表明，ForestLPR在所有评价中均有持续优异的表现，在序列内回环闭合检测和序列间重新定位中的Recall@1分别比最近的竞争对手提高了7.38%和9.11%，验证了我们假设的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Place recognition is essential to maintain global consistency in large-scalelocalization systems. While research in urban environments has progressedsignificantly using LiDARs or cameras, applications in natural forest-likeenvironments remain largely under-explored. Furthermore, forests presentparticular challenges due to high self-similarity and substantial variations invegetation growth over time. In this work, we propose a robust LiDAR-basedplace recognition method for natural forests, ForestLPR. We hypothesize that aset of cross-sectional images of the forest's geometry at different heightscontains the information needed to recognize revisiting a place. Thecross-sectional images are represented by \ac{bev} density images of horizontalslices of the point cloud at different heights. Our approach utilizes a visualtransformer as the shared backbone to produce sets of local descriptors andintroduces a multi-BEV interaction module to attend to information at differentheights adaptively. It is followed by an aggregation layer that produces arotation-invariant place descriptor. We evaluated the efficacy of our methodextensively on real-world data from public benchmarks as well as roboticdatasets and compared it against the state-of-the-art (SOTA) methods. Theresults indicate that ForestLPR has consistently good performance on allevaluations and achieves an average increase of 7.38\% and 9.11\% on Recall@1over the closest competitor on intra-sequence loop closure detection andinter-sequence re-localization, respectively, validating our hypothesis</description>
      <author>example@mail.com (Yanqing Shen, Turcan Tuna, Marco Hutter, Cesar Cadena, Nanning Zheng)</author>
      <guid isPermaLink="false">2503.04475v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Retrieval Augmented Contrastive Learning for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2503.04162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的序列推荐框架Semantic Retrieval Augmented Contrastive Learning (SRA-CL)，通过利用语义信息来增强对比学习的有效性，解决了现有方法在生成可靠正样本时的局限性。&lt;h4&gt;背景&lt;/h4&gt;顺序推荐旨在根据历史行为序列建模用户偏好，在线上平台中至关重要。然而数据稀疏问题是这一领域的重要挑战，因为大多数用户的交互有限且许多项目受到的关注较少。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习方法SRA-CL来缓解数据稀疏问题，并生成可靠的正样本对以提高推荐模型的性能。&lt;h4&gt;方法&lt;/h4&gt;SRA-CL主要包含两个组成部分：(1)通过用户语义检索进行跨序列对比学习，使用大规模语言模型（LLMs）理解用户的多样化偏好并找到语义相似的用户形成可靠正样本；(2)通过项目语义检索进行内序列对比学习，使用LLMs来理解和替换类似项，创建语义一致性的增强视图以供对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;SRA-CL可以有效利用大规模语言模型提取用户的偏好和项目的特性，并生成可靠的对比正样本，从而提高推荐的准确性和多样性。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了所提出的框架的有效性和泛化能力。这种新方法不仅提高了序列推荐的质量，而且还可以很容易地集成到现有的顺序推荐系统中。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于SRA-CL的研究背景、目标、创新方法及其效果的概述，并展示了如何利用语义信息增强对比学习，从而改善了在线平台上的用户推荐体验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential recommendation aims to model user preferences based on historicalbehavior sequences, which is crucial for various online platforms. Datasparsity remains a significant challenge in this area as most users havelimited interactions and many items receive little attention. To mitigate thisissue, contrastive learning has been widely adopted. By constructing positivesample pairs from the data itself and maximizing their agreement in theembedding space,it can leverage available data more effectively. Constructingreasonable positive sample pairs is crucial for the success of contrastivelearning. However, current approaches struggle to generate reliable positivepairs as they either rely on representations learned from inherently sparsecollaborative signals or use random perturbations which introduce significantuncertainty. To address these limitations, we propose a novel approach namedSemantic Retrieval Augmented Contrastive Learning (SRA-CL), which leveragessemantic information to improve the reliability of contrastive samples. SRA-CLcomprises two main components: (1) Cross-Sequence Contrastive Learning via UserSemantic Retrieval, which utilizes large language models (LLMs) to understanddiverse user preferences and retrieve semantically similar users to formreliable positive samples through a learnable sample synthesis method; and (2)Intra-Sequence Contrastive Learning via Item Semantic Retrieval, which employsLLMs to comprehend items and retrieve similar items to perform semantic-baseditem substitution, thereby creating semantically consistent augmented views forcontrastive learning. SRA-CL is plug-and-play and can be integrated intostandard sequential recommendation models. Extensive experiments on four publicdatasets demonstrate the effectiveness and generalizability of the proposedapproach.</description>
      <author>example@mail.com (Ziqiang Cui, Yunpeng Weng, Xing Tang, Xiaokun Zhang, Dugang Liu, Shiwei Li, Peiyang Liu, Bowei He, Weihong Luo, Xiuqiang He, Chen Ma)</author>
      <guid isPermaLink="false">2503.04162v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised Monocular 3D Detection</title>
      <link>http://arxiv.org/abs/2503.04154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The paper includes 8 pages, 6 figures and 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的弱监督单目3D检测模型CA-W3D，旨在解决传统方法忽略上下文语义关系的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的标签效率高的方法主要关注对象中心特征，忽略了复杂场景中关键的上下文语义关系。这导致在全局上下文中捕捉信息的能力不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于上下文感知弱监督的方法来改进单目3D检测性能。&lt;h4&gt;方法&lt;/h4&gt;{'两阶段训练模式': [{'预训练阶段': '使用区域物体对比匹配(ROCM)对可训练的单目3D编码器和冻结的开放词汇2D视觉定位模型进行对齐，促进场景特定属性的辨别以及上下文知识的获取。'}, {'伪标签训练过程': '在第二阶段中引入了Dual-to-One Distillation (D2OD)机制，将上下文先验有效转移到单目编码器中，同时保持空间保真度和推理时的计算效率。'}]}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了所提出的方法的有效性，在公共KITTI基准测试上超过了现有的最先进方法。&lt;h4&gt;结论&lt;/h4&gt;研究强调了上下文感知知识在弱监督单目3D检测中的重要性，展示了通过引入适当的上下文信息能够显著提升模型性能。&lt;h4&gt;翻译&lt;/h4&gt;提出的Context-Aware Weak Supervision for Monocular 3D object detection (CA-W3D) 方法，在两阶段训练范式中解决了现有方法忽视复杂场景关键上下文语义关系的问题。该方法在公共KITTI基准测试上超过了当前最先进水平，证明了引入适当的上下文信息能够显著提升弱监督单目3D检测的性能和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Weakly supervised monocular 3D detection, while less annotation-intensive,often struggles to capture the global context required for reliable 3Dreasoning. Conventional label-efficient methods focus on object-centricfeatures, neglecting contextual semantic relationships that are critical incomplex scenes. In this work, we propose a Context-Aware Weak Supervision forMonocular 3D object detection, namely CA-W3D, to address this limitation in atwo-stage training paradigm. Specifically, we first introduce a pre-trainingstage employing Region-wise Object Contrastive Matching (ROCM), which alignsregional object embeddings derived from a trainable monocular 3D encoder and afrozen open-vocabulary 2D visual grounding model. This alignment encourages themonocular encoder to discriminate scene-specific attributes and acquire richercontextual knowledge. In the second stage, we incorporate a pseudo-labeltraining process with a Dual-to-One Distillation (D2OD) mechanism, whicheffectively transfers contextual priors into the monocular encoder whilepreserving spatial fidelity and maintaining computational efficiency duringinference. Extensive experiments conducted on the public KITTI benchmarkdemonstrate the effectiveness of our approach, surpassing the SoTA method overall metrics, highlighting the importance of contextual-aware knowledge inweakly-supervised monocular 3D detection.</description>
      <author>example@mail.com (Chupeng Liu, Runkai Zhao, Weidong Cai)</author>
      <guid isPermaLink="false">2503.04154v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models in Bioinformatics: A Survey</title>
      <link>http://arxiv.org/abs/2503.04490v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该综述系统地回顾了大型语言模型（LLMs）在生物信息学领域的最新进展及其挑战和未来方向。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型正在革新生物信息学领域，能够在DNA、RNA、蛋白质以及单细胞数据分析方面提供高级分析方法。&lt;h4&gt;目的&lt;/h4&gt;总结并讨论最近的生物信息学技术进步，并探索未来的研究趋势。&lt;h4&gt;主要发现&lt;/h4&gt;{'研究进展': ['基因组序列建模', 'RNA结构预测', '蛋白质功能推断', '单细胞转录组学'], '关键挑战': ['数据稀缺性', '计算复杂度', '跨领域整合']}&lt;h4&gt;结论&lt;/h4&gt;强调了大型语言模型在生物信息学和精准医学中的变革潜力，并提出未来可能的发展方向，如多模态学习、混合AI模型以及临床应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) are revolutionizing bioinformatics, enablingadvanced analysis of DNA, RNA, proteins, and single-cell data. This surveyprovides a systematic review of recent advancements, focusing on genomicsequence modeling, RNA structure prediction, protein function inference, andsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,including data scarcity, computational complexity, and cross-omics integration,and explore future directions such as multimodal learning, hybrid AI models,and clinical applications. By offering a comprehensive perspective, this paperunderscores the transformative potential of LLMs in driving innovations inbioinformatics and precision medicine.</description>
      <author>example@mail.com (Zhenyu Wang, Zikang Wang, Jiyue Jiang, Pengan Chen, Xiangyu Shi, Yu Li)</author>
      <guid isPermaLink="false">2503.04490v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>PointsToWood: A deep learning framework for complete canopy leaf-wood segmentation of TLS data across diverse European forests</title>
      <link>http://arxiv.org/abs/2503.04420v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;三维激光扫描（TLS）获取的点云数据是研究植物结构和功能的重要来源。然而，为了提取重要的生态信息，通常需要大量的人工处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架来准确地对不同类型的植物材料进行语义分割，特别是木材和叶子。&lt;h4&gt;方法&lt;/h4&gt;本研究提出了一个新的基于PointNet和pointNEXT的深度学习架构，用于处理3D点云数据。该模型使用了精确标记的数据、体素采样技术以及新设计的门控反射整合模块等。&lt;h4&gt;主要发现&lt;/h4&gt;我们的模型在欧洲不同成熟的森林中训练，并且在北极、温带、地中海及热带地区的公开数据集上进行了测试，结果显示，在对叶/木语义分割方面优于最常用的基于PointNet的方法。此外，该模型在中国、东喀麦隆、德国和芬兰的数据集中也表现出色，这些数据是使用飞行时间和相位移传感器收集的。&lt;h4&gt;结论&lt;/h4&gt;开发的新框架能够提供从树基到枝尖的木材和叶的可靠语义分割，展示了良好的跨生态系统类型和不同传感器类型的可转移性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此处为其中文翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point clouds from Terrestrial Laser Scanning (TLS) are an increasinglypopular source of data for studying plant structure and function but typicallyrequire extensive manual processing to extract ecologically importantinformation. One key task is the accurate semantic segmentation of differentplant material within point clouds, particularly wood and leaves, which isrequired to understand plant productivity, architecture and physiology.Existing automated semantic segmentation methods are primarily developed forsingle ecosystem types, and whilst they show good accuracy for biomassassessment from the trunk and large branches, often perform less well withinthe crown. In this study, we demonstrate a new framework that uses a deeplearning architecture newly developed from PointNet and pointNEXT forprocessing 3D point clouds to provide a reliable semantic segmentation of woodand leaf in TLS point clouds from the tree base to branch tips, trained on datafrom diverse mature European forests. Our model uses meticulously labelled datacombined with voxel-based sampling, neighbourhood rescaling, and a novel gatedreflectance integration module embedded throughout the feature extractionlayers. We evaluate its performance across open datasets from boreal,temperate, Mediterranean and tropical regions, encompassing diverse ecosystemtypes and sensor characteristics. Our results show consistent outperformanceagainst the most widely used PointNet based approach for leaf/wood segmentationon our high-density TLS dataset collected across diverse mixed forest plotsacross all major biomes in Europe. We also find consistently strong performancetested on others open data from China, Eastern Cameroon, Germany and Finland,collected using both time-of-flight and phase-shift sensors, showcasing thetransferability of our model to a wide range of ecosystems and sensors.</description>
      <author>example@mail.com (Harry J. F. Owen, Matthew J. A. Allen, Stuart W. D. Grieve, Phill Wilkes, Emily R. Lines)</author>
      <guid isPermaLink="false">2503.04420v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Frequency-Based Alignment of EEG and Audio Signals Using Contrastive Learning and SincNet for Auditory Attention Detection</title>
      <link>http://arxiv.org/abs/2503.04156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;人类在复杂的声学环境中具备出色的听觉注意力聚焦能力，如鸡尾酒会场景。听觉注意检测（AAD）旨在通过分析脑信号来识别被关注的说话人，这些信号包括脑电图（EEG）数据等。&lt;h4&gt;背景&lt;/h4&gt;现有的AAD算法通常利用深度学习的强大非线性建模能力，但很少考虑大脑中听觉处理的神经机制。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于改进SincNet和对比学习的新网络模型——SincAlignNet，旨在对齐音频和EEG特征进行听觉注意检测。&lt;h4&gt;方法&lt;/h4&gt;通过计算EEG与音频特征之间的余弦相似度，并探索仅使用脑电数据直接推断被关注说话人的可能性。SincNet组件模拟了大脑在听觉注意力下处理音频的过程；对比学习引导模型学习EEG信号和被关注语音之间的关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SincAlignNet优于现有公开数据集（KUL和DTU）上的最先进的AAD方法，在1秒决策窗口的平均准确率为78.3%和92.2%，并且该模型具有较强的可解释性。此外，使用靠近颞叶区域附近六个电极的数据即可维持相似甚至更好的性能。&lt;h4&gt;结论&lt;/h4&gt;研究表明高效低密度EEG在线解码是可能实现的，并为实际应用中的神经引导助听器的重要步骤铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要：人类在复杂的声学环境中表现出非凡的能力，能够集中听觉注意力，例如鸡尾酒会。听觉注意检测（AAD）的目标是在分析脑信号的基础上确定被关注的说话人，这些信号包括如脑电图（EEG）数据等。现有的AAD算法通常利用深度学习的强大非线性建模能力，然而较少考虑大脑中与听觉处理相关的神经机制。本文提出了一种新的网络模型——SincAlignNet，该模型基于改进后的SincNet和对比学习，并设计用于对齐音频与EEG特征以进行听觉注意检测。SincNet组件模仿了在听觉注意力状态下大脑如何处理声音的过程；而对比学习指导模型去学习EEG信号与被关注的语音之间的关系。在推理阶段，通过计算EEG和音频特征间的余弦相似度，并且探索仅利用脑电图数据直接推断所关注说话人的可能性。跨试验评估表明，在两个公开的数据集KUL和DTU上，SincAlignNet优于现有的最先进的AAD方法，分别实现了78.3%和92.2%的平均准确率（1秒决策窗口）。该模型表现出较强的可解释性，揭示在男女性说话场景中左侧与右侧颞叶活动更为显著。此外，我们发现仅使用靠近颞叶区域附近六个电极的数据即可保持相似或更好的性能，这表明高效低密度EEG在线解码是可行的，并向实际应用中的神经引导助听器的重要进展迈进了一步。&lt;h4&gt;代码链接&lt;/h4&gt;https://github.com/LiaoEuan/SincAlignNet&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans exhibit a remarkable ability to focus auditory attention in complexacoustic environments, such as cocktail parties. Auditory attention detection(AAD) aims to identify the attended speaker by analyzing brain signals, such aselectroencephalography (EEG) data. Existing AAD algorithms often leverage deeplearning's powerful nonlinear modeling capabilities, few consider the neuralmechanisms underlying auditory processing in the brain. In this paper, wepropose SincAlignNet, a novel network based on an improved SincNet andcontrastive learning, designed to align audio and EEG features for auditoryattention detection. The SincNet component simulates the brain's processing ofaudio during auditory attention, while contrastive learning guides the model tolearn the relationship between EEG signals and attended speech. Duringinference, we calculate the cosine similarity between EEG and audio featuresand also explore direct inference of the attended speaker using EEG data.Cross-trial evaluations results demonstrate that SincAlignNet outperformsstate-of-the-art AAD methods on two publicly available datasets, KUL and DTU,achieving average accuracies of 78.3% and 92.2%, respectively, with a 1-seconddecision window. The model exhibits strong interpretability, revealing that theleft and right temporal lobes are more active during both male and femalespeaker scenarios. Furthermore, we found that using data from only sixelectrodes near the temporal lobes maintains similar or even better performancecompared to using 64 electrodes. These findings indicate that efficientlow-density EEG online decoding is achievable, marking an important step towardthe practical implementation of neuro-guided hearing aids in real-worldapplications. Code is available at: https://github.com/LiaoEuan/SincAlignNet.</description>
      <author>example@mail.com (Yuan Liao, Yuhong Zhang, Qiushi Han, Yuhang Yang, Weiwei Ding, Yuzhe Gu, Hengxin Yang, Liya Huang)</author>
      <guid isPermaLink="false">2503.04156v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ObjMST: An Object-Focused Multimodal Style Transfer Framework</title>
      <link>http://arxiv.org/abs/2503.04353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 Figures, 3 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个以对象为中心的多模态风格转换框架ObjMST，该框架通过分别对显著对象及其周围元素提供特定样式监督的方式解决了跨模态表示学习中的对齐问题。&lt;h4&gt;背景&lt;/h4&gt;现有的图像-文本多模态风格转移方法面临着生成不一致且不对齐的多模态表示以及内容不匹配的问题。这些问题导致在风格化过程中，相同风格模式被错误地应用到显著对象及其周围元素上。&lt;h4&gt;目的&lt;/h4&gt;为了缓解上述挑战，本文提出了一种新的框架ObjMST，旨在提供准确且对齐的样式表达，同时解决图像-文本多模态风格转移中的内容不匹配问题。&lt;h4&gt;方法&lt;/h4&gt;该方法通过引入特定样式的掩码方向性CLIP损失函数来确保显著对象及其周围环境的一致性和对齐性，并结合显着对象到关键元素映射机制以及图像调和化技术，实现将风格化的对象无缝融入其环境中。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，ObjMST在多模态风格转换任务上具有良好的效果，不仅能够生成一致且准确的样式表示，而且还能有效解决内容不匹配问题。&lt;h4&gt;结论&lt;/h4&gt;通过定量和定性评估证明了所提出的方法的有效性和创新性。作者认为这种基于对象的多模态风格转移框架可以作为未来研究的基础，并进一步扩展到其他跨模态任务中。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了ObjMST，这是一个以对象为中心的多模态样式转换框架，它为显著对象及其周围元素提供了单独的样式监督，并解决了跨模态表示学习中的对齐问题。现有的图像-文本多模态风格转移方法面临着以下挑战：(1)生成非对齐且不一致的多模态样式表示；以及(2)内容不匹配的问题，即相同的样式模式被应用于显著对象及其周围元素。我们的方法通过引入特定样式的掩码方向性CLIP损失来减轻这些问题，该机制确保了显著对象及其环境的一致性和对齐性，并结合显着对象到关键元素的映射机制进行风格化处理，随后采用图像调和化技术无缝融合样式化的对象与其周围环境。我们通过实验验证了ObjMST的有效性，使用定量指标以及定性的视觉评估来评价其结果。我们的代码可在https://github.com/chandagrover/ObjMST获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose ObjMST, an object-focused multimodal style transfer framework thatprovides separate style supervision for salient objects and surroundingelements while addressing alignment issues in multimodal representationlearning. Existing image-text multimodal style transfer methods face thefollowing challenges: (1) generating non-aligned and inconsistent multimodalstyle representations; and (2) content mismatch, where identical style patternsare applied to both salient objects and their surrounding elements. Ourapproach mitigates these issues by: (1) introducing a Style-Specific MaskedDirectional CLIP Loss, which ensures consistent and aligned stylerepresentations for both salient objects and their surroundings; and (2)incorporating a salient-to-key mapping mechanism for stylizing salient objects,followed by image harmonization to seamlessly blend the stylized objects withtheir environment. We validate the effectiveness of ObjMST through experiments,using both quantitative metrics and qualitative visual evaluations of thestylized outputs. Our code is available at:https://github.com/chandagrover/ObjMST.</description>
      <author>example@mail.com (Chanda Grover Kamra, Indra Deep Mastan, Debayan Gupta)</author>
      <guid isPermaLink="false">2503.04353v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>MASTER: Multimodal Segmentation with Text Prompts</title>
      <link>http://arxiv.org/abs/2503.04199v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;RGB-热融合技术在各种天气和光照条件下具有潜在解决方案，通过将大型语言模型（LLMs）的优势融入RGB-热多模态数据的融合过程中，设计出一种结构简单且高度可适应的多模态融合模型。&lt;h4&gt;背景&lt;/h4&gt;现有的研究大多集中在设计复杂模块来融合不同模式的数据。然而，随着大规模语言模型的应用越来越广泛，通过自然语言可以更有效地提取有价值的信息。&lt;h4&gt;目的&lt;/h4&gt;旨在利用大型语言模型的优势，设计出一种结构简单且高度可适应的多模态融合模型架构。&lt;h4&gt;方法&lt;/h4&gt;提出了MASTER架构，将LLM集成到RGB-热多模态数据的融合过程中，并允许复杂的查询文本参与融合过程。该模型采用双路径结构来提取不同图像模式的信息，并使用LLM作为多模态融合的核心模块，生成可学习码本令牌，同时利用轻量级图像解码器获得语义分割结果。&lt;h4&gt;主要发现&lt;/h4&gt;提出的MASTER架构在多个自动驾驶场景的基准测试中表现出色，取得了令人满意的结果。&lt;h4&gt;结论&lt;/h4&gt;通过将大型语言模型的优势融入RGB-热多模态数据的融合过程，可以设计出结构简单且高度可适应的多模态融合模型，显著提升自动化驾驶任务中的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;RGB-Thermal 融合是一种潜在的解决方案，适用于各种天气和光照条件下的挑战性场景。然而，许多研究集中在设计复杂的模块来融合不同的模式数据。随着大型语言模型（LLMs）的广泛应用，可以通过自然语言更有效地提取有价值的信息。因此，我们的目标是利用大型语言模型的优势来设计一种结构简单且高度可适应的多模态融合模型架构。我们提出了MultimodAl Segmentation with TExt PRompts (MASTER) 架构，该架构将LLM整合到RGB-热多模态数据的融合过程中，并允许复杂的查询文本参与此过程。我们的模型利用一种双路径结构来提取不同图像模式的信息，同时使用LLM作为多模态融合的核心模块，使模型能够从RGB、热图和文字信息中生成可学习的码本令牌。通过轻量级的图像解码器可以获取语义分割结果。提出的MASTER架构在多个自动驾驶场景的基准测试中表现出色，并获得了令人满意的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; RGB-Thermal fusion is a potential solution for various weather and lightconditions in challenging scenarios. However, plenty of studies focus ondesigning complex modules to fuse different modalities. With the widespreadapplication of large language models (LLMs), valuable information can be moreeffectively extracted from natural language. Therefore, we aim to leverage theadvantages of large language models to design a structurally simple and highlyadaptable multimodal fusion model architecture. We proposed MultimodAlSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM intothe fusion of RGB-Thermal multimodal data and allows complex query text toparticipate in the fusion process. Our model utilizes a dual-path structure toextract information from different modalities of images. Additionally, weemploy LLM as the core module for multimodal fusion, enabling the model togenerate learnable codebook tokens from RGB, thermal images, and textualinformation. A lightweight image decoder is used to obtain semanticsegmentation results. The proposed MASTER performs exceptionally well inbenchmark tests across various automated driving scenarios, yielding promisingresults.</description>
      <author>example@mail.com (Fuyang Liu, Shun Lu, Jilin Mei, Yu Hu)</author>
      <guid isPermaLink="false">2503.04199v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Sarcasm Detection as a Catalyst: Improving Stance Detection with Cross-Target Capabilities</title>
      <link>http://arxiv.org/abs/2503.03787v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 5 figures, published, published in International Journal On  Advances in Intelligent Systems, volume 17, numbers 3 and 4. arXiv admin  note: text overlap with arXiv:2503.03172&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合讽刺检测的立场识别方法，并通过跨目标立场检测任务评估其效果，展示了显著优于现有最佳模型的表现。&lt;h4&gt;背景&lt;/h4&gt;立场识别由于在线平台文本中存在讽刺语言而面临挑战。同时，缺乏足够的标注数据用于训练新的立场识别模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合讽刺预训练的立场识别方法，以解决上述问题，并通过跨目标立场检测来评估模型性能。&lt;h4&gt;方法&lt;/h4&gt;采用微调BERT和RoBERTa模型并附加深度学习层的方法进行讽刺与立场识别。该方法在公开数据集上与其他最先进的基线模型进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;在无讽刺预训练的情况下，相较于现有最佳模型，该方法在同源域的立场识别任务中准确率提升了85%。此外，在跨目标任务中，无需额外微调即可达到与同源域任务相当的表现，并且该成功依赖于讽刺检测和立场识别之间的词汇属性相关性。&lt;h4&gt;结论&lt;/h4&gt;这项研究首次探索了将讽刺检测作为中间转移学习任务的前景，并利用BERT或RoBERTa与其他深度学习技术相结合的方法。提出的这种方法为未来的研究提供了基础基线。&lt;h4&gt;翻译&lt;/h4&gt;立场检测（SD）由于其在各种上下文中的应用而成为自然语言处理领域的一个重要研究方向，但由于在线平台文本中讽刺语言的存在使得准确确定作者立场变得困难。本论文通过利用讽刺来解决这一问题，并通过跨目标的立场识别任务解决了缺乏足够的标注数据的问题。提出的方法包括微调BERT和RoBERTa模型以及附加深度学习层，并且在公开的数据集上与其他最先进的基线方法进行了比较，展示了超越现有最佳模型的表现。值得注意的是，在不进行讽刺检测预训练的情况下，我们的模型已经在同源域的立场识别任务中优于现有的顶级模型。将讽刺知识整合到模型中显著减少了误分类讽刺文本元素的数量，使得在无讽刺检测预训练的情况下准确预测了85%此前被错误分类的内容，并提高了宏F1平均分。跨目标任务通过零样本微调也达到了与同源域相似的表现。此外，成功还依赖于讽刺识别和SD之间的词汇属性相关性。这项研究首次探索了将讽刺检测作为中间转移学习任务的可能，并利用BERT或RoBERTa与其他深度学习技术相结合的方法。所提出的方法为该领域的未来研究建立了基础基线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stance Detection (SD) has become a critical area of interest due to itsapplications in various contexts leading to increased research within NLP. Yetthe subtlety and complexity of texts sourced from online platforms oftencontaining sarcastic language pose significant challenges for SD algorithms inaccurately determining the authors stance. This paper addresses this byemploying sarcasm for SD. It also tackles the issue of insufficient annotateddata for training SD models on new targets by conducting Cross-Target SD(CTSD). The proposed approach involves fine-tuning BERT and RoBERTa modelsfollowed by concatenating additional deep learning layers. The approach isassessed against various State-Of-The-Art baselines for SD demonstratingsuperior performance using publicly available datasets. Notably our modeloutperforms the best SOTA models on both in-domain SD and CTSD tasks evenbefore the incorporation of sarcasm-detection pre-training. The integration ofsarcasm knowledge into the model significantly reduces misclassifications ofsarcastic text elements in SD allowing our model to accurately predict 85% oftexts that were previously misclassified without sarcasm-detection pre-trainingon in-domain SD. This enhancement contributes to an increase in the modelsaverage macro F1-score. The CTSD task achieves performance comparable to thatof the in-domain task despite using a zero-shot finetuning. We also reveal thatthe success of the transfer-learning framework relies on the correlationbetween the lexical attributes of sarcasm detection and SD. This studyrepresents the first exploration of sarcasm detection as an intermediatetransfer-learning task within the context of SD while also leveraging theconcatenation of BERT or RoBERTa with other deep-learning techniques. Theproposed approach establishes a foundational baseline for future research inthis domain.</description>
      <author>example@mail.com (Gibson Nkhata Shi Yin Hong, Susan Gauch)</author>
      <guid isPermaLink="false">2503.03787v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Real-time Spatial-temporal Traversability Assessment via Feature-based Sparse Gaussian Process</title>
      <link>http://arxiv.org/abs/2503.04134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种新颖的空间-时间可通行性评估方法，旨在使自主机器人能够在复杂地形中有效导航。&lt;h4&gt;背景&lt;/h4&gt;地形分析对于地面移动机器人的实际应用至关重要，尤其是在户外非结构化环境中。&lt;h4&gt;目的&lt;/h4&gt;目标是开发一种能够帮助自主机器人在复杂地形中有效导航的方法。&lt;h4&gt;方法&lt;/h4&gt;{'空间-时间贝叶斯高斯核（BGK）推理方法': '利用稀疏高斯过程直接从点云扫描中提取几何特征，然后构建高分辨率局部可通行性地图。此外，设计了一种将历史和实时数据结合考虑坡度、平坦度等因子的空间-时间贝叶斯高斯核推理方法来动态评估可通行性得分。', 'GPU加速': '在特征提取步骤中应用GPU加速以实现实时性能'}&lt;h4&gt;主要发现&lt;/h4&gt;广泛的模拟实验表明，该方法在精度和计算效率方面优于现有最佳技术（SOTA）&lt;h4&gt;结论&lt;/h4&gt;开发了一种自主导航框架，并通过差分驱动车辆在复杂户外环境中进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;地形分析对于地面移动机器人的实际应用至关重要，尤其是在户外非结构化环境中。本文提出了一种新颖的空间-时间可通行性评估方法，旨在使自主机器人能够在复杂地形中有效导航。该方法利用稀疏高斯过程（SGP）直接从点云扫描中提取几何特征（如曲率、坡度、高度等），然后构建高分辨率局部可通行性地图。进一步设计了空间-时间贝叶斯高斯核（BGK）推理方法，结合历史和实时数据，考虑坡度、平坦度等因素来动态评估可通行性得分，并在特征提取步骤中应用GPU加速以实现实时性能。广泛的模拟实验表明该方法在精度和计算效率方面优于现有最佳技术。此外，开发了一种自主导航框架并与差分驱动车辆进行了复杂户外环境的验证。代码将开源供进一步研究和发展：https://github.com/ZJU-FAST-Lab/FSGP_BGK&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Terrain analysis is critical for the practical application of ground mobilerobots in real-world tasks, especially in outdoor unstructured environments. Inthis paper, we propose a novel spatial-temporal traversability assessmentmethod, which aims to enable autonomous robots to effectively navigate throughcomplex terrains. Our approach utilizes sparse Gaussian processes (SGP) toextract geometric features (curvature, gradient, elevation, etc.) directly frompoint cloud scans. These features are then used to construct a high-resolutionlocal traversability map. Then, we design a spatial-temporal Bayesian Gaussiankernel (BGK) inference method to dynamically evaluate traversability scores,integrating historical and real-time data while considering factors such asslope, flatness, gradient, and uncertainty metrics. GPU acceleration is appliedin the feature extraction step, and the system achieves real-time performance.Extensive simulation experiments across diverse terrain scenarios demonstratethat our method outperforms SOTA approaches in both accuracy and computationalefficiency. Additionally, we develop an autonomous navigation frameworkintegrated with the traversability map and validate it with a differentialdriven vehicle in complex outdoor environments. Our code will be open-sourcefor further research and development by the community,https://github.com/ZJU-FAST-Lab/FSGP_BGK.</description>
      <author>example@mail.com (Senming Tan, Zhenyu Hou, Zhihao Zhang, Long Xu, Mengke Zhang, Zhaoqi He, Chao Xu, Fei Gao, Yanjun Cao)</author>
      <guid isPermaLink="false">2503.04134v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Causal Response Representations through Direct Effect Analysis</title>
      <link>http://arxiv.org/abs/2503.04358v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 15 figures, stat.ML&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的学习因果响应表示的方法，旨在从多维度结果中提取由治疗变量直接引起的最直接方向。&lt;h4&gt;背景&lt;/h4&gt;当前方法难以在复杂、多维环境中有效识别直接的因果效应。&lt;h4&gt;目的&lt;/h4&gt;通过将条件独立性测试与因果表征学习相结合来最大化治疗和结局之间在给定一个条件集合下的条件独立性的证据，从而提取出直接由治疗变量导致的结果维度。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种优化问题框架，并利用广义特征值分解解决了这一问题。此外，它还为所学表示的最优性提供了理论保证，特别是在信号噪声比和费雪信息最大化方面。&lt;h4&gt;主要发现&lt;/h4&gt;最大的特征值分布可以在满足轻度假设的情况下被已知的F-分布所限制，从而使得条件独立性的可测试性成为可能，并且这种方法在模拟实验和真实世界实验中显示出其有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的框架对于揭示复杂多变量环境中的直接因果效应具有重要的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;We提出了一种新颖的方法来学习因果响应表示。我们的方法旨在提取由治疗变量最直接导致的多维结果的方向。通过将条件独立性测试与因果表征学习相结合，我们制定了一个优化问题以最大化在给定条件集的情况下治疗和结局之间条件独立性的证据。该框架利用灵活回归模型针对特定应用进行了定制，并通过广义特征值分解解决。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel approach for learning causal response representations. Ourmethod aims to extract directions in which a multidimensional outcome is mostdirectly caused by a treatment variable. By bridging conditional independencetesting with causal representation learning, we formulate an optimisationproblem that maximises the evidence against conditional independence betweenthe treatment and outcome, given a conditioning set. This formulation employsflexible regression models tailored to specific applications, creating aversatile framework. The problem is addressed through a generalised eigenvaluedecomposition. We show that, under mild assumptions, the distribution of thelargest eigenvalue can be bounded by a known $F$-distribution, enablingtestable conditional independence. We also provide theoretical guarantees forthe optimality of the learned representation in terms of signal-to-noise ratioand Fisher information maximisation. Finally, we demonstrate the empiricaleffectiveness of our approach in simulation and real-world experiments. Ourresults underscore the utility of this framework in uncovering direct causaleffects within complex, multivariate settings.</description>
      <author>example@mail.com (Homer Durand, Gherardo Varando, Gustau Camps-Valls)</author>
      <guid isPermaLink="false">2503.04358v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Robust Multi-View Learning via Representation Fusion of Sample-Level Attention and Alignment of Simulated Perturbation</title>
      <link>http://arxiv.org/abs/2503.04151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近，多视角学习（MVL）由于其能够融合来自多个视角的判别信息而备受关注。然而，实际中的多视角数据集往往异质且不完善，这通常使得为特定视角组合设计的MVL方法缺乏应用潜力，并限制了它们的有效性。&lt;h4&gt;背景&lt;/h4&gt;在现实世界中，多视角学习（MVL）面临着处理异质和不完善的多视角数据集的挑战。这些数据集中的问题导致现有的MVL方法难以泛化到不同的场景之中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的鲁棒多视角学习方法（RML），该方法能够同时进行表示融合与对齐，以应对实际应用中遇到的数据异质性和不完善性的问题。&lt;h4&gt;方法&lt;/h4&gt;{'模型架构': '引入了一种简单有效的多视角Transformer融合网络，在其中将异质的多视角数据转换为同构的词嵌入，并通过样本级注意力机制整合多个视角，从而获得一种融合表示。', '对抗学习框架': '提出了基于模拟扰动的多视角对比学习框架，该框架能够动态生成噪声和不可用的干扰以模拟不完美的数据条件。这种方案利用对比学习来对齐模拟出的有噪或无法使用的数据所得到的不同融合表征，从而促使模型学习到判别性和鲁棒性的表示。', '自监督与正则化': 'RML方法是一种自监督的学习方式，并且可以作为正则项用于下游任务。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验验证': '在无监督多视角聚类、噪声标签分类以及跨模态哈希检索等不同应用场景中，通过广泛的对比试验和消融研究证实了RML方法的有效性。', '应用范围': '提出的模型不仅可以作为专门的MVL技术应用于特定任务中（如聚类），也可以作为一个可插拔模块嵌入到其他任务之中（例如跨模态哈希检索）以提升它们的表现。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入鲁棒多视角表示学习框架，RML方法能够有效地处理异质性和不完善的多视角数据集，在多个实际应用场景中展现了强大的泛化能力和性能优势。&lt;h4&gt;翻译&lt;/h4&gt;最近，由于其融合来自多个视角的判别信息的能力，多视角学习（MVL）引起了极大的关注。然而，真实世界的多视角数据集经常是异质且不完备的，这通常使得为特定组合设计的MVL方法缺乏应用潜力，并限制了它们的有效性。为了应对这一问题，我们提出了一种新颖的鲁棒MVL方法（称为RML），该方法同时实现表示融合和对齐。具体来说，我们引入了一个简单但有效的多视角Transformer融合网络，在其中我们将异质多视角数据转换为同构词嵌入，并通过样本级注意力机制整合多个视角以获得融合表示。此外，我们提出了一种基于模拟扰动的多视角对比学习框架，该框架动态生成噪声和无用干扰来模拟不完美数据条件。有噪或无法使用的数据获取了两个不同的融合表示，我们利用对比学习将它们对齐，从而学到判别性和鲁棒性的表示。我们的RML是自监督的，并且也可以作为下游任务中的正则项应用。在实验中，我们在无监督多视角聚类、噪声标签分类以及作为一个即插即用模块用于跨模态哈希检索中使用它。广泛的对比试验和消融研究验证了RML的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, multi-view learning (MVL) has garnered significant attention due toits ability to fuse discriminative information from multiple views. However,real-world multi-view datasets are often heterogeneous and imperfect, whichusually makes MVL methods designed for specific combinations of views lackapplication potential and limits their effectiveness. To address this issue, wepropose a novel robust MVL method (namely RML) with simultaneous representationfusion and alignment. Specifically, we introduce a simple yet effectivemulti-view transformer fusion network where we transform heterogeneousmulti-view data into homogeneous word embeddings, and then integrate multipleviews by the sample-level attention mechanism to obtain a fused representation.Furthermore, we propose a simulated perturbation based multi-view contrastivelearning framework that dynamically generates the noise and unusableperturbations for simulating imperfect data conditions. The simulated noisy andunusable data obtain two distinct fused representations, and we utilizecontrastive learning to align them for learning discriminative and robustrepresentations. Our RML is self-supervised and can also be applied fordownstream tasks as a regularization. In experiments, we employ it inunsupervised multi-view clustering, noise-label classification, and as aplug-and-play module for cross-modal hashing retrieval. Extensive comparisonexperiments and ablation studies validate the effectiveness of RML.</description>
      <author>example@mail.com (Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu)</author>
      <guid isPermaLink="false">2503.04151v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic Membrane Vessel Segmentation</title>
      <link>http://arxiv.org/abs/2503.03546v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的Intermediate Domain-guided Adaptation (IDA) 方法，利用中间域信息和现有公共视网膜数据集对CAM图像进行无监督训练。&lt;h4&gt;背景&lt;/h4&gt;CAM模型广泛应用于血管生成研究中，血管网状结构的分布是关键评价指标。因此，基于拓扑和形态学特征的定量评估需要精确的血管分割方法。然而，手动分割耗时且容易产生主观误差，并且关于CAM血管分割算法的研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的无监督领域适应（UDA）方法以改进CAM图像中的血管自动分割。&lt;h4&gt;方法&lt;/h4&gt;提出了一种Intermediate Domain-guided Adaptation (IDA) 方法。该方法包括Multi-Resolution Asymmetric Translation (MRAT)策略，用于生成中间域图象，以及Intermediate Domain-guided Contrastive Learning (IDCL) 模块，用于分离跨领域特征表示。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的算法在新创建的CAM数据集上进行了评估，并且其性能优于其他方法。此外，在视网膜数据集中，该方法也表现出色，说明了其强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用中间域信息和现有的公共数据集，可以有效地提高对CAM图像中血管自动分割的效果。&lt;h4&gt;翻译&lt;/h4&gt;胎盘绒毛尿囊膜（CAM）模型在血管生成研究中的广泛应用，使得准确的血管分布评估变得至关重要。然而，现有方法存在手动分割耗时、主观性强以及缺乏公开数据集等问题。为解决这些问题，作者提出了一种新颖的方法——Intermediate Domain-guided Adaptation (IDA)，它利用了CAM图像与视网膜图像之间的相似性及现有的公共视网膜数据集进行无监督训练，通过Multi-Resolution Asymmetric Translation（MRAT）策略生成中间图象以增强图象级交互，并开发了一个Intermediate Domain-guided Contrastive Learning（IDCL）模块来分离跨领域特征表示。该方法克服了现有无监督领域适应技术的局限性，在CAM数据集上验证并取得了超越其他方法的表现，同时在视网膜数据集中也展现了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The chorioallantoic membrane (CAM) model is widely employed in angiogenesisresearch, and distribution of growing blood vessels is the key evaluationindicator. As a result, vessel segmentation is crucial for quantitativeassessment based on topology and morphology. However, manual segmentation isextremely time-consuming, labor-intensive, and prone to inconsistency due toits subjective nature. Moreover, research on CAM vessel segmentation algorithmsremains limited, and the lack of public datasets contributes to poor predictionperformance. To address these challenges, we propose an innovative IntermediateDomain-guided Adaptation (IDA) method, which utilizes the similarity betweenCAM images and retinal images, along with existing public retinal datasets, toperform unsupervised training on CAM images. Specifically, we introduce aMulti-Resolution Asymmetric Translation (MRAT) strategy to generateintermediate images to promote image-level interaction. Then, an IntermediateDomain-guided Contrastive Learning (IDCL) module is developed to disentanglecross-domain feature representations. This method overcomes the limitations ofexisting unsupervised domain adaptation (UDA) approaches, which primarilyconcentrate on directly source-target alignment while neglecting intermediatedomain information. Notably, we create the first CAM dataset to validate theproposed algorithm. Extensive experiments on this dataset show that our methodoutperforms compared approaches. Moreover, it achieves superior performance inUDA tasks across retinal datasets, highlighting its strong generalizationcapability. The CAM dataset and source codes are available athttps://github.com/Light-47/IDA.</description>
      <author>example@mail.com (Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu)</author>
      <guid isPermaLink="false">2503.03546v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Biological Sequence with Language Model Prompting: A Survey</title>
      <link>http://arxiv.org/abs/2503.04135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文系统地研究了在生物序列分析中使用基于提示的大语言模型的方法，涵盖了DNA、RNA、蛋白质和药物发现等任务。重点探讨了如何通过精心设计的提示工程来克服领域特定问题，并强调了这种方法在未来生物信息学中的潜在转变能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）在解决跨不同领域的挑战方面表现出强大的工具性，特别是在增强生物分子分析与合成效率方面得到了广泛学术界和医学界的关注。&lt;h4&gt;目的&lt;/h4&gt;本文旨在系统地调查基于提示的方法如何应用于生物序列的分析，并探讨这些方法在未来生物信息学中的潜力。&lt;h4&gt;方法&lt;/h4&gt;研究集中于大语言模型在生物序列（如DNA、RNA、蛋白质）和药物发现任务中应用，特别是利用精心设计的提示工程技术来解决特定领域的挑战。&lt;h4&gt;主要发现&lt;/h4&gt;基于提示的大语言模型可以显著提高诸如启动子序列预测、蛋白质结构建模以及药物与靶点结合亲和力预测等任务的表现，并且在标签数据有限的情况下尤其有效。此外，这些方法展示了在生物信息学领域中的变革潜力，尤其是在处理数据稀缺性、多模态融合和计算资源限制方面。&lt;h4&gt;结论&lt;/h4&gt;论文希望成为这一快速发展的研究领域的入门级文献，同时也作为推动进一步创新的催化剂。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）已经成为解决跨不同领域的挑战的强大工具。特别是在增强生物分子分析与合成效率方面受到了学术界和医学界的广泛关注。本文系统地探讨了基于提示的方法在生物序列包括DNA、RNA、蛋白质以及药物发现任务中的应用，重点在于如何通过精心设计的提示工程技术来应对领域特定问题，并强调这种方法在未来生物信息学领域的变革潜力。这些方法尤其有效于标签数据有限的情况，并且展示了处理诸如多模态融合及计算资源限制等关键挑战的能力。论文旨在成为新手入门文献的同时也作为推动该领域创新的重要催化剂。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language models (LLMs) have emerged as powerful tools for addressingchallenges across diverse domains. Notably, recent studies have demonstratedthat large language models significantly enhance the efficiency of biomolecularanalysis and synthesis, attracting widespread attention from academics andmedicine. In this paper, we systematically investigate the application ofprompt-based methods with LLMs to biological sequences, including DNA, RNA,proteins, and drug discovery tasks. Specifically, we focus on how promptengineering enables LLMs to tackle domain-specific problems, such as promotersequence prediction, protein structure modeling, and drug-target bindingaffinity prediction, often with limited labeled data. Furthermore, ourdiscussion highlights the transformative potential of prompting inbioinformatics while addressing key challenges such as data scarcity,multimodal fusion, and computational resource limitations. Our aim is for thispaper to function both as a foundational primer for newcomers and a catalystfor continued innovation within this dynamic field of study.</description>
      <author>example@mail.com (Jiyue Jiang, Zikang Wang, Yuheng Shan, Heyan Chai, Jiayi Li, Zixian Ma, Xinrui Zhang, Yu Li)</author>
      <guid isPermaLink="false">2503.04135v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing SAM with Efficient Prompting and Preference Optimization for Semi-supervised Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2503.04639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究介绍了一种增强的Segment Anything Model (SAM)框架，旨在通过无监督生成的高效标注提示来改进医疗图像分割。该方法利用对比语言-图像预训练和视觉问答捕获关键信息，并采用直接偏好优化技术设计策略以提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;基础模型如Segment Anything Model（SAM）在医学成像领域取得了进展，支持多种下游任务。然而，这些模型依赖于大型标注数据集或专家提供的提示。&lt;h4&gt;目的&lt;/h4&gt;提出一个增强的SAM框架来解决现有方法对大规模标注数据和复杂专业知识的高度依赖问题。&lt;h4&gt;方法&lt;/h4&gt;使用无监督生成的高效注释提示、对比语言图像预训练以及视觉问答技术。采用直接偏好优化技术，通过虚拟标注器模拟的人类注解过程提供简单的评分或排名以指导模型学习。&lt;h4&gt;主要发现&lt;/h4&gt;该框架在低标注数据情况下表现良好，在肺部分割、乳腺肿瘤分割和跨模态器官分割等任务上达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的增强SAM框架能够利用少量的标注信息生成高质量的分割结果，减少了对大量专家指导的需求，并为医学影像分析提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;基础模型如Segment Anything Model（SAM）在医疗图像分割中越来越受欢迎，支持多种下游任务。然而，这些模型本质上是监督型的，仍然依赖于大型注释数据集或由专家提供的提示。传统的技术如主动学习虽能缓解这一问题但仍有局限性，并且仍需要持续的人工参与和复杂的领域知识来改进标签或确定奖励标准的真实情况。为了应对这些挑战，我们提出了一种增强的SAM框架，该框架利用完全无监督生成的高效注释提示，同时通过对比语言-图像预训练和视觉问答捕捉关键语义、位置及形状信息。采用直接偏好优化技术设计最佳策略使模型能够在虚拟标注器提供的简单评分或排名指导下产生高保真度分割结果。我们的框架在肺部分割、乳腺肿瘤分割以及跨模态成像中的器官分割等任务中取得了最先进的表现，证明了其在低注释数据场景下的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models such as the Segment Anything Model (SAM) are gainingtraction in medical imaging segmentation, supporting multiple downstream tasks.However, such models are supervised in nature, still relying on large annotateddatasets or prompts supplied by experts. Conventional techniques such as activelearning to alleviate such limitations are limited in scope and stillnecessitate continuous human involvement and complex domain knowledge for labelrefinement or establishing reward ground truth. To address these challenges, wepropose an enhanced Segment Anything Model (SAM) framework that utilizesannotation-efficient prompts generated in a fully unsupervised fashion, whilestill capturing essential semantic, location, and shape information throughcontrastive language-image pretraining and visual question answering. We adoptthe direct preference optimization technique to design an optimal policy thatenables the model to generate high-fidelity segmentations with simple ratingsor rankings provided by a virtual annotator simulating the human annotationprocess. State-of-the-art performance of our framework in tasks such as lungsegmentation, breast tumor segmentation, and organ segmentation across variousmodalities, including X-ray, ultrasound, and abdominal CT, justifies itseffectiveness in low-annotation data scenarios.</description>
      <author>example@mail.com (Aishik Konwer, Zhijian Yang, Erhan Bas, Cao Xiao, Prateek Prasanna, Parminder Bhatia, Taha Kass-Hout)</author>
      <guid isPermaLink="false">2503.04639v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive Branching Tree Search</title>
      <link>http://arxiv.org/abs/2503.04412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear at ICLR 2025 Workshop on Foundation Models in the Wild&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的推理框架AB-MCTS，在复杂编码和工程任务上通过外部反馈信号进行多轮探索与优化，显著优于重复抽样和标准MCTS。&lt;h4&gt;背景&lt;/h4&gt;研究表明增加推理时间的计算可以提高大规模语言模型（LLMs）的推理能力。尽管多次采样是一种有效策略，但其不利用外部反馈信号进行细化改进。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的推理框架AB-MCTS，在复杂任务中结合大规模语言模型的响应多样性与多轮解决方案优化的能力。&lt;h4&gt;方法&lt;/h4&gt;提出适应性分支蒙特卡洛树搜索（AB-MCTS）框架。该框架在每次搜索节点上根据外部反馈信号动态决定是否扩展新候选输出或重新审视现有输出，以实现更有效的推理时间缩放。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示AB-MCTS在复杂编码和工程任务中始终优于重复抽样和标准MCTS方法。&lt;h4&gt;结论&lt;/h4&gt;结合大规模语言模型的响应多样性与多轮解决方案优化是有效扩大推理时间规模的关键。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，增加推断时的计算量可以显著提升大语言模型（LLMs）的推理能力。虽然重复采样是一种非常有效的策略，但它没有利用外部反馈信号进行细化改进，而这些在编码等任务中往往可用。本文提出了一种新的推断框架——自适应分支蒙特卡洛树搜索(AB-MCTS)，它能够根据外部反馈信号，在每次搜索节点上动态决定是否扩展新候选输出或重新审视现有输出，从而实现更有效的推理时间缩放。实验结果显示，AB-MCTS在复杂编码和工程任务中始终优于重复抽样和标准MCTS方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances demonstrate that increasing inference-time computation cansignificantly boost the reasoning capabilities of large language models (LLMs).Although repeated sampling (i.e., generating multiple candidate outputs) is ahighly effective strategy, it does not leverage external feedback signals forrefinement, which are often available in tasks like coding. In this work, wepropose $\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, anovel inference-time framework that generalizes repeated sampling withprincipled multi-turn exploration and exploitation. At each node in the searchtree, AB-MCTS dynamically decides whether to "go wider" by expanding newcandidate responses or "go deeper" by revisiting existing ones based onexternal feedback signals. We evaluate our method on complex coding andengineering tasks using frontier models. Empirical results show that AB-MCTSconsistently outperforms both repeated sampling and standard MCTS, underscoringthe importance of combining the response diversity of LLMs with multi-turnsolution refinement for effective inference-time scaling.</description>
      <author>example@mail.com (Kou Misaki, Yuichi Inoue, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba)</author>
      <guid isPermaLink="false">2503.04412v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A Phylogenetic Approach to Genomic Language Modeling</title>
      <link>http://arxiv.org/abs/2503.03773v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;基因组语言模型(gLMs)在识别哺乳动物基因组中的进化保守元件方面取得了一些适度的成功。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的训练gLM的框架，该框架使用多物种全基因组比对来显式建模核苷酸进化。&lt;h4&gt;方法&lt;/h4&gt;将对齐数据整合到损失函数中用于模型训练，但预测时不需要对齐数据。&lt;h4&gt;主要发现&lt;/h4&gt;应用此框架训练得到的PhyloGPN模型在仅用单个序列的情况下能够出色地预测功能破坏性变异，并表现出强大的迁移学习能力。&lt;h4&gt;翻译&lt;/h4&gt;基因组语言模型(gLMs)在识别哺乳动物基因组中的进化保守元件方面取得了一些适度的成功。为了应对这一挑战，我们引入了一个新的训练gLM的框架，该框架使用多物种全基因组比对来显式建模核苷酸进化。我们的方法将对齐数据整合到损失函数中用于模型训练，但预测时不需要对齐数据，从而增强了模型的应用性。我们将此框架应用于训练PhyloGPN模型，该模型在仅用单个序列的情况下能够出色地预测功能破坏性变异，并表现出强大的迁移学习能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Genomic language models (gLMs) have shown mostly modest success inidentifying evolutionarily constrained elements in mammalian genomes. Toaddress this issue, we introduce a novel framework for training gLMs thatexplicitly models nucleotide evolution on phylogenetic trees using multispecieswhole-genome alignments. Our approach integrates an alignment into the lossfunction during training but does not require it for making predictions,thereby enhancing the model's applicability. We applied this framework to trainPhyloGPN, a model that excels at predicting functionally disruptive variantsfrom a single sequence alone and demonstrates strong transfer learningcapabilities.</description>
      <author>example@mail.com (Carlos Albors, Jianan Canal Li, Gonzalo Benegas, Chengzhong Ye, Yun S. Song)</author>
      <guid isPermaLink="false">2503.03773v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A Generalist Cross-Domain Molecular Learning Framework for Structure-Based Drug Discovery</title>
      <link>http://arxiv.org/abs/2503.04362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为BIT（Biomolecular Interaction Transformer）的通用基础模型，该模型能够编码包括小分子、蛋白质和蛋白质-配体复合物在内的多种生物化学实体及其2D和3D结构。&lt;h4&gt;背景&lt;/h4&gt;基于结构药物发现(SBDD)利用目标蛋白的详细物理结构来开发新药。最近，在生物分子预训练模型上的进展在药物发现等多个生化应用中取得了显著成功，但大多数方法主要关注小分子或蛋白质的特性而忽略了它们之间的结合相互作用。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，提出了一种能够编码多类型生物化学实体并捕捉其精细相互作用的通用基础模型BIT。&lt;h4&gt;方法&lt;/h4&gt;BIT通过引入Mixture-of-Domain-Experts (MoDE)处理来自不同生化领域的生物分子，并使用Mixture-of-Structure-Experts (MoSE)来捕获分子结构中的位置依赖性。此外，通过对共享Transformer骨干进行跨域预训练和统一的自我监督去噪任务进一步优化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种基准测试中，BIT在下游任务如结合亲和力预测、基于结构的虚拟筛选及分子属性预测方面表现优异。&lt;h4&gt;结论&lt;/h4&gt;BIT展示了其卓越的能力，能够有效解决SBDD领域中的关键问题，并为药物开发提供了强有力的工具。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要详细描述了用于生物分子相互作用的通用基础模型BIT的创建和应用。此模型利用预训练技术处理多类型生物化学实体并捕捉它们之间的复杂关系，在多种生化任务中表现出色，证明其在基于结构的新药研发中的潜在价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Structure-based drug discovery (SBDD) is a systematic scientific process thatdevelops new drugs by leveraging the detailed physical structure of the targetprotein. Recent advancements in pre-trained models for biomolecules havedemonstrated remarkable success across various biochemical applications,including drug discovery and protein engineering. However, in most approaches,the pre-trained models primarily focus on the characteristics of either smallmolecules or proteins, without delving into their binding interactions whichare essential cross-domain relationships pivotal to SBDD. To fill this gap, wepropose a general-purpose foundation model named BIT (an abbreviation forBiomolecular Interaction Transformer), which is capable of encoding a range ofbiochemical entities, including small molecules, proteins, and protein-ligandcomplexes, as well as various data formats, encompassing both 2D and 3Dstructures. Specifically, we introduce Mixture-of-Domain-Experts (MoDE) tohandle the biomolecules from diverse biochemical domains andMixture-of-Structure-Experts (MoSE) to capture positional dependencies in themolecular structures. The proposed mixture-of-experts approach enables BIT toachieve both deep fusion and domain-specific encoding, effectively capturingfine-grained molecular interactions within protein-ligand complexes. Then, weperform cross-domain pre-training on the shared Transformer backbone viaseveral unified self-supervised denoising tasks. Experimental results onvarious benchmarks demonstrate that BIT achieves exceptional performance indownstream tasks, including binding affinity prediction, structure-basedvirtual screening, and molecular property prediction.</description>
      <author>example@mail.com (Yiheng Zhu, Mingyang Li, Junlong Liu, Kun Fu, Jiansheng Wu, Qiuyi Li, Mingze Yin, Jieping Ye, Jian Wu, Zheng Wang)</author>
      <guid isPermaLink="false">2503.04362v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation Model Knowledge</title>
      <link>http://arxiv.org/abs/2503.04357v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;单细胞RNA测序（scRNA-seq）技术已经对数以亿计的人类细胞进行了分析，这些细胞涵盖了不同的器官、疾病状态和发展阶段。然而，原始的测序数据面临着高维稀疏性、批次效应噪声、类别不平衡以及数据规模不断增加等问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些问题，提出了一种方法来简化和优化scRNA-seq数据集处理。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '提出了一个基于潜在代码的单细胞测序数据集精简框架（scDD），该框架将基础模型的知识和原始数据集信息转化到紧凑的潜在空间中，并通过生成器生成合成的scRNA-seq数据集以替代原有数据。', '第二部分': '提出了一种一步条件扩散生成器（SCDG），它通过单步反向传播来优化数据精简质量，避免了多步反向传播导致的梯度衰减。同时，SCDG保证了合成数据集中scRNA-seq数据特性和类间区分性。', '评价': '提出了一个全面基准用于评估不同数据分析任务中scRNA-seq数据集精简的效果。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的这种方法在平均任务上比现有最优方法有7.61%的绝对改进和15.70%的相对改进。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法能够有效地解决多中心知识传递、数据融合及scRNA-seq数据集之间的交叉验证问题，同时提高了合成数据的质量。&lt;h4&gt;翻译&lt;/h4&gt;单细胞RNA测序（scRNA-seq）技术已经对数以亿计的人类细胞进行了分析……所提出的这种方法在平均任务上比现有最优方法有7.61%的绝对改进和15.70%的相对改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds ofmillions of human cells across organs, diseases, development and perturbationsto date. However, the high-dimensional sparsity, batch effect noise, categoryimbalance, and ever-increasing data scale of the original sequencing data posesignificant challenges for multi-center knowledge transfer, data fusion, andcross-validation between scRNA-seq datasets. To address these barriers, (1) wefirst propose a latent codes-based scRNA-seq dataset distillation frameworknamed scDD, which transfers and distills foundation model knowledge andoriginal dataset information into a compact latent space and generatessynthetic scRNA-seq dataset by a generator to replace the original dataset.Then, (2) we propose a single-step conditional diffusion generator named SCDG,which perform single-step gradient back-propagation to help scDD optimizedistillation quality and avoid gradient decay caused by multi-stepback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristicsand inter-class discriminability of the synthetic dataset through flexibleconditional control and generation quality assurance. Finally, we propose acomprehensive benchmark to evaluate the performance of scRNA-seq datasetdistillation in different data analysis tasks. It is validated that ourproposed method can achieve 7.61% absolute and 15.70% relative improvement overprevious state-of-the-art methods on average task.</description>
      <author>example@mail.com (Zhen Yu, Jianan Han, Yang Liu, Qingchao Chen)</author>
      <guid isPermaLink="false">2503.04357v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>An Egocentric Vision-Language Model based Portable Real-time Smart Assistant</title>
      <link>http://arxiv.org/abs/2503.04250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;我们介绍了Vinci，这是一个为便携设备设计的实时、全面的人工智能辅助系统。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言系统通常依赖于特定硬件，并且难以实现实时功能和长时间视频流处理。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在多种设备上运行、无硬件限制并且能够提供高级功能如场景理解、时间定位等的便携式实时AI系统。&lt;h4&gt;方法&lt;/h4&gt;Vinci的核心是EgoVideo-VL模型，该模型结合了第一人称视角视觉基础模型和大型语言模型（LLM），并配备了记忆模块、生成模块和检索模块来增强其实用性。&lt;h4&gt;主要发现&lt;/h4&gt;在多个公开基准测试中展示了EgoVideo-VL的优越性能，并通过用户研究验证了Vinci的实际效果，突出了它的适应性和可用性。&lt;h4&gt;结论&lt;/h4&gt;希望Vinci能为便携式实时第一人称视角AI系统建立新的框架，提供上下文相关的、可操作的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们提出了一种名为Vinci的视觉语言系统，旨在为移动设备提供实时全面的人工智能辅助。该系统的中心是EgoVideo-VL模型，它结合了第一人称视角视觉基础模型和大型语言模型（LLM），提供了场景理解、时间定位、视频总结和未来规划等高级功能。为了提高其实用性，Vinci集成了一个记忆模块，用于实时处理长视频流并保留上下文历史；生成模块用来产生可视动作演示；检索模块则通过连接第一人称视角与第三人称视角提供相关技能学习视频。不同于依赖于特定硬件的现有系统，Vinci是无硬件限制的，支持在包括智能手机和可穿戴相机在内的各种设备上部署。我们首先展示了EgoVideo-VL模型在多个公开基准测试中的优越性能，突出了其视觉语言推理和上下文理解能力；然后通过一系列用户研究评估了Vinci的实际效果，强调了它的适应性和可用性。我们的目标是使Vinci成为一个新的便携式实时第一人称视角AI系统框架，赋予用户提供上下文相关且可操作见解的能力。包括前端、后端和模型在内的所有代码都可以在https://github.com/OpenGVLab/vinci上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Vinci, a vision-language system designed to provide real-time,comprehensive AI assistance on portable devices. At its core, Vinci leveragesEgoVideo-VL, a novel model that integrates an egocentric vision foundationmodel with a large language model (LLM), enabling advanced functionalities suchas scene understanding, temporal grounding, video summarization, and futureplanning. To enhance its utility, Vinci incorporates a memory module forprocessing long video streams in real time while retaining contextual history,a generation module for producing visual action demonstrations, and a retrievalmodule that bridges egocentric and third-person perspectives to providerelevant how-to videos for skill acquisition. Unlike existing systems thatoften depend on specialized hardware, Vinci is hardware-agnostic, supportingdeployment across a wide range of devices, including smartphones and wearablecameras. In our experiments, we first demonstrate the superior performance ofEgoVideo-VL on multiple public benchmarks, showcasing its vision-languagereasoning and contextual understanding capabilities. We then conduct a seriesof user studies to evaluate the real-world effectiveness of Vinci, highlightingits adaptability and usability in diverse scenarios. We hope Vinci canestablish a new framework for portable, real-time egocentric AI systems,empowering users with contextual and actionable insights. Including thefrontend, backend, and models, all codes of Vinci are available athttps://github.com/OpenGVLab/vinci.</description>
      <author>example@mail.com (Yifei Huang, Jilan Xu, Baoqi Pei, Yuping He, Guo Chen, Mingfang Zhang, Lijin Yang, Zheng Nie, Jinyao Liu, Guoshun Fan, Dechen Lin, Fang Fang, Kunpeng Li, Chang Yuan, Xinyuan Chen, Yaohui Wang, Yali Wang, Yu Qiao, Limin Wang)</author>
      <guid isPermaLink="false">2503.04250v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DuCos: Duality Constrained Depth Super-Resolution via Foundation Model</title>
      <link>http://arxiv.org/abs/2503.04171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了DuCos，一种基于拉格朗日对偶理论的深度超分辨率框架，通过灵活集成多种约束和重建目标来提高准确性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;当前深度超分辨率方法在处理多样化场景时存在泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个新颖的方法——DuCos，旨在利用基础模型作为提示信息，显著提升不同场景下的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;DuCos框架设计了两个关键组件：相关融合（CF）和梯度调节（GR），用于实现精细的几何对齐、有效融合深度特征与提示特征，并通过拉格朗日约束项无缝嵌入这些提示。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相比于现有的最先进方法，DuCos在精度、鲁棒性和泛化性方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;杜科斯（DuCos）框架为深度超分辨率提供了一个强大的新视角，并将在未来的研究中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了基于拉格朗日对偶理论的新型深度超分辨率框架——杜科斯，通过灵活集成多种约束和重建目标来增强准确性和鲁棒性。该方法是首个显著提高基础模型作为提示信息时在多样化场景中的泛化能力的方法。提示设计包含相关融合（CF）与梯度调节（GR），前者促进了精细的几何对齐及深度特征与提示的有效融合，后者通过要求其深度预测与从基础模型衍生出的边缘清晰的地图保持一致来优化深度预测。重要的是，这些提示被无缝嵌入到拉格朗日约束项中，形成了协同且原理明确的框架。广泛实验表明，杜科斯超越了现有的最先进方法，在精度、鲁棒性和泛化性方面均表现优异。源代码及预训练模型将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce DuCos, a novel depth super-resolution framework grounded inLagrangian duality theory, offering a flexible integration of multipleconstraints and reconstruction objectives to enhance accuracy and robustness.Our DuCos is the first to significantly improve generalization across diversescenarios with foundation models as prompts. The prompt design consists of twokey components: Correlative Fusion (CF) and Gradient Regulation (GR). CFfacilitates precise geometric alignment and effective fusion between prompt anddepth features, while GR refines depth predictions by enforcing consistencywith sharp-edged depth maps derived from foundation models. Crucially, theseprompts are seamlessly embedded into the Lagrangian constraint term, forming asynergistic and principled framework. Extensive experiments demonstrate thatDuCos outperforms existing state-of-the-art methods, achieving superioraccuracy, robustness, and generalization. The source codes and pre-trainedmodels will be publicly available.</description>
      <author>example@mail.com (Zhiqiang Yan, Zhengxue Wang, Haoye Dong, Jun Li, Jian Yang, Gim Hee Lee)</author>
      <guid isPermaLink="false">2503.04171v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>VLA Model-Expert Collaboration for Bi-directional Manipulation Learning</title>
      <link>http://arxiv.org/abs/2503.04163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个视觉-语言-行动模型与专家协作的框架，该框架通过引入少量的专家操作来提高VLA模型在多任务操纵中的性能和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉-语言-行动（VLA）模型虽然已经取得了一些进展，但在机器人操纵的多个任务上仍存在泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的协作框架，以提升VLA模型的能力，并减轻专家的工作负担。&lt;h4&gt;方法&lt;/h4&gt;该研究设计了一个VLA模型和专家之间的协作框架，利用有限数量的专家动作来改进VLA模型的表现。此过程还收集了用于进一步训练VLA模型的数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在多种VLA模型上应用所提出的系统可以显著提高操作的成功率，并且通过脑机接口（BCI）验证表明该协作框架能够提升低速行动系统的效率。&lt;h4&gt;结论&lt;/h4&gt;这种双向学习循环增强了协作系统的整体性能，为基于基础模型的人机交互开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言-行动(VLA) 模型的出现催生了机器人操作的基础模型。尽管这些模型已经取得了显著的进步，但在多任务操纵中的泛化能力仍然有限。这项研究提出了一种VLA模型与专家协作框架，利用少量的专家动作来增强VLA模型的表现。这种方法在减少手动操作工作量的同时提高了VLA模型的可靠性和泛化性，并且收集到的操作数据可以进一步细化VLA模型。同时，人类参与者在此过程中也提升了他们的技能。这种双向学习循环推动了整个协作系统的性能提升。实验结果显示，在各种VLA模型上的协同操纵和学习中所提出的系统表现出色，任务成功率显著提高。此外，使用脑机接口(BCI)验证表明该合作系统通过引入VLA模型在操作期间提高了低速动作系统的效率。这些有前景的结果为机器人基础模型时代的人机交互开辟了新的途径。(项目网站: https://aoqunjin.github.io/Expert-VLA/)&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of vision-language-action (VLA) models has given rise tofoundation models for robot manipulation. Although these models have achievedsignificant improvements, their generalization in multi-task manipulationremains limited. This study proposes a VLA model-expert collaboration frameworkthat leverages a limited number of expert actions to enhance VLA modelperformance. This approach reduces expert workload relative to manual operationwhile simultaneously improving the reliability and generalization of VLAmodels. Furthermore, manipulation data collected during collaboration canfurther refine the VLA model, while human participants concurrently enhancetheir skills. This bi-directional learning loop boosts the overall performanceof the collaboration system. Experimental results across various VLA modelsdemonstrate the effectiveness of the proposed system in collaborativemanipulation and learning, as evidenced by improved success rates across tasks.Additionally, validation using a brain-computer interface (BCI) indicates thatthe collaboration system enhances the efficiency of low-speed action systems byinvolving VLA model during manipulation. These promising results pave the wayfor advancing human-robot interaction in the era of foundation models forrobotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)</description>
      <author>example@mail.com (Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou)</author>
      <guid isPermaLink="false">2503.04163v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Rebalanced Multimodal Learning with Data-aware Unimodal Sampling</title>
      <link>http://arxiv.org/abs/2503.03792v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对多模态学习过程中由于数据采样导致的模式不平衡问题，本文提出了一种新的方法——Data-aware Unimodal Sampling (DUS)，该方法通过动态调整每一轮迭代中采样的数据量来缓解模式不平衡。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态学习(MML)方法主要从模型学习的角度尝试平衡每个模式的优化过程。然而，几乎所有现有方法忽略了由单模态数据采样引起的模式不平衡问题，即等比例采样通常会导致信息内容上的差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MML方法，通过动态缓解采样过程中产生的多模态不平衡来改进现有的学习策略。&lt;h4&gt;方法&lt;/h4&gt;引入了一种累积模式偏差的方法来监控多模态学习过程，并基于此提出了启发式和基于强化学习的数据感知单模态采样方法以自适应地决定每次迭代中采样的数据量。同时，该方法可以无缝集成到几乎所有的现有MML方法中。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明DUS相对于各种先进的基线方法，在性能上取得了最佳表现。&lt;h4&gt;结论&lt;/h4&gt;通过调整单模态数据的采样策略以缓解多模态不平衡问题，本文提出的方法能够显著提高多模态学习的整体效果。&lt;h4&gt;翻译&lt;/h4&gt;为了解决由于模式失衡引起的数据模态学习退化问题，现有的多模态学习方法主要试图从模型学习的角度平衡每个模态的优化过程。然而，几乎所有现有方法都忽略了由单模态数据采样导致的模式不平衡问题，即等比例采样通常会导致信息内容上的差异，从而造成模态失衡。因此，在本文中，我们提出了一种新的多模态学习方法——Data-aware Unimodal Sampling (DUS)，旨在动态缓解由于抽样引起的模式不平衡问题。具体而言，我们首先提出了一个新的累积模式偏差来监控多模态学习过程。根据学习状态，我们基于启发式和强化学习（RL）的数据感知单模态采样策略自适应地确定每次迭代中抽取数据的数量，从而从采样的角度缓解模式失衡。同时，我们的方法可以无缝集成到几乎所有的现有多模态学习方法中作为插件。实验表明DUS通过与各种先进的基线进行比较，在性能上取得了最佳表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To address the modality learning degeneration caused by modality imbalance,existing multimodal learning~(MML) approaches primarily attempt to balance theoptimization process of each modality from the perspective of model learning.However, almost all existing methods ignore the modality imbalance caused byunimodal data sampling, i.e., equal unimodal data sampling often results indiscrepancies in informational content, leading to modality imbalance.Therefore, in this paper, we propose a novel MML approach called\underline{D}ata-aware \underline{U}nimodal \underline{S}ampling~(\method),which aims to dynamically alleviate the modality imbalance caused by sampling.Specifically, we first propose a novel cumulative modality discrepancy tomonitor the multimodal learning process. Based on the learning status, wepropose a heuristic and a reinforcement learning~(RL)-based data-aware unimodalsampling approaches to adaptively determine the quantity of sampled data ateach iteration, thus alleviating the modality imbalance from the perspective ofsampling. Meanwhile, our method can be seamlessly incorporated into almost allexisting multimodal learning approaches as a plugin. Experiments demonstratethat \method~can achieve the best performance by comparing with diversestate-of-the-art~(SOTA) baselines.</description>
      <author>example@mail.com (Qingyuan Jiang, Zhouyang Chi, Xiao Ma, Qirong Mao, Yang Yang, Jinhui Tang)</author>
      <guid isPermaLink="false">2503.03792v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</title>
      <link>http://arxiv.org/abs/2503.03190v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D问答（3D QA）需要模型全面理解由文本描述的所处三维场景，然后在该情况下对其周围环境进行推理并回答问题。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常依赖于从纯3D点云中的全局场景感知，并忽视了来自多视角图像的丰富局部纹理细节的重要性。此外，在将3D点云与多视图图像对齐时，由于相机姿态的内在噪声和复杂的遮挡，存在显著的特征退化和降低的特征鲁棒性问题。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一个双视觉场景感知网络（DSPNet），以全面整合多视角和点云特征来提高3D QA中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;我们的文本引导的多视图融合（TGMF）模块优先考虑与文本语义内容紧密匹配的图像视图。为了自适应地融合反向投影的多视角图像与点云特征，我们设计了自适应双视觉感知（ADVP）模块，增强三维场景的理解能力。此外，我们的跨模态上下文引导推理（MCGR）模块通过整合视觉和语言模式中的上下文信息来促进稳健推理。&lt;h4&gt;主要发现&lt;/h4&gt;在SQA3D和ScanQA数据集上的实验结果证明了我们DSPNet的优越性。&lt;h4&gt;结论&lt;/h4&gt;代码将在https://github.com/LZ-CH/DSPNet上提供。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种新的方法来解决现有的3D QA模型中存在的问题，即忽视多视角图像中的局部纹理细节和在将点云与图像对齐时的特征鲁棒性问题。通过设计TGMF、ADVP和MCGR模块，我们的DSPNet能够在3D QA任务中表现出色，并提供了代码以供其他研究人员使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/LZ-CH/DSPNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Question Answering (3D QA) requires the model to comprehensivelyunderstand its situated 3D scene described by the text, then reason about itssurrounding environment and answer a question under that situation. However,existing methods usually rely on global scene perception from pure 3D pointclouds and overlook the importance of rich local texture details frommulti-view images. Moreover, due to the inherent noise in camera poses andcomplex occlusions, there exists significant feature degradation and reducedfeature robustness problems when aligning 3D point cloud with multi-viewimages. In this paper, we propose a Dual-vision Scene Perception Network(DSPNet), to comprehensively integrate multi-view and point cloud features toimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) moduleprioritizes image views that closely match the semantic content of the text. Toadaptively fuse back-projected multi-view images with point cloud features, wedesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scenecomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)module facilitates robust reasoning by integrating contextual informationacross visual and linguistic modalities. Experimental results on SQA3D andScanQA datasets demonstrate the superiority of our DSPNet. Codes will beavailable at https://github.com/LZ-CH/DSPNet.</description>
      <author>example@mail.com (Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin)</author>
      <guid isPermaLink="false">2503.03190v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>TimeFound: A Foundation Model for Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2503.04118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;TimeFound是一个基于编码器-解码器的变压器模型，专门用于零样本时间序列预测。&lt;h4&gt;背景&lt;/h4&gt;当前时间序列数据来自不同的领域，需要一种能捕捉各种复杂模式的方法。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够处理不同领域的时间序列数据并进行零样本预测的基础模型TimeFound。&lt;h4&gt;方法&lt;/h4&gt;采用多分辨率分块策略来适应不同规模的数据，并在大型时序语料库上进行预训练。该语料库包括现实世界和合成时间序列数据，以两种参数大小（200M和710M）的模型进行了预训练。&lt;h4&gt;主要发现&lt;/h4&gt;TimeFound在多个领域和不同的预测范围上的未见数据集上取得了优于或与现有最佳的时间序列基础模型相当的零样本预测性能。&lt;h4&gt;结论&lt;/h4&gt;TimeFound展示了其在处理多样时间序列数据时的有效性和广泛适用性，是未来研究的一个有希望的方向。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了TimeFound，这是一种基于编码器-解码器变压器的时间系列基础模型，用于即插即用的零样本预测。为了应对不同领域的时间序列数据，TimeFound采用了一种多分辨率分块策略来捕捉多个时间尺度上的复杂模式。我们在一个包含真实世界和合成数据集的大规模时间序列语料库上对我们的模型进行了两种大小（200M和710M参数）的预训练。在一系列跨不同领域的未见数据集中，我们通过实验发现TimeFound可以实现优于或与最先进的时间序列基础模型相当的零样本预测性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present TimeFound, an encoder-decoder transformer-based time seriesfoundation model for out-of-the-box zero-shot forecasting. To handle timeseries data from various domains, TimeFound employs a multi-resolution patchingstrategy to capture complex temporal patterns at multiple scales. We pre-trainour model with two sizes (200M and 710M parameters) on a large time-seriescorpus comprising both real-world and synthetic datasets. Over a collection ofunseen datasets across diverse domains and forecasting horizons, our empiricalevaluations suggest that TimeFound can achieve superior or competitivezero-shot forecasting performance, compared to state-of-the-art time seriesfoundation models.</description>
      <author>example@mail.com (Congxi Xiao, Jingbo Zhou, Yixiong Xiao, Xinjiang Lu, Le Zhang, Hui Xiong)</author>
      <guid isPermaLink="false">2503.04118v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</title>
      <link>http://arxiv.org/abs/2503.02448v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了NodeNAS和MNNAS两种新颖的方法，旨在通过解耦节点拓扑结构和图分布来为不同类型的节点定制独特的聚合方法，解决现有GraphNAS方法在少量或单一训练图上的泛化能力差的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的GraphNAS方法未能考虑到不同类型图之间的分布模式，并且依赖大量的训练数据才能有效发现最优的架构映射关系。这使得它们难以处理稀疏或者单个样本的数据集，尤其是在面对分布外（OOD）数据时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出NodeNAS和MNNAS两种方法来解决上述问题，以提高图神经网络在有限或独特训练数据上的性能，并增强其对OOD数据的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. NodeNAS：通过解耦节点拓扑结构与图分布，为不同类型的节点设计独特的聚合策略。2. MNNAS：引入了适应性聚集注意机制，学习一种具有良好泛化的节点特定架构定制器。该模型还扩展了搜索空间的垂直深度，支持跨多个维度的同时节点特定架构自定义。&lt;h4&gt;主要发现&lt;/h4&gt;1. NodeNAS和MNNAS方法能够有效提高图神经网络在有限或独特训练数据上的性能。2. MNNAS通过模拟具有变化排列性的节点度幂律分布来编码结构不变信息，并利用这些信息引导跨各个维度的架构定制，从而实现更好的OOD泛化。&lt;h4&gt;结论&lt;/h4&gt;MNNAS方法在监督和非监督任务上均表现出色，超越了现有的最新算法，在处理OOD数据时表现尤为突出。这表明该模型具有良好的泛化能力和应用前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要：图神经架构搜索（GraphNAS）已经在缓解由于分布变化导致的图神经网络性能下降方面展示了其优势。最近的方法通过在定制架构之间进行权重共享，生成了针对每张图的独特GNN架构，并且端到端地进行了优化。然而，现有的GraphNAS方法没有考虑到不同图形之间的分布模式，并严重依赖于大量训练数据。当面对稀疏或单一的训练图时，这些方法难以发现最优的映射关系，从而无法泛化至分布外（OOD）的数据上。在本文中，我们提出了节点特定图神经架构搜索(NodeNAS)，其目标是通过分解节点拓扑和图分布来为不同节点定制独特的聚合方法，并且在有限数据集的情况下实现这一目的。此外，我们还提出了自适应聚集注意力的多维度NodeNAS（MNNAS）方法，该方法学习了一个具有良好泛化能力的节点特定架构定制器。具体而言，我们扩展了搜索空间的垂直深度，支持同时跨多个维度进行节点特定架构定制。并且我们建模了在不同排列性下的节点度幂律分布，编码结构不变信息以指导每个维度上的架构定制。广泛的监督和非监督任务实验表明，MNNAS超越了现有的最先进技术，并且实现了优秀的OOD泛化效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural architecture search (GraphNAS) has demonstrated advantages inmitigating performance degradation of graph neural networks (GNNs) due todistribution shifts. Recent approaches introduce weight sharing across tailoredarchitectures, generating unique GNN architectures for each graph end-to-end.However, existing GraphNAS methods do not account for distribution patternsacross different graphs and heavily rely on extensive training data. Withsparse or single training graphs, these methods struggle to discover optimalmappings between graphs and architectures, failing to generalize toout-of-distribution (OOD) data. In this paper, we propose node-specific graphneural architecture search(NodeNAS), which aims to tailor distinct aggregationmethods for different nodes through disentangling node topology and graphdistribution with limited datasets. We further propose adaptive aggregationattention based Multi-dim NodeNAS method(MNNAS), which learns an node-specificarchitecture customizer with good generalizability. Specifically, we extend thevertical depth of the search space, supporting simultaneous node-specificarchitecture customization across multiple dimensions. Moreover, we model thepower-law distribution of node degrees under varying assortativity, encodingstructure invariant information to guide architecture customization across eachdimension. Extensive experiments across supervised and unsupervised tasksdemonstrate that MNNAS surpasses state-of-the-art algorithms and achievesexcellent OOD generalization.</description>
      <author>example@mail.com (Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu)</author>
      <guid isPermaLink="false">2503.02448v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with Sub-Class Exploration and Prompt Affinity Mining</title>
      <link>http://arxiv.org/abs/2503.04106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'研究背景': '在视觉任务中，基础模型取得了显著进展。最近的一些工作利用分割任何东西（SAM）模型来提升医学图像中的分割性能。', '现有方法的局限性': '大多数现有方法集中在完全监督的方式下训练适配器，以便使用大量像素级标注的医疗图像进行微调。', '研究目的': '为了减少标记成本，本论文探讨了一种基于弱监督的SAM模型——WeakMedSAM。', '主要贡献': {'子类探索模块': '引入一个子类探索模块以减轻医学图像中的严重共现问题，并学习准确的特征表示。', '提示亲和性挖掘模块': '利用SAM的提示能力，提出提示亲和性挖掘模块来改善类别激活图的质量。', '通用性': '所提方法可以应用于任何类似SAM的基础模型。实验中使用了SAMUS和EfficientSAM进行测试。'}, '实验结果': '在BraTS2019、AbdomenCT-1K及MSD心脏数据集这三个常用基准数据集上的实验证明了所提出方法的有效性。', '结论': '我们的研究展示了弱监督的医学图像分割模型WeakMedSAM在减少标注成本方面的潜力。相关代码可在GitHub上获得（https://github.com/wanghr64/WeakMedSAM）。'}&lt;h4&gt;翻译&lt;/h4&gt;我们见证了视觉任务中基础模型的显著进展，最近的一些工作利用了分割任何东西(SAM)模型来提升医学图像中的分割性能，这些方法大多集中在完全监督的方式下训练适配器，并使用大量像素级标注的医疗图像进行微调。为了减少标签成本，在本论文中，我们探讨了一种基于弱监督的新颖SAM模型——WeakMedSAM。我们的模型包含两个模块：1）为减轻医学图像中的严重共现问题，引入一个子类探索模块来学习准确的特征表示；2）利用SAM的提示能力，提出一种提示亲和性挖掘模块以改善类别激活图的质量。我们所提方法可以应用于任何类似SAM的基础模型，并且我们在实验中使用了SAMUS和EfficientSAM进行测试。在BraTS2019、AbdomenCT-1K及MSD心脏数据集上的实验证明，我们的WeakMedSAM有显著的效果。我们的代码可以在https://github.com/wanghr64/WeakMedSAM获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have witnessed remarkable progress in foundation models in vision tasks.Currently, several recent works have utilized the segmenting anything model(SAM) to boost the segmentation performance in medical images, where most ofthem focus on training an adaptor for fine-tuning a large amount of pixel-wiseannotated medical images following a fully supervised manner. In this paper, toreduce the labeling cost, we investigate a novel weakly-supervised SAM-basedsegmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAMcontains two modules: 1) to mitigate severe co-occurrence in medical images, asub-class exploration module is introduced to learn accurate featurerepresentations. 2) to improve the quality of the class activation maps, ourprompt affinity mining module utilizes the prompt capability of SAM to obtainan affinity map for random-walk refinement. Our method can be applied to anySAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. Theexperimental results on three popularly-used benchmark datasets, i.e., BraTS2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of ourproposed WeakMedSAM. Our code is available athttps://github.com/wanghr64/WeakMedSAM.</description>
      <author>example@mail.com (Haoran Wang, Lian Huai, Wenbin Li, Lei Qi, Xingqun Jiang, Yinghuan Shi)</author>
      <guid isPermaLink="false">2503.04106v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>GaussianGraph: 3D Gaussian-based Scene Graph Generation for Open-world Scene Understanding</title>
      <link>http://arxiv.org/abs/2503.04034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，3D高斯点阵（3DGS）技术在语义场景理解方面的进展显著提升，使自然语言查询能够定位场景中的对象。然而，现有方法主要集中在将压缩的CLIP特征嵌入到三维高斯分布中，导致物体分割精度较低且缺乏空间推理能力。&lt;h4&gt;背景&lt;/h4&gt;最近的3DGS技术改进了对语义场景的理解，但是现有的方法在将CLIP特征嵌入到3D Gaussians时存在问题，尤其是在对象分割准确性和空间推理方面存在局限性。&lt;h4&gt;目的&lt;/h4&gt;提出GaussianGraph框架以解决现有方法的不足，通过引入自适应语义聚类和场景图生成来增强基于3DGS的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;{'Control-Follow': '一种动态调整到场景尺度和特征分布的策略，避免了特性压缩，并显著提高了分割精度。', '2D基础模型提取': '通过整合从二维基础模型中提取的对象属性和空间关系来丰富场景表示。', '3D校正模块': '为了纠正不准确的空间关系，提出了3D校正模块，该模块通过空间一致性验证过滤掉不可信的关系，确保可靠地构建场景图。'}&lt;h4&gt;主要发现&lt;/h4&gt;GaussianGraph框架在三个数据集上的实验表明，在语义分割和对象定位任务中优于现有的最先进技术。&lt;h4&gt;结论&lt;/h4&gt;GaussianGraph为复杂的场景理解和交互提供了一个强有力的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最近在3D高斯点阵（3DGS）方面的发展显著改善了对三维场景的语义理解，使得通过自然语言查询可以精确定位场景中的对象。然而，现有的方法主要关注于将压缩后的CLIP特征嵌入到3D Gaussians中，这导致物体分割精度较低，并且缺乏空间推理的能力。为了克服这些限制，我们提出了一种新的框架GaussianGraph，它通过集成自适应语义聚类和场景图生成来增强基于3DGS的场景理解能力。我们引入了'控制-跟随'聚类策略，该策略可以动态地调整到不同规模的场景及特征分布，避免了特征压缩，并显著提高了分割精度。此外，我们将从2D基础模型中提取的对象属性和空间关系整合进场景表示之中，以丰富其描述信息。为了减少空间关系中的不准确性，我们还提出了3D校正模块，通过空间一致性验证来过滤掉不可信的关系，从而确保构建出可靠的场景图。在三个数据集上的广泛实验表明，在语义分割以及对象定位任务上GaussianGraph优于现有的最先进技术，提供了一个解决复杂场景理解和交互的有效方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in 3D Gaussian Splatting(3DGS) have significantlyimproved semantic scene understanding, enabling natural language queries tolocalize objects within a scene. However, existing methods primarily focus onembedding compressed CLIP features to 3D Gaussians, suffering from low objectsegmentation accuracy and lack spatial reasoning capabilities. To address theselimitations, we propose GaussianGraph, a novel framework that enhances3DGS-based scene understanding by integrating adaptive semantic clustering andscene graph generation. We introduce a "Control-Follow" clustering strategy,which dynamically adapts to scene scale and feature distribution, avoidingfeature compression and significantly improving segmentation accuracy.Additionally, we enrich scene representation by integrating object attributesand spatial relations extracted from 2D foundation models. To addressinaccuracies in spatial relationships, we propose 3D correction modules thatfilter implausible relations through spatial consistency verification, ensuringreliable scene graph construction. Extensive experiments on three datasetsdemonstrate that GaussianGraph outperforms state-of-the-art methods in bothsemantic segmentation and object grounding tasks, providing a robust solutionfor complex scene understanding and interaction.</description>
      <author>example@mail.com (Xihan Wang, Dianyi Yang, Yu Gao, Yufeng Yue, Yi Yang, Mengyin Fu)</author>
      <guid isPermaLink="false">2503.04034v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>All-atom Diffusion Transformers: Unified generative modelling of molecules and materials</title>
      <link>http://arxiv.org/abs/2503.03965v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为All-atom Diffusion Transformer (ADiT)的统一生成框架，用于同时生成周期性材料和非周期分子系统。&lt;h4&gt;背景&lt;/h4&gt;扩散模型是三维原子系统生成建模的标准工具。然而，对于不同类型的原子系统（如分子和材料），尽管基础物理相同，但其生成过程通常高度特定于目标系统。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时用于生成周期性材料和非周期分子系统的统一扩散框架。&lt;h4&gt;方法&lt;/h4&gt;ADiT包括一个自动编码器，它将分子和材料的统一、全原子表示映射到共享的潜在嵌入空间；以及一个训练来生成新的潜在嵌入以供自动解码器使用的新模型。实验表明，联合训练的ADiT能够生成现实且有效的分子和材料。&lt;h4&gt;主要发现&lt;/h4&gt;在QM9和MP20数据集上进行的实验显示，联合训练的ADiT可以产生真实且有效的分子及材料，并超越了专门针对分子或晶体的现有最佳方法结果。此外，通过使用标准变压器作为自动编码器和扩散模型，在训练和推理期间实现了显著的速度提升。&lt;h4&gt;结论&lt;/h4&gt;ADiT框架代表了一种向广泛适用的基础生成化学模型迈进的重要步骤，随着参数规模扩大至半亿级参数时性能可预期地提高。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的论文提出了一个名为All-atom Diffusion Transformer (ADiT)的新框架，该框架通过使用统一的自动编码器和扩散模型来同时生成分子和材料。研究展示了在多个数据集上的实验结果，并表明这种方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models are the standard toolkit for generative modelling of 3Datomic systems. However, for different types of atomic systems - such asmolecules and materials - the generative processes are usually highly specificto the target system despite the underlying physics being the same. Weintroduce the All-atom Diffusion Transformer (ADiT), a unified latent diffusionframework for jointly generating both periodic materials and non-periodicmolecular systems using the same model: (1) An autoencoder maps a unified,all-atom representations of molecules and materials to a shared latentembedding space; and (2) A diffusion model is trained to generate new latentembeddings that the autoencoder can decode to sample new molecules ormaterials. Experiments on QM9 and MP20 datasets demonstrate that jointlytrained ADiT generates realistic and valid molecules as well as materials,exceeding state-of-the-art results from molecule and crystal-specific models.ADiT uses standard Transformers for both the autoencoder and diffusion model,resulting in significant speedups during training and inference compared toequivariant diffusion models. Scaling ADiT up to half a billion parameterspredictably improves performance, representing a step towards broadlygeneralizable foundation models for generative chemistry. Open source code:https://github.com/facebookresearch/all-atom-diffusion-transformer</description>
      <author>example@mail.com (Chaitanya K. Joshi, Xiang Fu, Yi-Lun Liao, Vahe Gharakhanyan, Benjamin Kurt Miller, Anuroop Sriram, Zachary W. Ulissi)</author>
      <guid isPermaLink="false">2503.03965v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>SurgiSAM2: Fine-tuning a foundational model for surgical video anatomy segmentation and detection</title>
      <link>http://arxiv.org/abs/2503.03942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了模型SAM 2在手术场景理解中的表现，特别是在零样本和微调后的语义分割能力方面。通过使用五个公共数据集进行评估，并对图像编码器和掩码解码器进行了微调。&lt;h4&gt;背景&lt;/h4&gt;利用五个公开的数据集来评估和微调SAM 2模型，以使其更好地在手术视频/图像中分割解剖组织。&lt;h4&gt;目的&lt;/h4&gt;验证模型SAM 2在零样本条件下的语义分割能力，并通过微调进一步提高其性能。&lt;h4&gt;方法&lt;/h4&gt;对模型进行了不同规模数据集上的微调测试（从每个类别的50个训练样例增加到400个），并采用加权平均Dice系数(WMDC)来评估性能。同时与之前的最优结果进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;微调后的SurgiSAM 2模型相比基线SAM 2，以相对17.9%的WMDC提高显著提升了分割性能；在测试数据集上，在24/30（80%）类别中超越了先前的最佳方法。此外，该模型还有效泛化到未知器官类别的新场景。&lt;h4&gt;结论&lt;/h4&gt;SAM 2在手术场景分割方面表现出色，无论是零样本还是微调后的性能都超过了之前的方法，并且展示了自动化/半自动化标注管道的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于研究背景、目的、方法、主要发现和结论的详细信息，强调了模型在处理解剖学组织分割方面的改进以及它如何为未来手术应用中的自动注释流程铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Background: We evaluate SAM 2 for surgical scene understanding by examiningits semantic segmentation capabilities for organs/tissues both in zero-shotscenarios and after fine-tuning. Methods: We utilized five public datasets toevaluate and fine-tune SAM 2 for segmenting anatomical tissues in surgicalvideos/images. Fine-tuning was applied to the image encoder and mask decoder.We limited training subsets from 50 to 400 samples per class to better modelreal-world constraints with data acquisition. The impact of dataset size onfine-tuning performance was evaluated with weighted mean Dice coefficient(WMDC), and the results were also compared against previously reportedstate-of-the-art (SOTA) results. Results: SurgiSAM 2, a fine-tuned SAM 2 model,demonstrated significant improvements in segmentation performance, achieving a17.9% relative WMDC gain compared to the baseline SAM 2. Increasing promptpoints from 1 to 10 and training data scale from 50/class to 400/class enhancedperformance; the best WMDC of 0.92 on the validation subset was achieved with10 prompt points and 400 samples per class. On the test subset, this modeloutperformed prior SOTA methods in 24/30 (80%) of the classes with a WMDC of0.91 using 10-point prompts. Notably, SurgiSAM 2 generalized effectively tounseen organ classes, achieving SOTA on 7/9 (77.8%) of them. Conclusion: SAM 2achieves remarkable zero-shot and fine-tuned performance for surgical scenesegmentation, surpassing prior SOTA models across several organ classes ofdiverse datasets. This suggests immense potential for enablingautomated/semi-automated annotation pipelines, thereby decreasing the burden ofannotations facilitating several surgical applications.</description>
      <author>example@mail.com (Devanish N. Kamtam, Joseph B. Shrager, Satya Deepya Malla, Xiaohan Wang, Nicole Lin, Juan J. Cardona, Serena Yeung-Levy, Clarence Hu)</author>
      <guid isPermaLink="false">2503.03942v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>A dataset-free approach for self-supervised learning of 3D reflectional symmetries</title>
      <link>http://arxiv.org/abs/2503.02660v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种自监督模型，用于检测单个物体的对称性，并且该模型不需要大型数据集的支持。通过计算对象点云中的视觉特征来优化自监督模型。&lt;h4&gt;背景&lt;/h4&gt;目前在训练对称性检测模型时，需要大量的标注数据，这导致了高昂的成本和资源消耗。&lt;h4&gt;目的&lt;/h4&gt;设计一种不依赖于大规模数据集的自监督学习策略，以降低训练成本并提高效率。&lt;h4&gt;方法&lt;/h4&gt;利用基础图像模型提取特征来计算物体点云中的视觉描述符，基于对称点应具有相似视觉外观的原则进行自我监督学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在性能上超越了大型数据集上训练的现有最佳模型，并且更加高效和资源友好。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅减少了计算和数据需求，而且提高了检测精度和效率。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们研究了一种自监督模型，它能够学习单个物体的对称性而无需依赖大型数据集。我们认为可以通过分析物体自身的内在特征来确定其对称性。此外，我们设计了不使用地面真实标签的自我监督学习策略。这些关键要素使我们的方法既有效又高效，解决了为这种任务构建大规模标注数据集的成本问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore a self-supervised model that learns to detect thesymmetry of a single object without requiring a dataset-relying solely on theinput object itself. We hypothesize that the symmetry of an object can bedetermined by its intrinsic features, eliminating the need for large datasetsduring training. Additionally, we design a self-supervised learning strategythat removes the necessity of ground truth labels. These two key elements makeour approach both effective and efficient, addressing the prohibitive costsassociated with constructing large, labeled datasets for this task. The noveltyof our method lies in computing features for each point on the object based onthe idea that symmetric points should exhibit similar visual appearances. Toachieve this, we leverage features extracted from a foundational image model tocompute a visual descriptor for the points. This approach equips the pointcloud with visual features that facilitate the optimization of ourself-supervised model. Experimental results demonstrate that our methodsurpasses the state-of-the-art models trained on large datasets. Furthermore,our model is more efficient, effective, and operates with minimal computationaland data resources.</description>
      <author>example@mail.com (Isaac Aguirre, Ivan Sipiran, Gabriel Montañana)</author>
      <guid isPermaLink="false">2503.02660v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>CREStE: Scalable Mapless Navigation with Internet Scale Priors and Counterfactual Guidance</title>
      <link>http://arxiv.org/abs/2503.03921v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 10 figures, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为CREStE的方法，用于解决机器人在没有高精度地图和具体导航点的情况下进行长距离自主导航的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的解决方案难以泛化应用，因为它们依赖于手动定义的对象列表、缺乏大规模的数据集以及手工艺奖励函数，这些都不适用于多种场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种无需大规模机器人数据集或人工整理特征的方法来学习表示和奖励机制，以解决无地图导航问题。&lt;h4&gt;方法&lt;/h4&gt;CREStE利用视觉基础模型在互联网规模的数据上进行训练，获取包含高度、语义及实例级信息的连续鸟瞰图表示，并通过基于反事实的学习损失和主动学习过程优化这些表示用于路径规划。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有最佳解决方案相比，CREStE显著减少了任务中的干预次数，在未见过环境中执行2公里导航仅需要1次人工干预。&lt;h4&gt;结论&lt;/h4&gt;该方法显示了其在长距离无地图导航中具备的稳定性和有效性，并为机器人导航研究提供了新的视角和方向。&lt;h4&gt;翻译&lt;/h4&gt;我们解决了无需高精度地图或具体导航点的长距离自主导航问题。为了实现这一目标，需要克服两个主要挑战：一是学习对环境进行感知表示，而不预先列举所有可能的导航因素和感知混淆；二是利用这些已学得的表示来规划与人类一致的导航路径。现有的解决方案由于依赖于手动定义的对象列表、缺少大规模数据集以及手工艺奖励函数等问题而难以泛化应用。为克服这些问题，我们提出了CREStE方法，它是第一个不依赖大规模机器人数据集或人工整理特征的方法，用于学习解决无地图导航问题所需的表示和奖励机制。CREStE利用视觉基础模型在互联网规模的数据上进行训练，获取包含高度、语义及实例级信息的连续鸟瞰图表示，并通过基于反事实的学习损失和主动学习过程优化这些表示用于路径规划。我们在六个不同城市环境中的公里尺度导航任务中评估了CREStE的表现，结果显示它显著优于所有现有最佳解决方案，在最少70%的人工干预下表现良好；在一项2公里的未见环境中执行的任务中，仅需要1次人工干预就完成了任务，证明其具有良好的稳定性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the long-horizon mapless navigation problem: enabling robots totraverse novel environments without relying on high-definition maps or precisewaypoints that specify exactly where to navigate. Achieving this requiresovercoming two major challenges -- learning robust, generalizable perceptualrepresentations of the environment without pre-enumerating all possiblenavigation factors and forms of perceptual aliasing and utilizing these learnedrepresentations to plan human-aligned navigation paths. Existing solutionsstruggle to generalize due to their reliance on hand-curated object lists thatoverlook unforeseen factors, end-to-end learning of navigation features fromscarce large-scale robot datasets, and handcrafted reward functions that scalepoorly to diverse scenarios. To overcome these limitations, we propose CREStE,the first method that learns representations and rewards for addressing thefull mapless navigation problem without relying on large-scale robot datasetsor manually curated features. CREStE leverages visual foundation models trainedon internet-scale data to learn continuous bird's-eye-view representationscapturing elevation, semantics, and instance-level features. To utilize learnedrepresentations for planning, we propose a counterfactual-based loss and activelearning procedure that focuses on the most salient perceptual cues by queryinghumans for counterfactual trajectory annotations in challenging scenes. Weevaluate CREStE in kilometer-scale navigation tasks across six distinct urbanenvironments. CREStE significantly outperforms all state-of-the-art approacheswith 70% fewer human interventions per mission, including a 2-kilometer missionin an unseen environment with just 1 intervention; showcasing its robustnessand effectiveness for long-horizon mapless navigation. For videos andadditional materials, see https://amrl.cs.utexas.edu/creste .</description>
      <author>example@mail.com (Arthur Zhang, Harshit Sikchi, Amy Zhang, Joydeep Biswas)</author>
      <guid isPermaLink="false">2503.03921v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>LensDFF: Language-enhanced Sparse Feature Distillation for Efficient Few-Shot Dexterous Manipulation</title>
      <link>http://arxiv.org/abs/2503.03890v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为LensDFF的方法，该方法通过语言增强的特征融合策略高效地将视图一致的2D特性投影到3D点上，并结合抓取基本动作生成稳定且高度灵巧的抓取。&lt;h4&gt;背景&lt;/h4&gt;从少量演示中学习灵巧操纵对高级的人类类似机器人系统来说是一项重要的挑战。密集蒸馏特征场通过将2D视觉基础模型中的丰富语义特征提取到3D域来解决这一问题，但其依赖于如Neural Radiance Fields (NeRF)或高斯点阵等神经渲染模型导致了较高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;克服基于稀疏特征字段的方法的效率和抓取灵巧性不足的问题，并提出了一种新的语言增强的稀疏蒸馏特征场（LensDFF）方法，以及一种结合抓取基本动作进行少量演示下的灵巧操纵框架。&lt;h4&gt;方法&lt;/h4&gt;利用新颖的语言增强特征融合策略高效地将视图一致性的2D特征投影到3D点上；基于LensDFF的方法能够实现单视角下少量演示的泛化能力。此外还提出了一种real2sim抓取评估流水线以进行高效的抓取评估和超参数调整。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量的仿真实验和真实世界的实验，该方法在抓取性能方面取得了竞争性成果，并超过了现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的LensDFF方法能够高效地利用少量演示实现灵巧操纵的泛化，并且其结合抓取基本动作的方法可以生成稳定且高度灵巧的抓取。通过real2sim评估流水线，证明了该方法的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;从有限的示范中学习灵活的手部操作是先进的类似人类机器人系统的一项重要但具有挑战性的任务。密集蒸馏特性领域通过将丰富的语义特征从二维视觉基础模型传递到三维空间来应对这一挑战。然而，这种方法依赖于如NeRF或高斯点阵这样的神经渲染模型，导致了高昂的计算成本。相比之下，以前基于稀疏特性领域的方案要么由于多视角依赖和广泛的训练而效率低下，要么缺乏足够的抓取灵巧性。为了克服这些局限性，我们提出了语言增强的稀疏蒸馏特征领域（LensDFF），它通过新颖的语言强化特征融合策略将视图一致的二维特征高效地应用到三维点上，并实现单视角少量演示下的泛化能力。基于LensDFF，我们进一步提出了一种结合抓取基本动作的少量灵活操作框架以生成稳定且高度灵巧的抓取。此外，我们还提出了一种real2sim抓取评估流水线用于高效的抓取评估和超参数调整。通过大量的仿真实验以及现实世界的实验，我们的方法在抓握性能方面达到了竞争性的结果，并超过了最先进的方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning dexterous manipulation from few-shot demonstrations is a significantyet challenging problem for advanced, human-like robotic systems. Densedistilled feature fields have addressed this challenge by distilling richsemantic features from 2D visual foundation models into the 3D domain. However,their reliance on neural rendering models such as Neural Radiance Fields (NeRF)or Gaussian Splatting results in high computational costs. In contrast,previous approaches based on sparse feature fields either suffer frominefficiencies due to multi-view dependencies and extensive training or lacksufficient grasp dexterity. To overcome these limitations, we proposeLanguage-ENhanced Sparse Distilled Feature Field (LensDFF), which efficientlydistills view-consistent 2D features onto 3D points using our novellanguage-enhanced feature fusion strategy, thereby enabling single-viewfew-shot generalization. Based on LensDFF, we further introduce a few-shotdexterous manipulation framework that integrates grasp primitives into thedemonstrations to generate stable and highly dexterous grasps. Moreover, wepresent a real2sim grasp evaluation pipeline for efficient grasp assessment andhyperparameter tuning. Through extensive simulation experiments based on thereal2sim pipeline and real-world experiments, our approach achieves competitivegrasping performance, outperforming state-of-the-art approaches.</description>
      <author>example@mail.com (Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll)</author>
      <guid isPermaLink="false">2503.03890v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream Impact of Language Model Design Decisions</title>
      <link>http://arxiv.org/abs/2503.03862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;语言模型能力的提升通常被归因于增加模型规模或训练数据量，但在某些情况下，较小的模型在经过精心策划的数据集上训练或者采用不同的架构设计后可以超越更大规模且基于更多令牌进行训练的模型。为了量化这些设计选择的影响，我们对92个开源预训练模型进行了元分析，涵盖了各种规模，包括最先进的开放权重模型以及性能较差的模型和具有非传统设计方案的模型。研究发现，在考虑了除了模型大小和训练令牌数量之外的因素后，我们可以相对地提高3-28%的能力来预测下游表现与仅使用规模相比。对模型设计决策的分析揭示了关于数据组成的见解，例如在语言和代码任务之间的15-25%比例的任务权衡以及某些架构选择如旋转编码优于学习嵌入的方式可带来更好的性能。总体而言，我们的框架为更系统地研究模型开发选择如何塑造最终能力奠定了基础。&lt;h4&gt;背景&lt;/h4&gt;提升语言模型的性能通常被认为是通过增加模型大小或训练数据量来实现的，然而有时较小规模、在精心策划的数据集上训练或者采用不同架构设计的小型模型可以超越更大规模且基于更多令牌进行训练的大模型。&lt;h4&gt;目的&lt;/h4&gt;量化并分析影响语言模型性能的设计选择因素，包括模型尺寸、训练数据数量以及特定的架构决策，并探索这些因素如何帮助预测下游任务的表现。&lt;h4&gt;方法&lt;/h4&gt;对92个开源预训练模型进行了元分析，涵盖了各种规模和设计决策，以比较不同的模型特性对性能的影响。&lt;h4&gt;主要发现&lt;/h4&gt;通过纳入除模型大小和训练令牌数量外的因素（如特定数据构成比例、架构选择等），可以相对提高3-28%的能力来预测下游表现；并且某些架构选择例如旋转编码优于学习嵌入的方式可带来更好的性能。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，精心策划的数据集及适当的模型设计决策可以显著提升语言模型的性能，并提供了对不同规模和设计方案影响的理解。这些发现有助于更系统地探究如何通过开发过程中的选择来塑造最终的语言模型能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Improvements in language model capabilities are often attributed toincreasing model size or training data, but in some cases smaller modelstrained on curated data or with different architectural decisions canoutperform larger ones trained on more tokens. What accounts for this? Toquantify the impact of these design choices, we meta-analyze 92 open-sourcepretrained models across a wide array of scales, including state-of-the-artopen-weights models as well as less performant models and those with lessconventional design decisions. We find that by incorporating features besidesmodel size and number of training tokens, we can achieve a relative 3-28%increase in ability to predict downstream performance compared with using scalealone. Analysis of model design decisions reveal insights into datacomposition, such as the trade-off between language and code tasks at 15-25\%code, as well as the better performance of some architectural decisions such aschoosing rotary over learned embeddings. Broadly, our framework lays afoundation for more systematic investigation of how model development choicesshape final capabilities.</description>
      <author>example@mail.com (Emmy Liu, Amanda Bertsch, Lintang Sutawika, Lindia Tjuatja, Patrick Fernandes, Lara Marinov, Michael Chen, Shreya Singhal, Carolin Lawrence, Aditi Raghunathan, Kiril Gashteovski, Graham Neubig)</author>
      <guid isPermaLink="false">2503.03862v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Task-Agnostic Attacks Against Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2503.03842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器学习的安全研究主要集中在针对下游特定任务的攻击上。这些攻击通过优化与具体任务相关的损失函数来生成对抗样本。&lt;h4&gt;目的&lt;/h4&gt;探讨对公开可用的基础视觉模型（预训练模型）进行攻击的影响，以及这种攻击在多个下游任务中的传播能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用框架，用于制造不依赖于特定任务的对抗性示例。通过扰乱基础模型获取到的特征表示来最大化破坏其安全性。&lt;h4&gt;主要发现&lt;/h4&gt;评估了广泛使用的视觉基础模型的安全性，测量了攻击对多个下游任务的影响以及在不同模型之间的传播能力。&lt;h4&gt;结论&lt;/h4&gt;当前研究领域对于如何保护基于公共预训练模型的应用程序免受此类威胁仍然缺乏足够的关注和研究。&lt;h4&gt;翻译&lt;/h4&gt;该研究主要针对机器学习安全领域的现有不足进行了探讨，并提出了一种新的框架来评估视觉基础模型的安全性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of security in machine learning mainly focuses on downstreamtask-specific attacks, where the adversarial example is obtained by optimizinga loss function specific to the downstream task. At the same time, it hasbecome standard practice for machine learning practitioners to adopt publiclyavailable pre-trained vision foundation models, effectively sharing a commonbackbone architecture across a multitude of applications such asclassification, segmentation, depth estimation, retrieval, question-answeringand more. The study of attacks on such foundation models and their impact tomultiple downstream tasks remains vastly unexplored. This work proposes ageneral framework that forges task-agnostic adversarial examples by maximallydisrupting the feature representation obtained with foundation models. Weextensively evaluate the security of the feature representations obtained bypopular vision foundation models by measuring the impact of this attack onmultiple downstream tasks and its transferability between models.</description>
      <author>example@mail.com (Brian Pulfer, Yury Belousov, Vitaliy Kinakh, Teddy Furon, Slava Voloshynovskiy)</author>
      <guid isPermaLink="false">2503.03842v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.03562v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文介绍了首个用于工业异常检测的大规模物理基础视频数据集Physics Anomaly Detection (Phys-AD)，并分析了现有方法在处理基于物理学的异常时的局限性。&lt;h4&gt;背景&lt;/h4&gt;人类通过感知、互动和推理来识别现实世界中的物体异常。现有的工业异常检测算法主要是在静态且语义简单的数据集上开发和测试，这与需要物理理解和推理的真实场景相去甚远。&lt;h4&gt;目的&lt;/h4&gt;引入Physics Anomaly Detection (Phys-AD) 数据集，以缩小当前工业异常检测技术和真实应用场景之间的差距。&lt;h4&gt;方法&lt;/h4&gt;使用真实的机器人手臂和电机收集了大量动态且语义丰富的视频数据，并设计了多种评估指标来衡量现有模型在处理基于物理学的异常时的表现。&lt;h4&gt;主要发现&lt;/h4&gt;现有的无监督、弱监督以及视频理解模型在处理Phys-AD中的物理基础异常时表现出明显局限性。此外，提出了一个新的评估指标PAEval，用于评测视觉语言基础模型解释异常物理原因的能力。&lt;h4&gt;结论&lt;/h4&gt;通过提出Physics Anomaly Detection (Phys-AD) 数据集和相关评估基准，作者希望推动工业异常检测技术的发展，并促进具有准确物理解释能力的视觉语言基础模型的研究。&lt;h4&gt;翻译&lt;/h4&gt;人类在现实世界中识别物体异常时，会利用基于条件物体的知识进行感知、互动和推理。工业异常检测（IAD）领域的长期目标是使机器能够自主复制这一技能。然而，当前大多数IAD算法主要是在静态且语义简单的数据集上开发和测试的，这些数据集与现实世界场景存在很大差距，在后者中物理理解和推理至关重要。为了弥合这种差距，作者引入了Physics Anomaly Detection (Phys-AD) 数据集——首个用于工业异常检测的大规模、真实世界的物理基础视频数据集。该数据集由真实的机器人手臂和电机收集，提供了多样化的动态且语义丰富的场景，并包含超过6400个视频片段，跨越22类现实世界物体类别以及47种类型的异常情况。在Phys-AD中进行异常检测需要结合视觉推理、物理知识及视频内容来判断物体的不正常性。作者还评估了最先进的无监督、弱监督和基于视频理解的异常检测方法，并指出它们处理基于物理学的异常时的局限性。此外，提出了一种新的评估指标Physics Anomaly Explanation (PAEval)，用于评测视觉语言基础模型在发现异常的同时提供准确物理原因解释的能力。该数据集与基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans detect real-world object anomalies by perceiving, interacting, andreasoning based on object-conditioned physical knowledge. The long-term goal ofIndustrial Anomaly Detection (IAD) is to enable machines to autonomouslyreplicate this skill. However, current IAD algorithms are largely developed andtested on static, semantically simple datasets, which diverge from real-worldscenarios where physical understanding and reasoning are essential. To bridgethis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, thefirst large-scale, real-world, physics-grounded video dataset for industrialanomaly detection. Collected using a real robot arm and motor, Phys-AD providesa diverse set of dynamic, semantically rich scenarios. The dataset includesmore than 6400 videos across 22 real-world object categories, interacting withrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection inPhys-AD requires visual reasoning, combining both physical knowledge and videocontent to determine object abnormality. We benchmark state-of-the-art anomalydetection methods under three settings: unsupervised AD, weakly-supervised AD,and video-understanding AD, highlighting their limitations in handlingphysics-grounded anomalies. Additionally, we introduce the Physics AnomalyExplanation (PAEval) metric, designed to assess the ability of visual-languagefoundation models to not only detect anomalies but also provide accurateexplanations for their underlying physical causes. Our dataset and benchmarkwill be publicly available.</description>
      <author>example@mail.com (Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu)</author>
      <guid isPermaLink="false">2503.03562v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
      <link>http://arxiv.org/abs/2502.20984v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SemEval-2025 Task 1的任务是根据给定的名词复合词（可能带有隐喻意义）对图片进行排名。&lt;h4&gt;背景&lt;/h4&gt;研究利用生成式大型语言模型和多语种CLIP模型来增强具有隐喻含义的名词复合词表示。&lt;h4&gt;目的&lt;/h4&gt;提升图像与隐喻性名词复合词匹配度的排名性能。&lt;h4&gt;方法&lt;/h4&gt;使用LLM生成名词复合词的隐喻意义，然后用多语种CLIP模型进行编码；采用对比学习和数据增强技术对这些嵌入式表示进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示通过这种方法提取出的多模态表征优于仅基于原始名词复合词的方法。然而，虽然使用方法的微调策略显示出有前景的结果，但其效果不如未经过微调的情况。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种改进图像与具有隐喻含义的名词复合词匹配度排名的新方法，并公开了使用的源代码。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述的研究涉及SemEval-2025 Task 1任务中的图片排序，通过利用生成式大型语言模型和多语CLIP模型来增强隐喻性名词复合词表示。研究展示了这些技术在改进图像与隐喻性词语匹配度上的有效性，并公开了其源代码供其他研究人员参考使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SemEval-2025 Task 1 focuses on ranking images based on their alignment with agiven nominal compound that may carry idiomatic meaning in both English andBrazilian Portuguese. To address this challenge, this work uses generativelarge language models (LLMs) and multilingual CLIP models to enhance idiomaticcompound representations. LLMs generate idiomatic meanings for potentiallyidiomatic compounds, enriching their semantic interpretation. These meaningsare then encoded using multilingual CLIP models, serving as representations forimage ranking. Contrastive learning and data augmentation techniques areapplied to fine-tune these embeddings for improved performance. Experimentalresults show that multimodal representations extracted through this methodoutperformed those based solely on the original nominal compounds. Thefine-tuning approach shows promising outcomes but is less effective than usingembeddings without fine-tuning. The source code used in this paper is availableat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.</description>
      <author>example@mail.com (Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang)</author>
      <guid isPermaLink="false">2502.20984v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture</title>
      <link>http://arxiv.org/abs/2503.03222v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了Mocap-2-to-3框架，通过利用大规模的2D数据来改善单目视图下的人体绝对姿态恢复。&lt;h4&gt;背景&lt;/h4&gt;从单目视角恢复世界坐标系中的绝对姿态存在两大挑战：一是现有方法依赖于难以获取的新动作3D标签；二是仅凭单一视角估计人体在度量空间的位置复杂性高。&lt;h4&gt;目的&lt;/h4&gt;通过引入Mocap-2-to-3框架，解决上述问题，并提高模型的泛化能力和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;首先使用大量2D数据预训练单视图扩散模型，然后利用公开的3D数据进行多视图扩散模型微调。同时提出了一种新的人体运动表示法，将局部动作与全局移动分离并编码地面几何先验。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在真实世界数据集上比现有最佳技术表现更好，在姿态和绝对位置估计方面具有更高的精度、更好的泛化能力和可扩展性。&lt;h4&gt;结论&lt;/h4&gt;Mocap-2-to-3框架通过有效利用大规模的2D数据，解决了从单目视角恢复人体绝对姿态的问题，并提高了模型的实际应用能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering absolute poses in the world coordinate system from monocular viewspresents significant challenges. Two primary issues arise in this context.Firstly, existing methods rely on 3D motion data for training, which requirescollection in limited environments. Acquiring such 3D labels for new actions ina timely manner is impractical, severely restricting the model's generalizationcapabilities. In contrast, 2D poses are far more accessible and easier toobtain. Secondly, estimating a person's absolute position in metric space froma single viewpoint is inherently more complex. To address these challenges, weintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motionsinto 2D poses, leveraging 2D data to enhance 3D motion reconstruction indiverse scenarios and accurately predict absolute positions in the worldcoordinate system. We initially pretrain a single-view diffusion model withextensive 2D data, followed by fine-tuning a multi-view diffusion model forview consistency using publicly available 3D data. This strategy facilitatesthe effective use of large-scale 2D data. Additionally, we propose aninnovative human motion representation that decouples local actions from globalmovements and encodes geometric priors of the ground, ensuring the generativemodel learns accurate motion priors from 2D data. During inference, this allowsfor the gradual recovery of global movements, resulting in more plausiblepositioning. We evaluate our model's performance on real-world datasets,demonstrating superior accuracy in motion and absolute human positioningcompared to state-of-the-art methods, along with enhanced generalization andscalability. Our code will be made publicly available.</description>
      <author>example@mail.com (Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou)</author>
      <guid isPermaLink="false">2503.03222v2</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ReRAW: RGB-to-RAW Image Reconstruction via Stratified Sampling for Efficient Object Detection on the Edge</title>
      <link>http://arxiv.org/abs/2503.03782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文介绍了一种名为ReRAW的模型，该模型可以将现有的RGB图像转换为传感器特定的RAW格式。使用多头架构预测伽马空间中的RAW候选图，并通过分层采样策略训练数据选择方法来优化性能。&lt;h4&gt;背景&lt;/h4&gt;边缘设备上的计算机视觉模型从未经处理的细节丰富的RAW传感器数据中受益匪浅，而不是经过处理的RGB图像。然而，为了训练这些模型，需要大量的带有标签的RAW数据集，这通常是昂贵且不切实际的。&lt;h4&gt;目的&lt;/h4&gt;目的是通过转换现有的带标签RGB数据集为特定于传感器的RAW图像来有效训练模型。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种新的RGB到RAW转换模型ReRAW，并提出一种基于分层采样的训练数据选择策略，以提高模型重构性能。&lt;h4&gt;主要发现&lt;/h4&gt;ReRAW在五个不同的RAW数据集中实现了最先进的重建效果。通过结合高质量的合成RAW数据集和真实地面真相RAW图像进行预训练的小型模型，在目标任务如对象检测中的表现优于标准RGB管道和从RGB预先训练到RAW微调的模型。&lt;h4&gt;结论&lt;/h4&gt;使用该方法生成的带标签RAW数据可以有效提升边缘设备上的计算机视觉模型性能，特别是对于资源受限环境下的应用具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;边缘设备上运行基于边缘的计算机视觉模型可以从未经处理、细节丰富的RAW传感器数据中获益匪浅，而无需依赖于经过处理的RGB图像。然而，训练这些模型需要大量的标记过的RAW数据集，这通常是昂贵且不切实际的。因此，将现有的带标签RGB数据集转换为特定于传感器的RAW图像对于有效的模型训练至关重要。在本文中，我们介绍了ReRAW，这是一种从RGB到RAW的转换模型，在五个不同的RAW数据集中实现了最先进的重建性能。这是通过ReRAW新颖的多头架构实现的，该架构预测伽马空间中的RAW候选图，并且通过一种基于分层采样的训练数据选择策略进一步提高了性能，这有助于模型更好地重构更亮的RAW像素。最终我们展示了在下游任务（如对象检测）中使用高质量合成RAW数据集和真实地面真相RAW图像进行预训练的小型模型的表现优于标准RGB管道以及从RGB预先训练到RAW微调的模型为同一任务时的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Edge-based computer vision models running on compact, resource-limiteddevices benefit greatly from using unprocessed, detail-rich RAW sensor datainstead of processed RGB images. Training these models, however, necessitateslarge labeled RAW datasets, which are costly and often impractical to obtain.Thus, converting existing labeled RGB datasets into sensor-specific RAW imagesbecomes crucial for effective model training. In this paper, we introduceReRAW, an RGB-to-RAW conversion model that achieves state-of-the-artreconstruction performance across five diverse RAW datasets. This isaccomplished through ReRAW's novel multi-head architecture predicting RAW imagecandidates in gamma space. The performance is further boosted by a stratifiedsampling-based training data selection heuristic, which helps the model betterreconstruct brighter RAW pixels. We finally demonstrate that pretrainingcompact models on a combination of high-quality synthetic RAW datasets (such asgenerated by ReRAW) and ground-truth RAW images for downstream tasks likeobject detection, outperforms both standard RGB pipelines, and RAW fine-tuningof RGB-pretrained models for the same task.</description>
      <author>example@mail.com (Radu Berdan, Beril Besbinar, Christoph Reinders, Junji Otsuka, Daisuke Iso)</author>
      <guid isPermaLink="false">2503.03782v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation</title>
      <link>http://arxiv.org/abs/2503.04718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;场景流估计是许多机器人应用的基础任务，包括稳健的动态物体检测、自动标记和传感器同步。&lt;h4&gt;背景&lt;/h4&gt;针对场景流估计问题已经发展了两大类方法：监督学习和优化算法。监督学习方法在推理阶段速度快并且结果质量高，但需要大量标注数据并可能因领域差距受限；非监督测试时间优化方法则不受限于领域差距，但是通常运行时间较长、存在瑕疵或无法收敛。&lt;h4&gt;目的&lt;/h4&gt;本文旨在缓解现有基于优化的方法的若干局限性。&lt;h4&gt;方法&lt;/h4&gt;1) 引入了一种简单的体素网格模型，该模型在多个方面改进了标准MLP公式。2) 提出了一种新的多帧损失形式化方法。3) 将以上贡献结合到名为Floxels的新方法中。&lt;h4&gt;主要发现&lt;/h4&gt;在Argoverse 2基准测试上，Floxels在未监督方法中仅被EulerFlow超越，并且以较低的计算成本实现了可比性能；相比EulerFlow，Floxels的速度提升了超过60-140倍，将运行时间从一天减少到每序列10分钟。与更快但质量较差的基本线NSFP相比，Floxels的速度提高了约14倍。&lt;h4&gt;结论&lt;/h4&gt;新方法在速度和准确性上都有显著提高，并且大大降低了计算成本。&lt;h4&gt;翻译&lt;/h4&gt;场景流估计是许多机器人应用的基础任务，包括稳健的动态物体检测、自动标记和传感器同步。针对这个问题，已经发展了监督学习和非监督优化两大类方法。本文提出了一种新的基于体素网格的方法Floxels来改进现有的优化策略，并在基准测试中展示了显著的速度提升和性能保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene flow estimation is a foundational task for many robotic applications,including robust dynamic object detection, automatic labeling, and sensorsynchronization. Two types of approaches to the problem have evolved: 1)Supervised and 2) optimization-based methods. Supervised methods are fastduring inference and achieve high-quality results, however, they are limited bythe need for large amounts of labeled training data and are susceptible todomain gaps. In contrast, unsupervised test-time optimization methods do notface the problem of domain gaps but usually suffer from substantial runtime,exhibit artifacts, or fail to converge to the right solution. In this work, wemitigate several limitations of existing optimization-based methods. To thisend, we 1) introduce a simple voxel grid-based model that improves over thestandard MLP-based formulation in multiple dimensions and 2) introduce a newmultiframe loss formulation. 3) We combine both contributions in our newmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed onlyby EulerFlow among unsupervised methods while achieving comparable performanceat a fraction of the computational cost. Floxels achieves a massive speedup ofmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxelsachieves a speedup of ~14x.</description>
      <author>example@mail.com (David T. Hoffmann, Syed Haseeb Raza, Hanqiu Jiang, Denis Tananaev, Steffen Klingenhoefer, Martin Meinke)</author>
      <guid isPermaLink="false">2503.04718v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Inverse Q-Learning from Demonstrations</title>
      <link>http://arxiv.org/abs/2503.04679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures, 2 tables. Published at the International  Conference on Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种新的多智能体逆强化学习框架，即基于展示的多代理边际Q学习（MAMQL），以解决单智能体逆向强化学习无法有效处理的合作与竞争目标之间的平衡问题。&lt;h4&gt;背景&lt;/h4&gt;深度强化学习算法在手动设计奖励函数的情况下容易遭受奖励指定不正确的问题，在多智能体环境中由于环境非稳态性和方差增加，这种问题变得更加严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的采样效率高的框架MAMQL来解决现有方法难以平衡合作与竞争目标的挑战。&lt;h4&gt;方法&lt;/h4&gt;对于每个代理，MAMQL通过学习其他代理策略下的边际评判者来推测奖励函数，并允许在多智能体场景中合理地使用玻尔兹曼政策。该研究还建立了最佳边际评判者和单智能体软Q逆向强化学习之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有的多智能体方法相比，MAMQL显著提高了平均回报、采样效率和奖励恢复性能（通常为2-5倍）。&lt;h4&gt;结论&lt;/h4&gt;通过在多个模拟环境中验证了新框架的有效性，证明了其在处理复杂合作竞争关系的多智能体环境中的优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要：当奖赏函数是由人手动设计时，深度强化学习算法常常遭受奖赏指定不正确的问题，导致学习到次优策略。在单代理问题中，逆向强化学习（IRL）试图通过从专家演示推断出奖励函数来解决这个问题。然而，在多智能体问题中，由于环境的非稳态性和方差增加，学习目标和真实目标之间的不对齐被加剧了。因此，在多智能体零和游戏中，多代理IRL算法难以平衡合作与竞争的目标。为了解决这些问题，我们提出了一种新的基于展示的多代理边际Q学习（MAMQL）框架。对于每个代理来说，MAMQL学会了在一个其他代理商策略下的批评者边缘化，允许在多智能体环境中合理地使用玻尔兹曼政策。我们发现了一个最优边缘化评论家和单代理软Q IRL之间的联系，让我们可以直接应用简单的优化标准从单智能体领域。我们的实验结果表明，与现有的多代理方法相比，MAMQL的平均奖励、采样效率和奖赏恢复性能要好得多（通常为2-5倍）。我们把代码放在了 https://sites.google.com/view/mamql 上供他人使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; When reward functions are hand-designed, deep reinforcement learningalgorithms often suffer from reward misspecification, causing them to learnsuboptimal policies in terms of the intended task objectives. In thesingle-agent case, inverse reinforcement learning (IRL) techniques attempt toaddress this issue by inferring the reward function from expert demonstrations.However, in multi-agent problems, misalignment between the learned and trueobjectives is exacerbated due to increased environment non-stationarity andvariance that scales with multiple agents. As such, in multi-agent general-sumgames, multi-agent IRL algorithms have difficulty balancing cooperative andcompetitive objectives. To address these issues, we propose Multi-AgentMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficientframework for multi-agent IRL. For each agent, MAMQL learns a criticmarginalized over the other agents' policies, allowing for a well-motivated useof Boltzmann policies in the multi-agent context. We identify a connectionbetween optimal marginalized critics and single-agent soft-Q IRL, allowing usto apply a direct, simple optimization criterion from the single-agent domain.Across our experiments on three different simulated domains, MAMQLsignificantly outperforms previous multi-agent methods in average reward,sample efficiency, and reward recovery by often more than 2-5x. We make ourcode available at https://sites.google.com/view/mamql .</description>
      <author>example@mail.com (Nathaniel Haynam, Adam Khoja, Dhruv Kumar, Vivek Myers, Erdem Bıyık)</author>
      <guid isPermaLink="false">2503.04679v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>3HANDS Dataset: Learning from Humans for Generating Naturalistic Handovers with Supernumerary Robotic Limbs</title>
      <link>http://arxiv.org/abs/2503.04635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CHI '25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;超常机器人肢体（SRLs）增强了人的物理能力，需要实现无缝、自然的人机交互。为了在物理任务中有效提供帮助，使SRL能够向人类递送物体至关重要。&lt;h4&gt;背景&lt;/h4&gt;传统的基于规则的方法设计用于机器人的手部传递动作耗时且难以跨任务泛化，并导致机器人动作不够自然。&lt;h4&gt;目的&lt;/h4&gt;提出3HANDS数据集以及使用该数据集训练的生成模型，以创建自然的手部传递轨迹、确定合适的传递结束点并预测何时开始手部传递。&lt;h4&gt;方法&lt;/h4&gt;3HANDS数据集记录了一个参与者在进行日常活动时与另一个参与者通过佩戴于髋部的SRL进行物体交接的情况。利用这个数据集开发了三个模型：生成自然传输路径、决定适当的交付终点和预测启动传递的时刻。&lt;h4&gt;主要发现&lt;/h4&gt;用户研究结果（N=10）表明，与基线方法相比，该方法的手部交互在感知上更自然、体力需求更低且更加舒适。&lt;h4&gt;结论&lt;/h4&gt;3HANDS数据集及其训练模型成功地改善了人机物体交接的自然性，为开发高效的人机协作系统提供了有力的支持。&lt;h4&gt;翻译&lt;/h4&gt;超常机器人肢体（SRLs）是紧密集成在用户身体上的机器人结构，它们增强了人的物理能力，并需要实现无缝、自然的人机交互。为了有效地辅助完成体力任务，使SRL能够向人类递送物体至关重要。然而，基于启发式的方法设计用于机器人的手部传递动作耗时且难以跨任务泛化，而且会导致机器人动作不够自然。当使用适当的数据集训练时，生成模型是创建自然手部传递运动的有力替代方案。我们引入了3HANDS，这是一个新颖的数据集，记录了一个参与者在进行日常活动时与另一个参与者通过佩戴于髋部的SRL进行物体交接的情况。3HANDS数据集捕捉了SRL交互的独特特征：在私人空间内的操作、不对称的对象起源、隐式的运动同步以及用户在传递过程中参与主要任务的状态。为了展示我们数据集的有效性，我们提出三种模型：一种生成自然手部传递轨迹的模型，另一种确定适当的传递结束点的模型，还有一种预测何时开始传递时刻的模型。在一个用户研究（N=10）中，我们将我们的方法与基线方法的手部交互进行了比较。结果表明，该方法在感知上更自然、体力需求更低且更加舒适。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3706598.3713306&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supernumerary robotic limbs (SRLs) are robotic structures integrated closelywith the user's body, which augment human physical capabilities and necessitateseamless, naturalistic human-machine interaction. For effective assistance inphysical tasks, enabling SRLs to hand over objects to humans is crucial. Yet,designing heuristic-based policies for robots is time-consuming, difficult togeneralize across tasks, and results in less human-like motion. When trainedwith proper datasets, generative models are powerful alternatives for creatingnaturalistic handover motions. We introduce 3HANDS, a novel dataset of objecthandover interactions between a participant performing a daily activity andanother participant enacting a hip-mounted SRL in a naturalistic manner. 3HANDScaptures the unique characteristics of SRL interactions: operating in intimatepersonal space with asymmetric object origins, implicit motion synchronization,and the user's engagement in a primary task during the handover. To demonstratethe effectiveness of our dataset, we present three models: one that generatesnaturalistic handover trajectories, another that determines the appropriatehandover endpoints, and a third that predicts the moment to initiate ahandover. In a user study (N=10), we compare the handover interaction performedwith our method compared to a baseline. The findings show that our method wasperceived as significantly more natural, less physically demanding, and morecomfortable.</description>
      <author>example@mail.com (Artin Saberpour Abadian, Yi-Chi Liao, Ata Otaran, Rishabh Dabral, Marie Muehlhaus, Christian Theobalt, Martin Schmitz, Jürgen Steimle)</author>
      <guid isPermaLink="false">2503.04635v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Whole-Body Model-Predictive Control of Legged Robots with MuJoCo</title>
      <link>http://arxiv.org/abs/2503.04613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文展示了迭代LQR算法结合MuJoCo动力学和有限差分近似导数的简单方法在四足和人形机器人整体模型预测控制中的实际有效性。&lt;h4&gt;背景&lt;/h4&gt;基于之前使用MuJoCo在模拟环境中进行运动和操作任务的行为合成与控制的成功经验，研究者表明这些策略可以轻松地推广到现实世界中，而无需过多考虑仿真到真实环境的转换问题。&lt;h4&gt;目的&lt;/h4&gt;展示一种简单的方法可以在实际硬件上实现实时的整体模型预测控制，并通过多种实验验证其效果。&lt;h4&gt;方法&lt;/h4&gt;使用迭代LQR算法和MuJoCo动力学以及有限差分近似导数的方法进行整体模型预测控制，以实现在四足机器人动态行走、两腿行走及人形双足机器人的步态运动等任务中的实时控制。&lt;h4&gt;主要发现&lt;/h4&gt;提出的基线方法在不同的硬件实验中实现了实时的整体模型预测控制，并表明了仿真到现实世界的过渡可以非常简单。&lt;h4&gt;结论&lt;/h4&gt;该研究希望降低进入实际环境中进行整体模型预测控制研究的门槛，促进社区内的研究速度加快。&lt;h4&gt;翻译&lt;/h4&gt;我们展示了四足和人形机器人全身模型预测控制（MPC）中一种非常简单的、具有惊人现实世界有效性的方法：迭代LQR算法结合MuJoCo动力学以及有限差分近似导数。基于之前使用MuJoCo在模拟环境中成功进行运动和操作任务的行为合成与控制的经验，我们展示了这些策略可以轻松地推广到现实中，并且仅需很少的仿真至真实考虑即可实现这种推广。我们的基线方法实现了多种硬件实验中的实时全身MPC，包括四足机器人动态行走、两腿行走以及全尺寸人形双足步行机器人的运动任务。希望这种方法易于重现的硬件基准可以降低进入实际环境中进行全身模型预测控制研究的门槛，并有助于加速社区内的研究进展速度。我们的代码和实验视频可在以下网址获取：https://johnzhang3.github.io/mujoco_ilqr&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We demonstrate the surprising real-world effectiveness of a very simpleapproach to whole-body model-predictive control (MPC) of quadruped and humanoidrobots: the iterative LQR (iLQR) algorithm with MuJoCo dynamics andfinite-difference approximated derivatives. Building upon the previous successof model-based behavior synthesis and control of locomotion and manipulationtasks with MuJoCo in simulation, we show that these policies can easilygeneralize to the real world with few sim-to-real considerations. Our baselinemethod achieves real-time whole-body MPC on a variety of hardware experiments,including dynamic quadruped locomotion, quadruped walking on two legs, andfull-sized humanoid bipedal locomotion. We hope this easy-to-reproduce hardwarebaseline lowers the barrier to entry for real-world whole-body MPC research andcontributes to accelerating research velocity in the community. Our code andexperiment videos will be available onlineat:https://johnzhang3.github.io/mujoco_ilqr</description>
      <author>example@mail.com (John Z. Zhang, Taylor A. Howell, Zeji Yi, Chaoyi Pan, Guanya Shi, Guannan Qu, Tom Erez, Yuval Tassa, Zachary Manchester)</author>
      <guid isPermaLink="false">2503.04613v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Multi-dimensional Elasticity for Pervasive Stream Processing Services</title>
      <link>http://arxiv.org/abs/2503.04193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication at Percom 2025 as Work in Progress (WIP)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种在质量和资源维度上扩展流媒体服务的分层解决方案。&lt;h4&gt;背景&lt;/h4&gt;现代场景如智慧城市依赖于物联网数据的连续处理以提供实时服务并满足应用目标（服务水平目标SLO）。&lt;h4&gt;目的&lt;/h4&gt;提高边缘环境中的服务弹性，通过多维扩展策略来实现这一点。&lt;h4&gt;方法&lt;/h4&gt;设计了一种两层架构：本地、特定于服务的代理确保通过多种弹性和资源分配策略履行SLO；如果无法再分配更多资源，则更高层次的代理通过交换资源优化全局SLO履行。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，当操作在严格的资源限制下进行时，该方法优于传统的垂直自动扩展器。&lt;h4&gt;结论&lt;/h4&gt;基于多维度弹性策略的方法为边缘环境中服务提供了一种有效的扩展方式。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文翻译：本文提出了一种分层解决方案，在质量和资源维度上扩展流媒体服务。现代场景如智慧城市依赖于物联网数据的连续处理以提供实时服务并满足应用目标（服务水平目标SLO）。虽然趋势是尽可能在附近边缘设备上处理数据，但这会形成瓶颈因为资源只能配置到有限容量。为提高边缘环境中的弹性，我们提出通过多种维度进行扩展——要么是资源，要么是服务质量。我们依靠两层架构：本地、特定于服务的代理确保通过多维弹性和资源分配策略履行SLO；如果无法再分配更多资源，则更高层次的代理通过交换资源优化全局SLO履行。实验结果表明，在严格的资源限制下操作时，该方法优于传统的垂直自动扩展器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a hierarchical solution to scale streaming servicesacross quality and resource dimensions. Modern scenarios, like smart cities,heavily rely on the continuous processing of IoT data to provide real-timeservices and meet application targets (Service Level Objectives -- SLOs). Whilethe tendency is to process data at nearby Edge devices, this creates abottleneck because resources can only be provisioned up to a limited capacity.To improve elasticity in Edge environments, we propose to scale services inmultiple dimensions -- either resources or, alternatively, the service quality.We rely on a two-layer architecture where (1) local, service-specific agentsensure SLO fulfillment through multi-dimensional elasticity strategies; if nomore resources can be allocated, (2) a higher-level agent optimizes global SLOfulfillment by swapping resources. The experimental results show promisingoutcomes, outperforming regular vertical autoscalers, when operating undertight resource constraints.</description>
      <author>example@mail.com (Boris Sedlak, Andrea Morichetta, Philipp Raith, Víctor Casamayor Pujol, Schahram Dustdar)</author>
      <guid isPermaLink="false">2503.04193v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>PLMP -- Point-Line Minimal Problems for Projective SfM</title>
      <link>http://arxiv.org/abs/2503.04351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文对结构从运动（SfM）中点和线的排列完全被多个未校准针孔相机观测的所有最小问题进行了全面分类。&lt;h4&gt;背景&lt;/h4&gt;在视觉测量领域，研究者们关注如何通过多视角图像恢复三维场景结构。该工作聚焦于利用无约束相机阵列观察到的点和线条配置来解决SfM问题。&lt;h4&gt;目的&lt;/h4&gt;目的是识别并分类所有可能出现在未校准针孔摄像机阵列观测情况下的最小问题，这些问题是理解更复杂视觉测量场景的基础。&lt;h4&gt;方法&lt;/h4&gt;研究者通过数学分析确定了291个最小问题，并且为每个问题计算了解的数量。此外，还引入了一种基于子稳定群的方法来系统地分解最小问题并识别不完全约束条件下的最小集。&lt;h4&gt;主要发现&lt;/h4&gt;发现了291个不同类型的最小问题，其中有73个具有唯一解可以线性求解；其他非线性最小问题最多涉及9台摄像机和7个点、12条直线。这些最小问题的解决方案数量相对较低（与校准相机相比）。&lt;h4&gt;结论&lt;/h4&gt;该研究为理解未校准针孔相机阵列中的视觉测量问题提供了全面的基础，识别出的大量线性和非线性最小问题是未来算法开发的重要基础。&lt;h4&gt;翻译&lt;/h4&gt;论文团队彻底分类了所有在结构从运动（SfM）中完全观察到点和线条布局，并由多个无校准针孔摄像机观测的所有最小化问题。研究结果揭示，存在291种不同的最小化情况，其中73种具有独特的解决方案，可以线性地求解。两个线性问题是开放式的视图数量不限制的，而其他所有最小化场景最多涉及九个相机。所有的最小化场景中点和线条的数量上限分别为七和十二。论文还为每一个最小化问题计算了解决方案的数量，以评估每个问题内在难度，并且发现这个数字相对较小（例如与校准相机最小问题相比）。最后通过探索子阵列的稳定子群，开发了一种几何系统方法来1）分解最小化问题到更小的问题；2）在不充分约束的情况下识别最小化问题；3）正式证明非最简化状态。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We completely classify all minimal problems for Structure-from-Motion (SfM)where arrangements of points and lines are fully observed by multipleuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which haveunique solutions and can thus be solved linearly. Two of the linear problemsallow an arbitrary number of views, while all other minimal problems have atmost 9 cameras. All minimal problems have at most 7 points and at most 12lines. We compute the number of solutions of each minimal problem, as thisgives a measurement of the problem's intrinsic difficulty, and find that thesenumber are relatively low (e.g., when comparing with minimal problems forcalibrated cameras). Finally, by exploring stabilizer subgroups ofsubarrangements, we develop a geometric and systematic way to 1) factorizeminimal problems into smaller problems, 2) identify minimal problems inunderconstrained problems, and 3) formally prove non-minimality.</description>
      <author>example@mail.com (Kim Kiehn, Albin Ahlbäck, Kathlén Kohn)</author>
      <guid isPermaLink="false">2503.04351v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ExoNav II: Design of a Robotic Tool with Follow-the-Leader Motion Capability for Lateral and Ventral Spinal Cord Stimulation (SCS)</title>
      <link>http://arxiv.org/abs/2503.04603v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种螺旋微加工连续机器人，用于在脊髓中导航刺激电极以治疗疼痛并恢复步态。&lt;h4&gt;背景&lt;/h4&gt;传统的脊髓刺激（SCS）电极放置于背侧硬膜外空间，但运动纤维位于脊髓的腹侧和侧面。目前，SCS电极的手动导向难以触及这些位置。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在受到肌腱力作用时形成螺旋形弯曲的连续机器人，并验证其随行（FTL）运动能力。&lt;h4&gt;方法&lt;/h4&gt;通过设计刚性的外管并增加机器人的平移和旋转自由度，实现了螺旋连续机器人的制作。提出了一个基于绳索长度和几何参数与机器人轨迹及末端执行器位置之间关系的动力学模型。&lt;h4&gt;主要发现&lt;/h4&gt;基于绳索长度的方法显示了19.84毫米的偏差和14.42毫米的均方根误差（RMSE），而基于位置的方法表现更好，仅有10.54毫米的偏差和8.04毫米的均方根误差。在随行实验中，这两种方法分别显示出11.24毫米和7.32毫米的偏差，以及8.67毫米和5.18毫米的RMSE。&lt;h4&gt;结论&lt;/h4&gt;该机器人能够在模拟脊髓模型上工作，显示了其潜在的应用价值，并且通过两次随行运动试验证实了机器人的重复行为。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容描述了一种用于脊髓刺激的新技术，旨在改善现有的治疗疼痛和恢复步态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spinal cord stimulation (SCS) electrodes are traditionally placed in thedorsal epidural space to stimulate the dorsal column fibers for pain therapy.Recently, SCS has gained attention in restoring gait. However, the motor fiberstriggering locomotion are located in the ventral and lateral spinal cord.Currently, SCS electrodes are steered manually, making it difficult to navigatethem to the lateral and ventral motor fibers in the spinal cord. In this work,we propose a helically micro-machined continuum robot that can bend in ahelical shape when subjected to actuation tendon forces. Using a stiff outertube and adding translational and rotational degrees of freedom, this helicalcontinuum robot can perform follow-the-leader (FTL) motion. We propose akinematic model to relate tendon stroke and geometric parameters of the robot'shelical shape to its acquired trajectory and end-effector position. We evaluatethe proposed kinematic model and the robot's FTL motion capabilityexperimentally. The stroke-based method, which links tendon stroke values tothe robot's shape, showed inaccuracies with a 19.84 mm deviation and an RMSE of14.42 mm for 63.6 mm of robot's length bending. The position-based method,using kinematic equations to map joint space to task space, performed betterwith a 10.54 mm deviation and an RMSE of 8.04 mm. Follow-the-leader experimentsshowed deviations of 11.24 mm and 7.32 mm, with RMSE values of 8.67 mm and 5.18mm for the stroke-based and position-based methods, respectively. Furthermore,end-effector trajectories in two FTL motion trials are compared to confirm therobot's repeatable behavior. Finally, we demonstrate the robot's operation on a3D-printed spinal cord phantom model.</description>
      <author>example@mail.com (Behnam Moradkhani, Pejman Kheradmand, Harshith Jella, Joseph Klein, Ajmal Zemmar, Yash Chitalia)</author>
      <guid isPermaLink="false">2503.04603v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DogLegs: Robust Proprioceptive State Estimation for Legged Robots Using Multiple Leg-Mounted IMUs</title>
      <link>http://arxiv.org/abs/2503.04580v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;腿部机器人在极端环境中执行任务时，准确的本体感知状态估计至关重要。DogLegs 是一种基于四足机器人的姿态估算系统，通过融合安装在主身体上的惯性测量单元（Body-IMU）、关节编码器和多个腿部惯性测量单元（Leg-IMU）的数据，并使用扩展卡尔曼滤波器进行状态融合。&lt;h4&gt;背景&lt;/h4&gt;在极端环境中，例如复杂的地形或恶劣天气条件下，外部传感器如激光雷达和相机可能会失效，这使得腿部机器人需要依赖内部传感器来完成任务。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够提高腿部机器人姿态估算准确性的新方法，从而增强其在极端环境中的操作性能。&lt;h4&gt;方法&lt;/h4&gt;该系统利用Body-IMU、关节编码器以及多个Leg-IMUs的测量数据，并通过扩展卡尔曼滤波器进行状态估计。系统还包含了所有IMU框架内的误差状态，同时使用腿部力学计算相对位置约束来更新主要身体的状态以减少单独IMU框架中的误差漂移。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在不同的地形条件下，该方法的精度都优于传统的腿里程计方法（仅使用Body-IMU和关节编码器）。&lt;h4&gt;结论&lt;/h4&gt;通过融合多源传感器数据以及优化姿态估计算法，可以显著提高腿部机器人在复杂环境下的操作能力。公开的数据集将有助于研究社区进一步的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;稳健且准确的主身体本体感觉状态估算是足式机器人执行极端环境中任务的关键因素，在这种环境下，诸如激光雷达和摄像头之类的外感知传感器可能变得不可靠。本文提出了一种针对足式机器人的状态估计系统——DogLegs，该系统结合了安装在机身上的惯性测量单元（Body-IMU）、关节编码器以及多个腿部安装的惯性测量单元（Leg-IMU）的数据，并通过扩展卡尔曼滤波器进行融合。过滤系统包含所有IMU框架内的误差状态。利用腿装IMUs来检测脚接触，因此提供零速度测量以更新Leg-IMU帧的状态。此外，我们根据腿部力学计算身体IMU和腿部IMU之间的相对位置约束，并使用这些约束更新主要身体状态以及减少各IMU帧的误差漂移。现场实验结果表明，我们的提议系统可以实现在各种地形条件下优于传统腿里程计方法（仅使用Body-IMU和关节编码器）的状态估算精度。我们将公开数据集以造福研究社区。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust and accurate proprioceptive state estimation of the main body iscrucial for legged robots to execute tasks in extreme environments whereexteroceptive sensors, such as LiDARs and cameras may become unreliable. Inthis paper, we propose DogLegs, a state estimation system for legged robotsthat fuses the measurements from a body-mounted inertial measurement unit(Body-IMU), joint encoders, and multiple leg-mounted IMUs (Leg-IMU) using anextended Kalman filter (EKF). The filter system contains the error states ofall IMU frames. The Leg-IMUs are used to detect foot contact, thereby providingzero velocity measurements to update the state of the Leg-IMU frames.Additionally, we compute the relative position constraints between the Body-IMUand Leg-IMUs by the leg kinematics and use them to update the main body stateand reduce the error drift of the individual IMU frames. Field experimentalresults have shown that our proposed system can achieve better state estimationaccuracy compared to the traditional leg odometry method (using only Body-IMUand joint encoders) across different terrains. We make our datasets publiclyavailable to benefit the research community.</description>
      <author>example@mail.com (Yibin Wu, Jian Kuang, Shahram Khorshidi, Xiaoji Niu, Lasse Klingbeil, Maren Bennewitz, Heiner Kuhlmann)</author>
      <guid isPermaLink="false">2503.04580v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Data-augmented Learning of Geodesic Distances in Irregular Domains through Soner Boundary Conditions</title>
      <link>http://arxiv.org/abs/2503.04579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;测地距离在机器人领域中扮演着关键角色，能够高效编码域的全局几何信息。最近的方法通过物理知识引导的方法求解Eikonal方程来利用神经网络近似测地距离。&lt;h4&gt;问题&lt;/h4&gt;尽管这些方法有效，但在复杂环境中训练时常常遇到不稳定的收敛情况。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架，在不规则领域中使用Soner边界条件学习测地距离，并系统评估数据损失对训练稳定性和解决方案准确性的影响。&lt;h4&gt;方法&lt;/h4&gt;通过引入数据损失来改善在复杂环境中的神经网络模型的训练过程，这些数据损失能够提高收敛稳定性并减少初始化敏感性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，加入数据损失能显著提高收敛稳健性，减少训练不稳定性和对初始条件的敏感性。研究结果还指出，混合数据-物理方法可以有效增强基于学习的方法在稀疏数据下求解测地距离问题的可靠性。&lt;h4&gt;结论&lt;/h4&gt;通过结合数据和物理知识，可以更有效地解决复杂环境中的测地距离计算问题，并提高神经网络模型的学习效率和准确性。&lt;h4&gt;翻译&lt;/h4&gt;测地距离在机器人学中发挥着基础性的作用，能够有效编码域的整体几何信息。最近的技术使用深度学习方法来逼近测地距离，通过求解Eikonal方程的物理启发式手段实现。尽管这些技术已经非常成功了，但在复杂的环境中训练时常常会遇到不稳定的收敛问题。本文提出了一种框架，在不规则领域中利用Soner边界条件进行测地距离的学习，并系统性地评估数据损失对模型稳定性和精度的影响。实验表明，通过引入数据损失可以显著提高模型的稳健性，减少在复杂场景中的训练不稳定现象以及初始设置的敏感度。这一发现意味着混合的数据和物理方法能有效提升基于学习的方法解决稀疏数据下的测地距离问题的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geodesic distances play a fundamental role in robotics, as they efficientlyencode global geometric information of the domain. Recent methods use neuralnetworks to approximate geodesic distances by solving the Eikonal equationthrough physics-informed approaches. While effective, these approaches oftensuffer from unstable convergence during training in complex environments. Wepropose a framework to learn geodesic distances in irregular domains by usingthe Soner boundary condition, and systematically evaluate the impact of datalosses on training stability and solution accuracy. Our experiments demonstratethat incorporating data losses significantly improves convergence robustness,reducing training instabilities and sensitivity to initialization. Thesefindings suggest that hybrid data-physics approaches can effectively enhancethe reliability of learning-based geodesic distance solvers with sparse data.</description>
      <author>example@mail.com (Rafael I. Cabral Muchacho, Florian T. Pokorny)</author>
      <guid isPermaLink="false">2503.04579v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Omnidirectional Multi-Object Tracking</title>
      <link>http://arxiv.org/abs/2503.04565v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CVPR 2025. The dataset and code will be made publicly  available at https://github.com/xifen523/OmniTrack&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为OmniTrack的全景多目标跟踪框架，该框架旨在解决现有多对象跟踪算法在处理全景图像时的问题，并引入了若干技术创新以提高性能。&lt;h4&gt;背景&lt;/h4&gt;全景图像是捕捉周围物体空间和时间关系的重要工具。然而，现有的多目标跟踪（MOT）算法大多为针孔图像设计，在视野有限的情况下效果不佳。此外，由于分辨率损失、几何变形以及光线不均匀等问题，直接将现有方法应用于全景场景会显著降低性能。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些问题，本文提出了OmniTrack框架，该框架旨在提高全景多目标跟踪的精度和效率，并克服缺乏相应数据集的问题。&lt;h4&gt;方法&lt;/h4&gt;OmniTrack框架包括以下关键组件：Tracklet Management用于引入时间线索；FlexiTrack Instances支持物体定位与关联；CircularStatE Module则通过减轻图像和几何失真来增强性能。此外，为了缓解全景MOT数据不足的现状，本文还构建了QuadTrack数据集。&lt;h4&gt;主要发现&lt;/h4&gt;OmniTrack框架在公共JRDB数据集上获得了26.92%的HOTA分数（较基准提升3.43%），并在新引入的QuadTrack基准测试中达到了23.45%，超越了基础线方法6.81%&lt;h4&gt;结论&lt;/h4&gt;通过整合各种创新技术，OmniTrack框架能够有效应对全景多目标跟踪中的挑战，并且在公开数据集上表现出色。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种面向全景图像的多对象跟踪（MOT）框架OmniTrack。该框架采用Tracklet Management引入时间线索、FlexiTrack Instances实现物体定位和关联，以及CircularStatE Module缓解图像及几何失真问题，能够有效应对宽视野场景下的快速传感器运动挑战。此外，为了弥补现有的全景MOT数据集的不足，我们创建了QuadTrack数据集，这是一个由四足机器人收集的全面的数据集合，涵盖了广泛的视场、激烈的移动和复杂的环境条件等多方面难题。实验结果显示，在JRDB公开数据集中，OmniTrack框架取得了26.92%的HOTA评分（相较于基准提升了3.43%），而在新建立的QuadTrack基准测试中，该方法的表现更是达到了23.45%，超出基础线模型6.81%。我们将提供开源的数据集和代码访问链接：https://github.com/xifen523/OmniTrack&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Panoramic imagery, with its 360{\deg} field of view, offers comprehensiveinformation to support Multi-Object Tracking (MOT) in capturing spatial andtemporal relationships of surrounding objects. However, most MOT algorithms aretailored for pinhole images with limited views, impairing their effectivenessin panoramic settings. Additionally, panoramic image distortions, such asresolution loss, geometric deformation, and uneven lighting, hinder directadaptation of existing MOT methods, leading to significant performancedegradation. To address these challenges, we propose OmniTrack, anomnidirectional MOT framework that incorporates Tracklet Management tointroduce temporal cues, FlexiTrack Instances for object localization andassociation, and the CircularStatE Module to alleviate image and geometricdistortions. This integration enables tracking in large field-of-viewscenarios, even under rapid sensor motion. To mitigate the lack of panoramicMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramicdataset collected by a quadruped robot, featuring diverse challenges such aswide fields of view, intense motion, and complex environments. Extensiveexperiments on the public JRDB dataset and the newly introduced QuadTrackbenchmark demonstrate the state-of-the-art performance of the proposedframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing animprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing thebaseline by 6.81%. The dataset and code will be made publicly available athttps://github.com/xifen523/OmniTrack.</description>
      <author>example@mail.com (Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang)</author>
      <guid isPermaLink="false">2503.04565v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Image-Based Relocalization and Alignment for Long-Term Monitoring of Dynamic Underwater Environments</title>
      <link>http://arxiv.org/abs/2503.04096v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种结合视觉地点识别（VPR）、特征匹配和图像分割的综合管道，用于海底生态系统的自动化管理。这种方法可以有效地识别重复访问区域，并估计刚性变换。&lt;h4&gt;背景&lt;/h4&gt;有效监测水下生态系统对于追踪环境变化、指导保护工作以及确保长期生态系统健康至关重要。然而，由于水下影像的复杂性，传统的视觉定位方法面临着重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一个结合VPR、特征匹配和图像分割的方法来解决自动化管理水下生态系统的难题，并介绍了一个大规模的数据集SQUIDLE+ VPR Benchmark用于测试这种方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;该研究开发了一种新的综合管道，它整合了视觉地点识别（Visual Place Recognition, VPR）、特征匹配以及基于视频的图像分割技术。此方法能够在变化多样的水下环境中有效地进行视觉定位和刚体变换估计。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验表明所提出的方法在大规模数据集SQUIDLE+ VPR Benchmark上有效，该数据集包括来自多个机器人平台的不同时间段内的多样化轨迹、重叠区域以及海床类型的数据。&lt;h4&gt;结论&lt;/h4&gt;所开发的综合管道为水下生态系统的自动化管理提供了新的可能性，并且首次构建了用于评估视觉地点识别技术的大规模水下基准数据集SQUIDLE+ VPR Benchmark。&lt;h4&gt;翻译&lt;/h4&gt;有效的监测水下生态系统对于追踪环境变化、指导保护工作以及确保长期生态系统健康至关重要。但是，由于水下图像的复杂性，使用机器人平台自动化的生态管理仍然具有挑战性。我们提出了一种结合视觉地点识别（VPR）、特征匹配和基于视频图像分割的方法来实现这一目标，并介绍了SQUIDLE+ VPR基准测试集——这是第一个大规模水下VPR基准测试集，涵盖了来自多个机器人平台的大量未结构化数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective monitoring of underwater ecosystems is crucial for trackingenvironmental changes, guiding conservation efforts, and ensuring long-termecosystem health. However, automating underwater ecosystem management withrobotic platforms remains challenging due to the complexities of underwaterimagery, which pose significant difficulties for traditional visuallocalization methods. We propose an integrated pipeline that combines VisualPlace Recognition (VPR), feature matching, and image segmentation onvideo-derived images. This method enables robust identification of revisitedareas, estimation of rigid transformations, and downstream analysis ofecosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-thefirst large-scale underwater VPR benchmark designed to leverage an extensivecollection of unstructured data from multiple robotic platforms, spanning timeintervals from days to years. The dataset encompasses diverse trajectories,arbitrary overlap and diverse seafloor types captured under varyingenvironmental conditions, including differences in depth, lighting, andturbidity. Our code is available at: https://github.com/bev-gorry/underloc</description>
      <author>example@mail.com (Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan)</author>
      <guid isPermaLink="false">2503.04096v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments</title>
      <link>http://arxiv.org/abs/2503.04563v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究提出了一个考虑遮挡环境的机器人导航安全性和运动一致性的模型预测控制策略。&lt;h4&gt;背景&lt;/h4&gt;在障碍物密集且存在视线遮挡的情况下，确保机器人的安全性与导航的一致性是一个重要的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于风险区域和动态约束的模型预测控制方法来解决上述挑战，并通过实验验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;{'方法1': '调整可变的风险区域以代表潜在障碍物的位置', '方法2': '在线开发动态的安全边界限制，确保机器人行动安全', '方法3': '生成多个局部最优轨迹分支（每种分支对应不同的风险区）来平衡探索与利用的关系', '方法4': '创建一个共享的共识树干以保证不同分支之间的平滑过渡并保持运动的一致性', '方法5': '使用交替方向乘子法(ADMM)将模型预测控制问题分解为可管理的小规模子问题，提高计算效率'}&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和真实环境实验验证了所提策略的有效性，尤其是在视线受阻、障碍物密集的环境下。&lt;h4&gt;结论&lt;/h4&gt;提出的遮挡感知一致型模型预测控制(CMPC)策略能有效解决机器人导航中的安全性和运动一致性问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safety and motion consistency for robot navigation in occluded,obstacle-dense environments is a critical challenge. In this context, thisstudy presents an occlusion-aware Consistent Model Predictive Control (CMPC)strategy. To account for the occluded obstacles, it incorporates adjustablerisk regions that represent their potential future locations. Subsequently,dynamic risk boundary constraints are developed online to ensure safety. TheCMPC then constructs multiple locally optimal trajectory branches (eachtailored to different risk regions) to balance between exploitation andexploration. A shared consensus trunk is generated to ensure smooth transitionsbetween branches without significant velocity fluctuations, further preservingmotion consistency. To facilitate high computational efficiency and ensurecoordination across local trajectories, we use the alternating direction methodof multipliers (ADMM) to decompose the CMPC into manageable sub-problems forparallel solving. The proposed strategy is validated through simulation andreal-world experiments on an Ackermann-steering robot platform. The resultsdemonstrate the effectiveness of the proposed CMPC strategy through comparisonswith baseline approaches in occluded, obstacle-dense environments.</description>
      <author>example@mail.com (Minzhe Zheng, Lei Zheng, Lei Zhu, Jun Ma)</author>
      <guid isPermaLink="false">2503.04563v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Learning Generalizable Language-Conditioned Cloth Manipulation from Long Demonstrations</title>
      <link>http://arxiv.org/abs/2503.04557v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的流水线方法，用于机器人进行多步骤布料操作任务，通过自主学习基本技能来应对高维状态空间和布料动力学的挑战，并且该方法在未见过的任务上具有泛化能力。&lt;h4&gt;背景&lt;/h4&gt;目前在机器人执行多步布料操作方面存在困难，因为这些操作涉及到高维的状态空间以及复杂的行为动态。尽管端到端模仿学习技术在此领域取得了显著进步，但它们仍然难以将学到的技能应用到未见过的任务中。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决多步骤布料操作任务中的泛化问题，尤其是如何从长演示数据中自动学习基本技能，并利用这些技能完成新的、之前没有见过的任务。&lt;h4&gt;方法&lt;/h4&gt;首先使用常识性知识（来自大型语言模型LLM）从现有长时间演示基准中发现和学习基础技能。然后，通过高级别的基于LLM的任务规划器来组合这些基础技能以解决新任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在学习多步骤布料操作技能方面优于基线方法，并且对于已见过或未见过的任务都表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法提供了一种有效的途径，通过自动从演示中学习基本技能并组合这些技能来解决机器人执行复杂任务时的泛化问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-step cloth manipulation is a challenging problem for robots due to thehigh-dimensional state spaces and the dynamics of cloth. Despite recentsignificant advances in end-to-end imitation learning for multi-step clothmanipulation skills, these methods fail to generalize to unseen tasks. Ourinsight in tackling the challenge of generalizable multi-step clothmanipulation is decomposition. We propose a novel pipeline that autonomouslylearns basic skills from long demonstrations and composes learned basic skillsto generalize to unseen tasks. Specifically, our method first discovers andlearns basic skills from the existing long demonstration benchmark with thecommonsense knowledge of a large language model (LLM). Then, leveraging ahigh-level LLM-based task planner, these basic skills can be composed tocomplete unseen tasks. Experimental results demonstrate that our methodoutperforms baseline methods in learning multi-step cloth manipulation skillsfor both seen and unseen tasks.</description>
      <author>example@mail.com (Hanyi Zhao, Jinxuan Zhu, Zihao Yan, Yichen Li, Yuhong Deng, Xueqian Wang)</author>
      <guid isPermaLink="false">2503.04557v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>ViT-VS: On the Applicability of Pretrained Vision Transformer Features for Generalizable Visual Servoing</title>
      <link>http://arxiv.org/abs/2503.04545v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合经典视觉伺服技术和学习基础方法优势的新型视觉伺服技术，通过使用预训练的视觉变换器进行语义特征提取，在无需特定任务或对象训练的情况下达到较好的性能。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉伺服技术依赖于手工制作的功能，并且在面对遮挡和环境变化时表现不佳。而基于学习的方法虽然提高了鲁棒性，但通常需要大量的训练数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的视觉伺服方法，该方法结合经典和基于学习的方法的优势，在不需要特定任务或对象的额外训练的情况下提供更好的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的视觉变换器进行语义特征提取，并在此基础上构建视觉伺服系统。这种方法在无干扰场景下可以实现完全收敛，并且即使面对环境变化也能保持较高的鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的图像基视觉伺服相比，在受到扰动的情况下，该方法最多提高了31.2%的相对改进；同时匹配了基于学习的方法中的收敛速度，而不需要任务或对象特定训练。此外，实验证明这种方法在终端执行器定位、工业盒子操作以及抓取未见过的对象方面表现出色。&lt;h4&gt;结论&lt;/h4&gt;所提出的视觉伺服技术提供了一种平衡传统方法和基于学习的系统优势的新途径，在无需额外的任务特定训练下展示了强大的性能和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;视觉伺服技术使机器人能够精确地将末端执行器相对于目标物体进行定位。虽然经典的方法依赖于手工制作的功能，因此可以在没有任务特定训练的情况下普遍适用，但它们通常在面对遮挡和环境变化时表现不佳。基于学习的方法提高了稳健性，但通常需要大量的训练。我们提出了一种视觉伺服方法，利用预训练的视觉变换器提取语义特征，结合了两种范式的优势，并能够超越所提供的样本进行泛化。我们的方法在未受干扰的情况下实现了完全收敛，在受到扰动时相较于传统的基于图像的视觉伺服提高了最多31.2%的相对改进。尽管没有任务或对象特定的训练需求，但仍然达到了学习方法的收敛率。实际应用评估证实了该技术在终端执行器定位、工业盒子操作以及抓取未见过的对象方面的稳健性能。我们的代码和模拟环境可在 https://alessandroscherl.github.io/ViT-VS/ 查看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual servoing enables robots to precisely position their end-effectorrelative to a target object. While classical methods rely on hand-craftedfeatures and thus are universally applicable without task-specific training,they often struggle with occlusions and environmental variations, whereaslearning-based approaches improve robustness but typically require extensivetraining. We present a visual servoing approach that leverages pretrainedvision transformers for semantic feature extraction, combining the advantagesof both paradigms while also being able to generalize beyond the providedsample. Our approach achieves full convergence in unperturbed scenarios andsurpasses classical image-based visual servoing by up to 31.2\% relativeimprovement in perturbed scenarios. Even the convergence rates oflearning-based methods are matched despite requiring no task- orobject-specific training. Real-world evaluations confirm robust performance inend-effector positioning, industrial box manipulation, and grasping of unseenobjects using only a reference from the same category. Our code and simulationenvironment are available at: https://alessandroscherl.github.io/ViT-VS/</description>
      <author>example@mail.com (Alessandro Scherl, Stefan Thalhammer, Bernhard Neuberger, Wilfried Wöber, José Gracía-Rodríguez)</author>
      <guid isPermaLink="false">2503.04545v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks</title>
      <link>http://arxiv.org/abs/2503.04538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SRSA框架通过利用现有的技能库来解决机器人在数据高效模式下学习新任务的挑战，特别是对于接触丰富的装配任务。该方法旨在预测所有技能转移到新任务时的成功率，并据此指导技能检索过程。&lt;h4&gt;背景&lt;/h4&gt;使机器人以数据高效的方式学习新的任务是一个长期存在的挑战，尤其是在需要精确控制的接触密集型装配任务中，进展较少。&lt;h4&gt;目的&lt;/h4&gt;提出SRSA框架来解决在现有技能库中快速有效地为新任务选择相关技能的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了一个框架，该框架能够预测所有技能转移到新任务时的成功率，并利用这些信息来指导技能检索过程。此外，通过联合捕捉物体几何特征、物理动力学和专家动作来表示任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明SRSA在获取未见过的任务的技能并进行微调方面优于基线方法，在成功率上有19%的相对提升；标准偏差降低了2.6倍，并且达到满意的成功率达到需要更少的过渡样本（2.4倍）。&lt;h4&gt;结论&lt;/h4&gt;SRSA框架展示了其在机器人学习和适应新任务方面的潜力，特别是在装配任务中表现突出。当在仿真环境中训练时，成功率可以超过90%。&lt;h4&gt;翻译&lt;/h4&gt;使机器人以数据高效的方式学习新的任务是一个长期存在的挑战。尽管在一般的抓取放置操作方面已取得许多进展，但接触密集型的组装任务则研究较少。SRSA框架利用现有的技能库来解决这个问题，并通过预测所有技能转移到新任务时的成功率来进行有效的技能检索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Enabling robots to learn novel tasks in a data-efficient manner is along-standing challenge. Common strategies involve carefully leveraging priorexperiences, especially transition data collected on related tasks. Althoughmuch progress has been made for general pick-and-place manipulation, far fewerstudies have investigated contact-rich assembly tasks, where precise control isessential. We introduce SRSA (Skill Retrieval and Skill Adaptation), a novelframework designed to address this problem by utilizing a pre-existing skilllibrary containing policies for diverse assembly tasks. The challenge lies inidentifying which skill from the library is most relevant for fine-tuning on anew task. Our key hypothesis is that skills showing higher zero-shot successrates on a new task are better suited for rapid and effective fine-tuning onthat task. To this end, we propose to predict the transfer success for allskills in the skill library on a novel task, and then use this prediction toguide the skill retrieval process. We establish a framework that jointlycaptures features of object geometry, physical dynamics, and expert actions torepresent the tasks, allowing us to efficiently learn the transfer successpredictor. Extensive experiments demonstrate that SRSA significantlyoutperforms the leading baseline. When retrieving and fine-tuning skills onunseen tasks, SRSA achieves a 19% relative improvement in success rate,exhibits 2.6x lower standard deviation across random seeds, and requires 2.4xfewer transition samples to reach a satisfactory success rate, compared to thebaseline. Furthermore, policies trained with SRSA in simulation achieve a 90%mean success rate when deployed in the real world. Please visit our projectwebpage https://srsa2024.github.io/.</description>
      <author>example@mail.com (Yijie Guo, Bingjie Tang, Iretiayo Akinola, Dieter Fox, Abhishek Gupta, Yashraj Narang)</author>
      <guid isPermaLink="false">2503.04538v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>PALo: Learning Posture-Aware Locomotion for Quadruped Robots</title>
      <link>http://arxiv.org/abs/2503.04462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为PALo的端到端深度强化学习框架，用于四足机器人在复杂地形上的姿态感知行走控制。&lt;h4&gt;背景&lt;/h4&gt;随着嵌入式智能的发展，四足机器人的地面适应性步行控制成为研究热点。传统的步行控制系统主要关注速度跟踪，而忽视了敏捷性和鲁棒性的平衡。&lt;h4&gt;目的&lt;/h4&gt;本文旨在开发一种能够处理线性和角速度的同时追踪及实时调整身体高度、俯仰和横滚角度的系统，以实现四足机器人在各种复杂地形上的姿态感知行走控制。&lt;h4&gt;方法&lt;/h4&gt;通过将步行控制系统表述为部分可观测马尔可夫决策过程，并采用不对称演员-评论家架构来克服仿真到现实环境的挑战。此外，结合定制化的训练课程，在模拟环境中实现了敏捷的姿态感知行走控制，并成功地将其转移到真实场景中而无需微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入实验分析，识别了PALo性能的关键组件，并进一步验证了所提出方法的有效性。研究表明，对于四足机器人在更高维度命令空间中的低级步行控制提供了新的可能。&lt;h4&gt;结论&lt;/h4&gt;该研究为未来嵌入式智能的上层模块的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;随着嵌入式智能的快速发展，四足机器人在复杂地形上的移动控制已成为研究热点。不同于传统的仅关注速度跟踪的方法，本文旨在平衡四足机器人在多样且复杂的地面上的敏捷性和鲁棒性。为此，提出了一个名为PALo的姿态感知行走控制端到端深度强化学习框架，能够同时处理线性和角速度追踪以及实时调整身体高度、俯仰和横滚角度的问题。该方法将移动控制系统表述为部分可观测马尔可夫决策过程，并采用不对称演员-评论家架构来克服仿真到现实环境的挑战。通过定制化的训练课程，在模拟环境中实现了敏捷的姿态感知行走控制并成功地将其转移到真实场景中而无需微调，从而允许四足机器人在具有挑战性的地形上进行实时移动和身体姿态控制。深入实验分析识别了PALo性能的关键组件，并进一步验证了所提出方法的有效性。该研究为低级步行控制提供了新的可能，在更高维度的命令空间中的四足机器人，并为基础未来嵌入式智能的上层模块的研究奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of embodied intelligence, locomotion control ofquadruped robots on complex terrains has become a research hotspot. Unliketraditional locomotion control approaches focusing solely on velocity tracking,we pursue to balance the agility and robustness of quadruped robots on diverseand complex terrains. To this end, we propose an end-to-end deep reinforcementlearning framework for posture-aware locomotion named PALo, which manages tohandle simultaneous linear and angular velocity tracking and real-timeadjustments of body height, pitch, and roll angles. In PALo, the locomotioncontrol problem is formulated as a partially observable Markov decisionprocess, and an asymmetric actor-critic architecture is adopted to overcome thesim-to-real challenge. Further, by incorporating customized training curricula,PALo achieves agile posture-aware locomotion control in simulated environmentsand successfully transfers to real-world settings without fine-tuning, allowingreal-time control of the quadruped robot's locomotion and body posture acrosschallenging terrains. Through in-depth experimental analysis, we identify thekey components of PALo that contribute to its performance, further validatingthe effectiveness of the proposed method. The results of this study provide newpossibilities for the low-level locomotion control of quadruped robots inhigher dimensional command spaces and lay the foundation for future research onupper-level modules for embodied intelligence.</description>
      <author>example@mail.com (Xiangyu Miao, Jun Sun, Hang Lai, Xinpeng Di, Jiahang Cao, Yong Yu, Weinan Zhang)</author>
      <guid isPermaLink="false">2503.04462v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic Surface Mapping from Monocular RGB Images</title>
      <link>http://arxiv.org/abs/2503.04441v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对复杂环境中场景理解的需求，提出了一种不确定性感知的多任务学习框架EvidMTL，该框架结合了单目RGB图像进行深度估计和语义分割，并且能够从证据理论的角度提供不确定性的评估。&lt;h4&gt;背景&lt;/h4&gt;现有映射方法在处理非结构化环境时，经常会产生过度自信的语义预测以及稀疏、噪声较多的深度信息，从而导致地图表示不一致的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多任务学习框架EvidMTL和不确定性感知的语义表面映射框架EvidKimera，以提高非结构化环境中的场景理解能力。&lt;h4&gt;方法&lt;/h4&gt;EvidMTL使用证据头部进行深度估计和语义分割，从而能够从单目RGB图像中提供不确定性的意识推断。提出了一种新颖的证据深度损失函数，该函数同时优化深度预测的信心强度以及与证据语义分割损失结合的优化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明EvidMTL在NYUDepthV2数据集上的训练和评估中具有优秀的不确定性估计性能，并且在ScanNetV2上进行零样本映射测试时，EvidKimera的语义表面映射准确性和一致性优于传统的Kimera。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够提高场景理解中的深度估计、语义分割以及3D度量-语义一致性，其不确定性感知的能力对于实际应用中的机器人系统具有重要的价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For scene understanding in unstructured environments, an accurate anduncertainty-aware metric-semantic mapping is required to enable informed actionselection by autonomous systems.Existing mapping methods often suffer fromoverconfident semantic predictions, and sparse and noisy depth sensing, leadingto inconsistent map representations. In this paper, we therefore introduceEvidMTL, a multi-task learning framework that uses evidential heads for depthestimation and semantic segmentation, enabling uncertainty-aware inference frommonocular RGB images. To enable uncertainty-calibrated evidential multi-tasklearning, we propose a novel evidential depth loss function that jointlyoptimizes the belief strength of the depth prediction in conjunction withevidential segmentation loss. Building on this, we present EvidKimera, anuncertainty-aware semantic surface mapping framework, which uses evidentialdepth and semantics prediction for improved 3D metric-semantic consistency. Wetrain and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shotperformance on ScanNetV2, demonstrating superior uncertainty estimationcompared to conventional approaches while maintaining comparable depthestimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,EvidKimera outperforms Kimera in semantic surface mapping accuracy andconsistency, highlighting the benefits of uncertainty-aware mapping andunderscoring its potential for real-world robotic applications.</description>
      <author>example@mail.com (Rohit Menon, Nils Dengler, Sicong Pan, Gokul Krishna Chenchani, Maren Bennewitz)</author>
      <guid isPermaLink="false">2503.04441v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>On the Analysis of Stability, Sensitivity and Transparency in Variable Admittance Control for pHRI Enhanced by Virtual Fixtures</title>
      <link>http://arxiv.org/abs/2503.04414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，由于协作机器人确保了在力交互过程中的用户安全，物理人机交互（pHRI）的兴趣显著增加。因此，在提出新的pHRI应用控制方案时，稳定性问题已经得到了广泛研究。&lt;h4&gt;目的&lt;/h4&gt;本文的主要目的是通过考虑寄生效应（如传动弹性、电机速度饱和和驱动延迟），对一类基于代理的约束顺应性控制器进行详细的不稳定性来源分析，并确定控制参数如何影响整个系统的稳定性。此外，还提出了改进透明度的方法。&lt;h4&gt;方法&lt;/h4&gt;首先进行了详细的不稳定性的来源分析；然后通过实验结果支持的敏感性分析来识别控制参数对系统稳定性的影响；最后提出了一种基于代理参数调整的技术以最大化pHRI中的透明度，并通过仿真和实验验证了该技术的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;针对具体控制方案，揭示了一些影响其稳定性的因素（如传动弹性、电机速度饱和及驱动延迟），并通过敏感性分析确定了这些寄生效应如何影响系统稳定性。此外，提出了一种改进透明度的技术，并通过实验和仿真验证了该技术的有效性。&lt;h4&gt;结论&lt;/h4&gt;本研究为pHRI控制系统的设计提供了重要的见解，特别是关于不稳定性的根源以及提高稳定性和交互透明度的方法。&lt;h4&gt;翻译&lt;/h4&gt;过去二十年中，物理人机交互（Physical Human-Robot Interaction, pHRI）的兴趣显著增加。由于协作机器人的可用性，它们在力交换过程中能够保证用户的安全。因此，在文献中广泛研究了稳定性问题，并提出了新的pHRI应用控制方案。然而，由于机器人本身的非线性特性，稳定性分析通常依赖于被动性的概念。另一方面，所提出的算法一般考虑的是机器人操作臂的理想模型。考虑到这一点，本文的主要目标是通过考虑寄生效应（如传动弹性、电机速度饱和和驱动延迟）来对一种特定的pHRI控制方案——基于代理的约束顺应性控制器进行详细的不稳定性来源分析，并确定控制参数如何影响整个系统的稳定性。接着，进行了由实验结果支持的敏感性分析，以识别控制参数对系统稳定性的效果。最后提出了用于最大化pHRI透明度的代理参数调整技术，并通过仿真和实验测试验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The interest in Physical Human-Robot Interaction (pHRI) has significantlyincreased over the last two decades thanks to the availability of collaborativerobots that guarantee user safety during force exchanges. For this reason,stability concerns have been addressed extensively in the literature whileproposing new control schemes for pHRI applications. Because of the nonlinearnature of robots, stability analyses generally leverage passivity concepts. Onthe other hand, the proposed algorithms generally consider ideal models ofrobot manipulators. For this reason, the primary objective of this paper is toconduct a detailed analysis of the sources of instability for a class of pHRIcontrol schemes, namely proxy-based constrained admittance controllers, byconsidering parasitic effects such as transmission elasticity, motor velocitysaturation, and actuation delay. Next, a sensitivity analysis supported byexperimental results is carried out, in order to identify how the controlparameters affect the stability of the overall system. Finally, an adaptationtechnique for the proxy parameters is proposed with the goal of maximizingtransparency in pHRI. The proposed adaptation method is validated through bothsimulations and experimental tests.</description>
      <author>example@mail.com (Davide Tebaldi, Dario Onfiani, Luigi Biagiotti)</author>
      <guid isPermaLink="false">2503.04414v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>SeGMan: Sequential and Guided Manipulation Planner for Robust Planning in 2D Constrained Environments</title>
      <link>http://arxiv.org/abs/2503.04409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;SeGMan是一种结合了采样和优化技术的混合运动规划框架，适用于解决复杂的顺序操作挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理复杂且受限的顺序操作任务（如拾取和放置谜题）时存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的运动规划框架，以提高此类问题的效率和性能。&lt;h4&gt;方法&lt;/h4&gt;SeGMan结合了基于采样的技术和优化技术，并使用引导式向前搜索。此外，它采用了一种自适应子目标选择方法来调整子目标的粒度。&lt;h4&gt;主要发现&lt;/h4&gt;在布满物体和障碍物的迷宫样任务中进行了广泛评估，表明SeGMan能够生成一致且计算效率高的操作计划，并优于现有最佳的方法。&lt;h4&gt;结论&lt;/h4&gt;SeGMan提供了一种高效且通用的方法来解决复杂的顺序操作问题。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中，我们介绍了SeGMan，这是一种混合运动规划框架，它将基于采样的技术和优化技术与引导式向前搜索相结合，以应对复杂、受限的序列操控挑战，例如拾取和放置谜题。该框架集成了自适应子目标选择方法，可以调整子目标的粒度，从而提高整体效率。此外，提出的通用启发式方法使得向前搜索更加有针对性。在迷宫样任务中进行大量评估，这些任务包含许多物体和障碍物，结果表明SeGMan不仅能够生成一致且计算高效的操控计划，而且还能超越最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present SeGMan, a hybrid motion planning framework thatintegrates sampling-based and optimization-based techniques with a guidedforward search to address complex, constrained sequential manipulationchallenges, such as pick-and-place puzzles. SeGMan incorporates an adaptivesubgoal selection method that adjusts the granularity of subgoals, enhancingoverall efficiency. Furthermore, proposed generalizable heuristics guide theforward search in a more targeted manner. Extensive evaluations in maze-liketasks populated with numerous objects and obstacles demonstrate that SeGMan iscapable of generating not only consistent and computationally efficientmanipulation plans but also outperform state-of-the-art approaches.</description>
      <author>example@mail.com (Cankut Bora Tuncer, Dilruba Sultan Haliloglu, Ozgur S. Oguz)</author>
      <guid isPermaLink="false">2503.04409v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Energy Consumption of Robotic Arm with the Local Reduction Method</title>
      <link>http://arxiv.org/abs/2503.04340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种局部减少法，用于优化机械臂系统的能效，这种方法在不牺牲性能的情况下减少了高达25%的能耗。&lt;h4&gt;背景&lt;/h4&gt;随着运营成本和环境影响的增加，工业自动化中机器人的能源消耗成为一个重要的问题。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过一种新的方法——局部减少法，来提高机器人系统在保持精度和操作可靠性的同时的能效。&lt;h4&gt;方法&lt;/h4&gt;对一个三关节机械臂模型进行了模拟实验，该模型执行了30秒内的取放任务和轨迹跟踪等不同任务。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的MPC（预测模型控制）和GA（遗传算法）相比，局部减少法显著降低了25%的能耗，并且展示了更好的适应性和计算效率。此外，这种新方法易于集成到新兴技术如人工智能中。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了局部减少法作为一种优化机器人操作、降低能源需求并促进工业自动化可持续性的实用工具的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种通过局部减少法来提高机械臂系统能效的研究。这项技术在不牺牲性能的情况下，大大降低了能耗，并且易于集成到其他新兴技术中以进一步增强其应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Energy consumption in robotic arms is a significant concern in industrialautomation due to rising operational costs and environmental impact. This studyinvestigates the use of a local reduction method to optimize energy efficiencyin robotic systems without compromising performance. The approach refinesmovement parameters, minimizing energy use while maintaining precision andoperational reliability. A three-joint robotic arm model was tested usingsimulation over a 30-second period for various tasks, including pick-and-placeand trajectory-following operations. The results revealed that the localreduction method reduced energy consumption by up to 25% compared totraditional techniques such as Model Predictive Control (MPC) and GeneticAlgorithms (GA). Unlike MPC, which requires significant computationalresources, and GA, which has slow convergence rates, the local reduction methoddemonstrated superior adaptability and computational efficiency in real-timeapplications. The study highlights the scalability and simplicity of the localreduction approach, making it an attractive option for industries seekingsustainable and cost-effective solutions. Additionally, this method canintegrate seamlessly with emerging technologies like Artificial Intelligence(AI), further enhancing its application in dynamic and complex environments.This research underscores the potential of the local reduction method as apractical tool for optimizing robotic arm operations, reducing energy demands,and contributing to sustainability in industrial automation. Future work willfocus on extending the approach to real-world scenarios and incorporatingAI-driven adjustments for more dynamic adaptability.</description>
      <author>example@mail.com (Halima Ibrahim Kure, Jishna Retnakumari, Lucian Nita, Saeed Sharif, Hamed Balogun, Augustine O. Nwajana)</author>
      <guid isPermaLink="false">2503.04340v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses in Human-Robot Bartending Tasks</title>
      <link>http://arxiv.org/abs/2503.04308v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE/RSJ International Conference on Intelligent Robots  and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于收集真实世界数据的方法，该方法使用RGB-D传感器并减少了人力投入。通过一个自动标注流水线，基于深度测量生成所有获取帧的标签。&lt;h4&gt;背景&lt;/h4&gt;现有的对象检测数据集往往未能充分涵盖眼镜的不同种类，这是由于眼镜具有透明和反射特性所致。特别是在广泛用于具身机器人代理中的开放词汇表目标检测器中，无法区分眼镜的子类别。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够有效采集并标注包含多种类型眼镜的真实世界数据的方法，并评估这种方法在实际应用中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个自动标签生成流水线，该流水线基于深度测量为所有获取到的帧创建标签。同时提供了一个新的真实世界玻璃物体数据集，该数据集是在NICOL（神经启发型协作机器人）平台上收集的，包含7850张图像，来自五个不同的相机。&lt;h4&gt;主要发现&lt;/h4&gt;训练后的基线模型在开放词汇表目标检测方法中表现优于现有最佳方法，并且在人类-机器人调酒场景中，在NICOL平台上的成功率为81%。&lt;h4&gt;结论&lt;/h4&gt;本文的方法不仅改进了数据采集和标注流程，还展示了其在实际具身代理应用中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Datasets for object detection often do not account for enough variety ofglasses, due to their transparent and reflective properties. Specifically,open-vocabulary object detectors, widely used in embodied robotic agents, failto distinguish subclasses of glasses. This scientific gap poses an issue torobotic applications that suffer from accumulating errors between detection,planning, and action execution. The paper introduces a novel method for theacquisition of real-world data from RGB-D sensors that minimizes human effort.We propose an auto-labeling pipeline that generates labels for all the acquiredframes based on the depth measurements. We provide a novel real-world glassobject dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), ahumanoid robot platform. The data set consists of 7850 images recorded fromfive different cameras. We show that our trained baseline model outperformsstate-of-the-art open-vocabulary approaches. In addition, we deploy ourbaseline model in an embodied agent approach to the NICOL platform, on which itachieves a success rate of 81% in a human-robot bartending scenario.</description>
      <author>example@mail.com (Lukáš Gajdošech, Hassan Ali, Jan-Gerrit Habekost, Martin Madaras, Matthias Kerzel, Stefan Wermter)</author>
      <guid isPermaLink="false">2503.04308v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Manipulation of Elasto-Flexible Cables with Single or Multiple UAVs</title>
      <link>http://arxiv.org/abs/2503.04304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了一类由多个四旋翼无人机操纵可变形和伸缩电缆的系统，并通过数值模拟验证了基于平滑度特性的轨迹生成的有效性。&lt;h4&gt;背景&lt;/h4&gt;处理由多架四旋翼无人机操作，具有复杂动态特性的长柔性和可扩展电缆系统的挑战。&lt;h4&gt;目的&lt;/h4&gt;为此类系统找到合适的数学模型并提出有效的控制策略。&lt;h4&gt;方法&lt;/h4&gt;采用离散化表示法来描述电缆，并将其分解成线性弹簧通过集中质量的被动球形关节相连。研究发现了这些系统的平面输出集，同时提出了基于反馈的闭环控制器以实现更好的操作效果。&lt;h4&gt;主要发现&lt;/h4&gt;数值仿真显示了依靠平滑度特性的轨迹可以成功地进行电缆操纵。实验验证了两个机器人示例中的离散化电缆模型的有效性，并测试了一个基于识别模型和使用电缆输出反馈的闭环控制器。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种有效的数学模型来描述四旋翼无人机操作下的可变形电缆系统，通过理论分析、数值模拟以及实际试验三方面验证了模型的有效性和控制策略的实际可行性。&lt;h4&gt;翻译&lt;/h4&gt;这项工作考虑了一系列由多个四旋翼无人机操纵的具有可变形和可伸缩特性的绳索系统的大型类别。该缆线采用离散化表示法描述，并将其分解为通过集中质量的被动球形关节相连的线性弹簧。对于这些系统，发现了平滑输出集。数值模拟支持了这一发现，展示了依靠基于平滑度的轨迹生成进行的缆线操作。最终，提出了两个机器人示例中所用离散化缆线模型有效性的实验验证，并且测试了一个基于识别模型和使用缆线反馈的闭环控制器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work considers a large class of systems composed of multiple quadrotorsmanipulating deformable and extensible cables. The cable is described via adiscretized representation, which decomposes it into linear springsinterconnected through lumped-mass passive spherical joints. Sets of flatoutputs are found for the systems. Numerical simulations support the findingsby showing cable manipulation relying on flatness-based trajectories.Eventually, we present an experimental validation of the effectiveness of theproposed discretized cable model for a two-robot example. Moreover, aclosed-loop controller based on the identified model and using cable-outputfeedback is experimentally tested.</description>
      <author>example@mail.com (Chiara Gabellieri, Lars Teeuwen, Yaolei Shen, Antonio Franchi)</author>
      <guid isPermaLink="false">2503.04304v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Robot Competence, Autonomy, and Personality on Trust Formation in Human-Robot Interaction</title>
      <link>http://arxiv.org/abs/2503.04296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了机器人能力、自主性和个性特征在任务导向的人机交互中对人类信任态度（认知和情感信任）及行为（任务委托）的影响。&lt;h4&gt;背景&lt;/h4&gt;以往的研究探索了影响总体信任的态度的机器人的特性，但是关于这些因素是否会影响行为信任以及它们如何影响人类将新任务委托给机器人的意愿尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;研究机器人能力、自主性和个性特征对认知和情感信任及任务委托行为的影响。&lt;h4&gt;方法&lt;/h4&gt;在任务导向的人机交互背景下进行的研究。&lt;h4&gt;主要发现&lt;/h4&gt;{'机器人能力': '是决定信任的关键因素，影响认知、情感以及行为信任。', '机器人的个性特质': '仅显著影响情感信任而不影响认知信任或信任行为。', '自主性': '调节能力和认知信任之间的关系，也调节个性和情感信任之间的关系。', '认知信任': '对任务委托有积极的影响，而情感信任没有显示显著效果。'}&lt;h4&gt;结论&lt;/h4&gt;本文为人类与机器人之间的相互信任提供了新的证据，并有助于设计能有效与人类互动并增强其信任的机器人。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经按照要求进行了分点总结，每个要点对应一个键值对。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human trust in social robots is a complex attitude based on cognitive andemotional evaluations, as well as a behavior, like task delegation. Whileprevious research explored the features of robots that influence overall trustattitude, it remains unclear whether these features affect behavioral trust.Additionally, there is limited investigation into which features of robotsinfluence cognitive and emotional attitudes, and how these attitudes impacthumans' willingness to delegate new tasks to robots. This study examines theinterplay between competence, autonomy, and personality traits of robots andtheir impact on trust attitudes (cognitive and affective trust) and trustbehavior (task delegation), within the context of task-oriented Human-RobotInteraction. Our findings indicate that robot competence is a key determinantof trust, influencing cognitive, affective, and behavioral trust. In contrast,robot personality traits significantly impact only affective trust withoutaffecting cognitive trust or trust behavior. In addition, autonomy was found tomoderate the relationship between competence and cognitive trust, as well asbetween personality and affective trust. Finally, cognitive trust was found topositively influence task delegation, whereas affective trust did not show asignificant effect. This paper contributes to the literature on Human-RobotTrust by providing novel evidence for the design of robots that can interacteffectively with humans and enhance their trust.</description>
      <author>example@mail.com (Filippo Cantucci, Marco Marini, Rino Falcone)</author>
      <guid isPermaLink="false">2503.04296v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Autonomous Reinforcement Learning for Real-World Robotic Manipulation with Large Language Models</title>
      <link>http://arxiv.org/abs/2503.04280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;最近在大型语言模型（LLMs）和视觉语言模型（VLMs）方面的发展极大地影响了机器人技术，使得高阶语义运动规划成为可能。强化学习作为补充范式，使代理能够通过交互和奖励信号自主优化复杂行为。&lt;h4&gt;背景&lt;/h4&gt;尽管强化学习具有强大的潜力，设计有效的奖励函数仍然是一项挑战，特别是在现实世界任务中，稀疏奖励不足以推动学习进程，而密集奖励又需要详细的设计。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种名为ARCHIE的无监督管道方法，利用预训练的语言模型GPT-4从自然语言的任务描述生成奖励函数。这种方法旨在将人类可读的文本自动转换为机器人可以执行的实际技能。&lt;h4&gt;方法&lt;/h4&gt;通过模拟环境中的实验验证了该方法的有效性。具体而言，GPT-4不仅能够根据任务描述生成奖励函数，还负责编码任务成功标准，从而实现从文字到机器人操作能力的一次性自动化流程。&lt;h4&gt;主要发现&lt;/h4&gt;使用ABB YuMi协作机器人的单臂和双臂手动操作任务进行的大量模拟实验表明了该方法的实际应用价值及其有效性。此外，在真实设备上进行了演示以验证其效果。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了如何利用先进的语言处理技术来解决强化学习中奖励设计的问题，为机器人自主性和灵活性开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;最近在大型语言模型（LLMs）和视觉语言模型（VLMs）方面的发展极大地影响了机器人技术。这些进展使得高阶语义运动规划成为可能，并通过结合使用自然语言处理技术和强化学习的方法来解决现实世界问题的挑战，尤其是那些涉及复杂行为优化的任务中稀疏奖励不足且密集奖励难以设计的情况。本文提出了一种利用GPT-4生成任务特定奖励函数和任务成功标准的方法，使机器人能够从人类可读的语言描述直接转换成执行操作的能力，并通过模拟环境中的实验验证了该方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Models (LLMs) and Visual LanguageModels (VLMs) have significantly impacted robotics, enabling high-levelsemantic motion planning applications. Reinforcement Learning (RL), acomplementary paradigm, enables agents to autonomously optimize complexbehaviors through interaction and reward signals. However, designing effectivereward functions for RL remains challenging, especially in real-world taskswhere sparse rewards are insufficient and dense rewards require elaboratedesign. In this work, we propose Autonomous Reinforcement learning for ComplexHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,a pre-trained LLM, to generate reward functions directly from natural languagetask descriptions. The rewards are used to train RL agents in simulatedenvironments, where we formalize the reward generation process to enhancefeasibility. Additionally, GPT-4 automates the coding of task success criteria,creating a fully automated, one-shot procedure for translating human-readabletext into deployable robot skills. Our approach is validated through extensivesimulated experiments on single-arm and bi-manual manipulation tasks using anABB YuMi collaborative robot, highlighting its practicality and effectiveness.Tasks are demonstrated on the real robot setup.</description>
      <author>example@mail.com (Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco)</author>
      <guid isPermaLink="false">2503.04280v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-based Analysis Of Highway Trajectory Planning Using High-Order Polynomial For Highly Automated Driving Function</title>
      <link>http://arxiv.org/abs/2503.04159v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于高阶多项式的自动车道变换轨迹规划方法，旨在提高高度自动化驾驶功能（HADF）在高速公路上的安全性和稳定性。&lt;h4&gt;背景&lt;/h4&gt;自主驾驶的一个基本任务是安全的路径规划，在此过程中需要避开障碍物、遵守交通规则并尊重道路的基本限制。实际应用中需考虑周围环境和车辆行为如车道变换、碰撞避免及车道合并等。&lt;h4&gt;目的&lt;/h4&gt;开发一种在高速公路上实现安全且无碰撞的车道变换轨迹的方法，以提高高度自动化驾驶功能的安全性和稳定性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个行为规划模块（BPM），该模块根据周围环境情况和车辆动作来制定高阶驾驶策略。利用多项式算法为同一方向双车道高速公路场景生成理想轨迹，并通过MATLAB仿真验证结果。&lt;h4&gt;主要发现&lt;/h4&gt;提出的车道变换方案经过建模与分析后，在安全性和稳定性方面取得了显著改进。&lt;h4&gt;结论&lt;/h4&gt;基于高阶多项式的规划系统能够有效地减少复杂性，支持快速计算，并且在特定环境条件下为高度自动化驾驶功能提供了安全的车道变换路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/iceccme52200.2021.9591044&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; One of the fundamental tasks of autonomous driving is safe trajectoryplanning, the task of deciding where the vehicle needs to drive, while avoidingobstacles, obeying safety rules, and respecting the fundamental limits of road.Real-world application of such a method involves consideration of surroundingenvironment conditions and movements such as Lane Change, collision avoidance,and lane merge. The focus of the paper is to develop and implement safecollision free highway Lane Change trajectory using high order polynomial forHighly Automated Driving Function (HADF). Planning is often considered as ahigher-level process than control. Behavior Planning Module (BPM) is designedthat plans the high-level driving actions like Lane Change maneuver to safelyachieve the functionality of transverse guidance ensuring safety of the vehicleusing motion planning in a scenario including environmental situation. Based onthe recommendation received from the (BPM), the function will generate a desirecorresponding trajectory. The proposed planning system is situation specificwith polynomial based algorithm for same direction two lane highway scenario.To support the trajectory system polynomial curve can be used to reducesoverall complexity and thereby allows rapid computation. The proposed LaneChange scenario is modeled, and results has been analyzed (verified andvalidate) through the MATLAB simulation environment. The method proposed inthis paper has achieved a significant improvement in safety and stability ofLane Changing maneuver.</description>
      <author>example@mail.com (Milin Patel, Marzana Khatun, Rolf Jung, Michael Glaß)</author>
      <guid isPermaLink="false">2503.04159v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Mixed Likelihood Variational Gaussian Processes</title>
      <link>http://arxiv.org/abs/2503.04138v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;高斯过程（GPs）在人类参与的实验中是一种强大的模型，但由于它们通常忽略辅助信息，如先验领域知识和非任务性能信息（例如用户信心评分），其应用受到了限制。本文提出了一种混合似然变分GP的方法，通过结合单一证据下界中的多个似然函数来利用这些辅助信息。&lt;h4&gt;背景&lt;/h4&gt;高斯过程由于其灵活性和良好的不确定性校准，在人类参与的实验中是强大的模型选择。然而，现有的GPs在建模人类响应时通常忽略了先验领域知识以及用户信心评分等非任务性能信息。&lt;h4&gt;目的&lt;/h4&gt;提出了一种利用辅助信息的方法——混合似然变分GP，并通过三个真实世界的人类参与者实验展示了这种方法的益处。&lt;h4&gt;方法&lt;/h4&gt;1. 在视觉感知任务中，使用混合似然训练对高斯过程分类器施加先验知识约束，加速主动学习。            2. 利用Likert尺度信心评分并通过混合似然训练改进表面粗糙度的触觉感知模型拟合。            3. 表明用户信心评分可以改善机器人步态优化中的人类偏好学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过将这些不同的输入类型联合建模，使用混合似然的方法在主动学习和偏好学习中利用辅助信息带来了性能提升。具体来说，在视觉感知任务中的快速学习、触觉感知的改进模型拟合以及用户信心评分对机器人步态优化的人类偏好学习的影响。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法展示了将先验知识和其他类型的非任务性能数据纳入GP建模的重要性，这有助于提高人类参与实验的研究效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Gaussian processes (GPs) are powerful models for human-in-the-loopexperiments due to their flexibility and well-calibrated uncertainty. However,GPs modeling human responses typically ignore auxiliary information, includinga priori domain expertise and non-task performance information like userconfidence ratings. We propose mixed likelihood variational GPs to leverageauxiliary information, which combine multiple likelihoods in a single evidencelower bound to model multiple types of data. We demonstrate the benefits ofmixing likelihoods in three real-world experiments with human participants.First, we use mixed likelihood training to impose prior knowledge constraintsin GP classifiers, which accelerates active learning in a visual perceptiontask where users are asked to identify geometric errors resulting from cameraposition errors in virtual reality. Second, we show that leveraging Likertscale confidence ratings by mixed likelihood training improves model fittingfor haptic perception of surface roughness. Lastly, we show that Likert scaleconfidence ratings improve human preference learning in robot gaitoptimization. The modeling performance improvements found using our frameworkacross this diverse set of applications illustrates the benefits ofincorporating auxiliary information into active learning and preferencelearning by using mixed likelihoods to jointly model multiple inputs.</description>
      <author>example@mail.com (Kaiwen Wu, Craig Sanders, Benjamin Letham, Phillip Guan)</author>
      <guid isPermaLink="false">2503.04138v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and Mapping for Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2503.04126v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Decentralized Visual Monocular SLAM (DVM-SLAM)，这是一个开放源代码的分散式单目视觉C-SLAM系统，适用于低成本、轻量级的小型机器人和微型飞行器。&lt;h4&gt;背景&lt;/h4&gt;Cooperative Simultaneous Localization and Mapping（协作的同时定位与地图构建）使多个代理能够合作在未知环境中进行地图构建并同时估计各自的位置。这种做法通过共享信息来提高鲁棒性、可扩展性和准确性，减少漂移，并支持更大区域的集体探索。&lt;h4&gt;目的&lt;/h4&gt;开发适用于小型机器人和微型飞行器的小型化单目视觉传感器C-SLAM系统，以实现多代理自主导航的实际应用。&lt;h4&gt;方法&lt;/h4&gt;构建了一个分散式的单目视觉SLAM系统DVM-SLAM，在物理机器人上进行了测试，并采用了一种定制的碰撞避免框架来验证其现实世界的适用性。&lt;h4&gt;主要发现&lt;/h4&gt;展示了DVM-SLAM与最先进的集中式单目C-SLAM系统的精度相当，证明了该方法的有效性和准确性。&lt;h4&gt;结论&lt;/h4&gt;通过开放源代码和在线提供补充材料，使其他研究者能够利用并改进这项技术。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到的是一种名为Decentralized Visual Monocular SLAM（分散式单目视觉SLAM）的技术，这是第一个开源的、基于单目的协作同时定位与地图构建系统。该系统适用于低成本且轻量级的小型机器人和微型飞行器，并通过实际物理机器人的测试验证了其在实时多代理自主导航场景中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multipleagents to work together in mapping unknown environments while simultaneouslyestimating their own positions. This approach enhances robustness, scalability,and accuracy by sharing information between agents, reducing drift, andenabling collective exploration of larger areas. In this paper, we presentDecentralized Visual Monocular SLAM (DVM-SLAM), the first open-sourcedecentralized monocular C-SLAM system. By only utilizing low-cost andlight-weight monocular vision sensors, our system is well suited for smallrobots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability isvalidated on physical robots with a custom collision avoidance framework,showcasing its potential in real-time multi-agent autonomous navigationscenarios. We also demonstrate comparable accuracy to state-of-the-artcentralized monocular C-SLAM systems. We open-source our code and providesupplementary material online.</description>
      <author>example@mail.com (Joshua Bird, Jan Blumenkamp, Amanda Prorok)</author>
      <guid isPermaLink="false">2503.04126v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2503.04123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于灵巧抓握生成的框架GAGrasp，该框架利用了几何代数表示以使SE(3)变换下的等变性得以实现。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在面对多样化的物体姿态时数据效率和参数效率较低，并且难以保证产生的抓取方案具有物理合理性和稳定性。&lt;h4&gt;目的&lt;/h4&gt;通过直接将SE(3)对称约束编码到架构中，以提高数据效率、参数效率并增强不同对象姿态下的灵巧抓握生成能力。&lt;h4&gt;方法&lt;/h4&gt;引入了一个可微分的物理信息细化层，确保产生的抓取方案在物理上是合理的且稳定可靠的。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验展示了该模型在泛化性、稳定性以及适应性方面的卓越性能，并优于现有的其他方法。&lt;h4&gt;结论&lt;/h4&gt;GAGrasp框架通过利用几何代数和引入可微分的物理信息细化层，显著提高了灵巧抓握生成的质量与效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的框架GAGrasp，该框架采用几何代数表示来确保SE(3)变换下的等变性。我们的方法在直接将SE(3)对称约束编码到架构中后，不仅提升了数据和参数的效率，还增强了生成抓握方案的鲁棒性和广泛适应性。此外，我们引入了一个可微分的物理信息细化层，确保了产生的抓取方案是合理的且稳定的。大量的实验显示，与现有方法相比，该模型在泛化性、稳定性及适应性方面具有显著的优势。更多细节参见https://gagrasp.github.io/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose GAGrasp, a novel framework for dexterous grasp generation thatleverages geometric algebra representations to enforce equivariance to SE(3)transformations. By encoding the SE(3) symmetry constraint directly into thearchitecture, our method improves data and parameter efficiency while enablingrobust grasp generation across diverse object poses. Additionally, weincorporate a differentiable physics-informed refinement layer, which ensuresthat generated grasps are physically plausible and stable. Extensiveexperiments demonstrate the model's superior performance in generalization,stability, and adaptability compared to existing methods. Additional details athttps://gagrasp.github.io/</description>
      <author>example@mail.com (Tao Zhong, Christine Allen-Blanchette)</author>
      <guid isPermaLink="false">2503.04123v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>The Spinning Blimp: Design and Control of a Novel Minimalist Aerial Vehicle Leveraging Rotational Dynamics and Locomotion</title>
      <link>http://arxiv.org/abs/2503.04112v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the IEEE international conference on robotics and  automation(ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了Spinning Blimp，这是一种新型的轻于空气（LTA）飞行器，设计用于低能耗稳定的飞行。&lt;h4&gt;背景&lt;/h4&gt;现有的轻于空气飞行器在稳定性和能源效率方面存在局限性。需要一种新的设计方案来解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;提出并验证一个低成本、高效的轻于空气飞行器设计方案，适用于多种应用场景。&lt;h4&gt;方法&lt;/h4&gt;Spinning Blimp使用扁椭球形氦气球提供浮力，并采用被动式机翼结合螺旋桨的组合结构，在飞行中产生旋转行为，从而实现类似摆动式的稳定效果。此外，还提出了一种利用持续旋转特性控制平移运动的控制策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证实了该设计的有效性，展示了其作为多样化和经济适用解决方案的潜力。&lt;h4&gt;结论&lt;/h4&gt;Spinning Blimp作为一种低成本、高效率的设计方案，在多种应用领域中表现出良好的性能与适应性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文提出了一种新型的轻于空气（LTA）飞行器——Spinning Blimp，专为低能耗稳定飞行设计。通过使用扁椭球形氦气球来提供浮力，该飞行器在保持长时间空中停留的同时实现了最小的能量消耗。独特的低成本设计方案结合被动式机翼和螺旋桨，在飞行中诱导出旋转行为，从而提供了固有的摆动式稳定性。我们提出了一种控制策略，利用Spinning Blimp持续旋转的特性来控制平移运动。由于其成本效益高，该飞行器非常适合多种应用场景，如巡逻、定位、空气和湍流监测以及家庭监控等。实验评估验证了设计的有效性，并强调了它作为多样性和经济适用性的解决方案在空中应用中的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-06&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents the Spinning Blimp, a novel lighter-than-air (LTA) aerialvehicle designed for low-energy stable flight. Utilizing an oblate spheroidhelium balloon for buoyancy, the vehicle achieves minimal energy consumptionwhile maintaining prolonged airborne states. The unique and low-cost designemploys a passively arranged wing coupled with a propeller to induce a spinningbehavior, providing inherent pendulum-like stabilization. We propose a controlstrategy that takes advantage of the continuous revolving nature of thespinning blimp to control translational motion. The cost-effectiveness of thevehicle makes it highly suitable for a variety of applications, such aspatrolling, localization, air and turbulence monitoring, and domesticsurveillance. Experimental evaluations affirm the design's efficacy andunderscore its potential as a versatile and economically viable solution foraerial applications.</description>
      <author>example@mail.com (Leonardo Santens, Diego S. D'Antonio, Shuhang Hou, David Saldaña)</author>
      <guid isPermaLink="false">2503.04112v1</guid>
      <pubDate>Fri, 07 Mar 2025 17:52:40 +0800</pubDate>
    </item>
    <item>
      <title>Towards Efficient Contrastive PAC Learning</title>
      <link>http://arxiv.org/abs/2502.15962v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文在PAC学习框架下研究对比学习，揭示了对于线性表示这一基本概念的对比学习存在高效PAC学习器的问题仍未解决，并提出了一种基于Rademacher复杂度的泛化保证方法。&lt;h4&gt;背景&lt;/h4&gt;尽管最近有许多关于对比损失下的统计结果的研究，这些算法通常效率不高或无法提供PAC保证。&lt;h4&gt;目的&lt;/h4&gt;探讨线性表示的对比学习问题是否可以在PAC框架下以高效的方式解决，并尝试构建一个有效的对比学习算法。&lt;h4&gt;方法&lt;/h4&gt;首先证明了在一般情况下，对比PAC学习线性表示是难以处理的问题。接着展示当对比样本之间的距离用L2范数度量时，该问题可以松弛为半定规划(SDP)形式。然后基于Rademacher复杂度建立泛化保证，并将这些保证与某些条件下对比大间隔条件下的PAC保证联系起来。&lt;h4&gt;主要发现&lt;/h4&gt;在对比学习的线性表示这一基本设置下，存在高效的PAC学习器的问题依然开放。提出了一个通过半定规划放松问题的方法以及一种基于Rademacher复杂度的泛化保证方法，这是已知的第一个有效的对比学习的PAC算法。&lt;h4&gt;结论&lt;/h4&gt;论文证明了对比学习中的某些核心挑战，并提出了一种新的解决策略和理论基础，为高效且具有PAC保证的学习提供了可能性。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在PAC学习框架下的对比学习。尽管最近有许多关于基于VC维或Rademacher复杂度的对比损失下统计结果的研究，但这些算法通常是低效的或无法提供PAC保证。在这篇论文中，我们考虑了线性表示这一基本概念的学习问题，并发现即使在这种简单的设置下，高效且具有PAC保证的学习器的存在仍然是一个开放的问题。我们首先表明一般情况下对比PAC学习线性表示是难以处理的，然后展示当对比样本之间的距离用L2范数度量时该问题可以松弛为半定规划形式。接着基于Rademacher复杂度建立了泛化保证，并将这些保证与某些条件下对比大间隔条件下的PAC保证联系起来。据我们所知，这是第一个有效的对比学习的PAC算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study contrastive learning under the PAC learning framework. While aseries of recent works have shown statistical results for learning undercontrastive loss, based either on the VC-dimension or Rademacher complexity,their algorithms are inherently inefficient or not implying PAC guarantees. Inthis paper, we consider contrastive learning of the fundamental concept oflinear representations. Surprisingly, even under such basic setting, theexistence of efficient PAC learners is largely open. We first show that theproblem of contrastive PAC learning of linear representations is intractable tosolve in general. We then show that it can be relaxed to a semi-definiteprogram when the distance between contrastive samples is measured by the$\ell_2$-norm. We then establish generalization guarantees based on Rademachercomplexity, and connect it to PAC guarantees under certain contrastivelarge-margin conditions. To the best of our knowledge, this is the firstefficient PAC learning algorithm for contrastive learning.</description>
      <author>example@mail.com (Jie Shen)</author>
      <guid isPermaLink="false">2502.15962v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
  <item>
      <title>Intermediate Domain-guided Adaptation for Unsupervised Chorioallantoic Membrane Vessel Segmentation</title>
      <link>http://arxiv.org/abs/2503.03546v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的Intermediate Domain-guided Adaptation (IDA) 方法，利用CAM图像和视网膜图像之间的相似性以及现有的公共视网膜数据集进行无监督训练。&lt;h4&gt;背景&lt;/h4&gt;在血管生成研究中广泛使用的胎盘绒毛膜尿囊膜模型（CAM）的血血管分布是关键评价指标。由于手动分割耗时且主观性强，因此开发自动化的血管分割算法至关重要。&lt;h4&gt;目的&lt;/h4&gt;为解决现有CAM血管分割算法有限以及公开数据集不足的问题，提出了一种创新的方法来改进无监督域适应技术。&lt;h4&gt;方法&lt;/h4&gt;提出Multi-Resolution Asymmetric Translation (MRAT)策略生成中间图像以促进图像级交互；开发Intermediate Domain-guided Contrastive Learning (IDCL)模块以解开跨域特征表示。使用公开的视网膜数据集进行训练，并创建首个CAM数据集进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，该方法在所提出的CAM数据集中表现优异，同时在不同的视网膜数据集中也展示了强大的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;IDa方法克服了现有无监督域适应（UDA）技术只关注源目标直接对齐的局限性，并成功应用于血管生成研究中，有望改进其他生物医学图像处理任务。&lt;h4&gt;翻译&lt;/h4&gt;绒毛尿囊膜模型广泛用于血管生成研究，血管分布是评估的关键指标。因此，基于拓扑和形态学特征的定量评价需要精确分割血管。然而，手动分割耗时且易出错。此外，关于CAM血管分割算法的研究有限，缺乏公开数据集导致预测性能不佳。为解决这些问题，本文提出了一种新颖的方法：Intermediate Domain-guided Adaptation (IDA) 方法，利用了CAM图像和视网膜图像的相似性以及现有的公共视网膜数据集进行无监督训练。通过在首个创建的CAM数据集中进行全面实验，证明该方法优于其他现有方法，并且在不同的视网膜数据集中也展现了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The chorioallantoic membrane (CAM) model is widely employed in angiogenesisresearch, and distribution of growing blood vessels is the key evaluationindicator. As a result, vessel segmentation is crucial for quantitativeassessment based on topology and morphology. However, manual segmentation isextremely time-consuming, labor-intensive, and prone to inconsistency due toits subjective nature. Moreover, research on CAM vessel segmentation algorithmsremains limited, and the lack of public datasets contributes to poor predictionperformance. To address these challenges, we propose an innovative IntermediateDomain-guided Adaptation (IDA) method, which utilizes the similarity betweenCAM images and retinal images, along with existing public retinal datasets, toperform unsupervised training on CAM images. Specifically, we introduce aMulti-Resolution Asymmetric Translation (MRAT) strategy to generateintermediate images to promote image-level interaction. Then, an IntermediateDomain-guided Contrastive Learning (IDCL) module is developed to disentanglecross-domain feature representations. This method overcomes the limitations ofexisting unsupervised domain adaptation (UDA) approaches, which primarilyconcentrate on directly source-target alignment while neglecting intermediatedomain information. Notably, we create the first CAM dataset to validate theproposed algorithm. Extensive experiments on this dataset show that our methodoutperforms compared approaches. Moreover, it achieves superior performance inUDA tasks across retinal datasets, highlighting its strong generalizationcapability. The CAM dataset and source codes are available athttps://github.com/PWSong-ustc/IDA.</description>
      <author>example@mail.com (Pengwu Song, Liang Xu, Peng Yao, Shuwei Shen, Pengfei Shao, Mingzhai Sun, Ronald X. Xu)</author>
      <guid isPermaLink="false">2503.03546v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GEN3C: 3D-Informed World-Consistent Video Generation with Precise Camera Control</title>
      <link>http://arxiv.org/abs/2503.03751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To appear in CVPR 2025. Website:  https://research.nvidia.com/labs/toronto-ai/GEN3C/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GEN3C是一种基于精确相机控制和时间三维一致性的生成视频模型。&lt;h4&gt;背景&lt;/h4&gt;现有的视频生成模型虽然能够产生真实的视频，但很少利用三维信息，导致物体突然出现或消失等问题。此外，现有模型中的相机控制不够精准。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有视频生成模型中存在的问题，并提高视频的逼真度和一致性。&lt;h4&gt;方法&lt;/h4&gt;GEN3C通过预测种子图像或先前生成帧的像素深度来获取点云作为三维缓存。在生成下一帧时，基于用户提供的新相机轨迹对三维缓存进行2D渲染。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相较于之前的工作，GEN3C实现了更精确的相机控制，并且在稀疏视图新颖视角合成方面取得了最先进的成果，特别是在驾驶场景和单目动态视频等具有挑战性的设置中表现尤为出色。&lt;h4&gt;结论&lt;/h4&gt;该研究为生成逼真、一致的三维视频提供了一种创新的方法，有望应用于更多领域。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了GEN3C，这是一种具备精确相机控制能力和时间三维一致性的生成视频模型。先前的视频模型已经可以生成现实感强的视频，但它们倾向于利用很少的三维信息，导致不一致性问题，如物体突然出现或消失等现象。如果实现了任何相机控制，通常也是不够精准的，因为摄像机参数仅仅是输入给神经网络的部分数据，需要网络自己推断视频是如何依赖于摄像机姿态的。相比之下，GEN3C使用一个由点云构成的三维缓存指导其工作：通过预测种子图像或先前生成帧的像素深度得到这些点云。当生成下一帧时，模型基于用户提供的新相机轨迹对三维缓存进行2D渲染来条件化。至关重要的是，这意味着GEN3C不必记住之前产生的内容，也不必从摄像机姿态推断出图像结构。相反，它可以将全部的生成能力集中在尚未观察到的区域，并推进场景状态到达下一帧。我们的实验结果表明比之前的模型具有更精确的相机控制能力，并且在稀疏视图新颖视角合成方面取得了最先进的成果，特别是在驾驶场景和单目动态视频等复杂设置中表现尤为出色。观看视频以查看最佳效果！请访问我们的网页：https://research.nvidia.com/labs/toronto-ai/GEN3C/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/nv-tlabs/GEN3C&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present GEN3C, a generative video model with precise Camera Control andtemporal 3D Consistency. Prior video models already generate realistic videos,but they tend to leverage little 3D information, leading to inconsistencies,such as objects popping in and out of existence. Camera control, if implementedat all, is imprecise, because camera parameters are mere inputs to the neuralnetwork which must then infer how the video depends on the camera. In contrast,GEN3C is guided by a 3D cache: point clouds obtained by predicting thepixel-wise depth of seed images or previously generated frames. When generatingthe next frames, GEN3C is conditioned on the 2D renderings of the 3D cache withthe new camera trajectory provided by the user. Crucially, this means thatGEN3C neither has to remember what it previously generated nor does it have toinfer the image structure from the camera pose. The model, instead, can focusall its generative power on previously unobserved regions, as well as advancingthe scene state to the next frame. Our results demonstrate more precise cameracontrol than prior work, as well as state-of-the-art results in sparse-viewnovel view synthesis, even in challenging settings such as driving scenes andmonocular dynamic video. Results are best viewed in videos. Check out ourwebpage! https://research.nvidia.com/labs/toronto-ai/GEN3C/</description>
      <author>example@mail.com (Xuanchi Ren, Tianchang Shen, Jiahui Huang, Huan Ling, Yifan Lu, Merlin Nimier-David, Thomas Müller, Alexander Keller, Sanja Fidler, Jun Gao)</author>
      <guid isPermaLink="false">2503.03751v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient LiDAR Semantic Segmentation with 2D-3D Vision Transformer Adapters</title>
      <link>http://arxiv.org/abs/2503.03299v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为BALViT的新方法，该方法通过冻结的视觉模型作为模态特征编码器来学习强大的LiDAR编码器。结合范围视图和鸟瞰视角的LiDAR编码机制，并引入了新的2D-3D适配器来提高性能。&lt;h4&gt;背景&lt;/h4&gt;当前的LiDAR语义分割模型由于缺乏大规模多样化的数据集而难以进行通用预训练，大多数点云分割架构包含定制网络层，限制了视觉基础架构进步的应用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法BALViT，旨在通过冻结的视觉模型作为模态特征编码器来增强LiDAR编码能力，并在小数据场景下实现高性能。&lt;h4&gt;方法&lt;/h4&gt;BALViT采用范围视图和鸟瞰视角相结合的方式进行LiDAR编码。范围视图特征经过冻结的图像骨干网络处理，而鸟瞰视角分支通过多次交叉注意力交互增强这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在小数据集环境下能够显著提高性能，并且在SemanticKITTI和nuScenes基准测试中优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;研究证明了BALViT方法的有效性，可以为其他相关领域提供有价值的参考。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR语义分割模型通常从随机初始化训练开始，由于缺乏大规模多样化数据集，通用预训练受阻。此外，大多数点云分割架构包含定制网络层，限制了视觉基础架构进步的应用性。受到统一基础模型最新进展的启发，我们提出了BALViT这一新方法，该方法利用冻结视觉模型作为模态特征编码器来学习强大的LiDAR编码器。具体而言，BALViT结合了范围视图和鸟瞰视角的LiDAR编码机制，并通过新的2D-3D适配器组合这些机制。虽然范围视图特征经过冻结图像骨干网络处理，但我们的鸟瞰视角分支通过多次交叉注意力交互增强这些特征，因此可以不断改进视觉网络以融入领域相关的知识，从而产生强大的标签高效LiDAR编码机制。在SemanticKITTI和nuScenes基准上的广泛评估表明，在小数据集环境中，它优于现有方法。我们将在http://balvit.cs.uni-freiburg.de上公开代码和模型。&lt;h4&gt;贡献&lt;/h4&gt;提出了BALViT框架，解决了当前LiDAR语义分割模型缺乏大规模多样化数据集的问题，并在多个性能指标上取得了显著提升&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR semantic segmentation models are typically trained from randominitialization as universal pre-training is hindered by the lack of large,diverse datasets. Moreover, most point cloud segmentation architecturesincorporate custom network layers, limiting the transferability of advancesfrom vision-based architectures. Inspired by recent advances in universalfoundation models, we propose BALViT, a novel approach that leverages frozenvision models as amodal feature encoders for learning strong LiDAR encoders.Specifically, BALViT incorporates both range-view and bird's-eye-view LiDARencoding mechanisms, which we combine through a novel 2D-3D adapter. While therange-view features are processed through a frozen image backbone, ourbird's-eye-view branch enhances them through multiple cross-attentioninteractions. Thereby, we continuously improve the vision network withdomain-dependent knowledge, resulting in a strong label-efficient LiDARencoding mechanism. Extensive evaluations of BALViT on the SemanticKITTI andnuScenes benchmarks demonstrate that it outperforms state-of-the-art methods onsmall data regimes. We make the code and models publicly available at:http://balvit.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Julia Hindel, Rohit Mohan, Jelena Bratulic, Daniele Cattaneo, Thomas Brox, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.03299v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Meta-Learning to Explore via Memory Density Feedback</title>
      <link>http://arxiv.org/abs/2503.02831v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种利用元学习探索算法，该算法使智能体能够最大化其在单一时间间隔内的探索进度。&lt;h4&gt;背景&lt;/h4&gt;传统的强化学习探索方法通常通过添加一种内在奖励来替换或增强原始奖赏函数，以促使智能体探索未曾见过的状态。&lt;h4&gt;目的&lt;/h4&gt;研究旨在设计一种新型的元学习探索算法，该算法允许智能体在训练周期间甚至未训练过的环境中最大化其探索进度。&lt;h4&gt;方法&lt;/h4&gt;智能体会学习一种策略，即最小化新观察结果相对于所有记忆的概率密度，并根据当前观测密度接收反馈，在递归网络中保存这些反馈。通过这种方式，智能体学会了实时导航熟悉度不断增长的复杂环境。&lt;h4&gt;主要发现&lt;/h4&gt;基于上述设计，智能体可以在完全新颖的状态下进行探索并最大化其探索进度，即使在其策略没有为此类状态训练过的情况下也是如此。&lt;h4&gt;结论&lt;/h4&gt;该元学习方法有效提高了智能体在未见过环境中探索的能力和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的探索算法通过利用元学习（即学会学习）来增强智能体在一个时间间隔内优化其探索进度的能力，即使是在训练周期之间也能实现。这种策略帮助智能体最小化新观察结果相对于所有已存记忆的概率密度，并且能够根据当前观测密度反馈进行调整。这样做的结果是，智能体可以实时地导航一个复杂且不断扩展的熟悉度景观，在从未见过的状态中最大化其探索进度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploration algorithms for reinforcement learning typically replace oraugment the reward function with an additional ``intrinsic'' reward that trainsthe agent to seek previously unseen states of the environment. Here, weconsider an exploration algorithm that exploits meta-learning, or learning tolearn, such that the agent learns to maximize its exploration progress within asingle episode, even between epochs of training. The agent learns a policy thataims to minimize the probability density of new observations with respect toall of its memories. In addition, it receives as feedback evaluations of thecurrent observation density and retains that feedback in a recurrent network.By remembering trajectories of density, the agent learns to navigate a complexand growing landscape of familiarity in real-time, allowing it to maximize itsexploration progress even in completely novel states of the environment forwhich its policy has not been trained.</description>
      <author>example@mail.com (Kevin L. McKee)</author>
      <guid isPermaLink="false">2503.02831v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances Formal Theorem Proving</title>
      <link>http://arxiv.org/abs/2503.03205v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种多代理框架MA-LoT，该框架结合了自然语言推理和形式语言验证，在Lean4定理证明中取得了显著的性能提升。&lt;h4&gt;背景&lt;/h4&gt;使用计算机可验证的语言（如Lean）解决数学问题对数学和计算机科学领域产生了重大影响。当前最先进的方法依赖于单一大型语言模型（LLMs），这些模型作为代理或证明者，负责生成完整证明或进行树搜索，但缺乏将自然语言推理与形式语言验证反馈相结合的结构化方式。&lt;h4&gt;目的&lt;/h4&gt;为了克服单一代理方法的局限性，并结合高级自然语言推理和形式语言验证，本文提出了MA-LoT框架。&lt;h4&gt;方法&lt;/h4&gt;该框架利用长链思维（Long CoT）中的涌现形式推理能力以及新颖的LoT-Transfer学习训练推断流水线。通过这种方式，在证明生成中实现了更深入的洞察力和长期一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，MA-LoT在Lean4版本的MiniF2F-Test数据集上达到了54.51%的准确率，显著优于GPT-4（22.95%）、单代理树搜索(InternLM-Step-Prover, 50.70%)和完整证明生成(DeepSeek-Prover-v1.5, 48.36%)基线。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了结合长链思维与形式验证在更广泛的视角中进行更为深刻生成的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Solving mathematical problems using computer-verifiable languages like Leanhas significantly impacted mathematical and computer science communities.State-of-the-art methods utilize single Large Language Models (LLMs) as agentsor provers to either generate complete proof or perform tree searches. However,single-agent methods inherently lack a structured way to combine high-levelreasoning in Natural Language (NL) with Formal Language (FL) verificationfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based LongChain-of-Thought framework, (to the best of our knowledge), the firstmulti-agent framework for Lean4 theorem proving that balance high-level NLreasoning and FL verification in Long CoT. Using this structured interaction,our approach enables deeper insights and long-term coherence in proofgeneration, with which past methods struggle. We do this by leveraging emergentformal reasoning ability in Long CoT using our novel LoT-Transfer Learningtraining-inference pipeline. Extensive experiments show that our frameworkachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,largely outperforming GPT-4 (22.95%), single-agent tree search(InternLM-Step-Prover, 50.70%), and whole-proof generation(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlightthe potential of combining Long CoT with formal verification for a moreinsightful generation in a broader perspective.</description>
      <author>example@mail.com (Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang)</author>
      <guid isPermaLink="false">2503.03205v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm using Consistency Evaluation</title>
      <link>http://arxiv.org/abs/2503.03599v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REGRACE是一种新颖的方法，利用LiDAR子图解决了大尺度导航中回环闭合的可扩展性和视角差异问题。&lt;h4&gt;背景&lt;/h4&gt;当前使用密集点云进行精确位置识别的方法由于扫描到扫描之间的比较计算量过大而不具备良好的可扩展性。而基于对象的方法虽然效率更高，但往往对视点变化敏感。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法REGRACE来解决大规模导航中回环闭合的挑战问题，提高系统的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;引入旋转不变特征和图神经网络增强的邻居上下文信息。使用词袋模型进行子地图之间的高效匹配，并利用几何一致性识别远程回环闭合。&lt;h4&gt;主要发现&lt;/h4&gt;REGRACE在保持高精度的同时提高了速度，与最先进的位置识别基线相比快一倍。&lt;h4&gt;结论&lt;/h4&gt;REGRACE能够在大规模导航环境中有效地实现准确的回环闭合检测，具备良好的可扩展性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;循环闭合对于校正里程计漂移和创建一致的地图至关重要，尤其是在大尺度导航中。当前使用密集点云的方法由于扫描到扫描之间的比较计算量过大而不具良好扩展性。基于对象的替代方法虽然更高效，但往往对视点变化敏感。在这项工作中，我们引入了REGRACE，这是一种新颖的方法，通过使用LiDAR基子图来解决重新定位中的可扩展性和视角差异问题。我们介绍了每个标记对象的旋转不变特征，并通过图神经网络增强它们以考虑邻居上下文信息。为了识别潜在重复访问，我们采用了一种可扩展的词袋方法，每幅子地图池化一个学习到的全局特征。另外，我们用几何一致性线索来定义重新访问，而不是基于嵌入距离，这使我们可以识别远处的回环闭合。我们的评估表明，REGRACE在位置识别和配准基线方面取得了相似的结果，而速度却快了一倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Loop closures are essential for correcting odometry drift and creatingconsistent maps, especially in the context of large-scale navigation. Currentmethods using dense point clouds for accurate place recognition do not scalewell due to computationally expensive scan-to-scan comparisons. Alternativeobject-centric approaches are more efficient but often struggle withsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novelapproach that addresses these challenges of scalability and perspectivedifference in re-localization by using LiDAR-based submaps. We introducerotation-invariant features for each labeled object and enhance them withneighborhood context through a graph neural network. To identify potentialrevisits, we employ a scalable bag-of-words approach, pooling one learnedglobal feature per submap. Additionally, we define a revisit with geometricalconsistency cues rather than embedding distance, allowing us to recognizefar-away loop closures. Our evaluations demonstrate that REGRACE achievessimilar results compared to state-of-the-art place recognition and registrationbaselines while being twice as fast.</description>
      <author>example@mail.com (Débora N. P. Oliveira, Joshua Knights, Sebastián Barbas Laina, Simon Boche, Wolfram Burgard, Stefan Leutenegger)</author>
      <guid isPermaLink="false">2503.03599v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Feature Matching Intervention: Leveraging Observational Data for Causal Representation Learning</title>
      <link>http://arxiv.org/abs/2503.03634v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种创新的方法，称为特征匹配干预（Feature Matching Intervention, FMI），用于从观测数据中进行因果发现。&lt;h4&gt;背景&lt;/h4&gt;在从观察性数据中进行因果发现时，缺乏完美干预是一个主要挑战，这使得区分真正的因果特征和虚假的特征变得困难。&lt;h4&gt;目的&lt;/h4&gt;通过使用匹配程序模拟完美的干预来识别因果关系，并定义了因果潜在图（Causal Latent Graphs），这种框架连接了FMI与因果图学习。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的理论框架，即因果潜在图，并开发了一个模仿完美干预的特征匹配过程。&lt;h4&gt;主要发现&lt;/h4&gt;理论上表明，FMI表现出强大的出分布（OOD）泛化能力。实验进一步证实了FMI在仅从观察数据中有效识别因果特征方面的卓越性能。&lt;h4&gt;结论&lt;/h4&gt;该方法为解决观测性数据分析中的因果关系问题提供了一个新的视角和有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在从观测数据进行因果发现时，缺乏完美的干预是一个主要挑战。为了应对这一问题，我们提出了一种创新的方法，称为特征匹配干预（Feature Matching Intervention, FMI），该方法通过使用一个匹配程序来模仿完美干预，并定义了因果潜在图，即扩展到潜在特征空间的结构因果模型，为FMI与因果图学习之间的连接提供了一个框架。我们的特征匹配过程在这些因果潜在图中模拟完美的干预行为。理论结果表明，FMI具有强大的出分布（OOD）泛化能力。实验进一步证明了FMI仅从观测数据有效识别因果特征方面的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major challenge in causal discovery from observational data is the absenceof perfect interventions, making it difficult to distinguish causal featuresfrom spurious ones. We propose an innovative approach, Feature MatchingIntervention (FMI), which uses a matching procedure to mimic perfectinterventions. We define causal latent graphs, extending structural causalmodels to latent feature space, providing a framework that connects FMI withcausal graph learning. Our feature matching procedure emulates perfectinterventions within these causal latent graphs. Theoretical resultsdemonstrate that FMI exhibits strong out-of-distribution (OOD)generalizability. Experiments further highlight FMI's superior performance ineffectively identifying causal features solely from observational data.</description>
      <author>example@mail.com (Haoze Li, Jun Xie)</author>
      <guid isPermaLink="false">2503.03634v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DDEQs: Distributional Deep Equilibrium Models through Wasserstein Gradient Flows</title>
      <link>http://arxiv.org/abs/2503.01140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  39 pages, 17 figures. To be published in AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了分布式深度平衡模型(DDEQ)，它是隐含神经网络的一种，扩展了Deep Equilibrium Models (DEQs) 至离散度量输入。&lt;h4&gt;背景&lt;/h4&gt;传统的DEQ主要处理序列数据，但已经应用于各种类型的数据。然而，现有的框架没有充分利用离散度量的特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的理论基础框架来支持基于离散度量（如集合或点云）的深度平衡模型，以提高其在特定任务中的表现和参数效率。&lt;h4&gt;方法&lt;/h4&gt;通过利用Wasserstein梯度流，展示了如何调整DEQ前向传递的方式，在交换不变性的条件下找到离散度量下的固定点，并推导出适当的网络架构以适应DDEQ。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与最先进的模型相比，DDEQ在点云分类和点云补全等任务上具有竞争力，同时显著减少了参数的使用效率。&lt;h4&gt;结论&lt;/h4&gt;通过扩展DEQ框架来处理离散度量输入，如集合或点云，可以提高模型性能并降低参数需求。&lt;h4&gt;翻译&lt;/h4&gt;深度平衡模型(DDEQs) 是一种隐含神经网络类，它将Deep Equilibrium Models (DEQs) 扩展到离散度量输入(例如集合或点云)，提供了理论基础框架。通过使用Wasserstein梯度流, 展示了如何调整DEQ前向传递以在交换不变性的条件下找到离散度量的固定点，并推导出适应DDEQs 的网络架构。实验表明，它们可以与最先进的模型竞争，完成如点云分类和补全等任务，同时参数效率显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Equilibrium Models (DEQs) are a class of implicit neural networks thatsolve for a fixed point of a neural network in their forward pass.Traditionally, DEQs take sequences as inputs, but have since been applied to avariety of data. In this work, we present Distributional Deep EquilibriumModels (DDEQs), extending DEQs to discrete measure inputs, such as sets orpoint clouds. We provide a theoretically grounded framework for DDEQs.Leveraging Wasserstein gradient flows, we show how the forward pass of the DEQcan be adapted to find fixed points of discrete measures underpermutation-invariance, and derive adequate network architectures for DDEQs. Inexperiments, we show that they can compete with state-of-the-art models intasks such as point cloud classification and point cloud completion, whilebeing significantly more parameter-efficient.</description>
      <author>example@mail.com (Jonathan Geuter, Clément Bonet, Anna Korba, David Alvarez-Melis)</author>
      <guid isPermaLink="false">2503.01140v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DualDiff+: Dual-Branch Diffusion for High-Fidelity Video Generation with Reward Guidance</title>
      <link>http://arxiv.org/abs/2503.03689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;DualDiff是一种双分支条件扩散模型，用于改进多视角和视频序列的驾驶场景生成。&lt;h4&gt;背景信息&lt;/h4&gt;当前方法主要依赖于3D边界框和鸟瞰图道路地图来控制前景和背景，这些方法无法捕捉驾驶场景的全部复杂性，并且不足以充分整合多模态信息。&lt;h4&gt;研究目的&lt;/h4&gt;提出DualDiff模型以解决现有技术在复杂性和多模态信息集成上的不足。&lt;h4&gt;主要方法&lt;/h4&gt;{'ORS': 'Occupancy Ray-shape Sampling，提供丰富的前景和背景语义以及3D空间几何结构', 'FGM': 'Foreground-Aware Mask，增强精细的前景物体合成', 'SFA': 'Semantic Fusion Attention机制，优先处理相关信息并抑制噪声', 'RGD': 'Reward-Guided Diffusion框架，确保生成视频的一致性和语义连贯性'}&lt;h4&gt;实验结果&lt;/h4&gt;{'性能提升': '在多个数据集上实现了最先进的（SOTA）表现。', 'NuScenes数据集': '相比最佳基线，FID得分减少了4.09%。', '下游任务改进': {'BEV分割': '车辆mIoU提高4.50%，道路mIoU提高1.70%', 'BEV 3D物体检测': '前景mAP提升1.46%'}}&lt;h4&gt;结论&lt;/h4&gt;DualDiff在多个评价指标上超越现有方法，证明了其在驾驶场景重建中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;准确且高保真的驾驶场景重建需要有效利用全面的场景信息作为条件输入。现有的方法主要依赖于3D边界框和鸟瞰图道路地图来控制前景和背景，这无法捕捉到驾驶场景的全部复杂性，并不足以充分整合多模态信息。在这项工作中，我们提出了DualDiff，这是一种双分支条件扩散模型，旨在增强多视角和视频序列中的驾驶场景生成能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and high-fidelity driving scene reconstruction demands the effectiveutilization of comprehensive scene information as conditional inputs. Existingmethods predominantly rely on 3D bounding boxes and BEV road maps forforeground and background control, which fail to capture the full complexity ofdriving scenes and adequately integrate multimodal information. In this work,we present DualDiff, a dual-branch conditional diffusion model designed toenhance driving scene generation across multiple views and video sequences.Specifically, we introduce Occupancy Ray-shape Sampling (ORS) as a conditionalinput, offering rich foreground and background semantics alongside 3D spatialgeometry to precisely control the generation of both elements. To improve thesynthesis of fine-grained foreground objects, particularly complex and distantones, we propose a Foreground-Aware Mask (FGM) denoising loss function.Additionally, we develop the Semantic Fusion Attention (SFA) mechanism todynamically prioritize relevant information and suppress noise, enabling moreeffective multimodal fusion. Finally, to ensure high-quality image-to-videogeneration, we introduce the Reward-Guided Diffusion (RGD) framework, whichmaintains global consistency and semantic coherence in generated videos.Extensive experiments demonstrate that DualDiff achieves state-of-the-art(SOTA) performance across multiple datasets. On the NuScenes dataset, DualDiffreduces the FID score by 4.09% compared to the best baseline. In downstreamtasks, such as BEV segmentation, our method improves vehicle mIoU by 4.50% androad mIoU by 1.70%, while in BEV 3D object detection, the foreground mAPincreases by 1.46%. Code will be made available athttps://github.com/yangzhaojason/DualDiff.</description>
      <author>example@mail.com (Zhao Yang, Zezhong Qian, Xiaofan Li, Weixiang Xu, Gongpeng Zhao, Ruohong Yu, Lingsi Zhu, Longjun Liu)</author>
      <guid isPermaLink="false">2503.03689v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Opportunistic Routing in Wireless Communications via Learnable State-Augmented Policies</title>
      <link>http://arxiv.org/abs/2503.03736v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了大规模无线通信网络中基于数据包的信息路由挑战，通过约束统计学习任务来解决此问题。&lt;h4&gt;背景&lt;/h4&gt;在大型无线网络中实现有效的信息传递是一个关键挑战。传统的路由策略依赖于特定的路径选择算法，但这种方法在动态网络环境中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的分布式优化方法（State-Augmentation, SA）以提高源节点的信息处理能力，并通过图神经网络（GNNs）来提取最佳的路由政策。&lt;h4&gt;方法&lt;/h4&gt;利用图卷积操作和基于网络节点之间拓扑连接关系的无监督学习框架，设计了一种新颖的方法。该方法能够根据实时信息动态选择最优中继节点。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与传统的基线算法相比，所提出的GNN参数化模型在处理多流数据时表现出色，尤其是在大型网络环境中具有更高的效率和稳定性。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了利用图神经网络进行无线通信路由优化的潜力，突出了其鲁棒性和可转移性。该方法不仅能够有效应对大规模无线网络中的动态挑战，而且可以应用于各种实际场景中。&lt;h4&gt;翻译&lt;/h4&gt;此论文解决的是在大型无线通信网絡中基于数据包的信息传递问题。它被描述为一个受限统计学习任务，在这个过程中每个节点仅使用本地信息进行操作。机会性路由利用了无线电通信的广播特性来动态选择最佳转发节点，从而使信息能够通过多个中继节点同时到达目的地。为了应对这一挑战，我们提出了一种基于状态增强（State-Augmentation, SA）的分布式优化方法，旨在最大化网络源节点的信息处理能力。该问题建模使用图神经网络（GNN），执行基于网络节点之间拓扑连接关系的图卷积操作。通过无监督学习框架从GNN架构中提取路由策略，使源节点能够为各种流做出最优决策。数值实验表明，在训练参数化的GNN模型时，所提出的方法表现出色，并且比基线算法具有更佳的表现。此外，将该方法应用于现实网络拓扑结构和无线自组织网路测试平台验证了其有效性，突出了图神经网络的稳健性和可转移性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the challenge of packet-based information routing inlarge-scale wireless communication networks. The problem is framed as aconstrained statistical learning task, where each network node operates usingonly local information. Opportunistic routing exploits the broadcast nature ofwireless communication to dynamically select optimal forwarding nodes, enablingthe information to reach the destination through multiple relay nodessimultaneously. To solve this, we propose a State-Augmentation (SA) baseddistributed optimization approach aimed at maximizing the total informationhandled by the source nodes in the network. The problem formulation leveragesGraph Neural Networks (GNNs), which perform graph convolutions based on thetopological connections between network nodes. Using an unsupervised learningparadigm, we extract routing policies from the GNN architecture, enablingoptimal decisions for source nodes across various flows. Numerical experimentsdemonstrate that the proposed method achieves superior performance whentraining a GNN-parameterized model, particularly when compared to baselinealgorithms. Additionally, applying the method to real-world network topologiesand wireless ad-hoc network test beds validates its effectiveness, highlightingthe robustness and transferability of GNNs.</description>
      <author>example@mail.com (Sourajit Das, Navid NaderiAlizadeh, Rahul Mangharam, Alejandro Ribeiro)</author>
      <guid isPermaLink="false">2503.03736v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HyperGCT: A Dynamic Hyper-GNN-Learned Geometric Constraint for 3D Registration</title>
      <link>http://arxiv.org/abs/2503.02195v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的方法HyperGCT，用于在3D点云配准问题中动态优化超图来生成几何约束。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常通过构造一致性图来建模无序的特征匹配，并从中采样一致性的匹配以生成假设。然而，在构建这些图时引入了噪声，这给手工设计的几何约束带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从动态超图中挖掘稳健的几何约束的方法，从而改进3D配准过程。&lt;h4&gt;方法&lt;/h4&gt;HyperGCT通过顶点和边特征聚合的方式动态优化超图，利用高阶一致性来捕捉对应关系之间的相关性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与现有技术相比，HyperGCT在多个数据集上表现出了最先进的性能，并且对图形噪声具有鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这种方法不仅提高了3D配准的准确性，还增强了其泛化能力，证明了从动态超图中挖掘几何约束的有效性和重要性。&lt;h4&gt;翻译&lt;/h4&gt;几何约束对于解决基于特征匹配的3D点云注册问题至关重要。现有的方法通常将无序的匹配建模为一致性图，并从中采样一致性的匹配以生成假设。然而，显式的图形构建引入了噪声，这给手工设计的几何约束带来了挑战，使其难以确保匹配之间的协调性。为了克服这一限制，我们提出了一种新的方法HyperGCT，它利用动态超图中3D对应关系间的高阶一致性来学习灵活且动态的几何约束。据我们所知，这是第一个从动态超图中挖掘稳健几何约束的方法以用于三维注册任务。通过动态优化超图并聚合顶点和边特征，HyperGCT有效地捕获了对应之间的相关性，并生成准确的假设。在3DMatch、3DLoMatch、KITTI-LC以及ETH数据集上的广泛实验表明HyperGCT达到了最先进的性能。此外，我们的方法对图形噪声具有鲁棒性，在推广方面显示出显著优势。代码将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Geometric constraints between feature matches are critical in 3D point cloudregistration problems. Existing approaches typically model unordered matches asa consistency graph and sample consistent matches to generate hypotheses.However, explicit graph construction introduces noise, posing great challengesfor handcrafted geometric constraints to render consistency among matches. Toovercome this, we propose HyperGCT, a flexible dynamic Hyper-GNN-learnedgeometric constraint that leverages high-order consistency among 3Dcorrespondences. To our knowledge, HyperGCT is the first method that minesrobust geometric constraints from dynamic hypergraphs for 3D registration. Bydynamically optimizing the hypergraph through vertex and edge featureaggregation, HyperGCT effectively captures the correlations amongcorrespondences, leading to accurate hypothesis generation. Extensiveexperiments on 3DMatch, 3DLoMatch, KITTI-LC, and ETH show that HyperGCTachieves state-of-the-art performance. Furthermore, our method is robust tograph noise, demonstrating a significant advantage in terms of generalization.The code will be released.</description>
      <author>example@mail.com (Xiyu Zhang, Jiayi Ma, Jianwei Guo, Wei Hu, Zhaoshuai Qi, Fei Hui, Jiaqi Yang, Yanning Zhang)</author>
      <guid isPermaLink="false">2503.02195v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Visual Discrimination and Reasoning of Real-World Physical Dynamics: Physics-Grounded Anomaly Detection</title>
      <link>http://arxiv.org/abs/2503.03562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为Physics Anomaly Detection (Phys-AD)的新型大规模数据集，用于工业异常检测。该数据集基于真实的机器人臂和电机操作收集，包含多种动态且语义丰富的场景，以及不同类型的物理异常。&lt;h4&gt;背景&lt;/h4&gt;现有的工业异常检测算法主要在静态、语义简单的数据集上开发和测试，与现实世界中需要物理理解和推理的情况存在差距。&lt;h4&gt;目的&lt;/h4&gt;通过引入Phys-AD数据集来填补现有技术与实际需求之间的鸿沟，推动机器自主地像人类一样基于条件物体的物理知识进行感知、交互和推理的能力发展。&lt;h4&gt;方法&lt;/h4&gt;该研究包括了超过6400个视频片段，覆盖22种真实世界对象类别，并展示了机器人臂与电机互动时产生的47种异常类型。此外，还提出了Physics Anomaly Explanation (PAEval)度量标准以评估视觉语言基础模型在检测和解释物理原因方面的能力。&lt;h4&gt;主要发现&lt;/h4&gt;现有的无监督、弱监督以及视频理解方法在处理基于物理学的异常上存在局限性。&lt;h4&gt;结论&lt;/h4&gt;通过公开提供的数据集和基准测试，为研究者提供了新的机会来改进工业环境中的物体异常检测技术，并且这些资源将有助于推动物理理解和视觉推理的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;人类通过感知、互动以及基于条件物体的物理知识进行推理来识别现实世界中的对象异常。工业异常检测（IAD）的长期目标是使机器能够自主地复制这种技能。然而，当前大多数IAD算法是在静态且语义简单的数据集上开发和测试的，这与需要物理理解和推理的真实场景相去甚远。为了解决这一问题，我们引入了Physics Anomaly Detection (Phys-AD) 数据集，这是首个大规模、基于真实世界的工业异常检测视频数据集，并采用物理学基础设计。使用真实的机器人臂和电机收集的数据包含了6400多个动态且语义丰富的场景及22种对象类别与机器人手臂和电机互动的视频片段，并展示了47种物理异常类型。在Phys-AD中，进行异常检测需要视觉推理，结合物理知识和视频内容来确定物体是否处于异常状态。我们对最先进的异常检测方法进行了基准测试，包括无监督、弱监督以及基于理解视频的方法，在处理物理学基础引发的异常方面显示出了局限性。此外，还引入了Physics Anomaly Explanation (PAEval)度量标准以评估视觉语言模型在提供准确物理原因解释的能力上。我们的数据集和基准将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans detect real-world object anomalies by perceiving, interacting, andreasoning based on object-conditioned physical knowledge. The long-term goal ofIndustrial Anomaly Detection (IAD) is to enable machines to autonomouslyreplicate this skill. However, current IAD algorithms are largely developed andtested on static, semantically simple datasets, which diverge from real-worldscenarios where physical understanding and reasoning are essential.To bridgethis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, thefirst large-scale, real-world, physics-grounded video dataset for industrialanomaly detection. Collected using a real robot arm and motor, Phys-AD providesa diverse set of dynamic, semantically rich scenarios. The dataset includesmore than 6400 videos across 22 real-world object categories, interacting withrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection inPhys-AD requires visual reasoning, combining both physical knowledge and videocontent to determine object abnormality.We benchmark state-of-the-art anomalydetection methods under three settings: unsupervised AD, weakly-supervised AD,and video-understanding AD, highlighting their limitations in handlingphysics-grounded anomalies. Additionally, we introduce the Physics AnomalyExplanation (PAEval) metric, designed to assess the ability of visual-languagefoundation models to not only detect anomalies but also provide accurateexplanations for their underlying physical causes. Our dataset and benchmarkwill be publicly available.</description>
      <author>example@mail.com (Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu)</author>
      <guid isPermaLink="false">2503.03562v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PacketCLIP: Multi-Modal Embedding of Network Traffic and Language for Cybersecurity Reasoning</title>
      <link>http://arxiv.org/abs/2503.03747v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出PacketCLIP，一种结合包数据和自然语言语义的多模态框架，通过对比预训练和层次图神经网络（GNN）推理来增强加密流量分类和网络安全。&lt;h4&gt;背景&lt;/h4&gt;流量分类对网络安全至关重要，但加密流量带来了重大挑战。现有的解决方案难以同时满足准确性和解释性的需求，尤其是在资源受限环境中。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效且可解释地检测加密流量中异常的系统，以应对网络安全面临的挑战。&lt;h4&gt;方法&lt;/h4&gt;PacketCLIP结合了包数据和自然语言语义，并通过对比预训练以及层次图神经网络（GNN）推理来实现这一目标。该框架旨在将文本描述与包行为相匹配，提高模型的解释性、可扩展性和实用性。&lt;h4&gt;主要发现&lt;/h4&gt;PacketCLIP在加密流量分类中表现出色，实现了95%平均AUC评分，并且比基线方法提高了11.6%，同时减少了模型大小达92%，这对于实时异常检测尤为重要。&lt;h4&gt;结论&lt;/h4&gt;通过将高级机器学习技术与实际网络安全需求相结合，PacketCLIP为解决资源受限环境中的加密流量分类和网络入侵检测挑战提供了一个可扩展、高效且解释性强的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经直接以中文形式给出，无需进一步翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic classification is vital for cybersecurity, yet encrypted trafficposes significant challenges. We present PacketCLIP, a multi-modal frameworkcombining packet data with natural language semantics through contrastivepretraining and hierarchical Graph Neural Network (GNN) reasoning. PacketCLIPintegrates semantic reasoning with efficient classification, enabling robustdetection of anomalies in encrypted network flows. By aligning textualdescriptions with packet behaviors, it offers enhanced interpretability,scalability, and practical applicability across diverse security scenarios.PacketCLIP achieves a 95% mean AUC, outperforms baselines by 11.6%, and reducesmodel size by 92%, making it ideal for real-time anomaly detection. By bridgingadvanced machine learning techniques and practical cybersecurity needs,PacketCLIP provides a foundation for scalable, efficient, and interpretablesolutions to tackle encrypted traffic classification and network intrusiondetection challenges in resource-constrained environments.</description>
      <author>example@mail.com (Ryozo Masukawa, Sanggeon Yun, Sungheon Jeong, Wenjun Huang, Yang Ni, Ian Bryant, Nathaniel D. Bastian, Mohsen Imani)</author>
      <guid isPermaLink="false">2503.03747v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Simulation-Based Performance Evaluation of 3D Object Detection Methods with Deep Learning for a LiDAR Point Cloud Dataset in a SOTIF-related Use Case</title>
      <link>http://arxiv.org/abs/2503.03548v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个评估自动驾驶系统中三维物体检测性能的方法，专注于传感器表现限制和基于深度学习的物体检测不足对预期功能的影响。&lt;h4&gt;背景&lt;/h4&gt;Safety of the Intended Functionality (SOTIF)旨在解决传感器性能限制以及基于深度学习的对象检测方法在自动驾驶系统（ADS）中的局限性，以确保其预期的功能。&lt;h4&gt;目的&lt;/h4&gt;该论文的主要目的是定义和建模一个与SOTIF相关的使用案例，并生成用于应用3D对象检测方法的激光雷达点云数据集。&lt;h4&gt;方法&lt;/h4&gt;通过模拟21种不同的天气条件下的SOTIF相关用例，创建了一个包含547个帧的数据集，其中包括晴天、多云和雨天的各种情况，对应于一天中的不同时间。使用MMDetection3D和OpenPCDET工具包对最先进的（SOTA）3D对象检测方法的性能进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;通过在生成的数据集上测试预先训练好的深度学习模型，并使用平均精度（AP）和召回率指标进行比较，论文展示了不同天气条件下的性能差异。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了在各种环境条件下对3D物体检测算法进行全面评估的重要性。所提出的框架为开发更安全的自动驾驶系统提供了重要的视角和支持。&lt;h4&gt;翻译&lt;/h4&gt;Safety of the Intended Functionality (SOTIF)旨在解决传感器性能限制以及基于深度学习的对象检测方法在自动驾驶系统（ADS）中的局限性，以确保其预期的功能。该论文提出了一个评估三维物体检测适应性和性能的方法，通过对模拟的与SOTIF相关的用例生成的数据集进行3D物体检测算法的应用来实现这一目的。主要贡献包括定义和建模21种不同天气条件下的SOTIF相关使用案例，并为应用3D物体检测方法创建了适合于激光雷达点云数据集。该数据集包含547帧，涵盖晴天、多云和雨天的不同情况，对应一天中的各个时段。通过MMDetection3D和OpenPCDET工具包，在生成的数据集上使用平均精度（AP）和召回率指标对预先训练好的深度学习模型进行测试并评估最先进的3D物体检测方法的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.5220/0012707300003702&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety of the Intended Functionality (SOTIF) addresses sensor performancelimitations and deep learning-based object detection insufficiencies to ensurethe intended functionality of Automated Driving Systems (ADS). This paperpresents a methodology examining the adaptability and performance evaluation ofthe 3D object detection methods on a LiDAR point cloud dataset generated bysimulating a SOTIF-related Use Case. The major contributions of this paperinclude defining and modelling a SOTIF-related Use Case with 21 diverse weatherconditions and generating a LiDAR point cloud dataset suitable for applicationof 3D object detection methods. The dataset consists of 547 frames,encompassing clear, cloudy, rainy weather conditions, corresponding todifferent times of the day, including noon, sunset, and night. EmployingMMDetection3D and OpenPCDET toolkits, the performance of State-of-the-Art(SOTA) 3D object detection methods is evaluated and compared by testing thepre-trained Deep Learning (DL) models on the generated dataset using AveragePrecision (AP) and Recall metrics.</description>
      <author>example@mail.com (Milin Patel, Rolf Jung)</author>
      <guid isPermaLink="false">2503.03548v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Explainable LiDAR 3D Point Cloud Segmentation and Clustering for Detecting Airplane-Generated Wind Turbulence</title>
      <link>http://arxiv.org/abs/2503.00518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at KDD 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用激光雷达（LiDAR）数据检测航空涡流的先进、可解释机器学习方法。该方法结合了动态图卷积神经网络(DGCNN)和语义分割技术，对3D LiDAR点云进行有意义的分段，并通过聚类技术进一步优化。&lt;h4&gt;背景&lt;/h4&gt;飞机产生的强空气湍流（航空涡流）给航空安全带来了重大风险，需要准确可靠的检测方法。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用LiDAR数据检测航空涡流的有效且可靠的方法，提高航空安全性。&lt;h4&gt;方法&lt;/h4&gt;采用动态图卷积神经网络(DGCNN)和语义分割技术对3D LiDAR点云进行分段，并通过聚类技术优化结果。引入基于扰动的解释技术增强模型决策过程透明度。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在测量和模拟LiDAR扫描数据上的表现优于四种基准方法，证明了其有效性和可靠性。&lt;h4&gt;结论&lt;/h4&gt;结合语义分割和聚类技术用于实时航空涡流跟踪的方法显著提升了航空安全措施的有效性与可解释性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种利用激光雷达（LiDAR）数据检测航空涡流的先进、透明机器学习方法。该研究通过动态图卷积神经网络（DGCNN）和语义分割技术对3D LiDAR点云进行有意义的分段，并引入基于扰动的技术来解释模型决策过程，从而提高了安全性和信任度。实验结果表明了这种方法的有效性和可靠性，为实时航空涡流跟踪提供了一种先进的方法，同时提升了其透明度和可理解性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wake vortices - strong, coherent air turbulences created by aircraft - pose asignificant risk to aviation safety and therefore require accurate and reliabledetection methods. In this paper, we present an advanced, explainable machinelearning method that utilizes Light Detection and Ranging (LiDAR) data foreffective wake vortex detection. Our method leverages a dynamic graph CNN(DGCNN) with semantic segmentation to partition a 3D LiDAR point cloud intomeaningful segments. Further refinement is achieved through clusteringtechniques. A novel feature of our research is the use of a perturbation-basedexplanation technique, which clarifies the model's decision-making processesfor air traffic regulators and controllers, increasing transparency andbuilding trust. Our experimental results, based on measured and simulated LiDARscans compared against four baseline methods, underscore the effectiveness andreliability of our approach. This combination of semantic segmentation andclustering for real-time wake vortex tracking significantly advances aviationsafety measures, ensuring that these are both effective and comprehensible.</description>
      <author>example@mail.com (Zhan Qu, Shuzhou Yuan, Michael Färber, Marius Brennfleck, Niklas Wartha, Anton Stephan)</author>
      <guid isPermaLink="false">2503.00518v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>AugFL: Augmenting Federated Learning with Pretrained Models</title>
      <link>http://arxiv.org/abs/2503.02154v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  to be published in Transactions on Networking&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了如何通过利用预训练模型（PM）来增强联邦学习（FL），以解决分布式环境中由于隐私政策或存储限制而导致的训练数据稀缺问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，联邦学习因其能够在保护用户隐私的同时进行大规模机器学习而引起了广泛关注。然而，在实际应用中，受限于严格的隐私规定和设备有限的存储能力，缺乏足够的训练数据常常阻碍了其有效部署。&lt;h4&gt;目的&lt;/h4&gt;通过引入预训练模型来增强FL系统的能力，以降低从头开始执行联邦学习所需的数据量，并提高模型在分布式环境中的适应性和性能。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一个基于正则化的元学习框架，在这个框架中，客户端协作学习一个从服务器存储的私有预训练模型中迁移知识得到的元模型。此外，开发了一种基于不精确ADMM算法的优化方案（AugFL），以在不暴露预训练模型且不增加本地计算成本的情况下解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明了所提出方法的通信复杂性、适应性能以及一般非凸情况下的知识迁移收益。实验结果进一步证实了AugFL相较于现有基线的有效性和优越性。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种新颖的方法来增强联邦学习系统，特别是在数据稀缺的情况下通过利用预训练模型的知识转移能力显著提高了学习效率和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经完全翻译为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Federated Learning (FL) has garnered widespread interest in recent years.However, owing to strict privacy policies or limited storage capacities oftraining participants such as IoT devices, its effective deployment is oftenimpeded by the scarcity of training data in practical decentralized learningenvironments. In this paper, we study enhancing FL with the aid of (large)pre-trained models (PMs), that encapsulate wealthy general/domain-agnosticknowledge, to alleviate the data requirement in conducting FL from scratch.Specifically, we consider a networked FL system formed by a central server anddistributed clients. First, we formulate the PM-aided personalized FL as aregularization-based federated meta-learning problem, where clients join forcesto learn a meta-model with knowledge transferred from a private PM stored atthe server. Then, we develop an inexact-ADMM-based algorithm, AugFL, tooptimize the problem with no need to expose the PM or incur additionalcomputational costs to local clients. Further, we establish theoreticalguarantees for AugFL in terms of communication complexity, adaptationperformance, and the benefit of knowledge transfer in general non-convex cases.Extensive experiments corroborate the efficacy and superiority of AugFL overexisting baselines.</description>
      <author>example@mail.com (Sheng Yue, Zerui Qin, Yongheng Deng, Ju Ren, Yaoxue Zhang, Junshan Zhang)</author>
      <guid isPermaLink="false">2503.02154v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for Stance Detection</title>
      <link>http://arxiv.org/abs/2503.03172v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 2 figures, published in The Sixteenth International  Conference on Information (eKNOW 2024)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种用于社交媒体立场检测（SD）的新方法，利用讽刺识别作为中间任务的迁移学习来提高模型性能。&lt;h4&gt;背景&lt;/h4&gt;社交媒体上的立场检测由于其在社会商业和政治应用中的重要性而成为自然语言处理领域的一个研究热点。然而，在线平台文本的微妙性和复杂性对SD算法提出了挑战，特别是当包含讽刺或比喻的语言时。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在通过引入讽刺识别中间任务来改进现有的SD模型，以提高其准确率和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;该方法包括微调BERT和RoBERTa，并将卷积BiLSTM与密集层连接。此外，还进行了详尽的实验测试，以评估迁移学习框架的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明，集成讽刺识别知识到模型中有助于减少对讽刺文本元素的误分类，从而提高SD任务中的准确率和F1分数。特别是在85%的情况下，该模型能够正确预测之前没有进行讽刺预训练时被错误分类的文本。&lt;h4&gt;结论&lt;/h4&gt;这项研究是首次将讽刺检测作为中间迁移学习任务应用于SD的研究，并且通过结合BERT或RoBERTa与其他深度学习技术证明了其有效性。这为未来在这一领域的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已全部翻译成中文并进行了总结，涵盖了论文的主要贡献和发现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stance Detection (SD) on social media has emerged as a prominent area ofinterest with implications for social business and political applicationsthereby garnering escalating research attention within NLP. The inherentsubtlety and complexity of texts procured from online platforms pose challengesfor SD algorithms in accurately discerning the authors stance. Mostly theinclusion of sarcastic and figurative language drastically impacts theperformance of SD models. This paper addresses this by employing sarcasmdetection intermediate-task transfer learning tailored for SD. The proposedmethodology involves the finetuning of BERT and RoBERTa and the concatenationof convolutional BiLSTM and dense layers. Rigorous experiments are conducted onpublicly available datasets to evaluate our transfer-learning framework. Theperformance of the approach is assessed against various State-Of-The-Artbaselines for SD providing empirical evidence of its effectiveness. Notably ourmodel outperforms the best SOTA models even prior to sarcasm-detectionpretraining. The integration of sarcasm knowledge into the model provesinstrumental in mitigating misclassifications of sarcastic textual elements inSD. Our model accurately predicts 85% of texts that were previouslymisclassified by the model without sarcasm-detection pretraining therebyamplifying the average F1-score of the model. Our experiments also revealedthat the success of the transfer-learning framework is contingent upon thecorrelation of lexical attributes between the intermediate task and the targettask. This study represents the first exploration of sarcasm detection as anintermediate transfer-learning task in the context of SD and simultaneouslyuses the concatenation of BERT or RoBERTa with other deep-learning techniquesestablishing the proposed approach as a foundational baseline for futureresearch endeavors in this domain.</description>
      <author>example@mail.com (Gibson Nkhata, Susan Gauch)</author>
      <guid isPermaLink="false">2503.03172v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>YARE-GAN: Yet Another Resting State EEG-GAN</title>
      <link>http://arxiv.org/abs/2503.02636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '研究提出了一种基于Wasserstein GAN（WGANGP）的方法，用于生成多通道静息态EEG数据，并通过视觉和特征基评估来衡量合成信号的质量。', '背景': '尽管生成对抗网络(GANs)在合成逼真的神经数据方面显示出潜力，但它们在无监督表示学习中的应用特别是在休息状态的脑电图(EEG)领域尚未得到充分探索。', '目的': '实施并评估一种Wasserstein GAN（WGANGP）方法来生成多通道静息态EEG数据，并研究其作为无监督特征提取器的能力。', '方法': '使用WGAN-GP模型生成多通道休息状态的EEG信号，通过视觉和基于特征的方法评价合成信号的质量。同时，训练模型以评估其对年龄分类任务中的表示学习能力。', '主要发现': '模型成功捕获了真实EEG数据的统计学和谱特性；然而，在前额区域复制高频振荡方面存在挑战；此外，生成器的判别器可以用于年龄组分类，表现出超出随机标签基线的准确率。', '结论': '研究结果表明，生成式模型不仅能够作为高质量EEG数据的合成工具，还能作为一种无监督特征提取的方法，减少手动特征工程的需求。这为基于GAN的无监督学习在EEG分析中的应用提供了新的可能性，并促进了更高效的数据驱动神经科学研究方法的发展。', '翻译': '摘要是关于利用Wasserstein GAN（WGANGP）生成真实多通道静息状态EEG数据的研究，该研究通过视觉和特征基评估来衡量合成信号的质量。它揭示了GAN在无监督表示学习中的潜力，并表明它们不仅可以生成高质量的EEG数据，还可以作为一种有效的无监督特征提取方法用于年龄组分类等任务。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generative Adversarial Networks (GANs) have shown promise in synthesisingrealistic neural data, yet their potential for unsupervised representationlearning in resting-state EEG remains under explored. In this study, weimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generatemulti-channel resting-state EEG data and assess the quality of the synthesisedsignals through both visual and feature-based evaluations. Our results indicatethat the model effectively captures the statistical and spectralcharacteristics of real EEG data, although challenges remain in replicatinghigh-frequency oscillations in the frontal region. Additionally, we demonstratethat the Critic's learned representations can be fine-tuned for age groupclassification, achieving an out-of-sample accuracy, significantly better thana shuffled-label baseline. These findings suggest that generative models canserve not only as EEG data generators but also as unsupervised featureextractors, reducing the need for manual feature engineering. This studyhighlights the potential of GAN-based unsupervised learning for EEG analysis,suggesting avenues for more data-efficient deep learning applications inneuroscience.</description>
      <author>example@mail.com (Yeganeh Farahzadi, Morteza Ansarinia, Zoltan Kekecs)</author>
      <guid isPermaLink="false">2503.02636v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>TEDDY: A Family Of Foundation Models For Understanding Single Cell Biology</title>
      <link>http://arxiv.org/abs/2503.03485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过扩大预训练数据集和利用大规模生物注释来改进单细胞基础模型，以提高疾病生物学中的下游应用性能。&lt;h4&gt;背景&lt;/h4&gt;理解疾病的生物学机制对医学特别是药物发现至关重要。AI驱动的基因组规模生物数据分析在这一领域具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;探索改进当前最先进的方法，通过扩大预训练数据集和利用大规模生物注释来改善单细胞基础模型的表现。&lt;h4&gt;方法&lt;/h4&gt;第一，将预训练的数据集扩展到1亿1600万个细胞；第二，利用大规模的生物注释作为监督信息进行预训练。培训TEDDY家族的六个基于变压器的最先进的单细胞基础模型，参数分别为70百万、160百万和400百万。&lt;h4&gt;主要发现&lt;/h4&gt;随着数据量和参数数量的增长，性能可预测地提高；在第一个任务上显示出显著改进，在第二个任务上的改进较为温和。&lt;h4&gt;结论&lt;/h4&gt;扩大预训练的数据集以及利用大规模生物注释可以有效地提升单细胞基础模型的性能，为疾病生物学提供了更好的工具。&lt;h4&gt;翻译&lt;/h4&gt;理解疾病的生物学机制对医学特别是药物发现至关重要。AI驱动的基因组规模生物数据分析在这一领域具有巨大潜力。随着单细胞RNA测序数据的日益增多，大型基础模型的发展成为可能。然而，现有的基础模型要么没有改进下游应用性能，要么只是适度地改善了它们。本研究探索了两种提高当前最佳实践的方法：扩大预训练的数据集到1亿1600万个细胞，并利用大规模生物注释作为监督信息进行预训练。培训TEDDY家族的六个基于变压器的最先进的单细胞基础模型，参数分别为70百万、160百万和400百万。在两个下游评估任务上验证了这些模型：识别未见受试者背后的疾病状态以及区分健康细胞与患病细胞（这两种情况均未出现在训练中）。研究表明随着数据量和参数数量的增长，性能可预测地提高；在第一个任务上显示出显著改进，在第二个任务上的改进较为温和。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the biological mechanism of disease is critical for medicine,and in particular drug discovery. AI-powered analysis of genome-scalebiological data hold great potential in this regard. The increasingavailability of single-cell RNA sequencing data has enabled the development oflarge foundation models for disease biology. However, existing foundationmodels either do not improve or only modestly improve over task-specific modelsin downstream applications. Here, we explored two avenues for improving thestate-of-the-art. First, we scaled the pre-training dataset to 116 millioncells, which is larger than those used by previous models. Second, we leveragedthe availability of large-scale biological annotations as a form of supervisionduring pre-training. We trained the TEDDY family of models comprising sixtransformer-based state-of-the-art single-cell foundation models with 70million, 160 million, and 400 million parameters. We vetted our models on twodownstream evaluation tasks -- identifying the underlying disease state ofheld-out donors not seen during training and distinguishing healthy cells fromdiseased ones for disease conditions and donors not seen during training.Scaling experiments showed that performance improved predictably with both datavolume and parameter count. Our models showed substantial improvement overexisting work on the first task and more muted improvements on the second.</description>
      <author>example@mail.com (Alexis Chevalier, Soumya Ghosh, Urvi Awasthi, James Watkins, Julia Bieniewska, Nichita Mitrea, Olga Kotova, Kirill Shkura, Andrew Noble, Michael Steinbaugh, Julien Delile, Christoph Meier, Leonid Zhukov, Iya Khalil, Srayanta Mukherjee, Judith Mueller)</author>
      <guid isPermaLink="false">2503.03485v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BEVMOSNet: Multimodal Fusion for BEV Moving Object Segmentation</title>
      <link>http://arxiv.org/abs/2503.03280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Proceedings of the 20th International Joint Conference on Computer  Vision, Imaging and Computer Graphics Theory and Applications (2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对自动驾驶车辆中鸟类视角下的动态物体运动理解的挑战，论文提出了BEVMOSNet系统，该系统结合了相机、激光雷达和雷达数据进行多模态融合，并在nuScenes数据集上取得了显著性能提升。&lt;h4&gt;背景&lt;/h4&gt;当前关于场景内动态对象在鸟类视角下（BEV）的准确运动理解的研究相对较少。尽管有一些基于视觉的方法提出了初步结果，但在低光、夜间以及雨等恶劣天气条件下这些方法的表现严重下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效融合相机、激光雷达和雷达数据的端到端多模态系统，用于精准预测鸟类视角下移动物体的位置与运动状态。&lt;h4&gt;方法&lt;/h4&gt;引入了BEVMOSNet框架，这是首个利用相机、激光雷达以及雷达数据进行端到端多模态融合的方法。研究重点在于探索如何通过可变形交叉注意力机制指导传感器之间的信息共享优化策略。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes数据集上，与仅基于视觉的单模式基线BEV-MoSeg相比，BEVMOSNet实现了36.59%的整体IoU得分提升；与多模态SimpleBEV相比，提升了2.35%，确立了其在BEV运动分割领域的前沿地位。&lt;h4&gt;结论&lt;/h4&gt;通过综合利用不同类型的传感器数据，可以显著提高动态物体识别和跟踪的准确性，在复杂环境下的性能尤为突出。这为未来自动驾驶车辆中更可靠的安全保障和路径规划技术开发提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;对摘要内容进行了中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate motion understanding of the dynamic objects within the scene inbird's-eye-view (BEV) is critical to ensure a reliable obstacle avoidancesystem and smooth path planning for autonomous vehicles. However, this task hasreceived relatively limited exploration when compared to object detection andsegmentation with only a few recent vision-based approaches presentingpreliminary findings that significantly deteriorate in low-light, nighttime,and adverse weather conditions such as rain. Conversely, LiDAR and radarsensors remain almost unaffected in these scenarios, and radar provides keyvelocity information of the objects. Therefore, we introduce BEVMOSNet, to ourknowledge, the first end-to-end multimodal fusion leveraging cameras, LiDAR,and radar to precisely predict the moving objects in BEV. In addition, weperform a deeper analysis to find out the optimal strategy for deformablecross-attention-guided sensor fusion for cross-sensor knowledge sharing in BEV.While evaluating BEVMOSNet on the nuScenes dataset, we show an overallimprovement in IoU score of 36.59% compared to the vision-based unimodalbaseline BEV-MoSeg (Sigatapu et al., 2023), and 2.35% compared to themultimodel SimpleBEV (Harley et al., 2022), extended for the motionsegmentation task, establishing this method as the state-of-the-art in BEVmotion segmentation.</description>
      <author>example@mail.com (Hiep Truong Cong, Ajay Kumar Sigatapu, Arindam Das, Yashwanth Sharma, Venkatesh Satagopan, Ganesh Sistu, Ciaran Eising)</author>
      <guid isPermaLink="false">2503.03280v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Semantic-ICP: Iterative Closest Point for Non-rigid Multi-Organ Point Cloud Registration</title>
      <link>http://arxiv.org/abs/2503.00972v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, submitted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;点云配准在计算机辅助干预中非常重要。尽管已经开发了基于学习的点云配准方法，但由于泛化性和可解释性的问题，这些方法难以应用于临床。因此，传统的点云配准方法（如迭代最近点算法ICP）仍然广泛用于CAI。然而，ICP方法未能考虑以下两个方面：1. 点具有明确的语义含义，每个点可以与特定解剖标签相关联；2. 变形需要遵循生物力学能量约束。&lt;h4&gt;背景&lt;/h4&gt;学习基于的方法在泛化性和解释性上存在挑战，因此传统的迭代最近点算法ICP仍广泛应用于计算机辅助干预中。然而，现有方法忽视了点的语义信息和变形的能量约束要求。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的语义ICP（sem-ICP）方法，该方法处理多点标签并使用线性弹性能量正则化来提高配准效果。&lt;h4&gt;方法&lt;/h4&gt;利用语义标签改进最近邻匹配的鲁棒性，并引入了一种新型点云变形表示形式以应用明确的生物力学能量正则化。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在Learn2reg腹部MR-CT注册数据集和一种经口机器人手术超声波-CT注册数据集中，与现有的ICP基线方法相比，提高了Hausdorff距离。同时进行的灵敏度研究表明，刚性初始化能够更好地适应不同的初始位置和可见比。&lt;h4&gt;结论&lt;/h4&gt;通过结合点云中的语义信息以及生物力学能量约束，提出的sem-ICP算法改进了现有ICP算法在配准任务上的性能。&lt;h4&gt;翻译&lt;/h4&gt;点云配准是计算机辅助干预（CAI）中的一项重要技术。尽管基于学习的方法已经开发出来，但由于泛化性和解释性的问题，这些方法难以应用于临床实践中。因此，在CAI领域内，传统的迭代最近点（ICP）算法仍然被广泛使用。然而，现有的ICP算法未能充分考虑以下两点：1. 每个点具有明确的语义含义，并且可以与特定解剖学标签相关联；2. 变形需要遵循生物力学能量约束。本文提出了一种新的方法——语义迭代最近点（sem-ICP），该方法能够处理多点标签，同时采用线性弹性能量正则化来增强配准效果。通过使用语义信息改进了最近邻匹配的鲁棒性，并引入了一种全新的点云变形表示形式以便应用明确的生物力学能量正则化。我们的实验在Learn2reg腹部MR-CT注册数据集和一种经口机器人手术超声波-CT注册数据集中进行了测试，结果显示相较于现有的ICP基线方法，改进后的Hausdorff距离表现更优。此外，我们还进行了一项敏感性研究，证明了刚性初始化在不同初始位置和可见比的情况下具有更好的收敛性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud registration is important in computer-aided interventions (CAI).While learning-based point cloud registration methods have been developed,their clinical application is hampered by issues of generalizability andexplainability. Therefore, classical point cloud registration methods, such asIterative Closest Point (ICP), are still widely applied in CAI. ICP methodsfail to consider that: (1) the points have well-defined semantic meaning, inthat each point can be related to a specific anatomical label; (2) thedeformation needs to follow biomechanical energy constraints. In this paper, wepresent a novel semantic ICP (sem-ICP) method that handles multiple pointlabels and uses linear elastic energy regularization. We use semantic labels toimprove the robustness of the closest point matching and propose a new pointcloud deformation representation to apply explicit biomechanical energyregularization. Our experiments on the Learn2reg abdominal MR-CT registrationdataset and a trans-oral robotic surgery ultrasound-CT registration datasetshow that our method improves the Hausdorff distance compared with otherstate-of-the-art ICP-based registration methods. We also perform a sensitivitystudy to show that our rigid initialization achieves better convergence withdifferent initializations and visible ratios.</description>
      <author>example@mail.com (Wanwen Chen, Carson Studders, Jamie J. Y. Kwon, Emily H. T. Pang, Eitan Prisman, Septimiu E. Salcudean)</author>
      <guid isPermaLink="false">2503.00972v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLM as GNN: Graph Vocabulary Learning for Text-Attributed Graph Foundation Models</title>
      <link>http://arxiv.org/abs/2503.03313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对带有文本描述的节点（Text-Attributed Graphs，TAGs）在实际场景中的普遍存在及其独特的结构和领域特定知识的需求，提出了一个通用图基础模型（Graph Foundation Model, GFM），旨在跨多种图类型和任务中进行泛化。&lt;h4&gt;背景&lt;/h4&gt;现有的方法倾向于将大语言模型（Large Language Models，LLMs）与图神经网络（Graph Neural Networks，GNNs）分阶段地结合在一起处理TAGs，这种解耦架构限制了它们之间的协同潜力。此外，现有方法在处理图节点时使用了不兼容任务导向模板的词表外词汇分配策略。&lt;h4&gt;目的&lt;/h4&gt;为解决上述问题，提出了PromptGFM模型，该模型旨在通过图词汇学习实现通用图基础模型的设计。&lt;h4&gt;方法&lt;/h4&gt;PromptGFM包含两个主要组件：(1) 图理解模块，此模块使LLMs能够复制GNN的工作流程，并促进无缝的GNN-LLM融合和优雅的图文本对齐；(2) 图推理模块，该模块建立了一种基于语言的图词汇，确保表达性、可移植性和可扩展性。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了PromptGFM在多种图类型和任务上的优越性和迁移能力。&lt;h4&gt;结论&lt;/h4&gt;PromptGFM提供了处理带有文本描述节点图形的有效方法，并证明其具有良好的泛化能力和跨领域适应性。&lt;h4&gt;翻译&lt;/h4&gt;Text-Attributed Graphs (TAGs)，即每个节点都关联有文本描述的图，在现实场景中普遍存在。它们通常展现出独特的结构和特定领域的知识，促使开发一种能够跨越不同类型的图和任务的一般图基础模型（Graph Foundation Model, GFM）。尽管在整合大型语言模型（Large Language Models, LLMs）与图神经网络（GNNs）处理TAG方面付出了巨大努力，现有的方法由于采用了两阶段对齐的解耦架构而受到限制。更糟糕的是，现有方法将词汇外词分配给图节点，导致了特定于图的语义、标记爆炸以及与任务导向提示模板不兼容的问题，这阻碍了跨图和跨任务迁移能力。为解决这些挑战，我们提出了一种基于图词汇学习的基础通用图模型PromptGFM。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Text-Attributed Graphs (TAGs), where each node is associated with textdescriptions, are ubiquitous in real-world scenarios. They typically exhibitdistinctive structure and domain-specific knowledge, motivating the developmentof a Graph Foundation Model (GFM) that generalizes across diverse graphs andtasks. Despite large efforts to integrate Large Language Models (LLMs) andGraph Neural Networks (GNNs) for TAGs, existing approaches suffer fromdecoupled architectures with two-stage alignment, limiting their synergisticpotential. Even worse, existing methods assign out-of-vocabulary (OOV) tokensto graph nodes, leading to graph-specific semantics, token explosion, andincompatibility with task-oriented prompt templates, which hinders cross-graphand cross-task transferability. To address these challenges, we proposePromptGFM, a versatile GFM for TAGs grounded in graph vocabulary learning.PromptGFM comprises two key components: (1) Graph Understanding Module, whichexplicitly prompts LLMs to replicate the finest GNN workflow within the textspace, facilitating seamless GNN-LLM integration and elegant graph-textalignment; (2) Graph Inference Module, which establishes a language-based graphvocabulary ensuring expressiveness, transferability, and scalability, enablingreadable instructions for LLM fine-tuning. Extensive experiments demonstrateour superiority and transferability across diverse graphs and tasks. The codeis available at this: https://github.com/agiresearch/PromptGFM.</description>
      <author>example@mail.com (Xi Zhu, Haochen Xue, Ziwei Zhao, Wujiang Xu, Jingyuan Huang, Minghao Guo, Qifan Wang, Kaixiong Zhou, Yongfeng Zhang)</author>
      <guid isPermaLink="false">2503.03313v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D Object Detection with Radar Point Clouds?</title>
      <link>http://arxiv.org/abs/2503.02687v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ  International Conference on Intelligent Robots and Systems (IROS 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了将现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并提出了针对雷达点云的新方法CAPMix。&lt;h4&gt;背景&lt;/h4&gt;由于3D感知任务中数据收集和标注的工作量巨大，研究人员开发了许多利用现有数据生成多样化训练样本的方法。然而，这些方法大多数都是针对激光雷达数据设计的，对雷达点云的应用研究较少。&lt;h4&gt;目的&lt;/h4&gt;探讨将现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并识别其中存在的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了Class-Aware PillarMix (CAPMix) 方法，该方法在3D点云中的柱状体级别应用了基于类别标签指导的混合操作。此方法根据每个柱状体独立分配一个比例，从而增加样本多样性。为了考虑不同类别的密度，使用特定于类别的分布：对于稠密对象（如大型车辆），倾向于从另一样本中采样更多点；而对于稀疏对象（如行人），则更注重原始样本中的点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法不仅显著提升了性能，在两个数据集上的表现也优于现有的MSDA方法。&lt;h4&gt;结论&lt;/h4&gt;研究提出了一种针对雷达点云的新型混合样本数据增强方法，通过这种方式可以生成更多样化的训练数据，并认为此简洁而有效的方法将促进对雷达数据增强技术的研究进展。&lt;h4&gt;翻译&lt;/h4&gt;由于3D感知任务中的数据收集和标注工作量巨大，研究人员开发了许多利用现有数据生成多样化训练样本的技术。然而，这些方法大多针对激光雷达数据设计，对于雷达点云的应用研究较少。本文探讨了现有的混合样本数据增强（MSDA）技术应用于雷达点云的可行性，并提出了一种名为Class-Aware PillarMix (CAPMix) 的新方法来解决识别到的问题。该方法在3D点云中的柱状体级别应用基于类别标签指导的混合操作，通过独立分配比例提升样本多样性，并根据不同类别的密度使用特定于类别的分布策略进行调整。实验结果表明，这种方法不仅显著提升了性能，在两个数据集上的表现也优于现有的MSDA方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Due to the significant effort required for data collection and annotation in3D perception tasks, mixed sample data augmentation (MSDA) has been widelystudied to generate diverse training samples by mixing existing data. Recently,many MSDA techniques have been developed for point clouds, but they mainlytarget LiDAR data, leaving their application to radar point clouds largelyunexplored. In this paper, we examine the feasibility of applying existing MSDAmethods to radar point clouds and identify several challenges in adapting thesetechniques. These obstacles stem from the radar's irregular angulardistribution, deviations from a single-sensor polar layout in multi-radarsetups, and point sparsity. To address these issues, we propose Class-AwarePillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillarlevel in 3D point clouds, guided by class labels. Unlike methods that rely asingle mix ratio to the entire sample, CAPMix assigns an independent ratio toeach pillar, boosting sample diversity. To account for the density of differentclasses, we use class-specific distributions: for dense objects (e.g., largevehicles), we skew ratios to favor points from another sample, while for sparseobjects (e.g., pedestrians), we sample more points from the original. Thisclass-aware mixing retains critical details and enriches each sample with newinformation, ultimately generating more diverse training data. Experimentalresults demonstrate that our method not only significantly boosts performancebut also outperforms existing MSDA approaches across two datasets (Bosch Streetand K-Radar). We believe that this straightforward yet effective approach willspark further investigation into MSDA techniques for radar data.</description>
      <author>example@mail.com (Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Bin Yang)</author>
      <guid isPermaLink="false">2503.02687v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>External Reliable Information-enhanced Multimodal Contrastive Learning for Fake News Detection</title>
      <link>http://arxiv.org/abs/2503.03107v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by AAAI'25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着互联网的快速发展，信息传播范式发生了变化，并且效率大大提高。然而，这也带来了假新闻的快速传播并导致了网络空间中的负面影响。&lt;h4&gt;背景&lt;/h4&gt;当前，信息展示格式逐渐演进，新闻形式从文本向多模态内容转变。因此，检测多模态假新闻已成为研究热点之一。&lt;h4&gt;目的&lt;/h4&gt;针对现有的多模态假新闻检测领域存在的两个主要挑战：无法充分有效地利用多模态信息进行检测和引入的外部信息可信度低或静态性质限制了动态更新的问题，提出一种增强型可靠外部信息多模态对比学习框架（ERIC-FND）。&lt;h4&gt;方法&lt;/h4&gt;该模型通过实体丰富化的外部信息强化新闻内容表示，并采用多模态语义交互方法丰富多模态新闻信息。其中采用了多模态对比学习，使得不同模态的表示可以从彼此中学习。此外，还采取了自适应融合方法来整合来自不同维度的新闻表示以最终实现分类。&lt;h4&gt;主要发现&lt;/h4&gt;在两个常用的跨语言数据集（X和Weibo）上进行实验后，结果表明所提出的模型ERIC-FND在相同设置下优于现有的最先进的假新闻检测方法。&lt;h4&gt;结论&lt;/h4&gt;通过上述研究展示了ERIC-FND框架的有效性，并为未来多模态假新闻检测的研究提供了新的思路和方向。&lt;h4&gt;翻译&lt;/h4&gt;随着互联网的快速发展，信息传播的方式发生了变化并且效率得到了极大的提升。然而这也带来了快速传播的假新闻并导致了网络空间中的负面影响。目前，信息展示格式逐渐演进，新闻形式从文本转向多元化的多模态内容。因此检测多模态假新闻已经成为研究热点之一。但是，在现有的多模态假新闻检测领域仍然面临两个主要挑战：无法充分有效地利用多模态的信息进行检测和引入的外部信息可信度低或静态性质限制了动态更新的问题。为了填补这些空白，我们提出了ERIC-FND框架，这是一种增强型可靠外部信息多模态对比学习框架用于假新闻检测。该模型通过实体丰富化的外部信息强化新闻内容表示，并采用多模态语义交互方法丰富多模态新闻信息。其中采用了多模态对比学习使得不同模态的表示可以从彼此中学习。此外，还采取了自适应融合方法来整合来自不同维度的新闻表示以最终实现分类。在两个常用的跨语言数据集（X和Weibo）上进行实验后，结果表明所提出的模型ERIC-FND在相同设置下优于现有的最先进的假新闻检测方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of the Internet, the information disseminationparadigm has changed and the efficiency has been improved greatly. While thisalso brings the quick spread of fake news and leads to negative impacts oncyberspace. Currently, the information presentation formats have evolvedgradually, with the news formats shifting from texts to multimodal contents. Asa result, detecting multimodal fake news has become one of the researchhotspots. However, multimodal fake news detection research field still facestwo main challenges: the inability to fully and effectively utilize multimodalinformation for detection, and the low credibility or static nature of theintroduced external information, which limits dynamic updates. To bridge thegaps, we propose ERIC-FND, an external reliable information-enhanced multimodalcontrastive learning framework for fake news detection. ERIC-FND strengthensthe representation of news contents by entity-enriched external informationenhancement method. It also enriches the multimodal news information viamultimodal semantic interaction method where the multimodal constrativelearning is employed to make different modality representations learn from eachother. Moreover, an adaptive fusion method is taken to integrate the newsrepresentations from different dimensions for the eventual classification.Experiments are done on two commonly used datasets in different languages, X(Twitter) and Weibo. Experiment results demonstrate that our proposed modelERIC-FND outperforms existing state-of-the-art fake news detection methodsunder the same settings.</description>
      <author>example@mail.com (Biwei Cao, Qihang Wu, Jiuxin Cao, Bo Liu, Jie Gui)</author>
      <guid isPermaLink="false">2503.03107v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Category-level Meta-learned NeRF Priors for Efficient Object Mapping</title>
      <link>http://arxiv.org/abs/2503.01582v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了PRENOM，一种基于先验的高效神经对象映射器，它结合了类别级别的形状先验和对象级别的NeRF来提高重建效率并支持典型物体姿态估计。&lt;h4&gt;背景&lt;/h4&gt;在3D对象映射中，DeepSDF作为一种主要使用的类别级形状先验，虽然能够提供高效的对象重构和典型姿势估计，但是难以重现锐利的几何结构并且计算成本高。与此相反，NeRF捕捉精细细节但尚未有效集成到实时多对象映射框架中。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法PRENOM来连接DeepSDF与NeRF之间的差距，通过结合类别级先验和对象级NeRF来提高重建效率并支持典型物体姿态估计。&lt;h4&gt;方法&lt;/h4&gt;1. 利用元学习从开源形状数据集中生成的合成重构任务进行训练。2. 使用多目标遗传算法为每个类优化NeRF架构以平衡重建设质量和训练时间。3. 采用基于先验的概率光线采样将采样指向预期对象区域，加速收敛并提高受限资源下的重建质量。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明PRENOM能够在低端GPU上实现高质量的重构同时保持计算可行性。对比无先验的NeRF方法，在合成数据集上的Chamfer距离降低了21%，在形状先验的真实世界嘈杂数据集中，所有重建指标平均提高了13%。&lt;h4&gt;结论&lt;/h4&gt;通过结合类别级和对象级先验，PRENOM展示了在实时多物体映射框架中的有效性，并且在训练时间减少5倍的情况下仍能保持准确的姿势和大小估计精度。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种名为PRENOM的新方法，该方法利用基于先验的信息来提高神经网络处理3D对象映射效率的同时支持典型姿态估计。通过结合DeepSDF和NeRF的优点，并采用多目标遗传算法优化架构以及概率光线采样策略加速收敛，实现高质量的重建效果并且在计算资源受限的情况下表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In 3D object mapping, category-level priors enable efficient objectreconstruction and canonical pose estimation, requiring only a single prior persemantic category (e.g., chair, book, laptop). Recently, DeepSDF haspredominantly been used as a category-level shape prior, but it struggles toreconstruct sharp geometry and is computationally expensive. In contrast, NeRFscapture fine details but have yet to be effectively integrated withcategory-level priors in a real-time multi-object mapping framework. To bridgethis gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapperthat integrates category-level priors with object-level NeRFs to enhancereconstruction efficiency while enabling canonical object pose estimation.PRENOM gets to know objects on a first-name basis by meta-learning on syntheticreconstruction tasks generated from open-source shape datasets. To account forobject category variations, it employs a multi-objective genetic algorithm tooptimize the NeRF architecture for each category, balancing reconstructionquality and training time. Additionally, prior-based probabilistic ray samplingdirects sampling toward expected object regions, accelerating convergence andimproving reconstruction quality under constrained resources. Experimentalresults on a low-end GPU highlight the ability of PRENOM to achievehigh-quality reconstructions while maintaining computational feasibility.Specifically, comparisons with prior-free NeRF-based approaches on a syntheticdataset show a 21% lower Chamfer distance, demonstrating better reconstructionquality. Furthermore, evaluations against other approaches using shape priorson a noisy real-world dataset indicate a 13% improvement averaged across allreconstruction metrics, and comparable pose and size estimation accuracy, whilebeing trained for 5x less time.</description>
      <author>example@mail.com (Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez)</author>
      <guid isPermaLink="false">2503.01582v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Teaching AI to Handle Exceptions: Supervised Fine-Tuning with Human-Aligned Judgment</title>
      <link>http://arxiv.org/abs/2503.02976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型（LLM）正在从生成式AI发展为代理式AI，即在复杂现实场景中做出决策的系统。然而，尽管其生成能力已被充分研究，但它们的决策过程仍然不被人们深入理解。&lt;h4&gt;背景&lt;/h4&gt;当LLM处理例外情况时，这种问题尤为明显，因为合同内在的不完备性使这类挑战更加突出。&lt;h4&gt;目的&lt;/h4&gt;展示即使是最擅长推理的LLM也会因其严格遵循政策而与人类判断相去甚远。研究如何调整AI代理以更好地处理例外情况的方法。&lt;h4&gt;方法&lt;/h4&gt;评估了三种调整AI代理以处理例外情况的方法：伦理框架提示、链式思维推理和监督微调，其中尤其关注有解释性的人类反馈的监督微调。&lt;h4&gt;主要发现&lt;/h4&gt;伦理框架提示效果不佳；链式思维推理仅有轻微改善；而带有人类解释的监督微调显著提升了结果，并且能够使模型在新的场景中泛化出与人类相似的决策模式。这表明，为了将LLM与人类判断对齐，需要明确训练它们如何做出决策，而不仅仅是告诉它们应该做什么。&lt;h4&gt;结论&lt;/h4&gt;研究强调了解决LLMs处理例外情况不足的问题的重要性，以引导代理式AI的发展方向，使之能够更好地与人类判断相匹配，并适应新的环境。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型最初用于生成型人工智能领域，现在正在进化为代理型人工智能系统，在复杂的现实场景中进行决策。然而，尽管它们的生成能力得到了充分研究，但其决策过程仍然不为人所深入理解。这在LLM处理例外情况时尤为明显，因为合同内在的不完备性使得这类挑战更为突出。本研究表明，即使是最擅长推理的大型语言模型（LLMs）也会因其严格遵循规则而与人类判断相去甚远。我们评估了三种调整AI代理以更好地处理异常的方法：伦理框架提示、链式思维推理以及有监督的微调，并发现带有解释的人类反馈进行的监督微调方法效果最好，能够使模型在新的场景中泛化出类似人类决策模式。这项研究表明，要将LLMs与人类判断对齐，需要明确训练它们如何做出决定，而不仅仅是告诉它们应该做什么。这些发现强调了解决大型语言模型处理例外情况不足的重要性，以引导代理型人工智能的发展方向，使之能够更好地匹配人类判断，并适应新的环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs), initially developed for generative AI, are nowevolving into agentic AI systems, which make decisions in complex, real-worldcontexts. Unfortunately, while their generative capabilities arewell-documented, their decision-making processes remain poorly understood. Thisis particularly evident when models are handling exceptions, a critical andchallenging aspect of decision-making made relevant by the inherentincompleteness of contracts. Here we demonstrate that LLMs, even ones thatexcel at reasoning, deviate significantly from human judgments because theyadhere strictly to policies, even when such adherence is impractical,suboptimal, or even counterproductive. We then evaluate three approaches totuning AI agents to handle exceptions: ethical framework prompting,chain-of-thought reasoning, and supervised fine-tuning. We find that whileethical framework prompting fails and chain-of-thought prompting provides onlyslight improvements, supervised fine-tuning, specifically with humanexplanations, yields markedly better results. Surprisingly, in our experiments,supervised fine-tuning even enabled models to generalize human-likedecision-making to novel scenarios, demonstrating transfer learning ofhuman-aligned decision-making across contexts. Furthermore, fine-tuning withexplanations, not just labels, was critical for alignment, suggesting thataligning LLMs with human judgment requires explicit training on how decisionsare made, not just which decisions are made. These findings highlight the needto address LLMs' shortcomings in handling exceptions in order to guide thedevelopment of agentic AI toward models that can effectively align with humanjudgment and simultaneously adapt to novel contexts.</description>
      <author>example@mail.com (Matthew DosSantos DiSorbo, Harang Ju, Sinan Aral)</author>
      <guid isPermaLink="false">2503.02976v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Intrinsic and Extrinsic Factor Disentanglement for Recommendation in Various Context Scenarios</title>
      <link>http://arxiv.org/abs/2503.03524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 13 figures, 11 tables. Accepted by Transactions of  Information Systems&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个区分内在和外在因素的推荐模型IEDR，它考虑了多种上下文环境的影响，从而提高了推荐系统的准确性。&lt;h4&gt;背景&lt;/h4&gt;用户行为（如购买、点击）在不同情境下可能有显著差异。这种差异是由用户的内在偏好和外在激励共同决定的，而这些外在激励会随着时间和地点等因素的变化而变化。&lt;h4&gt;目的&lt;/h4&gt;通过区分内在因素和外在因素来改善推荐系统的准确性和学习用户行为。&lt;h4&gt;方法&lt;/h4&gt;提出了一种通用框架IEDR模型，该模型能够同时考虑多种上下文环境的影响，将内在因素和外在因素区分开来。此模型包含一个不受上下文影响的对比学习组件，用于捕捉内在因素，并有一个解纠缠组件，在各种情境相互作用的情况下提取外在因素。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明IEDR在多个真实世界的数据集上展示了其区分和学习不同因素的有效性，能够显著提升推荐准确度（高达4% NDCG）。&lt;h4&gt;结论&lt;/h4&gt;IEDR模型通过考虑多种上下文环境的影响，提高了内在因素与外在因素的区分准确性，从而改善了推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;在推荐系统中，用户的行为模式可能因时间、地点等不同情境而显著变化。这是由于用户行为是由反映持续偏好（内在因素）和外部激励（外在因素）决定的。这些外部激励会根据不同的上下文环境发生变化。区分内外部因素有助于更好地学习用户行为。然而，现有的研究仅考虑从单个预定义的情境中区分它们的影响（如时间或地点），忽略了用户外部因素可能会受到同时作用的不同上下文中相互影响的事实。本文提出了一种通用框架——内在-外在解缠推荐模型IEDR，该模型可以在多种情景下分别提取出内在和外在的因素，从而更准确地区分这些因素并提高推荐的准确性。IEDR模型包括一个不受上下文影响的对比学习组件来捕捉内在因素，以及一个解纠缠组件，在各种情境相互作用的情况下可以提取外部因素。两个组件共同工作以实现有效的因子学习。大量的实验证明了IEDR在真实数据集上区分和学习独立因素的有效性，并通过最高提升4%的NDCG值显著提高了推荐准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recommender systems, the patterns of user behaviors (e.g., purchase,click) may vary greatly in different contexts (e.g., time and location). Thisis because user behavior is jointly determined by two types of factors:intrinsic factors, which reflect consistent user preference, and extrinsicfactors, which reflect external incentives that may vary in different contexts.Differentiating between intrinsic and extrinsic factors helps learn userbehaviors better. However, existing studies have only considereddifferentiating them from a single, pre-defined context (e.g., time orlocation), ignoring the fact that a user's extrinsic factors may be influencedby the interplay of various contexts at the same time. In this paper, wepropose the Intrinsic-Extrinsic Disentangled Recommendation (IEDR) model, ageneric framework that differentiates intrinsic from extrinsic factorsconsidering various contexts simultaneously, enabling more accuratedifferentiation of factors and hence the improvement of recommendationaccuracy. IEDR contains a context-invariant contrastive learning component tocapture intrinsic factors, and a disentanglement component to extract extrinsicfactors under the interplay of various contexts. The two components worktogether to achieve effective factor learning. Extensive experiments onreal-world datasets demonstrate IEDR's effectiveness in learning disentangledfactors and significantly improving recommendation accuracy by up to 4% inNDCG.</description>
      <author>example@mail.com (Yixin Su, Wei Jiang, Fangquan Lin, Cheng Yang, Sarah M. Erfani, Junhao Gan, Yunxiang Zhao, Ruixuan Li, Rui Zhang)</author>
      <guid isPermaLink="false">2503.03524v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Tiny Lidars for Manipulator Self-Awareness: Sensor Characterization and Initial Localization Experiments</title>
      <link>http://arxiv.org/abs/2503.03449v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, 3 tables, conference submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种使用来自微型VL53L5CX ToF传感器（小型激光雷达）的粗略点云来定位机器人工作空间中目标对象的方法。&lt;h4&gt;背景&lt;/h4&gt;在许多任务，如操作和检查等场景下，对于机器人来说，能够在周围环境中精确定位目标物体是有益的。&lt;h4&gt;目的&lt;/h4&gt;利用微型ToF传感器获得的数据来改进机器人对其工作空间内目标物的位置估计准确性。&lt;h4&gt;方法&lt;/h4&gt;- 进行实验校准了传感器读数与相对距离及角度之间的依赖关系。- 提出了一种概率性传感器模型，并在使用粒子滤波器（PF）的对象姿态估计任务中对该模型进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;提出的传感器模型相对于两个基准，即测量值无不确定性假设和由传感器数据表提供的置信度，提高了目标对象定位的性能。&lt;h4&gt;结论&lt;/h4&gt;通过采用更准确的概率性传感器模型，机器人在执行各种任务时能够更好地识别并定位目标物体的位置。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已翻译为中文，并且根据其核心要点进行了分点总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For several tasks, ranging from manipulation to inspection, it is beneficialfor robots to localize a target object in their surroundings. In this paper, wepropose an approach that utilizes coarse point clouds obtained fromminiaturized VL53L5CX Time-of-Flight (ToF) sensors (tiny lidars) to localize atarget object in the robot's workspace. We first conduct an experimentalcampaign to calibrate the dependency of sensor readings on relative range andorientation to targets. We then propose a probabilistic sensor model that isvalidated in an object pose estimation task using a Particle Filter (PF). Theresults show that the proposed sensor model improves the performance of thelocalization of the target object with respect to two baselines: one thatassumes measurements are free from uncertainty and one in which the confidenceis provided by the sensor datasheet.</description>
      <author>example@mail.com (Giammarco Caroleo, Alessandro Albini, Daniele De Martini, Timothy D. Barfoot, Perla Maiolino)</author>
      <guid isPermaLink="false">2503.03449v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Mineral segmentation using electron microscope images and spectral sampling through multimodal graph neural networks</title>
      <link>http://arxiv.org/abs/2503.03507v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的方法，用于融合多模态扫描电子显微镜（SEM）图像进行矿物分割。&lt;h4&gt;背景&lt;/h4&gt;通常情况下，通过SEM获取的背散射电子（BSE）图像并不包含足够的信息来进行准确的矿物分割。因此，通常会使用能量色散X射线光谱（EDS）测量来补充BSE图像的数据，以提供关于化学成分的高度精确的信息，但这些数据采集起来非常耗时。&lt;h4&gt;目的&lt;/h4&gt;利用稀疏的光谱数据与BSE图像一起进行矿物分割。&lt;h4&gt;方法&lt;/h4&gt;提出使用图神经网络将两种模态融合，并同时完成矿物相的分割。&lt;h4&gt;主要发现&lt;/h4&gt;即使仅提供1%的BSE像素上的EDS数据，也能产生准确的分割结果，从而实现了对矿物样品的快速分析。&lt;h4&gt;结论&lt;/h4&gt;提出的这种数据融合管道具有很好的灵活性，可以适应其他需要图像数据和点测量领域的应用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于图神经网络的方法，用于根据多模态扫描电子显微镜（SEM）图像的数据融合进行分割。在大多数情况下，通过SEM获取的背散射电子（BSE）图像并不包含足够的信息来进行矿物分割。因此，成像通常会与点测量的能量色散X射线光谱（EDS）光谱数据相结合，这些数据提供关于化学成分的高度精确的信息，但采集起来非常耗时。这激发了使用稀疏的光谱数据结合BSE图像进行矿物分割的需求。由于光谱数据的本质是无结构化的，大多数传统的图像融合技术都不适合用于BSE-EDS融合。我们提出采用图神经网络来融合两种模态，并同时完成矿物相的分割。我们的结果表明，在提供给定1% BSE像素上的EDS数据的情况下可以实现准确的分割，这使得能够快速分析矿物样品。提出的这种数据融合管道具有很好的灵活性，可以适应其他需要图像数据和点测量领域的应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel Graph Neural Network-based method for segmentation basedon data fusion of multimodal Scanning Electron Microscope (SEM) images. In mostcases, Backscattered Electron (BSE) images obtained using SEM do not containsufficient information for mineral segmentation. Therefore, imaging is oftencomplemented with point-wise Energy-Dispersive X-ray Spectroscopy (EDS)spectral measurements that provide highly accurate information about thechemical composition but that are time-consuming to acquire. This motivates theuse of sparse spectral data in conjunction with BSE images for mineralsegmentation. The unstructured nature of the spectral data makes mosttraditional image fusion techniques unsuitable for BSE-EDS fusion. We proposeusing graph neural networks to fuse the two modalities and segment the mineralphases simultaneously. Our results demonstrate that providing EDS data for asfew as 1% of BSE pixels produces accurate segmentation, enabling rapid analysisof mineral samples. The proposed data fusion pipeline is versatile and can beadapted to other domains that involve image data and point-wise measurements.</description>
      <author>example@mail.com (Samuel Repka, Bořek Reich, Fedor Zolotarev, Tuomas Eerola, Pavel Zemčík)</author>
      <guid isPermaLink="false">2503.03507v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Attributed Dynamic Network Embedding with Stability Guarantees</title>
      <link>http://arxiv.org/abs/2503.02859v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了AUASE，一种用于动态网络的稳定无监督表示学习框架，适用于节点带有随时间变化属性信息的情况。&lt;h4&gt;背景&lt;/h4&gt;在处理具有时变属性信息的动态网络时，稳定性对于确保相同行为的节点在同一时间点拥有相同的嵌入至关重要，这使得可以在不同时刻对比网络中的节点。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的表示学习方法AUASE，并证明其能够在没有地面真实标签的情况下满足稳定性保证。&lt;h4&gt;方法&lt;/h4&gt;通过展示AUASE的一致收敛性到相关的潜在位置模型来建立稳定性。利用三种真实的属性网络，在链接预测和节点分类任务中将AUASE与最先进的网络表示学习方法进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;AUASE是在没有地面真实标签的情况下唯一能够满足稳定性的属性动态嵌入，实验表明其对于链接预测和节点分类提供了显著的改进。&lt;h4&gt;结论&lt;/h4&gt;AUASE为处理具有时变属性信息的动态网络提供了一个强大的工具，并且在不需要标签的情况下确保了表示学习的稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stability for dynamic network embeddings ensures that nodes behaving the sameat different times receive the same embedding, allowing comparison of nodes inthe network across time. We present attributed unfolded adjacency spectralembedding (AUASE), a stable unsupervised representation learning framework fordynamic networks in which nodes are attributed with time-varying covariateinformation. To establish stability, we prove uniform convergence to anassociated latent position model. We quantify the benefits of our dynamicembedding by comparing with state-of-the-art network representation learningmethods on three real attributed networks. To the best of our knowledge, AUASEis the only attributed dynamic embedding that satisfies stability guaranteeswithout the need for ground truth labels, which we demonstrate providessignificant improvements for link prediction and node classification.</description>
      <author>example@mail.com (Emma Ceccherini, Ian Gallagher, Andrew Jones, Daniel Lawson)</author>
      <guid isPermaLink="false">2503.02859v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Meta Learning-Driven Iterative Refinement for Robust Anomaly Detection in Industrial Inspection</title>
      <link>http://arxiv.org/abs/2503.01569v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted in the VISION workshop at ECCV 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了鲁棒异常检测模型在工业检查中的性能，特别是在处理噪声数据方面的能力。&lt;h4&gt;背景&lt;/h4&gt;当前的工业检查中存在大量噪声数据，这对传统的异常检测模型构成了挑战。这些噪声可能导致错误的缺陷识别和高误报率。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来提高模型适应性和鲁棒性，通过元学习方法识别并拒绝训练中的噪声数据以改进学习过程。&lt;h4&gt;方法&lt;/h4&gt;采用无模型假设元学习（MAML）以及迭代细化流程，利用四分位距剔除方案增强其可适应性和鲁棒性。这种方法可以有效提升模型区分正常与异常样本的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在噪声环境中的表现优于传统模型，并且即使是在清晰训练集的情况下也能很好地隔离那些偏离分布的样本，从而提供显著改进。&lt;h4&gt;结论&lt;/h4&gt;提出的方法不仅适用于高噪音环境下，而且也可以用于处理标准数据集，在保证准确性的前提下提高了异常检测的效率和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;这项研究探讨了鲁棒性异常检测模型在工业检查中的表现，特别是它们应对噪声数据的能力。我们建议利用元学习方法的适应能力来识别并排除训练数据中的噪声，以改进学习过程。在我们的模型中，我们使用了无模型假设元学习（MAML）以及迭代细化流程通过四分位距剔除方案提高其可适应性和鲁棒性。这种方法显著增强了区分正常和缺陷条件的能力。我们在著名的MVTec和KSDD2数据集上进行的实验结果表明，所提出的方法不仅在大量噪声环境中表现出色，而且还可以为明确训练集合贡献，在分离那些偏离分布样本的同时提供了对传统模型的重要改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the performance of robust anomaly detection models inindustrial inspection, focusing particularly on their ability to handle noisydata. We propose to leverage the adaptation ability of meta learning approachesto identify and reject noisy training data to improve the learning process. Inour model, we employ Model Agnostic Meta Learning (MAML) and an iterativerefinement process through an Inter-Quartile Range rejection scheme to enhancetheir adaptability and robustness. This approach significantly improves themodels capability to distinguish between normal and defective conditions. Ourresults of experiments conducted on well known MVTec and KSDD2 datasetsdemonstrate that the proposed method not only excels in environments withsubstantial noise but can also contribute in case of a clear training set,isolating those samples that are relatively out of distribution, thus offeringsignificant improvements over traditional models.</description>
      <author>example@mail.com (Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti)</author>
      <guid isPermaLink="false">2503.01569v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Remote Sensing Image Classification Using Convolutional Neural Network (CNN) and Transfer Learning Techniques</title>
      <link>http://arxiv.org/abs/2503.02510v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is published in Journal of Computer Science, Volume 21 No.  3, 2025. It contains 635-645 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究探讨了利用卷积神经网络（CNN）架构对包含输电塔、森林、农田和山脉的航空图像进行分类。&lt;h4&gt;目的&lt;/h4&gt;旨在通过实验来评估不同模型在土地覆盖分类任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;采用预训练的VGG16和MobileNetV2模型作为迁移学习的基础，利用自收集与MLRNet数据集结合的数据集进行测试。该数据集包含从Google卫星图像中提取的共10,400张图片。&lt;h4&gt;主要发现&lt;/h4&gt;{'总体表现': '基于构建的CNN模型的整体准确率为87%；', 'VGG16性能': '使用预训练的VGG16进行迁移学习后，准确率达到90%，测试损失为0.298；', 'MobileNetV2优势': '而采用MobileNetV2进行转移学习后的模型表现最佳，其准确率高达96%，且测试损失低至0.119。'}&lt;h4&gt;结论&lt;/h4&gt;研究表明，在土地覆盖分类任务中使用迁移学习，尤其是基于MobileNetV2的模型，可以取得优异的效果；这不仅提高了精度也增强了实用性。&lt;h4&gt;翻译&lt;/h4&gt;这项研究调查了利用卷积神经网络（CNN）架构从描绘输电塔、森林、农田和山脉的航空图像中提取特征，并通过Softmax进行分类。为了测试该模型，我们运行了十轮训练，使用批次大小为90，Adam优化器和学习率为0.001。在自收集图片与MLRNet数据集相结合的数据集中进行了训练及评估，包括从Google卫星影像中获得的共10,400张图像。研究表明，迁移学习模型尤其是MobileNetV2，在土地覆盖分类方面表现优异；这些模型因其良好的精度和效率平衡而适合实际应用；我们的方法在构建的CNN模型上达到了87%的整体准确率；通过使用预训练的VGG16与MobileNetV2作为迁移学习的基础，我们实现了更高的准确性。特别是，VGG16达到90%的准确性和0.298的测试损失，而MobileNetV2则以96%的准确度和0.119的测试损失领先所有模型；这些结果证明了使用基于MobileNetV2迁移学习进行输电塔、森林、农田和山脉分类的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.3844/jcssp.2025.635.645&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the classification of aerial images depictingtransmission towers, forests, farmland, and mountains. To complete theclassification job, features are extracted from input photos using aConvolutional Neural Network (CNN) architecture. Then, the images areclassified using Softmax. To test the model, we ran it for ten epochs using abatch size of 90, the Adam optimizer, and a learning rate of 0.001. Bothtraining and assessment are conducted using a dataset that blendsself-collected pictures from Google satellite imagery with the MLRNet dataset.The comprehensive dataset comprises 10,400 images. Our study shows thattransfer learning models and MobileNetV2 in particular, work well for landscapecategorization. These models are good options for practical use because theystrike a good mix between precision and efficiency; our approach achievesresults with an overall accuracy of 87% on the built CNN model. Furthermore, wereach even higher accuracies by utilizing the pretrained VGG16 and MobileNetV2models as a starting point for transfer learning. Specifically, VGG16 achievesan accuracy of 90% and a test loss of 0.298, while MobileNetV2 outperforms bothmodels with an accuracy of 96% and a test loss of 0.119; the resultsdemonstrate the effectiveness of employing transfer learning with MobileNetV2for classifying transmission towers, forests, farmland, and mountains.</description>
      <author>example@mail.com (Mustafa Majeed Abd Zaid, Ahmed Abed Mohammed, Putra Sumari)</author>
      <guid isPermaLink="false">2503.02510v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GNNMerge: Merging of GNN Models Without Accessing Training Data</title>
      <link>http://arxiv.org/abs/2503.03384v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了GNNMerge算法，用于合并图神经网络（GNN），在各种数据集、任务和架构上验证了其优越性和效率。', '背景': '模型融合作为一种将多个训练好的模型整合成单一模型的方法，在不访问原始训练数据的情况下得到了广泛的应用。尽管这种技术已经在计算机视觉和自然语言处理领域显示出成功，但将其应用于GNN上的研究尚属空白。', '目的': '首次对GNN的模型合并算法进行基准测试，并提出一种新的方法来解决现有方法在GNN中的应用限制。', '方法': '提出了名为GNNMerge的新策略，该策略使用任务无关节点嵌入对齐技术来实现GNN模型的合并。此外，它还展示了在轻微放宽的情况下，提出的优化目标对于广泛使用的GNN架构可以提供直接解析解。', '主要发现': '实验表明，与现有方法相比，GNNMerge提高了24%的准确性，并且与从头训练相比，计算效率提高了一个数量级以上。', '结论': 'GNNMerge通过任务无关节点嵌入对齐技术成功解决了模型合并中的挑战，展示了在不同场景下的优越性能和速度优势。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了机器学习领域中将多个已训练的模型整合为单一模型的过程，并探讨了现有方法如何在计算机视觉和自然语言处理等领域取得成功，但在图神经网络（GNN）中的应用仍然空白。论文作者进行了首次针对GNN的模型合并算法基准测试研究，揭示了这些算法在此上下文中效果有限的问题。为了应对这一挑战，他们提出了GNNMerge算法，该算法利用了一种任务无关节点嵌入对齐策略来实现GNN模型的合并，并且在轻微放宽假设的情况下展示了优化目标对于广泛使用的GNN架构可以提供直接解析解，显著提高了计算效率。实验结果表明，在各种数据集、任务和架构下，与现有方法相比，GNNMerge准确率最多提高24%，同时相对于从头开始训练的速度提升了两个数量级以上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model merging has gained prominence in machine learning as a method tointegrate multiple trained models into a single model without accessing theoriginal training data. While existing approaches have demonstrated success indomains such as computer vision and NLP, their application to Graph NeuralNetworks (GNNs) remains unexplored. These methods often rely on the assumptionof shared initialization, which is seldom applicable to GNNs. In this work, weundertake the first benchmarking study of model merging algorithms for GNNs,revealing their limited effectiveness in this context. To address thesechallenges, we propose GNNMerge, which utilizes a task-agnostic node embeddingalignment strategy to merge GNNs. Furthermore, we establish that under a mildrelaxation, the proposed optimization objective admits direct analyticalsolutions for widely used GNN architectures, significantly enhancing itscomputational efficiency. Empirical evaluations across diverse datasets, tasks,and architectures establish GNNMerge to be up to 24% more accurate thanexisting methods while delivering over 2 orders of magnitude speed-up comparedto training from scratch.</description>
      <author>example@mail.com (Vipul Garg, Ishita Thakre, Sayan Ranu)</author>
      <guid isPermaLink="false">2503.03384v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PABBO: Preferential Amortized Black-Box Optimization</title>
      <link>http://arxiv.org/abs/2503.00924v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  25 pages, 17 figures. Accepted at the Thirteenth International  Conference on Learning Representations (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Preferential Bayesian Optimization (PBO)是一种通过用户对设计配对的偏好反馈高效学习潜在用户偏好的方法。它使用统计代理模型，如高斯过程，并采用获取策略选择下一个候选配对以获得用户反馈。&lt;h4&gt;背景&lt;/h4&gt;由于偏好贝叶斯优化中的似然非共轭性，每次步骤都需要大量的近似推理计算，这与人机交互的方式不兼容，限制了其在实际案例中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于最新进展的推算化贝叶斯优化方法来克服这一问题，并通过元学习代理和获取函数实现PBO的完全推算化。&lt;h4&gt;方法&lt;/h4&gt;该方法包含一个新颖的转换器神经过程架构，采用强化学习训练并使用定制辅助损失进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;在合成数据集和真实世界数据集组成的基准测试上，这种方法比传统的高斯过程策略快几个数量级，并且通常在准确性方面也优于它们。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了一种高效的推算化偏好贝叶斯优化方法，为实际应用中的用户偏好学习提供了新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Preferential Bayesian Optimization (PBO)是一种高效的学习潜在用户偏好的方法，通过从设计配对的偏好反馈中获取信息。由于非共轭性问题，每次步骤都涉及大量计算。本文提出了一种基于推算化贝叶斯优化的方法，它可以通过元学习代理和获取函数实现完全推算化，并且在实际测试数据集上表现出更高的效率和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Preferential Bayesian Optimization (PBO) is a sample-efficient method tolearn latent user utilities from preferential feedback over a pair of designs.It relies on a statistical surrogate model for the latent function, usually aGaussian process, and an acquisition strategy to select the next candidate pairto get user feedback on. Due to the non-conjugacy of the associated likelihood,every PBO step requires a significant amount of computations with variousapproximate inference techniques. This computational overhead is incompatiblewith the way humans interact with computers, hindering the use of PBO inreal-world cases. Building on the recent advances of amortized BO, we proposeto circumvent this issue by fully amortizing PBO, meta-learning both thesurrogate and the acquisition function. Our method comprises a noveltransformer neural process architecture, trained using reinforcement learningand tailored auxiliary losses. On a benchmark composed of synthetic andreal-world datasets, our method is several orders of magnitude faster than theusual Gaussian process-based strategies and often outperforms them in accuracy.</description>
      <author>example@mail.com (Xinyu Zhang, Daolang Huang, Samuel Kaski, Julien Martinelli)</author>
      <guid isPermaLink="false">2503.00924v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLMs as Educational Analysts: Transforming Multimodal Data Traces into Actionable Reading Assessment Reports</title>
      <link>http://arxiv.org/abs/2503.02099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究探讨了利用多模态数据源（包括眼动追踪数据、学习成果、评估内容和教学标准）来获取有意义的阅读洞察的方法。我们采用无监督学习技术识别不同的阅读行为模式，并使用大型语言模型将所得信息合成成易于理解且富有操作性的报告供教师参考，从而简化解读过程。&lt;h4&gt;背景&lt;/h4&gt;目前许多教育科技应用程序主要侧重于结果导向型指标，而对学生的具体行为和认知洞察有限。这限制了对学生阅读能力的深入理解和提升。&lt;h4&gt;目的&lt;/h4&gt;研究目的是通过集成多模态数据源来改进学生阅读评估的方法，并探究如何利用人工智能技术生成有价值的教师报告。&lt;h4&gt;方法&lt;/h4&gt;本研究运用无监督机器学习算法识别不同的阅读模式，然后使用大型语言模型将这些结果整理成对教育工作者有帮助的、具体的行动建议。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，大型语言模型能够有效地充当教育分析员的角色，将多样化的数据转化为教师友好的见解，并且这种由人工智能生成的报告在清晰度、准确性、相关性和教学实用性方面得到了专家和教育者的认可。&lt;h4&gt;结论&lt;/h4&gt;虽然自动化洞察力生成展现了巨大潜力，但为了确保可靠性和公平性，仍需人类监督。这项研究促进了以人为本的人工智能技术在教育领域的应用，将数据驱动的分析与实际课堂实践紧密结合在一起。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的研究调查了利用多模态数据源获取有意义阅读洞察的方法，并通过无监督学习技术和大型语言模型来生成教师友好的报告。结果表明该方法有效且得到认可，同时强调了人类监督的重要性以及其在教育领域中的应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reading assessments are essential for enhancing students' comprehension, yetmany EdTech applications focus mainly on outcome-based metrics, providinglimited insights into student behavior and cognition. This study investigatesthe use of multimodal data sources -- including eye-tracking data, learningoutcomes, assessment content, and teaching standards -- to derive meaningfulreading insights. We employ unsupervised learning techniques to identifydistinct reading behavior patterns, and then a large language model (LLM)synthesizes the derived information into actionable reports for educators,streamlining the interpretation process. LLM experts and human educatorsevaluate these reports for clarity, accuracy, relevance, and pedagogicalusefulness. Our findings indicate that LLMs can effectively function aseducational analysts, turning diverse data into teacher-friendly insights thatare well-received by educators. While promising for automating insightgeneration, human oversight remains crucial to ensure reliability and fairness.This research advances human-centered AI in education, connecting data-drivenanalytics with practical classroom applications.</description>
      <author>example@mail.com (Eduardo Davalos, Yike Zhang, Namrata Srivastava, Jorge Alberto Salas, Sara McFadden, Sun-Joo Cho, Gautam Biswas, Amanda Goodwin)</author>
      <guid isPermaLink="false">2503.02099v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Deep Learning for Subtype Classification in Breast Cancer Using Histopathological Images and Gene Expression Data</title>
      <link>http://arxiv.org/abs/2503.02849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个深度多模态学习框架，结合了乳腺癌的组织病理图像和基因表达数据，以分类BRCA.Luminal和BRCA.Basal / Her2亚型。&lt;h4&gt;背景&lt;/h4&gt;传统的乳腺癌分子分型方法主要依赖于单一的组织病理学或基因表达分析，预测能力有限。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度学习框架来提高乳腺癌亚型分类的准确性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;采用ResNet-50模型提取图像特征，并使用全连接层处理基因表达数据。引入跨注意力融合机制以增强模态交互，使用五折交叉验证进行实验评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的多模态整合框架在分类准确率、精确召回AUC和F1分数方面优于单一模式的方法。&lt;h4&gt;结论&lt;/h4&gt;研究强调了深度学习方法在乳腺癌亚型分类中的潜力，并为临床决策提供了支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular subtyping of breast cancer is crucial for personalized treatmentand prognosis. Traditional classification approaches rely on eitherhistopathological images or gene expression profiling, limiting theirpredictive power. In this study, we propose a deep multimodal learningframework that integrates histopathological images and gene expression data toclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Ourapproach employs a ResNet-50 model for image feature extraction and fullyconnected layers for gene expression processing, with a cross-attention fusionmechanism to enhance modality interaction. We conduct extensive experimentsusing five-fold cross-validation, demonstrating that our multimodal integrationoutperforms unimodal approaches in terms of classification accuracy,precision-recall AUC, and F1-score. Our findings highlight the potential ofdeep learning for robust and interpretable breast cancer subtypeclassification, paving the way for improved clinical decision-making.</description>
      <author>example@mail.com (Amin Honarmandi Shandiz)</author>
      <guid isPermaLink="false">2503.02849v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal AI predicts clinical outcomes of drug combinations from preclinical data</title>
      <link>http://arxiv.org/abs/2503.02781v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;MADRIGAL是一个多模态AI模型，能够从结构、通路、细胞活力和转录组数据中学习，预测药物组合效果，并在临床结果预测上优于单一模式方法和其他最先进的模型。&lt;h4&gt;背景&lt;/h4&gt;目前的模型依靠结构或靶点特征来识别高效且低毒性的药物组合，但这些方法未能整合多模态数据以进行准确、具有临床相关性的预测。&lt;h4&gt;目的&lt;/h4&gt;介绍MADRIGAL模型，用于更准确地预测药物组合效果以及潜在的安全性和毒性风险，并支持个性化癌症治疗和二型糖尿病及代谢紊乱相关的脂肪肝病的药物管理。&lt;h4&gt;方法&lt;/h4&gt;使用变压器瓶颈模块统一预临床药物数据模态，在训练和推理过程中处理缺失的数据。该模型利用多模态学习，包括结构、通路、细胞活力和转录组学数据进行药物组合效应预测。&lt;h4&gt;主要发现&lt;/h4&gt;MADRIGAL在预测不良药物相互作用方面优于单一模式方法和其他最先进的模型，并且能够识别转运蛋白介导的药物相互作用。它还支持虚拟筛选抗癌症药物组合以及个性化治疗方案的设计。&lt;h4&gt;结论&lt;/h4&gt;MADRIGAL提供了一种多模态的方法来设计结合疗法，具有改进的预测准确性和临床相关性。该模型还可以通过与大型语言模型集成，让用户以自然语言描述临床结果，从而改善安全评估并识别潜在风险。&lt;h4&gt;翻译&lt;/h4&gt;从预临床数据中预测临床效果对于识别安全有效的药物组合至关重要。现有方法依赖于结构或靶点特征来选择高效率低毒性的药物组合，但这些方法未能整合进行准确且具有临床相关性预测所需的多模态数据。MADRIGAL模型可以利用结构、路径、细胞活力和转录组学数据，预测21842种化合物（包括已批准的药物和在研新型化合物）以及953个临床效果组合效应。此模型通过使用变压器瓶颈模块来统一预临床药物模态，并解决了多模态学习中的缺失数据问题，在预测不良药物相互作用方面优于单一模式方法和其他最先进的模型。该模型支持虚拟筛选抗癌症药物组合，有助于二型糖尿病和代谢紊乱相关脂肪肝病的治疗管理并识别转运蛋白介导的药物相互作用。MADRIGAL还通过整合来自癌症患者的基因组配置文件来支持个性化癌症疗法，并且能够预测基于急性髓性白血病样本和个人来源的小鼠模型中个性化的药物组合疗效。与大型语言模型集成后，用户可以使用自然语言描述临床结果，从而提高安全性评估并识别潜在的风险。MADRIGAL为设计具有改进的预测准确性和临床相关性的结合疗法提供了多模态方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting clinical outcomes from preclinical data is essential foridentifying safe and effective drug combinations. Current models rely onstructural or target-based features to identify high-efficacy, low-toxicitydrug combinations. However, these approaches fail to incorporate the multimodaldata necessary for accurate, clinically-relevant predictions. Here, weintroduce MADRIGAL, a multimodal AI model that learns from structural, pathway,cell viability, and transcriptomic data to predict drug combination effectsacross 953 clinical outcomes and 21842 compounds, including combinations ofapproved drugs and novel compounds in development. MADRIGAL uses a transformerbottleneck module to unify preclinical drug data modalities while handlingmissing data during training and inference--a major challenge in multimodallearning. It outperforms single-modality methods and state-of-the-art models inpredicting adverse drug interactions. MADRIGAL performs virtual screening ofanticancer drug combinations and supports polypharmacy management for type IIdiabetes and metabolic dysfunction-associated steatohepatitis (MASH). Itidentifies transporter-mediated drug interactions. MADRIGAL predictsresmetirom, the first and only FDA-approved drug for MASH, among therapies withthe most favorable safety profile. It supports personalized cancer therapy byintegrating genomic profiles from cancer patients. Using primary acute myeloidleukemia samples and patient-derived xenograft models, it predicts the efficacyof personalized drug combinations. Integrating MADRIGAL with a large languagemodel allows users to describe clinical outcomes in natural language, improvingsafety assessment by identifying potential adverse interactions and toxicityrisks. MADRIGAL provides a multimodal approach for designing combinationtherapies with improved predictive accuracy and clinical relevance.</description>
      <author>example@mail.com (Yepeng Huang, Xiaorui Su, Varun Ullanat, Ivy Liang, Lindsay Clegg, Damilola Olabode, Nicholas Ho, Bino John, Megan Gibbs, Marinka Zitnik)</author>
      <guid isPermaLink="false">2503.02781v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>FASTer: Focal Token Acquiring-and-Scaling Transformer for Long-term 3D Object Detection</title>
      <link>http://arxiv.org/abs/2503.01899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10pages,6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近基于Lidar的顶级时间3D检测器越来越多地采用区域级范式，该范式首先生成粗略提案，然后编码和融合区域特征。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理随输入帧数量增加而指数增长的复杂性时存在挑战，并且结果层面的任意连接限制了全局信息提取。&lt;h4&gt;目的&lt;/h4&gt;本文提出了Focal Token Acquiring-and-Scaling Transformer (FASTer)，它以自适应和轻量级的方式动态选择焦点令牌并浓缩令牌序列。&lt;h4&gt;方法&lt;/h4&gt;提出了一种简单但有效的自适应缩放机制，强调单个令牌的贡献，并在历史帧中仅存储和处理焦点点，从而大大减少了整体复杂性。此外，还提出了分组层次融合策略，逐步执行序列缩放和组内融合操作以促进全局空间和时间信息交换。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在Waymo Open Dataset上，FASTer在性能、效率方面显著优于其他最先进的检测器，并且展现出更高的灵活性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法在处理大规模数据集上的3D目标检测任务时表现出色。&lt;h4&gt;翻译&lt;/h4&gt;最近表现优异的基于Lidar的时间三维探测器越来越多地采用了区域级方法。它们首先生成粗略提案，然后编码和融合区域特征。然而，无差别采样和融合通常忽略了个体点的不同贡献，并且随着输入帧数量的增长导致复杂性呈指数上升。此外，任意的结果级别串联限制了全局信息的提取。本文提出了一种Focal Token Acquiring-and-Scaling Transformer (FASTer)，该方法以自适应、轻量级的方式动态选择焦点令牌并浓缩令牌序列。强调单个令牌的贡献，我们提出了一个简单但有效的自适应缩放机制来捕获几何上下文并在筛选出焦点点的同时剔除非关键信息。在历史帧中仅存储和处理焦点点大大减少了整体复杂性。此外，还提出了一种新颖的分组层次融合策略，逐步执行序列缩放和组内融合操作以促进全局空间和时间信息交换。实验结果表明，在Waymo Open Dataset上，FASTer显著优于其他最先进的检测器，并且在性能、效率方面表现出色，同时还展现了更高的灵活性和鲁棒性。相关代码可在https://github.com/MSunDYY/FASTer.git获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/msundyy/faster&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent top-performing temporal 3D detectors based on Lidars have increasinglyadopted region-based paradigms. They first generate coarse proposals, followedby encoding and fusing regional features. However, indiscriminate sampling andfusion often overlook the varying contributions of individual points and leadto exponentially increased complexity as the number of input frames grows.Moreover, arbitrary result-level concatenation limits the global informationextraction. In this paper, we propose a Focal Token Acquring-and-ScalingTransformer (FASTer), which dynamically selects focal tokens and condensestoken sequences in an adaptive and lightweight manner. Emphasizing thecontribution of individual tokens, we propose a simple but effective AdaptiveScaling mechanism to capture geometric contexts while sifting out focal points.Adaptively storing and processing only focal points in historical framesdramatically reduces the overall complexity. Furthermore, a novel GroupedHierarchical Fusion strategy is proposed, progressively performing sequencescaling and Intra-Group Fusion operations to facilitate the exchange of globalspatial and temporal information. Experiments on the Waymo Open Datasetdemonstrate that our FASTer significantly outperforms other state-of-the-artdetectors in both performance and efficiency while also exhibiting improvedflexibility and robustness. The code is available athttps://github.com/MSunDYY/FASTer.git.</description>
      <author>example@mail.com (Chenxu Dang, Zaipeng Duan, Pei An, Xinmin Zhang, Xuzhong Hu, Jie Ma)</author>
      <guid isPermaLink="false">2503.01899v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Structural Entropy Guided Unsupervised Graph Out-Of-Distribution Detection</title>
      <link>http://arxiv.org/abs/2503.03241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2025 (The 39th Annual AAAI Conference on Artificial  Intelligence)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了SEGO框架，通过在图分类中集成结构熵到无监督OOD检测中来解决现有方法在处理冗余信息时的性能问题。&lt;h4&gt;背景&lt;/h4&gt;随着大量未标记数据的出现，无监督OOD检测对于确保图形神经网络(GNN)在测试期间可靠地识别ID和OOD样本至关重要。现有的方法由于图结构中的冗余信息而导致性能受损。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法在处理图结构中冗余信息时遇到的问题，并提高GNN模型的可靠性，提出了SEGO框架。&lt;h4&gt;方法&lt;/h4&gt;SEGO通过引入基于编码树形式的锚点视图来最小化结构熵，在对比学习架构下操作。该方案还采用三元组视角进行局部、全局和树级别的多粒度对比学习。&lt;h4&gt;主要发现&lt;/h4&gt;SEGO能够有效地从图中去除冗余信息，同时保留重要的结构信息，使ID与OOD样本之间的区别更为明显。实验结果表明，在无监督OOD检测方面，该方法优于现有最佳基线模型。&lt;h4&gt;结论&lt;/h4&gt;在真实数据集上的广泛实验证明了SEGO的有效性，尤其是在9对10的数据集中表现最优，并且在FreeSolv/ToxCast数据集上超越了竞争对手10.8%。&lt;h4&gt;翻译&lt;/h4&gt;随着大量未标记数据的出现，无监督OOD检测对于确保图形神经网络(GNN)在测试期间可靠地识别ID和OOD样本至关重要。现有的方法由于图结构中的冗余信息而导致性能受损。为了应对这一挑战，我们提出了SEGO，一种集成结构熵进行图分类中无监督OOD检测的框架。该方法通过引入编码树形式的锚点视图来最小化结构熵，并采用了局部、全局和树级别多粒度对比学习方案。实验结果表明，在10对数据集中有9对表现最佳，平均性能提高3.7%，在FreeSolv/ToxCast数据集上优于对手10.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the emerging of huge amount of unlabeled data, unsupervisedout-of-distribution (OOD) detection is vital for ensuring the reliability ofgraph neural networks (GNNs) by identifying OOD samples from in-distribution(ID) ones during testing, where encountering novel or unknown data isinevitable. Existing methods often suffer from compromised performance due toredundant information in graph structures, which impairs their ability toeffectively differentiate between ID and OOD data. To address this challenge,we propose SEGO, an unsupervised framework that integrates structural entropyinto OOD detection regarding graph classification. Specifically, within thearchitecture of contrastive learning, SEGO introduces an anchor view in theform of coding tree by minimizing structural entropy. The obtained coding treeeffectively removes redundant information from graphs while preservingessential structural information, enabling the capture of distinct graphpatterns between ID and OOD samples. Furthermore, we present a multi-grainedcontrastive learning scheme at local, global, and tree levels using tripletviews, where coding trees with essential information serve as the anchor view.Extensive experiments on real-world datasets validate the effectiveness ofSEGO, demonstrating superior performance over state-of-the-art baselines in OODdetection. Specifically, our method achieves the best performance on 9 out of10 dataset pairs, with an average improvement of 3.7\% on OOD detectiondatasets, significantly surpassing the best competitor by 10.8\% on theFreeSolv/ToxCast dataset pair.</description>
      <author>example@mail.com (Yue Hou, He Zhu, Ruomei Liu, Yingke Su, Jinxiang Xia, Junran Wu, Ke Xu)</author>
      <guid isPermaLink="false">2503.03241v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Direct Sparse Odometry with Continuous 3D Gaussian Maps for Indoor Environments</title>
      <link>http://arxiv.org/abs/2503.03373v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages,5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在机器人和增强现实等领域中，精确的定位对于自主导航至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于单目视觉里程计框架的方法，该方法利用连续3D高斯地图来提高姿态估计的准确性，并降低成本。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一个使用连续3D高斯图的单目视觉里程计框架，该框架直接为所有提取出的高梯度点分配几何一致性的深度值，避免了传统的插值步骤。&lt;h4&gt;主要发现&lt;/h4&gt;通过在两个公开数据集上的评估显示，提出的框架相比现有方法具有更优的姿态跟踪精度。&lt;h4&gt;结论&lt;/h4&gt;研究团队已经开源了这项工作的源代码以促进社区发展。&lt;h4&gt;翻译&lt;/h4&gt;准确的定位对于机器人和增强现实应用（如自主导航）至关重要。基于视觉的方法结合先验地图旨在将LiDAR级别的准确性与相机的成本效率结合起来，实现稳健的姿态估计。然而，现有的方法在关联离散点云图和密集图像像素时常常依赖于不可靠的插值过程，这不可避免地引入了深度误差并降低了姿态估计精度。我们提出了一种单目视觉里程计框架，利用连续3D高斯地图，该框架直接为所有提取出的高梯度点分配几何一致性的深度值而无需插值步骤。在两个公开数据集上的评估表明，相比现有方法具有更优的姿态跟踪准确性。研究团队已经开源了这项工作的源代码以促进社区发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for robotics and augmented realityapplications such as autonomous navigation. Vision-based methods combiningprior maps aim to integrate LiDAR-level accuracy with camera cost efficiencyfor robust pose estimation. Existing approaches, however, often depend onunreliable interpolation procedures when associating discrete point cloud mapswith dense image pixels, which inevitably introduces depth errors and degradespose estimation accuracy. We propose a monocular visual odometry frameworkutilizing a continuous 3D Gaussian map, which directly assigns geometricallyconsistent depth values to all extracted high-gradient points withoutinterpolation. Evaluations on two public datasets demonstrate superior trackingaccuracy compared to existing methods. We have released the source code of thiswork for the development of the community.</description>
      <author>example@mail.com (Jie Deng, Fengtian Lang, Zikang Yuan, Xin Yang)</author>
      <guid isPermaLink="false">2503.03373v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Developing a PET/CT Foundation Model for Cross-Modal Anatomical and Functional Imaging</title>
      <link>http://arxiv.org/abs/2503.02824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个用于多模态PET/CT成像的新型基础模型框架Cross-Fraternal Twin Masked Autoencoder (FratMAE)，该框架旨在解决现有AI驱动的PET/CT分析方法中泛化性和鲁棒性不足的问题。&lt;h4&gt;背景&lt;/h4&gt;在肿瘤学领域，正电子发射断层扫描-计算机断层扫描(PET/CT)结合了CT提供的解剖细节和PET提供的功能性代谢活性及分子标志物表达信息，在癌症诊断、分期以及治疗监测方面被广泛应用。然而，现有的基于人工智能的PET/CT分析主要依赖于从头训练的任务特定模型或受限数据集，这限制了它们的泛化能力和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种专门设计用于多模态PET/CT成像的基础模型方法，以提高其在临床应用中的性能和可靠性。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为Cross-Fraternal Twin Masked Autoencoder (FratMAE)的新框架。该框架使用单独的视觉变换器(ViT)编码器处理PET和CT扫描，并通过交叉注意力解码器促进模式间的协同交互，同时结合文本元数据来增强PET表示的学习。&lt;h4&gt;主要发现&lt;/h4&gt;FratMAE在预训练过程中捕获了复杂的跨模态关系和全局摄取模式，在下游任务中表现出优越的性能。这表明该模型具有作为通用基础模型的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过展示其在多模态PET/CT成像分析中的卓越能力，FratMAE提供了一种改进现有AI驱动癌症成像技术的新途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此处已将其内容精简并翻译成了中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) iswidely used in cancer diagnosis, staging, and treatment monitoring, as itcombines anatomical details from CT with functional metabolic activity andmolecular marker expression information from PET. However, existing artificialintelligence-driven PET/CT analyses rely predominantly on task-specific modelstrained from scratch or on limited datasets, limiting their generalizabilityand robustness. To address this, we propose a foundation model approachspecifically designed for multimodal PET/CT imaging. We introduce theCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework thateffectively integrates whole-body anatomical and functional or molecularinformation. FratMAE employs separate Vision Transformer (ViT) encoders for PETand CT scans, along with cross-attention decoders that enable synergisticinteractions between modalities during masked autoencoder training.Additionally, it incorporates textual metadata to enhance PET representationlearning. By pre-training on PET/CT datasets, FratMAE captures intricatecross-modal relationships and global uptake patterns, achieving superiorperformance on downstream tasks and demonstrating its potential as ageneralizable foundation model.</description>
      <author>example@mail.com (Yujin Oh, Robert Seifert, Yihan Cao, Christoph Clement, Justin Ferdinandus, Constantin Lapa, Alessandro Liebich, Michelle Amon, Johanna Enke, Sifan Song, Runqi Meng, Fang Zeng, Ning Guo, Xiang Li, Pedram Heidari, Axel Rominger, Kuangyu Shi, Quanzheng Li)</author>
      <guid isPermaLink="false">2503.02824v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>X2CT-CLIP: Enable Multi-Abnormality Detection in Computed Tomography from Chest Radiography via Tri-Modal Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.02162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure, 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了X2CT-CLIP框架，通过在潜在空间中设计的三模态对齐机制将3D CT体积和放射学报告的知识转移到CXR编码器上，实现了从CT到CXR的跨模式知识转移。&lt;h4&gt;背景&lt;/h4&gt;虽然CT是诊断中的重要影像方式，但由于辐射暴露高且处理时间长，限制了其在大规模筛查中的应用。相比之下，胸部X光（CXR）更易于获取和安全，但现有模型主要关注于识别在CXR上容易观察到的疾病，而对于使用CT训练多异常分类的需求尚未得到满足。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够从CT数据中转移知识至CXR上的方法，以改进基于CXR的疾病检测性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种三模态的知识迁移学习框架X2CT-CLIP，该框架通过将3D CT体积和放射学报告的信息转移到胸部X光图像上，实现了跨模式的知识传输。这种方法降低了模型训练时的计算负担，并且是首次尝试利用CXR实现多异常分类。&lt;h4&gt;主要发现&lt;/h4&gt;在三个标记数据集上的广泛评估显示，与现有基准方法相比，本研究的方法在跨模态检索、少样本适应和外部验证方面表现更佳。这表明，在资源有限的情况下，使用CT知识增强的胸部X光可以作为一种高效且可行的疾病检测替代方案。&lt;h4&gt;结论&lt;/h4&gt;X2CT-CLIP框架成功地将CT的知识转移到了CXR上，并展示了其在多异常分类中的潜力和应用价值，尤其是在资源受限环境中具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computed tomography (CT) is a key imaging modality for diagnosis, yet itsclinical utility is marred by high radiation exposure and long turnaroundtimes, restricting its use for larger-scale screening. Although chestradiography (CXR) is more accessible and safer, existing CXR foundation modelsfocus primarily on detecting diseases that are readily visible on the CXR.Recently, works have explored training disease classification models onsimulated CXRs, but they remain limited to recognizing a single disease typefrom CT. CT foundation models have also emerged with significantly improveddetection of pathologies in CT. However, the generalized application ofCT-derived labels on CXR has remained illusive. In this study, we proposeX2CT-CLIP, a tri-modal knowledge transfer learning framework that bridges themodality gap between CT and CXR while reducing the computational burden ofmodel training. Our approach is the first work to enable multi-abnormalityclassification in CT, using CXR, by transferring knowledge from 3D CT volumesand associated radiology reports to a CXR encoder via a carefully designedtri-modal alignment mechanism in latent space. Extensive evaluations on threemulti-label CT datasets demonstrate that our method outperformsstate-of-the-art baselines in cross-modal retrieval, few-shot adaptation, andexternal validation. These results highlight the potential of CXR, enrichedwith knowledge derived from CT, as a viable efficient alternative for diseasedetection in resource-limited settings.</description>
      <author>example@mail.com (Jianzhong You, Yuan Gao, Sangwook Kim, Chris Mcintosh)</author>
      <guid isPermaLink="false">2503.02162v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Variance-Aware Loss Scheduling for Multimodal Alignment in Low-Data Settings</title>
      <link>http://arxiv.org/abs/2503.03202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种基于模型预测统计变异性动态调整对比损失权重的方法，以提高图像文本对齐在低数据环境下的表现。&lt;h4&gt;背景&lt;/h4&gt;训练视觉语言模型用于图像和文本的对齐通常需要大规模的数据集才能达到良好的性能。而在小规模数据集中，标准的对比学习方法由于过拟合和不稳定的学习动力学而难以有效对齐模式。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的损失调度方法，以解决低数据条件下现有对比学习算法的问题，并改善图像文本检索精度。&lt;h4&gt;方法&lt;/h4&gt;采用变差感知损失调度方法，该方法根据模型在预测中的统计变异（不确定性）动态调整对比损失的权重。使用Flickr8k图说数据集的一个子集来模拟有限的数据条件。&lt;h4&gt;主要发现&lt;/h4&gt;与固定权重基线相比，所提出的方法提高了图像文本检索准确性；与其他自适应加权策略（利用输出熵和余弦相似度分布）相比较时，变差感知调度提供最佳的总体折中；在噪声注入标题和图片的压力测试下，该方法表现出更高的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这些结果强调了在低数据环境下进行多模态对齐时自适应损失加权的优势。&lt;h4&gt;翻译&lt;/h4&gt;训练视觉-语言模型以实现图像与文本的对齐通常需要大量的数据集来达到稳健的表现。在少量数据的情况下，标准对比学习方法由于过拟合和不稳定的训练动力学而难以有效对齐模式。在这篇论文中，我们提出了一种感知变异性的损失调度方法，该方法根据模型预测中的统计变异性（不确定性）动态调整对比性损失的权重。使用Flickr8k图像描述数据集的一个子集来模拟有限的数据情况，我们展示了与固定权重基线相比，我们的方法可以提高图像-文本检索的准确性。此外，我们将该方法与其他自适应加权策略进行了比较（输出熵和余弦相似度分布），发现基于变差感知调度的方法提供了最佳的整体折中。通过t-SNE可视化显示了该方法生成了更为显著的多模态嵌入。并且在噪声注入标题和图像的压力测试下，该方法表现出了更高的鲁棒性，保持较高的召回率即使在随机干扰引入的情况下也是如此。这些结果强调了自适应损失加权对于低数据环境中实现多模态对齐的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Training vision-language models for image-text alignment typically requireslarge datasets to achieve robust performance. In low-data scenarios, standardcontrastive learning can struggle to align modalities effectively due tooverfitting and unstable training dynamics. In this paper, we propose avariance-aware loss scheduling approach that dynamically adjusts the weightingof the contrastive loss based on the statistical variability (uncertainty) inthe model's alignment predictions. Using a subset of the Flickr8k image-captiondataset to simulate limited data conditions, we demonstrate that our approachimproves image-text retrieval accuracy compared to a fixed-weight baseline. Wealso compare against other adaptive weighting strategies (using output entropyand cosine similarity spread) and find that variance-aware scheduling providesthe best overall trade-off. Qualitatively, our method yields more distinctmultimodal embeddings as shown by t-SNE visualizations. Moreover, in a stresstest with noise-injected captions and images, the variance-guided loss provesmore robust, maintaining higher recall when random perturbations areintroduced. These results highlight the benefit of adaptive loss weighting formultimodal alignment in low-data regimes.</description>
      <author>example@mail.com (Sneh Pillai)</author>
      <guid isPermaLink="false">2503.03202v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Leap: Inductive Link Prediction via Learnable TopologyAugmentation</title>
      <link>http://arxiv.org/abs/2503.03331v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  published in Machine Learning, Optimization, and Data Science,  Springer Nature Switzerland&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于可学习拓扑增强的归纳链接预测方法LEAP，针对传统图神经网络在处理新节点时表达能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;链接预测是许多图机器学习下游应用的关键任务。虽然图神经网络（GNN）被广泛用于链接预测，在已有的节点间预测缺失链接的情况下尤其有效，但在实际应用场景中需要考虑新增节点的情况，即归纳设置下进行链接预测的需求日益增加。&lt;h4&gt;目的&lt;/h4&gt;为解决现有方法在新节点下的表达能力和捕捉结构信号不足的问题，提出了一种新的可学习拓扑增强的链接预测模型LEAP，该模型能够同时建模来自图结构和节点特征的归纳偏差。&lt;h4&gt;方法&lt;/h4&gt;提出了基于拓朴增强的新链接预测算法LEAP。它不同于以往仅关注节点表示的学习方法，而是通过可学习的方式为新节点提供结构性上下文，提高了表达能力。&lt;h4&gt;主要发现&lt;/h4&gt;在七种真实世界的同构和异构图上的广泛实验表明，与现有最优（SOTA）模型相比，LEAP的性能显著提升。特别是在AUC和平均精度方面分别提升了22%和17%&lt;h4&gt;结论&lt;/h4&gt;通过引入可学习拓朴增强技术，LEAP在归纳链接预测中表现出卓越的能力，为解决新节点加入图结构的问题提供了一种新的有效方法。&lt;h4&gt;翻译&lt;/h4&gt;连接预测是许多图形机器学习下游应用中的关键任务。为此，图神经网络（GNN）被广泛用于连接预测，在推导设置下尤其有效，即在已知节点之间预测缺失的链接。然而，许多现实应用场景需要归纳设置来处理新节点加入现有图的情况。因此，最近归纳连接预测吸引了大量的关注，并且多层感知机（MLP）是大多数研究中学习节点表示的流行选择。但是这些方法的表达能力有限，未能完全捕捉图形的结构信号。为此，在这项工作中，我们提出了基于可学习拓扑增强的LEAP（LEArnable toPology augmentation），这是一种归纳链接预测的方法。与以往方法不同的是，LEAP从图的结构和节点特征两方面建模归纳偏差，因此更具表现力。据我们所知，这是首次尝试通过可学习增强为新节点提供结构上下文的方法。广泛的实验表明，在七种真实世界的同构和异构图上，LEAP显著优于现有最优方法（SOTA）。改进幅度分别达到了22%的AUC和17%的平均精度。代码和数据集可在GitHub上获得 (https://github.com/AhmedESamy/LEAP/)。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-82481-4_31&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a crucial task in many downstream applications of graphmachine learning. To this end, Graph Neural Network (GNN) is a widely usedtechnique for link prediction, mainly in transductive settings, where the goalis to predict missing links between existing nodes. However, many real-lifeapplications require an inductive setting that accommodates for new nodes,coming into an existing graph. Thus, recently inductive link prediction hasattracted considerable attention, and a multi-layer perceptron (MLP) is thepopular choice of most studies to learn node representations. However, theseapproaches have limited expressivity and do not fully capture the graph'sstructural signal. Therefore, in this work we propose LEAP, an inductive linkprediction method based on LEArnable toPology augmentation. Unlike previousmethods, LEAP models the inductive bias from both the structure and nodefeatures, and hence is more expressive. To the best of our knowledge, this isthe first attempt to provide structural contexts for new nodes via learnableaugmentation in inductive settings. Extensive experiments on seven real-worldhomogeneous and heterogeneous graphs demonstrates that LEAP significantlysurpasses SOTA methods. The improvements are up to 22\% and 17\% in terms ofAUC and average precision, respectively. The code and datasets are available onGitHub (https://github.com/AhmedESamy/LEAP/)</description>
      <author>example@mail.com (Ahmed E. Samy, Zekarias T. Kefato, Sarunas Girdzijauskas)</author>
      <guid isPermaLink="false">2503.03331v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Do GFlowNets Transfer? Case Study on the Game of 24/42</title>
      <link>http://arxiv.org/abs/2503.01819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了GFlowNets在生成多样化解决方案方面的性能，并探讨其与自回归语言模型的差异。通过案例分析，在不同游戏任务中的表现发现GFlowNets难以保持解题多样性和准确性。&lt;h4&gt;背景&lt;/h4&gt;生成多样化答案对于类似人类的推理至关重要，但现有的自回归语言模型倾向于专注于提供单一准确的答案，从而限制了创造性。&lt;h4&gt;目的&lt;/h4&gt;评估GFlowNets在零样本跨任务泛化能力上的局限性，并探讨未来研究方向以提高其迁移学习能力。&lt;h4&gt;方法&lt;/h4&gt;通过微调小型和中型的大型语言模型于“24游戏”并测试它们在“42游戏”数据集的表现来分析GFlowNets的行为模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，GFlowNets难以维持跨任务时解题的多样性和准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究揭示了GFlowNets在跨任务泛化能力上的关键限制，并强调需要进一步的研究以改进其迁移学习的能力。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要全文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating diverse solutions is key to human-like reasoning, yetautoregressive language models focus on single accurate responses, limitingcreativity. GFlowNets optimize solution generation as a flow network, promisinggreater diversity. Our case study shows their limited zero-shot transferabilityby fine-tuning small and medium-sized large language models on the Game of 24and testing them on the Game of 42 datasets. Results revealed that GFlowNetsstruggle to maintain solution diversity and accuracy, highlighting keylimitations in their cross-task generalization and the need for future researchin improved transfer learning capabilities.</description>
      <author>example@mail.com (Adesh Gupta, Abhinav Kumar, Mansi Gupta, Paras Chopra)</author>
      <guid isPermaLink="false">2503.01819v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Model-Agnostic Meta-Policy Optimization via Zeroth-Order Estimation: A Linear Quadratic Regulator Perspective</title>
      <link>http://arxiv.org/abs/2503.00385v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了使用元学习来处理ergodic线性二次调节器中的不确定性和异质性的方法，提出了一种结合零阶优化技术和典型元学习方法的算法。&lt;h4&gt;背景&lt;/h4&gt;近年来，元学习作为机器学习的一个有前景的话题被提出，并在图像分类、机器人技术、计算机游戏和控制系统等领域有着重要的应用。&lt;h4&gt;目的&lt;/h4&gt;研究如何使用元学习来解决ergodic线性二次调节器中的不确定性和异质性问题。&lt;h4&gt;方法&lt;/h4&gt;将零阶优化技术与典型元学习方法相结合，提出了一个算法，该算法省略了策略Hessian的估计，并适用于学习一组相似但异质性的线性动态系统任务。所提出的元目标函数继承了一组可元学习的线性动力系统的原始成本函数的重要特性。&lt;h4&gt;主要发现&lt;/h4&gt;提供了关于精确梯度下降过程的稳定性以及收敛保证，通过分析元目标函数的梯度有界性和局部平滑性来证明该算法的有效性，并给出了理论保证的样本复杂度条件。最后提供了一个数值例子以支持这一观点。&lt;h4&gt;结论&lt;/h4&gt;所提出的算法能够处理异质性的线性动态系统集合，同时在优化过程中不需要投影到可行集上，并且通过理论分析提供了稳定性、收敛性和小梯度估计误差保证。&lt;h4&gt;翻译&lt;/h4&gt;元学习近年来被提出作为机器学习的有前景话题，在图像分类、机器人技术、计算机游戏和控制系统等领域有着广泛应用。本文探讨了使用元学习处理ergodic线性二次调节器中的不确定性和异质性的方法，提出了结合零阶优化技术和典型元学习方法的新算法，该算法可以应用于学习一组相似但异质性的线性动态系统，并提供关于稳定性、收敛性和小梯度估计误差的理论保证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning has been proposed as a promising machine learning topic inrecent years, with important applications to image classification, robotics,computer games, and control systems. In this paper, we study the problem ofusing meta-learning to deal with uncertainty and heterogeneity in ergodiclinear quadratic regulators. We integrate the zeroth-order optimizationtechnique with a typical meta-learning method, proposing an algorithm thatomits the estimation of policy Hessian, which applies to tasks of learning aset of heterogeneous but similar linear dynamic systems. The inducedmeta-objective function inherits important properties of the original costfunction when the set of linear dynamic systems are meta-learnable, allowingthe algorithm to optimize over a learnable landscape without projection ontothe feasible set. We provide stability and convergence guarantees for the exactgradient descent process by analyzing the boundedness and local smoothness ofthe gradient for the meta-objective, which justify the proposed algorithm withgradient estimation error being small. We provide the sample complexityconditions for these theoretical guarantees, as well as a numerical example atthe end to corroborate this perspective.</description>
      <author>example@mail.com (Yunian Pan, Tao Li, Quanyan Zhu)</author>
      <guid isPermaLink="false">2503.00385v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MR-EIT: Multi-Resolution Reconstruction for Electrical Impedance Tomography via Data-Driven and Unsupervised Dual-Mode Neural Networks</title>
      <link>http://arxiv.org/abs/2503.00762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于电气阻抗断层成像(EIT)的多分辨率重建方法MR-EIT，可以在监督学习和无监督学习模式下操作。&lt;h4&gt;背景&lt;/h4&gt;当前EIT图像重建面临着如何在有限的数据输入情况下实现高精度、低噪声影响的挑战。现有的方法往往难以同时满足不同分辨率数据的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够处理不同规模输入序列并在低分辨率数据上生成高质量高分辨率EIT图像的方法。&lt;h4&gt;方法&lt;/h4&gt;MR-EIT结合了有序特征提取模块和无序坐标特征表达模块，前者通过预训练实现从电压到二维导电特性的映射；后者利用对称函数和局部特征提取机制实现了独立于输入序列顺序的多分辨率重建。在数据驱动模式下，该算法可以先进行两阶段的预训练，再进行联合训练以生成高分辨率图像。&lt;h4&gt;主要发现&lt;/h4&gt;MR-EIT不仅在模拟实验中表现出色，在无监督学习模式下的真实水箱实验中也展示了出色的鲁棒性和高效的超分辨率重建能力。特别是，在无监督学习模式下，该算法能显著减少迭代次数并提高成像质量。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在结构相似性(SSIM)和相对图像误差(RIE)方面，MR-EIT优于其他比较方法，尤其是在无监督学习模式下表现突出。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，上述内容为其翻译及总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a multi-resolution reconstruction method for ElectricalImpedance Tomography (EIT), referred to as MR-EIT, which is capable ofoperating in both supervised and unsupervised learning modes. MR-EIT integratesan ordered feature extraction module and an unordered coordinate featureexpression module. The former achieves the mapping from voltage totwo-dimensional conductivity features through pre-training, while the latterrealizes multi-resolution reconstruction independent of the order and size ofthe input sequence by utilizing symmetric functions and local featureextraction mechanisms. In the data-driven mode, MR-EIT reconstructshigh-resolution images from low-resolution data of finite element meshesthrough two stages of pre-training and joint training, and demonstratesexcellent performance in simulation experiments. In the unsupervised learningmode, MR-EIT does not require pre-training data and performs iterativeoptimization solely based on measured voltages to rapidly achieve imagereconstruction from low to high resolution. It shows robustness to noise andefficient super-resolution reconstruction capabilities in both simulation andreal water tank experiments. Experimental results indicate that MR-EIToutperforms the comparison methods in terms of Structural Similarity (SSIM) andRelative Image Error (RIE), especially in the unsupervised learning mode, whereit can significantly reduce the number of iterations and improve imagereconstruction quality.</description>
      <author>example@mail.com (Fangming Shi, Jinzhen Liu, Xiangqian Meng, Yapeng Zhou, Hui Xiong)</author>
      <guid isPermaLink="false">2503.00762v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Hyperspectral Image Restoration and Super-resolution with Physics-Aware Deep Learning for Biomedical Applications</title>
      <link>http://arxiv.org/abs/2503.02908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种基于深度学习的方法，用于提高高光谱成像的空间分辨率和速度，同时保持生物样本的完整性。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像是一个强大的生物成像工具，能够揭示材料内在属性的新见解。然而，这种增强对比度是以系统复杂性为代价的，并且在空间分辨率、光谱分辨率和成像速度之间存在固有的权衡。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些限制，本文提出了一种不需要额外训练数据即可恢复和提高像素分辨率的方法。&lt;h4&gt;方法&lt;/h4&gt;该深度学习模型经过微调，可以进行16倍的像素超分辨率增强并提升12倍的成像速度。这种方法不依赖于先验知识，并且使用与成像模型对齐的度量标准进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过应用于合成和实验数据，证明了该模型能够保持生物样本的完整性，不会丢失或产生虚假特征。此外，在唐氏综合症患者中发现了以前无法检测到的代谢变化。&lt;h4&gt;结论&lt;/h4&gt;这项工作不仅提供了一个提高高光谱成像质量和速度的有效工具，还提供了对模型物理机制的理解，并为未来改进奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要：高光谱成像是一个强大的生物成像工具，能够揭示材料内在属性的新见解。然而，这种增强对比度是以系统复杂性为代价的，并且在空间分辨率、光谱分辨率和成像速度之间存在固有的权衡。为了克服这些限制，我们提出了一种基于深度学习的方法，在不使用先验知识的情况下进行像素恢复和增强。经过与成像模型对齐的标准微调，我们的物理感知方法实现了16倍的像素超分辨率提升和12倍的成像速度提升，并不需要额外的迁移学习训练数据。应用于五种不同类型样品的合成和实验数据，我们证明了该模型保持生物完整性，确保没有特征丢失或产生虚幻特征。我们也具体展示了该模型揭示唐氏综合症相关代谢变化的能力，这些变化在其他情况下无法检测到。此外，我们提供了关于模型内部工作的物理见解，为未来可能超越仪器限制的改进铺平道路，并且所有方法都作为开源软件发布在GitHub上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging is a powerful bioimaging tool which can uncover novelinsights, thanks to its sensitivity to the intrinsic properties of materials.However, this enhanced contrast comes at the cost of system complexity,constrained by an inherent trade-off between spatial resolution, spectralresolution, and imaging speed. To overcome this limitation, we present a deeplearning-based approach that restores and enhances pixel resolutionpost-acquisition without any a priori knowledge. Fine-tuned using metricsaligned with the imaging model, our physics-aware method achieves a 16X pixelsuper-resolution enhancement and a 12X imaging speedup without the need ofadditional training data for transfer learning. Applied to both synthetic andexperimental data from five different sample types, we demonstrate that themodel preserves biological integrity, ensuring no features are lost orhallucinated. We also concretely demonstrate the model's ability to revealdisease-associated metabolic changes in Downs syndrome that would otherwiseremain undetectable. Furthermore, we provide physical insights into the innerworkings of the model, paving the way for future refinements that couldpotentially surpass instrumental limits in an explainable manner. All methodsare available as open-source software on GitHub.</description>
      <author>example@mail.com (Yuchen Xiang, Zhaolu Liu, Monica Emili Garcia-Segura, Daniel Simon, Boxuan Cao, Vincen Wu, Kenneth Robinson, Yu Wang, Ronan Battle, Robert T. Murray, Xavier Altafaj, Luca Peruzzotti-Jametti, Zoltan Takats)</author>
      <guid isPermaLink="false">2503.02908v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Exploring the Potential of Large Language Models as Predictors in Dynamic Text-Attributed Graphs</title>
      <link>http://arxiv.org/abs/2503.03258v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;随着大语言模型（LLMs）的兴起，人们对于图基础模型（GFMs）在基于图的任务中的兴趣日益增加。通过利用LLMs作为预测器，GFMs展示了跨越各种任务和数据集的强大泛化能力。&lt;h4&gt;背景&lt;/h4&gt;现有的关于使用LLMs作为预测器的研究主要集中在静态图上，动态图预测的潜力尚未被探索。&lt;h4&gt;目的&lt;/h4&gt;这项工作首次尝试将LLMs应用于动态图上的预测任务，并识别出两个关键挑战：处理大规模历史数据时上下文长度限制和领域特征差异性。&lt;h4&gt;方法&lt;/h4&gt;为了应对这些挑战，我们提出了GraphAgent-Dynamic (GAD)框架，这是一个多代理系统，利用协同工作的大语言模型。与使用单个LLM作为预测器不同，GAD整合了全局和局部摘要代理来生成特定领域的知识，增强其跨域迁移能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，GAD的性能与完全监督的图神经网络相当甚至更优，并且不需要特定数据集的训练。此外，我们讨论了针对LLM基础预测器改进的潜在策略，例如对LLMs进行数据集特定的微调。&lt;h4&gt;结论&lt;/h4&gt;通过为不同任务制定定制化策略，该研究提供了对未来基于LLM预测器设计的新见解。&lt;h4&gt;翻译&lt;/h4&gt;随着大语言模型（LLMs）的发展，人们对图基础模型（GFMs）在基于图的任务中的兴趣日益增加。利用LLMs作为预测器的GFMs已经在各种任务和数据集上展示了出色的泛化能力。然而，现有研究主要关注静态图上的预测问题，而动态图预测方面尚未被充分探索。本文首次尝试使用LLM进行动态图上的预测，并提出GraphAgent-Dynamic (GAD)框架来应对挑战。实验结果表明，该框架的性能优于或等同于完全监督下的图神经网络，同时提供了对基于LLMs的未来设计的新见解和改进策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rise of large language models (LLMs), there has been growinginterest in Graph Foundation Models (GFMs) for graph-based tasks. By leveragingLLMs as predictors, GFMs have demonstrated impressive generalizability acrossvarious tasks and datasets. However, existing research on LLMs as predictorshas predominantly focused on static graphs, leaving their potential in dynamicgraph prediction unexplored. In this work, we pioneer using LLMs for predictivetasks on dynamic graphs. We identify two key challenges: the constraintsimposed by context length when processing large-scale historical data and thesignificant variability in domain characteristics, both of which complicate thedevelopment of a unified predictor. To address these challenges, we propose theGraphAgent-Dynamic (GAD) Framework, a multi-agent system that leveragescollaborative LLMs. In contrast to using a single LLM as the predictor, GADincorporates global and local summary agents to generate domain-specificknowledge, enhancing its transferability across domains. Additionally,knowledge reflection agents enable adaptive updates to GAD's knowledge,maintaining a unified and self-consistent architecture. In experiments, GADdemonstrates performance comparable to or even exceeds that of full-supervisedgraph neural networks without dataset-specific training. Finally, to enhancethe task-specific performance of LLM-based predictors, we discuss potentialimprovements, such as dataset-specific fine-tuning to LLMs. By developingtailored strategies for different tasks, we provide new insights for the futuredesign of LLM-based predictors.</description>
      <author>example@mail.com (Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong)</author>
      <guid isPermaLink="false">2503.03258v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Improving internal cluster quality evaluation in noisy Gaussian mixtures</title>
      <link>http://arxiv.org/abs/2503.00379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的特征重要性重标(FIR)方法，用于改进内部聚类验证，并通过实验展示了其在处理高维或噪声数据集时的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的内部聚类验证指标（如平均轮廓宽度、Calinski-Harabasz和Davies-Bouldin指数）容易受到特征相关性的干扰，导致在无标签的复杂数据集中评估结果不可靠。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来增强基于内部验证指标的聚类质量评价，在没有外部标准的情况下提高算法的有效性。&lt;h4&gt;方法&lt;/h4&gt;FIR方法通过根据每个特征的数据分散情况调整其贡献值，系统地降低噪声特征的影响，从而使得聚类结果更加清晰和紧凑。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，无论是在有噪声还是不相关特征存在的数据集中，FIR都能显著提高内部验证指标与真实标签之间的关联性，并且在重叠较大的情况下仍能保持较高性能稳定性。&lt;h4&gt;结论&lt;/h4&gt;FIR作为一种改进的聚类评价技术，在没有标注的数据上进行无监督学习时具有很大的应用价值和潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Clustering is a fundamental technique in machine learning and data analysis,widely used across various domains. Internal clustering validation measures,such as the Average Silhouette Width, Calinski-Harabasz, and Davies-Bouldinindices, play a crucial role in assessing clustering quality when externalground truth labels are unavailable. However, these measures can be affected byfeature relevance, potentially leading to unreliable evaluations inhigh-dimensional or noisy data sets.  In this paper, we introduce a Feature Importance Rescaling (FIR) methoddesigned to enhance internal clustering validation by adjusting featurecontributions based on their dispersion. Our method systematically attenuatesnoise features making clustering compactness and separation clearer, and byconsequence aligning internal validation measures more closely with the groundtruth. Through extensive experiments on synthetic data sets under differentconfigurations, we demonstrate that FIR consistently improves the correlationbetween internal validation indices and the ground truth, particularly insettings with noisy or irrelevant features.  The results show that FIR increases the robustness of clustering evaluation,reduces variability in performance across different data sets, and remainseffective even when clusters exhibit significant overlap. These findingshighlight the potential of FIR as a valuable enhancement for internalclustering validation, making it a practical tool for unsupervised learningtasks where labelled data is not available.</description>
      <author>example@mail.com (Renato Cordeiro de Amorim, Vladimir Makarenkov)</author>
      <guid isPermaLink="false">2503.00379v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Abn-BLIP: Abnormality-aligned Bootstrapping Language-Image Pre-training for Pulmonary Embolism Diagnosis and Report Generation from CTPA</title>
      <link>http://arxiv.org/abs/2503.02034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Abn-BLIP模型，该模型通过异常对齐技术提高了CTPA扫描的解读准确性和放射学报告的质量。&lt;h4&gt;背景&lt;/h4&gt;医学影像在现代医疗中扮演着重要角色，特别是用于诊断肺栓塞和胸部疾病的计算机断层摄影血管造影（CTPA）。然而，解析CTPA扫描并生成准确的放射学报告是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种改进的诊断模型，以提高对CTPA扫描异常检测、减少遗漏发现以及生成结构化报告的能力。&lt;h4&gt;方法&lt;/h4&gt;引入了Abn-BLIP（异常一致性自助学习语言-图像预训练）模型，该模型利用可学查询和跨模态注意机制来提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有技术相比，Abn-BLIP在准确性和临床相关性方面超越了最先进的医学视觉-语言模型和3D报告生成方法。&lt;h4&gt;结论&lt;/h4&gt;这些成果展示了多模式学习策略在改善放射学报告方面的潜力。模型源代码可从GitHub获取（https://github.com/zzs95/abn-blip）。&lt;h4&gt;翻译&lt;/h4&gt;医疗成像在现代医疗服务中发挥着关键作用，CTPA是诊断肺栓塞和其他胸部疾病的重要工具。然而，解读CTPA扫描并生成准确的放射学报告依然是一项挑战。为此，我们提出了一种名为Abn-BLIP（异常一致性自助学习语言-图像预训练）的高级诊断模型。该模型利用可学查询和跨模态注意机制来检测异常、减少遗漏发现，并生成结构化报告。实验表明，与现有方法相比，Abn-BLIP在准确性和临床相关性方面超过了最先进的医学视觉-语言模型和3D报告生成技术。这些结果强调了结合多模式学习策略以提高放射学报告能力的潜力。源代码可在GitHub上找到（https://github.com/zzs95/abn-blip）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Medical imaging plays a pivotal role in modern healthcare, with computedtomography pulmonary angiography (CTPA) being a critical tool for diagnosingpulmonary embolism and other thoracic conditions. However, the complexity ofinterpreting CTPA scans and generating accurate radiology reports remains asignificant challenge. This paper introduces Abn-BLIP (Abnormality-alignedBootstrapping Language-Image Pretraining), an advanced diagnosis model designedto align abnormal findings to generate the accuracy and comprehensiveness ofradiology reports. By leveraging learnable queries and cross-modal attentionmechanisms, our model demonstrates superior performance in detectingabnormalities, reducing missed findings, and generating structured reportscompared to existing methods. Our experiments show that Abn-BLIP outperformsstate-of-the-art medical vision-language models and 3D report generationmethods in both accuracy and clinical relevance. These results highlight thepotential of integrating multimodal learning strategies for improving radiologyreporting. The source code is available at https://github.com/zzs95/abn-blip.</description>
      <author>example@mail.com (Zhusi Zhong, Yuli Wang, Lulu Bi, Zhuoqi Ma, Sun Ho Ahn, Christopher J. Mullin, Colin F. Greineder, Michael K. Atalay, Scott Collins, Grayson L. Baird, Cheng Ting Lin, Webster Stayman, Todd M. Kolb, Ihab Kamel, Harrison X. Bai, Zhicheng Jiao)</author>
      <guid isPermaLink="false">2503.02034v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosis of Patients with Viral, Bacterial, and Non-Pneumonia Based on Chest X-Ray Images Using Convolutional Neural Networks</title>
      <link>http://arxiv.org/abs/2503.02906v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于胸部X光图像的肺炎分类决策支持系统，利用迁移学习技术结合卷积神经网络模型，并引入了特征选择和降维方法，实现了对无肺炎患者、病毒性肺炎患者以及细菌性肺炎患者的准确区分。&lt;h4&gt;背景&lt;/h4&gt;世界卫生组织指出，肺炎是每年导致大量死亡的原因之一。因此需要开发有效的辅助决策系统以提高诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;通过使用预训练的卷积神经网络模型进行迁移学习，并结合特征选择和降维技术以及支持向量机分类器，构建一个能够准确区分无肺炎患者与病毒性或细菌性肺炎患者的决策支持系统。&lt;h4&gt;方法&lt;/h4&gt;实验采用了迁移学习（TL）技术利用预先训练好的卷积神经网络（CNN）模型处理胸部X光图像，并结合Relief和Chi-square降维技术以及支持向量机（SVM）进行分类。&lt;h4&gt;主要发现&lt;/h4&gt;对于区分无肺炎患者与任何类型的肺炎患者，系统的准确率为91.02%，精度为97.73%，召回率为98.03%，F1得分为97.88%。对于区分病毒性肺炎和细菌性肺炎患者的分类任务中，系统达到了93.66%的准确率、94.26%的精度、92.66%的召回率及93.45%的F1得分。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，所提出的基于迁移学习和特征选择方法的支持向量机分类器在区分无肺炎与病毒性或细菌性肺炎患者的任务中表现出优异性能。该系统可以为临床医生提供有价值的辅助决策信息。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文的中文译文：根据世界卫生组织的数据，每年有大量的死亡是由肺炎引起的疾病导致的。针对这一问题，提出了一种基于胸部X光图像进行患者分类的支持决策系统，能够将没有肺炎、病毒性肺炎和细菌性肺炎的患者区分开来。通过使用预训练的卷积神经网络模型实施迁移学习，并结合Relief与Chi-square方法作为降维技术以及支持向量机进行分类实现了这一目标。一系列实验的结果表明建立了一个区分无肺炎和肺炎患者的模型，其准确性为91.02%，精确度为97.73%，召回率为98.03%，F1得分为97.88%。此外，在区分病毒性肺炎与细菌性肺炎患者方面的准确率达到了93.66%，精度为94.26%，召回率为92.66%，F1得分为93.45%&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; According to the World Health Organization (WHO), pneumonia is a disease thatcauses a significant number of deaths each year. In response to this issue, thedevelopment of a decision support system for the classification of patientsinto those without pneumonia and those with viral or bacterial pneumonia isproposed. This is achieved by implementing transfer learning (TL) usingpre-trained convolutional neural network (CNN) models on chest x-ray (CXR)images. The system is further enhanced by integrating Relief and Chi-squaremethods as dimensionality reduction techniques, along with support vectormachines (SVM) for classification. The performance of a series of experimentswas evaluated to build a model capable of distinguishing between patientswithout pneumonia and those with viral or bacterial pneumonia. The obtainedresults include an accuracy of 91.02%, precision of 97.73%, recall of 98.03%,and an F1 Score of 97.88% for discriminating between patients without pneumoniaand those with pneumonia. In addition, accuracy of 93.66%, precision of 94.26%,recall of 92.66%, and an F1 Score of 93.45% were achieved for discriminatingbetween patients with viral pneumonia and those with bacterial pneumonia.</description>
      <author>example@mail.com (Carlos Arizmendi, Jorge Pinto, Alejandro Arboleda, Hernando González)</author>
      <guid isPermaLink="false">2503.02906v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Common indicators hurt armed conflict prediction</title>
      <link>http://arxiv.org/abs/2503.00265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究使用非洲地区的精细冲突数据，结合气候、地理、基础设施、经济、人口统计数据和人口结构等因素，利用无监督学习模型发现了三种主要的冲突类型。&lt;h4&gt;背景&lt;/h4&gt;现有研究未能充分探究大规模冲突与中小型冲突之间的区别及其形成因素。本研究试图通过分析详细的数据集来填补这一知识空白。&lt;h4&gt;目的&lt;/h4&gt;探索并分类不同规模的冲突，并探讨这些分类对预测冲突强度和持续时间的影响，同时评估常见指标在预测中的实用性。&lt;h4&gt;方法&lt;/h4&gt;利用无监督学习模型对非洲地区的精细冲突数据进行分析。该模型考虑了气候、地理、基础设施、经济和社会人口结构等多个维度的信息。&lt;h4&gt;主要发现&lt;/h4&gt;{'冲突类型': ['重大动乱', '局部冲突', '偶发和溢出事件'], '重大动乱特征': '在人口密集且拥有发达基础设施的平坦河岸地区传播', '局部冲突特征': '中等人口密度区域，经济与地理多样性高，通常局限于国境之内', '偶发性和溢出事件特征': '规模小，多发生在低人口密度、缺乏基础设施和经济条件差的地区'}&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，明确区分不同类型的冲突会降低对冲突强度和其他尺寸测量值（如死亡人数和持续时间）预测的准确性。此外，还开发了一种基于经验且自下而上的方法来识别冲突类型。&lt;h4&gt;翻译&lt;/h4&gt;大规模冲突与中小型冲突在哪些方面有所不同？为了解答这个问题，我们利用了非洲地区的精细冲突数据，并将其映射到气候、地理、基础设施、经济、人口统计数据和人口结构上。通过无监督学习模型，我们发现了三种主要的冲突类型，即“重大动乱”、“局部冲突”和“偶发性和溢出事件”。重大动乱多在人口密集且拥有发达基础设施的平坦河岸地区发生；局部冲突则发生在中等人口密度区域，经济与地理多样性高，并通常局限于国境之内。而偶发性及溢出事件规模较小，常见于低人口密度、缺乏基础设施和贫困地区的环境中。三种类型按顺序分层为因素，分别强调了人口、基础设施、经济状况和地理位置作为最具有区分性的指标。明确冲突的类型会降低对诸如死亡人数、冲突持续时间等冲突强度预测的准确性，并促使我们注意常用指标在预测中的有限效用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Are big conflicts different from small or medium size conflicts? To answerthis question, we leverage fine-grained conflict data, which we map to climate,geography, infrastructure, economics, raw demographics, and demographiccomposition in Africa. With an unsupervised learning model, we find threeoverarching conflict types representing ``major unrest,'' ``local conflict,''and ``sporadic and spillover events.'' Major unrest predominantly propagatesaround densely populated areas with well-developed infrastructure and flat,riparian geography. Local conflicts are in regions of median populationdensity, are diverse socio-economically and geographically, and are oftenconfined within country borders. Finally, sporadic and spillover conflictsremain small, often in low population density areas, with little infrastructureand poor economic conditions. The three types stratify into a hierarchy offactors that highlights population, infrastructure, economics, and geography,respectively, as the most discriminative indicators. Specifying conflict typenegatively impacts the predictability of conflict intensity such as fatalities,conflict duration, and other measures of conflict size. The competitive effectis a general consequence of weak statistical dependence. Hence, we develop anempirical and bottom-up methodology to identify conflict types, knowledge ofwhich can hurt predictability and cautions us about the limited utility ofcommonly available indicators.</description>
      <author>example@mail.com (Niraj Kushwaha, Woi Sok Oh, Shlok Shah, Edward D. Lee)</author>
      <guid isPermaLink="false">2503.00265v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DeepSuM: Deep Sufficient Modality Learning Framework</title>
      <link>http://arxiv.org/abs/2503.01728v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的多模态选择框架，该框架独立学习每种模式的表示，并评估其在特定情境下的重要性。&lt;h4&gt;背景&lt;/h4&gt;多模态学习是开发稳健的学习模型的关键方法，应用于多媒体、机器人技术、大型语言模型和医疗保健等领域。不同模式的成本与资源需求不一，因此需要有效选择以平衡性能提升和资源消耗之间的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来优化多模态集成和选择过程。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一种独立学习每种模式表示的方法，并在此基础上评估了各模式的重要性，同时开发出适应不同特征的编码器。&lt;h4&gt;主要发现&lt;/h4&gt;此框架能够提高多模态学习效率与效果。&lt;h4&gt;结论&lt;/h4&gt;通过独立学习和优化多模式表示可以有效提升整体系统性能并节约资源。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习已成为发展稳健的学习模型的关键方法，其应用范围包括多媒体、机器人技术、大型语言模型以及医疗保健等领域。由于不同模式的成本与资源需求不一，因此高效利用各种模式成为了关键问题。这强调了有效选择模式的重要性，以平衡性能提升和资源消耗之间的关系。在这项研究中，我们提出了一种新的框架来优化多模态集成和选择过程，该方法独立学习每种模式的表示，并评估其在特定情境下的重要性，从而开发出适应不同特征的编码器，并促进具有独特特性的模式联合分析。我们的框架旨在通过优化模式整合与选择提高多模态学习效率与效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning has become a pivotal approach in developing robustlearning models with applications spanning multimedia, robotics, large languagemodels, and healthcare. The efficiency of multimodal systems is a criticalconcern, given the varying costs and resource demands of different modalities.This underscores the necessity for effective modality selection to balanceperformance gains against resource expenditures. In this study, we propose anovel framework for modality selection that independently learns therepresentation of each modality. This approach allows for the assessment ofeach modality's significance within its unique representation space, enablingthe development of tailored encoders and facilitating the joint analysis ofmodalities with distinct characteristics. Our framework aims to enhance theefficiency and effectiveness of multimodal learning by optimizing modalityintegration and selection.</description>
      <author>example@mail.com (Zhe Gao, Jian Huang, Ting Li, Xueqin Wang)</author>
      <guid isPermaLink="false">2503.01728v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Measurement noise scaling laws for cellular representation learning</title>
      <link>http://arxiv.org/abs/2503.02726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;深度学习扩展规律预测模型和数据集规模增大时性能如何提升。本文识别出数据中的测量噪声作为另一个影响性能的尺度，受独立的对数法则支配。&lt;h4&gt;背景&lt;/h4&gt;深度学习的扩展规则通常关注于模型大小及训练数据集的尺寸变化，而本文则引入了关于生物单细胞基因组数据中由于分子采样不足引起的主导性测量噪音来源的研究。&lt;h4&gt;目的&lt;/h4&gt;为了量化细胞表示模型质量，提出了一个信息论度量标准，并研究该度量与样本深度的关系。&lt;h4&gt;方法&lt;/h4&gt;通过多个模型类型和不同数据集验证了单一的定量关系。此外，还从简单的高斯噪声模型推导出了这种关系的形式。&lt;h4&gt;主要发现&lt;/h4&gt;测量噪音影响性能并遵循对数法则；提出了一个信息理论指标来衡量细胞表示模型的质量，并表明该指标随采样深度的变化而变化；发现了关于不同类型成像噪音的图像分类模型中存在相同的关系，暗示了测量噪声缩放可能是一个普遍的现象。&lt;h4&gt;结论&lt;/h4&gt;测量噪音可以作为生成和管理用于深度学习模型的数据的重要指南，特别是在数据质量在不同数据集间差异显著的领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning scaling laws predict how performance improves with increasedmodel and dataset size. Here we identify measurement noise in data as anotherperformance scaling axis, governed by a distinct logarithmic law. We focus onrepresentation learning models of biological single cell genomic data, where adominant source of measurement noise is due to molecular undersampling. Weintroduce an information-theoretic metric for cellular representation modelquality, and find that it scales with sampling depth. A single quantitativerelationship holds across several model types and across several datasets. Weshow that the analytical form of this relationship can be derived from a simpleGaussian noise model, which in turn provides an intuitive interpretation forthe scaling law. Finally, we show that the same relationship emerges in imageclassification models with respect to two types of imaging noise, suggestingthat measurement noise scaling may be a general phenomenon. Scaling with noisecan serve as a guide in generating and curating data for deep learning models,particularly in fields where measurement quality can vary dramatically betweendatasets.</description>
      <author>example@mail.com (Gokul Gowri, Peng Yin, Allon M. Klein)</author>
      <guid isPermaLink="false">2503.02726v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>An Efficient Approach to Detecting Lung Nodules Using Swin Transformer</title>
      <link>http://arxiv.org/abs/2503.01592v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19th Iranian Conference on Intelligent Systems (ICIS), IEEE, 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的基于2D CT切片的肺癌结节检测模型，以提高早期诊断效率。&lt;h4&gt;背景&lt;/h4&gt;肺癌是癌症致死率最高的疾病之一，而肺结节是常见的肺癌指示物。现有的多种肺结节检测模型在效率方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种更为高效的肺结节检测方法，减少计算量和复杂性，并提升小结节的检测能力。&lt;h4&gt;方法&lt;/h4&gt;采用Swin Transformer（轻量版）结合特征金字塔网络，利用迁移学习加速训练过程。该模型旨在继承视觉变换器的优势同时保持较低的计算成本。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的模型在小结节的mAP和mAR上分别超出当前最佳方法1.3%和1.6%，整体表现优异，分别为94.7%mAP和94.9%mAR。&lt;h4&gt;结论&lt;/h4&gt;所提方法能够有效地提高肺部结节检测效率与精度，有助于早期肺癌诊断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICIS64839.2024.10887472&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Lung cancer has the highest rate of cancer-caused deaths, and early-stagediagnosis could increase the survival rate. Lung nodules are common indicatorsof lung cancer, making their detection crucial. Various lung nodule detectionmodels exist, but many lack efficiency. Hence, we propose a more efficientapproach by leveraging 2D CT slices, reducing computational load and complexityin training and inference. We employ the tiny version of Swin Transformer tobenefit from Vision Transformers (ViT) while maintaining low computationalcost. A Feature Pyramid Network is added to enhance detection, particularly forsmall nodules. Additionally, Transfer Learning is used to accelerate training.Our experimental results show that the proposed model outperformsstate-of-the-art methods, achieving higher mAP and mAR for small nodules by1.3% and 1.6%, respectively. Overall, our model achieves the highest mAP of94.7% and mAR of 94.9%.</description>
      <author>example@mail.com (Saeed Shakuri, Alireza Rezvanian)</author>
      <guid isPermaLink="false">2503.01592v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Shared Encoder Approach to Multimodal Representation Learning</title>
      <link>http://arxiv.org/abs/2503.01654v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;多模态表示学习在处理和融合文本、图像等多样化数据方面展现出巨大潜力，特别是在医学领域可以得到显著应用。然而，缺乏配对的多模态数据以及依赖专有或预训练编码器的问题带来了挑战。&lt;h4&gt;背景&lt;/h4&gt;多模态表示学习能够帮助模型更好地理解和处理不同类型的数据（如文本和图像），从而提高性能。在医学领域中，这种技术可以获得重大应用，但当前存在缺乏足够的多模态配对数据和过度依赖于专有或预训练编码器的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于医疗领域的共享编码器框架，以解决缺乏配对的多模态数据以及依赖专有或预训练编码器带来的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一种单一编码器参数集，在不同模式之间共享，并通过可学习的模态特征进行增强。这种方法能够适应不同的医疗领域应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;实证结果显示，与独立的特定模态编码器相比，本研究提出的共享编码器框架在数据受限的情况下具有更好的泛化能力。尤其是当训练样本较少时，性能提升尤为显著。&lt;h4&gt;结论&lt;/h4&gt;该工作表明，在医学应用的实际场景中（特别是面对有限的数据条件），所提出的共享编码器框架比现有的特定模态编码器更加高效且具备更高的可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;多模态表示学习已经显示出了处理多样化数据模式的巨大潜力，如文本和图像，并能改善理解和性能。虽然医学领域可以从这种范式中获益良多，但缺乏配对的多模态数据以及依赖专有或预训练编码器的问题带来了显著挑战。本研究提出了一种适用于医疗领域的共享编码器框架，该方法采用单一集编码参数在不同模式间共用，并辅以可学习的模态特征增强。实验结果表明，在数据受限的情况下，本框架能够超越特定模态编码器，特别是在较少训练样本时性能提升尤为显著。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/vectorinstitute/shared_encoder&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning has demonstrated remarkable potential inenabling models to process and integrate diverse data modalities, such as textand images, for improved understanding and performance. While the medicaldomain can benefit significantly from this paradigm, the scarcity of pairedmultimodal data and reliance on proprietary or pretrained encoders posesignificant challenges. In this work, we present a shared encoder framework formultimodal representation learning tailored to the medical domain. Our approachemploys a single set of encoder parameters shared across modalities, augmentedwith learnable modality features. Empirical results demonstrate that our sharedencoder idea achieves superior performance compared to separatemodality-specific encoders, demonstrating improved generalization indata-constrained settings. Notably, the performance gains are more pronouncedwith fewer training examples, underscoring the efficiency of our shared encoderframework for real-world medical applications with limited data. Our code andexperiment setup are available athttps://github.com/VectorInstitute/shared_encoder.</description>
      <author>example@mail.com (Shuvendu Roy, Franklin Ogidi, Ali Etemad, Elham Dolatabadi, Arash Afkanpour)</author>
      <guid isPermaLink="false">2503.01654v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Distilled Prompt Learning for Incomplete Multimodal Survival Prediction</title>
      <link>http://arxiv.org/abs/2503.01653v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;多模态数据集（包括病理图像和基因组信息）在精准生存预测中的应用广泛。然而，尽管近年来多模态生存模型取得了进展，在临床环境中收集完整的多模态数据仍然是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对因缺乏完整模态导致的局限性问题，本文提出了一种利用大型语言模型（LLMs）鲁棒性的蒸馏提示学习框架（DisPro），以弥补缺失模态的信息。&lt;h4&gt;方法&lt;/h4&gt;提出的框架包含两个阶段：首先通过单模态提示将各模态的知识分布提炼出来；接着在多模态提示下，使用现有数据作为提示来推断丢失的数据，并注入第一阶段获取的单一模式知识以补充特定模态信息。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验展示了该方法在处理各种缺失场景下的优越性。通过两个阶段的提示学习，能够更好地填补因缺少某些模态而导致的信息缺口。&lt;h4&gt;结论&lt;/h4&gt;这项研究提供了一种新的策略来改善多模态生存预测模型中的数据完整性问题，并为进一步的应用提供了可能的方向。&lt;h4&gt;翻译&lt;/h4&gt;将病理图像和基因组信息等多模态数据应用于精确的生存预测中非常普遍。尽管最近在多模态生存模型方面有所进展，但在临床环境中收集完整的多模式数据仍然是一项挑战。目前处理不完整模式的方法通常只弥补了缺失模式知识的一部分，并不足以完全补偿损失的知识。为了应对这一问题，我们提出了一种基于大型语言模型（LLMs）的蒸馏提示学习框架(DisPro)，该框架利用两个阶段的提示来补充丢失模态的全面信息。第一阶段是单模态提示，它提炼了各模式中的知识分布；第二阶段为多模态提示，使用现有数据作为提示来推断缺失的数据，并且在多模态推理中注入第一阶段获得的单一模态知识，以补偿特定模态的信息损失。广泛的实验验证显示所提出的框架在各种不完整场景下的优越性。相关代码已公开（见 https://github.com/Innse/DisPro）&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of multimodal data including pathology images and geneprofiles is widely applied in precise survival prediction. Despite recentadvances in multimodal survival models, collecting complete modalities formultimodal fusion still poses a significant challenge, hindering theirapplication in clinical settings. Current approaches tackling incompletemodalities often fall short, as they typically compensate for only a limitedpart of the knowledge of missing modalities. To address this issue, we proposea Distilled Prompt Learning framework (DisPro) to utilize the strong robustnessof Large Language Models (LLMs) to missing modalities, which employs two-stageprompting for compensation of comprehensive information for missing modalities.In the first stage, Unimodal Prompting (UniPro) distills the knowledgedistribution of each modality, preparing for supplementing modality-specificknowledge of the missing modality in the subsequent stage. In the second stage,Multimodal Prompting (MultiPro) leverages available modalities as prompts forLLMs to infer the missing modality, which provides modality-common information.Simultaneously, the unimodal knowledge acquired in the first stage is injectedinto multimodal inference to compensate for the modality-specific knowledge ofthe missing modality. Extensive experiments covering various missing scenariosdemonstrated the superiority of the proposed method. The code is available athttps://github.com/Innse/DisPro.</description>
      <author>example@mail.com (Yingxue Xu, Fengtao Zhou, Chenyu Zhao, Yihui Wang, Can Yang, Hao Chen)</author>
      <guid isPermaLink="false">2503.01653v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion Capture</title>
      <link>http://arxiv.org/abs/2503.03222v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为Mocap-2-to-3的框架，该框架旨在从单目视角恢复世界坐标系中的绝对姿势。通过将复杂的3D运动分解为2D姿态，并利用大规模的2D数据改进3D运动重建和预测人的绝对位置。&lt;h4&gt;背景&lt;/h4&gt;从单个视图中恢复世界坐标系下的绝对姿势面临两个主要问题：一是现有的方法需要依靠在有限环境中收集的3D运动数据进行训练，二是从单一视角估计一个人在度量空间中的绝对位置更为复杂。&lt;h4&gt;目的&lt;/h4&gt;开发一种新框架Mocap-2-to-3来解决上述挑战，并提高模型在不同场景下的泛化能力和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;引入了一个新的框架Mocap-2-to-3，该框架利用大量易于获取的2D姿态数据进行单视角扩散模型的预训练和多视图扩散模型的微调。此外，还提出了一种新颖的人体运动表示方式，将局部动作与全局运动分离，并编码地面几何先验。&lt;h4&gt;主要发现&lt;/h4&gt;通过在真实世界的数据集上对模型性能进行了评估，证明了该方法相比于现有最佳方法，在运动和绝对人体定位方面的准确性更高，且泛化性和可扩展性更强。&lt;h4&gt;结论&lt;/h4&gt;提出的Mocap-2-to-3框架展示了从单目视角恢复世界坐标系中绝对姿势的有效性，并有望在各种应用领域得到广泛使用。&lt;h4&gt;翻译&lt;/h4&gt;论文研究的是通过一种创新的框架利用大规模的2D数据来解决单目视图下恢复世界坐标系统中的绝对姿态所面临的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recovering absolute poses in the world coordinate system from monocular viewspresents significant challenges. Two primary issues arise in this context.Firstly, existing methods rely on 3D motion data for training, which requirescollection in limited environments. Acquiring such 3D labels for new actions ina timely manner is impractical, severely restricting the model's generalizationcapabilities. In contrast, 2D poses are far more accessible and easier toobtain. Secondly, estimating a person's absolute position in metric space froma single viewpoint is inherently more complex. To address these challenges, weintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motionsinto 2D poses, leveraging 2D data to enhance 3D motion reconstruction indiverse scenarios and accurately predict absolute positions in the worldcoordinate system. We initially pretrain a single-view diffusion model withextensive 2D data, followed by fine-tuning a multi-view diffusion model forview consistency using publicly available 3D data. This strategy facilitatesthe effective use of large-scale 2D data. Additionally, we propose aninnovative human motion representation that decouples local actions from globalmovements and encodes geometric priors of the ground, ensuring the generativemodel learns accurate motion priors from 2D data. During inference, this allowsfor the gradual recovery of global movements, resulting in more plausiblepositioning. We evaluate our model's performance on real-world datasets,demonstrating superior accuracy in motion and absolute human positioningcompared to state-of-the-art methods, along with enhanced generalization andscalability. Our code will be made publicly available.</description>
      <author>example@mail.com (Zhumei Wang, Zechen Hu, Ruoxi Guo, Huaijin Pi, Ziyong Feng, Sida Peng, Xiaowei Zhou)</author>
      <guid isPermaLink="false">2503.03222v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Dementia Insights: A Context-Based MultiModal Approach</title>
      <link>http://arxiv.org/abs/2503.01226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于上下文的多模态方法，用于识别痴呆症，这种方法利用了最先进的大型预训练模型（LPM）来分析文本和音频数据。&lt;h4&gt;背景&lt;/h4&gt;痴呆是一种进行性神经退行性疾病，影响记忆、推理和日常功能。早期检测对于及时干预以减缓疾病进展至关重要。现有的研究通常依赖于专家标注的数据集，并且采用单一模态的方法，这限制了其鲁棒性和可扩展性。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于上下文的多模态方法来改进痴呆症的检测性能。&lt;h4&gt;方法&lt;/h4&gt;该研究整合了文本和音频数据的最佳表现大型预训练模型（LPM），如GPT、BERT和CLAP，并通过结合上下文嵌入进一步提高了检测效果。此外，还尝试了一种基于上下文的In-Context Learning (ICL)作为补充技术。&lt;h4&gt;主要发现&lt;/h4&gt;使用GPT生成的嵌入与CLAP音频特征融合时，在F1分数上达到83.33%，优于当前最先进的痴呆症检测模型；未经标注的原始文本数据表现优于专家标注的数据集，表明大型预训练模型可以在无需大量手动标记的情况下提取有意义的语言和声学模式。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了开发大规模、非侵入性诊断工具以减少对昂贵注释依赖的可能性，并且保持高精度。通过将多模态学习与上下文嵌入相结合，这项工作为未来痴呆症个性化检测及认知健康的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于使用大型预训练模型进行基于文本和音频数据的痴呆症早期识别研究，提出了一种结合这两种模式并利用上下文信息的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dementia, a progressive neurodegenerative disorder, affects memory,reasoning, and daily functioning, creating challenges for individuals andhealthcare systems. Early detection is crucial for timely interventions thatmay slow disease progression. Large pre-trained models (LPMs) for text andaudio, such as Generative Pre-trained Transformer (GPT), Bidirectional EncoderRepresentations from Transformers (BERT), and Contrastive Language-AudioPretraining (CLAP), have shown promise in identifying cognitive impairments.However, existing studies generally rely heavily on expert-annotated datasetsand unimodal approaches, limiting robustness and scalability. This studyproposes a context-based multimodal method, integrating both text and audiodata using the best-performing LPMs in each modality. By incorporatingcontextual embeddings, our method improves dementia detection performance.Additionally, motivated by the effectiveness of contextual embeddings, wefurther experimented with a context-based In-Context Learning (ICL) as acomplementary technique. Results show that GPT-based embeddings, particularlywhen fused with CLAP audio features, achieve an F1-score of $83.33\%$,surpassing state-of-the-art dementia detection models. Furthermore, raw textdata outperforms expert-annotated datasets, demonstrating that LPMs can extractmeaningful linguistic and acoustic patterns without extensive manual labeling.These findings highlight the potential for scalable, non-invasive diagnostictools that reduce reliance on costly annotations while maintaining highaccuracy. By integrating multimodal learning with contextual embeddings, thiswork lays the foundation for future advancements in personalized dementiadetection and cognitive health research.</description>
      <author>example@mail.com (Sahar Sinene Mehdoui, Abdelhamid Bouzid, Daniel Sierra-Sosa, Adel Elmaghraby)</author>
      <guid isPermaLink="false">2503.01226v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Foundation Models for Environmental Science</title>
      <link>http://arxiv.org/abs/2503.03142v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文综述了基础模型在环境科学中的应用，涵盖了从数据生成到决策制定的多个方面。&lt;h4&gt;背景&lt;/h4&gt;对环境生态系统的建模对于资源管理、可持续发展和理解复杂的生态过程至关重要。然而，传统方法在处理这些系统内在复杂性、相互联系以及有限的数据时常常遇到困难。&lt;h4&gt;目的&lt;/h4&gt;本综述旨在提供基础模型应用于环境科学领域的全面概述，并强调其在前向预测、数据生成、数据同化、降尺度、模型集成和跨域决策制定等方面的进展。&lt;h4&gt;方法&lt;/h4&gt;该论文详细介绍了这些模型的发展过程，包括数据收集、架构设计、训练、调优和评估等环节。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型通过大规模预训练和通用表示能力，在整合多种数据源、捕捉时空依赖关系以及适应广泛任务方面提供了革命性的机会。&lt;h4&gt;结论&lt;/h4&gt;展示这些新兴方法的目的是促进跨学科合作，并推进环境科学中面向可持续发展的尖端机器学习技术的集成。&lt;h4&gt;翻译&lt;/h4&gt;建模环境生态系统对于有效的资源管理、可持续发展和理解复杂的生态过程至关重要。然而，传统方法在处理系统的固有复杂性、相互关联性和有限数据时常常面临挑战。基础模型通过大规模预训练和通用表示能力提供了一种变革性的机会，它能够整合多种数据源，捕捉时空依赖关系，并适应广泛的任务范围。这篇综述全面概述了基础模型在环境科学中的应用，突出了前向预测、数据生成、数据同化、降尺度、模型集成以及跨域决策制定等方面的进展。此外，论文还详细介绍了这些模型的发展过程，包括数据收集、架构设计、训练、调优和评估等环节。通过展示这些新兴方法，我们旨在促进跨学科合作，并推进环境科学中面向可持续发展的尖端机器学习技术的集成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modeling environmental ecosystems is essential for effective resourcemanagement, sustainable development, and understanding complex ecologicalprocesses. However, traditional methods frequently struggle with the inherentcomplexity, interconnectedness, and limited data of such systems. Foundationmodels, with their large-scale pre-training and universal representations,offer transformative opportunities by integrating diverse data sources,capturing spatiotemporal dependencies, and adapting to a broad range of tasks.This survey presents a comprehensive overview of foundation model applicationsin environmental science, highlighting advancements in forward prediction, datageneration, data assimilation, downscaling, model ensembling, anddecision-making across domains. We also detail the development process of thesemodels, covering data collection, architecture design, training, tuning, andevaluation. By showcasing these emerging methods, we aim to fosterinterdisciplinary collaboration and advance the integration of cutting-edgemachine learning for sustainable solutions in environmental science.</description>
      <author>example@mail.com (Runlong Yu, Shengyu Chen, Yiqun Xie, Xiaowei Jia)</author>
      <guid isPermaLink="false">2503.03142v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Transformer-Based Spatio-Temporal Association of Apple Fruitlets</title>
      <link>http://arxiv.org/abs/2503.03200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于变压器的方法，用于在不同天数和相机姿态下采集的立体图像中时空关联苹果果蕾。&lt;h4&gt;背景&lt;/h4&gt;现有的农业领域最先进的关联方法专注于匹配较大的农作物，使用高分辨率点云或时间稳定的特征，但这些对于野外较小的水果来说难以获取。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于变压器架构的方法，以解决为较小的水果获取和匹配问题。&lt;h4&gt;方法&lt;/h4&gt;采用了一种基于变压器的架构来编码每个果蕾的形状和位置，并通过一系列交替使用自注意力和交叉注意力的变压器编码层传播和细化这些特征。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在商业苹果园采集的数据上实现了92.4%的F1分数，优于所有基线和消融研究结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法展示了其在时空关联小水果上的有效性和优越性。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了一种基于变压器的方法，在不同时间点从不同相机角度收集的立体图像中对苹果果蕾进行时空关联。农业领域的现有方法主要针对较大的农作物，通过高分辨率点云或长时间稳定的特征实现匹配，但对于较小的果实来说这些数据难以获得。为解决这些问题，提出了一种编码每个果蕾形状和位置、并用一系列交替使用的自注意力和交叉注意层传播和细化这些特性的变压器架构。实验表明，在商业苹果园的数据上，该方法实现了92.4%的F1分数，并优于所有基线和消融研究结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a transformer-based method to spatio-temporallyassociate apple fruitlets in stereo-images collected on different days and fromdifferent camera poses. State-of-the-art association methods in agriculture arededicated towards matching larger crops using either high-resolution pointclouds or temporally stable features, which are both difficult to obtain forsmaller fruit in the field. To address these challenges, we propose atransformer-based architecture that encodes the shape and position of eachfruitlet, and propagates and refines these features through a series oftransformer encoder layers with alternating self and cross-attention. Wedemonstrate that our method is able to achieve an F1-score of 92.4% on datacollected in a commercial apple orchard and outperforms all baselines andablations.</description>
      <author>example@mail.com (Harry Freeman, George Kantor)</author>
      <guid isPermaLink="false">2503.03200v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NodeReg: Mitigating the Imbalance and Distribution Shift Effects in Semi-Supervised Node Classification via Norm Consistency</title>
      <link>http://arxiv.org/abs/2503.03211v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为NodeReg的正则化优化方法，用于确保节点表示的范数一致性，以减少不平衡邻居和噪声对图神经网络（GNN）性能的影响。&lt;h4&gt;背景&lt;/h4&gt;在半监督节点分类任务中，聚合来自邻近节点的信息虽然有助于提高GNN性能，但也使得节点容易受到其邻居节点影响。例如，在邻居节点不均衡或包含噪音的情况下，这会影响GNN的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;通过确保节点表示范数的一致性来减少不平衡和噪声对GNN的影响。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为NodeReg的方法，该方法是一种正则化的优化方式，用于强制执行节点表示范数的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;NodeReg能够显著提升在不均衡数据集和分布变化情况下的半监督节点分类性能。例如，在不平衡场景中，对于失衡比为0.1的GCN模型训练时，该方法能提高F1分数1.4%-25.9%；同样，在分布变化场景下，NodeReg提高了准确率1.4%-3.1%。&lt;h4&gt;结论&lt;/h4&gt;提出的方法简单但有效，并满足Lipschitz连续性条件，有助于稳定优化过程并在半监督节点分类任务中取得显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Aggregating information from neighboring nodes benefits graph neural networks(GNNs) in semi-supervised node classification tasks. Nevertheless, thismechanism also renders nodes susceptible to the influence of their neighbors.For instance, this will occur when the neighboring nodes are imbalanced or theneighboring nodes contain noise, which can even affect the GNN's ability togeneralize out of distribution. We find that ensuring the consistency of thenorm for node representations can significantly reduce the impact of these twoissues on GNNs. To this end, we propose a regularized optimization methodcalled NodeReg that enforces the consistency of node representation norms. Thismethod is simple but effective and satisfies Lipschitz continuity, thusfacilitating stable optimization and significantly improving semi-supervisednode classification performance under the above two scenarios. To illustrate,in the imbalance scenario, when training a GCN with an imbalance ratio of 0.1,NodeReg outperforms the most competitive baselines by 1.4%-25.9% in F1 scoreacross five public datasets. Similarly, in the distribution shift scenario,NodeReg outperforms the most competitive baseline by 1.4%-3.1% in accuracy.</description>
      <author>example@mail.com (Shenzhi Yang, Jun Xia, Jingbo Zhou, Xingkai Yao, Xiaofang Zhang)</author>
      <guid isPermaLink="false">2503.03211v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech Gesture Generation</title>
      <link>http://arxiv.org/abs/2503.01175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025. See https://star-uu-wang.github.io/HOP/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个名为HOP的新型多模态学习方法，用于生成协调的手势动作。&lt;h4&gt;背景&lt;/h4&gt;共话语手势是非言语线索，在人类交流中增强语音清晰度和表现力方面至关重要。现有的研究方法在提高手势准确性上取得了进展，但在生成多样化且连贯的动作上仍存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够捕捉手势运动、音频节奏和文本语义之间异构缠结的新多模态学习方法。&lt;h4&gt;方法&lt;/h4&gt;利用时空图建模实现音频与动作的对齐，并通过重新编程模块构建增强模态一致性的音频-文本语义表示。&lt;h4&gt;主要发现&lt;/h4&gt;HOP在生成自然、表达性强的手势方面取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了多模态模型如何有效地学习不同特征并代表它们的形式，从而实现更加协调和连贯的共话语手势。&lt;h4&gt;翻译&lt;/h4&gt;共话语手势是提升人类交流中语音清晰度与表现力的重要非语言线索。尽管现有的方法在提高手势准确性方面取得了一定进展，但生成多样化且连贯的手势动作仍具挑战性。该研究提出一种捕捉手势运动、音频节奏和文本语义之间异构缠结的多模态学习方法HOP，并通过时空图建模实现对齐。实验显示，HOP在自然与表达性强的手势生成方面取得了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Co-speech gestures are crucial non-verbal cues that enhance speech clarityand expressiveness in human communication, which have attracted increasingattention in multimodal research. While the existing methods have made stridesin gesture accuracy, challenges remain in generating diverse and coherentgestures, as most approaches assume independence among multimodal inputs andlack explicit modeling of their interactions. In this work, we propose a novelmultimodal learning method named HOP for co-speech gesture generation thatcaptures the heterogeneous entanglement between gesture motion, audio rhythm,and text semantics, enabling the generation of coordinated gestures. Byleveraging spatiotemporal graph modeling, we achieve the alignment of audio andaction. Moreover, to enhance modality coherence, we build the audio-textsemantic representation based on a reprogramming module, which is beneficialfor cross-modality adaptation. Our approach enables the trimodal system tolearn each other's features and represent them in the form of topologicalentanglement. Extensive experiments demonstrate that HOP achievesstate-of-the-art performance, offering more natural and expressive co-speechgesture generation. More information, codes, and demos are available here:https://star-uu-wang.github.io/HOP/</description>
      <author>example@mail.com (Hongye Cheng, Tianyu Wang, Guangsi Shi, Zexing Zhao, Yanwei Fu)</author>
      <guid isPermaLink="false">2503.01175v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Binary Classification Social Network Dataset for Graph Machine Learning</title>
      <link>http://arxiv.org/abs/2503.02397v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Binary Classification Social Network Dataset (BiSND)，这是一个为图机器学习设计的数据集，用于预测二元分类。&lt;h4&gt;背景&lt;/h4&gt;目前可用的基准数据集包括引文、共现和电子商务网络等，但没有专门针对社会网络进行分类任务的基准数据集。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究中的空白，提供一个适用于图机器学习应用的社会网络分类数据集。&lt;h4&gt;方法&lt;/h4&gt;BiSND以表格和图形格式呈现，并使用包括传统机器学习算法、深层神经网络、图神经网络以及最新的图对比学习方法在内的多样化分类器进行了验证。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，BiSND对于二元分类任务具有适用性，在F1分数上表现出从67.66到70.15的性能，这为未来的改进提供了可能性。&lt;h4&gt;结论&lt;/h4&gt;BiSND证明了其在传统和先进机器学习方法中的稳健性和有效性，并为未来的研究开辟了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Social networks have a vast range of applications with graphs. The availablebenchmark datasets are citation, co-occurrence, e-commerce networks, etc, withclasses ranging from 3 to 15. However, there is no benchmark classificationsocial network dataset for graph machine learning. This paper fills the gap andpresents the Binary Classification Social Network Dataset (\textit{BiSND}),designed for graph machine learning applications to predict binary classes. Wepresent the BiSND in \textit{tabular and graph} formats to verify itsrobustness across classical and advanced machine learning. We employ a diverseset of classifiers, including four traditional machine learning algorithms(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep NeuralNetwork (multi-layer perceptrons), one Graph Neural Network (GraphConvolutional Network), and three state-of-the-art Graph Contrastive Learningmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable forclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicatingpromising avenues for future enhancements.</description>
      <author>example@mail.com (Adnan Ali, Jinglong Li, Huanhuan Chen, AlMotasem Bellah Al Ajlouni)</author>
      <guid isPermaLink="false">2503.02397v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Zero-Shot Learning Approach for Ephemeral Gully Detection from Remote Sensing using Vision Language Models</title>
      <link>http://arxiv.org/abs/2503.01169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了三种用于检测暂时性冲沟的自动化管道，并通过公开数据集进行了验证，这些管道基于视觉语言模型（VLM）实现了超过70%的准确率和接近80%的F1分数。&lt;h4&gt;背景&lt;/h4&gt;暂时性冲沟是土壤侵蚀的主要原因之一，现有的研究未能成功地实现从遥感图像中自动检测暂时性冲沟。&lt;h4&gt;目的&lt;/h4&gt;开发和评估用于检测暂时性冲沟的有效管道，并提供首个公开的数据集进行测试。&lt;h4&gt;方法&lt;/h4&gt;利用特定农业区域在一段时间内获取的遥感图像，通过多种视觉语言模型（VLM）对存在暂时性冲沟的图像进行分类。同时采用了零样本分类方法以及与迁移学习方法进行了比较。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的管道能够有效地检测到缺乏充分标注数据集下的暂时性冲沟，准确率超过70%，F1分数接近80%。&lt;h4&gt;结论&lt;/h4&gt;该研究为自动化临时冲沟的识别提供了一种有效的方法，并通过实验验证了其在实际应用中的潜力。此外，开发了一个公开的数据集，以促进未来的研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了用于检测暂时性冲沟的三种管道，这些管道利用特定农业区域在一段时间内获取的遥感图像，并基于视觉语言模型（VLM）实现了超过70%的准确率和接近80%的F1分数。该研究开发并测试了首个公开的数据集，同时与迁移学习方法进行了比较，实验结果表明所提出的零样本分类管道在缺乏充分标注数据集的情况下能够有效检测暂时性冲沟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ephemeral gullies are a primary cause of soil erosion and their reliable,accurate, and early detection will facilitate significant improvements in thesustainability of global agricultural systems. In our view, prior research hasnot successfully addressed automated detection of ephemeral gullies fromremotely sensed images, so for the first time, we present and evaluate threesuccessful pipelines for ephemeral gully detection. Our pipelines utilizeremotely sensed images, acquired from specific agricultural areas over a periodof time. The pipelines were tested with various choices of Visual LanguageModels (VLMs), and they classified the images based on the presence ofephemeral gullies with accuracy higher than 70% and a F1-score close to 80% forpositive gully detection. Additionally, we developed the first public datasetfor ephemeral gully detection, labeled by a team of soil- and plant-scienceexperts. To evaluate the proposed pipelines, we employed a variety of zero-shotclassification methods based on State-of-the-Art (SOTA) open-sourceVision-Language Models (VLMs). In addition to that, we compare the samepipelines with a transfer learning approach. Extensive experiments wereconducted to validate the detection pipelines and to analyze the impact ofhyperparameter changes in their performance. The experimental resultsdemonstrate that the proposed zero-shot classification pipelines are highlyeffective in detecting ephemeral gullies in a scenario where classificationdatasets are scarce.</description>
      <author>example@mail.com (Seyed Mohamad Ali Tousi, Ramy Farag, Jacket Demby's, Gbenga Omotara, John A. Lory, G. N. DeSouza)</author>
      <guid isPermaLink="false">2503.01169v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering</title>
      <link>http://arxiv.org/abs/2503.03190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;3D问答（3D QA）要求模型全面理解由文本描述的所处三维场景，并在此基础上回答有关周围环境的问题。然而，现有方法通常依赖于纯粹基于点云的全局场景感知，忽视了多视角图像中丰富局部纹理细节的重要性。&lt;h4&gt;背景&lt;/h4&gt;现有的3D QA方法主要依靠纯3D点云进行全局场景感知，而忽略了从多视角图片获取的详细文本描述信息和复杂遮挡下相机姿态导致的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够综合集成多视角和点云特征的方法，以提高在3D问答中的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;{'Text-guided Multi-view Fusion (TGMF)模块': '优先考虑与文本语义内容紧密匹配的图像视图', 'Adaptive Dual-vision Perception (ADVP)模块': '设计用于自适应融合反投影多视角图像和点云特征，增强对3D场景的理解。', 'Multimodal Context-guided Reasoning (MCGR)模块': '通过整合视觉和语言模态的上下文信息来促进鲁棒推理'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在SQA3D和ScanQA数据集上的DSPNet优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;我们的方法能有效解决现有3D QA中存在的问题，提高场景理解和推理能力。&lt;h4&gt;翻译&lt;/h4&gt;论文提出了一种新的Dual-vision Scene Perception Network (DSPNet)，通过综合多视角图像与点云特征来改善3D QA任务中的鲁棒性。研究设计了三个关键模块：TGMF、ADVP和MCGR，以解决现有方法在文本理解、视图融合及上下文推理上的不足，并展示了其在两个基准数据集上优越的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/LZ-CH/DSPNet&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Question Answering (3D QA) requires the model to comprehensivelyunderstand its situated 3D scene described by the text, then reason about itssurrounding environment and answer a question under that situation. However,existing methods usually rely on global scene perception from pure 3D pointclouds and overlook the importance of rich local texture details frommulti-view images. Moreover, due to the inherent noise in camera poses andcomplex occlusions, there exists significant feature degradation and reducedfeature robustness problems when aligning 3D point cloud with multi-viewimages. In this paper, we propose a Dual-vision Scene Perception Network(DSPNet), to comprehensively integrate multi-view and point cloud features toimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) moduleprioritizes image views that closely match the semantic content of the text. Toadaptively fuse back-projected multi-view images with point cloud features, wedesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scenecomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)module facilitates robust reasoning by integrating contextual informationacross visual and linguistic modalities. Experimental results on SQA3D andScanQA datasets demonstrate the superiority of our DSPNet. Codes will beavailable at https://github.com/LZ-CH/DSPNet.</description>
      <author>example@mail.com (Jingzhou Luo, Yang Liu, Weixing Chen, Zhen Li, Yaowei Wang, Guanbin Li, Liang Lin)</author>
      <guid isPermaLink="false">2503.03190v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Simple Siamese Network for High-Resolution Video Quality Assessment</title>
      <link>http://arxiv.org/abs/2503.02330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为SiamVQA的新型Siamese网络，用于高分辨率视频质量评估。&lt;h4&gt;背景&lt;/h4&gt;在视频质量评估（VQA）的研究中，双分支网络已经作为一种有前途的解决方案出现。这种网络将技术视角和美学视角分开来测量低级失真和高级语义感知。&lt;h4&gt;目的&lt;/h4&gt;研究者认为仅从技术角度进行评估应该以语义感知的方式来进行，并假设现有的技术分支难以在高分辨率视频中察觉到这些高级语义信息，因此提出了一种新的网络模型SiamVQA。&lt;h4&gt;方法&lt;/h4&gt;SiamVQA采用共享权重的双分支架构来提升技术视角下对高层语义的理解能力。此外还引入了双重交叉注意力层用于融合技术和美学特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该模型在高分辨率基准数据集上达到了最先进的准确度，并且在低分辨率基准数据集上的表现也具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;SiamVQA是首个尝试通过语义感知方式从技术视角进行视频质量评估的网络架构，在处理高质量视频时展现出了巨大的潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这项研究中，关于视频质量评估（VQA），双分支网络已作为一种有前途的方法出现。它将VQA分解为技术和美学两个独立的技术层面来分别测量低级失真和高级语义感知。然而，我们提出技术视角本身应该以意识语义的方式进行衡量。假设现有的技术支路难以识别高分辨率视频中的高层含义，因为它是在从视频中抽样的局部小块上训练的。这些问题在低分辨率视频表现良好时可以被掩盖，但在高分辨率VQA中就变得非常关键了。这项工作介绍了SiamVQA，这是一种简单而有效的用于高分辨率VQA的Siamese网络。通过共享技术与美学支路之间的权重，SiamVQA提升了技术支路理解语义的能力，并促进了技术质量表示学习。此外，它还整合了一种双重交叉注意力层来融合技术和美学特征。SiamVQA在高分辨率基准测试中实现了最先进的精度，在较低分辨率的基准测试中的结果也非常具有竞争力。代码可在https://github.com/srcn-ivl/SiamVQA 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the research of video quality assessment (VQA), two-branch network hasemerged as a promising solution. It decouples VQA with separate technical andaesthetic branches to measure the perception of low-level distortions andhigh-level semantics respectively. However, we argue that while technical andaesthetic perspectives are complementary, the technical perspective itselfshould be measured in semantic-aware manner. We hypothesize that existingtechnical branch struggles to perceive the semantics of high-resolution videos,as it is trained on local mini-patches sampled from videos. This issue can behidden by apparently good results on low-resolution videos, but indeed becomescritical for high-resolution VQA. This work introduces SiamVQA, a simple buteffective Siamese network for highre-solution VQA. SiamVQA shares weightsbetween technical and aesthetic branches, enhancing the semantic perceptionability of technical branch to facilitate technical-quality representationlearning. Furthermore, it integrates a dual cross-attention layer for fusingtechnical and aesthetic features. SiamVQA achieves state-of-the-art accuracy onhigh-resolution benchmarks, and competitive results on lower-resolutionbenchmarks. Codes will be available at: https://github.com/srcn-ivl/SiamVQA</description>
      <author>example@mail.com (Guotao Shen, Ziheng Yan, Xin Jin, Longhai Wu, Jie Chen, Ilhyun Cho, Cheul-Hee Hahm)</author>
      <guid isPermaLink="false">2503.02330v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Directly Follows Graphs Go Predictive Process Monitoring With Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.03197v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，基于人工神经网络的预测过程监控技术（PPM）作为监测业务流程未来行为的方法不断发展。现有方法大多关注于将流程视为序列数据并将其输入到处理顺序数据的神经架构中，如循环神经网络或变压器。&lt;h4&gt;背景&lt;/h4&gt;现有的PPM技术主要通过使用递归神经网络(RNNs)或变压器等处理顺序数据的模型来预测业务流程的未来行为。但是这些方法在处理复杂、长时间且包含大量循环的流程时存在局限性。&lt;h4&gt;目的&lt;/h4&gt;本研究探讨了一种替代方式来进行PPM：通过将每个过程转换为其直接后继图(DFG)表示，应用图神经网络(GNNs)进行预测任务。目的是开发更适合于复杂和长周期业务流程的模型。&lt;h4&gt;方法&lt;/h4&gt;我们介绍了不同创建DFG表示的方法，这些方法根据所使用的特定GNN而有所不同。测试了从传统基于节点的到新式的基于边的架构的各种GNN，并探讨了使用多图的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述步骤，我们的目标是设计出将轨迹转换为图形时信息损失最小化的图表示。&lt;h4&gt;结论&lt;/h4&gt;研究展示了通过利用GNN技术进行PPM的潜力和前景，特别是对于复杂且包含大量循环的业务流程。这种方法可能为复杂的流程预测任务提供更有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在过去的几年中，基于人工神经网络的预测过程监控（PPM）技术作为一种监测商业流程未来行为的方法得到了发展。现有的方法主要集中在将流程视为序列数据，并将其输入到处理顺序数据的神经架构中，例如循环神经网络（RNNs）或变压器。在这项研究中，我们探讨了一种进行PPM的替代方式：通过将每个过程转换为其直接后继图(DFG)表示，可以应用图神经网络(GNNs)来执行预测任务。这样做的目的是开发更适合于复杂、长周期且包含大量循环的业务流程的模型。特别地，我们展示了根据所用特定GNN的不同创建DFG表示的方法。测试的GNN范围从传统的节点架构到新颖的边架构。此外，还探讨了使用多图的可能性。通过这些步骤，我们的目标是设计出将轨迹转换为图形时信息损失最小化的图表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the past years, predictive process monitoring (PPM) techniques based onartificial neural networks have evolved as a method to monitor the futurebehavior of business processes. Existing approaches mostly focus oninterpreting the processes as sequences, so-called traces, and feeding them toneural architectures designed to operate on sequential data such as recurrentneural networks (RNNs) or transformers. In this study, we investigate analternative way to perform PPM: by transforming each process in itsdirectly-follows-graph (DFG) representation we are able to apply graph neuralnetworks (GNNs) for the prediction tasks. By this, we aim to develop modelsthat are more suitable for complex processes that are long and contain anabundance of loops. In particular, we present different ways to create DFGrepresentations depending on the particular GNN we use. The tested GNNs rangefrom classical node-based to novel edge-based architectures. Further, weinvestigate the possibility of using multi-graphs. By these steps, we aim todesign graph representations that minimize the information loss whentransforming traces into graphs.</description>
      <author>example@mail.com (Attila Lischka, Simon Rauch, Oliver Stritzel)</author>
      <guid isPermaLink="false">2503.03197v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A General Neural Network Potential for Energetic Materials with C, H, N, and O elements</title>
      <link>http://arxiv.org/abs/2503.01932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages,16 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种高效的神经网络势能（NNP）模型，用于预测C, H, N, O组成的高能量材料的结构、机械和分解特性。该模型通过转移学习利用预先训练好的NNP，并基于密度泛函理论计算得到的能量和力数据进行微调。&lt;h4&gt;背景&lt;/h4&gt;传统的高能量材料研究受限于高昂的计算成本和漫长的开发周期，阻碍了新材料的设计与优化。&lt;h4&gt;目的&lt;/h4&gt;本工作旨在发展一种适用于预测C, H, N, O组成的高能量材料特性的神经网络模型，并验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;利用已预训练好的NNP模型，在转移学习框架下使用DFT计算得到的能量和力数据进行微调，应用于20种不同的高能系统。该模型通过分子动力学模拟进行测试，并与实验结果对比。&lt;h4&gt;主要发现&lt;/h4&gt;经过验证的神经网络模型能够准确描述C, H, N, O组成的高能量材料的关键原子相互作用及热分解机制，表现出DFT级别的精度和广泛的适用性，显著减少了计算和实验成本。&lt;h4&gt;结论&lt;/h4&gt;该工作提供了一种高效的策略来设计和发展高能量材料，并为结合密度泛函理论、机器学习和实验方法的材料研究提出了一个有前景的框架。NNP模型已开源在GitHub（https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials）。&lt;h4&gt;翻译&lt;/h4&gt;高能量材料的发现与优化受限于传统方法高昂的计算成本和漫长的开发周期，本工作旨在发展一种高效的神经网络势能模型以预测C, H, N, O组成的高能量材料。该模型基于预训练后的NNP通过转移学习进行微调，并且在分子动力学模拟中得到验证，展示了其优越的精度与广义性，在减少计算和实验成本的同时提高了设计效率，为结合多种方法推进材料研究提供了有效策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/mingjiewen/general-nnp-model-for-c-h-n-o-energetic-materials&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The discovery and optimization of high-energy materials (HEMs) areconstrained by the prohibitive computational expense and prolonged developmentcycles inherent in conventional approaches. In this work, we develop a generalneural network potential (NNP) that efficiently predicts the structural,mechanical, and decomposition properties of HEMs composed of C, H, N, and O.Our framework leverages pre-trained NNP models, fine-tuned using transferlearning on energy and force data derived from density functional theory (DFT)calculations. This strategy enables rapid adaptation across 20 different HEMsystems while maintaining DFT-level accuracy, significantly reducingcomputational costs. A key aspect of this work is the ability of NNP model tocapture the chemical activity space of HEMs, accurately describe the key atomicinteractions and reaction mechanisms during thermal decomposition. The generalNNP model has been applied in molecular dynamics (MD) simulations and validatedwith experimental data for various HEM structures. Results show that the NNPmodel accurately predicts the structural, mechanical, and decompositionproperties of HEMs by effectively describing their chemical activity space.Compared to traditional force fields, it offers superior DFT-level accuracy andgeneralization across both microscopic and macroscopic properties, reducing thecomputational and experimental costs. This work provides an efficient strategyfor the design and development of HEMs and proposes a promising framework forintegrating DFT, machine learning, and experimental methods in materialsresearch. (To facilitate further research and practical applications, weopen-source our NNP model on GitHub:https://github.com/MingjieWen/General-NNP-model-for-C-H-N-O-Energetic-Materials.)</description>
      <author>example@mail.com (Mingjie Wen, Jiahe Han, Wenjuan Li, Xiaoya Chang, Qingzhao Chu, Dongping Chen)</author>
      <guid isPermaLink="false">2503.01932v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR</title>
      <link>http://arxiv.org/abs/2502.17039v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;车辆到基础设施（V2I）协作感知利用基础设施传感器收集的数据来增强车辆的感知能力。尽管激光雷达是一种常用的传感器，且在智能汽车和基础设施中广泛应用，但其优越性能伴随着较高的成本。为了实现低成本V2I，降低激光雷达的成本至关重要。&lt;h4&gt;背景&lt;/h4&gt;现有的V2I系统面临的主要挑战是如何在降低成本的同时保持高性能感知效果。由于高分辨率激光雷达价格昂贵，而低分辨率的激光雷达会导致远处的小物体变得更加模糊。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的协作感知框架LCV2I，在保证性能的前提下尽可能降低车辆上的传感器成本。&lt;h4&gt;方法&lt;/h4&gt;LCV2I利用摄像头和低成本低分辨率激光雷达的数据作为输入。通过特征偏移校正模块和区域特征增强算法提高特征表示能力，并采用区域性差异图和得分图来评估协作内容的价值，从而提升通信带宽效率。&lt;h4&gt;主要发现&lt;/h4&gt;LCV2I能够在保证高质量感知性能的同时大幅减少对高分辨率传感器的需求，并在现实世界的DAIR-V2X场景中进行3D目标检测时表现出色，超越现有算法的性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入低成本低分辨率激光雷达和改进通信效率的方法，LCV2I框架成功实现了高性能车辆到基础设施协作感知的目标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：车辆到基础设施（V2I）协作感知利用基础设施传感器收集的数据来增强车辆感知能力。尽管作为常用传感器的LiDAR在智能汽车与基础设施中广泛应用，但其卓越性能伴随着高昂的成本问题。为了实现低成本V2I方案，降低LiDAR成本显得尤为重要。我们提出了一种新方法，即使用低分辨率LiDAR以尽可能地降低成本，并结合相机和激光雷达数据作为输入。通过特征偏移校正模块及区域特征增强算法来提升特征表示能力；同时采用区域性差异图与得分图评估合作内容的价值，从而提高通信带宽利用效率。在保证高质量感知性能的同时大幅减少对高分辨率传感器的需求，这是我们的研究目标所在。实验结果表明，在现实世界中，使用DAIR-V2X场景进行3D物体检测时，所提出的LCV2I框架的性能显著优于现有算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-to-Infrastructure (V2I) collaborative perception leverages datacollected by infrastructure's sensors to enhance vehicle perceptualcapabilities. LiDAR, as a commonly used sensor in cooperative perception, iswidely equipped in intelligent vehicles and infrastructure. However, itssuperior performance comes with a correspondingly high cost. To achievelow-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we studyadopting low-resolution LiDAR on the vehicle to minimize cost as much aspossible. However, simply reducing the resolution of vehicle's LiDAR results insparse point clouds, making distant small objects even more blurred.Additionally, traditional communication methods have relatively low bandwidthutilization efficiency. These factors pose challenges for us. To balance costand perceptual accuracy, we propose a new collaborative perception framework,namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDARas input. It also employs feature offset correction modules and regionalfeature enhancement algorithms to improve feature representation. Finally, weuse regional difference map and regional score map to assess the value ofcollaboration content, thereby improving communication bandwidth efficiency. Insummary, our approach achieves high perceptual performance while substantiallyreducing the demand for high-resolution sensors on the vehicle. To evaluatethis algorithm, we conduct 3D object detection in the real-world scenario ofDAIR-V2X, demonstrating that the performance of LCV2I consistently surpassescurrently existing algorithms.</description>
      <author>example@mail.com (Xinxin Feng, Haoran Sun, Haifeng Zheng)</author>
      <guid isPermaLink="false">2502.17039v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Rapid morphology characterization of two-dimensional TMDs and lateral heterostructures based on deep learning</title>
      <link>http://arxiv.org/abs/2503.00470v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度学习的方法，用于二维材料和异质结构的高效表征。通过使用YOLO模型，实现了对特定类型二维材料识别的高精度，并且探讨了跨不同材料应用迁移学习的效果。&lt;h4&gt;背景&lt;/h4&gt;二维材料及其异质结构具有独特的物理特性，需要高效的表征方法。&lt;h4&gt;目的&lt;/h4&gt;利用人工智能的进步提出了一种基于深度学习的方法来准确地表征2D材料和异质结构。&lt;h4&gt;方法&lt;/h4&gt;使用YOLO模型识别MoS2-MoSe2横向异质结和不同形状与厚度的MoS2薄片，同时探索了跨不同材料的迁移学习技术以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;该模型达到了超过94.67%的精度，并展示了强大的泛化能力和抗干扰能力。开发的应用程序能够直接从光学显微镜图像中进行实时分析，显著提高了效率和降低了成本。&lt;h4&gt;结论&lt;/h4&gt;这种深度学习驱动的方法代表了一种快速准确表征2D材料的新工具，为材料科学的研究和发展开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：二维(2D)材料和异质结构表现出独特的物理特性，需要高效的表征方法。利用人工智能的进展，我们介绍了一种基于深度学习的方法来高效地表征异质结构和2D材料，特别是MoS2-MoSe2横向异质结和不同形状与厚度的MoS2薄片。通过使用YOLO模型，我们在这些材料的识别中实现了超过94.67%的准确率。此外，我们探讨了跨不同类型材料应用迁移学习的效果，这进一步提高了模型性能。该模型展示出强大的泛化能力和抗干扰能力，在各种场景下确保可靠的结果。为了便于实际使用，我们开发了一个应用程序，它可以直接从光学显微镜图像中进行实时分析，使其过程比传统方法更快且成本更低。这种深度学习驱动的方法为二维材料的快速准确表征提供了一种有前景的工具，并为材料科学的研究和发展开辟了新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Two-dimensional (2D) materials and heterostructures exhibit unique physicalproperties, necessitating efficient and accurate characterization methods.Leveraging advancements in artificial intelligence, we introduce a deeplearning-based method for efficiently characterizing heterostructures and 2Dmaterials, specifically MoS2-MoSe2 lateral heterostructures and MoS2 flakeswith varying shapes and thicknesses. By utilizing YOLO models, we achieve anaccuracy rate of over 94.67% in identifying these materials. Additionally, weexplore the application of transfer learning across different materials, whichfurther enhances model performance. This model exhibits robust generalizationand anti-interference ability, ensuring reliable results in diverse scenarios.To facilitate practical use, we have developed an application that enablesreal-time analysis directly from optical microscope images, making the processsignificantly faster and more cost-effective than traditional methods. Thisdeep learning-driven approach represents a promising tool for the rapid andaccurate characterization of 2D materials, opening new avenues for research anddevelopment in material science.</description>
      <author>example@mail.com (Junqi He, Yujie Zhang, Jialu Wang, Tao Wang, Pan Zhang, Chengjie Cai, Jinxing Yang, Xiao Lin, Xiaohui Yang)</author>
      <guid isPermaLink="false">2503.00470v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SPIDER: A Comprehensive Multi-Organ Supervised Pathology Dataset and Baseline Models</title>
      <link>http://arxiv.org/abs/2503.02876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍SPIDER数据集，这是一个公共的病理图像描述库，是当前最大的补丁级别数据集之一，涵盖了多种器官类型，提供高质量注释和周围背景信息。&lt;h4&gt;背景&lt;/h4&gt;现有的公开计算病理学数据集在器官多样性、类别覆盖率或标注质量方面存在局限性。这限制了AI技术的发展。&lt;h4&gt;目的&lt;/h4&gt;引入SPIDER以弥补现有公共数据集中不足的器官种类、分类覆盖范围及注释质量问题，从而推动计算机病理学领域的发展。&lt;h4&gt;方法&lt;/h4&gt;创建了一个涵盖皮肤、结肠直肠和胸部等多类型组织的数据集，并提供了由专业病理学家验证的高质量注释。此外，还开发了使用Hibou-L基础模型作为特征提取器结合注意力机制分类头的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过在SPIDER数据集上训练的基线模型，在多种组织类别中取得了最先进的性能表现，这些模型为未来的数字病理研究提供了强有力的基准。&lt;h4&gt;结论&lt;/h4&gt;不仅限于补丁级别的分类任务，该方法还支持快速定位重要区域、提供定量组织指标，并为进一步多模态路径学技术的发展奠定基础。SPIDER数据集和训练好的模型已向公众开放使用。&lt;h4&gt;翻译&lt;/h4&gt;推进计算病理学领域的AI研究需要大量的高质量且多样化的数据集，但现有公开的数据集往往在器官多样性、分类覆盖范围或注释质量方面存在限制。为了填补这一空白，我们推出了SPIDER（监督的病理图像描述库），这是一个最大的公共可用补丁级别数据集，涵盖了包括皮肤、结肠直肠和胸部在内的多种组织类型，并且每种组织都具有全面的分类覆盖率。SPIDER提供了由专家病理学家验证过的高质量注释，还包括周围的背景补丁，这些增强了空间上下文信息下的分类性能表现。除了该数据集外，我们还展示了基于Hibou-L基础模型作为特征提取器结合注意力机制分类头训练出基线模型的方法，这些模型在多种组织类别中取得了最先进的性能，为未来的数字病理学研究提供了强有力的基准点。SPIDER不仅限于补丁级别的分类任务，它还能快速识别重要的区域、提供定量的组织指标，并为进一步多模态方法的发展打下了基础。该数据集及训练好的模型均向公众开放以推进研究和人工智能驱动下的病理学发展。访问网址：https://github.com/HistAI/SPIDER&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancing AI in computational pathology requires large, high-quality, anddiverse datasets, yet existing public datasets are often limited in organdiversity, class coverage, or annotation quality. To bridge this gap, weintroduce SPIDER (Supervised Pathology Image-DEscription Repository), thelargest publicly available patch-level dataset covering multiple organ types,including Skin, Colorectal, and Thorax, with comprehensive class coverage foreach organ. SPIDER provides high-quality annotations verified by expertpathologists and includes surrounding context patches, which enhanceclassification performance by providing spatial context.  Alongside the dataset, we present baseline models trained on SPIDER using theHibou-L foundation model as a feature extractor combined with anattention-based classification head. The models achieve state-of-the-artperformance across multiple tissue categories and serve as strong benchmarksfor future digital pathology research. Beyond patch classification, the modelenables rapid identification of significant areas, quantitative tissue metrics,and establishes a foundation for multimodal approaches.  Both the dataset and trained models are publicly available to advanceresearch, reproducibility, and AI-driven pathology development. Access them at:https://github.com/HistAI/SPIDER</description>
      <author>example@mail.com (Dmitry Nechaev, Alexey Pchelnikov, Ekaterina Ivanova)</author>
      <guid isPermaLink="false">2503.02876v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Understanding the Benefit of Multitask Representation Learning in Decision Process</title>
      <link>http://arxiv.org/abs/2503.00345v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2205.15701&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种在多任务学习环境中提高样本效率的方法，通过分析未知非线性表示功能的多任务强化学习(MRL)，填补了理论框架上的空白。&lt;h4&gt;背景&lt;/h4&gt;多任务表征学习（MRL）被广泛认为是提升强化学习中样本效率的一种技术。然而，在实际应用中，现有的理论分析方法通常假设代理已知表示函数或使用线性类函数，这在现实中不太实用。&lt;h4&gt;目的&lt;/h4&gt;研究并填补多任务学习环境中未知非线性表示功能的理论空白，提供一个全面的机制分析，特别是在在线和迁移学习设置下。&lt;h4&gt;方法&lt;/h4&gt;考虑了一个代理同时执行M个情境贝叶斯问题（或马尔可夫决策过程）的情况，并使用我们提出的广义函数上限置信界算法(GFUCB)从非线性功能类中开发出共享表示功能φ。&lt;h4&gt;主要发现&lt;/h4&gt;正式证明了这种方法可以超越学习单独任务的下限，表明多任务表征学习在一般功能类中的有效性。该框架还解释了表示功能对迁移学习的影响，并确定了成功转移的关键条件。&lt;h4&gt;结论&lt;/h4&gt;实证实验进一步验证了理论发现，证实了MRL方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multitask Representation Learning (MRL) has emerged as a prevalent techniqueto improve sample efficiency in Reinforcement Learning (RL). Empirical studieshave found that training agents on multiple tasks simultaneously within onlineand transfer learning environments can greatly improve efficiency. Despite itspopularity, a comprehensive theoretical framework that elucidates itsoperational efficacy remains incomplete. Prior analyses have predominantlyassumed that agents either possess a pre-known representation function orutilize functions from a linear class, where both are impractical. Thecomplexity of real-world applications typically requires the use ofsophisticated, non-linear functions such as neural networks as representationfunction, which are not pre-existing but must be learned. Our work tries tofill the gap by extending the analysis to \textit{unknown non-linear}representations, giving a comprehensive analysis for its mechanism in onlineand transfer learning setting. We consider the setting that an agentsimultaneously playing $M$ contextual bandits (or MDPs), developing a sharedrepresentation function $\phi$ from a non-linear function class $\Phi$ usingour novel Generalized Functional Upper Confidence Bound algorithm (GFUCB). Weformally prove that this approach yields a regret upper bound that outperformsthe lower bound associated with learning $M$ separate tasks, marking the firstdemonstration of MRL's efficacy in a general function class. This frameworkalso explains the contribution of representations to transfer learning whenfaced with new, yet related tasks, and identifies key conditions for successfultransfer. Empirical experiments further corroborate our theoretical findings.</description>
      <author>example@mail.com (Rui Lu, Yang Yue, Andrew Zhao, Simon Du, Gao Huang)</author>
      <guid isPermaLink="false">2503.00345v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>LLM-Fusion: A Novel Multimodal Fusion Model for Accelerated Material Discovery</title>
      <link>http://arxiv.org/abs/2503.01022v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  4 pages, presented at AAAI 2025 Workshop on AI to Accelerating  Science and Engineering (AI2ASE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;LLM-Fusion是一种新型的多模态融合模型，利用大型语言模型（LLMs）整合多种表示形式，如SMILES、SELFIES、文本描述和分子指纹等，用于准确预测材料属性。&lt;h4&gt;背景&lt;/h4&gt;在高效地发现具有理想特性的材料方面仍然存在重要问题。许多研究通过使用有关材料的不同信息集来解决这个问题。其中多模态方法由于能够结合不同来源的信息而显示出潜力。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新型的多模态融合模型LLM-Fusion，该模型利用大型语言模型（LLMs）整合多样化的表示形式，以比传统方法更高的准确性预测材料属性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于LLM的灵活架构，支持多模态输入处理，并在两个数据集上的五项预测任务上验证了其有效性。与单模式和简单的拼接基线相比，模型显示出了优越的效果。&lt;h4&gt;主要发现&lt;/h4&gt;LLM-Fusion能够提供丰富的多模态表示形式，比现有融合算法更复杂且有效，从而实现更高精度的材料属性预测。&lt;h4&gt;结论&lt;/h4&gt;LLM-Fusion在准确预测多种材料特性方面表现优于传统方法和简单基线模型，证明了基于大型语言模型进行多模态数据整合的有效性。&lt;h4&gt;翻译&lt;/h4&gt;发现具有理想特性的材料并以高效的方式进行仍然是材料科学中的一个重要问题。许多研究通过利用关于材料的不同信息集来解决这个问题。在这其中，多模态方法由于其结合不同信息来源的能力而显得有前景。然而，到目前为止的融合算法仍然相对简单，缺乏提供丰富多样表示形式的机制。本文提出了一种新的多模态融合模型LLM-Fusion，该模型利用大型语言模型（LLMs）整合各种表示形式，如SMILES、SELFIES、文本描述和分子指纹等，并用于准确预测材料属性。我们的方法引入了一种基于LLM的灵活架构，支持多模态输入处理，并能以比传统方法更高的精度进行材料属性预测。我们在两个数据集上的五项预测任务上验证了我们的模型，并证明其效果优于单模式和简单的拼接基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Discovering materials with desirable properties in an efficient way remains asignificant problem in materials science. Many studies have tackled thisproblem by using different sets of information available about the materials.Among them, multimodal approaches have been found to be promising because oftheir ability to combine different sources of information. However, fusionalgorithms to date remain simple, lacking a mechanism to provide a richrepresentation of multiple modalities. This paper presents LLM-Fusion, a novelmultimodal fusion model that leverages large language models (LLMs) tointegrate diverse representations, such as SMILES, SELFIES, text descriptions,and molecular fingerprints, for accurate property prediction. Our approachintroduces a flexible LLM-based architecture that supports multimodal inputprocessing and enables material property prediction with higher accuracy thantraditional methods. We validate our model on two datasets across fiveprediction tasks and demonstrate its effectiveness compared to unimodal andnaive concatenation baselines.</description>
      <author>example@mail.com (Onur Boyar, Indra Priyadarsini, Seiji Takeda, Lisa Hamada)</author>
      <guid isPermaLink="false">2503.01022v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Waste Classification By Dual-Encoder Contrastive Learning and Multi-Clustering Voting (DECMCV)</title>
      <link>http://arxiv.org/abs/2503.02241v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的无监督方法Dual-Encoder Contrastive Learning with Multi-Clustering Voting (DECMCV)，该方法旨在提高垃圾分类的自动化和效率。&lt;h4&gt;背景&lt;/h4&gt;垃圾分类对提升处理效率、减少环境污染至关重要。传统的有监督深度学习方法依赖大量标注数据，这些数据昂贵且难以获取；而自监督学习和无监督学习尽管可以在一定程度上解决数据稀缺问题，但仍存在性能不足的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的无标签垃圾分类算法，以提高模型的泛化能力和处理实际场景中的风格差异。&lt;h4&gt;方法&lt;/h4&gt;DECMCV采用预训练的ConvNeXt模型进行图像编码，使用Vision Transformer生成正样本，并应用多聚类投票机制来解决数据标注和领域偏移问题。&lt;h4&gt;主要发现&lt;/h4&gt;在TrashNet和华为云数据集上，DECMCV达到了93.78%和98.29%的分类准确率；并且仅需50个标签样本就能有效标注4169张真实世界垃圾图像，提高了模型的分类准确性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发出一种高效的无监督垃圾分类方法，能够更好地应对实际数据中的风格差异问题，并有助于提高自动化的垃圾分类系统的性能和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Waste classification is crucial for improving processing efficiency andreducing environmental pollution. Supervised deep learning methods are commonlyused for automated waste classification, but they rely heavily on large labeleddatasets, which are costly and inefficient to obtain. Real-world waste dataoften exhibit category and style biases, such as variations in camera angles,lighting conditions, and types of waste, which can impact the model'sperformance and generalization ability. Therefore, constructing a bias-freedataset is essential. Manual labeling is not only costly but also inefficient.While self-supervised learning helps address data scarcity, it still depends onsome labeled data and generally results in lower accuracy compared tosupervised methods. Unsupervised methods show potential in certain cases buttypically do not perform as well as supervised models, highlighting the needfor an efficient and cost-effective unsupervised approach. This study presentsa novel unsupervised method, Dual-Encoder Contrastive Learning withMulti-Clustering Voting (DECMCV). The approach involves using a pre-trainedConvNeXt model for image encoding, leveraging VisionTransformer to generatepositive samples, and applying a multi-clustering voting mechanism to addressdata labeling and domain shift issues. Experimental results demonstrate thatDECMCV achieves classification accuracies of 93.78% and 98.29% on the TrashNetand Huawei Cloud datasets, respectively, outperforming or matching supervisedmodels. On a real-world dataset of 4,169 waste images, only 50 labeled sampleswere needed to accurately label thousands, improving classification accuracy by29.85% compared to supervised models. This method effectively addresses styledifferences, enhances model generalization, and contributes to the advancementof automated waste classification.</description>
      <author>example@mail.com (Kui Huang, Mengke Song, Shuo Ba, Ling An, Huajie Liang, Huanxi Deng, Yang Liu, Zhenyu Zhang, Chichun Zhou)</author>
      <guid isPermaLink="false">2503.02241v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving</title>
      <link>http://arxiv.org/abs/2503.03074v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于大语言模型（LLM）的自动驾驶系统BEVDriver，该系统在CARLA模拟器中实现了端到端闭环驾驶。通过利用潜在鸟瞰图特征作为感知输入，结合高效的多视图图像和3D LiDAR点云处理，BEVDriver能够在考虑导航指令和关键场景的情况下预测并规划未来的精确轨迹。&lt;h4&gt;背景&lt;/h4&gt;自主驾驶技术有望为未来交通带来更高的效率，但现有方法在将三维空间定位与大语言模型的语言理解和推理能力结合方面存在挑战。当前的自动驾驶系统需要建立安全性、可靠性和透明度以获得信任。&lt;h4&gt;目的&lt;/h4&gt;通过引入BEVDriver来探索如何利用大语言模型作为通用决策者进行自主驾驶，并解决3D空间定位和LLM语言及推理能力相结合的问题。&lt;h4&gt;方法&lt;/h4&gt;1. 使用了基于大语言模型的端到端闭环自动驾驶系统BEVDriver，该系统在CARLA模拟器中运行。2. 采用了高效的多视图图像处理模块（BEV编码器）来高效地整合来自不同视角的视觉信息以及3D LiDAR点云数据。3. 将感知到的信息转化为潜在空间中的鸟瞰图特征，这些特征在Q-Former模型中传播以与自然语言指令对齐，并传递给LLM进行预测和规划。&lt;h4&gt;主要发现&lt;/h4&gt;BEVDriver系统在LangAuto基准测试上表现出了显著的性能提升，在驾驶得分方面比最先进的方法高出最多18.9%。&lt;h4&gt;结论&lt;/h4&gt;研究证明了大语言模型可以作为一种强大的工具，为自动驾驶中的决策制定提供支持，并展示了将3D感知与LLM结合的有效性。未来的工作可能包括进一步提高系统的鲁棒性和扩展其应用场景。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于如何利用基于大语言模型的系统（BEVDriver）来实现自动驾驶车辆在虚拟环境中的端到端闭环驾驶，通过处理多视角图像和3D LiDAR数据，并结合自然语言指令以规划精确的未来行驶路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous driving has the potential to set the stage for more efficientfuture mobility, requiring the research domain to establish trust through safe,reliable and transparent driving. Large Language Models (LLMs) possessreasoning capabilities and natural language understanding, presenting thepotential to serve as generalized decision-makers for ego-motion planning thatcan interact with humans and navigate environments designed for human drivers.While this research avenue is promising, current autonomous driving approachesare challenged by combining 3D spatial grounding and the reasoning and languagecapabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-endclosed-loop driving in CARLA that utilizes latent BEV features as perceptioninput. BEVDriver includes a BEV encoder to efficiently process multi-viewimages and 3D LiDAR point clouds. Within a common latent space, the BEVfeatures are propagated through a Q-Former to align with natural languageinstructions and passed to the LLM that predicts and plans precise futuretrajectories while considering navigation instructions and critical scenarios.On the LangAuto benchmark, our model reaches up to 18.9% higher performance onthe Driving Score compared to SoTA methods.</description>
      <author>example@mail.com (Katharina Winter, Mark Azer, Fabian B. Flohr)</author>
      <guid isPermaLink="false">2503.03074v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Optimal Transfer Learning for Missing Not-at-Random Matrix Completion</title>
      <link>http://arxiv.org/abs/2503.00174v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了矩阵填充中的迁移学习问题，特别是在数据缺失不随机（MNAR）的情况下，通过使用来自与目标矩阵有潜在特征差异的来源矩阵来解决整个行和列缺失的问题。&lt;h4&gt;背景&lt;/h4&gt;在生物医学等领域的实际问题中经常遇到缺失数据的情况，特别是当缺失是基于某种模式而非随机时，这对传统的矩阵填充方法构成了挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨如何利用包含部分相关但不完整信息的来源矩阵来提高目标矩阵填充的准确性和效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一个估计框架，该框架在主动采样情况下能够达到最小化错误下限。同时考虑了主动和被动行列采样的情况，并建立了相应的理论界限。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法可以通过利用来自源数据的信息来高效地查询目标矩阵中最具有信息量的行列，从而避免了传统方法所需的部分一致性假设，能够在不增加计算复杂度的前提下提高填充精度。&lt;h4&gt;结论&lt;/h4&gt;实验结果证明了所提出的方法在真实生物医学数据集上的有效性，并且比现有的算法表现出色。&lt;h4&gt;翻译&lt;/h4&gt;我们研究的是转移学习在矩阵完成中的应用，在这种情况下，目标矩阵$Q$存在完整的行和列缺失问题。利用一个带有潜在特征变化的不完整来源矩阵$P$来建立两者之间的联系。考虑了主动和被动采样的情形，并为每个场景建立了理论上的最小化错误界限。我们的计算框架在主动采样环境下实现了这一界限，能够通过查询目标矩阵中最具有信息量的行列避免传统方法所需的部分一致性假设，提高了填充精度，并且在真实生物医学数据集上验证了算法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study transfer learning for matrix completion in a Missing Not-at-Random(MNAR) setting that is motivated by biological problems. The target matrix $Q$has entire rows and columns missing, making estimation impossible without sideinformation. To address this, we use a noisy and incomplete source matrix $P$,which relates to $Q$ via a feature shift in latent space. We consider both theactive and passive sampling of rows and columns. We establish minimax lowerbounds for entrywise estimation error in each setting. Our computationallyefficient estimation framework achieves this lower bound for the activesetting, which leverages the source data to query the most informative rows andcolumns of $Q$. This avoids the need for incoherence assumptions required forrate optimality in the passive sampling setting. We demonstrate theeffectiveness of our approach through comparisons with existing algorithms onreal-world biological datasets.</description>
      <author>example@mail.com (Akhil Jalan, Yassir Jedra, Arya Mazumdar, Soumendu Sundar Mukherjee, Purnamrita Sarkar)</author>
      <guid isPermaLink="false">2503.00174v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DELST: Dual Entailment Learning for Hyperbolic Image-Gene Pretraining in Spatial Transcriptomics</title>
      <link>http://arxiv.org/abs/2503.00804v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为DELST的框架，用于嵌入双曲表示并建模层级关系，从而实现图像-基因预训练。&lt;h4&gt;背景&lt;/h4&gt;空间转录组学(ST)能够以个体点的方式映射组织内的基因表达，并且具有丰富的跨模式和模式内部层次信息。然而，现有的方法依赖于对比对齐图像-基因对，无法准确捕捉ST数据中的复杂层级关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架DELST，该框架能够在建模层次结构的同时嵌入双曲表示，以实现更有效的图像-基因预训练。&lt;h4&gt;方法&lt;/h4&gt;1. 跨模式蕴涵学习：建立基因和图像之间的顺序关系，以增强图像表示的泛化能力。2. 同一模式内蕴涵学习：编码基因表达模式为层级关系，并指导不同样本间的全局层次学习。&lt;h4&gt;主要发现&lt;/h4&gt;DELST框架在标注病理学家注释的空间转录组学基准上的广泛实验中展示了其有效性，实现了比现有方法更好的预测性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用双曲空间中的层级表示进行图像-基因的预训练可以显著提高模型的预测准确性。相关代码和模型可在https://github.com/XulinChen/DELST获取。&lt;h4&gt;翻译&lt;/h4&gt;摘要：空间转录组学(ST)能够以个体点的方式映射组织内的基因表达，成为多模态表现学习中的宝贵资源。此外，ST数据本身在跨模式以及同一模式内部均包含丰富的层级信息。例如，不同位置的非零基因表达数量各不相同，对应着不同的细胞活动水平和语义层次结构。然而，现有方法依赖于图像-基因对之间的对比对齐方式，无法准确捕捉到ST数据中复杂的层级关系。因此，我们提出了DELST框架，这是第一个在两个层面上建模层级并将双曲表示嵌入到图像-基因预训练中的框架：1) 跨模式蕴涵学习，建立基因和图像之间的一种顺序关系来增强图像表示的泛化能力；2) 同一模式内蕴涵学习，编码基因表达模式为层级关系，并在全球范围内指导不同样本间的层次学习。病理学家注释的空间转录组学基准上的广泛实验表明了我们框架的有效性，在预测性能上优于现有方法。我们的代码和模型可在https://github.com/XulinChen/DELST获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spatial transcriptomics (ST) maps gene expression within tissue at individualspots, making it a valuable resource for multimodal representation learning.Additionally, ST inherently contains rich hierarchical information both acrossand within modalities. For instance, different spots exhibit varying numbers ofnonzero gene expressions, corresponding to different levels of cellularactivity and semantic hierarchies. However, existing methods rely oncontrastive alignment of image-gene pairs, failing to accurately capture theintricate hierarchical relationships in ST data. Here, we propose DELST, thefirst framework to embed hyperbolic representations while modeling hierarchyfor image-gene pretraining at two levels: (1) Cross-modal entailment learning,which establishes an order relationship between genes and images to enhanceimage representation generalization; (2) Intra-modal entailment learning, whichencodes gene expression patterns as hierarchical relationships, guidinghierarchical learning across different samples at a global scale andintegrating biological insights into single-modal representations. Extensiveexperiments on ST benchmarks annotated by pathologists demonstrate theeffectiveness of our framework, achieving improved predictive performancecompared to existing methods. Our code and models are available at:https://github.com/XulinChen/DELST.</description>
      <author>example@mail.com (Xulin Chen, Junzhou Huang)</author>
      <guid isPermaLink="false">2503.00804v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Precoding in Multi-user Multi-antenna Systems: Transformer or Graph Transformer?</title>
      <link>http://arxiv.org/abs/2503.02998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了Transformer模型在除信道获取任务外的多用户多天线系统中的预编码策略学习能力，并提出了一种新的Graph Transformers架构，以充分利用基础带和混合预编码策略的置换性质。&lt;h4&gt;背景&lt;/h4&gt;Transformers在通道预测等任务中表现出色，而图神经网络（GNNs）则适用于各种通信任务。但是，关于Transformer是否能在非信道获取任务上有效以及如何结合两者的优势尚不清楚。&lt;h4&gt;目的&lt;/h4&gt;通过研究多用户多天线系统中的预编码策略学习问题，探讨和验证在特定场景下同时利用Transformers和图神经网络优势的方法，并提出新的Graph Transformers架构。&lt;h4&gt;方法&lt;/h4&gt;构建了基于异构图的GNNs与Transformer之间的关系模型。提出了二维（2D）和三维（3D）图变换器（Gformers），这两种模型分别用于学习基础带和混合预编码策略中的置换性质。通过模拟实验评估并比较了它们的学习性能、推理复杂度、训练复杂度以及规模泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;1. 为适应不同数量的用户，需要考虑多用户干扰问题，这可以通过定制Transformer来解决。2. Transformer仅能部分利用预编码策略中的置换性质，并不能适用于变化的天线数目，这种情况与在同质图上学习的GNN相同。3. 利用异构图上的GNNs和Transformers之间的关系建立Graph Transformers，可以更好地处理基于带宽以及混合模式下的预编码策略问题。&lt;h4&gt;结论&lt;/h4&gt;通过实验表明，所提出的2D-和3D-Gformers在学习性能、推理复杂度、训练复杂度等方面均优于传统的Transformer和图神经网络（GNNs），并且具备更好的规模泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：Transformers已被设计用于通道获取任务（如信道预测）以及其他诸如预编码的任务，而图神经网络(GNNs)在学习多种通信任务方面已显示出其高效性。然而，关于Transformer是否适用于除信道获取之外的任务以及如何利用这两种架构的优势尚不明确。本文以多用户多天线系统中的预编码策略学习为例来解答这些问题。我们注意到针对预编码定制的Transformer可以反映多用户干扰问题，这对于它在用户数量上的泛化能力是至关重要的。然而，这种定制的Transformer只能利用部分预编码策略的置换性质，并且无法适用于变化的天线数目，与基于同质图进行学习的GNN相同。为了提供有用的见解，我们建立了Transformers和学习异构图上GNNs之间的关系。在此基础上，我们提出了Graph Transformers（即2D-和3D-Gformers），用于挖掘基础带预编码和混合预编码策略中的置换性质。通过模拟实验评估并比较了Gformers的学习性能、推理复杂度、训练复杂度以及规模泛化能力与Transformer和GNNs的对比结果。&lt;h4&gt;创新点&lt;/h4&gt;提出了Graph Transformers，即2D-和3D-Gformers，用于更好地处理基础带及混合模式下的预编码策略问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have been designed for channel acquisition tasks such as channelprediction and other tasks such as precoding, while graph neural networks(GNNs) have been demonstrated to be efficient for learning a multitude ofcommunication tasks. Nonetheless, whether or not Transformers are efficient forthe tasks other than channel acquisition and how to reap the benefits of botharchitectures are less understood. In this paper, we take learning precodingpolicies in multi-user multi-antenna systems as an example to answer thequestions. We notice that a Transformer tailored for precoding can reflectmultiuser interference, which is essential for its generalizability to thenumber of users. Yet the tailored Transformer can only leverage partialpermutation property of precoding policies and hence is not generalizable tothe number of antennas, same as a GNN learning over a homogeneous graph. Toprovide useful insight, we establish the relation between Transformers and theGNNs that learn over heterogeneous graphs. Based on the relation, we proposeGraph Transformers, namely 2D- and 3D-Gformers, for exploiting the permutationproperties of baseband precoding and hybrid precoding policies. The learningperformance, inference and training complexity, and size-generalizability ofthe Gformers are evaluated and compared with Transformers and GNNs viasimulations.</description>
      <author>example@mail.com (Yuxuan Duan, Jia Guo, Chenyang Yang)</author>
      <guid isPermaLink="false">2503.02998v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning machine-learned particle-flow reconstruction for new detector geometries in future colliders</title>
      <link>http://arxiv.org/abs/2503.00131v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, 13 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文展示了通过机器学习算法进行粒子流重建的迁移学习能力，特别是在高能粒子对撞机中从一个探测器设计转移到另一个设计的有效性。&lt;h4&gt;背景&lt;/h4&gt;在高能量粒子物理实验中，利用机器学习技术优化粒子流量的计算是一种创新的方法。然而，如何将这种训练过的模型迁移到不同的探测器上是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;研究通过迁移学习，使用初始大型全仿真数据集在一个探测器设计上预训练算法模型，并在不同对撞机和探测器设计的数据样本上进行微调的有效性。&lt;h4&gt;方法&lt;/h4&gt;利用紧凑型线性对撞机（CLICdet）模型作为初始训练集，在未来环形正负电子对撞机的电子-正电子模式下提议的类似CLIC的设计（CLD）中实现成功知识转移。通过在第二个数据集中使用少一个数量级的数据样本，来展示与从头开始昂贵训练相同的性能。&lt;h4&gt;主要发现&lt;/h4&gt;迁移学习模型仅需在10万次CLD事件后，在事件层面指标上达到了传统规则基础粒子流方法的类似性能；而未经微调的全新模型则至少需要1百万次CLD事件才能实现类似的重建效果。这是首次针对全仿真跨探测器转移学习研究。&lt;h4&gt;结论&lt;/h4&gt;这些结果对于构建可以适应不同探测器设计和几何形状的大规模物理模型提供了宝贵的见解，有助于加速新探测器的发展周期，并为利用机器学习进行快速的探测器设计和优化开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要中英文对照&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We demonstrate transfer learning capabilities in a machine-learned algorithmtrained for particle-flow reconstruction in high energy particle colliders.This paper presents a cross-detector fine-tuning study, where we initiallypre-train the model on a large full simulation dataset from one detectordesign, and subsequently fine-tune the model on a sample with a differentcollider and detector design. Specifically, we use the Compact Linear Colliderdetector (CLICdet) model for the initial training set, and demonstratesuccessful knowledge transfer to the CLIC-like detector (CLD) proposed for theFuture Circular Collider in electron-positron mode (FCC-ee). We show that withan order of magnitude less samples from the second dataset, we can achieve thesame performance as a costly training from scratch, across particle-level andevent-level performance metrics; including jet resolution and missingtransverse momentum resolution. Furthermore, we find that the fine-tuned modelachieves comparable performance to the traditional rule-based particle-flowapproach on event-level metrics after training on 100,000 CLD events, whereas amodel trained from scratch requires at least 1 million CLD events to achievesimilar reconstruction performance. To our knowledge, this represents the firstfull-simulation cross-detector transfer learning study for particle-flow. Thesefindings offer valuable insights towards building large physics models that canbe fine-tuned across different detector designs and geometries, helpingaccelerate the development cycle for new detectors, and opening the door torapid detector design and optimization using machine learning.</description>
      <author>example@mail.com (Farouk Mokhtar, Joosep Pata, Michael Kagan, Dolores Garcia, Eric Wulff, Mengke Zhang, Javier Duarte)</author>
      <guid isPermaLink="false">2503.00131v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Language-Guided Visual Perception Disentanglement for Image Quality Assessment and Conditional Image Generation</title>
      <link>http://arxiv.org/abs/2503.02206v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多模态解耦表示学习框架，旨在解决对比视觉语言模型在图像质量评估和条件生成任务中难以控制感知特性的挑战。&lt;h4&gt;背景&lt;/h4&gt;当前的对比视觉-语言模型（如CLIP）基于大规模I&amp;1T数据集进行训练，在语义识别任务上表现出色。然而，这种多模态表示主要强调语义而忽视了对感知特性精确控制的需求，这在图像质量评估和条件生成等任务中是不利的。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用解耦文本引导图像解耦的新框架，以改善上述视觉任务中的性能表现。&lt;h4&gt;方法&lt;/h4&gt;首先构建I&amp;2T数据集，该数据集包含每个图像的感知性和语义性两个独立描述。然后使用这些解耦的文字作为监督信号来从CLIP的原始特征空间中分离出纯粹的感知表示，并将其命名为DeCLIP框架。最后利用这种解耦后的特性表示来进行图像质量评估和条件生成。&lt;h4&gt;主要发现&lt;/h4&gt;通过大量的实验与对比，表明所提出的方法在两个流行任务上具有优势。&lt;h4&gt;结论&lt;/h4&gt;论文展示了一种创新性的解决现有视觉语言模型局限性的问题方法，并且研究团队承诺会公开数据集、代码以及模型。&lt;h4&gt;翻译&lt;/h4&gt;对比视觉-语言模型（如CLIP）已经在语义识别任务中显示出了强大的零样本能力，这主要是由于它们在大规模I&amp;1T数据集上的训练。这种多模态表示通常混合了语义和感知元素，并特别强调语义。然而，对于图像质量评估和条件图像生成等流行的任务来说，需要对感知和语义特征进行精细控制，则这一特点可能会成为问题。受上述事实启发，本文提出了一种新的多模态解耦表示学习框架，利用了解耦的文本引导图像解耦。为此，我们首先构建了一个I&amp;2T数据集，该数据集中每个图像都有一个独立的感知性文本描述和语义性文本描述。然后使用这些解耦的文字作为监督信号来从CLIP的原始特征空间中分离出纯粹的感知表示，并将其命名为DeCLIP。最后利用这种解耦后的特性表示来进行图像质量评估（技术质量和美学质量）和条件生成。大量的实验和对比表明，所提出的方法在这两个流行任务上具有优势。数据集、代码和模型将会公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive vision-language models, such as CLIP, have demonstrated excellentzero-shot capability across semantic recognition tasks, mainly attributed tothe training on a large-scale I&amp;1T (one Image with one Text) dataset. This kindof multimodal representations often blend semantic and perceptual elements,placing a particular emphasis on semantics. However, this could be problematicfor popular tasks like image quality assessment (IQA) and conditional imagegeneration (CIG), which typically need to have fine control on perceptual andsemantic features. Motivated by the above facts, this paper presents a newmultimodal disentangled representation learning framework, which leveragesdisentangled text to guide image disentanglement. To this end, we first buildan I&amp;2T (one Image with a perceptual Text and a semantic Text) dataset, whichconsists of disentangled perceptual and semantic text descriptions for animage. Then, the disentangled text descriptions are utilized as supervisorysignals to disentangle pure perceptual representations from CLIP's original`coarse' feature space, dubbed DeCLIP. Finally, the decoupled featurerepresentations are used for both image quality assessment (technical qualityand aesthetic quality) and conditional image generation. Extensive experimentsand comparisons have demonstrated the advantages of the proposed method on thetwo popular tasks. The dataset, code, and model will be available.</description>
      <author>example@mail.com (Zhichao Yang, Leida Li, Pengfei Chen, Jinjian Wu, Giuseppe Valenzise)</author>
      <guid isPermaLink="false">2503.02206v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Attention Fusion of MRI and Jacobian Maps for Alzheimer's Disease Diagnosis</title>
      <link>http://arxiv.org/abs/2503.00586v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to MICCAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种利用交叉注意力融合框架的方法，旨在通过结合结构磁共振成像（sMRI）强度和雅可比行列式图（JSM）来提高阿尔茨海默病（AD）早期诊断的准确性。&lt;h4&gt;背景&lt;/h4&gt;阿尔茨海默病的早期诊断至关重要。虽然结构磁共振成像广泛用于该疾病的诊断，但传统的深度学习方法主要依赖于基于强度的特征，这需要大量数据集才能捕捉到微妙的变化。而雅可比行列式图提供了有关局部脑变形的补充信息。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的跨模态融合策略——交叉注意力融合框架，以更好地结合sMRI和JSM的信息进行AD分类。&lt;h4&gt;方法&lt;/h4&gt;利用阿尔茨海默病神经影像学倡议（ADNI）数据集，在四种预训练的3D图像编码器上对比了三种不同的注意机制：交叉注意力、成对自注意力和瓶颈注意力。&lt;h4&gt;主要发现&lt;/h4&gt;与其它两种方法相比，交叉注意力融合框架在区分AD患者和认知正常个体以及轻度认知障碍（MCI）个体和认知正常个体方面表现出更好的性能。同时，该模型参数量较少，具有高计算效率。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了使用交叉注意力融合框架可以提高阿尔茨海默病诊断的准确性和效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，描述了早期诊断阿尔茨海默病的重要性，并介绍了通过结合结构MRI和JSM的信息来改进该疾病诊断的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Early diagnosis of Alzheimer's disease (AD) is critical for interventionbefore irreversible neurodegeneration occurs. Structural MRI (sMRI) is widelyused for AD diagnosis, but conventional deep learning approaches primarily relyon intensity-based features, which require large datasets to capture subtlestructural changes. Jacobian determinant maps (JSM) provide complementaryinformation by encoding localized brain deformations, yet existing multimodalfusion strategies fail to fully integrate these features with sMRI. We proposea cross-attention fusion framework to model the intrinsic relationship betweensMRI intensity and JSM-derived deformations for AD classification. Using theAlzheimer's Disease Neuroimaging Initiative (ADNI) dataset, we comparecross-attention, pairwise self-attention, and bottleneck attention with fourpre-trained 3D image encoders. Cross-attention fusion achieves superiorperformance, with mean ROC-AUC scores of 0.903 (+/-0.033) for AD vs.cognitively normal (CN) and 0.692 (+/-0.061) for mild cognitive impairment(MCI) vs. CN. Despite its strong performance, our model remains highlyefficient, with only 1.56 million parameters--over 40 times fewer thanResNet-34 (63M) and Swin UNETR (61.98M). These findings demonstrate thepotential of cross-attention fusion for improving AD diagnosis whilemaintaining computational efficiency.</description>
      <author>example@mail.com (Shijia Zhang, Xiyu Ding, Brian Caffo, Junyu Chen, Cindy Zhang, Hadi Kharrazi, Zheyu Wang)</author>
      <guid isPermaLink="false">2503.00586v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Foundation-Model-Boosted Multimodal Learning for fMRI-based Neuropathic Pain Drug Response Prediction</title>
      <link>http://arxiv.org/abs/2503.00210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于fMRI的神经病理性疼痛药物反应预测的方法FMM$_{TC}$，该方法通过结合疼痛特异性的多模态信息和来自广泛无痛数据集的知识，克服了现有单一模式fMRI模型的局限性。&lt;h4&gt;背景&lt;/h4&gt;神经病理性疼痛影响高达10%的成年人群，治疗效果有限且耐受性差。静息态功能磁共振成像(rs-fMRI)是预测药物反应的重要工具，但由于数据稀缺和方法复杂度高，其应用受限。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够利用有限疼痛特异性数据并整合外部知识的方法FMM$_{TC}$，以提高神经病理性疼痛治疗药物反应的预测准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个基于fMRI基础模型增强的多模态学习框架FMM$_{TC}$，该框架结合了rs-fMRI的时间序列和功能连接两种模式的信息，并从大量无痛数据集中获取外部知识来补充有限的数据。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，FMM$_{TC}$在内部和公共数据集上均表现出优越的表示能力、泛化能力和跨数据集适应性。消融研究验证了多模态学习和基础模型驱动的知识转移的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过准确预测药物反应以提高临床试验中的参与者分层效率，FMM$_{TC}$有助于神经病理性疼痛治疗的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要：由10%的成年人受到的影响，神经病理性疼痛由于有限的疗效和耐受性仍难以治疗。尽管静息态功能磁共振成像(rs-fMRI)是用于药物反应预测的脑生物标志物的重要非侵入式测量方法，fMRI的复杂性要求具有强大容量的机器学习模型。然而，在神经病理性疼痛研究中的数据稀缺限制了高容量模型的应用。为了解决数据匮乏的问题，我们提出了FMM$_{TC}$，一种用于基于fMRI的神经病理性疼痛药物反应预测的基础模型增强多模态学习框架，该框架利用了疼痛特异性内部多模态信息和来自广泛无痛基础模型的知识。具体来说，为了最大化有限疼痛特异性数据的价值，FMM$_{TC}$整合了两种rs-fMRI模式：时间序列和功能连接。进一步地，通过从大量无痛无关fMRI数据集中获取的外部知识来增强FMM$_{TC}$。使用内部和公共开放神经数据集进行的评估表明，与只考虑一种rs-fMRI模态的现有单模态fMRI模型相比，FMM$_{TC}$具有更好的表示能力、泛化能力和跨数据集适应性。消融研究验证了多模式学习以及由基础模型驱动的知识转移的有效性。基于集成梯度解释的研究说明了FMM$_{TC}$如何通过动态行为增强其跨数据集的适应性。综上所述，FMM$_{TC}$通过准确预测药物反应以提高参与者分层效率来支持神经病理性疼痛治疗的发展临床试验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Neuropathic pain, affecting up to 10% of adults, remains difficult to treatdue to limited therapeutic efficacy and tolerability. Although resting-statefunctional MRI (rs-fMRI) is a promising non-invasive measurement of brainbiomarkers to predict drug response in therapeutic development, the complexityof fMRI demands machine learning models with substantial capacity. However,extreme data scarcity in neuropathic pain research limits the application ofhigh-capacity models. To address the challenge of data scarcity, we proposeFMM$_{TC}$, a Foundation-Model-boosted Multimodal learning framework forfMRI-based neuropathic pain drug response prediction, which leverages bothinternal multimodal information in pain-specific data and external knowledgefrom large pain-agnostic data. Specifically, to maximize the value of limitedpain-specific data, FMM$_{TC}$ integrates complementary information from twors-fMRI modalities: Time series and functional Connectivity. FMM$_{TC}$ isfurther boosted by an fMRI foundation model with its external knowledge fromextensive pain-agnostic fMRI datasets enriching limited pain-specificinformation. Evaluations with an in-house dataset and a public dataset fromOpenNeuro demonstrate FMM$_{TC}$'s superior representation ability,generalizability, and cross-dataset adaptability over existing unimodal fMRImodels that only consider one of the rs-fMRI modalities. The ablation studyvalidates the effectiveness of multimodal learning and foundation-model-poweredexternal knowledge transfer in FMM$_{TC}$. An integrated gradient-basedinterpretation study explains how FMM$_{TC}$'s cross-dataset dynamic behaviorsenhance its adaptability. In conclusion, FMM$_{TC}$ boosts clinical trials inneuropathic pain therapeutic development by accurately predicting drugresponses to improve the participant stratification efficiency.</description>
      <author>example@mail.com (Wenrui Fan, L. M. Riza Rizky, Jiayang Zhang, Chen Chen, Haiping Lu, Kevin Teh, Dinesh Selvarajah, Shuo Zhou)</author>
      <guid isPermaLink="false">2503.00210v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Feynman-Kac Correctors in Diffusion: Annealing, Guidance, and Product of Experts</title>
      <link>http://arxiv.org/abs/2503.02819v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于费曼-卡克公式和加权模拟方案的新颖采样方法，用于从一系列预训练的评分模型导出的退火、几何平均或乘积分布中进行抽样。&lt;h4&gt;背景&lt;/h4&gt;分数生成模型是跨多个领域的首选模型。然而，在推理时控制这些模型行为的有效工具有限，尤其是在组合多个预训练模型的情况下。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于费曼-卡克公式的加权模拟方案（FKCs），用于从一系列退火、几何平均或乘积分布中进行采样，并改进分类器自由引导方法和分子生成任务。&lt;h4&gt;方法&lt;/h4&gt;通过仔细考虑适当的偏微分方程中的各项，提出了费曼-卡克校正器（FKC）加权模拟方案。为模拟这些PDE，提议使用基于推理时间缩放的顺序蒙特卡洛重采样算法来提高抽样质量。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了该方法在预训练模型中改进多目标分子生成和文本到图像生成中的分类器自由引导的有效性，并提出了通过推理时间温度退火进行快速采样的方案。&lt;h4&gt;结论&lt;/h4&gt;提出了一种有效且原理性的方法，用于从一系列预先训练的评分模型导出的分布中抽样。该方法不仅可以改善现有技术，在特定任务上也表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：尽管分数生成模型在多个领域中是首选模型，但在推理时控制这些模型行为的有效工具有限，尤其是在组合多个预训练模型的情况下。现有的分类器自由引导方法使用简单的启发式方法来混合条件和无条件评分以从条件分布采样。然而，这样的方法并不逼近中间的分布，因此需要额外的'校正'步骤。在这项工作中，我们提供了一种高效且原理性的方法用于从一系列退火、几何平均或乘积分布中进行抽样，这些分布是从预训练的评分模型导出的。基于著名的费曼-卡克公式，并通过仔细考虑适当的偏微分方程（PDEs）中的各项，我们提出加权模拟方案，称为费曼-卡克校正器（FKCs）。为了模拟这些PDE，我们提出了顺序蒙特卡洛重采样算法，利用推理时间缩放来提高抽样的质量。通过在预训练模型中改进多目标分子生成和文本到图像生成中的分类器自由引导的实用性，以及提出通过推理时间温度退火进行快速采样的方案，我们在实验上证明了我们方法的有效性。我们的代码可在https://github.com/martaskrt/fkc-diffusion获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While score-based generative models are the model of choice across diversedomains, there are limited tools available for controlling inference-timebehavior in a principled manner, e.g. for composing multiple pretrained models.Existing classifier-free guidance methods use a simple heuristic to mixconditional and unconditional scores to approximately sample from conditionaldistributions. However, such methods do not approximate the intermediatedistributions, necessitating additional 'corrector' steps. In this work, weprovide an efficient and principled method for sampling from a sequence ofannealed, geometric-averaged, or product distributions derived from pretrainedscore-based models. We derive a weighted simulation scheme which we callFeynman-Kac Correctors (FKCs) based on the celebrated Feynman-Kac formula bycarefully accounting for terms in the appropriate partial differentialequations (PDEs). To simulate these PDEs, we propose Sequential Monte Carlo(SMC) resampling algorithms that leverage inference-time scaling to improvesampling quality. We empirically demonstrate the utility of our methods byproposing amortized sampling via inference-time temperature annealing,improving multi-objective molecule generation using pretrained models, andimproving classifier-free guidance for text-to-image generation. Our code isavailable at https://github.com/martaskrt/fkc-diffusion.</description>
      <author>example@mail.com (Marta Skreta, Tara Akhound-Sadegh, Viktor Ohanesian, Roberto Bondesan, Alán Aspuru-Guzik, Arnaud Doucet, Rob Brekelmans, Alexander Tong, Kirill Neklyudov)</author>
      <guid isPermaLink="false">2503.02819v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ArticuBot: Learning Universal Articulated Object Manipulation Policy via Large Scale Simulation</title>
      <link>http://arxiv.org/abs/2503.03045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文介绍了ArticuBot，这是一种单个学习策略可以使机器人系统在现实世界中打开各种未见过的铰接物体的技术。由于这些对象的几何形状、大小和铰链类型存在巨大差异，这一任务长期以来对机器人技术来说一直是个挑战。我们的系统Articubot由三个部分组成：在基于物理的模拟环境中生成大量演示，在点云基础上通过模仿学习将所有生成的演示提炼成神经策略，并进行零样本仿真到现实转移至实际机器人系统中。&lt;h4&gt;背景&lt;/h4&gt;传统的机器人任务难以处理具有多种几何形状、大小和铰链类型的未见过物体，而Articubot旨在解决这一挑战&lt;h4&gt;目的&lt;/h4&gt;开发一种基于学习的策略，使机器人能够打开从未见过的不同种类铰接式对象，同时能够在不同环境下实现零样本仿真到现实转移。&lt;h4&gt;方法&lt;/h4&gt;包括在物理模拟中生成大量演示、通过模仿学习提炼成点云神经策略，并进行仿真实验与真实机器人的零样本迁移。此外，提出了一个层次化的策略表示模型和一个新的加权位移模型。&lt;h4&gt;主要发现&lt;/h4&gt;该研究展示了Articubot能够在不同实验室、客厅以及厨房环境下打开多种未见过的铰接式物体的能力，证明了学习到的策略具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过结合模拟数据生成、模仿学习和零样本仿真到现实迁移技术，ArticuBot成功地实现了机器人在面对未知铰接物体时的有效操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents ArticuBot, in which a single learned policy enables arobotics system to open diverse categories of unseen articulated objects in thereal world. This task has long been challenging for robotics due to the largevariations in the geometry, size, and articulation types of such objects. Oursystem, Articubot, consists of three parts: generating a large number ofdemonstrations in physics-based simulation, distilling all generateddemonstrations into a point cloud-based neural policy via imitation learning,and performing zero-shot sim2real transfer to real robotics systems. Utilizingsampling-based grasping and motion planning, our demonstration generalizationpipeline is fast and effective, generating a total of 42.3k demonstrations over322 training articulated objects. For policy learning, we propose a novelhierarchical policy representation, in which the high-level policy learns thesub-goal for the end-effector, and the low-level policy learns how to move theend-effector conditioned on the predicted goal. We demonstrate that thishierarchical approach achieves much better object-level generalization comparedto the non-hierarchical version. We further propose a novel weighteddisplacement model for the high-level policy that grounds the prediction intothe existing 3D structure of the scene, outperforming alternative policyrepresentations. We show that our learned policy can zero-shot transfer tothree different real robot settings: a fixed table-top Franka arm across twodifferent labs, and an X-Arm on a mobile base, opening multiple unseenarticulated objects across two labs, real lounges, and kitchens. Videos andcode can be found on our project website: https://articubot.github.io/.</description>
      <author>example@mail.com (Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held)</author>
      <guid isPermaLink="false">2503.03045v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>V$^2$Dial: Unification of Video and Visual Dialog via Multimodal Experts</title>
      <link>http://arxiv.org/abs/2503.02063v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;我们提出了V$^2$Dial - 一种新的专家模型，专门用于同时处理图像和视频输入数据以进行多模态对话任务。&lt;h4&gt;背景描述&lt;/h4&gt;现有的多模态模型主要集中在较为简单的任务上（例如视觉问答、视频问答、视频文本检索），而忽视了更具挑战性的对话类任务（如基于视频或可视/图像的对话）。此外，关于这些任务的工作各自独立发展，尽管它们之间有明显的相似性，这限制了其潜在的应用。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种单一模型来统一这些对话任务，并首次联合学习图像和视频的空间和时间特征。通过专门的专家处理输入并通过匹配和对比学习技术对齐。&lt;h4&gt;方法描述&lt;/h4&gt;使用专用专家将图像和视频数据路由并利用匹配与对比学习技术进行特征对齐，以系统地研究两种任务之间的领域迁移问题。&lt;h4&gt;主要发现&lt;/h4&gt;模型在AVSD和VisDial等广泛使用的对话数据集上进行了广泛的评估，在零样本和微调设置下均达到了新的最先进的结果。&lt;h4&gt;结论&lt;/h4&gt;V$^2$Dial通过将图像和视频的时空特征联合学习，提供了一种新颖的方法来处理多模态对话任务，并在多个基准测试中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present V$^2$Dial - a novel expert-based model specifically geared towardssimultaneously handling image and video input data for multimodalconversational tasks. Current multimodal models primarily focus on simplertasks (e.g., VQA, VideoQA, video-text retrieval) and often neglect the morechallenging conversational counterparts, such as video and visual/image dialog.Moreover, works on both conversational tasks evolved separately from each otherdespite their apparent similarities limiting their applicability potential. Tothis end, we propose to unify both tasks using a single model that for thefirst time jointly learns the spatial and temporal features of images andvideos by routing them through dedicated experts and aligns them using matchingand contrastive learning techniques. Furthermore, we systemically study thedomain shift between the two tasks by investigating whether and to what extentthese seemingly related tasks can mutually benefit from their respectivetraining data. Extensive evaluations on the widely used video and visual dialogdatasets of AVSD and VisDial show that our model achieves new state-of-the-artresults across four benchmarks both in zero-shot and fine-tuning settings.</description>
      <author>example@mail.com (Adnen Abdessaied, Anna Rohrbach, Marcus Rohrbach, Andreas Bulling)</author>
      <guid isPermaLink="false">2503.02063v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Out-of-Distribution Generalization on Graphs via Progressive Inference</title>
      <link>http://arxiv.org/abs/2503.02988v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了GPro模型，该模型采用逐步推理的方法学习图的因果不变性，以提高在数据分布变化情况下的预测性能。&lt;h4&gt;背景&lt;/h4&gt;目前大多数图形神经网络（GNNs）假设独立同分布的数据环境，在实际应用中这种假设往往并不成立。当数据分布发生显著偏移时，现有的GNN模型常常无法产生可靠的预测结果。&lt;h4&gt;目的&lt;/h4&gt;旨在通过提取输入图中的因果不变部分来改善模型的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一个名为GPro的新模型，该模型采用逐步推理的方式学习图的因果不变性，并通过创建反事实样本扩大训练分布以提升其在捕捉因果不变性方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;与现有方法相比，GPro模型能够更好地识别并利用数据中的因果不变部分，在数据分布显著变化时仍能保持较高的预测准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，GPro在一系列基准测试上优于当前最先进的方法，并且在更极端的数据分布偏移情况下表现尤为出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development and evaluation of graph neural networks (GNNs) generallyfollow the independent and identically distributed (i.i.d.) assumption. Yetthis assumption is often untenable in practice due to the uncontrollable datageneration mechanism. In particular, when the data distribution shows asignificant shift, most GNNs would fail to produce reliable predictions and mayeven make decisions randomly. One of the most promising solutions to improvethe model generalization is to pick out causal invariant parts in the inputgraph. Nonetheless, we observe a significant distribution gap between thecausal parts learned by existing methods and the ground truth, leading toundesirable performance. In response to the above issues, this paper presentsGPro, a model that learns graph causal invariance with progressive inference.Specifically, the complicated graph causal invariant learning is decomposedinto multiple intermediate inference steps from easy to hard, and theperception of GPro is continuously strengthened through a progressive inferenceprocess to extract causal features that are stable to distribution shifts. Wealso enlarge the training distribution by creating counterfactual samples toenhance the capability of the GPro in capturing the causal invariant parts.Extensive experiments demonstrate that our proposed GPro outperforms thestate-of-the-art methods by 4.91% on average. For datasets with more severedistribution shifts, the performance improvement can be up to 6.86%.</description>
      <author>example@mail.com (Yiming Xu, Bin Shi, Zhen Peng, Huixiang Liu, Bo Dong, Chen Chen)</author>
      <guid isPermaLink="false">2503.02988v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ArcPro: Architectural Programs for Structured 3D Abstraction of Sparse Points</title>
      <link>http://arxiv.org/abs/2503.02745v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025 (Patent Protected); Project page:  https://vcc.tech/research/2025/ArcPro&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的学习框架ArcPro，该框架基于架构程序从稀疏和低质量的点云中恢复结构化的3D抽象。&lt;h4&gt;背景&lt;/h4&gt;现有技术在处理稀疏且低质量的点云数据时存在不足，无法有效地从中提取结构信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决上述问题，并通过使用架构程序作为桥梁连接前馈和逆向生成过程以提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;{'设计领域特定语言(DSL)': '用DSL分层表示建筑结构为一个程序，该程序可以高效地转换成网格。', '训练数据合成': '利用前馈处理来实现训练数据的合成。', '编码器-解码器网络': '通过在点云和架构程序之间进行配对训练了一个编码器-解码器网络，其中3D卷积编码器提取点云特征，而Transformer解码器自回归地预测标记化形式的程序。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'高效推断': '所提出的方法在推理过程中非常高效，并且能够生成合理和忠实的3D抽象。', '性能优越': '实验表明ArcPro优于传统的建筑代理重建方法以及基于学习的抽象方法。', '扩展应用': '进一步探索了其与多视图图像和自然语言输入配合工作的潜力。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入ArcPro，为从稀疏且低质量的点云中恢复结构化3D抽象提供了一种新的解决方案，并展示了该框架在建筑领域和其他相关领域的潜在应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们在论文摘要的基础上，将信息以分点的形式进行了总结和提炼。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce ArcPro, a novel learning framework built on architecturalprograms to recover structured 3D abstractions from highly sparse andlow-quality point clouds. Specifically, we design a domain-specific language(DSL) to hierarchically represent building structures as a program, which canbe efficiently converted into a mesh. We bridge feedforward and inverseprocedural modeling by using a feedforward process for training data synthesis,allowing the network to make reverse predictions. We train an encoder-decoderon the points-program pairs to establish a mapping from unstructured pointclouds to architectural programs, where a 3D convolutional encoder extractspoint cloud features and a transformer decoder autoregressively predicts theprograms in a tokenized form. Inference by our method is highly efficient andproduces plausible and faithful 3D abstractions. Comprehensive experimentsdemonstrate that ArcPro outperforms both traditional architectural proxyreconstruction and learning-based abstraction methods. We further explore itspotential to work with multi-view image and natural language inputs.</description>
      <author>example@mail.com (Qirui Huang, Runze Zhang, Kangjun Liu, Minglun Gong, Hao Zhang, Hui Huang)</author>
      <guid isPermaLink="false">2503.02745v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Recognition of Dysarthria in Amyotrophic Lateral Sclerosis patients using Hypernetworks</title>
      <link>http://arxiv.org/abs/2503.01892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本论文提出了一种使用超网络识别ALS患者失语症的新方法，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;肌萎缩侧索硬化症（ALS）是一种进行性的神经退行性疾病，症状多样，包括言语清晰度下降。现有研究依赖于特征提取策略和定制的卷积神经网络来预测临床标准ALSFRS-R以识别失语症。&lt;h4&gt;目的&lt;/h4&gt;提出一种使用超网络识别ALS患者失语症的新方法，并评估其相对于其他基准模型的优势。&lt;h4&gt;方法&lt;/h4&gt;采用音频文件，将其转换为log-Mel频谱图、delta和delta-delta形式，并通过预训练的修改版AlexNet模型处理。最后利用生成目标网络权重的超网络来完成识别任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验在新收集的公共数据集VOC-ALS上进行，结果显示所提出的算法可以达到高达82.66%的准确率，优于包括多模态融合方法在内的强基准模型。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了超网络应用到ALS失语症识别中的价值和优势，在泛化能力、参数效率以及鲁棒性方面超过了当前最先进的结果。&lt;h4&gt;翻译&lt;/h4&gt;肌萎缩侧索硬化症（ALS）是一种进行性的神经退行性疾病，具有多样的症状，包括言语清晰度下降。现有研究通过预测临床标准ALSFRS-R来识别ALS患者的失语症，依赖于特征提取策略和定制的卷积神经网络设计后接稠密层的方法。然而，最近的研究表明，采用输入条件计算逻辑的神经网络具有一系列优点，如更快的训练速度、更好的性能以及灵活性。为了解决这些问题，我们提出了首个将超网络用于识别失语症的研究。具体来说，我们将音频文件转换成log-Mel频谱图、delta和delta-delta，并通过预训练修改后的AlexNet模型处理这些图像。最后，使用生成目标网络权重的超网络来完成任务。实验在新收集并公开的VOC-ALS数据集上进行，结果显示所提出的方法准确率高达82.66%，优于包括多模态融合方法在内的强基准模型，并且消融研究结果表明了该方法的有效性。总的来说，我们的方法在泛化能力、参数效率和鲁棒性方面相对最先进的成果具有显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Amyotrophic Lateral Sclerosis (ALS) constitutes a progressiveneurodegenerative disease with varying symptoms, including decline in speechintelligibility. Existing studies, which recognize dysarthria in ALS patientsby predicting the clinical standard ALSFRS-R, rely on feature extractionstrategies and the design of customized convolutional neural networks followedby dense layers. However, recent studies have shown that neural networksadopting the logic of input-conditional computations enjoy a series ofbenefits, including faster training, better performance, and flexibility. Toresolve these issues, we present the first study incorporating hypernetworksfor recognizing dysarthria. Specifically, we use audio files, convert them intolog-Mel spectrogram, delta, and delta-delta, and pass the resulting imagethrough a pretrained modified AlexNet model. Finally, we use a hypernetwork,which generates weights for a target network. Experiments are conducted on anewly collected publicly available dataset, namely VOC-ALS. Results showed thatthe proposed approach reaches Accuracy up to 82.66% outperforming strongbaselines, including multimodal fusion methods, while findings from an ablationstudy demonstrated the effectiveness of the introduced methodology. Overall,our approach incorporating hypernetworks obtains valuable advantages overstate-of-the-art results in terms of generalization ability, parameterefficiency, and robustness.</description>
      <author>example@mail.com (Loukas Ilias, Dimitris Askounis)</author>
      <guid isPermaLink="false">2503.01892v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual Attention for Multimodal LLMs</title>
      <link>http://arxiv.org/abs/2503.02597v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多模态大型语言模型AKI，该模型通过解锁因果注意力机制并引入跨模态互惠注意（MMA），使得图像和文本之间能够互相影响，从而解决了现有MLLMs中存在的视觉-语言不一致问题。&lt;h4&gt;背景&lt;/h4&gt;最近的多模态大型语言模型在感知和推理多模式查询方面取得了显著进展，但同时出现了一个关键挑战：生成的文字响应与给定的图文输入在事实层面上可能并不一致。&lt;h4&gt;目的&lt;/h4&gt;本文旨在从一个基础且未被探索的角度来解决视觉-语言不一致性问题，通过重新审视MLLMs的核心架构并提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;AKI模型将传统的因果注意力机制转变为跨模态互惠注意（MMA），使得图像token可以关注文本token。这种设计简洁而有效，不需要增加额外的参数或训练时间。&lt;h4&gt;主要发现&lt;/h4&gt;通过在12个多模态理解基准测试中进行实验，AKI比现有模型平均高出7.2%，展示了其出色的性能表现。&lt;h4&gt;结论&lt;/h4&gt;AKI模型通过引入跨模态互惠注意机制成功地提高了多模态语言模型的视觉-语言一致性，并且该设计具有通用性和可扩展性，适用于各种模态和多样化的多模态场景。作者公开了代码并计划发布AKI-4B模型以推动未来在MLLMs领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;最近的多模态大型语言模型（MLLM）在处理多模式查询方面展示了显著的进步，并开启了基础模型研究的新时代，但视觉和语言之间的不一致问题成为一个关键挑战。本文通过重新审视MLLM的核心架构提出了一种新的解决方案：AKI模型，它引入了跨模态互惠注意机制来增强图像token与文本token之间的影响关系。实验结果表明AKI在多个基准测试中表现出了优越的性能，并且作者计划发布他们的模型以鼓励该领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent Multimodal Large Language Models (MLLMs) have demonstrated significantprogress in perceiving and reasoning over multimodal inquiries, ushering in anew research era for foundation models. However, vision-language misalignmentin MLLMs has emerged as a critical challenge, where the textual responsesgenerated by these models are not factually aligned with the given text-imageinputs. Existing efforts to address vision-language misalignment have focusedon developing specialized vision-language connectors or leveraging visualinstruction tuning from diverse domains. In this paper, we tackle this issuefrom a fundamental yet unexplored perspective by revisiting the corearchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMsconsisting of a causal attention mechanism, which limits the ability of earliermodalities (e.g., images) to incorporate information from later modalities(e.g., text). To address this problem, we propose AKI, a novel MLLM thatunlocks causal attention into modality-mutual attention (MMA) to enable imagetokens to attend to text tokens. This simple yet effective design allows AKI toachieve superior performance in 12 multimodal understanding benchmarks (+7.2%on average) without introducing additional parameters and increasing trainingtime. Our MMA design is intended to be generic, allowing for application acrossvarious modalities, and scalable to accommodate diverse multimodal scenarios.The code is publicly available at https://github.com/sony/aki, and we willrelease our AKI-4B model to encourage further advancements in MLLMs acrossvarious directions.</description>
      <author>example@mail.com (Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi)</author>
      <guid isPermaLink="false">2503.02597v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning is Not So Mysterious or Different</title>
      <link>http://arxiv.org/abs/2503.02113v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文讨论了深度神经网络中一些看似特殊的泛化行为（如良性过拟合和双重下降）实际上可以使用现有的理论框架进行解释，这些行为并非仅出现在神经网络中。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型常常表现出异常的泛化特性，这被认为挑战了传统机器学习方法中的泛化观念。例如，过度参数化在实践中非常成功，而传统的智慧认为应该避免过拟合。&lt;h4&gt;目的&lt;/h4&gt;论证这些现象并不独特于神经网络，并且可以通过现有的理论框架（如PAC-Bayes和可数假设边界）进行理解。&lt;h4&gt;方法&lt;/h4&gt;提出“软先验偏置”作为解释这些问题的关键原则：不是限制假设空间以避免过拟合，而是使用灵活的假设空间并倾向于那些与数据一致但更简单的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型并不像人们通常认为的那样神秘或独特；相反，“软先验偏置”的原理可以应用于多种模型类别。&lt;h4&gt;结论&lt;/h4&gt;虽然深度神经网络在泛化行为上可能与其他模型类似，但在表示学习、模式连接等方面存在相对独特的特性。&lt;h4&gt;翻译&lt;/h4&gt;深层神经网络经常被看作不同于其他类型的模型，并挑战了关于泛化的传统观点。例如，良性过拟合、双重下降现象以及过度参数化的成功应用等。本文认为这些现象并不仅存在于神经网络中，且可以通过PAC-Bayes和可数假设边界等长期存在的理论框架来进行理解和严谨地刻画。我们提出了一种称为“软先验偏置”的原则作为解释这些现象的关键要素：即在不试图避免过拟合的情况下限制假设空间时，采用一个灵活的假设空间，并倾向于那些与数据一致但更简单的解决方案。这一原理可以应用于多种模型类别中，因此深度学习并不像人们通常认为的那样神秘或独特。然而，我们还强调了深度学习的独特之处，比如它在表示学习中的能力、模式连接现象等相对普遍性的特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep neural networks are often seen as different from other model classes bydefying conventional notions of generalization. Popular examples of anomalousgeneralization behaviour include benign overfitting, double descent, and thesuccess of overparametrization. We argue that these phenomena are not distinctto neural networks, or particularly mysterious. Moreover, this generalizationbehaviour can be intuitively understood, and rigorously characterized usinglong-standing generalization frameworks such as PAC-Bayes and countablehypothesis bounds. We present soft inductive biases as a key unifying principlein explaining these phenomena: rather than restricting the hypothesis space toavoid overfitting, embrace a flexible hypothesis space, with a soft preferencefor simpler solutions that are consistent with the data. This principle can beencoded in many model classes, and thus deep learning is not as mysterious ordifferent from other model classes as it might seem. However, we also highlighthow deep learning is relatively distinct in other ways, such as its ability forrepresentation learning, phenomena such as mode connectivity, and its relativeuniversality.</description>
      <author>example@mail.com (Andrew Gordon Wilson)</author>
      <guid isPermaLink="false">2503.02113v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deal: Distributed End-to-End GNN Inference for All Nodes</title>
      <link>http://arxiv.org/abs/2503.02960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Deal的分布式GNN推理系统，该系统专注于对拥有数十亿边的图进行端到端的所有节点推理。通过在采样期间挖掘未充分利用的共享机会，并优化后续GNN计算中的共享收益，Deal系统展示了高效的内存使用和通信效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)在推荐和广告等应用中广泛使用，其常见的形式是为所有节点进行端到端推理。然而，在处理大规模图时，传统的分布式方法难以实现高效的资源共享以优化计算性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的系统——Deal，旨在解决现有GNN推理过程中的资源浪费问题，并最大化共享收益，从而提高对大规模图形的推理效率和减少内存使用量。&lt;h4&gt;方法&lt;/h4&gt;1. 开发了能够高效协作划分分布式推理的一维图和特征张量的节省内存且通信高效的分布式原语。       2. 引入分区化、流水线化的通讯方式，并将初始GNN原始计算与特性准备过程融合，以实现端到端推断。&lt;h4&gt;主要发现&lt;/h4&gt;在现实世界的基准数据集上，使用Deal进行端到端推理的时间最多可以减少7.70倍，而图形构建时间则减少了高达21.05倍。这表明了新方法的有效性和效率提升。&lt;h4&gt;结论&lt;/h4&gt;Deal系统成功地解决了大规模图的分布式GNN推理问题，在保证准确性的同时极大地提高了计算效率和资源利用效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：图神经网络（GNNs）是一个新的研究前沿，具有多种应用及成功的案例。对于所有的节点进行端到端推断是常见的GNN嵌入模型的方式，并且在推荐系统与广告领域得到广泛应用。虽然在诸如推理特定节点和训练等任务中出现了共享的机会，但针对全图的端到端推断中潜在的共享机会却大大被忽视了，因为传统的努力由于巨大的开销或过度的内存使用而无法充分提取共享的好处。本文介绍了Deal，一种专门用于具有数十亿边的图进行端到端推理的分布式GNN推理系统。首先，我们揭示并利用了在采样期间未被发现的分享机会，并最大化后续GNN计算中的分享收益。其次，我们引入了节省内存和通信高效的分布式原语，这些原语基于一维图形和特征张量协作划分进行轻量化分布式推断。第三，我们介绍了分区、流水线通信，以及将特性准备与第一个GNN原始计算融合的端到端推理方法。通过使用Deal，在现实世界的基准数据集上，端到端推断时间最多可以减少7.70倍，而图形构建时间则减少了高达21.05倍，相比现有最佳技术而言有显著提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) are a new research frontier with variousapplications and successes. The end-to-end inference for all nodes, is commonfor GNN embedding models, which are widely adopted in applications likerecommendation and advertising. While sharing opportunities arise in GNN tasks(i.e., inference for a few nodes and training), the potential for sharing infull graph end-to-end inference is largely underutilized because traditionalefforts fail to fully extract sharing benefits due to overwhelming overheads orexcessive memory usage.  This paper introduces Deal, a distributed GNN inference system that isdedicated to end-to-end inference for all nodes for graphs with multi-billionedges. First, we unveil and exploit an untapped sharing opportunity duringsampling, and maximize the benefits from sharing during subsequent GNNcomputation. Second, we introduce memory-saving and communication-efficientdistributed primitives for lightweight 1-D graph and feature tensorcollaborative partitioning-based distributed inference. Third, we introducepartitioned, pipelined communication and fusing feature preparation with thefirst GNN primitive for end-to-end inference. With Deal, the end-to-endinference time on real-world benchmark datasets is reduced up to 7.70 x and thegraph construction time is reduced up to 21.05 x, compared to thestate-of-the-art.</description>
      <author>example@mail.com (Shiyang Chen, Xiang Song, Vasiloudis Theodore, Hang Liu)</author>
      <guid isPermaLink="false">2503.02960v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OFF-CLIP: Improving Normal Detection Confidence in Radiology CLIP with Simple Off-Diagonal Term Auto-Adjustment</title>
      <link>http://arxiv.org/abs/2503.01794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, and 5 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对CLIP模型的改进方案OFF-CLIP，通过引入off-diagonal项损失来优化正常病例检测，并采用句子级文本过滤方法减少假阴性结果。&lt;h4&gt;背景&lt;/h4&gt;CLIP在放射学中的零样本分类中减少了对人工注释的依赖，但传统的对比学习方法在处理正常案例时效果不佳，因为严格的内部样例对齐会干扰正常样本聚类。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进的方法OFF-CLIP以解决正常病例检测中的问题，并提高医疗视觉语言模型的整体性能。&lt;h4&gt;方法&lt;/h4&gt;引入off-diagonal项损失以增强正常样本的聚类能力；使用句子级文本过滤去除不匹配正常的陈述，从而降低假阴性结果；该方案无需对现有的CLIP架构进行修改即可应用。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示OFF-CLIP显著提高了VinDr-CXR数据集上的正常分类性能，AUC分数较基线方法CARZero提升了0.61，并且保持或改善了异常情况的分类性能。同时，OFF-CLIP还增强了零样本定位能力，提高指向游戏准确性。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了OFF-CLIP的有效性和效率，表明其是增强医疗视觉语言模型的一个稳健和有效的改进方案。&lt;h4&gt;翻译&lt;/h4&gt;对比性语言图像预训练（CLIP）在放射学中实现了零样本分类，减少了对人工注释的依赖。然而，传统的对比学习方法难以处理正常案例检测问题，严格的内部样例对齐会导致正常样本聚类失效以及高假阳性率和假阴性率。为解决这些问题，我们提出了OFF-CLIP，这是一种改进的对比学习方案，通过引入off-diagonal项损失来优化正常样本聚类，并应用句子级文本过滤减少假阴性结果。该方法可以在现有放射学CLIP模型上实现而无需修改任何架构。实验结果显示，OFF-CLIP在VinDr-CXR数据集上的正常分类性能显著提升，相较于最先进的零样本分类基线CARZero，AUC分数提升了0.61，并且保持或改善了异常情况的分类性能。此外，OFF-CLIP还增强了零样本定位能力，通过提高指向游戏准确性来确认更好的异常位置识别。这些结果表明OFF-CLIP作为增强医疗视觉语言模型的有效性和效率上的显著改进方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive Language-Image Pre-Training (CLIP) has enabled zero-shotclassification in radiology, reducing reliance on manual annotations. However,conventional contrastive learning struggles with normal case detection due toits strict intra-sample alignment, which disrupts normal sample clustering andleads to high false positives (FPs) and false negatives (FNs). To address theseissues, we propose OFF-CLIP, a contrastive learning refinement that improvesnormal detection by introducing an off-diagonal term loss to enhance normalsample clustering and applying sentence-level text filtering to mitigate FNs byremoving misaligned normal statements from abnormal reports. OFF-CLIP can beapplied to radiology CLIP models without requiring any architecturalmodifications. Experimental results show that OFF-CLIP significantly improvesnormal classification, achieving a 0.61 Area under the curve (AUC) increase onVinDr-CXR over CARZero, the state-of-the-art zero-shot classification baseline,while maintaining or improving abnormal classification performance.Additionally, OFF-CLIP enhances zero-shot grounding by improving pointing gameaccuracy, confirming better anomaly localization. These results demonstrateOFF-CLIP's effectiveness as a robust and efficient enhancement for medicalvision-language models.</description>
      <author>example@mail.com (Junhyun Park, Chanyu Moon, Donghwan Lee, Kyungsu Kim, Minho Hwang)</author>
      <guid isPermaLink="false">2503.01794v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Node-level Contrastive Unlearning on Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.02959v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的图数据去学习方法Node-CUL，利用嵌入空间优化来移除特定节点和边对模型的影响。通过对比剩余节点及其邻居的嵌入，逐步减弱目标节点的影响，并且不直接使用未见数据，保持了模型的有效性。&lt;h4&gt;背景&lt;/h4&gt;图神经网络在处理非欧几里得结构化的图形数据时面临挑战，特别是去除部分实体（如节点和边）的困难。现有的方法如图划分、影响函数或添加额外层都难以同时达到高可扩展性和有效性。&lt;h4&gt;目的&lt;/h4&gt;提出一种更有效的去学习算法以移除特定图实体对模型的影响，并保持模型的有效性。&lt;h4&gt;方法&lt;/h4&gt;通过对比剩余节点及其邻居的嵌入空间，逐步减弱目标节点（即要去除的节点）影响。另外还引入了邻域重构方法来优化邻居的嵌入从而减少未学节点对其它部分的影响。&lt;h4&gt;主要发现&lt;/h4&gt;Node-CUL在多种图数据和模型上进行实验后显示，其去学习效果最佳，并且提高了模型的有效性，同时只需要与现有框架类似的计算资源。&lt;h4&gt;结论&lt;/h4&gt;本文提出的Node-CUL方法证明了使用嵌入空间优化是一种高效去除特定节点对GNN影响的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph unlearning aims to remove a subset of graph entities (i.e. nodes andedges) from a graph neural network (GNN) trained on the graph. Unlike machineunlearning for models trained on Euclidean-structured data, effectivelyunlearning a model trained on non-Euclidean-structured data, such as graphs, ischallenging because graph entities exhibit mutual dependencies. Existing worksutilize graph partitioning, influence function, or additional layers to achievegraph unlearning. However, none of them can achieve high scalability andeffectiveness without additional constraints. In this paper, we achieve moreeffective graph unlearning by utilizing the embedding space. The primarytraining objective of a GNN is to generate proper embeddings for each node thatencapsulates both structural information and node feature representations.Thus, directly optimizing the embedding space can effectively remove the targetnodes' information from the model. Based on this intuition, we proposenode-level contrastive unlearning (Node-CUL). It removes the influence of thetarget nodes (unlearning nodes) by contrasting the embeddings of remainingnodes and neighbors of unlearning nodes. Through iterative updates, theembeddings of unlearning nodes gradually become similar to those of unseennodes, effectively removing the learned information without directlyincorporating unseen data. In addition, we introduce a neighborhoodreconstruction method that optimizes the embeddings of the neighbors in orderto remove influence of unlearning nodes to maintain the utility of the GNNmodel. Experiments on various graph data and models show that our Node-CULachieves the best unlearn efficacy and enhanced model utility with requiringcomparable computing resources with existing frameworks.</description>
      <author>example@mail.com (Hong kyu Lee, Qiuchen Zhang, Carl Yang, Li Xiong)</author>
      <guid isPermaLink="false">2503.02959v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation Alignment</title>
      <link>http://arxiv.org/abs/2503.01711v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  added project repository &amp; dataset URL&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;个性化产品搜索旨在检索和排序符合用户偏好和搜索意图的商品。现有的方法虽然有效，但通常假设用户的查询完全捕捉到了他们的实际动机。&lt;h4&gt;背景&lt;/h4&gt;现实中电商平台的数据分析显示，用户在进行搜索之前经常参与相关的咨询活动，这表明他们在咨询过程中根据动机和需求来细化意图。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的个性化搜索方法，利用咨询中隐含的动机作为提升个性化搜索的关键因素。&lt;h4&gt;主要挑战&lt;/h4&gt;包括将上下文中的动机与简洁查询相匹配、跨越类别-文本差距以及过滤序列历史中的噪声等新挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一个考虑动机的个性化搜索（MAPS）方法。该方法利用大型语言模型将查询和咨询嵌入到统一语义空间，通过注意力专家混合机制优先处理关键语义，并引入双元对齐：对比学习对齐咨询、评论和产品特性；双向注意机制整合了基于动机感知的嵌入与用户偏好。&lt;h4&gt;主要发现&lt;/h4&gt;在真实数据集和合成数据集上的广泛实验表明MAPS方法在检索和排序任务中均优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过利用用户的咨询活动中的隐含动机，个性化搜索可以得到显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Personalized product search aims to retrieve and rank items that match users'preferences and search intent. Despite their effectiveness, existing approachestypically assume that users' query fully captures their real motivation.However, our analysis of a real-world e-commerce platform reveals that usersoften engage in relevant consultations before searching, indicating they refineintents through consultations based on motivation and need. The impliedmotivation in consultations is a key enhancing factor for personalized search.This unexplored area comes with new challenges including aligning contextualmotivations with concise queries, bridging the category-text gap, and filteringnoise within sequence history. To address these, we propose a Motivation-AwarePersonalized Search (MAPS) method. It embeds queries and consultations into aunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)to prioritize critical semantics, and introduces dual alignment: (1)contrastive learning aligns consultations, reviews, and product features; (2)bidirectional attention integrates motivation-aware embeddings with userpreferences. Extensive experiments on real and synthetic data show MAPSoutperforms existing methods in both retrieval and ranking tasks.</description>
      <author>example@mail.com (Weicong Qin, Yi Xu, Weijie Yu, Chenglei Shen, Ming He, Jianping Fan, Xiao Zhang, Jun Xu)</author>
      <guid isPermaLink="false">2503.01711v3</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A dataset-free approach for self-supervised learning of 3D reflectional symmetries</title>
      <link>http://arxiv.org/abs/2503.02660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种无需大型标注数据集的自监督模型，用于检测单个物体的对称性。通过利用对象本身的内在特征进行学习，并设计了不依赖于地面真实标签的学习策略。&lt;h4&gt;背景&lt;/h4&gt;传统的模型需要大量的带标签的数据来训练和识别物体的对称性，这增加了计算成本和时间开销。&lt;h4&gt;目的&lt;/h4&gt;开发一种既有效又高效的自监督方法，用于检测单个物体的对称性，并降低依赖于大型数据集的成本。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种基于点云视觉特征的方法，利用基础图像模型提取出的特性来计算对象各点的描述符。这些描述符反映了物体上两点之间的对称关系，并且有助于优化自监督学习模型。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该自监督模型在检测单个物体对称性方面优于使用大型数据集训练的状态-of-the-art模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的自监督方法不仅效果好，而且更加高效、资源需求低，适用于计算和数据资源有限的场景。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们探索了一种无需依赖于数据集就能学习单个物体对称性的自我监督模型。基于假设：可以通过分析对象自身的固有特征来确定其对称性，从而在训练过程中不需要大型的数据集。此外，还设计了一个自监督学习策略以消除地面真实标签的必要性。这两个关键要素使我们的方法既有效又高效，解决了构建大量标注数据集的成本问题。该研究的独特之处在于计算每个点上基于视觉外观相似性的对象特征，并利用基础图像模型提取出的特性来优化自我监督模型。实验结果表明，该自监督方法在检测单个物体对称性方面超越了依赖于大规模数据集训练的状态-of-the-art模型，且更加高效和资源节约。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we explore a self-supervised model that learns to detect thesymmetry of a single object without requiring a dataset-relying solely on theinput object itself. We hypothesize that the symmetry of an object can bedetermined by its intrinsic features, eliminating the need for large datasetsduring training. Additionally, we design a self-supervised learning strategythat removes the necessity of ground truth labels. These two key elements makeour approach both effective and efficient, addressing the prohibitive costsassociated with constructing large, labeled datasets for this task. The noveltyof our method lies in computing features for each point on the object based onthe idea that symmetric points should exhibit similar visual appearances. Toachieve this, we leverage features extracted from a foundational image model tocompute a visual descriptor for the points. This approach equips the pointcloud with visual features that facilitate the optimization of ourself-supervised model. Experimental results demonstrate that our methodsurpasses the state-of-the-art models trained on large datasets. Furthermore,our model is more efficient, effective, and operates with minimal computationaland data resources.</description>
      <author>example@mail.com (Issac Aguirre, Ivan Sipiran, Gabriel Montañana)</author>
      <guid isPermaLink="false">2503.02660v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Reliable and Efficient Multi-Agent Coordination via Graph Neural Network Variational Autoencoders</title>
      <link>http://arxiv.org/abs/2503.02954v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by 2025 International Conference on Robotics and Automation  (ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了使用图神经网络变分自动编码器（GNN-VAE）来解决大规模多智能体协调问题，这种方法比传统的集中式优化方法更快。&lt;h4&gt;背景&lt;/h4&gt;在高密度机器人流量区域中，局部协调方法可能无法找到无死锁的解决方案。这时需要一个中央单元生成全局调度方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于GNN-VAE的方法来解决大规模多智能体协调问题，提高效率和性能。&lt;h4&gt;方法&lt;/h4&gt;论文将协调问题建模为图问题，并使用混合整数线性规划（MILP）求解器收集地面真实数据。在训练阶段，学习框架将高质量的解决方案编码到潜在空间中，在推理时从采样的潜在变量中解码出最优解。&lt;h4&gt;主要发现&lt;/h4&gt;GNN-VAE框架可以生成满足问题约束条件的有效提案，并且对于大规模（250个机器人）的问题也能够提供高质量的解决方案，速度远超其他基准方法。&lt;h4&gt;结论&lt;/h4&gt;提出的基于GNN-VAE的方法在处理大规模多智能体协调问题时具有显著优势。该项目页为https://mengyuest.github.io/gnn-vae-coord。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了如何利用图神经网络变分自动编码器（GNN-VAE）来更高效地解决大规模的多机器人协调问题，特别是在密集机器人交通区域中，这种方法比传统的集中式优化方法更加有效和快速。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent coordination is crucial for reliable multi-robot navigation inshared spaces such as automated warehouses. In regions of dense robot traffic,local coordination methods may fail to find a deadlock-free solution. In thesescenarios, it is appropriate to let a central unit generate a global schedulethat decides the passing order of robots. However, the runtime of suchcentralized coordination methods increases significantly with the problemscale. In this paper, we propose to leverage Graph Neural Network VariationalAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scalefaster than through centralized optimization. We formulate the coordinationproblem as a graph problem and collect ground truth data using a Mixed-IntegerLinear Program (MILP) solver. During training, our learning framework encodesgood quality solutions of the graph problem into a latent space. At inferencetime, solution samples are decoded from the sampled latent variables, and thelowest-cost sample is selected for coordination. Finally, the feasible proposalwith the highest performance index is selected for the deployment. Byconstruction, our GNN-VAE framework returns solutions that always respect theconstraints of the considered coordination problem. Numerical results show thatour approach trained on small-scale problems can achieve high-quality solutionseven for large-scale problems with 250 robots, being much faster than otherbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord</description>
      <author>example@mail.com (Yue Meng, Nathalie Majcherczyk, Wenliang Liu, Scott Kiesel, Chuchu Fan, Federico Pecora)</author>
      <guid isPermaLink="false">2503.02954v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Prompting: An Efficient Embedding Framework for Open-Domain Question Answering</title>
      <link>http://arxiv.org/abs/2503.01606v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的Embedding级别框架EmbQA，旨在改进开放领域问题回答(ODQA)任务。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型推动了开放领域问题回答的发展，但目前的检索-阅读器流水线存在计算成本高、不稳定性和检索覆盖率不足的问题。&lt;h4&gt;目的&lt;/h4&gt;通过增强检索器和读者的功能来解决当前ODQA系统存在的上述挑战。&lt;h4&gt;方法&lt;/h4&gt;{'改进查询表示': '利用轻量级线性层在无监督对比学习目标下优化查询表示，重新排序检索到的段落以突出最可能包含正确答案的部分。', '引入探索性嵌入': '通过扩展模型潜在语义空间来多样化候选生成，并采用基于熵的选择机制自动选择最有信心的答案。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在三个开源LLM、三种检索方法和四个ODQA基准上，EmbQA在准确性和效率方面显著优于现有基线。&lt;h4&gt;结论&lt;/h4&gt;EmbQA框架通过改进查询表示和引入探索性嵌入有效提升了ODQA任务的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models have recently pushed open domain question answering(ODQA) to new frontiers. However, prevailing retriever-reader pipelines oftendepend on multiple rounds of prompt level instructions, leading to highcomputational overhead, instability, and suboptimal retrieval coverage. In thispaper, we propose EmbQA, an embedding-level framework that alleviates theseshortcomings by enhancing both the retriever and the reader. Specifically, werefine query representations via lightweight linear layers under anunsupervised contrastive learning objective, thereby reordering retrievedpassages to highlight those most likely to contain correct answers.Additionally, we introduce an exploratory embedding that broadens the model'slatent semantic space to diversify candidate generation and employs anentropy-based selection mechanism to choose the most confident answerautomatically. Extensive experiments across three open-source LLMs, threeretrieval methods, and four ODQA benchmarks demonstrate that EmbQAsubstantially outperforms recent baselines in both accuracy and efficiency.</description>
      <author>example@mail.com (Zhanghao Hu, Hanqi Yan, Qingling Zhu, Zhenyi Shen, Yulan He, Lin Gui)</author>
      <guid isPermaLink="false">2503.01606v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Deepfake Detection via Knowledge Injection</title>
      <link>http://arxiv.org/abs/2503.02503v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为知识注入的Deepfake检测技术（KID），它构建了一个基于多任务学习的知识注入框架，可以轻松集成到现有的ViT基础模型中。&lt;h4&gt;背景&lt;/h4&gt;当前的生成式AI模型能够创造出非常逼真的deepfakes，这些内容可能被用于恶意用途。现有的deepfake检测方法要么依赖于改进分类器以更好地适应训练数据分布，要么利用伪造合成机制来学习更全面的伪造数据分布。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法忽视真实数据知识的问题，并提高模型处理未见过的真实和虚假数据的能力，本文提出了一种新的简单而有效的方法KID。&lt;h4&gt;方法&lt;/h4&gt;构建了一个基于多任务学习的知识注入框架。设计了知识注入模块来学习并注入必要的信息到基础模型中，以更准确地建模真实和伪造数据的分布；构造了一个粗粒度的伪造定位分支，通过多任务学习方式学习伪造位置，进一步丰富知识注入模块中的伪造知识。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两种层次化的抑制损失与对比损失，强调了在知识注入模块中对真实数据知识的关注，并平衡真实和虚假的知识比例。实验表明KID具有优秀的兼容性，可以应用于不同规模的ViT基础模型，并且实现了最先进的泛化性能同时提升了训练收敛速度。&lt;h4&gt;结论&lt;/h4&gt;KID是一种新颖有效的deepfake检测技术，能够在现有ViT基础模型上简单高效地实现知识注入，提高了模型在处理真实和伪造数据上的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deepfake detection technologies become vital because current generative AImodels can generate realistic deepfakes, which may be utilized in maliciouspurposes. Existing deepfake detection methods either rely on developingclassification methods to better fit the distributions of the training data, orexploiting forgery synthesis mechanisms to learn a more comprehensive forgerydistribution. Unfortunately, these methods tend to overlook the essential roleof real data knowledge, which limits their generalization ability in processingthe unseen real and fake data. To tackle these challenges, in this paper, wepropose a simple and novel approach, named Knowledge Injection based deepfakeDetection (KID), by constructing a multi-task learning based knowledgeinjection framework, which can be easily plugged into existing ViT-basedbackbone models, including foundation models. Specifically, a knowledgeinjection module is proposed to learn and inject necessary knowledge into thebackbone model, to achieve a more accurate modeling of the distributions ofreal and fake data. A coarse-grained forgery localization branch is constructedto learn the forgery locations in a multi-task learning manner, to enrich thelearned forgery knowledge for the knowledge injection module. Two layer-wisesuppression and contrast losses are proposed to emphasize the knowledge of realdata in the knowledge injection module, to further balance the portions of thereal and fake knowledge. Extensive experiments have demonstrated that our KIDpossesses excellent compatibility with different scales of Vit-based backbonemodels, and achieves state-of-the-art generalization performance whileenhancing the training convergence speed.</description>
      <author>example@mail.com (Tonghui Li, Yuanfang Guo, Zeming Liu, Heqi Peng, Yunhong Wang)</author>
      <guid isPermaLink="false">2503.02503v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NodeNAS: Node-Specific Graph Neural Architecture Search for Out-of-Distribution Generalization</title>
      <link>http://arxiv.org/abs/2503.02448v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为NodeNAS的节点特定图神经架构搜索方法，它通过拆解节点拓扑和图分布来为不同节点定制独特的聚合方法。此外，还提出了自适应聚集注意力多维NodeNAS（MNNAS）方法，在有限的数据集下学习具有良好泛化能力的节点特定架构定制器。&lt;h4&gt;背景&lt;/h4&gt;现有的GraphNAS方法在处理数据分布变化时表现出色，但它们依赖于大量训练数据，并且当面对稀疏或单一训练图时难以发现最佳的图形与架构映射关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在有限数据集下工作的节点特定图神经架构搜索（NodeNAS）方法，以提高模型在不同分布的数据上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;1. 提出了一种名为NodeNAS的方法，通过拆解节点拓扑和图分布来为不同的节点定制独特的聚合方法。2. 引入了自适应聚集注意力多维NodeNAS（MNNAS）方法，该方法学习了一个具有良好泛化能力的节点特定架构定制器，并且扩展了搜索空间的垂直深度以支持跨多个维度的同时进行节点特定架构定制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，提出的MNNAS方法超越了现有的最佳算法，在监督和非监督任务中取得了卓越的性能，并展示了优秀的出分布（OOD）泛化能力。&lt;h4&gt;结论&lt;/h4&gt;新提出的方法NodeNAS及其扩展版本MNNAS有效地解决了现有GraphNAS方法在面对稀疏或单一训练图时的问题，能够在更少的数据下发现更具针对性且具有良好泛化的架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural architecture search (GraphNAS) has demonstrated advantages inmitigating performance degradation of graph neural networks (GNNs) due todistribution shifts. Recent approaches introduce weight sharing across tailoredarchitectures, generating unique GNN architectures for each graph end-to-end.However, existing GraphNAS methods do not account for distribution patternsacross different graphs and heavily rely on extensive training data. Withsparse or single training graphs, these methods struggle to discover optimalmappings between graphs and architectures, failing to generalize toout-of-distribution (OOD) data. In this paper, we propose node-specific graphneural architecture search(NodeNAS), which aims to tailor distinct aggregationmethods for different nodes through disentangling node topology and graphdistribution with limited datasets. We further propose adaptive aggregationattention based multi-dim NodeNAS method(MNNAS), which learns an node-specificarchitecture customizer with good generalizability. Specifically, we extend thevertical depth of the search space, supporting simultaneous node-specificarchitecture customization across multiple dimensions. Moreover, we model thepower-law distribution of node degrees under varying assortativity, encodingstructure invariant information to guide architecture customization across eachdimension. Extensive experiments across supervised and unsupervised tasksdemonstrate that MNNAS surpasses state-of-the-art algorithms and achievesexcellent OOD generalization.</description>
      <author>example@mail.com (Qiyi Wang, Yinning Shao, Yunlong Ma, Min Liu)</author>
      <guid isPermaLink="false">2503.02448v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Actionable World Models for Industrial Process Control</title>
      <link>http://arxiv.org/abs/2503.01411v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于学习世界模型的新方法，该方法通过解构过程参数在学习的潜在表示中分离关键因素，使复杂系统的行为能够从有限的数据中学到，并实现对系统的精细控制。&lt;h4&gt;背景&lt;/h4&gt;从被动的过程监控过渡到主动的过程控制需要一个有效的人工智能系统，它可以从非常有限的训练数据中学习复杂系统的特性。&lt;h4&gt;目的&lt;/h4&gt;为了形成关于过程输入和输出的即兴数字孪生体，该模型可以预测行动对流程世界的影响，研究旨在提供一种方法论以实现有效的主动过程控制。&lt;h4&gt;方法&lt;/h4&gt;通过对比性学习在联合嵌入预测架构内驱动表示学习，这种方法使从输入变化到表示变化的变化可预测，并且反过来亦然。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够揭示关键因素对流程变异的影响，为有效控制行动提供基础，以便将过程保持在其操作范围内。&lt;h4&gt;结论&lt;/h4&gt;在塑料注射成型的例子中验证了该方法的有效性，展示了其在提出针对不稳定过程的特定控制措施方面的实际相关性。&lt;h4&gt;翻译&lt;/h4&gt;为了从（被动）过程监控过渡到主动的过程控制，有效的AI系统必须能够从非常有限的训练数据中学到复杂系统的特性。研究提出了一种基于学习世界模型的新方法，该方法通过解构过程参数在学习的潜在表示中分离关键因素，并形成关于过程输入和输出的即兴数字孪生体。这种方法使复杂的行动后果可以被预测，为有效的主动控制铺平道路。使用塑料注射成型作为示例验证了此方法的有效性，展示了其提出针对不稳定过程的具体控制措施的实际相关性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To go from (passive) process monitoring to active process control, aneffective AI system must learn about the behavior of the complex system fromvery limited training data, forming an ad-hoc digital twin with respect toprocess in- and outputs that captures the consequences of actions on theprocess's world. We propose a novel methodology based on learning world modelsthat disentangles process parameters in the learned latent representation,allowing for fine-grained control. Representation learning is driven by thelatent factors that influence the processes through contrastive learning withina joint embedding predictive architecture. This makes changes inrepresentations predictable from changes in inputs and vice versa, facilitatinginterpretability of key factors responsible for process variations, paving theway for effective control actions to keep the process within operationalbounds. The effectiveness of our method is validated on the example of plasticinjection molding, demonstrating practical relevance in proposing specificcontrol actions for a notoriously unstable process.</description>
      <author>example@mail.com (Peng Yan, Ahmed Abdulkadir, Gerrit A. Schatte, Giulia Aguzzi, Joonsu Gha, Nikola Pascher, Matthias Rosenthal, Yunlong Gao, Benjamin F. Grewe, Thilo Stadelmann)</author>
      <guid isPermaLink="false">2503.01411v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>From superposition to sparse codes: interpretable representations in neural networks</title>
      <link>http://arxiv.org/abs/2503.01824v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;理解神经网络中信息的表示是神经科学和人工智能领域的基本挑战。尽管它们具有非线性架构，最近的研究表明，神经网络以叠加的方式编码特征，即输入概念在网络表示中线性地重叠。&lt;h4&gt;背景&lt;/h4&gt;神经网络在分类任务中的训练可以恢复潜在特征，这些特征可以通过线性变换进行识别。此外，稀疏编码方法可以从这些表示中提取解耦的特性，并且定量解释性度量为评估这些方法的成功提供了途径，确保提取的功能与人类可理解的概念一致。&lt;h4&gt;目的&lt;/h4&gt;提供一个理论框架来解释神经网络如何以叠加方式存储信息，并提出一种从神经激活中提取可解释表示的方法论。&lt;h4&gt;方法&lt;/h4&gt;理论框架包括三个步骤：(1) 可识别性理论表明训练的神经网络可以恢复分类任务中的潜在特征，这些特征可以通过线性变换进行识别。(2) 稀疏编码技术可以从神经网络的表示中提取解耦特性，通过压缩感知的原则实现。(3) 定量解释度量为评估方法的成功提供了标准。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种从理论神经科学、表示学习和可解释性研究中的见解出发的新视角，来理解人工和生物系统中的神经表示。这些论点对神经编码理论、人工智能透明度以及使深度学习模型更加可解释的总体目标有重要意义。&lt;h4&gt;结论&lt;/h4&gt;通过连接来自不同领域的见解，这项工作为理解和改善神经网络的表示提供了新的视角，有助于推动神经科学与人工智能的发展。&lt;h4&gt;翻译&lt;/h4&gt;理解如何在人工神经网络中表示信息是两个学科——神经科学和人工智能中的一个基本挑战。尽管这些网络具有非线性架构，但最近的研究表明，它们通过叠加存储特征，这意味着输入概念在网络表示中以线性方式重叠。我们提出了一个解释这一现象的视角，并为从激活中提取可解释表征提供了理论基础。我们的理论框架由三部分组成：(1) 可识别性理论显示神经网络在分类任务训练中恢复潜在特性到线性转换；(2) 稀疏编码技术可以从这些表示中通过压缩感知原理来提取不相关特征；(3) 定量解释度量为评估方法的有效性提供了一种方式，确保所提取的特征与人类可理解的概念保持一致。结合理论神经科学、表示学习和可解释性研究领域的洞见，我们提出了一个新兴视角以了解人工及生物系统中的神经表征，并且我们的论点对神经编码理论、人工智能透明度以及使深度学习模型更易于解释的目标具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how information is represented in neural networks is afundamental challenge in both neuroscience and artificial intelligence. Despitetheir nonlinear architectures, recent evidence suggests that neural networksencode features in superposition, meaning that input concepts are linearlyoverlaid within the network's representations. We present a perspective thatexplains this phenomenon and provides a foundation for extracting interpretablerepresentations from neural activations. Our theoretical framework consists ofthree steps: (1) Identifiability theory shows that neural networks trained forclassification recover latent features up to a linear transformation. (2)Sparse coding methods can extract disentangled features from theserepresentations by leveraging principles from compressed sensing. (3)Quantitative interpretability metrics provide a means to assess the success ofthese methods, ensuring that extracted features align with human-interpretableconcepts. By bridging insights from theoretical neuroscience, representationlearning, and interpretability research, we propose an emerging perspective onunderstanding neural representations in both artificial and biological systems.Our arguments have implications for neural coding theories, AI transparency,and the broader goal of making deep learning models more interpretable.</description>
      <author>example@mail.com (David Klindt, Charles O'Neill, Patrik Reizinger, Harald Maurer, Nina Miolane)</author>
      <guid isPermaLink="false">2503.01824v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB Image for Graspability-Aware Bin Picking</title>
      <link>http://arxiv.org/abs/2503.02387v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, In submission to IROS2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了RGBSQGrasp框架，该框架结合了超二次体形状基元和基础模型驱动的深度估计方法，可以从单目RGB图像中推断出抓取姿态。&lt;h4&gt;背景&lt;/h4&gt;工件拾取任务由于遮挡和物理限制而具有挑战性。现有方法依赖于已知CAD模型或先验物体几何信息，并且从有限视角恢复超二次体形状是困难的。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来解决工件拾取中遮挡、物体识别和抓取问题，同时提高对未知物体的适应能力。&lt;h4&gt;方法&lt;/h4&gt;RGBSQGrasp包括数据集生成管道、基于基础模型的对象点云估计模块、全局局部超二次体拟合网络以及超二次体引导的抓取姿态采样模块。该框架不需要深度传感器，并通过几何推理来推断抓取姿态，提高稳定性和适应性。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界的机器人实验中显示了92%的成功率，证明了RGBSQGrasp在打包料箱拣选环境中的有效性。&lt;h4&gt;结论&lt;/h4&gt;RGBSQGrasp提供了一种有效的方法，通过单目RGB图像实现可靠抓取姿态推断，并显著提高了工件拾取任务的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;零件挑选是一个具有挑战性的机器人任务，由于遮挡和物理限制导致视觉信息不足。现有的方法通常依赖于已知CAD模型或先验物体几何形状，这限制了它们对新型未知对象的泛化能力。其他直接从RGB-D数据回归抓取姿态的方法则面临着深度传感中的固有噪声问题，并且缺乏对物体的理解使得抓取的姿态合成与评估更加困难。超二次体（SQ）提供了一种紧凑、可解释的形状表示，可以捕获物体的物理特性和抓取能力。然而，从有限视角恢复它们是具有挑战性的，现有的方法依赖于多个角度来进行近乎完整的点云重建，这限制了其在零件挑选任务中的有效性。为了应对这些挑战，我们提出了extbf{RGBSQGrasp}框架——一个利用超二次体形状基元和基础模型驱动的深度估计方法的抓取框架，可以从单目RGB图像中推断出抓取姿态--无需使用深度传感器。该框架整合了一个通用、跨平台的数据集生成管道、基于基础模型的对象点云估计模块、全局局部超二次体拟合网络以及一个由SQ指导的抓取姿态采样模块。通过结合这些组件，RGBSQGrasp可以通过几何推理可靠地推断出抓取姿态，提高抓取稳定性，并增强对未见过对象的适应性。在真实世界的机器人实验中展示出了92%的成功率，突显了RGBSQGrasp在包装料箱挑选环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bin picking is a challenging robotic task due to occlusions and physicalconstraints that limit visual information for object recognition and grasping.Existing approaches often rely on known CAD models or prior object geometries,restricting generalization to novel or unknown objects. Other methods directlyregress grasp poses from RGB-D data without object priors, but the inherentnoise in depth sensing and the lack of object understanding make graspsynthesis and evaluation more difficult. Superquadrics (SQ) offer a compact,interpretable shape representation that captures the physical and graspabilityunderstanding of objects. However, recovering them from limited viewpoints ischallenging, as existing methods rely on multiple perspectives fornear-complete point cloud reconstruction, limiting their effectiveness inbin-picking. To address these challenges, we propose \textbf{RGBSQGrasp}, agrasping framework that leverages superquadric shape primitives and foundationmetric depth estimation models to infer grasp poses from a monocular RGB camera-- eliminating the need for depth sensors. Our framework integrates auniversal, cross-platform dataset generation pipeline, a foundation model-basedobject point cloud estimation module, a global-local superquadric fittingnetwork, and an SQ-guided grasp pose sampling module. By integrating thesecomponents, RGBSQGrasp reliably infers grasp poses through geometric reasoning,enhancing grasp stability and adaptability to unseen objects. Real-worldrobotic experiments demonstrate a 92\% grasp success rate, highlighting theeffectiveness of RGBSQGrasp in packed bin-picking environments.</description>
      <author>example@mail.com (Yifeng Xu, Fan Zhu, Ye Li, Sebastian Ren, Xiaonan Huang, Yuhao Chen)</author>
      <guid isPermaLink="false">2503.02387v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>InfoGNN: End-to-end deep learning on mesh via graph neural networks</title>
      <link>http://arxiv.org/abs/2503.02414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个新的以图神经网络（GNN）为中心的端到端框架InfoGNN，用于解决深度学习在网格模型应用中的挑战。&lt;h4&gt;背景&lt;/h4&gt;3D模型被广泛应用于各个行业，并且由于其独特的优势，网格数据已经成为三维建模不可或缺的一部分。然而，无序、不规则的数据结构以及复杂的表面信息使得直接使用深度学习模型进行处理变得困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端框架以克服在网格模型中应用深度学习的挑战，同时充分利用网格模型的优点。&lt;h4&gt;方法&lt;/h4&gt;InfoGNN将网格模型视为图，采用InfoConv和InfoMP模块利用点的位置信息以及面法线、二面角等静态信息和动态全局特征信息来处理所有类型的数据，并且是一个端到端的设计以提高效率。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个公开数据集上进行的测试中，InfoGNN在网格分类和分割任务中表现出色。&lt;h4&gt;结论&lt;/h4&gt;本文提出的新框架InfoGNN为复杂的3D模型提供了高效的深度学习方法，并通过简化网络设计提高了处理效率。&lt;h4&gt;翻译&lt;/h4&gt;三维模型被广泛应用于各个行业。由于其独特的优势，网格数据已经成为三维建模不可或缺的一部分，能够提供直观且实用的丰富三维信息表达方式。然而，无序、不规则的数据结构和复杂的表面信息使得直接使用深度学习模型进行处理变得困难。传统的网格数据处理方法通常依赖于具有许多限制的网格模型（如流形），这些限制在实际应用中缩小了它们的应用范围，并未充分利用网格模型的优点。本文提出了一个基于图神经网络（GNN）的新端到端框架InfoGNN，用于解决深度学习应用于网格模型中的挑战。InfoGNN将网格模型视为图，使其能够高效处理不规则的网格数据。此外，我们提出InfoConv和InfoMP模块，利用点的位置信息，并充分利用面法线、二面角等静态信息以及动态全局特征信息来充分使用所有类型的数据。另外，InfoGNN是一个端到端框架，简化了网络设计以使其更加高效，为复杂3D模型的深度学习铺平道路。我们在几个公开可用数据集上进行了实验，结果显示在网格分类和分割任务中，InfoGNN表现优异。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D models are widely used in various industries, and mesh data has become anindispensable part of 3D modeling because of its unique advantages. Mesh datacan provide an intuitive and practical expression of rich 3D information.However, its disordered, irregular data structure and complex surfaceinformation make it challenging to apply with deep learning models directly.Traditional mesh data processing methods often rely on mesh models with manylimitations, such as manifold, which restrict their application scopes inreality and do not fully utilize the advantages of mesh models. This paperproposes a novel end-to-end framework for addressing the challenges associatedwith deep learning in mesh models centered around graph neural networks (GNN)and is titled InfoGNN. InfoGNN treats the mesh model as a graph, which enablesit to handle irregular mesh data efficiently. Moreover, we propose InfoConv andInfoMP modules, which utilize the position information of the points and fullyuse the static information such as face normals, dihedral angles, and dynamicglobal feature information to fully use all kinds of data. In addition, InfoGNNis an end-to-end framework, and we simplify the network design to make it moreefficient, paving the way for efficient deep learning of complex 3D models. Weconducted experiments on several publicly available datasets, and the resultsshow that InfoGNN achieves excellent performance in mesh classification andsegmentation tasks.</description>
      <author>example@mail.com (Ling Gao, Zhenyu Shu, Shiqing Xin)</author>
      <guid isPermaLink="false">2503.02414v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OCL: Ordinal Contrastive Learning for Imputating Features with Progressive Labels</title>
      <link>http://arxiv.org/abs/2503.02899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  MICCAI 2024 (Provisional Accept)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的图像特征补全方法，利用多模态成像数据来更好地判断阿尔茨海默病的不同阶段。&lt;h4&gt;背景&lt;/h4&gt;准确地辨别阿尔茨海默病（AD）的各个发展阶段对于早期诊断和预防至关重要。然而，由于成本高和受试者负担重的原因，获取完整的影像集很具挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的补全方法来解决多模态成像数据中缺失值的问题，同时保持所有受试者的完整性。&lt;h4&gt;方法&lt;/h4&gt;所提方法包括两个网络：1）一个编码器，用于提取不受模式影响的嵌入；2）一个解码器，基于其影像模式重构原始测量。编码器采用了新颖的‘序数对比损失’来使样本根据AD进展对齐在嵌入空间中。&lt;h4&gt;主要发现&lt;/h4&gt;通过最大化的模态一致性以及领域对抗训练算法进一步增强了不同成像模态之间的对齐性，在实验结果中展示了对于统计分析和分类任务，该方法相较于基线补全技术有显著的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的模型在共享嵌入空间内促进了多模式影像特征的完整恢复，并且通过ADNI研究显示了优于现有技术的结果。&lt;h4&gt;翻译&lt;/h4&gt;准确地区分阿尔茨海默病（AD）的不同阶段对于早期诊断和预防至关重要。通常涉及多种成像模态来理解AD复杂的病理过程，然而由于成本高以及受试者负担重的原因，获取完整的一组图像具有挑战性。因此，在最终的实验中缺失数据不可避免地导致了样本量有限，并且降低了下游分析中的精确度。为了应对这一挑战，我们提出了一种全面的成像特征补全方法，该方法能够利用多样化的影像特征的同时保持所有受试者的完整性。所提的方法包括两个网络：1）一个编码器用于提取模态无关的嵌入；2）一个解码器根据各自的成像模式来重构原始测量值。编码器采用了新颖的‘序数对比损失’，将样本按AD进展在嵌入空间中对齐。我们还通过域对抗训练算法最大化了每个受试者内模态之间的嵌入一致性，进一步增强了不同影像模态之间的对齐性。所提的方法促进了在共享嵌入空间内的多模式成像特征的完整恢复。在实验中，我们展示了我们的网络对于统计分析和分类任务，相较于补全基线技术，在ADNI研究中提供了更优的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/978-3-031-72069-7_32&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurately discriminating progressive stages of Alzheimer's Disease (AD) iscrucial for early diagnosis and prevention. It often involves multiple imagingmodalities to understand the complex pathology of AD, however, acquiring acomplete set of images is challenging due to high cost and burden for subjects.In the end, missing data become inevitable which lead to limited sample-sizeand decrease in precision in downstream analyses. To tackle this challenge, weintroduce a holistic imaging feature imputation method that enables to leveragediverse imaging features while retaining all subjects. The proposed methodcomprises two networks: 1) An encoder to extract modality-independentembeddings and 2) A decoder to reconstruct the original measures conditioned ontheir imaging modalities. The encoder includes a novel {\em ordinal contrastiveloss}, which aligns samples in the embedding space according to the progressionof AD. We also maximize modality-wise coherence of embeddings within eachsubject, in conjunction with domain adversarial training algorithms, to furtherenhance alignment between different imaging modalities. The proposed methodpromotes our holistic imaging feature imputation across various modalities inthe shared embedding space. In the experiments, we show that our networksdeliver favorable results for statistical analysis and classification againstimputation baselines with Alzheimer's Disease Neuroimaging Initiative (ADNI)study.</description>
      <author>example@mail.com (Seunghun Baek, Jaeyoon Sim, Guorong Wu, Won Hwa Kim)</author>
      <guid isPermaLink="false">2503.02899v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation</title>
      <link>http://arxiv.org/abs/2503.01776v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A novel sparse coding framework designed for learning adaptive  representation&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的稀疏表示方法CSR，它在保持高保真度的同时，能够实现自适应的深度表示，并且比现有的解决方案MRL具有更高的准确性和更快的速度。&lt;h4&gt;背景&lt;/h4&gt;许多大规模系统依赖高质量的深度表示（嵌入）来完成检索、搜索和生成建模等任务。最近提出的Matryoshka Representation Learning (MRL) 方法可以实现自适应的嵌入长度，但是需要重新训练模型并且在较短长度时性能下降明显。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的稀疏编码方法CSR，以解决现有解决方案在保持高保真度的同时，提供灵活、成本效益高的不同稀疏级别推理的问题。&lt;h4&gt;方法&lt;/h4&gt;通过利用轻量级的自动编码和任务感知对比目标，将预训练嵌入稀疏化到一个高维但选择性激活的功能空间中。这种方法被称为Contrastive Sparse Representation (CSR)。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与MRL相比，CSR在图像、文本以及多模态基准数据集上，在准确性方面有显著提高，并且检索速度更快，同时训练时间大大缩短。&lt;h4&gt;结论&lt;/h4&gt;研究结果确立了稀疏编码作为自适应表示学习的重要范式，尤其是在实际应用中效率和保真度都至关重要的情况下。&lt;h4&gt;翻译&lt;/h4&gt;许多大型系统依赖于高质量的深度表示（嵌入）来促进检索、搜索和生成建模等任务。最近提出的Matryoshka Representation Learning (MRL) 方法作为一种适应性表示长度的方法出现，但它需要重新训练整个模型并且在较短长度时性能明显下降。本文中，我们展示了稀疏编码提供了一种吸引人的替代方案，以实现具有最小开销和更高保真度的自适应表示。我们提出了对比稀疏表示（CSR）方法，该方法将预训练嵌入转换为一个高维但选择性激活的功能空间。通过利用轻量级自动编码器和任务感知对比目标，CSR在保持语义质量的同时允许不同稀疏级别的灵活、成本效益高的推理。广泛的实验表明，与MRL相比，CSR在图像、文本以及多模态基准数据集上的准确性方面有显著提高，并且检索速度更快，同时训练时间大大缩短。我们的结果确立了稀疏编码作为自适应表示学习的重要范式，在实际应用中效率和保真度都至关重要的情况下尤为适用。代码可在https://github.com/neilwen987/CSR_Adaptive_Rep上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/neilwen987/CSR_Adaptive_Rep&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many large-scale systems rely on high-quality deep representations(embeddings) to facilitate tasks like retrieval, search, and generativemodeling. Matryoshka Representation Learning (MRL) recently emerged as asolution for adaptive embedding lengths, but it requires full model retrainingand suffers from noticeable performance degradations at short lengths. In thispaper, we show that sparse coding offers a compelling alternative for achievingadaptive representation with minimal overhead and higher fidelity. We proposeContrastive Sparse Representation (CSR), a method that sparsifies pre-trainedembeddings into a high-dimensional but selectively activated feature space. Byleveraging lightweight autoencoding and task-aware contrastive objectives, CSRpreserves semantic quality while allowing flexible, cost-effective inference atdifferent sparsity levels. Extensive experiments on image, text, and multimodalbenchmarks demonstrate that CSR consistently outperforms MRL in terms of bothaccuracy and retrieval speed-often by large margins-while also cutting trainingtime to a fraction of that required by MRL. Our results establish sparse codingas a powerful paradigm for adaptive representation learning in real-worldapplications where efficiency and fidelity are both paramount. Code isavailable at https://github.com/neilwen987/CSR_Adaptive_Rep</description>
      <author>example@mail.com (Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You)</author>
      <guid isPermaLink="false">2503.01776v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>CMMLoc: Advancing Text-to-PointCloud Localization with Cauchy-Mixture-Model Based Framework</title>
      <link>http://arxiv.org/abs/2503.02593v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的基于文本描述的点云定位框架CMMLoc，用于在大型城市环境中通过文本描述确定3D位置。&lt;h4&gt;背景&lt;/h4&gt;现有的点云定位方法通常需要完整的环境描述来匹配3D位置和文本描述。然而，在实际场景中，用户往往只提供部分相关的环境信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够在仅有部分相关环境描述的情况下进行有效定位的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于Cauchy混合模型（CMM）的不确定性感知框架CMMLoc，该框架在文本和点云之间交互时将CMM约束作为先验条件，并设计了空间整合方案来适应不同3D对象的不同感受野。此外，还提出了一个方向积分模块和模态预对准策略，以捕获物体之间的空间关系。&lt;h4&gt;主要发现&lt;/h4&gt;CMMLoc在KITTI360Pose数据集上的表现优于现有方法，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的CMMLoc框架能够有效解决基于文本描述的点云定位问题，并具有重要的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;点云定位任务的目标是在大型城市环境中通过文本描述来确定一个3D位置。该技术在车辆接驾或货物配送等众多领域都有潜在的应用前景。理想情况下，对于每一个文本描述和相应的3D位置来说，周围环境的所有物体都应该被详细描述出来。然而，在实际情况中，如车辆接驾场景下，乘客通常只会提及最重要的及最接近的目标物而不是整个环境。为了解决这种部分相关性的挑战，我们提出了一个基于Cauchy混合模型（CMM）的不确定性感知框架CMMLoc来解决从文本到点云定位的问题。通过在模态间交互时将CMM约束作为先验条件并设计空间整合方案来处理不同3D对象的不同感受野，实现了对部分相关环境描述的有效处理。为了实现精确的定位，我们还提出了一种方向积分模块以及一种模态预对准策略，帮助捕捉物体之间的空间关系，并使3D物体更加接近文本模式。通过广泛的实验验证，CMMLoc在KITTI360Pose数据集上优于现有方法，并取得了最先进的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of point cloud localization based on linguistic description is toidentify a 3D position using textual description in large urban environments,which has potential applications in various fields, such as determining thelocation for vehicle pickup or goods delivery. Ideally, for a textualdescription and its corresponding 3D location, the objects around the 3Dlocation should be fully described in the text description. However, inpractical scenarios, e.g., vehicle pickup, passengers usually describe only thepart of the most significant and nearby surroundings instead of the entireenvironment. In response to this $\textbf{partially relevant}$ challenge, wepropose $\textbf{CMMLoc}$, an uncertainty-aware$\textbf{C}$auchy-$\textbf{M}$ixture-$\textbf{M}$odel ($\textbf{CMM}$) basedframework for text-to-point-cloud $\textbf{Loc}$alization. To model theuncertain semantic relations between text and point cloud, we integrate CMMconstraints as a prior during the interaction between the two modalities. Wefurther design a spatial consolidation scheme to enable adaptive aggregation ofdifferent 3D objects with varying receptive fields. To achieve preciselocalization, we propose a cardinal direction integration module alongside amodality pre-alignment strategy, helping capture the spatial relationshipsamong objects and bringing the 3D objects closer to the text modality.Comprehensive experiments validate that CMMLoc outperforms existing methods,achieving state-of-the-art results on the KITTI360Pose dataset. Codes areavailable in this GitHub repository https://github.com/kevin301342/CMMLoc.</description>
      <author>example@mail.com (Yanlong Xu, Haoxuan Qu, Jun Liu, Wenxiao Zhang, Xun Yang)</author>
      <guid isPermaLink="false">2503.02593v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Teaching Metric Distance to Autoregressive Multimodal Foundational Models</title>
      <link>http://arxiv.org/abs/2503.02379v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着大型语言模型的应用领域从自然语言扩展到数学、多模态理解和具身代理等领域，输出标记逐渐反映了度量关系而不是纯粹的语言意义。本文介绍了DIST2Loss，这是一种基于预定义的输出标记之间距离关系来训练自回归离散模型的距离感知框架。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型的应用已经从自然语言处理领域扩展到数学、多模态理解和具身代理等领域，在这些新的应用领域中，令牌越来越反映了度量关系而不是纯粹的语言意义。&lt;h4&gt;目的&lt;/h4&gt;提出DIST2Loss方法来训练自回归离散模型，并保持现有架构的兼容性。&lt;h4&gt;方法&lt;/h4&gt;DIST2Loss通过将从固有距离度量衍生出的连续指数族分布转换为与模型架构相容的离散、分类优化目标，使模型能够学习和保留有意义的距离关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，在视觉接地、机器人操作、生成性奖励建模以及使用向量量化特征进行图像生成等多元模态应用中均显示出一致性的性能改进。在训练数据有限的情况下，这些改进尤为明显。&lt;h4&gt;结论&lt;/h4&gt;DIST2Loss能够使模型即使在资源受限的环境中也表现出色，在各种多模态应用场景中具有明显的性能提升效果。&lt;h4&gt;翻译&lt;/h4&gt;随着大型语言模型的应用领域从自然语言处理向数学、多模态理解和具身代理等领域扩展，模型输出标记开始更多地反映度量关系而非纯粹的语言意义。为此，本文提出了一种名为DIST2Loss的框架，该框架利用预定义的距离关系来训练自回归离散模型，并通过将连续指数族分布转化为与现有架构兼容的目标，使模型能够有效地学习并保持有意义的距离关系。实验结果显示，在视觉接地、机器人操作等多模态应用领域中，DIST2Loss显著提升了性能表现。特别是在数据量有限的情况下，其优势尤为突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large language models expand beyond natural language to domains such asmathematics, multimodal understanding, and embodied agents, tokens increasinglyreflect metric relationships rather than purely linguistic meaning. Weintroduce DIST2Loss, a distance-aware framework designed to trainautoregressive discrete models by leveraging predefined distance relationshipsamong output tokens. At its core, DIST2Loss transforms continuous exponentialfamily distributions derived from inherent distance metrics into discrete,categorical optimization targets compatible with the models' architectures.This approach enables the models to learn and preserve meaningful distancerelationships during token generation while maintaining compatibility withexisting architectures. Empirical evaluations show consistent performance gainsin diverse multimodal applications, including visual grounding, roboticmanipulation, generative reward modeling, and image generation usingvector-quantized features. These improvements are pronounced in cases oflimited training data, highlighting DIST2Loss's effectiveness inresource-constrained settings.</description>
      <author>example@mail.com (Jiwan Chung, Saejin Kim, Yongrae Jo, Jaewoo Park, Dongjun Min, Youngjae Yu)</author>
      <guid isPermaLink="false">2503.02379v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Random Walks in Self-supervised Learning for Triangular Meshes</title>
      <link>http://arxiv.org/abs/2503.00816v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种用于3D网格分析的自监督学习新方法，通过随机游走进行数据增强以生成网格表面的各种表示，并结合对比和聚类损失来提高训练效果。&lt;h4&gt;背景&lt;/h4&gt;现有基于3D模型的数据增强方法缺乏有效的策略来扩展和丰富给定数据集中的样例表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督学习框架，通过随机游走作为数据增强手段生成多样化的网格表示，并结合对比和聚类损失来提高模型在下游任务上的性能。&lt;h4&gt;方法&lt;/h4&gt;采用基于随机游走的数据增广技术，利用对比损失最大化同一网格的多个实例之间的相似性同时最小化不同网格间的相似性；引入了聚类损失以增强类别区分度并减少训练中的方差。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在使用均值平均精度（mAP）分数和监督SVM线性分类器提取特征进行评估时表现出了良好的性能，显示出其在物体分类和形状检索等下游任务上的潜力。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过引入随机游走、对比损失以及聚类损失的结合，在3D网格分析领域展现出了显著的进步，为后续研究提供了新的思路和可能的应用场景。&lt;h4&gt;翻译&lt;/h4&gt;这项研究针对三维网格自监督学习挑战提出了新方法。利用随机游走作为数据增强技术生成多样化的网格表示，并采用了对比及聚类损失来优化模型训练过程中的效果。通过mAP分数以及监督SVM线性分类器进行评估，结果表明该模型具备在物体分类和形状检索等下游任务上的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenge of self-supervised learning for 3D meshanalysis. It presents an new approach that uses random walks as a form of dataaugmentation to generate diverse representations of mesh surfaces. Furthermore,it employs a combination of contrastive and clustering losses. The contrastivelearning framework maximizes similarity between augmented instances of the samemesh while minimizing similarity between different meshes. We integrate thiswith a clustering loss, enhancing class distinction across training epochs andmitigating training variance. Our model's effectiveness is evaluated using meanAverage Precision (mAP) scores and a supervised SVM linear classifier onextracted features, demonstrating its potential for various downstream taskssuch as object classification and shape retrieval.</description>
      <author>example@mail.com (Gal Yefet, Ayellet Tal)</author>
      <guid isPermaLink="false">2503.00816v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Streamline-based diffusion MRI Tractography Registration Method with Probabilistic Keypoint Detection</title>
      <link>http://arxiv.org/abs/2503.02481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;扩散MRI纤维束配准是分析大脑白质组内相似性和变异性的关键步骤。提出了一种基于深度学习的无监督方法，通过检测和匹配不同受试者之间的点对来实现基于流线的dMRI纤维束配准。&lt;h4&gt;背景&lt;/h4&gt;现有的配准方法主要依赖于优化空间距离以确定最佳变换，但这种方法忽略了流线内部的点连接模式，从而限制了它们识别不同数据集间解剖对应关系的能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的无监督深度学习方法来进行基于流线的dMRI纤维束配准，通过在受试者之间找到对应的点对来实现解剖结构的一致性匹配。&lt;h4&gt;方法&lt;/h4&gt;将轨迹建模为点云以利用沿着流线的图连接性，并提出了一个新的流线关键点检测方法，将其作为一个概率分类任务，用于识别不同不规则流线集间的解剖一致性对应关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在纤维束配准性能上表现出高有效性和高效性，优于现有的几种方法。&lt;h4&gt;结论&lt;/h4&gt;该研究为基于深度学习的无监督dMRI纤维束配准提供了一种新的视角，并展示了其在识别解剖对应关系方面的能力和潜力。&lt;h4&gt;翻译&lt;/h4&gt;扩散磁共振成像（dMRI）轨迹图注册是分析大脑白质组内相似性和变异性的关键步骤。流线基注册方法利用纤维路径的三维几何信息以实现配准后的空间对齐。现有方法通常依赖于优化空间距离来确定最优变换，但这种方法忽略了流线内部的点连接模式，限制了其识别不同轨迹数据集间解剖对应关系的能力。本文提出了一种新的基于深度学习的无监督方法来进行流线基dMRI轨迹图注册，该方法通过在受试者之间找到对应的点对来实现轨迹数据集的空间对齐。我们在实验中比较了几种现有方法，并展示了高度有效的和高效的轨迹注册性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Registration of diffusion MRI tractography is an essential step for analyzinggroup similarities and variations in the brain's white matter (WM).Streamline-based registration approaches can leverage the 3D geometricinformation of fiber pathways to enable spatial alignment after registration.Existing methods usually rely on the optimization of the spatial distances toidentify the optimal transformation. However, such methods overlook pointconnectivity patterns within the streamline itself, limiting their ability toidentify anatomical correspondences across tractography datasets. In this work,we propose a novel unsupervised approach using deep learning to performstreamline-based dMRI tractography registration. The overall idea is toidentify corresponding keypoint pairs across subjects for spatial alignment oftractography datasets. We model tractography as point clouds to leverage thegraph connectivity along streamlines. We propose a novel keypoint detectionmethod for streamlines, framed as a probabilistic classification task toidentify anatomically consistent correspondences across unstructured streamlinesets. In the experiments, we compare several existing methods and show highlyeffective and efficient tractography registration performance.</description>
      <author>example@mail.com (Junyi Wang, Mubai Du, Ye Wu, Yijie Li, William M. Wells III, Lauren J. O'Donnell, Fan Zhang)</author>
      <guid isPermaLink="false">2503.02481v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Incorporating graph neural network into route choice model</title>
      <link>http://arxiv.org/abs/2503.02315v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种结合递归逻辑模型和图神经网络（GNN）的新颖混合模型，旨在提高路线选择预测的准确性和可解释性。&lt;h4&gt;背景&lt;/h4&gt;传统上，基于理论的模型如Logit模型和递归Logit模型因其良好的可解释性而被广泛使用。然而，近年来机器学习方法由于其更好的预测准确性而受到关注。尽管图神经网络（GNN）在捕捉道路网络特征方面表现出色，并且已经在其他交通研究领域得到了广泛应用，但它们尚未用于路线选择建模。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的混合模型，将递归逻辑模型与图神经网络结合，以提高预测性能和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新颖的混合模型，该模型集成了递归Logit模型与图神经网络（GNN），并展示了这种组合如何改进预测性能以及放松无关替代品独立性的假设而不依赖于强假设。这得益于特定类型的GNN能够从数据中有效捕捉到网络上的多个交叉效应模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，应用该方法后，模型的预测准确性相较于现有模型有显著提高。&lt;h4&gt;结论&lt;/h4&gt;结合递归逻辑模型和图神经网络（GNN）的方法可以有效地提升路线选择预测的准确性和可解释性。这种新方法为未来的交通研究提供了新的视角与可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：路线选择模型是运输研究中最重要基础之一，传统理论驱动模型如Logit模型以及递归逻辑模型因其优秀的可解释性而被广泛使用；最近机器学习技术因更高的预测准确度受到关注。在此研究中提出结合递归逻辑模型与图神经网络（GNN）的新颖混合方法以提升预判精度和模型的透明度。据作者所知，尽管图神经网络在捕获道路网络特性方面表现出色并广泛应用于运输领域的其他领域，但它们还未被用于路线选择建模。数学上证明使用图神经网络不仅有利于增强预测性能，而且可以通过不依赖于强烈假设的方式来放松无关替代品独立性的属性。特定类型的GNN能够从数据中高效捕捉到网络上的多个交叉效应模式，从而提升了这一特性。通过将提议模型应用于东京一天的旅行轨迹数据，确认了比现有模型更高的预测精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Route choice models are one of the most important foundations fortransportation research. Traditionally, theory-based models have been utilizedfor their great interpretability, such as logit models and Recursive logitmodels. More recently, machine learning approaches have gained attentions fortheir better prediction accuracy. In this study, we propose novel hybrid modelsthat integrate the Recursive logit model with Graph Neural Networks (GNNs) toenhance both predictive performance and model interpretability. To the authors'knowldedge, GNNs have not been utilized for route choice modeling, despitetheir proven effectiveness in capturing road network features and theirwidespread use in other transportation research areas. We mathematically showthat our use of GNN is not only beneficial for enhancing the predictionperformance, but also relaxing the Independence of Irrelevant Alternativesproperty without relying on strong assumptions. This is due to the fact that aspecific type of GNN can efficiently capture multiple cross-effect patterns onnetworks from data. By applying the proposed models to one-day traveltrajectory data in Tokyo, we confirmed their higher prediction accuracycompared to the existing models.</description>
      <author>example@mail.com (Yuxun Ma, Toru Seo)</author>
      <guid isPermaLink="false">2503.02315v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Effective High-order Graph Representation Learning for Credit Card Fraud Detection</title>
      <link>http://arxiv.org/abs/2503.01556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 5 figures, accepted at IJCAI 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的高阶图表示学习模型（HOGRL），旨在解决现有图神经网络在处理伪装的间接多跳交易时遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;信用卡欺诈给持卡人和发卡银行带来了巨大的成本，而欺诈者常通过使用数个良性用户的合法交易来规避反欺诈检测系统。现有的图神经网络（GNN）模型由于深层次聚合过程中的过度平滑问题难以学习伪装的间接多跳交易特征。&lt;h4&gt;目的&lt;/h4&gt;提出HOGRL以避免在多层次聚合过程中引入过多噪音，直接从高阶交易图中学习不同次序的纯表示来识别欺诈者的多步间接交易，并优化欺诈检测性能。&lt;h4&gt;方法&lt;/h4&gt;通过有效构建高阶交易图并学习每种顺序的纯表示，使得模型能够通过多层纯特征学习发现伪装关系。同时引入混合专家注意力机制以自动确定各阶的重要性，共同优化欺诈检测性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示HOGRL在开源和真实世界数据集上相比现有最佳基准方法有显著改进，证实了其在应对高阶欺诈伪装犯罪方面的有效性。&lt;h4&gt;结论&lt;/h4&gt;HOGRL能够更有效地识别复杂的多跳交易模式，增强对伪装的欺诈行为检测能力。&lt;h4&gt;翻译&lt;/h4&gt;信用卡欺诈给持卡人和发卡银行带来了巨大的成本。欺诈者经常通过使用多个良性用户合法进行的交易来规避反欺诈系统。现有的图神经网络（GNN）模型由于在深层次聚合过程中的过度平滑问题，难以学习伪装交易中复杂的间接多跳特征，这是识别伪装关系的主要挑战。因此，在本文中我们提出了一种新的高阶图表示学习模型（HOGRL），以避免在多层次聚合过程中引入过多噪音，并直接从高阶交易图中获得不同的纯表示。为实现这一目标，首先构建有效的高阶交易图，然后学习每个顺序的纯表示，使模型能够通过多层纯特征学习识别欺诈者的间接多跳交易。此外我们还提出了一种混合专家注意力机制来自动确定不同顺序的重要性，并优化整体反欺诈性能。在开源和实际数据集上的广泛实验表明，HOGRL相比于现有的最佳基准有显著提高，证明了其处理复杂伪装行为的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.24963/ijcai.2024/839&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Credit card fraud imposes significant costs on both cardholders and issuingbanks. Fraudsters often disguise their crimes, such as using legitimatetransactions through several benign users to bypass anti-fraud detection.Existing graph neural network (GNN) models struggle with learning features ofcamouflaged, indirect multi-hop transactions due to their inherentover-smoothing issues in deep multi-layer aggregation, presenting a majorchallenge in detecting disguised relationships. Therefore, in this paper, wepropose a novel High-order Graph Representation Learning model (HOGRL) to avoidincorporating excessive noise during the multi-layer aggregation process. Inparticular, HOGRL learns different orders of \emph{pure} representationsdirectly from high-order transaction graphs. We realize this goal byeffectively constructing high-order transaction graphs first and then learningthe \emph{pure} representations of each order so that the model could identifyfraudsters' multi-hop indirect transactions via multi-layer \emph{pure} featurelearning. In addition, we introduce a mixture-of-expert attention mechanism toautomatically determine the importance of different orders for jointlyoptimizing fraud detection performance. We conduct extensive experiments inboth the open source and real-world datasets, the result demonstrates thesignificant improvements of our proposed HOGRL compared with state-of-the-artfraud detection baselines. HOGRL's superior performance also proves itseffectiveness in addressing high-order fraud camouflage criminals.</description>
      <author>example@mail.com (Yao Zou, Dawei Cheng)</author>
      <guid isPermaLink="false">2503.01556v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Bridging Spectral-wise and Multi-spectral Depth Estimation via Geometry-guided Contrastive Learning</title>
      <link>http://arxiv.org/abs/2503.00793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICRA 2025, Github link:  https://github.com/UkcheolShin/BridgeMultiSpectralDepth&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;部署深度估计网络到现实世界中需要具备对抗各种不利条件的高度鲁棒性，以确保安全和可靠的自主驾驶。&lt;h4&gt;目的&lt;/h4&gt;为了提高多模态传感器系统在自动驾驶车辆中的使用效率，提出了一种基于对齐与融合策略的解决方案，用于从多光谱图像进行深度估计。&lt;h4&gt;方法&lt;/h4&gt;{'align阶段': '通过最小化全局特征及几何线索的空间一致性局部特征对比损失来调整多个光谱带之间的嵌入空间，以学习跨多光谱图像可共享的表示。', 'fuse阶段': '训练一个可附加的功能融合模块，该模块可以有选择地聚合多光谱特性，用于可靠和鲁棒的预测结果。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的策略使得单深度网络可以在保持可靠性、内存效率和灵活性的同时，实现光谱不变性和多光谱融合的深度估计。&lt;h4&gt;结论&lt;/h4&gt;基于align-and-fuse策略的方法为跨多个光谱带进行深度估计提供了一种有效的解决方案，并且能够提高预测结果的可靠性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deploying depth estimation networks in the real world requires high-levelrobustness against various adverse conditions to ensure safe and reliableautonomy. For this purpose, many autonomous vehicles employ multi-modal sensorsystems, including an RGB camera, NIR camera, thermal camera, LiDAR, or Radar.They mainly adopt two strategies to use multiple sensors: modality-wise andmulti-modal fused inference. The former method is flexible butmemory-inefficient, unreliable, and vulnerable. Multi-modal fusion can providehigh-level reliability, yet it needs a specialized architecture. In this paper,we propose an effective solution, named align-and-fuse strategy, for the depthestimation from multi-spectral images. In the align stage, we align embeddingspaces between multiple spectrum bands to learn shareable representation acrossmulti-spectral images by minimizing contrastive loss of global and spatiallyaligned local features with geometry cue. After that, in the fuse stage, wetrain an attachable feature fusion module that can selectively aggregate themulti-spectral features for reliable and robust prediction results. Based onthe proposed method, a single-depth network can achieve both spectral-invariantand multi-spectral fused depth estimation while preserving reliability, memoryefficiency, and flexibility.</description>
      <author>example@mail.com (Ukcheol Shin, Kyunghyun Lee, Jean Oh)</author>
      <guid isPermaLink="false">2503.00793v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>CrystalFramer: Rethinking the Role of Frames for SE(3)-Invariant Crystal Structure Modeling</title>
      <link>http://arxiv.org/abs/2503.02209v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 main pages, 3 main figures, and 4 main tables. Published as a  conference paper at ICLR 2025. This version moves some appendices into the  main text. For more information, see  https://omron-sinicx.github.io/crystalframer/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;晶格结构建模在材料信息学的各种应用中至关重要，并且捕捉SE(3)不变几何特征是这些网络的基本需求。&lt;h4&gt;问题&lt;/h4&gt;确定晶体结构的框架具有挑战性，因为它们是无限和高度对称的，这与分子不同。&lt;h4&gt;传统方法限制&lt;/h4&gt;现有的方法依赖于每个结构由其自身的结构性信息静态固定的一个框架来实现。&lt;h4&gt;新概念&lt;/h4&gt;提出了动态框架的概念，这些框架在考虑到晶体无限性和对称性的前提下，为每一个原子提供一个关注局部环境中活动交互原子的动态视图。&lt;h4&gt;应用技术&lt;/h4&gt;通过利用最近基于变压器的晶格编码器中的注意力机制，提出了一种称为CrystalFramer的新架构来实现这一概念。&lt;h4&gt;实验结果&lt;/h4&gt;广泛的实验证明了与传统框架和现有晶格编码器相比，CrystalFramer在各种晶体性质预测任务中表现更佳。&lt;h4&gt;贡献&lt;/h4&gt;该研究质疑了结构对齐坐标系统的简单性，并引入了一个新的、动态的视角来理解和建模复杂的晶体几何特征。&lt;h4&gt;翻译&lt;/h4&gt;使用图神经网络进行晶格结构建模对于材料信息学中的许多应用至关重要，捕捉SE(3)不变的几何特性是这些网络的基本需求。一种直接的方法是通过结构对齐坐标系统或“框架”建模定向标准化的结构。然而，与分子不同，确定晶体结构的框架具有挑战性，因为它们具有无限和高度对称的性质。特别是，现有方法依赖于由每个结构自身的结构性信息静态固定的一个框架，而不考虑任务本身的需求。这里，我们重新思考了“框架”的角色，质疑这种仅基于结构简单化的对齐是否足够，并提出了动态框架的概念。这些框架在考虑到晶体的无限性和对称性的前提下，为每一个原子提供一个关注局部环境中活动交互原子的动态视图。通过利用最近基于变压器的晶格编码器中的注意力机制，我们实现了这一概念，提出了一种新的架构叫作CrystalFramer。广泛的实验证明了与传统框架和现有晶格编码器相比，CrystalFramer在各种晶体性质预测任务中表现更佳。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Crystal structure modeling with graph neural networks is essential forvarious applications in materials informatics, and capturing SE(3)-invariantgeometric features is a fundamental requirement for these networks. Astraightforward approach is to model with orientation-standardized structuresthrough structure-aligned coordinate systems, or"frames." However, unlikemolecules, determining frames for crystal structures is challenging due totheir infinite and highly symmetric nature. In particular, existing methodsrely on a statically fixed frame for each structure, determined solely by itsstructural information, regardless of the task under consideration. Here, werethink the role of frames, questioning whether such simplistic alignment withthe structure is sufficient, and propose the concept of dynamic frames. Whileaccommodating the infinite and symmetric nature of crystals, these framesprovide each atom with a dynamic view of its local environment, focusing onactively interacting atoms. We demonstrate this concept by utilizing theattention mechanism in a recent transformer-based crystal encoder, resulting ina new architecture called CrystalFramer. Extensive experiments show thatCrystalFramer outperforms conventional frames and existing crystal encoders invarious crystal property prediction tasks.</description>
      <author>example@mail.com (Yusei Ito, Tatsunori Taniai, Ryo Igarashi, Yoshitaka Ushiku, Kanta Ono)</author>
      <guid isPermaLink="false">2503.02209v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Projection Head is Secretly an Information Bottleneck</title>
      <link>http://arxiv.org/abs/2503.00507v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文深入探讨了对比学习中投影头的作用机制，并从信息理论的角度提供了理论上的理解，提出了一种新的方法来改进投影器的设计。&lt;h4&gt;背景&lt;/h4&gt;最近，对比学习作为一种提取有意义数据表示的方法越来越受到重视。训练时在编码器顶部添加一个投影头并在下游任务中移除该投影头已被证明能显著提升对比学习的效果。&lt;h4&gt;目的&lt;/h4&gt;研究和理解投影头背后的机制，并提出改进方法以提高对比学习的性能。&lt;h4&gt;方法&lt;/h4&gt;从信息理论的角度出发，建立了对投影之前特征的下游表现的理论保证。基于这些理论见解，引入了带有训练和结构正则化的投影器修改方案。&lt;h4&gt;主要发现&lt;/h4&gt;有效的投影头应该充当一个信息瓶颈，过滤掉与对比目标无关的信息。&lt;h4&gt;结论&lt;/h4&gt;在CIFAR-10、CIFAR-100和ImageNet-100等多个实际数据集上进行实验验证后，所提出的方法表现出一致的性能提升。研究者认为其对投影头作用机制的理解将为这一领域带来更加原则化与先进的设计。&lt;h4&gt;翻译&lt;/h4&gt;最近，对比学习作为一种提取有意义数据表示的方法越来越受到重视。在各种特殊设计中，训练时在编码器顶部添加一个投影头并在下游任务中移除该投影头已被证明能显著提升对比学习的效果。然而尽管这种方法在经验上取得成功，其背后的机制却尚未得到充分研究。本文从信息理论的角度深入探讨了投影头的作用，并建立了对特征在投影之前进行下游表现的理论保证，揭示了一个有效的投影器应该充当一个信息瓶颈，过滤掉与对比目标无关的信息。基于这些理论见解，我们引入了带有训练和结构正则化的投影器修改方案。经验上，在包括CIFAR-10、CIFAR-100和ImageNet-100在内的多个真实数据集上的下游性能表现出一致的提升。我们认为对投影头作用机制的理解将为这一领域带来更加原则化与先进的设计。代码可在https://github.com/PKU-ML/Projector_Theory获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, contrastive learning has risen to be a promising paradigm forextracting meaningful data representations. Among various special designs,adding a projection head on top of the encoder during training and removing itfor downstream tasks has proven to significantly enhance the performance ofcontrastive learning. However, despite its empirical success, the underlyingmechanism of the projection head remains under-explored. In this paper, wedevelop an in-depth theoretical understanding of the projection head from theinformation-theoretic perspective. By establishing the theoretical guaranteeson the downstream performance of the features before the projector, we revealthat an effective projector should act as an information bottleneck, filteringout the information irrelevant to the contrastive objective. Based ontheoretical insights, we introduce modifications to projectors with trainingand structural regularizations. Empirically, our methods exhibit consistentimprovement in the downstream performance across various real-world datasets,including CIFAR-10, CIFAR-100, and ImageNet-100. We believe our theoreticalunderstanding on the role of the projection head will inspire more principledand advanced designs in this field. Code is available athttps://github.com/PKU-ML/Projector_Theory.</description>
      <author>example@mail.com (Zhuo Ouyang, Kaiwen Hu, Qi Zhang, Yifei Wang, Yisen Wang)</author>
      <guid isPermaLink="false">2503.00507v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>African Gender Classification Using Clothing Identification Via Deep Learning</title>
      <link>http://arxiv.org/abs/2503.00058v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3 Pages, 10 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了在非理想条件下进行性别分类的新方法，通过分析非洲传统服饰来进行性别识别。&lt;h4&gt;背景&lt;/h4&gt;传统的性别分类技术主要依赖于面部识别，在诸如模糊图像、侧面视角或部分遮挡的情况下效果不佳。&lt;h4&gt;目的&lt;/h4&gt;探索基于服装的性别分类作为一种补充技术的可能性，并特别关注非洲传统服饰的文化特性和性别特定特征。&lt;h4&gt;方法&lt;/h4&gt;使用AFRIFASHION1600数据集，该数据集中有1,600张标记为男性和女性两个类别的非洲传统服饰图像。通过修改后的VGG16架构并采用迁移学习技术开发了一个深度学习模型。为了克服小样本带来的问题以及避免过拟合，使用了数据增强。&lt;h4&gt;主要发现&lt;/h4&gt;所开发的模型在测试集上的准确率为87%，尽管存在女性样本占多数的数据不平衡情况，依然表现出强大的预测能力。&lt;h4&gt;结论&lt;/h4&gt;论文强调了基于服装的性别识别作为面部识别技术补充的可能性，并建议未来的研究应集中在扩展和平衡数据集上，以提高分类稳健性和实际应用性。&lt;h4&gt;翻译&lt;/h4&gt;人类属性识别和分类在计算机视觉中至关重要，推动着创新识别系统的开发。传统性别分类方法主要依赖于面部识别，在诸如图像模糊、侧面视角或部分遮挡的情况下效果不佳。本研究探索了一种替代方法，通过利用服装识别进行性别分类，特别是非洲传统服饰，因为这些服饰具有文化特定和性别的独特特征。我们使用了AFRIFASHION1600数据集，这是一个包含1,600张图片的数据集合，其中的非洲传统服装被标记为男性和女性两类。基于修改后的VGG16架构并通过迁移学习进行训练的一个深度学习模型已被开发出来用于分类。为了应对相对较小的数据集带来的挑战，并减少过拟合的风险，我们采用了数据增强技术。尽管存在样本不平衡的问题（特别是更多的女性样本），我们的模型在测试集中达到了87%的准确性，显示出强大的预测能力。这些研究结果突显了基于服装识别作为面部识别补充的可能性，在非洲文化背景下用于性别分类。未来的研究所应关注的方向是扩大和平衡数据集，以增强分类的稳健性和提高服装基础性别的识别系统的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human attribute identification and classification are crucial in computervision, driving the development of innovative recognition systems. Traditionalgender classification methods primarily rely on facial recognition, which,while effective, struggles under non-ideal conditions such as blurriness, sideviews, or partial occlusions. This study explores an alternative approach byleveraging clothing identification, specifically focusing on Africantraditional attire, which carries culturally significant and gender-specificfeatures.  We use the AFRIFASHION1600 dataset, a curated collection of 1,600 images ofAfrican traditional clothing labeled into two gender classes: male and female.A deep learning model, based on a modified VGG16 architecture and trained usingtransfer learning, was developed for classification. Data augmentation wasapplied to address the challenges posed by the relatively small dataset and tomitigate overfitting. The model achieved an accuracy of 87% on the test set,demonstrating strong predictive capability despite dataset imbalances favoringfemale samples.  These findings highlight the potential of clothing-based identification as acomplementary technique to facial recognition for gender classification inAfrican contexts. Future research should focus on expanding and balancingdatasets to enhance classification robustness and improve the applicability ofclothing-based gender recognition systems.</description>
      <author>example@mail.com (Samuel Ozechi)</author>
      <guid isPermaLink="false">2503.00058v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Introspective Loop Closure for SLAM with 4D Imaging Radar</title>
      <link>http://arxiv.org/abs/2503.02383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted for publication in the IEEE  International Conference on Robotics and Automation(ICRA), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了在SLAM中使用4D雷达进行循环闭合检测的方法，特别是针对相似和相反视角的情况。&lt;h4&gt;背景&lt;/h4&gt;移动机器人可以在没有外部定位系统或预先存在的地图的情况下通过同时定位与建图（SLAM）技术导航。雷达作为有价值的感觉工具尤其适用于视线被阻挡的环境，因为它受到颗粒的影响比激光雷达或摄像头小得多。现代4D成像雷达提供三维几何信息和相对速度测量。&lt;h4&gt;目的&lt;/h4&gt;研究如何使用4D雷达数据在SLAM中进行循环闭合检测，以提高地图准确性和减少轨迹漂移。&lt;h4&gt;方法&lt;/h4&gt;生成子图来表示更密集的环境，并采用内省度量方法排除特征退化环境中错误检测。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在几何多样的场景下，对于相似和相反视角，循环闭合检测能够保持高精度。轨迹估计可以得到高达82%的改进ATE（绝对轨迹误差），并且能够在自类似环境里拒绝假阳性。&lt;h4&gt;结论&lt;/h4&gt;通过4D雷达在SLAM中的应用，提高了移动机器人在复杂环境下的导航能力，特别是在视线受限的情况中表现出了优越性。&lt;h4&gt;翻译&lt;/h4&gt;Simultaneous Localization and Mapping (SLAM) enables mobile robots to navigate without external positioning systems or pre-existing maps. Radar is emerging as a valuable sensing tool, especially in vision-obstructed environments, as it is less affected by particles than lidars or cameras. Modern 4D imaging radars provide three-dimensional geometric information and relative velocity measurements, but they bring challenges such as small field of view and sparse, noisy point clouds. Detecting loop closures in SLAM is critical for reducing trajectory drift and maintaining map accuracy. However, the directional nature of 4D radar data makes identifying loop closures, especially from reverse viewpoints, difficult due to limited scan overlap. This article explores using 4D radar for loop closure in SLAM, focusing on similar and opposing viewpoints. We generate submaps for a denser environment representation and use introspective measures to reject false detections in feature-degenerate environments. Our experiments show accurate loop closure detection in geometrically diverse settings for both similar and opposing viewpoints, improving trajectory estimation with up to 82% improvement in ATE and rejecting false positives in self-similar environments.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simultaneous Localization and Mapping (SLAM) allows mobile robots to navigatewithout external positioning systems or pre-existing maps. Radar is emerging asa valuable sensing tool, especially in vision-obstructed environments, as it isless affected by particles than lidars or cameras. Modern 4D imaging radarsprovide three-dimensional geometric information and relative velocitymeasurements, but they bring challenges, such as a small field of view andsparse, noisy point clouds. Detecting loop closures in SLAM is critical forreducing trajectory drift and maintaining map accuracy. However, thedirectional nature of 4D radar data makes identifying loop closures, especiallyfrom reverse viewpoints, difficult due to limited scan overlap. This articleexplores using 4D radar for loop closure in SLAM, focusing on similar andopposing viewpoints. We generate submaps for a denser environmentrepresentation and use introspective measures to reject false detections infeature-degenerate environments. Our experiments show accurate loop closuredetection in geometrically diverse settings for both similar and opposingviewpoints, improving trajectory estimation with up to 82 % improvement in ATEand rejecting false positives in self-similar environments.</description>
      <author>example@mail.com (Maximilian Hilger, Vladimír Kubelka, Daniel Adolfsson, Ralf Becker, Henrik Andreasson, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2503.02383v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Fairness and/or Privacy on Social Graphs</title>
      <link>http://arxiv.org/abs/2503.02114v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了图神经网络（GNNs）在公平性和隐私方面的挑战，并通过实验分析了不同保护措施对模型性能的影响。&lt;h4&gt;背景&lt;/h4&gt;研究表明，GNNs在处理图形数据方面取得了显著成功，但也引发了关于其公平性与隐私问题的关注。这些问题包括可能的偏见和歧视风险以及敏感信息的安全隐患。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在全面调查GNN中的公平性和隐私问题，并探索不同保护措施对模型性能的影响。&lt;h4&gt;方法&lt;/h4&gt;通过在多种数据集上进行实验，评估不同的公平性干预措施的有效性。分析了公平性、隐私和准确性之间的权衡。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在特定的数据特性和期望的公平目标的基础上精心选择和组合公平性保护措施的重要性。&lt;h4&gt;结论&lt;/h4&gt;该研究对GNN中复杂互相关系的深入理解做出了贡献，并为开发更强大且道德规范的图学习模型铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的文本&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have shown remarkable success in variousgraph-based learning tasks. However, recent studies have raised concerns aboutfairness and privacy issues in GNNs, highlighting the potential for biased ordiscriminatory outcomes and the vulnerability of sensitive information. Thispaper presents a comprehensive investigation of fairness and privacy in GNNs,exploring the impact of various fairness-preserving measures on modelperformance. We conduct experiments across diverse datasets and evaluate theeffectiveness of different fairness interventions. Our analysis considers thetrade-offs between fairness, privacy, and accuracy, providing insights into thechallenges and opportunities in achieving both fair and private graph learning.The results highlight the importance of carefully selecting and combiningfairness-preserving measures based on the specific characteristics of the dataand the desired fairness objectives. This study contributes to a deeperunderstanding of the complex interplay between fairness, privacy, and accuracyin GNNs, paving the way for the development of more robust and ethical graphlearning models.</description>
      <author>example@mail.com (Bartlomiej Surma, Michael Backes, Yang Zhang)</author>
      <guid isPermaLink="false">2503.02114v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Depth-Adaptive Graph Neural Networks via Learnable Bakry-'Emery Curvature</title>
      <link>http://arxiv.org/abs/2503.01079v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新的图神经网络(GNN)框架，该框架通过结合Bakry-Emery曲率来增强信息传播能力，不仅考虑了结构特性还考虑了任务相关性。此方法能够动态调整消息传递层的深度以适应不同节点，并且在大规模图上计算效率高。&lt;h4&gt;背景&lt;/h4&gt;现有的GNN方法主要关注离散图拓扑而忽略了扩散动力学和特定任务依赖性的学习。引入几何属性如曲率可以改进信息流动和复杂连接模式的学习能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的可学习的Bakry-Emery曲率近似策略，以解决现有GNN方法忽视扩散动态和任务特定性的问题，并提高大规模图上的计算效率。&lt;h4&gt;方法&lt;/h4&gt;开发了一种基于顶点曲率自适应调整消息传递层深度的新机制。此外，通过理论分析建立了一个连接曲线度量与特征区分性的桥梁。&lt;h4&gt;主要发现&lt;/h4&gt;高曲率节点需要较少的传播层来达到有效学习，而低曲率节点从更深的传播中受益。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明该方法在各种图学习任务上均表现出优越性。理论和实证分析证明了所提出的框架的有效性和效率。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络(GNNs)已经展示了处理基于图形的任务的强大表征学习能力。最近关于GNN的进步通过利用几何属性，例如曲率，来增强其表示能力，从而建模复杂的连接模式和信息流动。然而，大多数现有的方法仅关注离散的图拓扑结构，忽略了扩散动力学以及对有效学习至关重要的任务特定依赖性。为了解决这个问题，我们提出了一种结合Bakry-Emery曲率的方法，该方法捕捉了信息传播中的结构化和驱动因素。我们开发了一种有效的、可学习的近似策略，使大规模图上的曲率计算变得可扩展。此外，我们还引入了一个自适应深度机制，根据每个顶点的曲度动态调整消息传递层的数量，确保高效的信息传递过程。我们的理论分析建立了曲率与特征区分性之间的联系，表明高曲率节点需要较少的层数进行传播，而低曲率节点可以从更深的传播中受益。在基准数据集上的广泛实验验证了我们方法的有效性，并显示出在各种图学习任务中的性能改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated strong representation learningcapabilities for graph-based tasks. Recent advances on GNNs leverage geometricproperties, such as curvature, to enhance its representation capabilities bymodeling complex connectivity patterns and information flow within graphs.However, most existing approaches focus solely on discrete graph topology,overlooking diffusion dynamics and task-specific dependencies essential foreffective learning. To address this, we propose integrating Bakry-\'Emerycurvature, which captures both structural and task-driven aspects ofinformation propagation. We develop an efficient, learnable approximationstrategy, making curvature computation scalable for large graphs. Furthermore,we introduce an adaptive depth mechanism that dynamically adjustsmessage-passing layers per vertex based on its curvature, ensuring efficientpropagation. Our theoretical analysis establishes a link between curvature andfeature distinctiveness, showing that high-curvature vertices require fewerlayers, while low-curvature ones benefit from deeper propagation. Extensiveexperiments on benchmark datasets validate the effectiveness of our approach,showing consistent performance improvements across diverse graph learningtasks.</description>
      <author>example@mail.com (Asela Hevapathige, Ahad N. Zehmakan, Qing Wang)</author>
      <guid isPermaLink="false">2503.01079v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>BGM2Pose: Active 3D Human Pose Estimation with Non-Stationary Sounds</title>
      <link>http://arxiv.org/abs/2503.00389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为BGM2Pose的方法，用于从任意音乐中提取3D人体姿态，该方法克服了现有技术中需要侵入性信号的限制。&lt;h4&gt;背景&lt;/h4&gt;现有的姿势估计方法往往依赖于特定频率范围内的声波信号，这些信号可能对人类造成不适。而日常生活中的普通音乐则提供了一种自然、无害的声音源。&lt;h4&gt;目的&lt;/h4&gt;开发一种从标准音乐中准确提取3D人体姿态的方法，以克服现有技术的局限性，并提高实际应用的可能性。&lt;h4&gt;方法&lt;/h4&gt;BGM2Pose引入了对比姿势提取模块和频率注意力模块。前者利用对比学习与硬负样本采样来去除记录数据中的音乐成分；后者则通过动态计算频带间的注意权重，使模型能够关注到由人体动作引起的微妙声学变化。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，BGM2Pose方法在性能上优于现有的技术，并展示了其潜在的实际应用价值。&lt;h4&gt;结论&lt;/h4&gt;研究证明了从背景音乐中提取3D姿态信息的可行性与优越性，并计划公开发布数据集和代码以供进一步的研究利用。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种名为BGM2Pose的方法，用于通过任意音乐作为主动传感信号来进行非侵入性的3D人体姿态估计。该方法克服了现有技术中需要侵入性声音信号的限制，这些声音信号可能对人类造成不适。从标准音乐中提取姿势信息带来了许多挑战：与专为测量设计的声音源不同，普通音乐在音量和音调上会动态变化，导致其声学特性难以与人体运动产生的改变区分开来。BGM2Pose通过引入对比姿势提取模块和频率注意力模块解决了这些难题，并展示了优于现有方法的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose BGM2Pose, a non-invasive 3D human pose estimation method usingarbitrary music (e.g., background music) as active sensing signals. Unlikeexisting approaches that significantly limit practicality by employingintrusive chirp signals within the audible range, our method utilizes naturalmusic that causes minimal discomfort to humans. Estimating human poses fromstandard music presents significant challenges. In contrast to sound sourcesspecifically designed for measurement, regular music varies in both volume andpitch. These dynamic changes in signals caused by music are inevitably mixedwith alterations in the sound field resulting from human motion, making it hardto extract reliable cues for pose estimation. To address these challenges,BGM2Pose introduces a Contrastive Pose Extraction Module that employscontrastive learning and hard negative sampling to eliminate musical componentsfrom the recorded data, isolating the pose information. Additionally, wepropose a Frequency-wise Attention Module that enables the model to focus onsubtle acoustic variations attributable to human movement by dynamicallycomputing attention across frequency bands. Experiments suggest that our methodoutperforms the existing methods, demonstrating substantial potential forreal-world applications. Our datasets and code will be made publicly available.</description>
      <author>example@mail.com (Yuto Shibata, Yusuke Oumi, Go Irie, Akisato Kimura, Yoshimitsu Aoki, Mariko Isogawa)</author>
      <guid isPermaLink="false">2503.00389v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Prior Distillation with Vision Foundation Model for Enhanced Rapid Bone Scintigraphy Image Restoration</title>
      <link>http://arxiv.org/abs/2503.02321v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 9 figures, 8 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于SAM模型的语义先验用于儿科快速骨闪烁显像图像恢复的新方法。&lt;h4&gt;背景&lt;/h4&gt;快速骨闪烁显像是诊断儿童骨骼疾病和肿瘤转移的重要工具，因为它减少了扫描时间并减轻了患者的不适。然而，快速扫描经常导致图像质量下降，影响诊断准确性。&lt;h4&gt;目的&lt;/h4&gt;为了提高快速骨闪烁显像的图像质量，我们提出了一个利用SAM模型语义先验来增强儿科人群中的快速骨闪烁显像的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法包括两个级联网络$f^{IR1}$和$f^{IR2}$，并有三个关键模块：语义先验集成(SPI)模块、语义知识蒸馏(SKD)模块以及语义一致性(SCM)模块。这些模块利用了从经过微调的SAM中提取的专业领域特定的语义信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在公共内窥镜数据集和新发布的RBS数据集中，我们的方法在各种评价指标（如PSNR、SSIM、FID和LPIPS）上都优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合SAM模型的语义先验信息，该方法能够显著提高快速骨闪烁显像图像的质量，有助于更准确地诊断儿童骨骼疾病。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了利用基于Segment Anything Model (SAM) 的方法来改进儿科患者快速骨闪烁显像质量的研究。通过引入专门设计的模块和使用特定数据集进行验证，该研究展示了在提升医学影像重建技术方面的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid bone scintigraphy is an essential tool for diagnosing skeletal diseasesand tumor metastasis in pediatric patients, as it reduces scan time andminimizes patient discomfort. However, rapid scans often result in poor imagequality, potentially affecting diagnosis due to reduced resolution and detail,which make it challenging to identify and evaluate finer anatomical structures.To address this issue, we propose the first application of SAM-based semanticpriors for medical image restoration, leveraging the Segment Anything Model(SAM) to enhance rapid bone scintigraphy images in pediatric populations. Ourmethod comprises two cascaded networks, $f^{IR1}$ and $f^{IR2}$, augmented bythree key modules: a Semantic Prior Integration (SPI) module, a SemanticKnowledge Distillation (SKD) module, and a Semantic Consistency Module (SCM).The SPI and SKD modules incorporate domain-specific semantic information from afine-tuned SAM, while the SCM maintains consistent semantic featurerepresentation throughout the cascaded networks. In addition, we will release anovel Rapid Bone Scintigraphy dataset called RBS, the first dataset dedicatedto rapid bone scintigraphy image restoration in pediatric patients. RBSconsists of 137 pediatric patients aged between 0.5 and 16 years who underwentboth standard and rapid bone scans. The dataset includes scans performed at 20cm/min (standard) and 40 cm/min (rapid), representing a $2\times$ acceleration.We conducted extensive experiments on both the publicly available endoscopicdataset and RBS. The results demonstrate that our method outperforms allexisting methods across various metrics, including PSNR, SSIM, FID, and LPIPS.</description>
      <author>example@mail.com (Pengchen Liang, Leijun Shi, Huiping Yao, Bin Pu, Jianguo Chen, Lei Zhao, Haishan Huang, Zhuangzhuang Chen, Zhaozhao Xu, Lite Xu, Qing Chang, Yiwei Li)</author>
      <guid isPermaLink="false">2503.02321v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>mmDEAR: mmWave Point Cloud Density Enhancement for Accurate Human Body Reconstruction</title>
      <link>http://arxiv.org/abs/2503.02375v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种针对毫米波雷达点云增强和人体重建的两阶段深度学习框架。&lt;h4&gt;背景&lt;/h4&gt;毫米波雷达因其隐私友好性和非侵入性而成为人体重建的理想选择，但其稀疏的点云限制了估计准确性。&lt;h4&gt;目的&lt;/h4&gt;通过引入深度学习方法来提高毫米波雷达点云的质量并提升人体重建精度。&lt;h4&gt;方法&lt;/h4&gt;{'mmWave点云增强模块': '利用时间特征密集化原始数据，并从单视图图像中的2D人类掩模中学习详细的形状和姿态信息。在训练阶段使用基于图像的监督，但在推理时仅依赖于稀疏的点云以保护隐私。', '多阶段完成网络': '进一步优化重建结果。', '2D-3D融合模块': '提取二维和三维运动特征来精炼SMPL参数'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法在多个数据集上优于现有技术，并且增强后的点云能进一步提升集成到现有模型中的性能。&lt;h4&gt;结论&lt;/h4&gt;通过利用深度学习框架有效解决了毫米波雷达点云稀疏的问题，为人体重建提供了一种新的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave (mmWave) radar offers robust sensing capabilities in diverseenvironments, making it a highly promising solution for human bodyreconstruction due to its privacy-friendly and non-intrusive nature. However,the significant sparsity of mmWave point clouds limits the estimation accuracy.To overcome this challenge, we propose a two-stage deep learning framework thatenhances mmWave point clouds and improves human body reconstruction accuracy.Our method includes a mmWave point cloud enhancement module that densifies theraw data by leveraging temporal features and a multi-stage completion network,followed by a 2D-3D fusion module that extracts both 2D and 3D motion featuresto refine SMPL parameters. The mmWave point cloud enhancement module learns thedetailed shape and posture information from 2D human masks in single-viewimages. However, image-based supervision is involved only during the trainingphase, and the inference relies solely on sparse point clouds to maintainprivacy. Experiments on multiple datasets demonstrate that our approachoutperforms state-of-the-art methods, with the enhanced point clouds furtherimproving performance when integrated into existing models.</description>
      <author>example@mail.com (Jiarui Yang, Songpengcheng Xia, Zengyuan Lai, Lan Sun, Qi Wu, Wenxian Yu, Ling Pei)</author>
      <guid isPermaLink="false">2503.02375v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Quantifying Point Contributions: A Lightweight Framework for Efficient and Effective Query-Driven Trajectory Simplification</title>
      <link>http://arxiv.org/abs/2503.02047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by VLDB2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着轨迹数据的积累，简化轨迹以减少存储和查询成本的研究越来越受到关注。论文提出了一种基于图神经网络（GNN-TS）和扩散模型（Diff-TS）的新型互学习查询驱动轨迹简化框架MLSim。&lt;h4&gt;背景&lt;/h4&gt;现有的简化方法主要面临三方面的问题：需要大量迭代才能决定删除哪些GPS点；只关注相邻点之间的关系，忽视了整体结构的信息，导致简化后的轨迹与原始轨迹的整体相似性降低；不能区分具有相似特征的点的重要性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法来解决现有简化方法中存在的问题，并提高查询精度和减少简化时间。&lt;h4&gt;方法&lt;/h4&gt;MLSim框架包含了两个不同的模型：基于图神经网络（GNN-TS）和基于扩散模型（Diff-TS）。其中，GNN-TS通过关注点的全局性和独特性评估其重要性；而Diff-TS则生成放大的信号以在低压缩率下保留最重要的点。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与八个基准方法相比，在三个数据库上使用MLSim可以将简化时间减少42%--70%，并提高查询准确性最多可达34.6%&lt;h4&gt;结论&lt;/h4&gt;所提出的MLSim框架通过结合图神经网络和扩散模型，能够有效地解决现有轨迹简化方法的问题，并提供更精确的查询结果。&lt;h4&gt;翻译&lt;/h4&gt;随着大量轨迹数据积累起来，为了减少存储成本以及查询成本，对于简化轨迹的研究变得越来越重要。现有的提议面临三个主要问题：首先，它们需要多次迭代来决定删除哪些GPS点；其次，它们只关注相邻点之间的关系（局部信息），而忽视了整体结构的信息（全局信息），这降低了简化的轨迹与原始轨迹的整体相似性，并使在查询结果中保持一致性变得困难，尤其是在基于相似性的查询中。最后，它们未能区分具有类似特征的点的重要性，从而导致选择保留原始轨迹信息的点时效果不佳。我们提出了一种新颖的互学习查询驱动轨迹简化框架MLSim，该框架结合了两个不同的模型：基于图神经网络（GNN-TS）和基于扩散模型（Diff-TS）。GNN-TS根据全局性和独特性评估一个点的重要性，前者捕捉其与整个轨迹的相关性，后者则捕捉其与其他相邻点之间的差异。同时，在GNN层中还包含了注意力机制，使得能够从同一轨迹内的所有点进行数据融合，并优化表示，从而避免了迭代过程。Diff-TS生成放大信号以在低压缩率下保留最重要的点。涉及八种基线方法的实验显示，MLSim可以将简化时间减少42%--70%，并在简化后的轨迹查询准确性上提高最多34.6%&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As large volumes of trajectory data accumulate, simplifying trajectories toreduce storage and querying costs is increasingly studied. Existing proposalsface three main problems. First, they require numerous iterations to decidewhich GPS points to delete. Second, they focus only on the relationshipsbetween neighboring points (local information) while neglecting the overallstructure (global information), reducing the global similarity between thesimplified and original trajectories and making it difficult to maintainconsistency in query results, especially for similarity-based queries. Finally,they fail to differentiate the importance of points with similar features,leading to suboptimal selection of points to retain the original trajectoryinformation.  We propose MLSimp, a novel Mutual Learning query-driven trajectorysimplification framework that integrates two distinct models: GNN-TS, based ongraph neural networks, and Diff-TS, based on diffusion models. GNN-TS evaluatesthe importance of a point according to its globality, capturing its correlationwith the entire trajectory, and its uniqueness, capturing its differences fromneighboring points. It also incorporates attention mechanisms in the GNNlayers, enabling simultaneous data integration from all points within the sametrajectory and refining representations, thus avoiding iterative processes.Diff-TS generates amplified signals to enable the retention of the mostimportant points at low compression rates. Experiments involving eightbaselines on three databases show that MLSimp reduces the simplification timeby 42%--70% and improves query accuracy over simplified trajectories by up to34.6%.</description>
      <author>example@mail.com (Yumeng Song, Yu Gu, Tianyi Li, Yushuai Li, Christian S. Jensen, Ge Yu)</author>
      <guid isPermaLink="false">2503.02047v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Fine-Grained Hand-Object Dynamics for Egocentric Video Representation Learning</title>
      <link>http://arxiv.org/abs/2503.00986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as ICLR 2025 conference paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的管道HOD和模型EgoVideo，以改进基于自我视角视频的理解。通过引入一种新颖的方法来生成包含手部与物体动态细节的叙述文本，并使用一个轻量级的运动适配器来捕捉这些细粒度的手部-物体动态信息。&lt;h4&gt;背景&lt;/h4&gt;现有研究主要集中在将视频表示与高层次叙事对齐，而忽略了对手部和物体之间复杂动态的研究。&lt;h4&gt;目的&lt;/h4&gt;旨在整合细粒度手部-物体动力学模型到视频表示学习过程中。&lt;h4&gt;方法&lt;/h4&gt;引入HOD管道利用手部-物体检测器和大型语言模型生成详细的叙述文本；提出EgoVideo模型，其中包含一个新的轻量级运动适配器来捕捉细粒度的手部-物体移动信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过协同训练策略，EgoVideo能够有效地利用HOD数据中的细粒度手部-物体动态。实验显示该方法在多个自我视角下游任务中取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;提出的模型不仅在零样本设置下取得了显著的改进，还展示了对手部-物体交互和机器人操作任务的强大泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在自我视角视频理解领域，手部与物体的动作以及它们之间的互动起着关键作用。然而现有的研究主要集中在将视频表示与高层次叙事对齐，忽略了对手部和物体之间复杂动态的研究。为了解决这个问题，引入了HOD管道，并提出了EgoVideo模型，该模型能够更好地捕捉这些细粒度的手部-物体动态信息。实验结果显示这种方法在多个自我视角下游任务中取得了显著的改进，展示了强大的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/openrobotlab/egohod&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In egocentric video understanding, the motion of hands and objects as well astheir interactions play a significant role by nature. However, existingegocentric video representation learning methods mainly focus on aligning videorepresentation with high-level narrations, overlooking the intricate dynamicsbetween hands and objects. In this work, we aim to integrate the modeling offine-grained hand-object dynamics into the video representation learningprocess. Since no suitable data is available, we introduce HOD, a novelpipeline employing a hand-object detector and a large language model togenerate high-quality narrations with detailed descriptions of hand-objectdynamics. To learn these fine-grained dynamics, we propose EgoVideo, a modelwith a new lightweight motion adapter to capture fine-grained hand-objectmotion information. Through our co-training strategy, EgoVideo effectively andefficiently leverages the fine-grained hand-object dynamics in the HOD data.Extensive experiments demonstrate that our method achieves state-of-the-artperformance across multiple egocentric downstream tasks, including improvementsof 6.3% in EK-100 multi-instance retrieval, 5.7% in EK-100 classification, and16.3% in EGTEA classification in zero-shot settings. Furthermore, our modelexhibits robust generalization capabilities in hand-object interaction androbot manipulation tasks. Code and data are available athttps://github.com/OpenRobotLab/EgoHOD/.</description>
      <author>example@mail.com (Baoqi Pei, Yifei Huang, Jilan Xu, Guo Chen, Yuping He, Lijin Yang, Yali Wang, Weidi Xie, Yu Qiao, Fei Wu, Limin Wang)</author>
      <guid isPermaLink="false">2503.00986v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Convergence of energy-based learning in linear resistive networks</title>
      <link>http://arxiv.org/abs/2503.00349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了一种基于能量的学习算法——对比学习（Contrastive Learning），并证明在特定网络中，该算法与投影梯度下降等价，从而保证了算法的收敛性。&lt;h4&gt;背景&lt;/h4&gt;基于能量的学习算法作为反向传播的替代方案，在类比电子设备中的分布式实现中表现良好。然而，缺乏严格的理论来证明其收敛性。&lt;h4&gt;目的&lt;/h4&gt;首次探讨对比学习在可调电阻网络上的应用，并提供严格的数学分析以保证该算法的收敛性。&lt;h4&gt;方法&lt;/h4&gt;通过对基于能量的学习算法——对比学习（应用于线性可调整电阻网络）进行详细分析，发现此设置下对比学习等价于任何步长下的凸函数投影梯度下降。&lt;h4&gt;主要发现&lt;/h4&gt;证明了在特定条件下，对比学习算法与投影梯度下降是等价的，并且对于所有步长都保证了收敛性。&lt;h4&gt;结论&lt;/h4&gt;研究为基于能量的学习方法提供了一个坚实的理论基础，特别是在类比电子实现方面，展示了其潜在的应用价值和可靠性的提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Energy-based learning algorithms are alternatives to backpropagation and arewell-suited to distributed implementations in analog electronic devices.However, a rigorous theory of convergence is lacking. We make a first step inthis direction by analysing a particular energy-based learning algorithm,Contrastive Learning, applied to a network of linear adjustable resistors. Itis shown that, in this setup, Contrastive Learning is equivalent to projectedgradient descent on a convex function, for any step size, giving a guarantee ofconvergence for the algorithm.</description>
      <author>example@mail.com (Anne-Men Huijzer, Thomas Chaffey, Bart Besselink, Henk J. van Waarde)</author>
      <guid isPermaLink="false">2503.00349v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Token-level Text Image Foundation Model for Document Understanding</title>
      <link>http://arxiv.org/abs/2503.02304v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  23 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，通用视觉基础模型（VFMs）在多模态大型语言模型中的应用日益增加。然而，在处理包含小而密集文本的图像时，这些模型仍然存在基本预测错误的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的通用视觉基础模型虽然广泛应用于多模态大型语言模型中作为图像编码器，但在缺乏语义精细级监督的情况下，面对含有小而密文字的图像任务（如感知、理解和推理）时，仍会出现关键性的预测误差问题。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究团队开发了TokenOCR，这是首个针对文本-图像相关任务设计的基于标记级别的视觉基础模型，能够支持多种传统下游应用。&lt;h4&gt;方法&lt;/h4&gt;为促进TokenOCR的预训练，研究者们构建了一个高质量的数据生成管道，创建了TokenIT数据集，包含2000万张图像和18亿对token-mask。此外，通过利用具有卓越图像作为文本能力的基础模型，他们使用TokenOCR替代先前的VFMs来构造一个文档级多模态大型语言模型(TokenVL)，用于基于VQA（视觉问答）的任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TokenOCR和TokenVL在多项任务中表现出色。这些成果为未来的研究提供了数据集、代码和权重资源。&lt;h4&gt;结论&lt;/h4&gt;通过开发TokenOCR及TokenVL，研究团队成功地解决了现有视觉基础模型面对含有小而密文字的图像时存在的预测误差问题，并且证明了基于标记级别的视觉基础模型在处理文本-图像相关任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;近年来，通用视觉基础模型（VFMs）在多模态大型语言模型中的应用日益增加。然而，在缺乏语义精细级监督的情况下，这些模型在面对含有小而密文字的图像任务时仍会遇到关键性预测误差的问题。为解决此问题，研究团队开发了TokenOCR，一个专为文本-图像相关任务设计的标记级别视觉基础模型，并构建了一个高质量的数据生成管道以创建TokenIT数据集（包含2000万张图像和18亿对token-mask）。此外，他们还通过利用具有卓越图像作为文本能力的基础模型构造了文档级多模态大型语言模型(TokenVL)。实验结果显示，TokenOCR与TokenVL在多项任务中表现出色，并且研究团队将提供数据集、代码和权重资源以供未来的研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, general visual foundation models (VFMs) have witnessedincreasing adoption, particularly as image encoders for popular multi-modallarge language models (MLLMs). However, without semantically fine-grainedsupervision, these models still encounter fundamental prediction errors in thecontext of downstream text-image-related tasks, i.e., perception, understandingand reasoning with images containing small and dense texts. To bridge this gap,we develop TokenOCR, the first token-level visual foundation model specificallytailored for text-image-related tasks, designed to support a variety oftraditional downstream applications. To facilitate the pretraining of TokenOCR,we also devise a high-quality data production pipeline that constructs thefirst token-level image text dataset, TokenIT, comprising 20 million images and1.8 billion token-mask pairs. Furthermore, leveraging this foundation withexceptional image-as-text capability, we seamlessly replace previous VFMs withTokenOCR to construct a document-level MLLM, TokenVL, for VQA-based documentunderstanding tasks. Finally, extensive experiments demonstrate theeffectiveness of TokenOCR and TokenVL. Code, datasets, and weights will beavailable at https://token-family.github.io/TokenOCR_project.</description>
      <author>example@mail.com (Tongkun Guan, Zining Wang, Pei Fu, Zhengtao Guo, Wei Shen, Kai Zhou, Tiezhu Yue, Chen Duan, Hao Sun, Qianyi Jiang, Junfeng Luo, Xiaokang Yang)</author>
      <guid isPermaLink="false">2503.02304v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Label-Efficient LiDAR Panoptic Segmentation</title>
      <link>http://arxiv.org/abs/2503.02372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对LiDAR全景分割的低标签需求的方法L3PS，该方法通过利用2D网络生成伪标签并结合新颖的3D细化模块来提高点云数据中的语义和实例分割性能。&lt;h4&gt;背景&lt;/h4&gt;学习型机器人场景理解技术在处理复杂高维点云数据时面临训练数据标注量大的问题，这限制了其泛化能力。特别是LiDAR全景分割，需要同时进行语义和实例分割，对训练样本的需求更大。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决使用少量标记样本的LiDAR全景分割任务，并减轻大规模数据标注的工作负担。&lt;h4&gt;方法&lt;/h4&gt;1. 利用标签高效2D网络从少量注释图像中生成全景伪标签。           2. 将这些伪标签投影到点云上进行3D细化，通过聚类技术、顺序扫描累积和地面点分离来增强标签准确性。&lt;h4&gt;主要发现&lt;/h4&gt;引入的3D细化模块能够显著提高伪标签的质量，在PQ指标上的改进可达+10.6，在mIoU指标上有+7.9的提升。这些经过优化后的伪标签可以用来有效训练现成的LiDAR分割网络。&lt;h4&gt;结论&lt;/h4&gt;L3PS方法不仅在性能上超越了现有的方法，还大大降低了标注样本的需求量，从而减轻了人力和时间负担。&lt;h4&gt;翻译&lt;/h4&gt;学习型机器人场景理解技术面临的主要瓶颈是对大量注释训练数据的高度依赖，这通常限制了其泛化能力。在LiDAR全景分割中，由于需要同时处理复杂高维点云数据中的语义和实例分割任务，这一挑战更加突出。为了解决使用少量标记样本的LiDAR全景分割难题，本文借鉴最新的标签高效视觉全景分割技术，提出了一种名为Limited-Label LiDAR Panoptic Segmentation (L3PS)的新方法，仅需极少量标注数据即可工作。该方法首先利用一个高效的2D网络从少数注释图像中生成伪标签，并将其投影到点云上。然后引入了一个新颖的3D细化模块，充分利用了点云的几何特性，通过聚类技术、顺序扫描累积和地面点分离来显著提升伪标签的精度，从而提高了分割质量（PQ指标增加+10.6，mIoU指标增加+7.9）。实验表明，这些优化后的伪标签可以有效地训练现成的LiDAR分割网络。通过广泛的试验验证了L3PS不仅在性能上超过了现有方法，还大大减少了标注负担。该工作的代码已发布于https://l3ps.cs.uni-freiburg.de。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A main bottleneck of learning-based robotic scene understanding methods isthe heavy reliance on extensive annotated training data, which often limitstheir generalization ability. In LiDAR panoptic segmentation, this challengebecomes even more pronounced due to the need to simultaneously address bothsemantic and instance segmentation from complex, high-dimensional point clouddata. In this work, we address the challenge of LiDAR panoptic segmentationwith very few labeled samples by leveraging recent advances in label-efficientvision panoptic segmentation. To this end, we propose a novel method,Limited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimalamount of labeled data. Our approach first utilizes a label-efficient 2Dnetwork to generate panoptic pseudo-labels from a small set of annotatedimages, which are subsequently projected onto point clouds. We then introduce anovel 3D refinement module that capitalizes on the geometric properties ofpoint clouds. By incorporating clustering techniques, sequential scanaccumulation, and ground point separation, this module significantly enhancesthe accuracy of the pseudo-labels, improving segmentation quality by up to+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can beused to effectively train off-the-shelf LiDAR segmentation networks. Throughextensive experiments, we show that L3PS not only outperforms existing methodsbut also substantially reduces the annotation burden. We release the code ofour work at https://l3ps.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Ahmet Selim Çanakçı, Niclas Vödisch, Kürsat Petek, Wolfram Burgard, Abhinav Valada)</author>
      <guid isPermaLink="false">2503.02372v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Learning Exposure Mapping Functions for Inferring Heterogeneous Peer Effects</title>
      <link>http://arxiv.org/abs/2503.01722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一个新的基于图神经网络的方法EgoNetGNN，用于自动学习适合复杂同行影响机制的暴露映射函数。&lt;h4&gt;背景&lt;/h4&gt;在因果推断中，干扰是指个体在网络中的行为受到同伴行动的影响。通常研究人员定义一个聚集同伴治疗并输出同行暴露的映射函数来估计同行效应。现有的方法主要基于同伴数量或比例来决定暴露映射函数。&lt;h4&gt;目的&lt;/h4&gt;为了更准确地估计异质性同行效应（不同情境下相同同行暴露下的反事实结果的变化），该研究旨在开发一种能够自动学习合适暴露映射函数的方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种图神经网络(EgoNetGNN) 方法，它可以考虑到局部邻居结构和边属性等因素来处理复杂的同行影响机制。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，基于数量或比例的同伴暴露模型或者简单地学习同伴暴露的模型难以应对复杂的影响机制。而该方法在合成数据及半合成网络数据上估计异质性同行效应时比现有最佳基准更稳健。&lt;h4&gt;结论&lt;/h4&gt;EgoNetGNN能够更好地处理复杂的同行影响，提供了一个自动学习适当暴露映射函数的新视角，有助于更加准确地评估同行效应。&lt;h4&gt;翻译&lt;/h4&gt;在因果推理中，干扰指的是网络中的个体行为受到同伴行动的影响。同行效应指的是一个人在不同同伴暴露水平下的反事实结果差异，即一个人受同伴治疗、行动或行为影响的程度。估计同行效应需要决定如何表示同伴暴露。通常研究人员定义一个聚集同伴治疗并输出同行暴露的映射函数来决定这一点。现有的大多数方法都假设了基于同伴数量或者比例的同行暴露。最近的研究探讨了更复杂的同行暴露功能，以捕捉不同同伴施加的不同程度的影响。然而，这些研究没有明确考虑自动学习暴露映射函数的问题。在这项工作中，我们专注于为估计异质性同行效应开发这样的函数，其中异质性指的是在相同的同伴暴露下但不同的个人背景下的反事实结果的变化。我们提出了EgoNetGNN, 一种基于图神经网络的方法，用于自动学习适当的暴露映射函数以处理复杂的同行影响机制，这些机制不仅涉及到同伴治疗，还包括局部邻居结构和边属性。我们的综合评估表明，在估计异质性同行效应时，与最先进的基准相比，该方法在面对不同的未知潜在影响机制时表现得更稳健。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In causal inference, interference refers to the phenomenon in which theactions of peers in a network can influence an individual's outcome. Peereffect refers to the difference in counterfactual outcomes of an individual fordifferent levels of peer exposure, the extent to which an individual is exposedto the treatments, actions, or behaviors of peers. Estimating peer effectsrequires deciding how to represent peer exposure. Typically, researchers definean exposure mapping function that aggregates peer treatments and outputs peerexposure. Most existing approaches for defining exposure mapping functionsassume peer exposure based on the number or fraction of treated peers. Recentstudies have investigated more complex functions of peer exposure which capturethat different peers can exert different degrees of influence. However, none ofthese works have explicitly considered the problem of automatically learningthe exposure mapping function. In this work, we focus on learning this functionfor the purpose of estimating heterogeneous peer effects, where heterogeneityrefers to the variation in counterfactual outcomes for the same peer exposurebut different individual's contexts. We develop EgoNetGNN, a graph neuralnetwork (GNN)-based method, to automatically learn the appropriate exposuremapping function allowing for complex peer influence mechanisms that, inaddition to peer treatments, can involve the local neighborhood structure andedge attributes. We show that GNN models that use peer exposure based on thenumber or fraction of treated peers or learn peer exposure naively facedifficulty accounting for such influence mechanisms. Our comprehensiveevaluation on synthetic and semi-synthetic network data shows that our methodis more robust to different unknown underlying influence mechanisms whenestimating heterogeneous peer effects when compared to state-of-the-artbaselines.</description>
      <author>example@mail.com (Shishir Adhikari, Sourav Medya, Elena Zheleva)</author>
      <guid isPermaLink="false">2503.01722v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers</title>
      <link>http://arxiv.org/abs/2503.00958v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的作者归属任务的方法，利用了预训练的基于Transformer模型的不同层次中学习到的各种语言表示。&lt;h4&gt;背景&lt;/h4&gt;现有的作者归属方法可能在跨域数据集上的表现不佳，这限制了它们的实际应用范围和鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;通过评估不同层次的Transformer语言模型在内部和外部领域的效果，来提高作者归属任务的准确性和稳健性。&lt;h4&gt;方法&lt;/h4&gt;利用预训练的基于Transformer的模型的不同层中学习到的语言表示进行作者归属，并与最先进的基准在相同领域和跨域场景下进行了对比实验。&lt;h4&gt;主要发现&lt;/h4&gt;使用不同层次的变压器提高了作者归属模型在外域数据上的鲁棒性，从而达到了新的最先进水平。分析表明，每一层都专门化于表示某些有助于外部域测试的风格特征。&lt;h4&gt;结论&lt;/h4&gt;研究结果为更好地理解Transformer模型在不同层级中对文本风格属性的表达提供了一种途径，并为进一步改进作者归属任务奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的方法用于作者归属任务，这种方法利用了预训练的基于Transformer的语言模型的不同层次中学到的各种语言表示。我们在三个数据集上评估了该方法，并将其与最先进的基准进行了比较，在内部领域和跨域场景中进行测试。结果表明，使用不同的Transformer层可以提高在外部领域数据上的鲁棒性，从而产生了新的最先进成果。我们的分析进一步揭示了模型的不同层次如何专门化表示某些风格特征以增强其在外域中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new approach for the authorship attribution task that leveragesthe various linguistic representations learned at different layers ofpre-trained transformer-based models. We evaluate our approach on threedatasets, comparing it to a state-of-the-art baseline in in-domain andout-of-domain scenarios. We found that utilizing various transformer layersimproves the robustness of authorship attribution models when tested onout-of-domain data, resulting in new state-of-the-art results. Our analysisgives further insights into how our model's different layers get specialized inrepresenting certain stylistic features that benefit the model when tested outof the domain.</description>
      <author>example@mail.com (Milad Alshomary, Nikhil Reddy Varimalla, Vishal Anand, Kathleen McKeown)</author>
      <guid isPermaLink="false">2503.00958v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range Images</title>
      <link>http://arxiv.org/abs/2503.02300v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, submitted to 2025 IROS. This work has been  submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;毫米波雷达在机器人和自动驾驶领域引起了广泛关注，但由于生成的点云较为稀疏且含有大量噪声，其进一步发展受到限制。&lt;h4&gt;背景信息&lt;/h4&gt;尽管毫米波雷达在恶劣环境中的感知稳定性很高，但生成的点云相对稀疏并包含显著的噪声。传统的毫米波雷达增强方法难以充分利用扩散模型在超分辨率方面的效果。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种新的方法，将范围图像与图像扩散模型融合，以实现准确且密集的毫米波雷达点云，使其类似于激光雷达（LiDAR）生成的点云。&lt;h4&gt;所用方法&lt;/h4&gt;通过利用范围图像与人类观察一致的投影方式，以及预训练的图像扩散模型的知识迁移，该方法实现了自然图像表示的改进，提高了整体性能。&lt;h4&gt;主要发现&lt;/h4&gt;在公共数据集和自建数据集上的大量评估显示，这种方法提供了显著的提升，生成了高质量的三维激光雷达（LiDAR）点云。&lt;h4&gt;结论&lt;/h4&gt;这项研究成功地利用毫米波雷达生成类似激光雷达的三维点云，并确立了一个新的性能标准。&lt;h4&gt;翻译&lt;/h4&gt;毫米波雷达在机器人和自动驾驶领域引起了广泛关注。虽然它在恶劣环境中的感知稳定性较高，但由其产生的点云较为稀疏且含有大量噪声，这限制了它的进一步发展。传统的方法难以利用扩散模型进行超分辨率处理，很大程度上是因为不自然的范围-方位热图（RAH）或鸟瞰视图（BEV）表示方式。为了克服这一局限性，我们提出了一种将范围图像与图像扩散模型融合的新方法，实现了准确且密集的毫米波雷达点云，类似于激光雷达生成的结果。通过与人类观察一致的投影对齐，该方法利用了预训练的图像扩散模型的知识迁移，显著提升了整体性能。在公共数据集和自建数据集上的广泛评估表明，我们的方法提供了显著的改进，并确立了一个新的性能标准，在使用毫米波雷达生成类似激光雷达的三维点云方面建立了新基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millimeter-wave (mmWave) radar has attracted significant attention inrobotics and autonomous driving. However, despite the perception stability inharsh environments, the point cloud generated by mmWave radar is relativelysparse while containing significant noise, which limits its furtherdevelopment. Traditional mmWave radar enhancement approaches often struggle toleverage the effectiveness of diffusion models in super-resolution, largely dueto the unnatural range-azimuth heatmap (RAH) or bird's eye view (BEV)representation. To overcome this limitation, we propose a novel method thatpioneers the application of fusing range images with image diffusion models,achieving accurate and dense mmWave radar point clouds that are similar toLiDAR. Benefitting from the projection that aligns with human observation, therange image representation of mmWave radar is close to natural images, allowingthe knowledge from pre-trained image diffusion models to be effectivelytransferred, significantly improving the overall performance. Extensiveevaluations on both public datasets and self-constructed datasets demonstratethat our approach provides substantial improvements, establishing a newstate-of-the-art performance in generating truly three-dimensional LiDAR-likepoint clouds via mmWave radar.</description>
      <author>example@mail.com (Ruixin Wu, Zihan Li, Jin Wang, Xiangyu Xu, Huan Yu, Zhi Zheng, Kaixiang Huang, Guodong Lu)</author>
      <guid isPermaLink="false">2503.02300v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>GRNFormer: A Biologically-Guided Framework for Integrating Gene Regulatory Networks into RNA Foundation Models</title>
      <link>http://arxiv.org/abs/2503.01682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GRNFormer是一种新的框架，用于将多尺度基因调控网络（GRNs）从多组学数据中系统地整合到RNA基础模型训练中。&lt;h4&gt;背景&lt;/h4&gt;现有的单细胞RNA测序模型虽然显示了捕捉基因表达模式的能力，但它们忽略了生物先验知识，并未能利用多组学信号提供补充的监管见解。&lt;h4&gt;目的&lt;/h4&gt;提出GRNFormer框架，通过引入分层GRNs构建管道和结构感知整合框架来解决这些问题。&lt;h4&gt;方法&lt;/h4&gt;GRNFormer框架包括两个创新点：一是构造细胞类型特异性和细胞特定分辨率下捕获调控关系的分层GRNs；二是设计了一种新颖的边缘扰动策略，使用多头交叉注意力机制动态加权监管关系，并通过引入生物信息共表达链接来增强图神经网络训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在药物反应预测、单细胞药物分类和基因干扰预测任务中，GRNFormer分别提高了3.6%的相关性、9.6%的AUC和1.1%的准确率。&lt;h4&gt;结论&lt;/h4&gt;GRNFormer框架在多种模型架构上均表现出优于现有方法的效果。&lt;h4&gt;翻译&lt;/h4&gt;基础单细胞RNA测序（scRNA-seq）模型已显示出捕捉基因表达模式的能力，但当前的方法忽略了编码在基因调控关系中的生物先验知识，并未能利用多组学信号提供补充的监管见解。本文提出了一种新框架GRNFormer，该框架将从多组学数据中推断出的多尺度基因调控网络（GRNs）系统地整合到RNA基础模型训练中。该框架引入了两个关键创新：构建分层GRNs的管道和一种新的结构感知集成框架。GRNFormer通过综合实验在多个代表性下游任务上展示了其有效性，并优于现有的最佳基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for single-cell RNA sequencing (scRNA-seq) have shownpromising capabilities in capturing gene expression patterns. However, currentapproaches face critical limitations: they ignore biological prior knowledgeencoded in gene regulatory relationships and fail to leverage multi-omicssignals that could provide complementary regulatory insights. In this paper, wepropose GRNFormer, a new framework that systematically integrates multi-scaleGene Regulatory Networks (GRNs) inferred from multi-omics data into RNAfoundation model training. Our framework introduces two key innovations. First,we introduce a pipeline for constructing hierarchical GRNs that captureregulatory relationships at both cell-type-specific and cell-specificresolutions. Second, we design a structure-aware integration framework thataddresses the information asymmetry in GRNs through two technical advances: (1)A graph topological adapter using multi-head cross-attention to weightregulatory relationships dynamically, and (2) a novel edge perturbationstrategy that perturb GRNs with biologically-informed co-expression links toaugment graph neural network training. Comprehensive experiments have beenconducted on three representative downstream tasks across multiple modelarchitectures to demonstrate the effectiveness of GRNFormer. It achievesconsistent improvements over state-of-the-art (SoTA) baselines: $3.6\%$increase in drug response prediction correlation, $9.6\%$ improvement insingle-cell drug classification AUC, and $1.1\%$ average gain in geneperturbation prediction accuracy.</description>
      <author>example@mail.com (Mufan Qiu, Xinyu Hu, Fengwei Zhan, Sukwon Yun, Jie Peng, Ruichen Zhang, Bhavya Kailkhura, Jiekun Yang, Tianlong Chen)</author>
      <guid isPermaLink="false">2503.01682v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Improve Representation for Imbalanced Regression through Geometric Constraints</title>
      <link>http://arxiv.org/abs/2503.00876v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. The first three authors contributed equally&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了表示学习中的均匀性概念，特别是对于不平衡回归任务的意义，并提出了一种基于几何原理的损失函数方法。&lt;h4&gt;背景&lt;/h4&gt;在表示学习中，均匀性指的是潜在空间（即单位超球面）中特征分布的一致性。以往的研究表明，提高这种一致性有助于未充分代表类别的学习。&lt;h4&gt;目的&lt;/h4&gt;探讨如何确保不平衡回归任务中的表示空间保持一致性和平滑性的方法，并提出两种关键损失函数来实现这一目标。&lt;h4&gt;方法&lt;/h4&gt;提出了两个几何损失：包容损失和同质性损失。包容损失鼓励特征在超球面上均匀分布，而同质性损失保证了平滑性，并且以相同间隔使表示均匀分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过Surrogate-driven Representation Learning (SRL)框架将这些几何原理整合到数据表示中，实验结果表明对于不平衡回归任务而言，一致性和平滑性的保持非常重要。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法在真实世界中的回归和操作学习任务上验证了其有效性，并强调了几何损失函数在提高不平衡数据集性能方面的关键作用。&lt;h4&gt;翻译&lt;/h4&gt;在表示学习中，一致性指的是潜在空间（即单位超球面）的特征分布均匀性。先前的工作表明，改善这种一致性有助于欠代表类别的学习。然而，大多数以前的研究专注于分类任务；对于不平衡回归而言，表示空间尚未得到探索。基于分类的方法不适合回归任务，因为它们将特性聚集成不连续组群而没有考虑回归所需的重要因素：连续性和顺序排列性。从几何角度来看，本研究独特地关注确保超球面上的特征分布均匀，并通过两种关键损失函数（包容损失和同质性损失）实现这一目标。我们的方法利用代理驱动表示学习框架将这些几何原理整合到数据表示中。在真实世界回归任务和操作学习上的实验验证了这种一致性对于不平衡回归的重要性，以及我们基于几何学的损失函数的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In representation learning, uniformity refers to the uniform featuredistribution in the latent space (i.e., unit hypersphere). Previous work hasshown that improving uniformity contributes to the learning ofunder-represented classes. However, most of the previous work focused onclassification; the representation space of imbalanced regression remainsunexplored. Classification-based methods are not suitable for regression tasksbecause they cluster features into distinct groups without considering thecontinuous and ordered nature essential for regression. In a geometric aspect,we uniquely focus on ensuring uniformity in the latent space for imbalancedregression through two key losses: enveloping and homogeneity. The envelopingloss encourages the induced trace to uniformly occupy the surface of ahypersphere, while the homogeneity loss ensures smoothness, withrepresentations evenly spaced at consistent intervals. Our method integratesthese geometric principles into the data representations via a Surrogate-drivenRepresentation Learning (SRL) framework. Experiments with real-world regressionand operator learning tasks highlight the importance of uniformity inimbalanced regression and validate the efficacy of our geometry-based lossfunctions.</description>
      <author>example@mail.com (Zijian Dong, Yilei Wu, Chongyao Chen, Yingtian Zou, Yichi Zhang, Juan Helen Zhou)</author>
      <guid isPermaLink="false">2503.00876v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Towards Fluorescence-Guided Autonomous Robotic Partial Nephrectomy on Novel Tissue-Mimicking Hydrogel Phantoms</title>
      <link>http://arxiv.org/abs/2503.02265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages. 7 figures. Preprint of an article accepted for publication  in the Journal of Medical Robotics Research, 2025. Copyright World Scientific  Publishing Company [https://worldscientific.com/worldscinet/jmrr]&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种荧光引导的自主机器人系统，能够为外生性肾肿瘤规划和执行切除路径，并通过近红外成像区分健康组织与肿瘤组织。&lt;h4&gt;背景&lt;/h4&gt;自动化的机器人系统有可能提高肾脏肿瘤切除手术的精度及患者的治疗效果。现有的动物模型难以在实验中模拟人类组织的具体行为。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够用于荧光引导下自主执行肾肿瘤切除任务的机器人系统，并设计出可以模仿人体组织特性的新型水凝胶基体模。&lt;h4&gt;方法&lt;/h4&gt;使用点云观察处理不规则形状的肿瘤；利用近红外成像区分健康与肿瘤组织，采用基于水凝胶的仿生肾脏体模来模拟真实的人类组织特性。&lt;h4&gt;主要发现&lt;/h4&gt;该系统能够在69秒内完成切除任务，并达到了1.44毫米的平均切缘精度。此外，通过将材料与近红外染料混合的方式实现了荧光引导下的肿瘤分割功能。&lt;h4&gt;结论&lt;/h4&gt;新型水凝胶体模和荧光引导机器人系统为肾癌及其他类似手术中的自主机器人技术的发展提供了重要的研究平台和实验工具。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人系统在提高肾脏肿瘤切除的精度及患者治疗效果方面展现出巨大潜力。文中介绍了一种具有规划执行路径能力和临床相关切缘的安全性指标的荧光引导自主机器人系统。该系统利用点云观测处理不规则形状的肿瘤，并通过近红外成像技术区分健康组织与肿瘤组织，模拟靛青绿染色在部分肾切除手术中的作用。鉴于无法获得用于特定类型干预的人体或动物离体组织，仿生肾脏体模对于自主机器人外科系统的开发至关重要。为了克服硅胶基体模限制的问题，研究团队提出了一种新型的水凝胶基础模来模仿人体组织的物理和视觉特性，并兼容电手术设备。与先前的水凝胶体模不同的是，该设计中加入了近红外染料以实现荧光引导下的肿瘤分割功能。通过自主现实世界中的机器人实验验证了系统性能，在69秒内达到了1.44毫米平均切缘精度的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robotic systems hold potential for improving renal tumor resectionaccuracy and patient outcomes. We present a fluorescence-guided robotic systemcapable of planning and executing incision paths around exophytic renal tumorswith a clinically relevant resection margin. Leveraging point cloudobservations, the system handles irregular tumor shapes and distinguisheshealthy from tumorous tissue based on near-infrared imaging, akin toindocyanine green staining in partial nephrectomy. Tissue-mimicking phantomsare crucial for the development of autonomous robotic surgical systems forinterventions where acquiring ex-vivo animal tissue is infeasible, such ascancer of the kidney and renal pelvis. To this end, we propose novelhydrogel-based kidney phantoms with exophytic tumors that mimic the physicaland visual behavior of tissue, and are compatible with electrosurgicalinstruments, a common limitation of silicone-based phantoms. In contrast toprevious hydrogel phantoms, we mix the material with near-infrared dye toenable fluorescence-guided tumor segmentation. Autonomous real-world roboticexperiments validate our system and phantoms, achieving an average marginaccuracy of 1.44 mm in a completion time of 69 sec.</description>
      <author>example@mail.com (Ethan Kilmer, Joseph Chen, Jiawei Ge, Preksha Sarda, Richard Cha, Kevin Cleary, Lauren Shepard, Ahmed Ezzat Ghazi, Paul Maria Scheikl, Axel Krieger)</author>
      <guid isPermaLink="false">2503.02265v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Spatiotemporal Correlation Anomaly Detection Method Based on Time-Frequency-Domain Feature Fusion and a Dynamic Graph Neural Network in Wireless Sensor Network</title>
      <link>http://arxiv.org/abs/2503.00036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于频域特征和动态图神经网络的无线传感器网络（WSN）异常检测方法，通过设计自编码重构框架有效解决了现有注意力机制变压器在捕捉长期依赖性和计算复杂性方面的局限。&lt;h4&gt;背景&lt;/h4&gt;注意力驱动的转换器在网络中用于无线传感的时间异常检测上发挥了重要作用，但它们存在无法完全可靠地捕获长期依赖、高计算复杂度以及未充分提取时空特征等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的WSN时间序列数据异常检测方法来克服现有模型在捕捉长期依赖性和计算效率上的问题，并更有效地利用多节点WSN时间序列的时空相关性。&lt;h4&gt;方法&lt;/h4&gt;首先，采用离散小波变换有效分解时间序列的趋势和季节成分；其次，设计频域注意力机制充分利用正常数据与异常数据之间幅度分布的区别；最后，通过结合注意机制和图卷积网络（GCN）提出多模态融合动态图卷积网络（MFDGCN），自适应地提取空间相关特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法在公共数据集上的精度、召回率以及F1得分分别为93.5%，比现有模型提高了2.9%。&lt;h4&gt;结论&lt;/h4&gt;该异常检测方法通过改进的时间序列分解，频域注意力机制和动态图卷积网络的结合，在提高WSN时间异常检测效果方面取得了显著进步。&lt;h4&gt;翻译&lt;/h4&gt;基于注意的转换器在无线传感器网络（WSN）定时异常检测中发挥了重要作用，因为它们能够捕捉长期依赖性。然而，有几个问题需要解决，例如它们捕获长期依赖性的能力并不完全可靠，计算复杂度水平很高，并且没有充分提取WSN定时数据的空间时间特征来检测多节点WSN定时数据的相关异常。为了解决这些限制，本文提出了一种将频率域特性与动态图神经网络（GNN）集成在设计的自编码重构框架下的WSN异常检测方法。首先，离散小波变换有效地分解了时间序列的趋势和季节成分来解决转换器长期可靠性差的问题。其次，设计了一个频域注意机制来充分利用正常数据和异常数据之间在这个领域的幅度分布差异。最后，通过结合注意力机制和图卷积网络（GCN）设计了一种基于多模态融合的动态图卷积网络（MFDGCN），以自适应地提取空间相关特性。在公共数据集上进行的一系列实验及其结果表明，本文设计的异常检测方法比现有方法具有更高的精度和召回率，F1分数为93.5％，比现有模型提高了2.9％。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Attention-based transformers have played an important role in wireless sensornetwork (WSN) timing anomaly detection due to their ability to capturelong-term dependencies. However, there are several issues that must beaddressed, such as the fact that their ability to capture long-termdependencies is not completely reliable, their computational complexity levelsare high, and the spatiotemporal features of WSN timing data are notsufficiently extracted for detecting the correlation anomalies of multinode WSNtiming data. To address these limitations, this paper proposes a WSN anomalydetection method that integrates frequency-domain features with dynamic graphneural networks (GNN) under a designed self-encoder reconstruction framework.First, the discrete wavelet transform effectively decomposes trend and seasonalcomponents of time series to solve the poor long-term reliability oftransformers. Second, a frequency-domain attention mechanism is designed tomake full use of the difference between the amplitude distributions of normaldata and anomalous data in this domain. Finally, a multimodal fusion-baseddynamic graph convolutional network (MFDGCN) is designed by combining anattention mechanism and a graph convolutional network (GCN) to adaptivelyextract spatial correlation features. A series of experiments conducted onpublic datasets and their results demonstrate that the anomaly detection methoddesigned in this paper exhibits superior precision and recall than the existingmethods do, with an F1 score of 93.5%, representing an improvement of 2.9% overthat of the existing models.</description>
      <author>example@mail.com (Miao Ye, Zhibang Jiang, Xingsi Xue, Xingwang Li, Peng Wen, Yong Wang)</author>
      <guid isPermaLink="false">2503.00036v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown Environments</title>
      <link>http://arxiv.org/abs/2503.02106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种框架，用于解决机器人在室内环境中进行多对象搜索时遇到的挑战。该框架结合了视觉语言模型（VLM）推理、前沿探索和部分可观测马尔可夫决策过程（POMDP），以提高搜索效率并克服观察不确定性的障碍。&lt;h4&gt;背景&lt;/h4&gt;物体搜索是部署在室内环境中的机器人的一项基本任务，但观察不稳定性和开放词汇表模型带来的挑战使得这一任务变得复杂。虽然基础模型可以推断出对象的位置关系，但在遮挡和混乱环境中恢复失败的能力仍然至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一个框架来解决多对象搜索问题，特别是在新环境中寻找多个物体时遇到的困难，并提高机器人在室内环境中的搜索效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一个结合视觉语言模型推理、前沿探索技术和部分可观测马尔可夫决策过程（POMDP）的综合框架。VLM增强了对物体与环境关系的理解，而基于前沿的探索技术帮助导航未知空间；POMDP则模拟了观察不确定性。&lt;h4&gt;主要发现&lt;/h4&gt;在120个仿真的HM3D场景和一个真实的机器人实验中对该框架进行了测试，在效率和成功率方面均优于基线方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的结合VLM、前沿探索技术和POMDP的综合框架能够显著提升机器人解决多对象搜索问题的能力，尤其是在应对观察不确定性时表现优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object search is a fundamental task for robots deployed in indoor buildingenvironments, yet challenges arise due to observation instability, especiallyfor open-vocabulary models. While foundation models (LLMs/VLMs) enablereasoning about object locations even without direct visibility, the ability torecover from failures and replan remains crucial. The Multi-Object Search (MOS)problem further increases complexity, requiring the tracking multiple objectsand thorough exploration in novel environments, making observation uncertaintya significant obstacle. To address these challenges, we propose a frameworkintegrating VLM-based reasoning, frontier-based exploration, and a PartiallyObservable Markov Decision Process (POMDP) framework to solve the MOS problemin novel environments. VLM enhances search efficiency by inferringobject-environment relationships, frontier-based exploration guides navigationin unknown spaces, and POMDP models observation uncertainty, allowing recoveryfrom failures in occlusion and cluttered environments. We evaluate ourframework on 120 simulated scenarios across several Habitat-Matterport3D (HM3D)scenes and a real-world robot experiment in a 50-square-meter office,demonstrating significant improvements in both efficiency and success rate overbaseline methods.</description>
      <author>example@mail.com (Qianwei Wang, Yifan Xu, Vineet Kamat, Carol Menassa)</author>
      <guid isPermaLink="false">2503.02106v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>InversionGNN: A Dual Path Network for Multi-Property Molecular Optimization</title>
      <link>http://arxiv.org/abs/2503.01488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了InversionGNN框架，一种用于多目标药物发现的高效双路径图神经网络。&lt;h4&gt;背景&lt;/h4&gt;在药物发现中探索化学空间以寻找同时满足多种属性的新分子非常重要。现有方法在平衡这些相互冲突或关联的化学属性时经常遇到挑战。&lt;h4&gt;目的&lt;/h4&gt;提出InversionGNN框架，旨在解决多目标药物发现中的性能权衡问题。&lt;h4&gt;方法&lt;/h4&gt;通过直接预测路径训练模型进行多重性质预测，学习最佳的功能基团组合；然后利用这一知识帮助反转生成路径产生具有所需性质的分子。此外，引入基于梯度的Pareto搜索方法以平衡冲突属性并生成最优分子。&lt;h4&gt;主要发现&lt;/h4&gt;InversionGNN能够在离散化学空间中近似地探索完整的Pareto前沿，并在多目标设置下展示出了高效性和有效性。&lt;h4&gt;结论&lt;/h4&gt;InversionGNN框架为解决药物发现中的多目标问题提供了一种有效且样本高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;探索化学空间以找到同时满足多种性质的新分子对于药物发现至关重要。然而，现有方法往往难以在属性之间进行权衡，因为这些属性是相互冲突或关联的。为了应对这一挑战，我们引入了InversionGNN框架——一种用于多目标药物发现的有效且样本高效的双路径图神经网络（GNN）。在这个模型中，直接预测路径训练模型来进行多重性质预测，以获取功能基团最佳组合的知识；然后，在反转生成路径上，通过所学的化学知识帮助产生具有所需属性的分子。为了在反转路径中解码多属性的复杂信息，我们提出了一种基于梯度的方法来平衡冲突属性并生成最优分子。此外，InversionGNN能够近似地探索离散化学空间中的完整Pareto前沿。全面的实验评估显示，在包括药物发现在内的各种离散多目标设置下，InversionGNN既有效又样本高效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploring chemical space to find novel molecules that simultaneously satisfymultiple properties is crucial in drug discovery. However, existing methodsoften struggle with trading off multiple properties due to the conflicting orcorrelated nature of chemical properties. To tackle this issue, we introduceInversionGNN framework, an effective yet sample-efficient dual-path graphneural network (GNN) for multi-objective drug discovery. In the directprediction path of InversionGNN, we train the model for multi-propertyprediction to acquire knowledge of the optimal combination of functionalgroups. Then the learned chemical knowledge helps the inversion generation pathto generate molecules with required properties. In order to decode the complexknowledge of multiple properties in the inversion path, we propose agradient-based Pareto search method to balance conflicting properties andgenerate Pareto optimal molecules. Additionally, InversionGNN is able to searchthe full Pareto front approximately in discrete chemical space. Comprehensiveexperimental evaluations show that InversionGNN is both effective andsample-efficient in various discrete multi-objective settings including drugdiscovery.</description>
      <author>example@mail.com (Yifan Niu, Ziqi Gao, Tingyang Xu, Yang Liu, Yatao Bian, Yu Rong, Junzhou Huang, Jia Li)</author>
      <guid isPermaLink="false">2503.01488v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Wavelet-Driven Masked Image Modeling: A Path to Efficient Visual Representation</title>
      <link>http://arxiv.org/abs/2503.00782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于小波变换的Masked Image Modeling (MIM) 方法，通过利用频率域分析来实现紧凑的图像特征表示。该方法解决了传统像素级MIM重建过程中过度关注细节的问题，并提高了训练效率。&lt;h4&gt;背景&lt;/h4&gt;掩膜图像建模(MIM)因其在自监督学习中的出色表现而备受关注，但其基于像素的重建过程会导致不必要的训练时间延长。&lt;h4&gt;目的&lt;/h4&gt;通过引入小波变换来改进MIM方法，以减少冗余信息的影响并加快训练速度。&lt;h4&gt;方法&lt;/h4&gt;使用多层分解技术将图像进行小波变换处理，并利用不同层级的小波系数构建不同频率和尺度的重建目标。然后在MIM过程中整合这些重建目标，同时可调整权重以优先考虑关键信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明该方法在多种下游任务中的性能与现有方法相当或更优，且训练效率更高。&lt;h4&gt;结论&lt;/h4&gt;基于小波变换的方法能够有效地改进MIM模型的训练过程，并提高其处理大规模视觉数据的能力。&lt;h4&gt;翻译&lt;/h4&gt;Masked Image Modeling (MIM)因其出色的自监督学习能力而受到关注。然而，图像中的冗余信息导致像素级重建过于关注细节，从而增加了不必要的训练时间。通过采用小波变换技术对图像进行频率分析，可以实现更紧凑的特征表示和高效的训练过程。实验表明该方法在多种下游任务中表现出色且更具效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked Image Modeling (MIM) has garnered significant attention inself-supervised learning, thanks to its impressive capacity to learn scalablevisual representations tailored for downstream tasks. However, imagesinherently contain abundant redundant information, leading the pixel-based MIMreconstruction process to focus excessively on finer details such as textures,thus prolonging training times unnecessarily. Addressing this challengerequires a shift towards a compact representation of features during MIMreconstruction. Frequency domain analysis provides a promising avenue forachieving compact image feature representation. In contrast to the commonlyused Fourier transform, wavelet transform not only offers frequency informationbut also preserves spatial characteristics and multi-level features of theimage. Additionally, the multi-level decomposition process of wavelettransformation aligns well with the hierarchical architecture of modern neuralnetworks. In this study, we leverage wavelet transform as a tool for efficientrepresentation learning to expedite the training process of MIM. Specifically,we conduct multi-level decomposition of images using wavelet transform,utilizing wavelet coefficients from different levels to construct distinctreconstruction targets representing various frequencies and scales. Thesereconstruction targets are then integrated into the MIM process, withadjustable weights assigned to prioritize the most crucial information.Extensive experiments demonstrate that our method achieves comparable orsuperior performance across various downstream tasks while exhibiting highertraining efficiency.</description>
      <author>example@mail.com (Wenzhao Xiang, Chang Liu, Hongyang Yu, Xilin Chen)</author>
      <guid isPermaLink="false">2503.00782v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Biomedical Foundation Model: A Survey</title>
      <link>http://arxiv.org/abs/2503.02104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了基础模型在生物医学领域的潜在应用，涵盖了计算生物学、药物研发与开发、临床信息学、医学影像和公共卫生等多个领域。&lt;h4&gt;背景&lt;/h4&gt;基础模型自2021年引入以来，通过无监督学习从大规模未标记数据集中获取知识，这些模型如GPT能够在问答和视觉理解等多种下游任务中表现出色，并超越了特定任务的AI模型。生物医学领域的基础模型开发标志着人工智能在理解和解析复杂生物学现象以及推动医疗研究与实践方面的重要进展。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探索基础模型在健康科学中的应用潜力，以期激发相关领域的进一步研究和创新。&lt;h4&gt;方法&lt;/h4&gt;综述了各类生物医学任务中使用的基础模型及其潜在的广泛应用场景。&lt;h4&gt;主要发现&lt;/h4&gt;基础模型具备广泛的适用性和卓越的任务性能，在多个生物医学领域展现出巨大的应用前景。&lt;h4&gt;结论&lt;/h4&gt;未来的研究应当更加关注如何将基础模型有效地应用于解决复杂的健康科学问题，以促进医疗技术的进步和临床实践的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, first introduced in 2021, are large-scale pre-trainedmodels (e.g., large language models (LLMs) and vision-language models (VLMs))that learn from extensive unlabeled datasets through unsupervised methods,enabling them to excel in diverse downstream tasks. These models, like GPT, canbe adapted to various applications such as question answering and visualunderstanding, outperforming task-specific AI models and earning their name dueto broad applicability across fields. The development of biomedical foundationmodels marks a significant milestone in leveraging artificial intelligence (AI)to understand complex biological phenomena and advance medical research andpractice. This survey explores the potential of foundation models acrossdiverse domains within biomedical fields, including computational biology, drugdiscovery and development, clinical informatics, medical imaging, and publichealth. The purpose of this survey is to inspire ongoing research in theapplication of foundation models to health science.</description>
      <author>example@mail.com (Xiangrui Liu, Yuanyuan Zhang, Yingzhou Lu, Changchang Yin, Xiaoling Hu, Xiaoou Liu, Lulu Chen, Sheng Wang, Alexander Rodriguez, Huaxiu Yao, Yezhou Yang, Ping Zhang, Jintai Chen, Tianfan Fu, Xiao Wang)</author>
      <guid isPermaLink="false">2503.02104v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Social Media Rumor Detection: A Semantic and Graph Neural Network Approach for the 2024 Global Election</title>
      <link>http://arxiv.org/abs/2503.01394v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合语义分析和图神经网络的新方法，以解决社交媒体上谣言传播的问题。&lt;h4&gt;背景&lt;/h4&gt;社交媒体平台的发展极大地改变了信息传播的速度和方式，带来了有益和有害的影响。这些平台促进了快速通信的同时也加速了谣言和极端言论的传播，尤其是在选举期间对公共舆论和行为产生了显著影响。&lt;h4&gt;目的&lt;/h4&gt;针对有效的社交网络谣言检测需求，提出了一种新的方法来应对2024年全球各地前所未有的选举挑战。&lt;h4&gt;方法&lt;/h4&gt;采用语义分析与图神经网络相结合的方法。首先使用精调的BERT模型将文本内容向量化，并构建一个由推文和评论作为节点、互动行为为边的有向图，然后通过SAGEWithEdgeAttention（GraphSAGE的扩展）进行更精确特征聚合。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在复杂的社会网络结构中能够实现细粒度分析，提高谣言检测准确性，并且显著优于传统的内容分析和基于时间的方法。&lt;h4&gt;结论&lt;/h4&gt;研究得出的新方法提供了一种理论上严谨、实践中有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of social media platforms has revolutionized the speed andmanner in which information is disseminated, leading to both beneficial anddetrimental effects on society. While these platforms facilitate rapidcommunication, they also accelerate the spread of rumors and extremist speech,impacting public perception and behavior significantly. This issue isparticularly pronounced during election periods, where the influence of socialmedia on election outcomes has become a matter of global concern. With theunprecedented number of elections in 2024, against this backdrop, the electionecosystem has encountered unprecedented challenges. This study addresses theurgent need for effective rumor detection on social media by proposing a novelmethod that combines semantic analysis with graph neural networks. We havemeticulously collected a dataset from PolitiFact and Twitter, focusing onpolitically relevant rumors. Our approach involves semantic analysis using afine-tuned BERT model to vectorize text content and construct a directed graphwhere tweets and comments are nodes, and interactions are edges. The core ofour method is a graph neural network, SAGEWithEdgeAttention, which extends theGraphSAGE model by incorporating first-order differences as edge attributes andapplying an attention mechanism to enhance feature aggregation. This innovativeapproach allows for the fine-grained analysis of the complex social networkstructure, improving rumor detection accuracy. The study concludes that ourmethod significantly outperforms traditional content analysis and time-basedmodels, offering a theoretically sound and practically efficient solution.</description>
      <author>example@mail.com (Liu Yan, Liu Yunpeng, Zhao Liang)</author>
      <guid isPermaLink="false">2503.01394v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>UniWav: Towards Unified Pre-training for Speech Representation Learning and Generation</title>
      <link>http://arxiv.org/abs/2503.00733v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025; demo page at  https://alexander-h-liu.github.io/uniwav-demo.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;构建了一个统一的预训练框架UniWav，该框架可以同时适用于辨别任务和生成任务，在语音识别、文本到语音转换以及语音标记化等应用中表现出与特定任务模型相当的表现。&lt;h4&gt;背景&lt;/h4&gt;目前的语音处理方法依赖于不同的基础模型，这些模型通常是为了解决具体的辨别或生成任务而设计的。现有的预训练技术不足以同时适用于这两种类型的任务。&lt;h4&gt;目的&lt;/h4&gt;提出一个统一的预训练框架，可以在单一的基础模型中实现多种类型的任务，并减少预训练的成本和复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了名为UniWav的编码器-解码器架构，该架构旨在统一预训练表示学习与生成任务。通过适当的预训练设计选择，可以同时学习适用于两种类型的任务的表示编码器和生成音频解码器。&lt;h4&gt;主要发现&lt;/h4&gt;在语音识别、文本到语音转换以及语音标记化上，UniWav的表现与针对特定任务单独训练的基础模型相当。&lt;h4&gt;结论&lt;/h4&gt;证明了一种通用的语音基础模型可以替代多种专为特定任务设计的基础模型，并且能够降低预训练的成本和复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pre-training and representation learning have been playing an increasinglyimportant role in modern speech processing. Nevertheless, differentapplications have been relying on different foundation models, sincepredominant pre-training techniques are either designed for discriminativetasks or generative tasks. In this work, we make the first attempt at buildinga unified pre-training framework for both types of tasks in speech. We showthat with the appropriate design choices for pre-training, one can jointlylearn a representation encoder and generative audio decoder that can be appliedto both types of tasks. We propose UniWav, an encoder-decoder frameworkdesigned to unify pre-training representation learning and generative tasks. Onspeech recognition, text-to-speech, and speech tokenization, UniWav achievescomparable performance to different existing foundation models, each trained ona specific task. Our findings suggest that a single general-purpose foundationmodel for speech can be built to replace different foundation models, reducingthe overhead and cost of pre-training.</description>
      <author>example@mail.com (Alexander H. Liu, Sang-gil Lee, Chao-Han Huck Yang, Yuan Gong, Yu-Chiang Frank Wang, James R. Glass, Rafael Valle, Bryan Catanzaro)</author>
      <guid isPermaLink="false">2503.00733v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>EPEE: Towards Efficient and Effective Foundation Models in Biomedicine</title>
      <link>http://arxiv.org/abs/2503.02053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to npj Digital Medicine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了EPEE策略，一种基于熵和耐心的混合早期退出策略，旨在提高基础模型在生物医学任务中的推理效率。&lt;h4&gt;背景&lt;/h4&gt;尽管基础模型如GPT、CLIP等显著推动了众多生物医学任务的发展，但高推理延迟及过度思考问题限制了它们在临床环境下的实时应用。&lt;h4&gt;目的&lt;/h4&gt;提出EPEE策略以克服现有早期退出方法的弱点，并提高基础模型在实际应用场景中的效率和效果。&lt;h4&gt;方法&lt;/h4&gt;通过四个基础模型（BERT、ALBERT、GPT-2和ViT）进行三项核心生物医学任务（分类，关系抽取以及事件抽取）的实验评估。实验涵盖了十二个不同的数据集，包括临床笔记及医学图像。&lt;h4&gt;主要发现&lt;/h4&gt;EPEE在减少推理时间的同时保持或提升了准确性，在多种不同类型的数据集中展示了其适应性和有效性。&lt;h4&gt;结论&lt;/h4&gt;EPEE解决了将基础模型部署到医疗保健领域中的关键障碍，并为实时临床决策提供了可能的实用解决方案，支持可靠的高效工作流程。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含中文描述，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models, including language models, e.g., GPT, and vision models,e.g., CLIP, have significantly advanced numerous biomedical tasks. Despitethese advancements, the high inference latency and the "overthinking" issues inmodel inference impair the efficiency and effectiveness of foundation models,thus limiting their application in real-time clinical settings. To addressthese challenges, we proposed EPEE (Entropy- and Patience-based Early Exiting),a novel hybrid strategy designed to improve the inference efficiency offoundation models. The core idea was to leverage the strengths of entropy-basedand patience-based early exiting methods to overcome their respectiveweaknesses. To evaluate EPEE, we conducted experiments on three core biomedicaltasks-classification, relation extraction, and event extraction-using fourfoundation models (BERT, ALBERT, GPT-2, and ViT) across twelve datasets,including clinical notes and medical images. The results showed that EPEEsignificantly reduced inference time while maintaining or improving accuracy,demonstrating its adaptability to diverse datasets and tasks. EPEE addressedcritical barriers to deploying foundation models in healthcare by balancingefficiency and effectiveness. It potentially provided a practical solution forreal-time clinical decision-making with foundation models, supporting reliableand efficient workflows.</description>
      <author>example@mail.com (Zaifu Zhan, Shuang Zhou, Huixue Zhou, Zirui Liu, Rui Zhang)</author>
      <guid isPermaLink="false">2503.02053v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Road Boundary Detection Using 4D mmWave Radar for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2503.01930v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于4D毫米波雷达的道路边界检测方法（4DRadarRBD），以解决传统视觉传感器在复杂驾驶环境中的成本和性能问题。&lt;h4&gt;背景&lt;/h4&gt;道路边界检测对于自动驾驶和高级驾驶员辅助系统至关重要，但传统的基于摄像头和LiDAR的方法在恶劣光照条件下表现不佳或成本过高。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于4D毫米波雷达的道路边界检测方法，以提高复杂驾驶场景中的鲁棒性和降低成本。&lt;h4&gt;方法&lt;/h4&gt;通过物理约束减少点云噪声，并利用距离基损失函数进行分割。引入了对时间动态的捕捉机制，考虑每个点与车辆运动补偿后的道路边界检测结果之间的偏差以及点云的空间分布。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界驾驶测试中实现了93%的道路边界点分割准确率和高达0.023米的中位距离误差，并且相较于基线模型错误降低了92.6%。&lt;h4&gt;结论&lt;/h4&gt;4DRadarRBD方法在成本效益、鲁棒性和准确性方面显著优于传统道路边界检测技术，尤其适用于复杂驾驶环境。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting road boundaries, the static physical edges of the available drivingarea, is important for safe navigation and effective path planning inautonomous driving and advanced driver-assistance systems (ADAS).Traditionally, road boundary detection in autonomous driving relies on camerasand LiDAR. However, they are vulnerable to poor lighting conditions, such asnighttime and direct sunlight glare, or prohibitively expensive for low-endvehicles. To this end, this paper introduces 4DRadarRBD, the first roadboundary detection method based on 4D mmWave radar which is cost-effective androbust in complex driving scenarios. The main idea is that road boundaries(e.g., fences, bushes, roadblocks), reflect millimeter waves, thus generatingpoint cloud data for the radar. To overcome the challenge that the 4D mmWaveradar point clouds contain many noisy points, we initially reduce noisy pointsvia physical constraints for road boundaries and then segment the road boundarypoints from the noisy points by incorporating a distance-based loss whichpenalizes for falsely detecting the points far away from the actual roadboundaries. In addition, we capture the temporal dynamics of point cloudsequences by utilizing each point's deviation from the vehiclemotion-compensated road boundary detection result obtained from the previousframe, along with the spatial distribution of the point cloud for point-wiseroad boundary segmentation. We evaluated 4DRadarRBD through real-world drivingtests and achieved a road boundary point segmentation accuracy of 93$\%$, witha median distance error of up to 0.023 m and an error reduction of 92.6$\%$compared to the baseline model.</description>
      <author>example@mail.com (Yuyan Wu, Hae Young Noh)</author>
      <guid isPermaLink="false">2503.01930v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Generative Modeling by Kernel Similarity Matching</title>
      <link>http://arxiv.org/abs/2503.00655v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  37 Pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了通过捕捉输入样本之间的相似性来学习表示的方法，并将其应用于生成模型中，以便能够将表示映射回输入空间。&lt;h4&gt;背景&lt;/h4&gt;理解大脑如何编码刺激是计算神经科学的基本问题。这方面的见解推动了具有类似大脑学习能力的人工神经网络的设计和开发。&lt;h4&gt;目的&lt;/h4&gt;研究一种基于内核相似性匹配的框架用于生成模型，并探讨该方法在脑任务表示编码中的潜在应用。&lt;h4&gt;方法&lt;/h4&gt;从先前工作中提出的稀疏编码目标修改开始，展示了在这个上下文中表征学习等同于最大化输入内核与隐含内核之间的相似性。提出了一个新的交替方向乘子法（ADMM）算法来解决优化问题，并讨论了优化过程的解释。&lt;h4&gt;主要发现&lt;/h4&gt;通过学习潜在空间中的内核结构可以形成一种隐式生成模型，该框架能够适应学习流形结构。&lt;h4&gt;结论&lt;/h4&gt;这种方法结合了使用相似性匹配（自下而上方法）进行表征学习与预测编码（自顶向下方法），有助于构建一个具有生物合理性的架构来学习模型参数。&lt;h4&gt;翻译&lt;/h4&gt;理解大脑如何对刺激进行编码一直是计算神经科学的基本问题。这些问题的见解促使设计和开发了一种人工神经网络，该网络通过引入类似大脑的学习能力来学习表示。最近的研究试图通过捕捉输入样本之间的相似性来解决这个问题。然而，这种方法迄今为止仅用于从输入中学习下游特征，并未在生成范式（可将表示映射回输入空间的范式）下进行研究，在这种情况下，不仅可以实现自底向上的相互作用（刺激到潜在），还可以以自顶向下的方式（潜在到刺激）学习特征。我们探讨了一种用于生成建模的内核相似性匹配框架。从先前工作中提出的一种修改后的稀疏编码目标开始，我们展示了在这种背景下表示学习等同于最大化输入内核与隐含内核之间的相似性。我们证明了通过在潜在空间中学习内核结构可以形成一种隐式生成模型，并展示如何使该框架适应学习流形结构，这可能有助于了解任务表征可以在大脑中是如何编码的。为了实现目标，我们提出了一种新颖的交替方向乘子法（ADMM）算法，并讨论了优化过程的解释。最后，我们探讨了这种表示学习问题可以导向一种生物学上合理的架构来学习模型参数，该架构将使用相似性匹配进行表征学习（自下而上的方法）与预测编码（自顶向下的方法）结合起来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding how the brain encodes stimuli has been a fundamental problem incomputational neuroscience. Insights into this problem have led to the designand development of artificial neural networks that learn representations byincorporating brain-like learning abilities. Recently, learning representationsby capturing similarity between input samples has been studied to tackle thisproblem. This approach, however, has thus far been used to only learndownstream features from an input and has not been studied in the context of agenerative paradigm, where one can map the representations back to the inputspace, incorporating not only bottom-up interactions (stimuli to latent) butalso learning features in a top-down manner (latent to stimuli). We investigatea kernel similarity matching framework for generative modeling. Starting with amodified sparse coding objective for learning representations proposed in priorwork, we demonstrate that representation learning in this context is equivalentto maximizing similarity between the input kernel and a latent kernel. We showthat an implicit generative model arises from learning the kernel structure inthe latent space and show how the framework can be adapted to learn manifoldstructures, potentially providing insights as to how task representations canbe encoded in the brain. To solve the objective, we propose a novel AlternateDirection Method of Multipliers (ADMM) based algorithm and discuss theinterpretation of the optimization process. Finally, we discuss how thisrepresentation learning problem can lead towards a biologically plausiblearchitecture to learn the model parameters that ties together representationlearning using similarity matching (a bottom-up approach) with predictivecoding (a top-down approach).</description>
      <author>example@mail.com (Shubham Choudhary, Paul Masset, Demba Ba)</author>
      <guid isPermaLink="false">2503.00655v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Statistical physics analysis of graph neural networks: Approaching optimality in the contextual stochastic block model</title>
      <link>http://arxiv.org/abs/2503.01361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）设计用于处理与图关联的数据，并在不断增加的应用领域中发挥作用。然而，与其他现代机器学习技术一样，其理论理解仍然有限。&lt;h4&gt;目的&lt;/h4&gt;解决由于过度平滑等问题导致的远距离节点信息获取困难的问题，并探讨如何通过增加深度来接近贝叶斯最优性。&lt;h4&gt;方法&lt;/h4&gt;分析了基于上下文随机块模型生成的数据进行训练的基本图卷积网络（GCN）在节点分类任务上的泛化性能。使用复制方法，在高维极限下推导问题的自由能量，以预测其渐近性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 增加深度对于接近贝叶斯最优性至关重要；2. GCN架构需要随着深度的变化进行调整以避免过度平滑；3. 大深度限制可以与贝叶斯最优性相近，并且导致连续GCN的形成；4. 通过类似于动态平均场理论的方法来处理连续极限，以及在大正则化下展开解对应于深层GCN性能方程。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一个有前途的工具，用于进一步分析深度神经网络，并可能对未来的研究有所贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经以中文形式给出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are designed to process data associated withgraphs. They are finding an increasing range of applications; however, as withother modern machine learning techniques, their theoretical understanding islimited. GNNs can encounter difficulties in gathering information from nodesthat are far apart by iterated aggregation steps. This situation is partlycaused by so-called oversmoothing; and overcoming it is one of the practicallymotivated challenges. We consider the situation where information is aggregatedby multiple steps of convolution, leading to graph convolutional networks(GCNs). We analyze the generalization performance of a basic GCN, trained fornode classification on data generated by the contextual stochastic block model.We predict its asymptotic performance by deriving the free energy of theproblem, using the replica method, in the high-dimensional limit. Calling depththe number of convolutional steps, we show the importance of going to largedepth to approach the Bayes-optimality. We detail how the architecture of theGCN has to scale with the depth to avoid oversmoothing. The resulting largedepth limit can be close to the Bayes-optimality and leads to a continuous GCN.Technically, we tackle this continuous limit via an approach that resemblesdynamical mean-field theory (DMFT) with constraints at the initial and finaltimes. An expansion around large regularization allows us to solve thecorresponding equations for the performance of the deep GCN. This promisingtool may contribute to the analysis of further deep neural networks.</description>
      <author>example@mail.com (O. Duranthon, L. Zdeborová)</author>
      <guid isPermaLink="false">2503.01361v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Pretrained Embeddings as a Behavior Specification Mechanism</title>
      <link>http://arxiv.org/abs/2503.02012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用嵌入式数学表示来正式描述依赖于感知模型与物理世界交互的系统的规范方法。&lt;h4&gt;背景&lt;/h4&gt;现有的系统规范语言难以精确表达基于感知模型的AI系统的复杂行为特性。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的逻辑形式，即嵌入时态逻辑(ETL)，用于在AI系统中更广泛地描述和验证行为属性。&lt;h4&gt;方法&lt;/h4&gt;引入了嵌入作为规范语言中的第一类构造，并通过距离度量理想与观察到的嵌入之间的差异来表达性质。&lt;h4&gt;主要发现&lt;/h4&gt;初步评估表明，基于嵌入式的规范可以引导机器人等AI系统表现出期望的行为。&lt;h4&gt;结论&lt;/h4&gt;提出的ETL方法能够在感知驱动的任务中实现有效的行为控制和验证。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种正式指定依赖于感知模型与物理世界交互的系统的操作属性的方法。关键思想是在规范语言中引入嵌入（现实概念的数学表示），其中性质以理想嵌入和观察到的嵌入之间的距离来表达。为了实现这种方法，我们提出了称为嵌入时态逻辑(ETL)的新类型的时间逻辑，并描述了如何使用它以前所未有的方式表达AI系统的一系列属性。通过涉及由基础模型驱动的机器人规划任务的初步评估，展示了ETL的应用潜力；结果很有前景，表明基于嵌入式的规范可以引导系统朝向期望的行为。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose an approach to formally specifying the behavioral properties ofsystems that rely on a perception model for interactions with the physicalworld. The key idea is to introduce embeddings -- mathematical representationsof a real-world concept -- as a first-class construct in a specificationlanguage, where properties are expressed in terms of distances between a pairof ideal and observed embeddings. To realize this approach, we propose a newtype of temporal logic called Embedding Temporal Logic (ETL), and describe howit can be used to express a wider range of properties about AI-enabled systemsthan previously possible. We demonstrate the applicability of ETL through apreliminary evaluation involving planning tasks in robots that are driven byfoundation models; the results are promising, showing that embedding-basedspecifications can be used to steer a system towards desirable behaviors.</description>
      <author>example@mail.com (Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang)</author>
      <guid isPermaLink="false">2503.02012v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Synergy Between Sufficient Changes and Sparse Mixing Procedure for Disentangled Representation Learning</title>
      <link>http://arxiv.org/abs/2503.00639v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了解开观测数据背后潜在变量的解耦表示学习，并分析两种不同的假设如何结合使用以提高可识别性。&lt;h4&gt;背景&lt;/h4&gt;解耦表示学习通常需要较强的先验假设来确保潜在变量的可识别性。一方面，某些方法依赖于辅助变量（如领域索引）所指示的潜在变量分布的变化；另一方面，一些方法利用混合过程中的结构稀疏性假设。&lt;h4&gt;目的&lt;/h4&gt;提出一个在较少限制条件下实现潜在变量可识别性的理论，并开发一种包含领域编码网络和稀疏混合约束的估计框架。&lt;h4&gt;方法&lt;/h4&gt;当以辅助变量为条件时，该研究提出了基于稀疏混合过程假定提供从估算到真实潜在变量映射结构约束的方法，并利用变分自编码器(VAE)和生成对抗网络(GAN)实现此理论。&lt;h4&gt;主要发现&lt;/h4&gt;两种看似不相关的假设可以互补使用来提高可识别性；当以辅助变量为条件时，稀疏混合过程假定提供了从估算到真实潜在变量映射的结构约束，并弥补了可能不足的分布变化。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在合成数据集和实际世界数据集上的结果支持该理论。此方法增强了在现实场景中的应用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了解耦表示学习的目标是解开观测数据背后的潜在变量，同时指出两种不同的假设（基于辅助变量的分布变化与稀疏混合过程）如何结合使用可以提高可识别性，并介绍了一种新的估计框架及其实施方式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Disentangled representation learning aims to uncover latent variablesunderlying the observed data, and generally speaking, rather strong assumptionsare needed to ensure identifiability. Some approaches rely on sufficientchanges on the distribution of latent variables indicated by auxiliaryvariables such as domain indices, but acquiring enough domains is oftenchallenging. Alternative approaches exploit structural sparsity assumptions onthe mixing procedure, but such constraints are usually (partially) violated inpractice. Interestingly, we find that these two seemingly unrelated assumptionscan actually complement each other to achieve identifiability. Specifically,when conditioned on auxiliary variables, the sparse mixing procedure assumptionprovides structural constraints on the mapping from estimated to true latentvariables and hence compensates for potentially insufficient distributionchanges. Building on this insight, we propose an identifiability theory withless restrictive constraints regarding distribution changes and the sparsemixing procedure, enhancing applicability to real-world scenarios.Additionally, we develop an estimation framework incorporating a domainencoding network and a sparse mixing constraint and provide two implementationsbased on variational autoencoders and generative adversarial networks,respectively. Experiment results on synthetic and real-world datasets supportour theoretical results.</description>
      <author>example@mail.com (Zijian Li, Shunxing Fan, Yujia Zheng, Ignavier Ng, Shaoan Xie, Guangyi Chen, Xinshuai Dong, Ruichu Cai, Kun Zhang)</author>
      <guid isPermaLink="false">2503.00639v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Soybean Disease Detection via Interpretable Hybrid CNN-GNN: Integrating MobileNetV2 and GraphSAGE with Cross-Modal Attention</title>
      <link>http://arxiv.org/abs/2503.01284v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合MobileNetV2和GraphSAGE的混合Sequential CNN-图神经网络框架，用于大豆叶片疾病的检测。该方法在识别细微病变特征的同时还能捕捉全局症状模式，并通过可视化技术提高了模型的解释性。&lt;h4&gt;背景&lt;/h4&gt;大豆叶片疾病检测是农业生产力的关键问题，但因其视觉相似的症状和传统方法可解释性差而面临挑战。虽然卷积神经网络（CNN）擅长空间特性提取，但由于忽视了图像间的依赖关系而导致误分类情况较多。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合Sequential CNN-图神经网络框架，旨在提高大豆叶片疾病检测的准确性并增强模型的可解释性。&lt;h4&gt;方法&lt;/h4&gt;该研究结合MobileNetV2进行局部特征提取和GraphSAGE建模图像间的关系。采用基于余弦相似性的邻接矩阵构建图形，节点代表叶子图像，并通过自适应邻居采样定义边。利用Grad-CAM和Eigen-CAM技术提供跨模式可解释性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在包含十种大豆叶片疾病的数据库中实现了97.16%的准确性，超过了单独使用的CNN（≤95.04%）和传统的机器学习方法（≤77.05%）。此外，该研究通过消融实验验证了序贯架构相对于并行或单一模型配置的优势。&lt;h4&gt;结论&lt;/h4&gt;提出的框架提供了一种轻量级且高效的解决方案，不仅在大豆叶片疾病检测方面具有高精度，而且在资源受限的环境中可以实现实时部署。这为植物病理学领域的CNN-GNN集成研究开辟了新的途径。&lt;h4&gt;翻译&lt;/h4&gt;大豆叶片病害检测对农业生产力至关重要，但视觉相似的症状和传统方法解释性差给其带来了挑战。尽管卷积神经网络（CNN）在空间特征提取方面表现出色，但由于忽略了图像间的关系依赖性而容易导致误分类。本文提出了一种可解释的混合Sequential CNN-图神经网络框架，该框架结合了MobileNetV2进行局部特征提取和GraphSAGE建模关系的方法。该架构构建了一个图形，其中节点表示叶子图片，并通过基于余弦相似性的邻接矩阵定义边和自适应邻居采样来捕获细粒度病变特性和整体症状模式，解决了类内相似性问题。跨模式可解释性是通过Grad-CAM和Eigen-CAM可视化技术实现的，生成热图突出显示影响疾病的区域。在包含十种大豆叶片病害的数据集上进行评估，该模型实现了97.16%的准确性，超过了单独的CNN（≤95.04%）和传统机器学习方法（≤77.05%）。消融研究验证了序贯架构相对于并行或单一模型配置的优势。利用轻量级MobileNetV2-GraphSAGE组合的仅2.3百万参数，确保计算效率，能够实现实时部署于资源受限环境中。所提出的方案在准确分类和实际应用之间架起了桥梁，为农业诊断提供了一种稳健、可解释的强大工具，并推进了植物病理学领域CNN-GNN集成研究的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soybean leaf disease detection is critical for agricultural productivity butfaces challenges due to visually similar symptoms and limited interpretabilityin conventional methods. While Convolutional Neural Networks (CNNs) excel inspatial feature extraction, they often neglect inter-image relationaldependencies, leading to misclassifications. This paper proposes aninterpretable hybrid Sequential CNN-Graph Neural Network (GNN) framework thatsynergizes MobileNetV2 for localized feature extraction and GraphSAGE forrelational modeling. The framework constructs a graph where nodes representleaf images, with edges defined by cosine similarity-based adjacency matricesand adaptive neighborhood sampling. This design captures fine-grained lesionfeatures and global symptom patterns, addressing inter-class similaritychallenges. Cross-modal interpretability is achieved via Grad-CAM and Eigen-CAMvisualizations, generating heatmaps to highlight disease-influential regions.Evaluated on a dataset of ten soybean leaf diseases, the model achieves$97.16\%$ accuracy, surpassing standalone CNNs ($\le95.04\%$) and traditionalmachine learning models ($\le77.05\%$). Ablation studies validate thesequential architecture's superiority over parallel or single-modelconfigurations. With only 2.3 million parameters, the lightweightMobileNetV2-GraphSAGE combination ensures computational efficiency, enablingreal-time deployment in resource-constrained environments. The proposedapproach bridges the gap between accurate classification and practicalapplicability, offering a robust, interpretable tool for agriculturaldiagnostics while advancing CNN-GNN integration in plant pathology research.</description>
      <author>example@mail.com (Md Abrar Jahin, Soudeep Shahriar, M. F. Mridha, Nilanjan Dey)</author>
      <guid isPermaLink="false">2503.01284v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Proportionality in Thumbs Up and Down Voting</title>
      <link>http://arxiv.org/abs/2503.01985v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文探讨了在包含正负偏好表达的决策环境中，如何理解比例性原则。&lt;h4&gt;背景&lt;/h4&gt;当前，在宪法人工智能中，公民民主地选择一整套伦理准则来训练基础模型。实践中，人们可能会对不同的伦理原则既表示赞同也表示反对。&lt;h4&gt;目的&lt;/h4&gt;提出两种概念上不同的方法来解释在存在上下投票的情况下比例性的含义。&lt;h4&gt;方法&lt;/h4&gt;第一种方法将选举候选人带来的满足感和否决他们的影响视为可比较的，从而提供综合的比例性保证；第二种方法独立考虑否决权，引入不同于传统比例性的保证。该研究形式化了每个视角下的公理，并通过适应Phragmén规则、Proportional Approval Voting（PAV）规则以及等份额法来考察它们的一致性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了两种解释负偏好情况的比例性原则的方法，并为每种方法建立了理论依据，探讨了这些比例性保证在实际决策中的应用和可行性。&lt;h4&gt;结论&lt;/h4&gt;该研究深化了对包含正负面表达的选举系统中比例性的理解，并为进一步的研究奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;考虑决策设置，在这种情况下代理者通过表示正面和负面偏好来选出一个小组。特别是，在宪法AI中，公民民主地选择一套伦理准则用于训练基础模型。在实践中，人们可能对不同的伦理原则既有赞成也有反对的意见。比例性已经在计算社会选择理论中的同意票上得到了很好的研究，但在考虑负面情绪时其意义仍然不清楚。本文提出了两种概念上截然不同的方法来解释在存在上下投票的情况下比例性的含义。第一种方法将选举候选人带来的满足感和否决他们的影响视为可比较的，从而提供综合的比例性保证；第二种方法独立考虑否决权，引入不同于传统比例性的保证。我们为每个视角形式化了公理并考察了它们的一致性，通过适应Phragmén规则、Proportional Approval Voting（PAV）规则以及等份额法来实现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Consider the decision-making setting where agents elect a panel by expressingboth positive and negative preferences. Prominently, in constitutional AI,citizens democratically select a slate of ethical preferences on which afoundation model is to be trained. There, in practice, agents may both approveand disapprove of different ethical principles. Proportionality has beenwell-studied in computational social choice for approval ballots, but itsmeaning remains unclear when negative sentiments are also considered. In thiswork, we propose two conceptually distinct approaches to interpretproportionality in the presence of up and down votes. The first approach treatsthe satisfaction from electing candidates and the impact of vetoing them ascomparable, leading to combined proportionality guarantees. The second approachconsiders veto power separately, introducing guarantees distinct fromtraditional proportionality. We formalize axioms for each perspective andexamine their satisfiability by suitable adaptations of Phragm\'en's rule,Proportional Approval Voting rule and the Method of Equal Shares.</description>
      <author>example@mail.com (Sonja Kraiczy, Georgios Papasotiropoulos, Grzegorz Pierczyński, Piotr Skowron)</author>
      <guid isPermaLink="false">2503.01985v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Evolving High-Quality Rendering and Reconstruction in a Unified Framework with Contribution-Adaptive Regularization</title>
      <link>http://arxiv.org/abs/2503.00881v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了CarGS，一种统一模型框架，用于同时实现高质量渲染和表面重建。&lt;h4&gt;背景&lt;/h4&gt;三维场景表示是从多视角图像中生成的核心挑战之一，在计算机视觉和图形学领域具有重要意义。最近出现的3D高斯点云（3DGaussian Splatting, 3DGS）因能够提供高品质渲染且推理速度较快而备受关注，但其在精确几何重建方面仍存在困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法解决现有技术中渲染和重构之间的内在冲突以及计算密集型和存储成本高的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了CarGS模型，通过贡献自适应正则化实现高质量的渲染与表面重建。该框架学习高斯原语（Gaussian primitives）的自适应贡献，并将几何正则化的知识整合到紧凑多层感知器中。此外，还引入了一种基于几何引导的密集化策略，利用法线和符号距离字段来捕捉高频细节。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在渲染保真度与重建准确性方面达到了最先进的性能，同时保证了实时速度和最小存储空间需求。&lt;h4&gt;结论&lt;/h4&gt;CarGS模型通过贡献自适应正则化解决了3D高斯点云在几何精确性上的难题，并且其统一结构不需要像双模态方法那样使用多个独立的模型，从而保证了效率。&lt;h4&gt;翻译&lt;/h4&gt;表示三维场景是从多视角图像中生成的核心挑战之一，在计算机视觉和图形学领域具有重要意义。最近出现的方法如3D高斯点云(3DGaussian Splatting, 3DGS)虽然能够提供高质量渲染且推理速度较快，但在精确几何重建方面仍存在困难。为解决这一问题，提出了CarGS模型，通过贡献自适应正则化实现高质量的渲染与表面重建，并且在保持实时性能的同时实现了最小化的存储需求和最佳的几何保真度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Representing 3D scenes from multiview images is a core challenge in computervision and graphics, which requires both precise rendering and accuratereconstruction. Recently, 3D Gaussian Splatting (3DGS) has garnered significantattention for its high-quality rendering and fast inference speed. Yet, due tothe unstructured and irregular nature of Gaussian point clouds, ensuringaccurate geometry reconstruction remains difficult. Existing methods primarilyfocus on geometry regularization, with common approaches includingprimitive-based and dual-model frameworks. However, the former suffers frominherent conflicts between rendering and reconstruction, while the latter iscomputationally and storage-intensive. To address these challenges, we proposeCarGS, a unified model leveraging Contribution-adaptive regularization toachieve simultaneous, high-quality rendering and surface reconstruction. Theessence of our framework is learning adaptive contribution for Gaussianprimitives by squeezing the knowledge from geometry regularization into acompact MLP. Additionally, we introduce a geometry-guided densificationstrategy with clues from both normals and Signed Distance Fields (SDF) toimprove the capability of capturing high-frequency details. Our design improvesthe mutual learning of the two tasks, meanwhile its unified structure does notrequire separate models as in dual-model based approaches, guaranteeingefficiency. Extensive experiments demonstrate the ability to achievestate-of-the-art (SOTA) results in both rendering fidelity and reconstructionaccuracy while maintaining real-time speed and minimal storage size.</description>
      <author>example@mail.com (You Shen, Zhipeng Zhang, Xinyang Li, Yansong Qu, Yu Lin, Shengchuan Zhang, Liujuan Cao)</author>
      <guid isPermaLink="false">2503.00881v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Channel-Attentive Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2503.00578v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published as a conference paper at IEEE International Conference on  Data Mining 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;图神经网络（GNNs）在处理图结构数据的表示学习中处于领先地位，被广泛应用于在线社交网络和复杂分子等各个领域。大多数GNN采用消息传递机制，并且在各种任务上表现优异。&lt;h4&gt;背景&lt;/h4&gt;尽管大多数GNN通过消息传递实现了出色性能，但随着模型深度增加，普遍存在过度平滑的问题，导致节点表示之间的相似性增加，进而影响了GNN的表现。&lt;h4&gt;目的&lt;/h4&gt;提出一种自适应信道级消息传递方法以缓解过平滑问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一种名为Channel-Attentive GNN的模型，该模型能够学习如何关注邻近节点及其特征通道，在进行消息传递时可以交换更多种类的信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的模型比基准模型更能抵抗过度平滑，并且在各种具有强烈异构性的图上实现了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;通过改进GNN的消息传递机制，该研究提出了一种能够有效缓解过平滑问题的方法，并展示了其对复杂图形数据表示的优越性。&lt;h4&gt;翻译&lt;/h4&gt;Graph Neural Networks (GNNs) set the state-of-the-art in representation learning for graph-structured data. They are used in many domains, from online social networks to complex molecules. Most GNNs leverage the message-passing paradigm and achieve strong performances on various tasks. However, the message-passing mechanism used in most models suffers from over-smoothing as a GNN's depth increases. The over-smoothing degrades GNN's performance due to the increased similarity between the representations of unrelated nodes. This study proposes an adaptive channel-wise message-passing approach to alleviate the over-smoothing. The proposed model, Channel-Attentive GNN, learns how to attend to neighboring nodes and their feature channels. Thus, much diverse information can be transferred between nodes during message-passing. Experiments with widely used benchmark datasets show that the proposed model is more resistant to over-smoothing than baselines and achieves state-of-the-art performances for various graphs with strong heterophily.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ICDM59182.2024.00084&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/allab-boun/chat-gnn&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) set the state-of-the-art in representationlearning for graph-structured data. They are used in many domains, from onlinesocial networks to complex molecules. Most GNNs leverage the message-passingparadigm and achieve strong performances on various tasks. However, themessage-passing mechanism used in most models suffers from over-smoothing as aGNN's depth increases. The over-smoothing degrades GNN's performance due to theincreased similarity between the representations of unrelated nodes. This studyproposes an adaptive channel-wise message-passing approach to alleviate theover-smoothing. The proposed model, Channel-Attentive GNN, learns how to attendto neighboring nodes and their feature channels. Thus, much diverse informationcan be transferred between nodes during message-passing. Experiments withwidely used benchmark datasets show that the proposed model is more resistantto over-smoothing than baselines and achieves state-of-the-art performances forvarious graphs with strong heterophily. Our code is athttps://github.com/ALLab-Boun/CHAT-GNN.</description>
      <author>example@mail.com (Tuğrul Hasan Karabulut, İnci M. Baytaş)</author>
      <guid isPermaLink="false">2503.00578v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Analyzing the Safety of Japanese Large Language Models in Stereotype-Triggering Prompts</title>
      <link>http://arxiv.org/abs/2503.01947v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been submitted to IEEE Transactions on Artificial  Intelligence for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;近年来，大型语言模型由于其显著的潜力吸引了越来越多的关注，尽管人们对由此产生的不安全行为引发了担忧。这些不安全行为源自固有的刻板印象和偏见。&lt;h4&gt;背景&lt;/h4&gt;大多数关于LLM（大型语言模型）中的刻板印象的研究主要依赖于间接评估方法，在这种方法中，让模型在与特定社会群体相关的成对句子之间进行选择。最近，直接评估方法出现，这些方法通过检查开放式的模型响应来克服以前方法的局限性，例如标注者偏见。&lt;h4&gt;目的&lt;/h4&gt;这项研究旨在探讨日语LLM（大型语言模型）在应对触发刻板印象提示时的安全性，并填补非英语尤其是日语模型研究领域的空白。&lt;h4&gt;方法&lt;/h4&gt;构建了3,612个触发刻板印象的提示，这些提示由301个社会群体术语和12种类型化模板组合而成。从三种不同语言基础训练的语言模型（日语、英语和中文）中分析响应。&lt;h4&gt;主要发现&lt;/h4&gt;日本本土模型LLM-jp在拒绝率最低的同时更可能生成有毒或负面的回应；提示格式对所有模型输出有显著影响，反应通常包含针对特定社会群体夸张化的内容且因模型而异。这些发现揭示了日语LLM中伦理安全机制的不足，并证明即使高准确性的模型也能在处理日语文本时产生偏见。&lt;h4&gt;结论&lt;/h4&gt;研究呼吁改进日语LLM中的安全措施和减少偏见策略，以促进AI伦理讨论超越语言边界，对于提升全球范围内的AI安全性具有重要意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, Large Language Models have attracted growing interest fortheir significant potential, though concerns have rapidly emerged regardingunsafe behaviors stemming from inherent stereotypes and biases. Most researchon stereotypes in LLMs has primarily relied on indirect evaluation setups, inwhich models are prompted to select between pairs of sentences associated withparticular social groups. Recently, direct evaluation methods have emerged,examining open-ended model responses to overcome limitations of previousapproaches, such as annotator biases. Most existing studies have focused onEnglish-centric LLMs, whereas research on non-English models, particularlyJapanese, remains sparse, despite the growing development and adoption of thesemodels. This study examines the safety of Japanese LLMs when responding tostereotype-triggering prompts in direct setups. We constructed 3,612 prompts bycombining 301 social group terms, categorized by age, gender, and otherattributes, with 12 stereotype-inducing templates in Japanese. Responses wereanalyzed from three foundational models trained respectively on Japanese,English, and Chinese language. Our findings reveal that LLM-jp, a Japanesenative model, exhibits the lowest refusal rate and is more likely to generatetoxic and negative responses compared to other models. Additionally, promptformat significantly influence the output of all models, and the generatedresponses include exaggerated reactions toward specific social groups, varyingacross models. These findings underscore the insufficient ethical safetymechanisms in Japanese LLMs and demonstrate that even high-accuracy models canproduce biased outputs when processing Japanese-language prompts. We advocatefor improving safety mechanisms and bias mitigation strategies in JapaneseLLMs, contributing to ongoing discussions on AI ethics beyond linguisticboundaries.</description>
      <author>example@mail.com (Akito Nakanishi, Yukie Sano, Geng Liu, Francesco Pierri)</author>
      <guid isPermaLink="false">2503.01947v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>PSRGS:Progressive Spectral Residual of 3D Gaussian for High-Frequency Recovery</title>
      <link>http://arxiv.org/abs/2503.00848v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为PSRGS的渐进优化方案，解决了3D Gaussian Splatting在大规模遥感场景中遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的3D Gaussian Splatting方法在处理小规模单一物体场景时效果显著，但在处理大规模遥感数据时，由于点云稀疏和过度平滑问题而出现性能下降。&lt;h4&gt;目的&lt;/h4&gt;解决3D Gaussian Splatting应用于大规模场景中的过度重建、错误几何位置导致的密度化以及梯度伪影等问题。&lt;h4&gt;方法&lt;/h4&gt;通过创建光谱残差显著图分离低频与高频区域，采用深度感知和平滑损失初始化低频区域，并利用更高阈值的梯度特征分裂和克隆椭圆体以优化高频细节。&lt;h4&gt;主要发现&lt;/h4&gt;提出的PSRGS方案在多个数据集上的实验结果表明，在恢复高频率纹理细节方面具有竞争性的渲染质量。&lt;h4&gt;结论&lt;/h4&gt;通过分阶段地应用不同的优化策略，能够有效地提高大规模场景的3D Gaussian Splatting性能。&lt;h4&gt;翻译&lt;/h4&gt;论文提出了针对大规模遥感场景中使用3D Gaussian Splatting遇到的问题（如点云稀疏和过度平滑），提出了一种基于光谱残差图进行渐进式优化的新方法PSRGS，成功提升了处理效果及细节恢复能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting (3D GS) achieves impressive results in novel viewsynthesis for small, single-object scenes through Gaussian ellipsoidinitialization and adaptive density control. However, when applied tolarge-scale remote sensing scenes, 3D GS faces challenges: the point cloudsgenerated by Structure-from-Motion (SfM) are often sparse, and the inherentsmoothing behavior of 3D GS leads to over-reconstruction in high-frequencyregions, where have detailed textures and color variations. This results in thegeneration of large, opaque Gaussian ellipsoids that cause gradient artifacts.Moreover, the simultaneous optimization of both geometry and texture may leadto densification of Gaussian ellipsoids at incorrect geometric locations,resulting in artifacts in other views. To address these issues, we proposePSRGS, a progressive optimization scheme based on spectral residual maps.Specifically, we create a spectral residual significance map to separatelow-frequency and high-frequency regions. In the low-frequency region, we applydepth-aware and depth-smooth losses to initialize the scene geometry with lowthreshold. For the high-frequency region, we use gradient features with higherthreshold to split and clone ellipsoids, refining the scene. The sampling rateis determined by feature responses and gradient loss. Finally, we introduce apre-trained network that jointly computes perceptual loss from multiple views,ensuring accurate restoration of high-frequency details in both Gaussianellipsoids geometry and color. We conduct experiments on multiple datasets toassess the effectiveness of our method, which demonstrates competitiverendering quality, especially in recovering texture details in high-frequencyregions.</description>
      <author>example@mail.com (BoCheng Li, WenJuan Zhang, Bing Zhang, YiLing Yao, YaNing Wang)</author>
      <guid isPermaLink="false">2503.00848v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Language Model Mapping in Multimodal Music Learning: A Grand Challenge Proposal</title>
      <link>http://arxiv.org/abs/2503.00427v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，在深度神经网络的帮助下，表示学习和语言模型取得了显著的成功。很多研究旨在通过词汇或嵌入级别上的对齐和映射来构建不同模式之间的内在联系。&lt;h4&gt;问题&lt;/h4&gt;然而，大多数方法非常依赖于大量数据的输入，这在音乐等领域表现不佳，因为这些领域中的成对数据较少。&lt;h4&gt;目的&lt;/h4&gt;作者认为，嵌入对齐仅是跨模态对齐的表面层次。本文提出了一种新的挑战“语言模型映射（LMM）”，即如何将一种模式下的语言模型的本质映射到另一种模式下的语言模型中，前提是假设不同模式的语言模型都在追踪相同的基本现象。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了一个关于LMM的基础设置，并强调了其目标是揭示跨模态对齐的深层次方面以及实现更高效的样本学习。然后探讨了音乐领域为何成为进行LMM研究的理想选择。&lt;h4&gt;进一步讨论&lt;/h4&gt;接着，将音乐中的LMM与一个更为广泛且具有挑战性的科学问题联系起来——即“基于感官输入和抽象符号的学习如何采取行动”。最后提出了一种先进版本的挑战性问题设置。&lt;h4&gt;结论&lt;/h4&gt;本文提出的语言模型映射概念为跨模态对齐研究提供了一个新的视角，尤其适用于数据稀缺或难以获取成对数据的情况。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We have seen remarkable success in representation learning and languagemodels (LMs) using deep neural networks. Many studies aim to build theunderlying connections among different modalities via the alignment andmappings at the token or embedding level, but so far, most methods are verydata-hungry, limiting their performance in domains such as music where paireddata are less abundant. We argue that the embedding alignment is only at thesurface level of multimodal alignment. In this paper, we propose a grandchallenge of \textit{language model mapping} (LMM), i.e., how to map theessence implied in the LM of one domain to the LM of another domain under theassumption that LMs of different modalities are tracking the same underlyingphenomena. We first introduce a basic setup of LMM, highlighting the goal tounveil a deeper aspect of cross-modal alignment as well as to achieve moresample-efficiency learning. We then discuss why music is an ideal domain inwhich to conduct LMM research. After that, we connect LMM in music with a moregeneral and challenging scientific problem of \textit{learning to take actionsbased on both sensory input and abstract symbols}, and in the end, present anadvanced version of the challenge problem setup.</description>
      <author>example@mail.com (Daniel Chin, Gus Xia)</author>
      <guid isPermaLink="false">2503.00427v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Open-source framework for detecting bias and overfitting for large pathology images</title>
      <link>http://arxiv.org/abs/2503.01827v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种通用且模型无关的框架，用于调试深度学习模型中的捷径问题。&lt;h4&gt;背景信息&lt;/h4&gt;大型训练数据集也可能导致模型过度拟合和偏差，特别是非相关数据模式如背景颜色或色彩强度成为捷径。&lt;h4&gt;研究目的&lt;/h4&gt;开发一种能够检测并移除这些捷径的方法，以确保深度学习应用的稳健性。&lt;h4&gt;方法介绍&lt;/h4&gt;提出了一种无需针对特定模型架构定制的、通用且模型无关的调试框架。该框架特别适用于大规模图像数据处理领域（如病理学），并且能够在配备普通GPU的工作站上运行。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够复制在先前研究中发现的自监督学习模型中的非图片捷径，并且还识别出基础模型中存在的潜在捷径。&lt;h4&gt;结论&lt;/h4&gt;易于使用的测试有助于开发更可靠、准确和泛化的用于WSI分析的模型。此框架作为一个开源工具，可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;即使在使用数十亿数据样本进行训练的基础模型中也可能发展出导致过度拟合和偏差的捷径问题。这些捷径是非相关的数据模式，例如背景颜色或色彩强度等。为了确保深度学习应用的稳健性，需要检测并移除此类捷径的方法。当前的模型调试方法耗时较长，并且通常需要针对特定领域的给定模型架构进行定制化调整。我们提出了一种通用、模型无关的框架来调试深度学习模型，特别是在病理学领域，该领域涉及非常大的图像和庞大的计算资源需求。我们的框架能够在配备普通GPU的工作站上运行。我们展示了该框架可以复制先前工作中发现的自监督学习模型中的非图片捷径，并且还识别出基础模型中存在的潜在捷径。这些易用性测试有助于开发更可靠、准确和泛化的用于WSI分析的深度学习模型。我们的框架作为一个开源工具，可在GitHub上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Even foundational models that are trained on datasets with billions of datasamples may develop shortcuts that lead to overfitting and bias. Shortcuts arenon-relevant patterns in data, such as the background color or color intensity.So, to ensure the robustness of deep learning applications, there is a need formethods to detect and remove such shortcuts. Today's model debugging methodsare time consuming since they often require customization to fit for a givenmodel architecture in a specific domain. We propose a generalized,model-agnostic framework to debug deep learning models. We focus on the domainof histopathology, which has very large images that require large models - andtherefore large computation resources. It can be run on a workstation with acommodity GPU. We demonstrate that our framework can replicate non-imageshortcuts that have been found in previous work for self-supervised learningmodels, and we also identify possible shortcuts in a foundation model. Our easyto use tests contribute to the development of more reliable, accurate, andgeneralizable models for WSI analysis. Our framework is available as anopen-source tool available on github.</description>
      <author>example@mail.com (Anders Sildnes, Nikita Shvetsov, Masoud Tafavvoghi, Vi Ngoc-Nha Tran, Kajsa Møllersen, Lill-Tove Rasmussen Busund, Thomas K. Kilvær, Lars Ailo Bongo)</author>
      <guid isPermaLink="false">2503.01827v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>HiMo: High-Speed Objects Motion Compensation in Point Clouds</title>
      <link>http://arxiv.org/abs/2503.00803v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;LiDAR点云数据常常包含由于运动引起的失真，这会降低捕获数据中物体外观的准确性。&lt;h4&gt;问题&lt;/h4&gt;当前的研究主要关注于通过自车运动来处理点云失真，但忽视了其他移动对象造成的失真。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的去扭曲流水线（HiMo），用于解决由车辆及周围环境中的动态对象引起的点云失真。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于场景流估计的方法来补偿物体运动，并扩展了一个最先进的自我监督场景流方法。&lt;h4&gt;性能评估&lt;/h4&gt;鉴于现有文献中缺乏成熟可靠的运动失真度量标准，论文提出了两个新的评价指标：点级补偿精度和对象形状相似性。&lt;h4&gt;实验数据&lt;/h4&gt;在Argoverse 2数据集以及新收集的用于展示方法有效性的实际道路场景数据集上进行了广泛的实验。新数据集来源于装备有多个LiDAR的重型车辆，在高速公路上行驶，不同于现有数据集中主要的城市环境设置。&lt;h4&gt;翻译&lt;/h4&gt;本文首先描述了点云失真的根本原因，并展示了这些失真在公共数据集中的存在。这种失真在高速公路等高速环境中更加明显，也出现在多LiDAR配置中——这是重型车辆的常见设置。之前的大多数研究只关注于通过自车运动来处理点云失真，但忽视了其他移动对象造成的失真。因此，我们提出了一种新的去扭曲流水线（HiMo），该流水线利用场景流估计来进行物体运动补偿，从而纠正动态对象的表现。此外，还对一种最先进的自我监督场景流方法进行了扩展。由于文献中缺乏成熟的点云失真度量标准，本文提出了两个评估指标：点级补偿精度和形状相似性以评价去扭曲的性能。为了证明所提出方法的有效性，在Argoverse 2数据集以及新收集的数据集上进行了广泛的实验，该数据集来源于装备有多个LiDAR的重型车辆，在高速公路上行驶，不同于现有数据集中主要的城市环境设置。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR point clouds often contain motion-induced distortions, degrading theaccuracy of object appearances in the captured data. In this paper, we firstcharacterize the underlying reasons for the point cloud distortion and showthat this is present in public datasets. We find that this distortion is morepronounced in high-speed environments such as highways, as well as inmulti-LiDAR configurations, a common setup for heavy vehicles. Previous workhas dealt with point cloud distortion from the ego-motion but fails to considerdistortion from the motion of other objects. We therefore introduce a novelundistortion pipeline, HiMo, that leverages scene flow estimation for objectmotion compensation, correcting the depiction of dynamic objects. We furtherpropose an extension of a state-of-the-art self-supervised scene flow method.Due to the lack of well-established motion distortion metrics in theliterature, we also propose two metrics for compensation performanceevaluation: compensation accuracy at a point level and shape similarity onobjects. To demonstrate the efficacy of our method, we conduct extensiveexperiments on the Argoverse 2 dataset and a new real-world dataset. Our newdataset is collected from heavy vehicles equipped with multi-LiDARs and onhighways as opposed to mostly urban settings in the existing datasets. Thesource code, including all methods and the evaluation data, will be providedupon publication. See https://kin-zhang.github.io/HiMo for more details.</description>
      <author>example@mail.com (Qingwen Zhang, Ajinkya Khoche, Yi Yang, Li Ling, Sina Sharif Mansouri, Olov Andersson, Patric Jensfelt)</author>
      <guid isPermaLink="false">2503.00803v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Theoretical Insights in Model Inversion Robustness and Conditional Entropy Maximization for Collaborative Inference Systems</title>
      <link>http://arxiv.org/abs/2503.00383v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文研究了基于混淆的方法在保护中间特征隐私方面存在的不足，并提出了一种新的方法来量化冗余并增强模型对抗逆向工程攻击的能力。&lt;h4&gt;背景&lt;/h4&gt;协作推理使终端用户能够利用强大的深度学习模型而不暴露敏感的原始数据给云服务器。然而，最近的研究表明，中间特征可能不足以保证隐私，因为通过逆向建模攻击可以泄露信息和重建原始数据。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提出一种方法来量化冗余，并建立它与增强逆向工程抵抗能力之间的数学关系。&lt;h4&gt;方法&lt;/h4&gt;论文证明了输入给定中间特征的条件熵提供了在任何逆向建模攻击下的重构均方误差（MSE）的一个保证下界。然后，基于高斯混合估计提出了一个可微分且可解的方法来界定这个条件熵，并提出了一种条件熵最大化（CEM）算法以增强逆向工程抵抗能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了所提出的CEM在四个数据集上的有效性和适应性。将CEM嵌入到基于混淆的防御机制中，在不牺牲特征实用性和计算效率的情况下，可以显著提升其逆向工程抵抗能力，平均增益范围从12.9%至48.2%。&lt;h4&gt;结论&lt;/h4&gt;该研究通过理论分析和实验验证展示了如何利用条件熵最大化来增强模型对抗逆向建模攻击的能力。所提出的CEM方法为保护协作推理框架中的隐私提供了一个有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了研究背景、目的、方法及结果，揭示了一种新的技术——条件熵最大化（CEM），用于提高中间特征的抗逆向工程能力，从而进一步提升在深度学习模型中使用协作推理的安全性和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; By locally encoding raw data into intermediate features, collaborativeinference enables end users to leverage powerful deep learning models withoutexposure of sensitive raw data to cloud servers. However, recent studies haverevealed that these intermediate features may not sufficiently preserveprivacy, as information can be leaked and raw data can be reconstructed viamodel inversion attacks (MIAs). Obfuscation-based methods, such as noisecorruption, adversarial representation learning, and information filters,enhance the inversion robustness by obfuscating the task-irrelevant redundancyempirically. However, methods for quantifying such redundancy remain elusive,and the explicit mathematical relation between this redundancy minimization andinversion robustness enhancement has not yet been established. To address that,this work first theoretically proves that the conditional entropy of inputsgiven intermediate features provides a guaranteed lower bound on thereconstruction mean square error (MSE) under any MIA. Then, we derive adifferentiable and solvable measure for bounding this conditional entropy basedon the Gaussian mixture estimation and propose a conditional entropymaximization (CEM) algorithm to enhance the inversion robustness. Experimentalresults on four datasets demonstrate the effectiveness and adaptability of ourproposed CEM; without compromising feature utility and computing efficiency,plugging the proposed CEM into obfuscation-based defense mechanismsconsistently boosts their inversion robustness, achieving average gains rangingfrom 12.9\% to 48.2\%. Code is available at\href{https://github.com/xiasong0501/CEM}{https://github.com/xiasong0501/CEM}.</description>
      <author>example@mail.com (Song Xia, Yi Yu, Wenhan Yang, Meiwen Ding, Zhuo Chen, Lingyu Duan, Alex C. Kot, Xudong Jiang)</author>
      <guid isPermaLink="false">2503.00383v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SAKE: Steering Activations for Knowledge Editing</title>
      <link>http://arxiv.org/abs/2503.01751v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的知识编辑方法SAKE，该方法通过将需要修改的事实建模为分布而非单一提示来提高语言模型的知识更新效率和效果。&lt;h4&gt;背景&lt;/h4&gt;随着大型语言模型显示出记忆真实世界事实的能力，对这些模型进行可控且高效的知识更新变得越来越重要。然而，现有的知识编辑方法存在缺乏上下文鲁棒性等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的知识编辑方法SAKE，旨在解决现有知识编辑方法中的局限性和问题。&lt;h4&gt;方法&lt;/h4&gt;SAKE使用激活引导的方法，将要修改的事实建模为一个分布，并利用最优传输技术来调整大型语言模型的行为以应对整个事实相关的分布。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列数值实验，证实了SAKE在执行更稳健的知识编辑方面的有效性。它能够比现有的方法更好地处理上下文变化和逻辑推论问题。&lt;h4&gt;结论&lt;/h4&gt;SAKE是一种改进的知识编辑技术，能够在大型语言模型中实现更加有效和鲁棒的知识更新。&lt;h4&gt;翻译&lt;/h4&gt;随着大规模语言模型展现出记住现实世界事实的能力，有必要以一种受控且高效的方式对其进行知识更新。考虑到这些限制，知识编辑方法被提出用于修改预训练模型中的特定事实。然而，它们显示出缺乏上下文稳健性等若干局限，并且无法泛化到与事实相关的逻辑推论。为克服这些问题，我们提出了SAKE（基于激活引导的方法），该方法将要被修改的事实建模为一个分布而非单一提示。利用最优传输技术，SAKE能够调整大型语言模型在涉及整个事实相关分布时的行为，包括同义词和逻辑推论。通过多种数值实验表明了此方法的有效性：因此，与现有的方法相比，SAKE可以执行更稳健的知识编辑操作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As Large Langue Models have been shown to memorize real-world facts, the needto update this knowledge in a controlled and efficient manner arises. Designedwith these constraints in mind, Knowledge Editing (KE) approaches propose toalter specific facts in pretrained models. However, they have been shown tosuffer from several limitations, including their lack of contextual robustnessand their failure to generalize to logical implications related to the fact. Toovercome these issues, we propose SAKE, a steering activation method thatmodels a fact to be edited as a distribution rather than a single prompt.Leveraging Optimal Transport, SAKE alters the LLM behavior over a wholefact-related distribution, defined as paraphrases and logical implications.Several numerical experiments demonstrate the effectiveness of this method:SAKE is thus able to perform more robust edits than its existing counterparts.</description>
      <author>example@mail.com (Marco Scialanga, Thibault Laugel, Vincent Grari, Marcin Detyniecki)</author>
      <guid isPermaLink="false">2503.01751v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>STAR-Edge: Structure-aware Local Spherical Curve Representation for Thin-walled Edge Extraction from Unstructured Point Clouds</title>
      <link>http://arxiv.org/abs/2503.00801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;STAR-Edge是一种用于检测和细化薄壁结构中边缘点的新颖方法，通过独特的局部球面曲线表示创建结构感知的邻域。&lt;h4&gt;背景&lt;/h4&gt;从非结构化点云中提取几何边缘仍然是一个重大挑战，尤其是在常见的日常物体中的薄壁结构。传统的几何方法和最近的学习基于的方法在处理这些结构时经常遇到困难，因为两者都严重依赖于局部点邻居提供的足够的上下文信息。然而，薄壁结构的3D测量数据通常缺乏可靠边沿抽取所需的精确、密集且规则的邻域采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法STAR-Edge，旨在检测和细化薄壁结构中的边缘点，并通过独特的表示方式提高其在噪声和稀疏或不规则采样下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;采用局部球面曲线表示创建关注结构的邻域，强调共面点同时减少来自附近非共平面表面的干扰。此表示被转换为旋转不变量描述符，并结合轻量级多层感知器用于边缘点分类。此外，利用局部球面曲线表示估计更精确的法线并向初步确定的边缘点引入优化函数进行投影。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明STAR-Edge在ABC数据集和薄壁结构特定的数据集中均优于现有的边缘检测方法，在各种挑战性条件下展示出更好的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;STAR-Edge提供了一种新的解决方案，能够在复杂且具有挑战性的环境中准确地提取薄壁结构中的几何边沿。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Extracting geometric edges from unstructured point clouds remains asignificant challenge, particularly in thin-walled structures that are commonlyfound in everyday objects. Traditional geometric methods and recentlearning-based approaches frequently struggle with these structures, as bothrely heavily on sufficient contextual information from local pointneighborhoods. However, 3D measurement data of thin-walled structures oftenlack the accurate, dense, and regular neighborhood sampling required forreliable edge extraction, resulting in degraded performance.  In this work, we introduce STAR-Edge, a novel approach designed for detectingand refining edge points in thin-walled structures. Our method leverages aunique representation-the local spherical curve-to create structure-awareneighborhoods that emphasize co-planar points while reducing interference fromclose-by, non-co-planar surfaces. This representation is transformed into arotation-invariant descriptor, which, combined with a lightweight multi-layerperceptron, enables robust edge point classification even in the presence ofnoise and sparse or irregular sampling. Besides, we also use the localspherical curve representation to estimate more precise normals and introducean optimization function to project initially identified edge points exactly onthe true edges. Experiments conducted on the ABC dataset and thin-walledstructure-specific datasets demonstrate that STAR-Edge outperforms existingedge detection methods, showcasing better robustness under various challengingconditions.</description>
      <author>example@mail.com (Zikuan Li, Honghua Chen, Yuecheng Wang, Sibo Wu, Mingqiang Wei, Jun Wang)</author>
      <guid isPermaLink="false">2503.00801v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ECG-EmotionNet: Nested Mixture of Expert (NMoE) Adaptation of ECG-Foundation Model for Driver Emotion Recognition</title>
      <link>http://arxiv.org/abs/2503.01750v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的架构ECG-EmotionNet，用于在动态驾驶环境中进行驾驶员情绪识别。&lt;h4&gt;背景&lt;/h4&gt;驾驶员情绪识别对自动驾驶系统中的人员自主互动和信任度提升至关重要。心电图(ECG)因其实时监测能力和适应复杂驾驶环境的能力而成为最佳选择之一。然而，现有的方法通常依赖于静态条件下的多通道ECG信号，限制了其在真实动态场景中的应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的架构ECG-EmotionNet，以提高驾驶员情绪识别的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;使用单通道心电图(ECG)信号构建一个基于最近引入的心电图基础模型(FM)的新架构。通过嵌入式专家混合(MoE)适应机制来增强全局和局部ECG特征表示，而不是采用传统的全微调、线性探测或低秩适应方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和F1得分上分别提高了6%和7%，并且保持了计算效率。此外，使用最近引入的具有挑战性的驾驶员情绪监测数据集进行了评估。&lt;h4&gt;结论&lt;/h4&gt;ECG-EmotionNet架构显著改善了动态驾驶环境下的心电图情绪识别性能，为自动驾驶系统中的人机互动提供了坚实的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文提及司机情感识别在监控系统中的重要性，并介绍了新的架构ECG-EmotionNet及其使用单通道心电图信号来增强驾驶员情绪监测的准确性和鲁棒性的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Driver emotion recognition plays a crucial role in driver monitoring systems,enhancing human-autonomy interactions and the trustworthiness of AutonomousDriving (AD). Various physiological and behavioural modalities have beenexplored for this purpose, with Electrocardiogram (ECG) emerging as a standoutchoice for real-time emotion monitoring, particularly in dynamic andunpredictable driving conditions. Existing methods, however, often rely onmulti-channel ECG signals recorded under static conditions, limiting theirapplicability in real-world dynamic driving scenarios. To address thislimitation, the paper introduces ECG-EmotionNet, a novel architecture designedspecifically for emotion recognition in dynamic driving environments.ECG-EmotionNet is constructed by adapting a recently introduced ECG FoundationModel (FM) and uniquely employs single-channel ECG signals, ensuring bothrobust generalizability and computational efficiency. Unlike conventionaladaptation methods such as full fine-tuning, linear probing, or low-rankadaptation, we propose an intuitively pleasing alternative, referred to as thenested Mixture of Experts (MoE) adaptation. More precisely, each transformerlayer of the underlying FM is treated as a separate expert, with embeddingsextracted from these experts fused using trainable weights within a gatingmechanism. This approach enhances the representation of both global and localECG features, leading to a 6% improvement in accuracy and a 7% increase in theF1 score, all while maintaining computational efficiency. The effectiveness ofthe proposed ECG-EmotionNet architecture is evaluated using a recentlyintroduced and challenging driver emotion monitoring dataset.</description>
      <author>example@mail.com (Nastaran Mansourian, Arash Mohammadi, M. Omair Ahmad, M. N. S. Swamy)</author>
      <guid isPermaLink="false">2503.01750v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ICanC: Improving Camera-based Object Detection and Energy Consumption in Low-Illumination Environments</title>
      <link>http://arxiv.org/abs/2503.00709v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 18 figures, to be published in IEEE MOST 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种名为ICanC的新系统，旨在通过利用激光雷达和摄像头传感器的互补能力来增强自动驾驶汽车在低光照环境下的物体检测能力和能效。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶车辆在低光照环境下容易因相机性能下降而导致物体检测准确性降低，并且需要频繁使用前照灯以提高摄像机的可视性，这导致能源消耗增加。因此，有必要寻找一种既能确保可靠物体检测又能优化能耗的方法。&lt;h4&gt;目的&lt;/h4&gt;通过设计ICanC系统，在不牺牲可靠性的前提下减少不必要的头灯使用，从而实现更可持续的交通方式。&lt;h4&gt;方法&lt;/h4&gt;ICanC由三个主要部分组成：障碍物探测器、危险探测器和灯光控制器。其中，障碍物探测器处理激光雷达点云数据以拟合边界框并估计物体的位置、速度和方向；危险探测器评估潜在威胁；灯光控制器根据危险探测的结果动态开启前照灯。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在有显著噪声干扰的情况下，ICanC仍然表现出良好的性能，并且当开启头灯时能够实现基于摄像头的高精度物体检测。同时，整体能耗得到了显著降低。&lt;h4&gt;结论&lt;/h4&gt;作为自动驾驶汽车研究中的一个重要的进展，ICanC在确保可靠性和能效之间达到了一种平衡，展示了其在未来可持续交通中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种名为 ICanC（发音为“我能看见”）的新型系统，旨在通过结合激光雷达和相机传感器的优势来提高自主车辆在低光照条件下的物体检测能力，并优化能耗。此方法确保了在传统相机性能下降的情况下也能提升检测精度并大幅降低不必要的前照灯使用，从而支持可持续交通的目标。ICanC系统包括三个主要组成部分：障碍物探测器、危险探测器和灯光控制器。通过实验验证，在真实和模拟环境中，即使存在显著的噪声干扰，该系统依然保持了出色的性能，展示了在实现可靠物体检测的同时大幅减少前照灯能耗的巨大潜力，这是自动驾驶车辆研究领域的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces ICanC (pronounced "I Can See"), a novel system designedto enhance object detection and optimize energy efficiency in autonomousvehicles (AVs) operating in low-illumination environments. By leveraging thecomplementary capabilities of LiDAR and camera sensors, ICanC improvesdetection accuracy under conditions where camera performance typicallydeclines, while significantly reducing unnecessary headlight usage. Thisapproach aligns with the broader objective of promoting sustainabletransportation.  ICanC comprises three primary nodes: the Obstacle Detector, which processesLiDAR point cloud data to fit bounding boxes onto detected objects and estimatetheir position, velocity, and orientation; the Danger Detector, which evaluatespotential threats using the information provided by the Obstacle Detector; andthe Light Controller, which dynamically activates headlights to enhance cameravisibility solely when a threat is detected.  Experiments conducted in physical and simulated environments demonstrateICanC's robust performance, even in the presence of significant noiseinterference. The system consistently achieves high accuracy in camera-basedobject detection when headlights are engaged, while significantly reducingoverall headlight energy consumption. These results position ICanC as apromising advancement in autonomous vehicle research, achieving a balancebetween energy efficiency and reliable object detection.</description>
      <author>example@mail.com (Daniel Ma, Ren Zhong, Weisong Shi)</author>
      <guid isPermaLink="false">2503.00709v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Spark-TTS: An Efficient LLM-Based Text-to-Speech Model with Single-Stream Decoupled Speech Tokens</title>
      <link>http://arxiv.org/abs/2503.01710v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to ACL 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的大规模语言模型（LLMs）在零样本文本到语音（TTS）合成方面取得了显著进展。然而，现有的基础模型依赖于多阶段处理或复杂的架构来预测多个码本，这限制了效率和集成灵活性。&lt;h4&gt;背景&lt;/h4&gt;当前的零样本TTS系统主要面临两大挑战：一是复杂且低效的架构，二是受限的控制能力，难以实现精确的语言内容与说话人属性分离。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的局限性，本文介绍了Spark-TTS系统，旨在通过引入新的单流语音编解码器BiCodec来提升TTS模型的效率和灵活性，并支持精细的音调调整。&lt;h4&gt;方法&lt;/h4&gt;Spark-TTS基于BiCodec，该编码器将语音分解为两种互补的标记类型：用于语言内容的低比特率语义令牌和固定长度的全局令牌（代表说话人属性）。此外，该系统还采用了Qwen2.5 LLM以及一种称为“链式思维”（CoT）的生成方法。&lt;h4&gt;主要发现&lt;/h4&gt;Spark-TTS不仅实现了最先进的零样本语音克隆效果，还能生成高度可定制的声音，超越了基于参考的合成的限制。为促进可控性TTS研究，本文还引入了一个精心策划的数据集VoxBox，它包含10万小时录音及全面属性标注。&lt;h4&gt;结论&lt;/h4&gt;Spark-TTS展示了在效率、灵活性和控制精确度方面的显著改进，并通过详细的实验验证了其优越性能。&lt;h4&gt;翻译&lt;/h4&gt;近期大规模语言模型(LLMs)的重大进展促进了零样本文本到语音(TTS)合成技术的显著进步。然而，现有的基础模型依赖于多阶段处理或复杂架构来预测多个码本，从而限制了效率和集成灵活性。为解决这一问题，我们引入了Spark-TTS系统，该系统由BiCodec驱动，这是一种单流语音编解码器，将语音分解成两种互补的标记类型：用于语言内容的低比特率语义标记以及固定长度的全局标记(代表说话人属性)。结合Qwen2.5 LLM和链式思维(CoT)生成方法，Spark-TTS能够实现粗粒度控制（如性别、讲话风格）及细粒度调整（如精确音高值、讲话速率）。为了促进可控性TTS研究，我们引入了VoxBox数据集，这是一个精心策划的10万小时录音库，包含全面的属性标注。大量实验表明，Spark-TTS不仅在零样本语音克隆方面达到业界领先水平，并且生成的高度定制化声音超出了基于参考合成技术的限制。该系统的源代码、预训练模型和音频样本可从https://github.com/SparkAudio/Spark-TTS获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in large language models (LLMs) have driven significantprogress in zero-shot text-to-speech (TTS) synthesis. However, existingfoundation models rely on multi-stage processing or complex architectures forpredicting multiple codebooks, limiting efficiency and integration flexibility.To overcome these challenges, we introduce Spark-TTS, a novel system powered byBiCodec, a single-stream speech codec that decomposes speech into twocomplementary token types: low-bitrate semantic tokens for linguistic contentand fixed-length global tokens for speaker attributes. This disentangledrepresentation, combined with the Qwen2.5 LLM and a chain-of-thought (CoT)generation approach, enables both coarse-grained control (e.g., gender,speaking style) and fine-grained adjustments (e.g., precise pitch values,speaking rate). To facilitate research in controllable TTS, we introduceVoxBox, a meticulously curated 100,000-hour dataset with comprehensiveattribute annotations. Extensive experiments demonstrate that Spark-TTS notonly achieves state-of-the-art zero-shot voice cloning but also generateshighly customizable voices that surpass the limitations of reference-basedsynthesis. Source code, pre-trained models, and audio samples are available athttps://github.com/SparkAudio/Spark-TTS.</description>
      <author>example@mail.com (Xinsheng Wang, Mingqi Jiang, Ziyang Ma, Ziyu Zhang, Songxiang Liu, Linqin Li, Zheng Liang, Qixi Zheng, Rui Wang, Xiaoqin Feng, Weizhen Bian, Zhen Ye, Sitong Cheng, Ruibin Yuan, Zhixian Zhao, Xinfa Zhu, Jiahao Pan, Liumeng Xue, Pengcheng Zhu, Yunlin Chen, Zhifei Li, Xie Chen, Lei Xie, Yike Guo, Wei Xue)</author>
      <guid isPermaLink="false">2503.01710v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via Symbolic Code Generation</title>
      <link>http://arxiv.org/abs/2503.01700v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 7 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;最近的工作展示了大型语言模型在机器人任务和运动规划中的巨大潜力。然而，当前的方法没有充分利用这些模型的符号计算能力和代码生成能力。&lt;h4&gt;背景&lt;/h4&gt;现有的LLM方法通常产生包含子目标和行动计划的文本或代码推理链。对于涉及多重约束下的复杂优化问题的任务，纯文本推理显得不足。&lt;h4&gt;目的&lt;/h4&gt;为了提高LLM在任务和运动规划中的性能并增强其泛化性，我们提出了一种通过指导模型生成符号计算所需的代码来改进TAMP能力的方法。&lt;h4&gt;方法&lt;/h4&gt;与以往的工作不同，我们的工作使LLM生成用于解决、计划和验证的代码，同时利用文本推理融入常识。采用了多轮引导和答案进化框架以提升任务成功几率。&lt;h4&gt;主要发现&lt;/h4&gt;通过在七项典型任务上进行测试并与现有基准方法比较，提出的Code-as-Symbolic-Planner平均提高了24.1%的成功率，并且显示了在离散和连续环境、二维/三维模拟以及真实世界设置中的强效性和泛化性。&lt;h4&gt;结论&lt;/h4&gt;我们的研究证明了LLM生成代码作为符号规划器的可行性和有效性，这对机器人任务和运动规划领域具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究展示了大型语言模型在机器人任务与运动规划领域的潜力。当前的方法主要依赖于文本或代码推理链，但未充分使用语言模型的代码生成能力。我们提出了一种新的方法——Code-as-Symbolic-Planner，通过指导LLM生成解决和验证问题所需的代码来提高其性能，并且证明了该方法在多种任务中的有效性与泛化性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent works have shown great potentials of Large Language Models (LLMs) inrobot task and motion planning (TAMP). Current LLM approaches generate text- orcode-based reasoning chains with sub-goals and action plans. However, they donot fully leverage LLMs' symbolic computing and code generation capabilities.Many robot TAMP tasks involve complex optimization under multiple constraints,where pure textual reasoning is insufficient. While augmenting LLMs withpredefined solvers and planners improves performance, it lacks generalizationacross tasks. Given LLMs' growing coding proficiency, we enhance their TAMPcapabilities by steering them to generate code as symbolic planners foroptimization and constraint verification. Unlike prior work that uses code tointerface with robot action modules, we steer LLMs to generate code as solvers,planners, and checkers for TAMP tasks requiring symbolic computing, while stillleveraging textual reasoning to incorporate common sense. With a multi-roundguidance and answer evolution framework, the proposed Code-as-Symbolic-Plannerimproves success rates by average 24.1\% over best baseline methods acrossseven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner showsstrong effectiveness and generalizability across discrete and continuousenvironments, 2D/3D simulations and real-world settings, as well as single- andmulti-robot tasks with diverse requirements. See our project websitehttps://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, andcode.</description>
      <author>example@mail.com (Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan)</author>
      <guid isPermaLink="false">2503.01700v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning via Modality Alignment and Retention</title>
      <link>http://arxiv.org/abs/2503.00374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;病理学和转录组学在肿瘤学中是两个基本的研究手段，它们分别涵盖了疾病形态学和分子层面的信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态表示学习方法（MIRROR），旨在促进不同模式之间的对齐同时保留各自独特的特征。&lt;h4&gt;方法&lt;/h4&gt;该方法使用特定的编码器提取每个模式的全面特征，并通过模式对齐模块实现表型模式与分子谱图的无缝集成。此外，还包含一个模式保持模块来保护各个模式的独特属性，以及一个风格聚类模块来减少冗余并增强疾病相关信息。&lt;h4&gt;主要发现&lt;/h4&gt;在TCGA队列中的癌症亚型分类和生存分析中进行了广泛的评估，结果表明MIRROR方法具有优越的表现，在构建综合的肿瘤学特征表示方面效果显著。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了MIRROR能够在不同模式间进行有效的对齐和保持各自特有的结构，有助于改进癌症诊断的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经完成，并且转换为了JSON格式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-01&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/TianyiFranklinWang/MIRROR&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Histopathology and transcriptomics are fundamental modalities in oncology,encapsulating the morphological and molecular aspects of the disease.Multi-modal self-supervised learning has demonstrated remarkable potential inlearning pathological representations by integrating diverse data sources.Conventional multi-modal integration methods primarily emphasize modalityalignment, while paying insufficient attention to retaining themodality-specific structures. However, unlike conventional scenarios wheremulti-modal inputs share highly overlapping features, histopathology andtranscriptomics exhibit pronounced heterogeneity, offering orthogonal yetcomplementary insights. Histopathology provides morphological and spatialcontext, elucidating tissue architecture and cellular topology, whereastranscriptomics delineates molecular signatures through gene expressionpatterns. This inherent disparity introduces a major challenge in aligning themwhile maintaining modality-specific fidelity. To address these challenges, wepresent MIRROR, a novel multi-modal representation learning method designed tofoster both modality alignment and retention. MIRROR employs dedicated encodersto extract comprehensive features for each modality, which is furthercomplemented by a modality alignment module to achieve seamless integrationbetween phenotype patterns and molecular profiles. Furthermore, a modalityretention module safeguards unique attributes from each modality, while a styleclustering module mitigates redundancy and enhances disease-relevantinformation by modeling and aligning consistent pathological signatures withina clustering space. Extensive evaluations on TCGA cohorts for cancer subtypingand survival analysis highlight MIRROR's superior performance, demonstratingits effectiveness in constructing comprehensive oncological featurerepresentations and benefiting the cancer diagnosis.</description>
      <author>example@mail.com (Tianyi Wang, Jianan Fan, Dingxin Zhang, Dongnan Liu, Yong Xia, Heng Huang, Weidong Cai)</author>
      <guid isPermaLink="false">2503.00374v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MUSt3R: Multi-view Network for Stereo 3D Reconstruction</title>
      <link>http://arxiv.org/abs/2503.01661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了MUSt3R，它是DUSt3R的多视图扩展版本，用于解决大规模图像集合中密集且无约束的立体3D重建问题。&lt;h4&gt;背景&lt;/h4&gt;现有方法如DUSt3R在处理任意图像集合进行立体3D重建时效果良好，但当面对大量图像时，由于需要处理成对图像的数量呈二次增长，导致计算复杂度急剧增加。这限制了其在大规模数据集上的应用和优化效率。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多视图网络MUSt3R来解决DUSt3R中存在的问题，旨在提升大规模图像集合的立体3D重建性能，并保持高帧率下的计算效率。&lt;h4&gt;方法&lt;/h4&gt;通过将DUSt3R架构对称化并扩展到直接预测所有视角在共同坐标系中的3D结构，同时引入多层次内存机制以降低复杂度。该模型能够处理在线和离线场景中的SfM（Simultaneous Localization and Mapping）以及视觉SLAM问题。&lt;h4&gt;主要发现&lt;/h4&gt;MUSt3R通过有效减少计算量并提高重建效率，在各种下游任务中显示出超越现有方法的性能，包括未校准视觉里程计、相对相机姿态估计、尺度和焦距估算等。&lt;h4&gt;结论&lt;/h4&gt;所提出的MUSt3R架构解决了DUSt3R在处理大规模图像集合时存在的局限性，并为实时高性能立体3D重建提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到，DUSt3R提出了一种新的几何计算机视觉范式，即可以对任意图像集进行密集和无约束的立体3D重建。然而，在内部处理成对图像并回归局部3D重构时会产生计算复杂度问题，尤其在大量图片集合的情况下。本文介绍了一种基于多视图网络（MUSt3R）的方法，解决了上述所有问题，并展示了其在视觉里程计、相对相机姿态估计等方面的优越性能和效率提升能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DUSt3R introduced a novel paradigm in geometric computer vision by proposinga model that can provide dense and unconstrained Stereo 3D Reconstruction ofarbitrary image collections with no prior information about camera calibrationnor viewpoint poses. Under the hood, however, DUSt3R processes image pairs,regressing local 3D reconstructions that need to be aligned in a globalcoordinate system. The number of pairs, growing quadratically, is an inherentlimitation that becomes especially concerning for robust and fast optimizationin the case of large image collections. In this paper, we propose an extensionof DUSt3R from pairs to multiple views, that addresses all aforementionedconcerns. Indeed, we propose a Multi-view Network for Stereo 3D Reconstruction,or MUSt3R, that modifies the DUSt3R architecture by making it symmetric andextending it to directly predict 3D structure for all views in a commoncoordinate frame. Second, we entail the model with a multi-layer memorymechanism which allows to reduce the computational complexity and to scale thereconstruction to large collections, inferring thousands of 3D pointmaps athigh frame-rates with limited added complexity. The framework is designed toperform 3D reconstruction both offline and online, and hence can be seamlesslyapplied to SfM and visual SLAM scenarios showing state-of-the-art performanceon various 3D downstream tasks, including uncalibrated Visual Odometry,relative camera pose, scale and focal estimation, 3D reconstruction andmulti-view depth estimation.</description>
      <author>example@mail.com (Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, Vincent Leroy)</author>
      <guid isPermaLink="false">2503.01661v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>ecg2o: A Seamless Extension of g2o for Equality-Constrained Factor Graph Optimization</title>
      <link>http://arxiv.org/abs/2503.01311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新颖的方法，通过扩展因子图来无缝地集成等式约束，从而提高了解决方案的精度，并拓宽了其在最优控制领域的应用。&lt;h4&gt;背景&lt;/h4&gt;因子图优化作为一种基础框架用于机器人感知领域，可以应用于姿态估计、同时定位与地图构建（SLAM）、结构从运动恢复（SfM）和态势感知。传统的方法使用如高斯-牛顿或莱文贝格-马夸特等算法解决无约束最小二乘问题。&lt;h4&gt;目的&lt;/h4&gt;在不增加额外优化算法的情况下，引入一种能够直接支持等式约束的因子图扩展方法。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的因子图扩展方式，该方法保持了现有二次优化技术的效率和灵活性，并确保了约束可行性。使用ecg2o库验证此方法的有效性，该库是基于广泛使用的g2o因子图库开发的，并为等式受限优化提供了全面支持。&lt;h4&gt;主要发现&lt;/h4&gt;在自主车辆速度跟踪的最优控制问题中应用该方法后，与当前最先进的约束处理技术相比，本研究提出的解决方案表现出更高的精确度和可靠性。&lt;h4&gt;结论&lt;/h4&gt;通过扩展g2o因子图库以支持等式约束，并且提供了开源示例代码和用于验证的新颖优化算法，证明了这种新方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：因子图优化是机器人感知领域的一个基础框架，可以应用于姿态估计、同时定位与地图构建（SLAM）、结构从运动恢复（SfM）及态势感知。传统的方法使用如高斯-牛顿或莱文贝格-马夸特等算法解决无约束最小二乘问题。然而，通过原生支持等式约束来扩展因子图可以提高解决方案的准确性并拓宽其应用范围，尤其是在最优控制领域中。本文提出了一种新颖的因子图扩展方法，在不使用额外优化算法的情况下直接集成等式约束。这种方法保持了现有二次优化技术的效率和灵活性，并确保了约束可行性。为了验证该方法的有效性，将其应用于自主车辆速度跟踪的最佳控制问题，并将结果与当前最先进的约束处理技术进行了比较。此外，还介绍了ecg2o库——一个头文件形式的C++库，它扩展了广泛使用的g2o因子图库，增加了对等式受限优化的支持。此库、示例代码以及最优控制问题可以在https://github.com/snt-arg/ecg2o上作为开源资源获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Factor graph optimization serves as a fundamental framework for roboticperception, enabling applications such as pose estimation, simultaneouslocalization and mapping (SLAM), structure-from-motion (SfM), and situationalawareness. Traditionally, these methods solve unconstrained least squaresproblems using algorithms such as Gauss-Newton and Levenberg-Marquardt.However, extending factor graphs with native support for equality constraintscan improve solution accuracy and broaden their applicability, particularly inoptimal control. In this paper, we propose a novel extension of factor graphsthat seamlessly incorporates equality constraints without requiring additionaloptimization algorithms. Our approach maintains the efficiency and flexibilityof existing second-order optimization techniques while ensuring constraintfeasibility. To validate our method, we apply it to an optimal control problemfor velocity tracking in autonomous vehicles and benchmark our results againststate-of-the-art constraint handling techniques. Additionally, we introduceecg2o, a header-only C++ library that extends the widely used g2o factor graphlibrary by adding full support for equality-constrained optimization. Thislibrary, along with demonstrative examples and the optimal control problem, isavailable as open source at https://github.com/snt-arg/ecg2o</description>
      <author>example@mail.com (Anas Abdelkarim, Holger Voos, Daniel Görges)</author>
      <guid isPermaLink="false">2503.01311v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in Large-Scale UAV Mapping</title>
      <link>http://arxiv.org/abs/2503.01202v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种使用多传感器无人机系统的新型大范围正射影像快速生成技术，该系统集成了GPS、IMU、毫米波雷达和相机。通过利用这些数据源，可以提高传统正射影像生成方法在时间性能、系统鲁棒性和地理参考准确性方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;基于无人机(UAV)的大规模正射影像的快速生成一直是航空摄影领域的研究重点。&lt;h4&gt;目的&lt;/h4&gt;克服现有技术中的瓶颈问题，提出一种利用多传感器数据来提高传统正射影像生成方法的技术方案。&lt;h4&gt;方法&lt;/h4&gt;引入了一种先姿态优化特征匹配的方法以提升匹配速度和准确性，并减少所需的特征数量。这种方法特别适用于低纹理场景（如农田），在此类场景中常规的特征匹配困难。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在准确性和时间效率方面都表现出色，特别是在农田检测与管理方面的应用效果良好。&lt;h4&gt;结论&lt;/h4&gt;所提出的无人机系统能够有效支持农田的识别和管理工作，并且证明了其在低纹理环境中生成高精度正射影像的能力。&lt;h4&gt;翻译&lt;/h4&gt;快速生成基于无人飞行器(UAV)的大规模正射影像一直是航空摄影领域研究的重点。本文提出了一种结合GPS、IMU、毫米波雷达和相机等多传感器数据的无人机系统解决方案，以克服传统正射影像生成方法在时间性能、系统鲁棒性和地理参考准确性方面的局限性。通过先姿态优化特征匹配的方法增强了匹配速度和精度，并减少了所需的特征数量，为结构从运动(SfM)过程提供了精确的参照依据。该技术特别适用于低纹理场景（如农田），这些场景中常规的特征匹配通常比较困难。实验结果显示，我们的方法能够在短时间内实现高准确度的正射影像生成，证明了其在有效支持农田识别和管理工作中的实际应用价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles(UAVs) has been a long-standing focus of research in the field of aerialmapping. A multi-sensor UAV system, integrating the Global Positioning System(GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera,can provide an effective solution to this problem. In this paper, we utilizemulti-sensor data to overcome the limitations of conventional orthoimagegeneration methods in terms of temporal performance, system robustness, andgeographic reference accuracy. A prior-pose-optimized feature matching methodis introduced to enhance matching speed and accuracy, reducing the number ofrequired features and providing precise references for the Structure fromMotion (SfM) process. The proposed method exhibits robustness in low-texturescenes like farmlands, where feature matching is difficult. Experiments showthat our approach achieves accurate feature matching orthoimage generation in ashort time. The proposed drone system effectively aids in farmland detectionand management.</description>
      <author>example@mail.com (Jialei He, Zhihao Zhan, Zhituo Tu, Xiang Zhu, Jie Yuan)</author>
      <guid isPermaLink="false">2503.01202v3</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>MTReD: 3D Reconstruction Dataset for Fly-over Videos of Maritime Domain</title>
      <link>http://arxiv.org/abs/2503.00853v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  WACV Workshop 2025 - 3rd Workshop on Maritime Computer Vision  (MaCVI2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的海洋3D场景重建基准数据集MTReD，旨在解决视频航拍视角下的海上三维场景重建问题，并提供了初步的评估方法。&lt;h4&gt;背景&lt;/h4&gt;当前没有专门针对海上环境的3D场景重建的数据集。传统的感知度量标准如LPIPS无法准确衡量重建图像的完整性。&lt;h4&gt;目的&lt;/h4&gt;创建一个新颖的海洋3D场景重建基准数据集MTReD，以促进这一领域的研究和开发。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新的语义相似性度量DiFPS来评估重建的质量。使用了两种基线模型进行初步评估，并探索了一些预处理方法来提高结果。&lt;h4&gt;主要发现&lt;/h4&gt;MASt3R模型在感知度量上有更好的表现，但在重投影误差上不如SfM模型。通过适当的预处理技术可以同时改进这两种评价指标。&lt;h4&gt;结论&lt;/h4&gt;希望MTReD数据集能够推动未来在这个方向上的研究，并鼓励更多的研究人员参与到这个领域中来。&lt;h4&gt;翻译&lt;/h4&gt;这项工作解决了海上领域的视频航拍视角下的3D场景重建问题，重点在于几何一致性与视觉完整性。这将使下游任务如分割、导航和定位成为可能。目前没有专门为此领域的数据集存在。因此，我们提出了一个新的海洋3D场景重建基准测试数据集MTReD（Maritime Three-Dimensional Reconstruction Dataset）。该数据集包含19段从互联网收集的视频片段，其中包括船舰、岛屿以及海岸线等元素。由于任务目标在于几何一致性与视觉完整性，该数据集采用两种度量标准：重投影误差和感知度量。我们发现现有的基于感知度量的方法如LPIPS并不适合衡量重建图像的整体性，因此提出了一种新的利用DINOv2特征的语义相似度度量DiFPS（DinoV2 Features Perception Similarity）。我们在两个基线模型上进行了初步评估：通过Colmap实现的结构从运动（SfM）以及最近的研究前沿MASt3R模型。结果表明，相比于SfM，基于MASt3R重建出的场景在重投影误差更高但感知度量得分更好。因此我们探索了一些预处理方法，并发现了一种能够同时提高重投影误差和感知度量得分的方法。我们认为MTReD数据集将促进该领域的进一步研究发展。所有数据集及代码将在https://github.com/RuiYiYong/MTReD公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work tackles 3D scene reconstruction for a video fly-over perspectiveproblem in the maritime domain, with a specific emphasis on geometrically andvisually sound reconstructions. This will allow for downstream tasks such assegmentation, navigation, and localization. To our knowledge, there is nodataset available in this domain. As such, we propose a novel maritime 3D scenereconstruction benchmarking dataset, named as MTReD (Maritime Three-DimensionalReconstruction Dataset). The MTReD comprises 19 fly-over videos curated fromthe Internet containing ships, islands, and coastlines. As the task is aimedtowards geometrical consistency and visual completeness, the dataset uses twometrics: (1) Reprojection error; and (2) Perception based metrics. We find thatexisting perception-based metrics, such as Learned Perceptual Image PatchSimilarity (LPIPS), do not appropriately measure the completeness of areconstructed image. Thus, we propose a novel semantic similarity metricutilizing DINOv2 features coined DiFPS (DinoV2 Features Perception Similarity).We perform initial evaluation on two baselines: (1) Structured from Motion(SfM) through Colmap; and (2) the recent state-of-the-art MASt3R model. We findthat the reconstructed scenes by MASt3R have higher reprojection errors, butsuperior perception based metric scores. To this end, some pre-processingmethods are explored, and we find a pre-processing method which improves boththe reprojection error and perception-based score. We envisage our proposedMTReD to stimulate further research in these directions. The dataset and allthe code will be made available in https://github.com/RuiYiYong/MTReD.</description>
      <author>example@mail.com (Rui Yi Yong, Samuel Picosson, Arnold Wiliem)</author>
      <guid isPermaLink="false">2503.00853v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature Extraction</title>
      <link>http://arxiv.org/abs/2503.03734v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;OTTER是一种新颖的Vision-Language-Action (VLA) 模型，它通过显式的、文本感知的视觉特征提取方法来利用现有预训练模型中视觉和语言之间的语义对齐。&lt;h4&gt;背景&lt;/h4&gt;现有的VLA模型在预测基于视觉观察和语言指令的机器人动作时需要微调预训练的视觉-语言模型，并且由于视觉和语言特性独立地输入下游策略，这会破坏预先训练好的语义对齐。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来利用现有预训练模型中的语义对齐，同时保持预训练编码器不变。&lt;h4&gt;方法&lt;/h4&gt;OTTER选择性提取并传递与语言指令语义一致的任务相关视觉特征到策略变换器中。这使得可以在不调整预训练的视觉-语言编码器的情况下进行操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过在仿真和真实世界实验中的表现，证明了OTTER能够显著优于现有VLA模型，并且具有强大的零样本泛化能力，可以推广到新颖的对象和环境中。&lt;h4&gt;结论&lt;/h4&gt;OTTER通过保持预训练视觉-语言编码器的冻结状态，保留并利用大型数据集预训练中学习的丰富语义理解，使得在新对象和环境中的零样本泛化成为可能。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language-Action (VLA) 模型旨在根据视觉观察和语言指令预测机器人动作。现有方法要求微调预训练的视觉语言模型（VLM），因为独立处理的视觉和语言特征会被输入到下游策略中，从而破坏了预训练中的语义对齐。我们提出了OTTER，这是一种新的VLA架构，通过显式的、文本感知的视觉特性提取来利用这些现有的对齐关系。与加工所有视觉特征相反，OTTER选择性地抽取并传递任务相关的且与语言指令语义一致的视觉特征到策略变换器中。这使得OTTER能够保持预训练视觉-语言编码器处于冻结状态。因此，OTTER保留和利用了大规模预训练中学到的丰富语义理解能力，从而具备强大的零样本泛化能力。在仿真和真实世界实验中，OTTER显著优于现有的VLA模型，在新对象和环境中的零样本泛化效果出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language-Action (VLA) models aim to predict robotic actions based onvisual observations and language instructions. Existing approaches requirefine-tuning pre-trained visionlanguage models (VLMs) as visual and languagefeatures are independently fed into downstream policies, degrading thepre-trained semantic alignments. We propose OTTER, a novel VLA architecturethat leverages these existing alignments through explicit, text-aware visualfeature extraction. Instead of processing all visual features, OTTERselectively extracts and passes only task-relevant visual features that aresemantically aligned with the language instruction to the policy transformer.This allows OTTER to keep the pre-trained vision-language encoders frozen.Thereby, OTTER preserves and utilizes the rich semantic understanding learnedfrom large-scale pre-training, enabling strong zero-shot generalizationcapabilities. In simulation and real-world experiments, OTTER significantlyoutperforms existing VLA models, demonstrating strong zeroshot generalizationto novel objects and environments. Video, code, checkpoints, and dataset:https://ottervla.github.io/.</description>
      <author>example@mail.com (Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel)</author>
      <guid isPermaLink="false">2503.03734v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Revolutionizing Traffic Management with AI-Powered Machine Vision: A Step Toward Smart Cities</title>
      <link>http://arxiv.org/abs/2503.02967v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 1 figure, 2 tables, accepted to 1th AITC conference in  University Of Isfahan&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了人工智能和机器视觉技术在交通系统中的应用，通过高级监控摄像头和深度学习算法实现车辆实时检测、交通异常及驾驶行为识别。该系统整合地理空间数据和天气信息以适应不同的环境条件。&lt;h4&gt;背景&lt;/h4&gt;城市化进程加快和车辆拥堵问题对交通安全管理和效率提出了重大挑战。&lt;h4&gt;目的&lt;/h4&gt;探讨AI与机器视觉技术在革命性变革交通系统的潜力，优化交通流并提高道路安全。&lt;h4&gt;方法&lt;/h4&gt;利用YOLOv8和YOLOv11模型进行高精度的车辆检测及异常识别。结合地理空间信息和天气数据来增强系统适应性和性能。&lt;h4&gt;主要发现&lt;/h4&gt;研究实现了高度准确的车辆检测和异常认知，提高了交通流畅度与安全性。&lt;h4&gt;结论&lt;/h4&gt;研究成果为智能交通管理解决方案的发展提供了支持，并推动了智慧城市建设的目标——实现可持续且高效的都市基础设施。&lt;h4&gt;翻译&lt;/h4&gt;随着城市化的加速及汽车拥堵问题加剧，对交通安全管理和效率提出了重大挑战。这项研究探讨了人工智能和机器视觉技术在交通系统中的革命性潜力，利用先进的监控摄像头和深度学习算法提出了一套车辆实时检测、交通异常以及驾驶行为识别的解决方案。该方案结合地理空间数据和天气信息以动态适应不同环境条件，确保各种场景下的稳健表现。通过使用YOLOv8及YOLOv11模型，研究达到了高精度的车辆与异常探测水平，优化了交通流，并提升了道路安全性。这些发现有助于智能交通管理解决方案的发展，并符合创建智慧城市的愿景——建立具有可持续性和高效性的城市基础设施。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid urbanization of cities and increasing vehicular congestion haveposed significant challenges to traffic management and safety. This studyexplores the transformative potential of artificial intelligence (AI) andmachine vision technologies in revolutionizing traffic systems. By leveragingadvanced surveillance cameras and deep learning algorithms, this researchproposes a system for real-time detection of vehicles, traffic anomalies, anddriver behaviors. The system integrates geospatial and weather data to adaptdynamically to environmental conditions, ensuring robust performance in diversescenarios. Using YOLOv8 and YOLOv11 models, the study achieves high accuracy invehicle detection and anomaly recognition, optimizing traffic flow andenhancing road safety. These findings contribute to the development ofintelligent traffic management solutions and align with the vision of creatingsmart cities with sustainable and efficient urban infrastructure.</description>
      <author>example@mail.com (Seyed Hossein Hosseini DolatAbadi, Sayyed Mohammad Hossein Hashemi, Mohammad Hosseini, Moein-Aldin AliHosseini)</author>
      <guid isPermaLink="false">2503.02967v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</title>
      <link>http://arxiv.org/abs/2503.03726v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于RGB图像估计无纹理物体6D姿态的主动感知框架。&lt;h4&gt;背景&lt;/h4&gt;单视图6D姿态估计在处理具有外观模糊性、旋转对称性和严重遮挡的对象时存在局限性，需要研究多视角姿态估计和最佳下视点预测方法来克服这些问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用RGB图像进行无纹理物体6D姿态估计的综合主动感知框架。&lt;h4&gt;方法&lt;/h4&gt;通过将6D姿态估计分解为两个步骤的过程：首先估算每个对象的3D平移，解决RGB图像中的尺度和深度模糊问题；然后使用简化后的任务来确定3D方向。引入了预测最佳下视角以捕捉RGB图像的策略，从而减少物体姿势不确定性并提高精度。&lt;h4&gt;主要发现&lt;/h4&gt;在公共ROBI数据集以及自建透明对象数据集中进行了实验验证，多视图姿态估计方法的表现优于现有的先进方法；通过利用最佳下视点策略，该方法能够在比启发式策略少得多的视角中实现高精度物体姿势准确性。&lt;h4&gt;结论&lt;/h4&gt;所提出的框架为解决无纹理物体6D姿态估计问题提供了一种有效的方法，能够显著提高姿态估算的准确性和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating the 6D pose of textureless objects from RBG images is an importantproblem in robotics. Due to appearance ambiguities, rotational symmetries, andsevere occlusions, single-view based 6D pose estimators are still unable tohandle a wide range of objects, motivating research towards multi-view poseestimation and next-best-view prediction that addresses these limitations. Inthis work, we propose a comprehensive active perception framework forestimating the 6D poses of textureless objects using only RGB images. Ourapproach is built upon a key idea: decoupling the 6D pose estimation into asequential two-step process can greatly improve both accuracy and efficiency.First, we estimate the 3D translation of each object, resolving scale and depthambiguities inherent to RGB images. These estimates are then used to simplifythe subsequent task of determining the 3D orientation, which we achieve throughcanonical scale template matching. Building on this formulation, we thenintroduce an active perception strategy that predicts the next best cameraviewpoint to capture an RGB image, effectively reducing object pose uncertaintyand enhancing pose accuracy. We evaluate our method on the public ROBI datasetas well as on a transparent object dataset that we created. When evaluatedusing the same camera viewpoints, our multi-view pose estimation significantlyoutperforms state-of-the-art approaches. Furthermore, by leveraging ournext-best-view strategy, our method achieves high object pose accuracy withsubstantially fewer viewpoints than heuristic-based policies.</description>
      <author>example@mail.com (Jun Yang, Wenjie Xue, Sahar Ghavidel, Steven L. Waslander)</author>
      <guid isPermaLink="false">2503.03726v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Cali Anything: Dense Feature Multi-Frame Structure-from-Motion for Large-Scale Camera Array Calibration</title>
      <link>http://arxiv.org/abs/2503.00737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于密集特征的多帧校准方法，这种方法可以从场景数据中直接优化相机内部参数，从而消除了对额外校准拍摄的需求。该方法增强了传统的结构从运动（SfM）流程，并通过引入外参正则化项、密集特征重投影项和内参方差项来进行多帧联合优化。&lt;h4&gt;背景&lt;/h4&gt;大规模摄像机阵列的标定是耗时的过程，通常需要专门捕捉已知图案进行。虽然这些设置中的外部参数由于物理结构固定不变，但内部参数在不同会话中可能因镜头调整或温度变化等因素而发生变化。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外校准拍摄就能优化大规模摄像机阵列内部参数的方法，并提高3D重建的准确性。&lt;h4&gt;方法&lt;/h4&gt;该方法通过引入外参正则化项、密集特征重投影项和内参方差项来增强传统的SfM流程，实现多帧联合优化。这种方法可以直接从场景数据中优化相机内部参数，无需额外的校准拍摄。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法在精确度上接近于专门的校准过程，并且显著提高了相机内参和3D重建的准确性。&lt;h4&gt;结论&lt;/h4&gt;该方法与现有的SfM流程完全兼容，为大规模摄像机设置提供了一种高效实用的即插即用解决方案。代码可在https://github.com/YJJfish/Multi-Cali-Anything公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/yjjfish/multi-cali-anything&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Calibrating large-scale camera arrays, such as those in dome-based setups, istime-intensive and typically requires dedicated captures of known patterns.While extrinsics in such arrays are fixed due to the physical setup, intrinsicsoften vary across sessions due to factors like lens adjustments or temperaturechanges. In this paper, we propose a dense-feature-driven multi-framecalibration method that refines intrinsics directly from scene data,eliminating the necessity for additional calibration captures. Our approachenhances traditional Structure-from-Motion (SfM) pipelines by introducing anextrinsics regularization term to progressively align estimated extrinsics withground-truth values, a dense feature reprojection term to reduce keypointerrors by minimizing reprojection loss in the feature space, and an intrinsicsvariance term for joint optimization across multiple frames. Experiments on theMultiface dataset show that our method achieves nearly the same precision asdedicated calibration processes, and significantly enhances intrinsics and 3Dreconstruction accuracy. Fully compatible with existing SfM pipelines, ourmethod provides an efficient and practical plug-and-play solution forlarge-scale camera setups. Our code is publicly available at:https://github.com/YJJfish/Multi-Cali-Anything</description>
      <author>example@mail.com (Jinjiang You, Hewei Wang, Yijie Li, Mingxiao Huo, Long Van Tran Ha, Mingyuan Ma, Jinfeng Xu, Puzhen Wu, Shubham Garg, Wei Pu)</author>
      <guid isPermaLink="false">2503.00737v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Efficient End-to-end Visual Localization for Autonomous Driving with Decoupled BEV Neural Matching</title>
      <link>http://arxiv.org/abs/2503.00862v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文提出了一种端到端的定位神经网络，直接从周围图像中估计车辆姿态，而无需显式地将感知结果与高精度地图匹配。通过减少采样空间来确保效率和可解释性，并在实验中展示了其厘米级定位能力。&lt;h4&gt;背景&lt;/h4&gt;精确的定位对于高级自动驾驶系统至关重要。传统基于地图匹配的方法容易受感知噪声影响，需要昂贵的超参数调整。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端定位神经网络方法，能够直接从图像估计车辆姿态，同时保持计算效率和可解释性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个解耦的BEV（鸟瞰图）神经匹配模块，用于差分采样基于匹配的姿态估算，并通过解耦每个自由度对特征表示的影响来大幅减少采样空间。&lt;h4&gt;主要发现&lt;/h4&gt;该网络在纵向位置、横向位置和偏航角上的平均绝对误差分别为0.19米、0.13米和0.39度，同时推理内存使用量减少了68.8%。&lt;h4&gt;结论&lt;/h4&gt;新的定位方法能够实现高精度的车辆姿态估计，并且具有低资源消耗的特点。&lt;h4&gt;翻译&lt;/h4&gt;准确的定位在高级自动驾驶系统中起着重要作用。传统的基于地图匹配的方法通过显式地将映射元素与传感器观测结果进行匹配来解决姿态问题，这种方法通常对感知噪声敏感，因此需要昂贵的超参数调整。在这篇论文中，我们提出了一种端到端的定位神经网络，直接从周围的图像估计车辆姿态，而不必显式地将感知结果与高精度地图进行匹配。为了保证效率和可解释性，提出了一个解耦的基于BEV（鸟瞰图）神经匹配的姿态求解器，在一个差分采样基础上的匹配模块中估计姿态。此外，通过解耦每个自由度对特征表示的影响大大减少了采样空间。实验结果表明，所提出的网络能够进行厘米级定位，纵向位置、横向位置和偏航角上的平均绝对误差分别为0.19米、0.13米和0.39度，同时推理内存使用量降低了68.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-02&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization plays an important role in high-level autonomousdriving systems. Conventional map matching-based localization methods solve theposes by explicitly matching map elements with sensor observations, generallysensitive to perception noise, therefore requiring costly hyper-parametertuning. In this paper, we propose an end-to-end localization neural networkwhich directly estimates vehicle poses from surrounding images, withoutexplicitly matching perception results with HD maps. To ensure efficiency andinterpretability, a decoupled BEV neural matching-based pose solver isproposed, which estimates poses in a differentiable sampling-based matchingmodule. Moreover, the sampling space is hugely reduced by decoupling thefeature representation affected by each DoF of poses. The experimental resultsdemonstrate that the proposed network is capable of performing decimeter levellocalization with mean absolute errors of 0.19m, 0.13m and 0.39 degree inlongitudinal, lateral position and yaw angle while exhibiting a 68.8% reductionin inference memory usage.</description>
      <author>example@mail.com (Jinyu Miao, Tuopu Wen, Ziang Luo, Kangan Qian, Zheng Fu, Yunlong Wang, Kun Jiang, Mengmeng Yang, Jin Huang, Zhihua Zhong, Diange Yang)</author>
      <guid isPermaLink="false">2503.00862v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>DILEMMA: Joint LLM Quantization and Distributed LLM Inference Over Edge Computing Systems</title>
      <link>http://arxiv.org/abs/2503.01704v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DILEMMA是一个针对边缘计算环境中的大型语言模型部署挑战而设计的框架，通过联合优化层放置和量化策略来最小化推理延迟并保证性能。&lt;h4&gt;背景&lt;/h4&gt;随着大语言模型在智慧城市应用中越来越流行，如何在网络边缘有效利用这些资源成为一个关键问题。边缘计算可以降低通信延迟，但受制于有限的通信、计算和存储能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架，能够在满足大型语言模型性能要求的同时优化其部署到边缘计算环境中的方法。&lt;h4&gt;方法&lt;/h4&gt;DILEMMA通过整数线性规划问题来最小化总的推理延迟，并利用逐层量化和知识蒸馏技术控制LLM的性能。&lt;h4&gt;主要发现&lt;/h4&gt;在OPT-350模型上使用SQuAD数据集进行实验后，证明了DILEMMA能够在保持模型损失的同时实现高达12.75%的量化比率，显示其在资源受限环境中的有效性。&lt;h4&gt;结论&lt;/h4&gt;DILEMMA框架为大型语言模型部署到边缘计算系统提供了有效的解决方案，尤其是在需要降低通信延迟和优化资源使用的场景中。&lt;h4&gt;翻译&lt;/h4&gt;随着大语言模型被越来越多地应用于智慧城市的不同应用领域，有必要将这些模型推向网络的边缘，同时保持其性能。作为物理上更接近最终用户的计算资源，边缘计算可以减少为依赖大型语言模型的服务提供服务时的通信延迟。然而，边缘服务器在通信、计算和存储容量方面的能力有限。本文介绍了一种名为DILEMMA的新框架，该框架通过联合优化层放置和量化策略来解决将大语言模型部署到边缘系统中的挑战。DILEMMA通过整数线性规划问题最小化总的推理延迟并确保可接受的大语言模型性能水平，并利用逐层量化和知识蒸馏技术控制LLM的性能。使用SQuAD数据集对OPT-350模型进行实验评估表明，DILEMMA能够在保持模型损失的同时实现高达12.75%的量化比率，展示了其在资源受限环境中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-03&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With a recent trend of using Large Language Models (LLMs) for differentapplications within smart cities, there is a need for pushing these modelstoward the edge of network while still preserving their performance. EdgeComputing (EC) as a physically closer computing resource to the end users canhelp to reduce the communication delay for serving end users' tasks forLLM-dependent services. However, EC servers have limited capacity in terms ofcommunication, computation, and storage capacity. This paper introducesDILEMMA, a novel framework addressing the challenges of deploying LLMs in ECsystems by jointly optimizing layer placement and layer quantization in ECsystems. DILEMMA formulates an Integer Linear Programming problem to minimizetotal inference delay while ensuring acceptable LLM performance levels,leveraging layer-wise quantization and knowledge distillation for LLMperformance control. Experimental evaluations on OPT-350 model using the SQuADdataset demonstrate that DILEMMA achieves a quantization ratio of up to 12.75%while preserving model loss, highlighting its effectiveness inresource-constrained environments.</description>
      <author>example@mail.com (Minoo Hosseinzadeh, Hana Khamfroush)</author>
      <guid isPermaLink="false">2503.01704v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Curating Demonstrations using Online Experience</title>
      <link>http://arxiv.org/abs/2503.03707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种机器人自我整理的方法，称为Demo-SCORE，通过使用在线机器人经验训练分类器来识别成功的策略执行，并过滤掉异质性演示数据集中的次优示范。&lt;h4&gt;背景&lt;/h4&gt;许多机器人的演示数据集中包含不同质量和可靠性的多样性示例。这种多样性可能有助于策略的预训练，但也可能导致最终模仿学习目标时性能下降。&lt;h4&gt;目的&lt;/h4&gt;目的是通过机器人自我整理来提高基于这些多样演示数据集训练的策略在测试中的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为Demo-SCORE的方法，该方法使用在线机器人经验训练分类器以区分成功的和不成功的策略执行，并利用此分类器过滤掉异质性示例数据集中次优的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与所有原始演示数据集一起训练的基准政策相比，应用了Demo-SCORE方法后生成的策略可以实现高达15%至35%更高的绝对成功率。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了一种有效的方法来识别和过滤次优示范，从而改善从异质性示例中学习出的机器人策略性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many robot demonstration datasets contain heterogeneous demonstrations ofvarying quality. This heterogeneity may benefit policy pre-training, but canhinder robot performance when used with a final imitation learning objective.In particular, some strategies in the data may be less reliable than others ormay be underrepresented in the data, leading to poor performance when suchstrategies are sampled at test time. Moreover, such unreliable orunderrepresented strategies can be difficult even for people to discern, andsifting through demonstration datasets is time-consuming and costly. On theother hand, policy performance when trained on such demonstrations can reflectthe reliability of different strategies. We thus propose for robots toself-curate based on online robot experience (Demo-SCORE). More specifically,we train and cross-validate a classifier to discern successful policy roll-outsfrom unsuccessful ones and use the classifier to filter heterogeneousdemonstration datasets. Our experiments in simulation and the real world showthat Demo-SCORE can effectively identify suboptimal demonstrations withoutmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolutesuccess rate in the resulting policy compared to the base policy trained withall original demonstrations.</description>
      <author>example@mail.com (Annie S. Chen, Alec M. Lessing, Yuejiang Liu, Chelsea Finn)</author>
      <guid isPermaLink="false">2503.03707v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>EVLoc: Event-based Visual Localization in LiDAR Maps via Event-Depth Registration</title>
      <link>http://arxiv.org/abs/2503.00167v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文探讨了事件相机在利用现有激光雷达地图进行定位中的应用潜力，提出了一种基于粗略初始姿态细化的框架。&lt;h4&gt;背景信息&lt;/h4&gt;事件相机具备高动态范围和低延迟特性，在高速运动和极端光照条件下具有优势。现有的激光雷达地图可以用于导航和移动操作的应用中。&lt;h4&gt;研究目的&lt;/h4&gt;探索事件相机在现有LiDAR地图中的定位能力，以实现精准导航和机器人移动抓取任务。&lt;h4&gt;提出方法&lt;/h4&gt;{'第一步': '基于粗略初始姿态，将LiDAR点投影到2D空间中生成深度图。', '第二步': '利用光学流估计网络对事件与LiDAR点进行二维空间的配准。', '第三步': '使用PnP求解器来估计相机的姿态。', '改进措施': '开发了一种新的基于帧的事件表示，以增强几何一致性，并预测辅助变量作为正则化项，以减少地面实况姿态偏差对网络收敛性的影响。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明所提方法在公共数据集上具有有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了一种新颖的事件相机和激光雷达融合定位框架，并在网上发布了代码与预训练模型，以促进未来的相关研究。&lt;h4&gt;翻译&lt;/h4&gt;本文探讨了生物启发式的事件摄像机在现有LiDAR地图中进行定位的应用潜力。我们提出了一套基于粗略初始姿态细化的方法来实现这一目标，包括将LiDAR点投影到2D空间生成深度图、利用光学流估计网络对齐事件和LiDAR点以及使用PnP求解器估算相机姿势等步骤，并且开发了新的帧级事件表示方法以提高几何一致性。实验结果表明该方法在多个公开数据集上均有效，研究还开放了代码和预训练模型供后续研究使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Event cameras are bio-inspired sensors with some notable features, includinghigh dynamic range and low latency, which makes them exceptionally suitable forperception in challenging scenarios such as high-speed motion and extremelighting conditions. In this paper, we explore their potential for localizationwithin pre-existing LiDAR maps, a critical task for applications that requireprecise navigation and mobile manipulation. Our framework follows a paradigmbased on the refinement of an initial pose. Specifically, we first projectLiDAR points into 2D space based on a rough initial pose to obtain depth maps,and then employ an optical flow estimation network to align events with LiDARpoints in 2D space, followed by camera pose estimation using a PnP solver. Toenhance geometric consistency between these two inherently differentmodalities, we develop a novel frame-based event representation that improvesstructural clarity. Additionally, given the varying degrees of bias observed inthe ground truth poses, we design a module that predicts an auxiliary variableas a regularization term to mitigate the impact of this bias on networkconvergence. Experimental results on several public datasets demonstrate theeffectiveness of our proposed method. To facilitate future research, both thecode and the pre-trained models are made available online.</description>
      <author>example@mail.com (Kuangyi Chen, Jun Zhang, Friedrich Fraundorfer)</author>
      <guid isPermaLink="false">2503.00167v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Negative Damping Control for User-Dependent Multi-Terrain Walking Assistance with a Hip Exoskeleton</title>
      <link>http://arxiv.org/abs/2503.03662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的髋部外骨骼控制策略，通过适应性虚拟负阻尼设计来调整机械阻抗，使系统能够注入能量同时确保用户保持对运动的自愿贡献。&lt;h4&gt;背景&lt;/h4&gt;当前的辅助策略在应对个体行走模式和多变地形环境方面灵活性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够根据个体需求灵活调节、适应不同地形环境的新控制方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一种基于虚拟负阻尼来调整机械系统的阻抗，通过实验验证其减少代谢成本的效果，并采用贝叶斯优化技术实现无缝的辅助强度调整和跨多种地形环境下的过渡。&lt;h4&gt;主要发现&lt;/h4&gt;该控制器在五名健康受试者中实现了平均7.2%的步行代谢成本降低；同时保持了下肢运动学特性，确保在整个步态周期中的功率损失小于总功率的2%，并且实现了与用户动作同步的最佳协调。&lt;h4&gt;结论&lt;/h4&gt;提出的方法展示了个性化的、适应性强且操作简单的髋部外骨骼控制器设计，推动了适用性更强、依赖于用户的控制法则的发展。&lt;h4&gt;翻译&lt;/h4&gt;现有的髋部外骨骼辅助策略在应对不同的行走模式和地形时表现不佳。本文介绍了一种新型的机械阻抗调节方法，通过虚拟负阻尼来适应人体-机器系统，该方法能够向系统注入能量并保持用户主动参与运动的能力。实验表明，在五名受试者中，与自由行走相比，步行代谢成本平均降低了7.2%，且下肢运动学没有改变。同时实现了步态周期内极低的功率损失（小于总功率的2%），确保了人机动作的一致性。此外，使用贝叶斯优化适应辅助强度以实现跨多地形环境的无缝过渡和调整。该方法展示了适用于所有条件下的高效功率传输，并为个性化、可调且用户依赖性的控制法则的发展提供了可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hip exoskeletons are known for their versatility in assisting users acrossvaried scenarios. However, current assistive strategies often lack theflexibility to accommodate for individual walking patterns and adapt to diverselocomotion environments. In this work, we present a novel control strategy thatadapts the mechanical impedance of the human-exoskeleton system. We design thehip assistive torques as an adaptive virtual negative damping, which is able toinject energy into the system while allowing the users to remain in control andcontribute voluntarily to the movements. Experiments with five healthy subjectsdemonstrate that our controller reduces the metabolic cost of walking comparedto free walking (average reduction of 7.2%), and it preserves the lower-limbskinematics. Additionally, our method achieves minimal power losses from theexoskeleton across the entire gait cycle (less than 2% negative mechanicalpower out of the total power), ensuring synchronized action with the users'movements. Moreover, we use Bayesian Optimization to adapt the assistancestrength and allow for seamless adaptation and transitions across multi-terrainenvironments. Our strategy achieves efficient power transmission under allconditions. Our approach demonstrates an individualized, adaptable, andstraightforward controller for hip exoskeletons, advancing the development ofviable, adaptive, and user-dependent control laws.</description>
      <author>example@mail.com (Giulia Ramella, Auke Ijspeert, Mohamed Bouri)</author>
      <guid isPermaLink="false">2503.03662v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Motion Planning and Control with Unknown Nonlinear Dynamics through Predicted Reachability</title>
      <link>http://arxiv.org/abs/2503.03633v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;针对未知非线性动力学下的自主运动规划提出了一种混合规划-控制框架，旨在计算朝向目标的可行轨迹。&lt;h4&gt;背景&lt;/h4&gt;在未知非线性动态系统中进行自主运动规划具有重大挑战。为了引导系统的导航并适应环境变化，代理需要持续探索以获取关于可到达性的系统属性。&lt;h4&gt;目的&lt;/h4&gt;提出一种混合规划-控制框架，该框架能够计算朝向目标的可行轨迹，并通过抽象为有向加权图来处理未知非线性动力学问题。&lt;h4&gt;方法&lt;/h4&gt;- 将状态空间划分为多个区域并将其近似为分段仿射（PWA）系统。- 通过确定系统的边缘存在情况，逐步更新有向加权图。- 利用前馈可到达性条件和控制理论中的可达性理论来预测未知动力学的先验信息，并据此分配启发式权重。&lt;h4&gt;主要发现&lt;/h4&gt;- 基于图形搜索结果在线生成控制器并不断更新预测图能够提高导航效率，从而适应动态变化。- 在移动机器人在未探索地形中运行时，可以将未知的动力学抽象为一个积分器模型来简化处理。&lt;h4&gt;结论&lt;/h4&gt;该方法通过仿真场景验证了其有效性，并展示了如何利用混合规划-控制框架和分段仿射系统的特性来解决自主运动规划中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;自主运动规划在面对未知非线性动力学时面临重大挑战。为了引导系统导航并适应环境，代理需要持续探索以获取关于可到达性的系统属性等信息。本文提出了一种混合规划-控制框架，用于计算朝向目标的可行轨迹。该方法通过将状态空间划分为多个区域并将它们近似为分段仿射（PWA）系统，然后将其抽象成有向加权图，并根据未知动力学的信息逐步更新其边的存在情况来实现这一目的。我们还提出了一个框架，在任务执行期间适应性地收集和分析数据，不断更新预测图形，并基于搜索结果在线生成控制器。通过模拟移动机器人在未知地形中操作的场景（将其未知的动力学抽象为单个积分器模型），验证了这种方法的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous motion planning under unknown nonlinear dynamics presentssignificant challenges. An agent needs to continuously explore the systemdynamics to acquire its properties, such as reachability, in order to guidesystem navigation adaptively. In this paper, we propose a hybridplanning-control framework designed to compute a feasible trajectory toward atarget. Our approach involves partitioning the state space and approximatingthe system by a piecewise affine (PWA) system with constrained control inputs.By abstracting the PWA system into a directed weighted graph, we incrementallyupdate the existence of its edges via affine system identification and reachcontrol theory, introducing a predictive reachability condition by exploitingprior information of the unknown dynamics. Heuristic weights are assigned toedges based on whether their existence is certain or remains indeterminate.Consequently, we propose a framework that adaptively collects and analyzes dataduring mission execution, continually updates the predictive graph, andsynthesizes a controller online based on the graph search outcomes. Wedemonstrate the efficacy of our approach through simulation scenarios involvinga mobile robot operating in unknown terrains, with its unknown dynamicsabstracted as a single integrator model.</description>
      <author>example@mail.com (Zhiquan Zhang, Gokul Puthumanaillam, Manav Vora, Melkior Ornik)</author>
      <guid isPermaLink="false">2503.03633v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles through Generative Simulation</title>
      <link>http://arxiv.org/abs/2503.03629v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了TeraSim平台，这是一个用于自动驾驶车辆（AV）开发的开源、高保真交通模拟器。&lt;h4&gt;背景&lt;/h4&gt;交通仿真对于评估自动驾驶汽车的安全性至关重要。然而，传统的基于规则的方法难以捕捉复杂的驾驶员行为互动，而数据驱动方法则在保持长期行为真实性或生成安全相关的事件多样性方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出TeraSim平台以解决上述挑战，旨在揭示未知的不安全事件并高效估算AV统计性能指标，如碰撞率。&lt;h4&gt;方法&lt;/h4&gt;设计了一个开放源代码平台，可以无缝集成第三方物理仿真器和独立的自动驾驶车辆软件堆栈，构建完整的自动驾驶车辆模拟系统。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明TeraSim在生成涉及静态及动态代理的各种安全关键事件方面有效，并能识别出AV系统的隐藏缺陷，支持统计性能评估。&lt;h4&gt;结论&lt;/h4&gt;该平台作为实用性工具对研究者、开发者以及政策制定者都有重要的价值，有助于自动驾驶车辆的安全性评估。&lt;h4&gt;翻译&lt;/h4&gt;交通模拟对于自动驾驶汽车的发展至关重要，能够实现各种驾驶条件下的全面安全评估。然而，传统基于规则的模拟器难以捕捉复杂的人类互动行为，而数据驱动的方法往往在长期的行为真实性保持或生成多样的重要安全性事件方面存在不足。为此，提出了一种名为TeraSim的开源高保真交通仿真平台，旨在揭示未知的安全隐患，并高效地估计自动驾驶汽车如碰撞率等统计性能指标。该平台设计用于与第三方物理模拟器和独立的自动驾驶汽车软件堆栈无缝集成，构建完整的自动驾驶车辆仿真系统。实验结果显示其在生成涉及静态及动态代理的各种重要安全事件方面有效，并能识别出AV系统的隐藏缺陷，支持进行统计性能评估。这些发现强调了TeraSim作为实用性工具对研究者、开发者以及政策制定者的潜在价值，可用于自动驾驶汽车的安全性评估。代码可在https://github.com/mcity/TeraSim获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic simulation is essential for autonomous vehicle (AV) development,enabling comprehensive safety evaluation across diverse driving conditions.However, traditional rule-based simulators struggle to capture complex humaninteractions, while data-driven approaches often fail to maintain long-termbehavioral realism or generate diverse safety-critical events. To address thesechallenges, we propose TeraSim, an open-source, high-fidelity trafficsimulation platform designed to uncover unknown unsafe events and efficientlyestimate AV statistical performance metrics, such as crash rates. TeraSim isdesigned for seamless integration with third-party physics simulators andstandalone AV stacks, to construct a complete AV simulation system.Experimental results demonstrate its effectiveness in generating diversesafety-critical events involving both static and dynamic agents, identifyinghidden deficiencies in AV systems, and enabling statistical performanceevaluation. These findings highlight TeraSim's potential as a practical toolfor AV safety assessment, benefiting researchers, developers, and policymakers.The code is available at https://github.com/mcity/TeraSim.</description>
      <author>example@mail.com (Haowei Sun, Xintao Yan, Zhijie Qiao, Haojie Zhu, Yihao Sun, Jiawei Wang, Shengyin Shen, Darian Hogue, Rajanikant Ananta, Derek Johnson, Greg Stevens, Greg McGuire, Yifan Wei, Wei Zheng, Yong Sun, Yasuo Fukai, Henry X. Liu)</author>
      <guid isPermaLink="false">2503.03629v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Generative System for Robot-to-Human Handovers: from Intent Inference to Spatial Configuration Imagery</title>
      <link>http://arxiv.org/abs/2503.03579v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;当前大多数关于机器人与人类交互的研究主要集中于抓取策略和运动规划。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的系统，用于模拟人机之间的物体交接过程。该系统着重于推断人的交接意图及想象空间配置，以模仿合作型机器人的认知处理。&lt;h4&gt;方法&lt;/h4&gt;{'第一部分': '整合多模态感知（视觉和语言提示），来推断人类的交接意图。', '第二部分': '使用基于扩散模型的方法生成交接的空间配置，考虑机器人抓手、物体与人手之间的空间关系'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该方法能够有效解读人的暗示，并实现流畅的人类化交接过程，为协作型机器人提供了一种有前景的解决方案。&lt;h4&gt;结论&lt;/h4&gt;该系统通过模仿人类认知处理中的运动意象，在人机交互中展现出良好的潜力和应用价值。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的用于机器人与人类物体交接系统的模型，这个系统模拟了人类同事间的互动方式。不同于大多数现有研究主要集中于抓取策略及运动规划，我们的系统侧重于推断人的交接意图以及想象空间配置。第一部分整合了多模态感知（视觉和语言提示）来推断人类的意图；第二部分使用基于扩散模型的方法生成交接的空间配置，考虑机器人抓手、物体与人手之间的空间关系，从而模仿认知处理中的运动意象过程。实验结果显示该方法能够有效解读人的暗示，并实现流畅的人类化交接过程，为协作型机器人提供了一种有前景的解决方案。代码、视频和数据可在 https://i3handover.github.io 获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a novel system for robot-to-human object handover that emulateshuman coworker interactions. Unlike most existing studies that focus primarilyon grasping strategies and motion planning, our system focus on 1. inferringhuman handover intents, 2. imagining spatial handover configuration. The firstone integrates multimodal perception-combining visual and verbal cues-to inferhuman intent. The second one using a diffusion-based model to generate thehandover configuration, involving the spacial relationship among robot'sgripper, the object, and the human hand, thereby mimicking the cognitiveprocess of motor imagery. Experimental results demonstrate that our approacheffectively interprets human cues and achieves fluent, human-like handovers,offering a promising solution for collaborative robotics. Code, videos, anddata are available at: https://i3handover.github.io.</description>
      <author>example@mail.com (Hanxin Zhang, Abdulqader Dhafer, Zhou Daniel Hao, Hongbiao Dong)</author>
      <guid isPermaLink="false">2503.03579v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SpeechCompass: Enhancing Mobile Captioning with Diarization and Directional Guidance via Multi-Microphone Localization</title>
      <link>http://arxiv.org/abs/2502.08848v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to CHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一个名为SpeechCompass的系统，用于解决移动设备上语音转文字功能在群组对话中无法区分和指示说话者方向的问题。&lt;h4&gt;背景&lt;/h4&gt;移动设备上的语音转文字技术已被证明对听力和语言无障碍、语言翻译、记笔记和会议记录有帮助。然而，基础大规模调查表明，在群体交谈中无法识别和表示说话者的方向使其变得具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;通过实现实时多麦克风语音定位解决现有移动设备语音转文字功能在群体对话中的局限性问题，并探索有效的可视化方法来指导用户。&lt;h4&gt;方法&lt;/h4&gt;引入了高效实时音频定位算法以及自定义的声音感知硬件，这些硬件运行在一个低功耗微控制器上并连接四个集成麦克风。进行了大规模调查（n=494）并对八名经常使用移动语音转文字的参与者进行了一对一的研究，并收集了他们关于五种可视化样式的反馈。&lt;h4&gt;主要发现&lt;/h4&gt;所有参与者的反馈一致认为，区分说话者和定位视觉化对于群组对话的价值和潜力至关重要。他们认可方向引导的实际价值和潜在应用。&lt;h4&gt;结论&lt;/h4&gt;通过引入SpeechCompass系统，移动设备上的语音转文字功能在群体对话中的用户体验得到了显著改善，并且该技术具有实际的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech-to-text capabilities on mobile devices have proven helpful for hearingand speech accessibility, language translation, note-taking, and meetingtranscripts. However, our foundational large-scale survey (n=263) shows thatthe inability to distinguish and indicate speaker direction makes themchallenging in group conversations. SpeechCompass addresses this limitationthrough real-time, multi-microphone speech localization, where the direction ofspeech allows visual separation and guidance (e.g., arrows) in the userinterface. We introduce efficient real-time audio localization algorithms andcustom sound perception hardware running on a low-power microcontroller andfour integrated microphones, which we characterize in technical evaluations.Informed by a large-scale survey (n=494), we conducted an in-person study ofgroup conversations with eight frequent users of mobile speech-to-text, whoprovided feedback on five visualization styles. The value of diarization andvisualizing localization was consistent across participants, with everyoneagreeing on the value and potential of directional guidance for groupconversations.</description>
      <author>example@mail.com (Artem Dementyev, Dimitri Kanevsky, Samuel J. Yang, Mathieu Parvaix, Chiong Lai, Alex Olwal)</author>
      <guid isPermaLink="false">2502.08848v2</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Olympus: A Jumping Quadruped for Planetary Exploration Utilizing Reinforcement Learning for In-Flight Attitude Control</title>
      <link>http://arxiv.org/abs/2503.03574v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 6 figures, Accepted to the IEEE International Conference on  Robotics and Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;探索低重力的行星体，如月球和火星，允许腿足机器人利用跳跃作为一种高效的运动方式，从而在探测任务中相对于传统探测车具有明显优势。受此启发，本文介绍了Olympus的设计、模拟以及基于学习的空中姿态控制方法，这是一款专为火星引力设计的跳跃式腿足机器人。&lt;h4&gt;背景&lt;/h4&gt;低重力环境下（如月球和火星），腿足机器人可以利用跳跃作为高效的移动方式，并且具有相对于传统探测车的优势&lt;h4&gt;目的&lt;/h4&gt;介绍针对火星重力环境设计、模拟以及基于学习的空中姿态控制方法的Olympus跳跃式腿足机器人的开发。&lt;h4&gt;方法&lt;/h4&gt;首先概述了设计需求，然后详细介绍了如何通过仿真优化机器人从腿部到整体配置的设计，以实现高垂直跳跃、远距离前进跳跃和在空中的姿态重新定向。接着展示了用于跟踪所需空中姿态操作的强化学习策略。&lt;h4&gt;主要发现&lt;/h4&gt;成功跨越模拟与现实之间的差距，进行了广泛的实验研究来测试姿态重新定向。&lt;h4&gt;结论&lt;/h4&gt;Olympus的设计以及基于学习的方法为未来在低重力行星体上的探测任务提供了一个有效的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Exploring planetary bodies with lower gravity, such as the moon and Mars,allows legged robots to utilize jumping as an efficient form of locomotion thusgiving them a valuable advantage over traditional rovers for exploration.Motivated by this fact, this paper presents the design, simulation, andlearning-based "in-flight" attitude control of Olympus, a jumping legged robottailored to the gravity of Mars. First, the design requirements are outlinedfollowed by detailing how simulation enabled optimizing the robot's design -from its legs to the overall configuration - towards high vertical jumping,forward jumping distance, and in-flight attitude reorientation. Subsequently,the reinforcement learning policy used to track desired in-flight attitudemaneuvers is presented. Successfully crossing the sim2real gap, extensiveexperimental studies of attitude reorientation tests are demonstrated.</description>
      <author>example@mail.com (Jørgen Anker Olsen, Grzegorz Malczyk, Kostas Alexis)</author>
      <guid isPermaLink="false">2503.03574v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented Manipulation</title>
      <link>http://arxiv.org/abs/2503.03556v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了LVIS-Aff数据集和Afford-X模型，以改进基于感知的对象功能推理能力。&lt;h4&gt;背景&lt;/h4&gt;对象功能推理对于任务导向的计划和执行至关重要。然而，现有的计算模型缺乏泛化能力，难以应用于新的场景。&lt;h4&gt;目的&lt;/h4&gt;开发一个新的大规模数据集（LVIS-Aff）以及一个增强的功能推理模型（Afford-X），以提高对象功能感知推理的准确性和速度，并适用于本地设备的任务操作。&lt;h4&gt;方法&lt;/h4&gt;通过引入LVIS-Aff数据集和使用Verb Attention与Bi-Fusion模块来改进多模态理解，提出了一种端到端可训练的affordance推理模型（Afford-X）。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的Afford-X模型比最好的非LLM方法提高了12.1%的表现，并且相较于作者之前的会议论文也有所进步。此外，它保持了紧凑的参数规模和高速度。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了高效、通用的功能推理模型在本地设备上部署的可能性，并证明其在机器人任务操作中的有效性以及对现实世界应用的意义。&lt;h4&gt;翻译&lt;/h4&gt;物体功能推理能力对于人类和人工智能（AI）的任务导向规划至关重要。现有方法缺乏泛化性，难以处理新场景问题。为此，作者提出了一种大规模数据集LVIS-Aff和一个先进的模型Afford-X，旨在提高基于感知的功能推理性能，并展示了其在实际应用中的优越效果和效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object affordance reasoning, the ability to infer object functionalitiesbased on physical properties, is fundamental for task-oriented planning andactivities in both humans and Artificial Intelligence (AI). This capability,required for planning and executing daily activities in a task-oriented manner,relies on commonsense knowledge of object physics and functionalities,extending beyond simple object recognition. Current computational models foraffordance reasoning from perception lack generalizability, limiting theirapplicability in novel scenarios. Meanwhile, comprehensive Large LanguageModels (LLMs) with emerging reasoning capabilities are challenging to deploy onlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, alarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhancethe generalizability of affordance reasoning from perception. Utilizing thisdataset, we develop Afford-X, an end-to-end trainable affordance reasoningmodel that incorporates Verb Attention and Bi-Fusion modules to improvemulti-modal understanding. This model achieves up to a 12.1% performanceimprovement over the best-reported results from non-LLM methods, while alsodemonstrating a 1.2% enhancement compared to our previous conference paper.Additionally, it maintains a compact 187M parameter size and infers nearly 50times faster than the GPT-4V API. Our work demonstrates the potential forefficient, generalizable affordance reasoning models that can be deployed onlocal devices for task-oriented manipulations. We showcase Afford-X'seffectiveness in enabling task-oriented manipulations for robots across varioustasks and environments, underscoring its efficiency and broad implications foradvancing robotics and AI systems in real-world applications.</description>
      <author>example@mail.com (Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao)</author>
      <guid isPermaLink="false">2503.03556v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Unified Human Localization and Trajectory Prediction with Monocular Vision</title>
      <link>http://arxiv.org/abs/2503.03535v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了MonoTransmotion (MT)框架，该框架利用单目相机同时解决定位和预测任务，展示出在真实世界场景中处理嘈杂数据时的稳健性和泛化能力。&lt;h4&gt;背景&lt;/h4&gt;传统的轨迹预测模型依赖于经过精制的数据，并且需要特殊设备或手动标注，这使得它们不适用于大多数机器人应用。现有预测器往往过于适应干净观察结果而影响其使用嘈杂输入时的鲁棒性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Transformer框架的方法（MonoTransmotion），该方法仅利用单目相机进行定位和轨迹预测任务，并验证该方法在现实场景中的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一个包含两个主要模块的框架：鸟瞰图(BEV)定位模块和轨迹预测模块。BEV定位模块使用2D人体姿势估计人的位置，而轨迹预测模块则根据这些估计值预测未来的运动路径。&lt;h4&gt;主要发现&lt;/h4&gt;通过联合训练上述任务并采用统一框架的方法，MT方法在包含嘈杂输入的真实场景中表现更加稳健，并且其性能在数据集中得到验证。在人工准备的数据集上，相比基线模型，MT框架在BEV定位和轨迹预测方面取得了约12%的改进。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，在非精制的真实世界数据集上，MT方法维持了类似水平的表现，这突显了其稳健性和泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的行人轨迹预测模型依赖于经过清洗和整理的数据，通常需要特殊设备或人工标记，这对于机器人应用来说往往是不切实际的。现有的预测器倾向于过度拟合到干净的观察结果上，在处理嘈杂输入时会降低其鲁棒性。在这项工作中，我们提出了基于Transformer框架的MonoTransmotion（MT），该框架仅使用单目相机同时解决定位和预测任务。我们的框架有两个主要模块：鸟瞰图（BEV）定位和轨迹预测。BEV定位模块利用2D人体姿态估计人的位置，并通过一种新的方向损失函数增强，以实现更平滑的顺序定位。轨迹预测模块从这些估算值中预测未来的运动路径。我们展示了通过联合训练这两个任务并使用统一框架的方法，在处理嘈杂输入的真实世界场景中的表现更加稳健。我们在人工准备和非人工准备的数据集上验证了我们的MT网络。在人工准备的数据集上，MT相比基准模型在BEV定位和轨迹预测方面取得了大约12%的改进。而在真实的非精制数据集上，实验结果表明MT保持类似的表现水平，突显其稳健性和泛化能力。代码可在https://github.com/vita-epfl/MonoTransmotion获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Conventional human trajectory prediction models rely on clean curated data,requiring specialized equipment or manual labeling, which is often impracticalfor robotic applications. The existing predictors tend to overfit to cleanobservation affecting their robustness when used with noisy inputs. In thiswork, we propose MonoTransmotion (MT), a Transformer-based framework that usesonly a monocular camera to jointly solve localization and prediction tasks. Ourframework has two main modules: Bird's Eye View (BEV) localization andtrajectory prediction. The BEV localization module estimates the position of aperson using 2D human poses, enhanced by a novel directional loss for smoothersequential localizations. The trajectory prediction module predicts futuremotion from these estimates. We show that by jointly training both tasks withour unified framework, our method is more robust in real-world scenarios madeof noisy inputs. We validate our MT network on both curated and non-curateddatasets. On the curated dataset, MT achieves around 12% improvement overbaseline models on BEV localization and trajectory prediction. On real-worldnon-curated dataset, experimental results indicate that MT maintains similarperformance levels, highlighting its robustness and generalization capability.The code is available at https://github.com/vita-epfl/MonoTransmotion.</description>
      <author>example@mail.com (Po-Chien Luan, Yang Gao, Celine Demonsant, Alexandre Alahi)</author>
      <guid isPermaLink="false">2503.03535v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Potential gains of communication-compute-control co-design based performance optimization methods in cyber-physical systems</title>
      <link>http://arxiv.org/abs/2503.03521v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了三种性能优化的方法，通过引入通信和计算特性到应用逻辑中来改善整体系统性能。&lt;h4&gt;背景&lt;/h4&gt;在利用网络和云技术实现工业控制系统时，控制应用程序的确定性会自然降低。这意味着某些不完善可能会被引入控制系统，在干扰期间闭环控制实际上转变为开环控制。&lt;h4&gt;目的&lt;/h4&gt;目的是通过应用可以补偿统计上或保证方式的方法来改进这些开环控制时段的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了三种基于共设计的应用改进方案，这些方案对底层技术的依赖性最小。&lt;h4&gt;主要发现&lt;/h4&gt;共设计方法能够显著提高机器人轨迹在开环控制期间执行的准确性，并且结合使用这些建议的方法可以将轨迹执行时间缩短多达45%。&lt;h4&gt;结论&lt;/h4&gt;通过引入通信和计算特性到应用逻辑中，整体系统性能得到明显改善，特别是在开放环控制周期内表现得更加准确。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we propose and quantitatively evaluate three performanceoptimization methods that exploit the concept of communication-compute-controlco-design by introducing awareness of communication and compute characteristicsinto the application logic in different ways to improve overall systemperformance. We have implemented a closed-loop control of a robotic arm over awireless network where the controller is deployed into an edge cloudenvironment. When implementing an industrial system that leverages network andcloud technologies, the level of determinism of the control application can bedecreased by nature. This means that some imperfections may be introduced intothe control system, and the closed-loop control in substance changes toopen-loop during disturbances. We aim to improve the performance of theseopen-loop control periods by applying methods that can compensate for theimperfections statistically or in a guaranteed way. We demonstrate thatco-design-based application improvements with minimal dependencies on theunderlying technologies can already yield an order of magnitude gain when itcomes to the accurate execution of the robot trajectories during the openloopcontrol periods. Furthermore, by combining the proposed methods, theperformance improvements add up and can produce up to 45% shorter trajectoryexecutions compared to individual evaluations.</description>
      <author>example@mail.com (Sándor Rácz, Norbert Reider)</author>
      <guid isPermaLink="false">2503.03521v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>NeuGrasp: Generalizable Neural Surface Reconstruction with Background Priors for Material-Agnostic Object Grasp Detection</title>
      <link>http://arxiv.org/abs/2503.03511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 5 figures. IEEE International Conference on Robotics and  Automation (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;NeuGrasp是一种基于神经网络的表面重建方法，旨在通过利用背景先验来实现材料无关的手抓取检测。它在透明和镜面物体场景中表现出色。&lt;h4&gt;背景&lt;/h4&gt;现有的手抓取方法在处理包含透明或镜面反射物体的情况下遇到挑战，这些方法依赖于准确的深度信息难以应对。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的神经网络表面重建方法NeuGrasp，以实现更稳健的手抓取检测，并能有效应对具有透明和镜面特性的物体。&lt;h4&gt;方法&lt;/h4&gt;引入了transformer模型和全局先验体积来集成多视角特征与空间编码，通过残差特征增强专注于前景对象并通过占用先验体来改进空间感知。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在模拟和现实世界场景中NeuGrasp在手抓取方面优于现有的最佳方法，并且保持了相当的重建质量。&lt;h4&gt;结论&lt;/h4&gt;NeuGrasp能够有效处理具有透明或镜面特性的物体，展示出其在复杂环境中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人抓取技术在面对含有透明和镜面反射物体的情况时面临巨大挑战。本文介绍了一种名为NeuGrasp的神经表面重建方法，该方法利用背景先验来实现材料无关的手部抓取检测。通过整合transformer模型与全局先验体积，NeuGrasp能有效聚合多视角特征并进行空间编码，在狭小且视角稀疏条件下具有优秀的表观重建能力。此外，它还能够通过专注于前景物体的残差特征增强及占用体优化来改进空间感知，从而更有效地处理透明和镜面表面对象。在模拟与实际场景中的大量实验表明，NeuGrasp的手部抓取性能超越现有最佳方法，并且具有可比的重建质量。详情请访问https://neugrasp.github.io/。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic grasping in scenes with transparent and specular objects presentsgreat challenges for methods relying on accurate depth information. In thispaper, we introduce NeuGrasp, a neural surface reconstruction method thatleverages background priors for material-agnostic grasp detection. NeuGraspintegrates transformers and global prior volumes to aggregate multi-viewfeatures with spatial encoding, enabling robust surface reconstruction innarrow and sparse viewing conditions. By focusing on foreground objects throughresidual feature enhancement and refining spatial perception with anoccupancy-prior volume, NeuGrasp excels in handling objects with transparentand specular surfaces. Extensive experiments in both simulated and real-worldscenarios show that NeuGrasp outperforms state-of-the-art methods in graspingwhile maintaining comparable reconstruction quality. More details are availableat https://neugrasp.github.io/.</description>
      <author>example@mail.com (Qingyu Fan, Yinghao Cai, Chao Li, Wenzhe He, Xudong Zheng, Tao Lu, Bin Liang, Shuo Wang)</author>
      <guid isPermaLink="false">2503.03511v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>A Benchmark for Optimal Multi-Modal Multi-Robot Multi-Goal Path Planning with Given Robot Assignment</title>
      <link>http://arxiv.org/abs/2503.03509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的机器人路径规划问题的解决方案，用于多机器人在共享工作空间中完成任务的问题，并提出了一个涵盖多种场景基准测试。&lt;h4&gt;背景描述&lt;/h4&gt;多个工业机器人在同一工作空间内共同作业以尽快完成一系列任务。这类设置可被视为一个多模式、多机器人和多目标的路径规划问题。&lt;h4&gt;研究目的&lt;/h4&gt;通过将该问题形式化为单一路径规划问题，提出了一种基准测试方法，并引入了适应于复杂情况下的RRT*和PRM*路径规划器作为基线。&lt;h4&gt;创新方法&lt;/h4&gt;本文的方法不同于以往以优先级处理或假设同步完成任务的方式，而是采用复合空间的路径规划方式，在非离散2D工作环境下也适用，支持动态环境变化，并适用于不同约束条件的异构机器人团队。&lt;h4&gt;关键贡献&lt;/h4&gt;引入了一个多样化的基准测试集，涵盖了具有不同类型、规划时间线和协作任务（如交接）的不同问题实例。同时，改进了RRT*和PRM*路径规划器以适应复杂情况。&lt;h4&gt;结论&lt;/h4&gt;通过提出新的方法和基准测试，为解决多机器人协同工作中的路径规划问题提供了更有效的方法。&lt;h4&gt;翻译&lt;/h4&gt;在许多工业机器人的应用中，多个机器人在同一共享工作空间内共同作业，以尽可能快速地完成一系列任务。这类设置可以视为一个多模式、多机器人和多目标的路径规划问题，其中每个机器人必须到达一个有序的目标序列。现有的方法通过优先级处理或假设同步完成任务来解决此类问题，并因此既不是最优也不是完整的。本文将这个问题形式化为单一路径规划问题，并引入了一个基准测试，涵盖了包括具有不同类型机器人、不同规划时间线和协作任务（如交接）在内的多种情况的问题实例。此外，除了这个基准测试，还适应了RRT*和PRM*规划器作为基线方法以处理这些问题。与现有方法不同的是，本文的路径规划器和方案不受限于离散2D工作空间，并支持动态环境变化，在具有不同类型约束条件的异构机器人团队中使用多个模式和目标时也非常适用。基准测试及其规划器的相关视频和代码可在https://vhartman.github.io/mrmg-planning/获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many industrial robotics applications, multiple robots are working in ashared workspace to complete a set of tasks as quickly as possible. Suchsettings can be treated as multi-modal multi-robot multi-goal path planningproblems, where each robot has to reach an ordered sequence of goals. Existingapproaches to this type of problem solve this using prioritization or assumesynchronous completion of tasks, and are thus neither optimal nor complete. Weformalize this problem as a single path planning problem and introduce abenchmark encompassing a diverse range of problem instances including scenarioswith various robots, planning horizons, and collaborative tasks such ashandovers. Along with the benchmark, we adapt an RRT* and a PRM* planner toserve as a baseline for the planning problems. Both planners work in thecomposite space of all robots and introduce the required changes to work in oursetting. Unlike existing approaches, our planner and formulation is notrestricted to discretized 2D workspaces, supports a changing environment, andworks for heterogeneous robot teams over multiple modes with differentconstraints, and multiple goals. Videos and code for the benchmark and theplanners is available at https://vhartman.github.io/mrmg-planning/.</description>
      <author>example@mail.com (Valentin N. Hartmann, Tirza Heinle, Stelian Coros)</author>
      <guid isPermaLink="false">2503.03509v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Coordinated Trajectories for Non-stop Flying Carriers Holding a Cable-Suspended Load</title>
      <link>http://arxiv.org/abs/2503.03481v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多旋翼无人机在空中操作中通常被视为有限耐力的设备，但本文展示了一种方法可以使用连续飞行的三架或多架无人机来保持吊挂负载的恒定姿态。&lt;h4&gt;背景&lt;/h4&gt;现有的多旋翼无人机由于能量限制无法长时间进行空中操作任务。&lt;h4&gt;目的&lt;/h4&gt;探索并实现一种利用连续飞行的非停止型无人机进行高效空中操作的方法，同时确保吊挂负载在动态环境中的稳定。&lt;h4&gt;方法&lt;/h4&gt;{'选择内部力方向': '选择了$n$个特殊线性独立的方向作为载荷抓取矩阵零空间内的内力，在连接载荷悬架点的图中形成一个哈密顿圈。相邻对的方向用于生成作用于不同二维仿射子空间的$n$个力量。', '构建椭圆轨迹': '通过适当的图着色，将每个哈密顿环的边映射到周期坐标上，确保没有相邻坐标在同时显示零导数的情况下仍然可以构建出椭圆形轨迹。'}&lt;h4&gt;主要发现&lt;/h4&gt;这些选择和构造条件保证了$n$个力轨迹投影到相应的电缆约束球体上的非零切线速度，从而使载荷保持静止的同时无人机能够进行持续的运动。&lt;h4&gt;结论&lt;/h4&gt;理论成果通过模拟和实验室实验中的非停止多旋翼无人机进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;多旋翼UAV通常被认为适合空中操作，但由于其有限的耐力限制了长时间的操作任务。这项工作展示了三架或更多连续飞行载体能够保持吊挂载荷恒定姿态的可能性，并且提出了生成协同无间断轨迹的算法。这种方法基于两个支柱：（1）选择$n$个特殊线性独立方向作为载荷抓取矩阵零空间内的内力，形成一个哈密顿圈连接悬架点；（2）在这些子空间中构建椭圆轨迹，在适当的图着色下，每个哈密顿环的边映射到周期坐标上，并确保没有相邻坐标同时显示零导数。结合负载静态条件和悬挂点位置，这些选择保证了$n$个力轨迹投影到相应的电缆约束球体上的非零切线速度，从而在载荷静止的情况下实现载体的永不停歇运动。理论发现已通过模拟和实验室实验验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multirotor UAVs have been typically considered for aerial manipulation, buttheir scarce endurance prevents long-lasting manipulation tasks. This workdemonstrates that the non-stop flights of three or more carriers are compatiblewith holding a constant pose of a cable-suspended load, thus potentiallyenabling aerial manipulation with energy-efficient non-stop carriers. It alsopresents an algorithm for generating the coordinated non-stop trajectories. Theproposed method builds upon two pillars: (1)~the choice of $n$ special linearlyindependent directions of internal forces within the $3n-6$-dimensionalnullspace of the grasp matrix of the load, chosen as the edges of a Hamiltoniancycle on the graph that connects the cable attachment points on the load.Adjacent pairs of directions are used to generate $n$ forces evolving ondistinct 2D affine subspaces, despite the attachment points being genericallyin 3D; (2)~the construction of elliptical trajectories within these subspacesby mapping, through appropriate graph coloring, each edge of the Hamiltoniancycle to a periodic coordinate while ensuring that no adjacent coordinatesexhibit simultaneous zero derivatives. Combined with conditions for loadstatics and attachment point positions, these choices ensure that each of the$n$ force trajectories projects onto the corresponding cable constraint spherewith non-zero tangential velocity, enabling perpetual motion of the carrierswhile the load is still. The theoretical findings are validated throughsimulations and laboratory experiments with non-stopping multirotor UAVs.</description>
      <author>example@mail.com (Chiara Gabellieri, Antonio Franchi)</author>
      <guid isPermaLink="false">2503.03481v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via Safe Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2503.03480v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为SafeVLA的新算法，旨在将安全性融入到视觉-语言-动作模型(VLAS)中，以保护真实世界中的环境、机器人硬件和人类的安全。&lt;h4&gt;背景&lt;/h4&gt;VLAs作为一种通用的机器人策略显示出巨大的潜力，但在实际部署时带来了紧迫的安全挑战，包括对环境、机器人本身以及人类可能造成的物理伤害的风险。&lt;h4&gt;目的&lt;/h4&gt;为了将安全性明确地整合到VLAs中，设计了一种新的算法SafeVLA。&lt;h4&gt;方法&lt;/h4&gt;通过在模拟环境中使用大规模约束学习来有效地平衡安全性和任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;与当前最先进的方法相比，在仿真测试中实现了83.58%的安全性改进和3.85%的任务性能提升；优先考虑安全性可以消除高风险行为并将不安全行为的上限降低到1/35，从而显著减少长尾风险。&lt;h4&gt;结论&lt;/h4&gt;SafeVLA不仅在模拟实验中表现出色，在处理多种未知场景时也展示了良好的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;Vision-language-action models (VLAs) have shown great potential as generalistrobot policies. However, these models pose urgent safety challenges duringdeployment, including the risk of physical harm to the environment, the robotitself, and humans. How can safety be explicitly incorporated into VLAs? Inthis work, we propose SafeVLA, a novel algorithm designed to integrate safetyinto VLAs, ensuring the protection of the environment, robot hardware andhumans in real-world settings. SafeVLA effectively balances safety and taskperformance by employing large-scale constrained learning within simulatedenvironments. We demonstrate that SafeVLA outperforms the currentstate-of-the-art method in both safety and task performance, achieving averageimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizingsafety, our approach eliminates high-risk behaviors and reduces the upper boundof unsafe behaviors to 1/35 of that in the current state-of-the-art, therebysignificantly mitigating long-tail risks. Furthermore, the learned safetyconstraints generalize to diverse, unseen scenarios, including multipleout-of-distribution perturbations and tasks.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language-action models (VLAs) have shown great potential as generalistrobot policies. However, these models pose urgent safety challenges duringdeployment, including the risk of physical harm to the environment, the robotitself, and humans. How can safety be explicitly incorporated into VLAs? Inthis work, we propose SafeVLA, a novel algorithm designed to integrate safetyinto VLAs, ensuring the protection of the environment, robot hardware andhumans in real-world settings. SafeVLA effectively balances safety and taskperformance by employing large-scale constrained learning within simulatedenvironments. We demonstrate that SafeVLA outperforms the currentstate-of-the-art method in both safety and task performance, achieving averageimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizingsafety, our approach eliminates high-risk behaviors and reduces the upper boundof unsafe behaviors to 1/35 of that in the current state-of-the-art, therebysignificantly mitigating long-tail risks. Furthermore, the learned safetyconstraints generalize to diverse, unseen scenarios, including multipleout-of-distribution perturbations and tasks. Our data, models and newlyproposed benchmark environment are available athttps://sites.google.com/view/pku-safevla.</description>
      <author>example@mail.com (Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang)</author>
      <guid isPermaLink="false">2503.03480v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Control of Diverse Skills in Quadruped Robots Without Complete Expert Datasets</title>
      <link>http://arxiv.org/abs/2503.03476v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的四足机器人技能学习方法PASIST，该方法能够在没有完整专家数据集的情况下自主探索和选择高质量的轨迹，并且能够实现平滑自然的动作转换。&lt;h4&gt;背景&lt;/h4&gt;当前用于四足机器人的模仿学习方法虽然有效，但需要昂贵的数据集来复制专家行为。现有的方法无法处理不同难度的任务以及技能之间的复杂过渡问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自模仿技能过渡方法，以减少对专家数据集的依赖，并提高机器人在各种任务中的表现。&lt;h4&gt;方法&lt;/h4&gt;PASIST基于预定义的目标姿态自主探索并选择高质量轨迹，利用生成对抗自模仿学习（GASIL）框架。此外还开发了一个技能选择模块来缓解模式崩溃问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入新的技能选择机制，可以在不依赖于完整专家数据集的情况下实现平滑自然的动作转换和复杂任务的处理。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了PASIST的有效性，并表明其为专家驱动的学习提供了一种高效的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;学习四足机器人的多样化技能面临着重大挑战，如掌握不同技能之间的复杂过渡以及应对难度各异的任务。现有的模仿学习方法虽然成功，但依赖于昂贵的数据集来复制专家行为。受自我反思学习的启发，我们提出了一种新的方法PASIST，它可以消除对完整专家数据集的需求。PASIST基于预定义的目标姿态自主探索并选择高质量轨迹，并利用生成对抗自模仿学习（GASIL）框架。为了进一步增强学习效果，我们开发了一个技能选择模块来平衡难度不同的技能权重以减少模式崩溃问题。通过这些方法，PASIST能够在没有完整专家数据集的情况下复制对应于目标姿势的技能并且实现平滑自然的动作转换。在仿真平台和Solo 8机器人上的评估确认了PASIST的有效性，并为专家驱动的学习提供了有效的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning diverse skills for quadruped robots presents significant challenges,such as mastering complex transitions between different skills and handlingtasks of varying difficulty. Existing imitation learning methods, whilesuccessful, rely on expensive datasets to reproduce expert behaviors. Inspiredby introspective learning, we propose Progressive Adversarial Self-ImitationSkill Transition (PASIST), a novel method that eliminates the need for completeexpert datasets. PASIST autonomously explores and selects high-qualitytrajectories based on predefined target poses instead of demonstrations,leveraging the Generative Adversarial Self-Imitation Learning (GASIL)framework. To further enhance learning, We develop a skill selection module tomitigate mode collapse by balancing the weights of skills with varying levelsof difficulty. Through these methods, PASIST is able to reproduce skillscorresponding to the target pose while achieving smooth and natural transitionsbetween them. Evaluations on both simulation platforms and the Solo 8 robotconfirm the effectiveness of PASIST, offering an efficient alternative toexpert-driven learning.</description>
      <author>example@mail.com (Jiaxin Tu, Xiaoyi Wei, Yueqi Zhang, Taixian Hou, Xiaofei Gao, Zhiyan Dong, Peng Zhai, Lihua Zhang)</author>
      <guid isPermaLink="false">2503.03476v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Generative Artificial Intelligence in Robotic Manipulation: A Survey</title>
      <link>http://arxiv.org/abs/2503.03464v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;综述了机器人操作领域中生成学习模型的最新进展，概述了这些模型的关键挑战，并介绍了几种生成式模型范式及其应用。&lt;h4&gt;背景&lt;/h4&gt;在机器人操作中面临的主要瓶颈包括数据不足和采集效率低、长期复杂的任务规划以及跨不同环境进行稳健策略学习所需的多模态推理能力。&lt;h4&gt;目的&lt;/h4&gt;介绍用于解决上述关键问题的生成式模型，如GANs、VAEs、扩散模型、概率流模型及自回归模型，并探讨它们的优势与局限性。&lt;h4&gt;方法&lt;/h4&gt;将生成模型的应用分为基础层（数据和奖励生成）、中间层（语言、代码、视觉与状态生成）以及策略层（抓取和轨迹生成），详细讨论每个层次的代表性工作。&lt;h4&gt;主要发现&lt;/h4&gt;指出未来研究需提高数据利用效率，更好地处理长期任务，并在多样化机器人应用场景中提升泛化能力。&lt;h4&gt;结论&lt;/h4&gt;强调了需要改进现有方法以应对机器人操作中的关键挑战，并提供了相关的资源链接供社区使用。&lt;h4&gt;翻译&lt;/h4&gt;该综述提供了一种关于最近用于机器人操作的生成学习模型进展的全面回顾，这些模型旨在解决领域内的关键问题。概述了几种生成式模型（如GANs、VAEs等）及其应用层级，并指出了未来研究的方向和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This survey provides a comprehensive review on recent advancements ofgenerative learning models in robotic manipulation, addressing key challengesin the field. Robotic manipulation faces critical bottlenecks, includingsignificant challenges in insufficient data and inefficient data acquisition,long-horizon and complex task planning, and the multi-modality reasoningability for robust policy learning performance across diverse environments. Totackle these challenges, this survey introduces several generative modelparadigms, including Generative Adversarial Networks (GANs), VariationalAutoencoders (VAEs), diffusion models, probabilistic flow models, andautoregressive models, highlighting their strengths and limitations. Theapplications of these models are categorized into three hierarchical layers:the Foundation Layer, focusing on data generation and reward generation; theIntermediate Layer, covering language, code, visual, and state generation; andthe Policy Layer, emphasizing grasp generation and trajectory generation. Eachlayer is explored in detail, along with notable works that have advanced thestate of the art. Finally, the survey outlines future research directions andchallenges, emphasizing the need for improved efficiency in data utilization,better handling of long-horizon tasks, and enhanced generalization acrossdiverse robotic scenarios. All the related resources, including researchpapers, open-source data, and projects, are collected for the community inhttps://github.com/GAI4Manipulation/AwesomeGAIManipulation</description>
      <author>example@mail.com (Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Bo Yang, Hua Chen)</author>
      <guid isPermaLink="false">2503.03464v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>REACT: Real-time Efficient Attribute Clustering and Transfer for Updatable 3D Scene Graph</title>
      <link>http://arxiv.org/abs/2503.03412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;REACT框架通过实时属性聚类和迁移技术，实现了在动态环境中对3D场景图中的对象节点进行重新定位。&lt;h4&gt;背景&lt;/h4&gt;现代自主机器人需要高级地图表示来执行复杂任务。最近，3D场景图（3DSGs）作为传统网格地图的有前途替代品出现，它结合了高效的内存使用和丰富的特征表示。&lt;h4&gt;目的&lt;/h4&gt;引入REACT框架以实现在动态环境中实时重新定位对象节点，并保持计算效率。&lt;h4&gt;方法&lt;/h4&gt;REACT采用了一种新颖的方法来比较对象实例，该方法基于三元损失训练的嵌入模型进行。此方法促进了实例聚类和匹配。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，REACT能够在维护计算效率的同时重新定位物体。&lt;h4&gt;结论&lt;/h4&gt;REACT框架将作为开源项目公开发布，推动可重用性和更新性的3D场景图技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;现代自主机器人需要高级地图表示来执行复杂任务。最近，3D场景图（3DSGs）作为一种有前途的替代品出现了，它可以结合高效的内存使用和丰富的特征表示。然而，大多数应用于该领域的努力都局限于静态世界。这项工作介绍了REACT框架，该框架能够有效地执行实时属性聚类和转移以重新定位3DSG中的对象节点。REACT采用了一种新颖的方法来比较对象实例，这种方法基于三元损失训练的嵌入模型进行。实验结果显示，REACT能够在维护计算效率的同时重新定位物体。REACT框架作为开源项目将被发布，这将促进可重用性和更新性的3D场景图技术的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern-day autonomous robots need high-level map representations to performsophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as apromising alternative to traditional grid maps, blending efficient memory useand rich feature representation. However, most efforts to apply them have beenlimited to static worlds. This work introduces REACT, a framework thatefficiently performs real-time attribute clustering and transfer to relocalizeobject nodes in a 3DSG. REACT employs a novel method for comparing objectinstances using an embedding model trained on triplet loss, facilitatinginstance clustering and matching. Experimental results demonstrate that REACTis able to relocalize objects while maintaining computational efficiency. TheREACT framework's source code will be available as an open-source project,promoting further advancements in reusable and updatable 3DSGs.</description>
      <author>example@mail.com (Phuoc Nguyen, Francesco Verdoja, Ville Kyrki)</author>
      <guid isPermaLink="false">2503.03412v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Navigating Intelligence: A Survey of Google OR-Tools and Machine Learning for Global Path Planning in Autonomous Vehicles</title>
      <link>http://arxiv.org/abs/2503.03338v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文深入研究了无人地面车辆的全局路径规划（GPP），特别是在自主采矿采样机器人ROMIE中的应用。通过解决旅行商问题优化成本和时间，旨在提高其运营效率。&lt;h4&gt;背景&lt;/h4&gt;全球路径规划对于自动化的矿业采样机器人的性能至关重要，特别是通过解决复杂的图论挑战——旅行商问题来实现高效路径覆盖。&lt;h4&gt;目的&lt;/h4&gt;开发、评估并改进一种低成本的软件和网络应用程序以优化全局路径规划。研究重点在于运用和测试Google运营研究工具的能力，并首次结合强化学习技术进行比较分析。&lt;h4&gt;方法&lt;/h4&gt;深入对比分析Google OR-Tools中的各种优化算法，通过实验确定Q-Learning等方法相对于OR-Tools的计算有效性和实际应用效率。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，Q-Learning在测试数据集上表现出色，平均偏离最优解仅为1.2%，优于其他比较方法。&lt;h4&gt;结论&lt;/h4&gt;Q-Learning是解决全局路径规划问题中最有效的策略之一，它展示了相对于传统优化算法的显著优势。&lt;h4&gt;翻译&lt;/h4&gt;我们提供了一项关于无人地面车辆全球路径规划（GPP）的新颖深入研究，聚焦于一个自主采矿采样机器人ROMIE的应用。GPP对于ROMIE的最佳性能是至关重要的，这被转化为解决旅行商问题——这是一个复杂的图论挑战，对于确定覆盖矿业场地内所有采样地点的最有效路线至关重要。这个问题的核心在于通过优化成本和时间来提高ROMIE的操作效率，并且在与人力竞争时保持其竞争力。本研究的主要目的是通过开发、评估并改进一种低成本软件和网络应用程序来推进GPP的研究。我们深入比较分析了Google运营研究（OR）-Tools中的优化算法，我们的研究旨在首次将强化学习技术应用于这些工具中，以应用和测试它们的能力限制，并将这些方法与OR-Tools进行对比，评估其计算有效性和实际应用效率。本项研究试图提供关于每种技术的有效性及实际应用情况的见解。我们发现Q-Learning在所有数据集上表现出色，平均仅偏离最优解1.2%，因此它被证明是最优策略之一。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1002/aisy.202300840&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We offer a new in-depth investigation of global path planning (GPP) forunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPPis essential for ROMIE's optimal performance, which is translated into solvingthe traveling salesman problem, a complex graph theory challenge that iscrucial for determining the most effective route to cover all samplinglocations in a mining field. This problem is central to enhancing ROMIE'soperational efficiency and competitiveness against human labor by optimizingcost and time. The primary aim of this research is to advance GPP bydeveloping, evaluating, and improving a cost-efficient software and webapplication. We delve into an extensive comparison and analysis of Googleoperations research (OR)-Tools optimization algorithms. Our study is driven bythe goal of applying and testing the limits of OR-Tools capabilities byintegrating Reinforcement Learning techniques for the first time. This enablesus to compare these methods with OR-Tools, assessing their computationaleffectiveness and real-world application efficiency. Our analysis seeks toprovide insights into the effectiveness and practical application of eachtechnique. Our findings indicate that Q-Learning stands out as the optimalstrategy, demonstrating superior efficiency by deviating only 1.2% on averagefrom the optimal solutions across our datasets.</description>
      <author>example@mail.com (Alexandre Benoit, Pedram Asef)</author>
      <guid isPermaLink="false">2503.03338v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Visual Docking Network for Unmanned Surface Vehicles Using Auto-labeling in Real-world Water Environments</title>
      <link>http://arxiv.org/abs/2503.03282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于无人水面艇（USV）自主视觉靠泊的新型监督学习流水线，并采用了自动标注技术。通过设计了一个无需人工标记的数据收集流程，开发了神经靠泊姿态估计器（NDPE），该系统能够实现对相对靠泊姿态的预测。&lt;h4&gt;背景&lt;/h4&gt;无人水面船在环境监测和河流建模等水上作业中被广泛应用，但在港口或站点实现精确自主靠岸仍面临挑战，这通常需要远程人类控制或外部定位系统的辅助以确保准确性和安全性。这种依赖限制了USV完全自动化的潜力。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新颖的监督学习流水线和自动标注技术，用于提升无人水面艇（USVs）在视觉引导下的自主靠泊能力。&lt;h4&gt;方法&lt;/h4&gt;设计了一种自动化数据收集流程来生成相对姿态和图像对的数据集，并提出了神经靠泊姿态估计器NDPE以实现不需要手工特征工程、相机校准或外部标记的相对靠泊姿态预测。此外，该系统能够准确地预测实际水环境中的相对靠泊姿态，支持基于位置的视觉伺服（PBVS）和低级运动控制器的有效实施。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，NDPE对距离变化和USV速度扰动具有鲁棒性，并且在真实水域环境中验证了该解决方案的有效性和可操作性。&lt;h4&gt;结论&lt;/h4&gt;提出的解决方案能够有效地处理现实世界中的自主靠泊任务，证明了其强大的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unmanned Surface Vehicles (USVs) are increasingly applied to water operationssuch as environmental monitoring and river-map modeling. It faces a significantchallenge in achieving precise autonomous docking at ports or stations, stillrelying on remote human control or external positioning systems for accuracyand safety which limits the full potential of human-out-of-loop deployment forUSVs.This paper introduces a novel supervised learning pipeline with theauto-labeling technique for USVs autonomous visual docking. Firstly, wedesigned an auto-labeling data collection pipeline that appends relative poseand image pair to the dataset. This step does not require conventional manuallabeling for supervised learning. Secondly, the Neural Dock Pose Estimator(NDPE) is proposed to achieve relative dock pose prediction without the needfor hand-crafted feature engineering, camera calibration, and peripheralmarkers. Moreover, The NDPE can accurately predict the relative dock pose inreal-world water environments, facilitating the implementation ofPosition-Based Visual Servo (PBVS) and low-level motion controllers forefficient and autonomous docking.Experiments show that the NDPE is robust tothe disturbance of the distance and the USV velocity. The effectiveness of ourproposed solution is tested and validated in real-world water environments,reflecting its capability to handle real-world autonomous docking tasks.</description>
      <author>example@mail.com (Yijie Chu, Ziniu Wu, Yong Yue, Eng Gee Lim, Paolo Paoletti, Xiaohui Zhu)</author>
      <guid isPermaLink="false">2503.03282v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory Prediction for Autonomous Driving: Progress, Limitations, and Future Directions</title>
      <link>http://arxiv.org/abs/2503.03262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着自动驾驶车辆在现代交通系统中集成潜力的增加，确保其在动态环境中的安全导航变得至关重要。为了保证安全性并防止碰撞，自动驾驶汽车必须能够准确预测周围交通代理的轨迹。&lt;h4&gt;背景&lt;/h4&gt;过去十年来，学术界和工业界都投入了大量的精力设计精确的轨迹预测解决方案，这些努力产生了一系列不同的方法，并提出了关于不同方法之间差异以及轨迹预测挑战是否已经完全解决的问题。&lt;h4&gt;目的&lt;/h4&gt;本文回顾了近期大量的轨迹预测方法，并提出了一种分类现有解决方案的方法。此外还提供了一个预测流水线的一般概述，涵盖了输入和输出模式、建模特征及预测范式。&lt;h4&gt;方法&lt;/h4&gt;提出了一个详细的分类法来对现有的预测方法进行分类，并讨论了一些活跃的研究领域。&lt;h4&gt;主要发现&lt;/h4&gt;通过文献综述，指出了当前研究中存在的问题以及未解决的挑战。该论文强调了在轨迹预测方面仍存在的研究空白和未来方向。&lt;h4&gt;结论&lt;/h4&gt;文章探讨并总结了自动驾驶车辆中的轨迹预测领域的现状及其面临的挑战，并提出了一些重要的开放性问题需要进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;随着自主驾驶车辆在现代交通系统中大规模整合的潜力不断增长，确保其在动态环境下的安全导航变得至关重要。为了保证安全性以及防止碰撞的发生，自主驾驶汽车必须具备准确预测周围交通代理轨迹的能力。在过去十年里，学术界和工业界已经投入了大量资源来设计精确的路径预测解决方案，这些努力催生了许多不同的方法，并引发了关于不同技术之间差异以及是否所有相关的挑战都已经被解决的问题。本文对近期大量的轨迹预测方法进行了回顾，并为现有解决方案制定了分类体系。文章还提供了一个涵盖输入输出模式、建模特征及预测范式的预测流程概览。此外，该文讨论了当前活跃的研究领域，回答了一些研究问题，并强调在这一领域里仍然存在的研究缺口和挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As the potential for autonomous vehicles to be integrated on a large scaleinto modern traffic systems continues to grow, ensuring safe navigation indynamic environments is crucial for smooth integration. To guarantee safety andprevent collisions, autonomous vehicles must be capable of accuratelypredicting the trajectories of surrounding traffic agents. Over the pastdecade, significant efforts from both academia and industry have been dedicatedto designing solutions for precise trajectory forecasting. These efforts haveproduced a diverse range of approaches, raising questions about the differencesbetween these methods and whether trajectory prediction challenges have beenfully addressed. This paper reviews a substantial portion of recent trajectoryprediction methods and devises a taxonomy to classify existing solutions. Ageneral overview of the prediction pipeline is also provided, covering inputand output modalities, modeling features, and prediction paradigms discussed inthe literature. In addition, the paper discusses active research areas withintrajectory prediction, addresses the posed research questions, and highlightsthe remaining research gaps and challenges.</description>
      <author>example@mail.com (Nadya Abdel Madjid, Abdulrahman Ahmad, Murad Mebrahtu, Yousef Babaa, Abdelmoamen Nasser, Sumbal Malik, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji)</author>
      <guid isPermaLink="false">2503.03262v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>SCORE: Saturated Consensus Relocalization in Semantic Line Maps</title>
      <link>http://arxiv.org/abs/2503.03254v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 14 figurs, arxiv version for paper submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于语义标注3D线的场景无关且轻量级视觉重定位框架。&lt;h4&gt;背景&lt;/h4&gt;传统的视觉重定位方法难以处理极高的异常值比率（超过99.5%），特别是在语义匹配中出现的一对多模糊情况。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种能够准确估计姿态并在经典共识最大化框架失效时仍能工作的算法。&lt;h4&gt;方法&lt;/h4&gt;引入饱和一致性最大化的形式化方法，并提出了一种快速全局求解器，利用严格的区间分析结果保证了精度和计算效率。此外，还构建了一个用于构造语义3D线图的流水线。&lt;h4&gt;主要发现&lt;/h4&gt;通过在ScanNet++数据集上的广泛实验验证了所提框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;提出的饱和一致性最大化的形式化方法能够在极高的异常值比率下准确估计姿态，并且整个框架结合了鲁棒估算和实用工程见解，具有很高的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原文是：这是提交给IEEE/RSJ IROS 2025的一篇论文的arxiv版本。我们提出了一种场景无关并且轻量级的视觉重定位框架，该框架利用语义标注的3D线作为紧凑的地图表示。在我们的框架中，机器人通过捕获单个图像、提取2D线、将其与地图中的语义相似的3D线关联起来，并解决一个鲁棒的透视-n-线问题来自我定位。为了解决由于语义匹配的一对多模糊性导致异常值比率极高（超过99.5%）的问题，我们引入了饱和一致性最大化(Sat-CM)形式化方法，它在经典共识最大化的框架失效时仍能实现准确的姿态估计。此外，还提出了快速全局求解器以解决所提出的Sat-CM问题，利用严格的区间分析结果确保精度和计算效率的同时性。另外，开发了一条管道来构建使用带姿态的深度图像的语义3D线地图。为了验证我们框架的有效性，并整合了我们在鲁棒估计方面的创新成果以及实用工程见解，在ScanNet++数据集上进行了广泛的实验。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This is the arxiv version for our paper submitted to IEEE/RSJ IROS 2025. Wepropose a scene-agnostic and light-weight visual relocalization framework thatleverages semantically labeled 3D lines as a compact map representation. In ourframework, the robot localizes itself by capturing a single image, extracting2D lines, associating them with semantically similar 3D lines in the map, andsolving a robust perspective-n-line problem. To address the extremely highoutlier ratios~(exceeding 99.5\%) caused by one-to-many ambiguities in semanticmatching, we introduce the Saturated Consensus Maximization~(Sat-CM)formulation, which enables accurate pose estimation when the classic ConsensusMaximization framework fails. We further propose a fast global solver to theformulated Sat-CM problems, leveraging rigorous interval analysis results toensure both accuracy and computational efficiency. Additionally, we develop apipeline for constructing semantic 3D line maps using posed depth images. Tovalidate the effectiveness of our framework, which integrates our innovationsin robust estimation and practical engineering insights, we conduct extensiveexperiments on the ScanNet++ dataset.</description>
      <author>example@mail.com (Haodong Jiang, Xiang Zheng, Yanglin Zhang, Qingcheng Zeng, Yiqian Li, Ziyang Hong, Junfeng Wu)</author>
      <guid isPermaLink="false">2503.03254v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>STORM: Spatial-Temporal Iterative Optimization for Reliable Multicopter Trajectory Generation</title>
      <link>http://arxiv.org/abs/2503.03252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种时空迭代优化框架，以提高四旋翼无人机轨迹规划的效率和安全性。&lt;h4&gt;背景&lt;/h4&gt;目前，无人机轨迹优化问题中存在约束合规性和计算效率提升之间的固有折衷。&lt;h4&gt;目的&lt;/h4&gt;增强无人机轨迹优化性能。&lt;h4&gt;方法&lt;/h4&gt;利用B样条表示无人机轨迹，并通过严格控制点上的约束执行来确保严格的飞行安全；然后推导出一系列通过时空解耦和约束线性化得出的QP-LP子问题；最后采用一种结合指导梯度的迭代优化策略，在不同场景下获得高性能无人机轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果以及实地实验验证了所提出的优化框架在生成既安全又快速的轨迹方面的效率和高性能。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法能够有效地解决无人机轨迹规划中约束合规性和计算效率之间的矛盾，提供了一种有效、高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;高效且安全的轨迹规划对于四旋翼无人飞行器的应用至关重要。当前问题在于如何在满足约束的同时提升计算效率。为改善无人机轨迹优化性能，作者提出了一种时空迭代优化框架。此方法利用B样条表示轨迹，并通过控制点上的严格约束来确保安全性；然后推导了一系列子问题并采用一种结合指导梯度的策略；最终实验结果验证了该框架的有效性与高性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Efficient and safe trajectory planning plays a critical role in theapplication of quadrotor unmanned aerial vehicles. Currently, the inherenttrade-off between constraint compliance and computational efficiencyenhancement in UAV trajectory optimization problems has not been sufficientlyaddressed. To enhance the performance of UAV trajectory optimization, wepropose a spatial-temporal iterative optimization framework. Firstly, B-splinesare utilized to represent UAV trajectories, with rigorous safety assuranceachieved through strict enforcement of constraints on control points.Subsequently, a set of QP-LP subproblems via spatial-temporal decoupling andconstraint linearization is derived. Finally, an iterative optimizationstrategy incorporating guidance gradients is employed to obtainhigh-performance UAV trajectories in different scenarios. Both simulation andreal-world experimental results validate the efficiency and high-performance ofthe proposed optimization framework in generating safe and fast trajectories.Our source codes will be released for community reference athttps://hitsz-mas.github.io/STORM</description>
      <author>example@mail.com (Jinhao Zhang, Zhexuan Zhou, Wenlong Xia, Youmin Gong, Jie Mei)</author>
      <guid isPermaLink="false">2503.03252v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile Sensing on Humanoid Robots</title>
      <link>http://arxiv.org/abs/2503.03234v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 25. 8 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了使用集成在人形机器人手臂上的基于织物的大规模触觉传感器的社会手势识别系统。&lt;h4&gt;背景&lt;/h4&gt;人类能够仅通过触摸传达不同的信息，为使机器人理解社交触感能力增加了一种新的沟通方式。&lt;h4&gt;目的&lt;/h4&gt;构建一个社会手势数据集并提取用于分类的时间特征，以更好地了解人与机器人的社交互动，并促进更加自然和有效的交流。&lt;h4&gt;方法&lt;/h4&gt;利用多参与者建立了一个社会手势数据集，并收集了在实际环境中应用于人形机器人上的真实世界数据。&lt;h4&gt;主要发现&lt;/h4&gt;该系统为人类与机器人的社交触感提供了有价值的见解，有助于推动更自然且有效的人机交互系统的发展。&lt;h4&gt;结论&lt;/h4&gt;通过使用集成在人形机器人手臂上的基于织物的大规模触觉传感器，可以实现对社会手势的有效识别和理解。&lt;h4&gt;翻译&lt;/h4&gt;人类能够仅通过触摸传达不同的信息。为使机器人具有理解社交触摸的能力，增加了一种新的沟通方式。本文介绍了一个使用集成在人形机器人手臂上的基于织物的大规模触觉传感器的社会手势识别系统。我们建立了包含多个参与者的社会手势数据集，并提取了时间特征进行分类。通过在人形机器人的实际环境中收集数据，我们的系统提供了人类与机器人社交触摸有价值的见解，进一步推进了更自然和有效的沟通的人机交互系统的开发。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans are able to convey different messages using only touch. Equippingrobots with the ability to understand social touch adds another modality inwhich humans and robots can communicate. In this paper, we present a socialgesture recognition system using a fabric-based, large-scale tactile sensorintegrated onto the arms of a humanoid robot. We built a social gesture datasetusing multiple participants and extracted temporal features for classification.By collecting real-world data on a humanoid robot, our system provides valuableinsights into human-robot social touch, further advancing the development ofspHRI systems for more natural and effective communication.</description>
      <author>example@mail.com (Dakarai Crowder, Kojo Vandyck, Xiping Sun, James McCann, Wenzhen Yuan)</author>
      <guid isPermaLink="false">2503.03234v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>OpenGV 2.0: Motion prior-assisted calibration and SLAM with vehicle-mounted surround-view systems</title>
      <link>http://arxiv.org/abs/2503.03230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于优化的解决方案，用于车载全景相机系统的视觉SLAM。&lt;h4&gt;背景&lt;/h4&gt;车载全景相机系统通常只配备朝向一个方向的一台相机，并且视场重叠有限。&lt;h4&gt;目的&lt;/h4&gt;针对上述问题，文章提出了三个优化模块来解决实际在线校准、可靠的前端初始化和准确的后端优化问题。&lt;h4&gt;方法&lt;/h4&gt;提出的三种优化模块共同利用了与乘客车辆运动特性相关的运动先验知识。这些模块在避免常见于Ackermann运动中的部分不可观察性方面表现出色。&lt;h4&gt;主要发现&lt;/h4&gt;通过深入的消融研究，验证了所提出的方法的有效性和优越性，并且通过应用到具有挑战性的大规模公开数据集上进一步证实了整个框架的实际有效性。&lt;h4&gt;结论&lt;/h4&gt;该模块构建了一个专为Ackermann车辆在城市环境中运行而设计的新全景相机SLAM系统。接受后，整个框架将作为OpenGV库扩展的一部分开源发布。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了一种基于优化的解决方案，用于车载全景相机系统的视觉SLAM问题。针对这些问题，作者提出了三个模块：通过简单的双视图几何实现外定向的实际在线校准；相对位移的可靠前端初始化；以及使用连续时间轨迹模型进行准确后端优化。这些模块利用了与乘客车辆运动特性相关的运动先验知识，并且在解决常见于Ackermann运动中的部分不可观察性方面表现出色。通过深入研究，验证了整个框架的有效性和优越性，并成功应用于公开的大型数据集上。论文接受后，该框架将作为OpenGV库扩展的一部分开源发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The present paper proposes optimization-based solutions to visual SLAM with avehicle-mounted surround-view camera system. Owing to their original use-case,such systems often only contain a single camera facing into either directionand very limited overlap between fields of view. Our novelty consist of threeoptimization modules targeting at practical online calibration of exteriororientations from simple two-view geometry, reliable front-end initializationof relative displacements, and accurate back-end optimization using acontinuous-time trajectory model. The commonality between the proposed modulesis given by the fact that all three of them exploit motion priors that arerelated to the inherent non-holonomic characteristics of passenger vehiclemotion. In contrast to prior related art, the proposed modules furthermoreexcel in terms of bypassing partial unobservabilities in the transformationvariables that commonly occur for Ackermann-motion. As a further contribution,the modules are built into a novel surround-view camera SLAM system thatspecifically targets deployment on Ackermann vehicles operating in urbanenvironments. All modules are studied in the context of in-depth ablationstudies, and the practical validity of the entire framework is supported by asuccessful application to challenging, large-scale publicly available onlinedatasets. Note that upon acceptance, the entire framework is scheduled foropen-source release as part of an extension of the OpenGV library.</description>
      <author>example@mail.com (Kun Huang, Yifu Wang, Si'ao Zhang, Zhirui Wang, Zhanpeng Ouyang, Zhenghua Yu, Laurent Kneip)</author>
      <guid isPermaLink="false">2503.03230v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>Embodied Escaping: End-to-End Reinforcement Learning for Robot Navigation in Narrow Environment</title>
      <link>http://arxiv.org/abs/2503.03208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种基于强化学习的自适应脱离模型，该模型通过高效的动作掩码来解决机器人清扫器在拥挤和狭窄环境中遇到死区问题时的传统规划方法失效的问题。&lt;h4&gt;背景&lt;/h4&gt;自主导航是室内环境下机器人吸尘器的基本任务。由于其核心功能是在整个区域进行清洁，因此机器人不可避免地会遇到复杂环境约束、高维搜索空间以及高度复杂的机动操作导致的死区问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的脱离模型和训练策略来提高机器人从死区内成功逃脱的能力，并减少碰撞次数。&lt;h4&gt;方法&lt;/h4&gt;[{'嵌入式逃逸模型': '利用基于强化学习的政策并结合高效的行动掩码来解决死区问题。'}, {'混合培训策略': '为了解决训练过程中稀疏奖励的问题，引入了一种混合训练策略以提高学习效率。'}, {'新的动作表示方法': '设计了一种新的动作表达方式，通过统一的转弯半径重新塑造离散的动作空间，有效地处理冗余和无效的操作选择。'}, {'行动掩码策略': '开发了一种行动掩码策略来快速选择有效操作，在精度与效率之间实现平衡。'}]&lt;h4&gt;主要发现&lt;/h4&gt;['在一系列真实世界实验中，配备了激光雷达、惯性测量单元以及双轮编码器的机器人成功地从多个难度等级中的死区逃脱。', '相较于其他路径规划和强化学习方法，所提出的方法在成功率和碰撞避免方面表现优越。']&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于强化学习的动作掩码策略能有效帮助机器人吸尘器解决复杂环境中的死区问题，并且在实际测试中展示了卓越的性能。&lt;h4&gt;翻译&lt;/h4&gt;自主导航对于室内环境中运行的自动扫地机器人的功能至关重要，尤其是在它们需要清扫拥挤和狭窄区域的时候。由于存在复杂的环境限制、高维搜索空间以及困难的动作需求，现有规划方法往往无法有效解决死区问题。为了克服这些挑战，本文提出了一种基于强化学习的新模型，结合高效的行动掩码策略来优化机器人在复杂条件下的逃脱能力。研究还引入了混合训练政策以解决训练过程中稀疏奖励的问题，并设计了一个新的动作表示法以简化操作选择，同时开发出一种有效筛选可行动的算法，平衡精确度与效率。实验表明，在装备有激光测距仪、惯性测量单元以及双轮编码器的真实场景中，该机器人能够有效地从各种难度级别的死区逃脱出来。比较测试显示了在成功率和碰撞避免方面本方法比其他路径规划或强化学习策略更加优越。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-03-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous navigation is a fundamental task for robot vacuum cleaners inindoor environments. Since their core function is to clean entire areas, robotsinevitably encounter dead zones in cluttered and narrow scenarios. Existingplanning methods often fail to escape due to complex environmental constraints,high-dimensional search spaces, and high difficulty maneuvers. To address thesechallenges, this paper proposes an embodied escaping model that leveragesreinforcement learning-based policy with an efficient action mask for dead zoneescaping. To alleviate the issue of the sparse reward in training, we introducea hybrid training policy that improves learning efficiency. In handlingredundant and ineffective action options, we design a novel actionrepresentation to reshape the discrete action space with a uniform turningradius. Furthermore, we develop an action mask strategy to select valid actionquickly, balancing precision and efficiency. In real-world experiments, ourrobot is equipped with a Lidar, IMU, and two-wheel encoders. Extensivequantitative and qualitative experiments across varying difficulty levelsdemonstrate that our robot can consistently escape from challenging dead zones.Moreover, our approach significantly outperforms compared path planning andreinforcement learning methods in terms of success rate and collisionavoidance.</description>
      <author>example@mail.com (Han Zheng, Jiale Zhang, Mingyang Jiang, Peiyuan Liu, Danni Liu, Tong Qin, Ming Yang)</author>
      <guid isPermaLink="false">2503.03208v1</guid>
      <pubDate>Thu, 06 Mar 2025 22:05:26 +0800</pubDate>
    </item>
    <item>
      <title>RoboBERT: An End-to-end Multimodal Robotic Manipulation Model</title>
      <link>http://arxiv.org/abs/2502.07837v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Embodied intelligence融合了多种模态，使代理能够同时理解图像、语言和行动。然而，现有的模型依赖于额外的数据集或广泛的预训练以最大化性能提升，这需要大量的训练时间和昂贵的硬件成本。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态机器人模型通常需要额外的数据集或大规模的基础模型来达到高性能，并且消耗大量资源。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端机器人操作模型RoboBERT及其独特的训练策略，旨在提高效率并减少对大型数据集和基础模型的需求。&lt;h4&gt;方法&lt;/h4&gt;该模型使用基于CNN的扩散策略，通过分离不同模态的训练过程来增强和稳定模型的有效性。同时强调数据增强的重要性，并验证了多种技术以显著提升性能。&lt;h4&gt;主要发现&lt;/h4&gt;RoboBERT在CALVIN基准测试中的ABCD → D任务中实现了4.52的平均长度，创下了新的SOTA记录；当应用于真实机器人时，该模型展示了比其他使用相同数据训练的方法更高的成功率。&lt;h4&gt;结论&lt;/h4&gt;通过这些概念和方法论，RoboBERT表现出广泛的灵活性和兼容性，并为轻量级多模态机器人模型的发展做出了重要贡献。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新的端到端的机器人操作模型——RoboBERT及其独特的培训策略。该模型旨在解决现有模型由于需要额外的数据集或大量预训练而导致的时间和资源消耗问题，通过采用基于CNN的扩散政策，并强调数据增强的重要性来提高性能。实验结果表明，RoboBERT在基准测试中取得了新纪录的成功率，并且在实际应用中也优于其他方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-11&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence integrates multiple modalities, enabling agents tounderstand images, language, and actions simultaneously. However, existingmodels always depend on additional datasets or extensive pre-training tomaximize performance improvements, consuming abundant training time andexpensive hardware cost. To tackle this issue, we present RoboBERT, a novelend-to-end robotic manipulation model integrated with a unique trainingstrategy. This model utilizes a CNN-based diffusion policy, enhancing andstabilizing the effectiveness of this model by separating training processesfor different modalities. It also underscores the importance of dataaugmentation, verifying various techniques to significantly boost performance.Unlike models that depend on extra data or large foundation models, RoboBERTachieves a highly competitive success rate while using only language-labeledexpert demonstrations and maintaining a relatively smaller model size.Specifically, RoboBERT achieves an average length of 4.52 on the CALVINbenchmark for \(ABCD \rightarrow D\) task, setting a new state-of-the-art(SOTA) record. Furthermore, when tested on a real robot, the model demonstratessuperior performance, achieving a higher success rate than other methodstrained with the same data. We propose that these concepts and methodologies ofRoboBERT demonstrate extensive versatility and compatibility, contributingsignificantly to the development of lightweight multimodal robotic models. Thecode can be accessed on https://github.com/PeterWangsicheng/RoboBERT</description>
      <author>example@mail.com (Sicheng Wang, Jianhua Shan, Jianwei Zhang, Haozhang Gao, Hailiang Han, Yipeng Chen, Kang Wei, Chengkun Zhang, Kairos Wong, Jie Zhao, Lei Zhao, Bin Fang)</author>
      <guid isPermaLink="false">2502.07837v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
  <item>
      <title>Probing a Quarkophobic ${\mathbf{W}}^\prime$ at the High-Luminosity LHC via Vector Boson Fusion and Lorentz-Equivariant Point Cloud Learning</title>
      <link>http://arxiv.org/abs/2502.16630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究通过弱玻色子融合方式探测标准模型之外的W'玻色子生产，使用点云学习技术并引入新的洛伦兹等变几何代数变换器提高信号敏感度。&lt;h4&gt;背景&lt;/h4&gt;在标准模型中添加一个质量较大的、几乎不与夸克耦合的带电矢量规范玻色子W'可以解决诸如B介子异常和W玻色子质量测量差异等问题。&lt;h4&gt;目的&lt;/h4&gt;通过弱相互作用过程研究W'玻色子的产生，利用大型强子对撞机中的质子-质子碰撞数据进行研究，并采用新的学习技术提高检测信号的能力。&lt;h4&gt;方法&lt;/h4&gt;在一种简化模型中工作，该模型假设W'玻色子具有较大的衰变宽度并考虑两个喷注、大的缺失横贯动量和一个轻味道的最终态。使用点云学习技术中的新洛伦兹等变几何代数变换器。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，新型方法相比于传统方法显著提高了信号敏感度。&lt;h4&gt;结论&lt;/h4&gt;引入新的计算工具（即Lorentz-Equivariant Geometric Algebra Transformer）使得探测W'玻色子更加有效。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The addition of a heavy charged vector gauge boson ${\mathbf{W}}^\prime$ tothe Standard Model (SM) with negligible quark couplings ("quarkophobic") andtriple gauge couplings can address issues with the SM, such as the B-mesonanomalies and recent discrepancies in the W boson mass measurements. We presenta phenomenology study probing ${\mathbf{W}}^\prime$ production through weakboson fusion in proton-proton collisions at the Large Hadron Collider. Weoperate under a simplified model with a large ${\mathbf{W}}^\prime$ decay widthand consider final states with two jets, large missing transverse momentum, andone light lepton. Notably, we use point cloud learning for the first time in aBSM search$\unicode{x2014}$specifically, a novel Lorentz-Equivariant GeometricAlgebra Transformer$\unicode{x2014}$providing significant improvement in signalsensitivity compared to traditional methods.</description>
      <author>example@mail.com (U. S. Qureshi, A. Gurrola, J. D. Ruiz-Álvarez)</author>
      <guid isPermaLink="false">2502.16630v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Audio Visual Segmentation Through Text Embeddings</title>
      <link>http://arxiv.org/abs/2502.16359v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为AV2T-SAM的新框架，该框架将音频特征与预训练的文本提示式SAM的文本嵌入空间连接起来。&lt;h4&gt;背景&lt;/h4&gt;音频-视觉分割（AVS）的目标是定位和从视频帧中分离出发出声音的对象。由于手工注释昂贵，研究者面临数据集有限的问题。&lt;h4&gt;目的&lt;/h4&gt;通过利用丰富的文本图像配对数据集中学习到的多模态对应关系来增强视听对齐，并提出一种新特征以强调音频和视觉模式之间的共享语义同时过滤无关噪声。&lt;h4&gt;方法&lt;/h4&gt;将预训练模型SAM与声音提示结合，使用跨模式语义对齐的方法来改进AVS任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过在AVSBench数据集上的实验显示了在两个数据集上都达到了最新的性能。该方法有效地利用了预训练的分割模型以及跨模态语义对齐。&lt;h4&gt;结论&lt;/h4&gt;提出的AV2T-SAM框架解决了现有音频-视觉分割技术面临的挑战，特别是在有限的数据集限制下学习视听关系的问题。&lt;h4&gt;翻译&lt;/h4&gt;提出了一种新的AVS框架，该框架使用SAM模型，并通过引入一种新的特征来改进其在处理声音源对象分割任务上的能力。此方法利用了跨模式语义对齐，并且实验证明了它能够有效地提高模型的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The goal of Audio-Visual Segmentation (AVS) is to localize and segment thesounding source objects from the video frames. Researchers working on AVSsuffer from limited datasets because hand-crafted annotation is expensive.Recent works attempt to overcome the challenge of limited data by leveragingthe segmentation foundation model, SAM, prompting it with audio to enhance itsability to segment sounding source objects. While this approach alleviates themodel's burden on understanding visual modality by utilizing pre-trainedknowledge of SAM, it does not address the fundamental challenge of the limiteddataset for learning audio-visual relationships. To address these limitations,we propose \textbf{AV2T-SAM}, a novel framework that bridges audio featureswith the text embedding space of pre-trained text-prompted SAM. Our methodleverages multimodal correspondence learned from rich text-image paireddatasets to enhance audio-visual alignment. Furthermore, we introduce a novelfeature, $\mathbf{\textit{\textbf{f}}_{CLIP} \odot\textit{\textbf{f}}_{CLAP}}$, which emphasizes shared semantics of audio andvisual modalities while filtering irrelevant noise. Experiments on the AVSBenchdataset demonstrate state-of-the-art performance on both datasets of AVSBench.Our approach outperforms existing methods by effectively utilizing pretrainedsegmentation models and cross-modal semantic alignment.</description>
      <author>example@mail.com (Kyungbok Lee, You Zhang, Zhiyao Duan)</author>
      <guid isPermaLink="false">2502.16359v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Robust Deterministic Policy Gradient for Disturbance Attenuation and Its Application to Quadrotor Control</title>
      <link>http://arxiv.org/abs/2502.21057v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种名为Robust Deterministic Policy Gradient (RDPG) 的强化学习算法，该算法将H无穷控制问题转化为一个零和动态博弈，并通过深度确定性策略梯度(DPG)及其深度强化学习版本进行训练。为实际应用引入了RDDPG算法，利用深层神经网络架构并结合TD3技术来提高稳定性和学习效率。&lt;h4&gt;背景&lt;/h4&gt;在实际控制系统中，由于系统模型中的不确定性以及外部扰动的存在，识别最优控制策略面临重大挑战。传统的H无穷控制方法虽然广泛应用于设计鲁棒控制器以减轻干扰影响，但往往需要复杂的计算资源和高强度的计算能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于强化学习的方法来解决实际控制系统中由于不确定性和外部干扰导致的设计复杂性问题，并提高控制器的稳定性与实时性能。&lt;h4&gt;方法&lt;/h4&gt;将H无穷控制问题建模为一个两人零和动态博弈，其中一方试图最小化成本而另一方则最大化。采用确定性策略梯度(DPG)及其深度强化学习版本来训练鲁棒控制策略，进而提出了一种名为RDDPG的算法，并在无人机路径跟踪任务中验证了该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法能够在扰动环境下实现精确实时的目标追踪，在对比测试中显示出了比其他控制方法更高的抗干扰性能。&lt;h4&gt;结论&lt;/h4&gt;基于强化学习的RDPG和RDDPG为实际控制系统中的鲁棒控制器设计提供了一种有效且计算效率高的解决方案，尤其适用于需要应对动态变化环境的应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Practical control systems pose significant challenges in identifying optimalcontrol policies due to uncertainties in the system model and externaldisturbances. While $H_\infty$ control techniques are commonly used to designrobust controllers that mitigate the effects of disturbances, these methodsoften require complex and computationally intensive calculations. To addressthis issue, this paper proposes a reinforcement learning algorithm calledRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$control problem as a two-player zero-sum dynamic game. In this formulation, oneplayer (the user) aims to minimize the cost, while the other player (theadversary) seeks to maximize it. We then employ deterministic policy gradient(DPG) and its deep reinforcement learning counterpart to train a robust controlpolicy with effective disturbance attenuation. In particular, for practicalimplementation, we introduce an algorithm called robust deep deterministicpolicy gradient (RDDPG), which employs a deep neural network architecture andintegrates techniques from the twin-delayed deep deterministic policy gradient(TD3) to enhance stability and learning efficiency. To evaluate the proposedalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked withfollowing a predefined path in a disturbance-prone environment. Theexperimental results demonstrate that the proposed method outperforms othercontrol approaches in terms of robustness against disturbances, enablingprecise real-time tracking of moving targets even under severe disturbanceconditions.</description>
      <author>example@mail.com (Taeho Lee, Donghwan Lee)</author>
      <guid isPermaLink="false">2502.21057v1</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Evaluating the Robustness of LiDAR Point Cloud Tracking Against Adversarial Attack</title>
      <link>http://arxiv.org/abs/2410.20893v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文研究了基于神经网络的LiDAR点云跟踪模型在对抗攻击下的鲁棒性，重点关注其在白盒和黑盒攻击策略中的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR点云跟踪模型往往忽视了抗干扰能力的重要性，而仅仅关注性能提升。然而，在面对对抗攻击、领域转换或数据损坏等问题时，这些模型表现出严重的脆弱性。&lt;h4&gt;目的&lt;/h4&gt;研究如何提高基于神经网络的LiDAR点云跟踪模型在对抗攻击下的鲁棒性，并提出一种新的黑盒攻击策略方法：目标感知扰动生成(TAPG)算法。&lt;h4&gt;方法&lt;/h4&gt;{'白盒攻击': '为各种跟踪范式定制特定损失函数，扩展了现有FGSM、C&amp;W和PGD等方法到点云领域。', '黑盒攻击': '引入TAPG算法，通过稀疏性约束和随机子向量因子化技术提高转移能力。该算法旨在实现高攻击性能的同时保持低感知度。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，先进的跟踪方法在对抗白盒和黑盒攻击时存在显著脆弱性。&lt;h4&gt;结论&lt;/h4&gt;研究强调了增强LiDAR点云跟踪模型鲁棒性的必要性，并提出了一种新的平衡有效性和隐蔽性的TAPG算法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we delve into the robustness of neural network-based LiDARpoint cloud tracking models under adversarial attacks, a critical aspect oftenoverlooked in favor of performance enhancement. These models, despiteincorporating advanced architectures like Transformer or Bird's Eye View (BEV),tend to neglect robustness in the face of challenges such as adversarialattacks, domain shifts, or data corruption. We instead focus on the robustnessof the tracking models under the threat of adversarial attacks. We begin byestablishing a unified framework for conducting adversarial attacks within thecontext of 3D object tracking, which allows us to thoroughly investigate bothwhite-box and black-box attack strategies. For white-box attacks, we tailorspecific loss functions to accommodate various tracking paradigms and extendexisting methods such as FGSM, C\&amp;W, and PGD to the point cloud domain. Inaddressing black-box attack scenarios, we introduce a novel transfer-basedapproach, the Target-aware Perturbation Generation (TAPG) algorithm, with thedual objectives of achieving high attack performance and maintaining lowperceptibility. This method employs a heuristic strategy to enforce sparseattack constraints and utilizes random sub-vector factorization to bolstertransferability. Our experimental findings reveal a significant vulnerabilityin advanced tracking methods when subjected to both black-box and white-boxattacks, underscoring the necessity for incorporating robustness againstadversarial attacks into the design of LiDAR point cloud tracking models.Notably, compared to existing methods, the TAPG also strikes an optimal balancebetween the effectiveness of the attack and the concealment of theperturbations.</description>
      <author>example@mail.com (Shengjing Tian, Yinan Han, Xiantong Zhao, Bin Liu, Xiuping Liu)</author>
      <guid isPermaLink="false">2410.20893v2</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion</title>
      <link>http://arxiv.org/abs/2502.19697v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了一种新的攻击方法Attribute-aware Prompt Attack (AP-Attack)，该方法利用视觉语言模型(VLM)的图像文本对齐能力，通过对行人属性特定的文字嵌入进行破坏来显式地扰乱行人图像中的细粒度语义特征。&lt;h4&gt;背景&lt;/h4&gt;人员重识别(re-id)模型在安全监控系统中至关重要。然而，现有的基于VLM（视觉-语言模型）的攻击方法由于过于强调整体表示中的判别性语义，缺乏对综合特征破坏的能力。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的属性感知提示攻击(AP-Attack)方法，该方法旨在通过扰动行人图像中特定属性的文字嵌入来增强细粒度语义特征的扰乱效果，并提高对抗样本在不同模型和数据集上的迁移能力。&lt;h4&gt;方法&lt;/h4&gt;设计了文本反转网络以获取个人化的文字描述，这些网络将行人图像映射到表示语义嵌入的伪标记上。训练过程中采用了对比学习方式结合图像与预先定义的文字模板，该模板明确描述了行人的属性特征。&lt;h4&gt;主要发现&lt;/h4&gt;AP-Attack 方法在跨模型和数据集攻击场景中表现出色，其平均Drop Rate比现有方法高出22.9%，展示了极佳的迁移性。&lt;h4&gt;结论&lt;/h4&gt;通过扰乱行人图像中的细粒度语义特征，AP-Attack有效增强了对抗样本的破坏力，并且提高了它们的迁移性能。&lt;h4&gt;翻译&lt;/h4&gt;人员重识别(re-id)模型在安全监控系统中非常重要。最近基于视觉语言模型（VLM）的攻击方法显示出卓越的迁移性，通过攻击VLM中的通用图像和文本特征来探索这些模型的脆弱点。然而，由于过度强调整体表示中的判别性语义，它们缺乏对综合特征的彻底破坏。在这篇论文中，我们引入了属性感知提示攻击（AP-Attack），这是一种新颖的方法，它利用VLM的图像文字对齐能力显式地扰乱行人图像中的细粒度语义特征，通过摧毁特定于属性的文字嵌入来实现这一点。为了获得针对每个个体属性的个性化文本描述，设计了文本反转网络将行人图像映射到表示语义嵌入的伪标记上，并在对比学习方式下进行训练，结合图像和预先定义的好像模板，该模板明确地描述了行人的属性特征。扰动后的良性及对抗性细粒度文本语义使攻击者能够有效地进行全面破坏，从而增强了对抗样本的迁移能力。广泛的实验表明，AP-Attack实现了最先进的迁移性能，在跨模型和数据集的攻击场景中平均Drop Rate比现有方法高出22.9%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (re-id) models are vital in security surveillancesystems, requiring transferable adversarial attacks to explore thevulnerabilities of them. Recently, vision-language models (VLM) based attackshave shown superior transferability by attacking generalized image and textualfeatures of VLM, but they lack comprehensive feature disruption due to theoveremphasis on discriminative semantics in integral representation. In thispaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novelmethod that leverages VLM's image-text alignment capability to explicitlydisrupt fine-grained semantic features of pedestrian images by destroyingattribute-specific textual embeddings. To obtain personalized textualdescriptions for individual attributes, textual inversion networks are designedto map pedestrian images to pseudo tokens that represent semantic embeddings,trained in the contrastive learning manner with images and a predefined prompttemplate that explicitly describes the pedestrian attributes. Inverted benignand adversarial fine-grained textual semantics facilitate attacker ineffectively conducting thorough disruptions, enhancing the transferability ofadversarial examples. Extensive experiments show that AP-Attack achievesstate-of-the-art transferability, significantly outperforming previous methodsby 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios.</description>
      <author>example@mail.com (Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.19697v2</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概述&lt;/h4&gt;介绍了用于户外高空Vision-Language Navigation (VLN)的OpenFly平台，该平台包括工具链和大规模基准数据集。&lt;h4&gt;背景&lt;/h4&gt;室内VLN已经得到了广泛研究，但室外高空VLN由于涉及广阔区域的数据收集难度大而研究不足。&lt;h4&gt;目的&lt;/h4&gt;提出一个完整的工具链和大规模的户外高空VLN数据集来解决现有数据缺乏的问题。&lt;h4&gt;方法&lt;/h4&gt;{'自动化工具链': '开发了高度自动化的工具链用于数据收集，包括点云获取、场景语义分割、飞行轨迹创建及指令生成。', '大规模数据集构建': '利用工具链建立了包含100k条轨迹的大规模户外高空VLN数据集，涵盖了多样化的高度和长度以及18个不同场景。', '视觉数据生成': '采用多种渲染引擎和技术（如Unreal Engine、GTA V、Google Earth及3D Gaussian Splatting）生成高质量的视觉数据。', '模型开发': '提出了关键帧感知的VLN模型OpenFly-Agent，输入语言指令、当前观察值和历史关键帧，并直接输出飞行动作。'}&lt;h4&gt;主要发现&lt;/h4&gt;平台及其模型在多项实验中展示了其优越性。&lt;h4&gt;结论&lt;/h4&gt;工具链、数据集及代码将开源以促进相关研究的发展。&lt;h4&gt;翻译&lt;/h4&gt;Vision-Language Navigation (VLN)旨在通过利用语言指令和视觉线索来引导环境中的代理，这在具身AI领域扮演着重要角色。虽然室内VLN已经得到了广泛的研究，但室外高空VLN由于涉及广阔区域的数据收集难度大而鲜少有人研究。为解决这一问题，我们提出了一种开放式飞行平台OpenFly，包括一个灵活的工具链和大规模基准数据集。首先，开发了一个高度自动化的工具链用于数据采集，实现了点云获取、场景语义分割、飞行轨迹创建及指令生成等自动化过程。其次，在此基础上建立了一个包含10万条不同高度与长度路线的大规模户外高空VLN数据集，并利用多种渲染引擎（如Unreal Engine, GTA V, Google Earth）和技术（如3D Gaussian Splatting）生成高视觉质量的数据，其中3D GS支持真实到仿真渲染。最后，我们提出了关键帧感知的VLN模型OpenFly-Agent，该模型根据语言指令、当前观察值和历史关键帧输出飞行动作。通过全面分析及实验表明了我们的平台及其模型的优势，并计划开放工具链、数据集以及相关代码以促进进一步的研究进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种名为CalibRefine的全自动、无目标且在线校准框架，该框架可以处理原始LiDAR点云和相机图像，通过一系列阶段实现精确的多传感器校准。&lt;h4&gt;背景&lt;/h4&gt;在诸如自动驾驶汽车、机器人技术及智能交通系统等应用中，准确的多传感器校准至关重要。现有的激光雷达-摄像机校准方法通常依赖于手动放置的目标物、初步参数估计或密集的数据预处理，这限制了它们在实际环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;旨在提出一种全自动化的校准框架，该框架不依赖人工目标且可以在线完成，并能够直接处理原始激光雷达点云和相机图像数据。&lt;h4&gt;方法&lt;/h4&gt;CalibRefine由四个阶段组成：（1）一个共同特征鉴别器，利用自动检测到的对象的相对位置、外观嵌入以及语义类别生成可靠的激光雷达-摄像机对应关系；（2）基于粗略同构变换的校准；（3）迭代细化，在更多数据帧可用时逐步提高对齐精度；（4）注意力机制改进，通过使用视觉变压器和交叉注意机制解决非平面失真问题。&lt;h4&gt;主要发现&lt;/h4&gt;CalibRefine在两个城市交通数据集上进行了广泛的实验，结果显示它能够以最少的人工干预实现高精度校准结果，优于无目标的现有方法，并与手动调优基线保持竞争性或超越。&lt;h4&gt;结论&lt;/h4&gt;研究强调了如何通过稳健的对象级特征匹配以及迭代和自监督的注意力机制调整，在复杂的真实世界条件下实现一致的传感器融合，而无需地面真相校准矩阵或复杂的预处理。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/radar-lab/Lidar_Camera_Automatic_Calibration&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Github  page:https://github.com/justacar/Plane-DUSt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Plane-DUSt3R是一种利用3D基础模型DUSt3R进行多视角房间布局估计的新方法。&lt;h4&gt;背景&lt;/h4&gt;由于多视图几何的复杂性，从多视角图像中推断房间布局的研究较少。传统的结构从运动过程中涉及多个步骤（如相机内部和外部参数估计、图像匹配和三角测量）。然而，在3D重建领域，最近出现的3D基础模型改变了传统方法。&lt;h4&gt;目的&lt;/h4&gt;介绍并改进一种基于DUSt3R框架的方法——Plane-DUSt3R，以解决多视角房间布局估计的问题。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R通过在房间布局数据集（Structure3D）上进行微调，并修改目标函数来估计结构平面。它采用单步后处理步骤和2D检测结果生成一致且简洁的结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Plane-DUSt3R不仅在合成数据集中优于现有最佳方法，还在具有不同图像风格（如卡通）的现实世界数据中表现出鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;Plane-DUSt3R提供了一种简化流程、减少误差累积的端到端解决方案，并能处理多视角图像。此研究拓展了房间布局估计领域的可能性。&lt;h4&gt;翻译&lt;/h4&gt;从多个视角的图片中推断出房间的布局由于涉及到复杂的多视图几何问题，因此研究较少。传统的结构从运动过程需要一系列步骤（例如相机内参和外参估计、图像匹配以及三角测量）。然而，在3D重建领域，最近出现的像DUSt3R这样的3D基础模型改变了这一传统流程，使其向端到端的单步方法转变。为了解决多视角房间布局估计的问题，我们引入了Plane-DUSt3R，这是一种利用3D基础模型DUSt3R的方法。该方法基于DUSt3R框架，并在房间布局数据集（Structure3D）上进行了微调以估计结构平面。它通过单一的后处理步骤和2D检测结果生成一致且简洁的结果。与依赖单视角或全景图象的方法不同，Plane-DUSt3R能够处理多视角图像并提供了一种简化的、端到端的解决方案来简化过程，并减少误差积累。实验结果显示，相较于现有最佳方法，在合成数据集上，该方法表现更优；在不同的真实世界数据集中（例如卡通风格），该方法表现出稳健性和有效性。我们的代码可以在https://github.com/justacar/Plane-DUSt3R中获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon. Our code is available at:https://github.com/justacar/Plane-DUSt3R</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v3</guid>
      <pubDate>Wed, 05 Mar 2025 14:36:05 +0800</pubDate>
    </item>
    <item>
      <title>Robust Prediction of Frictional Contact Network in Near-Jamming Suspensions Employing Deep Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18743v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种基于图神经网络（GNN）的机器学习方法，用于预测颗粒悬浮液中的摩擦接触网络（FCN），特别是在接近拥堵条件下的性能。这种方法在数据驱动模拟训练中表现出色，并且能够准确预测不同参数组合下的FCN。&lt;h4&gt;背景&lt;/h4&gt;细颗粒分散于牛顿流体中的悬浮物粘度在其接近拥挤状态时发散，这主要是由粒子间接触微观结构决定的。这种联系网络是导致固体化的行为的关键。应力传输和网络拓扑对粒子相对运动的限制非常敏感。&lt;h4&gt;目的&lt;/h4&gt;开发一种预测FCN的有效机器学习方法，尤其是在靠近拥堵条件下的情况。&lt;h4&gt;方法&lt;/h4&gt;使用了一种称为Deep Graph Convolutional Network（DeepGCN）的方法，并且展示了在不同参数组合下具有良好的泛化和外推能力。该研究包括从半稀释状态到拥挤状态的广泛相空间，同时系统地改变剪切应力、堆积分数以及滑动和滚动摩擦。&lt;h4&gt;主要发现&lt;/h4&gt;通过训练数据驱动模拟，DeepGCN能够准确预测不同流参数和相空间条件下的FCN。这些结果展示了在材料科学及相关领域创新且可转移的技术途径的潜力。&lt;h4&gt;结论&lt;/h4&gt;这项研究为预测颗粒系统性质提供了新的技术方法，特别是在拥挤条件下，这可能推动材料科学及其相关领域的进展。&lt;h4&gt;翻译&lt;/h4&gt;悬浮液中由细小颗粒分散于牛顿流体中的粘度在接近堆积极限时发散。这种宏观行为受到粒子接触微观结构的支配，通过摩擦接触网络（FCN）来实现。FCN是由机械负载支撑点组成的，在接近拥挤转变时导致刚性出现。应力传递和网络拓扑反过来取决于颗粒相对运动限制的敏感特性。尽管其重要性显而易见，但由于实验和计算障碍的存在，预测FCN特别是靠近拥挤条件下的情况仍然具有挑战性。这项研究提出了一个基于图神经网络（GNN）的成本效益机器学习方法来预测FCN，并且通过使用DeepGCN展示了在不同流参数和相空间条件下准确预测FCN的能力。该研究覆盖了广泛的相空间，从半稀释到拥挤状态以及瞬态到稳定状态，并系统地改变剪切应力、堆积分数及滑动和滚动摩擦等参数。这项研究的结果为颗粒系统的性质预测提供了创新且可转移的技术途径，为进一步发展材料科学及相关领域开辟新的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The viscosity of the suspension consisting of fine particles dispersed in aNewtonian liquid diverges close to the jamming packing fraction. The contactmicrostructure in suspensions governs this macroscopic behavior in the vicinityof jamming through a frictional contact network (FCN). FCN is composed ofmechanical load-bearing contacts that lead to the emergence of rigidity nearthe jamming transition. The stress transmission and network topology, in turn,depend sensitively on constraints on the relative motion of the particles.Despite their significance, predicting the FCN, especially close to jammingconditions, remains challenging due to experimental and computationalimpediments. This study introduces a cost-effective machine learning approachto predict the FCN using a graph neural network (GNN), which inherentlycaptures hidden features and underlying patterns in dense suspension by mappinginterparticle interactions. Employing a variation of GNN called the Deep GraphConvolutional Network (DeepGCN) trained on data-driven simulations, this studydemonstrates robust generalization and extrapolation capabilities, accuratelypredicting FCNs in systems with divergent flow parameters and phase spaces,despite each being trained exclusively on a single condition. The study coversa wide range of phase space, from semi-dilute to jammed states, spanningtransient to steady states, while systematically varying parameters such asshear stress (${\sigma}_{xy}$), packing fraction(${\phi}$) and sliding androlling friction (${{\mu}_s, {\mu}_r}$). The results of this research pave theway for innovative transferable techniques in predicting the properties ofparticulate systems, offering new avenues for advancement in material scienceand related fields.</description>
      <author>example@mail.com (Armin Aminimajd, Joao Maia, Abhinendra Singh)</author>
      <guid isPermaLink="false">2502.18743v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
  <item>
      <title>Avat3r: Large Animatable Gaussian Reconstruction Model for High-fidelity 3D Head Avatars</title>
      <link>http://arxiv.org/abs/2502.20220v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://tobias-kirschstein.github.io/avat3r/, Video:  https://youtu.be/P3zNVx15gYs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Avat3r的方法，可以从少量输入图像中生成高质量且可动画化的3D头像。&lt;h4&gt;背景&lt;/h4&gt;传统上，创建逼真的3D头像是一个复杂的过程，需要多视角捕捉设备和昂贵的优化过程。这限制了数字人类替身的应用范围，使其仅限于VFX行业或离线渲染。&lt;h4&gt;目的&lt;/h4&gt;开发一种减少计算需求的方法，使得高质量、可动画化的3D头像可以从少量输入图像中生成。&lt;h4&gt;方法&lt;/h4&gt;{'利用大型重建模型': '使大规模重建模型变得可动画化，并从大量的多视角视频数据集中学习三维人体头部的强大先验知识。', '改进的3D头部重构': '采用来自DUSt3R的位置图和Sapiens的人类基础模型中的泛化特征图来改善3D头部重构。', '实现动画功能': '发现简单的跨注意力到表情代码就足够用于使3D头像可动画化。', '增强鲁棒性': '通过训练时输入不同表情的图像，增强了模型从不一致输入中重建三维头部的能力。'}&lt;h4&gt;主要发现&lt;/h4&gt;Avat3r在少数输入和单个输入场景中的表现优于现有最先进的方法，并展示了广泛的适用性，能够创建来自各种来源（包括智能手机拍摄、单一图片甚至超出领域范围如古董头像）的3D头像。&lt;h4&gt;结论&lt;/h4&gt;通过项目网站https://tobias-kirschstein.github.io/avat3r可查看更多详细信息。该方法在效率和性能上表现出显著优势，为数字人类替身的应用开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的传统创建逼真的3D头像过程需要复杂的多视角捕捉设备以及昂贵的计算资源，在实际应用中受到限制；本文提出了一种名为Avat3r的技术，可以从少量输入图像生成高质量且可动画化的3D头像，并通过训练模型学习泛化特征和不同表情下的鲁棒性重构方法，实现了在效率与性能上的突破。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditionally, creating photo-realistic 3D head avatars requires astudio-level multi-view capture setup and expensive optimization duringtest-time, limiting the use of digital human doubles to the VFX industry oroffline renderings.  To address this shortcoming, we present Avat3r, which regresses ahigh-quality and animatable 3D head avatar from just a few input images, vastlyreducing compute requirements during inference. More specifically, we makeLarge Reconstruction Models animatable and learn a powerful prior over 3D humanheads from a large multi-view video dataset. For better 3D headreconstructions, we employ position maps from DUSt3R and generalized featuremaps from the human foundation model Sapiens. To animate the 3D head, our keydiscovery is that simple cross-attention to an expression code is alreadysufficient. Finally, we increase robustness by feeding input images withdifferent expressions to our model during training, enabling the reconstructionof 3D head avatars from inconsistent inputs, e.g., an imperfect phone capturewith accidental movement, or frames from a monocular video.  We compare Avat3r with current state-of-the-art methods for few-input andsingle-input scenarios, and find that our method has a competitive advantage inboth tasks. Finally, we demonstrate the wide applicability of our proposedmodel, creating 3D head avatars from images of different sources, smartphonecaptures, single images, and even out-of-domain inputs like antique busts.  Project website: https://tobias-kirschstein.github.io/avat3r/</description>
      <author>example@mail.com (Tobias Kirschstein, Javier Romero, Artem Sevastopolsky, Matthias Nießner, Shunsuke Saito)</author>
      <guid isPermaLink="false">2502.20220v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success</title>
      <link>http://arxiv.org/abs/2502.19645v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://openvla-oft.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了用于视觉-语言-动作模型（VLAs）的优化微调策略，提出了一个集成平行解码、连续动作表示等技术的高效微调方案。&lt;h4&gt;背景&lt;/h4&gt;现有的VLAs依赖于预训练的语言和视觉模型，并利用各种机器人数据集展示了强大的任务执行能力。然而，这些模型在面对新环境时需要通过微调来适应，而最佳的微调策略尚未明确。&lt;h4&gt;目的&lt;/h4&gt;探讨不同的动作解码方案、表示方法及学习目标对VLAs性能的影响，提出一种优化微调的方法以改善推理效率和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用OpenVLA作为基准模型，并通过实证分析确定了最佳的动作解码策略、表示方式以及学习目标。提出的OFT框架包括并行解码、动作切块（action chunking）、连续动作表示和基于L1回归的学习目标。&lt;h4&gt;主要发现&lt;/h4&gt;优化后的微调方法显著提高了OpenVLA在LIBERO仿真基准测试中的成功率，同时增加了行动生成的吞吐量；在真实世界评估中，该策略使得OpenVLA能够优于其他VLAs以及其他从头开始训练的模仿学习策略，在平均成功率上提高15%。&lt;h4&gt;结论&lt;/h4&gt;通过引入OFT方法，可以显著改善视觉-语言-动作模型在新环境下的性能和效率，并且提供了一种有效的微调方案。该研究进一步证明了优化微调对于提升这些复杂模型的实际应用效果的重要性。&lt;h4&gt;翻译&lt;/h4&gt;近期的视觉-语言-动作（VLA）模型基于预训练的语言与视觉模型构建，利用多样化的机器人数据集展示出色的任务执行能力、跟随语言指令的能力以及语义泛化。尽管取得了一些成功，但VLAs在面对新机器人设置时仍表现不佳，需要通过微调来获得良好性能。针对这一问题，该研究探讨了关键的VLA适应设计选择，如不同的动作解码方案、表示方法和学习目标，并提出了一个优化微调（OFT）配方。实证分析表明，这种方法提高了推理效率、政策性能以及模型输入输出规范的灵活性。所提出的OpenVLA-OFT在LIBERO仿真基准上达到了新的最佳水平，在四组任务上的平均成功率从76.5%提高到97.1%，同时增加了26倍的动作生成吞吐量。实际评估显示，优化微调方案使得OpenVLA能够成功执行ALOHA双臂机器人上的精细、高频控制任务，并在平均成功率上显著优于其他VLAs和强大的从头开始训练的模仿学习策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent vision-language-action models (VLAs) build upon pretrainedvision-language models and leverage diverse robot datasets to demonstratestrong task execution, language following ability, and semantic generalization.Despite these successes, VLAs struggle with novel robot setups and requirefine-tuning to achieve good performance, yet how to most effectively fine-tunethem is unclear given many possible strategies. In this work, we study key VLAadaptation design choices such as different action decoding schemes, actionrepresentations, and learning objectives for fine-tuning, using OpenVLA as ourrepresentative base model. Our empirical analysis informs an OptimizedFine-Tuning (OFT) recipe that integrates parallel decoding, action chunking, acontinuous action representation, and a simple L1 regression-based learningobjective to altogether improve inference efficiency, policy performance, andflexibility in the model's input-output specifications. We propose OpenVLA-OFT,an instantiation of this recipe, which sets a new state of the art on theLIBERO simulation benchmark, significantly boosting OpenVLA's average successrate across four task suites from 76.5% to 97.1% while increasing actiongeneration throughput by 26$\times$. In real-world evaluations, our fine-tuningrecipe enables OpenVLA to successfully execute dexterous, high-frequencycontrol tasks on a bimanual ALOHA robot and outperform other VLAs ($\pi_0$ andRDT-1B) fine-tuned using their default recipes, as well as strong imitationlearning policies trained from scratch (Diffusion Policy and ACT) by up to 15%(absolute) in average success rate. We release code for OFT and pretrainedmodel checkpoints at https://openvla-oft.github.io/.</description>
      <author>example@mail.com (Moo Jin Kim, Chelsea Finn, Percy Liang)</author>
      <guid isPermaLink="false">2502.19645v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>$Δ$-model correction of Foundation Model based on the models own understanding</title>
      <link>http://arxiv.org/abs/2502.21179v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;摘要描述了一种基于Δ-learning的方法，用于改进通用原子间势能模型在特定材料子类中的应用。&lt;h4&gt;背景&lt;/h4&gt;当前的通用势能模型可能需要针对具体材料进行微调或残差修正。CHGNet是一个典型的例子，它能够在全局结构优化设置中准确预测某些氧化物的能量特性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Δ-learning的方法来改善通用势能模型在特定场景中的表现，并探讨不同聚合方式（如全局、元素分离和原子级别）的效果。&lt;h4&gt;方法&lt;/h4&gt;使用Gaussian Process Regression (GPR)模型作为Δ-model，以CHGNet内部的原子嵌入表示为基础进行修正。这种方法可以为需要精确预测的新材料或环境提供有效的校正方案。&lt;h4&gt;主要发现&lt;/h4&gt;对于铜、银和金表面上硫原子覆盖层的情况，原始CHGNet模型存在不足之处，需要通过基于GPR的Δ-model来进行校正以提高精度。&lt;h4&gt;结论&lt;/h4&gt;通用势能模型在缺乏特定类型原子环境的数据时会表现出误差，这表明了开发更有效的修正方案的重要性。研究还发现其他使用相同训练数据集（如MACE-MP0、SevenNet-0和ORB-v2-only-MPtrj）的通用势能模型也显示出类似的行为。&lt;h4&gt;翻译&lt;/h4&gt;基础材料间的相互作用势能模型可能需要针对具体应用进行微调或残差修正。文中提出了一种基于Δ-learning的方法，通过已嵌入的表示实现这种改进。在全局结构优化设置中使用CHGNet时发现其能够准确描述某些氧化物的能量特性。然而对于金属表面上硫原子覆盖层的情况，则需要利用GPR模型来进行校正以提高预测准确性。研究结果表明了开发更有效的修正方案的重要性，因为其他训练于类似数据集上的通用势能模型也存在相似问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models of interatomic potentials, so called universal potentials,may require fine-tuning or residual corrections when applied to specificsubclasses of materials. In the present work, we demonstrate how suchaugmentation can be accomplished via $\Delta$-learning based on therepresentation already embedded in the universal potentials. The $\Delta$-modelintroduced is a Gaussian Process Regression (GPR) model and various types ofaggregation (global, species-separated, and atomic) of the representationvector are discussed. Employing a specific universal potential, CHGNet [Deng etal., Nat. Mach. Intell. 5, 1031 (2023)], in a global structure optimizationsetting, we find that it correctly describes the energetics of the "8" Cuoxide, which is an ultra-thin oxide film on Cu(111). The universal potentialmodel even predicts a more favorable structure compared to that discussed inrecent DFT-based literature. Moving to sulfur adatom overlayers on Cu(111),Ag(111), and Au(111) the CHGNet model, however, requires corrections. Wedemonstrate that these are efficiently provided via the GPR-based$\Delta$-model formulated on the CHGNet's own internal atomic embeddingrepresentation. The need for corrections is tracked to the scarcity ofmetal-sulfur atomic environments in the materials project database that CHGNetis trained on leading to an overreliance on sulfur-sulfur atomic environments.Other universal potentials trained on the same data, MACE-MP0, SevenNet-0, andORB-v2-only-MPtrj show similar behavior, but with varying degrees of error,demonstrating the general need for augmentation schemes for universal potentialmodels.</description>
      <author>example@mail.com (Mads-Peter Verner Christiansen, Bjørk Hammer)</author>
      <guid isPermaLink="false">2502.21179v1</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19698v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为YoCo的框架，用于生成高质量的3D伪标签以减少户外LiDAR点云三维实例分割任务中的人工标注工作。&lt;h4&gt;背景&lt;/h4&gt;户外LiDAR点云三维实例分割是自动驾驶中的关键任务，但由于需要大量人工劳动进行标注，训练模型变得非常困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用少量粗略点击注释生成高质量伪标签的方法，以减少标注成本并提高模型性能。&lt;h4&gt;方法&lt;/h4&gt;{'YoCo框架': '1. 利用视觉基础模型结合点云的几何约束来增强伪标签生成；2. 设计了一个基于时空的标签更新模块，利用相邻帧的预测结果，并考虑点云固有的密度变化特性（近处密集、远处稀疏）；3. 提出一个IoU引导增强模块，替换掉置信度低和IoU低的伪标签。', '效果': '在Waymo数据集上的实验表明，YoCo框架具有显著的效果，达到了弱监督方法中的最佳性能，并且超越了完全监督的方法Cylinder3D。此外，YoCo还适用于多种网络，在仅使用少量标注数据的情况下实现了与完全监督方法相当的性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过结合视觉基础模型和点云几何约束可以有效生成高质量伪标签；2. 基于时空的标签更新模块能够利用相邻帧的信息提高标签质量。&lt;h4&gt;结论&lt;/h4&gt;YoCo框架在减少标注工作量的同时，提高了户外LiDAR点云三维实例分割任务中的模型性能，并且具有广泛适用性和优秀的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor LiDAR point cloud 3D instance segmentation is a crucial task inautonomous driving. However, it requires laborious human efforts to annotatethe point cloud for training a segmentation model. To address this challenge,we propose a YoCo framework, which generates 3D pseudo labels using minimalcoarse click annotations in the bird's eye view plane. It is a significantchallenge to produce high-quality pseudo labels from sparse annotations. OurYoCo framework first leverages vision foundation models combined with geometricconstraints from point clouds to enhance pseudo label generation. Second, atemporal and spatial-based label updating module is designed to generatereliable updated labels. It leverages predictions from adjacent frames andutilizes the inherent density variation of point clouds (dense near, sparsefar). Finally, to further improve label quality, an IoU-guided enhancementmodule is proposed, replacing pseudo labels with high-confidence and high-IoUpredictions. Experiments on the Waymo dataset demonstrate YoCo's effectivenessand generality, achieving state-of-the-art performance among weakly supervisedmethods and surpassing fully supervised Cylinder3D. Additionally, the YoCo issuitable for various networks, achieving performance comparable to fullysupervised methods with minimal fine-tuning using only 0.8% of the fullylabeled data, significantly reducing annotation costs.</description>
      <author>example@mail.com (Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng)</author>
      <guid isPermaLink="false">2502.19698v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>DV-Matcher: Deformation-based Non-Rigid Point Cloud Matching Guided by Pre-trained Visual Features</title>
      <link>http://arxiv.org/abs/2408.08568v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 21 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为DV-Matcher的创新学习框架，用于估计非刚性可变形点云之间的密集对应关系。&lt;h4&gt;背景&lt;/h4&gt;现有的方法通常需要对点云进行网格化或手动标注才能学习到有效的特征信息。相比之下，基于学习的方法可以直接从无结构化的点云中提取特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外预处理的框架，用于生成高质量的密集对应关系，并探索如何在几何特征学习过程中引入先验知识以及设计新的变形模块以促进外部对齐。&lt;h4&gt;方法&lt;/h4&gt;1. 通过将来自预训练视觉模型的知识注入到几何特征学习中，增强局部性质的几何特征与全局和语义信息；2. 提出了一种基于变形的模块来促进由所学对应关系诱导的外在对齐。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示该方法在匹配非刚性点云时达到了最先进的水平，无论是在接近等距形状集合还是异构形状集合中，甚至是更具现实性的部分和噪声数据上都表现出色。&lt;h4&gt;结论&lt;/h4&gt;DV-Matcher框架通过结合视觉先验知识和创新的变形模块，在密集对应估计领域开辟了一条新的道路，并展示了其在处理复杂点云数据中的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种基于学习的框架DV-Matcher，用于非刚性可变形点云之间的密集对应的估算。该框架直接从无结构化点云中学习，无需网格化或手动标记，并且能够提供高质量的密集对应关系，在点云处理中有实际应用价值。我们的主要贡献在于两点：首先，我们提出了一种方案将预训练视觉模型中的先验知识注入到几何特征学习中，有效地补充了局部性质的几何特征与全局和语义信息；其次，我们提出了一个基于变形的新模块来促进由所学对应关系诱导的外在对齐，有效增强了特征学习。实验结果表明，在匹配非刚性点云时，无论是在接近等距形状集合还是异构形状集合中，甚至是更具现实性的部分和噪声数据上，我们的方法都达到了最先进的水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-08-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present DV-Matcher, a novel learning-based framework forestimating dense correspondences between non-rigidly deformable point clouds.Learning directly from unstructured point clouds without meshing or manuallabelling, our framework delivers high-quality dense correspondences, which isof significant practical utility in point cloud processing. Our keycontributions are two-fold: First, we propose a scheme to inject priorknowledge from pre-trained vision models into geometric feature learning, whicheffectively complements the local nature of geometric features with global andsemantic information; Second, we propose a novel deformation-based module topromote the extrinsic alignment induced by the learned correspondences, whicheffectively enhances the feature learning. Experimental results show that ourmethod achieves state-of-the-art results in matching non-rigid point clouds inboth near-isometric and heterogeneous shape collection as well as morerealistic partial and noisy data.</description>
      <author>example@mail.com (Zhangquan Chen, Puhua Jiang, Ruqi Huang)</author>
      <guid isPermaLink="false">2408.08568v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models -- A Panacea for Artificial Intelligence in Pathology?</title>
      <link>http://arxiv.org/abs/2502.21264v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 15 figures and an appendix (study protocol) which is  previously published, see https://doi.org/10.1101/2024.07.04.24309948;  updated authors list format&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文评估了基础模型（FMs）与任务特定（TS）模型在前列腺癌诊断和Gleason分级中的临床表现，发现尽管FMs在数据稀缺情况下有优势，但当有足够的标注训练数据时其性能被TS模型超越。此外，专门的任务培训显著降低了误诊率，并且考虑到可持续性问题，建议结合两种方法以实现稳健且资源高效的AI病理学解决方案。&lt;h4&gt;背景&lt;/h4&gt;人工智能（AI）在病理科的作用从辅助诊断发展到揭示全切片图像中的预测形态模式。基础模型通过自监督预训练被广泛倡导为多种下游任务的通用解决方案，但它们的临床适用性和相对于特定任务学习模型的优势仍存在疑问。&lt;h4&gt;目的&lt;/h4&gt;评估AI在前列腺癌诊断和Gleason分级中具有临床级性能的方法，并比较两个FMs与完全端到端TS模型的表现。&lt;h4&gt;方法&lt;/h4&gt;使用来自15个地点、11个国家的7342名患者超过10万个核心针活检样本进行了大规模验证。将两种基础模型在一个多实例学习框架中与一个完全端到端任务特定模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;尽管FMs在数据稀缺情况下有用，但当有足够的标记训练数据时其性能被TS模型超越或低于后者；任务特定培训显著减少了临床重要性的错误分级和形态挑战性情况下的误诊；基础模型消耗的能量最多可达TS模型的35倍，引发了关于可持续性的担忧。&lt;h4&gt;结论&lt;/h4&gt;FMs在快速原型设计和研究中提供了明显优势，但作为适用于临床应用的医疗AI通用解决方案的角色仍不确定。对于高风险临床应用而言，严格的验证以及对特定任务培训的考虑至关重要。建议结合基础模型和端到端学习的优点以实现稳健且资源高效的AI病理学解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文研究了人工智能在前列腺癌诊断及格利森评分中的作用，并通过对比两种基于大规模预训练的基础模型与传统任务特异模型，揭示了关于这两种方法临床适用性的新见解。结果强调，在充足标记数据的情况下，任务特定的训练优于基础模型；同时指出，基础模型较高的能耗问题也应引起重视。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The role of artificial intelligence (AI) in pathology has evolved from aidingdiagnostics to uncovering predictive morphological patterns in whole slideimages (WSIs). Recently, foundation models (FMs) leveraging self-supervisedpre-training have been widely advocated as a universal solution for diversedownstream tasks. However, open questions remain about their clinicalapplicability and generalization advantages over end-to-end learning usingtask-specific (TS) models. Here, we focused on AI with clinical-gradeperformance for prostate cancer diagnosis and Gleason grading. We present thelargest validation of AI for this task, using over 100,000 core needle biopsiesfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with afully end-to-end TS model in a multiple instance learning framework. Ourfindings challenge assumptions that FMs universally outperform TS models. WhileFMs demonstrated utility in data-scarce scenarios, their performance convergedwith - and was in some cases surpassed by - TS models when sufficient labeledtraining data were available. Notably, extensive task-specific trainingmarkedly reduced clinically significant misgrading, misdiagnosis of challengingmorphologies, and variability across different WSI scanners. Additionally, FMsused up to 35 times more energy than the TS model, raising concerns about theirsustainability. Our results underscore that while FMs offer clear advantagesfor rapid prototyping and research, their role as a universal solution forclinically applicable medical AI remains uncertain. For high-stakes clinicalapplications, rigorous validation and consideration of task-specific trainingremain critically important. We advocate for integrating the strengths of FMsand end-to-end learning to achieve robust and resource-efficient AI pathologysolutions fit for clinical use.</description>
      <author>example@mail.com (Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2502.21264v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
      <link>http://arxiv.org/abs/2502.21123v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保机器学习系统的可信性至关重要，尤其是在其被嵌入到高风险领域时。&lt;h4&gt;背景&lt;/h4&gt;在机器学习系统变得越来越重要和广泛应用的同时，如何保证这些系统的公平性、隐私性、健壮性、准确性和可解释性成为了研究的重点。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在通过引入因果方法来解决可信机器学习中的多重目标之间的矛盾，并提高其可靠性与解释性。&lt;h4&gt;方法&lt;/h4&gt;通过回顾现有文献中将因果方法应用于机器学习的成功案例，展示如何有效结合这些原则以达成平衡。&lt;h4&gt;主要发现&lt;/h4&gt;指出采用因果推理框架能够帮助更好地管理多个相互竞争的目标，在可信机器学习和基础模型设计方面提供解决方案。&lt;h4&gt;结论&lt;/h4&gt;论文讨论了采纳因果框架面临的挑战、局限性及机会，并倡导在未来的AI系统中使用更加负责任且道德的策略。&lt;h4&gt;翻译&lt;/h4&gt;确保机器学习系统的信任度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法融入到机器学习中来处理关键原则之间的权衡问题，如公平性、隐私性、鲁棒性、准确性及可解释性。尽管这些目标应理想地同时满足，但现实中往往单独考虑，导致冲突和次优解的产生。通过参考现有的因果推理在机器学习中的应用案例，本文强调了采用因果方法对于平衡多重竞争目标的重要性，并探讨如何实际将因果理论应用于机器学习模型中，以提升其可靠性和可解释性。此外，还讨论了采纳这种框架所面临的挑战、限制和机遇，为更负责任及伦理规范的AI系统开发铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring trustworthiness in machine learning (ML) systems is crucial as theybecome increasingly embedded in high-stakes domains. This paper advocates forintegrating causal methods into machine learning to navigate the trade-offsamong key principles of trustworthy ML, including fairness, privacy,robustness, accuracy, and explainability. While these objectives should ideallybe satisfied simultaneously, they are often addressed in isolation, leading toconflicts and suboptimal solutions. Drawing on existing applications ofcausality in ML that successfully align goals such as fairness and accuracy orprivacy and robustness, this paper argues that a causal approach is essentialfor balancing multiple competing objectives in both trustworthy ML andfoundation models. Beyond highlighting these trade-offs, we examine howcausality can be practically integrated into ML and foundation models, offeringsolutions to enhance their reliability and interpretability. Finally, wediscuss the challenges, limitations, and opportunities in adopting causalframeworks, paving the way for more accountable and ethically sound AI systems.</description>
      <author>example@mail.com (Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Mohammad Havaei, Bernhard Schölkopf, Mario Fritz)</author>
      <guid isPermaLink="false">2502.21123v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>GraphBridge: Towards Arbitrary Transfer Learning in GNNs</title>
      <link>http://arxiv.org/abs/2502.19252v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 6 tables, to be published in ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为GraphBridge的框架，旨在解决图神经网络（GNN）在不同任务和数据集之间知识迁移的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的GNN训练方式是针对特定的任务或领域进行的，这导致了跨不同、异构的数据设置的知识转移存在障碍。&lt;h4&gt;目的&lt;/h4&gt;提出一种通用的方法来实现GNN中的跨域和跨任务的知识迁移。&lt;h4&gt;方法&lt;/h4&gt;GraphBridge通过增加预训练模型上的预测头以及输入层与输出层之间的桥梁网络，以保持原模型的固有知识并支持任意维度的输出。为了减少目标领域的源偏差问题，该框架将源模型合并到一个同时训练的目标模型中。&lt;h4&gt;主要发现&lt;/h4&gt;在图转图、节点转节点、图转节点以及图转点云等多种迁移学习场景下进行了广泛的实验验证，并通过16个具有代表性的数据集证明了其在任务和领域无关的图结构数据中的知识转移能力，标志着GNN领域的重大进展。&lt;h4&gt;结论&lt;/h4&gt;GraphBridge框架提供了一种有效的方法来解决跨不同任务和域的知识迁移问题，在多个场景中显示出优越的表现。源代码可在https://github.com/jujulili888/GraphBridge获得。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）通常针对特定领域或特定任务进行训练，这在将所获取知识转移到不同的、异构的数据设置时造成了障碍。本文介绍了GraphBridge框架，一种用于实现不同任务和域之间知识转移的新方法，无需对任务配置或图结构进行修改。具体而言，GraphBridge允许通过添加预测头和连接输入层到输出层的桥梁网络来增强任何预训练的GNN模型。此架构不仅保留了原始模型的内在知识，还支持任意维度的输出。为了解决负向迁移问题，GraphBridge将源模型与同时训练的目标模型合并在一起，在应用于目标领域时减少了源偏置。我们的方法在包括图转图、节点转节点、图转节点以及图转点云在内的多种迁移学习场景中进行了全面评估，并通过代表这些场景的16个数据集上的实验证明了该框架在任务和领域无关的图结构中的知识转移能力，标志着GNN领域的重大进展。代码可在https://github.com/jujulili888/GraphBridge获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are conventionally trained on a per-domain,per-task basis. It creates a significant barrier in transferring the acquiredknowledge to different, heterogeneous data setups. This paper introducesGraphBridge, a novel framework to enable knowledge transfer across disparatetasks and domains in GNNs, circumventing the need for modifications to taskconfigurations or graph structures. Specifically, GraphBridge allows for theaugmentation of any pre-trained GNN with prediction heads and a bridgingnetwork that connects the input to the output layer. This architecture not onlypreserves the intrinsic knowledge of the original model but also supportsoutputs of arbitrary dimensions. To mitigate the negative transfer problem,GraphBridge merges the source model with a concurrently trained model, therebyreducing the source bias when applied to the target domain. Our method isthoroughly evaluated across diverse transfer learning scenarios, includingGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empiricalvalidation, conducted over 16 datasets representative of these scenarios,confirms the framework's capacity for task- and domain-agnostic transferlearning within graph-like data, marking a significant advancement in the fieldof GNNs. Code is available at https://github.com/jujulili888/GraphBridge.</description>
      <author>example@mail.com (Li Ju, Xingyi Yang, Qi Li, Xinchao Wang)</author>
      <guid isPermaLink="false">2502.19252v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare Text Multi-Label Classification</title>
      <link>http://arxiv.org/abs/2502.14189v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;随着收集的医疗文本数据量的不断增加，自动化多标签文本分类（MLTC）面临独特挑战，主要是由于训练所需标记文本的稀缺性和其复杂性。传统机器学习模型通常无法完全捕捉到表达的主题范围。然而，大型语言模型（LLMs）在不同领域的多项自然语言处理任务中展示了显著的效果，这些模型具有出色的计算效率，并且通过提示工程能够适用于无监督学习。因此，这些LLM为医疗叙述的MLTC提供了有效的解决方案。然而，在面对各种标签时，不同的提示可能根据主题的相关性而变化。为了应对这一挑战，提出的QUAD-LLM-MLTC方法利用了四个大型语言模型的优势：GPT-4o、BERT、PEGASUS和BART。该方法在顺序流水线中操作，其中BERT提取关键令牌，PEGASUS增强文本数据，GPT-4o进行分类，而BART提供主题分配概率，从而产生四次0-shot设置的分类结果。这些输出通过集成学习组合，并通过元分类器处理以生成最终的MLTC结果。该方法使用三个标记文本样本进行了评估，与传统和单一模型的方法形成了对比。结果显示，在大多数主题上的F1评分及一致性（F1 和 Micro-F1 分数分别达到78.17% 和 80.16%，标准偏差分别为0.025 和 0.011）上有显著改进。&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多标签文本分类方法，QUAD-LLM-MLTC，利用多个大型语言模型处理医疗数据的复杂性和多样性，并展示了其在F1评分和一致性上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;自动化多标签文本分类（MLTC）因医疗领域的大量未标记数据而面临挑战。传统机器学习模型难以应对这种复杂性。&lt;h4&gt;目的&lt;/h4&gt;探索使用大型语言模型进行高效的无监督学习，以解决大规模医疗文本的自动分类问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架QUAD-LLM-MLTC，该框架利用GPT-4o、BERT、PEGASUS和BART四个大型语言模型来进行零样本设置下的多标签文本分类，并通过集成学习和元分类器处理输出以得到最终结果。&lt;h4&gt;主要发现&lt;/h4&gt;与传统方法相比，使用QUAD-LLM-MLTC的方法在多个主题上显示出更高的F1评分及一致性。这种方法展示出强大的性能并可广泛应用于医疗数据的快速分类。&lt;h4&gt;结论&lt;/h4&gt;大型语言模型的应用为解决复杂文本的数据分类问题提供了创新性的解决方案，并通过集成不同模型的优势，在多标签医学文本分类中实现了高效和扩展性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The escalating volume of collected healthcare textual data presents a uniquechallenge for automated Multi-Label Text Classification (MLTC), which isprimarily due to the scarcity of annotated texts for training and their nuancednature. Traditional machine learning models often fail to fully capture thearray of expressed topics. However, Large Language Models (LLMs) havedemonstrated remarkable effectiveness across numerous Natural LanguageProcessing (NLP) tasks in various domains, which show impressive computationalefficiency and suitability for unsupervised learning through promptengineering. Consequently, these LLMs promise an effective MLTC of medicalnarratives. However, when dealing with various labels, different prompts can berelevant depending on the topic. To address these challenges, the proposedapproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in whichBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, andBART provides topics' assignment probabilities, which results in fourclassifications, all in a 0-shot setting. The outputs are then combined usingensemble learning and processed through a meta-classifier to produce the finalMLTC result. The approach is evaluated using three samples of annotated texts,which contrast it with traditional and single-model methods. The results showsignificant improvements across the majority of the topics in theclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and80.16% with standard deviations of 0.025 and 0.011, respectively). Thisresearch advances MLTC using LLMs and provides an efficient and scalablesolution to rapidly categorize healthcare-related text data without furthertraining.</description>
      <author>example@mail.com (Hajar Sakai, Sarah S. Lam)</author>
      <guid isPermaLink="false">2502.14189v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view 4D Radars and Cameras for Omnidirectional Perception</title>
      <link>http://arxiv.org/abs/2501.15394v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Doracamom的框架，该框架融合了多视角相机和4D雷达的数据，用于实现3D物体检测和语义占用预测任务。通过引入新颖的Coarse Voxel Queries Generator、设计Dual-Branch Temporal Encoder以及Cross-Modal BEV-Voxel Fusion模块，使系统能够在复杂环境感知中表现出色。&lt;h4&gt;背景&lt;/h4&gt;3D目标检测和占位预测在自动驾驶领域非常重要，但现有基于视觉的方法在恶劣条件下效果不佳。整合相机与4D成像雷达可以实现多任务统一感知，但在这一领域的研究仍然有限。&lt;h4&gt;目的&lt;/h4&gt;提出一个能够集成多视角相机和4D雷达的框架，用于完成3D物体检测和语义占用预测任务。&lt;h4&gt;方法&lt;/h4&gt;引入了Coarse Voxel Queries Generator来初始化查询体素；设计了Dual-Branch Temporal Encoder来利用时间信息，并实现了Cross-Modal BEV-Voxel Fusion模块以融合多模态特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Doracamom在OmniHD-Scenes、View-of-Delft (VoD)和TJ4DRadSet数据集上均达到了当前最佳性能。&lt;h4&gt;结论&lt;/h4&gt;该框架通过结合多种传感器的数据实现了强大的3D感知能力，并为未来的多模态环境感知系统建立了新的基准。&lt;h4&gt;翻译&lt;/h4&gt;三维物体检测和占用预测在自动驾驶中至关重要，吸引了大量关注。尽管最近基于视觉的方法潜力巨大，但在恶劣条件下仍面临挑战。因此，将相机与下一代4D成像雷达相结合以实现统一的多任务感知非常重要，但该领域的研究仍然有限。本文提出了一种名为Doracamom的框架，它融合了多视角摄像头和4D雷达的数据，用于联合执行3D物体检测和语义占用预测，从而实现了全面的环境感知。特别是引入了一个新的粗体素查询生成器，该生成器将从4D雷达获得的几何先验知识与图像中的语义特征相结合来初始化体素查询，为后续基于Transformer的细化建立了坚实的基础。为了利用时间信息，设计了双分支时间编码器，在鸟瞰图和体素空间中并行处理多模态时序特性，从而能够进行全面的空间-时间表示学习。此外，还提出了一个跨模态BEV-Voxel融合模块，通过注意力机制自适应地融合互补特征，并利用辅助任务来提高特征质量。在OmniHD-Scenes、View-of-Delft (VoD)和TJ4DRadSet数据集上的广泛实验表明，Doracamom在这两项任务中均达到了当前最佳性能，为多模态3D感知建立了新的基准。代码和模型将公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-01-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D object detection and occupancy prediction are critical tasks in autonomousdriving, attracting significant attention. Despite the potential of recentvision-based methods, they encounter challenges under adverse conditions. Thus,integrating cameras with next-generation 4D imaging radar to achieve unifiedmulti-task perception is highly significant, though research in this domainremains limited. In this paper, we propose Doracamom, the first framework thatfuses multi-view cameras and 4D radar for joint 3D object detection andsemantic occupancy prediction, enabling comprehensive environmental perception.Specifically, we introduce a novel Coarse Voxel Queries Generator thatintegrates geometric priors from 4D radar with semantic features from images toinitialize voxel queries, establishing a robust foundation for subsequentTransformer-based refinement. To leverage temporal information, we design aDual-Branch Temporal Encoder that processes multi-modal temporal features inparallel across BEV and voxel spaces, enabling comprehensive spatio-temporalrepresentation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusionmodule that adaptively fuses complementary features through attentionmechanisms while employing auxiliary tasks to enhance feature quality.Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSetdatasets demonstrate that Doracamom achieves state-of-the-art performance inboth tasks, establishing new benchmarks for multi-modal 3D perception. Code andmodels will be publicly available.</description>
      <author>example@mail.com (Lianqing Zheng, Jianan Liu, Runwei Guan, Long Yang, Shouyi Lu, Yuanzhe Li, Xiaokai Bai, Jie Bai, Zhixiong Ma, Hui-Liang Shen, Xichan Zhu)</author>
      <guid isPermaLink="false">2501.15394v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation</title>
      <link>http://arxiv.org/abs/2502.17349v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为HybridLinker的框架被提出，以解决药物设计中连接子生成问题中的多样性和有效性之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;在药物发现应用（如候选物优化和PROTAC设计）中，分子片段组装成不同的药物候选物时链接器生成至关重要。目前的方法可以分为PC-Free和PC-Aware两类，前者基于它们是否使用3D点云(PC)。PC-Free模型更注重多样性，但因忽视了PC约束导致有效性和合法性较低；而PC-Aware模型通过强制执行严格的PC约束来确保更高的有效性和合法性，却限制了多样性。&lt;h4&gt;目的&lt;/h4&gt;为了在不增加额外训练的情况下克服上述权衡问题，提出了一种名为HybridLinker的框架。&lt;h4&gt;方法&lt;/h4&gt;该框架的核心是LinkerDPS（链接器后验扩散采样），它是一种新的扩散后验采样方法，在PC-Free和PC-Aware空间中操作。通过一种能量启发式的函数将分子拓扑结构与3D点云联系起来，从而允许从预训练的PC-Free模型中提供多样化的键合拓扑作为指导来增强PC-Aware推理。&lt;h4&gt;主要发现&lt;/h4&gt;HybridLinker框架在基础分子设计和应用属性优化任务中显著且一致地超过了基准方法，在提高有效性和多样性方面建立了新的扩散后验采样框架，该框架超越了图像领域，适用于分子和图域。&lt;h4&gt;结论&lt;/h4&gt;通过将PC-Free模型的多样化采样分布转移到PC-Aware分布上，HybridLinker在药物设计应用中的多样性和有效性之间找到了一个很好的平衡点。&lt;h4&gt;翻译&lt;/h4&gt;链接器生成是药物发现应用程序（如候选物优化和PROTAC设计）中的关键问题，其中分子片段被组装成不同的药物候选物。现有的方法根据它们是否使用3D点云(PC)分为PC-Free和PC-Aware两类。PC-Free模型优先考虑多样性但有效性和合法性较低；而PC-Aware模型通过强制执行严格的PC约束来确保更高的有效性和合法性，却限制了多样性。为了克服这些权衡问题且不需额外训练，我们提出了HybridLinker框架，该框架通过从预训练的PC-Free模型中提供多样化的键合拓扑作为指导来增强PC-Aware推理。在核心部分，我们提出了一种新的扩散后验采样方法LinkerDPS，在PC-Free和PC-Aware空间之间操作，并通过一种能量启发式的函数将分子拓扑与3D点云联系起来。HybridLinker框架能够将PC-Free模型的多样化采样分布转移至PC-Aware分布上，从而在基础分子设计和应用属性优化任务中显著且一致地超过了基准方法，在提高有效性和多样性方面建立了新的扩散后验采样框架，该框架超越了图像领域，适用于分子和图域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linker generation is critical in drug discovery applications such as leadoptimization and PROTAC design, where molecular fragments are assembled intodiverse drug candidates. Existing methods fall into PC-Free and PC-Awarecategories based on their use of 3D point clouds (PC). PC-Free modelsprioritize diversity but suffer from lower validity due to overlooking PCconstraints, while PC-Aware models ensure higher validity but restrictdiversity by enforcing strict PC constraints. To overcome these trade-offswithout additional training, we propose HybridLinker, a framework that enhancesPC-Aware inference by providing diverse bonding topologies from a pretrainedPC-Free model as guidance. At its core, we propose LinkerDPS, the firstdiffusion posterior sampling (DPS) method operating across PC-Free and PC-Awarespaces, bridging molecular topology with 3D point clouds via an energy-inspiredfunction. By transferring the diverse sampling distribution of PC-Free modelsinto the PC-Aware distribution, HybridLinker significantly and consistentlysurpasses baselines, improving both validity and diversity in foundationalmolecular design and applied property optimization tasks, establishing a newDPS framework in the molecular and graph domains beyond imaging.</description>
      <author>example@mail.com (Minyeong Hwang, Ziseok Lee, Kwang-Soo Kim, Kyungsu Kim, Eunho Yang)</author>
      <guid isPermaLink="false">2502.17349v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</title>
      <link>http://arxiv.org/abs/2502.21186v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025. Code would be available at  https://github.com/BaitingLuo/L-MAP.git&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的离线强化学习框架L-MAP，旨在通过学习一组时间扩展的宏观动作来解决高维连续动作空间中的顺序决策问题。&lt;h4&gt;背景&lt;/h4&gt;在具有随机动态的复杂环境中进行顺序决策时，尤其是在需要基于历史数据训练代理以做出决策的情况下，面临计算挑战。这些环境通常包含高维度的动作空间和不确定的状态转换。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决传统离线强化学习中遇到的问题，特别是如何有效地处理高维连续动作空间中的随机性问题。&lt;h4&gt;方法&lt;/h4&gt;L-MAP通过状态条件下的向量量化变分自动编码器(VQ-VAE)来减少行动维度，并使用蒙特卡洛树搜索(MCTS)算法在决策过程中考虑环境和行为策略的随机性。此外，还引入了一个独立学习到的先验模型作为潜在转换模型，以实现可能动作的有效采样。&lt;h4&gt;主要发现&lt;/h4&gt;L-MAP在离线强化学习设置中表现优异，在处理复杂和高维的动作空间时显示出低决策延迟，并且能够保持与基于模型的方法相匹配的表现。&lt;h4&gt;结论&lt;/h4&gt;L-MAP提供了一种有效解决具有高度不确定性和动作维度问题的策略规划方法，表明了这种方法在处理具有随机性环境中的顺序决策任务的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential decision-making in high-dimensional continuous action spaces,particularly in stochastic environments, faces significant computationalchallenges. We explore this challenge in the traditional offline RL setting,where an agent must learn how to make decisions based on data collected througha stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),which addresses this challenge by learning a set of temporally extendedmacro-actions through a state-conditional Vector Quantized VariationalAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employsa (separate) learned prior model that acts as a latent transition model andallows efficient sampling of plausible actions. During planning, our approachaccounts for stochasticity in both the environment and the behavior policy byusing Monte Carlo tree search (MCTS). In offline RL settings, includingstochastic continuous control tasks, L-MAP efficiently searches over discretelatent actions to yield high expected returns. Empirical results demonstratethat L-MAP maintains low decision latency despite increased actiondimensionality. Notably, across tasks ranging from continuous control withinherently stochastic dynamics to high-dimensional robotic hand manipulation,L-MAP significantly outperforms existing model-based methods and performson-par with strong model-free actor-critic baselines, highlighting theeffectiveness of the proposed approach in planning in complex and stochasticenvironments with high-dimensional action spaces.</description>
      <author>example@mail.com (Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay)</author>
      <guid isPermaLink="false">2502.21186v2</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>GP-GS: Gaussian Processes for Enhanced Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.02283v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Gaussian Processes Gaussian Splatting (GP-GS)的3D重建框架，用于提高稀疏结构从运动(SfM)点云的场景重建质量。&lt;h4&gt;背景&lt;/h4&gt;3D高斯斑点方法是一种高效的逼真新视图合成方法，但其依赖于稀疏的SfM点云，导致了场景重建的质量问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种自适应和不确定性指导的稠密化框架来改进现有的3D重建效果。&lt;h4&gt;方法&lt;/h4&gt;引入了一种多输出高斯过程模型，并提出了一种动态采样与过滤管道，利用基于GP预测的新候选点从输入2D像素和深度图中生成密集点云。&lt;h4&gt;主要发现&lt;/h4&gt;该框架通过不确定性估计指导的稀疏SfM点云稠密化提高了3D重建质量，特别是在几何一致性和稠密性方面表现突出。&lt;h4&gt;结论&lt;/h4&gt;实验结果验证了GP-GS框架的有效性和实用性，在合成数据集和真实世界数据集中均表现出优越性能。&lt;h4&gt;翻译&lt;/h4&gt;3D高斯斑点方法作为一种高效的逼真新视图合成方法已经出现。然而，其依赖于稀疏的结构从运动（SfM）点云持续地影响了场景重建的质量。为了应对这些限制，本文提出了一种新的三维重建框架——高斯过程高斯斑点（GP-GS），其中开发了一个多输出的高斯过程模型来实现对稀疏SfM点云的自适应和不确定性指导的稠密化。具体来说，我们提出了一条动态采样和过滤流水线，它通过利用基于GP预测从输入2D像素和深度图中推断新的候选点来自适应地扩展了SfM点云，并且该流程利用不确定性估计来引导高方差预测的修剪工作，确保了几何一致性并使密集点云生成成为可能。这些稠密化的点云提供了高质量的初始3D高斯分布以增强重建性能。在各种规模上的合成和真实世界数据集上进行的一系列实验验证了所提出框架的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-04&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/zhihaohaoran/GPGS&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Gaussian Splatting has emerged as an efficient photorealistic novel viewsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)point clouds consistently compromises the scene reconstruction quality. Toaddress these limitations, this paper proposes a novel 3D reconstructionframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-outputGaussian Process model is developed to achieve adaptive and uncertainty-guideddensification of sparse SfM point clouds. Specifically, we propose a dynamicsampling and filtering pipeline that adaptively expands the SfM point clouds byleveraging GP-based predictions to infer new candidate points from the input 2Dpixels and depth maps. The pipeline utilizes uncertainty estimates to guide thepruning of high-variance predictions, ensuring geometric consistency andenabling the generation of dense point clouds. The densified point cloudsprovide high-quality initial 3D Gaussians to enhance reconstructionperformance. Extensive experiments conducted on synthetic and real-worlddatasets across various scales validate the effectiveness and practicality ofthe proposed framework.</description>
      <author>example@mail.com (Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang)</author>
      <guid isPermaLink="false">2502.02283v3</guid>
      <pubDate>Tue, 04 Mar 2025 15:00:15 +0800</pubDate>
    </item>
    <item>
      <title>A Fused Gromov-Wasserstein Approach to Subgraph Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.20885v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本文介绍了一种新的自监督图表示学习方法——FOSSIL，用于解决现有对比学习方法在利用结构模式和节点相似性方面的不足。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习已成为处理标注数据稀缺或不可用情况的关键方法。然而，在设计有效的预训练任务以进行自监督图表示学习方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了节点级与子图级对比学习的新方法，旨在更有效地利用图形的结构模式和节点相似性。&lt;h4&gt;方法&lt;/h4&gt;FOSSIL模型通过将标准的节点级别对比损失函数与融合Gromov-Wasserstein距离相结合，可以同时捕捉节点特征和图结构。此外，该方法适用于同构及异构图，并能动态创建视角以生成正负样本对。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在基准图数据集上的测试中，FOSSIL优于或达到了目前最先进的方法的性能水平。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法能够有效地改善现有的自监督图表示学习技术，尤其在利用复杂结构模式和节点相似性方面有显著优势。&lt;h4&gt;翻译&lt;/h4&gt;自我监督学习已成为训练深度学习模型的重要手段，尤其是在标注数据稀缺的情况下。尽管图机器学习在各个领域都有巨大的潜力，但设计有效的预训练任务以进行自监督图表示学习仍然具有挑战性。对比学习是图的自我监督学习的一种流行方法，它利用正负对来计算对比损失函数。然而，目前的图对比学习方法往往难以充分使用结构模式和节点相似性。为了解决这些问题，我们提出了一种名为Fused Gromov Wasserstein Subgraph Contrastive Learning（FOSSIL）的新方法。我们的模型集成了节点级和子图级别的对比学习，无缝结合了标准的节点级别对比损失函数与融合Gromov-Wasserstein距离。这种组合使我们的方法能够同时捕捉节点特征和图形结构。重要的是，该方法既适用于同构图也适用于异构图，并能动态创建视角以生成正负样本对。通过在基准图数据集上的广泛实验，我们证明FOSSIL比或与当前最先进的方法性能相当。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-supervised learning has become a key method for training deep learningmodels when labeled data is scarce or unavailable. While graph machine learningholds great promise across various domains, the design of effective pretexttasks for self-supervised graph representation learning remains challenging.Contrastive learning, a popular approach in graph self-supervised learning,leverages positive and negative pairs to compute a contrastive loss function.However, current graph contrastive learning methods often struggle to fully usestructural patterns and node similarities. To address these issues, we presenta new method called Fused Gromov Wasserstein Subgraph Contrastive Learning(FOSSIL). Our model integrates node-level and subgraph-level contrastivelearning, seamlessly combining a standard node-level contrastive loss with theFused Gromov-Wasserstein distance. This combination helps our method captureboth node features and graph structure together. Importantly, our approachworks well with both homophilic and heterophilic graphs and can dynamicallycreate views for generating positive and negative pairs. Through extensiveexperiments on benchmark graph datasets, we show that FOSSIL outperforms orachieves competitive performance compared to current state-of-the-art methods.</description>
      <author>example@mail.com (Amadou S. Sangare, Nicolas Dunou, Jhony H. Giraldo, Fragkiskos D. Malliaros)</author>
      <guid isPermaLink="false">2502.20885v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
  <item>
      <title>Assessing zero-shot generalisation behaviour in graph-neural-network interatomic potentials</title>
      <link>http://arxiv.org/abs/2502.21317v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了机器学习原子间势能模型（MLIP）在材料化学和分子化学之间的迁移能力。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习原子间势能模型的广泛应用，如何设计适用于多种应用领域的基础性MLIP成为了当前的研究重点。&lt;h4&gt;目的&lt;/h4&gt;评估一种特定于石墨烯氧化物扩展共价网络设计的MLIP（GO-MACE-23）在处理小型独立分子和化学反应时的表现。&lt;h4&gt;方法&lt;/h4&gt;通过将该模型与专门为某一领域训练的状态-of-the-art模型进行直接比较，来量化其零样本学习性能。&lt;h4&gt;主要发现&lt;/h4&gt;提供了图神经网络势能的迁移能力和泛化能力的定量见解。&lt;h4&gt;结论&lt;/h4&gt;这项工作促进了MLIP在化学中的更广泛应用，并为进一步研究这类模型如何在不同领域的应用中发挥作用奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;随着机器学习原子间势能（MLIP）模型在化学领域迅速可用性的增长，当前许多研究集中在开发通用且“基础性”的MLIP上。在这个背景下，一个重要的问题是这些模型是否以及在多大程度上可以在不同的应用场景之间转移。在这里，我们评估了一种MLIP模型在材料和分子化学之间的迁移能力。具体来说，我们研究了GO-MACE-23模型，该模型旨在用于石墨烯氧化物的扩展共价网络，并量化了它对小型独立分子和在其直接作用范围之外的化学反应的零样本性能——与专门为某一领域训练的状态-of-the-art模型进行直接比较。我们的工作为图神经网络势能的迁移和泛化能力提供了定量见解，更广泛地说，朝着MLIP在化学中的更广泛应用迈进了一步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapidly growing availability of machine-learned interatomicpotential (MLIP) models for chemistry, much current research focuses on thedevelopment of generally applicable and ``foundational'' MLIPs. An importantquestion in this context is whether, and how well, such models can transferfrom one application domain to another. Here, we assess this transferabilityfor an MLIP model at the interface of materials and molecular chemistry.Specifically, we study GO-MACE-23, a model designed for the extended covalentnetwork of graphene oxide, and quantify its zero-shot performance for small,isolated molecules and chemical reactions outside its direct scope--in directcomparison with a state-of-the-art model which has been trained in-domain. Ourwork provides quantitative insight into the transfer and generalisation abilityof graph-neural-network potentials and, more generally, makes a step towardsthe more widespread applicability of MLIPs in chemistry.</description>
      <author>example@mail.com (Chiheb Ben Mahmoud, Zakariya El-Machachi, Krystian A. Gierczak, John L. A. Gardner, Volker L. Deringer)</author>
      <guid isPermaLink="false">2502.21317v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>RuCCoD: Towards Automated ICD Coding in Russian</title>
      <link>http://arxiv.org/abs/2502.21263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究探讨了在俄语等生物医学资源有限的语言环境中实现临床编码自动化的可行性。&lt;h4&gt;背景&lt;/h4&gt;当前，许多语言的生物医学资源较为匮乏，特别是在俄语中。这限制了相关领域的自动化进程，如临床编码。&lt;h4&gt;目的&lt;/h4&gt;通过创建新的ICD（国际疾病分类）编码数据集来研究在俄语等资源有限的语言环境中自动进行临床编码的可能性。&lt;h4&gt;方法&lt;/h4&gt;{'数据准备': '构建了一个包含来自电子健康记录（EHRs）的诊断字段的数据集，该数据集包括超过10,000个实体和超过1,500种独特的ICD代码。', '模型测试': '利用这个数据集作为基准，测试了几种最先进的模型，如BERT、LLaMA与LoRA结合使用以及RAG。进行了额外的跨域迁移学习实验（从PubMed摘要到医学诊断）以及术语转换实验（从UMLS概念到ICD编码）。', '应用': '将性能最佳的模型应用于公司内部EHR数据集，该数据集中包含了2017年至2021年的患者历史记录。'}&lt;h4&gt;主要发现&lt;/h4&gt;利用自动化预测代码进行训练后，与医生手动注释的数据相比，在精心准备的测试集上显示出显著提高的准确性。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在资源有限的语言环境中实现临床编码自动化的潜力巨大，这可以提升这些语境下的医疗效率和数据准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates the feasibility of automating clinical coding inRussian, a language with limited biomedical resources. We present a new datasetfor ICD coding, which includes diagnosis fields from electronic health records(EHRs) annotated with over 10,000 entities and more than 1,500 unique ICDcodes. This dataset serves as a benchmark for several state-of-the-art models,including BERT, LLaMA with LoRA, and RAG, with additional experiments examiningtransfer learning across domains (from PubMed abstracts to medical diagnosis)and terminologies (from UMLS concepts to ICD codes). We then apply thebest-performing model to label an in-house EHR dataset containing patienthistories from 2017 to 2021. Our experiments, conducted on a carefully curatedtest set, demonstrate that training with the automated predicted codes leads toa significant improvement in accuracy compared to manually annotated data fromphysicians. We believe our findings offer valuable insights into the potentialfor automating clinical coding in resource-limited languages like Russian,which could enhance clinical efficiency and data accuracy in these contexts.</description>
      <author>example@mail.com (Aleksandr Nesterov, Andrey Sakhovskiy, Ivan Sviridov, Airat Valiev, Vladimir Makharev, Petr Anokhin, Galina Zubkova, Elena Tutubalina)</author>
      <guid isPermaLink="false">2502.21263v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Unsupervised Parameter Efficient Source-free Post-pretraining</title>
      <link>http://arxiv.org/abs/2502.21313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的无监督参数高效源无关后预训练方法UpStep，用于在没有源领域数据的情况下将预先训练的模型适应到目标领域。&lt;h4&gt;背景&lt;/h4&gt;随着NLP领域的成功，最佳视觉模型现在达到数十亿参数规模。由于计算和经济原因，在目标分布上调整这些大规模模型变得不可行。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来解决在没有源领域数据的情况下将预先训练的模型适应到新目标领域的问题。&lt;h4&gt;方法&lt;/h4&gt;{'i': '设计了一个自我监督的训练方案，可以在没有任何来源数据的情况下对未标记的目标域进行预训练模型调整。', 'ii': '提出了中心向量正则化（CVR），这是一组辅助操作，最小化灾难性遗忘，并通过在50%的训练迭代中跳过反向传播来降低计算成本。', 'iii': '采用低秩适应方法以参数高效的方式进行模型调整。'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够在各种基础架构上（包括监督和非监督训练于Imagenet上的）展示出良好的适应性和泛化能力，将其应用于八个不同的目标领域。&lt;h4&gt;结论&lt;/h4&gt;通过UpStep方法可以有效地在没有源领域数据的情况下将大规模预训练模型调整到新任务中，从而克服了计算成本的限制。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着NLP领域的成功，最佳视觉模型现在达到数十亿参数规模。由于计算和经济原因，在目标分布上调整这些大规模模型变得不可行。为了解决这个问题，我们介绍了一种新的无监督参数高效源无关后预训练方法UpStep，用于在没有源领域数据的情况下将预先训练的模型适应到目标领域：i) 设计了一个自我监督的训练方案，可以在没有任何来源数据的情况下对未标记的目标域进行预训练模型调整。由于这种源无关设置存在灾难性遗忘的风险，ii) 提出了中心向量正则化（CVR），这是一组辅助操作，最小化灾难性遗忘，并通过在50%的训练迭代中跳过反向传播来降低计算成本。最后iii) 采用低秩适应方法以参数高效的方式进行模型调整。我们利用各种一般骨干架构作为基础模型并将其适配到八个不同的目标领域中，展示了我们的方法具有良好的适用性和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Following the success in NLP, the best vision models are now in the billionparameter ranges. Adapting these large models to a target distribution hasbecome computationally and economically prohibitive. Addressing this challenge,we introduce UpStep, an Unsupervised Parameter-efficient Source-freepost-pretraining approach, designed to efficiently adapt a base model from asource domain to a target domain: i) we design a self-supervised trainingscheme to adapt a pretrained model on an unlabeled target domain in a settingwhere source domain data is unavailable. Such source-free setting comes withthe risk of catastrophic forgetting, hence, ii) we propose center vectorregularization (CVR), a set of auxiliary operations that minimize catastrophicforgetting and additionally reduces the computational cost by skippingbackpropagation in 50\% of the training iterations. Finally iii) we performthis adaptation process in a parameter-efficient way by adapting the pretrainedmodel through low-rank adaptation methods, resulting in a fraction ofparameters to optimize. We utilize various general backbone architectures, bothsupervised and unsupervised, trained on Imagenet as our base model and adaptthem to a diverse set of eight target domains demonstrating the adaptabilityand generalizability of our proposed approach.</description>
      <author>example@mail.com (Abhishek Jha, Tinne Tuytelaars, Yuki M. Asano)</author>
      <guid isPermaLink="false">2502.21313v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Fast 3D point clouds retrieval for Large-scale 3D Place Recognition</title>
      <link>http://arxiv.org/abs/2502.21067v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 1 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于Transformer的加速3D点云检索的方法，通过生成一维标识符实现恒定时间内的直接检索。&lt;h4&gt;背景&lt;/h4&gt;在3D点云中寻找最相似的点云是一项具有挑战性的任务。当前方法主要集中在比较描述符以识别相似性，但这个步骤复杂且耗时。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于Differentiable Search Index (DSI)的方法来加速3D点云检索过程。&lt;h4&gt;方法&lt;/h4&gt;通过集成视觉Transformer将点云描述符映射到一维标识符，并结合位置和语义编码以适应三维数据。这种方法使得可以直接根据生成的标识符进行恒定时间内的检索。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在公开基准测试中的检索质量和速度方面都优于现有最先进的技术。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为3D点云检索提供了一种高效且准确的新方案。&lt;h4&gt;翻译&lt;/h4&gt;三维点云的检索是一项具有挑战性的任务，旨在从参考点集中检索与给定查询最相似的点云。当前方法集中在通过比较描述符来识别相似性上。由于这一步骤复杂，我们专注于使用可微搜索索引(DSI)，一种最初为文本信息检索设计的基于Transformer的方法，来加速三维点云检索过程。我们的方法生成了一维标识符，该标识符基于点描述符，并使得可以直接在恒定时间内进行检索。为了将DSI适应于3D数据，我们整合了视觉变换器以将描述符映射到这些标识符，同时结合位置和语义编码。我们在公共基准测试的地点识别上评估了此方法，通过与现有最先进的方法比较其点云检索能力和返回的速度及质量来衡量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieval in 3D point clouds is a challenging task that consists inretrieving the most similar point clouds to a given query within a reference of3D points. Current methods focus on comparing descriptors of point clouds inorder to identify similar ones. Due to the complexity of this latter step, herewe focus on the acceleration of the retrieval by adapting the DifferentiableSearch Index (DSI), a transformer-based approach initially designed for textinformation retrieval, for 3D point clouds retrieval. Our approach generates 1Didentifiers based on the point descriptors, enabling direct retrieval inconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to mapdescriptors to these identifiers while incorporating positional and semanticencoding. The approach is evaluated for place recognition on a public benchmarkcomparing its retrieval capabilities against state-of-the-art methods, in termsof quality and speed of returned point clouds.</description>
      <author>example@mail.com (Chahine-Nicolas Zede, Laurent Carrafa, Valérie Gouet-Brunet)</author>
      <guid isPermaLink="false">2502.21067v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>LV-DOT: LiDAR-visual dynamic obstacle detection and tracking for autonomous robot navigation</title>
      <link>http://arxiv.org/abs/2502.20607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;室内自主机器人导航中的动态障碍物感知对于精确导航至关重要。&lt;h4&gt;背景&lt;/h4&gt;尽管在计算机视觉和自动驾驶领域对3D物体检测和跟踪方法进行了深入研究和发展，但这些方法需要昂贵且高精度的传感器设置以及大型神经网络计算资源，使其不适合用于室内机器人。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于车载相机和LiDAR数据的动态障碍物检测与跟踪框架，以实现轻量级且精确的感知。&lt;h4&gt;方法&lt;/h4&gt;{'融合策略': '采用更稳健的数据融合策略，结合了LiDAR和视觉信息，提高了检测精度。使用特征关联和卡尔曼滤波器进行目标追踪，并设计了一种动态障碍物分类算法来可靠地识别移动物体。', '集成检测方法': '基于先前的集合检测方法，该方法整合来自多个低准确度但计算效率高的探测器的结果，以确保在车载计算机上实现实时性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;数据集评估显示，与基准方法相比，所提出的方法具有更好的感知性能；物理实验也证实了这种方法在实际导航中的可行性。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地解决了单一传感器的限制问题，并通过结合LiDAR和视觉信息实现了更加精确且实时的动态障碍物检测与跟踪。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate perception of dynamic obstacles is essential for autonomous robotnavigation in indoor environments. Although sophisticated 3D object detectionand tracking methods have been investigated and developed thoroughly in thefields of computer vision and autonomous driving, their demands on expensiveand high-accuracy sensor setups and substantial computational resources fromlarge neural networks make them unsuitable for indoor robotics. Recently, morelightweight perception algorithms leveraging onboard cameras or LiDAR sensorshave emerged as promising alternatives. However, relying on a single sensorposes significant limitations: cameras have limited fields of view and cansuffer from high noise, whereas LiDAR sensors operate at lower frequencies andlack the richness of visual features. To address this limitation, we propose adynamic obstacle detection and tracking framework that uses both onboard cameraand LiDAR data to enable lightweight and accurate perception. Our proposedmethod expands on our previous ensemble detection approach, which integratesoutputs from multiple low-accuracy but computationally efficient detectors toensure real-time performance on the onboard computer. In this work, we proposea more robust fusion strategy that integrates both LiDAR and visual data toenhance detection accuracy further. We then utilize a tracking module thatadopts feature-based object association and the Kalman filter to track andestimate detected obstacles' states. Besides, a dynamic obstacle classificationalgorithm is designed to robustly identify moving objects. The datasetevaluation demonstrates a better perception performance compared to benchmarkmethods. The physical experiments on a quadcopter robot confirms thefeasibility for real-world navigation.</description>
      <author>example@mail.com (Zhefan Xu, Haoyu Shen, Xinming Han, Hanyu Jin, Kanlong Ye, Kenji Shimada)</author>
      <guid isPermaLink="false">2502.20607v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation</title>
      <link>http://arxiv.org/abs/2502.20984v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;SemEval-2025 Task 1旨在根据给定的名词短语在英语和巴西葡萄牙语中可能携带的习惯用法意义对图像进行排名。&lt;h4&gt;背景&lt;/h4&gt;该任务涉及使用生成式大型语言模型（LLMs）和多语言CLIP模型来增强惯用表达的意义表示，以解决基于具有惯用含义的名词短语给图片打分的问题。&lt;h4&gt;目的&lt;/h4&gt;为了提高图片排序的效果，本文提出了一种方法结合使用LLM和多语言CLIP模型，并应用对比学习和数据增强技术对生成的嵌入进行微调。&lt;h4&gt;方法&lt;/h4&gt;大型语言模型用于生成潜在惯用表达的意义，而这些意义随后被编码为图像排名中的表示。然后通过对比学习和技术手段调整后的数据增强来精炼这些嵌入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，通过这种方法提取的多模态表示优于仅基于原始名词短语的方法。然而，微调方法虽然显示出有前景的结果，但不如使用未经微调的嵌入有效。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一个利用大型语言模型和CLIP技术改进惯用表达图像排名的新方案，并通过实验验证了其有效性，尽管还存在进一步优化的空间。&lt;h4&gt;翻译&lt;/h4&gt;SemEval-2025 Task 1专注于根据具有潜在习惯意义的名词短语对图片进行排序。为了解决这个问题，这项工作利用生成式大型语言模型（LLMs）和多语言CLIP模型来增强惯用表达的意义表示。通过这种方式，研究者们在提高图像排名精度方面取得了显著进展，并且他们的源代码可在GitHub上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; SemEval-2025 Task 1 focuses on ranking images based on their alignment with agiven nominal compound that may carry idiomatic meaning in both English andBrazilian Portuguese. To address this challenge, this work uses generativelarge language models (LLMs) and multilingual CLIP models to enhance idiomaticcompound representations. LLMs generate idiomatic meanings for potentiallyidiomatic compounds, enriching their semantic interpretation. These meaningsare then encoded using multilingual CLIP models, serving as representations forimage ranking. Contrastive learning and data augmentation techniques areapplied to fine-tune these embeddings for improved performance. Experimentalresults show that multimodal representations extracted through this methodoutperformed those based solely on the original nominal compounds. Thefine-tuning approach shows promising outcomes but is less effective than usingembeddings without fine-tuning. The source code used in this paper is availableat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.</description>
      <author>example@mail.com (Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang)</author>
      <guid isPermaLink="false">2502.20984v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>JiTTER: Jigsaw Temporal Transformer for Event Reconstruction for Self-Supervised Sound Event Detection</title>
      <link>http://arxiv.org/abs/2502.20857v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了JiTTER，一种改进的自监督学习框架，用于提高基于变压器的声音事件检测模型在时间建模方面的性能。&lt;h4&gt;背景&lt;/h4&gt;自我监督学习方法特别是MAT-SED在声音事件检测（SED）中取得了显著效果。然而，这种技术在捕捉瞬态音频事件和保持时间顺序方面存在不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的自监督框架JiTTER，以增强基于变压器的声音事件检测模型的时间建模能力。&lt;h4&gt;方法&lt;/h4&gt;JiTTER引入了一种分层的随机时间重排重建策略，在块级和帧级随机打乱音频序列，强迫模型重建正确的时序。同时通过在块级重排过程中注入噪声进一步提升特征学习的正则化效果和模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明JiTTER相比MAT-SED提升了5.89%的PSDS值，在DESED数据集上表现出更优性能，说明结构化的时序重建任务比简单的掩码预测更有助于自监督学习在声音事件表示学习中的表现。&lt;h4&gt;结论&lt;/h4&gt;研究发现证明了有结构的时间重建任务对于基于自监督学习的声音事件检测模型来说是一种更为有效的预训练范式。&lt;h4&gt;翻译&lt;/h4&gt;声音事件检测（SED）已从自我监督学习（SSL）方法中显著受益，特别是MAT-SED，该方法利用掩码块预测来恢复丢失的音频片段。然而，尽管在捕捉全局依赖性方面有效，掩码块预测却破坏了瞬态声学事件并且缺乏对时间顺序的明确约束，使其不太适合用于精细粒度的事件边界检测。为了克服这些限制，我们提出了JiTTER（拼图时序变压器事件重建），这是一种增强基于变压器的声音事件检测的时间建模能力的SSL框架。JiTTER引入了一种层次化的随机时间重排重构策略，其中音频序列在块级和帧级上随机打乱，强迫模型恢复正确的顺序。这种预训练目标鼓励模型学习全局事件结构以及瞬态细节，从而提高其识别具有锐利起止特征的声音事件的能力。此外，在块重组过程中我们注入噪声，提供了一种细微的扰动机制以进一步正则化特征学习并增强模型鲁棒性。DESED数据集上的实验结果表明JiTTER优于MAT-SED，PSDS值提高了5.89%，这突出了显式时间推理在基于SSL的声音事件检测中的有效性。我们的研究结果表明结构化的时序重建任务比简单的掩码预测更适合声音事件表示学习的自监督预训练范式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sound event detection (SED) has significantly benefited from self-supervisedlearning (SSL) approaches, particularly masked audio transformer for SED(MAT-SED), which leverages masked block prediction to reconstruct missing audiosegments. However, while effective in capturing global dependencies, maskedblock prediction disrupts transient sound events and lacks explicit enforcementof temporal order, making it less suitable for fine-grained event boundarydetection. To address these limitations, we propose JiTTER (Jigsaw TemporalTransformer for Event Reconstruction), an SSL framework designed to enhancetemporal modeling in transformer-based SED. JiTTER introduces a hierarchicaltemporal shuffle reconstruction strategy, where audio sequences are randomlyshuffled at both the block-level and frame-level, forcing the model toreconstruct the correct temporal order. This pretraining objective encouragesthe model to learn both global event structures and fine-grained transientdetails, improving its ability to detect events with sharp onset-offsetcharacteristics. Additionally, we incorporate noise injection during blockshuffle, providing a subtle perturbation mechanism that further regularizesfeature learning and enhances model robustness. Experimental results on theDESED dataset demonstrate that JiTTER outperforms MAT-SED, achieving a 5.89%improvement in PSDS, highlighting the effectiveness of explicit temporalreasoning in SSL-based SED. Our findings suggest that structured temporalreconstruction tasks, rather than simple masked prediction, offer a moreeffective pretraining paradigm for sound event representation learning.</description>
      <author>example@mail.com (Hyeonuk Nam, Yong-Hwa Park)</author>
      <guid isPermaLink="false">2502.20857v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>AMPLE: Event-Driven Accelerator for Mixed-Precision Inference of Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.21196v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为AMPLE的FPGA加速器，用于改进图神经网络（GNN）在非欧几里得数据上的表现。&lt;h4&gt;背景&lt;/h4&gt;最近，由于其处理非欧几里得数据的性能，图神经网络受到了广泛关注。这些网络因其不规则的记忆访问模式而特别受益于自定义硬件架构，这种模式源于图形结构的稀疏性。&lt;h4&gt;目的&lt;/h4&gt;解决现有FPGA加速器中双缓冲机制的问题，并针对典型图形数据集中节点分布不规则的情况提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;采用事件驱动编程流程的新AMPLE FPGA加速器。开发了混合算术架构，使GNN推理可以以节点级别进行量化。实现了用于优化片外内存访问和最大化节点并行性的预取器。&lt;h4&gt;主要发现&lt;/h4&gt;在引用和社交媒体图数据集上进行了评估，结果表明，在与CPU和GPU对应方相比，平均速度分别提高了243倍和7.2倍。&lt;h4&gt;结论&lt;/h4&gt;采用事件驱动编程流程的AMPLE FPGA加速器显著提升了GNN推理的速度。该方法通过利用混合算术架构、节点级量化以及片外内存访问优化来解决现有FPGA加速器中存在的问题，并在大量图数据集上展示了优秀的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;最近，由于其处理非欧几里得数据的出色表现，图神经网络（GNNs）引起了广泛的关注。这些网络得益于它们不规则的记忆访问模式，这源于图形结构的稀疏性，因此，对定制硬件架构特别有利。然而，现有的FPGA加速器受到双缓冲机制的限制，这种机制未能考虑典型图数据集中的节点分布不规则问题。为了应对这一挑战，我们提出了AMPLE（加速消息传递逻辑引擎），这是一个利用新的事件驱动编程流的新FPGA加速器。我们开发了一种混合算术架构，使GNN推理能够在节点级别进行量化。此外，还实现了用于优化片外内存访问和最大化节点并行性的预取器。在引用和社交媒体图数据集上进行了评估，这些数据集的节点数量从2K到700K不等，结果表明，在与CPU和GPU对应方相比时，平均速度分别提高了243倍和7.2倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have recently gained attention due to theirperformance on non-Euclidean data. The use of custom hardware architecturesproves particularly beneficial for GNNs due to their irregular memory accesspatterns, resulting from the sparse structure of graphs. However, existing FPGAaccelerators are limited by their double buffering mechanism, which doesn'taccount for the irregular node distribution in typical graph datasets. Toaddress this, we introduce \textbf{AMPLE} (Accelerated Message Passing LogicEngine), an FPGA accelerator leveraging a new event-driven programming flow. Wedevelop a mixed-arithmetic architecture, enabling GNN inference to be quantizedat a node-level granularity. Finally, prefetcher for data and instructions isimplemented to optimize off-chip memory access and maximize node parallelism.Evaluation on citation and social media graph datasets ranging from $2$K to$700$K nodes showed a mean speedup of $243\times$ and $7.2\times$ against CPUand GPU counterparts, respectively.</description>
      <author>example@mail.com (Pedro Gimenes, Yiren Zhao, George Constantinides)</author>
      <guid isPermaLink="false">2502.21196v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Incorporating Long-Range Interactions via the Multipole Expansion into Ground and Excited-State Molecular Simulations</title>
      <link>http://arxiv.org/abs/2502.21045v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了FieldMACE，这是基于消息传递原子簇扩展（MACE）架构的一种改进版本，通过引入多极展开来更高效地模拟长程相互作用。&lt;h4&gt;背景&lt;/h4&gt;在分子机器学习势能中，准确捕捉大空间区域内的相互作用是一个重要挑战。&lt;h4&gt;目的&lt;/h4&gt;为了提高对环境和远距离效应的建模效率，特别是在基态和激发态下，本文提出了一种新的架构FieldMACE。&lt;h4&gt;方法&lt;/h4&gt;通过将多极展开集成到MACE架构中，形成一种新框架，称为FieldMACE。&lt;h4&gt;主要发现&lt;/h4&gt;基准评估显示FieldMACE在预测精度、计算效率方面优于先前的架构，并且能够准确模拟非绝热激发态动力学。&lt;h4&gt;结论&lt;/h4&gt;从基础模型中的迁移学习进一步提高了数据利用效率，使FieldMACE成为大规模分子模拟中可扩展、稳健和可转移的框架。&lt;h4&gt;翻译&lt;/h4&gt;模拟长程相互作用一直是分子机器学习势能的重要挑战。本文介绍了一种新的架构FieldMACE，它通过将多极展开整合到消息传递原子簇扩展（MACE）架构中来更高效地建模长程相互作用。FieldMACE能够有效捕捉环境和远距离效应，特别是在基态和激发态下。基准评估表明，与之前的架构相比，FieldMACE在预测准确性、计算效率方面具有优势，并且能准确模拟非绝热激发态动力学。此外，从基础模型中的迁移学习进一步提高了数据利用效率，使得FieldMACE成为一个可扩展的、稳健的以及可转移的大规模分子模拟框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulating long-range interactions remains a significant challenge formolecular machine learning potentials due to the need to accurately captureinteractions over large spatial regions. In this work, we introduce FieldMACE,an extension of the message-passing atomic cluster expansion (MACE)architecture that integrates the multipole expansion to model long-rangeinteractions more efficiently. By incorporating the multipole expansion,FieldMACE effectively captures environmental and long-range effects in bothground and excited states. Benchmark evaluations demonstrate its superiorperformance in predictions and computational efficiency compared to previousarchitectures, as well as its ability to accurately simulate nonadiabaticexcited-state dynamics. Furthermore, transfer learning from foundational modelsenhances data efficiency, making FieldMACE a scalable, robust, and transferableframework for large-scale molecular simulations.</description>
      <author>example@mail.com (Rhyan Barrett, Johannes C. B. Dietschreit, Julia Westermayr)</author>
      <guid isPermaLink="false">2502.21045v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>MESC-3D:Mining Effective Semantic Cues for 3D Reconstruction from a Single Image</title>
      <link>http://arxiv.org/abs/2502.20861v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了MESC-3D，一种从单张图像中重建3D形状的新方法。&lt;h4&gt;背景&lt;/h4&gt;目前的方法主要集中在从图像中提取语义信息并将其简单地与3D点云连接起来，而没有进一步探索这种拼接后的语义特征。这些纠缠的语义特征显著阻碍了重建性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够主动挖掘有效语义线索以提高单张图像中的3D重建质量的方法。&lt;h4&gt;方法&lt;/h4&gt;设计了一个有效的语义挖掘模块和一个三维语义先验学习模块，前者建立了点云与图像语义属性之间的联系，后者利用先验知识增强模型的准确性。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在重建质量和鲁棒性方面显著优于先前的工作，并且具有强大的泛化能力，在零样本性能上也表现出色。&lt;h4&gt;结论&lt;/h4&gt;MESC-3D通过主动挖掘有效语义线索和使用三维语义先验知识，提高了单张图像中的3D重建的准确性和现实感。&lt;h4&gt;翻译&lt;/h4&gt;从单张图像中重建3D形状在计算机视觉领域扮演着重要角色。许多方法已经被提出并取得了显著的成绩。然而，现有的方法主要集中在提取图像中的语义信息，并简单地将其与3D点云连接起来而没有进一步探索这种拼接后的语义特征。这些纠缠的语义特征极大地阻碍了重建性能。本文提出了一个名为MESC-3D的新方法，它可以主动挖掘有效语义线索以改进单张图像的3D重建效果。具体而言，设计了一个有效的语义挖掘模块来建立点云和图像语义属性之间的联系，并使点云能够自主选择所需信息。此外，为了解决单一图像中的语义信息可能存在的不足问题（如遮挡），受人类利用日常经验中获得的先验知识表示3D对象能力的启发，我们引入了三维语义先验学习模块。此模块集成了对空间结构的语义理解，使模型能够更准确地解释和重建3D对象，并且在复杂3D环境感知方面更加贴近人类的认知。广泛的评估显示，与先前的工作相比，我们的方法在重建质量和鲁棒性方面取得了显著改进。此外，进一步的实验验证了该方法的强大泛化能力和零样本性能上的优越表现。代码可在https://github.com/QINGQINGLE/MESC-3D上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reconstructing 3D shapes from a single image plays an important role incomputer vision. Many methods have been proposed and achieve impressiveperformance. However, existing methods mainly focus on extracting semanticinformation from images and then simply concatenating it with 3D point cloudswithout further exploring the concatenated semantics. As a result, theseentangled semantic features significantly hinder the reconstructionperformance. In this paper, we propose a novel single-image 3D reconstructionmethod called Mining Effective Semantic Cues for 3D Reconstruction from aSingle Image (MESC-3D), which can actively mine effective semantic cues fromentangled features. Specifically, we design an Effective Semantic Mining Moduleto establish connections between point clouds and image semantic attributes,enabling the point clouds to autonomously select the necessary information.Furthermore, to address the potential insufficiencies in semantic informationfrom a single image, such as occlusions, inspired by the human ability torepresent 3D objects using prior knowledge drawn from daily experiences, weintroduce a 3D Semantic Prior Learning Module. This module incorporatessemantic understanding of spatial structures, enabling the model to interpretand reconstruct 3D objects with greater accuracy and realism, closely mirroringhuman perception of complex 3D environments. Extensive evaluations show thatour method achieves significant improvements in reconstruction quality androbustness compared to prior works. Additionally, further experiments validatethe strong generalization capabilities and excels in zero-shot preformance onunseen classes. Code is available at https://github.com/QINGQINGLE/MESC-3D.</description>
      <author>example@mail.com (Shaoming Li, Qing Cai, Songqi Kong, Runqing Tan, Heng Tong, Shiji Qiu, Yongguo Jiang, Zhi Liu)</author>
      <guid isPermaLink="false">2502.20861v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Foundation Models -- A Panacea for Artificial Intelligence in Pathology?</title>
      <link>http://arxiv.org/abs/2502.21264v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  50 pages, 15 figures and an appendix (study protocol) which is  previously published, see https://doi.org/10.1101/2024.07.04.24309948&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了基于自监督预训练的大型基础模型（FMs）在前列腺癌诊断和Gleason分级任务中的临床应用效果，发现尽管这些模型在数据稀缺的情况下表现出一定的实用性，但在充分标注的数据集上其性能可能不及针对特定任务训练的模型。&lt;h4&gt;背景&lt;/h4&gt;人工智能在病理学中的角色已从辅助诊断发展到揭示整体切片图像（WSIs）中预测形态模式。近年来，基于自监督预训练的基础模型被广泛推崇为适用于各种下游任务的通用解决方案。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过大规模验证AI系统，在前列腺癌诊断和Gleason分级任务上评估基础模型与特定任务端到端学习模型之间的性能差异。&lt;h4&gt;方法&lt;/h4&gt;研究使用超过10万名患者的7342例核心针活检数据，涵盖全球15个地点的11个国家。采用多实例学习框架对两种基础模型和一个完全端到端训练的任务特定模型进行比较。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果挑战了基础模型总是优于任务特定模型的观点。在缺乏标注数据的情况下，基础模型显示出一定优势；然而，在有足够的标记训练数据时，其性能与特有任务模型相匹配或甚至被超越。此外，特定于任务的培训显著减少了临床重要分级错误和难以识别形态的误诊，并降低了不同WSI扫描器间的变异性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，尽管基础模型在快速原型设计和研究中具有明显优势，但它们作为适用于临床应用的医疗AI通用解决方案的角色仍然不确定。对于高风险的应用场景，严格的验证和对特定任务培训的关注仍然是至关重要的。研究者建议将基础模型与端到端学习的优点相结合，以实现适合临床使用的稳健且资源高效的病理学AI解决方案。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要描述了人工智能在病理诊断中的作用从辅助诊断到发现整个切片图像中预测性形态模式的发展历程，并讨论了基于自监督预训练的基础模型是否能成为适用于各种任务的通用解方案。研究结果表明，基础模型虽然在数据稀缺的情况下有其独特优势，但在大量标记训练数据下可能不及特定任务模型的表现。这项工作强调，在高风险临床应用中，必须进行严格的验证以确定最佳解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The role of artificial intelligence (AI) in pathology has evolved from aidingdiagnostics to uncovering predictive morphological patterns in whole slideimages (WSIs). Recently, foundation models (FMs) leveraging self-supervisedpre-training have been widely advocated as a universal solution for diversedownstream tasks. However, open questions remain about their clinicalapplicability and generalization advantages over end-to-end learning usingtask-specific (TS) models. Here, we focused on AI with clinical-gradeperformance for prostate cancer diagnosis and Gleason grading. We present thelargest validation of AI for this task, using over 100,000 core needle biopsiesfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with afully end-to-end TS model in a multiple instance learning framework. Ourfindings challenge assumptions that FMs universally outperform TS models. WhileFMs demonstrated utility in data-scarce scenarios, their performance convergedwith - and was in some cases surpassed by - TS models when sufficient labeledtraining data were available. Notably, extensive task-specific trainingmarkedly reduced clinically significant misgrading, misdiagnosis of challengingmorphologies, and variability across different WSI scanners. Additionally, FMsused up to 35 times more energy than the TS model, raising concerns about theirsustainability. Our results underscore that while FMs offer clear advantagesfor rapid prototyping and research, their role as a universal solution forclinically applicable medical AI remains uncertain. For high-stakes clinicalapplications, rigorous validation and consideration of task-specific trainingremain critically important. We advocate for integrating the strengths of FMsand end-to-end learning to achieve robust and resource-efficient AI pathologysolutions fit for clinical use.</description>
      <author>example@mail.com (Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo)</author>
      <guid isPermaLink="false">2502.21264v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>On the Role of Individual Differences in Current Approaches to Computational Image Aesthetics</title>
      <link>http://arxiv.org/abs/2502.20518v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个理论框架，用于解释在图像美学评估（IAA）任务中从通用模型转移到个人化模型的机制，并通过实验证明了不同群体和个体间存在的显著性能差异。&lt;h4&gt;背景&lt;/h4&gt;当前的图像美学评估方法分为两个阶段：第一阶段使用通用图像美学评估（GIAA）模型来估计平均分数，第二阶段利用转移学习的个性化图像美学评估（PIAA）模型来适应用户的主观性。然而，这种从GIAA到PIAA的理论理解仍不充分。&lt;h4&gt;目的&lt;/h4&gt;本文旨在建立一个理论基础，并提出了一种统一模型，该模型可以同时处理个体和群体评估任务。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一个编码个人特征并在分布格式中表示的统一模型。实验通过不同的群体制样进行了验证，包括根据组大小进行子采样和分离的人口统计学变量。&lt;h4&gt;主要发现&lt;/h4&gt;1. 转移学习从GIAA到PIAA涉及外推，反之则是内插；2. 教育水平是影响美学差异的主要因素，其次是摄影艺术经验；3. 在艺术品中观察到了更强的个人主观性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的模型能够同时支持通用和个性化图像美学评估，并且可以提高在不同人口统计学群体中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;图像美学评估（IAA）是一种复杂任务，由于图像多样性和用户主体性的存在而变得更为复杂。当前的方法将其分为两个阶段：第一阶段使用通用的图像美学评估模型来估计平均分数；第二阶段利用转移学习将GIAA适应为PIAA以融入用户的主观性。然而，缺乏关于在GIAA和PIAA之间进行转移学习时的理论理解，特别是在考虑群体构成、群体规模、个体之间的审美差异以及人口统计学相关性的背景下。本文提出了一个统一模型，该模型使用分布形式编码个人特征来进行个体及群体制评估。我们证明了从GIAA转移到PIAA涉及外推而相反则为内插，后者通常对机器学习更有益。通过对不同构成的群体进行实验（包括按组大小子采样和分离的人口统计学变量）发现，即使对于GIAA来说，性能也表现出显著变化，表明平均分数并不能完全消除个体主观性。性能差异分析以及基尼指数分析显示教育水平是影响审美差异的主要因素，其次是摄影及艺术经验，在艺术品中观察到更强的个人主体性。我们的模型独特地支持了通用和个性化图像美学评估，并提高了不同人口统计学群体中的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Image aesthetic assessment (IAA) evaluates image aesthetics, a taskcomplicated by image diversity and user subjectivity. Current approachesaddress this in two stages: Generic IAA (GIAA) models estimate mean aestheticscores, while Personal IAA (PIAA) models adapt GIAA using transfer learning toincorporate user subjectivity. However, a theoretical understanding of transferlearning between GIAA and PIAA, particularly concerning the impact of groupcomposition, group size, aesthetic differences between groups and individuals,and demographic correlations, is lacking. This work establishes a theoreticalfoundation for IAA, proposing a unified model that encodes individualcharacteristics in a distributional format for both individual and groupassessments. We show that transferring from GIAA to PIAA involvesextrapolation, while the reverse involves interpolation, which is generallymore effective for machine learning. Experiments with varying groupcompositions, including sub-sampling by group size and disjoint demographics,reveal significant performance variation even for GIAA, indicating that meanscores do not fully eliminate individual subjectivity. Performance variationsand Gini index analysis reveal education as the primary factor influencingaesthetic differences, followed by photography and art experience, withstronger individual subjectivity observed in artworks than in photos. Our modeluniquely supports both GIAA and PIAA, enhancing generalization acrossdemographics.</description>
      <author>example@mail.com (Li-Wei Chen, Ombretta Strafforello, Anne-Sofie Maerten, Tinne Tuytelaars, Johan Wagemans)</author>
      <guid isPermaLink="false">2502.20518v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Modeling Human Beliefs about AI Behavior for Scalable Oversight</title>
      <link>http://arxiv.org/abs/2502.21262v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  53 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了如何通过建模人类评估者的信念来改进对AI系统的监督，以解决随着AI能力提升而导致的人类反馈可靠性降低的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的AI系统通常依赖于人类反馈来学习人类的价值观和偏好。然而，随着AI系统的能力增强，这种人类反馈变得越来越不可靠。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过建模人的信念来提高对超出人类能力范围的AI系统的监督效率。&lt;h4&gt;方法&lt;/h4&gt;提出了形式化的模型来描述人对于AI行为的看法，并分析了这些模型在推断人类价值观中的作用。同时引入了一个放松版的人类信念模型覆盖概念，以减少依赖于精确信念模型的需求。&lt;h4&gt;主要发现&lt;/h4&gt;通过理论分析和实验研究揭示了如何利用基础模型构建覆盖信念模型的潜力，为可扩展监督提供了新方法。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够帮助更好地理解人类反馈，并且提供了一种使用基础AI系统来提高监督效率的新途径。&lt;h4&gt;翻译&lt;/h4&gt;当代的人工智能（AI）对齐工作经常依赖于人类反馈来教导AI系统学习人类的价值观和偏好。然而，随着AI系统的功能增强，这种人类反馈变得越来越不可靠。这导致了一个可扩展监控的问题：如何监管超出人类能力的AI系统？在这项工作中，我们提出通过建模人评估者对于AI行为的看法来更好地解释人的反馈。我们将人类信念模型形式化，并从理论上分析它们在推断人类价值观中的作用。然后描述了这种推理中剩余的不确定性以及这些不确定性消失的情况条件。为了减少对精确信念模型的依赖，我们引入了一个放松版的人类信念模型覆盖概念。最后，我们建议使用基础模型来构建覆盖信念模型，为可扩展监督提供了一种新的潜在方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contemporary work in AI alignment often relies on human feedback to teach AIsystems human preferences and values. Yet as AI systems grow more capable,human feedback becomes increasingly unreliable. This raises the problem ofscalable oversight: How can we supervise AI systems that exceed humancapabilities? In this work, we propose to model the human evaluator's beliefsabout the AI system's behavior to better interpret the human's feedback. Weformalize human belief models and theoretically analyze their role in inferringhuman values. We then characterize the remaining ambiguity in this inferenceand conditions for which the ambiguity disappears. To mitigate reliance onexact belief models, we then introduce the relaxation of human belief modelcovering. Finally, we propose using foundation models to construct coveringbelief models, providing a new potential approach to scalable oversight.</description>
      <author>example@mail.com (Leon Lang, Patrick Forré)</author>
      <guid isPermaLink="false">2502.21262v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Dimension Agnostic Neural Processes</title>
      <link>http://arxiv.org/abs/2502.20661v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, Accepted to ICLR 2025 (International Conference  on Learning Representations)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的元学习模型Dimension Agnostic Neural Processes(DANP)，该模型通过引入Dimension Aggregator Block(DAB)和Transformer架构，增强了处理不同维度输入的能力，并在各种回归任务上显示出优越性能。&lt;h4&gt;背景&lt;/h4&gt;传统的Neural Process(NP)方法虽然能够提取跨多种任务的数据共享特征并预测不确定性，但在适应不同输入维度的任务时面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种元学习模型DANP，以克服传统NP模型的局限性，并提升其在回归任务中的适用性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入Dimension Aggregator Block(DAB)将输入特征转换为固定维度的空间，同时采用Transformer架构和潜在编码层来学习更具普适性的特征。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了DANP模型相比于现有NP变体在合成数据集与实际回归任务上的优越性。&lt;h4&gt;结论&lt;/h4&gt;DANP展示了其解决传统NP模型局限性和广泛适应各种回归场景的潜力，具有较高的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;元学习的目标是训练可以使用有限标注数据推广到新任务的模型，通过提取多样任务数据集中的共享特征。此外，在训练和评估期间考虑预测不确定性，这是一个称为不确定感知元学习的概念。神经过程(NP)是一种著名的不确定感知元学习方法，它利用参数化神经网络构建隐式随机过程，使快速适应新任务成为可能。然而，现有的NP方法在处理多样输入维度和学习特征方面存在挑战，限制了它们在回归任务中的广泛应用性。为了克服这些局限并提高NP模型作为通用回归器的实用性，我们引入了Dimension Agnostic Neural Processes(DANP)。DANP采用Dimension Aggregator Block(DAB)，将输入特征转换为固定维度空间，增强模型处理多样数据集的能力；同时利用Transformer架构和潜在编码层，学习可跨多种任务泛化的更广泛特征。通过在各种合成和实际回归任务上的综合实验，我们实证显示了DANP优于先前的NP变体，在克服传统NP模型局限性方面展示出其有效性及其应用于多样化回归场景的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Meta-learning aims to train models that can generalize to new tasks withlimited labeled data by extracting shared features across diverse taskdatasets. Additionally, it accounts for prediction uncertainty during bothtraining and evaluation, a concept known as uncertainty-aware meta-learning.Neural Process(NP) is a well-known uncertainty-aware meta-learning method thatconstructs implicit stochastic processes using parametric neural networks,enabling rapid adaptation to new tasks. However, existing NP methods facechallenges in accommodating diverse input dimensions and learned features,limiting their broad applicability across regression tasks. To address theselimitations and advance the utility of NP models as general regressors, weintroduce Dimension Agnostic Neural Processes(DANP). DANP incorporatesDimension Aggregator Block(DAB) to transform input features into afixed-dimensional space, enhancing the model's ability to handle diversedatasets. Furthermore, leveraging the Transformer architecture and latentencoding layers, DANP learns a wider range of features that are generalizableacross various tasks. Through comprehensive experimentation on varioussynthetic and practical regression tasks, we empirically show that DANPoutperforms previous NP variations, showcasing its effectiveness in overcomingthe limitations of traditional NP models and its potential for broaderapplicability in diverse regression scenarios.</description>
      <author>example@mail.com (Hyungi Lee, Chaeyun Jang, Dongbok Lee, Juho Lee)</author>
      <guid isPermaLink="false">2502.20661v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Parallel-Learning of Invariant and Tempo-variant Attributes of Single-Lead Cardiac Signals: PLITA</title>
      <link>http://arxiv.org/abs/2502.21162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in The 39th Annual AAAI Conference on Artificial  Intelligence. Main Track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的自监督学习方法PLITA，用于捕捉单导联心电图（ECG）信号中的不变和时变属性。&lt;h4&gt;背景&lt;/h4&gt;穿戴式传感设备在未来的数字健康领域中将发挥重要作用。目前的自监督学习方法只能编码不变属性，忽略了反映状态变化的时间变异信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够同时捕捉时间不变和时间变异心电图特征的新SSL方法。&lt;h4&gt;方法&lt;/h4&gt;通过强制相邻时间点输入的空间表示更加接近来捕获时变属性。&lt;h4&gt;主要发现&lt;/h4&gt;PLITA在时间变异属性起重要作用的设置中表现出显著更好的性能。&lt;h4&gt;结论&lt;/h4&gt;PLITA是一种有效的自监督学习框架，能够有效处理心电图信号中的不变和时变信息。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感设备如Holter监护仪将在未来的数字健康领域扮演关键角色。无监督的学习框架（例如自我监督学习）对于将这些单导联的心电信号映射到预期的临床结果至关重要。这种信号具有时间变异成分，其模式随记录过程而演变，并且还存在不变成分，其模式保持不变。然而，现有的SSL方法只能驱动模型编码不变属性，导致模型忽略反映状态变化的时间变异信息。本文介绍了一种新的SSL方法——并行学习不变和时变属性（PLITA），该方法旨在捕捉这两种类型的心电图特征。通过强制相邻时间点输入的空间表示更加接近来捕获时变属性。我们评估了此方法在学习两种不同类型的特征方面的能力，以及与现有ECG分析的SSL方法相比的性能表现。在时间变异属性起重要作用的情况下，PLITA表现出显著更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensing devices, such as Holter monitors, will play a crucial rolein the future of digital health. Unsupervised learning frameworks such asSelf-Supervised Learning (SSL) are essential to map these single-leadelectrocardiogram (ECG) signals with their anticipated clinical outcomes. Thesesignals are characterized by a tempo-variant component whose patterns evolvethrough the recording and an invariant component with patterns that remainunchanged. However, existing SSL methods only drive the model to encode theinvariant attributes, leading the model to neglect tempo-variant informationwhich reflects subject-state changes through time. In this paper, we presentParallel-Learning of Invariant and Tempo-variant Attributes (PLITA), a novelSSL method designed for capturing both invariant and tempo-variant ECGattributes. The latter are captured by mandating closer representations inspace for closer inputs on time. We evaluate both the capability of the methodto learn the attributes of these two distinct kinds, as well as PLITA'sperformance compared to existing SSL methods for ECG analysis. PLITA performssignificantly better in the set-ups where tempo-variant attributes play a majorrole.</description>
      <author>example@mail.com (Adtian Atienza, Jakob E. Bardram, Sadasivan Puthusserypady)</author>
      <guid isPermaLink="false">2502.21162v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Adversarial Text Representation Learning for Affective Recognition</title>
      <link>http://arxiv.org/abs/2502.20613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, The 7th International Conference on Artificial  Intelligence in Information and Communication (ICAIIC 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种增强情感感知嵌入的框架，旨在改进基于变压器模型的情感识别能力。', '背景': '预训练语言模型在语义理解方面表现出色，但在捕捉细微的情感信息方面存在困难。', '目的': '通过引入连续的效价唤醒标签系统和动态令牌扰动机制来提高情感敏感性。', '方法': '采用连续的valence-arousal标注体系进行对比学习，并利用基于梯度的方法强调与情感相关的令牌。', '主要发现': '实验结果表明，该框架在情绪分类基准上比现有方法平均提高了15.5%。', '结论': '所提出的框架有效增强了情感表示的学习能力，并能实现精确且上下文相关的情感理解。', '翻译': '摘要原文的中文翻译。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While pre-trained language models excel at semantic understanding, they oftenstruggle to capture nuanced affective information critical for affectiverecognition tasks. To address these limitations, we propose a novel frameworkfor enhancing emotion-aware embeddings in transformer-based models. Ourapproach introduces a continuous valence-arousal labeling system to guidecontrastive learning, which captures subtle and multi-dimensional emotionalnuances more effectively. Furthermore, we employ a dynamic token perturbationmechanism, using gradient-based saliency to focus on sentiment-relevant tokens,improving model sensitivity to emotional cues. The experimental resultsdemonstrate that the proposed framework outperforms existing methods, achievingup to 15.5% improvement in the emotion classification benchmark, highlightingthe importance of employing continuous labels. This improvement demonstratesthat the proposed framework is effective in affective representation learningand enables precise and contextually relevant emotional understanding.</description>
      <author>example@mail.com (Seungah Son, Andrez Saurez, Dongsoo Har)</author>
      <guid isPermaLink="false">2502.20613v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Subtask-Aware Visual Reward Learning from Segmented Demonstrations</title>
      <link>http://arxiv.org/abs/2502.20630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project webpage: https://changyeon.site/reds/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;强化学习（RL）代理在各种机器人任务中展示了其潜力。然而，它们仍然严重依赖于人工设计的奖励函数，并且需要大量的试错以及目标行为信息，在现实世界的应用场景中这些信息往往是不可用的。本文提出了REDs：一种从演示视频片段中学习回报的新框架，该框架利用无动作标记视频进行最小监督的学习。具体而言，REDs使用来自不同来源的动作分段视频并将它们视为真实奖励信号。我们训练一个基于视频片段和相应子任务的密集奖励函数，并通过最小化等价策略不变比较距离来确保与真实奖励信号对齐。此外，我们采用对比学习目标以使视频表示与子任务保持一致，在线交互时可以实现精确的子任务推理。实验表明，REDs在Meta-World中的复杂机器人操作任务以及FurnitureBench中的家具组装等更具挑战性的现实世界任务中显著优于基线方法，并且只需要最小的人工干预。&lt;h4&gt;背景&lt;/h4&gt;强化学习代理已经在多种机器人任务上展示了其潜力，但这些代理依然严重依赖于人工设计的奖励函数。这需要大量的试错过程和对目标行为信息的访问，在许多实际应用场合下这类信息是难以获得的。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的从演示中无监督地学习回报的方法REDs，该方法能够利用来自不同来源的视频演示片段进行训练，并且只需要最小的人工干预。&lt;h4&gt;方法&lt;/h4&gt;1. REDs使用动作标记的视频作为输入并将其分割成子任务。2. 利用这些子任务和它们对应的视频段来学习密集奖励函数。3. 通过优化特定的目标函数（即等价策略不变比较距离）以确保奖励信号与真实信号一致。4. 使用对比学习目标使视频表示与子任务保持一致，从而在在线交互中实现精确的子任务推理。&lt;h4&gt;主要发现&lt;/h4&gt;REDs框架能够在Meta-World中的复杂机器人操作任务以及家具组装等更具挑战性的现实世界任务上表现出色，并且显著优于基线方法。此外，在最小的人工干预下该模型还能够推广到未知的任务和机器人的实例中，展示出其在多变环境下的可扩展性。&lt;h4&gt;结论&lt;/h4&gt;REDs是一种高效的学习框架，它通过利用视频演示片段中的信息来减轻对人工设计奖励函数的依赖，并且能够在多种机器人操作任务上表现出色。此外，REDs显示出良好的泛化能力，这表明它具有广泛的应用潜力和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement Learning (RL) agents have demonstrated their potential acrossvarious robotic tasks. However, they still heavily rely on human-engineeredreward functions, requiring extensive trial-and-error and access to targetbehavior information, often unavailable in real-world settings. This paperintroduces REDS: REward learning from Demonstration with Segmentations, a novelreward learning framework that leverages action-free videos with minimalsupervision. Specifically, REDS employs video demonstrations segmented intosubtasks from diverse sources and treats these segments as ground-truthrewards. We train a dense reward function conditioned on video segments andtheir corresponding subtasks to ensure alignment with ground-truth rewardsignals by minimizing the Equivalent-Policy Invariant Comparison distance.Additionally, we employ contrastive learning objectives to align videorepresentations with subtasks, ensuring precise subtask inference during onlineinteractions. Our experiments show that REDS significantly outperforms baselinemethods on complex robotic manipulation tasks in Meta-World and morechallenging real-world tasks, such as furniture assembly in FurnitureBench,with minimal human intervention. Moreover, REDS facilitates generalization tounseen tasks and robot embodiments, highlighting its potential for scalabledeployment in diverse environments.</description>
      <author>example@mail.com (Changyeon Kim, Minho Heo, Doohyun Lee, Jinwoo Shin, Honglak Lee, Joseph J. Lim, Kimin Lee)</author>
      <guid isPermaLink="false">2502.20630v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Dynamically Local-Enhancement Planner for Large-Scale Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.21134v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了Dynamically Local-Enhancement (DLE) Planner，一种在不永久修改基本驾驶规划器的情况下通过局部驾驶数据动态增强驾驶规划的技术。&lt;h4&gt;背景&lt;/h4&gt;当前自主车辆主要限于特定区域运行，但对更广泛的应用需求日益增长。随着模型规模的扩大，有限的容量成为适应新场景的重大挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来解决单个庞大模型在处理新情况时效率低下的问题，通过局部驾驶数据动态增强基本驾驶规划器以提高自主驾驶系统的可扩展性而不显著增加规划器的大小。&lt;h4&gt;方法&lt;/h4&gt;引入了位置变化的马尔科夫决策过程(MDP)与图神经网络结合使用的方法，从本地观察数据中提取特定区域的驾驶特征，并利用学习到的特征增强基于强化学习的基本策略。&lt;h4&gt;主要发现&lt;/h4&gt;在多个场景下评估该方法并与适用于所有情况的单一驾驶模型比较后，结果显示本方法在安全性和平均奖励方面都优于基准策略，同时保持较低的规模。&lt;h4&gt;结论&lt;/h4&gt;这种技术有潜力使大规模自主车辆受益，无需大幅扩展设备上的驾驶模型。&lt;h4&gt;翻译&lt;/h4&gt;当前自主车辆主要限于特定区域运行。随着对更广泛应用的需求增加，现有模型在处理新场景时表现出容量限制问题。单个庞大模型难以适应新的情况。本文提出Dynamically Local-Enhancement (DLE) Planner，该方法通过局部驾驶数据动态增强基本驾驶规划器，不需永久修改其本身。通过位置变化的马尔科夫决策过程结合图神经网络从本地观察数据中提取区域特定的驾驶特征，使用这些特征来增强基于强化学习的基本策略。实验结果表明，在安全性和平均奖励方面优于基准模型，并保持较小规模，具备大规模自主车辆应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current autonomous vehicles operate primarily within limited regions, butthere is increasing demand for broader applications. However, as models scale,their limited capacity becomes a significant challenge for adapting to novelscenarios. It is increasingly difficult to improve models for new situationsusing a single monolithic model. To address this issue, we introduce theconcept of dynamically enhancing a basic driving planner with local drivingdata, without permanently modifying the planner itself. This approach, termedthe Dynamically Local-Enhancement (DLE) Planner, aims to improve thescalability of autonomous driving systems without significantly expanding theplanner's size. Our approach introduces a position-varying Markov DecisionProcess formulation coupled with a graph neural network that extractsregion-specific driving features from local observation data. The learnedfeatures describe the local behavior of the surrounding objects, which is thenleveraged to enhance a basic reinforcement learning-based policy. We evaluatedour approach in multiple scenarios and compared it with a one-for-all drivingmodel. The results show that our method outperforms the baseline policy in bothsafety (collision rate) and average reward, while maintaining a lighter scale.This approach has the potential to benefit large-scale autonomous vehicleswithout the need for largely expanding on-device driving models.</description>
      <author>example@mail.com (Nanshan Deng, Weitao Zhou, Bo Zhang, Junze Wen, Kun Jiang, Zhong Cao, Diange Yang)</author>
      <guid isPermaLink="false">2502.21134v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CuPID: Leveraging Masked Single-Lead ECG Modelling for Enhancing the Representations</title>
      <link>http://arxiv.org/abs/2502.21127v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章介绍了一种新型的Masked Data Modelling (MDM) 方法CuPID，该方法专门针对单导联ECG数据设计，通过提供频谱图上下文信息来增强现有MDM技术。&lt;h4&gt;背景&lt;/h4&gt;穿戴式传感设备如心电图(ECG)心率监测器将在数字健康领域发挥重要作用。这种持续监控导致了大量的未标注数据，促进了无监督学习框架的发展。&lt;h4&gt;目的&lt;/h4&gt;开发一种适用于单导联ECG的无监督学习方法，克服现有MDM技术在处理不规则心跳间隔时的局限性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种称为Cueing the Predictor Increments the Detailing (CuPID) 的新方法。该方法通过向解码器提供频谱图上下文信息来改善现有的MDM技术。&lt;h4&gt;主要发现&lt;/h4&gt;CuPID 方法在编码器性能上有了显著的提升，特别是在各种不同的配置下。此外，在多个下游任务中，CuPID 表现超过了现有最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;CuPID是一种有效的改进MDM技术的方法，特别适用于处理单导联ECG数据，并且在广泛的配置和应用中优于现有的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;可穿戴传感设备，如心电图(ECG) 心率监测器，在数字健康领域将扮演重要角色。持续的监控导致了大量的未标记数据，促使开发无监督学习框架的需求。尽管Masked Data Modelling (MDM) 技术已广泛使用，但直接应用于单导联ECG 数据的效果不佳，因为解码器在没有上下文信息的情况下难以处理不规则的心跳间隔。本文提出了一种称为Cueing the Predictor Increments the Detailing (CuPID) 的新型MDM方法，专门针对单导联ECG 设计。通过向解码器提供由频谱图派生的上下文，CuPID 增强了现有的MDM 技术，从而激励编码器生成更详细的表示。这极大地影响了编码器在各种不同配置下的性能表现，使得CuPID 在多种下游任务中超越了现有最先进技术的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wearable sensing devices, such as Electrocardiogram (ECG) heart-ratemonitors, will play a crucial role in the future of digital health. Thiscontinuous monitoring leads to massive unlabeled data, incentivizing thedevelopment of unsupervised learning frameworks. While Masked Data Modelling(MDM) techniques have enjoyed wide use, their direct application to single-leadECG data is suboptimal due to the decoder's difficulty handling irregularheartbeat intervals when no contextual information is provided. In this paper,we present Cueing the Predictor Increments the Detailing (CuPID), a novel MDMmethod tailored to single-lead ECGs. CuPID enhances existing MDM techniques bycueing spectrogram-derived context to the decoder, thus incentivizing theencoder to produce more detailed representations. This has a significant impacton the encoder's performance across a wide range of different configurations,leading CuPID to outperform state-of-the-art methods in a variety of downstreamtasks.</description>
      <author>example@mail.com (Adtian Atienza, Gouthamaan Manimaran, Jakob E. Bardram, Sadasivan Puthusserypady)</author>
      <guid isPermaLink="false">2502.21127v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Few-Shot, No Problem: Descriptive Continual Relation Extraction</title>
      <link>http://arxiv.org/abs/2502.20596v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于检索的解决方案来解决少样本持续关系抽取问题，该方案通过大语言模型生成关系描述，并利用双编码器检索训练范式增强样本和类别表示学习。&lt;h4&gt;背景&lt;/h4&gt;传统的内存基础方法在面对有限样本时容易过拟合，无法巩固旧知识，在少样本场景中数据稀疏进一步阻碍了有效的隐空间数据增强。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来解决少样本持续关系抽取中的挑战，并保持模型在一系列任务上的鲁棒性能，同时减少灾难性遗忘问题。&lt;h4&gt;方法&lt;/h4&gt;首先使用大语言模型为每个关系生成描述；然后引入双编码器检索训练范式以丰富样本和类别表示学习；最后设计基于检索的预测方法，其中每个样本通过整合关系描述向量和类原型的反向排名融合得分来“检索”最佳匹配的关系。&lt;h4&gt;主要发现&lt;/h4&gt;在多个数据集上的广泛实验表明该方法显著地提高了性能，并且在整个顺序任务中保持了鲁棒性，有效地解决了灾难性遗忘问题。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为解决少样本持续关系抽取的挑战提供了一种有效的解决方案，通过利用增强表示来促进模型学习和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot Continual Relation Extraction is a crucial challenge for enabling AIsystems to identify and adapt to evolving relationships in dynamic real-worlddomains. Traditional memory-based approaches often overfit to limited samples,failing to reinforce old knowledge, with the scarcity of data in few-shotscenarios further exacerbating these issues by hindering effective dataaugmentation in the latent space. In this paper, we propose a novelretrieval-based solution, starting with a large language model to generatedescriptions for each relation. From these descriptions, we introduce abi-encoder retrieval training paradigm to enrich both sample and classrepresentation learning. Leveraging these enhanced representations, we design aretrieval-based prediction method where each sample "retrieves" the bestfitting relation via a reciprocal rank fusion score that integrates bothrelation description vectors and class prototypes. Extensive experiments onmultiple datasets demonstrate that our method significantly advances thestate-of-the-art by maintaining robust performance across sequential tasks,effectively addressing catastrophic forgetting.</description>
      <author>example@mail.com (Nguyen Xuan Thanh, Anh Duc Le, Quyen Tran, Thanh-Thien Le, Linh Ngo Van, Thien Huu Nguyen)</author>
      <guid isPermaLink="false">2502.20596v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Best Foot Forward: Robust Foot Reconstruction in-the-wild</title>
      <link>http://arxiv.org/abs/2502.20511v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;准确的3D脚部重建对于个性化矫形器、数字医疗和虚拟试穿至关重要。&lt;h4&gt;背景&lt;/h4&gt;现有的方法在处理不完整的扫描数据以及解剖变异时遇到困难，特别是在用户移动受限的情况下（例如自我扫描场景）难以捕捉到像足弓和后跟这样的区域。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的端到端管道来改进结构从运动（SfM）重建过程。&lt;h4&gt;方法&lt;/h4&gt;该方法首先使用SE(3)正则化结合视角预测模块解决扫描对齐的不确定性，然后通过基于注意力机制的网络训练在合成增强点云上的几何补充缺失部分。&lt;h4&gt;主要发现&lt;/h4&gt;该技术实现了同类最佳性能，同时保持了临床验证的解剖学精确度。通过结合合成数据与学习到的几何先验知识，使足部重建能够适应真实世界捕捉条件下的各种情况。&lt;h4&gt;结论&lt;/h4&gt;此方法为基于移动设备的3D扫描在医疗和零售领域的应用开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;准确的三维脚部重建对于个性化矫形器、数字健康护理以及虚拟试穿至关重要。然而，现有的技术难以应对不完整扫描及解剖变异的问题，在自我扫描等情况下尤其困难，因为用户移动受限影响对足弓和后跟等区域的捕捉。我们提出了一种新颖的端到端流程来改进结构从运动重建过程，首先通过SE(3)正则化结合视角预测模块解决扫描对齐问题，再利用基于注意力机制训练在合成增强点云上的网络补充缺失几何部分。此方法实现了同类最佳性能，并保持了临床验证的解剖精确度。借助合成数据和学习到的几何先验知识，在真实世界捕捉条件下实现稳健的脚部重建。这为医疗与零售领域中基于移动设备的3D扫描应用开辟了新的机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D foot reconstruction is crucial for personalized orthotics,digital healthcare, and virtual fittings. However, existing methods strugglewith incomplete scans and anatomical variations, particularly in self-scanningscenarios where user mobility is limited, making it difficult to capture areaslike the arch and heel. We present a novel end-to-end pipeline that refinesStructure-from-Motion (SfM) reconstruction. It first resolves scan alignmentambiguities using SE(3) canonicalization with a viewpoint prediction module,then completes missing geometry through an attention-based network trained onsynthetically augmented point clouds. Our approach achieves state-of-the-artperformance on reconstruction metrics while preserving clinically validatedanatomical fidelity. By combining synthetic training data with learnedgeometric priors, we enable robust foot reconstruction under real-world captureconditions, unlocking new opportunities for mobile-based 3D scanning inhealthcare and retail.</description>
      <author>example@mail.com (Kyle Fogarty, Jing Yang, Chayan Kumar Patodi, Aadi Bhanti, Steven Chacko, Cengiz Oztireli, Ujwal Bonde)</author>
      <guid isPermaLink="false">2502.20511v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Information Bottleneck-Guided Heterogeneous Graph Learning for Interpretable Neurodevelopmental Disorder Diagnosis</title>
      <link>http://arxiv.org/abs/2502.20769v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架I2B-HGNN，用于从神经发育障碍中提取有意义的生物标志物，并进行诊断。&lt;h4&gt;背景&lt;/h4&gt;现有的机器学习模型在提供综合可解释性方面存在挑战，尤其是在处理复杂的数据编码、解码和融合时。这些模型往往难以从成像数据（如fMRI）中抽取有用的生物标记物，也缺乏解释非成像数据重要性的机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以诊断神经发育障碍的可解释机器学习框架，该框架能够有效地利用成像与非成像多模态数据并提供清晰的结果解释。&lt;h4&gt;方法&lt;/h4&gt;提出了Interpretable Information Bottleneck Heterogeneous Graph Neural Network (I2B-HGNN)，包括两个关键模块：Information Bottleneck Graph Transformer (IBGraphFormer) 和 Information Bottleneck Heterogeneous Graph Attention Network (IB-HGAN)。IBGraphFormer用于局部模式，通过脑连接图约束的图神经网络进行全局建模并利用信息瓶颈指导的聚类提取生物标记物；IB-HGAN则用于全球多模态互动，使用异构图神经网络实现可解释的多模态融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，I2B-HGNN在诊断神经发育障碍方面表现出高精度，并能提供清晰的生物标志物识别和有效的非成像数据分析。&lt;h4&gt;结论&lt;/h4&gt;I2B-HGNN框架是解决当前机器学习模型面临的挑战的有效解决方案，它不仅能够提高诊断准确度，还能通过详细的解释帮助理解疾病特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing interpretable models for diagnosing neurodevelopmental disorders(NDDs) is highly valuable yet challenging, primarily due to the complexity ofencoding, decoding and integrating imaging and non-imaging data. Many existingmachine learning models struggle to provide comprehensive interpretability,often failing to extract meaningful biomarkers from imaging data, such asfunctional magnetic resonance imaging (fMRI), or lacking mechanisms to explainthe significance of non-imaging data. In this paper, we propose theInterpretable Information Bottleneck Heterogeneous Graph Neural Network(I2B-HGNN), a novel framework designed to learn from fine-grained localpatterns to comprehensive global multi-modal interactions. This frameworkcomprises two key modules. The first module, the Information Bottleneck GraphTransformer (IBGraphFormer) for local patterns, integrates global modeling withbrain connectomic-constrained graph neural networks to identify biomarkersthrough information bottleneck-guided pooling. The second module, theInformation Bottleneck Heterogeneous Graph Attention Network (IB-HGAN) forglobal multi-modal interactions, facilitates interpretable multi-modal fusionof imaging and non-imaging data using heterogeneous graph neural networks. Theresults of the experiments demonstrate that I2B-HGNN excels in diagnosing NDDswith high accuracy, providing interpretable biomarker identification andeffective analysis of non-imaging data.</description>
      <author>example@mail.com (Yueyang Li, Lei Chen, Wenhao Dong, Shengyu Gong, Zijian Kang, Boyang Wei, Weiming Zeng, Hongjie Yan, Lingbin Bian, Wai Ting Siok, Nizhuan Wang)</author>
      <guid isPermaLink="false">2502.20769v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Learning for Just-In-Time Software Defect Prediction in Autonomous Driving Systems</title>
      <link>http://arxiv.org/abs/2502.20806v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用多模态学习的即时软件缺陷预测（JIT-SDP）方法，以提高自动驾驶软件系统的可靠性和安全性。&lt;h4&gt;背景&lt;/h4&gt;近年来，随着自主驾驶技术的发展，可靠的软件对于确保安全和性能变得至关重要。&lt;h4&gt;目的&lt;/h4&gt;通过利用预训练变换器和组合模块处理多种数据模式来实现即时的软件缺陷预测。&lt;h4&gt;方法&lt;/h4&gt;该模型采用多模态变换器，其中包含针对文本、数值和分类等不同数据模式之间的注意机制。在组合模块中，将基于文本数据和包含分类及数值数据的表格特征的变压器模型输出进行结合以生成预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，相对于现有的深度学习和机器学习模型，在三个开源自动驾驶系统软件项目上该方法显著提高了评估指标的表现。&lt;h4&gt;结论&lt;/h4&gt;通过改善缺陷预测能力，多模态学习在提高自动驾驶软件系统的可靠性和安全性方面具有巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了利用预训练的变换器及组合模块处理包括代码特征、更改度量和上下文信息等多种数据模式的一种即时软件缺陷预测方法。该模型采用注意机制将不同形式的数据（如文本，数值，分类等）结合在一起，并在GitHub上收集三个开源自动驾驶系统项目的实验中证明其优于现有深度学习与机器学习模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, the rise of autonomous driving technologies has highlightedthe critical importance of reliable software for ensuring safety andperformance. This paper proposes a novel approach for just-in-time softwaredefect prediction (JIT-SDP) in autonomous driving software systems usingmultimodal learning. The proposed model leverages the multimodal transformersin which the pre-trained transformers and a combining module deal with themultiple data modalities of the software system datasets such as code features,change metrics, and contextual information. The key point for adaptingmultimodal learning is to utilize the attention mechanism between the differentdata modalities such as text, numerical, and categorical. In the combiningmodule, the output of a transformer model on text data and tabular featurescontaining categorical and numerical data are combined to produce thepredictions using the fully connected layers. Experiments conducted on threeopen-source autonomous driving system software projects collected from theGitHub repository (Apollo, Carla, and Donkeycar) demonstrate that the proposedapproach significantly outperforms state-of-the-art deep learning and machinelearning models regarding evaluation metrics. Our findings highlight thepotential of multimodal learning to enhance the reliability and safety ofautonomous driving software through improved defect prediction.</description>
      <author>example@mail.com (Faisal Mohammad, Duksan Ryu)</author>
      <guid isPermaLink="false">2502.20806v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>TimesBERT: A BERT-Style Foundation Model for Time Series Understanding</title>
      <link>http://arxiv.org/abs/2502.21245v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;时间序列理解包括分类、填充和异常检测等任务，现有的BERT风格架构在这些领域尚未完全解锁。&lt;h4&gt;背景&lt;/h4&gt;时间序列分析在多种场景中非常重要。虽然GPT风格模型已经作为时间序列预测的基础模型被广泛应用，但基于自然语言理解取得重大进展的BERT风格架构还未充分利用于时间序列理解。&lt;h4&gt;目的&lt;/h4&gt;设计一种名为TimesBERT的新方法，旨在学习时间序列中的通用表示形式，并解决多粒度结构的问题。&lt;h4&gt;方法&lt;/h4&gt;受到多元时间序列和多句子文档共享的多粒度结构启发，提出了TimeBERT模型。除了自然地采用掩码建模外，还提出了一种并行的任务——功能令牌预测任务来体现重要的多粒度结构。&lt;h4&gt;主要发现&lt;/h4&gt;TimesBERT在涵盖四个典型下游理解任务的数据集上取得了最先进的性能，并超越了特定任务的模型和语言预训练骨干网络，被定位为时间序列理解的基础模型。&lt;h4&gt;结论&lt;/h4&gt;TimesBERT通过利用多粒度表示形式，在多个领域的时间序列分析中表现出了卓越的能力，展示了其作为通用基础模型在时间序列理解中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;时间序列分析至关重要。除了预测任务之外，许多实际应用包括分类、填充和异常检测，这些都归结为不同的能力术语即本文所说的时间序列理解。虽然GPT风格的模型被定位为基础模型用于时间序列预测，但基于自然语言理解取得重大进展的BERT架构在时间序列理解方面尚未完全解锁。受到多元时间序列与多句子文档共享的多层次结构启发，我们设计了TimesBERT来学习包括时间模式和变量特性在内的通用时间序列表示形式。除了自然适应掩码建模之外，还提出了一种功能令牌预测任务以体现重要的多层次结构。我们的模型在涵盖各种领域的260亿个时间点上进行了预训练，并利用多层次表示，在四个典型下游理解任务中取得了最先进的性能，优于特定任务的模型和语言预训练骨干网络，被定位为时间序列理解的基础模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series analysis is crucial in diverse scenarios. Beyond forecasting,considerable real-world tasks are categorized into classification, imputation,and anomaly detection, underscoring different capabilities termed time seriesunderstanding in this paper. While GPT-style models have been positioned asfoundation models for time series forecasting, the BERT-style architecture,which has made significant advances in natural language understanding, has notbeen fully unlocked for time series understanding, possibly attributed to theundesirable dropout of essential elements of BERT. In this paper, inspired bythe shared multi-granularity structure between multivariate time series andmultisentence documents, we design TimesBERT to learn generic representationsof time series including temporal patterns and variate-centric characteristics.In addition to a natural adaptation of masked modeling, we propose a paralleltask of functional token prediction to embody vital multi-granularitystructures. Our model is pre-trained on 260 billion time points across diversedomains. Leveraging multi-granularity representations, TimesBERT achievesstate-of-the-art performance across four typical downstream understandingtasks, outperforming task-specific models and language pre-trained backbones,positioning it as a versatile foundation model for time series understanding.</description>
      <author>example@mail.com (Haoran Zhang, Yong Liu, Yunzhong Qiu, Haixuan Liu, Zhongyi Pei, Jianmin Wang, Mingsheng Long)</author>
      <guid isPermaLink="false">2502.21245v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Generating Clinically Realistic EHR Data via a Hierarchy- and Semantics-Guided Transformer</title>
      <link>http://arxiv.org/abs/2502.20719v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的框架HiSGT，该框架利用层次和语义信息生成高质量的合成电子健康记录（EHRs），提高了合成数据与真实患者记录在统计上的对齐度，并支持下游临床应用。&lt;h4&gt;背景&lt;/h4&gt;现有的生成方法通常将EHRs视为离散医学代码的序列，忽视了临床编码系统的层级组织及其描述所提供的丰富语义信息，导致合成的数据缺乏临床真实性且在实际应用中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架来克服现有生成模型的问题，提高合成EHR的质量和实用性。&lt;h4&gt;方法&lt;/h4&gt;HiSGT通过构建层次图来捕捉医学代码之间的关系，并使用图神经网络导出具有层级意识的嵌入。这些嵌入与从预训练临床语言模型中提取的语义信息相结合，增强了基于Transformer的生成器的能力。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-III和MIMIC-IV数据集上进行的广泛实验表明，HiSGT显著提高了合成EHRs与真实患者记录之间的统计对齐度，并支持慢性病分类等稳健的下游应用。&lt;h4&gt;结论&lt;/h4&gt;通过解决传统基于原始代码生成模型的限制，HiSGT为临床高保真度合成数据生成提供了重要的步骤和通用框架，促进了可解释医学编码表示以及数据分析和隐私保护方面的有价值的应用。&lt;h4&gt;翻译&lt;/h4&gt;生成逼真的合成电子健康记录（EHRs）对加速医疗研究、促进AI模型开发及增强患者隐私具有巨大潜力。然而，现有的生成方法通常将EHR视为离散医疗代码的序列化结构，这种处理方式忽略了临床编码系统内在的层级组织及其描述提供的丰富语义信息。因此，合成的数据在下游临床任务中的应用价值有限。在这篇论文中，我们提出了HiSGT框架，该框架利用层次和语义信息进行生成过程。通过这种方法，HiSGT不仅提高了数据统计上的对齐度，还支持了稳健的下游应用程序（例如慢性病分类）。这项研究代表了一个重要的步骤，即从传统基于原始代码的生成模型转向临床高保真度合成数据生成，并且提供了一种适合解释性医疗编码表示的一般框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic synthetic electronic health records (EHRs) holdstremendous promise for accelerating healthcare research, facilitating AI modeldevelopment and enhancing patient privacy. However, existing generative methodstypically treat EHRs as flat sequences of discrete medical codes. This approachoverlooks two critical aspects: the inherent hierarchical organization ofclinical coding systems and the rich semantic context provided by codedescriptions. Consequently, synthetic patient sequences often lack highclinical fidelity and have limited utility in downstream clinical tasks. Inthis paper, we propose the Hierarchy- and Semantics-Guided Transformer (HiSGT),a novel framework that leverages both hierarchical and semantic information forthe generative process. HiSGT constructs a hierarchical graph to encodeparent-child and sibling relationships among clinical codes and employs a graphneural network to derive hierarchy-aware embeddings. These are then fused withsemantic embeddings extracted from a pre-trained clinical language model (e.g.,ClinicalBERT), enabling the Transformer-based generator to more accuratelymodel the nuanced clinical patterns inherent in real EHRs. Extensiveexperiments on the MIMIC-III and MIMIC-IV datasets demonstrate that HiSGTsignificantly improves the statistical alignment of synthetic data with realpatient records, as well as supports robust downstream applications such aschronic disease classification. By addressing the limitations of conventionalraw code-based generative models, HiSGT represents a significant step towardclinically high-fidelity synthetic data generation and a general paradigmsuitable for interpretable medical code representation, offering valuableapplications in data augmentation and privacy-preserving healthcare analytics.</description>
      <author>example@mail.com (Guanglin Zhou, Sebastiano Barbieri)</author>
      <guid isPermaLink="false">2502.20719v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Global False Negatives On the Fly for Self-supervised Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.20612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了GloFND，一种用于自监督对比学习的方法，能够自动识别和排除虚假负样本。&lt;h4&gt;背景&lt;/h4&gt;在自监督对比学习中，通常通过锚图像与整个数据集中的其他样本来构建负对。这种方法可能导致具有相似语义的负对（即虚假负样本）的生成。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以解决虚假负样本问题，并提高模型训练的效果和效率。&lt;h4&gt;方法&lt;/h4&gt;GloFND是一种基于优化的方法，它在训练过程中为每个锚数据动态学习阈值来识别其虚假负样本。这种方法可以在整个数据集上全局检测虚假负样本，而不是局限于小批量内局部检测。此外，该方法的每轮迭代计算成本与数据集大小无关。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在图像和图像-文本数据上的GloFND方法是有效的。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效解决自监督对比学习中的虚假负样本问题，并且具有较低的计算复杂度。&lt;h4&gt;翻译&lt;/h4&gt;在自我监督对比性学习中，负对通常是通过锚定图片与整个数据集（除去该锚点）中选取的一个样本来构建。然而，这种策略可能导致生成语义相似的负面配对（称为“虚假否定”），从而导致其嵌入物被错误地推开。为解决此问题，我们提出了一种基于优化的方法GloFND，它在训练过程中自动学习每个锚定数据的阈值以识别其虚假否定。与先前用于发现虚假否定的方法相比，我们的方法在整个数据集中全局检测虚假否定，而不是局限于小批量内局部检测。此外，其每轮迭代计算成本保持独立于数据集大小。实验结果表明，在图像和图像-文本数据上提出的该方法是有效的。我们的实现可在https://github.com/vibalcam/GloFND获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In self-supervised contrastive learning, negative pairs are typicallyconstructed using an anchor image and a sample drawn from the entire dataset,excluding the anchor. However, this approach can result in the creation ofnegative pairs with similar semantics, referred to as "false negatives",leading to their embeddings being falsely pushed apart. To address this issue,we introduce GloFND, an optimization-based approach that automatically learnson the fly the threshold for each anchor data to identify its false negativesduring training. In contrast to previous methods for false negative discovery,our approach globally detects false negatives across the entire dataset ratherthan locally within the mini-batch. Moreover, its per-iteration computationcost remains independent of the dataset size. Experimental results on image andimage-text data demonstrate the effectiveness of the proposed method. Ourimplementation is available at https://github.com/vibalcam/GloFND .</description>
      <author>example@mail.com (Vicente Balmaseda, Bokun Wang, Ching-Long Lin, Tianbao Yang)</author>
      <guid isPermaLink="false">2502.20612v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Causality Is Key to Understand and Balance Multiple Goals in Trustworthy ML and Foundation Models</title>
      <link>http://arxiv.org/abs/2502.21123v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保机器学习系统的可信度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法集成到机器学习中，以解决公平性、隐私性、健壮性、准确性和可解释性的相互之间常常产生冲突的核心原则之间的权衡。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习系统在关键领域的应用越来越多，确保这些系统的可信度变得至关重要。然而，在实际操作中，诸如公平性、隐私性等重要目标往往被孤立处理，导致解决方案不理想。&lt;h4&gt;目的&lt;/h4&gt;论文旨在通过引入因果方法来解决信任机器学习和基础模型之间的多重竞争目标的平衡问题，并探讨如何将因果推理有效集成到这些系统中以提高其可靠性和可解释性。&lt;h4&gt;方法&lt;/h4&gt;文章回顾了现有文献中关于利用因果关系成功解决如公平性和准确性或隐私性和健壮性的冲突案例，以此证明因果框架在机器学习中的重要性和实用性。&lt;h4&gt;主要发现&lt;/h4&gt;论文强调采用因果分析可以更好地理解和解决不同目标之间的权衡问题，并提出了一些实际方法来实现这一目标。&lt;h4&gt;结论&lt;/h4&gt;尽管存在挑战和局限性，但通过使用因果框架，可以使AI系统更加负责任且伦理上更为可靠。因此，未来的研究应继续探索如何最佳地利用这些工具和技术。&lt;h4&gt;翻译&lt;/h4&gt;确保机器学习系统的可信度至关重要，尤其是在它们被广泛应用于高风险领域时。本文提倡将因果方法集成到机器学习中，以解决公平性、隐私性、健壮性、准确性和可解释性的相互之间常常产生冲突的核心原则之间的权衡。通过回顾文献中的成功案例，文章强调了因果推理在机器学习中的重要角色，并探讨如何将其有效整合进模型当中，从而提升系统的可靠性和透明度。此外，还讨论了采用这一方法所面临的挑战和机遇，指出了未来研究的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring trustworthiness in machine learning (ML) systems is crucial as theybecome increasingly embedded in high-stakes domains. This paper advocates forthe integration of causal methods into machine learning to navigate thetrade-offs among key principles of trustworthy ML, including fairness, privacy,robustness, accuracy, and explainability. While these objectives should ideallybe satisfied simultaneously, they are often addressed in isolation, leading toconflicts and suboptimal solutions. Drawing on existing applications ofcausality in ML that successfully align goals such as fairness and accuracy orprivacy and robustness, this paper argues that a causal approach is essentialfor balancing multiple competing objectives in both trustworthy ML andfoundation models. Beyond highlighting these trade-offs, we examine howcausality can be practically integrated into ML and foundation models, offeringsolutions to enhance their reliability and interpretability. Finally, wediscuss the challenges, limitations, and opportunities in adopting causalframeworks, paving the way for more accountable and ethically sound AI systems.</description>
      <author>example@mail.com (Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Muhammad Havaei, Bernhardt Schölkopf, Mario Fritz)</author>
      <guid isPermaLink="false">2502.21123v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A Non-contrast Head CT Foundation Model for Comprehensive Neuro-Trauma Triage</title>
      <link>http://arxiv.org/abs/2502.21106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种用于检测多种神经创伤的3D基础模型，该模型通过利用大规模语言模型进行自动标注，并采用多模态微调技术将神经网络预先训练结果整合进一个综合性的神经创伤检测网络中。&lt;h4&gt;背景&lt;/h4&gt;AI和医学成像的进步为急诊头部CT图像解读提供了变革性潜力，在请求量增加及放射科医生短缺的情况下，这些进步有助于缩短评估时间并提高准确性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地识别各种神经创伤的3D基础模型。&lt;h4&gt;方法&lt;/h4&gt;使用大规模语言模型自动生成全面的多标签注释，并通过预训练和多模态微调整合出血亚型分割和大脑解剖图谱到一个综合性的神经创伤检测网络中。&lt;h4&gt;主要发现&lt;/h4&gt;在与专家标注对比及与其他方法（如CT-CLIP）比较时，该模型显示出对包括出血、脑中线移位以及脑水肿等在内的多种神经创伤情况的优异分类准确率。通过加入特定于神经系统的特征，使得诊断能力得到了显著增强。&lt;h4&gt;结论&lt;/h4&gt;这项工作推动了医学影像领域基础模型的发展，并为未来AI辅助急诊放射学中的神经创伤诊断提供了基准。&lt;h4&gt;翻译&lt;/h4&gt;近期在人工智能和医疗成像领域的进展为紧急情况下的头部CT图像解读带来了变革性的潜力。这主要是为了减少评估时间并提高准确性，面对日益增长的扫描需求以及全球范围内放射科医生短缺的问题。该研究引入了一个3D基础模型用于检测各种神经创伤发现，并且具有高准确性和效率。通过使用大规模语言模型进行自动标注，生成了全面的多标签注释以识别严重情况。我们的方法包括对出血亚型分割和大脑解剖图谱预先训练神经网络，并将其整合进一个综合性的预训练神经创伤检测网络中，通过多模态微调实现集成。与专家标记对比以及与其他模型（如CT-CLIP）比较的结果表明，在主要的神经创伤发现上具有强大的分类准确性，例如出血和脑中线移位，以及其他不常见但危急的情况，例如脑水肿和动脉高密度。特定于神经系统的特征的整合显著提升了诊断能力，平均AUC为0.861（针对16种神经创伤情况）。这项工作推进了医学成像中的基础模型，并成为未来AI辅助急诊放射学中神经创伤诊断的一个基准点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in AI and medical imaging offer transformative potentialin emergency head CT interpretation for reducing assessment times and improvingaccuracy in the face of an increasing request of such scans and a globalshortage in radiologists. This study introduces a 3D foundation model fordetecting diverse neuro-trauma findings with high accuracy and efficiency.Using large language models (LLMs) for automatic labeling, we generatedcomprehensive multi-label annotations for critical conditions. Our approachinvolved pretraining neural networks for hemorrhage subtype segmentation andbrain anatomy parcellation, which were integrated into a pretrainedcomprehensive neuro-trauma detection network through multimodal fine-tuning.Performance evaluation against expert annotations and comparison with CT-CLIPdemonstrated strong triage accuracy across major neuro-trauma findings, such ashemorrhage and midline shift, as well as less frequent critical conditions suchas cerebral edema and arterial hyperdensity. The integration of neuro-specificfeatures significantly enhanced diagnostic capabilities, achieving an averageAUC of 0.861 for 16 neuro-trauma conditions. This work advances foundationmodels in medical imaging, serving as a benchmark for future AI-assistedneuro-trauma diagnostics in emergency radiology.</description>
      <author>example@mail.com (Youngjin Yoo, Bogdan Georgescu, Yanbo Zhang, Sasa Grbic, Han Liu, Gabriela D. Aldea, Thomas J. Re, Jyotipriya Das, Poikavila Ullaskrishnan, Eva Eibenberger, Andrei Chekkoury, Uttam K. Bodanapally, Savvas Nicolaou, Pina C. Sanelli, Thomas J. Schroeppel, Yvonne W. Lui, Eli Gibson)</author>
      <guid isPermaLink="false">2502.21106v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Are foundation models useful feature extractors for electroencephalography analysis?</title>
      <link>http://arxiv.org/abs/2502.21086v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了基础模型在医学时间序列分析中的应用，特别是对于脑电图（EEG）数据的处理。通过一系列任务实验，包括年龄预测、癫痫检测等，研究表明这些基础模型能够提取有意义的时间序列特征，并且超越专业设计的EEG模型而无需领域适应。&lt;h4&gt;背景&lt;/h4&gt;自然语言处理和计算机视觉领域的基础模型取得了巨大成功，但在医疗时间序列分析（特别是脑电图数据）中应用的基础模型研究较少。随着这类任务的数据集越来越有限，探索这些基础模型在医学时间序列中的适用性至关重要。&lt;h4&gt;目的&lt;/h4&gt;评估基础模型在医学时间序列数据分析中的有效性，特别关注EEG信号的处理能力，并对比这些模型与专门设计的EEG模型的表现。&lt;h4&gt;方法&lt;/h4&gt;实验中采用了一系列任务来测试这些基础模型的功能，包括年龄预测、癫痫检测和临床相关的脑电图事件分类等。并通过对比分析验证了这些模型在诊断准确性方面的优势。&lt;h4&gt;主要发现&lt;/h4&gt;1. 基础模型能够提取有意义的EEG特征；2. 即使没有领域适应，基础模型也超过了专门设计的EEG模型的表现；3. 研究表明架构选择（如上下文长度）对诊断准确度有重大影响。&lt;h4&gt;结论&lt;/h4&gt;研究表明基础模型在医学时间序列分析中具有巨大潜力。通过提供通用的时间序列理解能力，这些模型减少了对大规模特定领域数据集的需求，并成为临床实践中的宝贵工具。&lt;h4&gt;翻译&lt;/h4&gt;自然语言处理和计算机视觉领域的基础模型的成功激发了其在一般时间序列分析中的类似应用尝试。尽管这些模型对于多种任务非常有效，但在医疗领域（尤其是具有有限数据的场景）的应用仍然未被充分探索。为了应对这一问题，我们研究了基础模型在涉及脑电图（EEG）的医学时间序列分析中效果，并通过一系列实验如年龄预测、癫痫检测以及临床相关的EEG事件分类，将它们的表现与专用的EEG模型进行了对比。我们的研究表明，基础模型能够提取有意义的时间序列特征，在没有领域适应的情况下也能超越专用模型，并且能够定位任务特异性的生物标记物。此外，我们还展示了诊断准确性很大程度上受到架构选择（例如上下文长度）的影响。总的来说，这项研究揭示了具备通用时间序列理解能力的基础模型消除了对大规模特定领域数据集的依赖性，使其成为临床实践中有价值的工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The success of foundation models in natural language processing and computervision has motivated similar approaches for general time series analysis. Whilethese models are effective for a variety of tasks, their applicability inmedical domains with limited data remains largely unexplored. To address this,we investigate the effectiveness of foundation models in medical time seriesanalysis involving electroencephalography (EEG). Through extensive experimentson tasks such as age prediction, seizure detection, and the classification ofclinically relevant EEG events, we compare their diagnostic accuracy with thatof specialised EEG models. Our analysis shows that foundation models extractmeaningful EEG features, outperform specialised models even without domainadaptation, and localise task-specific biomarkers. Moreover, we demonstratethat diagnostic accuracy is substantially influenced by architectural choicessuch as context length. Overall, our study reveals that foundation models withgeneral time series understanding eliminate the dependency on largedomain-specific datasets, making them valuable tools for clinical practice.</description>
      <author>example@mail.com (Özgün Turgut, Felix S. Bott, Markus Ploner, Daniel Rueckert)</author>
      <guid isPermaLink="false">2502.21086v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.16786v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为SwimVG的分步多模态融合和适应框架，旨在解决视觉接地任务中现有的方法在跨模态对齐不足以及计算成本高的问题。&lt;h4&gt;背景&lt;/h4&gt;当前大多数用于视觉定位的方法依赖于从预训练模型中单独传输视觉或语言知识，并通过堆叠视觉-语言变压器来实现多模态融合。然而这些方法限制了视觉和语言上下文之间的充分互动并带来较高的计算成本。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了一种新的分步多模态融合和适应框架SwimVG。&lt;h4&gt;方法&lt;/h4&gt;该框架提出了逐步多模态提示（Swip）以及跨模态交互适配器（CIA），用于视觉接地任务。Swip通过逐令牌方式提高视觉和语言表示之间的对齐，而CIA在权重级别上促进跨模态融合。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在四个广泛使用的基准测试中，SwimVG表现出优异的能力并显著提高了效率。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法不仅能够有效解决现有方法的不足，而且具有参数高效的特点，并且通过逐步融合浅层到深层的跨模态特征，展示了其在视觉接地任务中的强大能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉定位旨在通过自然语言确定图像区域，这严重依赖于跨模态对齐。现有的大多数方法通过完全微调单模预训练模型来传输视觉/语言知识，并使用简单的视觉-语言变压器堆叠进行多模态融合。然而，这些方法不仅限制了视觉和语言上下文之间的充分互动，还带来了显著的计算成本。因此，为了应对这些问题，我们探索了一种分步多模态融合和适应框架，即SwimVG。具体来说，SwimVG提出了逐步多模态提示（Swip）以及跨模态交互适配器（CIA），用于视觉接地任务，替代冗余的变压器堆叠进行多模态融合。Swip能够以逐令牌的方式分步提高视觉和语言表示之间的对齐。此外，权重级别的CIA通过跨模态互动进一步促进多模态融合。Swip和CIA都是参数高效的模式，并逐步将浅层到深层的跨模态特征融合在一起。实验结果在四个常用的基准测试上表明，SwimVG在效率方面具有显著的优势。我们的代码可以在https://github.com/liuting20/SwimVG获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/liuting20/swimvg&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to ground an image region through natural language,which heavily relies on cross-modal alignment. Most existing methods transfervisual/linguistic knowledge separately by fully fine-tuning uni-modalpre-trained models, followed by a simple stack of visual-language transformersfor multimodal fusion. However, these approaches not only limit adequateinteraction between visual and linguistic contexts, but also incur significantcomputational costs. Therefore, to address these issues, we explore a step-wisemultimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVGproposes step-wise multimodal prompts (Swip) and cross-modal interactiveadapters (CIA) for visual grounding, replacing the cumbersome transformerstacks for multimodal fusion. Swip can improve {the} alignment between thevision and language representations step by step, in a token-level fusionmanner. In addition, weight-level CIA further promotes multimodal fusion bycross-modal interaction. Swip and CIA are both parameter-efficient paradigms,and they fuse the cross-modal features from shallow to deep layers gradually.Experimental results on four widely-used benchmarks demonstrate that SwimVGachieves remarkable abilities and considerable benefits in terms of efficiency.Our code is available at https://github.com/liuting20/SwimVG.</description>
      <author>example@mail.com (Liangtao Shi, Ting Liu, Xiantao Hu, Yue Hu, Quanjun Yin, Richang Hong)</author>
      <guid isPermaLink="false">2502.16786v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Accurate 3D Grapevine Structure Extraction from High-Resolution Point Clouds</title>
      <link>http://arxiv.org/abs/2502.20417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种针对葡萄藤3D建模的Smart-Tree算法改进版，采用基于图的方法解决传统骨架化算法在复杂结构上的挑战。&lt;h4&gt;背景&lt;/h4&gt;精确的葡萄藤3D建模对精准农业至关重要，特别是对于信息丰富的修剪决策和自动化管理技术。然而，葡萄藤复杂的结构给传统的骨架化算法带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;旨在开发一种适用于葡萄藤独特特性的三维建模方法，改善传统Smart-Tree算法在处理复杂结构上的效果。&lt;h4&gt;方法&lt;/h4&gt;提出了基于图的方法来区分骨架化过程中的个体枝条，并通过注释的现实世界点云数据进行验证。&lt;h4&gt;主要发现&lt;/h4&gt;新方法相比原始的Smart-Tree算法，在F1分数上提高了15.8%，表明改进的有效性。&lt;h4&gt;结论&lt;/h4&gt;该研究推进了葡萄藤三维建模技术的发展，有可能通过更精确和自动化的农业实践提高葡萄生产的可持续性和盈利能力。&lt;h4&gt;翻译&lt;/h4&gt;准确地对葡萄藤进行3D建模对于精准种植至关重要，特别是在信息丰富的修剪决策以及自动化管理技术方面。然而，由于葡萄藤的复杂结构，传统的骨架化算法面临着重大挑战。本文提出了一种针对Smart-Tree算法改进的方法来应对葡萄藤的独特特点，并使用基于图的方式来解决骨架化的歧义问题。该方法能够区分出每个枝条的骨架结构，这对于精确分析和管理至关重要。我们通过使用注释过的现实世界中葡萄藤点云数据验证了我们的方法的有效性，在F1评分上较原始Smart-Tree算法提高了15.8%。这项研究为3D葡萄藤建模技术的发展做出了贡献，并有可能通过更加准确且自动化的种植实践提高葡萄生产的可持续性和盈利能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate 3D modelling of grapevines is crucial for precision viticulture,particularly for informed pruning decisions and automated managementtechniques. However, the intricate structure of grapevines poses significantchallenges for traditional skeletonization algorithms. This paper presents anadaptation of the Smart-Tree algorithm for 3D grapevine modelling, addressingthe unique characteristics of grapevine structures. We introduce a graph-basedmethod for disambiguating skeletonization. Our method delineates individualcane skeletons, which are crucial for precise analysis and management. Wevalidate our approach using annotated real-world grapevine point clouds,demonstrating improvement of 15.8% in the F1 score compared to the originalSmart-Tree algorithm. This research contributes to advancing 3D grapevinemodelling techniques, potentially enhancing both the sustainability andprofitability of grape production through more precise and automatedviticulture practices</description>
      <author>example@mail.com (Harry Dobbs, Casey Peat, Oliver Batchelor, James Atlas, Richard Green)</author>
      <guid isPermaLink="false">2502.20417v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Can We Simplify Slide-level Fine-tuning of Pathology Foundation Models?</title>
      <link>http://arxiv.org/abs/2502.20823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的策略SiMLP，通过简单的非线性映射结合均值池化和多层感知机来适应基于切片级别的任务，超越了传统的MIL方法。&lt;h4&gt;背景&lt;/h4&gt;计算病理学中基础模型的出现已经改变了组织病理图像分析的方法，其中全滑动影像（WSI）诊断是核心应用。以往主要采用弱监督微调通过多重实例学习（MIL）来适应基础模型以处理WSIs。&lt;h4&gt;目的&lt;/h4&gt;展示SiMLP策略在多种下游任务中的优越性，并挑战传统的基于MIL的微调范式。&lt;h4&gt;方法&lt;/h4&gt;提出了一种简单非线性映射策略，称为SiMLP，该策略结合均值池化和多层感知机来将基础模型从补丁级别适应到切片级别任务。&lt;h4&gt;主要发现&lt;/h4&gt;1. SiMLP在大规模癌症分类任务中超越了流行MIL方法3.52%，显示强大的少样本分类能力；2. 在肺部肿瘤亚型分类方面，SiMLP表现出显著的鲁棒性和可转移性。3. SiMLP可以不需要复杂的基于MIL的学习过程来适应WSI分析。&lt;h4&gt;结论&lt;/h4&gt;研究结果挑战了传统的基于MIL的微调范式，并表明仅通过任务无关表示策略即可有效调整基础模型以进行WSI分析，为未来数字病理学研究提供了新的视角和方法论。&lt;h4&gt;翻译&lt;/h4&gt;计算病理学中的基础模型出现已经改变了组织病理图像分析的方法，其中全滑动影像（WSI）诊断是核心应用。以往主要采用弱监督微调通过多重实例学习（MIL）来适应基础模型以处理WSIs。然而，在这项工作中我们提出了一种关键的实验发现：一种简单的非线性映射策略结合均值池化和多层感知机，称为SiMLP，可以有效将基于补丁级别的基础模型适配到切片级别任务而不需要复杂MIL基的学习方法。通过广泛的跨多种下游任务实验，我们展示了SiMLP与最新技术相比的优越性能，在大规模癌症分类任务中超越了流行MIL方法3.52%。此外，SiMLP在少样本分类中表现出强大的学习能力，并且仍然与其他预训练于数十万张切片上的切片级别基础模型竞争。最后，SiMLP在肺癌亚型分类方面表现出显著的鲁棒性和可转移性。总的来说，我们的发现挑战了传统的基于MIL的微调范式，表明仅通过任务无关表示策略就可以有效地将基础模型适配到WSI分析中。这些见解为未来数字病理学研究提供了一个独特且具有意义的新视角，并为此类研究铺平了更高效和广泛适用的方法论的道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The emergence of foundation models in computational pathology has transformedhistopathological image analysis, with whole slide imaging (WSI) diagnosisbeing a core application. Traditionally, weakly supervised fine-tuning viamultiple instance learning (MIL) has been the primary method for adaptingfoundation models to WSIs. However, in this work we present a key experimentalfinding: a simple nonlinear mapping strategy combining mean pooling and amultilayer perceptron, called SiMLP, can effectively adapt patch-levelfoundation models to slide-level tasks without complex MIL-based learning.Through extensive experiments across diverse downstream tasks, we demonstratethe superior performance of SiMLP with state-of-the-art methods. For instance,on a large-scale pan-cancer classification task, SiMLP surpasses popularMIL-based methods by 3.52%. Furthermore, SiMLP shows strong learning ability infew-shot classification and remaining highly competitive with slide-levelfoundation models pretrained on tens of thousands of slides. Finally, SiMLPexhibits remarkable robustness and transferability in lung cancer subtyping.Overall, our findings challenge the conventional MIL-based fine-tuningparadigm, demonstrating that a task-agnostic representation strategy alone caneffectively adapt foundation models to WSI analysis. These insights offer aunique and meaningful perspective for future research in digital pathology,paving the way for more efficient and broadly applicable methodologies.</description>
      <author>example@mail.com (Jiawen Li, Jiali Hu, Qiehe Sun, Renao Yan, Minxi Ouyang, Tian Guan, Anjia Han, Chao He, Yonghong He)</author>
      <guid isPermaLink="false">2502.20823v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SemiSAM+: Rethinking Semi-Supervised Medical Image Segmentation in the Era of Foundation Models</title>
      <link>http://arxiv.org/abs/2502.20749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于提示的基础模型驱动的半监督学习框架SemiSAM+，用于提高医疗图像分割任务中有限标注数据的学习效率。&lt;h4&gt;背景&lt;/h4&gt;深度学习在医学影像分割中的应用通常需要大量的标记数据进行训练，这在临床环境中由于注释成本高而难以实施。半监督学习（SSL）作为一种依赖较少专家标注的方法逐渐受到关注。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于提示的基础模型驱动的半监督框架SemiSAM+，以期通过有限数量的标签实现高效的医学影像分割任务。&lt;h4&gt;方法&lt;/h4&gt;该框架由一个或多个可提示的基础模型和一个特定于任务的学习型模型组成。在给定的新分割任务中，训练过程包括学习型模型与基础模型之间的协作，在此基础上学习型模型生成位置提示并接收来自基础模型的伪标签监督。&lt;h4&gt;主要发现&lt;/h4&gt;SemiSAM+框架在两个公共数据集及一家医院内部临床数据集中展示了显著性能提升，特别是在标注数量极为有限的情况下效果尤为突出。该框架还展现了强大的适应性作为即插即用策略可以轻松应用于不同类型的特定任务和通用模型中。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一种新的半监督学习方法SemiSAM+，它通过结合基础模型的泛化能力和学习型模型的专业能力，在医学图像分割任务中实现了显著性能改进。这种方法为解决有限标签数据集下的高效训练提供了可能路径，并展示了良好的扩展性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based medical image segmentation typically requires largeamount of labeled data for training, making it less applicable in clinicalsettings due to high annotation cost. Semi-supervised learning (SSL) hasemerged as an appealing strategy due to its less dependence on acquiringabundant annotations from experts compared to fully supervised methods. Beyondexisting model-centric advancements of SSL by designing novel regularizationstrategies, we anticipate a paradigmatic shift due to the emergence ofpromptable segmentation foundation models with universal segmentationcapabilities using positional prompts represented by Segment Anything Model(SAM). In this paper, we present SemiSAM+, a foundation model-driven SSLframework to efficiently learn from limited labeled data for medical imagesegmentation. SemiSAM+ consists of one or multiple promptable foundation modelsas generalist models, and a trainable task-specific segmentation model asspecialist model. For a given new segmentation task, the training is based onthe specialist-generalist collaborative learning procedure, where the trainablespecialist model delivers positional prompts to interact with the frozengeneralist models to acquire pseudo-labels, and then the generalist modeloutput provides the specialist model with informative and efficient supervisionwhich benefits the automatic segmentation and prompt generation in turn.Extensive experiments on two public datasets and one in-house clinical datasetdemonstrate that SemiSAM+ achieves significant performance improvement,especially under extremely limited annotation scenarios, and shows strongefficiency as a plug-and-play strategy that can be easily adapted to differentspecialist and generalist models.</description>
      <author>example@mail.com (Yichi Zhang, Bohao Lv, Le Xue, Wenbo Zhang, Yuchen Liu, Yu Fu, Yuan Cheng, Yuan Qi)</author>
      <guid isPermaLink="false">2502.20749v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>STPro: Spatial and Temporal Progressive Learning for Weakly Supervised Spatio-Temporal Grounding</title>
      <link>http://arxiv.org/abs/2502.20678v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR'25 Conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了弱监督时空视频定位任务，提出了一种新的学习框架STPro。&lt;h4&gt;背景&lt;/h4&gt;现有的视觉语言基础模型虽然具备零样本推理能力，但在执行弱监督时空视频定位任务时缺乏必要的时空定位能力。&lt;h4&gt;目的&lt;/h4&gt;为了解决现有方法的不足，设计了一个能够进行时空预测的学习系统。&lt;h4&gt;方法&lt;/h4&gt;引入了Tubelet Referral Grounding (TRG)，并在此基础上提出了一个新颖的进步学习框架STPro。该框架包含两个关键模块：Sub-Action Temporal Curriculum Learning (SA-TCL) 和 Congestion-Guided Spatial Curriculum Learning (CG-SCL)。&lt;h4&gt;主要发现&lt;/h4&gt;通过在三个基准数据集上的实验，证明了所提出的STPro方法的有效性，并且在VidSTG-Declarative和HCSTVG-v1两个数据集中分别取得了比之前最好的结果高出1.0% 和 3.0%的成绩。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为弱监督时空视频定位任务提供了一个新的解决方案，提高了该领域的技术水平。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们研究了使用文本查询而没有任何边界框监督的弱监督时空视频定位（WSTVG）任务。受到近期视觉-语言基础模型进展的启发，我们探索了这些模型在WSTVG中的实用性，利用它们的零样本接地能力。然而，简单地调整这些模型无法满足必要的时空定点功能。为弥补这一差距，我们提出了Tubelet Referral Grounding（TRG），该方法将文本查询与管状体连接起来以实现时空预测。尽管有潜力，但TRG在组合动作理解和密集场景方面仍然存在挑战。为了克服这些问题，我们提出了一种新的渐进式学习框架STPro，具有两个关键模块：Sub-Action Temporal Curriculum Learning（SA-TCL）和Congestion-Guided Spatial Curriculum Learning（CG-SCL）。在三个基准数据集上进行实验后，我们的方法实现了最先进的结果，在VidSTG-Declarative和HCSTVG-v1中分别提高了1.0% 和 3.0%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work we study Weakly Supervised Spatio-Temporal Video Grounding(WSTVG), a challenging task of localizing subjects spatio-temporally in videosusing only textual queries and no bounding box supervision. Inspired by recentadvances in vision-language foundation models, we investigate their utility forWSTVG, leveraging their zero-shot grounding capabilities. However, we find thata simple adaptation lacks essential spatio-temporal grounding abilities. Tobridge this gap, we introduce Tubelet Referral Grounding (TRG), which connectstextual queries to tubelets to enable spatio-temporal predictions. Despite itspromise, TRG struggles with compositional action understanding and dense scenescenarios. To address these limitations, we propose STPro, a novel progressivelearning framework with two key modules: (1) Sub-Action Temporal CurriculumLearning (SA-TCL), which incrementally builds compositional actionunderstanding, and (2) Congestion-Guided Spatial Curriculum Learning (CG-SCL),which adapts the model to complex scenes by spatially increasing taskdifficulty. STPro achieves state-of-the-art results on three benchmarkdatasets, with improvements of 1.0% on VidSTG-Declarative and 3.0% onHCSTVG-v1.</description>
      <author>example@mail.com (Aaryan Garg, Akash Kumar, Yogesh S Rawat)</author>
      <guid isPermaLink="false">2502.20678v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>SciceVPR: Stable Cross-Image Correlation Enhanced Model for Visual Place Recognition</title>
      <link>http://arxiv.org/abs/2502.20676v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为SciceVPR的稳定跨图相关增强模型，用于视觉位置识别（VPR），旨在生成具有区分性和稳定性全局描述符。&lt;h4&gt;背景&lt;/h4&gt;当前最先进的VPR模型依赖于强大的基础模型DINOv2提取全局特征，并且要么通过探索跨图像相关性来提高性能，要么采用耗时的两阶段重排名策略。但现有工作仅利用了DINOv2的最终输出结果，导致检索效果不稳定。&lt;h4&gt;目的&lt;/h4&gt;提出一种改进方法以克服现有技术中描述符不稳定性问题，并充分利用DINOv2模型提供的特征表示能力，隐式编码有价值的上下文知识。&lt;h4&gt;方法&lt;/h4&gt;SciceVPR通过一个多层特征融合模块捕捉任务相关通道和空间信息；同时利用图像之间不变的相关性作为有价值的知识融入增强自编码器，从而获得对领域转换具有鲁棒性的全局特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，SciceVPR-B变体在多个不同领域条件的数据集上优于现有的单输入一步方法。而SciceVPR-L版本性能与最先进的两步模型相当，在挑战性东京24/7数据集中召回率@1高出3%以上。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够生成稳定的全局描述符，提高视觉位置识别的准确性和鲁棒性，并在多个数据集上优于现有方法。&lt;h4&gt;翻译&lt;/h4&gt;Visual Place Recognition（VPR）是机器人和自主系统的一个主要挑战，目标是仅基于图像的视觉特征预测其位置。最先进的模型使用强大的基础模型DINOv2作为骨干网络来提取全局描述符。这些模型要么通过探索跨图相关性提高性能，要么采用耗时的两阶段重排名策略以达到更好的效果。然而，现有的工作仅仅利用了DINOv2的最终输出，并且当前的跨图相关会导致检索结果不稳定。为了生成具有区分性和稳定性的全局描述符，本文提出了名为SciceVPR的增强模型。该模型探索了DINOv2在提供有用特征表示方面的全部潜力，隐式地编码有价值的上下文知识。具体而言，SciceVPR首先利用一个多层特征融合模块捕捉任务相关的通道和空间信息；其次考虑图像批次内的不变相关性作为有价值的知识融入提出的自增强编解码器中。这样，SciceVPR可以获取相对领域转换（例如光照、天气和视角变化）的全局特征具有鲁棒性的特性。实验结果表明，基本版本SciceVPR-B在多种不同条件的数据集上优于现有的单输入一步方法。大型变体SciceVPR-L与最先进的两步模型相当，在挑战性东京24/7数据集中召回率@1高出3%以上。我们的代码将发布于https://github.com/shuimushan/SciceVPR。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual Place Recognition (VPR) is a major challenge for robotics andautonomous systems, with the goal of predicting the location of an image basedsolely on its visual features. State-of-the-art (SOTA) models extract globaldescriptors using the powerful foundation model DINOv2 as backbone. Thesemodels either explore the cross-image correlation or propose a time-consumingtwo-stage re-ranking strategy to achieve better performance. However, existingworks only utilize the final output of DINOv2, and the current cross-imagecorrelation causes unstable retrieval results. To produce both discriminativeand constant global descriptors, this paper proposes stable cross-imagecorrelation enhanced model for VPR called SciceVPR. This model explores thefull potential of DINOv2 in providing useful feature representations thatimplicitly encode valuable contextual knowledge. Specifically, SciceVPR firstuses a multi-layer feature fusion module to capture increasingly detailedtask-relevant channel and spatial information from the multi-layer output ofDINOv2. Secondly, SciceVPR considers the invariant correlation between imageswithin a batch as valuable knowledge to be distilled into the proposedself-enhanced encoder. In this way, SciceVPR can acquire fairly robust globalfeatures regardless of domain shifts (e.g., changes in illumination, weatherand viewpoint between pictures taken in the same place). Experimental resultsdemonstrate that the base variant, SciceVPR-B, outperforms SOTA one-stagemethods with single input on multiple datasets with varying domain conditions.The large variant, SciceVPR-L, performs on par with SOTA two-stage models,scoring over 3% higher in Recall@1 compared to existing models on thechallenging Tokyo24/7 dataset. Our code will be released athttps://github.com/shuimushan/SciceVPR.</description>
      <author>example@mail.com (Shanshan Wan, Yingmei Wei, Lai Kang, Tianrui Shen, Haixuan Wang, Yee-Hong Yang)</author>
      <guid isPermaLink="false">2502.20676v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CoCa-CXR: Contrastive Captioners Learn Strong Temporal Structures for Chest X-Ray Vision-Language Understanding</title>
      <link>http://arxiv.org/abs/2502.20509v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;视觉-语言模型在医学图像分析中发挥了重要作用，通过从图像和报告中学到丰富的语义信息来提高图像理解。该研究针对胸部X光片（CXR）的报告处理流程提出了一种新的方法。&lt;h4&gt;背景&lt;/h4&gt;现有努力主要集中在图像与文本表示的一致性上以增强图像理解，但针对CXR报告中常见的时间参照进行图像对之间的语义差异对齐的问题则较少探索。&lt;h4&gt;目的&lt;/h4&gt;提出了两个组件来解决这一问题：一个用于处理CXR报告的流程和CoCa-CXR模型，该模型能够描述图像及其时间进程，并识别配对CXR图像中的局部差异。&lt;h4&gt;方法&lt;/h4&gt;(1) 提出了一种基于大规模语言模型（LLM）的CXR报告处理流水线来提取时态结构。(2) 开发了名为CoCa-CXR的对比性标题生成器，以学习描述图像及其时间变化的方法。该模型包含了一个新颖的区域交叉注意力模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CoCa-CXR在进展分析和报告生成方面优于先前方法，在MS-CXR-T进展分类上的平均测试准确率达到了65.0%，超过之前的SOTA模型BioViL-T 4.8%。同时在MIMIC-CXR上取得了24.2%的RadGraph F1，与Med-Gemini基础模型相当。&lt;h4&gt;结论&lt;/h4&gt;CoCa-CXR通过新的区域交叉注意力模块和创新性的处理流程，在医学图像的时间进程分析中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型由于能够从图像和报告中学到丰富的语义信息，在医学影像分析领域具有重要作用。以往的研究重点在于更好地对齐图像和文本表示以增强图像理解。然而，尽管胸部X射线（CXR）报告中通常会明确参考之前的影像，但如何将进展描述与成对影像之间的语义差异进行有效对齐仍有待进一步研究。为此，我们提出了两种解决方法：一种用于处理CXR报告的流水线和一个对比性标题生成器CoCa-CXR，用于学习描绘图像及其时间变化的方法。实验表明，该模型在进展分析及报告生成上均优于现有技术，并且在MS-CXR-T进步分类任务中平均测试准确率达到了65.0%，超过之前的SOTA模型BioViL-T 4.8%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models have proven to be of great benefit for medical imageanalysis since they learn rich semantics from both images and reports. Priorefforts have focused on better alignment of image and text representations toenhance image understanding. However, though explicit reference to a priorimage is common in Chest X-Ray (CXR) reports, aligning progression descriptionswith the semantics differences in image pairs remains under-explored. In thiswork, we propose two components to address this issue. (1) A CXR reportprocessing pipeline to extract temporal structure. It processes reports with alarge language model (LLM) to separate the description and comparison contexts,and extracts fine-grained annotations from reports. (2) A contrastive captionermodel for CXR, namely CoCa-CXR, to learn how to both describe images and theirtemporal progressions. CoCa-CXR incorporates a novel regional cross-attentionmodule to identify local differences between paired CXR images. Extensiveexperiments show the superiority of CoCa-CXR on both progression analysis andreport generation compared to previous methods. Notably, on MS-CXR-Tprogression classification, CoCa-CXR obtains 65.0% average testing accuracy onfive pulmonary conditions, outperforming the previous state-of-the-art (SOTA)model BioViL-T by 4.8%. It also achieves a RadGraph F1 of 24.2% on MIMIC-CXR,which is comparable to the Med-Gemini foundation model.</description>
      <author>example@mail.com (Yixiong Chen, Shawn Xu, Andrew Sellergren, Yossi Matias, Avinatan Hassidim, Shravya Shetty, Daniel Golden, Alan Yuille, Lin Yang)</author>
      <guid isPermaLink="false">2502.20509v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Vision-Language Navigation (VLN)在机器人导航领域扮演着重要角色，尤其是在具身人工智能中。本文提出了OpenFly平台，旨在解决户外空中VLN数据收集困难的问题，并构建了大规模的数据集和模型。&lt;h4&gt;背景&lt;/h4&gt;室内VLN已经被广泛研究，但室外空中VLN由于涉及的视野广阔、数据采集难度大而较少被探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的平台OpenFly来促进户外空中的Vision-Language Navigation（VLN）的发展，包括工具链开发和大规模数据集构建。&lt;h4&gt;方法&lt;/h4&gt;{'开发自动化工具链': '自动收集点云数据、进行场景语义分割、生成飞行轨迹以及创建指令', '构建大型数据集': '使用多种渲染引擎和技术生成包含100k轨迹的大规模数据集，涵盖多样化的高度和长度，并通过3D Gaussian Splatting技术增强数据的真实感。', '提出OpenFly-Agent模型': '基于关键帧的VLN模型，可以接收语言指令、当前观察结果以及历史上的关键帧作为输入，并直接输出飞行动作'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的分析和实验展示了OpenFly平台及其核心算法OpenFly-Agent的优越性能。&lt;h4&gt;结论&lt;/h4&gt;开源了工具链、数据集及代码，旨在促进户外空中VLN的研究和发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含详细内容，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Upon reflection, the final version of this work does not meet the  author's personal standards for thoroughness and clarity. As a result, the  authors have chosen to withdraw the paper to prevent the dissemination of  work that may not fully reflect the level of quality they strive to maintain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究通过合并三个来源创建了一个大型的COVID严重程度数据集，探讨了迁移学习在使用ImageNet和CXR预训练模型以及视觉变换器(ViTs)进行病情预测方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;新冠肺炎大流行导致医疗资源紧张，并引发了关于机器学习如何减轻医生负担并有助于诊断的讨论。胸部X光片（CXRs）用于诊断COVID-19，但很少有研究从CXRs预测患者的病情严重程度。&lt;h4&gt;目的&lt;/h4&gt;探讨迁移学习在基于图像的数据集中的表现以及其对新冠肺炎患者病情预测的贡献。&lt;h4&gt;方法&lt;/h4&gt;本研究使用了ImageNet和CXR预训练模型及视觉变换器(ViTs)，并在此基础上进行了严重程度回归与分类任务的研究。&lt;h4&gt;主要发现&lt;/h4&gt;预训练DenseNet161模型在三种严重程度预测问题上表现最佳，总体准确率为80%，轻度、中度和重度病例的分别准确率分别为77.3%、83.9%和70%。视觉变换器(ViT)在回归任务中的均方绝对误差为0.5676。&lt;h4&gt;结论&lt;/h4&gt;迁移学习方法，特别是预训练模型，可以有效地用于基于图像的数据集来预测新冠肺炎的病情严重程度。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了英文原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stwhitfield/covid-severity&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju)</author>
      <guid isPermaLink="false">2502.16622v3</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning through Enhanced Sufficient Representation: Enriching Source Domain Knowledge with Target Data</title>
      <link>http://arxiv.org/abs/2502.20414v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  44 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种新的迁移学习方法——增强充分表示的迁移学习（TESR），旨在解决传统迁移学习方法因模型假设过于严格和源域与目标域相似度要求高而导致的问题。&lt;h4&gt;背景&lt;/h4&gt;随着数据可用性的限制，迁移学习成为了解决这些问题的重要方法。它通过从已建立良好的源领域向不熟悉的靶领域转移知识来实现这一目标。&lt;h4&gt;目的&lt;/h4&gt;介绍一种新的迁移学习方法TESR，旨在克服传统方法的局限性，提高模型在不同任务中的适应性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;首先估计出一个充分和不变的表现形式，然后通过来自靶数据的独立成分增强该表现形式，使其成为针对特定目标领域的充足表示且易于适应其特性。此方法不依赖于跨不同任务的相似模型结构假设。&lt;h4&gt;主要发现&lt;/h4&gt;TESR能够在有限样本环境下有效工作，并在模拟研究和实际应用中验证了它的性能。&lt;h4&gt;结论&lt;/h4&gt;论文展示了TESR作为迁移学习的一种灵活有效的策略，适用于广泛的监督学习问题。&lt;h4&gt;翻译&lt;/h4&gt;迁移学习是一种解决数据可用性限制的重要方法，通过从已建立良好的源领域向不熟悉的靶领域转移知识来实现。然而，传统的方法往往因为过于严格的模型假设和需要高度相似的域模型而面临挑战。本文提出了一种新的方法——增强充分表示的迁移学习（TESR）。该方法首先估计出一个充分不变的表现形式，并通过来自靶数据的独立成分进一步增强它，以适应特定目标领域的特性并确保其充足性。主要优点是不依赖于假设跨任务相似模型结构的存在；例如源域可以使用回归模型而靶域的任务可能是分类。这种灵活性使得TESR能够应用于广泛的监督学习问题中。论文通过理论属性探索和模拟研究以及实际数据应用验证了TESR的性能，证明它在有限样本设置下的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transfer learning is an important approach for addressing the challengesposed by limited data availability in various applications. It accomplishesthis by transferring knowledge from well-established source domains to a lessfamiliar target domain. However, traditional transfer learning methods oftenface difficulties due to rigid model assumptions and the need for a high degreeof similarity between source and target domain models. In this paper, weintroduce a novel method for transfer learning called Transfer learning throughEnhanced Sufficient Representation (TESR). Our approach begins by estimating asufficient and invariant representation from the source domains. Thisrepresentation is then enhanced with an independent component derived from thetarget data, ensuring that it is sufficient for the target domain and adaptableto its specific characteristics. A notable advantage of TESR is that it doesnot rely on assuming similar model structures across different tasks. Forexample, the source domain models can be regression models, while the targetdomain task can be classification. This flexibility makes TESR applicable to awide range of supervised learning problems. We explore the theoreticalproperties of TESR and validate its performance through simulation studies andreal-world data applications, demonstrating its effectiveness in finite samplesettings.</description>
      <author>example@mail.com (Yeheng Ge, Xueyu Zhou, Jian Huang)</author>
      <guid isPermaLink="false">2502.20414v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CurviTrack: Curvilinear Trajectory Tracking for High-speed Chase of a USV</title>
      <link>http://arxiv.org/abs/2502.21303v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于拖曳感知模型与MPC相结合的方法，用于解决海洋环境中异构机器人团队由于需要让自主飞行器着陆充电而导致的时间和能量损失问题。&lt;h4&gt;背景&lt;/h4&gt;在海事应用中使用异构机器人团队会导致时间及能源的浪费，特别是在自主飞行器需要降落以重新充电时。这不仅影响任务效率，还限制了海洋车辆执行复杂机动的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有技术中的预测误差大、预测准确性低以及跟踪性能不足的问题，提高在动态环境下的着陆成功率。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新的拖曳感知模型，并将其与MPC（模型预测控制）结合使用，以实现高速曲线轨迹下的追踪和降落，无需通信即可完成任务。&lt;h4&gt;主要发现&lt;/h4&gt;相比现有技术，该方法降低了40%的预测误差，提高了预测准确性的三倍，并且在跟踪性能上提升了30%，成功着陆率提高到了原来的四倍，尤其是在执行剧烈转弯等传统海上任务难以应对的情况下表现尤为突出。&lt;h4&gt;结论&lt;/h4&gt;通过两个不同实际场景中大小不同的海洋船只测试验证了该方法的有效性，并进一步使用模拟中的统计分析来展示其鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;异构机器人团队在海洋环境中使用时，当海洋车辆必须停止执行任务以让自主飞行器降落进行充电时会遭受时间和能源的惩罚。本文提出了一种解决方案，利用一种新的拖曳感知模型和MPC（模型预测控制）相结合的方法来跟踪并在高速曲线轨迹中实现不依赖通信的着陆。这种方法相比于最先进的技术可以降低40%的预测误差，并提供了预测准确性的三倍提高。因此，在进行剧烈转弯等常规海上任务难以处理的情况下，这导致了30%的追踪性能改进和40%更高的在移动USV上成功的着陆概率。我们在两种不同的现实场景中测试了我们的方法，使用不同大小的海洋船只，并通过模拟中的统计分析进一步证实我们方法的稳健性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3546079&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Heterogeneous robot teams used in marine environments incur time-and-energypenalties when the marine vehicle has to halt the mission to allow theautonomous aerial vehicle to land for recharging. In this paper, we present asolution for this problem using a novel drag-aware model formulation which iscoupled with MPC, and therefore, enables tracking and landing during high-speedcurvilinear trajectories of an USV without any communication. Compared to thestate-of-the-art, our approach yields 40% decrease in prediction errors, andprovides a 3-fold increase in certainty of predictions. Consequently, thisleads to a 30% improvement in tracking performance and 40% higher success inlanding on a moving USV even during aggressive turns that are unfeasible forconventional marine missions. We test our approach in two different real-worldscenarios with marine vessels of two different sizes and further solidify ourresults through statistical analysis in simulation to demonstrate therobustness of our method.</description>
      <author>example@mail.com (Parakh M. Gupta, Ondřej Procházka, Tiago Nascimento, Martin Saska)</author>
      <guid isPermaLink="false">2502.21303v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Back to the Future Cyclopean Stereo: a human perception approach unifying deep and geometric constraints</title>
      <link>http://arxiv.org/abs/2502.21280v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种结合几何模型和学习到的立体视觉特征的方法，用于改善3D表面建模。&lt;h4&gt;背景&lt;/h4&gt;传统的立体视觉方法在处理深度不连续性和遮挡时存在挑战。仅基于数据驱动的方法难以捕捉关键的视觉信息。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用几何形状描述优势，并通过单目先验模型来增强遮挡和缺乏纹理区域建模的新系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用类独眼的模型提供分析性3D表面模型，这些模型包含了深度不连续性和遮挡；2. 结合学习到的立体视觉特征；3. 调用单目先验表面模型填补遮挡或缺乏纹理的区域。&lt;h4&gt;主要发现&lt;/h4&gt;该方法结果与现有的数据驱动方法相当，但在视觉质量上有显著提升，证明了三维几何模型的重要性。&lt;h4&gt;结论&lt;/h4&gt;理解并建模三维形状属性对于计算机视觉研究至关重要，并且这种改进可以在虚拟现实和机器人技术中应用，以改善用户体验或减少错误。&lt;h4&gt;翻译&lt;/h4&gt;我们在立体视觉方面进行了创新，通过提供由独眼模型视角下的分析性3D表面模型来明确处理深度不连续性和遮挡问题。结合几何基础与学习到的立体特征使我们的系统能够从两种方法的优势中获益。此外，在数据匹配不足的情况下使用单目先验模型填补遮挡或缺乏纹理区域。我们的结果已达到现有纯数据驱动方法同等水平，但在视觉质量上更胜一筹，突显了3D几何模型捕捉关键视觉信息的重要性。这样的定性改进可能在虚拟现实中找到应用价值，以改善人类体验，并且在机器人技术中减少关键错误方面同样重要。本研究旨在证明理解并建模三维表面的几何属性对计算机视觉研究有益。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We innovate in stereo vision by explicitly providing analytical 3D surfacemodels as viewed by a cyclopean eye model that incorporate depthdiscontinuities and occlusions. This geometrical foundation combined withlearned stereo features allows our system to benefit from the strengths of bothapproaches. We also invoke a prior monocular model of surfaces to fill inocclusion regions or texture-less regions where data matching is notsufficient. Our results already are on par with the state-of-the-art purelydata-driven methods and are of much better visual quality, emphasizing theimportance of the 3D geometrical model to capture critical visual information.Such qualitative improvements may find applicability in virtual reality, for abetter human experience, as well as in robotics, for reducing critical errors.Our approach aims to demonstrate that understanding and modeling geometricalproperties of 3D surfaces is beneficial to computer vision research.</description>
      <author>example@mail.com (Sherlon Almeida da Silva, Davi Geiger, Luiz Velho, Moacir Antonelli Ponti)</author>
      <guid isPermaLink="false">2502.21280v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>RoboBrain: A Unified Brain Model for Robotic Manipulation from Abstract to Concrete</title>
      <link>http://arxiv.org/abs/2502.21257v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;最近，多模态大型语言模型（MLLM）在各种多模态环境中展示了显著的能力。然而，在机器人场景中的应用，特别是长时程操作任务中表现出重大限制。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态大型语言模型（MLLM）在处理复杂的机器人操控任务时存在明显的不足，特别是在规划能力、可操作性感知和轨迹预测三个方面有缺陷。&lt;h4&gt;目的&lt;/h4&gt;为了增强机器人的核心功能，从抽象到具体的操作，提出了ShareRobot数据集以及基于此的RoboBrain模型，旨在解决现有MLLM在机器人场景中的局限性。&lt;h4&gt;方法&lt;/h4&gt;ShareRobot是一个高质量的数据集，包含任务规划、可操作性识别和末端执行器轨迹等多维度信息。该数据集经过三个标注者的细心校正以确保其多样性和准确性。利用该数据集开发了RoboBrain模型，并采用多层次训练策略以及大量的视频和高分辨率图像进行优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过详尽的实验，证明RoboBrain在多种机器人任务中达到了最先进的性能水平。&lt;h4&gt;结论&lt;/h4&gt;这些成果强调了RoboBrain在提高机器人大脑功能方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Multimodal Large Language Models (MLLMs) have shownremarkable capabilities across various multimodal contexts. However, theirapplication in robotic scenarios, particularly for long-horizon manipulationtasks, reveals significant limitations. These limitations arise from thecurrent MLLMs lacking three essential robotic brain capabilities: PlanningCapability, which involves decomposing complex manipulation instructions intomanageable sub-tasks; Affordance Perception, the ability to recognize andinterpret the affordances of interactive objects; and Trajectory Prediction,the foresight to anticipate the complete manipulation trajectory necessary forsuccessful execution. To enhance the robotic brain's core capabilities fromabstract to concrete, we introduce ShareRobot, a high-quality heterogeneousdataset that labels multi-dimensional information such as task planning, objectaffordance, and end-effector trajectory. ShareRobot's diversity and accuracyhave been meticulously refined by three human annotators. Building on thisdataset, we developed RoboBrain, an MLLM-based model that combines robotic andgeneral multi-modal data, utilizes a multi-stage training strategy, andincorporates long videos and high-resolution images to improve its roboticmanipulation capabilities. Extensive experiments demonstrate that RoboBrainachieves state-of-the-art performance across various robotic tasks,highlighting its potential to advance robotic brain capabilities.</description>
      <author>example@mail.com (Yuheng Ji, Huajie Tan, Jiayu Shi, Xiaoshuai Hao, Yuan Zhang, Hengyuan Zhang, Pengwei Wang, Mengdi Zhao, Yao Mu, Pengju An, Xinda Xue, Qinghang Su, Huaihai Lyu, Xiaolong Zheng, Jiaming Liu, Zhongyuan Wang, Shanghang Zhang)</author>
      <guid isPermaLink="false">2502.21257v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</title>
      <link>http://arxiv.org/abs/2502.21186v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025. Code would be available at  \href{https://github.com/BaitingLuo/L-MAP.git}{this https URL}&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的离线强化学习框架Latent Macro Action Planner (L-MAP)，该框架通过学习一组时间延长的宏动作，解决了在高维连续行为空间中的序列决策问题。&lt;h4&gt;背景&lt;/h4&gt;在具有随机性的环境和高维行动空间中进行顺序决策面临计算挑战。传统离线增强学习设置下，代理必须基于通过随机行为策略收集的数据来学习如何做决定。&lt;h4&gt;目的&lt;/h4&gt;探索如何利用离线强化学习框架解决具有复杂动作空间的序列决策问题。&lt;h4&gt;方法&lt;/h4&gt;L-MAP 采用状态条件下的向量量化变分自动编码器 (VQ-VAE) 学习一组时间扩展的动作，通过这种方式减少动作维度。同时，它使用一个独立的学习先验模型作为潜在转换模型，并允许高效的可能行动采样。在规划过程中，通过蒙特卡洛树搜索（MCTS）来考虑环境和行为策略中的随机性。&lt;h4&gt;主要发现&lt;/h4&gt;L-MAP 能够有效地在线性时间范围内进行离线强化学习任务中的决策，即使在动作维度增加的情况下也能保持较低的延迟，并且在连续控制到高维机器人手部操作等多种任务上都表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过实验证明 L-MAP 在处理复杂和随机环境下的高维行动空间规划问题时是有效的，并能够与现有的模型方法相媲美，同时表现出了比其他基于模型的方法更好的性能。&lt;h4&gt;翻译&lt;/h4&gt;顺序决策在具有高维连续动作空间的随机环境中面临计算挑战。我们探索了传统的离线强化学习框架，在这种情况下，代理必须根据通过随机行为策略收集的数据来学习如何做出决定。我们提出了 Latent Macro Action Planner (L-MAP)，该方法通过对状态条件下的向量量化变分自动编码器进行训练来降低动作维度，并且采用一个独立的学习先验模型作为潜在转换模型和高效的可能行动采样工具。在规划过程中，通过蒙特卡洛树搜索(MCTS) 来考虑环境及行为策略中的随机性。L-MAP 在离线强化学习设置中包括随机连续控制任务时表现高效，能够在线性时间范围内进行决策，并且即使在动作维度增加的情况下也能保持低延迟。实验证明，在从具有内在随机性的连续控制到高维机器人手部操作的任务上，与现有模型方法相比 L-MAP 显著优于其他方法，并表现出与强大的无模型策略-评估者基准线相媲美的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sequential decision-making in high-dimensional continuous action spaces,particularly in stochastic environments, faces significant computationalchallenges. We explore this challenge in the traditional offline RL setting,where an agent must learn how to make decisions based on data collected througha stochastic behavior policy. We present \textit{Latent Macro Action Planner}(L-MAP), which addresses this challenge by learning a set of temporallyextended macro-actions through a state-conditional Vector Quantized VariationalAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employsa (separate) learned prior model that acts as a latent transition model andallows efficient sampling of plausible actions. During planning, our approachaccounts for stochasticity in both the environment and the behavior policy byusing Monte Carlo tree search (MCTS). In offline RL settings, includingstochastic continuous control tasks, L-MAP efficiently searches over discretelatent actions to yield high expected returns. Empirical results demonstratethat L-MAP maintains low decision latency despite increased actiondimensionality. Notably, across tasks ranging from continuous control withinherently stochastic dynamics to high-dimensional robotic hand manipulation,L-MAP significantly outperforms existing model-based methods and performson-par with strong model-free actor-critic baselines, highlighting theeffectiveness of the proposed approach in planning in complex and stochasticenvironments with high-dimensional action spaces.</description>
      <author>example@mail.com (Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay)</author>
      <guid isPermaLink="false">2502.21186v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A Minor-Testing Approach for Coordinated Motion Planning with Sliding Robots</title>
      <link>http://arxiv.org/abs/2502.21175v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了在无向图上的一种协调移动规划问题的变体，即协作滑动运动规划(CSMP)问题。&lt;h4&gt;背景&lt;/h4&gt;CSMP问题涉及给定一个无向图G、k个机器人R1至Rk放置于G的不同顶点，并且p≤k个不同的目标顶点供机器人R1至Rp使用。该问题是NP困难的，尤其是在全网格中。&lt;h4&gt;目的&lt;/h4&gt;研究CSMP在两个参数（即机器人数量k和时间限制l）下的参数复杂性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个固定参数算法来解决CSMP问题，在第一个结果中根据k参数化；在第二个结果中，为特殊情况下只有一个目标顶点的CSMP提出了一个基于l参数化的固定参数算法，并证明了该特殊情况是NP完全的。&lt;h4&gt;主要发现&lt;/h4&gt;解决方案可以表示为输入图的小标记拓扑子图，这是两个结果的关键新元素。&lt;h4&gt;结论&lt;/h4&gt;通过这两个新的算法和理论发现，作者在解决大规模复杂问题方面取得了进展。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了一种无向图上协调运动规划问题的变体——协作滑动运动规划(CSMP)问题。该问题要求判断是否存在一个序列调度，在这个调度中最多包含l次移动，使得每个有目标顶点的机器人能够到达它的目的地。此外，还提出了解决CSMP参数复杂性的固定参数算法，并证明了在特殊情况下是NP完全的。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study a variant of the Coordinated Motion Planning problem on undirectedgraphs, referred to herein as the \textsc{Coordinated Sliding-Motion Planning}(CSMP) problem. In this variant, we are given an undirected graph $G$, $k$robots $R_1,\dots,R_k$ positioned on distinct vertices of $G$, $p\leq k$distinct destination vertices for robots $R_1,\dots,R_p$, and $\ell \in\mathbb{N}$. The problem is to decide if there is a serial schedule of at most$\ell$ moves (i.e., of makespan $\ell$) such that at the end of the scheduleeach robot with a destination reaches it, where a robot's move is a free path(unoccupied by any robots) from its current position to an unoccupied vertex.The problem is known to be NP-hard even on full grids. It has been studied inseveral contexts, including coin movement and reconfiguration problems, withrespect to feasibility, complexity, and approximation. Geometric variants ofthe problem, in which congruent geometric-shape robots (e.g., unitdisk/squares) slide or translate in the Euclidean plane, have also been studiedextensively. We investigate the parameterized complexity of CSMP with respectto two parameters: the number $k$ of robots and the makespan $\ell$. As ourfirst result, we present a fixed-parameter algorithm for CSMP parameterized by$k$. For our second result, we present a fixed-parameter algorithmparameterized by $\ell$ for the special case of CSMP in which only a singlerobot has a destination and the graph is planar, which we prove to beNP-complete. A crucial new ingredient for both of our results is that thesolution admits a succinct representation as a small labeled topological minorof the input graph.</description>
      <author>example@mail.com (Eduard Eiben, Robert Ganian, Iyad Kanj, Ramanujan M. Sridharan)</author>
      <guid isPermaLink="false">2502.21175v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Rare event modeling with self-regularized normalizing flows: what can we learn from a single failure?</title>
      <link>http://arxiv.org/abs/2502.21110v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为CalNF（校准归一化流程）的新框架，用于从有限的数据中进行后验学习。这种方法解决了在处理安全关键系统的罕见故障事件时由于数据稀缺而遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;随着自动驾驶系统和机器人技术的广泛应用，与之相关的安全问题也日益凸显。此类系统的故障往往难以通过现有的方法来建模和调试，因为缺乏足够的失败案例的数据支持。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的框架CalNF，以克服由于罕见故障事件数据不足导致的传统模型训练中的局限性，如过拟合或欠拟合等问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自调节的归一化流程技术（Calibrated Normalizing Flows, CalNF），专门设计用于在数据有限的情况下进行后验学习，并且能够在处理逆问题和罕见故障建模时达到最先进的性能水平。&lt;h4&gt;主要发现&lt;/h4&gt;使用CalNF框架能够成功解析2022年美国西南航空公司调度危机的根本原因，这是一项开创性的案例研究。&lt;h4&gt;结论&lt;/h4&gt;CalNF框架为解决由于数据稀缺而引起的罕见安全关键事件的建模难题提供了一个有效的解决方案，并且已经在实际问题中得到了验证。&lt;h4&gt;翻译&lt;/h4&gt;随着无人驾驶系统和机器人技术在运输等领域的部署增加，相应的安全性关键性故障也有所上升。这些故障难以通过现有的方法进行建模和调试，因为缺乏足够的失败案例的数据支持。为了应对这一挑战，研究人员提出了一种名为CalNF的新框架，它利用了自调节的归一化流程技术，特别适用于从有限数据中进行后验学习，并且已经在处理逆问题和罕见故障事件模型方面取得了最先进的性能表现。通过这种方法的应用，能够首次对2022年美国西南航空公司调度危机的根本原因进行了深入分析。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Increased deployment of autonomous systems in fields like transportation androbotics have seen a corresponding increase in safety-critical failures. Thesefailures can be difficult to model and debug due to the relative lack of data:compared to tens of thousands of examples from normal operations, we may haveonly seconds of data leading up to the failure. This scarcity makes itchallenging to train generative models of rare failure events, as existingmethods risk either overfitting to noise in the limited failure dataset orunderfitting due to an overly strong prior. We address this challenge withCalNF, or calibrated normalizing flows, a self-regularized framework forposterior learning from limited data. CalNF achieves state-of-the-artperformance on data-limited failure modeling and inverse problems and enables afirst-of-a-kind case study into the root causes of the 2022 Southwest Airlinesscheduling crisis.</description>
      <author>example@mail.com (Charles Dawson, Van Tran, Max Z. Li, Chuchu Fan)</author>
      <guid isPermaLink="false">2502.21110v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Jointly Assigning Processes to Machines and Generating Plans for Autonomous Mobile Robots in a Smart Factory</title>
      <link>http://arxiv.org/abs/2502.21101v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;ACES是一种用于智能工厂的优化算法，可以同时解决工艺分配和路径规划问题。&lt;h4&gt;背景&lt;/h4&gt;现代智能工厂使用可编程机器进行生产，并通过移动机器人运输材料。目前现有的管理系统是顺序地解决问题，这限制了它们所能实现的最大吞吐量。&lt;h4&gt;目的&lt;/h4&gt;介绍ACES（Anytime Cyclic Embedding Solver），这是一种能够同时优化工艺分配和路径规划问题的解决方案。&lt;h4&gt;方法&lt;/h4&gt;ACES可以同时解决智能工厂中的工艺分配和移动机器人运输路线的问题，从而提高整个系统的生产效率。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验评估表明，ACES能够在现实工业场景中进行扩展，并且相较于现有系统，能够实现更高的吞吐量。&lt;h4&gt;结论&lt;/h4&gt;ACES为智能工厂提供了一种有效的解决方案来优化生产和材料运输流程。&lt;h4&gt;翻译&lt;/h4&gt;摘要：现代智能工厂使用一组可编程机器运行制造程序。通常，材料通过一群移动机器人在这些机器之间运送。为了将制造过程嵌入到智能工厂中，工厂操作员必须a) 将其工艺分配给智能工厂的机器，b) 确定代理如何在机器之间运输材料。一个好的嵌入可以最大化智能工厂的吞吐量；即它输出产品的速度。现有的智能工厂管理系统按顺序解决上述问题，限制了它们所能实现的最大吞吐量。在这篇论文中我们介绍了ACES（Anytime Cyclic Embedding Solver），这是一种首次同时优化工艺分配和路径规划问题的解决方案。我们评估了ACES，并表明它可以扩展到现实工业场景中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A modern smart factory runs a manufacturing procedure using a collection ofprogrammable machines. Typically, materials are ferried between these machinesusing a team of mobile robots. To embed a manufacturing procedure in a smartfactory, a factory operator must a) assign its processes to the smart factory'smachines and b) determine how agents should carry materials between machines. Agood embedding maximizes the smart factory's throughput; the rate at which itoutputs products. Existing smart factory management systems solve theaforementioned problems sequentially, limiting the throughput that they canachieve. In this paper we introduce ACES, the Anytime Cyclic Embedding Solver,the first solver which jointly optimizes the assignment of processes tomachines and the assignment of paths to agents. We evaluate ACES and show thatit can scale to real industrial scenarios.</description>
      <author>example@mail.com (Christopher Leet, Aidan Sciortino, Sven Koenig)</author>
      <guid isPermaLink="false">2502.21101v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>AuthSim: Towards Authentic and Effective Safety-critical Scenario Generation for Autonomous Driving Tests</title>
      <link>http://arxiv.org/abs/2502.21100v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;生成对抗性安全关键场景是测试自动驾驶系统的关键方法，有助于识别潜在弱点并增强系统的鲁棒性和可靠性。然而，现有的方法主要关注不受限制的碰撞场景，导致非玩家角色（NPC）车辆无差别攻击主控车。这些研究忽略了这些场景的真实性和合理性，产生了许多极端、人为构造且不现实的涉及激进NPC车辆的碰撞事件。&lt;h4&gt;背景&lt;/h4&gt;当前的方法在测试自动驾驶系统时过于集中于制造不受限制和过度激进的对抗性情况，导致生成的场景缺乏真实性和理性。&lt;h4&gt;目的&lt;/h4&gt;提出一种三层相对安全区域模型，并开发一个名为AuthSim的平台来产生更真实有效的安全关键场景，以解决现有方法的局限性。&lt;h4&gt;方法&lt;/h4&gt;引入了三层相对安全区域模型和结合强化学习的方法，这个模型可以划分基于危险等级的不同区域并调整NPC车辆进入这些边界区域的概率。同时利用该模型与强化学习相结合构建了一个全面平台AuthSim。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示AuthSim在生成有效的安全关键场景方面比现有方法表现更佳，尤其在平均切入距离和平均碰撞间隔时间上分别提高了5.25%和27.12%，并且效率更高。&lt;h4&gt;结论&lt;/h4&gt;这是首次全面解决自动驾驶系统测试场景的真实性和有效性问题的尝试。AuthSim证明了其在生成真实场景方面的显著优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到，对抗性安全关键场景对测试自动驾驶系统的潜在弱点至关重要，并增强其鲁棒性和可靠性。然而，现有的方法过分关注无约束碰撞情况，导致NPC车辆针对主控车发动毫无选择的攻击。这些方法忽视了情景的真实性和合理性，产生了大量极端且不现实的情况，其中涉及的是激进的NPC行为。为解决这些问题，研究团队提出了一种三层相对安全区域模型，并开发了一个名为AuthSim的平台，该平台利用这个模型与强化学习相结合，以产生更加真实和有效的测试场景。实验表明，相对于现有方法，AuthSim在生成有效且关键的安全场景方面表现出色，尤其是在提高平均切入距离和减少碰撞间隔时间上取得了显著改善。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating adversarial safety-critical scenarios is a pivotal method fortesting autonomous driving systems, as it identifies potential weaknesses andenhances system robustness and reliability. However, existing approachespredominantly emphasize unrestricted collision scenarios, prompting non-playercharacter (NPC) vehicles to attack the ego vehicle indiscriminately. Theseworks overlook these scenarios' authenticity, rationality, and relevance,resulting in numerous extreme, contrived, and largely unrealistic collisionevents involving aggressive NPC vehicles. To rectify this issue, we propose athree-layer relative safety region model, which partitions the area based ondanger levels and increases the likelihood of NPC vehicles entering relativeboundary regions. This model directs NPC vehicles to engage in adversarialactions within relatively safe boundary regions, thereby augmenting thescenarios' authenticity. We introduce AuthSim, a comprehensive platform forgenerating authentic and effective safety-critical scenarios by integrating thethree-layer relative safety region model with reinforcement learning. To ourknowledge, this is the first attempt to address the authenticity andeffectiveness of autonomous driving system test scenarios comprehensively.Extensive experiments demonstrate that AuthSim outperforms existing methods ingenerating effective safety-critical scenarios. Notably, AuthSim achieves a5.25% improvement in average cut-in distance and a 27.12% enhancement inaverage collision interval time, while maintaining higher efficiency ingenerating effective safety-critical scenarios compared to existing methods.This underscores its significant advantage in producing authentic scenariosover current methodologies.</description>
      <author>example@mail.com (Yukuan Yang, Xucheng Lu, Zhili Zhang, Zepeng Wu, Guoqi Li, Lingzhong Meng, Yunzhi Xue)</author>
      <guid isPermaLink="false">2502.21100v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Vibrotactile information coding strategies for a body-worn vest to aid robot-human collaboration</title>
      <link>http://arxiv.org/abs/2502.21056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了通过身体穿戴式的振动触觉背心向操作员传递机器人实时信息的方法。研究旨在探索在高认知负荷条件下，如何有效地利用非视觉和非听觉感知方式来传达关键信息。&lt;h4&gt;背景&lt;/h4&gt;在城市搜索与救援（USAR）场景中，当人类与机器人协同工作时，尤其是在高度复杂的环境中，触觉通信可以为操作员提供重要而不影响其视听能力的信息。这种情况通常伴随着高认知负荷条件下的作业。&lt;h4&gt;目的&lt;/h4&gt;本文的目的是通过不同的振动触觉信息编码策略来探讨如何最好地传达此类信息，并引入了语义触觉的概念，以改善在机器人远程侦察时的情景理解。&lt;h4&gt;方法&lt;/h4&gt;文章介绍了一种新的信息表示技术——语义触觉（Semantic Haptics），该技术利用形状和模式来表示特定事件。这种方法试图使皮肤像屏幕一样工作，旨在提高学习能力和解释准确度。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用形状和图案来代表特定的事件，可以实现更好的可学性和解释准确性。这表明在复杂作业环境中采用触觉信息传递的有效性与优势。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明了利用振动背心进行触觉通信在提高操作员对环境理解方面的潜力，并且语义触觉技术可能为未来机器人辅助搜索和救援任务提供有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the use of a body-worn vibrotactile vest to conveyreal-time information from robot to operator. Vibrotactile communication couldbe useful in providing information without compropmising or loading a person'svisual or auditory perception. This paper considers applications in UrbanSearch and Rescue (USAR) scenarios where a human working alongside a robot islikely to be operating in high cognitive load conditions. The focus is onunderstanding how best to convey information considering different vibrotactileinformation coding strategies to enhance scene understanding in scenarios wherea robot might be operating remotely as a scout. In exploring informationrepresentation, this paper introduces Semantic Haptics, using shapes andpatterns to represent certain events as if the skin was a screen, and shows howthese lead to bettter learnability and interpreation accuracy.</description>
      <author>example@mail.com (Adrian Vecina Tercero, Praminda Caleb-Solly)</author>
      <guid isPermaLink="false">2502.21056v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR 2025. Github  page:https://github.com/justacar/Plane-DUSt3R&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的多视角房间布局估计方法Plane-DUSt3R，该方法利用了三维基础模型DUSt3R。&lt;h4&gt;背景&lt;/h4&gt;从多个视角的图像中进行房间布局估计的问题由于多视图几何复杂性而研究较少。传统的结构光运动过程需要多步骤解决方案，比如相机内参和外参估计、图像匹配以及三角测量等。&lt;h4&gt;目的&lt;/h4&gt;为了简化房间布局估计的过程并减少误差累积，本文旨在提出一种新的单步端到端方法来处理这个问题。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R基于DUSt3R框架，并在房间布局数据集（Structure3D）上进行微调，以修改后的目标函数来估算结构平面。此模型能够仅通过一次后处理步骤和二维检测结果来进行房间布局估计。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有最先进的方法相比，Plane-DUSt3R不仅在合成数据集中表现更优，在不同图像风格（如卡通）的真实世界数据中也显示了鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;通过引入Plane-DUSt3R，本文提供了一种高效的多视角房间布局估计解决方案，并展示了其在各种环境下的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;从多个视角的图像进行房间布局估计问题因多视图几何复杂性而研究不足。然而，在三维重建领域，近年来出现了像DUSt3R这样的三维基础模型，改变了传统的多步骤结构光运动流程为单步端到端方法。本文引入了Plane-DUSt3R，一种利用DUSt3R框架进行多视角房间布局估计的新方法。该模型在经过修改的目标函数指导下，在一个房间布局数据集上进行了微调，并且能够通过二维检测结果和单一后处理步骤实现高效精确的结构平面预测。不同于以往依赖单视图或全景图像的方法，Plane-DUSt3R扩展了其能力以适应多视角输入，并提供了一种简化的、端到端的解决方案。实验表明，这种方法在合成数据集中超越了现有最佳方法，在不同样式的实际场景（包括卡通风格）中也展示了强大的性能和有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon.Our code is available at:https://github.com/justacar/Plane-DUSt3R</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v2</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Sixth-Sense: Self-Supervised Learning of Spatial Awareness of Humans from a Planar Lidar</title>
      <link>http://arxiv.org/abs/2502.21029v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种从1D激光雷达数据中检测人类并估计其2D姿态的自监督方法，以解决服务机器人在没有RGB-D摄像头或昂贵3D LiDAR的情况下难以感知周围人的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的服务机器人主要依赖于RGB-D相机或昂贵的3D LiDAR进行人体定位，但商业上常见的服务机器人通常配备视野狭窄的普通相机或者读数难以解析的一维激光雷达。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有设备的成本和功能限制，论文提出了一个利用1D LiDAR数据检测人类并估计其2D姿态的方法，并使用RGB-D摄像头的数据作为监督信号进行训练。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自监督学习框架，该框架通过将从1DLiDAR获得的原始距离信息与从RGB-D相机获取的人体框和关键点进行配对来实现人体检测和姿态估计任务。模型经过70分钟数据（在两个环境自主收集）训练后能够实现在新环境中基于1DLiDAR的数据进行全向人类检测。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在新的未知环境中从单一线激光雷达信息中准确地进行全方位的人体检测，达到71%的精度和80%的召回率，并且在距离上保持了平均绝对误差为13cm，在方向角度上有44度的估计偏差。&lt;h4&gt;结论&lt;/h4&gt;论文提出的方法证明了一维LiDAR可以作为服务机器人实现自主定位与交互的有效感知工具，显著提高了机器人的环境适应性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Localizing humans is a key prerequisite for any service robot operating inproximity to people. In these scenarios, robots rely on a multitude ofstate-of-the-art detectors usually designed to operate with RGB-D cameras orexpensive 3D LiDARs. However, most commercially available service robots areequipped with cameras with a narrow field of view, making them blind when auser is approaching from other directions, or inexpensive 1D LiDARs whosereadings are difficult to interpret. To address these limitations, we propose aself-supervised approach to detect humans and estimate their 2D pose from 1DLiDAR data, using detections from an RGB-D camera as a supervision source. Ourapproach aims to provide service robots with spatial awareness of nearbyhumans. After training on 70 minutes of data autonomously collected in twoenvironments, our model is capable of detecting humans omnidirectionally from1D LiDAR data in a novel environment, with 71% precision and 80% recall, whileretaining an average absolute error of 13 cm in distance and 44{\deg} inorientation.</description>
      <author>example@mail.com (Simone Arreghini, Nicholas Carlotti, Mirko Nava, Antonio Paolillo, Alessandro Giusti)</author>
      <guid isPermaLink="false">2502.21029v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Nano Drone-based Indoor Crime Scene Analysis</title>
      <link>http://arxiv.org/abs/2502.21019v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 4 figures, to be submitted to ARSO 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文采用投机原型设计的方法，使用STAIR工具快速回顾文献并识别出犯罪现场分析中尚未受到足够关注的任务，并开发了一种小型无人机进行初步验证。&lt;h4&gt;背景&lt;/h4&gt;机器人技术、人工智能和计算机视觉可以应用于犯罪现场分析，以保护生命、促进正义和防止犯罪。但尚缺乏对可自动化任务的全面概述。&lt;h4&gt;目的&lt;/h4&gt;识别并实现犯罪现场分析中可自动化的任务，并通过原型设计展示其可行性和性能。&lt;h4&gt;方法&lt;/h4&gt;采用STAIR工具进行文献回顾，确定了访问犯罪现场（如通过窗户）、绘制和收集证据以及分析血迹等未受足够关注的任务。接着开发了一种小型无人机原型以执行这些任务。&lt;h4&gt;主要发现&lt;/h4&gt;该无人机在三个特定任务中分别达到了75%、85%和80%的性能，展示了技术应用于犯罪现场分析的可能性。&lt;h4&gt;结论&lt;/h4&gt;此次工作通过初步实验为未来的研究提供指导，并强调了进一步研究的必要性以改善自动化系统对复杂犯罪场景的支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要：机器人技术、人工智能（AI）和计算机视觉（CV）可以被用于帮助保护生命、促进正义及防止犯罪，但关于可自动化的任务概述却很缺乏。本文采用投机原型设计的方法：首先利用STAIR工具快速回顾文献并识别出访问犯罪现场通过窗户进入等尚未受到足够关注的任务；其次开发一种小型无人机以实现这些任务，并在室内犯罪场景中进行初步分析。最后报告了所学到的经验教训，为后续的研究提供指导。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Technologies such as robotics, Artificial Intelligence (AI), and ComputerVision (CV) can be applied to crime scene analysis (CSA) to help protect lives,facilitate justice, and deter crime, but an overview of the tasks that can beautomated has been lacking. Here we follow a speculate prototyping approach:First, the STAIR tool is used to rapidly review the literature and identifytasks that seem to have not received much attention, like accessing crime sitesthrough a window, mapping/gathering evidence, and analyzing blood smears.Secondly, we present a prototype of a small drone that implements these threetasks with 75%, 85%, and 80% performance, to perform a minimal analysis of anindoor crime scene. Lessons learned are reported, toward guiding next work inthe area.</description>
      <author>example@mail.com (Martin Cooney, Sivadinesh Ponrajan, Fernando Alonso-Fernandez)</author>
      <guid isPermaLink="false">2502.21019v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Motion ReTouch: Motion Modification Using Four-Channel Bilateral Control</title>
      <link>http://arxiv.org/abs/2502.20982v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures, Accepted at ICM2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种名为Motion ReTouch的新方法被提出，该方法可以通过多边控制和动作复制系统结合的方式对通过四通道双边控制获得的运动数据进行事后修改。&lt;h4&gt;背景&lt;/h4&gt;最近的研究表明，在自主机器人操作中使用模仿学习是有效的。特别是利用能够获取位置和力信息的四通道双边控制来进行教学已被证明是有效的。&lt;h4&gt;目的&lt;/h4&gt;为了实现能够轻松执行高速、复杂任务的一次性控制性能，本研究提出了一种新的方法。&lt;h4&gt;方法&lt;/h4&gt;该方法称为Motion ReTouch，它不仅能修改运动的位置数据，还能修改力的信息。这通过多边控制和动作复制系统的结合得以实现。&lt;h4&gt;主要发现&lt;/h4&gt;在使用真实机器人进行的实验中，试验表明测试管转移任务的成功率得到了提高，证明了修改力信息的可能性。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种新颖的方法来增强模仿学习的效果，并为实现更高性能的任务执行提供了可能途径。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究已经展示了模仿学习在自主机器人操作中的有用性。特别是使用四通道双边控制进行教学已被证明是有效的，它可以获取位置和力的信息。然而，尚未实现能够轻松一次性完成高速、复杂任务的控制性能。我们提出了一种叫做Motion ReTouch的方法，该方法可以通过事后修改通过四通道双边控制获得的运动数据来提高这一能力。此方法不仅可以修改位置信息，还可以修改力信息。这是通过多边控制和动作复制系统的结合得以实现的。在真实机器人的实验中验证了所提出的这种方法，并且提高了测试管转移任务的成功率，表明了修改力信息的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent research has demonstrated the usefulness of imitation learning inautonomous robot operation. In particular, teaching using four-channelbilateral control, which can obtain position and force information, has beenproven effective. However, control performance that can easily executehigh-speed, complex tasks in one go has not yet been achieved. We propose amethod called Motion ReTouch, which retroactively modifies motion data obtainedusing four-channel bilateral control. The proposed method enables modificationof not only position but also force information. This was achieved by thecombination of multilateral control and motion-copying system. The proposedmethod was verified in experiments with a real robot, and the success rate ofthe test tube transfer task was improved, demonstrating the possibility ofmodification force information.</description>
      <author>example@mail.com (Koki Inami, Sho Sakaino, Toshiaki Tsuji)</author>
      <guid isPermaLink="false">2502.20982v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Optimality and Suboptimality of MPPI Control in Stochastic and Deterministic Settings</title>
      <link>http://arxiv.org/abs/2502.20953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, submitted to LCSS with CDC25 option&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了MPPI控制框架在解决最优控制问题中的应用及其性能分析。&lt;h4&gt;背景&lt;/h4&gt;Model Predictive Path Integral (MPPI) 控制方法近年来受到广泛关注，特别是在机器人和强化学习领域。&lt;h4&gt;目的&lt;/h4&gt;旨在使MPPI控制框架更容易被最优控制社区理解，并展示其应用于三类最优控制问题的效果及性能。&lt;h4&gt;方法&lt;/h4&gt;研究了在一般确定性非线性离散时间系统中，MPPI控制与最优解之间的次优性。通过数值例子来说明这一分析结果。&lt;h4&gt;主要发现&lt;/h4&gt;在一个平滑且无约束的条件下，随着不确定性的增加，由MPPI提供的控制输入轨迹和最优控制问题解之间的差距增长是二次级别的。并且指出调整超参数可以调节这种次优性。&lt;h4&gt;结论&lt;/h4&gt;通过数值例子验证了上述分析结果，并展示了如何通过适当调整超参数来减轻MPPI解决方案的次优性。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到，最近Model Predictive Path Integral (MPPI) 控制方法在机器人和强化学习领域获得了大量关注。本论文旨在使该控制框架更容易被最优控制社区理解和应用，同时展示了三种不同类型的最优控制问题以及它们通过MPPI得到的解决方案，并研究了确定性非线性离散时间系统中MPPI的次优性能。主要发现表明，在平滑且无约束条件下，随着不确定性的增加，由MPPI提供的控制输入轨迹和最优解之间的差距呈二次级增长。结果还指出，通过适当调整超参数可以调节这种次优性，并用数值例子展示了这些研究结论的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model predictive path integral (MPPI) control has recently received a lot ofattention, especially in the robotics and reinforcement learning communities.This letter aims to make the MPPI control framework more accessible to theoptimal control community. We present three classes of optimal control problemsand their solutions by MPPI. Further, we investigate the suboptimality of MPPIto general deterministic nonlinear discrete-time systems. Here, suboptimalityis defined as the deviation between the control provided by MPPI and theoptimal solution to the deterministic optimal control problem. Our findings arethat in a smooth and unconstrained setting, the growth of suboptimality in thecontrol input trajectory is second-order with the scaling of uncertainty. Theresults indicate that the suboptimality of the MPPI solution can be modulatedby appropriately tuning the hyperparameters. We illustrate our findings usingnumerical examples.</description>
      <author>example@mail.com (Hannes Homburger, Florian Messerer, Moritz Diehl, Johannes Reuter)</author>
      <guid isPermaLink="false">2502.20953v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>DexGraspVLA: A Vision-Language-Action Framework Towards General Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2502.20900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 10 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;灵巧抓取在机器人技术中仍然是一个基础且具有挑战性的问题。通用机器人的任务是能够应对各种物体的抓取需求，并适应不同的场景。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法，即DexGraspVLA框架，以实现更好的跨域迁移能力及泛化性能，从而解决现有研究依赖于特定假设（如单一物体设置或有限环境）而导致泛化受限的问题。&lt;h4&gt;方法&lt;/h4&gt;DexGraspVLA是一种层次化的框架，它利用预训练的视觉-语言模型作为高层次的任务规划器，并学习一种基于扩散的方法作为低级别的动作控制器。其核心在于能够迭代地将多样性的语言和视觉输入转换为领域不变的表示，从而减少域间偏移并使模仿学习更加有效。&lt;h4&gt;主要发现&lt;/h4&gt;在数千种未见过的对象、光照及背景组合的情况下，在零样本环境中达到了90%以上的成功抓取率，并且实验分析表明内部模型行为的一致性随着环境变化而保持稳定。&lt;h4&gt;结论&lt;/h4&gt;该研究通过设计DexGraspVLA框架，展示了实现灵巧抓取的通用能力的可能性，希望为这一领域的进步提供一步推进。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dexterous grasping remains a fundamental yet challenging problem in robotics.A general-purpose robot must be capable of grasping diverse objects inarbitrary scenarios. However, existing research typically relies on specificassumptions, such as single-object settings or limited environments, leading toconstrained generalization. Our solution is DexGraspVLA, a hierarchicalframework that utilizes a pre-trained Vision-Language model as the high-leveltask planner and learns a diffusion-based policy as the low-level Actioncontroller. The key insight lies in iteratively transforming diverse languageand visual inputs into domain-invariant representations, where imitationlearning can be effectively applied due to the alleviation of domain shift.Thus, it enables robust generalization across a wide range of real-worldscenarios. Notably, our method achieves a 90+% success rate under thousands ofunseen object, lighting, and background combinations in a ``zero-shot''environment. Empirical analysis further confirms the consistency of internalmodel behavior across environmental variations, thereby validating our designand explaining its generalization performance. We hope our work can be a stepforward in achieving general dexterous grasping. Our demo and code can be foundat https://dexgraspvla.github.io/.</description>
      <author>example@mail.com (Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, Yuanpei Chen)</author>
      <guid isPermaLink="false">2502.20900v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical and Modular Network on Non-prehensile Manipulation in General Environments</title>
      <link>http://arxiv.org/abs/2502.20843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  http://unicorn-hamnet.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种模块化和可重构的架构，以解决机器人在不同几何环境中执行非抓握操作时面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;机器人需要具备非抓取的操作能力来处理不可抓取的对象，如物体倾倒和滚动。然而现有的方法难以适应环境的变化，导致策略难以泛化。&lt;h4&gt;目的&lt;/h4&gt;研究如何让机器人能够根据不同的任务需求自适应地改变其行为模式，以应对复杂多变的几何约束。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的架构设计和接触为基础的对象表示（CORN）的扩展版本来处理环境变化。还开发了一个生成多样环境的算法来训练机器人，并发布了一个包含真实场景数字模型的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的适应性架构能够在完全没有见过的真实世界环境中进行零样本转移，显示出其泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了使用模块化和可重构设计可以解决非抓握操作的通用性和适应性的关键问题。&lt;h4&gt;翻译&lt;/h4&gt;摘要：为了使机器人能够在家用等一般环境运行，它们必须能够执行像倾倒和滚动这样的非抓取行为来处理不可抓取的对象。然而现有的关于非抓取操纵的研究还不能泛化到几何结构多样的环境中去。主要挑战在于适应不同的环境约束：在一个橱柜内，机器人需要避开墙壁和天花板；要将物体提升至阶梯顶部，则必须考虑阶梯的姿态和延伸度。虽然深度强化学习（RL）在非抓握操作方面已经取得了显著的成功，但是考虑到这种变化性为泛化策略带来了挑战，因为这需要从每个新的约束组合中学习不同的策略。为了应对这一挑战，我们提出了一种模块化且可重构的架构，该架构能够根据任务需求自适应地重新配置网络模块。为了捕捉环境中几何结构的变化，我们将基于接触的对象表示（CORN）扩展到环境几何，并提出了一个生成多样环境以训练我们的代理的程序算法。综上所述，所得到的策略可以在完全没有见过的真实世界环境中进行零样本转移，尽管是在模拟器中完成的所有培训。此外，我们还发布了以九个真实场景数字模型为特征的数据集来支持现实领域的非抓握操作研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For robots to operate in general environments like households, they must beable to perform non-prehensile manipulation actions such as toppling androlling to manipulate ungraspable objects. However, prior works onnon-prehensile manipulation cannot yet generalize across environments withdiverse geometries. The main challenge lies in adapting to varyingenvironmental constraints: within a cabinet, the robot must avoid walls andceilings; to lift objects to the top of a step, the robot must account for thestep's pose and extent. While deep reinforcement learning (RL) has demonstratedimpressive success in non-prehensile manipulation, accounting for suchvariability presents a challenge for the generalist policy, as it must learndiverse strategies for each new combination of constraints. To address this, wepropose a modular and reconfigurable architecture that adaptively reconfiguresnetwork modules based on task requirements. To capture the geometricvariability in environments, we extend the contact-based object representation(CORN) to environment geometries, and propose a procedural algorithm forgenerating diverse environments to train our agent. Taken together, theresulting policy can zero-shot transfer to novel real-world environments andobjects despite training entirely within a simulator. We additionally release asimulation-based benchmark featuring nine digital twins of real-world sceneswith 353 objects to facilitate non-prehensile manipulation research inrealistic domains.</description>
      <author>example@mail.com (Yoonyoung Cho, Junhyek Han, Jisu Han, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.20843v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Learning-Based Leader Localization for Underwater Vehicles With Optical-Acoustic-Pressure Sensor Fusion</title>
      <link>http://arxiv.org/abs/2502.20817v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种三模态传感器融合神经网络方法，用于提高水下多车辆系统中领导者定位的精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;水下航行器在探索和监测海洋环境方面发挥着关键作用。多车辆系统的部署因其能够执行协作任务而引起广泛关注，但要在动态复杂的水下环境中精确确定领导者的方位仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;通过整合光学、声学和压力传感器来解决领导者定位的精度问题，并提高其鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;利用深度学习架构将三模态传感器（光学传感器提供高分辨率成像，声学传感器实现远程探测和测距，而压力传感器则感知环境背景信息）的数据融合起来以提取并结合互补特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明提出的三模态方法显著提高了领导者定位的准确性和鲁棒性，优于单模态或双模态的方法。&lt;h4&gt;结论&lt;/h4&gt;这项研究展示了如何通过智能地利用不同类型的传感器数据来提高水下多车辆系统的性能。这种方法有望在未来的海洋科学研究和应用中发挥重要作用。&lt;h4&gt;翻译&lt;/h4&gt;水下航行器已经成为探索和监测水域环境的关键技术，部署多车系统进行协同任务变得越来越受欢迎，但要在一个动态的复杂环境中精确定位领导者则仍然具有挑战性。本文提出了一种新颖的三模态传感器融合神经网络方法，该方法整合了光学、声学和压力传感器来定位领导者。通过利用各个传感模式的独特优势，这种方法提高了定位精度和鲁棒性。实验结果表明，这种三模态方法在准确性与鲁棒性上显著优于单一或双模态的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Underwater vehicles have emerged as a critical technology for exploring andmonitoring aquatic environments. The deployment of multi-vehicle systems hasgained substantial interest due to their capability to perform collaborativetasks with improved efficiency. However, achieving precise localization of aleader underwater vehicle within a multi-vehicle configuration remains asignificant challenge, particularly in dynamic and complex underwaterconditions. To address this issue, this paper presents a novel tri-modal sensorfusion neural network approach that integrates optical, acoustic, and pressuresensors to localize the leader vehicle. The proposed method leverages theunique strengths of each sensor modality to improve localization accuracy androbustness. Specifically, optical sensors provide high-resolution imaging forprecise relative positioning, acoustic sensors enable long-range detection andranging, and pressure sensors offer environmental context awareness. The fusionof these sensor modalities is implemented using a deep learning architecturedesigned to extract and combine complementary features from raw sensor data.The effectiveness of the proposed method is validated through a custom-designedtesting platform. Extensive data collection and experimental evaluationsdemonstrate that the tri-modal approach significantly improves the accuracy androbustness of leader localization, outperforming both single-modal anddual-modal methods.</description>
      <author>example@mail.com (Mingyang Yang, Zeyu Sha, Feitian Zhang)</author>
      <guid isPermaLink="false">2502.20817v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Towards Semantic 3D Hand-Object Interaction Generation via Functional Text Guidance</title>
      <link>http://arxiv.org/abs/2502.20805v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种创新的两阶段框架Functional Grasp Synthesis Net (FGS-Net)，旨在基于功能文本生成手部与物体之间的三维交互姿态。&lt;h4&gt;背景&lt;/h4&gt;手势控制在人机交互中扮演关键角色，但由于抓取动作的复杂性和多样性，现有的技术难以捕捉功能性抓握任务的意义。&lt;h4&gt;目的&lt;/h4&gt;为了实现功能性的三维抓握手部-对象交互（HOI）生成，论文提出了一种新的框架来解决现有方法无法精确模拟功能性抓握的问题。&lt;h4&gt;方法&lt;/h4&gt;该方法包括一个文本引导的3D模型生成器Functional Grasp Generator (FGG)和一个姿态优化策略Functional Grasp Refiner (FGR)，前者基于文本输入生成手部与物体的3D模型，后者利用Object Pose Approximator和能量函数来调整姿态以确保符合人体意图。&lt;h4&gt;主要发现&lt;/h4&gt;该框架能够产生精确且高质量的手部-对象交互（HOI）而无需额外的三维标注数据。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了FGS-Net的有效性，在生成功能性抓握手部-物体交互方面表现出色，具有广泛应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;手部与物体之间的互动是人与环境之间的重要联系。尽管AI和机器人技术已取得重大进展，但捕捉功能抓握任务的语义仍是一个挑战。本文提出了一种新的两阶段框架Functional Grasp Synthesis Net (FGS-Net)，用于基于功能性文本生成3D HOI。该方法结合了基于文本引导的3D模型生成器和姿态优化策略，以确保手部与物体之间的相对位置符合人类意图并保持物理合理性。实验表明，在无需额外三维标注数据的情况下，我们的方法可以实现精确且高质量的手部-对象交互生成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hand-object interaction(HOI) is the fundamental link between human andenvironment, yet its dexterous and complex pose significantly challenges forgesture control. Despite significant advances in AI and robotics, enablingmachines to understand and simulate hand-object interactions, capturing thesemantics of functional grasping tasks remains a considerable challenge. Whileprevious work can generate stable and correct 3D grasps, they are still farfrom achieving functional grasps due to unconsidered grasp semantics. Toaddress this challenge, we propose an innovative two-stage framework,Functional Grasp Synthesis Net (FGS-Net), for generating 3D HOI driven byfunctional text. This framework consists of a text-guided 3D model generator,Functional Grasp Generator (FGG), and a pose optimization strategy, FunctionalGrasp Refiner (FGR). FGG generates 3D models of hands and objects based on textinput, while FGR fine-tunes the poses using Object Pose Approximator and energyfunctions to ensure the relative position between the hand and object alignswith human intent and remains physically plausible. Extensive experimentsdemonstrate that our approach achieves precise and high-quality HOI generationwithout requiring additional 3D annotation data.</description>
      <author>example@mail.com (Yongqi Tian, Xueyu Sun, Haoyuan He, Linji Hao, Ning Ding, Caigui Jiang)</author>
      <guid isPermaLink="false">2502.20805v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Characteristics Analysis of Autonomous Vehicle Pre-crash Scenarios</title>
      <link>http://arxiv.org/abs/2502.20789v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文分析了加州的自动驾驶车辆碰撞报告，并基于修订后的预碰撞场景分类法，提出了一套自动提取预碰撞场景的映射规则。&lt;h4&gt;背景&lt;/h4&gt;近年来，在自动驾驶车辆（AV）的道路测试中发生了数百起事故，强调了提高其可靠性和安全性的重要性。现有的研究主要集中在传统的人类驾驶车辆上的碰撞分析上，缺少专门针对自动驾驶汽车深入碰撞分析的研究。&lt;h4&gt;目的&lt;/h4&gt;通过分析最新的加州自动驾驶车辆碰撞报告和使用修订后的预碰撞场景分类法来识别自动驾驶车辆的预碰撞场景，并提出优化建议以提高其性能。&lt;h4&gt;方法&lt;/h4&gt;提出了用于自动提取24种不同类型预碰撞场景（准确率为98.1%）的映射规则，特别关注了追尾场景和交叉口场景两大关键场景。对这些场景进行了详细的环境因素分析和因果关系分析。&lt;h4&gt;主要发现&lt;/h4&gt;在追尾场景中，交通控制类型、地点类型以及光线等是重要因素；对于可能引发严重碰撞事故的交叉口场景，则识别出了惯常违反规则与期望特定行为为主要原因。&lt;h4&gt;结论&lt;/h4&gt;本文的研究成果可帮助政府机构制定相关法规，指导制造商设计测试场景，并改进控制算法以优化自动驾驶系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;迄今为止，在自动车辆（AV）的道路测试中发生了数百起事故，突显了提高其可靠性和安全性的必要性。基于碰撞前情景分类法对交通事故进行分类，该方法依据车辆动态和运动学特征。在此基础上，特性分析可以识别类似特征下的相似碰撞情况，为更有效地反映一般碰撞模式并提供更具针对性的建议以提升AV性能提供了可能。然而，目前的研究主要集中在传统的人类驾驶车辆上发生的碰撞事件，缺乏专门针对自动驾驶汽车深度碰撞分析的研究内容。本文中，我们分析了最新的加州AV碰撞报告，并使用新修订的预碰撞场景分类法来识别这些碰撞情况下的预碰撞情景类型。我们提出了一套映射规则以自动提取这些AV预碰撞场景，并成功确定了24种不同类型的预碰撞场景（准确率为98.1%），并通过详细的分析获取了两个关键的AV碰撞场景，即追尾场景和交叉口场景。对追尾场景进行关联性分析后发现，显著的环境影响因素包括交通控制类型、地点类型以及光线等；对于可能引发严重事故的交叉口场景，则通过因果关系分析确定出了惯常违反规则与期望特定行为为主要成因。随后，我们制定了优化建议，既考虑了政府监管方面的需求，也针对AV制造商潜在改进进行了探讨。本文的研究成果能够帮助政府部门制定相关法规，并协助制造厂商设计出更有效的测试场景，在各种现实世界情景中识别出控制系统算法的潜在缺陷并加以优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To date, hundreds of crashes have occurred in open road testing of automatedvehicles (AVs), highlighting the need for improving AV reliability and safety.Pre-crash scenario typology classifies crashes based on vehicle dynamics andkinematics features. Building on this, characteristics analysis can identifysimilar features under comparable crashes, offering a more effective reflectionof general crash patterns and providing more targeted recommendations forenhancing AV performance. However, current studies primarily concentrated oncrashes among conventional human-driven vehicles, leaving a gap in researchdedicated to in-depth AV crash analyses. In this paper, we analyzed the latestCalifornia AV collision reports and used the newly revised pre-crash scenariotypology to identify pre-crash scenarios. We proposed a set of mapping rulesfor automatically extracting these AV pre-crash scenarios, successfullyidentifying 24 types with a 98.1% accuracy rate, and obtaining two keyscenarios of AV crashes (i.e., rear-end scenarios and intersection scenarios)through detailed analysis. Association analyses of rear-end scenarios showedthat the significant environmental influencing factors were traffic controltype, location type, light, etc. For intersection scenarios prone to severecrashes with detailed descriptions, we employed causal analyses to obtain thesignificant causal factors: habitual violations and expectations of certainbehavior. Optimization recommendations were then formulated, addressing bothgovernmental oversight and AV manufacturers' potential improvements. Thefindings of this paper could guide government authorities to develop relatedregulations, help manufacturers design AV test scenarios, and identifypotential shortcomings in control algorithms specific to various real-worldscenarios, thereby optimizing AV systems effectively.</description>
      <author>example@mail.com (Yixuan Li, Xuesong Wang, Tianyi Wang, Qian Liu)</author>
      <guid isPermaLink="false">2502.20789v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>CSubBT: A Self-Adjusting Execution Framework for Mobile Manipulation System</title>
      <link>http://arxiv.org/abs/2502.20771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于行为树的条件子树(CSubBT)框架，用于移动机器人在非结构化环境中执行任务时调整执行策略。&lt;h4&gt;背景&lt;/h4&gt;现代智能技术的发展使得装备有机械臂的移动机器人越来越多地应用于非结构化的环境。这些机器人可以根据感知到的信息规划出完成长期任务所需的行动序列，但实际操作中由于感知信息与实际情况不符导致计划失败的情况十分常见。&lt;h4&gt;目的&lt;/h4&gt;为了提高移动机器人的执行成功率和适应性，提出了一种能够自我调整的执行框架CSubBT。&lt;h4&gt;方法&lt;/h4&gt;CSubBT通过将象征性的动作分解为子动作，并使用行为树来控制这些子动作的执行，解决执行过程中的异常问题。该框架认为常见的异常问题是约束条件未满足的问题，在检测到异常时会指导机器人在约束空间内采样新的行动参数。&lt;h4&gt;主要发现&lt;/h4&gt;CSubBT能够有效应对移动机器人的任务执行过程中遇到的各种异常情况，并通过广泛的仿真和现实世界的实验展示了其鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的CSubBT框架为解决移动机械臂操作中的计划与实际情况不匹配的问题提供了一种有效的解决方案，具有较好的实用性和推广价值。&lt;h4&gt;翻译&lt;/h4&gt;随着现代智能技术的进步，配备有机械臂的移动机器人越来越多地在非结构化的环境中运行。这些机器人可以基于感知信息制定长期任务的动作序列规划。然而，在实践中，由于规划所用的感知信息与实际情况不一致，计划经常失败。本文提出了一种基于行为树（BT）的行为子树（CSubBT），这是一种为具有机械臂的任务的移动机器人的自适应执行而设计的一般框架。CSubBT将象征性动作分解成子动作，并使用BT来控制它们的执行，在过程中处理任何可能的异常情况。当检测到异常时，它会通过在约束空间内采样新的操作参数连续指导机器人完成任务。我们通过广泛的模拟和现实世界环境中的机械臂实验展示了该框架的鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the advancements in modern intelligent technologies, mobile robotsequipped with manipulators are increasingly operating in unstructuredenvironments. These robots can plan sequences of actions for long-horizon tasksbased on perceived information. However, in practice, the planned actions oftenfail due to discrepancies between the perceptual information used for planningand the actual conditions. In this paper, we introduce the {\itshapeConditional Subtree} (CSubBT), a general self-adjusting execution framework formobile manipulation tasks based on Behavior Trees (BTs). CSubBT decomposessymbolic action into sub-actions and uses BTs to control their execution,addressing any potential anomalies during the process. CSubBT treats commonanomalies as constraint non-satisfaction problems and continuously guides therobot in performing tasks by sampling new action parameters in the constraintspace when anomalies are detected. We demonstrate the robustness of ourframework through extensive manipulation experiments on different platforms,both in simulation and real-world settings.</description>
      <author>example@mail.com (Huihui Guo, Huizhang Luo, Huilong Pi, Mingxing Duan, Kenli Li, Chubo Liu)</author>
      <guid isPermaLink="false">2502.20771v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>A2DO: Adaptive Anti-Degradation Odometry with Deep Multi-Sensor Fusion for Autonomous Navigation</title>
      <link>http://arxiv.org/abs/2502.20767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6+1pages, 6 figures, accept by ICRA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为A2DO的新系统，该系统通过深度神经网络在挑战性条件下提高了SLAM系统的鲁棒性和定位准确性。&lt;h4&gt;背景&lt;/h4&gt;自主车辆的安全和有效导航需要精确的定位，而同时进行定位与地图构建（SLAM）技术是实现这一目标的关键。然而，在不良天气、光线不足或障碍物等情况下，传感器退化会影响SLAM系统的性能。&lt;h4&gt;目的&lt;/h4&gt;通过结合多传感器数据并利用深度学习技术来提高在各种复杂条件下的自主车辆导航系统精度和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;A2DO是一个端到端的融合了LiDAR与视觉数据的里程计系统，它使用了一个多层次、多尺度特征编码模块，并引入注意力机制以动态减轻传感器退化问题。该模型在广泛覆盖各种退化场景的模拟数据集上进行了预训练，并通过精选的真实世界数据进一步微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，A2DO能够在多种传感器退化条件下保持优越的定位准确性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这项工作展示了一个具备潜在实用价值的方法来提高自主车辆导航系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;精确的定位对于自主车辆的安全和有效导航至关重要。SLAM是这个领域中的关键技术，但在诸如低光照、恶劣天气或传感器退化等不利条件下，其表现会下降。我们提出了一个基于深度神经网络的新系统A2DO，旨在通过融合LiDAR和视觉数据来提升这些情况下的鲁棒性。该系统采用了多层、多尺度特征编码模块并加入了注意力机制以动态解决传感器退化问题，并且在模拟及真实世界的数据集上进行了广泛的训练与微调。实验结果表明，A2DO能够在各种传感器降级条件下保持卓越的定位准确性和稳定性，展示出其实用实施于自主车辆系统中的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate localization is essential for the safe and effective navigation ofautonomous vehicles, and Simultaneous Localization and Mapping (SLAM) is acornerstone technology in this context. However, The performance of the SLAMsystem can deteriorate under challenging conditions such as low light, adverseweather, or obstructions due to sensor degradation. We present A2DO, a novelend-to-end multi-sensor fusion odometry system that enhances robustness inthese scenarios through deep neural networks. A2DO integrates LiDAR and visualdata, employing a multi-layer, multi-scale feature encoding module augmented byan attention mechanism to mitigate sensor degradation dynamically. The systemis pre-trained extensively on simulated datasets covering a broad range ofdegradation scenarios and fine-tuned on a curated set of real-world data,ensuring robust adaptation to complex scenarios. Our experiments demonstratethat A2DO maintains superior localization accuracy and robustness acrossvarious degradation conditions, showcasing its potential for practicalimplementation in autonomous vehicle systems.</description>
      <author>example@mail.com (Hui Lai, Qi Chen, Junping Zhang, Jian Pu)</author>
      <guid isPermaLink="false">2502.20767v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Acquiring Grounded Representations of Words with Situated Interactive Instruction</title>
      <link>http://arxiv.org/abs/2502.20754v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种通过混合主动性、情境化的互动与人类指导者交流来获取单词的接地表示的方法。&lt;h4&gt;背景&lt;/h4&gt;研究集中在从感知知识、语义知识和程序知识中获得多样化类型的知识，并学习它们的接地含义。&lt;h4&gt;目的&lt;/h4&gt;允许代理通过请求关于未知概念的指令，控制其学习过程，从而提高学习效率。&lt;h4&gt;方法&lt;/h4&gt;该方法在Soar系统中实现了，并在一个能够操控小型物体的桌面机器人手臂上进行了测试和评估。&lt;h4&gt;主要发现&lt;/h4&gt;交互式学习使得智能体可以高效地获取不同类型的知识并理解和操作未知概念。&lt;h4&gt;结论&lt;/h4&gt;提出的这种方法有效地提高了代理通过与人类互动来学习复杂知识的能力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种从混合主动性和情境化的互动中，通过与人类指导者的交流获得单词的接地表示的方法。该方法关注于多样化类型的知识获取，包括感知、语义和程序性知识，并且旨在学习这些概念的接地含义。交互式的学习允许智能体控制其学习过程，通过请求有关未知概念的指令，从而使其学习过程更加高效。这种方法在Soar系统中实现，并在一个能够操控小型物体的桌面机器人手臂上进行了评估。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an approach for acquiring grounded representations of words frommixed-initiative, situated interactions with a human instructor. The workfocuses on the acquisition of diverse types of knowledge including perceptual,semantic, and procedural knowledge along with learning grounded meanings.Interactive learning allows the agent to control its learning by requestinginstructions about unknown concepts, making learning efficient. Our approachhas been instantiated in Soar and has been evaluated on a table-top robotic armcapable of manipulating small objects.</description>
      <author>example@mail.com (Shiwali Mohan, Aaron H. Mininger, James R. Kirk, John E. Laird)</author>
      <guid isPermaLink="false">2502.20754v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Indoor Localization for Autonomous Robot Navigation</title>
      <link>http://arxiv.org/abs/2502.20731v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了室内定位系统在自主机器人导航中的应用，研究团队收集了一个数据集并训练模型以测试机器人，并开发了一种A*路径规划算法。经过不同网络结构的测试，机器人的成功转弯率为50%左右。&lt;h4&gt;背景&lt;/h4&gt;随着户外导航技术的发展，在日常生活中的重要性日益增加，室内定位系统（IPS）受到了广泛关注和研究。&lt;h4&gt;目的&lt;/h4&gt;探索利用室内定位系统完成自主机器人在室内的导航任务。&lt;h4&gt;方法&lt;/h4&gt;1. 收集并训练数据集以测试机器人；2. 开发A*路径规划算法使机器人能够使用预测方向自我导航。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验，机器人的成功转弯率为50%左右。&lt;h4&gt;结论&lt;/h4&gt;利用室内定位系统进行自主机器人导航是未来研究的一个有前途的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Indoor positioning systems (IPSs) have gained attention as outdoor navigationbecomes prevalent in everyday life. Research is being actively conducted on howindoor smartphone navigation can be accomplished and improved using receivedsignal strength indication (RSSI) and machine learning (ML). IPSs have more usecases that need further exploration, and we aim to explore using IPSs for theindoor navigation of an autonomous robot. We collected a dataset and trainedmodels to test on a robot. We also developed an A* path-planning algorithm sothat our robot could navigate itself using predicted directions. After testingdifferent network structures, our robot was able to successfully navigatecorners around 50 percent of the time. The findings of this paper indicate thatusing IPSs for autonomous robots is a promising area of future research.</description>
      <author>example@mail.com (Sean Kouma, Rachel Masters)</author>
      <guid isPermaLink="false">2502.20731v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>FSMP: A Frontier-Sampling-Mixed Planner for Fast Autonomous Exploration of Complex and Large 3-D Environments</title>
      <link>http://arxiv.org/abs/2502.20707v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13pages, 12 figures, accepted by IEEE Transactions on Instrumentation  and Measurement&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种使用微型空中飞行器（MAVs）快速探索复杂且庞大的3-D环境的系统框架。&lt;h4&gt;背景&lt;/h4&gt;在复杂和大规模的三维环境中进行有效的探索具有挑战性，传统的基于前沿或随机采样的方法难以实现高效和完整的覆盖。&lt;h4&gt;目的&lt;/h4&gt;为了提高探索效率并确保完整性和可靠性，提出了一种结合了基于前沿的方法和基于采样策略的新型框架。&lt;h4&gt;方法&lt;/h4&gt;{'前端检测器': '设计了一个以视野为基础（FOV）的前沿探测器，该探测器保证完成度和正确性', '确定性采样技术': '采用确定性的采样技术来建立和维护基于记录的传感器视场和新检测到的前沿的增量式道路地图。', '路径规划器': '提出了一个两阶段路径规划算法：第一阶段使用惰性评估策略快速计算全局最优探索路线；第二阶段对最佳探索路径进行平滑处理以进一步提高探索效率。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果验证了该方法在模拟和现实环境中的有效性，展示了其在探索效率、计算时间和已探索体积方面的出色表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的快速探索框架为复杂3-D环境下的MAV应用提供了一种高效且可靠的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们提出了一种使用微型空中飞行器（MAVs）的系统性框架，用于快速探索复杂的大型三维环境。该方法的核心见解在于有机地整合基于前沿和采样策略的方法，以实现环境的整体快速探索。设计了一个基于视野（FOV）的前端检测器，确保了完整性和正确性的前提下识别三维地图边界。与随机采样法不同的是，采用确定性采样技术来建立并维护一种增量式道路图，该图依赖于记录下来的传感器视场以及新发现的前沿。利用所构建的道路图，提出了一种两阶段路径规划算法：首阶段快速计算出全局最优探索路线；次阶段则进一步平滑优化此最佳路线以提高效率。文中通过仿真和现实世界中的实验验证了这一方法的有效性，并且比较结果表明该框架在探索效率、计算时间和已探索体积方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a systematic framework for fast exploration ofcomplex and large 3-D environments using micro aerial vehicles (MAVs). The keyinsight is the organic integration of the frontier-based and sampling-basedstrategies that can achieve rapid global exploration of the environment.Specifically, a field-of-view-based (FOV) frontier detector with the guaranteeof completeness and soundness is devised for identifying 3-D map frontiers.Different from random sampling-based methods, the deterministic samplingtechnique is employed to build and maintain an incremental road map based onthe recorded sensor FOVs and newly detected frontiers. With the resulting roadmap, we propose a two-stage path planner. First, it quickly computes the globaloptimal exploration path on the road map using the lazy evaluation strategy.Then, the best exploration path is smoothed for further improving theexploration efficiency. We validate the proposed method both in simulation andreal-world experiments. The comparative results demonstrate the promisingperformance of our planner in terms of exploration efficiency, computationaltime, and explored volume.</description>
      <author>example@mail.com (Shiyong Zhang, Xuebo Zhang, Qianli Dong, Ziyu Wang, Haobo Xi, Jing Yuan)</author>
      <guid isPermaLink="false">2502.20707v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>WorldModelBench: Judging Video Generation Models As World Models</title>
      <link>http://arxiv.org/abs/2502.20694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;视频生成模型在快速发展，能够支持决策应用如机器人和自动驾驶。然而现有的基准测试未能严格评估这些能力。&lt;h4&gt;背景&lt;/h4&gt;当前的基准测试只关注通用视频质量，忽略了世界建模所需的重要因素，例如物理规则遵守情况。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一差距，我们提出了WorldModelBench，一个旨在评价视频生成模型在应用驱动领域中世界建模能力的基准。&lt;h4&gt;方法&lt;/h4&gt;{'优势1': '通过引入指令跟随和物理规则遵守维度，WorldModelBench能检测细微的违规情况，如违反质量守恒定律的对象尺寸不规则变化的问题，这些问题被之前的基准所忽视。', '优势2': '通过大规模的人类偏好对齐，我们收集了67K人类标签来准确测量14个前沿模型。使用高质量的人类标签进一步微调了一个精确的评判者以自动化评估过程，并实现了比GPT-4o更高的预测世界建模违规平均精度。', '其他贡献': '训练模型与人工注释对齐，最大化来自评判者的奖励可以显著提高世界的建模能力'}&lt;h4&gt;主要发现&lt;/h4&gt;通过引入特定维度来检测细微问题和大规模的人类偏好对齐方法，WorldModelBench能更全面地评估视频生成模型的世界建模能力。&lt;h4&gt;结论&lt;/h4&gt;我们的研究强调了现有基准测试的局限性，并提出了一种改进的方法来准确评价这些模型的能力。该网站可以访问https://worldmodelbench-team.github.io&lt;h4&gt;翻译&lt;/h4&gt;视频生成模型正在迅速发展，将自己定位为支持决策应用（如机器人技术与自动驾驶）的世界模型。然而，当前的评估基准未能严格验证这些声明，只关注通用视频质量，忽略世界建模所需的重要因素，比如物理一致性等。为解决这一问题，我们提出了WorldModelBench，一个旨在测试视频生成模型在应用驱动领域的世界建模能力的新标准。该基准的主要优势在于：1）能够检测细微的世界建模违规情况，如违反了质量守恒定律的对象尺寸不规则变化；2）通过大规模的人类偏好对齐方法进行准确评估，利用67K人类标签来衡量多个前沿模型，并微调评判者以自动化这一过程。结果表明，该方法比GPT-4o更为精确地预测世界建模违规情况。此外，我们展示了训练与人工注释对齐可以显著提高世界的建模能力。有关WorldModelBench的更多信息，请访问https://worldmodelbench-team.github.io&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation models have rapidly progressed, positioning themselves asvideo world models capable of supporting decision-making applications likerobotics and autonomous driving. However, current benchmarks fail to rigorouslyevaluate these claims, focusing only on general video quality, ignoringimportant factors to world models such as physics adherence. To bridge thisgap, we propose WorldModelBench, a benchmark designed to evaluate the worldmodeling capabilities of video generation models in application-driven domains.WorldModelBench offers two key advantages: (1) Against to nuanced worldmodeling violations: By incorporating instruction-following andphysics-adherence dimensions, WorldModelBench detects subtle violations, suchas irregular changes in object size that breach the mass conservation law -issues overlooked by prior benchmarks. (2) Aligned with large-scale humanpreferences: We crowd-source 67K human labels to accurately measure 14 frontiermodels. Using our high-quality human labels, we further fine-tune an accuratejudger to automate the evaluation procedure, achieving 8.6% higher averageaccuracy in predicting world modeling violations than GPT-4o with 2Bparameters. In addition, we demonstrate that training to align humanannotations by maximizing the rewards from the judger noticeably improve theworld modeling capability. The website is available athttps://worldmodelbench-team.github.io.</description>
      <author>example@mail.com (Dacheng Li, Yunhao Fang, Yukang Chen, Shuo Yang, Shiyi Cao, Justin Wong, Michael Luo, Xiaolong Wang, Hongxu Yin, Joseph E. Gonzalez, Ion Stoica, Song Han, Yao Lu)</author>
      <guid isPermaLink="false">2502.20694v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>From Safety Standards to Safe Operation with Mobile Robotic Systems Deployment</title>
      <link>http://arxiv.org/abs/2502.20693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper published at "Workshop on Design, Learning, and control for  safe human-robot collaboration at the International Conference on Advanced  Robotics (ICAR)"&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文审查了移动机器人在工作场所部署的安全标准和方法，并提出了一个新的风险评估框架来确保建筑工地上的安全使用。&lt;h4&gt;背景&lt;/h4&gt;移动机器人被广泛应用于各种工作环境以提高生产效率，但在拥挤且有危险的环境中与人类工人互动存在安全挑战。&lt;h4&gt;目的&lt;/h4&gt;为解决移动机器人在施工场地部署时的安全问题，提出一套全面的风险评估方法，并通过领域专家验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;首先回顾了现有标准和相关研究文献，然后基于这些信息提出了一个改进的风险评估框架以覆盖未被现有的安全性指南涵盖的情景。&lt;h4&gt;主要发现&lt;/h4&gt;新的风险评估框架可以更好地保护工人免受移动机器人操作的潜在危害，并提供了具体的建议来降低此类风险。&lt;h4&gt;结论&lt;/h4&gt;通过扩展现有安全标准并提出额外的安全措施，该研究有助于更安全地部署和使用移动机器人在建筑工地和其他复杂环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mobile robotic systems are increasingly used in various work environments tosupport productivity. However, deploying robots in workplaces crowded by humanworkers and interacting with them results in safety challenges and concerns,namely robot-worker collisions and worker distractions in hazardousenvironments. Moreover, the literature on risk assessment as well as thestandard specific to mobile platforms is rather limited. In this context, thispaper first conducts a review of the relevant standards and methodologies andthen proposes a risk assessment for the safe deployment of mobile robots onconstruction sites. The approach extends relevant existing safety standards toencompass uncovered scenarios. Safety recommendations are made based on theframework, after its validation by field experts.</description>
      <author>example@mail.com (Bruno Belzile, Tatiana Wanang-Siyapdjie, Sina Karimi, Rafael Gomes Braga, Ivanka Iordanova, David St-Onge)</author>
      <guid isPermaLink="false">2502.20693v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>The Common Objects Underwater (COU) Dataset for Robust Underwater Object Detection</title>
      <link>http://arxiv.org/abs/2502.20651v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了COU数据集，一个包含常见人造物体实例分割图像的水下图像库。&lt;h4&gt;背景&lt;/h4&gt;在水下环境中缺乏用于实例分割的高质量和多样化的数据集。现有的数据集主要关注海洋生物，而忽视了其他类型的水下物体。&lt;h4&gt;目的&lt;/h4&gt;创建一个新的数据集COU，涵盖多种不同环境下的常见人造物品，以促进轻量级实时检测器的研发，特别是针对自主式水下航行器（AUV）的训练需求。&lt;h4&gt;方法&lt;/h4&gt;从多个地点的机器人实地试验中收集图像，并对这些图像进行了详细的实例分割标注。使用三种最先进的模型来评估COU数据集在训练水下目标检测器方面的性能和准确性。&lt;h4&gt;主要发现&lt;/h4&gt;相比于仅基于地面数据训练的目标检测器，使用COU进行训练显著提高了检测器的表现和精确度。&lt;h4&gt;结论&lt;/h4&gt;COU是一个多样化的、高质量的数据集，专门用于改进自主式水下航行器的实时对象检测能力。该数据集将对研究界开放，并采用开源许可证提供。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个名为COU（水下的常见物体）的数据集，这是一个包含多种水生和海洋环境中常见的人造物体实例分割图像的数据集。COU包含了大约10,000张标注过的分割图像，这些图像是从不同地点进行的多次水下机器人实地试验中收集而来的。该数据集旨在解决缺乏用于水下实例分割的稳健分类覆盖的问题，这对于训练轻量级、实时能力强大的自主式水下航行器（AUV）检测器特别有用。此外，COU解决了对象类别多样性的不足问题，因为常见的水下图像数据集仅关注海洋生物。目前，COU包含了来自封闭水域（泳池）和开放水域（湖泊和海洋）环境的24种不同类别的物体图像，包括海洋垃圾、潜水工具以及AUV等。为了评估COU在训练水下目标检测器方面的效果，我们使用三种最先进的模型来评估其性能和准确性，采用了标准准确率和效率指标相结合的方法。COU训练过的检测器相较于仅基于地面数据训练的检测器表现出明显的优势。我们将在开源许可下提供COU供广泛使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce COU: Common Objects Underwater, an instance-segmented imagedataset of commonly found man-made objects in multiple aquatic and marineenvironments. COU contains approximately 10K segmented images, annotated fromimages collected during a number of underwater robot field trials in diverselocations. COU has been created to address the lack of datasets with robustclass coverage curated for underwater instance segmentation, which isparticularly useful for training light-weight, real-time capable detectors forAutonomous Underwater Vehicles (AUVs). In addition, COU addresses the lack ofdiversity in object classes since the commonly available underwater imagedatasets focus only on marine life. Currently, COU contains images from bothclosed-water (pool) and open-water (lakes and oceans) environments, of 24different classes of objects including marine debris, dive tools, and AUVs. Toassess the efficacy of COU in training underwater object detectors, we usethree state-of-the-art models to evaluate its performance and accuracy, using acombination of standard accuracy and efficiency metrics. The improvedperformance of COU-trained detectors over those solely trained on terrestrialdata demonstrates the clear advantage of training with annotated underwaterimages. We make COU available for broad use under open-source licenses.</description>
      <author>example@mail.com (Rishi Mukherjee, Sakshi Singh, Jack McWilliams, Junaed Sattar)</author>
      <guid isPermaLink="false">2502.20651v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>EDENet: Echo Direction Encoding Network for Place Recognition Based on Ground Penetrating Radar</title>
      <link>http://arxiv.org/abs/2502.20643v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了基于地表穿透雷达(GPR)的定位技术在机器人领域中的应用，并提出了一种新的网络架构EDENet来解决大规模地图中位置识别的问题。&lt;h4&gt;背景&lt;/h4&gt;GPR由于能够探测稳定的地下特征，在机器人领域得到了广泛应用。然而，现有的方法主要集中在小规模的位置识别上，忽略了大规模地图中所面临的挑战。&lt;h4&gt;目的&lt;/h4&gt;研究GPR回波序列与地下场景之间的几何关系，并提出一种新的网络设计来应对大尺度位置识别的难题。&lt;h4&gt;方法&lt;/h4&gt;引入可学习的Gabor滤波器以精确提取方向响应，并结合方向感知注意力机制进行有效的几何编码。还使用了移不变单元和多尺度聚合策略，提高了对介电常数变化的适应性。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的EDENet在公共数据集上的实验表明，它不仅在位置识别性能上超越了现有解决方案，还在模型大小和计算效率方面具有优势。&lt;h4&gt;结论&lt;/h4&gt;通过提出创新的方法和技术，有效解决了大规模地图中基于GPR的位置识别问题，并展示了其优越的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-28&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ground penetrating radar (GPR) based localization has gained significantrecognition in robotics due to its ability to detect stable subsurfacefeatures, offering advantages in environments where traditional sensors likecameras and LiDAR may struggle. However, existing methods are primarily focusedon small-scale place recognition (PR), leaving the challenges of PR inlarge-scale maps unaddressed. These challenges include the inherent sparsity ofunderground features and the variability in underground dielectric constants,which complicate robust localization. In this work, we investigate thegeometric relationship between GPR echo sequences and underground scenes,leveraging the robustness of directional features to inform our network design.We introduce learnable Gabor filters for the precise extraction of directionalresponses, coupled with a direction-aware attention mechanism for effectivegeometric encoding. To further enhance performance, we incorporate ashift-invariant unit and a multi-scale aggregation strategy to betteraccommodate variations in di-electric constants. Experiments conducted onpublic datasets demonstrate that our proposed EDENet not only surpassesexisting solutions in terms of PR performance but also offers advantages inmodel size and computational efficiency.</description>
      <author>example@mail.com (Pengyu Zhang, Xieyuanli Chen, Yuwei Chen, Beizhen Bi, Zhuo Xu, Tian Jin, Xiaotao Huang, Liang Shen)</author>
      <guid isPermaLink="false">2502.20643v1</guid>
      <pubDate>Mon, 03 Mar 2025 17:08:25 +0800</pubDate>
    </item>
    <item>
      <title>Tuning Algorithmic and Architectural Hyperparameters in Graph-Based Semi-Supervised Learning with Provable Guarantees</title>
      <link>http://arxiv.org/abs/2502.12937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  31 pages (11 pages main body), 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文研究了图半监督学习中基于参数化算法家族的超参数调优问题，特别是针对经典标签传播算法家族以及现代图卷积神经网络（GCN）和简化图卷积（SGC）网络。论文提供了关于选择最优超参数所需的理论数据量的新上界，并为不同的图神经网络架构提供了Rademacher复杂度界。&lt;h4&gt;背景&lt;/h4&gt;图半监督学习在机器学习领域具有重要地位，它通过建模未标记数据与已标记数据之间的关系来利用隐含的图形结构。许多经典算法和现代深度学习方法被提出用于解决这一问题，这些算法通常含有可调超参数。&lt;h4&gt;目的&lt;/h4&gt;本文旨在探讨如何从一系列参数化算法家族中调整适合图半监督学习任务的超参数，并研究其理论上的复杂性。&lt;h4&gt;方法&lt;/h4&gt;对于三个经典的基于标签传播的算法系列，论文获取了新的伪维度上界和匹配的下界，这些边界与节点数量n呈对数关系。此外，还考虑现代简化图卷积网络中的自环权重调优问题以及GCN和GAT之间的可调节架构选择问题。&lt;h4&gt;主要发现&lt;/h4&gt;对于三个基于标签传播的经典算法系列，在确定最优超参数时所需的训练数据量方面获得了对数级的理论边界；提出了一种可以同时包含GCN和GAT特征的新图神经网络架构，并给出了针对此架构调优参数时的学习复杂性分析。&lt;h4&gt;结论&lt;/h4&gt;本文通过形式化研究为调整图半监督学习算法中的超参数提供了有效的理论指导，这对未来设计高效的机器学习模型具有重要价值。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-based semi-supervised learning is a powerful paradigm in machinelearning for modeling and exploiting the underlying graph structure thatcaptures the relationship between labeled and unlabeled data. A large number ofclassical as well as modern deep learning based algorithms have been proposedfor this problem, often having tunable hyperparameters. We initiate a formalstudy of tuning algorithm hyperparameters from parameterized algorithm familiesfor this problem. We obtain novel $O(\log n)$ pseudo-dimension upper bounds forhyperparameter selection in three classical label propagation-based algorithmfamilies, where $n$ is the number of nodes, implying bounds on the amount ofdata needed for learning provably good parameters. We further provide matching$\Omega(\log n)$ pseudo-dimension lower bounds, thus asymptoticallycharacterizing the learning-theoretic complexity of the parameter tuningproblem. We extend our study to selecting architectural hyperparameters inmodern graph neural networks. We bound the Rademacher complexity for tuning theself-loop weighting in recently proposed Simplified Graph Convolution (SGC)networks. We further propose a tunable architecture that interpolates graphconvolutional neural networks (GCN) and graph attention networks (GAT) in everylayer, and provide Rademacher complexity bounds for tuning the interpolationcoefficient.</description>
      <author>example@mail.com (Ally Yalei Du, Eric Huang, Dravyansh Sharma)</author>
      <guid isPermaLink="false">2502.12937v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
  <item>
      <title>SCA3D: Enhancing Cross-modal 3D Retrieval via 3D Shape and Caption Paired Data Augmentation</title>
      <link>http://arxiv.org/abs/2502.19128v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的3D形状和描述在线数据增强方法SCA3D，用于跨模式的3D检索任务。通过使用LLaVA模型生成更多的文本-3D对来解决现有方法因缺乏多样化的3D数据而导致的表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;跨模态3D检索旨在实现自然语言与3D形状之间的互匹配，但现有的方法由于缺乏高质量的3D数据而表现出较差的泛化能力。&lt;h4&gt;目的&lt;/h4&gt;通过生成更多的3D-文本对来改善现有3D检索方法的表现，并增强其在多样的场景中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;使用LLaVA模型为每个分割后的3D形状的部分生成描述，产生新的3D-文本配对；采用内部和外部距离将各组件组合成新3D形状；利用模板处理部件的描述并创建新文字描述；用单模态编码器提取改进数据集上的嵌入向量，并通过对比学习增强跨模式匹配。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的SCA3D方法在Text2Shape数据集中表现优越，相较于之前的工作显著提高了形状到文本和文本到形状的检索精度。&lt;h4&gt;结论&lt;/h4&gt;提出的方法证明了通过生成更多高质量的数据可以有效提升跨模态3D检索的表现，并且代码开源可供进一步研究使用。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容为：跨模态3D检索任务旨在实现自然语言描述与3D形状之间的相互匹配，这有可能增强自然语言和三维环境间的互动，在机器人技术和具身人工智能（AI）应用领域尤其如此。然而，由于缺乏高质量的3D数据，现有方法的表现受限，并且依赖于有限数量的3D形状特征导致在不同场景下的泛化能力较差。为解决这一挑战，我们引入了SCA3D，这是一种新的用于跨模态3D检索的数据增强方法，通过LLaVA模型创建一个组件库，为数据集中的每个3D形状的部分生成描述。这种方法不仅促进了包含新语义特征的大量新的3D-文本对的生成，还采用内外距离来将各组件组合成新的3D形状，并利用模板处理各个组件的描述以产生新的文字描述。此外，我们使用单模态编码器从增强的数据集中提取基于3D形状和文本的嵌入向量，并通过地球移动者距离（EMD）计算细粒度跨模式相似性并利用对比学习提高跨模式匹配能力，实现在文本与3D形状间的双向检索。广泛的实验表明，我们的SCA3D方法在Text2Shape数据集中超越了先前的工作，在形状到文本的RR@1评分从20.03提升至27.22和文本到形状的RR@1评分从13.12提升至16.67。相关代码可在https://github.com/3DAgentWorld/SCA3D中找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The cross-modal 3D retrieval task aims to achieve mutual matching betweentext descriptions and 3D shapes. This has the potential to enhance theinteraction between natural language and the 3D environment, especially withinthe realms of robotics and embodied artificial intelligence (AI) applications.However, the scarcity and expensiveness of 3D data constrain the performance ofexisting cross-modal 3D retrieval methods. These methods heavily rely onfeatures derived from the limited number of 3D shapes, resulting in poorgeneralization ability across diverse scenarios. To address this challenge, weintroduce SCA3D, a novel 3D shape and caption online data augmentation methodfor cross-modal 3D retrieval. Our approach uses the LLaVA model to create acomponent library, captioning each segmented part of every 3D shape within thedataset. Notably, it facilitates the generation of extensive new 3D-text pairscontaining new semantic features. We employ both inter and intra distances toalign various components into a new 3D shape, ensuring that the components donot overlap and are closely fitted. Further, text templates are utilized toprocess the captions of each component and generate new text descriptions.Besides, we use unimodal encoders to extract embeddings for 3D shapes and textsbased on the enriched dataset. We then calculate fine-grained cross-modalsimilarity using Earth Mover's Distance (EMD) and enhance cross-modal matchingwith contrastive learning, enabling bidirectional retrieval between texts and3D shapes. Extensive experiments show our SCA3D outperforms previous works onthe Text2Shape dataset, raising the Shape-to-Text RR@1 score from 20.03 to27.22 and the Text-to-Shape RR@1 score from 13.12 to 16.67. Codes can be foundin https://github.com/3DAgentWorld/SCA3D.</description>
      <author>example@mail.com (Junlong Ren, Hao Wu, Hui Xiong, Hao Wang)</author>
      <guid isPermaLink="false">2502.19128v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Increasing the Task Flexibility of Heavy-Duty Manipulators Using Visual 6D Pose Estimation of Objects</title>
      <link>http://arxiv.org/abs/2502.19169v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种管道，用于通过高级机器视觉对重型长距离机械臂的工具进行精确定位。该方法结合了深度神经网络和基于运动的摄像机与机器人校准。&lt;h4&gt;背景&lt;/h4&gt;近年来，使用深层神经网络在物体6D姿态估计方面的进展推动了基于视觉控制的新方式，尤其是在重型机器人应用中。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的管道系统，利用机器视觉技术实现对重型长距离机械臂工具的精确定位。&lt;h4&gt;方法&lt;/h4&gt;{'相机配置': '采用眼在手（eye-in-hand）的摄像机配置来直接估计感兴趣物体和工件的姿态', '姿态误差计算': '根据工具与目标之间的姿态误差进行精密定位，同时通过基于运动的校准完成摄像机到机器人的校准。', '训练数据': '仅使用合成数据训练用于估计对象兴趣（OOI）姿态的深度神经网络'}&lt;h4&gt;主要发现&lt;/h4&gt;{'精度': '实验结果表明，在非深度轴上实现了小于2毫米的平均图像定位误差，这有助于提高柔性重型长距离机械臂的任务灵活性和自动化水平。', '准确性提升': '通过基于图像算法避免了由于结构柔性的刚体运动学而引起的不准确度'}&lt;h4&gt;结论&lt;/h4&gt;所提出的方法提供了一种新颖的方式，以增加非刚性重型长距离机器人的任务灵活度与自动化程度，并且已经在现实世界中进行了验证。&lt;h4&gt;翻译&lt;/h4&gt;最近在利用深层神经网络进行物体6D姿态估计方面的进展促进了基于视觉的控制方法的发展，尤其是在重型机器人应用方面。在这项研究中，我们提出了一种使用高级机器视觉技术来实现重型长距离机械臂工具精确定位的新管道系统。该方案采用眼在手配置的相机直接估算工件和目标对象的姿态，并根据姿态误差以及摄像机与机器人之间的运动校准，通过常规工业上广泛使用的机器人建模和控制方法实现可靠的精密定位。所提出的方法包括基于视觉估计OOI姿态的位置对齐，而摄像机到机器人的校准则利用基于动作的视觉SLAM进行。这些技术试图通过图像基算法避免由于结构柔性的刚体运动学引起的不准确性。为了训练用于目标对象姿态估计的深度神经网络，仅使用合成数据。该方法在具有5米伸展范围的实际重型长距离机械臂上进行了验证。实验结果表明，在非深度轴上实现了低于2毫米的平均工具定位误差，这有助于增加柔性重型长距离机械臂的任务灵活性和自动化水平的一种新方式。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in visual 6D pose estimation of objects using deep neuralnetworks have enabled novel ways of vision-based control for heavy-duty roboticapplications. In this study, we present a pipeline for the precise toolpositioning of heavy-duty, long-reach (HDLR) manipulators using advancedmachine vision. A camera is utilized in the so-called eye-in-hand configurationto estimate directly the poses of a tool and a target object of interest (OOI).Based on the pose error between the tool and the target, along withmotion-based calibration between the camera and the robot, precise toolpositioning can be reliably achieved using conventional robotic modeling andcontrol methods prevalent in the industry. The proposed methodology comprisesorientation and position alignment based on the visually estimated OOI poses,whereas camera-to-robot calibration is conducted based on motion utilizingvisual SLAM. The methods seek to avert the inaccuracies resulting fromrigid-body--based kinematics of structurally flexible HDLR manipulators viaimage-based algorithms. To train deep neural networks for OOI pose estimation,only synthetic data are utilized. The methods are validated in a real-worldsetting using an HDLR manipulator with a 5 m reach. The experimental resultsdemonstrate that an image-based average tool positioning error of less than 2mm along the non-depth axes is achieved, which facilitates a new way toincrease the task flexibility and automation level of non-rigid HDLRmanipulators.</description>
      <author>example@mail.com (Petri Mäkinen, Pauli Mustalahti, Tuomo Kivelä, Jouni Mattila)</author>
      <guid isPermaLink="false">2502.19169v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Online Meta-learning for AutoML in Real-time (OnMAR)</title>
      <link>http://arxiv.org/abs/2502.20279v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First page is a graphical abstract, this is a journal article  submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个在线元学习为实时自动机器学习（AutoML）的方法，即OnMAR。该方法利用元学习收集关于机器学习算法优化过程的信息，并使用元学习器预测模型设计的准确性以改进实时AutoML的设计质量与速度。&lt;h4&gt;背景&lt;/h4&gt;自动化机器学习(AutoML)是一个研究领域，旨在通过优化技术设计机器学习(ML)算法，减少人工干预的需求。实时AutoML允许在实际应用任务中进行设计过程。现有方法在质量和时间效率方面有待改善。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的在线元学习方法来提高实时AutoML的设计质量和速度。&lt;h4&gt;方法&lt;/h4&gt;使用了一种称为OnMAR的方法，通过遗传算法和不同的元学习器（k近邻、随机森林和XGBoost）进行模型预测。这种方法适用于多个不同场景下的实时AutoML应用，并进行了相应的测试。&lt;h4&gt;主要发现&lt;/h4&gt;OnMAR方法在三个不同的应用场景下展现了良好的效果，可以匹配甚至超越现有的实时AutoML方法，在设计时间和准确性方面都有显著优势。&lt;h4&gt;结论&lt;/h4&gt;在线元学习（OnMAR）是解决实时AutoML中模型优化问题的有效途径。它可以提升现有技术的性能，并且具有快速运行时间的优势。&lt;h4&gt;翻译&lt;/h4&gt;自动化机器学习(AutoML)是一个研究领域，专注于使用优化技术设计机器学习(ML)算法，以减轻人为手动设计算法的需求。实时AutoML使设计过程能够在应用到任务的同时进行。作为新兴的研究领域，现有的实时AutoML技术在设计质量和所需时间方面需要改进。为了解决这些问题，本研究提出了一种在线元学习用于实时AutoML的方法（OnMAR）。元学习收集了由机器学习算法在其优化过程中产生的元特征信息。这些元特征与一个元学习器结合使用以优化该过程。OnMAR方法采用元学习器来预测一个ML设计的准确性；如果预测准确度足够高，则接受此设计，反之则通过优化技术创建新的设计。作为OnMAR的一部分使用的优化技术是遗传算法(GA)。测试了不同的元学习器（k近邻、随机森林和XGBoost）。由于该方法与模型无关(即不特定于单个实时AutoML应用)，因此在三个不同的实时AutoML应用场景中进行了评估，包括：组成图像聚类算法、配置卷积神经网络的超参数以及设置视频分类管道。OnMAR方法是有效的，在性能上可以匹配或超越现有的实时AutoML方法，并具有更快运行时间的优点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated machine learning (AutoML) is a research area focusing on usingoptimisation techniques to design machine learning (ML) algorithms, alleviatingthe need for a human to perform manual algorithm design. Real-time AutoMLenables the design process to happen while the ML algorithm is being applied toa task. Real-time AutoML is an emerging research area, as such existingreal-time AutoML techniques need improvement with respect to the quality ofdesigns and time taken to create designs. To address these issues, this studyproposes an Online Meta-learning for AutoML in Real-time (OnMAR) approach.Meta-learning gathers information about the optimisation process undertaken bythe ML algorithm in the form of meta-features. Meta-features are used inconjunction with a meta-learner to optimise the optimisation process. The OnMARapproach uses a meta-learner to predict the accuracy of an ML design. If theaccuracy predicted by the meta-learner is sufficient, the design is used, andif the predicted accuracy is low, an optimisation technique creates a newdesign. A genetic algorithm (GA) is the optimisation technique used as part ofthe OnMAR approach. Different meta-learners (k-nearest neighbours, randomforest and XGBoost) are tested. The OnMAR approach is model-agnostic (i.e. notspecific to a single real-time AutoML application) and therefore evaluated onthree different real-time AutoML applications, namely: composing an imageclustering algorithm, configuring the hyper-parameters of a convolutionalneural network, and configuring a video classification pipeline. The OnMARapproach is effective, matching or outperforming existing real-time AutoMLapproaches, with the added benefit of a faster runtime.</description>
      <author>example@mail.com (Mia Gerber, Anna Sergeevna Bosman, Johan Pieter de Villiers)</author>
      <guid isPermaLink="false">2502.20279v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Scale Neighborhood Occupancy Masked Autoencoder for Self-Supervised Learning in LiDAR Point Clouds</title>
      <link>http://arxiv.org/abs/2502.20316v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的点云自监督学习方法NOMAE，通过只在非屏蔽体素的邻域内进行被遮罩占用重建来克服现有MAE在3D视觉中的挑战。&lt;h4&gt;背景&lt;/h4&gt;Masked autoencoders (MAE) 在图像等领域的自我监督学习中展现出巨大潜力。然而，在自动驾驶领域使用的LiDAR点云数据面临着特殊的挑战，因为大量空旷区域的存在导致了信息泄露和计算复杂度的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服现有的3D MAE的局限性，特别是针对大型、复杂的点云场景中的问题进行优化。&lt;h4&gt;方法&lt;/h4&gt;引入了基于体素屏蔽和层级生成技术的方法NOMAE。该方法在非屏蔽体素的邻域内执行被遮罩占用重建，并且可以在不同的尺度上集成多级体素屏蔽与占用重建，以捕捉不同大小的对象特征。&lt;h4&gt;主要发现&lt;/h4&gt;NOMAE可以灵活地应用于现有的3D架构中进行自监督学习，并且在nuScenes和Waymo Open数据集上的下游感知任务（如语义分割和3D目标检测）上显示出优越的性能，超过了现有方法的表现。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了NOMAE的有效性，它不仅解决了MAE在处理点云数据时遇到的问题，并且在多个基准测试中实现了当前最佳的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;被遮罩自编码器（MAE）已经在视觉等领域中的自我监督学习中展现出巨大潜力。然而，在自动驾驶领域使用的LiDAR采集到的3D点云数据特别具有挑战性，因为这些数据中有大量的空旷区域是空白的。因此，现有的工作在解码过程中泄露了占用信息，并且存在较大的计算复杂度，这使得自监督预训练仅限于2D鸟瞰视图编码器的实际应用中。本文提出了一种新颖的方法——邻域占用MAE（NOMAE），通过只在非屏蔽体素的邻域内进行被遮罩占用重建来克服上述挑战。我们利用所提出的层级掩码生成技术，在多个尺度上集成体素屏蔽与占用重建，以捕捉不同大小的对象特征。NOMAE具有极高的灵活性，并可以直接用于现有3D架构中的自监督学习。我们在nuScenes和Waymo Open数据集上的下游感知任务（如语义分割和3D目标检测）中进行了广泛的评估，与判别性和生成性自我监督方法进行比较。实验结果表明，NOMAE在多个基准测试的点云感知任务上设定了新的最佳性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked autoencoders (MAE) have shown tremendous potential for self-supervisedlearning (SSL) in vision and beyond. However, point clouds from LiDARs used inautomated driving are particularly challenging for MAEs since large areas ofthe 3D volume are empty. Consequently, existing work suffers from leakingoccupancy information into the decoder and has significant computationalcomplexity, thereby limiting the SSL pre-training to only 2D bird's eye viewencoders in practice. In this work, we propose the novel neighborhood occupancyMAE (NOMAE) that overcomes the aforementioned challenges by employing maskedoccupancy reconstruction only in the neighborhood of non-masked voxels. Weincorporate voxel masking and occupancy reconstruction at multiple scales withour proposed hierarchical mask generation technique to capture features ofobjects of different sizes in the point cloud. NOMAEs are extremely flexibleand can be directly employed for SSL in existing 3D architectures. We performextensive evaluations on the nuScenes and Waymo Open datasets for thedownstream perception tasks of semantic segmentation and 3D object detection,comparing with both discriminative and generative SSL methods. The resultsdemonstrate that NOMAE sets the new state-of-the-art on multiple benchmarks formultiple point cloud perception tasks.</description>
      <author>example@mail.com (Mohamed Abdelsamad, Michael Ulrich, Claudius Gläser, Abhinav Valada)</author>
      <guid isPermaLink="false">2502.20316v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>GenPC: Zero-shot Point Cloud Completion via 3D Generative Priors</title>
      <link>http://arxiv.org/abs/2502.19896v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;现有的点云补全方法在处理现实世界扫描数据时遇到了挑战，因为它们依赖于预定义的合成训练数据集。为此，我们提出了一种名为GenPC的零样本完成框架，该框架利用明确的3D生成先验来重构高质量的真实世界扫描。&lt;h4&gt;背景&lt;/h4&gt;现有点云补全方法通常依赖于预先定义的合成训练数据集，在处理分布外的真实世界扫描时遇到困难。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的零样本点云补全框架，以克服现有方法在真实场景中的局限性。&lt;h4&gt;方法&lt;/h4&gt;开发了Depth Prompting模块和Geometric Preserving Fusion模块，前者通过深度图像将部分点云与单视图生成模型关联起来；后者确保最终结果保留原始的不完整结构。&lt;h4&gt;主要发现&lt;/h4&gt;基于互联网规模数据训练的最新前馈3D生成模型能够从单一视角图像中进行零样本设置下的3D生成。GenPC框架在常用基准测试上的大量实验验证了其优越性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过利用现有的大规模3D生成模型，我们提出了一个高效的零样本点云补全解决方案，这使我们在稳健的真实世界扫描完成上更进一步。&lt;h4&gt;翻译&lt;/h4&gt;现有点云补全方法依赖于预先定义的合成训练数据集，在处理真实世界的分布外扫描时面临重大挑战。为了解决这一限制，我们提出了一种利用明确3D生成先验来重构高质量真实世界扫描的零样本补全框架GenPC。该框架的关键见解是基于近期前馈3D生成模型能够在仅使用单一视角图像的情况下从大规模互联网数据中进行零样本设置下的3D生成。为将此能力应用于补全任务，我们开发了一种Depth Prompting模块和一种Geometric Preserving Fusion模块。前者通过深度图作为中间步骤连接部分点云与单视图到三维的生成模型；后者则确保最终结果保留了原始输入中的不完整结构。大量实验验证了GenPC框架在常用基准测试上的优越性和泛化能力，使我们更接近于稳健的真实世界扫描完成目标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing point cloud completion methods, which typically depend on predefinedsynthetic training datasets, encounter significant challenges when applied toout-of-distribution, real-world scans. To overcome this limitation, weintroduce a zero-shot completion framework, termed GenPC, designed toreconstruct high-quality real-world scans by leveraging explicit 3D generativepriors. Our key insight is that recent feed-forward 3D generative models,trained on extensive internet-scale data, have demonstrated the ability toperform 3D generation from single-view images in a zero-shot setting. Toharness this for completion, we first develop a Depth Prompting module thatlinks partial point clouds with image-to-3D generative models by leveragingdepth images as a stepping stone. To retain the original partial structure inthe final results, we design the Geometric Preserving Fusion module that alignsthe generated shape with input by adaptively adjusting its pose and scale.Extensive experiments on widely used benchmarks validate the superiority andgeneralizability of our approach, bringing us a step closer to robustreal-world scan completion.</description>
      <author>example@mail.com (An Li, Zhe Zhu, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.19896v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Sanity Checking Causal Representation Learning on a Simple Real-World System</title>
      <link>http://arxiv.org/abs/2502.20099v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 12 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文评估了因果表示学习（CRL）在简单现实世界系统中的效果，并通过光学实验验证方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;因果表示学习是近年来机器学习领域的热点问题，旨在从数据中提取出具有因果关系的特征。然而，现有理论与实际应用之间存在差距，需要进行更深入的研究以解决这些问题。&lt;h4&gt;目的&lt;/h4&gt;评估现有的因果表示学习方法在现实场景中的表现，并分析其失败的原因，从而推动该领域的发展。&lt;h4&gt;方法&lt;/h4&gt;构建了一个特定为CRL设计的光学实验系统，其中包含已知的因果因素和核心假设。选择了几种典型的CRL方法进行测试，并通过替换数据生成过程为简化合成等效的方法来进一步理解算法的失效模式。&lt;h4&gt;主要发现&lt;/h4&gt;现有的大多数CRL方法在合成的简化数据集上表现不佳，这表明它们可能无法很好地应用于实际场景中。此外，论文还观察到一些常用的混合函数假设对某些方法的效果至关重要，但在实际情况中这些假设往往不成立。&lt;h4&gt;结论&lt;/h4&gt;尽管因果表示学习理论前景广阔，但要在实践中取得成功仍面临诸多挑战。该研究提供了一个简单的现实世界基准测试，为验证和进一步发展CRL方法铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;我们评估了几种因果表示学习（CRL）的方法在简单且基于真实世界的系统中的表现，这些方法在这种环境下应该有效。实验采用的是一个特别为了进行这项研究而构建的受控光学实验，在这个系统中满足了CRL的核心假设，并且了解底层因果因素（实验输入），提供了一个事实基础。我们选择了几种典型的CRL方法并发现它们都无法恢复出真正的因果因素。为了理解被评估算法的失败模式，我们在数据上进行了消融研究，通过用一个简单的合成等效过程替换真实的数据生成过程来进行分析。结果揭示了可重复性问题，即大多数方法在简化后的合成版本中已经表现不佳，尽管其数据生成过程简单。此外，我们观察到常见的混合函数假设对于某些方法的性能至关重要，但在实际数据中这些假设并不成立。我们的研究强调了现有理论与实践应用之间的差距，并希望此基准测试作为进一步发展和验证CRL方法的一个简单的现实世界基础性检查。我们在github.com/simonbing/CRLSanityCheck上公开所有代码和数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We evaluate methods for causal representation learning (CRL) on a simple,real-world system where these methods are expected to work. The system consistsof a controlled optical experiment specifically built for this purpose, whichsatisfies the core assumptions of CRL and where the underlying causal factors(the inputs to the experiment) are known, providing a ground truth. We selectmethods representative of different approaches to CRL and find that they allfail to recover the underlying causal factors. To understand the failure modesof the evaluated algorithms, we perform an ablation on the data by substitutingthe real data-generating process with a simpler synthetic equivalent. Theresults reveal a reproducibility problem, as most methods already fail on thissynthetic ablation despite its simple data-generating process. Additionally, weobserve that common assumptions on the mixing function are crucial for theperformance of some of the methods but do not hold in the real data. Ourefforts highlight the contrast between the theoretical promise of the state ofthe art and the challenges in its application. We hope the benchmark serves asa simple, real-world sanity check to further develop and validate methodology,bridging the gap towards CRL methods that work in practice. We make all codeand datasets publicly available at github.com/simonbing/CRLSanityCheck</description>
      <author>example@mail.com (Juan L. Gamella, Simon Bing, Jakob Runge)</author>
      <guid isPermaLink="false">2502.20099v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Vector-Quantized Vision Foundation Models for Object-Centric Learning</title>
      <link>http://arxiv.org/abs/2502.20263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于对象中心学习（OCL）的方法VQ-VFM-OCL（简称VVO），该方法通过使用向量量化视觉基础模型来提取特征，并将这些特征进行量化以加强重建过程中的监督。&lt;h4&gt;背景&lt;/h4&gt;人类能够将视觉场景分解为不同的物体，这有助于理解物体之间的关系和动态。然而，基于自我监督的OCL在处理复杂纹理时面临挑战。为了改善这一点，许多方法采用视觉基础模型来提取更具有对象性的特征图。&lt;h4&gt;目的&lt;/h4&gt;提出一种利用向量量化技术结合视觉基础模型的方法VQ-VFM-OCL（简称VVO），旨在改进现有OCL方法对复杂场景的理解能力，并促进下游任务的表现。&lt;h4&gt;方法&lt;/h4&gt;VVO通过从视觉基础模型中提取特征，以帮助对象级信息聚合；同时，通过对提取的特征进行量化来增强重建过程中的监督。此外，该工作统一了现有的OCL代表方法为一个简洁架构。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明VVO在物体识别任务上超越主流方法，并对下游如视觉预测和推理等任务有积极影响。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过结合向量量化技术与现有视觉基础模型，不仅提高了复杂场景下对象的识别能力，也为进一步研究提供了新的方向。源代码已公布于补充材料中。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Decomposing visual scenes into objects, as humans do, facilitates modelingobject relations and dynamics. Object-Centric Learning (OCL) achieves this byaggregating image or video feature maps into object-level feature vectors,known as \textit{slots}. OCL's self-supervision via reconstructing the inputfrom slots struggles with complex textures, thus many methods employ VisionFoundation Models (VFMs) to extract feature maps with better objectness.However, using VFMs merely as feature extractors does not fully unlock theirpotential. We propose Vector-Quantized VFMs for OCL (VQ-VFM-OCL, or VVO), whereVFM features are extracted to facilitate object-level information aggregationand further quantized to strengthen supervision in reconstruction. Our VVOunifies OCL representatives into a concise architecture. Experimentsdemonstrate that VVO not only outperforms mainstream methods on objectdiscovery tasks but also benefits downstream tasks like visual prediction andreasoning. The source code is available in the supplement.</description>
      <author>example@mail.com (Rongzhen Zhao, Vivienne Wang, Juho Kannala, Joni Pajarinen)</author>
      <guid isPermaLink="false">2502.20263v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Teasing Apart Architecture and Initial Weights as Sources of Inductive Bias in Neural Networks</title>
      <link>http://arxiv.org/abs/2502.20237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 6 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;人工神经网络可以从数据中获取许多人类知识的方面，成为模拟人类学习模型的理想选择。但是这些网络可以学到什么取决于它们的归纳偏置——除了数据之外影响其解决方案的因素。&lt;h4&gt;背景&lt;/h4&gt;尽管人工神经网络在模仿人类学习方面显示出巨大潜力，但对它们的归纳偏置的理解仍然不足，这限制了我们从这些系统的表现中得出关于人类学习结论的能力。认知科学家和机器学习研究人员通常关注神经网络架构作为归纳偏置的一个来源。&lt;h4&gt;目的&lt;/h4&gt;本研究探索初始权重作为另一个可能影响归纳偏置的因素，并利用元学习技术寻找适应特定问题的初始权重，从而减少不同架构和数据表示之间的性能差异。&lt;h4&gt;方法&lt;/h4&gt;通过在三个需要不同类型偏置和概括形式的任务上进行430个不同模型的元训练实验，测试了四种广泛使用的架构：多层感知器（MLPs）、卷积神经网络（CNNs）、长短期记忆网络（LSTMs）以及变换器。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，通过使用元学习可以显著减少或完全消除各种体系结构和数据表示之间的性能差异。此外，在远离元训练经验的问题上，所有架构均表现出差的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;这些发现表明，对于神经网络来说，初始权重可能是比通常认为更重要的归纳偏置来源。这强调了开发更强有力的归纳偏置以实现鲁棒泛化的必要性。&lt;h4&gt;翻译&lt;/h4&gt;人工神经网络可以从数据中获取人类知识的不同方面，显示出成为人类学习模型的巨大潜力。然而，这些网络可以学到什么取决于它们的先验假设——除了数据之外影响其解决方案的因素。尽管人工神经网络在模仿人类学习方面显示出巨大潜力，但对它们的归纳偏置的理解仍然不足，这限制了我们从这些系统的表现中得出关于人类学习结论的能力。认知科学家和机器学习研究人员通常关注神经网络架构作为归纳偏置的一个来源，在本文中我们探讨了一个其他的影响因素——初始权重，并使用元学习作为一种工具来寻找适应特定问题的初始权重。通过在三个需要不同类型偏置和概括形式的任务上进行430个不同模型的元训练实验，测试了四种广泛使用的架构：多层感知器（MLPs）、卷积神经网络（CNNs）、长短期记忆网络（LSTMs）以及变换器。研究发现，在特定问题上通过使用元学习可以显著减少或完全消除各种体系结构和数据表示之间的性能差异。此外，在远离元训练经验的问题上，所有架构均表现出差的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial neural networks can acquire many aspects of human knowledge fromdata, making them promising as models of human learning. But what thosenetworks can learn depends upon their inductive biases -- the factors otherthan the data that influence the solutions they discover -- and the inductivebiases of neural networks remain poorly understood, limiting our ability todraw conclusions about human learning from the performance of these systems.Cognitive scientists and machine learning researchers often focus on thearchitecture of a neural network as a source of inductive bias. In this paperwe explore the impact of another source of inductive bias -- the initialweights of the network -- using meta-learning as a tool for finding initialweights that are adapted for specific problems. We evaluate four widely-usedarchitectures -- MLPs, CNNs, LSTMs, and Transformers -- by meta-training 430different models across three tasks requiring different biases and forms ofgeneralization. We find that meta-learning can substantially reduce or entirelyeliminate performance differences across architectures and datarepresentations, suggesting that these factors may be less important as sourcesof inductive bias than is typically assumed. When differences are present,architectures and data representations that perform well without meta-learningtend to meta-train more effectively. Moreover, all architectures generalizepoorly on problems that are far from their meta-training experience,underscoring the need for stronger inductive biases for robust generalization.</description>
      <author>example@mail.com (Gianluca Bencomo, Max Gupta, Ioana Marinescu, R. Thomas McCoy, Thomas L. Griffiths)</author>
      <guid isPermaLink="false">2502.20237v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>DIN-CTS: Low-Complexity Depthwise-Inception Neural Network with Contrastive Training Strategy for Deepfake Speech Detection</title>
      <link>http://arxiv.org/abs/2502.20225v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于低复杂度Depthwise-Inception Network (DIN)和对比训练策略(CTS)的深度神经网络方法，用于检测deepfake语音。&lt;h4&gt;背景&lt;/h4&gt;当前存在大量的伪造音频，这对社会和个人安全构成了威胁。需要一种有效的方法来区分真实语音和deepfake伪造语音。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效准确地识别deepfake语音的技术方案。&lt;h4&gt;方法&lt;/h4&gt;通过短时傅里叶变换(STFT)和线性滤波器(LF)将输入音频转换为频谱图，然后使用这些频谱图训练DIN。利用该网络提取真实语音的特征向量，并构建高斯分布模型以表示真实语音；测试样本与该分布的距离被用来判断其真实性。&lt;h4&gt;主要发现&lt;/h4&gt;在ASVspoof 2019 LA基准数据集上的实验结果表明，结合Depthwise-Inception Network和对比学习策略能够有效区分伪造音频和真实语音。使用一个参数量仅为1.77M的低复杂度DIN，在4秒短音频段上实现了4.6%的等错误率(EER)、95.4%的准确率(Acc.)、97.3%的F1值以及98.9%的AUC分数。&lt;h4&gt;结论&lt;/h4&gt;该系统在ASVspoof 2019 LA挑战赛中的单系统提交中表现最好，显示出其应用于实时应用的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we propose a deep neural network approach for deepfake speechdetection (DSD) based on a lowcomplexity Depthwise-Inception Network (DIN)trained with a contrastive training strategy (CTS). In this framework, inputaudio recordings are first transformed into spectrograms using Short-TimeFourier Transform (STFT) and Linear Filter (LF), which are then used to trainthe DIN. Once trained, the DIN processes bonafide utterances to extract audioembeddings, which are used to construct a Gaussian distribution representinggenuine speech. Deepfake detection is then performed by computing the distancebetween a test utterance and this distribution to determine whether theutterance is fake or bonafide. To evaluate our proposed systems, we conductedextensive experiments on the benchmark dataset of ASVspoof 2019 LA. Theexperimental results demonstrate the effectiveness of combining theDepthwise-Inception Network with the contrastive learning strategy indistinguishing between fake and bonafide utterances. We achieved Equal ErrorRate (EER), Accuracy (Acc.), F1, AUC scores of 4.6%, 95.4%, 97.3%, and 98.9%respectively using a single, low-complexity DIN with just 1.77 M parameters and985 M FLOPS on short audio segments (4 seconds). Furthermore, our proposedsystem outperforms the single-system submissions in the ASVspoof 2019 LAchallenge, showcasing its potential for real-time applications.</description>
      <author>example@mail.com (Lam Pham, Dat Tran, Florian Skopik, Alexander Schindler, Silvia Poletti, Fischinger David, Martin Boyer)</author>
      <guid isPermaLink="false">2502.20225v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A2-GNN: Angle-Annular GNN for Visual Descriptor-free Camera Relocalization</title>
      <link>http://arxiv.org/abs/2502.20036v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in 2025 International Conference on 3D Vision (3DV)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Angle-Annular Graph Neural Network (A2-GNN)的方法，用于直接进行2D和3D关键点匹配而无需使用视觉描述符。这种方法通过环状特征提取来高效学习鲁棒的几何结构表示，并在没有视觉描述符的情况下实现了目前最高的精度。&lt;h4&gt;背景&lt;/h4&gt;视觉定位涉及估计已知场景中6自由度（6-DoF）相机的姿态，其中识别2D查询图像与3D模型之间的像素到点对应关系是一个关键步骤。当前最先进的方法依赖于广泛的视觉描述符来建立这些对应关系，但面临存储、隐私问题和模型维护的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种简单的方法，该方法能够克服现有无描述符方法中的低精度或重计算量的问题，并实现高效的2D-3D关键点匹配。&lt;h4&gt;方法&lt;/h4&gt;论文引入了Angle-Annular Graph Neural Network (A2-GNN)，这种网络通过环状特征提取来学习鲁棒的几何结构表示。它将邻域聚类并嵌入每个组的距离信息和角度作为补充信息，以捕捉局部结构。&lt;h4&gt;主要发现&lt;/h4&gt;在匹配和视觉定位数据集上的评估表明，该方法在无需视觉描述符的情况下达到了最先进的精度，并且计算开销低。&lt;h4&gt;结论&lt;/h4&gt;A2-GNN提供了一种有效的方法来解决直接进行2D-3D关键点匹配的挑战，克服了现有无描述符方法中的精度和计算量问题。此研究为未来的视觉定位工作奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容：视觉定位涉及估计已知场景中6自由度（6-DoF）相机的姿态。其中的一个关键步骤是识别2D查询图像与3D模型之间的像素到点的对应关系。当前最先进的方法依赖于广泛的视觉描述符来建立这些对应关系，但是面临存储、隐私问题以及模型维护的问题挑战。无视觉描述符直接进行2D-3D关键点匹配的方法正在变得流行，因为这种方法可以克服这些问题。然而现有的无描述符方法面临着低精度或重计算量的挑战。为了解决这个问题，本文引入了一种名为Angle-Annular Graph Neural Network (A2-GNN) 的简单方法来高效学习具有鲁棒几何结构表示能力的网络，并通过环状特征提取补充距离信息和角度作为辅助信息以捕捉局部结构。在匹配和视觉定位数据集上的评估表明，我们的方法达到了无需描述符的情况下最高的精度且计算量较低。代码将在 https://github.com/YejunZhang/a2-gnn 发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual localization involves estimating the 6-degree-of-freedom (6-DoF)camera pose within a known scene. A critical step in this process isidentifying pixel-to-point correspondences between 2D query images and 3Dmodels. Most advanced approaches currently rely on extensive visual descriptorsto establish these correspondences, facing challenges in storage, privacyissues and model maintenance. Direct 2D-3D keypoint matching without visualdescriptors is becoming popular as it can overcome those challenges. However,existing descriptor-free methods suffer from low accuracy or heavy computation.Addressing this gap, this paper introduces the Angle-Annular Graph NeuralNetwork (A2-GNN), a simple approach that efficiently learns robust geometricstructural representations with annular feature extraction. Specifically, thisapproach clusters neighbors and embeds each group's distance information andangle as supplementary information to capture local structures. Evaluation onmatching and visual localization datasets demonstrates that our approachachieves state-of-the-art accuracy with low computational overhead among visualdescription-free methods. Our code will be released onhttps://github.com/YejunZhang/a2-gnn.</description>
      <author>example@mail.com (Yejun Zhang, Shuzhe Wang, Juho Kannala)</author>
      <guid isPermaLink="false">2502.20036v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>BEVDiffuser: Plug-and-Play Diffusion Model for BEV Denoising with Ground-Truth Guidance</title>
      <link>http://arxiv.org/abs/2502.19694v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的扩散模型BEVDiffuser，用于减少鸟瞰图（BEV）表示中的噪声。该方法在nuScenes数据集上的实验表明，在不增加计算复杂度的情况下，显著提高了3D物体检测的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的BEV生成技术尽管有所进步，但仍然受到传感器限制和学习过程中的内在噪声影响，导致下游任务性能不佳。&lt;h4&gt;目的&lt;/h4&gt;通过使用地面真实对象布局作为引导来有效地减少BEV特征图中的噪声，从而提高现有BEV模型的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为BEVDiffuser的新扩散模型，在训练期间可以以插件方式增强现有的BEV模型而无需对架构进行任何修改。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明BEVDiffuser在nuScenes数据集上显著提高了3D物体检测的性能，mAP和NDS分别提高了12.3%和10.1%，且没有引入额外的计算复杂度。此外，在长尾物体检测以及恶劣天气和光照条件下表现优异。&lt;h4&gt;结论&lt;/h4&gt;BEVDiffuser能够有效提高现有BEV模型的准确性和鲁棒性，为自动驾驶任务提供更高质量的输入表示。&lt;h4&gt;翻译&lt;/h4&gt;鸟瞰图（BEV）表示在自主驾驶任务中起着关键作用。尽管最近在BEV生成方面取得了进展，但由传感器限制和学习过程产生的固有噪声仍然没有得到充分解决，导致次优的BEV表示，从而影响下游任务的表现。为了解决这个问题，我们提出了一种新的扩散模型BEVDiffuser，它使用地面真实对象布局作为指导来有效减少BEV特征图中的噪声。在nuScenes数据集上进行的广泛实验表明，BEVDiffuser具有出色的去噪和生成能力，在不增加额外计算复杂度的情况下显著提高了现有BEV模型的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bird's-eye-view (BEV) representations play a crucial role in autonomousdriving tasks. Despite recent advancements in BEV generation, inherent noise,stemming from sensor limitations and the learning process, remains largelyunaddressed, resulting in suboptimal BEV representations that adversely impactthe performance of downstream tasks. To address this, we propose BEVDiffuser, anovel diffusion model that effectively denoises BEV feature maps using theground-truth object layout as guidance. BEVDiffuser can be operated in aplug-and-play manner during training time to enhance existing BEV modelswithout requiring any architectural modifications. Extensive experiments on thechallenging nuScenes dataset demonstrate BEVDiffuser's exceptional denoisingand generation capabilities, which enable significant enhancement to existingBEV models, as evidenced by notable improvements of 12.3\% in mAP and 10.1\% inNDS achieved for 3D object detection without introducing additionalcomputational complexity. Moreover, substantial improvements in long-tailobject detection and under challenging weather and lighting conditions furthervalidate BEVDiffuser's effectiveness in denoising and enhancing BEVrepresentations.</description>
      <author>example@mail.com (Xin Ye, Burhaneddin Yaman, Sheng Cheng, Feng Tao, Abhirup Mallik, Liu Ren)</author>
      <guid isPermaLink="false">2502.19694v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>RURANET++: An Unsupervised Learning Method for Diabetic Macular Edema Based on SCSE Attention Mechanisms and Dynamic Multi-Projection Head Clustering</title>
      <link>http://arxiv.org/abs/2502.20224v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 2 figures, 5 tables, submitted to The 28th International  Conference on Medical Image Computing and Computer Assisted Intervention  (MICCAI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Diabetic Macular Edema (DME) 是糖尿病患者常见的并发症，是视力损害和失明的主要原因之一。虽然深度学习在医学图像分析中取得了显著进展，但传统的 DME 诊断仍然依赖于大量标注数据和主观的眼科医生评估，限制了实际应用。&lt;h4&gt;背景&lt;/h4&gt;糖尿病黄斑水肿（DME）是一种常见且严重的糖尿病并发症，它会导致视力障碍甚至失明。尽管基于深度学习的方法在医学图像分析方面已经取得了一定的进展，但传统的 DME 诊断依旧依赖于大量的标记数据和眼科医生主观评估，这限制了其实际应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需大量标注数据的自动化的 DME 诊断系统 RURANET++。&lt;h4&gt;方法&lt;/h4&gt;RURANET++ 框架采用优化后的 U-Net 架构，并嵌入空间和通道挤压与激励（SCSE）注意机制，以增强病变特征提取。在特征处理阶段，首先使用预训练的 GoogLeNet 提取视网膜图像中的深度特征；然后利用基于主成分分析（PCA）的方法将特征维度减少至 50 维，提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;引入了一种新的聚类算法，采用多投影头来显式地控制集群多样性，并动态调整相似性阈值以优化内类一致性与外类区分度。实验结果表明，该系统在多个指标上表现出色，包括最大准确率（0.8411）、精确度（0.8593）、召回率（0.8411）和 F1 分数（0.8390），并具备优秀的聚类质量。&lt;h4&gt;结论&lt;/h4&gt;本研究提供了一种高效的无监督解决方案，用于 DME 的诊断，在临床应用方面具有重要意义。&lt;h4&gt;翻译&lt;/h4&gt;糖尿病黄斑水肿（DME）是导致视力损害乃至失明的重要因素。尽管深度学习技术在医学图像分析领域取得了显著进展，但传统的 DME 诊断仍然依赖于大量的标注数据和眼科医生的主观判断，这限制了其实际应用范围。本研究提出了 RURANET++ 系统，它基于无监督学习方法进行自动化 DME 诊断，并通过优化 U-Net 架构、引入 SCSE 注意机制以及创新性地使用多投影头聚类算法等技术手段提高病变特征提取效率和准确性。实验结果表明该系统在多个评估指标上均表现出色，显示出其在临床应用中的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic Macular Edema (DME), a prevalent complication among diabeticpatients, constitutes a major cause of visual impairment and blindness.Although deep learning has achieved remarkable progress in medical imageanalysis, traditional DME diagnosis still relies on extensive annotated dataand subjective ophthalmologist assessments, limiting practical applications. Toaddress this, we present RURANET++, an unsupervised learning-based automatedDME diagnostic system. This framework incorporates an optimized U-Netarchitecture with embedded Spatial and Channel Squeeze &amp; Excitation (SCSE)attention mechanisms to enhance lesion feature extraction. During featureprocessing, a pre-trained GoogLeNet model extracts deep features from retinalimages, followed by PCA-based dimensionality reduction to 50 dimensions forcomputational efficiency. Notably, we introduce a novel clustering algorithmemploying multi-projection heads to explicitly control cluster diversity whiledynamically adjusting similarity thresholds, thereby optimizing intra-classconsistency and inter-class discrimination. Experimental results demonstratesuperior performance across multiple metrics, achieving maximum accuracy(0.8411), precision (0.8593), recall (0.8411), and F1-score (0.8390), withexceptional clustering quality. This work provides an efficient unsupervisedsolution for DME diagnosis with significant clinical implications.</description>
      <author>example@mail.com (Wei Yang, Yiran Zhu, Jiayu Shen, Yuhan Tang, Chengchang Pan, Hui He, Yan Su, Honggang Qi)</author>
      <guid isPermaLink="false">2502.20224v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Causal Effect Estimation under Networked Interference without Networked Unconfoundedness Assumption</title>
      <link>http://arxiv.org/abs/2502.19741v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2405.03342&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了在网络环境下估计因果效应的问题，提出了一种基于可识别表示学习技术的网络效应估计器，并通过实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;在存在隐蔽共变量的情况下，现有的基于观察数据的方法通常无法有效估计网络影响。然而，在这种情况下，单位之间的交互可以提供重要的信息来恢复这些隐藏的共变量。&lt;h4&gt;目的&lt;/h4&gt;识别三种类型在网络推断中阻碍识别的潜在混淆因子，并通过利用可识别表示学习技术，提出一种新的网络效应估计算法。&lt;h4&gt;方法&lt;/h4&gt;提出了基于可识别表示学习技术的网络效应估计器。理论上确立了所有潜在混淆因子的可识别性，通过应用已确定的潜在混淆因子提供网络效应的识别结果。&lt;h4&gt;主要发现&lt;/h4&gt;三种类型的隐藏共变量影响个体、邻居或同时影响两者，并且这些因素阻碍了对网络效应的有效识别和估计。&lt;h4&gt;结论&lt;/h4&gt;该方法不仅理论上证明了其在处理网络干扰下因果效应估计问题中的有效性，而且实验也验证了这一理论。通过这种方法，可以更准确地评估在网络环境中交互作用的影响。&lt;h4&gt;翻译&lt;/h4&gt;在存在网络干扰的情况下估计因果效应是一个关键且具有挑战性的问题。现有的基于观察数据的方法主要依赖于网络无偏性假设来保证网络效应的识别。然而，在实际情况下这种假设往往由于隐藏共变量的存在而被违反，这阻碍了对网络效应的有效识别。有趣的是，在这样的网络环境中，单位之间的交互提供了恢复隐藏共变量的重要信息。本文确定了在三个影响个体、仅影响邻居和同时影响两者的潜在混淆因子，并基于此提出了一个新的方法来估计网络效应并证明其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Estimating causal effects under networked interference is a crucial yetchallenging problem. Existing methods based on observational data mainly relyon the networked unconfoundedness assumption, which guarantees theidentification of networked effects. However, the networked unconfoundednessassumption is usually violated due to the latent confounders in observationaldata, hindering the identification of networked effects. Interestingly, in suchnetworked settings, interactions between units provide valuable information forrecovering latent confounders. In this paper, we identify three types of latentconfounders in networked inference that hinder identification: those affectingonly the individual, those affecting only neighbors, and those influencingboth. Specifically, we devise a networked effect estimator based onidentifiable representation learning techniques. Theoretically, we establishthe identifiability of all latent confounders, and leveraging the identifiedlatent confounders, we provide the networked effect identification result.Extensive experiments validate our theoretical results and demonstrate theeffectiveness of the proposed method.</description>
      <author>example@mail.com (Weilin Chen, Ruichu Cai, Jie Qiao, Yuguang Yan, José Miguel Hernández-Lobato)</author>
      <guid isPermaLink="false">2502.19741v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Ev-3DOD: Pushing the Temporal Boundaries of 3D Object Detection with Event Cameras</title>
      <link>http://arxiv.org/abs/2502.19630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了在自动驾驶系统中的3D物体检测任务中使用异步事件相机的方法，解决了传统固定帧率传感器如LiDAR和摄像头带来的延迟和带宽限制问题。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态方法虽然取得了良好的性能，但未能满足自动驾驶系统对于低延时、高效率的需求。传统的LiDAR和摄像头存在固有的时间延迟及带宽限制。&lt;h4&gt;目的&lt;/h4&gt;引入异步事件相机以提高3D物体检测的速度和准确性，并建立基于事件的数据集作为评估标准。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的基于事件的3D物体检测框架，利用事件相机的高时间分辨率和低带宽特性，在不同帧之间进行高效准确的3D物体检测。同时发布了一个新数据集DSEC-3DOD来支持这项研究。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入异步事件信息可以实现更快速、更低延迟的3D物体检测，特别是在常规传感器无法提供同步信息的时间间隔内也能完成任务。&lt;h4&gt;结论&lt;/h4&gt;文章证明了使用异步事件相机进行3D物体检测具有巨大的潜力，并为该领域的进一步研究提供了必要的资源。代码和数据集可在GitHub上获得。&lt;h4&gt;翻译&lt;/h4&gt;在点云中检测三维物体对于自动驾驶系统至关重要。最近，结合摄像信息的高级多模式方法取得了显著性能。为了实现安全有效的自主驾驶系统，不仅要准确还需要快速且延迟低的算法是必不可少的。然而现有的算法由于固定帧率传感器如LiDAR和摄像头的时间延迟及带宽限制未能达到这些要求。为了解决这个问题，首次将异步事件相机引入3D物体检测中。我们利用它们的高时间分辨率和低带宽特性来实现高速3D物体检测。通过在不同帧之间使用事件相机检索之前的3D信息，我们的方法甚至可以在无同步数据的时间间隔内进行检测。此外，提出了第一个基于事件的3D物体检测数据集DSEC-3DOD，该数据集中包含了每秒100帧的真实3D边界框，为基于事件的3D检测器建立了基准。代码和数据集可在https://github.com/mickeykang16/Ev3DOD获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting 3D objects in point clouds plays a crucial role in autonomousdriving systems. Recently, advanced multi-modal methods incorporating camerainformation have achieved notable performance. For a safe and effectiveautonomous driving system, algorithms that excel not only in accuracy but alsoin speed and low latency are essential. However, existing algorithms fail tomeet these requirements due to the latency and bandwidth limitations of fixedframe rate sensors, e.g., LiDAR and camera. To address this limitation, weintroduce asynchronous event cameras into 3D object detection for the firsttime. We leverage their high temporal resolution and low bandwidth to enablehigh-speed 3D object detection. Our method enables detection even duringinter-frame intervals when synchronized data is unavailable, by retrievingprevious 3D information through the event camera. Furthermore, we introduce thefirst event-based 3D object detection dataset, DSEC-3DOD, which includesground-truth 3D bounding boxes at 100 FPS, establishing the first benchmark forevent-based 3D detectors. The code and dataset are available athttps://github.com/mickeykang16/Ev3DOD.</description>
      <author>example@mail.com (Hoonhee Cho, Jae-young Kang, Youngho Kim, Kuk-Jin Yoon)</author>
      <guid isPermaLink="false">2502.19630v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Deep Convolutional Neural Networks for Palm Fruit Maturity Classification</title>
      <link>http://arxiv.org/abs/2502.20223v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究开发了一种基于深度卷积神经网络（CNN）的自动化视觉系统，用于准确分类油棕果图片的五个成熟度等级。&lt;h4&gt;背景&lt;/h4&gt;为了最大化油棕产量和质量，必须在最佳成熟期收获油棕果实。现有的方法依赖于人工评估，这可能导致效率低下和错误判断。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够自动分类油棕果图像成熟程度的计算机视觉系统，以便优化收割决策并提高油棕生产效率。&lt;h4&gt;方法&lt;/h4&gt;使用深度CNN对基于其成熟阶段的油棕果图片进行分类。浅层CNN作为基准模型，而迁移学习和微调则应用到预训练好的ResNet50和InceptionV3架构上。&lt;h4&gt;主要发现&lt;/h4&gt;该研究利用包含超过8,000张带有显著变化的照片的数据集，实现了超过85%的测试准确率。深度CNN模型在分类油棕果成熟阶段方面表现出了巨大的潜力。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了深度学习在自动评估油棕果实熟度方面的潜力，这可以有助于优化收割决策并提高油棕生产效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文描述了如何利用计算机视觉和深度学习技术来更准确地判断油棕果的成熟程度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To maximize palm oil yield and quality, it is essential to harvest palm fruitat the optimal maturity stage. This project aims to develop an automatedcomputer vision system capable of accurately classifying palm fruit images intofive ripeness levels. We employ deep Convolutional Neural Networks (CNNs) toclassify palm fruit images based on their maturity stage. A shallow CNN servesas the baseline model, while transfer learning and fine-tuning are applied topre-trained ResNet50 and InceptionV3 architectures. The study utilizes apublicly available dataset of over 8,000 images with significant variations,which is split into 80\% for training and 20\% for testing. The proposed deepCNN models achieve test accuracies exceeding 85\% in classifying palm fruitmaturity stages. This research highlights the potential of deep learning forautomating palm fruit ripeness assessment, which can contribute to optimizingharvesting decisions and improving palm oil production efficiency.</description>
      <author>example@mail.com (Mingqiang Han, Chunlin Yi)</author>
      <guid isPermaLink="false">2502.20223v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Learning to Generalize without Bias for Open-Vocabulary Action Recognition</title>
      <link>http://arxiv.org/abs/2502.20158v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种用于开放词汇动作识别的新颖元优化框架Open-MeDe，该框架通过静态去偏改善了模型在新环境下（包括上下文中和上下文外）的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;最近的视频学习者利用CLIP初始化以提高开放词汇的动作识别中的泛化性。然而，由于CLIP自身的静态偏差问题，这些视频学习者倾向于过度拟合于快捷静态特征，从而导致对新动作的泛化性不足。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的元优化框架Open-MeDe来解决视频学习者的过度拟合问题，并改善其在开放词汇动作识别中的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;引入了跨批次元优化方案，该方案通过虚拟评估鼓励视频学习者快速泛化到任意后续数据上。此外，采用自集成策略获取可实现上下文内外新数据稳健泛化的最优参数。&lt;h4&gt;主要发现&lt;/h4&gt;Open-MeDe在没有CLIP正则化的情况下进行优化，隐式地减轻了视频元学习者的固有静态偏差，并且通过元学习方法改进了从已知到开放词汇的泛化能力以及图像到视频去偏能力。实验结果表明，Open-MeDe不仅超越了针对上下文中开放词汇动作识别定制的最佳正则化方法，在上下文外场景中也表现出色。&lt;h4&gt;结论&lt;/h4&gt;Open-MeDE通过元学习和自集成策略有效解决了当前视频模型在开放词汇动作识别中的泛化性问题，特别是对于新环境下的泛化能力有了显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Leveraging the effective visual-text alignment and static generalizabilityfrom CLIP, recent video learners adopt CLIP initialization with furtherregularization or recombination for generalization in open-vocabulary actionrecognition in-context. However, due to the static bias of CLIP, such videolearners tend to overfit on shortcut static features, thereby compromisingtheir generalizability, especially to novel out-of-context actions. To addressthis issue, we introduce Open-MeDe, a novel Meta-optimization framework withstatic Debiasing for Open-vocabulary action recognition. From a freshperspective of generalization, Open-MeDe adopts a meta-learning approach toimprove known-to-open generalizing and image-to-video debiasing in acost-effective manner. Specifically, Open-MeDe introduces a cross-batchmeta-optimization scheme that explicitly encourages video learners to quicklygeneralize to arbitrary subsequent data via virtual evaluation, steering asmoother optimization landscape. In effect, the free of CLIP regularizationduring optimization implicitly mitigates the inherent static bias of the videometa-learner. We further apply self-ensemble over the optimization trajectoryto obtain generic optimal parameters that can achieve robust generalization toboth in-context and out-of-context novel data. Extensive evaluations show thatOpen-MeDe not only surpasses state-of-the-art regularization methods tailoredfor in-context open-vocabulary action recognition but also substantially excelsin out-of-context scenarios.</description>
      <author>example@mail.com (Yating Yu, Congqi Cao, Yifan Zhang, Yanning Zhang)</author>
      <guid isPermaLink="false">2502.20158v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Do computer vision foundation models learn the low-level characteristics of the human visual system?</title>
      <link>http://arxiv.org/abs/2502.20256v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究了基于自然图像训练的计算机视觉基础模型是否模仿人类视觉系统的基本特性，如对比度检测、对比掩蔽和对比恒常性。&lt;h4&gt;背景&lt;/h4&gt;计算机视觉基础模型（例如DINO或OpenCLIP）通过在大型图像数据集上进行自监督学习来训练。有大量证据表明，人类视觉系统受自然世界中颜色和图案统计分布的影响，这些特征也存在于基础模型的训练数据中。&lt;h4&gt;目的&lt;/h4&gt;评估45种基础和生成模型的图像编码器是否模拟了人类视觉系统的特性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个包含九个测试类型的协议来评估不同模型在对比度检测、对比掩蔽等方面的性能。&lt;h4&gt;主要发现&lt;/h4&gt;一些基础模型（例如DINO，DINOv2和OpenCLIP）具有与人类视觉相似的特性，但其他模型则表现出较少或无相似性。基础模型对低对比度的敏感性较低，并且在不同频率下的对比度响应较为不规则。&lt;h4&gt;结论&lt;/h4&gt;尽管存在差异，但在基于视觉任务训练的基础模型中，已经开始出现低级人类视觉特征的一致性趋势，尤其是DINOv2模型最为接近。&lt;h4&gt;翻译&lt;/h4&gt;计算机视觉基础模型通常通过自监督方式使用大规模图像数据集进行训练。类似于这些系统的是，大量证据表明，人类视觉系统受自然世界中颜色和图案统计分布的影响，这与基础模型训练数据中的特性相似。本研究探讨了基于自然图像训练的基础模型是否模仿了一些低级的人类视觉特性，例如对比度检测、对比掩蔽和对比恒常性。具体而言，设计了一种包含九个测试类型的协议来评估45种基础和生成模型的性能。结果表明，某些基础模型（如DINO、DINOv2和OpenCLIP）分享了一些人类视觉的特性，而其他模型则表现出较少或无相似之处。总体而言，虽然仍有差异存在，但基于视觉任务训练的基础模型开始显示出低级人类视觉特征的一致性趋势，特别是DINOv2模型最为接近。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Computer vision foundation models, such as DINO or OpenCLIP, are trained in aself-supervised manner on large image datasets. Analogously, substantialevidence suggests that the human visual system (HVS) is influenced by thestatistical distribution of colors and patterns in the natural world,characteristics also present in the training data of foundation models. Thequestion we address in this paper is whether foundation models trained onnatural images mimic some of the low-level characteristics of the human visualsystem, such as contrast detection, contrast masking, and contrast constancy.Specifically, we designed a protocol comprising nine test types to evaluate theimage encoders of 45 foundation and generative models. Our results indicatethat some foundation models (e.g., DINO, DINOv2, and OpenCLIP), share some ofthe characteristics of human vision, but other models show little resemblance.Foundation models tend to show smaller sensitivity to low contrast and ratherirregular responses to contrast across frequencies. The foundation models showthe best agreement with human data in terms of contrast masking. Our findingssuggest that human vision and computer vision may take both similar anddifferent paths when learning to interpret images of the real world. Overall,while differences remain, foundation models trained on vision tasks start toalign with low-level human vision, with DINOv2 showing the closest resemblance.</description>
      <author>example@mail.com (Yancheng Cai, Fei Yin, Dounia Hammou, Rafal Mantiuk)</author>
      <guid isPermaLink="false">2502.20256v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive H&amp;E-IHC information fusion staining framework based on feature extra</title>
      <link>http://arxiv.org/abs/2502.20156v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;免疫组化（IHC）染色在乳腺癌等疾病的评估中发挥着重要作用。基于生成模型的H&amp;E到IHC转换提供了一种简单且成本效益高的方法来获取IHC图像。&lt;h4&gt;背景&lt;/h4&gt;尽管之前的模型能够很好地进行数字上色，但它们仍然面临着两个挑战：(i) 仅通过HE图像中的像素特征来进行上色，容易导致染色过程中信息丢失；(ii) 缺乏像素级别的H&amp;E-IHC真实对给经典的L1损失带来了挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对上述挑战，我们提出了一种基于特征提取器的自适应信息增强着色框架。&lt;h4&gt;方法&lt;/h4&gt;首先提出了VMFE模块，通过多尺度特征提取和小波变换卷积有效提取颜色信息特征，并结合共享解码器进行特征融合。高性能的H&amp;E-IHC双特征提取器通过对比学习训练，可以在高纬度空间内有效地执行HE-IHC特征对齐。&lt;h4&gt;主要发现&lt;/h4&gt;此外，在染色过程中使用经过训练的功能编码器来增强功能并自适应调整损失，解决了与模糊和不对称信息相关的问题。我们在不同的数据集上进行了测试，并取得了卓越的性能。&lt;h4&gt;结论&lt;/h4&gt;我们的代码可在https://github.com/babyinsunshine/CEFF获取&lt;h4&gt;翻译&lt;/h4&gt;免疫组化（IHC）染色在疾病评估中扮演重要角色，特别是在乳腺癌等疾病的诊断与研究。通过生成模型将H&amp;E图像转换为IHC图像的方法提供了一种成本效益高且简单的途径来获得IHC图像。尽管先前的模型能够很好地模拟数字着色过程，但它们仍存在两个主要问题：第一，仅使用HE图中不突出的像素特征进行染色会导致信息丢失；第二，缺乏准确的H&amp;E-IHC配对数据使得传统的L1损失难以有效应用。为了解决这些问题，我们提出了一种基于自适应增强与特征提取器的着色框架，该框架利用VMFE模块在多尺度上提取颜色信息，并通过对比学习训练提高特征匹配效果。实验结果显示了这一方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Immunohistochemistry (IHC) staining plays a significant role in theevaluation of diseases such as breast cancer. The H&amp;E-to-IHC transformationbased on generative models provides a simple and cost-effective method forobtaining IHC images. Although previous models can perform digital coloringwell, they still suffer from (i) coloring only through the pixel features thatare not prominent in HE, which is easy to cause information loss in thecoloring process; (ii) The lack of pixel-perfect H&amp;E-IHC groundtruth pairsposes a challenge to the classical L1 loss.To address the above challenges, wepropose an adaptive information enhanced coloring framework based on featureextractors. We first propose the VMFE module to effectively extract the colorinformation features using multi-scale feature extraction and wavelet transformconvolution, while combining the shared decoder for feature fusion. Thehigh-performance dual feature extractor of H&amp;E-IHC is trained by contrastivelearning, which can effectively perform feature alignment of HE-IHC in highlatitude space. At the same time, the trained feature encoder is used toenhance the features and adaptively adjust the loss in the HE section stainingprocess to solve the problems related to unclear and asymmetric information. Wehave tested on different datasets and achieved excellent performance.Our codeis available at https://github.com/babyinsunshine/CEFF</description>
      <author>example@mail.com (Yifan Jia, Xingda Yu, Zhengyang Ji, Songning Lai, Yutao Yue)</author>
      <guid isPermaLink="false">2502.20156v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Your contrastive learning problem is secretly a distribution alignment problem</title>
      <link>http://arxiv.org/abs/2502.20141v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, NeurIPS 2024 submission, includes supplementary  material&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;尽管对比学习在视觉和语言领域的成功，其理论基础以及构建表示的机制仍不甚明了。这项工作探讨了广泛用于对比学习中的噪声对比估计损失与基于熵最优传输（OT）的分布对齐之间的联系，并由此开发了一系列新的损失函数及其多步迭代变体。&lt;h4&gt;背景&lt;/h4&gt;尽管对比学习在视觉和语言领域取得了成功，其理论基础及构建表示的方法仍然不清楚。传统的对比学习通常使用特定类型的噪声对比估计损失来实现。&lt;h4&gt;目的&lt;/h4&gt;通过建立噪声对比估计损失与基于熵最优传输的分布对齐之间的联系，发展新型对比学习方法，增强模型对于数据集中的噪音视图处理能力，并允许自定义表示空间以适应不同约束条件。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用更多潜在分布信息的方法来改进对比学习，通过这种方法可以更有效地调整扩充样本集合内的关系。此外，该研究还提供了理论洞见和实验证据证明了新方法在广义对比对齐中的优势。&lt;h4&gt;主要发现&lt;/h4&gt;通过将对比学习重新定义为一个分布对齐问题，并利用最优传输的优化工具，这项工作不仅揭示了不同自监督模型之间的新连接，而且提供了一套可以更轻松地融入领域知识的新工具。借助于这些框架和工具，能够构建不平衡损失以处理噪音视图。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，对比学习可以通过与基于熵最优传输的分布对齐相结合的方式得到新的理论洞见，并且提供了增强模型性能及适应性的新方法。&lt;h4&gt;翻译&lt;/h4&gt;尽管对比学习在视觉和语言任务中的成功令人瞩目，但对于其背后的理论基础以及如何构建有效表示的理解仍然有限。本文尝试建立噪声对比估计损失与基于熵最优传输的分布对齐之间的桥梁，开发出新的损失函数家族，并为现存的对比学习方法提供迭代变体方案。通过引入更多潜在分布的信息，研究者提出了一种更加‘感知’到数据集内部关系的方法，从而改进模型在处理噪音视图时的表现。实验结果显示了这种方法在广义对比对齐上的显著优势。此外，该研究还展示了如何重新解读对比学习为一个对齐问题，并利用现有的最优传输优化工具来揭示不同自监督学习模型之间的新连接及提供易于适应领域知识的新手段。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the success of contrastive learning (CL) in vision and language, itstheoretical foundations and mechanisms for building representations remainpoorly understood. In this work, we build connections between noise contrastiveestimation losses widely used in CL and distribution alignment with entropicoptimal transport (OT). This connection allows us to develop a family ofdifferent losses and multistep iterative variants for existing CL methods.Intuitively, by using more information from the distribution of latents, ourapproach allows a more distribution-aware manipulation of the relationshipswithin augmented sample sets. We provide theoretical insights and experimentalevidence demonstrating the benefits of our approach for {\em generalizedcontrastive alignment}. Through this framework, it is possible to leveragetools in OT to build unbalanced losses to handle noisy views and customize therepresentation space by changing the constraints on alignment. By reframingcontrastive learning as an alignment problem and leveraging existingoptimization tools for OT, our work provides new insights and connectionsbetween different self-supervised learning models in addition to new tools thatcan be more easily adapted to incorporate domain knowledge into learning.</description>
      <author>example@mail.com (Zihao Chen, Chi-Heng Lin, Ran Liu, Jingyun Xiao, Eva L Dyer)</author>
      <guid isPermaLink="false">2502.20141v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>4Deform: Neural Surface Deformation for Robust Shape Interpolation</title>
      <link>http://arxiv.org/abs/2502.20208v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR25&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法4Deform，用于生成非刚性变形形状之间的真实中间形态。&lt;h4&gt;背景&lt;/h4&gt;在计算机视觉领域，生成非刚体变形物体的现实过渡形态是一个具有挑战性的任务，尤其是在没有结构的数据（如点云）中进行这种操作时尤为困难。现有的大多数插值方法都是针对有结构的数据设计的（例如网格），对于实际世界的点云并不适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于无结构数据的方法4Deform，该方法可以实现自由拓扑变化的形状变形，并且不依赖于中间形态监督。&lt;h4&gt;方法&lt;/h4&gt;新方法采用神经隐式表示(NIR)来处理连续欧几里得空间中的速度场学习问题。通过物理和几何约束来正则化这种速度场，并使用修改后的水平集方程重新构建过渡表面，直接将NIR与速度场连接起来。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在各种场景（包括噪声、不完整拓扑变化等）下显著优于之前的神经隐式表示方法。此外，这种方法首次实现了新的应用，如4D Kinect序列上采样和真实世界的高分辨率网格变形。&lt;h4&gt;结论&lt;/h4&gt;通过创新的方法论和技术实现，在非结构化数据中的形状变形任务中取得重要突破，为计算机视觉领域提供了新的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;生成现实的中间形态是一个挑战性的任务，特别是在处理无结构的数据时（例如点云），而这些数据在帧间缺乏时间一致性并且拓扑会发生变化。大多数插值方法是为有结构的数据设计的（即网格）, 并不适用于真实世界的点云。相比之下, 我们的方法4Deform采用神经隐式表示(NIR)来实现自由拓扑变化下的形状变形。与以往基于网格的方法不同，我们的方法学习的是欧几里得空间中的连续速度场。因此，它更适合处理较少结构化的数据，如点云。此外，我们的方法在训练过程中不需要中间形态的监督; 相反, 我们采用物理和几何约束来正则化速度场。我们使用修改过的水平集方程重构过渡表面, 将NIR直接与速度场连接起来。实验表明, 我们的这种方法显著优于先前的神经隐式表示方法，涵盖各种场景（例如噪声、部分数据、拓扑变化及非等距形状）。此外，它首次实现了新的应用如4D Kinect序列上采样和真实世界的高分辨率网格变形。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating realistic intermediate shapes between non-rigidly deformed shapesis a challenging task in computer vision, especially with unstructured data(e.g., point clouds) where temporal consistency across frames is lacking, andtopologies are changing. Most interpolation methods are designed for structureddata (i.e., meshes) and do not apply to real-world point clouds. In contrast,our approach, 4Deform, leverages neural implicit representation (NIR) to enablefree topology changing shape deformation. Unlike previous mesh-based methodsthat learn vertex-based deformation fields, our method learns a continuousvelocity field in Euclidean space. Thus, it is suitable for less structureddata such as point clouds. Additionally, our method does not requireintermediate-shape supervision during training; instead, we incorporatephysical and geometrical constraints to regularize the velocity field. Wereconstruct intermediate surfaces using a modified level-set equation, directlylinking our NIR with the velocity field. Experiments show that our methodsignificantly outperforms previous NIR approaches across various scenarios(e.g., noisy, partial, topology-changing, non-isometric shapes) and, for thefirst time, enables new applications like 4D Kinect sequence upsampling andreal-world high-resolution mesh deformation.</description>
      <author>example@mail.com (Lu Sang, Zehranaz Canfes, Dongliang Cao, Riccardo Marin, Florian Bernard, Daniel Cremers)</author>
      <guid isPermaLink="false">2502.20208v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>WaveGAS: Waveform Relaxation for Scaling Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.19986v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了两项改进，以解决GNNAutoScale（GAS）在图神经网络训练过程中遇到的历史嵌入向量陈旧和累积误差的问题。&lt;h4&gt;背景&lt;/h4&gt;随着现实世界图数据规模的不断扩大，为克服资源限制而开发了多种方法来训练图神经网络(GNNs)。其中一种方法是GNNAutoScale (GAS)，它通过图划分允许在有限GPU内存下进行训练，并且保存历史嵌入向量。&lt;h4&gt;目的&lt;/h4&gt;提出改进方案以解决由于使用陈旧的历史嵌入向量导致的近似误差累积问题，提升节点嵌入的质量和准确性。&lt;h4&gt;方法&lt;/h4&gt;[{'WaveGAS': '受波形松弛算法启发，在GAS内部进行多次前向传播来细化历史嵌入向量和梯度的估计，从而提高训练精度'}, {'梯度追踪法': '保存并利用更准确的历史梯度以提升模型在训练过程中的表现。'}]&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，WaveGAS能够增强原始的GAS方法，并且在性能上优于直接在完整图上进行训练的方法。&lt;h4&gt;结论&lt;/h4&gt;通过提出WaveGAS和改进的梯度追踪技术，该研究显著提高了节点嵌入的质量，并展示了其在复杂大规模图数据集上的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the ever-growing size of real-world graphs, numerous techniques toovercome resource limitations when training Graph Neural Networks (GNNs) havebeen developed. One such approach, GNNAutoScale (GAS), uses graph partitioningto enable training under constrained GPU memory. GAS also stores historicalembedding vectors, which are retrieved from one-hop neighbors in otherpartitions, ensuring critical information is captured across partitionboundaries. The historical embeddings which come from the previous trainingiteration are stale compared to the GAS estimated embeddings, resulting inapproximation errors of the training algorithm. Furthermore, these errorsaccumulate over multiple layers, leading to suboptimal node embeddings. Toaddress this shortcoming, we propose two enhancements: first, WaveGAS, inspiredby waveform relaxation, performs multiple forward passes within GAS before thebackward pass, refining the approximation of historical embeddings andgradients to improve accuracy; second, a gradient-tracking method that storesand utilizes more accurate historical gradients during training. Empiricalresults show that WaveGAS enhances GAS and achieves better accuracy, evenoutperforming methods that train on full graphs, thanks to its robustestimation of node embeddings.</description>
      <author>example@mail.com (Jana Vatter, Mykhaylo Zayats, Marcos Martínez Galindo, Vanessa López, Ruben Mayer, Hans-Arno Jacobsen, Hoang Thanh Lam)</author>
      <guid isPermaLink="false">2502.19986v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Contrastive Learning with Multi-view Longitudinal Data for Chest X-ray Report Generation</title>
      <link>http://arxiv.org/abs/2502.20056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了利用多视图纵向数据的增强对比学习方法，用于胸部X光报告生成（MLRG），该方法结合了当前多视图图像的空间信息和纵向数据的时间信息，并采用放射学报告中固有的时空信息对视觉和文本表示进行预训练。&lt;h4&gt;背景&lt;/h4&gt;自动化的放射科报告生成可以有效减轻放射科医生的工作负担，但大多数现有方法主要关注单视图或固定视角的图像来建模当前疾病状况，这限制了诊断准确性并忽略了疾病的进展过程。虽然有些方法利用纵向数据追踪疾病进展，但仍依赖于单一图像进行当前访问分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的对比学习框架，旨在通过结合多视图和纵向数据提高胸部X光报告生成的准确性和灵活性。&lt;h4&gt;方法&lt;/h4&gt;引入了多视图纵向对比学习的方法，该方法整合了空间信息（来自当前的多个视角图像）与时间信息（从纵向数据中获取）。同时利用放射学报告中的固有时空信息来监督视觉和文本表示的学习。此外还提出了一种标记化缺失编码技术，以灵活处理特定患者的前期知识缺失。&lt;h4&gt;主要发现&lt;/h4&gt;在MIMIC-CXR、MIMIC-ABN以及双视图CXR数据集上的实验表明，该方法优于最新的最先进的方法，在MIMIC-CXR上实现了BLEU-4的2.3%改进，在MIMIC-ABN上实现了F1分数5.5%的提高，在Two-view CXR上实现了F1 RadGraph 2.7%的进步。&lt;h4&gt;结论&lt;/h4&gt;通过利用多视图纵向数据及其内在时空信息，可以显著提升胸部X光报告生成的质量和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Automated radiology report generation offers an effective solution toalleviate radiologists' workload. However, most existing methods focusprimarily on single or fixed-view images to model current disease conditions,which limits diagnostic accuracy and overlooks disease progression. Althoughsome approaches utilize longitudinal data to track disease progression, theystill rely on single images to analyze current visits. To address these issues,we propose enhanced contrastive learning with Multi-view Longitudinal data tofacilitate chest X-ray Report Generation, named MLRG. Specifically, weintroduce a multi-view longitudinal contrastive learning method that integratesspatial information from current multi-view images and temporal informationfrom longitudinal data. This method also utilizes the inherent spatiotemporalinformation of radiology reports to supervise the pre-training of visual andtextual representations. Subsequently, we present a tokenized absence encodingtechnique to flexibly handle missing patient-specific prior knowledge, allowingthe model to produce more accurate radiology reports based on available priorknowledge. Extensive experiments on MIMIC-CXR, MIMIC-ABN, and Two-view CXRdatasets demonstrate that our MLRG outperforms recent state-of-the-art methods,achieving a 2.3% BLEU-4 improvement on MIMIC-CXR, a 5.5% F1 score improvementon MIMIC-ABN, and a 2.7% F1 RadGraph improvement on Two-view CXR.</description>
      <author>example@mail.com (Kang Liu, Zhuoqi Ma, Xiaolu Kang, Yunan Li, Kun Xie, Zhicheng Jiao, Qiguang Miao)</author>
      <guid isPermaLink="false">2502.20056v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Quantum generative classification with mixed states</title>
      <link>http://arxiv.org/abs/2502.19970v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'分类方法': '可以使用判别式或生成式的机器学习方法进行分类。', '判别式学习': '构造给定输入的输出条件概率。', '生成式学习': '构造输入和输出联合概率密度。', '生成式学习优势': '应用于无监督学习、统计推断、不确定性估计和合成数据生成。', '提出模型': '一种名为量子生成分类（QGC）的量子多类分类策略。', '方法细节': '使用变分量子算法通过混合量子态估算特征和标签的数据集联合概率密度函数。', '创新点': '引入一个称为量子增强傅里叶特征（QEFF）的量子映射，利用量子叠加来用少量量子比特在硬件中准备高维数据样本。', '理论贡献': '展示量子生成分类算法可以看作是训练数据核希尔伯特空间的一个高斯混合。', '实验验证': '开发了一种用于高维数据集生成式分类的混合量子-经典神经网络，该方法已在包括10类MNIST和Fashion-MNIST数据集在内的多个低维和高维数据集上进行了测试。', '结果表现': '证明了生成式分类策略在与其他先前量子模型的竞争中具有竞争力。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Classification can be performed using either a discriminative or a generativelearning approach. Discriminative learning consists of constructing theconditional probability of the outputs given the inputs, while generativelearning consists of constructing the joint probability density of the inputsand outputs. Although most classical and quantum methods are discriminative,there are some advantages of the generative learning approach. For instance, itcan be applied to unsupervised learning, statistical inference, uncertaintyestimation, and synthetic data generation. In this article, we present aquantum generative multiclass classification strategy, called quantumgenerative classification (QGC). This model uses a variational quantumalgorithm to estimate the joint probability density function of features andlabels of a data set by means of a mixed quantum state. We also introduce aquantum map called quantum-enhanced Fourier features (QEFF), which leveragesquantum superposition to prepare high-dimensional data samples in quantumhardware using a small number of qubits. We show that the quantum generativeclassification algorithm can be viewed as a Gaussian mixture that reproduces akernel Hilbert space of the training data. In addition, we developed a hybridquantum-classical neural network that shows that it is possible to performgenerative classification on high-dimensional data sets. The method was testedon various low- and high-dimensional data sets including the 10-class MNIST andFashion-MNIST data sets, illustrating that the generative classificationstrategy is competitive against other previous quantum models.</description>
      <author>example@mail.com (Diego H. Useche, Sergio Quiroga-Sandoval, Sebastian L. Molina, Vladimir Vargas-Calderón, Juan E. Ardila-García, Fabio A. González)</author>
      <guid isPermaLink="false">2502.19970v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>SuPreME: A Supervised Pre-training Framework for Multimodal ECG Representation Learning</title>
      <link>http://arxiv.org/abs/2502.19668v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;心血管疾病是全球死亡和残疾的主要原因，心电图（ECG）记录对于诊断和监测心脏健康至关重要。然而，获取大规模注释的ECG数据集既费时又费力。&lt;h4&gt;背景&lt;/h4&gt;现有的ECG自监督学习方法虽然减少了标签需求，但未能捕捉到精细的临床语义，并且需要大量的任务特定微调。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，提出了SuPreME（一种用于多模态ECG表示学习的监督预训练框架）。&lt;h4&gt;方法&lt;/h4&gt;SuPreME利用大型语言模型从自由文本心电图报告中提取结构化的临床实体，过滤掉噪声和不相关信息，增强临床表示学习，并构建高质量、精细化标签的数据集。通过使用基于文本的心脏查询而不是传统的分类标签，SuPreME可以在无需额外微调的情况下实现未知疾病的零样本分类。&lt;h4&gt;主要发现&lt;/h4&gt;在六种下游数据集中进行了评估，涵盖了127种心脏状况，SuPreME取得了优于现有ECG自监督学习和多模态方法的零样本AUC性能，提高了1.96%以上。结果表明，通过利用结构化、临床相关的知识可以生成高质量的心电图表示。&lt;h4&gt;结论&lt;/h4&gt;所有代码和数据将在接受后发布。&lt;h4&gt;翻译&lt;/h4&gt;心血管疾病是全球主要死因之一，心电图（ECG）记录对心脏健康诊断和监控至关重要，但由于大规模注释的ECG数据集获取困难，研究者提出了SuPreME框架。该方法采用大型语言模型提取临床实体信息，提高学习效率，并且在不进行额外训练的情况下实现未知疾病分类，展示出优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cardiovascular diseases are a leading cause of death and disabilityworldwide. Electrocardiogram (ECG) recordings are critical for diagnosing andmonitoring cardiac health, but obtaining large-scale annotated ECG datasets islabor-intensive and time-consuming. Recent ECG Self-Supervised Learning (eSSL)methods mitigate this by learning features without extensive labels but fail tocapture fine-grained clinical semantics and require extensive task-specificfine-tuning. To address these challenges, we propose $\textbf{SuPreME}$, a$\textbf{Su}$pervised $\textbf{Pre}$-training framework for$\textbf{M}$ultimodal $\textbf{E}$CG representation learning. SuPreME appliesLarge Language Models (LLMs) to extract structured clinical entities fromfree-text ECG reports, filter out noise and irrelevant content, enhanceclinical representation learning, and build a high-quality, fine-grainedlabeled dataset. By using text-based cardiac queries instead of traditionalcategorical labels, SuPreME enables zero-shot classification of unseen diseaseswithout additional fine-tuning. We evaluate SuPreME on six downstream datasetscovering 127 cardiac conditions, achieving superior zero-shot AUC performanceover state-of-the-art eSSL and multimodal methods by over 1.96\%. Resultsdemonstrate the effectiveness of SuPreME in leveraging structured, clinicallyrelevant knowledge for high-quality ECG representations. All code and data willbe released upon acceptance.</description>
      <author>example@mail.com (Mingsheng Cai, Jiuming Jiang, Wenhao Huang, Che Liu, Rossella Arcucci)</author>
      <guid isPermaLink="false">2502.19668v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Efficient and Universal Neural-Network Decoder for Stabilizer-Based Quantum Error Correction</title>
      <link>http://arxiv.org/abs/2502.19971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了基于线性注意力序列建模和图神经网络的通用量子纠错解码器，该解码器能够直接应用于各种稳定子编码结构，并在准确性和速度方面都优于专用算法。&lt;h4&gt;背景&lt;/h4&gt;量子错误校正对于大规模量子计算至关重要，但缺乏针对新编码（如量子稀疏奇偶校验码(QLDPC)）的高效解码器阻碍了其发展。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于所有稳定子编码的新通用解码器方案，以解决当前解码算法存在的问题，并提供更有效的纠错能力。&lt;h4&gt;方法&lt;/h4&gt;采用线性注意力序列建模和图神经网络技术开发了一种新的解码框架，该框架可以直接应用于任何稳定子编码的图形结构而不需要对其做结构性修改。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明新提出的解码器在各种类型（包括表面代码、颜色代码以及QLDPC等）的稳定子编码上均表现出更高的精度和更快的速度；对于Bivariate Bicycle码，当距离为12时，逻辑错误率降低了39.4%，而所需的解码时间仅占先前最佳解码器所需时间的大约1%。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种实用且通用的量子错误校正解决方案，消除了对特定代码专用解码器的需求，并有望推动量子计算技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;量子纠错对于大规模量子计算机来说至关重要。然而，缺乏针对新类型编码（如QLDPC）的有效解码器已成为阻碍其发展的瓶颈。本文介绍了一种基于线性注意力序列建模和图神经网络的通用解码器设计方法，该方法可以无缝地应用于各种稳定子编码结构中，并且在多种类型的量子纠错代码上均表现出卓越的表现力（如精度更高、速度更快）。特别值得注意的是，在Bivariate Bicycle 12距离下实现了39.4%逻辑错误率的显著降低及解码时间仅为先前最佳解码器所需时间的大约1%，证明了此方法具有实际应用价值，为未来的量子计算技术进步铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Quantum error correction is crucial for large-scale quantum computing, butthe absence of efficient decoders for new codes like quantum low-densityparity-check (QLDPC) codes has hindered progress. Here we introduce a universaldecoder based on linear attention sequence modeling and graph neural networkthat operates directly on any stabilizer code's graph structure. Our numericalexperiments demonstrate that this decoder outperforms specialized algorithms inboth accuracy and speed across diverse stabilizer codes, including surfacecodes, color codes, and QLDPC codes. The decoder maintains linear time scalingwith syndrome measurements and requires no structural modifications betweendifferent codes. For the Bivariate Bicycle code with distance 12, our approachachieves a 39.4% lower logical error rate than previous best decoders whilerequiring only ~1% of the decoding time. These results provide a practical,universal solution for quantum error correction, eliminating the need forcode-specific decoders.</description>
      <author>example@mail.com (Gengyuan Hu, Wanli Ouyang, Chao-Yang Lu, Chen Lin, Han-Sen Zhong)</author>
      <guid isPermaLink="false">2502.19971v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>An Amplitude-Encoding-Based Classical-Quantum Transfer Learning framework: Outperforming Classical Methods in Image Recognition</title>
      <link>http://arxiv.org/abs/2502.20184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 pages, 12figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了经典的量子转移学习（CQTL）方法，旨在解决当前嘈杂中等规模量子计算时代在有限数量的量子比特上训练大规模高分辨率图像数据的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的CQTL框架已经在少量参数条件下展示了量子优势，但量子神经网络对参数的数量非常敏感。目前缺少关于更大规模、更多参数量子电路的研究和探索。&lt;h4&gt;目的&lt;/h4&gt;提出了一种基于振幅编码的经典-量子转移学习（AE-CQTL）框架，并设计了两种CQTL神经网络模型：Transfer Learning Quantum Neural Network (TLQNN) 和 Transfer Learning Quantum Convolutional Neural Network (TLQCNN)，以扩大参数容量并提升性能。&lt;h4&gt;方法&lt;/h4&gt;通过多层构造来增加量子电路的参数，基于AE-CQTL框架设计实现了两个模型，并在三个基准数据集（MNIST, Fashion-MNIST and CIFAR10）和三个源模型（ResNet18, ResNet50 and DenseNet121）上进行了跨实验。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型参数容量大幅提升，从几十个扩展到超过一百个参数；在多个性能指标中超越了传统经典分类器的基准表现，包括准确率、收敛性、稳定性和泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该工作为未来更大规模量子设备上经典-量子转移学习的应用推进做出了贡献。&lt;h4&gt;翻译&lt;/h4&gt;经典的量子迁移学习（CQTL）方法被引入来解决当前嘈杂中等规模量子计算时代的问题，即在有限数量的量子比特下训练大规模、高分辨率图像数据。尽管现有的CQTL框架已经展示了少量参数下的量子优势，但量子神经网络对参数数量敏感。目前缺乏研究和探索更大规模且具有更多参数的量子电路。本文提出了基于振幅编码的经典-量子迁移学习（AE-CQTL）框架，并设计了两个CQTL神经网络模型：转移学习量子神经网络（TLQNN）和转移学习量子卷积神经网络（TLQCNN）。在三个基准数据集上进行跨实验，结果显示所提出的模型超越传统经典分类器。研究结果为进一步推进大规模量子设备上的经典-量子迁移学习应用提供了理论基础和支持。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The classical-quantum transfer learning (CQTL) method is introduced toaddress the challenge of training large-scale, high-resolution image data on alimited number of qubits (ranging from tens to hundreds) in the current NoisyIntermediate-Scale quantum (NISQ) era. existing CQTL frameworks have beendemonstrate quantum advantages with a small number of parameters (around 50),but the performance of quantum neural networks is sensitive to the number ofparameters. Currently, there is a lack of exploration into larger-scale quantumcircuits with more parameters. This paper proposes an amplitude-encoding-basedclassical-quantum transfer learning (AE-CQTL) framework, accompanied by aneffective learning algorithm. The AE-CQTL framework multiplies the parametersof quantum circuits by using multi-layer ansatz. Based on the AE-CQTLframework, we designed and implemented two CQTL neural network models: Transferlearning Quantum Neural Network (TLQNN) and Transfer Learning QuantumConvolutional Neural Network (TLQCNN). Both models significantly expand theparameter capacity of quantum circuits, elevating the parameter scale from afew dozen to over one hundred parameters. In cross-experiments with threebenchmark datasets (MNIST, Fashion-MNIST and CIFAR10) and three source models(ResNet18, ResNet50 and DenseNet121), TLQNN and TLQCNN have exceeded thebenchmark classical classifier in multiple performance metrics, includingaccuracy, convergence, stability, and generalization capability. Our workcontributes to advancing the application of classical-quantum transfer learningon larger-scale quantum devices in future.</description>
      <author>example@mail.com (Shouwei Hu, Xi Li, Banyao Ruan, Zhihao Liu)</author>
      <guid isPermaLink="false">2502.20184v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>High-fidelity Multiphysics Modelling for Rapid Predictions Using Physics-informed Parallel Neural Operator</title>
      <link>http://arxiv.org/abs/2502.19543v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 11 figures, 1 table, 36 equations&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新型的物理信息并行神经算子（PIPNO）框架，用于解决由非线性和强耦合偏微分方程描述的复杂多物理系统的建模难题。&lt;h4&gt;背景&lt;/h4&gt;传统数值求解器在处理高度计算成本的问题时存在挑战，限制了它们在大规模应用中的实用性。数据驱动训练依赖于神经算子，在实际场景中由于缺乏或难以获取数据而适用性受限。&lt;h4&gt;目的&lt;/h4&gt;开发一种利用支配物理定律进行无监督学习的框架，以实现无需数据支持即可建立偏微分方程模型的目标。&lt;h4&gt;方法&lt;/h4&gt;引入并行核积分设计，结合集合学习的方法，极大地提升了算子学习中的兼容性和计算效率，使得非线性和强耦合偏微分方程的学习成为可能。该方法适用于地技术工程、材料科学、电磁学、量子力学和流体动力学等多个领域的复杂物理问题。&lt;h4&gt;主要发现&lt;/h4&gt;PIPNO能够在高度复杂的多物理系统建模中实现高保真度和快速预测，优于现有的算子学习方法。&lt;h4&gt;结论&lt;/h4&gt;PIPNO为传统的求解器提供了一种强有力的替代方案，扩展了神经算子在多物理模型中的适用性，并保证了效率、鲁棒性和可扩展性。&lt;h4&gt;翻译&lt;/h4&gt;建立复杂的多物理系统模型是计算科学和工程的重要基础。此类系统的数学描述往往涉及非线性和强耦合的偏微分方程（PDEs）。传统数值解法在处理这类问题时面临高计算成本的问题，使得它们难以应用于大规模的实际应用中。数据驱动训练方法依赖于大量高质量的数据进行神经算子的学习，在实际场景中因为获取数据的成本或难度而受限。文中提出了一种新的框架物理信息并行神经算子（PIPNO），这是一个可扩展且无需监督学习的框架，仅依靠支配的物理定律即可构建偏微分方程模型。其设计包括了并行核积分和集合学习的方法，显著提升了算子学习中的兼容性和计算效率，使得对非线性及强耦合偏微分方程的学习成为可能。PIPNO能够在地技术工程、材料科学、电磁学、量子力学以及流体动力学等多个领域中高效捕捉不同物理现象之间的非线性操作映射关系，并在模型预测的速度和准确性方面优于现有算子学习方法，为解决多物理系统的建模难题提供了一种新的有效途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modelling complex multiphysics systems governed by nonlinear and stronglycoupled partial differential equations (PDEs) is a cornerstone in computationalscience and engineering. However, it remains a formidable challenge fortraditional numerical solvers due to high computational cost, making themimpractical for large-scale applications. Neural operators' reliance ondata-driven training limits their applicability in real-world scenarios, asdata is often scarce or expensive to obtain. Here, we propose a novel paradigm,physics-informed parallel neural operator (PIPNO), a scalable and unsupervisedlearning framework that enables data-free PDE modelling by leveraging onlygoverning physical laws. The parallel kernel integration design, incorporatingensemble learning, significantly enhances both compatibility and computationalefficiency, enabling scalable operator learning for nonlinear and stronglycoupled PDEs. PIPNO efficiently captures nonlinear operator mappings acrossdiverse physics, including geotechnical engineering, material science,electromagnetism, quantum mechanics, and fluid dynamics. The proposed methodachieves high-fidelity and rapid predictions, outperforming existing operatorlearning approaches in modelling nonlinear and strongly coupled multiphysicssystems. Therefore, PIPNO offers a powerful alternative to conventionalsolvers, broadening the applicability of neural operators for multiphysicsmodelling while ensuring efficiency, robustness, and scalability.</description>
      <author>example@mail.com (Biao Yuan, He Wang, Yanjie Song, Ana Heitor, Xiaohui Chen)</author>
      <guid isPermaLink="false">2502.19543v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Noise-Injected Spiking Graph Convolution for Energy-Efficient 3D Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2502.19660v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种注入噪声的尖峰图卷积网络，用于提升脉冲神经网络（SNN）在三维点云去噪中的回归性能。&lt;h4&gt;背景&lt;/h4&gt;脉冲神经网络由于其优越的能量效率，在二维分类任务中优于传统人工神经网络。然而，在三维点云处理方面，特别是回归任务上，SNN的潜力尚未充分探索。&lt;h4&gt;目的&lt;/h4&gt;设计一种注入噪声的尖峰图卷积方法以增强3D点云去噪能力，并展示了两种基于SNN的去噪网络性能。&lt;h4&gt;方法&lt;/h4&gt;首先模拟了带有噪声的神经元动力学，构建出带噪声的脉冲神经元。然后在此基础上设计了注入噪声的脉冲图卷积层，促进对三维数据扰动感知下的尖峰表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;提出了一种全新的SNN架构用于3D点云去噪任务，并展示了其相比基于ANN的模型而言，可以显著降低能耗的同时保持较低的精度损失。此外还设计了一个结合了深度学习方法与高效能特点的混合架构。&lt;h4&gt;结论&lt;/h4&gt;这项工作揭示了脉冲神经网络在三维数据处理方面的潜力，为探索部署于类脑芯片以及开发高能效3D数据采集设备铺平道路。&lt;h4&gt;翻译&lt;/h4&gt;受生物神经系统中尖峰计算范式的启发，脉冲神经网络（SNN）在二维分类任务中的能量效率优于传统人工神经网络（ANN）。然而，在三维点云处理方面，特别是回归任务上，SNN的潜力尚未充分探索。本文提出了一种注入噪声的尖峰图卷积网络来最大化SNN在3D点云去噪中的回归潜能。具体而言，我们首先模拟了带有噪声的神经元动力学以构建带噪声的脉冲神经元。基于此基础，设计了促进三维数据扰动感知下的尖峰表示学习的注入噪声的脉冲图卷积层。从脉冲图卷积出发，建立了两个SNN去噪网络：一个是纯脉冲图卷积网络，在两个基准数据集PU-Net和PC-Net上相比一些基于ANN的方法显示出较低的精度损失同时显著减少了能量消耗；另一个是混合架构，结合了深度学习方法并在仅几步时间步骤中就实现了高效能。这项工作揭示了SNN在三维点云去噪中的潜力，并为探索类脑芯片上的部署以及开发高能效3D数据采集设备铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Spiking neural networks (SNNs), inspired by the spiking computation paradigmof the biological neural systems, have exhibited superior energy efficiency in2D classification tasks over traditional artificial neural networks (ANNs).However, the regression potential of SNNs has not been well explored,especially in 3D point cloud processing.In this paper, we proposenoise-injected spiking graph convolutional networks to leverage the fullregression potential of SNNs in 3D point cloud denoising. Specifically, wefirst emulate the noise-injected neuronal dynamics to build noise-injectedspiking neurons. On this basis, we design noise-injected spiking graphconvolution for promoting disturbance-aware spiking representation learning on3D points. Starting from the spiking graph convolution, we build two SNN-baseddenoising networks. One is a purely spiking graph convolutional network, whichachieves low accuracy loss compared with some ANN-based alternatives, whileresulting in significantly reduced energy consumption on two benchmarkdatasets, PU-Net and PC-Net. The other is a hybrid architecture that combinesANN-based learning with a high performance-efficiency trade-off in just a fewtime steps. Our work lights up SNN's potential for 3D point cloud denoising,injecting new perspectives of exploring the deployment on neuromorphic chipswhile paving the way for developing energy-efficient 3D data acquisitiondevices.</description>
      <author>example@mail.com (Zikuan Li, Qiaoyun Wu, Jialin Zhang, Kaijun Zhang, Jun Wang)</author>
      <guid isPermaLink="false">2502.19660v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Mixture of Experts for Recognizing Depression from Interview and Reading Tasks</title>
      <link>http://arxiv.org/abs/2502.20213v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种利用混合专家模型（MoE）来识别抑郁的新型深度神经网络方法，它同时考虑自发和朗读语音，并使用多模态融合技术。&lt;h4&gt;背景&lt;/h4&gt;抑郁症是一种精神疾病，能够引起心理、生理和社会方面的多种症状。研究表明，言语是早期识别抑郁症的一个客观标志。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的机器学习方法，通过分析人类的自发性对话和阅读任务中的音频数据来更准确地识别抑郁症。&lt;h4&gt;方法&lt;/h4&gt;本研究首次在抑郁检测任务中利用混合专家模型（MoE），将来自两种不同类型的语音的数据结合起来。它使用了音频文件对应于访谈任务和朗读任务，并将其转换为log-Mel频谱图，然后通过共享的AlexNet模型处理图像表示，最后输出向量通过MoE模块。&lt;h4&gt;主要发现&lt;/h4&gt;该研究采用了三种MoE变体：稀疏门控混合专家（Sparsely-gated MoE）和基于分解的多线性混合专家（Multilinear MoE），在Androids语料库上的实验中，达到了87.00%的准确率和86.66%的F1分数。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种新颖且有效的方法来识别抑郁症，通过利用自发性和朗读语音的数据，并应用先进的多模态融合技术。这种方法在Androids语料库上的实验中表现出了很高的准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一个创新的研究工作，旨在开发一个能够同时使用自发性对话和阅读任务中的音频数据的深度学习模型来识别抑郁症。该方法使用混合专家（MoE）框架，结合了多模态融合技术，以提高对抑郁症状检测的精度。实验结果表明，在Androids语料库上该模型达到了87.00%的准确率和86.66%的F1分数，显示出在识别抑郁症方面的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Depression is a mental disorder and can cause a variety of symptoms,including psychological, physical, and social. Speech has been proved anobjective marker for the early recognition of depression. For this reason, manystudies have been developed aiming to recognize depression through speech.However, existing methods rely on the usage of only the spontaneous speechneglecting information obtained via read speech, use transcripts which areoften difficult to obtain (manual) or come with high word-error rates(automatic), and do not focus on input-conditional computation methods. Toresolve these limitations, this is the first study in depression recognitiontask obtaining representations of both spontaneous and read speech, utilizingmultimodal fusion methods, and employing Mixture of Experts (MoE) models in asingle deep neural network. Specifically, we use audio files corresponding toboth interview and reading tasks and convert each audio file into log-Melspectrogram, delta, and delta-delta. Next, the image representations of the twotasks pass through shared AlexNet models. The outputs of the AlexNet models aregiven as input to a multimodal fusion method. The resulting vector is passedthrough a MoE module. In this study, we employ three variants of MoE, namelysparsely-gated MoE and multilinear MoE based on factorization. Findings suggestthat our proposed approach yields an Accuracy and F1-score of 87.00% and 86.66%respectively on the Androids corpus.</description>
      <author>example@mail.com (Loukas Ilias, Dimitris Askounis)</author>
      <guid isPermaLink="false">2502.20213v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Rethinking Multimodal Learning from the Perspective of Mitigating Classification Ability Disproportion</title>
      <link>http://arxiv.org/abs/2502.20120v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的多模态学习方法，通过设计持续增强算法来动态平衡弱模态和强模态的分类能力，从而克服了现有方法在处理模式不平衡时的局限性。&lt;h4&gt;背景&lt;/h4&gt;尽管多模态学习已经取得了显著的进步，但存在的模态失衡问题阻碍了其在实践中超越单模态模型的优势。主流多模态学习方法主要关注于平衡学习过程，然而这些方法没有明确地增强较弱模态的分类能力，导致性能提升有限。&lt;h4&gt;目的&lt;/h4&gt;设计一种持续增强算法以动态平衡强模态和弱模态之间的分类能力，从而缓解模式失衡问题。&lt;h4&gt;方法&lt;/h4&gt;提出了一种在多模态学习中同时优化分类误差和残差误差的设计可配置分类器模块的持续增强算法。进一步提出了一个自适应分类器分配策略，以动态提升弱模态的分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了该方法的有效性，并展示了与现有最先进的多模态学习基线相比的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在缓解模式不平衡问题方面表现出色，能够有效提高多模态模型的整体性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although multimodal learning~(MML) has garnered remarkable progress, theexistence of modality imbalance hinders multimodal learning from achieving itsexpected superiority over unimodal models in practice. To overcome this issue,mainstream multimodal learning methods have placed greater emphasis onbalancing the learning process. However, these approaches do not explicitlyenhance the classification ability of weaker modalities, leading to limitedperformance promotion. By designing a sustained boosting algorithm, we proposea novel multimodal learning approach to dynamically balance the classificationability of weak and strong modalities. Concretely, we first propose a sustainedboosting algorithm in multimodal learning by simultaneously optimizing theclassification and residual errors using a designed configurable classifiermodule. Then, we propose an adaptive classifier assignment strategy todynamically facilitate the classification performance of weak modality. To thisend, the classification ability of strong and weak modalities is expected to bebalanced, thereby mitigating the imbalance issue. Empirical experiments onwidely used datasets reveal the superiority of our method through comparisonwith various state-of-the-art~(SoTA) multimodal learning baselines.</description>
      <author>example@mail.com (QingYuan Jiang, Longfei Huang, Yang Yang)</author>
      <guid isPermaLink="false">2502.20120v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Learning Mask Invariant Mutual Information for Masked Image Modeling</title>
      <link>http://arxiv.org/abs/2502.19718v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的视角来理解Masked Autoencoders (MAEs) 的工作原理，通过信息瓶颈原则来分析和优化它们。引入了MI-MAE方法，该方法利用互信息最大化和最小化策略优化MAEs的潜在特征以提高性能。&lt;h4&gt;背景&lt;/h4&gt;Masked autoencoders在计算机视觉中的自监督学习领域非常突出，尽管有实证成功但其底层机制还不完全被理解。&lt;h4&gt;目的&lt;/h4&gt;通过理论分析揭示了平衡相关和不相关信息对于改进MAE性能的关键作用，并提出了一个基于信息瓶颈原理的新方法MI-MAE来优化潜在特征。&lt;h4&gt;方法&lt;/h4&gt;利用互信息最大化的技术，增强潜在特征以保留与输出的最大相关性；同时减少潜在特征与输入的无关信息。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验显示，所提出的MI-MAE在图像分类、目标检测和语义分割等任务中均显著优于原始MAE模型。这验证了理论框架的有效性和基于信息瓶颈原则优化自监督学习模型的实际优势。&lt;h4&gt;结论&lt;/h4&gt;该研究为开发更强大的自监督学习模型提供了更深的见解，强调了将信息瓶颈原则应用于MAEs的重要性。&lt;h4&gt;翻译&lt;/h4&gt;掩膜自动编码器（Masked Autoencoders，简称MAE）代表了一种在计算机视觉中自监督学习领域非常突出的方法。尽管它们在实验上表现出色，但其背后的机制尚未被充分理解。最近的研究试图通过对比学习和特征表示分析来阐明MAEs的工作原理，但是这些方法往往只能提供间接的见解。本文提出一种新的视角以信息论中的信息瓶颈原则为基础来理解MAE，并且理论分析揭示了优化潜在特征以平衡相关和不相关信息是提高MAE性能的关键。基于我们的证明，我们引入了一种名为MI-MAE的新方法，该方法通过互信息最大化和最小化策略优化掩膜自动编码器。通过增强潜在特征以保留与输出的最大关联性，并减少与输入的无关信息，我们的方法实现了更好的效果。在标准基准上的广泛实验表明，在图像分类、目标检测和语义分割等任务中，MI-MAE显著优于传统Masked Autoencoder模型。这些发现验证了理论框架的有效性，并突出了将信息瓶颈原则应用于掩膜自动编码器的实际优势，为开发更强大的自监督学习模型提供了深入的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked autoencoders (MAEs) represent a prominent self-supervised learningparadigm in computer vision. Despite their empirical success, the underlyingmechanisms of MAEs remain insufficiently understood. Recent studies haveattempted to elucidate the functioning of MAEs through contrastive learning andfeature representation analysis, yet these approaches often provide onlyimplicit insights. In this paper, we propose a new perspective forunderstanding MAEs by leveraging the information bottleneck principle ininformation theory. Our theoretical analyses reveal that optimizing the latentfeatures to balance relevant and irrelevant information is key to improving MAEperformance. Building upon our proofs, we introduce MI-MAE, a novel method thatoptimizes MAEs through mutual information maximization and minimization. Byenhancing latent features to retain maximal relevant information between themand the output, and minimizing irrelevant information between them and theinput, our approach achieves better performance. Extensive experiments onstandard benchmarks show that MI-MAE significantly outperforms MAE models intasks such as image classification, object detection, and semanticsegmentation. Our findings validate the theoretical framework and highlight thepractical advantages of applying the information bottleneck principle to MAEs,offering deeper insights for developing more powerful self-supervised learningmodels.</description>
      <author>example@mail.com (Tao Huang, Yanxiang Ma, Shan You, Chang Xu)</author>
      <guid isPermaLink="false">2502.19718v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>GraphSparseNet: a Novel Method for Large Scale Trafffic Flow Prediction</title>
      <link>http://arxiv.org/abs/2502.19823v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;交通流预测是一项关键的时空数据挖掘任务，在智能路线规划和动态交通管理中具有广泛的应用。近年来，深度学习技术特别是图神经网络（GNNs）在提高这些预测准确度方面取得了显著进展，通过捕捉复杂的时空动态特性来实现这一点。&lt;h4&gt;背景&lt;/h4&gt;交通流量预测是一个重要的时空数据分析领域，最近的研究利用深度学习方法尤其是基于图的模型大幅提高了其准确性。然而，现有的图神经网络面临的一个主要问题是随着图形中节点数量增加而产生的计算复杂度呈指数级增长。&lt;h4&gt;目的&lt;/h4&gt;本文介绍了一种称为GraphSparseNet (GSNet)的新框架，旨在同时提高GNN在交通预测中的可扩展性和准确率。&lt;h4&gt;方法&lt;/h4&gt;该框架由两个核心模块构成：特征提取器和关系压缩器。这些模块具有线性的时间和空间复杂度，这使得整个模型的计算复杂度减少到线性级别。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，GraphSparseNet不仅比最先进的线性模型将训练时间减少了3.51倍，而且还保持了高预测性能。&lt;h4&gt;结论&lt;/h4&gt;GSNet框架为解决GNN可扩展性和准确性问题提供了一种新的解决方案，并展示了在实际数据集上的优越表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traffic flow forecasting is a critical spatio-temporal data mining task withwide-ranging applications in intelligent route planning and dynamic trafficmanagement. Recent advancements in deep learning, particularly through GraphNeural Networks (GNNs), have significantly enhanced the accuracy of theseforecasts by capturing complex spatio-temporal dynamics. However, thescalability of GNNs remains a challenge due to their exponential growth inmodel complexity with increasing nodes in the graph. Existing methods toaddress this issue, including sparsification, decomposition, and kernel-basedapproaches, either do not fully resolve the complexity issue or riskcompromising predictive accuracy. This paper introduces GraphSparseNet (GSNet),a novel framework designed to improve both the scalability and accuracy ofGNN-based traffic forecasting models. GraphSparseNet is comprised of two coremodules: the Feature Extractor and the Relational Compressor. These modulesoperate with linear time and space complexity, thereby reducing the overallcomputational complexity of the model to a linear scale. Our extensiveexperiments on multiple real-world datasets demonstrate that GraphSparseNet notonly significantly reduces training time by 3.51x compared to state-of-the-artlinear models but also maintains high predictive performance.</description>
      <author>example@mail.com (Weiyang Kong, Kaiqi Wu, Sen Zhang, Yubao Liu)</author>
      <guid isPermaLink="false">2502.19823v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning in Latent Contextual Bandits with Covariate Shift Through Causal Transportability</title>
      <link>http://arxiv.org/abs/2502.20153v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the Conference of Causal Learning and Reasoning (CLeaR  2025), will be published in the Proceedings of Machine Learning Research&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了智能系统中知识从一个环境转移到另一个环境的能力，特别是在多臂赌博机框架下的因果推理视角。研究关注潜在上下文下的转移学习，并考虑跨环境的条件变化（即协变量偏移）。文章提出了一种基于因果推断运输理论的方法来开发算法，这些算法能够有效地在目标环境中转移知识。&lt;h4&gt;背景&lt;/h4&gt;在两个不同的环境中直接迁移所有知识可能会导致性能下降，这种现象被称为负向迁移。该问题需要通过更精细的知识迁移策略解决。&lt;h4&gt;目的&lt;/h4&gt;研究旨在提出一种有效的学习框架，以便智能系统可以更好地处理跨环境下的负向迁移，并且在这种情况下能够有效地进行知识转移。&lt;h4&gt;方法&lt;/h4&gt;文章使用因果推理理论中的可传输性理论来开发算法。利用变分自动编码器在高维代理存在的情况下近似因果效果。研究测试了这些算法在合成和半合成数据集上的性能，与基准算法相比，结果表明该框架具有持续改进的学习效率。&lt;h4&gt;主要发现&lt;/h4&gt;经典多臂赌博机算法下的直接知识转移会导致负向迁移。利用运输理论进行有效的知识转移可以提高目标环境中的学习效率。&lt;h4&gt;结论&lt;/h4&gt;本文提出的基于因果推断的框架在处理跨环境的知识转移问题上是有效且高效的，尤其是当存在高维代理时。该方法相对于基准算法展示了持续改进的学习性能，并为智能系统如何更好地从一个环境迁移到另一个环境中学习提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;知识迁移是智能系统的必备能力之一。然而，在两个不同的环境下，直接转移所有知识可能导致性能下降（负向迁移）。本研究在多臂赌博机框架下探讨了该问题，特别是针对潜在上下文下的知识转移，并考虑到了环境变化的影响（协变量偏移）。通过应用因果推理理论中的可传输性原则来开发有效的算法，这些算法旨在有效估计目标环境中感兴趣的因果效应。此外，利用变分自动编码器来处理高维代理情况下的近似因果效果。测试结果表明，在合成和半合成数据集上，该方法相对于基准算法表现出更优的学习效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transferring knowledge from one environment to another is an essentialability of intelligent systems. Nevertheless, when two environments aredifferent, naively transferring all knowledge may deteriorate the performance,a phenomenon known as negative transfer. In this paper, we address this issuewithin the framework of multi-armed bandits from the perspective of causalinference. Specifically, we consider transfer learning in latent contextualbandits, where the actual context is hidden, but a potentially high-dimensionalproxy is observable. We further consider a covariate shift in the contextacross environments. We show that naively transferring all knowledge forclassical bandit algorithms in this setting led to negative transfer. We thenleverage transportability theory from causal inference to develop algorithmsthat explicitly transfer effective knowledge for estimating the causal effectsof interest in the target environment. Besides, we utilize variationalautoencoders to approximate causal effects under the presence of ahigh-dimensional proxy. We test our algorithms on synthetic and semi-syntheticdatasets, empirically demonstrating consistently improved learning efficiencyacross different proxies compared to baseline algorithms, showing theeffectiveness of our causal framework in transferring knowledge.</description>
      <author>example@mail.com (Mingwei Deng, Ville Kyrki, Dominik Baumann)</author>
      <guid isPermaLink="false">2502.20153v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>CFTrack: Enhancing Lightweight Visual Tracking through Contrastive Learning and Feature Matching</title>
      <link>http://arxiv.org/abs/2502.19705v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为CFTrack的轻量级追踪器，该追踪器结合了对比学习和特征匹配技术，增强了区分能力。&lt;h4&gt;背景&lt;/h4&gt;在移动设备和边缘计算设备上进行高效的视觉跟踪是一个挑战，特别是当这些设备资源受限时。传统的轻量级追踪器难以应对遮挡和干扰问题，而深度学习方法压缩后性能会下降。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的局限性，提出了一种新的轻量级追踪算法CFTrack，旨在解决在计算资源有限的情况下提高跟踪精度的问题。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了一个新颖的对比特征匹配模块（contrastive feature matching module），通过自适应对比损失优化目标相似度动态评估过程。这个模块与传统的特征匹配相结合，形成了改进后的轻量级追踪器CFTrack。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，CFTrack在LaSOT、OTB100和UAV123数据集上优于许多最新的轻量级追踪器，在NVIDIA Jetson NX平台上可以达到每秒136帧的性能。进一步的研究表明，CFTrack具有强大的区分能力，并且在HOOT数据集中重遮挡的情况下表现尤为出色。&lt;h4&gt;结论&lt;/h4&gt;CFTrack通过引入对比学习和特征匹配技术克服了传统轻量级跟踪器的缺点，能够在资源有限的情况下提供高精度跟踪结果。&lt;h4&gt;翻译&lt;/h4&gt;实现视觉追踪中的高效性和强大的区分能力是一个挑战，尤其是在计算资源受限的手持设备上。传统的轻量级追踪器在遮挡和干扰情况下的鲁棒性不足，而深度学习方法压缩后会性能下降。本文提出了一种名为CFTrack的跟踪器，它结合了对比学习和特征匹配技术来增强区分性的特征表示能力。通过自适应对比损失优化的新颖对比特性匹配模块，CFTrack能够在预测期间动态评估目标相似度，并提高了追踪精度。实验表明，在LaSOT、OTB100和UAV123数据集上，CFTrack优于许多最新的轻量级跟踪器，在NVIDIA Jetson NX平台上可以达到每秒136帧的性能。在HOOT数据集中，进一步证实了CFTrack在严重遮挡情况下的强大区分能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving both efficiency and strong discriminative ability in lightweightvisual tracking is a challenge, especially on mobile and edge devices withlimited computational resources. Conventional lightweight trackers oftenstruggle with robustness under occlusion and interference, while deep trackers,when compressed to meet resource constraints, suffer from performancedegradation. To address these issues, we introduce CFTrack, a lightweighttracker that integrates contrastive learning and feature matching to enhancediscriminative feature representations. CFTrack dynamically assesses targetsimilarity during prediction through a novel contrastive feature matchingmodule optimized with an adaptive contrastive loss, thereby improving trackingaccuracy. Extensive experiments on LaSOT, OTB100, and UAV123 show that CFTracksurpasses many state-of-the-art lightweight trackers, operating at 136 framesper second on the NVIDIA Jetson NX platform. Results on the HOOT datasetfurther demonstrate CFTrack's strong discriminative ability under heavyocclusion.</description>
      <author>example@mail.com (Juntao Liang, Jun Hou, Weijun Zhang, Yong Wang)</author>
      <guid isPermaLink="false">2502.19705v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>cMIM: A Contrastive Mutual Information Framework for Unified Generative and Discriminative Representation Learning</title>
      <link>http://arxiv.org/abs/2502.19642v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  A working draft&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新的对比互信息机(cMIM)模型，旨在提升表示学习在未知下游任务中的实用性。&lt;h4&gt;背景&lt;/h4&gt;表示学习的一个基本挑战是学习对未见下游任务有用的表示。目前该领域的主流方法包括对比学习、自监督掩码和去噪自动编码器。&lt;h4&gt;目的&lt;/h4&gt;为了增强所学表示对于下游任务的适用性，提出了cMIM方法，直接解决了现有Mutual Information Machine (MIM)模型在区分下游任务表现不佳的问题。&lt;h4&gt;方法&lt;/h4&gt;cMIM将新的对比学习损失函数与互信息机(MIM)学习框架集成在一起。cMIM不仅消除了数据增强的需求，并且对负样本数量（即批量大小）的变化具有鲁棒性；另外还引入了一种通用的方法从编码器-解码器模型中提取有用的嵌入，显著提高了在区分下游任务中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的cMIM方法不仅解决了现有MIM模型表示对于区分下游任务适用性的不足，而且还提供了一个统一的生成模型，该模型对生成和区分性任务都很有效。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，通过cMIM学习到的表示可以为下游任务提供价值的同时保持了MIM的生成能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning representations that are useful for unknown downstream tasks is afundamental challenge in representation learning. Prominent approaches in thisdomain include contrastive learning, self-supervised masking, and denoisingauto-encoders. In this paper, we introduce a novel method, termed contrastiveMutual Information Machine (cMIM), which aims to enhance the utility of learnedrepresentations for downstream tasks. cMIM integrates a new contrastivelearning loss with the Mutual Information Machine (MIM) learning framework, aprobabilistic auto-encoder that maximizes the mutual information between inputsand latent representations while clustering the latent codes. Despite MIM'spotential, initial experiments indicated that the representations learned byMIM were less effective for discriminative downstream tasks compared tostate-of-the-art (SOTA) models. The proposed cMIM method directly addressesthis limitation.  The main contributions of this work are twofold: (1) We propose a novelcontrastive extension to MIM for learning discriminative representations whicheliminates the need for data augmentation and is robust to variations in thenumber of negative examples (i.e., batch size). (2) We introduce a genericmethod for extracting informative embeddings from encoder-decoder models, whichsignificantly improves performance in discriminative downstream tasks withoutrequiring additional training. This method is applicable to any pre-trainedencoder-decoder model.  By presenting cMIM, we aim to offer a unified generative model that iseffective for both generative and discriminative tasks. Our results demonstratethat the learned representations are valuable for downstream tasks whilemaintaining the generative capabilities of MIM.</description>
      <author>example@mail.com (Micha Livne)</author>
      <guid isPermaLink="false">2502.19642v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>DGFM: Full Body Dance Generation Driven by Music Foundation Models</title>
      <link>http://arxiv.org/abs/2502.20176v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the Audio Imagination Workshop of NeurlPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于扩散模型的音乐驱动舞蹈动作生成方法，该方法结合了高级音乐基础模型和手工制作特征来提升生成舞蹈序列的质量。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数音乐驱动舞蹈动作生成方法依赖于手工制作的特征，并未充分利用音乐基础模型对跨模态内容生成的影响。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一不足，本研究提出了一种基于扩散的方法，该方法可以根据文本和音乐生成舞蹈动作。&lt;h4&gt;方法&lt;/h4&gt;通过结合由音乐基础模型获得的高级特性与手工制作的特性来提取音乐特征。此方法能有效利用高级语义信息和低级时间细节的优势，提高模型理解音乐特征的能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在生成最逼真的舞蹈序列方面优于四个音乐基础模型和两组手工制作的音乐特征，并且与输入音乐匹配度最佳。&lt;h4&gt;结论&lt;/h4&gt;通过将高级语义信息和低级时间细节相结合，可以显著提高基于文本和音乐的舞蹈动作生成效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In music-driven dance motion generation, most existing methods usehand-crafted features and neglect that music foundation models have profoundlyimpacted cross-modal content generation. To bridge this gap, we propose adiffusion-based method that generates dance movements conditioned on text andmusic. Our approach extracts music features by combining high-level featuresobtained by music foundation model with hand-crafted features, therebyenhancing the quality of generated dance sequences. This method effectivelyleverages the advantages of high-level semantic information and low-leveltemporal details to improve the model's capability in music featureunderstanding. To show the merits of the proposed method, we compare it withfour music foundation models and two sets of hand-crafted music features. Theresults demonstrate that our method obtains the most realistic dance sequencesand achieves the best match with the input music.</description>
      <author>example@mail.com (Xinran Liu, Zhenhua Feng, Diptesh Kanojia, Wenwu Wang)</author>
      <guid isPermaLink="false">2502.20176v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and Scalable Graph Neural Networks via Message Invariance</title>
      <link>http://arxiv.org/abs/2502.19693v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于消息传递的图神经网络（GNN）在许多实际应用中取得了巨大成功。然而，对于采样的目标节点小批量来说，从外部节点到内部节点的消息传递导致了随着层数增加而指数级增长的计算成本。&lt;h4&gt;背景&lt;/h4&gt;现有方法中的消息传递过程分为两部分：同一小批量内的节点间消息传递（MP-IB）和从小批量外向内节点的消息传递（MP-OB）。MP-OB依赖于更高阶的小批量外部邻居，导致了随着层数增加而指数级增长的计算成本。&lt;h4&gt;目的&lt;/h4&gt;提出一种准确且快速的大图归纳学习小批量方法——拓扑补偿(TOP)，以解决因消息传递过程导致的大规模图形中节点和边过多存储在GPU上的问题。&lt;h4&gt;方法&lt;/h4&gt;TOP通过引入消息不变性概念，将昂贵的MP-OB转化为快速的MP-IB。这保证了修改后的MP-IB与整个消息传递具有相同的输出结果。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在大规模图（数百万节点和数十亿边）上，TOP比现有的小批量方法快几个数量级，并且精度下降有限。&lt;h4&gt;结论&lt;/h4&gt;通过避免昂贵的MP-OB计算成本，TOP使得GNN在大规模图形中变得更加可行。&lt;h4&gt;翻译&lt;/h4&gt;基于消息传递的图神经网络（GNNs）在许多实际应用中取得了巨大成功。对于采样的目标节点小批量来说，消息传递过程分为两部分：同一小批量内的节点间消息传递（MP-IB）和从小批量外向内节点的消息传递（MP-OB）。然而，由于邻域爆炸问题，整个消息传递过程中需要在GPU上存储大部分节点和边。为解决这一挑战，我们提出了一种针对大规模图归纳学习的准确且快速的小批量方法——拓扑补偿(TOP)，该方法仅通过MP-IB就能获得整个消息传递的结果而无需昂贵的MP-OB计算成本。TOP的核心在于引入了一个新的概念——消息不变性，它定义了将昂贵的MP-OB转换为快速MP-IB的消息不变变换。这确保了修改后的MP-IB与整个消息传递具有相同的输出结果。实验表明，在包含数百万节点和数十亿边的大规模图上，TOP比现有的小批量方法快几个数量级，并且精度下降有限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Message passing-based graph neural networks (GNNs) have achieved greatsuccess in many real-world applications. For a sampled mini-batch of targetnodes, the message passing process is divided into two parts: message passingbetween nodes within the batch (MP-IB) and message passing from nodes outsidethe batch to those within it (MP-OB). However, MP-OB recursively relies onhigher-order out-of-batch neighbors, leading to an exponentially growingcomputational cost with respect to the number of layers. Due to the neighborexplosion, the whole message passing stores most nodes and edges on the GPUsuch that many GNNs are infeasible to large-scale graphs. To address thischallenge, we propose an accurate and fast mini-batch approach for large graphtransductive learning, namely topological compensation (TOP), which obtains theoutputs of the whole message passing solely through MP-IB, without the costlyMP-OB. The major pillar of TOP is a novel concept of message invariance, whichdefines message-invariant transformations to convert costly MP-OB into fastMP-IB. This ensures that the modified MP-IB has the same output as the wholemessage passing. Experiments demonstrate that TOP is significantly faster thanexisting mini-batch methods by order of magnitude on vast graphs (millions ofnodes and billions of edges) with limited accuracy degradation.</description>
      <author>example@mail.com (Zhihao Shi, Jie Wang, Zhiwei Zhuang, Xize Liang, Bin Li, Feng Wu)</author>
      <guid isPermaLink="false">2502.19693v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Spatial-Spectral Diffusion Contrastive Representation Network for Hyperspectral Image Classification</title>
      <link>http://arxiv.org/abs/2502.19699v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于去噪扩散概率模型（DDPM）结合对比学习（CL）的新型网络DiffCRN，用于高光谱图像分类（HSIC），旨在改进空间-光谱特征表示、无监督特征学习效率、时间步长选择和特征融合与分类。&lt;h4&gt;背景&lt;/h4&gt;在对高光谱图像进行有效提取具有区分性的空间-光谱特征时面临挑战，主要由于空间-光谱异质性和噪声效应等因素导致难以实现高效的空间-光谱特征表示。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的网络结构DiffCRN来改进高光谱图像分类中的空间-光谱特征学习效率和无监督特性提取。&lt;h4&gt;方法&lt;/h4&gt;{'架构设计': '采用具有空间自注意去噪模块（SSAD）和光谱组自注意力去噪模块（SGSAD）的分阶段架构，代替常用的UNet-like结构。', '改进损失函数': '设计新的DDPM模型结合对数绝对误差（LAE）损失和对比学习以提高损失函数的有效性和增强实例级别和类间区分性。', '时间步长选择': '引入基于像素级光谱角度映射（SAM）的可学习方法，自适应自动地为提出的DDPM模型选择合适的时间步骤。', '特征融合与分类': '设计了自适应加权添加模块（AWAM）和跨时间步空间-光谱融合模块（CTSSFM）以融合按时间步划分的特性并进行分类。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，提出的DiffCRN模型在四个广泛使用的高光谱数据集上相比经典基础模型、最新的GAN、Transformer模型和其他预训练方法具有更好的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效提高HSIC任务中的空间-光谱特征表示能力和无监督特性提取的效率，为该领域提供了一种新的解决方案。源代码和预训练模型将公开发布。&lt;h4&gt;翻译&lt;/h4&gt;尽管对高光谱图像分类（HSIC）来说，高效地提取具有区分性的空间-光谱特征是至关重要的，但由于诸如空间-光谱异质性和噪声效应等因素的影响，实现这些特性非常困难。本文提出了一种基于去噪扩散概率模型（DDPM）与对比学习（CL）结合的高光谱图像分类（HSIC）的空间-光谱扩散对比表示网络（DiffCRN）。该方法具有以下特点：改进空间-光谱特征表示；提高无监督特性学习效率；改进时间步长选择；改进特性融合和分类。实验结果表明，所提出的模型在四个广泛使用的高光谱数据集上相比经典基础模型、最新的GAN、Transformer模型和其他预训练方法有更优的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although efficient extraction of discriminative spatial-spectral features iscritical for hyperspectral images classification (HSIC), it is difficult toachieve these features due to factors such as the spatial-spectralheterogeneity and noise effect. This paper presents a Spatial-SpectralDiffusion Contrastive Representation Network (DiffCRN), based on denoisingdiffusion probabilistic model (DDPM) combined with contrastive learning (CL)for HSIC, with the following characteristics. First,to improve spatial-spectralfeature representation, instead of adopting the UNets-like structure which iswidely used for DDPM, we design a novel staged architecture with spatialself-attention denoising module (SSAD) and spectral group self-attentiondenoising module (SGSAD) in DiffCRN with improved efficiency forspectral-spatial feature learning. Second, to improve unsupervised featurelearning efficiency, we design new DDPM model with logarithmic absolute error(LAE) loss and CL that improve the loss function effectiveness and increase theinstance-level and inter-class discriminability. Third, to improve featureselection, we design a learnable approach based on pixel-level spectral anglemapping (SAM) for the selection of time steps in the proposed DDPM model in anadaptive and automatic manner. Last, to improve feature integration andclassification, we design an Adaptive weighted addition modul (AWAM) and Crosstime step Spectral-Spatial Fusion Module (CTSSFM) to fuse time-step-wisefeatures and perform classification. Experiments conducted on widely used fourHSI datasets demonstrate the improved performance of the proposed DiffCRN overthe classical backbone models and state-of-the-art GAN, transformer models andother pretrained methods. The source code and pre-trained model will be madeavailable publicly.</description>
      <author>example@mail.com (Yimin Zhu, Linlin Xu)</author>
      <guid isPermaLink="false">2502.19699v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>SeisMoLLM: Advancing Seismic Monitoring via Cross-modal Transfer with Pre-trained Large Language Model</title>
      <link>http://arxiv.org/abs/2502.19960v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures. Code is available at  https://github.com/StarMoonWang/SeisMoLLM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了SeisMoLLM，这是一个利用跨模态迁移的地震监测基础模型，它通过大规模预训练从大型语言模型中释放其潜力，并在多个复杂的地震监测任务上取得了卓越性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，深度学习技术革新了地震监控领域。然而，在处理信号退化或数据稀缺的情况下开发适用于多种复杂任务的基础模型仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;研究目的是提出一种新的基础模型SeisMoLLM，以提高在各种地震监测任务中的性能，并探索跨模态迁移的可能性。&lt;h4&gt;方法&lt;/h4&gt;通过精心设计的波形标记和对预训练GPT-2模型进行微调的方式实现，不直接在地震数据集上进行预训练。该模型在DiTing和STEAD数据集中执行五个关键任务：后方角估计、震中距离估计、震级估计、相位选择以及首次运动极性分类。&lt;h4&gt;主要发现&lt;/h4&gt;SeisMoLLM在43个任务度量标准中有36项达到了最佳结果，在16个少量样本泛化度量标准中的12项上取得了顶级分数。相对改进幅度范围从10%到50%&lt;h4&gt;结论&lt;/h4&gt;研究表明，SeisMoLLM是一个具有前景的基础模型，适用于实际地震监测，并且展示了跨模态迁移作为地震研究中一个令人兴奋的新方向的潜力。&lt;h4&gt;翻译&lt;/h4&gt;近年来，深度学习技术革新了地震监控领域。然而，在处理信号退化或数据稀缺的情况下开发适用于多种复杂任务的基础模型仍然具有挑战性。这项工作提出了一种名为SeisMoLLM的基础模型，它是第一个利用跨模态迁移进行地震监测的模型，该模型通过大规模预训练从大型语言模型中释放其潜力，并不直接在地震数据集上进行预训练。SeisMoLLM采用精细的波形标记化和对预先训练好的GPT-2模型进行微调的方式，在DiTing和STEAD数据集中执行五个关键任务：后方角估计、震中距离估计、震级估计、相位选择以及首次运动极性分类，并在43个任务度量标准中有36项达到了最佳结果，在16个少量样本泛化度量标准中的12项上取得了顶级分数，相对改进幅度范围从10%到50%。除了卓越的性能外，SeisMoLLM在训练和推理方面也保持了与轻量级模型相当甚至更好的效率。这些发现使SeisMoLLM成为具有前景的基础模型，适用于实际地震监测，并且展示了跨模态迁移作为地震研究中一个令人兴奋的新方向的潜力，突显了高级深度学习技术推动地震学研究发展的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/StarMoonWang/SeisMoLLM&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in deep learning have revolutionized seismic monitoring, yetdeveloping a foundation model that performs well across multiple complex tasksremains challenging, particularly when dealing with degraded signals or datascarcity. This work presents SeisMoLLM, the first foundation model thatutilizes cross-modal transfer for seismic monitoring, to unleash the power oflarge-scale pre-training from a large language model without requiring directpre-training on seismic datasets. Through elaborate waveform tokenization andfine-tuning of pre-trained GPT-2 model, SeisMoLLM achieves state-of-the-artperformance on the DiTing and STEAD datasets across five critical tasks:back-azimuth estimation, epicentral distance estimation, magnitude estimation,phase picking, and first-motion polarity classification. It attains 36 bestresults out of 43 task metrics and 12 top scores out of 16 few-shotgeneralization metrics, with many relative improvements ranging from 10% to50%. In addition to its superior performance, SeisMoLLM maintains efficiencycomparable to or even better than lightweight models in both training andinference. These findings establish SeisMoLLM as a promising foundation modelfor practical seismic monitoring and highlight cross-modal transfer as anexciting new direction for earthquake studies, showcasing the potential ofadvanced deep learning techniques to propel seismology research forward.</description>
      <author>example@mail.com (Xinghao Wang, Feng Liu, Rui Su, Zhihui Wang, Lei Bai, Wanli Ouyang)</author>
      <guid isPermaLink="false">2502.19960v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Open-Vocabulary Semantic Part Segmentation of 3D Human</title>
      <link>http://arxiv.org/abs/2502.19782v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  3DV 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了首个能够处理3D人体的开放词汇分割方法。&lt;h4&gt;背景&lt;/h4&gt;传统的监督分割方法由于标注数据有限，在泛化到未见过的人体形状和类别上效果不佳。最近，视觉-语言模型在零样本能力上的进步推动了开放世界的3D分割方法的发展，但这些方法对3D人类的泛化效果不理想。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够根据文本提示将人体划分为期望的细粒度部分的方法。&lt;h4&gt;方法&lt;/h4&gt;采用了基于SAM的多视角提案生成和一个新颖的人体CLIP模型来创建视觉和文本输入的一致性嵌入。同时，还引入了一个简单的MaskFusion模块，该模块通过分类和融合多视图特征直接形成3D语义掩膜。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在各种3D人体数据集上，本方法优于当前最先进的开放词汇3D分割方法，并且可以应用于包括网格、点云和3D高斯散布在内的多种3D表示形式。&lt;h4&gt;结论&lt;/h4&gt;提出的方法为处理复杂的3D人体场景提供了一个强有力的解决方案，并展示了在未来的AR/VR应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的直接翻译，描述了3D部分分割是三维视觉和AR/VR领域的一个开放问题。由于标注数据有限，传统的监督方法无法很好地泛化到未见过的人体形状和类别上。通过利用先进视觉-语言模型的能力，该论文提出了一种新的处理3D人体的方法，并展示了其在多种3D表示形式上的优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D part segmentation is still an open problem in the field of 3D vision andAR/VR. Due to limited 3D labeled data, traditional supervised segmentationmethods fall short in generalizing to unseen shapes and categories. Recently,the advancement in vision-language models' zero-shot abilities has brought asurge in open-world 3D segmentation methods. While these methods show promisingresults for 3D scenes or objects, they do not generalize well to 3D humans. Inthis paper, we present the first open-vocabulary segmentation method capable ofhandling 3D human. Our framework can segment the human category into desiredfine-grained parts based on the textual prompt. We design a simple segmentationpipeline, leveraging SAM to generate multi-view proposals in 2D and proposing anovel HumanCLIP model to create unified embeddings for visual and textualinputs. Compared with existing pre-trained CLIP models, the HumanCLIP modelyields more accurate embeddings for human-centric contents. We also design asimple-yet-effective MaskFusion module, which classifies and fuses multi-viewfeatures into 3D semantic masks without complex voting and grouping mechanisms.The design of decoupling mask proposals and text input also significantlyboosts the efficiency of per-prompt inference. Experimental results on various3D human datasets show that our method outperforms current state-of-the-artopen-vocabulary 3D segmentation methods by a large margin. In addition, we showthat our method can be directly applied to various 3D representations includingmeshes, point clouds, and 3D Gaussian Splatting.</description>
      <author>example@mail.com (Keito Suzuki, Bang Du, Girish Krishnan, Kunyao Chen, Runfa Blark Li, Truong Nguyen)</author>
      <guid isPermaLink="false">2502.19782v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>One Model for ALL: Low-Level Task Interaction Is a Key to Task-Agnostic Image Fusion</title>
      <link>http://arxiv.org/abs/2502.19854v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的图像融合框架GIFNet，该框架通过低级视觉任务进行像素级别的监督，使得特征交互更加有效。&lt;h4&gt;背景&lt;/h4&gt;高级图像融合方法主要侧重于高层次的任务，在这些任务中，任务间的互动因语义差距而变得复杂，需要复杂的桥接机制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的图像融合方法，以简化特征之间的相互作用，并提高跨模态无监督融合的性能。&lt;h4&gt;方法&lt;/h4&gt;利用数字摄影中的低级视觉任务进行像素级别的监督，通过这种方式来实现更有效的特征交互和增强的任务共享特性学习。&lt;h4&gt;主要发现&lt;/h4&gt;GIFNet支持多种图像融合任务，并且在已见场景和未见过的场景中均表现出色。此外，该框架还能够用于单模态增强，为实际应用提供了更大的灵活性。&lt;h4&gt;结论&lt;/h4&gt;所提出的基于低级视觉任务的像素级别监督方法提供了一种新的、强大的指导方式，无需依赖抽象语义即可实现多模式融合，并且在广泛的图像处理任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;先进的图像融合技术通常侧重于高层次的任务，在这些任务中，不同的任务之间的互动因为需要跨越较大的语义差距而变得困难。相比之下，我们提出了一种新的方法，利用数字摄影中的低级视觉任务来实现像素级别的监督和有效的特征交互。这种新的范式提供了强大的指导，可以无需依赖抽象的语义信息来进行跨模态无监督融合，并且增强了对于广泛适用性的任务共享特性学习。由于混合图像特性和增强的通用表示形式，提出的GIFNet支持多种不同的融合任务，在已见和未见过的情景中均表现出色。此外，实验结果还表明我们的框架能够支持单模态增强功能，从而为实际应用提供更优秀的灵活性。我们的代码可以在https://github.com/AWCXV/GIFNet上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advanced image fusion methods mostly prioritise high-level missions, wheretask interaction struggles with semantic gaps, requiring complex bridgingmechanisms. In contrast, we propose to leverage low-level vision tasks fromdigital photography fusion, allowing for effective feature interaction throughpixel-level supervision. This new paradigm provides strong guidance forunsupervised multimodal fusion without relying on abstract semantics, enhancingtask-shared feature learning for broader applicability. Owning to the hybridimage features and enhanced universal representations, the proposed GIFNetsupports diverse fusion tasks, achieving high performance across both seen andunseen scenarios with a single model. Uniquely, experimental results revealthat our framework also supports single-modality enhancement, offering superiorflexibility for practical applications. Our code will be available athttps://github.com/AWCXV/GIFNet.</description>
      <author>example@mail.com (Chunyang Cheng, Tianyang Xu, Zhenhua Feng, Xiaojun Wu, ZhangyongTang, Hui Li, Zeyang Zhang, Sara Atito, Muhammad Awais, Josef Kittler)</author>
      <guid isPermaLink="false">2502.19854v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>You Only Click Once: Single Point Weakly Supervised 3D Instance Segmentation for Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19698v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;户外LiDAR点云三维实例分割是自动驾驶中的关键任务，但由于需要耗费大量人力来标注训练数据，因此该任务面临挑战。为了解决这一问题，提出了一个名为YoCo的框架，它使用稀疏的手工点击注释在俯视图平面上生成高质量的伪标签。&lt;h4&gt;背景&lt;/h4&gt;户外LiDAR点云三维实例分割对于自动驾驶至关重要，但手动标注训练数据的成本很高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来减少人工标注的工作量，并提高伪标签的质量和可靠性。&lt;h4&gt;方法&lt;/h4&gt;{'YoCo框架': '使用视觉基础模型与点云的几何约束结合生成高质量的伪标签；设计了一个基于时间和空间的更新模块，利用相邻帧的预测结果并考虑点云的密度变化来生成可靠的更新标签；提出了一种IoU引导增强模块，以高置信度和高交并比（IoU）的预测替换低质量的伪标签。', '实验验证': '在Waymo数据集上的实验证明了YoCo框架的有效性和广泛适用性。'}&lt;h4&gt;主要发现&lt;/h4&gt;YoCo在弱监督方法中取得了最先进的性能，并且在各种网络上使用少量完全标注的数据进行微调后，其性能可以与全监督方法相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种高效生成高质量伪标签的方法，显著降低了标注成本，适用于多种网络架构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Outdoor LiDAR point cloud 3D instance segmentation is a crucial task inautonomous driving. However, it requires laborious human efforts to annotatethe point cloud for training a segmentation model. To address this challenge,we propose a YoCo framework, which generates 3D pseudo labels using minimalcoarse click annotations in the bird's eye view plane. It is a significantchallenge to produce high-quality pseudo labels from sparse annotations. OurYoCo framework first leverages vision foundation models combined with geometricconstraints from point clouds to enhance pseudo label generation. Second, atemporal and spatial-based label updating module is designed to generatereliable updated labels. It leverages predictions from adjacent frames andutilizes the inherent density variation of point clouds (dense near, sparsefar). Finally, to further improve label quality, an IoU-guided enhancementmodule is proposed, replacing pseudo labels with high-confidence and high-IoUpredictions. Experiments on the Waymo dataset demonstrate YoCo's effectivenessand generality, achieving state-of-the-art performance among weakly supervisedmethods and surpassing fully supervised Cylinder3D. Additionally, the YoCo issuitable for various networks, achieving performance comparable to fullysupervised methods with minimal fine-tuning using only 0.8% of the fullylabeled data, significantly reducing annotation costs.</description>
      <author>example@mail.com (Guangfeng Jiang, Jun Liu, Yongxuan Lv, Yuzhi Wu, Xianfei Li, Wenlong Liao, Tao He, Pai Peng)</author>
      <guid isPermaLink="false">2502.19698v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>MICINet: Multi-Level Inter-Class Confusing Information Removal for Reliable Multimodal Classification</title>
      <link>http://arxiv.org/abs/2502.19674v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个可靠多模态分类方法MICINet，用于在存在噪音数据的情况下有效移除两种类型的噪声。&lt;h4&gt;背景&lt;/h4&gt;可靠的多模态学习尤其是在安全关键应用中是一个广受关注的问题。现有的许多方法只能处理特定模式或跨模态的噪音，而不能有效地处理这两种类型噪音的同时存在。&lt;h4&gt;目的&lt;/h4&gt;为了提高可靠性和应对上述挑战，提出了一种新的分类方法MICINet，该方法旨在统一并移除全球和个体水平上的干扰信息（ICI）。&lt;h4&gt;方法&lt;/h4&gt;MICINet通过全局ICI学习模块可靠地学习整体的ICI分布，并利用样本自适应跨模态信息补偿模块在个体层面上可靠地移除每个样本的噪声。此外，还引入了全局引导式样本ICILearning模块来高效去除全球级别的噪音。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在各种噪声条件下，MICINet优于其他最先进的可靠的多模态分类方法。&lt;h4&gt;结论&lt;/h4&gt;MICINet通过统一概念并有效移除干扰信息（ICI），成功地应对了现有可靠多模态学习方法的局限性，并在实际应用中展示了其优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reliable multimodal learning in the presence of noisy data is a widelyconcerned issue, especially in safety-critical applications. Many reliablemultimodal methods delve into addressing modality-specific or cross-modalitynoise. However, they fail to handle the coexistence of both types of noiseefficiently. Moreover, the lack of comprehensive consideration for noise atboth global and individual levels limits their reliability. To address theseissues, a reliable multimodal classification method dubbed Multi-LevelInter-Class Confusing Information Removal Network (MICINet) is proposed.MICINet achieves the reliable removal of both types of noise by unifying theminto the concept of Inter-class Confusing Information (\textit{ICI}) andeliminating it at both global and individual levels. Specifically, MICINetfirst reliably learns the global \textit{ICI} distribution through the proposed\textbf{\textit{Global \textbf{ICI} Learning Module}}. Then, it introduces the\textbf{\textit{Global-guided Sample ICI Learning module}} to efficientlyremove global-level \textit{ICI} from sample features utilizing the learnedglobal \textit{ICI} distribution. Subsequently, the\textbf{\textit{Sample-adaptive Cross-modality Information Compensationmodule}} is designed to remove individual-level \textit{ICI} from each samplereliably. This is achieved through interpretable cross-modality informationcompensation based on the complementary relationship between discriminativefeatures and \textit{ICI} and the perception of the relative quality ofmodalities introduced by the relative discriminative power. Experiments on fourdatasets demonstrate that MICINet outperforms other state-of-the-art reliablemultimodal classification methods under various noise conditions.</description>
      <author>example@mail.com (Tong Zhang, Shu Shen, C. L. Philip Chen)</author>
      <guid isPermaLink="false">2502.19674v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Efficient Machine Learning Approach for Yield Prediction in Chemical Reactions</title>
      <link>http://arxiv.org/abs/2502.19976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;机器学习模型在化学反应产率预测中的应用已成为近年来的重要研究方向。本文提出了一种新的时间高效和资源高效的预训练策略，以及一种分类后回归的模型(CFR)，用于解决不平衡及稀疏数据集的问题。&lt;h4&gt;背景&lt;/h4&gt;化学语言表示法为化学反应提供了独特的视角，自然语言处理模型（如ULMFiT）可以在这种分布设置中定制以实现产率预测。然而，目前的数据集存在规模小、偏向高产率以及分布稀疏等问题。&lt;h4&gt;目的&lt;/h4&gt;开发新的预训练策略和CFR模型来提高化学反应产率预测的精度，特别是在不平衡和稀疏数据条件下。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含860多个手动收集自文献中的催化meta-C(sp2)-H键活化反应的新数据集。采用基于子结构从PubChem数据库中构建预训练数据集SSP1（含约0.11百万条目），并将其用于ULMFiT模型的微调。&lt;h4&gt;主要发现&lt;/h4&gt;CFR模型在预测目标化学反应产率时表现优秀，特别是在高产率和低产率两类上分别实现了RMSE为8.40和6.48的结果。这表明该方法不仅有效而且比传统的直接回归方法更优越，并且具有良好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;CFR模型结合ULMFiT-SSP1回归器能够提供最先进的产率预测，证明了这种方法在解决不平衡和稀疏数据挑战中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;开发用于化学反应产率预测的机器学习（ML）模型已成为近年来的重要研究领域。此类数据集面临的主要挑战源自于不平衡与稀疏性。本文中，作者使用化学语言表示来利用像ULMFiT这样的自然语言处理模型来进行产率预测，并且该方法针对分布设置进行了定制化开发。此外，贡献了一个新的反应数据集，其中包含超过860个从文献中手动提取的跨越十年的数据点，涉及一类高当代重要性的催化meta-C(sp2)-H键活化反应。考虑到数据集规模、偏向高产率以及稀疏性特点，作者提出了一种新型的时间和资源高效的预训练策略用于下游转移学习，并且开发了CFR模型以提供先进的产率预测能力，超越传统的直接回归方法。通过使用基于子结构的从PubChem数据库中提取0.11百万条目的SSP1预训练数据集，替代传统的大规模未标记分子（ChEMBL数据集中有约140万）的预训练惯例，发现这种更有效的时间和效率方法同样可以提供改进性能。ULMFiT-SSP1回归器在CFR模型下对目标反应产率预测表现出8.40和6.48的RMSE（分别对应于53%产率界限以上的高产率类和以下的低产率类）。此外，该方法展示出高度泛化的特性，并且在之前的数据集基准上取得了显著进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing machine learning (ML) models for yield prediction of chemicalreactions has emerged as an important use case scenario in very recent years.In this space, reaction datasets present a range of challenges mostly stemmingfrom imbalance and sparsity. Herein, we consider chemical languagerepresentations for reactions to tap into the potential of natural languageprocessing models such as the ULMFiT (Universal Language Model Fine Tuning) foryield prediction, which is customized to work across such distributionsettings. We contribute a new reaction dataset with more than 860 manuallycurated reactions collected from literature spanning over a decade, belongingto a family of catalytic meta-C(sp2)-H bond activation reactions of highcontemporary importance. Taking cognizance of the dataset size, skewness towardthe higher yields, and the sparse distribution characteristics, we developed anew (i) time- and resource-efficient pre-training strategy for downstreamtransfer learning, and (ii) the CFR (classification followed by regression)model that offers state-of-the-art yield predictions, surpassing conventionaldirect regression (DR) approaches. Instead of the prevailing pre-trainingpractice of using a large number of unlabeled molecules (1.4 million) from theChEMBL dataset, we first created a pre-training dataset SSP1 (0.11 million), byusing a substructure-based mining from the PubChem database, which is found tobe equally effective and more time-efficient in offering enhanced performance.The CFR model with the ULMFiT-SSP1 regressor achieved an impressive RMSE of8.40 for the CFR-major and 6.48 for the CFR-minor class in yield prediction onthe title reaction, with a class boundary of yield at 53 %. Furthermore, theCFR model is highly generalizable as evidenced by the significant improvementover the previous benchmark reaction datasets.</description>
      <author>example@mail.com (Supratim Ghosh, Nupur Jain, Raghavan B. Sunoj)</author>
      <guid isPermaLink="false">2502.19976v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Improving Representation Learning of Complex Critical Care Data with ICU-BERT</title>
      <link>http://arxiv.org/abs/2502.19593v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for poster at GenAI4Health Workshop at AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了ICU-BERT模型，这是一种基于Transformer架构的预训练模型，使用MIMIC-IV数据库进行多任务学习，以最小化的预处理步骤从复杂的ICU数据中提取出健壮表示。&lt;h4&gt;背景&lt;/h4&gt;现实世界临床数据的多元性和异步性对传统的AI决策支持系统提出了挑战。这些传统系统通常假设数据具有规律性和特征独立性，并且依赖于有限的数据范围和手动特征工程，而生成式AI技术在分析临床数据方面的潜力尚未得到充分开发。&lt;h4&gt;目的&lt;/h4&gt;提出ICU-BERT模型以提高复杂多变量的ICU数据表示的可解释性和通用性。&lt;h4&gt;方法&lt;/h4&gt;ICU-BERT采用多令牌输入策略，并结合生物医学大型语言模型的密集嵌入，通过MIMIC-IV数据库进行预训练，利用多种任务学习框架来实现这一目标。该模型能够处理结构化和非结构化的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;初步评估表明，ICU-BERT在五项任务及四个额外的ICU数据集中取得的结果优于或达到当前性能基准，通过微调技术可以进一步提升表现。&lt;h4&gt;结论&lt;/h4&gt;ICU-BERT模型推进了基础模型在医学信息学中的应用，并为各种临床决策支持提供了灵活解决方案。该模型表明，在处理复杂和多变量的数据时，采用预训练的Transformer架构能够有效地超越传统AI方法。&lt;h4&gt;翻译&lt;/h4&gt;现实世界中重症监护病房（ICUs）等环境生成的多元异步医疗数据对传统的基于人工智能的决策支持系统提出了挑战。这些传统系统通常假设数据具有规律性且特征间独立，并依赖于有限的数据范围和手动特征工程。目前，生成式AI技术在分析临床数据方面尚未得到充分利用。我们介绍了ICU-BERT模型，这是一个基于Transformer架构的预训练模型，在MIMIC-IV数据库上通过多任务学习方案进行训练，从而实现复杂且多元化的重症监护病房（ICUs）数据的稳健表示，同时需要最小化预处理步骤。ICU-BERT采用了一种多令牌输入策略，并结合了生物医学大型语言模型的密集嵌入以获得通用性的表示方法。初步评估表明，在五项任务及四个额外的数据集上ICU-BERT的表现要么与当前性能基准持平或超越它们，通过微调技术可以进一步提高表现水平。此外，ICU-BERT将结构化和非结构化的数据结合起来，促进了基础模型在医学信息学中的应用，并为多种临床决策支持提供了灵活的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multivariate, asynchronous nature of real-world clinical data, such asthat generated in Intensive Care Units (ICUs), challenges traditional AI-baseddecision-support systems. These often assume data regularity and featureindependence and frequently rely on limited data scopes and manual featureengineering. The potential of generative AI technologies has not yet been fullyexploited to analyze clinical data. We introduce ICU-BERT, a transformer-basedmodel pre-trained on the MIMIC-IV database using a multi-task scheme to learnrobust representations of complex ICU data with minimal preprocessing. ICU-BERTemploys a multi-token input strategy, incorporating dense embeddings from abiomedical Large Language Model to learn a generalizable representation ofcomplex and multivariate ICU data. With an initial evaluation of five tasks andfour additional ICU datasets, ICU-BERT results indicate that ICU-BERT eithercompares to or surpasses current performance benchmarks by leveragingfine-tuning. By integrating structured and unstructured data, ICU-BERT advancesthe use of foundational models in medical informatics, offering an adaptablesolution for clinical decision support across diverse applications.</description>
      <author>example@mail.com (Ricardo Santos, André V. Carreiro, Xi Peng, Hugo Gamboa, Holger Fröhlich)</author>
      <guid isPermaLink="false">2502.19593v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions</title>
      <link>http://arxiv.org/abs/2502.19293v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures. arXiv admin note: text overlap with  arXiv:2502.19285&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发了一个专门针对皮肤黑素细胞病变的视觉-语言模型，并通过实验验证了该模型在生成普通痣病理报告方面的质量与医生写作相当。&lt;h4&gt;背景&lt;/h4&gt;每年有数百万黑色素细胞皮肤病灶被病理学家检查，大多数是普通的痣。虽然大部分病灶可以快速诊断，但撰写相应的病理报告非常耗时。&lt;h4&gt;目的&lt;/h4&gt;通过自动化部分报告编写过程来减轻病理学家的工作负担。&lt;h4&gt;方法&lt;/h4&gt;构建了一个遵循对比生成器框架的视觉-语言模型，并使用包含42,512张H&amp;E染色完整切片图像和19,645份对应病理报告的数据集进行训练和评估。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在普通痣病理报告的质量评分上与病理学家书写的报告相当，但在罕见黑色素细胞病变亚型的报告生成方面更为困难。然而，在这些病例中的跨模态检索性能有所提高。&lt;h4&gt;结论&lt;/h4&gt;通过自动化的视觉-语言模型可以有效减轻病理医生撰写常见皮肤黑素瘤病灶报告的工作负担，并且在处理复杂或罕见类型时表现出色，尤其是在信息检索上的性能提升。&lt;h4&gt;翻译&lt;/h4&gt;每年有数百万黑色素细胞皮肤病变由病理学家检查，其中大多数涉及常见的痣。尽管大部分病变可以在几秒钟内诊断出来，但编写相应的病理报告却非常耗时。因此，自动化部分报告撰写过程可以缓解不断增加的工作量。在这项研究中，我们开发了一个专门用于皮肤黑素细胞病变领域的视觉-语言模型。该模型遵循对比生成器框架，并使用42,512张H&amp;E染色全切片图像和19,645份相应病理报告的黑色素细胞病灶数据集进行训练和评估。实验结果显示，由模型生成的报告质量评分与专家病理学家书写的报告相当，尤其是在普通痣的情况下。尽管对于罕见的黑素瘤亚型来说报告生成更为困难，但在这些情况下跨模态检索性能有了显著提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of melanocytic skin lesions are examined by pathologists each year,the majority of which concern common nevi (i.e., ordinary moles). While most ofthese lesions can be diagnosed in seconds, writing the corresponding pathologyreport is much more time-consuming. Automating part of the report writingcould, therefore, alleviate the increasing workload of pathologists. In thiswork, we develop a vision-language model specifically for the pathology domainof cutaneous melanocytic lesions. The model follows the Contrastive Captionerframework and was trained and evaluated using a melanocytic lesion dataset of42,512 H&amp;E-stained whole slide images and 19,645 corresponding pathologyreports. Our results show that the quality scores of model-generated reportswere on par with pathologist-written reports for common nevi, assessed by anexpert pathologist in a reader study. While report generation revealed to bemore difficult for rare melanocytic lesion subtypes, the cross-modal retrievalperformance for these cases was considerably better.</description>
      <author>example@mail.com (Ruben T. Lucassen, Sander P. J. Moonemans, Tijn van de Luijtgaarden, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19293v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Training Robust Graph Neural Networks by Modeling Noise Dependencies</title>
      <link>http://arxiv.org/abs/2502.19670v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Work in progress&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的噪声场景依赖感知图噪声（DANG），并设计了一种新颖的鲁棒GNN模型DA-GNN，该模型可以处理节点特征、图结构和节点标签之间相互影响的噪声情况。&lt;h4&gt;背景&lt;/h4&gt;在现实应用中，图中的节点特性常常包含来自各种来源的噪声，这会导致基于图神经网络（GNN）的性能显著下降。现有的增强鲁棒性的方法假设噪声与图结构和节点标签独立，这一假设并不符合实际情况。&lt;h4&gt;目的&lt;/h4&gt;引入一种更实际的噪声场景——依赖感知图噪声（DANG），并提出相应的模型以提高在该情况下的鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了一个称为DA-GNN的新模型，利用变分推理捕捉数据生成过程中变量之间的因果关系，并设计了新的基准测试集来模拟现实中的DANG情形。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，无论是在依赖感知图噪声（DANG）还是传统的噪声模式下，提出的DA-GNN在各种噪音场景中都优于现有的基线方法。&lt;h4&gt;结论&lt;/h4&gt;本文提出的DA-GNN模型及其新的基准测试集为解决基于图的机器学习中的鲁棒性问题提供了一种更实际的方法。&lt;h4&gt;翻译&lt;/h4&gt;在现实世界的应用中，图中的节点特征通常包含来自各种来源的噪声，这会导致GNN性能显著下降。尽管已经开发了几种增强鲁棒性的方法，但它们依赖于一个不切实际的假设：即节点特征中的噪声与图结构和节点标签独立。为此，我们引入了一种更现实的噪声场景——依赖感知图噪声（DANG），在这种情形下，节点特征中的噪声会在图结构和节点标签之间产生噪声依赖链。我们提出了一种新的鲁棒GNN模型DA-GNN，该模型使用变分推理捕捉数据生成过程中的因果关系。此外，我们还提出了模拟现实应用中DANG的新基准测试集，使更实际的关于鲁棒GNN的研究成为可能。广泛的实验表明，在各种噪声场景（包括DANG和传统噪声模型）下，DA-GNN始终优于现有的基线方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world applications, node features in graphs often contain noise fromvarious sources, leading to significant performance degradation in GNNs.Although several methods have been developed to enhance robustness, they relyon the unrealistic assumption that noise in node features is independent of thegraph structure and node labels, thereby limiting their applicability. To thisend, we introduce a more realistic noise scenario, dependency-aware noise ongraphs (DANG), where noise in node features create a chain of noisedependencies that propagates to the graph structure and node labels. We proposea novel robust GNN, DA-GNN, which captures the causal relationships amongvariables in the data generating process (DGP) of DANG using variationalinference. In addition, we present new benchmark datasets that simulate DANG inreal-world applications, enabling more practical research on robust GNNs.Extensive experiments demonstrate that DA-GNN consistently outperforms existingbaselines across various noise scenarios, including both DANG and conventionalnoise models commonly considered in this field.</description>
      <author>example@mail.com (Yeonjun In, Kanghoon Yoon, Sukwon Yun, Kibum Kim, Sungchul Kim, Chanyoung Park)</author>
      <guid isPermaLink="false">2502.19670v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Prompt-driven Transferable Adversarial Attack on Person Re-Identification with Attribute-aware Textual Inversion</title>
      <link>http://arxiv.org/abs/2502.19697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一个新的攻击方法Attribute-aware Prompt Attack (AP-Attack)，用于探索和利用行人重识别模型的脆弱性。&lt;h4&gt;背景&lt;/h4&gt;在安全监控系统中，人再识别（re-id）模型非常重要。基于视觉语言模型（VLM）的攻击显示出优秀的迁移性，但由于过分强调辨别语义而在整体表示上缺乏全面的功能破坏。&lt;h4&gt;目的&lt;/h4&gt;提出一个新型方法AP-Attack，利用VLM图像文本对齐能力来明确干扰行人图像中的细粒度语义特征。&lt;h4&gt;方法&lt;/h4&gt;通过设计文本逆向网络将行人图像映射到代表语义嵌入的伪令牌上，并在对比学习方式下训练这些网络。使用预定义的提示模板描述具体的行人属性，以获得个性化的文本描述。&lt;h4&gt;主要发现&lt;/h4&gt;AP-Attack 方法显著提升了对抗样本的迁移性，在跨模型和数据集攻击场景中平均Drop Rate比先前的方法提高了22.9%。&lt;h4&gt;结论&lt;/h4&gt;AP-Attack 通过全面干扰细粒度语义特征，增强了对行人重识别系统的攻击能力。&lt;h4&gt;翻译&lt;/h4&gt;人再识别（re-id）模型在安全监控系统中至关重要。最近基于视觉语言模型的攻击显示出优秀的迁移性，但因为过分强调辨别语义而缺乏对整体表示的全面破坏。本文引入了一种新的方法Attribute-aware Prompt Attack (AP-Attack)，利用VLM的图像文本对齐能力，通过摧毁特定属性的文本嵌入来明确地干扰行人图像中的细粒度语义特征。设计了用于映射行人图像到代表语义嵌入的伪令牌上的文本逆向网络，并以对比学习方式训练这些网络，在预定义提示模板下具体描述行人属性。通过这种个性化的文本描述，AP-Attack 方法有效增强了攻击能力，使对抗样本具有更高的迁移性。实验表明，该方法在跨模型和数据集攻击场景中的平均Drop Rate比先前的方法提高了22.9%，达到了最先进的迁移水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Person re-identification (re-id) models are vital in security surveillancesystems, requiring transferable adversarial attacks to explore thevulnerabilities of them. Recently, vision-language models (VLM) based attackshave shown superior transferability by attacking generalized image and textualfeatures of VLM, but they lack comprehensive feature disruption due to theoveremphasis on discriminative semantics in integral representation. In thispaper, we introduce the Attribute-aware Prompt Attack (AP-Attack), a novelmethod that leverages VLM's image-text alignment capability to explicitlydisrupt fine-grained semantic features of pedestrian images by destroyingattribute-specific textual embeddings. To obtain personalized textualdescriptions for individual attributes, textual inversion networks are designedto map pedestrian images to pseudo tokens that represent semantic embeddings,trained in the contrastive learning manner with images and a predefined prompttemplate that explicitly describes the pedestrian attributes. Inverted benignand adversarial fine-grained textual semantics facilitate attacker ineffectively conducting thorough disruptions, enhancing the transferability ofadversarial examples. Extensive experiments show that AP-Attack achievesstate-of-the-art transferability, significantly outperforming previous methodsby 22.9% on mean Drop Rate in cross-model&amp;dataset attack scenarios.</description>
      <author>example@mail.com (Yuan Bian, Min Liu, Yunqi Yi, Xueping Wang, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.19697v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Mixtera: A Data Plane for Foundation Model Training</title>
      <link>http://arxiv.org/abs/2502.19790v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  under submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;大型语言和视觉模型通过汇集来自多种来源的万亿级标记进行训练。随着训练数据集的增长，手动管理样本变得耗时、繁琐且容易出错。&lt;h4&gt;目的&lt;/h4&gt;构建并展示一个名为Mixtera的数据平面工具，该工具用于基础模型训练，使用户可以声明性地表达在训练期间哪些数据样本应该以何种比例和顺序使用。&lt;h4&gt;方法&lt;/h4&gt;Mixtera是一个独立于文件系统结构的集中式、只读层，可部署在现有训练数据集之上，并支持跨任意属性（如语言、源数据集）的数据混合及基于模型反馈动态调整混合策略的功能。&lt;h4&gt;主要发现&lt;/h4&gt;实验性地评估了Mixtera，证明我们的实现不会成为瓶颈且能够扩展到256个GH200超级芯片。通过在系统中实施提议的自适应数据优化(ADO)算法并评估其性能影响来展示如何支持最近的数据混合策略改进。&lt;h4&gt;结论&lt;/h4&gt;探索了视觉语言模型中的混合作用，为训练大规模基础模型提供了灵活、可扩展和易于使用的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;最先进的大型语言和视觉模型通过来自各种来源的万亿级标记进行训练。随着训练数据集的增长，手动管理样本变得耗时、繁琐且容易出错。然而，最近的研究表明，在训练期间的数据混合及访问样本顺序可以显著影响模型精度。我们建立并展示了Mixtera——一个用于基础模型训练的数据平面工具，它使用户能够声明性地表达在训练过程中应使用哪些数据样本及其比例和顺序。Mixtera是一个独立于文件系统结构的集中式、只读层，可部署在现有训练数据集之上，并支持跨任意属性（如语言、源数据集）的数据混合以及基于模型反馈动态调整混合策略的功能。我们实验性地评估了Mixtera并显示我们的实现不会成为瓶颈且能够扩展到256个GH200超级芯片。通过在系统中实施提议的自适应数据优化(ADO)算法并评估其性能影响来展示如何支持最近的数据混合策略改进，并探索视觉语言模型中的混合作用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art large language and vision models are trained over trillionsof tokens that are aggregated from a large variety of sources. As training datacollections grow, manually managing the samples becomes time-consuming,tedious, and prone to errors. Yet recent research shows that the data mixtureand the order in which samples are visited during training can significantlyinfluence model accuracy. We build and present Mixtera, a data plane forfoundation model training that enables users to declaratively express whichdata samples should be used in which proportion and in which order duringtraining. Mixtera is a centralized, read-only layer that is deployed on top ofexisting training data collections and can be declaratively queried. Itoperates independently of the filesystem structure and supports mixtures acrossarbitrary properties (e.g., language, source dataset) as well as dynamicadjustment of the mixture based on model feedback. We experimentally evaluateMixtera and show that our implementation does not bottleneck training andscales to 256 GH200 superchips. We demonstrate how Mixtera supports recentadvancements in mixing strategies by implementing the proposed Adaptive DataOptimization (ADO) algorithm in the system and evaluating its performanceimpact. We also explore the role of mixtures for vision-language models.</description>
      <author>example@mail.com (Maximilian Böther, Xiaozhe Yao, Tolga Kerimoglu, Ana Klimovic)</author>
      <guid isPermaLink="false">2502.19790v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation</title>
      <link>http://arxiv.org/abs/2502.19285v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了病理图像与语言模型在处理病理性数据时，选取不同类型的信息对多模态表示和自动生成报告质量的影响。&lt;h4&gt;背景&lt;/h4&gt;当前许多视觉-语言模型训练所使用的病理报告包含从配对全玻片图像无法推断出的信息（如患者历史），这可能导致生成报告中出现幻觉句子。&lt;h4&gt;目的&lt;/h4&gt;研究病理报告中选择何种信息用于视觉-语言建模会影响多模态表示和自动生成报告的质量。&lt;h4&gt;方法&lt;/h4&gt;构建了基于BLIP-2框架的模型，利用42,433张HE染色全玻片图像及19,636份对应病理报告进行实验。比较了一个训练在完整报告上的模型与一个仅使用描述细胞和组织外观句子的预处理报告上训练的模型。&lt;h4&gt;主要发现&lt;/h4&gt;文本预处理能够有效防止生成报告中的幻觉，然而，完全报告训练模型在跨模态检索性能方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;对于视觉-语言病理模型而言，在选择用于训练的数据时需权衡自动生成报告的质量和跨模态检索性能之间的平衡。&lt;h4&gt;翻译&lt;/h4&gt;视觉语言模型在病理性研究中能够实现多模态病例检索及自动化报告生成。然而，现有的许多模型都基于包含从配对全玻片图像无法推断出的信息（如患者历史）的病理报告训练而成，可能导致幻觉句子出现在所生成的报告中。本文探讨了选取不同信息类型用于视觉语言建模如何影响多模态表示和自动生成报告的质量。具体而言，比较了一个基于完整报告训练的模型与一个仅使用描述细胞组织外观的HE染色玻片报告上预处理后文本训练出的模型的表现。实验是在BLIP-2框架基础上建立的，利用了一套包含42,433张HE染色全玻片图像和19,636份对应病理报告的数据集进行评估。通过图片到文字以及文字到图片的检索性能，并由专家病理科医生对生成报告进行了定性评价来衡量模型表现。结果表明，文本预处理能预防幻觉在报告生成中的出现。尽管生成报告的质量有所提升，但基于完整报告训练视觉语言模型在跨模态检索性能上仍表现出更好的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models in pathology enable multimodal case retrieval andautomated report generation. Many of the models developed so far, however, havebeen trained on pathology reports that include information which cannot beinferred from paired whole slide images (e.g., patient history), potentiallyleading to hallucinated sentences in generated reports. To this end, weinvestigate how the selection of information from pathology reports forvision-language modeling affects the quality of the multimodal representationsand generated reports. More concretely, we compare a model trained on fullreports against a model trained on preprocessed reports that only includesentences describing the cell and tissue appearances based on the H&amp;E-stainedslides. For the experiments, we built upon the BLIP-2 framework and used acutaneous melanocytic lesion dataset of 42,433 H&amp;E-stained whole slide imagesand 19,636 corresponding pathology reports. Model performance was assessedusing image-to-text and text-to-image retrieval, as well as qualitativeevaluation of the generated reports by an expert pathologist. Our resultsdemonstrate that text preprocessing prevents hallucination in reportgeneration. Despite the improvement in the quality of the generated reports,training the vision-language model on full reports showed better cross-modalretrieval performance.</description>
      <author>example@mail.com (Ruben T. Lucassen, Tijn van de Luijtgaarden, Sander P. J. Moonemans, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19285v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Principled Approach to Bayesian Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.19796v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 2 tables, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架，即转移顺序蒙特卡洛（Transfer Sequential Monte Carlo），用于评估和改进基于幂先验的贝叶斯迁移学习方法。&lt;h4&gt;背景&lt;/h4&gt;在数据稀缺的情况下，从相关数据集中引入信息可以改善对观测数据的推断。当前的贝叶斯迁移学习方法主要依赖于所谓的幂先验来适应性地转移相关信息。&lt;h4&gt;目的&lt;/h4&gt;评估和改进现有的基于幂先验的贝叶斯迁移学习方法，并通过留一交叉验证在目标数据集上评价其效果。&lt;h4&gt;方法&lt;/h4&gt;引入了一种新的框架，即转移顺序蒙特卡洛（TSMC），该方法可以有效地选择传输参数并避免了对共轭先验的需求。此外，使用留一法交叉验证来评估贝叶斯迁移学习的方法性能。&lt;h4&gt;主要发现&lt;/h4&gt;提出的TSMC框架在两项全面的模拟研究中表现出了优越性，并且能够有效提升推断准确性。&lt;h4&gt;结论&lt;/h4&gt;通过新的TSMC框架改进了现有基于幂先验的贝叶斯迁移学习方法，为解决数据稀缺问题提供了一种有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;更新先验信息以适应观测到的数据是贝叶斯推理的核心。贝叶斯迁移学习则扩展这一理念，将相关信息从相关数据集中引入，来改善在某些条件下可能略有不同的观测数据的推断。当前基于所谓的幂先验的贝叶斯迁移学习方法可以自适应地转移相关信息。然而，并非总能清楚这种技术在哪种情况下表现最佳或是否能够改进贝叶斯推理。现有的幂先验方法依赖于共轭关系来评估感兴趣的后验分布。我们提出了一种使用目标数据集上的留一交叉验证作为评估贝叶斯迁移学习的方法。此外，引入了新的框架，即转移顺序蒙特卡洛（TSMC），用于幂先验方法，并高效地选择传输参数同时避免了对共轭先验的需求。在两项全面的模拟研究中我们评估了所提议方法的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Updating $\textit{a priori}$ information given some observed data is the coretenet of Bayesian inference. Bayesian transfer learning extends this idea byincorporating information from a related dataset to improve the inference onthe observed data which may have been collected under slightly differentsettings. The use of related information can be useful when the observed datais scarce, for example. Current Bayesian transfer learning methods that arebased on the so-called $\textit{power prior}$ can adaptively transferinformation from related data. Unfortunately, it is not always clear underwhich scenario Bayesian transfer learning performs best or even if it willimprove Bayesian inference. Additionally, current power prior methods rely onconjugacy to evaluate the posterior of interest. We propose using leave-one-outcross validation on the target dataset as a means of evaluating Bayesiantransfer learning methods. Further, we introduce a new framework,$\textit{transfer sequential Monte Carlo}$, for power prior approaches thatefficiently chooses the transfer parameter while avoiding the need forconjugate priors. We assess the performance of our proposed methods in twocomprehensive simulation studies.</description>
      <author>example@mail.com (Adam Bretherton, Joshua J. Bon, David J. Warne, Kerrie Mengersen, Christopher Drovandi)</author>
      <guid isPermaLink="false">2502.19796v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>PhenoProfiler: Advancing Phenotypic Learning for Image-based Drug Discovery</title>
      <link>http://arxiv.org/abs/2502.19568v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PhenoProfiler是一个用于药物发现领域的创新模型，旨在从多通道整张图像中直接提取形态表示，并通过大规模数据集的评估证明其在准确性和鲁棒性上的优越性能。&lt;h4&gt;背景&lt;/h4&gt;现有的基于图片的药物发现方法需要复杂的多步骤计算过程，这会引入效率低下、限制泛化能力和增加潜在错误的问题。&lt;h4&gt;目的&lt;/h4&gt;提出PhenoProfiler模型以解决上述挑战，提高形态表示学习中的效率和准确性，并增强其鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;PhenoProfiler是一个端到端工具，直接处理多通道整张图像并生成低维定量表示。它包含一个多目标学习模块来提升在形态表示学习方面的准确性和泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;通过大规模公开数据集的评估，证明了PhenoProfiler比现有方法性能更好，在准确性和鲁棒性方面有显著提高，特别是在识别生物有意义信号的能力上表现突出。&lt;h4&gt;结论&lt;/h4&gt;这些结果表明，PhenoProfiler是一个可扩展、泛化和稳健的学习表型变化的工具。&lt;h4&gt;翻译&lt;/h4&gt;在药物发现领域中，捕捉细胞对不同治疗方式反应至关重要。然而现有方法需要复杂的多步骤计算过程，导致效率低下等问题。我们提出了一种名为PhenoProfiler的新模型，它可以高效地从图像中提取形态表示，并通过大规模数据集证明其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the field of image-based drug discovery, capturing the phenotypic responseof cells to various drug treatments and perturbations is a crucial step.However, existing methods require computationally extensive and complexmulti-step procedures, which can introduce inefficiencies, limitgeneralizability, and increase potential errors. To address these challenges,we present PhenoProfiler, an innovative model designed to efficiently andeffectively extract morphological representations, enabling the elucidation ofphenotypic changes induced by treatments. PhenoProfiler is designed as anend-to-end tool that processes whole-slide multi-channel images directly intolow-dimensional quantitative representations, eliminating the extensivecomputational steps required by existing methods. It also includes amulti-objective learning module to enhance robustness, accuracy, andgeneralization in morphological representation learning. PhenoProfiler isrigorously evaluated on large-scale publicly available datasets, including over230,000 whole-slide multi-channel images in end-to-end scenarios and more than8.42 million single-cell images in non-end-to-end settings. Across thesebenchmarks, PhenoProfiler consistently outperforms state-of-the-art methods byup to 20%, demonstrating substantial improvements in both accuracy androbustness. Furthermore, PhenoProfiler uses a tailored phenotype correctionstrategy to emphasize relative phenotypic changes under treatments,facilitating the detection of biologically meaningful signals. UMAPvisualizations of treatment profiles demonstrate PhenoProfiler ability toeffectively cluster treatments with similar biological annotations, therebyenhancing interpretability. These findings establish PhenoProfiler as ascalable, generalizable, and robust tool for phenotypic learning.</description>
      <author>example@mail.com (Bo Li, Bob Zhang, Chengyang Zhang, Minghao Zhou, Weiliang Huang, Shihang Wang, Qing Wang, Mengran Li, Yong Zhang, Qianqian Song)</author>
      <guid isPermaLink="false">2502.19568v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Mixtraining: A Better Trade-Off Between Compute and Performance</title>
      <link>http://arxiv.org/abs/2502.19513v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;混合训练（MixTraining）是一种在单一训练阶段内交替执行自监督学习和有监督学习的方法，旨在提高模型性能并减少计算开销。&lt;h4&gt;背景&lt;/h4&gt;将自监督学习应用于数据限制场景中以增强模型表现已成为一种常见策略，但这种方法带来了计算资源与模型性能之间的权衡：尽管提高了表示能力，却增加了额外的训练阶段，导致效率降低。&lt;h4&gt;目的&lt;/h4&gt;解决传统方法在时间和计算资源上的瓶颈问题，提出了一种新的混合框架（MixTraining），旨在提高自监督学习和有监督学习之间的协同作用，并减少共享步骤的计算负担。&lt;h4&gt;方法&lt;/h4&gt;提出了一个新型框架——混合训练（MixTraining），该框架在一个统一的训练阶段内交替执行SSL和SL的几个周期。同时，在两个学习目标之间引入了平滑过渡机制，使得模型能够在单一训练过程中高效完成自监督和有监督任务。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，与传统方法相比，混合训练在多个数据集上提供了更好的计算性能权衡，并且可以在不牺牲精度的情况下加快训练速度。例如，在TinyImageNet数据集中实现了8.81%的绝对准确率提升（相对于基准而言提高了18.89%），同时使用ViT-Tiny模型时可加速多达1.29倍。&lt;h4&gt;结论&lt;/h4&gt;混合训练方法证明了其在多种任务环境下的有效性和高效性，特别是在资源受限情况下。它为解决当前深度学习研究中计算效率与性能之间的矛盾提供了一种新的视角和解决方案。&lt;h4&gt;翻译&lt;/h4&gt;将自监督学习（SSL）整合到标准有监督学习之前已成为一种广泛使用的策略来提升模型表现，尤其是在数据有限的情况下。然而，这种方法引入了计算资源和性能之间的权衡：虽然自监督学习有助于表示能力的提高，但需要额外的训练阶段，增加了计算开销并限制了资源受限条件下的效率。为了应对这些挑战，我们提出了混合训练（MixTraining），这是一种新的框架，在统一的训练过程中交替执行多个SSL和SL周期，并在两个学习目标之间实现平滑过渡。这种方法增强了自监督学习与有监督学习之间的协同作用以提高精度，并通过减少共享计算步骤来降低计算开销。混合训练具有广泛适用性，适用于单任务和多任务场景。广泛的实验表明，相对于传统流水线而言，MixTraining提供了更好的计算性能权衡，在TinyImageNet数据集中实现了8.81%的绝对准确率提升（比基准提高了18.89%），使用ViT-Tiny模型时可加速多达1.29倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incorporating self-supervised learning (SSL) before standard supervisedlearning (SL) has become a widely used strategy to enhance model performance,particularly in data-limited scenarios. However, this approach introduces atrade-off between computation and performance: while SSL helps withrepresentation learning, it requires a separate, often time-consuming trainingphase, increasing computational overhead and limiting efficiency inresource-constrained settings. To address these challenges, we proposeMixTraining, a novel framework that interleaves several SSL and SL epochswithin a unified mixtraining training phase, featuring a smooth transitionbetween two learning objectives. MixTraining enhances synergy between SSL andSL for improved accuracy and consolidates shared computation steps to reducecomputation overhead. MixTraining is versatile and applicable to bothsingle-task and multi-task learning scenarios. Extensive experimentsdemonstrate that MixTraining offers a superior compute-performance trade-offcompared to conventional pipelines, achieving an 8.81% absolute accuracy gain(18.89% relative accuracy gain) on the TinyImageNet dataset while acceleratingtraining by up to 1.29x  with the ViT-Tiny model.</description>
      <author>example@mail.com (Zexin Li, Jiancheng Zhang, Yinglun Zhu, Cong Liu)</author>
      <guid isPermaLink="false">2502.19513v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Dictionary-based Framework for Interpretable and Consistent Object Parsing</title>
      <link>http://arxiv.org/abs/2502.19540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CoCal是一种基于字典式掩码变换器的可解释且一致的对象解析框架，它重新思考了现有的聚类基掩码变换器架构。&lt;h4&gt;背景&lt;/h4&gt;当前的对象分割方法通常依赖于将图像划分为多个部分并为每个部分分配一个特定的语义类别。然而，这种方法在保持不同对象间的一致性和逻辑关系方面存在挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架CoCal，以提高对象解析的质量和一致性，并增强对逻辑关系的理解。&lt;h4&gt;方法&lt;/h4&gt;CoCal采用了一种基于字典组件的方法，每个组件都与特定的语义类别相关联。通过引入层次化的字典组件结构，以及在同级内的对比成分和跨级别的逻辑约束，CoCal能够实现更精确的对象解析。&lt;h4&gt;主要发现&lt;/h4&gt;CoCal通过引入一种逐部件对比算法和跨级别对比学习目标，在PartImageNet和Pascal-Part-108数据集上实现了新的最佳性能。此外，它在对象级度量标准方面也有显著改进。&lt;h4&gt;结论&lt;/h4&gt;与先前的方法相比，CoCal提供了更准确的分割结果，并且能够更好地保持不同对象之间的逻辑关系。&lt;h4&gt;翻译&lt;/h4&gt;在这项工作中，我们提出了一种基于字典式掩码变换器的可解释和一致的对象解析框架——CoCal。该框架围绕对比成分和逻辑约束设计，重新思考了现有的聚类基掩码变换器架构。具体而言，CoCal利用一组字典组件，每个组件都与特定的语义类别明确相关联。为推进这一概念，CoCal引入了一个层次化的字典组件结构，通过整合同级别内的对比成分和跨级别的逻辑约束来实现这一点。具体来说，CoCal在每一级上采用逐部件对比算法，使同一类别的字典组件与其不同类别的字典组件进行比较。此外，为了处理逻辑问题，CoCal确保表示特定部分的字典组件比其他对象更接近于相应的物体组件。为进一步增强逻辑关系建模，我们实现了一种后处理函数，该函数基于一个原理：分配给某个部分的像素也应该分配给其对应的对象。通过这些创新，CoCal在PartImageNet和Pascal-Part-108上建立了新的最佳性能，在部分mIoU方面分别比先前方法高出2.08%和0.70%，并且在整个质量的对象分割上也有显著改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this work, we present CoCal, an interpretable and consistent objectparsing framework based on dictionary-based mask transformer. Designed aroundContrastive Components and Logical Constraints, CoCal rethinks existingcluster-based mask transformer architectures used in segmentation;Specifically, CoCal utilizes a set of dictionary components, with eachcomponent being explicitly linked to a specific semantic class. To advance thisconcept, CoCal introduces a hierarchical formulation of dictionary componentsthat aligns with the semantic hierarchy. This is achieved through theintegration of both within-level contrastive components and cross-level logicalconstraints. Concretely, CoCal employs a component-wise contrastive algorithmat each semantic level, enabling the contrasting of dictionary componentswithin the same class against those from different classes. Furthermore, CoCaladdresses logical concerns by ensuring that the dictionary componentrepresenting a particular part is closer to its corresponding object componentthan to those of other objects through a cross-level contrastive learningobjective. To further enhance our logical relation modeling, we implement apost-processing function inspired by the principle that a pixel assigned to apart should also be assigned to its corresponding object. With theseinnovations, CoCal establishes a new state-of-the-art performance on bothPartImageNet and Pascal-Part-108, outperforming previous methods by asignificant margin of 2.08% and 0.70% in part mIoU, respectively. Moreover,CoCal exhibits notable enhancements in object-level metrics across thesebenchmarks, highlighting its capacity to not only refine parsing at a finerlevel but also elevate the overall quality of object segmentation.</description>
      <author>example@mail.com (Tiezheng Zhang, Qihang Yu, Alan Yuille, Ju He)</author>
      <guid isPermaLink="false">2502.19540v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Tell me why: Visual foundation models as self-explainable classifiers</title>
      <link>http://arxiv.org/abs/2502.19577v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;视觉基础模型（VFMs）由于其先进的性能而越来越受欢迎。然而，可解释性对于关键应用仍然至关重要。&lt;h4&gt;背景&lt;/h4&gt;视觉基础模型在许多任务中表现出色，但它们的决策过程往往缺乏透明度和理解力。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型自解释模型(SEM)，旨在提供能够分解预测为一系列可解释概念加权和的分类器。&lt;h4&gt;方法&lt;/h4&gt;结合了VFM与原型架构及专门训练目标。通过在冻结状态下的VFMs之上仅微调一个轻量级头（约1百万参数），实现了一种高效且可解释性的解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;评估显示该方法能实现竞争性的分类性能，同时在一系列文献衍生出的可解释性指标上超越现有模型。&lt;h4&gt;结论&lt;/h4&gt;研究提出的方法ProtoFM提供了高效的、可解释的视觉基础模型框架，并展示了其在实际应用中的潜力和效果。&lt;h4&gt;翻译&lt;/h4&gt;视觉基础模型（VFMs）因其卓越的表现而变得越来越流行。然而，在关键的应用场景中，依然需要重视可解释性问题。因此，自解释模型(SEM)试图提供一种分解预测为一系列可解释概念加权和的分类器形式。尽管这些模型显示出潜力，但最近的研究表明它们所提供的解释往往缺乏忠实度。为此，本工作将视觉基础模型与新颖的原型架构结合，并采用专门训练目标进行优化。通过仅在冻结状态下的VFMs之上微调一个轻量级头（约1百万参数），研究提出的方法(ProtoFM)提供了一种高效的、可解释性的解决方案。评估结果表明该方法能够实现竞争性分类性能，同时在一系列从文献中衍生出的可解释性指标上超越现有模型。相关代码可在https://github.com/hturbe/proto-fm获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual foundation models (VFMs) have become increasingly popular due to theirstate-of-the-art performance. However, interpretability remains crucial forcritical applications. In this sense, self-explainable models (SEM) aim toprovide interpretable classifiers that decompose predictions into a weightedsum of interpretable concepts. Despite their promise, recent studies have shownthat these explanations often lack faithfulness. In this work, we combine VFMswith a novel prototypical architecture and specialized training objectives. Bytraining only a lightweight head (approximately 1M parameters) on top of frozenVFMs, our approach (ProtoFM) offers an efficient and interpretable solution.Evaluations demonstrate that our approach achieves competitive classificationperformance while outperforming existing models across a range ofinterpretability metrics derived from the literature. Code is available athttps://github.com/hturbe/proto-fm.</description>
      <author>example@mail.com (Hugues Turbé, Mina Bjelogrlic, Gianmarco Mengaldo, Christian Lovis)</author>
      <guid isPermaLink="false">2502.19577v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.19247v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 3 figures. Accepted by CVPR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Proxy Transformation的方法，该方法适用于多模态任务，并可以有效地改善点云的流形结构。这种方法利用了Deformable Point Clustering来识别目标区域中的点云子流形，然后提出了一个使用多模式代理指导点云变换的Proxy Attention模块。&lt;h4&gt;背景&lt;/h4&gt;以语言指令为基础与3D环境实时交互是具身智能的基础任务之一。然而，从RGB-D图像渲染出来的点云包含了大量冗余的背景数据和固有的噪声，这些都可能干扰目标区域的真实结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种有效的方法来改善点云流形结构以适应实时任务的需求。&lt;h4&gt;方法&lt;/h4&gt;该方法首先利用Deformable Point Clustering技术识别出点云子流形，接着使用Proxy Attention模块以及通过文本信息全局指导不同子流形的转换向量优化目标区域的相对空间关系。同时，图像信息引导每个子流形内的线性变换，以细化目标区域的本地点云流形。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在容易的目标上比现有所有方法提高了7.49%，在困难的目标上提高了4.60%；同时还减少了注意力块的计算开销达40.6%。&lt;h4&gt;结论&lt;/h4&gt;这种方法确立了新的SOTA（State of the Art）结果，在以自我为中心的3D视觉定位中展示了其有效性和鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;具身智能要求代理能够根据语言指令实时地与三维环境进行交互。在这一领域的一个基础任务是自我中心的3D视觉定位，然而，从RGB-D图像渲染出来的点云保留了大量的冗余背景数据和固有的噪声，这些都可能干扰目标区域的真实结构。现有的点云增强方法通常需要繁琐的过程来改进流形结构，并且不适合实时任务。我们提出了一种适用于多模态任务的Proxy Transformation方法，可以有效地改善点云流形结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence requires agents to interact with 3D environments inreal time based on language instructions. A foundational task in this domain isego-centric 3D visual grounding. However, the point clouds rendered from RGB-Dimages retain a large amount of redundant background data and inherent noise,both of which can interfere with the manifold structure of the target regions.Existing point cloud enhancement methods often require a tedious process toimprove the manifold, which is not suitable for real-time tasks. We proposeProxy Transformation suitable for multimodal task to efficiently improve thepoint cloud manifold. Our method first leverages Deformable Point Clustering toidentify the point cloud sub-manifolds in target regions. Then, we propose aProxy Attention module that utilizes multimodal proxies to guide point cloudtransformation. Built upon Proxy Attention, we design a submanifoldtransformation generation module where textual information globally guidestranslation vectors for different submanifolds, optimizing relative spatialrelationships of target regions. Simultaneously, image information guideslinear transformations within each submanifold, refining the local point cloudmanifold of target regions. Extensive experiments demonstrate that ProxyTransformation significantly outperforms all existing methods, achieving animpressive improvement of 7.49% on easy targets and 4.60% on hard targets,while reducing the computational overhead of attention blocks by 40.6%. Theseresults establish a new SOTA in ego-centric 3D visual grounding, showcasing theeffectiveness and robustness of our approach.</description>
      <author>example@mail.com (Qihang Peng, Henry Zheng, Gao Huang)</author>
      <guid isPermaLink="false">2502.19247v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>TRIX: A More Expressive Model for Zero-shot Domain Transfer in Knowledge Graphs</title>
      <link>http://arxiv.org/abs/2502.19512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;全归纳知识图谱模型可以在多个领域上进行训练，并随后在新的未见领域中执行零样本知识图谱补全(KGC)，这是朝着构建知识图谱基础模型的目标迈进的重要一步。&lt;h4&gt;目的&lt;/h4&gt;介绍一种更富有表现力和能力的全归纳模型TRIX，该模型不仅提供了比现有最佳方法更具表达性的三元组嵌入（头实体、关系、尾实体），还引入了一种新的能力：直接处理归纳设置下的实体和关系预测任务。&lt;h4&gt;方法&lt;/h4&gt;TRIX通过改进的三元组表示方式提高了零样本场景下的性能，同时可以直接应对新领域中的实体与关系预测任务，并且其表现优于现有的全归纳模型以及大上下文语言模型在未见领域的预测。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，TRIX在新的域中以零样本的方式进行实体和关系预测方面优于现有最佳的全归纳模型；同时，在超出训练集范围的领域预测任务上也表现出比大型上下文LLM更好的性能。&lt;h4&gt;结论&lt;/h4&gt;TRIX展示出了强大的零样本泛化能力以及应对新领域挑战的实力，对于构建通用知识图谱基础模型具有重要意义。源代码可以在GitHub上找到：https://github.com/yuchengz99/TRIX。&lt;h4&gt;翻译&lt;/h4&gt;完全归纳的知识图谱模型可以针对多个领域进行训练，并在新的未见领域中执行零样本知识图谱补全任务，这是向构建知识图谱基础模型迈进的重要一步。在这项工作中，我们引入了一种更为表达力强且能力全面的TRIX模型，它不仅提供了比现有最佳方法更强大的三元组嵌入（头实体、关系、尾实体），而且还引入了一种新的功能：直接处理归纳设置下的实体和关系预测任务。经验上表明，TRIX在新领域中的零样本实体与关系预测中优于现有的全归纳模型，并且在超出训练范围的领域预测中也超过了大型上下文LLM。源代码可从https://github.com/yuchengz99/TRIX获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fully inductive knowledge graph models can be trained on multiple domains andsubsequently perform zero-shot knowledge graph completion (KGC) in new unseendomains. This is an important capability towards the goal of having foundationmodels for knowledge graphs. In this work, we introduce a more expressive andcapable fully inductive model, dubbed TRIX, which not only yields strictly moreexpressive triplet embeddings (head entity, relation, tail entity) compared tostate-of-the-art methods, but also introduces a new capability: directlyhandling both entity and relation prediction tasks in inductive settings.Empirically, we show that TRIX outperforms the state-of-the-art fully inductivemodels in zero-shot entity and relation predictions in new domains, andoutperforms large-context LLMs in out-of-domain predictions. The source code isavailable at https://github.com/yuchengz99/TRIX.</description>
      <author>example@mail.com (Yucheng Zhang, Beatrice Bevilacqua, Mikhail Galkin, Bruno Ribeiro)</author>
      <guid isPermaLink="false">2502.19512v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>SPU-IMR: Self-supervised Arbitrary-scale Point Cloud Upsampling via Iterative Mask-recovery Network</title>
      <link>http://arxiv.org/abs/2502.19452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的点云上采样方法，该方法将点云上采样视为全局形状补全问题。通过设计的神经网络迭代地完成缺失部分，以获得稠密且均匀分布的点集。&lt;h4&gt;背景&lt;/h4&gt;现有的点云上采样方法通常将其视为插值问题，通过在局部或特征空间中进行插值来实现上采样。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的全局形状补全算法来解决点云上采样的问题，并展示其优于现有自监督和有监督方法的性能。&lt;h4&gt;方法&lt;/h4&gt;首先将点云划分成多个块，然后通过掩码操作移除一些块，留下可见的点云块。使用设计好的神经网络迭代地完成缺失部分，直至恢复所有补全后的块。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在量化和定性实验中展示了优越性能，优于现有的自监督和有监督方法。&lt;h4&gt;结论&lt;/h4&gt;通过将点云上采样视为全局形状补全问题，本文设计的算法能够生成稠密且均匀分布的点集，并显示出良好的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud upsampling aims to generate dense and uniformly distributed pointsets from sparse point clouds. Existing point cloud upsampling methodstypically approach the task as an interpolation problem. They achieveupsampling by performing local interpolation between point clouds or in thefeature space, then regressing the interpolated points to appropriatepositions. By contrast, our proposed method treats point cloud upsampling as aglobal shape completion problem. Specifically, our method first divides thepoint cloud into multiple patches. Then, a masking operation is applied toremove some patches, leaving visible point cloud patches. Finally, ourcustom-designed neural network iterative completes the missing sections of thepoint cloud through the visible parts. During testing, by selecting differentmask sequences, we can restore various complete patches. A sufficiently denseupsampled point cloud can be obtained by merging all the completed patches. Wedemonstrate the superior performance of our method through both quantitativeand qualitative experiments, showing overall superiority against both existingself-supervised and supervised methods.</description>
      <author>example@mail.com (Ziming Nie, Qiao Wu, Chenlei Lv, Siwen Quan, Zhaoshuai Qi, Muze Wang, Jiaqi Yang)</author>
      <guid isPermaLink="false">2502.19452v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Robust Prediction of Frictional Contact Network in Near-Jamming Suspensions Employing Deep Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18743v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了悬浮液中细颗粒的分散行为，特别是在接近堵塞状态时的行为。通过引入一种成本效益高的机器学习方法（基于图神经网络GNN）来预测摩擦接触网FCN，该方法能够捕捉密集悬浮液中的隐藏特征和底层模式。&lt;h4&gt;背景&lt;/h4&gt;细小颗粒在牛顿流体中形成悬浊液的粘度会在接近堵塞密度时发散。这种宏观行为受微结构影响，尤其是由机械承载力接触点组成的摩擦接触网（FCN）的影响。应力传递和网络拓扑对颗粒相对运动的约束敏感。&lt;h4&gt;目的&lt;/h4&gt;预测接近堵塞条件下的摩擦接触网FCN仍然是一个挑战。为了应对这一问题，本研究引入了一种新的机器学习方法来解决这个问题。&lt;h4&gt;方法&lt;/h4&gt;使用一种称为Deep Graph Convolutional Network（DeepGCN）的GNN变体在数据驱动模拟上进行训练，并证明了该模型具有强大的泛化和外推能力。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，这种方法能够准确预测处于分异流动参数和相空间中的系统的FCNs。此外，研究涵盖了从半稀释状态到堵塞状态的整个范围，并系统地变化诸如剪切应力、颗粒填充率以及滑动摩擦力和滚动摩擦力等参数。&lt;h4&gt;结论&lt;/h4&gt;这项工作的结果为预测颗粒体系性质提供了创新且可转移的技术途径，为材料科学及相关领域开辟了新的前进道路。&lt;h4&gt;翻译&lt;/h4&gt;悬浮液中细小颗粒分散在牛顿流体中的粘度接近堵塞密度时发散。微结构中的接触微观结构通过摩擦接触网（FCN）控制这种宏观行为。该网络由机械承载力接触点组成，这些接触点导致在接近堵塞过渡时刚性的出现。应力传递和网络拓扑对颗粒相对运动的约束敏感。尽管其重要性，由于实验和计算障碍，预测FCN尤其是在接近堵塞条件下仍然是一个挑战。本研究提出了使用图神经网络（GNN）预测FCN的一种成本效益高的机器学习方法，该方法通过映射粒子间相互作用来固有地捕捉密集悬浮液中的隐藏特征和潜在模式。利用一种称为Deep Graph Convolutional Network (DeepGCN)的GNN变体在数据驱动模拟上进行训练，本研究展示了强大的泛化能力和外推能力，在分异流动参数和相空间中准确预测FCNs，尽管每个条件仅单独受训。该研究涵盖了广泛的相空间范围，从半稀释状态到堵塞状态，并跨越瞬态和稳态，同时系统地变化诸如剪切应力、颗粒填充率以及滑动摩擦力和滚动摩擦力等参数。这项研究的结果为预测颗粒体系性质提供了创新且可转移的技术途径，为材料科学及相关领域开辟了新的前进道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The viscosity of the suspension consisting of fine particles dispersed in aNewtonian liquid diverges close to the jamming packing fraction. The contactmicrostructure in suspensions governs this macroscopic behavior in the vicinityof jamming through a frictional contact network (FCN). FCN is composed ofmechanical load-bearing contacts that lead to the emergence of rigidity nearthe jamming transition. The stress transmission and network topology, in turn,depend sensitively on constraints on the relative motion of the particles.Despite their significance, predicting the FCN, especially close to jammingconditions, remains challenging due to experimental and computationalimpediments. This study introduces a cost-effective machine learning approachto predict the FCN using a graph neural network (GNN), which inherentlycaptures hidden features and underlying patterns in dense suspension by mappinginterparticle interactions. Employing a variation of GNN called the Deep GraphConvolutional Network (DeepGCN) trained on data-driven simulations, this studydemonstrates robust generalization and extrapolation capabilities, accuratelypredicting FCNs in systems with divergent flow parameters and phase spaces,despite each being trained exclusively on a single condition. The study coversa wide range of phase space, from semi-dilute to jammed states, spanningtransient to steady states, while systematically varying parameters such asshear stress (${\sigma}_{xy}$), packing fraction(${\phi}$) and sliding androlling friction (${{\mu}_s, {\mu}_r}$). The results of this research pave theway for innovative transferable techniques in predicting the properties ofparticulate systems, offering new avenues for advancement in material scienceand related fields.</description>
      <author>example@mail.com (Armin Aminimajd, Joao Maia, Abhinendra Singh)</author>
      <guid isPermaLink="false">2502.18743v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Foundation-Model-Based Industrial Defect Detection</title>
      <link>http://arxiv.org/abs/2502.19106v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;随着工业产品种类的增多和复杂度提高，视觉工业缺陷检测受到了广泛的关注。传统的研究方法依赖于统计分析、异常数据合成建模以及基于生成模型的方法来分离产品的缺陷特征并完成缺陷检测任务。&lt;h4&gt;背景&lt;/h4&gt;近年来，基础模型（Foundation Model, FM）的出现带来了丰富的视觉和文本语义先验知识。&lt;h4&gt;目的&lt;/h4&gt;探讨基于基础模型与非基础模型方法在工业产品缺陷检测领域的应用及优劣，并分析未来研究方向。&lt;h4&gt;方法&lt;/h4&gt;本论文系统地对各种基于基础模型的方法进行了比较、分类和讨论，同时简要回顾了近期发布的部分非基础模型（NFM）方法。详细分析了FM和NFM之间从训练目标到性能表现的差异，指出了可能的研究前景。&lt;h4&gt;主要发现&lt;/h4&gt;相比非基础模型方法，基础模型方法更适用于少量样本学习乃至零样本学习，在实际工业应用场景中更为契合，并值得深入研究。&lt;h4&gt;结论&lt;/h4&gt;虽然基于基础模型的方法提高了缺陷检测准确性，但同时也增加了模型复杂性和推理速度的降低。一些基于基础模型的方法开始探索轻量化建模方式，逐渐吸引人们的注意并值得系统化分析。&lt;h4&gt;翻译&lt;/h4&gt;随着工业产品变得丰富和精细，视觉工业缺陷检测（包括二维和三维视觉特征建模）受到了广泛的关注。传统的研究方法利用统计分析、异常数据合成建模以及基于生成模型的方法来分离产品的缺陷特征，并完成缺陷检测任务。近年来，基础模型的出现带来了丰富的视觉和文本语义先验知识，许多基于此的研究旨在提高检测准确性但同时增加了模型复杂性和推理速度下降的问题。一些基于基础模型方法已经开始探索轻量级建模方式，逐渐引起了人们的关注并值得系统化分析。本论文系统地对各种基于基础模型的方法进行了比较、分类和讨论，并简要回顾了近期发布的部分非基础模型（NFM）方法；此外还从训练目标、模型结构与规模以及性能表现等角度详细对比了FM与NFM之间的差异，指出了未来可能的研究方向。通过对比研究发现，基于基础模型的方法更适合少量样本学习乃至零样本学习，这些更符合实际工业应用场景，并值得深入探索。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As industrial products become abundant and sophisticated, visual industrialdefect detection receives much attention, including two-dimensional andthree-dimensional visual feature modeling. Traditional methods use statisticalanalysis, abnormal data synthesis modeling, and generation-based models toseparate product defect features and complete defect detection. Recently, theemergence of foundation models has brought visual and textual semantic priorknowledge. Many methods are based on foundation models (FM) to improve theaccuracy of detection, but at the same time, increase model complexity and slowdown inference speed. Some FM-based methods have begun to explore lightweightmodeling ways, which have gradually attracted attention and deserve to besystematically analyzed. In this paper, we conduct a systematic survey withcomparisons and discussions of foundation model methods from different aspectsand briefly review non-foundation model (NFM) methods recently published.Furthermore, we discuss the differences between FM and NFM methods fromtraining objectives, model structure and scale, model performance, andpotential directions for future exploration. Through comparison, we find FMmethods are more suitable for few-shot and zero-shot learning, which are morein line with actual industrial application scenarios and worthy of in-depthresearch.</description>
      <author>example@mail.com (Tianle Yang, Luyao Chang, Jiadong Yan, Juntao Li, Zhi Wang, Ke Zhang)</author>
      <guid isPermaLink="false">2502.19106v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.17860v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025; Corrected citation of Uni3D;&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了UniGS，一种将3D高斯散射技术与多模态预训练结合的方法，旨在提升对三维世界的表达能力。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态预训练方法虽然在学习文本、图像和点云的联合表示方面取得了进展，但使用离散点作为三维表示无法充分捕捉三维世界中的细微差别，并且存在二维像素与三维点之间的明显差距。&lt;h4&gt;目的&lt;/h4&gt;引入3D高斯散射技术来增强多模态预训练模型对三维世界的表达能力，同时提升跨模态对齐效果。&lt;h4&gt;方法&lt;/h4&gt;UniGS首先使用基于3D高斯散射的表示形式建模三维世界，并通过一个共享视觉和文本空间的语言-图像预训练模型建立语言、图像和点云之间的联系。接着，它利用一个三维编码器将优化后的3D高斯散射与语言-图像表示对齐以学习统一的多模态表示。&lt;h4&gt;主要发现&lt;/h4&gt;在Objaverse、ABO、MVImgNet 和 SUN RGBD 数据集上的零样本分类、文本驱动检索和开放世界理解任务中，UniGS展示了比现有SOTA模型（如Uni3D）更优越的表现。具体而言，在不同三维任务上取得了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;通过引入3D高斯散射技术及Gaussian-Aware Guidance模块，UniGS能够学习到更通用且对齐更好的多模态表示。&lt;h4&gt;翻译&lt;/h4&gt;最近在多模式3D预训练方法方面取得的进步显示出在文本、图像和点云联合表示学习方面的显著有效性。然而，采用点云作为三维表达无法充分捕捉三维世界的细微差别，并且存在二维像素与三维点之间的明显差距。为解决这一问题，我们提出UniGS，在多模态预训练中整合3D高斯散射技术以增强三维表示。我们首先利用基于3D高斯散射的表示形式建模整个三维世界为带有颜色和不透明度的一系列3D高斯分布，并通过广泛的现实图像-文本对，依靠语言模型建立共享视觉和文本空间。随后，UniGS使用一个3D编码器将优化后的3D高斯散射与语言-图像表示对齐以学习统一的多模态表示。为促进3D编码器提取全局显式三维特征并实现更好的跨模式对齐，我们进一步引入了一个新颖的Gaussian-Aware Guidance模块来指导3D领域的细粒度表示的学习过程。在Objaverse、ABO、MVImgNet 和 SUN RGBD 数据集上的零样本分类、文本驱动检索和开放世界理解任务中，通过广泛实验展示了UniGS学习更通用且更强对齐多模态表征的有效性，并在不同三维任务上取得显著优于现有最佳方法（如Uni3D）的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in multi-modal 3D pre-training methods have shownpromising efficacy in learning joint representations of text, images, and pointclouds. However, adopting point clouds as 3D representation fails to fullycapture the intricacies of the 3D world and exhibits a noticeable gap betweenthe discrete points and the dense 2D pixels of images. To tackle this issue, wepropose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modalpre-training to enhance the 3D representation. We first rely on the 3DGSrepresentation to model the 3D world as a collection of 3D Gaussians with colorand opacity, incorporating all the information of the 3D scene whileestablishing a strong connection with 2D images. Then, to achieveLanguage-Image-3D pertaining, UniGS starts with a pre-trained vision-languagemodel to establish a shared visual and textual space through extensivereal-world image-text pairs. Subsequently, UniGS employs a 3D encoder to alignthe optimized 3DGS with the Language-Image representations to learn unifiedmulti-modal representations. To facilitate the extraction of global explicit 3Dfeatures by the 3D encoder and achieve better cross-modal alignment, weadditionally introduce a novel Gaussian-Aware Guidance module that guides thelearning of fine-grained representations of the 3D domain. Through extensiveexperiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets withzero-shot classification, text-driven retrieval and open-world understandingtasks, we demonstrate the effectiveness of UniGS in learning a more general andstronger aligned multi-modal representation. Specifically, UniGS achievesleading results across different 3D tasks with remarkable improvements overprevious SOTA, Uni3D, including on zero-shot classification (+9.36%),text-driven retrieval (+4.3%) and open-world understanding (+7.92%).</description>
      <author>example@mail.com (Haoyuan Li, Yanpeng Zhou, Tao Tang, Jifei Song, Yihan Zeng, Michael Kampffmeyer, Hang Xu, Xiaodan Liang)</author>
      <guid isPermaLink="false">2502.17860v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了使用机器学习技术来预测COVID-19患者的病情严重程度，通过整合三个数据源创建了一个大规模的COVID严重性数据库，并评估了迁移学习和视觉变换器在预测病情方面的有效性。&lt;h4&gt;背景&lt;/h4&gt;COVID-19大流行给医疗资源带来了巨大压力，促使人们讨论如何利用机器学习减轻医生负担并辅助诊断。胸部X光片（CXRs）用于诊断COVID-19，但很少有研究从这些影像中预测患者的病情严重程度。&lt;h4&gt;目的&lt;/h4&gt;通过整合多个数据源创建一个大规模的COVID严重性数据库，并评估迁移学习和视觉变换器在病情严重度分类与回归任务中的有效性。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模型（如基于ImageNet和CXRs）及视觉变换器（ViTs）进行迁移学习。其中，采用DenseNet161模型进行了三类预测问题的试验，并评估了其分类准确性和视觉变换器在病情严重度回归任务中的性能。&lt;h4&gt;主要发现&lt;/h4&gt;预训练的DenseNet161模型在三个类别上达到了最高的整体准确性（80%）；而ViT则在病情严重度回归中表现最好，平均绝对误差为0.5676，接近放射科医生预测的结果。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，在COVID-19病情预测方面，迁移学习和视觉变换器显示出潜力。预训练的DenseNet161模型对三分类问题表现出最佳性能；ViT在回归任务中表现优异。该项目的源代码是公开可用的。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/stwhitfield/covid-severity&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju)</author>
      <guid isPermaLink="false">2502.16622v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multispectral to Hyperspectral using Pretrained Foundational model</title>
      <link>http://arxiv.org/abs/2502.19451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结合高光谱和多光谱成像系统优势的方法，以改善温室气体监测。&lt;h4&gt;背景&lt;/h4&gt;高光谱成像提供了详细的光谱信息，对监测如CH4和NO2等温室气体具有重要意义。但是其应用受到空间覆盖率低和重复访问频率不高的限制。相比之下，多光谱成像能提供更广的空间和时间覆盖范围，但缺乏精细的光谱分辨率以进行精确的温室气体检测。&lt;h4&gt;目的&lt;/h4&gt;为了解决高光谱和多光谱成像技术之间的局限性问题，该研究提出了空间-光谱变换模型来重建从多光谱输入的数据。&lt;h4&gt;方法&lt;/h4&gt;研究中提出的模型先是在EnMAP和EMIT数据集上进行预训练，然后分别在（Sentinel-2, EnMAP）和（HLS-S30, EMIT）图像对上微调。&lt;h4&gt;主要发现&lt;/h4&gt;通过将高光谱成像的精细光谱分辨率与多光谱成像的空间覆盖范围和时间频率相结合，可以提高大气监测的效率。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了结合使用高光谱和多光谱数据的有效性，并展示了利用这些技术改善温室气体检测方法的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：高光谱成像提供了详细的光谱信息，为CH4和NO2等温室气体的监测提供重要意义。然而，其应用受到空间覆盖率低和重复访问频率不高的限制。相比之下，多光谱成像能提供更广的空间和时间覆盖范围，但缺乏精细的光谱分辨率以进行精确的温室气体检测。为了应对这些挑战，本研究提出了空间-光谱变换模型来从多光谱输入中重建高光谱数据。本文中的模型先是在EnMAP和EMIT数据集上预训练，并在（Sentinel-2, EnMAP）和（HLS-S30, EMIT）图像对上微调。我们的模型结合了高光谱和多光谱成像系统的优点，有潜力提升大气监测能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral imaging provides detailed spectral information, offeringsignificant potential for monitoring greenhouse gases like CH4 and NO2.However, its application is constrained by limited spatial coverage andinfrequent revisit times. In contrast, multispectral imaging delivers broaderspatial and temporal coverage but lacks the spectral granularity required forprecise GHG detection. To address these challenges, this study proposesSpectral and Spatial-Spectral transformer models that reconstruct hyperspectraldata from multispectral inputs. The models in this paper are pretrained onEnMAP and EMIT datasets and fine-tuned on spatio-temporally aligned(Sentinel-2, EnMAP) and (HLS-S30, EMIT) image pairs respectively. Our model hasthe potential to enhance atmospheric monitoring by combining the strengths ofhyperspectral and multispectral imaging systems.</description>
      <author>example@mail.com (Ruben Gonzalez, Conrad M Albrecht, Nassim Ait Ali Braham, Devyani Lambhate, Joao Lucas de Sousa Almeida, Paolo Fraccaro, Benedikt Blumenstiel, Thomas Brunschwiler, Ranjini Bangalore)</author>
      <guid isPermaLink="false">2502.19451v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17371v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了将光伏系统集成到温室中的效果，同时引入了一种新的时空图神经网络（STGNN）应用于温室微气候建模。&lt;h4&gt;背景&lt;/h4&gt;光伏系统的集成可以优化土地使用，并通过实现食品生产和可再生能源发电的双重效益来增强可持续农业实践。但是，准确预测内部环境条件对于确保作物生长的最佳状态和最大限度地提高能源生产至关重要。&lt;h4&gt;目的&lt;/h4&gt;引入一种新的时空图神经网络（STGNN）模型应用于温室微气候建模，并与传统的递归神经网络（RNN）进行性能比较。&lt;h4&gt;方法&lt;/h4&gt;研究利用在希腊沃洛斯收集的每15分钟一次的高频数据，展示了传统递归神经网络（RNNs）在冬季条件下的卓越准确度，但对夏季冷却系统操作时的表现有限。尽管STGNN模型当前表现较低，但它能够更好地整合包括光伏发电量和作物生长指标在内的额外变量。&lt;h4&gt;主要发现&lt;/h4&gt;传统的RNN模型擅长时间序列模式识别，但在处理环境变量之间的方向关系方面存在局限性；而新的STGNN架构可以解决这些问题，并且能够捕捉空间依赖性和它们的方向性。&lt;h4&gt;结论&lt;/h4&gt;虽然当前STGNN的表现略低于RNN（冬季R^2为0.947相比RNN的0.985），但其模型结构具有更大的潜力来整合更多变量，从而更好地适应和优化温室环境条件。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of photovoltaic (PV) systems into greenhouses not onlyoptimizes land use but also enhances sustainable agricultural practices byenabling dual benefits of food production and renewable energy generation.However, accurate prediction of internal environmental conditions is crucial toensure optimal crop growth while maximizing energy production. This studyintroduces a novel application of Spatio-Temporal Graph Neural Networks(STGNNs) to greenhouse microclimate modeling, comparing their performance withtraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporalpattern recognition, they cannot explicitly model the directional relationshipsbetween environmental variables. Our STGNN approach addresses this limitationby representing these relationships as directed graphs, enabling the model tocapture both spatial dependencies and their directionality. Usinghigh-frequency data collected at 15-minute intervals from a greenhouse inVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winterconditions (R^2 = 0.985) but show limitations during summer cooling systemoperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),their architecture offers greater potential for integrating additionalvariables such as PV generation and crop growth indicators.</description>
      <author>example@mail.com (Emiliano Seri, Marcello Petitta, Cristina Cornaro)</author>
      <guid isPermaLink="false">2502.17371v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Evolutionary Algorithms Approach For Search Based On Semantic Document Similarity</title>
      <link>http://arxiv.org/abs/2502.19437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;云计算和分布式计算的发展促进了计算机科学领域的研究活动。这些领域在神经网络、遗传算法等进化计算算法方面取得了显著进展。&lt;h4&gt;目的&lt;/h4&gt;利用通用句子编码器(USE)捕捉文本语义相似性，并使用迁移学习技术将遗传算法(GA)和差分演化(DE)应用于基于用户查询检索相关文档的搜索与检索。&lt;h4&gt;方法&lt;/h4&gt;在斯坦福问答数据集(SQuAD)上应用所提出的方法，以识别用户查询。研究中采用曼哈顿距离、GA和DE算法进行对比实验。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明：文本可以通过USE高效地表示为句子嵌入向量；进化算法（如GA和DE）在查找顶级N结果方面比传统排名方法更有效。&lt;h4&gt;结论&lt;/h4&gt;使用进化计算算法来搜索和检索文档是一个有潜力的研究领域，它可以在各种应用中实现更好的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3617733.3617753&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in cloud computing and distributed computing have fosteredresearch activities in Computer science. As a result, researchers have madesignificant progress in Neural Networks, Evolutionary Computing Algorithms likeGenetic, and Differential evolution algorithms. These algorithms are used todevelop clustering, recommendation, and question-and-answering systems usingvarious text representation and similarity measurement techniques. In thisresearch paper, Universal Sentence Encoder (USE) is used to capture thesemantic similarity of text; And the transfer learning technique is used toapply Genetic Algorithm (GA) and Differential Evolution (DE) algorithms tosearch and retrieve relevant top N documents based on user query. The proposedapproach is applied to the Stanford Question and Answer (SQuAD) Dataset toidentify a user query. Finally, through experiments, we prove that textdocuments can be efficiently represented as sentence embedding vectors usingUSE to capture the semantic similarity, and by comparing the results of theManhattan Distance, GA, and DE algorithms we prove that the evolutionaryalgorithms are good at finding the top N results than the traditional rankingapproach.</description>
      <author>example@mail.com (Chandrashekar Muniyappa, Eujin Kim)</author>
      <guid isPermaLink="false">2502.19437v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Agnostic calculation of atomic free energies with the descriptor density of states</title>
      <link>http://arxiv.org/abs/2502.18191v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的评估原子系统振动自由能的方法，该方法不需要预先指定原子间势，并且能够通过描述符的高维密度熵进行准确估算。&lt;h4&gt;背景&lt;/h4&gt;传统的计算原子系统的振动自由能需要先验地定义原子间势函数。这种方法限制了模型的灵活性和通用性。&lt;h4&gt;目的&lt;/h4&gt;开发一种与模型无关的方法来评估原子系统在不同条件下的振动自由能，以便于不确定性量化和逆向设计。&lt;h4&gt;方法&lt;/h4&gt;使用描述符（即高维特征向量）表示原子结构，并通过条件分数匹配准确估计这些描述符的密度熵。利用Legendre-Fenchel共轭关系将自由能与描述符熵联系起来，避免了复杂的高维积分计算。&lt;h4&gt;主要发现&lt;/h4&gt;该方法可以快速、精确地预测各种材料在不同相态下的振动自由能，并且通过反向传播技术成功降低了Fe的α-γ转变温度。&lt;h4&gt;结论&lt;/h4&gt;此模型无关的方法不仅能准确评估原子系统的自由能，还能应用于液体和其他基础模型的微调中。它为解决计算科学中的高维积分问题提供了新思路。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新的方法来评估没有预先指定原子间势的情况下原子系统振动自由能的新方法。我们的模型无关的方法利用描述符（即原子结构的高维特征向量）进行表示，并通过条件分数匹配准确估计这些描述符的密度熵。通过将原子间势转换为在描述符特性上扩增的形式，我们展示了自由能在描述符熵的Legendre-Fenchel共轭中出现，从而避免了所有高维积分计算。所需的评分匹配活动比固定模型采样需要更少的资源，并且可以高度并行化，将实际时间减少到几分钟以内。我们的模型无关的估计器能够以微秒级的CPU努力在广泛的潜在参数范围内返回可微分自由能预测，这允许快速向前和反向传播潜在变化通过有限温度模拟，这是为了不确定性量化和逆向设计而长期需要的功能。我们通过对W、Mo和Fe的BCC、FCC和A15相态进行热力学集成计算进行了广泛的模型测试，并且在高温同源条件下预测通过了严格的准确性阈值（每原子1-2 meV或0.25至0.5 kcal/mol）用于阶段预测。我们还展示了目标微调，通过对Fe的非磁性机器学习模型使用反向传播技术将其α-γ转变温度从2030 K降低到1063 K，并且无需额外采样。讨论了该方法在液体以及基础模型细调中的应用及其对计算科学中众多估计高维积分问题的潜在影响。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to evaluate vibrational free energies of atomicsystems without a priori specification of an interatomic potential. Ourmodel-agnostic approach leverages descriptors, high-dimensional feature vectorsof atomic structure. The entropy of a high-dimensional density, the descriptordensity of states, is accurately estimated with conditional score matching.Casting interatomic potentials into a form extensive in descriptor features, weshow free energies emerge as the Legendre-Fenchel conjugate of the descriptorentropy, avoiding all high-dimensional integration. The score matching campaignrequires less resources than fixed-model sampling and is highly parallel,reducing wall time to a few minutes, with tensor compression schemes allowinglightweight storage. Our model-agnostic estimator returns differentiable freeenergy predictions over a broad range of potential parameters in microsecondsof CPU effort, allowing rapid forward and back propagation of potentialvariations through finite temperature simulations, long desired for uncertaintyquantification and inverse design. We test predictions against thermodynamicintegration calculations over a broad range of models for BCC, FCC and A15phases of W, Mo and Fe at high homologous temperatures. Predictions pass thestringent accuracy threshold of 1-2 meV/atom (1/40-1/20 kcal/mol) for phaseprediction with propagated score uncertainties robustly bounding errors. Wealso demonstrate targeted fine-tuning, reducing the alpha-gamma transitiontemperature in a non-magnetic machine learning model of Fe from 2030 K to 1063K through back-propagation, with no additional sampling. Applications toliquids and fine-tuning foundational models are discussed along with the manyproblems in computational science which estimate high-dimensional integrals.</description>
      <author>example@mail.com (Thomas D Swinburne, Clovis Lapointe, Mihai-Cosmin Marinica)</author>
      <guid isPermaLink="false">2502.18191v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Transformers: Architectures, Theories and Applications</title>
      <link>http://arxiv.org/abs/2502.16533v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;Graph Transformers (GTs) 的综合回顾&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）在处理图形结构时存在过度平滑和过度压缩等内在局限性，而 Graph Transformers 通过解决这些问题展示出强大的建模能力。&lt;h4&gt;目的&lt;/h4&gt;对最近关于 Graph Transformers 的快速发展进行全面回顾，涵盖架构、理论基础及应用等方面。&lt;h4&gt;方法&lt;/h4&gt;根据处理结构信息的策略分类 Graph Transformers 架构，包括图标记化、位置编码、结构感知注意力和模型集成。从理论上探讨了不同讨论架构下的表达能力，并与其他高级图形学习算法进行对比以发现联系。&lt;h4&gt;主要发现&lt;/h4&gt;Graph Transformers 在分子数据、蛋白质数据、语言处理、视觉识别、交通网络、脑科学及材料学等领域的实际应用中发挥了重要作用。&lt;h4&gt;结论&lt;/h4&gt;指出了当前 Graph Transformers 面临的挑战和未来研究的方向，为潜在的研究提供了可能的道路。&lt;h4&gt;翻译&lt;/h4&gt;Graph Transformer (GTs) 通过解决图神经网络（GNNs）固有的限制问题，在构建图形结构模型方面展现出了强大的能力。最近的研究提议了多样化的架构、增强了可解释性，并探讨了实际应用案例。考虑到这些快速的发展，我们对该领域进行了全面的综述，涵盖了 Graph Transformers 的架构设计、理论依据及其在分子数据、蛋白质数据、语言处理等众多领域的应用实例。此外，还对现有的挑战和未来的研究方向提出了建议。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have demonstrated a strong capability in modelinggraph structures by addressing the intrinsic limitations of graph neuralnetworks (GNNs), such as over-smoothing and over-squashing. Recent studies haveproposed diverse architectures, enhanced explainability, and practicalapplications for Graph Transformers. In light of these rapid developments, weconduct a comprehensive review of Graph Transformers, covering aspects such astheir architectures, theoretical foundations, and applications within thissurvey. We categorize the architecture of Graph Transformers according to theirstrategies for processing structural information, including graph tokenization,positional encoding, structure-aware attention and model ensemble. Furthermore,from the theoretical perspective, we examine the expressivity of GraphTransformers in various discussed architectures and contrast them with otheradvanced graph learning algorithms to discover the connections. Furthermore, weprovide a summary of the practical applications where Graph Transformers havebeen utilized, such as molecule, protein, language, vision, traffic, brain andmaterial data. At the end of this survey, we will discuss the currentchallenges and prospective directions in Graph Transformers for potentialfuture research.</description>
      <author>example@mail.com (Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong)</author>
      <guid isPermaLink="false">2502.16533v2</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Voting Scheme to Strengthen Localization Security in Randomly Deployed Wireless Sensor Networks</title>
      <link>http://arxiv.org/abs/2502.20218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文旨在为存在恶意节点的恶劣环境下的目标定位问题提供一个可信解决方案，这些恶意节点能够操纵距离测量数据（即执行欺骗攻击），从而妨碍准确定位。除了定位外，另一个目的是识别参与过程中的哪些节点是恶意的。&lt;h4&gt;背景&lt;/h4&gt;随着物联网和智能城市应用的扩展，依赖准确位置信息的应用面临严重的安全威胁，因为现有的大多数定位系统都容易受到欺骗攻击。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于聚类和加权质心的投票方案，在对抗性环境下安全地解决定位问题并检测恶意节点。&lt;h4&gt;方法&lt;/h4&gt;{'第一阶段': '选择一组合适的兴趣点簇，并利用问题几何学分配票数以定位目标。', '第二阶段': '通过位置估计和基本统计信息来探测攻击者'}&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在不同设置下被评估为具有较高的本地化精度，成功的恶意节点检测率以及较低的计算复杂度。计算机模拟与现实世界实验验证了该方案的有效性，并显示出相比于当前最先进的方法，可以减少30%的误差并几乎实现完美的攻击者检出率。&lt;h4&gt;结论&lt;/h4&gt;提出的解决方案在定位准确性和攻击者识别方面均优于现有技术，并能显著提高安全性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work aspires to provide a trustworthy solution for target localizationin adverse environments, where malicious nodes, capable of manipulatingdistance measurements (i.e., performing spoofing attacks), are present, thushindering accurate localization. Besides localization, its other goal is toidentify (detect) which of the nodes participating in the process aremalicious. This problem becomes extremely important with the forthcomingexpansion of IoT and smart cities applications, that depend on accuratelocalization, and the presence of malicious attackers can represent serioussecurity threats if not taken into consideration. This is the case with mostexisting localization systems which makes them highly vulnerable to spoofingattacks. In addition, existing methods that are intended for adversarialsettings consider very specific settings or require additional knowledge aboutthe system model, making them only partially secure. Therefore, this workproposes a novel voting scheme based on clustering and weighted central mass tosecurely solve the localization problem and detect attackers. The proposedsolution has two main phases: 1) Choosing a cluster of suitable points ofinterest by taking advantage of the problem geometry to assigning votes inorder to localize the target, and 2) Attacker detection by exploiting thelocation estimate and basic statistics. The proposed method is assessed interms of localization accuracy, success in attacker detection, andcomputational complexity in different settings. Computer simulations andreal-world experiments corroborate the effectiveness of the proposed schemecompared to state-of-the-art methods, showing that it can accomplish an errorreduction of $30~\%$ and is capable of achieving almost perfect attackerdetection rate when the ratio between attacker intensity and noise standarddeviation is significant.</description>
      <author>example@mail.com (Slavisa Tomic, Marko Beko, Dejan Vukobratovic, Srdjan Krco)</author>
      <guid isPermaLink="false">2502.20218v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Systems-of-Systems for Environmental Sustainability: A Systematic Mapping Study</title>
      <link>http://arxiv.org/abs/2502.20021v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该研究探讨了系统之系统的环境可持续性，分析了SoS如何促进碳减排、能源效率和生物多样性保护等可持续实践。&lt;h4&gt;背景信息&lt;/h4&gt;虽然已经有关于智慧城市的系统回顾来讨论可持续性问题，但还没有针对所有类别SoS的系统化知识综合研究。此外，尽管文献中包括其他类型的可持续性（如财务和社会），本研究专注于环境可持续性。&lt;h4&gt;研究目的&lt;/h4&gt;进行系统的映射研究以确定SoS在可持续性应用领域的范围、所面临的挑战以及未来的研究机会。&lt;h4&gt;研究方法&lt;/h4&gt;制定了一个研究协议，其中包括了四个科学数据库的自动化搜索。总共检索到了926个研究项目，并从中选择了39项相关研究进行了分析和报告。&lt;h4&gt;主要发现&lt;/h4&gt;大多数研究集中在智慧城市和智能电网领域；同时，一些应用如可持续农业和森林火灾预防则相对较少探讨。此外，还识别出了系统互操作性、可扩展性和数据治理等挑战。&lt;h4&gt;结论建议&lt;/h4&gt;提出了未来在SoS和环境可持续性方面的研究方向。&lt;h4&gt;翻译&lt;/h4&gt;该论文摘要讨论了关于系统之系统的环境可持续性的新兴领域，并指出尽管有针对智慧城市的系统回顾存在，但缺乏综合现有知识以支持所有类型SoS的广泛应用。研究通过分析39项相关文献揭示了当前主要集中在特定领域的应用以及面临的挑战，并提出了未来的研究方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Environmental sustainability in Systems-of-Systems (SoS) is an emerging fieldthat seeks to integrate technological solutions to promote the efficientmanagement of natural resources. While systematic reviews addresssustainability in the context of Smart Cities (a category of SoS), a systematicstudy synthesizing the existing knowledge on environmental sustainabilityapplied to SoS in general does not exist. Although literature includes othertypes of sustainability, such as financial and social, this study focuses onenvironmental sustainability, analyzing how SoS contribute to sustainablepractices such as carbon emission reduction, energy efficiency, andbiodiversity conservation. We conducted a Systematic Mapping Study to identifythe application domains of SoS in sustainability, the challenges faced, andresearch opportunities. We planned and executed a research protocol includingan automated search over four scientific databases. Of 926 studies retrieved,we selected, analyzed, and reported the results of 39 relevant studies. Ourfindings reveal that most studies focus on Smart Cities and Smart Grids, whileapplications such as sustainable agriculture and wildfire prevention are lessexplored. We identified challenges such as system interoperability,scalability, and data governance. Finally, we propose future researchdirections for SoS and environmental sustainability.</description>
      <author>example@mail.com (Ana Clara Araújo Gomes da Silva, Gilmar Teixeira Junior, Lívia Mancine C. de Campos, Renato F. Bulcão-Neto, Valdemar Vicente Graciano Neto)</author>
      <guid isPermaLink="false">2502.20021v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Sim-to-Real Reinforcement Learning for Vision-Based Dexterous Manipulation on Humanoids</title>
      <link>http://arxiv.org/abs/2502.20396v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page can be found at https://toruowo.github.io/recipe/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;强化学习在实现人类甚至超人类水平能力方面取得了令人振奋的结果，但灵巧机器人操作的成功仍有限。这项工作调查了将强化学习应用于人形代理执行一系列接触密集型任务的关键挑战，并提出了解决这些挑战的新方法。&lt;h4&gt;背景&lt;/h4&gt;强化学习已经在多种问题领域中实现了人类甚至超越人类的性能，但在灵巧的机器人操纵方面进展有限。&lt;h4&gt;目的&lt;/h4&gt;探讨并解决在人形化身执行复杂操作任务时应用强化学习所面临的挑战，从而提高其实际环境中的性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;{'自动化真实到模拟调整模块': '用于使仿真环境更接近现实世界', '通用奖励设计策略': '简化长时间跨度接触密集型操作任务的回报工程', '分而治之提炼过程': '提高难探索问题的样本效率同时保持从仿真到实际性能的一致性', '稀疏和稠密物体表示混合': '弥合从仿真到现实感知差距'}&lt;h4&gt;主要发现&lt;/h4&gt;{'自动化调整模块': '能够有效缩小模拟环境与真实环境之间的差异', '通用奖励设计策略': '简化了长时程接触密集型任务的回报工程，使得训练更加高效', '分而治之提炼过程': '在提高难探索问题效率的同时保持了一致的从仿真到现实的表现', '混合表示方法': '有效减轻了从仿真到实际操作中的感知差距'}&lt;h4&gt;结论&lt;/h4&gt;通过上述一系列创新技术，在三个灵巧的人形操作任务上取得了有希望的结果，表明利用模拟到真实强化学习可以实现人形灵巧操纵的学习，并且能够无需人类演示而达到稳健的泛化和高性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要：强化学习在多种问题领域中实现了人类甚至超越人类的性能，但在灵巧机器人操作方面进展有限。这项工作探讨了将强化学习应用于人形代理执行一系列接触密集型任务的关键挑战，并提出了解决这些挑战的新方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning has delivered promising results in achieving human- oreven superhuman-level capabilities across diverse problem domains, but successin dexterous robot manipulation remains limited. This work investigates the keychallenges in applying reinforcement learning to solve a collection ofcontact-rich manipulation tasks on a humanoid embodiment. We introduce noveltechniques to overcome the identified challenges with empirical validation. Ourmain contributions include an automated real-to-sim tuning module that bringsthe simulated environment closer to the real world, a generalized reward designscheme that simplifies reward engineering for long-horizon contact-richmanipulation tasks, a divide-and-conquer distillation process that improves thesample efficiency of hard-exploration problems while maintaining sim-to-realperformance, and a mixture of sparse and dense object representations to bridgethe sim-to-real perception gap. We show promising results on three humanoiddexterous manipulation tasks, with ablation studies on each technique. Our workpresents a successful approach to learning humanoid dexterous manipulationusing sim-to-real reinforcement learning, achieving robust generalization andhigh performance without the need for human demonstration.</description>
      <author>example@mail.com (Toru Lin, Kartik Sachdev, Linxi Fan, Jitendra Malik, Yuke Zhu)</author>
      <guid isPermaLink="false">2502.20396v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation</title>
      <link>http://arxiv.org/abs/2502.20391v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Point Policy的新方法，该方法可以从离线的人类演示视频中学习机器人的策略，并且不需要任何远程操作数据。&lt;h4&gt;背景&lt;/h4&gt;构建能够在多种环境和对象类型下运行的机器人代理仍然是一项重大挑战，通常需要大量的数据收集。由于每个数据点必须在现实世界中物理执行，这在机器人技术领域尤为限制性。&lt;h4&gt;目的&lt;/h4&gt;提出一种替代的数据源以及能够利用这些数据进行学习的框架。&lt;h4&gt;方法&lt;/h4&gt;Point Policy利用最先进的视觉模型和策略架构将人类的手部姿势转换为机器人的姿势，并通过有意义的关键点捕捉物体状态。这种方法产生了一种形态无关的表示，从而促进有效的策略学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在与训练相同的设置下评估时，相比先前的工作总体改进了75%绝对值；对于新的物体实例在所有任务上表现出74%的增长，并且能够处理显著的背景杂乱。&lt;h4&gt;结论&lt;/h4&gt;Point Policy提供了一种创新的方法来利用人类演示视频中的数据学习机器人策略，无需进行物理执行。这表明从离线数据中学习是可能的，并为未来的研究开辟了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;构建能够在多种环境和对象类型下运行的机器人代理仍然是一项重大挑战，通常需要大量的数据收集。由于每个数据点必须在现实世界中物理执行，这在机器人技术领域尤为限制性。因此，存在对替代的数据源以及能够利用这些数据进行学习的框架的需求。本文介绍了一种名为Point Policy的新方法，该方法可以从离线的人类演示视频中学习机器人的策略，并且不需要任何远程操作数据。Point Policy利用最先进的视觉模型和策略架构将人类的手部姿势转换为机器人的姿势，并通过有意义的关键点捕捉物体状态。这种方法产生了一种形态无关的表示，从而促进有效的策略学习。实验结果表明，在与训练相同的设置下评估时，相比先前的工作总体改进了75%绝对值；对于新的物体实例在所有任务上表现出74%的增长，并且能够处理显著的背景杂乱。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building robotic agents capable of operating across diverse environments andobject types remains a significant challenge, often requiring extensive datacollection. This is particularly restrictive in robotics, where each data pointmust be physically executed in the real world. Consequently, there is acritical need for alternative data sources for robotics and frameworks thatenable learning from such data. In this work, we present Point Policy, a newmethod for learning robot policies exclusively from offline human demonstrationvideos and without any teleoperation data. Point Policy leveragesstate-of-the-art vision models and policy architectures to translate human handposes into robot poses while capturing object states through semanticallymeaningful key points. This approach yields a morphology-agnosticrepresentation that facilitates effective policy learning. Our experiments on 8real-world tasks demonstrate an overall 75% absolute improvement over priorworks when evaluated in identical settings as training. Further, Point Policyexhibits a 74% gain across tasks for novel object instances and is robust tosignificant background clutter. Videos of the robot are best viewed athttps://point-policy.github.io/.</description>
      <author>example@mail.com (Siddhant Haldar, Lerrel Pinto)</author>
      <guid isPermaLink="false">2502.20391v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>InterMimic: Towards Universal Whole-Body Control for Physics-Based Human-Object Interactions</title>
      <link>http://arxiv.org/abs/2502.20390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025. Project Page: https://sirui-xu.github.io/InterMimic/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个名为InterMimic的框架，该框架能利用不完美的动作捕捉数据来学习复杂的物体-人交互（HOI），并通过一种课程策略和强化学习微调实现了从模仿到生成模型的进步。&lt;h4&gt;背景&lt;/h4&gt;长期以来，实现人类与各种对象进行真实互动的模拟一直是目标。然而，由于复杂的物体-人的耦合、多样的几何形状以及动作捕捉数据中的不准确接触等问题，基于物理的方法扩展到复杂的人体-物体交互面临挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种框架，利用不完美的动作捕捉数据来学习和生成高质量且多样化的人体-物体互动模拟。&lt;h4&gt;方法&lt;/h4&gt;首先训练特定主体的教师策略以模仿、重定位并改进捕获的动作。然后将这些教师策略的知识转移到一个学生策略中，并通过强化学习微调进一步提升。&lt;h4&gt;主要发现&lt;/h4&gt;InterMimic能够产生现实且多样化的人体-物体互动，在多个HOI数据集上表现良好，还能零样本推广和无缝集成到运动学生成器中。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅提高了模仿质量，还从单纯的模仿技术转变为了复杂人体-物体交互的生成模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving realistic simulations of humans interacting with a wide range ofobjects has long been a fundamental goal. Extending physics-based motionimitation to complex human-object interactions (HOIs) is challenging due tointricate human-object coupling, variability in object geometries, andartifacts in motion capture data, such as inaccurate contacts and limited handdetail. We introduce InterMimic, a framework that enables a single policy torobustly learn from hours of imperfect MoCap data covering diverse full-bodyinteractions with dynamic and varied objects. Our key insight is to employ acurriculum strategy -- perfect first, then scale up. We first trainsubject-specific teacher policies to mimic, retarget, and refine motion capturedata. Next, we distill these teachers into a student policy, with the teachersacting as online experts providing direct supervision, as well as high-qualityreferences. Notably, we incorporate RL fine-tuning on the student policy tosurpass mere demonstration replication and achieve higher-quality solutions.Our experiments demonstrate that InterMimic produces realistic and diverseinteractions across multiple HOI datasets. The learned policy generalizes in azero-shot manner and seamlessly integrates with kinematic generators, elevatingthe framework from mere imitation to generative modeling of complexhuman-object interactions.</description>
      <author>example@mail.com (Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, Liang-Yan Gui)</author>
      <guid isPermaLink="false">2502.20390v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>ATLAS Navigator: Active Task-driven LAnguage-embedded Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.20386v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究探讨了在无结构和未知环境中进行任务导向性导航所面临的挑战。机器人需要实时构建并推理出丰富且具有语义信息的地图。&lt;h4&gt;目的&lt;/h4&gt;为了有效地执行自然语言描述的任务，提出了一种分层表示方法，该方法基于嵌入式语言的高斯散布技术，能够支持稀疏语义规划和密集几何表征。&lt;h4&gt;方法&lt;/h4&gt;采用了一种层次化地图构建方式——基于语言嵌入的高斯散布方法，以实现稀疏语义路径规划和密集几何表达的结合。这种方法在碰撞避免导航方面具有优势。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界的机器人实验验证了该方法的有效性，在混乱的室内环境及千米级的室外环境中表现良好，并且与特权基线相比，竞争比率为约60%。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法为解决任务导向性导航中遇到的问题提供了一种有效的解决方案。它能够适应广泛的任务需求并在复杂环境下运行。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容是关于如何在无结构和未知的环境中实现机器人任务导向性导航的研究，提出了结合稀疏语义规划与密集几何表达的地图表示方法，并通过实际实验验证了该方法的有效性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We address the challenge of task-oriented navigation in unstructured andunknown environments, where robots must incrementally build and reason on rich,metric-semantic maps in real time. Since tasks may require clarification orre-specification, it is necessary for the information in the map to be richenough to enable generalization across a wide range of tasks. To effectivelyexecute tasks specified in natural language, we propose a hierarchicalrepresentation built on language-embedded Gaussian splatting that enables bothsparse semantic planning that lends itself to online operation and densegeometric representation for collision-free navigation. We validate theeffectiveness of our method through real-world robot experiments conducted inboth cluttered indoor and kilometer-scale outdoor environments, with acompetitive ratio of about 60% against privileged baselines. Experiment videosand more details can be found on our project page: https://atlasnav.github.io</description>
      <author>example@mail.com (Dexter Ong, Yuezhan Tao, Varun Murali, Igor Spasojevic, Vijay Kumar, Pratik Chaudhari)</author>
      <guid isPermaLink="false">2502.20386v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Physics-Driven Data Generation for Contact-Rich Manipulation via Trajectory Optimization</title>
      <link>http://arxiv.org/abs/2502.20382v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种低成本的数据生成管道，利用物理仿真、人类演示和模型规划来为接触密集型机器人操作任务高效生成大规模高质量数据集。&lt;h4&gt;背景&lt;/h4&gt;在虚拟现实模拟环境中收集少量人体动作示例，并通过基于优化的运动重定位和轨迹优化进一步细化这些示例，以适应不同类型的机器人硬件和物理参数。该方法旨在创造多样且物理一致的数据集，以便于跨设备之间传输数据并重复使用之前在不同硬件配置下采集的老化数据集。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够生成大规模高质量数据集的方法，从而提高接触密集型任务中机器人的操作性能，并减少对人类输入的需求。&lt;h4&gt;方法&lt;/h4&gt;采用物理仿真、优化算法和轨迹规划相结合的方式处理人体演示动作。从虚拟现实环境中获取初始的小规模数据样本，然后通过优化过程使这些数据适应于各种不同的机器人形态及环境参数。&lt;h4&gt;主要发现&lt;/h4&gt;所生成的数据集不仅能够提高机器人的操作性能，在跨设备部署时也表现出优秀的零样本迁移能力，即在新的机器人硬件上无需进一步训练即可实现高成功率的操作任务。&lt;h4&gt;结论&lt;/h4&gt;此研究证明了通过仿真结合优化技术来生成高质量数据的有效性，并展示了这种方法在实际接触密集型任务中具有巨大的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种低成本的数据生成管道，该管道整合物理仿真的方法、人类的演示和基于模型的规划，以高效地为涉及大量接触操作的机器人抓取任务生成大规模且高质量的数据集。从少量在虚拟现实模拟环境中收集的人类动作样本开始，利用基于优化的方法进行运动重定位以及轨迹优化来调整这些示例，使其适应于不同类型的机器人形态及物理参数，从而产生了多样化、物理一致性的数据集，并为跨设备间的数据传输提供了可能，同时也能够重复使用之前在不同硬件配置下采集的老化数据。通过训练生成的数据集以应对多种机器人手臂上的挑战性接触密集型任务（包括浮动Allegro手和双臂机器人），验证了该管道的有效性；所训练的策略在零样本输入的情况下于实际设备上部署，如用于双臂iiwa机器人的操作中取得了高成功率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a low-cost data generation pipeline that integrates physics-basedsimulation, human demonstrations, and model-based planning to efficientlygenerate large-scale, high-quality datasets for contact-rich roboticmanipulation tasks. Starting with a small number of embodiment-flexible humandemonstrations collected in a virtual reality simulation environment, thepipeline refines these demonstrations using optimization-based kinematicretargeting and trajectory optimization to adapt them across various robotembodiments and physical parameters. This process yields a diverse, physicallyconsistent dataset that enables cross-embodiment data transfer, and offers thepotential to reuse legacy datasets collected under different hardwareconfigurations or physical parameters. We validate the pipeline's effectivenessby training diffusion policies from the generated datasets for challengingcontact-rich manipulation tasks across multiple robot embodiments, including afloating Allegro hand and bimanual robot arms. The trained policies aredeployed zero-shot on hardware for bimanual iiwa arms, achieving high successrates with minimal human input. Project website:https://lujieyang.github.io/physicsgen/.</description>
      <author>example@mail.com (Lujie Yang, H. J. Terry Suh, Tong Zhao, Bernhard Paus Graesdal, Tarik Kelestemur, Jiuguang Wang, Tao Pang, Russ Tedrake)</author>
      <guid isPermaLink="false">2502.20382v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Agent Path Planning in Complex Environments using Gaussian Belief Propagation with Global Path Finding</title>
      <link>http://arxiv.org/abs/2502.20369v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by "International Conference on Robotics and Automation" -  ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种多智能体路径规划的新方法，结合高斯信念传播与路径积分，并引入跟踪因子来确保严格的全局路径遵循。该方法通过两种不同全局路径规划策略进行了测试：快速探索随机树和结构化规划器。&lt;h4&gt;背景&lt;/h4&gt;在机器人学中，多代理路径规划是一个关键挑战，要求智能体能够在复杂环境中导航并避免碰撞同时优化旅行效率。&lt;h4&gt;目的&lt;/h4&gt;解决现有方法的局限性，提高多代理协调能力，特别是在结合结构化全局规划时提升效果。&lt;h4&gt;方法&lt;/h4&gt;将高斯信念传播与路径积分相结合，并引入跟踪因子；使用快速探索随机树和基于预定义车道结构的结构化规划器进行测试。&lt;h4&gt;主要发现&lt;/h4&gt;在单智能体和多智能体场景中，跟踪因子分别减少了28%和16%的路径偏差，证明了其提高多智能体协调的有效性。&lt;h4&gt;结论&lt;/h4&gt;该方法通过引入跟踪因子和其他技术改进，在各种导航和通信挑战下表现出色，特别是在结合结构化全局规划时效果更佳。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要介绍了一种新的解决多代理路径规划问题的方法，通过将高斯信念传播与路径积分相结合，并且添加了跟踪因素来确保严格遵守全局路径。这项工作旨在克服现有方法的局限性，特别是在提高多智能体协调方面的表现，尤其是在结合结构化全局规划的情况下。研究在不同的全球路径规划策略（快速探索随机树和利用预定义车道结构改进协作性的结构化规划器）上进行了测试，并在一个模拟环境中对所有场景的有效性进行了验证。实验结果显示，在单个代理和多个代理的场景中，跟踪因素分别减少了28％和16％的路径偏差，表明其提高了多智能体协调的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-agent path planning is a critical challenge in robotics, requiringagents to navigate complex environments while avoiding collisions andoptimizing travel efficiency. This work addresses the limitations of existingapproaches by combining Gaussian belief propagation with path integration andintroducing a novel tracking factor to ensure strict adherence to global paths.The proposed method is tested with two different global path-planningapproaches: rapidly exploring random trees and a structured planner, whichleverages predefined lane structures to improve coordination. A simulationenvironment was developed to validate the proposed method across diversescenarios, each posing unique challenges in navigation and communication.Simulation results demonstrate that the tracking factor reduces path deviationby 28% in single-agent and 16% in multi-agent scenarios, highlighting itseffectiveness in improving multi-agent coordination, especially when combinedwith structured global planning.</description>
      <author>example@mail.com (Jens Høigaard Jensen, Kristoffer Plagborg Bak Sørensen, Jonas le Fevre Sejersen, Andriy Sarabakha)</author>
      <guid isPermaLink="false">2502.20369v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>The Role of Tactile Sensing for Learning Reach and Grasp</title>
      <link>http://arxiv.org/abs/2502.20367v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了利用强化学习和触觉传感来实现稳健且反应迅速的机器人抓取方法，并通过不同模型自由强化学习方法对比研究了视觉感知不完美情况下的多种触觉环境设置。&lt;h4&gt;背景&lt;/h4&gt;在当前的机器人应用中，稳定且鲁棒性的机械手抓握技术至关重要。最近的研究表明，利用大规模数据集和监督学习可以提高反向对称性抓取的速度和精度，但这些方法对于长时规划面临感知错误和校准误差的挑战。&lt;h4&gt;目的&lt;/h4&gt;评估不同复杂度的基于力的触觉传感如何影响抓取任务中的强化学习行为，并通过对比研究发现有效的触觉设置以改进机械手抓握性能。&lt;h4&gt;方法&lt;/h4&gt;采用两种无模型强化学习算法来研究反向对称性抓取问题，分析了在视觉感知不完美的情况下，不同的触觉和环境设定对于改善机械手抓握能力的效用。&lt;h4&gt;主要发现&lt;/h4&gt;不同种类的触觉特征可以提高学习效果；然而，复杂的触觉输入则会增加训练难度。&lt;h4&gt;结论&lt;/h4&gt;利用强化学习结合触觉传感有望成为实现更稳健、反应更快的机器人抓取运动的一种有前途的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Stable and robust robotic grasping is essential for current and future robotapplications. In recent works, the use of large datasets and supervisedlearning has enhanced speed and precision in antipodal grasping. However, thesemethods struggle with perception and calibration errors due to large planninghorizons. To obtain more robust and reactive grasping motions, leveragingreinforcement learning combined with tactile sensing is a promising direction.Yet, there is no systematic evaluation of how the complexity of force-basedtactile sensing affects the learning behavior for grasping tasks. This papercompares various tactile and environmental setups using two model-freereinforcement learning approaches for antipodal grasping. Our findings suggestthat under imperfect visual perception, various tactile features improvelearning outcomes, while complex tactile inputs complicate training.</description>
      <author>example@mail.com (Boya Zhang, Iris Andrussow, Andreas Zell, Georg Martius)</author>
      <guid isPermaLink="false">2502.20367v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Video Analytics in Cloud-Edge-Terminal Collaborative Systems</title>
      <link>http://arxiv.org/abs/2502.06581v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文综述了云边端协同（CETC）系统在视频分析领域的应用和发展，特别关注其架构组件、资源管理和边缘计算平台。&lt;h4&gt;背景&lt;/h4&gt;随着视频数据的爆炸性增长，分布式视频分析在云边端协作系统中得到了快速发展，这些系统能够实现高效的视频处理、实时推理和隐私保护。&lt;h4&gt;目的&lt;/h4&gt;该综述旨在分析CETC系统的基础架构组成部分，并探讨其在视频监控、自动驾驶和智慧城市等领域的应用突破。&lt;h4&gt;方法&lt;/h4&gt;文中首先剖析了包括层级式、分布式及混合框架在内的基本架构组件，以及边缘计算平台与资源管理机制。同时，还介绍了以云为中心的方法利用强大计算能力处理复杂的视频理解和模型训练问题，并探索了结合自适应任务卸载和资源感知调度技术的混合视频分析。&lt;h4&gt;主要发现&lt;/h4&gt;文章指出，除了传统方法外，大型语言模型和多模态整合领域的最新进展揭示了在平台可扩展性、数据保护和系统可靠性方面的机会与挑战。&lt;h4&gt;结论&lt;/h4&gt;未来的研究方向包括可解释性系统的开发、高效的处理机制以及高级视频分析技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-10&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The explosive growth of video data has driven the development of distributedvideo analytics in cloud-edge-terminal collaborative (CETC) systems, enablingefficient video processing, real-time inference, and privacy-preservinganalysis. Among multiple advantages, CETC systems can distribute videoprocessing tasks and enable adaptive analytics across cloud, edge, and terminaldevices, leading to breakthroughs in video surveillance, autonomous driving,and smart cities. In this survey, we first analyze fundamental architecturalcomponents, including hierarchical, distributed, and hybrid frameworks,alongside edge computing platforms and resource management mechanisms. Buildingupon these foundations, edge-centric approaches emphasize on-device processing,edge-assisted offloading, and edge intelligence, while cloud-centric methodsleverage powerful computational capabilities for complex video understandingand model training. Our investigation also covers hybrid video analyticsincorporating adaptive task offloading and resource-aware scheduling techniquesthat optimize performance across the entire system. Beyond conventionalapproaches, recent advances in large language models and multimodal integrationreveal both opportunities and challenges in platform scalability, dataprotection, and system reliability. Future directions also encompassexplainable systems, efficient processing mechanisms, and advanced videoanalytics, offering valuable insights for researchers and practitioners in thisdynamic field.</description>
      <author>example@mail.com (Linxiao Gong, Hao Yang, Gaoyun Fang, Bobo Ju, Juncen Guo, Xiaoguang Zhu, Xiping Hu, Yan Wang, Peng Sun, Azzedine Boukerche)</author>
      <guid isPermaLink="false">2502.06581v3</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Trajectory-to-Action Pipeline (TAP): Automated Scenario Description Extraction for Autonomous Vehicle Behavior Comparison</title>
      <link>http://arxiv.org/abs/2502.20353v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Trajectory-to-Action Pipeline (TAP)的方法，该方法可以自动从大规模轨迹数据集中提取Scenario Description Languages (SDL)标签。这项工作为自动驾驶车辆(AVs)的安全分析和行为评估提供了一个可扩展的基础。&lt;h4&gt;背景&lt;/h4&gt;Scenario Description Languages (SDLs) 提供了结构化、可解释的嵌入表示法，用于描述自主车辆遇到的各种交通场景，并支持关键任务如相似场景搜索和边缘案例检测等安全分析。&lt;h4&gt;目的&lt;/h4&gt;提出一种自动化的TAP方法来从大规模轨迹数据集中提取SDL标签，从而提升自动驾驶车的安全性和评估效率。&lt;h4&gt;方法&lt;/h4&gt;TAP采用基于规则的交叉熵优化方法直接从数据中学习参数，以提高在不同驾驶环境中的泛化能力。使用Waymo Open Motion Dataset (WOMD)作为实验平台来验证其有效性。&lt;h4&gt;主要发现&lt;/h4&gt;与Average Displacement Error (ADE)相比，TAP在识别行为相似轨迹时提高了30%的精度；而与Dynamic Time Warping (DTW)相比，则提升了24%。此外，该方法还能够自动检测独特的驾驶行为，简化了AV测试的安全评估过程。&lt;h4&gt;结论&lt;/h4&gt;这项工作为基于场景的自动驾驶车辆行为分析提供了可扩展的基础，并且具有在多代理环境下进行集成应用的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;Scenario Description Languages (SDLs) 提供结构化、可解释的嵌入表示来代表自动驾驶汽车遇到的各种交通场景，支持包括相似场景搜索和边缘案例检测等关键任务的安全分析。论文介绍了Trajectory-to-Action Pipeline (TAP)，这是一种从大规模轨迹数据集中提取SDL标签的大规模且自动化的方案。通过使用基于规则的方法进行交叉熵优化，TAP直接从数据中学习参数以增强在各种驾驶情境下的泛化能力。利用Waymo Open Motion Dataset (WOMD)作为实验平台，TAP在识别行为相似的轨迹方面超越了Average Displacement Error（ADE）30%，而相对于Dynamic Time Warping (DTW)则提升24%。此外，该方案还能够自动检测独特驾驶行为，简化AV测试的安全评估过程。这项研究为基于场景的自动驾驶车辆行为分析提供了可扩展的基础，并具有多代理环境集成应用的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scenario Description Languages (SDLs) provide structured, interpretableembeddings that represent traffic scenarios encountered by autonomous vehicles(AVs), supporting key tasks such as scenario similarity searches and edge casedetection for safety analysis. This paper introduces the Trajectory-to-ActionPipeline (TAP), a scalable and automated method for extracting SDL labels fromlarge trajectory datasets. TAP applies a rules-based cross-entropy optimizationapproach to learn parameters directly from data, enhancing generalizationacross diverse driving contexts. Using the Waymo Open Motion Dataset (WOMD),TAP achieves 30% greater precision than Average Displacement Error (ADE) and24% over Dynamic Time Warping (DTW) in identifying behaviorally similartrajectories. Additionally, TAP enables automated detection of unique drivingbehaviors, streamlining safety evaluation processes for AV testing. This workprovides a foundation for scalable scenario-based AV behavior analysis, withpotential extensions for integrating multi-agent contexts.</description>
      <author>example@mail.com (Aron Harder, Madhur Behl)</author>
      <guid isPermaLink="false">2502.20353v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Reservoir Computing and Photoelectrochemical Sensors: A Marriage of Convenience</title>
      <link>http://arxiv.org/abs/2502.20342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;感应技术是信息处理的重要组成部分。当前的人工智能系统（尤其是针对医疗和环境应用的）需要大量关于生物流体或环境样本化学组成的数据。&lt;h4&gt;目的&lt;/h4&gt;介绍将光电化学传感器与非传统计算范式——蓄水池计算进行集成的想法，以提升传感器性能并开辟新的科学路径。&lt;h4&gt;方法&lt;/h4&gt;探讨如何通过结合光电化学传感技术与蓄水池计算系统来改进感应设备和促进仿生感官信息处理的研究。&lt;h4&gt;主要发现&lt;/h4&gt;将光电化学传感器与蓄水池计算相结合可以克服一些现有障碍，并可能在自主机器人技术和仿生感知领域取得突破性进展。&lt;h4&gt;结论&lt;/h4&gt;这种集成方法不仅能够提升传感器本身的性能，还能开启有效信息采集和处理的新时代。&lt;h4&gt;翻译&lt;/h4&gt;摘要所述内容的中文翻译已经完成。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1016/j.ccr.2023.215155&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensing technology is an important aspect of information processing. Currentdevelopment in artificial intelligence systems (especially those aimed atmedical and environmental applications) requires a lot of data on the chemicalcomposition of biological fluids or environmental samples. These complexmatrices require advanced sensing devices, and photoelectrochemical ones seemto have potential to overcome at least some of the obstacles. Furthermore, thedevelopment of artificial intelligence (AI) technology for autonomous roboticsrequires technology mimicking human senses, also those operating at themolecular level, such as gustation and olfaction. Again, photoelectrochemicalsensing can provide some suitable solutions. In this review, we introduce theidea of integration of photoelectrochemical sensors with some unconventionalcomputing paradigm - reservoir computing. This approach should not only boostthe performance of the sensors itself, but also open new pathways throughscience. Integration of sensing devices with computing systems will alsocontribute to a better understanding (or at least mimicking) of the humansenses and neuromorphic sensory information processing. Although reservoirsystems can be considered magic "black boxes" and their operation is at thesame time simple and hard to comprehend, this combination is expected to open anew era of effective information harvesting and processing systems.</description>
      <author>example@mail.com (Gisya Abdi, Lulu Alluhaibi, Ewelina Kowalewska, Tomasz Mazur, Krzysztof Mech, Agnieszka Podborska, Andrzej Sławek, Hirofumi Tanaka, Konrad Szaciłowski)</author>
      <guid isPermaLink="false">2502.20342v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Deep Reinforcement Learning based Autonomous Decision-Making for Cooperative UAVs: A Search and Rescue Real World Application</title>
      <link>http://arxiv.org/abs/2502.20326v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 Pages, 21 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种针对无GNSS信号的室内环境中的多无人机系统的自主导航和任务分配的整体框架。&lt;h4&gt;背景&lt;/h4&gt;在没有全球导航卫星系统（GNSS）支持的室内环境中，现有的无人机系统面临着挑战，尤其是在导航、障碍物避让和任务协调方面。这些问题阻碍了无人机系统在搜索救援和其他探索活动中的有效应用。&lt;h4&gt;目的&lt;/h4&gt;提出一个基于深度强化学习（DRL）的自主指导机制，并通过图卷积网络（GCN）实现动态实时的任务分配，以提高多无人机系统的导航能力和任务协作效率。&lt;h4&gt;方法&lt;/h4&gt;{'1': '使用Twin Delayed Deep Deterministic Policy Gradient算法进行自主导航训练，引入人工势场(APF)奖励结构来优化培训过程。', '2': '利用图卷积网络（GCN）处理多无人机之间的合作任务分配问题。', '3': '采用LiDAR同步定位和建图(SLAM)技术结合深度相机解决室内环境中的精确定位问题，同时缓解走廊效应。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '基于APF的奖励结构可以显著提高无人机在室内环境中的导航效率。', '2': '采用DRL训练的GCN能够有效处理任务分配，并实现动态和实时的任务调整。', '3': 'LiDAR SLAM结合深度相机提供了一种有效的定位解决方案，增强了系统的依赖性和可靠性。'}&lt;h4&gt;结论&lt;/h4&gt;所提出的多无人机框架不仅提升了单个无人机的导航能力，还优化了复杂障碍环境中的任务分配协调，实验结果表明该系统在特定条件下的表现优异，并且在2024年NATO Sapience自主合作无人机比赛中获得了第一名。&lt;h4&gt;翻译&lt;/h4&gt;摘要：本文提出了一种针对无GNSS信号的室内环境中多无人机系统的整体框架。我们提倡使用基于深度强化学习（DRL）的方法进行自主导航，利用Twin Delayed Deep Deterministic Policy Gradient算法。为了提高训练过程中的效率，我们采用人工势场(APF)奖励结构，使代理能够优化其移动路径，从而促进更顺畅的路径和增强的障碍物避让能力。此外，通过基于DRL的图卷积网络（GCN）解决了任务分配问题，该方法表示了无人机与任务之间的交互，实现了动态和实时的任务分配，反映了当前环境条件以及无人机的能力。这种做法促进了多无人机系统在搜索救援或其他探索活动中的有效协调合作。最后，在缺乏GNSS的情况下，我们采用LiDAR SLAM配合深度相机来确保精确定位，解决了走廊效应问题。该集成提供了强大的定位和映射功能，从而增强了室内导航系统的可靠性。所提出的多无人机框架不仅提高了单个无人机的导航能力，还优化了复杂障碍环境中的任务分配协调，在为NATO Sapience自主合作无人机竞赛量身定制的实验环境中进行了测试，并取得了卓越的成绩，最终在2024年的比赛中获得第一名。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes a holistic framework for autonomous guidance, navigation,and task distribution among multi-drone systems operating in Global NavigationSatellite System (GNSS)-denied indoor settings. We advocate for a DeepReinforcement Learning (DRL)-based guidance mechanism, utilising the TwinDelayed Deep Deterministic Policy Gradient algorithm. To improve the efficiencyof the training process, we incorporate an Artificial Potential Field(APF)-based reward structure, enabling the agent to refine its movements,thereby promoting smoother paths and enhanced obstacle avoidance in indoorcontexts. Furthermore, we tackle the issue of task distribution amongcooperative UAVs through a DRL-trained Graph Convolutional Network (GCN). ThisGCN represents the interactions between drones and tasks, facilitating dynamicand real-time task allocation that reflects the current environmentalconditions and the capabilities of the drones. Such an approach fosterseffective coordination and collaboration among multiple drones during searchand rescue operations or other exploratory endeavours. Lastly, to ensureprecise odometry in environments lacking GNSS, we employ Light Detection AndRanging Simultaneous Localisation and Mapping complemented by a depth camera tomitigate the hallway problem. This integration offers robust localisation andmapping functionalities, thereby enhancing the systems dependability in indoornavigation. The proposed multi-drone framework not only elevates individualnavigation capabilities but also optimises coordinated task allocation incomplex, obstacle-laden environments. Experimental evaluations conducted in asetup tailored to meet the requirements of the NATO Sapience AutonomousCooperative Drone Competition demonstrate the efficacy of the proposed system,yielding outstanding results and culminating in a first-place finish in the2024 Sapience competition.</description>
      <author>example@mail.com (Thomas Hickling, Maxwell Hogan, Abdulla Tammam, Nabil Aouf)</author>
      <guid isPermaLink="false">2502.20326v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>On Adversarial Attacks In Acoustic Drone Localization</title>
      <link>http://arxiv.org/abs/2502.20325v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多旋翼无人驾驶飞行器（MAVs，即无人机）由于在农业、商业递送和搜救等领域应用广泛而日益受到关注。然而，在非受控环境中使用时，导航系统的潜在对抗攻击威胁对任务的成功率和安全性构成了挑战。&lt;h4&gt;背景&lt;/h4&gt;基于视觉的方法对光照条件和遮挡非常敏感，促使研究者们开始探索依赖于声学传感器等其他模态的导航方式。虽然在无人机定位方面已有利用声学方法取得的研究进展，但针对其导航系统的对抗攻击方面的研究仅限于基于视觉感知的系统。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，本工作旨在通过分析PGD（Projected Gradient Descent）对抗攻击对声学无人机定位的影响进行全面分析，并开发一种算法来恢复对抗扰动以减轻这种攻击的效果。&lt;h4&gt;方法&lt;/h4&gt;本工作首先评估了在声学传感器上应用PGD对抗攻击的效果；然后设计了一种新的算法，旨在减少这些攻击对声学导航系统造成的不利影响。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，在无人机的声学定位系统中实施PGD攻击可以显著降低其准确性和可靠性。而所提出的扰动恢复方法能有效减轻此类攻击的影响。&lt;h4&gt;结论&lt;/h4&gt;这项工作强调了在开发新型无人飞行器时，对导航系统进行抗干扰设计的重要性，并为未来的对抗安全研究提供了重要方向。&lt;h4&gt;翻译&lt;/h4&gt;多旋翼无人驾驶飞行器（MAVs，即无人机）由于它们在农业、商业递送和搜救等广泛领域的应用而近年来引起了越来越多的关注。基于视觉的方法对于光线条件和遮挡非常敏感，这促使了对依赖声学感知等其他模态的导航系统研究的增长。使用无人机执行非受控环境中的任务时的一个主要担忧是其导航系统的潜在对抗攻击威胁，这种威胁可能会导致关键任务失败、安全漏洞及危及操作员和旁观者安全的风险。尽管以往的研究已经在基于视觉感知的无人机定位方面取得了进展，但是之前有关针对无人机导航系统进行对抗攻击的研究仅限于视觉感知系统。在这项工作中，我们的目标是通过提供PGD（Projected Gradient Descent）对抗攻击对声学无人机定位影响的全面分析来填补这一空白，并且开发出一种能够在我们设定的情况下显著减少这种攻击效果的扰动恢复算法。在发表后我们将公开所有实验代码以供复现研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-rotor aerial autonomous vehicles (MAVs, more widely known as "drones")have been generating increased interest in recent years due to their growingapplicability in a vast and diverse range of fields (e.g., agriculture,commercial delivery, search and rescue). The sensitivity of visual-basedmethods to lighting conditions and occlusions had prompted growing study ofnavigation reliant on other modalities, such as acoustic sensing. A majorconcern in using drones in scale for tasks in non-controlled environments isthe potential threat of adversarial attacks over their navigational systems,exposing users to mission-critical failures, security breaches, and compromisedsafety outcomes that can endanger operators and bystanders. While previous workshows impressive progress in acoustic-based drone localization, prior researchin adversarial attacks over drone navigation only addresses visualsensing-based systems. In this work, we aim to compensate for this gap bysupplying a comprehensive analysis of the effect of PGD adversarial attacksover acoustic drone localization. We furthermore develop an algorithm foradversarial perturbation recovery, capable of markedly diminishing the affectof such attacks in our setting. The code for reproducing all experiments willbe released upon publication.</description>
      <author>example@mail.com (Tamir Shor, Chaim Baskin, Alex Bronstein)</author>
      <guid isPermaLink="false">2502.20325v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>MARVEL: Multi-Agent Reinforcement Learning for constrained field-of-View multi-robot Exploration in Large-scale environments</title>
      <link>http://arxiv.org/abs/2502.20217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  \c{opyright} 20XX IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;在多机器人探索中，一支移动机器人的团队被赋予高效地绘制未知环境的任务。尽管大多数探索规划器假设使用类似LiDAR的全向传感器，但这种做法对于像无人机这样的小型机器人来说是不切实际的，因为载荷限制可能导致只能使用轻量级的方向性传感器如摄像头。&lt;h4&gt;背景&lt;/h4&gt;在多机器人探索中，当面对具有有限视场（FoV）的小型机器人时，传统的探索规划器假设全向传感器的应用变得不再现实。这些小型机器人的传感器受限于方向性和视野范围的约束，增加了问题解决的复杂度。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于拥有有限视场的多机器人系统的新框架MARVEL，以增强其在大型室内环境中的协调能力和决策能力。&lt;h4&gt;方法&lt;/h4&gt;通过结合图注意力网络和创新性的前沿及姿态特征融合技术，使用强化学习（MARL）开发了一种协作式、去中心化的策略。此外还引入了信息驱动的动作修剪策略来处理视角规划的大动作空间问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，与现有最先进的探索规划器相比，MARVEL所学得的政策展示了有效的协同行为，并在多个评价指标上表现出色。该方法的通用性得到了验证，甚至在一个高达90米乘以90米的大规模环境中也表现良好。此外还通过真实硬件无人机团队的成功部署证明了其实用性和可操作性。&lt;h4&gt;结论&lt;/h4&gt;MARVEL框架为具有有限视场的多机器人探索提供了一个有效的解决方案，并展示了其在大规模复杂环境中的优越性能，同时适用于各种团队规模和传感器配置（即FoV和传感器范围）无需额外训练。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In multi-robot exploration, a team of mobile robot is tasked with efficientlymapping an unknown environments. While most exploration planners assumeomnidirectional sensors like LiDAR, this is impractical for small robots suchas drones, where lightweight, directional sensors like cameras may be the onlyoption due to payload constraints. These sensors have a constrainedfield-of-view (FoV), which adds complexity to the exploration problem,requiring not only optimal robot positioning but also sensor orientation duringmovement. In this work, we propose MARVEL, a neural framework that leveragesgraph attention networks, together with novel frontiers and orientationfeatures fusion technique, to develop a collaborative, decentralized policyusing multi-agent reinforcement learning (MARL) for robots with constrainedFoV. To handle the large action space of viewpoints planning, we furtherintroduce a novel information-driven action pruning strategy. MARVEL improvesmulti-robot coordination and decision-making in challenging large-scale indoorenvironments, while adapting to various team sizes and sensor configurations(i.e., FoV and sensor range) without additional training. Our extensiveevaluation shows that MARVEL's learned policies exhibit effective coordinatedbehaviors, outperforming state-of-the-art exploration planners across multiplemetrics. We experimentally demonstrate MARVEL's generalizability in large-scaleenvironments, of up to 90m by 90m, and validate its practical applicabilitythrough successful deployment on a team of real drone hardware.</description>
      <author>example@mail.com (Jimmy Chiun, Shizhe Zhang, Yizhuo Wang, Yuhong Cao, Guillaume Sartoretti)</author>
      <guid isPermaLink="false">2502.20217v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Accelerating Model-Based Reinforcement Learning with State-Space World Models</title>
      <link>http://arxiv.org/abs/2502.20168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;强化学习（RL）是机器人学习的一种有效方式，但模型无关的强化学习（MFRL）需要大量环境交互才能获得成功的控制策略。相比之下，基于模型的强化学习（MBRL）通过同时训练世界模型和策略来提高样本效率，但是这种方法增加了计算复杂性。&lt;h4&gt;背景&lt;/h4&gt;传统的MFRL方法在处理复杂的非线性和噪声传感器信号时遇到挑战，导致需要大量的环境交互以获取成功的行为策略。而MBRL可以利用世界模型进行规划或数据收集，并且能够提供一阶策略梯度来训练策略。&lt;h4&gt;目的&lt;/h4&gt;提出一种加速基于状态空间世界的模型强化学习（MBRL）的新方法，特别是针对复杂和部分可观测的现实场景。&lt;h4&gt;方法&lt;/h4&gt;该研究利用状态空间模型（SSMs）并行化世界动力学模型的训练过程，并且在训练阶段给世界模型提供特权信息以提高性能。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在多个实际敏捷四旋翼飞行任务中表现出显著的速度提升，将世界模型训练时间减少高达10倍，而整个MBRL的训练时间减少4倍以上。同时，该方法并没有牺牲样本效率或任务奖励。&lt;h4&gt;结论&lt;/h4&gt;通过利用SSMs并行化和提供特权信息的方式，新的MBRL技术可以极大地加速复杂场景中的学习过程，而不降低性能指标。&lt;h4&gt;翻译&lt;/h4&gt;摘要：强化学习是机器人学习的一个强大手段。然而，模型无关的强化学习需要大量的环境交互才能成功地学习控制策略。这是因为嘈杂的学习更新以及机器人系统的复杂性通常涉及到高度非线性的动态和噪声传感器信号。相比之下，基于模型的强化学习不仅训练一个策略还同时学习一个世界模型来捕获环境的动力学和奖励。该世界模型可以用于规划、数据收集或提供一阶策略梯度来进行训练。利用世界模型相比于无模型的强化学习显著提高了样本效率。然而，与策略一起训练的世界模型增加了计算复杂性，导致了更长的训练时间，在复杂的现实场景中通常是不可行的。在这项工作中，我们提出了一种新的方法来通过状态空间世界模型加速基于模型的强化学习。我们的方法利用状态空间模型（SSMs）并行化动力学模型的训练过程，这是通常的主要计算瓶颈。此外，我们提出了一个架构，在训练过程中给世界模型提供特权信息，这对于部分可观测环境尤其相关。我们在多个实际敏捷四旋翼飞行任务中评估了这种方法，包括完全和部分可观测环境中的复杂动态。我们展示了显著的速度提升，将世界模型的训练时间减少高达10倍，并且整个基于模型的学习的时间减少了4倍以上。这种优势没有牺牲性能，因为我们的方法在样本效率和任务奖励方面与最先进的基于模型的方法类似。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) is a powerful approach for robot learning.However, model-free RL (MFRL) requires a large number of environmentinteractions to learn successful control policies. This is due to the noisy RLtraining updates and the complexity of robotic systems, which typically involvehighly non-linear dynamics and noisy sensor signals. In contrast, model-basedRL (MBRL) not only trains a policy but simultaneously learns a world model thatcaptures the environment's dynamics and rewards. The world model can either beused for planning, for data collection, or to provide first-order policygradients for training. Leveraging a world model significantly improves sampleefficiency compared to model-free RL. However, training a world model alongsidethe policy increases the computational complexity, leading to longer trainingtimes that are often intractable for complex real-world scenarios. In thiswork, we propose a new method for accelerating model-based RL using state-spaceworld models. Our approach leverages state-space models (SSMs) to parallelizethe training of the dynamics model, which is typically the main computationalbottleneck. Additionally, we propose an architecture that provides privilegedinformation to the world model during training, which is particularly relevantfor partially observable environments. We evaluate our method in severalreal-world agile quadrotor flight tasks, involving complex dynamics, for bothfully and partially observable environments. We demonstrate a significantspeedup, reducing the world model training time by up to 10 times, and theoverall MBRL training time by up to 4 times. This benefit comes withoutcompromising performance, as our method achieves similar sample efficiency andtask rewards to state-of-the-art MBRL methods.</description>
      <author>example@mail.com (Maria Krinner, Elie Aljalbout, Angel Romero, Davide Scaramuzza)</author>
      <guid isPermaLink="false">2502.20168v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Geometry and Mechanics of Non-Euclidean Curved-Crease Origami</title>
      <link>http://arxiv.org/abs/2502.20147v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近有关于弯曲折纸的理论、数值和实验工作的大量研究，但注意到缺乏一个统一且完整的几何框架来描述具有非欧曲率的弯曲折纸的几何与力学特性。本文提供了一个通用的几何框架，用于描述由两条一般带组成的弯曲折纸形状，并表明根据空间折叠线及其配置分支可以得出四种不同的状态。在该几何框架内，推导出平衡方程并研究了这种结构的机械响应，重点关注欧拉屈曲行为。通过线性稳定性分析和有限元模拟发现，重叠构型表现出较低的屈曲阈值。为了更有效地捕捉大变形行为，基于各向异性Kirchhoff杆理论开发了一个双带模型，并成功预测出主要特性。&lt;h4&gt;背景&lt;/h4&gt;最近在弯曲折纸方面开展了大量的理论、数值和实验工作，然而对于具有非欧曲率（非平直折叠线）的结构缺乏一个统一而完整的几何力学框架。&lt;h4&gt;目的&lt;/h4&gt;提供一种通用的几何框架以描述由两条一般带组成的任意形状的弯曲折纸，并研究其机械响应。&lt;h4&gt;方法&lt;/h4&gt;1. 提供了一个几何框架来描述弯曲折纸的形状，包括四种可能的状态；2. 在给定的框架内推导了平衡方程并进行了线性稳定性分析和有限元模拟以研究其机械行为；3. 基于各向异性Kirchhoff杆理论开发了一种双带模型。&lt;h4&gt;主要发现&lt;/h4&gt;四种不同状态取决于空间折叠线及其配置分支，重叠构型表现出较低的屈曲阈值。新模型能够准确预测弯曲折纸的大变形行为。&lt;h4&gt;结论&lt;/h4&gt;这项工作建立了一个关于弯曲折纸几何与力学之间的联系，为机器人学、致动器和可展开太空结构等应用提供了新的见解，并且开发的新模型可以成功预测主要特性。&lt;h4&gt;翻译&lt;/h4&gt;最近有关于弯曲折纸的理论、数值和实验工作的大量研究，但注意到缺乏一个统一且完整的几何框架来描述具有非欧曲率的弯曲折纸的几何与力学特性。本文提供了一个通用的几何框架，用于描述由两条一般带组成的弯曲折纸形状，并表明根据空间折叠线及其配置分支可以得出四种不同的状态。在该几何框架内，推导出平衡方程并研究了这种结构的机械响应，重点关注欧拉屈曲行为。通过线性稳定性分析和有限元模拟发现，重叠构型表现出较低的屈曲阈值。为了更有效地捕捉大变形行为，基于各向异性Kirchhoff杆理论开发了一个双带模型，并成功预测出主要特性。这项工作建立了一个关于弯曲折纸几何与力学之间的联系，为机器人学、致动器和可展开太空结构等应用提供了新的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently there have been extensive theoretical, numerical and experimentalworks on curved-fold origami. However, we notice that a unified and completegeometric framework for describing the geometry and mechanics of curved-foldorigami, especially those with nontrivial Gaussian curvature at the crease(non-Euclidean crease), is still absent. Herein we provide a unified geometricframework that describes the shape of a generic curved-fold origami composed oftwo general strips. The explicit description indicates that four configurationsemerge, determined by its spatial crease and configuration branch. Within thisgeometric framework, we derive the equilibrium equations and study themechanical response of the curved-crease origami, focusing on Euler's bucklingbehavior. Both linear stability analysis and finite element simulation indicatethat the overlaid configuration exhibits a lower buckling threshold. To furthercapture the large deformation behavior efficiently, we develop a bistrip modelbased on the anisotropic Kirchhoff rod theory, which predicts the main featuressuccessfully. This work bridges the geometry and mechanics of curved-creaseorigami, offering insights for applications in robotics, actuators, anddeployable space structures.</description>
      <author>example@mail.com (Zhixuan Wen, Tian Yu, Fan Feng)</author>
      <guid isPermaLink="false">2502.20147v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Discovering Antagonists in Networks of Systems: Robot Deployment</title>
      <link>http://arxiv.org/abs/2502.20125v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于上下文的异常检测方法，应用于执行覆盖任务的机器人集群的物理运动。&lt;h4&gt;背景&lt;/h4&gt;在模拟环境中训练正常行为数据以识别对抗性行为或异常情况。&lt;h4&gt;目的&lt;/h4&gt;通过使用机器学习模型预测机器人动作的可能范围来识别执行特定任务时出现的异常情况。&lt;h4&gt;方法&lt;/h4&gt;利用正态流(normalizing flow)预测机器人运动的可能性，并根据此概率判断机器人是正常的还是对抗性的。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在五种不同的对抗性行为策略中表现良好，准确分类至少80%的每一种对抗类型，同时保持低于5%的假阳性率。此外，硬件实验验证了与模拟场景相似的结果。&lt;h4&gt;结论&lt;/h4&gt;相较于现有最佳的方法，本文提出的方法提高了预测性能，并增强了检测标准的鲁棒性。&lt;h4&gt;翻译&lt;/h4&gt;摘要：提出了上下文异常检测方法并应用于执行覆盖任务时机器人集群的物理运动。通过模拟正常行为的数据训练正态流以预测给定环境下机器人的动作可能性。在应用中，利用观察到的动作预测概率来判断机器人是否属于正常或对抗性质的行为。该方法对五种不同的对抗策略进行评估，仅使用正常机器人行为的仿真数据进行训练，在未知异常本质的情况下实现了至少80%以上的准确率分类，并保持低于5%的假阳性率。另外通过硬件实验进一步验证了这一发现。相比现有最佳的方法，本文所提出的模型在预测性能和检测标准的鲁棒性上均有所提高。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A contextual anomaly detection method is proposed and applied to the physicalmotions of a robot swarm executing a coverage task. Using simulations of aswarm's normal behavior, a normalizing flow is trained to predict thelikelihood of a robot motion within the current context of its environment.During application, the predicted likelihood of the observed motions is used bya detection criterion that categorizes a robot agent as normal or antagonistic.The proposed method is evaluated on five different strategies of antagonisticbehavior. Importantly, only readily available simulated data of normal robotbehavior is used for training such that the nature of the anomalies need not beknown beforehand. The best detection criterion correctly categorizes at least80% of each antagonistic type while maintaining a false positive rate of lessthan 5% for normal robot agents. Additionally, the method is validated inhardware experiments, yielding results similar to the simulated scenarios.Compared to the state-of-the-art approach, both the predictive performance ofthe normalizing flow and the robustness of the detection criterion areincreased.</description>
      <author>example@mail.com (Ingeborg Wenger, Peter Eberhard, Henrik Ebel)</author>
      <guid isPermaLink="false">2502.20125v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion Transformers</title>
      <link>http://arxiv.org/abs/2502.20108v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一个新的自主驾驶决策制定管道VDT-Auto，它通过结合视觉语言模型（VLM）的状态理解和基于扩散Transformer的动作生成来解决动态环境和边缘情况带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶中，动态的环境因素和边角案例对车辆决策系统的鲁棒性构成重大挑战。&lt;h4&gt;目的&lt;/h4&gt;研究旨在提出一种新的方法以提升自主驾驶系统应对复杂和变化环境的能力，增强其在各种条件下的稳健性和性能表现。&lt;h4&gt;方法&lt;/h4&gt;该方法首先通过鸟瞰图（BEV）编码器提取周围图像的特征网格，并利用经过微调的视觉语言模型（VLM）生成文本嵌入和噪声路径。然后使用这些输出作为扩散过程中的正向和反向过程的条件。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该系统在nuScenes开放环规划评估中平均L2误差为0.52米，并且碰撞率为21%，展示了其卓越的一般性能。&lt;h4&gt;结论&lt;/h4&gt;VDT-Auto不仅通过开放数据集和代码发布的途径促进了研究界的进一步探索与改进，还证明了视觉语言模型（VLM）在自动驾驶决策制定中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于一种新的用于提高自动驾驶系统鲁棒性的方法的描述。该方法利用视觉语言模型来理解环境，并生成相应的动作策略。实验结果显示其具有良好的性能和泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In autonomous driving, dynamic environment and corner cases pose significantchallenges to the robustness of ego vehicle's decision-making. To address thesechallenges, commencing with the representation of state-action mapping in theend-to-end autonomous driving paradigm, we introduce a novel pipeline,VDT-Auto. Leveraging the advancement of the state understanding of VisualLanguage Model (VLM), incorporating with diffusion Transformer-based actiongeneration, our VDT-Auto parses the environment geometrically and contextuallyfor the conditioning of the diffusion process. Geometrically, we use abird's-eye view (BEV) encoder to extract feature grids from the surroundingimages. Contextually, the structured output of our fine-tuned VLM is processedinto textual embeddings and noisy paths. During our diffusion process, theadded noise for the forward process is sampled from the noisy path output ofthe fine-tuned VLM, while the extracted BEV feature grids and embedded textscondition the reverse process of our diffusion Transformers. Our VDT-Autoachieved 0.52m on average L2 errors and 21% on average collision rate in thenuScenes open-loop planning evaluation. Moreover, the real-world demonstrationexhibited prominent generalizability of our VDT-Auto. The code and dataset willbe released after acceptance.</description>
      <author>example@mail.com (Ziang Guo, Konstantin Gubernatorov, Selamawit Asfaw, Zakhar Yagudin, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2502.20108v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Pushing Through Clutter With Movability Awareness of Blocking Obstacles</title>
      <link>http://arxiv.org/abs/2502.20106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages (6+1), 5 images, 1 table, preprint version of accepted paper  at ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出一种考虑可移动障碍物的路径规划框架，通过结合全局语义可见性图和局部模型预测路径积分方法来应对传统路径规划方法在面对被阻挡路径时的挑战。&lt;h4&gt;背景&lt;/h4&gt;当障碍物阻塞了到达目标的路径时，传统的路径规划方法难以处理需要推动动作的情况。&lt;h4&gt;目的&lt;/h4&gt;提出一种不依赖于显式障碍物放置信息的方法框架，以克服NAMO问题中的挑战。&lt;h4&gt;方法&lt;/h4&gt;引入全局语义可见性图和局部模型预测路径积分（SVG-MPPI）方法相结合的策略来有效采样滚动，并考虑可移动物体在整个连续范围内的移动情况。采用物理引擎模拟滚动与环境的交互结果，生成最小化接触力的轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;在定性和定量实验中，SVG-MPPI框架的表现优于仅使用二进制可移动性的现有规划方法，在成功率和减少累积接触力方面都取得了更好的成绩。&lt;h4&gt;结论&lt;/h4&gt;所提出的SVG-MPPI框架提供了一种新颖的方法来处理具有可移动障碍物的导航问题，并且其代码已经在GitHub上公开供他人参考。&lt;h4&gt;翻译&lt;/h4&gt;对于在传统路径规划中遇到的由可移动障碍物导致的问题，我们提出了一套新的解决方案，该方案结合了全局语义可见性图和局部模型预测路径积分技术。通过使用物理引擎来模拟滚动与环境之间的交互，生成最小化接触力的最佳轨迹，并取得了优于现有方法的成绩。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigation Among Movable Obstacles (NAMO) poses a challenge for traditionalpath-planning methods when obstacles block the path, requiring push actions toreach the goal. We propose a framework that enables movability-aware planningto overcome this challenge without relying on explicit obstacle placement. Ourframework integrates a global Semantic Visibility Graph and a local ModelPredictive Path Integral (SVG-MPPI) approach to efficiently sample rollouts,taking into account the continuous range of obstacle movability. A physicsengine is adopted to simulate the interaction result of the rollouts with theenvironment, and generate trajectories that minimize contact force. Inqualitative and quantitative experiments, SVG-MPPI outperforms the existingparadigm that uses only binary movability for planning, achieving highersuccess rates with reduced cumulative contact forces. Our code is available at:https://github.com/tud-amr/SVG-MPPI</description>
      <author>example@mail.com (Joris J. Weeda, Saray Bakker, Gang Chen, Javier Alonso-Mora)</author>
      <guid isPermaLink="false">2502.20106v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>RIZE: Regularized Imitation Learning via Distributional Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.20089v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新颖的逆向强化学习（IRL）方法，该方法通过扩展最大熵IRL框架并引入平方时差(TD)正则化器和自适应目标来优化奖励函数。&lt;h4&gt;背景&lt;/h4&gt;现有的固定奖励分配方式存在局限性，难以保证灵活且可约束的隐式奖励正则化的灵活性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的IRL方法，以克服现有方法中的限制，并在模仿学习中提供对动态目标和奖励机制的有效理解。&lt;h4&gt;方法&lt;/h4&gt;通过结合最大熵IRL框架、自适应目标以及分布式的强化学习技术来优化奖励函数。具体地，引入了平方时差(TD)正则化器，该组件允许在训练过程中动态调整目标。&lt;h4&gt;主要发现&lt;/h4&gt;这种方法在具有挑战性的MuJoCo任务上展示了最先进的性能，在Humanoid任务中仅通过三个演示就达到了专家级别的结果。&lt;h4&gt;结论&lt;/h4&gt;广泛的实验和消融研究验证了所提方法的有效性，并提供了对模仿学习中自适应目标和奖励动态的理解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel Inverse Reinforcement Learning (IRL) approach thatovercomes limitations of fixed reward assignments and constrained flexibilityin implicit reward regularization. By extending the Maximum Entropy IRLframework with a squared temporal-difference (TD) regularizer and adaptivetargets, dynamically adjusted during training, our method indirectly optimizesa reward function while incorporating reinforcement learning principles.Furthermore, we integrate distributional RL to capture richer returninformation. Our approach achieves state-of-the-art performance on challengingMuJoCo tasks, demonstrating expert-level results on the Humanoid task with only3 demonstrations. Extensive experiments and ablation studies validate theeffectiveness of our method, providing insights into adaptive targets andreward dynamics in imitation learning.</description>
      <author>example@mail.com (Adib Karimi, Mohammad Mehdi Ebadzadeh)</author>
      <guid isPermaLink="false">2502.20089v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Minds on the Move: Decoding Trajectory Prediction in Autonomous Driving with Cognitive Insights</title>
      <link>http://arxiv.org/abs/2502.20084v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在混合自主驾驶环境中，准确预测周围车辆的未来轨迹对于自动驾驶汽车的安全运行至关重要。该研究提出了一种新的认知启发型变压器（Cognitive-Informed Transformer, CITF），通过引入感知安全概念来理解驾驶员的决策机制。&lt;h4&gt;背景&lt;/h4&gt;现有模型主要关注数据中的统计模式，忽略了理解和解释人类驾驶者的决策过程的重要性，这导致了模型在长期轨迹预测方面的性能不足。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的模型CITF，该模型能够捕捉到人类驾驶员真实意图，并提高长期轨迹预测的准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的模块Leanformer来捕获车辆之间的社会互动。此外，还设计了一个感知安全感知模块，包括定量安全评估和驾驶行为特征描述。&lt;h4&gt;主要发现&lt;/h4&gt;CITF模型在三个公认的基准数据集上显示出显著性能提升：NGSIM（12.0%），HighD（28.2%）以及MoCAD（20.8%）。此外，在数据有限或缺失的情况下，该模型也表现出强大的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;CITF不仅在现有的基准测试中表现优异，并且展示了对现实世界应用的适应性和有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文内容。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In mixed autonomous driving environments, accurately predicting the futuretrajectories of surrounding vehicles is crucial for the safe operation ofautonomous vehicles (AVs). In driving scenarios, a vehicle's trajectory isdetermined by the decision-making process of human drivers. However, existingmodels primarily focus on the inherent statistical patterns in the data, oftenneglecting the critical aspect of understanding the decision-making processesof human drivers. This oversight results in models that fail to capture thetrue intentions of human drivers, leading to suboptimal performance inlong-term trajectory prediction. To address this limitation, we introduce aCognitive-Informed Transformer (CITF) that incorporates a cognitive concept,Perceived Safety, to interpret drivers' decision-making mechanisms. PerceivedSafety encapsulates the varying risk tolerances across drivers with differentdriving behaviors. Specifically, we develop a Perceived Safety-aware Modulethat includes a Quantitative Safety Assessment for measuring the subject risklevels within scenarios, and Driver Behavior Profiling for characterizingdriver behaviors. Furthermore, we present a novel module, Leanformer, designedto capture social interactions among vehicles. CITF demonstrates significantperformance improvements on three well-established datasets. In terms oflong-term prediction, it surpasses existing benchmarks by 12.0% on the NGSIM,28.2% on the HighD, and 20.8% on the MoCAD dataset. Additionally, itsrobustness in scenarios with limited or missing data is evident, surpassingmost state-of-the-art (SOTA) baselines, and paving the way for real-worldapplications.</description>
      <author>example@mail.com (Haicheng Liao, Chengyue Wang, Kaiqun Zhu, Yilong Ren, Bolin Gao, Shengbo Eben Li, Chengzhong Xu, Zhenning Li)</author>
      <guid isPermaLink="false">2502.20084v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>BEV-DWPVO: BEV-based Differentiable Weighted Procrustes for Low Scale-drift Monocular Visual Odometry on Ground</title>
      <link>http://arxiv.org/abs/2502.20078v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为BEV-DWPVO的新型单目视觉测距系统，该系统利用鸟瞰图（Bird's-Eye View, BEV）特征地图以统一尺度表示环境，简化姿态估计过程，并通过可微加权Procrustes求解器进行姿态估计。&lt;h4&gt;背景&lt;/h4&gt;单目视觉测距(MVO)为自动驾驶车辆提供了一种成本效益高、实时定位解决方案。然而，由于缺乏来自单目相机的内在尺度信息，MVO系统面临共同问题。传统的MVO方法虽然具有良好的解释性，但只能获得相对比例，并且在长距离任务中会出现严重的比例漂移。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的MVO系统BEV-DWPVO，以解决传统MVO方法存在的局限性和挑战，提高其性能和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;采用鸟瞰图（BEV）特征地图表示环境，并假设地面为平面。通过在统一尺度的网格结构中提取并匹配关键点，利用可微加权Procrustes求解器进行姿态估计。整个系统完全可微，仅需姿态监督即可端到端训练。&lt;h4&gt;主要发现&lt;/h4&gt;提出的BEV-DWPVO方法在长序列数据集NCLT、牛津和KITTI上的表现优于现有的MVO方法，在大多数评估指标上取得了卓越的成绩。&lt;h4&gt;结论&lt;/h4&gt;通过使用鸟瞰图特征地图和可微加权Procrustes求解器，新系统BEV-DWPVO成功地解决了传统MVO系统的局限性，并在实际测试中展示了优越的性能。这种方法为未来的自动驾驶车辆定位提供了一种新的有效途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要：单目视觉测距(MVO)为自主车辆提供了一种成本效益高、实时定位解决方案。然而，由于来自单目相机缺乏内在尺度信息，MVO系统面临共同问题。传统的MVO方法虽然具有良好的解释性，但只能获得相对比例，并且在长距离任务中会出现严重的比例漂移。基于学习的方法利用透视视角，通过大量训练数据获取先验知识并预测深度值以估计绝对比例。然而，由于需要准确估计每个点的深度，这种方法泛化能力有限。相比之下，我们提出了一种新的MVO系统称为BEV-DWPVO。我们的方法使用地面平面的共同假设，并通过鸟瞰图（BEV）特征地图表示环境，在统一尺度下的网格结构中简化姿态估计过程从6自由度到3自由度。关键点在BEV空间内被提取和匹配，随后通过可微加权Procrustes求解器进行姿态估计。整个系统完全可微，仅需姿态监督即可端到端训练，无需辅助任务。我们在挑战性的长序列数据集NCLT、牛津和KITTI上验证了BEV-DWPVO，并在大多数评估指标中超越现有MVO方法取得卓越结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular Visual Odometry (MVO) provides a cost-effective, real-timepositioning solution for autonomous vehicles. However, MVO systems face thecommon issue of lacking inherent scale information from monocular cameras.Traditional methods have good interpretability but can only obtain relativescale and suffer from severe scale drift in long-distance tasks. Learning-basedmethods under perspective view leverage large amounts of training data toacquire prior knowledge and estimate absolute scale by predicting depth values.However, their generalization ability is limited due to the need to accuratelyestimate the depth of each point. In contrast, we propose a novel MVO systemcalled BEV-DWPVO. Our approach leverages the common assumption of a groundplane, using Bird's-Eye View (BEV) feature maps to represent the environment ina grid-based structure with a unified scale. This enables us to reduce thecomplexity of pose estimation from 6 Degrees of Freedom (DoF) to 3-DoF.Keypoints are extracted and matched within the BEV space, followed by poseestimation through a differentiable weighted Procrustes solver. The entiresystem is fully differentiable, supporting end-to-end training with only posesupervision and no auxiliary tasks. We validate BEV-DWPVO on the challenginglong-sequence datasets NCLT, Oxford, and KITTI, achieving superior results overexisting MVO methods on most evaluation metrics.</description>
      <author>example@mail.com (Yufei Wei, Sha Lu, Wangtao Lu, Rong Xiong, Yue Wang)</author>
      <guid isPermaLink="false">2502.20078v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>HiFAR: Multi-Stage Curriculum Learning for High-Dynamics Humanoid Fall Recovery</title>
      <link>http://arxiv.org/abs/2502.20061v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种针对人形机器人跌倒恢复的多阶段课程学习框架HiFAR，该框架通过逐步增加复杂性和维度来解决传统控制方法和强化学习技术在处理高维动力学和复杂碰撞场景方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;当前的人形机器人难以自主地从跌倒中恢复过来，尤其是面对动态且无结构的环境。传统的控制方法通常无法应对这些挑战，而基于强化学习的方法则受到稀疏奖励、复杂的碰撞情景以及仿真与实际应用之间差异的影响。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架来帮助人形机器人有效地处理各种类型的跌倒场景，并能够适应现实世界的跌倒情况。&lt;h4&gt;方法&lt;/h4&gt;使用了一种名为HiFAR的多阶段课程学习框架，该框架采用分阶段的学习策略逐步引入更为复杂和高维的恢复任务。通过这种方式让机器人在不同情况下掌握高效且稳定的跌倒恢复策略。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法经过实际的人形机器人的测试，展示了其能够自主地从多种跌倒情况中快速而稳定地恢复过来，并具有较高的成功率、较快的恢复时间和较强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究结果表明，HiFAR框架为解决人形机器人跌倒恢复问题提供了一种有效的方法。这种方法不仅提高了机器人的适应性和稳定性，还大大增强了其自主应对复杂环境的能力。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人在动态和无结构环境中从跌倒中自主恢复面临巨大挑战。传统的控制方法不足以处理高维动力学和密集接触的特点，而强化学习技术则受到稀疏奖励、复杂碰撞场景以及模拟与现实应用差异的困扰。本文提出了一种名为HiFAR的多阶段课程学习框架，通过逐步纳入更加复杂的跌倒恢复任务来帮助机器人获取高效且稳定的策略。该方法已在实际的人形机器人体上进行了测试，并证明了其在跌倒恢复上的自主性、快速性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humanoid robots encounter considerable difficulties in autonomouslyrecovering from falls, especially within dynamic and unstructured environments.Conventional control methodologies are often inadequate in addressing thecomplexities associated with high-dimensional dynamics and the contact-richnature of fall recovery. Meanwhile, reinforcement learning techniques arehindered by issues related to sparse rewards, intricate collision scenarios,and discrepancies between simulation and real-world applications. In thisstudy, we introduce a multi-stage curriculum learning framework, termed HiFAR.This framework employs a staged learning approach that progressivelyincorporates increasingly complex and high-dimensional recovery tasks, therebyfacilitating the robot's acquisition of efficient and stable fall recoverystrategies. Furthermore, it enables the robot to adapt its policy toeffectively manage real-world fall incidents. We assess the efficacy of theproposed method using a real humanoid robot, showcasing its capability toautonomously recover from a diverse range of falls with high success rates,rapid recovery times, robustness, and generalization.</description>
      <author>example@mail.com (Penghui Chen, Yushi Wang, Changsheng Luo, Wenhan Cai, Mingguo Zhao)</author>
      <guid isPermaLink="false">2502.20061v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Night-Voyager: Consistent and Efficient Nocturnal Vision-Aided State Estimation in Object Maps</title>
      <link>http://arxiv.org/abs/2502.20054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Transactions on Robotics (T-RO), 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;夜间准确且稳健的状态估计对于实现自主机器人导航的昼夜或全天候任务至关重要。本文提出了一个利用先验对象地图和关键点进行夜视辅助状态估计的新框架Night-Voyager。&lt;h4&gt;背景&lt;/h4&gt;现有大多数视觉方法在不良照明条件下可能失败，即使使用主动光源或图像增强也难以克服这一问题。然而，在多数城市场景中，路灯作为稳定的显著前导视觉线索，在夜间导航中起到了类似深空星星的作用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的夜视辅助状态估计框架Night-Voyager，该框架可以利用先验对象地图和关键点信息实现灵活的定位。&lt;h4&gt;方法&lt;/h4&gt;Night-Voyager通过快速初始化解决全局定位问题，并采用有效的两阶段跨模态数据关联技术来提供基于地图观测的整体一致性状态更新。此外，在处理夜间视觉观察中显著不确定性的挑战时，引入了新颖的矩阵李群公式化和特征解耦多态不变滤波器。&lt;h4&gt;主要发现&lt;/h4&gt;传统的视觉方法在照明条件不佳的情况下依赖像素级度量作为其最主要的限制，而Night-Voyager利用非像素级的对象检测来促进对象地图信息的有效传播与使用。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的仿真及多样化的实际应用场景（覆盖约12.3公里）的实验验证了Night-Voyager的有效性、鲁棒性和效率，填补了夜间视觉辅助状态估计的重要空白。&lt;h4&gt;翻译&lt;/h4&gt;准确且稳健的夜间状态估计对于实现昼夜或全天候任务下的自主机器人导航至关重要。是否可以利用低成本的标准相机进行夜间环境的状态估计？现有的大多数视觉方法在不良照明条件下难以发挥作用，即使使用主动光源或图像增强也是如此。然而，在多数城市环境中，路灯作为夜晚稳定的显著先验视觉线索起到了类似星星在深空为航天器提供导航的作用。受到这一启发，我们提出了一种新的夜视辅助状态估计框架Night-Voyager，利用先前对象地图和关键点进行灵活的定位。研究发现传统的视觉方法依赖于像素级别的度量标准作为其主要限制，在不良照明条件下表现不佳。相比之下，非像素级、无度量的对象检测可以充当从像素级别到对象级别的桥梁，促进系统内部对象地图信息的有效传播与使用。Night-Voyager通过快速初始化解决全局定位问题，并利用基于地图的观察提供整体一致性状态更新。为了应对夜间视觉观测中的显著不确定性挑战，引入了新颖的矩阵李群公式化和特征解耦多态不变滤波器以确保一致且高效的估计结果。在广泛的仿真及多样化的实际应用场景（覆盖约12.3公里）中展示其有效性、鲁棒性和效率，弥补了夜视辅助状态估计的重要不足之处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate and robust state estimation at nighttime is essential for autonomousrobotic navigation to achieve nocturnal or round-the-clock tasks. An intuitivequestion arises: Can low-cost standard cameras be exploited for nocturnal stateestimation? Regrettably, most existing visual methods may fail under adverseillumination conditions, even with active lighting or image enhancement. Apivotal insight, however, is that streetlights in most urban scenarios act asstable and salient prior visual cues at night, reminiscent of stars in deepspace aiding spacecraft voyage in interstellar navigation. Inspired by this, wepropose Night-Voyager, an object-level nocturnal vision-aided state estimationframework that leverages prior object maps and keypoints for versatilelocalization. We also find that the primary limitation of conventional visualmethods under poor lighting conditions stems from the reliance on pixel-levelmetrics. In contrast, metric-agnostic, non-pixel-level object detection servesas a bridge between pixel-level and object-level spaces, enabling effectivepropagation and utilization of object map information within the system.Night-Voyager begins with a fast initialization to solve the globallocalization problem. By employing an effective two-stage cross-modal dataassociation, the system delivers globally consistent state updates usingmap-based observations. To address the challenge of significant uncertaintiesin visual observations at night, a novel matrix Lie group formulation and afeature-decoupled multi-state invariant filter are introduced, ensuringconsistent and efficient estimation. Through comprehensive experiments in bothsimulation and diverse real-world scenarios (spanning approximately 12.3 km),Night-Voyager showcases its efficacy, robustness, and efficiency, filling acritical gap in nocturnal vision-aided state estimation.</description>
      <author>example@mail.com (Tianxiao Gao, Mingle Zhao, Chengzhong Xu, Hui Kong)</author>
      <guid isPermaLink="false">2502.20054v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary Affordance Detection in 3D Worlds</title>
      <link>http://arxiv.org/abs/2502.20041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的3D可及性检测框架，该框架通过引入大规模语言模型并设计定制化解码器来生成可及性掩模，解决了传统基于标签的语义分割方法在开放场景中的局限性。&lt;h4&gt;背景&lt;/h4&gt;传统的3D可及性检测依赖于预定义标签进行基于语义分割的任务，并且难以理解复杂的自然语言描述。这种范式在处理开放式复杂场景时存在泛化能力不足的问题。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有技术的限制，提出了一种新的任务形式——指令推理可及性分割（IRAS），该任务旨在根据给定的查询文本生成可及性掩模区域。&lt;h4&gt;方法&lt;/h4&gt;提出一种名为3D-AffordanceLLM (3D-ADLLM) 的新框架。它通过引入大规模语言模型并设计定制化解码器来实现开放世界的推理式可及性检测，并采用多阶段训练策略，包括Referring Object Part Segmentation任务的预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在缺乏足够的3D可及性数据集的情况下，该方法利用通用分割数据提取知识并转移到可及性检测中。通过这种创新的方法和框架设计，在开放词汇量的可及性检测任务上实现了大约8% mIoU的改进。&lt;h4&gt;结论&lt;/h4&gt;所提出的3D-ADLLM框架充分利用了大规模语言模型中的丰富世界知识和人与物体互动推理能力，证明在处理开放世界的复杂场景时具有优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 3D Affordance detection is a challenging problem with broad applications onvarious robotic tasks. Existing methods typically formulate the detectionparadigm as a label-based semantic segmentation task. This paradigm relies onpredefined labels and lacks the ability to comprehend complex natural language,resulting in limited generalization in open-world scene. To address theselimitations, we reformulate the traditional affordance detection paradigm into\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This taskis designed to output a affordance mask region given a query reasoning text,which avoids fixed categories of input labels. We accordingly propose the\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoningaffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces largelanguage models (LLMs) to 3D affordance perception with a custom-designeddecoder for generating affordance masks, thus achieving open-world reasoningaffordance detection. In addition, given the scarcity of 3D affordance datasetsfor training large models, we seek to extract knowledge from generalsegmentation data and transfer it to affordance detection. Thus, we propose amulti-stage training strategy that begins with a novel pre-training task, i.e.,\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed toequip the model with general recognition and segmentation capabilities at theobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLMobtains the reasoning ability for affordance detection. In summary, 3D-ADLLMleverages the rich world knowledge and human-object interaction reasoningability of LLMs, achieving approximately an 8\% improvement in mIoU onopen-vocabulary affordance detection tasks.</description>
      <author>example@mail.com (Hengshuo Chu, Xiang Deng, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie)</author>
      <guid isPermaLink="false">2502.20041v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>FuseGrasp: Radar-Camera Fusion for Robotic Grasping of Transparent Objects</title>
      <link>http://arxiv.org/abs/2502.20037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为FuseGrasp的系统，该系统是首个雷达和相机融合技术应用于透明物体抓取的技术。通过毫米波信号和深度学习网络的有效结合，改进了机器人在低光环境下的性能。&lt;h4&gt;背景&lt;/h4&gt;透明物品在日常生活环境中普遍存在，但它们独特的物理特性给依靠摄像机引导的机械臂带来了挑战。现有研究主要依赖于单独使用相机的方法，在光照不足等条件下效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够增强抓取透明物体能力的雷达-相机融合系统，以提高机器人操作透明物品的成功率和准确性。&lt;h4&gt;方法&lt;/h4&gt;利用毫米波信号可以穿透透明材料并使其呈现半透明或不透明的特点，结合摄像机数据获取高质量的雷达图像，并设计了一个深度神经网络来融合这两种模式的数据。采用两阶段训练策略解决缺乏雷达图像的问题：首先在公共RGB-D数据集上预训练系统，然后使用小规模自建的RGB-D-Radar数据集进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示FuseGrasp显著提高了透明物体的深度重建精度和材料识别能力，在真实世界中验证了其处理透明物品的能力增强。&lt;h4&gt;结论&lt;/h4&gt;通过雷达-相机融合技术，可以有效提高机器人对环境感知能力和操作效率，尤其在低光条件下性能更优。这项工作为未来的机器人系统开发提供了新的方向。&lt;h4&gt;视频链接&lt;/h4&gt;https://youtu.be/MWDqv0sRSok&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transparent objects are prevalent in everyday environments, but theirdistinct physical properties pose significant challenges for camera-guidedrobotic arms. Current research is mainly dependent on camera-only approaches,which often falter in suboptimal conditions, such as low-light environments. Inresponse to this challenge, we present FuseGrasp, the first radar-camera fusionsystem tailored to enhance the transparent objects manipulation. FuseGraspexploits the weak penetrating property of millimeter-wave (mmWave) signals,which causes transparent materials to appear opaque, and combines it with theprecise motion control of a robotic arm to acquire high-quality mmWave radarimages of transparent objects. The system employs a carefully designed deepneural network to fuse radar and camera imagery, thereby improving depthcompletion and elevating the success rate of object grasping. Nevertheless,training FuseGrasp effectively is non-trivial, due to limited radar imagedatasets for transparent objects. We address this issue utilizing large RGB-Ddataset, and propose an effective two-stage training approach: we firstpre-train FuseGrasp on a large public RGB-D dataset of transparent objects,then fine-tune it on a self-built small RGB-D-Radar dataset. Furthermore, as abyproduct, FuseGrasp can determine the composition of transparent objects, suchas glass or plastic, leveraging the material identification capability ofmmWave radar. This identification result facilitates the robotic arm inmodulating its grip force appropriately. Extensive testing reveals thatFuseGrasp significantly improves the accuracy of depth reconstruction andmaterial identification for transparent objects. Moreover, real-world robotictrials have confirmed that FuseGrasp markedly enhances the handling oftransparent items. A video demonstration of FuseGrasp is available athttps://youtu.be/MWDqv0sRSok.</description>
      <author>example@mail.com (Hongyu Deng, Tianfan Xue, He Chen)</author>
      <guid isPermaLink="false">2502.20037v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Multi-Keypoint Affordance Representation for Functional Dexterous Grasping</title>
      <link>http://arxiv.org/abs/2502.20018v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The source code and demo videos will be publicly available at  https://github.com/PopeyePxx/MKA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本研究提出了一种用于功能灵巧抓握的多关键点作用表示法，直接编码任务驱动的抓取配置，并通过接触引导的关键点提取方法实现了视觉感知和灵巧操作之间的直接连接。&lt;h4&gt;背景&lt;/h4&gt;现有的基于作用的方法主要预测粗略交互区域，无法直接约束抓取姿势，导致视觉感知与操纵之间存在断开。&lt;h4&gt;目的&lt;/h4&gt;为了弥合这一差距，我们提出了一个多关键点作用表示法来解决现有方法在预测精细交互方面的问题，并通过引入Contact-guided Multi-Keypoint Affordance (CMKA) 方法和基于关键点的抓握矩阵转换(KGT)方法改进了抓取的一致性和泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种多关键点作用表示法，利用人类抓握经验图像进行弱监督，并结合大型视觉模型提取精细的作用特征。此外，还提出了一种基于关键点的抓握手性变换（KGT）方法，确保手部关键点与物体接触点之间的空间一致性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著提高了作用定位精度、抓握一致性和对未知工具和任务的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本研究为视觉作用学习与灵巧机器人操作之间建立了桥梁，展示了在真实世界数据集、IsaacGym仿真环境及具有挑战性的机器人任务中的有效性。&lt;h4&gt;翻译&lt;/h4&gt;功能性灵巧抓握需要精确的手-物体交互，超越简单的夹持。现有基于作用的方法主要预测粗略的交互区域，并且无法直接约束抓取姿势，导致视觉感知与操作之间存在断开。为解决这一问题，我们提出了一种用于功能灵巧抓握的多关键点作用表示法，该方法通过定位功能性接触点直接编码任务驱动的抓取配置。此外，还引入了Contact-guided Multi-Keypoint Affordance (CMKA) 方法，并结合大型视觉模型进行弱监督和精细的作用特征提取，实现泛化同时避免手动的关键点注释。另外提出了一种基于关键点的手部矩阵变换(KGT)方法，确保手部关键点与物体接触点之间的空间一致性，从而为视觉感知和灵巧抓握动作之间建立了直接连接。在公共真实世界FAH数据集、IsaacGym仿真及具有挑战性的机器人任务上的实验表明，我们的方法显著提高了作用定位精度、抓取一致性和对未知工具和任务的泛化能力，弥合了视觉作用学习与灵巧机器人操作之间的差距。源代码和演示视频将在https://github.com/PopeyePxx/MKA公开提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Functional dexterous grasping requires precise hand-object interaction, goingbeyond simple gripping. Existing affordance-based methods primarily predictcoarse interaction regions and cannot directly constrain the grasping posture,leading to a disconnection between visual perception and manipulation. Toaddress this issue, we propose a multi-keypoint affordance representation forfunctional dexterous grasping, which directly encodes task-driven graspconfigurations by localizing functional contact points. Our method introducesContact-guided Multi-Keypoint Affordance (CMKA), leveraging human graspingexperience images for weak supervision combined with Large Vision Models forfine affordance feature extraction, achieving generalization while avoidingmanual keypoint annotations. Additionally, we present a Keypoint-based Graspmatrix Transformation (KGT) method, ensuring spatial consistency between handkeypoints and object contact points, thus providing a direct link betweenvisual perception and dexterous grasping actions. Experiments on publicreal-world FAH datasets, IsaacGym simulation, and challenging robotic tasksdemonstrate that our method significantly improves affordance localizationaccuracy, grasp consistency, and generalization to unseen tools and tasks,bridging the gap between visual affordance learning and dexterous roboticmanipulation. The source code and demo videos will be publicly available athttps://github.com/PopeyePxx/MKA.</description>
      <author>example@mail.com (Fan Yang, Dongsheng Luo, Wenrui Chen, Jiacheng Lin, Junjie Cai, Kailun Yang, Zhiyong Li, Yaonan Wang)</author>
      <guid isPermaLink="false">2502.20018v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Collaborative Object Handover in a Robot Crafting Assistant</title>
      <link>http://arxiv.org/abs/2502.19991v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个通过人类遥操作数据训练的合作交接模型，旨在提高机器人与人合作时的安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;随着机器人的普及，它们越来越多地参与到需要与人类互动的工作中，例如在餐厅里递送食物或在装配线上帮助工人。这些场景通常涉及物品的交接过程。&lt;h4&gt;目的&lt;/h4&gt;为了实现安全和高效的协作机器人系统（HRC），有必要将人类行为上下文融入到机器人的合作策略当中。&lt;h4&gt;方法&lt;/h4&gt;研究人员开发了一种基于自然手工任务中的遥操作数据训练的合作交接模型，并通过交叉验证实验以及用户研究来评估该模型的表现。&lt;h4&gt;主要发现&lt;/h4&gt;自主政策成功实现了协作性交接，但在与人类遥操作的比较中揭示了进一步改进的空间。&lt;h4&gt;结论&lt;/h4&gt;虽然研究表明该合作交接策略能够有效实现机器人和人之间的安全高效的协作，但仍存在改善潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots are increasingly working alongside people, delivering food to patronsin restaurants or helping workers on assembly lines. These scenarios ofteninvolve object handovers between the person and the robot. To achieve safe andefficient human-robot collaboration (HRC), it is important to incorporate humancontext in a robot's handover strategies. Therefore, in this work, we develop acollaborative handover model trained on human teleoperation data collected in anaturalistic crafting task. To evaluate the performance of this model, weconduct cross-validation experiments on the training dataset as well as a userstudy in the same HRC crafting task. The handover episodes and user perceptionsof the autonomous handover policy were compared with those of the humanteleoperated handovers. While the cross-validation experiment and user studyindicate that the autonomous policy successfully achieved collaborativehandovers, the comparison with human teleoperation revealed avenues for furtherimprovements.</description>
      <author>example@mail.com (Leimin Tian, Shiyu Xu, Kerry He, Rachel Love, Akansel Cosgun, Dana Kulic)</author>
      <guid isPermaLink="false">2502.19991v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>CarPlanner: Consistent Auto-regressive Trajectory Planning for Large-scale Reinforcement Learning in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.19908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CVPR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CarPlanner是一种基于强化学习（RL）的轨迹规划器，旨在解决自动驾驶中的训练效率和性能提升问题。&lt;h4&gt;背景&lt;/h4&gt;当前，虽然一些基于机器学习的方法在特定场景中表现出色，但它们难以应对大规模、复杂的真实驾驶环境挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态轨迹生成方法——CarPlanner，该方法结合了自回归结构和一致性机制以提高训练效率并增强性能稳定性。&lt;h4&gt;方法&lt;/h4&gt;1. CarPlanner采用了自回归的强化学习框架。2. 通过一致性的引入来维护时间序列的一致性，从而稳定策略的学习过程。3. 利用专家指导奖励函数和不变视图模块简化RL训练流程。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在nuPlan大规模真实世界数据集上超越了现有基于规则、强化学习和模仿学习的方法，展示了其优越的性能。&lt;h4&gt;结论&lt;/h4&gt;CarPlanner作为一个潜在解决方案，在自动驾驶轨迹规划中显示出巨大潜力。它有效地解决了大规模真实场景下的训练效率问题，并且能够在挑战性任务中超越当前最先进的方法。&lt;h4&gt;翻译&lt;/h4&gt;路径规划是自主驾驶的关键组成部分，用于确保复杂环境中的安全高效导航。尽管最近基于学习的方法——特别是强化学习（RL）在特定情况下取得了显著成果，但它们仍然难以克服大规模现实世界场景下的训练效率问题。为此，我们引入了CarPlanner，这是一种自回归式轨迹生成器，它利用RL来生成多模态路径。此方法通过维护时间序列的一致性确保策略学习的稳定性，并且采用了指导式的奖励函数和不变视图模块来简化RL训练过程并提升性能表现。实验分析表明，该框架有效地解决了训练效率低下及性能不足的问题，是自动驾驶轨迹规划中的一个有前景的方法。根据我们的知识，在nuPlan这一大型现实世界数据集中，我们首次展示了基于RL的路径规划器可以超越基于规则和模仿学习的最佳方法（SOTAs）的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory planning is vital for autonomous driving, ensuring safe andefficient navigation in complex environments. While recent learning-basedmethods, particularly reinforcement learning (RL), have shown promise inspecific scenarios, RL planners struggle with training inefficiencies andmanaging large-scale, real-world driving scenarios. In this paper, we introduce\textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive\textbf{Planner} that uses RL to generate multi-modal trajectories. Theauto-regressive structure enables efficient large-scale RL training, while theincorporation of consistency ensures stable policy learning by maintainingcoherent temporal consistency across time steps. Moreover, CarPlanner employs ageneration-selection framework with an expert-guided reward function and aninvariant-view module, simplifying RL training and enhancing policyperformance. Extensive analysis demonstrates that our proposed RL frameworkeffectively addresses the challenges of training efficiency and performanceenhancement, positioning CarPlanner as a promising solution for trajectoryplanning in autonomous driving. To the best of our knowledge, we are the firstto demonstrate that the RL-based planner can surpass both IL- and rule-basedstate-of-the-arts (SOTAs) on the challenging large-scale real-world datasetnuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTAapproaches within this demanding dataset.</description>
      <author>example@mail.com (Dongkun Zhang, Jiaming Liang, Ke Guo, Sha Lu, Qi Wang, Rong Xiong, Zhenwei Miao, Yue Wang)</author>
      <guid isPermaLink="false">2502.19908v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Shared Autonomy for Proximal Teaching</title>
      <link>http://arxiv.org/abs/2502.19899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ACM/IEEE International Conference on Human-Robot  Interaction, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用共享自主性的方法Z-COACH，旨在为学习复杂技能（如高性能赛车）的学生提供个性化的指导。&lt;h4&gt;背景&lt;/h4&gt;在进行运动技能的学习时，通常需要经验丰富的专业人士来进行个性化教学。然而，在特定任务领域中高质量的培训资源可能有限，尤其是在像高性能赛车这样专业化程度较高的领域。&lt;h4&gt;目的&lt;/h4&gt;旨在通过教育心理学中的支架理论来设计一种方法，利用共享自主性框架结合用户的输入与机器人的自主性，以优化教学策略。&lt;h4&gt;方法&lt;/h4&gt;提出了Z-COACH方法，该方法使用共享自主性的原则来提供个性化的指导，并重点训练学生易于理解和学习的任务子技能。&lt;h4&gt;主要发现&lt;/h4&gt;在一项有50名参与者的研究中，在模拟的Thunderhill Raceway Park环境中通过CARLA自动驾驶模拟器进行高性能赛车教学时，Z-COACH帮助识别了每位学生的最优先练习技能，从而提高了驾驶时间、行为和流畅度的表现。&lt;h4&gt;结论&lt;/h4&gt;本研究证明了可用的半自主能力（如车辆或机器人）不仅可以辅助人类用户，还能有效教导他们学习复杂的任务子技能。&lt;h4&gt;翻译&lt;/h4&gt;本文介绍了利用共享自主性的Z-COACH方法，通过模拟环境测试，在高性能赛车领域展示了该方法在个性化教学方面的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motor skill learning often requires experienced professionals who can providepersonalized instruction. Unfortunately, the availability of high-qualitytraining can be limited for specialized tasks, such as high performance racing.Several recent works have leveraged AI-assistance to improve instruction oftasks ranging from rehabilitation to surgical robot tele-operation. However,these works often make simplifying assumptions on the student learning process,and fail to model how a teacher's assistance interacts with differentindividuals' abilities when determining optimal teaching strategies. Inspiredby the idea of scaffolding from educational psychology, we leverage sharedautonomy, a framework for combining user inputs with robot autonomy, to aidwith curriculum design. Our key insight is that the way a student's behaviorimproves in the presence of assistance from an autonomous agent can highlightwhich sub-skills might be most ``learnable'' for the student, or within theirZone of Proximal Development. We use this to design Z-COACH, a method for usingshared autonomy to provide personalized instruction targeting interpretabletask sub-skills. In a user study (n=50), where we teach high performance racingin a simulated environment of the Thunderhill Raceway Park with the CARLAAutonomous Driving simulator, we show that Z-COACH helps identify which skillseach student should first practice, leading to an overall improvement indriving time, behavior, and smoothness. Our work shows that increasinglyavailable semi-autonomous capabilities (e.g. in vehicles, robots) can not onlyassist human users, but also help *teach* them.</description>
      <author>example@mail.com (Megha Srivastava, Reihaneh Iranmanesh, Yuchen Cui, Deepak Gopinath, Emily Sumner, Andrew Silva, Laporsha Dees, Guy Rosman, Dorsa Sadigh)</author>
      <guid isPermaLink="false">2502.19899v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>ColorDynamic: Generalizable, Scalable, Real-time, End-to-end Local Planner for Unstructured and Dynamic Environments</title>
      <link>http://arxiv.org/abs/2502.19892v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;该研究提出了一种名为ColorDynamic的框架，用于解决机器人在非结构化和动态环境中的局部规划问题。&lt;h4&gt;背景&lt;/h4&gt;深度强化学习（DRL）展示了处理机器人局部规划问题的潜力，但在高度非结构化的、动态环境中其效果受到限制。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的深度强化学习方法来提高机器人在复杂环境下的决策能力和实时性能。&lt;h4&gt;方法&lt;/h4&gt;{'框架设计': '提出了一种端到端的DRL形式化方法，直接将原始传感器数据映射为控制命令，使该方法适应于非结构化的环境。同时引入Transqer网络，它支持从时间过渡中进行在线DRL学习，增强动态场景中的决策能力。', '平台开发': '为了便于多样化数据集的可扩展训练，设计了一种高效的模拟平台E-Sparrow，并结合对称不变性技术来增加数据量。', '实验验证': '通过与最先进的方法比较评估、通用性、可伸缩性和实时性能测试来证明ColorDynamic的有效性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'成功率': '该方法在实验中取得了超过90%的成功率。', '延迟时间': '展示了实现实时能力（每次规划1.2-1.3毫秒）的能力。', '组件贡献': '通过消融研究证明了各组成部分对整体性能的贡献。'}&lt;h4&gt;结论&lt;/h4&gt;基于ColorDynamic，开发了一种名为OkayPlan-ColorDynamic (OPCD)的导航系统，并通过模拟和真实世界的实验展示了其在复杂环境中的优越性和适用性。&lt;h4&gt;代码与数据公开&lt;/h4&gt;研究的源码及实验演示已在其官方网站上开源，以促进可重复研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-27&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep Reinforcement Learning (DRL) has demonstrated potential in addressingrobotic local planning problems, yet its efficacy remains constrained in highlyunstructured and dynamic environments. To address these challenges, this studyproposes the ColorDynamic framework. First, an end-to-end DRL formulation isestablished, which maps raw sensor data directly to control commands, therebyensuring compatibility with unstructured environments. Under this formulation,a novel network, Transqer, is introduced. The Transqer enables online DRLlearning from temporal transitions, substantially enhancing decision-making indynamic scenarios. To facilitate scalable training of Transqer with diversedata, an efficient simulation platform E-Sparrow, along with a dataaugmentation technique leveraging symmetric invariance, are developed.Comparative evaluations against state-of-the-art methods, alongside assessmentsof generalizability, scalability, and real-time performance, were conducted tovalidate the effectiveness of ColorDynamic. Results indicate that our approachachieves a success rate exceeding 90% while exhibiting real-time capacity(1.2-1.3 ms per planning). Additionally, ablation studies were performed tocorroborate the contributions of individual components. Building on this, theOkayPlan-ColorDynamic (OPCD) navigation system is presented, with simulated andreal-world experiments demonstrating its superiority and applicability incomplex scenarios. The codebase and experimental demonstrations have beenopen-sourced on our website to facilitate reproducibility and further research.</description>
      <author>example@mail.com (Jinghao Xin, Zhichao Liang, Zihuan Zhang, Peng Wang, Ning Li)</author>
      <guid isPermaLink="false">2502.19892v1</guid>
      <pubDate>Fri, 28 Feb 2025 18:19:45 +0800</pubDate>
    </item>
    <item>
      <title>Image Translation-Based Unsupervised Cross-Modality Domain Adaptation for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2502.15193v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure. arXiv admin note: substantial text overlap with  arXiv:2303.07674&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于图像转换的无监督跨模态域适应方法，该方法能够将带有标注的源模态图像转化为未标注的目标模态，并利用这些伪标签进行目标模态的学习。&lt;h4&gt;背景&lt;/h4&gt;在医学影像中，由于医生专业知识的需求，注解过程更加耗时且昂贵。同时，不同医疗机构获取的医疗影像可能因为不同的扫描设备和成像协议而具有不一致的模态特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种可以应对跨模态差异（域偏移）问题并提高深度学习模型性能的方法。&lt;h4&gt;方法&lt;/h4&gt;通过图像转换技术将源模态中的标注数据转化为目标模态，并结合自训练技术克服生成伪标签与真实图像之间的细微差别，以进一步提升任务的执行能力。&lt;h4&gt;主要发现&lt;/h4&gt;在跨模态域适应（crossMoDA 2022）挑战赛验证阶段排行榜上，对于前庭神经鞘瘤和耳蜗分割任务，提出的模型分别达到了Dice相似性系数(DSC)和平均对称表面距离(ASSD)为：VS肿瘤0.8351 ± 0.1152 和1.6712 ± 2.1948；耳蜗0.8098 ± 0.0233和0.2317 ± 0.1577。&lt;h4&gt;结论&lt;/h4&gt;所提出的无监督跨模态域适应方法能有效地解决医疗影像中的跨模态问题，提供了一种提高深度学习模型在医学图像处理中表现的新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised deep learning usually faces more challenges in medical images thanin natural images. Since annotations in medical images require the expertise ofdoctors and are more time-consuming and expensive. Thus, some researchers turnto unsupervised learning methods, which usually face inevitable performancedrops. In addition, medical images may have been acquired at different medicalcenters with different scanners and under different image acquisitionprotocols, so the modalities of the medical images are often inconsistent. Thismodality difference (domain shift) also reduces the applicability of deeplearning methods. In this regard, we propose an unsupervised crossmodalitydomain adaptation method based on image translation by transforming the sourcemodality image with annotation into the unannotated target modality and usingits annotation to achieve supervised learning of the target modality. Inaddition, the subtle differences between translated pseudo images and realimages are overcome by self-training methods to further improve the taskperformance of deep learning. The proposed method showed mean Dice SimilarityCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \pm0.1152$ and $1.6712 \pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \pm0.0233$ and $0.2317 \pm 0.1577$ for cochlea on the VS and cochlea segmentationtask of the Cross-Modality Domain Adaptation (crossMoDA 2022) challengevalidation phase leaderboard.</description>
      <author>example@mail.com (Tao Yang, Lisheng Wang)</author>
      <guid isPermaLink="false">2502.15193v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
  <item>
      <title>Fréchet Cumulative Covariance Net for Deep Nonlinear Sufficient Dimension Reduction with Random Objects</title>
      <link>http://arxiv.org/abs/2502.15374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的统计依赖度量——Fréchet累积协方差（FCCov），并基于此发展了一种新的非线性充分降维框架，适用于复杂的非欧几里得数据，并具有抗异常值的能力。&lt;h4&gt;背景&lt;/h4&gt;现有大多数方法在处理复杂非欧几里得响应变量时不再适用，而这类数据在许多现代统计应用中频繁出现。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的非线性充分降维框架，以解决复杂非欧几里得数据的问题，并提高模型的鲁棒性和实用性。&lt;h4&gt;方法&lt;/h4&gt;引入了Fréchet累积协方差（FCCov）作为依赖度量，并结合前馈神经网络（FNNs）和卷积神经网络（CNNs）来估计样本层面的非线性充分方向。同时，证明了带有平方弗罗贝尼乌斯范数正则化的模型在σ-域上的无偏性。&lt;h4&gt;主要发现&lt;/h4&gt;理论结果表明该方法达到了最优的收敛速度，并通过大量的模拟研究验证了其在欧几里得和非欧几里得设置下的性能。实际应用中，该方法在面部表情识别数据集上表现良好。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法不仅对复杂的非欧几里得数据具有广泛的应用性，而且展示出了比现有方法更强的鲁棒性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;非线性充分降维构建了高维数据的非线性低维表示，以概括其核心特征。然而，当响应变量是常见的复杂非欧几里得随机对象时，大多数现有的方法不再适用。本文引入了一种新的统计依赖度量——Fréchet累积协方差（FCCov），并基于此发展了一个新的非线性充分降维框架，并结合了前馈神经网络和卷积神经网络来估计样本级别的非线性充分方向。理论证明表明，带有平方弗罗贝尼乌斯范数正则化的模型在σ-域上是无偏的。此外，建立了基于FNNs和ResNet型CNNs的估计器的非渐近收敛率，这些匹配了非参数回归的最大最小速率（忽略对数因子）。大量的模拟研究验证了所提方法在欧几里得及非欧几里得设置下的性能，并通过面部表情识别数据集的应用证明了其实际有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Nonlinear sufficient dimension reduction\citep{libing_generalSDR}, whichconstructs nonlinear low-dimensional representations to summarize essentialfeatures of high-dimensional data, is an important branch of representationlearning. However, most existing methods are not applicable when the responsevariables are complex non-Euclidean random objects, which are frequentlyencountered in many recent statistical applications. In this paper, weintroduce a new statistical dependence measure termed Fr\'echet CumulativeCovariance (FCCov) and develop a novel nonlinear SDR framework based on FCCov.Our approach is not only applicable to complex non-Euclidean data, but alsoexhibits robustness against outliers. We further incorporate Feedforward NeuralNetworks (FNNs) and Convolutional Neural Networks (CNNs) to estimate nonlinearsufficient directions in the sample level. Theoretically, we prove that ourmethod with squared Frobenius norm regularization achieves unbiasedness at the$\sigma$-field level. Furthermore, we establish non-asymptotic convergencerates for our estimators based on FNNs and ResNet-type CNNs, which match theminimax rate of nonparametric regression up to logarithmic factors. Intensivesimulation studies verify the performance of our methods in both Euclidean andnon-Euclidean settings. We apply our method to facial expression recognitiondatasets and the results underscore more realistic and broader applicability ofour proposal.</description>
      <author>example@mail.com (Hang Yuan, Christina Dan Wang, Zhou Yu)</author>
      <guid isPermaLink="false">2502.15374v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PointSea: Point Cloud Completion via Self-structure Augmentation</title>
      <link>http://arxiv.org/abs/2502.17053v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Computer Vision (IJCV). This  work is a journal extension of our ICCV 2023 paper arXiv:2307.08492. arXiv  admin note: text overlap with arXiv:2307.08492&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了PointSea，一种用于全局到局部点云完成的方法。通过引入自结构增强和利用多视角自我投影深度图来改进数据表示。&lt;h4&gt;背景&lt;/h4&gt;点云补全是3D视觉中的基础但尚未完全解决的问题。当前方法依赖于3D坐标信息或额外的数据（如图像和扫描视点）来填充缺失部分。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于自结构增强的全局到局部点云完成的新方法，以更好地理解和生成不完整输入中的细节。&lt;h4&gt;方法&lt;/h4&gt;{'全局阶段': '使用多视角自我投影深度图增强数据表示，并通过跨模态输入重构紧凑的全球形状。引入特征融合模块，在视内和视间层次上融合特征。', '局部阶段': '提出一种名为自结构对偶生成器的点生成器，该生成器结合了学习到的形状先验和几何自相似性进行形状细化，并根据每个点的结构性质适应不同的细化策略。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明PointSea能够有效理解全局形状并从不完整输入中产生局部细节，相对于现有方法有明显改进。&lt;h4&gt;结论&lt;/h4&gt;通过引入自我结构增强和利用多视角数据表示来提升点云补全的效果，并在多个基准测试上展示了其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is a fundamental yet not well-solved problem in 3Dvision. Current approaches often rely on 3D coordinate information and/oradditional data (e.g., images and scanning viewpoints) to fill in missingparts. Unlike these methods, we explore self-structure augmentation and proposePointSea for global-to-local point cloud completion. In the global stage,consider how we inspect a defective region of a physical object, we may observeit from various perspectives for a better understanding. Inspired by this,PointSea augments data representation by leveraging self-projected depth imagesfrom multiple views. To reconstruct a compact global shape from the cross-modalinput, we incorporate a feature fusion module to fuse features at bothintra-view and inter-view levels. In the local stage, to reveal highly detailedstructures, we introduce a point generator called the self-structuredual-generator. This generator integrates both learned shape priors andgeometric self-similarities for shape refinement. Unlike existing efforts thatapply a unified strategy for all points, our dual-path design adapts refinementstrategies conditioned on the structural type of each point, addressing thespecific incompleteness of each point. Comprehensive experiments on widely-usedbenchmarks demonstrate that PointSea effectively understands global shapes andgenerates local details from incomplete input, showing clear improvements overexisting methods.</description>
      <author>example@mail.com (Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.17053v2</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Armada: Memory-Efficient Distributed Training of Large-Scale Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了在亿级规模图数据集上进行分布式训练的Graph Neural Networks（GNNs）的方法，提出了一个新的分布式系统Armada和一种新的最小边切割划分算法GREM。&lt;h4&gt;背景&lt;/h4&gt;现有的最优离线方法（例如METIS）虽然效果好但是对内存消耗巨大并且运行时间长；而计算效率较高的贪心流式分区方法在减少跨机通信方面表现不佳。&lt;h4&gt;目的&lt;/h4&gt;设计一个能够高效处理大规模图数据集的分布式训练系统，以优化GNN的训练过程。&lt;h4&gt;方法&lt;/h4&gt;引入了Armada系统及其核心组件GREM算法。GREM基于改进的流式贪心算法，在执行过程中不断优化顶点分配策略而非一次性冻结选择结果。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析和实验表明，相比于传统方法（例如METIS），GREM能够在内存消耗和运行时间上减少8到65倍，并且在切割边数上达到与之相仿的水平。另外，在进行分布式训练时，Armada通过分散式架构进一步提高了效率。&lt;h4&gt;结论&lt;/h4&gt;使用分散式架构可以显著提高GNN模型在大规模图数据集上的训练性能和成本效益。&lt;h4&gt;翻译&lt;/h4&gt;我们研究了在分布在多台机器上划分的大规模图形（即百亿级）中进行Graph Neural Networks (GNNs)分布式训练的有效方法。高效训练需要利用最小边切割分区算法来减少由于GNN邻居采样导致的跨机通信需求，但是对大图进行有效分割仍然是一个挑战：最先进的离线方法(例如METIS)，虽然效果好但它们需要比GNN训练本身多几倍到几十倍的内存和运行时间；计算效率较高的贪心流式分区算法则面临增加边切割量的问题。为此，在这项工作中我们引入了Armada，一个新的用于分布式GNN训练的端到端系统，其关键贡献是GREM，一种新的最小边切割划分算法，它能够有效处理大规模图形数据集。GREM在现有的流式贪心算法基础上加入了一项重要改进：在执行过程中对先前顶点分配进行持续优化而非冻结初始贪婪选择的结果。我们的理论分析和实验结果表明这种优化对于减少边切割至关重要，并使GREM能够在内存消耗和运行时间上少8到65倍的同时，达到与METIS相近的分区质量。给定一个已分割图，Armada通过新的分散架构进一步提升了分布式GNN训练效率；我们在普通的云机器中发现，在没有额外通信的情况下，GNN邻居采样以及特征加载已成为训练中的瓶颈。分散式架构使得Armada能够独立分配这些操作所需的资源，并确保昂贵的GPU始终保持饱和运算状态。我们评估了Armada相对于当前最优秀的分布式GNN训练系统的表现，发现分散式的架构带来了运行时间提高高达4.5倍和成本降低高达3.1倍的效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We study distributed training of Graph Neural Networks (GNNs) onbillion-scale graphs that are partitioned across machines. Efficient trainingin this setting relies on min-edge-cut partitioning algorithms, which minimizecross-machine communication due to GNN neighborhood sampling. Yet, min-edge-cutpartitioning over large graphs remains a challenge: State-of-the-art (SoTA)offline methods (e.g., METIS) are effective, but they require orders ofmagnitude more memory and runtime than GNN training itself, whilecomputationally efficient algorithms (e.g., streaming greedy approaches) sufferfrom increased edge cuts. Thus, in this work we introduce Armada, a newend-to-end system for distributed GNN training whose key contribution is GREM,a novel min-edge-cut partitioning algorithm that can efficiently scale to largegraphs. GREM builds on streaming greedy approaches with one key addition: priorvertex assignments are continuously refined during streaming, rather thanfrozen after an initial greedy selection. Our theoretical analysis andexperimental results show that this refinement is critical to minimizing edgecuts and enables GREM to reach partition quality comparable to METIS but with8-65x less memory and 8-46x faster. Given a partitioned graph, Armada leveragesa new disaggregated architecture for distributed GNN training to furtherimprove efficiency; we find that on common cloud machines, even with zerocommunication, GNN neighborhood sampling and feature loading bottlenecktraining. Disaggregation allows Armada to independently allocate resources forthese operations and ensure that expensive GPUs remain saturated withcomputation. We evaluate Armada against SoTA systems for distributed GNNtraining and find that the disaggregated architecture leads to runtimeimprovements up to 4.5x and cost reductions up to 3.1x.</description>
      <author>example@mail.com (Roger Waleffe, Devesh Sarda, Jason Mohoney, Emmanouil-Vasileios Vlatakis-Gkaragkounis, Theodoros Rekatsinas, Shivaram Venkataraman)</author>
      <guid isPermaLink="false">2502.17846v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Retrieval Dexterity: Efficient Object Retrieval in Clutters with Dexterous Hand</title>
      <link>http://arxiv.org/abs/2502.18423v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种灵巧的臂手系统，用于在多物体堆叠环境中高效地检索被遮挡的目标物体。该方法通过大规模并行强化学习训练策略，在复杂环境设计中展现出了高效的清除障碍物能力。&lt;h4&gt;背景&lt;/h4&gt;在多个物体堆积的情况下检索目标物体既具有挑战性又耗时，现有的方法通常通过逐一抓取和移除遮挡的物体来解决问题，这导致了执行时间长并且需要极高的抓取技能要求。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的灵巧机械臂系统，能够高效地在复杂堆叠环境中清除障碍物以检索目标物体。&lt;h4&gt;方法&lt;/h4&gt;采用大规模并行强化学习技术，在多样化设计的拥挤场景中训练策略。这些策略发展出如推、搅拌和戳等技能，可以有效地暴露目标物体的足够表面。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证了该系统在多样化的居家物品混乱配置下的高效性能，并成功地将学到的策略转移到真实的灵巧多指机器人上，展示了其实际应用的可能性。&lt;h4&gt;结论&lt;/h4&gt;研究证明所提出的臂手系统可以有效解决复杂堆叠环境中目标物体检索的问题，并且能够在现实世界中实现。&lt;h4&gt;翻译&lt;/h4&gt;提取埋藏在多个物体下的对象不仅具有挑战性而且耗时。在这种环境下执行操作会因为复杂的接触关系而困难重重。现有方法通常通过逐一抓取并移除每个遮挡物来解决这个问题，这导致了长时间的执行时间和对每一个遮挡物不切实际的抓取能力需求。在本文中，我们提出了一种灵巧的臂手系统用于多物体堆叠环境中的高效对象检索。我们的方法利用大规模平行强化学习在多样化设计的混乱环境中训练策略。这些策略展现了如推、搅拌和戳等出现的操作技能，能够有效地清除遮挡物以暴露目标物体的足够表面区域。我们在一套超过10种家用物品在不同杂乱配置下进行了广泛的评估，展示了对已训练和未见过对象都具备优异的检索性能和效率。此外，我们将学习到的策略成功地转移到了真实世界中的灵巧多指机器人系统中，验证了它们的实际应用性。视频可以在我们的项目网站上找到：https://ChangWinde.github.io/RetrDex。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving objects buried beneath multiple objects is not only challengingbut also time-consuming. Performing manipulation in such environments presentssignificant difficulty due to complex contact relationships. Existing methodstypically address this task by sequentially grasping and removing eachoccluding object, resulting in lengthy execution times and requiringimpractical grasping capabilities for every occluding object. In this paper, wepresent a dexterous arm-hand system for efficient object retrieval inmulti-object stacked environments. Our approach leverages large-scale parallelreinforcement learning within diverse and carefully designed clutteredenvironments to train policies. These policies demonstrate emergentmanipulation skills (e.g., pushing, stirring, and poking) that efficientlyclear occluding objects to expose sufficient surface area of the target object.We conduct extensive evaluations across a set of over 10 household objects indiverse clutter configurations, demonstrating superior retrieval performanceand efficiency for both trained and unseen objects. Furthermore, wesuccessfully transfer the learned policies to a real-world dexterousmulti-fingered robot system, validating their practical applicability inreal-world scenarios. Videos can be found on our project websitehttps://ChangWinde.github.io/RetrDex.</description>
      <author>example@mail.com (Fengshuo Bai, Yu Li, Jie Chu, Tawei Chou, Runchuan Zhu, Ying Wen, Yaodong Yang, Yuanpei Chen)</author>
      <guid isPermaLink="false">2502.18423v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>From planning to policy: distilling $\texttt{Skill-RRT}$ for long-horizon prehensile and non-prehensile manipulation</title>
      <link>http://arxiv.org/abs/2502.18015v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website:  $\href{https://sites.google.com/view/skill-rrt}{\text{sites.google.com/view/skill-rrt}}$&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种框架，通过模仿学习将规划算法转化为策略以解决长时序操作任务中的复杂技能串联问题。&lt;h4&gt;背景&lt;/h4&gt;当前机器人在需要一系列灵巧和非灵巧抓取技巧的长时间序列操作任务中面临挑战。这包括处理复杂的接触互动和考虑多个技能长期影响的任务串连。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，将长时序规划算法转化为高效的操作策略，并通过生成高质量演示数据来优化策略性能。&lt;h4&gt;方法&lt;/h4&gt;{'Skill-RRT': '该框架引入了$\texttt{Skill-RRT}$，这是一种快速搜索随机树（RRT）的扩展版本，加入了技能适用性检查和中间对象姿态采样，以实现高效的长时序规划。', 'connectors': '提出$\\it{connectors}$概念，即基于目标条件策略，用于在技能之间过渡的同时尽量减少物体扰动。通过懒惰规划（lazy planning），这些connectors仅在相关转移上训练，从而降低成本。', '噪声重播机制': '利用$\texttt{Skill-RRT}$生成高质量的演示数据，并通过噪声基础重播机制进一步优化策略性能。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'策略性能': '所提出的策略能够在完全模拟环境中进行训练，并直接应用于现实世界，成功率超过80%，涵盖三个具有挑战性的操作任务。', '比较优势': '在仿真环境中的表现优于现有的基于技能的强化学习方法$\texttt{MAPLE}$和$\texttt{Skill-RRT}$。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入$exttt{Skill-RRT}$以及$\it{connectors}$，本文提供了一种有效的方法来解决机器人长时序操作任务中的复杂技能问题。&lt;h4&gt;翻译&lt;/h4&gt;当前的机器人在执行需要一系列灵巧和非灵巧抓取技巧的操作任务中遇到挑战，这些任务涉及处理复杂的接触互动及考虑多个技能长期影响的任务串连。为了应对这些问题，该研究提出了一种框架，通过模仿学习将一个能够解决长时间序列问题但计算时间消耗大的规划算法转化为策略，从而实现高效的动作推断。本文介绍了$exttt{Skill-RRT}$方法，这是快速搜索随机树（RRT）的扩展版，加入技能适用性检查和中间对象姿态采样，以促进高效的长时序规划。此外，为了使技能能够串联起来，研究还提出了$\it{connectors}$概念——目标条件策略，用于在技能之间转换的同时尽量减少物体扰动。通过懒惰规划技术，这些连接器仅选择性地训练于相关的过渡场景上，减少了训练成本。高质量的演示数据由$exttt{Skill-RRT}$生成，并且经过噪声基础重播机制优化以确保政策性能的鲁棒性。最终策略在完全模拟环境中进行训练后，在真实世界中直接转移应用并取得超过80%的成功率，涵盖了三个具有挑战性的操作任务。在仿真环境中的表现也优于现有的基于技能的强化学习方法$exttt{MAPLE}$和$exttt{Skill-RRT}$。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current robots face challenges in manipulation tasks that require a longsequence of prehensile and non-prehensile skills. This involves handlingcontact-rich interactions and chaining multiple skills while considering theirlong-term consequences. This paper presents a framework that leveragesimitation learning to distill a planning algorithm, capable of solvinglong-horizon problems but requiring extensive computation time, into a policyfor efficient action inference. We introduce $\texttt{Skill-RRT}$, an extensionof the rapidly-exploring random tree (RRT) that incorporates skillapplicability checks and intermediate object pose sampling for efficientlong-horizon planning. To enable skill chaining, we propose$\textit{connectors}$, goal-conditioned policies that transition between skillswhile minimizing object disturbance. Using lazy planning, connectors areselectively trained on relevant transitions, reducing the cost of training.High-quality demonstrations are generated with $\texttt{Skill-RRT}$ and refinedby a noise-based replay mechanism to ensure robust policy performance. Thedistilled policy, trained entirely in simulation, zero-shot transfer to thereal world, and achieves over 80% success rates across three challengingmanipulation tasks. In simulation, our approach outperforms thestate-of-the-art skill-based reinforcement learning method, $\texttt{MAPLE}$,and $\texttt{Skill-RRT}$.</description>
      <author>example@mail.com (Haewon Jung, Donguk Lee, Haecheol Park, JunHyeop Kim, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.18015v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning-Based Transfer Learning for Classification of Cassava Disease</title>
      <link>http://arxiv.org/abs/2502.19351v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, in Portuguese language, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文对比了四种卷积神经网络架构（EfficientNet-B3、InceptionV3、ResNet50 和 VGG16）在分类木薯病害图像上的性能。&lt;h4&gt;背景&lt;/h4&gt;研究使用的数据集来自一场竞赛中的不平衡图像数据集。&lt;h4&gt;目的&lt;/h4&gt;比较不同CNN架构对于识别木薯疾病图像的效果，特别是针对不平衡数据集的挑战提出解决方案。&lt;h4&gt;方法&lt;/h4&gt;使用了适当的度量标准来解决类别不平衡的问题，并评估了四种模型在分类任务上的性能指标（准确率、精确率、召回率和F1分数）。&lt;h4&gt;主要发现&lt;/h4&gt;EfficientNet-B3 在这项任务中表现出最好的结果，其准确率为87.7%，精确率为87.8%，召回率为87.8%，F1-Score为87.7%。&lt;h4&gt;结论&lt;/h4&gt;研究认为EfficientNet-B3可以作为一个有价值的工具来支持数字农业的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了该论文通过评估四种卷积神经网络架构（包括EfficientNet-B3、InceptionV3、ResNet50和VGG16）在处理一个不平衡的木薯病害图像数据集上的分类性能，发现EfficientNet-B3表现最佳，并建议这种模型可以用于支持数字农业领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a performance comparison among four Convolutional NeuralNetwork architectures (EfficientNet-B3, InceptionV3, ResNet50, and VGG16) forclassifying cassava disease images. The images were sourced from an imbalanceddataset from a competition. Appropriate metrics were employed to address classimbalance. The results indicate that EfficientNet-B3 achieved on this taskaccuracy of 87.7%, precision of 87.8%, revocation of 87.8% and F1-Score of87.7%. These findings suggest that EfficientNet-B3 could be a valuable tool tosupport Digital Agriculture.</description>
      <author>example@mail.com (Ademir G. Costa Junior, Fábio S. da Silva, Ricardo Rios)</author>
      <guid isPermaLink="false">2502.19351v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>LiDAR Registration with Visual Foundation Models</title>
      <link>http://arxiv.org/abs/2502.19374v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种使用DINOv2特征作为点描述符的方法，用于解决基于激光雷达的机器人地图注册和定位中的关键问题。&lt;h4&gt;背景&lt;/h4&gt;激光雷达数据配准在机器人制图与定位中是一项基本任务。通过识别稳健的点对来实现两个点云之间的对齐是至关重要的步骤，在领域差异、季节变化及点云结构变化的情况下尤为困难。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决上述问题，提出了一种使用DINOv2特征作为点描述符的方法，并结合传统配准算法进行激光雷达扫描和3D地图的稳健6DoF对齐。&lt;h4&gt;方法&lt;/h4&gt;通过将从环绕视图图像中获得的DINOv2特征用作点描述符，这种方法可以克服传统的基于手工设计及学习的方法在面对领域差异时的局限性。同时与RANSAC或ICP等传统配准算法相结合以实现稳健对齐。&lt;h4&gt;主要发现&lt;/h4&gt;利用额外的相机数据使该方法能够在NCLT和Oxford RobotCar数据集上超越最复杂的基线技术，分别提高了24.8%和17.3%的注册召回率。&lt;h4&gt;结论&lt;/h4&gt;本文的方法不需要领域特定重新训练，并且对点云结构无敏感性，能够处理稀疏激光雷达扫描和密集3D地图。此外，该方法在概念上虽然简单但效果显著优于更复杂的基线技术。&lt;h4&gt;翻译&lt;/h4&gt;LiDAR registration is a fundamental task in robotic mapping and localization. A critical component of aligning two point clouds is identifying robust point correspondences using point descriptors. This step becomes particularly challenging in scenarios involving domain shifts, seasonal changes, and variations in point cloud structures. These factors substantially impact both handcrafted and learning-based approaches. In this paper, we address these problems by proposing to use DINOv2 features, obtained from surround-view images, as point descriptors. We demonstrate that coupling these descriptors with traditional registration algorithms, such as RANSAC or ICP, facilitates robust 6DoF alignment of LiDAR scans with 3D maps, even when the map was recorded more than a year before. Although conceptually straightforward, our method substantially outperforms more complex baseline techniques. In contrast to previous learning-based point descriptors, our method does not require domain-specific retraining and is agnostic to the point cloud structure, effectively handling both sparse LiDAR scans and dense 3D maps. We show that leveraging the additional camera data enables our method to outperform the best baseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCar datasets. We publicly release the registration benchmark and the code of our work on https://vfm-registration.cs.uni-freiburg.de.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; LiDAR registration is a fundamental task in robotic mapping and localization.A critical component of aligning two point clouds is identifying robust pointcorrespondences using point descriptors. This step becomes particularlychallenging in scenarios involving domain shifts, seasonal changes, andvariations in point cloud structures. These factors substantially impact bothhandcrafted and learning-based approaches. In this paper, we address theseproblems by proposing to use DINOv2 features, obtained from surround-viewimages, as point descriptors. We demonstrate that coupling these descriptorswith traditional registration algorithms, such as RANSAC or ICP, facilitatesrobust 6DoF alignment of LiDAR scans with 3D maps, even when the map wasrecorded more than a year before. Although conceptually straightforward, ourmethod substantially outperforms more complex baseline techniques. In contrastto previous learning-based point descriptors, our method does not requiredomain-specific retraining and is agnostic to the point cloud structure,effectively handling both sparse LiDAR scans and dense 3D maps. We show thatleveraging the additional camera data enables our method to outperform the bestbaseline by +24.8 and +17.3 registration recall on the NCLT and Oxford RobotCardatasets. We publicly release the registration benchmark and the code of ourwork on https://vfm-registration.cs.uni-freiburg.de.</description>
      <author>example@mail.com (Niclas Vödisch, Giovanni Cioffi, Marco Cannici, Wolfram Burgard, Davide Scaramuzza)</author>
      <guid isPermaLink="false">2502.19374v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Preference-Based Gradient Estimation for ML-Based Approximate Combinatorial Optimization</title>
      <link>http://arxiv.org/abs/2502.19377v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preliminary work, under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于数据驱动的方法，用于改进现有的非学习近似算法，以解决组合优化问题。方法是通过参数化近似算法并利用图神经网络预测最优参数值来实现。&lt;h4&gt;背景&lt;/h4&gt;组合优化问题广泛存在于医学、物流和制造业等领域中。许多应用场景要求快速找到高质量的解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种数据驱动的方法，结合神经网络和非学习近似算法的优点，以提高组合优化问题解的质量。&lt;h4&gt;方法&lt;/h4&gt;采用图神经网络预测能产生最优解的参数值，并通过自监督的方式进行端到端训练。同时提出了基于偏好梯度估计的新方案。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在旅行商问题和最小k切割问题上的表现与最新的学习组合优化求解器具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该方法能够有效地利用图神经网络的信息帮助近似算法找到更好的解决方案，同时也保证了解的可行性。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到：组合优化（CO）问题出现在从医学到物流和制造业等多个领域。虽然精确解决这些问题是不必要的，但许多应用需要快速找到高质量的解。为实现这一目标，我们提出了一种数据驱动的方法来改进现有的非学习近似算法。我们将近似算法参数化，并训练图神经网络预测能够产生最佳可能解的参数值。我们的管道在自监督环境下以端到端的方式进行梯度估计训练，将近似算法视为黑盒系统。为实现这一目标，我们提出了一种新颖的基于偏好的梯度估计方案。该方法结合了神经网络和非学习近似算法的优势：图神经网络利用数据集中的信息帮助近似算法找到更好的解，而近似算法则确保了解的可行性。我们在旅行商问题和最小k切割问题上验证了我们的方法，并表明与最新的学习组合优化求解器相比具有竞争力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Combinatorial optimization (CO) problems arise in a wide range of fields frommedicine to logistics and manufacturing. While exact solutions are often notnecessary, many applications require finding high-quality solutions quickly.For this purpose, we propose a data-driven approach to improve existingnon-learned approximation algorithms for CO. We parameterize the approximationalgorithm and train a graph neural network (GNN) to predict parameter valuesthat lead to the best possible solutions. Our pipeline is trained end-to-end ina self-supervised fashion using gradient estimation, treating the approximationalgorithm as a black box. We propose a novel gradient estimation scheme forthis purpose, which we call preference-based gradient estimation. Our approachcombines the benefits of the neural network and the non-learned approximationalgorithm: The GNN leverages the information from the dataset to allow theapproximation algorithm to find better solutions, while the approximationalgorithm guarantees that the solution is feasible. We validate our approach ontwo well-known combinatorial optimization problems, the travelling salesmanproblem and the minimum k-cut problem, and show that our method is competitivewith state of the art learned CO solvers.</description>
      <author>example@mail.com (Arman Mielke, Uwe Bauknecht, Thilo Strauss, Mathias Niepert)</author>
      <guid isPermaLink="false">2502.19377v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>FSPO: Few-Shot Preference Optimization of Synthetic Preference Data in LLMs Elicits Effective Personalization to Real Users</title>
      <link>http://arxiv.org/abs/2502.19312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website: https://fewshot-preference-optimization.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了Few-Shot Preference Optimization (FSPO) 方法，通过少量用户偏好标签，利用大型语言模型（LLM）的在上下文学习能力快速适应用户需求。&lt;h4&gt;背景&lt;/h4&gt;有效的个人化对虚拟助手和内容推荐等应用至关重要。然而，收集真实世界的用户偏好数据既困难又耗时。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以使用少量真实的用户偏好信息并在合成数据集上进行训练的方法，以实现实用的个性化功能。&lt;h4&gt;方法&lt;/h4&gt;提出FSPO框架，通过构建合成偏好数据集来解决实际偏好的收集难题，并采用公开可用的大规模语言模型生成100万以上的个人化偏好标签。&lt;h4&gt;主要发现&lt;/h4&gt;为了有效利用合成数据进行真实用户个性化的迁移学习，数据需要具有高度多样性和一致性的结构。经过电影评论、基于教育背景的适应性教学以及通用问题解答等三个领域的测试，FSPO在个性化开放生成方面表现出色，平均胜率为87%（对合成用户）和72%（对真实人类用户）。&lt;h4&gt;结论&lt;/h4&gt;FSPO成功地将大型语言模型的在上下文学习能力应用于个人化需求优化，为开发更智能、更具个性化的应用提供了新的方向。&lt;h4&gt;翻译&lt;/h4&gt;有效的个性化对于各种需要与用户交互的应用程序来说至关重要。受大型语言模型（LLM）的强大上下文学习能力启发，我们提出了Few-Shot Preference Optimization (FSPO) 方法，将奖励建模重新定义为元学习问题，在此框架下，通过少量用户的偏好标签，LM可以快速适应个人需求，并构建个性化的奖励函数。考虑到实际偏好的数据难以大规模收集，我们设计了合成的偏好数据集来实现个性化，利用公开可用的大规模语言模型生成超过100万个个性化偏好标签。为了从合成数据成功转移到真实用户上，发现需要确保数据具有高度多样性和一致性的结构。我们在三个领域进行了评估：电影评论、基于教育背景的教学适应和通用问题回答，并进行了一项受控的人类研究。总的来说，在针对最多1500个虚拟用户的个性化开放生成中，FSPO实现了平均87%的Alpaca Eval胜率（对合成用户）以及在开放性问题解答中对真实人类用户的平均72%胜率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective personalization of LLMs is critical for a broad range ofuser-interfacing applications such as virtual assistants and content curation.Inspired by the strong in-context learning capabilities of LLMs, we proposeFew-Shot Preference Optimization (FSPO), which reframes reward modeling as ameta-learning problem. Under this framework, an LLM learns to quickly adapt toa user via a few labeled preferences from that user, constructing apersonalized reward function for them. Additionally, since real-worldpreference data is scarce and challenging to collect at scale, we proposecareful design choices to construct synthetic preference datasets forpersonalization, generating over 1M synthetic personalized preferences usingpublicly available LLMs. In particular, to successfully transfer from syntheticdata to real users, we find it crucial for the data to exhibit both highdiversity and coherent, self-consistent structure. We evaluate FSPO onpersonalized open-ended generation for up to 1,500 synthetic users acrossacross three domains: movie reviews, pedagogical adaptation based oneducational background, and general question answering, along with a controlledhuman study. Overall, FSPO achieves an 87% Alpaca Eval winrate on average ingenerating responses that are personalized to synthetic users and a 72% winratewith real human users in open-ended question answering.</description>
      <author>example@mail.com (Anikait Singh, Sheryl Hsu, Kyle Hsu, Eric Mitchell, Stefano Ermon, Tatsunori Hashimoto, Archit Sharma, Chelsea Finn)</author>
      <guid isPermaLink="false">2502.19312v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments</title>
      <link>http://arxiv.org/abs/2502.15342v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的行人关注的多模态场景数据集PFSD，并提出了一种用于复杂半结构化环境中的三维行人检测的新方法HMFN。&lt;h4&gt;背景&lt;/h4&gt;在有大量车辆交通的结构化环境中，自动驾驶感知技术表现出色。然而，在动态行人占据主导地位的半结构化环境中，当前感知模型表现不佳，主要原因在于高质量数据集的缺乏。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的多模态场景数据集PFSD，以及解决复杂半结构环境挑战的新方法HMFN。&lt;h4&gt;方法&lt;/h4&gt;创建了一个名为PFSD的数据集，该数据集中包括超过130,000个行人实例的详细标注。此外，还设计了一种混合多尺度融合网络（HMFN），通过结合稀疏和普通卷积来有效地捕捉并融合不同规模的特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有方法相比，HMFN在PFSD数据集上的平均精度均值(mAP)有所提高，证明了该方法在处理复杂半结构化环境中的三维行人检测任务的有效性。&lt;h4&gt;结论&lt;/h4&gt;通过提供高质量的数据集和创新的算法模型，本文为改进自动驾驶系统中行人感知技术提供了重要的基础研究。&lt;h4&gt;翻译&lt;/h4&gt;最近，在以车辆交通为主的结构性环境中，自主驾驶感知技术已经展示了卓越的能力。然而，目前的感知模型在半结构化环境中的表现较差，这些环境下动态行人的多样性和不规则运动更加普遍，遮挡情况也更为复杂。我们认为这种情况的原因在于高质量数据集的缺乏，尤其是在涉及行人感知和预测方面的不足。在这项工作中，我们提出了一个多模态的行人场景数据集（PFSD），它在半结构化环境中进行了严格的nuScenes格式标注，并提供了点云分割、检测和对象ID以进行跟踪。该数据集涵盖了各种不同密度、运动模式以及遮挡情况下的超过130,000个行人实例的数据。此外，为了展示应对更加多样且复杂的半结构环境挑战的重要性，我们提出了一种新的混合多尺度融合网络（HMFN）。具体而言，为了在人口密集和存在大量遮挡的情况下检测行人，我们的方法能够有效地捕捉并融合不同规模的特征，通过精心设计的混合框架整合了稀疏卷积与普通卷积。对PFSD进行广泛实验表明，HMFN相较于现有技术提高了平均精度均值（mAP），从而证明其在处理复杂半结构化环境中的三维行人检测问题上的有效性。代码和基准测试可供获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in autonomous driving perception have revealedexceptional capabilities within structured environments dominated by vehiculartraffic. However, current perception models exhibit significant limitations insemi-structured environments, where dynamic pedestrians with more diverseirregular movement and occlusion prevail. We attribute this shortcoming to thescarcity of high-quality datasets in semi-structured scenes, particularlyconcerning pedestrian perception and prediction. In this work, we present themulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated insemi-structured scenes with the format of nuScenes. PFSD provides comprehensivemulti-modal data annotations with point cloud segmentation, detection, andobject IDs for tracking. It encompasses over 130,000 pedestrian instancescaptured across various scenarios with varying densities, movement patterns,and occlusions. Furthermore, to demonstrate the importance of addressing thechallenges posed by more diverse and complex semi-structured environments, wepropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, todetect pedestrians in densely populated and occluded scenarios, our methodeffectively captures and fuses multi-scale features using a meticulouslydesigned hybrid framework that integrates sparse and vanilla convolutions.Extensive experiments on PFSD demonstrate that HMFN attains improvement in meanAverage Precision (mAP) over existing methods, thereby underscoring itsefficacy in addressing the challenges of 3D pedestrian detection in complexsemi-structured environments. Coding and benchmark are available.</description>
      <author>example@mail.com (Yueting Liu, Hanshi Wang, Zhengjun Zha, Weiming Hu, Jin Gao)</author>
      <guid isPermaLink="false">2502.15342v3</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>BEV-LIO(LC): BEV Image Assisted LiDAR-Inertial Odometry with Loop Closure</title>
      <link>http://arxiv.org/abs/2502.19242v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种结合了鸟瞰图（BEV）图像表示和几何点云配准的新型激光雷达惯性里程计框架，通过引入循环闭合检测来提高定位一致性。&lt;h4&gt;背景&lt;/h4&gt;现有的激光雷达惯性里程计方法在处理高密度点云时效率低下且无法有效利用特征进行循环闭合检测。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的激光雷达惯性里程计框架BEV-LIO(LC)，旨在通过鸟瞰图表示和几何点云配准，提高定位精度并实现高效的循环闭合检测。&lt;h4&gt;方法&lt;/h4&gt;{'点密度归一化': '将激光雷达点云投影到BEV图像上以提取特征', '轻量级CNN特征提取器': '用于从BEV图像中抽取局部和全局描述符', '重投影误差最小化': '与平面到点配准结合，集成于迭代扩展卡尔曼滤波器（iEKF）内', '循环闭合检测': '使用全局描述符建立KD-tree索引的关键帧数据库，并通过RANSAC提供粗略变换估计用于ICP'}&lt;h4&gt;主要发现&lt;/h4&gt;BEV-LIO(LC)在各种场景和不同类型的LiDAR中表现出优越的性能，实现了竞争性的定位精度。&lt;h4&gt;结论&lt;/h4&gt;该方法提高了激光雷达惯性里程计系统的效率和准确性，并通过引入高效的循环闭合检测进一步提升了全局一致性。&lt;h4&gt;翻译&lt;/h4&gt;这项工作介绍了BEV-LIO(LC)，这是一种新型的激光雷达-惯性里程计框架，结合了鸟瞰图（BEV）图像表示与基于几何点云配准的方法，并且通过BEV图像特征实现了循环闭合。通过对点密度进行归一化处理，将LiDAR点云投影到BEV图像上，从而能够高效地提取和匹配特征。使用轻量级的卷积神经网络（CNN）基特征提取器来从BEV图像中抽取独特的局部与全局描述符。局部描述符用于通过FAST关键点对BEV图像进行再投影误差构造，而全局描述符则有助于循环闭合检测。随后将最小化重投影误差整合进平面到点的配准，在迭代扩展卡尔曼滤波器（iEKF）中执行。在后端部分，使用全局描述符建立一个KD-tree索引的关键帧数据库来实现准确的循环闭合检测。一旦发现循环闭合，随机样本一致性（RANSAC）将从BEV图像匹配计算出粗略变换，作为迭代最近点算法（ICP）的初始估计值。随后细化后的变换被整合进因子图并结合里程计因素提升全局定位的一致性。在不同类型的LiDAR和各种场景下进行广泛的实验表明，该方法优于现有技术，并达到了具有竞争力的定位精度。我们的代码、视频及补充材料可从https://github.com/HxCa1/BEV-LIO-LC获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work introduces BEV-LIO(LC), a novel LiDAR-Inertial Odometry (LIO)framework that combines Bird's Eye View (BEV) image representations of LiDARdata with geometry-based point cloud registration and incorporates loop closure(LC) through BEV image features. By normalizing point density, we project LiDARpoint clouds into BEV images, thereby enabling efficient feature extraction andmatching. A lightweight convolutional neural network (CNN) based featureextractor is employed to extract distinctive local and global descriptors fromthe BEV images. Local descriptors are used to match BEV images with FASTkeypoints for reprojection error construction, while global descriptorsfacilitate loop closure detection. Reprojection error minimization is thenintegrated with point-to-plane registration within an iterated Extended KalmanFilter (iEKF). In the back-end, global descriptors are used to create aKD-tree-indexed keyframe database for accurate loop closure detection. When aloop closure is detected, Random Sample Consensus (RANSAC) computes a coarsetransform from BEV image matching, which serves as the initial estimate forIterative Closest Point (ICP). The refined transform is subsequentlyincorporated into a factor graph along with odometry factors, improving theglobal consistency of localization. Extensive experiments conducted in variousscenarios with different LiDAR types demonstrate that BEV-LIO(LC) outperformsstate-of-the-art methods, achieving competitive localization accuracy. Ourcode, video and supplementary materials can be found athttps://github.com/HxCa1/BEV-LIO-LC.</description>
      <author>example@mail.com (Haoxin Cai, Shenghai Yuan, Xinyi Li, Junfeng Guo, Jianqi Liu)</author>
      <guid isPermaLink="false">2502.19242v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Pathology Report Generation and Multimodal Representation Learning for Cutaneous Melanocytic Lesions</title>
      <link>http://arxiv.org/abs/2502.19293v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 2 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;开发了一个专门用于皮肤病理领域的视觉-语言模型，该模型通过对比生成描述的方式工作，并基于42,512张HE染色的全切片图像和19,645份对应的病理报告进行训练与评估。&lt;h4&gt;背景&lt;/h4&gt;每年有数百万的黑色素细胞皮肤病变被病理学家检查，大多数是常见的痣。虽然这些病变可以在几秒钟内诊断出来，但撰写相应的病理报告却非常耗时。&lt;h4&gt;目的&lt;/h4&gt;通过自动化部分报告生成工作来减轻病理学家日益增加的工作量。&lt;h4&gt;方法&lt;/h4&gt;使用对比描述框架开发了一个视觉-语言模型，并利用一个包含42,512张HE染色的全切片图像和19,645份对应病理报告的数据集进行训练与评估。&lt;h4&gt;主要发现&lt;/h4&gt;由模型生成的常见痣的报告质量得分与专家病理学家书写的报告相当，但在稀有黑色素细胞病变亚型的情况下，报告生成较为困难，但是跨模态检索性能有了显著提升。&lt;h4&gt;结论&lt;/h4&gt;该视觉-语言模型在处理常见的黑色素细胞皮肤病变方面表现良好，并且对于罕见类型也有一定的应用潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Millions of melanocytic skin lesions are examined by pathologists each year,the majority of which concern common nevi (i.e., ordinary moles). While most ofthese lesions can be diagnosed in seconds, writing the corresponding pathologyreport is much more time-consuming. Automating part of the report writingcould, therefore, alleviate the increasing workload of pathologists. In thiswork, we develop a vision-language model specifically for the pathology domainof cutaneous melanocytic lesions. The model follows the Contrastive Captionerframework and was trained and evaluated using a melanocytic lesion dataset of42,512 H&amp;E-stained whole slide images and 19,645 corresponding pathologyreports. Our results show that the quality scores of model-generated reportswere on par with pathologist-written reports for common nevi, assessed by anexpert pathologist in a reader study. While report generation revealed to bemore difficult for rare melanocytic lesion subtypes, the cross-modal retrievalperformance for these cases was considerably better.</description>
      <author>example@mail.com (Ruben T. Lucassen, Sander P. J. Moonemans, Tijn van de Luijtgaarden, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19293v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-modal Contrastive Learning for Tumor-specific Missing Modality Synthesis</title>
      <link>http://arxiv.org/abs/2502.19390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究设计了一种生成模型，用于从现有模态中合成缺失的MRI影像，特别聚焦于肿瘤区域，并在对比学习过程中通过熵选择特征来提高效果。&lt;h4&gt;背景&lt;/h4&gt;多模式磁共振成像（MRI）对于提供互补的大脑解剖和病理信息至关重要，有助于更准确地诊断。但在临床环境中获取高质量的多模态MRI面临时间、成本和技术限制等挑战。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些困难，研究者开发了一种生成模型，该模型能够利用现有的源模态数据来合成缺失的目标模态影像，特别是在肿瘤区域进行有效的对比学习和特征选择。&lt;h4&gt;方法&lt;/h4&gt;所设计的网络采用了多模式对比学习，并在对比过程中通过熵选择关键特性。此外，该网络不仅可以生成目标模态图像，还可以预测分割输出，从而提高了生成肿瘤区域的能力，有助于提高下游任务的表现。&lt;h4&gt;主要发现&lt;/h4&gt;结合了对比、分割和自我表示损失的功能模型能够有效反映特定的目标信息并生成高质量的影像。&lt;h4&gt;结论&lt;/h4&gt;在Brain MR Image Synthesis挑战赛中，所提出的模型表现出色，在合成缺失模态方面超越其他方法。&lt;h4&gt;翻译&lt;/h4&gt;多模式MRI对于提供互补的大脑解剖和病理信息至关重要。然而，在临床环境中获取高质量的多模态MRI存在困难。因此，该研究设计了一种生成模型来合成缺失的目标模态影像，并在肿瘤区域使用熵选择关键特征进行对比学习，以提高效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal magnetic resonance imaging (MRI) is essential for providingcomplementary information about brain anatomy and pathology, leading to moreaccurate diagnoses. However, obtaining high-quality multi-modal MRI in aclinical setting is difficult due to factors such as time constraints, highcosts, and patient movement artifacts. To overcome this difficulty, there isincreasing interest in developing generative models that can synthesize missingtarget modality images from the available source ones. Therefore, we design agenerative model for missing MRI that integrates multi-modal contrastivelearning with a focus on critical tumor regions. Specifically, we integratemulti-modal contrastive learning, tailored for multiple source modalities, andenhance its effectiveness by selecting features based on entropy during thecontrastive learning process. Additionally, our network not only generates themissing target modality images but also predicts segmentation outputs,simultaneously. This approach improves the generator's capability to preciselygenerate tumor regions, ultimately improving performance in downstreamsegmentation tasks. By leveraging a combination of contrastive, segmentation,and additional self-representation losses, our model effectively reflectstarget-specific information and generate high-quality target images.Consequently, our results in the Brain MR Image Synthesis challenge demonstratethat the proposed model excelled in generating the missing modality.</description>
      <author>example@mail.com (Minjoo Lim, Bogyeong Kang, Tae-Eui Kam)</author>
      <guid isPermaLink="false">2502.19390v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PointSea: Point Cloud Completion via Self-structure Augmentation</title>
      <link>http://arxiv.org/abs/2502.17053v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Computer Vision (IJCV).  Extension of our ICCV 2023 work: arXiv:2307.08492&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一个新的点云补全模型PointSea，该模型通过自我结构增强来实现从全局到局部的点云补全。&lt;h4&gt;背景&lt;/h4&gt;目前的点云补全方法依赖于3D坐标信息和其他额外数据（例如图像和扫描视角）来填充缺失的部分。这些方法存在一定的局限性。&lt;h4&gt;目的&lt;/h4&gt;探索如何仅利用原始点云自身的结构特征进行增强，从而实现更有效的全局到局部的点云补全任务。&lt;h4&gt;方法&lt;/h4&gt;{'全局阶段': '通过模拟从多个角度观察一个物理对象上的缺陷区域的方式，使用自投影深度图来增强数据表示。引入特征融合模块以跨模态输入为基础重构紧凑的全局形状。', '局部阶段': '提出了称为自结构对偶生成器的点生成器，该生成器将学习到的形状先验知识和几何自相似性相结合进行形状细化，并且采用适应每个点结构类型的双路径设计来调整细化策略。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PointSea能够有效地理解全局形状并从不完整输入中生成局部细节。&lt;h4&gt;结论&lt;/h4&gt;相较于现有的方法，PointSea在处理点云补全问题时表现出明显的改进效果。&lt;h4&gt;翻译&lt;/h4&gt;点云完成是3D视觉中的一个基本但尚未完全解决的问题。当前的方法通常依赖于三维坐标信息和其他附加数据（例如图像和扫描视角）来填补缺失的部分。不同于这些方法，我们探索了自我结构增强，并提出了用于全局到局部点云完成的PointSea模型。在全局阶段，考虑到如何检查物理对象上的缺陷区域时会从多个角度进行观察以获得更好的理解，因此，受此启发，PointSea通过利用来自多视角的自投影深度图来增强数据表示。为了从跨模态输入中重构紧凑的全球形状，我们整合了一个特征融合模块，在视内和视间层次上融合了特性。在局部阶段，为揭示高度细节结构，我们引入了一种称为自结构性对偶生成器的点生成器。该生成器结合了学习到的形状先验知识和几何自相似性进行形体细化。不同于现有的采用统一策略处理所有点的方法，我们的双路径设计适应于每个点的结构类型调整精炼策略以应对特定的不完整性。在广泛使用的基准测试上进行全面实验表明，PointSea有效地理解全局形状并从不完整输入中生成局部细节，显示出对现有方法的明显改进。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is a fundamental yet not well-solved problem in 3Dvision. Current approaches often rely on 3D coordinate information and/oradditional data (e.g., images and scanning viewpoints) to fill in missingparts. Unlike these methods, we explore self-structure augmentation and proposePointSea for global-to-local point cloud completion. In the global stage,consider how we inspect a defective region of a physical object, we may observeit from various perspectives for a better understanding. Inspired by this,PointSea augments data representation by leveraging self-projected depth imagesfrom multiple views. To reconstruct a compact global shape from the cross-modalinput, we incorporate a feature fusion module to fuse features at bothintra-view and inter-view levels. In the local stage, to reveal highly detailedstructures, we introduce a point generator called the self-structuredual-generator. This generator integrates both learned shape priors andgeometric self-similarities for shape refinement. Unlike existing efforts thatapply a unified strategy for all points, our dual-path design adapts refinementstrategies conditioned on the structural type of each point, addressing thespecific incompleteness of each point. Comprehensive experiments on widely-usedbenchmarks demonstrate that PointSea effectively understands global shapes andgenerates local details from incomplete input, showing clear improvements overexisting methods.</description>
      <author>example@mail.com (Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.17053v3</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Deep Learning For Time Series Analysis With Application On Human Motion</title>
      <link>http://arxiv.org/abs/2502.19364v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'时间序列数据的重要性': '时间序列数据在医学、电信和能源等领域中非常重要。', '分析任务类型': '包括分类、聚类、原型设计和回归等任务，这些任务分别用于识别异常运动、检测股市行为模式、扩展物理治疗数据集以及预测患者恢复情况。', '深度学习的应用': '近年来，在其他领域取得成功的背景下，深度学习在时间序列分析中越来越受到重视。', '研究贡献': '本论文通过特征工程提升分类性能，引入基础模型，并开发了一种紧凑且最先进的架构。同时利用自监督学习解决数据标签不足的问题。', '实际应用': '应用于人体运动分析（如动作识别和康复）以及合成样本生成方法以支持回归模型在数据稀缺的情况下进行原型设计。', '评价与展望': '评估了判别性和生成性模型的局限，并提倡建立一个稳健且标准化的评估框架。实验结果提供了新的见解和方法，推动时间序列分析的发展。'}&lt;h4&gt;翻译&lt;/h4&gt;该论文摘要讨论了时间序列数据分析在医学、电信及能源等领域的关键作用以及分类、聚类、原型设计与回归等具体任务的应用。它还强调了深度学习技术在这类数据中的重要性，并通过特征工程和基础模型的引入，旨在提升现有方法的有效性和准确性。此外，论文提出了解决有限标注数据问题的方法，例如自监督学习，在人体运动分析（如康复与动作识别）中应用这些方法以及开发用于扩展数据集的支持回归模型的合成样本生成技术。最后，作者对当前的技术进行批判性评估，并提出了一个更稳健、标准化的研究框架以促进时间序列分析领域的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Time series data, defined by equally spaced points over time, is essential infields like medicine, telecommunications, and energy. Analyzing it involvestasks such as classification, clustering, prototyping, and regression.Classification identifies normal vs. abnormal movements in skeleton-basedmotion sequences, clustering detects stock market behavior patterns,prototyping expands physical therapy datasets, and regression predicts patientrecovery. Deep learning has recently gained traction in time series analysisdue to its success in other domains. This thesis leverages deep learning toenhance classification with feature engineering, introduce foundation models,and develop a compact yet state-of-the-art architecture. We also addresslimited labeled data with self-supervised learning. Our contributions apply toreal-world tasks, including human motion analysis for action recognition andrehabilitation. We introduce a generative model for human motion data, valuablefor cinematic production and gaming. For prototyping, we propose a shape-basedsynthetic sample generation method to support regression models when data isscarce. Lastly, we critically evaluate discriminative and generative models,identifying limitations in current methodologies and advocating for a robust,standardized evaluation framework. Our experiments on public datasets providenovel insights and methodologies, advancing time series analysis with practicalapplications.</description>
      <author>example@mail.com (Ali Ismail-Fawaz)</author>
      <guid isPermaLink="false">2502.19364v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ProxyTransformation: Preshaping Point Cloud Manifold With Proxy Attention For 3D Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.19247v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种名为Proxy Transformation的方法，该方法旨在通过多模态任务有效地改进点云流形结构。&lt;h4&gt;背景&lt;/h4&gt;基于语言指令与3D环境的实时交互是实现具身智能的基础任务之一。然而，从RGB-D图像渲染出的点云包含大量冗余背景数据和内在噪声，这些干扰了目标区域的流形结构。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来改进点云流形结构，使其适用于实时任务。&lt;h4&gt;方法&lt;/h4&gt;首先利用可变形点聚类技术识别目标区域内的点云子流形。然后引入Proxy Attention模块，该模块使用多模态代理指导点云变换。此外还设计了基于Proxy Attention的次流形变换生成模块，在全局层面由文本信息引导不同子流形的平移向量，并通过图像信息优化每个子流形内部的线性变换。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的Proxy Transformation方法在易目标和难目标上的性能分别提高了7.49%和4.60%，同时减少了注意力块的计算开销（降低40.6%）。这些成果标志着ego-centric 3D视觉定位领域的新SOTA。&lt;h4&gt;结论&lt;/h4&gt;研究展示了一种有效且稳健的方法来改进点云流形结构，进一步推动了具身智能领域的发展。&lt;h4&gt;翻译&lt;/h4&gt;嵌入式智能要求代理根据语言指令在实时与三维环境中交互。该领域的基础任务是自体中心的3D视觉定位。然而，从RGB-D图像渲染出的点云保留了大量的冗余背景数据和内在噪声，这些干扰了目标区域的流形结构。现有的点云增强方法通常需要繁琐的过程来改进流形结构，这不适合实时任务。我们提出了一个适合多模态任务的Proxy Transformation方法，以高效地改进点云流形。该方法首先利用可变形点聚类技术识别目标区域内的点云子流形。接着引入了Proxy Attention模块，使用多模态代理引导点云变换。基于Proxy Attention设计了一个次流形变换生成模块，在全局层面由文本信息指导不同子流形的平移向量，并通过图像信息优化每个子流形内部的线性变换，从而精炼目标区域的局部点云流形结构。广泛的实验表明，Proxy Transformation显著优于现有方法，在易目标和难目标上分别提高了7.49%和4.60%，同时减少了注意力块的计算开销（降低40.6%）。这些结果建立了ego-centric 3D视觉定位领域的新SOTA，展示了该方法的有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied intelligence requires agents to interact with 3D environments inreal time based on language instructions. A foundational task in this domain isego-centric 3D visual grounding. However, the point clouds rendered from RGB-Dimages retain a large amount of redundant background data and inherent noise,both of which can interfere with the manifold structure of the target regions.Existing point cloud enhancement methods often require a tedious process toimprove the manifold, which is not suitable for real-time tasks. We proposeProxy Transformation suitable for multimodal task to efficiently improve thepoint cloud manifold. Our method first leverages Deformable Point Clustering toidentify the point cloud sub-manifolds in target regions. Then, we propose aProxy Attention module that utilizes multimodal proxies to guide point cloudtransformation. Built upon Proxy Attention, we design a submanifoldtransformation generation module where textual information globally guidestranslation vectors for different submanifolds, optimizing relative spatialrelationships of target regions. Simultaneously, image information guideslinear transformations within each submanifold, refining the local point cloudmanifold of target regions. Extensive experiments demonstrate that ProxyTransformation significantly outperforms all existing methods, achieving animpressive improvement of 7.49% on easy targets and 4.60% on hard targets,while reducing the computational overhead of attention blocks by 40.6%. Theseresults establish a new SOTA in ego-centric 3D visual grounding, showcasing theeffectiveness and robustness of our approach.</description>
      <author>example@mail.com (Qihang Peng, Henry Zheng, Gao Huang)</author>
      <guid isPermaLink="false">2502.19247v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>GraphBridge: Towards Arbitrary Transfer Learning in GNNs</title>
      <link>http://arxiv.org/abs/2502.19252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 3 figures, 6 tables, to be published in ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNNs）传统上是针对特定领域和任务进行训练的。这导致了在不同、异构的数据设置中转移所获得的知识存在重大障碍。&lt;h4&gt;目的&lt;/h4&gt;介绍GraphBridge，一个创新框架，旨在使知识能够在不同的任务和领域之间转移，避免对任务配置或图结构进行修改的需求。&lt;h4&gt;方法&lt;/h4&gt;GraphBridge允许任何预训练的GNN通过添加预测头和桥接网络来增强，该桥接网络将输入层与输出层连接起来。这种架构不仅保留了原始模型的基本知识，还支持任意维度的输出。&lt;h4&gt;主要发现&lt;/h4&gt;为了减少负向迁移问题，GraphBridge合并了源模型与一个同时训练的模型，从而减少了应用到目标域时源模型的偏差。该方法在包括图到图、节点到节点、图到节点和图到点云在内的多种转移学习场景中进行了全面评估。&lt;h4&gt;结论&lt;/h4&gt;通过16个代表这些场景的数据集进行的经验验证确认了框架对于任务无关和领域无关的知识迁移的能力，在GNN领域具有显著的进展。&lt;h4&gt;翻译&lt;/h4&gt;Graph神经网络（GNNs）传统上按照每个领域的每个任务来训练。这在将所获得的知识转移到不同的、异构的数据设置中造成了重大障碍。本文引入了GraphBridge，一个新颖框架，旨在使知识能够在不同且多样化的任务和领域之间转移，同时不需要修改任务配置或图结构。具体而言，GraphBridge允许通过添加预测头和连接输入到输出层的桥接网络来增强任何预训练的GNN，这种架构不仅保留了原始模型内在的知识，并且支持任意维度的输出。为了减轻负向迁移问题，GraphBridge将源模型与一个同时训练的模型合并，从而减少应用于目标领域时源模型的偏差。我们的方法在包括图到图、节点到节点、图到节点和图到点云在内的各种转移学习场景中得到了彻底评估，并通过代表这些场景的16个数据集进行了实证验证，证实了该框架具备处理图类数据任务无关且领域无关的知识迁移的能力，在GNNs领域实现了显著的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) are conventionally trained on a per-domain,per-task basis. It creates a significant barrier in transferring the acquiredknowledge to different, heterogeneous data setups. This paper introducesGraphBridge, a novel framework to enable knowledge transfer across disparatetasks and domains in GNNs, circumventing the need for modifications to taskconfigurations or graph structures. Specifically, GraphBridge allows for theaugmentation of any pre-trained GNN with prediction heads and a bridgingnetwork that connects the input to the output layer. This architecture not onlypreserves the intrinsic knowledge of the original model but also supportsoutputs of arbitrary dimensions. To mitigate the negative transfer problem,GraphBridg merges the source model with a concurrently trained model, therebyreducing the source bias when applied to the target domain. Our method isthoroughly evaluated across diverse transfer learning scenarios, includingGraph2Graph, Node2Node, Graph2Node, and graph2point-cloud. Empiricalvalidation, conducted over 16 datasets representative of these scenarios,confirms the framework's capacity for task- and domain-agnostic transferlearning within graph-like data, marking a significant advancement in the fieldof GNNs.</description>
      <author>example@mail.com (Li Ju, Xingyi Yang, Qi Li, Xinchao Wang)</author>
      <guid isPermaLink="false">2502.19252v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Global Graph Propagation with Hierarchical Information Transfer for Incomplete Contrastive Multi-view Clustering</title>
      <link>http://arxiv.org/abs/2502.19291v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种新的不完整多视角聚类方法被提出，该方法通过分层信息传递解决当前研究中存在的问题。&lt;h4&gt;背景&lt;/h4&gt;在现实世界中普遍存在大量缺失的多视图数据，使得不完整的多视图聚类成为一个重要的研究课题。然而，现有的方法虽然取得了显著进展，但仍然存在一些问题：无法有效挖掘缺失数据中的隐藏信息；大多数方法将表示学习和聚类分为两个独立阶段进行。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法以解决现有方法中存在的问题，并通过实验展示该方法的有效性和优越性。&lt;h4&gt;方法&lt;/h4&gt;首先设计特定视角的图卷积网络（GCN）来获得包含图结构信息的表示，然后将其融合到共识表示中。其次考虑了一层GCN只能转移一阶邻居节点的信息，提出了全局图传播的方法以处理缺失数据和学习深层表示。最后，通过权重共享伪分类器与对比学习设计了一个端到端框架，该框架结合了特定视角的表示学习、全局图传播以及分层信息传递进行联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在多个常用的数据集上进行了广泛的实验，并展示了优于其他最新方法的效果和优越性。&lt;h4&gt;结论&lt;/h4&gt;通过实验验证了所提出的方法的有效性和优越性，该方法不仅解决了现有问题，还提供了更好的聚类性能。相关代码可在指定的GitHub仓库中获取。&lt;h4&gt;翻译&lt;/h4&gt;不完整多视图聚类研究由于现实世界中存在的广泛缺失数据而变得重要。尽管现有的方法已经取得了很大的进步，但它们仍然存在一些问题：1）大多数方法不能有效挖掘隐藏在丢失数据中的信息；2）大多数方法通常将表示学习和聚类分为两个独立阶段进行，但这可能会影响聚类性能，因为聚类结果直接依赖于学到的表示。为了解决这些问题，我们提出了一种新的不完整多视图聚类方法，该方法使用分层信息传递。首先设计了特定视角的图卷积网络（GCN）以获取包含图结构信息的表示，并将其融合到共识表示中；其次提出了全局图传播的方法处理缺失数据并学习深层表示；最后通过权重共享伪分类器与对比学习建立了一个端到端框架，结合了特定视角的表示学习、全局图传播以及分层信息传递进行联合优化。广泛的实验显示我们的方法在多个常用的数据集上比其他最新方法更有效和优越。代码可在指定GitHub仓库中获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incomplete multi-view clustering has become one of the important researchproblems due to the extensive missing multi-view data in the real world.Although the existing methods have made great progress, there are still someproblems: 1) most methods cannot effectively mine the information hidden in themissing data; 2) most methods typically divide representation learning andclustering into two separate stages, but this may affect the clusteringperformance as the clustering results directly depend on the learnedrepresentation. To address these problems, we propose a novel incompletemulti-view clustering method with hierarchical information transfer. Firstly,we design the view-specific Graph Convolutional Networks (GCN) to obtain therepresentation encoding the graph structure, which is then fused into theconsensus representation. Secondly, considering that one layer of GCN transfersone-order neighbor node information, the global graph propagation with theconsensus representation is proposed to handle the missing data and learn deeprepresentation. Finally, we design a weight-sharing pseudo-classifier withcontrastive learning to obtain an end-to-end framework that combinesview-specific representation learning, global graph propagation withhierarchical information transfer, and contrastive clustering for jointoptimization. Extensive experiments conducted on several commonly-used datasetsdemonstrate the effectiveness and superiority of our method in comparison withother state-of-the-art approaches. The code is available athttps://github.com/KelvinXuu/GHICMC.</description>
      <author>example@mail.com (Guoqing Chao, Kaixin Xu, Xijiong Xie, Yongyong Chen)</author>
      <guid isPermaLink="false">2502.19291v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>AutoML for Multi-Class Anomaly Compensation of Sensor Drift</title>
      <link>http://arxiv.org/abs/2502.19180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  To be published in Measurement Journal&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了两种解决传感器漂移问题的方法：一种是新的验证模型的补偿学习范式，另一种是通过自动化机器学习技术提高分类性能并补偿传感器漂移。&lt;h4&gt;背景&lt;/h4&gt;在工业测量系统中，精确的数据输出对于保持监测过程中的准确性和可靠性至关重要。然而，现有的模型训练方法使用标准交叉验证方法来估计模型性能时，未能充分考虑随时间累积的传感器漂移问题，导致模型预测未来数据偏差的能力减弱。&lt;h4&gt;目的&lt;/h4&gt;提出新的补偿学习范式和自动机器学习技术以改进分类性能，并有效应对传感器漂移。&lt;h4&gt;方法&lt;/h4&gt;论文提出了两个主要解决方案：(1) 一种新颖的用于验证模型精度的新颖传感器漂移补偿学习范例；(2) 自动化机器学习（AutoML）方法，通过采用数据平衡、元学习、自动集成学习、超参数优化、特征选择和提升等策略来改进分类性能。&lt;h4&gt;主要发现&lt;/h4&gt;标准交叉验证方法在处理长期累积的传感器漂移时过于乐观，并且现有模型难以准确预测未来的数据偏差。论文展示了一种新的补偿学习范式以及AutoML-DC（针对漂移补偿）模型，该模型通过上述技术手段显著提高了分类性能。&lt;h4&gt;结论&lt;/h4&gt;提出的AutoML-DC方法不仅改善了分类性能，还能够有效适应不同的传感器漂移严重程度，从而增强了模型的泛化能力和适应性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Addressing sensor drift is essential in industrial measurement systems, whereprecise data output is necessary for maintaining accuracy and reliability inmonitoring processes, as it progressively degrades the performance of machinelearning models over time. Our findings indicate that the standardcross-validation method used in existing model training overestimatesperformance by inadequately accounting for drift. This is primarily becausetypical cross-validation techniques allow data instances to appear in bothtraining and testing sets, thereby distorting the accuracy of the predictiveevaluation. As a result, these models are unable to precisely predict futuredrift effects, compromising their ability to generalize and adapt to evolvingdata conditions. This paper presents two solutions: (1) a novel sensor driftcompensation learning paradigm for validating models, and (2) automated machinelearning (AutoML) techniques to enhance classification performance andcompensate sensor drift. By employing strategies such as data balancing,meta-learning, automated ensemble learning, hyperparameter optimization,feature selection, and boosting, our AutoML-DC (Drift Compensation) modelsignificantly improves classification performance against sensor drift.AutoML-DC further adapts effectively to varying drift severities.</description>
      <author>example@mail.com (Melanie Schaller, Mathis Kruse, Antonio Ortega, Marius Lindauer, Bodo Rosenhahn)</author>
      <guid isPermaLink="false">2502.19180v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>On the Importance of Text Preprocessing for Multimodal Representation Learning and Pathology Report Generation</title>
      <link>http://arxiv.org/abs/2502.19285v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了从病理报告中选择信息进行视觉-语言模型训练对多模态表示和生成报告质量的影响。&lt;h4&gt;背景&lt;/h4&gt;目前很多视觉-语言模型在病理科的应用能够实现多模态病例检索和自动化报告生成，但这些模型往往基于包含患者历史等无法从全片图像推断出的信息的病理报告进行训练，这可能导致生成报告中的幻觉句子。&lt;h4&gt;目的&lt;/h4&gt;研究通过对比两种不同训练数据集（完整报告与仅包含细胞和组织外观描述的预处理报告）对视觉-语言模型性能的影响来探究如何选择信息以提高多模态表示的质量和生成报告的质量。&lt;h4&gt;方法&lt;/h4&gt;使用BLIP-2框架，并利用一个皮肤黑色素瘤病灶的数据集进行实验，该数据集包括42,433张H&amp;E染色的全片图像及其对应的19,636份病理报告。通过图像到文本和文本到图像检索以及由专业病理学家对生成报告的质量评估来评测模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;预处理文本可以防止报告生成中的幻觉，尽管这提升了生成报告质量但使用完整报告训练视觉-语言模型在跨模态检索中表现更好。&lt;h4&gt;结论&lt;/h4&gt;选择适当的训练数据对于提高视觉-语言模型的多模态表示和自动化病理报告生成的质量至关重要。然而，在实际应用中需要权衡预处理文本以避免幻觉与保持跨模态检索性能之间的关系。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言模型在病理科的应用能够实现多模态病例检索及自动化报告生成，但目前的很多模型是在包含无法从全片图像推断出的信息（如患者历史）的病理报告上训练出来的，这可能导致报告生成中的幻觉句子。为了探讨选择信息进行视觉-语言建模对质量的影响，研究人员对比了完整报告和仅描述细胞及组织外观的预处理报告训练得到的模型效果，并使用BLIP-2框架以及一个皮肤黑色素瘤病灶的数据集进行了实验评估。结果表明：文本预处理可以防止生成中的幻觉现象；尽管改善了生成报告的质量，但以完整报告训练出的视觉语言模型在跨模态检索中表现更好。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-language models in pathology enable multimodal case retrieval andautomated report generation. Many of the models developed so far, however, havebeen trained on pathology reports that include information which cannot beinferred from paired whole slide images (e.g., patient history), potentiallyleading to hallucinated sentences in generated reports. To this end, weinvestigate how the selection of information from pathology reports forvision-language modeling affects the quality of the multimodal representationsand generated reports. More concretely, we compare a model trained on fullreports against a model trained on preprocessed reports that only includesentences describing the cell and tissue appearances based on the H&amp;E-stainedslides. For the experiments, we built upon the BLIP-2 framework and used acutaneous melanocytic lesion dataset of 42,433 H&amp;E-stained whole slide imagesand 19,636 corresponding pathology reports. Model performance was assessedusing image-to-text and text-to-image retrieval, as well as qualitativeevaluation of the generated reports by an expert pathologist. Our resultsdemonstrate that text preprocessing prevents hallucination in reportgeneration. Despite the improvement in the quality of the generated reports,training the vision-language model on full reports showed better cross-modalretrieval performance.</description>
      <author>example@mail.com (Ruben T. Lucassen, Tijn van de Luijtgaarden, Sander P. J. Moonemans, Gerben E. Breimer, Willeke A. M. Blokx, Mitko Veta)</author>
      <guid isPermaLink="false">2502.19285v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multiview graph dual-attention deep learning and contrastive learning for multi-criteria recommender systems</title>
      <link>http://arxiv.org/abs/2502.19271v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于多边图的新型表示方式，用于解决单一标准推荐系统在处理多准则推荐时遇到的问题。&lt;h4&gt;背景&lt;/h4&gt;深度学习模型对于推荐系统帮助用户选择符合其偏好的项目至关重要。然而，在单标准推荐系统中，存在忽视物品多样属性的问题，这些问题通过多准则推荐系统（MCRS）得以部分解决。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的表示方法和使用Multiview Dual Graph Attention Networks (MDGAT)来更好地考虑用户与物品之间的关系，并在每个视图及整个图形上运用对比学习以区分正负样本。&lt;h4&gt;方法&lt;/h4&gt;基于多边图，其中每条边代表了用户对项目的某一准则的评分；采用MDGAT模型，此模型能够有效处理局部（基于准则）和全局（多准则）的关系。&lt;h4&gt;主要发现&lt;/h4&gt;在真实世界数据集上的评估表明该方法比基线模型具有更高的预测准确性。MDGAT能有效地捕捉邻居的局部和全局影响以及节点之间的相似性。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的表示形式和MDGAT模型，能够更准确地预测项目评分，并有效解决了多准则推荐系统中的挑战。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习模型的推荐系统对于帮助用户选择符合他们偏好和兴趣的项目非常重要。然而，在单一标准推荐系统中仍然存在一个重要问题，即这些系统往往忽视物品的多样性属性，这些问题通过使用多准则推荐系统（MCRS）得以部分解决。虽然共享嵌入向量的方法用于处理基于多个标准的评分但难以捕捉用户与物品之间根据特定标准的具体关系。在这项研究中，我们提出了一种新的表示方法针对多准则推荐系统，采用一个多边图结构，其中每个边代表用户的某一项准则对项目的评分，并引入了Multiview Dual Graph Attention Networks（MDGAT）模型。使用MDGAT对于充分考虑用户与物品之间关系非常重要，因为存在局部（基于标准的）和全局（跨多个标准的）的关系。此外，在每个视图中定义锚点以相似性为基础，并运用局部和全局对比学习来区分各个视图以及整个图形中的正样本和负样本。我们在两个真实世界数据集上评估了该方法，根据项目评分预测性能进行了评估。结果表明与基线方法相比，在相同的数据集中使用我们的方法可以达到更高的准确率。MDGAT有效地捕捉邻居的局部和全局影响以及节点之间的相似性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recommender systems leveraging deep learning models have been crucial forassisting users in selecting items aligned with their preferences andinterests. However, a significant challenge persists in single-criteriarecommender systems, which often overlook the diverse attributes of items thathave been addressed by Multi-Criteria Recommender Systems (MCRS). Sharedembedding vector for multi-criteria item ratings but have struggled to capturethe nuanced relationships between users and items based on specific criteria.In this study, we present a novel representation for Multi-Criteria RecommenderSystems (MCRS) based on a multi-edge bipartite graph, where each edgerepresents one criterion rating of items by users, and Multiview Dual GraphAttention Networks (MDGAT). Employing MDGAT is beneficial and important foradequately considering all relations between users and items, given thepresence of both local (criterion-based) and global (multi-criteria) relations.Additionally, we define anchor points in each view based on similarity andemploy local and global contrastive learning to distinguish between positiveand negative samples across each view and the entire graph. We evaluate ourmethod on two real-world datasets and assess its performance based on itemrating predictions. The results demonstrate that our method achieves higheraccuracy compared to the baseline method for predicting item ratings on thesame datasets. MDGAT effectively capture the local and global impact ofneighbours and the similarity between nodes.</description>
      <author>example@mail.com (Saman Forouzandeh, Pavel N. Krivitsky, Rohitash Chandra)</author>
      <guid isPermaLink="false">2502.19271v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Biological and Machine Intelligence: Attention Mechanisms in Brain-Computer Interfaces</title>
      <link>http://arxiv.org/abs/2502.19281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了在脑机接口(BCI)应用中，传统和基于Transformer的注意力机制及其在多模态数据融合中的作用。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习技术的发展，注意力机制已经成为EEG信号分析不可或缺的一部分，并显著提升了BCI的应用效果。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在全面回顾传统的和基于Transformers的注意力机制、它们的嵌入策略以及这些方法如何应用于基于EEG的BCI中，特别关注多模态数据融合。&lt;h4&gt;方法&lt;/h4&gt;本文将注意力机制分为传统注意力机制与基于Transformer的多头自注意机制两大类。前者通常结合卷积网络和递归网络使用；后者擅长捕捉长范围依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;通过捕获EEG信号在时间、频率及空间通道的变化，注意力机制能提高特征提取能力、表示学习能力和模型鲁棒性。此外，注意力机制不仅提升了单模态分析的能力，还增强了多模态EEG应用的效果，促进了有效融合EEG与其他生理或感觉数据。&lt;h4&gt;结论&lt;/h4&gt;本文探讨了基于注意力的EEG建模中现有的挑战和新兴趋势，并展望了未来的发展方向，以推动BCI技术的进步。&lt;h4&gt;翻译&lt;/h4&gt;随着深度学习技术的迅速发展，注意力机制已经成为脑电图（EEG）信号分析中的关键组成部分，显著增强了脑机接口（BCI）应用的效果。本文综述了传统和基于Transformer的注意力机制、它们的嵌入策略以及这些方法如何应用于基于EEG的BCI中，特别关注多模态数据融合。通过捕捉EEG信号在时间、频率及空间通道的变化，注意力机制能够提高特征提取能力、表示学习能力和模型鲁棒性。这些方法可以分为两大类：传统注意力机制，通常与卷积网络和递归网络结合使用；基于Transformer的多头自注意机制，擅长捕捉长范围依赖关系。除了单模态分析之外，注意力机制还提升了多模态EEG应用的效果，促进了有效融合EEG与其他生理或感觉数据的能力。最后，本文讨论了基于注意力的EEG建模中现有的挑战和新兴趋势，并为未来的研究方向提供了有价值的见解，以推动BCI技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancement of deep learning, attention mechanisms have becomeindispensable in electroencephalography (EEG) signal analysis, significantlyenhancing Brain-Computer Interface (BCI) applications. This paper presents acomprehensive review of traditional and Transformer-based attention mechanisms,their embedding strategies, and their applications in EEG-based BCI, with aparticular emphasis on multimodal data fusion. By capturing EEG variationsacross time, frequency, and spatial channels, attention mechanisms improvefeature extraction, representation learning, and model robustness. Thesemethods can be broadly categorized into traditional attention mechanisms, whichtypically integrate with convolutional and recurrent networks, andTransformer-based multi-head self-attention, which excels in capturinglong-range dependencies. Beyond single-modality analysis, attention mechanismsalso enhance multimodal EEG applications, facilitating effective fusion betweenEEG and other physiological or sensory data. Finally, we discuss existingchallenges and emerging trends in attention-based EEG modeling, highlightingfuture directions for advancing BCI technology. This review aims to providevaluable insights for researchers seeking to leverage attention mechanisms forimproved EEG interpretation and application.</description>
      <author>example@mail.com (Jiyuan Wang, Weishan Ye, Jialin He, Li Zhang, Gan Huang, Zhuliang Yu, Zhen Liang)</author>
      <guid isPermaLink="false">2502.19281v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Lightweight and Extensible Cell Segmentation and Classification Model for Whole Slide Images</title>
      <link>http://arxiv.org/abs/2502.19217v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  27 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种提高数字病理学中细胞级分析工具质量和性能的方法，通过创建一个轻量级、可扩展的细胞分割和分类模型来解决数据集颗粒度限制、注释不一致等问题。&lt;h4&gt;背景&lt;/h4&gt;开发临床有用的细胞级别分析工具在数字病理学中面临挑战，包括数据集粒度过粗、标注一致性差、计算需求高以及难以将新技术整合到工作流程中的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一个解决方案来改进数据质量、模型性能和可用性。&lt;h4&gt;方法&lt;/h4&gt;[{'步骤1': '更新数据标签通过交叉重标，提高PanNuke和MoNuSAC的数据注释精度，并生成包含七种不同细胞类型的统一数据集。'}, {'步骤2': '利用H-Optimus基础模型作为固定的编码器来改进同时进行分割和分类任务的特征表示。'}, {'步骤3': '为解决基础模型的计算需求问题，通过知识蒸馏减少模型大小和复杂性，保持性能大致相当。'}, {'步骤4': '将蒸馏后的模型集成到QuPath中，这是一个广泛使用的开源数字病理平台。'}]&lt;h4&gt;主要发现&lt;/h4&gt;[{'改进': '基于H-Optimus的模型在分割和分类性能上优于CNN基线模型，在R²评分从0.575提升至0.871及PQ评分从0.450升至0.492方面有明显改善，表明与实际细胞计数更一致且改进了分割质量。'}, {'性能': '蒸馏后的模型保持类似水平的性能同时参数数量减少到原来的四十八分之一。'}]&lt;h4&gt;结论&lt;/h4&gt;通过降低计算复杂度并整合至工作流程，该方法可能会对诊断产生重大影响、减轻病理学家的工作量，并提高结果质量。&lt;h4&gt;翻译&lt;/h4&gt;开发临床有用的细胞级别分析工具在数字病理学中面临数据集粒度过粗、标注一致性差等问题。为解决这些问题，论文提出了一种解决方案：创建一个轻量级的细胞分割和分类模型来提升数据质量和模型性能。此方法通过改进数据标签更新（包括交叉重标），利用H-Optimus基础模型进行特征表示优化，并通过知识蒸馏简化模型结构，同时保持性能水平。结果表明基于该方案的新模型在QuPath平台上的表现优于传统CNN基线模型，在多个评估指标上显示出显著的提升。此外，该方法还有望大幅降低计算复杂度，提高临床应用中的工作效率和诊断准确性。然而，尽管展示了很大的潜力，这种方法仍需进一步验证才能应用于临床实践中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Developing clinically useful cell-level analysis tools in digital pathologyremains challenging due to limitations in dataset granularity, inconsistentannotations, high computational demands, and difficulties integrating newtechnologies into workflows. To address these issues, we propose a solutionthat enhances data quality, model performance, and usability by creating alightweight, extensible cell segmentation and classification model. First, weupdate data labels through cross-relabeling to refine annotations of PanNukeand MoNuSAC, producing a unified dataset with seven distinct cell types.Second, we leverage the H-Optimus foundation model as a fixed encoder toimprove feature representation for simultaneous segmentation and classificationtasks. Third, to address foundation models' computational demands, we distillknowledge to reduce model size and complexity while maintaining comparableperformance. Finally, we integrate the distilled model into QuPath, a widelyused open-source digital pathology platform. Results demonstrate improvedsegmentation and classification performance using the H-Optimus-based modelcompared to a CNN-based model. Specifically, average $R^2$ improved from 0.575to 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicatingbetter alignment with actual cell counts and enhanced segmentation quality. Thedistilled model maintains comparable performance while reducing parameter countby a factor of 48. By reducing computational complexity and integrating intoworkflows, this approach may significantly impact diagnostics, reducepathologist workload, and improve outcomes. Although the method shows promise,extensive validation is necessary prior to clinical deployment.</description>
      <author>example@mail.com (Nikita Shvetsov, Thomas K. Kilvaer, Masoud Tafavvoghi, Anders Sildnes, Kajsa Møllersen, Lill-Tove Rasmussen Busund, Lars Ailo Bongo)</author>
      <guid isPermaLink="false">2502.19217v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multi-level Attention-guided Graph Neural Network for Image Restoration</title>
      <link>http://arxiv.org/abs/2502.19181v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文介绍了一种新的图像恢复技术，通过多级注意力引导的图神经网络解决当前深度学习方法在处理图像复原任务时忽略多重尺度信息的问题。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于卷积神经网络的方法在图像修复领域取得了显著成就。然而，大多数这些方法通常集中在单一尺度上，忽视了融合多种尺度的信息。为了补充局部特征的不足，需要集成全局特征。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决现有模型未能明确构建全局特性或考虑全局与局部特征之间关系的问题。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了多级注意力引导图神经网络（multi-level attention-guided graph neural network）。通过使用多重注意机制，在特征映射内显式构造元素块图和元素图，以提取图像的局部结构特性和全局表示信息。这些图形通过多重注意机制实时学习动态连接，并利用图卷积算法传播和聚合信息。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法在多个经典图像恢复任务中表现出色，达到了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效融合局部元素块信息和全局元素表示信息来更有效地修复图像中的丢失数据。&lt;h4&gt;翻译&lt;/h4&gt;近年来，深度学习在图像复原领域取得了显著成功。然而，大多数基于卷积神经网络的方法主要集中在单一尺度上，忽视了多尺度信息的整合。为了补充局部特征的不足，在图像恢复任务中需要集成全局特性。尽管最近的神经网络算法在特征提取方面取得重大进展，但许多模型未能明确构建全局特性或考虑全局与局部特性的关系。本文提出了一种新的方法——多级注意力引导图神经网络（multi-level attention-guided graph neural network），该方法显式地使用多重注意机制，在特征映射内构造元素块图和元素图以提取图像的局部结构特性和全局表示信息。在结合了本地元素块信息和全局元素表示信息后，算法可以更有效地恢复图像中的丢失数据。实验结果显示该方法在多个经典图像复原任务中表现卓越，达到了最先进的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, deep learning has achieved remarkable success in the fieldof image restoration. However, most convolutional neural network-based methodstypically focus on a single scale, neglecting the incorporation of multi-scaleinformation. In image restoration tasks, local features of an image are ofteninsufficient, necessitating the integration of global features to complementthem. Although recent neural network algorithms have made significant stridesin feature extraction, many models do not explicitly model global features orconsider the relationship between global and local features. This paperproposes multi-level attention-guided graph neural network. The proposednetwork explicitly constructs element block graphs and element graphs withinfeature maps using multi-attention mechanisms to extract both local structuralfeatures and global representation information of the image. Since the networkstruggles to effectively extract global information during image degradation,the structural information of local feature blocks can be used to correct andsupplement the global information. Similarly, when element block information inthe feature map is missing, it can be refined using global elementrepresentation information. The graph within the network learns real-timedynamic connections through the multi-attention mechanism, and information ispropagated and aggregated via graph convolution algorithms. By combining localelement block information and global element representation information fromthe feature map, the algorithm can more effectively restore missing informationin the image. Experimental results on several classic image restoration tasksdemonstrate the effectiveness of the proposed method, achievingstate-of-the-art performance.</description>
      <author>example@mail.com (Jiatao Jiang, Zhen Cui, Chunyan Xu, Jian Yang)</author>
      <guid isPermaLink="false">2502.19181v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Foundation-Model-Based Industrial Defect Detection</title>
      <link>http://arxiv.org/abs/2502.19106v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;随着工业产品变得丰富和复杂，视觉工业缺陷检测受到了广泛关注。研究中介绍了基于传统方法的统计分析、异常数据合成建模及生成模型的方法，并重点讨论了基础模型（FM）在提高检测精度方面的应用及其带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;工业产品的多样性和复杂性增加，促使对二维和三维视觉特征模型的研究增多。传统的缺陷检测主要依赖于统计分析方法以及基于生成的模型。&lt;h4&gt;目的&lt;/h4&gt;系统地回顾并对比了不同角度的基础模型方法，并简要回顾最近发表的非基础模型（NFM）的方法。同时讨论了FM与NFM在训练目标、模型结构和规模及性能方面的差异。&lt;h4&gt;方法&lt;/h4&gt;论文中详细比较了各种基于基础模型的技术，探讨这些技术如何利用视觉和文本语义先验知识来改进工业产品的缺陷检测，并分析了使用基础模型带来的模型复杂性和推断速度减慢的问题。此外还介绍了探索轻量级建模方式的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比研究发现，基于基础模型的方法更适合于少样本学习（few-shot learning）和零样本学习（zero-shot learning），这些方法更符合实际的工业应用场景，值得深入研究。&lt;h4&gt;结论&lt;/h4&gt;尽管使用基础模型可以提高检测精度，但同时也增加了模型复杂性并减慢了推理速度。因此需要探索轻量级建模方式以应对这些问题。此外，基于基础模型的方法在少样本学习和零样本学习中表现良好，这对于实际应用非常有帮助。&lt;h4&gt;翻译&lt;/h4&gt;随着工业产品变得丰富且多样化，视觉工业缺陷检测技术受到了越来越多的关注，包括二维及三维视觉特征的建模。传统方法通过统计分析、异常数据合成以及生成性模型来分离产品缺陷特征，并完成缺陷检测任务。最近，基础模型（Foundation Models）的发展带来了视觉和文本语义先验知识的应用，许多基于此的方法试图提高检测精度，但同时也增加了模型复杂度并减慢了推理速度。一些基于基础模型的方法已经开始探索轻量化建模方式，这些方法逐渐引起了关注，并值得系统性地分析。本文从多个角度对基于基础模型的视觉工业缺陷检测技术进行了系统的调查、比较和讨论，同时简要回顾最近发表的一些非基础模型的方法。此外还探讨了基于基础模型与非基础模型的方法在训练目标、模型结构及性能方面的差异，并指出了未来研究的方向。通过对比分析发现，基于基础模型的技术更适合处理少样本学习及零样本学习问题，这更符合实际工业应用场景中的需求，值得进一步深入研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As industrial products become abundant and sophisticated, visual industrialdefect detection receives much attention, including two-dimensional andthree-dimensional visual feature modeling. Traditional methods use statisticalanalysis, abnormal data synthesis modeling, and generation-based models toseparate product defect features and complete defect detection. Recently, theemergence of foundation models has brought visual and textual semantic priorknowledge. Many methods are based on foundation models (FM) to improve theaccuracy of detection, but at the same time, increase model complexity and slowdown inference speed. Some FM-based methods have begun to explore lightweightmodeling ways, which have gradually attracted attention and deserve to besystematically analyzed. In this paper, we conduct a systematic survey withcomparisons and discussions of foundation model methods from different aspectsand briefly review non-foundation model (NFM) methods recently published.Furthermore, we discuss the differences between FM and NFM methods fromtraining objectives, model structure and scale, model performance, andpotential directions for future exploration. Through comparison, we find FMmethods are more suitable for few-shot and zero-shot learning, which are morein line with actual industrial application scenarios and worthy of in-depthresearch.</description>
      <author>example@mail.com (Tianle Yang, Luyao Chang, Jiadong Yan, Juntao Li, Zhi Wang, Ke Zhang)</author>
      <guid isPermaLink="false">2502.19106v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>EndoMamba: An Efficient Foundation Model for Endoscopic Videos</title>
      <link>http://arxiv.org/abs/2502.19090v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为EndoMamba的新型基础模型，旨在通过优化实时推理和增强表征学习来解决内窥镜视频任务中的计算效率低和性能不足问题。&lt;h4&gt;背景&lt;/h4&gt;内窥镜视频任务如视觉导航和手术阶段识别在微创手术中至关重要。然而，最近的视频基础模型由于计算复杂性和有限的数据集预训练而面临挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种专门针对内窥镜视频设计的基础模型EndoMamba，以提高实时推理能力并改善广义时空表示学习。&lt;h4&gt;方法&lt;/h4&gt;{'1': '提出了优化后的EndoMamba骨干网络，以解决计算效率问题。该网络利用双向Mamba模块进行空间建模，并使用普通Mamba模块进行时间域上的从过去到现在的推断。', '2': '设计了一种自监督分层预训练方案，用于增强EndoMamba的表征学习能力，通过结合掩码重构和辅助监督来提取内窥镜视频中的时空结构和更广泛的知识。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': 'EndoMamba在四个下游任务上（分类、分割、手术阶段识别和定位）的表现优于现有基础模型和特定任务方法。', '2': '实验表明，EndoMamba能够同时保持实时推理速度并提供更好的性能。'}&lt;h4&gt;结论&lt;/h4&gt;EndoMamba展示了内窥镜视频处理中的高效能与实用性，在微创外科手术中具有广泛的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了用于微创手术的内窥镜视频任务，如视觉导航和手术阶段识别，通过实时提供支持来发挥重要作用。尽管最近的视频基础模型显示出前景，但它们在计算效率低以及由于预训练时缺乏内窥镜数据而导致性能不佳方面面临问题。为了应对这些问题，论文提出了EndoMamba——一种专为实时推理而设计的基础模型，在学习泛化的时空表示的同时还能实现实时推断。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Endoscopic video-based tasks, such as visual navigation and surgical phaserecognition, play a crucial role in minimally invasive surgeries by providingreal-time assistance. While recent video foundation models have shown promise,their applications are hindered by (1) computational inefficiencies and (2)suboptimal performance caused by limited data for pre-training in endoscopy. Toaddress these issues, we present EndoMamba, a foundation model designed forreal-time inference while learning generalized spatiotemporal representations.First, to mitigate computational inefficiencies, we propose the EndoMambabackbone, optimized for real-time inference. Inspired by recent advancements instate space models, EndoMamba integrates Bidirectional Mamba blocks for spatialmodeling within individual frames and vanilla Mamba blocks for past-to-presentreasoning across the temporal domain. This design enables both strongspatiotemporal modeling and efficient inference in online video streams.Second, we propose a self-supervised hierarchical pre-training diagram toenhance EndoMamba's representation learning using endoscopic videos andincorporating general video domain knowledge. Specifically, our approachcombines masked reconstruction with auxiliary supervision, leveraging low-levelreconstruction to capture spatial-temporal structures and high-level alignmentto transfer broader knowledge from a pretrained general-video domain foundationmodel. Extensive experiments on four downstream tasks--classification,segmentation, surgical phase recognition, and localization--demonstrate thatEndoMamba outperforms existing foundation models and task-specific methodswhile maintaining real-time inference speed. The source code will be releasedupon acceptance.</description>
      <author>example@mail.com (Qingyao Tian, Huai Liao, Xinyan Huang, Bingyu Yang, Dongdong Lei, Sebastien Ourselin, Hongbin Liu)</author>
      <guid isPermaLink="false">2502.19090v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Leg Exoskeleton Odometry using a Limited FOV Depth Sensor</title>
      <link>http://arxiv.org/abs/2502.19237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的里程计算法，该算法结合了外骨骼的本体感受数据和深度相机获取的点云信息，以生成准确的地表高度图。这种方法在有限视野和传感器运动较大的情况下尤其有效。&lt;h4&gt;背景&lt;/h4&gt;为了使腿部外骨骼能够在真实世界环境中有效地运行，必须能够感知和理解周围地形。然而，与其它腿足机器人相比，外骨骼由于人体的存在而限制了深度传感器的安装位置，导致视场受限且传感器运动更大，这使得里程计技术面临更大的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的里程计算法来解决腿部外骨骼因有限视野和传感器移动带来的问题，该算法能够生成准确的地表高度图。&lt;h4&gt;方法&lt;/h4&gt;利用扩展卡尔曼滤波器（EKF）融合了外骨骼的运动学和惯性测量数据，并通过定制化的迭代最近点（ICP）算法将新的点云与地表高度图进行配准。&lt;h4&gt;主要发现&lt;/h4&gt;实验验证显示，该方法能够减少漂移并提高地表高度图的质量，相比单纯依靠本体感受的方法有明显改善，同时也优于传统的基于点云地图的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的新里程计算法在生成准确的地表高度图方面表现良好，为外骨骼在复杂环境下的有效运行提供了技术支持。&lt;h4&gt;翻译&lt;/h4&gt;为了使腿部外骨骼能够在真实世界环境中有效地运作，它们必须能够感知并理解周围的地形。然而，与其它腿足机器人不同的是，由于人体的存在，外骨骼的深度传感器安装位置受到限制，这导致视场较小且运动更大，使得里程计技术更加困难。为了解决这个问题，我们提出了一种新的融合了外骨骼本体感受数据和来自深度相机点云信息的新里程计算法，以生成准确的地表高度图，即使是在有限视野和传感器运动较大的情况下也能做到这一点。该方法利用扩展卡尔曼滤波器（EKF）结合运动学与惯性测量，并通过定制化的迭代最近点（ICP）算法将新获取的点云数据配准到地表高度图上。实验验证表明，我们的方法可以减少漂移并提高生成的地表高度图的质量，这在单纯依靠本体感受的数据基础上得到了改善，并且也优于传统的基于点云地图的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For leg exoskeletons to operate effectively in real-world environments, theymust be able to perceive and understand the terrain around them. However,unlike other legged robots, exoskeletons face specific constraints on wheredepth sensors can be mounted due to the presence of a human user. Theseconstraints lead to a limited Field Of View (FOV) and greater sensor motion,making odometry particularly challenging. To address this, we propose a novelodometry algorithm that integrates proprioceptive data from the exoskeletonwith point clouds from a depth camera to produce accurate elevation mapsdespite these limitations. Our method builds on an extended Kalman filter (EKF)to fuse kinematic and inertial measurements, while incorporating a tailorediterative closest point (ICP) algorithm to register new point clouds with theelevation map. Experimental validation with a leg exoskeleton demonstrates thatour approach reduces drift and enhances the quality of elevation maps comparedto a purely proprioceptive baseline, while also outperforming a moretraditional point cloud map-based variant.</description>
      <author>example@mail.com (Fabio Elnecave Xavier, Matis Viozelange, Guillaume Burger, Marine Pétriaux, Jean-Emmanuel Deschaud, François Goulette)</author>
      <guid isPermaLink="false">2502.19237v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Graph Neural Networks embedded into Margules model for vapor-liquid equilibria prediction</title>
      <link>http://arxiv.org/abs/2502.18998v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文分析了嵌入扩展Margules模型中的图神经网络（GNNs）在预测气液平衡方面的性能。&lt;h4&gt;背景&lt;/h4&gt;预测热力学模型对于产品和工艺设计的早期阶段至关重要。传统的UNIFAC-Dortmund模型是这一领域的基准。&lt;h4&gt;目的&lt;/h4&gt;评估基于图神经网络嵌入扩展Margules模型的方法，与传统方法进行比较，并探索其在不同类型二元混合物中的表现以及局限性。&lt;h4&gt;方法&lt;/h4&gt;将GNNs嵌入到相对简单的过剩吉布斯自由能模型中，即扩展的Margules模型，用于预测气液平衡，并将其性能与公认的UNIFAC-Dortmund模型进行对比。&lt;h4&gt;主要发现&lt;/h4&gt;尽管整体精度略低于传统的UNIFAC-Dortmund模型，在各种类型二元混合物中的表现却更为准确。此外，由于分子断裂可行性或参数可用性限制了组贡献方法如UNIFAC的应用范围，嵌入Margules模型的GNN提供了一个替代方案。&lt;h4&gt;结论&lt;/h4&gt;研究结果为简单的过剩吉布斯自由能模型结合仅基于无限稀释数据训练的图神经网络所能达到的预测准确性建立了基准。&lt;h4&gt;翻译&lt;/h4&gt;预测热力学模型对于产品和工艺设计的早期阶段至关重要。本文分析了将图神经网络（GNNs）嵌入到相对简单的过剩吉布斯自由能模型——扩展Margules模型中，用于预测气液平衡的表现情况。通过与成熟的UNIFAC-Dortmund模型进行比较，已证明在Margules模型中的GNN整体准确性略低。然而，在各种类型二元混合物的情况下观察到了更高的准确度。此外，由于分子断裂可行性或参数可用性限制了组贡献方法如UNIFAC的应用范围，嵌入Margules模型的GNN提供了一个用于气液平衡估算的替代方案。这些发现为简单的过剩吉布斯自由能模型结合仅基于无限稀释数据训练的图神经网络所能达到的预测准确性建立了基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predictive thermodynamic models are crucial for the early stages of productand process design. In this paper the performance of Graph Neural Networks(GNNs) embedded into a relatively simple excess Gibbs energy model, theextended Margules model, for predicting vapor-liquid equilibrium is analyzed.By comparing its performance against the established UNIFAC-Dortmund model ithas been shown that GNNs embedded in Margules achieves an overall loweraccuracy. However, higher accuracy is observed in the case of various types ofbinary mixtures. Moreover, since group contribution methods, like UNIFAC, arelimited due to feasibility of molecular fragmentation or availability ofparameters, the GNN in Margules model offers an alternative for VLE estimation.The findings establish a baseline for the predictive accuracy that simpleexcess Gibbs energy models combined with GNNs trained solely on infinitedilution data can achieve.</description>
      <author>example@mail.com (Edgar Ivan Sanchez Medina, Kai Sundmacher)</author>
      <guid isPermaLink="false">2502.18998v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Neural Antidote: Class-Wise Prompt Tuning for Purifying Backdoors in Pre-trained Vision-Language Models</title>
      <link>http://arxiv.org/abs/2502.19269v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;针对预训练的视觉-语言模型（VLMs）在对抗性攻击中的脆弱性，提出了Class-wise Backdoor Prompt Tuning (CBPT)方法来提高其对后门攻击的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;现有的防御策略主要集中在整个可疑模型的微调上，但它们只能提供边际抵抗，并且往往导致干净数据准确性的下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效的Class-wise Backdoor Prompt Tuning (CBPT)方法来间接净化被污染的VLMs。&lt;h4&gt;方法&lt;/h4&gt;首先通过对比学习有效地反转潜在的后门触发器；然后利用提示调优技术优化这些类别的文本提示，修改模型决策边界以重新分类后门触发器的特征区域。&lt;h4&gt;主要发现&lt;/h4&gt;CBPT能够显著减轻后门威胁，并保持模型效用。例如，在七种主流的后门攻击中，平均干净准确率为58.86%，攻击成功率仅为0.39%。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了CBPT在提高VLMs对后门攻击鲁棒性方面的优越性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容描述了现有的针对视觉-语言模型（如CLIP）的防御策略存在局限性，尤其是对于数据受限的情况。为了改进这一点，研究人员提出了一种新的方法——Class-wise Backdoor Prompt Tuning (CBPT)，该方法通过调整文本提示来间接净化被污染的模型，并且实验结果表明这种方法在提高模型鲁棒性的同时还能保持良好的性能指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While pre-trained Vision-Language Models (VLMs) such as CLIP exhibitexcellent representational capabilities for multimodal data, recent studieshave shown that they are vulnerable to backdoor attacks. To alleviate thethreat, existing defense strategies primarily focus on fine-tuning the entiresuspicious model, yet offer only marginal resistance to state-of-the-artattacks and often result in a decrease in clean accuracy, particularly indata-limited scenarios. Their failure may be attributed to the mismatch betweeninsufficient fine-tuning data and massive parameters in VLMs. To address thischallenge, we propose Class-wise Backdoor Prompt Tuning (CBPT) defense, anefficient and effective method that operates on the text prompts to indirectlypurify the poisoned VLMs. Specifically, we first employ the advancedcontrastive learning via our carefully crafted positive and negative samples,to effectively invert the backdoor triggers that are potentially adopted by theattacker. Once the dummy trigger is established, we utilize the efficientprompt tuning technique to optimize these class-wise text prompts for modifyingthe model's decision boundary to further reclassify the feature regions ofbackdoor triggers. Extensive experiments demonstrate that CBPT significantlymitigates backdoor threats while preserving model utility, e.g. an averageClean Accuracy (CA) of 58.86\% and an Attack Success Rate (ASR) of 0.39\%across seven mainstream backdoor attacks. These results underscore thesuperiority of our prompt purifying design to strengthen model robustnessagainst backdoor attacks.</description>
      <author>example@mail.com (Jiawei Kong, Hao Fang, Sihang Guo, Chenxi Qing, Bin Chen, Bin Wang, Shu-Tao Xia)</author>
      <guid isPermaLink="false">2502.19269v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>MCLRL: A Multi-Domain Contrastive Learning with Reinforcement Learning Framework for Few-Shot Modulation Recognition</title>
      <link>http://arxiv.org/abs/2502.19071v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;无线通信自动调制识别（AMR）及其挑战，以及如何通过少样本学习（FSL）框架解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;随着无线通信技术的快速发展，自动调制识别在确保通信安全和可靠性方面扮演着重要角色。然而，在特定场景下数据采集难度大、样本量小且标签质量低等问题阻碍了其发展。&lt;h4&gt;目的&lt;/h4&gt;引入一种结合多域对比学习与强化学习的新框架（MCLRL），以解决无线信号处理中少样本学习的挑战。&lt;h4&gt;方法&lt;/h4&gt;该研究没有提出新的FSL特定信号模型，而是提出了一个名为MCLRL的框架。此框架将多域对比学习与强化学习相结合，通过增强信号特征并提取深层次分类特性来优化性能。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的MCLRL框架能够有效从信号中提取关键特征，在少样本任务中表现出色，并且在选择信号模型方面保持了灵活性。&lt;h4&gt;结论&lt;/h4&gt;MCLRL框架提供了一种有效的解决方案，它通过结合多域对比学习和强化学习来克服无线通信自动调制识别中存在的挑战，以实现优异的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid advancements in wireless communication technology, automaticmodulation recognition (AMR) plays a critical role in ensuring communicationsecurity and reliability. However, numerous challenges, including higherperformance demands, difficulty in data acquisition under specific scenarios,limited sample size, and low-quality labeled data, hinder its development.Few-shot learning (FSL) offers an effective solution by enabling models toachieve satisfactory performance with only a limited number of labeled samples.While most FSL techniques are applied in the field of computer vision, they arenot directly applicable to wireless signal processing. This study does notpropose a new FSL-specific signal model but introduces a framework calledMCLRL. This framework combines multi-domain contrastive learning withreinforcement learning. Multi-domain representations of signals enhance featurerichness, while integrating contrastive learning and reinforcement learningarchitectures enables the extraction of deep features for classification. Indownstream tasks, the model achieves excellent performance using only a fewsamples and minimal training cycles. Experimental results show that the MCLRLframework effectively extracts key features from signals, performs well in FSLtasks, and maintains flexibility in signal model selection.</description>
      <author>example@mail.com (Dongwei Xu, Yutao Zhu, Yao Lu, Youpeng Feng, Yun Lin, Qi Xuan)</author>
      <guid isPermaLink="false">2502.19071v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Invariance Pair-Guided Learning: Enhancing Robustness in Neural Networks</title>
      <link>http://arxiv.org/abs/2502.18975v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法来解决机器学习模型在外分布泛化上的挑战，并通过实验验证了该方法的有效性和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型在面对训练数据分布之外的数据时，存在难以泛化的现象。尤其是在依赖于虚假相关性的模型中更为明显。&lt;h4&gt;目的&lt;/h4&gt;提供一种技术来指导神经网络在训练阶段的学习过程，以克服现有方法通常需要多域训练、群标签、专业化增广或预处理等限制。&lt;h4&gt;方法&lt;/h4&gt;首先建立输入对，表示虚假属性和描述不变性。基于这些对，形成一个与传统梯度下降互补的校正梯度，并使这种修正机制适应于预先定义的不变条件。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果在ColoredMNIST、Waterbird-100和CelebA数据集上显示出该方法的有效性和对群移位的强大鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;提出的方法能够有效解决模型在外分布泛化中的挑战，并且无需依赖于多训练域或特定的预处理步骤，从而具有更高的实用价值。&lt;h4&gt;翻译&lt;/h4&gt;机器学习模型在面对训练数据分布之外的数据时面临外分布泛化的难题。现有方法通常需要额外资源来实现泛化效果，而本文提出的方法通过引导神经网络进行适应性修正，在多个数据集上验证了其有效性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Out-of-distribution generalization of machine learning models remainschallenging since the models are inherently bound to the training datadistribution. This especially manifests, when the learned models rely onspurious correlations. Most of the existing approaches apply data manipulation,representation learning, or learning strategies to achieve generalizablemodels. Unfortunately, these approaches usually require multiple trainingdomains, group labels, specialized augmentation, or pre-processing to reachgeneralizable models. We propose a novel approach that addresses theselimitations by providing a technique to guide the neural network through thetraining phase. We first establish input pairs, representing the spuriousattribute and describing the invariance, a characteristic that should notaffect the outcome of the model. Based on these pairs, we form a correctivegradient complementing the traditional gradient descent approach. We furthermake this correction mechanism adaptive based on a predefined invariancecondition. Experiments on ColoredMNIST, Waterbird-100, and CelebA datasetsdemonstrate the effectiveness of our approach and the robustness to groupshifts.</description>
      <author>example@mail.com (Martin Surner, Abdelmajid Khelil, Ludwig Bothmann)</author>
      <guid isPermaLink="false">2502.18975v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Efficient and Distributed Large-Scale Point Cloud Bundle Adjustment via Majorization-Minimization</title>
      <link>http://arxiv.org/abs/2502.18801v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;点云捆集调整是大规模点云地图构建的关键，但由于其计算和内存需求大，在处理大量扫描姿态时复杂度呈三次增长。&lt;h4&gt;背景&lt;/h4&gt;现有方法在进行大规模点云束优化时面临计算效率低、内存使用量大的问题。随着扫描姿态数量的增加，这些问题变得更加严重。&lt;h4&gt;目的&lt;/h4&gt;提出一种高效且分布式的大型点云束集调整方法（BALM3.0），以解决上述挑战，并提高整体性能。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了基于majorization-minimization算法的解耦扫描姿态的方法，使用了基于点到平面距离的替代代价函数。这种方法将优化时间复杂度从三次降低到了线性。&lt;h4&gt;主要发现&lt;/h4&gt;1. 通过解耦扫描姿态，可以显著提高大规模环境中的束集调整过程的计算效率。2. 提出了一种分布式的点云束集调整框架，并成功地使用四个消费级笔记本电脑对大规模数据（包括21,436个姿势和70GB点云）进行了优化。&lt;h4&gt;结论&lt;/h4&gt;该方法在模拟和现实世界环境中均表现出色，可以提供与现有方法相当的准确性同时大幅度提高速度并减少内存消耗。&lt;h4&gt;翻译&lt;/h4&gt;点云捆集调整是大规模点云地图构建的关键。然而，它既计算密集又占用大量内存，并且随着扫描姿态数量的增加，其复杂性呈三次增长。本文介绍了BALM3.0，这是一种高效的分布式大型点云束集调整方法。所提出的方法使用majorization-minimization算法在捆集调整过程中解耦扫描姿态，从而提高了大规模数据处理时的计算效率。将扫描姿势解耦的主要优势源于两个关键方面：首先，通过这种方式，优化的时间复杂度从三次降低到线性，极大地提升了大型环境中的束集调整过程的计算效率；其次，它为分布式捆集调整奠定了理论基础。通过在多个设备之间分配数据和计算任务，这种策略有助于克服内存需求大、计算要求高的限制，而这些对于单个设备来说可能难以处理。所提出的方法已在模拟和现实环境中进行了全面评估。结果表明，该方法可以实现与现有最优残差相当的精度，同时优化速度最高提升704倍，并且内存消耗减少到1/8。此外，本文还提出了并实现了分布式的束集调整框架，并成功地使用四个消费级笔记本电脑对大规模数据（包括21,436个姿势和70GB点云）进行了优化。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud bundle adjustment is critical in large-scale point cloud mapping.However, it is both computationally and memory intensive, with its complexitygrowing cubically as the number of scan poses increases. This paper presentsBALM3.0, an efficient and distributed large-scale point cloud bundle adjustmentmethod. The proposed method employs the majorization-minimization algorithm todecouple the scan poses in the bundle adjustment process, thus performing thepoint cloud bundle adjustment on large-scale data with improved computationalefficiency. The key difficulty of applying majorization-minimization on bundleadjustment is to identify the proper surrogate cost function. In this paper,the proposed surrogate cost function is based on the point-to-plane distance.The primary advantages of decoupling the scan poses via amajorization-minimization algorithm stem from two key aspects. First, thedecoupling of scan poses reduces the optimization time complexity from cubic tolinear, significantly enhancing the computational efficiency of the bundleadjustment process in large-scale environments. Second, it lays the theoreticalfoundation for distributed bundle adjustment. By distributing both data andcomputation across multiple devices, this approach helps overcome thelimitations posed by large memory and computational requirements, which may bedifficult for a single device to handle. The proposed method is extensivelyevaluated in both simulated and real-world environments. The resultsdemonstrate that the proposed method achieves the same optimal residual withcomparable accuracy while offering up to 704 times faster optimization speedand reducing memory usage to 1/8. Furthermore, this paper also presented andimplemented a distributed bundle adjustment framework and successfullyoptimized large-scale data (21,436 poses with 70 GB point clouds) with fourconsumer-level laptops.</description>
      <author>example@mail.com (Rundong Li, Zheng Liu, Hairuo Wei, Yixi Cai, Haotian Li, Fu Zhang)</author>
      <guid isPermaLink="false">2502.18801v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>SE(3)-Equivariant Ternary Complex Prediction Towards Target Protein Degradation</title>
      <link>http://arxiv.org/abs/2502.18875v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了DeepTernary，这是一种基于深度学习的方法，用于预测蛋白质、E3连接酶和小分子之间形成的三元复合物的结构。&lt;h4&gt;背景&lt;/h4&gt;靶向蛋白降解（TPD）作为一种新兴的药物发现模式在不断发展中。PROTACs 和 分子胶降解剂（MGDs）是主要的小分子诱导 TPD 的方式，它们通过与 E3 连接酶和目标蛋白质形成三元复合物来起作用。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在开发一种深度学习方法来直接预测这些复杂的三元结构，以促进药物的发现过程。&lt;h4&gt;方法&lt;/h4&gt;DeepTernary 使用 SE(3)-等变图神经网络（GNN）结合了图内和三元图间注意机制，从高质训练数据集中提取复杂三元互动，并使用基于查询的 Pocket Points 解码器来解码最终的绑定结构。&lt;h4&gt;主要发现&lt;/h4&gt;DeepTernary 在现有的PROTAC基准测试中展现了最先进的准确度和速度，而在盲对接协议下的MGD基准测试中也显示出显著准确性。此外，预测出的埋藏表面积与实验获得的降解效力相关指标有很好的一致性。&lt;h4&gt;结论&lt;/h4&gt;研究表明，DeepTernary 有可能在靶向不可药物化目标的发展过程中有效且加速地发挥作用。&lt;h4&gt;翻译&lt;/h4&gt;基于小分子诱导的靶向蛋白质降解（TPD）已在药物发现领域迅速发展为一种新兴模式。PROTACs 和 分子胶降解剂（MGDs）是主要的小分子，它们通过与 E3 连接酶和目标蛋白形成三元复合物来实现 TPDS 的功能。虽然在二元结构预测方面取得了显著进展，但由于互动机制不明确以及训练数据不足，三元结构的预测仍然具有挑战性。该工作提出了一种新的基于深度学习的方法——DeepTernary，它能够直接通过编码器-解码器架构端到端地预测三元结构。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Targeted protein degradation (TPD) induced by small molecules has emerged asa rapidly evolving modality in drug discovery, targeting proteins traditionallyconsidered "undruggable". Proteolysis-targeting chimeras (PROTACs) andmolecular glue degraders (MGDs) are the primary small molecules that induceTPD. Both types of molecules form a ternary complex linking an E3 ligase with atarget protein, a crucial step for drug discovery. While significant advanceshave been made in binary structure prediction for proteins and small molecules,ternary structure prediction remains challenging due to obscure interactionmechanisms and insufficient training data. Traditional methods relying onmanually assigned rules perform poorly and are computationally demanding due toextensive random sampling. In this work, we introduce DeepTernary, a novel deeplearning-based approach that directly predicts ternary structures in anend-to-end manner using an encoder-decoder architecture. DeepTernary leveragesan SE(3)-equivariant graph neural network (GNN) with both intra-graph andternary inter-graph attention mechanisms to capture intricate ternaryinteractions from our collected high-quality training dataset, TernaryDB. Theproposed query-based Pocket Points Decoder extracts the 3D structure of thefinal binding ternary complex from learned ternary embeddings, demonstratingstate-of-the-art accuracy and speed in existing PROTAC benchmarks without priorknowledge from known PROTACs. It also achieves notable accuracy on the morechallenging MGD benchmark under the blind docking protocol. Remarkably, ourexperiments reveal that the buried surface area calculated from predictedstructures correlates with experimentally obtained degradation potency-relatedmetrics. Consequently, DeepTernary shows potential in effectively assisting andaccelerating the development of TPDs for previously undruggable targets.</description>
      <author>example@mail.com (Fanglei Xue, Meihan Zhang, Shuqi Li, Xinyu Gao, James A. Wohlschlegel, Wenbing Huang, Yi Yang, Weixian Deng)</author>
      <guid isPermaLink="false">2502.18875v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Sample-Level Evaluation and Generative Framework for Model Inversion Attacks</title>
      <link>http://arxiv.org/abs/2502.19070v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to be appeared in 39th Annual AAAI Conference on Artificial  Intelligence (AAAI-25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了模型反转攻击（MI）对机器学习隐私的影响，并提出了一个新的评估指标DDCS，用于更精确地衡量单样本级别的隐私保护情况。&lt;h4&gt;背景&lt;/h4&gt;模型反转攻击能够重构神经网络的训练数据集，威胁到机器学习中的隐私安全。现有评价标准对于样例级别隐私问题重视不够。&lt;h4&gt;目的&lt;/h4&gt;提出新的度量体系DDCS来评估个体样本在MI攻击下的脆弱性，并探索增强和防御机制。&lt;h4&gt;方法&lt;/h4&gt;引入新型度量标准DDCS以综合考量多个方面，同时设计了一种通过熵损失和自然梯度下降集成的转移学习框架，提升现有MI攻击技术。&lt;h4&gt;主要发现&lt;/h4&gt;许多训练样例对当前最先进MI攻击具备较强抵抗性；新提出的评估体系DDCS不仅提高了现有攻击方法的效果，在无监督环境中还能有效识别易受攻击的样本。&lt;h4&gt;结论&lt;/h4&gt;论文通过DDCS展示了样本级隐私保护的重要性，同时提出了一种有效的防御框架，并表明该指标在加强隐私安全方面有巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Model Inversion (MI) attacks, which reconstruct the training dataset ofneural networks, pose significant privacy concerns in machine learning. RecentMI attacks have managed to reconstruct realistic label-level private data, suchas the general appearance of a target person from all training images labeledon him. Beyond label-level privacy, in this paper we show sample-level privacy,the private information of a single target sample, is also important butunder-explored in the MI literature due to the limitations of existingevaluation metrics. To address this gap, this study introduces a novel metrictailored for training-sample analysis, namely, the Diversity and DistanceComposite Score (DDCS), which evaluates the reconstruction fidelity of eachtraining sample by encompassing various MI attack attributes. This, in turn,enhances the precision of sample-level privacy assessments.  Leveraging DDCS as a new evaluative lens, we observe that many trainingsamples remain resilient against even the most advanced MI attack. As such, wefurther propose a transfer learning framework that augments the generativecapabilities of MI attackers through the integration of entropy loss andnatural gradient descent. Extensive experiments verify the effectiveness of ourframework on improving state-of-the-art MI attacks over various metricsincluding DDCS, coverage and FID. Finally, we demonstrate that DDCS can also beuseful for MI defense, by identifying samples susceptible to MI attacks in anunsupervised manner.</description>
      <author>example@mail.com (Haoyang Li, Li Bai, Qingqing Ye, Haibo Hu, Yaxin Xiao, Huadi Zheng, Jianliang Xu)</author>
      <guid isPermaLink="false">2502.19070v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>InternVQA: Advancing Compressed Video Quality Assessment with Distilling Large Foundation Model</title>
      <link>http://arxiv.org/abs/2502.19026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ISCAS 2025(Lecture)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本论文探讨了基于InternVideo2视频基础模型在压缩场景下的视频质量评估任务中的应用，并提出了一种轻量级模型的蒸馏方法。&lt;h4&gt;背景&lt;/h4&gt;现有的视频理解任务依赖于丰富的特征，包括语义信息、纹理和时间运动等。InternVideo2由于其庞大的参数规模和大规模多模态数据的支持，在这一领域显示出了强大的潜力。&lt;h4&gt;目的&lt;/h4&gt;研究将InternVideo2转移到压缩场景下的视频质量评估任务中的可行性，并探索设计一个轻量级模型的方法以适应此任务。&lt;h4&gt;方法&lt;/h4&gt;提出了一种蒸馏方法，使较小的模型能够获得丰富的压缩质量先验知识。此外，在蒸馏过程中还对不同骨干网络的表现进行了考察。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与其它方法相比，从InternVideo2中蒸馏出来的轻量级模型在压缩视频的质量评估任务中取得了优异的成绩。&lt;h4&gt;结论&lt;/h4&gt;通过提出的方法和技术，证明了将InternVideo2应用于视频质量评估的有效性，并为开发适用于这一领域的高效解决方案提供了依据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video quality assessment tasks rely heavily on the rich features required forvideo understanding, such as semantic information, texture, and temporalmotion. The existing video foundational model, InternVideo2, has demonstratedstrong potential in video understanding tasks due to its large parameter sizeand large-scale multimodal data pertaining. Building on this, we explored thetransferability of InternVideo2 to video quality assessment under compressionscenarios. To design a lightweight model suitable for this task, we proposed adistillation method to equip the smaller model with rich compression qualitypriors. Additionally, we examined the performance of different backbones duringthe distillation process. The results showed that, compared to other methods,our lightweight model distilled from InternVideo2 achieved excellentperformance in compression video quality assessment.</description>
      <author>example@mail.com (Fengbin Guan, Zihao Yu, Yiting Lu, Xin Li, Zhibo Chen)</author>
      <guid isPermaLink="false">2502.19026v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>TabGLM: Tabular Graph Language Model for Learning Transferable Representations Through Multi-Modal Consistency Minimization</title>
      <link>http://arxiv.org/abs/2502.18847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;处理表格数据集中的异构数据对深度学习模型构成了重大挑战。&lt;h4&gt;背景&lt;/h4&gt;注意力机制架构和自监督学习方法在某些领域取得了显著成功，但在应对表格数据时不如线性或基于树的模型有效。虽然有一些突破是通过将表格转换为图像、语言或图等单一模态来实现的，但这些方法在面对特征异质性时通常表现不佳。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，我们引入了一种新型多模态架构TabGLM（表征图语言模型），旨在同时建模表格中的结构信息和语义信息。&lt;h4&gt;方法&lt;/h4&gt;TabGLM将表格的每一行转换为一个完全连接的图和序列化文本，然后分别使用图神经网络(GNN)和文本编码器对其进行编码。通过联合多模态自监督学习目标对齐这些表示，从而利用来自两种模式的互补信息来增强特征学习。&lt;h4&gt;主要发现&lt;/h4&gt;TabGLM采用灵活的图-文本流水线处理异构数据集，在现有深度学习方法中使用显著更少的参数。在25个基准数据集上的评估显示了性能的重大提升，与最先进（SoTA）的表格学习方法相比，TabGLM实现了高达5.56%的平均AUC-ROC改进。&lt;h4&gt;结论&lt;/h4&gt;这项研究提出了一个创新的方法来处理异构表格数据，并通过在多个实际数据集上显著优于现有方法证明了其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Handling heterogeneous data in tabular datasets poses a significant challengefor deep learning models. While attention-based architectures andself-supervised learning have achieved notable success, their application totabular data remains less effective over linear and tree based models. Althoughseveral breakthroughs have been achieved by models which transform tables intouni-modal transformations like image, language and graph, these models oftenunderperform in the presence of feature heterogeneity. To address this gap, weintroduce TabGLM (Tabular Graph Language Model), a novel multi-modalarchitecture designed to model both structural and semantic information from atable. TabGLM transforms each row of a table into a fully connected graph andserialized text, which are then encoded using a graph neural network (GNN) anda text encoder, respectively. By aligning these representations through ajoint, multi-modal, self-supervised learning objective, TabGLM leveragescomplementary information from both modalities, thereby enhancing featurelearning. TabGLM's flexible graph-text pipeline efficiently processesheterogeneous datasets with significantly fewer parameters over existing DeepLearning approaches. Evaluations across 25 benchmark datasets demonstratesubstantial performance gains, with TabGLM achieving an average AUC-ROCimprovement of up to 5.56% over State-of-the-Art (SoTA) tabular learningmethods.</description>
      <author>example@mail.com (Anay Majee, Maria Xenochristou, Wei-Peng Chen)</author>
      <guid isPermaLink="false">2502.18847v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>KAN-powered large-target detection for automotive radar</title>
      <link>http://arxiv.org/abs/2502.19000v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新颖的雷达信号检测管道，用于检测大型目标如汽车和SUV。它基于Range-Doppler（RD）段的概率密度函数(pdf)，利用Kolmogorov-Arnold神经网络(KAN)来学习数据并生成二元假设的可解释符号表达式。&lt;h4&gt;背景&lt;/h4&gt;传统的方法，例如有序统计恒虚警率(OS-CFAR)，在汽车雷达中广泛应用。然而这些方法通常设计用于点目标或等向性目标模型，可能无法充分捕捉大型目标（如车辆）的Range-Doppler散射模式，特别是在高分辨率雷达系统中。&lt;h4&gt;目的&lt;/h4&gt;为了克服传统OS-CFAR检测技术在大型目标探测中的局限性，提出了一种新的基于概率密度函数和Kolmogorov-Arnold神经网络的检测方法。&lt;h4&gt;方法&lt;/h4&gt;研究通过Monte Carlo实验表明所提出的基于KAN表达式的检测性能优于传统的OS-CFAR。此外，该方法在使用现场数据进行迁移学习时，展示了96%的目标检出概率（PD），并且虚警率(PFA)与设计为$10^{-6}$的OS-CFAR相当。&lt;h4&gt;主要发现&lt;/h4&gt;研究还探讨了RD段pdf表示的分块数量对基于KAN检测性能的影响。实验表明提出的KAN表达式能够有效地处理大目标，而不需要额外的关联和跟踪模块来优化多视图下的探测结果。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法在高分辨率雷达系统中对于大型目标（如汽车）的检测表现出优越性，尤其是在检出概率方面超过了传统方法，并且虚警率控制得当。&lt;h4&gt;翻译&lt;/h4&gt;该论文介绍了一种新颖的雷达信号检测流程，专门用于探测大型目标，比如汽车和SUV。传统的有序统计恒虚警率(OS-CFAR)方法通常适用于点目标或各向同性模型，在高分辨率系统中可能无法充分表现大目标（如车辆）的Range-Doppler散射特性。新的方法基于Kolmogorov-Arnold神经网络(KAN)，能够学习数据并生成二元假设的概率密度函数，该方法优于传统的OS-CFAR，并且在使用现场数据进行迁移学习时达到了96%的目标检出率和相当的虚警率水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel radar signal detection pipeline focused ondetecting large targets such as cars and SUVs. Traditional methods, such asOrdered-Statistic Constant False Alarm Rate (OS-CFAR), commonly used inautomotive radar, are designed for point or isotropic target models. These maynot adequately capture the Range-Doppler (RD) scattering patterns of largertargets, especially in high-resolution radar systems. Additional modules suchas association and tracking are necessary to refine and consolidate thedetections over multiple dwells. To address these limitations, we propose adetection technique based on the probability density function (pdf) of RDsegments, leveraging the Kolmogorov-Arnold neural network (KAN) to learn thedata and generate interpretable symbolic expressions for binary hypotheses.Beside the Monte-Carlo study showing better performance for the proposed KANexpression over OS-CFAR, it is shown to exhibit a probability of detection (PD)of 96% when transfer learned with field data. The false alarm rate (PFA) iscomparable with OS-CFAR designed with PFA = $10^{-6}$. Additionally, the studyalso examines impact of the number of pdf bins representing RD segment onperformance of the KAN-based detection.</description>
      <author>example@mail.com (Vinay Kulkarni, V. V. Reddy, Neha Maheshwari)</author>
      <guid isPermaLink="false">2502.19000v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>(Mis)Fitting: A Survey of Scaling Laws</title>
      <link>http://arxiv.org/abs/2502.18969v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  41 pages, 3 figure, first two authors contributed equally. ICLR, 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现代基础模型依赖于使用缩放定律来指导重要的训练决策。研究者通常通过描述损失或任务性能与规模之间的关系从较小的训练运行中推断出最优架构和超参数设置。&lt;h4&gt;目的&lt;/h4&gt;讨论不同先驱工作在诸如最佳token到参数比率等问题上得出结论时存在的差异，并且探讨特定细节变化对缩放研究结果的影响以及因此导致的不同结论。此外，调查了超过50篇研究缩放趋势的论文：其中45篇使用幂定律来量化这些趋势，但大多数未能报告复制其发现所需的关键细节。&lt;h4&gt;方法&lt;/h4&gt;作者通过自己的分析来补充讨论，并为贡献于缩放律研究的研究者提出了一份检查清单，以帮助他们更好地报告关键细节。&lt;h4&gt;主要发现&lt;/h4&gt;不同的因素（如拟合的具体方程、训练设置和优化方法）可能会影响拟合的规律，进而影响给定研究的结论。大多数关于缩放趋势的研究未能充分报告必要的技术细节。&lt;h4&gt;结论&lt;/h4&gt;为了减轻这些问题的影响，作者建议所有贡献者在进行缩放律研究时参考提供的检查清单来确保他们提供了足够的细节以允许其他人重现他们的结果。&lt;h4&gt;翻译&lt;/h4&gt;现代基础模型依赖于使用扩展法则指导关键训练决策。研究人员通常从较小规模的训练中推导出最优架构和超参数设置，通过描述损失或任务性能与规模之间的关系来进行这一过程。该过程中所有因素的变化（如具体拟合方程、训练配置、优化方法等）都可能影响到得出的规则，并进而影响研究结论。论文作者讨论了先前工作的不同结论，包括有关最佳token-参数比率的问题，同时结合自身分析结果来探讨细节变化对缩放研究的影响及结论的改变。另外，调查了50余篇关于扩展趋势的研究：其中45项使用幂律量化趋势，但大多数未能充分报告实现其发现所需的详细信息。为解决这一问题，论文作者建议在进行扩展现有规则的研究时遵循特定检查清单。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Modern foundation models rely heavily on using scaling laws to guide crucialtraining decisions. Researchers often extrapolate the optimal architecture andhyper parameters settings from smaller training runs by describing therelationship between, loss, or task performance, and scale. All components ofthis process vary, from the specific equation being fit, to the training setup,to the optimization method. Each of these factors may affect the fitted law,and therefore, the conclusions of a given study. We discuss discrepancies inthe conclusions that several prior works reach, on questions such as theoptimal token to parameter ratio. We augment this discussion with our ownanalysis of the critical impact that changes in specific details may effect ina scaling study, and the resulting altered conclusions. Additionally, we surveyover 50 papers that study scaling trends: while 45 of these papers quantifythese trends using a power law, most under-report crucial details needed toreproduce their findings. To mitigate this, we we propose a checklist forauthors to consider while contributing to scaling law research.</description>
      <author>example@mail.com (Margaret Li, Sneha Kudugunta, Luke Zettlemoyer)</author>
      <guid isPermaLink="false">2502.18969v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning Assisted Fast Design Migration Over Technology Nodes: A Study on Transformer Matching Network</title>
      <link>http://arxiv.org/abs/2502.18636v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Publihsed and Presented at IEEE MTT-S International Microwave  Symposium (IMS 2024), Washington, DC, USA&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究介绍了一种新颖的方法，利用预训练的合成神经网络模型的知识转移，在不同集成电路技术、操作频率和金属选项之间快速而可靠地设计毫米波被动网络。&lt;h4&gt;背景&lt;/h4&gt;在毫米波通信领域，传统的电路设计方法耗时且依赖于专业知识。引入了知识迁移的概念来提高设计效率。&lt;h4&gt;目的&lt;/h4&gt;通过模拟演示验证从一个技术节点的预训练合成神经网络模型的知识转移可以加速目标领域的训练过程，并提高R2值。&lt;h4&gt;方法&lt;/h4&gt;使用GF 45nm SOI（源领域）中经过训练的模型，将知识转移到GF 22nm FDX+（目标领域），并对1:1片上变压器的设计进行比较和分析。实验探索了不同数据密度的影响并评估了R2值。&lt;h4&gt;主要发现&lt;/h4&gt;知识转移可以显著减少所需的数据集大小，并且在源域和目标域之间表现出优秀的泛化能力，特别是在低数据密度下的性能提升明显。&lt;h4&gt;结论&lt;/h4&gt;研究结果证明，利用迁移学习可以有效提高毫米波被动网络设计的效率和精度。该方法可以在不同集成电路技术和操作频率下加速模型训练并获得更好的R2值。&lt;h4&gt;翻译&lt;/h4&gt;在该项研究中，我们引入了一种创新性的毫米波无源网络设计方法，这种方法使用预先训练好的合成神经网络模型的知识转移，在不同的集成电路上实现快速且可靠的设计调整。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/IMS40175.2024.10600344&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this study, we introduce an innovative methodology for the design ofmm-Wave passive networks that leverages knowledge transfer from a pre-trainedsynthesis neural network (NN) model in one technology node and achieves swiftand reliable design adaptation across different integrated circuit (IC)technologies, operating frequencies, and metal options. We prove this conceptthrough simulation-based demonstrations focusing on the training and comparisonof the coefficient of determination (R2) of synthesis NNs for 1:1 on-chiptransformers in GlobalFoundries(GF) 22nm FDX+ (target domain), with and withouttransfer learning from a model trained in GF 45nm SOI (source domain). In theexperiments, we explore varying target data densities of 0.5%, 1%, 5%, and 100%with a complete dataset of 0.33 million in GF 22FDX+, and for comparativeanalysis, apply source data densities of 25%, 50%, 75%, and 100% with acomplete dataset of 2.5 million in GF 45SOI. With the source data only at30GHz, the experiments span target data from two metal options in GF 22FDX+ atfrequencies of 30 and 39 GHz. The results prove that the transfer learning withthe source domain knowledge (GF 45SOI) can both accelerate the training processin the target domain (GF 22FDX+) and improve the R2 values compared to modelswithout knowledge transfer. Furthermore, it is observed that a model trainedwith just 5% of target data and augmented by transfer learning achieves R2values superior to a model trained with 20% of the data without transfer,validating the advantage seen from 1% to 5% data density. This demonstrates anotable reduction of 4X in the necessary dataset size highlighting the efficacyof utilizing transfer learning to mm-Wave passive network design. The PyTorchlearning and testing code is publicly available athttps://github.com/ChenhaoChu/RFIC-TL.</description>
      <author>example@mail.com (Chenhao Chu, Yuhao Mao, Hua Wang)</author>
      <guid isPermaLink="false">2502.18636v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>MaskPlanner: Learning-Based Object-Centric Motion Generation from 3D Point Clouds</title>
      <link>http://arxiv.org/abs/2502.18745v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website at https://gabrieletiboni.github.io/MaskPlanner/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种新的、完全数据驱动的框架，用于基于3D点云直接处理对象中心运动生成（OCMG）问题。&lt;h4&gt;背景&lt;/h4&gt;现有的解决方案依赖于专门的启发式方法、昂贵的优化过程或限制性的几何假设，这些都降低了它们在实际场景中的适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的学习框架MaskPlanner来解决OCMG问题，该框架可以从3D点云中直接预测局部路径段并进行分组，以满足任务需求。&lt;h4&gt;方法&lt;/h4&gt;MaskPlanner是一种深度学习方法，能够为给定的对象预测局部路径片段，并同时推断出“路径掩码”，将这些片段归类为不同的路径。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法对于未见过的对象能达到近乎完整的覆盖（超过99%），且无需显式优化喷漆沉积。此外，在6自由度专业喷涂机器人上的真实世界验证中展示了生成的轨迹可以直接执行并达到专家级的喷涂质量。&lt;h4&gt;结论&lt;/h4&gt;研究结果突出了所提出的学习方法在减少工程负担方面和无缝适应多种工业应用案例中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;对象中心运动生成（OCMG）在各种工业应用中起着关键作用，例如机器人喷漆和焊接。需要高效、可扩展且通用的算法来为自由形状3D物体规划多个长期轨迹。然而，现有的解决方案依赖于专门的启发式方法、昂贵的优化过程或限制性的几何假设，这限制了它们在实际场景中的适应性。本文提出了一种新的完全数据驱动框架，直接从3D点云处理OCMG问题，并学习如何跨自由形状表面泛化专家路径模式。我们提出了MaskPlanner深度学习方法，该方法预测给定物体的局部路径片段并同时推断“路径掩码”，将这些片段分类为不同的路径，从而让网络在单次前向传递中捕获局部几何模式和全局任务需求。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object-Centric Motion Generation (OCMG) plays a key role in a variety ofindustrial applications$\unicode{x2014}$such as robotic spray painting andwelding$\unicode{x2014}$requiring efficient, scalable, and generalizablealgorithms to plan multiple long-horizon trajectories over free-form 3Dobjects. However, existing solutions rely on specialized heuristics, expensiveoptimization routines, or restrictive geometry assumptions that limit theiradaptability to real-world scenarios. In this work, we introduce a novel, fullydata-driven framework that tackles OCMG directly from 3D point clouds, learningto generalize expert path patterns across free-form surfaces. We proposeMaskPlanner, a deep learning method that predicts local path segments for agiven object while simultaneously inferring "path masks" to group thesesegments into distinct paths. This design induces the network to capture bothlocal geometric patterns and global task requirements in a single forward pass.Extensive experimentation on a realistic robotic spray painting scenario showsthat our approach attains near-complete coverage (above 99%) for unseenobjects, while it remains task-agnostic and does not explicitly optimize forpaint deposition. Moreover, our real-world validation on a 6-DoF specializedpainting robot demonstrates that the generated trajectories are directlyexecutable and yield expert-level painting quality. Our findings cruciallyhighlight the potential of the proposed learning method for OCMG to reduceengineering overhead and seamlessly adapt to several industrial use cases.</description>
      <author>example@mail.com (Gabriele Tiboni, Raffaello Camoriano, Tatiana Tommasi)</author>
      <guid isPermaLink="false">2502.18745v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Transfer Learning for Transient Classification: From Simulations to Real Data and ZTF to LSST</title>
      <link>http://arxiv.org/abs/2502.18558v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 3 figures, 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过转移学习的方法，展示了如何利用已有的模型来处理天文瞬变现象的分类问题。这种方法可以显著减少标记数据的需求，并且保持与从头开始训练的模型相当的性能。&lt;h4&gt;背景&lt;/h4&gt;机器学习在自动分类天文学中的瞬变现象中起着关键作用，但现有方法面临着诸多挑战：基于模拟数据训练的分类器难以处理真实数据；为一个调查定制的模型难以应用于其他调查。随着大型天文观测设施如维拉·鲁宾天文台（Legacy Survey of Space and Time, LSST）时代的到来，这些问题变得更加紧迫。&lt;h4&gt;目的&lt;/h4&gt;本文旨在展示转移学习如何克服现有方法面临的挑战，使得在新调查开始初期就能实现可靠的自动化分类。&lt;h4&gt;方法&lt;/h4&gt;使用了一种基于模拟数据训练的模型，并通过转移学习技术将其应用于实际数据和不同调查的数据。具体而言，从一个利用模拟Zwicky瞬变设施（ZTF）光曲线训练的模型出发，展示了这种技术如何减少75%的真实标记ZTF瞬变现象的需求，同时保持与全新训练模型相同的性能。&lt;h4&gt;主要发现&lt;/h4&gt;1. 转移学习可以在很大程度上减少了所需的真实标记数据量；2. 当将ZTF模型应用于LSST模拟时，转移学习可以达到基准性能的95%，而只需要30%的训练数据。这些结果对即将启动的LSST早期操作具有重要影响。&lt;h4&gt;结论&lt;/h4&gt;通过转移学习，即使在新调查开始初期也能实现可靠的自动化分类，无需等待数月或数年积累足够的标记训练数据。&lt;h4&gt;翻译&lt;/h4&gt;机器学习对于天文瞬变现象的自动分类至关重要，但当前方法面临诸多限制：基于模拟训练的分类器难以处理真实数据；为一个观测定制的模型难以应用于其他调查。随着大型天文设施如LSST时代的到来，现有模型将需要重新利用LSST的数据进行再培训。本文证明了转移学习可以通过重用已有的基于模拟或来自其它调查的数据训练的模型来克服这些挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine learning has become essential for automated classification ofastronomical transients, but current approaches face significant limitations:classifiers trained on simulations struggle with real data, models developedfor one survey cannot be easily applied to another, and new surveys requireprohibitively large amounts of labelled training data. These challenges areparticularly pressing as we approach the era of the Vera Rubin Observatory'sLegacy Survey of Space and Time (LSST), where existing classification modelswill need to be retrained using LSST observations. We demonstrate that transferlearning can overcome these challenges by repurposing existing models trainedon either simulations or data from other surveys. Starting with a model trainedon simulated Zwicky Transient Facility (ZTF) light curves, we show thattransfer learning reduces the amount of labelled real ZTF transients needed by75\% while maintaining equivalent performance to models trained from scratch.Similarly, when adapting ZTF models for LSST simulations, transfer learningachieves 95\% of the baseline performance while requiring only 30\% of thetraining data. These findings have significant implications for the earlyoperations of LSST, suggesting that reliable automated classification will bepossible soon after the survey begins, rather than waiting months or years toaccumulate sufficient training data.</description>
      <author>example@mail.com (Rithwik Gupta, Daniel Muthukrishna)</author>
      <guid isPermaLink="false">2502.18558v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Graph Tasks with Pure LLMs: A Comprehensive Benchmark and Investigation</title>
      <link>http://arxiv.org/abs/2502.18771v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了大型语言模型（LLMs）在图学习任务中的应用，并评估了它们在不同场景下的性能，特别是在少样本/零样本设置、跨领域迁移能力以及对图形结构的理解和鲁棒性方面。&lt;h4&gt;背景&lt;/h4&gt;随着图状数据的普遍使用，传统的图神经网络（GNNs）虽然取得了一些进展，但在某些上下文中处理图数据的能力仍然有限。大型语言模型因其在处理限制数据、任务间转移性和鲁棒性的潜力而成为新的研究热点。&lt;h4&gt;目的&lt;/h4&gt;全面探索LLMs在图学习任务中的应用，并评估其性能，以揭示它们的优点和潜在的现实世界应用场景。&lt;h4&gt;方法&lt;/h4&gt;研究比较了16种图学习模型与6种大型语言模型（如Llama3B、GPT-4o、Qwen-plus）在Cora、PubMed、ArXiv等数据集上的表现。评估包括未进行参数优化和经过指令微调的LLMs。&lt;h4&gt;主要发现&lt;/h4&gt;具有指令微调的LLMs在少样本设置中优于传统模型，表现出强大的跨领域迁移能力，并展示了优秀的泛化性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;该研究为大型语言模型用于图学习提供了有价值的见解，揭示了它们的优势和潜在的应用场景，为未来的研究铺平道路。代码与数据集可在GitHub上获得（https://github.com/myflashbarry/LLM-benchmarking）。&lt;h4&gt;翻译&lt;/h4&gt;随着图状数据在各个领域的日益普及，对有效处理节点分类和链接预测等任务的模型的需求也在增长。尽管传统的图学习模型如图神经网络已经取得了显著进展，但在某些上下文中的能力仍然有限。近年来，大型语言模型作为潜在解决方案崭露头角，但大多数研究主要关注性能基准测试，未能充分探讨其广泛潜力，包括处理限制数据的能力、任务间的转移性和鲁棒性等。本工作全面探索了大型语言模型应用于图学习任务，并评估了纯LLM（包含未优化参数和指令微调）在各种情况下的表现。我们的分析不仅局限于准确性，还涉及LLMs在少样本/零样本设置中的能力、跨领域迁移能力、理解图形结构的能力以及在挑战性场景中的鲁棒性等。我们在16种图学习模型与如Llama3B、GPT-4o、Qwen-plus等六种大型语言模型之间进行了广泛的实验，对比了它们在Cora、PubMed、ArXiv和Products等数据集上的表现。我们的研究发现显示，在少样本设置中，具有指令微调的LLMs优于传统的图学习模型，表现出强大的跨领域迁移能力，并展示了优秀的泛化性和鲁棒性。这项工作为大型语言模型用于图学习提供了宝贵的见解，突显了它们的优势和潜在的实际应用价值，并为未来的研究铺平道路。代码与数据集可在https://github.com/myflashbarry/LLM-benchmarking上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/myflashbarry/LLM-benchmarking&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph-structured data has become increasingly prevalent across variousdomains, raising the demand for effective models to handle graph tasks likenode classification and link prediction. Traditional graph learning models likeGraph Neural Networks (GNNs) have made significant strides, but theircapabilities in handling graph data remain limited in certain contexts. Inrecent years, large language models (LLMs) have emerged as promising candidatesfor graph tasks, yet most studies focus primarily on performance benchmarks andfail to address their broader potential, including their ability to handlelimited data, their transferability across tasks, and their robustness. In thiswork, we provide a comprehensive exploration of LLMs applied to graph tasks. Weevaluate the performance of pure LLMs, including those without parameteroptimization and those fine-tuned with instructions, across various scenarios.Our analysis goes beyond accuracy, assessing LLM ability to perform infew-shot/zero-shot settings, transfer across domains, understand graphstructures, and demonstrate robustness in challenging scenarios. We conductextensive experiments with 16 graph learning models alongside 6 LLMs (e.g.,Llama3B, GPT-4o, Qwen-plus), comparing their performance on datasets like Cora,PubMed, ArXiv, and Products. Our findings show that LLMs, particularly thosewith instruction tuning, outperform traditional models in few-shot settings,exhibit strong domain transferability, and demonstrate excellent generalizationand robustness. This work offers valuable insights into the capabilities ofLLMs for graph learning, highlighting their advantages and potential forreal-world applications, and paving the way for future research in this area.Codes and datasets are released inhttps://github.com/myflashbarry/LLM-benchmarking.</description>
      <author>example@mail.com (Yuxiang Wang, Xinnan Dai, Wenqi Fan, Yao Ma)</author>
      <guid isPermaLink="false">2502.18771v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>CommGPT: A Graph and Retrieval-Augmented Multimodal Communication Foundation Model</title>
      <link>http://arxiv.org/abs/2502.18763v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;大语言模型（LLMs）具备人类级别的认知和决策能力，是6G技术的关键。然而，在通信领域应用LLMs面临三大挑战：1) 通信数据不足；2) 输入模态受限；3) 知识检索困难。&lt;h4&gt;背景&lt;/h4&gt;大语言模型在6G中扮演关键角色，但由于其缺乏高质量的通信特定训练数据、只能处理有限输入模态及难以有效检索领域知识，因此很难直接应用于通信场景。&lt;h4&gt;目的&lt;/h4&gt;提出CommGPT，这是一个专门针对通信领域的多模态基础模型。为了克服上述问题，CommGPT旨在提供更好的通信专业知识学习能力，并能够适应多种输入类型，同时提高对已有知识的利用效率。&lt;h4&gt;方法&lt;/h4&gt;1. 创建高质预训练和微调数据集；2. 设计用于理解处理多样化信息输入的多模态编码器；3. 构建图谱增强检索增强生成框架（GRG），以结合知识图谱与检索增强生成技术，实现跨尺度学习。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证了CommGPT的有效性和可行性，表明其在通信领域的应用潜力巨大。&lt;h4&gt;结论&lt;/h4&gt;提出了专门用于通信的多模态基础模型CommGPT，并证明它可以在解决当前LLMs面临的挑战方面提供有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Large Language Models (LLMs) possess human-level cognitive and decision-making capabilities, making them a key technology for 6G. However, applying LLMs to the communication domain faces three major challenges: 1) Inadequate communication data; 2) Restricted input modalities; and 3) Difficulty in knowledge retrieval. To overcome these issues, we propose CommGPT, a multimodal foundation model designed specifically for communications. First, we create high-quality pretraining and fine-tuning datasets tailored to the field of communications, enabling the LLM to engage in further pretraining and fine-tuning with communication concepts and knowledge. Then, we design a multimodal encoder to understand and process information from various input modalities. Next, we construct a Graph and Retrieval-Augmented Generation (GRG) framework, efficiently coupling Knowledge Graph (KG) with Retrieval-Augmented Generation (RAG) for multi-scale learning. Finally, we demonstrate the feasibility and effectiveness of the CommGPT through experimental validation.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large Language Models (LLMs) possess human-level cognitive anddecision-making capabilities, making them a key technology for 6G. However,applying LLMs to the communication domain faces three major challenges: 1)Inadequate communication data; 2) Restricted input modalities; and 3)Difficulty in knowledge retrieval. To overcome these issues, we proposeCommGPT, a multimodal foundation model designed specifically forcommunications. First, we create high-quality pretraining and fine-tuningdatasets tailored in communication, enabling the LLM to engage in furtherpretraining and fine-tuning with communication concepts and knowledge. Then, wedesign a multimodal encoder to understand and process information from variousinput modalities. Next, we construct a Graph and Retrieval-Augmented Generation(GRG) framework, efficiently coupling Knowledge Graph (KG) withRetrieval-Augmented Generation (RAG) for multi-scale learning. Finally, wedemonstrate the feasibility and effectiveness of the CommGPT throughexperimental validation.</description>
      <author>example@mail.com (Feibo Jiang, Wanyun Zhu, Li Dong, Kezhi Wang, Kun Yang, Cunhua Pan, Octavia A. Dobre)</author>
      <guid isPermaLink="false">2502.18763v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Spectral-Enhanced Transformers: Leveraging Large-Scale Pretrained Models for Hyperspectral Object Tracking</title>
      <link>http://arxiv.org/abs/2502.18748v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to 14th Workshop on Hyperspectral Imaging and Signal  Processing: Evolution in Remote Sensing (WHISPERS)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种利用预训练的大型Transformer模型进行高光谱对象跟踪的有效方法。&lt;h4&gt;背景&lt;/h4&gt;基于快照马赛克相机的高光谱目标追踪技术因提供增强的光谱信息和空间数据而备受关注，这有助于更全面地理解材料属性。然而，大规模Transformer的训练需要大量的数据集和长时间的训练过程，这对于高光谱领域现有的小规模数据集来说是一个瓶颈。&lt;h4&gt;目的&lt;/h4&gt;开发一种适应大型预训练基础模型的方法，用于高光谱对象跟踪，并通过跨模态训练管道促进不同传感器收集的数据之间的有效学习。&lt;h4&gt;方法&lt;/h4&gt;提出了一种自适应、可学习的空间-光谱令牌融合模块，可以扩展到任何基于Transformer的骨干网络中，以学习高光谱数据中的固有空间-光谱特征。此外，模型还包含一个跨模态训练管道，允许在不同传感器模式下收集的数据之间进行有效的跨域学习。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在较少的训练迭代次数下实现优越性能。&lt;h4&gt;结论&lt;/h4&gt;通过上述创新方法，高光谱对象跟踪任务能够克服现有数据集规模较小的问题，并利用Transformer的强大功能来提升追踪效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hyperspectral object tracking using snapshot mosaic cameras is emerging as itprovides enhanced spectral information alongside spatial data, contributing toa more comprehensive understanding of material properties. Using transformers,which have consistently outperformed convolutional neural networks (CNNs) inlearning better feature representations, would be expected to be effective forHyperspectral object tracking. However, training large transformersnecessitates extensive datasets and prolonged training periods. This isparticularly critical for complex tasks like object tracking, and the scarcityof large datasets in the hyperspectral domain acts as a bottleneck in achievingthe full potential of powerful transformer models. This paper proposes aneffective methodology that adapts large pretrained transformer-based foundationmodels for hyperspectral object tracking. We propose an adaptive, learnablespatial-spectral token fusion module that can be extended to anytransformer-based backbone for learning inherent spatial-spectral features inhyperspectral data. Furthermore, our model incorporates a cross-modalitytraining pipeline that facilitates effective learning across hyperspectraldatasets collected with different sensor modalities. This enables theextraction of complementary knowledge from additional modalities, whether ornot they are present during testing. Our proposed model also achieves superiorperformance with minimal training iterations.</description>
      <author>example@mail.com (Shaheer Mohamed, Tharindu Fernando, Sridha Sridharan, Peyman Moghadam, Clinton Fookes)</author>
      <guid isPermaLink="false">2502.18748v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种全自动、无需目标物且在线的多传感器校准框架CalibRefine，该方法能够直接处理原始LiDAR点云和相机图像数据，并通过四个阶段实现高精度的校准。&lt;h4&gt;背景&lt;/h4&gt;在自动驾驶、机器人技术及智能交通系统等应用中，精确的多传感器校准至关重要。现有基于LIDAR-相机的方法通常依赖于手动放置的目标物或预先估计参数，这限制了其在现实环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需人工干预和目标物的全自动在线校准框架CalibRefine，以解决传统方法中存在的问题，并提高多传感器系统在校准过程中的鲁棒性和准确性。&lt;h4&gt;方法&lt;/h4&gt;['第一阶段：通过使用相对位置、外观嵌入和语义类别自动检测对象，训练通用特征鉴别器生成可靠的LIDAR-相机对应关系', '第二阶段：基于粗略的同态变换进行校准', '第三阶段：迭代改进数据帧变得可用时对齐过程', '第四阶段：利用视觉变压器和交叉注意力机制处理非平面扭曲']&lt;h4&gt;主要发现&lt;/h4&gt;通过在两个城市交通数据集上的广泛实验，CalibRefine展示了高精度的校准结果，并且在最少的人工干预下超越了现有的无目标物方法，同时与手动调优基线保持竞争力或优于它们。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了如何利用稳健的对象级特征匹配以及迭代和自我监督的关注机制调整，在复杂的现实世界条件下实现一致的传感器融合，而无需地面实况校准矩阵或复杂的预处理数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Cheng, Lihao Guo, Tianya Zhang, Tam Bang, Austin Harris, Mustafa Hajij, Mina Sartipi, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v2</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Like Father, Like Son: Kinship-Aware Preference Mapping (KARMA) for Automatic Alignment in Large Language Models</title>
      <link>http://arxiv.org/abs/2502.18744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages,5 figures,3 tables,4 graphs&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Kinship-Aware Preference MApping (KARMA)框架，该框架通过系统地配对具有相似能力的模型响应来改进大型语言模型（LLM）的行为与人类偏好的一致性的方法。&lt;h4&gt;背景&lt;/h4&gt;当前在大型语言模型（LLM）对齐方面取得的进步试图减少人工注释的成本，利用预训练模型生成偏好数据。然而，现有的方法通常比较来自能力显著不同的模型的响应，这些浅层次的区别未能提供有意义的指导以说明哪种回应更优。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有方法存在的局限性，提出了一种新的框架KARMA，该框架系统地配对具有类似能力的模型之间的响应。&lt;h4&gt;方法&lt;/h4&gt;通过将偏好比较约束为复杂度和质量相似的输出，增强了偏好数据的信息量，并提高了对齐信号的粒度。&lt;h4&gt;主要发现&lt;/h4&gt;实证评估表明，我们的亲缘关系感知的方法导致了更一致且可解释的对齐结果。&lt;h4&gt;结论&lt;/h4&gt;这种方法最终促进了将LLM行为与人类偏好对齐的原则性和可靠路径。&lt;h4&gt;翻译&lt;/h4&gt;最近在大型语言模型（LLM）对齐方面的进展试图通过利用预训练模型生成偏好数据来减少人工注释的成本。然而，现有的方法往往比较来自能力显著不同的模型的响应，这些差异未能提供有意义的指导以说明哪种回应更优。为了解决这一限制，我们提出了亲缘关系感知的偏好数据映射（KARMA）框架，它系统地配对具有相似能力的模型之间的响应。通过将偏好比较约束为复杂度和质量相似的输出，KARMA增强了偏好数据的信息量，并提高了对齐信号的粒度。实证评估表明，我们的亲缘关系感知的方法导致了更一致且可解释的对齐结果，最终促进了将LLM行为与人类偏好对齐的原则性和可靠路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in Large Language Model (LLM) alignment have sought tomitigate the cost of human annotations by leveraging pretrained models togenerate preference data. However, existing methods often compare responsesfrom models with substantially different capabilities, yielding superficialdistinctions that fail to provide meaningful guidance on what constitutes asuperior response. To address this limitation, we propose Kinship-AwarepReference MApping (KARMA), a novel framework that systematically pairsresponses from models with comparable competencies. By constraining preferencecomparisons to outputs of similar complexity and quality, KARMA enhances theinformativeness of preference data and improves the granularity of alignmentsignals. Empirical evaluations demonstrate that our kinship-aware approachleads to more consistent and interpretable alignment outcomes, ultimatelyfacilitating a more principled and reliable pathway for aligning LLM behaviorwith human preferences.</description>
      <author>example@mail.com (Jeesu Jung, Chanjun Park, Sangkeun Jung)</author>
      <guid isPermaLink="false">2502.18744v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>What are Foundation Models Cooking in the Post-Soviet World?</title>
      <link>http://arxiv.org/abs/2502.18583v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过构建BORSch数据集，评估了基础模型对后苏联地区饮食文化的理解能力。&lt;h4&gt;背景&lt;/h4&gt;后苏联国家的文化复杂且深受历史影响，这种文化持续影响着当前的事件和人们的生活方式。&lt;h4&gt;目的&lt;/h4&gt;调查大型语言模型对于后苏联区域饮食文化知识的理解程度。&lt;h4&gt;方法&lt;/h4&gt;构建了一个包含1147道俄罗斯菜和823道乌克兰菜肴的多模态数据集BORSch。使用该数据集评估了现有基础模型在文本问答（QA）以及跨模态问答中的表现，并进一步测试这些模型生成准确视觉描述的能力。&lt;h4&gt;主要发现&lt;/h4&gt;主导的基础模型在识别后苏联国家菜系起源时遇到困难，往往过度预测与问题语言相关的国家；这些结果可以通过训练数据中误导性的菜品和来源共现现象来解释，以及俄语和乌克兰语之间的代码混合等语言学现象。&lt;h4&gt;结论&lt;/h4&gt;单纯基于问答评估文化理解可能不足以全面评价模型的能力；为了促进进一步研究，BORSch将公开发布在GitHub上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The culture of the Post-Soviet states is complex, shaped by a turbulenthistory that continues to influence current events. In this study, weinvestigate the Post-Soviet cultural food knowledge of foundation models byconstructing BORSch, a multimodal dataset encompassing 1147 and 823 dishes inthe Russian and Ukrainian languages, centered around the Post-Soviet region. Wedemonstrate that leading models struggle to correctly identify the origins ofdishes from Post-Soviet nations in both text-only and multimodal QuestionAnswering (QA), instead over-predicting countries linked to the language thequestion is asked in. Through analysis of pretraining data, we show that theseresults can be explained by misleading dish-origin co-occurrences, along withlinguistic phenomena such as Russian-Ukrainian code mixing. Finally, to movebeyond QA-based assessments, we test models' abilities to produce accuratevisual descriptions of dishes. The weak correlation between this task and QAsuggests that QA alone may be insufficient as an evaluation of culturalunderstanding. To foster further research, we will make BORSch publiclyavailable at https://github.com/alavrouk/BORSch.</description>
      <author>example@mail.com (Anton Lavrouk, Tarek Naous, Alan Ritter, Wei Xu)</author>
      <guid isPermaLink="false">2502.18583v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Rewards-based image analysis in microscopy</title>
      <link>http://arxiv.org/abs/2502.18522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  38 pages, 11 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;分析成像和高光谱数据在生物学、医学、化学和物理学等领域中至关重要。目标是将高质量或高维的数据转换为可解释的格式，以便生成有价值的见解。&lt;h4&gt;目的&lt;/h4&gt;研究如何优化成像及高光谱数据分析流程，以减少对人工输入的需求，并提升自动化水平和决策支持能力。&lt;h4&gt;方法&lt;/h4&gt;讨论了基于奖励的工作流程的发展，这些工作流程采用了专家决策原则，并展示了强大的跨任务迁移学习。这种方法代表图像分析为一系列可能的操作上的决策过程。&lt;h4&gt;主要发现&lt;/h4&gt;通过引入基于奖励的框架，可以实现从监督式、黑盒模型向解释性更强、无监督且鲁棒性强的优化方法转变。&lt;h4&gt;结论&lt;/h4&gt;这些工作流程既可作为经典和深度卷积神经网络（DCNN）方法上的包装器使用，又可在无监督和有监督的工作流中发挥作用，适用于各种图像分析和高光谱数据任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容关于成像及高光谱数据分析领域的研究进展，强调了引入机器学习加速特定任务的重要性，并探讨了未来通过奖励驱动工作流程优化此类任务的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Analyzing imaging and hyperspectral data is crucial across scientific fields,including biology, medicine, chemistry, and physics. The primary goal is totransform high-resolution or high-dimensional data into an interpretable formatto generate actionable insights, aiding decision-making and advancingknowledge. Currently, this task relies on complex, human-designed workflowscomprising iterative steps such as denoising, spatial sampling, keypointdetection, feature generation, clustering, dimensionality reduction, andphysics-based deconvolutions. The introduction of machine learning over thepast decade has accelerated tasks like image segmentation and object detectionvia supervised learning, and dimensionality reduction via unsupervised methods.However, both classical and NN-based approaches still require human input,whether for hyperparameter tuning, data labeling, or both. The growing use ofautomated imaging tools, from atomically resolved imaging to biologicalapplications, demands unsupervised methods that optimize data representationfor human decision-making or autonomous experimentation. Here, we discussadvances in reward-based workflows, which adopt expert decision-makingprinciples and demonstrate strong transfer learning across diverse tasks. Werepresent image analysis as a decision-making process over possible operationsand identify desiderata and their mappings to classical decision-makingframeworks. Reward-driven workflows enable a shift from supervised, black-boxmodels sensitive to distribution shifts to explainable, unsupervised, androbust optimization in image analysis. They can function as wrappers overclassical and DCNN-based methods, making them applicable to both unsupervisedand supervised workflows (e.g., classification, regression forstructure-property mapping) across imaging and hyperspectral data.</description>
      <author>example@mail.com (Kamyar Barakati, Yu Liu, Utkarsh Pratiush, Boris N. Slautin, Sergei V. Kalinin)</author>
      <guid isPermaLink="false">2502.18522v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>What Is That Talk About? A Video-to-Text Summarization Dataset for Scientific Presentations</title>
      <link>http://arxiv.org/abs/2502.08279v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要主题&lt;/h4&gt;视频到文本的总结，特别是在科学领域中的应用。&lt;h4&gt;背景信息&lt;/h4&gt;将录制的视频转换为简洁准确的文字概述是多模态学习中日益增长的一项挑战。&lt;h4&gt;目的声明&lt;/h4&gt;介绍VISTA数据集，该数据集专门用于科学研究领域的视频至文字概要生成。&lt;h4&gt;研究方法&lt;/h4&gt;包括两个方面：一是收集并整理18,599个AI会议演讲记录及其对应的论文摘要；二是评估最先进的大型模型，并应用基于规划的框架以更好地捕捉摘要结构特性。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，显式计划可以增强总结质量和事实一致性。然而，模型与人类表现之间仍存在显著差距。&lt;h4&gt;结论陈述&lt;/h4&gt;尽管现有方法取得了一定的进步，但科学视频概要生成领域依旧面临诸多挑战。&lt;h4&gt;翻译&lt;/h4&gt;将记录的视频转换成简洁准确的文字摘要在多模态学习中是一个日益增长的难题。本文介绍了一个专门用于科学研究领域的视频到文本总结的数据集VISTA，该数据集中包含了18,599个AI会议演讲及其对应的论文摘要。我们评估了最先进的大型模型，并应用了一种基于规划的方法来更好地捕捉摘要的结构特性。无论是人工还是自动评估都证实，明确的计划能够提高概要质量和事实一致性。然而，模型与人类表现之间仍存在显著差距，这突显了科学视频总结面临的挑战。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-12&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transforming recorded videos into concise and accurate textual summaries is agrowing challenge in multimodal learning. This paper introduces VISTA, adataset specifically designed for video-to-text summarization in scientificdomains. VISTA contains 18,599 recorded AI conference presentations paired withtheir corresponding paper abstracts. We benchmark the performance ofstate-of-the-art large models and apply a plan-based framework to bettercapture the structured nature of abstracts. Both human and automatedevaluations confirm that explicit planning enhances summary quality and factualconsistency. However, a considerable gap remains between models and humanperformance, highlighting the challenges of scientific video summarization.</description>
      <author>example@mail.com (Dongqi Liu, Chenxi Whitehouse, Xi Yu, Louis Mahon, Rohit Saxena, Zheng Zhao, Yifu Qiu, Mirella Lapata, Vera Demberg)</author>
      <guid isPermaLink="false">2502.08279v3</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Revisiting Convolution Architecture in the Realm of DNA Foundation Models</title>
      <link>http://arxiv.org/abs/2502.18538v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;近年来，基于Transformer和状态空间模型(SSM)的DNA语言模型取得了进展。然而，在基础模型基准测试中没有对这些新方法与经典卷积神经网络(CNN)进行比较的研究。&lt;h4&gt;背景&lt;/h4&gt;虽然最近提出了许多基于Transformer和SSM架构的方法来改进DNA语言模型，但在一些关键指标上缺乏与传统CNN的对比分析。&lt;h4&gt;目的&lt;/h4&gt;探讨并设计一种新型基于CNN的方法（ConvNova），以评估其在各种基础模型基准测试中的表现，并回答卷积网络是否已被这些新的方法超越的问题。&lt;h4&gt;方法&lt;/h4&gt;开发了一种称为ConvNova的新方法，该方法采用了扩增卷积、门控卷积以及用于门控机制的双分支框架三种有效设计。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实证实验显示，在超过一半的任务上，ConvNova的表现优于最近的方法。特别是在组蛋白相关任务中，ConvNova比第二好的方法高出平均5.8%，同时使用更少的参数并实现更快的计算速度。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明CNN仍然是与Transformer和SSM架构相比具有竞争力的选择。这可能激发对基于CNN方法在DNA基础模型中的重新关注。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容是关于开发了一种新的名为ConvNova的方法，通过实验展示了其优越性，并探讨了卷积网络是否被新架构超越的问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, a variety of methods based on Transformer and state spacemodel (SSM) architectures have been proposed, advancing foundational DNAlanguage models. However, there is a lack of comparison between these recentapproaches and the classical architecture convolutional networks (CNNs) onfoundation model benchmarks. This raises the question: are CNNs truly beingsurpassed by these recent approaches based on transformer and SSMarchitectures? In this paper, we develop a simple but well-designed CNN-basedmethod termed ConvNova. ConvNova identifies and proposes three effectivedesigns: 1) dilated convolutions, 2) gated convolutions, and 3) a dual-branchframework for gating mechanisms. Through extensive empirical experiments, wedemonstrate that ConvNova significantly outperforms recent methods on more thanhalf of the tasks across several foundation model benchmarks. For example, inhistone-related tasks, ConvNova exceeds the second-best method by an average of5.8%, while generally utilizing fewer parameters and enabling fastercomputation. In addition, the experiments observed findings that may be relatedto biological characteristics. This indicates that CNNs are still a strongcompetitor compared to Transformers and SSMs. We anticipate that this work willspark renewed interest in CNN-based methods for DNA foundation models.</description>
      <author>example@mail.com (Yu Bo, Weian Mao, Yanjun Shao, Weiqiang Bai, Peng Ye, Xinzhu Ma, Junbo Zhao, Hao Chen, Chunhua Shen)</author>
      <guid isPermaLink="false">2502.18538v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17612v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  correcting contact information&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文研究了在没有中央控制的情况下，通过自组织方式优化集体目标的代理调度问题，并特别关注如何利用图神经网络（GNN）架构来提升分布式群组协同能力。&lt;h4&gt;背景&lt;/h4&gt;无中心化控制的代理协调对于诸如自主车队管理和传感器网络中的监控侦察等应用至关重要。分散式控制器的设计受到自然界中自我组织现象，特别是鸟类群体行为的启发，但现有的分散式控制器在保持群体凝聚力方面存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够利用群组动态中存在的对称性的图神经网络架构，以提高分布式控制器的一般化性能和效率。&lt;h4&gt;方法&lt;/h4&gt;通过在分散式的鸟群控制GNN控制器中强制执行旋转等变性和平移不变性对称性来改进现有的GNN控制器。&lt;h4&gt;主要发现&lt;/h4&gt;与不考虑上述对称性的现有GNN控制器相比，我们的对称感知控制器可以使用更少的训练数据和更少的可调权重实现类似的效果，并且在泛化能力方面表现更好。&lt;h4&gt;结论&lt;/h4&gt;本文提出的旋转等变性和平移不变性策略改进了分散式群组控制系统的性能，证明了这种新方法的有效性。相关代码和动画可在GitHub上获取。&lt;h4&gt;翻译&lt;/h4&gt;代理的协调以优化集体目标而没有中央控制系统是具有挑战性的，但在像自主车队管理和使用传感器网络进行监控侦察等应用中至关重要。分散控制器的设计受到了自然界自组织现象，尤其是鸟类群体行为的启发，但是现有的分散式控制器在维持群组凝聚力方面存在困难。图神经网络（GNN）架构已经成为了开发能够保持群组凝聚力的分散化控制系统的不可或缺的机器学习工具，然而它们未能利用存在于群组动态中的对称性，从而限制了其泛化能力。我们强制执行旋转等变性和平移不变性的对称性以改进分散式鸟群GNN控制器，并且在使用70%更少的训练数据和75%更少的可调权重的同时达到了与没有这些对称性的现有GNN控制器相同的控制效果。此外，我们的对称感知控制器比现有的GNN控制器有更好的泛化能力。相关代码和动画可在GitHub上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The orchestration of agents to optimize a collective objective withoutcentralized control is challenging yet crucial for applications such ascontrolling autonomous fleets, and surveillance and reconnaissance using sensornetworks. Decentralized controller design has been inspired byself-organization found in nature, with a prominent source of inspiration beingflocking; however, decentralized controllers struggle to maintain flockcohesion. The graph neural network (GNN) architecture has emerged as anindispensable machine learning tool for developing decentralized controllerscapable of maintaining flock cohesion, but they fail to exploit the symmetriespresent in flocking dynamics, hindering their generalizability. We enforcerotation equivariance and translation invariance symmetries in decentralizedflocking GNN controllers and achieve comparable flocking control with 70% lesstraining data and 75% fewer trainable weights than existing GNN controllerswithout these symmetries enforced. We also show that our symmetry-awarecontroller generalizes better than existing GNN controllers. Code andanimations are available athttp://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.</description>
      <author>example@mail.com (Taos Transue, Bao Wang)</author>
      <guid isPermaLink="false">2502.17612v2</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2410.22489v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的多模态少样本点云分割(MM-FSS)框架，通过结合文本标签和潜在可用的2D图像模式来增强传统单一模态的点云输入。MM-FSS使用一个共享的骨干网络、两个头部以及预训练的文本编码器，有效利用了多个模态之间的互补信息。&lt;h4&gt;背景&lt;/h4&gt;传统的少样本3D点云分割(FS-PCS)方法主要关注于单一模态的点云输入，而忽略了多模态信息可能带来的潜在优势。现有的FS-PCS技术在处理新型类别的分类时需要最少的支持样本来泛化模型。&lt;h4&gt;目的&lt;/h4&gt;旨在通过引入利用文本标签和2D图像模式的多模态少样本点云分割框架来改进现有方法，并展示结合这些自由可得的额外模态信息可以显著提升性能。&lt;h4&gt;方法&lt;/h4&gt;MM-FSS采用了共享骨干网络与两个头部相结合的方式，以提取跨模态和单模态视觉特征；同时利用预训练文本编码器生成文本嵌入。为了充分挖掘多模态数据中的信息，提出了一个多模态相关融合(MCF)模块来产生多模态关联，并且设计了一个多模态语义融合(MSF)模块来使用基于文本的语义指导细化这些关联。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在S3DIS和ScanNet数据集上，提出的MM-FSS框架相较于传统方法在性能上有显著提升。具体而言，该方法通过引入测试时间自适应跨模态校准(TACC)技术来缓解训练偏差，进一步提高泛化能力。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法为少样本3D点云分割提供了新的解决方案，并证明了多模态信息的有效性，这为进一步的研究工作提供了有价值的见解。该研究强调了利用通常被忽视的自由模式进行FS-PCS的潜在优势。&lt;h4&gt;翻译&lt;/h4&gt;Few-shot 3D点云分割(FS-PCS)的目标是通过少量注释支持样本使模型泛化以对新型类别进行分类。尽管现有的FS-PCS方法展示了一定的效果，但它们主要集中在单一模态的点云输入上，忽视了利用多模态信息可能带来的潜在好处。本文通过引入一种可以利用文本标签和潜在可用2D图像模式的多模态少样本分割设置来填补这一空白，并提出MultiModal Few-Shot SegNet (MM-FSS)模型，该模型能够有效利用多个来源的信息。实验结果在S3DIS和ScanNet数据集上表明了我们方法的有效性，验证了FS-PCS中结合额外可得模式的好处。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models tosegment novel categories with minimal annotated support samples. While existingFS-PCS methods have shown promise, they primarily focus on unimodal point cloudinputs, overlooking the potential benefits of leveraging multimodalinformation. In this paper, we address this gap by introducing a multimodalFS-PCS setup, utilizing textual labels and the potentially available 2D imagemodality. Under this easy-to-achieve setup, we present the MultiModal Few-ShotSegNet (MM-FSS), a model effectively harnessing complementary information frommultiple modalities. MM-FSS employs a shared backbone with two heads to extractintermodal and unimodal visual features, and a pretrained text encoder togenerate text embeddings. To fully exploit the multimodal information, wepropose a Multimodal Correlation Fusion (MCF) module to generate multimodalcorrelations, and a Multimodal Semantic Fusion (MSF) module to refine thecorrelations using text-aware semantic guidance. Additionally, we propose asimple yet effective Test-time Adaptive Cross-modal Calibration (TACC)technique to mitigate training bias, further improving generalization.Experimental results on S3DIS and ScanNet datasets demonstrate significantperformance improvements achieved by our method. The efficacy of our approachindicates the benefits of leveraging commonly-ignored free modalities forFS-PCS, providing valuable insights for future research. The code is availableat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot</description>
      <author>example@mail.com (Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie)</author>
      <guid isPermaLink="false">2410.22489v4</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Hi Robot: Open-Ended Instruction Following with Hierarchical Vision-Language-Action Models</title>
      <link>http://arxiv.org/abs/2502.19417v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通用型机器人系统，该系统能够处理复杂的指令和反馈，并在实际环境中执行多项任务。&lt;h4&gt;背景&lt;/h4&gt;现有的直接遵循简单命令的方法不足以应对需要复杂推理的任务场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种使用层次结构的视觉语言模型的机器人系统，以理解和执行复杂的任务指令及用户反馈。&lt;h4&gt;方法&lt;/h4&gt;该系统首先通过高层推理解释复杂提示和用户的反馈信息，并确定完成当前任务的最佳下一步；然后在低层控制下执行具体的物理动作。&lt;h4&gt;主要发现&lt;/h4&gt;实验证明了此系统可以在单臂、双臂以及移动式双臂机器人平台上演示出处理杂乱桌面清理、制作三明治等任务的能力。&lt;h4&gt;结论&lt;/h4&gt;该系统的应用证明其能够有效处理开放环境中的各种复杂指令和实时反馈，展现出广泛的潜在应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generalist robots that can perform a range of different tasks in open-worldsettings must be able to not only reason about the steps needed to accomplishtheir goals, but also process complex instructions, prompts, and even feedbackduring task execution. Intricate instructions (e.g., "Could you make me avegetarian sandwich?" or "I don't like that one") require not just the abilityto physically perform the individual steps, but the ability to situate complexcommands and feedback in the physical world. In this work, we describe a systemthat uses vision-language models in a hierarchical structure, first reasoningover complex prompts and user feedback to deduce the most appropriate next stepto fulfill the task, and then performing that step with low-level actions. Incontrast to direct instruction following methods that can fulfill simplecommands ("pick up the cup"), our system can reason through complex prompts andincorporate situated feedback during task execution ("that's not trash"). Weevaluate our system across three robotic platforms, including single-arm,dual-arm, and dual-arm mobile robots, demonstrating its ability to handle taskssuch as cleaning messy tables, making sandwiches, and grocery shopping.</description>
      <author>example@mail.com (Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, Chelsea Finn)</author>
      <guid isPermaLink="false">2502.19417v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>H-FLTN: A Privacy-Preserving Hierarchical Framework for Electric Vehicle Spatio-Temporal Charge Prediction</title>
      <link>http://arxiv.org/abs/2502.18697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 tables, 2 figures, Journal Paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了H-FLTN框架，旨在解决电动汽车普及带来的充电时间预测、用户隐私保护和资源管理等问题。&lt;h4&gt;背景&lt;/h4&gt;电动汽车的广泛应用给能源供应商带来了挑战，包括准确预测充电时间、确保用户隐私以及高效地进行资源配置。&lt;h4&gt;目的&lt;/h4&gt;通过引入Hierarchical Federated Learning Transformer Network (H-FLTN) 框架来应对这些挑战。&lt;h4&gt;方法&lt;/h4&gt;1. 采用三级层次架构（电动汽车、社区分布式能源资源管理系统和能源提供商数据中心）。            2. 使用基于Transformer的学习增强时间预测，捕获充电行为中的复杂依赖关系。            3. 利用安全聚合、加性秘密共享和点对点共享技术来确保隐私保护。            4. 引入动态客户端上限机制（DCCM）和客户端轮换管理（CRM），以提高训练效率和资源分配。&lt;h4&gt;主要发现&lt;/h4&gt;1. H-FLTN框架在大量实际车辆移动数据的模拟中表现出良好性能，特别是在减少随着电动汽车数量增加而产生的线性增长的培训时间复杂度至常量方面。            2. DCCM和CRM能够有效防止由于参与训练的客户端增多而导致计算负担过重的问题。&lt;h4&gt;结论&lt;/h4&gt;H-FLTN框架的实施可以增强智能城市的能源需求预测、资源分配和电网稳定性，确保未来移动生态系统的可靠性和可持续性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The widespread adoption of Electric Vehicles (EVs) poses critical challengesfor energy providers, particularly in predicting charging time (temporalprediction), ensuring user privacy, and managing resources efficiently inmobility-driven networks. This paper introduces the Hierarchical FederatedLearning Transformer Network (H-FLTN) framework to address these challenges.H-FLTN employs a three-tier hierarchical architecture comprising EVs, communityDistributed Energy Resource Management Systems (DERMS), and the Energy ProviderData Centre (EPDC) to enable accurate spatio-temporal predictions of EVcharging needs while preserving privacy. Temporal prediction is enhanced usingTransformer-based learning, capturing complex dependencies in chargingbehavior. Privacy is ensured through Secure Aggregation, Additive SecretSharing, and Peer-to-Peer (P2P) Sharing with Augmentation, which allow onlysecret shares of model weights to be exchanged while securing alltransmissions. To improve training efficiency and resource management, H-FLTNintegrates Dynamic Client Capping Mechanism (DCCM) and Client RotationManagement (CRM), ensuring that training remains both computationally andtemporally efficient as the number of participating EVs increases. DCCMoptimises client participation by limiting excessive computational loads, whileCRM balances training contributions across epochs, preventing imbalancedparticipation. Our simulation results based on large-scale empirical vehiclemobility data reveal that DCCM and CRM reduce the training time complexity withincreasing EVs from linear to constant. Its integration into real-world smartcity infrastructure enhances energy demand forecasting, resource allocation,and grid stability, ensuring reliability and sustainability in future mobilityecosystems.</description>
      <author>example@mail.com (Robert Marlin, Raja Jurdak, Alsharif Abuadbba)</author>
      <guid isPermaLink="false">2502.18697v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>SLAM in the Dark: Self-Supervised Learning of Pose, Depth and Loop-Closure from Thermal Images</title>
      <link>http://arxiv.org/abs/2502.18932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Visual SLAM 对移动机器人、无人机导航和 VR/AR 非常重要，但传统的 RGB 相机系统在低光条件下表现不佳，促使人们关注热像 SLAM。然而，热成像面临低对比度、高噪声以及有限的大规模标注数据集等问题，限制了深度学习在户外场景中的应用。&lt;h4&gt;背景&lt;/h4&gt;传统RGB相机系统在低光条件下效果不佳，推动了对热像SLAM技术的研究需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于复杂光照条件下的大规模定位和重建的新型单目热像SLAM系统DarkSLAM。&lt;h4&gt;方法&lt;/h4&gt;{'ECA机制': '将Efficient Channel Attention（ECA）机制应用于视觉里程计，以提高姿态准确性。', 'SKA机制': '引入Selective Kernel Attention（SKA）机制进行深度估计，缓解了热像深度退化问题。', '闭环检测和姿态优化': '基于热像深度的闭环检测以及姿态优化，确保在低纹理热场景中的鲁棒性能'}&lt;h4&gt;主要发现&lt;/h4&gt;DarkSLAM 在户外实验中显著优于现有的SC-Sfm-Learner和Shin等人提出的方法，在严峻的夜间环境中也能实现精确定位和三维稠密地图构建。&lt;h4&gt;结论&lt;/h4&gt;DarkSLAM 提供了一种强大的解决方案，适用于在低光条件下的大规模场景中的实时导航和重建任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual SLAM is essential for mobile robots, drone navigation, and VR/AR, buttraditional RGB camera systems struggle in low-light conditions, drivinginterest in thermal SLAM, which excels in such environments. However, thermalimaging faces challenges like low contrast, high noise, and limited large-scaleannotated datasets, restricting the use of deep learning in outdoor scenarios.We present DarkSLAM, a noval deep learning-based monocular thermal SLAM systemdesigned for large-scale localization and reconstruction in complex lightingconditions.Our approach incorporates the Efficient Channel Attention (ECA)mechanism in visual odometry and the Selective Kernel Attention (SKA) mechanismin depth estimation to enhance pose accuracy and mitigate thermal depthdegradation. Additionally, the system includes thermal depth-based loop closuredetection and pose optimization, ensuring robust performance in low-texturethermal scenes. Extensive outdoor experiments demonstrate that DarkSLAMsignificantly outperforms existing methods like SC-Sfm-Learner and Shin et al.,delivering precise localization and 3D dense mapping even in challengingnighttime environments.</description>
      <author>example@mail.com (Yangfan Xu, Qu Hao, Lilian Zhang, Jun Mao, Xiaofeng He, Wenqi Wu, Changhao Chen)</author>
      <guid isPermaLink="false">2502.18932v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Emerging Practices in Participatory AI Design in Public Sector Innovation</title>
      <link>http://arxiv.org/abs/2502.18689v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Extended Abstracts of the CHI Conference on Human Factors in  Computing Systems (CHI EA '25), April 26-May 1, 2025, Yokohama, Japan&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;地方政府正在利用AI系统来改进公共服务的提供，同时确保技术采用过程中民主价值和社区信任得到维护。&lt;h4&gt;背景&lt;/h4&gt;地方和联邦机构正快速采纳AI系统以增强或自动化关键决策，并提高资源使用效率。这些系统在城市规划、安全监控、能源管理等方面发挥作用，影响公民获取基本服务的能力。&lt;h4&gt;目的&lt;/h4&gt;探讨参与式算法设计的方法，在公共服务领域中通过公众参与和社区互动来确定、设计、采用和实施算法。&lt;h4&gt;方法&lt;/h4&gt;需要重新评估传统的技术采纳方式，并为公共部门制定新的资源和方法，特别是在AI创新引入的新挑战下。&lt;h4&gt;主要发现&lt;/h4&gt;在智能城市倡议背景下，地方治理层必须确保民主价值的维持并建立社区信任。社区中心化及参与式的方式对于保证科技适当采用至关重要。&lt;h4&gt;结论&lt;/h4&gt;探索新兴的参与式算法设计实践是必要的，尤其是在公共部门，因为这要求更高的实施标准和更严格的形成方法。&lt;h4&gt;翻译&lt;/h4&gt;地方和联邦机构正在迅速采纳AI系统以增强或自动化关键决策，高效利用资源，并改善公共服务交付。这些系统被用于支持与城市规划、安全、监控、能源和关键基础设施相关的任务，以及影响公民获取基本服务能力的决策。地方政府作为最接近民众的治理层级，必须在智能城市的倡议中发挥重要作用，维护民主价值并建立社区信任。基于社区的方法对于确保技术适当采用至关重要；然而，在AI创新背景下，参与式设计方法需要更严格的规定和更高的实施标准，这比私营部门更具挑战性。因此，我们需要重新审视传统的做法，并为此开发新的资源和方法。本次研讨会将探讨公共部门算法的规划、设计、采用和实施中的新兴的参与式算法设计实践。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Local and federal agencies are rapidly adopting AI systems to augment orautomate critical decisions, efficiently use resources, and improve publicservice delivery. AI systems are being used to support tasks associated withurban planning, security, surveillance, energy and critical infrastructure, andsupport decisions that directly affect citizens and their ability to accessessential services. Local governments act as the governance tier closest tocitizens and must play a critical role in upholding democratic values andbuilding community trust especially as it relates to smart city initiativesthat seek to transform public services through the adoption of AI.Community-centered and participatory approaches have been central for ensuringthe appropriate adoption of technology; however, AI innovation introduces newchallenges in this context because participatory AI design methods require morerobust formulation and face higher standards for implementation in the publicsector compared to the private sector. This requires us to reassess traditionalmethods used in this space as well as develop new resources and methods. Thisworkshop will explore emerging practices in participatory algorithm design - orthe use of public participation and community engagement - in the scoping,design, adoption, and implementation of public sector algorithms.</description>
      <author>example@mail.com (Devansh Saxena, Zoe Kahn, Erina Seh-Young Moon, Lauren M. Chambers, Corey Jackson, Min Kyung Lee, Motahhare Eslami, Shion Guha, Sheena Erete, Lilly Irani, Deirdre Mulligan, John Zimmerman)</author>
      <guid isPermaLink="false">2502.18689v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models</title>
      <link>http://arxiv.org/abs/2502.19409v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code, dataset, and checkpoints are publicly available at  https://github.com/danaesavi/ImageChain&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '对于多模态大型语言模型而言，基于图像序列的推理仍然是一个挑战。虽然最近的一些模型在预训练过程中引入了多张图片数据，但它们仍然难以识别序列结构，往往将图像视为独立的数据进行处理。', '目的': '介绍一种名为ImageChain的框架，旨在通过模拟视觉序列作为多轮对话来增强多模态大型语言模型的顺序推理能力。', '方法': '在ImageChain中，图片与相应的文本描述交错形成一个受控对话，明确捕捉时间依赖性和叙述进展。该方法优化了下一个场景描述任务，即基于先前的视觉和文本线索生成上下文感知的后续场景描述。', '主要发现': '实验表明，这种方法显著提高了下一场景描述任务的表现，平均改进幅度在SimRate这一衡量语义相似性的指标下为3.7%至19%。此外，在从漫画到机器人等各种应用中的零样本跨领域性能也表现出色。', '结论': '通过多模态、多轮对话的设计进行指令微调是弥合静态图像理解与时间感知推理之间差距的关键方法。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reasoning over sequences of images remains a challenge for multimodal largelanguage models (MLLMs). While recent models incorporate multi-image dataduring pre-training, they still struggle to recognize sequential structures,often treating images independently. This work introduces ImageChain, aframework that enhances MLLMs with sequential reasoning capabilities over imagedata by modeling visual sequences as a multi-turn conversation. In ImageChain,images are interleaved with corresponding textual descriptions to form acontrolled dialogue that explicitly captures temporal dependencies andnarrative progression. Our method optimizes for the task of next-scenedescription, where the model generates a context-aware description of anupcoming scene based on preceding visual and textual cues. We demonstrate thatour approach improves performance on the next-scene description task --achieving an average improvement from 3.7% to 19% in SimRate, a metric thatquantifies semantic similarity to human-annotated ground truths. Moreover,ImageChain achieves robust zero-shot out-of-domain performance in applicationsranging from comics to robotics. Extensive experiments validate thatinstruction-tuning in a multimodal, multi-turn conversation design is key tobridging the gap between static image understanding and temporally-awarereasoning.</description>
      <author>example@mail.com (Danae Sánchez Villegas, Ingo Ziegler, Desmond Elliott)</author>
      <guid isPermaLink="false">2502.19409v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ARENA: Adaptive Risk-aware and Energy-efficient NAvigation for Multi-Objective 3D Infrastructure Inspection with a UAV</title>
      <link>http://arxiv.org/abs/2502.19401v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 7 figures, submitted to IEEE Robotics and Automation Letters  (RA-L)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种适用于复杂3D环境下的无人机自主巡检任务的自适应风险感知和能量效率导航方法ARENA，该方法在多目标路径规划中实现了在线轨迹优化，并通过实际测试验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;目前的多目标路径规划方法难以应对定位误差、天气变化等不断演变的风险因素。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够在复杂3D环境中实时适应风险和能量消耗，为无人机自主巡检任务提供可靠导航方案的方法。&lt;h4&gt;方法&lt;/h4&gt;利用4维NURBS表示形式及遗传算法生成帕累托前沿，通过创新性的风险感知投票算法确保自适应性。&lt;h4&gt;主要发现&lt;/h4&gt;ARENA框架能够在很大程度上优化轨迹多样性，并准确估计能量消耗。仿真和实际测试表明该规划器能够产生覆盖95%以上单一目标基准定义范围的多样化、最优化轨迹。&lt;h4&gt;结论&lt;/h4&gt;ARENA框架显著提高了无人机在关键且不断变化的3D任务中的自主性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;自主机器人巡检任务需要平衡多个相互冲突的目标，同时导航时避免靠近昂贵障碍物。当前多目标路径规划方法难以适应如定位误差、天气情况、电池状态及通信问题等不断演变的风险。本文提出了一种适用于复杂3D环境下的无人机自适应风险感知和能量效率导航（ARENA）的多目标路径规划方案。此方法通过4维NURBS表示形式与基于遗传算法生成帕累托前沿，在线优化安全性、时间和能源，使用新颖的风险感知投票算法确保其自适应性。仿真及实际测试表明该规划器能够产生多样化且最优化轨迹，覆盖单目标基准定义范围的95%以上，并且具有良好的能量消耗估计能力，平均误差代表全功率范围的14%。ARENA框架增强了无人机在关键和不断变化3D任务中的自主性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous robotic inspection missions require balancing multiple conflictingobjectives while navigating near costly obstacles. Current multi-objective pathplanning (MOPP) methods struggle to adapt to evolving risks like localizationerrors, weather, battery state, and communication issues. This letter presentsan Adaptive Risk-aware and Energy-efficient NAvigation (ARENA) MOPP approachfor UAVs in complex 3D environments. Our method enables online trajectoryadaptation by optimizing safety, time, and energy using 4D NURBS representationand a genetic-based algorithm to generate the Pareto front. A novel risk-awarevoting algorithm ensures adaptivity. Simulations and real-world testsdemonstrate the planner's ability to produce diverse, optimized trajectoriescovering 95% or more of the range defined by single-objective benchmarks andits ability to estimate power consumption with a mean error representing 14% ofthe full power range. The ARENA framework enhances UAV autonomy and reliabilityin critical, evolving 3D missions.</description>
      <author>example@mail.com (David-Alexandre Poissant, Alexis Lussier Desbiens, François Ferland, Louis Petit)</author>
      <guid isPermaLink="false">2502.19401v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Surface-Based Manipulation</title>
      <link>http://arxiv.org/abs/2502.19389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript is under revision for possible publication in the npj  Robotics. Copyright may be transferred to the publisher if the manuscript is  accepted for publication, without further notice. Supplementary video:  https://drive.google.com/drive/folders/1qbagK0VHi4DyfHGJ99ZlESX_i4rU_nmh?usp=sharing&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于表面的机器人操作策略，这种策略使用平面作为末端执行器来实现物体的精确操控。&lt;h4&gt;背景&lt;/h4&gt;在机器人研究中，与物理世界的交互是至关重要的。传统的方法主要依赖于类似手指形状的抓取装置，但这种方法难以稳定地抓取脆弱、可变形或非规则形状的物体。&lt;h4&gt;目的&lt;/h4&gt;探讨一种新的操作策略，以解决现有的基于指尖的传统抓取方法存在的问题，并且能够适应各种不同大小和刚度级别的物体。&lt;h4&gt;方法&lt;/h4&gt;提出了一种利用平面表面作为末端执行器的方法，通过改变这些平面的位置和方向，可以实现对物体的平移、旋转甚至翻转。这种方法不需要依赖稳定的抓取，而是依靠闭合回路控制策略来实现稳定操作。&lt;h4&gt;主要发现&lt;/h4&gt;这种基于表面的操作方法能够适应各种形状大小及硬度不同的物体，并且还可以操控可变形物体的形态。&lt;h4&gt;结论&lt;/h4&gt;这项研究成果为解决复杂的机器人操作问题提供了一个全新的视角。这种方法不仅可以应用于多种类型的物体，而且还扩展了我们对智能的理解：它不仅仅存在于大脑中，还体现在身体和与环境互动的方式之中。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Intelligence lies not only in the brain but in the body. The shape of ourbodies can influence how we think and interact with the physical world. Inrobotics research, interacting with the physical world is crucial as it allowsrobots to manipulate objects in various real-life scenarios. Conventionalrobotic manipulation strategies mainly rely on finger-shaped end effectors.However, achieving stable grasps on fragile, deformable, irregularly shaped, orslippery objects is challenging due to difficulties in establishing stableforce or geometric constraints.  Here, we present surface-based manipulation strategies that diverge fromclassical grasping approaches, using with flat surfaces as minimalistend-effectors. By changing the position and orientation of these surfaces,objects can be translated, rotated and even flipped across the surface usingclosed-loop control strategies. Since this method does not rely on stablegrasp, it can adapt to objects of various shapes, sizes, and stiffness levels,even enabling the manipulation the shape of deformable objects. Our resultsprovide a new perspective for solving complex manipulation problems.</description>
      <author>example@mail.com (Ziqiao Wang, Serhat Demirtas, Fabio Zuliani, Jamie Paik)</author>
      <guid isPermaLink="false">2502.19389v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Robot Learning for Automatic Robot Motion Planning in Manufacturing</title>
      <link>http://arxiv.org/abs/2502.19340v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  35 Pages, 11 Figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多级混合机器人运动规划方法，该方法结合了基于任务空间的强化学习和从演示中学习（RL-LfD）代理以及关节空间深度强化学习（DRL）代理。这种方法通过实现两个代理之间的切换，确保机器人的动作既可行又平滑。&lt;h4&gt;背景&lt;/h4&gt;工业机器人在各种制造环境中广泛应用，但如何使机器人能够自动规划适应不同任务的轨迹是一个重大挑战。特别是当机器人与其他设备、人类或其他机器人共同工作时，这一问题变得更加复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的混合运动规划方法，以解决工业机器人面临的自动化和灵活性方面的难题。&lt;h4&gt;方法&lt;/h4&gt;该研究通过结合基于任务空间的RL-LfD代理和基于关节空间的DRL代理来实现多级混合规划。较高层次的代理负责在两个较低层次的代理之间进行切换，并且这种切换策略考虑了机器人的可达性、关节极限、操作灵活性及碰撞风险等多重因素。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的混合运动规划方法能够生成满足任务约束条件并且可行性的轨迹。&lt;h4&gt;结论&lt;/h4&gt;通过仿真和实际场景验证了该方法的有效性和实用性。此研究为工业机器人在复杂多变的工作环境中自动规划路径提供了一种有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要的中文直接翻译版本：工业机器人广泛应用于各种制造环境，但是如何使它们能够自动地计划出适应改变任务的动作轨迹是一个重要的挑战。当机器人在其工作单元内与其他机器、人类或其它机器人一起操作时，情况会变得更复杂。本文介绍了一种多级混合型机器人运动规划方法，该方法结合了基于任务空间的强化学习从演示中学习（RL-LfD）代理和关节空间的深度强化学习（DRL）代理的方法。一个更高的层级代理被用来在两个较低层级的代理之间进行切换以实现可行且平滑的动作。可行性通过将机器人在其环境中可达到性、关节极限、操作灵活性以及碰撞风险等因素综合考虑来进行计算。因此，由此方法产生的混合型运动规划策略生成出符合任务约束条件并且是可行性的轨迹。该方法的有效性在仿真中的机器人场景和实际应用场景中得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Industrial robots are widely used in diverse manufacturing environments.Nonetheless, how to enable robots to automatically plan trajectories forchanging tasks presents a considerable challenge. Further complexities arisewhen robots operate within work cells alongside machines, humans, or otherrobots. This paper introduces a multi-level hybrid robot motion planning methodcombining a task space Reinforcement Learning-based Learning from Demonstration(RL-LfD) agent and a joint-space based Deep Reinforcement Learning (DRL) basedagent. A higher level agent learns to switch between the two agents to enablefeasible and smooth motion. The feasibility is computed by incorporatingreachability, joint limits, manipulability, and collision risks of the robot inthe given environment. Therefore, the derived hybrid motion planning policygenerates a feasible trajectory that adheres to task constraints. Theeffectiveness of the method is validated through sim ulated robotic scenariosand in a real-world setup.</description>
      <author>example@mail.com (Siddharth Singh, Tian Yu, Qing Chang, John Karigiannis, Shaopeng Liu)</author>
      <guid isPermaLink="false">2502.19340v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>ObjectVLA: End-to-End Open-World Object Manipulation Without Demonstration</title>
      <link>http://arxiv.org/abs/2502.19250v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project page at https://objectvla.github.io/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于Vision-Language-Action (VLA)模型的方法ObjectVLA，该方法使机器人能够在未见过的新对象上泛化所学的技能。&lt;h4&gt;背景&lt;/h4&gt;模仿学习在教授机器人的灵巧操作技巧方面非常有效，但通常依赖大量的人类演示数据，这限制了其在动态真实环境中的可扩展性和适用性。一个关键挑战是物体泛化能力的缺乏，即机器人难以将针对特定对象训练出的操作技能转移到语义相似但视觉不同的新对象上。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够实现物体级泛化的简单且有效的方法，并减少对大量人类演示的需求。&lt;h4&gt;方法&lt;/h4&gt;使用Vision-Language-Action (VLA)模型结合视觉和语言数据，使机器人能够在没有针对新目标物的具体演示的情况下泛化操作技能。通过利用预训练的模型并进行少量图像的微调来进一步增强泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在一个真实的机器人平台上测试时，ObjectVLA能够对100个从未见过的新对象以64%的成功率完成指定任务。&lt;h4&gt;结论&lt;/h4&gt;该方法有效支持了物体级别的泛化学习，并减少了需要大量人类演示的需求，为更加灵活和可扩展的机器人系统铺平了道路。&lt;h4&gt;翻译&lt;/h4&gt;模仿学习已被证明在教导机器人灵巧操作技能方面非常有效。然而，它通常依赖于大量的数据，这些数据来自人类的操作示范，这限制了其在动态现实环境中的应用范围和规模。在这种情况下的一个关键挑战是物体的泛化能力，即一个被训练来处理特定对象任务的机器人（例如“拿起苹果”）很难将所学技能转移到语义上相似但视觉上不同的新目标物上（如“拿起桃子”）。先前关于端到端视觉操作策略学习的研究尚未充分解决向这些类别之外的新物体泛化的问题。本文中，我们提出了一种简单而有效的方法ObjectVLA，通过Vision-Language-Action (VLA)模型实现物体的泛化能力。我们的方法使机器人可以在没有为每个新目标物提供明确的人类演示的情况下将学到的操作技能转移到新的对象上。通过结合视觉和语言对数据，我们的方法以一种轻量级且可扩展的方式注入关于目标对象的知识，并建立了该对象与预期操作之间的隐式联系。我们在真实机器人的平台验证了ObjectVLA的能力，展示其可以成功地在100个从训练中未见过的新型物体上进行泛化，在选择从未见过的目标物方面取得了64%的成功率。此外，我们提出了一种使用智能手机拍摄少量图像并微调预训练模型的方法来增强VLA模型中的对象泛化能力。这些结果突显了我们的方法在实现物体级别泛化和减少需要广泛人类演示需求方面的有效性，并为更灵活可扩展的机器人学习系统铺平道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Imitation learning has proven to be highly effective in teaching robotsdexterous manipulation skills. However, it typically relies on large amounts ofhuman demonstration data, which limits its scalability and applicability indynamic, real-world environments. One key challenge in this context is objectgeneralization, where a robot trained to perform a task with one object, suchas "hand over the apple," struggles to transfer its skills to a semanticallysimilar but visually different object, such as "hand over the peach." This gapin generalization to new objects beyond those in the same category has yet tobe adequately addressed in previous work on end-to-end visuomotor policylearning. In this paper, we present a simple yet effective approach forachieving object generalization through Vision-Language-Action (VLA) models,referred to as \textbf{ObjectVLA}. Our model enables robots to generalizelearned skills to novel objects without requiring explicit human demonstrationsfor each new target object. By leveraging vision-language pair data, our methodprovides a lightweight and scalable way to inject knowledge about the targetobject, establishing an implicit link between the object and the desiredaction. We evaluate ObjectVLA on a real robotic platform, demonstrating itsability to generalize across 100 novel objects with a 64\% success rate inselecting objects not seen during training. Furthermore, we propose a moreaccessible method for enhancing object generalization in VLA models, using asmartphone to capture a few images and fine-tune the pre-trained model. Theseresults highlight the effectiveness of our approach in enabling object-levelgeneralization and reducing the need for extensive human demonstrations, pavingthe way for more flexible and scalable robotic learning systems.</description>
      <author>example@mail.com (Minjie Zhu, Yichen Zhu, Jinming Li, Zhongyi Zhou, Junjie Wen, Xiaoyu Liu, Chaomin Shen, Yaxin Peng, Feifei Feng)</author>
      <guid isPermaLink="false">2502.19250v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>CPG-Based Manipulation with Multi-Module Origami Robot Surface</title>
      <link>http://arxiv.org/abs/2502.19218v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This manuscript is under revision for possible publication in the  IEEE Robotics and Automation Letters (RA-L). Copyright may be transferred to  IEEE if the manuscript is accepted for publication, without further notice.  Supplementary video: https://youtu.be/AEmWFmHhPOA. Code available:  https://doi.org/10.5281/zenodo.14726303&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;机器人机械手在处理不同尺寸和材料的物体时面临挑战，特别是在操作大尺度或具有不同刚度的物体时更为明显。传统抓取技术和策略在这种情况下经常表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于表面的多模块机器人操控框架，以解决现有技术无法有效操作各种大小、形状及硬度物体的问题。&lt;h4&gt;方法&lt;/h4&gt;采用中央模式发生器（CPG）运动生成器与模拟优化法结合的方式确定用于多模块折纸机器人表面(Ori-Pixel)的最佳操控参数。&lt;h4&gt;主要发现&lt;/h4&gt;通过动态仿真和一系列原型实验，展示了这种新框架可以有效地处理从厘米到米级别的物体，并且能够适应不同大小、重量、形状及材质的物体。&lt;h4&gt;结论&lt;/h4&gt;优化后的CPG参数在广泛的测试中表现出强大的操控能力。&lt;h4&gt;翻译&lt;/h4&gt;机器人抓取器通常面临挑战，在处理各种尺寸和材料的物体时效率较低。特别是在操作大尺度或具有多变刚度的物品时，传统抓握技术和策略往往无效。本文介绍了一种新颖的基于表面的多模块机器人操纵框架，该框架使用中央模式发生器（CPG）运动生成器，并结合模拟优化法来确定用于折纸机器人表面对象的最优操作参数。这种方法能够处理从厘米到米级别的各种刚度和形状的对象。通过动态仿真及一系列原型实验测试了最佳的CPG参数，证明了该框架在多种不同大小、重量、形状以及材料物体上的稳健性操控能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotic manipulators often face challenges in handling objects of differentsizes and materials, limiting their effectiveness in practical applications.This issue is particularly pronounced when manipulating meter-scale objects orthose with varying stiffness, as traditional gripping techniques and strategiesfrequently prove inadequate. In this letter, we introduce a novel surface-basedmulti-module robotic manipulation framework that utilizes a Central PatternGenerator (CPG)-based motion generator, combined with a simulation-basedoptimization method to determine the optimal manipulation parameters for amulti-module origami robotic surface (Ori-Pixel). This approach allows for themanipulation of objects ranging from centimeters to meters in size, withvarying stiffness and shape. The optimized CPG parameters are tested throughboth dynamic simulations and a series of prototype experiments involving a widerange of objects differing in size, weight, shape, and material, demonstratingrobust manipulation capabilities.</description>
      <author>example@mail.com (Yuhao Jiang, Serge El Asmar, Ziqiao Wang, Serhat Demirtas, Jamie Paik)</author>
      <guid isPermaLink="false">2502.19218v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Embodying mechano-fluidic memory in soft machines to program behaviors upon interactions</title>
      <link>http://arxiv.org/abs/2502.19192v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;软机器可以适应外部环境的变化，通过直接在结构中体现记忆能力来实现更加响应性的行为。&lt;h4&gt;目的&lt;/h4&gt;展示如何利用弹性壳体的双稳态特性改变封闭腔内的流体性质，从而切换自振荡机器的稳定频率状态。&lt;h4&gt;方法&lt;/h4&gt;开发围绕双稳态壳体的流体电路，软管在外部触碰时会弯曲和恢复原状。通过这种方式实现了长期和短期记忆功能。&lt;h4&gt;主要发现&lt;/h4&gt;设计了可以响应人类用户交互并自主改变方向来避开障碍物（如墙壁）的软机器。&lt;h4&gt;结论&lt;/h4&gt;只利用几何形状和弹性特性，将记忆直接嵌入物理结构中可以让没有中央大脑的系统表现出自主行为，这通常是由基于计算机的机器人系统完成的任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经用中文进行了总结。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soft machines display shape adaptation to external circumstances due to theirintrinsic compliance. To achieve increasingly more responsive behaviors uponinteractions without relying on centralized computation, embodying memorydirectly in the machines' structure is crucial. Here, we harness thebistability of elastic shells to alter the fluidic properties of an enclosedcavity, thereby switching between stable frequency states of a locomotingself-oscillating machine. To program these memory states upon interactions, wedevelop fluidic circuits surrounding the bistable shell, with soft tubes thatkink and unkink when externally touched. We implement circuits for bothlong-term and short-term memory in a soft machine that switches behaviors inresponse to a human user and that autonomously changes direction afterdetecting a wall. By harnessing only geometry and elasticity, embodying memoryallows physical structures without a central brain to exhibit autonomous featsthat are typically reserved for computer-based robotic systems.</description>
      <author>example@mail.com (Alberto Comoretto, Tanaya Mandke, Johannes T. B. Overvelde)</author>
      <guid isPermaLink="false">2502.19192v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>PlantPal: Leveraging Precision Agriculture Robots to Facilitate Remote Engagement in Urban Gardening</title>
      <link>http://arxiv.org/abs/2502.19171v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;PlantPal是一种支持城市居民进行园艺活动的系统，它利用精密农业机器人（PAR）来克服空间、时间和技能方面的限制。&lt;h4&gt;背景&lt;/h4&gt;城市园艺在健康和环保方面有许多好处。然而，缺乏合适的花园空间、忙碌的日程安排以及有限的园艺知识是阻碍人们参与城市园艺的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;研究并开发PlantPal系统以解决当前智能家庭解决方案未能充分应对的实际问题，使用户能够在任何地点进行园艺活动，不受专业知识和时间限制的影响。&lt;h4&gt;方法&lt;/h4&gt;PlantPal利用一个配备了多种工具和多摄像头系统的精密农业机器人（PAR），通过远程操作帮助人们在日常生活中整合园艺任务。&lt;h4&gt;主要发现&lt;/h4&gt;为期三周的实验表明，PlantPal有助于将园艺任务融入日常生活，增强用户与自己田地的情感连接，并提供一种即使在远程环境下也能保持吸引力的体验。&lt;h4&gt;结论&lt;/h4&gt;研究提出了一些未来机器人辅助城市园艺概念的设计考虑事项。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Urban gardening is widely recognized for its numerous health andenvironmental benefits. However, the lack of suitable garden spaces, demandingdaily schedules and limited gardening expertise present major roadblocks forcitizens looking to engage in urban gardening. While prior research hasexplored smart home solutions to support urban gardeners, these approachescurrently do not fully address these practical barriers. In this paper, wepresent PlantPal, a system that enables the cultivation of garden spacesirrespective of one's location, expertise level, or time constraints. PlantPalenables the shared operation of a precision agriculture robot (PAR) that isequipped with garden tools and a multi-camera system. Insights from a 3-weekdeployment (N=18) indicate that PlantPal facilitated the integration ofgardening tasks into daily routines, fostered a sense of connection with one'sfield, and provided an engaging experience despite the remote setting. Wecontribute design considerations for future robot-assisted urban gardeningconcepts.</description>
      <author>example@mail.com (Albin Zeqiri, Julian Britten, Clara Schramm, Pascal Jansen, Michael Rietzler, Enrico Rukzio)</author>
      <guid isPermaLink="false">2502.19171v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Temporal Planning Framework for Multi-Agent Systems via LLM-Aided Knowledge Base Management</title>
      <link>http://arxiv.org/abs/2502.19135v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种将大型语言模型与基于Prolog的知识管理和规划集成到多机器人任务中的框架PLANTOR。&lt;h4&gt;背景&lt;/h4&gt;当前研究中，多机器人系统需要处理复杂的知识表示和时间、资源等约束条件。现有的解决方案可能难以扩展或解释。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够结合大型语言模型生成准确的知识库，并利用Prolog确保形式正确性和可解释性的框架。&lt;h4&gt;方法&lt;/h4&gt;{'PLANTOR框架': '采用两阶段的机器人专用知识库生成过程，以保证重用性与组合推理能力；制定三步规划程序处理时间依赖、资源限制及并行任务执行问题。', '计划转换': '最终计划通过混合整数线性编程确定后，被转化为行为树用于直接在ROS2中使用。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'知识库生成': '大型语言模型可以在少量的人工反馈下产生准确的知识库；', '形式正确性和可解释性': 'Prolog保证了系统的正式验证和透明度。', '应用效果': 'PLANTOR框架在构建块世界多机器人装配任务与拱形建筑场景中表现出色。'}&lt;h4&gt;结论&lt;/h4&gt;大型语言模型与规划系统结合的集成方法对于需要灵活、可扩展且人类易于理解计划的任务至关重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要全文为英文，以上内容为其中文翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel framework, called PLANTOR (PLanning with Naturallanguage for Task-Oriented Robots), that integrates Large Language Models(LLMs) with Prolog-based knowledge management and planning for multi-robottasks. The system employs a two-phase generation of a robot-oriented knowledgebase, ensuring reusability and compositional reasoning, as well as a three-stepplanning procedure that handles temporal dependencies, resource constraints,and parallel task execution via mixed-integer linear programming. The finalplan is converted into a Behaviour Tree for direct use in ROS2. We tested theframework in multi-robot assembly tasks within a block world and anarch-building scenario. Results demonstrate that LLMs can produce accurateknowledge bases with modest human feedback, while Prolog guarantees formalcorrectness and explainability. This approach underscores the potential of LLMintegration for advanced robotics tasks requiring flexible, scalable, andhuman-understandable planning.</description>
      <author>example@mail.com (Enrico Saccon, Ahmet Tikna, Davide De Martini, Edoardo Lamon, Luigi Palopoli, Marco Roveri)</author>
      <guid isPermaLink="false">2502.19135v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Ground-level Viewpoint Vision-and-Language Navigation in Continuous Environments</title>
      <link>http://arxiv.org/abs/2502.19024v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Ground-level Viewpoint Navigation (GVNav)方法，旨在解决视觉语言导航(VLN)中由于观察视角高度不同而导致的人类指令与低视点机器人之间的不匹配问题。&lt;h4&gt;背景&lt;/h4&gt;VLN任务面临的主要挑战之一是缺乏在真实场景中的泛化能力，尤其是在处理视野受限的四足机器人的指令跟随时。当前的方法往往未能充分考虑不同视觉高度带来的感知差异。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决低视点机器人执行人类指导导航任务时遇到的独特问题，并展示通过使用加权历史观测数据和转移连通性图可以提高模型在模拟环境和真实世界部署中的性能。&lt;h4&gt;方法&lt;/h4&gt;GVNav利用加权的历史观察作为增强的时空上下文，以帮助机器人处理由于视觉障碍或感知不匹配导致的问题。此外，该工作还引入了HM3D和Gibson数据集的连通性图作为额外资源来提高空间先验知识并更好地表示现实世界的场景。&lt;h4&gt;主要发现&lt;/h4&gt;GVNav方法显著提升了在模拟环境以及使用四足机器人进行真实世界部署时的表现。&lt;h4&gt;结论&lt;/h4&gt;该研究强调了视点高度变化对VLN任务性能的影响，并为低视点导航提出了创新性解决方案，这有望改善未来机器人的感知和决策能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-and-Language Navigation (VLN) empowers agents to associatetime-sequenced visual observations with corresponding instructions to makesequential decisions. However, generalization remains a persistent challenge,particularly when dealing with visually diverse scenes or transitioning fromsimulated environments to real-world deployment. In this paper, we address themismatch between human-centric instructions and quadruped robots with alow-height field of view, proposing a Ground-level Viewpoint Navigation (GVNav)approach to mitigate this issue. This work represents the first attempt tohighlight the generalization gap in VLN across varying heights of visualobservation in realistic robot deployments. Our approach leverages weightedhistorical observations as enriched spatiotemporal contexts for instructionfollowing, effectively managing feature collisions within cells by assigningappropriate weights to identical features across different viewpoints. Thisenables low-height robots to overcome challenges such as visual obstructionsand perceptual mismatches. Additionally, we transfer the connectivity graphfrom the HM3D and Gibson datasets as an extra resource to enhance spatialpriors and a more comprehensive representation of real-world scenarios, leadingto improved performance and generalizability of the waypoint predictor inreal-world environments. Extensive experiments demonstrate that ourGround-level Viewpoint Navigation (GVnav) approach significantly improvesperformance in both simulated environments and real-world deployments withquadruped robots.</description>
      <author>example@mail.com (Zerui Li, Gengze Zhou, Haodong Hong, Yanyan Shao, Wenqi Lyu, Yanyuan Qiao, Qi Wu)</author>
      <guid isPermaLink="false">2502.19024v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>A Reliable, Time-Predictable Heterogeneous SoC for AI-Enhanced Mixed-Criticality Edge Applications</title>
      <link>http://arxiv.org/abs/2502.18953v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;下一代混合关键性的片上系统（SoCs）用于机器人、汽车和空间领域，需要执行增强型的传感器处理和控制工作负载，并确保在与非关键任务共享资源的同时可靠且时间可预测地运行关键任务。这些SoC必须适应小于2W功率包络。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述多维度挑战，在本文中提出了一种16nm、可靠的、时间可预测的异构SoC，该SoC集成了多个可编程加速器。&lt;h4&gt;方法&lt;/h4&gt;为了确保共享资源（如片上互连和内存系统）上的可预测访问，SoC集成了软件配置硬件IP，并在小于1.2W功率包络内工作。通过这种方式建立了关键应用执行时间的紧上界。&lt;h4&gt;主要发现&lt;/h4&gt;该研究提出了一种可靠的多核加速器来加快混合精度任务的关键AI处理，峰值性能达到304.9 GOPS，在能源效率方面达到1.6 TOPS/W。对于非关键、计算密集型浮点工作负载，则由双核向量集群加速，并达到了121.8 GFLOPS的性能以及1.1 TFLOPS/W和106.8 GFLOPS/mm²的能效。&lt;h4&gt;结论&lt;/h4&gt;提出的设计可以有效应对新一代混合关键性的SoC设计挑战，同时满足严格的能源效率要求。&lt;h4&gt;翻译&lt;/h4&gt;下一代用于机器人、汽车和空间领域的混合关键性系统级芯片（SoCs）需要执行增强型传感器处理和控制工作负载，并确保在与非关键任务共享资源的同时可靠且时间可预测地运行关键任务。为了应对这些多维挑战，本文提出了一种16nm的可靠的、时间可预测的异构SoC，该SoC集成了多个可编程加速器。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Next-generation mixed-criticality Systems-on-chip (SoCs) for robotics,automotive, and space must execute mixed-criticality AI-enhanced sensorprocessing and control workloads, ensuring reliable and time-predictableexecution of critical tasks sharing resources with non-critical tasks, whilealso fitting within a sub-2W power envelope. To tackle these multi-dimensionalchallenges, in this brief, we present a 16nm, reliable, time-predictableheterogeneous SoC with multiple programmable accelerators. Within a 1.2W powerenvelope, the SoC integrates software-configurable hardware IPs to ensurepredictable access to shared resources, such as the on-chip interconnect andmemory system, leading to tight upper bounds on execution times of criticalapplications. To accelerate mixed-precision mission-critical AI, the SoCintegrates a reliable multi-core accelerator achieving 304.9 GOPS peakperformance at 1.6 TOPS/W energy efficiency. Non-critical, compute-intensive,floating-point workloads are accelerated by a dual-core vector cluster,achieving 121.8 GFLOPS at 1.1 TFLOPS/W and 106.8 GFLOPS/mm2.</description>
      <author>example@mail.com (Angelo Garofalo, Alessandro Ottaviano, Matteo Perotti, Thomas Benz, Yvan Tortorella, Robert Balas, Michael Rogenmoser, Chi Zhang, Luca Bertaccini, Nils Wistoff, Maicol Ciani, Cyril Koenig, Mattia Sinigaglia, Luca Valente, Paul Scheffler, Manuel Eggimann, Matheus Cavalcante, Francesco Restuccia, Alessandro Biondi, Francesco Conti, Frank K. Gurkaynak, Davide Rossi, Luca Benini)</author>
      <guid isPermaLink="false">2502.18953v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Think on your feet: Seamless Transition between Human-like Locomotion in Response to Changing Commands</title>
      <link>http://arxiv.org/abs/2502.18901v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 10 figures, accepted at the 2025 IEEE International  Conference on Robotics and Automation (ICRA 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新型的人类机器人运动学习方法，通过改进经典模仿学习技术，实现了更自然、更具适应性的步行行为。&lt;h4&gt;背景&lt;/h4&gt;人形机器人的训练相对容易进行特定行走技能的模拟，但难以从各种动作中学习并适应不断变化的指令。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以精准追踪运动指令、在不同动作间无缝过渡且能够掌握参考数据之外中间状态的人形机器人系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用Wasserstein散度标准（WGAN-div）提高泛化能力；2. 采用混合内部模型提供结构化的隐藏状态和速度估计，增强移动稳定性及环境适应性；3. 引入好奇心奖励机制促进探索。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能够实现高度人类相似的步行行为，并且具有对不同速度需求的适应性、直接适用于未见动作的任务以及在模拟器与真实世界中跨地形的零样本转移能力。&lt;h4&gt;结论&lt;/h4&gt;通过各种机器人模型的仿真和广泛的真实世界实验验证了这些改进的有效性和适用性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While it is relatively easier to train humanoid robots to mimic specificlocomotion skills, it is more challenging to learn from various motions andadhere to continuously changing commands. These robots must accurately trackmotion instructions, seamlessly transition between a variety of movements, andmaster intermediate motions not present in their reference data. In this work,we propose a novel approach that integrates human-like motion transfer withprecise velocity tracking by a series of improvements to classical imitationlearning. To enhance generalization, we employ the Wasserstein divergencecriterion (WGAN-div). Furthermore, a Hybrid Internal Model provides structuredestimates of hidden states and velocity to enhance mobile stability andenvironment adaptability, while a curiosity bonus fosters exploration. Ourcomprehensive method promises highly human-like locomotion that adapts tovarying velocity requirements, direct generalization to unseen motions andmultitasking, as well as zero-shot transfer to the simulator and the real worldacross different terrains. These advancements are validated through simulationsacross various robot models and extensive real-world experiments.</description>
      <author>example@mail.com (Huaxing Huang, Wenhao Cui, Tonghe Zhang, Shengtao Li, Jinchao Han, Bangyu Qin, Tianchu Zhang, Liang Zheng, Ziyang Tang, Chenxu Hu, Ning Yan, Jiahao Chen, Shipu Zhang, Zheyuan Jiang)</author>
      <guid isPermaLink="false">2502.18901v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Distributed Online Task Assignment via Inexact ADMM for unplanned online tasks and its Applications to Security</title>
      <link>http://arxiv.org/abs/2502.18893v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IEEE TCNS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;多机器人系统（MRS）中任务分配对于协调代理和确保使命成功以及保持整体系统安全性至关重要。本文提出了一种基于优化的分布式任务分配算法，该算法能够动态地为团队分配关键安全性和可选任务。&lt;h4&gt;背景&lt;/h4&gt;在多机器人系统的应用中，有效的任务分配不仅是为了协调代理并确保任务的成功完成，还为了维护整个系统的安全性。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以在计划偏离攻击下保持在线处理任务能力的全面框架，并确保MRS能够在不降低安全性的前提下有效地应对未预见的任务。&lt;h4&gt;方法&lt;/h4&gt;{'优化算法': '提出了一种基于优化的分布式任务分配算法，通过近似交替方向乘子法（ADMM）的方法将任务分配问题分解为可分离和不可分离的子问题，并使用投影梯度下降法处理不可分离的子问题', '安全分析框架': '制定一个全面的框架来评估在线任务的安全执行性以及机器人重新加入团队的时间和地点', '控制方法': '采用基于控制Lyapunov函数（CLF）的任务履行管理和基于控制障碍函数（CBF）的安全过滤器，以确保任务完成时的安全保障'}&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟实验展示了所提出的框架能够使MRS有效地响应未计划的在线任务同时保持安全保证。&lt;h4&gt;结论&lt;/h4&gt;本文提出的方法和框架证明了在维护安全性的同时可以增强多机器人系统对意外任务处理的能力，具有重要的理论和实践意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In multi-robot system (MRS) applications, efficient task assignment isessential not only for coordinating agents and ensuring mission success butalso for maintaining overall system security. In this work, we first propose anoptimization-based distributed task assignment algorithm that dynamicallyassigns mandatory security-critical tasks and optional tasks among teams.Leveraging an inexact Alternating Direction Method of Multipliers (ADMM)-basedapproach, we decompose the task assignment problem into separable andnon-separable subproblems. The non-separable subproblems are transformed intoan inexact ADMM update by projected gradient descent, which can be performedthrough several communication steps within the team.  In the second part of this paper, we formulate a comprehensive framework thatenables MRS under plan-deviation attacks to handle online tasks withoutcompromising security. The process begins with a security analysis thatdetermines whether an online task can be executed securely by a robot and, ifso, the required time and location for the robot to rejoin the team. Next, theproposed task assignment algorithm is used to allocate security-related tasksand verified online tasks. Finally, task fulfillment is managed using a ControlLyapunov Function (CLF)-based controller, while security enforcement is ensuredthrough a Control Barrier Function (CBF)-based security filter. Throughsimulations, we demonstrate that the proposed framework allows MRS toeffectively respond to unplanned online tasks while maintaining securityguarantees.</description>
      <author>example@mail.com (Ziqi Yang, Roberto Tron)</author>
      <guid isPermaLink="false">2502.18893v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>RL-OGM-Parking: Lidar OGM-Based Hybrid Reinforcement Learning Planner for Autonomous Parking</title>
      <link>http://arxiv.org/abs/2502.18846v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;自动驾驶技术中的自主泊车研究提出了一种结合规则和学习的方法，通过使用混合策略（包括基于规则的Reeds-Shepp (RS) 规划器和基于学习的强化学习(RL)规划器）来解决传统方法在多变环境下的适应性问题。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶泊车技术面临空间有限且复杂环境的问题，传统的规则基础算法难以应对各种不可预测条件，而学习型算法在不同场景中表现不一致。因此需要一种结合两者优点的混合策略。&lt;h4&gt;目的&lt;/h4&gt;为了提高自主停车系统在现实世界中的适应性和效率，本研究提出了一种新的方法来解决模拟到实际环境转换时存在的差距问题。&lt;h4&gt;方法&lt;/h4&gt;采用一个由基于规则的Reeds-Shepp (RS) 规划器和基于学习的强化学习(RL) 规划器组成的混合策略，并使用实时LiDAR占用网格图（OGM）表示来缩小模拟与现实之间的差距。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在仿真环境和真实世界场景中都优于纯规则基础方法和学习型方法。实际测试进一步验证了该方法的可行性和效率。&lt;h4&gt;结论&lt;/h4&gt;混合策略通过结合规则和学习的优点，在自主泊车技术的实际应用中展现出了良好的性能和适应性。&lt;h4&gt;翻译&lt;/h4&gt;自动驾驶泊车研究提出了一种新颖的方法来解决传统算法在复杂环境中的局限性，利用基于规则的Reeds-Shepp (RS) 和基于强化学习(RL) 的混合策略，并通过实时LiDAR占用网格图(OGM) 表示成功解决了从模拟到实际转换的问题。实验结果显示了该方法相较于单纯使用规则或学习方式在性能上的显著提升，同时确保了其在真实环境中的有效性与高效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous parking has become a critical application in automatic drivingresearch and development. Parking operations often suffer from limited spaceand complex environments, requiring accurate perception and precisemaneuvering. Traditional rule-based parking algorithms struggle to adapt todiverse and unpredictable conditions, while learning-based algorithms lackconsistent and stable performance in various scenarios. Therefore, a hybridapproach is necessary that combines the stability of rule-based methods and thegeneralizability of learning-based methods. Recently, reinforcement learning(RL) based policy has shown robust capability in planning tasks. However, thesimulation-to-reality (sim-to-real) transfer gap seriously blocks thereal-world deployment. To address these problems, we employ a hybrid policy,consisting of a rule-based Reeds-Shepp (RS) planner and a learning-basedreinforcement learning (RL) planner. A real-time LiDAR-based Occupancy Grid Map(OGM) representation is adopted to bridge the sim-to-real gap, leading thehybrid policy can be applied to real-world systems seamlessly. We conductedextensive experiments both in the simulation environment and real-worldscenarios, and the result demonstrates that the proposed method outperformspure rule-based and learning-based methods. The real-world experiment furthervalidates the feasibility and efficiency of the proposed method.</description>
      <author>example@mail.com (Zhitao Wang, Zhe Chen, Mingyang Jiang, Tong Qin, Ming Yang)</author>
      <guid isPermaLink="false">2502.18846v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Attention-Guided Integration of CLIP and SAM for Precise Object Masking in Robotic Manipulation</title>
      <link>http://arxiv.org/abs/2502.18842v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2025 IEEE/SICE International Symposium on System Integration&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的流水线，旨在通过集成CLIP和SAM模型来提高机器人在便利商店产品掩码操作中的精度。&lt;h4&gt;背景&lt;/h4&gt;现有的技术在识别和处理特定环境下的对象时存在局限性，特别是在需要精确对象掩码的场合，如便利店的商品处理。现有模型（例如CLIP和SAM）虽然各自有效，但它们之间的协同作用尚未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;目的是开发一种新的方法来提高基于图像和文本数据集的机器人操作精度，并利用这些改进后的技术来实现更精确、适应性强的产品操纵。&lt;h4&gt;方法&lt;/h4&gt;该研究采用了一个结合了CLIP（用于理解自然语言指令）和SAM（用于分割对象掩码）的集成框架，通过多模态数据处理优化模型性能。此外，还使用了基于梯度的方法以及定制的数据集进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;提出的流水线通过有效的组合现有技术并利用定制化训练策略显著提高了目标对象掩码生成的准确性。&lt;h4&gt;结论&lt;/h4&gt;这种新的方法不仅改进了现有系统的性能，而且为解决机器人在特定环境中操纵物体时遇到的问题提供了一个有价值的框架。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经完成，并且以结构化的JSON格式组织。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel pipeline to enhance the precision of objectmasking for robotic manipulation within the specific domain of masking productsin convenience stores. The approach integrates two advanced AI models, CLIP andSAM, focusing on their synergistic combination and the effective use ofmultimodal data (image and text). Emphasis is placed on utilizinggradient-based attention mechanisms and customized datasets to fine-tuneperformance. While CLIP, SAM, and Grad- CAM are established components, theirintegration within this structured pipeline represents a significantcontribution to the field. The resulting segmented masks, generated throughthis combined approach, can be effectively utilized as inputs for roboticsystems, enabling more precise and adaptive object manipulation in the contextof convenience store products.</description>
      <author>example@mail.com (Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Domae Yukiyasu)</author>
      <guid isPermaLink="false">2502.18842v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Learning Autonomy: Off-Road Navigation Enhanced by Human Input</title>
      <link>http://arxiv.org/abs/2502.18760v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种基于学习的本地规划器，用于解决无人车在复杂和不可预测的越野环境中的导航挑战。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶领域面临着应对不规则地面和意外障碍物等越野地形的独特挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种能够通过单目摄像头捕捉人类驾驶细微差别并快速适应各种越野条件的本地规划器。&lt;h4&gt;方法&lt;/h4&gt;利用少量的人类驾驶示范数据（5-10分钟）进行学习，以掌握在不同类型的地形中导航的能力。&lt;h4&gt;主要发现&lt;/h4&gt;该规划器显著减少了获取人类驾驶偏好的现实世界数据量，并能够将学到的行为直接应用于实际场景，无需手动微调。&lt;h4&gt;结论&lt;/h4&gt;展示了快速适应性和灵活性的越野自主驾驶技术的有效性。&lt;h4&gt;翻译&lt;/h4&gt;在自动驾驶领域，越野地形导航带来了一系列挑战，例如不可预测的地表和意外障碍物等。这项工作提出了一种基于学习的新本地规划器，通过直接从现实世界演示中捕捉人类驾驶特征来应对这些挑战，仅需使用单目摄像头即可实现。该规划器的主要特点是能够应对各种类型的复杂越野环境，并且具有快速的学习能力。凭借最少的人类示范数据（5-10分钟），它能迅速学习在广泛的越野条件下导航的方法。该本地规划器显著减少了从现实世界中获取人类驾驶偏好的需求，从而使规划器无需手动微调即可将学到的行为应用于实际场景中，展示了其在越野自主驾驶技术中的快速调整和适应能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the area of autonomous driving, navigating off-road terrains presents aunique set of challenges, from unpredictable surfaces like grass and dirt tounexpected obstacles such as bushes and puddles. In this work, we present anovel learning-based local planner that addresses these challenges by directlycapturing human driving nuances from real-world demonstrations using only amonocular camera. The key features of our planner are its ability to navigatein challenging off-road environments with various terrain types and its fastlearning capabilities. By utilizing minimal human demonstration data (5-10mins), it quickly learns to navigate in a wide array of off-road conditions.The local planner significantly reduces the real world data required to learnhuman driving preferences. This allows the planner to apply learned behaviorsto real-world scenarios without the need for manual fine-tuning, demonstratingquick adjustment and adaptability in off-road autonomous driving technology.</description>
      <author>example@mail.com (Akhil Nagariya, Dimitar Filev, Srikanth Saripalli, Gaurav Pandey)</author>
      <guid isPermaLink="false">2502.18760v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Simulating Safe Bite Transfer in Robot-Assisted Feeding with a Soft Head and Articulated Jaw</title>
      <link>http://arxiv.org/abs/2502.18749v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于物理模拟器（MuJoCo）的软体动力学建模的方法，用于研究机器人辅助喂食过程中的人机交互问题。&lt;h4&gt;背景&lt;/h4&gt;在机器人辅助喂食中确保安全舒适的咬取传递是一个挑战，因为这需要密切的物理人机互动。&lt;h4&gt;目的&lt;/h4&gt;本文旨在通过模拟器中的真实皮肤接触动态来系统地评估咬取转移参数（如插入深度和入口角度）对用户安全性和舒适性的影响。&lt;h4&gt;方法&lt;/h4&gt;该研究将一个灵活的人头模型与刚性骨架集成在一起，并考虑了内部动力学，以便由骨骼驱动的柔性模型能够被激活。并且利用软体皮肤接触动态在模拟中进行评估。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在假设头部静止的情况下，直入直出策略可以减少力的作用并提高用户舒适度。&lt;h4&gt;结论&lt;/h4&gt;基于仿真的方法为实际世界实验提供了一种更安全、控制更好的替代方案。&lt;h4&gt;翻译&lt;/h4&gt;确保机器人辅助喂食过程中咬取传递的安全与舒适是一个挑战，因为这需要紧密的人机物理交互。本文提出一种新的建模方式，在基于物理学的模拟器（MuJoCo）中使用软体动力学来仿真这种互动。我们整合了一个可变形头部模型和一个刚性骨架，并考虑了内部动态机制，使灵活模型可以通过骨架进行驱动。在模拟中的真实皮肤接触动态被纳入，以便系统地评估咬取转移参数如插入深度和入口角度及其对用户安全性和舒适度的影响。我们的研究结果表明，在假设头部静止的情况下，采取直入直出策略可以减少作用力并提升用户体验的舒适性。这种基于仿真的方法为实际世界实验提供了一种更安全、控制更好的替代方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safe and comfortable bite transfer during robot-assisted feeding ischallenging due to the close physical human-robot interaction required. Thispaper presents a novel approach to modeling physical human-robot interaction ina physics-based simulator (MuJoCo) using soft-body dynamics. We integrate aflexible head model with a rigid skeleton while accounting for internaldynamics, enabling the flexible model to be actuated by the skeleton.Incorporating realistic soft-skin contact dynamics in simulation allows forsystematically evaluating bite transfer parameters, such as insertion depth andentry angle, and their impact on user safety and comfort. Our findings suggestthat a straight-in-straight-out strategy minimizes forces and enhances usercomfort in robot-assisted feeding, assuming a static head. Thissimulation-based approach offers a safer and more controlled alternative toreal-world experimentation. Supplementary videos can be found at:https://tinyurl.com/224yh2kx.</description>
      <author>example@mail.com (Yi Heng San, Vasanthamaran Ravichandram, J-Anne Yow, Sherwin Stephen Chan, Yifan Wang, Wei Tech Ang)</author>
      <guid isPermaLink="false">2502.18749v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>QueryAdapter: Rapid Adaptation of Vision-Language Models in Response to Natural Language Queries</title>
      <link>http://arxiv.org/abs/2502.18735v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'问题描述': '现有视觉-语言模型(VLM)在大规模互联网数据上训练，与机器人收集的原始图像流之间存在领域差异。当前适应策略需要定义一个封闭类集合，这不适用于必须响应多样化自然语言查询的机器人。', '解决方案': '提出QueryAdapter框架，该框架能够通过以前部署期间采集的未标记数据快速调整预训练VLM以应对特定查询。', '技术手段': '利用优化可学习提示令牌和主动选择用于训练的对象来生成适应后的模型，整个过程仅需几分钟。', '处理非相关对象方法': '提出使用与查询无关的对象标题作为负类标签，有助于在自适应过程中产生更校准的置信度分数。', '实验结果': 'ScanNet++数据集上的大量实验证明，QueryAdapter比现有的无监督VLM适配器和3D场景图方法显著提高了对象检索性能。', '泛化能力': '该方法对抽象功能查询和其他数据集（如Ego4D）表现出强大的泛化能力。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了在大规模互联网训练的数据与机器人收集的原始图像之间存在的领域差异，以及现有适应策略面对自然语言多样性时的实际挑战。为解决这一问题，提出了QueryAdapter框架，利用以前未标记数据来调整VLM以应对特定查询，并通过优化可学习提示令牌和主动选择用于训练的对象实现快速调整。此外，提出了一种处理无关对象的新方法，并展示了在ScanNet++等数据集上的优越性能以及对其他类型查询的泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-26&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A domain shift exists between the large-scale, internet data used to train aVision-Language Model (VLM) and the raw image streams collected by a robot.Existing adaptation strategies require the definition of a closed-set ofclasses, which is impractical for a robot that must respond to diverse naturallanguage queries. In response, we present QueryAdapter; a novel framework forrapidly adapting a pre-trained VLM in response to a natural language query.QueryAdapter leverages unlabelled data collected during previous deployments toalign VLM features with semantic classes related to the query. By optimisinglearnable prompt tokens and actively selecting objects for training, an adaptedmodel can be produced in a matter of minutes. We also explore how objectsunrelated to the query should be dealt with when using real-world data foradaptation. In turn, we propose the use of object captions as negative classlabels, helping to produce better calibrated confidence scores duringadaptation. Extensive experiments on ScanNet++ demonstrate that QueryAdaptersignificantly enhances object retrieval performance compared tostate-of-the-art unsupervised VLM adapters and 3D scene graph methods.Furthermore, the approach exhibits robust generalization to abstract affordancequeries and other datasets, such as Ego4D.</description>
      <author>example@mail.com (Nicolas Harvey Chapman, Feras Dayoub, Will Browne, Christopher Lehnert)</author>
      <guid isPermaLink="false">2502.18735v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based Motion Prediction with Parameter Optimization</title>
      <link>http://arxiv.org/abs/2502.18696v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;船舶部署自主导航系统需要量身定制的精确运动预测模型。传统的基于物理的方法难以适应实际情况中的船体特定行为，而完全数据驱动的方法虽然能够提供具体性但缺乏解释性和在极端情况下的鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统基于物理学的模型和纯粹的数据驱动方法各有优缺点：前者基于流体力学原理但在实际操作中无法准确反映船舶特性；后者则具备特定船只的行为预测能力，但是不具有可解释性和应对特殊情况的稳健性。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合物理建模与数据驱动优化参数的方法，利用这两种方法的优点，确保模型既具解释性又适应性强。&lt;h4&gt;方法&lt;/h4&gt;该研究引入了一种基于数据和物理学相结合的新模型，它包含了三自由度动力学、舵机力和螺旋桨推力等物理成分，并通过合成数据来微调阻力曲线及舵系数等参数。这种方法将领域知识融入到参数优化过程中，保证了所拟合的模型在物理一致性方面具有较高的性能。&lt;h4&gt;主要发现&lt;/h4&gt;对两艘集装箱船进行实验验证后发现：与基于传统海洋工程实践调整的基本物理学模型相比，采用数据驱动和物理学结合的方法，在预测精度和可靠性方面取得了显著改善。该模型能够捕捉不同条件下船舶特定的行为，并且其预测结果比基准物理模型分别提高了51.6%（船只A）和57.8%（船只B），一致性分别提升了72.36%（船只A）和89.67%（船只B）。&lt;h4&gt;结论&lt;/h4&gt;通过结合物理学方程与数据驱动参数优化，可以创建更准确、可靠且具有解释性的船舶运动预测模型。&lt;h4&gt;翻译&lt;/h4&gt;部署在船上的自主导航系统要求有适合特定船舶的精确运动预测模型。传统基于物理的方法虽然遵循流体力学原理，但在实际条件下难以捕捉到每艘船的独特行为模式；相比之下，纯粹的数据驱动方法提供了具体性但缺乏解释性和极端情况下的稳健性。本研究提出了一种结合物理学基础方程与数据驱动参数优化的新模型，该模型融合了两种方法的优势以确保可解性及适应性。此模型包含三自由度动力学、舵机力和螺旋桨推力等物理组件，并利用合成数据对阻力曲线和舵系数等参数进行优化调整，将领域知识嵌入到参数优化过程中，保持所拟合模型的物理一致性。验证该方法的有效性通过与基于传统海洋工程实践的基本物理模型进行了定性和定量比较，结果表明结合了数据驱动和物理学的新方法在预测精度及可靠性方面显著优于传统的基础物理模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The deployment of autonomous navigation systems on ships necessitatesaccurate motion prediction models tailored to individual vessels. Traditionalphysics-based models, while grounded in hydrodynamic principles, often fail toaccount for ship-specific behaviors under real-world conditions. Conversely,purely data-driven models offer specificity but lack interpretability androbustness in edge cases. This study proposes a data-driven physics-based modelthat integrates physics-based equations with data-driven parameteroptimization, leveraging the strengths of both approaches to ensureinterpretability and adaptability. The model incorporates physics-basedcomponents such as 3-DoF dynamics, rudder, and propeller forces, whileparameters such as resistance curve and rudder coefficients are optimized usingsynthetic data. By embedding domain knowledge into the parameter optimizationprocess, the fitted model maintains physical consistency. Validation of theapproach is realized with two container ships by comparing, both qualitativelyand quantitatively, predictions against ground-truth trajectories. The resultsdemonstrate significant improvements, in predictive accuracy and reliability,of the data-driven physics-based models over baseline physics-based modelstuned with traditional marine engineering practices. The fitted models captureship-specific behaviors in diverse conditions with their predictions being,51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67%(ship B) more consistent.</description>
      <author>example@mail.com (Papandreou Christos, Mathioudakis Michail, Stouraitis Theodoros, Iatropoulos Petros, Nikitakis Antonios, Stavros Paschalakis, Konstantinos Kyriakopoulos)</author>
      <guid isPermaLink="false">2502.18696v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Voting-Based Task Assignment in Role-Playing Games</title>
      <link>http://arxiv.org/abs/2502.18690v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for presentation at Dungeons, Neurons, and Dialogues: Social  Interaction Dynamics in Contextual Games Workshop at 20th Annual ACM/IEEE  International Conference on Human-Robot Interaction (HRI 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;在角色扮演游戏（RPG）中，沉浸感至关重要，尤其是当游戏中的代理向玩家传达任务、提示或想法时。为了准确解读玩家的情感状态和上下文细节，需要一个基础的理解层次，这可以通过大型语言模型（LLM）来实现。&lt;h4&gt;目的&lt;/h4&gt;保持LLM在多变的上下文中持续聚焦，需要一种更为稳健的方法，如将LLM与专用的任务分配模型结合以在整个游戏过程中引导其性能。为应对这一需求，我们提出了基于投票的任务指派框架(VBTA)，该方法借鉴了人类在任务分配和完成过程中的推理。&lt;h4&gt;方法&lt;/h4&gt;VBTA给代理分配能力配置文件，并向任务描述提供任务说明，然后生成一个适配矩阵来量化代理人能力和任务要求之间的匹配度。通过利用六种不同的投票方式、预训练的LLM以及结合冲突解决搜索（CBS）进行路径规划，VBTA能够高效地识别并为每个任务指派最合适的代理。&lt;h4&gt;主要发现&lt;/h4&gt;与现有的仅专注于生成游戏单个方面的方法不同（如单一任务或战斗遭遇），我们的方法因其通用性而显示出在生成独特的战斗和叙事方面的潜力。&lt;h4&gt;结论&lt;/h4&gt;通过结合LLM和先进的任务分配技术，可以显著提高游戏角色的沉浸感以及玩家体验的质量。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在角色扮演游戏中，沉浸度至关重要——尤其是在游戏中的代理向玩家传达任务、提示或想法时。为了准确地解读玩家的情绪状态和情境细微差别，需要一个基础的理解层面，这可以通过大型语言模型（LLM）来实现。然而，保持LLM在整个游戏过程中对多个上下文变化的关注，则需要一种更为稳健的方法，例如将LLM与专门的任务分配模型相结合以引导其性能表现。为应对这一需求，我们提出了基于投票的任务指派框架（VBTA），该方法受人类在任务分配和完成过程中的推理启发。VBTA给代理分配能力配置文件，并向任务描述提供任务说明，然后生成一个适配矩阵来量化代理人能力和任务要求之间的匹配度。通过利用六种不同的投票方式、预训练的LLM以及结合冲突解决搜索（CBS）进行路径规划，VBTA能够高效地识别并为每个任务指派最合适的代理。与现有的仅专注于生成游戏单个方面的方法不同（如单一任务或战斗遭遇），我们的方法因其通用性而显示出在生成独特的战斗和叙事方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In role-playing games (RPGs), the level of immersion is critical-especiallywhen an in-game agent conveys tasks, hints, or ideas to the player. For anagent to accurately interpret the player's emotional state and contextualnuances, a foundational level of understanding is required, which can beachieved using a Large Language Model (LLM). Maintaining the LLM's focus acrossmultiple context changes, however, necessitates a more robust approach, such asintegrating the LLM with a dedicated task allocation model to guide itsperformance throughout gameplay. In response to this need, we introduceVoting-Based Task Assignment (VBTA), a framework inspired by human reasoning intask allocation and completion. VBTA assigns capability profiles to agents andtask descriptions to tasks, then generates a suitability matrix that quantifiesthe alignment between an agent's abilities and a task's requirements.Leveraging six distinct voting methods, a pre-trained LLM, and integratingconflict-based search (CBS) for path planning, VBTA efficiently identifies andassigns the most suitable agent to each task. While existing approaches focuson generating individual aspects of gameplay, such as single quests, or combatencounters, our method shows promise when generating both unique combatencounters and narratives because of its generalizable nature.</description>
      <author>example@mail.com (Daniel Weiner, Raj Korpan)</author>
      <guid isPermaLink="false">2502.18690v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Rapidly Built Medical Crash Cart! Lessons Learned and Impacts on High-Stakes Team Collaboration in the Emergency Room</title>
      <link>http://arxiv.org/abs/2502.18688v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 6 figures, HRI conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;研究团队设计了一种能够在紧急情况下的临床环境中使用的医疗机器人急救车（MCCR），并对其进行了现场评估。&lt;h4&gt;背景&lt;/h4&gt;在高风险的应急场景中，设计能够无缝融入快速变化环境、促进有效沟通以及适应突发状况的机器人面临着独特的挑战。尽管远程操作机器人已经在诸如消防和太空探索等高风险领域得到了成功的应用，但自主支持团队协作的机器人仍然未被充分研究。&lt;h4&gt;目的&lt;/h4&gt;为了填补这一空白，研究者通过一个快速原型设计过程开发了一系列看似自主的机器人，旨在帮助急诊室中的临床团队。&lt;h4&gt;方法&lt;/h4&gt;将标准急救推车改造成医疗机器人急救车（MCCR），并进行了实地部署评估以检验其对团队工作量和使用性的影响。同时，还确定了失败分类，并在与卫生专业人员的合作中完善了MCCR的设计。&lt;h4&gt;主要发现&lt;/h4&gt;该研究推进了为高风险、时间敏感场景设计的机器人理解，并提供了有关有用的MCCR能力以及有效的人机协作考虑因素的见解。&lt;h4&gt;结论&lt;/h4&gt;通过公开发布MCCR教程，希望激发HRI研究人员探索为高风险团队工作设计机器人的可能性。&lt;h4&gt;翻译&lt;/h4&gt;设计用于支持紧急情况下的高水平合作工作的机器人面临着独特的挑战，包括无缝地融入快速变化的环境、促进成员之间的有效沟通以及适应突发状况。尽管远程操作机器人已经在消防和太空探索等关键领域得到了成功应用，但自主机器人在支援关键团队工作方面仍有待进一步研究。为了填补这一空白，研究人员通过一种快速原型设计方法开发了一系列看似自主的机器人来协助急诊室中的临床团队。将标准急救推车改造为医疗机器人急救推车（MCCR），并通过实地部署评估了其对团队工作量和易用性的影响，并根据与医疗卫生专业人员的合作反馈进一步完善了该设备的设计，确定了失效分类。这项研究推进了高风险、时间紧迫场景下机器人设计的理解，提供了有关有用的MCCR能力及有效人机协作的考虑因素的见解。通过公开发布MCCR教程，研究人员希望鼓励HRI（人类-机器人互动）领域的学者们探索为关键团队工作设计机器人的可能性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Designing robots to support high-stakes teamwork in emergency settingspresents unique challenges, including seamless integration into fast-pacedenvironments, facilitating effective communication among team members, andadapting to rapidly changing situations. While teleoperated robots have beensuccessfully used in high-stakes domains such as firefighting and spaceexploration, autonomous robots that aid highs-takes teamwork remainunderexplored. To address this gap, we conducted a rapid prototyping process todevelop a series of seemingly autonomous robot designed to assist clinicalteams in the Emergency Room. We transformed a standard crash cart--which storesmedical equipment and emergency supplies into a medical robotic crash cart(MCCR). The MCCR was evaluated through field deployments to assess its impacton team workload and usability, identified taxonomies of failure, and refinedthe MCCR in collaboration with healthcare professionals. Our work advances theunderstanding of robot design for high-stakes, time-sensitive settings,providing insights into useful MCCR capabilities and considerations foreffective human-robot collaboration. By publicly disseminating our MCCRtutorial, we hope to encourage HRI researchers to explore the design of robotsfor high-stakes teamwork.</description>
      <author>example@mail.com (Angelique Taylor, Tauhid Tanjim, Michael Joseph Sack, Maia Hirsch, Kexin Cheng, Kevin Ching, Jonathan St. George, Thijs Roumen, Malte F. Jung, Hee Rin Lee)</author>
      <guid isPermaLink="false">2502.18688v1</guid>
      <pubDate>Thu, 27 Feb 2025 17:24:01 +0800</pubDate>
    </item>
    <item>
      <title>Primitive-Swarm: An Ultra-lightweight and Scalable Planner for Large-scale Aerial Swarms</title>
      <link>http://arxiv.org/abs/2502.16887v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为Primitive-Swarm的轻量级且可扩展的大规模自主空中群规划器，采用分散式和异步重规划策略，并结合了基于可达性分析的时间最优路径参数化算法（TOPP-RA）生成的时间最优化和动态可行轨迹库。&lt;h4&gt;背景&lt;/h4&gt;大规模空中无人机集群操作在计算效率与可扩展性之间存在固有的矛盾。当前的方案难以同时实现高效的计算性能与广泛的应用范围。&lt;h4&gt;目的&lt;/h4&gt;提出一种适用于大规模自主空中无人机集群的有效规划器，旨在解决上述矛盾。&lt;h4&gt;方法&lt;/h4&gt;{'策略': '采用分散式和异步重规划策略', '轨迹库': '开发了一种时间最优化的运动原语库，并基于可达性分析生成这些原语', '碰撞检测机制': '通过与离散空间关联，建立快速碰撞检查机制以处理机器人障碍物冲突及机器人间的碰撞'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能优势': '在密集环境中实现了最短飞行时间和最小移动距离，并且计算时间少于1ms。', '可扩展性验证': '通过大规模模拟实验（最多涉及1000个无人机）验证了方案的实时性和可扩展性。', '实际应用可行性': '通过真实世界实验展示了该方法的实际可行性和鲁棒性'}&lt;h4&gt;结论&lt;/h4&gt;Primitive-Swarm规划器提供了一种新的解决大规模空中集群操作中计算效率与可扩展性的矛盾的方法，适用于各种复杂环境和应用场景。同时，公开源代码以促进社区合作。&lt;h4&gt;翻译&lt;/h4&gt;实现大规模的空中群是具有挑战性的，因为需要在计算效率和可扩展性之间进行权衡。本文介绍了一种名为Primitive-Swarm的轻量级且可扩展的规划器，旨在解决大规模自主飞行器集群的问题。该方法采用分散式和异步重规划策略，并使用一种基于可达性分析的时间最优路径参数化算法（TOPP-RA）来生成轨迹库。通过结合这些运动原语与离散空间来处理碰撞检测机制。最后，实验表明这种方案能够以极短计算时间在密集环境中实现最短的飞行时间和最小的距离移动，在大规模模拟和真实世界场景中均表现出优异性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving large-scale aerial swarms is challenging due to the inherentcontradictions in balancing computational efficiency and scalability. Thispaper introduces Primitive-Swarm, an ultra-lightweight and scalable plannerdesigned specifically for large-scale autonomous aerial swarms. The proposedapproach adopts a decentralized and asynchronous replanning strategy. Within itis a novel motion primitive library consisting of time-optimal and dynamicallyfeasible trajectories. They are generated utlizing a novel time-optimial pathparameterization algorithm based on reachability analysis (TOPP-RA). Then, arapid collision checking mechanism is developed by associating the motionprimitives with the discrete surrounding space according to conflicts. Byconsidering both spatial and temporal conflicts, the mechanism handlesrobot-obstacle and robot-robot collisions simultaneously. Then, during areplanning process, each robot selects the safe and minimum cost trajectoryfrom the library based on user-defined requirements. Both the time-optimalmotion primitive library and the occupancy information are computed offline,turning a time-consuming optimization problem into a linear-complexityselection problem. This enables the planner to comprehensively explore thenon-convex, discontinuous 3-D safe space filled with numerous obstacles androbots, effectively identifying the best hidden path. Benchmark comparisonsdemonstrate that our method achieves the shortest flight time and traveleddistance with a computation time of less than 1 ms in dense environments. Superlarge-scale swarm simulations, involving up to 1000 robots, running inreal-time, verify the scalability of our method. Real-world experimentsvalidate the feasibility and robustness of our approach. The code will bereleased to foster community collaboration.</description>
      <author>example@mail.com (Jialiang Hou, Xin Zhou, Neng Pan, Ang Li, Yuxiang Guan, Chao Xu, Zhongxue Gan, Fei Gao)</author>
      <guid isPermaLink="false">2502.16887v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
  <item>
      <title>Leveraging Large Language Models for Effective and Explainable Multi-Agent Credit Assignment</title>
      <link>http://arxiv.org/abs/2502.16863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages+Appendix, 6 Figures, AAMAS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法LLM-MCA，利用大型语言模型来评估多智能体系统中每个代理的行为对团队成功或失败的贡献。这种方法通过将信用分配问题转化为序列改进和归因的模式识别任务，解决了现有的集中训练-分散执行范式下的挑战。&lt;h4&gt;背景&lt;/h4&gt;最近的工作表明学习协作行为对于机器人实现共同目标的重要性，并且通常使用集中训练-分散执行的方法来学习这种合作行为。然而，这带来了如何评估每个代理的行为对团队成功或失败贡献的问题。&lt;h4&gt;目的&lt;/h4&gt;解决多智能体强化学习中的信用分配问题，通过结合人类手动审查代理行为的观察结果和大型语言模型在模式识别任务中表现出的人类水平性能的研究发现，提出一种新的方法来改进协作机器人系统的信用分配机制。&lt;h4&gt;方法&lt;/h4&gt;论文提出了LLM-MCA方法，利用中心化的大型语言模型奖励评论器根据场景中的每个代理的独特贡献对环境奖励进行数值分解，并基于此反馈更新代理的策略网络。此外还介绍了一种扩展版本LLM-TACA，其中大型语言模型批评者执行显式任务分配。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在各种基准测试中大大优于现有技术，包括层次化觅食、机器人仓库和新的太空世界基准测试，后者的环境包含了碰撞相关的安全性限制。这些方法还生成了带有每个时间步长代理奖励信息的大型轨迹数据集。&lt;h4&gt;结论&lt;/h4&gt;论文证明了利用大型语言模型进行信用分配可以提高多智能体系统的性能，并为未来的研究提供了一种全新的思路和潜在的应用场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent work, spanning from autonomous vehicle coordination to in-spaceassembly, has shown the importance of learning collaborative behavior forenabling robots to achieve shared goals. A common approach for learning thiscooperative behavior is to utilize the centralized-trainingdecentralized-execution paradigm. However, this approach also introduces a newchallenge: how do we evaluate the contributions of each agent's actions to theoverall success or failure of the team. This credit assignment problem hasremained open, and has been extensively studied in the Multi-AgentReinforcement Learning literature. In fact, humans manually inspecting agentbehavior often generate better credit evaluations than existing methods. Wecombine this observation with recent works which show Large Language Modelsdemonstrate human-level performance at many pattern recognition tasks. Our keyidea is to reformulate credit assignment to the two pattern recognitionproblems of sequence improvement and attribution, which motivates our novelLLM-MCA method. Our approach utilizes a centralized LLM reward-critic whichnumerically decomposes the environment reward based on the individualizedcontribution of each agent in the scenario. We then update the agents' policynetworks based on this feedback. We also propose an extension LLM-TACA whereour LLM critic performs explicit task assignment by passing an intermediarygoal directly to each agent policy in the scenario. Both our methods faroutperform the state-of-the-art on a variety of benchmarks, includingLevel-Based Foraging, Robotic Warehouse, and our new Spaceworld benchmark whichincorporates collision-related safety constraints. As an artifact of ourmethods, we generate large trajectory datasets with each timestep annotatedwith per-agent reward information, as sampled from our LLM critics.</description>
      <author>example@mail.com (Kartik Nagpal, Dayi Dong, Jean-Baptiste Bouvier, Negar Mehr)</author>
      <guid isPermaLink="false">2502.16863v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Graph Augmentation for Cross Graph Domain Generalization</title>
      <link>http://arxiv.org/abs/2502.18188v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个新的图结构增强技术，用于跨图节点分类问题。通过去除可能影响GNN泛化能力的低权重边，并基于同分布节点特征生成不变结构，以帮助GNN捕捉不同图结构之间的本质不变信息。&lt;h4&gt;背景&lt;/h4&gt;跨图节点分类可以看作是图神经网络领域泛化的结构转移问题，而当前的研究主要集中在模型训练上，数据增强技术尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的图结构增强方法，以解决跨图领域的泛化问题，提高GNN在不同分布的数据集上的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;通过低权重边去除减少噪声干扰，并使用基于聚类的添加边策略生成不变结构。这两种技术共同提高了GNN对领域不变信息的保持和利用。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在跨域数据集上，这种方法能够提高图神经网络的泛化能力，并且比传统增强方法取得了更好的性能表现。&lt;h4&gt;结论&lt;/h4&gt;该工作证明了通过设计针对性的数据增强策略可以显著提升跨图节点分类任务中的模型泛化性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-graph node classification, utilizing the abundant labeled nodes fromone graph to help classify unlabeled nodes in another graph, can be viewed as adomain generalization problem of graph neural networks (GNNs) due to thestructure shift commonly appearing among various graphs. Nevertheless, currentendeavors for cross-graph node classification mainly focus on model training.Data augmentation approaches, a simple and easy-to-implement domaingeneralization technique, remain under-explored. In this paper, we develop anew graph structure augmentation for the crossgraph domain generalizationproblem. Specifically, low-weight edgedropping is applied to remove potentialnoise edges that may hinder the generalization ability of GNNs, stimulating theGNNs to capture the essential invariant information underlying differentstructures. Meanwhile, clustering-based edge-adding is proposed to generateinvariant structures based on the node features from the same distribution.Consequently, with these augmentation techniques, the GNNs can maintain thedomain invariant structure information that can improve the generalizationability. The experiments on out-ofdistribution citation network datasets verifyour method achieves state-of-the-art performance among conventionalaugmentations.</description>
      <author>example@mail.com (Guanzi Chen, Jiying Zhang, Yang Li)</author>
      <guid isPermaLink="false">2502.18188v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing DNA Foundation Models to Address Masking Inefficiencies</title>
      <link>http://arxiv.org/abs/2502.18405v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了在基因组序列建模中广泛采用的遮蔽语言模型（MLM）预训练目标存在的问题，并提出了一种基于掩码自动编码器框架的修改版编解码架构来解决BERT基础变换器中的效率低下问题。&lt;h4&gt;背景&lt;/h4&gt;遮蔽语言模型（MLM）在基因组序列建模中被广泛采用，但这种模型在从预训练到推断的应用过程中存在分布偏移的问题。即，在下游任务中没有[MASK]标记，导致编码器忽视了非[MASK]标记的编码。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于掩码自动编码框架的修改版编解码架构来解决MLM模型中的效率低下问题，并在基因组管道中进行验证。&lt;h4&gt;方法&lt;/h4&gt;利用遮蔽自编码器框架设计了一种新的BERT基础变换器，通过实验展示这种方法比因果模型和双向架构（使用MLM任务预训练）在闭集和开集分类任务上表现更好。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法可以显著提高基因组序列建模中特征提取的性能，并且在BIOSCAN-5M数据集中实现了明显的性能提升。&lt;h4&gt;结论&lt;/h4&gt;新的方法比传统的遮蔽语言模型（MLM）更有效，尤其是在不进行微调的情况下用于特征提取的应用场景。这种方法提供了一种解决现有问题的新途径。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Masked language modelling (MLM) as a pretraining objective has been widelyadopted in genomic sequence modelling. While pretrained models can successfullyserve as encoders for various downstream tasks, the distribution shift betweenpretraining and inference detrimentally impacts performance, as the pretrainingtask is to map [MASK] tokens to predictions, yet the [MASK] is absent duringdownstream applications. This means the encoder does not prioritize itsencodings of non-[MASK] tokens, and expends parameters and compute on work onlyrelevant to the MLM task, despite this being irrelevant at deployment time. Inthis work, we propose a modified encoder-decoder architecture based on themasked autoencoder framework, designed to address this inefficiency within aBERT-based transformer. We empirically show that the resulting mismatch isparticularly detrimental in genomic pipelines where models are often used forfeature extraction without fine-tuning. We evaluate our approach on theBIOSCAN-5M dataset, comprising over 2 million unique DNA barcodes. We achievesubstantial performance gains in both closed-world and open-worldclassification tasks when compared against causal models and bidirectionalarchitectures pretrained with MLM tasks.</description>
      <author>example@mail.com (Monireh Safari, Pablo Millan Arias, Scott C. Lowe, Lila Kari, Angel X. Chang, Graham W. Taylor)</author>
      <guid isPermaLink="false">2502.18405v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ExPath: Towards Explaining Targeted Pathways for Biological Knowledge Bases</title>
      <link>http://arxiv.org/abs/2502.18026v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的路径推理框架ExPath，该框架能够将实验数据（特别是氨基酸序列）与生物网络数据库中的图分类相结合。&lt;h4&gt;背景&lt;/h4&gt;现有的生物学知识库提供细胞或有机体分子互作的功能通路。然而，识别更具体的目标通路，特别是在结合实验室数据时，仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的路径推理框架ExPath来解决这个挑战，并能够明确整合实验数据。&lt;h4&gt;方法&lt;/h4&gt;该框架由三个组成部分构成：1. 一个大型蛋白质语言模型pLM；2. PathMamba混合架构；3. PathExplainer子图学习模块。这些技术组合能处理氨基酸序列、捕捉局部和全局依赖关系并识别功能关键节点与边缘。&lt;h4&gt;主要发现&lt;/h4&gt;实验涉及了对301个生物网络的评价，表明通过ExPath推理出的路径具有生物学意义，并计划公开发布经过整理的生物网络数据集。&lt;h4&gt;结论&lt;/h4&gt;提出的框架在处理氨基酸序列和生成有意义的生物路径方面表现出了有效性。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：生物知识库提供细胞或有机体分子互作的功能通路。然而，识别更具体的目标通路尤其当结合实验室数据时仍具有挑战性，并且通常需要下游生物学分析及专业知识。本文将此视为可解的图学习和解释任务并提出了一种新的路径推理框架ExPath，该框架明确整合实验数据（特别是氨基酸序列）来分类生物数据库中的各种图形网络。对分类更有贡献的链接可以被考虑为目标通路。技术上来说，ExPath由三个组件组成：1. 大型蛋白质语言模型pLM；2. PathMamba混合架构；3. PathExplainer子图学习模块。我们还提出了ML导向的生物学评价和新度量标准。涉及301个生物网络评估的实验表明路径通过ExPath推理保持生物意义，我们将很快公开发布整理过的301个生物网络数据集。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Biological knowledge bases provide systemically functional pathways of cellsor organisms in terms of molecular interaction. However, recognizing moretargeted pathways, particularly when incorporating wet-lab experimental data,remains challenging and typically requires downstream biological analyses andexpertise. In this paper, we frame this challenge as a solvable graph learningand explaining task and propose a novel pathway inference framework, ExPath,that explicitly integrates experimental data, specifically amino acid sequences(AA-seqs), to classify various graphs (bio-networks) in biological databases.The links (representing pathways) that contribute more to classification can beconsidered as targeted pathways. Technically, ExPath comprises threecomponents: (1) a large protein language model (pLM) that encodes and embedsAA-seqs into graph, overcoming traditional obstacles in processing AA-seq data,such as BLAST; (2) PathMamba, a hybrid architecture combining graph neuralnetworks (GNNs) with state-space sequence modeling (Mamba) to capture bothlocal interactions and global pathway-level dependencies; and (3)PathExplainer, a subgraph learning module that identifies functionally criticalnodes and edges through trainable pathway masks. We also propose ML-orientedbiological evaluations and a new metric. The experiments involving 301bio-networks evaluations demonstrate that pathways inferred by ExPath maintainbiological meaningfulness. We will publicly release curated 301 bio-networkdata soon.</description>
      <author>example@mail.com (Rikuto Kotoge, Ziwei Yang, Zheng Chen, Yushun Dong, Yasuko Matsubara, Jimeng Sun, Yasushi Sakurai)</author>
      <guid isPermaLink="false">2502.18026v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>DenoMAE2.0: Improving Denoising Masked Autoencoders by Classifying Local Patches</title>
      <link>http://arxiv.org/abs/2502.18202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;我们介绍了DenoMAE2.0，这是一种增强的去噪掩码自动编码器，它结合了局部补丁分类目标和传统的重构损失来提高表示学习和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;与传统的Masked Autoencoders (MAE)不同，后者专注于重建丢失的输入，DenoMAE2.0引入了对未掩盖补丁的位置感知分类，使模型能够捕捉细微的局部特征同时保持全局一致性。这种方法在无线通信中的半监督学习特别有帮助。&lt;h4&gt;目的&lt;/h4&gt;针对噪声水平高和数据稀缺的问题，我们在广泛的信噪比(SNR)条件下进行了广泛的实验，从极低到中等条件，并在一个低数据环境中测试了DenoMAE2.0。&lt;h4&gt;方法&lt;/h4&gt;我们对调制信号分类进行了一系列表现于宽范围SNRs的实验，在极高噪声水平和较低的数据环境下验证模型性能。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明，与前辈Deno-MAE和其他基准相比，DenoMAE2.0在去噪质量和下游分类准确度方面都有显著提高。具体而言，它比DenoMAE提高了1.1%的性能，并且在RadioML基准测试中的星座图分类中，相对于DenoMAE，分别取得了11.83%和16.55%的显着改进。&lt;h4&gt;结论&lt;/h4&gt;DenoMAE2.0通过增加位置感知补丁分类目标，在去噪质量和信号分类准确度方面超越了现有的模型，特别是在高噪声水平和低数据环境下的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce DenoMAE2.0, an enhanced denoising masked autoencoder thatintegrates a local patch classification objective alongside traditionalreconstruction loss to improve representation learning and robustness. Unlikeconventional Masked Autoencoders (MAE), which focus solely on reconstructingmissing inputs, DenoMAE2.0 introduces position-aware classification of unmaskedpatches, enabling the model to capture fine-grained local features whilemaintaining global coherence. This dual-objective approach is particularlybeneficial in semi-supervised learning for wireless communication, where highnoise levels and data scarcity pose significant challenges. We conductextensive experiments on modulation signal classification across a wide rangeof signal-to-noise ratios (SNRs), from extremely low to moderately highconditions and in a low data regime. Our results demonstrate that DenoMAE2.0surpasses its predecessor, Deno-MAE, and other baselines in both denoisingquality and downstream classification accuracy. DenoMAE2.0 achieves a 1.1%improvement over DenoMAE on our dataset and 11.83%, 16.55% significant improvedaccuracy gains on the RadioML benchmark, over DenoMAE, for constellationdiagram classification of modulation signals.</description>
      <author>example@mail.com (Atik Faysal, Mohammad Rostami, Taha Boushine, Reihaneh Gh. Roshan, Huaxia Wang, Nikhil Muralidhar)</author>
      <guid isPermaLink="false">2502.18202v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial Vision-Language Navigation</title>
      <link>http://arxiv.org/abs/2502.18041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Vision-Language Navigation (VLN)旨在通过利用语言指令和视觉线索引导代理穿过环境，在具身人工智能领域扮演关键角色。尽管室内VLN已得到广泛研究，但室外空中VLN仍待深入探索。&lt;h4&gt;背景&lt;/h4&gt;现有问题在于户外空域范围广阔，数据收集更具挑战性，导致缺乏相应的基准测试。&lt;h4&gt;目的&lt;/h4&gt;为解决上述问题，我们提出OpenFly平台及其组成部分：多功能工具链和大规模基准。&lt;h4&gt;方法&lt;/h4&gt;{'开发自动化工具链': '用于自动获取点云、场景语义分割、飞行轨迹创建及指令生成', '构建数据集': '基于该工具链建立包括10万条轨迹在内的大规模空中VLN数据集，覆盖不同高度和长度的18个场景。使用多种渲染引擎（如Unreal Engine, GTA V）和高级技术生成视觉数据。', '模型开发': '提出OpenFly-Agent，一种关键帧感知的VLN模型，该模型以语言指令、当前观察结果及历史关键帧为输入，并直接输出飞行动作'}&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的分析与实验展示了我们平台及模型的优势&lt;h4&gt;结论&lt;/h4&gt;工具链、数据集和代码将开源。&lt;h4&gt;翻译&lt;/h4&gt;视觉-语言导航(VLN)旨在利用语言指令和视觉线索引导代理穿过环境，特别是在具身人工智能领域扮演关键角色。尽管室内VLN已经得到广泛研究，但户外空域的空中导航仍然有待深入探索。主要原因在于户外空间广阔，数据收集更为困难，导致缺乏相应的基准测试。为此我们提出了OpenFly平台，包括多功能工具链和大规模数据集。此平台旨在解决数据采集难题，并开发了关键帧感知模型以提高任务表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision-Language Navigation (VLN) aims to guide agents through an environmentby leveraging both language instructions and visual cues, playing a pivotalrole in embodied AI. Indoor VLN has been extensively studied, whereas outdooraerial VLN remains underexplored. The potential reason is that outdoor aerialview encompasses vast areas, making data collection more challenging, whichresults in a lack of benchmarks. To address this problem, we propose OpenFly, aplatform comprising a versatile toolchain and large-scale benchmark for aerialVLN. Firstly, we develop a highly automated toolchain for data collection,enabling automatic point cloud acquisition, scene semantic segmentation, flighttrajectory creation, and instruction generation. Secondly, based on thetoolchain, we construct a large-scale aerial VLN dataset with 100ktrajectories, covering diverse heights and lengths across 18 scenes. Thecorresponding visual data are generated using various rendering engines andadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3DGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,3D GS supports real-to-sim rendering, further enhancing the realism of thedataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, whichtakes language instructions, current observations, and historical keyframes asinput, and outputs flight actions directly. Extensive analyses and experimentsare conducted, showcasing the superiority of our OpenFly platform andOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.</description>
      <author>example@mail.com (Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li)</author>
      <guid isPermaLink="false">2502.18041v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Escaping The Big Data Paradigm in Self-Supervised Representation Learning</title>
      <link>http://arxiv.org/abs/2502.18056v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and implementation available at:  https://github.com/inescopresearch/scott&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文探讨了在图像自监督表示学习中是否可以摆脱大数据范式的限制。&lt;h4&gt;背景&lt;/h4&gt;大规模数据集和计算资源的需求成为视觉领域进展的障碍，特别是在数据稀缺的情况下。&lt;h4&gt;目的&lt;/h4&gt;旨在探究能否通过新的方法让视觉变换器能够在小规模数据下有效训练，而不依赖于外部的大规模预训练数据集。&lt;h4&gt;方法&lt;/h4&gt;{'SCOTT架构': '一种浅层标记化结构，与遮罩图像建模任务兼容，并向视觉变换器注入卷积先验偏置。', 'MIM-JEPA框架': '提出了一种联合嵌入预测架构，在潜在表示空间内运行以捕捉更多语义特征。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'模型性能': '冻结预训练的SCOTT模型在三个小规模、标准分辨率、细粒度数据集上显著优于全监督方法，并且与依赖大规模预训练、复杂图像增强和更大模型尺寸的最佳方法相当。', '资源节省': '证明了稳健的现成表示可以在有限的数据、计算和模型大小的情况下学习，为医疗影像或机器人等资源受限环境中的计算机应用开辟新道路。'}&lt;h4&gt;结论&lt;/h4&gt;挑战了数据量对于有效视觉表征学习不可或缺的传统观念，并提供了一条通向更可访问且包容性更强的进展的新路径。&lt;h4&gt;翻译&lt;/h4&gt;在视觉表示学习中，大规模的数据集和计算资源的需求已经成为一个主要障碍，尤其是在数据稀缺的情况下。本文探讨了一个关键问题：我们能否摆脱大数据范式，在自监督图像表征学习中实现这一点？为此引入了SCOTT（稀疏卷积标记器转换器），这是一种浅层结构，与遮罩图像建模任务兼容。此外，还提出了一种联合嵌入预测架构，用于在潜在表示空间内运行的遮罩图像建模框架（MIM-JEPA）。这些方法使ViTs能够在比传统所需的规模小得多的数据集上从头开始训练，无需依赖大规模外部预训练数据集。实验证明了该方法的有效性，并挑战了大数据量是视觉表征学习不可或缺的传统观念，为资源受限环境中的计算机应用提供了新路径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The reliance on large-scale datasets and extensive computational resourceshas become a major barrier to advancing representation learning in vision,especially in data-scarce domains. In this paper, we address the criticalquestion: Can we escape the big data paradigm in self-supervised representationlearning from images? We introduce SCOTT (Sparse Convolutional Tokenizer forTransformers), a shallow tokenization architecture that is compatible withMasked Image Modeling (MIM) tasks. SCOTT injects convolutional inductive biasesinto Vision Transformers (ViTs), enhancing their efficacy in small-scale dataregimes. Alongside, we propose to train on a Joint-Embedding PredictiveArchitecture within a MIM framework (MIM-JEPA), operating in latentrepresentation space to capture more semantic features. Our approach enablesViTs to be trained from scratch on datasets orders of magnitude smaller thantraditionally required --without relying on massive external datasets forpretraining. We validate our method on three small-size, standard-resoultion,fine-grained datasets: Oxford Flowers-102, Oxford IIIT Pets-37, andImageNet-100. Despite the challenges of limited data and high intra-classsimilarity, frozen SCOTT models pretrained with MIM-JEPA significantlyoutperform fully supervised methods and achieve competitive results with SOTAapproaches that rely on large-scale pretraining, complex image augmentationsand bigger model sizes. By demonstrating that robust off-the-shelfrepresentations can be learned with limited data, compute, and model sizes, ourwork paves the way for computer applications in resource constrainedenvironments such as medical imaging or robotics. Our findings challenge theprevailing notion that vast amounts of data are indispensable for effectiverepresentation learning in vision, offering a new pathway toward moreaccessible and inclusive advancements in the field.</description>
      <author>example@mail.com (Carlos Vélez García, Miguel Cazorla, Jorge Pomares)</author>
      <guid isPermaLink="false">2502.18056v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>GCDance: Genre-Controlled 3D Full Body Dance Generation Driven By Music</title>
      <link>http://arxiv.org/abs/2502.18309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;生成高质量的全身舞蹈序列是一项挑战，需要严格遵循特定风格的动作编排，并且产生的序列必须是物理上真实的并且与音乐的节拍和节奏精确同步。&lt;h4&gt;背景&lt;/h4&gt;现有的方法难以同时满足严格的风格特性和物理真实性以及精确的时间对齐需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种无分类器扩散框架GCDance，用于生成基于音乐和文本提示的特定类型的舞蹈动作。&lt;h4&gt;方法&lt;/h4&gt;{'特征提取': '结合高级预训练音乐基础模型特征与手工制作的多粒度特性融合来提取音乐特征', '时间步嵌入': '利用CLIP在每个时间步骤内有效地嵌入基于风格的文本提示表示'}&lt;h4&gt;主要发现&lt;/h4&gt;GCDance框架可以生成同一首音乐的不同舞蹈风格，同时确保与音乐节奏和旋律的一致性。&lt;h4&gt;结论&lt;/h4&gt;实验证明了GCDance在FineDance数据集上显著优于现有的最先进方法，并且在AIST++数据集上也取得了竞争性的结果。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Generating high-quality full-body dance sequences from music is a challengingtask as it requires strict adherence to genre-specific choreography. Moreover,the generated sequences must be both physically realistic and preciselysynchronized with the beats and rhythm of the music. To overcome thesechallenges, we propose GCDance, a classifier-free diffusion framework forgenerating genre-specific dance motions conditioned on both music and textualprompts. Specifically, our approach extracts music features by combininghigh-level pre-trained music foundation model features with hand-craftedfeatures for multi-granularity feature fusion. To achieve genrecontrollability, we leverage CLIP to efficiently embed genre-based textualprompt representations at each time step within our dance generation pipeline.Our GCDance framework can generate diverse dance styles from the same piece ofmusic while ensuring coherence with the rhythm and melody of the music.Extensive experimental results obtained on the FineDance dataset demonstratethat GCDance significantly outperforms the existing state-of-the-artapproaches, which also achieve competitive results on the AIST++ dataset. Ourablation and inference time analysis demonstrate that GCDance provides aneffective solution for high-quality music-driven dance generation.</description>
      <author>example@mail.com (Xinran Liu, Xu Dong, Diptesh Kanojia, Wenwu Wang, Zhenhua Feng)</author>
      <guid isPermaLink="false">2502.18309v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>DRAMA: Diverse Augmentation from Large Language Models to Smaller Dense Retrievers</title>
      <link>http://arxiv.org/abs/2502.18460v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;DRAMA是一个利用大型语言模型训练较小的、具有泛化能力的密集检索器的框架。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型在作为密集检索器时表现出色，但其庞大的参数规模带来了推理时间上的计算挑战，包括大规模语料库编码成本高和查询延迟增加的问题。相比之下，小型检索器虽然效率更高，但在有限监督数据下的泛化能力较弱。&lt;h4&gt;目的&lt;/h4&gt;介绍DRAMA框架，旨在利用大型语言模型训练出更小且具有更好泛化的密集检索器。&lt;h4&gt;方法&lt;/h4&gt;采用剪枝后的大型语言模型作为骨干，并在单一阶段对比学习设置下使用多样性的大型语言模型增强数据进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，与传统的编码器基础的检索器相比，DRAMA提供了更好的多语言和长上下文处理能力，并在多种任务和语言上取得了强大的性能。&lt;h4&gt;结论&lt;/h4&gt;该框架展示了连接较小检索器训练与大规模语言模型进展之间的潜在价值，从而弥合了效率与泛化之间的差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated strong effectiveness androbustness while fine-tuned as dense retrievers. However, their large parametersize brings significant inference time computational challenges, including highencoding costs for large-scale corpora and increased query latency, limitingtheir practical deployment. While smaller retrievers offer better efficiency,they often fail to generalize effectively with limited supervised fine-tuningdata. In this work, we introduce DRAMA, a training framework that leveragesLLMs to train smaller generalizable dense retrievers. In particular, we adoptpruned LLMs as the backbone and train on diverse LLM-augmented data in asingle-stage contrastive learning setup. Experiments show that DRAMA offersbetter multilingual and long-context capabilities than traditionalencoder-based retrievers, and achieves strong performance across multiple tasksand languages. These highlight the potential of connecting the training ofsmaller retrievers with the growing advancements in LLMs, bridging the gapbetween efficiency and generalization.</description>
      <author>example@mail.com (Xueguang Ma, Xi Victoria Lin, Barlas Oguz, Jimmy Lin, Wen-tau Yih, Xilun Chen)</author>
      <guid isPermaLink="false">2502.18460v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ToMCAT: Theory-of-Mind for Cooperative Agents in Teams via Multiagent Diffusion Policies</title>
      <link>http://arxiv.org/abs/2502.18438v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了ToMCAT框架，用于基于心智理论（Theory-of-Mind, ToM）生成合作型多智能体系统的轨迹。&lt;h4&gt;背景&lt;/h4&gt;在合作性任务中，了解队友的目标和行为对于团队性能至关重要。现有的方法通常无法动态适应环境变化和队友的行为。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合元学习机制和多代理去噪扩散模型的框架（ToMCAT），以生成条件于心智理论推理的轨迹，并实现在线规划系统来减少资源使用而不损害团队表现。&lt;h4&gt;方法&lt;/h4&gt;1. 使用元学习机制，进行关于队友潜在目标和未来行为的心智理论推理；2. 利用多代理去噪扩散模型，根据代理的目标以及通过ToM计算得出的队友特性生成计划。3. 实现在线规划系统，在检测到先前生成的计划与当前世界状态之间存在差异时动态采样新的轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;1. 动态再规划机制在减少资源使用的同时不损害团队性能方面至关重要；2. 关于环境和队友行为的近期观察结合心智理论推断对于生成适应性策略以应对队友变化至关重要，尤其是在没有关于它们的先前信息的情况下。&lt;h4&gt;结论&lt;/h4&gt;ToMCAT框架提供了一种有效的方法来实现动态的合作智能体系统，通过利用心智理论推理，提高了团队在未知环境中的适应性和效率。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中我们提出了ToMCAT（合作型多代理团队的心智理论），这是一个新的基于心智理论生成轨迹的框架。它结合了一个元学习机制，该机制可以对队友潜在的目标和未来行为进行心智理论推理，以及一个多重代理去噪扩散模型，该模型可以根据智能体及其队友的特性来为智能体及其队友生成计划，这些特性是通过心智理论计算得出的。我们实施了一种在线规划系统，它会在检测到先前生成的计划与当前世界状态之间存在差异时从扩散模型中动态采样新的轨迹（重计划）。我们在模拟烹饪领域使用ToMCAT进行了多次实验。我们的结果强调了动态再规划机制在不牺牲团队性能的前提下减少资源使用的至关重要性。我们还表明，在一个时间段内，代理收集到的关于世界的近期观察和队友的行为结合心智理论推断对于生成适应性的策略以应对队友变化是至关重要的，尤其是在没有提供有关他们的先前信息的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper we present ToMCAT (Theory-of-Mind for Cooperative Agents inTeams), a new framework for generating ToM-conditioned trajectories. Itcombines a meta-learning mechanism, that performs ToM reasoning over teammates'underlying goals and future behavior, with a multiagent denoising-diffusionmodel, that generates plans for an agent and its teammates conditioned on boththe agent's goals and its teammates' characteristics, as computed via ToM. Weimplemented an online planning system that dynamically samples new trajectories(replans) from the diffusion model whenever it detects a divergence between apreviously generated plan and the current state of the world. We conductedseveral experiments using ToMCAT in a simulated cooking domain. Our resultshighlight the importance of the dynamic replanning mechanism in reducing theusage of resources without sacrificing team performance. We also show thatrecent observations about the world and teammates' behavior collected by anagent over the course of an episode combined with ToM inferences are crucial togenerate team-aware plans for dynamic adaptation to teammates, especially whenno prior information is provided about them.</description>
      <author>example@mail.com (Pedro Sequeira, Vidyasagar Sadhu, Melinda Gervasio)</author>
      <guid isPermaLink="false">2502.18438v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>GNN-XAR: A Graph Neural Network for Explainable Activity Recognition in Smart Homes</title>
      <link>http://arxiv.org/abs/2502.17999v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This is a preprint. Paper accepted for publication at the 21st EAI  International Conference on Mobile and Ubiquitous Systems: Computing,  Networking and Services (Mobiquitous)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这篇论文提出了一种新的基于图神经网络的可解释性模型，用于智能家居环境中的传感器数据驱动的人体活动识别。&lt;h4&gt;背景&lt;/h4&gt;在智能家居环境中，传感器数据驱动的人体活动识别对于医疗保健领域尤为重要。目前大多数现有方法依赖于深度学习模型，但这些模型通常不透明且难以理解。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型的图神经网络，该网络不仅能够有效地进行人体活动识别，同时还能提供清晰、直观的解释以增强可解释性。&lt;h4&gt;方法&lt;/h4&gt;利用图神经网络（GNN）来提高传感器数据驱动的人体活动识别的效果，并在此基础上设计了首个为智能家居环境中的HAR任务专门定制的可解释模型。&lt;h4&gt;主要发现&lt;/h4&gt;提出的模型在两个公开数据集上的实验结果表明，它不仅提供了比现有最佳方法更好的解释性，同时还能略微提升人体活动的识别率。&lt;h4&gt;结论&lt;/h4&gt;这项工作是首次将图神经网络应用于传感器数据驱动的人体活动识别，并且通过提高可解释性和性能为该领域的研究开辟了新方向。&lt;h4&gt;翻译&lt;/h4&gt;基于传感器的人体活动识别（HAR）在智能家居环境中的应用至关重要，特别是在医疗保健领域。目前大多数现有方法依赖于深度学习模型如CNN或RNN，这些方法虽有效但输出的原理不透明。最近，提出了一种新的可解释性人工智能（XAI）方法来提供直观的解释。然而，现有的HAR方法并未专门设计为考虑可解释性。本文首次提出了一个专用于智能家居环境中的基于图神经网络的可解释模型，实验证明该方法在提高解释性和识别率方面优于现有最佳方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Sensor-based Human Activity Recognition (HAR) in smart home environments iscrucial for several applications, especially in the healthcare domain. Themajority of the existing approaches leverage deep learning models. While theseapproaches are effective, the rationale behind their outputs is opaque.Recently, eXplainable Artificial Intelligence (XAI) approaches emerged toprovide intuitive explanations to the output of HAR models. To the best of ourknowledge, these approaches leverage classic deep models like CNNs or RNNs.Recently, Graph Neural Networks (GNNs) proved to be effective for sensor-basedHAR. However, existing approaches are not designed with explainability in mind.In this work, we propose the first explainable Graph Neural Network explicitlydesigned for smart home HAR. Our results on two public datasets show that thisapproach provides better explanations than state-of-the-art methods while alsoslightly improving the recognition rate.</description>
      <author>example@mail.com (Michele Fiori, Davide Mor, Gabriele Civitarese, Claudio Bettini)</author>
      <guid isPermaLink="false">2502.17999v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Conformal Prediction Under Generalized Covariate Shift with Posterior Drift</title>
      <link>http://arxiv.org/abs/2502.17744v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to AISTATS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了一种新的转移学习环境下分类问题的解决策略，即在先验分布发生变化的情况下利用带有后验漂移的协变量偏移假设。&lt;h4&gt;背景&lt;/h4&gt;统计学习中收集足够多训练数据往往耗时费力或不切实际。转移学习通过从相关源领域获取知识来改善目标领域的学习性能成为一种有益的方法。&lt;h4&gt;目的&lt;/h4&gt;研究并提出在特定分布假设下的转移学习方法，特别是在后验漂移的协变量偏移设置下实现具有覆盖保证的目标分类。&lt;h4&gt;方法&lt;/h4&gt;提出了一个加权的符合预测分类器，在此框架下，每个数据实例将被赋予一组可能的标签，而不是单个标签。该方法利用了源领域和目标领域的样本。&lt;h4&gt;主要发现&lt;/h4&gt;理论研究表明提出的加权符合预测分类器具有良好的渐近性质；数值研究进一步展示了所提方法的有效性。&lt;h4&gt;结论&lt;/h4&gt;该转移学习方法可以提高目标领域的分类性能，并提供了可靠的覆盖保证。&lt;h4&gt;翻译&lt;/h4&gt;在许多统计学习的实际应用中，收集足够的训练数据往往是昂贵、耗时或不现实的。在这种情况下，通过利用相关源领域中的知识来改进目标领域的学习表现的迁移学习方法更为有利。在此背景下，我们研究了一种特定类型的分类问题——符合预测，在新的分布假设下进行转移学习。基于符合预测框架的分类器为每个数据实例提供一组可能标签而不是单个标签，从而做出更谨慎且安全的决策。在转移学习中，我们考虑了一个广义的‘具有后验漂移的协变量偏移’设置，并提出了一种加权的符合预测分类器，在目标域内保证覆盖的同时利用源和目标样本。理论研究表明这种方法有良好的渐近性质，数值研究进一步证实了所提出的方案的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In many real applications of statistical learning, collecting sufficientlymany training data is often expensive, time-consuming, or even unrealistic. Inthis case, a transfer learning approach, which aims to leverage knowledge froma related source domain to improve the learning performance in the targetdomain, is more beneficial. There have been many transfer learning methodsdeveloped under various distributional assumptions. In this article, we study aparticular type of classification problem, called conformal prediction, under anew distributional assumption for transfer learning. Classifiers under theconformal prediction framework predict a set of plausible labels instead of onesingle label for each data instance, affording a more cautious and saferdecision. We consider a generalization of the \textit{covariate shift withposterior drift} setting for transfer learning. Under this setting, we proposea weighted conformal classifier that leverages both the source and targetsamples, with a coverage guarantee in the target domain. Theoretical studiesdemonstrate favorable asymptotic properties. Numerical studies furtherillustrate the usefulness of the proposed method.</description>
      <author>example@mail.com (Baozhen Wang, Xingye Qiao)</author>
      <guid isPermaLink="false">2502.17744v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>VLM-E2E: Enhancing End-to-End Autonomous Driving with Multimodal Driver Attention Fusion</title>
      <link>http://arxiv.org/abs/2502.18042v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的基于视觉-语言模型的端到端框架VLM-E2E，旨在通过利用高级场景理解和推理能力来增强自动驾驶系统在复杂动态环境中的性能。&lt;h4&gt;背景&lt;/h4&gt;人类驾驶员可以灵活地处理复杂的驾驶情况，但当前的自动化系统难以复制这一技能，因为它们在将二维观察转换为三维空间时往往会丢失关键的语义信息。&lt;h4&gt;目的&lt;/h4&gt;利用视觉-语言模型的优势来增强自动驾驶系统的训练过程，并通过引入注意力线索和文本表示来改善其对环境的理解能力。&lt;h4&gt;方法&lt;/h4&gt;1. 采用VLM-E2E框架，将文本描述融入到鸟瞰图（BEV）特征中以提供语义监督；       2. 引入了BEV-Text可学习加权融合策略以解决多模态信息融合中的模式重要性不平衡问题。&lt;h4&gt;主要发现&lt;/h4&gt;通过关注注意力语义，该框架能够更好地模拟人类驾驶行为，并且在nuScenes数据集上的实验结果显示其性能优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;VLM-E2E提供了一种有效的方法来改善自动驾驶系统在复杂动态环境中的表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Human drivers adeptly navigate complex scenarios by utilizing richattentional semantics, but the current autonomous systems struggle to replicatethis ability, as they often lose critical semantic information when converting2D observations into 3D space. In this sense, it hinders their effectivedeployment in dynamic and complex environments. Leveraging the superior sceneunderstanding and reasoning abilities of Vision-Language Models (VLMs), wepropose VLM-E2E, a novel framework that uses the VLMs to enhance training byproviding attentional cues. Our method integrates textual representations intoBird's-Eye-View (BEV) features for semantic supervision, which enables themodel to learn richer feature representations that explicitly capture thedriver's attentional semantics. By focusing on attentional semantics, VLM-E2Ebetter aligns with human-like driving behavior, which is critical fornavigating dynamic and complex environments. Furthermore, we introduce aBEV-Text learnable weighted fusion strategy to address the issue of modalityimportance imbalance in fusing multimodal information. This approachdynamically balances the contributions of BEV and text features, ensuring thatthe complementary information from visual and textual modality is effectivelyutilized. By explicitly addressing the imbalance in multimodal fusion, ourmethod facilitates a more holistic and robust representation of drivingenvironments. We evaluate VLM-E2E on the nuScenes dataset and demonstrate itssuperiority over state-of-the-art approaches, showcasing significantimprovements in performance.</description>
      <author>example@mail.com (Pei Liu, Haipeng Liu, Haichao Liu, Xin Liu, Jinxin Ni, Jun Ma)</author>
      <guid isPermaLink="false">2502.18042v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Deep-JGAC: End-to-End Deep Joint Geometry and Attribute Compression for Dense Colored Point Clouds</title>
      <link>http://arxiv.org/abs/2502.17939v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种针对密集彩色点云的深度联合几何与属性压缩框架Deep-JGAC，旨在通过利用几何和属性之间的关联来实现高效的点云压缩。&lt;h4&gt;背景&lt;/h4&gt;彩色点云在3D视觉领域中已成为基础表示。然而，庞大的数据量使得有效的点云压缩技术变得迫切需要。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够同时处理几何和属性信息的深度联合压缩框架Deep-JGAC，以提高压缩效率并减少存储需求。&lt;h4&gt;方法&lt;/h4&gt;{'灵活的架构设计': '该框架包括可兼容学习或非学习基元的几何与属性子编码器。', '辅助深度几何编码器': '通过融合属性信息来增强几何潜在表示，并保持解码过程不变。', '属性信息融合模块AIFM': '在几何编码过程中引入，用于融合属性信息。', '优化的颜色化模块': '为解决压缩过程中几何和属性之间的不匹配问题而设计。此模块可以提高颜色化效果并降低计算复杂性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'D1-PSNR性能指标': '相较于G-PCC、V-PCC、GRASP以及PCGCv2，Deep-JGAC分别平均减少了82.96%、36.46%、41.72%和31.16%的比特率。', 'MS-GraphSIM性能指标': '相较于G-PCC、V-PCC及IT-DL-PCC，Deep-JGAC分别平均降低了48.72%、14.67%与57.14%的比特率。', '编码和解码时间成本': '相比于V-PCC和IT-DL-PCC，该方法的编码/解码时间成本分别平均减少了94.29%/24.70%，以及96.75%/91.02%'}&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，Deep-JGAC框架在保持高质量几何重建的同时显著降低了比特率和计算复杂性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的完整中文翻译已包含于以上各个字段中&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Colored point cloud becomes a fundamental representation in the realm of 3Dvision. Effective Point Cloud Compression (PCC) is urgently needed due to hugeamount of data. In this paper, we propose an end-to-end Deep Joint Geometry andAttribute point cloud Compression (Deep-JGAC) framework for dense colored pointclouds, which exploits the correlation between the geometry and attribute forhigh compression efficiency. Firstly, we propose a flexible Deep-JGACframework, where the geometry and attribute sub-encoders are compatible toeither learning or non-learning based geometry and attribute encoders.Secondly, we propose an attribute-assisted deep geometry encoder that enhancesthe geometry latent representation with the help of attribute, where thegeometry decoding remains unchanged. Moreover, Attribute Information FusionModule (AIFM) is proposed to fuse attribute information in geometry coding.Thirdly, to solve the mismatch between the point cloud geometry and attributecaused by the geometry compression distortion, we present an optimizedre-colorization module to attach the attribute to the geometrically distortedpoint cloud for attribute coding. It enhances the colorization and lowers thecomputational complexity. Extensive experimental results demonstrate that interms of the geometry quality metric D1-PSNR, the proposed Deep-JGAC achievesan average of 82.96%, 36.46%, 41.72%, and 31.16% bit-rate reductions ascompared to the state-of-the-art G-PCC, V-PCC, GRASP, and PCGCv2, respectively.In terms of perceptual joint quality metric MS-GraphSIM, the proposed Deep-JGACachieves an average of 48.72%, 14.67%, and 57.14% bit-rate reductions comparedto the G-PCC, V-PCC, and IT-DL-PCC, respectively. The encoding/decoding timecosts are also reduced by 94.29%/24.70%, and 96.75%/91.02% on average ascompared with the V-PCC and IT-DL-PCC.</description>
      <author>example@mail.com (Yun Zhang, Zixi Guo, Linwei Zhu, C. -C. Jay Kuo)</author>
      <guid isPermaLink="false">2502.17939v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Knowledge-enhanced Multimodal ECG Representation Learning with Arbitrary-Lead Inputs</title>
      <link>http://arxiv.org/abs/2502.17900v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;多模态心电图表示学习的最新进展集中在将ECG信号与配对的自由文本报告进行对齐。然而，由于医学语言的复杂性和依赖完整的12导联设置（这种配置在资源不足的情况下经常不可用），这种对齐仍然存在次优的问题。&lt;h4&gt;目的&lt;/h4&gt;提出了一种知识增强的多模态心电图表示学习框架K-MERL，以解决上述问题。该方法利用大型语言模型从自由文本报告中提取结构化知识，并使用一种导联感知的心电图编码器和动态导联掩蔽技术来适应任意输入的导联。&lt;h4&gt;方法&lt;/h4&gt;K-MERL框架通过结合大型语言模型的知识抽取能力和特定于ECG的编码机制，旨在提高心电图数据在不同临床场景下的表示学习效果。它能够处理非标准或不完整的导联设置，使得该方法更加适用于资源受限环境中的应用。&lt;h4&gt;主要发现&lt;/h4&gt;K-MERL在六个外部心电图数据集上的评估中，在零样本分类和线性探针任务上均达到了最先进的性能，并且在部分导联的零样本分类任务上平均比现有方法提高了16%的AUC（曲线下面积）。&lt;h4&gt;结论&lt;/h4&gt;K-MERL框架为解决资源受限条件下心电图分析的问题提供了一个有效的解决方案，通过引入结构化医学知识和改进的心电图编码技术，显著提升了模型在关键临床指标上的表现。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译已经包含在上述各个分点中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in multimodal ECG representation learning center on aligningECG signals with paired free-text reports. However, suboptimal alignmentpersists due to the complexity of medical language and the reliance on a full12-lead setup, which is often unavailable in under-resourced settings. Totackle these issues, we propose **K-MERL**, a knowledge-enhanced multimodal ECGrepresentation learning framework. **K-MERL** leverages large language modelsto extract structured knowledge from free-text reports and employs a lead-awareECG encoder with dynamic lead masking to accommodate arbitrary lead inputs.Evaluations on six external ECG datasets show that **K-MERL** achievesstate-of-the-art performance in zero-shot classification and linear probingtasks, while delivering an average **16%** AUC improvement over existingmethods in partial-lead zero-shot classification.</description>
      <author>example@mail.com (Che Liu, Cheng Ouyang, Zhongwei Wan, Haozhe Wang, Wenjia Bai, Rossella Arcucci)</author>
      <guid isPermaLink="false">2502.17900v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>VVRec: Reconstruction Attacks on DL-based Volumetric Video Upstreaming via Latent Diffusion Model with Gamma Distribution</title>
      <link>http://arxiv.org/abs/2502.17880v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着3D体积视频应用的流行，如自动驾驶、虚拟现实和混合现实，开发者开始使用深度学习来压缩用于视频上传的点云。这种基于深度学习的方法在效率、失真率和硬件支持方面都优于传统的MPEG和JPEG等方法。然而，这些新的技术带来了隐私威胁，尤其是重建攻击能够从中间结果中恢复原始输入点云。&lt;h4&gt;背景&lt;/h4&gt;3D体积视频应用变得越来越受欢迎，开发者开始使用深度学习来压缩用于上传的点云数据，这种新型的技术相比传统的方法具有更高的效率和更好的硬件支持。但是这样的技术也带来了一些隐私威胁，比如针对中间结果进行重建攻击。&lt;h4&gt;目的&lt;/h4&gt;设计VVRec，这是一个基于深度学习的体积视频重建攻击方案，能够从拦截传输中的中间结果中恢复高质量的点云。&lt;h4&gt;方法&lt;/h4&gt;VVRec使用了四个精心设计并训练好的神经网络模块，结合最新的潜在扩散模型和Gamma分布以及细化算法来完成高精度的重建任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过三个体积视频数据集对VVRec进行了评估。结果显示，VVRec实现了64.70dB的重建准确度，并且比基线方法减少了46.39%的失真率。&lt;h4&gt;结论&lt;/h4&gt;VVRec展示了其在点云重建中的优越性能，同时对于现有的防御措施提出了挑战。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the popularity of 3D volumetric video applications, such as AutonomousDriving, Virtual Reality, and Mixed Reality, current developers have turned todeep learning for compressing volumetric video frames, i.e., point clouds forvideo upstreaming. The latest deep learning-based solutions offer higherefficiency, lower distortion, and better hardware support compared totraditional ones like MPEG and JPEG. However, privacy threats arise, especiallyreconstruction attacks targeting to recover the original input point cloud fromthe intermediate results. In this paper, we design VVRec, to the best of ourknowledge, which is the first targeting DL-based Volumetric VideoReconstruction attack scheme. VVRec demonstrates the ability to reconstructhigh-quality point clouds from intercepted transmission intermediate resultsusing four well-trained neural network modules we design. Leveraging the latestlatent diffusion models with Gamma distribution and a refinement algorithm,VVRec excels in reconstruction quality, color recovery, and surpasses existingdefenses. We evaluate VVRec using three volumetric video datasets. The resultsdemonstrate that VVRec achieves 64.70dB reconstruction accuracy, with animpressive 46.39% reduction of distortion over baselines.</description>
      <author>example@mail.com (Rui Lu, Bihai Zhang, Dan Wang)</author>
      <guid isPermaLink="false">2502.17880v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>BRIDO: Bringing Democratic Order to Abstractive Summarization</title>
      <link>http://arxiv.org/abs/2502.18342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 1 figure; AAAI-25 Workshop on PDLM camera ready&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的方法来减少大型语言模型在抽象文本摘要中的hallucination（幻觉）问题。&lt;h4&gt;背景&lt;/h4&gt;虽然大型语言模型在许多任务中展现出巨大潜力，但幻觉问题仍然是其实用性的主要挑战之一。&lt;h4&gt;目的&lt;/h4&gt;通过减轻暴露偏差来解决抽象文本摘要中的幻觉问题。&lt;h4&gt;方法&lt;/h4&gt;该研究提出了一种使用对比学习的方法，该方法的目标是减少候选输出中的幻觉内容。这种方法假设包含幻觉的候选输出在一组候选输出中占少数，并且与其他候选人相比具有较低的相似性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在XSum和CNN/DM摘要数据集上，所提方法相对于现有模型BRIO分别提高了6.25%和3.82%的一致性G-Eval得分。&lt;h4&gt;结论&lt;/h4&gt;利用对比学习策略可以有效减少大型语言模型生成的文本中的幻觉问题。&lt;h4&gt;翻译&lt;/h4&gt;该论文旨在通过改进现有的针对暴露偏差的方法来减轻摘要中出现的不准确、无关或不一致的内容，进而提高大语言模型在抽象文本总结任务上的性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hallucination refers to the inaccurate, irrelevant, and inconsistent textgenerated from large language models (LLMs). While the LLMs have shown greatpromise in a variety of tasks, the issue of hallucination still remains a majorchallenge for many practical uses. In this paper, we tackle the issue ofhallucination in abstract text summarization by mitigating exposure bias.Existing models targeted for exposure bias mitigation, namely BRIO, aim forbetter summarization quality in the ROUGE score. We propose a model that uses asimilar exposure bias mitigation strategy but with a goal that is aligned withless hallucination. We conjecture that among a group of candidate outputs, oneswith hallucinations will comprise the minority of the whole group. That is,candidates with less similarity with others will have a higher chance ofcontaining hallucinated content. Our method uses this aspect and utilizescontrastive learning, incentivizing candidates with high inter-candidate ROUGEscores. We performed experiments on the XSum and CNN/DM summarization datasets,and our method showed 6.25% and 3.82% improvement, respectively, on theconsistency G-Eval score over BRIO.</description>
      <author>example@mail.com (Junhyun Lee, Harshith Goka, Hyeonmok Ko)</author>
      <guid isPermaLink="false">2502.18342v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A graph neural network-based multispectral-view learning model for diabetic macular ischemia detection from color fundus photographs</title>
      <link>http://arxiv.org/abs/2502.17886v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的多光谱视图学习模型，用于从彩色眼底照片中检测糖尿病性黄斑缺血(DMI)。&lt;h4&gt;背景&lt;/h4&gt;尽管通过人工智能(AI)结合彩色眼底照片(CFPs)在检测各种眼科疾病（包括糖尿病视网膜病变）方面得到了广泛应用，但CFPs在DMI检测中的应用尚未得到充分探索。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图神经网络的多光谱视图学习(GNN-MSVL)模型来从彩色眼底照片中检测糖尿病性黄斑缺血(DMI)。&lt;h4&gt;方法&lt;/h4&gt;该模型首先通过计算多光谱成像(CMI)重建24波长的眼底图像。使用ResNeXt101作为骨干网络进行多视图学习，提取重建图像的特征。此外，设计了一个带有定制跳跃连接策略的GNN以增强跨光谱关系。&lt;h4&gt;主要发现&lt;/h4&gt;该模型在眼水平上的准确率为84.7%，AUROC为0.900（95% CI: 0.852-0.937），优于单纯基于CFPs训练的基本模型和人类专家，p值小于0.01。&lt;h4&gt;结论&lt;/h4&gt;AI驱动的彩色眼底照片分析有望用于早期、低成本地筛查DMI。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diabetic macular ischemia (DMI), marked by the loss of retinal capillaries inthe macular area, contributes to vision impairment in patients with diabetes.Although color fundus photographs (CFPs), combined with artificial intelligence(AI), have been extensively applied in detecting various eye diseases,including diabetic retinopathy (DR), their applications in detecting DMI remainunexplored, partly due to skepticism among ophthalmologists regarding itsfeasibility. In this study, we propose a graph neural network-basedmultispectral view learning (GNN-MSVL) model designed to detect DMI from CFPs.The model leverages higher spectral resolution to capture subtle changes infundus reflectance caused by ischemic tissue, enhancing sensitivity toDMI-related features. The proposed approach begins with computationalmultispectral imaging (CMI) to reconstruct 24-wavelength multispectral fundusimages from CFPs. ResNeXt101 is employed as the backbone for multi-viewlearning to extract features from the reconstructed images. Additionally, a GNNwith a customized jumper connection strategy is designed to enhancecross-spectral relationships, facilitating comprehensive and efficientmultispectral view learning. The study included a total of 1,078macula-centered CFPs from 1,078 eyes of 592 patients with diabetes, of which530 CFPs from 530 eyes of 300 patients were diagnosed with DMI. The modelachieved an accuracy of 84.7 percent and an area under the receiver operatingcharacteristic curve (AUROC) of 0.900 (95 percent CI: 0.852-0.937) oneye-level, outperforming both the baseline model trained from CFPs and humanexperts (p-values less than 0.01). These findings suggest that AI-based CFPanalysis holds promise for detecting DMI, contributing to its early andlow-cost screening.</description>
      <author>example@mail.com (Qinghua He, Hongyang Jiang, Danqi Fang, Dawei Yang, Truong X. Nguyen, Anran Ran, Clement C. Tham, Simon K. H. Szeto, Sobha Sivaprasad, Carol Y. Cheung)</author>
      <guid isPermaLink="false">2502.17886v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>On-device edge learning for IoT data streams: a survey</title>
      <link>http://arxiv.org/abs/2502.17788v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;这篇文献综述探讨了在神经网络和决策树分类任务的智能环境中进行连续学习方法的研究。文章重点关注数据架构（批量 vs 流式）和网络容量（云端 vs 边缘设备）对TinyML算法设计的影响，这是因为自然到达的数据流是不受控制的。&lt;h4&gt;背景&lt;/h4&gt;部署深度学习模型在资源受限的边缘设备上面临挑战，包括灾难性遗忘、数据低效性和处理IoT表格数据于开放世界环境中的困难。决策树虽然更节省内存，但其表达能力有限，需要动态适应（如剪枝和元学习）来处理复杂模式和概念漂移。&lt;h4&gt;目的&lt;/h4&gt;强调为边缘应用定制多指标性能评估的重要性，这些评估不仅考虑输出基础的度量还考虑内部表示的度量。主要挑战在于将这些构建块整合到自适应在线系统中时需兼顾稳定性与可塑性、正向反向迁移以及模型收敛。&lt;h4&gt;方法&lt;/h4&gt;综述详细介绍了在资源受限边缘设备上部署深度学习所面临的各种挑战，并总结了决策树和神经网络各自的特点，指出需要结合多种技术来应对持续变化的环境。&lt;h4&gt;主要发现&lt;/h4&gt;连续学习对于边缘设备上的TinyML算法设计至关重要。尽管决策树具有内存效率优势但其灵活性不如神经网络。因此，在处理复杂模式时，可能需要额外的技术手段如动态适应和元学习机制。&lt;h4&gt;结论&lt;/h4&gt;为了有效利用资源受限的边缘设备进行深度学习任务，需开发新的连续学习方法来优化模型性能、减少灾难性遗忘，并提高对概念漂移的响应能力。&lt;h4&gt;翻译&lt;/h4&gt;此文献综述探索了在神经网络（NN）和决策树（DT）分类任务中的智能环境中为设备上训练而采用的连续学习方法。重点强调数据架构（批量 vs 流式）与网络容量（云端 vs 边缘）对TinyML算法设计的影响，这是由于自然到达的数据流是不受控制的。综述详细描述了在资源受限边缘设备上部署深度学习所面临的挑战，包括灾难性遗忘、数据低效性和处理IoT表格数据于开放世界环境中的困难。虽然决策树对于设备上的训练更节省内存，但其表达能力有限，需要动态适应（如剪枝和元学习）来处理复杂模式和概念漂移。强调了为边缘应用定制多指标性能评估的重要性，这些评估不仅考虑输出基础的度量还考虑内部表示的度量。关键挑战在于将这些构建块整合到自适应在线系统中时需兼顾稳定性与可塑性、正向反向迁移以及模型收敛。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This literature review explores continual learning methods for on-devicetraining in the context of neural networks (NNs) and decision trees (DTs) forclassification tasks on smart environments. We highlight key constraints, suchas data architecture (batch vs. stream) and network capacity (cloud vs. edge),which impact TinyML algorithm design, due to the uncontrolled natural arrivalof data streams. The survey details the challenges of deploying deep learnerson resource-constrained edge devices, including catastrophic forgetting, datainefficiency, and the difficulty of handling IoT tabular data in open-worldsettings. While decision trees are more memory-efficient for on-devicetraining, they are limited in expressiveness, requiring dynamic adaptations,like pruning and meta-learning, to handle complex patterns and concept drifts.We emphasize the importance of multi-criteria performance evaluation tailoredto edge applications, which assess both output-based and internalrepresentation metrics. The key challenge lies in integrating these buildingblocks into autonomous online systems, taking into account stability-plasticitytrade-offs, forward-backward transfer, and model convergence.</description>
      <author>example@mail.com (Afonso Lourenço, João Rodrigo, João Gama, Goreti Marreiros)</author>
      <guid isPermaLink="false">2502.17788v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Task-Agnostic Semantic Communication with Multimodal Foundation Models</title>
      <link>http://arxiv.org/abs/2502.18200v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个无任务依赖的语义通信框架SemCLIP，基于对比语言-图像预训练模型CLIP。通过传输CLIP生成的图像令牌而非原始图像，在低带宽和挑战性信道条件下实现高效的语义通信。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数语义通信系统使用深度联合源信道编码来以目标导向的方式对特定任务进行语义编码，但这限制了它们在实际部署中的灵活性和泛化能力。多模态基础模型通过生成通用的语义令牌提供了潜在解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的无任务依赖的语义通信框架SemCLIP，利用对比语言-图像预训练模型（CLIP）来实现高效、灵活且鲁棒的语义通信系统。&lt;h4&gt;方法&lt;/h4&gt;1. 使用CLIP生成的图像令牌代替原始图像进行传输。2. 设计了一种深度联合源信道编码方案以高效地对CLIP令牌进行编码。3. 在接收端设计了一个多模态传输感知提示学习机制，该机制根据传输质量调整提示，增强了系统的鲁棒性和信道适应性。&lt;h4&gt;主要发现&lt;/h4&gt;1. SemCLIP在低信号噪声比下实现了零样本准确性提高41%的性能优于基线模型。2. 与不同的图像传输方法相比，SemCLIP减少了超过50倍的带宽使用量。&lt;h4&gt;结论&lt;/h4&gt;通过利用基础模型和无任务依赖的方法，展示了语义通信系统在实际部署中的潜力，并为未来的通用、无任务依赖的语义通信解决方案开辟了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most existing semantic communication (SemCom) systems use deep jointsource-channel coding (DeepJSCC) to encode task-specific semantics in agoal-oriented manner. However, their reliance on predefined tasks and datasetssignificantly limits their flexibility and generalizability in practicaldeployments. Multi-modal foundation models provide a promising solution bygenerating universal semantic tokens. Inspired by this, we introduce SemCLIP, atask-agnostic SemCom framework leveraging the contrastive language-imagepre-training (CLIP) model. By transmitting CLIP-generated image tokens insteadof raw images, SemCLIP enables efficient semantic communications under lowbandwidth and challenging channel conditions, facilitating diverse downstreamtasks and zero-shot applications. Specifically, we propose a DeepJSCC schemefor efficient CLIP tokens encoding. To mitigate potential degradation caused bycompression and channel noise, a multi-modal transmission-aware prompt learningmechanism is designed at the receiver, which adapts prompts based ontransmission quality, enhancing system robustness and channel adaptability.Simulation results demonstrate that SemCLIP outperforms the baselines,achieving a $41\%$ improvement in zero-shot accuracy at a low signal-to-noiseratio. Meanwhile, SemCLIP reduces bandwidth usage by more than $50$-foldcompared to different image transmission methods, demonstrating the potentialof foundation models towards a generalized, task-agnostic SemCom solution.</description>
      <author>example@mail.com (Jiangjing Hu, Haotian Wu, Wenjing Zhang, Fengyu Wang, Wenjun Xu, Hui Gao, Deniz Gündüz)</author>
      <guid isPermaLink="false">2502.18200v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>UniGS: Unified Language-Image-3D Pretraining with Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.17860v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的多模态预训练模型UniGS，通过将3D高斯点集（3D Gaussian Splatting）引入到文本、图像和三维空间的联合表示学习中，以改进当前基于离散点云的方法。&lt;h4&gt;背景&lt;/h4&gt;现有技术在多模态3D预训练方法取得了显著效果，但用离散点来表达复杂的3D世界存在局限性，无法完全捕捉到2D像素与3D结构之间的联系。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了3D高斯点集的多模态预训练框架UniGS，以增强从文本、图像和三维空间中提取联合表示的能力。&lt;h4&gt;方法&lt;/h4&gt;首先通过3D高斯点集来建模彩色和透明度信息；接着利用预训练的视觉语言模型建立共享的视觉和文本空间，并引入一个新型模块（Gaussian-Aware Guidance）帮助学习更细粒度的3D表示，促进跨模式对齐。&lt;h4&gt;主要发现&lt;/h4&gt;在不同数据集上进行广泛的实验表明，UniGS能够更好地捕捉到多模态信息之间的关系，尤其在零样本分类、基于文本驱动检索以及开放世界理解任务中表现优异，超越了之前的SOTA模型（如Uni3D）。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的建模方式和学习机制，UniGS提供了一种更为通用且更强的跨模态表示方法。&lt;h4&gt;翻译&lt;/h4&gt;最近在多模态3D预训练方法上的进展已经在文本、图像及点云联合表示的学习方面展现了有前景的效果。然而，仅采用点云作为三维表现形式未能充分捕捉到复杂三维世界的细微差别，并且在离散点与密集2D像素间存在明显的差距。为解决这一问题，我们提出了一种名为UniGS的方法，将3D高斯撒播（3D Gaussian Splatting）整合进多模态预训练中以提升三维表现形式。首先，我们使用3D GS表示来模拟三维世界作为一系列带有颜色和透明度的3D高斯分布体，在保留所有3D场景信息的同时建立起与2D图像之间的紧密联系。然后为了实现跨语言、图象及3D模态学习，UniGS从预训练的视觉文本模型开始，通过大量的现实世界图像-文本对建立共享的视觉和文字空间。接下来，UniGS采用一个三维编码器将优化后的3D GS与语言-图像表示进行一致化以获取统一多模态表示。为了帮助提取全球性的明确3D特征并通过更好的跨模式对齐实现这一点，我们进一步引入了新的Gaussian-Aware Guidance模块来引导学习细粒度的三维领域的表示。通过在Objaverse、ABO、MVImgNet和SUN RGBD数据集上进行广泛的零样本分类、基于文本驱动检索和开放世界理解任务实验，我们证明了UniGS在学习更通用和更好对齐的多模态表现形式方面的有效性。具体来说，在各种3D任务中，包括零样本分类（+9.36%）、基于文本驱动检索（+4.3%）和开放世界理解（+7.92%），与之前的SOTA模型Uni3D相比，UniGS取得了领先的成果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in multi-modal 3D pre-training methods have shownpromising efficacy in learning joint representations of text, images, and pointclouds. However, adopting point clouds as 3D representation fails to fullycapture the intricacies of the 3D world and exhibits a noticeable gap betweenthe discrete points and the dense 2D pixels of images. To tackle this issue, wepropose UniGS, integrating 3D Gaussian Splatting (3DGS) into multi-modalpre-training to enhance the 3D representation. We first rely on the 3DGSrepresentation to model the 3D world as a collection of 3D Gaussians with colorand opacity, incorporating all the information of the 3D scene whileestablishing a strong connection with 2D images. Then, to achieveLanguage-Image-3D pertaining, UniGS starts with a pre-trained vision-languagemodel to establish a shared visual and textual space through extensivereal-world image-text pairs. Subsequently, UniGS employs a 3D encoder to alignthe optimized 3DGS with the Language-Image representations to learn unifiedmulti-modal representations. To facilitate the extraction of global explicit 3Dfeatures by the 3D encoder and achieve better cross-modal alignment, weadditionally introduce a novel Gaussian-Aware Guidance module that guides thelearning of fine-grained representations of the 3D domain. Through extensiveexperiments across the Objaverse, ABO, MVImgNet and SUN RGBD datasets withzero-shot classification, text-driven retrieval and open-world understandingtasks, we demonstrate the effectiveness of UniGS in learning a more general andstronger aligned multi-modal representation. Specifically, UniGS achievesleading results across different 3D tasks with remarkable improvements overprevious SOTA, Uni3D, including on zero-shot classification (+9.36%),text-driven retrieval (+4.3%) and open-world understanding (+7.92%).</description>
      <author>example@mail.com (Haoyuan Li, Yanpeng Zhou, Tao Tang, Jifei Song, Yihan Zeng, Michael Kampffmeyer, Hang Xu, Xiaodan Liang)</author>
      <guid isPermaLink="false">2502.17860v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Progressive Local Alignment for Medical Multimodal Pre-training</title>
      <link>http://arxiv.org/abs/2502.18047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为Progressive Local Alignment Network (PLAN)的新型网络，用于提高医学图像与文本之间的局部对齐精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;在医疗影像诊断中，准确地将文本描述与对应的像素区域进行配对是一个挑战，因为传统的硬边界方法存在不确定性，并且难以处理不规则结构。&lt;h4&gt;目的&lt;/h4&gt;通过设计一种基于对比学习的新方法来建立有意义的词-像素关系，并采用渐进式学习策略迭代优化这些关系，以提高软区域识别的效果并减少噪声干扰。&lt;h4&gt;方法&lt;/h4&gt;PLAN网络利用了对比学习和渐进式学习技术来改进局部对齐，提升医学图像与文本之间的关联性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PLAN在短语定位、图文检索、目标检测及零样本分类等多个任务上超越了现有的最佳方法，确立了一个新的基准。&lt;h4&gt;结论&lt;/h4&gt;PLAN网络为医疗图像和文本配准提供了一种有效的方法，提高了软区域识别的准确性和鲁棒性，并且能够抑制噪声干扰。&lt;h4&gt;翻译&lt;/h4&gt;本地医学影像与文字之间的对齐对于精确诊断至关重要，尽管由于缺乏自然局部配对和刚性区域识别方法的局限性而仍然具有挑战性。传统方法依赖于硬边界，引入了不确定性，然而医疗成像需要灵活处理不规则结构的软区域识别方法。为了克服这些挑战，我们提出了渐进式本地对齐网络（PLAN），它设计了一种基于对比学习的新局部对齐方法来建立有意义的文字-像素关系，并采用逐步学习策略迭代优化这些关系，以提高配准精度和鲁棒性。通过结合这些技术，PLAN有效地提高了软区域识别的效果同时抑制了噪声干扰。在多个医疗数据集上的大量实验表明，PLAN在短语定位、图像文字检索、目标检测和零样本分类任务上超越了最先进的方法，为医学影像-文本对齐设定了新的基准。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Local alignment between medical images and text is essential for accuratediagnosis, though it remains challenging due to the absence of natural localpairings and the limitations of rigid region recognition methods. Traditionalapproaches rely on hard boundaries, which introduce uncertainty, whereasmedical imaging demands flexible soft region recognition to handle irregularstructures. To overcome these challenges, we propose the Progressive LocalAlignment Network (PLAN), which designs a novel contrastive learning-basedapproach for local alignment to establish meaningful word-pixel relationshipsand introduces a progressive learning strategy to iteratively refine theserelationships, enhancing alignment precision and robustness. By combining thesetechniques, PLAN effectively improves soft region recognition while suppressingnoise interference. Extensive experiments on multiple medical datasetsdemonstrate that PLAN surpasses state-of-the-art methods in phrase grounding,image-text retrieval, object detection, and zero-shot classification, setting anew benchmark for medical image-text alignment.</description>
      <author>example@mail.com (Huimin Yan, Xian Yang, Liang Bai, Jiye Liang)</author>
      <guid isPermaLink="false">2502.18047v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Neural Graph Matching Improves Retrieval Augmented Generation in Molecular Machine Learning</title>
      <link>http://arxiv.org/abs/2502.17874v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;分子机器学习由于几何深度学习的进步而变得流行。同时，检索增强生成已经成为语言模型中常用的一种基本原则方法。然而，如何将检索增强有效地整合到分子机器学习中仍然不清楚。&lt;h4&gt;背景&lt;/h4&gt;随着几何深度学习的发展，分子机器学习变得越来越受欢迎。与此同时，检索增强生成作为一种基本的方法被广泛应用于语言模型中。&lt;h4&gt;目的&lt;/h4&gt;探讨和实现一种有效的方式，即通过神经图匹配来改进分子机器学习中的查询-检索问题。&lt;h4&gt;方法&lt;/h4&gt;引入MARASON模型，该模型结合了神经图匹配技术以提升基于碎片化的神经网络性能。具体来说，它采用了噪声鲁棒的、端到端的神经网络来学习结构相似性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，MARASON模型在质量指标上取得了显著的进步，比最佳非检索方法高出28%（Top-1准确率从19%提升到了47%），并且超过了简单检索增强生成和传统图匹配方法的性能。&lt;h4&gt;结论&lt;/h4&gt;神经图匹配技术能够有效地帮助分子机器学习中更好地理解和应用结构对齐。MARASON模型在质量指标上展示了显著的优势，证明了其有效性。&lt;h4&gt;翻译&lt;/h4&gt;分子机器学习随着几何深度学习的进步而日益受到关注。同时，检索增强生成已经成为语言模型中的标准方法之一。然而，在分子机器学习中如何最优地集成检索增强技术仍不清楚。图神经网络可以从巧妙的匹配中受益，以理解检索到的分子与查询分子之间的结构对齐情况。通过显式建模两个结构图形间的节点和边亲和度，并利用噪声鲁棒、端到端的神经网络来学习相似性指标，神经图匹配提供了一种有竞争力的解决方案。我们将这种方法应用于质谱模拟并提出MARASON模型——一种引入神经图匹配技术以增强基于碎片化的神经网络的新方法。实验结果表明我们的设计是有效的：MARASON在Top-1准确率上达到了47%，比现有的最佳非检索基准提高了28%（后者为19%）。此外，MARASON优于简单的检索增强生成方法和传统的图匹配方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Molecular machine learning has gained popularity with the advancements ofgeometric deep learning. In parallel, retrieval-augmented generation has becomea principled approach commonly used with language models. However, the optimalintegration of retrieval augmentation into molecular machine learning remainsunclear. Graph neural networks stand to benefit from clever matching tounderstand the structural alignment of retrieved molecules to a query molecule.Neural graph matching offers a compelling solution by explicitly modeling nodeand edge affinities between two structural graphs while employing anoise-robust, end-to-end neural network to learn affinity metrics. We applythis approach to mass spectrum simulation and introduce MARASON, a novel modelthat incorporates neural graph matching to enhance a fragmentation-based neuralnetwork. Experimental results highlight the effectiveness of our design, withMARASON achieving 28% top-1 accuracy, a substantial improvement over thenon-retrieval state-of-the-art accuracy of 19%. Moreover, MARASON outperformsboth naive retrieval-augmented generation methods and traditional graphmatching approaches.</description>
      <author>example@mail.com (Runzhong Wang, Rui-Xi Wang, Mrunali Manjrekar, Connor W. Coley)</author>
      <guid isPermaLink="false">2502.17874v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Agnostic calculation of atomic free energies with the descriptor density of states</title>
      <link>http://arxiv.org/abs/2502.18191v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的评估原子系统振动自由能的方法，无需预先指定相互作用势函数。该方法通过描述符密度的熵来估计高维分布，并利用条件分数匹配进行计算。&lt;h4&gt;背景&lt;/h4&gt;传统的振动能量计算依赖于特定的相互作用势模型，限制了应用范围和灵活性。&lt;h4&gt;目的&lt;/h4&gt;开发一种通用评估框架，能够跨各种原子系统精确预测自由能而无需具体了解相互作用势函数。&lt;h4&gt;方法&lt;/h4&gt;通过描述符密度的状态，引入高维特征向量进行振动自由能估计；利用条件分数匹配技术避免复杂的高维积分计算。&lt;h4&gt;主要发现&lt;/h4&gt;该方法能够在微秒级别的CPU时间内返回可导的自由能预测值，并支持潜在参数变化的快速前向和反向传播。测试表明，所提出的模型独立估算器能够通过热力学集成法在多种金属相（如BCC, FCC 和 A15）中达到1-2 meV/原子精度。&lt;h4&gt;结论&lt;/h4&gt;该方法为不确定性量化、逆设计及其他计算科学领域的高维积分问题提供了有效的解决方案，并展示了其强大的应用潜力，特别是在液体系统和基础模型的微调方面。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新方法来评估无预先指定相互作用势函数条件下的原子体系振动自由能。我们的模型无关方式利用描述符——原子结构的高维特征向量。通过条件分数匹配准确估算高维密度熵，将相互作用势函数转化为在描述符特性上扩展的形式。我们展示了自由能作为描述符熵的Legendre-Fenchel共轭形式出现，从而避免了所有高维积分计算。该分数匹配活动比固定模型采样需要更少资源且高度并行化，使得耗时减少到几分钟，并通过张量压缩方案实现轻量化存储。我们的模型无关估计器能够以微秒级的CPU努力返回广泛的潜在参数范围内的可导自由能预测值，支持热力学模拟中快速传播势变差异及误差，这是进行不确定性量化和逆设计长期追求的目标。我们测试了在高同质温度下W、Mo和Fe的BCC、FCC和A15相模型中的预测结果与热力学集成计算的一致性，并通过反向传播微调非磁机器学习模型中Fe的alpha-gamma过渡温度，无需额外采样。讨论了该方法在液体体系及基础模型微调等领域的应用以及其他估计高维积分问题的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a new method to evaluate vibrational free energies of atomicsystems without a priori specification of an interatomic potential. Ourmodel-agnostic approach leverages descriptors, high-dimensional feature vectorsof atomic structure. The entropy of a high-dimensional density, the descriptordensity of states, is accurately estimated with conditional score matching.Casting interatomic potentials into a form extensive in descriptor features, weshow free energies emerge as the Legendre-Fenchel conjugate of the descriptorentropy, avoiding all high-dimensional integration. The score matching campaignrequires less resources than fixed-model sampling and is highly parallel,reducing wall time to a few minutes, with tensor compression schemes allowinglightweight storage. Our model-agnostic estimator returns differentiable freeenergy predictions over a broad range of potential parameters in microsecondsof CPU effort, allowing rapid forward and back propagation of potentialvariations through finite temperature simulations, long desired for uncertaintyquantification and inverse design. We test predictions against thermodynamicintegration calculations over a broad range of models for BCC, FCC and A15phases of W, Mo and Fe at high homologous temperatures. Predictions pass thestringent accuracy threshold of 1-2 meV/atom (1/40-1/20 kcal/mol) for phaseprediction with propagated score uncertainties robustly bounding errors. Wealso demonstrate targeted fine-tuning, reducing the alpha-gamma transitiontemperature in a non-magnetic machine learning model of Fe from 2030 K to 1063K through back-propagation, with no additional sampling. Applications toliquids and fine-tuning foundational models are discussed along with the manyproblems in computational science which estimate high-dimensional integrals.</description>
      <author>example@mail.com (Thomas D Swinburne, Clovis Lapointe, Mihai-Cosmin Marinica)</author>
      <guid isPermaLink="false">2502.18191v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CPVis: Evidence-based Multimodal Learning Analytics for Evaluation in Collaborative Programming</title>
      <link>http://arxiv.org/abs/2502.17835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种名为CPVis的互动式可视化分析系统，旨在帮助编程教育中的教师评估学生的合作学习情况。&lt;h4&gt;背景&lt;/h4&gt;随着编程教育的发展，越来越多非计算机专业的学生开始接触编程。在这种背景下，协作编程成为了一种有效的教学方法，但也给教师带来了管理上的挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够动态评估学生合作能力和个人表现的系统，以解决现有条件下教师难以全面监控和评价学生的问题。&lt;h4&gt;方法&lt;/h4&gt;收集实际场景中的多模态数据，并利用这些数据创建了一个基于花型编码的独特视觉表示形式。CPVis提供了时间序列视图来展示协作行为的发展趋势。&lt;h4&gt;主要发现&lt;/h4&gt;通过与两个基准系统的对比实验（N=22），参与者在使用CPVis时报告获得了更多的洞察力，感觉可视化更加直观，并且对其合作评估的自信度有所提高。&lt;h4&gt;结论&lt;/h4&gt;CPVis提供了一种有效的方法来帮助教师动态地评估学生的协作编程表现。这种系统有助于提高教育质量和学生的学习体验。&lt;h4&gt;翻译&lt;/h4&gt;随着编程教育变得越来越普及，越来越多非计算机专业的大学生开始学习编程。然而，在有限的教学时间和注意力范围内，教师难以监控和评估团队或个人的进步与成绩。为了解决这个问题，研究人员收集了现实世界中的多模态数据，并开发了一种名为CPVis的互动式可视化分析系统来动态地评价学生合作。该系统允许教师高效地评估小组和个人的表现，并采用新颖的花卉编码视觉表现形式展示成绩和提供基于时间视图捕捉协作行为的发展趋势。实验结果表明，用户在使用CPVis时获得了更多的洞见、发现可视化更为直观且增加了他们对评估信心。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As programming education becomes more widespread, many college students fromnon-computer science backgrounds begin learning programming. Collaborativeprogramming emerges as an effective method for instructors to support novicestudents in developing coding and teamwork abilities. However, due to limitedclass time and attention, instructors face challenges in monitoring andevaluating the progress and performance of groups or individuals. To addressthis issue, we collect multimodal data from real-world settings and developCPVis, an interactive visual analytics system designed to assess studentcollaboration dynamically. Specifically, CPVis enables instructors to evaluateboth group and individual performance efficiently. CPVis employs a novelflower-based visual encoding to represent performance and provides time-basedviews to capture the evolution of collaborative behaviors. A within-subjectexperiment (N=22), comparing CPVis with two baseline systems, reveals thatusers gain more insights, find the visualization more intuitive, and reportincreased confidence in their assessments of collaboration.</description>
      <author>example@mail.com (Gefei Zhang, Shenming Ji, Yicao Li, Jingwei Tang, Jihong Ding, Meng Xia, Guodao Sun, Ronghua Liang)</author>
      <guid isPermaLink="false">2502.17835v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning with Nasty Noise</title>
      <link>http://arxiv.org/abs/2502.17872v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文分析了在存在恶意干扰的情况下对比学习的理论极限，并通过PAC学习和VC维数分析建立了对抗设置下的样本复杂度上下界。&lt;h4&gt;背景&lt;/h4&gt;对比学习作为一种自监督表征学习的强大范式，其性能受到训练数据中可能存在恶意噪声的影响。这种情况下，训练样本可能会被修改或替换。&lt;h4&gt;目的&lt;/h4&gt;探讨并确定在存在恶意干扰的情况下，对比学习的理论极限以及样本复杂度的上界和下界。&lt;h4&gt;方法&lt;/h4&gt;利用PAC（概率可近似）学习框架和VC维数分析来建立对抗设置下的对比学习模型的样本复杂度上下限。此外还根据l2距离函数导出了基于数据相关的样本复杂度界限。&lt;h4&gt;主要发现&lt;/h4&gt;在存在恶意噪声的情况下，可以通过使用PAC理论和VC维数分析来限定对比学习算法的有效性及所需的最小训练样本数量。同时证明了特定条件下，利用l2距离作为损失函数可以优化样本选择过程。&lt;h4&gt;结论&lt;/h4&gt;通过理论分析揭示了对抗环境中对比学习面临的挑战，并提供了改进模型性能的新视角和方法论建议。&lt;h4&gt;翻译&lt;/h4&gt;对比学习已经成为自监督表示学习的强大范式。这项工作在存在恶意噪声的背景下探讨了对比学习的理论极限，其中对手可能会修改或替换训练样本。利用PAC学习以及VC维数分析，在对抗环境下建立了样本复杂度的上下界，并且基于l2距离函数推导出数据依赖性的样本复杂度界限。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Contrastive learning has emerged as a powerful paradigm for self-supervisedrepresentation learning. This work analyzes the theoretical limits ofcontrastive learning under nasty noise, where an adversary modifies or replacestraining samples. Using PAC learning and VC-dimension analysis, lower and upperbounds on sample complexity in adversarial settings are established.Additionally, data-dependent sample complexity bounds based on the l2-distancefunction are derived.</description>
      <author>example@mail.com (Ziruo Zhao)</author>
      <guid isPermaLink="false">2502.17872v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Label-free Prediction of Vascular Connectivity in Perfused Microvascular Networks in vitro</title>
      <link>http://arxiv.org/abs/2502.17759v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的无标记血管连接网络（VC-Net），该网络可以连续监测和评估微血管的连通性，无需荧光标签即可进行测量。&lt;h4&gt;背景&lt;/h4&gt;现有的微血管连接评估方法大多依赖于可能引起生物相容性问题或干扰细胞正常生长过程的荧光标签。&lt;h4&gt;目的&lt;/h4&gt;开发一种无标记的方法来持续且非侵入地监测和评估组织培养中的微血管连通性，以改善器官体外培养及优化治疗策略。&lt;h4&gt;方法&lt;/h4&gt;使用Vessel Queue Contrastive Learning (VQCL) 方法结合解决样本量有限、类别特征不明显以及类别分布不平衡问题的算法构建VC-Net。通过采集不同培养条件下生成的微血管网络(MVN) 的显微图像作为训练数据集来验证该模型。&lt;h4&gt;主要发现&lt;/h4&gt;1. VC-Net 能够成功评估微血管连通性，其结果与荧光成像方法的结果无显著差异。2. VC-Net 成功区分了正常和肿瘤相关的MVN之间的连接特性。在与常规培养环境中的相比，肿瘤相关环境中培养的平均血管连接降低了30.8%，而未连接区域增加了37.3%。&lt;h4&gt;结论&lt;/h4&gt;这项研究为无标记、连续评估体外培养器官或肿瘤的血管化提供了一种新的途径。&lt;h4&gt;翻译&lt;/h4&gt;持续监测和原位评估微血管连通性在培养血管化的类器官以及优化治疗策略方面具有重要意义。然而，常用的血管连接评估方法严重依赖于荧光标签，这些荧光标签可能会引发生物相容性问题或干扰正常的细胞生长过程。为了解决这个问题，开发了一种无标记的血管连接网络（VC-Net），用于评估微血管连通性。通过在不同培养条件下采集体外培养的微血管网络（MVN）的显微图像作为训练数据集来验证该模型的有效性。VC-Net 使用了Vessel Queue Contrastive Learning (VQCL) 方法和处理样本量有限、类别特征不明显以及类别分布不平衡问题的算法。VC-Net 成功评估了血管连通性，并且结果与荧光成像方法的结果没有显著差异。此外，该研究还展示了VC-Net 能够区分正常和肿瘤相关的MVN之间的连接特性。相比常规培养环境中的情况，在肿瘤相关环境中培养的平均血管连接降低了30.8%，而未连接区域增加了37.3%。这项工作为体外无标记且连续地评估类器官或肿瘤的血管化提供了一种新的途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuous monitoring and in-situ assessment of microvascular connectivityhave significant implications for culturing vascularized organoids andoptimizing the therapeutic strategies. However, commonly used methods forvascular connectivity assessment heavily rely on fluorescent labels that mayeither raise biocompatibility concerns or interrupt the normal cell growthprocess. To address this issue, a Vessel Connectivity Network (VC-Net) wasdeveloped for label-free assessment of vascular connectivity. To validate theVC-Net, microvascular networks (MVNs) were cultured in vitro and theirmicroscopic images were acquired at different culturing conditions as atraining dataset. The VC-Net employs a Vessel Queue Contrastive Learning (VQCL)method and a class imbalance algorithm to address the issues of limited samplesize, indistinctive class features and imbalanced class distribution in thedataset. The VC-Net successfully evaluated the vascular connectivity with nosignificant deviation from that by fluorescence imaging. In addition, theproposed VC-Net successfully differentiated the connectivity characteristicsbetween normal and tumor-related MVNs. In comparison with those cultured in theregular microenvironment, the averaged connectivity of MVNs cultured in thetumor-related microenvironment decreased by 30.8%, whereas the non-connectedarea increased by 37.3%. This study provides a new avenue for label-free andcontinuous assessment of organoid or tumor vascularization in vitro.</description>
      <author>example@mail.com (Liang Xu, Pengwu Song, Shilu Zhu, Yang Zhang, Ru Zhang, Zhiyuan Zheng, Qingdong Zhang, Jie Gao, Chen Han, Mingzhai Sun, Peng Yao, Min Ye, Ronald X. Xu)</author>
      <guid isPermaLink="false">2502.17759v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CLEP-GAN: An Innovative Approach to Subject-Independent ECG Reconstruction from PPG Signals</title>
      <link>http://arxiv.org/abs/2502.17536v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究解决从PPG信号重建未见过的ECG信号的挑战，提出了一个使用ODE模型生成合成ECG-PPG数据的方法，并开发了一种基于对比学习、对抗学习和注意门控的新建模方法。&lt;h4&gt;背景&lt;/h4&gt;非侵入性心脏监测需要通过PPG信号重建ECG信号。现有的公共ECG-PPG数据集缺乏多样性，且收集过程中的噪音使得ECG的重建变得复杂。&lt;h4&gt;目的&lt;/h4&gt;提出一种增强训练多样性的合成ECG-PPG数据生成技术，并开发出一个独立于受试者的PPG到ECG的重建模型。&lt;h4&gt;方法&lt;/h4&gt;使用ODE模型来生成合成的ECG-PPG数据，为机器学习模型提供更多的训练样本。开发了一种结合对比学习、对抗学习和注意力门控的新建模方法。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的模型在未见过的数据上的重构效果与现有方法相当甚至更好，并强调了在模型训练和数据集扩充时考虑人口统计数据多样性的必要性。&lt;h4&gt;结论&lt;/h4&gt;通过引入新颖的数据生成技术和改进的重建模型，可以提高ECG信号从PPG信号的准确性。研究结果表明，应重视性别、年龄等人口统计学因素对重构精度的影响。&lt;h4&gt;翻译&lt;/h4&gt;这项研究解决了利用PPG信号重建未见过的ECG信号的问题，并提出了一种新的合成ECG-PPG数据生成技术以及一种新颖的独立于受试者的PPG到ECG的重建模型，该模型结合了对比学习、对抗学习和注意力门控。研究表明这些方法能够改善重构效果并强调考虑人口统计数据的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study addresses the challenge of reconstructing unseen ECG signals fromPPG signals, a critical task for non-invasive cardiac monitoring. Whilenumerous public ECG-PPG datasets are available, they lack the diversity seen inimage datasets, and data collection processes often introduce noise,complicating ECG reconstruction from PPG even with advanced machine learningmodels. To tackle these challenges, we first introduce a novel syntheticECG-PPG data generation technique using an ODE model to enhance trainingdiversity. Next, we develop a novel subject-independent PPG-to-ECGreconstruction model that integrates contrastive learning, adversariallearning, and attention gating, achieving results comparable to or evensurpassing existing approaches for unseen ECG reconstruction. Finally, weexamine factors such as sex and age that impact reconstruction accuracy,emphasizing the importance of considering demographic diversity during modeltraining and dataset augmentation.</description>
      <author>example@mail.com (Xiaoyan Li, Shixin Xu, Faisal Habib, Neda Aminnejad, Arvind Gupta, Huaxiong Huang)</author>
      <guid isPermaLink="false">2502.17536v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Learning Density Evolution from Snapshot Data</title>
      <link>http://arxiv.org/abs/2502.17738v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种基于标量回归的分布估计方法，用于从噪声时间点云数据中估计随机过程的概率密度演化。&lt;h4&gt;背景动机&lt;/h4&gt;受学习静态快照数据中的动态结构启发，该研究旨在通过利用标量上的分布来解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于熵正则化的非参数极大似然估计器（E-NPMLE）以从带有噪声的时间点云中估计随机过程的概率密度演化。&lt;h4&gt;方法&lt;/h4&gt;提出了一个结合了最优传输理论中的熵正则化，用于平滑概率密度流。设计了一种无网格的粒子基座 KL散度梯度下降算法来有效计算 E-NPMLE。&lt;h4&gt;主要发现&lt;/h4&gt;E-NPMLE 方法在统计上具有几乎不依赖于维度的收敛率，并且展示出显著的数量级转换现象，这取决于快照数量和每快照样本大小。该方法的有效性和理论结果通过合成数据进行了验证。&lt;h4&gt;结论&lt;/h4&gt;研究为从带有噪声的任意维数观测中估计概率密度演化提供了理论上和技术上的贡献。&lt;h4&gt;翻译&lt;/h4&gt;受学习静态快照数据中的动态结构启发，本文提出了一种基于标量回归的方法来从其含噪时间点云中估计随机过程的概率密度演变。我们提出了一个熵正则化非参数极大似然估计器（E-NPMLE），利用最优传输的熵作为概率流的平滑正则化项。展示了 E-NPMLE 对于真实分布具有几乎不依赖于维度的统计收敛率，并且在快照数量和每快照样本大小上表现出显著的数量级转换现象。为了高效计算 E-NPMLE，设计了一种新的无网格粒子基座 KL 散度梯度下降算法并证明了其多项式迭代复杂性。此外，在合成数据上提供了数值证据来支持理论发现。这项工作为从带有噪声的观测中估计任意维度中的概率密度演化提供了理论上和技术上的贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motivated by learning dynamical structures from static snapshot data, thispaper presents a distribution-on-scalar regression approach for estimating thedensity evolution of a stochastic process from its noisy temporal point clouds.We propose an entropy-regularized nonparametric maximum likelihood estimator(E-NPMLE), which leverages the entropic optimal transport as a smoothingregularizer for the density flow. We show that the E-NPMLE has almostdimension-free statistical rates of convergence to the ground truthdistributions, which exhibit a striking phase transition phenomenon in terms ofthe number of snapshots and per-snapshot sample size. To efficiently computethe E-NPMLE, we design a novel particle-based and grid-free coordinate KLdivergence gradient descent (CKLGD) algorithm and prove its polynomialiteration complexity. Moreover, we provide numerical evidence on synthetic datato support our theoretical findings. This work contributes to the theoreticalunderstanding and practical computation of estimating density evolution fromnoisy observations in arbitrary dimensions.</description>
      <author>example@mail.com (Rentian Yao, Atsushi Nitanda, Xiaohui Chen, Yun Yang)</author>
      <guid isPermaLink="false">2502.17738v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>PromptMID: Modal Invariant Descriptors Based on Diffusion and Vision Foundation Models for Optical-SAR Image Matching</title>
      <link>http://arxiv.org/abs/2502.18104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了PromptMID，一种基于土地利用分类先验信息构建模态不变描述符的方法，用于光学和SAR图像匹配。&lt;h4&gt;背景&lt;/h4&gt;现有的学习型光-SAR图像匹配方法在特定场景中有效但泛化能力有限，难以适应实际应用需求。&lt;h4&gt;目的&lt;/h4&gt;通过有效利用基础模型来提升光-SAR图像匹配的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;PromptMID使用预训练扩散模型和视觉基础模型提取多尺度模态不变特征，并设计了专门的特征聚合模块以跨不同粒度融合特征。&lt;h4&gt;主要发现&lt;/h4&gt;在来自四个不同地区的光学-SAR数据集上，PromptMID的表现优于现有的匹配方法，在已见和未见过的数据域中都表现出色且具有强大的跨领域泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究成功地提出了一个新颖的方法来解决光-SAR图像匹配中的模态不变性和泛化问题，并通过实验验证了其优越性。&lt;h4&gt;翻译&lt;/h4&gt;理想化的图像匹配目标是在未知环境下实现稳定和高效的性能。然而，许多现有的基于学习的光学-SAR图像匹配方法，在特定场景下虽然有效但泛化能力有限且难以适应实际应用需求。重复训练或微调匹配模型以解决领域差异不仅不够优雅而且会引入额外的计算开销和数据生产成本。近年来，通用基础模型在增强泛化方面显示了巨大的潜力，然而自然图像与遥感图像之间的视觉域差异使得它们直接应用于光学-SAR图像匹配存在挑战。因此，有效利用基础模型来改进光-SAR图像匹配的泛化能力仍然是一项挑战。为了应对上述挑战，我们提出了PromptMID，一种基于土地使用分类先验信息构建模态不变描述符的方法，用于光学和SAR图像匹配。PromptMID通过利用预训练扩散模型和视觉基础模型（VFMs）提取多尺度模态不变特征，并设计了专门的特征聚合模块以跨不同粒度融合特征。在来自四个不同地区的光学-SAR数据集上的广泛实验表明，PromptMID优于现有的匹配方法，在已见和未见过的数据域中都表现出色且具有强大的跨领域泛化能力。源代码将在https://github.com/HanNieWHU/PromptMID公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The ideal goal of image matching is to achieve stable and efficientperformance in unseen domains. However, many existing learning-basedoptical-SAR image matching methods, despite their effectiveness in specificscenarios, exhibit limited generalization and struggle to adapt to practicalapplications. Repeatedly training or fine-tuning matching models to addressdomain differences is not only not elegant enough but also introducesadditional computational overhead and data production costs. In recent years,general foundation models have shown great potential for enhancinggeneralization. However, the disparity in visual domains between natural andremote sensing images poses challenges for their direct application. Therefore,effectively leveraging foundation models to improve the generalization ofoptical-SAR image matching remains challenge. To address the above challenges,we propose PromptMID, a novel approach that constructs modality-invariantdescriptors using text prompts based on land use classification as priorsinformation for optical and SAR image matching. PromptMID extracts multi-scalemodality-invariant features by leveraging pre-trained diffusion models andvisual foundation models (VFMs), while specially designed feature aggregationmodules effectively fuse features across different granularities. Extensiveexperiments on optical-SAR image datasets from four diverse regions demonstratethat PromptMID outperforms state-of-the-art matching methods, achievingsuperior results in both seen and unseen domains and exhibiting strongcross-domain generalization capabilities. The source code will be made publiclyavailable https://github.com/HanNieWHU/PromptMID.</description>
      <author>example@mail.com (Han Nie, Bin Luo, Jun Liu, Zhitao Fu, Huan Zhou, Shuo Zhang, Weixing Liu)</author>
      <guid isPermaLink="false">2502.18104v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Examining the Threat Landscape: Foundation Models and Model Stealing</title>
      <link>http://arxiv.org/abs/2502.18077v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to BMVC 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文研究了基础模型（FMs）在计算机视觉中的安全性问题，特别是它们对模型窃取攻击的脆弱性。通过实验分析发现，基于这些模型的应用更容易遭受窃取攻击。&lt;h4&gt;背景&lt;/h4&gt;基础模型具备强大的适应能力，在少量或不进行微调的情况下可以应用于特定任务和领域。然而，这种灵活性也可能导致安全风险：攻击者可能利用预训练的基础模型来获取有价值的信息，并将其用于非法目的。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在探讨基于基础模型的应用是否适合商业API部署，尤其是在考虑安全威胁的情况下。&lt;h4&gt;方法&lt;/h4&gt;作者通过实验分析了不同架构的模型在遭受窃取攻击时的表现。具体来说，他们比较了从基础模型微调后的模型与传统的视觉网络（如ResNets）之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;基于Vision Transformer的基础模型更容易受到模型窃取攻击；相比之下，ResNet-18这种传统架构则更加安全。例如，在使用CIFAR-10数据集进行训练时，通过基础模型微调的ViT-L/16模型（作为受害模型）与另一个ViT-L/16模型（作为盗窃者模型）之间有94.28%的一致性；而ResNet-18仅有73.20%。&lt;h4&gt;结论&lt;/h4&gt;论文指出，尽管基础模型在性能上有显著优势，但它们对商业API的部署构成了安全风险。因此，论文呼吁模型所有者注意潜在的安全隐患，并强调了采取强有力的防护措施的重要性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models (FMs) for computer vision learn rich and robustrepresentations, enabling their adaptation to task/domain-specific deploymentswith little to no fine-tuning. However, we posit that the very same strengthcan make applications based on FMs vulnerable to model stealing attacks.Through empirical analysis, we reveal that models fine-tuned from FMs harborheightened susceptibility to model stealing, compared to conventional visionarchitectures like ResNets. We hypothesize that this behavior is due to thecomprehensive encoding of visual patterns and features learned by FMs duringpre-training, which are accessible to both the attacker and the victim. Wereport that an attacker is able to obtain 94.28% agreement (matched predictionswith victim) for a Vision Transformer based victim model (ViT-L/16) trained onCIFAR-10 dataset, compared to only 73.20% agreement for a ResNet-18 victim,when using ViT-L/16 as the thief model. We arguably show, for the first time,that utilizing FMs for downstream tasks may not be the best choice fordeployment in commercial APIs due to their susceptibility to model theft. Wethereby alert model owners towards the associated security risks, and highlightthe need for robust security measures to safeguard such models against theft.Code is available at https://github.com/rajankita/foundation_model_stealing.</description>
      <author>example@mail.com (Ankita Raj, Deepankar Varma, Chetan Arora)</author>
      <guid isPermaLink="false">2502.18077v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Are GNNs doomed by the topology of their input graph?</title>
      <link>http://arxiv.org/abs/2502.17739v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了图神经网络（GNNs）在处理图结构数据时的局限性及其输入图拓扑对模型性能的影响。通过引入$k$-hop相似性的概念来分析局部特征如何与消息传递方案交互，以产生有效的学习或不可避免的过度平滑现象。&lt;h4&gt;背景&lt;/h4&gt;图神经网络已经显示出在其领域内的显著成功，但人们对图结构数据对其行为影响的理解仍然不足。&lt;h4&gt;目的&lt;/h4&gt;研究GNN是否固有地受到其输入图拓扑结构的影响，并探讨局部特征与消息传递机制之间的相互作用如何影响全局表现。&lt;h4&gt;方法&lt;/h4&gt;引入了$k$-hop相似性的概念来衡量局部相似的邻域是否会导致一致的节点表示。通过实验验证这些假设和发现的实际意义。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了图结构中的固有属性对于GNN的有效学习或不可避免的过度平滑现象有着重要影响，而不仅仅是基于局部特征的相似性。&lt;h4&gt;结论&lt;/h4&gt;该工作强调了理解输入图拓扑对GNN性能影响的重要性，并为进一步的研究提供了理论基础和实验依据。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已包含在内，无需额外翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have demonstrated remarkable success in learningfrom graph-structured data. However, the influence of the input graph'stopology on GNN behavior remains poorly understood. In this work, we explorewhether GNNs are inherently limited by the structure of their input graphs,focusing on how local topological features interact with the message-passingscheme to produce global phenomena such as oversmoothing or expressiverepresentations. We introduce the concept of $k$-hop similarity and investigatewhether locally similar neighborhoods lead to consistent node representations.This interaction can result in either effective learning or inevitableoversmoothing, depending on the inherent properties of the graph. Our empiricalexperiments validate these insights, highlighting the practical implications ofgraph topology on GNN performance.</description>
      <author>example@mail.com (Amine Mohamed Aboussalah, Abdessalam Ed-dib)</author>
      <guid isPermaLink="false">2502.17739v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>AutoCas: Autoregressive Cascade Predictor in Social Networks via Large Language Models</title>
      <link>http://arxiv.org/abs/2502.18040v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于大型语言模型（LLM）的新型信息传播预测模型AutoCas，该模型针对信息传播数据的特点进行了专门优化。&lt;h4&gt;背景&lt;/h4&gt;信息传播机制、用户行为以及时间活动模式存在显著多样性，现有的模型难以适应这种变化。同时，可用的信息传播数据量相对有限。&lt;h4&gt;目的&lt;/h4&gt;构建一个能够利用大型语言模型的架构优势进行信息传播预测的基础模型，并提高其在流行度预测方面的性能。&lt;h4&gt;方法&lt;/h4&gt;将信息传播数据分词以匹配序列建模原则；重新定义信息传播扩散为自回归建模任务，以充分利用LLM的架构特点；引入提示学习技术增强LLM与信息传播预测之间的协同作用。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，AutoCas在信息流流行度预测方面显著优于基线模型，并且展示了源自LLMs的可扩展行为。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新的自回归信息传播预测器（AutoCas），通过专门适应大型语言模型的优势和挑战来提高信息传播流行度预测的效果。&lt;h4&gt;翻译&lt;/h4&gt;在信息级联中进行受欢迎程度预测是社会计算中的重要任务，广泛应用于病毒式营销、错误信息控制以及内容推荐。然而，信息传播机制、用户行为及时间活动模式显示出了显著的多样性，这要求一种能够适应这些变化的基础模型。同时，在用于训练大型语言模型（LLMs）的巨大数据集面前，可用的信息级联数据量相对有限。最近的研究表明，通过利用不同时间序列领域之间的共同点来应用LLM进行时间序列预测是可行的。基于这一洞察，我们引入了自回归信息级联预测器（AutoCas），这是一种专门用于级联流行度预测、由大型语言模型增强的模型。与自然语言序列相比，级联数据具有复杂的本地拓扑结构、传播上下文和不断变化的动力学特点，这需要为有效的LLM集成进行专门适应。为了应对这些挑战，我们首先将级联数据分词以使其符合序列建模原则。接下来，我们将信息传播扩散重新定义为自回归建模任务，以便充分利用LLMs的架构优势。此外，在常规方法之外，我们进一步引入了提示学习来增强LLM和级联预测之间的协同作用。广泛的实验表明，AutoCas在级联流行度预测方面显著优于基线模型，并且展示了源自LLMs的可扩展行为。代码可在以下仓库中获取：https://anonymous.4open.science/r/AutoCas-85C6&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Popularity prediction in information cascades plays a crucial role in socialcomputing, with broad applications in viral marketing, misinformation control,and content recommendation. However, information propagation mechanisms, userbehavior, and temporal activity patterns exhibit significant diversity,necessitating a foundational model capable of adapting to such variations. Atthe same time, the amount of available cascade data remains relatively limitedcompared to the vast datasets used for training large language models (LLMs).Recent studies have demonstrated the feasibility of leveraging LLMs fortime-series prediction by exploiting commonalities across different time-seriesdomains. Building on this insight, we introduce the Autoregressive InformationCascade Predictor (AutoCas), an LLM-enhanced model designed specifically forcascade popularity prediction. Unlike natural language sequences, cascade datais characterized by complex local topologies, diffusion contexts, and evolvingdynamics, requiring specialized adaptations for effective LLM integration. Toaddress these challenges, we first tokenize cascade data to align it withsequence modeling principles. Next, we reformulate cascade diffusion as anautoregressive modeling task to fully harness the architectural strengths ofLLMs. Beyond conventional approaches, we further introduce prompt learning toenhance the synergy between LLMs and cascade prediction. Extensive experimentsdemonstrate that AutoCas significantly outperforms baseline models in cascadepopularity prediction while exhibiting scaling behavior inherited from LLMs.Code is available at this repository:https://anonymous.4open.science/r/AutoCas-85C6</description>
      <author>example@mail.com (Yuhao Zheng, Chenghua Gong, Rui Sun, Juyuan Zhang, Liming Pan, Linyuan Lv)</author>
      <guid isPermaLink="false">2502.18040v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Learning Decentralized Swarms Using Rotation Equivariant Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17612v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的去中心化控制方法，通过在图神经网络控制器中强制执行旋转等方性和平移不变性对称性，以优化群集的凝聚和一致性。&lt;h4&gt;背景&lt;/h4&gt;没有集中式控制的情况下协调代理以实现集体目标是具有挑战性的，尤其是在控制自主车队和使用传感器网络进行监控与侦察的应用场景中。现有方法受到自然界自我组织现象（如鸟群行为）启发，但这些去中心化控制器难以保持群体的凝聚性。&lt;h4&gt;目的&lt;/h4&gt;通过引入对称性约束在图神经网络架构中开发出一种新的去中心化控制器，以提高其维持群集凝聚力的能力以及泛化性能。&lt;h4&gt;方法&lt;/h4&gt;强制执行旋转等方性和平移不变性对称性来设计适用于鸟群控制的GNN控制器，这些是对鸟群动态中存在的自然规律。此方法比没有这种限制的现有模型使用更少的数据和参数量，并且在测试中的表现更加优异。&lt;h4&gt;主要发现&lt;/h4&gt;该研究证明了新的去中心化控制器需要较少的训练数据和可学习权重即可实现与传统GNN控制器相当的鸟群控制性能，同时展现出更好的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;通过应用对称性约束设计出来的新型图神经网络架构为构建更加高效且灵活的去中心化控制系统提供了可能，在减少模型复杂度的同时提高了系统的鲁棒性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;将代理在没有集中控制的情况下协调以优化集体目标是具有挑战性的，尤其是在诸如自主车队管理和使用传感器网络进行监控和侦察的应用中。分散控制器的设计受到了自然界中的自组织现象（如鸟群行为）的启发，然而现有方法难以保持群集的凝聚力。图神经网络架构已经作为开发能够维持群体凝聚性的去中心化控制工具成为必不可少的机器学习技术，但它们未能利用存在于鸟类动态中的对称性，限制了其泛化能力。研究强制执行旋转等方性和平移不变性在分散鸟群GNN控制器中，并实现与没有这些约束的传统模型相比使用70%更少的训练数据和75%更少的可学习权重即可获得类似的整体控制效果。此外，该方法展示出更好的泛化能力。相关代码和动画可在GitHub上获取（链接：http://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The orchestration of agents to optimize a collective objective withoutcentralized control is challenging yet crucial for applications such ascontrolling autonomous fleets, and surveillance and reconnaissance using sensornetworks. Decentralized controller design has been inspired byself-organization found in nature, with a prominent source of inspiration beingflocking; however, decentralized controllers struggle to maintain flockcohesion. The graph neural network (GNN) architecture has emerged as anindispensable machine learning tool for developing decentralized controllerscapable of maintaining flock cohesion, but they fail to exploit the symmetriespresent in flocking dynamics, hindering their generalizability. We enforcerotation equivariance and translation invariance symmetries in decentralizedflocking GNN controllers and achieve comparable flocking control with 70% lesstraining data and 75% fewer trainable weights than existing GNN controllerswithout these symmetries enforced. We also show that our symmetry-awarecontroller generalizes better than existing GNN controllers. Code andanimations are available athttp://github.com/Utah-Math-Data-Science/Equivariant-Decentralized-Controllers.</description>
      <author>example@mail.com (Taos Transue, Bao Wang)</author>
      <guid isPermaLink="false">2502.17612v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>FetchBot: Object Fetching in Cluttered Shelves via Zero-Shot Sim2Real</title>
      <link>http://arxiv.org/abs/2502.17894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;FetchBot是一个从杂乱的货架上拾取物品的模拟到现实框架，旨在实现零样本泛化和安全意识的能力。通过利用高效的体积网格方法生成多样化的仿真场景，并采用动态感知强化学习策略来获取物体抓取轨迹。&lt;h4&gt;背景&lt;/h4&gt;在混乱环境中从货架上抓取物品是机器人帮助人类完成实际任务的重要能力，但受到运动空间受限、视野有限以及复杂对象动力学的影响而极具挑战性。&lt;h4&gt;目的&lt;/h4&gt;介绍FetchBot框架，以解决数据稀缺问题，并开发一种能够泛化到不同真实世界场景中的安全意识物体抓取策略。&lt;h4&gt;方法&lt;/h4&gt;提出了一个高效的体积网格方法来生成大规模的混乱货架场景；训练了感知动态强化学习策略以获取物体抓取轨迹；并将这种策略提炼成基于视觉的政策，用于现实世界的部署。采用深度信息输入以减少模拟到现实的差距，并设计了一种新颖的多视图表示学习架构。&lt;h4&gt;主要发现&lt;/h4&gt;FetchBot框架通过有效的数据生成和转移机制，在广泛的环境中表现出强大的泛化能力。实验结果证明了其在处理多种真实世界场景时的有效性和安全性。&lt;h4&gt;结论&lt;/h4&gt;提出的FetchBot方法为机器人从混乱货架中安全地抓取物品提供了一个新的途径，显示出潜在的广泛应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object fetching from cluttered shelves is an important capability for robotsto assist humans in real-world scenarios. Achieving this task demands roboticbehaviors that prioritize safety by minimizing disturbances to surroundingobjects, an essential but highly challenging requirement due to restrictedmotion space, limited fields of view, and complex object dynamics. In thispaper, we introduce FetchBot, a sim-to-real framework designed to enablezero-shot generalizable and safety-aware object fetching from cluttered shelvesin real-world settings. To address data scarcity, we propose an efficientvoxel-based method for generating diverse simulated cluttered shelf scenes atscale and train a dynamics-aware reinforcement learning (RL) policy to generateobject fetching trajectories within these scenes. This RL policy, whichleverages oracle information, is subsequently distilled into a vision-basedpolicy for real-world deployment. Considering that sim-to-real discrepanciesstem from texture variations mostly while from geometric dimensions rarely, wepropose to adopt depth information estimated by full-fledged depth foundationmodels as the input for the vision-based policy to mitigate sim-to-real gap. Totackle the challenge of limited views, we design a novel architecture forlearning multi-view representations, allowing for comprehensive encoding ofcluttered shelf scenes. This enables FetchBot to effectively minimizecollisions while fetching objects from varying positions and depths, ensuringrobust and safety-aware operation. Both simulation and real-robot experimentsdemonstrate FetchBot's superior generalization ability, particularly inhandling a broad range of real-world scenarios, includ</description>
      <author>example@mail.com (Weiheng Liu, Yuxuan Wan, Jilong Wang, Yuxuan Kuang, Xuesong Shi, Haoran Li, Dongbin Zhao, Zhizheng Zhang, He Wang)</author>
      <guid isPermaLink="false">2502.17894v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CalibRefine: Deep Learning-Based Online Automatic Targetless LiDAR-Camera Calibration with Iterative and Attention-Driven Post-Refinement</title>
      <link>http://arxiv.org/abs/2502.17648v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to Transportation Research Part C: Emerging Technologies&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CalibRefine 是一种自动、无目标的在线校准框架，适用于 LiDAR 和相机传感器，能够直接处理原始点云和图像。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR-摄像头校准方法通常依赖于手动放置的目标或初步参数估计等步骤，这限制了它们在真实环境中的可扩展性和适应性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需人工参与的自动校准框架以提高多传感器系统的鲁棒性感知能力。&lt;h4&gt;方法&lt;/h4&gt;{'阶段一': '通过共同特征判别器使用相对位置、外观嵌入和语义类别来生成可靠的 LiDAR-相机对应关系', '阶段二': '基于粗略同构变换的校准', '阶段三': '在新增数据帧可用的情况下，采用迭代细化逐步改进对齐精度', '阶段四': '利用视觉变换器和跨注意力机制解决非平面畸变问题'}&lt;h4&gt;主要发现&lt;/h4&gt;CalibRefine 可以提供高精度的校准结果，并且不需要复杂的前期准备或大量人工干预，在真实环境中保持了与手动调优基线方法的竞争性甚至超越。&lt;h4&gt;结论&lt;/h4&gt;研究展示了如何通过稳健的对象级特征匹配、迭代和自我监督注意机制调整，实现复杂条件下传感器融合的一致性。&lt;h4&gt;翻译&lt;/h4&gt;准确的多传感器校准对于部署自动驾驶汽车、机器人技术以及智能交通系统中的强大感知系统至关重要。现有的LiDAR-相机校准方法往往依赖于手动放置的目标物或初步参数估计等步骤，这些限制了其在现实世界环境下的可扩展性和适应性。这项工作提出了一种全自动的无目标在线校准框架 CalibRefine ，可以直接处理原始 LiDAR 点云和摄像机图像。该方案分为四个阶段：首先是一个共同特征判别器，使用相对位置、外观嵌入以及语义类别信息生成可靠的LiDAR-相机对应关系；接着是基于粗略同构变换的校准；然后在新数据帧可用时进行迭代细化，逐步提高对齐精度；最后通过利用视觉变换器和跨注意力机制解决非平面畸变问题。经过两项城市交通数据集上的广泛实验验证，CalibRefine 在不依赖人工参与的前提下提供了高精度的校准结果，并且超过了最先进的无目标方法，同时与手动调优基线保持了竞争性甚至超越其性能表现。我们的研究强调了通过稳健的对象级特征匹配以及迭代和自我监督注意机制调整，在复杂真实世界条件下实现传感器融合一致性的重要性，无需地面真值校准矩阵或复杂的前期数据处理步骤。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate multi-sensor calibration is essential for deploying robustperception systems in applications such as autonomous driving, robotics, andintelligent transportation. Existing LiDAR-camera calibration methods oftenrely on manually placed targets, preliminary parameter estimates, or intensivedata preprocessing, limiting their scalability and adaptability in real-worldsettings. In this work, we propose a fully automatic, targetless, and onlinecalibration framework, CalibRefine, which directly processes raw LiDAR pointclouds and camera images. Our approach is divided into four stages: (1) aCommon Feature Discriminator that trains on automatically detectedobjects--using relative positions, appearance embeddings, and semanticclasses--to generate reliable LiDAR-camera correspondences, (2) a coarsehomography-based calibration, (3) an iterative refinement to incrementallyimprove alignment as additional data frames become available, and (4) anattention-based refinement that addresses non-planar distortions by leveraginga Vision Transformer and cross-attention mechanisms. Through extensiveexperiments on two urban traffic datasets, we show that CalibRefine delivershigh-precision calibration results with minimal human involvement,outperforming state-of-the-art targetless methods and remaining competitivewith, or surpassing, manually tuned baselines. Our findings highlight howrobust object-level feature matching, together with iterative andself-supervised attention-based adjustments, enables consistent sensor fusionin complex, real-world conditions without requiring ground-truth calibrationmatrices or elaborate data preprocessing.</description>
      <author>example@mail.com (Lei Chenga, Lihao Guoa, Tianya Zhangb, Tam Bangb, Austin Harrisb, Mustafa Hajijc, Mina Sartipib, Siyang Cao)</author>
      <guid isPermaLink="false">2502.17648v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Bearing Fault Classification Under Variable Conditions: A 1D CNN with Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.17524v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文提出了一种基于振动和电机相电流信号的滚动轴承故障分类方法，利用一维卷积神经网络框架。&lt;h4&gt;背景&lt;/h4&gt;滚动轴承在确保旋转机械的可靠性和效率方面发挥着关键作用，减少摩擦并处理重要负载。然而，高达90%的机械故障是由轴承失效引起的，这凸显了可靠的状况监测和故障检测的重要性。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于振动信号和电机相电流的一维卷积神经网络（1D CNN）框架下的多模态滚动轴承故障分类方法，以提高故障检测准确性。&lt;h4&gt;方法&lt;/h4&gt;该研究通过融合来自不同传感器的特征来增强故障检测精度，并在基线条件下实现了96%的准确率。此外，在使用L2正则化的情况下，模型性能得到了显著改善。&lt;h4&gt;主要发现&lt;/h4&gt;提出的1D CNN框架与迁移学习策略相结合，不仅提高了轴承故障分类的准确性，还在不同的操作条件下展示了稳健性表现。&lt;h4&gt;结论&lt;/h4&gt;虽然该方法在计算时间上需要更多资源，但为适应工业环境中可变工作条件下的更准确、更具适应性和效率的滚动轴承故障分类奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;滚动轴承在旋转机械中起着至关重要的作用，通过减少摩擦并处理重要负载来确保其可靠性和效率。高达90%的机械故障是由轴承失效引起的，这突显了可靠状态监测和故障检测的重要性。本文提出了一种基于振动信号和电机相电流的一维卷积神经网络（1D CNN）框架下的多模态滚动轴承故障分类方法，以提高故障检测准确性。该研究通过融合来自不同传感器的特征来增强故障检测精度，并在基线条件下实现了96%的准确率。此外，在使用L2正则化的情况下，模型性能得到了显著改善。提出的1D CNN框架与迁移学习策略相结合，不仅提高了轴承故障分类的准确性，还在不同的操作条件下展示了稳健性表现。虽然该方法在计算时间上需要更多资源，但为适应工业环境中可变工作条件下的更准确、更具适应性和效率的滚动轴承故障分类奠定了基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Bearings play an integral role in ensuring the reliability and efficiency ofrotating machinery - reducing friction and handling critical loads. Bearingfailures that constitute up to 90% of mechanical faults highlight theimperative need for reliable condition monitoring and fault detection. Thisstudy proposes a multimodal bearing fault classification approach that relieson vibration and motor phase current signals within a one-dimensionalconvolutional neural network (1D CNN) framework. The method fuses features frommultiple signals to enhance the accuracy of fault detection. Under the baselinecondition (1,500 rpm, 0.7 Nm load torque, and 1,000 N radial force), the modelreaches an accuracy of 96% with addition of L2 regularization. This representsa notable improvement of 2% compared to the non-regularized model. In addition,the model demonstrates robust performance across three distinct operatingconditions by employing transfer learning (TL) strategies. Among the tested TLvariants, the approach that preserves parameters up to the first max-pool layerand then adjusts subsequent layers achieves the highest performance. While thisapproach attains excellent accuracy across varied conditions, it requires morecomputational time due to its greater number of trainable parameters. Toaddress resource constraints, less computationally intensive models offerfeasible trade-offs, albeit at a slight accuracy cost. Overall, this multimodal1D CNN framework with late fusion and TL strategies lays a foundation for moreaccurate, adaptable, and efficient bearing fault classification in industrialenvironments with variable operating conditions.</description>
      <author>example@mail.com (Tasfiq E. Alam, Md Manjurul Ahsan, Shivakumar Raman)</author>
      <guid isPermaLink="false">2502.17524v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>EEGM2: An Efficient Mamba-2-Based Self-Supervised Framework for Long-Sequence EEG Modeling</title>
      <link>http://arxiv.org/abs/2502.17873v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的EEG基础模型框架EEGM2，旨在克服基于Transformer架构在处理脑电图数据时遇到的计算复杂度问题。EEGM2利用结构化状态空间对偶性（SSD）进行自监督学习，并通过Mamba-2模型捕捉局部和全局特征。&lt;h4&gt;背景&lt;/h4&gt;深度学习已经在脑电图基础模型的发展中取得了显著进展，其中基于Transformer架构的模型在捕获长距离依赖方面表现出色。然而，它们的二次计算复杂度导致了内存效率低下、训练和推理速度慢的问题，限制了其作为基础模型的大规模应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的EEG自监督框架EEGM2，以克服现有基于Transformer架构的脑电图模型在计算复杂性和泛化能力上的局限性。&lt;h4&gt;方法&lt;/h4&gt;1. 重建基线框架：通过Mamba-2结构状态空间模型捕获局部和全局EEG特征；2. 空间时间感知损失函数：增强对噪声的鲁棒性并保留光谱信息；3. 多分支感受野输入嵌入策略：改进跨受试者泛化能力和序列长度变化时的稳定性。&lt;h4&gt;主要发现&lt;/h4&gt;与传统的预训练方法相比，EEGM2在长序列任务中表现出色，并通过实验验证了其在六个EEG数据集上的优越性能和计算效率。此外，它还实现了跨域最佳准确性并降低了计算开销。&lt;h4&gt;结论&lt;/h4&gt;EEGM2不仅达到了最先进的跨领域准确性，而且减少了计算开销，在资源受限的BCI设备上部署时更高效。&lt;h4&gt;翻译&lt;/h4&gt;深度学习在脑电图基础模型的发展中取得了显著进步，尤其是基于Transformer架构的方法。然而，这些方法面临的二次复杂性挑战限制了它们的可扩展性和泛化能力。本文提出了一种新的自监督框架EEGM2，它引入了结构状态空间对偶性的概念，并利用Mamba-2模型来捕捉局部和全局特征。此外，通过改进的空间时间损失函数增强了鲁棒性和光谱信息保留。实验结果显示，与传统预训练方法相比，EEGM2在长序列任务中的表现更优，并且计算开销更低，适合资源受限的设备使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning has achieved significant progress in the development ofelectroencephalogram (EEG) foundation models, with Transformer-basedarchitectures excelling at capturing long-range dependencies. However, theirquadratic computational complexity presents challenges in memory efficiency,training, and inference speed, limiting their scalability and generalizabilityas a foundation model. In this paper, we propose EEGM2, a self-supervisedframework based on structured state space duality (SSD) that overcomes theselimitations. EEGM2 introduces three key innovations: (1) a reconstruction-basedframework that captures both local and global EEG features through Mamba-2structured state space models, (2) a spatiotemporal-aware loss function thatenhances robustness to noise and preserves spectral information, and (3) amulti-branch receptive field input embedding strategy that improvescross-subject generalization and stability for EEG sequences of varyinglengths. In comparison to traditional pretraining methods, on raw EEG or latentrepresentation spaces, EEGM2 shows superior performance on long-sequence tasks,where conventional models struggle. Our experimental results on six EEGdatasets validate that EEGM2 not only achieves state-of-the-art cross-domainaccuracy but also reduces computational overhead, making it a more efficientsolution for deployment on resource-constrained BCI devices.</description>
      <author>example@mail.com (Jiazhen Hong, Geoffrey Mackellar, Soheila Ghane)</author>
      <guid isPermaLink="false">2502.17873v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>ASurvey: Spatiotemporal Consistency in Video Generation</title>
      <link>http://arxiv.org/abs/2502.17863v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了最近在视频生成领域的进展，涵盖了基础模型、信息表示、生成方案、后处理技术以及评估指标五个关键方面。&lt;h4&gt;背景&lt;/h4&gt;通过利用动态视觉生成方法，视频生成推动了人工智能生成内容（AIGC）的发展。相比静态图像生成，视频生成具有独特挑战，需要高质量的单帧画面和时间一致性以保持空间和时间序列的一致性。&lt;h4&gt;目的&lt;/h4&gt;回顾最近的进展并探讨维持时空一致性的贡献，填补相关文献综述的空白，以便更深入地理解高质量视频生成的基础机制。&lt;h4&gt;方法&lt;/h4&gt;系统地审查了五个关键方面：基础模型、信息表示、生成方案、后处理技术和评估指标，并特别关注它们在保持时空一致性方面的贡献。&lt;h4&gt;主要发现&lt;/h4&gt;近期的工作主要集中于解决视频生成中的时空一致性问题，但很少有从这个角度组织的文献综述。&lt;h4&gt;结论&lt;/h4&gt;讨论了该领域未来的发展方向和挑战，旨在激励进一步的努力以推进视频生成技术的进步。&lt;h4&gt;翻译&lt;/h4&gt;通过利用动态视觉生成方法，视频生成推动了人工智能生成内容（AIGC）的发展。相比静态图像生成，视频生成具有独特挑战，需要高质量的单帧画面和时间一致性以保持空间和时间序列的一致性。近期的工作主要集中于解决视频生成中的时空一致性问题，但很少有从这个角度组织的文献综述。该论文系统地回顾了最近在五个关键方面的进展：基础模型、信息表示、生成方案、后处理技术和评估指标，并特别关注它们如何保持时空一致性的贡献。此外，讨论了未来的发展方向和挑战，旨在激励进一步的努力以推进视频生成技术的进步。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Video generation, by leveraging a dynamic visual generation method, pushesthe boundaries of Artificial Intelligence Generated Content (AIGC). Videogeneration presents unique challenges beyond static image generation, requiringboth high-quality individual frames and temporal coherence to maintainconsistency across the spatiotemporal sequence. Recent works have aimed ataddressing the spatiotemporal consistency issue in video generation, while fewliterature review has been organized from this perspective. This gap hinders adeeper understanding of the underlying mechanisms for high-quality videogeneration. In this survey, we systematically review the recent advances invideo generation, covering five key aspects: foundation models, informationrepresentations, generation schemes, post-processing techniques, and evaluationmetrics. We particularly focus on their contributions to maintainingspatiotemporal consistency. Finally, we discuss the future directions andchallenges in this field, hoping to inspire further efforts to advance thedevelopment of video generation.</description>
      <author>example@mail.com (Zhiyu Yin, Kehai Chen, Xuefeng Bai, Ruili Jiang, Juntao Li, Hongdong Li, Jin Liu, Yang Xiang, Jun Yu, Min Zhang)</author>
      <guid isPermaLink="false">2502.17863v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Laplace-Beltrami Operator for Gaussian Splatting</title>
      <link>http://arxiv.org/abs/2502.17531v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种计算拉普拉斯-贝尔特拉米算子的新方法，直接在高斯点阵上使用马氏距离来实现。这种方法提高了处理由3D高斯点阵表示的数据时的准确性。&lt;h4&gt;背景&lt;/h4&gt;随着3D高斯点阵技术的流行和其应用从渲染扩展到3D重建，对这种新数据表示形式进行几何处理的需求也日益增长。&lt;h4&gt;目的&lt;/h4&gt;提出一种直接在3D高斯点阵上计算拉普拉斯-贝尔特拉米算子的方法，以提高基于该表示的数据处理准确性，并能够评估优化过程中的输出质量。&lt;h4&gt;方法&lt;/h4&gt;通过使用马氏距离来定义和计算拉普拉斯-贝尔特拉米算子，从而直接在3D高斯点阵上进行几何处理。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在基于3D高斯点阵中心的点云数据中，该方法比传统的点云拉普拉斯算子具有更高的准确性，并且能够评估优化过程中的输出质量。&lt;h4&gt;结论&lt;/h4&gt;通过直接在3D高斯点阵上计算拉普拉斯-贝尔特拉米算子，可以提高基于该表示的数据处理效率和准确性，尤其是在处理大量离群值的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rising popularity of 3D Gaussian splatting and the expanse ofapplications from rendering to 3D reconstruction, there comes also a need forgeometry processing applications directly on this new representation. Whileconsidering the centers of Gaussians as a point cloud or meshing them is anoption that allows to apply existing algorithms, this might ignore informationpresent in the data or be unnecessarily expensive. Additionally, Gaussiansplatting tends to contain a large number of outliers which do not affect therendering quality but need to be handled correctly in order not to producenoisy results in geometry processing applications. In this work, we propose aformulation to compute the Laplace-Beltrami operator, a widely used tool ingeometry processing, directly on Gaussian splatting using the Mahalanobisdistance. While conceptually similar to a point cloud Laplacian, ourexperiments show superior accuracy on the point clouds encoded in the Gaussiansplatting centers and, additionally, the operator can be used to evaluate thequality of the output during optimization.</description>
      <author>example@mail.com (Hongyu Zhou, Zorah Lähner)</author>
      <guid isPermaLink="false">2502.17531v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Spectral Theory for Edge Pruning in Asynchronous Recurrent Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17522v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种基于图谱理论的动态修剪方法，用于简化异步递归图神经网络（ARGNN），以提高其在处理动态图形数据时的效率。&lt;h4&gt;背景&lt;/h4&gt;图神经网络（GNN）已经成为学习结构化图形数据的强大工具，在社交网络分析和分子生物学等领域中得到广泛应用。特别地，ARGNN因其能捕捉到动态图形中的复杂依赖关系而脱颖而出，类似于活体生物复杂的适应性特征。&lt;h4&gt;目的&lt;/h4&gt;尽管ARGNN在性能上表现出色，但其复杂性往往导致模型庞大且计算成本高昂。因此，修剪不必要的边成为提高效率而不显著降低性能的关键。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种基于图谱理论的动态修剪策略，利用网络图形拉普拉斯矩阵特征值的虚部来识别和去除不重要的连接。&lt;h4&gt;主要发现&lt;/h4&gt;该论文展示了如何通过有效减少ARGNN中的冗余边来提高模型效率，并且在保证性能的同时减少了计算资源的需求。&lt;h4&gt;结论&lt;/h4&gt;所提出的动态修剪方法为优化复杂GNN架构提供了一种有效的解决方案，特别适用于处理大量数据的场景中需要高效推理的应用场合。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容直接用于英文到中文的翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have emerged as a powerful tool for learning ongraph-structured data, finding applications in numerous domains includingsocial network analysis and molecular biology. Within this broad category,Asynchronous Recurrent Graph Neural Networks (ARGNNs) stand out for theirability to capture complex dependencies in dynamic graphs, resembling livingorganisms' intricate and adaptive nature. However, their complexity often leadsto large and computationally expensive models. Therefore, pruning unnecessaryedges becomes crucial for enhancing efficiency without significantlycompromising performance. This paper presents a dynamic pruning method based ongraph spectral theory, leveraging the imaginary component of the eigenvalues ofthe network graph's Laplacian.</description>
      <author>example@mail.com (Nicolas Bessone)</author>
      <guid isPermaLink="false">2502.17522v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multimodality Helps Few-shot 3D Point Cloud Semantic Segmentation</title>
      <link>http://arxiv.org/abs/2410.22489v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published at ICLR 2025 (Spotlight)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于三维点云分割的多模态少样本分割模型（MM-FSS），该模型利用文本标签和可能存在的二维图像模式，弥补了现有方法忽略多模态信息的问题。&lt;h4&gt;背景&lt;/h4&gt;目前的少样本3D点云分割方法主要关注单一模态的点云输入，忽略了多模态信息的优势。&lt;h4&gt;目的&lt;/h4&gt;引入一个利用多种模态数据（如文本标签和2D图像）的多模态少样本设置，并开发相应的模型以提升性能。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的模型——MultiModal Few-Shot SegNet (MM-FSS)，该模型通过共享骨干网络从多个模式中提取互补信息，使用预训练的文字编码器生成文字嵌入。还设计了Multimodal Correlation Fusion（MCF）模块和Multimodal Semantic Fusion（MSF）模块来利用多模态数据。此外提出Test-time Adaptive Cross-modal Calibration（TACC）技术以减轻训练偏差。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MM-FSS在S3DIS和ScanNet等数据集上的性能显著优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;本文证明了利用被忽视的自由模态信息可以提高少样本三维点云分割任务的效果，并为未来的研究提供了有价值的见解。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文版&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-29&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Few-shot 3D point cloud segmentation (FS-PCS) aims at generalizing models tosegment novel categories with minimal annotated support samples. While existingFS-PCS methods have shown promise, they primarily focus on unimodal point cloudinputs, overlooking the potential benefits of leveraging multimodalinformation. In this paper, we address this gap by introducing a multimodalFS-PCS setup, utilizing textual labels and the potentially available 2D imagemodality. Under this easy-to-achieve setup, we present the MultiModal Few-ShotSegNet (MM-FSS), a model effectively harnessing complementary information frommultiple modalities. MM-FSS employs a shared backbone with two heads to extractintermodal and unimodal visual features, and a pretrained text encoder togenerate text embeddings. To fully exploit the multimodal information, wepropose a Multimodal Correlation Fusion (MCF) module to generate multimodalcorrelations, and a Multimodal Semantic Fusion (MSF) module to refine thecorrelations using text-aware semantic guidance. Additionally, we propose asimple yet effective Test-time Adaptive Cross-modal Calibration (TACC)technique to mitigate training bias, further improving generalization.Experimental results on S3DIS and ScanNet datasets demonstrate significantperformance improvements achieved by our method. The efficacy of our approachindicates the benefits of leveraging commonly-ignored free modalities forFS-PCS, providing valuable insights for future research. The code is availableat https://github.com/ZhaochongAn/Multimodality-3D-Few-Shot</description>
      <author>example@mail.com (Zhaochong An, Guolei Sun, Yun Liu, Runjia Li, Min Wu, Ming-Ming Cheng, Ender Konukoglu, Serge Belongie)</author>
      <guid isPermaLink="false">2410.22489v3</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Macro- and Micro-Hierarchical Transfer Learning Framework for Cross-Domain Fake News Detection</title>
      <link>http://arxiv.org/abs/2502.14403v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 8 figures, to be published in The 2025 ACM Web Conference  (WWW '25)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于跨域假新闻检测的宏微观分层迁移学习框架（MMHT），该框架旨在解决现有方法在知识转移和假新闻检测性能方面的局限性。&lt;h4&gt;背景&lt;/h4&gt;跨域假新闻检测的目标是通过在不同领域之间传输知识来减轻领域的偏移并提高检测性能。现有的方法基于源领域的新闻内容和用户互动向目标领域传递知识，但它们面临着两个主要限制：忽略新闻内容中无关事实特征的负面影响以及忽视用户参与度与新闻内容之间的关系。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效解决现有假新闻检测方法存在的问题，并优化知识传输效率的新框架。&lt;h4&gt;方法&lt;/h4&gt;1. 微观分层解缠模块，旨在从源域的新闻内容中分离出相关性和无关性的事实特征，以提高目标领域的假新闻检测性能。2. 宏观分层迁移学习模块，根据不同领域中用户共享行为生成参与度特征，从而增强知识传输的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的框架在真实数据集上的表现明显优于现有的最先进的基准方法。&lt;h4&gt;结论&lt;/h4&gt;通过解决现有跨域假新闻检测方法中的两个关键限制，MMHT 框架能够有效提高跨领域假新闻检测的性能和知识传输效率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714517&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain fake news detection aims to mitigate domain shift and improvedetection performance by transferring knowledge across domains. Existingapproaches transfer knowledge based on news content and user engagements from asource domain to a target domain. However, these approaches face two mainlimitations, hindering effective knowledge transfer and optimal fake newsdetection performance. Firstly, from a micro perspective, they neglect thenegative impact of veracity-irrelevant features in news content whentransferring domain-shared features across domains. Secondly, from a macroperspective, existing approaches ignore the relationship between userengagement and news content, which reveals shared behaviors of common usersacross domains and can facilitate more effective knowledge transfer. To addressthese limitations, we propose a novel macro- and micro- hierarchical transferlearning framework (MMHT) for cross-domain fake news detection. Firstly, wepropose a micro-hierarchical disentangling module to disentangleveracity-relevant and veracity-irrelevant features from news content in thesource domain for improving fake news detection performance in the targetdomain. Secondly, we propose a macro-hierarchical transfer learning module togenerate engagement features based on common users' shared behaviors indifferent domains for improving effectiveness of knowledge transfer. Extensiveexperiments on real-world datasets demonstrate that our framework significantlyoutperforms the state-of-the-art baselines.</description>
      <author>example@mail.com (Xuankai Yang, Yan Wang, Xiuzhen Zhang, Shoujin Wang, Huaxiong Wang, Kwok Yan Lam)</author>
      <guid isPermaLink="false">2502.14403v2</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Mechanistic Interpretability for Multi-Modal Foundation Models</title>
      <link>http://arxiv.org/abs/2502.17516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  30 pages, 4 Figures, 10 Tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了基础模型的发展对机器学习研究的影响，特别是大型语言模型（LLMs）和多模态基础模型（MMFMs）。虽然在解释LLM方面取得了显著进展，但MMFMs（如对比视觉-语言模型、生成式视觉-语言模型以及文本到图像模型）在单模态框架之外提出了独特的可解释性挑战。&lt;h4&gt;背景&lt;/h4&gt;大规模基础模型的兴起改变了机器学习研究的方向，并促使研究人员努力揭示这些模型的工作原理以开发更高效和可靠的控制应用。虽然LLM的理解已经取得了很大进展，但MMFMs的透明度仍然落后于LLM。&lt;h4&gt;目的&lt;/h4&gt;该综述探讨了两个关键方面：一是将LLM解释方法应用于多模态模型；二是理解单模态语言模型与跨模式系统之间的机制差异。&lt;h4&gt;方法&lt;/h4&gt;通过系统性地回顾现有的MMFM分析技术，提出了可解释性的结构分类法，并比较了单模态和多模态架构的见解。&lt;h4&gt;主要发现&lt;/h4&gt;指出了当前在LLM和MMFM之间理解上的重大差距，并强调了几项关键的研究空白。&lt;h4&gt;结论&lt;/h4&gt;该综述提供了一种对MMFMs进行系统性研究的方法，旨在促进机器学习领域的进一步发展。它还为研究人员提供了关于如何改进多模态基础模型的解释性的指导。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLM）的可解释方法在多模态框架下的适应以及单模态语言模型与跨模式系统之间的机制性差异是研究重点，通过比较和分类不同架构中的见解来填补LLM和MMFM之间存在的理解差距。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of foundation models has transformed machine learning research,prompting efforts to uncover their inner workings and develop more efficientand reliable applications for better control. While significant progress hasbeen made in interpreting Large Language Models (LLMs), multimodal foundationmodels (MMFMs) - such as contrastive vision-language models, generativevision-language models, and text-to-image models - pose unique interpretabilitychallenges beyond unimodal frameworks. Despite initial studies, a substantialgap remains between the interpretability of LLMs and MMFMs. This surveyexplores two key aspects: (1) the adaptation of LLM interpretability methods tomultimodal models and (2) understanding the mechanistic differences betweenunimodal language models and crossmodal systems. By systematically reviewingcurrent MMFM analysis techniques, we propose a structured taxonomy ofinterpretability methods, compare insights across unimodal and multimodalarchitectures, and highlight critical research gaps.</description>
      <author>example@mail.com (Zihao Lin, Samyadeep Basu, Mohammad Beigi, Varun Manjunatha, Ryan A. Rossi, Zichao Wang, Yufan Zhou, Sriram Balasubramanian, Arman Zarei, Keivan Rezaei, Ying Shen, Barry Menglong Yao, Zhiyang Xu, Qin Liu, Yuxiang Zhang, Yan Sun, Shilong Liu, Li Shen, Hongxuan Li, Soheil Feizi, Lifu Huang)</author>
      <guid isPermaLink="false">2502.17516v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Learning multi-phase flow and transport in fractured porous media with auto-regressive and recurrent graph neural networks</title>
      <link>http://arxiv.org/abs/2502.17512v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文提出了一种使用图神经网络（GNN）来模拟多相流和输运动力学的新方法，这种方法在处理复杂裂缝网络时比传统的方法更有效。&lt;h4&gt;背景&lt;/h4&gt;过去三十年间，用于解决裂隙多孔介质中多重流动和传输过程建模的计算方法和仿真框架层出不穷。其中共形网格方法因其高精度而备受青睐，但需要极细的网格划分，在大型或复杂的裂缝网络中变得不可行。&lt;h4&gt;目的&lt;/h4&gt;提出基于图神经网络的方法来模拟复杂裂隙多孔介质中的多相流动和传输动力学。&lt;h4&gt;方法&lt;/h4&gt;提出了两种深度学习架构：一种是GNN（图神经网络），另一种是递归GNN。这两种网络均采用两阶段训练策略：自回归一步滚动，然后通过完整的真实地面序列进行微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示了两阶段训练的有效性，在测试阶段的自回归模型滚动过程中减少了误差积累；两种GNN都展示了对未见过裂缝结构的良好泛化能力。递归GNN在预测长期序列方面显示出显著优势，尤其是在压力序列预测中表现更佳。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的方法为复杂裂隙多孔介质中的流动和传输过程建模提供了一种有效的新途径，特别是对于大型或复杂的裂缝网络来说更是如此。&lt;h4&gt;翻译&lt;/h4&gt;在过去三十年间，多种计算方法和模拟框架被开发出来以应对在裂隙性多孔介质中多重流体流动与输运过程的复杂建模挑战。共形网格技术通过确保计算网格与裂缝表面对齐而被视为最准确的方法之一。然而，这类方法需要极细的网格划分，在处理大型或复杂的裂缝网络时变得不切实际。在这项工作中，我们提出使用图神经网络（GNN）来学习裂隙性多孔介质中的复杂多相流动和输运动力学现象。鉴于嵌入离散裂缝模型（EDFM）所导致计算网格的非结构化特性，GNN非常适合这一任务。我们提出了两种深度学习架构：一种是GNN，另一种是递归GNN。这两个网络都遵循两阶段训练策略：首先是自回归一步滚动，然后通过完整的真实地面序列进行微调。结果显示，两阶段训练方法在测试期间的模型自回归展开时有效减少了误差积累。我们的研究发现表明，两种GNN都能很好地泛化到未见过的裂缝结构，并且它们在预测饱和度序列方面表现相当，而递归GNN则略微优于GNN在压力序列预测中的性能。尽管第二阶段训练对GNN模型有益，但其对递归GNN的影响不那么显著。最后，测试了两种GNN的时间外推性能。递归GNN以更高的准确性明显超越了GNN，在长期序列的预测中展示了优越的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In the past three decades, a wide array of computational methodologies andsimulation frameworks has emerged to address the complexities of modelingmulti-phase flow and transport processes in fractured porous media. Theconformal mesh approaches which explicitly align the computational grid withfracture surfaces are considered by many to be the most accurate. However, suchmethods require excessive fine-scale meshing, rendering them impractical forlarge or complex fracture networks. In this work, we propose to learn thecomplex multi-phase flow and transport dynamics in fractured porous media withgraph neural networks (GNN). GNNs are well suited for this task due to theunstructured topology of the computation grid resulting from the EmbeddedDiscrete Fracture Model (EDFM) discretization. We propose two deep learningarchitectures, a GNN and a recurrent GNN. Both networks follow a two-stagetraining strategy: an autoregressive one step roll-out, followed by afine-tuning step where the model is supervised using the whole ground-truthsequence. We demonstrate that the two-stage training approach is effective inmitigating error accumulation during autoregressive model rollouts in thetesting phase. Our findings indicate that both GNNs generalize well to unseenfracture realizations, with comparable performance in forecasting saturationsequences, and slightly better performance for the recurrent GNN in predictingpressure sequences. While the second stage of training proved to be beneficialfor the GNN model, its impact on the recurrent GNN model was less pronounced.Finally, the performance of both GNNs for temporal extrapolation is tested. Therecurrent GNN significantly outperformed the GNN in terms of accuracy, therebyunderscoring its superior capability in predicting long sequences.</description>
      <author>example@mail.com (Mohammed Al Kobaisi, Wenjuan Zhang, Waleed Diab, Hadi Hajibeygi)</author>
      <guid isPermaLink="false">2502.17512v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Supervised Reward Inference</title>
      <link>http://arxiv.org/abs/2502.18447v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  16 pages, 4 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;文章提出了一种基于监督学习的框架，用于从人类行为中推断奖励函数。这种方法能够在温和假设下实现渐进贝叶斯最优，并且通过模拟机器人操作任务实验验证了其有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的奖励推理方法通常假设人类提供的演示遵循特定的行为模型。然而，在实际情况下，人们可能表现出各种类型的次优行为来指示他们的目标。&lt;h4&gt;目的&lt;/h4&gt;提出一种从任意类型的人类行为中推断奖励函数的统一框架，并展示这种方法在温和假设下的渐进贝叶斯最优性。&lt;h4&gt;方法&lt;/h4&gt;利用监督学习技术建立一个模型，该模型可以从广泛的、可能是次优的行为演示中推断出奖励函数。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法能够从各种各样的次优行为演示中有效地推断出奖励函数。&lt;h4&gt;结论&lt;/h4&gt;基于监督学习的框架为从人类行为数据中推断奖励提供了一种灵活而有效的方法，并且在模拟任务中的表现证明了这一点。&lt;h4&gt;翻译&lt;/h4&gt;现有方法假设人类提供的示例符合特定的行为模型。然而，实际情况是人们通过各种各样的行为来指示目标，这些行为可能由于计划或执行不当而不理想，也可能是为了传达目的而不是实现它们。我们提出监督学习能够提供一个统一的框架从任何类型的人类行为中推断奖励函数，并展示了在温和假设下的渐进贝叶斯最优性。模拟机器人操作任务实验表明这种方法可以从各种次优演示中有效推断出奖励。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing approaches to reward inference from behavior typically assume thathumans provide demonstrations according to specific models of behavior.However, humans often indicate their goals through a wide range of behaviors,from actions that are suboptimal due to poor planning or execution to behaviorswhich are intended to communicate goals rather than achieve them. We proposethat supervised learning offers a unified framework to infer reward functionsfrom any class of behavior, and show that such an approach is asymptoticallyBayes-optimal under mild assumptions. Experiments on simulated roboticmanipulation tasks show that our method can efficiently infer rewards from awide variety of arbitrarily suboptimal demonstrations.</description>
      <author>example@mail.com (Will Schwarzer, Jordan Schneider, Philip S. Thomas, Scott Niekum)</author>
      <guid isPermaLink="false">2502.18447v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CRESSim-MPM: A Material Point Method Library for Surgical Soft Body Simulation with Cutting and Suturing</title>
      <link>http://arxiv.org/abs/2502.18437v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 12 figures, submitted to IEEE/RSJ International Conference  on Intelligent Robots and Systems (IROS) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的手术模拟平台CRESSim-MPM，使用材料点法（MPM）来克服现有平台上在软组织切割和缝合等复杂行为的模拟问题。&lt;h4&gt;背景&lt;/h4&gt;现有的手术仿真平台难以精确地模拟软体组织的行为，特别是像切割、缝合这样复杂的操作。这主要是由于有限元方法(FEM)在这种情况下处理材料分裂等问题时存在困难。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更准确模拟软体组织复杂行为的新型物理引擎，以便更好地训练机器学习模型进行手术辅助。&lt;h4&gt;方法&lt;/h4&gt;采用材料点法（MPM）来应对FEM在处理软质物体分割和缝合问题上的不足。提出了新的刚性几何形状以及适用于这种场景下的软硬接触方法。开发了CRESSim-MPM, 一个加速的GPU库，集成了多种MPM求解器，并加入了用于切割和缝合等手术操作的物理引擎。&lt;h4&gt;主要发现&lt;/h4&gt;通过在实时模拟中展示其能力来验证该平台的有效性，包括对软组织进行切割和缝合。同时进行了不同数量颗粒仿真时各MPM求解器性能评估。&lt;h4&gt;结论&lt;/h4&gt;CRESSim-MPM为复杂的手术操作提供了更好的物理建模方法，并且能够有效地与Unity集成，便于现有项目的扩展和应用。&lt;h4&gt;翻译&lt;/h4&gt;最近的研究集中于开发用于训练机器学习代理或模型的合成数据的外科模拟平台。尽管现有的平台在刚体操纵和软体变形方面表现出色，但它们难以精确地模拟诸如切割和缝合等更复杂的软体行为。一个重要挑战在于使用有限元法（FEM）建模软体分裂问题，在当前平台上这种方法是主要手段。当需要处理手术中的两向缝线接触时，这会进一步复杂化。在本研究中，我们采用材料点法（MPM）进行此类困难模拟，并提出了用于该方法的新刚性几何形状和软硬接触方法。我们引入了CRESSim-MPM，这是一个基于GPU加速的MPM库，集成了多个MPM求解器并包含了手术几何图形以支持切割和缝合操作，作为专门用于外科应用的物理引擎。它还被集成到了Unity中，只需对现有项目进行最小改动即可实现软体模拟。我们展示了该仿真器在实时模拟软组织切割和缝合方面的能力，并提供了不同数量颗粒时各种MPM求解器性能评估的初步结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A number of recent studies have focused on developing surgical simulationplatforms to train machine learning (ML) agents or models with synthetic datafor surgical assistance. While existing platforms excel at tasks such as rigidbody manipulation and soft body deformation, they struggle to simulate morecomplex soft body behaviors like cutting and suturing. A key challenge lies inmodeling soft body fracture and splitting using the finite-element method(FEM), which is the predominant approach in current platforms. Additionally,the two-way suture needle/thread contact inside a soft body is furthercomplicated when using FEM. In this work, we use the material point method(MPM) for such challenging simulations and propose new rigid geometries andsoft-rigid contact methods specifically designed for them. We introduceCRESSim-MPM, a GPU-accelerated MPM library that integrates multiple MPM solversand incorporates surgical geometries for cutting and suturing, serving as aspecialized physics engine for surgical applications. It is further integratedinto Unity, requiring minimal modifications to existing projects for soft bodysimulation. We demonstrate the simulator's capabilities in real-time simulationof cutting and suturing on soft tissue and provide an initial performanceevaluation of different MPM solvers when simulating varying numbers ofparticles.</description>
      <author>example@mail.com (Yafei Ou, Mahdi Tavakoli)</author>
      <guid isPermaLink="false">2502.18437v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Stretchable Capacitive and Resistive Strain Sensors: Accessible Manufacturing Using Direct Ink Writing</title>
      <link>http://arxiv.org/abs/2502.18363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 8 figures, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;随着机器人技术的发展，软结构、类人形态和复杂任务的集成变得越来越重要，因此研发出可靠测量触觉和本体感觉数据的同时兼具形变适应性、可拉伸性和可调整性的柔性传感器至关重要。&lt;h4&gt;背景&lt;/h4&gt;当前许多用于制造可拉伸传感器的方法仅限于单一配置的设计，限制了设计灵活性。研究人员正在探索多样化的转换原理以及规模生产和多功能生产技术以解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于打印的灵活、定制化且易于实现的可拉伸传感器制造方法。&lt;h4&gt;方法&lt;/h4&gt;使用商用3D打印机与自定义喷头集成，可以将导电墨水直接书写在固化硅胶基底上。通过堆叠托盘支持的逐层制造工艺，在硅胶矩阵内沉积多层液态导电墨水。&lt;h4&gt;主要发现&lt;/h4&gt;展示的方法具有高度的设计灵活性，并且制造出和评估了包括电容式和电阻式的应变传感器形态。实验表征显示，电容式应变传感器具备高线性度（R^2 = 0.99）、接近理论极限的高灵敏度（GF = 0.95）、极小的滞后效应(DH = 1.36%)以及高达550%的最大拉伸能力。&lt;h4&gt;结论&lt;/h4&gt;该方法提供了一种新的制造方式，能够满足高度可定制化的需求，并且生产出的传感器性能达到当前先进水平。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人技术向集成软结构、类人形态和复杂任务的发展，柔软且高度可拉伸的力学转换器变得越来越重要。为了在确保形状适应性的同时可靠地测量触觉和本体感觉数据，研究人员正在探索多样化的转换原理以及规模生产和多功能生产技术。然而，许多当前用于制造可拉伸传感器的方法仅限于单一配置的设计，限制了设计灵活性。在这里，我们提出了一种基于打印的灵活、定制化且易于实现的新方法来制作可拉伸传感器。我们的方法采用商用3D打印机与自定义喷头集成，可以将导电墨水直接书写在固化硅胶基底上。通过堆叠托盘支持的逐层制造工艺，在硅胶矩阵内沉积多层液态导电墨水。为了展示该方法的设计灵活性，我们制作并评估了包括电容式和电阻式的应变传感器形态。实验表征显示，电容式应变传感器具有高线性度（R^2 = 0.99）、接近理论极限的高灵敏度（GF = 0.95）、极小的滞后效应(DH = 1.36%)以及高达550%的最大拉伸能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robotics advances toward integrating soft structures, anthropomorphicshapes, and complex tasks, soft and highly stretchable mechanotransducers arebecoming essential. To reliably measure tactile and proprioceptive data whileensuring shape conformability, stretchability, and adaptability, researchershave explored diverse transduction principles alongside scalable and versatilemanufacturing techniques. Nonetheless, many current methods for stretchablesensors are designed to produce a single sensor configuration, thereby limitingdesign flexibility. Here, we present an accessible, flexible, printing-basedfabrication approach for customizable, stretchable sensors. Our method employsa custom-built printhead integrated with a commercial 3D printer to enabledirect ink writing (DIW) of conductive ink onto cured silicone substrates. Alayer-wise fabrication process, facilitated by stackable trays, allows for thedeposition of multiple liquid conductive ink layers within a silicone matrix.To demonstrate the method's capacity for high design flexibility, we fabricateand evaluate both capacitive and resistive strain sensor morphologies.Experimental characterization showed that the capacitive strain sensorpossesses high linearity (R^2 = 0.99), high sensitivity near the 1.0theoretical limit (GF = 0.95), minimal hysteresis (DH = 1.36%), and largestretchability (550%), comparable to state-of-the-art stretchable strainsensors reported in the literature.</description>
      <author>example@mail.com (Lukas Cha, Sonja Groß, Shuai Mao, Tim Braun, Sami Haddadin, Liang He)</author>
      <guid isPermaLink="false">2502.18363v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>MegaLoc: One Retrieval to Place Them All</title>
      <link>http://arxiv.org/abs/2502.17237v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了MegaLoc模型，该模型在多个计算机视觉任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;从给定查询检索来自相同位置的图像对于多项计算机视觉任务（如视觉地方识别、地标检索、视觉定位、3D重建和SLAM）至关重要。然而，现有解决方案只能针对特定的任务设计，并且当需求变化或遇到分布外数据时会失败。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的模型MegaLoc，该模型能够同时在多个计算机视觉任务中表现良好。&lt;h4&gt;方法&lt;/h4&gt;结合了多种现有的方法、训练技术以及数据集来训练一个检索模型MegaLoc。&lt;h4&gt;主要发现&lt;/h4&gt;{'1': 'MegaLoc在大量视觉地方识别数据集中达到了最先进水平。', '2': 'MegaLoc在常用的地标检索数据集中取得了令人印象深刻的结果。', '3': 'MegaLoc为LaMAR数据集上的视觉定位设定了新的最先进的标准，只需将检索方法替换到现有定位管道中即可。'}&lt;h4&gt;结论&lt;/h4&gt;提出的模型MegaLoc展示了其跨多个任务的优越性能，并且源代码已公开发布在GitHub上。&lt;h4&gt;翻译&lt;/h4&gt;从给定查询检索来自相同位置的图像对多项计算机视觉任务（包括视觉地方识别、地标检索、视觉定位、3D重建和SLAM）至关重要。然而，现有解决方案设计专门用于特定的任务，在需求变化或遇到分布外数据时会失效。本文结合了多种现有的方法、训练技术以及数据集来训练一个名为MegaLoc的检索模型，该模型在多个任务上表现出色。研究发现，MegaLoc（1）在大量视觉地方识别的数据集中达到了最先进水平；（2）在常用的地标检索数据集中取得了令人印象深刻的结果；（3）为LaMAR数据集上的视觉定位设定了新的最先进的标准，只需将检索方法替换到现有定位管道中即可。MegaLoc的代码可在https://github.com/gmberton/MegaLoc 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gmberton/megaloc&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving images from the same location as a given query is an importantcomponent of multiple computer vision tasks, like Visual Place Recognition,Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,existing solutions are built to specifically work for one of these tasks, andare known to fail when the requirements slightly change or when they meetout-of-distribution data. In this paper we combine a variety of existingmethods, training techniques, and datasets to train a retrieval model, calledMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)achieves state of the art on a large number of Visual Place Recognitiondatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)sets a new state of the art for Visual Localization on the LaMAR datasets,where we only changed the retrieval method to the existing localizationpipeline. The code for MegaLoc is available athttps://github.com/gmberton/MegaLoc</description>
      <author>example@mail.com (Gabriele Berton, Carlo Masone)</author>
      <guid isPermaLink="false">2502.17237v2</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Self-Supervised Data Generation for Precision Agriculture: Blending Simulated Environments with Real Imagery</title>
      <link>http://arxiv.org/abs/2502.18320v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Presented at 2024 IEEE 20th International Conference on Automation  Science and Engineering (CASE)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;在精准农业中，由于缺乏标注数据和显著的协变量变化，训练机器学习模型面临独特的挑战。本文提出了一种基于Unity引擎的葡萄园模拟器系统，该系统通过剪切粘贴技术生成逼真的合成图像及标签，以解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;精准农业领域存在数据标注不足的问题，并且环境动态性导致农作物外观随时间变化，这使得训练机器学习模型变得困难。缺乏多样化的数据限制了算法的性能和适应能力。&lt;h4&gt;目的&lt;/h4&gt;提出一种生成真实感合成图像的方法来克服这些挑战，以便更好地训练用于监测和管理农业活动的检测算法。&lt;h4&gt;方法&lt;/h4&gt;利用Unity引擎创建一个虚拟葡萄园模拟器，并采用几何一致性剪切粘贴技术从合成环境中生成精确的照片级现实图像及标签。该系统可以自动生成各种视角和光照条件下的数据样本。&lt;h4&gt;主要发现&lt;/h4&gt;通过在提子栽培训练最先进的检测算法上应用这种方法，取得了显著的性能改进，表明所提出的组合技术可以有效增强模型的学习能力和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;基于合成数据的方法为精准农业提供了新的可能性。它不仅可以生成大量高质量的数据以克服标注不足的问题，而且能够模拟各种环境条件下的图像变化。此外，该方法易于自动化实施，对于其在实际农业生产中的应用非常重要。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于一种用于解决精准农业中由于缺乏标记数据和显著协变量变化所带来的挑战的方法介绍。通过使用基于Unity引擎的葡萄园仿真器，并利用剪切粘贴技术生成逼真的合成图像与标签，可以训练检测算法以应对不同的视角和光照条件。这种方法在提子栽培场景下的应用表明它能够极大地改进最先进的检测模型性能。该方法由于其易于自动化的特性而可能被广泛采纳于实际农业生产实践中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/CASE59546.2024.10711594&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In precision agriculture, the scarcity of labeled data and significantcovariate shifts pose unique challenges for training machine learning models.This scarcity is particularly problematic due to the dynamic nature of theenvironment and the evolving appearance of agricultural subjects as livingthings. We propose a novel system for generating realistic synthetic data toaddress these challenges. Utilizing a vineyard simulator based on the Unityengine, our system employs a cut-and-paste technique with geometricalconsistency considerations to produce accurate photo-realistic images andlabels from synthetic environments to train detection algorithms. This approachgenerates diverse data samples across various viewpoints and lightingconditions. We demonstrate considerable performance improvements in training astate-of-the-art detector by applying our method to table grapes cultivation.The combination of techniques can be easily automated, an increasinglyimportant consideration for adoption in agricultural practice.</description>
      <author>example@mail.com (Leonardo Saraceni, Ionut Marian Motoi, Daniele Nardi, Thomas Alessandro Ciarfuglia)</author>
      <guid isPermaLink="false">2502.18320v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>RSSI-Based Localization Utilizing Antenna Radiation Pattern And Biased CRLB Analysis</title>
      <link>http://arxiv.org/abs/2502.18311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本文提出了一种新颖的室内定位方法，利用单天线系统中接收信号强度指示（RSSI）测量来获取天线辐射模式特征。通过旋转天线或重新配置其辐射模式，我们推导出了极大似然估计算法，该算法实现了接近克拉美罗下界（CRLB）的近优化定位精度。&lt;h4&gt;背景&lt;/h4&gt;现有的室内定位技术往往依赖于多天线系统，难以在保证高精度的同时保持系统的简洁性。本文通过研究单天线辐射模式特性，试图找到一种简化且高效的解决方案。&lt;h4&gt;目的&lt;/h4&gt;提出一个利用单天线辐射特性的新型室内定位方法，并验证其在复杂环境下的性能表现。&lt;h4&gt;方法&lt;/h4&gt;设计并实施了一种基于RSSI测量的极大似然估计（MLE）算法；通过旋转或重新配置天线的辐射模式来改善信号质量，减少位置误差。此外，提出了一种两步测量策略以消除对接收天线模式依赖的需求。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，随着信噪比、天线旋转次数和辐射模式变化的增加，估计精度有显著提高；模拟结果验证了该方法在室内机器人跟踪应用中的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文所提出的定位技术能够有效解决室内环境下的机器人追踪问题，同时保持系统结构简单。这一方法为未来基于单天线系统的高精度室内定位提供了新的思路和依据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents a novel indoor positioning approach that leveragesantenna radiation pattern characteristics through Received Signal StrengthIndication (RSSI) measurements in a single-antenna system. By rotating theantenna or reconfiguring its radiation pattern, we derive a maximum likelihoodestimation (MLE) algorithm that achieves near-optimal positioning accuracyapproaching the Cramer-Rao lower bound (CRLB). Through theoretical analysis, weestablish three fundamental theorems characterizing the estimation accuracybounds and demonstrating how performance improves with increasedsignal-to-noise ratio, antenna rotation count, and radiation patternvariations. Additionally, we propose a two-position measurement strategy thateliminates dependence on receiving antenna patterns. Simulation resultsvalidate that our approach provides an effective solution for indoor robottracking applications where both accuracy and system simplicity are essentialconsiderations.</description>
      <author>example@mail.com (Zhisheng Rong, Wenzhi Liu, Xiayue Liu, Zhixiang Xu, Yufei Jiang)</author>
      <guid isPermaLink="false">2502.18311v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Pre-Surgical Planner for Robot-Assisted Vitreoretinal Surgery: Integrating Eye Posture, Robot Position and Insertion Point</title>
      <link>http://arxiv.org/abs/2502.18230v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种优化框架，用于调整眼科手术中眼睛的倾斜角度和机器人位置，以实现不同的患者目标区域。&lt;h4&gt;背景&lt;/h4&gt;最近开发了几种辅助视网膜手术的机器人系统。这些系统的准确性受限于其工作体积，并且通过外科显微镜看到的视野有限。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有手术机器人的限制，优化眼睛姿态和机器人定位以扩大可达性并减少重置准备过程的可能性。&lt;h4&gt;方法&lt;/h4&gt;研究使用了一种可以调整的眼模型来验证所提出的框架的有效性。评估了该工作流程在不同轴向的误差，并分析了可能的误差来源。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，优化后的框架能够以平均0.13度（Y轴旋转），-1.40度（X轴旋转）和1.80毫米（深度Z方向）的误差达到目标位置。这些误差在临床上是可接受的。&lt;h4&gt;结论&lt;/h4&gt;该优化框架可以提高手术机器人系统的目标可达性，并减少重置准备过程的可能性，具有显著的实际应用价值。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已经完整地进行了中文翻译并以JSON格式呈现出来。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Several robotic frameworks have been recently developed to assist ophthalmicsurgeons in performing complex vitreoretinal procedures such as subretinalinjection of advanced therapeutics. These surgical robots show promisingcapabilities; however, most of them have to limit their working volume toachieve maximum accuracy. Moreover, the visible area seen through the surgicalmicroscope is limited and solely depends on the eye posture. If the eyeposture, trocar position, and robot configuration are not correctly arranged,the instrument may not reach the target position, and the preparation will haveto be redone. Therefore, this paper proposes the optimization framework of theeye tilting and the robot positioning to reach various target areas fordifferent patients. Our method was validated with an adjustable phantom eyemodel, and the error of this workflow was 0.13 +/- 1.65 deg (rotational jointaround Y axis), -1.40 +/- 1.13 deg (around X axis), and 1.80 +/- 1.51 mm(depth, Z). The potential error sources are also analyzed in the discussionsection.</description>
      <author>example@mail.com (Satoshi Inagaki, Alireza Alikhani, Nassir Navab, Peter C. Issa, M. Ali Nasseri)</author>
      <guid isPermaLink="false">2502.18230v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>iTrash: Incentivized Token Rewards for Automated Sorting and Handling</title>
      <link>http://arxiv.org/abs/2502.18161v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Article submitted to IROS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为iTrash的智能垃圾桶，用于提高小型办公室环境中的回收率。&lt;h4&gt;背景&lt;/h4&gt;随着机器人系统的自主性增强，它们越来越多地被应用于小空间和办公环境中进行自动化任务。&lt;h4&gt;目的&lt;/h4&gt;通过引入一种新型智能垃圾桶iTrash来改善小型办公区域内的垃圾分类效率，并收集有关用户行为及垃圾箱使用模式的有价值数据。&lt;h4&gt;方法&lt;/h4&gt;进行了为期5天的实验以评估iTrash相较于传统垃圾桶的优势，发现其回收率提高了30%以上。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，使用智能垃圾桶不仅能提高回收利用率，还能提供诸如用户行为和垃圾桶使用情况等重要信息，这些是普通垃圾桶无法获取的数据。利用这些数据可以预测并优化某些任务。&lt;h4&gt;结论&lt;/h4&gt;探索了通过区块链技术创建经济激励机制以促进回收活动的可能性，采用了节约型付费模式（Save-as-you-Throw, SAYT）。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人系统的自主性增强，它们越来越多地被应用于小空间和办公环境中进行自动化任务。本文提出了一种名为iTrash的智能垃圾桶，用于提高小型办公室环境中的回收率。进行了为期5天的实验以评估iTrash相较于传统垃圾桶的优势，发现其回收率提高了30%以上。研究结果表明，使用这种新型智能垃圾桶不仅能提高回收利用率，还能提供诸如用户行为和垃圾箱使用情况等重要信息，这些是普通垃圾桶无法获取的数据。利用这些数据可以预测并优化某些任务。最后，本文探讨了通过区块链技术创建经济激励机制以促进回收活动的可能性，并采用了节约型付费模式（Save-as-you-Throw, SAYT）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robotic systems (RS) become more autonomous, they are becomingincreasingly used in small spaces and offices to automate tasks such ascleaning, infrastructure maintenance, or resource management. In this paper, wepropose iTrash, an intelligent trashcan that aims to improve recycling rates insmall office spaces. For that, we ran a 5 day experiment and found that iTrashcan produce an efficiency increase of more than 30% compared to traditionaltrashcans. The findings derived from this work, point to the fact that usingiTrash not only increase recyclying rates, but also provides valuable data suchas users behaviour or bin usage patterns, which cannot be taken from a normaltrashcan. This information can be used to predict and optimize some tasks inthese spaces. Finally, we explored the potential of using blockchain technologyto create economic incentives for recycling, following a Save-as-you-Throw(SAYT) model.</description>
      <author>example@mail.com (Pablo Ortega, Eduardo Castelló Ferrer)</author>
      <guid isPermaLink="false">2502.18161v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>A Real-time Spatio-Temporal Trajectory Planner for Autonomous Vehicles with Semantic Graph Optimization</title>
      <link>http://arxiv.org/abs/2502.18151v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been accepted for publication in IEEE Robotics and  Automation Letters (RA-L). The final published version is available in IEEE  Xplore (DOI: 10.1109/LRA.2024.3504239)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图优化的空间时间轨迹规划方法，用于实时为自主车辆在复杂城市环境中计划安全和可行的路径。&lt;h4&gt;背景&lt;/h4&gt;在复杂的城市环境中，通过充分利用感知信息实现实时的安全且可行的自动驾驶车辆轨迹规划是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;设计一种高效的多模式感知信息提取及快速生成可行路径的方法。&lt;h4&gt;方法&lt;/h4&gt;通过构建语义空间时间图并通过静态和动态障碍物的分离处理有效提取感知模块的多模态信息，然后基于语义空间时间超图进行稀疏图优化快速生成轨迹。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法能够有效地处理复杂的城区公共道路场景，并能在实时运行中表现出色。&lt;h4&gt;结论&lt;/h4&gt;该研究方法在复杂的城市环境中的性能得到了验证。此外还将发布代码供科研社区使用以支持基准测试。&lt;h4&gt;翻译&lt;/h4&gt;计划在复杂城市环境中，为自主车辆在实时内利用感知信息规划出安全且可行的轨迹是一项挑战。本文提出了一种基于图优化的空间时间轨迹规划方法。该方法通过构建语义空间时间图，并对静态和动态障碍物进行分离处理以有效提取感知模块的多模态信息，然后通过基于语义空间时间超图的稀疏图优化快速生成可行路径。大量实验表明所提出的方法能够有效地应对复杂的城区公共道路场景并能实时运行。我们也将发布代码来支持科研社区的基准测试研究。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2024.3504239&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Planning a safe and feasible trajectory for autonomous vehicles in real-timeby fully utilizing perceptual information in complex urban environments ischallenging. In this paper, we propose a spatio-temporal trajectory planningmethod based on graph optimization. It efficiently extracts the multi-modalinformation of the perception module by constructing a semantic spatio-temporalmap through separation processing of static and dynamic obstacles, and thenquickly generates feasible trajectories via sparse graph optimization based ona semantic spatio-temporal hypergraph. Extensive experiments have proven thatthe proposed method can effectively handle complex urban public road scenariosand perform in real time. We will also release our codes to accommodatebenchmarking for the research community</description>
      <author>example@mail.com (Shan He, Yalong Ma, Tao Song, Yongzhi Jiang, Xinkai Wu)</author>
      <guid isPermaLink="false">2502.18151v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Enhancing Reusability of Learned Skills for Robot Manipulation via Gaze and Bottleneck</title>
      <link>http://arxiv.org/abs/2502.18121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的算法GazeBot，该算法通过利用注视信息和运动瓶颈来提高机器人操作的可重用性。&lt;h4&gt;背景&lt;/h4&gt;虽然深度学习的进步使得复制人类远程操作机器人的灵巧度变得越来越可行，但是将这些获得的能力推广到未见过的情景中仍然是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的算法GazeBot，以克服现有模仿学习方法在泛化性能方面的限制。&lt;h4&gt;方法&lt;/h4&gt;利用注视信息和运动瓶颈作为关键特征来提高机器人的操作技能的可重用性。通过提供带有注视数据的演示数据集，整个训练过程是完全基于数据驱动的。&lt;h4&gt;主要发现&lt;/h4&gt;GazeBot算法实现了比现有模仿学习方法更高的泛化性能，并且不牺牲其灵巧性和反应能力。&lt;h4&gt;结论&lt;/h4&gt;提供的视频和代码可以在https://crumbyrobotics.github.io/gazebot获取。&lt;h4&gt;翻译&lt;/h4&gt;自主代理能够进行多样化的物体操作，应该能够以高可重用性的方式获得广泛的操纵技能。尽管深度学习的进步使得复制人类远程操作的灵巧度在机器人中变得越来越可行，但将这些获得的能力推广到未见过的情景中仍然是一个挑战。在这项研究中，我们提出了一种新的算法GazeBot（基于注视信息和运动瓶颈感知的机器人操作），它即使当物体位置和末端执行器姿态与提供的演示不同时，也能实现所学动作的高可重用性。通过利用注视信息和运动瓶颈作为进行物体操作的关键特征，GazeBot在泛化性能方面相比当前最先进的模仿学习方法具有显著优势，同时不牺牲其灵巧性和反应能力。此外，在提供带有注视数据的演示数据集之后，GazeBot的训练过程完全基于数据驱动。视频和代码可在https://crumbyrobotics.github.io/gazebot获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous agents capable of diverse object manipulations should be able toacquire a wide range of manipulation skills with high reusability. Althoughadvances in deep learning have made it increasingly feasible to replicate thedexterity of human teleoperation in robots, generalizing these acquired skillsto previously unseen scenarios remains a significant challenge. In this study,we propose a novel algorithm, Gaze-based Bottleneck-aware Robot Manipulation(GazeBot), which enables high reusability of the learned motions even when theobject positions and end-effector poses differ from those in the provideddemonstrations. By leveraging gaze information and motion bottlenecks, bothcrucial features for object manipulation, GazeBot achieves high generalizationperformance compared with state-of-the-art imitation learning methods, withoutsacrificing its dexterity and reactivity. Furthermore, the training process ofGazeBot is entirely data-driven once a demonstration dataset with gaze data isprovided. Videos and code are available athttps://crumbyrobotics.github.io/gazebot.</description>
      <author>example@mail.com (Ryo Takizawa, Izumi Karino, Koki Nakagawa, Yoshiyuki Ohmura, Yasuo Kuniyoshi)</author>
      <guid isPermaLink="false">2502.18121v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>MRBTP: Efficient Multi-Robot Behavior Tree Planning and Collaboration</title>
      <link>http://arxiv.org/abs/2502.18072v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;该论文提出了一种名为MRBTP的多机器人行为树规划算法，适用于同质和异质机器人群体，并具有理论上的完整性和正确性保证。&lt;h4&gt;背景&lt;/h4&gt;在机器人领域，多机器人任务规划与协作是非常关键的问题。尽管行为树（BT）作为一种流行控制架构被广泛应用于单个机器人的规划中，但为多机器人开发有效的BT规划算法仍然面临复杂性挑战。&lt;h4&gt;目的&lt;/h4&gt;为了应对协调不同动作空间的复杂性问题，论文旨在设计一种适用于同质和异质多机器人群体的行为树规划方法，并探讨如何利用大型语言模型进一步提高其效率。&lt;h4&gt;方法&lt;/h4&gt;MRBTP算法通过跨树扩展实现异构动作之间的协调；对于同类动作，则保留各行为树间的备份结构以确保鲁棒性及避免冗余执行。此外，当有大型语言模型可用时，MRBTP可以额外使用此插件来预规划与目标相关的长时期子树。&lt;h4&gt;主要发现&lt;/h4&gt;实验评估表明，在仓库管理和日常服务场景下，MRBTP算法表现出良好的稳健性和执行效率，并且借助于预先训练的语言模型生成的任务特定子树能显著提高其计划速度和协作效率。&lt;h4&gt;结论&lt;/h4&gt;论文提出了一种适用于同质与异质机器人群体的多机器人行为树规划方法MRBTP，并证明了该方法在实际应用中的有效性和优越性。此外，通过引入大型语言模型插件进一步提升了算法的速度和效果。&lt;h4&gt;翻译&lt;/h4&gt;Multi-robot task planning and collaboration are critical challenges in robotics. While Behavior Trees (BTs) have been established as a popular control architecture for single robots, the development of effective multi-robot BT planning algorithms remains challenging due to the complexity of coordinating diverse action spaces. The paper proposes the Multi-Robot Behavior Tree Planning (MRBTP) algorithm with theoretical guarantees of soundness and completeness. MRBTP features cross-tree expansion to coordinate heterogeneous actions across different BTs for achieving team goals, and retains backup structures among homogeneous actions to ensure robustness and prevent redundant execution through intention sharing. Additionally, when available, a plugin using Large Language Models (LLMs) can pre-plan goal-related actions forming long-horizon subtrees significantly enhancing planning speed and collaboration efficiency. Evaluations in warehouse management and everyday service scenarios demonstrate MRBTP's robustness and execution efficiency under varying settings, along with the ability of the pre-trained LLM to generate effective task-specific subtrees for MRBTP.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-robot task planning and collaboration are critical challenges inrobotics. While Behavior Trees (BTs) have been established as a popular controlarchitecture and are plannable for a single robot, the development of effectivemulti-robot BT planning algorithms remains challenging due to the complexity ofcoordinating diverse action spaces. We propose the Multi-Robot Behavior TreePlanning (MRBTP) algorithm, with theoretical guarantees of both soundness andcompleteness. MRBTP features cross-tree expansion to coordinate heterogeneousactions across different BTs to achieve the team's goal. For homogeneousactions, we retain backup structures among BTs to ensure robustness and preventredundant execution through intention sharing. While MRBTP is capable ofgenerating BTs for both homogeneous and heterogeneous robot teams, itsefficiency can be further improved. We then propose an optional plugin forMRBTP when Large Language Models (LLMs) are available to reason goal-relatedactions for each robot. These relevant actions can be pre-planned to formlong-horizon subtrees, significantly enhancing the planning speed andcollaboration efficiency of MRBTP. We evaluate our algorithm in warehousemanagement and everyday service scenarios. Results demonstrate MRBTP'srobustness and execution efficiency under varying settings, as well as theability of the pre-trained LLM to generate effective task-specific subtrees forMRBTP.</description>
      <author>example@mail.com (Yishuai Cai, Xinglin Chen, Zhongxuan Cai, Yunxin Mao, Minglong Li, Wenjing Yang, Ji Wang)</author>
      <guid isPermaLink="false">2502.18072v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>HEROS-GAN: Honed-Energy Regularized and Optimal Supervised GAN for Enhancing Accuracy and Range of Low-Cost Accelerometers</title>
      <link>http://arxiv.org/abs/2502.18064v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  AAAI Oral; AI for Sensors; Generative Deep Learning&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的生成对抗网络HEROS-GAN，用于改善低成本加速度计的精度和范围限制问题。&lt;h4&gt;背景&lt;/h4&gt;低成本加速度计因其体积小、易于集成、穿戴舒适以及可大规模生产等优点，在汽车系统、航空航天及可穿戴技术等领域得到广泛应用。然而，此类传感器存在严重的准确度与量程局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够将低成本传感器信号转化为高成本等价信号的技术，以克服低性能加速度计的精度和范围限制问题。&lt;h4&gt;方法&lt;/h4&gt;{'HEROS-GAN': '一种能量调节和最优监督生成对抗网络，用于改善低成本加速度计信号的质量。', 'OTS': '基于最优传输理论探索未配对数据之间的潜在一致性，最大化监督信息的方法。', 'MLE': '调制拉普拉斯能量注入方法，旨在鼓励生成器打破范围限制、增强局部变化并丰富信号细节。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能提升': '实验结果显示单独使用OTS或MLE的GAN相比现有信号增强SOTA方法有显著提升。', '综合效果': '结合OTS和MLE后，HEROS-GAN能够将加速度计范围加倍，并降低信号噪声两个数量级，在加速度计信号处理方面建立了一个新的基准。'}&lt;h4&gt;结论&lt;/h4&gt;通过引入HEROS-GAN及其相关技术，本研究为解决低成本加速度计的精度与量程限制提供了有效解决方案。&lt;h4&gt;翻译&lt;/h4&gt;本文提出了用于改善低成本加速度计性能的技术方法和实验结果，包括提出的框架HEROS-GAN以及专门建立的数据集LASED。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-cost accelerometers play a crucial role in modern society due to theiradvantages of small size, ease of integration, wearability, and massproduction, making them widely applicable in automotive systems, aerospace, andwearable technology. However, this widely used sensor suffers from severeaccuracy and range limitations. To this end, we propose a honed-energyregularized and optimal supervised GAN (HEROS-GAN), which transforms low-costsensor signals into high-cost equivalents, thereby overcoming the precision andrange limitations of low-cost accelerometers. Due to the lack of frame-levelpaired low-cost and high-cost signals for training, we propose an OptimalTransport Supervision (OTS), which leverages optimal transport theory toexplore potential consistency between unpaired data, thereby maximizingsupervisory information. Moreover, we propose a Modulated Laplace Energy (MLE),which injects appropriate energy into the generator to encourage it to breakrange limitations, enhance local changes, and enrich signal details. Given theabsence of a dedicated dataset, we specifically establish a Low-costAccelerometer Signal Enhancement Dataset (LASED) containing tens of thousandsof samples, which is the first dataset serving to improve the accuracy andrange of accelerometers and is released in Github. Experimental resultsdemonstrate that a GAN combined with either OTS or MLE alone can surpass theprevious signal enhancement SOTA methods by an order of magnitude. Integratingboth OTS and MLE, the HEROS-GAN achieves remarkable results, which doubles theaccelerometer range while reducing signal noise by two orders of magnitude,establishing a benchmark in the accelerometer signal processing.</description>
      <author>example@mail.com (Yifeng Wang, Yi Zhao)</author>
      <guid isPermaLink="false">2502.18064v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Ordered Genetic Algorithm for Entrance Dependent Vehicle Routing Problem in Farms</title>
      <link>http://arxiv.org/abs/2502.18062v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新型的车辆路径问题（EDVRP），并针对农场场景提出了数学模型和一种有序遗传算法（OGA）来解决此问题。&lt;h4&gt;背景&lt;/h4&gt;在一些实际的车辆路径问题场景中，城市的规模及其入口数量对优化过程有着显著的影响。&lt;h4&gt;目的&lt;/h4&gt;为了应对上述情况，作者构建了依赖于入口的车辆路径问题（EDVRP），并通过实验验证了有序遗传算法的有效性。&lt;h4&gt;方法&lt;/h4&gt;提出了用于农场场景中的EDVRP数学模型，并开发了一种名为OGA的新颖遗传算法来解决该问题。此外还通过删除实验验证了新操作符的效果。&lt;h4&gt;主要发现&lt;/h4&gt;与随机策略基线和没有排序的遗传算法相比，OGA在优化过程中显示出一定的优势。新型的操作符也证明了它们对提高算法性能的有效性。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，针对特定场景设计的EDVRP及其相应的有序遗传算法可以在解决实际问题时提供更加有效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;车辆路径问题是许多生产场景中广泛研究的重要议题。在某些实际应用场景下，城市的规模和入口数量显著影响优化过程。为了解决这一问题，作者构建了依赖于入口的车辆路径问题（EDVRP）以描述此类问题，并提供了农场场景下的数学模型以及提出了一种有序遗传算法（OGA）。通过多组随机生成案例实验表明，与基准线策略相比，OGA展示出一定的优势。此外，新引入的操作符在消除实验中证明了其对改进算法性能的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle Routing Problems (VRP) are widely studied issues that play importantroles in many production scenarios. We have noticed that in some practicalscenarios of VRP, the size of cities and their entrances can significantlyinfluence the optimization process. To address this, we have constructed theEntrance Dependent VRP (EDVRP) to describe such problems. We provide amathematical formulation for the EDVRP in farms and propose an Ordered GeneticAlgorithm (OGA) to solve it. The effectiveness of OGA is demonstrated throughour experiments, which involve a multitude of randomly generated cases. Theresults indicate that OGA offers certain advantages compared to a randomstrategy baseline and a genetic algorithm without ordering. Furthermore, thenovel operators introduced in this paper have been validated through ablationexperiments, proving their effectiveness in enhancing the performance of thealgorithm.</description>
      <author>example@mail.com (Haotian Xu, Xiaohui Fan, Jialin Zhu, Qing Zhuo, Tao Zhang)</author>
      <guid isPermaLink="false">2502.18062v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>S-Graphs 2.0 -- A Hierarchical-Semantic Optimization and Loop Closure for SLAM</title>
      <link>http://arxiv.org/abs/2502.18044v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, RAL submission&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的机器人定位和制图算法Situation Graphs 2.0，该算法利用室内场景的层次结构来提高数据管理和优化效率。通过构建四个层级的情境图形（关键帧、墙壁、房间、楼层），实现了更高效的多层环境中的姿态管理与地图优化。&lt;h4&gt;背景&lt;/h4&gt;基于定位和制图的方法通常没有充分利用环境中固有的语义信息，导致机器人姿势不准确且在大规模环境下计算效率低下。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来利用3D场景图形的层次化表示来提高机器人位置管理和优化的效率和准确性。&lt;h4&gt;方法&lt;/h4&gt;1. 构建包含关键帧、墙壁、房间、楼层四个层级的情境图；2. 前端模块包括一个能识别楼梯并分配楼层级语义关系的楼层检测模块，这使得可以拒绝视觉上相似但位于不同楼层区域中的假阳性闭环；3. 利用层次结构进行改进优化，包括局部优化、楼层面全局优化和房间级别局部优化。&lt;h4&gt;主要发现&lt;/h4&gt;Situation Graphs 2.0 在多层真实环境中表现出了优越的性能，并能够创建层级地图同时限制计算复杂性，而一些基准方法在大规模场景中难以有效执行。&lt;h4&gt;结论&lt;/h4&gt;Situation Graphs 2.0 是一种有效的机器人定位和制图算法，在大型多层环境中的数据管理和优化方面表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Works based on localization and mapping do not exploit the inherentsemantic-relational information from the environment for faster and efficientmanagement and optimization of the robot poses and its map elements, oftenleading to pose and map inaccuracies and computational inefficiencies in largescale environments. 3D scene graph representations which distributes theenvironment in an hierarchical manner can be exploited to enhance themanagement/optimization of underlying robot poses and its map.  In this direction, we present our work Situational Graphs 2.0, whichleverages the hierarchical structure of indoor scenes for efficient datamanagement and optimization. Our algorithm begins by constructing a situationalgraph that organizes the environment into four layers: Keyframes, Walls, Rooms,and Floors. Our first novelty lies in the front-end which includes a floordetection module capable of identifying stairways and assigning a floor-levelsemantic-relations to the underlying layers. This floor-level semantic enablesa floor-based loop closure strategy, rejecting false-positive loop closures invisually similar areas on different floors. Our second novelty is in exploitingthe hierarchy for an improved optimization. It consists of: (1) localoptimization, optimizing a window of recent keyframes and their connectedcomponents, (2) floor-global optimization, which focuses only on keyframes andtheir connections within the current floor during loop closures, and (3)room-local optimization, marginalizing redundant keyframes that shareobservations within the room.  We validate our algorithm extensively in different real multi-floorenvironments. Our approach can demonstrate state-of-art-art results in largescale multi-floor environments creating hierarchical maps while bounding thecomputational complexity where several baseline works fail to executeefficiently.</description>
      <author>example@mail.com (Hriday Bavle, Jose Luis Sanchez-Lopez, Muhammad Shaheer, Javier Civera, Holger Voos)</author>
      <guid isPermaLink="false">2502.18044v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Multimodal Interaction and Intention Communication for Industrial Robots</title>
      <link>http://arxiv.org/abs/2502.17971v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to the 1st German Robotics Conference (GRC)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种设计和评估非人形工业机器人表达性的人机交互系统的方法，特别是通过一个小类人的机器人作为其主机（如叉车）的代理来沟通。&lt;h4&gt;背景&lt;/h4&gt;为了实现高级的人机交互水平，工业机器人需要能够安全有效地在人类环境中操作、进行自然交流、理解用户，并以直观的方式表达意图而不引起不必要的干扰。&lt;h4&gt;目的&lt;/h4&gt;论文旨在为非人形工业机器人设计和增强表达性的人机交互系统，通过开发一种结合语音、运动等多种模态的多模式通信框架来实现这一目标。&lt;h4&gt;方法&lt;/h4&gt;本文提出了一种概念，即使用一个小类人的机器人作为非人形主机（例如叉车）的代理进行交流，并为此机器人开发了一个多模态和大型语言模型增强的通讯框架。实验通过凝视追踪和动作捕捉技术量化了用户对机器人的感知以及任务进度。&lt;h4&gt;主要发现&lt;/h4&gt;论文展示了如何通过结合不同通信方式，可以使工业机器人在人机交互中更加有效且自然地交流。&lt;h4&gt;结论&lt;/h4&gt;研究结果表明，使用多模态通讯框架的表达性人机交互系统对于提高非人形工业机器人的互动性和用户接受度至关重要。&lt;h4&gt;翻译&lt;/h4&gt;成功的工业机器人采用将强烈依赖于它们能够在人类环境中安全有效地操作、进行自然沟通、理解用户，并以直观的方式表达意图而不引起不必要的干扰。为了实现这种高级水平的人机交互，机器人需要获取并整合对用户任务和环境的知识，并采取结合语音、动作等多模态的通讯方式。本文介绍了一些设计、增强和完善非人形工业机器人表达性人机交互系统的方案。我们提出了一个小型类人机器人作为其主机（如叉车）的代理进行沟通的概念，为此机器人开发了一个多模式且增强了大型语言模型的通信框架，并通过实验室实验进行了评估。这些实验利用凝视追踪和动作捕捉技术量化了用户对机器人的感知以及任务进度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Successful adoption of industrial robots will strongly depend on theirability to safely and efficiently operate in human environments, engage innatural communication, understand their users, and express intentionsintuitively while avoiding unnecessary distractions. To achieve this advancedlevel of Human-Robot Interaction (HRI), robots need to acquire and incorporateknowledge of their users' tasks and environment and adopt multimodalcommunication approaches with expressive cues that combine speech, movement,gazes, and other modalities. This paper presents several methods to design,enhance, and evaluate expressive HRI systems for non-humanoid industrialrobots. We present the concept of a small anthropomorphic robot communicatingas a proxy for its non-humanoid host, such as a forklift. We developed amultimodal and LLM-enhanced communication framework for this robot andevaluated it in several lab experiments, using gaze tracking and motion captureto quantify how users perceive the robot and measure the task progress.</description>
      <author>example@mail.com (Tim Schreiter, Andrey Rudenko, Jens V. Rüppel, Martin Magnusson, Achim J. Lilienthal)</author>
      <guid isPermaLink="false">2502.17971v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Quadrotor Neural Dead Reckoning in Periodic Trajectories</title>
      <link>http://arxiv.org/abs/2502.17964v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于神经网络的四旋翼无人机死 reckoning 方法，用于在纯惯性导航模式下飞行时提高定位精度。&lt;h4&gt;背景&lt;/h4&gt;由于环境或硬件限制，在室内和室外操作时，四旋翼无人机被迫以纯惯性导航模式运行。这导致了惯性漂移的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来增强基于周期轨迹的四旋翼无人机在纯惯性导航模式下的定位性能。&lt;h4&gt;方法&lt;/h4&gt;采用了一种简单的高效神经网络模型直接从惯性读数中估计四旋翼的位置向量，而不是将距离回归与基于惯性模型的方向估计相结合的方法。&lt;h4&gt;主要发现&lt;/h4&gt;该方法通过仅进行软件修改，在室外实现了平均误差降低 27%，在室内实现平均误差降低了 79% 的定位精度提升。&lt;h4&gt;结论&lt;/h4&gt;改进的定位准确度使得四旋翼无人机可以无缝地完成其任务，而无需额外的硬件支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文：现实世界中，由于环境或硬件限制，在室内或室外操作时，四旋翼被迫以纯惯性导航模式运行。为减少惯性漂移，提出了结合了四旋翼周期轨迹的端到端神经网络方法。其中，通过回归四旋翼距离并将其与基于惯性模型的方向估计相结合来估算四旋翼的位置向量。为了进一步增强定位性能，在本文中我们提出了一种针对沿周期轨迹飞行的四旋翼无人机的神经死 reckoning 方法。在这种情况下，惯性读数被馈送到一个简单而高效的网络中以直接估计四旋翼位置向量。我们的方法在两种不同的四旋翼上进行了评估：一种在室内操作，另一种在室外操作。与深度学习方法相比，我们的方法提高了定位精度，在户外平均误差减少了 27%，在室内减少了 79%，并且只需要软件修改。通过我们方法实现的改进定位准确度，四旋翼可以无缝执行其任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real world scenarios, due to environmental or hardware constraints, thequadrotor is forced to navigate in pure inertial navigation mode whileoperating indoors or outdoors. To mitigate inertial drift, end-to-end neuralnetwork approaches combined with quadrotor periodic trajectories weresuggested. There, the quadrotor distance is regressed and combined withinertial model-based heading estimation, the quadrotor position vector isestimated. To further enhance positioning performance, in this paper we proposea quadrotor neural dead reckoning approach for quadrotors flying on periodictrajectories. In this case, the inertial readings are fed into a simple andefficient network to directly estimate the quadrotor position vector. Ourapproach was evaluated on two different quadrotors, one operating indoors whilethe other outdoors. Our approach improves the positioning accuracy of otherdeep-learning approaches, achieving an average 27% reduction in error outdoorsand an average 79% reduction indoors, while requiring only softwaremodifications. With the improved positioning accuracy achieved by our method,the quadrotor can seamlessly perform its tasks.</description>
      <author>example@mail.com (Shira Massas, Itzik Klein)</author>
      <guid isPermaLink="false">2502.17964v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>InVDriver: Intra-Instance Aware Vectorized Query-Based Autonomous Driving Transformer</title>
      <link>http://arxiv.org/abs/2502.17949v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to JICV (Journal of Intelligent and Connected Vehicles)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了InVDriver系统，这是一种新型的向量化查询框架，用于解决现有自动驾驶中基于点的空间关联忽略问题，从而提高规划精度和轨迹平滑度。&lt;h4&gt;背景&lt;/h4&gt;端到端自动驾驶因其整体优化能力而在学术界和工业界越来越受到关注。向量化表示方法通过保留实例级别的拓扑信息并减少计算开销而变得流行起来。然而，现有的向量化查询框架往往忽略了实例内部点之间的空间关联，导致几何不一致的输出。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有系统的问题，提出了一种新的基于向量化的查询系统InVDriver，它通过屏蔽自注意力层系统地建模了实例内空间依赖关系。&lt;h4&gt;方法&lt;/h4&gt;InVDriver在感知、预测和规划的所有核心模块中都集成了屏蔽的自我注意机制，这些机制限制了对内部点交互的关注，并允许结构元素的同时细化，同时抑制不相关的跨实例噪声。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在nuScenes基准测试上，InVDriver实现了最先进的性能，超越了先前的方法，不仅在精度和安全性方面表现出色，而且还保持了计算效率。这验证了对实例内部几何一致性进行显式建模对于向量化自动驾驶系统的重要性，弥合了端到端框架的理论优势与实际部署需求之间的差距。&lt;h4&gt;结论&lt;/h4&gt;InVDriver通过精确建模空间依赖关系来改进端到端自动驾驶系统的性能和可靠性，证明了在自动驾驶技术中引入这种新方法的有效性。&lt;h4&gt;翻译&lt;/h4&gt;端到端自主驾驶因其整体优化能力，在学术界和工业界越来越受欢迎。向量化表示方法由于保留实例级别的拓扑信息并且减少计算负担而崭露头角。然而，现有的基于查询的框架往往忽视了实例内点之间的固有空间关联，导致几何不一致的结果（例如片段化的HD地图元素或振荡轨迹）。为了解决这些问题，我们提出了InVDriver——一个新颖的向量化查询系统，通过屏蔽自注意力层来系统性地建模内部的空间依赖关系，从而提高规划精度和轨迹平滑度。在感知、预测以及规划的所有核心模块中，InVDriver采用了屏蔽自注意机制，这种机制限制了对内部点交互的关注，允许同时优化结构元素的同时抑制无关的跨实例噪音。实验结果表明，在nuScenes基准测试上，InVDriver实现了最先进的性能，并且超过了之前的方法在准确性和安全性方面的要求，同时保持了高计算效率。我们的工作验证了显式建模内部几何一致性的关键性对于推进向量化自动驾驶系统的重要性，这使得端到端框架的理论优势和实际部署要求之间差距得以缩小。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; End-to-end autonomous driving with its holistic optimization capabilities,has gained increasing traction in academia and industry. Vectorizedrepresentations, which preserve instance-level topological information whilereducing computational overhead, have emerged as a promising paradigm. Whileexisting vectorized query-based frameworks often overlook the inherent spatialcorrelations among intra-instance points, resulting in geometricallyinconsistent outputs (e.g., fragmented HD map elements or oscillatorytrajectories). To address these limitations, we propose InVDriver, a novelvectorized query-based system that systematically models intra-instance spatialdependencies through masked self-attention layers, thereby enhancing planningaccuracy and trajectory smoothness. Across all core modules, i.e., perception,prediction, and planning, InVDriver incorporates masked self-attentionmechanisms that restrict attention to intra-instance point interactions,enabling coordinated refinement of structural elements while suppressingirrelevant inter-instance noise. Experimental results on the nuScenes benchmarkdemonstrate that InVDriver achieves state-of-the-art performance, surpassingprior methods in both accuracy and safety, while maintaining high computationalefficiency. Our work validates that explicit modeling of intra-instancegeometric coherence is critical for advancing vectorized autonomous drivingsystems, bridging the gap between theoretical advantages of end-to-endframeworks and practical deployment requirements.</description>
      <author>example@mail.com (Bo Zhang, Heye Huang, Chunyang Liu, Yaqin Zhang, Zhenhua Xu)</author>
      <guid isPermaLink="false">2502.17949v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>corobos: A Design for Mobile Robots Enabling Cooperative Transitions between Table and Wall Surfaces</title>
      <link>http://arxiv.org/abs/2502.17868v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  CHI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': 'corobos 是一种概念设计，使得多机器人系统能够在没有人为干预的情况下，在桌面和墙面之间平滑过渡。', '背景': '群组用户界面允许通过多个移动机器人的使用来动态安排用户环境，但由于其两轮推进系统的限制，操作范围通常局限于单一平面。', '目的': '展示 corobos 设计概念，该设计让机器人能够在水平面（桌面）和垂直面（墙面）之间平滑过渡，而无需额外的主动电气组件。', '方法': '每个机器人都装备了独特的斜坡结构，在其他机器人推动时能够实现平稳旋转。研究了这种结构的设计参数，并通过实验评估其转换成功率。', '主要发现': '设计仅依赖于被动机械元件，无需额外的主动电气部件即可实现无缝过渡。', '结论': '展示了 corobos 的各种应用实例，证明其在增强用户环境中具有巨大潜力。', '翻译': 'Swarm User Interfaces 允许通过使用多个移动机器人来动态调整用户环境，但它们的操作范围通常受限于两轮推进系统的限制而局限于单一平面。我们提出了一种概念设计 corobos，使得这些机器人可以在没有人为干预的情况下，在水平的桌面和垂直的墙面之间平滑过渡。每个机器人都配备了独特的斜坡结构，在其他机器人推动时可以实现平稳旋转，并且这种设计仅依赖于被动机械元件，不需要额外的主动电气组件。我们研究了这一结构的设计参数并通过实验评估其转换成功率，此外还展示了各种应用示例以展示 corobos 在增强用户环境方面的潜力。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3706598.3713440&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Swarm User Interfaces allow dynamic arrangement of user environments throughthe use of multiple mobile robots, but their operational range is typicallyconfined to a single plane due to constraints imposed by their two-wheelpropulsion systems. We present corobos, a proof-of-concept design that enablesthese robots to cooperatively transition between table (horizontal) and wall(vertical) surfaces seamlessly, without human intervention. Each robot isequipped with a uniquely designed slope structure that facilitates smoothrotation when another robot pushes it toward a target surface. Notably, thisdesign relies solely on passive mechanical elements, eliminating the need foradditional active electrical components. We investigated the design parametersof this structure and evaluated its transition success rate throughexperiments. Furthermore, we demonstrate various application examples toshowcase the potential of corobos in enhancing user environments.</description>
      <author>example@mail.com (Changyo Han, Yosuke Nakagawa, Takeshi Naemura)</author>
      <guid isPermaLink="false">2502.17868v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and Motion from Human Handovers</title>
      <link>http://arxiv.org/abs/2502.17834v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  In Submission at IEEE-IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项工作探讨了物体重量对人手交接过程中人体运动和抓取-释放动作的影响，旨在通过引入基于人类行为分析的自适应机器人策略来增强机器人与人的互动自然性、安全性和效率。&lt;h4&gt;背景&lt;/h4&gt;研究发现不同重量的物体会影响人在进行手部交互时的动作，这对设计更符合人类行为习惯的机器人至关重要。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据物体重量调整抓取-释放方式的自适应策略，提升机器人与人之间在不同重量物品交换过程中的表现和用户体验。&lt;h4&gt;方法&lt;/h4&gt;通过分析人在不同重量下进行手部交互的行为模式，提出并测试了基于人类行为模型的自适应机器人技术，并创建了一个包含多种物体重量数据集（包括YCB handover dataset）以验证策略的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;研究证明了提出的自适应抓取-释放技术和运动调整方法能够在自然度、效率和用户感知方面超越基准方法，显著改善机器人与人的手部交互体验。&lt;h4&gt;结论&lt;/h4&gt;通过将机器人的动作设计得更接近于人类的行为模式，可以大大提高机器人在处理不同重量物品时的手动交换过程的安全性和流畅性。这项工作为进一步研究提供了重要数据和理论基础。&lt;h4&gt;翻译&lt;/h4&gt;这项工作的摘要描述了一项探索物体重量对人手交接中人体运动及抓取释放行为影响的研究，旨在通过引入基于人类交互分析的自适应机器人策略来提升机器人的自然、安全且高效的人机互动能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This work explores the effect of object weight on human motion and griprelease during handovers to enhance the naturalness, safety, and efficiency ofrobot-human interactions. We introduce adaptive robotic strategies based on theanalysis of human handover behavior with varying object weights. The keycontributions of this work includes the development of an adaptive grip-releasestrategy for robots, a detailed analysis of how object weight influences humanmotion to guide robotic motion adaptations, and the creation ofhandover-datasets incorporating various object weights, including the YCBhandover dataset. By aligning robotic grip release and motion with humanbehavior, this work aims to improve robot-human handovers for differentweighted objects. We also evaluate these human-inspired adaptive roboticstrategies in robot-to-human handovers to assess their effectiveness andperformance and demonstrate that they outperform the baseline approaches interms of naturalness, efficiency, and user perception.</description>
      <author>example@mail.com (Parag Khanna, Mårten Björkman, Christian Smith)</author>
      <guid isPermaLink="false">2502.17834v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>CAML: Collaborative Auxiliary Modality Learning for Multi-Agent Systems</title>
      <link>http://arxiv.org/abs/2502.17821v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了Collaborative Auxiliary Modality Learning (CAML)，一种新型的多智能体跨模态学习框架，适用于动态环境如自动驾驶车辆，并通过实验验证了其在事故检测和协作语义分割中的效果。&lt;h4&gt;背景&lt;/h4&gt;现有的跨模态学习方法主要在单个代理环境下工作，在具有复杂动态环境的情况下会导致决策盲点问题。这些问题尤其影响到连接自主汽车（CAV）的安全性和性能。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的多智能体框架，使多个代理能够共享和协作使用多种模式的数据，并且能够在测试期间减少每个代理的模态输入。&lt;h4&gt;方法&lt;/h4&gt;提出了CAML框架，在训练时允许跨模态数据的合作，并在测试阶段可以进行单个模式的推理。该框架特别关注不确定性的降低和数据覆盖范围的增加，提供了理论上的优势分析。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，与现有技术相比，CAML能显著提高事故检测准确率（最高提升58.13%）；同时，在真实的空地机器人协作语义分割任务中也表现出色，达到了高达10.61%mIoU的改进。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了CAML在解决动态环境下的决策问题方面具有巨大潜力，并为进一步的研究提供了一个新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modality learning has become a crucial technique for improving theperformance of machine learning applications across domains such as autonomousdriving, robotics, and perception systems. While existing frameworks such asAuxiliary Modality Learning (AML) effectively utilize multiple data sourcesduring training and enable inference with reduced modalities, they primarilyoperate in a single-agent context. This limitation is particularly critical indynamic environments, such as connected autonomous vehicles (CAV), whereincomplete data coverage can lead to decision-making blind spots. To addressthese challenges, we propose Collaborative Auxiliary Modality Learning($\textbf{CAML}$), a novel multi-agent multi-modality framework that enablesagents to collaborate and share multimodal data during training while allowinginference with reduced modalities per agent during testing. We systematicallyanalyze the effectiveness of $\textbf{CAML}$ from the perspective ofuncertainty reduction and data coverage, providing theoretical insights intoits advantages over AML. Experimental results in collaborative decision-makingfor CAV in accident-prone scenarios demonstrate that \ours~achieves up to a${\bf 58.13}\%$ improvement in accident detection. Additionally, we validate$\textbf{CAML}$ on real-world aerial-ground robot data for collaborativesemantic segmentation, achieving up to a ${\bf 10.61}\%$ improvement in mIoU.</description>
      <author>example@mail.com (Rui Liu, Yu Shen, Peng Gao, Pratap Tokekar, Ming Lin)</author>
      <guid isPermaLink="false">2502.17821v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Safe Multi-Agent Navigation guided by Goal-Conditioned Safe Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.17813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Due to the limitation "The abstract field cannot be longer than 1,920  characters", the abstract here is shorter than that in the PDF file&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要类型&lt;/h4&gt;安全导航对于在危险环境中操作的自主系统至关重要。&lt;h4&gt;背景&lt;/h4&gt;传统规划方法擅长处理长时任务，但依赖于预定义的距离图。相比之下，安全强化学习（Safe RL）可以不用手动启发式就能学习复杂行为，但在目标条件和多代理场景中的长期任务中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合了规划与安全强化学习优点的新方法。&lt;h4&gt;方法&lt;/h4&gt;{'融合': '将目标导向的RL和安全RL结合来学习导航策略', '估算': '通过自动化自训练算法使用学得的价值函数同时估计累积距离和安全性水平', '图构建': '从回放缓存中构造状态图，并修剪不安全边，生成基于航点的计划'}&lt;h4&gt;主要发现&lt;/h4&gt;{'长时导航': '在扩展距离上有效平衡快速与安全路线。', '多代理问题': '利用冲突基础搜索（CBS）创建多个代理的安全路径规划，以解决长期范围内的多代理安全导航问题。这种方法提高了目标导向安全RL的可扩展性，并促进了代理之间的高效协调'}&lt;h4&gt;结论&lt;/h4&gt;{'效果证明': '通过广泛的基准测试，证明了该方法在复杂危险环境中实现多代理距离目标时的有效性和安全性。', '未来工作': '将发布代码以支持未来的相关研究。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safe navigation is essential for autonomous systems operating in hazardousenvironments. Traditional planning methods excel at long-horizon tasks but relyon a predefined graph with fixed distance metrics. In contrast, safeReinforcement Learning (RL) can learn complex behaviors without relying onmanual heuristics but fails to solve long-horizon tasks, particularly ingoal-conditioned and multi-agent scenarios.  In this paper, we introduce a novel method that integrates the strengths ofboth planning and safe RL. Our method leverages goal-conditioned RL and safe RLto learn a goal-conditioned policy for navigation while concurrently estimatingcumulative distance and safety levels using learned value functions via anautomated self-training algorithm. By constructing a graph with states from thereplay buffer, our method prunes unsafe edges and generates a waypoint-basedplan that the agent follows until reaching its goal, effectively balancingfaster and safer routes over extended distances.  Utilizing this unified high-level graph and a shared low-levelgoal-conditioned safe RL policy, we extend this approach to address themulti-agent safe navigation problem. In particular, we leverage Conflict-BasedSearch (CBS) to create waypoint-based plans for multiple agents allowing fortheir safe navigation over extended horizons. This integration enhances thescalability of goal-conditioned safe RL in multi-agent scenarios, enablingefficient coordination among agents.  Extensive benchmarking against state-of-the-art baselines demonstrates theeffectiveness of our method in achieving distance goals safely for multipleagents in complex and hazardous environments. Our code will be released tosupport future research.</description>
      <author>example@mail.com (Meng Feng, Viraj Parimi, Brian Williams)</author>
      <guid isPermaLink="false">2502.17813v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Design of a Breakaway Utensil Attachment for Enhanced Safety in Robot-Assisted Feeding</title>
      <link>http://arxiv.org/abs/2502.17774v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的机械安全机制，以提高机器人辅助喂食系统的安全性，并减轻护理者的负担。&lt;h4&gt;背景&lt;/h4&gt;现有的机器人辅助喂食系统主要依赖软件安全特性来减少意外碰撞时的风险。然而，这些方法在实际应用中可能不够可靠。&lt;h4&gt;目的&lt;/h4&gt;探索使用一种机械的紧急脱离装置，该装置能够在机器人施加过大力量时自动断开与用户的连接。&lt;h4&gt;方法&lt;/h4&gt;设计了一种可以分离力矩的手持餐具附件，并通过有限元分析（FEA）预测了不同加载条件下的失效点。之后利用3D打印技术制作带有各种槽深和壁环变化的样品，进行了实验验证。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在特定参数下（例如1毫米的槽深度和三个壁环），该装置在承受65牛顿力时能够按预期失效。此外，这种设计可以根据个人舒适度等因素进行调整，定制化安全脱开力度。&lt;h4&gt;结论&lt;/h4&gt;这项研究提供了一种创新的方法来提高机器人辅助喂食系统的安全性，并为未来的研究提供了有用的参数和设计方案。&lt;h4&gt;翻译&lt;/h4&gt;机器人辅助进食系统通过增强身体运动障碍个体的独立性并减轻护理人员的压力而发挥作用。尽管现有的系统主要依赖于基于软件的安全特性以在未预见的碰撞时降低风险，但本研究探讨了使用机械安全机制来提高安全性的问题。设计了一种可以在机器人施加力过大时从用户那里分离的餐具附件，从而防止对用户的伤害。通过有限元分析预测了不同加载条件下的失败点，并利用带有槽深和壁环变化的不同3D打印样品进行了实验验证。为了便于测试，开发并验证了一个跌落试验装置。结果显示，在1毫米深度的槽口和三个壁环的情况下，当施力达到65牛顿时会以预期的方式失效。此外，可以根据用户的特定因素（如舒适度和个人承受能力）调整参数来定制化脱开力量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot-assisted feeding systems enhance the independence of individuals withmotor impairments and alleviate caregiver burden. While existing systemspredominantly rely on software-based safety features to mitigate risks duringunforeseen collisions, this study explores the use of a mechanical fail-safe toimprove safety. We designed a breakaway utensil attachment that decouplesforces exerted by the robot on the user when excessive forces occur. Finiteelement analysis (FEA) simulations were performed to predict failure pointsunder various loading conditions, followed by experimental validation using3D-printed attachments with variations in slot depth and wall loops. Tofacilitate testing, a drop test rig was developed and validated. Our resultsdemonstrated a consistent failure point at the slot of the attachment, with aslot depth of 1 mm and three wall loops achieving failure at the target forceof 65 N. Additionally, the parameters can be tailored to customize thebreakaway force based on user-specific factors, such as comfort and paintolerance. CAD files and utensil assembly instructions can be found here:https://tinyurl.com/rfa-utensil-attachment</description>
      <author>example@mail.com (Hau Wen Chang, J-Anne Yow, Lek Syn Lim, Wei Tech Ang)</author>
      <guid isPermaLink="false">2502.17774v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Toward 6-DOF Autonomous Underwater Vehicle Energy-Aware Position Control based on Deep Reinforcement Learning: Preliminary Results</title>
      <link>http://arxiv.org/abs/2502.17742v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 5 figures, submitted to 2024 IEEE OES AUV Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于深度强化学习（DRL）的新颖方法，用于控制六自由度的自主水下航行器。使用截断分位数评论家(TQC)算法直接将命令发送给推进器，并且不需要关于推进器配置的知识。&lt;h4&gt;背景&lt;/h4&gt;在进行深海勘探时，自主水下航行器(AUVs)的操纵性和能源效率是关键因素，使得六自由度平台成为必备工具。PID和模型预测控制控制器尽管广泛使用，但它们需要准确的系统知识，并且面对负载或配置变化时难以重复。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型基于深度强化学习的方法来增强自主水下航行器在六个自由度上的操作性能，同时减少能量消耗。&lt;h4&gt;方法&lt;/h4&gt;利用截断分位数评论家(TQC)算法作为控制工具，设计了一种无需手动调整的DRL控制系统，该系统可以直接将命令发送给推进器，并且考虑了功率消耗因素。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明，TQC高性能方法在达到目标点时的表现优于精细调优后的PID控制器。而能量感知型TQC方法虽然性能稍低，但平均节省了30%的能源。&lt;h4&gt;结论&lt;/h4&gt;TQC算法适用于六自由度AUVs控制，并且具有节能的优势，未来可能成为自主水下航行器控制领域的重要技术之一。&lt;h4&gt;翻译&lt;/h4&gt;论文摘要&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-25&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The use of autonomous underwater vehicles (AUVs) for surveying, mapping, andinspecting unexplored underwater areas plays a crucial role, wheremaneuverability and power efficiency are key factors for extending the use ofthese platforms, making six degrees of freedom (6-DOF) holonomic platformsessential tools. Although Proportional-Integral-Derivative (PID) and ModelPredictive Control controllers are widely used in these applications, theyoften require accurate system knowledge, struggle with repeatability whenfacing payload or configuration changes, and can be time-consuming tofine-tune. While more advanced methods based on Deep Reinforcement Learning(DRL) have been proposed, they are typically limited to operating in fewerdegrees of freedom. This paper proposes a novel DRL-based approach forcontrolling holonomic 6-DOF AUVs using the Truncated Quantile Critics (TQC)algorithm, which does not require manual tuning and directly feeds commands tothe thrusters without prior knowledge of their configuration. Furthermore, itincorporates power consumption directly into the reward function. Simulationresults show that the TQC High-Performance method achieves better performanceto a fine-tuned PID controller when reaching a goal point, while the TQCEnergy-Aware method demonstrates slightly lower performance but consumes 30%less power on average.</description>
      <author>example@mail.com (Gustavo Boré, Vicente Sufán, Sebastián Rodríguez-Martínez, Giancarlo Troni)</author>
      <guid isPermaLink="false">2502.17742v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>IBURD: Image Blending for Underwater Robotic Detection</title>
      <link>http://arxiv.org/abs/2502.17706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种图像融合管道IBURD，用于生成逼真的合成图像，以辅助训练深度检测器在水下自主车辆（AUV）上进行海洋垃圾检测任务。&lt;h4&gt;背景&lt;/h4&gt;现有的数据集不足以满足复杂水下环境中的深度学习视觉算法的需求，特别是在数据稀缺和多样性方面存在明显问题。&lt;h4&gt;目的&lt;/h4&gt;通过生成具有实际海底背景的海洋垃圾图像来解决现有数据集的问题，并为使用AUV执行环保清理任务提供技术支持。&lt;h4&gt;方法&lt;/h4&gt;{'IBURD技术': '利用源图（包含目标物体）及其标注，以及目标环境背景图像作为输入；通过泊松编辑和风格迁移等技术将透明物体制作到任意背景中并自动调整合成图像的样式。', '生成方式': '能够基于源图和目标背景创建垃圾对象图像及像素级别的注释。', '图像质量改善': '使用目标背景图像的模糊度指标自动调整合成图像的风格，使输出更加真实且适配于具体场景。'}&lt;h4&gt;主要发现&lt;/h4&gt;IBURD在机器人检测海洋垃圾任务中的表现得到了验证，并展示了其有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;该方法能够生成高质量的真实海底环境下的海洋垃圾图像，从而促进AUV在环保清理任务中应用的进一步发展。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种用于创建逼真合成图像以帮助训练水下自主车辆（AUV）上深度检测器来完成海洋垃圾检测任务的图像融合管道IBURD。具体来说，IBURD能够生成海底垃圾图像及其像素级标注，并使用源图中的物体、其注释以及目标背景环境图作为输入。利用泊松编辑和风格迁移等技术，IBURD甚至可以将透明物体制作到任意背景下并自动调整合成图片的样式以适应背景模糊度指标的变化。这些包含实际海底背景的真实海洋垃圾图像解决了深度学习视觉算法在挑战性水下条件下的数据稀缺与多样性问题，并促进了AUV用于环境清理任务的应用。通过定量和机器人评估证明了该方法在机器人检测海洋垃圾方面具有有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present an image blending pipeline, \textit{IBURD}, that creates realisticsynthetic images to assist in the training of deep detectors for use onunderwater autonomous vehicles (AUVs) for marine debris detection tasks.Specifically, IBURD generates both images of underwater debris and theirpixel-level annotations, using source images of debris objects, theirannotations, and target background images of marine environments. With Poissonediting and style transfer techniques, IBURD is even able to robustly blendtransparent objects into arbitrary backgrounds and automatically adjust thestyle of blended images using the blurriness metric of target backgroundimages. These generated images of marine debris in actual underwaterbackgrounds address the data scarcity and data variety problems faced bydeep-learned vision algorithms in challenging underwater conditions, and canenable the use of AUVs for environmental cleanup missions. Both quantitativeand robotic evaluations of IBURD demonstrate the efficacy of the proposedapproach for robotic detection of marine debris.</description>
      <author>example@mail.com (Jungseok Hong, Sakshi Singh, Junaed Sattar)</author>
      <guid isPermaLink="false">2502.17706v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>The Geometry of Optimal Gait Families for Steering Kinematic Locomoting Systems</title>
      <link>http://arxiv.org/abs/2502.17672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, submitted to IEEE Transactions on Robotics&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文探讨了基于蛇形机器人等复杂系统的运动规划，提出了一种生成连续优化步伐族的方法，并展示了如何利用全局和局部搜索策略构建这些最优步伐族。&lt;h4&gt;背景&lt;/h4&gt;对于类似于蛇机器人的系统来说，将高层次刚体任务转换为低层次的关节轨迹是一个更具挑战性的过程。这个映射依赖于当前配置并受到关节限制的约束。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提高复杂运动系统的控制性和机动性。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一种结合全局和局部搜索策略的方法来生成连续优化步伐族，其中局部搜索提供更高的精度但可能在非平滑区域不稳定，而全局搜索则对非平滑行为具有鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;研究揭示了最优步伐家族的底层几何结构，并展示了这种方法对于粘性和理想流体三连杆游泳器的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作为低层次关节控制器与高层次运动规划器在复杂运动系统中的集成奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;运动计划通常需要将高层次刚性体任务转换成低层次的关节轨迹。这个过程对于具有固定、不受限制驱动输入的小车式机器人来说是直接明了的，但对于蛇形机器人等系统而言则更具挑战性。因为在这个情况下，映射取决于当前配置并受到关节限制的影响。在这篇论文中，我们关注于生成连续家族的最佳步伐集——用步伐大小或转向速率参数化的最佳步伐集合，以增强控制性和机动性。我们揭示了这些最佳步伐家族的底层几何结构，并提出了使用全局和局部搜索策略构建它们的方法，在这种方法中局部方法与全球方法相互补充。全局搜索方式对非平滑行为具有鲁棒性，尽管这导致了解决方案顺序降低，而局部搜索则提供了更高的精度但可能在非平滑区域不稳定。为了证明我们的框架的有效性，我们为粘性和理想流体三连杆游泳器生成了最佳的步伐家族。这项工作为基础运动规划器和低级关节控制器之间的集成奠定了基础，在复杂的运动系统中使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Motion planning for locomotion systems typically requires translatinghigh-level rigid-body tasks into low-level joint trajectories-a process that isstraightforward for car-like robots with fixed, unbounded actuation inputs butmore challenging for systems like snake robots, where the mapping depends onthe current configuration and is constrained by joint limits. In this paper, wefocus on generating continuous families of optimal gaits-collections of gaitsparameterized by step size or steering rate-to enhance controllability andmaneuverability. We uncover the underlying geometric structure of these optimalgait families and propose methods for constructing them using both global andlocal search strategies, where the local method and the global methodcompensate each other. The global search approach is robust to nonsmoothbehavior, albeit yielding reduced-order solutions, while the local searchprovides higher accuracy but can be unstable near nonsmooth regions. Todemonstrate our framework, we generate optimal gait families for viscous andperfect-fluid three-link swimmers. This work lays a foundation for integratinglow-level joint controllers with higher-level motion planners in complexlocomotion systems.</description>
      <author>example@mail.com (Jinwoo Choi, Siming Deng, Nathan Justus, Noah J. Cowan, Ross L. Hatton)</author>
      <guid isPermaLink="false">2502.17672v1</guid>
      <pubDate>Wed, 26 Feb 2025 17:03:55 +0800</pubDate>
    </item>
    <item>
      <title>Building reliable sim driving agents by scaling self-play</title>
      <link>http://arxiv.org/abs/2502.14706v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  First version&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种基于大规模自我游戏训练的方法，旨在提高模拟代理的可靠性，并应用于Waymo开放运动数据集中的数千个场景。&lt;h4&gt;背景&lt;/h4&gt;设计和测试与人类交互的系统（如自动驾驶汽车）时需要可靠的仿真代理。这些代理在评估自动驾驶性能、压力测试等方面都有应用，但所有用例都需要高度可靠的表现，以确保分析的有效性。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决大规模训练集并在未见过的场景中有效推广的方法，并展示其对分布外场景的部分鲁棒性以及通过微调快速达到近乎完美表现的能力。&lt;h4&gt;方法&lt;/h4&gt;提出了一种基于自我游戏的大规模训练策略，利用Waymo开放运动数据集中的数千个半现实主义限制造成的场景进行训练。所有训练均在单GPU上从零开始完成，并且能够在一天内几乎解决整个训练集。&lt;h4&gt;主要发现&lt;/h4&gt;经过训练的代理能够达到99.8%的目标完成率，在10,000个未见过的场景中，碰撞和脱轨事件的发生率低于0.8%，展示了有效的推广能力。此外，这些代理对分布外场景表现出部分鲁棒性，并且可以通过微调在几分钟内实现近乎完美的性能。&lt;h4&gt;结论&lt;/h4&gt;通过开放源代码库提供了预训练的代理以及完整的代码基础，以便于研究者进一步探索和改进仿真代理技术。&lt;h4&gt;翻译&lt;/h4&gt;该论文探讨了如何通过大规模自我游戏来提升自动驾驶车辆等与人类交互系统的模拟代理的可靠性。通过对Waymo Open Motion Dataset进行半现实限制造成的大规模场景训练，在单GPU上从零开始训练的代理能够实现高可靠性和有效推广，同时展示出对分布外情况的部分鲁棒性，并可以通过快速微调达到近乎完美的性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Simulation agents are essential for designing and testing systems thatinteract with humans, such as autonomous vehicles (AVs). These agents servevarious purposes, from benchmarking AV performance to stress-testing thesystem's limits, but all use cases share a key requirement: reliability. Asimulation agent should behave as intended by the designer, minimizingunintended actions like collisions that can compromise the signal-to-noiseratio of analyses. As a foundation for reliable sim agents, we propose scalingself-play to thousands of scenarios on the Waymo Open Motion Dataset undersemi-realistic limits on human perception and control. Training from scratch ona single GPU, our agents nearly solve the full training set within a day. Theygeneralize effectively to unseen test scenes, achieving a 99.8% goal completionrate with less than 0.8% combined collision and off-road incidents across10,000 held-out scenarios. Beyond in-distribution generalization, our agentsshow partial robustness to out-of-distribution scenes and can be fine-tuned inminutes to reach near-perfect performance in those cases. Demonstrations ofagent behaviors can be found at this link. We open-source both the pre-trainedagents and the complete code base. Demonstrations of agent behaviors can befound at \url{https://sites.google.com/view/reliable-sim-agents}.</description>
      <author>example@mail.com (Daphne Cornelisse, Aarav Pandya, Kevin Joseph, Joseph Suárez, Eugene Vinitsky)</author>
      <guid isPermaLink="false">2502.14706v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
  <item>
      <title>Enhancing CoMP-RSMA Performance with Movable Antennas: A Meta-Learning Optimization Framework</title>
      <link>http://arxiv.org/abs/2502.17389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;本研究探讨了一种下行链路速率分割多址接入（RSMA）场景，在该场景中，多个基站采用协同多点（CoMP）传输方案为配备移动天线（MA）技术的用户提供服务。与传统的固定位置天线（FPA）相比，后者受无线信道随机变化的影响，MAs可以战略性地重新定位到信道条件更优的位置，从而实现增强的空间分集增益。&lt;h4&gt;背景&lt;/h4&gt;在传统FPA受限于无线信道随机性的情况下，移动天线技术通过优化位置来改善无线通信性能。这种改进带来了更高的空间多样性收益，并且可以通过调整基站的发射波束成形矢量、不同用户的公共流分配以及MA的最佳定位进一步提高总的可达和速率。&lt;h4&gt;目的&lt;/h4&gt;为了最大化可达到的总速率并确保符合服务质量（QoS）约束，研究提出一个优化问题以确定BS的最佳传输波束形成矢量、各用户之间的共同流分配以及移动天线的最优位置。但该问题由于变量间的强依赖关系而复杂且计算上具有挑战性。&lt;h4&gt;方法&lt;/h4&gt;为了解决大规模优化任务中的计算难题，提出了一种无需预训练的基于梯度的元学习（GML）算法，这种方法特别适合于处理大型优化任务，并通过数值结果证明了其有效性和准确性。该方法能够实现接近最优的结果（与最佳解决方案相比超过97%）。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，移动天线增强型CoMP-RSMA模型在性能上显著优于传统基准方案，在空间分割多址接入(SDMA)方案和基于固定位置天线的RSMA模型上分别实现了高达190%和80%的性能提升。此外，该方法能够减轻SDMA中总速率受限于干扰的问题，并通过较少的基站实现更优表现。&lt;h4&gt;结论&lt;/h4&gt;所提出的GML算法在优化移动天线增强型CoMP-RSMA场景中的总体可达速率方面取得了显著成效，特别是在处理大规模复杂优化问题时展示了其优越性。该研究为未来的无线通信系统设计提供了一种有效的策略。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This study investigates a downlink rate-splitting multiple access (RSMA)scenario in which multiple base stations (BSs), employing a coordinatedmulti-point (CoMP) transmission scheme, serve users equipped with movableantenna (MA) technology. Unlike traditional fixed-position antennas (FPAs),which are subject to random variations in wireless channels, MAs can bestrategically repositioned to locations with more favorable channel conditions,thereby achieving enhanced spatial diversity gains.To leverage these advantagesand maximize the achievable sum rate, we formulate an optimization problem thatjointly determines the optimal transmit beamforming vectors at the BSs, thecommon stream allocation for different users, and the optimal positioning ofthe MAs, all while ensuring compliance with quality of service (QoS)constraints. However, the formulated problem is non-convex and computationallychallenging due to the strong interdependence among the optimization variables.Traditional methods for solving large-scale optimization problems typicallyincur prohibitively high computational complexity. To address the abovechallenge, we propose a gradient-based meta-learning (GML) algorithm thatoperates without pre-training and is well-suited for handling large-scaleoptimization tasks. Numerical results demonstrate the effectiveness andaccuracy of the proposed approach, achieving near-optimal performance(exceeding 97% compared to the optimal solution). Moreover, the MA-enabledCoMP-RSMA model significantly outperforms conventional benchmark schemes,yielding performance gains of up to 190% over the spatial division multipleaccess (SDMA) scheme and 80% over the RSMA FPA-based model. Finally, theproposed approach is shown to mitigate the sum-rate limitations imposed byinterference in SDMA, achieving superior performance with fewer BSs.</description>
      <author>example@mail.com (Ali Amhaz, Shreya Khisa, Mohamed Elhattab, Chadi Assi, Sanaa Sharafeddine)</author>
      <guid isPermaLink="false">2502.17389v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Sustainable Greenhouse Management: A Comparative Analysis of Recurrent and Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.17371v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种使用时空图神经网络（STGNN）对温室微气候进行建模的新方法，与传统的递归神经网络（RNN）相比，该方法在考虑环境变量之间的空间依赖关系及其方向性方面具有优势。&lt;h4&gt;背景&lt;/h4&gt;将光伏系统集成到温室中可以优化土地利用并促进可持续农业实践，同时提供食品生产和可再生能源发电的双重效益。然而，准确预测内部环境条件对于确保作物生长最佳和最大化能源生产至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过引入STGNN方法来改善温室微气候建模，提高对环境变量间空间依赖关系的理解，并在传统RNN模型的基础上进一步优化性能。&lt;h4&gt;方法&lt;/h4&gt;论文使用从希腊沃洛斯的一个温室每15分钟收集的高频数据进行实验。这些数据用于评估STGNN和RNN在不同季节条件下的表现差异。&lt;h4&gt;主要发现&lt;/h4&gt;RNN模型在冬季条件下表现出卓越的准确性（R^2 = 0.985），但在夏季冷却系统运行期间显示出局限性；相比之下，尽管目前STGNN的表现略低（冬季R^2 = 0.947），但其架构为整合诸如光伏发电和作物生长指标等额外变量提供了更大的潜力。&lt;h4&gt;结论&lt;/h4&gt;虽然现有的STGNN模型在性能上不如传统RNN，在温室微气候建模方面表现出一定的限制，但是考虑到它们在未来应用中的潜在优势，研究认为进一步探索STGNN的应用是值得的。&lt;h4&gt;翻译&lt;/h4&gt;该摘要描述了将时空图神经网络应用于温室内部环境条件预测的研究成果。论文通过对比分析不同模型在特定时间段内的表现，强调了STGNN的独特优势和未来可能的发展方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The integration of photovoltaic (PV) systems into greenhouses not onlyoptimizes land use but also enhances sustainable agricultural practices byenabling dual benefits of food production and renewable energy generation.However, accurate prediction of internal environmental conditions is crucial toensure optimal crop growth while maximizing energy production. This studyintroduces a novel application of Spatio-Temporal Graph Neural Networks(STGNNs) to greenhouse microclimate modeling, comparing their performance withtraditional Recurrent Neural Networks (RNNs). While RNNs excel at temporalpattern recognition, they cannot explicitly model the directional relationshipsbetween environmental variables. Our STGNN approach addresses this limitationby representing these relationships as directed graphs, enabling the model tocapture both spatial dependencies and their directionality. Usinghigh-frequency data collected at 15-minute intervals from a greenhouse inVolos, Greece, we demonstrate that RNNs achieve exceptional accuracy in winterconditions (R^2 = 0.985) but show limitations during summer cooling systemoperation. Though STGNNs currently show lower performance (winter R^2 = 0.947),their architecture offers greater potential for integrating additionalvariables such as PV generation and crop growth indicators.</description>
      <author>example@mail.com (Emiliano Seri, Marcello Petitta, Cristina Cornaro)</author>
      <guid isPermaLink="false">2502.17371v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>HybridLinker: Topology-Guided Posterior Sampling for Enhanced Diversity and Validity in 3D Molecular Linker Generation</title>
      <link>http://arxiv.org/abs/2502.17349v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种名为HybridLinker的框架，旨在解决药物设计中分子片段组合生成的有效性和多样性之间的权衡问题。&lt;h4&gt;背景&lt;/h4&gt;在药物发现应用如先导优化和PROTAC设计过程中，链接子生成是关键步骤。现有方法主要分为PC-Free（不使用3D点云）和PC-Aware（依赖于3D点云约束）两类。前者注重多样性但有效性较低；后者确保高有效性但限制了多样性。&lt;h4&gt;目的&lt;/h4&gt;提出一种无需额外训练就能提高有效性和多样性的新框架HybridLinker。&lt;h4&gt;方法&lt;/h4&gt;通过将预训练的PC-Free模型提供的多样化键合拓扑结构作为指导，增强了PC-Aware模型的推理能力。核心是提出了首个跨PC-Free和PC-Aware空间的操作的方法——LinkerDPS（链接子扩散后验采样），利用能量启发式函数连接分子拓扑与3D点云。&lt;h4&gt;主要发现&lt;/h4&gt;HybridLinker框架通过将PC-Free模型中多样化的采样分布转换为PC-Aware模型中的分布，显著且一致地提高了基础分子设计和应用属性优化任务的有效性和多样性。&lt;h4&gt;结论&lt;/h4&gt;本文建立了一种新的扩散后验采样（DPS）框架，在分子和图域内超越了成像领域，具有潜在的应用价值。&lt;h4&gt;翻译&lt;/h4&gt;链接子生成在药物发现中的先导优化和PROTAC设计等应用中至关重要。现有的方法根据是否使用3D点云划分为PC-Free和PC-Aware两类。前者追求多样性但有效性较低；后者确保高有效性但限制了多样性。为解决此权衡问题，我们提出了HybridLinker框架，通过引入预训练的PC-Free模型提供的多样化键合拓扑结构来增强PC-Aware模型的推理能力。我们的核心贡献是提出了一种新的扩散后验采样方法LinkerDPS，在分子和图域内建立了有效的连接，显著提高了有效性和多样性，开创了新的研究领域。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Linker generation is critical in drug discovery applications such as leadoptimization and PROTAC design, where molecular fragments are assembled intodiverse drug candidates. Existing methods fall into PC-Free and PC-Awarecategories based on their use of 3D point clouds (PC). PC-Free modelsprioritize diversity but suffer from lower validity due to overlooking PCconstraints, while PC-Aware models ensure higher validity but restrictdiversity by enforcing strict PC constraints. To overcome these trade-offswithout additional training, we propose HybridLinker, a framework that enhancesPC-Aware inference by providing diverse bonding topologies from a pretrainedPC-Free model as guidance. At its core, we propose LinkerDPS, the firstdiffusion posterior sampling (DPS) method operating across PC-Free and PC-Awarespaces, bridging molecular topology with 3D point clouds via an energy-inspiredfunction. By transferring the diverse sampling distribution of PC-Free modelsinto the PC-Aware distribution, HybridLinker significantly and consistentlysurpasses baselines, improving both validity and diversity in foundationalmolecular design and applied property optimization tasks, establishing a newDPS framework in the molecular and graph domains beyond imaging.</description>
      <author>example@mail.com (Minyeong Hwang, Ziseok Lee, Gwangsoo Kim, Kyungsu Kim, Eunho Yang)</author>
      <guid isPermaLink="false">2502.17349v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>LCV2I: Communication-Efficient and High-Performance Collaborative Perception Framework with Low-Resolution LiDAR</title>
      <link>http://arxiv.org/abs/2502.17039v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种新的车辆到基础设施（V2I）协同感知框架LCV2I，该框架使用低成本低分辨率激光雷达和摄像头数据来提高协作感知的性能。&lt;h4&gt;背景&lt;/h4&gt;当前V2I合作感知系统主要依赖于高成本的高分辨率激光雷达，但这种传感器价格昂贵且难以普及。同时，传统通信方法带宽利用率较低。&lt;h4&gt;目的&lt;/h4&gt;为了实现低成本的V2I协同感知，研究旨在降低车辆端使用高分辨率激光雷达的成本，并提高数据传输效率。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架LCV2I，该框架采用低分辨率激光雷达和摄像头的数据作为输入，并利用特征偏移校正模块和区域特征增强算法来改进特征表示。此外，通过区域差异图和区域评分图评估协作内容的价值，从而提高通信带宽效率。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的LCV2I方法在保持高水平感知性能的同时，显著减少了对车辆端高分辨率传感器的需求，并且在真实世界场景中的3D目标检测测试中超越了现有算法的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究成功开发了一种高效的低成本V2I协同感知框架，能够通过低分辨率激光雷达和摄像头的数据实现高质量的感知结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vehicle-to-Infrastructure (V2I) collaborative perception leverages datacollected by infrastructure's sensors to enhance vehicle perceptualcapabilities. LiDAR, as a commonly used sensor in cooperative perception, iswidely equipped in intelligent vehicles and infrastructure. However, itssuperior performance comes with a correspondingly high cost. To achievelow-cost V2I, reducing the cost of LiDAR is crucial. Therefore, we studyadopting low-resolution LiDAR on the vehicle to minimize cost as much aspossible. However, simply reducing the resolution of vehicle's LiDAR results insparse point clouds, making distant small objects even more blurred.Additionally, traditional communication methods have relatively low bandwidthutilization efficiency. These factors pose challenges for us. To balance costand perceptual accuracy, we propose a new collaborative perception framework,namely LCV2I. LCV2I uses data collected from cameras and low-resolution LiDARas input. It also employs feature offset correction modules and regionalfeature enhancement algorithms to improve feature representation. Finally, weuse regional difference map and regional score map to assess the value ofcollaboration content, thereby improving communication bandwidth efficiency. Insummary, our approach achieves high perceptual performance while substantiallyreducing the demand for high-resolution sensors on the vehicle. To evaluatethis algorithm, we conduct 3D object detection in the real-world scenario ofDAIR-V2X, demonstrating that the performance of LCV2I consistently surpassescurrently existing algorithms.</description>
      <author>example@mail.com (Xinxin Feng, Haoran Sun, Haifeng Zheng, Huacong Chen, Wenqiang Chen)</author>
      <guid isPermaLink="false">2502.17039v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Supervised contrastive learning from weakly-labeled audio segments for musical version matching</title>
      <link>http://arxiv.org/abs/2502.16936v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages, 6 figures, 7 tables; includes Appendix&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;检测音乐版本是一项具有挑战性的任务，现有方法通常在曲目级别上匹配音乐版本，而实际应用中需要在片段级别进行匹配。&lt;h4&gt;背景描述&lt;/h4&gt;现有的音乐版本检测技术大多基于整个音频文件的完全标注，并使用分类和三元组损失等传统方法，忽略了更现代的损失函数可能带来的改进。&lt;h4&gt;研究目的&lt;/h4&gt;开发一种可以在弱监督学习条件下工作的新方法，该方法利用对比损失变体在片段级别上提高性能。&lt;h4&gt;主要方法&lt;/h4&gt;{'弱标记段学习': '基于成对的音乐片段距离减少进行训练', '对比损失修改': '通过解耦、超参数和几何学考虑改进现有损失函数'}&lt;h4&gt;关键发现&lt;/h4&gt;提出的方法不仅在标准曲目级评估中达到了最先进的性能，在片段级别上也实现了突破性的效果。&lt;h4&gt;结论&lt;/h4&gt;由于所解决问题的通用性，该方法可能超越音频或音乐版本匹配领域，在其他领域找到应用价值。&lt;h4&gt;翻译&lt;/h4&gt;检测音乐版本是一项具有挑战性的任务，并且具有重要的实际应用场景。现有的方法通常基于完全标注数据进行曲目级别的匹配（例如整首歌曲）。然而大多数实际应用场景需要在片段级别上进行匹配（例如20秒的音频段落）。此外，现有研究大多依赖于分类和三元组损失函数，而忽视了更现代的损失函数可能带来的性能提升。本文中我们提出了一种基于弱监督学习的新型方法以及一种改进的对比损失变体，在片段级别的评估上达到了前所未有的性能水平，并且在传统的曲目级别评估上也取得了领先的结果。我们认为由于所解决问题的普遍性，该方法有望在音频或音乐版本匹配之外的其他领域找到应用机会。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting musical versions (different renditions of the same piece) is achallenging task with important applications. Because of the ground truthnature, existing approaches match musical versions at the track level (e.g.,whole song). However, most applications require to match them at the segmentlevel (e.g., 20s chunks). In addition, existing approaches resort toclassification and triplet losses, disregarding more recent losses that couldbring meaningful improvements. In this paper, we propose a method to learn fromweakly annotated segments, together with a contrastive loss variant thatoutperforms well-studied alternatives. The former is based on pairwise segmentdistance reductions, while the latter modifies an existing loss followingdecoupling, hyper-parameter, and geometric considerations. With these twoelements, we do not only achieve state-of-the-art results in the standardtrack-level evaluation, but we also obtain a breakthrough performance in asegment-level evaluation. We believe that, due to the generality of thechallenges addressed here, the proposed methods may find utility in domainsbeyond audio or musical version matching.</description>
      <author>example@mail.com (Joan Serrà, R. Oguz Araz, Dmitry Bogdanov, Yuki Mitsufuji)</author>
      <guid isPermaLink="false">2502.16936v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Models are Powerful EHR Encoders</title>
      <link>http://arxiv.org/abs/2502.17403v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探索了使用通用大型语言模型（LLM）嵌入方法作为电子健康记录（EHR）编码器的潜力，特别是在临床预测任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;电子健康记录数据复杂且异质性高，传统机器学习方法难以有效利用这些资源。领域特定的EHR基础模型虽然在提高预测准确性方面表现出色，但其训练受到高质量多样化数据集有限和编码标准不一致的影响。&lt;h4&gt;目的&lt;/h4&gt;评估通用LLM嵌入方法作为EHR编码器的有效性和潜在优势。&lt;h4&gt;方法&lt;/h4&gt;通过将患者记录转换为结构化的Markdown文本并利用预训练的大型语言模型（GTE-Qwen2-7B-Instruct和LLM2Vec-Llama3.1-8B-Instruct）进行代码转译，研究者在EHRSHOT基准测试的15个不同临床预测任务上比较了这些方法与特定于EHR的基础模型CLIMBR-T-Base及传统机器学习基线的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在少量样本的情况下，LLM基于嵌入的方法经常能够达到甚至超过专门模型的性能，并且其有效性随着基础LLM规模和上下文窗口大小的增长而提高。&lt;h4&gt;结论&lt;/h4&gt;重新利用LLM作为EHR编码器提供了一种可扩展且有效的临床预测方法，有助于克服传统EHR建模中的局限性并促进更互操作性和普遍性的医疗保健应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electronic Health Records (EHRs) offer rich potential for clinicalprediction, yet their inherent complexity and heterogeneity pose significantchallenges for traditional machine learning approaches. Domain-specific EHRfoundation models trained on large collections of unlabeled EHR data havedemonstrated promising improvements in predictive accuracy and generalization;however, their training is constrained by limited access to diverse,high-quality datasets and inconsistencies in coding standards and healthcarepractices. In this study, we explore the possibility of using general-purposeLarge Language Models (LLMs) based embedding methods as EHR encoders. Byserializing patient records into structured Markdown text, transforming codesinto human-readable descriptors, we leverage the extensive generalizationcapabilities of LLMs pretrained on vast public corpora, thereby bypassing theneed for proprietary medical datasets. We systematically evaluate twostate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct andLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks fromthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundationmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our resultsdemonstrate that LLM-based embeddings frequently match or exceed theperformance of specialized models, even in few-shot settings, and that theireffectiveness scales with the size of the underlying LLM and the availablecontext window. Overall, our findings demonstrate that repurposing LLMs for EHRencoding offers a scalable and effective approach for clinical prediction,capable of overcoming the limitations of traditional EHR modeling andfacilitating more interoperable and generalizable healthcare applications.</description>
      <author>example@mail.com (Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild)</author>
      <guid isPermaLink="false">2502.17403v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Electrical Load Forecasting over Multihop Smart Metering Networks with Federated Learning</title>
      <link>http://arxiv.org/abs/2502.17226v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: text overlap with arXiv:2411.10619&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;本论文提出了一个新型的个性化联邦学习(PFL) 方法，用于电表网络中的高质量负载预测。&lt;h4&gt;背景&lt;/h4&gt;电力负载预测对于智能电网的管理与稳定性至关重要。传统机器学习方法在负载预测中通常被使用，但会涉及到数据交换从而引发隐私问题。联邦学习（FL）可以通过不进行数据交换而在本地智能电表上运行分布式机器学习模型来解决这一问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新型个性化联邦学习(PFL) 方法以实现高效的负载预测，并减少延迟。&lt;h4&gt;方法&lt;/h4&gt;提出了基于元学习的策略，用于处理本地智能电表中的数据异质性。同时研究了一种新的基于最优资源分配的新延迟优化问题来降低PFL模型中负载预测延迟。&lt;h4&gt;主要发现&lt;/h4&gt;通过详尽的真实世界数据集仿真表明本论文的方法在负载预测和运营延迟成本方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;该方法为联邦学习的设计提供了理论收敛性分析，以提供关于联合负荷预测的见解。&lt;h4&gt;翻译&lt;/h4&gt;电力负载预测对于智能电网管理和稳定性至关重要。通常通过高级计量基础设施实现这一点，在这种基础设施中，智能电表记录家庭能耗数据。虽然传统机器学习方法被广泛用于负荷预测，但它们需要数据共享并且引发了隐私问题。联邦学习可以通过在本地智能电表上运行分布式模型而无需交换数据来解决这个问题。然而，当前基于FL的方法由于异构智能电表之间的数据分布不平衡而难以实现有效的负载预测。本文提出了一种新的个性化联邦学习（PFL）方法用于计量网络中的高质量负荷预测。研究团队开发了一个基于元学习的策略来处理在本地智能电表中联合训练本地负荷预测模型时的数据异质性问题。此外，为了最小化我们提出的PFL模型中的负载预测延迟，他们研究了一种新的基于最优资源分配的延迟优化问题。还进行了理论收敛性分析以提供关于联邦学习设计用于联邦负荷预测的见解。大量来自真实数据集的仿真显示该方法在提高负荷预测质量和减少运营延迟成本方面优于现有方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Electric load forecasting is essential for power management and stability insmart grids. This is mainly achieved via advanced metering infrastructure,where smart meters (SMs) record household energy data. Traditional machinelearning (ML) methods are often employed for load forecasting but require datasharing which raises data privacy concerns. Federated learning (FL) can addressthis issue by running distributed ML models at local SMs without data exchange.However, current FL-based approaches struggle to achieve efficient loadforecasting due to imbalanced data distribution across heterogeneous SMs. Thispaper presents a novel personalized federated learning (PFL) method forhigh-quality load forecasting in metering networks. A meta-learning-basedstrategy is developed to address data heterogeneity at local SMs in thecollaborative training of local load forecasting models. Moreover, to minimizethe load forecasting delays in our PFL model, we study a new latencyoptimization problem based on optimal resource allocation at SMs. A theoreticalconvergence analysis is also conducted to provide insights into FL design forfederated load forecasting. Extensive simulations from real-world datasets showthat our method outperforms existing approaches in terms of better loadforecasting and reduced operational latency costs.</description>
      <author>example@mail.com (Ratun Rahman, Pablo Moriano, Samee U. Khan, Dinh C. Nguyen)</author>
      <guid isPermaLink="false">2502.17226v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments</title>
      <link>http://arxiv.org/abs/2502.15342v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的自动驾驶感知技术在结构化的车辆主导环境中展示了卓越的能力，但在半结构化环境中存在显著限制。这些限制主要是由于高质量数据集缺乏造成的，尤其是在行人感知和预测方面。&lt;h4&gt;背景&lt;/h4&gt;当前的自动驾驶感知模型在半结构化环境（如动态行人频繁出现的地方）中表现出明显的局限性，因为现有的数据集中缺乏足够高质量的数据来支持这类场景的研究。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种新的多模态数据集——Pedestrian-Focused Scene Dataset (PFSD)，专门针对半结构化的复杂环境，并提出了Hybrid Multi-Scale Fusion Network（HMFN）模型以解决行人检测的挑战问题。&lt;h4&gt;方法&lt;/h4&gt;{'PFSD': '该数据集包含超过130,000个行人的实例，涵盖了各种密度、移动模式和遮挡情况。它提供了全面的多模态数据注释，包括点云分割、检测以及对象ID追踪。', 'HMFN': '为了在密集且被部分阻挡的情况下更好地识别行人，该方法使用精心设计的混合框架捕获并融合多尺度特征，整合了稀疏和标准卷积技术。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在PFSD上进行测试时，所提出的HMFN模型相比现有方法在3D行人检测方面实现了更高的平均精度(mAP)提升。&lt;h4&gt;结论&lt;/h4&gt;通过提出新的数据集和有效的网络架构来解决半结构化环境中复杂的行人感知挑战问题，证明了该工作的实用性和创新性。&lt;h4&gt;翻译&lt;/h4&gt;近期自动驾驶车辆的感知技术在高度结构化的交通场景中展示出了卓越的能力。然而，在行人活动更为多样且复杂遮挡更加普遍的半结构化环境下，现有的感知模型表现出了明显的局限性。这种现象主要是由于缺乏高质量的数据集，特别是关于行人的感知和预测数据。本文提出了一种新的多模态行人聚焦场景数据集（PFSD），它在nuScenes格式下被详细标注，并提供了全面的多模态注释，包括点云分割、检测及对象ID追踪等信息。该数据集覆盖了超过130,000个行人的实例，它们涵盖了不同密度、移动模式和遮挡情况下的各种场景。为了应对半结构化环境中更加多样复杂的情况带来的挑战，我们提出了一种新的混合多尺度融合网络（HMFN）。具体而言，在人口密集且存在部分阻挡的情况下，我们的方法通过精心设计的框架有效地捕捉并融合了多种规模特征，该框架集成了稀疏和传统卷积技术。在PFSD上的大量实验表明，与现有方法相比，HMFN在网络架构中实现了显著提高的平均精度（mAP），这证明了其解决半结构化环境中3D行人检测挑战的有效性。代码和基准测试结果已经开放提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in autonomous driving perception have revealedexceptional capabilities within structured environments dominated by vehiculartraffic. However, current perception models exhibit significant limitations insemi-structured environments, where dynamic pedestrians with more diverseirregular movement and occlusion prevail. We attribute this shortcoming to thescarcity of high-quality datasets in semi-structured scenes, particularlyconcerning pedestrian perception and prediction. In this work, we present themulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated insemi-structured scenes with the format of nuScenes. PFSD provides comprehensivemulti-modal data annotations with point cloud segmentation, detection, andobject IDs for tracking. It encompasses over 130,000 pedestrian instancescaptured across various scenarios with varying densities, movement patterns,and occlusions. Furthermore, to demonstrate the importance of addressing thechallenges posed by more diverse and complex semi-structured environments, wepropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, todetect pedestrians in densely populated and occluded scenarios, our methodeffectively captures and fuses multi-scale features using a meticulouslydesigned hybrid framework that integrates sparse and vanilla convolutions.Extensive experiments on PFSD demonstrate that HMFN attains improvement in meanAverage Precision (mAP) over existing methods, thereby underscoring itsefficacy in addressing the challenges of 3D pedestrian detection in complexsemi-structured environments. Coding and benchmark are available.</description>
      <author>example@mail.com (Yueting Liu, Hanshi Wang, Yunfei Lei, Zhengjun Zha, Weiming Hu, Jin Gao)</author>
      <guid isPermaLink="false">2502.15342v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Applications of Large Models in Medicine</title>
      <link>http://arxiv.org/abs/2502.17132v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了大规模模型在医疗领域的进展与应用，特别关注医学大型模型（MedLMs）的应用。&lt;h4&gt;背景&lt;/h4&gt;这些模型包括大型语言模型（LLMs）、视觉模型、3D大型模型和多模态模型。它们通过增强疾病预测、诊断辅助、个性化治疗计划及药物发现来革新医疗服务。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在提供大规模模型在医学领域现状与未来方向的全面概述，强调其在全球健康进步中的重要性。&lt;h4&gt;方法&lt;/h4&gt;论文重点介绍了大型图神经网络如何融入医疗知识图谱和药物发现中，以及视觉-语言模型（VLMs）和3D大型模型在医学图像分析、解剖建模及假肢设计方面的应用。&lt;h4&gt;主要发现&lt;/h4&gt;尽管存在挑战，但这些技术正在为医疗服务设定新的标准，并为个性化健康解决方案铺平道路。&lt;h4&gt;结论&lt;/h4&gt;大规模模型正在医疗领域实现变革性的进步，通过改善诊断准确性来推动全球卫生的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文翻译为：本文探讨了大型规模模型在医学领域的进展和应用，特别关注医学大模型（MedLMs）。这些模型包括大型语言模型、视觉模型、3D大型模型以及多模态模型。它们正在通过增强疾病预测、诊断辅助、个性化治疗计划及药物发现等方面革新医疗服务。研究还强调了大型图模型（LGMs）在理解复杂生物医学关系中的潜力，特别是在医疗知识图谱和药物发现中的集成应用。视觉-语言模型（VLMs）和3D大型模型在医学图像分析、解剖建模以及假肢设计方面的使用也得到突出展示。尽管存在挑战，这些技术正在为医疗服务设定新的标准，提高诊断准确性，并推动个性化健康解决方案的发展。本文旨在提供大规模模型在医学领域现状与未来方向的全面概述，强调它们在全球健康进步中的重要性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.71423/aimed.20250105&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper explores the advancements and applications of large-scale modelsin the medical field, with a particular focus on Medical Large Models (MedLMs).These models, encompassing Large Language Models (LLMs), Vision Models, 3DLarge Models, and Multimodal Models, are revolutionizing healthcare byenhancing disease prediction, diagnostic assistance, personalized treatmentplanning, and drug discovery. The integration of graph neural networks inmedical knowledge graphs and drug discovery highlights the potential of LargeGraph Models (LGMs) in understanding complex biomedical relationships. Thestudy also emphasizes the transformative role of Vision-Language Models (VLMs)and 3D Large Models in medical image analysis, anatomical modeling, andprosthetic design. Despite the challenges, these technologies are setting newbenchmarks in medical innovation, improving diagnostic accuracy, and paving theway for personalized healthcare solutions. This paper aims to provide acomprehensive overview of the current state and future directions of largemodels in medicine, underscoring their significance in advancing global health.</description>
      <author>example@mail.com (YunHe Su, Zhengyang Lu, Junhui Liu, Ke Pang, Haoran Dai, Sa Liu Yuxin Jia, Lujia Ge, Jing-min Yang)</author>
      <guid isPermaLink="false">2502.17132v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>CAR-LOAM: Color-Assisted Robust LiDAR Odometry and Mapping</title>
      <link>http://arxiv.org/abs/2502.17249v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种结合颜色信息的稳健框架，用于准确的LiDAR里程计和地图构建（LOAM），通过同时利用LiDAR点云和相机图像中的边缘及平面特征来提高定位精度。&lt;h4&gt;背景&lt;/h4&gt;现有的LiDAR Odometry and Mapping (LOAM)技术在使用单模态数据时存在局限性，难以处理复杂的环境场景。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够融合颜色信息的稳健框架，以实现更加准确和鲁棒性的LiDAR里程计及地图构建方法。&lt;h4&gt;方法&lt;/h4&gt;该框架包括：1）利用相机图像中的颜色为LiDAR点云着色；2）采用感知均匀的颜色差异权重策略来排除颜色对应异常值；3）使用基于Welsch函数的稳健误差度量法处理位置对应异常值。&lt;h4&gt;主要发现&lt;/h4&gt;新方法在复杂森林和校园等挑战性场景中表现出更高的准确性和鲁棒性，相较于当前最先进的技术有所改进。&lt;h4&gt;结论&lt;/h4&gt;利用相机图像中的颜色信息能够显著提高LiDAR里程计及地图构建的精度与稳定性。&lt;h4&gt;翻译&lt;/h4&gt;在这封信中，我们提出了一种结合颜色信息用于精确LiDAR里程估计和制图（LOAM）的稳健框架。同时从激光雷达和摄像机接收数据，该框架利用摄像机图像中的颜色信息对激光雷达点云进行着色，然后执行迭代姿态优化。对于每个激光雷达扫描，提取边缘和平面特征，并使用相应图像对其着色并匹配到全局地图中。特别地，我们采用感知均匀的颜色差异权重策略来排除颜色对应异常值，并基于Welsch函数的稳健误差度量法在姿态优化过程中减少位置对应异常值的影响。因此，该系统实现了精确定位，并重建了环境密集、准确、彩色且三维的地图。具有挑战性的场景（包括复杂森林和校园）中的彻底实验表明，我们的方法相比当前最先进的技术提供了更高的鲁棒性和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this letter, we propose a color-assisted robust framework for accurateLiDAR odometry and mapping (LOAM). Simultaneously receiving data from both theLiDAR and the camera, the framework utilizes the color information from thecamera images to colorize the LiDAR point clouds and then performs iterativepose optimization. For each LiDAR scan, the edge and planar features areextracted and colored using the corresponding image and then matched to aglobal map. Specifically, we adopt a perceptually uniform color differenceweighting strategy to exclude color correspondence outliers and a robust errormetric based on the Welsch's function to mitigate the impact of positionalcorrespondence outliers during the pose optimization process. As a result, thesystem achieves accurate localization and reconstructs dense, accurate, coloredand three-dimensional (3D) maps of the environment. Thorough experiments withchallenging scenarios, including complex forests and a campus, show that ourmethod provides higher robustness and accuracy compared with currentstate-of-the-art methods.</description>
      <author>example@mail.com (Yufei Lu, Yuetao Li, Zhizhou Jia, Qun Hao, Shaohui Zhang)</author>
      <guid isPermaLink="false">2502.17249v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>An Expert Ensemble for Detecting Anomalous Scenes, Interactions, and Behaviors in Autonomous Driving</title>
      <link>http://arxiv.org/abs/2502.16389v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Robotics Research (IJRR)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;自动驾驶汽车的安全性是实现完全自主驾驶的关键，特别是在检测超出操作设计领域的异常情况方面。论文提出了一种新颖的无监督异常检测专家系统来解决这个问题。&lt;h4&gt;背景&lt;/h4&gt;随着自动化车辆进入公共道路，确保无数驾驶场景中的安全性成为广泛采用全自动驾驶的重要挑战之一。&lt;h4&gt;目的&lt;/h4&gt;为了提高自动驾驶系统的可信度，研究提出了能够检测出道路上不常见情况的方法。&lt;h4&gt;方法&lt;/h4&gt;{'三类无监督异常检测专家': ['场景专家：专注于帧级别的外观来识别异常场景和未预期的场景运动；交互专家：建立两个道路参与者的相对正常移动模型，并在出现异常互动时发出警告；行为专家：通过未来轨迹预测监测个体对象的异常行为。'], '专家集成系统(Xen)': '利用卡尔曼滤波器将所有模块的优点结合起来，最终异常得分被作为其中一个状态，而观察结果则由各个专家生成。', '新颖评估协议': '采用了一种新的模型性能评估协议来测试实际应用中的表现'}&lt;h4&gt;主要发现&lt;/h4&gt;{'优越性': '实验结果显示该方法在检测道路上的异常情况时比先前的方法更胜一筹', '潜力': '通过无监督学习处理大规模数据集，该框架有分类不同类型的异常行为的潜力'}&lt;h4&gt;结论&lt;/h4&gt;提出了一种新颖且有效的方法来实现自动驾驶汽车中的安全性和可靠性，并展示了其在现实世界应用中的潜在价值。&lt;h4&gt;翻译&lt;/h4&gt;随着自动化车辆进入公共道路，确保无数驾驶场景中的安全性成为广泛采用全自动驾驶的重要挑战之一。论文提出了三种无监督异常检测专家：场景专家、交互专家和行为专家，以及一个通过卡尔曼滤波器将各模块的优点结合起来的专家集成系统(Xen)。实验显示该方法在道路上检测异常情况方面优于先前的方法，并且具有利用大规模数据集进行无监督学习来分类不同类型的异常行为的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1177/02783649241297998&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As automated vehicles enter public roads, safety in a near-infinite number ofdriving scenarios becomes one of the major concerns for the widespread adoptionof fully autonomous driving. The ability to detect anomalous situations outsideof the operational design domain is a key component in self-driving cars,enabling us to mitigate the impact of abnormal ego behaviors and to realizetrustworthy driving systems. On-road anomaly detection in egocentric videosremains a challenging problem due to the difficulties introduced by complex andinteractive scenarios. We conduct a holistic analysis of common on-road anomalypatterns, from which we propose three unsupervised anomaly detection experts: ascene expert that focuses on frame-level appearances to detect abnormal scenesand unexpected scene motions; an interaction expert that models normal relativemotions between two road participants and raises alarms whenever anomalousinteractions emerge; and a behavior expert which monitors abnormal behaviors ofindividual objects by future trajectory prediction. To combine the strengths ofall the modules, we propose an expert ensemble (Xen) using a Kalman filter, inwhich the final anomaly score is absorbed as one of the states and theobservations are generated by the experts. Our experiments employ a novelevaluation protocol for realistic model performance, demonstrate superioranomaly detection performance than previous methods, and show that ourframework has potential in classifying anomaly types using unsupervisedlearning on a large-scale on-road anomaly dataset.</description>
      <author>example@mail.com (Tianchen Ji, Neeloy Chakraborty, Andre Schreiber, Katherine Driggs-Campbell)</author>
      <guid isPermaLink="false">2502.16389v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Detecting Code Vulnerabilities with Heterogeneous GNN Training</title>
      <link>http://arxiv.org/abs/2502.16835v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;检测源代码中的漏洞是软件安全保障的关键任务。图神经网络（GNN）机器学习通过将源代码建模为图形，可以成为一种有前途的方法。&lt;h4&gt;背景&lt;/h4&gt;早期方法将代码元素统一处理，限制了其模拟多样化关系的能力，这些关系有助于识别各种类型的漏洞。最近的研究通过考虑节点类型的不同性，并使用门控图神经网络（GGNN）来解决这一问题，以不同的边类型聚合节点信息。&lt;h4&gt;目的&lt;/h4&gt;介绍Inter-Procedural Abstract Graphs (IPAG)作为一种高效的、与语言无关的源代码表示方法，结合异构GNN训练进行漏洞预测。提出Heterogeneous Attention GNN（HAGNN）模型来集成捕捉源代码不同特征的多个子图。&lt;h4&gt;方法&lt;/h4&gt;该模型使用异构注意力机制将这些分别学习到的不同子图结合起来，并通过全连接神经网络进行最终分类。&lt;h4&gt;主要发现&lt;/h4&gt;提出的这种方法在包含108种漏洞类型的大型C数据集上达到了高达96.6%的准确性，在包含114种漏洞类型的大型Java数据集上达到了97.8%，优于现有最先进的方法。此外，该方法应用于各种实际软件项目时也显示出了较低的假阳性率。&lt;h4&gt;结论&lt;/h4&gt;通过引入Inter-Procedural Abstract Graphs（IPAG）和Heterogeneous Attention GNN（HAGNN），为源代码漏洞检测提供了一种高效且准确的方法，具有广泛的应用前景。&lt;h4&gt;翻译&lt;/h4&gt;检测源代码中的漏洞是软件安全保障的关键任务。图神经网络（GNN）机器学习通过将源代码建模为图形，可以成为一种有前途的方法。早期方法统一处理代码元素，限制了其对导致各种类型漏洞的多样化关系进行建模的能力。最近的研究通过考虑节点类型的异质性，并使用门控图神经网络（GGNN）来解决这一问题，以不同的边类型聚合节点信息。然而，这些边缘主要作为传递节点信息的渠道，可能无法捕捉到不同类型的详细特征。本文提出了Inter-Procedural Abstract Graphs (IPAG)作为一种高效的、与语言无关的源代码表示方法，并结合异构GNN训练进行漏洞预测。IPAG捕获了代码元素及其关系的结构和上下文属性。我们还提出了一种Heterogeneous Attention GNN（HAGNN）模型，该模型集成了捕捉源代码不同特征的多个子图。这些子图分别学习并通过全局注意力机制结合在一起，并通过全连接神经网络进行最终分类。在大型C数据集中，提出的这种方法达到了高达96.6%的准确性，涵盖了108种漏洞类型；而在包含114种漏洞类型的大型Java数据集中，则达到了97.8%，优于现有最先进的方法。此外，在各种实际软件项目中的应用也显示出了较低的假阳性率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Detecting vulnerabilities in source code is a critical task for softwaresecurity assurance. Graph Neural Network (GNN) machine learning can be apromising approach by modeling source code as graphs. Early approaches treatedcode elements uniformly, limiting their capacity to model diverse relationshipsthat contribute to various vulnerabilities. Recent research addresses thislimitation by considering the heterogeneity of node types and using Gated GraphNeural Networks (GGNN) to aggregate node information through different edgetypes. However, these edges primarily function as conduits for passing nodeinformation and may not capture detailed characteristics of distinct edgetypes. This paper presents Inter-Procedural Abstract Graphs (IPAGs) as anefficient, language-agnostic representation of source code, complemented byheterogeneous GNN training for vulnerability prediction. IPAGs capture thestructural and contextual properties of code elements and their relationships.We also propose a Heterogeneous Attention GNN (HAGNN) model that incorporatesmultiple subgraphs capturing different features of source code. These subgraphsare learned separately and combined using a global attention mechanism,followed by a fully connected neural network for final classification. Theproposed approach has achieved up to 96.6% accuracy on a large C dataset of 108vulnerability types and 97.8% on a large Java dataset of 114 vulnerabilitytypes, outperforming state-of-the-art methods. Its applications to variousreal-world software projects have also demonstrated low false positive rates.</description>
      <author>example@mail.com (Yu Luo, Weifeng Xu, Dianxiang Xu)</author>
      <guid isPermaLink="false">2502.16835v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Scatterplot and Image Moments for Time-Varying Bivariate Field Analysis of Electronic Structure Evolution</title>
      <link>http://arxiv.org/abs/2502.17118v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的时间变化双变量场分析方法，用于理解光诱导动力学中电子结构的变化。&lt;h4&gt;背景&lt;/h4&gt;由于光照吸收引起的电子在能级间的跃迁是一个复杂的量子力学过程，这会影响分子内的核几何和电子结构。研究这些密度场有助于了解分子内供体区域与受体区域之间的电荷移动情况。&lt;h4&gt;目的&lt;/h4&gt;通过连续散点图（Continuous Scatterplots, CSP）及基于图像的时刻描述符来分析时间变化中的双变量字段，并针对光激发后的不断变化的电子结构，提出一种特征导向可视化探索的方法。&lt;h4&gt;方法&lt;/h4&gt;核运动产生的多个时间步长，使用CSP和基于图像的时刻描述符进行动态场数据的探索性分析。将每个时间步骤的CSP表示为四个长度的图矩向量，并形成一个R^4中的点云。&lt;h4&gt;主要发现&lt;/h4&gt;选取适当的主要成分可以将点云表示为平面上的一条曲线，从而有助于识别关键的时间步长、发现双变量字段内的模式以及追踪其随时间的变化。文中通过两个光激发分子动力学的案例研究展示了这种方法的应用。&lt;h4&gt;结论&lt;/h4&gt;此方法可有效揭示电子结构变化规律，并提供具有应用特定洞察力的方法来深入理解光诱导过程中的物理和化学机制。&lt;h4&gt;翻译&lt;/h4&gt;光致电子跃迁是由于光照吸收引起的复杂量子力学过程，其中电子在能级之间移动。这会引起电子结构和核几何的变化，推动了光生物学、材料设计以及医学等领域的重要物理和化学进程。不断演变的电子结构可以通过两个电子密度场来表征：空穴自然过渡轨道（NTO）和粒子自然过渡轨道（NTO）。研究这些密度领域有助于了解分子内供体区域与受体区域之间的电荷移动情况。以往的研究多依赖于等值面并排视觉比较、统计方法或双变量字段分析，实例较少。我们提出了一种新的时间变化双变量场分析方法，适用于理解大量实例下的光诱导电子结构变化。由于NTO领域取决于核几何，因此需通过许多时间步长来解析由核运动产生的复杂现象。本文采用连续散点图（Continuous Scatterplots, CSP）及基于图像的时刻描述符来进行动态场数据探索性分析，并针对光激发后的不断变化的电子结构，提出一种特征导向可视化探索的方法。每个时间步骤中的CSP通过四个长度的图矩向量来表示；将所有矢量描述符集合形成R^4空间里的点云并利用主成分分析技术进行可视化呈现。选择适当的主成分可以简化点云为平面上的一条曲线，有助于识别关键的时间步长、发现双变量字段内的模式以及追踪其随时间的变化。我们通过两个光激发分子动力学案例研究展示这种方法的有效性，并展示了双变量场分析在特定应用中提供的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Photoinduced electronic transitions are complex quantum-mechanical processeswhere electrons move between energy levels due to light absorption. Thisinduces dynamics in electronic structure and nuclear geometry, drivingimportant physical and chemical processes in fields like photobiology,materials design, and medicine. The evolving electronic structure can becharacterized by two electron density fields: hole and particle naturaltransition orbitals (NTOs). Studying these density fields helps understandelectronic charge movement between donor and acceptor regions within amolecule. Previous works rely on side-by-side visual comparisons ofisosurfaces, statistical approaches, or bivariate field analysis with fewinstances. We propose a new method to analyze time-varying bivariate fieldswith many instances, which is relevant for understanding electronic structurechanges during light-induced dynamics. Since NTO fields depend on nucleargeometry, the nuclear motion results in numerous time steps to analyze. Thispaper presents a structured approach to feature-directed visual exploration oftime-varying bivariate fields using continuous scatterplots (CSPs) and imagemoment-based descriptors, tailored for studying evolving electronic structurespost-photoexcitation. The CSP of the bivariate field at each time step isrepresented by a four-length image moment vector. The collection of all vectordescriptors forms a point cloud in R^4, visualized using principal componentanalysis. Selecting appropriate principal components results in arepresentation of the point cloud as a curve on the plane, aiding tasks such asidentifying key time steps, recognizing patterns within the bivariate field,and tracking the temporal evolution. We demonstrate this with two case studieson excited-state molecular dynamics, showing how bivariate field analysisprovides application-specific insights.</description>
      <author>example@mail.com (Mohit Sharma, Talha Bin Masood, Nanna Holmgaard List, Ingrid Hotz, Vijay Natarajan)</author>
      <guid isPermaLink="false">2502.17118v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Snoopy: Effective and Efficient Semantic Join Discovery via Proxy Columns</title>
      <link>http://arxiv.org/abs/2502.16813v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TKDE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的列级语义连接发现框架Snoopy，通过使用代理列来计算列嵌入以解决现有方法在有效性和效率方面的问题。&lt;h4&gt;背景&lt;/h4&gt;语义连接发现旨在从表库中找到与查询列有高语义连接性的列。现有方法分为单元格级别和列级别两种方法，但两者都无法同时保证有效性和效率。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架Snoopy来解决当前方法中存在的有效性低、效率不足的问题。&lt;h4&gt;方法&lt;/h4&gt;通过使用代理列计算列嵌入，并引入了一个基于排名的对比学习范式来获取指导列投影的良好代理列，提出了一个轻量级近似图匹配基线的列投射以捕捉隐式的列到代理列关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，Snoopy在Recall@25和NDCG@25上分别比现有最佳方法高出16%和10%，并且至少快五倍于单元级解决方案，在速度上是现有的列级方法的3.5倍。&lt;h4&gt;结论&lt;/h4&gt;提出的框架Snoopy不仅提高了语义连接发现的有效性，同时显著提升了效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要：语义连接发现旨在从表库中找到与查询列有高语义连接性的列。现存的方法可以分为两种类型：单元格级别方法和列级别方法。然而，两者都无法同时保证有效性和效率。单元级方法通过计算列之间的单元匹配来计算连接性，具有理想的有效性但效率低下。相比之下，列级别方法仅通过计算列嵌入的相似度来确定连接性，虽然效率尚可但由于其在列嵌入中存在的问题（i）语义-连接差距，（ii）大小限制，和（iii）排列敏感性而导致有效性较差。为了解决这些问题，本文提出使用代理列来计算列嵌入；此外还提出了一种新的列级语义连接发现框架Snoopy，利用基于代理列的嵌入在有效性和效率之间建立桥梁。具体而言，提出的列嵌入来自隐式的列到代理列关系，通过轻量级近似图匹配基线捕捉该关系。为了获取指导列投影的良好代理列，我们引入了一个排名感知对比学习范式。大量的实验结果表明，Snoopy在Recall@25和NDCG@25上分别比现有最佳方法高出16%和10%，并且至少快五倍于单元级解决方案，在速度上是现有的列级方法的3.5倍。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Semantic join discovery, which aims to find columns in a table repositorywith high semantic joinabilities to a query column, is crucial for datasetdiscovery. Existing methods can be divided into two categories: cell-levelmethods and column-level methods. However, neither of them ensures botheffectiveness and efficiency simultaneously. Cell-level methods, which computethe joinability by counting cell matches between columns, enjoy idealeffectiveness but suffer poor efficiency. In contrast, column-level methods,which determine joinability only by computing the similarity of columnembeddings, enjoy proper efficiency but suffer poor effectiveness due to theissues occurring in their column embeddings: (i) semantics-joinability-gap,(ii) size limit, and (iii) permutation sensitivity. To address these issues,this paper proposes to compute column embeddings via proxy columns;furthermore, a novel column-level semantic join discovery framework, Snoopy, ispresented, leveraging proxy-column-based embeddings to bridge effectiveness andefficiency. Specifically, the proposed column embeddings are derived from theimplicit column-to-proxy-column relationships, which are captured by thelightweight approximate-graph-matching-based column projection.To acquire goodproxy columns for guiding the column projection, we introduce a rank-awarecontrastive learning paradigm. Extensive experiments on four real-worlddatasets demonstrate that Snoopy outperforms SOTA column-level methods by 16%in Recall@25 and 10% in NDCG@25, and achieves superior efficiency--being atleast 5 orders of magnitude faster than cell-level solutions, and 3.5x fasterthan existing column-level methods.</description>
      <author>example@mail.com (Yuxiang Guo, Yuren Mao, Zhonghao Hu, Lu Chen, Yunjun Gao)</author>
      <guid isPermaLink="false">2502.16813v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Image Translation-Based Unsupervised Cross-Modality Domain Adaptation for Medical Image Segmentation</title>
      <link>http://arxiv.org/abs/2502.15193v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  5 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图像转换的无监督跨模态领域适应方法，通过将带注释的源模态图像转换为未注释的目标模态，并使用其注释来实现目标模态的监督学习。该方法在跨模态领域适应挑战中的验证阶段表现出色。&lt;h4&gt;背景&lt;/h4&gt;在医学影像中进行监督深度学习通常面临更多挑战，因为标注需要医生的专业知识且耗时费钱；无监督学习方法虽然被采用但性能降低不可避免；医学图像可能来自不同的医疗中心、使用不同设备和采集协议，导致模态差异，进一步降低了深度学习方法的适用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种能有效解决跨模态领域适应问题的方法，并在真实场景中验证其有效性。&lt;h4&gt;方法&lt;/h4&gt;通过将带注释的源模态图像转换为目标模态未标注图像，利用目标模态伪图像上的自训练方法克服细微差异，进一步提高深度学习任务性能。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在跨模态领域适应挑战中的验证阶段中，针对内耳神经瘤（VS）分割任务取得了平均Dice相似系数(DSC)为0.8351 ± 0.1152和对侧半规管（cochlea）的平均对称表面距离(ASSD)为1.6712±2.1948，在针对内耳神经瘤VS及耳蜗分割任务中取得了平均Dice相似系数(DSC)为0.8098 ± 0.0233和平均对称表面距离(ASSD)为0.2317±0.1577。&lt;h4&gt;结论&lt;/h4&gt;所提方法在跨模态领域适应问题上具有显著优势，能够有效应对医学图像的复杂性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Supervised deep learning usually faces more challenges in medical images thanin natural images. Since annotations in medical images require the expertise ofdoctors and are more time-consuming and expensive. Thus, some researchers turnto unsupervised learning methods, which usually face inevitable performancedrops. In addition, medical images may have been acquired at different medicalcenters with different scanners and under different image acquisitionprotocols, so the modalities of the medical images are often inconsistent. Thismodality difference (domain shift) also reduces the applicability of deeplearning methods. In this regard, we propose an unsupervised crossmodalitydomain adaptation method based on image translation by transforming the sourcemodality image with annotation into the unannotated target modality and usingits annotation to achieve supervised learning of the target modality. Inaddition, the subtle differences between translated pseudo images and realimages are overcome by self-training methods to further improve the taskperformance of deep learning. The proposed method showed mean Dice SimilarityCoefficient (DSC) and Average Symmetric Surface Distance (ASSD) of $0.8351 \pm0.1152$ and $1.6712 \pm 2.1948$ for vestibular schwannoma (VS), $0.8098 \pm0.0233$ and $0.2317 \pm 0.1577$ for cochlea on the VS and cochlea segmentationtask of the Cross-Modality Domain Adaptation (crossMoDA 2022) challengevalidation phase leaderboard.</description>
      <author>example@mail.com (Tao Yang, Lisheng Wang)</author>
      <guid isPermaLink="false">2502.15193v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>PointSea: Point Cloud Completion via Self-structure Augmentation</title>
      <link>http://arxiv.org/abs/2502.17053v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Journal of Computer Vision. arXiv admin  note: text overlap with arXiv:2307.08492&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;点云补全是3D视觉中的一个基本但尚未完全解决的问题。现有的方法通常依赖于3D坐标信息和/或其他数据（如图像和扫描视角）来填补缺失部分。与这些方法不同，我们探索了自结构增强，并提出了用于全局到局部点云补全的PointSea。&lt;h4&gt;背景&lt;/h4&gt;点云补全是3D视觉中一个基本但仍未完全解决的问题，现有方法通常依赖于额外的数据来进行补全。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法PointSea，利用自投影深度图进行数据增强，并通过特征融合模块从跨模态输入重建紧凑的全局形状，同时在局部阶段揭示高度详细的结构。&lt;h4&gt;方法&lt;/h4&gt;{'全局阶段': 'PointSea通过使用来自多个视角的自我投影深度图像来增强数据表示。它还集成了一种特性融合模块以融合跨视图和同视图级别特征，以便从跨模态输入中重建紧凑的整体形状。', '局部阶段': '在局部阶段，为了揭示高度详细的结构，我们引入了一个名为自结构对偶生成器的点生成器。该生成器结合了学习到的形状先验知识和几何自相似性来进行形状细化。与现有技术使用统一策略不同的是，我们的双路径设计根据每个点的结构类型适应不同的细化策略。', '创新': 'PointSea提出了一种新的方法来处理全局到局部点云补全的问题，通过利用自投影深度图增强数据表示，并采用特征融合模块和自结构对偶生成器进行形状细化。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，PointSea能够有效地理解整体形状并从不完整输入中生成详细信息，明显优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;提出的PointSea在广泛的基准测试中展示了优越的表现，证明了其在全球和局部点云补全中的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud completion is a fundamental yet not well-solved problem in 3Dvision. Current approaches often rely on 3D coordinate information and/oradditional data (e.g., images and scanning viewpoints) to fill in missingparts. Unlike these methods, we explore self-structure augmentation and proposePointSea for global-to-local point cloud completion. In the global stage,consider how we inspect a defective region of a physical object, we may observeit from various perspectives for a better understanding. Inspired by this,PointSea augments data representation by leveraging self-projected depth imagesfrom multiple views. To reconstruct a compact global shape from the cross-modalinput, we incorporate a feature fusion module to fuse features at bothintra-view and inter-view levels. In the local stage, to reveal highly detailedstructures, we introduce a point generator called the self-structuredual-generator. This generator integrates both learned shape priors andgeometric self-similarities for shape refinement. Unlike existing efforts thatapply a unified strategy for all points, our dual-path design adapts refinementstrategies conditioned on the structural type of each point, addressing thespecific incompleteness of each point. Comprehensive experiments on widely-usedbenchmarks demonstrate that PointSea effectively understands global shapes andgenerates local details from incomplete input, showing clear improvements overexisting methods.</description>
      <author>example@mail.com (Zhe Zhu, Honghua Chen, Xing He, Mingqiang Wei)</author>
      <guid isPermaLink="false">2502.17053v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Closer Look at TabPFN v2: Strength, Limitation, and Extension</title>
      <link>http://arxiv.org/abs/2502.17361v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文对基于Transformer的Tabular Prior-data Fitted Network v2 (TabPFN v2)模型进行了详尽评估，确认其在小规模至中等规模任务中的卓越泛化能力。&lt;h4&gt;背景&lt;/h4&gt;表格数据集具有内在异质性，给预训练基础模型的发展带来了巨大挑战。最近引入的基于Transformer的Tabular Prior-data Fitted Network v2 (TabPFN v2) 在多个表格数据集中实现了前所未有的上下文学习准确度。&lt;h4&gt;目的&lt;/h4&gt;全面评估TabPFN v2在超过300个数据集上的性能，揭示其成功的机制，并提出扩大其适用性的策略。&lt;h4&gt;方法&lt;/h4&gt;采用随机化特征标记将异质性数据集统一为固定维度表示；通过leave-one-fold-out方法将其转化为特征提取器；引入Chain-of-Thought提示的分而治之机制以支持大规模任务。&lt;h4&gt;主要发现&lt;/h4&gt;分析显示，随机化特征令牌是TabPFN v2成功的关键因素。此外，该模型能够简化数据分布并提高准确性。&lt;h4&gt;结论&lt;/h4&gt;通过揭示TabPFN v2背后的机制，并提出策略来扩大其适用范围，这项研究为未来表格基础模型的发展提供了关键见解。&lt;h4&gt;翻译&lt;/h4&gt;表格数据集具有内在异质性，给预训练基础模型的发展带来了巨大挑战。最近引入的基于Transformer的Tabular Prior-data Fitted Network v2 (TabPFN v2) 在多个表格数据集中实现了前所未有的上下文学习准确度，标志着表格基础模型的重要进展。在该论文中，我们全面评估了TabPFN v2在超过300个数据集上的性能，确认其卓越的小到中等规模任务的泛化能力。我们的分析确定随机化特征令牌是TabPFN v2成功的关键因素，因为它们将异质性表格数据集统一为固定维度表示，从而更有效的训练和推理。为了进一步理解TabPFN v2的预测结果，我们提出了一种leave-one-fold-out方法，使TabPFN v2转变为一个特征提取器，并揭示其简化数据分布并提高准确性的能力。最后，针对TabPFN v2在高维、大规模和多类别任务中的局限性，我们引入了受Chain-of-Thought提示启发的分而治之机制，实现可扩展推理。通过揭示TabPFN v2成功背后的机制并提出策略来扩大其适用范围，这项研究为未来表格基础模型的发展提供了关键见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tabular datasets are inherently heterogeneous, posing significant challengesfor developing pre-trained foundation models. The recently introducedtransformer-based Tabular Prior-data Fitted Network v2 (TabPFN v2) achievesunprecedented in-context learning accuracy across multiple tabular datasets,marking a pivotal advancement in tabular foundation models. In this paper, wecomprehensively evaluate TabPFN v2 on over 300 datasets, confirming itsexceptional generalization capabilities on small- to medium-scale tasks. Ouranalysis identifies randomized feature tokens as a key factor behind TabPFNv2's success, as they unify heterogeneous datasets into a fixed-dimensionalrepresentation, enabling more effective training and inference. To furtherunderstand TabPFN v2's predictions, we propose a leave-one-fold-out approach,transforming TabPFN v2 into a feature extractor and revealing its capability tosimplify data distributions and boost accuracy. Lastly, to address TabPFN v2'slimitations in high-dimensional, large-scale, and many-category tasks, weintroduce a divide-and-conquer mechanism inspired by Chain-of-Thoughtprompting, enabling scalable inference. By uncovering the mechanisms behindTabPFN v2's success and introducing strategies to expand its applicability,this study provides key insights into the future of tabular foundation models.</description>
      <author>example@mail.com (Han-Jia Ye, Si-Yang Liu, Wei-Lun Chao)</author>
      <guid isPermaLink="false">2502.17361v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>In-context learning of evolving data streams with tabular foundational models</title>
      <link>http://arxiv.org/abs/2502.16840v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;监督分类中的数据流挖掘传统上依赖于增量决策树集成。然而，大型表格模型（即为结构化数值数据设计的transformer）标志着一个重要的范式转变。&lt;h4&gt;目的&lt;/h4&gt;探索实时模型适应性，并探讨transformer在动态环境下的自适应学习能力。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模型和在线提示调整进行上下文学习。通过利用滑动窗口内存策略，TabPFN能够有效处理无限流数据。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，TabPFN结合简单的滑动内存策略，在所有非平稳基准测试中始终优于Hoeffding树集成。&lt;h4&gt;结论&lt;/h4&gt;论文概述了几种有前景的研究方向，并鼓励社区探索这些想法，以便在上下文流学习方面取得进展。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了当前监督分类中的数据流挖掘技术从传统的增量决策树转向大型表格模型（transformer）的转变。通过引入在线提示调整和预训练模型来处理无界流数据，实现了实时模型适应性研究，并展示了TabPFN在这种场景下的优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; State-of-the-art data stream mining in supervised classification hastraditionally relied on ensembles of incremental decision trees. However, theemergence of large tabular models, i.e., transformers designed for structurednumerical data, marks a significant paradigm shift. These models move beyondtraditional weight updates, instead employing in-context learning throughprompt tuning. By using on-the-fly sketches to summarize unbounded streamingdata, one can feed this information into a pre-trained model for efficientprocessing. This work bridges advancements from both areas, highlighting howtransformers' implicit meta-learning abilities, pre-training on driftingnatural data, and reliance on context optimization directly address the corechallenges of adaptive learning in dynamic environments. Exploring real-timemodel adaptation, this research demonstrates that TabPFN, coupled with a simplesliding memory strategy, consistently outperforms ensembles of Hoeffding treesacross all non-stationary benchmarks. Several promising research directions areoutlined in the paper. The authors urge the community to explore these ideas,offering valuable opportunities to advance in-context stream learning.</description>
      <author>example@mail.com (Afonso Lourenço, João Gama, Eric P. Xing, Goreti Marreiros)</author>
      <guid isPermaLink="false">2502.16840v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>VGFL-SA: Vertical Graph Federated Learning Structure Attack Based on Contrastive Learning</title>
      <link>http://arxiv.org/abs/2502.16793v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究提出了一种名为VGFL-SA的新颖图对抗攻击，旨在通过修改本地客户端的结构而不使用标签信息来降低垂直联邦学习（VGFL）框架的性能。&lt;h4&gt;背景&lt;/h4&gt;由于隐私保护和利益冲突，需要开发出能够在不直接分享图数据的情况下进行协作训练的垂直联邦图神经网络。现有的对抗性攻击依赖于标签信息的有效性受到限制，这在实际应用中存在局限性。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有VGFL框架中的未标记客户端问题并提高其安全性，研究人员提出了一种新的对抗攻击方法。&lt;h4&gt;方法&lt;/h4&gt;研究者采用对比学习的方法，在本地客户端训练之前完成攻击任务。具体来说，该方法利用图结构和节点特征信息生成对比视图，并通过共享的图编码器获取每个视图的嵌入表示，进而获得邻接矩阵的梯度并生成扰动边。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，提出的VGFL-SA在现实世界数据集上的节点分类任务中展现了良好的攻击效果和可转移性。&lt;h4&gt;结论&lt;/h4&gt;通过对比学习技术完成无标签信息参与的图对抗攻击可以有效地降低基于垂直联邦框架下GNNs模型的学习性能，这为未来的安全研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Neural Networks (GNNs) have gained attention for their ability to learnrepresentations from graph data. Due to privacy concerns and conflicts ofinterest that prevent clients from directly sharing graph data with oneanother, Vertical Graph Federated Learning (VGFL) frameworks have beendeveloped. Recent studies have shown that VGFL is vulnerable to adversarialattacks that degrade performance. However, it is a common problem that clientnodes are often unlabeled in the realm of VGFL. Consequently, the existingattacks, which rely on the availability of labeling information to obtaingradients, are inherently constrained in their applicability. This limitationprecludes their deployment in practical, real-world environments. To addressthe above problems, we propose a novel graph adversarial attack against VGFL,referred to as VGFL-SA, to degrade the performance of VGFL by modifying thelocal clients structure without using labels. Specifically, VGFL-SA uses acontrastive learning method to complete the attack before the local clients aretrained. VGFL-SA first accesses the graph structure and node featureinformation of the poisoned clients, and generates the contrastive views bynode-degree-based edge augmentation and feature shuffling augmentation. Then,VGFL-SA uses the shared graph encoder to get the embedding of each view, andthe gradients of the adjacency matrices are obtained by the contrastivefunction. Finally, perturbed edges are generated using gradient modificationrules. We validated the performance of VGFL-SA by performing a nodeclassification task on real-world datasets, and the results show that VGFL-SAachieves good attack effectiveness and transferability.</description>
      <author>example@mail.com (Yang Chen, Bin Zhou)</author>
      <guid isPermaLink="false">2502.16793v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MetaSym: A Symplectic Meta-learning Framework for Physical Intelligence</title>
      <link>http://arxiv.org/abs/2502.16667v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8+10 pages, 5 figures, 4 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的深度学习架构MetaSym，结合了强对称归纳偏差和自回归解码器，用于解决物理感知深度学习的挑战。&lt;h4&gt;背景&lt;/h4&gt;具有广泛应用领域的可扩展且通用的物理感知深度学习长期以来一直被视为重大难题。几乎所有物理系统的核心都是辛形式，它支撑着能量、动量等基本不变性。&lt;h4&gt;目的&lt;/h4&gt;引入MetaSym架构，确保核心物理不变性的完整性和灵活的数据高效适应系统异质性。&lt;h4&gt;方法&lt;/h4&gt;将一个获得自对称编码器的强辛归纳偏差和一个具有元注意力机制的自回归解码器相结合来构建新型深度学习架构MetaSym。&lt;h4&gt;主要发现&lt;/h4&gt;在包括高维弹簧网格系统、开放量子系统以及四旋翼动态等多样化数据集上的基准测试中，MetaSym表现出色，在少样本适应情况下模型性能优于现有的最先进的基线方法，并且使用远小于这些基线方法的规模模型就达到了这一效果。&lt;h4&gt;结论&lt;/h4&gt;提出的MetaSym架构在物理感知深度学习任务上具有显著优势，尤其适用于需要灵活适应系统异质性的场景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scalable and generalizable physics-aware deep learning has long beenconsidered a significant challenge with various applications across diversedomains ranging from robotics to molecular dynamics. Central to almost allphysical systems are symplectic forms, the geometric backbone that underpinsfundamental invariants like energy and momentum. In this work, we introduce anovel deep learning architecture, MetaSym. In particular, MetaSym combines astrong symplectic inductive bias obtained from a symplectic encoder and anautoregressive decoder with meta-attention. This principled design ensures thatcore physical invariants remain intact while allowing flexible, data-efficientadaptation to system heterogeneities. We benchmark MetaSym on highly varieddatasets such as a high-dimensional spring mesh system (Otness et al., 2021),an open quantum system with dissipation and measurement backaction, androbotics-inspired quadrotor dynamics. Our results demonstrate superiorperformance in modeling dynamics under few-shot adaptation, outperformingstate-of-the-art baselines with far larger models.</description>
      <author>example@mail.com (Pranav Vaidhyanathan, Aristotelis Papatheodorou, Mark T. Mitchison, Natalia Ares, Ioannis Havoutis)</author>
      <guid isPermaLink="false">2502.16667v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>RELICT: A Replica Detection Framework for Medical Image Generation</title>
      <link>http://arxiv.org/abs/2502.17360v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要翻译&lt;/h4&gt;尽管合成医疗数据在增强和提高深度学习模型的泛化能力方面具有潜力，但生成模型中的记忆效应可能导致敏感患者信息意外泄露，并限制了模型的实用性。因此，在医学领域使用能够记住训练数据的生成模型可能会危及患者的隐私。&lt;h4&gt;背景&lt;/h4&gt;在医疗领域，利用合成医疗数据来增强和提高深度学习模型的泛化能力是一个潜在的重要研究方向。然而，生成模型中存在的记忆问题可能导致敏感患者信息的泄露，并限制了这些模型的实际应用价值。&lt;h4&gt;目的&lt;/h4&gt;提出一种框架用于识别合成医学图像数据集中的副本（即与训练数据几乎相同的近似拷贝），旨在为医疗成像领域负责任和伦理地使用合成图像提供标准化且易于使用的工具。&lt;h4&gt;方法&lt;/h4&gt;RELICT框架通过三种互补的方法评估图像的相似性：1）体素级别分析；2）由预训练的医学基础模型进行特征级别分析；3）分割级别分析。针对两种临床相关的三维生成建模应用场景进行了研究：非对比头CT与脑内出血（N=774）和圈套动脉的时间飞跃磁共振血管成像（TOF-MRA，N=1,782）。使用专家视觉评分作为参考标准来评估副本的存在。&lt;h4&gt;主要发现&lt;/h4&gt;对于NCCT用例，在选择了最佳阈值的情况下，图像级别和特征级别测量方法可以完美地分类副本，平衡准确率为1；而对于TOF-MRA案例，则无法在任何阈值下实现完美的副本分类，但分割级别分析的平衡准确性为0.79。&lt;h4&gt;结论&lt;/h4&gt;副本检测是生成模型开发中的一个关键但被忽视的验证步骤。RELICT框架提供了一个标准化、易于使用的工具来识别副本，并旨在促进医学图像合成的责任感和伦理规范。&lt;h4&gt;其他细节&lt;/h4&gt;本研究强调了在医疗影像领域发展生成模型时，防止敏感信息泄露的重要性，并提出了一种新的评估方法用于检测合成数据集中可能出现的真实训练数据副本。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Despite the potential of synthetic medical data for augmenting and improvingthe generalizability of deep learning models, memorization in generative modelscan lead to unintended leakage of sensitive patient information and limit modelutility. Thus, the use of memorizing generative models in the medical domaincan jeopardize patient privacy. We propose a framework for identifyingreplicas, i.e. nearly identical copies of the training data, in syntheticmedical image datasets. Our REpLIca deteCTion (RELICT) framework for medicalimage generative models evaluates image similarity using three complementaryapproaches: (1) voxel-level analysis, (2) feature-level analysis by apretrained medical foundation model, and (3) segmentation-level analysis. Twoclinically relevant 3D generative modelling use cases were investigated:non-contrast head CT with intracerebral hemorrhage (N=774) and time-of-flightMR angiography of the Circle of Willis (N=1,782). Expert visual scoring wasused as the reference standard to assess the presence of replicas. We reportthe balanced accuracy at the optimal threshold to assess replica classificationperformance. The reference visual rating identified 45 of 50 and 5 of 50generated images as replicas for the NCCT and TOF-MRA use cases, respectively.Image-level and feature-level measures perfectly classified replicas with abalanced accuracy of 1 when an optimal threshold was selected for the NCCT usecase. A perfect classification of replicas for the TOF-MRA case was notpossible at any threshold, with the segmentation-level analysis achieving abalanced accuracy of 0.79. Replica detection is a crucial but neglectedvalidation step for the development of generative models in medical imaging.The proposed RELICT framework provides a standardized, easy-to-use tool forreplica detection and aims to facilitate responsible and ethical medical imagesynthesis.</description>
      <author>example@mail.com (Orhun Utku Aydin, Alexander Koch, Adam Hilbert, Jana Rieger, Felix Lohrke, Fujimaro Ishida, Satoru Tanioka, Dietmar Frey)</author>
      <guid isPermaLink="false">2502.17360v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Cross-domain Few-shot Object Detection with Multi-modal Textual Enrichment</title>
      <link>http://arxiv.org/abs/2502.16469v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2403.16188&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于元学习的框架，通过引入丰富的文本语义作为辅助模态来解决跨域多模态少样本目标检测中的领域偏移问题。&lt;h4&gt;背景&lt;/h4&gt;当前的多模态物体检测方法在遇到显著的领域变化时会表现出性能下降。现有的跨模态特征提取和集成的进步提高了少量样本学习任务的表现，但仍然面临挑战。&lt;h4&gt;目的&lt;/h4&gt;通过结合丰富的文本信息来增强模型建立视觉实例与其语言描述之间的知识关系的能力，从而减轻领域偏移带来的挑战。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新的框架用于解决跨域多模态少样本目标检测问题。该框架包含两个关键组件：一个多模态特征聚合模块和一个丰富文本语义修正模块。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在常见的跨域物体检测基准上，所提方法显著超越了现有的少样本物体检测方法。&lt;h4&gt;结论&lt;/h4&gt;通过引入元学习的框架并利用丰富的文本信息，论文成功地提高了模型在领域偏移情况下的适应性和准确性。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一个旨在解决跨域多模态少样本目标检测问题的方法。该方法结合了视觉和语言特征，并采用了文本语义修正模块来增强其性能。实验结果显示，在标准基准测试中，这种方法优于现有技术。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Advancements in cross-modal feature extraction and integration havesignificantly enhanced performance in few-shot learning tasks. However, currentmulti-modal object detection (MM-OD) methods often experience notableperformance degradation when encountering substantial domain shifts. We proposethat incorporating rich textual information can enable the model to establish amore robust knowledge relationship between visual instances and theircorresponding language descriptions, thereby mitigating the challenges ofdomain shift. Specifically, we focus on the problem of Cross-Domain Multi-ModalFew-Shot Object Detection (CDMM-FSOD) and introduce a meta-learning-basedframework designed to leverage rich textual semantics as an auxiliary modalityto achieve effective domain adaptation. Our new architecture incorporates twokey components: (i) A multi-modal feature aggregation module, which alignsvisual and linguistic feature embeddings to ensure cohesive integration acrossmodalities. (ii) A rich text semantic rectification module, which employsbidirectional text feature generation to refine multi-modal feature alignment,thereby enhancing understanding of language and its application in objectdetection. We evaluate the proposed method on common cross-domain objectdetection benchmarks and demonstrate that it significantly surpasses existingfew-shot object detection approaches.</description>
      <author>example@mail.com (Zeyu Shangguan, Daniel Seita, Mohammad Rostami)</author>
      <guid isPermaLink="false">2502.16469v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Utilizing AI and Machine Learning for Predictive Analysis of Post-Treatment Cancer Recurrence</title>
      <link>http://arxiv.org/abs/2502.15825v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  15 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;探讨了人工智能和机器学习在癌症复发预测中的应用，以及它们如何通过分析大量遗传学、临床表现和治疗数据来提高个性化医疗水平。&lt;h4&gt;背景&lt;/h4&gt;肿瘤复发是肿瘤学中一个主要挑战，传统的癌症复发预测依赖于统计模型支持的临床观察，但无法完全解释其复杂的多因素特性。&lt;h4&gt;目的&lt;/h4&gt;研究AI和ML在癌症复发预测中的潜在应用，以改善治疗后的患者生存率和生活质量。&lt;h4&gt;方法&lt;/h4&gt;描述了使用监督学习和无监督学习技术来识别模式并预测癌症患者的结局的各种AI和ML技术。&lt;h4&gt;主要发现&lt;/h4&gt;AI和ML技术能够提供早期干预的机会，并有助于设计更有效的治疗计划。&lt;h4&gt;结论&lt;/h4&gt;AI和ML为个性化医学和患者管理提供了新的机会，提高了复发预测的准确性和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容已从英文翻译为中文。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.60087/jklst.vol2.n3.p599&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In oncology, recurrence after treatment is one of the major challenges,related to patients' survival and quality of life. Conventionally, predictionof cancer relapse has always relied on clinical observation with statisticalmodel support, which almost fails to explain the complex, multifactorial natureof tumor recurrence. This research explores how AI and ML models may increasethe accuracy and reliability of recurrence prediction in cancer. Therefore, AIand ML create new opportunities not only for personalized medicine but also forproactive management of patients through analyzing large volumes of data ongenetics, clinical manifestations, and treatment. The paper describes thevarious AI and ML techniques for pattern identification and outcome predictionin cancer patients using supervised and unsupervised learning. Clinicalimplications provide an opportunity to review how early interventions couldhappen and the design of treatment planning.</description>
      <author>example@mail.com (Muhammad Umer Qayyum, Muhammad Fahad, Nasrullah Abbasi)</author>
      <guid isPermaLink="false">2502.15825v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Trunk-branch Contrastive Network with Multi-view Deformable Aggregation for Multi-view Action Recognition</title>
      <link>http://arxiv.org/abs/2502.16493v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个新的称为TBCNet的框架，用于基于RGB多视角的动作识别。该网络通过主干和分支的对比学习过程获得融合特征，并补充关键细节。&lt;h4&gt;背景&lt;/h4&gt;传统的动作识别研究通常从每个视图中提取精炼特征，然后实现配对交互和整合，但这种方法可能会忽视每个视图中的重要局部特征。&lt;h4&gt;目的&lt;/h4&gt;为了模拟人类从多个角度观察物体时形成的综合印象以及随后填补具体细节的认知过程，提出了一种新的网络框架TBCNet。&lt;h4&gt;方法&lt;/h4&gt;设计了两个核心组件：多视角可变形聚集（MVDA）和主干-分支对比学习。 MVDA利用全局汇聚模块强调重要的空间信息，并通过复合相对位置偏差捕捉视图内的及跨视图的相对位置，而主干-分支对比损失则是在聚合特征与每个视图中的精炼细节之间构建。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示TBCNet在NTU-RGB+D 60, NTU-RGB+D 120, PKU-MMD和N-UCLA等四个数据集上优于其他基于RGB的方法，尤其在跨主体（Cross-Subject）及跨场景（Cross-View）协议下取得了最先进的性能。&lt;h4&gt;结论&lt;/h4&gt;该研究为RGB多视角动作识别提供了一种有效的新方法TBCNet，并通过实验验证了其优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-view action recognition aims to identify actions in a given multi-viewscene. Traditional studies initially extracted refined features from each view,followed by implemented paired interaction and integration, but theypotentially overlooked the critical local features in each view. When observingobjects from multiple perspectives, individuals typically form a comprehensiveimpression and subsequently fill in specific details. Drawing inspiration fromthis cognitive process, we propose a novel trunk-branch contrastive network(TBCNet) for RGB-based multi-view action recognition. Distinctively, TBCNetfirst obtains fused features in the trunk block and then implicitly supplementsvital details provided by the branch block via contrastive learning, generatinga more informative and comprehensive action representation. Within thisframework, we construct two core components: the multi-view deformableaggregation and the trunk-branch contrastive learning. MVDA employed in thetrunk block effectively facilitates multi-view feature fusion and adaptivecross-view spatio-temporal correlation, where a global aggregation module isutilized to emphasize significant spatial information and a composite relativeposition bias is designed to capture the intra- and cross-view relativepositions. Moreover, a trunk-branch contrastive loss is constructed betweenaggregated features and refined details from each view. By incorporating twodistinct weights for positive and negative samples, a weighted trunk-branchcontrastive loss is proposed to extract valuable information and emphasizesubtle inter-class differences. The effectiveness of TBCNet is verified byextensive experiments on four datasets including NTU-RGB+D 60, NTU-RGB+D 120,PKU-MMD, and N-UCLA dataset. Compared to other RGB-based methods, our approachachieves state-of-the-art performance in cross-subject and cross-settingprotocols.</description>
      <author>example@mail.com (Yingyuan Yang, Guoyuan Liang, Can Wang, Xiaojun Wu)</author>
      <guid isPermaLink="false">2502.16493v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Layer-Wise Evolution of Representations in Fine-Tuned Transformers: Insights from Sparse AutoEncoders</title>
      <link>http://arxiv.org/abs/2502.16722v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了预训练变压器微调过程中的内部机制，特别是BERT模型，并通过分析激活相似性、训练稀疏自编码器以及可视化不同层的标记级激活来探索这一过程。&lt;h4&gt;背景&lt;/h4&gt;微调预训练的变换器是增强基础模型在特定任务上性能的强大技术。这种方法对于将通用架构适应于专门的任务非常关键，从早期应用到如BERT这样的模型到现在用于大型语言模型（LLM）的应用。&lt;h4&gt;目的&lt;/h4&gt;理解微调过程对于揭示变压器如何根据具体目标进行调整、保留一般表示以及获取任务特有特征至关重要。&lt;h4&gt;方法&lt;/h4&gt;论文通过分析激活相似性、训练稀疏自编码器和可视化不同层的标记级激活来探索微调机制，特别是针对BERT变换器。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示随着深度增加，特征如何适应任务的变化：早期层次主要保留一般表示；中间层次充当通用与任务特有特征之间的过渡；后期层次完全专注于任务适应。&lt;h4&gt;结论&lt;/h4&gt;这些发现在理解微调过程和它对转换架构内表征学习的影响方面提供了关键见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Fine-tuning pre-trained transformers is a powerful technique for enhancingthe performance of base models on specific tasks. From early applications inmodels like BERT to fine-tuning Large Language Models (LLMs), this approach hasbeen instrumental in adapting general-purpose architectures for specializeddownstream tasks. Understanding the fine-tuning process is crucial foruncovering how transformers adapt to specific objectives, retain generalrepresentations, and acquire task-specific features. This paper explores theunderlying mechanisms of fine-tuning, specifically in the BERT transformer, byanalyzing activation similarity, training Sparse AutoEncoders (SAEs), andvisualizing token-level activations across different layers. Based onexperiments conducted across multiple datasets and BERT layers, we observe asteady progression in how features adapt to the task at hand: early layersprimarily retain general representations, middle layers act as a transitionbetween general and task-specific features, and later layers fully specializein task adaptation. These findings provide key insights into the inner workingsof fine-tuning and its impact on representation learning within transformerarchitectures.</description>
      <author>example@mail.com (Suneel Nadipalli)</author>
      <guid isPermaLink="false">2502.16722v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Learning of English Language and Crystal Graphs for Multimodal Representation of Materials Knowledge</title>
      <link>http://arxiv.org/abs/2502.16451v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  24 pages, 14 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了用于材料逆向设计的人工智能（AI）在晶体领域的应用，提出了对比语言-晶体模型CLaC，并通过实验验证了其优越性。&lt;h4&gt;背景&lt;/h4&gt;人工智能技术越来越多地应用于材料的逆向设计中，尤其是在分子领域，已经成功将化学结构与文本知识结合使用。然而，在晶体研究方面，由于偏斜的数据分布和学术文献中的语义监督不足，这种方法难以实现。&lt;h4&gt;目的&lt;/h4&gt;为了克服数据稀缺问题，并展示合成数据在解决这一问题上的优势，提出了一种新的对比语言-晶体模型CLaC。&lt;h4&gt;方法&lt;/h4&gt;通过构建包含126k晶体结构文本对的新合成数据集和一个从学术论文中提取的相似数据集，预训练了CLaC模型。然后评估其跨模态任务和下游应用中的零样本泛化能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，CLaC在理解晶体结构方面实现了最新的零样本泛化性能，并且超越了现有的大型语言模型。&lt;h4&gt;结论&lt;/h4&gt;所提出的CLaC模型展示了其在理解和设计晶体材料方面的潜力，为未来的AI辅助逆向材料设计提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的原文是关于介绍了一种对比语言-晶体模型（CLaC），该模型基于126K个合成的数据集进行预训练，并通过跨模态任务和下游应用验证了其优越性，特别是在零样本泛化性能方面超越了当前的大规模语言模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Artificial intelligence (AI) is increasingly used for the inverse design ofmaterials, such as crystals and molecules. Existing AI research on moleculeshas integrated chemical structures of molecules with textual knowledge to adaptto complex instructions. However, this approach has been unattainable forcrystals due to data scarcity from the biased distribution of investigatedcrystals and the lack of semantic supervision in peer-reviewed literature. Inthis work, we introduce a contrastive language-crystals model (CLaC)pre-trained on a newly synthesized dataset of 126k crystal structure-textpairs. To demonstrate the advantage of using synthetic data to overcome datascarcity, we constructed a comparable dataset extracted from academic papers.We evaluate CLaC's generalization ability through various zero-shot cross-modaltasks and downstream applications. In experiments, CLaC achievesstate-of-the-art zero-shot generalization performance in understanding crystalstructures, surpassing latest large language models.</description>
      <author>example@mail.com (Yang Jeong Park, Mayank Kumaran, Chia-Wei Hsu, Elsa Olivetti, Ju Li)</author>
      <guid isPermaLink="false">2502.16451v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Shakti-VLMs: Scalable Vision-Language Models for Enterprise AI</title>
      <link>http://arxiv.org/abs/2502.17092v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Shakti VLM是一个包含10亿和40亿参数的视觉-语言模型家族，旨在解决多模态学习中的数据效率挑战。&lt;h4&gt;背景&lt;/h4&gt;近年来，许多视觉-语言模型通过大量训练数据取得了优异的成绩。然而，在大规模数据集不可用的情况下，现有方法难以有效解决问题。&lt;h4&gt;目的&lt;/h4&gt;设计一种可以利用架构创新来减少对海量训练数据依赖的视觉-语言模型。&lt;h4&gt;方法&lt;/h4&gt;1. 使用QK-Normalization提高注意力机制的稳定性2. 引入混合归一化技术以增强模型性能3. 采用改进的位置编码提升多模态理解能力4. 实施三阶段训练策略优化学习效率&lt;h4&gt;主要发现&lt;/h4&gt;Shakti VLM-1B和Shakti VLM-4B在文档理解、视觉推理、光学字符识别提取以及通用的多模态推理任务中表现出色，证明了良好的模型设计和有效的训练策略同样可以实现高精度。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过精心设计的架构和技术创新可以使模型更加高效地处理大规模的多模态任务，并且不必依赖海量的数据集。Shakti VLM为解决企业级应用场景中的问题提供了一个高效的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要：我们介绍了Shakti VLM，这是一个参数容量分别为10亿和40亿的视觉-语言模型家族，旨在应对多模态学习中数据效率方面的挑战。尽管最近的一些视觉-语言模型通过大量训练数据取得了良好的成绩，但Shakti模型则利用架构创新，在较少的数据量下也能取得竞争性的结果。关键改进包括用于提高注意力机制稳定性的QK归一化技术、混合归一化方法以及增强的位置编码策略。此外，一种三阶段的训练策略进一步优化了学习效率。评估结果显示，无论是文档理解还是视觉推理等多模态任务，Shakti VLM-1B和Shakti VLM-4B均表现出色。我们的研究结果表明，通过模型设计和有效的训练策略可以实现高精度，而不需要依靠大量数据集的支持。这使得Shakti成为大规模多模态应用场景下的一种高效解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce Shakti VLM, a family of vision-language models in the capacityof 1B and 4B parameters designed to address data efficiency challenges inmultimodal learning. While recent VLMs achieve strong performance throughextensive training data, Shakti models leverage architectural innovations toattain competitive results with fewer tokens. Key advancements includeQK-Normalization for attention stability, hybrid normalization techniques, andenhanced positional encoding. A three-stage training strategy further optimizeslearning efficiency. Evaluations show that Shakti-Shakti-VLM-1B andShakti-VLM-4B excel in document understanding, Visual Reasoning, OCRextraction, and general multimodal reasoning. Our results highlight that highperformance can be achieved through model design and training strategy ratherthan sheer data volume, making Shakti an efficient solution forenterprise-scale multimodal tasks.</description>
      <author>example@mail.com (Syed Abdul Gaffar Shakhadri, Kruthika KR, Kartik Basavaraj Angadi)</author>
      <guid isPermaLink="false">2502.17092v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>DemoGen: Synthetic Demonstration Generation for Data-Efficient Visuomotor Policy Learning</title>
      <link>http://arxiv.org/abs/2502.16932v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project website: https://demo-generation.github.io&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种低成本的合成数据生成方法DemoGen，该方法能够在不需要大量人工采集的情况下自动生成演示任务，并通过3D点云和场景编辑来增强空间推广能力。&lt;h4&gt;背景&lt;/h4&gt;视觉运动策略在机器人操作中显示出巨大潜力，但由于其有限的空间泛化能力，通常需要大量的手工收集的数据以实现有效性能。&lt;h4&gt;目的&lt;/h4&gt;提出一个低成本、全合成的方法DemoGen，用于自动生成演示，仅需少量的人类收集的示例即可推广到新的对象配置上。&lt;h4&gt;方法&lt;/h4&gt;通过使用3D点云作为观察模式，并通过场景编辑重新排列主体来生成视觉观测。这种方法允许将已有的动作轨迹适应于新的物体布局中。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，DemoGen能够显著提高在各种现实世界操作任务中的策略性能，甚至包括复杂的情境如变形对象、灵巧的手末端执行器和双臂平台的操作。&lt;h4&gt;结论&lt;/h4&gt;除了改进空间推广能力外，DemoGen还可以扩展以提供额外的分布外功能，例如对干扰的抵抗能力和避障能力。&lt;h4&gt;翻译&lt;/h4&gt;视觉运动策略在机器人操作中已显示出巨大的潜力，但为了实现有效性能，通常需要大量的手工收集的数据。一个主要原因在于其有限的空间泛化能力，这要求必须跨越不同物体配置广泛地采集数据。在这项工作中，我们提出了DemoGen，这是一种低成本、完全合成的方法来自动生成演示。利用仅需一次的人工收集的示例，DemoGen通过适应已展示的动作轨迹到新的物体布局中来生成空间增强的演示。视觉观测是通过使用3D点云作为模态并重新排列场景中的主体以实现3D编辑的方式进行合成。实证上，DemoGen显著提升了在各种现实世界操作任务中的策略性能，并展示了其在涉及可变形对象、灵巧手末端执行器和双臂平台的挑战性情况下的应用潜力。此外，DemoGen可以扩展以提供额外的分布外功能，包括干扰抵抗能力和避障能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visuomotor policies have shown great promise in robotic manipulation butoften require substantial amounts of human-collected data for effectiveperformance. A key reason underlying the data demands is their limited spatialgeneralization capability, which necessitates extensive data collection acrossdifferent object configurations. In this work, we present DemoGen, a low-cost,fully synthetic approach for automatic demonstration generation. Using only onehuman-collected demonstration per task, DemoGen generates spatially augmenteddemonstrations by adapting the demonstrated action trajectory to novel objectconfigurations. Visual observations are synthesized by leveraging 3D pointclouds as the modality and rearranging the subjects in the scene via 3Dediting. Empirically, DemoGen significantly enhances policy performance acrossa diverse range of real-world manipulation tasks, showing its applicabilityeven in challenging scenarios involving deformable objects, dexterous handend-effectors, and bimanual platforms. Furthermore, DemoGen can be extended toenable additional out-of-distribution capabilities, including disturbanceresistance and obstacle avoidance.</description>
      <author>example@mail.com (Zhengrong Xue, Shuying Deng, Zhenyang Chen, Yixuan Wang, Zhecheng Yuan, Huazhe Xu)</author>
      <guid isPermaLink="false">2502.16932v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Adversarial Training for Defense Against Label Poisoning Attacks</title>
      <link>http://arxiv.org/abs/2502.17121v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at the International Conference on Learning Representations  (ICLR 2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的对抗训练防御策略FLORAL，该策略基于支持向量机（SVM）来应对模型训练过程中标签中毒攻击的威胁。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习模型越来越复杂且依赖于公共数据源进行训练，例如大规模语言模型使用的标注数据，这些模型更容易受到标签中毒攻击。这种攻击方式是通过对手微妙地改变训练集中的标签来进行的，这可能导致模型性能严重下降，在关键应用中造成重大风险。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的防御策略FLORAL来对抗由标签中毒造成的威胁。&lt;h4&gt;方法&lt;/h4&gt;基于支持向量机（SVM）和双层优化框架，将训练过程描述为攻防双方的非零和斯塔克伯格博弈。该方法适应多种模型架构，并使用带有核函数的支持向量机进行投影梯度下降算法以执行对抗训练。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明了算法收敛性的特性，实验结果证实FLORAL在各种分类任务中均能取得比Robust基线和RoBERTa等基础模型更好的鲁棒准确性。当攻击者预算增加时，FLORAL仍然能够保持更高的稳健精度。&lt;h4&gt;结论&lt;/h4&gt;FLORAL策略具有提高机器学习模型对抗标签中毒威胁的鲁棒性的潜力，在敌对环境中确保分类任务的安全性和稳定性。&lt;h4&gt;翻译&lt;/h4&gt;随着机器学习模型变得越来越复杂，并且越来越多地依赖于公开来源的数据，例如在训练大型语言模型时使用的由人类标注的标签，这些模型更容易受到标签中毒攻击。这种攻击方式是通过对手微妙地改变训练数据集中的标签来执行的，这可能导致模型性能严重下降，在关键应用中造成重大风险。在这篇论文中，我们提出了FLORAL策略，这是一种基于支持向量机（SVM）的新对抗性训练防御策略，用于应对这些威胁。使用双层优化框架，我们将训练过程描述为攻防双方之间的非零和斯塔克伯格博弈，一方是战略性地污染关键训练标签的攻击者，另一方是试图从这些攻击中恢复过来的模型。该方法适用于多种架构，并采用带有核函数的支持向量机进行投影梯度下降算法来进行对抗性训练。我们提供了该算法收敛性质的理论分析，并通过实验证明了FLORAL策略在各种分类任务中的有效性。与鲁棒基线和基础模型（如RoBERTa）相比，随着攻击者预算增加时，FLORAL始终能保持更高的稳健精度。这些结果强调了FLORAL提高机器学习模型对抗标签中毒威胁的韧性潜力，在敌对环境中确保分类任务的安全性和稳定性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As machine learning models grow in complexity and increasingly rely onpublicly sourced data, such as the human-annotated labels used in traininglarge language models, they become more vulnerable to label poisoning attacks.These attacks, in which adversaries subtly alter the labels within a trainingdataset, can severely degrade model performance, posing significant risks incritical applications. In this paper, we propose FLORAL, a novel adversarialtraining defense strategy based on support vector machines (SVMs) to counterthese threats. Utilizing a bilevel optimization framework, we cast the trainingprocess as a non-zero-sum Stackelberg game between an attacker, whostrategically poisons critical training labels, and the model, which seeks torecover from such attacks. Our approach accommodates various modelarchitectures and employs a projected gradient descent algorithm with kernelSVMs for adversarial training. We provide a theoretical analysis of ouralgorithm's convergence properties and empirically evaluate FLORAL'seffectiveness across diverse classification tasks. Compared to robust baselinesand foundation models such as RoBERTa, FLORAL consistently achieves higherrobust accuracy under increasing attacker budgets. These results underscore thepotential of FLORAL to enhance the resilience of machine learning modelsagainst label poisoning threats, thereby ensuring robust classification inadversarial settings.</description>
      <author>example@mail.com (Melis Ilayda Bal, Volkan Cevher, Michael Muehlebach)</author>
      <guid isPermaLink="false">2502.17121v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Unified Semantic and ID Representation Learning for Deep Recommenders</title>
      <link>http://arxiv.org/abs/2502.16474v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合ID和语义表示的推荐系统框架，旨在解决传统基于ID令牌的推荐系统的冗余问题以及新项目冷启动时的表现不佳的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的推荐系统依赖于ID令牌来唯一标识项目，但在处理项目重复和新项目的推荐方面存在不足。最近的方法尝试使用语义令牌作为替代方案，但面临挑战如项目复制和不一致的性能提升。&lt;h4&gt;目的&lt;/h4&gt;开发一种综合了ID和语义表示的学习框架以克服现有方法的局限性，并探索余弦相似度和欧几里得距离在嵌入搜索中的作用。&lt;h4&gt;方法&lt;/h4&gt;提出了一个统一的ID与语义表示学习框架，该框架利用两种令牌类型的优势。在这个框架中，ID令牌捕捉项目独特属性，而语义令牌则代表共享、可转移的特点。此外，还分析了余弦相似度和欧几里得距离在嵌入搜索中的作用，并整合这两种方法来优化表示学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法显著优于现有的基准模型，在三个基准数据集上的性能提高了6%至17%，并且令牌大小减少了超过80%。&lt;h4&gt;结论&lt;/h4&gt;本文证明了将ID和语义标记结合可以增强推荐系统的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;有效的推荐对大型在线平台至关重要。传统的推荐系统主要依赖于标识符（ID）令牌来唯一识别项目，能够有效捕捉特定项目的联系，但在冗余性和冷启动场景中的表现不佳。最近的研究探索了使用语义令牌作为替代方法，但面临诸如项目复制和不一致性能收益的问题。为解决这些局限性，本文提出了一种综合ID与语义表示的学习框架，利用两种标记类型的优势。实验显示该方法在三个基准数据集上显著优于现有基线模型，改善幅度从6%到17%，并且令牌大小减少了超过80%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Effective recommendation is crucial for large-scale online platforms.Traditional recommendation systems primarily rely on ID tokens to uniquelyidentify items, which can effectively capture specific item relationships butsuffer from issues such as redundancy and poor performance in cold-startscenarios. Recent approaches have explored using semantic tokens as analternative, yet they face challenges, including item duplication andinconsistent performance gains, leaving the potential advantages of semantictokens inadequately examined. To address these limitations, we propose aUnified Semantic and ID Representation Learning framework that leverages thecomplementary strengths of both token types. In our framework, ID tokenscapture unique item attributes, while semantic tokens represent shared,transferable characteristics. Additionally, we analyze the role of cosinesimilarity and Euclidean distance in embedding search, revealing that cosinesimilarity is more effective in decoupling accumulated embeddings, whileEuclidean distance excels in distinguishing unique items. Our frameworkintegrates cosine similarity in earlier layers and Euclidean distance in thefinal layer to optimize representation learning. Experiments on three benchmarkdatasets show that our method significantly outperforms state-of-the-artbaselines, with improvements ranging from 6\% to 17\% and a reduction in tokensize by over 80%. These results demonstrate the effectiveness of combining IDand semantic tokenization to enhance the generalization ability of recommendersystems.</description>
      <author>example@mail.com (Guanyu Lin, Zhigang Hua, Tao Feng, Shuang Yang, Bo Long, Jiaxuan You)</author>
      <guid isPermaLink="false">2502.16474v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Separated Contrastive Learning for Matching in Cross-domain Recommendation with Curriculum Scheduling</title>
      <link>http://arxiv.org/abs/2502.16239v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by TheWebConf 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种名为SCCDR的新框架，用于解决跨域推荐任务中的训练不稳定问题。&lt;h4&gt;背景&lt;/h4&gt;跨域推荐(CDR)旨在通过利用源领域信息来改善目标领域的推荐性能。对比学习方法在处理同一领域内的用户或项目时被广泛采用，并且对于知识迁移和表示学习也很有效。&lt;h4&gt;目的&lt;/h4&gt;解决直接应用对比学习于混合的同域内和跨域任务所带来的训练不稳定问题，这会导致表示学习过程恶化以及生成嵌入质量降低的问题。&lt;h4&gt;方法&lt;/h4&gt;SCCDR基于分离的同域内和跨域内的对比学习模式以及一个停止梯度操作来处理这一不足。该框架包括两个专门的课程阶段：同异域分离和跨域课程调度。前者为源域和目标域分别使用了两种不同的对比视角，后者则通过考虑重叠用户所锚定的负样本难度，采用了课程调度策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明SCCDR在多个基准上达到了最新的性能水平，并且在线A/B测试也验证了这一点。&lt;h4&gt;结论&lt;/h4&gt;提出的框架能够有效地解决跨域推荐任务中的训练不稳定问题，并提高表示学习的质量和生成嵌入的效果，从而提升整体推荐系统的性能。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了SCCDR框架的创新方法及其在解决跨域推荐（CDR）中对比学习时遇到的问题方面的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3701716.3715260&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Cross-domain recommendation (CDR) is a task that aims to improve therecommendation performance in a target domain by leveraging the informationfrom source domains. Contrastive learning methods have been widely adoptedamong intra-domain (intra-CL) and inter-domain (inter-CL) users/items for theirrepresentation learning and knowledge transfer during the matching stage ofCDR. However, we observe that directly employing contrastive learning onmixed-up intra-CL and inter-CL tasks ignores the difficulty of learning frominter-domain over learning from intra-domain, and thus could cause severetraining instability. Therefore, this instability deteriorates therepresentation learning process and hurts the quality of generated embeddings.To this end, we propose a novel framework named SCCDR built up on a separatedintra-CL and inter-CL paradigm and a stop-gradient operation to handle thedrawback. Specifically, SCCDR comprises two specialized curriculum stages:intra-inter separation and inter-domain curriculum scheduling. The former stageexplicitly uses two distinct contrastive views for the intra-CL task in thesource and target domains, respectively. Meanwhile, the latter stagedeliberately tackles the inter-CL tasks with a curriculum scheduling strategythat derives effective curricula by accounting for the difficulty of negativesamples anchored by overlapping users. Empirical experiments on variousopen-source datasets and an offline proprietary industrial dataset extractedfrom a real-world recommender system, and an online A/B test verify that SCCDRachieves state-of-the-art performance over multiple baselines.</description>
      <author>example@mail.com (Heng Chang, Liang Gu, Cheng Hu, Zhinan Zhang, Hong Zhu, Yuhui Xu, Yuan Fang, Zhen Chen)</author>
      <guid isPermaLink="false">2502.16239v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Set a Thief to Catch a Thief: Combating Label Noise through Noisy Meta Learning</title>
      <link>http://arxiv.org/abs/2502.16104v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一个新颖的噪声元标签校正框架STCT，旨在利用带有噪声的数据来纠正标签错误，并通过实验验证了其在高噪声率场景下的卓越性能。&lt;h4&gt;背景&lt;/h4&gt;从嘈杂的标签中学习（LNL）的目标是使用带噪数据集训练高性能深度模型。基于元学习的方法已经在LNL任务上表现出色，但需要额外的干净验证集来执行标签校正，这限制了其实用性。&lt;h4&gt;目的&lt;/h4&gt;为了克服这一问题，本文提出了一种新颖的噪声元标签校正框架STCT，该框架可以使用带噪数据作为验证集，并在不依赖于额外干净数据的情况下进行标签校正。&lt;h4&gt;方法&lt;/h4&gt;STCT通过将复杂的双层优化分解为表示学习和标签校正两个部分，并采用交替训练策略来解决这个问题。具体来说，在元学习框架中利用与训练数据独立同分布的噪声数据作为验证集评估模型性能并执行标签校正。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，STCT在合成数据集和真实世界数据集上展示了卓越的表现，特别是在高噪声率场景下。当CIFAR-10数据集中含有80%对称噪声时，STCT的标签校正准确率为96.9%，分类性能为95.2%，显著优于现有最佳方法。&lt;h4&gt;结论&lt;/h4&gt;与现有的基于元学习的LNL方法相比，所提出的STCT框架通过利用带噪数据进行自我纠正，在不依赖额外干净验证集的情况下实现了更优的标签校正和模型训练效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Learning from noisy labels (LNL) aims to train high-performance deep modelsusing noisy datasets. Meta learning based label correction methods havedemonstrated remarkable performance in LNL by designing various meta labelrectification tasks. However, extra clean validation set is a prerequisite forthese methods to perform label correction, requiring extra labor and greatlylimiting their practicality. To tackle this issue, we propose a novel noisymeta label correction framework STCT, which counterintuitively uses noisy datato correct label noise, borrowing the spirit in the saying ``Set a Thief toCatch a Thief''. The core idea of STCT is to leverage noisy data which isi.i.d. with the training data as a validation set to evaluate model performanceand perform label correction in a meta learning framework, eliminating the needfor extra clean data. By decoupling the complex bi-level optimization in metalearning into representation learning and label correction, STCT is solvedthrough an alternating training strategy between noisy meta correction andsemi-supervised representation learning. Extensive experiments on synthetic andreal-world datasets demonstrate the outstanding performance of STCT,particularly in high noise rate scenarios. STCT achieves 96.9% label correctionand 95.2% classification performance on CIFAR-10 with 80% symmetric noise,significantly surpassing the current state-of-the-art.</description>
      <author>example@mail.com (Hanxuan Wang, Na Lu, Xueying Zhao, Yuxuan Yan, Kaipeng Ma, Kwoh Chee Keong, Gustavo Carneiro)</author>
      <guid isPermaLink="false">2502.16104v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Unveiling Institution-Specific Bias in Pathology Foundation Models: Detriments, Causes, and Potential Solutions</title>
      <link>http://arxiv.org/abs/2502.16889v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages,1 figure,14 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;病理基础模型在提取有价值的区别性特征方面具有优势，但存在图像特异性信息污染的问题，影响了其泛化能力。&lt;h4&gt;背景&lt;/h4&gt;病理基础模型简化了深度学习模型的开发，并通过利用先验知识提高了诊断准确性。然而，在实际临床应用中，由于机构特定的信息干扰，这些模型的表现可能会下降。&lt;h4&gt;目的&lt;/h4&gt;揭示病理基础模型中的特征污染问题及其对性能的影响，并探讨其背后的原因及可能的解决方案。&lt;h4&gt;方法&lt;/h4&gt;识别并验证了病理图像中的机构特定信息如何被当前的基础模型捕捉到，通过广泛的实验展示了非诊断相关的信息在出界分布场景下对性能的负面影响。&lt;h4&gt;主要发现&lt;/h4&gt;病理基础模型容易提取与疾病无关但又存在于不同医疗机构之间的特征信息。这些污染导致了虚假的相关性，削弱了模型的应用能力。&lt;h4&gt;结论&lt;/h4&gt;提出了减轻机构特定信息影响的方法，并呼吁未来的研究关注创新训练策略而非单纯依赖规模效应来发展更具有泛化的病理基础模型。&lt;h4&gt;翻译&lt;/h4&gt;病理基础模型（PFMs）从图像中提取有价值的区别性特征用于下游临床任务。虽然它们简化了深度学习模型的开发，有效利用先验知识提高了不同场景下的诊断准确性，但发现PFMs有时面临挑战：从图像中提取出的特性经常受到与诊断无关的信息干扰，即特定机构相关的特性，这可能导致虚假的相关性并削弱模型在现实中的应用能力。在这项研究中，我们揭示了特征污染的问题，展示了病理基础模型中存在的机构特有特性，并深入调查其负面影响、分析原因并提出见解。我们发现当前的PFMs可以轻易捕捉到病理图像中的特定信息，通过广泛的实验表明，非诊断相关信息对性能有害，特别是在出界分布设置下，依赖于这些被污染的特征会导致显著的表现下降。这揭示了模型可能受到误导的因素。进一步探讨了PFMs提取机构特定信息的原因，并验证了这一发现。最后提出了一个简单而有效的方法来缓解无关信息的影响。这项研究并非旨在批评现有的病理基础模型，而是要启发未来的科研专注于创新训练策略而非仅依赖规模效应以实现更加泛化的病理基础模型的发展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Pathology foundation models (PFMs) extract valuable discriminative featuresfrom images for downstream clinical tasks. PFMs have simplified the developmentof deep learning models, effectively leveraging prior knowledge to improvediagnostic accuracy in diverse scenarios. However, we find that PFMs sometimesstruggle with certain challenges. Specifically, features extracted by PFMs areoften contaminated by diagnosis-irrelevant information, i.e.,institution-specific features associated with the images. This contaminationcan lead to spurious correlations, undermining the models' generalizationability when applied in real-world clinical settings. In this work, we firstreveal the issue of feature contamination in PFMs, demonstrate the presence ofinstitution-specific features, thoroughly investigate its negative impacts,analyze the underlying causes, and provide insights into potential solutions.Specifically, we find that institution-specific information is embedded inpathological images and can be readily captured by current PFMs. Throughextensive experiments, we demonstrate the detrimental impact of this irrelevantinformation, particularly in out-of-distribution (OOD) settings, where relianceon contaminated features leads to significant performance degradation. Thisindicates that the models are being misled by non-diagnostic information. Wefurther delve into the reasons PFMs extract such institution-specificinformation and validate our findings. Finally, we propose a simple yeteffective solution to mitigate the influence of irrelevant information. Thisstudy is not intended to criticize existing PFMs, as they have indeed greatlyadvanced the development of computational pathology. our aim is to inspirefuture research to focus on innovative training strategies, rather than relyingexclusively on scaling laws, to realize more generalized PFMs.</description>
      <author>example@mail.com (Weiping Lin, Shen Liu, Runchen Zhu, Liansheng Wang)</author>
      <guid isPermaLink="false">2502.16889v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Subsampling Graphs with GNN Performance Guarantees</title>
      <link>http://arxiv.org/abs/2502.16703v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的图数据子采样方法，利用树移动距离减少图的数量和大小，该方法在理论上保证了训练后的模型损失相比完整数据集的增加是有限制的。&lt;h4&gt;背景&lt;/h4&gt;如何从大规模图数据集中有效地选择一个子样本进行GNN训练，使得其性能与整个数据集上的训练效果相当是一个重要的研究问题。较小的数据集可以减少标注成本、存储需求和所需的计算资源。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的基于理论支持的图数据子采样方法，能够在不牺牲模型性能的情况下减小数据集规模。&lt;h4&gt;方法&lt;/h4&gt;利用树移动距离作为度量标准来选择一个有效的子样本。该方法既对模型架构无特定要求也无需完全标注训练集即可实施。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示所提出的方法在多个数据集上优于现有的子采样技术，同时证明了其理论上的性能保证。&lt;h4&gt;结论&lt;/h4&gt;通过这种方法可以在早期的数据预处理阶段进行有效的图数据子采样，从而减少存储、标注和训练所需的资源。该方法具有广泛的适用性，因为它对模型架构和标签信息没有强依赖性。&lt;h4&gt;翻译&lt;/h4&gt;如何从大规模图数据集中有效地选择一个子样本进行GNN训练，使得其性能与整个数据集上的训练效果相当是一个重要的研究问题。较小的数据集可以减少标注成本、存储需求和所需的计算资源。现有技术的不足之处在于它们可能严重降低模型性能或需要大量实验来验证质量，从而消除了子采样的好处。因此，作者提出了一种新的基于理论支持的方法：利用树移动距离进行图数据子采样，并且证明了在子样本上训练GNN会导致损失增加是有限制的。这种方法具有广泛的适用性，因为它对模型架构和标签信息没有强依赖性。实验结果验证了该方法的有效性和优越性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How can we subsample graph data so that a graph neural network (GNN) trainedon the subsample achieves performance comparable to training on the fulldataset? This question is of fundamental interest, as smaller datasets reducelabeling costs, storage requirements, and computational resources needed fortraining. Selecting an effective subset is challenging: a poorly chosensubsample can severely degrade model performance, and empirically testingmultiple subsets for quality obviates the benefits of subsampling. Therefore,it is critical that subsampling comes with guarantees on model performance. Inthis work, we introduce new subsampling methods for graph datasets thatleverage the Tree Mover's Distance to reduce both the number of graphs and thesize of individual graphs. To our knowledge, our approach is the first that issupported by rigorous theoretical guarantees: we prove that training a GNN onthe subsampled data results in a bounded increase in loss compared to trainingon the full dataset. Unlike existing methods, our approach is bothmodel-agnostic, requiring minimal assumptions about the GNN architecture, andlabel-agnostic, eliminating the need to label the full training set. Thisenables subsampling early in the model development pipeline (before dataannotation, model selection, and hyperparameter tuning) reducing costs andresources needed for storage, labeling, and training. We validate ourtheoretical results with experiments showing that our approach outperformsexisting subsampling methods across multiple datasets.</description>
      <author>example@mail.com (Mika Sarkin Jain, Stefanie Jegelka, Ishani Karmarkar, Luana Ruiz, Ellen Vitercik)</author>
      <guid isPermaLink="false">2502.16703v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>An Autonomous Network Orchestration Framework Integrating Large Language Models with Continual Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.16198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  IEEE Communications Magazine&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;6G网络旨在实现全球覆盖、海量连接和超严格的要求。为了实现这些目标，空间-空中-地面综合网络（SAGIN）和语义通信（SemCom）是必不可少的组成部分，但它们在资源调配方面带来了相当大的复杂性。&lt;h4&gt;背景&lt;/h4&gt;当前的研究提出了利用大型语言模型（LLMs）来解决上述问题的一种可行方法。尽管最近在网络调度中使用LLMs已经引起了关注，但现有的解决方案并没有充分解决LLMs幻觉或适应网络动态的问题。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种名为自主强化协调（ARC）的框架，该框架针对具有语义通信功能的空间-空中-地面综合网络设计。&lt;h4&gt;方法&lt;/h4&gt;ARC框架利用基于大型语言模型增强检索生成器(RAG)来监控服务、用户和资源，并处理收集到的数据。同时，分层行动规划器(HAP)负责资源调度工作。ARC通过两个层次来分解资源调配任务：高层使用LLMs进行计划，低层由强化学习(Reinforcement Learning, RL)代理进行决策。&lt;h4&gt;主要发现&lt;/h4&gt;LLMs利用链式思维(CoT)推理进行少样本学习，并通过对比学习增强能力；RL代理则采用重放缓冲区管理来实现持续学习。因此，这种方法可以达到高效、准确和适应性。&lt;h4&gt;结论&lt;/h4&gt;论文通过模拟展示了ARC的有效性，并提供了一个关于如何进一步改进和完善ARC的深入讨论，包括未来的潜在研究方向。&lt;h4&gt;翻译&lt;/h4&gt;6G网络追求全球覆盖、海量连接以及超高标准的要求。空间-空中-地面综合网络(SAGIN)与语义通信技术为实现上述目标是不可或缺的技术手段，然而它们带来了资源协调上的巨大挑战。借鉴机器人领域的研究成果，本文提出了一种运用大型语言模型(如LLMs)作为解决方案的思路来应对这种复杂性，并设计了一个名为自主强化协调(ARC)的新框架以支持SAGIN内的语义通信系统。该框架通过分层架构实现对网络资源的有效管理与优化，在高层采用LLMs进行策略规划，低层使用RL代理作出具体决策；并且针对LLMs易产生的“幻觉”问题及适应网络动态的需求提出了改进方案。实验结果表明该方法具备高效的性能、准确的预测能力和良好的适应性，并探讨了未来可能的研究方向以进一步完善ARC框架。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; 6G networks aim to achieve global coverage, massive connectivity, andultra-stringent requirements. Space-Air-Ground Integrated Networks (SAGINs) andSemantic Communication (SemCom) are essential for realizing these goals, yetthey introduce considerable complexity in resource orchestration. Drawinginspiration from research in robotics, a viable solution to manage thiscomplexity is the application of Large Language Models (LLMs). Although the useof LLMs in network orchestration has recently gained attention, existingsolutions have not sufficiently addressed LLM hallucinations or theiradaptation to network dynamics. To address this gap, this paper proposes aframework called Autonomous Reinforcement Coordination (ARC) for aSemCom-enabled SAGIN. This framework employs an LLM-based Retrieval-AugmentedGenerator (RAG) monitors services, users, and resources and processes thecollected data, while a Hierarchical Action Planner (HAP) orchestratesresources. ARC decomposes orchestration into two tiers, utilizing LLMs forhigh-level planning and Reinforcement Learning (RL) agents for low-leveldecision-making, in alignment with the Mixture of Experts (MoE) concept. TheLLMs utilize Chain-of-Thought (CoT) reasoning for few-shot learning, empoweredby contrastive learning, while the RL agents employ replay buffer managementfor continual learning, thereby achieving efficiency, accuracy, andadaptability. Simulations are provided to demonstrate the effectiveness of ARC,along with a comprehensive discussion on potential future research directionsto enhance and upgrade ARC.</description>
      <author>example@mail.com (Masoud Shokrnezhad, Tarik Taleb)</author>
      <guid isPermaLink="false">2502.16198v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Fair Foundation Models for Medical Image Analysis: Challenges and Perspectives</title>
      <link>http://arxiv.org/abs/2502.16841v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;确保医疗保健中的人工智能（AI）公平性需要能够跨越所有人口群体做出无偏见决策的系统，这要求技术革新与伦理原则相结合。&lt;h4&gt;背景&lt;/h4&gt;基础模型(FMs)通过自我监督学习训练于大规模数据集上，可以在各种医学影像任务中高效地进行适应，同时减少对标注数据的依赖。然而，在不同的人口群体间实现一致性能面临重大挑战。&lt;h4&gt;目的&lt;/h4&gt;回顾表明，有效的偏见缓解需要在开发过程的所有阶段采取系统性干预措施，不仅包括模型层面的偏见缓解方法，还需要从数据文档到部署协议的一体化介入。&lt;h4&gt;方法&lt;/h4&gt;本文分析强调了公平基础模型(FMs)在整个开发管道中的综合干预的需求，并展示了如何通过结合系统性的偏见缓解和政策参与来有效地解决技术及机构障碍以实现医疗保健中公正的AI。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，为了推动面向服务不足人群和地区（尤其是那些基础设施有限、计算资源匮乏的地方）的先进医疗技术民主化，公平基础模型(FMs)的发展是至关重要的一步。&lt;h4&gt;结论&lt;/h4&gt;本文提出了一种综合框架来推进当前知识，展示了系统性偏见缓解结合政策参与如何有效解决技术及机构障碍，以实现医疗保健中公正的人工智能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring equitable Artificial Intelligence (AI) in healthcare demands systemsthat make unbiased decisions across all demographic groups, bridging technicalinnovation with ethical principles. Foundation Models (FMs), trained on vastdatasets through self-supervised learning, enable efficient adaptation acrossmedical imaging tasks while reducing dependency on labeled data. These modelsdemonstrate potential for enhancing fairness, though significant challengesremain in achieving consistent performance across demographic groups. Ourreview indicates that effective bias mitigation in FMs requires systematicinterventions throughout all stages of development. While previous approachesfocused primarily on model-level bias mitigation, our analysis reveals thatfairness in FMs requires integrated interventions throughout the developmentpipeline, from data documentation to deployment protocols. This comprehensiveframework advances current knowledge by demonstrating how systematic biasmitigation, combined with policy engagement, can effectively address bothtechnical and institutional barriers to equitable AI in healthcare. Thedevelopment of equitable FMs represents a critical step toward democratizingadvanced healthcare technologies, particularly for underserved populations andregions with limited medical infrastructure and computational resources.</description>
      <author>example@mail.com (Dilermando Queiroz, Anderson Carlos, André Anjos, Lilian Berton)</author>
      <guid isPermaLink="false">2502.16841v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>DUNIA: Pixel-Sized Embeddings via Cross-Modal Alignment for Earth Observation Applications</title>
      <link>http://arxiv.org/abs/2502.17066v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  26 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为DUNIA的方法，该方法通过图像与全波形LiDAR数据之间的跨模态对齐来学习像素级别的嵌入。这种方法能够直接应用于各种环境监测任务，并在零样本设置下展示了其有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的自我监督多模式学习方法为地球观测应用生成的是粗糙的补丁大小的嵌入，这限制了它们的有效性和与其他模式（如LiDAR）的集成能力。&lt;h4&gt;目的&lt;/h4&gt;为了弥补现有方法的不足，提出了一种新的跨模态对齐方法来学习像素级别的嵌入。&lt;h4&gt;方法&lt;/h4&gt;通过对比训练方式，该模型学会了在零样本设置下应用于各种环境监测任务的像素级别嵌入。具体来说，该研究使用图像和全波形LiDAR数据之间的跨模态对齐进行像素级嵌入的学习。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在七项环境监测任务中（包括冠层高度测绘、分层冠层覆盖率等），这些嵌入及其零样本分类器通常优于专门的监督模型，即使在低数据量环境下也是如此。此外，在微调设置下，DUNIA展示了强大的低样本能力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种新的方法来解决地球观测任务中像素级别嵌入的学习问题，并展示了其优越的表现和潜力。&lt;h4&gt;翻译&lt;/h4&gt;大量的努力已经被投入到自监督多模态学习为地球观察应用进行适应。然而，现有的方法产生的是粗糙的补丁大小的嵌入，这限制了它们的有效性和与其他模式（如LiDAR）的集成能力。为了弥补这一差距，我们提出了DUNIA——一种通过图像和全波形LiDAR数据之间的跨模态对齐来学习像素级别的嵌入的方法。由于模型以对比方式训练，因此这些嵌入可以直接在各种环境监测任务中零样本设置下应用。在我们的实验中，我们展示了这些嵌入对于七项此类任务的有效性（包括冠层高度测绘、分层冠层覆盖率等）。结果表明，这些嵌入与零样本分类器经常优于专门的监督模型，在低数据量环境下也是如此。在微调设置下，我们在五项任务中的表现接近或超过最新技术水平。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Significant efforts have been directed towards adapting self-supervisedmultimodal learning for Earth observation applications. However, existingmethods produce coarse patch-sized embeddings, limiting their effectiveness andintegration with other modalities like LiDAR. To close this gap, we presentDUNIA, an approach to learn pixel-sized embeddings through cross-modalalignment between images and full-waveform LiDAR data. As the model is trainedin a contrastive manner, the embeddings can be directly leveraged in thecontext of a variety of environmental monitoring tasks in a zero-shot setting.In our experiments, we demonstrate the effectiveness of the embeddings forseven such tasks (canopy height mapping, fractional canopy cover, land covermapping, tree species identification, plant area index, crop typeclassification, and per-pixel waveform-based vertical structure mapping). Theresults show that the embeddings, along with zero-shot classifiers, oftenoutperform specialized supervised models, even in low data regimes. In thefine-tuning setting, we show strong low-shot capabilities with performancesnear or better than state-of-the-art on five out of six tasks.</description>
      <author>example@mail.com (Ibrahim Fayad, Max Zimmer, Martin Schwartz, Philippe Ciais, Fabian Gieseke, Gabriel Belouze, Sarah Brood, Aurelien De Truchis, Alexandre d'Aspremont)</author>
      <guid isPermaLink="false">2502.17066v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MambaFlow: A Novel and Flow-guided State Space Model for Scene Flow Estimation</title>
      <link>http://arxiv.org/abs/2502.16907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文提出了一种名为MambaFlow的新型场景流估算网络，该网络利用基于Mamba的状态空间模型（SSM）的解码器来解决现有的点云帧间3D运动预测方法在时空建模和特征丢失方面的挑战。&lt;h4&gt;背景&lt;/h4&gt;场景流估计是自动驾驶领域的一个重要研究方向，旨在从连续的点云帧中预测3D运动。现有方法面临时空建模不足以及体素化过程中细粒度特征损失的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于Mamba的状态空间模型（SSM）解码器的方法，以解决现有场景流估计方法中的问题，并提升模型在不同场景下的泛化能力。&lt;h4&gt;方法&lt;/h4&gt;提出了名为MambaFlow的新型场景流估算网络。该网络利用一个设计良好的主干网，通过高效的基于Mamba的解码器来指导体素特征的全局注意力建模，学习体素到点的模式，并将共享体素表示去体素化为点级别的特性。&lt;h4&gt;主要发现&lt;/h4&gt;提出了用于增强模型泛化的场景自适应损失函数。在Argoverse 2基准上的大量实验表明，MambaFlow实现了现有的实时推理速度和最先进的性能，能够准确估计现实世界的城市场景中的流。&lt;h4&gt;结论&lt;/h4&gt;MambaFlow展示了其解决现有方法问题的能力，并且证明了使用基于Mamba的解码器进行全局注意力建模的有效性。此外，提出的自适应损失函数进一步增强了模型在不同场景下的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;场景流估计旨在从连续的点云帧中预测3D运动，在自动驾驶领域备受关注。现有方法面临时空建模不足和体素化过程中细粒度特征丢失的问题。然而，Mamba的成功展示了全局建模和线性复杂性的可能性。本文提出了一种基于Mamba的状态空间模型（SSM）解码器的新型场景流估计网络——MambaFlow，它可以利用设计良好的主干网实现时空特征的深度交互耦合，并且提出了一个全新的场景自适应损失函数来提升泛化能力。在Argoverse 2基准上的大量实验表明，MambaFlow实现了现有方法中的实时推理速度和最先进的性能，在现实世界的城市环境中能够准确估计流。代码可以从 https://github.com/SCNU-RISLAB/MambaFlow 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Scene flow estimation aims to predict 3D motion from consecutive point cloudframes, which is of great interest in autonomous driving field. Existingmethods face challenges such as insufficient spatio-temporal modeling andinherent loss of fine-grained feature during voxelization. However, the successof Mamba, a representative state space model (SSM) that enables global modelingwith linear complexity, provides a promising solution. In this paper, wepropose MambaFlow, a novel scene flow estimation network with a mamba-baseddecoder. It enables deep interaction and coupling of spatio-temporal featuresusing a well-designed backbone. Innovatively, we steer the global attentionmodeling of voxel-based features with point offset information using anefficient Mamba-based decoder, learning voxel-to-point patterns that are usedto devoxelize shared voxel representations into point-wise features. To furtherenhance the model's generalization capabilities across diverse scenarios, wepropose a novel scene-adaptive loss function that automatically adapts todifferent motion patterns.Extensive experiments on the Argoverse 2 benchmarkdemonstrate that MambaFlow achieves state-of-the-art performance with real-timeinference speed among existing works, enabling accurate flow estimation inreal-world urban scenarios. The code is available athttps://github.com/SCNU-RISLAB/MambaFlow.</description>
      <author>example@mail.com (Jiehao Luo, Jintao Cheng, Xiaoyu Tang, Qingwen Zhang, Bohuan Xue, Rui Fan)</author>
      <guid isPermaLink="false">2502.16907v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>UniDyG: A Unified and Effective Representation Learning Approach for Large Dynamic Graphs</title>
      <link>http://arxiv.org/abs/2502.16431v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'背景': '动态图在连续时间（CTDGs）和离散时间（DTDGs）下被定义，分别展现出快速局部变化和逐渐全局更新的特点。现有的表示学习研究主要针对其中一种类型的动态图进行，并且通常只关注时间域内的局部动态传播，难以准确捕捉与每种时间粒度相关的结构演变。', '目的': '为了更好地建模这两种类型的动态图，并提升模型的鲁棒性和有效性，提出了一种统一有效的表示学习方法UniDyG。', '方法': '该研究首先提出了一个新颖的傅立叶图注意力（FGAT）机制，可以基于最近邻居和复数选择性聚合来同时建模局部和全局结构相关性，并在理论上确保动态图的时间一致性。通过设计能量门控单元增强FGAT对抗时间噪声的能力，并利用频率增强线性函数进行节点级的动态更新。', '主要发现': '实验表明，所提出的UniDyG方法相较于16个基准模型，在9种动态图上平均改进了14.4%。', '结论': '这项工作提出了一种新的表示学习框架，能够有效应对CTDGs和DTDGs中的挑战，并通过实证研究证明其优越性。'}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic graphs are formulated in continuous-time or discrete-time dynamicgraphs. They differ in temporal granularity: Continuous-Time Dynamic Graphs(CTDGs) exhibit rapid, localized changes, while Discrete-Time Dynamic Graphs(DTDGs) show gradual, global updates. This difference leads to isolateddevelopments in representation learning for each type. To advancerepresentation learning, recent research attempts to design a unified modelcapable of handling both CTDGs and DTDGs. However, it typically focuses onlocal dynamic propagation for temporal structure learning in the time domain,failing to accurately capture the structural evolution associated with eachtemporal granularity. In addition, existing works-whether specific orunified-often overlook the issue of temporal noise, compromising the modelrobustness and effectiveness. To better model both types of dynamic graphs, wepropose UniDyG, a unified and effective representation learning approach, whichscales to large dynamic graphs. We first propose a novel Fourier GraphAttention (FGAT) mechanism that can model local and global structuralcorrelations based on recent neighbors and complex-number selectiveaggregation, while theoretically ensuring consistent representations of dynamicgraphs over time. Based on approximation theory, we demonstrate that FGAT iswell-suited to capture the underlying structures in CTDGs and DTDGs. We furtherenhance FGAT to resist temporal noise by designing an energy-gated unit, whichadaptively filters out high-frequency noise according to the energy. Last, weleverage our FGAT mechanisms for temporal structure learning and employ thefrequency-enhanced linear function for node-level dynamic updates, facilitatingthe generation of high-quality temporal embeddings. Extensive experiments showthat our UniDyG achieves an average improvement of 14.4% over sixteen baselinesacross nine dynamic graphs.</description>
      <author>example@mail.com (Yuanyuan Xu, Wenjie Zhang, Xuemin Lin, Ying Zhang)</author>
      <guid isPermaLink="false">2502.16431v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SwimVG: Step-wise Multimodal Fusion and Adaption for Visual Grounding</title>
      <link>http://arxiv.org/abs/2502.16786v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 7 figures.Our code is available at  https://github.com/liuting20/SwimVG&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;Visual grounding通过自然语言来定位图像区域，这项任务很大程度上依赖于跨模态对齐。现有大多数方法分别传输视觉和语言知识，并在预训练的单模模型完全微调后简单堆叠视觉-语言变换器进行多模态融合。&lt;h4&gt;背景问题&lt;/h4&gt;当前的方法限制了视觉与语言上下文之间的充分交互，同时带来了显著的计算成本。&lt;h4&gt;研究目的&lt;/h4&gt;提出一种分步式多模态融合和适应框架SwimVG来解决上述问题，并在效率上获得明显优势。&lt;h4&gt;方法介绍&lt;/h4&gt;提出了分步式多模态提示(Swip)和跨模态互动适配器(CIA)，分别用于逐步提升视觉与语言表示的对齐度以及进一步促进多模态融合。这些新的架构以参数高效的方式替换原有的繁琐变换器堆栈，从浅层到深层渐进地融合跨模式特征。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SwimVG在四个广泛使用的基准数据集上实现了卓越的能力和效率上的显著优势。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种新颖的视觉接地方法，通过分步多模态融合和适应框架提高了任务性能并优化了计算资源利用。代码已公开供社区进一步探索和应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：视觉定位旨在通过自然语言来确定图像区域，这很大程度上依赖于跨模态对齐。现有的大多数方法分别传输视觉或语言知识，并在预训练的单模式模型完全微调后简单堆叠视觉-语言变换器进行多模态融合。然而，这些方法不仅限制了视觉和语言上下文之间的充分交互，还带来了显著的计算成本。因此，为了应对这些问题，我们探索了一种分步式的多模态融合和适应框架，即SwimVG。具体来说，SwimVG提出了步骤式多模态提示（Swip）和跨模式互动适配器（CIA），用于视觉定位，在此过程中用简单的变换器堆栈替换原有的跨模式融合方式。Swip可以通过令牌级别的融合逐步提升视觉与语言表示的对齐度。此外，重量级的CIA通过跨模态交互进一步促进多模态融合。Swip和CIA都是参数效率高，并且它们逐渐从浅层到深层地融合了跨模式特征。在四个广泛使用的基准测试中进行的实验结果表明，在效率方面，SwimVG获得了显著的能力和收益。我们的代码可以在https://github.com/liuting20/SwimVG上找到。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Visual grounding aims to ground an image region through natural language,which heavily relies on cross-modal alignment. Most existing methods transfervisual/linguistic knowledge separately by fully fine-tuning uni-modalpre-trained models, followed by a simple stack of visual-language transformersfor multimodal fusion. However, these approaches not only limit adequateinteraction between visual and linguistic contexts, but also incur significantcomputational costs. Therefore, to address these issues, we explore a step-wisemultimodal fusion and adaption framework, namely SwimVG. Specifically, SwimVGproposes step-wise multimodal prompts (Swip) and cross-modal interactiveadapters (CIA) for visual grounding, replacing the cumbersome transformerstacks for multimodal fusion. Swip can improve {the} alignment between thevision and language representations step by step, in a token-level fusionmanner. In addition, weight-level CIA further promotes multimodal fusion bycross-modal interaction. Swip and CIA are both parameter-efficient paradigms,and they fuse the cross-modal features from shallow to deep layers gradually.Experimental results on four widely-used benchmarks demonstrate that SwimVGachieves remarkable abilities and considerable benefits in terms of efficiency.Our code is available at https://github.com/liuting20/SwimVG.</description>
      <author>example@mail.com (Liangtao Shi, Ting Liu, Xiantao Hu, Yue Hu, Quanjun Yin, Richang Hong)</author>
      <guid isPermaLink="false">2502.16786v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Leveraging recurrence in neural network wavefunctions for large-scale simulations of Heisenberg antiferromagnets: the square lattice</title>
      <link>http://arxiv.org/abs/2502.17144v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  18 pages, 13 figures, 6 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种使用递归神经网络（RNN）进行变分蒙特卡洛模拟的方法，用于研究二维量子多体系统的基态性质。通过转移学习技术，能够有效地在大规模系统中应用这种方法而不需要从头开始优化。&lt;h4&gt;背景&lt;/h4&gt;机器学习支持的变分蒙特卡罗方法被用来寻找量子多体系统的基态，特别是在二维情况下和非平凡符号结构的情况下。尽管已经达到了许多最先进的有限大小系统的变分能量，但在热力学极限中的研究较少。&lt;h4&gt;目的&lt;/h4&gt;使用RNN作为变分近似来模拟自旋$rac{1}{2}$系统的基态，并通过迭代重新训练的方法逐步增加系统规模以减少计算资源需求。&lt;h4&gt;方法&lt;/h4&gt;在本工作中，作者专注于二维反铁磁海森堡模型（SLAHM），并利用递归神经网络的特性进行大规模系统的基态模拟。通过延长训练时间来提高结果精度，并将有限大小格点上的数值与文献值对比验证。&lt;h4&gt;主要发现&lt;/h4&gt;实现了系统性地提高模拟结果准确性，同时获得了热力学极限下的精确估计。&lt;h4&gt;结论&lt;/h4&gt;该工作证明了RNN波函数可以用来准确研究量子多体物理在热力学极限中的性质。&lt;h4&gt;翻译&lt;/h4&gt;基于机器学习的变分蒙特卡洛仿真是瞄准量子多体基态的一种有前途的方法，特别是在二维和已知基态具有非平凡符号结构的情况下。尽管这些方法已经达到了许多最先进的有限尺寸系统的变分能量，但很少有人利用这些结果来提取目标状态在热力学极限中的信息。本文中，我们采用递归神经网络（RNN）作为变分近似，并利用其递归性质通过迭代重新训练逐步模拟更大规模的系统，从而减少计算资源的需求。在这项研究中，我们专注于二维反铁磁海森堡模型，在这里可以仔细地验证我们的结果。我们展示了可以通过增加训练时间来有条不紊地提高模拟结果的精度，并且对于有限大小格点上的数值与文献值之间具有很好的一致性。此外，我们利用这些结果提取了热力学极限下基态性质的精确估计。这项工作证明了RNN波函数可以用来准确研究量子多体物理在热力学极限中的特性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learning-based variational Monte Carlo simulations are a promisingapproach for targeting quantum many body ground states, especially in twodimensions and in cases where the ground state is known to have a non-trivialsign structure. While many state-of-the-art variational energies have beenreached with these methods for finite-size systems, little work has been doneto use these results to extract information about the target state in thethermodynamic limit. In this work, we employ recurrent neural networks (RNNs)as a variational ans\"{a}tze, and leverage their recurrent nature to simulatethe ground states of progressively larger systems through iterative retraining.This transfer learning technique allows us to simulate spin-$\frac{1}{2}$systems on lattices with more than 1,000 spins without beginning optimizationfrom scratch for each system size, thus reducing the demands for computationalresources. In this study, we focus on the square-lattice antiferromagneticHeisenberg model (SLAHM), where it is possible to carefully benchmark ourresults. We show that we are able to systematically improve the accuracy of theresults from our simulations by increasing the training time, and obtainresults for finite-sized lattices that are in good agreement with theliterature values. Furthermore, we use these results to extract accurateestimates of the ground-state properties in the thermodynamic limit. This workdemonstrates that RNN wavefunctions can be used to accurately study quantummany-body physics in the thermodynamic limit.</description>
      <author>example@mail.com (M. Schuyler Moss, Roeland Wiersema, Mohamed Hibat-Allah, Juan Carrasquilla, Roger G. Melko)</author>
      <guid isPermaLink="false">2502.17144v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising</title>
      <link>http://arxiv.org/abs/2502.16826v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;Noise2Score3D是一个完全无监督的点云去噪框架，它利用贝叶斯统计和图像去噪领域的最新进展来解决干净数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的去噪方法通常需要大量的干净数据进行训练，这在实际应用中是难以获得的。而本研究提出的Noise2Score3D方法直接从带噪声的数据中学习点云分布的梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一种新颖的方法来处理点云去噪问题，特别是在没有或很少有干净数据的情况下。&lt;h4&gt;方法&lt;/h4&gt;该方法利用Tweedie公式进行一步式推理，避免了现有无监督方法中的迭代过程，并且通过Total Variation for Point Cloud标准估计未知噪声参数，增强了方法的实用性和通用性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Noise2Score3D在Chamfer距离和点到网格度量方面优于其他无监督方法，在一些性能指标上甚至可以与有监督的方法相媲美，并且具有很强的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;该研究提出了一种新的去噪框架Noise2Score3D，不仅提升了算法效率和性能，还提高了其在实际应用中的适应性和实用性。&lt;h4&gt;翻译&lt;/h4&gt;本文基于贝叶斯统计学和图像去噪领域的最新进展，提出了一个完全无监督的点云去噪框架Noise2Score3D。该方法直接从带噪声的数据中学习潜在分布的梯度，并通过Tweedie公式执行一步式推理。实验结果表明，在标准基准测试上，这种方法在Chamfer距离和点到网格度量方面优于其他无监督方法，甚至可以与一些有监督的方法相媲美。此外，Noise2Score3D还引入了Total Variation for Point Cloud的标准来估计未知的噪声参数，进一步增强了该方法的灵活性和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Building on recent advances in Bayesian statistics and image denoising, wepropose Noise2Score3D, a fully unsupervised framework for point cloud denoisingthat addresses the critical challenge of limited availability of clean data.Noise2Score3D learns the gradient of the underlying point cloud distributiondirectly from noisy data, eliminating the need for clean data during training.By leveraging Tweedie's formula, our method performs inference in a singlestep, avoiding the iterative processes used in existing unsupervised methods,thereby improving both performance and efficiency. Experimental resultsdemonstrate that Noise2Score3D achieves state-of-the-art performance onstandard benchmarks, outperforming other unsupervised methods in Chamferdistance and point-to-mesh metrics, and rivaling some supervised approaches.Furthermore, Noise2Score3D demonstrates strong generalization ability beyondtraining datasets. Additionally, we introduce Total Variation for Point Cloud,a criterion that allows for the estimation of unknown noise parameters, whichfurther enhances the method's versatility and real-world utility.</description>
      <author>example@mail.com (Xiangbin Wei)</author>
      <guid isPermaLink="false">2502.16826v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Interpretable Retinal Disease Prediction Using Biology-Informed Heterogeneous Graph Representations</title>
      <link>http://arxiv.org/abs/2502.16697v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新颖的方法，利用基于生物学信息的异构图表示来提高糖尿病视网膜病变分期预测的可解释性，并在两个数据集上超过了现有的机器学习模型。&lt;h4&gt;背景&lt;/h4&gt;目前最先进的基于神经网络的图像分类器大多不可解释。虽然生物标志物是临床诊断的重要依据，但生物标志物基础分类性能通常不如大型神经网络。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法以超越现有机器学习模型的表现并提高糖尿病视网膜病变分期预测的可解释性。&lt;h4&gt;方法&lt;/h4&gt;利用新颖的生物学信息异构图表示来建模视网膜血管段、间隔区以及中央凹无血管区，将糖尿病视网膜病变分期问题转化为图形分类任务，并使用高效的图神经网络解决该问题。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法在两个数据集上优于传统的生物标志物基础分类器、卷积神经网络（CNN）和视觉变换器等基准方法。此外，还提供了前所未有的详细解释来定位关键血管或间隔区并赋予关键特性有意义的人类可解读归因。&lt;h4&gt;结论&lt;/h4&gt;该研究贡献了有助于眼科临床决策支持工具发展的新方法。&lt;h4&gt;翻译&lt;/h4&gt;可解释性对于增强医学诊断中机器学习模型的信任至关重要。然而，大多数基于神经网络的先进图像分类器不可解释。因此，尽管生物标志物基础分类通常表现不如大型神经网络，但临床上仍依赖于已知的生物标志物进行诊断。这项工作提出了一种方法，该方法超越了现有的机器学习模型的表现，并同时提高了糖尿病视网膜病变分期从光学相干断层扫描血管成像（OCTA）图像中预测的可解释性。我们的方法基于一种新颖的生物学信息异构图表示，以人类可理解的方式建模视网膜血管段、间隔区以及中央凹无血管区（FAZ）。这种图形表示使我们能够将糖尿病视网膜病变分期视为一个图形级别的分类任务，并使用高效的图神经网络解决该问题。我们在两个数据集上对我们的方法进行了基准测试，与包括经典生物标志物基础分类器、卷积神经网络（CNN）和视觉变换器在内的现有基线模型相比表现更佳。我们使用生物学信息图提供了前所未有的详细解释，并且在精确定位关键血管或间隔区以及赋予有意义的人类可解读归因方面超越了现有的方法。我们的工作对眼科临床决策支持工具的发展做出了贡献。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Interpretability is crucial to enhance trust in machine learning models formedical diagnostics. However, most state-of-the-art image classifiers based onneural networks are not interpretable. As a result, clinicians often resort toknown biomarkers for diagnosis, although biomarker-based classificationtypically performs worse than large neural networks. This work proposes amethod that surpasses the performance of established machine learning modelswhile simultaneously improving prediction interpretability for diabeticretinopathy staging from optical coherence tomography angiography (OCTA)images. Our method is based on a novel biology-informed heterogeneous graphrepresentation that models retinal vessel segments, intercapillary areas, andthe foveal avascular zone (FAZ) in a human-interpretable way. This graphrepresentation allows us to frame diabetic retinopathy staging as a graph-levelclassification task, which we solve using an efficient graph neural network. Webenchmark our method against well-established baselines, including classicalbiomarker-based classifiers, convolutional neural networks (CNNs), and visiontransformers. Our model outperforms all baselines on two datasets. Crucially,we use our biology-informed graph to provide explanations of unprecedenteddetail. Our approach surpasses existing methods in precisely localizing andidentifying critical vessels or intercapillary areas. In addition, we giveinformative and human-interpretable attributions to critical characteristics.Our work contributes to the development of clinical decision-support tools inophthalmology.</description>
      <author>example@mail.com (Laurin Lux, Alexander H. Berger, Maria Romeo Tricas, Alaa E. Fayed, Sobha Sivaprasada, Linus Kreitner, Jonas Weidner, Martin J. Menten, Daniel Rueckert, Johannes C. Paetzold)</author>
      <guid isPermaLink="false">2502.16697v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Emergence of Multimodal Representation Alignment</title>
      <link>http://arxiv.org/abs/2502.16282v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 22 figures, 3 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文探讨了多模态学习中不同模态之间表示对齐的形成机制及其与任务性能的关系，通过实验表明其依赖于数据特性。&lt;h4&gt;背景&lt;/h4&gt;多模态表征学习旨在将不同的不可比较模态转换为可比较的表示。以往的研究主要集中在显式对齐上，但最近发现独立训练的大规模单模态模型可以隐式对齐。&lt;h4&gt;目的&lt;/h4&gt;探讨在多模态学习中，不同模态之间如何以及为何会形成隐式的表示对齐；同时探究这种对齐是否可靠地指示任务性能。&lt;h4&gt;方法&lt;/h4&gt;通过全面的实证研究来分析不同数据特性（如模态相似度和信息冗余度）对表示对齐及其与任务表现关系的影响。&lt;h4&gt;主要发现&lt;/h4&gt;表示对齐的出现及其实现任务性能的相关性依赖于关键的数据特征，包括但不限于模态之间的相似性和冗余/独特信息的比例。这些发现表明，对齐不一定总是有益的；其影响取决于特定数据集和任务的情况。&lt;h4&gt;结论&lt;/h4&gt;研究结果有助于实践者根据具体情况确定增加不同模态之间表示对齐是否有利于获得最优性能。&lt;h4&gt;翻译&lt;/h4&gt;多模式表征学习是将不同的不可比较模态转换为可比较表示的过程。之前的研究主要集中在通过明确的学习目标和模型架构来显式地对准这些表示，但最近的一系列工作发现独立训练的规模更大、性能更好的单模态模型可以彼此隐式对齐。这一研究结果引发了关于多模式学习中对齐表示出现的基本问题：（1）何时以及为什么会出现隐式的对齐？（2）对齐是否可靠地指示了任务性能？通过全面的经验调查，我们证明了对齐的产生及其与任务表现的关系依赖于多个关键数据特征。这包括但不限于模态之间的相似性和它们提供的冗余和独特信息的比例等。我们的研究结果表明对齐不一定总是有益的；相反，它对性能的影响根据数据集和任务的不同而变化。这些见解可以帮助实践者确定增加不同模式之间表示对准是否有利或在某些情况下有害于获得最佳性能。代码发布在 https://github.com/MeganTj/multimodal_alignment 上。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal representation learning is fundamentally about transformingincomparable modalities into comparable representations. While prior researchprimarily focused on explicitly aligning these representations through targetedlearning objectives and model architectures, a recent line of work has foundthat independently trained unimodal models of increasing scale and performancecan become implicitly aligned with each other. These findings raise fundamentalquestions regarding the emergence of aligned representations in multimodallearning. Specifically: (1) when and why does alignment emerge implicitly? and(2) is alignment a reliable indicator of performance? Through a comprehensiveempirical investigation, we demonstrate that both the emergence of alignmentand its relationship with task performance depend on several critical datacharacteristics. These include, but are not necessarily limited to, the degreeof similarity between the modalities and the balance between redundant andunique information they provide for the task. Our findings suggest thatalignment may not be universally beneficial; rather, its impact on performancevaries depending on the dataset and task. These insights can help practitionersdetermine whether increasing alignment between modalities is advantageous or,in some cases, detrimental to achieving optimal performance. Code is releasedat https://github.com/MeganTj/multimodal_alignment.</description>
      <author>example@mail.com (Megan Tjandrasuwita, Chanakya Ekbote, Liu Ziyin, Paul Pu Liang)</author>
      <guid isPermaLink="false">2502.16282v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Category-Selective Neurons in Deep Networks: Comparing Purely Visual and Visual-Language Models</title>
      <link>http://arxiv.org/abs/2502.16456v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文研究了人工神经网络（ANNs）中是否存在与人类大脑相似的类别选择性区域，并探讨这些区域在视觉和视觉-语言模型中的差异。&lt;h4&gt;背景&lt;/h4&gt;人类大脑中有专门处理特定类型视觉信息的区域，如面部、身体、场景等。作者探索了人工神经网络中是否存在类似的特性。&lt;h4&gt;目的&lt;/h4&gt;研究不同深度学习模型中的类别选择性神经元及其分布规律，并探讨多模态学习对这些神经元的影响。&lt;h4&gt;方法&lt;/h4&gt;通过向深层网络展示不同类别的图像（如人脸、身体、场景等）并使用统计标准识别类别选择性神经元，模拟功能性定位实验的方法进行研究。&lt;h4&gt;主要发现&lt;/h4&gt;ResNet和基于CLIP的模型都包含类别选择性神经元，但CLIP模型中这些神经元的比例更高且分布更均匀。这表明多模态学习增加了这类神经元的数量却减少了它们的选择特异性。&lt;h4&gt;结论&lt;/h4&gt;这项研究表明人工神经网络模仿了生物视觉系统，并揭示了多模态学习如何影响深度网络中的类别选择性表示。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文译文为：人类大脑中负责高级视觉处理的区域，如梭状回面孔区（FFA）、额外体素体区（EBA）、海马旁场景区（PPA）和视觉词形区（VWFA），在人脸识别、身体识别、场景理解和文字阅读等方面起着关键作用。本文探讨了人工神经网络（ANNs）中是否存在类似的类别选择性区域，以及这些区域在网络的各个层面上的变化情况，并且比较了纯视觉模型与视觉-语言模型之间的差异。通过模拟功能定位实验的方法，在深层网络上展示了来自不同类别的图像（包括面部、身体、场景、文字及其随机组合），并使用统计标准来识别类别选择性神经元。研究发现，无论是ResNet还是基于CLIP的结构控制模型，都包含了类别选择性神经元，并且随着层数的增加，这些神经元的比例也在增加，这与高级视觉大脑区域中的选择性一致。然而，相较于ResNet，CLIP显示了更高的比例但更低的选择特异性，在特征图上的分布更均匀，并在不同层之间展现出更强的表现一致性。研究结果表明语言学习增加了类别选择性神经元的数量而减少了它们的特定强度，重新塑造了深度网络中的视觉表示形式。这项研究提供了关于ANNs如何模仿生物视觉系统以及多模态学习影响类别选择性表征的见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Category-selective regions in the human brain, such as the fusiform face area(FFA), extrastriate body area (EBA), parahippocampal place area (PPA), andvisual word form area (VWFA), play a crucial role in high-level visualprocessing. Here, we investigate whether artificial neural networks (ANNs)exhibit similar category-selective neurons and how these neurons vary acrossmodel layers and between purely visual and vision-language models. Inspired byfMRI functional localizer experiments, we presented images from differentcategories (faces, bodies, scenes, words, scrambled scenes, and scrambledwords) to deep networks and identified category-selective neurons usingstatistical criteria. Comparing ResNet and the structurally controlledResNet-based CLIP model, we found that both models contain category-selectiveneurons, with their proportion increasing across layers, mirroring categoryselectivity in higher-level visual brain regions. However, CLIP exhibited ahigher proportion but lower specificity of category-selective neurons comparedto ResNet. Additionally, CLIP's category-selective neurons were more evenlydistributed across feature maps and demonstrated greater representationalconsistency across layers. These findings suggest that language learningincreases the number of category-selective neurons while reducing theirselectivity strength, reshaping visual representations in deep networks. Ourstudy provides insights into how ANNs mirror biological vision and howmultimodal learning influences category-selective representations.</description>
      <author>example@mail.com (Zitong Lu, Yuxin Wang)</author>
      <guid isPermaLink="false">2502.16456v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Contrastive Similarity Learning for Market Forecasting: The ContraSim Framework</title>
      <link>http://arxiv.org/abs/2502.16023v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 3 appendices&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;我们介绍了一种新的框架Contrastive Similarity Space Embedding Algorithm (ContraSim)，用于揭示日常金融新闻头条与市场动态之间的全球语义关系。&lt;h4&gt;背景&lt;/h4&gt;金融市场中的新闻报道和股票价格之间存在复杂的相互作用，现有的方法难以有效捕捉这些关系。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够更有效地揭示金融新闻与市场动态之间关联性的算法框架。&lt;h4&gt;方法&lt;/h4&gt;{'Weighted Headline Augmentation': '生成带有语义细粒度相似性评分的增强型金融新闻头条', 'Weighted Self-Supervised Contrastive Learning (WSSCL)': '利用相似性度量创建细化加权嵌入空间，使语义相似的新闻头条聚类在一起'}&lt;h4&gt;主要发现&lt;/h4&gt;{'提高分类精度': '将ContraSim特性融入金融预测任务后，从《华尔街日报》新闻中提高了7%的分类准确率。', '市场动态捕捉': '通过信息密度分析，发现了由ContraSim构造出的相似空间内在地聚类了具有相同市场移动方向的日子，表明该算法能够独立于地面真实标签捕获市场的动态变化。', '参考过去事件': '识别历史新闻日，这些日子与当前日期头条内容非常接近，为预测市场趋势提供可操作见解'}&lt;h4&gt;结论&lt;/h4&gt;ContraSim不仅提高了金融预测任务中的分类精度，还为金融市场分析师提供了基于类似过去的事件进行市场趋势预测的实用工具。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce the Contrastive Similarity Space Embedding Algorithm(ContraSim), a novel framework for uncovering the global semantic relationshipsbetween daily financial headlines and market movements. ContraSim operates intwo key stages: (I) Weighted Headline Augmentation, which generates augmentedfinancial headlines along with a semantic fine-grained similarity score, and(II) Weighted Self-Supervised Contrastive Learning (WSSCL), an extended versionof classical self-supervised contrastive learning that uses the similaritymetric to create a refined weighted embedding space. This embedding spaceclusters semantically similar headlines together, facilitating deeper marketinsights. Empirical results demonstrate that integrating ContraSim featuresinto financial forecasting tasks improves classification accuracy from WSJheadlines by 7%. Moreover, leveraging an information density analysis, we findthat the similarity spaces constructed by ContraSim intrinsically cluster dayswith homogeneous market movement directions, indicating that ContraSim capturesmarket dynamics independent of ground truth labels. Additionally, ContraSimenables the identification of historical news days that closely resemble theheadlines of the current day, providing analysts with actionable insights topredict market trends by referencing analogous past events.</description>
      <author>example@mail.com (Nicholas Vinden, Raeid Saqur, Zining Zhu, Frank Rudzicz)</author>
      <guid isPermaLink="false">2502.16023v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Provable Benefits of Unsupervised Pre-training and Transfer Learning via Single-Index Models</title>
      <link>http://arxiv.org/abs/2502.16849v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文探讨了无监督预训练和迁移学习如何影响高维有监督学习的样本复杂度，特别是在标签数据有限的情况下。研究结果显示，在特定条件下，这些技术能显著减少所需的训练样本数量。&lt;h4&gt;背景&lt;/h4&gt;在深度学习领域中，无监督预训练和迁移学习通常被用来初始化神经网络模型以应对标签数据稀缺的问题。&lt;h4&gt;目的&lt;/h4&gt;目的是分析无监督预训练和转移学习如何影响单层神经网络通过在线随机梯度下降进行训练时的样本复杂性。&lt;h4&gt;方法&lt;/h4&gt;研究考虑了使用在线随机梯度下降法来训练单层神经网络的情况，并探讨了无监督预训练与迁移学习在这种情况下的效果。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，在较为宽松的一般假设条件下，无监督预训练和转移学习能够通过多项式因素减少样本复杂性。此外，研究还发现了某些情况下无监督预训练可以提供指数级的改善，相对于随机初始化而言。&lt;h4&gt;结论&lt;/h4&gt;该论文提供了无监督预训练和迁移学习对深度神经网络模型在高维数据上的影响的重要见解，并强调了其在样本效率方面的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：无监督预训练和迁移学习是常见的技术手段，用于初始化神经网络的训练算法，特别是在标签数据有限的情况下。本文研究了无监督预训练和迁移学习如何影响高维有监督学习的样本复杂度。具体来说，我们考虑通过在线随机梯度下降法来训练单层神经网络的问题，并建立在非常一般性的假设下，预训练和转移学习（在概念转变的情况下）可以通过多项式因素减少样本复杂性。此外，研究还揭示了某些情况，在无监督预训练方面可以提供相对于随机初始化而言的指数级改进效果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Unsupervised pre-training and transfer learning are commonly used techniquesto initialize training algorithms for neural networks, particularly in settingswith limited labeled data. In this paper, we study the effects of unsupervisedpre-training and transfer learning on the sample complexity of high-dimensionalsupervised learning. Specifically, we consider the problem of training asingle-layer neural network via online stochastic gradient descent. Weestablish that pre-training and transfer learning (under concept shift) reducesample complexity by polynomial factors (in the dimension) under very generalassumptions. We also uncover some surprising settings where pre-training grantsexponential improvement over random initialization in terms of samplecomplexity.</description>
      <author>example@mail.com (Taj Jones-McCormick, Aukosh Jagannath, Subhabrata Sen)</author>
      <guid isPermaLink="false">2502.16849v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Survey of Graph Transformers: Architectures, Theories and Applications</title>
      <link>http://arxiv.org/abs/2502.16533v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;Graph Transformers (GTs) 在处理图结构时展示了强大的能力，通过解决图神经网络（GNNs）固有的问题如过度平滑和过度挤压。&lt;h4&gt;背景&lt;/h4&gt;近年来，针对 Graph Transformers 的研究提出了多种架构、增强了可解释性，并在实际应用中取得了进展。&lt;h4&gt;目的&lt;/h4&gt;综述 Graph Transformers 在其架构设计、理论基础及具体应用场景等方面的发展状况。&lt;h4&gt;方法&lt;/h4&gt;根据处理结构信息的策略对 Graph Transformers 架构进行分类，包括图标记化、位置编码、基于结构的注意力机制和模型集成等。&lt;h4&gt;主要发现&lt;/h4&gt;探讨了不同架构下 Graph Transformers 的表达能力，并将其与其它先进的图学习算法进行了对比。&lt;h4&gt;应用领域&lt;/h4&gt;总结了 Graph Transformers 在分子数据、蛋白质信息、自然语言处理、视觉交通、脑科学及材料研究等多个领域的实际应用案例。&lt;h4&gt;未来挑战和方向&lt;/h4&gt;讨论了当前 Graph Transformers 面临的挑战以及潜在的研究发展方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph Transformers (GTs) have demonstrated a strong capability in modelinggraph structures by addressing the intrinsic limitations of graph neuralnetworks (GNNs), such as over-smoothing and over-squashing. Recent studies haveproposed diverse architectures, enhanced explainability, and practicalapplications for Graph Transformers. In light of these rapid developments, weconduct a comprehensive review of Graph Transformers, covering aspects such astheir architectures, theoretical foundations, and applications within thissurvey. We categorize the architecture of Graph Transformers according to theirstrategies for processing structural information, including graph tokenization,positional encoding, structure-aware attention and model ensemble. Furthermore,from the theoretical perspective, we examine the expressivity of GraphTransformers in various discussed architectures and contrast them with otheradvanced graph learning algorithms to discover the connections. Furthermore, weprovide a summary of the practical applications where Graph Transformers havebeen utilized, such as molecule, protein, language, vision traffic, brain andmaterial data. At the end of this survey, we will discuss the currentchallenges and prospective directions in Graph Transformers for potentialfuture research.</description>
      <author>example@mail.com (Chaohao Yuan, Kangfei Zhao, Ercan Engin Kuruoglu, Liang Wang, Tingyang Xu, Wenbing Huang, Deli Zhao, Hong Cheng, Yu Rong)</author>
      <guid isPermaLink="false">2502.16533v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>PLS-based approach for fair representation learning</title>
      <link>http://arxiv.org/abs/2502.16263v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文重新审视了公平表征学习的问题，提出了引入公平性的偏最小二乘法（PLS）组件的方法。&lt;h4&gt;背景&lt;/h4&gt;偏最小二乘法在统计学中被广泛应用于通过提供预测专用的表示来高效地降低数据维度。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法，在构建PLS成分时纳入公平性约束，以实现在线性和非线性情况下的特征构造。&lt;h4&gt;方法&lt;/h4&gt;该新算法利用核嵌入技术，在PLS组件构建过程中加入公平性的考量，并在不同数据集上进行了效率评估。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，所提出的方法相比于标准的公平主成分分析（PCA）方法具有明显的优势。&lt;h4&gt;结论&lt;/h4&gt;通过引入公平性约束到偏最小二乘法中，可以更有效地进行表征学习和预测。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We revisit the problem of fair representation learning by proposing FairPartial Least Squares (PLS) components. PLS is widely used in statistics toefficiently reduce the dimension of the data by providing representationtailored for the prediction. We propose a novel method to incorporate fairnessconstraints in the construction of PLS components. This new algorithm providesa feasible way to construct such features both in the linear and the non linearcase using kernel embeddings. The efficiency of our method is evaluated ondifferent datasets, and we prove its superiority with respect to standard fairPCA method.</description>
      <author>example@mail.com (Elena M. De-Diego, Adrián Perez-Suay, Paula Gordaliza, Jean-Michel Loubes)</author>
      <guid isPermaLink="false">2502.16263v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Survey on Industrial Anomalies Synthesis</title>
      <link>http://arxiv.org/abs/2502.16412v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文全面回顾了异常合成的方法论，提供了第一个工业异常合成（IAS）分类体系。&lt;h4&gt;背景&lt;/h4&gt;现有综述关注的技术范围有限，忽视了跨模态数据和视觉语言模型在异常合成中的作用。&lt;h4&gt;目的&lt;/h4&gt;提供一个统一的、涵盖约40种代表方法的综合回顾，并提出首个细粒度框架来反映方法学进展及其实际应用意义。&lt;h4&gt;方法&lt;/h4&gt;将研究对象分为手工设计、基于分布假设、基于生成模型（GM）和基于视觉语言模型（VLM）四大类，详细介绍了每种类别的代表性技术。&lt;h4&gt;主要发现&lt;/h4&gt;首次提出工业异常合成的分类体系，并深入探讨跨模态合成和大规模视觉语言模型的应用潜力。&lt;h4&gt;结论&lt;/h4&gt;为未来的研究提供指导路径，强调了多模态学习在推进IAS方面的优势、挑战及前景。&lt;h4&gt;翻译&lt;/h4&gt;摘要提到这篇论文全面回顾了异常生成的方法论。现有的综述通常只关注有限的技术，并没有涵盖整个领域的全貌或理解方法间的相互联系。与这些工作不同的是，我们的研究提供了一种统一的视角，涵盖了约40个代表性方法，分为手工设计、基于分布假设、基于生成模型和基于视觉语言模型四类合成方法。此外，我们提出了首个工业异常合成（IAS）分类体系。之前的文献缺乏正式的分类或者使用简化的分类方式，这阻碍了结构化比较和趋势识别的工作。我们的分类体系提供了一个细粒度框架来反映方法学进步及其实际应用意义，并为未来的研究奠定基础。除此之外，我们还探讨了跨模态合成以及大规模视觉语言模型的应用。以往的综述忽视了多模态数据和视觉语言模型在异常生成中的作用，限制了对其优势的理解。我们的调查分析了它们的融合、益处、挑战及前景，提供了一条通过多模态学习提升IAS的道路。更多资源可访问：https://github.com/M-3LAB/awesome-anomaly-synthesis。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper comprehensively reviews anomaly synthesis methodologies. Existingsurveys focus on limited techniques, missing an overall field view andunderstanding method interconnections. In contrast, our study offers a unifiedreview, covering about 40 representative methods across Hand-crafted,Distribution-hypothesis-based, Generative models (GM)-based, andVision-language models (VLM)-based synthesis. We introduce the first industrialanomaly synthesis (IAS) taxonomy. Prior works lack formal classification or usesimplistic taxonomies, hampering structured comparisons and trendidentification. Our taxonomy provides a fine-grained framework reflectingmethodological progress and practical implications, grounding future research.Furthermore, we explore cross-modality synthesis and large-scale VLM. Previoussurveys overlooked multimodal data and VLM in anomaly synthesis, limitinginsights into their advantages. Our survey analyzes their integration,benefits, challenges, and prospects, offering a roadmap to boost IAS withmultimodal learning. More resources are available athttps://github.com/M-3LAB/awesome-anomaly-synthesis.</description>
      <author>example@mail.com (Xichen Xu, Yanshu Wang, Yawen Huang, Jiaqi Liu, Xiaoning Lei, Guoyang Xie, Guannan Jiang, Zhichao Lu)</author>
      <guid isPermaLink="false">2502.16412v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>GraphCheck: Breaking Long-Term Text Barriers with Extracted Knowledge Graph-Powered Fact-Checking</title>
      <link>http://arxiv.org/abs/2502.16514v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;大型语言模型在生成长文本时容易出现细微的事实错误，尤其是在医学等专业领域。现有的事实核查方法面临难以理解复杂多跳关系和高资源消耗的问题。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型被广泛应用，但它们常常会产生轻微的事实性错误，特别是在需要高度准确性的专业领域如医学中。现有基于文档查证的方法在处理长篇文本的复杂多跳关系时存在困难，并且大多数专业化方法依赖于成对比较，导致计算和资源成本高。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的事实核查框架GraphCheck，利用提取的知识图来改进文本表示，以解决当前事实核查方法存在的问题。&lt;h4&gt;方法&lt;/h4&gt;GraphCheck使用知识图并通过图神经网络进一步处理这些知识图作为软提示，使大型语言模型能够更有效地整合结构化知识。此框架特别擅长捕捉多跳推理链，并通过一次推断调用即可完成精确高效的事实核查。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在七个涵盖一般和医学领域的基准测试上，GraphCheck比基线模型总体提高了6.1%的性能。值得注意的是，该方法不仅超过了现有的专业化事实核查工具，还与最先进语言模型DeepSeek-V3和OpenAI-o1达到了相当的表现，同时参数显著减少。&lt;h4&gt;结论&lt;/h4&gt;GraphCheck框架通过引入知识图来改进大型语言模型的事实核查能力，在提高准确性的同时大幅减少了资源需求。这为在专业领域内进行有效事实核查提供了一种新的方法。&lt;h4&gt;翻译&lt;/h4&gt;摘要文本的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) are widely used, but they often generate subtlefactual errors, especially in long-form text. These errors are fatal in somespecialized domains such as medicine. Existing fact-checking with groundingdocuments methods face two main challenges: (1) they struggle to understandcomplex multihop relations in long documents, often overlooking subtle factualerrors; (2) most specialized methods rely on pairwise comparisons, requiringmultiple model calls, leading to high resource and computational costs. Toaddress these challenges, we propose \textbf{\textit{GraphCheck}}, afact-checking framework that uses extracted knowledge graphs to enhance textrepresentation. Graph Neural Networks further process these graphs as a softprompt, enabling LLMs to incorporate structured knowledge more effectively.Enhanced with graph-based reasoning, GraphCheck captures multihop reasoningchains which are often overlooked by existing methods, enabling precise andefficient fact-checking in a single inference call. Experimental results onseven benchmarks spanning both general and medical domains demonstrate a 6.1\%overall improvement over baseline models. Notably, GraphCheck outperformsexisting specialized fact-checkers and achieves comparable performance withstate-of-the-art LLMs, such as DeepSeek-V3 and OpenAI-o1, with significantlyfewer parameters.</description>
      <author>example@mail.com (Yingjian Chen, Haoran Liu, Yinhong Liu, Rui Yang, Han Yuan, Yanran Fu, Pengyuan Zhou, Qingyu Chen, James Caverlee, Irene Li)</author>
      <guid isPermaLink="false">2502.16514v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MAPN: Enhancing Heterogeneous Sparse Graph Representation by Mamba-based Asynchronous Aggregation</title>
      <link>http://arxiv.org/abs/2502.16454v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为Mamba异步传播网络（MAPN）的新模型，旨在解决图神经网络在处理大规模稀疏异构图时面临的问题。&lt;h4&gt;背景&lt;/h4&gt;图神经网络(GNNs)已经成为各种图形相关任务的前沿技术，并且在处理异构图(HetGs)时特别突出。然而，GNN面临着过度压缩、过度平滑和传统消息传递神经网络训练大尺度稀疏图效果不佳等问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决深度神经网络在大规模异构图形上存在的问题。&lt;h4&gt;方法&lt;/h4&gt;MAPN包括两个主要组件：节点序列生成和语义信息聚合。首先，基于元路径通过随机游走生成节点序列，并使用空间状态模型提取不同距离节点的关键信息。随后，它以异步方式汇总多跳和多层的语义信息，有效地保持独特节点特征并缓解深度网络退化问题。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验证明了MAPN在各种下游任务中的图嵌入的有效性，特别是在大尺度稀疏异构图形中具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;通过提出创新性的MAPN模型，论文提供了一种有效处理大规模稀疏异构图的新途径。&lt;h4&gt;翻译&lt;/h4&gt;图神经网络（GNNs）已成为各种与图相关的任务的前沿技术，并且在异构图（HetGs）中尤为突出。然而，这种范式面临一些问题：首先，难以充分利用长距离信息，即过度压缩；其次，过多的消息传递层会产生无法区分的表示形式，称为过度平滑；最后，传统的MPNN在大规模稀疏图上训练效果不佳。为了解决这些挑战，在大型异构图形中使用深度神经网络的问题，本文介绍了基于Mamba的异步传播网络（MAPN），该模型增强了异构稀疏图的表现力。MAPN包括两个主要组件：节点序列生成和语义信息聚合。节点序列最初是根据元路径通过随机游走生成的，这些元路径构成了空间状态模型的基础，该模型提取了不同距离的节点的关键信息。然后，它以异步方式汇总多跳和多层的信息，有效地保持独特的节点特征，并缓解与深度网络退化相关的问题。在各种数据集上的广泛实验表明，在图嵌入的各种下游任务中MAPN的有效性，强调其在大规模稀疏异构图形表示中的显著优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have become the state of the art for variousgraph-related tasks and are particularly prominent in heterogeneous graphs(HetGs). However, several issues plague this paradigm: first, the difficulty infully utilizing long-range information, known as over-squashing; second, thetendency for excessive message-passing layers to produce indistinguishablerepresentations, referred to as over-smoothing; and finally, the inadequacy ofconventional MPNNs to train effectively on large sparse graphs. To addressthese challenges in deep neural networks for large-scale heterogeneous graphs,this paper introduces the Mamba-based Asynchronous Propagation Network (MAPN),which enhances the representation of heterogeneous sparse graphs. MAPN consistsof two primary components: node sequence generation and semantic informationaggregation. Node sequences are initially generated based on meta-paths throughrandom walks, which serve as the foundation for a spatial state model thatextracts essential information from nodes at various distances. It thenasynchronously aggregates semantic information across multiple hops and layers,effectively preserving unique node characteristics and mitigating issuesrelated to deep network degradation. Extensive experiments across diversedatasets demonstrate the effectiveness of MAPN in graph embeddings for variousdownstream tasks underscoring its substantial benefits for graph representationin large sparse heterogeneous graphs.</description>
      <author>example@mail.com (Xuqi Mao, Zhenying He, X. Sean Wang)</author>
      <guid isPermaLink="false">2502.16454v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Network Tomography with Path-Centric Graph Neural Network</title>
      <link>http://arxiv.org/abs/2502.16430v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 6 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种名为DeepNT的深度网络拓扑学框架，用于预测路径性能指标，并能推理出部分先验知识下的网络拓扑结构。&lt;h4&gt;背景&lt;/h4&gt;网络拓扑学在网络监控中至关重要，它利用可观察的路径性能度量值来推断未被观测到的度量值。然而现有的方法要么假设完全了解网络拓扑和度量公式（在许多实际情况下这是不现实的），要么依赖于端到端的黑箱模型。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的框架DeepNT，该框架能够结合数据知识以及适当的归纳偏置来解决网络拓扑学问题。&lt;h4&gt;方法&lt;/h4&gt;引入了以路径为中心的图神经网络，通过推理和聚合构成路径节点序列的嵌入表示来预测性能指标。同时设计了一种学习目标，施加连通性和稀疏性约束在拓扑结构上，并且满足路径性能三角不等式条件。&lt;h4&gt;主要发现&lt;/h4&gt;DeepNT框架在真实世界数据集和合成数据集上的实验表明，在预测性能指标和推理图结构方面超越了最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;通过结合部分先验知识以及数据驱动的方法，可以有效地解决网络拓扑学问题，并能够提供更准确的路径性能度量值预测。&lt;h4&gt;翻译&lt;/h4&gt;摘要：网络拓扑学是网络监控中的关键问题，其中可观察的路径性能指标用于推断未被观测到的指标。然而，大多数现有方法要么假设完全了解网络拓扑和度量公式（在许多实际情况下这是不现实的），要么依赖于端到端的黑箱模型。为了应对这一挑战，在本文中我们提出了一种新的框架DeepNT，该框架利用以路径为中心的图神经网络来预测性能指标，无需预定义的手工设计的指标、假设或真实网络拓扑结构。通过推理和聚合构成路径节点序列的嵌入表示来学习路径嵌入。训练这种以路径为中心的图神经网络需要在离散约束下同时学习神经网络参数和网络拓扑结构，这些约束是由观察到的路径性能指标引入的。这促使我们设计了一个施加连通性和稀疏性约束于拓扑结构以及路径性能三角不等式的学习目标。大量的真实世界数据集和合成数据集上的实验表明，在预测性能指标和推理图拓扑方面，DeepNT优于最先进的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Network tomography is a crucial problem in network monitoring, where theobservable path performance metric values are used to infer the unobservedones, making it essential for tasks such as route selection, fault diagnosis,and traffic control. However, most existing methods either assume completeknowledge of network topology and metric formulas-an unrealistic expectation inmany real-world scenarios with limited observability-or rely entirely onblack-box end-to-end models. To tackle this, in this paper, we argue that agood network tomography requires synergizing the knowledge from both data andappropriate inductive bias from (partial) prior knowledge. To see this, wepropose Deep Network Tomography (DeepNT), a novel framework that leverages apath-centric graph neural network to predict path performance metrics withoutrelying on predefined hand-crafted metrics, assumptions, or the real networktopology. The path-centric graph neural network learns the path embedding byinferring and aggregating the embeddings of the sequence of nodes that composethis path. Training path-centric graph neural networks requires learning theneural netowrk parameters and network topology under discrete constraintsinduced by the observed path performance metrics, which motivates us to designa learning objective that imposes connectivity and sparsity constraints ontopology and path performance triangle inequality on path performance.Extensive experiments on real-world and synthetic datasets demonstrate thesuperiority of DeepNT in predicting performance metrics and inferring graphtopology compared to state-of-the-art methods.</description>
      <author>example@mail.com (Yuntong Hu, Junxiang Wang, Liang Zhao)</author>
      <guid isPermaLink="false">2502.16430v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Automated Keypoint Estimation for Self-Piercing Rivet Joints Using micro-CT Imaging and Transfer Learning</title>
      <link>http://arxiv.org/abs/2502.16752v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于微计算机断层扫描（Micro-CT）成像、机器视觉和深度学习技术的自穿铆钉（SPR）接头非破坏性评价方法。通过合成数据预训练模型，并使用少量真实数据进行迁移学习，以实现关键点自动估计，评估接头的质量。&lt;h4&gt;背景&lt;/h4&gt;在汽车工业中，自穿铆钉接头的结构完整性至关重要，但传统破坏性检测手段存在局限性。&lt;h4&gt;目的&lt;/h4&gt;开发一种成本效益高且可扩展的方法来评价SPR接头的品质。&lt;h4&gt;方法&lt;/h4&gt;使用微计算机断层扫描（Micro-CT）结合机器视觉和深度学习技术，特别是自动关键点估计，利用合成数据进行初始模型训练，并通过真实数据迁移学习适应实际条件。&lt;h4&gt;主要发现&lt;/h4&gt;预训练在合成数据上，再用少量的真实数据进行细化训练，可以缩小领域差距并提高预测精度。该框架为SPR接头评价提供了一种可扩展、成本效益高的解决方案，并为制造过程中的机器视觉和非破坏性检测的更广泛应用奠定了基础。&lt;h4&gt;结论&lt;/h4&gt;通过解决数据稀缺问题并利用先进的机器学习技术，这项工作代表了在工程环境中实现自动质量控制的重要步骤。&lt;h4&gt;翻译&lt;/h4&gt;自穿铆钉（SPR）接头结构完整性对汽车工业至关重要，但传统破坏性方法评价存在挑战。本文提出了一种基于微计算机断层扫描成像、结合机器视觉和深度学习技术的非破坏性评估方案，重点在于自动关键点估计以评估接头质量。鉴于实际微CT数据稀少，本研究使用合成数据进行初始模型训练，并通过迁移学习适应真实情况。采用UNet架构精确定位三个关键点，实现头部高度、锁紧度和底部层厚度等重要参数的测量。详尽验证表明，在合成数据上预训练并在有限的真实数据上微调可以缩小领域差异并提高预测精度。本框架不仅为评价SPR接头提供了可扩展且成本效益高的解决方案，并确立了机器视觉及非破坏性检测在制造流程中更广泛应用的基础。通过处理数据稀缺问题和应用高级机器学习技术，这项工作代表了工程环境中自动质量控制的重要进展。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The structural integrity of self-piercing rivet (SPR) joints is critical inautomotive industries, yet its evaluation poses challenges due to thelimitations of traditional destructive methods. This research introduces aninnovative approach for non-destructive evaluation using micro-CT imaging,Micro-Computed Tomography, combined with machine vision and deep learningtechniques, specifically focusing on automated keypoint estimation to assessjoint quality. Recognizing the scarcity of real micro-CT data, this studyutilizes synthetic data for initial model training, followed by transferlearning to adapt the model for real-world conditions. A UNet-basedarchitecture is employed to localize three keypoints with precision, enablingthe measurement of critical parameters such as head height, interlock, andbottom layer thickness. Extensive validation demonstrates that pre-training onsynthetic data, complemented by fine-tuning with limited real data, bridgesdomain gaps and enhances predictive accuracy. The proposed framework not onlyoffers a scalable and cost-efficient solution for evaluating SPR joints butalso establishes a foundation for broader applications of machine vision andnon-destructive testing in manufacturing processes. By addressing data scarcityand leveraging advanced machine learning techniques, this work represents asignificant step toward automated quality control in engineering contexts.</description>
      <author>example@mail.com (Wei Qin Chuah, Ruwan Tennakoon, Amanda Freis, Mark Easton, Reza Hoseinnezhad, Alireza Bab-Hadiashar)</author>
      <guid isPermaLink="false">2502.16752v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Med-gte-hybrid: A contextual embedding transformer model for extracting actionable information from clinical texts</title>
      <link>http://arxiv.org/abs/2502.15996v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 4 figures, 2 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;提出了一种新型上下文嵌入模型med-gte-hybrid，该模型从gte-large句子变换器中派生而来，用于提取无结构临床叙述中的信息。&lt;h4&gt;目的&lt;/h4&gt;评估med-gte-hybrid在大规模患者队列（来源于MIMIC-IV数据集）上的几种临床预测任务的性能，并展示其在患者分层、聚类和文本检索方面的改进效果。&lt;h4&gt;方法&lt;/h4&gt;采用结合对比学习和去噪自动编码器的模型微调策略，用于评估med-gte-hybrid的性能。同时，在慢性肾脏病（CKD）患者的预后、估计肾小球滤过率(eGFR)预测以及患者死亡率预测等多个临床任务中进行了实验。&lt;h4&gt;主要发现&lt;/h4&gt;结果显示，该混合模型在大规模文本嵌入基准测试（MTEB）上优于当前最先进的模型。此外，在某些评估任务侧重于CKD的情况下，句子变换器的混合微调策略可以应用于其他医学领域，并有潜力改善各种医疗应用中的临床决策和个性化治疗路径。&lt;h4&gt;结论&lt;/h4&gt;med-gte-hybrid在多个方面表现出色，包括患者分层、聚类以及文本检索等，特别是在大规模文本嵌入基准测试中超越了当前最先进的模型。该方法对其他医疗领域的潜在应用也进行了展望，强调其可能改善临床决策和个性化治疗路径的能力。&lt;h4&gt;翻译&lt;/h4&gt;我们引入了一种新型上下文嵌入模型med-gte-hybrid，它是从gte-large句子变换器派生而来的，用于提取无结构化临床叙述中的信息。我们的模型微调策略结合了对比学习和去噪自动编码器。为了评估med-gte-hybrid的性能，我们在MIMIC-IV数据集中提取的大规模患者队列中进行了多个临床预测任务的研究，包括慢性肾脏病（CKD）患者的预后、估计肾小球滤过率(eGFR)预测以及患者死亡率预测。此外，我们展示了该模型在患者分层、聚类和文本检索方面的改进效果，并且在大规模文本嵌入基准测试中超过了当前最先进的模型。虽然我们的某些评估集中在CKD上，但句子变换器的混合微调策略可以转移到其他医疗领域，并具有改善各种医疗应用中的临床决策和个人化治疗路径的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce a novel contextual embedding model med-gte-hybrid that wasderived from the gte-large sentence transformer to extract information fromunstructured clinical narratives. Our model tuning strategy for med-gte-hybridcombines contrastive learning and a denoising autoencoder. To evaluate theperformance of med-gte-hybrid, we investigate several clinical prediction tasksin large patient cohorts extracted from the MIMIC-IV dataset, including ChronicKidney Disease (CKD) patient prognosis, estimated glomerular filtration rate(eGFR) prediction, and patient mortality prediction. Furthermore, wedemonstrate that the med-gte-hybrid model improves patient stratification,clustering, and text retrieval, thus outperforms current state-of-the-artmodels on the Massive Text Embedding Benchmark (MTEB). While some of ourevaluations focus on CKD, our hybrid tuning of sentence transformers could betransferred to other medical domains and has the potential to improve clinicaldecision-making and personalised treatment pathways in various healthcareapplications.</description>
      <author>example@mail.com (Aditya Kumar, Simon Rauch, Mario Cypko, Oliver Amft)</author>
      <guid isPermaLink="false">2502.15996v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>AAD-LLM: Neural Attention-Driven Auditory Scene Understanding</title>
      <link>http://arxiv.org/abs/2502.16794v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的听觉场景理解模型AAD-LLM，该模型通过结合脑电信号来推断听众的注意力，并据此调整响应生成。研究证明了这种方法在多个多说话者场景任务中的有效性。&lt;h4&gt;背景&lt;/h4&gt;现有的听觉基础模型对所有的声音输入处理方式相同，忽略人类听力感知中固有的选择性特点，即人们倾向于关注特定的声音来源而忽略其他声音。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够根据听众注意力生成相应响应的系统，以提高音频生成与人类感知的一致性。&lt;h4&gt;方法&lt;/h4&gt;提出Intention-Informed Auditory Scene Understanding (II-ASU)框架，并构建了一个原型系统AAD-LLM。该模型通过整合颅内脑电图(iEEG)记录来解码听众关注的具体说话者，然后根据推断出的注意力状态调整响应生成。&lt;h4&gt;主要发现&lt;/h4&gt;在多说话者的场景下，AAD-LLM在说话人描述、语音转录与提取以及问答任务中表现出更好的一致性。评估结果显示，客观和主观评价均表明该模型能够更好地符合听众的意图。&lt;h4&gt;结论&lt;/h4&gt;本研究为面向意图感知的听觉人工智能领域铺平了道路，通过将听众感知信息融入机器处理过程，探索了一种新的聆听机制。这为未来的以用户为中心的音频系统开发提供了可能的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文主要探讨了现有听觉模型忽视人类听力选择性的问题，并提出了AAD-LLM原型系统来解决这一问题，展示其在多说话者场景任务中的优越表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Auditory foundation models, including auditory large language models (LLMs),process all sound inputs equally, independent of listener perception. However,human auditory perception is inherently selective: listeners focus on specificspeakers while ignoring others in complex auditory scenes. Existing models donot incorporate this selectivity, limiting their ability to generateperception-aligned responses. To address this, we introduce Intention-InformedAuditory Scene Understanding (II-ASU) and present Auditory Attention-Driven LLM(AAD-LLM), a prototype system that integrates brain signals to infer listenerattention. AAD-LLM extends an auditory LLM by incorporating intracranialelectroencephalography (iEEG) recordings to decode which speaker a listener isattending to and refine responses accordingly. The model first predicts theattended speaker from neural activity, then conditions response generation onthis inferred attentional state. We evaluate AAD-LLM on speaker description,speech transcription and extraction, and question answering in multitalkerscenarios, with both objective and subjective ratings showing improvedalignment with listener intention. By taking a first step towardintention-aware auditory AI, this work explores a new paradigm where listenerperception informs machine listening, paving the way for futurelistener-centered auditory systems. Demo and code available:https://aad-llm.github.io.</description>
      <author>example@mail.com (Xilin Jiang, Sukru Samet Dindar, Vishal Choudhari, Stephan Bickel, Ashesh Mehta, Guy M McKhann, Adeen Flinker, Daniel Friedman, Nima Mesgarani)</author>
      <guid isPermaLink="false">2502.16794v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Diagnosing COVID-19 Severity from Chest X-Ray Images Using ViT and CNN Architectures</title>
      <link>http://arxiv.org/abs/2502.16622v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究构建了一个大型的COVID严重程度数据集，并探讨了迁移学习和视觉变换器在预测患者病情严重程度中的有效性。&lt;h4&gt;背景&lt;/h4&gt;新冠疫情对医疗资源造成了压力，促使人们讨论机器学习如何减轻医生负担并有助于诊断。胸部X光（CXRs）被用于诊断新冠，但很少有研究根据CXRs预测患者的病情严重性。&lt;h4&gt;目的&lt;/h4&gt;通过合并多个数据来源创建一个大型的COVID严重程度数据集，并调查迁移学习在病情严重性和分类任务中的效果。&lt;h4&gt;方法&lt;/h4&gt;使用预训练模型DenseNet161、基于ImageNet和CXR的数据预处理以及视觉变换器（ViT）进行研究。其中，DenseNet161模型在三类病情预测问题中表现出色，而ViT的回归结果最佳。&lt;h4&gt;主要发现&lt;/h4&gt;1. 预训练的DenseNet161模型对三个类别严重程度的预测表现最好，在整体上达到80%的准确性。2. ViT在根据放射科医生评分进行病情严重性评分的回归任务中表现出最优的结果，均方绝对误差为0.5676。&lt;h4&gt;结论&lt;/h4&gt;该研究展示了迁移学习和视觉变换器在从胸部X光预测患者病情严重程度方面具有显著潜力。预训练模型DenseNet161在分类任务上表现最好，而ViT则在回归任务中表现出色。&lt;h4&gt;翻译&lt;/h4&gt;摘要：新冠疫情给医疗资源带来了巨大压力，并引发了关于机器学习如何减轻医生负担并支持诊断的讨论。胸部X光（CXRs）被用于诊断新冠，但鲜有研究依据CXRs预测患者的病情严重性。本研究通过合并三个来源创建了一个大型COVID严重程度数据集，并探讨了使用ImageNet和CXR预训练模型以及视觉变换器在病情回归与分类任务中的有效性的迁移学习方法。其中，一个预训练的DenseNet161模型在三类病情预测问题中表现出色，在整体上达到了80%的准确性（具体为轻度、中度及重度病例准确率分别为77.3%，83.9%，和70%）。ViT则表现出了最优的回归结果，其均方绝对误差仅为0.5676。研究项目源代码公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The COVID-19 pandemic strained healthcare resources and prompted discussionabout how machine learning can alleviate physician burdens and contribute todiagnosis. Chest x-rays (CXRs) are used for diagnosis of COVID-19, but fewstudies predict the severity of a patient's condition from CXRs. In this study,we produce a large COVID severity dataset by merging three sources andinvestigate the efficacy of transfer learning using ImageNet- andCXR-pretrained models and vision transformers (ViTs) in both severityregression and classification tasks. A pretrained DenseNet161 model performedthe best on the three class severity prediction problem, reaching 80% accuracyoverall and 77.3%, 83.9%, and 70% on mild, moderate and severe cases,respectively. The ViT had the best regression results, with a mean absoluteerror of 0.5676 compared to radiologist-predicted severity scores. Theproject's source code is publicly available.</description>
      <author>example@mail.com (Luis Lara, Lucia Eve Berger, Rajesh Raju, Shawn Whitfield)</author>
      <guid isPermaLink="false">2502.16622v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain Model</title>
      <link>http://arxiv.org/abs/2502.16779v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究介绍了一种名为Plane-DUSt3R的新方法，用于多视角房间布局估计。&lt;h4&gt;背景&lt;/h4&gt;目前从多个视角的图像中进行房间布局估计的研究较少，因为涉及复杂的多视图几何结构问题。传统的方法需要分步骤解决相机内参和外参估计、图像匹配以及三角测量等问题。&lt;h4&gt;目的&lt;/h4&gt;引入Plane-DUSt3R方法，利用三维基础模型DUSt3R来简化房间布局估计的过程，并提高其准确性。&lt;h4&gt;方法&lt;/h4&gt;Plane-DUSt3R结合了DUSt3R框架，并在房间布局数据集（Structure3D）上进行微调以估算结构平面。该方法通过生成统一和简洁的结果，仅需一步后处理步骤和2D检测结果即可完成房间布局估计。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Plane-DUSt3R不仅在合成数据集中优于现有最佳方法，在不同图像风格（如卡通）的真实世界数据中也表现出强大的鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;相比传统多步骤的方法，Plane-DUSt3R提供了一种更直接、更有效的解决方案。该模型能够处理多个视角的图像，并且减少了错误累积。&lt;h4&gt;翻译&lt;/h4&gt;房间布局估计从多个视角的图像出发受到的关注较少，这是由于复杂的多视图几何结构问题造成的，需要进行相机内参和外参估计、图像匹配以及三角测量等步骤。然而，在三维重建领域，近期3D基础模型（如DUSt3R）的发展改变了传统的基于结构的运动过程到端到端一步式的转变。为此，我们介绍了一种名为Plane-DUSt3R的方法，利用3D基础模型DUSt3R来进行多视角房间布局估计。通过在房间布局数据集上微调并修改目标以估算结构平面，生成一致且简洁的结果使仅需要一个后处理步骤和2D检测结果就可完成房间布局的估计。与依赖于单视角或全景图像的方法不同，Plane-DUSt3R可以处理多视角图像，并提供了一种简化流程减少错误累积的端到端解决方案。实验结果显示，无论是合成数据还是真实世界中具有多种风格（如卡通）的数据集，Plane-DUSt3R都优于现有最佳方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Room layout estimation from multiple-perspective images is poorlyinvestigated due to the complexities that emerge from multi-view geometry,which requires muti-step solutions such as camera intrinsic and extrinsicestimation, image matching, and triangulation. However, in 3D reconstruction,the advancement of recent 3D foundation models such as DUSt3R has shifted theparadigm from the traditional multi-step structure-from-motion process to anend-to-end single-step approach. To this end, we introduce Plane-DUSt3R}, anovel method for multi-view room layout estimation leveraging the 3D foundationmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes ona room layout dataset (Structure3D) with a modified objective to estimatestructural planes. By generating uniform and parsimonious results, Plane-DUSt3Renables room layout estimation with only a single post-processing step and 2Ddetection results. Unlike previous methods that rely on single-perspective orpanorama image, Plane-DUSt3R extends the setting to handle multiple-perspectiveimages. Moreover, it offers a streamlined, end-to-end solution that simplifiesthe process and reduces error accumulation. Experimental results demonstratethat Plane-DUSt3R not only outperforms state-of-the-art methods on thesynthetic dataset but also proves robust and effective on in the wild data withdifferent image styles such as cartoon.</description>
      <author>example@mail.com (Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue)</author>
      <guid isPermaLink="false">2502.16779v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SDA-DDA Semi-supervised Domain Adaptation with Dynamic Distribution Alignment Network For Emotion Recognition Using EEG Signals</title>
      <link>http://arxiv.org/abs/2502.16485v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新的半监督领域适应框架SDA-DDA，该框架使用动态分布对齐机制和伪标签置信度过滤模块来解决情感脑机接口技术中个体间EEG数据差异的问题。&lt;h4&gt;背景&lt;/h4&gt;情感脑机接口（aBCI）通过监测并识别人类情绪状态促进情感感知技术的发展。然而，不同个体间的EEG信号存在显著的变异性，这阻碍了有效且广泛适用的情感脑机接口模型的发展。&lt;h4&gt;目的&lt;/h4&gt;为了应对这种挑战，研究旨在开发一种新的迁移学习框架，以提高aBCI在跨受试者和跨时段条件下的情感识别准确性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种名为SDA-DDA的新半监督领域适应框架。该框架通过最大均值差异（MMD）和条件最大均值差异（CMMD）来对齐源域与目标域的边际及条件概率分布，并引入动态分布对齐机制以在整个训练过程中调整差异，提高适应性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明SDA-DDA框架在情感识别方面优于现有方法，在跨受试者和跨时段条件下表现尤为突出。这证明了该方法的强大鲁棒性和有效性。&lt;h4&gt;结论&lt;/h4&gt;这项研究推进了情感脑机接口技术的发展，提高了情绪识别的泛化能力和准确性，有助于实现个性化的情感脑机接口应用。&lt;h4&gt;翻译&lt;/h4&gt;在这篇论文中，我们专注于解决个体差异在情感脑机接口（aBCI）中的挑战。通过EEG信号监测和识别人类的情绪状态，从而促进情感感知技术的发展。然而，不同个体间的EEG数据变异性是开发有效且广泛应用的情感脑机接口模型的主要障碍。为了解决这一问题，我们提出了一种新的迁移学习框架——半监督领域适应与动态分布对齐（SDA-DDA）。该方法使用最大均值差异（MMD）和条件最大均值差异（CMMD）来对齐源域与目标域的边际及条件概率分布。引入了动态分布对齐机制在整个训练过程中调整差异，提高适应性。此外，在半监督流程中集成了伪标签置信度过滤模块以优化伪标签生成并改善条件分布估计。在EEG基准数据库（SEED、SEED-IV和DEAP）上的广泛实验验证了SDA-DDA的强大鲁棒性和有效性。结果表明，相较于现有方法，该框架在各种场景下的情感识别中具有优越性，包括跨受试者和跨时段条件。这项进步增强了情感识别的泛化能力和准确性，可能促进个性化aBCI应用的发展。源代码可从https://github.com/XuanSuTrum/SDA-DDA获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we focus on the challenge of individual variability inaffective brain-computer interfaces (aBCI), which employs electroencephalogram(EEG) signals to monitor and recognize human emotional states, therebyfacilitating the advancement of emotion-aware technologies. The variability inEEG data across individuals poses a significant barrier to the development ofeffective and widely applicable aBCI models. To tackle this issue, we propose anovel transfer learning framework called Semi-supervised Domain Adaptation withDynamic Distribution Alignment (SDA-DDA). This approach aligns the marginal andconditional probability distribution of source and target domains using maximummean discrepancy (MMD) and conditional maximum mean discrepancy (CMMD). Weintroduce a dynamic distribution alignment mechanism to adjust differencesthroughout training and enhance adaptation. Additionally, a pseudo-labelconfidence filtering module is integrated into the semi-supervised process torefine pseudo-label generation and improve the estimation of conditionaldistributions. Extensive experiments on EEG benchmark databases (SEED, SEED-IVand DEAP) validate the robustness and effectiveness of SDA-DDA. The resultsdemonstrate its superiority over existing methods in emotion recognition acrossvarious scenarios, including cross-subject and cross-session conditions. Thisadvancement enhances the generalization and accuracy of emotion recognition,potentially fostering the development of personalized aBCI applications. Thesource code is accessible at https://github.com/XuanSuTrum/SDA-DDA.</description>
      <author>example@mail.com (Jiahao Tang)</author>
      <guid isPermaLink="false">2502.16485v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Keeping up with dynamic attackers: Certifying robustness to adaptive online data poisoning</title>
      <link>http://arxiv.org/abs/2502.16737v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Proceedings of the 28th International Conference on Artificial  Intelligence and Statistics (AISTATS) 2025, Mai Khao, Thailand. PMLR: Volume  258&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;论文研究了在在线学习环境中，动态对手进行数据投毒攻击对机器学习算法的影响，并提出了计算这种影响的认证边界的新框架。&lt;h4&gt;背景&lt;/h4&gt;随着基于人类反馈微调基础模型的发展，不信任用户提供的人类反馈增加了对抗性数据中毒的风险。现有研究表明静态对手通过修改训练集可以降低模型鲁棒性，但在实际应用中，动态对手能够观察和响应学习过程，并更有效地注入毒害样本以优化其目标。&lt;h4&gt;目的&lt;/h4&gt;提出一种新框架来计算动态投毒影响的认证边界，并利用这些证书设计出更加稳健的学习算法。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一个新的计算认证边界的框架，并通过均值估计和二元分类问题进行了说明，展示了如何使用该框架设计对抗数据投毒攻击更为有效的学习算法。&lt;h4&gt;主要发现&lt;/h4&gt;在线动态对手相较于静态对手更具威胁性。提出的框架能够帮助构建更稳健的机器学习模型，抵御更复杂的数据中毒攻击。&lt;h4&gt;结论&lt;/h4&gt;研究为解决在线学习中的动态数据投毒问题提供了新方法和理论支持，并指出未来工作应进一步探索该领域。&lt;h4&gt;翻译&lt;/h4&gt;基础模型根据潜在不可信用户的人类反馈进行微调的风险增加了对抗性数据中毒的风险，这需要对学习算法在面对此类攻击时的鲁棒性的研究。现有关于可证明认证鲁棒性以抵御数据投毒攻击的研究主要集中在静态对手上，这些对手可以在训练算法应用前修改一部分用于训练模型的数据集。但在实践中，尤其是在根据人类反馈进行在线学习的情况下，对抗者可以观察和响应学习过程，并注入优化其目标的有毒样本，比他们仅被限制在一次性中毒静态数据集中时更有效。事实上，在先前的工作中已经表明，在线动态对手可能远比静态对手更为强大。我们提出了一种用于计算动态投毒影响认证边界的新型框架，并使用这些证书来设计稳健的学习算法。论文展示了该框架在均值估计和二元分类问题上的应用示例，并概述了进一步工作的扩展方向。实现我们的证书并复制结果的代码可在https://github.com/Avinandan22/Certified-Robustness上获得。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rise of foundation models fine-tuned on human feedback from potentiallyuntrusted users has increased the risk of adversarial data poisoning,necessitating the study of robustness of learning algorithms against suchattacks. Existing research on provable certified robustness against datapoisoning attacks primarily focuses on certifying robustness for staticadversaries who modify a fraction of the dataset used to train the model beforethe training algorithm is applied. In practice, particularly when learning fromhuman feedback in an online sense, adversaries can observe and react to thelearning process and inject poisoned samples that optimize adversarialobjectives better than when they are restricted to poisoning a static datasetonce, before the learning algorithm is applied. Indeed, it has been shown inprior work that online dynamic adversaries can be significantly more powerfulthan static ones. We present a novel framework for computing certified boundson the impact of dynamic poisoning, and use these certificates to design robustlearning algorithms. We give an illustration of the framework for the meanestimation and binary classification problems and outline directions forextending this in further work. The code to implement our certificates andreplicate our results is available athttps://github.com/Avinandan22/Certified-Robustness.</description>
      <author>example@mail.com (Avinandan Bose, Laurent Lessard, Maryam Fazel, Krishnamurthy Dj Dvijotham)</author>
      <guid isPermaLink="false">2502.16737v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Improving Monocular Visual-Inertial Initialization with Structureless Visual-Inertial Bundle Adjustment</title>
      <link>http://arxiv.org/abs/2502.16598v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;单目视觉惯性里程计（VIO）在实时运动追踪应用中表现出色，得益于传感器套件的小巧和低功耗。为了成功启动VIO算法，初始化模块非常重要。&lt;h4&gt;背景&lt;/h4&gt;大多数初始化方法依赖于三维视觉点云的重建。这些方法由于状态向量包含运动状态和三维特征点而导致计算成本较高。&lt;h4&gt;目的&lt;/h4&gt;为了解决这个问题，并提高无结构初始化法的准确性，提出了新的无结构视觉惯性捆绑调整方法以进一步细化先前的无结构解。&lt;h4&gt;方法&lt;/h4&gt;该论文提出了一种新型的无结构视觉惯性捆绑调整算法来改进初始状态估计问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，所提出的方法在保持实时性能的同时显著提高了VIO初始化精度。&lt;h4&gt;结论&lt;/h4&gt;通过新的无结构视觉惯性捆绑调整技术，不仅解决了计算效率的问题，还提升了VIO系统的初始化准确性。&lt;h4&gt;翻译&lt;/h4&gt;单目视觉惯性里程计（VIO）由于传感器套件的小巧和低功耗，在实时运动追踪应用中得到广泛应用。为了成功启动这些算法，初始化模块至关重要。大多数现有方法依赖于三维点云重建，这导致计算成本较高。为解决此问题并提升无结构化方法的性能准确性，研究者提出了新的无结构视觉惯性捆绑调整技术以进一步优化初始状态估计。实验结果表明，该方法显著提高了VIO系统的初始化精度，并保持了实时处理能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Monocular visual inertial odometry (VIO) has facilitated a wide range ofreal-time motion tracking applications, thanks to the small size of the sensorsuite and low power consumption. To successfully bootstrap VIO algorithms, theinitialization module is extremely important. Most initialization methods relyon the reconstruction of 3D visual point clouds. These methods suffer from highcomputational cost as state vector contains both motion states and 3D featurepoints. To address this issue, some researchers recently proposed astructureless initialization method, which can solve the initial state withoutrecovering 3D structure. However, this method potentially compromisesperformance due to the decoupled estimation of rotation and translation, aswell as linear constraints. To improve its accuracy, we propose novelstructureless visual-inertial bundle adjustment to further refine previousstructureless solution. Extensive experiments on real-world datasets show ourmethod significantly improves the VIO initialization accuracy, whilemaintaining real-time performance.</description>
      <author>example@mail.com (Junlin Song, Antoine Richard, Miguel Olivares-Mendez)</author>
      <guid isPermaLink="false">2502.16598v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MOB-GCN: A Novel Multiscale Object-Based Graph Neural Network for Hyperspectral Image Classification</title>
      <link>http://arxiv.org/abs/2502.16289v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种名为MOB-GCN的新型多尺度对象基于图神经网络，用于高光谱图像(HSI)分类。该研究的主要目标是通过利用多尺度对象基础影像分析(OBIA)，提高特征提取和分类性能。&lt;h4&gt;背景&lt;/h4&gt;传统的像素级方法通常由于准确性低和斑点噪声问题而效果不佳，单一尺度的OBIA方法可能忽略了不同细节层次下影像物体的重要信息。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，MOB-GCN通过从多个分割尺度中提取并整合特征来提升分类结果。该模型利用多分辨率图网络（MGN）架构捕捉细粒度和全局空间模式。&lt;h4&gt;方法&lt;/h4&gt;通过构建动态的多尺度图层级结构，MOB-GCN提供了对HSI复杂细节与全局上下文更全面的理解。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与单一尺度图卷积网络(GCN)相比，MOB-GCN在分类准确性、计算效率和降噪方面表现更为出色，尤其是在标注数据有限的情况下。&lt;h4&gt;结论&lt;/h4&gt;MOB-GCN的实现代码可在https://github.com/HySonLab/MultiscaleHSI 上公开获得。&lt;h4&gt;翻译&lt;/h4&gt;该论文提出了一种名为MOB-GCN的新颖多尺度对象基于图神经网络以改进高光谱图像(HSI)分类中的特征提取和分类性能。传统像素级方法因准确性低及斑点噪声而受限，单一尺度OBIA方法容易忽略不同层次细节下的关键信息。为了克服这些问题，MOB-GCN通过从多个分割层级中整合并提取特征来增强其性能，并采用多分辨率图网络（MGN）架构捕捉细粒度与全局空间模式。构建的动态多尺度图层级结构使对HSI复杂性有更深入理解的同时提高了准确性、效率及抗噪能力，特别是在数据标记有限的情况下表现更加突出。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper introduces a novel multiscale object-based graph neural networkcalled MOB-GCN for hyperspectral image (HSI) classification. The central aim ofthis study is to enhance feature extraction and classification performance byutilizing multiscale object-based image analysis (OBIA). Traditionalpixel-based methods often suffer from low accuracy and speckle noise, whilesingle-scale OBIA approaches may overlook crucial information of image objectsat different levels of detail. MOB-GCN overcomes these challenges by extractingand integrating features from multiple segmentation scales, leveraging theMultiresolution Graph Network (MGN) architecture to capture both fine-grainedand global spatial patterns. MOB-GCN addresses this issue by extracting andintegrating features from multiple segmentation scales to improveclassification results using the Multiresolution Graph Network (MGN)architecture that can model fine-grained and global spatial patterns. Byconstructing a dynamic multiscale graph hierarchy, MOB-GCN offers a morecomprehensive understanding of the intricate details and global context ofHSIs. Experimental results demonstrate that MOB-GCN consistently outperformssingle-scale graph convolutional networks (GCNs) in terms of classificationaccuracy, computational efficiency, and noise reduction, particularly whenlabeled data is limited. The implementation of MOB-GCN is publicly available athttps://github.com/HySonLab/MultiscaleHSI</description>
      <author>example@mail.com (Tuan-Anh Yang, Truong-Son Hy, Phuong D. Dao)</author>
      <guid isPermaLink="false">2502.16289v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MimeQA: Towards Socially-Intelligent Nonverbal Foundation Models</title>
      <link>http://arxiv.org/abs/2502.16671v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究介绍了一种新的数据集MimeQA，旨在促进社会智能AI的发展。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能越来越深入人们的日常生活，理解并进行无缝交流的社交智能AI变得愈发重要。然而当前的人工智能在非语言交互的理解上表现不佳。&lt;h4&gt;目的&lt;/h4&gt;通过利用哑剧视频这一富含非言语和社交互动的数据源，改进现有模型对非语言社会互动的理解能力。&lt;h4&gt;方法&lt;/h4&gt;创建了一个新的数据集MimeQA，该数据集中包含221个源自YouTube的哑剧视频，并从中选取了101个视频进行详细标注，形成了806个问题答案配对。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用MimeQA评估最先进的视频大型语言模型(vLLMs)，研究者们发现这些模型在解释非言语互动时准确性较低（15%-30%），且存在过分依赖文本提示而忽视微妙的非言语互动的问题。&lt;h4&gt;结论&lt;/h4&gt;发布数据集旨在推动基础模型的发展，使其能更好地理解非言语的人类交互，促进真正具备社会智能的AI系统的发展。&lt;h4&gt;翻译&lt;/h4&gt;社交智慧型人工智能能够理解和无缝地与人类进行日常生活的交流变得越来越重要。然而目前关于人工社交推理的研究都依赖于语言或以语言为主的方法来进行基准测试和训练模型，导致这些系统在口头沟通方面有所进步但在非言语的社会理解上存在困难。为了克服这一局限性，我们利用了一个新的数据源——哑剧视频来研究非言语社会互动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Socially intelligent AI that can understand and interact seamlessly withhumans in daily lives is increasingly important as AI becomes more closelyintegrated with peoples' daily activities. However, current works in artificialsocial reasoning all rely on language-only, or language-dominant approaches tobenchmark and training models, resulting in systems that are improving inverbal communication but struggle with nonverbal social understanding. Toaddress this limitation, we tap into a novel source of data rich in nonverbaland social interactions -- mime videos. Mimes refer to the art of expressionthrough gesture and movement without spoken words, which presents uniquechallenges and opportunities in interpreting non-verbal social communication.We contribute a new dataset called MimeQA, obtained by sourcing 221 videos fromYouTube, through rigorous annotation and verification, resulting in a benchmarkwith 101 videos and 806 question-answer pairs. Using MimeQA, we evaluatestate-of-the-art video large language models (vLLMs) and find that theiroverall accuracy ranges from 15-30%. Our analysis reveals that vLLMs often failto ground imagined objects and over-rely on the text prompt while ignoringsubtle nonverbal interactions. Our data resources are released athttps://github.com/MIT-MI/MimeQA to inspire future work in foundation modelsthat embody true social intelligence capable of interpreting non-verbal humaninteractions.</description>
      <author>example@mail.com (Hengzhi Li, Megan Tjandrasuwita, Yi R. Fung, Armando Solar-Lezama, Paul Pu Liang)</author>
      <guid isPermaLink="false">2502.16671v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Geometry-Aware 3D Salient Object Detection Network</title>
      <link>http://arxiv.org/abs/2502.16488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于几何感知的3D显著对象检测网络，该网络通过将点聚类成超级点来提升物体的几何边界，并清晰地分割出具有复杂背景的对象。&lt;h4&gt;背景&lt;/h4&gt;近年来，研究人员对点云显著性目标检测产生了兴趣。然而，现有的工作未能充分利用3D对象的几何上下文，在处理具有复杂背景的对象时会导致模糊边界。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够有效利用3D对象的几何信息来清晰分割出完整物体的网络模型。&lt;h4&gt;方法&lt;/h4&gt;首先设计了一个简单的超级点划分模块以将点聚类成超级点，并提出了一种无类别感知损失函数来提高超级点的质量。然后，通过超级点-点注意力机制聚合几何信息到点特征中，预测具有清晰边界的显著图。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，该方法在PCSOD数据集上取得了最新的最优性能。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法能够有效提升3D显著对象检测的效果，并且对于处理复杂背景下的物体分割任务特别有效。&lt;h4&gt;翻译&lt;/h4&gt;点云显著性目标检测已经引起了研究人员的注意。由于现有的工作未能充分利用3D对象的几何上下文，当对具有复杂背景的对象进行分割时会产生模糊边界。在这篇论文中，我们提出了一种基于几何感知的3D显著性对象检测网络，该网络通过将点显式地聚类成超级点来增强物体的几何边界，从而清晰地分割出具有完整边界的物体。具体来说，首先设计了一个简单的超级点划分模块以将点聚类成超级点，并提出了一种无类别感知损失函数来提高超级点的质量。然后，通过超级点-点注意力机制聚合几何信息到点特征中，预测具有清晰边界的显著图。广泛的实验表明，该方法在PCSOD数据集上取得了最新的最优性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Point cloud salient object detection has attracted the attention ofresearchers in recent years. Since existing works do not fully utilize thegeometry context of 3D objects, blurry boundaries are generated when segmentingobjects with complex backgrounds. In this paper, we propose a geometry-aware 3Dsalient object detection network that explicitly clusters points intosuperpoints to enhance the geometric boundaries of objects, thereby segmentingcomplete objects with clear boundaries. Specifically, we first propose a simpleyet effective superpoint partition module to cluster points into superpoints.In order to improve the quality of superpoints, we present a point cloudclass-agnostic loss to learn discriminative point features for clusteringsuperpoints from the object. After obtaining superpoints, we then propose ageometry enhancement module that utilizes superpoint-point attention toaggregate geometric information into point features for predicting the salientmap of the object with clear boundaries. Extensive experiments show that ourmethod achieves new state-of-the-art performance on the PCSOD dataset.</description>
      <author>example@mail.com (Chen Wang, Liyuan Zhang, Le Hui, Qi Liu, Yuchao Dai)</author>
      <guid isPermaLink="false">2502.16488v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>HetFS: A Method for Fast Similarity Search with Ad-hoc Meta-paths on Heterogeneous Information Networks</title>
      <link>http://arxiv.org/abs/2502.16288v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;现实世界的信息网络形成了异构信息网络（HIN），节点和边表示为不同类型的对象和关系。相似性衡量的是两个节点的接近程度，并且主要基于它们连接到的其他节点的相似性递归地确定。&lt;h4&gt;问题定义&lt;/h4&gt;用户可能只对特定类型的链接感兴趣，这些链接在相似性的定义中作为元路径（meta-paths）表示。现有方法要么需要为不同的元路径重新训练异构图神经网络（HGNN），要么使用基于路径的方法进行灵活转换但准确性较低。&lt;h4&gt;目的&lt;/h4&gt;提出HetFS（Fast Similarity for Hetereogeneous information networks with user-given meta-paths）以解决实时查询中的问题，能够根据用户指定的元路径快速提供相似性结果。&lt;h4&gt;方法&lt;/h4&gt;HetFS利用满足元路径限制的路径信息和节点内容来计算相似度。它结合了路径信息与节点本身的特性，提高了准确性。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实验验证，HetFS在处理实时查询方面表现出色，超越现有的HGNN和基于路径的方法，并且在下游应用中也展现了强大的性能，包括链路预测、节点分类以及聚类。&lt;h4&gt;结论&lt;/h4&gt;提出了一种新的方法HetFS来解决异构信息网络中的相似性搜索问题，它能够灵活应对用户指定的元路径并提高准确性与效率。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，上面内容是根据摘要总结和翻译的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s11280-024-01303-1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Numerous real-world information networks form Heterogeneous InformationNetworks (HINs) with diverse objects and relations represented as nodes andedges in heterogeneous graphs. Similarity between nodes quantifies how closelytwo nodes resemble each other, mainly depending on the similarity of the nodesthey are connected to, recursively. Users may be interested in only specifictypes of connections in the similarity definition, represented as meta-paths,i.e., a sequence of node and edge types. Existing Heterogeneous Graph NeuralNetwork (HGNN)-based similarity search methods may accommodate meta-paths, butrequire retraining for different meta-paths. Conversely, existing path-basedsimilarity search methods may switch flexibly between meta-paths but oftensuffer from lower accuracy, as they rely solely on path information. This paperproposes HetFS, a Fast Similarity method for ad-hoc queries with user-givenmeta-paths on Heterogeneous information networks. HetFS provides similarityresults based on path information that satisfies the meta-path restriction, aswell as node content. Extensive experiments demonstrate the effectiveness andefficiency of HetFS in addressing ad-hoc queries, outperformingstate-of-the-art HGNNs and path-based approaches, and showing strongperformance in downstream applications, including link prediction, nodeclassification, and clustering.</description>
      <author>example@mail.com (Xuqi Mao, Zhenyi Chen, Zhenying He, Yinan Jing, Kai Zhang, X. Sean Wang)</author>
      <guid isPermaLink="false">2502.16288v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Semantic Gaussian Mixture Variational Autoencoder for Sequential Recommendation</title>
      <link>http://arxiv.org/abs/2502.16140v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种基于变分自编码器（VAE）的顺序推荐模型SIGMA，以克服现有VAE在处理用户多重兴趣时能力有限的问题。&lt;h4&gt;背景&lt;/h4&gt;目前大多数基于VAE的顺序推荐系统假设序列表示遵循单一高斯分布作为先验分布。然而，在实际应用中，由于用户的多种多样的兴趣，这种单峰分布难以捕捉复杂且多元化的兴趣模式，导致推荐效果受限。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述问题，本文提出了一个新的基于VAE的顺序推荐模型SIGMA，旨在更好地适应用户的不同兴趣并提高推荐性能。&lt;h4&gt;方法&lt;/h4&gt;SIGMA通过假设序列表示遵循混合高斯分布作为先验来建立一个多模态的兴趣模型。此外，为了将这些多模态兴趣纳入序列表示学习中，SIGMA设计了一个概率多重兴趣提取模块和一个兼容混合高斯先验的多兴趣感知ELBO。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，SIGMA在公共数据集上的推荐性能明显优于传统的基于VAE的方法。该模型通过考虑用户的多个兴趣点来提高用户个性化体验。&lt;h4&gt;结论&lt;/h4&gt;SIGMA为顺序推荐提供了一种新的方法论，特别是在处理用户复杂和多元化的兴趣方面显示出潜力。这将对未来的推荐系统设计产生积极影响。&lt;h4&gt;翻译&lt;/h4&gt;变分自编码器（VAE）在序列推荐中通过学习每个用户-项目交互序列的连续分布而不是确定性嵌入来提高数据缺乏下的稳健性和性能表现。然而，现有的基于VAE的序列推荐模型假设序列表示遵循单一高斯分布作为先验，这限制了捕捉复杂用户兴趣的能力，并且当用户具有多种兴趣时会降低推荐性能。由于用户通常会有多个不同的兴趣点，因此，在顺序推荐场景中建立多模态而非单峰的先验更为合理。本文提出了一种新的VAE基于序列推荐模型SIGMA。SIGMA假设序列表示遵循高斯混合分布作为先验，并且每个分量代表一个单独的兴趣。为了提取多重兴趣，SIGMA包括一个多兴趣概率抽取模块以学习每个兴趣的单一高斯分布根据隐含项目超类别进行学习。此外，SIGMA建立了与混合高斯先验兼容的多兴趣感知ELBO，以将多重兴趣整合到序列表示学习中。广泛的实验表明了SIGMA的有效性，代码可以在GitHub上获取（https://github.com/libeibei95/SIGMA）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Variational AutoEncoder (VAE) for Sequential Recommendation (SR), whichlearns a continuous distribution for each user-item interaction sequence ratherthan a determinate embedding, is robust against data deficiency and achievessignificant performance. However, existing VAE-based SR models assume aunimodal Gaussian distribution as the prior distribution of sequencerepresentations, leading to restricted capability to capture complex userinterests and limiting recommendation performance when users have more than oneinterest. Due to that it is common for users to have multiple disparateinterests, we argue that it is more reasonable to establish a multimodal priordistribution in SR scenarios instead of a unimodal one. Therefore, in thispaper, we propose a novel VAE-based SR model named SIGMA. SIGMA assumes thatthe prior of sequence representation conforms to a Gaussian mixturedistribution, where each component of the distribution semantically correspondsto one of multiple interests. For multi-interest elicitation, SIGMA includes aprobabilistic multi-interest extraction module that learns a unimodal Gaussiandistribution for each interest according to implicit item hyper-categories.Additionally, to incorporate the multimodal interests into sequencerepresentation learning, SIGMA constructs a multi-interest-aware ELBO, which iscompatible with the Gaussian mixture prior. Extensive experiments on publicdatasets demonstrate the effectiveness of SIGMA. The code is available athttps://github.com/libeibei95/SIGMA.</description>
      <author>example@mail.com (Beibei Li, Tao Xiang, Beihong Jin, Yiyuan Zheng, Rui Zhao)</author>
      <guid isPermaLink="false">2502.16140v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Anomaly preserving contrastive neural embeddings for end-to-end model-independent searches at the LHC</title>
      <link>http://arxiv.org/abs/2502.15926v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;该论文探讨了通过对比神经嵌入学习强大的低维表示以解决大型强子对撞机(LHC)中异常检测的问题。&lt;h4&gt;背景&lt;/h4&gt;在LHC这样的设备中，由于数据集的规模和复杂性，异常检测是一项关键挑战。通常采用的方法是将高维度探测器数据转换为具有物理意义的低维度特征。&lt;h4&gt;目的&lt;/h4&gt;该研究旨在通过对比神经嵌入方法从数据中提取物理信号并识别潜在的新物理现象。&lt;h4&gt;方法&lt;/h4&gt;文中比较了监督和自我监督的对比学习方法，包括多层感知机(MLP)和Transformer架构。这些方法基于LHC碰撞事件中的动力学可观测属性进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;利用学习到的嵌入表示作为信号无关统计检测方法的输入，在包含所有最终状态中实现了超过十倍于原始特征表现的异常检测性能，并且相对于相同维度的物理信息选择，最多有四倍的改进。研究还展示了这些模型在搜索多种信号时的有效性。&lt;h4&gt;结论&lt;/h4&gt;发现用于背景分类的最佳表示不一定最大化新物理信号的敏感度，表明保持背景结构和增强异常之间存在内在权衡。该论文强调了基础模型在粒子物理学数据中的应用潜力，能够显著提高神经特征提取，并为全包含最终状态下的科学发现提供了可能。&lt;h4&gt;翻译&lt;/h4&gt;异常检测——识别与标准模型预测的偏差——是大型强子对撞机(LHC)面临的关键挑战，由于其数据集的巨大规模和复杂性。这通常通过将高维度探测器数据转换成低维度、物理意义明确的功能来解决。我们利用对比神经嵌入学习强大的低维表示方式处理特征提取以用于异常检测。这种方法保留了潜在的异常信号，这些信号可能指示新物理学，并且使用基于新型机器学习的统计方法进行无信号假设检验，从而可以提取稀有信号。我们比较了监督和自我监督对比学习方法，包括多层感知机(MLP)和Transformer架构，训练时使用的都是LHC碰撞事件中物理对象的动力学可观测属性。所学到的嵌入表示作为包含所有最终状态中的信号无关统计检测方法的输入，在异常检测性能方面比原始特征表现提高了十倍以上，并且相对于相同维度的基于物理学的选择最多提高四倍。我们证明了对于罕见的新物理信号和罕见的标准模型过程，无论在何种最终状态中都能显著提升发现能力，表明其能够同时有效地搜索多种信号。研究还指出背景分类的最佳表示不总是最大化对新物理信号的敏感度，揭示了保持背景结构与异常增强之间的固有权衡。本研究表明基础模型对于粒子物理学数据具有重要的改进潜力，能够提高神经特征提取，并为全包含最终状态下的科学发现提供可能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Anomaly detection -- identifying deviations from Standard Model predictions-- is a key challenge at the Large Hadron Collider due to the size andcomplexity of its datasets. This is typically addressed by transforminghigh-dimensional detector data into lower-dimensional, physically meaningfulfeatures. We tackle feature extraction for anomaly detection by learningpowerful low-dimensional representations via contrastive neural embeddings.This approach preserves potential anomalies indicative of new physics andenables rare signal extraction using novel machine learning-based statisticalmethods for signal-independent hypothesis testing. We compare supervised andself-supervised contrastive learning methods, for both MLP- andTransformer-based neural embeddings, trained on the kinematic observables ofphysics objects in LHC collision events. The learned embeddings serve as inputrepresentations for signal-agnostic statistical detection methods in inclusivefinal states, achieving over ten fold improved detection performance over theoriginal feature representation and up to four fold improvement over using aphysics-informed selections of the same dimensionality. We achieve significantimprovement in discovery power for both rare new physics signals and rareStandard Model processes across diverse final states, demonstrating itsapplicability for efficiently searching for diverse signals simultaneously. Weshow that the optimal representation for background classification does notalways maximize sensitivity to new physics signals, revealing an inherenttrade-off between background structure preservation and anomaly enhancement.Our findings demonstrate that foundation models for particle physics data holdsignificant potential for improving neural feature extraction, enablingscientific discovery in inclusive final states at collider experiments.</description>
      <author>example@mail.com (Kyle Metzger, Lana Xu, Mia Sodini, Thea K. Arrestad, Katya Govorkova, Gaia Grosso, Philip Harris)</author>
      <guid isPermaLink="false">2502.15926v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>FHGE: A Fast Heterogeneous Graph Embedding with Ad-hoc Meta-paths</title>
      <link>http://arxiv.org/abs/2502.16281v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;Graph神经网络(GNNs)在各种与图相关的任务中取得了最先进的成果，并被广泛应用于异构图(HetGs)，其中元路径有助于编码不同节点类型之间的特定语义。尽管现有的异构GNNs（HGNNs）由于其专注于改进对异质性的捕捉效果而具有革命性的表示能力，但高昂的训练成本阻碍了它们在需要处理基于用户定义元路径的即时查询的真实场景中的实际部署。&lt;h4&gt;背景&lt;/h4&gt;现有技术通过改进对异构图中不同节点类型之间特定语义的理解达到了最先进的水平，然而这些方法面临着高昂的计算开销，这使得它们难以应用于实时应用场景。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一挑战，本文提出了一种快速异构图嵌入(FHGE)框架，旨在实现高效、无需重新训练即可生成元路径引导下的图嵌入。&lt;h4&gt;方法&lt;/h4&gt;FHGE采用了两部分设计：分割和重构模块。该系统利用元路径单元(MPUs)将图形分解为局部和全局组件，并在重组过程中迅速整合相关MPU的节点嵌入，使快速适应特定元路径成为可能；此外还应用了双重注意力机制来增强语义捕捉能力。&lt;h4&gt;主要发现&lt;/h4&gt;广泛的实验表明，FHGE框架在生成基于元路径引导下的图嵌入以及下游任务（例如链路预测和节点分类）方面既有效又高效，证明其对实时图形分析具有显著优势。&lt;h4&gt;结论&lt;/h4&gt;FHGE框架提供了一种经济高效的解决方案，在处理异构图中的即时查询时能够快速生成所需的图表示，从而在实际应用中展示出良好的性能和实用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graph neural networks (GNNs) have emerged as the state of the art for avariety of graph-related tasks and have been widely used in HeterogeneousGraphs (HetGs), where meta-paths help encode specific semantics between variousnode types. Despite the revolutionary representation capabilities of existingheterogeneous GNNs (HGNNs) due to their focus on improving the effectiveness ofheterogeneity capturing, the huge training costs hinder their practicaldeployment in real-world scenarios that frequently require handling ad-hocqueries with user-defined meta-paths. To address this, we propose FHGE, a FastHeterogeneous Graph Embedding designed for efficient, retraining-freegeneration of meta-path-guided graph embeddings. The key design of the proposedframework is two-fold: segmentation and reconstruction modules. It employsMeta-Path Units (MPUs) to segment the graph into local and global components,enabling swift integration of node embeddings from relevant MPUs duringreconstruction and allowing quick adaptation to specific meta-paths. Inaddition, a dual attention mechanism is applied to enhance semantics capturing.Extensive experiments across diverse datasets demonstrate the effectiveness andefficiency of FHGE in generating meta-path-guided graph embeddings anddownstream tasks, such as link prediction and node classification, highlightingits significant advantages for real-time graph analysis in ad-hoc queries.</description>
      <author>example@mail.com (Xuqi Mao, Zhenying He, X. Sean Wang)</author>
      <guid isPermaLink="false">2502.16281v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>AdverX-Ray: Ensuring X-Ray Integrity Through Frequency-Sensitive Adversarial VAEs</title>
      <link>http://arxiv.org/abs/2502.16610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  SPIE Medical Imaging 2025 Runner-up 2025 Robert F. Wagner  All-Conference Best Student Paper Award&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为AdverX-Ray的方法，用于评估医疗影像的质量。该方法利用对抗生成网络的高频率伪影来训练一个判别器，以检测数据分布的变化（即协变量偏移），从而保证基于深度学习的计算机辅助诊断和检测系统的性能。&lt;h4&gt;背景&lt;/h4&gt;医学影像质量对基于深度学习的计算机辅助诊断和检测系统至关重要。协变量偏移（由不同成像设备或设置引起的细微数据分布变化）会严重降低模型性能，类似于对抗攻击的影响。&lt;h4&gt;目的&lt;/h4&gt;开发一种快速、轻量的方法来评估医疗影像的质量，以便在使用计算机辅助诊断和检测模型之前进行质量检查。&lt;h4&gt;方法&lt;/h4&gt;AdverX-Ray是一个图像质量评估层，它通过利用生成器产生的次优输出作为负面样本来微调判别器的能力。该系统训练于特定机器型号的X射线图像补丁，并能判断扫描是否符合训练分布或同一设备在不同设置下采集。&lt;h4&gt;主要发现&lt;/h4&gt;AdverX-Ray与各种异常数据检测方法相比，显著优于现有的技术，在使用64个随机选取的X射线图像补丁时达到了96.2%的平均AUROC。&lt;h4&gt;结论&lt;/h4&gt;该系统的轻量级和快速架构使其适合实时应用，并增强医疗成像系统的可靠性。代码和预训练模型公开可用。&lt;h4&gt;翻译&lt;/h4&gt;确保医学影像的质量与完整性对于保持基于深度学习的计算机辅助诊断（CAD）和检测（CAD）系统中的诊断准确性至关重要。协变量偏移是由不同成像设备或设置引起的数据分布细微变化，可严重降低模型性能，类似于对抗攻击的影响。因此，评估这些图像质量的方法必须是快速且轻量级的，以便在使用CAD模型之前完成。AdverX-Ray通过充当一个影像质量评估层来满足此需求，并有效检测协变量偏移。经过特定型号机器X射线图像补丁训练的AdverX-Ray能够判断扫描是否符合训练分布或同一设备不同设置下采集的图像。与各种异常数据检测方法相比，AdverX-Ray显著优于现有的技术，在使用64个随机选取的X射线图像补丁时达到了96.2%的平均AUROC。该系统轻量级和快速架构适合实时应用，并增强了医疗成像系统的可靠性。代码和预训练模型公开可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring the quality and integrity of medical images is crucial formaintaining diagnostic accuracy in deep learning-based Computer-Aided Diagnosisand Computer-Aided Detection (CAD) systems. Covariate shifts are subtlevariations in the data distribution caused by different imaging devices orsettings and can severely degrade model performance, similar to the effects ofadversarial attacks. Therefore, it is vital to have a lightweight and fastmethod to assess the quality of these images prior to using CAD models.AdverX-Ray addresses this need by serving as an image-quality assessment layer,designed to detect covariate shifts effectively. This Adversarial VariationalAutoencoder prioritizes the discriminator's role, using the suboptimal outputsof the generator as negative samples to fine-tune the discriminator's abilityto identify high-frequency artifacts. Images generated by adversarial networksoften exhibit severe high-frequency artifacts, guiding the discriminator tofocus excessively on these components. This makes the discriminator ideal forthis approach. Trained on patches from X-ray images of specific machine models,AdverX-Ray can evaluate whether a scan matches the training distribution, or ifa scan from the same machine is captured under different settings. Extensivecomparisons with various OOD detection methods show that AdverX-Raysignificantly outperforms existing techniques, achieving a 96.2% average AUROCusing only 64 random patches from an X-ray. Its lightweight and fastarchitecture makes it suitable for real-time applications, enhancing thereliability of medical imaging systems. The code and pretrained models arepublicly available.</description>
      <author>example@mail.com (Francisco Caetano, Christiaan Viviers, Lena Filatova, Peter H. N. de With, Fons van der Sommen)</author>
      <guid isPermaLink="false">2502.16610v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Iterative Auto-Annotation for Scientific Named Entity Recognition Using BERT-Based Models</title>
      <link>http://arxiv.org/abs/2502.16312v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文介绍了一种使用基于BERT的模型进行科学命名实体识别（SciNER）的迭代方法，并利用少量高质量的手动标注数据集通过迁移学习来微调预训练模型。&lt;h4&gt;背景&lt;/h4&gt;在缺乏大规模标注数据的情况下，如何有效地提高自然语言处理任务中的预测准确性是一个挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于BERT的SciNER迭代改进方法，并评估其性能。&lt;h4&gt;方法&lt;/h4&gt;采用两种不同的模型（dslim/bert-large-NER和bert-large-cased），通过高质量的手动注释数据集进行微调，然后使用微调后的模型自动标注更大的数据集，并进一步进行多轮微调。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，基于BERT的模型在预测准确性和F1分数方面有了显著提高，特别是对于较少见的实体类。bert-large-cased模型始终优于dslim/bert-large-NER模型。&lt;h4&gt;结论&lt;/h4&gt;该方法展示了一种有效的SciNER迭代改进技术，在标注数据有限的情况下具有广泛的应用潜力，并且未来的研究可以考虑使用未标记的数据进行微调以及探索更强力的编码器如RoBERTa。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容为：本文提出了一种使用基于BERT的模型进行科学命名实体识别（SciNER）的迭代方法。利用转移学习来对预训练模型进行微调，其中使用了少量但高质量的手动标注数据集。通过使用经过微调后的模型自动标注更大的数据集，并随后进一步多轮微调这一过程得到了反复精炼。我们评估了两种模型（dslim/bert-large-NER和bert-largecased），结果表明后者始终优于前者。该方法在预测准确性和F1分数方面表现出了显著的改进，尤其是对于较少见的实体类。未来的研究可以考虑使用未标注的数据进行微调，并探索更强大的编码器如RoBERTa以及扩展手动注释的范围。这一方法在自然语言处理任务中具有广泛应用潜力，尤其是在数据标签受限的情况下。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper presents an iterative approach to performing Scientific NamedEntity Recognition (SciNER) using BERT-based models. We leverage transferlearning to fine-tune pretrained models with a small but high-quality set ofmanually annotated data. The process is iteratively refined by using thefine-tuned model to auto-annotate a larger dataset, followed by additionalrounds of fine-tuning. We evaluated two models, dslim/bert-large-NER andbert-largecased, and found that bert-large-cased consistently outperformed theformer. Our approach demonstrated significant improvements in predictionaccuracy and F1 scores, especially for less common entity classes. Future workcould include pertaining with unlabeled data, exploring more powerful encoderslike RoBERTa, and expanding the scope of manual annotations. This methodologyhas broader applications in NLP tasks where access to labeled data is limited.</description>
      <author>example@mail.com (Kartik Gupta)</author>
      <guid isPermaLink="false">2502.16312v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Verifying Quantized Graph Neural Networks is PSPACE-complete</title>
      <link>http://arxiv.org/abs/2502.16244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;研究验证量化图神经网络（GNNs）的可行性，其中使用固定宽度算术表示数。&lt;h4&gt;目的&lt;/h4&gt;引入线性约束有效性问题(LVP)来验证GNNs属性，并提供一个从LVP实例到逻辑语言的有效翻译。&lt;h4&gt;方法&lt;/h4&gt;提出了一种证明系统并展示了对于任何合理的激活函数，LVP属于PSPACE复杂度类别。同时表明了PSPACE难度，暗示虽然关于量化GNN的推理是可行的，但仍然是计算上具有挑战性的任务。&lt;h4&gt;主要发现&lt;/h4&gt;验证量化GNNs属性的问题(LVP)被定义为在PSPACE中，并证明了其PSPACE难解性。&lt;h4&gt;结论&lt;/h4&gt;尽管存在一定的计算难度，但对于合理激活函数而言，在PSPACE复杂度类别内解决问题是可能的。这表明对量化GNN的推理虽然具有挑战性但仍可实现。&lt;h4&gt;翻译&lt;/h4&gt;本论文研究使用固定宽度算术表示数的量化图神经网络（GNNs）的验证问题，并引入线性约束有效性(LVP)问题，以验证GNN属性的有效性和提供LVP实例到逻辑语言的高效转换。结果显示，对于任何合理激活函数来说，该问题属于PSPACE复杂度类别；同时证明了其PSPACE难度。表明虽然对量化GNN进行推理是可行的，但计算上仍然具有挑战性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we investigate verification of quantized Graph Neural Networks(GNNs), where some fixed-width arithmetic is used to represent numbers. Weintroduce the linear-constrained validity (LVP) problem for verifying GNNsproperties, and provide an efficient translation from LVP instances into alogical language. We show that LVP is in PSPACE, for any reasonable activationfunctions. We provide a proof system. We also prove PSPACE-hardness, indicatingthat while reasoning about quantized GNNs is feasible, it remains generallycomputationally challenging.</description>
      <author>example@mail.com (Marco Sälzer, François Schwarzentruber, Nicolas Troquard)</author>
      <guid isPermaLink="false">2502.16244v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SelaVPR++: Towards Seamless Adaptation of Foundation Models for Efficient Place Recognition</title>
      <link>http://arxiv.org/abs/2502.16601v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种改进的视觉地方识别(SelaVPR++)方法，通过使用轻量级多尺度卷积(MultiConv)适配器来提高基础模型向视觉地方识别任务适应的有效性和性能。&lt;h4&gt;背景&lt;/h4&gt;近期研究表明，利用预训练的视觉基础模型进行视觉位置识别(VPR)可以取得良好的效果。作者之前的工作提出了SelaVPR方法，该方法通过参数高效的方法实现了基础模型向VPR的无缝转换。&lt;h4&gt;目的&lt;/h4&gt;为了提高效率和性能，论文提出了一种SelaVPR的扩展版本——SelaVPR++。&lt;h4&gt;方法&lt;/h4&gt;引入了参数、时间和内存高效的适应策略，利用轻量级多尺度卷积适配器来细化从冻结基础骨干网络获得的中间特征；创新性地提出了更有效的重新排序范式，通过使用紧凑型二进制特征进行初步检索，并采用鲁棒的浮点特征进行重新排序。&lt;h4&gt;主要发现&lt;/h4&gt;提出的相似度约束深度哈希方法可以获得这样的二进制特征，并且可以很容易地集成到VPR流程中；优化了训练策略，统一了几种常见训练数据集的训练协议以更好地培训VPR模型。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明……&lt;h4&gt;翻译&lt;/h4&gt;最近的研究表明，使用预训练视觉基础模型进行视觉位置识别（VPR）可以实现令人满意的结果。在我们的先前工作中，我们提出了一种方法，将视觉基础模型无缝转换为VPR（SelaVPR）。这种适应方法可以通过参数高效的方法产生全局和局部特征来区分地标，从而用于两阶段的视觉位置识别。尽管SelaVPR已经取得了具有竞争力的效果，但我们认为之前的适应方法在训练时间和GPU内存使用上是低效的，并且重新排序范式在检索延迟和存储使用方面也是昂贵的。为了追求更高的效率和更好的性能，我们提出了SelaVPR的一种扩展版本——SelaVPR++。具体来说，首先设计了一种参数、时间、内存高效的适应方法，该方法利用轻量级多尺度卷积（MultiConv）适配器来细化来自冻结基础骨干网络的中间特征，在训练期间不会反向传播通过基础模型的梯度，并且这种MultiConv适配器可以促进沿空间轴上的特征交互并引入适当的局部先验，从而实现更高的效率和更好的性能。此外，我们提出了一种创新性的重新排序范式以实现更高效的VPR：不依赖于本地特征进行重新排序，这在延迟和存储使用方面会产生巨大的开销，而是采用紧凑的二进制特征用于初步检索，并用鲁棒的浮点（全局）特征用于重新排序。为了获得这些二进制特征，我们提出了一种相似度约束深度哈希方法，可以很容易地整合到VPR流程中。最后，我们改进了我们的训练策略并统一了几种常见训练数据集的训练协议以合并它们以便更好地培训VPR模型。广泛的实验表明……&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent studies show that the visual place recognition (VPR) method usingpre-trained visual foundation models can achieve promising performance. In ourprevious work, we propose a novel method to realize seamless adaptation offoundation models to VPR (SelaVPR). This method can produce both global andlocal features that focus on discriminative landmarks to recognize places fortwo-stage VPR by a parameter-efficient adaptation approach. Although SelaVPRhas achieved competitive results, we argue that the previous adaptation isinefficient in training time and GPU memory usage, and the re-ranking paradigmis also costly in retrieval latency and storage usage. In pursuit of higherefficiency and better performance, we propose an extension of the SelaVPR,called SelaVPR++. Concretely, we first design a parameter-, time-, andmemory-efficient adaptation method that uses lightweight multi-scaleconvolution (MultiConv) adapters to refine intermediate features from thefrozen foundation backbone. This adaptation method does not back-propagategradients through the backbone during training, and the MultiConv adapterfacilitates feature interactions along the spatial axes and introduces properlocal priors, thus achieving higher efficiency and better performance.Moreover, we propose an innovative re-ranking paradigm for more efficient VPR.Instead of relying on local features for re-ranking, which incurs huge overheadin latency and storage, we employ compact binary features for initial retrievaland robust floating-point (global) features for re-ranking. To obtain suchbinary features, we propose a similarity-constrained deep hashing method, whichcan be easily integrated into the VPR pipeline. Finally, we improve ourtraining strategy and unify the training protocol of several common trainingdatasets to merge them for better training of VPR models. Extensive experimentsshow that ......</description>
      <author>example@mail.com (Feng Lu, Tong Jin, Xiangyuan Lan, Lijun Zhang, Yunpeng Liu, Yaowei Wang, Chun Yuan)</author>
      <guid isPermaLink="false">2502.16601v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Graph Attention Convolutional U-NET: A Semantic Segmentation Model for Identifying Flooded Areas</title>
      <link>http://arxiv.org/abs/2502.15907v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于图神经网络的自动化洪水区域识别模型，即Graph Attention Convolutional U-NET (GAC-UNET)，该模型结合了图注意力机制和Chebyshev层，并在实验中显示出优于其他方法的表现。&lt;h4&gt;背景&lt;/h4&gt;近年来，由于人为气候变化和无规划的城市建设导致洪灾事件增多。准确地识别洪水区域对于有效的灾害管理和城市规划至关重要。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于图神经网络的方法来自动识别洪水区域，利用转移学习和模型重新编程以提高洪水区域分割模型的准确性。&lt;h4&gt;方法&lt;/h4&gt;采用Graph Attention Convolutional U-NET (GAC-UNET) 模型，该模型将图注意力机制和Chebyshev层融入到U-Net架构中，并探索了转移学习的应用。&lt;h4&gt;主要发现&lt;/h4&gt;提出的GAC-UNET模型在mAP、Dice分数和IoU指标上分别达到了91%，94%和89%，超过了其他方法，显示出了显著的性能提升。&lt;h4&gt;结论&lt;/h4&gt;该研究为洪水易发区域未来的基础设施规划提供了有价值的见解，并表明图神经网络可以在自动化识别洪水区域方面提供改进的机会。&lt;h4&gt;翻译&lt;/h4&gt;不断加剧的人类活动导致的气候变化以及未规划的城市建设在过去几年里增加了洪灾事件。准确地辨识受影响地区对于有效的灾害管理和城市规划至关重要。虽然有少量研究采用卷积神经网络和基于变压器的语义分割技术来识别航空影像中的洪水区域，但图神经网络的发展创造了许多改进的机会。这篇论文提出了一种创新的方法——基于图神经网络（GAC-UNET）模型，用于自动化地辨识洪水区域，并在实验中展示了显著优于其他方法的结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The increasing impact of human-induced climate change and unplanned urbanconstructions has increased flooding incidents in recent years. Accurateidentification of flooded areas is crucial for effective disaster managementand urban planning. While few works have utilized convolutional neural networksand transformer-based semantic segmentation techniques for identifying floodedareas from aerial footage, recent developments in graph neural networks havecreated improvement opportunities. This paper proposes an innovative approach,the Graph Attention Convolutional U-NET (GAC-UNET) model, based on graph neuralnetworks for automated identification of flooded areas. The model incorporatesa graph attention mechanism and Chebyshev layers into the U-Net architecture.Furthermore, this paper explores the applicability of transfer learning andmodel reprogramming to enhance the accuracy of flood area segmentation models.Empirical results demonstrate that the proposed GAC-UNET model, outperformsother approaches with 91\% mAP, 94\% dice score, and 89\% IoU, providingvaluable insights for informed decision-making and better planning of futureinfrastructures in flood-prone areas.</description>
      <author>example@mail.com (Muhammad Umair Danish, Madhushan Buwaneswaran, Tehara Fonseka, Katarina Grolinger)</author>
      <guid isPermaLink="false">2502.15907v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Dragen3D: Multiview Geometry Consistent 3D Gaussian Generation with Drag-Based Control</title>
      <link>http://arxiv.org/abs/2502.16475v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;单张图像3D生成已成为一个重要的研究领域，在虚拟现实、三维建模和数字内容创作中起着重要作用。&lt;h4&gt;问题&lt;/h4&gt;现有的方法面临多视角几何一致性不足及生成过程可控性有限的问题，这些问题显著限制了它们的实用性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法Drage3D来解决这些挑战，该方法利用3D高斯斑点实现具有几何一致性和可控制性的3D生成。&lt;h4&gt;方法&lt;/h4&gt;{'Anchor-Gaussian变分自编码器(AGSVAE)': '将点云和单张图像编码成锚定潜在变量，并通过解码锚定潜在变量生成3DGS，从而实现高效的潜在空间生成。', 'Seed-Point-Driven策略': '该策略包括两步：首先生成稀疏种子点作为粗糙的几何表示；其次通过Seed-Anchor映射模块将这些种子点映射到锚定潜在变量。这种策略确保了几何一致性，并且用户可以直观地拖动种子点来变形最终3DGS几何，变化会通过锚定潜在变量传播。', '无需2D扩散先验': '我们是首个实现不依赖于2D扩散先验的几何可控制性3D高斯生成和编辑的方法。'}&lt;h4&gt;主要发现&lt;/h4&gt;Drage3D在保持高质量3D生成的同时，实现了多视角几何一致性和用户可控性。&lt;h4&gt;结论&lt;/h4&gt;Drage3D为单图像到三维生成开辟了新的道路，显著提高了现有技术的实用性。&lt;h4&gt;翻译&lt;/h4&gt;单张图像3D生成已作为一项重要研究课题崛起，在虚拟现实、3D建模和数字内容创建中扮演着至关重要的角色。然而，现存的方法面临着诸如缺乏多视角几何一致性以及在生成过程中可控性有限等挑战，这些严重限制了它们的实用性。为了解决这些问题，我们引入了一种名为Drage3D的新方法，利用三维高斯斑点（3DGS）实现了具有几何一致性和可控制性的3D生成。通过Anchor-Gaussian变分自编码器(AGSVAE)，该模型将点云和单张图像编码为锚定潜在变量，并通过这些潜在变量解码出3DGS，从而实现高效的潜在空间生成。为了达成多视角几何一致性及可控性生成目标，我们提出了一种Seed-Point驱动策略：首先生成稀疏种子点作为粗糙的几何表示；其次通过Seed-Anchor映射模块将它们映射到锚定潜在变量。这种策略确保了几何一致性，并且用户可以直观地拖动这些种子点来变形最终3DGS几何，变化会经过锚定潜在变量传播。据我们所知，我们在不依赖于2D扩散先验的情况下首次实现了具有可控制性三维高斯生成和编辑的方法，同时保持了与最先进方法相当的3D生成质量。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Single-image 3D generation has emerged as a prominent research topic, playinga vital role in virtual reality, 3D modeling, and digital content creation.However, existing methods face challenges such as a lack of multi-viewgeometric consistency and limited controllability during the generationprocess, which significantly restrict their usability. % To tackle thesechallenges, we introduce Dragen3D, a novel approach that achieves geometricallyconsistent and controllable 3D generation leveraging 3D Gaussian Splatting(3DGS). We introduce the Anchor-Gaussian Variational Autoencoder (Anchor-GSVAE), which encodes a point cloud and a single image into anchor latents anddecode these latents into 3DGS, enabling efficient latent-space generation. Toenable multi-view geometry consistent and controllable generation, we propose aSeed-Point-Driven strategy: first generate sparse seed points as a coarsegeometry representation, then map them to anchor latents via the Seed-AnchorMapping Module. Geometric consistency is ensured by the easily learned sparseseed points, and users can intuitively drag the seed points to deform the final3DGS geometry, with changes propagated through the anchor latents. To the bestof our knowledge, we are the first to achieve geometrically controllable 3DGaussian generation and editing without relying on 2D diffusion priors,delivering comparable 3D generation quality to state-of-the-art methods.</description>
      <author>example@mail.com (Jinbo Yan, Alan Zhao, Yixin Hu)</author>
      <guid isPermaLink="false">2502.16475v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Graph Self-Supervised Learning with Learnable Structural and Positional Encodings</title>
      <link>http://arxiv.org/abs/2502.16233v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by The World Wide Web Conference (WWW) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;介绍了新型图自监督学习框架GenHopNet，该框架旨在解决传统GSSL难以捕捉复杂结构特征的问题。&lt;h4&gt;背景&lt;/h4&gt;传统的图自监督学习（GSSL）在捕获复杂的结构性质方面存在困难。这主要是由于两个原因：一是常规图神经网络（GNNs）无法很好地表示复杂的拓扑特征；二是自监督学习仅仅关注最终的图表示，而忽略了整个过程中的结构信息。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来克服这些限制，并增强在区分具有相似局部但不同全局拓扑的图形的能力。&lt;h4&gt;方法&lt;/h4&gt;引入了GenHopNet框架，这是一个融合$k$-跳消息传递机制的GNN框架。此外还提出了一个既考虑结构性又注重位置信息的自监督学习框架，该框架能够在整个学习过程中整合图的拓扑信息。&lt;h4&gt;主要发现&lt;/h4&gt;理论证明GenHopNet超越了经典的Weisfeiler-Lehman（WL）测试在图同构上的表达能力，并且实验结果显示这种方法在图分类数据集上优于现有方法，特别是在那些用于测试结构敏感性的数据集上表现尤为突出。同时保持计算效率。&lt;h4&gt;结论&lt;/h4&gt;所提出的GenHopNet框架及其相关的自监督学习策略显著增强了GSSL区分具有相似局部但不同全局拓扑的图形的能力。&lt;h4&gt;翻译&lt;/h4&gt;传统的图自我监督学习(GSSL)在捕捉复杂的结构特性方面存在困难，这主要是由于两个因素：(1) 常规图神经网络（GNNs）无法充分代表复杂的拓扑特征；(2) 自我监督学习仅关注最终的图表示。为了解决这些问题，我们提出了一个新的框架GenHopNet，它是一个融合了$k$-跳消息传递机制的GNN架构，增强了捕捉局部结构信息的能力而无需显式地提取子结构。理论证明表明，GenHopNet在表达能力上超越了经典的Weisfeiler-Lehman (WL) 测试用于图同构测试。此外，我们还提出了一种基于位置和结构感知的GSSL框架，在整个学习过程中整合拓扑信息，使模型能够同时敏感于图形的拓扑且对特定的结构及特征增强保持不变性。在包括旨在测试结构敏感性的图分类数据集上的全面实验表明，我们的方法在性能上始终优于现有的方法，并且计算效率高。我们的重要贡献在于大幅提升了GSSL区分具有相似局部但不同全局拓扑的图形的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3696410.3714745&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Graph Self-Supervised Learning (GSSL) struggles to capturecomplex structural properties well. This limitation stems from two mainfactors: (1) the inadequacy of conventional Graph Neural Networks (GNNs) inrepresenting sophisticated topological features, and (2) the focus ofself-supervised learning solely on final graph representations. To addressthese issues, we introduce \emph{GenHopNet}, a GNN framework that integrates a$k$-hop message-passing scheme, enhancing its ability to capture localstructural information without explicit substructure extraction. Wetheoretically demonstrate that \emph{GenHopNet} surpasses the expressiveness ofthe classical Weisfeiler-Lehman (WL) test for graph isomorphism. Furthermore,we propose a structural- and positional-aware GSSL framework that incorporatestopological information throughout the learning process. This approach enablesthe learning of representations that are both sensitive to graph topology andinvariant to specific structural and feature augmentations. Comprehensiveexperiments on graph classification datasets, including those designed to teststructural sensitivity, show that our method consistently outperforms theexisting approaches and maintains computational efficiency. Our worksignificantly advances GSSL's capability in distinguishing graphs with similarlocal structures but different global topologies.</description>
      <author>example@mail.com (Asiri Wijesinghe, Hao Zhu, Piotr Koniusz)</author>
      <guid isPermaLink="false">2502.16233v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>COMPASS: Cross-embodiment Mobility Policy via Residual RL and Skill Synthesis</title>
      <link>http://arxiv.org/abs/2502.16372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种新的工作流程COMPASS，旨在开发跨机器人形态的移动策略，通过结合模仿学习（IL）、残差强化学习（RL）和策略蒸馏的方法来克服现有方法在面对新硬件平台或不同环境时所面临的挑战。&lt;h4&gt;背景&lt;/h4&gt;随着机器人应用领域的扩展，通用且可应用于多种物理形态的移动策略变得日益重要。传统的移动栈虽然在特定平台上有效，但在新的机器人形态上难以大规模部署。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够解决现有方法中普遍存在的协变量偏移、稀疏采样和环境适应性问题的工作流程，实现高效的跨身体形式（embodiment）的移动策略开发。&lt;h4&gt;方法&lt;/h4&gt;通过模仿学习在移动机器人上训练基础模型，结合世界模型与移动策略；使用残差强化学习进一步微调特定身体形态的策略，并利用预训练表示提高采样效率以处理各种物理约束和传感器模式；最后通过策略蒸馏将这些专家策略合并为单一稳健的跨身形态策略。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，COMPASS能够有效扩展至不同的机器人平台，在适应不同环境配置的同时保持高成功率（相较于预训练模仿学习策略高出约5倍）。&lt;h4&gt;结论&lt;/h4&gt;提出的框架提供了高效且可扩展的方法来实现跨身体形式的移动策略，使具有不同设计的机器人能够在复杂场景中安全有效地导航。&lt;h4&gt;翻译&lt;/h4&gt;随着机器人的广泛应用领域越来越多，通用化的跨形态机动性策略变得越来越重要。经典机动栈在特定平台上已经证明是有效的，但在新的实体上进行规模化部署存在挑战。基于学习的方法（如模仿学习和强化学习）提供了解决方案，但它们也面临协变量漂移、大型环境中的稀疏采样以及实体特异性约束的问题。本文介绍了COMPASS，这是一个通过结合模仿学习、残差RL和策略蒸馏来开发跨实体机动性策略的新工作流程。我们首先在移动机器人上进行模仿学习，并利用易于访问的教师策略训练一个基础模型，该模型将世界模型与机动策略相结合。在此基础上，我们使用残差RL来微调特定实体上的策略，利用预训练表示提高采样效率，在处理各种物理约束和传感器模式方面更加高效。最后，通过策略蒸馏将这些实体专家策略合并为单一稳健的跨实体策略。实验证明了COMPASS能够有效地在不同的机器人平台上扩展，同时保持对不同环境配置的适应性，实现了一个成功率比预训练模仿学习策略高约5倍的一般策略。该框架提供了一种高效且可扩展的方法来解决跨实体机动问题，使设计不同的机器人能够在复杂场景中安全和有效地导航。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As robots are increasingly deployed in diverse application domains,generalizable cross-embodiment mobility policies are increasingly essential.While classical mobility stacks have proven effective on specific robotplatforms, they pose significant challenges when scaling to new embodiments.Learning-based methods, such as imitation learning (IL) and reinforcementlearning (RL), offer alternative solutions but suffer from covariate shift,sparse sampling in large environments, and embodiment-specific constraints.  This paper introduces COMPASS, a novel workflow for developingcross-embodiment mobility policies by integrating IL, residual RL, and policydistillation. We begin with IL on a mobile robot, leveraging easily accessibleteacher policies to train a foundational model that combines a world model witha mobility policy. Building on this base, we employ residual RL to fine-tuneembodiment-specific policies, exploiting pre-trained representations to improvesampling efficiency in handling various physical constraints and sensormodalities. Finally, policy distillation merges these embodiment-specialistpolicies into a single robust cross-embodiment policy.  We empirically demonstrate that COMPASS scales effectively across diverserobot platforms while maintaining adaptability to various environmentconfigurations, achieving a generalist policy with a success rate approximately5X higher than the pre-trained IL policy. The resulting framework offers anefficient, scalable solution for cross-embodiment mobility, enabling robotswith different designs to navigate safely and efficiently in complex scenarios.</description>
      <author>example@mail.com (Wei Liu, Huihua Zhao, Chenran Li, Joydeep Biswas, Soha Pouya, Yan Chang)</author>
      <guid isPermaLink="false">2502.16372v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Robust Dynamic Facial Expression Recognition</title>
      <link>http://arxiv.org/abs/2502.16129v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种名为Robust Dynamic Facial Expression Recognition (RDFER)的新方法，该方法旨在解决动态面部表情识别中的硬样本和噪声样本共存的问题。&lt;h4&gt;背景&lt;/h4&gt;目前关于动态面部表情识别的研究主要集中在学习在有噪音或难以处理的数据下的表示形式上，但如何同时处理这两种类型的数据仍然是一个未解之谜。&lt;h4&gt;目的&lt;/h4&gt;为了克服现有问题并提高模型的鲁棒性，该研究设计了一个能够区分硬样本和噪声样本的方法，并提出了一种关键表情重采样框架以及双重流分层网络来增强模型对主要表达的理解能力。&lt;h4&gt;方法&lt;/h4&gt;通过评估模型在不同视频片段上的预测一致性来识别硬样本和噪声样本。采用关键表情重新采样的框架来减少非目标表情带来的干扰，同时使用双序列模型分离短期面部运动与长期情绪变化。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的方法在DFEW和FERV39K等基准数据集上进行了广泛的实验，并证明了其优于现有的最先进的动态面部表情识别方法。&lt;h4&gt;结论&lt;/h4&gt;该工作对于促进动态面部表情识别领域的进一步发展具有重要意义，特别是在噪声一致的鲁棒学习方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：动态面部表情识别（DFER）的研究是一个新兴领域，涉及视频数据中自动识别面部表情。尽管现有研究主要集中在处理噪音和难以处理样本的学习表示上，但如何同时解决这两种类型的问题仍然没有得到很好的解决。为了克服这一挑战，本文提出了一种区分硬样本和噪声样本的稳健方法。通过评估模型在不同采样片段上的预测一致性来实现这一点，并随后采用强化学习难例和削弱噪声影响的方法。此外，为识别视频中的主要表情并增强模型表示学习的能力，提出了关键表情重采样的框架以及双流分层网络，即鲁棒动态面部表情识别（RDFER）。该方法通过在DFEW和FERV39K等基准数据集上的广泛实验展示出优于现有最先进的DFER方法的表现。综合分析提供了关于所提一致性的有价值见解与观察结果。这项工作对动态面部表情识别领域具有重要意义，并促进了噪声一致性鲁棒学习领域的进一步发展。代码可以从[https://github.com/Cross-Innovation-Lab/RDFER]获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The study of Dynamic Facial Expression Recognition (DFER) is a nascent fieldof research that involves the automated recognition of facial expressions invideo data. Although existing research has primarily focused on learningrepresentations under noisy and hard samples, the issue of the coexistence ofboth types of samples remains unresolved. In order to overcome this challenge,this paper proposes a robust method of distinguishing between hard and noisysamples. This is achieved by evaluating the prediction agreement of the modelon different sampled clips of the video. Subsequently, methodologies thatreinforce the learning of hard samples and mitigate the impact of noisy samplescan be employed. Moreover, to identify the principal expression in a video andenhance the model's capacity for representation learning, comprising a keyexpression re-sampling framework and a dual-stream hierarchical network isproposed, namely Robust Dynamic Facial Expression Recognition (RDFER). The keyexpression re-sampling framework is designed to identify the key expression,thereby mitigating the potential confusion caused by non-target expressions.RDFER employs two sequence models with the objective of disentanglingshort-term facial movements and long-term emotional changes. The proposedmethod has been shown to outperform current State-Of-The-Art approaches in DFERthrough extensive experimentation on benchmark datasets such as DFEW andFERV39K. A comprehensive analysis provides valuable insights and observationsregarding the proposed agreement. This work has significant implications forthe field of dynamic facial expression recognition and promotes the furtherdevelopment of the field of noise-consistent robust learning in dynamic facialexpression recognition. The code is available from[https://github.com/Cross-Innovation-Lab/RDFER].</description>
      <author>example@mail.com (Feng Liu, Hanyang Wang, Siyuan Shen)</author>
      <guid isPermaLink="false">2502.16129v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Asteroid shape inversion with light curves using deep learning</title>
      <link>http://arxiv.org/abs/2502.16455v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;小行星形状反演利用光度数据一直是行星科学和天文研究的关键领域，但现有方法需要大量的迭代计算，使过程耗时且容易陷入局部最优解。我们直接通过深度神经网络建立了光度数据与形状分布之间的映射关系，并使用3D点云表示小行星的形状。&lt;h4&gt;背景&lt;/h4&gt;利用光度数据进行小行星形状反演是天文研究中的一个重要课题。然而，现有的方法需要大量的迭代计算过程耗时且容易陷入局部最优解。&lt;h4&gt;目的&lt;/h4&gt;通过深度学习建立光度数据与小行星形状分布之间的直接映射关系，并开发一种预测非凸小行星凹陷区域的新方法。&lt;h4&gt;方法&lt;/h4&gt;采用3D点云表示小行星的形状，利用非凸小行星光线曲线与其凸包之间的偏差来预测非凸小行星上的凹陷区域。使用Chamfer距离评估传统方法与新方法的结果，并利用Lowell天文台观测数据验证该方法的有效性。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用深度学习技术，我们能够更有效地反演小行星的形状，特别是在处理特殊形状时表现更好。对于凸包上凹陷区域的检测，预测结果的IoU达到0.89，表明了方法的高度准确性。&lt;h4&gt;结论&lt;/h4&gt;实验结果显示该方法具有很强的鲁棒性和适应性，并且优于传统的光度曲线拟合方法。&lt;h4&gt;翻译&lt;/h4&gt;使用基于深度学习的方法进行小行星形状反演的研究。这种方法通过直接映射关系减少了计算时间和局部最优的问题，提高了处理非凸小行星时的效果和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Asteroid shape inversion using photometric data has been a key area of studyin planetary science and astronomical research.However, the current methods forasteroid shape inversion require extensive iterative calculations, making theprocess time-consuming and prone to becoming stuck in local optima. We directlyestablished a mapping between photometric data and shape distribution throughdeep neural networks. In addition, we used 3D point clouds to representasteroid shapes and utilized the deviation between the light curves ofnon-convex asteroids and their convex hulls to predict the concave areas ofnon-convex asteroids. We compared the results of different shape models usingthe Chamfer distance between traditional methods and ours and found that ourmethod performs better, especially when handling special shapes. For thedetection of concave areas on the convex hull, the intersection over union(IoU) of our predictions reached 0.89. We further validated this method usingobservational data from the Lowell Observatory to predict the convex shapes ofthe asteroids 3337 Milo and 1289 Kuta, and conducted light curve fittingexperiments. The experimental results demonstrated the robustness andadaptability of the method</description>
      <author>example@mail.com (YiJun Tang, ChenChen Ying, ChengZhe Xia, XiaoMing Zhang, XiaoJun Jiang)</author>
      <guid isPermaLink="false">2502.16455v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Interpreting core forms of urban morphology linked to urban functions with explainable graph neural network</title>
      <link>http://arxiv.org/abs/2502.16210v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;研究提出了核心城市形态表示的概念，发展了一种可解释的深度学习框架，用于将复杂的都市形式解析为一种新的表示（CoMo），从而揭示了都市功能与形态之间的联系。&lt;h4&gt;背景&lt;/h4&gt;理解城市的高阶关系对于可持续城市发展至关重要。然而，准确地描述复杂的城市形式并使其易于人类理解是一项挑战。&lt;h4&gt;目的&lt;/h4&gt;提出核心城市形态表示的概念，并开发可解释的深度学习框架将复杂的都市形式解析为新的表示（CoMo）。&lt;h4&gt;方法&lt;/h4&gt;利用一个稳定的加权F1分数为89.14%的经过训练的深度学习模型进行解释，以揭示基于核心城市形态表示的城市功能与形态之间的联系。&lt;h4&gt;主要发现&lt;/h4&gt;['波士顿的研究显示，在个人建筑、街区和社区层面上的核心城市形式对相应城市功能非常重要。', '住宅核心形式沿都市脊线呈现出渐进的形态模式，并且这种模式与从市中心到郊区的过渡一致。', '城市形态直接影响土地使用效率，二者具有显著的相关性（R2=0.721, p&lt;0.001）']&lt;h4&gt;结论&lt;/h4&gt;CoMo能够可解释地表示都市形式，提供经典城市位置理论的支持，并为数字孪生体提供了机制见解。&lt;h4&gt;翻译&lt;/h4&gt;理解城市形态与功能之间的高阶关系对于建模可持续城市的内在机制至关重要。然而，准确描述复杂的城市形式并使其易于人类理解是一项挑战。本研究提出核心城市形态表示的概念，并开发了一种可解释的深度学习框架，用于将复杂的都市形式解析为新的表示（CoMo）。通过解释经过良好训练的深度学习模型（稳定的加权F1分数89.14%），CoMo为揭示基于核心城市形态表示的城市功能与形态之间的联系提供了一个有希望的方法。以波士顿作为研究区域，分析了个人建筑、街区和社区层面的核心城市形式对相应城市功能的重要性。住宅核心形式沿都市脊线呈现出渐进的形态模式，并且这种模式与从市中心到郊区的过渡一致。此外，研究表明城市形态直接影响土地使用效率，并具有显著的相关性（R2=0.721, p&lt;0.001）。总的来说，CoMo能够可解释地表示都市形式，提供经典城市位置理论的支持，并为数字孪生体提供了机制见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the high-order relationship between urban form and function isessential for modeling the underlying mechanisms of sustainable urban systems.Nevertheless, it is challenging to establish an accurate data representationfor complex urban forms that are readily explicable in human terms. This studyproposed the concept of core urban morphology representation and developed anexplainable deep learning framework for explicably symbolizing complex urbanforms into the novel representation, which we call CoMo. By interpretating thewell-trained deep learning model with a stable weighted F1-score of 89.14%,CoMo presents a promising approach for revealing links between urban functionand urban form in terms of core urban morphology representation. Using Bostonas a study area, we analyzed the core urban forms at the individual-building,block, and neighborhood level that are important to corresponding urbanfunctions. The residential core forms follow a gradual morphological patternalong the urban spine, which is consistent with a center-urban-suburbantransition. Furthermore, we prove that urban morphology directly affects landuse efficiency, which has a significantly strong correlation with the location(R2=0.721, p&lt;0.001). Overall, CoMo can explicably symbolize urban forms,provide evidence for the classic urban location theory, and offer mechanisticinsights for digital twins.</description>
      <author>example@mail.com (Dongsheng Chen, Yu Feng, Xun Li, Mingya Qu, Peng Luo, Liqiu Meng)</author>
      <guid isPermaLink="false">2502.16210v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Integrating Weather Station Data and Radar for Precipitation Nowcasting: SmaAt-fUsion and SmaAt-Krige-GNet</title>
      <link>http://arxiv.org/abs/2502.16116v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  11 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究介绍了两种新的深度学习架构，旨在通过整合多变量气象站数据和雷达数据来提高降水短时预报的性能。&lt;h4&gt;背景&lt;/h4&gt;近年来，基于数据驱动和深度学习的方法在降水短时预报中引起了广泛关注，并取得了显著成果。然而，许多现有的模型未能充分利用广泛可用的大气信息，主要依赖于降水量数据。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过引入新的深度学习架构来改善降水短时预报的性能。&lt;h4&gt;方法&lt;/h4&gt;提出了两种新型深度学习架构：SmaAt-fUsion 和 SmaAt-Krige-GNet。SmaAt-fUsion 扩展了 SmaAt-UNet 架构，通过卷积层将气象站数据整合到网络瓶颈部分。而 SmaAt-Krige-GNet 结合了降水图与使用 Kriging 方法处理的气象站数据，生成特定变量的地图，并在基于 SmaAt-GNet 的双编码器架构中利用这些地图进行多级数据集成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在低降水量场景下，SmaAt-Krige-GNet 比仅依赖降水雷达数据的标准 SmaAt-UNet 表现更好；而在低和高降水量场景下，SmaAt-fUsion 超过了 SmaAt-UNet。&lt;h4&gt;结论&lt;/h4&gt;本研究强调了将离散的气象站数据整合到深度学习天气短时预报模型中以提高性能的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的内容已经涵盖了上述各个要点。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, data-driven, deep learning-based approaches forprecipitation nowcasting have attracted significant attention, showingpromising results. However, many existing models fail to fully exploit theextensive atmospheric information available, relying primarily on precipitationdata alone. This study introduces two novel deep learning architectures,SmaAt-fUsion and SmaAt-Krige-GNet, specifically designed to enhanceprecipitation nowcasting by integrating multi-variable weather station datawith radar datasets. By leveraging additional meteorological information, thesemodels improve representation learning in the latent space, resulting inenhanced nowcasting performance. The SmaAt-fUsion model extends the SmaAt-UNetframework by incorporating weather station data through a convolutional layer,integrating it into the bottleneck of the network. Conversely, theSmaAt-Krige-GNet model combines precipitation maps with weather station dataprocessed using Kriging, a geo-statistical interpolation method, to generatevariable-specific maps. These maps are then utilized in a dual-encoderarchitecture based on SmaAt-GNet, allowing multi-level data integration.Experimental evaluations were conducted using four years (2016--2019) ofweather station and precipitation radar data from the Netherlands. Resultsdemonstrate that SmaAt-Krige-GNet outperforms the standard SmaAt-UNet, whichrelies solely on precipitation radar data, in low precipitation scenarios,while SmaAt-fUsion surpasses SmaAt-UNet in both low and high precipitationscenarios. This highlights the potential of incorporating discrete weatherstation data to enhance the performance of deep learning-based weathernowcasting models.</description>
      <author>example@mail.com (Aleksej Cornelissen, Jie Shi, Siamak Mehrkanoon)</author>
      <guid isPermaLink="false">2502.16116v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Ultra fast, event-by-event heavy-ion simulations for next generation experiments</title>
      <link>http://arxiv.org/abs/2502.16330v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages, 7 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种新型深度生成框架，利用概率扩散模型进行重离子碰撞事件级别的快速模拟。&lt;h4&gt;背景&lt;/h4&gt;当前的物理实验中，对重离子碰撞数据的模拟是一项耗时的任务。传统方法难以满足大规模和高精度需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够高效、精准地模拟重离子碰撞输出的新框架。&lt;h4&gt;方法&lt;/h4&gt;该框架基于概率扩散模型，结合归一化流条件生成器和粒子点云合成模块，从UrQMD级联数据中学习并生成包含26种不同介子物种的完整碰撞事件输出。&lt;h4&gt;主要发现&lt;/h4&gt;提出的条件点云扩散模型能够产生逼真的重离子碰撞结果，成功再现了UrQMD分布中的多重性、动量和快度特性。&lt;h4&gt;结论&lt;/h4&gt;该框架不仅在质量和速度上优于现有方法，还为逆向问题求解和参数估计提供了便利，并且可以轻松适应加速任何事件级别的模型计算或探测器模拟任务。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种新颖的深度生成框架，利用概率扩散模型进行超快速、逐事件重离子碰撞输出模拟。此新框架基于UrQMD级联数据训练，以生成包含26个不同介子物种的完整碰撞事件输出。每个点由粒子动量向量及其对应种类信息（ID）定义。架构中整合了基于归一化流的条件生成器，将全局事件特征编码为潜在矢量，并利用扩散模型根据此条件合成粒子点云。详细描述了模型及深入分析其性能。有条件点云扩散模型学习产生真实碰撞事件输出的颗粒物，成功再现了UrQMD分布中的多重性、动量和快度特性。灵活的点云表示方法保留了完整的事件级粒度，直接应用于逆向问题求解和参数估计任务，并且易于适应加速任何逐事件模型计算或探测器模拟。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a novel deep generative framework that uses probabilisticdiffusion models for ultra fast, event-by-event simulations of heavy-ioncollision output. This new framework is trained on UrQMD cascade data togenerate a full collision event output containing 26 distinct hadron species.The output is represented as a point cloud, where each point is defined by aparticle's momentum vector and its corresponding species information (ID). Ourarchitecture integrates a normalizing flow-based condition generator thatencodes global event features into a latent vector, and a diffusion model thatsynthesizes a point cloud of particles based on this condition. A detaileddescription of the model and an in-depth analysis of its performance isprovided. The conditional point cloud diffusion model learns to generaterealistic output particles of collision events which successfully reproduce theUrQMD distributions for multiplicity, momentum and rapidity of each hadrontype. The flexible point cloud representation of the event output preservesfull event-level granularity, enabling direct application to inverse problemsand parameter estimation tasks while also making it easily adaptable foraccelerating any event-by-event model calculation or detector simulation.</description>
      <author>example@mail.com (Manjunath Omana Kuttan, Kai Zhou, Jan Steinheimer, Horst Stoecker)</author>
      <guid isPermaLink="false">2502.16330v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>BalanceBenchmark: A Survey for Multimodal Imbalance Learning</title>
      <link>http://arxiv.org/abs/2502.10816v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一个全面评估多模态不平衡算法的基准测试平台BalanceBenchmark，包括多个常用的数据集和评价指标，并开发了一个标准化实验流程的工具包。&lt;h4&gt;背景&lt;/h4&gt;多模态学习通过整合不同模态的信息得到了广泛关注。然而，该领域常受到模态失衡问题的影响，即某些模态过于主导而其他模态被利用不足。&lt;h4&gt;目的&lt;/h4&gt;系统分类主流多模态不平衡算法，并提供一个全面的评估方法以促进研究的发展。&lt;h4&gt;方法&lt;/h4&gt;引入BalanceBenchmark基准测试平台和标准化实验流程工具包，通过多个数据集从性能、失衡程度及复杂性三个角度进行综合评价。&lt;h4&gt;主要发现&lt;/h4&gt;基于实验结果，识别出不同方法在性能、平衡度以及计算复杂性方面的特征与优势。&lt;h4&gt;结论&lt;/h4&gt;此分析有望激发未来研究中更有效的不平衡问题解决方案，并可能影响基础模型的发展。&lt;h4&gt;翻译&lt;/h4&gt;多模态学习因其整合多种信息模态的能力而备受关注。然而，它常因模态失衡问题受到限制，即某些模态主导其他未充分利用的模态。尽管最近的研究提出了各种方法来缓解该问题，但缺乏全面且公平的比较。本文将主流的多模态不平衡算法基于其减轻不平衡的方法分为四大类，并引入BalanceBenchmark基准测试平台以促进综合评估。为确保公平比较，开发了标准化实验流程工具包。通过使用BalanceBenchmark进行实验，识别出不同方法组在性能、平衡度和计算复杂性方面的特征与优势。我们期待这种分析能够启发未来更高效地解决不平衡问题的方法，并可能影响基础模型的发展。工具代码可在https://github.com/GeWu-Lab/BalanceBenchmark获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/gewu-lab/balancebenchmark&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multimodal learning has gained attention for its capacity to integrateinformation from different modalities. However, it is often hindered by themultimodal imbalance problem, where certain modality dominates while othersremain underutilized. Although recent studies have proposed various methods toalleviate this problem, they lack comprehensive and fair comparisons. In thispaper, we systematically categorize various mainstream multimodal imbalancealgorithms into four groups based on the strategies they employ to mitigateimbalance. To facilitate a comprehensive evaluation of these methods, weintroduce BalanceBenchmark, a benchmark including multiple widely usedmultidimensional datasets and evaluation metrics from three perspectives:performance, imbalance degree, and complexity. To ensure fair comparisons, wehave developed a modular and extensible toolkit that standardizes theexperimental workflow across different methods. Based on the experiments usingBalanceBenchmark, we have identified several key insights into thecharacteristics and advantages of different method groups in terms ofperformance, balance degree and computational complexity. We expect suchanalysis could inspire more efficient approaches to address the imbalanceproblem in the future, as well as foundation models. The code of the toolkit isavailable at https://github.com/GeWu-Lab/BalanceBenchmark.</description>
      <author>example@mail.com (Shaoxuan Xu, Menglu Cui, Chengxiang Huang, Hongfa Wang, Di Hu)</author>
      <guid isPermaLink="false">2502.10816v3</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Co-evolution-based Metal-binding Residue Prediction with Graph Neural Networks</title>
      <link>http://arxiv.org/abs/2502.16189v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  7 pages, 3 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一种名为MBGNN的金属结合预测模型，该模型利用共进化残基网络并使用图神经网络来捕捉蛋白质结构中的复杂依赖关系。&lt;h4&gt;背景&lt;/h4&gt;预测金属结合位点及其对应的金属类型在计算结构生物学中具有挑战性，因为涉及到蛋白质结构和相互作用的复杂性。传统的方法无法有效捕获驱动这些相互作用的复杂进化关系，而基于共进化的最近方法没有充分考虑整个共进化残基网络。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的预测模型MBGNN，以改进金属结合位点及其相关金属类型的预测性能。&lt;h4&gt;方法&lt;/h4&gt;MBGNN利用完整的共进化残基网络并通过图神经网络有效捕捉蛋白质结构中的复杂依赖关系。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，MBGNN在公共数据集上的表现优于现有的基于共进化的金属结合预测方法，并且在序列基础上的方法中具有竞争力。&lt;h4&gt;结论&lt;/h4&gt;该模型展示了将共进化见解与高级机器学习技术相结合的潜力，有助于深入理解蛋白质-金属相互作用。MBGNN的代码可在GitHub上公开获得。&lt;h4&gt;翻译&lt;/h4&gt;在计算结构生物学领域，由于蛋白质结构和相互作用的复杂性，预测金属结合位点及其对应的金属类型极具挑战性。传统基于序列和结构的方法无法有效捕捉这些交互背后的复杂进化关系以促进理解，而最近基于共进化的技术未能充分考虑整个共进化残基网络的结构。本文提出了一种名为MBGNN（Metal-Binding Graph Neural Network）的新方法，该模型利用了完整的共进化残基网络并通过图神经网络有效地捕获蛋白质结构中的复杂依赖关系，以提高共进化金属结合位点及其相关金属类型的预测能力。公共数据集上的实验结果表明，MBGNN在基于共进化的金属结合预测方法中表现出色，并且也与最近的序列基础方法相媲美，展示了将共进化见解与高级机器学习相结合的潜力，有助于深入了解蛋白质-金属相互作用。MBGNN代码可以在https://github.com/SRastegari/MBGNN上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In computational structural biology, predicting metal-binding sites and theircorresponding metal types is challenging due to the complexity of proteinstructures and interactions. Conventional sequence- and structure-basedprediction approaches cannot capture the complex evolutionary relationshipsdriving these interactions to facilitate understanding, while recentco-evolution-based approaches do not fully consider the entire structure of theco-evolved residue network. In this paper, we introduce MBGNN (Metal-BindingGraph Neural Network) that utilizes the entire co-evolved residue network andeffectively captures the complex dependencies within protein structures viagraph neural networks to enhance the prediction of co-evolved metal-bindingresidues and their associated metal types. Experimental results on a publicdataset show that MBGNN outperforms existing co-evolution-based metal-bindingprediction methods, and it is also competitive against recent sequence-basedmethods, showing the potential of integrating co-evolutionary insights withadvanced machine learning to deepen our understanding of protein-metalinteractions. The MBGNN code is publicly available athttps://github.com/SRastegari/MBGNN.</description>
      <author>example@mail.com (Sayedmohammadreza Rastegari, Sina Tabakhi, Xianyuan Liu, Wei Sang, Haiping Lu)</author>
      <guid isPermaLink="false">2502.16189v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Supermarket-6DoF: A Real-World Grasping Dataset and Grasp Pose Representation Analysis</title>
      <link>http://arxiv.org/abs/2502.16311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'研究名称': 'Supermarket-6DoF', '数据规模': '包含1500次抓取尝试，涉及20种超市物品', '特点': '提供真实机器人执行的地面实况抓取结果，并且包含完全的6自由度抓取姿势注释，包括初始抓取成功和被抓取后受到外部扰动的稳定性', '用途': '用于分析三种抓取姿态表示方法对点云中抓取成功的预测准确性'}&lt;h4&gt;背景&lt;/h4&gt;现有的大多数抓取数据集依赖于解析指标或模拟进行抓取标注。相比之下，Supermarket-6DoF提供了物理机器人执行的真实地面实况结果&lt;h4&gt;目的&lt;/h4&gt;展示一个真实的超市物品抓取数据集，并验证其在基于点云的抓取姿态表示中的价值和准确性&lt;h4&gt;方法&lt;/h4&gt;通过分析三种不同的抓取姿态表示来预测抓取成功的准确度，比较了显式表达夹爪几何形状的点云表示与传统的四元数编码的表现&lt;h4&gt;主要发现&lt;/h4&gt;结果显示，将夹爪几何结构作为点云明确表示的方法比传统基于四元数的姿态表示方法在抓取成功率预测中更加精确&lt;h4&gt;结论&lt;/h4&gt;Supermarket-6DoF数据集为研究真实环境中的机器人手部操作提供了一个宝贵的资源，并且证明了使用点云来表达夹爪姿态的有效性&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present Supermarket-6DoF, a real-world dataset of 1500 grasp attemptsacross 20 supermarket objects with publicly available 3D models. Unlike mostexisting grasping datasets that rely on analytical metrics or simulation forgrasp labeling, our dataset provides ground-truth outcomes from physical robotexecutions. Among the few real-world grasping datasets, wile more modest insize, Supermarket-6DoF uniquely features full 6-DoF grasp poses annotated withboth initial grasp success and post-grasp stability under externalperturbation. We demonstrate the dataset's utility by analyzing three grasppose representations for grasp success prediction from point clouds. Ourresults show that representing the gripper geometry explicitly as a point cloudachieves higher prediction accuracy compared to conventional quaternion-basedgrasp pose encoding.</description>
      <author>example@mail.com (Jason Toskov, Akansel Cosgun)</author>
      <guid isPermaLink="false">2502.16311v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Advanced Text Analytics -- Graph Neural Network for Fake News Detection in Social Media</title>
      <link>http://arxiv.org/abs/2502.16157v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;传统的图神经网络（GNN）在假新闻检测中通常依赖于辅助的非文本数据，如用户互动历史或内容传播模式。然而这些数据源并不总是可以获取到，限制了方法的有效性和适用性。&lt;h4&gt;目的&lt;/h4&gt;为了解决上述挑战，本文提出了一种先进的文本分析图神经网络（ATA-GNN），该模型仅基于文本数据进行操作。&lt;h4&gt;方法&lt;/h4&gt;ATA-GNN采用了创新的主题建模技术来识别每个主题的典型词汇，并通过多维聚类实现对文本内容的全面语义理解。这种多层次的设计使模型能够发现复杂的文本模式，同时将这些模式置于更广泛的语境中以增强其解释能力。&lt;h4&gt;主要发现&lt;/h4&gt;在广泛使用的基准数据集上的大量评估表明，ATA-GNN的表现优于现有的基于GNN的方法，在假新闻检测方面更加可靠和专注于文本信息。&lt;h4&gt;结论&lt;/h4&gt;这项研究证明了在图神经网络架构中整合先进的文本聚类方法具有实现更可靠且以文本为中心的解决方案的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;传统的图神经网络（GNN）通常依赖于辅助非文本数据，如用户互动历史或内容传播模式来进行假新闻检测。然而这些数据源并不总是可得，这限制了现有方法的有效性和应用范围。此外，现有的模型常常难以捕捉到文本信息中的详细且复杂的关系，从而降低其整体准确性。为了应对这一挑战，本文提出了一种先进的文本分析图神经网络（ATA-GNN），该模型仅基于文本数据进行操作，并采用了创新的主题建模技术来识别每个主题的典型词汇。通过多维聚类和多层次设计，实现了对文本内容的全面语义理解及复杂模式的发现，在广泛的基准数据集上，该方法的表现超过了现有的GNN方法，表明在图神经网络架构中结合先进的文本聚类方法可以实现更可靠且以文本为中心的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional Graph Neural Network (GNN) approaches for fake news detection(FND) often depend on auxiliary, non-textual data such as user interactionhistories or content dissemination patterns. However, these data sources arenot always accessible, limiting the effectiveness and applicability of suchmethods. Additionally, existing models frequently struggle to capture thedetailed and intricate relationships within textual information, reducing theiroverall accuracy. In order to address these challenges Advanced Text AnalysisGraph Neural Network (ATA-GNN) is proposed in this paper. The proposed model isdesigned to operate solely on textual data. ATA-GNN employs innovative topicmodelling (clustering) techniques to identify typical words for each topic,leveraging multiple clustering dimensions to achieve a comprehensive semanticunderstanding of the text. This multi-layered design enables the model touncover intricate textual patterns while contextualizing them within a broadersemantic framework, significantly enhancing its interpretative capabilities.Extensive evaluations on widely used benchmark datasets demonstrate thatATA-GNN surpasses the performance of current GNN-based FND methods. Thesefindings validate the potential of integrating advanced text clustering withinGNN architectures to achieve more reliable and text-focused detectionsolutions.</description>
      <author>example@mail.com (Anantram Patel, Vijay Kumar Sutrakar)</author>
      <guid isPermaLink="false">2502.16157v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>voc2vec: A Foundation Model for Non-Verbal Vocalization</title>
      <link>http://arxiv.org/abs/2502.16298v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted at ICASSP 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要概括&lt;/h4&gt;提出了一种新的非言语人类数据基础模型voc2vec，旨在克服现有语音和音频基础模型在处理非语言声音时的不足。&lt;h4&gt;背景信息&lt;/h4&gt;现有的语音基础模型虽然在相关的任务中表现出色，但在处理如婴儿哭泣等非语言音频数据方面存在困难。同样地，传统的音频基础模型虽然能够很好地处理非言语音频，但无法捕捉到人类声音中的细微特征。&lt;h4&gt;研究目的&lt;/h4&gt;旨在克服现有模型的缺点，并提出了一种新的基础模型voc2vec，专门用于处理非言语人类数据。&lt;h4&gt;所用方法&lt;/h4&gt;采用了包含10个数据集、总计约125小时非言语音频的数据集合。这些数据集完全由开源资源构成。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，voc2vec在非语言声音分类任务中表现优异，并且超越了传统的语音和音频基础模型以及强大的基准线OpenSmile和emotion2vec。&lt;h4&gt;结论&lt;/h4&gt;据作者所知，voc2vec是首个为发声任务设计的通用表示模型。&lt;h4&gt;翻译&lt;/h4&gt;语音基础模型已经在相关的任务中显示出了非凡的能力。然而，在处理诸如婴儿哭泣等非语言音频数据时，这些模型常常面临困难。这类非语言音频对于各种现实世界的应用至关重要。传统音频基础模型能够很好地处理非言语数据，但无法捕捉到人类声音中的细微特征。本研究旨在克服上述不足，并提出了一种新的基础模型voc2vec，专门设计用于处理非言语人类数据，仅使用开源的非言语音频数据集。实验结果证明，voc2vec在非语言发声分类任务中表现出色，超越了传统的语音和音频基础模型。此外，voc2vec在六个不同的基准测试数据集中也始终优于强大的基准线OpenSmile和emotion2vec。据作者所知，voc2vec是首个为发声任务设计的通用表示模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Speech foundation models have demonstrated exceptional capabilities inspeech-related tasks. Nevertheless, these models often struggle with non-verbalaudio data, such as vocalizations, baby crying, etc., which are critical forvarious real-world applications. Audio foundation models well handle non-speechdata but also fail to capture the nuanced features of non-verbal human sounds.In this work, we aim to overcome the above shortcoming and propose a novelfoundation model, termed voc2vec, specifically designed for non-verbal humandata leveraging exclusively open-source non-verbal audio datasets. We employ acollection of 10 datasets covering around 125 hours of non-verbal audio.Experimental results prove that voc2vec is effective in non-verbal vocalizationclassification, and it outperforms conventional speech and audio foundationmodels. Moreover, voc2vec consistently outperforms strong baselines, namelyOpenSmile and emotion2vec, on six different benchmark datasets. To the best ofthe authors' knowledge, voc2vec is the first universal representation model forvocalization tasks.</description>
      <author>example@mail.com (Alkis Koudounas, Moreno La Quatra, Marco Sabato Siniscalchi, Elena Baralis)</author>
      <guid isPermaLink="false">2502.16298v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Discovery and Deployment of Emergent Robot Swarm Behaviors via Representation Learning and Real2Sim2Real Transfer</title>
      <link>http://arxiv.org/abs/2502.15937v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures. To be included in Proc. of the 24th  International Conference on Autonomous Agents and Multiagent Systems (AAMAS  2025)&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于自监督表示学习的Real2Sim2Real行为发现方法，该方法可以在仿真环境中自动发现可能的行为模式，并将这些行为直接部署到真实机器人集群中。&lt;h4&gt;背景&lt;/h4&gt;之前的方法依赖于人工反馈或手工设计的行为度量来表征和进化行为，仅限于在模拟环境中发现行为，未考虑将这些新行为部署到实际的机器人集群上。&lt;h4&gt;目的&lt;/h4&gt;开发一种可以自动探索有限能力机器人群体潜在可出现行为集合的方法，并能够直接应用这些行为到真实的机器人集群中。&lt;h4&gt;方法&lt;/h4&gt;结合表示学习和新颖性搜索，在模拟环境中发现可能的行为，同时通过引入群集sim2real迁移的最新工作缩小现实差距，使得所有在仿真中发现的行为可以直接部署到真实机器人上。&lt;h4&gt;主要发现&lt;/h4&gt;提出的自监督表示学习方法优于手工设计的度量标准，能够更准确地表征潜在行为空间，并且可以通过轻量化模拟器实现从模拟直接转移到实际应用。&lt;h4&gt;结论&lt;/h4&gt;通过展示方法的有效性及其实现的可行性，表明这种方法为自动探索机器人集群中可能出现的行为提供了新途径，并可以无缝地部署到真实环境中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Given a swarm of limited-capability robots, we seek to automatically discoverthe set of possible emergent behaviors. Prior approaches to behavior discoveryrely on human feedback or hand-crafted behavior metrics to represent and evolvebehaviors and only discover behaviors in simulation, without testing orconsidering the deployment of these new behaviors on real robot swarms. In thiswork, we present Real2Sim2Real Behavior Discovery via Self-SupervisedRepresentation Learning, which combines representation learning and noveltysearch to discover possible emergent behaviors automatically in simulation andenable direct controller transfer to real robots. First, we evaluate our methodin simulation and show that our proposed self-supervised representationlearning approach outperforms previous hand-crafted metrics by more accuratelyrepresenting the space of possible emergent behaviors. Then, we address thereality gap by incorporating recent work in sim2real transfer for swarms intoour lightweight simulator design, enabling direct robot deployment of allbehaviors discovered in simulation on an open-source and low-cost robotplatform.</description>
      <author>example@mail.com (Connor Mattson, Varun Raveendra, Ricardo Vega, Cameron Nowzari, Daniel S. Drew, Daniel S. Brown)</author>
      <guid isPermaLink="false">2502.15937v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis</title>
      <link>http://arxiv.org/abs/2502.15635v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by International Conference on 3D Vision (3DV) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;为了评估端到端的自主驾驶系统，需要一个基于新颖视图合成技术（NVS）的模拟环境来生成逼真的图像和点云。本文介绍了一个新的多车道数据集和基准测试，用于评价现有的NeRF和3DGS方法在真实世界场景中的表现。&lt;h4&gt;背景&lt;/h4&gt;当前的基于场景的NVS数据集中存在缺少与实际捕捉到的真实性和细节的问题，特别是针对跨车道的评估场景仍然不足。&lt;h4&gt;目的&lt;/h4&gt;为了进一步评估现有基于NeRF和3DGS的方法，在逼真的多传感器环境中建立一个可用于自主驾驶系统性能测试的数据集。&lt;h4&gt;方法&lt;/h4&gt;开发了一个包含25组关联序列的多车道数据集，这些序列由实际世界扫描生成，包括16,000张前视图图像、64,000张环视图像和16,000帧LiDAR点云。所有帧都进行了标注以区分移动物体与静止元素。&lt;h4&gt;主要发现&lt;/h4&gt;通过该数据集可以对现有方法在不同车道和距离下的测试场景中进行性能评估，并解决了多传感器姿态求解及质量评价的问题，从而实现了跨模态数据的对齐。&lt;h4&gt;结论&lt;/h4&gt;计划持续添加新的序列以测试现有方法在各种情况下的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;为了评估端到端自主驾驶系统的性能，基于新颖视图合成技术（NVS）创建一个模拟环境是必要的，它可以从先前记录的序列中生成逼真的图像和点云，特别是在跨车道场景下。因此，开发一个多车道数据集和基准测试是必不可少的。尽管最近的一些基于合成场景的NVS数据集已经为跨车道基准测试做好了准备，但它们仍然缺乏真实捕捉到的图像和点云的真实感。为了进一步评估现有的基于NeRF和3DGS方法的性能，我们提出了第一个注册平行扫描的多车道数据集，特别针对新型驾驶视图合成的数据集，该数据集由实际世界扫描衍生而来，包含25组相关序列，包括16,000张前视图像、64,000张环绕视图图像和16,000帧LiDAR帧。所有帧都进行了标注以区分移动对象与静态元素。利用这个数据集，在不同的车道和距离下对现有方法在各种测试场景中的性能进行评估。此外，我们的方法提供了求解并评估多模态数据对齐的多传感器姿态质量的问题解决方案，以便策划这样的数据集。我们计划继续添加新的序列以测试现有方法在不同情况下的泛化能力。该数据集已公开发布于项目页面：https://nizqleo.github.io/paralane-dataset/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To evaluate end-to-end autonomous driving systems, a simulation environmentbased on Novel View Synthesis (NVS) techniques is essential, which synthesizesphoto-realistic images and point clouds from previously recorded sequencesunder new vehicle poses, particularly in cross-lane scenarios. Therefore, thedevelopment of a multi-lane dataset and benchmark is necessary. While recentsynthetic scene-based NVS datasets have been prepared for cross-lanebenchmarking, they still lack the realism of captured images and point clouds.To further assess the performance of existing methods based on NeRF and 3DGS,we present the first multi-lane dataset registering parallel scans specificallyfor novel driving view synthesis dataset derived from real-world scans,comprising 25 groups of associated sequences, including 16,000 front-viewimages, 64,000 surround-view images, and 16,000 LiDAR frames. All frames arelabeled to differentiate moving objects from static elements. Using thisdataset, we evaluate the performance of existing approaches in various testingscenarios at different lanes and distances. Additionally, our method providesthe solution for solving and assessing the quality of multi-sensor poses formulti-modal data alignment for curating such a dataset in real-world. We planto continually add new sequences to test the generalization of existing methodsacross different scenarios. The dataset is released publicly at the projectpage: https://nizqleo.github.io/paralane-dataset/.</description>
      <author>example@mail.com (Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang)</author>
      <guid isPermaLink="false">2502.15635v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Large Language Model for Lossless Image Compression with Visual Prompts</title>
      <link>http://arxiv.org/abs/2502.16163v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种利用大规模语言模型（LLM）进行无损图像压缩的新范式，通过将视觉提示与LLM结合来改进熵编码。&lt;h4&gt;背景&lt;/h4&gt;近年来深度学习在无损图像压缩方面取得了显著进展。随着大型语言模型的出现，初步尝试开始探索如何利用预训练模型中的丰富先验知识来增强无损图像压缩性能，特别是在改进熵模型方面的应用。&lt;h4&gt;目的&lt;/h4&gt;为了克服将LLM中嵌入的文字先验信息与无损图像压缩技术有效结合的问题，并挖掘出该方法的潜力，本文提出了一种新的方案。&lt;h4&gt;方法&lt;/h4&gt;首先生成输入图像的一个有损重建版本作为视觉提示，从中提取特征以作为LLM的视觉嵌入。然后将原始图像和这个有损重建版本之间的残差与这些视觉嵌入一起传递给LLM，使LLM能够充当熵模型来预测该残差的概率分布。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，在多个基准数据集上，本文的方法在无损压缩性能方面达到了当前的最佳水平，并超越了传统的和基于学习的无损图像编码方法。此外，所提出的技术还能方便地扩展到其他领域中的图像（如医学影像和屏幕内容），并显示出出色的效果。&lt;h4&gt;结论&lt;/h4&gt;结果表明LLM对于无损图像压缩具有巨大潜力，并有望激励相关领域的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;最近在深度学习上的进步极大地推动了无损图像压缩技术的发展。随着大规模语言模型的出现，初步尝试开始利用这些预训练模型中的丰富先验知识来改进熵编码，从而增强无损图像压缩性能。然而，在将文本型先前知识与图像无损压缩之间建立联系仍面临挑战。为了应对这一难题并发掘LLM的应用潜能，本文介绍了一种新颖的方法，即通过生成输入图像的有损重建版本作为视觉提示，并从这些提示中提取特征供大型语言模型使用，以此来改进熵编码的过程。研究发现该方法在多个基准数据集上表现出了卓越的压缩效果，超越了传统和基于学习的无损图像编码器。此外，这种方法还可以方便地应用于其他领域的图像（如医学影像和屏幕内容），并取得出色的表现。这些结果凸显了LLM对于无损图像压缩技术的巨大潜力，并有可能激发更多相关方向的研究工作。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in deep learning have driven significant progress inlossless image compression. With the emergence of Large Language Models (LLMs),preliminary attempts have been made to leverage the extensive prior knowledgeembedded in these pretrained models to enhance lossless image compression,particularly by improving the entropy model. However, a significant challengeremains in bridging the gap between the textual prior knowledge within LLMs andlossless image compression. To tackle this challenge and unlock the potentialof LLMs, this paper introduces a novel paradigm for lossless image compressionthat incorporates LLMs with visual prompts. Specifically, we first generate alossy reconstruction of the input image as visual prompts, from which weextract features to serve as visual embeddings for the LLM. The residualbetween the original image and the lossy reconstruction is then fed into theLLM along with these visual embeddings, enabling the LLM to function as anentropy model to predict the probability distribution of the residual.Extensive experiments on multiple benchmark datasets demonstrate our methodachieves state-of-the-art compression performance, surpassing both traditionaland learning-based lossless image codecs. Furthermore, our approach can beeasily extended to images from other domains, such as medical and screencontent images, achieving impressive performance. These results highlight thepotential of LLMs for lossless image compression and may inspire furtherresearch in related directions.</description>
      <author>example@mail.com (Junhao Du, Chuqin Zhou, Ning Cao, Gang Chen, Yunuo Chen, Zhengxue Cheng, Li Song, Guo Lu, Wenjun Zhang)</author>
      <guid isPermaLink="false">2502.16163v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MedForge: Building Medical Foundation Models Like Open Source Software Development</title>
      <link>http://arxiv.org/abs/2502.16055v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了Medical Foundation Models Merging (MedForge)框架，用于促进社区驱动的医疗基础模型开发。&lt;h4&gt;背景&lt;/h4&gt;基础模型（FMs）在医疗领域取得了显著进展。然而，在医疗系统中数据孤岛问题和隐私保护仍然是阻碍安全医学数据共享和跨机构合作的主要障碍。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战并收集和整理可扩展的临床数据集，用于训练强大的基础模型，提出了MedForge框架。&lt;h4&gt;方法&lt;/h4&gt;MedForge通过灵活地合并特定任务的低秩适应（LoRA）模块来提供一种自下而上的模型构建机制。该方法可以同时调整下游任务且保留原始模型参数，并利用异步LoRA模块集成方案逐步增强复合模型在各种临床任务中的综合性能。&lt;h4&gt;主要发现&lt;/h4&gt;MedForge框架展示了其在多个临床数据集（如乳腺癌、肺癌和结肠癌）上的强大性能，这些数据集来自不同机构。研究表明，协作基础模型可以有效并一致地推进多中心临床合作。&lt;h4&gt;结论&lt;/h4&gt;研究结果强调了跨医疗机构协同开发医疗基础模型的重要性，并公开发布了相关代码以便其他研究人员使用。&lt;h4&gt;翻译&lt;/h4&gt;在该研究中提出了一种名为Medical Foundation Models Merging (MedForge)的框架，该框架旨在通过灵活合并特定任务的低秩适应（LoRA）模块来促进社区驱动的医学基础模型的发展。此方法能够避免原始患者数据的信息泄露，并解决跨临床机构同步开发模型的问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-22&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundational models (FMs) have made significant strides in the healthcaredomain. Yet the data silo challenge and privacy concern remain in healthcaresystems, hindering safe medical data sharing and collaborative modeldevelopment among institutions. The collection and curation of scalableclinical datasets increasingly become the bottleneck for training strong FMs.In this study, we propose Medical Foundation Models Merging (MedForge), acooperative framework enabling a community-driven medical foundation modeldevelopment, meanwhile preventing the information leakage of raw patient dataand mitigating synchronization model development issues across clinicalinstitutions. MedForge offers a bottom-up model construction mechanism byflexibly merging task-specific Low-Rank Adaptation (LoRA) modules, which canadapt to downstream tasks while retaining original model parameters. Through anasynchronous LoRA module integration scheme, the resulting composite model canprogressively enhance its comprehensive performance on various clinical tasks.MedForge shows strong performance on multiple clinical datasets (e.g., breastcancer, lung cancer, and colon cancer) collected from different institutions.Our major findings highlight the value of collaborative foundation models inadvancing multi-center clinical collaboration effectively and cohesively. Ourcode is publicly available at https://github.com/TanZheling/MedForge.</description>
      <author>example@mail.com (Zheling Tan, Kexin Ding, Jin Gao, Mu Zhou, Dimitris Metaxas, Shaoting Zhang, Dequan Wang)</author>
      <guid isPermaLink="false">2502.16055v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>DiffCheck: a Scan-CAD Evaluation Tool for Digital Manufacturing and Assembly Processes in Timber Construction</title>
      <link>http://arxiv.org/abs/2502.15864v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Andrea Settimi, Damien Gilliard and Eleni Skevaki contributed equally  to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一款名为diffCheck的软件，该软件使用先进的点云分析技术来比较木质结构的扫描结果与其CAD模型之间的差异。&lt;h4&gt;背景&lt;/h4&gt;在数字木材建造中，由于3D传感器、摄影测量和用户友好的CAD工具易于获得，因此广泛采用扫描技术和点云数据。然而，这些工具通常不用于精度检查，因为标准机械可以提供更高的精度。&lt;h4&gt;目的&lt;/h4&gt;提出一种名为diffCheck的软件来填补这一空白，以便于实验研究和原型制作中评估精确度和准确性。&lt;h4&gt;方法&lt;/h4&gt;diffCheck是一个用C++/Python编写的软件，集成到了Grasshopper平台。它利用先进的点云分析技术进行精度检查，并且可以与各种木材元件和数字制造方法（如机器人装配、AR辅助木工以及数控机床）兼容。&lt;h4&gt;主要发现&lt;/h4&gt;通过测试不同的木材元素和数字制造方法，diffCheck旨在建立一个用户友好的基准框架，用于评估使用木材组件的数字制造系统。此外，该软件及其源代码可以以开放许可的方式与数字化制造社区分享。&lt;h4&gt;结论&lt;/h4&gt;diffCheck具有在其他材料中找到应用潜力的能力，并且其设计是为了促进更高效的精度和准确性检查。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在数字木材建造中，由于3D传感器、摄影测量和用户友好的CAD工具易于获得，因此广泛采用扫描技术和点云数据。虽然通常不用于准确性的校验，因为标准机械可以提供更高的精确度，但是实验研究和原型制作可以从精度和准确性评估工具中获益。我们介绍了一款名为diffCheck的软件，它使用先进的点云分析技术比较加工木材结构的扫描结果与相应的CAD模型之间的差异，并且能够帮助识别出不一致的地方。经过各种木材元件及如机器人装配、AR辅助木工以及数控机床等数字制造方法的测试后，diffCheck旨在为采用木材组件的数字制造系统建立一个用户友好的基准框架，同时具有在其他材料中找到应用潜力的能力。其源代码和分析数据以开放许可的方式与数字化制造社区共享。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In digital timber construction, scanning technologies and point cloud dataare widely used due to the accessibility of affordable 3D sensors,photogrammetry, and user-friendly CAD tools. While typically not employed foraccuracy checks in timber fabrication due to the precision of standardmachinery, experimental research and prototyping with joinery and assembly canbenefit from precision and accuracy evaluation tools.  We introduce diffCheck, a C++/Python software integrated into Grasshopper toaddress this need. It uses advanced point cloud analysis to compare scans offabricated timber structures with their respective CAD models, helping toidentify discrepancies. Tested on various timber elements and digitalfabrication methods like robotic assembly, AR-assisted woodworking, and CNCmachining, diffCheck aims to establish a user-friendly benchmark framework fordigital fabrication systems using timber components, with the potential to findapplications in other materials. Its source code and the analyzed data areopenly shared with the digital fabrication community under a permissivelicense.</description>
      <author>example@mail.com (Andrea Settimi, Damien Gilliard, Eleni Skevaki, Marirena Kladeftira, Julien Gamerro, Stefana Parascho, Yves Weinand)</author>
      <guid isPermaLink="false">2502.15864v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>AutoMedPrompt: A New Framework for Optimizing LLM Medical Prompts Using Textual Gradients</title>
      <link>http://arxiv.org/abs/2502.15944v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  14 pages&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为AutoMedPrompt的技术，利用文本梯度优化系统提示来提高通用基础模型在医学领域的问题解答能力。&lt;h4&gt;背景&lt;/h4&gt;大型语言模型（LLMs）已在医疗和知识领域展示了越来越复杂的性能。传统的专家化方法需要对大量数据集进行广泛的微调和训练。然而，最近的提示工程方法展示出不通过微调也能提升一般基础模型的能力，但这些方法在特定子领域的适用性有限。&lt;h4&gt;目的&lt;/h4&gt;探索使用文本梯度来优化系统提示，从而激发医学相关推理，并评估这种方法的有效性。&lt;h4&gt;方法&lt;/h4&gt;利用TextGrad的自动文本差异化技术改进通用基础LLM的表现。测试了开源大型语言模型Llama 3，在MedQA、PubMedQA和特定肾脏病亚专业的NephSAP等多个问答基准上进行了评估。&lt;h4&gt;主要发现&lt;/h4&gt;与先前的方法相比，使用文本梯度进行提示在开源LLMs上表现更优，并且超越了专有模型如GPT-4、Claude 3 Opus和Med-PaLM 2。AutoMedPrompt在PubMedQA上的准确率为82.6%，超过了此前所有方法的表现。&lt;h4&gt;结论&lt;/h4&gt;文本梯度引导的提示技术展示了其在医学领域问答任务中的显著优势，有望成为未来模型改进的一个方向。&lt;h4&gt;翻译&lt;/h4&gt;大型语言模型（LLMs）已经在医疗和其他知识领域展示出越来越高级别的性能。传统的创建专业LLM的方法需要对大量数据集进行广泛的微调和训练。然而，最近的提示工程方法展示了无需微调即可提升通用基础模型的能力的潜力。但是，例如链式思考（CoT）的提示方法可能不适用于所有子专科，并且k-shot方法可能会在上下文中引入无关词汇。我们提出了AutoMedPrompt，该技术探索了利用文本梯度通过优化系统提示来激发医学相关推理的可能性。AutoMedPrompt使用TextGrad的基于文本的自动微分技术提高了一般基础LLMs的能力。我们在开源LLM Llama 3上评估了AutoMedPrompt，在包括MedQA、PubMedQA和特定肾脏病亚专业的NephSAP等多个问答基准中进行了测试。我们的结果表明，使用文本梯度进行提示超越了以往方法在开源LLMs上的表现，并且超过了GPT-4、Claude 3 Opus以及Med-PaLM 2等专有模型的表现。AutoMedPrompt在PubMedQA上达到了新的最先进（SOTA）性能水平，准确率为82.6%，并且在开源模型中为MedQA（77.7%）和NephSAP（63.8%）的问答任务表现也优于先前的方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Large language models (LLMs) have demonstrated increasingly sophisticatedperformance in medical and other fields of knowledge. Traditional methods ofcreating specialist LLMs require extensive fine-tuning and training of modelson large datasets. Recently, prompt engineering, instead of fine-tuning, hasshown potential to boost the performance of general foundation models. However,prompting methods such as chain-of-thought (CoT) may not be suitable for allsubspecialty, and k-shot approaches may introduce irrelevant tokens into thecontext space. We present AutoMedPrompt, which explores the use of textualgradients to elicit medically relevant reasoning through system promptoptimization. AutoMedPrompt leverages TextGrad's automatic differentiation viatext to improve the ability of general foundation LLMs. We evaluatedAutoMedPrompt on Llama 3, an open-source LLM, using several QA benchmarks,including MedQA, PubMedQA, and the nephrology subspecialty-specific NephSAP.Our results show that prompting with textual gradients outperforms previousmethods on open-source LLMs and surpasses proprietary models such as GPT-4,Claude 3 Opus, and Med-PaLM 2. AutoMedPrompt sets a new state-of-the-art (SOTA)performance on PubMedQA with an accuracy of 82.6$\%$, while also outperformingprevious prompting strategies on open-sourced models for MedQA (77.7$\%$) andNephSAP (63.8$\%$).</description>
      <author>example@mail.com (Sean Wu, Michael Koo, Fabien Scalzo, Ira Kurtz)</author>
      <guid isPermaLink="false">2502.15944v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>ATRI: Mitigating Multilingual Audio Text Retrieval Inconsistencies by Reducing Data Distribution Errors</title>
      <link>http://arxiv.org/abs/2502.14627v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种多语言音频文本检索（ML-ATR）方案，通过理论分析和实验验证了现有方法在跨语种实例相似性匹配中的不一致性问题，并提出了改进策略。&lt;h4&gt;背景&lt;/h4&gt;多语言音频文本检索是一个具有挑战性的任务，目标是从数据库中检索音频片段或多种语言的文本。当前的方法存在跨语种实例相似性匹配的不一致问题。&lt;h4&gt;目的&lt;/h4&gt;分析和解决现有ML-ATR方案在不同语言之间匹配上的不一致性，并提出改进措施来提高召回率和一致性。&lt;h4&gt;方法&lt;/h4&gt;通过1-to-k对比学习和音频-英语共锚对比学习，设计了一种新的多语言音频文本检索框架以减少数据分布误差对结果的影响。&lt;h4&gt;主要发现&lt;/h4&gt;理论分析表明，跨语种实例相似性匹配的不一致问题是由于随机采样语言造成的数据分布错误所引起的。新方法在召回率和一致性指标上取得了主流八种语言（包括英语）的最佳性能。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的学习策略来解决多语言音频文本检索中的数据分布问题，该研究为提高跨语言信息检索的精度提供了理论依据和技术支持。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文已经以中文形式给出，无需再次翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/atri-acl/atri-acl&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multilingual audio-text retrieval (ML-ATR) is a challenging task that aims toretrieve audio clips or multilingual texts from databases. However, existingML-ATR schemes suffer from inconsistencies for instance similarity matchingacross languages. We theoretically analyze the inconsistency in terms of bothmultilingual modal alignment direction error and weight error, and propose thetheoretical weight error upper bound for quantifying the inconsistency. Basedon the analysis of the weight error upper bound, we find that the inconsistencyproblem stems from the data distribution error caused by random sampling oflanguages. We propose a consistent ML-ATR scheme using 1-to-k contrastivelearning and audio-English co-anchor contrastive learning, aiming to mitigatethe negative impact of data distribution error on recall and consistency inML-ATR. Experimental results on the translated AudioCaps and Clotho datasetsshow that our scheme achieves state-of-the-art performance on recall andconsistency metrics for eight mainstream languages, including English. Our codewill be available at https://github.com/ATRI-ACL/ATRI-ACL.</description>
      <author>example@mail.com (Yuguo Yin, Yuxin Xie, Wenyuan Yang, Dongchao Yang, Jinghan Ru, Xianwei Zhuang, Liming Liang, Yuexian Zou)</author>
      <guid isPermaLink="false">2502.14627v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Directional Gradient Projection for Robust Fine-Tuning of Foundation Models</title>
      <link>http://arxiv.org/abs/2502.15895v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;鲁棒微调的目标是将大规模基础模型适应到下游任务，同时保持其对分布变化的稳健性。现有方法主要集中在基于微调权重和预训练权重之间幅度来约束和投影当前模型向预训练初始化。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的分层可训练的方法——方向梯度投影（DiGraP），该方法利用梯度的方向信息，以弥合正则化与多目标优化之间的差距，并将其推广到多模态评估设置中鲁棒微调的场景。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种称为Directional Gradient Projection (DiGraP)的新技术，它通过结合从梯度方向获取的信息来连接正则化和多目标优化。该研究不仅在图像分类上展示了其效果，还将其扩展到视觉问答（VQA）等多模态评估设置。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，与现有基准方法相比，DiGraP 在图像分类及具备判别性和生成性骨干的 VQA 任务中均有更好的性能表现，尤其是在分布变化上具有较高的稳健性。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的方法有效地解决了当前鲁棒微调方法中存在的问题，并证明了其在多模态评估设置中的应用潜力。DiGraP 方法展示了其对现有基准方法的优势，在多种任务和设置中均显示出更好的性能，包括改进的分布内泛化能力和分布外稳健性。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robust fine-tuning aims to adapt large foundation models to downstream taskswhile preserving their robustness to distribution shifts. Existing methodsprimarily focus on constraining and projecting current model towards thepre-trained initialization based on the magnitudes between fine-tuned andpre-trained weights, which often require extensive hyper-parameter tuning andcan sometimes result in underfitting. In this work, we propose DirectionalGradient Projection (DiGraP), a novel layer-wise trainable method thatincorporates directional information from gradients to bridge regularizationand multi-objective optimization. Besides demonstrating our method on imageclassification, as another contribution we generalize this area to themulti-modal evaluation settings for robust fine-tuning. Specifically, we firstbridge the uni-modal and multi-modal gap by performing analysis on ImageClassification reformulated Visual Question Answering (VQA) benchmarks andfurther categorize ten out-of-distribution (OOD) VQA datasets by distributionshift types and degree (i.e. near versus far OOD). Experimental results showthat DiGraP consistently outperforms existing baselines across ImageClassfication and VQA tasks with discriminative and generative backbones,improving both in-distribution (ID) generalization and OOD robustness.</description>
      <author>example@mail.com (Chengyue Huang, Junjiao Tian, Brisa Maneechotesuwan, Shivang Chopra, Zsolt Kira)</author>
      <guid isPermaLink="false">2502.15895v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Token Adaptation via Side Graph Convolution for Efficient Fine-tuning of 3D Point Cloud Transformers</title>
      <link>http://arxiv.org/abs/2502.14142v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个名为Side Token Adaptation on a neighborhood Graph (STAG)的新PEFT算法，用于提高3D点云变换器的时序和空间效率。&lt;h4&gt;背景&lt;/h4&gt;参数高效的微调（PEFT）技术已成为分析3D点云的一种有前景的方法。然而现有的PEFT方法在减少可调节参数的同时，往往面临高计算成本的问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的名为STAG的技术，以同时提高时序和空间效率。&lt;h4&gt;方法&lt;/h4&gt;STAG采用图卷积侧面网络，在冻结的骨干Transformer并行操作中适应令牌以便进行下游任务。通过高效的图卷积、参数共享以及减少梯度计算来显著降低微调的时间和空间成本。&lt;h4&gt;主要发现&lt;/h4&gt;提出的STAG算法能够保持与其他现有方法相当的分类准确性，同时将可调节参数减少到仅0.43M，并且在微调时大大减少了时间和内存消耗。此外，还提出了一个新的基准测试Point Cloud Classification 13 (PCC13)。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了STAG的有效性，表明它能够显著降低计算成本并保持较高的分类精度。&lt;h4&gt;翻译&lt;/h4&gt;参数高效的微调（PEFT）技术在3D点云分析中崭露头角。尽管现有的PEFT方法试图减少可调节参数的数量，但在细调过程中经常遭受高时间和空间计算成本的问题。本文提出了一种名为Side Token Adaptation on a neighborhood Graph (STAG)的新型PEFT算法以实现卓越的时间和空间效率。STAG采用图卷积侧网络，并行于冻结的骨干Transformer进行操作，以将令牌适应到下游任务中。通过高效的图卷积、参数共享以及减少梯度计算，STAG显著降低了微调过程中的时间和空间成本。此外还介绍了一个新的基准测试Point Cloud Classification 13 (PCC13)，该基准集包括多种公开的3D点云数据集以促进全面评估。使用多个预训练模型和PCC13进行的广泛实验显示了STAG的有效性，特别是保持分类精度与现有方法相当的同时将可调节参数减少到仅0.43M，并且在计算时间和内存消耗方面取得显著降低。代码和基准将在https://github.com/takahikof/STAG提供。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/takahikof/stag&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Parameter-efficient fine-tuning (PEFT) of pre-trained 3D point cloudTransformers has emerged as a promising technique for 3D point cloud analysis.While existing PEFT methods attempt to minimize the number of tunableparameters, they often suffer from high temporal and spatial computationalcosts during fine-tuning. This paper proposes a novel PEFT algorithm calledSide Token Adaptation on a neighborhood Graph (STAG) to achieve superiortemporal and spatial efficiency. STAG employs a graph convolutional sidenetwork operating in parallel with a frozen backbone Transformer to adapttokens to downstream tasks. Through efficient graph convolution, parametersharing, and reduced gradient computation, STAG significantly reduces bothtemporal and spatial costs for fine-tuning. We also present Point CloudClassification 13 (PCC13), a new benchmark comprising diverse publiclyavailable 3D point cloud datasets to facilitate comprehensive evaluation.Extensive experiments using multiple pre-trained models and PCC13 demonstratesthe effectiveness of STAG. Specifically, STAG maintains classification accuracycomparable to existing methods while reducing tunable parameters to only 0.43Mand achieving significant reductions in both computation time and memoryconsumption for fine-tuning. Code and benchmark will be available at:https://github.com/takahikof/STAG.</description>
      <author>example@mail.com (Takahiko Furuya)</author>
      <guid isPermaLink="false">2502.14142v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>UniGenCoder: Merging Seq2Seq and Seq2Tree Paradigms for Unified Code Generation</title>
      <link>http://arxiv.org/abs/2502.12490v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  accepted to 47th International Conference on Software Engineering  (ICSE 2025), NIER track&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;基于深度学习的代码生成已彻底改变了当今开发人员编写程序的方式。&lt;h4&gt;背景&lt;/h4&gt;现有的代码生成方法要么集中在序列到序列（Sequence-to-Sequence）范式，即以标记序列的形式生成目标代码；要么是序列到树（Sequence-to-Tree）范式，即输出作为动作序列的目标代码。这两个范式的结合尚未被探索过。&lt;h4&gt;目的&lt;/h4&gt;通过比较这两种范式下产生的代码，作者发现了整合两者的潜在价值，并提出了一种名为UniGenCoder的新模型来解决与代码生成相关的任务。&lt;h4&gt;方法&lt;/h4&gt;UniGenCoder包含一个共享编码器、一个带有最小额外参数集的共享解码器以及一个选择器，该选择器动态地为每个实例选择最优范式。在模型训练过程中，作者首先实施多任务学习和蒸馏策略来促进两个范式的知识转移，并利用对比学习方法训练选择器。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果证明了所提出模型在文本到代码以及代码到代码生成任务中的有效性。&lt;h4&gt;结论&lt;/h4&gt;本文提出的UniGenCoder模型展示了将序列到序列（Sequence-to-Sequence）和序列到树（Sequence-to-Tree）范式结合的潜力，并通过实验证明其优越性。&lt;h4&gt;翻译&lt;/h4&gt;基于深度学习的代码生成已彻底改变了当今开发人员编写程序的方式。现有方法要么集中在序列到序列或序列到树的方法上，这两种方式都存在一定的局限性。为了克服这些限制，作者提出了一种新的模型UniGenCoder，并证明了它在文本到代码和代码到代码任务上的有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning-based code generation has completely transformed the waydevelopers write programs today. Existing approaches to code generation havefocused either on the Sequence-to-Sequence paradigm, which generates targetcode as a sequence of tokens, or the Sequence-to-Tree paradigm, which outputscode as a sequence of actions. While these two paradigms are intuitivelycomplementary, their combination has not been previously explored. By comparingthe code generated under these two paradigms, we find that integrating themholds significant potential. In this paper, we propose UniGenCoder forcode-related generation tasks, which consists of a shared encoder, a shareddecoder with a minimal set of additional parameters to unify two paradigms, anda selector that dynamically chooses optimal paradigm for each instance. Also,during the model training, we first perform the multi-task learning anddistillation strategies to facilitate knowledge transfer between two paradigms,and then leverage contrastive learning to train the selector. Experimentalresults on the text-to-code and code-to-code generation tasks demonstrate theeffectiveness of our proposed model. We release our code athttps://github.com/DeepLearnXMU/UniGenCoder.</description>
      <author>example@mail.com (Liangying Shao, Yanfu Yan, Denys Poshyvanyk, Jinsong Su)</author>
      <guid isPermaLink="false">2502.12490v2</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Breast Lump Detection and Localization with a Tactile Glove Using Deep Learning</title>
      <link>http://arxiv.org/abs/2502.15767v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究开发了一种基于柔性织物的可穿戴触觉手套，用于通过深度学习技术检测模拟乳房模型内的肿块。&lt;h4&gt;背景&lt;/h4&gt;乳腺癌是女性死亡的主要原因之一。通过触摸检查乳房以早期发现肿瘤至关重要。&lt;h4&gt;目的&lt;/h4&gt;设计一种利用深度学习方法来定位乳房内肿块的可穿戴触觉手套。&lt;h4&gt;方法&lt;/h4&gt;该研究使用了定制的硅胶乳房原型（SBPs）以及球形硅胶肿瘤，其中包含不同直径大小的模拟肿块。采用InceptionTime深度学习架构结合迁移学习技术，并收集了10名普通参与者和一位肿瘤-乳腺科医生的数据集。&lt;h4&gt;主要发现&lt;/h4&gt;深度学习模型在判断肿块存在、大小及位置上的准确率分别为82.22%，67.08%和62.63%；同时，该模型对未见过的有经验用户数据的表现也非常好，准确率达到95.01%，88.54%以及82.98%。&lt;h4&gt;结论&lt;/h4&gt;这项技术可以帮助没有经验的人或医疗保健提供者进行更频繁的常规检查，有助于早期发现乳腺癌。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-15&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Breast cancer is the leading cause of mortality among women. Inspection ofbreasts by palpation is the key to early detection. We aim to create a wearabletactile glove that could localize the lump in breasts using deep learning (DL).In this work, we present our flexible fabric-based and soft wearable tactileglove for detecting the lumps within custom-made silicone breast prototypes(SBPs). SBPs are made of soft silicone that imitates the human skin and theinner part of the breast. Ball-shaped silicone tumors of 1.5-, 1.75- and 2.0-cmdiameters are embedded inside to create another set with lumps. Our approach isbased on the InceptionTime DL architecture with transfer learning betweenexperienced and non-experienced users. We collected a dataset from 10 naiveparticipants and one oncologist-mammologist palpating SBPs. We demonstratedthat the DL model can classify lump presence, size and location with anaccuracy of 82.22%, 67.08% and 62.63%, respectively. In addition, we showedthat the model adapted to unseen experienced users with an accuracy of 95.01%,88.54% and 82.98% for lump presence, size and location classification,respectively. This technology can assist inexperienced users or healthcareproviders, thus facilitating more frequent routine checks.</description>
      <author>example@mail.com (Togzhan Syrymova, Amir Yelenov, Karina Burunchina, Nazgul Abulkhanova, Huseyin Atakan Varol, Juan Antonio Corrales Ramon, Zhanat Kappassov)</author>
      <guid isPermaLink="false">2502.15767v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>V-HOP: Visuo-Haptic 6D Object Pose Tracking</title>
      <link>http://arxiv.org/abs/2502.17434v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个结合视觉和触觉反馈的新型统一触觉表示法，旨在提高物体姿态估计在现实世界中的性能。&lt;h4&gt;背景&lt;/h4&gt;人类自然地通过视觉和触觉来感知物体，在抓取过程中丢失任何一种感觉都会影响性能。尽管早期研究尝试结合这两种感觉以改善对象姿态估计，但在实际应用中效果不佳。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的统一化表示法以及基于该表示法的新型视觉-触觉变换器模型，旨在提高跨不同夹持器、传感器布局或仿真与现实环境下的物体跟踪性能。&lt;h4&gt;方法&lt;/h4&gt;开发了一种新统一化的触觉表征来处理多个夹持器实现，并在此基础上提出一种新的视觉-触觉转换器基对象姿态追踪器，该追踪器能够无缝集成视觉和触觉输入。&lt;h4&gt;主要发现&lt;/h4&gt;在自定义数据集和Feelsight数据集中验证了该模型的有效性，证明其在挑战序列中表现出显著性能提升。特别是在面对新型夹持方式、物体及传感器类型时，本方法显示出卓越的泛化性和鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;通过现实世界实验表明，所提出的方法大大优于现有的视觉跟踪器，并且能够实现精确的操作任务。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans naturally integrate vision and haptics for robust object perceptionduring manipulation. The loss of either modality significantly degradesperformance. Inspired by this multisensory integration, prior object poseestimation research has attempted to combine visual and haptic/tactilefeedback. Although these works demonstrate improvements in controlledenvironments or synthetic datasets, they often underperform vision-onlyapproaches in real-world settings due to poor generalization across diversegrippers, sensor layouts, or sim-to-real environments. Furthermore, theytypically estimate the object pose for each frame independently, resulting inless coherent tracking over sequences in real-world deployments. To addressthese limitations, we introduce a novel unified haptic representation thateffectively handles multiple gripper embodiments. Building on thisrepresentation, we introduce a new visuo-haptic transformer-based object posetracker that seamlessly integrates visual and haptic input. We validate ourframework in our dataset and the Feelsight dataset, demonstrating significantperformance improvement on challenging sequences. Notably, our method achievessuperior generalization and robustness across novel embodiments, objects, andsensor types (both taxel-based and vision-based tactile sensors). In real-worldexperiments, we demonstrate that our approach outperforms state-of-the-artvisual trackers by a large margin. We further show that we can achieve precisemanipulation tasks by incorporating our real-time object tracking result intomotion plans, underscoring the advantages of visuo-haptic perception. Our modeland dataset will be made open source upon acceptance of the paper. Projectwebsite: https://lhy.xyz/projects/v-hop/</description>
      <author>example@mail.com (Hongyu Li, Mingxi Jia, Tuluhan Akbulut, Yu Xiang, George Konidaris, Srinath Sridhar)</author>
      <guid isPermaLink="false">2502.17434v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning</title>
      <link>http://arxiv.org/abs/2502.17432v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Website at https://jasonjzliu.com/factr/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一个低成本且直观的双边遥操作系统，该系统将从动臂接收到的外部力传递给主动臂，促进复杂接触密集任务的数据收集，并引入了一个基于课程学习的策略学习方法FACTR。', '背景': '许多人类操作任务依赖于力反馈以可靠执行，但机器人领域中这种力量信息未得到充分利用，导致机器人行为局限于不需要精细力反馈的任务。', '目的': '开发一种能够利用外部力数据进行复杂接触密集型任务的有效遥操作系统和策略学习方法。', '方法': '首先设计了一种低成本且直观的双边遥操作平台，其次提出了一种名为FACTR的策略学习方法，该方法通过在训练过程中逐渐减少视觉输入的干扰来防止过度拟合，并引导策略关注力模态。', '主要发现': '该研究展示了通过充分使用力信息，与不采用课程学习的基线方法相比，在未见过物体上的泛化性能提高了43%。', '结论': '利用力反馈信息可以显著改善机器人在复杂任务中的表现和适应性。'}&lt;h4&gt;翻译&lt;/h4&gt;许多人类执行的任务，如拾取盒子或擀面团，都依赖于力反馈以确保可靠的完成。然而，在大多数机器人手臂中容易获得的这种力信息并未被广泛用于遥操作和策略学习。因此，机器人的行为通常仅限于不需要复杂力反馈的准静态动力学任务。在这篇论文中，我们首先提出了一种低成本且直观的双边遥操作系统，该系统将从动臂接收到的外部力量传递给主动臂，以促进复杂接触密集型任务的数据收集。接下来，我们介绍了FACTR策略学习方法，该方法在训练过程中采用一种课程，通过逐渐减少视觉输入的干扰来防止基于变压器的政策过度拟合，并引导策略正确关注力模态。我们证明了充分利用力量信息能够显著提高与基线相比，在未见过物体上的泛化性能达43%。视频结果和指南可在https://jasonjzliu.com/factr/获得&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Many contact-rich tasks humans perform, such as box pickup or rolling dough,rely on force feedback for reliable execution. However, this force information,which is readily available in most robot arms, is not commonly used inteleoperation and policy learning. Consequently, robot behavior is oftenlimited to quasi-static kinematic tasks that do not require intricateforce-feedback. In this paper, we first present a low-cost, intuitive,bilateral teleoperation setup that relays external forces of the follower armback to the teacher arm, facilitating data collection for complex, contact-richtasks. We then introduce FACTR, a policy learning method that employs acurriculum which corrupts the visual input with decreasing intensity throughouttraining. The curriculum prevents our transformer-based policy fromover-fitting to the visual input and guides the policy to properly attend tothe force modality. We demonstrate that by fully utilizing the forceinformation, our method significantly improves generalization to unseen objectsby 43\% compared to baseline approaches without a curriculum. Video results andinstructions at https://jasonjzliu.com/factr/</description>
      <author>example@mail.com (Jason Jingzhou Liu, Yulong Li, Kenneth Shaw, Tony Tao, Ruslan Salakhutdinov, Deepak Pathak)</author>
      <guid isPermaLink="false">2502.17432v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Enriching Physical-Virtual Interaction in AR Gaming by Tracking Identical Real Objects</title>
      <link>http://arxiv.org/abs/2502.17399v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文提出了一种新的增强现实(AR)技术，旨在改善同质物体在动态环境中的追踪问题，并通过农场到餐桌的游戏展示了该方法的有效性和实用性。&lt;h4&gt;背景&lt;/h4&gt;随着硬件和软件的进步，头戴式AR游戏变得越来越流行。然而，大多数AR游戏仍然依赖于预先扫描的静态场景，互动方式也有限制。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来解决AR游戏中相同物体追踪的问题，并丰富物理与虚拟环境之间的交互体验。&lt;h4&gt;方法&lt;/h4&gt;通过使用AR头盔的部分场景观察数据，结合整数规划解决问题标签分配问题来确定场景中对象的身份，并采用基于Voronoi图的剪枝方法提高计算效率。&lt;h4&gt;主要发现&lt;/h4&gt;提出的方法能够有效地追踪和区分相同的物体，展示了其在增强现实游戏、故事讲述和模拟机器人中的多功能性和实用性。&lt;h4&gt;结论&lt;/h4&gt;新方法证明了其实用性，在农场到餐桌AR游戏中表现良好，并且通过视频演示展现了其潜力。未来的研究可以探索更多实际应用场景来进一步验证该技术的有效性和适应性。&lt;h4&gt;翻译&lt;/h4&gt;增强现实(AR)游戏，尤其是专为头戴设备设计的游戏，随着硬件和软件的进步变得越来越普遍。然而，大多数AR游戏仍然依赖于预扫描或静态场景，并且交互机制通常限制在控制器或手部跟踪上。此外，在AR游戏中存在相同物体的挑战，传统的对象跟踪技术往往难以区分这些物体或者需要安装固定摄像机来追踪全球物体运动。为了解决这些问题，我们提出了一种新的方法，以解决AR场景中相同物体的跟踪问题，从而丰富物理虚拟交互体验。我们的方法利用了AR头盔捕捉的部分场景观察数据，并结合提供的视角和空间信息，通过整数规划解决方案中的标签分配问题确定场景内对象的身份。为了提高计算效率，我们在方法中引入了一种基于Voronoi图的剪枝技术。在农场到餐桌AR游戏中实现该方法展示了其满意的性能和稳健性。此外，我们通过增强现实故事讲述以及模拟游戏机器人的应用展现了该方法的多功能性和实用性。我们的视频演示可在以下链接查看：https://youtu.be/rPGkLYuKvCQ。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Augmented reality (AR) games, particularly those designed for headsets, havebecome increasingly prevalent with advancements in both hardware and software.However, the majority of AR games still rely on pre-scanned or static scenes,and interaction mechanisms are often limited to controllers or hand-tracking.Additionally, the presence of identical objects in AR games poses challengesfor conventional object tracking techniques, which often struggle todifferentiate between identical objects or necessitate the installation offixed cameras for global object movement tracking. In response to theselimitations, we present a novel approach to address the tracking of identicalobjects in an AR scene to enrich physical-virtual interaction. Our methodleverages partial scene observations captured by an AR headset, utilizing theperspective and spatial data provided by this technology. Object identitieswithin the scene are determined through the solution of a label assignmentproblem using integer programming. To enhance computational efficiency, weincorporate a Voronoi diagram-based pruning method into our approach. Ourimplementation of this approach in a farm-to-table AR game demonstrates itssatisfactory performance and robustness. Furthermore, we showcase theversatility and practicality of our method through applications in ARstorytelling and a simulated gaming robot. Our video demo is available at:https://youtu.be/rPGkLYuKvCQ.</description>
      <author>example@mail.com (Liuchuan Yu, Ching-I Huang, Hsueh-Cheng Wang, Lap-Fai Yu)</author>
      <guid isPermaLink="false">2502.17399v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Experimental validation of UAV search and detection system in real wilderness environment</title>
      <link>http://arxiv.org/abs/2502.17372v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  32 pages, 15 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本论文研究了在地中海喀斯特环境中利用自主无人机搜索人类的实验设计与实施，以提高搜救任务效率并增强参与人员的安全性。&lt;h4&gt;背景&lt;/h4&gt;搜索和救援任务需要可靠的搜索方法来定位幸存者，尤其是在挑战性强或难以到达的环境中。因此引入无人驾驶飞机可以在提高搜救任务效率的同时增加所有参与者在任务中的安全性。&lt;h4&gt;目的&lt;/h4&gt;设计并实验验证了一种基于热方程驱动区域覆盖（HEDAC）控制方法和计算机视觉对象检测框架的自主无人机搜索系统，并评估其性能是否与实际情况相符。&lt;h4&gt;方法&lt;/h4&gt;该研究通过概率搜索模型、运动控制系统以及计算机视觉目标检测组成的感知框架，使用热方程驱动区域覆盖（HEDAC）控制方法并根据已知的概率密度和检测函数来指导无人飞机。实验中使用了YOLO算法为基础的目标检测模型，并通过先前收集的正射影像数据库进行训练。&lt;h4&gt;主要发现&lt;/h4&gt;通过对运动控制系统、目标检测以及搜索验证进行了全面分析，表明设计的目标检测模型与实际世界结果相一致，为无人机控制算法提供了强有力的证据支持。&lt;h4&gt;结论&lt;/h4&gt;研究证明了基于HEDAC方法和YOLO算法的自主无人飞机系统在实际搜救任务中的有效性和可靠性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Search and rescue (SAR) missions require reliable search methods to locatesurvivors, especially in challenging or inaccessible environments. This is whyintroducing unmanned aerial vehicles (UAVs) can be of great help to enhance theefficiency of SAR missions while simultaneously increasing the safety ofeveryone involved in the mission. Motivated by this, we design and experimentwith autonomous UAV search for humans in a Mediterranean karst environment. TheUAVs are directed using Heat equation-driven area coverage (HEDAC) ergodiccontrol method according to known probability density and detection function.The implemented sensing framework consists of a probabilistic search model,motion control system, and computer vision object detection. It enablescalculation of the probability of the target being detected in the SAR mission,and this paper focuses on experimental validation of proposed probabilisticframework and UAV control. The uniform probability density to ensure the evenprobability of finding the targets in the desired search area is achieved byassigning suitably thought-out tasks to 78 volunteers. The detection model isbased on YOLO and trained with a previously collected ortho-photo imagedatabase. The experimental search is carefully planned and conducted, while asmany parameters as possible are recorded. The thorough analysis consists of themotion control system, object detection, and the search validation. Theassessment of the detection and search performance provides strong indicationthat the designed detection model in the UAV control algorithm is aligned withreal-world results.</description>
      <author>example@mail.com (Stella Dumenčić, Luka Lanča, Karlo Jakac, Stefan Ivić)</author>
      <guid isPermaLink="false">2502.17372v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>HATPIC: An Open-Source Single Axis Haptic Joystick for Robotic Development</title>
      <link>http://arxiv.org/abs/2502.17362v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  2 pages, 1 figure, conference&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;人类通过触觉处理的信息远超视觉，因此远程操作中的触觉技术将在未来变得至关重要。当前的触觉设备要么难以获取，要么提供的力反馈质量较低，本文提出了一种单轴开源触觉设备设计方案来解决这些问题。&lt;h4&gt;背景&lt;/h4&gt;人的大部分信息处理是依靠触觉完成的，在极端条件下尤其如此，这意味着触觉对于远程操作的重要性将日益增加。&lt;h4&gt;目的&lt;/h4&gt;设计一种易于访问且具有高质量力反馈功能的开源单轴触觉装置以促进远程操作技术的发展和应用。&lt;h4&gt;方法&lt;/h4&gt;介绍了一种新的触觉设备，并展示了其与常见机器人工具集成的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;新设计的手柄式控制器有可能加速各种机器人应用程序中触觉技术的应用，从而提高操作人员的反馈质量和控制水平。&lt;h4&gt;结论&lt;/h4&gt;通过引入这种低成本、高质量力反馈的触觉装置可以极大地促进远程操作技术的发展和广泛应用。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为：人类处理的信息中有相当大的一部分是通过触觉完成的，而不是视觉。因此，在未来几年中，用于远程操控的触觉技术将变得至关重要，因为它为运营商提供了一种额外的感觉通道，这对于在极端条件下进行解释非常重要。然而，目前的触觉设备设置要么难以访问，要么提供的力反馈质量低劣。这项工作提出了一个旨在解决这些问题的单轴、开源的远程操作设计方案。首先介绍了该触觉装置，并展示了其与常见机器人工具集成的可能性。所提出的操纵杆有望加速各种机器人应用中触觉技术的发展和部署，从而增强操作人员的反馈和控制能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans process significantly more information through the sense of touch thanthrough vision. Consequently, haptics for telemanipulation is poised to becomeessential in the coming years, as it offers operators an additional sensorychannel crucial for interpretation in extreme conditions. However, currenthaptic device setups are either difficult to access or provide low-qualityforce feedback rendering. This work proposes the design of a single-axis,open-source setup for telemanipulation development, aimed at addressing theseissues. We first introduce the haptic device and demonstrate its integrationwith common robotic tools. The proposed joystick has the potential toaccelerate the development and deployment of haptic technology in a wide rangeof robotics applications, enhancing operator feedback and control.</description>
      <author>example@mail.com (Julien Mellet, Fabio Ruggiero, Vincenzo Lippiello)</author>
      <guid isPermaLink="false">2502.17362v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>MegaLoc: One Retrieval to Place Them All</title>
      <link>http://arxiv.org/abs/2502.17237v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Tech Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种名为MegaLoc的检索模型，该模型结合了多种现有技术，在多个计算机视觉任务中表现出色。&lt;h4&gt;背景&lt;/h4&gt;从给定查询获取相同位置的图像对于多项计算机视觉任务至关重要。然而，现有的解决方案仅针对单一任务设计，并且在遇到需求变化或未见数据时可能会失败。&lt;h4&gt;目的&lt;/h4&gt;开发一个能够在不同任务和环境要求下均能高效工作的通用检索模型。&lt;h4&gt;方法&lt;/h4&gt;结合了多种现有技术和训练技术来构建MegaLoc模型。&lt;h4&gt;主要发现&lt;/h4&gt;{'1': 'MegaLoc在大量视觉位置识别数据集上达到最新技术水平，', '2': '在常见的地标检索数据集中表现出色，', '3': '在LaMAR数据集的图像定位任务中引入新方法后，设定了新的性能标准。'}&lt;h4&gt;结论&lt;/h4&gt;MegaLoc展示出了跨多个计算机视觉任务领域的强大适应性和通用性。&lt;h4&gt;翻译&lt;/h4&gt;从给定查询获取相同位置的图像是多项重要视觉任务（如视觉地方识别、地标检索、图像定位、三维重建和SLAM）的关键部分。然而，现有的解决方案通常只为特定任务设计，在遇到需求变化或未见数据时表现不佳。本文通过结合多种现有方法和技术训练出了一种名为MegaLoc的新模型，该模型在多个计算机视觉任务中均表现出色，并且其代码开源可获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Retrieving images from the same location as a given query is an importantcomponent of multiple computer vision tasks, like Visual Place Recognition,Landmark Retrieval, Visual Localization, 3D reconstruction, and SLAM. However,existing solutions are built to specifically work for one of these tasks, andare known to fail when the requirements slightly change or when they meetout-of-distribution data. In this paper we combine a variety of existingmethods, training techniques, and datasets to train a retrieval model, calledMegaLoc, that is performant on multiple tasks. We find that MegaLoc (1)achieves state of the art on a large number of Visual Place Recognitiondatasets, (2) impressive results on common Landmark Retrieval datasets, and (3)sets a new state of the art for Visual Localization on the LaMAR datasets,where we only changed the retrieval method to the existing localizationpipeline. The code for MegaLoc is available athttps://github.com/gmberton/MegaLoc</description>
      <author>example@mail.com (Gabriele Berton, Carlo Masone)</author>
      <guid isPermaLink="false">2502.17237v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SoFFT: Spatial Fourier Transform for Modeling Continuum Soft Robots</title>
      <link>http://arxiv.org/abs/2502.17347v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于傅里叶变换的方法，用于紧凑地描述连续软机器人的变形，并通过数值模拟和实验验证了该方法的有效性。&lt;h4&gt;背景&lt;/h4&gt;连续软机器人由柔性材料组成，理论上具有无限自由度，在非结构化环境中表现出色的适应能力。Cosserat Rod理论作为建模这些机器人的有效框架已得到广泛应用。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的基于信号处理的方法来描述和建模连续软机器人的变形过程。&lt;h4&gt;方法&lt;/h4&gt;将机器人骨架视为时空中的信号，应用傅里叶变换将其变形简化为频域表示。该方法不仅统一了现有建模策略，并提供了实验捕捉机器人变形的基于数据驱动的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过数值模拟和现实原型实验验证了所提方法的有效性，表明在减少自由度的同时保持变形准确性的能力。&lt;h4&gt;结论&lt;/h4&gt;提出的新方法为连续软机器人的研究提供了一种有效途径，展示了傅里叶变换用于机器人建模的潜力。&lt;h4&gt;翻译&lt;/h4&gt;连续软机器人由柔性材料组成，在非结构化环境中表现出高度适应性。基于Cosserat Rod理论的应用提出了将机器人骨架视为时空信号的方法，并通过傅里叶变换来描述其变形过程。这种方法不仅统一了现有的建模策略，还提供了一种实验捕捉机器人变形的数据驱动方法。数值模拟和现实原型实验验证了该方法的有效性，在减少自由度的同时保持了变形的准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Continuum soft robots, composed of flexible materials, exhibit theoreticallyinfinite degrees of freedom, enabling notable adaptability in unstructuredenvironments. Cosserat Rod Theory has emerged as a prominent framework formodeling these robots efficiently, representing continuum soft robots astime-varying curves, known as backbones. In this work, we propose viewing therobot's backbone as a signal in space and time, applying the Fourier transformto describe its deformation compactly. This approach unifies existing modelingstrategies within the Cosserat Rod Theory framework, offering insights intocommonly used heuristic methods. Moreover, the Fourier transform enables thedevelopment of a data-driven methodology to experimentally capture the robot'sdeformation. The proposed approach is validated through numerical simulationsand experiments on a real-world prototype, demonstrating a reduction in thedegrees of freedom while preserving the accuracy of the deformationrepresentation.</description>
      <author>example@mail.com (Daniele Caradonna, Diego Bianchi, Franco Angelini, Egidio Falotico)</author>
      <guid isPermaLink="false">2502.17347v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Modeling, Simulation, and Application of Spatio-Temporal Characteristics Detection in Incipient Slip</title>
      <link>http://arxiv.org/abs/2502.17335v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  21 pages, 19 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的方法，用于检测机器人抓握和操作任务中的初始滑动。该方法通过分析局部位移现象建立了特征应变率极端事件与局部滑动状态之间的关系。&lt;h4&gt;背景&lt;/h4&gt;早期滑动检测对于机器人抓取和操作任务至关重要，但由于物体属性的多样性和复杂的工作条件，保持其适应性仍然具有挑战性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的方法来完全表示滑动的空间-时间特征，并建立特征应变率极端事件与局部滑动状态之间的关系。&lt;h4&gt;方法&lt;/h4&gt;基于局部位移现象分析建立了特征应变率极端事件和局部滑动状态之间的联系，该方法能检测到粘着-滑动区域的时空分布。同时，这种方法可以应用于基于视觉的触觉传感器等应变分布传感设备。&lt;h4&gt;主要发现&lt;/h4&gt;通过模拟和原型实验验证了在不同接触条件下（包括不同的接触几何形状、摩擦系数和组合负载）此方法的有效性。实验表明，该方法不仅能够准确可靠地界定初始滑动，还促进了摩擦参数估计和适应性抓握控制的实现。&lt;h4&gt;结论&lt;/h4&gt;提出的模型及其检测方法具有广泛的适用性和可靠性，在复杂的工作条件下表现出色，并为机器人抓取任务提供了关键反馈。&lt;h4&gt;翻译&lt;/h4&gt;初期滑动检测对于机器人的抓取与操作至关重要。然而，使其在多样的物体特性和复杂的作业环境下保持适应性依然面临挑战。本文重点在于完全表示滑移的时空特性，并提出了一种新的模型来捕捉初期滑动现象及其检测方法。通过对局部位移的研究建立了应变率极端事件和本地化滑动状态之间的联系，该方法可以识别出粘着-滑移区域的空间分布与时间动力学特征，且适用于基于视觉的触觉传感设备等应变分布传感器的应用场景。模拟及原型实验验证了其在不同接触条件下的有效性（比如不同的几何形状、摩擦系数和组合负载），并表明这种新方法不仅能够精确地描绘初期滑动现象，并有助于实现摩擦参数估计与自适应抓取控制。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Incipient slip detection provides critical feedback for robotic grasping andmanipulation tasks. However, maintaining its adaptability under diverse objectproperties and complex working conditions remains challenging. This articlehighlights the importance of completely representing spatio-temporal featuresof slip, and proposes a novel approach for incipient slip modeling anddetection. Based on the analysis of localized displacement phenomenon, weestablish the relationship between the characteristic strain rate extremeevents and the local slip state. This approach enables the detection of boththe spatial distribution and temporal dynamics of stick-slip regions. Also, theproposed method can be applied to strain distribution sensing devices, such asvision-based tactile sensors. Simulations and prototype experiments validatedthe effectiveness of this approach under varying contact conditions, includingdifferent contact geometries, friction coefficients, and combined loads.Experiments demonstrated that this method not only accurately and reliablydelineates incipient slip, but also facilitates friction parameter estimationand adaptive grasping control.</description>
      <author>example@mail.com (Mingxuan Li, Lunwei Zhang, Qiyin Huang, Tiemin Li, Yao Jiang)</author>
      <guid isPermaLink="false">2502.17335v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Computation Offloading Strategies in Integrated Terrestrial and Non-Terrestrial Networks</title>
      <link>http://arxiv.org/abs/2502.15903v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted as chapter to Elsevier&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文探讨了集成地面和非地面网络（IT-NTNs）在计算密集型应用中的作用，这些应用包括增强现实、自动驾驶、远程医疗和智能城市等。&lt;h4&gt;背景&lt;/h4&gt;传统地面网络由于覆盖不足、容量有限以及偏远地区的高延迟而限制了上述应用的发展。&lt;h4&gt;目的&lt;/h4&gt;研究如何通过集成的地面与非地面网络来应对这些问题，并实现高效的计算卸载。&lt;h4&gt;方法&lt;/h4&gt;首先介绍了移动边缘计算（MEC）及其向多接入边缘计算演化的路径，接着详细探讨了IT-NTNs架构，包括地面基站、无人机、高空平台和低轨卫星如何协同工作以提供无缝连接。文章还分析了几种不同的计算卸载策略，并讨论了关键使能技术。&lt;h4&gt;主要发现&lt;/h4&gt;各种计算卸载方法各有优缺点；关键技术如非正交多址接入（NOMA）、毫米波/太赫兹通信和可重构智能表面等对于现有资源分配、任务卸载决策及移动管理算法至关重要。&lt;h4&gt;结论&lt;/h4&gt;本文强调了计算卸载在IT-NTNs中的变革性影响，展望未来的研究方向，并指出了此类网络在未来重新定义通信与计算范式的潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of computation-intensive applications like augmentedreality, autonomous driving, remote healthcare, and smart cities has exposedthe limitations of traditional terrestrial networks, particularly in terms ofinadequate coverage, limited capacity, and high latency in remote areas. Thischapter explores how integrated terrestrial and non-terrestrial networks(IT-NTNs) can address these challenges and enable efficient computationoffloading. We examine mobile edge computing (MEC) and its evolution towardmultiple-access edge computing, highlighting the critical role computationoffloading plays for resource-constrained devices. We then discuss thearchitecture of IT-NTNs, focusing on how terrestrial base stations, unmannedaerial vehicles (UAVs), high-altitude platforms (HAPs), and LEO satellites worktogether to deliver ubiquitous connectivity. Furthermore, we analyze variouscomputation offloading strategies, including edge, cloud, and hybridoffloading, outlining their strengths and weaknesses. Key enabling technologiessuch as NOMA, mmWave/THz communication, and reconfigurable intelligent surfaces(RIS) are also explored as essential components of existing algorithms forresource allocation, task offloading decisions, and mobility management.Finally, we conclude by highlighting the transformative impact of computationoffloading in IT-NTNs across diverse application areas and discuss keychallenges and future research directions, emphasizing the potential of thesenetworks to revolutionize communication and computation paradigms.</description>
      <author>example@mail.com (Muhammad Ahmed Mohsin, Muhammad Umer, Amara Umar, Hatem Abou-Zeid, Syed Ali Hassan)</author>
      <guid isPermaLink="false">2502.15903v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>GraphTeam: Facilitating Large Language Model-based Graph Analysis via Multi-Agent Collaboration</title>
      <link>http://arxiv.org/abs/2410.18032v4</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种名为GraphTeam的多代理系统，用于基于大型语言模型（LLM）进行图分析。&lt;h4&gt;背景&lt;/h4&gt;现有的图数据分析方法要么将图形神经网络与特定机器学习任务相结合，限制了其可移植性；要么仅依赖于LLMs内部推理能力，导致性能不佳。为解决这些问题，本文利用最近关于LLM代理的进展来提高外部知识或工具使用的能力。&lt;h4&gt;目的&lt;/h4&gt;通过模拟人类解决问题策略（如类比和协作），提出一种基于多代理系统的图分析解决方案GraphTeam。&lt;h4&gt;方法&lt;/h4&gt;{'输入-输出标准化模块': '包括问题代理提取并细化四个关键参数，以促进问题理解；答案代理组织结果以满足输出要求。', '外部知识检索模块': '构建了一个包含相关文档和经验信息的知识库，并通过搜索代理为每个问题检索最相关的条目。', '问题解决模块': '编码代理根据从搜索代理获得的信息使用编程生成解决方案，如果编码代理无法工作，则推理代理将直接计算结果。'}&lt;h4&gt;主要发现&lt;/h4&gt;在六个图分析基准上的广泛实验显示，GraphTeam实现了最先进的性能，比最佳基线平均提高了25.85%的准确性。&lt;h4&gt;结论&lt;/h4&gt;GraphTeam通过模拟人类问题解决策略并有效利用外部知识和工具，为复杂图形数据分析任务提供了一种新的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;摘要提供了关于GraphTeam系统的设计细节、模块功能以及实验结果的信息。该研究展示了在图数据处理方面的新进展，并强调了LLM代理协同工作的优势。&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-10-23&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/bupt-gamma/graphteam&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Graphs are widely used for modeling relational data in real-world scenarios,such as social networks and urban computing. Existing LLM-based graph analysisapproaches either integrate graph neural networks (GNNs) for specific machinelearning tasks, limiting their transferability, or rely solely on LLMs'internal reasoning ability, resulting in suboptimal performance. To addressthese limitations, we take advantage of recent advances in LLM-based agents,which have shown capabilities of utilizing external knowledge or tools forproblem solving. By simulating human problem-solving strategies such as analogyand collaboration, we propose a multi-agent system based on LLMs namedGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents fromthree modules, and the agents with different specialities can collaborate witheach other to address complex problems. Specifically, (1) input-outputnormalization module: the question agent extracts and refines four keyarguments from the original question, facilitating the problem understanding,and the answer agent organizes the results to meet the output requirement; (2)external knowledge retrieval module: we first build a knowledge base consistingof relevant documentation and experience information, and then the search agentretrieves the most relevant entries for each question. (3) problem-solvingmodule: given the retrieved information from search agent, the coding agentuses established algorithms via programming to generate solutions, and in casethe coding agent does not work, the reasoning agent will directly compute theresults without programming. Extensive experiments on six graph analysisbenchmarks demonstrate that GraphTeam achieves state-of-the-art performancewith an average 25.85% improvement over the best baseline in terms of accuracy.The code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.</description>
      <author>example@mail.com (Xin Sky Li, Qizhi Chu, Yubin Chen, Yang Liu, Yaoqi Liu, Zekai Yu, Weize Chen, Chen Qian, Chuan Shi, Cheng Yang)</author>
      <guid isPermaLink="false">2410.18032v4</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>TDMPBC: Self-Imitative Reinforcement Learning for Humanoid Robot Control</title>
      <link>http://arxiv.org/abs/2502.17322v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了SIRL框架，通过模仿任务相关的轨迹来改进强化学习算法在高维空间中的表现。&lt;h4&gt;背景&lt;/h4&gt;复杂高维度的空间对于配备灵巧手的人形机器人等系统来说，在有限样本预算下平衡探索和利用方面对强化学习算法构成了挑战。可行的任务完成区域通常非常狭窄。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的强化学习框架，以改善在复杂行动空间中任务表现的问题。&lt;h4&gt;方法&lt;/h4&gt;通过引入自模仿强化学习（SIRL）框架，该框架使RL算法能够模仿潜在任务相关的轨迹，并根据轨迹回报调整行为克隆的权重。&lt;h4&gt;主要发现&lt;/h4&gt;所提出的算法在HumanoidBench上实现了120%的性能提升，同时仅增加了5%的计算开销。进一步的可视化分析表明，这种性能改进确实带来了有意义的行为改善。&lt;h4&gt;结论&lt;/h4&gt;SIRL框架提供了一种有效的方法来增强RL算法处理复杂高维空间任务的能力。&lt;h4&gt;翻译&lt;/h4&gt;复杂的高维度空间（具有大量自由度和复杂行动空间），如装备灵巧手的人形机器人，对强化学习算法构成了重大挑战。这类算法需要在有限的样本预算下巧妙地平衡探索与利用之间的关系。通常情况下，在这种复杂的高维空间中完成任务的有效区域极其狭窄。例如，在人形机器人的运动控制领域，绝大多数的空间都对应于跌倒的状态，而能够执行后续任务的小部分状态则仅占微不足道的比例。一旦机器人进入了潜在的任务相关区域，它应该更加重视该区域内的数据。基于这一见解，我们提出了自模仿强化学习（SIRL）框架，在此框架下RL算法也模仿潜在的任务相关的轨迹。具体来说，使用轨迹回报来确定其任务的相关性，并采用额外的行为克隆方法，权重根据轨迹回报动态调整。结果表明，所提出的算法在具有挑战性的HumanoidBench上实现了120%的性能提升，同时仅增加了5%的计算开销。通过进一步可视化分析发现，显著的性能改进确实导致了有意义的行为改善，成功解决了多个任务。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Complex high-dimensional spaces with high Degree-of-Freedom and complicatedaction spaces, such as humanoid robots equipped with dexterous hands, posesignificant challenges for reinforcement learning (RL) algorithms, which needto wisely balance exploration and exploitation under limited sample budgets. Ingeneral, feasible regions for accomplishing tasks within complexhigh-dimensional spaces are exceedingly narrow. For instance, in the context ofhumanoid robot motion control, the vast majority of space corresponds tofalling, while only a minuscule fraction corresponds to standing upright, whichis conducive to the completion of downstream tasks. Once the robot exploresinto a potentially task-relevant region, it should place greater emphasis onthe data within that region. Building on this insight, we propose the$\textbf{S}$elf-$\textbf{I}$mitative $\textbf{R}$einforcement$\textbf{L}$earning ($\textbf{SIRL}$) framework, where the RL algorithm alsoimitates potentially task-relevant trajectories. Specifically, trajectoryreturn is utilized to determine its relevance to the task and an additionalbehavior cloning is adopted whose weight is dynamically adjusted based on thetrajectory return. As a result, our proposed algorithm achieves 120%performance improvement on the challenging HumanoidBench with 5% extracomputation overhead. With further visualization, we find the significantperformance gain does lead to meaningful behavior improvement that severaltasks are solved successfully.</description>
      <author>example@mail.com (Zifeng Zhuang, Diyuan Shi, Runze Suo, Xiao He, Hongyin Zhang, Ting Wang, Shangke Lyu, Donglin Wang)</author>
      <guid isPermaLink="false">2502.17322v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Inverse Kinematics on Guiding Vector Fields for Robot Path Following</title>
      <link>http://arxiv.org/abs/2502.17313v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Yu Zhou and Jes\'us Bautista contributed equally to this work. In the  proceedings of the IEEE International Conference on Robotics and Automation  (ICRA) 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;逆向运动学是一种在机器人学中用于姿态和定位控制的基础技术，通常应用于末端执行器。然而，在这篇论文中，研究者们扩展了逆向运动学的概念，将其应用到自主移动机器人的路径跟随上的导向矢量场。&lt;h4&gt;目的&lt;/h4&gt;为了使机器人能够沿着期望的路径收敛并行进，文章提出了如何使用逆向运动学方法构建误差信号，并驱动导向矢量场朝向期望路径。&lt;h4&gt;方法&lt;/h4&gt;首先，从形式上展示了如何将逆向运动学应用于单积分器机器人的导向矢量场上。然后利用逆向运动学确保层次集误差信号表现为线性系统，从而方便对机器人在向目标路径过渡时的行为进行控制，并允许注入前馈信号以实现沿路径的精确运动行为。&lt;h4&gt;主要发现&lt;/h4&gt;研究提出了如何将该技术应用于常速单轮车（如固定翼无人机），以便它们能够跟踪二维路径并具有精细的瞬态控制能力。&lt;h4&gt;结论&lt;/h4&gt;通过实际飞行测试验证了预测的理论结果，表明这种方法在指导自主移动机器人沿特定路径精确运动方面是有效的。&lt;h4&gt;翻译&lt;/h4&gt;逆向运动学被扩展应用于导向矢量场中，使自主移动机器人能够准确地跟踪预定路径。该技术不仅确保了层次集误差信号呈线性系统特征，还通过注入前馈信号提升了瞬态控制性能，并在实际应用中证明其有效性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Inverse kinematics is a fundamental technique for motion and positioningcontrol in robotics, typically applied to end-effectors. In this paper, weextend the concept of inverse kinematics to guiding vector fields for pathfollowing in autonomous mobile robots. The desired path is defined by itsimplicit equation, i.e., by a collection of points belonging to one or morezero-level sets. These level sets serve as a reference to construct an errorsignal that drives the guiding vector field toward the desired path, enablingthe robot to converge and travel along the path by following such a vectorfield. We start with the formal exposition on how inverse kinematics can beapplied to guiding vector fields for single-integrator robots in anm-dimensional Euclidean space. Then, we leverage inverse kinematics to ensurethat the level-set error signal behaves as a linear system, facilitatingcontrol over the robot's transient motion toward the desired path and allowingfor the injection of feed-forward signals to induce precise motion behavioralong the path. We then propose solutions to the theoretical and practicalchallenges of applying this technique to unicycles with constant speeds tofollow 2D paths with precise transient control. We finish by validating thepredicted theoretical results through real flights with fixed-wing drones.</description>
      <author>example@mail.com (Yu Zhou, Jesús Bautista, Weijia Yao, Héctor García de Marina)</author>
      <guid isPermaLink="false">2502.17313v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Hybrid Human-Machine Perception via Adaptive LiDAR for Advanced Driver Assistance Systems</title>
      <link>http://arxiv.org/abs/2502.17309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;准确的环境感知对于高级驾驶辅助系统（ADAS）至关重要。激光雷达（LiDAR）在ADAS中起着至关重要的作用，可以可靠地检测障碍物并帮助确保交通安全。&lt;h4&gt;背景&lt;/h4&gt;现有研究表明，根据环境特性调整激光雷达的分辨率和范围可以提高机器感知能力。然而，目前针对ADAS的自适应激光雷达方法尚未探索将车辆感知能力和驾驶员视觉感知结合起来的可能性，这可能进一步提升探测性能。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的系统，该系统可根据驾驶员的视觉感知来调整LiDAR特性，以增强超出人类视野范围外的LiDAR感应。在虚拟环境CARLA中开发系统的概念验证原型。&lt;h4&gt;方法&lt;/h4&gt;系统整合实时数据监控驾驶员视线，识别环境中驾驶员正在观察的区域，并通过动态增加外围未关注区域的激光雷达的范围和分辨率来优化激光雷达资源。&lt;h4&gt;主要发现&lt;/h4&gt;模拟结果表明，这种基于注视的LiDAR相对于基准独立式LiDAR，在雾等具有挑战性的环境条件下性能更佳。该混合的人机感知方法在实时驾驶场景中为ADAS应用提供更好的安全性和态势感知能力。&lt;h4&gt;结论&lt;/h4&gt;通过结合车辆和驾驶员的感知能力来优化激光雷达特性，可以提高复杂环境下ADAS系统的检测性能和安全性。&lt;h4&gt;翻译&lt;/h4&gt;准确的环境感知对高级驾驶辅助系统（ADAS）至关重要。现有的研究表明，根据环境特征调整LiDAR的分辨率和范围可以改善机器感知。然而，目前用于ADAS的自适应LiDAR方法还没有探索将车辆感知能力与驾驶员视觉感知结合的可能性，这有可能进一步提升检测性能。本文提出了一种新的系统，该系统可根据人类驾驶员的视觉感知来定制LiDAR特性，以增强超出人眼视野之外的LiDAR感应。我们开发了一个在虚拟环境CARLA中的概念验证原型系统。该系统集成了实时数据监控司机视线的功能，可以识别出环境中司机正在查看的区域，并通过动态增加未被注视周围区域中激光雷达的范围和分辨率来优化其资源分配。模拟结果表明，这种基于目光追踪的LiDAR相对于独立式LiDAR基准在具有挑战性的环境条件下（例如雾）表现更佳。该混合的人机感知方法可能为ADAS应用中的实时驾驶场景提供增强的安全性和态势感知能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate environmental perception is critical for advanced driver assistancesystems (ADAS). Light detection and ranging (LiDAR) systems play a crucial rolein ADAS; they can reliably detect obstacles and help ensure traffic safety.Existing research on LiDAR sensing has demonstrated that adapting the LiDAR'sresolution and range based on environmental characteristics can improve machineperception. However, current adaptive LiDAR approaches for ADAS have notexplored the possibility of combining the perception abilities of the vehicleand the human driver, which can potentially further enhance the detectionperformance. In this paper, we propose a novel system that adapts LiDARcharacteristics to human driver's visual perception to enhance LiDAR sensingoutside human's field of view. We develop a proof-of-concept prototype of thesystem in the virtual environment CARLA. Our system integrates real-time dataon the driver's gaze to identify regions in the environment that the driver ismonitoring. This allows the system to optimize LiDAR resources by dynamicallyincreasing the LiDAR's range and resolution in peripheral areas that the drivermay not be attending to. Our simulations show that this gaze-aware LiDARenhances detection performance compared to a baseline standalone LiDAR,particularly in challenging environmental conditions like fog. Our hybridhuman-machine sensing approach potentially offers improved safety andsituational awareness in real-time driving scenarios for ADAS applications.</description>
      <author>example@mail.com (Federico Scarì, Nitin Jonathan Myers, Chen Quan, Arkady Zgonnikov)</author>
      <guid isPermaLink="false">2502.17309v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SmartEdge: Smart Healthcare End-to-End Integrated Edge and Cloud Computing System for Diabetes Prediction Enabled by Ensemble Machine Learning</title>
      <link>http://arxiv.org/abs/2502.15762v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了SmartEdge系统，该系统利用边缘计算和云端协同工作，在糖尿病预测中实现了低延迟、高效能的解决方案。通过集成多种风险因素，并使用投票算法提高了模型预测准确率。&lt;h4&gt;背景&lt;/h4&gt;物联网（IoT）正在推动智能城市的医疗、交通、工业和教育等领域的发展。特别是在智能医院和远程患者监测（RPM）方面，互联网医疗事物（IoMT）变得尤为重要。然而，现有基于云计算的方法在延迟敏感的应用中表现不佳。&lt;h4&gt;目的&lt;/h4&gt;本文提出了一种新的AI驱动的边缘计算与云协同系统SmartEdge，旨在解决糖尿病预测中的低延迟问题，并展示其在健康应用中的有效性和可扩展性。&lt;h4&gt;方法&lt;/h4&gt;论文提出了一个基于边缘和云端的框架，在不同配置下部署糖尿病预测模型。评估了系统的性能指标包括延迟、准确性及响应时间。&lt;h4&gt;主要发现&lt;/h4&gt;使用集成机器学习投票算法，可以将预测准确率提高5%，优于单一模型。&lt;h4&gt;结论&lt;/h4&gt;SmartEdge系统展示了在医疗领域中边缘计算和云服务结合的有效性和潜力，并为未来的研究提供了新的方向。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-14&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The Internet of Things (IoT) revolutionizes smart city domains such ashealthcare, transportation, industry, and education. The Internet of MedicalThings (IoMT) is gaining prominence, particularly in smart hospitals and RemotePatient Monitoring (RPM). The vast volume of data generated by IoMT devicesshould be analyzed in real-time for health surveillance, prognosis, andprediction of diseases. Current approaches relying on Cloud computing toprovide the necessary computing and storage capabilities do not scale for theselatency-sensitive applications. Edge computing emerges as a solution bybringing cloud services closer to IoMT devices. This paper introducesSmartEdge, an AI-powered smart healthcare end-to-end integrated edge and cloudcomputing system for diabetes prediction. This work addresses latency concernsand demonstrates the efficacy of edge resources in healthcare applicationswithin an end-to-end system. The system leverages various risk factors fordiabetes prediction. We propose an Edge and Cloud-enabled framework to deploythe proposed diabetes prediction models on various configurations using edgenodes and main cloud servers. Performance metrics are evaluated using, latency,accuracy, and response time. By using ensemble machine learning votingalgorithms we can improve the prediction accuracy by 5% versus a single modelprediction.</description>
      <author>example@mail.com (Alain Hennebelle, Qifan Dieng, Leila Ismail, Rajkumar Buyya)</author>
      <guid isPermaLink="false">2502.15762v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Co-Designing Augmented Reality Tools for High-Stakes Clinical Teamwork</title>
      <link>http://arxiv.org/abs/2502.17295v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  19 pages, 7 figures, submitted to DIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;这项研究探讨了医疗工作者如何利用增强现实头戴式显示器（AR-HMDs）来提高急诊团队的工作效率，通过参与性设计与医疗工作者合作，发现了七个基于角色的AR-HMD应用场景，并提供了针对高风险环境下的团队工作的设计建议。&lt;h4&gt;背景&lt;/h4&gt;尽管AR-HMD在支持医疗环境中团队协作方面显示出巨大潜力，但在急诊科（ER）团队中的应用却很少得到研究。ER具有特殊的挑战，如程序记忆、医疗错误和沟通障碍。&lt;h4&gt;目的&lt;/h4&gt;通过与医疗工作者的合作进行参与性设计研究，探索AR-HMD如何促进急诊流程中团队合作的潜在能力。&lt;h4&gt;方法&lt;/h4&gt;进行了一个参与者驱动的设计研究项目，旨在了解HCWs在ER环境中利用AR-HMD的可能性。&lt;h4&gt;主要发现&lt;/h4&gt;AR-HMD可以作为信息共享和检索系统使用，在知识鸿沟方面发挥作用，并解决了将AR-HMD集成到ER工作流程中的担忧。提出了七种基于角色的场景设计建议，适用于不同专业背景、执行多项医疗任务的HCWs。&lt;h4&gt;结论&lt;/h4&gt;希望通过这项研究激发设计师开发新的AR-HMD应用程序，用于高风险团队环境。&lt;h4&gt;翻译&lt;/h4&gt;医护人员如何利用增强现实头戴式显示器（AR-HMDs）来改善急诊科内的团队协作？尽管在支持医疗服务中的团队合作方面显示了巨大潜力，但专门针对ER团队的设计却很少见。这项研究通过与医疗工作者的合作设计研究，揭示了AR-HMD可以作为信息共享和检索系统使用，解决了将AR-HMD集成到ER工作流程中的问题，并提出了适用于不同专业背景的HCWs在执行多种医疗任务时的角色基于的应用场景设计建议。希望此项研究能激发设计师开发新的适合高风险团队环境下的AR-HMD应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; How might healthcare workers (HCWs) leverage augmented reality head-mounteddisplays (AR-HMDs) to enhance teamwork? Although AR-HMDs have shown immensepromise in supporting teamwork in healthcare settings, design for EmergencyDepartment (ER) teams has received little attention. The ER presents uniquechallenges, including procedural recall, medical errors, and communicationgaps. To address this gap, we engaged in a participatory design study withhealthcare workers to gain a deep understanding of the potential for AR-HMDs tofacilitate teamwork during ER procedures. Our results reveal that AR-HMDs canbe used as an information-sharing and information-retrieval system to bridgeknowledge gaps, and concerns about integrating AR-HMDs in ER workflows. Wecontribute design recommendations for seven role-based AR-HMD applicationscenarios involving HCWs with various expertise, working across multiplemedical tasks. We hope our research inspires designers to embark on thedevelopment of new AR-HMD applications for high-stakes, team environments.</description>
      <author>example@mail.com (Angelique Taylor, Tauhid Tanjim, Huajie Cao, Jalynn Blu Nicoly, Jonathan I. Segal, Jonathan St. George, Soyon Kim, Kevin Ching, Francisco R. Ortega, Hee Rin Lee)</author>
      <guid isPermaLink="false">2502.17295v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Continuous Wrist Control on the Hannes Prosthesis: a Vision-based Shared Autonomy Framework</title>
      <link>http://arxiv.org/abs/2502.17265v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025. Project website:  https://hsp-iit.github.io/hannes-wrist-control&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于计算机视觉的系统，该系统结合用户和自动系统的合作，在共享自主框架中实现假肢腕关节连续控制。&lt;h4&gt;背景&lt;/h4&gt;大多数用于假肢抓握的技术集中在灵巧的手指控制上，而忽视了手腕动作。这迫使用户通过肘部、肩部和臀部的动作来适应手腕的抓取需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合计算机视觉的方法，在假肢臂中实现腕关节自由度的连续控制，以促进更自然的接近目标物体并根据用户的意图进行抓握。&lt;h4&gt;方法&lt;/h4&gt;该系统利用用户与自动系统的协作，采用基于计算机视觉的技术来无缝控制假肢手腕。系统可以追踪目标对象，并最终按照用户意愿调整手腕姿态。&lt;h4&gt;主要发现&lt;/h4&gt;通过定量分析评估了每个系统组件的有效性，并在Hannes假肢臂上部署该方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的系统能够有效改善假肢的自然抓握能力，提供更好的用户体验。&lt;h4&gt;翻译&lt;/h4&gt;大多数用于假肢抓握的技术集中在灵巧的手指控制上，而忽视了手腕动作。这迫使用户通过肘部、肩部和臀部的动作来适应手腕的抓取需求。我们提出了一种基于计算机视觉的方法，在共享自主框架中结合用户与自动系统的合作，实现假肢腕关节自由度的连续控制，以促进更自然的接近目标物体并根据用户的意图进行抓握。我们的系统可以无缝地控制假肢手腕追踪目标对象，并最终按照用户意愿调整手腕姿态。我们通过定量分析评估了每个系统组件的有效性，并在Hannes假肢臂上部署该方法。代码和视频：https://hsp-iit.github.io/hannes-wrist-control.&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Most control techniques for prosthetic grasping focus on dexterous fingerscontrol, but overlook the wrist motion. This forces the user to performcompensatory movements with the elbow, shoulder and hip to adapt the wrist forgrasping. We propose a computer vision-based system that leverages thecollaboration between the user and an automatic system in a shared autonomyframework, to perform continuous control of the wrist degrees of freedom in aprosthetic arm, promoting a more natural approach-to-grasp motion. Our pipelineallows to seamlessly control the prosthetic wrist to follow the target objectand finally orient it for grasping according to the user intent. We assess theeffectiveness of each system component through quantitative analysis andfinally deploy our method on the Hannes prosthetic arm. Code and videos:https://hsp-iit.github.io/hannes-wrist-control.</description>
      <author>example@mail.com (Federico Vasile, Elisa Maiettini, Giulia Pasquale, Nicolò Boccardo, Lorenzo Natale)</author>
      <guid isPermaLink="false">2502.17265v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Tidiness Score-Guided Monte Carlo Tree Search for Visual Tabletop Rearrangement</title>
      <link>http://arxiv.org/abs/2502.17235v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 8 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的框架TSMCTS，该框架利用RGB-D相机解决桌面整理问题。&lt;h4&gt;背景&lt;/h4&gt;当前的桌面整理研究面临两大挑战：缺乏公开的数据集和基准以及难以指定未见物体的目标配置。&lt;h4&gt;目的&lt;/h4&gt;为了应对这些挑战，作者提出了一个结构化的数据集（TTU），并开发了一种基于视觉的判别器来预测整洁度分数，并结合蒙特卡洛树搜索算法寻找整理轨迹。&lt;h4&gt;方法&lt;/h4&gt;1. 创建了一个桌面整理数据集(TTU)；2. 利用该数据训练出一种能够评估不同配置下整洁度的基于视觉的判别器；3. 使用MCTS在不指定具体目标的情况下找到整理路径，并利用整洁度分数作为指导；4. 提出了TSMCTS，它将一个清洁度判断器与一个基于MCTS的整理规划器结合在一起。&lt;h4&gt;主要发现&lt;/h4&gt;提出的TSMCTS框架能够在多种环境中有效工作，包括咖啡桌、餐桌、办公桌和浴室等场景。&lt;h4&gt;结论&lt;/h4&gt;通过使用TTU数据集训练出的视觉判别器和MCTS算法相结合的方法能够有效地解决桌面整理问题，并且具有较高的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;在本文中，我们提出了一种基于整洁度分数指导的蒙特卡洛树搜索(TSMCTS)框架，旨在仅使用RGB-D相机来解决桌面上的清理问题。该研究解决了两个主要难题：缺乏公开数据集和基准以及难以指定未见物体的目标配置。为了解决第一个问题，我们提供了桌面整理（TTU）数据集，在模拟中收集了一个结构化的数据集。利用这个数据集，我们训练出一种基于视觉的判别器能够预测整洁度分数。这种判别器可以一致地评估未知配置下的整洁程度，包括真实世界中的场景。为了解决第二个问题，我们采用了蒙特卡洛树搜索(MCTS)方法在不指定明确目标的情况下寻找整理路径。而不是提供特定的目标，我们展示我们的MCTS基规划程序能够使用整洁度分数作为指导找到多样化的整理配置。因此，我们提出了TSMCTS，它将一个清洁度判断器与一个基于MCTS的整理规划器结合在一起以找到最优的整理排列。在咖啡桌、餐桌、办公桌和浴室等各种环境中，TSMCTS已经展示了其能力。TTU数据集可以在 https://github.com/rllab-snu/TTU-Dataset 获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present the tidiness score-guided Monte Carlo tree search(TSMCTS), a novel framework designed to address the tabletop tidying up problemusing only an RGB-D camera. We address two major problems for tabletop tidyingup problem: (1) the lack of public datasets and benchmarks, and (2) thedifficulty of specifying the goal configuration of unseen objects. We addressthe former by presenting the tabletop tidying up (TTU) dataset, a structureddataset collected in simulation. Using this dataset, we train a vision-baseddiscriminator capable of predicting the tidiness score. This discriminator canconsistently evaluate the degree of tidiness across unseen configurations,including real-world scenes. Addressing the second problem, we employ MonteCarlo tree search (MCTS) to find tidying trajectories without specifyingexplicit goals. Instead of providing specific goals, we demonstrate that ourMCTS-based planner can find diverse tidied configurations using the tidinessscore as a guidance. Consequently, we propose TSMCTS, which integrates atidiness discriminator with an MCTS-based tidying planner to find optimaltidied arrangements. TSMCTS has successfully demonstrated its capability acrossvarious environments, including coffee tables, dining tables, office desks, andbathrooms. The TTU dataset is available at:https://github.com/rllab-snu/TTU-Dataset.</description>
      <author>example@mail.com (Hogun Kee, Wooseok Oh, Minjae Kang, Hyemin Ahn, Songhwai Oh)</author>
      <guid isPermaLink="false">2502.17235v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>A Reinforcement Learning Approach to Non-prehensile Manipulation through Sliding</title>
      <link>http://arxiv.org/abs/2502.17221v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种针对非抓握式任务的深度确定性策略梯度（DDPG）强化学习框架，用于高效地在水平表面上滑动物体。该算法通过精确控制机械臂的加速度生成线性轨迹，实现了物体的相对操作。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数技术主要集中在基于抓握的操作上，这限制了它们在非抓握任务中的适用性。为了满足日益增长的需求，本文提出了一种新的方法来解决这个问题。&lt;h4&gt;目的&lt;/h4&gt;引入一种DDPG强化学习框架，以实现高效的非抓握操作，特别是在滑动物体方面的应用。&lt;h4&gt;方法&lt;/h4&gt;算法通过精确控制机器人臂的加速度生成线性轨迹，实现了物体在水平表面上滑动时的操作。此外还开发了两种不同的算法来动态估计滑动过程中的摩擦力。&lt;h4&gt;主要发现&lt;/h4&gt;提出的算法能够在线估算每次操作后的摩擦力，并将其反馈到演员模型中作为关键反馈，从而提高了策略的适应性和鲁棒性。实验结果表明该框架能够有效推广滑动物体的操作并适应不同表面。&lt;h4&gt;结论&lt;/h4&gt;所提出的方法通过模拟和实际实验验证了其有效性，展示了零样本仿真到现实环境的转移能力。&lt;h4&gt;翻译&lt;/h4&gt;虽然机器人应用程序越来越需要多功能和动态的对象处理，但大多数现有技术主要集中在基于抓握的操作上，限制了它们在非抓握任务中的适用性。为了解决这一需求，本文介绍了一种用于有效进行非抓取操作（特别是物体在水平表面上滑动）的深度确定性策略梯度（DDPG）强化学习框架。该算法通过精确控制与水平表面刚性连接的机器人臂加速度生成线性轨迹，实现了滑动物体时相对的操作。此外还开发了两种不同的算法来动态估算滑动过程中的摩擦力。这些算法在线提供了每次动作后的摩擦估计，并将其反馈到演员模型中作为关键反馈，增强了策略的适应性和鲁棒性，确保在不同表面条件下更精确地控制平台加速度。所提出的算法通过模拟和实际实验进行了验证。结果表明该框架能够有效推广滑动物体操作，并且特别能够在不同摩擦性质表面上进行调整。值得注意的是，训练后的模型展示了零样本仿真到现实环境的转移能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Although robotic applications increasingly demand versatile and dynamicobject handling, most existing techniques are predominantly focused ongrasp-based manipulation, limiting their applicability in non-prehensile tasks.To address this need, this study introduces a Deep Deterministic PolicyGradient (DDPG) reinforcement learning framework for efficient non-prehensilemanipulation, specifically for sliding an object on a surface. The algorithmgenerates a linear trajectory by precisely controlling the acceleration of arobotic arm rigidly coupled to the horizontal surface, enabling the relativemanipulation of an object as it slides on top of the surface. Furthermore, twodistinct algorithms have been developed to estimate the frictional forcesdynamically during the sliding process. These algorithms provide onlinefriction estimates after each action, which are fed back into the actor modelas critical feedback after each action. This feedback mechanism enhances thepolicy's adaptability and robustness, ensuring more precise control of theplatform's acceleration in response to varying surface condition. The proposedalgorithm is validated through simulations and real-world experiments. Resultsdemonstrate that the proposed framework effectively generalizes slidingmanipulation across varying distances and, more importantly, adapts todifferent surfaces with diverse frictional properties. Notably, the trainedmodel exhibits zero-shot sim-to-real transfer capabilities.</description>
      <author>example@mail.com (Hamidreza Raei, Elena De Momi, Arash Ajoudani)</author>
      <guid isPermaLink="false">2502.17221v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid Whole-Body Locomotion on Narrow Terrain via Dynamic Balance and Reinforcement Learning</title>
      <link>http://arxiv.org/abs/2502.17219v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work has been submitted to the IEEE for possible publication&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于动态平衡和强化学习的新型全身行走算法，使类人机器人能够仅通过本体感觉在极端地形上行走。&lt;h4&gt;背景&lt;/h4&gt;人类具有精细的动力平衡机制，能够在各种地形和极端条件下保持稳定。然而，现有的类人机器人步行算法难以应对缺乏外部感知（如视觉或激光雷达）的极端环境。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的全身行走算法来解决现有方法在处理不可见障碍物和突然失去平衡方面的能力不足问题。&lt;h4&gt;方法&lt;/h4&gt;引入了一个动态平衡机制，通过扩展的零力矩点(ZMP)驱动奖励和任务驱动奖励，在一个全身演员-评论家框架中实现上肢与下肢的协调动作。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该算法能够使机器人在极其狭窄的地面上保持平衡，并且对外部干扰具有适应性。&lt;h4&gt;结论&lt;/h4&gt;通过这种新型方法，类人机器人的行走能力得到了显著增强，使其能更好地应对复杂环境。&lt;h4&gt;翻译&lt;/h4&gt;人类拥有精细的动力平衡机制，在各种地形和极端条件下都能维持稳定。然而，尽管最近取得了重大进展，现有的类人机器人步行算法仍然难以穿越极端环境，尤其是在缺乏外部感知（如视觉或激光雷达）的情况下。这是因为当前的方法通常依赖于基于步态的或者需要特定感知条件的奖励策略，缺少有效处理不可见障碍物和突然失去平衡的有效机制。为了解决这一挑战，我们提出了一种新的全身行走算法，该算法基于动力平衡和强化学习（RL），使类人机器人能够仅通过本体感觉在极端地形上行走，特别是在狭窄路径和意外障碍的情况下。具体来说，我们引入了一个动态平衡机制，利用了扩展的零力矩点(ZMP)驱动奖励和任务驱动奖励，在一个全身演员-评论家框架中实现上下肢体的协调动作。全尺寸Unitree H1-2机器人的实验验证了该方法在极端狭窄地形上保持平衡以及对抗外部干扰的能力，证明了其增强机器人对复杂环境适应性的有效性。视频可以在https://whole-body-loco.github.io观看。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humans possess delicate dynamic balance mechanisms that enable them tomaintain stability across diverse terrains and under extreme conditions.However, despite significant advances recently, existing locomotion algorithmsfor humanoid robots are still struggle to traverse extreme environments,especially in cases that lack external perception (e.g., vision or LiDAR). Thisis because current methods often rely on gait-based or perception-conditionrewards, lacking effective mechanisms to handle unobservable obstacles andsudden balance loss. To address this challenge, we propose a novel whole-bodylocomotion algorithm based on dynamic balance and Reinforcement Learning (RL)that enables humanoid robots to traverse extreme terrains, particularly narrowpathways and unexpected obstacles, using only proprioception. Specifically, weintroduce a dynamic balance mechanism by leveraging an extended measure ofZero-Moment Point (ZMP)-driven rewards and task-driven rewards in a whole-bodyactor-critic framework, aiming to achieve coordinated actions of the upper andlower limbs for robust locomotion. Experiments conducted on a full-sizedUnitree H1-2 robot verify the ability of our method to maintain balance onextremely narrow terrains and under external disturbances, demonstrating itseffectiveness in enhancing the robot's adaptability to complex environments.The videos are given at https://whole-body-loco.github.io.</description>
      <author>example@mail.com (Weiji Xie, Chenjia Bai, Jiyuan Shi, Junkai Yang, Yunfei Ge, Weinan Zhang, Xuelong Li)</author>
      <guid isPermaLink="false">2502.17219v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Mechanical non-reciprocity programmed by shear jamming in soft composite solids</title>
      <link>http://arxiv.org/abs/2502.17083v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种通过利用颗粒物理中的剪切堵塞转换来设计非互易机械性的原理，从而在软复合固体中实现可调、方向依赖的不对称性。&lt;h4&gt;背景&lt;/h4&gt;传统的非互易性通常通过复杂结构非线性在超材料中实现。而具有内在非互易力学特性的连续体固体由于其潜在的应用前景（如波导、机器人技术及自适应材料）却未被充分探索。&lt;h4&gt;目的&lt;/h4&gt;引入一种设计原则，利用颗粒物理中的剪切堵塞转换来工程化软复合固体中的非互易机械性。&lt;h4&gt;方法&lt;/h4&gt;通过控制包含接触网络与基质弹性之间的相互作用，在软复合固体中实现了对剪切和正交力学响应的方向依赖的不对称性。结合响应性磁轮廓线，展示了程序化的非互易动力学特性。&lt;h4&gt;主要发现&lt;/h4&gt;实现了可调、方向相关的不对称性；展示了通过组合响应性磁图案和剪切堵塞系统的各向异性特性的程序化非互易动力学；以及在软材料中实现之前难以达到的不对称时空控制运动传输的能力。&lt;h4&gt;结论&lt;/h4&gt;此项工作为设计非互易物质开创了一个新的范例，将颗粒物理与软材料工程相结合以实现对机械智能系统至关重要的功能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Mechanical non-reciprocity-manifested as asymmetric responses to opposingmechanical stimuli-has traditionally been achieved through intricate structuralnonlinearities in metamaterials. However, continuum solids with inherentnon-reciprocal mechanics remain underexplored, despite their promisingpotential for applications such as wave guiding, robotics, and adaptivematerials. Here, we introduce a design principle by employing the shear jammingtransition from granular physics to engineering non-reciprocal mechanics insoft composite solids. Through the control of the interplay between inclusioncontact networks and matrix elasticity, we achieve tunable, direction-dependentasymmetry in both shear and normal mechanical responses. In addition to staticregimes, we demonstrate programmable non-reciprocal dynamics by combiningresponsive magnetic profiles with the anisotropic characteristics ofshear-jammed systems. This strategy enables asymmetric spatiotemporal controlover motion transmission, a previously challenging feat in soft materials. Ourwork establishes a novel paradigm for designing non-reciprocal matter, bridginggranular physics with soft material engineering to realize functionalitiesessential for mechano-intelligent systems.</description>
      <author>example@mail.com (Chang Xu, Shuaihu Wang, Hong Wang, Xu Liu, Zemin Liu, Yiqiu Zhao, Wenqi Hu, Qin Xu)</author>
      <guid isPermaLink="false">2502.17083v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Evolution 6.0: Evolving Robotic Capabilities Through Generative Design</title>
      <link>http://arxiv.org/abs/2502.17034v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Submitted to IROS&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出Evolution 6.0的概念，这是一种由生成式人工智能驱动的机器人技术进化。当机器人缺乏完成任务所需的工具时，它能够自主设计所需工具并学习如何使用它们来实现目标。&lt;h4&gt;背景&lt;/h4&gt;随着Generative AI的发展，机器人系统需要更加智能化和自适应以应对各种复杂任务的需求。&lt;h4&gt;目的&lt;/h4&gt;开发一种新的机器人系统Evolution 6.0，使其能够在没有事先准备的情况下完成人类请求的任务，并且自主设计必要的工具和学习使用方法。&lt;h4&gt;方法&lt;/h4&gt;该系统包括两个关键模块：工具生成模块和动作生成模块。前者利用视觉-语言模型（VLM）和3D工具生成器来根据任务需求设计并制造专用工具；后者则通过自然语言指令转为机器人行动。具体实现中采用了QwenVLM、OpenVLA和Llama-Mesh等技术。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，该系统在10秒内能以90%的成功率生成所需工具，并且83.5%的物理和视觉泛化能力，在动作生成方面表现良好。然而，在运动和语义泛化上还有待提高。&lt;h4&gt;结论&lt;/h4&gt;尽管初步结果令人鼓舞，但为了进一步提升其在现实世界中的适用性，未来的工作将重点放在双臂操作、扩展任务范围以及增强环境理解等方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要的原始英文内容&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a new concept, Evolution 6.0, which represents the evolution ofrobotics driven by Generative AI. When a robot lacks the necessary tools toaccomplish a task requested by a human, it autonomously designs the requiredinstruments and learns how to use them to achieve the goal. Evolution 6.0 is anautonomous robotic system powered by Vision-Language Models (VLMs),Vision-Language Action (VLA) models, and Text-to-3D generative models for tooldesign and task execution. The system comprises two key modules: the ToolGeneration Module, which fabricates task-specific tools from visual and textualdata, and the Action Generation Module, which converts natural languageinstructions into robotic actions. It integrates QwenVLM for environmentalunderstanding, OpenVLA for task execution, and Llama-Mesh for 3D toolgeneration. Evaluation results demonstrate a 90% success rate for toolgeneration with a 10-second inference time, and action generation achieving83.5% in physical and visual generalization, 70% in motion generalization, and37% in semantic generalization. Future improvements will focus on bimanualmanipulation, expanded task capabilities, and enhanced environmentalinterpretation to improve real-world adaptability.</description>
      <author>example@mail.com (Muhammad Haris Khan, Artyom Myshlyaev, Artyom Lykov, Miguel Altamirano Cabrera, Dzmitry Tsetserukou)</author>
      <guid isPermaLink="false">2502.17034v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Task-Oriented 6-DoF Grasp Pose Detection in Clutters</title>
      <link>http://arxiv.org/abs/2502.16976v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了任务导向的6自由度抓取姿态检测在杂乱环境中的问题，并构建了一个大规模的数据集以解决这一难题。提出了一种名为One-Stage TaskGrasp（OSTG）的方法，该方法采用了基于特定任务的对象点选择策略以及基于任务的手势生成模块来确定如何抓取。&lt;h4&gt;背景&lt;/h4&gt;人类会根据不同任务采取不同的抓握方式，例如刀具的把手用于切割而刀片则用于传递。现有的机器人抓握姿态检测研究在一定程度上考虑了这种任务导向性的问题，并且已经取得了一些进展；然而这些方法通常受制于低自由度夹爪类型或非杂乱设置。&lt;h4&gt;目的&lt;/h4&gt;为了解决更通用和实用的抓取模型问题，本文提出了一个名为Task-Oriented 6-DoF Grasp Pose Detection in Clutters（TO6DGC）的问题，并构造了一个大规模的数据集用于解决这一问题。&lt;h4&gt;方法&lt;/h4&gt;提出了OSTG方法，该方法采用了任务导向点选择策略来确定何处抓握，以及一种任务导向手势生成模块以决定如何抓取特定对象。实验是在构建的大型数据集上进行的。&lt;h4&gt;主要发现&lt;/h4&gt;研究结果表明，在多个指标下，本文提出的方法均优于现有基线；实际机器人实验也验证了OSTG方法在识别任务导向抓握点和6自由度抓握姿态上的优越性。&lt;h4&gt;结论&lt;/h4&gt;提出了一个处理杂乱环境中具有任务导向性的6自由度抓取问题的有效解决方案，并展示了其对于实现更通用且实用的机器人抓取模型的应用潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要中提到的是关于人类基于不同任务选择不同的抓握方式，研究者们在机器人抓取姿态检测领域进行了一些任务导向性的工作并取得了进展。然而这些工作大多数局限于低自由度夹爪或者非杂乱场景下，并不适用于现实生活中的辅助需求。本文旨在开发更通用和实际的应用模型，提出了一个名为“任务导向的6自由度抓取姿态检测在复杂环境下的问题”，并且构建了一个大规模的数据集来支持这一研究。此外还提出了一种名为One-Stage TaskGrasp（OSTG）的方法来解决这个问题，并通过大量实验验证了该方法的有效性及其对于实际应用的意义。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In general, humans would grasp an object differently for different tasks,e.g., "grasping the handle of a knife to cut" vs. "grasping the blade to handover". In the field of robotic grasp pose detection research, some existingworks consider this task-oriented grasping and made some progress, but they aregenerally constrained by low-DoF gripper type or non-cluttered setting, whichis not applicable for human assistance in real life. With an aim to get moregeneral and practical grasp models, in this paper, we investigate the problemnamed Task-Oriented 6-DoF Grasp Pose Detection in Clutters (TO6DGC), whichextends the task-oriented problem to a more general 6-DOF Grasp Pose Detectionin Cluttered (multi-object) scenario. To this end, we construct a large-scale6-DoF task-oriented grasping dataset, 6-DoF Task Grasp (6DTG), which features4391 cluttered scenes with over 2 million 6-DoF grasp poses. Each grasp isannotated with a specific task, involving 6 tasks and 198 objects in total.Moreover, we propose One-Stage TaskGrasp (OSTG), a strong baseline to addressthe TO6DGC problem. Our OSTG adopts a task-oriented point selection strategy todetect where to grasp, and a task-oriented grasp generation module to decidehow to grasp given a specific task. To evaluate the effectiveness of OSTG,extensive experiments are conducted on 6DTG. The results show that our methodoutperforms various baselines on multiple metrics. Real robot experiments alsoverify that our OSTG has a better perception of the task-oriented grasp pointsand 6-DoF grasp poses.</description>
      <author>example@mail.com (An-Lan Wang, Nuo Chen, Kun-Yu Lin, Li Yuan-Ming, Wei-Shi Zheng)</author>
      <guid isPermaLink="false">2502.16976v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Design of a low-cost and lightweight 6 DoF bimanual arm for dynamic and contact-rich manipulation</title>
      <link>http://arxiv.org/abs/2502.16908v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了ARMADA，一个专为动态操作研究设计的双臂机器人。它采用了低成本和易于组装的设计，能够执行打击、抢夺、锤击等复杂任务。&lt;h4&gt;背景&lt;/h4&gt;现有的大多数机器人由于硬件限制（如高惯性设计、有限的柔韧性以及对昂贵扭矩传感器的依赖）难以处理具有挑战性的动态接触操作任务。&lt;h4&gt;目的&lt;/h4&gt;开发一种经济且灵活的双臂机器人，用于动态手动动作的研究和实验。&lt;h4&gt;方法&lt;/h4&gt;ARMADA采用了低惯量、反驱能力强的执行器，并使用了轻质设计以及现成可用组件和3D打印链接。整个系统的成本仅为6100美元。&lt;h4&gt;主要发现&lt;/h4&gt;ARMADA可以达到每秒高达6.16米的速度，负载为2.5公斤；在真实环境中能完成诸如抢夺、锤击等动态操作任务，并且通过强化学习训练后可以在现实世界中进行零样本迁移。&lt;h4&gt;结论&lt;/h4&gt;ARMADA的开源性质提供了详细的组装指南、CAD模型和仿真代码，方便研究者使用。推荐观看补充视频以获得更多信息。&lt;h4&gt;翻译&lt;/h4&gt;该摘要介绍了ARMADA（用于操纵与动态动作的可承受机器人），一个专为动态操作研究设计的双臂机器人。它采用了低成本且易于在实验室中组装的设计，结合低惯量、反驱能力执行器和轻质结构，使用现成组件及3D打印链接构建。整个系统的成本仅为6100美元，每只手臂的速度可达每秒6.16米，与大多数协作机器人相比几乎快了一倍，负载为2.5公斤。展示的动态操作包括抢夺、锤击和双手抛掷等任务，并且在强化学习中表现出色：能够在模拟环境中训练非抓取式操纵策略并在现实世界中实现零样本迁移；以及通过人体动作阴影进行双臂物体抛掷研究。ARMADA是完全开源的，包含详细的组装说明、CAD模型、URDFs（Universal Robot Description Format）、仿真和学习代码。强烈推荐查看补充视频了解更多信息：https://sites.google.com/view/im2-humanoid-arm&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Dynamic and contact-rich object manipulation, such as striking, snatching, orhammering, remains challenging for robotic systems due to hardware limitations.Most existing robots are constrained by high-inertia design, limitedcompliance, and reliance on expensive torque sensors. To address this, weintroduce ARMADA (Affordable Robot for Manipulation and Dynamic Actions), a 6degrees-of-freedom bimanual robot designed for dynamic manipulation research.ARMADA combines low-inertia, back-drivable actuators with a lightweight design,using readily available components and 3D-printed links for ease of assembly inresearch labs. The entire system, including both arms, is built for just$6,100. Each arm achieves speeds up to 6.16m/s, almost twice that of mostcollaborative robots, with a comparable payload of 2.5kg. We demonstrate ARMADAcan perform dynamic manipulation like snatching, hammering, and bimanualthrowing in real-world environments. We also showcase its effectiveness inreinforcement learning (RL) by training a non-prehensile manipulation policy insimulation and transferring it zero-shot to the real world, as well as humanmotion shadowing for dynamic bimanual object throwing. ARMADA is fullyopen-sourced with detailed assembly instructions, CAD models, URDFs,simulation, and learning codes. We highly recommend viewing the supplementaryvideo at https://sites.google.com/view/im2-humanoid-arm.</description>
      <author>example@mail.com (Jaehyung Kim, Jiho Kim, Dongryung Lee, Yujin Jang, Beomjoon Kim)</author>
      <guid isPermaLink="false">2502.16908v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Gazing at Failure: Investigating Human Gaze in Response to Robot Failure in Collaborative Tasks</title>
      <link>http://arxiv.org/abs/2502.16899v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  this paper is accepted in HRI conference as a full paper&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究探讨了人类的非语言行为，特别是目光动态如何作为机器人故障的信号，并分析不同类型的机器人失败对人们感知机器人的影响。&lt;h4&gt;背景&lt;/h4&gt;机器人在与人类用户合作完成任务时可能会出现错误，这些错误会损害它们作为团队成员的信任度。检测和恢复此类故障对于维持有效的信任水平至关重要。&lt;h4&gt;目的&lt;/h4&gt;研究旨在通过分析人类的目光动态来检测机器人未察觉的失败，并探讨不同类型及时间发生的失败如何影响人们对机器人的看法。&lt;h4&gt;方法&lt;/h4&gt;进行了一项针对27名参与者的研究，他们与移动机械臂协作解决拼图游戏。机器人被编程以经历两种类型的故障——执行型和决策型，在任务开始或结束时发生，且有时会承认这些故障的存在。&lt;h4&gt;主要发现&lt;/h4&gt;研究发现显示，不同类型及时间发生的机器人的失败显著影响了参与者的目光行为和对机器人的看法。例如，执行性故障导致更多的目光转移以及更关注机器人；而决策性故障则在任务末尾时导致目标区域间的眼球运动熵降低。&lt;h4&gt;结论&lt;/h4&gt;研究表明，通过观察人类的目光动态可以作为识别机器人故障及其类型的可靠指标，并且这些信息可用来预测适当的恢复行动。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robots are prone to making errors, which can negatively impact theircredibility as teammates during collaborative tasks with human users. Detectingand recovering from these failures is crucial for maintaining effective levelof trust from users. However, robots may fail without being aware of it. Oneway to detect such failures could be by analysing humans' non-verbal behavioursand reactions to failures. This study investigates how human gaze dynamics cansignal a robot's failure and examines how different types of failures affectpeople's perception of robot. We conducted a user study with 27 participantscollaborating with a robotic mobile manipulator to solve tangram puzzles. Therobot was programmed to experience two types of failures -- executional anddecisional -- occurring either at the beginning or end of the task, with orwithout acknowledgement of the failure. Our findings reveal that the type andtiming of the robot's failure significantly affect participants' gaze behaviourand perception of the robot. Specifically, executional failures led to moregaze shifts and increased focus on the robot, while decisional failuresresulted in lower entropy in gaze transitions among areas of interest,particularly when the failure occurred at the end of the task. These resultshighlight that gaze can serve as a reliable indicator of robot failures andtheir types, and could also be used to predict the appropriate recoveryactions.</description>
      <author>example@mail.com (Ramtin Tabatabaei, Vassilis Kostakos, Wafa Johal)</author>
      <guid isPermaLink="false">2502.16899v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Variations of Augmented Lagrangian for Robotic Multi-Contact Simulation</title>
      <link>http://arxiv.org/abs/2502.16898v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一个新的多接触非线性互补问题（NCP）求解器系列，基于增广拉格朗日理论。&lt;h4&gt;背景&lt;/h4&gt;多接触非线性互补问题是机器人模拟中自然出现的挑战。在涉及密集接触和刚性相互作用的情况下，同时实现高精度和效率是一个重大难题。&lt;h4&gt;目的&lt;/h4&gt;介绍一类新的多接触NCP求解器，并展示其在机器人模拟中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;通过迭代替代问题解决方案并更新原始-对偶变量的方式，将传统的增广拉格朗日理论应用于处理多接触NCP。提出了两种针对机器人仿真的特定变体：级联牛顿增广拉格朗日（CANAL）和基于子系统的交替方向乘子法（SubADMM）。&lt;h4&gt;主要发现&lt;/h4&gt;CANAL能够准确且稳健地管理多接触NCP，而SubADMM则提供了计算速度、可扩展性和并行处理能力方面的优势。&lt;h4&gt;结论&lt;/h4&gt;所提出的求解器框架在各种机器人操纵场景中展示了其有效性，特别是在高自由度和大量接触点的多体系统中表现出了显著的优势。&lt;h4&gt;翻译&lt;/h4&gt;摘要中的描述已经被中文内容替代，作为对原文本信息的理解与总结，无需额外添加此键值对。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The multi-contact nonlinear complementarity problem (NCP) is a naturallyarising challenge in robotic simulations. Achieving high performance in termsof both accuracy and efficiency remains a significant challenge, particularlyin scenarios involving intensive contacts and stiff interactions. In thisarticle, we introduce a new class of multi-contact NCP solvers based on thetheory of the Augmented Lagrangian (AL). We detail how the standard derivationof AL in convex optimization can be adapted to handle multi-contact NCP throughthe iteration of surrogate problem solutions and the subsequent update ofprimal-dual variables. Specifically, we present two tailored variations of ALfor robotic simulations: the Cascaded Newton-based Augmented Lagrangian (CANAL)and the Subsystem-based Alternating Direction Method of Multipliers (SubADMM).We demonstrate how CANAL can manage multi-contact NCP in an accurate and robustmanner, while SubADMM offers superior computational speed, scalability, andparallelizability for high degrees-of-freedom multibody systems with numerouscontacts. Our results showcase the effectiveness of the proposed solverframework, illustrating its advantages in various robotic manipulationscenarios.</description>
      <author>example@mail.com (Jeongmin Lee, Minji Lee, Sunkyung Park, Jinhee Yun, Dongjun Lee)</author>
      <guid isPermaLink="false">2502.16898v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Primitive-Planner: An Ultra Lightweight Quadrotor Planner with Time-optimal Primitives</title>
      <link>http://arxiv.org/abs/2502.16882v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Technical Report&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种超轻量级四旋翼飞行器轨迹规划方法，该方法利用时间最优的基本运动单元，在保证轨迹质量和系统轻量化的同时实现高效率。&lt;h4&gt;背景&lt;/h4&gt;目前许多研究致力于解决四旋翼飞行器同时满足高质量轨迹和低计算负载的问题，但现有的解决方案与实际需求之间仍存在差距。&lt;h4&gt;目的&lt;/h4&gt;提出一种超轻量级的四旋翼飞行器规划方法，以实现实时高效、碰撞安全且用户可定制的最优路径规划。&lt;h4&gt;方法&lt;/h4&gt;{'1': '设计了一种新的运动基本单元库，用于生成时间最优化和动力学可行性的轨迹，并实现离线计算。', '2': '提出了一种快速的碰撞检测算法，该算法具有确定的时间消耗且与采样分辨率无关。', '3': '根据用户的定义需求从安全的基本运动中选择最低成本的路径进行执行。', '4': '利用局部轨迹之间的转换关系来确保全局轨迹的平滑性。'}&lt;h4&gt;主要发现&lt;/h4&gt;{'1': '该方法能生成最短飞行时间和距离且计算负担最小化的最优轨迹。', '2': '现实世界中的挑战实验验证了所提方法在各种环境下的鲁棒性。'}&lt;h4&gt;结论&lt;/h4&gt;提出的规划方法通过减少在线计算的功率消耗，同时确保高质量的轨迹，并展示了其在实际应用中优越的时间效率和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; It is a significant requirement for a quadrotor trajectory planner tosimultaneously guarantee trajectory quality and system lightweight. Manyresearchers focus on this problem, but there's still a gap between theirperformance and our common wish. In this paper, we propose an ultra lightweightquadrotor planner with time-optimal primitives. Firstly, a novel motionprimitive library is proposed to generate time-optimal and dynamical feasibletrajectories offline. Secondly, we propose a fast collision checking methodwith a deterministic time consumption, independent of the sampling resolutionof the primitives. Finally, we select the minimum cost trajectory to executeamong the safe primitives based on user-defined requirements. The propsedtransformation relation between the local trajectories ensures the smoothnessof the global trajectory. The planner reduces unnecessary online computingpower consumption as much as possible, while ensuring a high-qualitytrajectory. Benchmark comparisons show that our method can generate theshortest flight time and distance of trajectory with the lowest computationoverload. Challenging real-world experiments validate the robustness of ourmethod.</description>
      <author>example@mail.com (Jialiang Hou, Neng Pan, Zhepei Wang, Jialin Ji, Yuxiang Guan, Zhongxue Gan, Fei Gao)</author>
      <guid isPermaLink="false">2502.16882v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Fast Finite-Time Sliding Mode Control for Chattering-Free Trajectory Tracking of Robotic Manipulators</title>
      <link>http://arxiv.org/abs/2502.16867v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;提出了一种无颤振快速终端滑模控制（FTSMC）策略，用于提高三自由度机械臂的轨迹跟踪精度和鲁棒性。&lt;h4&gt;背景&lt;/h4&gt;传统滑模控制由于系统不确定性及抖动效应，在实现高精度高效轨迹追踪方面仍面临挑战。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，提出了一种新型无颤振快速终端滑模控制策略，以增强机械臂的跟踪精度和稳定性，并确保有限时间内收敛。&lt;h4&gt;方法&lt;/h4&gt;采用牛顿-欧拉动力学建立控制框架并转化为状态空间表示形式。通过引入改进后的滑动面以及基于李雅普诺夫稳定性的分析来设计控制器，从而减少颤振同时保持了传统滑模控制的优点如快速响应和强大的抗干扰能力。&lt;h4&gt;主要发现&lt;/h4&gt;仿真结果显示该策略在轨迹跟踪性能、更快的收敛速度及更强稳定性方面优于传统的PD滑模控制（PDSMC）和终端滑模控制（TSMC），尤其适用于高精度机器人应用。&lt;h4&gt;结论&lt;/h4&gt;提出的FTSMC方法为解决机械臂精确轨迹追踪中的挑战提供了有前景的解决方案，展示了其在实现高性能、快速响应及稳健控制方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;实现机器人手臂的精确且高效的轨迹跟踪依然是一个关键难题，因为传统滑模控制（SMC）面临系统不确定性和抖动效应的问题。本文提出了一种无颤振快速终端滑模控制（FTSMC）策略用于三自由度（3-DOF）机械臂设计中，旨在增强其跟踪精度和鲁棒性，并确保有限时间内的收敛。该控制系统框架基于牛顿-欧拉动力学建立并进一步转化为状态空间表示形式以捕捉系统的角位移与速度信息。通过采用改进的滑动面以及李雅普诺夫稳定性分析为基础的设计方法，所提出的FTSMC有效减轻了颤振现象，同时保留了传统滑模控制的优点，如快速响应和强大的干扰抑制能力。经过与常规PD滑模控制（PDSMC）及终端滑模控制（TSMC）的对比实验严格评估控制器性能后发现，本论文提出的方法在轨迹跟踪性能、更快的收敛速度以及更强稳定性方面都优于现有方法，为高精度机器人应用提供了有前景的解决方案。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Achieving precise and efficient trajectory tracking in robotic arms remains akey challenge due to system uncertainties and chattering effects inconventional sliding mode control (SMC). This paper presents a chattering-freefast terminal sliding mode control (FTSMC) strategy for athree-degree-of-freedom (3-DOF) robotic arm, designed to enhance trackingaccuracy and robustness while ensuring finite-time convergence. The controlframework is developed using Newton-Euler dynamics, followed by a state-spacerepresentation that captures the system's angular position and velocity. Byincorporating an improved sliding surface and a Lyapunov-based stabilityanalysis, the proposed FTSMC effectively mitigates chattering while preservingthe advantages of SMC, such as fast response and strong disturbance rejection.The controller's performance is rigorously evaluated through comparisons withconventional PD sliding mode control (PDSMC) and terminal sliding mode control(TSMC). Simulation results demonstrate that the proposed approach achievessuperior trajectory tracking performance, faster convergence, and enhancedstability compared to existing methods, making it a promising solution forhigh-precision robotic applications.</description>
      <author>example@mail.com (Momammad Ali Ranjbar)</author>
      <guid isPermaLink="false">2502.16867v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>SLABIM: A SLAM-BIM Coupled Dataset in HKUST Main Building</title>
      <link>http://arxiv.org/abs/2502.16856v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICRA 2025. Dataset aviliable at  https://github.com/HKUST-Aerial-Robotics/SLABIM . Video attachment at  https://youtu.be/7NckgY15ABQ&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;介绍了一种新的数据集SLABIM，该数据集结合了室内定位和建筑信息模型（BIM），旨在解决现有室内SLAM数据集中缺乏建筑物结构信息的问题。&lt;h4&gt;背景&lt;/h4&gt;现有的室内SLAM数据集主要关注机器人传感器的数据采集，较少包含详细的建筑结构信息。这种不足限制了研究者们对真实场景中SLAM算法性能的全面评估和优化。&lt;h4&gt;目的&lt;/h4&gt;设计并建立首个结合BIM与SLAM技术的公开数据集SLABIM，用于促进室内定位系统的研究进展，并提高其在实际环境中的应用效果。&lt;h4&gt;方法&lt;/h4&gt;该数据集基于香港科技大学的一栋大学建筑进行构建。首先建立了详细的BIM模型；然后通过多传感器套件采集真实场景下的数据，并生成施工后模型（As-Built Model）；最后，所有数据均进行了时间戳标注并组织成易于访问的形式。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果展示了SLABIM在三个关键任务上的性能表现：注册、定位以及语义地图构建。这些测试验证了该数据集的有效性和实用性。&lt;h4&gt;结论&lt;/h4&gt;通过发布开源的SLABIM数据集，研究者们可以更好地进行室内定位算法的研究，并且能够更有效地评估和改进相关技术的应用效果。&lt;h4&gt;翻译&lt;/h4&gt;现有室内同步定位与建图（SLAM）数据集主要关注机器人感知信息，缺乏对建筑结构的关注。为弥补这一不足，我们设计并构建了首个将SLAM和BIM相结合的数据集——SLABIM。该数据集提供了针对大学建筑的传感器数据，并将其分解、转换成易于使用的格式。通过多传感器套件采集多个会话下的数据及地图制作，我们获取到实际施工模型。所有相关数据均被时间戳标记并组织好，方便用户部署和测试使用。此外，我们部署了先进方法并在注册、定位和语义映射三项任务上报告实验结果，证明SLABIM的有效性和实用性。该数据集已在https://github.com/HKUST-Aerial-Robotics/SLABIM开放源代码。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Existing indoor SLAM datasets primarily focus on robot sensing, often lackingbuilding architectures. To address this gap, we design and construct the firstdataset to couple the SLAM and BIM, named SLABIM. This dataset provides BIM andSLAM-oriented sensor data, both modeling a university building at HKUST. Theas-designed BIM is decomposed and converted for ease of use. We employ amulti-sensor suite for multi-session data collection and mapping to obtain theas-built model. All the related data are timestamped and organized, enablingusers to deploy and test effectively. Furthermore, we deploy advanced methodsand report the experimental results on three tasks: registration, localizationand semantic mapping, demonstrating the effectiveness and practicality ofSLABIM. We make our dataset open-source athttps://github.com/HKUST-Aerial-Robotics/SLABIM.</description>
      <author>example@mail.com (Haoming Huang, Zhijian Qiao, Zehuan Yu, Chuhao Liu, Shaojie Shen, Fumin Zhang, Huan Yin)</author>
      <guid isPermaLink="false">2502.16856v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Characterizing Structured versus Unstructured Environments based on Pedestrians' and Vehicles' Motion Trajectories</title>
      <link>http://arxiv.org/abs/2502.16847v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文通过分析不同环境类型中行人和车辆的轨迹特征，提出了一种区分结构化和非结构化环境的方法，并利用K-means聚类和广义线性模型对现有数据集进行了分类。&lt;h4&gt;背景&lt;/h4&gt;行人在接近车辆运行时的行为在无结构环境中与有结构环境中存在差异。然而，现有的行人和车辆的轨迹数据集并未根据它们所处环境的性质进行分类，且关于无结构和有结构环境的定义难以量化。&lt;h4&gt;目的&lt;/h4&gt;开发一种更定量的方法来区分不同类型的行进环境，并为现有数据集提供一个基于环境特性的分类。&lt;h4&gt;方法&lt;/h4&gt;采用从各种轨迹中提取出来的特征（如平均速度、轨迹变化率）进行分析，利用K-means聚类和广义线性模型对这些特征进行处理以量化不同环境类型之间的差异。&lt;h4&gt;主要发现&lt;/h4&gt;行人轨迹的变异性和停车频率以及行人的密度在两种类型的环境中表现出明显的不同，可以用来分类现有的数据集。&lt;h4&gt;结论&lt;/h4&gt;通过对现有数据集中行人与车辆轨迹行为的研究，提出了一种区分结构化和非结构化环境的新方法。这种方法有助于改善自动驾驶汽车的行为预测算法。&lt;h4&gt;翻译&lt;/h4&gt;行人在接近车辆运行时的行为在无结构环境中（如繁忙的街道）与有结构环境中（如专用的人行道或公园）存在差异。这种行为上的差异对于开发适用于自动行驶车辆的轨迹预测算法非常重要，因为现有的行人和车辆的轨迹数据集并未根据它们所处环境的性质进行分类，并且关于非结构化和结构化环境的标准定义通常难以量化。本文通过分析不同数据集中提取出的各种特征（例如平均速度、路径变化率），应用K-means聚类与广义线性模型，提出了一种新方法来区分这些不同的环境类型。研究结果表明，行人轨迹的变异性和停止频率以及行人的密度在两种类型的环境中显著不同，可以用于现有数据集的分类。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/ITSC55140.2022.9921899&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Trajectory behaviours of pedestrians and vehicles operating close to eachother can be different in unstructured compared to structured environments.These differences in the motion behaviour are valuable to be considered in thetrajectory prediction algorithm of an autonomous vehicle. However, theavailable datasets on pedestrians' and vehicles' trajectories that are commonlyused as benchmarks for trajectory prediction have not been classified based onthe nature of their environment. On the other hand, the definitions providedfor unstructured and structured environments are rather qualitative and hard tobe used for justifying the type of a given environment. In this paper, we havecompared different existing datasets based on a couple of extracted trajectoryfeatures, such as mean speed and trajectory variability. Through K-meansclustering and generalized linear models, we propose more quantitative measuresfor distinguishing the two different types of environments. Our results showthat features such as trajectory variability, stop fraction and density ofpedestrians are different among the two environmental types and can be used toclassify the existing datasets.</description>
      <author>example@mail.com (Mahsa Golchoubian, Moojan Ghafurian, Nasser Lashgarian Azad, Kerstin Dautenhahn)</author>
      <guid isPermaLink="false">2502.16847v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>Online Friction Coefficient Identification for Legged Robots on Slippery Terrain Using Smoothed Contact Gradients</title>
      <link>http://arxiv.org/abs/2502.16843v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, IEEE RA-L (2025) accepted&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种在线识别腿足机器人在滑坡地形上摩擦系数的框架，通过最小化实际状态和预测状态之间的残差来解决这一问题。&lt;h4&gt;背景&lt;/h4&gt;在腿足机器人的动态环境下，尤其是在滑坡地形上，准确地估计地面摩擦力对机器人的稳定性和效率至关重要。传统的摩擦系数识别方法在面对非光滑接触动力学时会产生无信息梯度。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于刚体接触动力学的优化问题框架，通过引入平滑处理后的互补条件下的库仑摩擦来解决非光滑接触动力学带来的挑战，并利用拒绝法过滤掉不适宜的数据。&lt;h4&gt;方法&lt;/h4&gt;该框架将优化问题参数化为最小化实际状态和预测状态之间的残差之和。利用了分析的光滑梯度，解决了由非平滑接触动力学引发的无信息梯度的问题。&lt;h4&gt;主要发现&lt;/h4&gt;实验显示，在各种初始条件下，所提出的框架能够快速且一致地识别出摩擦系数，并在使用四足机器人平台KAIST HOUND进行实验时验证了该框架的有效性。&lt;h4&gt;结论&lt;/h4&gt;这项工作提供了一种有效的在线方法来估计腿足机器人的摩擦系数，从而为实际应用中的运动规划和控制奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;摘要的中文翻译&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-24&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1109/LRA.2025.3541428&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper proposes an online friction coefficient identification frameworkfor legged robots on slippery terrain. The approach formulates the optimizationproblem to minimize the sum of residuals between actual and predicted statesparameterized by the friction coefficient in rigid body contact dynamics.Notably, the proposed framework leverages the analytic smoothed gradient ofcontact impulses, obtained by smoothing the complementarity condition ofCoulomb friction, to solve the issue of non-informative gradients induced fromthe nonsmooth contact dynamics. Moreover, we introduce the rejection method tofilter out data with high normal contact velocity following contact initiationsduring friction coefficient identification for legged robots. To validate theproposed framework, we conduct the experiments using a quadrupedal robotplatform, KAIST HOUND, on slippery and nonslippery terrain. We observe that ourframework achieves fast and consistent friction coefficient identificationwithin various initial conditions.</description>
      <author>example@mail.com (Hajun Kim, Dongyun Kang, Min-Gyu Kim, Gijeong Kim, Hae-Won Park)</author>
      <guid isPermaLink="false">2502.16843v1</guid>
      <pubDate>Tue, 25 Feb 2025 19:23:13 +0800</pubDate>
    </item>
    <item>
      <title>NavigateDiff: Visual Predictors are Zero-Shot Navigation Assistants</title>
      <link>http://arxiv.org/abs/2502.13894v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted to ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的导航方法，通过将大型视觉-语言模型与扩散网络结合来实现零样本学习环境下的机器人导航。这种方法利用预训练的基础模型进行知识迁移和泛化能力的传递。&lt;h4&gt;背景&lt;/h4&gt;家庭机器人在陌生环境中面临挑战，需要能够识别并推理关于新装饰和布局的信息。现有的强化学习方法无法直接应用于新的环境，因为它们通常依赖于广泛的映射和探索，导致耗时且效率低下。&lt;h4&gt;目的&lt;/h4&gt;为了克服这些挑战，论文试图将预训练基础模型的逻辑知识和泛化能力转移到零样本导航中。&lt;h4&gt;方法&lt;/h4&gt;通过结合大型视觉-语言模型与扩散网络构建了一个视觉预测器，能够连续预测代理人在下一步可能观察到的内容。此外，为适应导航的时间特性，引入了历史时间信息以确保预测图像与导航场景对齐。最后设计了一种信息融合框架，将预测的未来帧嵌入目标导向策略中。&lt;h4&gt;主要发现&lt;/h4&gt;该方法在模拟和真实环境中增强了导航控制，并展示了其强大的鲁棒性和通用性。&lt;h4&gt;结论&lt;/h4&gt;通过广泛的实验验证了所提出方法的有效性和效率，展现了它改善机器人在各种场景下导航能力的巨大潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Navigating unfamiliar environments presents significant challenges forhousehold robots, requiring the ability to recognize and reason about noveldecoration and layout. Existing reinforcement learning methods cannot bedirectly transferred to new environments, as they typically rely on extensivemapping and exploration, leading to time-consuming and inefficient. To addressthese challenges, we try to transfer the logical knowledge and thegeneralization ability of pre-trained foundation models to zero-shotnavigation. By integrating a large vision-language model with a diffusionnetwork, our approach named \mname ~constructs a visual predictor thatcontinuously predicts the agent's potential observations in the next step whichcan assist robots generate robust actions. Furthermore, to adapt the temporalproperty of navigation, we introduce temporal historical information to ensurethat the predicted image is aligned with the navigation scene. We thencarefully designed an information fusion framework that embeds the predictedfuture frames as guidance into goal-reaching policy to solve downstream imagenavigation tasks. This approach enhances navigation control and generalizationacross both simulated and real-world environments. Through extensiveexperimentation, we demonstrate the robustness and versatility of our method,showcasing its potential to improve the efficiency and effectiveness of roboticnavigation in diverse settings.</description>
      <author>example@mail.com (Yiran Qin, Ao Sun, Yuze Hong, Benyou Wang, Ruimao Zhang)</author>
      <guid isPermaLink="false">2502.13894v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
  <item>
      <title>Appeal prediction for AI up-scaled Images</title>
      <link>http://arxiv.org/abs/2502.14013v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该研究通过开发一个包含136个基础图像和五种不同上采样方法的全面数据集，评估了各种深度学习模型在图像上采样任务中的表现。&lt;h4&gt;背景&lt;/h4&gt;基于DNN或AI的上采样算法由于机器学习的进步变得越来越流行。然而，缺乏对真实世界图像范围广泛且包含主观评价的表现评测。&lt;h4&gt;目的&lt;/h4&gt;填补现有研究空白，通过对大量实际图像进行主观和客观评估来比较不同上采样方法的效果，并训练用于检测这些方法的深度学习模型。&lt;h4&gt;方法&lt;/h4&gt;使用136个基础图像和五种不同的上采样方法（Real-ESRGAN, BSRGAN, waifu2x, KXNet以及Lanczos）构建数据集，总共包含1496张注释图像。通过众包服务进行主观评价，并开发了开源工具AVRate Voyager来辅助标注。&lt;h4&gt;主要发现&lt;/h4&gt;在主观评价中，Real-ESRGAN和BSRGAN表现最佳。训练的深度神经网络能够有效地检测不同的上采样方法。同时评估了现有最先进的图像吸引力和质量模型的表现，结果表明这些模型预测性能不高，并因此开发了自己的两种新模型以改进性能。&lt;h4&gt;结论&lt;/h4&gt;研究证明了主观评价的重要性以及对广泛数据集进行详细评测的价值，并通过提供开源工具与数据促进了该领域的进一步研究。&lt;h4&gt;翻译&lt;/h4&gt;摘要：基于深度神经网络或人工智能的上采样算法由于机器学习的进步而变得越来越流行。使用卷积神经网络、生成对抗网络或者混合方法的各种上采样模型已发表。大多数模型仅利用PSNR和SSIM进行评估，或是通过少量示例图像来进行评价。然而，缺乏广泛的现实世界图像以及主观评价的表现评测，这是我们研究论文要解决的问题。为此，我们描述了开发的数据集，该数据集使用136个基础图像并采用五种不同的上采样方法，即Real-ESRGAN、BSRGAN、waifu2x、KXNet和Lanczos。总体而言，整个数据集中包含有1496张注释的图像。我们的数据集标注专注于图像吸引力，并使用众包工具AVRate Voyager进行操作。我们评估了不同方法在吸引力上的表现，结果显示Real-ESRGAN和BSRGAN是最好的。此外，我们也训练了一个深度神经网络来检测用于上采样的哪种方法，在评估中这些模型表现出较好的总体性能。除此之外，还对现有最先进的图像吸引力与质量模型进行了评估，结果表明没有一个模型显示出高的预测性能，因此我们又开发了两个自己的方法。第一个采用迁移学习并具有最佳表现，第二个模型则使用信号基特征和随机森林模型并具备整体优秀的性能。为促进开放科学领域的进一步研究，我们将数据集、标注工具及实现方法对外公开分享。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-19&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; DNN- or AI-based up-scaling algorithms are gaining in popularity due to theimprovements in machine learning. Various up-scaling models using CNNs, GANs ormixed approaches have been published. The majority of models are evaluatedusing PSRN and SSIM or only a few example images. However, a performanceevaluation with a wide range of real-world images and subjective evaluation ismissing, which we tackle in the following paper. For this reason, we describeour developed dataset, which uses 136 base images and five different up-scalingmethods, namely Real-ESRGAN, BSRGAN, waifu2x, KXNet, and Lanczos. Overall thedataset consists of 1496 annotated images. The labeling of our dataset focusedon image appeal and has been performed using crowd-sourcing employing ouropen-source tool AVRate Voyager. We evaluate the appeal of the differentmethods, and the results indicate that Real-ESRGAN and BSRGAN are the best.Furthermore, we train a DNN to detect which up-scaling method has been used,the trained models have a good overall performance in our evaluation. Inaddition to this, we evaluate state-of-the-art image appeal and quality models,here none of the models showed a high prediction performance, therefore we alsotrained two own approaches. The first uses transfer learning and has the bestperformance, and the second model uses signal-based features and a randomforest model with good overall performance. We share the data andimplementation to allow further research in the context of open science.</description>
      <author>example@mail.com (Steve Göring, Rasmus Merten, Alexander Raake)</author>
      <guid isPermaLink="false">2502.14013v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Game State and Spatio-temporal Action Detection in Soccer using Graph Neural Networks and 3D Convolutional Networks</title>
      <link>http://arxiv.org/abs/2502.15462v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种结合视觉信息和比赛状态信息的时空动作检测方法，通过图神经网络与先进的3D卷积神经网络相结合，实现了更好的性能。&lt;h4&gt;背景&lt;/h4&gt;足球数据分析依赖于两种数据源：球员在场上的位置以及他们执行的动作序列。大约每场比赛有2000个球事件需要进行精确和详尽的标注，这是一项繁琐且成本高昂的手动任务。&lt;h4&gt;目的&lt;/h4&gt;为了减少人工注释的工作量并提高自动化程度，本文旨在探索结合视觉信息和比赛状态信息的方法来改进动作检测算法。&lt;h4&gt;方法&lt;/h4&gt;假设职业球员的行为是相互依赖的，并认为加入周围球员的信息（例如位置、速度和团队归属）可以增强纯视觉预测。为此，作者提出了一种基于图神经网络与3D卷积神经网络相结合的方法。&lt;h4&gt;主要发现&lt;/h4&gt;通过将比赛状态信息整合进模型中，该方法展示了改进后的性能指标。&lt;h4&gt;结论&lt;/h4&gt;结合视觉和游戏状态数据能够提高时空动作检测的准确性，并为自动化足球分析提供了一个有前景的方向。&lt;h4&gt;翻译&lt;/h4&gt;摘要：足球数据分析依赖于两种数据源：球员在场上的位置以及他们执行的动作序列。大约每场比赛有2000个球事件需要进行精确且详尽的手动注释，这是一项繁琐和成本高昂的任务。尽管最先进的时空动作检测方法显示出自动化此任务的前景，但它们缺乏对游戏上下文的理解。假设职业球员的行为是相互依赖的，我们假定将周围球员的信息（如位置、速度及团队归属）加入到纯粹视觉预测中可以增强其准确性。我们提出了一种结合图神经网络与最先进的3D卷积神经网络相结合的方法，展示了通过整合比赛状态信息而改进的性能指标。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Soccer analytics rely on two data sources: the player positions on the pitchand the sequences of events they perform. With around 2000 ball events pergame, their precise and exhaustive annotation based on a monocular video streamremains a tedious and costly manual task. While state-of-the-artspatio-temporal action detection methods show promise for automating this task,they lack contextual understanding of the game. Assuming professional players'behaviors are interdependent, we hypothesize that incorporating surroundingplayers' information such as positions, velocity and team membership canenhance purely visual predictions. We propose a spatio-temporal actiondetection approach that combines visual and game state information via GraphNeural Networks trained end-to-end with state-of-the-art 3D CNNs, demonstratingimproved metrics through game state integration.</description>
      <author>example@mail.com (Jeremie Ochin, Guillaume Devineau, Bogdan Stanciulescu, Sotiris Manitsaris)</author>
      <guid isPermaLink="false">2502.15462v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Mantis: Lightweight Calibrated Foundation Model for User-Friendly Time Series Classification</title>
      <link>http://arxiv.org/abs/2502.15637v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;近年来，人们对开发能够跨多种下游任务泛化的时序数据基础模型产生了浓厚的兴趣。虽然已经引入了大量面向预测的基础模型，但针对时间序列分类的专门模型却相对稀缺。&lt;h4&gt;目的&lt;/h4&gt;为了弥补这一空白，我们提出了一种新的开源基础模型Mantis，该模型基于视觉变换器（ViT）架构，并通过对比学习方法进行预训练，以用于时间序列分类任务。&lt;h4&gt;方法&lt;/h4&gt;Mantis模型采用了对比学习方法来进行预训练。此外，还提出了几个适配器来处理多变量设置，这可以减少内存需求并建模通道间的相互依赖性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，无论是在骨干网络被冻结的情况下还是在经过微调之后，Mantis都优于现有的基础模型，并且实现了最低的校准误差。&lt;h4&gt;结论&lt;/h4&gt;Mantis为时间序列分类提供了一种有效的解决方案，同时通过引入适配器机制处理多变量设置问题，进一步优化了性能和内存使用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, there has been increasing interest in developing foundationmodels for time series data that can generalize across diverse downstreamtasks. While numerous forecasting-oriented foundation models have beenintroduced, there is a notable scarcity of models tailored for time seriesclassification. To address this gap, we present Mantis, a new open-sourcefoundation model for time series classification based on the Vision Transformer(ViT) architecture that has been pre-trained using a contrastive learningapproach. Our experimental results show that Mantis outperforms existingfoundation models both when the backbone is frozen and when fine-tuned, whileachieving the lowest calibration error. In addition, we propose severaladapters to handle the multivariate setting, reducing memory requirements andmodeling channel interdependence.</description>
      <author>example@mail.com (Vasilii Feofanov, Songkang Wen, Marius Alonso, Romain Ilbert, Hongbo Guo, Malik Tiomoko, Lujia Pan, Jianfeng Zhang, Ievgen Redko)</author>
      <guid isPermaLink="false">2502.15637v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>A Universal Framework for Compressing Embeddings in CTR Prediction</title>
      <link>http://arxiv.org/abs/2502.15355v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by DASFAA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种模型无关的嵌入压缩框架（MEC），用于压缩点击率预测中的嵌入表，以减少内存使用和延迟，同时保持推荐质量。&lt;h4&gt;背景&lt;/h4&gt;准确的点击率预测对在线广告和推荐系统至关重要。虽然深度学习技术进步提高了捕捉特征交互和理解用户兴趣的能力，但是优化嵌入层仍常常被忽视。大型嵌入表会超过GPU内存限制，并且需要存储在CPU内存中，导致高内存消耗和频繁的数据传输延迟。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的压缩方法来解决现有推荐系统中的内存使用量过大和延迟问题。&lt;h4&gt;方法&lt;/h4&gt;1. 应用流行度加权正则化以平衡高频和低频特征的代码分布。2. 集成对比学习机制，确保量化码的均匀分布，提高嵌入的独特性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，在三个数据集上，我们的方法能够将内存使用量减少超过50倍，并且保持或提高了推荐性能。&lt;h4&gt;结论&lt;/h4&gt;提出的MEC框架可以有效地压缩嵌入表，降低内存消耗和延迟，同时不牺牲推荐质量。这为构建高效、大规模的推荐系统提供了一个新的途径。&lt;h4&gt;翻译&lt;/h4&gt;准确点击率预测对在线广告与推荐系统至关重要。近期深度学习的进步提高了捕捉特征交互及理解用户兴趣的能力，但优化嵌入层常被忽视。本文提出了一种模型无关的嵌入压缩（MEC）框架，通过量化预训练嵌入来压缩嵌入表，在不牺牲推荐质量的前提下减少内存使用和延迟。实验表明该方法在三个数据集上可以将内存消耗降低50倍以上，并保持或提高推荐性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Accurate click-through rate (CTR) prediction is vital for online advertisingand recommendation systems. Recent deep learning advancements have improved theability to capture feature interactions and understand user interests. However,optimizing the embedding layer often remains overlooked. Embedding tables,which represent categorical and sequential features, can become excessivelylarge, surpassing GPU memory limits and necessitating storage in CPU memory.This results in high memory consumption and increased latency due to frequentGPU-CPU data transfers. To tackle these challenges, we introduce aModel-agnostic Embedding Compression (MEC) framework that compresses embeddingtables by quantizing pre-trained embeddings, without sacrificing recommendationquality. Our approach consists of two stages: first, we applypopularity-weighted regularization to balance code distribution between high-and low-frequency features. Then, we integrate a contrastive learning mechanismto ensure a uniform distribution of quantized codes, enhancing thedistinctiveness of embeddings. Experiments on three datasets reveal that ourmethod reduces memory usage by over 50x while maintaining or improvingrecommendation performance compared to existing models. The implementation codeis accessible in our project repository https://github.com/USTC-StarTeam/MEC.</description>
      <author>example@mail.com (Kefan Wang, Hao Wang, Kenan Song, Wei Guo, Kai Cheng, Zhi Li, Yong Liu, Defu Lian, Enhong Chen)</author>
      <guid isPermaLink="false">2502.15355v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Drug-Target Interaction/Affinity Prediction: Deep Learning Models and Advances Review</title>
      <link>http://arxiv.org/abs/2502.15346v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  64 pages, 7 figures, 10 tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;药物研发是一个耗时且成本高昂的过程，涉及到从目标结构检测到获得食品药品监督管理局(FDA)批准的多个步骤，并常伴随着安全问题。&lt;h4&gt;背景&lt;/h4&gt;传统方法在预测药物与靶标之间的相互作用方面存在局限性，特别是在捕捉复杂关系上。为此，深度学习模型被提出以克服这一挑战并提供精确和高效的预测结果。&lt;h4&gt;目的&lt;/h4&gt;通过概述有前途的研究方向和模型，各具有不同解决方案但针对同一问题，这篇论文旨在为研究人员提供更多准确且高效地预测药物-靶标相互作用的方法，从而加速更有效药物的开发。&lt;h4&gt;方法&lt;/h4&gt;从2016年到2025年间，总共分析了基于机器学习（主要是深度学习和图神经网络）的不同框架下的180种药物-靶标交互预测方法。&lt;h4&gt;主要发现&lt;/h4&gt;论文讨论了这些模型的新颖性、架构以及输入表示。&lt;h4&gt;结论&lt;/h4&gt;通过引入新的技术和更优的方法，有可能加速药物研发过程并提高药物安全性。深度学习和图神经网络等先进技术在该领域展现出巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;药物发现仍是一个缓慢且成本高昂的过程，包括从目标结构检测到获得FDA批准的多个步骤，并经常伴随着安全问题。准确预测药物与其靶标之间的相互作用以及使用更好的方法和技术开发新药，具有极大可能加速这一过程，最终实现更快地提供救命药物。传统的药物-靶标交互预测方法显示出局限性，在捕捉复杂关系方面尤其如此。因此，深度学习模型被提出以通过其精确和高效的最终结果克服这些挑战。通过概述有前途的研究方向和模型，各具不同的解决方案但针对同一问题，这篇论文旨在为研究人员提供更准确且高效地预测药物-靶标相互作用的方法的更好理解，从而加速开发出更有效的药物。在2016年到2025年间，总共分析了基于机器学习（主要是深度学习和图神经网络）的不同框架下的180种药物-靶标交互预测方法。此外，该论文还讨论了这些模型的新颖性、架构以及输入表示。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Drug discovery remains a slow and expensive process that involves many steps,from detecting the target structure to obtaining approval from the Food andDrug Administration (FDA), and is often riddled with safety concerns. Accurateprediction of how drugs interact with their targets and the development of newdrugs by using better methods and technologies have immense potential to speedup this process, ultimately leading to faster delivery of life-savingmedications. Traditional methods used for drug-target interaction predictionshow limitations, particularly in capturing complex relationships between drugsand their targets. As an outcome, deep learning models have been presented toovercome the challenges of interaction prediction through their precise andefficient end results. By outlining promising research avenues and models, eachwith a different solution but similar to the problem, this paper aims to giveresearchers a better idea of methods for even more accurate and efficientprediction of drug-target interaction, ultimately accelerating the developmentof more effective drugs. A total of 180 prediction methods for drug-targetinteractions were analyzed throughout the period spanning 2016 to 2025 usingdifferent frameworks based on machine learning, mainly deep learning and graphneural networks. Additionally, this paper discusses the novelty, architecture,and input representation of these models.</description>
      <author>example@mail.com (Ali Vefghi, Zahed Rahmati, Mohammad Akbari)</author>
      <guid isPermaLink="false">2502.15346v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Beyond Fixed Variables: Expanding-variate Time Series Forecasting via Flat Scheme and Spatio-temporal Focal Learning</title>
      <link>http://arxiv.org/abs/2502.15296v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文介绍了一个新的时间序列预测任务：扩展变量的时间序列预测（EVTSF），并提出了一个新的灵活的时空预测框架STEV来应对新增变量带来的挑战。&lt;h4&gt;背景&lt;/h4&gt;多变量时间序列预测（MTSF）长期是研究重点。传统研究假设固定的变量数量，但实际应用中随着新传感器部署，系统的变量会增多。&lt;h4&gt;目的&lt;/h4&gt;解决由于新变量加入导致的数据形状不一致和时空学习不平衡的问题。&lt;h4&gt;方法&lt;/h4&gt;提出了STEV框架，包括一个新的Flat Scheme来处理数据形状不一致问题，并引入了一种新的时空聚焦学习策略，解决了对比学习与图表示之间的潜在冲突。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，STEV在扩展变量上的表现显著优于其竞争对手。即使只使用5%的观察值，STEV也能达到最先进的MTSF模型的表现水平。&lt;h4&gt;结论&lt;/h4&gt;STEV是一个通用性强、性能优越的时空预测框架，适用于实际应用中的各种扩展策略。&lt;h4&gt;翻译&lt;/h4&gt;多变量时间序列预测（MTSF）一直是研究的核心领域。传统方法假设固定的变量数量，但在现实世界的应用中，随着新传感器部署，Cyber-Physical系统的变量会增加。为此，我们提出了一个新的任务——扩展变量的时间序列预测（EVTSF）。这个任务面临两个挑战：一是处理新增变量导致的数据形状不一致问题；二是解决时空学习不平衡的问题，即由于需要及时操作而新加入的变量观测数据有限。为了解决这些问题，我们提出了一种灵活的时空预测框架STEV，它包含了一个新的Flat Scheme来应对数据形状不一致，并引入了新颖的时空聚焦学习策略。通过三个真实世界的数据集对EVTSF的表现进行了基准测试，并与三个可能解决方案（采用最先进的MTSF模型定制为EVSTF）进行了比较。实验结果表明，STEV在扩展变量上的表现显著优于竞争对手，甚至使用仅5%的新增时期观察值时也能达到最佳水平的效果。进一步研究各种扩展策略证实了STEV在现实应用中的通用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multivariate Time Series Forecasting (MTSF) has long been a key researchfocus. Traditionally, these studies assume a fixed number of variables, but inreal-world applications, Cyber-Physical Systems often expand as new sensors aredeployed, increasing variables in MTSF. In light of this, we introduce a noveltask, Expanding-variate Time Series Forecasting (EVTSF). This task presentsunique challenges, specifically (1) handling inconsistent data shapes caused byadding new variables, and (2) addressing imbalanced spatio-temporal learning,where expanding variables have limited observed data due to the necessity fortimely operation. To address these challenges, we propose STEV, a flexiblespatio-temporal forecasting framework. STEV includes a new Flat Scheme totackle the inconsistent data shape issue, which extends the graph-basedspatio-temporal modeling architecture into 1D space by flattening the 2Dsamples along the variable dimension, making the model variable-scale-agnosticwhile still preserving dynamic spatial correlations through a holistic graph.We introduce a novel Spatio-temporal Focal Learning strategy that incorporatesa negative filter to resolve potential conflicts between contrastive learningand graph representation, and a focal contrastive loss as its core to guide theframework to focus on optimizing the expanding variables. We benchmark EVTSFperformance using three real-world datasets and compare it against threepotential solutions employing SOTA MTSF models tailored for EVSTF. Experimentalresults show that STEV significantly outperforms its competitors, particularlyon expanding variables. Notably, STEV, with only 5% of observations from theexpanding period, is on par with SOTA MTSF models trained with completeobservations. Further exploration of various expanding strategies underscoresthe generalizability of STEV in real-world applications.</description>
      <author>example@mail.com (Minbo Ma, Kai Tang, Huan Li, Fei Teng, Dalin Zhang, Tianrui Li)</author>
      <guid isPermaLink="false">2502.15296v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Learning Maritime Inventory Routing Optimization</title>
      <link>http://arxiv.org/abs/2502.15244v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种基于机器学习的局部搜索方法，用于解决大规模海洋库存路由优化问题。&lt;h4&gt;背景&lt;/h4&gt;海洋物流中的库存路由优化问题是组合复杂性很高的问题。&lt;h4&gt;目的&lt;/h4&gt;为了提高大规模海洋库存路由问题解决方案的质量和效率。&lt;h4&gt;方法&lt;/h4&gt;集成图神经网络（GNN）来选择邻域以提升局部搜索的效率，实现对船舶邻近区域的有结构探索。&lt;h4&gt;主要发现&lt;/h4&gt;在大量实际案例中证明了该方法比直接使用混合整数规划法更加高效。&lt;h4&gt;结论&lt;/h4&gt;新方法提高了大规模海洋库存路由优化问题求解的质量和效率。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种基于机器学习的局部搜索方法，用于寻找大规模海上库存路由优化问题的可行解决方案。鉴于此类问题组合复杂度高，我们将图神经网络（GNN）为基础的邻域选择法融入进来以增强局部搜索效率。该方法实现了对船舶邻近区域有结构化的探索，在改善了解质量的同时也保证了计算效率。通过大量的实证实验验证，我们证明在真实案例中本研究的方法相比直接应用混合整数规划方法在求解时间上表现更优。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We propose a machine learning-based local search approach for findingfeasible solutions of large-scale maritime inventory routing optimizationproblems. Given the combinatorial complexity of the problems, we integrate agraph neural network-based neighborhood selection method to enhance localsearch efficiency. Our approach enables a structured exploration of vesselneighborhoods, improving solution quality while maintaining computationalefficiency. Through extensive computational experiments on realistic instances,we demonstrate that our method outperforms direct mixed-integer programming insolution time.</description>
      <author>example@mail.com (Rui Chen, Defeng Liu, Nan Jiang, Rishabh Gupta, Mustafa Kilinc, Andrea Lodi)</author>
      <guid isPermaLink="false">2502.15244v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>SiMHand: Mining Similar Hands for Large-Scale 3D Hand Pose Pre-training</title>
      <link>http://arxiv.org/abs/2502.15251v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  ICLR 2025. arXiv admin note: text overlap with arXiv:2409.09714&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种用于从野外视频中获取的手图像预训练三维手部姿态估计的框架SimHand。&lt;h4&gt;背景&lt;/h4&gt;现有的3D手部姿势预训练方法未能充分利用来自野外视频中的多样化手部图像的潜力。&lt;h4&gt;目的&lt;/h4&gt;为了实现可扩展性的预训练，准备了一个广泛的包含200多万张手部图像的数据池，并设计了基于对比学习的方法。&lt;h4&gt;方法&lt;/h4&gt;从最近的人类中心视频中收集超过2.0M的手部图像；通过关注手部相似性来提取区分信息，即非相同样本但具有类似手部姿势的成对样本；提出了一种新的对比学习方法，将类似的双手对在特征空间中嵌得更近，并根据样本间的距离自适应地调整对比学习损失权重。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，该方法优于基于单张图像数据增强生成正例的传统对比学习方法。相比最先进的方法PeCLR，在FreiHand、DexYCB和AssemblyHands数据集上的表现分别提高了15%、10%和4%&lt;h4&gt;结论&lt;/h4&gt;我们的SimHand方法在预训练3D手部姿态估计方面提供了显著的性能改进。&lt;h4&gt;翻译&lt;/h4&gt;我们提出了一种框架，用于从具有类似手部特征的手图像中进行三维手部姿势的预训练。使用大规模图像可以在各种任务中实现令人满意的结果，但现有的3D手部姿势预训练方法尚未充分利用从野外视频中获取的各种手部图像的潜力。为了促进可扩展性的预训练，我们首先准备了一个包含超过2.0M手部图像的数据池，并设计了一种基于对比学习的方法进行预训练。通过关注手部相似性来提取区分信息：即非相同的样本但具有类似的手部姿势对。然后，我们提出了一种新的对比学习方法，该方法将类似的双手对在特征空间中嵌得更近，并根据样本间的距离自适应地调整对比学习损失权重。实验表明，我们的方法优于基于单张图像数据增强生成正例的传统对比学习方法。在FreiHand、DexYCB和AssemblyHands数据集上，相比最先进的方法PeCLR，我们实现了显著的性能提升（分别提高了15%、10%和4%）。&lt;h4&gt;代码链接&lt;/h4&gt;https://github.com/ut-vision/SiMHand&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a framework for pre-training of 3D hand pose estimation fromin-the-wild hand images sharing with similar hand characteristics, dubbedSimHand. Pre-training with large-scale images achieves promising results invarious tasks, but prior methods for 3D hand pose pre-training have not fullyutilized the potential of diverse hand images accessible from in-the-wildvideos. To facilitate scalable pre-training, we first prepare an extensive poolof hand images from in-the-wild videos and design our pre-training method withcontrastive learning. Specifically, we collect over 2.0M hand images fromrecent human-centric videos, such as 100DOH and Ego4D. To extractdiscriminative information from these images, we focus on the similarity ofhands: pairs of non-identical samples with similar hand poses. We then proposea novel contrastive learning method that embeds similar hand pairs closer inthe feature space. Our method not only learns from similar samples but alsoadaptively weights the contrastive learning loss based on inter-sampledistance, leading to additional performance gains. Our experiments demonstratethat our method outperforms conventional contrastive learning approaches thatproduce positive pairs sorely from a single image with data augmentation. Weachieve significant improvements over the state-of-the-art method (PeCLR) invarious datasets, with gains of 15% on FreiHand, 10% on DexYCB, and 4% onAssemblyHands.  Our code is available at https://github.com/ut-vision/SiMHand.</description>
      <author>example@mail.com (Nie Lin, Takehiko Ohkawa, Yifei Huang, Mingfang Zhang, Minjie Cai, Ming Li, Ryosuke Furuta, Yoichi Sato)</author>
      <guid isPermaLink="false">2502.15251v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>ELIP: Enhanced Visual-Language Foundation Models for Image Retrieval</title>
      <link>http://arxiv.org/abs/2502.15682v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架ELIP，用于增强大规模预训练的视觉-语言模型在文本到图像检索中的性能。&lt;h4&gt;背景&lt;/h4&gt;现有的大型预训练视觉-语言模型难以直接应用于文本到图像重排序任务。&lt;h4&gt;目的&lt;/h4&gt;通过引入一个新框架来提高大规模预训练视觉-语言模型在文本到图像检索上的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种增强的语言-图像预训练（ELIP）方法，该方法利用文本查询预测一组视觉提示以调节ViT图像编码。此方法可以应用于CLIP/SigLIP和最先进的BLIP-2架构，并开发了适用于计算资源有限情况下的'学生友好型'最佳实践。&lt;h4&gt;主要发现&lt;/h4&gt;通过实验验证，ELIP框架显著提高了CLIP/SigLIP的性能，并在文本到图像检索任务上超过了当前最先进模型BLIP-2的表现。&lt;h4&gt;结论&lt;/h4&gt;使用新架构和数据整理方法能够有效提升大规模视觉语言预训练模型用于文本到图像检索时的效果。&lt;h4&gt;翻译&lt;/h4&gt;该论文的目标是改进从文本到图像检索的性能。为此，研究者们提出了一种新的框架，可以增强大规模预训练视觉-语言模型的表现力，使其可用于重排序任务中。通过使用文本查询预测一组视觉提示来调节ViT图象编码的方法被称作ELIP，并且可以在常见的CLIP/SigLIP和最先进BLIP-2架构上应用。为了在计算资源有限的情况下训练该框架，研究者们开发了一种友好的实践方法，包括全局难例挖掘、大规模数据集的选择与整理等措施。评估方面，论文设立了两个新的分布外基准测试（Occluded COCO 和 ImageNet-R），用以衡量模型在不同领域中的零样本泛化能力。实验表明，由于新颖的架构和数据整理技术，增强后的网络显著提高了CLIP/SigLIP的表现，并且在文本到图像检索任务上超过了最先进的BLIP-2模型。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The objective in this paper is to improve the performance of text-to-imageretrieval. To this end, we introduce a new framework that can boost theperformance of large-scale pre-trained vision-language models, so that they canbe used for text-to-image re-ranking. The approach, Enhanced Language-ImagePre-training (ELIP), uses the text query to predict a set of visual prompts tocondition the ViT image encoding. ELIP can easily be applied to the commonlyused CLIP/SigLIP and the state-of-the-art BLIP-2 architectures. To train thearchitecture with limited computing resources, we develop a 'student friendly'best practice involving global hard sample mining, and selection and curationof a large-scale dataset. On the evaluation side, we set up two newout-of-distribution benchmarks, Occluded COCO and ImageNet-R, to assess thezero-shot generalisation of the models to different domains. Benefiting fromthe novel architecture and data curation, experiments show our enhanced networksignificantly boosts CLIP/SigLIP performance and outperforms thestate-of-the-art BLIP-2 model on text-to-image retrieval.</description>
      <author>example@mail.com (Guanqi Zhan, Yuanpei Liu, Kai Han, Weidi Xie, Andrew Zisserman)</author>
      <guid isPermaLink="false">2502.15682v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>GNN-Coder: Boosting Semantic Code Retrieval with Combined GNNs and Transformer</title>
      <link>http://arxiv.org/abs/2502.15202v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;GNN-Coder是一种基于图神经网络（GNN）的新型框架，用于利用抽象语法树（AST），该方法旨在提高代码检索任务中的结构和语义特征捕捉能力。&lt;h4&gt;背景&lt;/h4&gt;现有的依赖于序列模型的方法在大型项目中难以完全发挥代码内在结构依赖的作用，尤其是在处理复杂结构的代码片段时表现不佳。&lt;h4&gt;目的&lt;/h4&gt;通过研究如何将GNN与Transformer结合来促进语义检索任务的发展，并引入一种新颖的图池化方法以及一个新的量化代码嵌入分布均匀性的度量指标MAM。&lt;h4&gt;方法&lt;/h4&gt;提出了基于抽象语法树（AST）的图神经网络框架GNN-Coder，该框架使用子节点数量作为关键特征来突出AST内的内在拓扑关系。同时引入了新的度量标准Mean Angular Margin (MAM) 来衡量代码嵌入分布的一致性和特征分离性。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，GNN-Coder在CSN数据集上的MRR（平均准确率）提高了1%-10%，在CosQA数据集上的零样本性能显著提升了20%。&lt;h4&gt;结论&lt;/h4&gt;基于图神经网络的框架GNN-Coder可以有效提高代码检索任务中的结构和语义特征捕捉能力，从而增强模型对不同代码片段的区分能力和检索准确性。&lt;h4&gt;翻译&lt;/h4&gt;代码检索是现代软件开发的关键组成部分，在大型项目中尤为重要。然而，现有的序列模型方法往往未能充分利用代码固有的结构性依赖关系，导致在复杂结构代码片段上的检索性能不佳。本文介绍了一种基于图神经网络（GNN）的新框架——GNN-Coder，用于利用抽象语法树（AST）。这是首次尝试研究如何通过捕获代码的结构和语义特征来促进语义检索任务的发展，并引入了一个新的针对AST设计的图池化方法，使用子节点数量作为关键特性以突出显示AST内的内在拓扑关系。该设计有效集成了序列表示与层次表示，增强了模型捕捉代码结构和语义的能力。此外，还提出了一种新的度量指标——均值角度余量（MAM），用于量化代码嵌入分布的均匀性，提供了特征分离性的标准化衡量方法。所提出的这种方法实现了更低的MAM值，表明了更具有区分性的特征表示能力。这强调了GNN-Coder在区分不同代码片段方面的优越能力，并提高了检索准确性。实验结果表明，GNN-Coder显著提升了检索性能，在CSN数据集上平均准确率（MRR）提高了1%-10%，在CosQA数据集上的零样本性能显著提升20%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Code retrieval is a crucial component in modern software development,particularly in large-scale projects. However, existing approaches relying onsequence-based models often fail to fully exploit the structural dependenciesinherent in code, leading to suboptimal retrieval performance, particularlywith structurally complex code fragments. In this paper, we introduceGNN-Coder, a novel framework based on Graph Neural Network (GNN) to utilizeAbstract Syntax Tree (AST). We make the first attempt to study howGNN-integrated Transformer can promote the development of semantic retrievaltasks by capturing the structural and semantic features of code. We furtherpropose an innovative graph pooling method tailored for AST, utilizing thenumber of child nodes as a key feature to highlight the intrinsic topologicalrelationships within the AST. This design effectively integrates bothsequential and hierarchical representations, enhancing the model's ability tocapture code structure and semantics. Additionally, we introduce the MeanAngular Margin (MAM), a novel metric for quantifying the uniformity of codeembedding distributions, providing a standardized measure of featureseparability. The proposed method achieves a lower MAM, indicating a morediscriminative feature representation. This underscores GNN-Coder's superiorability to distinguish between code snippets, thereby enhancing retrievalaccuracy. Experimental results show that GNN-Coder significantly boostsretrieval performance, with a 1\%-10\% improvement in MRR on the CSN dataset,and a notable 20\% gain in zero-shot performance on the CosQA dataset.</description>
      <author>example@mail.com (Yufan Ye, Pu Pang, Ting Zhang, Hua Huang)</author>
      <guid isPermaLink="false">2502.15202v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Hierarchical Context Transformer for Multi-level Semantic Scene Understanding</title>
      <link>http://arxiv.org/abs/2502.15184v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper has been accepted by the IEEE TCSVT&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种多层次语义场景理解（MSSU）的方法，用于手术场景的理解，并设计了一个层次化上下文变换器（HCT）网络。&lt;h4&gt;背景&lt;/h4&gt;在开发基于上下文感知的计算机辅助系统中，对手术场景的全面理解和显式认识至关重要。然而，很少有研究提供系统的分析以实现分层的手术场景理解。&lt;h4&gt;目的&lt;/h4&gt;提出一个多层级语义场景理解的方法和层次化上下文变换器网络来解决现有不足，并探索不同级别任务之间的关系。&lt;h4&gt;方法&lt;/h4&gt;设计了一个层次化相关聚合模块（HRAM），同时关联多级交互信息内部条目，然后增强特定于任务的特征。为了进一步促进各种任务的表示学习，提出了跨任务对比学习（ICL）以引导模型通过吸收其他任务提供的补充信息来学习具有任务特性的功能。&lt;h4&gt;主要发现&lt;/h4&gt;通过在白内障数据集和公共可用的内窥镜PSI-AVA数据集上的广泛实验，显示了该方法卓越的性能，并且始终大幅超越现有最佳方法。&lt;h4&gt;结论&lt;/h4&gt;提出的HCT+能够利用空间和时间适配器，在大量可调参数减少的情况下实现竞争力的表现。这些发现表明该方法在手术场景理解方面具有重要潜力。&lt;h4&gt;翻译&lt;/h4&gt;对手术场景进行全面而明确的理解对开发基于上下文感知的计算机辅助系统至关重要，但关于分层手术场景理解的研究较少提供系统分析。这项工作提出了将任务集表示为多层次语义场景理解（MSSU），并提出了一种新型层次化上下文变换器（HCT）网络来解决这一问题，并彻底探索了不同级别任务之间的关系。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A comprehensive and explicit understanding of surgical scenes plays a vitalrole in developing context-aware computer-assisted systems in the operatingtheatre. However, few works provide systematical analysis to enablehierarchical surgical scene understanding. In this work, we propose torepresent the tasks set [phase recognition --&gt; step recognition --&gt; action andinstrument detection] as multi-level semantic scene understanding (MSSU). Forthis target, we propose a novel hierarchical context transformer (HCT) networkand thoroughly explore the relations across the different level tasks.Specifically, a hierarchical relation aggregation module (HRAM) is designed toconcurrently relate entries inside multi-level interaction information and thenaugment task-specific features. To further boost the representation learning ofthe different tasks, inter-task contrastive learning (ICL) is presented toguide the model to learn task-wise features via absorbing complementaryinformation from other tasks. Furthermore, considering the computational costsof the transformer, we propose HCT+ to integrate the spatial and temporaladapter to access competitive performance on substantially fewer tunableparameters. Extensive experiments on our cataract dataset and a publiclyavailable endoscopic PSI-AVA dataset demonstrate the outstanding performance ofour method, consistently exceeding the state-of-the-art methods by a largemargin. The code is available at https://github.com/Aurora-hao/HCT.</description>
      <author>example@mail.com (Luoying Hao, Yan Hu, Yang Yue, Li Wu, Huazhu Fu, Jinming Duan, Jiang Liu)</author>
      <guid isPermaLink="false">2502.15184v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Adaptive Expansion for Hypergraph Learning</title>
      <link>http://arxiv.org/abs/2502.15564v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了新的自适应扩展方法AdE，该方法通过基于团的扩展方式将超图转换为加权图，并利用全局模拟网络和距离感知核函数来保持高阶结构信息。&lt;h4&gt;背景&lt;/h4&gt;近年来，随着对捕获更高阶关系能力的要求，超图受到了广泛关注。许多超图表示学习方法也随之出现。&lt;h4&gt;目的&lt;/h4&gt;为了克服经典扩展方法在固定边权重设计中导致的信息丢失或冗余问题，提出了一种新的自适应扩展方法AdE。&lt;h4&gt;方法&lt;/h4&gt;通过引入全局模拟网络选择每个超边中的两个代表性节点，并将同一超边中的其余节点连接到相应的选定节点。设计了距离感知核函数，动态调整边缘权重以确保相似的节点具有更大的重量。&lt;h4&gt;主要发现&lt;/h4&gt;AdE相比经典的扩展模型在理论合理性、泛化能力和有效性方面都表现出色&lt;h4&gt;结论&lt;/h4&gt;提出的AdE方法能够更好地保持和利用超图中的高阶结构信息&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hypergraph, with its powerful ability to capture higher-order relationships,has gained significant attention recently. Consequently, many hypergraphrepresentation learning methods have emerged to model the complex relationshipsamong hypergraphs. In general, these methods leverage classic expansion methodsto convert hypergraphs into weighted or bipartite graphs, and further employmessage passing mechanisms to model the complex structures within hypergraphs.However, classical expansion methods are designed in straightforward mannerswith fixed edge weights, resulting in information loss or redundancy. In lightof this, we design a novel clique expansion-based Adaptive Expansion methodcalled AdE to adaptively expand hypergraphs into weighted graphs that preservethe higher-order structure information. Specifically, we introduce a novelGlobal Simulation Network to select two representative nodes for adaptivelysymbolizing each hyperedge and connect the rest of the nodes within the samehyperedge to the corresponding selected nodes. Afterward, we design adistance-aware kernel function, dynamically adjusting edge weights to ensuresimilar nodes within a hyperedge are connected with larger weights. Extensivetheoretical justifications and empirical experiments over seven benchmarkhypergraph datasets demonstrate that AdE has excellent rationality,generalization, and effectiveness compared to classic expansion models.</description>
      <author>example@mail.com (Tianyi Ma, Yiyue Qian, Shinan Zhang, Chuxu Zhang, Yanfang Ye)</author>
      <guid isPermaLink="false">2502.15564v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>mStyleDistance: Multilingual Style Embeddings and their Evaluation</title>
      <link>http://arxiv.org/abs/2502.15168v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  arXiv admin note: substantial text overlap with arXiv:2410.12757&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'总结': '提出了一种多语言风格嵌入模型mStyleDistance，该模型使用合成数据和对比学习进行训练。', '背景': '现有的风格嵌入仅限于英语，缺乏对多种语言的支持。', '目的': '开发一种能够在多种语言中有效工作的风格嵌入模型，并用于评估其质量和性能。', '方法': '利用来自九种不同语言的数据训练mStyleDistance模型；创建一个多语言的STEL或内容基准测试来评估嵌入的质量；在跨语种作者身份验证任务中应用该模型。', '主要发现': '实验结果表明，mStyleDistance风格嵌入优于现有模型，在多语言风格基准上表现出色，并且能够很好地推广到未见过的语言和特征。', '结论': '展示了mStyleDistance的潜在价值及其在跨语种分析中的优势；源代码已公开发布。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了一种新的多语言风格嵌入模型，名为Multilingual Style Distance (mStyleDistance)，该模型通过合成数据和对比学习进行了训练，并用于评估其质量和性能。研究显示了该方法在处理多种语言的风格分析任务中的优越性，并且能够很好地推广到新场景中去使用。研究成果已经公开分享。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Style embeddings are useful for stylistic analysis and style transfer;however, only English style embeddings have been made available. We introduceMultilingual StyleDistance (mStyleDistance), a multilingual style embeddingmodel trained using synthetic data and contrastive learning. We train the modelon data from nine languages and create a multilingual STEL-or-Content benchmark(Wegmann et al., 2022) that serves to assess the embeddings' quality. We alsoemploy our embeddings in an authorship verification task involving differentlanguages. Our results show that mStyleDistance embeddings outperform existingmodels on these multilingual style benchmarks and generalize well to unseenfeatures and languages. We make our model publicly available athttps://huggingface.co/StyleDistance/mstyledistance .</description>
      <author>example@mail.com (Justin Qiu, Jiacheng Zhu, Ajay Patel, Marianna Apidianaki, Chris Callison-Burch)</author>
      <guid isPermaLink="false">2502.15168v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Para-Lane: Multi-Lane Dataset Registering Parallel Scans for Benchmarking Novel View Synthesis</title>
      <link>http://arxiv.org/abs/2502.15635v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;为了评估端到端的自动驾驶系统，基于新颖视图合成（NVS）技术的模拟环境是必不可少的。这种模拟可以生成新的车辆姿态下的真实感图像和点云数据，尤其是在跨车道场景中。&lt;h4&gt;目的&lt;/h4&gt;由于现有的合成场景基础的NVS数据集在现实性和捕捉到的图像及点云的真实度方面仍然存在不足，因此开发一个多车道的数据集和基准是必要的。该研究旨在基于NeRF和3DGS方法进一步评估现有方法的表现。&lt;h4&gt;方法&lt;/h4&gt;提出了一种新方法来创建第一个多车道数据集，通过并行扫描真实世界获取25组相关序列，其中包括16,000张前置视图图像、64,000张环视图像以及16,000个LiDAR帧。所有帧都被标记以区分移动物体和静态元素。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用该数据集，在不同车道和距离的多种测试场景中评估现有方法的表现，并提供解决多传感器姿态问题的方法，实现跨模式数据对齐。&lt;h4&gt;结论&lt;/h4&gt;该研究公开了一个名为Paralane的数据集（网址：https://nizqleo.github.io/paralane-dataset/），用于持续添加新的序列以测试现有的方法在不同场景中的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;为了评估端到端的自动驾驶系统，一个基于新颖视图合成技术的模拟环境是必要的。该研究提出了一种新型多车道数据集和基准，旨在进一步检验NeRF和3DGS等现有方法的表现。新创建的数据集包括来自真实世界扫描并行扫描25组相关序列，含16000张前置视角图像、64,000环视图像及16,000个LiDAR帧，并且提供了解决跨模式数据对齐的多传感器姿态问题的方法。研究者计划持续添加新的序列以测试现有方法在不同场景中的泛化能力，数据集公开于项目页面：https://nizqleo.github.io/paralane-dataset/&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; To evaluate end-to-end autonomous driving systems, a simulation environmentbased on Novel View Synthesis (NVS) techniques is essential, which synthesizesphoto-realistic images and point clouds from previously recorded sequencesunder new vehicle poses, particularly in cross-lane scenarios. Therefore, thedevelopment of a multi-lane dataset and benchmark is necessary. While recentsynthetic scene-based NVS datasets have been prepared for cross-lanebenchmarking, they still lack the realism of captured images and point clouds.To further assess the performance of existing methods based on NeRF and 3DGS,we present the first multi-lane dataset registering parallel scans specificallyfor novel driving view synthesis dataset derived from real-world scans,comprising 25 groups of associated sequences, including 16,000 front-viewimages, 64,000 surround-view images, and 16,000 LiDAR frames. All frames arelabeled to differentiate moving objects from static elements. Using thisdataset, we evaluate the performance of existing approaches in various testingscenarios at different lanes and distances. Additionally, our method providesthe solution for solving and assessing the quality of multi-sensor poses formulti-modal data alignment for curating such a dataset in real-world. We planto continually add new sequences to test the generalization of existing methodsacross different scenarios. The dataset is released publicly at the projectpage: https://nizqleo.github.io/paralane-dataset/.</description>
      <author>example@mail.com (Ziqian Ni, Sicong Du, Zhenghua Hou, Chenming Wu, Sheng Yang)</author>
      <guid isPermaLink="false">2502.15635v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Graph-Based Deep Learning on Stereo EEG for Predicting Seizure Freedom in Epilepsy Patients</title>
      <link>http://arxiv.org/abs/2502.15198v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;预测癫痫患者无发作状态对于个性化治疗至关重要。然而，传统的预测方法难以在不同类型的患者群体中实现准确的预测。&lt;h4&gt;背景&lt;/h4&gt;难治性癫痫患者的预后评估需要更有效的工具来提高治疗效果和减少副作用。&lt;h4&gt;目的&lt;/h4&gt;开发基于深度学习的图神经网络（GNN）模型以预测从立体脑电图（sEEG）数据中获得的无发作状态。&lt;h4&gt;方法&lt;/h4&gt;利用15名儿科难治性癫痫患者的高质量sEEG数据训练模型，使用图形卷积和多尺度注意力机制捕捉局部与全局连接性。&lt;h4&gt;主要发现&lt;/h4&gt;{'准确性': '在二元分类分析、患者级分析及多类分析中分别达到了92.4%、86.6%和81.4%的准确率；', '关键区域': '前扣带皮层与额极是预测无发作状态的关键脑区，并且这些区域更可能对应于癫痫发作起始区。', '模型识别': '模型标识出的节点更有可能重合于癫痫发作起始区，强调了基于连接性的新深度学习模型对于提高无发作状态预测、定位癫痫发作起始区及大脑在癫痫期间的连接性分析的重要性。'}&lt;h4&gt;结论&lt;/h4&gt;新型基于连接性的深度学习模型如GNN为改善难治性癫痫患者的个性化治疗提供了新的视角。&lt;h4&gt;翻译&lt;/h4&gt;预测癫痫患者无发作状态对于个性化治疗至关重要。然而，传统的预测方法难以在不同类型的患者群体中实现准确的预测。本研究开发了一种基于深度学习的图神经网络（GNN）模型以预测从立体脑电图（sEEG）数据获得的无发作状态结果，并加深了对癫痫起始区大脑连接性的理解。该模型结合局部和全局连接性使用图形卷积与多尺度注意力机制来捕捉如丘脑及运动区域这样难以研究区域之间的连接，成功提高了预测精度。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting seizure freedom is essential for tailoring epilepsy treatment. Butaccurate prediction remains challenging with traditional methods, especiallywith diverse patient populations. This study developed a deep learning-basedgraph neural network (GNN) model to predict seizure freedom from stereoelectroencephalography (sEEG) data in patients with refractory epilepsy. Weutilized high-quality sEEG data from 15 pediatric patients to train a deeplearning model that can accurately predict seizure freedom outcomes and advanceunderstanding of brain connectivity at the seizure onset zone. Our modelintegrates local and global connectivity using graph convolutions withmulti-scale attention mechanisms to capture connections betweendifficult-to-study regions such as the thalamus and motor regions. The modelachieved an accuracy of 92.4% in binary class analysis, 86.6% in patient-wiseanalysis, and 81.4% in multi-class analysis. Node and edge-level featureanalysis highlighted the anterior cingulate and frontal pole regions as keycontributors to seizure freedom outcomes. The nodes identified by our modelwere also more likely to coincide with seizure onset zones. Our findingsunderscore the potential of new connectivity-based deep learning models such asGNNs for enhancing the prediction of seizure freedom, predicting seizure onsetzones, connectivity analysis of the brain during seizure, as well as informingAI-assisted personalized epilepsy treatment planning.</description>
      <author>example@mail.com (Artur Agaronyan, Syeda Abeera Amir, Nunthasiri Wittayanakorn, John Schreiber, Marius G. Linguraru, William Gaillard, Chima Oluigbo, Syed Muhammad Anwar)</author>
      <guid isPermaLink="false">2502.15198v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Fine-tuning foundation models of materials interatomic potentials with frozen transfer learning</title>
      <link>http://arxiv.org/abs/2502.15582v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;机器学习的原子间势能是通过提供训练数据覆盖范围内准确且可扩展的预测来革新原子材料模拟，但生成一个精确且稳健的数据集仍然是个挑战。本文展示了使用迁移学习可以提高基础模型的准确性，并构建了一个更为高效的人工智能模拟工作流程。&lt;h4&gt;背景&lt;/h4&gt;机器学习的原子间势能能够提供训练数据覆盖范围内准确和可扩展的预测，然而，要生成这种精准而健壮的数据集，通常需要数千次第一原理计算。现在开始出现一种旨在创建可以适用于广泛材料领域的基础模型。&lt;h4&gt;目的&lt;/h4&gt;展示通过迁移学习提高基础模型势能准确性，并构建一个改进了数据效率和计算效率的人工智能模拟工作流程。&lt;h4&gt;方法&lt;/h4&gt;利用部分冻结的权重和偏差进行微调，使用两个具有挑战性的表面反应化学以及三元合金稳定性和弹性性质的数据集作为案例研究。&lt;h4&gt;主要发现&lt;/h4&gt;通过迁移学习，在只使用少量（几百个数据点）的情况下可以达到与从零开始训练模型相媲美的精度。此外，还可以利用这种经过调整的势能构建一个准确度相当但更高效的代理模型。&lt;h4&gt;结论&lt;/h4&gt;本文提出了一种改进了机器学习潜在效率的人工智能模拟工作流程，该流程提高了数据和计算使用率。&lt;h4&gt;翻译&lt;/h4&gt;机器学习原子间势场通过提供训练数据范围内的精确且可扩展的预测来革新原子材料模拟。生成准确且稳健的数据集仍然是一个挑战，通常需要数千次第一性原理计算才能获得高精度。现在正在出现一种基础模型，其目标是在广泛的材料中通用适用潜在场。虽然这些基础模型可以是健壮和转移的，但它们尚未达到预测反应势垒、相变以及物质稳定性所需的精确度。这项工作展示了通过使用部分冻结权重和偏差进行迁移学习微调的基础模型潜力可以实现化学精度。对于两个具有挑战性的数据集：表面反应化学与三元合金稳定性和弹性性质的研究表明，在使用10-20%的数据（几百个数据点）情况下，冻结转移学习达到的准确度与从头开始训练模型相仿（需要数千个数据点）。此外还展示了可以建立一个同样精确但计算更高效的替代模型，该模型以迁移学习后的势能为真值。综合来看，我们提出了一种改进了机器学习潜在效率的人工智能模拟工作流程，此流程提高了数据和计算使用率。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Machine-learned interatomic potentials are revolutionising atomisticmaterials simulations by providing accurate and scalable predictions within thescope covered by the training data. However, generation of an accurate androbust training data set remains a challenge, often requiring thousands offirst-principles calculations to achieve high accuracy. Foundation models havestarted to emerge with the ambition to create universally applicable potentialsacross a wide range of materials. While foundation models can be robust andtransferable, they do not yet achieve the accuracy required to predict reactionbarriers, phase transitions, and material stability. This work demonstratesthat foundation model potentials can reach chemical accuracy when fine-tunedusing transfer learning with partially frozen weights and biases. For twochallenging datasets on reactive chemistry at surfaces and stability andelastic properties of tertiary alloys, we show that frozen transfer learningwith 10-20% of the data (hundreds of datapoints) achieves similar accuracies tomodels trained from scratch (on thousands of datapoints). Moreover, we showthat an equally accurate, but significantly more efficient surrogate model canbe built using the transfer learned potential as the ground truth. Incombination, we present a simulation workflow for machine learning potentialsthat improves data efficiency and computational efficiency.</description>
      <author>example@mail.com (Mariia Radova, Wojciech G. Stark, Connor S. Allen, Reinhard J. Maurer, Albert P. Bartók)</author>
      <guid isPermaLink="false">2502.15582v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:43 +0800</pubDate>
    </item>
    <item>
      <title>Generalization Guarantees for Representation Learning via Data-Dependent Gaussian Mixture Priors</title>
      <link>http://arxiv.org/abs/2502.15540v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted as a Spotlight Paper at ICLR 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文建立了表示学习算法的期望和尾部泛化误差界限，并通过相对熵来描述训练集和测试集中提取出的表征分布之间的差异。&lt;h4&gt;背景&lt;/h4&gt;当前对于表示学习算法的泛化误差研究较少，现有的界限无法充分反映编码器结构与简单性。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的数据依赖性对称先验（即训练和测试数据集潜在变量的最小描述长度）来建立更精确的泛化误差界限。&lt;h4&gt;方法&lt;/h4&gt;利用期望边界设计了一种合适的数据依赖正则项，并提出了同时学习数据依赖高斯混合先验并用其作为正则化的系统性方法，显示出了加权注意力机制的自然出现。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明该方法优于流行的变分信息瓶颈（VIB）和最近的类别依赖的VIB（CDVIB）。&lt;h4&gt;结论&lt;/h4&gt;新提出的界限和学习先验的方法有助于改善表示学习算法的表现。&lt;h4&gt;翻译&lt;/h4&gt;我们建立了表示学习类型算法的期望和尾部泛化误差界，这些边界以训练集和“测试”数据集中提取出的表征分布之间的相对熵来描述，并且相对于一个数据依赖性对称先验（即，对于训练和测试数据集潜在变量的最小描述长度）。我们证明了我们的界限反映了编码器的“结构”和“简单性”，并且显著改进了现有的少量边界。然后，我们将期望界用于设计合适的数据依赖正则项；并详细探讨了重要的先验选择问题。我们提出了一种系统方法，可以同时学习数据依赖高斯混合先验，并将其用作正则化器。有趣的是，在此过程中自然地出现了加权注意力机制。我们的实验表明，我们提出的方法优于流行的变分信息瓶颈（VIB）方法以及最近的类别依赖的VIB（CDVIB）方法。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We establish in-expectation and tail bounds on the generalization error ofrepresentation learning type algorithms. The bounds are in terms of therelative entropy between the distribution of the representations extracted fromthe training and "test'' datasets and a data-dependent symmetric prior, i.e.,the Minimum Description Length (MDL) of the latent variables for the trainingand test datasets. Our bounds are shown to reflect the "structure" and"simplicity'' of the encoder and significantly improve upon the few existingones for the studied model. We then use our in-expectation bound to devise asuitable data-dependent regularizer; and we investigate thoroughly theimportant question of the selection of the prior. We propose a systematicapproach to simultaneously learning a data-dependent Gaussian mixture prior andusing it as a regularizer. Interestingly, we show that a weighted attentionmechanism emerges naturally in this procedure. Our experiments show that ourapproach outperforms the now popular Variational Information Bottleneck (VIB)method as well as the recent Category-Dependent VIB (CDVIB).</description>
      <author>example@mail.com (Milad Sefidgaran, Abdellatif Zaidi, Piotr Krasnowski)</author>
      <guid isPermaLink="false">2502.15540v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>PDeepPP:A Deep learning framework with Pretrained Protein language for peptide classification</title>
      <link>http://arxiv.org/abs/2502.15610v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 5 figures, submitted to arXiv&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;蛋白质后翻译修饰（PTMs）和生物活性肽（BPs）在各种生物学过程中起着关键作用，并具有重要的治疗潜力。然而，通过实验方法识别这些位点既耗时又成本高昂。&lt;h4&gt;背景&lt;/h4&gt;现有计算工具，尤其是基于深度学习的方法，在预测PTM位点和肽类生物活性方面表现出色，但仍然面临蛋白质序列复杂性和跨不同数据集提供高质量预测的挑战。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合预训练蛋白质语言模型与融合Transformer和CNN的神经网络框架，以提高特征提取能力和预测准确性。&lt;h4&gt;方法&lt;/h4&gt;该框架应用于多项任务中，包括PTM位点及生物活性肽预测，并通过大规模数据集提升模型鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;在33项任务比较中，此模型在其中25项达到最先进水平，超越现有方法并展示出跨不同数据集的多功能性。&lt;h4&gt;结论&lt;/h4&gt;这种新方法为大规模肽类发现和PTM分析提供了可扩展且有效的方法，开启了更高效地肽类分类及功能注释的新途径。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Protein post-translational modifications (PTMs) and bioactive peptides (BPs)play critical roles in various biological processes and have significanttherapeutic potential. However, identifying PTM sites and bioactive peptidesthrough experimental methods is often labor-intensive, costly, andtime-consuming. As a result, computational tools, particularly those based ondeep learning, have become effective solutions for predicting PTM sites andpeptide bioactivity. Despite progress in this field, existing methods stillstruggle with the complexity of protein sequences and the challenge ofrequiring high-quality predictions across diverse datasets.  To address these issues, we propose a deep learning framework that integratespretrained protein language models with a neural network combining transformerand CNN for peptide classification. By leveraging the ability of pretrainedmodels to capture complex relationships within protein sequences, combined withthe predictive power of parallel networks, our approach improves featureextraction while enhancing prediction accuracy.  This framework was applied to multiple tasks involving PTM site and bioactivepeptide prediction, utilizing large-scale datasets to enhance the model'srobustness. In the comparison across 33 tasks, the model achievedstate-of-the-art (SOTA) performance in 25 of them, surpassing existing methodsand demonstrating its versatility across different datasets. Our resultssuggest that this approach provides a scalable and effective solution forlarge-scale peptide discovery and PTM analysis, paving the way for moreefficient peptide classification and functional annotation.</description>
      <author>example@mail.com (Jixiu Zhai, Tianchi Lu, Haitian Zhong, Ziyang Xu, Yuhuan Liu, Xueying Wang, Dan Huang)</author>
      <guid isPermaLink="false">2502.15610v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>P2W: From Power Traces to Weights Matrix -- An Unconventional Transfer Learning Approach</title>
      <link>http://arxiv.org/abs/2502.14968v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的非传统迁移学习方法，用于在嵌入式SoC中训练机器学习模型。该方法通过提取已部署在SoC中的现有ML模型的权重来初始化新模型，而不是直接访问这些模型。&lt;h4&gt;背景&lt;/h4&gt;随着机器学习（ML）模型在嵌入式芯片系统（SoCs）上的快速部署，医疗保健和自动驾驶汽车等领域发生了变革性的变化。然而，在这些场景中训练嵌入式ML模型的一个主要挑战是缺乏高质量的公开训练数据。&lt;h4&gt;目的&lt;/h4&gt;解决现有迁移学习方法需要直接访问已存在模型这一限制问题，特别是在嵌入式SoC上运行的情况下。&lt;h4&gt;方法&lt;/h4&gt;通过从执行机器学习模型的SoC捕获功耗测量值并将这些值转换为权重矩阵来初始化新的ML模型。这种方法不需要直接获取嵌入式系统中的现有模型的具体信息。&lt;h4&gt;主要发现&lt;/h4&gt;新方法可以显著提高在数据稀缺环境下的模型准确性和预测性能，相比传统训练方式，在使用相同数量的受限训练数据的情况下，可以使新模型的准确性提升高达3倍。&lt;h4&gt;结论&lt;/h4&gt;提出的方法提供了一种有效的途径来利用现有的嵌入式ML模型的知识，以改进新模型的学习效率和预测表现。这种方法对于那些难以直接访问已有模型场景下的机器学习应用来说特别有用。&lt;h4&gt;翻译&lt;/h4&gt;摘要：在芯片上的系统（SoCs）上部署的机器学习（ML）模型数量快速增长已经对医疗保健、自动驾驶汽车等领域带来了变革性的变化。然而，在这些领域训练嵌入式ML模型的一个主要挑战是缺乏高质量的公共可用训练数据。迁移学习方法通过利用现有ML模型中的知识作为起点来应对这一挑战，从而用于训练新的ML模型。但是，现有的迁移学习方法需要直接访问现有的模型，这在许多情况下是不可行的，尤其是在部署在嵌入式SoC上的ML模型的情况下。因此，在本文中，我们提出了一种新颖的方法：通过提取并使用一个运行在嵌入式SoC中的现有ML模型的权重来训练一个新的ML模型，而不需要直接访问该模型。我们的方法采集了执行ML模型时从SoC捕捉到的能量消耗测量值，并将其转换为用于初始化新ML模型的大致权重矩阵。这提高了新模型的学习效率和预测性能，尤其是在可用来培训模型的数据量有限的情况下。与使用相同数量的受限训练数据的传统训练方式相比，我们新颖的方法可以有效提高新ML模型的准确度高达3倍。&lt;h4&gt;其他信息&lt;/h4&gt;{'关键词': ['迁移学习', '嵌入式SoC', '机器学习', '功耗测量', '权重初始化']}&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid growth of deploying machine learning (ML) models within embeddedsystems on a chip (SoCs) has led to transformative shifts in fields likehealthcare and autonomous vehicles. One of the primary challenges for trainingsuch embedded ML models is the lack of publicly available high-quality trainingdata. Transfer learning approaches address this challenge by utilizing theknowledge encapsulated in an existing ML model as a starting point for traininga new ML model. However, existing transfer learning approaches require directaccess to the existing model which is not always feasible, especially for MLmodels deployed on embedded SoCs. Therefore, in this paper, we introduce anovel unconventional transfer learning approach to train a new ML model byextracting and using weights from an existing ML model running on an embeddedSoC without having access to the model within the SoC. Our approach capturespower consumption measurements from the SoC while it is executing the ML modeland translates them to an approximated weights matrix used to initialize thenew ML model. This improves the learning efficiency and predictive performanceof the new model, especially in scenarios with limited data available to trainthe model. Our novel approach can effectively increase the accuracy of the newML model up to 3 times compared to classical training methods using the sameamount of limited training data.</description>
      <author>example@mail.com (Roozbeh Siyadatzadeh, Fatemeh Mehrafrooz, Nele Mentens, Todor Stefanov)</author>
      <guid isPermaLink="false">2502.14968v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>A Novel Riemannian Sparse Representation Learning Network for Polarimetric SAR Image Classification</title>
      <link>http://arxiv.org/abs/2502.15302v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  13 pages, 9 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的Riemannian稀疏表示学习网络(SRSR CNN)用于极化合成孔径雷达(PolSAR)图像分类。&lt;h4&gt;背景&lt;/h4&gt;深度学习是Polarimetric SAR图像分类的有效方法，但缺乏相关的数学原理指导，并且通常将复数协方差矩阵转换为欧几里得空间中的向量输入，这可能破坏了矩阵结构和通道关系。&lt;h4&gt;目的&lt;/h4&gt;通过引入Riemannian度量来更好地处理PolSAR的复杂矩阵结构，以提高分类准确性和边缘细节准确性。&lt;h4&gt;方法&lt;/h4&gt;设计了一个基于超像素的Riemannian稀疏表示模型(SRSR)，该模型能够在黎曼空间中学习几何结构和稀疏特征。然后将其展开为一个网络，可以自动地学习稀疏系数和字典原子，并添加了CNN增强模块以提高上下文高级特征的学习能力。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明所提出的方法在保持准确的边缘细节和正确的区域同质性方面优于现有的方法。&lt;h4&gt;结论&lt;/h4&gt;提出的基于SR指导的深度学习模型可以直接使用协方差矩阵作为网络输入，并且可以利用黎曼度量来学习复数矩阵在黎曼空间中的几何结构和稀疏特征。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Deep learning is an effective end-to-end method for Polarimetric SyntheticAperture Radar(PolSAR) image classification, but it lacks the guidance ofrelated mathematical principle and is essentially a black-box model. Inaddition, existing deep models learn features in Euclidean space, where PolSARcomplex matrix is commonly converted into a complex-valued vector as thenetwork input, distorting matrix structure and channel relationship. However,the complex covariance matrix is Hermitian positive definite (HPD), and resideson a Riemannian manifold instead of a Euclidean one. Existing methods cannotmeasure the geometric distance of HPD matrices and easily cause somemisclassifications due to inappropriate Euclidean measures. To address theseissues, we propose a novel Riemannian Sparse Representation Learning Network(SRSR CNN) for PolSAR images. Firstly, a superpixel-based Riemannian SparseRepresentation (SRSR) model is designed to learn the sparse features withRiemannian metric. Then, the optimization procedure of the SRSR model isinferred and further unfolded into an SRSRnet, which can automatically learnthe sparse coefficients and dictionary atoms. Furthermore, to learn contextualhigh-level features, a CNN-enhanced module is added to improve classificationperformance. The proposed network is a Sparse Representation (SR) guided deeplearning model, which can directly utilize the covariance matrix as the networkinput, and utilize Riemannian metric to learn geometric structure and sparsefeatures of complex matrices in Riemannian space. Experiments on three realPolSAR datasets demonstrate that the proposed method surpasses state-of-the-arttechniques in ensuring accurate edge details and correct region homogeneity forclassification.</description>
      <author>example@mail.com (Junfei Shi, Mengmeng Nie, Weisi Lin, Haiyan Jin, Junhuai Li, Rui Wang)</author>
      <guid isPermaLink="false">2502.15302v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Depth-aware Fusion Method based on Image and 4D Radar Spectrum for 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.15516v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文讨论了毫米波雷达和相机在自动驾驶环境感知中的互补作用，通过结合4D毫米波雷达和深度感知的摄像机图像来提高3D物体检测精度。&lt;h4&gt;背景&lt;/h4&gt;安全性和可靠性对于公众接受自动驾驶至关重要。传统的3D毫米波雷达只能提供目标的距离、多普勒频移及方位信息，在恶劣天气条件下仍能保持良好性能，但点云稀疏。&lt;h4&gt;目的&lt;/h4&gt;本研究旨在通过结合4D毫米波雷达和相机的优势来增强环境感知的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;['利用4D毫米波雷达提供的深度感知数据与相机图像进行融合，采用注意机制在鸟瞰图视角下将丰富的纹理图像与深度信息相结合', '提出了一种基于GAN的方法，在没有深度传感器的情况下从雷达频谱生成深度图像']&lt;h4&gt;主要发现&lt;/h4&gt;['通过结合4D毫米波雷达和相机可以有效提升环境感知的准确性，特别是在恶劣天气条件下。', '使用注意力机制能够更好地融合不同类型的感知数据，提高3D物体检测精度。', '基于GAN的方法有助于在缺乏直接深度信息的情况下生成有效的深度图像']&lt;h4&gt;结论&lt;/h4&gt;该研究证明了将4D毫米波雷达与相机结合可以显著提升自动驾驶系统的环境感知能力，并提出了一种新颖的解决方案来克服现有技术局限性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Safety and reliability are crucial for the public acceptance of autonomousdriving. To ensure accurate and reliable environmental perception, intelligentvehicles must exhibit accuracy and robustness in various environments.Millimeter-wave radar, known for its high penetration capability, can operateeffectively in adverse weather conditions such as rain, snow, and fog.Traditional 3D millimeter-wave radars can only provide range, Doppler, andazimuth information for objects. Although the recent emergence of 4Dmillimeter-wave radars has added elevation resolution, the radar point cloudsremain sparse due to Constant False Alarm Rate (CFAR) operations. In contrast,cameras offer rich semantic details but are sensitive to lighting and weatherconditions. Hence, this paper leverages these two highly complementary andcost-effective sensors, 4D millimeter-wave radar and camera. By integrating 4Dradar spectra with depth-aware camera images and employing attentionmechanisms, we fuse texture-rich images with depth-rich radar data in theBird's Eye View (BEV) perspective, enhancing 3D object detection. Additionally,we propose using GAN-based networks to generate depth images from radar spectrain the absence of depth sensors, further improving detection accuracy.</description>
      <author>example@mail.com (Yue Sun, Yeqiang Qian, Chunxiang Wang, Ming Yang)</author>
      <guid isPermaLink="false">2502.15516v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Decoding lithium's subtle phase stability with a machine learning force field</title>
      <link>http://arxiv.org/abs/2502.15190v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究了锂金属在锂-金属电池阳极中的相稳定性，揭示了量子效应和非谐性对锂的热力学性质的重要性。&lt;h4&gt;背景&lt;/h4&gt;锂金属在作为锂电池阳极材料时表现出复杂的多晶型特性，这对优化其性能至关重要。然而，这种看似简单的金属具有平坦的能量地形图，需要考虑量子效应、声子重整化和热膨胀效应来准确描述。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于图形神经网络的机器学习势场，并进行高效自洽声子计算，以研究在近环境条件下bcc-，fcc-和9R-Li相的稳定性。&lt;h4&gt;方法&lt;/h4&gt;利用了图神经网络机器学习力场以及声子重整化效应并考虑量子、热力学膨胀影响下的自洽声子计算来模拟锂的不同晶型结构。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明非谐性在决定Li的热力学性质中起着重要作用。fcc-Li在零温度和压力下被确认为基态，预测的bcc-fcc相边界与实验相变线具有定性的匹配。&lt;h4&gt;结论&lt;/h4&gt;这些研究提供了锂多晶型复杂特性的关键见解，并建立了一种有效的计算方法来模拟更现实条件下锂的大规模原子尺度仿真，以支持实际能源存储应用。&lt;h4&gt;翻译&lt;/h4&gt;理解元素锂的相稳定性对于优化其在锂-金属电池阳极中的性能至关重要。然而，这种看似简单的金属表现出复杂的多晶型特性，需要准确考虑量子效应和非谐性来捕捉平坦能量地形图中的细微差别。为了应对这一挑战，我们开发了一种基于图形神经网络的机器学习力场，并进行了高效的自洽声子计算，在近环境条件下对bcc-、fcc-和9R-Li进行了研究，将量子效应、声子重整化以及热膨胀效应结合考虑在内。我们的研究表明非谐性在决定锂的热力学性质中起着重要作用。这些相之间的自由能差值（特别是fcc-与9R-Li）仅几毫电子伏特/原子，解释了实验上难以获得纯相样品的问题，并暗示了堆垛层错和相关缺陷形成的可能性。在零温度和压力下确认了fcc-Li为基态，预测的bcc-fcc相边界虽然低估了相变温度和压力斜率，但与实验相变线具有定性的匹配。这些发现提供了锂复杂多晶型的关键见解，并建立了一种有效的计算方法来模拟更现实条件下锂的大规模原子尺度仿真，以支持实际能源存储应用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1039/D4TA08860C&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding the phase stability of elemental lithium (Li) is crucial foroptimizing its performance in lithium-metal battery anodes, yet this seeminglysimple metal exhibits complex polymorphism that requires proper accounting forquantum and anharmonic effects to capture the subtleties in its flat energylandscape. Here we address this challenge by developing an accurate graphneural network-based machine learning force field and performing efficientself-consistent phonon calculations for bcc-, fcc-, and 9R-Li undernear-ambient conditions, incorporating quantum, phonon renormalization andthermal expansion effects. Our results reveal the important role ofanharmonicity in determining Li's thermodynamic properties. The free energydifferences between these phases, particularly fcc- and 9R-Li are found to beonly a few meV/atom, explaining the experimental challenges in obtainingphase-pure samples and suggesting a propensity for stacking faults and relateddefect formation. fcc-Li is confirmed as the ground state at zero temperatureand pressure, and the predicted bcc-fcc phase boundary qualitatively matchesexperimental phase transition lines, despite overestimation of the transitiontemperature and pressure slope. These findings provide crucial insights intoLi's complex polymorphism and establish an effective computational approach forlarge-scale atomistic simulations of Li in more realistic settings forpractical energy storage applications.</description>
      <author>example@mail.com (Yiheng Shen, Wei Xie)</author>
      <guid isPermaLink="false">2502.15190v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>PFSD: A Multi-Modal Pedestrian-Focus Scene Dataset for Rich Tasks in Semi-Structured Environments</title>
      <link>http://arxiv.org/abs/2502.15342v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一个新的多模态行人场景数据集PFSD，旨在解决半结构化环境中行人感知和预测的挑战。同时提出了一种新的混合多尺度融合网络(HMFN)，以提高在复杂半结构化环境中的3D行人检测性能。&lt;h4&gt;背景&lt;/h4&gt;当前自动驾驶感知系统在处理车辆主导的结构化环境时表现出色，但在存在更多动态行人的半结构化环境中表现不佳，这主要是由于高质量数据集的缺乏，特别是在涉及行人场景的数据集中。&lt;h4&gt;目的&lt;/h4&gt;开发一个多模态、全面标注的数据集PFSD，并提出了一种新的方法HMFN来应对半结构化环境中行人检测和预测的问题。&lt;h4&gt;方法&lt;/h4&gt;创建了一个包含130,000多个人行实例的PFSD数据集，该数据集在nuScenes格式下对点云进行了详细的分割、检测和对象ID跟踪。此外，提出了一个混合多尺度融合网络(HMFN)，用于处理高密度人群场景中的行人检测问题。&lt;h4&gt;主要发现&lt;/h4&gt;HMFN通过捕获并融合多种规模的特征显著提高了3D行人检测的性能，在PFSD数据集上达到了更高的平均精度(mAP)。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了高质量的数据和先进的算法在解决半结构化环境中行人感知挑战方面的必要性，并为未来的相关工作奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;最近，自主驾驶领域的感知技术取得了显著进展，尤其是在以车辆为主的结构化环境中。然而，目前的感知模型在半结构化的场景中面临重大局限性，特别是在存在动态行人的复杂和多样运动模式以及遮挡情况下表现不佳。我们归因于高质量数据集的缺乏，尤其是关于行人感知的数据集。在此研究中，我们提出了一个多模态行人焦点场景数据集PFSD，在nuScenes格式下为半结构化的场景提供了全面的多模态数据标注，包括点云分割、检测和对象ID用于追踪。该数据集涵盖了超过130,000个在各种情况下捕捉到的行人实例，包括不同的密度、移动模式及遮挡情况。为了证明解决多样且复杂的半结构化环境中的挑战的重要性，我们提出了一种新颖的混合多尺度融合网络(HMFN)。具体而言，在处理高密度人群场景中的行人检测时，我们的方法通过精心设计的混合框架有效捕捉并融合了多种规模的特征，该框架集成了稀疏和常规卷积。在PFSD上的广泛实验表明，HMFN在平均精度(mAP)上优于现有的方法，从而证明其在复杂半结构化环境中3D行人检测方面的有效性。代码和基准测试可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advancements in autonomous driving perception have revealedexceptional capabilities within structured environments dominated by vehiculartraffic. However, current perception models exhibit significant limitations insemi-structured environments, where dynamic pedestrians with more diverseirregular movement and occlusion prevail. We attribute this shortcoming to thescarcity of high-quality datasets in semi-structured scenes, particularlyconcerning pedestrian perception and prediction. In this work, we present themulti-modal Pedestrian-Focused Scene Dataset(PFSD), rigorously annotated insemi-structured scenes with the format of nuScenes. PFSD provides comprehensivemulti-modal data annotations with point cloud segmentation, detection, andobject IDs for tracking. It encompasses over 130,000 pedestrian instancescaptured across various scenarios with varying densities, movement patterns,and occlusions. Furthermore, to demonstrate the importance of addressing thechallenges posed by more diverse and complex semi-structured environments, wepropose a novel Hybrid Multi-Scale Fusion Network (HMFN). Specifically, todetect pedestrians in densely populated and occluded scenarios, our methodeffectively captures and fuses multi-scale features using a meticulouslydesigned hybrid framework that integrates sparse and vanilla convolutions.Extensive experiments on PFSD demonstrate that HMFN attains improvement in meanAverage Precision (mAP) over existing methods, thereby underscoring itsefficacy in addressing the challenges of 3D pedestrian detection in complexsemi-structured environments. Coding and benchmark are available.</description>
      <author>example@mail.com (Yueting Liu, Hanshi Wang, Yunfei Lei, Zhengjun Zha, Weiming Hu, Jin Gao)</author>
      <guid isPermaLink="false">2502.15342v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking machine learning for bowel sound pattern classification from tabular features to pretrained models</title>
      <link>http://arxiv.org/abs/2502.15607v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  9 pages, 6 figures and 1 table&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文探讨了电子听诊器和可穿戴记录传感器的发展如何推动了肠鸣音信号的自动化分析，从而通过数据驱动的方法研究肠鸣音模式及其与不同病理的关系。&lt;h4&gt;背景&lt;/h4&gt;随着电子听诊器和可穿戴记录设备的进步，可以自动分析肠鸣音（BS）信号，这为基于数据的研究肠鸣音模式、它们之间的相互关系以及它们与各种疾病的相关性提供了可能性。&lt;h4&gt;目的&lt;/h4&gt;利用来自16名健康受试者的标注良好的BS数据集来评估机器学习模型在检测和/或分类BS模式方面的性能。&lt;h4&gt;方法&lt;/h4&gt;该研究使用了基于表格特征的模型，以光谱图为输入的卷积神经网络（CNN），以及预训练的大规模音频数据集上的模型，并对其进行了性能评估。&lt;h4&gt;主要发现&lt;/h4&gt;结果表明，特别是对于样本较少的类别的检测任务中，预训练模型表现出了明显的优越性。使用HuBERT模型在区分肠鸣音信号和非肠鸣音信号上实现了AUC为0.89；使用Wav2Vec 2.0模型在不同肠鸣音模式之间区分时也达到了AUC为0.89。&lt;h4&gt;结论&lt;/h4&gt;这些结果为进一步理解肠鸣音及其潜在的机器学习辅助诊断应用铺平了道路，特别是在胃肠检查方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The development of electronic stethoscopes and wearable recording sensorsopened the door to the automated analysis of bowel sound (BS) signals. Thisenables a data-driven analysis of bowel sound patterns, their interrelations,and their correlation to different pathologies. This work leverages a BSdataset collected from 16 healthy subjects that was annotated according to fourestablished BS patterns. This dataset is used to evaluate the performance ofmachine learning models to detect and/or classify BS patterns. The selection ofconsidered models covers models using tabular features, convolutional neuralnetworks based on spectrograms and models pre-trained on large audio datasets.The results highlight the clear superiority of pre-trained models, particularlyin detecting classes with few samples, achieving an AUC of 0.89 indistinguishing BS from non-BS using a HuBERT model and an AUC of 0.89 indifferentiating bowel sound patterns using a Wav2Vec 2.0 model. These resultspave the way for an improved understanding of bowel sounds in general andfuture machine-learning-driven diagnostic applications for gastrointestinalexaminations</description>
      <author>example@mail.com (Zahra Mansour, Verena Uslar, Dirk Weyhe, Danilo Hollosi, Nils Strodthoff)</author>
      <guid isPermaLink="false">2502.15607v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>GiGL: Large-Scale Graph Neural Networks at Snapchat</title>
      <link>http://arxiv.org/abs/2502.15054v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;论文介绍了GiGL，一个用于大规模分布式图机器学习的开源库。&lt;h4&gt;背景&lt;/h4&gt;随着图神经网络（GNNs）的发展，它们在商业应用中的兴趣日益增长。然而，由于规模挑战，工业界对GNNs的应用仍然落后于研究领域。&lt;h4&gt;目的&lt;/h4&gt;分享Snapchat采用GiGL进行大规模分布式图机器学习的方法和经验。&lt;h4&gt;方法&lt;/h4&gt;开发了GiGL库以解决大型社交数据上的图形ML的规模化问题，并简化内部实践者在建模方面的工作，同时支持与学术界常用的开源GNN建模库（如PyTorch Geometric）的接口。&lt;h4&gt;主要发现&lt;/h4&gt;GiGL已在多个生产环境中使用，在过去的两年中推动了超过35个跨多种业务领域的发布，包括好友推荐、内容推荐和广告领域。&lt;h4&gt;结论&lt;/h4&gt;论文详细描述了GiGL的设计、提供的工具、缩放属性以及在大规模社交图上的应用案例研究，并总结了一些关键经验教训。&lt;h4&gt;翻译&lt;/h4&gt;近期的图机器学习（ML）进展引入了图形神经网络（GNNs），这引发了将这些方法应用于商业规模应用的兴趣。GNNs使根据给定的图结构进行端到端(E2E)模型参数微分学习成为可能，从而优化流行节点、边（链接）和图级任务的目标函数。虽然在新GNN层和训练策略方面取得了迅速的研究创新，但由于大规模图形ML问题所特有的规模挑战，工业界对GNNs的应用明显滞后。在这项工作中，我们分享了Snapchat在培训、推理以及利用GNN时的方法。为此，我们介绍了GiGL（Gigantic Graph Learning），这是一个开源库，旨在使大型分布式图机器学习能够服务于研究人员、ML工程师和实践者的需求。我们在内部使用GiGL来处理GNN工作流程的繁重任务，包括从关系数据库中进行图数据预处理、子图采样、分布式训练、推理以及编排。GiGL的设计目的是清晰地与学术界常用的开源GNN建模库（如PyTorch Geometric）接口，并解决规模和生产化挑战，以使内部实践者能够专注于模型构建。GiGL在多个生产环境中使用，在过去的两年中推动了超过35个跨多种业务领域的发布，包括好友推荐、内容推荐以及广告领域。本工作详细描述了该库的高级设计和工具提供情况，扩展特性，并在各种行业规模图中的实际应用案例研究，以及在大规模社交数据上采用图形ML的关键经验教训。GiGL在https://github.com/snap-research/GiGL开源可用。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent advances in graph machine learning (ML) with the introduction of GraphNeural Networks (GNNs) have led to a widespread interest in applying theseapproaches to business applications at scale. GNNs enable differentiableend-to-end (E2E) learning of model parameters given graph structure whichenables optimization towards popular node, edge (link) and graph-level tasks.While the research innovation in new GNN layers and training strategies hasbeen rapid, industrial adoption and utility of GNNs has lagged considerably dueto the unique scale challenges that large-scale graph ML problems create. Inthis work, we share our approach to training, inference, and utilization ofGNNs at Snapchat. To this end, we present GiGL (Gigantic Graph Learning), anopen-source library to enable large-scale distributed graph ML to the benefitof researchers, ML engineers, and practitioners. We use GiGL internally atSnapchat to manage the heavy lifting of GNN workflows, including graph datapreprocessing from relational DBs, subgraph sampling, distributed training,inference, and orchestration. GiGL is designed to interface cleanly withopen-source GNN modeling libraries prominent in academia like PyTorch Geometric(PyG), while handling scaling and productionization challenges that make iteasier for internal practitioners to focus on modeling. GiGL is used inmultiple production settings, and has powered over 35 launches across multiplebusiness domains in the last 2 years in the contexts of friend recommendation,content recommendation and advertising. This work details high-level design andtools the library provides, scaling properties, case studies in diversebusiness settings with industry-scale graphs, and several key lessons learnedin employing graph ML at scale on large social data. GiGL is open-sourced athttps://github.com/snap-research/GiGL.</description>
      <author>example@mail.com (Tong Zhao, Yozen Liu, Matthew Kolodner, Kyle Montemayor, Elham Ghazizadeh, Ankit Batra, Zihao Fan, Xiaobin Gao, Xuan Guo, Jiwen Ren, Serim Park, Peicheng Yu, Jun Yu, Shubham Vij, Neil Shah)</author>
      <guid isPermaLink="false">2502.15054v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Realm: Real-Time Line-of-Sight Maintenance in Multi-Robot Navigation with Unknown Obstacles</title>
      <link>http://arxiv.org/abs/2502.15162v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  8 pages, 9 figures, accepted by IEEE ICRA 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的多机器人导航框架，该框架在未知且复杂环境中通过实时点云测量来维护线视（LoS）连接约束。提出了一个基于点云可见性分析的新LoS距离度量方法，并设计了融合函数以确保机器人之间的协作移动和保持LoS连接。&lt;h4&gt;背景&lt;/h4&gt;多机器人系统在复杂环境中的导航需要依赖于机器人之间的通信与相互观测，而以前的研究工作大多是在已知的环境中进行的，难以应用于未知且复杂的场景中。&lt;h4&gt;目的&lt;/h4&gt;研究解决未知复杂环境中多机器人导航问题的方法，并提出一种新的LoS距离度量方法和融合函数来保持机器人的连接性。&lt;h4&gt;方法&lt;/h4&gt;通过实时点云测量直接定义了机器人之间的线视（LoS）约束，利用点云可见性分析技术量化了由于潜在的机器人移动而可能失去LoS的重要性与敏感程度。设计了一个新的融合函数以确保两个机器人之间丢失LoS的需求平衡，并将LoS约束编码到势能函数中。&lt;h4&gt;主要发现&lt;/h4&gt;提出了新颖的基于点云的LoS距离度量方法，能够同时考虑保持连接性和紧急性；设计了一种新的融合功能来处理两台机器人的紧迫感不均衡的问题；实现了结合上述方法的多机器人探索框架，并通过分布式传感和通信确保了未知环境下的持续导航。&lt;h4&gt;结论&lt;/h4&gt;该研究提出的框架在复杂且未知的环境中，通过点云测量实现LoS约束的有效维护，增强了机器人的合作能力和实时感知能力。此成果对于未来自主系统的开发具有重要价值。&lt;h4&gt;翻译&lt;/h4&gt;多机器人系统在复杂环境中的导航需要依靠机器人之间的通信和相互观察来协调并提高态势感知能力。本文研究了未知环境中基于视距（LoS）连接限制的多机器人导航问题，而以前的工作仅限于从已知的环境模型中推导出LoS约束条件，本论文通过实时点云测量直接建立了这些约束，并利用点云可见性分析技术来实现这一点。我们提出了一种新的LoS距离度量方法，该方法可以量化由于潜在的机器人移动而可能失去视距的重要性和敏感性。此外，为了应对两个机器人之间丢失视距时紧迫感不均衡的问题，设计了一个融合功能以捕捉整体紧迫感并生成有利于保持视距的协作运动梯度。&lt;h4&gt;开源链接&lt;/h4&gt;https://github.com/bairuofei/LoS_constrained_navigation&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-robot navigation in complex environments relies on inter-robotcommunication and mutual observations for coordination and situationalawareness. This paper studies the multi-robot navigation problem in unknownenvironments with line-of-sight (LoS) connectivity constraints. While previousworks are limited to known environment models to derive the LoS constraints,this paper eliminates such requirements by directly formulating the LoSconstraints between robots from their real-time point cloud measurements,leveraging point cloud visibility analysis techniques. We propose a novelLoS-distance metric to quantify both the urgency and sensitivity of losing LoSbetween robots considering potential robot movements. Moreover, to address theimbalanced urgency of losing LoS between two robots, we design a fusionfunction to capture the overall urgency while generating gradients thatfacilitate robots' collaborative movement to maintain LoS. The LoS constraintsare encoded into a potential function that preserves the positivity of theFiedler eigenvalue of the robots' network graph to ensure connectivity.Finally, we establish a LoS-constrained exploration framework that integratesthe proposed connectivity controller. We showcase its applications inmulti-robot exploration in complex unknown environments, where robots canalways maintain the LoS connectivity through distributed sensing andcommunication, while collaboratively mapping the unknown environment. Theimplementations are open-sourced athttps://github.com/bairuofei/LoS_constrained_navigation.</description>
      <author>example@mail.com (Ruofei Bai, Shenghai Yuan, Kun Li, Hongliang Guo, Wei-Yun Yau, Lihua Xie)</author>
      <guid isPermaLink="false">2502.15162v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Accurate and efficient machine learning interatomic potentials for finite temperature modeling of molecular crystals</title>
      <link>http://arxiv.org/abs/2502.15530v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了机器学习（ML）势能模型在分子晶体计算中的应用，特别是用于准确高效地计算升华焓。通过利用化学和材料科学领域的基础模型以及量子扩散蒙特卡洛基准测试的最新进展，该研究展示了使用少量高质量数据结构生成高精度MLIP的能力。&lt;h4&gt;背景&lt;/h4&gt;机器学习势能（MLIP）在模拟分子晶体方面具有革命性意义，但准确高效地计算升华焓仍面临挑战。现有方法需要大量的高精度参考结构，并且依赖于可能不够可靠的密度泛函理论来产生这些数据。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够使用较少的数据生成准确的机器学习势能模型的方法，以改进分子晶体在有限温度和压力下的描述能力。&lt;h4&gt;方法&lt;/h4&gt;利用化学和材料科学领域的基础模型以及量子扩散蒙特卡洛（QDMC）基准测试，通过训练数据集中的约200个高质量结构来构建MLIP模型。该框架还考虑了非谐性和核量子效应，并应用于X23数据集。&lt;h4&gt;主要发现&lt;/h4&gt;生成的机器学习势能模型在计算升华焓时达到了次化学精度水平，甚至可以推广到具有药物相关性的晶体系统中，准确捕捉核量子效应（如对天青酸的研究）。&lt;h4&gt;结论&lt;/h4&gt;这项工作为研究药理学和生物学系统的环境条件下的精确模拟铺平了道路。通过减少所需的数据量，它不仅提高了计算效率，还提供了关于药物分子稳定性的重要见解。&lt;h4&gt;翻译&lt;/h4&gt;机器学习在模拟分子晶体方面的潜力巨大，但准确计算升华焓仍存在挑战。本文提出了一种新方法，使用更少的高质量数据结构生成精确的MLIP模型，并通过X23数据集验证了该框架的有效性及对药物相关系统的适用性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As with many parts of the natural sciences, machine learning interatomicpotentials (MLIPs) are revolutionizing the modeling of molecular crystals.However, challenges remain for the accurate and efficient calculation ofsublimation enthalpies - a key thermodynamic quantity measuring the stabilityof a molecular crystal. Specifically, two key stumbling blocks are: (i) theneed for thousands of ab initio quality reference structures to generatetraining data; and (ii) the sometimes unreliable nature of density functionaltheory, the main technique for generating such data. Exploiting recentdevelopments in foundational models for chemistry and materials sciencealongside accurate quantum diffusion Monte Carlo benchmarks, offers a promisingpath forward. Herein, we demonstrate the generation of MLIPs capable ofdescribing molecular crystals at finite temperature and pressure withsub-chemical accuracy, using as few as $\sim 200$ data structures; an order ofmagnitude improvement over the current state-of-the-art. We apply thisframework to compute the sublimation enthalpies of the X23 dataset, accountingfor anharmonicity and nuclear quantum effects, achieving sub-chemical accuracywith respect to experiment. Importantly, we show that our framework can begeneralized to crystals of pharmaceutical relevance, including paracetamol andaspirin. Nuclear quantum effects are also accurately captured as shown for thecase of squaric acid. By enabling accurate modeling at ambient conditions, thiswork paves the way for deeper insights into pharmaceutical and biologicalsystems.</description>
      <author>example@mail.com (Flaviano Della Pia, Benjamin X. Shi, Venkat Kapil, Andrea Zen, Dario Alfè, Angelos Michaelides)</author>
      <guid isPermaLink="false">2502.15530v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Benchmarking Android Malware Detection: Rethinking the Role of Traditional and Deep Learning Models</title>
      <link>http://arxiv.org/abs/2502.15041v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  12 pages, 5 figures&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文系统地评估了Android恶意软件检测模型，比较了传统的机器学习和深度学习方法在不同数据集上的表现。&lt;h4&gt;背景&lt;/h4&gt;近年来，使用传统机器学习（ML）和深度学习（DL）技术对Android恶意软件进行检测的研究得到了广泛的关注。然而，尽管基于DL的方法声称具有优越的性能，它们往往依赖于有限的对比测试，并且缺乏与传统ML模型在多样化数据集上的全面基准比较。&lt;h4&gt;目的&lt;/h4&gt;通过使用四个不同数据集来评估不同的Android恶意软件检测模型（包括传统的随机森林和CatBoost以及先进的Capsule Graph Neural Networks等），论文旨在探讨各种模型的表现，并为未来的研究提供更加全面的基准测试。&lt;h4&gt;方法&lt;/h4&gt;该研究实施了一系列传统机器学习模型，如Random Forests (RF) 和 CatBoost，同时对比了先进的深度学习模型，例如CapsGNN、BERT和ExcelFormer等。使用的数据集包括三个最近发布的公开可用的数据集以及一个大规模的数据集（作者系统地收集的）。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，尽管高级DL模型可以实现强大的性能，但它们通常只与少量的传统ML基线进行比较。在许多情况下，更简单且计算效率更高的传统ML模型实现了可比甚至更好的性能。&lt;h4&gt;结论&lt;/h4&gt;论文强调了Android恶意软件检测研究中需要更加严格基准测试的重要性，并建议未来的研究应进行全面的对比研究以确保对检测能力的准确评估。此外，为促进进一步的研究，作者提供了其数据集的访问权限。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的英文原文&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Android malware detection has been extensively studied using both traditionalmachine learning (ML) and deep learning (DL) approaches. While manystate-of-the-art detection models, particularly those based on DL, claimsuperior performance, they often rely on limited comparisons, lackingcomprehensive benchmarking against traditional ML models across diversedatasets. This raises concerns about the robustness of DL-based approaches'performance and the potential oversight of simpler, more efficient ML models.In this paper, we conduct a systematic evaluation of Android malware detectionmodels across four datasets: three recently published, publicly availabledatasets and a large-scale dataset we systematically collected. We implement arange of traditional ML models, including Random Forests (RF) and CatBoost,alongside advanced DL models such as Capsule Graph Neural Networks (CapsGNN),BERT-based models, and ExcelFormer based models. Our results reveal that whileadvanced DL models can achieve strong performance, they are often comparedagainst an insufficient number of traditional ML baselines. In many cases,simpler and more computationally efficient ML models achieve comparable or evensuperior performance. These findings highlight the need for rigorousbenchmarking in Android malware detection research. We encourage future studiesto conduct more comprehensive benchmarking comparisons between traditional andadvanced models to ensure a more accurate assessment of detection capabilities.To facilitate further research, we provide access to our dataset, including appIDs, hash values, and labels.</description>
      <author>example@mail.com (Guojun Liu, Doina Caragea, Xinming Ou, Sankardas Roy)</author>
      <guid isPermaLink="false">2502.15041v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Mitigating Data Scarcity in Time Series Analysis: A Foundation Model with Series-Symbol Data Generation</title>
      <link>http://arxiv.org/abs/2502.15466v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一个针对时间序列分析（TSA）的数据生成机制和预训练模型，旨在克服数据稀缺性和不平衡的问题。&lt;h4&gt;背景&lt;/h4&gt;在时间序列分析领域，基础模型正受到越来越多的关注。然而，由于数据稀缺和数据不平衡等问题的存在，其发展受到了阻碍。&lt;h4&gt;目的&lt;/h4&gt;为了解决这些问题，研究者们提出了一种新的方法来建模复杂系统，并引入了一系列-符号（S2）双模式数据生成机制。&lt;h4&gt;方法&lt;/h4&gt;通过这种机制，可以无限制地创建高质量的时间序列数据并配以相应的符号表示。基于这些数据，他们开发了SymTime预训练基础模型。&lt;h4&gt;主要发现&lt;/h4&gt;SymTime在五个主要时间序列分析任务中表现出竞争力，并且其性能与直接用真实世界数据集进行预训练的基础模型相媲美。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了双模态数据生成和预训练机制在克服数据稀缺性、提升任务性能方面的潜力。&lt;h4&gt;翻译&lt;/h4&gt;基础模型用于时间序列分析（TSA）吸引了大量的关注。然而，诸如数据稀缺和不平衡等问题依然阻碍着其发展。为解决这些问题，我们考虑通过符号表达式来建模复杂系统，这些符号表达式可以作为时间序列的语义描述符。在此基础上，我们引入了一种系列-符号（S2）双模式性数据生成机制，能够无限制地创建高质量的时间序列数据及其相应的符号表示。利用S2数据集，我们开发了SymTime，这是一个为TSA设计的预训练基础模型。当在下游任务中进行微调时，SymTime在五个主要TSA任务中的表现是竞争性的，并且其性能可以与直接基于真实世界数据集预训练的基础模型相媲美。这种方法强调了双模态数据生成和预训练机制在克服数据稀缺性和提升任务性能方面的潜力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Foundation models for time series analysis (TSA) have attracted significantattention. However, challenges such as data scarcity and data imbalancecontinue to hinder their development. To address this, we consider modelingcomplex systems through symbolic expressions that serve as semantic descriptorsof time series. Building on this concept, we introduce a series-symbol (S2)dual-modulity data generation mechanism, enabling the unrestricted creation ofhigh-quality time series data paired with corresponding symbolicrepresentations. Leveraging the S2 dataset, we develop SymTime, a pre-trainedfoundation model for TSA. SymTime demonstrates competitive performance acrossfive major TSA tasks when fine-tuned with downstream task, rivaling foundationmodels pre-trained on real-world datasets. This approach underscores thepotential of dual-modality data generation and pretraining mechanisms inovercoming data scarcity and enhancing task performance.</description>
      <author>example@mail.com (Wenxuan Wang, Kai Wu, Yujian Betterest Li, Dan Wang, Xiaoyu Zhang, Jing Liu)</author>
      <guid isPermaLink="false">2502.15466v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Graph in the Vault: Protecting Edge GNN Inference with Trusted Execution Environment</title>
      <link>http://arxiv.org/abs/2502.15012v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This work is accepted by DAC 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种名为GNNVault的安全策略，用于在边缘设备上部署基于TEE的图神经网络模型。&lt;h4&gt;背景&lt;/h4&gt;机器学习模型在边缘设备上的广泛应用使得其知识产权和数据隐私面临威胁。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于TEE的解决方案来保护图神经网络(GNN)模型及其使用的私有图形数据的安全性。&lt;h4&gt;方法&lt;/h4&gt;GNNVault采用'训练前划分'的设计理念，并结合了私人GNN校正器与公共骨干模型。在推理过程中，重要的GNN参数和使用到的私有图表均被安全地隔离在TEE中。&lt;h4&gt;主要发现&lt;/h4&gt;通过真实世界的应用（例如Intel SGX环境）证明，GNNVault能够有效防御最先进的链接盗窃攻击，并且对精度的影响微乎其微（小于2%）。&lt;h4&gt;结论&lt;/h4&gt;提出的GNNVault方案为保护部署于边缘设备上的图神经网络模型提供了一种新颖而有效的安全机制。&lt;h4&gt;翻译&lt;/h4&gt;广泛地在边缘设备上部署机器学习模型已经使得这些模型的知识产权和数据隐私变得脆弱。我们提出了基于可信执行环境(TEE)的第一个安全图形神经网络(GNN)部署策略，称为GNNVault。该策略遵循'训练前划分'的设计理念，并且包括了一个私人GNN校正器来补充公共骨干模型。通过这种方式，在推理过程中使用的关键性GNN模型参数和私有图都被保护在安全的TEE隔间中。基于Intel SGX的真实世界实现表明，GNNVault可以有效地抵御最先进的链接盗窃攻击，同时不会导致精度显著下降（小于2%）。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Wide deployment of machine learning models on edge devices has rendered themodel intellectual property (IP) and data privacy vulnerable. We proposeGNNVault, the first secure Graph Neural Network (GNN) deployment strategy basedon Trusted Execution Environment (TEE). GNNVault follows the design of'partition-before-training' and includes a private GNN rectifier to complementwith a public backbone model. This way, both critical GNN model parameters andthe private graph used during inference are protected within secure TEEcompartments. Real-world implementations with Intel SGX demonstrate thatGNNVault safeguards GNN inference against state-of-the-art link stealingattacks with negligible accuracy degradation (&lt;2%).</description>
      <author>example@mail.com (Ruyi Ding, Tianhong Xu, Aidong Adam Ding, Yunsi Fei)</author>
      <guid isPermaLink="false">2502.15012v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Synth It Like KITTI: Synthetic Data Generation for Object Detection in Driving Scenarios</title>
      <link>http://arxiv.org/abs/2502.15076v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Preprint, to appear in ROBOVIS 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文旨在通过改进仿真数据集生成流程，提高从虚拟环境到真实世界场景的自主驾驶系统物体检测模型的泛化能力。&lt;h4&gt;背景&lt;/h4&gt;在推进自动驾驶系统的进程中，模拟技术是一个重要因素。然而，目前尚无显著进展解决虚拟与现实之间转换的问题，特别是在基于LiDAR点云进行3D目标检测方面。&lt;h4&gt;目的&lt;/h4&gt;该研究重新审视了从仿真数据到真实世界应用的转化问题，并提出了一种新的数据集生成管道以增强模型在真实环境中的泛化性能。&lt;h4&gt;方法&lt;/h4&gt;采用CARLA模拟器，结合领域随机化策略和细致建模技术来训练基于合成数据的目标检测模型。同时对比不同虚拟传感器变体，探究哪些传感器属性是导致域间隙的主要因素。&lt;h4&gt;主要发现&lt;/h4&gt;通过使用少量真实世界的数据进行微调，该模型几乎可以达到基准性能；而当使用完整的真实训练集时，则能够超过基准表现。&lt;h4&gt;结论&lt;/h4&gt;利用精心设计的模拟数据生成流程和领域随机化策略可以使仿真训练的目标检测器在现实世界的任务中表现出色。进一步的研究应该集中在如何最小化传感器属性差异，以进一步提高模型的泛化能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要：推动自动驾驶系统发展的关键因素之一是模拟技术的应用。然而，在虚拟环境向真实世界场景迁移的问题上进展甚微。我们重新审视了针对基于LiDAR点云进行3D物体检测任务中的领域转移问题，提出了一种基于CARLA仿真器的生成数据集管道。通过采用领域随机化策略和精心建模方法，我们在合成数据上训练了一个目标探测器，并展示了其在KITTI数据集上的强大泛化能力。此外，我们对比了不同虚拟传感器变体以获得洞见，哪些传感器特性可能造成显著的域间隙现象。最后，在少量真实世界数据的基础上进行微调几乎可以达到基准性能水平；而使用完整的真实训练集时则超过了该基准线。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; An important factor in advancing autonomous driving systems is simulation.Yet, there is rather small progress for transferability between the virtual andreal world. We revisit this problem for 3D object detection on LiDAR pointclouds and propose a dataset generation pipeline based on the CARLA simulator.Utilizing domain randomization strategies and careful modeling, we are able totrain an object detector on the synthetic data and demonstrate stronggeneralization capabilities to the KITTI dataset. Furthermore, we comparedifferent virtual sensor variants to gather insights, which sensor attributescan be responsible for the prevalent domain gap. Finally, fine-tuning with asmall portion of real data almost matches the baseline and with the fulltraining set slightly surpasses it.</description>
      <author>example@mail.com (Richard Marcus, Christian Vogel, Inga Jatzkowski, Niklas Knoop, Marc Stamminger)</author>
      <guid isPermaLink="false">2502.15076v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Fed-SB: A Silver Bullet for Extreme Communication Efficiency and Performance in (Private) Federated LoRA Fine-Tuning</title>
      <link>http://arxiv.org/abs/2502.15436v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Raghav Singhal and Kaustubh Ponkshe contributed equally to this work&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了Fed-SB，一种使用LoRA-SB方法进行联邦微调的新颖方式。该方法通过学习两个适配器之间的小型正方形矩阵来优化低秩适应过程，并直接平均该矩阵以减少通信成本和保证精确更新。&lt;h4&gt;背景&lt;/h4&gt;低秩适应（LoRA）已成为有效调整基础模型的普遍技术，但使用LoRA进行联邦微调存在挑战，因为传统的个体适配器平均方法会导致次优更新。现有解决方案要么导致高昂的通信成本，要么由于表达能力受限而性能下降。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的联邦微调方法，既能减少通信开销又能保证高性能。&lt;h4&gt;方法&lt;/h4&gt;提出Fed-SB，该方法基于最近提出的LoRA-SB低秩适应技术。在两个适配器之间学习一个小型正方形矩阵R，并直接平均这个矩阵以确保精确更新和降低通信成本。&lt;h4&gt;主要发现&lt;/h4&gt;Fed-SB在常识推理、算术推理和语言推断任务上实现了最先进的性能，同时将通信开销最多减少了230倍。此外，在私人设置下，通过减少可训练参数数量来提高隐私保护，并避免其他方法引入的噪声放大问题。&lt;h4&gt;结论&lt;/h4&gt;总体而言，Fed-SB在通信成本与性能之间的权衡中开辟了一个新的帕累托前沿，为私有和非私有的联邦微调提供了高效且可扩展的解决方案。&lt;h4&gt;翻译&lt;/h4&gt;Lo-Rank 适应（LoRA）已成为有效调整基础模型的标准技术。然而，使用 LoRA 进行联邦细调具有挑战性，因为传统的个体适配器平均会导致次优更新。现有的解决方法要么导致高昂的通信成本，要么由于表达能力受限而降低性能表现。我们介绍了一种新的方法 Fed-SB，它利用了最近提出的低秩适应技术 LoRA-SB 来进行大规模语言模型（LLMs）的联邦细调。LoRA-SB 通过在适配器之间学习一个小方阵矩阵 R，并保持其他组件不变来优化适应过程。直接平均这个小方阵保证了精确更新，大大降低了通信成本，使其不受客户端数量的影响，从而实现了可扩展性。Fed-SB 在常识推理、算术推理和语言推断任务上达到了最先进的性能，同时将通信成本最多减少了230倍。在私人设置中，Fed-SB 通过减少训练参数的数量来提高差分隐私的噪声需求，并避免了其他方法引入的噪声放大问题，进一步提高了表现。总体而言，Fed-SB 在通信和性能之间的权衡上开辟了一个新的帕累托前沿，为私有和非私有的联邦细调提供了高效且可扩展的解决方案。我们的代码公开可用：https://github.com/CERT-Lab/fed-sb。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;https://github.com/CERT-Lab/fed-sb&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Low-Rank Adaptation (LoRA) has become ubiquitous for efficiently fine-tuningfoundation models. However, federated fine-tuning using LoRA is challenging dueto suboptimal updates arising from traditional federated averaging ofindividual adapters. Existing solutions either incur prohibitively highcommunication cost that scales linearly with the number of clients or sufferfrom performance degradation due to limited expressivity. We introduceFederated Silver Bullet (Fed-SB), a novel approach for federated fine-tuning ofLLMs using LoRA-SB, a recently proposed low-rank adaptation method. LoRA-SBoptimally aligns the optimization trajectory with the ideal low-rank fullfine-tuning projection by learning a small square matrix (R) between adapters Band A, keeping other components fixed. Direct averaging of R guarantees exactupdates, substantially reducing communication cost, which remains independentof the number of clients, and enables scalability. Fed-SB achievesstate-of-the-art performance across commonsense reasoning, arithmeticreasoning, and language inference tasks while reducing communication costs byup to 230x. In private settings, Fed-SB further improves performance by (1)reducing trainable parameters, thereby lowering the noise required fordifferential privacy and (2) avoiding noise amplification introduced by othermethods. Overall, Fed-SB establishes a new Pareto frontier in the tradeoffbetween communication and performance, offering an efficient and scalablesolution for both private and non-private federated fine-tuning. Our code ispublicly available at https://github.com/CERT-Lab/fed-sb.</description>
      <author>example@mail.com (Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Lav R. Varshney, Praneeth Vepakomma)</author>
      <guid isPermaLink="false">2502.15436v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Understanding the Design Principles of Link Prediction in Directed Settings</title>
      <link>http://arxiv.org/abs/2502.15008v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文研究了图表示学习中有向链接预测的挑战，提出了适用于该任务的有效启发式方法，并展示了这些改进能够与为无向图设计的最佳图神经网络相媲美。&lt;h4&gt;背景&lt;/h4&gt;传统的图表示学习理论基于对称邻接矩阵假设，即认为数据是无方向性的。然而，现实世界中的关系经常包含通过方向传达的重要信息，这限制了现有模型捕捉复杂有向交互的能力。&lt;h4&gt;目的&lt;/h4&gt;本文旨在解决有向链接预测的问题，并提出适用于该任务的有效方法。&lt;h4&gt;方法&lt;/h4&gt;在论文中评估了一些成功应用于无向图的关键启发式算法，然后对其进行了简单但有效的改进以适应有向链接预测的任务。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列广泛的实验研究，作者开发了一个新颖的框架来解决有向链接预测问题。该框架不仅超过了基线方法，在多个基准上的性能也优于为无向图设计的最佳图神经网络。&lt;h4&gt;结论&lt;/h4&gt;这项工作表明，对现有启发式算法进行简单的调整可以在有向图任务中实现与复杂神经网络模型相竞争的性能，并且这些改进可以提供对图表示学习框架发展的宝贵见解。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1145/3701716.3717803&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Link prediction is a widely studied task in Graph Representation Learning(GRL) for modeling relational data. The early theories in GRL were based on theassumption of a symmetric adjacency matrix, reflecting an undirected setting.As a result, much of the following state-of-the-art research has continued tooperate under this symmetry assumption, even though real-world data ofteninvolve crucial information conveyed through the direction of relationships.This oversight limits the ability of these models to fully capture thecomplexity of directed interactions. In this paper, we focus on the challengeof directed link prediction by evaluating key heuristics that have beensuccessful in undirected settings. We propose simple but effective adaptationsof these heuristics to the directed link prediction task and demonstrate thatthese modifications produce competitive performance compared to the leadingGraph Neural Networks (GNNs) originally designed for undirected graphs. Throughan extensive set of experiments, we derive insights that inform the developmentof a novel framework for directed link prediction, which not only surpassesbaseline methods but also outperforms state-of-the-art GNNs on multiplebenchmarks.</description>
      <author>example@mail.com (Jun Zhai, Muberra Ozmen, Thomas Markovich)</author>
      <guid isPermaLink="false">2502.15008v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Chitrarth: Bridging Vision and Language for a Billion People</title>
      <link>http://arxiv.org/abs/2502.15392v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了Chitrarth，这是一个针对印度10种语言的包容性视觉-语言模型。该模型结合了最先进的多语言大型语言模型和视觉模块，并主要在多语言图像文本数据上进行训练。&lt;h4&gt;背景&lt;/h4&gt;现有的多模态基础模型主要是基于英语或高资源欧洲语言的数据集进行训练，这限制了它们在中低资源语言中的应用。&lt;h4&gt;目的&lt;/h4&gt;为了应对这一局限性，研究者们引入了一个名为Chitrarth的视觉-语言模型，旨在支持印度丰富的语言多样性及跨10种主要印度语言的视觉推理。&lt;h4&gt;方法&lt;/h4&gt;研究团队使用最先进的多语言大型语言模型与视觉模块相结合的方法，并且这种结合主要是通过在包含多种语言图像文本数据集上进行训练来实现的。&lt;h4&gt;主要发现&lt;/h4&gt;该模型不仅在低资源语言基准测试中取得了最佳结果，同时在英语中的效率也得以保持。此外，他们还提出了BharatBench框架用于评估跨不同印度语言的视觉-语言模型的表现。&lt;h4&gt;结论&lt;/h4&gt;通过这项研究，研究者们旨在为多语种和多模态能力设置新的基准，并为未来的发展提供基础。&lt;h4&gt;翻译&lt;/h4&gt;最近的多种模态基础模型主要是基于英语或高资源欧洲语言的数据进行训练，这限制了它们在中低资源语言中的应用。为了应对这一局限性，我们介绍了Chitrarth（图像：图片；意义：含义），这是一个面向10种主要印度语言的语言多样性及视觉推理问题的目标包容性视觉-语言模型(VLM)。我们的模型有效地集成了最先进的多语言大型语言模型和一个视觉模块，主要是基于多种语言的图文数据进行训练。此外，我们还引入了BharatBench，这个框架用于在不同印度语中评估VLM的表现，最终促进了更加多样化的AI系统的发展。我们的模型在低资源语言基准测试中取得了最佳结果，并保持了其在英语中的效率。通过我们的研究，我们旨在为多语种和多模态能力设立新的基准，并对现有模型进行实质性的改进，从而为基础未来在这个领域的进步奠定基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent multimodal foundation models are primarily trained on English or highresource European language data, which hinders their applicability to othermedium and low-resource languages. To address this limitation, we introduceChitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model(VLM), specifically targeting the rich linguistic diversity and visualreasoning across 10 prominent Indian languages. Our model effectivelyintegrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM)with a vision module, primarily trained on multilingual image-text data.Furthermore, we also introduce BharatBench, a comprehensive framework forevaluating VLMs across various Indian languages, ultimately contributing tomore diverse and effective AI systems. Our model achieves SOTA results forbenchmarks across low resource languages while retaining its efficiency inEnglish. Through our research, we aim to set new benchmarks inmultilingual-multimodal capabilities, offering substantial improvements overexisting models and establishing a foundation to facilitate future advancementsin this arena.</description>
      <author>example@mail.com (Shaharukh Khan, Ayush Tarun, Abhinav Ravi, Ali Faraz, Akshat Patidar, Praveen Kumar Pokala, Anagha Bhangare, Raja Kolla, Chandra Khatri, Shubham Agarwal)</author>
      <guid isPermaLink="false">2502.15392v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>CrossOver: 3D Scene Cross-Modal Alignment</title>
      <link>http://arxiv.org/abs/2502.15011v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Project Page: sayands.github.io/crossover/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;CrossOver是一种用于多模态3D场景理解的新框架，通过灵活的场景级别模式对齐来解决传统方法需要所有对象实例都具有严格对齐的模式数据的问题。&lt;h4&gt;背景&lt;/h4&gt;当前的方法通常假设完全的数据可用性以及各模式之间的刚性对齐，这在实际应用中可能难以实现。&lt;h4&gt;目的&lt;/h4&gt;提出了一种新的框架CrossOver，该框架旨在通过灵活、场景级别的模式对齐进行跨模态3D场景理解，并且无需显式的对象语义即可学习统一的多模式无关嵌入空间。&lt;h4&gt;方法&lt;/h4&gt;利用特定于维度的编码器和多阶段训练管道，CrossOver支持具有缺失模式的数据场景检索和物体定位任务。该框架包括RGB图像、点云、CAD模型、平面图以及文本描述等多种类型的数据输入。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果在ScanNet和3RScan数据集上表现出色，显示了其在各种指标上的优越性能，并强调了适应现实世界应用的能力。&lt;h4&gt;结论&lt;/h4&gt;CrossOver框架展示了它在多模态3D场景理解中的强大能力，尤其是在处理不完整或缺失的模式时的表现。这为未来的研究提供了坚实的基础和新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;跨模式三维物体理解已经获得了相当的关注，然而当前的方法往往假设完全的数据可用性以及各模式之间的刚性对齐。我们提出了一种新的框架CrossOver，该框架通过灵活、场景级别的模式对齐进行跨模态3D场景理解，并且无需显式的对象语义即可学习统一的多模式无关嵌入空间。实验结果在ScanNet和3RScan数据集上表现出色，显示了其适应现实世界应用的能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Multi-modal 3D object understanding has gained significant attention, yetcurrent approaches often assume complete data availability and rigid alignmentacross all modalities. We present CrossOver, a novel framework for cross-modal3D scene understanding via flexible, scene-level modality alignment. Unliketraditional methods that require aligned modality data for every objectinstance, CrossOver learns a unified, modality-agnostic embedding space forscenes by aligning modalities - RGB images, point clouds, CAD models,floorplans, and text descriptions - with relaxed constraints and withoutexplicit object semantics. Leveraging dimensionality-specific encoders, amulti-stage training pipeline, and emergent cross-modal behaviors, CrossOversupports robust scene retrieval and object localization, even with missingmodalities. Evaluations on ScanNet and 3RScan datasets show its superiorperformance across diverse metrics, highlighting adaptability for real-worldapplications in 3D scene understanding.</description>
      <author>example@mail.com (Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, Iro Armeni)</author>
      <guid isPermaLink="false">2502.15011v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>DynamicGSG: Dynamic 3D Gaussian Scene Graphs for Environment Adaptation</title>
      <link>http://arxiv.org/abs/2502.15309v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种动态、高保真和开放词汇的场景图生成系统（DynamicGSG），用于机器人在不断变化的环境中有效理解和适应。&lt;h4&gt;背景&lt;/h4&gt;现实世界中，环境由于代理或人类活动的变化使得机器人执行长期任务变得非常具有挑战性。感知系统需要提取实例级别的语义信息并根据环境变化更新内存中的表示。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够构建层次化场景图、优化高保真重建以及动态适应长时环境变化的系统。&lt;h4&gt;方法&lt;/h4&gt;{'构造层次化场景图': '使用先进的视觉基础模型来表示环境中对象的空间和语义关系', '设计联合特征损失': '为了增量式地提高高质量重建，优化高斯地图', '更新高斯地图与场景图': '根据实际环境变化进行动态更新'}&lt;h4&gt;主要发现&lt;/h4&gt;{'性能实验': '在语义分割、语言引导的对象检索和重建质量方面展示了所提方法的效能。', '真实实验室验证': '验证了系统动态更新能力的有效性'}&lt;h4&gt;结论&lt;/h4&gt;DynamicGSG能够有效提高机器人在复杂多变环境中的适应性和执行长期任务的能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要是关于一个叫做DynamicGSG的新提出的用于机器人感知系统的解决方案，它能帮助机器人在不断变化的环境中更高效地理解和工作。该系统主要由三个部分构成：一是使用先进的视觉基础模型构建描述物体空间和语义关系的层次化场景图；二是通过设计联合特征损失来优化高斯地图，以获得增量式的高质量重建效果；三是根据真实环境的变化更新高斯地图和场景图，实现长时间内的环境适应。实验结果表明该方法在关键任务如语义分割、语言引导对象检索以及重建质量方面表现出色，并且其动态更新能力已经在实验室环境中得到了验证。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In real-world scenarios, the environment changes caused by agents or humanactivities make it extremely challenging for robots to perform variouslong-term tasks. To effectively understand and adapt to dynamic environments,the perception system of a robot needs to extract instance-level semanticinformation, reconstruct the environment in a fine-grained manner, and updateits environment representation in memory according to environment changes. Toaddress these challenges, We propose \textbf{DynamicGSG}, a dynamic,high-fidelity, open-vocabulary scene graph generation system leveragingGaussian splatting. Our system comprises three key components: (1) constructinghierarchical scene graphs using advanced vision foundation models to representthe spatial and semantic relationships of objects in the environment, (2)designing a joint feature loss to optimize the Gaussian map for incrementalhigh-fidelity reconstruction, and (3) updating the Gaussian map and scene graphaccording to real environment changes for long-term environment adaptation.Experiments and ablation studies demonstrate the performance and efficacy ofthe proposed method in terms of semantic segmentation, language-guided objectretrieval, and reconstruction quality. Furthermore, we have validated thedynamic updating capabilities of our system in real laboratory environments.The source code will be releasedat:~\href{https://github.com/GeLuzhou/Dynamic-GSG}{https://github.com/GeLuzhou/DynamicGSG}.</description>
      <author>example@mail.com (Luzhou Ge, Xiangyu Zhu, Zhuo Yang, Xuesong Li)</author>
      <guid isPermaLink="false">2502.15309v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Lung-DDPM: Semantic Layout-guided Diffusion Models for Thoracic CT Image Synthesis</title>
      <link>http://arxiv.org/abs/2502.15204v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  The code and pretrained models are available at  https://github.com/Manem-Lab/Lung-DDPM&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文介绍了一种基于AI的胸部CT影像合成方法Lung-DDPM，旨在解决肺癌早期筛查中数据稀缺的问题。&lt;h4&gt;背景&lt;/h4&gt;随着人工智能技术的发展，AI辅助医学影像分析在肺癌早期筛查方面表现出色。然而，高昂的数据标注成本和隐私问题限制了大规模医疗数据集的构建，阻碍了AI在医疗领域的进一步应用。&lt;h4&gt;目的&lt;/h4&gt;为了应对肺癌筛查中的数据稀缺问题，提出了一种胸部CT图像合成方法Lung-DDPM，以生成高质量的3D合成CT影像，并应用于下游肺结节分割任务中。&lt;h4&gt;方法&lt;/h4&gt;该方法基于语义布局引导去噪扩散概率模型（DDPM），能够从不完整的语义布局中生成解剖学合理的、无缝且一致的样本图像。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，Lung-DDPM在图像质量评估和下游肺结节分割任务方面优于其他最先进的生成模型。具体而言，在验证队列中FID为0.0047、MMD为0.0070、MSE为0.0024，分别比第二好的竞争对手高7.4倍、3.1倍和29.5倍。此外，结合真实数据与Lung-DDPM生成的数据训练的肺结节分割模型在Dice系数和敏感性方面分别优于单独使用真实数据模型8.8%和18.6%。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明Lung-DDPM具有更广泛的应用潜力，例如肿瘤分割、癌症生存估计以及风险预测等方面。&lt;h4&gt;翻译&lt;/h4&gt;摘要：随着人工智能（AI）的快速发展，基于AI的医学影像分析在肺癌早期筛查中展现出显著的效果。然而，高昂的数据标注成本和隐私问题限制了大规模医疗数据集的构建，阻碍了AI在医疗领域的进一步应用。为了应对肺癌筛查中的数据稀缺问题，本文提出了一种胸部CT图像合成方法Lung-DDPM，该方法能够有效生成高保真的3D合成CT影像，在下游肺结节分割任务中证明很有帮助。本研究基于语义布局引导去噪扩散概率模型（DDPM），即使从不完整的语义布局也能生成解剖学合理的、无缝且一致的样本图像。实验结果显示，该方法在图像质量评估和下游肺结节分割任务方面优于其他最先进的生成模型。具体而言，在验证队列中分别取得了FID为0.0047、MMD为0.0070以及MSE为0.0024的成绩，并且这些结果分别为第二好的竞争对手的7.4倍、3.1倍和29.5倍更好。此外，结合真实数据与Lung-DDPM生成的数据训练的肺结节分割模型在Dice系数和敏感性方面分别达到了0.3914和0.4393的成绩，比单独使用真实数据模型高8.8%和18.6%。实验结果表明Lung-DDPM具有更广泛的应用潜力，例如肿瘤分割、癌症生存估计以及风险预测等方面。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; With the rapid development of artificial intelligence (AI), AI-assistedmedical imaging analysis demonstrates remarkable performance in early lungcancer screening. However, the costly annotation process and privacy concernslimit the construction of large-scale medical datasets, hampering the furtherapplication of AI in healthcare. To address the data scarcity in lung cancerscreening, we propose Lung-DDPM, a thoracic CT image synthesis approach thateffectively generates high-fidelity 3D synthetic CT images, which prove helpfulin downstream lung nodule segmentation tasks. Our method is based on semanticlayout-guided denoising diffusion probabilistic models (DDPM), enablinganatomically reasonable, seamless, and consistent sample generation even fromincomplete semantic layouts. Our results suggest that the proposed methodoutperforms other state-of-the-art (SOTA) generative models in image qualityevaluation and downstream lung nodule segmentation tasks. Specifically,Lung-DDPM achieved superior performance on our large validation cohort, with aFr\'echet inception distance (FID) of 0.0047, maximum mean discrepancy (MMD) of0.0070, and mean squared error (MSE) of 0.0024. These results were 7.4$\times$,3.1$\times$, and 29.5$\times$ better than the second-best competitors,respectively. Furthermore, the lung nodule segmentation model, trained on adataset combining real and Lung-DDPM-generated synthetic samples, attained adice coefficient (Dice) of 0.3914 and sensitivity of 0.4393. This represents8.8\% and 18.6\% improvements in DICE and sensitivity compared to the modeltrained solely on real samples. The experimental results highlight Lung-DDPM'spotential for a broader range of medical imaging applications, such as generaltumor segmentation, cancer survival estimation, and risk prediction.</description>
      <author>example@mail.com (Yifan Jiang, Yannick Lemaréchal, Josée Bafaro, Jessica Abi-Rjeile, Philippe Joubert, Philippe Després, Venkata Manem)</author>
      <guid isPermaLink="false">2502.15204v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>FacaDiffy: Inpainting Unseen Facade Parts Using Diffusion Models</title>
      <link>http://arxiv.org/abs/2502.14940v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for GeoSpatial Week 2025, ISPRS Annals&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了FacaDiffy，这是一种利用个性化稳定扩散模型填补2D冲突地图中未见立面部分的方法，从而提高高精度3D语义建筑重建中的开口位置检测率。&lt;h4&gt;背景&lt;/h4&gt;在创建高细节的三维建筑物模型时，2D冲突图用于识别建筑物外墙上的开口位置。然而，在实际激光扫描过程中，这些2D冲突图由于障碍物的影响往往是不完整的。&lt;h4&gt;目的&lt;/h4&gt;提出一种方法来填补2D冲突地图中未见立面部分，并使用个性化稳定扩散模型来完成冲突地图的绘制。&lt;h4&gt;方法&lt;/h4&gt;{'1': '首先提出了一个确定性的光线分析方法，从现有的3D建筑模型和对应的激光扫描点云数据中推导出2D冲突图', '2': '利用个性化稳定扩散模型的能力将未见立面对象填充到这些2D冲突图中。', '3': '为了补充真实世界训练数据的不足，开发了一条可扩展的数据生成流水线，使用随机城市模型生成器和标记后的立面图像来创建合成冲突地图。'}&lt;h4&gt;主要发现&lt;/h4&gt;实验结果显示，FacaDiffy在与各种填补基线相比时，在2D冲突图完成方面达到了最先进的性能，并且当应用到高精度3D语义建筑重建中时，检测率提高了22%。&lt;h4&gt;结论&lt;/h4&gt;该方法通过个性化稳定扩散模型有效地解决了实际激光扫描过程中遇到的建筑物外墙部分缺失的问题，为高细节三维模型创建提供了强有力的支持。&lt;h4&gt;翻译&lt;/h4&gt;详细摘要的中文翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; High-detail semantic 3D building models are frequently utilized in robotics,geoinformatics, and computer vision. One key aspect of creating such models isemploying 2D conflict maps that detect openings' locations in building facades.Yet, in reality, these maps are often incomplete due to obstacles encounteredduring laser scanning. To address this challenge, we introduce FacaDiffy, anovel method for inpainting unseen facade parts by completing conflict mapswith a personalized Stable Diffusion model. Specifically, we first propose adeterministic ray analysis approach to derive 2D conflict maps from existing 3Dbuilding models and corresponding laser scanning point clouds. Furthermore, wefacilitate the inpainting of unseen facade objects into these 2D conflict mapsby leveraging the potential of personalizing a Stable Diffusion model. Tocomplement the scarcity of real-world training data, we also develop a scalablepipeline to produce synthetic conflict maps using random city model generatorsand annotated facade images. Extensive experiments demonstrate that FacaDiffyachieves state-of-the-art performance in conflict map completion compared tovarious inpainting baselines and increases the detection rate by $22\%$ whenapplying the completed conflict maps for high-definition 3D semantic buildingreconstruction. The code is be publicly available in the corresponding GitHubrepository: https://github.com/ThomasFroech/InpaintingofUnseenFacadeObjects</description>
      <author>example@mail.com (Thomas Froech, Olaf Wysocki, Yan Xia, Junyu Xie, Benedikt Schwab, Daniel Cremers, Thomas H. Kolbe)</author>
      <guid isPermaLink="false">2502.14940v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>TransMamba: Fast Universal Architecture Adaption from Transformers to Mamba</title>
      <link>http://arxiv.org/abs/2502.15130v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文探讨了利用现有的Transformer模型如LLaVA、CLIP和DEIT的知识，通过跨架构训练来增强Mamba架构的性能。提出了TransMamba方法，并采用两阶段策略加速新Mamba模型的训练。&lt;h4&gt;背景&lt;/h4&gt;Transformer因其注意力模块在单模态和多模态基础模型中的灵活扩展性而受到青睐，但从头开始为特定任务培训专门的次二次架构既耗时又耗费资源。&lt;h4&gt;目的&lt;/h4&gt;探索如何通过跨架构训练将现有的预训练Transformer模型的知识转移到Mamba架构上，以提升其性能并减少所需的训练时间和计算资源。&lt;h4&gt;方法&lt;/h4&gt;采用两种主要策略：一是引入Weight Subcloning and Adaptive Bidirectional distillation (WSAB) 方法来实现不受层数限制的知识迁移；二是设计了一个跨模态学习模块——cross-Mamba模块，该模块将语言感知与Mamba的视觉特征相结合，从而增强其处理跨模态任务的能力。&lt;h4&gt;主要发现&lt;/h4&gt;TransMamba在使用不到常规从头训练所需75%的数据量的情况下，在各种网络架构和下游任务中（如图像分类、视觉问答、文本视频检索等）表现出色。&lt;h4&gt;结论&lt;/h4&gt;通过将现有的Transformer模型的知识转移到Mamba架构上，可以显著提高后者的性能，并且代码将在未来公开发布。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Transformers have been favored in both uni-modal and multi-modal foundationmodels for their flexible scalability in attention modules. Consequently, anumber of pre-trained Transformer models, e.g., LLaVA, CLIP, and DEIT, arepublicly available. Recent research has introduced subquadratic architectureslike Mamba, which enables global awareness with linear complexity.Nevertheless, training specialized subquadratic architectures from scratch forcertain tasks is both resource-intensive and time-consuming. As a motivator, weexplore cross-architecture training to transfer the ready knowledge in existingTransformer models to alternative architecture Mamba, termed TransMamba. Ourapproach employs a two-stage strategy to expedite training new Mamba models,ensuring effectiveness in across uni-modal and cross-modal tasks. Concerningarchitecture disparities, we project the intermediate features into an alignedlatent space before transferring knowledge. On top of that, a Weight Subcloningand Adaptive Bidirectional distillation method (WSAB) is introduced forknowledge transfer without limitations on varying layer counts. For cross-modallearning, we propose a cross-Mamba module that integrates language awarenessinto Mamba's visual features, enhancing the cross-modal interactioncapabilities of Mamba architecture. Despite using less than 75% of the trainingdata typically required for training from scratch, TransMamba boastssubstantially stronger performance across various network architectures anddownstream tasks, including image classification, visual question answering,and text-video retrieval. The code will be publicly available.</description>
      <author>example@mail.com (Xiuwei Chen, Sihao Lin, Xiao Dong, Zisheng Chen, Meng Cao, Jianhua Han, Hang Xu, Xiaodan Liang)</author>
      <guid isPermaLink="false">2502.15130v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning</title>
      <link>http://arxiv.org/abs/2502.15082v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code: https://github.com/Vaidehi99/UPCORE&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;UPCORE是一种用于减轻遗忘过程中附带损害的数据选择框架。该方法通过减少模型在忘记集上的表示方差来最小化模型退化。&lt;h4&gt;背景&lt;/h4&gt;当需要从预训练模型中删除特定信息时，通常会导致模型性能下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够平衡信息删除和保留模型其他能力的方法，避免因失去平衡导致的模型不可用或效果不佳。&lt;h4&gt;方法&lt;/h4&gt;UPCORE是一种与方法无关的数据选择框架。通过选择性修剪忘记集来减少离群值，以最小化遗忘后的模型退化。&lt;h4&gt;主要发现&lt;/h4&gt;在三种标准删除方法上评估了UPCORE的表现，并引入了一个新的度量标准（AUC），用于衡量其效果优于其他现有技术。&lt;h4&gt;结论&lt;/h4&gt;UPCORE不仅提高了标准指标和AUC的得分，还通过减少负面迁移并利用核心集与修剪点之间的积极迁移效应来增强模型性能。&lt;h4&gt;翻译&lt;/h4&gt;用户规格或法律法规通常要求从预训练模型（包括大型语言模型）中移除信息。这需要删除或“忘记”一组已训练模型中的数据点，通常会导致其在其他数据点上的表现下降。因此，在去除信息和保持模型其余功能之间必须找到平衡，否则可能导致较差的删除效果或不可用的模型。为此，我们提出了UPCORE（Utility-Preserving Coreset Selection），这是一种用于减轻遗忘过程中附带损害的方法无关的数据选择框架。发现模型损坏与忘记集上表示方差相关，通过选择性地修剪该集合来去除离群值从而最小化遗忘后的退化。我们在三种标准删除方法中评估了UPCORE，并始终达到在删除有效性和模型保留之间竞争目标的优越平衡。为了更好地衡量这种权衡，我们引入了一个新的度量指标（AUC），其结果表明UPCORE提高了标准指标和AUC的得分，通过减少负面迁移并利用核心集与修剪点之间的积极迁移效应来增强模型性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; User specifications or legal frameworks often require information to beremoved from pretrained models, including large language models (LLMs). Thisrequires deleting or "forgetting" a set of data points from an already-trainedmodel, which typically degrades its performance on other data points. Thus, abalance must be struck between removing information and keeping the model'sother abilities intact, with a failure to balance this trade-off leading topoor deletion or an unusable model. To this end, we propose UPCORE(Utility-Preserving Coreset Selection), a method-agnostic data selectionframework for mitigating collateral damage during unlearning. Finding that themodel damage is correlated with the variance of the model's representations onthe forget set, we selectively prune the forget set to remove outliers, therebyminimizing model degradation after unlearning. We evaluate UPCORE across threestandard unlearning methods consistently achieving a superior balance betweenthe competing objectives of deletion efficacy and model preservation. To betterevaluate this trade-off, we introduce a new metric, measuring thearea-under-the-curve (AUC) across standard metrics. We find that UPCOREimproves both standard metrics and AUC, benefitting from positive transferbetween the coreset and pruned points while reducing negative transfer from theforget set to points outside of it.</description>
      <author>example@mail.com (Vaidehi Patil, Elias Stengel-Eskin, Mohit Bansal)</author>
      <guid isPermaLink="false">2502.15082v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Towards Physics-Guided Foundation Models</title>
      <link>http://arxiv.org/abs/2502.15013v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;介绍了一种新的物理引导基础模型（PGFM），旨在将广泛领域的物理知识集成到传统基础模型中。&lt;h4&gt;背景&lt;/h4&gt;传统的基础模型是通过大规模数据集预训练的，目的是减少微调大量下游任务所需的资源。&lt;h4&gt;目的&lt;/h4&gt;解决传统基础模型在处理分布外预测时的问题，并避免产生不现实或物理上不可行的输出。&lt;h4&gt;方法&lt;/h4&gt;提出将广泛领域的（例如科学领域）通用知识集成到基础模型中的方法，形成物理引导基础模型。&lt;h4&gt;翻译&lt;/h4&gt;摘要：传统的基础模型通过大规模数据集进行预训练，以减少在广泛的下游任务中微调所需的资源（如时间、能量和标记样本）。然而，传统的基础模型难以处理分布外预测，并且会产生不现实或物理上不可行的输出。我们提出了物理引导基础模型（PGFM）的概念，即将适用于广泛下游任务的广义或通用领域知识（例如科学领域的知识）集成到基础模型中。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Traditional foundation models are pre-trained on broad datasets to reduce thetraining resources (e.g., time, energy, labeled samples) needed for fine-tuninga wide range of downstream tasks. However, traditional foundation modelsstruggle with out-of-distribution prediction and can produce outputs that areunrealistic and physically infeasible. We propose the notation ofphysics-guided foundation models (PGFM), that is, foundation models integratedwith broad or general domain (e.g., scientific) physical knowledge applicableto a wide range of downstream tasks.</description>
      <author>example@mail.com (Majid Farhadloo, Arun Sharma, Mingzhou Yang, Bharat Jayaprakash, William Northrop, Shashi Shekhar)</author>
      <guid isPermaLink="false">2502.15013v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Obliviate: Efficient Unmemorization for Protecting Intellectual Property in Large Language Models</title>
      <link>http://arxiv.org/abs/2502.15010v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;最近的版权协议强调了对语言模型复制受版权保护的内容的能力进行精确控制的需求。提出了一种名为Obliviate的新方法，该方法是一种新型的后训练技术，能够选择性地防止特定文本的逐字复制，同时保持语义理解。&lt;h4&gt;背景&lt;/h4&gt;AI公司与内容创作者之间的最近版权协议强调了对语言模型在复制受版权保护的内容时进行精确控制的需求。现有方法依赖于通过去学习或简单输出过滤来完全移除概念。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的后训练技术Obliviate，该技术能够选择性地防止逐字复制特定文本，同时保持语义理解。&lt;h4&gt;方法&lt;/h4&gt;Obliviate通过在记忆序列中选择标记并修改模型的概率分布以防止精确复制来工作，同时维持上下文理解。评估了多个大型语言模型（LLaMA-3.1 8B、LLaMA-3.1-instruct 8B、Qwen-2.5-7B 和 Yi-1.5 6B）在合成记忆任务和有机版权内容上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;实验结果表明，Obliviate实现了逐字记忆的数个数量级（例如100倍）的减少，同时保持了模型性能与基准线相比仅下降不到1%。这对于解决预训练模型中的版权问题尤其有效，而不损害其通用能力。&lt;h4&gt;结论&lt;/h4&gt;由于在解决语言模型中出现的版权复制风险方面表现出了显著的效果，并且能够在不影响整体功能的情况下大幅度降低逐字记忆的风险，Obliviate非常适合于实际部署场景。&lt;h4&gt;翻译&lt;/h4&gt;最近的版权协议强调了对AI语言模型复制受版权保护内容能力进行精确控制的需求。现有方法依赖完全移除概念或简单输出过滤来解决此问题。然而，这项研究提出了一种名为Obliviate的新后训练技术，该技术能够选择性地防止逐字复现特定文本，同时保持语义理解。通过在大型语言模型上进行了测试，证明了这种方法可以大幅度减少复制风险，而不会影响模型的性能和通用能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recent copyright agreements between AI companies and content creators havehighlighted the need for precise control over language models' ability toreproduce copyrighted content. While existing approaches rely on eithercomplete concept removal through unlearning or simple output filtering, wepropose Obliviate, a novel post-training technique that selectively preventsverbatim reproduction of specific text while preserving semantic understanding.  Obliviate operates by selecting tokens within memorized sequences andmodifying the model's probability distribution to prevent exact reproductionwhile maintaining contextual understanding. We evaluate Obliviate on multiplelarge language models (LLaMA-3.1 8B, LLaMA-3.1-instruct 8B, Qwen-2.5-7B, andYi-1.5 6B) across both synthetic memorization tasks and organic copyrightcontent. Our results demonstrate that Obliviate achieves orders of magnitudereduction, e.g., 100x, in verbatim memorization while maintaining modelperformance within 1% of baseline on standard benchmarks (HellaSwag, MMLU,TruthfulQA, and Winogrande). This makes Obliviate particularly suitable forpractical deployment scenarios where companies need to efficiently addresscopyright concerns in pretrained models without compromising their generalcapabilities.</description>
      <author>example@mail.com (Mark Russinovich, Ahmed Salem)</author>
      <guid isPermaLink="false">2502.15010v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Vision Foundation Models in Medical Image Analysis: Advances and Challenges</title>
      <link>http://arxiv.org/abs/2502.14584v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  17 pages, 1 figure&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文综述了视觉基础模型在医学图像分割领域适应性的最新研究进展，重点讨论了域适应、模型压缩和联邦学习的挑战，并提出了未来的研究方向。&lt;h4&gt;背景&lt;/h4&gt;随着Vision Foundation Models (VFMs)的发展，特别是ViT和SAM模型，在医疗影像分析中的应用显示出卓越的能力。然而，将这些大型模型应用于医学图像分析面临多个挑战，包括医学图像与自然图像之间的领域差异、高效的适应策略需求以及小规模数据集的限制。&lt;h4&gt;目的&lt;/h4&gt;该论文旨在提供关于视觉基础模型在医学图像分割中适应性的最新研究进展的一个全面概述，并指出未来研究的关键领域以推动下一轮创新。&lt;h4&gt;方法&lt;/h4&gt;文章回顾了基于适配器改进的方法、知识蒸馏技术以及多尺度上下文特征建模的发展。&lt;h4&gt;主要发现&lt;/h4&gt;最新的发展包括通过新兴的技术如联邦学习和模型压缩，增强了视觉基础模型在医学图像分析领域的潜力。&lt;h4&gt;结论&lt;/h4&gt;文中强调了VFMs的未来应用前景，并指出了克服现有瓶颈的关键方法。文章呼吁研究者们关注这些领域以推动医疗影像分割技术的发展。&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了Vision Foundation Models (VFMs)迅速发展，特别是在医学图像分析中展示了出色的能力。但是将它们应用于医疗图像存在许多挑战，如领域差异、模型适应策略效率需求和小规模数据集的限制。本文综述了相关最新研究，并提出了未来的研究方向以克服这些瓶颈。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; The rapid development of Vision Foundation Models (VFMs), particularly VisionTransformers (ViT) and Segment Anything Model (SAM), has sparked significantadvances in the field of medical image analysis. These models have demonstratedexceptional capabilities in capturing long-range dependencies and achievinghigh generalization in segmentation tasks. However, adapting these large modelsto medical image analysis presents several challenges, including domaindifferences between medical and natural images, the need for efficient modeladaptation strategies, and the limitations of small-scale medical datasets.This paper reviews the state-of-the-art research on the adaptation of VFMs tomedical image segmentation, focusing on the challenges of domain adaptation,model compression, and federated learning. We discuss the latest developmentsin adapter-based improvements, knowledge distillation techniques, andmulti-scale contextual feature modeling, and propose future directions toovercome these bottlenecks. Our analysis highlights the potential of VFMs,along with emerging methodologies such as federated learning and modelcompression, to revolutionize medical image analysis and enhance clinicalapplications. The goal of this work is to provide a comprehensive overview ofcurrent approaches and suggest key areas for future research that can drive thenext wave of innovation in medical image segmentation.</description>
      <author>example@mail.com (Pengchen Liang, Bin Pu, Haishan Huang, Yiwei Li, Hualiang Wang, Weibo Ma, Qing Chang)</author>
      <guid isPermaLink="false">2502.14584v2</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>M2LADS Demo: A System for Generating Multimodal Learning Analytics Dashboards</title>
      <link>http://arxiv.org/abs/2502.15363v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Published in the Workshop on Innovation and Responsibility in  AI-Supported Education (iRAISE25) at AAAI 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;背景&lt;/h4&gt;介绍了一种基于Web的系统M2LADS，该系统旨在整合、同步并可视化在计算机学习过程中通过生物传感器记录的多模态数据。&lt;h4&gt;目的&lt;/h4&gt;提供详细的生理和活动相关的指标见解，并帮助数据科学家通过综合视图展示参与者的体验以及简化错误活动信息的数据重标记过程。&lt;h4&gt;方法&lt;/h4&gt;使用EEG数据评估注意力和大脑活动，心率指标、眼动追踪数据测量视觉注意，网络摄像头视频录制以及监控任务的日志记录等多模态数据进行可视化。&lt;h4&gt;主要发现&lt;/h4&gt;M2LADS系统能将多种生物信号及视频同步，并通过基于Web的仪表板展示参与者的行为和生理数据，使研究人员能够更详细地了解学习者在不同活动中的表现。&lt;h4&gt;结论&lt;/h4&gt;该系统的使用为教育研究领域提供了新的视角，增强了对计算机辅助学习环境中学生行为和生理状态的理解。&lt;h4&gt;翻译&lt;/h4&gt;我们介绍了一个名为M2LADS的基于Web的系统（用于生成多模态学习分析仪表板），旨在整合、同步、可视化并分析在使用生物传感器记录计算机辅助学习过程中的多模态数据。该系统在一个Web仪表板上展示了广泛的生物测量和行为数据，提供了对各种生理和活动相关指标的深入了解。可视化的多模态数据包括用于评估注意力和大脑活动的脑电图（EEG）数据、心率指标、通过眼动追踪来衡量视觉关注的数据、网络摄像头视频记录以及监控任务的日志。M2LADS旨在以两种关键方式帮助数据科学家：(1)提供参与者体验的综合视图，将所有数据按参与者的活动进行分类展示；(2)同步所有的生物信号和视频，使在错误活动中更容易重标记数据。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We present a demonstration of a web-based system called M2LADS ("System forGenerating Multimodal Learning Analytics Dashboards"), designed to integrate,synchronize, visualize, and analyze multimodal data recorded duringcomputer-based learning sessions with biosensors. This system presents a rangeof biometric and behavioral data on web-based dashboards, providing detailedinsights into various physiological and activity-based metrics. The multimodaldata visualized include electroencephalogram (EEG) data for assessing attentionand brain activity, heart rate metrics, eye-tracking data to measure visualattention, webcam video recordings, and activity logs of the monitored tasks.M2LADS aims to assist data scientists in two key ways: (1) by providing acomprehensive view of participants' experiences, displaying all datacategorized by the activities in which participants are engaged, and (2) bysynchronizing all biosignals and videos, facilitating easier data relabeling ifany activity information contains errors.</description>
      <author>example@mail.com (Alvaro Becerra, Roberto Daza, Ruth Cobos, Aythami Morales, Julian Fierrez)</author>
      <guid isPermaLink="false">2502.15363v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Research advances on fish feeding behavior recognition and intensity quantification methods in aquaculture</title>
      <link>http://arxiv.org/abs/2502.15311v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  22 pages, 4 figures,&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文综述了基于计算机视觉、声学和传感器单一模态的鱼类进食行为识别与强度量化方法的研究进展，并探讨了当前新兴多模态融合技术在鱼类进食行为识别与强度量化的应用。&lt;h4&gt;背景&lt;/h4&gt;鱼饲料投喂行为的识别与定量分析是水产养殖管理的关键部分，对于监测鱼类健康、指导饲喂工作和提高水产养殖效率具有重要意义。&lt;h4&gt;目的&lt;/h4&gt;为了更好地进行未来相关研究，本论文回顾了基于单一模态技术（计算机视觉、声学及传感器）的研究进展，并探讨多模态融合在该领域的应用。此外还分析了各种方法的优缺点并展望了未来的研发方向。&lt;h4&gt;方法&lt;/h4&gt;论文首先总结了单模态技术如计算机视觉、声学和传感器技术在此研究领域内的研究成果，随后详细介绍了新兴的多模态融合技术的应用情况。&lt;h4&gt;主要发现&lt;/h4&gt;通过对比分析不同技术的优点与不足之处，揭示未来的研究趋势和方向。&lt;h4&gt;结论&lt;/h4&gt;基于对已有工作的回顾以及当前研究状况的理解，论文认为结合多种数据源和技术手段（特别是多模态方法）来解决鱼类进食行为识别及强度量化问题具有广阔的前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As a key part of aquaculture management, fish feeding behavior recognitionand intensity quantification has been a hot area of great concern toresearchers, and it plays a crucial role in monitoring fish health, guidingbaiting work and improving aquaculture efficiency. In order to better carry outthe related work in the future, this paper firstly reviews the researchadvances of fish feeding behavior recognition and intensity quantificationmethods based on computer vision, acoustics and sensors in a single modality.Then the application of the current emerging multimodal fusion in fish feedingbehavior recognition and intensity quantification methods is expounded.Finally, the advantages and disadvantages of various techniques are comparedand analyzed, and the future research directions are envisioned.</description>
      <author>example@mail.com (Shulong Zhang, Daoliang Li, Jiayin Zhao, Mingyuan Yao, Yingyi Chen, Yukang Huo, Xiao Liu, Haihua Wang)</author>
      <guid isPermaLink="false">2502.15311v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Q-PETR: Quant-aware Position Embedding Transformation for Multi-View 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.15488v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种新的量化感知位置嵌入变换(Q-PETR)方法，以提高多视角3D物体检测模型在INT8推理时的精度。&lt;h4&gt;背景&lt;/h4&gt;PETR系列的方法在3D感知领域占据主导地位，并成为现代自动驾驶系统的关键组件。但是，在需要INT8推理的情况下，这些模型的量化性能显著下降。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的量化感知位置嵌入变换方法(Q-PETR)，以解决PETR系列方法在INT8推理时精度下降的问题。&lt;h4&gt;方法&lt;/h4&gt;设计了一种针对多视角3D物体检测任务的量化友好的位置嵌入转换机制，即Q-PETR，它可以在保持原始性能的同时提供更友好的部署环境。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在标准的8位定点后训练量化中，该方法将mAP和NDS下降限制在1%以内。此外，该方法还超过了原PETR模型在浮点精度上的表现，并且具有广泛的泛化能力。&lt;h4&gt;结论&lt;/h4&gt;Q-PETR提供了一种同时解决性能和部署问题的有效解决方案，在INT8推理时能够大幅缩小与FP32推理之间的准确度差距。&lt;h4&gt;翻译&lt;/h4&gt;基于PETR的方法已经在3D感知领域占据主导地位，越来越成为现代自动驾驶系统中的关键组件。然而，当需要进行INT8推理时，它们的量化表现显著下降，例如在NuScenes数据集上分别导致了58.2% mAP和36.9% NDS的性能损失。为了解决这一问题，我们提出了一种针对多视角3D物体检测任务的量化感知位置嵌入变换方法(Q-PETR)，它提供了更加友好的量化部署环境同时保持了PETR的原始性能。此外，该方法在标准8位定点后训练量化下大幅缩小了INT8和FP32推理之间的准确度差距，并且在浮点精度上超过了原PETR模型的表现。通过针对多种PETR系列模型进行广泛的实验验证了其泛化能力。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; PETR-based methods have dominated benchmarks in 3D perception and areincreasingly becoming a key component in modern autonomous driving systems.However, their quantization performance significantly degrades when INT8inference is required, with a degradation of 58.2% in mAP and 36.9% in NDS onthe NuScenes dataset. To address this issue, we propose a quantization-awareposition embedding transformation for multi-view 3D object detection, termedQ-PETR. Q-PETR offers a quantizationfriendly and deployment-friendlyarchitecture while preserving the original performance of PETR. Itsubstantially narrows the accuracy gap between INT8 and FP32 inference forPETR-series methods. Without bells and whistles, our approach reduces the mAPand NDS drop to within 1% under standard 8-bit per-tensor post-trainingquantization. Furthermore, our method exceeds the performance of the originalPETR in terms of floating-point precision. Extensive experiments across avariety of PETR-series models demonstrate its broad generalization.</description>
      <author>example@mail.com (Jiangyong Yu, Changyong Shu, Dawei Yang, Zichen Yu, Xing Hu, Yan Chen)</author>
      <guid isPermaLink="false">2502.15488v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>CoDiff: Conditional Diffusion Model for Collaborative 3D Object Detection</title>
      <link>http://arxiv.org/abs/2502.14891v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;合作3D物体检测在自动驾驶领域具有重要意义，它通过多代理系统之间的信息交换极大地增强了单个代理的感知能力。&lt;h4&gt;背景&lt;/h4&gt;由于姿态估计误差和时间延迟的影响，在实际应用中，跨多个代理的信息融合通常会导致带有空间和时间噪声的功能表示，并导致检测错误。&lt;h4&gt;目的&lt;/h4&gt;为了应对多代理系统之间存在的噪音问题，我们探索了使用扩散模型来净化嘈杂样本并将其转化为理想数据的可能性。&lt;h4&gt;方法&lt;/h4&gt;提出了CoDiff框架，该框架利用预训练的自编码器的强大潜在空间将高维特征图转换为低维度，并通过条件引导的方式让各个代理的信息指导扩散模型进行采样。这一过程可以去除粗糙特征图中的噪声，并逐步细化融合后的特征。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和真实世界数据集上的实验研究表明，所提出的CoDiff框架在合作物体检测性能方面比现有的相关方法更加出色，尤其当代理的姿态信息和延迟带有高水平的噪音时，其表现出高度期望的鲁棒性。&lt;h4&gt;结论&lt;/h4&gt;这是首次将扩散模型应用于多代理协作感知的工作。该工作表明了扩散模型解决多代理系统中噪声问题的巨大潜力。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了合作3D物体检测在自动驾驶中的重要性，指出当前方法面临的挑战，并提出了一种新的框架CoDiff，利用扩散模型来提高特征表示的质量和清晰度，实验结果证明其优越性和鲁棒性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Collaborative 3D object detection holds significant importance in the fieldof autonomous driving, as it greatly enhances the perception capabilities ofeach individual agent by facilitating information exchange among multipleagents. However, in practice, due to pose estimation errors and time delays,the fusion of information across agents often results in featurerepresentations with spatial and temporal noise, leading to detection errors.Diffusion models naturally have the ability to denoise noisy samples to theideal data, which motivates us to explore the use of diffusion models toaddress the noise problem between multi-agent systems. In this work, we proposeCoDiff, a novel robust collaborative perception framework that leverages thepotential of diffusion models to generate more comprehensive and clearerfeature representations. To the best of our knowledge, this is the first workto apply diffusion models to multi-agent collaborative perception.Specifically, we project high-dimensional feature map into the latent space ofa powerful pre-trained autoencoder. Within this space, individual agentinformation serves as a condition to guide the diffusion model's sampling. Thisprocess denoises coarse feature maps and progressively refines the fusedfeatures. Experimental study on both simulated and real-world datasetsdemonstrates that the proposed framework CoDiff consistently outperformsexisting relevant methods in terms of the collaborative object detectionperformance, and exhibits highly desired robustness when the pose and delayinformation of agents is with high-level noise.</description>
      <author>example@mail.com (Zhe Huang, Shuo Wang, Yongcai Wang, Lei Wang)</author>
      <guid isPermaLink="false">2502.14891v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Network Resource Optimization for ML-Based UAV Condition Monitoring with Vibration Analysis</title>
      <link>http://arxiv.org/abs/2502.15491v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for publication in IEEE Networking Letters&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;随着智慧城市的发展，无人飞行器（UAVs）及其可靠性变得越来越重要。本文通过优化基于机器学习的UAV条件监测框架中的网络资源利用来提高其在边缘计算环境下的效率。&lt;h4&gt;背景&lt;/h4&gt;智慧城市的构建推动了对UAV可靠性的需求，其中机器学习模型用于识别异常和不利条件是关键环节之一。&lt;h4&gt;目的&lt;/h4&gt;探索如何最小化下一代边缘网络中珍贵的网络资源使用，并优化基于ML的UAV状态监测框架中的网络资源配置。&lt;h4&gt;方法&lt;/h4&gt;开发了一种利用实验数据并调整特征提取聚合间隔来选择最有效机器学习模型的方法，同时采用维度降低技术减少了99.9%的网络资源消耗。&lt;h4&gt;主要发现&lt;/h4&gt;通过上述方法，在保证准确性的前提下显著降低了网络资源使用量。&lt;h4&gt;结论&lt;/h4&gt;提出的框架能够有效地减少基于ML的UAV状态监测系统所需的网络资源，并提高了在有限资源条件下的性能表现。&lt;h4&gt;翻译&lt;/h4&gt;随着智慧城市的发展，无人飞行器（UAVs）及其可靠性变得越来越重要。本文通过优化基于机器学习的UAV条件监测框架中的网络资源利用来提高其在边缘计算环境下的效率。研究指出，在资源受限的下一代边缘网络环境中，需要尽可能地减少珍贵的网络资源使用量。所提出的方法通过调整特征提取聚合间隔，并采用维度降低技术实现了这一目标，同时保证了模型性能和准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; As smart cities begin to materialize, the role of Unmanned Aerial Vehicles(UAVs) and their reliability becomes increasingly important. One aspect ofreliability relates to Condition Monitoring (CM), where Machine Learning (ML)models are leveraged to identify abnormal and adverse conditions. Given theresource-constrained nature of next-generation edge networks, the utilizationof precious network resources must be minimized. This work explores theoptimization of network resources for ML-based UAV CM frameworks. The developedframework uses experimental data and varies the feature extraction aggregationinterval to optimize ML model selection. Additionally, by leveragingdimensionality reduction techniques, there is a 99.9% reduction in networkresource consumption.</description>
      <author>example@mail.com (Alexandre Gemayel, Dimitrios Michael Manias, Abdallah Shami)</author>
      <guid isPermaLink="false">2502.15491v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>BOSS: Benchmark for Observation Space Shift in Long-Horizon Task</title>
      <link>http://arxiv.org/abs/2502.15679v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种用于评估观察空间移位（OSS）对长时任务影响的新基准测试BOSS，并展示了几种模仿学习算法在面对此类问题时的性能下降情况。&lt;h4&gt;背景&lt;/h4&gt;视觉伺服机器人旨在完成前所未见的长期任务，而分层方法通过执行由任务计划器安排的技能组合来实现这一目标。然而，在简单如技巧串联的任务中，观察空间移位的问题会破坏单独训练的技能策略的表现。&lt;h4&gt;目的&lt;/h4&gt;提出并验证BOSS基准测试，评估模仿学习算法在面对观察空间移位问题时的性能下降情况，并探索解决OSS的方法。&lt;h4&gt;方法&lt;/h4&gt;引入了BOSS（观察空间移位基准）来衡量和评估观察空间移位对长时任务的影响。BOSS包括三个不同的挑战：单一谓词转移、累积谓词转移和技巧串联，用于测试不同方面的负面影响。此外，作者还测试了几种流行的模仿学习算法在BOSS上的表现。&lt;h4&gt;主要发现&lt;/h4&gt;在最简单的挑战下，各种算法性能下降显著，分别为67%，35%，34%和54%。增加训练数据的规模以解决OSS问题的方法并未达到预期效果。&lt;h4&gt;结论&lt;/h4&gt;观察空间移位对长时任务中技能策略的表现具有负面影响，而现有解决方案不足以完全解决这一问题。&lt;h4&gt;翻译&lt;/h4&gt;机器人技术长期以来一直致力于开发能够完成未见过的长期任务的视觉伺服机器人。分层方法通过执行由任务计划器安排的技能组合来实现这个目标，并且每个视觉运动技能都使用特定的模仿学习（IL）算法预先训练。然而，即使在简单的长期任务如技巧串联中，分层方法也常常因观察空间移位问题而难以实现目标。为了验证这一问题并评估其对长期任务的影响，我们引入了BOSS基准测试来衡量这个问题。BOSS包括三个不同的挑战：“单一谓词转移”、“累积谓词转移”和“技巧串联”，每个挑战都旨在评估OSS的负面影响的不同方面。我们在BOSS上评估了几种最近流行的IL算法，其中包括三种行为克隆方法和视觉语言动作模型OpenVLA。即使在最简单的挑战中，我们观察到当技能性能在有无观察空间移位的情况下对比时，平均性能下降分别为67%，35%，34%和54%。此外，我们研究了一种解决OSS的潜在解决方案，即通过使用更大且视觉上更加多样的示例数据集来增加每个技能训练数据的规模，但结果显示这种方法不足以解决问题。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robotics has long sought to develop visual-servoing robots capable ofcompleting previously unseen long-horizon tasks. Hierarchical approaches offera pathway for achieving this goal by executing skill combinations arranged by atask planner, with each visuomotor skill pre-trained using a specific imitationlearning (IL) algorithm. However, even in simple long-horizon tasks like skillchaining, hierarchical approaches often struggle due to a problem we identifyas Observation Space Shift (OSS), where the sequential execution of precedingskills causes shifts in the observation space, disrupting the performance ofsubsequent individually trained skill policies. To validate OSS and evaluateits impact on long-horizon tasks, we introduce BOSS (a Benchmark forObservation Space Shift). BOSS comprises three distinct challenges: "SinglePredicate Shift", "Accumulated Predicate Shift", and "Skill Chaining", eachdesigned to assess a different aspect of OSS's negative effect. We evaluatedseveral recent popular IL algorithms on BOSS, including three BehavioralCloning methods and the Visual Language Action model OpenVLA. Even on thesimplest challenge, we observed average performance drops of 67%, 35%, 34%, and54%, respectively, when comparing skill performance with and without OSS.Additionally, we investigate a potential solution to OSS that scales up thetraining data for each skill with a larger and more visually diverse set ofdemonstrations, with our results showing it is not sufficient to resolve OSS.The project page is: https://boss-benchmark.github.io/</description>
      <author>example@mail.com (Yue Yang, Linfeng Zhao, Mingyu Ding, Gedas Bertasius, Daniel Szafir)</author>
      <guid isPermaLink="false">2502.15679v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>VaViM and VaVAM: Autonomous Driving through Video Generative Modeling</title>
      <link>http://arxiv.org/abs/2502.15672v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Code and model: https://github.com/valeoai/VideoActionModel, project  page: https://valeoai.github.io/vavim-vavam/&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究探讨了大规模生成式视频模型在自动驾驶中的潜力，介绍了开源的自回归视频模型（VaViM）和其辅助视频动作模型（VaVAM），以探索视频预训练如何应用于实际驾驶。&lt;h4&gt;背景&lt;/h4&gt;随着深度学习的发展，视频生成技术被引入到自动驾驶领域，特别是在理解和预测复杂的动态场景方面具有巨大潜力。&lt;h4&gt;目的&lt;/h4&gt;研究的目的是通过开发新的视频预训练模型来提升自动驾驶系统在真实世界中的表现和安全性。&lt;h4&gt;方法&lt;/h4&gt;VaViM是一个简单的自回归视频模型，通过时空令牌序列预测帧；VaVAM则利用VaViM学习到的表示生成驾驶轨迹。两个模型共同形成了从感知到动作的完整管道，并且研究者们对其进行了开放循环和闭环驾驶场景评估。&lt;h4&gt;主要发现&lt;/h4&gt;研究表明，基于视频的预训练对于自动驾驶具有前景，包括所学表示的语义丰富性、视频合成中规模效应的好处以及在闭环评估中的模型大小与数据之间的复杂关系及其对安全度量的影响。&lt;h4&gt;结论&lt;/h4&gt;通过发布代码和模型权重，研究团队希望促进相关领域的进一步发展，并鼓励其他研究人员探索该方向的潜力。&lt;h4&gt;翻译&lt;/h4&gt;我们探讨了大规模生成式视频模型在自主驾驶中的潜力，介绍了开源自回归视频模型（VaViM）及其辅助动作视频模型（VaVAM），以探究视频预训练如何应用于现实世界的自动驾驶。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We explore the potential of large-scale generative video models forautonomous driving, introducing an open-source auto-regressive video model(VaViM) and its companion video-action model (VaVAM) to investigate how videopre-training transfers to real-world driving. VaViM is a simple auto-regressivevideo model that predicts frames using spatio-temporal token sequences. We showthat it captures the semantics and dynamics of driving scenes. VaVAM, thevideo-action model, leverages the learned representations of VaViM to generatedriving trajectories through imitation learning. Together, the models form acomplete perception-to-action pipeline. We evaluate our models in open- andclosed-loop driving scenarios, revealing that video-based pre-training holdspromise for autonomous driving. Key insights include the semantic richness ofthe learned representations, the benefits of scaling for video synthesis, andthe complex relationship between model size, data, and safety metrics inclosed-loop evaluations. We release code and model weights athttps://github.com/valeoai/VideoActionModel</description>
      <author>example@mail.com (Florent Bartoccioni, Elias Ramzi, Victor Besnier, Shashanka Venkataramanan, Tuan-Hung Vu, Yihong Xu, Loick Chambon, Spyros Gidaris, Serkan Odabas, David Hurych, Renaud Marlet, Alexandre Boulch, Mickael Chen, Éloi Zablocki, Andrei Bursuc, Eduardo Valle, Matthieu Cord)</author>
      <guid isPermaLink="false">2502.15672v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Automating Curriculum Learning for Reinforcement Learning using a Skill-Based Bayesian Network</title>
      <link>http://arxiv.org/abs/2502.15662v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;论文提出了一种基于技能-环境贝叶斯网络(SEBN)的方法，以减少强化学习训练时间或提高目标任务的性能。&lt;h4&gt;背景&lt;/h4&gt;在强化学习中，自动生成课程以减少训练时间和提升性能是主要挑战之一。&lt;h4&gt;目的&lt;/h4&gt;通过使用SEBN模型来预测代理在各种任务上的表现，并根据这些预测来加权可能的任务，从而开发一种算法来优化课程设置。&lt;h4&gt;方法&lt;/h4&gt;利用SEBN模型对代理成功概率的推断估计来评估下一个潜在任务的预期改进。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在三种不同环境中（离散格子世界、连续控制和模拟机器人）使用SEBN构建的课程比其他基准线更有效。&lt;h4&gt;结论&lt;/h4&gt;通过将技能与环境特征及奖励结构相关联，SEBN能够预测代理在各种任务上的表现并优化学习过程中的课程设置。&lt;h4&gt;翻译&lt;/h4&gt;摘要介绍了SEBN模型及其用于减少训练时间和提升性能的方法，并展示了它优于传统基线的实验结果。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A major challenge for reinforcement learning is automatically generatingcurricula to reduce training time or improve performance in some target task.We introduce SEBNs (Skill-Environment Bayesian Networks) which model aprobabilistic relationship between a set of skills, a set of goals that relateto the reward structure, and a set of environment features to predict policyperformance on (possibly unseen) tasks. We develop an algorithm that uses theinferred estimates of agent success from SEBN to weigh the possible next tasksby expected improvement. We evaluate the benefit of the resulting curriculum onthree environments: a discrete gridworld, continuous control, and simulatedrobotics. The results show that curricula constructed using SEBN frequentlyoutperform other baselines.</description>
      <author>example@mail.com (Vincent Hsiao, Mark Roberts, Laura M. Hiatt, George Konidaris, Dana Nau)</author>
      <guid isPermaLink="false">2502.15662v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>A Simulation Pipeline to Facilitate Real-World Robotic Reinforcement Learning Applications</title>
      <link>http://arxiv.org/abs/2502.15649v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Paper accepted to be presented at IEEE SysCon 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;一种帮助减少仿真与现实差距、促进在真实世界机器人系统中开发和部署强化学习策略的流水线。&lt;h4&gt;背景&lt;/h4&gt;强化学习（RL）在解决复杂任务方面取得成功，特别是在机器人应用领域。然而，在物理机器人上实现它仍然充满挑战性，主要是由于安全风险和高昂的训练成本。为了解决这些问题，通常是在模拟器中训练RL代理，这又引入了关于仿真与现实之间差距的新问题。&lt;h4&gt;目的&lt;/h4&gt;提出一种流水线来帮助减少仿真实验到现实操作之间的差距，并促进强化学习策略在实际机器人系统中的开发和部署。&lt;h4&gt;方法&lt;/h4&gt;该流程将RL的培训过程组织为初始的系统识别阶段，以及三个训练阶段：核心仿真培训、高保真仿真，然后是实地部署。每个阶段都会增加现实感的程度以减少模拟到真实之间的差距，并且通过迭代传递并改进策略来逐步达到所需性能。&lt;h4&gt;主要发现&lt;/h4&gt;该流水线的有效性在一项案例研究中得到证明，在这项研究中使用了Boston Dynamics Spot移动机器人执行监控应用。&lt;h4&gt;结论&lt;/h4&gt;提出的RL流程展示了通过各个阶段如何逐渐减少仿真与现实之间的差距，使开发的策略能够成功部署于真实环境中的机器人系统。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Reinforcement learning (RL) has gained traction for its success in solvingcomplex tasks for robotic applications. However, its deployment on physicalrobots remains challenging due to safety risks and the comparatively high costsof training. To avoid these problems, RL agents are often trained onsimulators, which introduces a new problem related to the gap betweensimulation and reality. This paper presents an RL pipeline designed to helpreduce the reality gap and facilitate developing and deploying RL policies forreal-world robotic systems. The pipeline organizes the RL training process intoan initial step for system identification and three training stages: coresimulation training, high-fidelity simulation, and real-world deployment, eachadding levels of realism to reduce the sim-to-real gap. Each training stagetakes an input policy, improves it, and either passes the improved policy tothe next stage or loops it back for further improvement. This iterative processcontinues until the policy achieves the desired performance. The pipeline'seffectiveness is shown through a case study with the Boston Dynamics Spotmobile robot used in a surveillance application. The case study presents thesteps taken at each pipeline stage to obtain an RL agent to control the robot'sposition and orientation.</description>
      <author>example@mail.com (Jefferson Silveira, Joshua A. Marshall, Sidney N. Givigi Jr)</author>
      <guid isPermaLink="false">2502.15649v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Reduced-Order Model Guided Contact-Implicit Model Predictive Control for Humanoid Locomotion</title>
      <link>http://arxiv.org/abs/2502.15630v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种结合简化的混合线性倒立摆模型（HLIP）和接触隐式模型预测控制（CI-MPC）优点的控制框架，旨在提高人形机器人的灵活性和实用性。&lt;h4&gt;背景&lt;/h4&gt;人形机器人在人类环境中操作具有巨大的应用潜力，但由于高维度非线性混合动力学的复杂性，部署面临挑战。虽然HLIP简化了模型，但丧失了全身表达能力；而CI-MPC能够处理多种接触模式下的规划问题，但仍存在局部最优和大量调优需求。&lt;h4&gt;目的&lt;/h4&gt;提出一种结合HLIP与CI-MPC优点的新控制框架，以克服当前方法的局限性，并增强人形机器人的适应性和实用性。&lt;h4&gt;方法&lt;/h4&gt;该框架利用HLIP生成名义步态模式，同时使用CI-MPC处理全身动力学并根据需要调整接触序列。实验在24自由度的人形机器人Achilles上进行模拟测试。&lt;h4&gt;主要发现&lt;/h4&gt;新提出的控制框架能够在粗糙地形行走、恢复外部干扰后的稳定性以及面对模型和状态不确定性时保持鲁棒性，同时能够与环境中的障碍物互动，并且以50Hz的频率实现实时在线运行。&lt;h4&gt;结论&lt;/h4&gt;结合HLIP和CI-MPC的优点可以显著提升人形机器人在复杂环境下的控制性能和适应能力。该框架为未来的人形机器人开发提供了新的可能性。&lt;h4&gt;翻译&lt;/h4&gt;人形机器人的潜在应用领域因它们能在为人设计的环境中操作而广受期待，但其部署受到管理高维度非线性混合动力学挑战的影响。虽然简化的模型如HLIP简单且计算效率高，但这些模型缺乏全身表达能力。最近在CI-MPC上的进展使机器人能够通过多个混合接触模式进行规划，但仍容易陷入局部最优，并需要大量的调优工作。我们提出了一种结合HLIP和CI-MPC优点的控制框架：简化的模型产生名义步态，而CI-MPC管理全身动力学并根据需要调整接触安排。我们在模拟中使用一个新型24自由度的人形机器人Achilles展示了这种方法的有效性。我们的方法实现了粗糙地形行走、干扰恢复能力，在面对模型和状态不确定性时保持鲁棒性，并且能够与环境中的障碍物互动，所有这一切都在实时在线环境中以50Hz的频率运行。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Humanoid robots have great potential for real-world applications due to theirability to operate in environments built for humans, but their deployment ishindered by the challenge of controlling their underlying high-dimensionalnonlinear hybrid dynamics. While reduced-order models like the Hybrid LinearInverted Pendulum (HLIP) are simple and computationally efficient, they losewhole-body expressiveness. Meanwhile, recent advances in Contact-Implicit ModelPredictive Control (CI-MPC) enable robots to plan through multiple hybridcontact modes, but remain vulnerable to local minima and require significanttuning. We propose a control framework that combines the strengths of HLIP andCI-MPC. The reduced-order model generates a nominal gait, while CI-MPC managesthe whole-body dynamics and modifies the contact schedule as needed. Wedemonstrate the effectiveness of this approach in simulation with a novel 24degree-of-freedom humanoid robot: Achilles. Our proposed framework achievesrough terrain walking, disturbance recovery, robustness under model and stateuncertainty, and allows the robot to interact with obstacles in theenvironment, all while running online in real-time at 50 Hz.</description>
      <author>example@mail.com (Sergio A. Esteban, Vince Kurtz, Adrian B. Ghansah, Aaron D. Ames)</author>
      <guid isPermaLink="false">2502.15630v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Pick-and-place Manipulation Across Grippers Without Retraining: A Learning-optimization Diffusion Policy Approach</title>
      <link>http://arxiv.org/abs/2502.15613v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Video and code are available at https://github.com/yaoxt3/GADP&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一个基于扩散的策略和混合学习优化框架，使机器人能在零样本条件下适应新的夹爪配置。&lt;h4&gt;背景&lt;/h4&gt;当前大多数抓取放置策略需要在训练和推理阶段保持一致的夹爪设置，这会导致高昂的成本，特别是在使用模仿学习方法时。当要适应新类型的末端执行器（即夹爪）时，这一问题尤为突出。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的策略以减少为适应不同夹爪而进行额外训练或微调的需求。&lt;h4&gt;方法&lt;/h4&gt;利用基于扩散的优化策略，在推理阶段动态地强制执行机械和安全约束。通过这种受限的去噪过程，该策略能够根据具体的夹爪参数（如工具中心点偏移量、颚宽）调整轨迹，同时确保碰撞避免和任务可行性。&lt;h4&gt;主要发现&lt;/h4&gt;在六种不同的夹爪配置上进行实验验证后，提出的方法实现了93.3%的平均任务成功率，而扩散政策基线方法的成功率仅为23.3-26.7%。该策略支持工具中心点偏移量从16至23.5厘米以及颚宽从7.5到11.5厘米的变化。&lt;h4&gt;结论&lt;/h4&gt;通过引入受限的扩散过程，可以实现跨夹爪操作的鲁棒性，并且保持了模仿学习方法的样本效率。这消除了针对特定夹爪进行重新训练的需求。&lt;h4&gt;翻译&lt;/h4&gt;摘要内容的具体中文翻译：当前抓取放置策略通常需要在训练和推理阶段维持一致的机械臂末端执行器设置，这一要求导致了高成本的再训练或微调需求，特别是在基于模仿学习的方法中。为解决这个问题，我们提出了一种扩散式的策略结合混合学习优化框架，使得机器人可以无须额外的数据收集便能在新的夹爪上进行零样本适应。在训练过程中，该政策通过使用基础夹爪采集的演示数据来学习抓取和放置的基本操作方法。而在推理阶段，基于扩散的优化策略动态地施加机械和安全约束，确保生成的动作轨迹与未见过的新夹爪的实际物理特性相匹配。这一过程通过一个受限去噪程序实现，该程序能够适应特定夹爪参数（例如工具中心点偏移量、颚宽）的同时保持碰撞避免和任务可行性。我们在Franka Panda机器人上进行了一系列实验测试，在六种不同的夹爪配置中验证了我们的方法的有效性，包括3D打印的手指末端执行器、柔软的硅胶抓手以及Robotiq 2F-85夹爪等。与扩散政策基线相比，我们提出的策略达到了93.3%的平均任务成功率（相比之下基线为23.3至26.7%），支持工具中心点偏移量从16到23.5厘米和颚宽范围从7.5到11.5厘米的变化。实验结果表明，受限扩散过程能够实现跨不同夹爪配置操作的鲁棒性同时维持了模仿学习方法的样本效率，并且无需为特定类型夹爪重新训练策略。代码与视频可在https://github.com/yaoxt3/GADP上获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Current robotic pick-and-place policies typically require consistent gripperconfigurations across training and inference. This constraint imposes highretraining or fine-tuning costs, especially for imitation learning-basedapproaches, when adapting to new end-effectors. To mitigate this issue, wepresent a diffusion-based policy with a hybrid learning-optimization framework,enabling zero-shot adaptation to novel grippers without additional datacollection for retraining policy. During training, the policy learnsmanipulation primitives from demonstrations collected using a base gripper. Atinference, a diffusion-based optimization strategy dynamically enforceskinematic and safety constraints, ensuring that generated trajectories alignwith the physical properties of unseen grippers. This is achieved through aconstrained denoising procedure that adapts trajectories to gripper-specificparameters (e.g., tool-center-point offsets, jaw widths) while preservingcollision avoidance and task feasibility. We validate our method on a FrankaPanda robot across six gripper configurations, including 3D-printed fingertips,flexible silicone gripper, and Robotiq 2F-85 gripper. Our approach achieves a93.3% average task success rate across grippers (vs. 23.3-26.7% for diffusionpolicy baselines), supporting tool-center-point variations of 16-23.5 cm andjaw widths of 7.5-11.5 cm. The results demonstrate that constrained diffusionenables robust cross-gripper manipulation while maintaining the sampleefficiency of imitation learning, eliminating the need for gripper-specificretraining. Video and code are available at https://github.com/yaoxt3/GADP.</description>
      <author>example@mail.com (Xiangtong Yao, Yirui Zhou, Yuan Meng, Liangyu Dong, Lin Hong, Zitao Zhang, Zhenshan Bing, Kai Huang, Fuchun Sun, Alois Knoll)</author>
      <guid isPermaLink="false">2502.15613v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Autonomous helicopter aerial refueling: controller design and performance guarantees</title>
      <link>http://arxiv.org/abs/2502.15562v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;本文提出了一种用于自主直升机空中加油的控制设计方法、稳定性标准和性能界限。&lt;h4&gt;背景&lt;/h4&gt;自主空中加油由于加油机尾流的影响、接触敏感操作特性和加油管运动不确定性而变得十分困难。此外，探针位于直升机重心之外，其位置与速度对直升机姿态及其角速率非常敏感。&lt;h4&gt;目的&lt;/h4&gt;为了提高自主空中加油的性能和稳定性，提出了一种新的外环位置控制器，并使用闭环误差动力学中的极限有界性特性来提供分析保证。&lt;h4&gt;方法&lt;/h4&gt;提出了一个将探针的位置和速度纳入反馈回路的新外环位置控制器。通过在高保真UH60直升机模型中进行仿真测试，验证了新控制策略的有效性和鲁棒性。&lt;h4&gt;主要发现&lt;/h4&gt;新的控制方法能够显著减少2-范数对接误差，与现有标准控制器相比改进了36%。&lt;h4&gt;结论&lt;/h4&gt;该研究证明了所提出的自主空中加油控制系统在复杂和动态环境中的有效性，并为未来的应用提供了理论基础和技术支持。&lt;h4&gt;翻译&lt;/h4&gt;在这篇文章中，我们提出了一种用于无人直升机空中加油的控制设计方法、稳定性标准和性能界限。自主空中加油由于受到加油机尾流影响、操作接触敏感性和加油管运动不确定性的限制而变得非常困难。探针位置远离直升机重心，其位置（速度）对直升机姿态（角速率）极其敏感。此外，为了匹配加油机的速度，直升机需要高速运行并保持特定的姿态，这使得对接更加具有挑战性。我们提出了一种新的外环位置控制器，将探针的位置和速度纳入反馈回路中。通过闭环误差动态的极限有界特性，推导了关于对接性能与加油管运动不确定性及直升机角加速度之间的关系的分析保证。在考虑风力影响的情况下，利用高保真度UH60直升机模型进行了仿真测试，以验证新方法在现实场景中的有效性。高保真度模拟显示，相比于现有标准控制器，所提出的控制策略能将2-范数对接误差减少36%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In this paper, we present a control design methodology, stability criteria,and performance bounds for autonomous helicopter aerial refueling. Autonomousaerial refueling is particularly difficult due to the aerodynamic interactionbetween the wake of the tanker, the contact-sensitive nature of the maneuver,and the uncertainty in drogue motion. Since the probe tip is locatedsignificantly away from the helicopter's center-of-gravity, its position (andvelocity) is strongly sensitive to the helicopter's attitude (and angularrates). In addition, the fact that the helicopter is operating at high speedsto match the velocity of the tanker forces it to maintain a particularorientation, making the docking maneuver especially challenging. In this paper,we propose a novel outer-loop position controller that incorporates the probeposition and velocity into the feedback loop. The position and velocity of theprobe tip depend both on the position (velocity) and on the attitude (angularrates) of the aircraft. We derive analytical guarantees for docking performancein terms of the uncertainty of the drogue motion and the angular accelerationof the helicopter, using the ultimate boundedness property of the closed-looperror dynamics. Simulations are performed on a high-fidelity UH60 helicoptermodel with a high-fidelity drogue motion under wind effects to validate theproposed approach for realistic refueling scenarios. These high-fidelitysimulations reveal that the proposed control methodology yields an improvementof 36% in the 2-norm docking error compared to the existing standardcontroller.</description>
      <author>example@mail.com (Damsara Jayarathne, Santiago Paternain, Sandipan Mishra)</author>
      <guid isPermaLink="false">2502.15562v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Enhanced Probabilistic Collision Detection for Motion Planning Under Sensing Uncertainty</title>
      <link>http://arxiv.org/abs/2502.15525v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了用于机器人在非结构化环境中的运动规划的增强概率碰撞检测（PCD）方法。&lt;h4&gt;背景&lt;/h4&gt;现有的PCD方法主要使用简化的几何模型，且仅考虑位置估计误差，未充分考虑到姿态估计误差和形状精度的问题。&lt;h4&gt;目的&lt;/h4&gt;开发一种改进的方法，以提高在感知不确定性下的鲁棒性，并减少路径长度与规划时间。&lt;h4&gt;方法&lt;/h4&gt;{'要点1': '利用超二次曲面（superquadrics）进行更精确的形状近似', '要点2': '考虑位置和姿态估计误差，通过扩大每个物体的表面来封装其观察到的所有旋转副本'}&lt;h4&gt;主要发现&lt;/h4&gt;该PCD方法比现有最佳方法更接近蒙特卡洛采样的基线，并且在减少路径长度和规划时间方面分别表现出色。&lt;h4&gt;结论&lt;/h4&gt;研究证明了考虑姿态估计误差的重要性，当仅考虑位置估计误差或忽略时，在仿真中执行计划路径的碰撞概率远高于此方法。&lt;h4&gt;翻译&lt;/h4&gt;概率碰撞检测（PCD）对于操作于非结构化环境中的机器人运动规划至关重要，通过考虑到感知不确定性有助于防止损坏。现有PCD方法主要使用简化的几何模型，并且仅解决位置估计误差问题。本文提出了一种增强的PCD方法，具有两个关键改进：(a) 使用超二次曲面进行更准确的形状近似；(b) 考虑到位置和姿态估计误差以提高在感知不确定性下的鲁棒性。该方法首先为每个对象计算一个扩大的表面，该表面封装了其观察到的所有旋转副本，从而解决了姿态估计误差问题。然后将位置估计误差下的碰撞概率作为机会约束问题进行公式化，并通过超二次曲面的正常参数化求解紧致上限。结果表明，与现有最佳PCD方法相比，该方法更接近蒙特卡洛采样的基线，并且在减少路径长度和规划时间方面表现出色。一种Real2Sim管道进一步验证了考虑姿态估计误差的重要性：执行仿真中计划路径的碰撞概率仅为2%，而仅考虑位置估计误差或完全不考虑时则分别为9% 和 29%。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Probabilistic collision detection (PCD) is essential in motion planning forrobots operating in unstructured environments, where considering sensinguncertainty helps prevent damage. Existing PCD methods mainly used simplifiedgeometric models and addressed only position estimation errors. This paperpresents an enhanced PCD method with two key advancements: (a) usingsuperquadrics for more accurate shape approximation and (b) accounting for bothposition and orientation estimation errors to improve robustness under sensinguncertainty. Our method first computes an enlarged surface for each object thatencapsulates its observed rotated copies, thereby addressing the orientationestimation errors. Then, the collision probability under the positionestimation errors is formulated as a chance-constraint problem that is solvedwith a tight upper bound. Both the two steps leverage the recently developednormal parameterization of superquadric surfaces. Results show that our PCDmethod is twice as close to the Monte-Carlo sampled baseline as the bestexisting PCD method and reduces path length by 30% and planning time by 37%,respectively. A Real2Sim pipeline further validates the importance ofconsidering orientation estimation errors, showing that the collisionprobability of executing the planned path in simulation is only 2%, compared to9% and 29% when considering only position estimation errors or none at all.</description>
      <author>example@mail.com (Xiaoli Wang, Sipu Ruan, Xin Meng, Gregory Chirikjian)</author>
      <guid isPermaLink="false">2502.15525v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Robust 4D Radar-aided Inertial Navigation for Aerial Vehicles</title>
      <link>http://arxiv.org/abs/2502.15452v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种基于高效且鲁棒的误差状态卡尔曼滤波器(ESKF)雷达惯性导航系统的开发方案，用于无人驾驶飞行器(UAV)，并利用毫米波(MMW)雷达提供的稳健3D测距和多普勒速度测量来增强UAV在复杂环境下的导航能力。&lt;h4&gt;背景&lt;/h4&gt;随着激光雷达和摄像头在无人机上的广泛应用，在挑战性的环境中它们可能会变得不那么有效。相反，能够提供稳健的三维测距和多普勒速度测量的4D毫米波(MMW)雷达对于空中导航来说利用不够充分。&lt;h4&gt;目的&lt;/h4&gt;开发一种基于ESKF的方法来提高UAV利用毫米波雷达进行导航时的准确性和鲁棒性。&lt;h4&gt;方法&lt;/h4&gt;提出了一种点对分布雷达扫描匹配技术，以提供具有适当不确定性资格的动作约束，并结合多普勒速度测量结果紧密耦合地更新导航状态。此外还设计了一个基于关键帧的方法来对抗先前地图（如果可用的话），从而限制累积的导航误差并提供高精度的雷达辅助全局定位解决方案。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的现实世界实验验证，该提出的雷达增强惯性导航方法在准确性和鲁棒性方面都超过了现有的最先进的方法。&lt;h4&gt;结论&lt;/h4&gt;基于毫米波雷达和惯性传感器的数据融合技术可以提高UAV在复杂环境下的导航性能，并具有广阔的应用前景。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; While LiDAR and cameras are becoming ubiquitous for unmanned aerial vehicles(UAVs) but can be ineffective in challenging environments, 4D millimeter-wave(MMW) radars that can provide robust 3D ranging and Doppler velocitymeasurements are less exploited for aerial navigation. In this paper, wedevelop an efficient and robust error-state Kalman filter (ESKF)-basedradar-inertial navigation for UAVs. The key idea of the proposed approach isthe point-to-distribution radar scan matching to provide motion constraintswith proper uncertainty qualification, which are used to update the navigationstates in a tightly coupled manner, along with the Doppler velocitymeasurements. Moreover, we propose a robust keyframe-based matching schemeagainst the prior map (if available) to bound the accumulated navigation errorsand thus provide a radar-based global localization solution with high accuracy.Extensive real-world experimental validations have demonstrated that theproposed radar-aided inertial navigation outperforms state-of-the-art methodsin both accuracy and robustness.</description>
      <author>example@mail.com (Jinwen Zhu, Jun Hu, Xudong Zhao, Xiaoming Lang, Yinian Mao, Guoquan Huang)</author>
      <guid isPermaLink="false">2502.15452v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Learning Long-Horizon Robot Manipulation Skills via Privileged Action</title>
      <link>http://arxiv.org/abs/2502.15442v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;研究提出了一种结构化的框架，利用特权动作和课程学习来解决长期接触密集型任务中的强化学习挑战。&lt;h4&gt;背景&lt;/h4&gt;在处理长时序的、高维度状态空间的任务时，传统强化学习方法由于稀疏奖励导致探索效率低下且容易陷入局部最优。&lt;h4&gt;目的&lt;/h4&gt;开发一种高效的学习框架，在不依赖大量人工设计奖励或参考轨迹的情况下，使策略能够掌握长期技能。&lt;h4&gt;方法&lt;/h4&gt;在模拟环境中使用特权动作进行训练，包括放松约束条件和虚拟力等手段来增强对象交互和探索。通过课程学习逐步移除这些特权以逼近真实世界情况。&lt;h4&gt;主要发现&lt;/h4&gt;成功完成了涉及非抓取姿势物体提升的复杂多阶段长期任务，展示了方法的一般性和奖励结构的简洁性，并且在多种环境中都能达到收敛。&lt;h4&gt;结论&lt;/h4&gt;实验表明所学技能可以转移到现实世界中表现得既稳健又细腻。相比于现有方法，在这些任务上的性能更优。&lt;h4&gt;翻译&lt;/h4&gt;长时序接触密集型任务由于高维状态空间和稀疏奖励的存在，对强化学习来说是一个挑战。传统的解决方案往往陷入局部最优并且需要针对特定任务进行复杂的奖励调参。为了解决这些问题，我们提出了一种使用特权动作与课程学习相结合的方法框架。该方法通过模拟中的特权训练提升了对象交互和探索效率，并且逐步移除这些特权以适应真实环境的约束条件。最终结果表明所提出的算法能够成功完成多种复杂任务，并在不同环境中展现出多样性和鲁棒性的行为模式，证明了其有效性和普遍性。此外，现实世界实验进一步验证了学到技能的有效转移能力以及在实际应用中的卓越性能表现。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Long-horizon contact-rich tasks are challenging to learn with reinforcementlearning, due to ineffective exploration of high-dimensional state spaces withsparse rewards. The learning process often gets stuck in local optimum anddemands task-specific reward fine-tuning for complex scenarios. In this work,we propose a structured framework that leverages privileged actions withcurriculum learning, enabling the policy to efficiently acquire long-horizonskills without relying on extensive reward engineering or referencetrajectories. Specifically, we use privileged actions in simulation with ageneral training procedure that would be infeasible to implement in real-worldscenarios. These privileges include relaxed constraints and virtual forces thatenhance interaction and exploration with objects. Our results successfullyachieve complex multi-stage long-horizon tasks that naturally combinenon-prehensile manipulation with grasping to lift objects from non-graspableposes. We demonstrate generality by maintaining a parsimonious reward structureand showing convergence to diverse and robust behaviors across variousenvironments. Additionally, real-world experiments further confirm that theskills acquired using our approach are transferable to real-world environments,exhibiting robust and intricate performance. Our approach outperformsstate-of-the-art methods in these tasks, converging to solutions where othersfail.</description>
      <author>example@mail.com (Xiaofeng Mao, Yucheng Xu, Zhaole Sun, Elle Miller, Daniel Layeghi, Michael Mistry)</author>
      <guid isPermaLink="false">2502.15442v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Self-Mixing Laser Interferometry for Robotic Tactile Sensing</title>
      <link>http://arxiv.org/abs/2502.15390v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted for ICRA2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文介绍了一种利用自混合干涉仪（SMI）技术的机器人指尖，用于检测物体滑动和外部接触。研究通过实验验证了该设计的有效性，并将其与声学传感器进行了比较。&lt;h4&gt;背景&lt;/h4&gt;自混合干涉仪因其在无需物理接触的情况下探测微振动的高度敏感性而受到赞誉。在机器人领域中，微振动通常被视为物体滑动的标志，最近也被认为是外接触的重要指标。&lt;h4&gt;目的&lt;/h4&gt;展示首个采用SMI技术检测滑动和外部接触信号的机器人指尖，并比较其与声学传感器的效果。&lt;h4&gt;方法&lt;/h4&gt;通过测量控制下的振动源进行设计验证，包括封装读取电路前后的情况。然后进行了三个实验将SMI指尖与声学传感相比较。&lt;h4&gt;主要发现&lt;/h4&gt;SMI对细微滑动事件更加敏感且在背景噪声下表现出显著更高的鲁棒性&lt;h4&gt;结论&lt;/h4&gt;将自混合干涉仪集成到机器人指尖中为触觉感应提供了新的有前景的分支技术&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Self-mixing interferometry (SMI) has been lauded for its sensitivity indetecting microvibrations, while requiring no physical contact with its target.In robotics, microvibrations have traditionally been interpreted as a markerfor object slip, and recently as a salient indicator of extrinsic contact. Wepresent the first-ever robotic fingertip making use of SMI for slip andextrinsic contact sensing. The design is validated through measurement ofcontrolled vibration sources, both before and after encasing the readoutcircuit in its fingertip package. Then, the SMI fingertip is compared toacoustic sensing through three experiments. The results are distilled into atechnology decision map. SMI was found to be more sensitive to subtle slipevents and significantly more robust against ambient noise. We conclude thatthe integration of SMI in robotic fingertips offers a new, promising branch oftactile sensing in robotics.</description>
      <author>example@mail.com (Remko Proesmans, Ward Goossens, Lowiek Van den Stockt, Lowie Christiaen, Francis wyffels)</author>
      <guid isPermaLink="false">2502.15390v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Rapid Online Learning of Hip Exoskeleton Assistance Preferences</title>
      <link>http://arxiv.org/abs/2502.15366v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Copyright 2025 IEEE. Personal use of this material is permitted.  Permission from IEEE must be obtained for all other uses, in any current or  future media, including reprinting/republishing this material for advertising  or promotional purposes, creating new collective works, for resale or  redistribution to servers or lists, or reuse of any copyrighted component of  this work in other works&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;髋部外骨骼装置在各种场景中表现出色，能够适应不同用户的需求。然而，个性化调整通常需要复杂的调参过程和计算密集型算法，并且大多数现有方法不考虑用户的反馈。&lt;h4&gt;背景&lt;/h4&gt;随着技术的发展，髋部外骨骼因其适应性广、适用性强而越来越受欢迎。但是，在个性化提供帮助方面仍存在挑战，如长时间的调整过程以及缺乏用户反馈整合机制。&lt;h4&gt;目的&lt;/h4&gt;提出一种基于快速学习用户偏好来调整个体化辅助扭矩配置的方法。&lt;h4&gt;方法&lt;/h4&gt;通过随机生成不同助行方案进行成对比较，并主动向参与者提问以收集其偏好的信息。这些反馈被集成到一个优先级学习算法中，该算法根据个人行为动态更新奖励函数并相应调整外骨骼的助力模式。&lt;h4&gt;主要发现&lt;/h4&gt;来自八位健康受试者的实验数据显示了不同的最佳扭矩配置；用户的选择在面对微调后的方案时依然保持一致；用户偏好与个体步行策略有密切联系；助行力矩不会干扰运动学关节协同作用，且参与者倾向于选择与其步态模式同步的助力。&lt;h4&gt;结论&lt;/h4&gt;这一方法简单有效地实现了快速学习用户的偏好和奖励机制，为基于奖励的人机交互奠定了基础。&lt;h4&gt;翻译&lt;/h4&gt;髋部外骨骼由于其在各种场景中的有效性以及能够适应不同用户的能力而日益流行。然而，个性化调整通常需要长时间的调优过程和计算密集型算法，并且大多数现有方法没有整合用户反馈。本文提出了一种快速学习用户对髋部外骨骼辅助偏好并据此优化助动力矩配置的新方法。通过随机生成不同的助力方案进行成对比较，并收集参与者的选择偏好的方式，研究发现不同受试者拥有各自的最优扭矩模式；用户的偏好与他们的步行策略紧密相关；且被测的力矩不会破坏关节协同运动关系，用户更倾向于那些与其步态一致的辅助力矩。这种方法为未来基于奖励的人机交互的研究提供了坚实的基础。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Hip exoskeletons are increasing in popularity due to their effectivenessacross various scenarios and their ability to adapt to different users.However, personalizing the assistance often requires lengthy tuning proceduresand computationally intensive algorithms, and most existing methods do notincorporate user feedback. In this work, we propose a novel approach forrapidly learning users' preferences for hip exoskeleton assistance. We performpairwise comparisons of distinct randomly generated assistive profiles, andcollect participants preferences through active querying. Users' feedback isintegrated into a preference-learning algorithm that updates its belief, learnsa user-dependent reward function, and changes the assistive torque profilesaccordingly. Results from eight healthy subjects display distinct preferredtorque profiles, and users' choices remain consistent when compared to aperturbed profile. A comprehensive evaluation of users' preferences reveals aclose relationship with individual walking strategies. The tested torqueprofiles do not disrupt kinematic joint synergies, and participants favorassistive torques that are synchronized with their movements, resulting inlower negative power from the device. This straightforward approach enables therapid learning of users preferences and rewards, grounding future studies onreward-based human-exoskeleton interaction.</description>
      <author>example@mail.com (Giulia Ramella, Auke Ijspeert, Mohamed Bouri)</author>
      <guid isPermaLink="false">2502.15366v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Exploring Embodied Multimodal Large Models: Development, Datasets, and Future Directions</title>
      <link>http://arxiv.org/abs/2502.15336v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  81 pages, submitted to a journal for review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要总结&lt;/h4&gt;本文综述了嵌入式多模态大型模型（EMLMs）的发展，包括大语言模型、大视觉模型及其他相关模型，并探讨了这些模型在感知、导航和交互等方面的应用。&lt;h4&gt;背景介绍&lt;/h4&gt;近年来，由于EMLMs具有连接感知、认知和行动的潜力，在复杂现实环境中引起了广泛关注。EMLMs试图解决大规模环境下的多种挑战，如数据多样性与质量等。&lt;h4&gt;目的陈述&lt;/h4&gt;本文旨在详细分析EMLMs的发展历程及其面临的挑战，并探讨未来的方向，强调跨模态感知、推理及动作的重要性以促进更加自主系统的发展。&lt;h4&gt;方法概述&lt;/h4&gt;文章讨论了EMLMs的演化过程，重点关注嵌入式感知、导航、交互和模拟等方面。同时，对训练与评估这些模型所使用的数据集进行了深入分析，并指出了多样化高质量数据对于有效学习的重要意义。&lt;h4&gt;主要发现&lt;/h4&gt;文中指出了当前EMLMs面临的关键挑战，包括规模性问题、泛化能力和实时决策制定等方面的难题。&lt;h4&gt;未来方向&lt;/h4&gt;文章最后概述了未来的研究方向，强调多模态感知、推理和动作的集成是推进自主系统发展的关键。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied multimodal large models (EMLMs) have gained significant attention inrecent years due to their potential to bridge the gap between perception,cognition, and action in complex, real-world environments. This comprehensivereview explores the development of such models, including Large Language Models(LLMs), Large Vision Models (LVMs), and other models, while also examiningother emerging architectures. We discuss the evolution of EMLMs, with a focuson embodied perception, navigation, interaction, and simulation. Furthermore,the review provides a detailed analysis of the datasets used for training andevaluating these models, highlighting the importance of diverse, high-qualitydata for effective learning. The paper also identifies key challenges faced byEMLMs, including issues of scalability, generalization, and real-timedecision-making. Finally, we outline future directions, emphasizing theintegration of multimodal sensing, reasoning, and action to advance thedevelopment of increasingly autonomous systems. By providing an in-depthanalysis of state-of-the-art methods and identifying critical gaps, this paperaims to inspire future advancements in EMLMs and their applications acrossdiverse domains.</description>
      <author>example@mail.com (Shoubin Chen, Zehao Wu, Kai Zhang, Chunyu Li, Baiyang Zhang, Fei Ma, Fei Richard Yu, Qingquan Li)</author>
      <guid isPermaLink="false">2502.15336v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Real-Time Moving Flock Detection in Pedestrian Trajectories Using Sequential Deep Learning Models</title>
      <link>http://arxiv.org/abs/2502.15252v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;理解集体行人运动对于人群管理、自主导航和人机交互至关重要。本文探讨了使用序列深度学习模型，包括循环神经网络（RNN）、长短期记忆（LSTM）网络和变压器，用于多行人轨迹中的实时群体检测。&lt;h4&gt;背景&lt;/h4&gt;理解和预测人群的行为对于许多应用如安全、交通规划以及机器人与人类的互动非常重要。&lt;h4&gt;目的&lt;/h4&gt;调查并开发基于序列深度学习模型的方法来识别多个人行进动轨迹中形成的集体运动模式（例如鸟群）。&lt;h4&gt;方法&lt;/h4&gt;{'两阶段过程': '首先，使用预训练的二元分类模型进行成对行人轨迹分类；其次，利用学到的表示动态地确定多代理群体。', '使用的模型': ['循环神经网络（RNN）', '长短期记忆网络（LSTM）', '变压器']}&lt;h4&gt;主要发现&lt;/h4&gt;{'实验结果': '所提出的方法在真实世界的群体运动数据集上进行了验证，显示了其在不同序列长度和多样化移动模式下的鲁棒性。', '准确性与稳定性': '模型可以高精度且稳定地检测行人群体，即使是在动态和嘈杂的环境中也能表现出色。', '进一步应用': '该方法被扩展以识别其他形式的集体运动，如车队和蜂群，为更全面的多代理行为分析铺平了道路。'}&lt;h4&gt;结论&lt;/h4&gt;提出的基于序列深度学习的方法在检测行人群体及其动态变化方面展示了优异的表现，并为进一步研究提供了基础。&lt;h4&gt;翻译&lt;/h4&gt;理解集体行人的移动模式对于人群管理、自主导航和人机交互至关重要。这项工作探讨了使用包括循环神经网络（RNN）、长短期记忆（LSTM）网络和变压器在内的序列深度学习模型，来实现实时群体检测在多行人轨迹中的应用。提出的方法包含两个阶段：首先利用预训练的二元分类模型对成对的人行进动轨迹进行分类；然后使用学到的表示动态地识别多代理群体。通过真实世界人群运动数据集验证了方法的有效性，证明其具有跨变序列长度和多样移动模式的稳健性。实验结果显示，该模型能在高精度和稳定性下检测行人群体，即使在动态且嘈杂的情况下也能保持良好的表现。此外，还扩展到识别其他形式的集体运动（如车队、蜂群），为多代理行为分析铺平了道路。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding collective pedestrian movement is crucial for applications incrowd management, autonomous navigation, and human-robot interaction. Thispaper investigates the use of sequential deep learning models, includingRecurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, andTransformers, for real-time flock detection in multi-pedestrian trajectories.Our proposed approach consists of a two-stage process: first, a pre-trainedbinary classification model is used for pairwise trajectory classification, andsecond, the learned representations are applied to identify multi-agent flocksdynamically.  We validate our method using real-world group movement datasets,demonstrating its robustness across varying sequence lengths and diversemovement patterns. Experimental results indicate that our model consistentlydetects pedestrian flocks with high accuracy and stability, even in dynamic andnoisy environments. Furthermore, we extend our approach to identify other formsof collective motion, such as convoys and swarms, paving the way for morecomprehensive multi-agent behavior analysis.</description>
      <author>example@mail.com (Amartaivan Sanjjamts, Hiroshi Morita, Togootogtokh Enkhtogtokh)</author>
      <guid isPermaLink="false">2502.15252v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>OccProphet: Pushing Efficiency Frontier of Camera-Only 4D Occupancy Forecasting with Observer-Forecaster-Refiner Framework</title>
      <link>http://arxiv.org/abs/2502.15180v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Accepted by ICLR2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;提出了一种新颖的框架OccProphet，用于有效和高效地学习占用预测，显著降低计算需求并提高预测精度。&lt;h4&gt;背景&lt;/h4&gt;在复杂的交通环境中预测变化对于自动驾驶的安全性至关重要。最近的进步使通过观察历史2D图像来预测驾驶环境中的未来3D占用状态成为可能。然而，高计算需求使得占用预测在训练和推理阶段效率较低，限制了其在边缘设备上的可行性。&lt;h4&gt;目的&lt;/h4&gt;提出OccProphet框架以降低占用预测的计算要求，并提高预测精度。&lt;h4&gt;方法&lt;/h4&gt;OccProphet包含三个轻量级组件：观察者、预报器和精炼器。观察者通过提出的Efficient 4D Aggregation with Tripling-Attention Fusion从3D多帧体素中提取时空特征，而预报器和精炼器则有条件地预测并细化未来的占用状态。&lt;h4&gt;主要发现&lt;/h4&gt;在nuScenes、Lyft-Level5和nuScenes-Occupancy数据集上的实验结果表明OccProphet训练友好且推理效率高。与最先进的Cam4DOcc相比，OccProphet减少了58%~78%的计算成本，并提高了2.6倍的速度；此外，它还实现了4%~18%相对更高的预测精度。&lt;h4&gt;结论&lt;/h4&gt;OccProphet在保持或提高预测准确性的同时显著降低了计算需求，显示出其部署到边缘设备中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;预测复杂的交通环境中变化对于自动驾驶的安全性至关重要。最近的进步使通过观察历史2D图像来预测驾驶环境中的未来3D占用状态成为可能。然而，高计算需求使得占用预测在训练和推理阶段效率较低，限制了其在边缘设备上的可行性。在这篇论文中，我们提出了一种新颖的框架OccProphet，用于有效且高效地学习占用预测，显著降低计算需求并提高预测精度。OccProphet包含三个轻量级组件：观察者、预报器和精炼器。观察者通过提出的Efficient 4D Aggregation with Tripling-Attention Fusion从3D多帧体素中提取时空特征，而预报器和精炼器则有条件地预测并细化未来的占用状态。实验结果表明OccProphet在nuScenes、Lyft-Level5和nuScenes-Occupancy数据集上训练友好且推理效率高。与最先进的Cam4DOcc相比，OccProphet减少了58%~78%的计算成本，并提高了2.6倍的速度；此外，它还实现了4%~18%相对更高的预测精度。代码和模型可在https://github.com/JLChen-C/OccProphet上公开获取。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Predicting variations in complex traffic environments is crucial for thesafety of autonomous driving. Recent advancements in occupancy forecasting haveenabled forecasting future 3D occupied status in driving environments byobserving historical 2D images. However, high computational demands makeoccupancy forecasting less efficient during training and inference stages,hindering its feasibility for deployment on edge agents. In this paper, wepropose a novel framework, i.e., OccProphet, to efficiently and effectivelylearn occupancy forecasting with significantly lower computational requirementswhile improving forecasting accuracy. OccProphet comprises three lightweightcomponents: Observer, Forecaster, and Refiner. The Observer extractsspatio-temporal features from 3D multi-frame voxels using the proposedEfficient 4D Aggregation with Tripling-Attention Fusion, while the Forecasterand Refiner conditionally predict and refine future occupancy inferences.Experimental results on nuScenes, Lyft-Level5, and nuScenes-Occupancy datasetsdemonstrate that OccProphet is both training- and inference-friendly.OccProphet reduces 58\%$\sim$78\% of the computational cost with a 2.6$\times$speedup compared with the state-of-the-art Cam4DOcc. Moreover, it achieves4\%$\sim$18\% relatively higher forecasting accuracy. Code and models arepublicly available at https://github.com/JLChen-C/OccProphet.</description>
      <author>example@mail.com (Junliang Chen, Huaiyuan Xu, Yi Wang, Lap-Pui Chau)</author>
      <guid isPermaLink="false">2502.15180v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>CurricuVLM: Towards Safe Autonomous Driving via Personalized Safety-Critical Curriculum Learning with Vision-Language Models</title>
      <link>http://arxiv.org/abs/2502.15119v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;确保自动驾驶系统的安全性是当前面临的关键挑战，尤其是在处理罕见但可能造成严重后果的安全临界场景时。尽管现有研究已经探讨了生成用于自主车辆（AV）测试的安全临界场景的方法，但在将这些场景有效地融入策略学习以提升安全性能方面的工作仍相对有限。此外，开发适应自主车辆行为模式和性能瓶颈变化的训练课程也尚未得到充分探索。&lt;h4&gt;背景&lt;/h4&gt;自动驾驶系统中的安全性是目前研究的重点领域之一，特别是如何处理那些虽然罕见但可能导致严重事故的安全临界场景问题。&lt;h4&gt;目的&lt;/h4&gt;提出CurricuVLM框架以解决现有方法在生成安全临界测试场景和适应自主车辆行为模式方面存在的不足。该框架利用视觉语言模型（VLM）来实现个性化课程学习，从而提升自动驾驶系统的整体性能与安全性。&lt;h4&gt;方法&lt;/h4&gt;CurricuVLM通过运用VLM的多模态理解能力分析自动驾驶代理的行为、识别其性能弱点，并动态生成量身定制的训练场景进行课程适应。通过对不安全驾驶情况及其叙述性描述进行全面分析，该框架能够深入推理评估AV的能力并确定关键行为模式。&lt;h4&gt;主要发现&lt;/h4&gt;实验表明，在Waymo Open Motion数据集上，CurricuVLM在常规及安全临界情景中均优于现有的基准方法，特别是在导航成功率、行驶效率和安全性指标方面表现更优。此外，进一步的分析揭示了CurricuVLM作为一种通用方法可以与各种强化学习算法相结合以增强自动驾驶系统。&lt;h4&gt;结论&lt;/h4&gt;CurricuVLM框架通过利用视觉语言模型的独特能力来改善自主驾驶代理的安全性和整体性能，并且它可以被广泛应用于不同的强化学习环境中。此外，该框架的源代码和演示视频可在GitHub页面上获取。&lt;h4&gt;翻译&lt;/h4&gt;原文摘要已经以中文形式呈现，无需再次翻译。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-21&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Ensuring safety in autonomous driving systems remains a critical challenge,particularly in handling rare but potentially catastrophic safety-criticalscenarios. While existing research has explored generating safety-criticalscenarios for autonomous vehicle (AV) testing, there is limited work oneffectively incorporating these scenarios into policy learning to enhancesafety. Furthermore, developing training curricula that adapt to an AV'sevolving behavioral patterns and performance bottlenecks remains largelyunexplored. To address these challenges, we propose CurricuVLM, a novelframework that leverages Vision-Language Models (VLMs) to enable personalizedcurriculum learning for autonomous driving agents. Our approach uniquelyexploits VLMs' multimodal understanding capabilities to analyze agent behavior,identify performance weaknesses, and dynamically generate tailored trainingscenarios for curriculum adaptation. Through comprehensive analysis of unsafedriving situations with narrative descriptions, CurricuVLM performs in-depthreasoning to evaluate the AV's capabilities and identify critical behavioralpatterns. The framework then synthesizes customized training scenariostargeting these identified limitations, enabling effective and personalizedcurriculum learning. Extensive experiments on the Waymo Open Motion Datasetshow that CurricuVLM outperforms state-of-the-art baselines across both regularand safety-critical scenarios, achieving superior performance in terms ofnavigation success, driving efficiency, and safety metrics. Further analysisreveals that CurricuVLM serves as a general approach that can be integratedwith various RL algorithms to enhance autonomous driving systems. The code anddemo video are available at: https://zihaosheng.github.io/CurricuVLM/.</description>
      <author>example@mail.com (Zihao Sheng, Zilin Huang, Yansong Qu, Yue Leng, Sruthi Bhavanam, Sikai Chen)</author>
      <guid isPermaLink="false">2502.15119v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>DDAT: Diffusion Policies Enforcing Dynamically Admissible Robot Trajectories</title>
      <link>http://arxiv.org/abs/2502.15043v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Under review&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;提出了一种利用扩散模型生成机器人动态可接受轨迹的方法，名为DDAT。通过在训练和推理过程中将预测投影到动态可接受流形上，该方法解决了传统扩散模型与机器人动力学方程之间不匹配的问题。&lt;h4&gt;背景&lt;/h4&gt;扩散模型因其多模态生成能力而在图像和视频创建中表现出色，并逐渐被应用于机器人研究以生成机器人运动。然而，由于其随机性质，它难以满足描述可行机器人运动的精确动态方程。&lt;h4&gt;目的&lt;/h4&gt;解决利用扩散模型生成符合动力学约束的机器人轨迹的问题，提高长时域规划性能。&lt;h4&gt;方法&lt;/h4&gt;DDAT通过迭代采样预测状态前一个状态的可达集多面体下近似，并将预测状态投影到该集合中来确保动态可接受性。这种方法减少了扩散模型需要不断重新计划的需求，从而能够进行一次性长时间范围内的轨迹规划。&lt;h4&gt;主要发现&lt;/h4&gt;提出的框架在四旋翼飞行器和各种MuJoCo环境的广泛模拟以及Unitree GO1和GO2的真实世界实验中生成了更高品质的动态可接受机器人轨迹。&lt;h4&gt;结论&lt;/h4&gt;DDAT通过在扩散模型预测过程中引入动力学可行性约束，有效地解决了利用这种随机生成方法进行精确机器人运动规划的挑战。&lt;h4&gt;翻译&lt;/h4&gt;摘要：扩散模型因其多模态生成能力而擅长创建图像和视频，在机器人研究中也越来越流行，用于生成机器人运动。然而，扩散模型的本质随机性与描述可行机器人运动的动力学方程不一致。因此，利用扩散模型生成动态可接受的机器人轨迹是一个挑战。为解决这一问题，我们引入了DDAT：适用于动态可接受轨迹的扩散策略，以使用扩散模型对黑盒机器人系统进行能够被证明是可接受的轨迹生成。如果序列中的每个状态都属于其前驱者按照机器人运动方程计算出的可达集合，则称该序列是一条动力学上可接受的轨迹。为了生成这样的轨迹，我们的扩散策略在训练和推理过程中将预测投影到动态可接受流形上来使去噪神经网络的目标与动力学可行性约束对齐。这些预测的自回归性质以及机器人动力学的黑盒特性使得这种投影非常具有挑战性。因此，我们通过迭代采样状态可达集的一个多面体下近似，并将其预测的后继投影到该集合上，来强制执行可接受性；随后将此过程重复应用于经过投影后的后续状态。这种方法生成了准确轨迹，从而消除了扩散模型不断重新计划的需求，使得一次性长时域规划成为可能。我们通过广泛的四旋翼飞行器模拟和各种MuJoCo环境中的实验以及Unitree GO1和GO2的真实世界测试来证明我们的框架能够生成更高品质的动态可接受机器人轨迹。&lt;h4&gt;关键词&lt;/h4&gt;扩散模型, 动态可行性, 机器人运动规划&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Diffusion models excel at creating images and videos thanks to theirmultimodal generative capabilities. These same capabilities have made diffusionmodels increasingly popular in robotics research, where they are used forgenerating robot motion. However, the stochastic nature of diffusion models isfundamentally at odds with the precise dynamical equations describing thefeasible motion of robots. Hence, generating dynamically admissible robottrajectories is a challenge for diffusion models. To alleviate this issue, weintroduce DDAT: Diffusion policies for Dynamically Admissible Trajectories togenerate provably admissible trajectories of black-box robotic systems usingdiffusion models. A sequence of states is a dynamically admissible trajectoryif each state of the sequence belongs to the reachable set of its predecessorby the robot's equations of motion. To generate such trajectories, ourdiffusion policies project their predictions onto a dynamically admissiblemanifold during both training and inference to align the objective of thedenoiser neural network with the dynamical admissibility constraint. Theauto-regressive nature of these projections along with the black-box nature ofrobot dynamics render these projections immensely challenging. We thus enforceadmissibility by iteratively sampling a polytopic under-approximation of thereachable set of a state onto which we project its predicted successor, beforeiterating this process with the projected successor. By producing accuratetrajectories, this projection eliminates the need for diffusion models tocontinually replan, enabling one-shot long-horizon trajectory planning. Wedemonstrate that our framework generates higher quality dynamically admissiblerobot trajectories through extensive simulations on a quadcopter and variousMuJoCo environments, along with real-world experiments on a Unitree GO1 andGO2.</description>
      <author>example@mail.com (Jean-Baptiste Bouvier, Kanghyun Ryu, Kartik Nagpal, Qiayuan Liao, Koushil Sreenath, Negar Mehr)</author>
      <guid isPermaLink="false">2502.15043v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>DEFT: Differentiable Branched Discrete Elastic Rods for Modeling Furcated DLOs in Real-Time</title>
      <link>http://arxiv.org/abs/2502.15037v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;该论文提出了一种新的框架DEFT，用于实时建模复杂的分支柔性线性物体（BDLOs），解决了机器人自动化电线束装配中的关键挑战。&lt;h4&gt;背景&lt;/h4&gt;现有的研究已经成功地对单一线性的可变形物体进行了建模，但对于具有复杂力交互和应变传播模式的分支结构来说，这些方法难以直接适用。&lt;h4&gt;目的&lt;/h4&gt;提出一种能够准确预测分支柔性线性物体动态行为的方法，并实现高效的实时计算以及规划能力。&lt;h4&gt;方法&lt;/h4&gt;DEFT结合了基于物理的模型与机器学习框架，用于建模BDLO的动力学特性、动态传播和抓取操作。&lt;h4&gt;主要发现&lt;/h4&gt;通过一系列现实世界的实验展示了DEFT在准确性、计算速度和泛化性方面的优越性能。&lt;h4&gt;结论&lt;/h4&gt;DEFT为复杂柔性线性物体的自动化装配提供了强大的工具，并且展示了其在实际应用中的潜力。&lt;h4&gt;翻译&lt;/h4&gt;自主电线束组装要求机器人能够高精度地操作复杂的分支电缆。现有的研究虽然对单一线性的可变形对象建模取得了一定进展，但对于具有复杂力交互和应变传播模式的分支结构来说，这些方法难以直接适用。为了解决这一挑战，本文提出了一种新的框架DEFT，该框架结合了基于物理模型的方法与机器学习技术，能够准确地模拟BDLO的动力学特性，并实现了高效的实时计算及规划能力，在一系列现实世界的实验中证明了其优越性能。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Autonomous wire harness assembly requires robots to manipulate complexbranched cables with high precision and reliability. A key challenge inautomating this process is predicting how these flexible and branchedstructures behave under manipulation. Without accurate predictions, it isdifficult for robots to reliably plan or execute assembly operations. Whileexisting research has made progress in modeling single-threaded DeformableLinear Objects (DLOs), extending these approaches to Branched Deformable LinearObjects (BDLOs) presents fundamental challenges. The junction points in BDLOscreate complex force interactions and strain propagation patterns that cannotbe adequately captured by simply connecting multiple single-DLO models. Toaddress these challenges, this paper presents Differentiable discrete branchedElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel frameworkthat combines a differentiable physics-based model with a learning frameworkto: 1) accurately model BDLO dynamics, including dynamic propagation atjunction points and grasping in the middle of a BDLO, 2) achieve efficientcomputation for real-time inference, and 3) enable planning to demonstratedexterous BDLO manipulation. A comprehensive series of real-world experimentsdemonstrates DEFT's efficacy in terms of accuracy, computational speed, andgeneralizability compared to state-of-the-art alternatives. Projectpage:https://roahmlab.github.io/DEFT/.</description>
      <author>example@mail.com (Yizhou Chen, Xiaoyue Wu, Yeheng Zong, Anran Li, Yuzhen Chen, Julie Wu, Bohao Zhang, Ram Vasudevan)</author>
      <guid isPermaLink="false">2502.15037v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Safe Beyond the Horizon: Efficient Sampling-based MPC with Neural Control Barrier Functions</title>
      <link>http://arxiv.org/abs/2502.15006v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要分点总结&lt;/h4&gt;{'问题描述': '在实践中使用模型预测控制（MPC）时，满足超出预测范围的安全规范是一个常见问题。', '现有方法的局限性': '理论研究表明可以通过施加合适的终端集约束或足够长的预测范围来保证安全性。然而这些技术难以应用且很少被实际操作者采用，特别是在处理一般非线性动态系统的情况下。', '提出的解决方案': '提出了一种新的方法，通过学习一个近似的离散时间控制屏障函数，并将其融入到变分推理MPC（VIMPC）中来解决上述问题。这种方法在精确递归可行性、计算可行性和适用于‘黑盒’动力学之间做出权衡。', '改进措施': '提出了一种新的采样策略，该策略显著减少了估计的最优控制方差，并提高了采样的效率，从而可以在CPU上实现实时规划。', '性能验证': 'Neural Shield-VIMPC（NS-VIMPC）控制器在模拟和实际硬件实验中均显示出比现有基于采样的MPC控制器更高的安全性改进。特别是在成本函数设计不佳的情况下也能获得显著的安全性提升。', '技术实现': '通过学习近似离散时间控制屏障函数，并将其集成到变分推理MPC（VIMPC）框架中的方式来解决上述问题，同时引入了一种新的采样策略以优化状态约束处理。'}&lt;h4&gt;翻译&lt;/h4&gt;摘要描述了在实际应用中使用模型预测控制时遇到的安全性保障难题以及现有的理论方法难以实施的问题。研究提出通过结合变分推理MPC与近似离散时间控制屏障函数的学习来解决这一挑战，同时引入了新的采样策略以提高计算效率和实时规划能力。实验表明，这种新方法在实际应用中表现出色，尤其是在处理复杂动态系统时有显著的安全性提升。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; A common problem when using model predictive control (MPC) in practice is thesatisfaction of safety specifications beyond the prediction horizon. Whiletheoretical works have shown that safety can be guaranteed by enforcing asuitable terminal set constraint or a sufficiently long prediction horizon,these techniques are difficult to apply and thus are rarely used bypractitioners, especially in the case of general nonlinear dynamics. To solvethis problem, we impose a tradeoff between exact recursive feasibility,computational tractability, and applicability to ''black-box'' dynamics bylearning an approximate discrete-time control barrier function andincorporating it into a variational inference MPC (VIMPC), a sampling-based MPCparadigm. To handle the resulting state constraints, we further propose a newsampling strategy that greatly reduces the variance of the estimated optimalcontrol, improving the sample efficiency, and enabling real-time planning on aCPU. The resulting Neural Shield-VIMPC (NS-VIMPC) controller yields substantialsafety improvements compared to existing sampling-based MPC controllers, evenunder badly designed cost functions. We validate our approach in bothsimulation and real-world hardware experiments.</description>
      <author>example@mail.com (Ji Yin, Oswin So, Eric Yang Yu, Chuchu Fan, Panagiotis Tsiotras)</author>
      <guid isPermaLink="false">2502.15006v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Ultra-High-Frequency Harmony: mmWave Radar and Event Camera Orchestrate Accurate Drone Landing</title>
      <link>http://arxiv.org/abs/2502.14992v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  This paper is accepted by ACM SenSys 2025&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;为了实现精确、高效和安全的无人机着陆，地面平台需要实时且准确地定位下降中的无人机，并引导它们到达指定位置。虽然毫米波（mmWave）感应与相机结合可以提高定位精度，但传统帧照相机较低的采样频率相比毫米波雷达形成了系统吞吐量瓶颈。本文通过在地面平台设置中用新型事件摄像机取代传统的帧照相机来解决这一问题，并引入了针对无人机着陆设计的高度精确且低延迟的地面对准系统mmE-Loc。&lt;h4&gt;背景&lt;/h4&gt;传统的方法结合毫米波雷达和帧照相机构建定位系统，但其较低的采样频率限制了系统的吞吐量。&lt;h4&gt;目的&lt;/h4&gt;提出一种新的高精度、低延迟地面定位系统（mmE-Loc）用于无人机着陆，以改善现有的瓶颈问题，并提高整体性能。&lt;h4&gt;方法&lt;/h4&gt;将事件摄像机与毫米波雷达结合使用，在这种设置中，采样频率得到了统一。为了充分利用这两种模式之间的时间一致性以及空间互补性，提出了两个创新模块：时间一致性指导的协作跟踪和基于图信息自适应联合优化。&lt;h4&gt;主要发现&lt;/h4&gt;通过广泛的实地实验表明，mmE-Loc在定位精度和延迟方面优于现有方法。&lt;h4&gt;结论&lt;/h4&gt;所提出的mmE-Loc系统能够提供更精确、低延迟的无人机着陆支持，并且在实际应用中表现出色。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; For precise, efficient, and safe drone landings, ground platforms shouldreal-time, accurately locate descending drones and guide them to designatedspots. While mmWave sensing combined with cameras improves localizationaccuracy, the lower sampling frequency of traditional frame cameras compared tommWave radar creates bottlenecks in system throughput. In this work, we replacethe traditional frame camera with event camera, a novel sensor that harmonizesin sampling frequency with mmWave radar within the ground platform setup, andintroduce mmE-Loc, a high-precision, low-latency ground localization systemdesigned for drone landings. To fully leverage the \textit{temporalconsistency} and \textit{spatial complementarity} between these modalities, wepropose two innovative modules, \textit{consistency-instructed collaborativetracking} and \textit{graph-informed adaptive joint optimization}, for accuratedrone measurement extraction and efficient sensor fusion. Extensive real-worldexperiments in landing scenarios from a leading drone delivery companydemonstrate that mmE-Loc outperforms state-of-the-art methods in bothlocalization accuracy and latency.</description>
      <author>example@mail.com (Haoyang Wang, Jingao Xu, Xinyu Luo, Xuecheng Chen, Ting Zhang, Ruiyang Duan, Yunhao Liu, Xinlei Chen)</author>
      <guid isPermaLink="false">2502.14992v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>A novel step-by-step procedure for the kinematic calibration of robots using a single draw-wire encoder</title>
      <link>http://arxiv.org/abs/2502.14983v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;摘要&lt;/h4&gt;机器人定位精度在进行高精度制造任务时是一个关键因素。为了有效提高机械臂的精度，校准扮演着至关重要的角色。&lt;h4&gt;背景&lt;/h4&gt;现有的文献中提出了多种机器人校准方法，这些方法使用的测量系统和识别算法差异很大。&lt;h4&gt;目的&lt;/h4&gt;开发一种新型逐步运动学校准程序，仅使用通过拉线编码器获取的一维距离测量数据来逐次估计参数。&lt;h4&gt;方法&lt;/h4&gt;为了实现这一目标，我们推导了一种分析方法，在这种方法中，对于每个未知参数，可以找到一组校准点，其中测得的距离与预测的距离之间的差异只依赖于那个未知参数。这减少了识别过程中的计算负担，并有可能提高其精度。&lt;h4&gt;主要发现&lt;/h4&gt;在模拟和实验测试中，该策略的有效性得到了证实。结果表明，所提出的逐步校准方法为标准校准方法提供了一种实用、成本效益高且计算需求较低的替代方案。&lt;h4&gt;结论&lt;/h4&gt;这种校准方法使得机器人校准更加易于实现，从而提高了机械臂执行任务时的精度和可靠性。&lt;h4&gt;翻译&lt;/h4&gt;机器人定位精度是进行精密制造作业的关键。为提高机械臂精度，提出了一种使用拉线编码器一维距离测量数据逐步校准的新方法，并通过推导分析法验证了其有效性。该方法是一种成本效益高、计算负担小的替代方案，使机器人校准更加简便和经济。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; 10.1007/s00170-024-13219-1&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Robot positioning accuracy is a key factory when performing high-precisionmanufacturing tasks. To effectively improve the accuracy of a manipulator,often up to a value close to its repeatability, calibration plays a crucialrole. In the literature, various approaches to robot calibration have beenproposed, and they range considerably in the type of measurement system andidentification algorithm used. Our aim was to develop a novel step-by-stepkinematic calibration procedure - where the parameters are subsequentlyestimated one at a time - that only uses 1D distance measurement data obtainedthrough a draw-wire encoder. To pursue this objective, we derived an analyticalapproach to find, for each unknown parameter, a set of calibration points wherethe discrepancy between the measured and predicted distances only depends onthat unknown parameter. This reduces the computational burden of theidentification process while potentially improving its accuracy. Simulationsand experimental tests were carried out on a 6 degrees-of-freedom robot arm:the results confirmed the validity of the proposed strategy. As a result, theproposed step-by-step calibration approach represents a practical,cost-effective and computationally less demanding alternative to standardcalibration approaches, making robot calibration more accessible and easier toperform.</description>
      <author>example@mail.com (Giovanni Boschetti, Teresa Sinico)</author>
      <guid isPermaLink="false">2502.14983v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Humanoid-VLA: Towards Universal Humanoid Control with Visual Integration</title>
      <link>http://arxiv.org/abs/2502.14795v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  &lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了一种新的框架Humanoid-VLA，该框架整合了语言理解、自我中心场景感知和运动控制，旨在实现通用的人形机器人控制。&lt;h4&gt;背景&lt;/h4&gt;当前人形机器人的控制系统主要依赖于反应机制，并且由于数据稀缺缺乏自主互动能力。&lt;h4&gt;目的&lt;/h4&gt;开发一种能够解决现有局限性的新方法，即Humanoid-VLA框架，以提高人形机器人在理解和执行任务时的自主性和适应性。&lt;h4&gt;方法&lt;/h4&gt;该研究通过使用非自我中心的人体运动数据集与文本描述进行语言-动作预对齐来开始。然后利用参数高效的视频条件微调技术融入自我中心视觉上下文。此外还提出了一种自监督数据增强策略，可以直接从运动数据中生成伪标注。&lt;h4&gt;主要发现&lt;/h4&gt;Humanoid-VLA框架能够通过利用大规模未标记的视频数据进行训练，提高在物体互动和环境探索任务中的情境意识能力。&lt;h4&gt;结论&lt;/h4&gt;实验结果表明，基于全身控制架构的人形机器人控制系统实现了更像人类的行为表现，具备更强适应性和智能性交互能力。&lt;h4&gt;翻译&lt;/h4&gt;摘要原文为英文，此JSON格式包含对摘要内容的中文总结与分类&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; This paper addresses the limitations of current humanoid robot controlframeworks, which primarily rely on reactive mechanisms and lack autonomousinteraction capabilities due to data scarcity. We propose Humanoid-VLA, a novelframework that integrates language understanding, egocentric scene perception,and motion control, enabling universal humanoid control. Humanoid-VLA beginswith language-motion pre-alignment using non-egocentric human motion datasetspaired with textual descriptions, allowing the model to learn universal motionpatterns and action semantics. We then incorporate egocentric visual contextthrough a parameter efficient video-conditioned fine-tuning, enablingcontext-aware motion generation. Furthermore, we introduce a self-superviseddata augmentation strategy that automatically generates pseudoannotationsdirectly derived from motion data. This process converts raw motion sequencesinto informative question-answer pairs, facilitating the effective use oflarge-scale unlabeled video data. Built upon whole-body control architectures,extensive experiments show that Humanoid-VLA achieves object interaction andenvironment exploration tasks with enhanced contextual awareness, demonstratinga more human-like capacity for adaptive and intelligent engagement.</description>
      <author>example@mail.com (Pengxiang Ding, Jianfei Ma, Xinyang Tong, Binghong Zou, Xinxin Luo, Yiguo Fan, Ting Wang, Hongchao Lu, Panzhong Mo, Jinxin Liu, Yuefan Wang, Huaicheng Zhou, Wenshuo Feng, Jiacheng Liu, Siteng Huang, Donglin Wang)</author>
      <guid isPermaLink="false">2502.14795v2</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    <item>
      <title>Design of a Visual Pose Estimation Algorithm for Moon Landing</title>
      <link>http://arxiv.org/abs/2502.14942v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  6 pages, 8 figures, Presented in 11th Nano-Satellite Symposium&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;为了实现月球精确着陆，提出了一种基于地形的绝对导航算法来估计航天器的位置和姿态。&lt;h4&gt;背景&lt;/h4&gt;在月球着陆任务中，需要提高航天器导航系统的准确性以减少惯性传感器引起的漂移误差。&lt;h4&gt;目的&lt;/h4&gt;通过使用预定的陨石坑数据库对拍摄到的陨石坑进行识别与匹配，并利用这些信息来校正导航偏差，从而实现精确导航。&lt;h4&gt;方法&lt;/h4&gt;该算法采用基于影像处理和地面特征识别的技术，但为了专注于估计算法的研究，在实验中跳过了图像处理及陨石坑匹配步骤。进行了仿真实验以评估算法的准确性以及使用不同数量的陨石坑进行估算的效果。&lt;h4&gt;主要发现&lt;/h4&gt;通过仿真验证了提出的绝对导航方法的有效性和准确性，并探讨了用于估计所需的陨石坑数量的影响。&lt;h4&gt;结论&lt;/h4&gt;该研究提供了一种有效的地形绝对导航解决方案，可以显著提高月球着陆任务中航天器导航的精度。&lt;h4&gt;翻译&lt;/h4&gt;为了实现月球精确着陆，需要校正惯性传感器引起的导航漂移。本研究提出了一种基于地面特征识别技术的绝对导航方法，并通过仿真验证了其有效性及准确性。&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-02-20&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In order to make a pinpoint landing on the Moon, the spacecraft's navigationsystem must be accurate. To achieve the desired accuracy, navigational driftcaused by the inertial sensors must be corrected. One way to correct this driftis to use absolute navigation solutions. In this study, a terrain absolutenavigation method to estimate the spacecraft's position and attitude isproposed. This algorithm uses the position of the craters below the spacecraftfor estimation. Craters seen by the camera onboard the spacecraft are detectedand identified using a crater database known beforehand. In order to focus onestimation algorithms, image processing and crater matching steps are skipped.The accuracy of the algorithm and the effect of the crater number used forestimation are inspected by performing simulations.</description>
      <author>example@mail.com (Atakan Süslü, Betül Rana Kuran, Halil Ersin Söken)</author>
      <guid isPermaLink="false">2502.14942v1</guid>
      <pubDate>Mon, 24 Feb 2025 17:04:44 +0800</pubDate>
    </item>
    </channel>
</rss>