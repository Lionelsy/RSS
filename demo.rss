<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 29 Jul 2025 12:10:59 +0800</lastBuildDate>
    <item>
      <title>Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</title>
      <link>http://arxiv.org/abs/2507.04047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Embodied AI; 3D Vision Language Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本文提出了Move to Understand (MTU3D)框架，整合主动感知与3D视觉语言学习，使具身智能体能有效探索和理解环境。&lt;h4&gt;背景&lt;/h4&gt;现有3D视觉语言模型主要关注静态观察中的物体定位，缺乏主动感知和探索环境的能力。&lt;h4&gt;目的&lt;/h4&gt;开发一个统一框架，将主动感知与3D视觉语言学习相结合，使智能体能够有效探索和理解环境。&lt;h4&gt;方法&lt;/h4&gt;通过三个关键创新实现：1) 在线查询表示学习，直接从RGB-D帧构建空间记忆；2) 统一的定位和探索目标，联合优化物体定位和前沿选择；3) 结合百万多样本轨迹的端到端轨迹学习。&lt;h4&gt;主要发现&lt;/h4&gt;在各项基准测试中，MTU3D的成功率显著高于现有方法，包括HM3D-OVON高出14%、GOAT-Bench高出23%、SG3D高出9%、A-EQA高出2%。&lt;h4&gt;结论&lt;/h4&gt;视觉定位和探索的结合对具身智能至关重要，MTU3D支持多种输入模态的导航任务。&lt;h4&gt;翻译&lt;/h4&gt;具身场景理解不仅需要理解已观察到的视觉空间信息，还需要确定在三维物理世界中下一步探索的位置。现有的三维视觉语言模型主要关注三维重建中静态观察的物体定位，如网格和点云，但缺乏主动感知和探索环境的能力。为解决这一局限，我们引入了Move to Understand (MTU3D)框架，将主动感知与三维视觉语言学习相结合，使具身智能体能有效探索和理解环境。这通过三个关键创新实现：1) 在线查询表示学习，直接从RGB-D帧构建空间记忆，无需显式三维重建；2) 统一的定位和探索目标，将未探索位置表示为前沿查询，联合优化物体定位和前沿选择；3) 结合来自模拟和真实世界RGB-D序列的百万多样本轨迹的视觉-语言-探索预训练的端到端轨迹学习。在各种具身导航和问答基准测试中的广泛评估表明，MTU3D在HM3D-OVON、GOAT-Bench、SG3D和A-EQA的成功率上分别比最先进的强化学习和模块化导航方法高出14%、23%、9%和2%。MTU3D的通用性使其能够利用多种输入模态进行导航，包括类别、语言描述和参考图像。这些发现强调了视觉定位和探索结合对具身智能的重要性。&lt;h4&gt;深入解读&lt;/h4&gt;# 论文分析：《Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation》## 1. 这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？这篇论文主要解决的问题是：现有3D视觉语言(3D-VL)模型只能处理静态的3D重建数据(如网格和点云)中的物体定位，缺乏主动感知和探索环境的能力；而强化学习(RL)为基础的具身智能体虽然可以探索环境，但存在样本效率低、泛化能力差和缺乏明确空间表示的问题。这个问题在现实中非常重要，因为真实的具身智能体(如机器人)需要在部分可观测和动态的环境中工作，它们不仅需要理解已观察到的视觉信息，还需要主动决定下一步探索哪里。就像人类进入陌生房间寻找食物时，会本能地探索厨房、检查台面、查看冰箱等，这种结合常识知识、空间推理和视觉定位的能力是当前AI系统所缺乏的。## 2. 作者是如何思考并设计出这个方法的？是否有借鉴现有工作？作者首先分析了现有3D-VL模型和RL方法的局限性，前者缺乏主动探索能力，后者存在样本效率低和泛化能力差的问题。基于这些分析，作者设计了一个统一框架，借鉴了多个现有工作的优点：1. 参考3DVLP、PQ3D和LEO等模型处理多任务的方法2. 借鉴EmbodiedSAM的流式RGB-D视频输入方法，但增加了主动探索能力3. 改进了PQ3D的查询机制，设计为在线查询表示学习4. 参考了空间记忆银行的概念，但实现了动态更新5. 借鉴了基于frontier的探索方法，但将其与视觉定位结合作者的设计思路是创建一个统一的框架，将视觉定位和探索整合在一起，使智能体能够像人类一样在理解已观察物体信息的同时，主动决定下一步探索哪里。## 3. 这个方法的核心思想是什么？整体实现流程是怎样的？核心思想：将视觉定位(visual grounding)和主动探索(exploration)统一到一个框架中，使具身智能体能够有效地探索和理解其环境。这种方法让智能体能够像人类一样，在理解已观察物体信息的同时，主动决定下一步探索哪里。整体实现流程：1. **在线查询表示学习**：   - 输入部分RGB-D序列   - 通过2D和3D编码器提取特征   - 生成本地查询对象   - 将查询写入全局空间记忆库2. **统一的定位和探索**：   - 接收自然语言目标或图像目标   - 空间推理层处理对象查询和frontier查询   - 生成统一的决策分数   - 选择分数最高的查询(对象或frontier)3. **轨迹规划与执行**：   - 根据选择的查询位置生成轨迹   - 执行导航动作   - 获取新的RGB-D序列   - 更新空间记忆4. **训练过程**：   - 第一阶段：低级感知训练，训练查询表示   - 第二阶段：视觉语言探索预训练，联合训练探索和定位   - 第三阶段：特定任务导航微调## 4. 论文的关键创新点有哪些？相比之前的工作，有什么不同？关键创新点：1. **在线查询表示学习**：直接从RGB-D帧构建空间记忆，无需显式的3D重建，利用2D基础模型(如DINO和SAM)的特征提取和分割先验，捕获丰富的语义和精确的3D空间信息。2. **统一的定位和探索目标**：将未探索区域表示为frontier查询，联合优化物体定位和探索，通过空间推理层同时处理对象查询和frontier查询。3. **端到端的视觉语言探索(VLE)预训练**：使用大规模轨迹数据(超过100万条)，结合模拟和真实世界的RGB-D序列，开发自动轨迹混合策略，结合专家和嘈杂的导航数据。与之前工作的不同：- 与3D-VL模型相比：不依赖静态3D表示，支持在线感知和主动探索，能处理部分可观测和动态环境- 与RL方法相比：具有更好的泛化能力，样本效率更高，具有明确的空间表示- 与模块化方法相比：端到端训练，统一处理多种任务和输入模态，支持终身学习- 与在线3D实例分割方法相比：增加了主动探索能力和高级推理能力，能处理复杂任务序列## 5. 如果要用一句话总结这篇论文的贡献，你会怎么说？"本文提出了MTU3D框架，通过统一视觉定位与主动探索，实现了具身智能体在3D环境中的高效、通用导航，显著提升了在开放词汇导航、多模态终身导航和顺序导航等任务中的性能。"&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied scene understanding requires not only comprehending visual-spatialinformation that has been observed but also determining where to explore nextin the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarilyfocus on grounding objects in static observations from 3D reconstruction, suchas meshes and point clouds, but lack the ability to actively perceive andexplore their environment. To address this limitation, we introduce\underline{\textbf{M}}ove \underline{\textbf{t}}o\underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework thatintegrates active perception with \underline{\textbf{3D}} vision-languagelearning, enabling embodied agents to effectively explore and understand theirenvironment. This is achieved by three key innovations: 1) Online query-basedrepresentation learning, enabling direct spatial memory construction from RGB-Dframes, eliminating the need for explicit 3D reconstruction. 2) A unifiedobjective for grounding and exploring, which represents unexplored locations asfrontier queries and jointly optimizes object grounding and frontier selection.3) End-to-end trajectory learning that combines\textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over amillion diverse trajectories collected from both simulated and real-world RGB-Dsequences. Extensive evaluations across various embodied navigation andquestion-answering benchmarks show that MTU3D outperforms state-of-the-artreinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%,and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA,respectively. \model's versatility enables navigation using diverse inputmodalities, including categories, language descriptions, and reference images.These findings highlight the importance of bridging visual grounding andexploration for embodied intelligence.</description>
      <author>example@mail.com (Ziyu Zhu, Xilin Wang, Yixuan Li, Zhuofan Zhang, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Wei Liang, Qian Yu, Zhidong Deng, Siyuan Huang, Qing Li)</author>
      <guid isPermaLink="false">2507.04047v1</guid>
      <pubDate>Tue, 29 Jul 2025 12:10:59 +0800</pubDate>
    </item>
  </channel>
</rss>
