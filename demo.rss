<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Tue, 29 Jul 2025 13:28:26 +0800</lastBuildDate>
    <item>
      <title>Move to Understand a 3D Scene: Bridging Visual Grounding and Exploration for Efficient and Versatile Embodied Navigation</title>
      <link>http://arxiv.org/abs/2507.04047v1</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  Embodied AI; 3D Vision Language Understanding&lt;h3&gt;GPT 摘要:&lt;/h3&gt;&lt;h4&gt;总结&lt;/h4&gt;本研究提出了MTU3D（Move to Understand）框架，这是一个将主动感知与3D视觉语言学习相结合的统一框架，使具身智能体能够有效地探索和理解其环境。&lt;h4&gt;背景&lt;/h4&gt;现有的3D视觉语言（3D-VL）模型主要关注基于3D重建（如网格和点云）的静态观察中的物体定位，但缺乏主动感知和探索环境的能力。&lt;h4&gt;目的&lt;/h4&gt;解决现有3D视觉语言模型无法主动感知和探索环境的局限性，使具身智能体能够更好地理解和探索3D物理世界。&lt;h4&gt;方法&lt;/h4&gt;MTU3D框架通过三个关键创新实现：1) 在线查询表示学习，能够直接从RGB-D帧构建空间记忆，无需显式的3D重建；2) 用于定位和探索的统一目标，将未探索的位置表示为前沿查询，并联合优化物体定位和前沿选择；3) 端到端轨迹学习，结合来自模拟和真实世界RGB-D序列的百万条多样化轨迹上的视觉-语言-探索预训练。&lt;h4&gt;主要发现&lt;/h4&gt;在各种具身导航和问答基准上的广泛评估显示，MTU3D在HM3D-OVON、GOAT-Bench、SG3D和A-EQA的成功率上分别比最先进的强化学习和模块化导航方法高出14%、23%、9%和2%。MTU3D的多功能性使其能够使用多种输入模态进行导航，包括类别、语言描述和参考图像。&lt;h4&gt;结论&lt;/h4&gt;这些发现强调了视觉定位和探索结合对于具身智能的重要性。&lt;h4&gt;翻译&lt;/h4&gt;具身场景理解不仅需要理解已经观察到的视觉空间信息，还需要确定在3D物理世界中下一步探索的位置。现有的3D视觉语言（3D-VL）模型主要关注基于3D重建（如网格和点云）的静态观察中的物体定位，但缺乏主动感知和探索环境的能力。为解决这一局限性，我们引入了MTU3D（Move to Understand）框架，这是一个将主动感知与3D视觉语言学习相结合的统一框架，使具身智能体能够有效地探索和理解其环境。这是通过三个关键创新实现的：1) 在线查询表示学习，能够直接从RGB-D帧构建空间记忆，无需显式的3D重建。2) 用于定位和探索的统一目标，将未探索的位置表示为前沿查询，并联合优化物体定位和前沿选择。3) 端到端轨迹学习，结合来自模拟和真实世界RGB-D序列的百万条多样化轨迹上的视觉-语言-探索预训练。在各种具身导航和问答基准上的广泛评估显示，MTU3D在HM3D-OVON、GOAT-Bench、SG3D和A-EQA的成功率上分别比最先进的强化学习和模块化导航方法高出14%、23%、9%和2%。MTU3D的多功能性使其能够使用多种输入模态进行导航，包括类别、语言描述和参考图像。这些发现强调了视觉定位和探索结合对于具身智能的重要性。&lt;h4&gt;深入解读&lt;/h4&gt;&lt;strong&gt;这篇论文主要想解决什么问题？这个问题在现实或研究中为什么重要？:&lt;/strong&gt; 论文主要解决如何让智能体同时具备视觉定位和主动探索3D环境的能力。现有3D视觉语言模型只能处理静态已重建的场景，无法主动探索；而强化学习方法虽能探索但效率低、泛化差。这个问题很重要，因为要让机器人在真实世界有效工作，它们需要像人类一样既能理解已观察环境，又能主动决定下一步去哪里，形成感知-推理-行动的闭环。&lt;br&gt;&lt;strong&gt;作者是如何思考并设计出这个方法的？是否有借鉴现有工作？:&lt;/strong&gt; 作者首先分析了现有方法的局限性：静态3D重建不适合实时环境，强化学习方法样本效率低。借鉴了DINO和SAM的2D特征提取、PQ3D的查询表示、occupancy map的空间探索概念以及CLIP的多模态处理。设计思路是通过三个关键创新整合视觉定位和探索：在线查询表示学习、统一的定位-探索目标、端到端的视觉-语言-探索预训练，形成一个统一的MTU3D框架。&lt;br&gt;&lt;strong&gt;这个方法的核心思想是什么？整体实现流程是怎样的？:&lt;/strong&gt; 核心思想是将视觉定位和主动探索整合在一个统一框架中，让智能体既能理解已观察环境又能决定下一步探索哪里。流程包括：1)在线查询表示学习：处理RGB-D数据生成查询并存储到空间记忆库；2)统一决策：结合语言目标、物体查询和前沿查询选择最佳行动；3)轨迹规划：生成路径并执行导航动作；4)训练过程：分三阶段进行低级感知训练、视觉-语言-探索预训练和任务特定微调。&lt;br&gt;&lt;strong&gt;论文的关键创新点有哪些？相比之前的工作，有什么不同？:&lt;/strong&gt; 三个关键创新：1)在线查询表示学习：直接从RGB-D帧构建空间记忆，无需3D重建；2)统一的定位-探索目标：将未探索区域表示为前沿查询，联合优化两个任务；3)端到端的视觉-语言-探索预训练：结合百万级模拟和真实世界轨迹。不同之处在于：不依赖静态3D重建，而是在线构建动态记忆；将视觉定位和探索整合在统一框架中；使用大规模预训练而非仅靠强化学习；支持多模态输入进行导航。&lt;br&gt;&lt;strong&gt;如果要用一句话总结这篇论文的贡献，你会怎么说？:&lt;/strong&gt; MTU3D通过整合视觉定位与主动探索，并利用大规模视觉-语言-探索预训练，实现了高效、多功能的具身导航，显著提升了智能体在3D环境中的理解和导航能力。&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2025-07-05&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; &lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Embodied scene understanding requires not only comprehending visual-spatialinformation that has been observed but also determining where to explore nextin the 3D physical world. Existing 3D Vision-Language (3D-VL) models primarilyfocus on grounding objects in static observations from 3D reconstruction, suchas meshes and point clouds, but lack the ability to actively perceive andexplore their environment. To address this limitation, we introduce\underline{\textbf{M}}ove \underline{\textbf{t}}o\underline{\textbf{U}}nderstand (\textbf{\model}), a unified framework thatintegrates active perception with \underline{\textbf{3D}} vision-languagelearning, enabling embodied agents to effectively explore and understand theirenvironment. This is achieved by three key innovations: 1) Online query-basedrepresentation learning, enabling direct spatial memory construction from RGB-Dframes, eliminating the need for explicit 3D reconstruction. 2) A unifiedobjective for grounding and exploring, which represents unexplored locations asfrontier queries and jointly optimizes object grounding and frontier selection.3) End-to-end trajectory learning that combines\textbf{V}ision-\textbf{L}anguage-\textbf{E}xploration pre-training over amillion diverse trajectories collected from both simulated and real-world RGB-Dsequences. Extensive evaluations across various embodied navigation andquestion-answering benchmarks show that MTU3D outperforms state-of-the-artreinforcement learning and modular navigation approaches by 14\%, 23\%, 9\%,and 2\% in success rate on HM3D-OVON, GOAT-Bench, SG3D, and A-EQA,respectively. \model's versatility enables navigation using diverse inputmodalities, including categories, language descriptions, and reference images.These findings highlight the importance of bridging visual grounding andexploration for embodied intelligence.</description>
      <author>example@mail.com (Ziyu Zhu, Xilin Wang, Yixuan Li, Zhuofan Zhang, Xiaojian Ma, Yixin Chen, Baoxiong Jia, Wei Liang, Qian Yu, Zhidong Deng, Siyuan Huang, Qing Li)</author>
      <guid isPermaLink="false">2507.04047v1</guid>
      <pubDate>Tue, 29 Jul 2025 13:28:26 +0800</pubDate>
    </item>
  </channel>
</rss>
