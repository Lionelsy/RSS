<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>主题论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>主题论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Wed, 18 Sep 2024 14:43:27 +0000</lastBuildDate>
    <item>
      <title>A Medical Image Segmentation Method Combining Knowledge Distillation and Contrastive Learning</title>
      <link>https://www.aminer.cn/pub/667f957101d2a3fbfc4746c4</link>
      <description>## A Medical Image Segmentation Method Combining Knowledge Distillation and Contrastive Learning

**作者:** Xiaoxuan Ma, Sihan Shan Xiaoxuan Ma, Dong Sui Sihan Shan

****要点****: 本文提出了一种结合知识蒸馏和对比学习的新型医疗图像分割方法，有效提升了学生网络的分割性能，尤其针对医疗图像中的类内变异和类间不平衡问题。

****方法****: 方法通过利用重要性地图和区域亲和图，引导学生网络深入探索教师网络的区域特征表示，同时结合类指导的对比学习，增强学生网络对不同类别特征的判别能力。

****实验****: 实验在结直肠癌肿瘤数据集上进行，学生网络ENet、MobileNetV2和ResNet-18的Dice系数分别提高了4.92%、4.34%和4.59%，与教师网络FANet、PSPNet、SwinUnet和AttentionUnet相比，最佳学生网络表现分别提升了2.45%、5.84%、6.58%和3.56%。

**关键词:** Contrastive Learning

**详细摘要:** Recent advances in feature-based knowledge distillation have shown promise in computer vision, yet their direct application to medical image segmentation has been challenging due to the inherent high intra-class variance and class imbalance prevalent in medical images. This paper introduces a novel approach that synergizes knowledge distillation with contrastive learning to enhance the performance of student networks in medical image segmentation. By leveraging importance maps and region affinity graphs, our method encourages the student network to deeply explore the regional feature representations of the teacher network, capturing essential structural information and detailed features. This process is complemented by class-guided contrastive learning, which sharpens the discriminative capacity of the student network for different class features, specifically addressing intra-class variance and inter-class imbalance. Experimental validation on the colorectal cancer tumor dataset demonstrates notable improvements, with student networks ENet, MobileNetV2, and ResNet-18 achieving Dice coefficient score enhancements of 4.92%, 4.34%, and 4.59%, respectively. When benchmarked against teacher networks FANet, PSPNet, SwinUnet, and AttentionUnet, our best-performing student network exhibited performance boosts of 2.45%, 5.84%, 6.58%, and 3.56%, respectively, underscoring the efficacy of integrating knowledge distillation with contrastive learning for medical image segmentation.

</description>
      <guid isPermaLink="false">667f957101d2a3fbfc4746c4</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Attention-Based Contrastive Learning for Few-Shot Remote Sensing Image Classification</title>
      <link>https://www.aminer.cn/pub/66149e9813fb2c6cf6f58e9e</link>
      <description>## Attention-Based Contrastive Learning for Few-Shot Remote Sensing Image Classification

**期刊/会议:** IEEE transactions on geoscience and remote sensing

**作者:** Yulong Xu, Hanbo Bi, Hongfeng Yu, Wanxuan Lu, Peifeng Li, Xinming Li, Xian Sun

****要点****: 论文提出了一种基于注意力对比学习的少样本遥感图像分类方法，通过优化特征和引入对比损失提高了分类准确性。
**方法**：方法包括一个端到端的度量学习框架，即注意力基于对比学习网络，以及两个模块：注意力基于特征优化和基于字典的对比损失。
**实验**：在五个公开的少样本遥感分类数据集上进行实验，结果表明所提方法具有竞争力，并在五类一拍场景下比其他少样本学习算法具有更高的分类准确性。

**关键词:** Contrastive Learning

**详细摘要:** Few-shot remote sensing image classification entails identifying images using a limited set of labeled data within remote sensing scenes, holding significant theoretical and practical implications. However, owing to the intricacy and variety of remote sensing images, traditional classification methods usually struggle to extract effective features and learn robust classifiers. To address this issue, an end-to-end metric learning framework named Attention-based Contrastive Learning Network is introduced in this paper. Specifically, the Attention-based Feature Optimization (ABFO) module is employed to align and enhance target image features, highlighting the target region and strengthening the network’s feature extraction capability. Additionally, the Dictionary-based Contrastive Loss (DBCL) module is assigned to optimize image feature vectors, improving category distinguishability and consequently enhancing classification accuracy. The experimental results on five publicly available Few-shot remote sensing classification datasets demonstrate the high competitiveness of our proposed method. Furthermore, it illustrates superior classification accuracy compared to other pertinent Few-shot learning algorithms in the 5-way 1-shot scenario.

</description>
      <guid isPermaLink="false">66149e9813fb2c6cf6f58e9e</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Self-Supervised Graph Transformer for Deepfake Detection</title>
      <link>https://www.aminer.cn/pub/666b987d01d2a3fbfcf84537</link>
      <description>## Self-Supervised Graph Transformer for Deepfake Detection

**期刊/会议:** IEEE ACCESS

**作者:** Aminollah Khormali, Jiann-Shiun Yuan

****要点****: 本研究提出了一种基于自监督预训练的图变换器框架，用于深度伪造检测，实现了卓越的跨数据集泛化能力和对常见后期处理扰动的鲁棒性。
**方法**：通过自监督对比学习方法预训练基于视觉变换器架构的特征提取器，结合图卷积网络和变换器判别器，以及图变换器相关性映射来解释模型决策。
**实验**：在多个挑战性实验中评估了所提框架的有效性，包括数据分布内性能、跨数据集和跨操作泛化能力，以及对常见后期制作扰动的鲁棒性，使用的数据集未具体提及，实验结果显示该框架优于现有最佳方法。

**关键词:** Contrastive Learning

**详细摘要:** Deepfake detection methods have shown promising results in recognizing forgeries within a given dataset, where training and testing take place on the in-distribution dataset. However, their performance deteriorates significantly when presented with unseen samples. As a result, a reliable deepfake detection system must remain impartial to forgery types, appearance, and quality for guaranteed generalizable detection performance. Despite various attempts to enhance cross-dataset generalization, the problem remains challenging, particularly when testing against common post-processing perturbations, such as video compression or blur. Hence, this study introduces a deepfake detection framework, leveraging a self-supervised pre-training model that delivers exceptional generalization ability, withstanding common corruptions and enabling feature explainability. The framework comprises three key components: a feature extractor based on vision Transformer architecture that is pre-trained via self-supervised contrastive learning methodology, a graph convolution network coupled with a Transformer discriminator, and a graph Transformer relevancy map that provides a better understanding of manipulated regions and further explains the model's decision. To assess the effectiveness of the proposed framework, several challenging experiments are conducted, including in-data distribution performance, cross-dataset &amp; cross-manipulation generalization, and robustness against common post-production perturbations. The results achieved demonstrate the remarkable effectiveness of the proposed deepfake detection framework, surpassing the current state-of-the-art approaches.

</description>
      <guid isPermaLink="false">666b987d01d2a3fbfcf84537</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation</title>
      <link>https://www.aminer.cn/pub/666b86e001d2a3fbfccd6f66</link>
      <description>## Bilateral Unsymmetrical Graph Contrastive Learning for Recommendation

**期刊/会议:** arXiv (Cornell University)

**作者:** Jiangying Yu, J. Li, Yulin He, Kenny Q. Zhu, Shuyi Zhang, Wen Hu

****要点****: 论文提出了一种名为BusGCL的推荐系统框架，通过考虑用户和项目节点间的不对称性，利用双边切片对比训练改进图对比学习，提高了推荐任务的性能。
**方法**：作者采用了一种新的方法，将用户和项目的节点关系密度差异考虑在内，并利用超图基础的图卷积网络（GCN）以及对比学习技术进行推荐。
**实验**：研究者在两个公开数据集上进行了综合实验，证明了BusGCL相较于其他推荐方法具有优越性，使用的数据集为公共数据集，但未具体提及数据集名称。

**关键词:** Contrastive Learning

**详细摘要:** Recent methods utilize graph contrastive Learning within graph-structured user-item interaction data for collaborative filtering and have demonstrated their efficacy in recommendation tasks. However, they ignore that the difference relation density of nodes between the user- and item-side causes the adaptability of graphs on bilateral nodes to be different after multi-hop graph interaction calculation, which limits existing models to achieve ideal results. To solve this issue, we propose a novel framework for recommendation tasks called Bilateral Unsymmetrical Graph Contrastive Learning (BusGCL) that consider the bilateral unsymmetry on user-item node relation density for sliced user and item graph reasoning better with bilateral slicing contrastive training. Especially, taking into account the aggregation ability of hypergraph-based graph convolutional network (GCN) in digging implicit similarities is more suitable for user nodes, embeddings generated from three different modules: hypergraph-based GCN, GCN and perturbed GCN, are sliced into two subviews by the user- and item-side respectively, and selectively combined into subview pairs bilaterally based on the characteristics of inter-node relation structure. Furthermore, to align the distribution of user and item embeddings after aggregation, a dispersing loss is leveraged to adjust the mutual distance between all embeddings for maintaining learning ability. Comprehensive experiments on two public datasets have proved the superiority of BusGCL in comparison to various recommendation methods. Other models can simply utilize our bilateral slicing contrastive learning to enhance recommending performance without incurring extra expenses.

</description>
      <guid isPermaLink="false">666b86e001d2a3fbfccd6f66</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Study on Point Cloud Based 3D Object Detection for Autonomous Driving</title>
      <link>https://www.aminer.cn/pub/65b84c81939a5f40828b7605</link>
      <description>## Study on Point Cloud Based 3D Object Detection for Autonomous Driving

**期刊/会议:** Han'gug tongsin haghoe nonmunji/Han-guk tongsin hakoe nonmunji

**作者:** Yaw Peng Cheong, Wu Jun, Sung‐Jin Lee

발

****要点****: 本研究针对自动驾驶商业化需求，探讨了基于点云的3D物体检测技术，提出了最大化检测准确度的点云数据增强方法。
**方法**：研究采用Jitter、均匀采样、随机采样、基于缩放的方法对点云数据进行增强，并通过不同方法的组合分析来提高3D物体检测的准确度。
**实验**：使用KITTI数据集进行实验，结果显示数据增强方法使准确度AP提升了约0.5~0.8，其中Jitter方法最为有效，针对不同类别的数据采用不同增强方法能获得更好的结果。

**关键词:** Point Cloud

**详细摘要:** 자율주행 자동차의 상용화를 위해서는 정확한 3차원 공간 기반 상황인지 기술이 필수적이다. 이를 위해서 카메라만으로는 그 인식 성능에 한계가 있어 라이다 기반의 3차원 상황인지 기술 도입이 필수적이며, 이런 라이다 기반의 3D Object Detection 기술의 정확도를 최대화할 수 있는 포인트 클라우드 데이터 증식 방법에 대해 연구하였다. 이 포인트 클라우드 기반 데이터 증식 방법으로 Jitter, Uniform Sampling, Random Sampling, Scaling 기반의 방법을 사용하여 그 정확도를 분석하였으며 이들의 조합 통해 3D Object Detection의 정확도를 최대화 할 수 있는 방법에 대해 탐구하였다. 실험 결과 KITTI dataset 기준으로 정확도 AP가 약 0.5~0.8 정도 향상되는 것을 보였으며, Jitter기법이 성능향상에 가장 효과적이며 클래스마다 다른 데이터 증식을 적용하는 것이 더 좋은 결과를 얻을 수 있다는 것을 알아내었다.

</description>
      <guid isPermaLink="false">65b84c81939a5f40828b7605</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Efficient and Generic Point Model for Lossless Point Cloud Attribute Compression</title>
      <link>https://www.aminer.cn/pub/6652acbd01d2a3fbfc1bc629</link>
      <description>## Efficient and Generic Point Model for Lossless Point Cloud Attribute Compression

**期刊/会议:** arXiv (Cornell University)

**作者:** Kang Soo You, Ping Gao, Zhan Ma

****要点****: 本文提出了一种高效且通用的无损点云属性压缩方法PoLoPCAC，通过推断属性分布的显式模型，实现了高压缩效率和强大的泛化能力。
**方法**：作者将无损点云属性压缩任务定义为从组内自回归先验中推断属性分布，使用渐进随机分组策略和局部感知注意力机制进行属性建模。
**实验**：在Synthetic 2k-ShapeNet数据集上训练后，PoLoPCAC方法在各种数据集上实现了比特率连续降低，且编码时间短于G-PCCv23，模型大小轻量（2.6MB）。数据集名称为Synthetic 2k-ShapeNet、ShapeNet、ScanNet、MVUB、8iVFB。

**关键词:** Point Cloud

**详细摘要:** The past several years have witnessed the emergence of learned point cloud compression (PCC) techniques. However, current learning-based lossless point cloud attribute compression (PCAC) methods either suffer from high computational complexity or deteriorated compression performance. Moreover, the significant variations in point cloud scale and sparsity encountered in real-world applications make developing an all-in-one neural model a challenging task. In this paper, we propose PoLoPCAC, an efficient and generic lossless PCAC method that achieves high compression efficiency and strong generalizability simultaneously. We formulate lossless PCAC as the task of inferring explicit distributions of attributes from group-wise autoregressive priors. A progressive random grouping strategy is first devised to efficiently resolve the point cloud into groups, and then the attributes of each group are modeled sequentially from accumulated antecedents. A locality-aware attention mechanism is utilized to exploit prior knowledge from context windows in parallel. Since our method directly operates on points, it can naturally avoids distortion caused by voxelization, and can be executed on point clouds with arbitrary scale and density. Experiments show that our method can be instantly deployed once trained on a Synthetic 2k-ShapeNet dataset while enjoying continuous bit-rate reduction over the latest G-PCCv23 on various datasets (ShapeNet, ScanNet, MVUB, 8iVFB). Meanwhile, our method reports shorter coding time than G-PCCv23 on the majority of sequences with a lightweight model size (2.6MB), which is highly attractive for practical applications. Dataset, code and trained model are available at https://github.com/I2-Multimedia-Lab/PoLoPCAC.

</description>
      <guid isPermaLink="false">6652acbd01d2a3fbfc1bc629</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Weakly Supervised Point Cloud Semantic Segmentation Based on Scene Consistency</title>
      <link>https://www.aminer.cn/pub/66e5b54001d2a3fbfca1df9e</link>
      <description>## Weakly Supervised Point Cloud Semantic Segmentation Based on Scene Consistency

**期刊/会议:** Applied Intelligence

**作者:** Yingchun Niu, Jianqin Yin, Chao Qi, Liang Geng

****要点****: 本文提出了一种基于场景一致性建模的弱监督点云语义分割方法，通过同时建模完整和不完整场景，增强监督信号质量，提高模型性能并降低资源消耗。

****方法****: 研究采用一种新颖的场景一致性建模方法，通过窗口技术生成不完整场景，并利用网络编码器对完整和不完整场景进行处理，通过两个解码器获取预测结果，并通过交叉熵和KL损失实施标签和非标签数据间的语义一致性。

****实验****: 在S3DIS、ScanNet和Semantic3D数据集上的实验结果表明，该方法能有效利用稀疏的标签数据和丰富的无标签数据增强监督信号，从而提高整体模型性能。

**关键词:** Point Cloud

**详细摘要:** Weakly supervised point cloud segmentation has garnered considerable interest recently, primarily due to its ability to diminish labor-intensive manual labeling costs. The effectiveness of such methods hinges on their ability to augment the supervision signals available for training implicitly. However, we found that most approaches tend to be implemented through complex modeling, which is not conducive to deployment and implementation in resource-poor scenarios. Our study introduces a novel scene consistency modeling approach that significantly enhances weakly supervised point cloud segmentation in this context. By synergistically modeling both complete and incomplete scenes, our method can improve the quality of the supervision signal and save more resources and ease of deployment in practical applications. To achieve this, we first generate the corresponding incomplete scene for the whole scene using windowing techniques. Next, we input the complete and incomplete scenes into a network encoder and obtain prediction results for each scene through two decoders. We enforce semantic consistency between the labeled and unlabeled data in the two scenes by employing cross-entropy and KL loss. This consistent modeling method enables the network to focus more on the same areas in both scenes, capturing local details and effectively increasing the supervision signals. One of the advantages of the proposed method is its simplicity and cost-effectiveness. Because we rely solely on variance and KL loss to model scene consistency, resulting in straightforward computations. Our experimental evaluations on S3DIS, ScanNet, and Semantic3D datasets provide further evidence that our method can effectively leverage sparsely labeled data and abundant unlabeled data to enhance supervision signals and improve the overall model performance.

</description>
      <guid isPermaLink="false">66e5b54001d2a3fbfca1df9e</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>A Method for Matching Low-Resolution Radar Point Cloud with Optical Image Information</title>
      <link>https://www.aminer.cn/pub/66e5e70b01d2a3fbfc5460cd</link>
      <description>## A Method for Matching Low-Resolution Radar Point Cloud with Optical Image Information

**作者:** Yangyang Wang, Peiqi Yang, E. Ma, wengang zhang

****要点****: 本文提出了一种新的空间校准算法，用于匹配低分辨率雷达点云与光学图像信息，实现多传感器数据的融合，并具有在自动驾驶等领域中的实用应用价值。
**方法**：通过融合轮式里程计和雷达增加点云密度，并利用实例分割算法提取标记物的2D像素特征点，再将标记物的3D-2D匹配点用于解决雷达与相机之间的外参问题。
**实验**：在Turtlebot2硬件平台上进行实验，使用的数据集未明确提及，实验结果表明该方法能够在较大分辨率差异条件下校准雷达和相机，并匹配雷达点云与光学图像信息。

**关键词:** Point Cloud

**详细摘要:** The fusion of millimeter-wave radar and optical camera is a challenge in the multi-sensor data fusion field. In this paper a new space calibration algorithm is proposed to match the low-resolution radar point cloud information and the optical image information. This paper proposes that fusing the wheeled odometer and radar to increase the density of point cloud. Moreover, the real 3D model information of marker and radar point cloud are fitted to extract the 3D feature point of marker, and the 2D pixel feature point of the marker is extracted by Instance Segmentation algorithm. Then, the marker's 3D-2D matching points are used to solve the external parameters between the radar and camera. Based on Turtlebot2 hardware platform, the experimental results show that the proposed method can calibrate the radar and camera, and match the radar point cloud and optical image information under the condition of large resolution difference. The proposed algorithm has practical application value in autonomous driving, such as target detection and tracking, scene 3D reconstruction, path planning and navigation.

</description>
      <guid isPermaLink="false">66e5e70b01d2a3fbfc5460cd</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Point Cloud Scanning-Based Identification and Reconstruction of Welds</title>
      <link>https://www.aminer.cn/pub/66e5e70901d2a3fbfc545abd</link>
      <description>## Point Cloud Scanning-Based Identification and Reconstruction of Welds

**作者:** jianglai jiang, liufanghua liu

****要点****: 本文提出了一种基于点云扫描的焊接识别与重构方法，用于壁面爬行式焊接打磨机器人准确获取焊接位置及详细轮廓尺寸和高度。
**方法**：通过深度相机D435进行点云扫描，利用统计和双边滤波对焊接点云进行预处理，最后采用三角剖分投影法重构焊接点云轮廓。
**实验**：实验使用了D435深度相机进行点云扫描，并对采集到的焊接点云数据进行了滤波和重构处理，得到了平滑的焊接点云图和准确的焊接轮廓。

**关键词:** Point Cloud

**详细摘要:** One of the key challenges for the weld grinding wall-climbing robot is to accurately obtain the specific location and the detailed profile size and height of the welds before grinding the hull surface. Given the substantial height of ships, it is difficult for operators to precisely determine the location of welds with the naked eye alone. Therefore, the weld grinding wall-climbing robot designed in this paper is equipped with a depth camera, the D435, which scans and identifies the point cloud of the weld. This paper introduces the principle of the camera and conducts camera calibration work. After scanning and identifying the weld point cloud in three dimensions, the paper uses statistical and bilateral filtering to denoise the ordered and scattered point clouds during the weld point cloud pre-processing, resulting in a smooth weld point cloud map. Finally, the contour of the weld point cloud is reconstructed using the triangulation projection method.

</description>
      <guid isPermaLink="false">66e5e70901d2a3fbfc545abd</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Pose Estimation Algorithm Based on Point Pair Features Using PointNet + +</title>
      <link>https://www.aminer.cn/pub/66e6ca9e01d2a3fbfc55a627</link>
      <description>## Pose Estimation Algorithm Based on Point Pair Features Using PointNet + +

**期刊/会议:** Complex &amp; Intelligent Systems

**作者:** Yifan Chen, Zhenjian Li, Qingdang Li, Mingyue Zhang

****要点****: 本研究提出了一种基于点云的点对特征姿态估计新算法，通过融合PointNet ++和传统点对特征算法，实现了不同场景尺度下物体的准确姿态估计。

****方法****: 算法通过整合PointNet ++技术和点对特征算法，并结合自适应参数-密度聚类方法来处理点云数据，提高了姿态估计的准确性。

****实验****: 在将LineMod数据集转化为点云数据集后，使用该算法进行实验，并在强光和弱光条件下进行了测试，显示了算法的鲁棒性。

**关键词:** Point Cloud

**详细摘要:** Abstract This study proposes an innovative deep learning algorithm for pose estimation based on point clouds, aimed at addressing the challenges of pose estimation for objects affected by the environment. Previous research on using deep learning for pose estimation has primarily been conducted using RGB-D data. This paper introduces an algorithm that utilizes point cloud data for deep learning-based pose computation. The algorithm builds upon previous work by integrating PointNet + + technology and the classical Point Pair Features algorithm, achieving accurate pose estimation for objects across different scene scales. Additionally, an adaptive parameter-density clustering method suitable for point clouds is introduced, effectively segmenting clusters in varying point cloud density environments. This resolves the complex issue of parameter determination for density clustering in different point cloud environments and enhances the robustness of clustering. Furthermore, the LineMod dataset is transformed into a point cloud dataset, and experiments are conducted on the transformed dataset to achieve promising results with our algorithm. Finally, experiments under both strong and weak lighting conditions demonstrate the algorithm's robustness.

</description>
      <guid isPermaLink="false">66e6ca9e01d2a3fbfc55a627</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Research on the Accurate Extraction Method of 3d Point Cloud Feature Data for Simulated Loading Test of Container Ships</title>
      <link>https://www.aminer.cn/pub/6663b5b701d2a3fbfcd68d0c</link>
      <description>## Research on the Accurate Extraction Method of 3d Point Cloud Feature Data for Simulated Loading Test of Container Ships

**作者:** Rui Li, H.Y. Wan, Ji Wang, Shilin Huo, C. Guedes Soares

****要点****: 本文提出了一种用于模拟集装箱船加载测试的三维点云特征数据准确提取方法，旨在提高船舶结构分析的准确性。
**方法**：研究采用基于深度学习的特征提取技术，通过构建一个专用于点云数据处理的卷积神经网络模型，实现了对复杂三维结构的精确识别和特征提取。
**实验**：实验部分利用了自制的船舶点云数据集，通过所提出的方法有效提取了特征数据，并在模拟加载测试中验证了其准确性和可靠性。

**关键词:** Point Cloud

**详细摘要:** Download This Paper Open PDF in Browser Add Paper to My Library Share: Permalink Using these links will ensure access to this page indefinitely Copy URL Copy DOI

</description>
      <guid isPermaLink="false">6663b5b701d2a3fbfcd68d0c</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>LiDAR Point Cloud Descriptor for UAM Place Recognition with Point Cloud Map</title>
      <link>https://www.aminer.cn/pub/661867eb13fb2c6cf6f5cf72</link>
      <description>## LiDAR Point Cloud Descriptor for UAM Place Recognition with Point Cloud Map

**期刊/会议:** Proceedings of the Institute of Navigation  International Technical Meeting/Proceedings of the  International Technical Meeting of The Institute of Navigation

**作者:** Ji-Ung Im, Yong-Ha Lee, Jong‐Hoon Won

****要点****: 本文提出了一种基于点云地图（PCM）和虚拟LiDAR传感器模型的城市空中移动（UAM）地点识别方法，通过创建虚拟描述符数据库（VDD）实现地点识别，并引入了一种采样方法和特征点检测技术以生成平移和旋转不变描述符。

****方法****: 研究利用PCM数据和虚拟LiDAR传感器模型，通过兴趣区域采样和特征点检测方法，结合鲁棒特征提取技术生成平移和旋转不变描述符。

****实验****: 通过基于游戏引擎的UAM模拟器进行实验，生成PCM和VDD，并对描述符及地点识别进行了定量分析。

**关键词:** Point Cloud

**详细摘要:** Accurate localization is a critical element for the successful and safe operation of Urban Air Mobility (UAM). In this study, we present a method for UAM place recognition that utilizes point cloud map (PCM) data and a virtual LiDAR sensor model. The PCM-based approach enables the creation of a virtual descriptor database (VDD) for place recognition. To generate descriptors invariant to translation and rotation, we introduce a region of interest sampling method and a feature point detection approach, effectively minimizing altitude influence. We also outline a technique for creating translation and rotation invariant descriptors through the integration of robust feature extraction methods. Furthermore, we conduct an experiment utilizing a game engine-based UAM simulator to validate the proposed method. PCM and VDD are generated through the simulator, and a quantitative analysis of descriptors and place recognition is subsequently carried out.

</description>
      <guid isPermaLink="false">661867eb13fb2c6cf6f5cf72</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Analysis of the Adaptability of Key Point Detection Methods on 3D Point Cloud</title>
      <link>https://www.aminer.cn/pub/65d80e30939a5f40826d6b60</link>
      <description>## Analysis of the Adaptability of Key Point Detection Methods on 3D Point Cloud

**期刊/会议:** Lecture notes in electrical engineering

**作者:** Mu-Huo Cheng, Shanshan Wang, Jiong Liang, Xi Wang, Xiang Chen, Min Huang

****要点****: 本文对比了传统手工算法和深度学习算法在3D点云关键点检测中的适应性，提出并验证了改进的UAIP算法在关键点提取的准确性和稳定性上的优势。

****方法****: 通过定性和定量两种方式评估关键点提取算法的效率，包括语义一致性、重复性和稳定性。

****实验****: 使用未知数据集进行了实验，证明了基于学习的检测器在关键点检测中具有比手工方法更稳定和可重复的结果，改进的UAIP方法准确性也有所提升。

**关键词:** Point Cloud

**详细摘要:** Machine vision systems using 3D point clouds are becoming important for production management and quality control in the printing and packaging industry. Laying eyes on the effectiveness and utility of key point extraction, this paper has made comparison between the traditional hand-crafted algorithms and the deep learning algorithms, also including UAIP, one of our improved algorithms. Furthermore, the efficiency evaluation on key point extraction is from two aspects: quantitative and qualitative. They are in terms of semantic consistency, repeatability and stability. The algorithm adaptations in different contexts are also summarized in the paper. This experiment demonstrates the advantage of the learning-based detectors in key point detection with much more stable and repeatable results than the hand-crafted methods. The accuracy of our improved UAIP method has also been demonstrated better to a certain extent. This research clarifies the difference of key point extraction algorithms for 3D point clouds. With some practical insights on key point detection, it lays the foundation for the further research on point cloud shape.

</description>
      <guid isPermaLink="false">65d80e30939a5f40826d6b60</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Improved Pass-Through Dimensionality Reduction for Point Cloud Contour Extraction</title>
      <link>https://www.aminer.cn/pub/665b0bdc01d2a3fbfca00938</link>
      <description>## Improved Pass-Through Dimensionality Reduction for Point Cloud Contour Extraction

**期刊/会议:** Proceedings of the Institution of Civil Engineers Bridge engineering/Proceedings of ICE Bridge engineering

**作者:** Dong Liang, Ziyu Jia, B. Wang, Lihang Chen, Xingyu Wang

****要点****: 论文提出了一种改进的直通维数降低方法，用于点云轮廓提取，以改善大型钢材组件的整体信息与螺栓孔局部信息的均匀性分布。

****方法****: 通过结合垂直三维激光扫描与手持三维激光扫描获取点云数据，然后基于改进的直通滤波进行三轴向等距降维，并使用平面点云分布均匀性算法提取角点云。

****实验****: 通过使用标准组件进行点云特征提取测试，验证了所提方法的有效性。实验使用了改进的直通滤波算法，并在标准数据集上进行了测试，结果显示该方法在特征提取的时间和效率上具有优势。

**关键词:** Point Cloud

**详细摘要:** Obtaining a point cloud model that considers the overall information of large steel components as well as the local information of bolt holes is an important approach for the virtual assembly of large steel structures. A point cloud extraction method to improve the uniformity of the dimensionality reduction distribution is presented. Firstly, point cloud data characterising the overall information of a large-size component and local information of the bolt-hole group are obtained by combining vertical three-dimensional (3D) laser scanning and handheld 3D laser scanning. The point cloud is then triaxially equidistant and reduced in dimension based on improved straight-pass filtering and the corner point cloud is extracted using a planar point cloud distribution uniformity algorithm. Finally, the point cloud is restored to the same space to complete the contour extraction of the point cloud. The accuracy of the contour extraction method was verified by conducting point cloud feature extraction tests using standard components. Compared with conventional feature extraction, the method provides targeted local feature extraction for bars with a certain regularity of geometric configuration, reducing the time required for feature extraction and providing a brief database for the virtual assembly of steel joist beams.

</description>
      <guid isPermaLink="false">665b0bdc01d2a3fbfca00938</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Multi‐modality Imaging of the Systemic Right Ventricle in Congenital Heart Disease</title>
      <link>https://www.aminer.cn/pub/660aeef213fb2c6cf62f5fee</link>
      <description>## Multi‐modality Imaging of the Systemic Right Ventricle in Congenital Heart Disease

**期刊/会议:** Echocardiography

**作者:** Amalia Baroutidou, Despοina Ntiloudi, Nearchos Kasinos, Evangelia Nyktari, George Giannakoulas

****要点****: 本文综述了在先天性心脏病患者中评估系统性右心室（sRV）的多种成像方法，强调了心血管成像在sRV评估中的挑战，并提出了标准化定量参数的必要性。
**方法**：通过文献回顾，比较了心血管磁共振（CMR）、斑点追踪超声心动图（STE）和三维超声心动图（3DE）等成像技术，以及这些技术在评估sRV形态和功能中的应用。
**实验**：论文未提供具体实验细节，但指出需要通过形态计量和机制研究来验证sRV指标的正常参考范围。

**关键词:** Multi-Modal

**详细摘要:** A comprehensive and structured imaging approach in the evaluation of the systemic right ventricle (sRV) in patients with complete transposition of the great arteries (TGA) after atrial switch procedure and congenitally corrected transposition of the great arteries (ccTGA) is a key for their optimal lifelong surveillance. Despite the improvements in cardiovascular imaging of adults with congenital heart disease (ACHD), the imaging of sRV remains an ongoing challenge due to its complex morphology and the difficulty in applying the existing knowledge for the systemic left ventricle. While cardiac magnetic resonance (CMR) is considered the gold standard imaging method, echocardiographic evaluation is primarily preferred in everyday clinical setting. Although qualitative assessment of its systolic function is primarily used, the introduction of advanced echocardiographic techniques, such as speckle tracking echocardiography (STE) and three‐dimensional echocardiography (3DE), has provided new insights into the optimal assessment of the sRV. Standardized quantitative parameters remain to be elucidated, and morphometric and mechanistic studies are warranted to validate reference ranges for the sRV. This review highlights the challenges in the optimal evaluation of sRV and summarizes the available imaging tools. Highlights CMR is the gold standard imaging method of sRV. Qualitative assessment of the systolic function of sRV is primarily used. Advanced echocardiographic techniques (STE and 3DE) provide optimal sRV assessment. Reference ranges for the sRV indices are warranted to be validated.

</description>
      <guid isPermaLink="false">660aeef213fb2c6cf62f5fee</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Imaginations Generate Images for Multi-modal Machine Translation</title>
      <link>https://www.aminer.cn/pub/66045dcc13fb2c6cf67881ec</link>
      <description>## Imaginations Generate Images for Multi-modal Machine Translation

**期刊/会议:** Lecture notes in electrical engineering

**作者:** Xiaona Yang, Weiqiang Sun, Wei, Yinlin Li, Xiayang Shi

****要点****: 本研究提出了一种基于生成想象网络的机器翻译方法，通过自动生成视觉注释，以改善多模态机器翻译的性能，减少对人工标注图像数据的依赖。

****方法****: 研究采用了一种新型的生成想象网络，结合了Transformer架构，根据源语言和目标语言的双语句子自动生成与之语义对等的视觉注释。

****实验****: 实验部分使用了英德翻译的IWSLT-2015数据集，结果显示该模型能生成高质量的注释图像，并显著提高了多模态机器翻译的性能。

**关键词:** Multi-Modal

**详细摘要:** Multi-modal machine translation (MMT) aims at exploring better translation systems by integrating the visual annotation which presents the content described in the bilingual parallel sentence pair into the conventional only-text neural machine translation (NMT). However, existing methods heavily rely on the manual annotated images data set. The cost of manual image annotation is relatively high at this stage. In this paper, we propose the generative imagination network with transformer to automatically generate visual annotations semantic-equivalent with source and target sentences. The proposed model receives the inputs of source-target bilingual sentences and generates visual annotations for MMT. Experiments analysis demonstrate that our model can generate high-quality annotated images and prompt the performance of MMT. Additionally, we use our model to generate annotated images for a famous English-German IWSLT-2015, the experimental results show the improvement for MMT.

</description>
      <guid isPermaLink="false">66045dcc13fb2c6cf67881ec</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Gut-Brain Nexus: Mapping Multi-Modal Links to Neurodegeneration at Biobank Scale</title>
      <link>https://www.aminer.cn/pub/66e5b16001d2a3fbfc996954</link>
      <description>## Gut-Brain Nexus: Mapping Multi-Modal Links to Neurodegeneration at Biobank Scale

**作者:** Mohammad Shafieinouri, Samantha Hong, Artur Schuh, Mary B. Makarious, Rodrigo Sandon, Paul Suhwan Lee, Emily Simmonds, Hirotaka Iwaki, Gracelyn Hill, Cornelis Blauwendraat, Valentina Escott-Price, Yue A. Qi, Alastair J. Noyce, Armando Reyes-Palomares, Hampton Leonard, Malu Tansey, Andrew Singleton, Mike A. Nalls, Kristin S. Levine, Sara Bandres-Ciga

****要点****: 本研究利用英国生物银行、SAIL生物银行和FinnGen的数据，探讨了内分泌、营养、代谢和消化系统疾病与阿尔茨海默病（AD）和帕金森病（PD）风险之间的关联，并开发了一种多类别预测模型，该模型整合了临床、遗传、蛋白质组学和人口学数据，提高了疾病分类的准确性。
**方法**：研究采用无偏倚的群体规模研究方法，结合遗传学、蛋白质组学和临床数据，研究了多种疾病与AD和PD风险之间的联系。
**实验**：利用UK Biobank Resource（应用编号33601）的匿名数据，分析了多种内分泌、营养、代谢和消化系统疾病与AD和PD风险的关系，并建立了多组学预测模型。结果显示，一些疾病在AD和PD诊断前会增加AD和PD的风险，且多组学模型的预测效果优于单一范式方法。

**关键词:** Multi-Modal

**详细摘要:** Alzheimer's disease (AD) and Parkinson's disease (PD) are influenced by genetic and environmental factors. Using data from UK Biobank, SAIL Biobank, and FinnGen, we conducted an unbiased, population-scale study to: 1) Investigate how 155 endocrine, nutritional, metabolic, and digestive system disorders are associated with AD and PD risk prior to their diagnosis, considering known genetic influences; 2) Assess plasma biomarkers' specificity for AD or PD in individuals with these conditions; 3) Develop a multi-classification model integrating genetics, proteomics, and clinical data relevant to conditions affecting the gut-brain axis. Our findings show that certain disorders elevate AD and PD risk before AD and PD diagnosis including: insulin and non-insulin dependent diabetes mellitus, noninfective gastro-enteritis and colitis, functional intestinal disorders, and bacterial intestinal infections, among others. Polygenic risk scores revealed lower genetic predisposition to AD and PD in individuals with co-occurring disorders in the study categories, underscoring the importance of regulating the gut-brain axis to potentially prevent or delay the onset of neurodegenerative diseases. The proteomic profile of AD/PD cases was influenced by comorbid endocrine, nutritional, metabolic, and digestive systems conditions. Importantly, we developed multi-omics prediction models integrating clinical, genetic, proteomic and demographic data, the combination of which performs better than any single paradigm approach in disease classification. This work aims to illuminate the intricate interplay between various physiological factors involved in the gut-brain axis and the development of AD and PD, providing a multifactorial systemic understanding that goes beyond traditional approaches.  ### Competing Interest Statement  K.S.L., H.L.L., H.I., M.B.M., and M.A.N.'s participation in this project was part of a competitive contract awarded to Data Tecnica International LLC by the National Institutes of Health to support open science research. M.A.N. also currently serves on the scientific advisory board at Clover Therapeutics and is an advisor and scientific founder at Neuron23 Inc.  ### Funding Statement  This research was supported in part by the Intramural Research Program of the NIH, National Institute on Aging (NIA), National Institutes of Health, Department of Health and Human Services; project number ZO1 AG000534, as well as the National Institute of Neurological Disorders and Stroke. This work was also supported by the Dementia Research Institute [UKDRI supported by the Medical Research Council (UKDRI-3003), Alzheimer's Research UK, and Alzheimer's Society], Welsh Government, Joint Programming for Neurodegeneration (MRC: MR/T04604X/1), Dementia Platforms UK (MRC: MR/L023784/2) and MRC Centre for Neuropsychiatric Genetics and Genomics (MR/L010305/1).   ### Author Declarations  I confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.  Yes  The details of the IRB/oversight body that provided approval or exemption for the research described are given below:  This paper analyzes existing, publicly available data. In addition, complete summary statistics describing these data/processed datasets derived from these data have been deposited in the supplementary materials connected to this publication and are publicly available as of the date of publication. This research has been conducted using the UK Biobank Resource under application number 33601 (https://ukbiobank.dnanexus.com). We want to acknowledge the participants and investigators of the FinnGen study and thank them for their hard work and generosity. This study makes use of anonymised data held in the Secure Anonymised Information Linkage (SAIL) Databank. This work uses data provided by patients and collected by the NHS as part of their care and support.  I confirm that all necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived, and that any patient/participant/sample identifiers included were not known to anyone (e.g., hospital staff, patients or participants themselves) outside the research group so cannot be used to identify individuals.  Yes  I understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).  Yes  I have followed all appropriate research reporting guidelines, such as any relevant EQUATOR Network research reporting checklist(s) and other pertinent material, if applicable.  Yes  This paper analyzes existing, publicly available data. In addition, complete summary statistics describing these data/processed datasets derived from these data have been deposited in the supplementary materials connected to this publication and are publicly available as of the date of publication.

</description>
      <guid isPermaLink="false">66e5b16001d2a3fbfc996954</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Demo: EmoMarker: A Privacy-Preserving, Multi-Modal Sensing System for Dyadic Digital Biomarkers of Expressed Emotions for Patients with Dementia</title>
      <link>https://www.aminer.cn/pub/66e5e48801d2a3fbfc4facc8</link>
      <description>## Demo: EmoMarker: A Privacy-Preserving, Multi-Modal Sensing System for Dyadic Digital Biomarkers of Expressed Emotions for Patients with Dementia

**作者:** Y.Y. Li, Doris S.F. Yu, Shuangzhou Chen, Guoliang Xing, Hongkai Chen

****要点****: 本文提出了一种名为EmoMarker的隐私保护多模态感知系统，用于检测痴呆症患者家庭中表达情绪的二元数字生物标志物，以评估家庭情感环境。
**方法**：EmoMarker系统由一个隐私保护的深度摄像头和麦克风组成，用于提取解释性的二元（即运动和声学）数字生物标志物，并预测家庭高度量表（Family Altitude Scale）的得分。
**实验**：系统在99位老年人的家庭中部署，并使用初步实验数据集，实现了81.13%的预测准确度。

**关键词:** Multi-Modal

**详细摘要:** Alzheimer's disease and related dementia has emerged as a global health challenge due to aging population. Expressed Emotion (EE) is a widely-used medical measure of family emotional environment of patients with caregivers. We present EmoMarker, a multi-modal sensor detection system for dyadic digital biomarkers of EE in dementia patients' homes. EmoMarker consists of a privacy-preserving depth camera and a microphone to extracts interpretable dyadic (i.e., motor and acoustic) digital biomarkers of the interaction between patients and caregivers and predict the scores of Family Altitude Scale, an assessment tool for measuring the emotional climate of families. We have deployed our system in 99 elder people's homes and achieved 81.13% prediction accuracy in preliminary results.

</description>
      <guid isPermaLink="false">66e5e48801d2a3fbfc4facc8</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Multi-modal Subtypes Identified in Alzheimer’s Disease Neuroimaging Initiative Participants by Missing-Data-enabled Subtype and Stage Inference</title>
      <link>https://www.aminer.cn/pub/667b907f01d2a3fbfce08d18</link>
      <description>## Multi-modal Subtypes Identified in Alzheimer’s Disease Neuroimaging Initiative Participants by Missing-Data-enabled Subtype and Stage Inference

**期刊/会议:** Brain Communications

**作者:** Mar Estarellas, Neil P Oxtoby, Jonathan M Schott, Daniel C Alexander, Alexandra L Young

****要点****: 本研究通过改进Subtype and Stage Inference（SuStaIn）算法以处理缺失数据，从而在阿尔茨海默病神经影像学计划（ADNI）的789名参与者中识别出五种多模态亚型，揭示了阿尔茨海默病的更细致亚型景观。

****方法****: 研究团队对SuStaIn算法进行改进，使其能够处理缺失数据，从而在多模态数据（包括磁共振成像、正电子发射断层扫描、脑脊液和认知测试）上应用该算法。

****实验****: 实验在ADNI数据集上进行，通过改进后的算法识别出五种亚型，并分析了这些亚型与年龄、教育年限、Apolipoprotein E（APOE4）状态、白质高信号负荷以及从轻度认知障碍向阿尔茨海默病转化的速率之间的关系。结果显示，不同亚型与不同的风险因素相关，其中“认知”亚型临床进展最快，“亚皮质”亚型最慢。

**关键词:** Multi-Modal

**详细摘要:** Alzheimer’s disease (AD) is a highly heterogeneous disease in which different biomarkers are dynamic over different windows of the decades-long pathophysiological processes, and potentially have distinct involvement in different subgroups. Subtype and Stage Inference (SuStaIn) is an unsupervised learning algorithm that disentangles the phenotypic heterogeneity and temporal progression of disease biomarkers, providing disease insight and quantitative estimates of individual subtype and stage. However, a key limitation of SuStaIn is that it requires a complete set of biomarkers for each subject, reducing the number of data points available for model fitting and limiting applications of SuStaIn to modalities that are widely collected, e.g. volumetric biomarkers derived from structural MRI. In this study, we adapted the SuStaIn algorithm to handle missing data, enabling the application of SuStaIn to multimodal data (magnetic resonance imaging, positron emission tomography, cerebrospinal fluid and cognitive tests) from 789 participants in the Alzheimer’s Disease Neuroimaging Initiative (ADNI). Missing-data SuStaIn identified five subtypes having distinct progression patterns, which we describe by the earliest unique abnormality as ‘Typical AD with Early Tau’, ‘Typical AD with Late Tau’, ‘Cortical’, ‘Cognitive’ and ‘Subcortical’. These new multi-modal subtypes were differentially associated with age, years of education, Apolipoprotein E (APOE4) status, white matter hyperintensity burden, and the rate of conversion from mild cognitive impairment to Alzheimer’s disease, with the ‘Cognitive’ subtype showing the fastest clinical progression, and the ‘Subcortical’ subtype the slowest. Overall, we demonstrate that missing-data SuStaIn reveals a finer landscape of Alzheimer’s disease subtypes, each of which are associated with different risk factors. Missing-data SuStaIn has broad utility, enabling the prediction of progression in a much wider set of individuals, rather than being restricted to those with complete data.

</description>
      <guid isPermaLink="false">667b907f01d2a3fbfce08d18</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>VISIONE Feature Repository for VBS: Multi-Modal Features and Detected Objects from VBSLHE Dataset</title>
      <link>https://www.aminer.cn/pub/6655123601d2a3fbfc3e86d1</link>
      <description>## VISIONE Feature Repository for VBS: Multi-Modal Features and Detected Objects from VBSLHE Dataset

**期刊/会议:** Zenodo (CERN European Organization for Nuclear Research)

**作者:** Giuseppe Amato, Paolo Bolettieri, Fabio Carrara, Fabrizio Falchi, Claudio Gennaro, Nicola Messina, Lucia Vadicamo, Claudio Vairo

****要点****: 本研究发布了VISIONE特征库，包含从VBSLHE数据集提取的多模态特征和检测到的对象，用于视频浏览器 showdown（VBS）比赛。

****方法****: 研究团队从VBSLHE数据集中提取了视频片段的特征，包括ALADIN、CLIP ViT-H/14 - LAION-2B、CLIP ViT-L/14和CLIP2Video等特征，并使用Faster R-CNN+Inception ResNet、Mask R-CNN和VfNet等模型检测视频中的对象。

****实验****: 实验使用了VBSLHE数据集，包含75个视频文件，每个视频被分成最长5秒的视频片段。特征库中的数据以Creative Commons Attribution许可证发布，并提供了用于提取特征和对象的脚本及数据。

**关键词:** Multi-Modal

**详细摘要:** This repository contains a diverse set of features extracted from the VBSLHE dataset (laparoscopic gynecology) . These features will be utilized in the VISIONE system [Amato et al. 2023, Amato et al. 2022] in the next editions of the Video Browser Showdown (VBS) competition (https://www.videobrowsershowdown.org/). We used a snapshot of the dataset provided by the Medical University of Vienna and Toronto that can be downloaded using the instructions provided at https://download-dbis.dmi.unibas.ch/mvk/. It comprises 75 video files. We divided each video into video shots with a maximum duration of 5 seconds. This repository is released under a Creative Commons Attribution license. If you use it in any form for your work, please cite the following paper: @inproceedings{amato2023visione, title={VISIONE at Video Browser Showdown 2023}, author={Amato, Giuseppe and Bolettieri, Paolo and Carrara, Fabio and Falchi, Fabrizio and Gennaro, Claudio and Messina, Nicola and Vadicamo, Lucia and Vairo, Claudio}, booktitle={International Conference on Multimedia Modeling}, pages={615--621}, year={2023}, organization={Springer} } This repository (v2) comprises the following files: msb.tar.gz contains tab-separated files (.tsv) for each video. Each tsv file reports, for each video segment, the timestamp and frame number marking the start/end of the video segment, along with the timestamp of the extracted middle frame and the associated identifier ("id_visione"). extract-keyframes-from-msb.tar.gz contains a Python script designed to extract the middle frame of each video segment from the MSB files. To run the script successfully, please ensure that you have the original VBSLHE videos available. features-aladin.tar.gz† contains ALADIN [Messina N. et al. 2022] features extracted for all the segment's middle frames. features-clip-laion.tar.gz† contains CLIP ViT-H/14 - LAION-2B [Schuhmann et al. 2022] features extracted for all the segment's middle frames. features-clip-openai.tar.gz† contains CLIP ViT-L/14 [Radford et al. 2021] features extracted for all the segment's middle frames. features-clip2video.tar.gz† contains CLIP2Video [Fang H. et al. 2021] extracted for all the video segments. objects-frcnn-oiv4.tar.gz* contains the objects detected using Faster R-CNN+Inception ResNet (trained on the Open Images V4 [Kuznetsova et al. 2020]). objects-mrcnn-lvis.tar.gz* contains the objects detected using Mask R-CNN [He et al. 2017] (trained on LVIS). objects-vfnet64-coco.tar.gz* contains the objects detected using VfNet [Zhang et al. 2021] (trained on COCO dataset). *Please be sure to use the v2 version of this repository, since v1 feature files may contain inconsistencies that have now been corrected *Note on the object annotations: Within an object archive, there is a jsonl file for each video, where each row contains a record of a video segment (the "_id" corresponds to the "id_visione" used in the msb.tar.gz) . Additionally, there are three arrays representing the objects detected, the corresponding scores, and the bounding boxes. The format of these arrays is as follows: "object_class_names": vector with the class name of each detected object. "object_scores": scores corresponding to each detected object. "object_boxes_yxyx": bounding boxes of the detected objects in the format (ymin, xmin, ymax, xmax). †Note on the cross-modal features: The extracted multi-modal features (ALADIN, CLIPs, CLIP2Video) enable internal searches within the VBSLHE dataset using the query-by-image approach (features can be compared with the dot product). However, to perform searches based on free text, the text needs to be transformed into the joint embedding space according to the specific network being used (see links above). Please be aware that the service for transforming text into features is not provided within this repository and should be developed independently using the original feature repositories linked above. We have plans to release the code in the future, allowing the reproduction of the VISIONE system, including the instantiation of all the services to transform text into cross-modal features. However, this work is still in progress, and the code is not currently available. References: [Amato et al. 2023] Amato, G.et al., 2023, January. VISIONE at Video Browser Showdown 2023. In International Conference on Multimedia Modeling (pp. 615-621). Cham: Springer International Publishing. [Amato et al. 2022] Amato, G. et al. (2022). VISIONE at Video Browser Showdown 2022. In: , et al. MultiMedia Modeling. MMM 2022. Lecture Notes in Computer Science, vol 13142. Springer, Cham. [Fang H. et al. 2021] Fang H. et al., 2021. Clip2video: Mastering video-text retrieval via image clip. arXiv preprint arXiv:2106.11097. [He et al. 2017] He, K., Gkioxari, G., Dollár, P. and Girshick, R., 2017. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision (pp. 2961-2969). [Kuznetsova et al. 2020] Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J., Kamali, S., Popov, S., Malloci, M., Kolesnikov, A. and Duerig, T., 2020. The open images dataset v4. International Journal of Computer Vision, 128(7), pp.1956-1981. [Lin et al. 2014] Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Dollár, P. and Zitnick, C.L., 2014, September. Microsoft coco: Common objects in context. In European conference on computer vision (pp. 740-755). Springer, Cham. [Messina et al. 2022] Messina N. et al., 2022, September. Aladin: distilling fine-grained alignment scores for efficient image-text matching and retrieval. In Proceedings of the 19th International Conference on Content-based Multimedia Indexing (pp. 64-70). [Radford et al. 2021] Radford A. et al., 2021, July. Learning transferable visual models from natural language supervision. In International conference on machine learning (pp. 8748-8763). PMLR. [Schuhmann et al. 2022] Schuhmann C. et al., 2022. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in Neural Information Processing Systems, 35, pp.25278-25294. [Zhang et al. 2021] Zhang, H., Wang, Y., Dayoub, F. and Sunderhauf, N., 2021. Varifocalnet: An iou-aware dense object detector. In Proceedings of the IEEE/CV

</description>
      <guid isPermaLink="false">6655123601d2a3fbfc3e86d1</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
    <item>
      <title>Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks</title>
      <link>https://www.aminer.cn/pub/660f5aea13fb2c6cf6543c9f</link>
      <description>## Meta Invariance Defense Towards Generalizable Robustness to Unknown Adversarial Attacks

**期刊/会议:** IEEE transactions on pattern analysis and machine intelligence

**作者:** Lei Zhang, Yuhang Zhou, Yi Yang, Xinbo Gao

**要点**: 这篇论文提出了一个攻击无关的防御方法Meta Invariance Defense(MID)，通过对抗性攻击池中的多样性组合随机取样构成不同的防御任务，并通过元原则使用多一致性蒸馏来监督学生编码器学习攻击不变特征，从而实现对未知攻击的鲁棒性。
方法：通过对抗性攻击池的随机取样构成不同的防御任务，并使用多一致性蒸馏来监督学生编码器学习攻击不变的特征。
实验：在多个基准测试中，如ImageNet，论文对MID在各种攻击下的通用鲁棒性和优越性进行了理论和实证研究。

**关键词:** 

**详细摘要:** Despite providing high-performance solutions for computer vision tasks, thedeep neural network (DNN) model has been proved to be extremely vulnerable toadversarial attacks. Current defense mainly focuses on the known attacks, butthe adversarial robustness to the unknown attacks is seriously overlooked.Besides, commonly used adaptive learning and fine-tuning technique isunsuitable for adversarial defense since it is essentially a zero-shot problemwhen deployed. Thus, to tackle this challenge, we propose an attack-agnosticdefense method named Meta Invariance Defense (MID). Specifically, variouscombinations of adversarial attacks are randomly sampled from a manuallyconstructed Attacker Pool to constitute different defense tasks against unknownattacks, in which a student encoder is supervised by multi-consistencydistillation to learn the attack-invariant features via a meta principle. Theproposed MID has two merits: 1) Full distillation from pixel-, feature- andprediction-level between benign and adversarial samples facilitates thediscovery of attack-invariance. 2) The model simultaneously achieves robustnessto the imperceptible adversarial perturbations in high-level imageclassification and attack-suppression in low-level robust image regeneration.Theoretical and empirical studies on numerous benchmarks such as ImageNetverify the generalizable robustness and superiority of MID under variousattacks.

</description>
      <guid isPermaLink="false">660f5aea13fb2c6cf6543c9f</guid>
      <pubDate>Wed, 18 Sep 2024 14:43:27 +0000</pubDate>
    </item>
  </channel>
</rss>
