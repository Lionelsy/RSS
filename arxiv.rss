<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 20 Sep 2024 19:33:41 +0800</lastBuildDate>
    <item>
      <title>Implicit Reasoning in Deep Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2409.10840v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，时间序列基础模型在来自多个领域的时间序列数据上展现了令人期待的零-shot预测性能。&lt;h4&gt;2. 研究问题&lt;/h4&gt;   - 目前尚不清楚这些成功是源于对时间动态的真正理解，还是仅仅来自于对训练数据的记忆。&lt;h4&gt;3. 现有研究的局限&lt;/h4&gt;   - 尽管语言模型中的隐性推理已被研究，但对时间序列模型的类似评估仍然基本未被探索。&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文旨在初步评估深度时间序列预测模型的推理能力。&lt;h4&gt;5. 模型类型&lt;/h4&gt;   - 研究发现某些线性模型、基于多层感知器（MLP）的模型以及基于补丁的Transformer模型在系统设计的分布外场景中有效地进行了泛化。&lt;h4&gt;6. 推理能力的发现&lt;/h4&gt;   - 这些结果表明，这些模型具备超出简单模式记忆的推理能力，显示出尚未被充分探索的推理潜力。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, time series foundation models have shown promising zero-shotforecasting performance on time series from a wide range of domains. However,it remains unclear whether their success stems from a true understanding oftemporal dynamics or simply from memorizing the training data. While implicitreasoning in language models has been studied, similar evaluations for timeseries models have been largely unexplored. This work takes an initial steptoward assessing the reasoning abilities of deep time series forecastingmodels. We find that certain linear, MLP-based, and patch-based Transformermodels generalize effectively in systematically orchestratedout-of-distribution scenarios, suggesting underexplored reasoning capabilitiesbeyond simple pattern memorization.</description>
      <author>example@mail.com (Willa Potosnak, Cristian Challu, Mononito Goswami, Michał Wiliński, Nina Żukowska, Artur Dubrawski)</author>
      <guid isPermaLink="false">2409.10840v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:33:41 +0800</pubDate>
    </item>
    <item>
      <title>Vision foundation models: can they be applied to astrophysics data?</title>
      <link>http://arxiv.org/abs/2409.11175v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, submitted to Foundation Models for Science
  Workshop at NeurIPS 2024&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 视觉基础模型的潜力&lt;/h4&gt;   - 视觉基础模型在许多多媒体应用中展示了显著的潜力，但在自然科学领域的应用较少。&lt;h4&gt;2. 领域特定数据的挑战&lt;/h4&gt;   - 这种低应用主要由于领域特定科学数据与基础模型训练数据的性质不匹配，导致分布偏移。&lt;h4&gt;3. 科学数据的特点&lt;/h4&gt;   - 科学数据在结构和特征上通常有显著差异，研究人员面临着使用有限标注数据（通常只有几百或几千张图像）优化模型性能的挑战。&lt;h4&gt;4. 模型适应性要求&lt;/h4&gt;   - 要有效适应基础模型，需要在预处理、数据增强和训练技术上采取定制化的方法。&lt;h4&gt;5. 模型的独特性&lt;/h4&gt;   - 每种视觉基础模型都有独特的优缺点，这些差异源于架构、训练程序和训练数据集的不同。&lt;h4&gt;6. 研究对象&lt;/h4&gt;   - 本研究评估了各种视觉基础模型在天体物理数据（特别是光学和射电天文学图像）中的应用。&lt;h4&gt;7. 分类准确性提升&lt;/h4&gt;   - 结果表明，使用特定基础模型提取的特征能提高光学星系图像的分类准确性，相比传统的监督训练方法更为有效。&lt;h4&gt;8. 物体检测性能&lt;/h4&gt;   - 在射电图像的物体检测任务中，这些模型也表现出等同或更好的性能。&lt;h4&gt;9. 射电星系图像分类的局限&lt;/h4&gt;   - 然而，在射电星系图像的分类中，它们的性能普遍较差，常常低于传统监督训练的结果。&lt;h4&gt;10. 模型选择的考虑&lt;/h4&gt;    - 这些发现表明，选择适合天体物理应用的视觉基础模型需要仔细考虑模型特性及与下游任务的具体要求的对齐。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models, which have demonstrated significant potential inmany multimedia applications, are often underutilized in the natural sciences.This is primarily due to mismatches between the nature of domain-specificscientific data and the typical training data used for foundation models,leading to distribution shifts. Scientific data often differ substantially instructure and characteristics; researchers frequently face the challenge ofoptimizing model performance with limited labeled data of only a few hundred orthousand images. To adapt foundation models effectively requires customizedapproaches in preprocessing, data augmentation, and training techniques.Additionally, each vision foundation model exhibits unique strengths andlimitations, influenced by differences in architecture, training procedures,and the datasets used for training. In this work, we evaluate the applicationof various vision foundation models to astrophysics data, specifically imagesfrom optical and radio astronomy. Our results show that using featuresextracted by specific foundation models improves the classification accuracy ofoptical galaxy images compared to conventional supervised training. Similarly,these models achieve equivalent or better performance in object detection taskswith radio images. However, their performance in classifying radio galaxyimages is generally poor and often inferior to traditional supervised trainingresults. These findings suggest that selecting suitable vision foundationmodels for astrophysics applications requires careful consideration of themodel characteristics and alignment with the specific requirements of thedownstream tasks.</description>
      <author>example@mail.com (E. Lastufka, M. Drozdova, V. Kinakh, D. Piras, S. Voloshynovskyy)</author>
      <guid isPermaLink="false">2409.11175v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:33:41 +0800</pubDate>
    </item>
    <item>
      <title>jina-embeddings-v3: Multilingual Embeddings With Task LoRA</title>
      <link>http://arxiv.org/abs/2409.10173v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, pp11-13 references, pp14-20 appendix and experiment tables&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 模型介绍&lt;/h4&gt;   - 介绍了jina-embeddings-v3，这是一个新的文本嵌入模型，拥有5.7亿个参数。&lt;h4&gt;2. 性能表现&lt;/h4&gt;   - 在多语言数据和长上下文检索任务上，jina-embeddings-v3实现了最先进的性能，支持最长达8192个标记的上下文长度。&lt;h4&gt;3. 任务特定适配器&lt;/h4&gt;   - 模型包含一组特定于任务的低秩适配器（LoRA），用于生成高质量的嵌入，适用于查询-文档检索、聚类、分类和文本匹配等任务。&lt;h4&gt;4. 基准评估&lt;/h4&gt;   - 在MTEB基准测试中，jina-embeddings-v3在英语任务上超越了OpenAI和Cohere的最新专有嵌入模型。&lt;h4&gt;5. 多语言任务的优势&lt;/h4&gt;   - 在所有多语言任务中，jina-embeddings-v3的表现优于multilingual-e5-large-instruct。&lt;h4&gt;6. 输出维度灵活性&lt;/h4&gt;   - 默认输出维度为1024，用户可以灵活地将嵌入维度降至最低32，而不会影响性能，这得益于Matryoshka Representation Learning。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce jina-embeddings-v3, a novel text embedding model with 570million parameters, achieves state-of-the-art performance on multilingual dataand long-context retrieval tasks, supporting context lengths of up to 8192tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)adapters to generate high-quality embeddings for query-document retrieval,clustering, classification, and text matching. Evaluation on the MTEB benchmarkshows that jina-embeddings-v3 outperforms the latest proprietary embeddingsfrom OpenAI and Cohere on English tasks, while achieving superior performancecompared to multilingual-e5-large-instruct across all multilingual tasks. Witha default output dimension of 1024, users can flexibly reduce the embeddingdimensions to as low as 32 without compromising performance, enabled byMatryoshka Representation Learning.</description>
      <author>example@mail.com (Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao)</author>
      <guid isPermaLink="false">2409.10173v3</guid>
      <pubDate>Fri, 20 Sep 2024 19:33:41 +0800</pubDate>
    </item>
    <item>
      <title>Human-like Affective Cognition in Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11733v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 情感理解的重要性&lt;/h4&gt;   - 理解情感是人类互动和体验的基础。&lt;h4&gt;2. 人类的情感推理能力&lt;/h4&gt;   - 人类能够轻松地从情境或面部表情推断情感，并能从情感推断情境，以及进行其他多种情感认知。&lt;h4&gt;3. 现代AI的推理能力&lt;/h4&gt;   - 本文探讨现代AI在这些推理方面的能力。&lt;h4&gt;4. 评估框架的提出&lt;/h4&gt;   - 介绍了一种评估框架，用于测试基础模型的情感认知能力。&lt;h4&gt;5. 基于心理学理论的场景生成&lt;/h4&gt;   - 从心理学理论出发，生成了1,280个多样的场景，以探索评估、情感、表情和结果之间的关系。&lt;h4&gt;6. 模型与人类的评估&lt;/h4&gt;   - 对基础模型（如GPT-4、Claude-3、Gemini-1.5-Pro）和567名人类进行评估。&lt;h4&gt;7. 研究结果&lt;/h4&gt;   - 结果显示，基础模型往往与人类直觉一致，匹配或超过参与者之间的协议。&lt;h4&gt;8. 超越人类的表现&lt;/h4&gt;   - 在某些条件下，这些模型表现出“超人类”的能力，能够更好地预测典型人类判断。&lt;h4&gt;9. 思维链的好处&lt;/h4&gt;   - 所有模型在链式推理方面均受益，这表明基础模型已获得人类般的情感理解及其对信念和行为的影响。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding emotions is fundamental to human interaction and experience.Humans easily infer emotions from situations or facial expressions, situationsfrom emotions, and do a variety of other affective cognition. How adept ismodern AI at these inferences? We introduce an evaluation framework for testingaffective cognition in foundation models. Starting from psychological theory,we generate 1,280 diverse scenarios exploring relationships between appraisals,emotions, expressions, and outcomes. We evaluate the abilities of foundationmodels (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefullyselected conditions. Our results show foundation models tend to agree withhuman intuitions, matching or exceeding interparticipant agreement. In someconditions, models are ``superhuman'' -- they better predict modal humanjudgements than the average human. All models benefit from chain-of-thoughtreasoning. This suggests foundation models have acquired a human-likeunderstanding of emotions and their influence on beliefs and behavior.</description>
      <author>example@mail.com (Kanishk Gandhi, Zoe Lynch, Jan-Philipp Fränken, Kayla Patterson, Sharon Wambu, Tobias Gerstenberg, Desmond C. Ong, Noah D. Goodman)</author>
      <guid isPermaLink="false">2409.11733v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:33:41 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11752v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 数字病理学领域的肿瘤分割在近年来取得了显著进展。&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 器官差异、组织制备方法和图像获取过程的变化可能导致数字病理图像之间的领域差异。&lt;h4&gt;3. 研究方法&lt;/h4&gt;   - 本文使用Rein，一种微调方法，以参数化和高效的方式对各种视觉基础模型（VFM）进行微调，针对MICCAI 2024跨器官和跨扫描腺癌分割（COSAS2024）挑战。&lt;h4&gt;4. Rein的核心&lt;/h4&gt;   - Rein的核心是一个可学习的令牌集合，直接与实例关联，提升了每层在实例级别的功能。&lt;h4&gt;5. 实验环境&lt;/h4&gt;   - 在COSAS2024挑战的数据环境中，进行了广泛的实验，证明Rein对VFM的微调取得了满意的结果。&lt;h4&gt;6. 具体实现&lt;/h4&gt;   - 使用Rein对ConvNeXt和DINOv2进行了微调。&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - ConvNeXt在任务1的初步测试阶段和最终测试阶段分别获得了0.7719和0.7557的分数。   - DINOv2在任务2的初步测试阶段和最终测试阶段分别获得了0.8848和0.8192的分数。&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, significant progress has been made in tumor segmentationwithin the field of digital pathology. However, variations in organs, tissuepreparation methods, and image acquisition processes can lead to domaindiscrepancies among digital pathology images. To address this problem, in thispaper, we use Rein, a fine-tuning method, to parametrically and efficientlyfine-tune various vision foundation models (VFMs) for MICCAI 2024 Cross-Organand Cross-Scanner Adenocarcinoma Segmentation (COSAS2024). The core of Reinconsists of a set of learnable tokens, which are directly linked to instances,improving functionality at the instance level in each layer. In the dataenvironment of the COSAS2024 Challenge, extensive experiments demonstrate thatRein fine-tuned the VFMs to achieve satisfactory results. Specifically, we usedRein to fine-tune ConvNeXt and DINOv2. Our team used the former to achievescores of 0.7719 and 0.7557 on the preliminary test phase and final test phasein task1, respectively, while the latter achieved scores of 0.8848 and 0.8192on the preliminary test phase and final test phase in task2. Code is availableat GitHub.</description>
      <author>example@mail.com (Pengzhou Cai, Xueyuan Zhang, Libin Lan, Ze Zhao)</author>
      <guid isPermaLink="false">2409.11752v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:33:41 +0800</pubDate>
    </item>
    <item>
      <title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title>
      <link>http://arxiv.org/abs/2409.12046v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;br&gt;&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. TFL的作用&lt;/h4&gt;   - 表格、图形和列表（TFLs）是总结临床试验数据的重要工具。&lt;h4&gt;2. 制作TFL的挑战&lt;/h4&gt;   - 创建TFL用于报告活动通常是一个耗时的任务，在临床试验执行过程中经常遇到。&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本研究探讨了使用大型语言模型（LLMs）通过提示工程和少量样本迁移学习来自动化生成TFL的可能性。&lt;h4&gt;4. 数据来源&lt;/h4&gt;   - 使用公共的ADaM格式临床试验数据进行研究。&lt;h4&gt;5. 研究结果&lt;/h4&gt;   - 结果表明，LLMs能够通过提示指令有效地生成TFL，展示了其在该领域的潜力。&lt;h4&gt;6. 开发应用程序&lt;/h4&gt;   - 开发了一种名为“临床试验TFL生成代理”的对话代理应用，能够将用户查询与预定义提示匹配。&lt;h4&gt;7. 功能特点&lt;/h4&gt;   - 该应用生成定制程序以生成特定的预定义TFL，从而提高生成效率。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tables, figures, and listings (TFLs) are essential tools for summarizingclinical trial data. Creation of TFLs for reporting activities is often atime-consuming task encountered routinely during the execution of clinicaltrials. This study explored the use of large language models (LLMs) to automatethe generation of TFLs through prompt engineering and few-shot transferlearning. Using public clinical trial data in ADaM format, our resultsdemonstrated that LLMs can efficiently generate TFLs with prompt instructions,showcasing their potential in this domain. Furthermore, we developed aconservational agent named Clinical Trial TFL Generation Agent: An app thatmatches user queries to predefined prompts that produce customized programs togenerate specific predefined TFLs.</description>
      <author>example@mail.com (Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu)</author>
      <guid isPermaLink="false">2409.12046v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:33:41 +0800</pubDate>
    </item>
  </channel>
</rss>
