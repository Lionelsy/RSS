<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0">
  <channel>
    <title>Arxiv论文推荐</title>
    <link>https://github.com/lionelsy/RSS</link>
    <description>Arxiv论文推荐</description>
    <docs>http://www.rssboard.org/rss-specification</docs>
    <generator>python-feedgen</generator>
    <language>zh-CN</language>
    <lastBuildDate>Fri, 20 Sep 2024 19:37:14 +0800</lastBuildDate>
    <item>
      <title>Representing Positional Information in Generative World Models for Object Manipulation</title>
      <link>http://arxiv.org/abs/2409.12005v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 物体操控能力是区分具身代理（embodied agents）与环境互动的重要技能，尤其在机器人领域。&lt;h4&gt;2. 交互预测的重要性&lt;/h4&gt;   - 预测与物体交互的结果在操控任务中至关重要。&lt;h4&gt;3. 现有方法的挑战&lt;/h4&gt;   - 尽管模型基于控制的方法开始用于处理操控任务，但在准确操控物体方面仍面临挑战。&lt;h4&gt;4. 性能不足的原因&lt;/h4&gt;   - 研究分析发现，当前世界模型在表示关键位置信息（尤其是目标物体位置任务的目标规格）方面存在不足，导致性能欠佳。&lt;h4&gt;5. 新方法的提出&lt;/h4&gt;   - 引入一种通用的方法，使基于世界模型的代理能够有效解决物体定位任务。&lt;h4&gt;6. 两种学习策略&lt;/h4&gt;   - 提出两种生成世界模型的策略：     - **位置条件政策学习（PCP）**：关注基于具体位置的学习。     - **潜在条件政策学习（LCP）**：使用对象中心的潜在表示，明确捕捉物体位置信息以进行目标规格。&lt;h4&gt;7. 多模态能力的出现&lt;/h4&gt;   - LCP方法使得通过空间坐标或视觉目标来指定目标的多模态能力自然出现。&lt;h4&gt;8. 评估结果&lt;/h4&gt;   - 这些方法在多个操控环境中经过严格评估，显示出相较于当前模型基于控制方法的优越性能。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Object manipulation capabilities are essential skills that set apart embodiedagents engaging with the world, especially in the realm of robotics. Theability to predict outcomes of interactions with objects is paramount in thissetting. While model-based control methods have started to be employed fortackling manipulation tasks, they have faced challenges in accuratelymanipulating objects. As we analyze the causes of this limitation, we identifythe cause of underperformance in the way current world models represent crucialpositional information, especially about the target's goal specification forobject positioning tasks. We introduce a general approach that empowers worldmodel-based agents to effectively solve object-positioning tasks. We proposetwo declinations of this approach for generative world models:position-conditioned (PCP) and latent-conditioned (LCP) policy learning. Inparticular, LCP employs object-centric latent representations that explicitlycapture object positional information for goal specification. This naturallyleads to the emergence of multimodal capabilities, enabling the specificationof goals through spatial coordinates or a visual goal. Our methods arerigorously evaluated across several manipulation environments, showingfavorable performance compared to current model-based control approaches.</description>
      <author>example@mail.com (Stefano Ferraro, Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Sai Rajeswar)</author>
      <guid isPermaLink="false">2409.12005v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>WeHelp: A Shared Autonomy System for Wheelchair Users</title>
      <link>http://arxiv.org/abs/2409.12159v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 背景介绍&lt;/h4&gt;   - 有大量的轮椅用户，大多数需要在日常活动中获得帮助。&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 根据近期报告，轮椅用户的需求未能得到有效满足，主要由于护理人员的缺乏。&lt;h4&gt;3. 项目目标&lt;/h4&gt;   - 本项目开发了WeHelp，一个面向轮椅用户的共享自主系统。&lt;h4&gt;4. 系统模式&lt;/h4&gt;   - WeHelp系统的机器人具有三种工作模式：     - **跟随模式**：机器人通过视觉追踪自动跟随轮椅用户，用户可以选择从后面、左侧或右侧跟随。     - **遥控模式**：遥控助手接管机器人，帮助用户完成复杂任务。     - **遥操作模式**：用户通过操纵杆控制机器人，执行如开门、移动障碍物、获取高处或低处物体等复杂任务。&lt;h4&gt;5. 语音识别功能&lt;/h4&gt;   - 当轮椅用户请求帮助时，机器人通过语音识别识别命令，并切换到遥操作模式或遥控模式。&lt;h4&gt;6. 实用性评估&lt;/h4&gt;   - 评估结果显示，该系统对轮椅用户是有用且实用的。&lt;h4&gt;7. 资源链接&lt;/h4&gt;   - 论文的源代码和演示可在[GitHub - Walleclipse/WeHelp](https://github.com/Walleclipse/WeHelp)获取。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; There is a large population of wheelchair users. Most of the wheelchair usersneed help with daily tasks. However, according to recent reports, their needsare not properly satisfied due to the lack of caregivers. Therefore, in thisproject, we develop WeHelp, a shared autonomy system aimed for wheelchairusers. A robot with a WeHelp system has three modes, following mode, remotecontrol mode and tele-operation mode. In the following mode, the robot followsthe wheelchair user automatically via visual tracking. The wheelchair user canask the robot to follow them from behind, by the left or by the right. When thewheelchair user asks for help, the robot will recognize the command via speechrecognition, and then switch to the teleoperation mode or remote control mode.In the teleoperation mode, the wheelchair user takes over the robot with a joystick and controls the robot to complete some complex tasks for their needs,such as opening doors, moving obstacles on the way, reaching objects on a highshelf or on the low ground, etc. In the remote control mode, a remote assistanttakes over the robot and helps the wheelchair user complete some complex tasksfor their needs. Our evaluation shows that the pipeline is useful and practicalfor wheelchair users. Source code and demo of the paper are available at\url{https://github.com/Walleclipse/WeHelp}.</description>
      <author>example@mail.com (Abulikemu Abuduweili, Alice Wu, Tianhao Wei, Weiye Zhao)</author>
      <guid isPermaLink="false">2409.12159v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>Implicit Reasoning in Deep Time Series Forecasting</title>
      <link>http://arxiv.org/abs/2409.10840v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 最近，时间序列基础模型在来自多个领域的时间序列数据上展现了令人期待的零-shot预测性能。&lt;h4&gt;2. 研究问题&lt;/h4&gt;   - 目前尚不清楚这些成功是源于对时间动态的真正理解，还是仅仅来自于对训练数据的记忆。&lt;h4&gt;3. 现有研究的局限&lt;/h4&gt;   - 尽管语言模型中的隐性推理已被研究，但对时间序列模型的类似评估仍然基本未被探索。&lt;h4&gt;4. 研究目标&lt;/h4&gt;   - 本文旨在初步评估深度时间序列预测模型的推理能力。&lt;h4&gt;5. 模型类型&lt;/h4&gt;   - 研究发现某些线性模型、基于多层感知器（MLP）的模型以及基于补丁的Transformer模型在系统设计的分布外场景中有效地进行了泛化。&lt;h4&gt;6. 推理能力的发现&lt;/h4&gt;   - 这些结果表明，这些模型具备超出简单模式记忆的推理能力，显示出尚未被充分探索的推理潜力。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Recently, time series foundation models have shown promising zero-shotforecasting performance on time series from a wide range of domains. However,it remains unclear whether their success stems from a true understanding oftemporal dynamics or simply from memorizing the training data. While implicitreasoning in language models has been studied, similar evaluations for timeseries models have been largely unexplored. This work takes an initial steptoward assessing the reasoning abilities of deep time series forecastingmodels. We find that certain linear, MLP-based, and patch-based Transformermodels generalize effectively in systematically orchestratedout-of-distribution scenarios, suggesting underexplored reasoning capabilitiesbeyond simple pattern memorization.</description>
      <author>example@mail.com (Willa Potosnak, Cristian Challu, Mononito Goswami, Michał Wiliński, Nina Żukowska, Artur Dubrawski)</author>
      <guid isPermaLink="false">2409.10840v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>Vision foundation models: can they be applied to astrophysics data?</title>
      <link>http://arxiv.org/abs/2409.11175v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  10 pages, 4 figures, submitted to Foundation Models for Science
  Workshop at NeurIPS 2024&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 视觉基础模型的潜力&lt;/h4&gt;   - 视觉基础模型在许多多媒体应用中展示了显著的潜力，但在自然科学领域的应用较少。&lt;h4&gt;2. 领域特定数据的挑战&lt;/h4&gt;   - 这种低应用主要由于领域特定科学数据与基础模型训练数据的性质不匹配，导致分布偏移。&lt;h4&gt;3. 科学数据的特点&lt;/h4&gt;   - 科学数据在结构和特征上通常有显著差异，研究人员面临着使用有限标注数据（通常只有几百或几千张图像）优化模型性能的挑战。&lt;h4&gt;4. 模型适应性要求&lt;/h4&gt;   - 要有效适应基础模型，需要在预处理、数据增强和训练技术上采取定制化的方法。&lt;h4&gt;5. 模型的独特性&lt;/h4&gt;   - 每种视觉基础模型都有独特的优缺点，这些差异源于架构、训练程序和训练数据集的不同。&lt;h4&gt;6. 研究对象&lt;/h4&gt;   - 本研究评估了各种视觉基础模型在天体物理数据（特别是光学和射电天文学图像）中的应用。&lt;h4&gt;7. 分类准确性提升&lt;/h4&gt;   - 结果表明，使用特定基础模型提取的特征能提高光学星系图像的分类准确性，相比传统的监督训练方法更为有效。&lt;h4&gt;8. 物体检测性能&lt;/h4&gt;   - 在射电图像的物体检测任务中，这些模型也表现出等同或更好的性能。&lt;h4&gt;9. 射电星系图像分类的局限&lt;/h4&gt;   - 然而，在射电星系图像的分类中，它们的性能普遍较差，常常低于传统监督训练的结果。&lt;h4&gt;10. 模型选择的考虑&lt;/h4&gt;    - 这些发现表明，选择适合天体物理应用的视觉基础模型需要仔细考虑模型特性及与下游任务的具体要求的对齐。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-17&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Vision foundation models, which have demonstrated significant potential inmany multimedia applications, are often underutilized in the natural sciences.This is primarily due to mismatches between the nature of domain-specificscientific data and the typical training data used for foundation models,leading to distribution shifts. Scientific data often differ substantially instructure and characteristics; researchers frequently face the challenge ofoptimizing model performance with limited labeled data of only a few hundred orthousand images. To adapt foundation models effectively requires customizedapproaches in preprocessing, data augmentation, and training techniques.Additionally, each vision foundation model exhibits unique strengths andlimitations, influenced by differences in architecture, training procedures,and the datasets used for training. In this work, we evaluate the applicationof various vision foundation models to astrophysics data, specifically imagesfrom optical and radio astronomy. Our results show that using featuresextracted by specific foundation models improves the classification accuracy ofoptical galaxy images compared to conventional supervised training. Similarly,these models achieve equivalent or better performance in object detection taskswith radio images. However, their performance in classifying radio galaxyimages is generally poor and often inferior to traditional supervised trainingresults. These findings suggest that selecting suitable vision foundationmodels for astrophysics applications requires careful consideration of themodel characteristics and alignment with the specific requirements of thedownstream tasks.</description>
      <author>example@mail.com (E. Lastufka, M. Drozdova, V. Kinakh, D. Piras, S. Voloshynovskyy)</author>
      <guid isPermaLink="false">2409.11175v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>jina-embeddings-v3: Multilingual Embeddings With Task LoRA</title>
      <link>http://arxiv.org/abs/2409.10173v3</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  20 pages, pp11-13 references, pp14-20 appendix and experiment tables&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 模型介绍&lt;/h4&gt;   - 介绍了jina-embeddings-v3，这是一个新的文本嵌入模型，拥有5.7亿个参数。&lt;h4&gt;2. 性能表现&lt;/h4&gt;   - 在多语言数据和长上下文检索任务上，jina-embeddings-v3实现了最先进的性能，支持最长达8192个标记的上下文长度。&lt;h4&gt;3. 任务特定适配器&lt;/h4&gt;   - 模型包含一组特定于任务的低秩适配器（LoRA），用于生成高质量的嵌入，适用于查询-文档检索、聚类、分类和文本匹配等任务。&lt;h4&gt;4. 基准评估&lt;/h4&gt;   - 在MTEB基准测试中，jina-embeddings-v3在英语任务上超越了OpenAI和Cohere的最新专有嵌入模型。&lt;h4&gt;5. 多语言任务的优势&lt;/h4&gt;   - 在所有多语言任务中，jina-embeddings-v3的表现优于multilingual-e5-large-instruct。&lt;h4&gt;6. 输出维度灵活性&lt;/h4&gt;   - 默认输出维度为1024，用户可以灵活地将嵌入维度降至最低32，而不会影响性能，这得益于Matryoshka Representation Learning。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-16&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; We introduce jina-embeddings-v3, a novel text embedding model with 570million parameters, achieves state-of-the-art performance on multilingual dataand long-context retrieval tasks, supporting context lengths of up to 8192tokens. The model includes a set of task-specific Low-Rank Adaptation (LoRA)adapters to generate high-quality embeddings for query-document retrieval,clustering, classification, and text matching. Evaluation on the MTEB benchmarkshows that jina-embeddings-v3 outperforms the latest proprietary embeddingsfrom OpenAI and Cohere on English tasks, while achieving superior performancecompared to multilingual-e5-large-instruct across all multilingual tasks. Witha default output dimension of 1024, users can flexibly reduce the embeddingdimensions to as low as 32 without compromising performance, enabled byMatryoshka Representation Learning.</description>
      <author>example@mail.com (Saba Sturua, Isabelle Mohr, Mohammad Kalim Akram, Michael Günther, Bo Wang, Markus Krimmel, Feng Wang, Georgios Mastrapas, Andreas Koukounas, Nan Wang, Han Xiao)</author>
      <guid isPermaLink="false">2409.10173v3</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>Human-like Affective Cognition in Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11733v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 情感理解的重要性&lt;/h4&gt;   - 理解情感是人类互动和体验的基础。&lt;h4&gt;2. 人类的情感推理能力&lt;/h4&gt;   - 人类能够轻松地从情境或面部表情推断情感，并能从情感推断情境，以及进行其他多种情感认知。&lt;h4&gt;3. 现代AI的推理能力&lt;/h4&gt;   - 本文探讨现代AI在这些推理方面的能力。&lt;h4&gt;4. 评估框架的提出&lt;/h4&gt;   - 介绍了一种评估框架，用于测试基础模型的情感认知能力。&lt;h4&gt;5. 基于心理学理论的场景生成&lt;/h4&gt;   - 从心理学理论出发，生成了1,280个多样的场景，以探索评估、情感、表情和结果之间的关系。&lt;h4&gt;6. 模型与人类的评估&lt;/h4&gt;   - 对基础模型（如GPT-4、Claude-3、Gemini-1.5-Pro）和567名人类进行评估。&lt;h4&gt;7. 研究结果&lt;/h4&gt;   - 结果显示，基础模型往往与人类直觉一致，匹配或超过参与者之间的协议。&lt;h4&gt;8. 超越人类的表现&lt;/h4&gt;   - 在某些条件下，这些模型表现出“超人类”的能力，能够更好地预测典型人类判断。&lt;h4&gt;9. 思维链的好处&lt;/h4&gt;   - 所有模型在链式推理方面均受益，这表明基础模型已获得人类般的情感理解及其对信念和行为的影响。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Understanding emotions is fundamental to human interaction and experience.Humans easily infer emotions from situations or facial expressions, situationsfrom emotions, and do a variety of other affective cognition. How adept ismodern AI at these inferences? We introduce an evaluation framework for testingaffective cognition in foundation models. Starting from psychological theory,we generate 1,280 diverse scenarios exploring relationships between appraisals,emotions, expressions, and outcomes. We evaluate the abilities of foundationmodels (GPT-4, Claude-3, Gemini-1.5-Pro) and humans (N = 567) across carefullyselected conditions. Our results show foundation models tend to agree withhuman intuitions, matching or exceeding interparticipant agreement. In someconditions, models are ``superhuman'' -- they better predict modal humanjudgements than the average human. All models benefit from chain-of-thoughtreasoning. This suggests foundation models have acquired a human-likeunderstanding of emotions and their influence on beliefs and behavior.</description>
      <author>example@mail.com (Kanishk Gandhi, Zoe Lynch, Jan-Philipp Fränken, Kayla Patterson, Sharon Wambu, Tobias Gerstenberg, Desmond C. Ong, Noah D. Goodman)</author>
      <guid isPermaLink="false">2409.11733v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>Cross-Organ and Cross-Scanner Adenocarcinoma Segmentation using Rein to Fine-tune Vision Foundation Models</title>
      <link>http://arxiv.org/abs/2409.11752v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. 研究背景&lt;/h4&gt;   - 数字病理学领域的肿瘤分割在近年来取得了显著进展。&lt;h4&gt;2. 问题陈述&lt;/h4&gt;   - 器官差异、组织制备方法和图像获取过程的变化可能导致数字病理图像之间的领域差异。&lt;h4&gt;3. 研究方法&lt;/h4&gt;   - 本文使用Rein，一种微调方法，以参数化和高效的方式对各种视觉基础模型（VFM）进行微调，针对MICCAI 2024跨器官和跨扫描腺癌分割（COSAS2024）挑战。&lt;h4&gt;4. Rein的核心&lt;/h4&gt;   - Rein的核心是一个可学习的令牌集合，直接与实例关联，提升了每层在实例级别的功能。&lt;h4&gt;5. 实验环境&lt;/h4&gt;   - 在COSAS2024挑战的数据环境中，进行了广泛的实验，证明Rein对VFM的微调取得了满意的结果。&lt;h4&gt;6. 具体实现&lt;/h4&gt;   - 使用Rein对ConvNeXt和DINOv2进行了微调。&lt;h4&gt;7. 实验结果&lt;/h4&gt;   - ConvNeXt在任务1的初步测试阶段和最终测试阶段分别获得了0.7719和0.7557的分数。   - DINOv2在任务2的初步测试阶段和最终测试阶段分别获得了0.8848和0.8192的分数。&lt;h4&gt;8. 代码可用性&lt;/h4&gt;   - 相关代码可在GitHub上获取。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; In recent years, significant progress has been made in tumor segmentationwithin the field of digital pathology. However, variations in organs, tissuepreparation methods, and image acquisition processes can lead to domaindiscrepancies among digital pathology images. To address this problem, in thispaper, we use Rein, a fine-tuning method, to parametrically and efficientlyfine-tune various vision foundation models (VFMs) for MICCAI 2024 Cross-Organand Cross-Scanner Adenocarcinoma Segmentation (COSAS2024). The core of Reinconsists of a set of learnable tokens, which are directly linked to instances,improving functionality at the instance level in each layer. In the dataenvironment of the COSAS2024 Challenge, extensive experiments demonstrate thatRein fine-tuned the VFMs to achieve satisfactory results. Specifically, we usedRein to fine-tune ConvNeXt and DINOv2. Our team used the former to achievescores of 0.7719 and 0.7557 on the preliminary test phase and final test phasein task1, respectively, while the latter achieved scores of 0.8848 and 0.8192on the preliminary test phase and final test phase in task2. Code is availableat GitHub.</description>
      <author>example@mail.com (Pengzhou Cai, Xueyuan Zhang, Libin Lan, Ze Zhao)</author>
      <guid isPermaLink="false">2409.11752v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
    <item>
      <title>Using Large Language Models to Generate Clinical Trial Tables and Figures</title>
      <link>http://arxiv.org/abs/2409.12046v2</link>
      <description>&lt;strong&gt;评论:&lt;/strong&gt;  None&lt;h3&gt;GPT 摘要:&lt;/h3&gt; &lt;h4&gt;1. TFL的作用&lt;/h4&gt;   - 表格、图形和列表（TFLs）是总结临床试验数据的重要工具。&lt;h4&gt;2. 制作TFL的挑战&lt;/h4&gt;   - 创建TFL用于报告活动通常是一个耗时的任务，在临床试验执行过程中经常遇到。&lt;h4&gt;3. 研究目的&lt;/h4&gt;   - 本研究探讨了使用大型语言模型（LLMs）通过提示工程和少量样本迁移学习来自动化生成TFL的可能性。&lt;h4&gt;4. 数据来源&lt;/h4&gt;   - 使用公共的ADaM格式临床试验数据进行研究。&lt;h4&gt;5. 研究结果&lt;/h4&gt;   - 结果表明，LLMs能够通过提示指令有效地生成TFL，展示了其在该领域的潜力。&lt;h4&gt;6. 开发应用程序&lt;/h4&gt;   - 开发了一种名为“临床试验TFL生成代理”的对话代理应用，能够将用户查询与预定义提示匹配。&lt;h4&gt;7. 功能特点&lt;/h4&gt;   - 该应用生成定制程序以生成特定的预定义TFL，从而提高生成效率。&lt;br&gt;&lt;br&gt;&lt;strong&gt;发布时间:&lt;/strong&gt;  2024-09-18&lt;br&gt;&lt;strong&gt;DOI:&lt;/strong&gt; None&lt;br&gt;&lt;strong&gt;repo:&lt;/strong&gt;null&lt;br&gt;&lt;h3&gt;摘要:&lt;/h3&gt; Tables, figures, and listings (TFLs) are essential tools for summarizingclinical trial data. Creation of TFLs for reporting activities is often atime-consuming task encountered routinely during the execution of clinicaltrials. This study explored the use of large language models (LLMs) to automatethe generation of TFLs through prompt engineering and few-shot transferlearning. Using public clinical trial data in ADaM format, our resultsdemonstrated that LLMs can efficiently generate TFLs with prompt instructions,showcasing their potential in this domain. Furthermore, we developed aconservational agent named Clinical Trial TFL Generation Agent: An app thatmatches user queries to predefined prompts that produce customized programs togenerate specific predefined TFLs.</description>
      <author>example@mail.com (Yumeng Yang, Peter Krusche, Kristyn Pantoja, Cheng Shi, Ethan Ludmir, Kirk Roberts, Gen Zhu)</author>
      <guid isPermaLink="false">2409.12046v2</guid>
      <pubDate>Fri, 20 Sep 2024 19:37:14 +0800</pubDate>
    </item>
  </channel>
</rss>
